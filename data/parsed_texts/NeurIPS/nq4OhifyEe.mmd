# Truncated Affinity Maximization: One-class

Homophily Modeling for Graph Anomaly Detection

 Hezhe Qiao, Guansong Pang

School of Computing and Information Systems, Singapore Management University

hezheqiao.2022@phdcs.smu.edu.sg, gspang@smu.edu.sg

Corresponding author: G. Pang

###### Abstract

We reveal a _one-class homophily_ phenomenon, which is one prevalent property we find empirically in real-world graph anomaly detection (GAD) datasets, _i.e._, normal nodes tend to have strong connection/affinity with each other, while the homophily in abnormal nodes is significantly weaker than normal nodes. However, this anomaly-discriminative property is ignored by existing GAD methods that are typically built using a conventional anomaly detection objective, such as data reconstruction. In this work, we explore this property to introduce a novel unsupervised anomaly scoring measure for GAD - _local node affinity_ - that assigns a larger anomaly score to nodes that are less affiliated with their neighbors, with the affinity defined as similarity on node attributes/representations. We further propose Truncated Affinity Maximization (TAM) that learns tailored node representations for our anomaly measure by maximizing the local affinity of nodes to their neighbors. Optimizing on the original graph structure can be biased by _non-homophily edges_ (_i.e._, edges connecting normal and abnormal nodes). Thus, TAM is instead optimized on truncated graphs where non-homophily edges are removed iteratively to mitigate this bias. The learned representations result in significantly stronger local affinity for normal nodes than abnormal nodes. Extensive empirical results on 10 real-world GAD datasets show that TAM substantially outperforms seven competing models, achieving over 10% increase in AUROC/AUPRC compared to the best contenders on challenging datasets. Our code is available at https://github.com/mala-lab/TAM-master/.

## 1 Introduction

Graph anomaly detection (GAD) aims to identify abnormal nodes that are different from the majority of the nodes in a graph. It has attracted great research interest in recent years due to its broad real-world applications, _e.g._, detection of abusive reviews or malicious/fraudulent users [9, 16, 37, 32, 57]. Since graph data is non-Euclidean with diverse graph structure and node attributes, it is challenging to effectively model the underlying normal patterns and detect abnormal nodes in different graphs. To address this challenge, graph neural networks (GNNs) have been widely used for GAD. The GNN-based methods are often built using a data reconstruction [8, 11, 59, 65, 31] or self-supervised learning [54, 63, 17, 28, 15] objective. The data reconstruction approaches focus on learning node representations for GAD by minimizing the errors of reconstructing both node attributes and graph structure, while the self-supervised approaches focus on designing a proxy task that is related to anomaly detection, such as prediction of neighbor hops [17] and prediction of the relationship between a node and a subgraph [28], to learn the node representations for anomaly detection.

These approaches, however, ignore one prevalent anomaly-discriminative property we find empirically in real-world GAD datasets, namely _one-class homophily_, _i.e._, normal nodes tend to have strong connection/affinity with each other, while the homophily in abnormal nodes is significantly weakerthan normal nodes. This phenomenon can be observed in datasets with synthetic/real anomalies, as shown in Fig. 1(a) (see App. A for results on more datasets). The abnormal nodes do not exhibit homophily relations to each other mainly because abnormal behaviors are unbounded and can be drawn from different distributions.

Motivated by the one-class homophily property, we introduce a novel unsupervised anomaly scoring measure for GAD - _local node affinity_ - that assigns a larger anomaly score to nodes that are less affiliated with their neighbors. Since we do not have access to class labels, we define the affinity in terms of similarity on node attributes/representations to capture the homophily relations within the normal class. Nodes having strong local affinity are the nodes that are connected to many nodes of similar attributes, and those nodes are considered more likely to be normal nodes. One challenge of using this anomaly measure is that some abnormal nodes can also be connected to nodes of similar abnormal behaviors. A straightforward solution to this problem is to apply our anomaly measure in a node representation space learned by off-the-shelf popular representation learning objectives like DGI [51], but the representations of normal and abnormal nodes can become similar due to the presence of _non-homophily edges_ (_i.e._, edges connecting normal and abnormal nodes) that homogenize the normal and abnormal node representations in GNN message passing. To address this issue, we further propose Truncated Affinity Maximization (TAM) that learns tailored node representations for our anomaly scoring measure by maximizing the local node affinity to their neighbors, with the non-homophily edges being truncated to avoid the over-smooth representation issue. The learned representations result in significantly stronger local affinity for normal nodes than abnormal nodes. As shown in Fig. 1(b), it is difficult for using the local node affinity to distinguish normal and abnormal nodes on raw node attributes and DGI-based node representations, whereas the TAM-based node representation space offers well-separable local affinity results between normal and abnormal nodes. In summary, this work makes the following main contributions:

* We, for the first time, empirically reveal the one-class homophily phenomenon that provides an anomaly-discriminative property for GAD. Motivated by this property, we introduce a novel unsupervised anomaly scoring measure, local node affinity (Sec. 3.2).
* We then introduce Truncated Affinity Maximization (TAM) that learns tailored node representations for the proposed anomaly measure. TAM makes full use of the one-class homophily to learn expressive normal representations by maximizing local node affinity on truncated graphs, offering discriminative local affinity scores for accurate GAD.
* We further introduce two novel components (Sec. 3.3), namely Local Affinity Maximization-based graph neural networks (LAMNet for short) and Normal Structure-preserved Graph Truncation (NSGT), to implement TAM. Empirical results on six real-world GAD datasets show that our TAM model substantially outperforms seven competing models.

## 2 Related Work

Numerous anomaly detection methods have been introduced, including both shallow and deep approaches [37; 5], but most of them are focused on non-graph data. There have been many studies on GAD exclusively. Most of previous work use shallow methods [2], such as Radar [22], AMEN [43],

Figure 1: (**a**) Homophily and (**b**) local affinity distributions of normal and abnormal nodes on two popular benchmarks, BlogCatalog [49] and Amazon [10]. The homophily of a given node is calculated using the number of nodes that have the same class label as the given node [13]. The local affinity is calculated on raw attributes (RA) and node representations learned by DGI [51] and TAM, respectively.

and ANOMALOUS [41]. Matrix decomposition and residual analysis are commonly used in these methods, whose performance is often bottlenecked due to the lack of representation power to capture the rich semantics of the graph data and to handle high-dimensional node attributes and/or sparse graph structures.

GNN-based GAD methods have shown substantially better detection performance in recent years [32]. Although some methods are focused on a supervised setting, such as CARE-GNN [10], PCGNN [27], Fraudre [58], BWGNN [47], and GHRN [12], most of them are unsupervised methods. These unsupervised methods can be generally categorized into two groups: data reconstruction-based and self-supervised-based approach. Below we discuss these most related methods in detail.

**Data Reconstruction-based Approach.** As one of the most popular methods, graph auto-encoder (GAE) is employed by many researchers to learn the distribution of normal samples for GAD. Ding _et al._[8] propose DOMINANT, which reconstructs the graph structure and node attributes by GAE. The anomaly score is defined as the reconstruction error from both node attributes and its structure. It is often difficult for GAE to learn discriminative representations because it can overfit the given graph data. Several mechanisms, including attention, sampling, edge removing, and meta-edge choosing [9, 27, 30, 45], are designed to alleviate this issue during neighborhood aggregation. Some recent variants like ComGA [31] and AnomalyDAE [11] incorporate an attention mechanism to improve the reconstruction. Other methods like SpaceAE [24] and ResGCN [39] aim to differentiate normal and abnormal nodes that do not have significant deviation, _e.g._, by exploring residual information between nodes. In general, reconstructing the structure of a node based on the similarities to its neighbors is related to but different from local node affinity in TAM (see Sec. 3.2), and GAE and TAM are also learned by a very different objective (see Sec. 3.3). Being GNN-based approaches, both the reconstruction-based approaches and our approach TAM rely on the node's neighborhood information to obtain the anomaly scores, but we explicitly define an anomaly measure from a new perspective, _i.e._, local node affinity. This offers a fundamentally different approach for GAD.

**Self-supervised Approach.** Although data reconstruction methods can also be considered as a self-supervised approach, here we focus on non-reconstruction pre-text tasks for GAD. One such popular method is a proxy classification or contrastive learning task [18, 53, 65]. Liu _et al._[28] propose CoLA, which combines contrastive learning and sub-graph extraction to perform self-supervised GAD. Based on CoLA, SL-GAD is proposed to develop generative attribute regression and multi-view contrastive learning [63]. There are some methods that leverage some auxiliary information such degree, symmetric and hop to design the self-supervised task [6, 61, 61, 23, 24, 25, 66]. For example, Huang _et al._ propose the method HCM-A [17] to utilize hop count prediction for GAD. Although some self-supervised methods construct the classification model on the relationship between the node and the contextual subgraph, this group of methods is not related to local node affinity directly, and its performance heavily depends on how the pre-text task is related to anomaly detection. Similar to the self-supervised approaches, the optimization of TAM also relies on an unsupervised objective. However, the self-supervised approaches require the use of some pre-text tasks like surrogate contrastive learning or classification tasks to learn the feature representations for anomaly detection. By contrast, the optimization of TAM is directly driven by a new, plausible anomaly measure, which enables end-to-end optimization of an explicitly defined anomaly measure.

## 3 Method

### The Proposed TAM Approach

**Problem Statement.** We tackle unsupervised anomaly detection on an attributed graph. Specifically, let \(G=(\mathcal{V},\mathcal{E},\mathbf{X})\) be an attributed graph, where \(\mathcal{V}=\{v_{1},\cdots,v_{N}\}\) denotes its node set, \(\mathcal{E}\subseteq\mathcal{V}\times\mathcal{V}\) with \(e\in\mathcal{E}\) is the edge set, \(e_{ij}=1\) represents there is a connection between node \(v_{i}\) and \(v_{j}\), \(\mathbf{X}\in\mathbb{R}^{N\times M}\) denotes the matrix of node attributes and \(\mathbf{x}_{i}\in\mathbb{R}^{M}\) is the attribute vector of \(v_{i}\), and \(\mathbf{A}\in\{0,1\}^{N\times N}\) is the adjacency matrix of \(G\) with \(\mathbf{A}_{ij}=1\) iff \((v_{i},v_{j})\in\mathcal{E}\), then GAD aims to learn an anomaly scoring function \(f:\mathcal{V}\rightarrow\mathbb{R}\), such that \(f(v)<f(v^{\prime})\), \(\forall v\in\mathcal{V}_{n},v^{\prime}\in\mathcal{V}_{a}\), where \(\mathcal{V}_{n}\) and \(\mathcal{V}_{a}\) denotes the set of normal and abnormal nodes, respectively. Per the nature of anomaly, it is typically assumed that \(|\mathcal{V}_{n}|\gg|\mathcal{V}_{a}|\). But in unsupervised GAD we do not have access to the class labels of the nodes during training. In this work, our problem is to learn an unsupervised local node affinity-based anomaly scoring function \(f\).

**Our Proposed Framework.** Motivated by the one-class homophily phenomenon, we introduce a local node affinity-based anomaly score measure. Since it is difficult to capture and quantify the one-class homophily in the raw attribute space and the generic node representation space, we introduce a truncated affinity maximization (TAM) approach to learn a tailored representation space where the local node affinity can well distinguish normal and abnormal nodes based on the one-class homophily property. As shown in Fig. 2, TAM consists of two novel components, namely _local affinity maximization-based graph neural networks_ (LAMNet) and _normal structure-preserved graph truncation_ (NSGT). LAMNet trains a graph neural network using a local affinity maximization objective on an iteratively truncated graph structure yielded by NSGT.

NSGT is designed in a way through which we preserve the homophily edges while eliminating non-homophily edges iteratively. The message passing in LAMNet is then performed using the truncated adjacency matrix rather than the original one. In doing so, we reinforce the strong affinity among nodes with homophily relations to their neighbors (_e.g._, normal nodes), avoiding the potential bias caused by non-homophily edges (_e.g._, connections to abnormal nodes). The output node affinity in LAMNet is used to define anomaly score, _i.e._, the weaker the local affinity is, the more likely the node is an abnormal node. There exists some randomness in our NSGT-based graph truncation. We utilize those randomness for more effective GAD by building a bagging ensemble of TAM.

### Local Node Affinity as Anomaly Measure

As shown in Fig. 1, normal nodes have significantly stronger homophily relations with each other than the abnormal nodes. However, we do not have class label information to calculate the homophily of each node in unsupervised GAD. We instead utilize the local affinity of each node to its neighbors to exploit this one-class homophily property for unsupervised GAD. The local affinity can be defined as an averaged similarity to the neighboring nodes, and the anomaly score \(f\) is opposite to the affinity:

\[h(v_{i})=\frac{1}{\left|\mathcal{N}\left(v_{i}\right)\right|}\sum_{v_{j}\in \mathcal{N}\left(v_{i}\right)}\text{sim}\left(\mathbf{x}_{i},\mathbf{x}_{j} \right)\text{ ; }f(v_{i})=-h(v_{i}),\] (1)

where \(\mathcal{N}\left(v_{i}\right)\) is the neighbor set of node \(v_{i}\) and \(\text{sim}(\mathbf{x}_{i},\mathbf{x}_{j})=\frac{\mathbf{x}_{i}^{\text{T}} \mathbf{x}_{j}}{\left\|\mathbf{x}_{i}\right\|\mathbf{x}_{j}\right\|}\) measures the similarity of a node pair \((v_{i},v_{j})\). The local node affinity \(h(v_{i})\) is a normal score: the larger the affinity is, the stronger homophily the node has w.r.t. its neighboring nodes based on node attributes, and thus, the more likely the node is a normal node.

The measure in Eq. (1) provides a new perspective to quantify the normality/abnormality of nodes, enabling a much simpler anomaly scoring than existing popular measures such as the reconstruction error \(f(v_{i})=(1-\alpha)\left\|\mathbf{a}_{i}-\mathbf{\widehat{a}}_{i}\right\|_{2 }+\alpha\left\|\mathbf{x}_{i}-\mathbf{\widehat{x}}_{i}\right\|_{2}\), where \(\mathbf{a}_{i}\) denotes the neighborhood structure of \(v_{i}\), \(\mathbf{\widehat{a}}_{i}\) and \(\mathbf{\widehat{x}}_{i}\) are the reconstructed structure and attributes for node \(v_{i}\), and \(\alpha\) is a hyperparameter.

Figure 2: Overview of TAM. (**a**) TAM leverages the observation that normal nodes have stronger affinity relations to their neighbors than anomalies to learn an unsupervised GAD model. It learns a set of affinity maximization GNNs (_i.e._, LAMNet) on a set of sequentially truncated graphs yielded by our probabilistic graph truncation method NSGT. We build an ensemble of TAM models to make use of the randomness in NSGT for more effective GAD. (**b**) NSGT iteratively removes edges with a probability proportional to the distance between the connected nodes.

Our anomaly measure also provides a new perspective to learn tailored node representations for unsupervised GAD. Instead of minimizing the commonly used data reconstruction errors, based on the one-class homophily, we can learn anomaly-discriminative node representations by maximizing the local node affinity. Our TAM approach is designed to achieve this goal.

### TAM: Truncated Affinity Maximization

The local node affinity may not work well in the raw node attribute space since (i) there can be many irrelevant attributes in the original data space and (ii) some abnormal nodes can also be connected to nodes of similar attributes. To tackle this issue, TAM is proposed to learn optimal node representations that maximize the local affinity of nodes that have strong homophily relations with their neighbors in terms of node attributes. Due to the overwhelming presence of normal nodes in a graph, the TAM-based learned node representation space is optimized for normal nodes, enabling stronger local affinity for the normal nodes than the abnormal ones. The TAM-based anomaly scoring using local node affinity can be defined as:

\[f_{\textit{TAM}}(v_{i};\Theta,\mathbf{A},\mathbf{X})=-\frac{1}{|\mathcal{N} \left(v_{i}\right)|}\sum_{v_{j}\in\mathcal{N}\left(v_{i}\right)}\sin\left( \mathbf{h}_{i},\mathbf{h}_{j}\right),\] (2)

where \(\mathbf{h}_{i}=\psi(v_{i};\Theta,\mathbf{A},\mathbf{X})\) is a GNN-based node representation of \(v_{i}\) learned by a mapping function \(\psi\) parameterized by \(\Theta\) in TAM. Below we introduce how we learn the \(\psi\) function via the two components of TAM, LAMNet and NSGT.

**Local Affinity Maximization Networks (LAMNet).** LAMNet aims to learn a GNN-based mapping function \(\psi\) that maximizes the affinity of nodes with homophily relations to their neighbors, while keeping the affinity of nodes with non-homophily edges are weak. Specifically, the projection from the graph nodes onto new representations using \(\ell\) GNN layers can be generally written as

\[\mathbf{H}^{(\ell)}=\mathrm{GNN}\left(\mathbf{A},\mathbf{H}^{(\ell-1)}; \mathbf{W}^{(\ell)}\right),\] (3)

where \(\mathbf{H}^{(\ell)}\in\mathbb{R}^{N\times h^{(\ell)}}\) and \(\mathbf{H}^{(\ell-1)}\in\mathbb{R}^{N\times h^{(l-1)}}\) are the \(h^{(l)}\)-dimensional and \(h^{(l-1)}\)-dimensional representations of node \(v_{i}\) in the \((\ell)\)-th layer and \((\ell-1)\)-th layer, respectively. In the first GNN layer, _i.e._, when \(\ell=1\), the input \(\mathbf{H}^{(0)}\) is set to the raw attribute matrix \(\mathbf{X}\). \(\mathbf{W}^{(\ell)}\) are the weight parameters of \((\ell)\)-th layer. For \(\mathrm{GNN}(\cdot)\), multiple types of GNNs can be used [50, 55]. In this work, we employ a GCN (graph convolutional network) [20] due to its high efficiency. Then \(\mathbf{H}^{(\ell)}\) can be obtained via

\[\mathbf{H}^{(\ell)}=\phi\left(\mathbf{D}^{-\frac{1}{2}}\mathbf{A}\mathbf{D}^{ -\frac{1}{2}}\mathbf{H}^{(\ell-1)}\mathbf{W}^{(\ell-1)}\right),\] (4)

where \(\mathbf{D}=\text{diag}\left(\mathbf{D}_{i}\right)=\sum\limits_{j}\mathbf{A}_{ ij}\) is the degree matrix for the graph \(G\), and \(\phi(\cdot)\) is an activation function. Let \(\mathbf{H}^{(\ell)}=\{\mathbf{h}_{1},\mathbf{h}_{2},...,\mathbf{h}_{N}\}\) be the node representations of the last GCN layer, then the mapping function \(\psi\) is a sequential mapping of graph convolutions as in Eq. (4), with \(\Theta=\{\mathbf{W}^{1},\mathbf{W}^{2},\cdots,\mathbf{W}^{(\ell)}\}\) be the parameter set in our LAMNet. The following objective can then be used to optimize \(\psi\):

\[\min_{\Theta}\quad\sum\limits_{v_{i}\in\mathcal{V}}\left(f_{\textit{TAM}}(v_{ i};\Theta,\mathbf{A},\mathbf{X})+\lambda\frac{1}{|\mathcal{V}\setminus\mathcal{N} \left(v_{i}\right)|}\sum\limits_{v_{k}\in\mathcal{V}\setminus\mathcal{N} \left(v_{i}\right)}\sin\left(\mathbf{h}_{i},\mathbf{h}_{k}\right)\right),\] (5)

where the first term is equivalent to maximizing the local affinity of each node based on the learned node representations, while the second term is a regularization term, and \(\lambda\) is a regularization hyperparameter. The regularization term adds a constraint that the representation of each node should be dissimilar from that of non-adjacent nodes to enforce that the representations of non-local nodes are distinguishable, while maximizing the similarity of the representations of local nodes.

The optimization using Eq. (5) can be largely biased by non-homophily edges. Below we introduce the NSGT component that helps overcome this issue.

**Normal Structure-preserved Graph Truncation (NSGT).** LAMNet is driven by the one-class homophily property, but its objective and graph convolution operations can be biased by the presence non-homophily edges, _i.e._, edges that connect normal and abnormal nodes. The NSGT component is designed to remove these non-homophily edges, yielding a truncated adjacency matrix \(\tilde{\mathbf{A}}\) with homophily edge-based normal graph structure. LAMNet is then performed using the truncated adjacency matrix \(\tilde{\mathbf{A}}\) rather than the original adjacency matrix \(\mathbf{A}\).

Since homophily edges (_i.e._, edges connecting normal nodes) connect nodes of similar attributes, the distance between the nodes of homophily edges is often substantially smaller than that of non-homophily edges. But there can also exist homophily edges that connect dissimilar normal nodes. These can be observed in GAD datasets, as shown in Fig. 3(a-b) for datasets with synthetic/real anomalies. Motivated by this, NSGT takes a probabilistic approach and performs the graph truncation as follows: for a given edge \(e_{ij}=1\), it is considered as a non-homophily edge and removed (i. e. \(e_{ij}=0\)) if and only if the distance between node \(v_{i}\) and node \(v_{j}\) is sufficiently large w.r.t. the neighbor sets of both \(v_{i}\) and \(v_{j}\), \(\mathcal{N}(v_{i})\) and \(\mathcal{N}(v_{j})\). Formally, NSGT truncates the graph by

\[e_{ij}\gets 0\text{ iff }d_{ij}>r_{i}\ \&\ d_{ij}>r_{j},\ \forall e_{ij}=1,\] (6)

where \(d_{ij}\) is a Euclidean distance between \(v_{i}\) and \(v_{j}\) based on node attributes, \(r_{i}\) is a randomly selected value from the range \([d_{mean},d_{i,max}]\) where \(d_{mean}=\frac{1}{m}\sum_{(v_{i},v_{j})\in\varepsilon}d_{ij}\) is the mean distance of graph, where \(m\) is the number of non-zero elements in the adjacent matrix, and \(d_{i,max}\) is the maximum distances in \(\{d_{ik},v_{k}\in\mathcal{N}(v_{i})\}\). Similarly, \(r_{j}\) is randomly sampled from the range \([d_{mean},d_{j,max}]\).

Theoretically, the probability of \(r_{i}<d_{ij}\) can be defined as

\[p\left(r_{i}<d_{ij}\right)=\frac{\max(d_{ij}-d_{mean},0)}{d_{i,\max}-d_{mean}}.\] (7)

Note that this probability is premised on \(d_{i,\max}>d_{mean}\), and we set \(p\left(r_{i}<d_{ij}\right)=0\) if \(d_{i,\max}\leq d_{mean}\). Then the probability of \(e_{ij}\) being removed during the truncation process is as follows

\[p(\mathcal{E}\setminus e_{ij})=p\left(r_{i}<d_{ij}\right)p\left(r_{j}<d_{ij} \right),\] (8)

As shown in Fig. 2(b), if \(e_{ij}\) is a homophily edge, we would have a small \(d_{ij}\), which results in small \(p\left(r_{i}<d_{ij}\right)\) and \(p\left(r_{j}<d_{ij}\right)\), and thus, \(p(\mathcal{E}\setminus e_{ij})\) is also small. By contrast, a non-homophily edge would result in a large \(d_{ij}\), and ultimately a large \(p(\mathcal{E}\setminus e_{ij})\). Therefore, NSGT can help eliminate non-homophily edges with a high probability, while preserving the genuine homophily graph structure.

Considering the large difference in the range \([d_{mean},d_{i,max}]\) for each node and the randomness in truncation, NSGT performs a sequentially iterative truncation rather than a single-pass truncation. In each iteration it randomly removes some non-homophily edges with probability proportional to the distance between their associated nodes, and then it updates the range \([d_{mean},d_{i,max}]\) for nodes that have edges being removed. Thus, for \(K\) iterations on a graph \(G\), it would produce a set of \(K\) sequentially truncated graphs with the corresponding truncated adjacency matrices \(\mathcal{A}=\{\tilde{\mathbf{A}}_{1},\tilde{\mathbf{A}}_{2},\cdots,\tilde{ \mathbf{A}}_{K}\}\). Note that since the graph is sequentially truncated, we have \(\mathcal{E}_{i+1}\subset\mathcal{E}_{i}\), where \(\mathcal{E}_{i}\) is the edge set left after the i-th iteration. As shown in Fig. 3(c-d), such a sequentially iterative truncation helps largely increase the homophily of the normal nodes, while at the same time effectively removing non-homophily edges gradually.

**Training.** To detect anomalies in different graph truncation scales, we train a LAMNet on each of the \(K\) sequentially truncated adjacency matrices in \(\mathcal{A}\), resulting in \(K\) LAMNets parameterized by \(\{\Theta_{1},\Theta_{2},\cdots,\Theta_{K}\}\) for various truncation depths. Particularly, for each LAMNet, instead of using \(\mathbf{A}\), its graph convolutions across all GCN layers are performed on the truncated adjacency matrix \(\tilde{\mathbf{A}}_{k}\) as follows to mitigate the biases caused by the

Figure 3: (**a**) and (**b**) are respectively the Euclidean distance statistics of the homophily (N-N) edges that connect normal nodes and the non-homophily (N-A) edges that connect normal and abnormal nodes on BlogCatalog and Amazon. (**c**) Homophily of normal nodes vs. (**d**) the number of non-homophily edges with increasing truncation iterations/depths.

non-homophily edges in message passing:

\[\mathbf{H}^{(\ell)}=\phi\left(\mathbf{D}^{-\frac{1}{2}}\tilde{\mathbf{A}}_{k} \mathbf{D}^{-\frac{1}{2}}\mathbf{H}^{(\ell-1)}\mathbf{W}^{(\ell-1)}\right).\] (9)

In doing so, we complete the training of a TAM-based base model, consisting of \(K\) LAMNets.

Further, being a probabilistic approach, NSGT has some randomnesses in the graph truncation, and so does the LAMNets. To make use of those randomness, we build an ensemble of TAM models with a size of \(T\), as shown in Fig. 2(a). That is, for a graph \(G\), we perform NSGT \(T\) times independently, resulting in \(T\) sets of the truncated adjacency matrix set \(\{\mathcal{A}_{1},\mathcal{A}_{2},\cdots,\mathcal{A}_{T}\}\), with each \(\mathcal{A}=\{\tilde{\mathbf{A}}_{1},\tilde{\mathbf{A}}_{2},\cdots,\tilde{ \mathbf{A}}_{K}\}\). We then train a LAMNet on each of these \(T\times K\) adjacency matrices, obtaining an ensemble of \(T\) TAM models (_i.e._, \(T\times K\) LAMNets).

Note that in training the LAMNets, the local affinity in the first term in Eq. (5) is computed using the original adjacency matrix \(\mathbf{A}\). This is because we aim to utilize the primary graph structure for local affinity-based GAD; the graph truncation is designed to mitigate the graph convolution biases only.

**Inference.** During inference, we can obtain one local node affinity-based anomaly score per node from each LAMNet, and we aggregate the local affinity scores from all \(T\times K\) LAMNets to compute an overall anomaly score as

\[\text{score}\left(v_{i}\right)=\frac{1}{T\times K}\sum_{t=1}^{T}\sum_{k=1}^{ K}f_{\text{TAM}}(v_{i};\Theta^{*}_{t,k},\mathbf{A},\mathbf{X}),\] (10)

where \(\Theta^{*}_{t,k}\) is the learned weight parameters for the LAMNet using the \(k\)-th truncated adjacency matrix in the \(t\)-th TAM model. The weaker the local node affinity in the learned representation spaces under various graph truncation scales, the larger the anomaly score the node \(v_{i}\) has.

## 4 Experiments

**Datasets.** We conduct the experiments on six commonly-used publicly-available real-world GAD datasets from diverse online shopping services and social networks, and citation networks, including BlogCatalog [49], ACM [48], Amazon [10], Facebook [56], Reddit, and YelpChi [21]. The first two datasets contain two types of injected anomalies - contextual and structural anomalies [8, 34] - that are nodes with significantly deviated graph structure and node attributes respectively. The other four datasets contain real anomalies. Detailed information about the datasets can be found in App. B.

**Competing Methods and Performance Metrics.** TAM is compared with two state-of-the-art (SOTA) shallow methods - iForest [25] and ANOMALOUS [41] - and five SOTA GNN-based deep methods, including three self-supervised learning based methods - CoLA [28], SL-GAD [63], and HCM-A [17] - and two reconstruction-based methods - DOMINANT [8] and ComGA [31]. Forest works on the raw node attributes, while ANOMALOUS works on the raw node attributes and graph structure. The other methods learn new representation space for GAD.

Following [4, 38, 52, 64], two popular and complementary evaluation metrics for anomaly detection, Area Under the Receiver Operating Characteristic Curve (AUROC) and Area Under the precision-recall curve (AUPRC), are used. Higher AUROC/AUPRC indicates better performance. The reported AUROC and AUPRC results are averaged over 5 runs with different random seeds.

**Implementation Details.** TAM is implemented in Pytorch 1.6.0 with python 3.7 and all the experiments are run on an NVIDIA GeForce RTX 3090 24GB GPU. In TAM, each LAMNet is implemented by a two-layer GCN, and its weight parameters are optimized using Adam [19] optimizer with 500 epochs and a learning rate of \(1e-5\) by default. \(T=3\) and \(K=4\) are used for all datasets. Datasets with injected anomalies, such as BlogCatalog and ACM, require strong regularization, so \(\lambda=1\) is used by default; whereas \(\lambda=0\) is used for the four real-world datasets. Hyperparameter analysis w.r.t. \(K\) is presented in Sec. 4.2. TAM can perform stably within a range of \(T\) and \(\lambda\) (see App. C.1 for detail). All the competing methods are based on their publicly-available official source code, and they are trained using their recommended optimization and hyperparameter settings in the original papers.

### Main Results

**Effectiveness on Diverse Real-world Datasets.** The AUROC and AUPRC results on six real-world GAD datasets are reported in Tab. 1. TAM substantially outperforms all seven competing methods on all datasets except Reddit in both metrics, having maximally 9% AUROC and 12% AUPRC improvement over the best-competing methods on the challenge dataset Amazon; on Reddit, it ranks second and performs similarly well to the best contender CoLA. Further, existing methods perform very unstably across different datasets. For example, DOMINANT performs well on ACM but badly on the other datasets; CoLA works well on Reddit but it fails in the other datasets. Similar observations are found in a recent comprehensive comparative study [26]. By contrast, TAM can perform consistently well on these diverse datasets. This is mainly because i) the proposed one-class homophily is more pervasive than the GAD intuitions used by existing methods in different datasets, and ii) TAM offers an effective anomaly scoring function to utilize this anomaly-discriminative property for accurate GAD.

**On Detecting Structural and Contextual Anomalies.** We further examine the effectiveness of TAM on detecting two commonly-studied anomaly types, structural and contextual anomalies, with the results on BlogCatalog and ACM reported in Tab. 2, where the three best competing methods on the two datasets in Tab. 1 are used as baselines. Compared to contextual anomalies, it is significantly more challenging to detect structural anomalies, for which TAM outperforms all three methods in both AUROC and AUPRC. As for contextual anomalies, although TAM underperforms SL-GAD and ranks in second in AUROC, it obtains substantially better AUPRC than SL-GAD. Note that compared to AUROC that can be biased by low false positives and indicates overoptimistic performance, AUPRC is a more indicative measure focusing on the performance on the anomaly class exclusively [3, 38]. So, achieving the best AUPRC on all four cases of the two datasets demonstrates the superiority of TAM in precision and recall rates for both types of anomaly. These results also indicate that structural anomalies may have stronger local affinity than contextual anomalies, but both of which often have weaker local affinity than normal nodes.

**TAM vs. Raw/Generic Node Representation Space.** As discussed in Sec. 1, local node affinity requires a new node representation space that is learned for the affinity without being biased by non-homophily edges. We provide quantitative supporting results in Tab. 3, where TAM is compared with **Raw Attribute (RA)**, and the spaces learned by DGI [51] and GMI [40]. It is clear that the representations learned by TAM significantly outperforms all three competing representation spaces on all six datasets.

\begin{table}
\begin{tabular}{c|c|c c c c c c} \hline \hline
**Metric** & **Method** & \multicolumn{5}{c}{**Dataset**} \\ \cline{3-8}  & & BlogCatalog & ACM & Amazon & Facebook & Reddit & YelpChi \\ \hline \multirow{8}{*}{AUROC} & iForest & 0.3765\(\pm\)0.019 & 0.5118\(\pm\)0.018 & 0.5621\(\pm\)0.008 & 0.5382\(\pm\)0.015 & 0.4363\(\pm\)0.020 & 0.4120\(\pm\)0.040 \\  & ANOMALOUS & 0.5652\(\pm\)0.025 & 0.6856\(\pm\)0.003 & 0.4457\(\pm\)0.003 & 0.9021\(\pm\)0.005 & 0.5387\(\pm\)0.012 & 0.4956\(\pm\)0.003 \\  & DOMINANT & 0.7590\(\pm\)0.010 & 0.8569\(\pm\)0.020 & 0.5996\(\pm\)0.004 & 0.5677\(\pm\)0.002 & 0.5555\(\pm\)0.011 & 0.4133\(\pm\)0.010 \\  & CoLA & 0.7746\(\pm\)0.000 & 0.8233\(\pm\)0.000 & 0.5898\(\pm\)0.008 & 0.8434\(\pm\)0.011 & **0.6028\(\pm\)**0.007 & 0.4636\(\pm\)0.001 \\  & SL-GAD & 0.8123\(\pm\)0.002 & 0.8479\(\pm\)0.005 & 0.5937\(\pm\)0.011 & 0.7936\(\pm\)0.005 & 0.5677\(\pm\)0.005 & 0.3312\(\pm\)0.035 \\  & HCA-A & 0.7980\(\pm\)0.004 & 0.8060\(\pm\)0.004 & 0.3956\(\pm\)0.014 & 0.7387\(\pm\)0.032 & 0.4593\(\pm\)0.011 & 0.4593\(\pm\)0.005 \\  & ComGA & 0.7683\(\pm\)0.004 & 0.8221\(\pm\)0.002 & 0.5895\(\pm\)0.008 & 0.6055\(\pm\)0.000 & 0.5453\(\pm\)0.003 & 0.4391\(\pm\)0.000 \\  & TAM (Ours) & **0.8248\(\pm\)**0.003 & **0.8878\(\pm\)**0.022 & **0.7064\(\pm\)**0.010 & **0.9144\(\pm\)**0.008 & 0.6023\(\pm\)0.004 & **0.5643\(\pm\)**0.007 \\ \hline \multirow{8}{*}{AUPRC} & iForest & 0.0423\(\pm\)0.002 & 0.0372\(\pm\)0.001 & 0.1371\(\pm\)0.002 & 0.0316\(\pm\)0.003 & 0.0269\(\pm\)0.001 & 0.0409\(\pm\)0.000 \\  & ANOMALOUS & 0.0652\(\pm\)0.005 & 0.0635\(\pm\)0.006 & 0.0558\(\pm\)0.001 & 0.1898\(\pm\)0.

[MISSING_PAGE_FAIL:9]

beneficial to GAD. TAM-T underperforms TAM since the local affinity based on the truncated graph structure is affected by randomness and uncertainty in the truncation, leading to unstable and suboptimal optimization. We also show in Fig. 4 (b) that aggregating the anomaly scores obtained from different truncation scales/depths helps largely improve the detection performance. Similar observations are found in the corresponding AUROC results (see App. C.3).

### Performance on Large-scale Graphs

In order to demonstrate the effectiveness of TAM on the large-scale datasets, we conduct the experiments on the four large-scale datasets with a large set of nodes and edges, Amazon-all and YelpChi-all by treating the different relations as a single relation following [6], T-Finance [47] and OGB-Proteins [14]. The experimental results are shown in Tab. 7. Due to the increased number of nodes and edges, we set \(K=7\) for these datasets. TAM can perform consistently well on these large-scale datasets and outperforms four comparing methods, which provides further evidence about the effectiveness of our proposed one-class homophily and anomaly measure.

### Handling Camouflage Attributes

Fraudsters may adjust their behaviors to camouflage their malicious activities, which could hamper the performance of GNN-based methods. We evaluate the performance of TAM when there are camouflages in the raw attributes. Particularly, we replace 10%, 20%, 30% randomly sampled original attributes with camouflaged attributes, in which the feature/attribute value of the abnormal nodes is replaced (camouflaged) with the mean feature value of the normal nodes. The results are shown in the Tab. 8, which demonstrates that TAM is robust to a high level of camouflaged features and maintains its superiority over the SOTA models that work on the original features. The reason is that NSGT can still successfully remove some non-homophily edges in the local domain, allowing LAMNet to make full use of the one-class homophily and achieve good performance under the camouflage.

## 5 Conclusion and Future Work

This paper reveals an important anomaly-discriminative property, the one-class homophily, in GAD datasets with either injected or real anomalies. We utilize this property to introduce a novel unsupervised GAD measure, local node affinity, and further introduce a truncated affinity maximization (TAM) approach that end-to-end optimizes the proposed anomaly measure on truncated adjacency matrix. Extensive experiments on 10 real-world GAD datasets show the superiority of TAM over seven SOTA detectors. We also show that the one-class homophily can be exploited to enhance the existing GAD methods.

**Limitation and Future Work.** TAM cannot directly handle primarily isolated nodes in a graph, though those isolated nodes are clearly abnormal. Additionally, like many GNN-based approaches, including GAD methods, TAM also requires a large memory to perform on graphs with a very large node/edge set. The one-class homophily may not hold for some datasets, such as datasets with strong heterophily relations/subgraphs of normal nodes [1, 29, 36, 46, 66, 62]. Our method would require some adaptations to work well on the dataset with strong heterophily or very large graphs, which are also left for future work.

\begin{table}
\begin{tabular}{c|c|c c c} \hline \hline
**Metric** & **Method** & \multicolumn{4}{c}{**Dataset**} \\  & & Amazon-all & YelpChi-all & T-Finance & OGB-Proteins \\ \hline \multirow{5}{*}{AUROC} & OOBINANTANT & 0.6937 & 0.5390 & 0.5380 & 0.7267 \\  & ComGA & 0.7154 & 0.5352 & 0.5542 & 0.7134 \\  & CoLA & 0.2641 & 0.4801 & 0.4839 & 0.7142 \\  & SL-CAD & 0.2718 & 0.3551 & 0.4648 & 0.7371 \\  & TAM & **0.8276** & **0.5818** & **0.6175** & **0.7449** \\ \hline \multirow{5}{*}{AUROC} & OOBINANTANT & 0.1015 & 0.1638 & 0.0474 & **0.2217** \\  & ComGA & 0.1854 & 0.1658 & 0.0481 & 0.1554 \\ \cline{1-1}  & CoLA & 0.0516 & 0.1361 & 0.0410 & 0.1349 \\ \cline{1-1}  & SL-CAD & 0.0444 & 0.1711 & 0.0386 & 0.1771 \\ \cline{1-1}  & TAM & **0.4346** & **0.1386** & **0.0547** & 0.2173 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Results on large-scale graphs

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline
**Metric** & **Method** & \multicolumn{4}{c}{**Dataset**} \\  & & 0\% & 10\% & 20\% & 30\% \\ \hline \multirow{5}{*}{AUROC} & BioCialog & **0.8218** & 0.8045 & 0.8022 & 0.7831 \\  & ACM & **0.8578** & 0.8727 & 0.8688 & 0.8652 \\  & Amoufl & **0.7044** & 0.7036 & 0.6954 & 0.6858 \\  & Facebook & **0.9144** & 0.8570 & 0.8500 & 0.8500 \\  & Reddit & **0.6423** & 0.5989 & 0.5915 & 0.5976 \\  & YelpChi & **0.6540** & 0.5447 & 0.5271 & 0.5301 \\ \hline \multirow{5}{*}{AUROC} & BioCialog & **0.4182** & 0.4014 & 0.4024 & 0.4095 \\  & ACM & **0.5134** & 0.4896 & 0.6461 & 0.4822 \\  & Amaro & **0.2634** & 0.2502 & 0.2400 & 0.2324 \\  & Facebook & **0.2233** & 0.2225 & 0.2126 & 0.2015 \\  & Reddit & **0.0446** & 0.0444 & 0.0144 & 0.0460 \\  & YelpChi & **0.0778** & 0.0726 & 0.0701 & 0.0678 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Results under different levels of camouflage attributes

[MISSING_PAGE_FAIL:11]

* [16] Mengda Huang, Yang Liu, Xiang Ao, Kuan Li, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing He. Auc-oriented graph neural network for fraud detection. In _Proceedings of the ACM Web Conference 2022_, pages 1311-1321, 2022.
* [17] Tianjin Huang, Yulong Pei, Vlado Menkovski, and Mykola Pechenizkiy. Hop-count based self-supervised anomaly detection on attributed networks. _arXiv preprint arXiv:2104.07917_, 2021.
* [18] Ming Jin, Yixin Liu, Yu Zheng, Lianhua Chi, Yuan-Fang Li, and Shirui Pan. Anemone: graph anomaly detection with multi-scale contrastive learning. In _Proceedings of the 30th ACM International Conference on Information & Knowledge Management_, pages 3122-3126, 2021.
* [19] D. Kingma and J. Ba. Adam: A method for stochastic optimization. _Computer Science_, 2014.
* [20] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. _arXiv preprint arXiv:1609.02907_, 2016.
* [21] Srijan Kumar, Xikun Zhang, and Jure Leskovec. Predicting dynamic embedding trajectory in temporal interaction networks. In _Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 1269-1278, 2019.
* [22] Jundong Li, Harsh Dani, Xia Hu, and Huan Liu. Radar: Residual analysis for anomaly detection in attributed networks. In _IJCAI_, pages 2152-2158, 2017.
* [23] Quitong Li, Yanshen He, Cong Xu, Feng Wu, Jianliang Gao, and Zhao Li. Dual-augment graph neural network for fraud detection. In _Proceedings of the 31st ACM International Conference on Information & Knowledge Management_, pages 4188-4192, 2022.
* [24] Yuening Li, Xiao Huang, Jundong Li, Mengnan Du, and Na Zou. Specae: Spectral autoencoder for anomaly detection in attributed networks. In _Proceedings of the 28th ACM international conference on information and knowledge management_, pages 2233-2236, 2019.
* [25] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation-based anomaly detection. _ACM Transactions on Knowledge Discovery from Data (TKDD)_, 6(1):1-39, 2012.
* [26] Kay Liu, Yingtong Dou, Yue Zhao, Xueying Ding, Xiyang Hu, Ruitong Zhang, Kaize Ding, Canyu Chen, Hao Peng, Kai Shu, et al. Bond: Benchmarking unsupervised outlier node detection on static attributed graphs. In _Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2022.
* [27] Yang Liu, Xiang Ao, Zidi Qin, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing He. Pick and choose: a gnn-based imbalanced learning approach for fraud detection. In _Proceedings of the Web Conference 2021_, pages 3168-3177, 2021.
* [28] Yixin Liu, Zhao Li, Shirui Pan, Chen Gong, Chuan Zhou, and George Karypis. Anomaly detection on attributed networks via contrastive self-supervised learning. _IEEE transactions on neural networks and learning systems_, 33(6):2378-2392, 2021.
* [29] Yixin Liu, Yizhen Zheng, Daokun Zhang, Vincent CS Lee, and Shirui Pan. Beyond smoothing: Unsupervised graph representation learning with edge heterophily discriminating. In _Proceedings of the AAAI conference on artificial intelligence_, volume 37, pages 4516-4524, 2023.
* [30] Zhiwei Liu, Yingtong Dou, Philip S Yu, Yutong Deng, and Hao Peng. Alleviating the inconsistency problem of applying graph neural network to fraud detection. In _Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval_, pages 1569-1572, 2020.
* [31] Xuexiong Luo, Jia Wu, Amin Beheshti, Jian Yang, Xiankun Zhang, Yuan Wang, and Shan Xue. Comga: Community-aware attributed graph anomaly detection. In _Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining_, pages 657-665, 2022.
* [32] Xiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Quan Z Sheng, Hui Xiong, and Leman Akoglu. A comprehensive survey on graph anomaly detection with deep learning. _IEEE Transactions on Knowledge and Data Engineering_, 2021.
* [33] Yao Ma, Xiaorui Liu, Neil Shah, and Jiliang Tang. Is homophily a necessity for graph neural networks? _arXiv preprint arXiv:2106.06134_, 2021.

* [34] Julian John McAuley and Jure Leskovec. From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews. In _Proceedings of the 22nd international conference on World Wide Web_, pages 897-908, 2013.
* [35] Arjun Mukherjee, Vivek Venkataraman, Bing Liu, and Natalie Glance. What yelp fake review filter might be doing? In _Proceedings of the international AAAI conference on web and social media_, volume 7, pages 409-418, 2013.
* [36] Caleb C Noble and Diane J Cook. Graph-based anomaly detection. In _Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 631-636, 2003.
* [37] Guansong Pang, Chunhua Shen, Longbing Cao, and Anton Van Den Hengel. Deep learning for anomaly detection: A review. _ACM computing surveys (CSUR)_, 54(2):1-38, 2021.
* [38] Guansong Pang, Anton van den Hengel, Chunhua Shen, and Longbing Cao. Toward deep supervised anomaly detection: Reinforcement learning from partially labeled anomaly data. In _Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining_, pages 1298-1308, 2021.
* [39] Yulong Pei, Tianjin Huang, Werner van Ipenburg, and Mykola Pechenizkiy. Resgcn: attention-based deep residual modeling for anomaly detection on attributed networks. _Machine Learning_, 111(2):519-541, 2022.
* [40] Zhen Peng, Wenbing Huang, Minnan Luo, Qinghua Zheng, Yu Rong, Tingyang Xu, and Junzhou Huang. Graph representation learning via graphical mutual information maximization. In _Proceedings of The Web Conference 2020_, pages 259-270, 2020.
* [41] Zhen Peng, Minnan Luo, Jundong Li, Huan Liu, and Qinghua Zheng. Anomalous: A joint modeling approach for anomaly detection on attributed networks. In _IJCAI_, pages 3513-3519, 2018.
* [42] Zhen Peng, Minnan Luo, Jundong Li, Luguo Xue, and Qinghua Zheng. A deep multi-view framework for anomaly detection on attributed networks. _IEEE Transactions on Knowledge and Data Engineering_, 34(6):2539-2552, 2020.
* [43] Bryan Perozzi and Leman Akoglu. Scalable anomaly ranking of attributed neighborhoods. In _Proceedings of the 2016 SIAM International Conference on Data Mining_, pages 207-215. SIAM, 2016.
* [44] Shebuti Rayana and Leman Akoglu. Collective opinion spam detection: Bridging review networks and metadata. In _Proceedings of the 21th acm sigkdd international conference on knowledge discovery and data mining_, pages 985-994, 2015.
* [45] Fengzhao Shi, Yanan Cao, Yanmin Shang, Yuchen Zhou, Chuan Zhou, and Jia Wu. H2-fdetector: a gnn-based fraud detector with homophilic and heterophilic connections. In _Proceedings of the ACM Web Conference 2022_, pages 1486-1494, 2022.
* [46] Jimeng Sun, Huiming Qu, Deepayan Chakrabarti, and Christos Faloutsos. Neighborhood formation and anomaly detection in bipartite graphs. In _Fifth IEEE International Conference on Data Mining (ICDM'05)_, pages 8-pp. IEEE, 2005.
* [47] Jianheng Tang, Jiajin Li, Ziqi Gao, and Jia Li. Rethinking graph neural networks for anomaly detection. _arXiv preprint arXiv:2205.15508_, 2022.
* [48] Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. Arnetminer: extraction and mining of academic social networks. In _Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 990-998, 2008.
* [49] Lei Tang and Huan Liu. Relational learning via latent social dimensions. In _Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 817-826, 2009.
* [50] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. _arXiv preprint arXiv:1710.10903_, 2017.
* [51] Petar Velickovic, William Fedus, William L Hamilton, Pietro Lio, Yoshua Bengio, and R Devon Hjelm. Deep graph infomax. _ICLR (Poster)_, 2(3):4, 2019.
* [52] Qizhou Wang, Guansong Pang, Mahsa Salehi, Wray Buntine, and Christopher Leckie. Cross-domain graph anomaly detection via anomaly-aware contrastive alignment. _arXiv preprint arXiv:2212.01096_, 2022.

* [53] Xuhong Wang, Baihong Jin, Ying Du, Ping Cui, Yingshui Tan, and Yupu Yang. One-class graph neural networks for anomaly detection in attributed networks. _Neural computing and applications_, 33(18):12073-12085, 2021.
* [54] Yanling Wang, Jing Zhang, Shasha Guo, Hongzhi Yin, Cuiping Li, and Hong Chen. Decoupling representation learning and classification for gnn-based anomaly detection. In _Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval_, pages 1239-1248, 2021.
* [55] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? _arXiv preprint arXiv:1810.00826_, 2018.
* [56] Zhiming Xu, Xiao Huang, Yue Zhao, Yushun Dong, and Jundong Li. Contrastive attributed network anomaly detection with data augmentation. In _Pacific-Asia Conference on Knowledge Discovery and Data Mining_, pages 444-457. Springer, 2022.
* [57] Yang Yang, Yuhong Xu, Yizhou Sun, Yuxiao Dong, Fei Wu, and Yueting Zhuang. Mining fraudsters and fraudulent strategies in large-scale mobile social networks. _IEEE Transactions on Knowledge and Data Engineering_, 33(1):169-179, 2019.
* [58] Ge Zhang, Jia Wu, Jian Yang, Amin Beheshti, Shan Xue, Chuan Zhou, and Quan Z Sheng. Fraudre: Fraud detection dual-resistant to graph inconsistency and imbalance. In _2021 IEEE International Conference on Data Mining (ICDM)_, pages 867-876. IEEE, 2021.
* [59] Jiaqiang Zhang, Senzhang Wang, and Songcan Chen. Reconstruction enhanced multi-view contrastive learning for anomaly detection on attributed networks. _arXiv preprint arXiv:2205.04816_, 2022.
* [60] Shijie Zhang, Hongzhi Yin, Tong Chen, Quoc Viet Nguyen Hung, Zi Huang, and Lizhen Cui. Gcn-based user representation learning for unifying robust recommendation and fraudster detection. In _Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval_, pages 689-698, 2020.
* [61] Tong Zhao, Chuchen Deng, Kaifeng Yu, Tianwen Jiang, Daheng Wang, and Meng Jiang. Error-bounded graph anomaly loss for gnns. In _Proceedings of the 29th ACM International Conference on Information & Knowledge Management_, pages 1873-1882, 2020.
* [62] Xin Zheng, Yixin Liu, Shirui Pan, Miao Zhang, Di Jin, and Philip S Yu. Graph neural networks for graphs with heterophily: A survey. _arXiv preprint arXiv:2202.07082_, 2022.
* [63] Yu Zheng, Ming Jin, Yixin Liu, Lianhua Chi, Khoa T Phan, and Yi-Ping Phoebe Chen. Generative and contrastive self-supervised learning for graph anomaly detection. _IEEE Transactions on Knowledge and Data Engineering_, 2021.
* [64] Shuang Zhou, Xiao Huang, Ninghao Liu, Qiaoyu Tan, and Fu-Lai Chung. Unseen anomaly detection on networks via multi-hypersphere learning. In _Proceedings of the 2022 SIAM International Conference on Data Mining (SDM)_, pages 262-270. SIAM, 2022.
* [65] Shuang Zhou, Qiaoyu Tan, Zhiming Xu, Xiao Huang, and Fu-lai Chung. Subtractive aggregation for attributed network anomaly detection. In _Proceedings of the 30th ACM International Conference on Information & Knowledge Management_, pages 3672-3676, 2021.
* [66] Jiong Zhu, Ryan A Rossi, Anup Rao, Tung Mai, Nedim Lipka, Nesreen K Ahmed, and Danai Koutra. Graph neural networks with heterophily. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 11168-11176, 2021.

One-Class Homophily Phenomenon

Fig. 5 shows the one-class homophily phenomenon on the rest of four datasets, in which ACM is a dataset with injected anomalies while the other three datasets contain real anomalies. The results here are consistent with that in the main text.

Given the graph along with the ground truth labels, following [13, 33], the homophily and heterophily are defined as the ratio of the edges connecting the node with the same classes and different classes, respectively:

\[\left\{\begin{array}{l}X_{\mathrm{hetero}}(v)=\frac{1}{|N(v)|}\left|\{u:u \in\mathcal{N}(v),y_{u}\neq y_{v}\}\right|\\ X_{\mathrm{homo}}(v)=\frac{1}{|N(v)|}\left|\{u:u\in\mathcal{N}(v),y_{u}=y_{v} \}\right|\end{array}\right.\] (11)

where \(y_{v}\) is the label to denote whether the node \(v\) is an anomaly or not. Note that one-class homophily may not always hold, or it is weak in some datasets. YelpChi which connects the reviews posted by the same user, _e.g._, YelpChi-RUR is an example of the latter case. As shown in Figure 5, the homophily distribution of normal and abnormal nodes is similar in YelpCh-RUR, whose pattern is different from the large distribution gap in the other three datasets. Since the normal nodes' homophily is generally stronger than the abnormal nodes, we can still successfully remove the non-homophily edges that connect normal and abnormal nodes with a higher probability than the homophily edges, resulting in a truncated graph with stronger one-class homophily.

## Appendix B Description of Datasets

We conduct the experiments on two real-world dataset with injected anomalies and four real-world with genuine anomalies in diverse online shopping services, social networks, and citation networks, including BlogCatalog [49], ACM [48], Amazon [10], Facebook [56], Reddit, YelpChi [21], as well as four large-scale graph datasets. The statistical information including the number of nodes, edge, the dimension of the feature, and the anomalies rate of the datasets can be found in Tab. 9.

Particularly, BlogCatalog is a social blog directory where users can follow each other. Each node represents a user, and each link indicates the following relationships between two users. The attributes of nodes are the tags that describe users and their blogs. ACM is a citation graph dataset where the nodes denote the published papers and the edge denotes the citations relationship between the papers. The attributes of each node are the content of the corresponding paper. BlogCatalog and ACM are popular GAD datasets where the anomalies are injected ones, including structural anomalies and contextual anomalies, which are created following the prior work [28]. Amazon is a graph dataset capturing the relations between users and product reviews. Following [10, 60], three different user-user graph datasets are derived from Amazon using different adjacency matrix construction approaches. In this work, we focus on the Amazon-UPU dataset that connects the users who give reviews to at least one same product. The users with less than 20% are treated as anomalies. Facebook [56] is a social network where users build relationships with others and share their same friends. Reddit is a network of forum posts from the social media Reddit, in which the user who has been banned from the platform is annotated as an anomaly. Their post texts were converted to the vector as their attribute. YelpChi includes hotel and restaurant reviews filtered (spam) and recommended (legitimate) by Yelp. Following [35, 44], three different graph datasets derived from Yelp using different connections in user, product review text, and time. In this work, we only use YelpChi-RUR which connects reviews posted by the same user. Note that considering it's difficult to conduct an evaluation on the isolated nodes in the graph, they were removed before modeling.

Amazon-all and YelpChi-all [10] are two datasets by treating the different relations as a single relation following [6]. T-Finance [47] is a transaction network. The nodes represent the unique account and the edges represent there are records between two accounts. The attributes of nodes are the features related to registration days, logging activities, and interaction frequency. The anomalies are fraud, money laundering and online gambling which are annotated manually. OGB-Protein [14] is a biological network where node represents proteins and edge indicates the meaningful association between proteins.

Figure 5: Homophily distribution of normal nodes and abnormal nodes on the rest of eight datasets

## Appendix C Additional Experimental Results

### Hyperparameter Analysis

This section analyzes the sensitivity of TAM w.r.t. two key hyperparameters, including the regularization hyperparameter \(\lambda\) and the ensemble parameters \(T\). The results on \(\lambda\) and \(T\) are shown in Fig. 6 and Fig. 7, respectively.

**Ensemble Hyperparameter \(T\)**. As shown in Fig. 6 with increasing \(T\), TAM generally performs better and becomes stable around \(T\approx 4\). This is mainly because the use of more ensemble models on truncated graphs reduces the impact of the randomness of truncation and increases the probability of weakening the affinity of abnormal nodes to its neighbors, and this effect would diminish when \(T\) is sufficiently large. The average of local affinity from multiple truncated graph sets is more conducive to anomaly detection.

**Regularization Hyperparameter \(\lambda\)**. In order to evaluate the effectiveness of \(\lambda\), we adopt different values to adjust the weight of the regularization. From Fig. 7, we can see that for BlogCatalog and Facebook, adding the regularization improves the effectiveness of the model by a large margin. This is mainly because the use of regularization can prevent all nodes from having identical feature representations. For most real-world datasets with genuine anomalies, the regularization does not significantly improve the effectiveness of the model while decreasing the performance as the increasing of \(\lambda\). The main reason is that Amazon, Reddit, and YelpChi are real-world datasets with diverse attributes, and the role of regularization term is not reflected during affinity maximization.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Data set & Type & R/I & Nodes & Edges & Attributes & Anomalies(Rate) \\ \hline BlogCatalog & Social Networks & I & 5,196 & 171,743 & 8,189 & 300(5.77\%) \\ ACM & Citation Networks & I & 16,484 & 71,980 & 8,337 & 597(3.63\%) \\ Amazon & Co-review & R & 10244 & 175,608 & 25 & 693(6.66\%) \\ Facebook & Social Networks & R & 4,039 & 88,234 & 576 & 27(0.67\%) \\ Reddit & Social Networks & R & 10,984 & 175,608 & 64 & 366(3.33\%) \\ YelpChi & Co-review & R & 24,741 & 49,315 & 32 & 1,217(4.91\%) \\ Amazon-all & Co-review & R & 11,944 & 4,398,392 & 25 & 821(6.87\%) \\ YelpChi-all & Co-review & R & 45,941 & 3,846,979 & 32 & 6,674(14.52\%) \\ T-finance & Transaction Record & R & 39,357 & 21,222,543 & 10 & 1,803(4.58\%) \\ OGB-Protein & Biology Network & I & 132,534 & 39,561,252 & 8 & 6000(4.5\%) \\ \hline \hline \end{tabular}
\end{table}
Table 9: Key statistics of the datasets. The real-world datasets with Injected/Real anomalies(I/R).

Figure 6: AUROC and AUPRC results w.r.t. ensemble parameter \(T\)

### Complexity Analysis

This subsection analyzes the time complexity of TAM. Specifically, the distance calculation takes \(\mathcal{O}(md_{0})\), \(m\) is the number of non-zero elements in the adjacent matrix \(\mathbf{A}\), and \(d_{0}\) is the dimension of attributes for each node. The graph truncation in TAM takes \(2N\eta\), where \(N\) is the number of nodes and \(\eta\) is the average degree in the graph. In LAMNet, we build a GCN for each truncated graph, which takes \(\mathcal{O}(md_{1}h)\) using sparse-dense matrix multiplications, where \(h\) and \(d_{1}\) denotes summation of all feature maps across different layer and feature dimensions in graph convolution operation, respectively. The construction of a GCN takes \(\mathcal{O}(md_{1}h)\). LAMNet also needs to compute all connected pairwise similarities, which takes \(\mathcal{O}(N^{2})\). Thus, the overall complexity of TAM is \(\mathcal{O}(md_{0}+(N^{2}+md_{1}h+2N\eta)KT)\), where \(K\) is the truncation depth and \(T\) is the ensemble parameter. The complexity is lower than the time complexity in many existing GNN-based graph anomaly detection methods based on the subgraph sampling and hop counting [17, 63].

### Anomaly Scoring

**AUROC Results of TAM and Its Variants.** We present the AUPRC results of TAM and its two variants, **Degree** and **TAM-T** in the main text, where TAM can consistently and significantly outperform both variants. Similar observation can also be found from the AUROC results in Fig. 8(a). Fig. 8(b) shows the AUROC results of anomaly scoring by aggregating the anomaly scores under all truncation scales/depths. Similar to the AUPRC results in the main text, the anomaly scores obtained from different truncation scales/depths can largely improve the detection performance and the results become stable with increasing graph truncation depth \(K\).

**Anomaly Scoring Using Multi-scale Truncation vs Single-scale Truncation.** Fig. 9 shows the results of TAM that performs anomaly scoring based on a single-scale graph truncation rather than the default multi-scale graph truncation. As shown in Fig. 9, the increase of \(K\) improves the performance on large datasets such as the Amazon datasets, but it often downgrades the performance on the datasets such as Reddit and YelpChi. The main reason is that the node attributes in these datasets are more similar than the other datasets, restricting the effect of graph truncation. However, the opposite case can occur on the other datasets. To tackle this issue, we define the overall anomaly score as an average score over the anomaly scores obtained from the LAMNets built using truncated graphs under all truncation depths/scales. This resulting multi-scale anomaly score, as shown in Fig. 8(b), performs much more stably than the single-scale anomaly score.

### Shared-weights LAMNet vs. Non-shared-weights LAMNet

In our experiments, the weight parameters in LAMNets are independent from each other by default, i.e., GNNs in LAMNets are independently trained. In this section, we compare TAM with its variant **TAM-S** where all LAMNets use a single GNN backbone with shared weight parameter.

Figure 7: AUROC and AUPRC results w.r.t. regularization hyperparameter \(\lambda\)The results are shown in Tab. 10. It is clear that TAM performs consistently better than, or comparably well to, TAM-S across the six datasets.

### Simple Similarity-based Non-homophily Edge Removal

Our ablation study in the main text has shown the inferior performance of _SimilarityCut_ method compared to our proposed NSGT, where we directly remove 5% least similar edges. Tab 11 shows the results of removing 10% and 30% least similar edges. It is clear that NSGT outperforms these two other cases of _SimilarityCut_.

### ROC and Precision-Recall Curves

The ROC curve and Precision-Recall curve of TAM on the six datasets are shown in Fig. 10.

Figure 8: AUROC results of **(a)** TAM vs. Degree and TAM-T, and **(b)** TAM w.r.t. graph truncation depth \(K\).

Figure 9: AUROC and AUPRC results of TAM using single-truncation scale-based anomaly scores

## Appendix D Description of algorithms

### Competing Methods

* iForest [25] builds multiple trees to isolate the data based on the node's feature. It has been widely used in outlier detection.
* ANOMALOUS [41] proposes a joint network to conduct the selection of attributes in the CUR decomposition and residual analysis. It can avoid the adverse effects brought by noise.
* DOMINANT [8] leverages the auto-encoder for graph anomaly detection. It consists of an encoder layer and a decoder layer which construct the feature and structure of the graph. The reconstruction errors from the feature and structural module are combined as the anomaly score.
* HCM-A [17] constructs an anomaly indicator by estimating hop count based on both global and local contextual information. It also employs Bayesian learning in predicting the shortest path between node pairs.
* CoLA [28] exploits the local information in a contrastive self-supervised framework. They define the positive pair and negative pair for a target node. The anomaly score is defined as the difference value between its negative and positive score.
* SL-GAD [63] constructs two modules including generative attribute regression and multi-view contrastive for anomaly detection based on CoLA. The anomaly score is generated from the degree of mismatch between the constructed and original features and the discrimination scores.
* ComGA [31] designs a tailor GCN to learn distinguishable node representations by explicitly capturing community structure.

Their implementation is taken directly from their official web pages or the widely-used PyGOD library [26]. The links to the source code pages are as follows:

* iForest: https://github.com/pygod-team/pygod
* ANOMALOUS: https://github.com/pygod-team/pygod
* DOMIANT: https://github.com/kaize0409/GCN_AnomalyDetection_pytorch

\begin{table}
\begin{tabular}{c|c|c c c c c} \hline \hline \multirow{2}{*}{**Metric**} & \multirow{2}{*}{**Method**} & \multicolumn{5}{c}{**Dataset**} \\  & & BlogCatalog & ACM & Amazon & Facebook & Reddit & YelpChi \\ \hline \multirow{3}{*}{AUROC} & TAM-S & 0.8170\(\pm\)0.002 & 0.8826\(\pm\)0.003 & 0.7044\(\pm\)0.008 & **0.9165\(\pm\)**0.005 & 0.6008\(\pm\)0.002 & 0.5407\(\pm\)0.008 \\  & TAM & **0.8248\(\pm\)**0.003 & **0.8878\(\pm\)0.024** & **0.7064\(\pm\)**0.010 & 0.9144\(\pm\)0.008 & **0.6023\(\pm\)**0.004 & **0.5643\(\pm\)**0.007 \\ \hline \multirow{3}{*}{AUPRC} & TAM-S & 0.3908\(\pm\)0.002 & 0.4960\(\pm\)0.001 & 0.2597\(\pm\)0.002 & 0.2087\(\pm\)0.006 & **0.0459\(\pm\)**0.003 & 0.0691\(\pm\)0.002 \\  & TAM & **0.4182\(\pm\)**0.005 & **0.5124\(\pm\)**0.018 & **0.2634\(\pm\)**0.008 & **0.2233\(\pm\)**0.016 & 0.0446\(\pm\)0.001 & **0.0778\(\pm\)**0.009 \\ \hline \hline \end{tabular}
\end{table}
Table 10: AUROC and AUPRC results of TAM using shared-weight LAMNets (TAM-S) vs. non-shared-weight LAMNets (TAM).

\begin{table}
\begin{tabular}{c|c|c c c c} \hline \hline \multirow{2}{*}{**Metric**} & \multirow{2}{*}{**Method**} & \multicolumn{5}{c}{**Dataset**} \\  & & \(\theta\)=0.05 & \(\theta\)=0.1 & \(\theta\) = 0.3 & TAM \\ \hline \multirow{4}{*}{AUROC} & BlogCatalog & 0.6650 & 0.6526 & 0.6583 & **0.8210** \\  & ACM & 0.8668 & 0.7986 & 0.6911 & **0.8878** \\  & Amazon & 0.5856 & 0.5827 & 0.6106 & **0.7064** \\  & Facebook & 0.6951 & 0.7293 & 0.7934 & **0.9144** \\  & Reddit & 0.6007 & 0.5945 & 0.5758 & **0.6028** \\  & YelpChi & 0.4910 & 0.4872 & 0.4754 & **0.5674** \\ \hline \hline \multirow{4}{*}{AUPRC} & BlogCatalog & 0.1621 & 0.1829 & 0.1729 & **0.4152** \\  & ACM & 0.5109 & 0.5068 & 0.4996 & **0.5124** \\ \cline{1-1}  & Amazon & 0.0924 & 0.1092 & 0.2079 & **0.2634** \\ \cline{1-1}  & Facebook & 0.0410 & 0.1154 & 0.1374 & **0.2233** \\ \cline{1-1}  & Reddit & **0.0467** & 0.0414 & 0.0420 & 0.0446 \\ \cline{1-1}  & YelpChi & 0.0598 & 0.0509 & 0.0519 & **0.0771** \\ \hline \hline \end{tabular}
\end{table}
Table 11: Performance under different ratio of similarity-based edge removal. \(\theta\) represents the edge-cut ratio based on similarity.

* HCM-A: https://github.com/TienjinHuang/GraphAnomalyDetection
* CoLA: https://github.com/GRAND-Lab/CoLA:
* SL-GAD: https://github.com/yixinliu233/SL-GAD
* ComGA:https://github.com/XuexiongLuoMQ/ComGA

### Pseudo Codes of TAM.

The training algorithms of TAM are summarized in Algorithm 1 and Algorithm 2. Algorithm 1 describes the process of NSGT. Algorithm 2 describes the training process of TAM.

Figure 10: The ROC curve and precision-recall curve of our method TAM on all the six datasets used. (a)(b) BlogCatalog; (c)(d) ACM; (e)(f) Amazon; (h)(i) Facebook; (j)(k) Reddit; (l)(m) YelpChi

```
1:Input: Attributed Graph, \(G\)\(=(\mathcal{V},\mathcal{E})\), Distance Matrix, \(\mathbf{M}\)
2:Output: A truncated graph structure \(\tilde{\mathcal{E}}\).
3:
4: /* Regard the graph as a direct graph */
5: Initialize the directed graph truncation indicator \(e_{(v_{i},v_{j})}=1\) iff \((v_{i},v_{j})\in\mathcal{E}\)
6: Find the mean \(d_{mean}\) from \(\mathbf{M}\) using \(d_{mean}=\frac{1}{m}\sum_{(v_{i},v_{j})\in\mathcal{E}}d_{ij}\)
7:for each \(v\) in \(\mathcal{V}\)do
8: Find the maximum \(d_{v,\max}\) from \(\{d(v,v^{\prime})\), \((v,v^{\prime})\in\mathcal{E}\}\)
9:if\(d_{v,\max}>d_{mean}\)then
10: Randomly sample \(r\) from [\(d_{mean}\), \(d_{v,\max}\)] for node \(v\)
11:for each \(v^{\prime}\) in \(\{v^{\prime},(v,v^{\prime})\in\mathcal{E}\}\)do
12:if\(d(v,v^{\prime})>r\)then
13: Plan to cut the edge \(v\) to \(v^{\prime}\), i.e., \(e_{(v,v^{\prime})}\gets 0\)
14:endif
15:endfor
16:endif
17:endfor /* The edge will be removed only when it is a non-homophily edge from both directions of the connected nodes */
18:for each \(v\) in \(\mathcal{V}\)do
19:for each \(v^{\prime}\) in \(\{v^{\prime},(v,v^{\prime})\in\mathcal{E}\}\)do
20: Cut the edge between \(v\) and \(v^{\prime}\), \(\tilde{\mathcal{E}}=\mathcal{E}\backslash\left((v,v^{\prime})\cup(v^{\prime},v )\right);\) iff \(e_{(v,v^{\prime})}=e_{(v^{\prime},v)}=0\)
21:endfor
22:endfor
23:return The truncated graph structure \(\tilde{\mathcal{E}}\) ```

**Algorithm 1** NSGT```
1:Input: Graph, \(G{=}(\mathcal{V},\mathcal{E},\mathbf{X})\), \(N\): Number of nodes, \(L\): Number of layers, \(E\): Training epochs, \(T\): Ensemble parameter, \(K\): Truncation depth. Output: Anomaly scores of all nodes \(s\).
2:Compute the Euclidean distance \(\mathbf{M}=\{d_{ij}\}\) for each connected node pair \((v_{i},v_{j})\in\mathcal{E}\)
3:Randomly initialize GNN \((\mathbf{h}_{1}^{(0)},\mathbf{h}_{2}^{(0)},...,\mathbf{h}_{N}^{(0)})\leftarrow \mathbf{X}\), \(\mathcal{E}^{(1,0)},,...,\mathcal{E}^{(T,0)}\leftarrow\mathcal{E}\)
4:for\(k=1,\cdots,K\)do
5:for\(t=1,\cdots,T\)do
6: /* Graph truncation and update the graph structure */
7:\(\mathcal{E}^{(t,k)}=\) NSGT\((\mathcal{V},\mathcal{E}^{(t,k-1)},\mathbf{M})\) /* LAMNet */
8:for\(epoch=1,\cdots,E\)do
9:for each \(v\) in \(\mathcal{V}\)do
10:for\(l=1,\cdots,L\)do
11:\(\mathbf{h}_{v,l}=\phi(\mathbf{h}_{v,l-1};\Theta_{t,k})\)
12:\(\mathbf{h}_{v,l}=\mathrm{ReLU}\left(\mathrm{AGG}(\{\mathbf{h}_{v^{\prime},l}:(v, v^{\prime})\in\mathcal{E}^{(t,k)}\})\right)\)
13:endfor
14: Calculate \(f_{\mathit{TAM}}(v_{i};\Theta_{t,k},\mathbf{A},\mathbf{X})\) by Eq. (2).
15:endfor
16:/* Affinity maximization */
17:\(\text{Update }\mathbf{\Theta}_{t,k}\) by using stochastic gradient descent
18:endfor
19:endfor
20:endfor /* Aggregated anomaly score over \(T\) sets of multi-scale graph truncation depths */
21:return Anomaly Score by \(s\left(v\right)=\frac{1}{T\times K}\sum\limits_{k=1}^{K}\sum\limits_{t=1}^{T}f _{\mathit{TAM}}(v_{i};\Theta_{t,k}^{*},\mathbf{A},\mathbf{X}))\) ```

**Algorithm 2** TAM