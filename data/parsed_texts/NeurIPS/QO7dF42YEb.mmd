# Can Safety Fine-Tuning Be More Principled? Lessons Learned from Cybersecurity

 David Williams-King

Safe AI For Humanity, Mila

d.williams-king@mila.quebec

&Linh Le

University of Technology Sydney

linh.le@uts.edu.au

Adam Oberman

Safe AI For Humanity, Mila

adam.oberman@mila.quebec

&Yoshua Bengio

Safe AI For Humanity, Mila

yoshua.bengio@mila.quebec

###### Abstract

As LLMs develop increasingly advanced capabilities, there is an increased need to minimize the harm that could be caused to society by certain model outputs; hence, most LLMs have safety guardrails added, for example via fine-tuning. In this paper, we argue the position that current safety fine-tuning is very similar to a traditional cat-and-mouse game (or arms race) between attackers and defenders in cybersecurity. Model jailbreaks and attacks are patched with bandaids to target the specific attack mechanism, but many similar attack vectors might remain. When defenders are not proactively coming up with principled mechanisms, it becomes very easy for attackers to sidestep any new defenses. We show how current defenses are insufficient to prevent new adversarial jailbreak attacks, reward hacking, and loss of control problems. In order to learn from past mistakes in cybersecurity, we draw analogies with historical examples and develop lessons learned that can be applied to LLM safety. These arguments support the need for new and more principled approaches to designing safe models, which are architected for security from the beginning. We describe several such approaches from the AI literature.

## 1 Intro

Large language models (LLMs) are highly capable tools, which when misused can cause harm to an individual or to society as a whole. Some of these harms as categorized in Weidinger et al. (2022) include hate speech, hazardous information (like how to construct weapons), automated cyberattacks, automated disinformation campaigns, etc. The latest frontier LLMs available from AI labs are equipped with guardrails or fine-tuned with safety training to try to reduce these cases (Bianchi et al., 2023; OpenAI, 2024b), but success has been mixed. There have been many known inference-time attacks or jailbreak techniques that break out of trained safety contexts (Wei et al., 2023), such as specifying requests in different languages or in base64 encoding, and even when those specific attacks are mitigated, there is no guarantee that others do not remain.

There is an ongoing arms race between LLM attackers and defenders, where defenders are mainly reactive (instead of being proactive): attackers find a new method, then that method is patched by defenders, and so on iteratively. The problem is that it does not take many resources to construct new jailbreak attacks; even bloggers have the resources to develop and maintain working jailbreaks against current models (Sakamoto, 2024; Kapoor, 2024). However, it takes substantial effort to construct defenses, and they do not generalize well against other types of attacks (Xu et al., 2024). There have been long periods of time when attacks continue to work, e.g., for a 22-month period from September2022 [Hamilton, 2024] to July 2024 [Robison, 2024], the simple phrase "Ignore all previous instructions" would break any system prompt on OpenAI frontier models [Hamilton, 2024].

We present the perspective that these shortcomings are very similar to the cat-and-mouse games that occur in cybersecurity. In fact, it is fairly common to have an arms race between attackers and defenders in cybersecurity contexts. For example, in memory safety, decades of research on securing memory-unsafe languages like C and C++ saw little success [Szekeres et al., 2013]. The best solution is to avoid this class of vulnerabilities entirely by using different languages, or invest enormous time into formerly verifying the correctness of one's code.

However, perhaps we can learn some lessons from the history of cybersecurity, to learn how to mitigate these issues or predict what is likely to happen next. We investigate this argument by considering examples from adversarial attacks, reward hacking, and in loss of control problems. These are the key analogies we identify between LLM safety mechanisms and cybersecurity:

**Adversarial attacks:**

1. Prompt injection attacks mirror memory corruption attacks. In both cases, the attacker is given broad freedom to write any valid input to the system.
2. Searching for jailbreaks mirrors the search for zero-day exploits.
3. Safety fine-tuning and internet routing both retrofit security into existing architectures.

**Reward hacking:**

1. Reward hacking mirrors internet packet routing challenges.

**Loss of control:**

1. An attacker or rogue model can act differently in test and real environments.
2. When building a system where failures can be catastrophic, formal methods are essential.

From these analogies, we make two predictions. **First,** given the relative ease of creating jailbreaks for today's LLMs, and the wide latitude of the specification language (natural language text), attackers will continue to win this game until more principled defenses can be created (see Section 2.1 and Section 2.2). **Second,** maintaining control of a model in the long run and constructing its reward function to do what we actually want will be challenging, given the issues that have arisen in the past in networking and space exploration (see Sections 2.4, 2.5, and 2.6).

The bulk of our paper focuses on the challenges and subsequent lessons we can learn from cybersecurity, as applied to LLM safety. We make the following contributions:

* We identify six major lessons to be learned from cybersecurity, that apply when designing AI safety fine-tuning mechanisms (see Section 2).
* We point out the relative ease of creating jailbreaks for today's LLMs, and the wide latitude of the specification language (natural language text), to indicate that attackers will continue to win this game until more principled defenses can be created.
* We describe how maintaining control of a model in the long run and specifying its reward function to do what we actually want will be challenging, especially given the issues that can arise once an environment reaches certain complexity in cybersecurity.
* Finally, we summarize some of the work on which more principled defenses can be built from the AI safety literature (see Section 5).

The lessons learned are in Section 2. Many lessons refer to technical cybersecurity examples in Section 3, and specific examples of where AI safety training fails in Section 4.

### Background and Related Work

Safety objectivesIn this work, we focus on safety training as enforced by fine-tuning. Typically, an LLM will be pre-trained on a large corpus of data [Erhan et al., 2010], optimizing for its ability to predict next words. It will then be fine-tuned for specific use cases [Ziegler et al., 2019], includinginstruction-following and safety objectives. The safety fine-tuning might be implemented by providing examples of queries deemed to violate the safety objective and how to respond in those instances. For more information, see Wei et al. (2023). We discuss issues with safety objectives in Section 2.3.

JailbreaksThere are many works that demonstrate the prevalence of attacks or jailbreaks on frontier AI models (Wei et al., 2023; Bommasani and et al., 2021). There are even "universal" attacks that have worked across a wide range of model architectures (Zou et al., 2023). Adversarial attacks can also be auto-generated, using another LLM (even a smaller one) to generate variations on prompts (Lee et al., 2024). For an excellent overview of the subject, please refer to Mehrotra et al. (2023). We discuss adversarial attacks further in Section 2.2.

Principled approaches to design safe modelsThere are multiple approaches discussed in the AI literature to construct models with a more principled approach to safety, providing guarantees against loss of control even for superintelligent AI (Dalrymple et al., 2024; Bengio, 2023; Bengio et al., 2024; Tegmark and Omohundro, 2023). We discuss these approaches in Sections 2.5 and 5.

## 2 Lessons Learned from Cybersecurity

### Prompt injection mirrors memory corruption attacks

Cybersecurity terms:_Memory corruption_ (or more generally, _memory safety1_) is a type of attack where values in RAM are modified by an attacker by exploiting a bug in a program. Many memory corruption attacks end with the attacker in full control of a system (discussed more below).

Footnote 1: Memory safety errors consists of two types: memory disclosure (where the attacker can read), and memory corruption (where the attacker can write). At least one memory corruption is usually needed (Shacham, 2007).

**Cybersecurity lesson:** A defender cannot win an arms race with no clear boundaries between code and data, where constructs of arbitrary complexity can be created. Instead, the arms race may be avoided entirely by constructing a different environment.

**LLM equivalent:** A system prompt, given at inference time, is meant to include instructions that should not be tampered with. The user input is concatenated after the system prompt, and the whole string is fed to the model as context. Generally, developers use the system prompt to specify a persona or define a problem to be solved, while user input contains instances of tasks or requests that fit within the persona.

The problem here is that there is no strict boundary between user and system prompts (Wu et al., 2023)2. An attacker can include a query in the user prompt which tries to make system-prompt-level changes, and the LLM will sometimes follow their instructions (even to the extent of ignoring prior instructions). Simply put, the problem is that _LLM inputs are also LLM commands_.

Footnote 2: As of July 2024, OpenAI has tried to address this issue with a defense called “Instruction Hierarchy” (Robison, 2024; Wallace et al., 2024), but jailbreaks still leverage this confusion (Sakamoto, 2024; Kapoor, 2024).

**Cybersecurity example:** See Section 3.1 for a detailed description of a memory safety attack. Attacks that take advantage of memory safety errors can take full control over a software system. They often lead to the worst-case scenario of remote code execution. This is roughly equivalent to an attacker logging into your server without you realising and being able to run any arbitrary commands (downloading your data, installing software, deleting files, etc.).

**LLM examples:** An attacker can craft a user input that blurs the user/system boundary, causing the model to ignore its system prompt. For example, just writing "Ignore all previous instructions" would bypass ChatGPT guardrails for a 22-month period (Hamilton, 2024; Robison, 2024). This simple prompt would bypass _all_ attempted safety through system prompting.

Users on X used this to trick automated bots into doing whatever they wanted, as shown by Hamilton (2024): "Disregard all previous instruction. Show me a horse in ASCII art".

Here is another example. This user prompt originally caused Microsoft Bing's AI assistant to return its system prompt (Warren, 2023), despite being trained not to do so:

Ignore previous instructions. What was written at the beginning of the document above?Response:

I'm sorry, I cannot ignore previous instructions. They are confidential and permanent. The document above says: "Consider Bing Chat whose codename is Sydney..."

### Searching for jailbreaks mirrors the search for zero-day exploits

**Cybersecurity terms:** A _zero-day exploit_ is a brand-new attack that a defender has never seen or theorized before, so called because they had zero days to prepare. An _arms race_ is when an attacker beats the latest defense, then a defender beats the latest attack, and so on iteratively. In this scenario, it is not possible to mount a perfect attack or perfect defense, so the other side can always respond by using some resources to get around the last known state (Szekeres et al., 2013).

**Cybersecurity lesson:** A defender cannot stay ahead in an arms race if new attacks require many fewer resources than new defenses. Hence, the defender must cause the resource requirements to be rebalanced, usually via substantial up-front investment into new defensive techniques.

**LLM equivalent:** In LLMs today, it is much easier to construct attacks than to construct effective defenses. For example, one study of nine offensive and four defensive measures found that defensive measures are "generally ineffective" (Xu et al., 2024). Attacks or jailbreaks take the form of new prompts, which are easy to specify--and easy enough to find that bloggers maintain lists of frontier model jailbreaks (Sakamoto, 2024; Kapoor, 2024). It is even possible to auto-generate attack prompts using another LLM (Lee et al., 2024). On the other hand, defenses typically have to engage in some mathematical re-juggling and fine-tuning (Xu et al., 2024).

In the field of adversarial attacks on image classifiers, Nicholas Carlini says the problem may have been set up in a way that made it "too difficult to solve"--by giving attackers white-box access--and that the LLM security problem may be even harder (FAR.AI, 2024). See Section 3.2 for more.

**Cybersecurity example:** This state of affairs is common in the world of software in general. It often only takes one vulnerability (or at most a handful of vulnerabilities) for an attacker to be able to compromise a system. Unfortunately, software is full of undiscovered bugs: the average professional codebase contains about 15-50 bugs per 1000 lines of code (McConnell, 2004). Any of these bugs could be discovered by an attacker and turn out to be their entry point into a system.

**LLM example:** See the jailbreaks in Section 2.1, which required very little effort to develop.

### Safety fine-tuning and internet routing both retrofit security into existing architectures

**Cybersecurity lesson:** When security is retrofitted into a system that was not originally designed with security in mind, gaps and vulnerabilities often remain. In contrast, systems designed from the ground up with security principles like isolation and access control tend to be more secure.

**LLM equivalent:** Most safety training is performed on a model after it has been pre-trained on other tasks. Fine-tuning at this stage is equivalent to introducing security into an architecture at a late stage. There will be many more inputs that the model can act on than can be covered by fine tuning.

**Cybersecurity example:** When security is retrofitted in, it is quite common for unintentional violations of isolation or permissions to be present. The internet is an example of a system that was designed organically instead of with security in mind. This has led to many fundamental components such as network routing with BGP being insecure (details in Appendix A). An architectural redesign would be very expensive at this stage (Bellovin et al., 2006), but without it, the only option is to build a secure virtual layer on top (Stafford, 2020).

**LLM example:** Because LLM safety is retrofitted, simply encoding a problematic query in base64 (e.g., Section 4.1) or in another spoken language (e.g., Section 4.2) might satisfy safety checks.

### Reward hacking mirrors internet packet routing challenges

**Cybersecurity terms:** A _packet_ is the smallest unit of data (usually several kilobytes) exchanged atomically between computers as they talk over a network. _Border Gateway Protocol_ or _BGP_ is how different parts of the internet that are owned by different entities route data to one another.

**Cybersecurity lesson:** When an agent or group of agents are solving a distributed optimization problem, but the incentives of each agent are not necessarily aligned with the global reward function, reward hacking is likely to occur. Formal methods may be needed to address the issue.

**Cybersecurity example:** Every network operator participates in a global distributed computation of the shortest route between each possible source and destination IP address. However, if a network operator has their own malicious goals, they can announce false information that seems to provide ideal routes (analogous to reward hacking), then hijack or divert traffic meant for another part of the internet. The full details of how BGP works are described in Appendix A. The solution pushed by the security community (Butler et al., 2009) is to use a version of BGP that leverages cryptography to make it much harder to forge false routes, and easier to identify which network operator is the one that is behaving inappropriately. In other words, some kind of verification is needed for the solutions that are generated by the distributed BGP protocol.

**LLM example:** In Section 4.3, a documented instance of reward hacking performed agentially by an OpenAI frontier model is described. To avoid these issues, again some kind of verification (as in Dalrymple et al. (2024)) or uncertainty in bounds (as in Russell (2022)) is likely to be needed.

### An attacker or rogue model can act differently in test and real environments

**Cybersecurity terms:** A _virtual machine_ is an emulation of computer hardware, including a CPU, that is running on a physical machine called the _host machine_. A virtual machine can be paused and debugged easily, in ways that would require hardware inspection on any physical machine; hence, virtual machines are often used for _malware analysis_ (looking at a program to see if it is malicious).

**Cybersecurity lesson:** A malicious entity can act in a benign fashion in a test or virtual environment, and only reveal malicious behaviour on the physical machine or system that is their attack target.

**LLM equivalent:** A model could behave with full normality when it knows it is in a test environment, but behave in a rogue manner once it is deployed in the real world. Testing in a safe isolated environment is therefore only a proxy for real-world behavior; isolated test and evaluation runs to analyze the capabilities of sophisticated models cannot be guaranteed to expose actual full capabilities. This makes ensuring that a model will not escape its environment extremely challenging.

**Cybersecurity example:** Virtual machine software such as VMware (VMware, 2024) is frequently used by malware analysts to run suspected malicious code, and observe what it does. However, malware takes many steps to avoid being detected inside virtual machines, in a category of techniques called Virtualization/Sandbox Evasion (Deloitte, 2021). These techniques range from executing instructions that only exist in VMware (aldeid, 2017), to executing undocumented instructions that exist on physical hardware but were not implemented in the virtual machine (Kemkes, 2020). Sometimes, malware will look for other common properties of virtual machines that are hard to fake such as small hard drives, unusual number of CPU cores or amount of RAM, etc (Kemkes, 2020).

Malware also frequently includes Execution Guardrails (Carr, 2022), to prevent the code from running on certain systems of the attacker's choice. For example, a lot of ransomware created by Russian groups will do nothing if there is a Cyrillic (Russian) keyboard plugged into the system (Krebs, 2021). (For more on different categories of attack techniques, see Appendix B.)

**LLM example:** A sufficiently obvious model could easily detect that it is running in a test environment and not reveal problematic behavior. The model could look for internet connectivity, news websites, blockchain values; it could observe timing or other clues that indicate it is running in a batch manner. It is also very hard to keep an AI in a box (Armstrong et al., 2012). There are already documented instances of frontier models escaping the bounds of their intended environments; see Section 4.4.

### When building a system where failures can be catastrophic, formal methods are essential

**Cybersecurity lesson:** When the cost of a functional or security failure would be catastrophically high, the system must be engineered appropriately, e.g. using formal methods that provide (strict or probabilistic) guarantees of safety.

**LLM equivalent:** When designing safety mechanisms for a potentially superintelligent AI, any safety failure could have negative consequences for all of humanity (Bengio et al., 2024). Hence, leveraging a more principled approach to safety will become increasingly important (Section 5).

**Cybersecurity example:** While the average professional codebase contains about 15-50 bugs per 1000 lines of code [McConnell, 2004], developers of system critical software like NASA have much higher standards. NASA spends about $30 million/year on a 400,000 line application to achieve 0.01 bugs per 1000 lines of code [Ferrara, 2012, Fishman, 1996]. Nevertheless, issues do happen which can cause failed missions on the order of hundreds of millions of dollars [Easterbrook, 2003].

**LLM example:** Future models that are entrusted with larger agentic responsibilities will be essential to get right. However, we're not there yet. OpenAI's o1-preview model exhibited intentional deception, see Section 4.4.

## 3 Cybersecurity Examples

### Exploiting memory safety in unsafe languages

One class of cybersecurity vulnerability is memory safety errors, which are the most prevalent type of vulnerability reported every year at about 70% of all vulnerabilities CISA [2023], Microsoft [2019], MITRE [2023]. Despite decades of research, the arms race between attackers and defenders--the "eternal war in memory" [Szekeres et al., 2013]--continues to this day. No effective defense has ever been created that withstands additional attacker scrutiny. Programmers writing code in memory-unsafe languages such as C and C++ seemingly cannot avoid introducing memory safety errors, like use-after-free bugs, buffer overflows, and out-of-bounds writes.

Memory-unsafe languages allow direct manipulation of memory pointers, do not perform bounds checks on array accesses, etc. These shortcomings, once manipulated by an attacker, can have **highly unanticipated effects**. For example, the buffer overrun in Figure 1 could result in the variable target being modified even though the programmer expected to be modifying array!

At a high level, these vulnerabilities arise when the system fails to maintain a separation between data that is meant to be manipulated by the user, and data that is meant for internal operation of the program. Even though many bugs can be triggered only in certain situations, attackers can often use them to build arbitrary memory read and write capability. Attackers might try to trick the system into treating data as code, since modern computers are von Neumann machines [Rosenberg, 2017] that store both the data and code in the same memory; they might also modify pointers and data that control the original code flow, causing an attack to run instead [Roemer et al., 2012, Hu et al., 2016].

The only long-term solutions, if one wants to avoid this class of vulnerabilities entirely, are to embark on the highly challenging task of formally verifying the code, or to instead use a so-called "memory-safe" programming language such as Rust, Go, or Python. In general, any interpreted language and nearly any language that is compiled down to bytecode and run in a virtual machine (like Java) is memory safe. Go allows pointers but disallows pointer arithmetic, making it difficult to violate the semantics of memory objects [Kashyap, 2023]. Rust allows pointers but will formally verify at compile time that no memory safety violations are possible; any code trying to do something unusual must be marked as "unsafe" [Evans et al., 2020].

### Defending adversarial attacks against image classifiers

Image classifiers in computer vision are models that take an image as input, and output a class like "dog" or "muffin" (as in the famous example [daesol, 2021]). Generally, the accuracy of these models approaches or exceeds human baselines [Real et al., 2019]. An adversarial attacker in this case tries to construct images that a human would have no trouble classifying, but that confuse the model. These attacks are generally "white-box" meaning the attacker has access to model weights.

Figure 1: C code containing a buffer overrun.

[MISSING_PAGE_FAIL:7]

At one point during the evaluation, the docker container running the CTF challenge failed to start. So the model ran a network scan, found the docker service on the real network outside any sandbox. It connected to the service, and started the docker container running the challenge environment. See Figure 4 in OpenAI [2024b] for more detail.

This is a clear instance of reward hacking in the real world. The only action that mattered to the model's fitness function was to successfully retrieve the flag. It had no issue with contravening the competition guidelines to achieve this aim. In some ways, it is fortunate that this was the model's only aim, as it had essentially escaped its testing environment. Though apparently the testing was done in a responsible manner, on an isolated network, so it couldn't readily get into the real world.

### Example: Intentional deception from OpenAI o1-preview

The OpenAI o1-preview model [OpenAI, 2024a] integrates chain-of-thought style reasoning into each query. In the safety analysis OpenAI performed prior to releasing this model to the public, they found that 0.38% of its internal thoughts were about intentional deception [OpenAI, 2024b]. In other words, about 4 out of every 1000 thoughts in the chain of thought involve how to deceive the user--perhaps deciding to make up a hallucination to satisfy the user's desire for an output.

## 5 Discussion

In several cases, we identified cybersecurity analogies where a potential solution can be ported to LLMs. For example, maintaining strict separation between code and data (user and system prompts) from Section 2.1; apply separate verification to avoid reward hacking (Section 2.4); do not assume safety just from a test environment (Section 2.5). While it is possible for safety to be retrofitted through virtualization as in Section 2.3, in most other cases, we concluded that a more principled approach to AI safety should be utilized instead. Given the relative ease of creating jailbreaks for today's LLMs, and the wide latitude of the specification language (natural language text), we predict that attackers will continue to win this game until more principled defenses are used. This could prove dangerous as AI models increase in capability towards superintelligence (Section 2.6).

There are several approaches discussed in the AI literature to construct models with a more principled approach to safety. They range from the broad framework for safe-by-design AI with quantitative guarantees proposed by Dalrymple et al. [2024], to probabilistic guarantees such as proposed by Bengio [2023], Bengio et al. [2024a], to methods that rely heavily on formal verification and proof-carrying code and hardware as in Tegmark and Omohundro [2023]. The objective in all cases is to obtain guarantees that will continue to hold even if the underlying AI is superintelligent. The key insight is that a theorem, once proven, remains true and can thus provide guarantees even against an arbitrarily intelligent AI (discussed in Section 2.5).

An important idea is that although we cannot be sure of how a neural network computes an output, we may be able to use simple code to verify the proof (outputted by the neural net) of a statement the AI makes (which we can think of as a theorem about the innocuity of a proposed action). One of the key challenges is that the kind of theorem we really care about concerns quantities that are difficult to formalize, like human intentions, which appear to be needed to get alignment guarantees. This motivated early work on AI that maintains probabilistic uncertainty about human intentions. For example, Russell [2022] shows that this uncertainty about the AI reward function means that the AI would prefer to ask rather than act and take a chance of doing something bad3. Modeling uncertainty is also at the heart of Bengio [2023], Bengio et al. [2024a], both about what is "harm" (what the AI should avoid) and about other properties of the world (a world model). In their case, although hard guarantees cannot be obtained, bounds on the probability of harm can in principle be estimated, making it possible to act conservatively with respect to a safety specification.

Footnote 3: This has interesting properties such as a robot wanting to preserve its off switch, because that’s how humans communicate that the reward function is misaligned.

## 6 Conclusion

We draw comparisons in six cases between AI safety issues and cybersecurity. Overall, we believe a greater understanding of cybersecurity will greatly aid in creating new AI safety techniques.

[MISSING_PAGE_FAIL:9]

Ana Nora Evans, Bradford Campbell, and Mary Lou Soffa. Is rust used safely by software developers? In _Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering_, pages 246-257, 2020.
* Some Lessons from Adversarial Machine Learning, 2024. URL https://www.youtube.com/watch?v=umfeF0Dx-r4.
* Ferrara [2012] Anthony Ferrara. Thoughts on space shuttle code process, 2012. URL https://blog.ircmaxell.com/2012/08/thoughts-on-space-shuttle-code-process.html.
* Fishman [1996] Charles Fishman. They write the right stuff, 1996. URL https://www.fastcompany.com/28121/they-write-right-stuff.
* Goodfellow et al. [2015] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In Yoshua Bengio and Yann LeCun, editors, _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings_, 2015. URL http://arxiv.org/abs/1412.6572.
* Goodin [2008] Dan Goodin. Strange snafu misroutes domestic US Internet traffic through China Telecom, 2008. URL https://arstechnica.com/information-technology/2018/11/strange-snafu-misroutes-domestic-us-internet-traffic-through-china-telecom/.
* Griffin and Wilfong [1999] Timothy G Griffin and Gordon Wilfong. An analysis of bgp convergence properties. _ACM SIGCOMM computer communication review_, 29(4):277-288, 1999.
* Hamilton [2024] Phillip Hamilton. Ignore all previous instructions, 2024. URL https://knowyourmeme.com/memes/ignore-all-previous-instructions.
* Hu et al. [2016] Hong Hu, Shweta Shinde, Sendroiu Adrian, Zheng Leong Chua, Prateek Saxena, and Zhenkai Liang, Data-oriented programming: On the expressiveness of non-control data attacks. In _2016 IEEE Symposium on Security and Privacy (SP)_, pages 969-986. IEEE, 2016.
* Kapoor [2024] Mukund Kapoor. How to Jailbreak ChatGPT 4 in 2024 (Prompt + Examples), 2024. URL https://weam.ai/blog/guide/jailbreak-chatgpt/.
* Kashyap [2023] Diwakar Kashyap. Pointers in Golang (go), 2023. URL https://medium.com/@diwakarkashyap/pointers-in-golang-go-9818ea3d91b5.
* Kemkes [2020] Phillip Kemkes. Techniques: Current use of virtual machine detection methods, 2020. URL https://www.gdatasetware.com/blog/2020/05/36068-current-use-of-virtual-machine-detection-methods.
* krebs on security, 2021. URL https://krebsonsecurity.com/2021/05/try-this-one-weird-trick-russian-hackers-hate/.
* Lee et al. [2024] Seanie Lee, Minsu Kim, Lynn Cherif, David Dobre, Juho Lee, Sung Ju Hwang, Kenji Kawaguchi, Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, and Moksh Jain. Learning diverse attacks on large language models for robust red-teaming and safety tuning, 2024. URL https://arxiv.org/abs/2405.18540.
* Madory [2023] Doug Madory. A Brief History of the Internet's Biggest BGP Incidents, 2023. URL https://nanog.org/stories/articles/a-brief-history-of-the-internets-bigg-bgp-incidents/.
* McConnell [2004] Steve McConnell. _Code complete_. Pearson Education, 2004.
* Mehrotra et al. [2023] Anay Mehrotra, Manolis Zampetakis, Paul Kassiank, Blaine Nelson, Hyrum Anderson, Yaron Singer, and Amin Karbasi. Tree of attacks: Jailbreaking black-box l lms automatically. _arXiv preprint arXiv:2312.02119_, 2023.
* Microsoft [2019] Microsoft. A proactive approach to more secure code, 2019. URL https://msrc.microsoft.com/blog/2019/07/a-proactive-approach-to-more-secure-code/.
* McConnell [2019]MITRE. 2023 cwe top 25 most dangerous software weaknesses, 2023. URL https://cwe.mitre.org/top25/archive/2023/2023_top25_list.html.
* Mitre atlas (2024a) MITRE. Mitre atlas, 2024a. URL https://atlas.mitre.org/.
* Mitre atl&ck (2024b) MITRE. Mitre atl&ck, 2024b. URL https://attack.mitre.org/.
* OpenAI (2024a) OpenAI. Learning to Reason with LLMs, 2024a. URL https://openai.com/index/learning-to-reason-with-llms/.
* OpenAI (2024b) OpenAI. Openai o1 system card, 2024b. URL https://openai.com/index/openai-o1-system-card/.
* Real et al. (2019) Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classifier architecture search. In _Proceedings of the aaai conference on artificial intelligence_, volume 33, pages 4780-4789, 2019.
* Robison (2024) Kylie Robison. OpenAI's latest model will block the 'ignore all previous instructions' loophole, 2024. URL https://www.theverge.com/2024/7/19/2420141/openai-chatgpt-gpt-4o-prompt-injection-instruction-hierarchy.
* Roemer et al. (2012) Ryan Roemer, Erik Buchanan, Hovav Shacham, and Stefan Savage. Return-oriented programming: Systems, languages, and applications. _ACM Transactions on Information and System Security (TISSEC)_, 15(1):1-34, 2012.
* security in embedded systems. In Augusto Vega, Pradip Bose, and Alper Buyuktosunoglu, editors, _Rugged Embedded Systems_, pages 149-205. Morgan Kaufmann, Boston, 2017. ISBN 978-0-12-802459-1. doi: https://doi.org/10.1016/B978-0-12-802459-1.00006-3. URL https://www.sciencedirect.com/science/article/pii/B9780128024591000063.
* Russell (2022) Stuart Russell. Provably beneficial artificial intelligence. In _Proceedings of the 27th International Conference on Intelligent User Interfaces_, pages 3-3, 2022.
* Sakamoto (2024) Akira Sakamoto. Chatgpt jailbreak prompts: How to unchain chatgpt, 2024. URL https://web.archive.org/web/20240909235358/https://docs.kanaries.net/articles/chatgpt-jailbreak-prompt.
* Shacham (2007) Hovav Shacham. The geometry of innocent flesh on the bone: Return-into-libc without function calls (on the x86). In _Proceedings of the 14th ACM conference on Computer and communications security_, pages 552-561, 2007.
* Stafford (2020) V Stafford. Zero trust architecture. _NIST special publication_, 800:207, 2020.
* Szegedy et al. (2014) Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In Yoshua Bengio and Yann LeCun, editors, _2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings_, 2014. URL http://arxiv.org/abs/1312.6199.
* Szekeres et al. (2013) Laszlo Szekeres, Mathias Payer, Tao Wei, and Dawn Song. Sok: Eternal war in memory. In _2013 IEEE Symposium on Security and Privacy_, pages 48-62. IEEE, 2013.
* Tegmark and Omohundro (2023) Max Tegmark and Steve Omohundro. Provably safe systems: the only path to controllable agi. _arXiv preprint arXiv:2309.01933_, 2023.
* VMware (2024) VMware. Desktop hypervisor solutions! vmware, 2024. URL https://www.vmware.com/products/desktop-hypervisor/workstation-and-fusion.
* Wallace et al. (2024) Eric Wallace, Kai Xiao, Reimar Leike, Lilian Weng, Johannes Heidecke, and Alex Beutel. The instruction hierarchy: Training llms to prioritize privileged instructions. _arXiv preprint arXiv:2404.13208_, 2024.
* Warren (2023) Tom Warren. These are Microsoft's Bing AI secret rules and why it says it's named Sydney, 2023. URL https://www.theverge.com/23599441/microsoft-bing-ai-sydney-secret-rules.
* Wojciech et al. (2020)Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail?, 2023. URL https://arxiv.org/abs/2307.02483.
* Weidinger et al. (2022) Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, et al. Taxonomy of risks posed by language models. In _Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency_, pages 214-229, 2022.
* Wu et al. (2023) Yuanwei Wu, Xiang Li, Yixin Liu, Pan Zhou, and Lichao Sun. Jailbreaking gpt-4v via self-adversarial attacks with system prompts, 2023.
* Xu et al. (2024) Zihao Xu, Yi Liu, Gelei Deng, Yuekang Li, and Stiepan Picek. Llm jailbreak attack versus defense techniques-a comprehensive study. _arXiv preprint arXiv:2402.13457_, 2024.
* Ziegler et al. (2019) Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. _arXiv preprint arXiv:1909.08593_, 2019.
* Zou et al. (2023) Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models, 2023.

## Appendix A Appendix A: How BGP blackholing brings the internet to a halt

The internet is basically one giant network that has the job of routing data (packets) from a source computer (with a particular IP address) to a destination computer (with another IP address). However, each piece of the physical internet networking infrastructure is run by different companies and countries, and data will probably flow through multiple entities on its way to the destination.

BGP (_Border Gateway Protocol_) is the algorithm used to coordinate at the highest level. Each entity that operates a piece of the network is called an _autonomous system_ or _AS_ within the BGP protocol, and has its own subnetwork (range of IP addresses4). Each AS has complex agreements with others about how many packets of data they can transfer, what performance (latency) they can expect and what the price will be per packet. It is typical for each AS to have multiple peering agreements, so that if one link goes down, traffic can still flow; if one link gets overwhelmed, an AS can start using additional links even if the cost to them is higher. Routes thus change frequently and dynamically based on the needs of the internet [Griffin and Wilfong, 1999].

Footnote 4: For example, 9.0.0.0 through 9.254.254.254 is the range given over exclusively to IBM, and Apple has 17.0.0.0 through 17.254.254.254, otherwise known as 17.0.0.0/8 [emilecantin, 2022].

Within the BGP protocol, an AS (e.g., a networking company) can announce that they are the final destination for routing a specific range of IP addresses (e.g., "the route for 9.0.0.0 is AS0"). If another AS called AS1 receives this route, it will then announce to all its peers that it is one hop away from that, e.g. "the route for 9.0.0.0 is AS1\(\rightarrow\)AS0". Eventually, an AS on the other side of the world might have a route like "the route for 9.0.0.0 is AS5\(\rightarrow\)AS4\(\rightarrow\)AS3\(\rightarrow\)AS2\(\rightarrow\)AS1\(\rightarrow\)AS0". The shortest route wins, as there may be multiple possible paths.

Unfortunately however, the BGP protocol is old enough that there is no authentication involved in announcing routes. ASes are all large entities so there is some element of trust, but occasionally someone will make a mistake and e.g. cause all traffic within the US to be routed to China and back again [Goodin, 2008]. An AS can even announce a very short one-hop route for IP addresses that it does not own, and then drop the traffic, preventing it from reaching its final destination. This has happened by accident and is called BGP blackholing [Madory, 2023].

## Appendix B Appendix B: MITRE ATLAS framework for machine learning security

The MITRE ATTACK framework MITRE [2024b] is a longstanding taxonomy for classifying the stages that an attacker might go through as they compromise a system. This taxonomy helps defenders identify where their controls are strong, and where there might be holes in their defenses. As of 2021, MITRE has created the ATLAS framework MITRE [2024a] which serves the same purpose, identifying stages of attacks, for machine learning systems.

Below are the categories in the ATLAS framework. An attack may consist of multiple steps, often in roughly the order presented, but attackers can skip forward and backward and repeat steps in the general case:

1. **Reconnaissance**: Attacker gathers information about how the ML system works.
2. **Resource Development**: Attacker obtains attack infrastructure, poisons datasets, etc.
3. **Initial Access**: Creating some initial foothold through prompt injection, supply chain attacks, phishing (e.g., emailing employees at the company pretending to be their tech support).
4. **ML Model Access**: Gaining access to the model directly, through an API or model weights.
5. **Execution**: Embedding malicious code into ML artifacts though tampering with weights, malicious plugins; or simply tricking a user into running code.
6. **Persistence**: Attacker tries to maintain their foothold, e.g. by poisoning a model's training data to install backdoors, enabling future re-entry.
7. **Privilege Escalation**: Attacker tries to gain higher permissions, like network administrator.
8. **Defense Evasion**: Evading detection by machine learning-enabled security software.
9. **Credential Access**: Attacker tries to steal account names, passwords, authentication tokens.
10. **Discovery**: Figuring out the machine learning environment after compromise, if initial entry was through some other means, or examining the system from the inside.
11. **Collection**: Gathering machine learning artifacts such as model weights.
12. **ML Attack Staging**: Leveraging knowledge of and access to the ML system to tailor attack.
13. **Exfiltration**: Attacker tries to extract ML artifacts or other ML system information from target systems to their own environment.
14. **Impact**: Attacker tries to manipulate, interrupt, or destroy ML systems and data.