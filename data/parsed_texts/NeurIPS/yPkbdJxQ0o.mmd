Three-Way Trade-Off in Multi-Objective Learning: Optimization, Generalization and Conflict-Avoidance

Lisha Chen

Rensselaer Polytechnic Institute

Troy, NY, United States

chenl21@rpi.edu

&Heshan Fernando

Rensselaer Polytechnic Institute

Troy, NY, United States

fernah@rpi.edu

&Yiming Ying

University of Sydney

Camperdown, Australia

yiming.ying@sydney.edu.au

&Tianyi Chen

Rensselaer Polytechnic Institute

Troy, NY, United States

chentianyi19@gmail.com

Equal contribution.The work of L. Chen, H. Fernando, and T. Chen was supported by the National Science Foundation (NSF) MoDL-SCALE project 2134168 and the RPI-IBM Artificial Intelligence Research Collaboration (AIRC). The work of Y. Ying was partially supported by NSF (DMS-2110836, IIS-2103450, and IIS-2110546).

###### Abstract

Multi-objective learning (MOL) often arises in emerging machine learning problems when multiple learning criteria or tasks need to be addressed. Recent works have developed various _dynamic weighting_ algorithms for MOL, including MGDA and its variants, whose central idea is to find an update direction that _avoids conflicts_ among objectives. Albeit its appealing intuition, empirical studies show that dynamic weighting methods may not always outperform static alternatives. To bridge this gap between theory and practice, we focus on a new variant of stochastic MGDA - the Multi-objective gradient with Double sampling (MoDo) algorithm and study its generalization performance and the interplay with optimization through the lens of algorithm stability. We find that the rationale behind MGDA - updating along conflict-avoidant direction - may _impede_ dynamic weighting algorithms from achieving the optimal \(\mathcal{O}(1/\sqrt{n})\) population risk, where \(n\) is the number of training samples. We further highlight the variability of dynamic weights and their impact on the three-way trade-off among optimization, generalization, and conflict avoidance that is unique in MOL. Code is available at https://github.com/heshandevaka/Trade-Off-NOL.

## 1 Introduction

Multi-objective learning (MOL) emerges frequently in recent machine learning problems such as learning under fairness and safety constraints [49]; learning across multiple tasks, including multi-task learning [39] and meta-learning [47]; and, learning across multiple agents that may not share a global utility including federated learning [40] and multi-agent reinforcement learning [33].

This work considers solving the empirical version of MOL defined on the training dataset as \(S=\{z_{1},\ldots,z_{n}\}\). The performance of a model \(x\in\mathbb{R}^{d}\) on a datum \(z\) for the \(m\)-th objective is denoted as \(f_{z,m}:\mathbb{R}^{d}\mapsto\mathbb{R}\), and its performance on the entire training dataset \(S\) is measured by the \(m\)-th empirical objective \(f_{S,m}(x)\) for \(m\in[M]\). MOL optimizes the vector-valued objective, given by

\[\min_{x\in\mathbb{R}^{d}}\ F_{S}(x)\coloneqq[f_{S,1}(x),\ldots,f_{S,M}(x)].\] (1.1)One natural method for solving (1.1) is to optimize the (weighted) average of the multiple objectives, also known as _static or unitary weighting_[18, 45]. However, this method may face challenges due to _potential conflicts_ among multiple objectives during the optimization process; e.g., conflicting gradient directions \(\langle\nabla f_{S,m}(x),\nabla f_{S,m^{\prime}}(x)\rangle<0\). A popular alternative is thus to _dynamically weight_ gradients from different objectives to avoid conflicts and obtain a direction \(d(x)\) that optimizes all objective functions jointly that we call a _conflict-avoidant_ (CA) direction. Algorithms in this category include the multi-gradient descent algorithm (MGDA) [9], its stochastic variants [30, 10, 52]. While the idea of finding CA direction in dynamic weighting-based approaches is very appealing, recent empirical studies reveal that dynamic weighting methods may not outperform static weighting in some MOL benchmarks [18, 45], especially when it involves stochastic updates and deep models. Specifically, observed by [18], the vanilla stochastic MGDA can be under-optimized, leading to larger training errors than static weighting. The reason behind this training performance degradation has been studied in [52, 10], which suggest the vanilla stochastic MGDA has biased updates, and propose momentum-based methods to address this issue. Nevertheless, in [45], it is demonstrated that the training errors of MGDA and static weighting are similar, while their main difference lies in the generalization performance. Unfortunately, the reason behind this testing performance degradation is not fully understood and remains an open question.

To gain a deeper understanding of the dynamic weighting-based algorithms, a natural question is

**Q1:**_What are the major sources of errors in dynamic weighting-based MOL methods?_

To answer this question theoretically, we first introduce a proper measure of testing performance in MOL - the _Pareto stationary measure_ in terms of the population objectives, which will immediately imply stronger measures such as Pareto optimality under strongly convex objectives. We then decompose this measure into _generalization_ error and _optimization_ error and further introduce a new metric on the _distance to CA directions_ that is unique to MOL; see Sections 2.1 and 2.2.

To characterize the performance of MOL methods in a unified manner, we introduce a generic dynamic weighting-based MOL method that we term stochastic Multi-Objective gradient with DOuble sampling algorithm (**MoDo**), which uses a step size \(\gamma\) to control the change of dynamic weights. Roughly speaking, by controlling \(\gamma\), MoDo approximates MGDA (large \(\gamma\)) and static weighting algorithm (\(\gamma=0\)) as two special cases; see Section 2.3. We first analyze the generalization error of the model learned by MoDo through the lens of algorithmic stability [3, 14, 24] in the framework of statistical learning theory. To our best knowledge, this is the _first-ever-known_ stability analysis for MOL algorithms. Here the key contributions lie in defining a new notion of stability - MOL uniform stability and then establishing a tight upper bound (matching lower bound) on the MOL uniform stability for MoDo algorithm that involves two coupled sequences; see Section 3.1. We then analyze the optimization error of MoDo and its distance to CA directions, where the key contributions lie in relaxing _the bounded function value/gradient assumptions_ and significantly improving the convergence rate of state-of-the-art dynamic weighting-based MOL methods [10]; see Section 3.2.

Different from the stability analysis for single-objective learning [14], the techniques used in our generalization and optimization analysis allow to remove conflicting assumptions and use larger step sizes to ensure both small generalization and optimization errors, which are of independent interest.

Given the holistic analysis of dynamic weighting methods provided in **Q1**, a follow-up question is

**Q2:**_What may cause the empirical performance degradation of dynamic weighting methods?_

Figure 1: An example from [29] with two objectives (1a and 1b) to show the three-way trade-off in MOL. Figures 1c-1e show the optimization trajectories, where the **black**\(\bullet\) marks initializations of the trajectories, colored from **red** (start) to yellow (end). The background solid/dotted contours display the landscape of the average empirical/population objectives. The gray/green bar marks empirical/population Pareto front, and the **black**\(\star\)/green\(\star\) marks solution to the average objectives.

Aligned with this toy example, our theoretical results suggest a novel _three-way trade-off_ in the performance of dynamic weighting-based MOL algorithm; see Section 3.3. Specifically, it suggests that the step size for dynamic weighting \(\gamma\) plays a central role in the trade-off among convergence to CA direction, convergence to empirical Pareto stationarity, and generalization error; see Figure 2. In this sense, MGDA has an edge in convergence to the CA direction but it could sacrifice generalization; the static weighting method cannot converge to the CA direction but guarantees convergence to the empirical Pareto solutions and their generalization. Our analysis also suggests that MoDo achieves a small population risk under a proper combination of step sizes and the number of iterations.

## 2 Problem Formulation and Target of Analysis

In this section, we first introduce the problem formulation of MOL, the target of analysis, the metric to measure its generalization, and then present the MGDA algorithm and its stochastic variant.

### Preliminaries of MOL

Denote the vector-valued objective function on datum \(z\) as \(F_{z}(x)=[f_{z,1}(x),\ldots,f_{z,M}(x)]\). The training and testing performance of \(x\) can then be measured by the empirical objective \(F_{S}(x)\) and the population objective \(F(x)\) which are, respectively, defined as \(F_{S}(x)\coloneqq\frac{1}{n}\sum_{i=1}^{n}F_{z_{i}}(x)\) and \(F(x)\coloneqq\mathbb{E}_{z\sim\mathcal{D}}[F_{z}(x)]\). Their corresponding gradients are denoted as \(\nabla F_{S}(x)\) and \(\nabla F(x)\in\mathbb{R}^{d\times M}\).

Analogous to the stationary solution and optimal solution in single-objective learning, we define the Pareto stationary point and Pareto optimal solution for MOL problem \(\min\limits_{x\in\mathbb{R}^{d}}F(x)\) as follows.

**Definition 1** (Pareto stationary and Pareto optimal).: _If there exists a convex combination of the gradient vectors that equals to zero, i.e., there exists \(\lambda\in\Delta^{M}\) such that \(\nabla F(x)\lambda=0\), then \(x\in\mathbb{R}^{d}\) is Pareto stationary. If there is no \(x\in\mathbb{R}^{d}\) and \(x\neq x^{*}\) such that, for all \(m\in[M]\)\(f_{m}(x)\leq f_{m}(x^{*})\), with \(f_{m^{\prime}}(x)<f_{m^{\prime}}(x^{*})\) for at least one \(m^{\prime}\in[M]\), then \(x^{*}\) is Pareto optimal. If there is no \(x\in\mathbb{R}^{d}\) such that for all \(m\in[M]\), \(f_{m}(x)<f_{m}(x^{*})\), then \(x^{*}\) is weakly Pareto optimal._

By definition, at a Pareto stationary solution, there is no common descent direction for all objectives. A necessary and sufficient condition for \(x\) being Pareto stationary for smooth objectives is that \(\min_{\lambda\in\Delta^{M}}\|\nabla F(x)\lambda\|=0\). Therefore, \(\min_{\lambda\in\Delta^{M}}\|\nabla F(x)\lambda\|\) can be used as a measure of Pareto stationarity (PS) [9, 11, 42, 30, 10]. We will refer to the aforementioned quantity as the _PS population risk_ henceforth and its empirical version as _PS empirical risk_ or _PS optimization error_. We next introduce the target of our analysis based on the above definitions.

### Target of analysis and error decomposition

In existing generalization analysis for MOL, measures based on function values have been used to derive generalization guarantees in terms of Pareto optimality [7, 41]. However, for general

Figure 2: An illustration of three-way trade-off among optimization, generalization, and conflict avoidance in the strongly convex case; \(\alpha\) is the step size for \(x\), \(\gamma\) is the step size for weights \(\lambda\), where \(o(\cdot)\) denotes a strictly slower growth rate, \(\omega(\cdot)\) denotes a strictly faster growth rate, and \(\Theta(\cdot)\) denotes the same growth rate. Arrows \(\downarrow\) and \(\uparrow\) respectively represent diminishing in an optimal rate and growing in a fast rate w.r.t. \(n\), while \(\searrow\) represents diminishing w.r.t. \(n\), but not in an optimal rate.

nonconvex smooth MOL problems, it can only be guaranteed for an algorithm to converge to Pareto stationarity of the empirical objective, i.e., a small \(\min_{\lambda\in\Delta^{M}}\|\nabla F_{S}(x)\lambda\|\)[9, 11, 30, 31]. Thus, it is not reasonable to measure population risk in terms of Pareto optimality in this case. Furthermore, when all the objectives are convex or strongly convex, Pareto stationarity is a sufficient condition for weak Pareto optimality or Pareto optimality, respectively, as stated in Proposition 1.

**Proposition 1** ([42, Lemma 2.2]).: _If \(f_{m}(x)\) are convex or strongly-convex for all \(m\in[M]\), and \(x\in\mathbb{R}^{d}\) is a Pareto stationary point of \(F(x)\), then \(x\) is weakly Pareto optimal or Pareto optimal._

Next, we proceed to decompose the PS population risk.

**Error Decomposition.** Given a model \(x\), the PS population risk can be decomposed into

\[\min_{\underbrace{\lambda\in\Delta^{M}}_{\text{PS population risk }R_{\text{pop}}(x)}}\ =\underbrace{\min_{\lambda\in\Delta^{M}}\|\nabla F (x)\lambda\|-\min_{\lambda\in\Delta^{M}}\|\nabla F_{S}(x)\lambda\|}_{\text{ PS generalization error }R_{\text{gen}}(x)}+\underbrace{\min_{\lambda\in\Delta^{M}}\|\nabla F_{S}(x)\lambda\|}_{ \text{PS optimization error }R_{\text{opt}}(x)}\] (2.1)

where the optimization error quantifies the training performance, i.e., how well does model \(x\) perform on the training data; and the generalization error (gap) quantifies the difference between the testing performance on new data sampled from \(\mathcal{D}\) and the training performance, i.e., how well the model \(x\) performs on unseen testing data compared to the training data.

Let \(A:\mathcal{Z}^{n}\mapsto\mathbb{R}^{d}\) denote a randomized MOL algorithm. Given training data \(S\), we are interested in the expected performance of the output model \(x=A(S)\), which is measured by \(\mathbb{E}_{A,S}[R_{\text{pop}}(A(S))]\). From (2.1) and linearity of expectation, it holds that

\[\mathbb{E}_{A,S}[R_{\text{pop}}(A(S))]=\mathbb{E}_{A,S}[R_{\text{ gen}}(A(S))]+\mathbb{E}_{A,S}[R_{\text{opt}}(A(S))].\] (2.2)

**Distance to CA direction.** As demonstrated in Figure 1, the key merit of dynamic weighting over static weighting algorithms lies in its ability to navigate through conflicting gradients. Consider an update direction \(d=-\nabla F_{S}(x)\lambda\), where \(\lambda\) is the dynamic weights from a simplex \(\lambda\in\Delta^{M}\coloneqq\{\lambda\in\mathbb{R}^{M}\mid\mathbf{1}^{\top} \lambda=1,\ \lambda\geq 0\}\). To obtain such a steepest CA direction in unconstrained learning that maximizes the minimum descent of all objectives, we can solve the following problem [11]

\[\text{CA direction}\quad d(x) =\operatorname*{arg\,min}_{d\in\mathbb{R}^{d}}\max_{m\in[M]} \bigg{\{}\langle\nabla f_{S,m}(x),d\rangle+\frac{1}{2}\|d\|^{2}\bigg{\}}\] (2.3a) \[\stackrel{{\text{equivalent to}}}{{\Longleftrightarrow}} d(x) =-\nabla F_{S}(x)\lambda^{*}(x)\ \ \text{s.t.}\ \ \lambda^{*}(x)\in \operatorname*{arg\,min}_{\lambda\in\Delta^{M}}\|\nabla F_{S}(x)\lambda\|^{2}.\] (2.3b)

Defining \(d_{\lambda}(x)=-\nabla F_{S}(x)\lambda\) given \(x\in\mathbb{R}^{d}\) and \(\lambda\in\Delta^{M}\), we measure the distance to \(d(x)\) via [10]

\[\text{CA direction error}\qquad\qquad\mathcal{E}_{\text{ca}}(x,\lambda) \coloneqq\|d_{\lambda}(x)-d(x)\|^{2}.\] (2.4)

With the above definitions of measures that quantify the performance of algorithms in different aspects, we then introduce a stochastic gradient algorithm for MOL that is analyzed in this work.

### A stochastic algorithm for MOL

MGDA finds \(\lambda^{*}(x)\) in (2.3b) using the full-batch gradient \(\nabla F_{S}(x)\), and then constructs \(d(x)=-\nabla F_{S}(x)\lambda^{*}(x)\), a CA direction for all empirical objectives \(f_{S,m}(x)\). However, in practical statistical learning settings, the full-batch gradient \(\nabla F_{S}(x)\) may be costly to obtain, and thus one may resort to a stochastic estimate of \(\nabla F_{S}(x)\) instead. The direct stochastic counterpart of MGDA, referred to as the stochastic multi-gradient algorithm in [30], replaces the full-batch gradients \(\nabla f_{S,m}(x)\) in (2.3b) with their stochastic approximations \(\nabla f_{z,m}(x)\) for \(z\in S\), which, however, introduces a biased stochastic estimate of \(\lambda^{*}_{t+1}\), thus a biased CA direction; see [10, Section 2.3].

To provide a tight analysis, we introduce a simple yet theoretically grounded stochastic variant of MGDA - stochastic Multi-Objective gradient with DOuble sampling algorithm (MoDo). MoDo obtains an unbiased stochastic estimate of the gradient of problem (2.3b) through double (independent) sampling and iteratively updates \(\lambda\), because \(\mathbb{E}_{z_{t,1},z_{t,2}}[\nabla F_{z_{t,1}}(x_{t})^{\top}\nabla F_{z_{t,2}}( x_{t})\lambda_{t}]=\nabla F_{S}(x_{t})^{\top}\nabla F_{S}(x_{t})\lambda_{t}\). At each iteration \(t\), denote \(z_{t,s}\) as an independent sample from \(S\) with \(s\in[3]\), and \(\nabla F_{z_{t,s}}(x_{t})\) as a stochastic estimate of \(\nabla F_{S}(x_{t})\). MoDo updates \(x_{t}\) and \(\lambda_{t}\) as

\[\lambda_{t+1} =\Pi_{\Delta^{M}}\left(\lambda_{t}-\gamma_{t}\nabla F_{z_{t,1}}(x _{t})^{\top}\nabla F_{z_{t,2}}(x_{t})\lambda_{t}\right)\] (2.5a) \[x_{t+1} =x_{t}-\alpha_{t}\nabla F_{z_{t,3}}(x_{t})\lambda_{t+1}\] (2.5b)

where \(\alpha_{t},\gamma_{t}\) are step sizes, and \(\Pi_{\Delta^{M}}(\cdot)\) denotes Euclidean projection to the simplex \(\Delta^{M}\). We have summarized the MoDo algorithm in Algorithm 1 and will focus on MoDo in the subsequent analysis.

## 3 Optimization, Generalization and Three-Way Trade-Off

This section presents the theoretical analysis of the PS population risk associated with the MoDo algorithm, where the analysis of generalization error is in Section 3.1 and that of optimization error is in Section 3.2. A summary of our main results is given in Table 1.

### Multi-objective generalization and uniform stability

We first bound the expected PS generalization error by the generalization in gradients in Proposition 2, then introduce the MOL uniform stability and establish its connection to the generalization in gradients. Finally, we bound the MOL uniform stability.

**Proposition 2**.: _With \(\|\cdot\|_{\mathrm{F}}\) denoting the Frobenious norm, \(R_{\mathrm{gen}}(A(S))\) in (2.2) can be bounded by_

\[\mathbb{E}_{A,S}[R_{\mathrm{gen}}(A(S))]\leq\mathbb{E}_{A,S}[\|\nabla F(A(S)) -\nabla F_{S}(A(S))\|_{\mathrm{F}}].\] (3.1)

With Proposition 2, next we introduce the concept of MOL uniform stability tailored for MOL problems and show that PS generalization error in MOL can be bounded by the MOL uniform stability. Then we analyze their bound in general nonconvex case and strongly convex case, respectively.

**Definition 2** (MOL uniform stability).: _A randomized algorithm \(A:\mathcal{Z}^{n}\mapsto\mathbb{R}^{d}\), is MOL-uniformly stable with \(\epsilon_{\mathrm{F}}\) if for all neighboring datasets \(S,S^{\prime}\) that differ in at most one sample, we have_

\[\sup_{z}\ \mathbb{E}_{A}\big{[}\|\nabla F_{z}(A(S))-\nabla F_{z}(A(S^{\prime})) \|_{\mathrm{F}}^{2}\big{]}\leq\epsilon_{\mathrm{F}}^{2}.\] (3.2)

Next we show the relation between the upper bound of PS generalization error in (3.1) and MOL uniform stability in Proposition 3.

**Proposition 3** (MOL uniform stability and generalization).: _Assume for any \(z\), the function \(F_{z}(x)\) is differentiable. If a randomized algorithm \(A:\mathcal{Z}^{n}\mapsto\mathbb{R}^{d}\) is MOL-uniformly stable with \(\epsilon_{\mathrm{F}}\), then_

\[\mathbb{E}_{A,S}[\|\nabla F(A(S))-\nabla F_{S}(A(S))\|_{\mathrm{F}}]\leq 4 \epsilon_{\mathrm{F}}+\sqrt{n^{-1}\mathbb{E}_{S}\left[\nabla_{z\sim\mathcal{D}} (\nabla F_{z}(A(S)))\right]}\] (3.3)

_where the variance is defined as \(\mathbb{V}_{z\sim\mathcal{D}}(\nabla F_{z}(A(S)))=\mathbb{E}_{z\sim\mathcal{D} }\big{[}\|\nabla F_{z}(A(S))-\mathbb{E}_{z\sim\mathcal{D}}[\nabla F_{z}(A(S)) ]\|_{\mathrm{F}}^{2}\big{]}\)._

\begin{table}
\begin{tabular}{c|c|c c c c} \hline \hline Assumption & Method & Optimization & Generalization & Risk & CA Distance \\ \hline \multirow{2}{*}{\begin{tabular}{c} NC, \\ Lip-C, S \\ \end{tabular} } & Static & \((\alpha T)^{-\frac{1}{2}}+\alpha^{\frac{1}{2}}\) & \(T^{\frac{1}{2}}n^{-\frac{1}{2}}\) & \(n^{-\frac{1}{6}}\) & ✗ \\  & Dynamic & \((\alpha T)^{-\frac{1}{2}}+\alpha^{\frac{1}{2}}+\gamma^{\frac{1}{2}}\) & \(T^{\frac{1}{2}}n^{-\frac{1}{2}}\) & \(n^{-\frac{1}{6}}\) & \((\gamma T)^{-1}+\alpha^{\frac{1}{2}}\gamma^{-\frac{1}{2}}+\gamma\) \\ \hline \multirow{2}{*}{
\begin{tabular}{c} SC, S \\ \end{tabular} } & Static & \((\alpha T)^{-\frac{1}{2}}+\alpha^{\frac{1}{2}}\) & \(n^{-\frac{1}{2}}\) & \(n^{-\frac{1}{2}}\) & ✗ \\  & Dynamic & \((\alpha T)^{-\frac{1}{2}}+\alpha^{\frac{1}{2}}+\gamma^{\frac{1}{2}}\) & \(\left\{\begin{array}{ll}n^{-\frac{1}{2}},&\gamma=\mathcal{O}(T^{-1})\\ T^{\frac{1}{2}}n^{-\frac{1}{2}},&\mathrm{o.w.}\\ \end{array}\right.\) & \(\left\{\begin{array}{ll}n^{-\frac{1}{2}}\\ n^{-\frac{1}{6}}\\ n^{-\frac{1}{6}}\\ \end{array}\right.\) & \((\gamma T)^{-1}+\alpha^{\frac{1}{2}}\gamma^{-\frac{1}{2}}+\gamma\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of optimization error, generalization error, and population risk under different assumptions for static and dynamic weighting. Use “NC”, “SC” to represent nonconvex and strongly convex, and “Lip-C”, “S” to represent Lipschitz continuous and smooth, respectively.

Proposition 3 establishes a connection between the upper bound of the PS generalization error and the MOL uniform stability, where the former can be bounded above by the latter plus the variance of the stochastic gradient over the population data distribution. It is worth noting that the standard arguments of bounding the generalization error measured in function values by the uniform stability measured in function values [14, Theorem 2.2] is not applicable here as the summation and norm operators are not exchangeable. More explanations are given in the proof in Appendix B.1.

**Theorem 1** (PS generalization error of MoDo in nonconvex case).: _If \(\sup_{z}\mathbb{E}_{A}\left[\left\|\nabla F_{z}(A(S))\right\|_{\mathrm{F}}^{2} \right]\leq G^{2}\) for any \(S\), then the MOL uniform stability, i.e., \(\epsilon_{\mathrm{F}}^{2}\) in Definition 2 is bounded by \(\epsilon_{\mathrm{F}}^{2}\leq 4G^{2}T\big{/}n\). And the PS generalization error \(\mathbb{E}_{A,S}[R_{\mathrm{gen}}(A(S))]=\mathcal{O}(T^{\frac{1}{2}}n^{-\frac{ 1}{2}})\)._

Compared to the function value uniform stability upper bound in [14, Theorem 3.12] for nonconvex single-objective learning, Theorem 1 does not require a step size decay \(\alpha_{t}=\mathcal{O}(1/t)\), thus can enjoy at least a polynomial convergence rate of optimization errors w.r.t. \(T\). Combining Theorem 1 with Proposition 3, to ensure the generalization error is diminishing with \(n\), one needs to choose \(T=o(n)\), which lies in the "early stopping" regime and results in potentially large optimization error. We then provide a tighter bound in the strongly convex case that allows a larger choice of \(T\). Below we list the standard assumptions used to derive the introduced MOL stability.

**Assumption 1** (Lipschitz continuity of \(\nabla F_{z}(x)\)).: _For all \(m\in[M]\), \(\nabla f_{z,m}(x)\) is \(\ell_{f,1}\)-Lipschitz continuous for all \(z\). And \(\nabla F_{z}(x)\) is \(\ell_{f,1}\)-Lipschitz continuous in Frobenius norm for all \(z\)._

**Assumption 2**.: _For all \(m\in[M]\), \(z\in\mathcal{Z}\), \(f_{z,m}(x)\) is \(\mu\)-strongly convex w.r.t. \(x\), with \(\mu>0\)._

Note that in the strongly convex case, the gradient norm \(\|\nabla F_{z}(x)\|_{\mathrm{F}}\) can be unbounded in \(\mathbb{R}^{d}\). Therefore, one cannot assume Lipschitz continuity of \(f_{z,m}(x)\) w.r.t. \(x\in\mathbb{R}^{d}\). We address this challenge by showing that \(\{x_{t}\}\) generated by the MoDo algorithm is bounded as stated in Lemma 1. Notably, combining with Assumption 1, we can derive that the gradient norm \(\|\nabla F_{z}(x_{t})\|_{\mathrm{F}}\) is also bounded, which serves as a stepping stone to derive the MOL stability bound.

**Lemma 1** (Boundedness of \(x_{t}\) for strongly convex and smooth objectives).: _Suppose Assumptions 1, 2 hold. For \(\{x_{t}\},t\in[T]\) generated by MoDo algorithm or other dynamic weighting algorithm with weight \(\lambda\in\Delta^{M}\), step size \(\alpha_{t}=\alpha\), and \(0\leq\alpha\leq\ell_{f,1}^{-1}\), there exists a finite positive constant \(c_{x}\) such that \(\|x_{t}\|\leq c_{x}\). And there exists finite positive constants \(\ell_{f}\), \(\ell_{F}=\sqrt{M}\ell_{f}\), such that for all \(\lambda\in\Delta^{M}\), we have \(\|\nabla F(x_{t})\lambda\|\leq\ell_{f}\), and \(\|\nabla F(x_{t})\|_{\mathrm{F}}\leq\ell_{F}\)._

With Lemma 1, the stability bound and PS generalization is provided below.

**Theorem 2** (PS generalization error of MoDo in strongly convex case).: _Suppose Assumptions 1, 2 hold. Let \(A\) be the MoDo algorithm (Algorithm 1). For the MOL uniform stability \(\epsilon_{F}\) of algorithm \(A\) in Definition 2, if the step sizes satisfy \(0<\alpha_{t}\leq\alpha\leq 1/(2\ell_{f,1})\), and \(0<\gamma_{t}\leq\gamma\leq\min\{\frac{\mu^{2}}{120\ell_{f}^{2}\ell_{g,1}}, \frac{1}{8(3\ell_{f}^{2}+2\ell_{g,1})}\}/T\), then it holds that_

\[\epsilon_{\mathrm{F}}^{2}\leq\frac{48}{\mu n}\ell_{f}^{2}\ell_{F,1}^{2}\Big{(} \alpha+\frac{12+4M\ell_{f}^{2}}{\mu n}+\frac{10M\ell_{f}^{4}\gamma}{\mu}\Big{)} \quad\text{and}\quad\mathbb{E}_{A,S}[R_{\mathrm{gen}}(A(S))]=\mathcal{O}(n^{ -\frac{1}{2}}).\] (3.4)

_And there exist functions \(F_{z}(x)\) that satisfy Assumptions 1, 2, neighboring datasets \(S\), \(S^{\prime}\) that differ in at most one sample, and MoDo algorithm with step sizes \(0<\alpha_{t}\leq\alpha\leq 1/(2\ell_{f,1})\), and \(0<\gamma_{t}\leq\gamma\leq\min\{\frac{\mu^{2}}{120\ell_{f}^{2}\ell_{g,1}}, \frac{1}{8(3\ell_{f}^{2}+2\ell_{g,1})}\}/T\) such that_

\[\mathbb{E}_{A}\big{[}\|\nabla F_{z}(A(S))-\nabla F_{z}(A(S^{\prime}))\|_{ \mathrm{F}}^{2}\big{]}\geq\frac{M\mu^{2}}{256n^{2}}.\] (3.5)

Theorem 2 provides both upper and lower bounds for the MOL uniform stability. In this case, we choose \(\alpha=\Theta(T^{-\frac{1}{2}})\), \(\gamma=o(T^{-1})\), and \(T=\Theta(n^{2})\) to minimize the PS population risk upper bound, as detailed in Section 3.3. With this choice, the MOL uniform stability upper bound matches the lower bound in an order of \(n^{-2}\), suggesting that our bound is tight. The generalization error bound in (3.4) is a direct implication from the MOL uniform stability bound in (3.4), Propositions 2, and 3.

It states that the PS generalization error of MoDo is \(\mathcal{O}(n^{-\frac{1}{2}})\), which matches the generalization error of static weighting up to a constant coefficient [22]. Our result also indicates that when all the objectives are strongly convex, choosing small step sizes \(\alpha\) and \(\gamma\) can benefit the generalization error.

### Multi-objective optimization error

In this section, we bound the multi-objective PS optimization error \(\min_{\lambda\in\Delta^{M}}\|\nabla F_{S}(x)\lambda\|\)[9; 30; 10]. As discussed in Section 2.2, this measure being zero implies the model \(x\) achieves a Pareto stationarity for the empirical problem.

Below we list an additional standard assumption used to derive the optimization error.

**Assumption 3** (Lipschitz continuity of \(F_{z}(x)\)).: _For all \(m\in[M]\), \(f_{z,m}(x)\) are \(\ell_{f}\)-Lipschitz continuous for all \(z\). Then \(F_{z}(x)\) are \(\ell_{F}\)-Lipschitz continuous in Frobenius norm for all \(z\) with \(\ell_{F}=\sqrt{M}\ell_{f}\)._

**Lemma 2** (Distance to CA direction).: _Suppose either: 1)Assumptions 1, 3 hold; or 2) Assumptions 1, 2 hold, with \(\ell_{f}\) and \(\ell_{F}\) defined in Lemma 1. For \(\{x_{t}\},\{\lambda_{t}\}\) generated by MoDo, it holds that_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}[\|d_{\lambda_{t}}(x_{t})-d(x_{t})\|^ {2}]\leq\frac{4}{\gamma T}+6\sqrt{M\ell_{f,1}\ell_{f}^{2}\frac{\alpha}{\gamma }}+\gamma M\ell_{f}^{4}.\] (3.6)

Lemma 2 analyzes convergence to the CA direction using the measure introduced in Section 2.2. By, e.g., choosing \(\alpha=\Theta(T^{-\frac{3}{4}})\), and \(\gamma=\Theta(T^{-\frac{1}{4}})\), the RHS of (3.6) converges in a rate of \(\mathcal{O}(T^{-\frac{1}{4}})\).

**Theorem 3** (PS optimization error of MoDo).: _Suppose either: 1) Assumptions 1, 3 hold; or, 2) Assumptions 1, 2 hold, with \(\ell_{f}\) defined in Lemma 1. Define \(c_{F}\) such that \(\mathbb{E}_{A}[F_{S}(x_{0})\lambda_{0}]=\min_{x\in\mathbb{R}^{d}}\mathbb{E}_{ A}[F_{S}(x)\lambda_{0}]\leq c_{F}\). Considering \(\{x_{t}\}\) generated by MoDo (Algorithm 1), with \(\alpha_{t}=\alpha\leq 1/(2\ell_{f,1})\), \(\gamma_{t}=\gamma\), then under either condition 1) or 2), it holds that_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}\Big{[}\min_{\lambda\in\Delta^{M}}\| \nabla F_{S}(x_{t})\lambda\|\Big{]}\leq\sqrt{\frac{c_{F}}{\alpha T}}+\sqrt{ \frac{3}{2}\gamma M\ell_{f}^{4}}+\sqrt{\frac{1}{2}\alpha\ell_{f,1}\ell_{f}^{2}}.\] (3.7)

The choice of step sizes \(\alpha=\Theta(T^{-\frac{3}{4}})\), and \(\gamma=\Theta(T^{-\frac{1}{4}})\) to ensure convergence to CA direction is suboptimal for the convergence to Pareto stationarity (see Theorem 3), exhibiting a trade-off between convergence to the CA direction and convergence to Pareto stationarity; see discussion in Section 3.3.

### Optimization, generalization and conflict avoidance trade-off

Combining the results in Sections 3.1 and 3.2, we are ready to analyze and summarize the three-way trade-off of MoDo in MOL. With \(A_{t}(S)=x_{t}\) denoting the output of algorithm \(A\) at the \(t\)-th iteration, we can decompose the PS population risk \(R_{\mathrm{pop}}(A_{t}(S))\) as (cf. (2.1) and (3.1))

\[\mathbb{E}_{A,S}\big{[}R_{\mathrm{pop}}(A_{t}(S))\big{]}\leq\mathbb{E}_{A,S} \Big{[}\min_{\lambda\in\Delta^{M}}\|\nabla F_{S}(A_{t}(S))\lambda\|\Big{]}+ \mathbb{E}_{A,S}\Big{[}\|\nabla F(A_{t}(S))-\nabla F_{S}(A_{t}(S))\|_{\mathrm{ F}}\Big{]}.\]

**The general nonconvex case.** Suppose Assumptions 1, 3 hold. By the generalization error in Theorem 1, and the optimization error bound in Theorem 3, the PS population risk of the output of MoDo can be bounded by

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A,S}\big{[}R_{\mathrm{pop}}(A_{t}(S)) \big{]}=\mathcal{O}\left(\alpha^{-\frac{1}{2}}T^{-\frac{1}{2}}+\alpha^{\frac{ 1}{2}}+\gamma^{\frac{1}{2}}+T^{\frac{1}{2}}n^{-\frac{1}{2}}\right).\] (3.8)

_Discussion of trade-off._ Choosing step sizes \(\alpha=\Theta(T^{-\frac{1}{2}})\), \(\gamma=\Theta(T^{-\frac{1}{2}})\), and number of steps \(T=\Theta(n^{\frac{3}{2}})\), then the expected PS population risk is \(\mathcal{O}(n^{-\frac{1}{6}})\), which matches the PS population risk upper bound of a general nonconvex single objective in [22]. A clear trade-off in this case is between the optimization error and generalization error, controlled by \(T\). Indeed, increasing \(T\) leads to smaller optimization errors but larger generalization errors, and vice versa. To satisfy convergence to CAdirection, it requires \(\gamma=\omega(\alpha)\) based on Lemma 2, and the optimization error in turn becomes worse, so does the PS population risk. Specifically, choosing \(\alpha=\Theta(T^{-\frac{1}{2}})\), \(\gamma=\Theta(T^{-\frac{1}{4}})\), and \(T=\Theta(n^{\frac{1}{5}})\) leads to the expected PS population risk in \(\mathcal{O}(n^{-\frac{1}{10}})\), and the distance to CA direction in \(\mathcal{O}(n^{-\frac{1}{10}})\). This shows another trade-off between conflict avoidance and optimization error.

**The strongly convex case.** Suppose Assumptions 1, 2 hold. By the generalization error and the optimization error given in Theorems 2 and 3, MoDo's PS population risk can be bounded by

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A,S}\big{[}R_{\mathrm{pop}}(A_{t}(S)) \big{]}=\mathcal{O}\left(\alpha^{-\frac{1}{2}}T^{-\frac{1}{2}}+\alpha^{\frac{ 1}{2}}+\gamma^{\frac{1}{2}}+n^{-\frac{1}{2}}\right).\] (3.9)

_Discussion of trade-off._ Choosing step sizes \(\alpha=\Theta(T^{-\frac{1}{2}})\), \(\gamma=o(T^{-1})\), and number of steps \(T=\Theta(n^{2})\), we have the expected PS population risk in gradients is \(\mathcal{O}(n^{-\frac{1}{2}})\). However, choosing \(\gamma=o(T^{-1})\) leads to large distance to the CA direction according to Lemma 2 because the term \(\frac{4}{\gamma T}\) in (3.6) increases with \(T\). To ensure convergence to the CA direction, it requires \(\gamma=\omega(T^{-1})\), under which the tighter bound in Theorem 2 does not hold but the bound in Theorem 1 still holds. In this case, the PS population risk under proper choices of \(\alpha,\gamma,T\) is \(\mathcal{O}(n^{-\frac{1}{4}})\) as discussed in the previous paragraph. Therefore, to avoid conflict of gradients, one needs to sacrifice the sample complexity of PS population risk, demonstrating a trade-off between conflict avoidance and PS population risk.

## 4 Related Works and Our Technical Contributions

**Multi-task learning (MTL).** MTL, as one application of MOL, leverages shared information among different tasks to train a model that can perform multiple tasks. MTL has been widely applied to natural language processing, computer vision, and robotics [15; 38; 50; 43]. From the optimization perspective, a simple method for MTL is to take the weighted average of the per-task losses as the objective. However, as studied in [16], the static weighting method is not able to find all the Pareto optimal models in general. Alternatively, the weights can be updated dynamically during optimization, and the weights for different tasks can be chosen based on different criteria such as uncertainty [17], gradient norms [6], or task difficulty [13]. These methods are often heuristic and designed for specific applications. Another line of work tackles MTL through MOL [39; 48; 29; 12]. A foundational algorithm in this regard is MGDA [9], which takes dynamic weighting of gradients to obtain a CA direction for all objectives. Stochastic versions of MGDA have been proposed in [30; 52; 10]. Besides finding one single model, algorithms for finding a set of Pareto optimal models rather than one have been proposed in [28; 35; 31; 20; 46; 51; 32; 27; 34].

**Theory of MOL.**_Optimization_ convergence analysis for the deterministic MGDA algorithm has been provided in [11]. Later on, stochastic variants of MGDA were introduced [30; 52; 10]. However, the vanilla stochastic MGDA introduces biased estimates of the dynamic weight, resulting in biased estimates of the CA directions during optimization. To address this, Liu et al. [30] proposed to increase the batch size during optimization, Zhou et al. [52] and Fernando et al. [10] proposed to use momentum-based bias reduction techniques. Compared to these works on the optimization analysis, we have improved the assumptions and/or the final convergence rate of the PS optimization error. A detailed comparison is summarized in Table 2. While the community has a rich history of investigating the optimization of MOL algorithms, their _generalization_ guarantee remains unexplored until recently. In [7], a min-max formulation to solve the MOL problem is analyzed, where the weights are chosen based on the maximum function values, rather than the CA direction. More recently, [41] provides generalization guarantees for MOL for a more general class of weighting. These two works analyze generalization based on the Rademacher complexity of the hypothesis class, with generalization bound independent of the training process. Different from these works, we use algorithm stability to derive the first algorithm-dependent generalization error bounds, highlighting the effect of the training dynamics. In contrast to previous MOL theoretical works that focus solely on either optimization [10; 52] or generalization [7; 41], we propose a holistic framework to analyze three types of errors, namely, optimization, generalization, and CA distance in MOL with an instantiation of the MoDo algorithm. This allows us to study the impact of the hyperparameters on the theoretical testing performance, and their optimal values to achieve the best trade-off among the errors. Our theory and algorithm design can also be applied to algorithms such as PCGrad [48], CAGrad [29], and GradNorm [6]. Specifically, for example, the implementation of CAGrad takes iterative updates of the dynamic weight using a single stochastic estimate, resulting in a biased estimate of the update direction, thus no guarantee of convergence for the stochastic algorithm. This issue can be addressed by the double sampling technique introduced in this paper. In addition, our analysis techniques have been applied to improve the analysis of convergence rates for other algorithms [44; 5].

**Algorithm stability and generalization.** Stability analysis dates back to the work [8] in 1970s. Uniform stability and its relationship with generalization were studied in [3] for the exact minimizer of the ERM problem with strongly convex objectives. The work [14] pioneered the stability analysis for stochastic gradient descent (SGD) algorithms with convex and smooth objectives. The results were extended and refined in [19] with data-dependent bounds, in [4; 37; 23] for non-convex objectives, and in [1; 24] for SGD with non-smooth and convex losses. However, all these studies mainly focus on single-objective learning problems. To our best knowledge, there is no existing work on the stability and generalization analysis for multi-objective learning problems and our results on its stability and generalization are the _first-ever-known_ ones.

**Challenges and contributions.** Our contributions are highly non-trivial as summarized below.

\(\bullet\) The definition of PS testing risk in gradient (2.1) is unique in MOL, and overcomes the unnecessarily small step sizes usually brought by the classical function value-based risk analysis. Specifically, prior stability analysis in function values for single objective learning [14] requires \(1/t\) step size decay in the nonconvex case, otherwise, the generalization error bound will depend exponentially on the number of iterations. However such step sizes lead to very slow convergence of the optimization error. This is addressed by the definitions of gradient-based measures and sampling-determined MOL algorithms, which yield stability bounds in \(\mathcal{O}(T/n)\) without any step size decay. See Theorem 1.

\(\bullet\) The stability of the dynamic weighting algorithm in the strongly convex (SC) case is non-trivial compared to single objective learning [14] because it involves two coupled sequences during the update. As a result, the classical contraction property for the update function of the model parameters that are often used to derive stability does not hold. This is addressed by controlling the change of \(\lambda_{t}\) by the step size \(\gamma\), and using mathematical induction to derive a tighter bound. See Appendix B.4.

\(\bullet\) In the SC case with an unbounded domain, the function is not Lipschitz or the gradients are unbounded, which violates the commonly used bounded gradient assumption for proving the stability and optimization error. We relax this assumption by proving that the iterates generated by dynamic weighting algorithms in the SC case are bounded on the optimization trajectory in Lemma 1.

## 5 Experiments

In this section, we conduct experiments to further demonstrate the three-way trade-off among the optimization, generalization, and conflict avoidance of the MoDo algorithm. An average of 10 random seeds with 0.5 standard deviation is reported if not otherwise specified.

### Synthetic experiments

Our theory in the SC case is first verified through a synthetic experiment; see the details in Appendix D.1. Figure 2(a) shows the PS optimization error and PS population risk, as well as the distance to CA direction, decreases as \(T\) increases, which corroborates Lemma 2, and Theorem 3. In addition, the generalization error, in this case, does not vary much with \(T\), verifying Theorems 2. In Figure 2(b), the optimization error first decreases and then increases as \(\alpha\) increases, which is consistent with Theorem 3. Notably, we observe a threshold for \(\alpha\) below which the distance to the CA direction converges even when the optimization error does not converge, while beyond which the distance to the CA direction becomes larger, verifying Lemma 2. Additionally, Figure 2(c) demonstrates that

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c} \hline \hline \multirow{2}{*}{Algorithm} & \multirow{2}{*}{\begin{tabular}{c} Batch \\ size \\ \end{tabular} } & \multirow{2}{*}{NC} & \multirow{2}{*}{\begin{tabular}{c} Lipschitz \\ \(\lambda^{*}(x)\) \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} Bounded \\ function \\ \end{tabular} } & \multirow{2}{*}{Opt.} & \multirow{2}{*}{
\begin{tabular}{c} CA \\ dist. \\ \end{tabular} } & \multirow{2}{*}{Gen.} \\ \hline SMG [30; Thm 5.3] & \(\mathcal{O}(t)\) & ✗ & ✓ & ✗ & \(T^{-\frac{1}{4}}\) & - & - \\ CR-MOGM [52; Thm 3] & \(\mathcal{O}(1)\) & ✓ & ✗ & ✓ & \(T^{-\frac{1}{4}}\) & - & - \\ MoCo [10; Thm 2] & \(\mathcal{O}(1)\) & ✓ & ✗ & ✗ & \(T^{-\frac{1}{20}}\) & \(T^{-\frac{1}{8}}\) & - \\ MoCo [10; Thm 4] & \(\mathcal{O}(1)\) & ✓ & ✗ & ✓ & \(T^{-\frac{1}{4}}\) & \(\mathcal{O}(1)\) & - \\ \hline \multicolumn{1}{l}{**MoDo (Ours, Thms 1,2,3)**} & \(\mathcal{O}(1)\) & ✓ & ✗ & ✗ & \(T^{-\frac{1}{4}}\) & \(\mathcal{O}(1)\) & \(T^{\frac{1}{2}}n^{-\frac{1}{2}}\) \\ \multicolumn{1}{l}{**MoDo (Ours, Thms 1,2,3)**} & \(\mathcal{O}(1)\) & ✓ & ✗ & ✗ & \(T^{-\frac{1}{8}}\) & \(T^{-\frac{1}{4}}\) & \(T^{\frac{1}{2}}n^{-\frac{1}{2}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison with prior stochastic MOL algorithms in terms of assumptions and the guarantees of the three errors, where logarithmic dependence is omitted, and Opt., CA dist., and Gen. are short for optimization error, CA distance, and generalization error, respectively.

increasing \(\gamma\) enlarges the PS optimization error, PS generalization error, and thus the PS population risk, but decreases the distance to CA direction, which supports Lemma 2.

### Image classification experiments

We further verify our theory in the NC case on MNIST image classification [21] using a multi-layer perceptron and three objectives: cross-entropy, mean squared error (MSE), and Huber loss. Following Section 2.2, we evaluate the performance in terms of \(R_{\text{pop}}(x)\), \(R_{\text{opt}}(x)\), \(R_{\text{gen}}(x)\), and \(\mathcal{E}_{\text{ca}}(x,\lambda)\). The exact PS population risk \(R_{\text{pop}}(x)\) is not accessible without the true data distribution. To estimate the PS population risk, we evaluate \(\min_{\lambda\in\Delta^{M}}\|\nabla F_{S_{\text{u}}}(x)\lambda\|\) on the testing data set \(S_{\text{te}}\) that is independent of training data set \(S\). The PS optimization error \(R_{\text{opt}}(x)\) is obtained by \(\min_{\lambda\in\Delta^{M}}\|\nabla F_{S}(x)\lambda\|\), and the PS generalization error \(R_{\text{gen}}(x)\) is estimated by \(\min_{\lambda\in\Delta^{M}}\|\nabla F_{S_{\text{u}}}(x)\lambda\|-R_{\text{opt }}(x)\).

We examine the impact of different \(T\), \(\alpha\), \(\gamma\) on the errors in Figure 4. Figure 3(a) shows that increasing \(T\) reduces optimization error and CA direction distance but increases generalization error, aligning with Theorems 1, 2, and 3. Figure 3(b) shows that increasing \(\alpha\) leads to an initial decrease and subsequent increase in PS optimization error and population risk. which aligns with Theorem 3 and (3.8). On the other hand, there is an overall increase in CA direction distance with \(\alpha\), which aligns with Theorem 2. Figure 3(c) shows that increasing \(\gamma\) increases both the PS population and optimization errors but decreases CA direction distance. This matches our bounds for PS optimization error in Theorem 3, PS population risk in (3.8), and CA direction distance in Theorem 2.

## 6 Conclusions

This work studies the three-way trade-off in MOL - among optimization, generalization, and conflict avoidance. Our results show that, in the general nonconvex setting, the traditional trade-off between optimization and generalization depending on the number of iterations also exists in MOL. Moreover, dynamic weighting algorithms like MoDo introduce a new dimension of trade-off in terms of conflict avoidance compared to static weighting. We demonstrate that this three-way trade-off can be controlled by the step size for updating the dynamic weighting parameter and the number of iterations. Proper choice of these parameters can lead to decent performance on all three metrics.

Figure 4: Optimization, generalization, and CA direction errors of MoDo for MNIST image classification under different \(T\), \(\alpha\), and \(\gamma\). The default parameters are \(T=1000\), \(\alpha=0.1\), and \(\gamma=0.01\).

\begin{table}
\begin{tabular}{c c c c c} \hline \hline \multirow{2}{*}{Method} & Amazon & DSLR & Webcam & \multirow{2}{*}{\(\Delta\mathcal{A}\%\downarrow\)} \\ \cline{2-2} \cline{4-4}  & Test Acc \(\uparrow\) & Test Acc \(\uparrow\) & Test Acc \(\uparrow\) \\ \hline Static & 84.62 \(\pm\) 0.71 & 94.43 \(\pm\) 0.96 & 97.44 \(\pm\) 1.20 & 2.56 \(\pm\) 0.37 \\ MGDA & 79.45 \(\pm\) 0.11 & **96.56 \(\pm\) 1.20** & **97.89 \(\pm\) 0.74** & 3.65 \(\pm\) 0.64 \\
**MoDo** & **85.13 \(\pm\) 0.58** & 95.41 \(\pm\) 0.98 & 96.78 \(\pm\) 0.65 & **2.26 \(\pm\) 0.31** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Classification results on Office-31 dataset.

Figure 3: Optimization, generalization, and CA direction errors of MoDo in the strongly convex case under different \(T,\alpha,\gamma\). The default parameters are \(T=100\), \(\alpha=0.01\), \(\gamma=0.001\).

## Broader impacts and limitations

This work has a potential impact on designing dynamic weighting algorithms and choosing hyperparameters such as step sizes and number of iterations based on the trade-off for MTL applications such as multi-language translation, and multi-agent reinforcement learning. No ethical concerns arise from this work. A limitation of this study is that the theory focuses on a specific algorithm, MoDo, for smooth objectives in unconstrained learning. Future research could explore the theory of other algorithms for non-smooth objectives or constrained learning, which would be interesting directions to pursue.

## References

* [1] Raef Bassily, Vitaly Feldman, Cristobal Guzman, and Kunal Talwar. Stability of stochastic gradient descent on nonsmooth convex losses. In _Proc. Advances in Neural Information Processing Systems_, volume 33, virtual, 2020.
* [2] Amir Beck and Marc Teboulle. Gradient-based algorithms with applications to signal-recovery problems. In _Convex Optimization in Signal Processing and Communications_, 2010.
* [3] Olivier Bousquet and Andre Elisseeff. Stability and generalization. _Journal of Machine Learning Research_, 2:499-526, March 2002.
* [4] Zachary Charles and Dimitris Papailiopoulos. Stability and generalization of learning algorithms that converge to global optima. In _Proc. International Conference on Machine Learning_, pages 744-753, Stockholm, Sweden, 2018.
* [5] Lisha Chen, Heshan Fernando, Yiming Ying, and Tianyi Chen. Three-way trade-off in multi-objective learning: Optimization, generalization and conflict-avoidance. _arXiv preprint arXiv:2305.20057_, 2023.
* [6] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. In _Proc. International Conference on Machine Learning_, virtual, July 2018.
* [7] Corinna Cortes, Mehryar Mohri, Javier Gonzalvo, and Dmitry Storcheus. Agnostic learning with multiple objectives. In _Proc. Advances in Neural Information Processing Systems_, volume 33, pages 20485-20495, virtual, 2020.
* [8] Luc Devroye and Terry Wagner. Distribution-free performance bounds for potential function rules. _IEEE Transactions on Information Theory_, 25(5):601-604, 1979.
* [9] Jean-Antoine Desideri. Multiple-gradient Descent Algorithm (MGDA) for Multi-objective Optimization. _Comptes Rendus Mathematique_, 350(5-6), 2012.
* [10] Heshan Fernando, Han Shen, Miao Liu, Subhajit Chaudhury, Keerthiram Murugesan, and Tianyi Chen. Mitigating gradient bias in multi-objective learning: A provably convergent stochastic approach. In _Proc. International Conference on Learning Representations_, Kigali, Rwanda, 2023.
* [11] Jorg Fliege, A Ismael F Vaz, and Luis Nunes Vicente. Complexity of Gradient Descent for Multi-objective Optimization. _Optimization Methods and Software_, 34(5):949-959, 2019.
* [12] Xiang Gu, Xi Yu, Jian Sun, Zongben Xu, et al. Adversarial reweighting for partial domain adaptation. In _Proc. Advances in Neural Information Processing Systems_, virtual, December 2021.
* [13] Michelle Guo, Albert Haque, De-An Huang, Serena Yeung, and Li Fei-Fei. Dynamic task prioritization for multitask learning. In _Proceedings of the European conference on computer vision_, Munich, Germany, July 2018.
* [14] Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In _Proc. International Conference on Machine Learning_, pages 1225-1234, New York City, NY, 2016.

* [15] Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. A Joint Many-task Model: Growing a Neural Network for Multiple NLP Tasks. _arXiv preprint:1611.01587_, 2016.
* [16] Yuzheng Hu, Ruicheng Xian, Qilong Wu, Qiuling Fan, Lang Yin, and Han Zhao. Revisiting scalarization in multi-task learning: A theoretical perspective. In _Proc. Advances in Neural Information Processing Systems_, New Orleans, LA, 2023.
* [17] A Kendall, Y Gal, and R Cipolla. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. _arXiv preprint:1705.07115_, 2017.
* [18] Vitaly Kurin, Alessandro De Palma, Ilya Kostrikov, Shimon Whiteson, and Pawan K Mudigonda. In defense of the unitary scalarization for deep multi-task learning. In _Proc. Advances in Neural Information Processing Systems_, volume 35, pages 12169-12183, New Orleans, LA, 2022.
* [19] Ilja Kuzborskij and Christoph Lampert. Data-dependent stability of stochastic gradient descent. In _Proc. International Conference on Machine Learning_, pages 2815-2824, Stockholm, Sweden, 2018.
* [20] Panagiotis Kyriakis, Jyotirmoy Deshmukh, and Paul Bogdan. Pareto policy adaptation. In _Proc. International Conference on Learning Representations_, virtual, 2021.
* [21] Yann LeCun. The mnist database of handwritten digits. _http://yann. lecun. com/exdb/mnist/_, 1998.
* [22] Yunwen Lei. Stability and generalization of stochastic optimization with nonconvex and nonsmooth problems. In _Proc. Annual Conference on Learning Theory_, Bangalore, India, July 2023.
* [23] Yunwen Lei, Rong Jin, and Yiming Ying. Stability and generalization analysis of gradient methods for shallow neural networks. In _Proc. Advances in Neural Information Processing Systems_, New Orleans, LA, 2022.
* [24] Yunwen Lei and Yiming Ying. Fine-grained analysis of stability and generalization for stochastic gradient descent. In _Proc. International Conference on Machine Learning_, pages 5809-5819, virtual, 2020.
* [25] Baijiong Lin, Ye Feiyang, Yu Zhang, and Ivor Tsang. Reasonable effectiveness of random weighting: A litmus test for multi-task learning. _Transactions on Machine Learning Research_, 2022.
* [26] Baijiong Lin and Yu Zhang. LibMTL: A python library for multi-task learning. _arXiv preprint arXiv:2203.14338_, 2022.
* [27] Xi Lin, Zhiyuan Yang, and Qingfu Zhang. Pareto set learning for neural multi-objective combinatorial optimization. _arXiv preprint arXiv:2203.15386_, 2022.
* [28] Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, and Sam Kwong. Pareto multi-task learning. In _Proc. Advances in Neural Information Processing Systems_, Vancouver, Canada, December 2019.
* [29] Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu. Conflict-Averse Gradient Descent for Multi-task Learning. In _Proc. Advances in Neural Information Processing Systems_, virtual, December 2021.
* [30] Suyun Liu and Luis Nunes Vicente. The Stochastic Multi-gradient Algorithm for Multi-objective Optimization and its Application to Supervised Machine Learning. _Annals of Operations Research_, pages 1-30, 2021.
* [31] Xingchao Liu, Xin Tong, and Qiang Liu. Profiling Pareto Front With Multi-Objective Stein Variational Gradient Descent. In _Proc. Advances in Neural Information Processing Systems_, virtual, December 2021.
* [32] Debabrata Mahapatra and Vaibhav Rajan. Exact pareto optimal search for multi-task learning: Touring the pareto front. _arXiv preprint:2108.00597_, 2021.

* [33] Kristof Van Moffaert and Ann Nowe. Multi-objective reinforcement learning using sets of pareto dominating policies. _Journal of Machine Learning Research_, 15(107):3663-3692, 2014.
* [34] Michinari Momma, Chaosheng Dong, and Jia Liu. A multi-objective/multi-task learning framework induced by pareto stationarity. In _Proc. International Conference on Machine Learning_, pages 15895-15907, Baltimore, MD, 2022.
* [35] Aviv Navon, Aviv Shamsian, Ethan Fetaya, and Gal Chechik. Learning the pareto front with hypernetworks. In _Proc. International Conference on Learning Representations_, virtual, April 2020.
* [36] Yurii Nesterov. _Introductory Lectures on Convex Optimization: A Basic Course_. Springer Publishing Company, Incorporated, 1 edition, 2014.
* [37] Dominic Richards and Ilja Kuzborskij. Stability & generalisation of gradient descent for shallow neural networks without the neural tangent kernel. In _Proc. Advances in Neural Information Processing Systems_, volume 34, virtual, 2021.
* [38] Sebastian Ruder. An overview of multi-task learning in deep neural networks. _arXiv preprint:1706.05098_, 2017.
* [39] Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. In _Proc. Advances in Neural Information Processing Systems_, Montreal, Canada, December 2018.
* [40] Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. Federated multi-task learning. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Proc. Advances in Neural Information Processing Systems_, volume 30, Long Beach, CA, 2017.
* [41] Peter Sukenik and Christoph H Lampert. Generalization in multi-objective machine learning. _arXiv preprint arXiv:2208.13499_, 2022.
* [42] Hiroki Tanabe, Ellen H. Fukuda, and Nobuo Yamashita. Proximal gradient methods for multi-objective optimization and their applications. _Computational Optimization and Applications_, 72(2):339-361, 2019.
* [43] Simon Vandenhende, Stamatios Georgoulis, Wouter Van Gansbeke, Marc Proesmans, Dengxin Dai, and Luc Van Gool. Multi-task learning for dense prediction tasks: A survey. _IEEE Trans. Pattern Anal. Machine Intell._, 2021.
* [44] Peiyao Xiao, Hao Ban, and Kaiyi Ji. Direction-oriented multi-objective learning: Simple and provable stochastic algorithms. In _Proc. Advances in Neural Information Processing Systems_, New Orleans, LA, 2023.
* [45] Derrick Xin, Behrooz Ghorbani, Justin Gilmer, Ankush Garg, and Orhan Firat. Do current multi-task optimization methods in deep learning even help? In _Proc. Advances in Neural Information Processing Systems_, volume 35, pages 13597-13609, New Orleans, LA, 2022.
* [46] Yijun Yang, Jing Jiang, Tianyi Zhou, Jie Ma, and Yuhui Shi. Pareto policy pool for model-based offline reinforcement learning. In _Proc. International Conference on Learning Representations_, virtual, 2021.
* [47] Feiyang Ye, Baijiong Lin, Zhixiong Yue, Pengxin Guo, Qiao Xiao, and Yu Zhang. Multi-objective meta learning. In _Proc. Advances in Neural Information Processing Systems_, volume 34, pages 21338-21351, virtual, 2021.
* [48] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. In _Proc. Advances in Neural Information Processing Systems_, virtual, December 2020.
* [49] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P Gummadi. Fairness constraints: Mechanisms for fair classification. In _Proc. International Conference on Artificial Intelligence and Statistics_, pages 962-970, Fort Lauderdale, FL, 2017.

* [50] Yu Zhang and Qiang Yang. A survey on multi-task learning. _IEEE Trans. Knowledge Data Eng._, 2021.
* [51] Yiyang Zhao, Linnan Wang, Kevin Yang, Tianjun Zhang, Tian Guo, and Yuandong Tian. Multi-objective optimization by learning space partitions. _arXiv preprint arXiv:2110.03173_, 2021.
* [52] Shiji Zhou, Wenpeng Zhang, Jiyan Jiang, Wenliang Zhong, Jinjie Gu, and Wenwu Zhu. On the convergence of stochastic multi-objective gradient manipulation and beyond. In _Proc. Advances in Neural Information Processing Systems_, volume 35, pages 38103-38115, New Orleans, LA, December 2022.

**Appendix for "Three-Way Trade-Off in Multi-Objective Learning: Optimization, Generalization and Conflict-Avoidance "**

## Appendix A Notations

A summary of notations used in this work is listed in Table 4 for ease of reference.

In the proof, we use \(\|\cdot\|\) to denote the spectral norm, and \(\|\cdot\|_{\mathrm{F}}\) to denote the Frobenius norm.

\begin{table}
\begin{tabular}{l|l} \hline \hline Notations & Descriptions \\ \hline \(x\in\mathbb{R}^{d}\) & Model parameter, or decision variable \\ \(z\in\mathcal{Z}\) & Data point for training or testing \\ \(S\in\mathcal{Z}^{n}\) & Dataset such that \(S=\{z_{1},\ldots,z_{n}\}\) \\ \(f_{z,m}(x)\), \(f_{S,m}(x)\) & A scalar-valued objective function evaluated on data point \(z\), \\  & with \(f_{z,m}:\mathbb{R}^{d}\mapsto\mathbb{R}\), or on dataset \(S\), \(f_{S,m}\), with \(f_{S,m}\coloneqq\frac{1}{|S|}\sum_{z\in S}f_{z,m}(x)\) \\ \(f_{m}(x)\) & A scalar-valued population objective function, \(f_{m}(x)\coloneqq\mathbb{E}_{z}[f_{z,m}(x)]\) \\ \(\nabla f_{m}(x)\) & Gradient of \(f_{m}(x)\), with \(\nabla f_{m}(x):\mathbb{R}^{d}\mapsto\mathbb{R}^{d}\) \\ \(F_{z}(x)\), \(F_{S}(x)\) & A vector-valued objective function evaluated on data point \(z\), \\  & with \(F_{z}:\mathbb{R}^{d}\mapsto\mathbb{R}^{M}\), or on dataset \(S\), with \(F_{S}\coloneqq\frac{1}{|S|}\sum_{z\in S}F_{z}(x)\) \\ \(F(x)\) & A vector-valued population objective, \(F(x)\coloneqq\mathbb{E}_{z}[f_{z,m}(x)]\) \\ \(\nabla F(x)\) & Gradient of \(F(x)\), with \(\nabla F(x):\mathbb{R}^{d}\mapsto\mathbb{R}^{d\times M}\) \\ \(\lambda\in\Delta^{M}\) & Weighting parameter in an \((M-1)\)-simplex \\ \(\lambda_{\rho}^{*}(x)\in\Delta^{M}\) & CA weight, optimal solution to (2.3), when \(\rho=0\), it is simplified as \(\lambda^{*}(x)\) \\ \(\mathbf{1}\in\mathbb{R}^{M}\) & All-one vector with dimension \(M\) \\ \hline \(\alpha\) & Step size to update model parameter \(x\) \\ \(\gamma\) & Step size to update weight \(\lambda\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Notations and their descriptions.

Bounding the generalization error

### Proof of Propositions 2-3

In this subsection, we prove Propositions 2-3, which establishes the relation between PS generalization error and MOL uniform stability.

Proof of Proposition 2.: For a given model \(x\), it holds that

\[R_{\mathrm{gen}}(x) =\min_{\lambda\in\Delta^{M}}\|\nabla F(x)\lambda\|-\min_{\lambda \in\Delta^{M}}\|\nabla F_{S}(x)\lambda\|\] \[=-\max_{\lambda\in\Delta^{M}}-\|\nabla F(x)\lambda\|+\max_{ \lambda\in\Delta^{M}}-\|\nabla F_{S}(x)\lambda\|\] \[\stackrel{{(a)}}{{\leq}}\max_{\lambda\in\Delta^{M}}( \|\nabla F(x)\lambda\|-\|\nabla F_{S}(x)\lambda\|)\stackrel{{(b)} }{{\leq}}\max_{\lambda\in\Delta^{M}}(\|(\nabla F(x)-\nabla F_{S}(x))\lambda \|)\] \[\stackrel{{(c)}}{{\leq}}\max_{\lambda\in\Delta^{M}}( \|\nabla F(x)-\nabla F_{S}(x)\|_{\mathrm{F}}\|\lambda\|_{\mathrm{F}})\leq\| \nabla F(x)-\nabla F_{S}(x)\|_{\mathrm{F}}\] (B.1)

where \((a)\) follows from the subadditivity of max operator, \((b)\) follows from triangle inequality, \((c)\) follows from Cauchy-Schwartz inequality.

Setting \(x=A(S)\), and taking expectation over \(A,S\) on both sides of the above inequality, we have

\[\mathbb{E}_{A,S}[R_{\mathrm{gen}}(A(S))]\leq\mathbb{E}_{A,S}[\|\nabla F(A(S)) -\nabla F_{S}(A(S))\|_{\mathrm{F}}].\] (B.2)

Proof of Proposition 3.: The proof extends that of [22] for single objective learning to our MOL setting. Recall that \(S=\{z_{1},\ldots,z_{n}\}\), which are drawn i.i.d. from the data distribution \(\mathcal{D}\). Define the perturbed dataset \(S^{(i)}=\{z_{1},\ldots,z^{\prime}_{i},\ldots,z_{n}\}\) sampled i.i.d. from \(\mathcal{D}\) with \(z^{\prime}_{i}\) independent of \(z_{j}\), for all \(i,j\in[n]\). Let \(\tilde{z}\) be an independent sample of \(z_{j},z^{\prime}_{j}\), for all \(j\in[n]\), and from the same distribution \(\mathcal{D}\). We first decompose the difference of population gradient and empirical gradient on the algorithm output \(n(\nabla F(A(S))-\nabla F_{S}(A(S)))\) as follows using the gradient on \(A(S^{(i)})\). Since \(\mathbb{E}_{\tilde{z}}[\nabla F_{\tilde{z}}(A(S))]=\nabla F(A(S))\), it holds that

\[n\big{(}\nabla F(A(S))-\nabla F_{S}(A(S))\big{)}=n\mathbb{E}_{ \tilde{z}}[\nabla F_{\tilde{z}}(A(S))]-n\nabla F_{S}(A(S))\] \[= n\mathbb{E}_{\tilde{z}}[\nabla F_{z}(A(S))]-\Bigg{(}\sum_{i=1}^ {n}\nabla F_{z_{i}}(A(S))\Bigg{)}+\sum_{i=1}^{n}\Big{(}\mathbb{E}_{z^{\prime }_{i}}[\nabla F(A(S^{(i)}))]-\mathbb{E}_{z^{\prime}_{i}}[\nabla F(A(S^{(i)}))] \Big{)}\] \[+\sum_{i=1}^{n}\Big{(}\mathbb{E}_{z^{\prime}_{i}}[\nabla F_{z_{i} }(A(S^{(i)}))]-\mathbb{E}_{z^{\prime}_{i}}[\nabla F_{z_{i}}(A(S^{(i)}))] \Big{)}\] \[= \sum_{i=1}^{n}\mathbb{E}_{\tilde{z},z^{\prime}_{i}}[\nabla F_{ \tilde{z}}(A(S))-\nabla F_{\tilde{z}}(A(S^{(i)}))]+\sum_{i=1}^{n}\underbrace{ \mathbb{E}_{z^{\prime}_{i}}[\mathbb{E}_{\tilde{z}}[\nabla F_{\tilde{z}}(A(S^{(i )}))]-\nabla F_{z_{i}}(A(S^{(i)}))]}_{\xi_{i}(S)}\] \[+\sum_{i=1}^{n}\mathbb{E}_{z^{\prime}_{i}}\big{[}\nabla F_{z_{i} }(A(S^{(i)}))-\nabla F_{z_{i}}(A(S))\big{]}\] (B.3)

where the last equality follows from rearranging and that \(z_{i},z^{\prime}_{i},\tilde{z}\) are mutually independent. Applying triangle inequality to (B.3), it then follows that

\[n\|\nabla F(A(S))-\nabla F_{S}(A(S))\|_{\mathrm{F}}\leq \sum_{i=1}^{n}\mathbb{E}_{\tilde{z},z^{\prime}_{i}}[\|\nabla F_{ \tilde{z}}(A(S))-\nabla F_{\tilde{z}}(A(S^{(i)}))\|_{\mathrm{F}}]+\Big{\|} \sum_{i=1}^{n}\xi_{i}(S)\Big{\|}_{\mathrm{F}}\] \[+\sum_{i=1}^{n}\mathbb{E}_{z^{\prime}_{i}}[\|\nabla F_{z_{i}}(A(S^ {(i)}))-\nabla F_{z_{i}}(A(S))\|_{\mathrm{F}}].\] (B.4)

Note \(S\) and \(S^{(i)}\) differ by a single sample. By Definition 2, the MOL uniform stability \(\epsilon_{\mathrm{F}}\), and Jensen's inequality, we further get

\[n\mathbb{E}\left[\|\nabla F(A(S))-\nabla F_{S}(A(S))\|_{\mathrm{F}}\right]\leq 2 n\epsilon_{\mathrm{F}}+\mathbb{E}\Big{[}\Big{\|}\sum_{i=1}^{n}\xi_{i}(S) \Big{\|}_{\mathrm{F}}\Big{]}.\] (B.5)We then proceed to bound \(\mathbb{E}\big{[}\big{\|}\sum_{i=1}^{n}\xi_{i}(S)\big{\|}_{\mathrm{F}}\big{]}\), which satisfies

\[\left(\mathbb{E}\big{[}\big{\|}\sum_{i=1}^{n}\xi_{i}(S)\big{\|}_{ \mathrm{F}}\big{]}\right)^{2}\leq\mathbb{E}\Big{[}\Big{\|}\sum_{i=1}^{n}\xi_{i} (S)\Big{\|}_{\mathrm{F}}^{2}\Big{]}=\sum_{i=1}^{n}\underbrace{\mathbb{E}\big{[} \|\xi_{i}(S)\|_{\mathrm{F}}^{2}\big{]}}_{J_{1,i}}+\sum_{i,j\in[n]:i\neq j} \underbrace{\mathbb{E}[\langle\xi_{i}(S),\xi_{j}(S)\rangle]}_{J_{2,i,j}}.\] (B.6)

For \(J_{1,i}\), according to the definition of \(\xi_{i}(S)\) in (B.3) and Jensen inequality, it holds that

\[J_{1,i}=\mathbb{E}[\|\xi_{i}(S)\|_{\mathrm{F}}^{2}] =\mathbb{E}\left[\big{\|}\mathbb{E}_{z_{i}^{\prime}}\big{[} \mathbb{E}_{\bar{z}}[\nabla F_{\bar{z}}(A(S^{(i)}))]-\nabla F_{z_{i}}(A(S^{(i )}))\big{]}\big{\|}_{\mathrm{F}}^{2}\right]\] \[\stackrel{{(a)}}{{\leq}}\mathbb{E}\left[\big{\|} \mathbb{E}_{\bar{z}}[\nabla F_{\bar{z}}(A(S^{(i)}))]-\nabla F_{z_{i}}(A(S^{(i) }))\big{\|}_{\mathrm{F}}^{2}\right]\] \[\stackrel{{(b)}}{{=}}\mathbb{E}\left[\big{\|} \mathbb{E}_{\bar{z}}[\nabla F_{\bar{z}}(A(S))]-\nabla F_{z_{i}^{\prime}}(A(S) )\big{\|}_{\mathrm{F}}^{2}\right]\] \[=\mathbb{E}\left[\nabla_{\bar{z}}(\nabla F_{\bar{z}}(A(S))) \right],\] (B.7)

where \((a)\) follows from Jensen's inequality, \((b)\) follows from the symmetry between \(z_{i}\) and \(z_{i}^{\prime}\). To bound \(J_{2,i,j}\) with \(i\neq j\), further introduce \(S^{\prime\prime}=\{z_{1}^{\prime\prime},\ldots,z_{n}^{\prime\prime}\}\) which are drawn i.i.d. from the data distribution \(\mathcal{D}\). Then for each \(i,j\in[n]\) with \(i\neq j\), introduce \(S_{j}\) as a neighboring dataset of \(S\) by replacing its \(z_{j}\) with \(z_{j}^{\prime\prime}\), and \(S_{j}^{(i)}\) as a neighboring dataset of \(S^{(i)}\) by replacing its \(z_{j}\) with \(z_{j}^{\prime\prime}\), i.e.,

\[S_{j} =\{z_{1},\ldots,z_{j-1},z_{j}^{\prime\prime},z_{j+1},\ldots,z_{n}\},\] (B.8a) \[S_{j}^{(i)} =\{z_{1},\ldots,z_{i-1},z_{i}^{\prime},z_{i+1},\ldots,z_{j-1},z_{ j}^{\prime\prime},z_{j+1},\ldots,z_{n}\}.\] (B.8b)

Then the idea is to bound \(J_{2,i,j}\) using the newly introduced neighboring datasets \(S_{j}\) and \(S_{j}^{(i)}\), so as to connect to the definition of the stability \(\epsilon_{\mathrm{F}}\). We first show that \(\mathbb{E}\left[\langle\xi_{i}(S),\xi_{j}(S)\rangle\right]=\mathbb{E}\left[ \langle\xi_{i}(S)-\xi_{i}(S_{j}),\xi_{j}(S)-\xi_{j}(S_{i})\rangle\right]\) because for \(i\neq j\),

\[\mathbb{E}\left[\langle\xi_{i}(S),\xi_{j}(S)\rangle\right]\stackrel{{ (c)}}{{=}}0,\ \ \mathbb{E}\left[\langle\xi_{i}(S_{j}),\xi_{j}(S_{i})\rangle\right] \stackrel{{(d)}}{{=}}0,\ \ \mathbb{E}\left[\langle\xi_{i}(S_{j}),\xi_{j}(S_{i})\rangle\right] \stackrel{{(e)}}{{=}}0.\] (B.9)

For \(i\neq j\), \((c)\) follows from

\[\mathbb{E}\left[\langle\xi_{i}(S_{j}),\xi_{j}(S)\rangle\right]= \mathbb{E}\mathbb{E}_{z_{j}}\left[\langle\xi_{i}(S_{j}),\xi_{j}(S)\rangle \right]=\mathbb{E}\left[\langle\xi_{i}(S_{j}),\mathbb{E}_{z_{j}}\left[\xi_{j}( S)\right]\rangle\right]=0,\] (B.10)

where the second identity holds since \(\xi_{i}(S_{j})\) is independent of \(z_{j}\) and the last identity follows from \(\mathbb{E}_{z_{j}}\left[\xi_{j}(S)\right]=0\) due to the symmetry between \(\bar{z}\) and \(z_{i}\), and their independence with \(S^{(i)}\), derived as

\[\mathbb{E}_{z_{i}}\left[\xi_{i}(S)\right]=\mathbb{E}_{z_{i}}\Big{[}\mathbb{E}_ {z_{i}^{\prime}}[\mathbb{E}_{\bar{z}}[\nabla F_{\bar{z}}(A(S^{(i)}))]-\nabla F _{z_{i}}(A(S^{(i)}))]\Big{]}=0,\quad\forall i\in[n].\] (B.11)

In a similar way, for \(i\neq j\), \((d)\) and \((e)\) follow from

\[\mathbb{E}\left[\langle\xi_{i}(S),\xi_{j}(S_{i})\rangle\right]= \mathbb{E}\mathbb{E}_{z_{i}}\left[\langle\xi_{i}(S),\xi_{j}(S_{i})\rangle \right]=\mathbb{E}\left[\langle\xi_{j}(S_{i}),\mathbb{E}_{z_{i}}\left[\xi_{i}(S) \rangle\right]\right]=0,\] (B.12) \[\mathbb{E}\left[\langle\xi_{i}(S_{j}),\xi_{j}(S_{i})\rangle \right]=\mathbb{E}\mathbb{E}_{z_{i}}\left[\langle\xi_{i}(S_{j}),\xi_{j}(S_{i}) \rangle\right]=\mathbb{E}\left[\langle\xi_{j}(S_{i}),\mathbb{E}_{z_{i}}\left[\xi_{i }(S_{j})\rangle\right]\right]=0.\] (B.13)

Based on (B.9), for \(i\neq j\) we have

\[J_{2,i,j}= \mathbb{E}\left[\langle\xi_{i}(S),\xi_{j}(S)\rangle\right]= \mathbb{E}\left[\langle\xi_{i}(S)-\xi_{i}(S_{j}),\xi_{j}(S)-\xi_{j}(S_{i})\rangle\right]\] \[\leq \mathbb{E}\left[\big{\|}\xi_{i}(S)-\xi_{i}(S_{j})\big{\|}_{ \mathrm{F}}\|\,\xi_{j}(S)-\xi_{j}(S_{i})\|_{\mathrm{F}}\right]\] \[\leq \frac{1}{2}\mathbb{E}\left[\big{\|}\xi_{i}(S)-\xi_{i}(S_{j})\big{\|} _{\mathrm{F}}^{2}\right]+\frac{1}{2}\mathbb{E}\left[\big{\|}\xi_{j}(S)-\xi_{j}(S_ {i})\big{\|}_{\mathrm{F}}^{2}\right]\] (B.14)

where we have used \(ab\leq\frac{1}{2}\left(a^{2}+b^{2}\right)\). According to the definition of \(\xi_{i}(S)\) and \(\xi_{i}(S_{j})\) we know the following identity for \(i\neq j\)

\[\mathbb{E}\left[\big{\|}\xi_{i}(S)-\xi_{i}(S_{j})\big{\|}_{ \mathrm{F}}^{2}\right]= \mathbb{E}\Big{[}\Big{\|}\mathbb{E}_{z_{i}^{\prime}}\mathbb{E}_{ \bar{z}}\big{[}\nabla F_{\bar{z}}(A(S^{(i)}))-\nabla F_{\bar{z}}(A(S_{j}^{(i)})) \big{]}\] \[+\mathbb{E}_{z_{i}^{\prime}}\big{[}\nabla F_{z_{i}}(A(S_{j}^{(i)})) -\nabla F_{z_{i}}(A(S^{(i)}))\big{]}\Big{\|}_{\mathrm{F}}^{2}\Big{]}.\] (B.15)It then follows from the inequality \((a+b)^{2}\leq 2\left(a^{2}+b^{2}\right)\) and the Jensen's inequality that

\[\mathbb{E}[\|\xi_{i}(S)-\xi_{i}(S_{j})\|_{\mathrm{F}}^{2}]\leq 2\mathbb{E}[\|\nabla F_{\hat{z}}(A(S^{(i)}))-\nabla F_{\hat{z}}(A(S ^{(i)}_{j}))\|_{\mathrm{F}}^{2}]\] \[+2\mathbb{E}[\|\nabla F_{z_{i}}(A(S^{(i)}_{j}))-\nabla F_{z_{i}}( A(S^{(i)}))\|_{\mathrm{F}}^{2}].\] (B.16)

Since \(S^{(i)}\), \(S^{(i)}_{j}\) and \(S^{(j)}\), \(S^{(j)}_{i}\) are two pairs of neighboring datasets, it follows from the definition of stability that

\[\mathbb{E}\left[\left\|\xi_{i}(S)-\xi_{i}(S_{j})\right\|_{\mathrm{F}}^{2} \right]\leq 4\epsilon_{\mathrm{F}}^{2},\ \ \text{and}\ \ \mathbb{E}\left[\left\|\xi_{j}(S)-\xi_{j}(S_{i})\right\|_{\mathrm{F}}^{2} \right]\leq 4\epsilon_{\mathrm{F}}^{2},\ \ \ \forall i\neq j.\] (B.17)

We can plug the above inequalities back into (B.14) and bound \(J_{2,i,j}\) by

\[J_{2,i,j}=\mathbb{E}\left[\langle\xi_{i}(S),\xi_{j}(S)\rangle\right]\leq 4 \epsilon_{\mathrm{F}}^{2},\ \ \ \forall i\neq j.\] (B.18)

Combining the bound for \(J_{1,i}\) in (B.7) and \(J_{2,i,j}\) in (B.18) and substituting them back into (B.6), it then follows that

\[\mathbb{E}\Big{[}\Big{\|}\sum_{i=1}^{n}\xi_{i}(S)\Big{\|}_{ \mathrm{F}}^{2}\Big{]} =\mathbb{E}\Big{[}\sum_{i=1}^{n}\|\xi_{i}(S)\|_{\mathrm{F}}^{2} \Big{]}+\sum_{i,j\in[n]:i\neq j}\mathbb{E}[\langle\xi_{i}(S),\xi_{j}(S)\rangle]\] \[\leq n\mathbb{E}\left[\mathbb{V}_{\hat{z}}(\nabla F_{\hat{z}}(A(S )))\right]+4n(n-1)\epsilon_{\mathrm{F}}^{2}.\] (B.19)

Plugging the above inequality back into (B.5), using the subadditivity of the square root function, we get

\[n\mathbb{E}[\|\nabla F(A(S))-\nabla F_{S}(A(S))\|_{\mathrm{F}}]\leq 4n \epsilon_{\mathrm{F}}+\sqrt{n\mathbb{E}\left[\mathbb{V}_{\hat{z}}(\nabla F_{ \hat{z}}(A(S)))\right]}.\] (B.20)

The proof is complete. 

### Proof of Theorem 1 - PS generalization error in nonconvex case

In this subsection, we prove Theorem 1, which establishes the PS generalization error of MoDo in the nonconvex case.

Organization of proof.To prove the PS generalization error of MoDo, we first define the concept of Sampling-determined algorithms in Definition 3. This concept has been defined in [22] for the analysis in single-objective learning. Then we show that MoDo is sampling-determined in Proposition 4. Finally, combining Propositions 2-4, we can prove Theorem 1, the MOL uniform stability and PS generalization error of MoDo.

**Definition 3** (Sampling-determined algorithm [22]).: _Let \(A\) be a randomized algorithm that randomly chooses an index sequence \(I(A)=\{i_{t,s}\}\) to compute stochastic gradients. We say a symmetric algorithm \(A\) is sampling-determined if the output model is fully determined by \(\{z_{i}:i\in I(A)\}\)._

**Proposition 4** (MoDo is sampling determined).: _MoDo (Algorithm 1) is sampling determined. In other words, Let \(I(A)=\{i_{t}\}\) be the sequence of index chosen by these algorithms from training set \(S=\{z_{1},\ldots,z_{n}\}\), and \(z_{i}\stackrel{{\mathrm{i.i.d.}}}{{\sim}}\mathcal{P}\) for all \(i\in[n]\) to build stochastic gradients, the output \(A(S)\) is determined by \(\{z_{j}:j\in I(A)\}\). To be precise, \(A(S)\) is independent of \(z_{j}\) if \(j\notin I(A)\)._

Proof of Proposition 4.: Let \(I(A)=\{I_{1},\ldots,I_{T}\}\), \(I_{t}=\{i_{t,s}\}_{s=1}^{3}\) and \(i_{t,s}\in[n]\) for all \(1\leq t\leq T\). And \(S_{I(A)}=\{z_{i_{t,s}}\}\). By the description in Algorithm 1, \(A(S)=G_{z_{I_{T}}}\circ\cdots\circ G_{z_{I_{1}}}(x_{0})\), where \(G_{z}(\cdot)\) is the stochastic update function of the model parameter given random sample \(z\). Therefore, for all possible sample realization \(z\), we have

\[\mathbb{P}(A(S)=x\mid z_{j}=z,j\notin I(A))= \mathbb{P}(G_{z_{I_{T}}}\circ\cdots\circ G_{z_{I_{1}}}(x_{0})=x\mid z _{j}=z,j\notin I(A))\] \[= \mathbb{P}(G_{z_{I_{T}}}\circ\cdots\circ G_{z_{I_{1}}}(x_{0})=x \mid j\notin I(A))\] \[= \mathbb{P}(A(S)=x\mid j\notin I(A))\] (B.21)

where the last equality holds because \(z_{j}\notin S_{I(A)}\), and \(z_{j}\) is independent of all elements in \(S_{I(A)}\) by \(\mathrm{i.i.d.}\) sampling. Therefore, \(A(S)\) is independent of \(z_{j}\) if \(j\notin I(A)\). The proof is complete.

[MISSING_PAGE_EMPTY:19]

**Lemma 4**.: _Suppose Assumptions 1, 2 hold. WLOG, assume \(\inf_{x\in\mathbb{R}^{d}}f_{m,z}(x)<\infty\) for all \(m\in[M]\) and \(z\in\mathcal{Z}\). For any given \(\lambda\in\Delta^{M}\), and stochastic sample \(z\in\mathcal{Z}\), define \(x_{\lambda,z}^{*}=\arg\min_{x\in\mathbb{R}^{d}}F_{z}(x)\lambda\), then \(\inf_{x\in\mathbb{R}^{d}}F_{z}(x)\lambda<\infty\) and \(\|x_{\lambda,z}^{*}\|<\infty\), i.e., there exist finite positive constants \(c_{F^{*}}\) and \(c_{x^{*}}\) such that_

\[\inf_{x\in\mathbb{R}^{d}}F_{z}(x)\lambda\leq c_{F^{*}}\ \ \ {\rm and}\ \ \|x_{\lambda,z}^{*}\|\leq c_{x^{*}}.\] (B.26)

Proof.: Under Assumption 2, for all \(m\in[M]\), \(f_{m,z}(x)\) is strongly convex w.r.t. \(x\), thus has a unique minimizer. Define the minimizer \(x_{m,z}^{*}=\arg\min_{x\in\mathbb{R}^{d}}f_{m,z}(x)\). Since a strongly convex function is coercive, \(\inf_{x\in\mathbb{R}^{d}}f_{m,z}(x)<\infty\), i.e., \(f_{m,z}(x_{m,z}^{*})<\infty\), implies that \(\|x_{m,z}^{*}\|<\infty\).

By Assumption 1, the \(\ell_{f,1}\)-Lipschitz smoothness of \(f_{m,z}(x)\), for \(x\) such that \(\|x\|<\infty\)

\[f_{m,z}(x)\leq f_{m,z}(x_{m,z}^{*})+\langle\nabla f_{m,z}(x_{m,z}^{*}),x-x_{m,z }^{*}\rangle+\frac{\ell_{f,1}}{2}\|x-x_{m,z}^{*}\|^{2}\] \[\leq f_{m,z}(x_{m,z}^{*})+\frac{\ell_{f,1}}{2}\|x-x_{m,z}^{*}\|^{2}<\infty.\] (B.27)

Since \(F_{z}(x)\lambda\) is convex w.r.t. \(x\), for all \(\lambda\in\Delta^{M}\), with \(\lambda=[\lambda_{1},\ldots,\lambda_{M}]^{\top}\), we have

\[F_{z}\Big{(}\frac{1}{M}\sum_{m=1}^{M}x_{m,z}^{*}\Big{)}\lambda\leq\frac{1}{M} \sum_{m=1}^{M}F_{z}(x_{m,z}^{*})\lambda=\frac{1}{M}\sum_{m=1}^{M}\sum_{m^{ \prime}=1}^{M}f_{m^{\prime},z}(x_{m,z}^{*})\lambda_{m^{\prime}}<\infty.\] (B.28)

Therefore, for all \(\lambda\in\Delta^{M}\), we have

\[\inf_{x\in\mathbb{R}^{d}}F_{z}(x)\lambda\leq F_{z}\Big{(}\frac{1}{M}\sum_{m= 1}^{M}x_{m,z}^{*}\Big{)}\lambda<\infty.\] (B.29)

Since \(F_{z}(x)\lambda\) is strongly convex, thus is coercive, we have \(\|x_{\lambda,z}^{*}\|<\infty\), which proves the result. 

**Lemma 5**.: _Suppose Assumptions 1, 2 hold, and define \(\kappa=3\ell_{f,1}/\mu\geq 3\). For any given \(\lambda\in\Delta^{M}\), and a stochastic sample \(z\in\mathcal{Z}\), define \(x_{\lambda,z}^{*}=\arg\min_{x}F_{z}(x)\lambda\). Then by Lemma 4, there exists a positive finite constant \(c_{x,1}\geq c_{x^{*}}\) such that \(\|x_{\lambda,z}^{*}\|\leq c_{x^{*}}\leq c_{x,1}\). Recall the multi-objective gradient update is_

\[G_{\lambda,z}(x)=x-\alpha\nabla F_{z}(x)\lambda\] (B.30)

_with step size \(0\leq\alpha\leq\ell_{f,1}^{-1}\). Defining \(c_{x,2}=(1+\sqrt{2\kappa})c_{x,1}\), we have that_

\[\text{if}\ \ \|x\|\leq c_{x,2},\ \ \text{then}\ \ \|G_{\lambda,z}(x)\|\leq c_{x,2}.\] (B.31)

Proof.: We divide the proof into two cases: 1) when \(\|x\|<c_{x,1}\); and, 2) when \(c_{x,1}\leq\|x\|\leq c_{x,2}\).

1) For the first case, \(\|x\|<c_{x,1}\leq c_{x,2}\), then we have

\[\|G_{\lambda,z}(x)\|\leq \|G_{\lambda,z}(x)-x^{*}\|+\|x^{*}\|\] \[\overset{(a)}{=} \|G_{\lambda,z}(x)-G_{\lambda,z}(x^{*})\|+\|x^{*}\|\overset{(b)}{ \leq}\|x-x^{*}\|+\|x^{*}\|\] \[\leq \|x\|+2\|x^{*}\|\leq 3c_{x,1}\leq(1+\sqrt{6})c_{x,1}\leq(1+\sqrt{2 \kappa})c_{x,1}\leq c_{x,2}\] (B.32)

where \((a)\) follows from \(\nabla F_{z}(x^{*})\lambda=0\), and \((b)\) follows from the non-expansiveness of the gradient update for strongly convex and smooth function.

2) For the second case, \(c_{x,1}\leq\|x\|\leq c_{x,2}\), we first consider \(\alpha=\ell_{f,1}^{-1}\). Let \(\mu^{\prime}=\mu/3\). Note that since \(F_{z}(x)\lambda\) is \(\mu\)-strongly convex, it is also \(\mu^{\prime}\)-strongly convex. By strong convexity and smoothness of \(F_{z}(x)\lambda\), the gradients are co-coercive [36, Theorem 2.1.12], i.e., for any \(x\) we have

\[(\nabla F_{z}(x)\lambda)^{\top}(x-x^{*})\geq\frac{\ell_{f,1}^{-1}\|\nabla F_{z }(x)\lambda\|^{2}}{1+\kappa^{-1}}+\frac{\mu^{\prime}\|x-x^{*}\|^{2}}{1+\kappa ^{-1}}.\] (B.33)Rearranging and applying Cauchy-Schwartz inequality, we have

\[(\nabla F_{z}(x)\lambda)^{\top}x \geq(\nabla F_{z}(x)\lambda)^{\top}x^{*}+\frac{\ell_{f,1}^{-1}\| \nabla F_{z}(x)\lambda\|^{2}}{1+\kappa^{-1}}+\frac{\mu^{\prime}\|x-x^{*}\|^{2} }{1+\kappa^{-1}}\] \[\geq-c_{x,1}\|\nabla F_{z}(x)\lambda\|+\frac{\ell_{f,1}^{-1}\| \nabla F_{z}(x)\lambda\|^{2}}{1+\kappa^{-1}}+\frac{\mu^{\prime}\|x-x^{*}\|^{2} }{1+\kappa^{-1}}.\] (B.34)

By the definition of \(G_{\lambda,z}(x)\),

\[\|G_{\lambda,z}(x)\|^{2}=\left\|x-\frac{1}{\ell_{f,1}}\nabla F_{z}(x)\lambda \right\|^{2}=\|x\|^{2}+\frac{1}{\ell_{f,1}^{2}}\|\nabla F_{z}(x)\lambda\|^{2} -\frac{2}{\ell_{f,1}}(\nabla F_{z}(x)\lambda)^{\top}x.\] (B.35)

Substituting (B.34) into (B.35) yields

\[\|G_{\lambda,z}(x)\|^{2}\leq \|x\|^{2}+\frac{1}{\ell_{f,1}^{2}}\|\nabla F_{z}(x)\lambda\|^{2} +\frac{2}{\ell_{f,1}}\Big{(}c_{x,1}\|\nabla F_{z}(x)\lambda\|-\frac{\ell_{f,1} ^{-1}\|\nabla F_{z}(x)\lambda\|^{2}}{1+\kappa^{-1}}-\frac{\mu^{\prime}\|x-x^{ *}\|^{2}}{1+\kappa^{-1}}\Big{)}\] \[= \|x\|^{2}+\frac{2}{\ell_{f,1}}\Big{(}c_{x,1}\|\nabla F_{z}(x) \lambda\|-\frac{1}{2\ell_{f,1}}(\frac{1-\kappa^{-1}}{1+\kappa^{-1}})\|\nabla F _{z}(x)\lambda\|^{2}-\frac{\mu^{\prime}}{1+\kappa^{-1}}\|x-x^{*}\|^{2}\Big{)}\] \[\leq \|x\|^{2}+\frac{2}{\ell_{f,1}}\sup_{\tau\in\mathbb{R}}\Big{(} \underbrace{c_{x,1}\cdot\tau-\frac{1}{2\ell_{f,1}}(\frac{1-\kappa^{-1}}{1+ \kappa^{-1}})\tau^{2}}_{I_{1}}-\frac{\mu^{\prime}\|x-x^{*}\|^{2}}{1+\kappa^{- 1}}\Big{)}.\] (B.36)

Since \(\kappa\geq 3\), thus \(\frac{1-\kappa^{-1}}{1+\kappa^{-1}}>0\), then \(I_{1}\) is a quadratic function w.r.t. \(\tau\), and is strictly concave, thus can be bounded above by

\[\sup_{\tau\in\mathbb{R}}\ \ c_{x,1}\cdot\tau-\frac{1}{2\ell_{f,1}}(\frac{1- \kappa^{-1}}{1+\kappa^{-1}})\tau^{2}\leq\frac{c_{x,1}^{2}\ell_{f,1}}{2}\frac{ 1+\kappa^{-1}}{1-\kappa^{-1}}.\] (B.37)

Substituting this back into (B.36) gives that

\[\|G_{\lambda,z}(x)\|^{2}\leq \|x\|^{2}+\frac{2}{\ell_{f,1}}\Big{(}\frac{c_{x,1}^{2}\ell_{f,1}}{ 2}\frac{1+\kappa^{-1}}{1-\kappa^{-1}}-\frac{\mu^{\prime}}{1+\kappa^{-1}}\|x- x^{*}\|^{2}\Big{)}\] \[= \|x\|^{2}+c_{x,1}^{2}\frac{1+\kappa^{-1}}{1-\kappa^{-1}}-2\frac{ \kappa^{-1}}{1+\kappa^{-1}}\|x-x^{*}\|^{2}\] \[\leq \|x\|^{2}+c_{x,1}^{2}\frac{1+\kappa^{-1}}{1-\kappa^{-1}}-2\frac{ \kappa^{-1}}{1+\kappa^{-1}}(\|x\|-\|x^{*}\|)^{2}\] \[\leq \underbrace{\|x\|^{2}+2c_{x,1}^{2}-\kappa^{-1}(\|x\|-c_{x,1})^{2} }_{I_{2}}\] (B.38)

where the last inequality follows from \(\kappa\geq 3\), thus \(\frac{1+\kappa^{-1}}{1-\kappa^{-1}}\leq 2\), \(-2\frac{\kappa^{-1}}{1+\kappa^{-1}}\leq-\kappa^{-1}\), and \(\|x^{*}\|\leq c_{x,1}\leq\|x\|\) by assumption.

For \(c_{x,1}\leq\|x\|\leq c_{x,2}\), \(I_{2}\) is a strictly convex quadratic function of \(\|x\|\), which achieves its maximum at \(\|x\|=c_{x,1}\) or \(\|x\|=c_{x,2}\). Therefore,

\[\|G_{\lambda,z}(x)\|^{2}\leq \max\{3c_{x,1}^{2},c_{x,2}^{2}+2c_{x,1}^{2}-\kappa^{-1}(c_{x,2}-c _{x,1})^{2}\}\] \[\stackrel{{(c)}}{{=}} \max\{3c_{x,1}^{2},c_{x,2}^{2}\}\stackrel{{(d)}}{{<}}c_{ x,2}^{2}\] (B.39)

where \((c)\) follows from the definition that \(c_{x,2}=(1+\sqrt{2\kappa})c_{x,1}\); \((d)\) follows from \(\kappa\geq 3\), and thus \(3c_{x,1}^{2}<(1+\sqrt{2\kappa})^{2}c_{x,1}^{2}=c_{x,2}^{2}\).

We have proved the case for \(\alpha=\ell_{f,1}^{-1}\). The result for \(0\leq\alpha<\ell_{f,1}^{-1}\) follows by observing that,

\[\|G_{\lambda,z}(x)\|= \|x-\alpha\nabla F_{z}(x)\lambda\|\] \[= \|(1-\alpha\ell_{f,1})x+\alpha\ell_{f,1}(x-\ell_{f,1}^{-1}\nabla F _{z}(x)\lambda)\|\] \[\leq (1-\alpha\ell_{f,1})\|x\|+\alpha\ell_{f,1}\|x-\ell_{f,1}^{-1} \nabla F_{z}(x)\lambda\|\leq c_{x,2}.\] (B.40)

The proof is complete.

**Lemma 6**.: _Suppose Assumptions 1, 2 hold. For all \(\lambda\in\Delta^{M}\) and \(z\in S\), define \(x_{\lambda,z}^{*}=\arg\min_{x}F_{z}(x)\lambda\), then there exist finite positive constants \(c_{F^{*}}\) and \(c_{x^{*}}\) such that \(F_{z}(x_{\lambda,z}^{*})\lambda\leq c_{F^{*}}\) and \(\|x_{\lambda,z}^{*}\|\leq c_{x^{*}}\). And for \(x\in\mathbb{R}^{d}\) such that \(\|x\|\) is bounded, i.e., there exists a finite positive constant \(c_{x}\) such that \(\|x\|\leq c_{x}\), then_

\[\|\nabla F_{z}(x)\lambda\|\leq\ell_{f,1}(c_{x}+c_{x^{*}}),\qquad\text{and} \qquad F_{z}(x)\lambda\leq\frac{\ell_{f,1}}{2}(c_{x}+c_{x^{*}})^{2}+c_{F^{*}}.\] (B.41)

Proof.: Under Assumptions 1, 2, by Lemma 4, there exist finite positive constants \(c_{F^{*}}\) and \(c_{x^{*}}\) such that \(F_{z}(x_{\lambda,z}^{*})\lambda\leq c_{F^{*}}\) and \(\|x_{\lambda,z}^{*}\|\leq c_{x^{*}}\).

By Assumption 1, the \(\ell_{f,1}\)-Lipschitz continuity of the gradient \(\nabla F_{z}(x)\lambda\), we have

\[\|\nabla F_{z}(x)\lambda\|= \|\nabla F_{z}(x)\lambda-\nabla F_{z}(x_{\lambda,z}^{*})\lambda\|\] \[\leq \ell_{f,1}\|x-x_{\lambda,z}^{*}\|\leq\ell_{f,1}(\|x\|+\|x_{ \lambda,z}^{*}\|)\leq\ell_{f,1}(c_{x}+c_{x^{*}})\] (B.42)

where the first equality uses the fact that \(\nabla F_{z}(x_{\lambda,z}^{*})\lambda=0\).

For the function value, by Assumption 1, the \(\ell_{f,1}\)-Lipschitz smoothness of \(F_{z}(x)\lambda\), we have

\[F_{z}(x)\lambda\leq F_{z}(x_{\lambda,z}^{*})\lambda+\langle\nabla F_{z}(x_{\lambda,z}^{* })\lambda,x-x_{\lambda,z}^{*}\rangle+\frac{\ell_{f,1}}{2}\|x-x_{\lambda,z}^{*} \|^{2}\] \[\leq F_{z}(x_{\lambda,z}^{*})\lambda+\frac{\ell_{f,1}}{2}\|x-x_{ \lambda,z}^{*}\|^{2}\] \[\leq c_{F^{*}}+\frac{\ell_{f,1}}{2}(c_{x}+c_{x^{*}})^{2}\] (B.43)

from which the proof is complete. 

**Corollary 4** (\(x_{t}\) bounded on the MoDo trajectory).: _Suppose Assumptions 1, 2 hold. Define \(\kappa=3\ell_{f,1}/\mu\) and \(x_{\lambda,z}^{*}=\arg\min_{x}F_{z}(x)\lambda\) with \(\lambda\in\Delta^{M}\). Then there exists a finite positive constant \(c_{x^{*}}\) such that \(\|x_{\lambda,z}^{*}\|\leq c_{x^{*}}\). Choose the initial iterate to be bounded, i.e., there exists a finite positive constant \(c_{x_{0}}\) such that \(\|x_{0}\|\leq c_{x_{0}}\), then for \(\{x_{t}\}\) generated by MoDo algorithm with \(\alpha_{t}=\alpha\) and \(0\leq\alpha\leq\ell_{f,1}^{-1}\), we have_

\[\|x_{t}\|\leq c_{x},\ \ \ \mathrm{with}\ \ \ c_{x}=\max\{(1+\sqrt{2\kappa})c_{x^{ *}},c_{x_{0}}\}.\] (B.44)

Proof of Corollary 4.: Under Assumptions 1, 2, by Lemma 4, \(\|x_{\lambda,z}^{*}\|<\infty\), i.e., there exists a finite positive constant \(c_{x^{*}}\) such that \(\|x_{\lambda,z}^{*}\|\leq c_{x^{*}}\). Let \(c_{x,1}=\max\{(1+\sqrt{2\kappa})^{-1}c_{x_{0}},c_{x^{*}}\}\), and \(c_{x,2}=(1+\sqrt{2\kappa})c_{x,1}=\max\{c_{x_{0}},(1+\sqrt{2\kappa})c_{x^{*}}\}\) in Lemma 5. We then consider the following two cases:

1) If \((1+\sqrt{2\kappa})c_{x^{*}}\leq c_{x_{0}}\), then \(\|x_{\lambda,z}^{*}\|\leq c_{x^{*}}\leq(1+\sqrt{2\kappa})^{-1}c_{x_{0}}\). Then it satisfies the condition in Lemma 5 that \(\|x_{\lambda,z}^{*}\|\leq c_{x,1}\) and \(\|x_{0}\|\leq c_{x,2}\). Applying Lemma 5 yields \(\|x_{1}\|\leq c_{x,2}\).

2) If \((1+\sqrt{2\kappa})c_{x^{*}}>c_{x_{0}}\), then \(\|x_{0}\|\leq c_{x_{0}}<(1+\sqrt{2\kappa})c_{x^{*}}\). Then it satisfies the condition in Lemma 5 that \(\|x_{\lambda,z}^{*}\|\leq c_{x,1}\) and \(\|x_{0}\|\leq c_{x,2}\). Applying Lemma 5 yields \(\|x_{1}\|\leq c_{x,2}\).

Therefore, (B.44) holds for \(t=1\). We then prove by induction that (B.44) also holds for \(t\in[T]\). Assume (B.44) holds at \(1\leq k\leq T-1\), i.e.,

\[\|x_{k}\|\leq c_{x}=c_{x,2}\] (B.45)

Then by Lemma 5, at \(k+1\),

\[\|x_{k+1}\|=\|G_{\lambda_{k+1},z_{k,3}}(x_{k})\|\leq c_{x,2}.\] (B.46)

Since \(\|x_{1}\|\leq c_{x,2}\), for \(t=0,\ldots,T-1\), we have

\[\|x_{t+1}\|=\|G_{\lambda_{t+1},z_{t,3}}(x_{t})\|\leq c_{x,2}.\] (B.47)

Therefore, by mathematical induction, \(\|x_{t}\|\leq c_{x,2}=c_{x},\text{for all }t\in[T]\). The proof is complete.

Proof of Lemma 1.: By Corollary 4, for \(\{x_{t}\}\) generated by MoDo algorithm with \(\alpha_{t}=\alpha\) and \(0\leq\alpha\leq\ell_{f,1}^{-1}\), we have

\[\|x_{t}\|\leq c_{x},\ \ \ \text{with}\ \ c_{x}=\max\{(1+\sqrt{2\kappa})c_{x^{*}},c_ {x_{0}}\}.\] (B.48)

According to Lemma 6, define \(\ell_{f}=\ell_{f,1}(c_{x}+c_{x^{*}})\), and \(\ell_{F}=\sqrt{M}\ell_{f}\), then it holds for all \(\lambda\in\Delta^{M}\)

\[\|\nabla F(x_{t})\lambda\|\leq\ell_{f}\ \ \ \text{and}\ \ \ \|\nabla F(x_{t})\|\leq\| \nabla F(x_{t})\|_{\mathrm{F}}\leq\ell_{F}.\] (B.49)

### Proof of Theorem 2 - PS generalization error in strongly convex case

Technical challenges.One challenge that the strongly convex objectives are not Lipschitz continuous for \(x\in\mathbb{R}^{d}\) is addressed by Lemma 1. Another challenge compared to static weighting or single-objective learning is that the MoDo algorithm involves the update of two coupled sequences \(\{x_{t}\}\) and \(\{\lambda_{t}\}\). Consequently, the traditional standard argument that the SGD update for strongly convex objectives has the contraction property [14] does not necessarily hold in our case since the weighting parameter \(\lambda\) is changing, as detailed in Section B.4.1. Nevertheless, we manage to derive a tight stability bound when \(\gamma=\mathcal{O}(T^{-1})\), as detailed in Section B.4.3.

Organization of proof.In Section B.4.1, we prove the properties of the MoDo update, including expansiveness or non-expansiveness and boundedness. Building upon these properties, in Section B.4.3, we prove the upper bound of argument stability in Theorem 5, and the upper bound of MOL uniform stability. To show the tightness of the upper bound, in Section B.4.4, Theorem 6, we derive a matching lower bound of MOL uniform stability. Combining the upper bound in Section B.4.3 and the lower bound in Section B.4.4 leads to the results in Theorem 2, whose proof is in Section B.4.5.

#### b.4.1 Expansiveness and boundedness of MoDo update

In this section, we prove the properties of the update function of MoDo at each iteration, including boundedness and approximate expansiveness, which is then used to derive the algorithm stability. For \(z,z_{1},z_{2}\in S\), \(\lambda\in\Delta^{M}\), recall that the update functions of MoDo is

\[G_{x,z_{1},z_{2}}(\lambda) =\Pi_{\Delta^{M}}\left(\lambda-\gamma\nabla F_{z_{1}}(x)^{\top} \nabla F_{z_{2}}(x)\lambda\right)\] \[G_{\lambda,z}(x) =x-\alpha\nabla F_{z}(x)\lambda.\]

**Lemma 7** (Boundedness of update function of MoDo).: _Let \(\ell_{f}\) be a positive constant. If \(\|\nabla F_{z}(x)\lambda\|\leq\ell_{f}\) for all \(\lambda\in\Delta^{M}\), \(z\in S\) and \(x\in\{x_{t}\}_{t=1}^{T}\) generated by the MoDo algorithm with step size \(\alpha_{t}\leq\alpha\), then \(G_{\lambda,z}(x)\) is \((\alpha\ell_{f})\)-bounded on the trajectory of MoDo, i.e.,_

\[\sup_{x\in\{x_{t}\}_{t=1}^{T}}\|G_{\lambda,z}(x)-x\|\leq\alpha\ell_{f}.\] (B.50)

Proof.: For all \(x\in\{x_{t}\}_{t=1}^{T}\), \(\lambda\in\Delta^{M}\), and \(z\in S\), since \(\|\nabla F_{z}(x)\lambda\|\leq\ell_{f}\), we have

\[\|G_{\lambda,z}(x)-x\|\leq \|\alpha\nabla F_{z}(x)\lambda\|\leq\alpha\ell_{f}\] (B.51)

which proves the boundedness. 

**Lemma 8** (Properties of update function of MoDo in convex case).: _Suppose Assumptions 1, 2 hold. Let \(\ell_{f}\) be a positive constant. If for all \(\lambda,\lambda^{\prime}\in\Delta^{M}\), \(z\in S\), and \(x\in\{x_{t}\}_{t=1}^{T}\), \(x^{\prime}\in\{x_{t}^{\prime}\}_{t=1}^{T}\) generated by the MoDo algorithm on datasets \(S\) and \(S^{\prime}\), respectively, we have \(\|\nabla F_{z}(x)\lambda\|\leq\ell_{f}\), \(\|\nabla F_{z}(x^{\prime})\lambda^{\prime}\|\leq\ell_{f}\), and \(\|\nabla F_{z}(x)\|\leq\ell_{F}\), \(\|\nabla F_{z}(x^{\prime})\|\leq\ell_{F}\), and step sizes of MoDo satisfy \(\alpha_{t}\leq\alpha\), \(\gamma_{t}\leq\gamma\), it holds that_

\[\|G_{\lambda,z}(x)-G_{\lambda^{\prime},z}(x^{\prime})\|^{2}\leq (1-2\alpha\mu+2\alpha^{2}\ell_{f,1}^{2})\|x-x^{\prime}\|^{2}\] \[+2\alpha\ell_{F}\|x-x^{\prime}\|\|\lambda-\lambda^{\prime}\|+2 \alpha^{2}\ell_{F}^{2}\|\lambda-\lambda^{\prime}\|^{2}\] (B.52) \[\|G_{x,z_{1},z_{2}}(\lambda)-G_{x^{\prime},z_{1},z_{2}}(\lambda^{ \prime})\|^{2}\leq \Big{(}(1+\ell_{F}^{2}\gamma)^{2}+(1+\ell_{F}^{2}\gamma)\ell_{g,1 }\gamma\Big{)}\|\lambda-\lambda^{\prime}\|^{2}\] \[+\Big{(}(1+\ell_{F}^{2}\gamma)\ell_{g,1}\gamma+\ell_{g,1}^{2} \gamma^{2}\Big{)}\|x-x^{\prime}\|^{2}.\] (B.53)Proof.: The squared norm of the difference of \(G_{\lambda,z}(x)\) and \(G_{\lambda^{\prime},z}(x^{\prime})\) can be bounded by

\[\|G_{\lambda,z}(x)-G_{\lambda^{\prime},z}(x^{\prime})\|^{2}\] \[= \|x-x^{\prime}\|^{2}-2\alpha\langle x-x^{\prime},\nabla F_{z}(x) \lambda-\nabla F_{z}(x^{\prime})\lambda^{\prime}\rangle+\alpha^{2}\|\nabla F_{z }(x)\lambda-\nabla F_{z}(x^{\prime})\lambda^{\prime}\|^{2}\] \[\overset{(a)}{\leq} \|x-x^{\prime}\|^{2}-2\alpha\langle x-x^{\prime},(\nabla F_{z}(x )-\nabla F_{z}(x^{\prime}))\lambda\rangle+2\alpha^{2}\|(\nabla F_{z}(x)-\nabla F _{z}(x^{\prime}))\lambda\|^{2}\] \[+2\alpha\langle x-x^{\prime},\nabla F_{z}(x^{\prime})(\lambda^{ \prime}-\lambda)\rangle+2\alpha^{2}\|\nabla F_{z}(x^{\prime})(\lambda-\lambda^ {\prime})\|^{2}\] \[\overset{(b)}{\leq} (1-2\alpha\mu+2\alpha^{2}\ell_{f,1}^{2})\|x-x^{\prime}\|^{2}+2 \alpha\langle x-x^{\prime},\nabla F_{z}(x^{\prime})(\lambda^{\prime}-\lambda) \rangle+2\alpha^{2}\ell_{F}^{2}\|\lambda^{\prime}-\lambda\|^{2}\] \[\overset{(c)}{\leq} (1-2\alpha\mu+2\alpha^{2}\ell_{f,1}^{2})\|x-x^{\prime}\|^{2}+2 \alpha\ell_{F}\|x-x^{\prime}\|\|\lambda^{\prime}-\lambda\|+2\alpha^{2}\ell_{F} ^{2}\|\lambda^{\prime}-\lambda\|^{2}\] (B.54)

where \((a)\) follows from rearranging and that \(\|a+b\|^{2}\leq 2\|a\|^{2}+2\|b\|^{2}\); \((b)\) follows from the \(\mu\)-strong convexity of \(F_{z}(x)\lambda\), \(\ell_{f,1}\)-Lipschitz continuity of \(\nabla F_{z}(x)\lambda\), and that \(\|\nabla F_{z}(x^{\prime})\|\leq\ell_{F}\) for \(x^{\prime}\in\{x^{\prime}_{t}\}_{t=1}^{T}\); and, \((c)\) follows from Cauchy-Schwartz inequality.

And \(\|G_{x,z_{1},z_{2}}(\lambda)-G_{x^{\prime},z_{1},z_{2}}(\lambda^{\prime})\|\) can be bounded by

\[\|G_{x,z_{1},z_{2}}(\lambda)-G_{x^{\prime},z_{1},z_{2}}(\lambda^{ \prime})\|\] \[= \|\Pi_{\Delta^{M}}(\lambda-\gamma\big{(}\nabla F_{z_{1}}(x)^{ \top}\nabla F_{z_{2}}(x)\big{)}\lambda)-\Pi_{\Delta^{M}}(\lambda^{\prime}- \gamma\big{(}\nabla F_{z_{1}}(x^{\prime})^{\top}\nabla F_{z_{2}}(x^{\prime}) \big{)}\lambda^{\prime})\|\] \[\overset{(d)}{\leq} \|\lambda-\lambda^{\prime}-\gamma(\nabla F_{z_{1}}(x)^{\top} \nabla F_{z_{2}}(x)\lambda-\nabla F_{z_{1}}(x^{\prime})^{\top}\nabla F_{z_{2} }(x^{\prime})\lambda^{\prime})\|\] \[\overset{(e)}{\leq} \|\lambda-\lambda^{\prime}\|+\gamma\|\nabla F_{z_{1}}(x)^{\top} \nabla F_{z_{2}}(x)(\lambda-\lambda^{\prime})\|+\gamma\|(\nabla F_{z_{1}}(x)^{ \top}\nabla F_{z_{2}}(x)-\nabla F_{z_{1}}(x^{\prime})^{\top}\nabla F_{z_{2}}(x ^{\prime}))\lambda^{\prime}\|\] \[\overset{(f)}{\leq} \|\lambda-\lambda^{\prime}\|+\gamma\ell_{F}^{2}\|\lambda-\lambda ^{\prime}\|+\gamma\|(\nabla F_{z_{1}}(x)^{\top}\nabla F_{z_{2}}(x)-\nabla F_{z _{1}}(x^{\prime})^{\top}\nabla F_{z_{2}}(x^{\prime}))\lambda^{\prime}\|\] \[\overset{(g)}{\leq} \|\lambda-\lambda^{\prime}\|+\gamma\ell_{F}^{2}\|\lambda-\lambda ^{\prime}\|+\gamma\big{(}\|(\nabla F_{z_{1}}(x)-\nabla F_{z_{1}}(x^{\prime}))^{ \top}\nabla F_{z_{2}}(x)\lambda^{\prime}\|\] \[+\|\nabla F_{z_{1}}(x)^{\top}(\nabla F_{z_{2}}(x)-\nabla F_{z_{2}}( x^{\prime}))\lambda^{\prime}\|\big{)}\] \[\overset{(h)}{\leq} (1+\ell_{F}^{2}\gamma)\|\lambda-\lambda^{\prime}\|+(\ell_{f}\ell_ {F,1}+\ell_{F}\ell_{f,1})\gamma\|x-x^{\prime}\|\] (B.55)

where \((d)\) follows from non-expansiveness of projection; \((e)\) follows from triangle inequality, \((f)\) follows from \(\|\nabla F_{z}(x)\|\leq\ell_{F}\) for \(x\in\{x^{\prime}_{t}\}_{t=1}^{T},(g)\) follows from triangle inequality; and \((h)\) follows from \(\ell_{f,1}\)-Lipschitz continuity of \(\nabla F_{z}(x)\lambda^{\prime}\), \(\ell_{F,1}\)-Lipschitz continuity of \(\nabla F_{z}(x)\), \(\|\nabla F_{z}(x)\|\leq\ell_{F}\) for \(x\in\{x^{\prime}_{t}\}_{t=1}^{T}\), and \(\|\nabla F_{z}(x)\lambda^{\prime}\|\leq\ell_{f}\) for \(x\in\{x_{t}\}_{t=1}^{T}\).

Let \(\ell_{g,1}=\ell_{f}\ell_{F,1}+\ell_{F}\ell_{f,1}\). Taking square on both sides of (B.55) yields

\[\|G_{x,z_{1},z_{2}}(\lambda)-G_{x^{\prime},z_{1},z_{2}}(\lambda^{ \prime})\|^{2}\] \[\leq \Big{(}(1+\ell_{F}^{2}\gamma)\|\lambda-\lambda^{\prime}\|+\ell_{g,1}\gamma\|x-x^{\prime}\|\Big{)}^{2}\] \[= (1+\ell_{F}^{2}\gamma)^{2}\|\lambda-\lambda^{\prime}\|^{2}+2(1+ \ell_{F}^{2}\gamma)\ell_{g,1}\gamma\|\lambda-\lambda^{\prime}\|\|x-x^{\prime}\|+ \ell_{g,1}^{2}\gamma^{2}\|x-x^{\prime}\|^{2}\] \[\leq (1+\ell_{F}^{2}\gamma)^{2}\|\lambda-\lambda^{\prime}\|^{2}+(1+ \ell_{F}^{2}\gamma)\ell_{g,1}\gamma(\|\lambda-\lambda^{\prime}\|^{2}+\|x-x^{ \prime}\|^{2})+\ell_{g,1}^{2}\gamma^{2}\|x-x^{\prime}\|^{2}\] \[= \Big{(}(1+\ell_{F}^{2}\gamma)^{2}+(1+\ell_{F}^{2}\gamma)\ell_{g,1}\gamma\Big{)}\|\lambda-\lambda^{\prime}\|^{2}+\Big{(}(1+\ell_{F}^{2}\gamma) \ell_{g,1}\gamma+\ell_{g,1}^{2}\gamma^{2}\Big{)}\|x-x^{\prime}\|^{2}.\] (B.56)

The proof is complete. 

#### b.4.2 Growth recursion

**Lemma 9** (Growth recursion with approximate expansiveness).: _Fix an arbitrary sequence of updates \(G_{1},\ldots,G_{T}\) and another sequence \(G_{1}^{\prime},\ldots,G_{T}^{\prime}\). Let \(x_{0}=x^{\prime}_{0}\) be a starting point in \(\Omega\) and define \(\delta_{t}=\|x^{\prime}_{t}-x_{t}\|\) where \(x_{t},x^{\prime}_{t}\) are defined recursively through_

\[x_{t+1}=G_{t}(x_{t}),\quad x^{\prime}_{t+1}=G^{\prime}_{t}(x^{\prime}_{t}

_Let \(\eta_{t}>0,\nu_{t}\geq 0\), and \(\varsigma_{t}\geq 0\). Then, for any \(p>0\), and \(t\in[T]\), we have the recurrence relation (with \(\delta_{0}=0\))_

\[\delta_{t+1}^{2}\leq\left\{\begin{array}{ll}\eta_{t}\delta_{t}^{2}+\nu_{t},&G _{t}=G_{t}^{\prime}\text{ is }(\eta_{t},\nu_{t})\text{-approximately expansive in square,}\\ (1+p)\min\{\eta_{t}\delta_{t}^{2}+\nu_{t},\delta_{t}^{2}\}+(1+\frac{1}{p})4 \varsigma_{t}^{2}&G_{t}\text{ and }G_{t}^{\prime}\text{ are }\varsigma_{t}\text{-bounded},\\ &G_{t}\text{ is }(\eta_{t},\nu_{t})\text{-approximately expansive in square.}\end{array}\right.\]

Proof.: When \(G_{t}\) and \(G_{t}^{\prime}\) are \(\varsigma_{t}\)-bounded, we can bound \(\delta_{t+1}\) by

\[\delta_{t+1}=\|x_{t+1}-x_{t+1}^{\prime}\|= \|G_{t}(x_{t})-G_{t}^{\prime}(x_{t}^{\prime})\|\] \[= \|G_{t}(x_{t})-x_{t}-G_{t}^{\prime}(x_{t}^{\prime})+x_{t}^{ \prime}+x_{t}-x_{t}^{\prime}\|\] \[\leq \|G_{t}(x_{t})-x_{t}\|+\|G_{t}^{\prime}(x_{t}^{\prime})-x_{t}^{ \prime}\|+\|x_{t}-x_{t}^{\prime}\|\] \[\leq 2\varsigma_{t}+\delta_{t}.\] (B.57)

Alternatively, when \(G_{t}\) and \(G_{t}^{\prime}\) are \(\varsigma_{t}\)-bounded, \(G_{t}\) is \((\eta_{t},\nu_{t})\)-approximately expansive, we have

\[\delta_{t+1}=\|x_{t+1}-x_{t+1}^{\prime}\|= \|G_{t}(x_{t})-G_{t}^{\prime}(x_{t}^{\prime})\|\] \[= \|G_{t}(x_{t})-G_{t}(x_{t}^{\prime})+G_{t}(x_{t}^{\prime})-G_{t}^ {\prime}(x_{t}^{\prime})\|\] \[\leq \|G_{t}(x_{t})-G_{t}(x_{t}^{\prime})\|+\|G_{t}(x_{t}^{\prime})-G_ {t}^{\prime}(x_{t}^{\prime})\|\] \[\leq \eta_{t}\delta_{t}+\nu_{t}+\|G_{t}(x_{t}^{\prime})-x_{t}^{\prime }-G_{t}^{\prime}(x_{t}^{\prime})+x_{t}^{\prime}\|\] \[\leq \eta_{t}\delta_{t}+\nu_{t}+\|G_{t}(x_{t}^{\prime})-x_{t}^{\prime }\|+\|G_{t}^{\prime}(x_{t}^{\prime})-x_{t}^{\prime}\|\] \[\leq \eta_{t}\delta_{t}+\nu_{t}+2\varsigma_{t}.\] (B.58)

When \(G_{t}=G_{t}^{\prime}\), is \((\eta_{t},\nu_{t})\)-approximately expansive in square, given \(\delta_{t}^{2}\), \(\delta_{t+1}^{2}\) can be bounded by

\[\delta_{t+1}^{2}=\|x_{t+1}-x_{t+1}^{\prime}\|^{2}=\|G_{t}(x_{t})-G_{t}(x_{t}^ {\prime})\|^{2}\leq\eta_{t}\|x_{t}-x_{t}^{\prime}\|^{2}+\nu_{t}=\eta_{t}\delta _{t}^{2}+\nu_{t}.\] (B.59)

When \(G_{t}\) and \(G_{t}^{\prime}\) are \(\varsigma_{t}\)-bounded, applying (B.57), we can bound \(\delta_{t+1}^{2}\) by

\[\delta_{t+1}^{2}\leq(\delta_{t}+2\varsigma_{t})^{2}\leq(1+p)\delta_{t}^{2}+( 1+1/p)4\varsigma_{t}^{2}\] (B.60)

where \(p>0\) and the last inequality follows from \((a+b)^{2}\leq(1+p)a^{2}+(1+1/p)b^{2}\).

Alternatively, when \(G_{t}\) and \(G_{t}^{\prime}\) are \(\varsigma_{t}\)-bounded, \(G_{t}\) is \((\eta_{t},\nu_{t})\)-approximately expansive in square, the following holds

\[\delta_{t+1}^{2}=\|x_{t+1}-x_{t+1}^{\prime}\|^{2}= \|G_{t}(x_{t})-G_{t}^{\prime}(x_{t}^{\prime})\|^{2}\] \[= \|G_{t}(x_{t})-G_{t}(x_{t}^{\prime})+G_{t}(x_{t}^{\prime})-G_{t}^ {\prime}(x_{t}^{\prime})\|^{2}\] \[\leq (1+p)\|G_{t}(x_{t})-G_{t}(x_{t}^{\prime})\|^{2}+(1+1/p)\|G_{t}(x _{t}^{\prime})-G_{t}^{\prime}(x_{t}^{\prime})\|^{2}\] \[\leq (1+p)(\eta_{t}\delta_{t}^{2}+\nu_{t})+(1+1/p)\|G_{t}(x_{t}^{ \prime})-x_{t}^{\prime}-G_{t}^{\prime}(x_{t}^{\prime})+x_{t}^{\prime}\|^{2}\] \[\leq (1+p)(\eta_{t}\delta_{t}^{2}+\nu_{t})+2(1+1/p)(\|G_{t}(x_{t}^{ \prime})-x_{t}^{\prime}\|^{2}+\|G_{t}^{\prime}(x_{t}^{\prime})-x_{t}^{\prime}\| ^{2})\] \[\leq (1+p)(\eta_{t}\delta_{t}^{2}+\nu_{t})+(1+1/p)4\varsigma_{t}^{2}.\] (B.61)

The proof is complete. 

#### b.4.3 Upper bound of MOL uniform stability

In Theorem 5 we bound the argument stability, which is then used to derive the MOL uniform stability and PS generalization error in Theorem 2.

**Theorem 5** (Argument stability bound in strongly convex case).: _Suppose Assumptions 1, 2, hold. Let \(A\) be the MoDo algorithm in Algorithm 1. Choose the step sizes \(\alpha_{t}\leq\alpha\leq\min\{1/(2\ell_{f,1}),\mu/(2\ell_{f,1}^{2})\}\), and \(\gamma_{t}\leq\gamma\leq\min\{\frac{\mu^{2}}{120\ell_{f,g}^{2}},\frac{1}{8(3 \ell_{f}^{2}+2\ell_{g,1})}\}/T\). Then it holds that_

\[\mathbb{E}_{A}[\|A(S)-A(S^{\prime})\|^{2}]\leq\frac{48}{\mu n}\ell_{f}^{2}\Big{(} \alpha+\frac{12+4M\ell_{f}^{2}}{\mu n}+\frac{10M\ell_{f}^{4}\gamma}{\mu}\Big{)}.\] (B.62)Proof of Theorem 5.: Under Assumptions 1, 2, Lemma 1 implies that for \(\{x_{t}\}\) generated by the MoDo algorithm, and for all \(\lambda\in\Delta^{M}\), and for all \(m\in[M]\),

\[\|\nabla F_{z}(x_{t})\lambda\|\leq\ell_{f,1}(c_{x}+c_{x^{*}})=\ell_{f}.\ \ \ \text{and}\ \ \ \|\nabla F_{z}(x_{t})\|\leq\|\nabla F_{z}(x_{t})\|_{\mathbb{F}}\leq\sqrt{M} \ell_{f}=\ell_{F}.\] (B.63)

For notation simplicity, denote \(\delta_{t}=\|x_{t}-x_{t}^{\prime}\|\), \(\zeta_{t}=\|\lambda_{t}-\lambda_{t}^{\prime}\|\), \(x_{T}=A_{T}(S)\) and \(x_{T}^{\prime}=A_{T}(S^{\prime})\). Denote the index of the different sample in \(S\) and \(S^{\prime}\) as \(i^{*}\), and the set of indices selected at the \(t\)-th iteration as \(I_{t}\), i.e., \(I_{t}=\{i_{t,s}\}_{s=1}^{3}\). When \(i^{*}\notin I_{t}\), for any \(c_{1}>0\), based on Lemma 8, we have

\[\delta_{t+1}^{2}\leq (1-2\alpha_{t}\mu+2\alpha_{t}^{2}\ell_{f,1}^{2})\delta_{t}^{2}+2 \alpha_{t}\ell_{F}\delta_{t}\zeta_{t+1}+2\alpha_{t}^{2}\ell_{F}^{2}\zeta_{t+1}^ {2}\] \[\leq (1-2\alpha_{t}\mu+2\alpha_{t}^{2}\ell_{f,1}^{2})\delta_{t}^{2}+ \alpha_{t}\ell_{F}(c_{1}\delta_{t}^{2}+c_{1}^{-1}\zeta_{t+1}^{2})+2\alpha_{t}^ {2}\ell_{F}^{2}\zeta_{t+1}^{2}\] \[\leq (1-\alpha_{t}\mu)\delta_{t}^{2}+\alpha_{t}\ell_{F}(c_{1}\delta_{t }^{2}+c_{1}^{-1}\zeta_{t+1}^{2})+2\alpha_{t}^{2}\ell_{F}^{2}\zeta_{t+1}^{2}\] (B.64)

where the second last inequality is due to Young's inequality; the last inequality is due to choosing \(\alpha_{t}\leq\mu/(2\ell_{f,1}^{2})\).

When \(i^{*}\in I_{t}\), from Lemma 7, the \((\alpha_{t}\ell_{f})\)-boundedness of the update at \(t\)-th iteration, and Lemma 9, the growth recursion, for a given constant \(p>0\), we have

\[\delta_{t+1}^{2}\leq(1+p)\delta_{t}^{2}+(1+1/p)4\alpha_{t}^{2}\ell_{f}^{2}.\] (B.65)

Taking expectation of \(\delta_{t+1}^{2}\) over \(I_{t}\), we have

\[\mathbb{E}_{I_{t}}[\delta_{t+1}^{2}]\leq \mathbb{P}(i^{*}\notin I_{t})\Big{(}(1-\alpha_{t}\mu)\delta_{t}^{ 2}+\alpha_{t}\ell_{F}c_{1}\delta_{t}^{2}+(\alpha_{t}\ell_{F}c_{1}^{-1}+2\alpha _{t}^{2}\ell_{F}^{2})\mathbb{E}_{I_{t}}[\zeta_{t+1}^{2}\mid i^{*}\notin I_{t}] \Big{)}\] \[\qquad\qquad+\mathbb{P}(i^{*}\in I_{t})\Big{(}(1+p)\delta_{t}^{ 2}+(1+1/p)4\alpha_{t}^{2}\ell_{f}^{2}\Big{)}\] \[\leq \Big{(}1-\alpha_{t}(\mu-\ell_{F}c_{1})\mathbb{P}(i^{*}\notin I_{ t})+p\mathbb{P}(i^{*}\in I_{t})\Big{)}\delta_{t}^{2}\] \[\quad+\alpha_{t}\underbrace{(\ell_{F}c_{1}^{-1}+2\alpha\ell_{F}^ {2})}_{c_{2}}\mathbb{E}_{I_{t}}[\zeta_{t+1}^{2}\mid i^{*}\notin I_{t}] \mathbb{P}(i^{*}\notin I_{t})+\Big{(}1+\frac{1}{p}\Big{)}\mathbb{P}(i^{*}\in I _{t})4\alpha_{t}^{2}\ell_{f}^{2}.\] (B.66)

At each iteration of MoDo, we randomly select three independent samples (instead of one) from the training set \(S\). Then the probability of selecting the different sample from \(S\) and \(S^{\prime}\) at the \(t\)-th iteration, \(\mathbb{P}(i^{*}\in I_{t})\) in the above equation, can be computed as follows

\[\mathbb{P}(i^{*}\in I_{t})=1-\Big{(}\frac{n-1}{n}\Big{)}^{3}\leq\frac{3}{n}.\] (B.67)

Consequently, the probability of selecting the same sample from \(S\) and \(S^{\prime}\) at the \(t\)-th iteration is \(\mathbb{P}(i^{*}\notin I_{t})=1-\mathbb{P}(i^{*}\in I_{t})\).

Let \(\ell_{g,1}=\ell_{f}\ell_{F,1}+\ell_{F}\ell_{f,1}\). Recalling when \(i^{*}\notin I_{t}\), \(\zeta_{t+1}\leq(1+\ell_{F}^{2}\gamma_{t})\zeta_{t}+2\gamma_{t}\ell_{g,1}\delta _{t}\) from Lemma 8, it follows that

\[\zeta_{t+1}^{2}\leq \Big{(}(1+\ell_{F}^{2}\gamma_{t})^{2}+(1+\ell_{F}^{2}\gamma_{t}) \ell_{g,1}\gamma_{t}\Big{)}\zeta_{t}^{2}+\Big{(}(1+\ell_{F}^{2}\gamma_{t}) \ell_{g,1}\gamma_{t}+\ell_{g,1}^{2}\gamma_{t}^{2}\Big{)}\delta_{t}^{2}\] \[\leq \big{(}1+(\underbrace{3\ell_{F}^{2}+2\ell_{g,1}}_{c_{3}})\gamma_ {t}\big{)}\zeta_{t}^{2}+3\ell_{g,1}\gamma_{t}\delta_{t}^{2}\] (B.68)

where the last inequality follows from \(\ell_{g,1}\gamma_{t}\leq 1\), and \(\ell_{F}^{2}\gamma_{t}\leq 1\).

And since \(\zeta_{t}\) and \(\delta_{t}\) are independent of \(I_{t}\), it follows that

\[\mathbb{E}_{I_{t}}[\zeta_{t+1}^{2}\mid i^{*}\notin I_{t}]\leq\big{(}1+c_{3} \gamma_{t}\big{)}\zeta_{t}^{2}+3\ell_{g,1}\gamma_{t}\delta_{t}^{2}.\] (B.69)

Combining (B.66) and (B.69), we have

\[\mathbb{E}_{I_{t}}[\delta_{t+1}^{2}]\leq \Big{(}1-\alpha_{t}(\mu-\ell_{F}c_{1})\mathbb{P}(i^{*}\notin I_{t}) +p\mathbb{P}(i^{*}\in I_{t})\Big{)}\delta_{t}^{2}+\Big{(}1+\frac{1}{p}\Big{)} \mathbb{P}(i^{*}\in I_{t})4\alpha_{t}^{2}\ell_{f}^{2}\] \[\qquad+\alpha_{t}c_{2}\Big{(}\big{(}1+c_{3}\gamma_{t}\big{)}\zeta_ {t}^{2}+3\ell_{g,1}\gamma_{t}\delta_{t}^{2}\Big{)}\mathbb{P}(i^{*}\notin I_{t})\] (B.70)\[\mathbb{E}[\delta_{1}^{2}]\leq \Bigg{(}\frac{3}{n}(1+\frac{1}{p})4\alpha^{2}\ell_{f}^{2}+24M\ell_{ f}^{4}c_{2}(\frac{8\gamma T}{n}+\gamma)\frac{\alpha}{n}\Bigg{)}\underbrace{\Big{(} \underbrace{\sum_{t^{\prime}=0}^{t-1}(1-\frac{1}{2}\alpha\mu+\frac{3p}{n})^{t- t^{\prime}-1}}_{\beta_{t}}}_{\beta_{t}}\] \[= \Bigg{(}\frac{3}{n}(1+\frac{1}{p})4\alpha^{2}\ell_{f}^{2}+24M\ell _{f}^{4}c_{2}\Big{(}\frac{8\gamma T}{n}+\gamma\Big{)}\frac{\alpha}{n}\Bigg{)} \beta_{t}.\] (B.74)Next, we will prove by induction that (B.74) also holds for \(t>1\).

Assuming that (B.74) holds for all \(0\leq t\leq k\leq T-1\), we apply (B.70) to the case where \(t=k\) to obtain

\[\mathbb{E}[\delta_{k+1}^{2}]\leq\Big{(}\eta_{k}+\frac{3p}{n}\Big{)} \mathbb{E}[\delta_{k}^{2}]+\alpha_{k}c_{2}\bigg{(}1+c_{3}\gamma_{k}\bigg{)} \mathbb{E}[\zeta_{k}^{2}]\mathbb{P}(i^{*}\notin I_{t})+\frac{3}{n}\Big{(}1+ \frac{1}{p}\Big{)}4\alpha_{k}^{2}\ell_{f}^{2}\] \[\stackrel{{(a)}}{{\leq}}\Big{(}\eta_{k}+\frac{3p}{n} \Big{)}\mathbb{E}[\delta_{k}^{2}]\] \[\qquad+2\alpha_{k}c_{2}\gamma\Bigg{(}\sum_{\ell^{\prime}=1}^{k} \Bigg{(}(1+\frac{8T}{n})\frac{12\gamma M\ell_{f}^{4}}{n}+3\ell_{g,1}\mathbb{E }[\delta_{\ell^{\prime}}^{2}]\Bigg{)}\Bigg{)}\mathbb{P}(i^{*}\notin I_{t})+ \frac{3}{n}\Bigg{(}1+\frac{1}{p}\Bigg{)}4\alpha_{k}^{2}\ell_{f}^{2}\] \[\stackrel{{(b)}}{{\leq}}\underbrace{\Bigg{(}\Big{(} \eta_{k}+\frac{3p}{n}\Big{)}\beta_{k}+1+6\alpha_{k}c_{2}\ell_{g,1}\gamma\Big{(} \sum_{\ell^{\prime}=1}^{k}\beta_{\ell^{\prime}}\Big{)}\mathbb{P}(i^{*}\notin I _{t})\Bigg{)}}_{J_{1}}\] \[\qquad\times\Bigg{(}\frac{3}{n}\Bigg{(}1+\frac{1}{p}\Bigg{)}4 \alpha^{2}\ell_{f}^{2}+24M\ell_{f}^{4}c_{2}\Bigg{(}\frac{8\gamma T}{n}+\gamma \Bigg{)}\frac{\alpha}{n}\Bigg{)}\] (B.75)

where \((a)\) follows from (B.73), and \((b)\) follows from (B.74) for \(0\leq t\leq k\) and that \(\gamma k\leq\gamma T\leq 1\). The coefficient \(J_{1}\) in (B.75) can be further bounded by

\[J_{1}= \Big{(}\eta_{k}+\frac{3p}{n}\Big{)}\beta_{k}+1+6\alpha_{k}c_{2} \ell_{g,1}\gamma(\sum_{\ell^{\prime}=1}^{k}c_{\ell^{\prime}})\mathbb{P}(i^{*} \notin I_{t})\] \[\stackrel{{(c)}}{{\leq}} \Big{(}\eta_{k}+\frac{3p}{n}\Big{)}\beta_{k}+1+6\alpha_{k}c_{2} \ell_{g,1}k\gamma\beta_{k}\mathbb{P}(i^{*}\notin I_{t})\] \[\stackrel{{(d)}}{{\leq}} \Big{(}1-\alpha_{k}(\mu-\ell_{F}c_{1}-3c_{2}\ell_{g,1}\gamma(1+2 k))\mathbb{P}(i^{*}\notin I_{t})+\frac{3p}{n}\Big{)}\beta_{k}+1\] \[\stackrel{{(e)}}{{\leq}} \Big{(}1-\frac{1}{2}\alpha\mu+\frac{3p}{n}\Big{)}\beta_{k}+1\] (B.76)

where \((c)\) follows from \(\beta_{t}\leq\beta_{t+1}\), \(\gamma_{t}\leq\gamma\) for all \(t=0,\ldots,T\); \((d)\) follows from the definition of \(\eta_{k}\); \((e)\) is because \(\gamma\leq\mu^{2}/(120\ell_{F}^{2}\ell_{g,1}T)\), \(\alpha\leq 1/(2\ell_{f,1})\leq 1/(2\mu)\) and choosing \(c_{1}=\mu/(4\ell_{F})\) leads to

\[\ell_{F}c_{1}+3c_{2}\ell_{g,1}\gamma(1+2k)\gamma\leq \ell_{F}c_{1}+6(\ell_{F}c_{1}^{-1}+2\alpha\ell_{F}^{2})\ell_{g,1}( k+1)\gamma\] \[\leq \frac{1}{4}\mu+6(4\mu^{-1}+2\alpha)\ell_{F}^{2}\ell_{g,1}\frac{k+ 1}{T}\frac{\mu^{2}}{120\ell_{F}^{2}\ell_{g,1}}\leq\frac{1}{2}\mu.\]

Combining (B.75) and (B.76) implies

\[\mathbb{E}[\delta_{k+1}^{2}]\leq\] \[= c_{k+1}\Bigg{(}\frac{3}{n}(1+\frac{1}{p})4\alpha^{2}\ell_{f}^{2}+ 24M\ell_{f}^{4}c_{2}\big{(}\frac{8\gamma T}{n}+\gamma\big{)}\frac{\alpha}{n} \Bigg{)}\] (B.77)

where the equality follows by the definition of \(\beta_{t}\) given in (B.74). The above statements from (B.75)-(B.77) show that if (B.74) holds for all \(t\) such that \(0\leq t\leq k\leq T-1\), it also holds for \(t=k+1\).

Therefore, we can conclude that for \(T\geq 0\), it follows

\[\mathbb{E}[\delta_{T}^{2}]\leq \beta_{T}\Bigg{(}\frac{3}{n}(1+\frac{1}{p})4\alpha^{2}\ell_{f}^{ 2}+24M\ell_{f}^{4}c_{2}\big{(}\frac{8\gamma T}{n}+\gamma\big{)}\frac{\alpha}{n} \Bigg{)}\] \[=\]\[=\!\!\left(\frac{3}{n}(1+\frac{12}{\alpha\mu n})4\alpha^{2}\ell_{f}^{2 }+24M\ell_{f}^{4}c_{2}\big{(}\frac{8\gamma T}{n}+\gamma\big{)}\frac{\alpha}{n} \right)\!\left(\frac{1}{4}\alpha\mu\right)^{-1}\!\!\left(1-\Big{(}1-\frac{1}{4} \alpha\mu\Big{)}^{T}\right)\] (B.78)

where the last equality follows from taking \(p=\alpha\mu n/12\), and compute the sum of geometric series. By plugging in \(c_{1}=\mu/(4\ell_{F})\), \(c_{2}=\ell_{F}c_{1}^{-1}+2\alpha\ell_{F}^{2}\), \(c_{3}=3\ell_{F}^{2}+2\ell_{g,1}\), we have that

\[\mathbb{E}[\delta_{T}^{2}]\leq \Big{(}\frac{3}{n}(1+\frac{12}{\alpha\mu n})4\alpha^{2}\ell_{f}^{ 2}+24M\ell_{f}^{4}c_{2}c_{3}^{-1}\frac{\alpha}{n^{2}}+24M\ell_{f}^{4}c_{2} \frac{\alpha\gamma}{n}\Big{)}(\frac{1}{4}\alpha\mu)^{-1}\] \[\leq \frac{48}{\mu n}\ell_{f}^{2}\Big{(}\alpha+\frac{12}{\mu n}+\frac {2M\ell_{f}^{2}c_{2}c_{3}^{-1}}{n}+2M\ell_{f}^{2}c_{2}\gamma\Big{)}\] \[\leq \frac{48}{\mu n}\ell_{f}^{2}\Big{(}\alpha+\frac{12+4M\ell_{f}^{2 }}{\mu n}+\frac{10M\ell_{f}^{4}\gamma}{\mu}\Big{)}\] (B.79)

where the last inequality follows from \(c_{2}=\ell_{F}^{2}(4\mu^{-1}+2\alpha)\leq 5M\ell_{f}^{2}\mu^{-1}\), and \(c_{2}c_{3}^{-1}\leq 5\ell_{F}^{2}\mu^{-1}/(3\ell_{F}^{2})\leq 2\mu^{-1}\). 

#### b.4.4 Lower bound of MOL uniform stability

In this section, we construct Example 1 with a lower bound of stability for the MoDo algorithm. Before proceeding to the example, we first define \(\mathbf{1}\) as the all-one vector in \(\mathbb{R}^{M}\), \(\widetilde{\Delta}^{M}\coloneqq\big{\{}\lambda\in\mathbb{R}^{M}\mid\mathbf{1} ^{\top}\lambda=1\big{\}}\), and \(P_{\mathbf{1}}\coloneqq I-\frac{1}{M}\mathbf{1}\mathbf{1}^{\top}\). Then given any vector \(u\in\mathbb{R}^{M}\), \(\Pi_{\widetilde{\Delta}^{M}}(u)=P_{\mathbf{1}}u+\frac{1}{M}\mathbf{1}\).

**Example 1**.: _Recall that \(S=\{z_{1},z_{2},\ldots,z_{j},\ldots z_{n}\}\), \(S^{\prime}=\{z_{1},z_{2},\ldots,z_{j}^{\prime},\ldots,z_{n}\}\), where \(S\) and \(S^{\prime}\) differ only in the \(j\)-th data point. Define the \(m\)-th objective function as_

\[f_{z,m}(x)=\frac{1}{2}x^{\top}Ax-b_{m}z^{\top}x\] (B.80)

_where \(A\) is a symmetric positive definite matrix, \(\mu=16n^{-\frac{1}{3}}>0\) is the smallest eigenvalue of \(A\), and \(v\) is the corresponding eigenvector of \(A\). For the datasets \(S\), and \(S^{\prime}\), let \(z_{i}=c_{i}v\), with \(\mathbb{E}_{z\in S}[z]=\mu v\), \(\mathbb{E}_{z\in S^{\prime}}[z]=\mu^{\prime}v\), \(z_{j}-z_{j}^{\prime}=v\), i.e., \(\mu-\mu^{\prime}=\frac{1}{n}\). For simplicity, let \(M=2\), \(b=[1,1+\sqrt{2}]^{\top}\) such that \(P_{\mathbf{1}}b=b_{P}=[-\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}]^{\top}\), where \(b_{P}\) is the eigenvector of \(P_{\mathbf{1}}\) with eigenvalue \(1\)._

Technical challenges.One challenge of deriving the lower bound lies in the projection operator when updating \(\lambda\). Unlike deriving the upper bound, where the projection operator can be handled by its non-expansive property, due to the nature of inequality constrained quadratic programming, neither a simple closed-form solution can be obtained, nor a non-trivial tight lower bound can be derived in general. We overcome this challenge by showing that when \(\gamma=\mathcal{O}(T^{-1})\), the Euclidean projection onto the simplex is equivalent to a linear operator in Lemma 10. Another challenge compared to deriving the lower bound for single-objective learning is that the update of MoDo involves two coupled sequences, \(\{x_{t}\}\) and \(\{\lambda_{t}\}\). The update of \(x_{t}\) and \(x_{t}^{\prime}\) involves different weighting parameters \(\lambda_{t}\) and \(\lambda_{t}^{\prime}\), where \(\{x_{t}\}\), \(\{\lambda_{t}\}\) and \(\{x_{t}^{\prime}\}\), \(\{\lambda_{t}^{\prime}\}\) are generated by the MoDo algorithm on neighboring training data \(S\) and \(S^{\prime}\), respectively. We overcome this challenge by deriving a recursive relation of the vector \([x_{t}-x_{t}^{\prime};\lambda_{t}-\lambda_{t}^{\prime}]\) in Lemma 11.

Organization of proof.Lemma 10 proves that under proper choice of initialization of \(\lambda\) and step size \(\gamma\), the projection of the updated \(\lambda\) onto simplex \(\Delta^{M}\) is equal to the projection of that onto the set \(\widetilde{\Delta}^{M}\coloneqq\{\lambda\in\mathbb{R}^{M}\mid\mathbf{1}^{\top} \lambda=1\}\). And thus the projection is equivalent to a linear transformation. Thanks to Lemma 10, we are able to derive a recursive relation the vector \([x_{t}-x_{t}^{\prime};\lambda_{t}-\lambda_{t}^{\prime}]\) in Lemma 11. Finally, relying on the recursive relation, we derive a lower bound for the recursion of \([\mathbb{E}_{A}\|x_{t}-x_{t}^{\prime}\|;\mathbb{E}_{A}\|\lambda_{t}-\lambda_{t}^ {\prime}\|]\), depending on a \(2\times 2\) transition matrix. And based on its eigen decomposition, we could compute the \(T\)-th power of such a transition matrix, which is used to derive the final lower bound of \(\mathbb{E}_{A}\|A(S)-A(S^{\prime})\|\) in Theorem 6.

**Lemma 10**.: _Suppose Assumptions 1, 2 hold. For MoDo algorithm, choose \(\lambda_{0}=\frac{1}{M}\mathbf{1}\), \(\gamma\leq\frac{1}{2MT\ell_{F}\ell_{f}}\), and define_

\[\lambda_{t}^{+}\coloneqq\lambda_{t}-\gamma\nabla F_{z_{t,1}}(x_{t})^{\top} \nabla F_{z_{t,2}}(x_{t})\lambda_{t}\] (B.81)_then the update of \(\lambda_{t}\) for MoDo algorithm is \(\lambda_{t+1}=\Pi_{\Delta^{M}}(\lambda_{t}^{+})\)._

_Define the set \(\widetilde{\Delta}^{M}:=\{\lambda\in\mathbb{R}^{M}\mid\mathbf{1}^{\top}\lambda=1\}\), \(P_{\mathbf{1}}:=I-\frac{1}{M}\mathbf{1}\mathbf{1}^{\top}\), \(\lambda_{P,t}\coloneqq\Pi_{\widetilde{\Delta}^{M}}(\lambda_{t}^{+})=P_{ \mathbf{1}}\lambda_{t}^{+}+\frac{1}{M}\mathbf{1}\). Then for \(t=0,\dots,T-1\), it holds that_

\[\lambda_{t+1}=P_{\mathbf{1}}\Big{(}\lambda_{t}-\gamma\nabla F_{z_{t,1}}(x_{t}) ^{\top}\nabla F_{z_{t,2}}(x_{t})\lambda_{t}\Big{)}+\frac{1}{M}\mathbf{1}=P_{ \mathbf{1}}\lambda_{t}^{+}+\frac{1}{M}\mathbf{1}.\] (B.82)

Proof.: By the update of \(\lambda_{t}\), we have

\[\|\lambda_{t+1}-\lambda_{t}\|= \|\Pi_{\Delta^{M}}\Big{(}\lambda_{t}-\gamma\nabla F_{z_{t,1}}(x_{ t})^{\top}\nabla F_{z_{t,2}}(x_{t})\lambda_{t}\Big{)}-\lambda_{t}\|\] \[\leq \|\lambda_{t}-\gamma\nabla F_{z_{t,1}}(x_{t})^{\top}\nabla F_{z_ {t,2}}(x_{t})\lambda_{t}-\lambda_{t}\|\] \[\leq \gamma\|\nabla F_{z_{t,1}}(x_{t})^{\top}\nabla F_{z_{t,2}}(x_{t}) \lambda_{t}\|\leq\gamma\ell_{F}\ell_{f}\] (B.83)

where the last inequality follows from Lemma 6, with \(\ell_{f}=\ell_{f,1}(c_{x}+c_{x^{*}})\).

Then for all \(t\in[T-1]\), it holds that

\[\|\lambda_{t}-\lambda_{0}\|=\Bigg{\|}\sum_{k=0}^{t-1}\lambda_{k+1}-\lambda_{k }\Bigg{\|}\leq\sum_{k=0}^{t-1}\|\lambda_{k+1}-\lambda_{k}\|\leq\gamma t\ell_{ F}\ell_{f}\leq\frac{t}{2MT}\] (B.84)

where the last inequality follows from \(\gamma\leq\frac{1}{2MT\ell_{F}\ell_{f}}\).

Then for \(t\in[T-1]\), it holds that

\[\|\lambda_{t}^{+}-\lambda_{0}\|\leq \|\lambda_{t}^{+}-\lambda_{t}\|+\|\lambda_{t}-\lambda_{0}\|\] \[\leq \gamma\|\nabla F_{z_{t,1}}(x_{t})^{\top}\nabla F_{z_{t,2}}(x_{t} )\lambda_{t}\|+\gamma t\ell_{F}\ell_{f}\leq\gamma(t+1)\ell_{F}\ell_{f}\leq \gamma T\ell_{F}\ell_{f}\leq\frac{1}{2M}.\] (B.85)

By the update of \(\lambda_{t}\), and the definition of projection,

\[\lambda_{t+1}=\Pi_{\Delta^{M}}(\lambda_{t}^{+})=\operatorname*{arg\,min}_{ \lambda\in\Delta^{M}}\|\lambda-\lambda_{t}^{+}\|^{2}.\] (B.86)

Also we have

\[\lambda_{P,t}=\Pi_{\widetilde{\Delta}^{M}}(\lambda_{t}^{+})=\operatorname*{ arg\,min}_{\lambda\in\widetilde{\Delta}^{M}}\|\lambda-\lambda_{t}^{+}\|^{2}.\] (B.87)

Let \(\lambda_{P,t}=[\lambda_{P,t,1},\dots,\lambda_{P,t,M}]^{\top}\). Then it holds that

\[|\lambda_{P,t,m}-\lambda_{0,m}|\leq\|\lambda_{P,t}-\lambda_{0}\|=\|\Pi_{ \widetilde{\Delta}^{M}}(\lambda_{t}^{+})-\lambda_{0}\|\stackrel{{ (a)}}{{\leq}}\|\lambda_{t}^{+}-\lambda_{0}\|\stackrel{{ (b)}}{{\leq}}\frac{1}{2M}\]

where \((a)\) follows from non-expansiveness of projection and that \(\lambda_{0}\in\widetilde{\Delta}^{M}\); \((b)\) follows from (B.85). Therefore, each element of \(\lambda_{P,t}\) satisfies

\[0\leq\frac{1}{M}-\frac{1}{2M}\leq\lambda_{0,m}-|\lambda_{P,t,m}-\lambda_{0,m}| \leq\lambda_{P,t,m}\leq\lambda_{0,m}+|\lambda_{P,t,m}-\lambda_{0,m}|\leq\frac{ 3}{2M}\leq 1\] (B.88)

which shows that \(\lambda_{P,t}=\Pi_{\widetilde{\Delta}^{M}}(\lambda_{t}^{+})\in\Delta^{M}\). Therefore it holds that,

\[\|\lambda_{P,t}-\lambda_{t}^{+}\|^{2}\stackrel{{(c)}}{{\geq}} \min_{\lambda\in\Delta^{M}}\|\lambda-\lambda_{t}^{+}\|^{2}\stackrel{{ (d)}}{{\geq}}\min_{\lambda\in\widetilde{\Delta}^{M}}\|\lambda-\lambda_{t}^{+} \|^{2}\stackrel{{(e)}}{{=}}\|\lambda_{P,t}-\lambda_{t}^{+}\|^{2}\] (B.89)

where \((c)\) is because \(\lambda_{P,t}\in\Delta^{M}\); \((d)\) is because \(\Delta^{M}\subset\widetilde{\Delta}^{M}\) by the definition of the simplex; \((e)\) is because \(\lambda_{P,t}=\Pi_{\widetilde{\Delta}^{M}}(\lambda_{t}^{+})\). Then the equality holds that

\[\|\lambda_{P,t}-\lambda_{t}^{+}\|^{2}=\min_{\lambda\in\Delta^{M}}\|\lambda- \lambda_{t}^{+}\|^{2}\] (B.90)

and

\[P_{\mathbf{1}}\lambda_{t}^{+}+\frac{1}{M}\mathbf{1}=\lambda_{P,t}=\operatorname* {arg\,min}_{\lambda\in\Delta^{M}}\|\lambda-\lambda_{t}^{+}\|^{2}=\Pi_{\lambda\in \Delta^{M}}(\lambda_{t}^{+})=\lambda_{t+1}.\] (B.91)

The proof is complete.

With the help of Lemma 10, which simplifies the Euclidean projection operator as a linear operator, we then prove in Lemma 11, the recursive relation of \(x_{t}-x_{t}^{\prime}\) and \(\lambda_{t}-\lambda_{t}^{\prime}\).

**Lemma 11**.: _Suppose Assumptions 1, 2 hold. Under Example 1, choose \(\lambda_{0}=\frac{1}{M}\mathbf{1}\), \(\gamma\leq\frac{1}{2MT\epsilon_{F}t}\) for the MoDo algorithm. Denote \(\{x_{t}\}\), \(\{\lambda_{t}\}\) and \(\{x_{t}^{\prime}\}\), \(\{\lambda_{t}^{\prime}\}\) as the sequences generated by the MoDo algorithm with dataset \(S\) and \(S^{\prime}\), respectively. Then it holds that_

\[x_{t}-x_{t}^{\prime}=\varphi_{x,t}v,\quad\text{and}\quad\lambda_{t}-\lambda_{t }^{\prime}=\varphi_{\lambda,t}b_{P}\] (B.92)

_and \(\varphi_{x,t}\), \(\varphi_{\lambda,t}\) satisfy the following recursion_

\[\varphi_{x,t+1}= \ (1-\alpha\mu+\alpha\gamma c_{t,3}c_{t,1}\mu)\varphi_{x,t}+ \alpha c_{t,3}(1-\gamma c_{t,1}c_{t,2})\varphi_{\lambda,t}+\mathds{1}(i_{t,3 }=j)\alpha(b^{\top}\lambda_{t+1}^{\prime})\] \[+\mathds{1}(i_{t,1}=j)\gamma\alpha c_{t,3}(\mu\upsilon^{\top}x_{ t}^{\prime}-c_{t,2}b^{\top}\lambda_{t}^{\prime})-\mathds{1}(i_{t,2}=j) \gamma\alpha c_{t,3}c_{t,1}^{\prime}b^{\top}\lambda_{t}^{\prime}\] (B.93) \[\varphi_{\lambda,t+1}= \ (1-\gamma c_{t,1}c_{t,2})\varphi_{\lambda,t}+\gamma c_{t,1}\mu \varphi_{x,t}\] \[+\mathds{1}(i_{t,1}=j)\gamma\big{(}\mu(v^{\top}x_{t}^{\prime})-c _{t,2}(b^{\top}\lambda_{t}^{\prime})\big{)}-\mathds{1}(i_{t,2}=j)\gamma c_{t,1}^{\prime}(b^{\top}\lambda_{t}^{\prime})\] (B.94)

_where \(\mathds{1}(\cdot)\) is the indicator function._

Proof.: Denote \(z_{t,s}\), and \(z_{t,s}^{\prime}\), \(s\in[3]\), as the samples selected in the \(t\)-th iteration from \(S\) and \(S^{\prime}\), respectively. According to the MoDo algorithm update of \(x_{t}\), and the definition of the problem in (B.80), we have

\[x_{t+1}= x_{t}-\alpha\nabla F_{z_{t,3}}(x_{t})\lambda_{t+1}=x_{t}-\alpha \big{[}Ax_{t}-b_{1}z_{t,3},\ldots,Ax_{t}-b_{M}z_{t,3}\big{]}\lambda_{t+1}\] \[= x_{t}-\alpha Ax_{t}+\alpha z_{t,3}(b^{\top}\lambda_{t+1}).\] (B.95)

The difference \(x_{t+1}-x_{t+1}^{\prime}\) can be computed by

\[x_{t+1}-x_{t+1}^{\prime}= \big{(}I-\alpha A\big{)}(x_{t}-x_{t}^{\prime})+\alpha(z_{t,3}b^{ \top}\lambda_{t+1}-z_{t,3}^{\prime}b^{\top}\lambda_{t+1}^{\prime})\] \[= \big{(}I-\alpha A\big{)}(x_{t}-x_{t}^{\prime})+\alpha z_{t,3}b^{ \top}(\lambda_{t+1}-\lambda_{t+1}^{\prime})+\alpha(z_{t,3}-z_{t,3}^{\prime}) b^{\top}\lambda_{t+1}^{\prime}\] \[= \big{(}I-\alpha A\big{)}(x_{t}-x_{t}^{\prime})+\alpha z_{t,3}b^{ \top}(\lambda_{t+1}-\lambda_{t+1}^{\prime})+\mathds{1}(i_{t,3}=j)\alpha vb^{ \top}\lambda_{t+1}^{\prime}\] (B.96)

where \(\mathds{1}(\cdot)\) denotes the indicator function, and the last equation follows from that \(z_{t,3}-z_{t,3}^{\prime}=0\) if \(i_{t,3}\neq j\), and \(z_{t,3}-z_{t,3}^{\prime}=z_{j}-z_{j}^{\prime}=v\) if \(i_{t,3}=j\).

By Lemma 10, in Example 1, \(\lambda_{t+1}=P_{\mathbf{1}}\big{(}\lambda_{t}-\gamma\nabla F_{z_{t,1}}(x_{t}) ^{\top}\nabla F_{z_{t,2}}(x_{t})\lambda_{t}\big{)}+\frac{1}{M}\mathbf{1}\), which can be further derived as

\[\lambda_{t+1}= P_{\mathbf{1}}\big{(}\lambda_{t}-\gamma\nabla F_{z_{t,1}}(x_{t}) ^{\top}\nabla F_{z_{t,2}}(x_{t})\lambda_{t}\big{)}+\frac{1}{M}\mathbf{1}\] \[= P_{\mathbf{1}}\Big{(}\lambda_{t}-\gamma\big{(}\mathbf{1}x_{t}^{ \top}A-bz_{t,1}^{\top}\big{)}\big{(}Ax_{t}-z_{t,2}b^{\top}\lambda_{t}\big{)} \Big{)}+\frac{1}{M}\mathbf{1}\] \[\stackrel{{(a)}}{{=}} \lambda_{t}-\gamma\big{(}P_{\mathbf{1}}\mathbf{1}x_{t}^{\top}A- P_{\mathbf{1}}bz_{t,1}^{\top}\big{)}\big{(}Ax_{t}-z_{t,2}b^{\top}\lambda_{t}\big{)}\] \[\stackrel{{(b)}}{{=}} \lambda_{t}+\gamma b_{P}z_{t,1}^{\top}\big{(}Ax_{t}-z_{t,2}b^{ \top}\lambda_{t}\big{)}\] (B.97)

where \((a)\) follows from rearranging the equation and that \(P_{\mathbf{1}}\lambda_{t}+\frac{1}{M}\mathbf{1}=\Pi_{\widetilde{\Delta}^{M}}( \lambda_{t})=\lambda_{t}\) as \(\lambda_{t}\in\widetilde{\Delta}^{M}\); \((b)\) follows from that \(P_{\mathbf{1}}b=b_{P}\) and \(P_{\mathbf{1}}\mathbf{1}=0\).

The difference \(\lambda_{t+1}-\lambda_{t+1}^{\prime}\) can be derived as

\[\lambda_{t+1}-\lambda_{t+1}^{\prime}= (\lambda_{t}-\lambda_{t}^{\prime})+\gamma b_{P}\big{(}z_{t,1}^{ \top}Ax_{t}-z_{t,1}^{\prime\top}Ax_{t}^{\prime}\big{)}-\gamma b_{P}\big{(}z_{t,1}^{\top}z_{t,2}b^{\top}\lambda_{t}-z_{t,1}^{\prime\top}z_{t,2}^{\prime}b^{ \top}\lambda_{t}^{\prime}\big{)}\] \[\stackrel{{(c)}}{{=}} (\lambda_{t}-\lambda_{t}^{\prime})+\gamma b_{P}z_{t,1}^{\top}A(x_{t}-x _{t}^{\prime})+\gamma b_{P}(z_{t,1}-z_{t,1}^{\prime})^{\top}Ax_{t}^{\prime}- \gamma b_{P}z_{t,1}^{\top}z_{t,2}b^{\top}(\lambda_{t}-\lambda_{t}^{\prime})\] \[-\gamma b_{P}(z_{t,1}-z_{t,1}^{\prime})^{\top}z_{t,2}b^{\top} \lambda_{t}^{\prime}-\gamma b_{P}z_{t,1}^{\prime\top}(z_{t,2}-z_{t,2}^{\prime}) b^{\top}\lambda_{t}^{\prime}\] \[\stackrel{{(d)}}{{=}} (\lambda_{t}-\lambda_{t}^{\prime})+\gamma b_{P}z_{t,1}^{\top}A(x_{t}-x _{t}^{\prime})-\gamma b_{P}z_{t,1}^{\top}z_{t,2}b^{\top}(\lambda_{t}-\lambda_{t }^{\prime})\] \[+\mathds{1}(i_{t,1}=j)\gamma b_{P}(\mu\upsilon^{\top}x_{t}^{ \prime}-\upsilon^{\top}z_{t,2}b^{\top}\lambda_{t}^{\prime})-\mathds{1}(i_{t,2 }=j)\gamma b_{P}z_{t,1}^{\prime\top}vb^{\top}\lambda_{t}^{\prime}\] (B.98)

where \((c)\) follows from rearranging the equation; \((d)\) follows from that \(z_{t,s}-z_{t,s}^{\prime}=0\) if \(i_{t,s}\neq j\), and \(z_{t,s}-z_{t,s}^{\prime}=z_{j}-z_{j}^{\prime}=v\) if \(i_{t,s}=j\), and that \(Av=\mu v\).

Combining (B.96) and (B.98) gives

\[\begin{bmatrix}x_{t+1}-x^{\prime}_{t+1}\\ \lambda_{t+1}-\lambda^{\prime}_{t+1}\end{bmatrix}= \begin{bmatrix}C_{x,x,t}&C_{x,\lambda,k}\end{bmatrix}\begin{bmatrix}x _{t}-x^{\prime}_{t}\\ \lambda_{t}-\lambda^{\prime}_{k}\end{bmatrix}+\mathds{1}(i_{t,3}=j)\alpha vb^{ \top}\lambda^{\prime}_{k+1}\\ \begin{matrix}0\end{bmatrix}-\mathds{1}(i_{t,2}=j)\gamma\begin{bmatrix} \alpha c_{t,3}c^{\prime}_{t,1}vb^{\top}\lambda^{\prime}_{t}\\ b_{P}c^{\prime}_{t,1}b^{\top}\lambda^{\prime}_{t}\end{bmatrix}\] \[+\mathds{1}(i_{t,1}=j)\gamma\begin{bmatrix}\alpha c_{t,3}v(\mu v^ {\top}x^{\prime}_{t}-c_{t,2}b^{\top}\lambda^{\prime}_{t})\\ b_{P}(\mu v^{\top}x^{\prime}_{t}-c_{t,2}b^{\top}\lambda^{\prime}_{t})\end{bmatrix}\] (B.99)

where the matrices are defined as

\[C_{x,x,t}= I-\alpha A+\alpha\gamma z_{t,3}b^{\top}b_{P}z^{\top}_{t,1}A=I- \alpha A+\alpha\gamma c_{t,3}c_{t,1}\mu vv^{\top}\] (B.100a) \[C_{x,\lambda,t}= \alpha z_{t,3}b^{\top}(I-\gamma b_{P}z^{\top}_{t,1}z_{t,2}b^{\top })=\alpha c_{t,3}vb^{\top}(1-\gamma c_{t,1}c_{t,2})\] (B.100b) \[C_{\lambda,x,t}= \gamma b_{P}z^{\top}_{t,1}A=\gamma c_{t,1}\mu b_{P}v^{\top}\] (B.100c) \[C_{\lambda,\lambda,t}= (I-\gamma b_{P}z^{\top}_{t,1}z_{t,2}b^{\top})=(I-\gamma c_{t,1}c _{t,2}b_{P}b^{\top}).\] (B.100d)

Next we show by induction that

\[x_{t}-x^{\prime}_{t}=\varphi_{x,t}v,\quad\text{and}\quad\lambda_{t}-\lambda^{ \prime}_{t}=\varphi_{\lambda,t}b_{P}.\] (B.101)

First, when \(t=0\), \(x_{0}-x^{\prime}_{0}=0=\varphi_{x,0}v\) and \(\lambda_{0}-\lambda^{\prime}_{0}=\varphi_{\lambda,0}b_{P}\) with \(\varphi_{x,0}=0\) and \(\varphi_{\lambda,0}=0\). Therefore (B.101) holds at \(t=0\). Supposing that (B.101) holds for \(t=k\), next we show that it also holds at \(t=k+1\).

At \(t=k+1\), applying (B.99) for \(x_{k+1}-x^{\prime}_{k+1}\), and substituting (B.100a), (B.100b) yields

\[x_{k+1}-x^{\prime}_{k+1}= \begin{bmatrix}C_{x,x,k}&C_{x,\lambda,k}\end{bmatrix}\begin{bmatrix} x_{k}-x^{\prime}_{k}\\ \lambda_{k}-\lambda^{\prime}_{k}\end{bmatrix}+\mathds{1}(i_{k,3}=j)\alpha vb^{ \top}\lambda^{\prime}_{k+1}\] \[+\mathds{1}(i_{k,1}=j)\gamma\alpha c_{k,3}v(\mu v^{\top}x^{ \prime}_{k}-c_{k,2}b^{\top}\lambda^{\prime}_{k})-\mathds{1}(i_{k,2}=j)\gamma \alpha c_{k,3}c^{\prime}_{k,1}vb^{\top}\lambda^{\prime}_{k}\] \[= (I-\alpha A+\alpha\gamma c_{k,3}c_{k,1}\mu vv^{\top})\varphi_{x,k} v+\alpha c_{k,3}v(1-\gamma c_{k,1}c_{k,2})\varphi_{\lambda,k}+\mathds{1}(i_{k,3}=j) \alpha vb^{\top}\lambda^{\prime}_{k+1}\] \[+\mathds{1}(i_{k,1}=j)\gamma\alpha c_{k,3}v(\mu v^{\top}x^{ \prime}_{k}-c_{k,2}b^{\top}\lambda^{\prime}_{k})-\mathds{1}(i_{k,2}=j)\gamma \alpha c_{k,3}c^{\prime}_{k,1}vb^{\top}\lambda^{\prime}_{k}\] \[= (1-\alpha\mu+\alpha\gamma c_{k,3}c_{k,1}\mu)\varphi_{x,k}v+\alpha c _{k,3}(1-\gamma c_{k,1}c_{k,2})\varphi_{\lambda,k}v+\mathds{1}(i_{k,3}=j) \alpha(b^{\top}\lambda^{\prime}_{k+1})v\] \[+\mathds{1}(i_{k,1}=j)\gamma\alpha c_{k,3}(\mu v^{\top}x^{ \prime}_{k}-c_{k,2}b^{\top}\lambda^{\prime}_{k})-\mathds{1}(i_{k,2}=j)\gamma \alpha c_{k,3}c^{\prime}_{k,1}vb^{\top}\lambda^{\prime}_{k}\] \[= \varphi_{x,k+1}v\] (B.102)

where \(\varphi_{x,k+1}\) is computed by

\[\varphi_{x,k+1}= (1-\alpha\mu+\alpha\gamma c_{k,3}c_{k,1}\mu)\varphi_{x,k}+\alpha c _{k,3}(1-\gamma c_{k,1}c_{k,2})\varphi_{\lambda,k}+\mathds{1}(i_{k,3}=j)\alpha(b ^{\top}\lambda^{\prime}_{k+1})\] \[+\mathds{1}(i_{k,1}=j)\gamma\alpha c_{k,3}(\mu v^{\top}x^{ \prime}_{k}-c_{k,2}b^{\top}\lambda^{\prime}_{k})-\mathds{1}(i_{k,2}=j)\gamma \alpha c_{k,3}c^{\prime}_{k,1}b^{\top}\lambda^{\prime}_{k}.\] (B.103)

Therefore, for all \(t\in[T]\), it holds that \(x_{t}-x^{\prime}_{t}=\varphi_{x,t}v\).

At \(t=k+1\), apply (B.99) for \(\lambda_{k+1}-\lambda^{\prime}_{k+1}\), and substitute (B.100c), (B.100d) yields

\[\lambda_{k+1}-\lambda^{\prime}_{k+1}= \begin{bmatrix}C_{\lambda,x,k}&C_{\lambda,k}\end{bmatrix}\begin{bmatrix} x_{k}-x^{\prime}_{k}\\ \lambda_{k}-\lambda^{\prime}_{k}\end{bmatrix}\] \[+\mathds{1}(i_{k,1}=j)\gamma\begin{pmatrix}\mu b_{P}v^{\top}x^{ \prime}_{k}-c_{k,2}b_{P}b^{\top}\lambda^{\prime}_{k}\end{pmatrix}-\mathds{1}(i_ {k,2}=j)\gamma c^{\prime}_{k,1}b_{P}b^{\top}\lambda^{\prime}_{k}\] \[= \gamma c_{k,1}\mu b_{P}v^{\top}\varphi_{x,k}v+(I-\gamma c_{k,1}c_{k,2 }b_{P}b^{\top})b_{P}\varphi_{\lambda,k}\] \[+\mathds{1}(i_{k,1}=j)\gamma\begin{pmatrix}\mu b_{P}v^{\top}x^{ \prime}_{k}-c_{k,2}b_{P}b^{\top}\lambda^{\prime}_{k}\end{pmatrix}-\mathds{1}(i_ {k,2}=j)\gamma c^{\prime}_{k,1}b_{P}b^{\top}\lambda^{\prime}_{k}\] \[= \gamma c_{t,1}\mu\varphi_{x,k}b_{P}+(1-\gamma c_{k,1}c_{k,2})\varphi_ {\lambda,k}b_{P}\] \[+\mathds{1}(i_{k,1}=j)\gamma\begin{pmatrix}\mu(v^{\top}x^{ \prime}_{k})-c_{k,2}(b^{\top}\lambda^{\prime}_{k})\end{pmatrix}b_{P}-\mathds{1}(i_ {k,2}=j)\gamma c^{\prime}_{k,1}(b^{\top}\lambda^{\prime}_{k})b_{P}\] \[= \varphi_{\lambda,k+1}b_{P}\] (B.104)

where \(\varphi_{\lambda,k+1}\) is computed by

\[\varphi_{\lambda,k+1}= \gamma c_{k,1}\mu\varphi_{x,k}+(1-\gamma c_{k,1}c_{k,2})\varphi_{ \lambda,k}\] \[+\mathds{1}(i_{k,1}=j)\gamma\begin{pmatrix}\mu(v^{\top}x^{ \prime}_{k})-c_{k,2}(b^{\top}\lambda^{\prime}_{k})\end{pmatrix}-\mathds{1}(i_ {k,2}=j)\gamma c^{\prime}_{k,1}(b^{\top}\lambda^{\prime}_{k}).\] (B.105)

Therefore, for all \(t\in[T]\), it holds that \(\lambda_{t}-\lambda^{\prime}_{t}=\varphi_{\lambda,t}b_{P}\)Lemma 11 provides the recursive relation of \(x_{t}-x_{t}^{\prime}\) and \(\lambda_{t}-\lambda_{t}^{\prime}\). And Lemma 12 below provides another property used to derive the lower bound of \(\mathbb{E}[\|x_{T}-x_{T}^{\prime}\|]\) in Theorem 6.

**Lemma 12**.: _Suppose Assumptions 1, 2 hold. Under Example 1, choose \(x_{0}=x_{0}^{\prime}=7v\), \(\alpha=\frac{1}{4\mu T}\) for the MoDo algorithm. Denote \(\{x_{t}\}\), \(\{\lambda_{t}\}\) and \(\{x_{t}^{\prime}\}\), \(\{\lambda_{t}^{\prime}\}\) as the sequences generated by the MoDo algorithm with dataset \(S\) and \(S^{\prime}\), respectively. Then it holds that_

\[v^{\top}\mathbb{E}_{A}[Ax_{t}^{\prime}-z_{t,2}b^{\top}\lambda_{t}^{\prime}]\geq 0 \quad\text{and}\quad b^{\top}\mathbb{E}_{A}[\lambda_{t+1}^{\prime}]\geq b^{ \top}\mathbb{E}_{A}[\lambda_{t}^{\prime}].\] (B.106)

Proof.: From the update of \(x_{t}^{\prime}\), we have

\[x_{t+1}^{\prime}= x_{t}^{\prime}-\alpha Ax_{t}^{\prime}+\alpha z_{t,3}^{\prime}(b^ {\top}\lambda_{t+1}^{\prime})\] \[= x_{t}^{\prime}-\alpha Ax_{t}^{\prime}+\alpha c_{t,3}^{\prime}(b^ {\top}\lambda_{t+1}^{\prime})\] (B.107)

Suppose \(x_{t}^{\prime}=c_{x,t}^{\prime}v\), then \(x_{t+1}^{\prime}=c_{x,t+1}^{\prime}v\) with

\[c_{x,t+1}^{\prime}=(1-\alpha\mu)c_{x,t}^{\prime}+\alpha c_{t,3}^ {\prime}(b^{\top}\lambda_{t+1}^{\prime})\] \[\text{and}\ \ \mathbb{E}_{A}[x_{t+1}^{\prime}]=(1-\alpha\mu) \mathbb{E}_{A}[x_{t}^{\prime}]+\alpha\mu^{\prime}v\mathbb{E}_{A}(b^{\top} \lambda_{t+1}^{\prime})\]

Applying the above inequality recursively gives

\[v^{\top}\mathbb{E}_{A}[x_{t}^{\prime}]= v^{\top}(1-\alpha\mu)^{t}x_{0}+\alpha\mu^{\prime}\Big{(}\sum_{t^{ \prime}=0}^{t-1}(1-\alpha\mu)^{t-1-t^{\prime}}\mathbb{E}_{A}(b^{\top}\lambda_ {t^{\prime}+1}^{\prime})\Big{)}\] \[\geq v^{\top}(1-\alpha\mu)^{t}x_{0}+\alpha\mu^{\prime}\frac{1-(1-\alpha \mu)^{t}}{\alpha\mu} b^{\top}\lambda\geq 1\text{ for all }\lambda\in\Delta^{M}\] \[= (1-\alpha\mu)^{t}\big{(}v^{\top}x_{0}-\mu^{\prime}\mu^{-1}\big{)} +\mu^{\prime}\mu^{-1}.\]

Since \(x_{0}=7v\), \(\mu^{\prime}\mu^{-1}\leq 1\), it holds that

\[v^{\top}\mathbb{E}_{A}[x_{t}^{\prime}]= (1-\alpha\mu)^{t}\big{(}7-\mu^{\prime}\mu^{-1}\big{)}+\mu^{ \prime}\mu^{-1}\geq 6(1-\alpha\mu)^{t}+\mu^{\prime}\mu^{-1}.\]

Then it follows that

\[v^{\top}\mathbb{E}_{A}[Ax_{t}^{\prime}-z_{t,2}b^{\top}\lambda_{t }^{\prime}]= \mu\mathbb{E}_{A}[v^{\top}x_{t}^{\prime}-b^{\top}\lambda_{t}^{ \prime}]\geq\mu\Big{(}6(1-\alpha\mu)^{t}+\mu^{\prime}\mu^{-1}-(1+\sqrt{2}) \Big{)}\] \[\geq \mu\Big{(}6(1-\alpha\mu t)-(1+\sqrt{2})\Big{)}\] \[\geq \mu\Big{(}6(1-\frac{1}{4})-(1+\sqrt{2})\Big{)}\geq 0 \alpha=\tfrac{1}{4\mu^{T}}\]

By the update of \(\lambda_{t}^{\prime}\) from (B.97),

\[b^{\top}\mathbb{E}_{A}[\lambda_{t+1}^{\prime}-\lambda_{t}^{\prime}]= b^{\top}\gamma b_{P}\mathbb{E}_{A}[z_{t,1}^{\prime\top}\big{(}Ax_{t}^{ \prime}-z_{t,2}^{\prime}b^{\top}\lambda_{t}^{\prime}\big{)}]\] \[= \gamma\mathbb{E}_{A}[\mu^{\prime}v^{\top}\big{(}Ax_{t}^{\prime}- z_{t,2}^{\prime}b^{\top}\lambda_{t}^{\prime}\big{)}]\] \[\qquad\qquad\qquad\qquad\mathbb{E}_{A}[z_{t,1}^{\prime}]=\mu^{ \prime}v\text{, }z_{t,1}^{\prime}\text{ independent of }\lambda_{t}^{\prime}\text{ and }x_{t}^{\prime}\] \[= \gamma\mu^{\prime}v^{\top}\mathbb{E}_{A}[Ax_{t}^{\prime}-\mu^{ \prime}vb^{\top}\lambda_{t}^{\prime}] \mathbb{E}_{A}[z_{t,2}^{\prime}]=\mu^{\prime}v\text{, }z_{t,2}^{\prime}\text{ independent of }\lambda_{t}^{\prime}\] \[\geq \gamma\mu^{\prime}v^{\top}\mathbb{E}_{A}[Ax_{t}^{\prime}-\muvb^{ \top}\lambda_{t}^{\prime}] \mu^{\prime}\leq\mu\text{, }b^{\top}\lambda\geq 1\text{ for all }\lambda\in\Delta^{M}\] \[= \gamma\mu^{\prime}v^{\top}\mathbb{E}_{A}[Ax_{t}^{\prime}-z_{t,2}b^{ \top}\lambda_{t}^{\prime}]\geq 0. \mathbb{E}_{A}[z_{t,2}]=\mu v\text{, }z_{t,2}\text{ independent of }\lambda_{t}^{\prime}\]

The proof is complete. 

**Theorem 6**.: _Suppose Assumptions 1 and 2 hold. Under Example 1 with \(M=2\), choose \(\lambda_{0}=\frac{1}{M}\mathbf{1}\), \(x_{0}=x_{0}^{\prime}=7v\), \(\alpha=\frac{1}{4\mu T}\), \(0<\gamma\leq\frac{1}{2MT_{F}\ell_{f}}\), and \(T\leq 4n^{\frac{2}{3}}\) for the MoDo algorithm. Denote \(\{x_{t}\}\), \(\{\lambda_{t}\}\) and \(\{x_{t}^{\prime}\}\), \(\{\lambda_{t}^{\prime}\}\) as the sequences generated by the MoDo algorithm with dataset \(S\) and \(S^{\prime}\), respectively. Then it holds that_

\[\mathbb{E}[\|x_{T}-x_{T}^{\prime}\|]\geq\frac{\gamma T}{2n^{2}}+\frac{1}{16n}.\] (B.108)Proof.: Denote \(\delta_{t}=\|x_{t}-x_{t}^{\prime}\|\), \(\zeta_{t}=\|\lambda_{t}-\lambda_{t}^{\prime}\|\). From Lemma 11, it holds that

\[\mathbb{E}[\delta_{t+1}] =\mathbb{E}[|\varphi_{x,t+1}|\|v\|]=\mathbb{E}[|\varphi_{x,t+1}|] \geq\mathbb{E}[\varphi_{x,t+1}]\] (B.109) \[\mathbb{E}[\zeta_{t+1}] =\mathbb{E}[|\varphi_{\lambda,t+1}|\|v\|]=\mathbb{E}[|\varphi_{ \lambda,t+1}|]\geq\mathbb{E}[\varphi_{\lambda,t+1}]\] (B.110)

where \(\varphi_{x,t}\), \(\varphi_{\lambda,t}\) satisfy

\[\varphi_{x,t+1} =(1-\alpha\mu+\alpha\gamma c_{t,3}c_{t,1}\mu)\varphi_{x,t}+ \alpha c_{t,3}(1-\gamma c_{t,1}c_{t,2})\varphi_{\lambda,t}+\mathds{1}(i_{t,3} =j)\alpha(b^{\top}\lambda_{t+1}^{\prime})\] \[\quad+\mathds{1}(i_{t,1}=j)\gamma\alpha c_{t,3}(\mu v^{\top}x_{t} ^{\prime}-c_{t,2}b^{\top}\lambda_{t}^{\prime})-\mathds{1}(i_{t,2}=j)\gamma \alpha c_{t,3}c_{t,1}^{\prime}b^{\top}\lambda_{t}^{\prime}\] (B.111) \[\varphi_{\lambda,t+1} =(1-\gamma c_{t,1}c_{t,2})\varphi_{\lambda,t}+\gamma c_{t,1}\mu \varphi_{x,t}\] \[\quad+\mathds{1}(i_{t,1}=j)\gamma\big{(}\mu(v^{\top}x_{t}^{\prime })-c_{t,2}(b^{\top}\lambda_{t}^{\prime})\big{)}-\mathds{1}(i_{t,2}=j)\gamma c _{t,1}^{\prime}(b^{\top}\lambda_{t}^{\prime}).\] (B.112)

The expectation of \(\varphi_{x,t+1}\) can be further bounded as

\[\mathbb{E}[\varphi_{x,t+1}]= \mathbb{E}[(1-\alpha\mu+\alpha\gamma c_{t,3}c_{t,1}\mu)\varphi_{x, t}+\alpha c_{t,3}(1-\gamma c_{t,1}c_{t,2})\varphi_{\lambda,t}+\frac{1}{n} \alpha(b^{\top}\lambda_{t+1}^{\prime})\] \[\quad+\frac{1}{n}\gamma\alpha c_{t,3}(\mu v^{\top}x_{t}^{\prime}- c_{t,2}b^{\top}\lambda_{t}^{\prime})-\frac{1}{n}\gamma\alpha c_{t,3}c_{t,1}^{ \prime}b^{\top}\lambda_{t}^{\prime}]\] \[\overset{(a)}{\geq} (1-\alpha\mu(1-\gamma\mu^{2}))\mathbb{E}[\varphi_{x,t}]+\alpha\mu (1-\gamma\mu^{2})\mathbb{E}[\varphi_{\lambda,t}]+\frac{1}{n}\alpha b^{\top} \mathbb{E}[\lambda_{t}^{\prime}]-\frac{1}{n}\gamma\alpha\mu\mu^{\prime}b^{\top }\mathbb{E}[\lambda_{t}^{\prime}]\] \[\geq (1-\alpha\mu(1-\gamma\mu^{2}))\mathbb{E}[\varphi_{x,t}]+\alpha\mu (1-\gamma\mu^{2})\mathbb{E}[\varphi_{x,t}]+\frac{1}{n}\alpha b^{\top} \mathbb{E}[\lambda_{t}^{\prime}](1-\gamma\mu^{2})\] (B.113)

where \((a)\) follows from \(\mathbb{E}_{A}[c_{t,s}]=\mu\), \(\mathbb{E}_{A}[c_{t,s}^{\prime}]=\mu^{\prime}\leq\mu\), \(\mathbb{E}[\lambda_{t+1}^{\prime}]\geq\mathbb{E}[\lambda_{t}^{\prime}]\) by Lemma 12, and the fact that \(c_{t,s}\) is independent of \(\varphi_{x,t}\).

Similarly, the expectation of \(\varphi_{\lambda,t+1}\) can be further bounded as

\[\mathbb{E}[\varphi_{\lambda,t+1}]= \mathbb{E}[(1-\gamma c_{t,1}c_{t,2})\varphi_{\lambda,t}+\gamma c_{ t,1}\mu\varphi_{x,t}+\frac{1}{n}\gamma\big{(}\mu(v^{\top}x_{t}^{\prime})-c_{t,2}(b^{ \top}\lambda_{t}^{\prime})\big{)}-\frac{1}{n}\gamma c_{t,1}^{\prime}(b^{\top} \lambda_{t}^{\prime})]\] \[\overset{(b)}{\geq} (1-\gamma\mu^{2})\mathbb{E}[\varphi_{\lambda,t}]+\gamma\mu^{2} \mathbb{E}[\varphi_{x,t}]-\frac{1}{n}\gamma\mu^{\prime}b^{\top}\mathbb{E}[ \lambda_{t}^{\prime}]\] (B.114)

where \((b)\) follows from \(\mathbb{E}_{A}[c_{t,s}]=\mu\), \(\mathbb{E}_{A}[c_{t,s}^{\prime}]=\mu^{\prime}\leq\mu\), and Lemma 12.

The above arguments prove that

\[\begin{bmatrix}\mathbb{E}[\delta_{t+1}]\\ \mathbb{E}[\zeta_{t+1}]\end{bmatrix}\geq\begin{bmatrix}\mathbb{E}[\varphi_{x,t+ 1}]\\ \mathbb{E}[\varphi_{\lambda,t+1}]\end{bmatrix}\geq\underbrace{\begin{bmatrix}(1- \alpha\mu(1-\gamma\mu^{2}))&\alpha\mu(1-\gamma\mu^{2})\\ \gamma\mu^{2}&(1-\gamma\mu^{2})\end{bmatrix}}_{B}\begin{bmatrix}\mathbb{E}[ \varphi_{x,t}]\\ \mathbb{E}[\varphi_{\lambda,t}]\end{bmatrix}+\frac{1}{n}\begin{bmatrix}\alpha(1- \gamma\mu^{2})\\ -\mu^{\prime}\gamma\end{bmatrix}\] (B.115)

where the inequality for vectors denotes the inequality of each corresponding element in the vectors, and matrix \(B\) has \(v_{B,1}=[1,1]^{\top}\) as an eigenvector associated with the eigenvalue \(1\) because

\[Bv_{B,1}=\begin{bmatrix}(1-\alpha\mu(1-\gamma\mu^{2}))&\alpha\mu(1-\gamma\mu^{ 2})\\ \gamma\mu^{2}&(1-\gamma\mu^{2})\end{bmatrix}\begin{bmatrix}1\\ 1\end{bmatrix}=\begin{bmatrix}1\\ 1\end{bmatrix}.\] (B.116)

Similarly, since

\[B\begin{bmatrix}\alpha(1-\gamma\mu^{2})\\ -\gamma\mu\end{bmatrix}= \begin{bmatrix}(1-\alpha\mu(1-\gamma\mu^{2}))&\alpha\mu(1-\gamma\mu^{ 2})\\ \gamma\mu^{2}&(1-\gamma\mu^{2})\end{bmatrix}\begin{bmatrix}\alpha(1-\gamma\mu^{ 2})\\ -\gamma\mu\end{bmatrix}\] \[= \begin{bmatrix}(1-\alpha\mu(1-\gamma\mu^{2}))\alpha(1-\gamma\mu^{ 2})-\gamma\mu\alpha\mu(1-\gamma\mu^{2})\\ \gamma\mu^{2}\alpha(1-\gamma\mu^{2})-\gamma\mu(1-\gamma\mu^{2})\end{bmatrix}\] \[= \begin{bmatrix}(1-\alpha\mu)(1-\gamma\mu^{2})\alpha(1-\gamma\mu^{ 2})\\ -(1-\alpha\mu)(1-\gamma\mu^{2})\gamma\mu\end{bmatrix}=(1-\alpha\mu)(1-\gamma\mu^{ 2})\begin{bmatrix}\alpha(1-\gamma\mu^{2})\\ -\gamma\mu\end{bmatrix},\] (B.117)

then \(v_{B,2}=[\alpha(1-\gamma\mu^{2}),-\gamma\mu]^{\top}\) is another eigenvector of \(B\) with a positive eigenvalue \((1-\alpha\mu)(1-\gamma\mu^{2})<1\). Let \(Q_{B}=[v_{B,1},v_{B,2}]\), which can be expressed as

\[Q_{B}=[v_{B,1},v_{B,2}]=\begin{bmatrix}1&\alpha(1-\gamma\mu^{2})\\ 1&-\gamma\mu\end{bmatrix}.\] (B.118)Then \(B\) has eigenvalue decomposition \(B=Q_{B}\Lambda_{B}Q_{B}^{-1}\), where \(\Lambda_{B}=\mathrm{diag}([1,(1-\alpha\mu)(1-\gamma\mu^{2})])\), and thus \(B^{t}=Q_{B}\Lambda_{B}^{t}Q_{B}^{-1}\) for \(t\in[T]\).

Let \([\alpha(1-\gamma\mu^{2}),-\mu^{\prime}\gamma]^{\top}=Q_{B}[c_{B,1},c_{B,2}]^{\top}\), where \([c_{B,1},c_{B,2}]^{\top}=Q_{B}^{-1}[\alpha(1-\gamma\mu^{2}),-\mu^{\prime} \gamma]^{\top}\) can be computed by

\[\begin{bmatrix}c_{B,1}\\ c_{B,2}\end{bmatrix}= Q_{B}^{-1}\begin{bmatrix}\alpha(1-\gamma\mu^{2})\\ -\mu^{\prime}\gamma\end{bmatrix}=-\frac{1}{\alpha(1-\gamma\mu^{2})+\gamma\mu} \begin{bmatrix}-\gamma\mu&-\alpha(1-\gamma\mu^{2})\\ -1&1\end{bmatrix}\begin{bmatrix}\alpha\\ -\mu^{\prime}\gamma\end{bmatrix}\] \[= \frac{1}{\alpha(1-\gamma\mu^{2})+\gamma\mu}\begin{bmatrix} \alpha\gamma(\mu-\mu^{\prime})(1-\gamma\mu^{2})\\ \alpha+\mu^{\prime}\gamma\end{bmatrix}\geq\begin{bmatrix}\frac{1}{2n}\gamma\\ 1\end{bmatrix}\] (B.119)

where the last inequality follows from \(\mu-\mu^{\prime}=\frac{1}{n}\) and \(\alpha(1-\gamma\mu^{2})\geq\frac{1}{2}\alpha=1/(8\mu T)\geq\gamma\mu\) for \(c_{B,1}\geq\frac{1}{2n}\gamma\), and \(\alpha\mu^{2}=\mu/(4T)=4n^{-\frac{1}{3}}T^{-1}\geq n^{-1}=\mu-\mu^{\prime}\), so that \(\alpha+\mu^{\prime}\gamma=\alpha+\gamma(\mu^{\prime}-\mu+\mu)\geq\alpha+\gamma (\mu-\alpha\mu^{2})=\alpha(1-\gamma\mu^{2})+\gamma\mu\) for \(c_{B,2}\geq 1\).

Since all elements in \(B\) are positive, multiplying \(B\) on both sides preserves inequality. Applying (B.115) recursively yields

\[\begin{bmatrix}\mathbb{E}[\delta_{T}]\\ \mathbb{E}[\zeta_{T}]\end{bmatrix}\geq \sum_{t=0}^{T-1}B^{T-1-t}\frac{1}{n}\begin{bmatrix}\alpha(1- \gamma\mu^{2})\\ -\mu^{\prime}\gamma\end{bmatrix}=\sum_{t=0}^{T-1}B^{T-1-t}\frac{1}{n} \begin{bmatrix}\alpha(1-\gamma\mu^{2})\\ -\mu^{\prime}\gamma\end{bmatrix}=\sum_{t=0}^{T-1}B^{T-1-t}\frac{1}{n}Q_{B} \begin{bmatrix}c_{B,1}\\ c_{B,2}\end{bmatrix}\] \[= \frac{1}{n}\sum_{t=0}^{T-1}1^{T-1-t}c_{B,1}v_{B,1}+\frac{1}{n} \sum_{t=0}^{T-1}\big{(}(1-\alpha\mu)(1-\gamma\mu^{2})\big{)}^{T-1-t}c_{B,2}v_{ B,2}\] \[\geq \frac{T}{n}c_{B,1}v_{B,1}+\frac{1}{8n\alpha}c_{B,2}v_{B,2}\geq \frac{\gamma T}{2n^{2}}v_{B,1}+\frac{1}{8n\alpha}v_{B,2}\] (B.120)

where the last inequality follows from \(c_{B,1}\geq\frac{1}{2n}\gamma\), and \(c_{B,2}\geq 1\). Plugging in \(v_{B,1}=[1,1]^{\top}\) and \(v_{B,2}=[\alpha(1-\gamma\mu^{2}),-\gamma\mu]^{\top}\), and since \((1-\gamma\mu^{2})\geq\frac{1}{2}\), it follows that \(\mathbb{E}[\delta_{T}]\geq\frac{\gamma T}{2n^{2}}+\frac{1}{16n}\). 

#### b.4.5 Proof of Theorem 2

**Proof of Theorem 2.** Combining the argument stability in Theorem 5, and Assumption 1, the MOL uniform stability can be bounded by

\[\sup_{z}\mathbb{E}_{A}[\|\nabla F_{z}(A(S))-\nabla F_{z}(A(S^{ \prime}))\|_{\mathrm{F}}^{2}]\] \[\leq \mathbb{E}_{A}[\ell_{F,1}^{2}\|A(S)-A(S^{\prime})\|^{2}]\] by Assumption 1 \[\leq \frac{48}{\mu n}\ell_{f}^{2}\ell_{F,1}^{2}\Big{(}\alpha+\frac{12+4 M\ell_{f}^{2}}{\mu n}+\frac{10M\ell_{f}^{4}\gamma}{\mu}\Big{)}.\] (B.121)

Then based on Propositions 2-3, we have

\[\mathbb{E}_{A,S}[R_{\mathrm{gen}}(A(S))]\leq \mathbb{E}_{A,S}[\|\nabla F(A(S))-\nabla F_{S}(A(S))\|_{\mathrm{ F}}]\] by Proposition 2 \[\leq 4\epsilon_{\mathrm{F}}+\sqrt{n^{-1}\mathbb{E}_{S}\left[\mathbb{V} _{z\sim\mathcal{D}}(\nabla F_{z}(A(S)))\right]}\] by Proposition 3 \[= \mathcal{O}(n^{-\frac{1}{2}}).\] by (B.121)

The proof of the upper bound is complete. We then prove the MOL uniform stability lower bound based on the argument uniform stability lower bound in Theorem 6. By the strong convexity of the function \(f_{m,z}(x)\), for all \(m\in[M]\)

\[\sup_{z}\mathbb{E}_{A}[\|\nabla F_{z}(A(S))-\nabla F_{z}(A(S^{ \prime}))\|_{\mathrm{F}}^{2}]\geq \mathbb{E}_{A}[M\mu^{2}\|A(S)-A(S^{\prime})\|^{2}]\] by Assumption 2 \[\geq \frac{M\mu^{2}}{256n^{2}}.\] by Theorem 6 and Jensen's inequality

The proof of the lower bound is complete.

Bounding the optimization error

### Auxiliary lemmas

**Lemma 13** (Uniqueness of CA direction).: _Given \(Q\in\mathbb{R}^{d\times M}\), then \(d_{Q}\coloneqq Q\lambda^{*}\) with \(\lambda^{*}\in\arg\min_{\lambda\in\Delta^{M}}\|Q\lambda\|^{2}\) exists, and \(d_{Q}\) is unique._

Proof.: This is a standard result due to convexity of the subproblem. Proof is given in [9, Section 2]. 

**Lemma 14**.: _For any \(x\in\mathbb{R}^{d}\), define \(\lambda^{*}(x)\) such that_

\[\lambda^{*}(x)\in\arg\min_{\lambda\in\Delta^{M}}\|\nabla F(x)\lambda\|^{2}.\] (C.1)

_Then, for any \(x\in\mathbb{R}^{d}\) and \(\lambda\in\Delta^{M}\), it holds that_

\[\langle\nabla F(x)\lambda^{*}(x),\nabla F(x)\lambda\rangle\geq\| \nabla F(x)\lambda^{*}(x)\|^{2},\] (C.2) \[\text{and }\ \|\nabla F(x)\lambda-\nabla F(x)\lambda^{*}(x)\|^{2} \leq\|\nabla F(x)\lambda\|^{2}-\|\nabla F(x)\lambda^{*}(x)\|^{2}.\] (C.3)

Proof.: By the first order optimality condition for (C.1), for any \(x\in\mathbb{R}^{d}\) and \(\lambda\in\Delta^{M}\), we have

\[\langle\nabla F(x)^{\top}\nabla F(x)\lambda^{*}(x),\lambda-\lambda^{*}(x) \rangle\geq 0.\] (C.4)

By rearranging the above inequality, we obtain

\[\langle\nabla F(x)\lambda^{*}(x),\nabla F(x)\lambda\rangle\geq\|\nabla F(x) \lambda^{*}(x)\|^{2},\] (C.5)

which is precisely the first inequality in the claim. Furthermore, we can also have

\[\|\nabla F(x)\lambda-\nabla F(x)\lambda^{*}(x)\|^{2} =\|\nabla F(x)\lambda\|^{2}+\|\nabla F(x)\lambda^{*}(x)\|^{2}-2 \langle\nabla F(x)\lambda^{*}(x),\nabla F(x)\lambda\rangle\] \[\leq\|\nabla F(x)\lambda\|^{2}+\|\nabla F(x)\lambda^{*}(x)\|^{2} -2\|\nabla F(x)\lambda^{*}(x)\|^{2}\] \[=\|\nabla F(x)\lambda\|^{2}-\|\nabla F(x)\lambda^{*}(x)\|^{2},\] (C.6)

which is the desired second inequality in the claim. Hence, the proof is complete. 

**Lemma 15** (Continuity of \(\lambda^{*}_{\rho}(x)\)).: _Given any \(\rho>0\) and \(x\in\mathbb{R}^{d}\), define \(\lambda^{*}_{\rho}(x)=\arg\min_{\lambda\in\Delta^{M}}\frac{1}{2}\|\nabla F_{S} (x)\lambda\|^{2}+\frac{1}{2}\rho\|\lambda\|^{2}\), then the following inequality holds_

\[\|\lambda^{*}_{\rho}(x)-\lambda^{*}_{\rho}(x^{\prime})\|\leq\rho^{-1}\|\nabla F (x)^{\top}\nabla F(x)-\nabla F(x^{\prime})^{\top}\nabla F(x^{\prime})\|.\] (C.7)

_Suppose either 1) Assumptions 1, 3 hold, or 2) Assumptions 1, 2 hold, with \(\ell_{F}\) defined in Lemma 1. Then for \(x\in\{x_{t}\}_{t=1}^{T}\), \(x^{\prime}\in\{x^{\prime}_{t}\}_{t=1}^{T}\) generated by MoDo algorithm on training dataset \(S\) and \(S^{\prime}\), respectively, it implies that_

\[\|\lambda^{*}_{\rho}(x)-\lambda^{*}_{\rho}(x^{\prime})\|\leq 2\rho^{-1}\ell_{ F,1}\ell_{F}\|x-x^{\prime}\|.\] (C.8)

Proof.: We provide proof leveraging the convergence properties of the projected gradient descent algorithm on strongly convex objectives below. Consider the problem \(\min_{\lambda\in\Delta^{M}}g(\lambda;x,\rho)=\frac{1}{2}\|\nabla F_{S}(x) \lambda\|^{2}+\frac{1}{2}\rho\|\lambda\|^{2}\), which is \(\rho\)-strongly convex. Let \(\{\lambda_{\rho,k}(x)\}\) for \(k=0,1,\ldots,K\) denote the sequence obtained from applying projected gradient descent (PGD) on the objective \(g(\lambda;x,\rho)=\frac{1}{2}\|\nabla F_{S}(x)\lambda\|^{2}+\frac{1}{2}\rho\| \lambda\|^{2}\), i.e.,

\[\lambda_{\rho,k+1}(x)= \Pi_{\Delta^{M}}\Big{(}\lambda_{\rho,k}(x)-\eta\nabla F_{S}(x)^{ \top}\nabla F_{S}(x)\lambda_{\rho,k}(x)-\eta\rho\lambda_{\rho,k}(x)\Big{)}\] \[= \Pi_{\Delta^{M}}\Big{(}\big{(}(1-\eta\rho)I-\eta\nabla F_{S}(x)^{ \top}\nabla F_{S}(x)\big{)}\lambda_{\rho,k}(x)\Big{)}\] (C.9)

where \(\eta\) is the step size that \(\eta<1/(\|\nabla F_{S}(x)^{\top}\nabla F_{S}(x)\|+\rho)\). Note that both \(\rho,\eta\) are independent of \(K\). By the convergence result of PGD on strongly convex objective, we know that \(\lambda^{*}_{\rho}(x)\) is the limit point of \(\{\lambda_{\rho,k}(x)\}_{k=0}^{\infty}\). By the non-expansiveness of projection, we have

\[\|\lambda_{\rho,k+1}(x)-\lambda_{\rho,k+1}(x^{\prime})\|\]\[\|\lambda_{\rho}^{*}(x)-\lambda_{\rho}^{*}(x^{\prime})\|\leq \rho^{-1}\|\nabla F_{S}(x)+\nabla F_{S}(x^{\prime})\|\|\nabla F_{S}(x )-\nabla F_{S}(x^{\prime})\|\leq 2\rho^{-1}\ell_{F,1}\ell_{F}\|x-x^{\prime}\|.\]

\(\Box\)

**Lemma 16**.: _For any \(\rho>0\) and \(x\in\mathbb{R}^{d}\), define \(\lambda^{*}(x)=\arg\min_{\lambda\in\Delta^{M}}\|\nabla F_{S}(x)\lambda\|^{2}\), and \(\lambda_{\rho}^{*}(x)=\arg\min_{\lambda\in\Delta^{M}}\|\nabla F_{S}(x)\lambda\|^ {2}+\rho\|\lambda\|^{2}\), then we have_

\[0\leq\|\nabla F_{S}(x)\lambda_{\rho}^{*}(x)\|^{2}-\|\nabla F_{S}(x)\lambda^{* }(x)\|^{2}\leq\rho\left(1-\frac{1}{M}\right).\] (C.14)

Proof.: Since \(\lambda^{*}(x)=\arg\min_{\lambda\in\Delta^{M}}\|\nabla F_{S}(x)\lambda\|^{2}\), therefore

\[\|\nabla F_{S}(x)\lambda_{\rho}^{*}(x)\|^{2}-\|\nabla F_{S}(x)\lambda^{*}(x)\| ^{2}=\|\nabla F_{S}(x)\lambda_{\rho}^{*}(x)\|^{2}-\min_{\lambda\in\Delta^{M}} \|\nabla F_{S}(x)\lambda\|^{2}\geq 0.\] (C.15)

[MISSING_PAGE_FAIL:38]

Proof.: Define \(\lambda_{\beta}^{*}(x_{t})=\arg\min_{\lambda\in\Delta^{M}}\frac{1}{2}\|\nabla F_{S} (x_{t})\lambda\|^{2}+\frac{\rho}{2}\|\lambda\|^{2}\) with \(\bar{\rho}>0\). Note that \(\bar{\rho}\) is strictly positive and is used only for analysis but not for algorithm update.

Substituting \(\lambda=\lambda_{\bar{\rho}}^{*}(x_{t})\) in Lemma 17, (C.19), we have

\[\gamma_{t}\mathbb{E}_{A}(\|\nabla F_{S}(x_{t})\lambda_{t}\|^{2}- \|\nabla F_{S}(x_{t})\lambda_{\bar{\rho}}^{*}(x_{t})\|^{2})\] \[\leq \mathbb{E}_{A}\|\lambda_{t}-\lambda_{\bar{\rho}}^{*}(x_{t})\|^{2} -\mathbb{E}_{A}\|\lambda_{t+1}-\lambda_{\bar{\rho}}^{*}(x_{t})\|^{2}+\gamma_{ t}^{2}\mathbb{E}_{A}\|(\nabla F_{z_{t,1}}(x_{t})^{\top}\nabla F_{z_{t,2}}(x_{t})) \lambda_{t}\|^{2}.\] (C.23)

Setting \(\gamma_{t}=\gamma>0\), taking expectation and telescoping the above inequality gives

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}[\|\nabla F_{S}(x_{t}) \lambda_{t}\|^{2}-\|\nabla F_{S}(x_{t})\lambda_{\bar{\rho}}^{*}(x_{t})\|^{2}]\] \[\leq \frac{1}{T}\sum_{t=0}^{T-1}\frac{1}{\gamma}\mathbb{E}_{A}[\| \lambda_{t}-\lambda_{\bar{\rho}}^{*}(x_{t})\|^{2}-\|\lambda_{t+1}-\lambda_{ \bar{\rho}}^{*}(x_{t})\|^{2}]+\frac{1}{T}\sum_{t=0}^{T-1}\gamma\mathbb{E}_{A} \|(\nabla F_{z_{t,1}}(x_{t})^{\top}\nabla F_{z_{t,2}}(x_{t}))\lambda_{t}\|^{2}\] \[= \frac{1}{\gamma T}\underbrace{\Big{(}\sum_{t=0}^{T-1}\mathbb{E}_{ A}[\|\lambda_{t}-\lambda_{\bar{\rho}}^{*}(x_{t})\|^{2}-\|\lambda_{t+1}-\lambda_{ \bar{\rho}}^{*}(x_{t})\|^{2}]\Big{)}}_{I_{1}}+\frac{1}{T}\sum_{t=0}^{T-1} \gamma\mathbb{E}_{A}\|(\nabla F_{z_{t,1}}(x_{t})^{\top}\nabla F_{z_{t,2}}(x_{ t}))\lambda_{t}\|^{2}\] (C.24)

where \(I_{1}\) can be further derived as

\[I_{1}= \sum_{t=0}^{T-1}\mathbb{E}_{A}\|\lambda_{t}-\lambda_{\bar{\rho}}^ {*}(x_{t})\|^{2}-\mathbb{E}_{A}\|\lambda_{t+1}-\lambda_{\bar{\rho}}^{*}(x_{t}) \|^{2}\] \[= \mathbb{E}_{A}\|\lambda_{0}-\lambda_{\bar{\rho}}^{*}(x_{0})\|^{2} -\mathbb{E}_{A}\|\lambda_{T}-\lambda_{\bar{\rho}}^{*}(x_{T})\|^{2}+\sum_{t=0}^{ T-2}\mathbb{E}_{A}[\|\lambda_{t+1}-\lambda_{\bar{\rho}}^{*}(x_{t+1})\|^{2}-\| \lambda_{t+1}-\lambda_{\bar{\rho}}^{*}(x_{t})\|^{2}]\] \[\leq \mathbb{E}_{A}\|\lambda_{0}-\lambda_{\bar{\rho}}^{*}(x_{0})\|^{2 }-\mathbb{E}_{A}\|\lambda_{T}-\lambda_{\bar{\rho}}^{*}(x_{T})\|^{2}\] \[+\sum_{t=0}^{T-2}\mathbb{E}_{A}[\|2\lambda_{t+1}-\lambda_{\bar{ \rho}}^{*}(x_{t+1})-\lambda_{\bar{\rho}}^{*}(x_{t})\|\|\lambda_{\bar{\rho}}^{*} (x_{t+1})-\lambda_{\bar{\rho}}^{*}(x_{t})\|]\] \[\leq 4+4\sum_{t=0}^{T-2}\mathbb{E}_{A}\|\lambda_{\bar{\rho}}^{*}(x_{t +1})-\lambda_{\bar{\rho}}^{*}(x_{t})\|\] (C.25)

where \(\|\lambda_{\bar{\rho}}^{*}(x_{t+1})-\lambda_{\bar{\rho}}^{*}(x_{t})\|\), by Lemma 15, can be bounded by

\[\|\lambda_{\bar{\rho}}^{*}(x_{t+1})-\lambda_{\bar{\rho}}^{*}(x_{t })\|\leq \bar{\rho}^{-1}\|\nabla F_{S}(x_{t+1})+\nabla F_{S}(x_{t})\|\| \nabla F_{S}(x_{t+1})-\nabla F_{S}(x_{t})\|\] \[\leq \bar{\rho}^{-1}\ell_{F,1}\|\nabla F_{S}(x_{t+1})+\nabla F_{S}(x_{ t})\|\|x_{t+1}-x_{t}\|\] \[\leq \bar{\rho}^{-1}\alpha\ell_{F,1}\|\nabla F_{S}(x_{t+1})+\nabla F_{S }(x_{t})\|\|\nabla F_{z_{t,3}}\lambda_{t+1}\|.\] (C.26)

Hence, it follows that

\[I_{1}\leq 4+4\bar{\rho}^{-1}\alpha\ell_{F,1}\sum_{t=0}^{T-1}\mathbb{E}_{A}\| \nabla F_{S}(x_{t+1})+\nabla F_{S}(x_{t})\|\|\nabla F_{z_{t,3}}\lambda_{t+1}\|\] \[= 4+4\bar{\rho}^{-1}\alpha\ell_{F,1}TS_{2,T}\] (C.27)

plugging which into (C.24) gives

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}[\|\nabla F_{S}(x_{t}) \lambda_{t}\|^{2}-\|\nabla F_{S}(x_{t})\lambda_{\bar{\rho}}^{*}(x_{t})\|^{2}] \leq\frac{4}{\gamma T}(1+\bar{\rho}^{-1}\alpha\ell_{F,1}TS_{2,T})+\gamma S_{1,T}.\] (C.28)

Define \(\lambda^{*}(x_{t})\in\arg\min_{\lambda\in\Delta^{M}}\|\nabla F_{S}(x_{t})\lambda \|^{2}\). Then

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}[\|\nabla F_{S}(x_{t}) \lambda_{t}\|^{2}-\|\nabla F_{S}(x_{t})\lambda^{*}(x_{t})\|^{2}]\]\[\leq \frac{4}{\gamma T}(1+\bar{\rho}^{-1}\alpha\ell_{F,1}TS_{2,T})+ \gamma S_{1,T}+\bar{\rho}\] (C.29)

where the last inequality follows from Lemma 16. The proof is complete. 

Proof of Lemma 2.: Building on the result in Lemma 18, and by the convexity of the subproblem, \(\min_{\lambda\in\Delta^{M}}\frac{1}{2}\|\nabla F_{S}(x_{t})\lambda\|^{2}\), and Lemma 14, we have

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}[\|\nabla F_{S}(x_{t}) \lambda_{t}-\nabla F_{S}(x_{t})\lambda^{*}(x_{t})\|^{2}]\leq \frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}[\|\nabla F_{S}(x_{t}) \lambda_{t}\|^{2}-\|\nabla F_{S}(x_{t})\lambda^{*}(x_{t})\|^{2}]\] \[\leq \bar{\rho}+\frac{4}{\gamma T}(1+\bar{\rho}^{-1}\alpha\ell_{F,1} TS_{2,T})+\gamma S_{1,T}.\] (C.30)

By Assumptions 1, 3 or Assumptions 1, 2 and Lemma 1, we have

\[S_{1,T}= \frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}\|(\nabla F_{z_{t,1}}(x _{t})^{\top}\nabla F_{z_{t,2}}(x_{t}))\lambda_{t}\|^{2}\leq(\ell_{f}\ell_{F}) ^{2}\leq M\ell_{f}^{4}\] (C.31) \[S_{2,T}= \frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}\|\nabla F_{S}(x_{t+1}) +\nabla F_{S}(x_{t})\|\|\nabla F_{z_{t,3}}\lambda_{t+1}\|\leq 2\ell_{f}\ell_{F}.\] (C.32)

Substituting \(S_{1,T}\), \(S_{2,T}\) in (C.30) with the above bound yields

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}[\|\nabla F_{S}(x_{t}) \lambda_{t}-\nabla F_{S}(x_{t})\lambda^{*}(x_{t})\|^{2}]\leq \bar{\rho}+\frac{4}{\gamma T}(1+2\bar{\rho}^{-1}\alpha T\ell_{F,1 }\ell_{f}\ell_{F})+\gamma M\ell_{f}^{4}.\] (C.33)

Because \(\ell_{F,1}\ell_{F}\leq M\ell_{f,1}\ell_{f}\), choosing \(\bar{\rho}=2(\alpha M\ell_{f,1}\ell_{f}^{2}/\gamma)^{\frac{1}{2}}\) yields

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}[\|\nabla F_{S}(x_{t}) \lambda_{t}-\nabla F_{S}(x_{t})\lambda^{*}(x_{t})\|^{2}]\leq \bar{\rho}+\frac{4}{\gamma T}(1+2\bar{\rho}^{-1}\alpha TM\ell_{f,1}\ell_{f}^{2})+\gamma M\ell_{f}^{4}\] \[= \frac{4}{\gamma T}+6\sqrt{M\ell_{f,1}\ell_{f}^{2}\frac{\alpha}{ \gamma}}+\gamma M\ell_{f}^{4}\] (C.34)

where \((a)\) follows from Lemma 14. This proves the result. 

### Proof of Theorem 3 - PS optimization error

Technical contributions.The optimization error bound in Theorem 3 is improved with either relaxed assumption or improved convergence rate compared to prior stochastic MOL algorithms [52, 10, 30] (see Table 2). This is achieved by 1) instead of bounding the approximation error to \(\lambda^{*}(x_{t})\), we bound that to the CA direction \(d(x_{t})=-\nabla F_{S}(x_{t})\lambda^{*}(x_{t})\) as a whole, and 2) instead of using the descent lemma of \(F_{S}(x_{t})\lambda^{*}(x_{t})\) with a dynamic weight, we use that of \(F_{S}(x_{t})\lambda\) with a fixed weight (see Lemma 19, (C.36)), thereby improving the tightness of the bound.

Organization of proof.In Lemma 19, we prove the upper bound of the PS optimization error, \(\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}\|\nabla F_{S}(x_{t})\lambda_{t}^{*} (x_{t})\|^{2}\), in terms of three average of sequences, \(S_{1,T}\), \(S_{3,T}\), and \(S_{4,T}\). Then we prove the upper bound of \(S_{1,T}\), \(S_{3,T}\), and \(S_{4,T}\), and thus the PS optimization error either in the nonconvex case under Assumptions 1, 3 or in the strongly convex case under Assumptions 1, 2. Combining the results leads to Theorem 3.

**Lemma 19**.: _Suppose Assumption 1 holds. Consider the sequence \(\{x_{t}\},\{\lambda_{t}\}\) generated by MoDo in unbounded domain for \(x\). Define_

\[S_{1,T}= \frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}\|\nabla F_{z_{t,1}}(x_{t}) ^{\top}\nabla F_{z_{t,2}}(x_{t})\lambda_{t}\|^{2}\] (C.35a) \[S_{3,T}= \frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}\|\nabla F_{z_{t,1}}(x_{ t})^{\top}\nabla F_{z_{t,2}}(x_{t})\lambda_{t}\|\|\nabla F_{S}(x_{t})^{\top} \nabla F_{S}(x_{t})\lambda_{1}\|\] (C.35b) \[S_{4,T}= \frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}\|\nabla F_{z_{t,3}}(x_{ t})\lambda_{t+1}\|^{2}.\] (C.35c)

_Then it holds that_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}\|\nabla F_{S}(x_{t}) \lambda_{t}^{*}(x_{t})\|^{2}\leq\frac{1}{2\alpha T}\mathbb{E}_{A}[F_{S}(x_{1} )-F_{S}(x_{T+1})]\lambda_{1}+\frac{1}{2}\gamma S_{1,T}+\gamma S_{3,T}+\frac{1} {2}\alpha\ell_{f,1}S_{4,T}.\]

Proof.: By the \(\ell_{f,1}\)-Lipschitz smoothness of \(F_{S}(x)\lambda\) for all \(\lambda\in\Delta^{M}\), we have

\[F_{S}(x_{t+1})\lambda-F_{S}(x_{t})\lambda\leq \langle\nabla F_{S}(x_{t})\lambda,x_{t+1}-x_{t}\rangle+\frac{\ell_ {f,1}}{2}\|x_{t+1}-x_{t}\|^{2}\] \[= -\alpha_{t}\langle\nabla F_{S}(x_{t})\lambda,\nabla F_{z_{t,3}}( x_{t})\lambda_{t+1}\rangle+\frac{\ell_{f,1}}{2}\alpha_{t}^{2}\|\nabla F_{z_{t,3}}(x_{t}) \lambda_{t+1}\|^{2}.\] (C.36)

Taking expectation over \(z_{t,3}\) on both sides of the above inequality gives

\[\mathbb{E}_{z_{t,3}}[F_{S}(x_{t+1})]\lambda-F_{S}(x_{t})\lambda \leq-\alpha_{t}\langle\nabla F_{S}(x_{t})\lambda,\nabla F_{S}(x_{t}) \lambda_{t+1}\rangle+\frac{\ell_{f,1}}{2}\alpha_{t}^{2}\mathbb{E}_{z_{t,3}}\| \nabla F_{z_{t,3}}(x_{t})\lambda_{t+1}\|^{2}.\] (C.37)

By Lemma 17, (C.18), we have

\[2\gamma_{t}\mathbb{E}_{A}\langle\lambda_{t}-\lambda,(\nabla F_{ S}(x_{t})^{\top}\nabla F_{S}(x_{t}))\lambda_{t}\rangle\] \[\leq \mathbb{E}_{A}\|\lambda_{t}-\lambda\|^{2}-\mathbb{E}_{A}\|\lambda _{t+1}-\lambda\|^{2}+\gamma_{t}^{2}\mathbb{E}_{A}\|(\nabla F_{z_{t,1}}(x_{t})^ {\top}\nabla F_{z_{t,2}}(x_{t}))\lambda_{t}\|^{2}.\] (C.38)

Rearranging the above inequality and letting \(\gamma_{t}=\gamma>0\) gives

\[-\mathbb{E}_{A}\langle\lambda,\nabla F_{S}(x_{t})^{\top}\nabla F _{S}(x_{t})\lambda_{t}\rangle\leq -\mathbb{E}_{A}\langle\lambda_{t},(\nabla F_{S}(x_{t})^{\top} \nabla F_{S}(x_{t}))\lambda_{t}\rangle+\frac{1}{2\gamma}\mathbb{E}_{A}(\| \lambda_{t}-\lambda\|^{2}-\|\lambda_{t+1}-\lambda\|^{2})\] \[+\frac{1}{2}\gamma\mathbb{E}_{A}\|(\nabla F_{z_{t,1}}(x_{t})^{ \top}\nabla F_{z_{t,2}}(x_{t}))\lambda_{t}\|^{2}\] \[\leq -\mathbb{E}_{A}\|\nabla F_{S}(x_{t})\lambda_{t}\|^{2}+\frac{1}{2 \gamma}\mathbb{E}_{A}(\|\lambda_{t}-\lambda\|^{2}-\|\lambda_{t+1}-\lambda\|^{2})\] \[+\frac{1}{2}\gamma\mathbb{E}_{A}\|(\nabla F_{z_{t,1}}(x_{t})^{ \top}\nabla F_{z_{t,2}}(x_{t}))\lambda_{t}\|^{2}.\] (C.39)

Plugging the above inequality into (C.37), and setting \(\alpha_{t}=\alpha>0\), we have

\[\mathbb{E}_{A}[F_{S}(x_{t+1})\lambda-F_{S}(x_{t})\lambda]\leq -\alpha\mathbb{E}_{A}\langle\nabla F_{S}(x_{t})\lambda,\nabla F_{ S}(x_{t})\lambda_{t+1}\rangle+\frac{\ell_{f,1}}{2}\alpha^{2}\mathbb{E}_{A}\| \nabla F_{z_{t,3}}(x_{t})\lambda_{t+1}\|^{2}\] \[\leq -\alpha\mathbb{E}_{A}\|\nabla F_{S}(x_{t})\lambda_{t}\|^{2}+\frac {\alpha}{2\gamma}\mathbb{E}_{A}[\|\lambda_{t}-\lambda\|^{2}-\|\lambda_{t+1}- \lambda\|^{2}]\] \[+\alpha\mathbb{E}_{A}\langle\nabla F_{S}(x_{t})\lambda,\nabla F_{ S}(x_{t})(\lambda_{t}-\lambda_{t+1})\rangle+\frac{1}{2}\alpha^{2}\ell_{f,1}\mathbb{E}_{A}\| \nabla F_{z_{t,3}}(x_{t})\lambda_{t+1}\|^{2}\] \[+\frac{1}{2}\alpha\gamma\mathbb{E}_{A}\|(\nabla F_{z_{t,1}}(x_{t})^ {\top}\nabla F_{z_{t,2}}(x_{t}))\lambda_{t}\|^{2}.\] (C.40)Taking telescope sum and rearranging yields, for all \(\lambda\in\Delta^{M}\),

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}\|\nabla F_{S}(x_{t}) \lambda_{t}\|^{2}\] \[\leq \frac{1}{2\gamma T}\sum_{t=0}^{T-1}\mathbb{E}_{A}[\|\lambda_{t}- \lambda\|^{2}-\|\lambda_{t+1}-\lambda\|^{2}]+\frac{1}{\alpha T}\sum_{t=0}^{T-1 }\mathbb{E}_{A}[F_{S}(x_{t})-F_{S}(x_{t+1})]\lambda\] \[+\frac{1}{2T}\sum_{t=0}^{T-1}\left(\gamma\mathbb{E}_{A}\|\nabla F _{z_{t,1}}(x_{t})^{\top}\nabla F_{z_{t,2}}(x_{t})\lambda_{t}\|^{2}+\alpha\ell_ {f,1}\mathbb{E}_{A}\|\nabla F_{z_{t,3}}(x_{t})\lambda_{t+1}\|^{2}\right.\] \[\left.\hskip 28.452756pt+2\mathbb{E}_{A}\langle\nabla F_{S}(x_{t} )\lambda,\nabla F_{S}(x_{t})(\lambda_{t}-\lambda_{t+1})\rangle\right)\] \[\leq \frac{1}{2\gamma T}\mathbb{E}_{A}[\|\lambda_{1}-\lambda\|^{2}- \|\lambda_{T+1}-\lambda\|^{2}]+\frac{1}{\alpha T}\mathbb{E}_{A}[F_{S}(x_{1})- F_{S}(x_{T+1})]\lambda+\frac{1}{2}\gamma S_{1,T}+\gamma S_{3,T}+\frac{1}{2} \alpha\ell_{f,1}S_{4,T}.\] (C.41)

Setting \(\lambda=\lambda_{1}\) in the above inequality yields

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}\|\nabla F_{S}(x_{t}) \lambda_{t}\|^{2}\leq \frac{1}{\alpha T}\mathbb{E}_{A}[F_{S}(x_{1})-F_{S}(x_{T+1})| \lambda_{1}+\frac{1}{2}\gamma S_{1,T}+\gamma S_{3,T}+\frac{1}{2}\alpha\ell_{f, 1}S_{4,T}\]

Finally, the results follow from the definition of \(\lambda_{t}^{*}(x_{t})\) such that \(\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}\|\nabla F_{S}(x_{t})\lambda_{t}^{*}( x_{t})\|^{2}\leq\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}\|\nabla F_{S}(x_{t}) \lambda_{t}\|^{2}\). 

Proof of Theorem 3.: Lemma 19 states that, under Assumption 1, we have

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}\|\nabla F_{S}(x_{t}) \lambda_{t}^{*}(x_{t})\|^{2}\leq \frac{1}{\alpha T}\mathbb{E}_{A}[F_{S}(x_{1})-F_{S}(x_{T+1})] \lambda_{1}+\frac{1}{2}\gamma S_{1,T}+\gamma S_{3,T}+\frac{1}{2}\alpha\ell_{f, 1}S_{4,T}.\]

Then we proceed to bound \(S_{1,T},S_{3,T},S_{4,T}\). Under either Assumptions 1, 3, or Assumptions 1, 2 with \(\ell_{f}\), \(\ell_{F}\) defined in Lemma 1, we have that for all \(z\in S\) and \(\lambda\in\Delta^{M}\), \(\|\nabla F_{z}(x_{t})\lambda\|\leq\ell_{f}\), and \(\|\nabla F_{z}(x_{t})\|\leq\ell_{F}\). Then \(S_{1,T},S_{3,T},S_{4,T}\) can be bounded below

\[S_{1,T}= \frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}\|(\nabla F_{z_{t,1}}(x_ {t})^{\top}\nabla F_{z_{t,2}}(x_{t}))\lambda_{t}\|^{2}\leq M\ell_{f}^{4}\] (C.42a) \[S_{3,T}= \frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}\|\nabla F_{z_{t,1}}(x_ {t})^{\top}\nabla F_{z_{t,2}}(x_{t})\lambda_{t}\|\|\nabla F_{S}(x_{t})^{\top} \nabla F_{S}(x_{t})\lambda_{1}\|\leq\ell_{F}^{2}\ell_{f}^{2}=M\ell_{f}^{4}\] (C.42b) \[S_{4,T}= \frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}\|\nabla F_{z_{t,3}}(x_{ t})\lambda_{t+1}\|^{2}\leq\ell_{f}^{2}\] (C.42c)

which proves that

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}\|\nabla F_{S}(x_{t}) \lambda_{t}^{*}(x_{t})\|^{2}\leq \frac{1}{\alpha T}c_{F}+\frac{3}{2}\gamma M\ell_{f}^{4}+\frac{1}{2} \alpha\ell_{f,1}\ell_{f}^{2}.\] (C.43)

We arrive at the results by \(\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}\|\nabla F_{S}(x_{t})\lambda_{t}^{*}( x_{t})\|\leq\left(\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{A}\|\nabla F_{S}(x_{t}) \lambda_{t}^{*}(x_{t})\|^{2}\right)^{\frac{1}{2}}\) from the Jensen's inequality and the convexity of the square function, as well as the subadditivity of square root function.

## Appendix D Additional experiments and implementation details

Compute.Experiments are done on a machine with GPU NVIDIA RTX A5000. We use MATLAB R2021a for the synthetic experiments in strongly convex case, and Python 3.8, CUDA 11.7, Pytorch 1.8.0 for other experiments. Unless otherwise stated, all experiments are repeated with 5 random seeds. And their average performance and standard deviations are reported.

### Synthetic experiments

#### d.1.1 Experiments on strongly convex objectives

Implementation details.Below we provide the details of experiments that generate Figure 3. We use the following synthetic example for the experiments in the strongly convex case. The \(m\)-th objective function with stochastic data sample \(z\) is specified as

\[f_{z,m}(x)=\frac{1}{2}b_{1,m}x^{\top}Ax-b_{2,m}z^{\top}x\] (D.1)

where \(b_{1,m}>0\) for all \(m\in[M]\), and \(b_{2,m}\) is another scalar. We set \(M=3\), \(b_{1}=[b_{1,1};b_{1,2};b_{1,3}]=[1;2;1]\), and \(b_{2}=[b_{2,1};b_{2,2};b_{2,3}]=[1;3;2]\). The default parameters are \(T=100\), \(\alpha=0.01\), \(\gamma=0.001\). In other words, in Figure 2(a), we fix \(\alpha=0.01,\gamma=0.001\), and vary \(T\); in Figure 2(b), we fix \(T=100,\gamma=0.001\), and vary \(\alpha\); and in Figure 2(c), we fix \(T=100,\alpha=0.01\), and vary \(\gamma\).

#### d.1.2 Experiments on nonconvex objectives

Implementation details.The toy example is modified from [29] to consider stochastic data. Denote the model parameter as \(x=[x_{1},x_{2}]^{\top}\in\mathbb{R}^{2}\), stochastic data as \(z=[z_{1},z_{2}]^{\top}\in\mathbb{R}^{2}\) sampled from the standard multi-variate Gaussian distribution. And the individual empirical objectives are defined as:

\[f_{z,1}(x) =c_{1}(x)h_{1}(x)+c_{2}(x)g_{z,1}(x)\text{ and }f_{z,2}(x)=c_{1}(x)h_{2}(x)+c_{2}(x)g_{z,2}(x), \text{ where}\] \[h_{1}(x) =\log(\max(|0.5(-x_{1}-7)-\tanh(-x_{2})|,0.000005))+6,\] \[h_{2}(x) =\log(\max(|0.5(-x_{1}+3)-\tanh(-x_{2})+2|,0.000005))+6,\]

Figure 5: Convergence of MGDA, static weighting and MoDo to the empirical (gray, upper) and **population (green, lower)** Pareto fronts. Horizontal and vertical axes in figures in the first / second row are the values of the two empirical / population objectives. Three colormaps are used for the trajectories from three initializations, respectively, where the same colormaps represent the trajectories of the same initializations, and darker colors in one colormap indicate earlier iterations and lighter colors indicate later iterations.

\[g_{z,1}(x) =((-x_{1}+3.5)^{2}+0.1*(-x_{2}-1)^{2})/10-20-2*z_{1}x_{1}-5.5*z_{2}x_{2},\] \[g_{z,2}(x) =((-x_{1}-3.5)^{2}+0.1*(-x_{2}-1)^{2})/10-20+2*z_{1}x_{1}-5.5*z_{2}x _{2},\] \[c_{1}(x) =\max(\tanh(0.5*x_{2}),0)\text{ and }c_{2}(x)=\max(\tanh(-0.5*x_{2}),0).\] (D.2)

Since \(z\) is zero-mean, the individual population objectives are correspondingly:

\[f_{1}(x) =c_{1}(x)h_{1}(x)+c_{2}(x)g_{1}(x)\text{ and }f_{2}(x)=c_{1}(x)h_{2}(x)+c_ {2}(x)g_{2}(x),\text{ where }\] \[g_{1}(x) =((-x_{1}+3.5)^{2}+0.1*(-x_{2}-1)^{2})/10-20,\] \[g_{2}(x) =((-x_{1}-3.5)^{2}+0.1*(-x_{2}-1)^{2})/10-20.\] (D.3)

The training dataset size is \(n=|S|=20\). For all methods, i.e., MGDA, static weighting, and MoDo, the number of iterations is \(T=50000\). The initialization of \(\lambda\) is \(\lambda_{0}=[0.5,0.5]^{\top}\). The hyperparameters for this experiment are summarized in Table 5.

Figure 5, in addition to Figure 1, shows the trajectories of different methods from different initializations to the empirical and population Pareto fronts (PF). With the visualized empirical and population PFs, it is clear in Figure 4(a), the first row, that the three trajectories of MGDA all converge to the empirical PF, but, it stops updating the model parameter as soon as it reaches the empirical PF. However, due to the difference between the empirical and population PFs caused by finite stochastic training data, as shown in Figure 4(a), the second row, not all three solutions from MGDA has small population risk, implied from the distance of the solution (yellow point) to the population PF colored in green. For static weighting method with uniform weights in Figure 4(b), one trajectory is able to converge to the center of the empirical PF, which is the optimal solution of the uniform average of the two objectives. However, the other two get stuck and oscillate around suboptimal parameters for a long time, corresponding to the clusters of scattered points in the figure. Nevertheless, in the second row of Figure 4(b), one empirically suboptimal solution (on the trajectory with red to yellow colormap) is able to achieve small population risk. This example demonstrates that even though static weighting method does not have small distance to CA direction, it might still be able to achieve small testing error. Finally, for MoDo in Figure 4(c), the first row shows that MoDo is slower than MGDA in convergence to the empirical PF, since it only approximately solves the CA direction using

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline  & Static & MGDA & MoDo \\ \hline optimizer of \(x_{t}\) & Adam & Adam & Adam \\ \(x_{t}\) step size (\(\alpha_{1}\)) & \(5\times 10^{-3}\) & \(5\times 10^{-3}\) & \(5\times 10^{-3}\) \\ \(\lambda_{t}\) step size (\(\gamma_{t}\)) & - & - & \(10^{-4}\) \\ batch size & \(16\) & full & \(16\) \\ \hline \end{tabular}
\end{table}
Table 5: Summary of hyper-parameter choices for nonconvex synthetic data.

Figure 6: Trajectories of MoDo under different \(\gamma\) on the contour of the average of objectives. The **black**\(\bullet\) marks initializations of the trajectories, colored from **red** (start) to yellow (end). The background solid/dotted contours display the landscape of the average empirical/population objectives. The gray/green bar marks empirical/population Pareto front, and the **black \(\star\)/green \(\star\)** marks solution to the average objectives.

[MISSING_PAGE_FAIL:45]

different loss functions applied for training an image classifier for MNIST handwritten digit dataset. We consider three loss functions: cross entropy, mean squared error (MSE), and Huber loss. The model architecture is a two-layer multi-layer perceptron (MLP). Each hidden layer has 512 neurons, and no hidden layer activation. The input size is 784, and the output size is 10, the number of digit classes. The training, validation, and testing data sizes are 50k, 10k, and 10k, respectively. Hyper-parameters such as step sizes are chosen based on each algorithm's validation accuracy performance, as given in Table 6.

We then discuss the results of the experiments in Table 7, which shows the performance of the last iterate for each method. Experiments are repeated 10 times with average performance and standard deviations reported. Observed from Table 7, for cross-entropy loss, MGDA performs the worst while static weighting performs the best. On the other hand, for Huber loss, MGDA performs the best while static weighting performs the worst. This is not surprising as cross-entropy loss has the largest scale and Huber loss has the smallest scale among the three losses. Since equal weights are assigned for all three objectives in the static weighting method, it tends to focus more on optimizing the loss with the largest scale. While for MGDA, it is the other way around. Compared to MGDA, MoDo performs much better on cross-entropy loss, and in the meantime, it achieves comparable performance on Huber loss. Comparing their PS population risks \(R_{\mathrm{pop}}\) and the decomposed PS optimization errors \(R_{\mathrm{opt}}\) and PS generalization errors \(R_{\mathrm{gen}}\), MGDA has the smallest PS population risk and PS optimization error. One potential reason is that MGDA performs best on Huber loss, with smaller gradients. Another reason is that the generalization errors for all the algorithms are similar and not dominating compared to optimization errors in this setting, making the PS population risk close to the PS optimization error. Overall, MoDo demonstrates a balance among objectives with different scales and performs reasonably well on all three objectives since it combines the properties of static weighting and MGDA.

#### d.2.2 Office-31 and Office-home dataset experiments

Implementation details.We give the details for the experiments conducted using Office-31 and Office-home datasets, which consist of multi-domain image classification tasks. Both of these are multi-input single-task learning problems. Office-31 and Office-home consist of 31 and 65 image classes, respectively. The image domains for Office-31 are; "Amazon", which includes object images from Amazon, "DSLR", which includes high-resolution images of objects, and "Webcam", which includes low-resolution images from objects. The image domains for Office-31 are "Art", "Clipart", "Product", and "Real-world" which include images of objects taken from the respective image domains. We use the MTL benchmark framework LibMTL [26] to run experiments on both of the aforementioned datasets. For both datasets, we tune the step size of \(x\) and weight decay parameters for Static and MGDA algorithms and tune the step sizes of \(x\) and \(\lambda\) for the MoDo algorithm. We use batch size 64 to update static weighting and MGDA, and use 2 independent samples of batch size 32 to update MoDo, for both Office-31 and Office-home. A summary of hyper-parameters used for

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline  & Static & MGDA & MoDo \\ \hline optimizer of \(x_{t}\) & SGD & SGD & SGD \\ \(x_{t}\) step size (\(\alpha_{t}\)) & 0.1 & 5.0 & 1.0 \\ \(\lambda_{t}\) step size (\(\gamma_{t}\)) & - & - & 1.0 \\ batch size & 64 & 64 & 64 \\ \hline \end{tabular}
\end{table}
Table 6: Summary of hyper-parameter choices for MNIST image classification

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Method} & Cross-entropy & MSE loss & Huber loss & \multirow{2}{*}{\(R_{\mathrm{pop}}\) (\(10^{-3}\)) \(\downarrow\)} & \multirow{2}{*}{\(R_{\mathrm{opt}}\) (\(10^{-3}\)) \(\downarrow\)} & \multirow{2}{*}{\(|R_{\mathrm{gen}}\) (\(10^{-3}\)) \(\downarrow\)} \\ \cline{2-2} \cline{5-6}  & Loss (\(10^{-3}\)) \(\downarrow\) & Loss (\(10^{-3}\)) \(\downarrow\) & Loss (\(10^{-3}\)) \(\downarrow\) & & \\ \hline Static & **306.9\(\pm\)3.9** & 13.2\(\pm\)0.14 & 2.2\(\pm\)0.03 & 2.1\(\pm\)0.56 & 1.9\(\pm\)0.5 & 0.2\(\pm\)0.19 \\ MGDA & 363.6\(\pm\)4.1 & 13.5\(\pm\)0.13 & **1.9\(\pm\)0.01** & **1.3\(\pm\)0.24** & **1.1\(\pm\)0.2** & 0.2\(\pm\)0.13 \\
**MoDo** & 317.9\(\pm\)3.4 & **13.1\(\pm\)0.13** & 2.1\(\pm\)0.05 & 2.1\(\pm\)0.38 & 1.9\(\pm\)0.4 & **0.1\(\pm\)0.09** \\ \hline \hline \end{tabular}
\end{table}
Table 7: MNIST classification with cross-entropy, MSE, and Huber loss as objectives.

Office-31 and Office-home for each algorithm are given in Table 8 and Table 9, respectively. All other experiment setup is shared for all algorithms, and the default LibMTL configuration is used.

The results of Office-31 and Office-home experiments are given in Tables 3 and 10, respectively (average over 5 seeds, the error indicates standard deviation). Here, we use the average per-task performance drop of metrics \(S_{A,m}\) for method \(\mathcal{A}\) with respect to corresponding baseline measures \(S_{\mathcal{B},m}\) as a measure of the overall performance of a given method. Specifically, this measure is

\[\Delta\mathcal{A}\%\!=\!\frac{1}{M}\sum_{m=1}^{M}(-1)^{\ell_{m}}\left(S_{ \mathcal{A},m}-S_{\mathcal{B},m}\right)\!/\!S_{\mathcal{B},m}\times 100,\] (D.4)

where \(M\) is the number of tasks. Here, \(\ell_{m}\!=\!1\) if higher values for \(S_{\mathcal{A},m}\) are better and \(0\) otherwise. We use the best results for each task obtained by dedicated independent task learners of each task as \(S_{\mathcal{B},m}\). The independent task learners are tuned for the learning rate and weight decay parameter. For Office-31, \(S_{\mathcal{B},m}\) values are \(87.50\%\) for "Amazon", \(98.88\%\) for "DSLR", and \(97.32\%\) for "Webcam". For Office-home, \(S_{\mathcal{B},m}\) values are \(66.98\%\) for "Art", \(82.02\%\) for "Clipart", \(91.53\%\) for "Product", and \(80.97\%\) for "Real-world". It can be seen from Tables 3 and 10 that static weighting outperforms MGDA method in some tasks and also in terms of \(\Delta\mathcal{A}\%\). However, by proper choices of hyper-parameters, MoDo performs on par or better compared to both static weighting and MGDA, and hence achieves the best overall performance in terms of \(\Delta\mathcal{A}\%\).

#### d.2.3 NYU-v2 dataset experiments

Implementation details.We give the details for the experiments conducted using NYU-v2 dataset, which consists of image segmentation, depth estimation, and surface normal estimation tasks. Unlike Office-31 and Office-home datasets, this is a single-input multi-task learning problem. The

\begin{table}
\begin{tabular}{c c c c c} \hline \hline \multirow{2}{*}{Method} & Art & Clipart & Product & Real-world \\ \cline{2-5}  & Test Acc \(\uparrow\) & Test Acc \(\uparrow\) & Test Acc \(\uparrow\) & \\ \hline Static & 64.14 \(\pm\) 1.40 & **79.57**\(\pm\) 1.09 & 90.00 \(\pm\) 0.50 & 78.94 \(\pm\) 0.87 & 2.85 \(\pm\) 1.08 \\ MGDA & 61.71 \(\pm\) 1.33 & 73.95 \(\pm\) 0.43 & **90.17**\(\pm\)**0.27 & 79.35 \(\pm\) 1.15 & 5.29 \(\pm\) 0.47 \\
**MoDo** & **65.50**\(\pm\)**0.55 & 79.44 \(\pm\) 0.29 & 89.72 \(\pm\) 0.94 & **79.65**\(\pm\)**0.67 & **2.24**\(\pm\)**0.48** \\ \hline \hline \end{tabular}
\end{table}
Table 10: Classification results on Office-home dataset.

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline  & Static & MGDA & MoDo \\ \hline optimizer of \(x_{t}\) & Adam & Adam & Adam \\ \(x_{t}\) step size (\(\alpha_{t}\)) & \(10^{-4}\) & \(10^{-4}\) & \(10^{-4}\) \\ \(\lambda_{t}\) step size (\(\gamma_{t}\)) & - & - & \(10^{-3}\) \\ weight decay & \(10^{-3}\) & \(10^{-7}\) & \(10^{-5}\) \\ batch size & 64 & 64 & 64 \\ \hline \end{tabular}
\end{table}
Table 8: Summary of hyper-parameter choices for Office-31 task

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline  & Static & MGDA & MoDo \\ \hline optimizer of \(x_{t}\) & Adam & Adam & Adam \\ \(x_{t}\) step size (\(\alpha_{t}\)) & \(10^{-4}\) & \(10^{-4}\) & \(10^{-4}\) \\ \(\lambda_{t}\) step size (\(\gamma_{t}\)) & - & - & \(10^{-3}\) \\ weight decay & \(10^{-3}\) & \(10^{-6}\) & \(10^{-5}\) \\ batch size & 64 & 64 & 64 \\ \hline \end{tabular}
\end{table}
Table 9: Summary of hyper-parameter choices for Office-home task dataset consists of images from indoor video sequences. We use the MTL benchmark framework LibMTL [26] to run experiments with this dataset. We tune step size of \(x\) and weight decay parameters for static weighting and MGDA algorithms, and tune step sizes of \(x\) and \(\lambda\) for MoDo algorithm. We use batch size 4 to update static weighting and MGDA, and use 2 independent batches of size 2 to update MoDo. Experiments were run for 50 epochs for all methods. Since there was no validation set for NYU-v2, we averaged and reported the test performance of the last 10 epochs. A summary of hyper-parameters used for each algorithm is given in Table 11. All other experiment setup is shared for all algorithms, and the default LibMTL configuration is used.

The results of NYU-v2 experiments are given in Tables 12 (average over 3 seeds, the error indicates standard deviation). Again, we use the average per-task performance drop of metrics \(S_{\mathcal{A},m}\) for method \(\mathcal{A}\) with respect to corresponding baseline measures \(S_{\mathcal{B},m}\) as a measure of the overall performance of a given method. We use the best results for each task obtained by dedicated independent task learners of each task as \(S_{\mathcal{B},m}\). The independent task learners are tuned for the learning rate and weight decay parameter. For segmentation task, \(S_{\mathcal{B},m}\) values are \(53.94\%\) for "mIoU", and \(75.67\%\) for "Pix Acc". For depth estimation task, \(S_{\mathcal{B},m}\) values are \(0.3949\) for "Abs Err", and \(0.1634\) for "Rel Err". For surface normal estimation task, \(S_{\mathcal{B},m}\) values are \(22.12\) for "Angle Distance - Mean", \(15.49\) for "Angle Distance - Median", \(38.35\%\) for "Within \(11.25^{\circ}\)", \(64.30\%\) for "Within \(22.5^{\circ}\)", and \(74.70\%\) for "Within \(30^{\circ}\)". It can be seen from Table 12 that MoDo outperforms both MGDA and static weighting in some tasks and also in terms of \(\Delta\mathcal{A}\%\). The worse performance of MGDA in terms of \(\Delta\mathcal{A}\%\) can be due to the large bias towards the Surface normal estimation task, which seems to affect other tasks adversely.

#### d.2.4 Additional experiments for comparison with other MOL baselines

In this section, we provide a comparison between MoDo and other popular MOL baselines. For this purpose, we use the same benchmark datasets as the previous section and use the experiment setup provided in [25] to run experiments with MoDo. Hence, we use experiment results provided by [25] for other baselines for comparison. Additionally, we implement MoCo [10], which is not included in [25]. For results in this section we report two holistic measures: \(\Delta\mathcal{A}_{\text{st}}\), which measures performance degradation w.r.t. static scalarization (similar to [25], but lower the better), and \(\Delta\mathcal{A}_{\text{id}}\), which measures performance degradation w.r.t. independent task learners (as defined in our paper)

The results on Office-31, Office-home, and NYU-v2 are given in Tables 13, 14, and 15, respectively, where MoDo outperforms all the baselines for most tasks, and has a better overall performance in \(\Delta\mathcal{A}_{\text{st}}\%\) and \(\Delta\mathcal{A}_{\text{id}}\%\). The hyper-parameters of MoDo for the above experiments are in Table 16.

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline  & Static & MGDA & MoDo \\ \hline optimizer of \(x_{t}\) & Adam & Adam & Adam \\ \(x_{t}\) step size (\(\alpha_{t}\)) & \(10^{-4}\) & \(10^{-4}\) & \(10^{-4}\) \\ \(\lambda_{t}\) step size (\(\gamma_{t}\)) & - & - & \(10^{-3}\) \\ weight decay & \(10^{-4}\) & \(10^{-6}\) & \(10^{-5}\) \\ batch size & 4 & 4 & 4 \\ \hline \end{tabular}
\end{table}
Table 11: Summary of hyper-parameter choices for NYU-v2 task

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c}{Segmentation} & \multicolumn{2}{c}{Depth} & \multicolumn{4}{c}{Surface Normal} \\ \cline{2-10}  & \multicolumn{2}{c}{(Higher Better)} & \multicolumn{2}{c}{(Lower Betec)} & \multicolumn{2}{c}{Angle Distance} & \multicolumn{2}{c}{Within t\({}^{\circ}\)} & \multirow{2}{*}{\(\Delta\mathcal{A}\%\downarrow\)} \\  & mIoU & Fix Acc & Abs Err & Rel Err & Mean & Median & 11.25 & 22.5 & 30 \\ \hline Static: & 52.02 \(\pm\) 0.69 & 74.21 \(\pm\) 0.57 & **0.3984 \(\pm\) 0.0032** & **0.1645 \(\pm\) 0.0010** & 23.79 \(\pm\) 0.10 & 17.44 \(\pm\) 0.15 & 34.07 \(\pm\) 0.17 & 60.17 \(\pm\) 0.31 & 71.48 \(\pm\) 0.29 & 3.98 \(\pm\) 0.70 \\ MGDA & 46.39 \(\pm\) 0.17 & 70.27 \(\pm\) 0.24 & 0.4269 \(\pm\) 0.0024 & 0.1737 \(\pm\) 0.0009 & **22.34 \(\pm\) 0.03** & **15.70 \(\pm\) 0.08** & **37.71 \(\pm\) 0.21** & **63.96 \(\pm\) 0.11** & **74.59 \(\pm\) 0.06** & 6.25 \(\pm\) 0.38 \\
**MoDo** & **52.64 \(\pm\) 0.19** & **74.67 \(\pm\) 0.08** & **0.3984 \(\pm\) 0.0020** & 0.1649 \(\pm\) 0.0018 & 23.45 \(\pm\) 0.06 & 17.09 \(\pm\) 0.05 & 34.79 \(\pm\) 0.11 & 60.90 \(\pm\) 0.13 & 72.12 \(\pm\) 0.11 & **3.21 \(\pm\) 0.34** \\ \hline \hline \end{tabular}
\end{table}
Table 12: Segmentation, depth, and surface normal estimation results on NYU-v2 dataset.

[MISSING_PAGE_FAIL:49]