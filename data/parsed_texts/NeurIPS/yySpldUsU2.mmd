# Changing the Training Data Distribution to

Reduce Simplicity Bias Improves

In-distribution Generalization

Tuan Hai Dang Nguyen  Paymon Haddad  Eric Gan  Baharan Mirzasoleiman

Department of Computer Science, UCLA

###### Abstract

Can we modify the training data distribution to encourage the underlying optimization method toward finding solutions with superior generalization performance on _in-distribution_ data? In this work, we approach this question for the first time by comparing the inductive bias of gradient descent (GD) with that of sharpness-aware minimization (SAM). By studying a two-layer CNN, we rigorously prove that SAM learns different features more uniformly, particularly in early epochs. That is, SAM is less susceptible to simplicity bias compared to GD. We also show that examples containing features that are learned early are separable from the rest based on the model's output. Based on this observation, we propose a method that (i) clusters examples based on the network output early in training, (ii) identifies a cluster of examples with similar network output, and (iii) upsamples the rest of examples only once to alleviate the simplicity bias. We show empirically that USEFUL effectively improves the generalization performance on the _original_ data distribution when training with various gradient methods, including (S)GD and SAM. Notably, we demonstrate that our method can be combined with SAM variants and existing data augmentation strategies to achieve, to the best of our knowledge, state-of-the-art performance for training ResNet18 on CIFAR10, STL10, CINIC10, Tiny-ImageNet; ResNet34 on CIFAR100; and VGG19 and DenseNet121 on CIFAR10.

## 1 Introduction

Training data is a key component of machine learning pipelines and directly impacts its performance. Over the last decade, there has been a large body of efforts concerned with improving learning from a given training dataset by designing more effective optimization methods [22, 38, 76] or neural networks with improved structures [47, 56, 82] or higher-capacity [49, 51]. More recently, improving the quality of the training data has emerged as a popular avenue to improve generalization performance. Interestingly, higher-quality data can further improve the performance when larger models and better optimization methods are unable to do so [23, 27]. Recent efforts to improve the data quality have mainly focused on filtering irrelevant, noisy, or harmful examples [23, 45, 66]. Nevertheless, it remains an open question if one can change the distribution of a _clean_ training data to further improve the _in-distribution_ generalization performance of models trained on it.

At first glance, the above question may seem unnatural, as it disputes a fundamental assumption that training and test data should come from the same distribution [29]. Under this assumption, minimizing the training loss generalizes well on the test data [7]. Nevertheless, for overparameterized neural networks with more parameters than training data, there are many zero training error solutions, all global minima of the training objective, with _different generalization_ performance [25]. Thus, one may still hope to carefully change the data distribution to drive the optimization algorithms towards finding more generalizable solutions on the _original_ data distribution.

In this work, we take the first steps towards addressing the above problem. To do so, we rely on recent results in non-convex optimization, showing the superior generalization performance of sharpness-aware-minimization (SAM) [22] over (stochastic) gradient descent (GD). SAM finds flatter local minima by simultaneously minimizing the loss value and loss sharpness. In doing so, it outperforms (S)GD and obtains state-of-the-art performance, at the expense of doubling the training time [20; 81]. Our key idea is that if one can change the training data distribution such that learning shares similar properties to that of training with SAM, then the new distribution can drive (S)GD and even SAM toward finding more generalizable solutions.

To address the above question, we first theoretically analyze the dynamics of training a two-layer convolutional neural network (CNN) with SAM and compare it with that of GD. We rigorously prove that SAM learns different features in a more _uniform speed_ compared to GD, particularly _early_ in training. In other words, we show that _SAM is less susceptible to simplicity bias_ than GD. Simplicity bias of SGD makes the model learn simple solutions with minimum norm [25] and has long been conjectured to be the reason for the superior generalization performance of overparameterized models by providing implicit regularization [7; 25; 28; 51; 52; 70]. Nevertheless, the minimum-norm solution found by GD can have a suboptimal performance [64].

Following our theoretical results, we formulate changing the distribution of a training dataset such that different features are learned at a more uniform speed. First, we prove that the model output for examples containing features that are learned early by GD is separable from the rest of examples in their class. Then, we propose changing the data distribution by (i) identifying a cluster of examples with similar model output early in training, (ii) upsampling the remaining examples once to speed up their learning, and (iii) restarting training on the modified training distribution. Our method, UpSample Early For Uniform Learning (USEFUL), effectively alleviates the simplicity bias and consequently improves the generalization performance. Intuitively, learning features in a more uniform speed prevents the model to overfit underrepresented but useful features that otherwise are learned in late training stages. When the model overfits an example, it cannot learn its features in a generalizable manner. This harms the generalization performance on the original data distribution.

We show the effectiveness of USEFUL in alleviating the simplicity bias and improving the generalization via extensive experiments. First, we show that despite being relatively lightweight, USEFUL effectively improves the generalization performance of SGD and SAM. Additionally, we show that USEFUL can be easily applied with various optimizers and data augmentation methods to improve in-distribution generalization performance even further. For example, applying USEFUL with SAM and TrivialAugment (TA) [50] achieves, to the best of our knowledge, _state-of-the-art_ accuracy for image classification for training ResNet18 on CIFAR10, STL10, CINIC10, Tiny-ImageNet; ResNet34 on CIFAR100; and VGG19 and DenseNet121 on CIFAR10. We also empirically confirm the benefits of USEFUL to out-of-distribution performance, but we emphasize that this is not the focus of our work.

## 2 Related Works

**Sharpness-aware-minimization (SAM).** Motivated by the generalization advantages of flat local minima, sharpness-aware minimization (SAM) was concurrently proposed in [22; 81] to minimize the training loss at the worst perturbed direction from the current parameters. SAM has been shown to obtain state-of-the-art on a variety of tasks [22]. Additionally, SAM has been shown to be beneficial in other settings, including label noise [22; 81], and domain generalization [9; 72].

There have been recent efforts to understand the generalization benefits of SAM. The most popular explanation is based on the Hessian spectra, empirically [22; 36] and theoretically [6; 73]. Other works showed that SAM finds a sparser solution in diagonal linear networks [3], and exhibits benign overfitting under much weaker signal strength compared to (S)GD [12]. More recently, SAM is shown to also benefit out-of-distribution (OOD). In particular, [65] suggested that SAM promotes diverse feature learning by empirically studying a simplified version of SAM which only perturbs the last layer. They showed that SAM upscales the last layer's weights to induce feature diversity, which benefits OOD. In contrast, we rigorously analyze a 2-layer non-linear CNN and prove that SAM learns (the same set of) features at a more uniform speed, which benefits the in-distribution (ID) settings. Our results reveal an orthogonal effect of SAM that benefits the ID generalization by reducing the simplicity bias, and provides a complementary view to prior works explaining superior ID generalization performance of SAM. We then propose a method to learn features more evenly by changing the data distribution.

**Simplicity bias (SB).** (S)GD has an inductive bias towards learning simpler solutions with minimum norm [25]. It is empirically observed [34] and theoretically proved [28] that SGD learns linear functions in the early training phase and more complex functions later in training. SB of SGD has been long conjectured to be the reason for the superior in-distribution generalization performance of overparameterized models, by providing capacity control or implicit regularization [26; 52; 55; 63]. On the other hand, in the OOD setting, simplicity bias is known to contribute to shortcut learning by causing models to exclusively rely on the simplest spurious feature and remain invariant to the complex but more predictive features [63; 67; 75]. Prior works on mitigating simplicity bias have been shown effective in the OOD settings [67; 68]. In contrast, our work shows, for the first time, that reducing the simplicity bias also benefits the ID settings. By studying the mechanism of feature learning in a two-layer nonlinear CNN, we prove that SAM is less susceptible to simplicity bias than GD, in particular early in training, which contributes to its superior performance. Then, we show that training data distribution can be modified to reduce the SB and improve the in-distribution generalization. In Appendix D.7, we empirically confirm that existing simplicity bias mitigation methods also improve the in-distribution performance, but to a smaller extent than ours.

**Distinction from Existing Settings.** Our work is distinct from the following literature:

(1) _Distribution Shift._ Unlike distribution shift and shortcut learning [18; 39; 57; 61], we _do not_ assume existence of domain-dependent (non-generalizable) features or strong spurious correlations in the training data, or shift between training and test distribution. We focus on _in-distribution_ generalization, where training and test distributions are the same and all the features in the training data are relevant for generalization. In Appendix D.5 we empirically show the benefits of our method to distribution shift, but we emphasize that this is not the focus of our study and we leave this direction to future work.

(2) _Long-tail distribution._ Long-tailed data is studied as a special case of distribution shift in which (sub)classes are highly imbalanced in training but are (more) balanced in test data [15; 71]. Long-tail methods resample the data at the class or subclass level to match the training and test distribution. In contrast, in our settings, training and test data follow the same distribution. Nevertheless, our method can be applied to improve the performance of long-tail datasets, as we confirm in Appendix D.5.

(3) _Improving Convergence._ A body of work speeds up convergence of (S)GD to find the _same solution_ faster. Such methods iteratively sample or reweight examples based on loss or gradient norm during training [21; 33; 35; 80]. In contrast, our work does not intend to speed up training to find the same solution faster, but intends to find a _more generalizable solution_ on the original data distribution.

(4) _Data Filtering Methods._ Filtering methods identify and discard or downweight noisy labeled [45], domain mismatched [23], redundant [1; 44; 59], or adversarial examples crafted by data poisoning attacks [66]. In contrast, we assume a _clean_ training data and no mismatch between training and test distribution. Our work can be applied to a filtered training data to further improve the performance.

## 3 Theoretical Analysis: SAM Learns Different Features More Evenly

In this section, we analyze and compare feature learning mechanism of SAM. First, we introduce our theoretical settings including data distribution and neural network model in Sec. 3.1. We then revisit the update rules of GD and SAM in Sec. 3.2 before presenting our theoretical results in Sec. 3.3.

### Theoretical Settings

**Notation.** We use lowercase letters, lowercase boldface letters, and uppercase boldface letters to denote scalars \((a)\), vectors \((\bm{v})\), and matrices \((\bm{W})\). For a vector \(\bm{v}\), we use \(\left\|\bm{v}\right\|_{2}\) to denote its Euclidean norm. Given two sequence \(\{x_{n}\}\) and \(\{y_{n}\}\), we denote \(x_{n}=O(y_{n})\) if \(\left|x_{n}\right|\leq C_{1}|y_{n}|\) for some absolute positive constant \(C_{1}\), \(x_{n}=\Omega(y_{n})\) if \(\left|x_{n}\right|\geq C_{2}|y_{n}|\) for some absolute positive constant \(C_{2}\), and \(x_{n}=\Theta(y_{n})\) if \(C_{3}|y_{n}|\leq\left|x_{n}\right|\leq C_{4}|y_{n}|\) for some absolute constant \(C_{3},C_{4}>0\). Besides, we use \(\tilde{O}(\cdot),\tilde{\Omega}(\cdot),\) and \(\tilde{\Theta}(\cdot)\) to hide logarithmic factors in these notations. Furthermore, we denote \(x_{n}=\text{poly}(y_{n})\) if \(x_{n}=O(y_{n}^{D})\) for some positive constant D, and \(x_{n}=\text{polylog}(y_{n})\) if \(x_{n}=\text{poly}(\log(y_{n}))\).

**Data distribution.** We use a popular data distribution used in recent works on feature learning [2; 8; 11; 12; 18; 32; 40] to represent data as a combination of two features and noise patches. Additionally, we introduce a probability \(\alpha\) to control the frequency of fast-learnable features in the data distribution.

**Definition 3.1** (Data distribution).: A data point \((\bm{x},y)\in(\mathbb{R}^{d})^{P}\times\)\(\{\pm 1\}\) is generated from the distribution \(\mathcal{D}(\beta_{e},\beta_{d},\alpha)\) as follows. We uniformly generate the label \(y\in\{\pm 1\}\). We generate \(\bm{x}\) as a collection of \(P\) patches: \(\bm{x}=(\bm{x}^{(1)},\bm{x}^{(2)},\dots,\bm{x}^{(P)})\in(\mathbb{R}^{d})^{P}\), where

* **Slow-learnable Feature.** One and only one patch is given by \(\beta_{d}\cdot y\cdot\bm{v}_{d}\) with \(\left\lVert\bm{v}_{d}\right\rVert_{2}=1\), \(\left\langle\bm{v}_{e},\bm{v}_{d}\right\rangle=0\), and \(0\leq\beta_{d}<\beta_{e}\in\mathbb{R}\).
* **Fast-learnable feature.** One and only one patch is given by \(\beta_{e}\cdot y\cdot\bm{v}_{e}\) with \(\left\lVert\bm{v}_{e}\right\rVert_{2}=1\) with a probability \(\alpha\leq 1\). With a probability of \(1-\alpha\), this patch is masked, i.e. \(\bm{0}\).
* **Random noise.** The rest of \(P-2\) patches are Gaussian noise \(\bm{\xi}\) that are independently drawn from \(N(0,(\sigma_{p}^{2}/d)\cdot\mathbf{I}_{d})\) with \(\sigma_{p}\) as an absolute constant.

For simplicity, we assume \(P=3\), and the noisy patch together with two features form an orthogonal set. Coefficients \(\beta_{e}\) and \(\beta_{d}\) characterize the feature strength in our data model. A larger coefficient means that the corresponding feature is learned faster.

**Two-layer nonlinear CNN.** To model modern state-of-the-art architectures, we analyze a two-layer nonlinear CNN which is also used in [8; 11; 18; 32; 40]. Unlike linear models, CNN can handle a data distribution that does not require a fixed position of patches as defined above. Formally,

\[f(\bm{x};\bm{W})=\sum_{j\in[J]}\sum_{p=1}^{P}\sigma(\left\langle\bm{w}_{j}, \bm{x}^{(p)}\right\rangle),\] (1)

where \(\bm{w}_{j}\in\mathbb{R}^{d}\) is the weight vector of the \(j\)-th filter, \(J\) is the number of filters (neurons) of the network, and \(\sigma(z)=z^{3}\) is the activation function, i.e., the main source of non-linearity. \(\bm{W}=[\bm{w}_{1},\dots,\bm{w}_{J}]\in\mathbb{R}^{d\times J}\) is the weight matrix of the CNN. Following [8; 18; 32], we assume a mild over-parameterization with \(J=\text{polylog}(d)\). We initialize \(\bm{W}^{(0)}\sim\mathcal{N}(0,\sigma_{0}^{2})\), where \(\sigma_{0}^{2}=\text{polylog}(d)/d\).

### Empirical Risk Minimization: GD vs SAM

Consider a \(N\)-sample training dataset \(D=\{(\bm{x}_{i},y_{i})\}_{i=1}^{N}\) in which each data point is generated from the data distribution in Definition 3.1. The empirical loss function of a model \(f(\bm{x};\bm{W})\) reads

\[\mathcal{L}(\bm{W})=\frac{1}{N}\sum_{i=1}^{N}l(y_{i}f(\bm{x}_{i};\bm{W})),\] (2)

where \(l\) is the logistic loss defined as \(l(z)=\log(1+\exp(-z))\). The solution \(\bm{W}^{\star}\) of the empirical risk minimization (ERM) minimizes the above loss, i.e., \(\bm{W}^{\star}\coloneqq\arg\min_{\bm{W}}\mathcal{L}(\bm{W})\).

**GD.** Typically, ERM is solved using gradient descent (GD). The update rule at iteration \(t\) of GD with learning rate \(\eta>0\) reads

\[\bm{W}^{(t+1)}=\bm{W}^{(t)}-\eta\nabla\mathcal{L}(\bm{W}^{(t)}).\] (3)

**SAM.** To find solutions with better generalization performance, Foret et al. [22] proposed the \(N\)-SAM algorithm that minimizes both loss and curvature. SAM's update rule at iteration \(t\) reads

\[\bm{W}^{(t+1)}=\bm{W}^{(t)}-\eta\nabla\mathcal{L}(\bm{W}^{(t)}+\rho^{(t)} \nabla\mathcal{L}(\bm{W}^{(t)})),\] (4)

where \(\rho^{(t)}=\rho>0\) is the inner step size that is usually normalized by gradient norm, i.e., \(\rho^{(t)}=\rho/\left\lVert\nabla\mathcal{L}(\bm{W}^{(t)})\right\rVert_{F}\).

### Comparing Learning Between fast-learnable & slow-learnable Features for GD & SAM

Next, we present our theoretical results on training dynamics of the two-layer nonlinear CNN using GD and SAM. We characterize the learning speed of features by studying the growth of the model outputs before the activation function, i.e., \(\left\langle\bm{w}_{j}^{(t)},\bm{v}_{e}\right\rangle\) and \(\left\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\right\rangle\). We first prove that _early_ in training, both GD and SAM _only_ learn fast-learnable feature. Then, we show SAM learns slow-learnable and fast-learnable features at a more uniform speed.

**Theorem 3.2** (**GD Feature Learning)**.: _Consider training a two-layer nonlinear CNN model initialized with \(\bm{W}^{(0)}\sim\mathcal{N}(0,\sigma_{0}^{2})\) on the training dataset \(D=\{(\bm{x}_{i},y_{i})\}_{i=1}^{N}\) with distribution \(\mathcal{D}(\beta_{e},\beta_{d},\alpha)\) with \(\alpha^{1/3}\beta_{e}>\beta_{d}\). For a small-enough learning rate \(\eta\), after training for \(T_{\text{GD}}\) iterations, w.h.p., the model: (1) learns the fast-learnable feature \(\bm{v}_{e}\colon\max_{j\in[J]}\langle\bm{w}_{j}^{(T_{\text{GD}})},\bm{v}_{e} \rangle\geq\tilde{\Omega}(1/\beta_{e})\); (2) does not learn the slow-learnable feature \(\bm{v}_{d}\colon\max_{j\in[J]}\langle\bm{w}_{j}^{(T_{\text{GD}})},\bm{v}_{d} \rangle=\tilde{O}(\sigma_{0})\)._

**Theorem 3.3** (**SAM Feature Learning)**.: _Consider training a two-layer nonlinear CNN model initialized with \(\bm{W}^{(0)}\sim\mathcal{N}(0,\sigma_{0}^{2})\) on the training dataset \(D=\{(\bm{x}_{i},y_{i})\}_{i=1}^{N}\) with distribution \(\mathcal{D}(\beta_{e},\beta_{d},\alpha)\) with \(\alpha^{1/3}\beta_{e}>\beta_{d}\). For small-enough learning rate \(\eta\) and perturbation radius \(\rho\), after training for \(T_{\text{SAM}}>T_{\text{GD}}\) iterations, w.h.p., the model: (1) learns the fast-learnable feature \(\bm{v}_{e}:\max_{j\in[J]}\langle\bm{w}_{j}^{(T_{\text{SAM}})},\bm{v}_{e} \rangle\geq\tilde{\Omega}(1/\beta_{e})\); (2) does not learn the slow-learnable feature \(\bm{v}_{d}:\max_{j\in[J]}\langle\bm{w}_{j}^{(T_{\text{SAM}})},\bm{v}_{d} \rangle=\tilde{O}(\sigma_{0})\)._

The detailed proof of Theorems 3.2 and 3.3 are deferred to Appendices A.1 and A.2.

**Discussion.** Note that a larger value of \(\langle\bm{w}_{j}^{(t)},\bm{v}\rangle\) for \(\bm{v}\in\{\bm{v}_{e},\bm{v}_{d}\}\) indicates better learning of the feature vector \(\bm{v}\) by neuron \(\bm{w}_{j}\) at iteration \(t\). From the above two theorems, the growth rate of the fast-learnable feature is significantly faster than that of the slow-learnable feature. As a small portion \((1-\alpha)\) of the dataset does not have the fast-learnable feature, the model needs to learn the slow-learnable feature to improve the performance.

Next, we show that SAM learns fast-learnable and slow-learnable features more evenly. We denote by \(G_{e}^{(t)}=\max_{j\in[J]}\langle\bm{w}_{j}^{(t)},\bm{v}_{e}\rangle\) and \(G_{d}^{(t)}=\max_{j\in[J]}\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle\) the alignment of model weights with fast-learnable and slow-learnable features, when training with GD. Similarly, we denote by \(S_{e}^{(t)}\) and \(S_{d}^{(t)}\) the alignment of model weights with fast-learnable and slow-learnable, when training with SAM.

**Theorem 3.4** (**SAM learns features more evenly than GD)**.: _Consider the same model and training dataset as Theorems 3.2 and 3.3. Assume that the learning rate \(\eta\) and the perturbation radius \(\rho\) are sufficiently small. Starting from the same initialization, the growth of fast-learnable and slow-learnable features in SAM is more balanced than that in SGD, i.e., for every iteration \(t\in[1,T_{0}]\):_

\[S_{e}^{(t)}-S_{d}^{(t)}<G_{e}^{(t)}-G_{d}^{(t)}.\] (5)

We prove Theorem 3.4 by induction in Appendix A.2 and back it by toy experiments in Section 5.1.

**Discussion.** Intuitively, our proof is based on the fact that the difference between the growth of fast-learnable and slow-learnable features in SAM is smaller than that of GD. Thus, starting from the same initialization, the slow-learnable feature contributes relatively more to the model prediction in SAM than it does in SGD. Thus, the slow-learnable feature benefits SAM, by reducing its overreliance on the fast-learnable features. We note that as neural networks are nonlinear, a small change in the output can actually result in a big change in the model and its performance. Even in the extreme setting when two features have identical strength and the fast-learnable feature exists in all examples, i.e., \(\beta_{e}=\beta_{d}=\alpha=1\), the gap in Eq. 5 is significant as we confirm in Figure 8 in Appendix D.

**Remark.** The network often overfits slow-learnable features that are learned late during the training and do not learn them in a generalizable manner. This harms the generalization performance on the test set sampled from the _original_ data distribution.

Theorems 3.2 and 3.3 show that we can make the model learn more from the slow-learnable feature by increasing the value of \(\beta_{d}\). Based on this intuition, we have the following theorem.

**Theorem 3.5** (**One-shot upsampling**).: _Under the assumptions of Theorems 3.2, 3.3, for a sufficiently small noise, from any iteration \(t\) during early training, we have the following results:_

1. _The slow-learnable feature has a larger contribution to the normalized gradient of the 1-step SAM update, compared to that of GD._
2. _Amplifying the strength of the slow-learnable feature increases its contribution to the normalized gradients of GD and SAM._
3. _There exists an upsampling factor_ \(k\) _s.t. the normalized gradient of the 1-step GD update on_ \(\mathcal{D}(\beta_{e},k\beta_{d},\alpha)\) _recovers the normalized gradient of the 1-step SAM update on_ \(\mathcal{D}(\beta_{e},\beta_{d},\alpha)\)

**Discussion.** Proof of Theorem 3.5 is given in Appendix A.4. We see that we can learn features at a more uniform speed by training on a new dataset \(\mathcal{D}(\beta_{e},\beta_{d}^{\prime},\alpha)\) with a larger strength \(\beta_{d}^{\prime}>\beta_{d}\). But, the value of coefficient \(\beta_{d}^{\prime}\) varies by the model weights and gradient at each iteration \(t\).

**Remark.** Intuitively, Theorem 3.5 implies that by descending over a flatter trajectory, SAM learns slow-learnable features relatively earlier in training, compared to GD. While the largest difference between feature learning of SAM and (S)GD is attributed to early training dynamics (due to the simplicity bias of (S)GD and its largest contribution early in training), SAM learns features at a more uniform speed during the _entire_ training. This effect is, however, difficult to theoretically characterize exactly. Therefore, learning features at a more uniform speed via SAM help yield flatter minima with better generalization performance. We note that while SAM learns fast-learnable and slow-learnable features at a more uniform speed, it still suffers from simplicity bias and learns fast-learnable features earlier (although less so than GD) as evidenced in our Theorem 3.3.

## 4 Method: UpSample Early For Uniform Learning (USEFUL)

Motivated by our theoretical results, we aim to speed up learning the slow-learnable features in the training data. This drive the network to learn fast-learnable and slow-learnable features at a more uniformly speed, and ultimately improves the in-distribution generalization performance.

**Step 1: Identifying examples with fast-learnable features.** As shown in Theorems 3.2 and 3.3, fast-learnable features are learned early in training, and the _model output for examples containing fast-learnable features are highly separable_ from the rest of examples in their class, early in training. This is illustrated for one class of a toy example and CIFAR-10 in Fig. 2. Motivated by our theory, we seek to find a cluster of examples with similar model outputs early in training. To do so, we apply \(k\)-means clustering to the last-layer activation vectors of examples in every class, to separate examples with fast-learnable features from the rest of examples. Formally, for examples in every class with \(y_{j}=c\), we find:

\[\operatorname*{arg\,min}_{C}\sum_{i\in\{1,2\}}\sum_{y_{j}=c,j\in C_{i}}\|f( \boldsymbol{x}_{j};\boldsymbol{W}^{(t)})-\boldsymbol{\mu}_{i}\|^{2},\] (6)

Figure 1: Examples of slow-learnable (top) and fast-learnable (bottom) in CIFAR-10 found by our method. Examples in the top row (slow-learnable) are harder to identify visually and look more ambiguous (part of the object is in the image or the object is smaller and the area associated with the background is larger). In contrast, examples in the bottom row (fast-learnable) are not ambiguous and are clear representatives of their corresponding class, hence are very easy to visually classify (the entire object is in the image and the area associated with the background is small).

where \(\bm{\mu}_{i}\) is the center of cluster \(S_{i}\). The cluster with lower average loss will contain the majority of examples containing fast-learnable features, whereas the remaining examples contain slow-learnable features in the training data. Examples of images in fast-learnable and slow-learnable clusters of CIFAR-10 found by USEFUL are illustrated in Fig. 1.

**The choice of clustering.** Our choice of clustering is motivated by our Theorems 3.2 and 3.3 which show that examples with fast-learnable features are separable based on model output from the rest of examples in their class. While examples with fast-learnable features are expected to have a lower loss, loss of examples may oscillate during the training and makes it difficult to find an accurate cut-off for separating the examples. Besides, as fast-learnable features may not be _fully_ learned early in training, examples containing fast-learnable features may not necessarily have the right prediction, thus misclassification cannot separate examples accurately. In contrast, clustering does not require hyperparameter tuning and performs well for separating examples, as we confirm in our ablation studies.

**Step 2: One-shot upsampling of slow-learnable features.** Next, we upsample examples that are not in the cluster of points containing fast-learnable features. This speeds up learning slow-learnable features and encourages the model to learn different features at a more uniform speed. Thus, it improves the in-distribution performance based on Theorem 3.5. As discussed earlier, the number of times we upsample these examples should change based on the model weight at each iteration. Hence, a multi-stage clustering and sampling can yield the best results. Nevertheless, we empirically confirm that a 1-shot algorithm that finds fast-learnable examples at an _early_ training iteration and upsample the remaining examples by a factor of \(k=2\) effectively improves the performance. Notably, in contrast to dynamic sampling or reweighting, USEFUL upsamples examples only once and restart training on the modified but fix distribution.

**When to separate the examples.** It is crucial to separate examples _early_ in training, to accurately identify examples that contribute the most to simplicity bias. We empirically verify the intuition that the optimal epoch \(t\) to separate examples is when the change in training error starts to shrink as visualized in Figure 14(a). More details can be found in Appendices C.2 and D.8.

The pseudocode of USEFUL is illustrated in Alg. 1 and the workflow is shown in Appendix Fig. 7.

Figure 3: **GD (blue) vs. SAM (orange) on toy datasets.** Data is generated based on Definition 3.1 with different \(\beta_{d}\) and fixed \(\beta_{e}=1,\)\(\alpha=0.9\). \(\cdot\) and \(--\) lines denote the alignment (i.e., inner product) of fast-learnable (\(\bm{v}_{e}\)) and slow-learnable (\(\bm{v}_{d}\)) features with the model weight (\(\bm{w}_{j}^{(t)}\)). (a), (b) GD and SAM first learn the fast-learnable feature. Notably, GD learns the fast-learnable feature very early. (c) Test accuracy of GD & SAM improves by increasing the strength of the slow-learnable feature.

Figure 2: TSNE visualization of output vectors. (left) ResNet18/CIFAR-10 at epoch 8. (right) CNN/toy data generated based on Definition 3.1 with \(\beta_{d}=0.2,\beta_{e}=1,\alpha=0.9\), iteration 200.

## 5 Experiments

**Outline.** In Sec. 5.1, we empirically validate our theoretical results on toy datasets. We then evaluate the performance of USEFUL on several real-world datasets in Sec. 5.2 and different model architectures in Sec. 5.3. In addition, Sec. 5.3 highlights the advantages of USEFUL over random upsampling. Furthermore, we show that USEFUL shares several properties with SAM in Sec. 5.4. Additional experimental results are deferred to Appendix D where we show that USEFUL also boosts the performance of other SAM variants, and present promising results for USEFUL applied to the OOD setting (spurious correlation, long-tail distribution), transfer learning, and label noise settings. We further conduct ablation studies on the effect of our data selection strategy for upsampling, training batch size, learning rate, upsampling factor, and separating epoch in Appendix D.8.

**Settings.** We used common datasets for image classification including CIFAR10, CIFAR100 [41], STL10 [13], CINIC10 [16], and Tiny-ImageNet [43]. Both CINIC10 and Tiny ImageNet are large-scale datasets containing images from the ImageNet dataset [17]. We trained ResNet18 on all datasets except for CIFAR100 on which we trained ResNet34. We closely followed the setting from [3] in which our models are trained for 200 epochs with a batch size of 128. We used SGD with the momentum parameter of 0.9 and set weight decay to 0.0005. We also fixed \(\rho=0.1\) for SAM in all experiments unless explicitly stated. We used a linear learning rate schedule starting at 0.1 and decay by a factor of 10 once at epoch 100 and again at epoch 150. More details are given in Appendix C.

### Toy Datasets

**Datasets.** Following [18], our toy dataset consists of training and test sets, each containing 10K examples generated from the data distribution defined in 3.1 with dimension \(d\!=\!50\) and \(P\!=\!3\). We set \(\beta_{e}\!=\!1,\beta_{d}\!=\!0.2,\alpha\!=\!0.9\), and \(\sigma_{p}/\sqrt{d}\!=\!0.125\). We also consider a scenario with larger \(\beta_{d}\!=\!0.4\). We shuffle the order of patches randomly to confirm that our theory holds with arbitrary order of patches.

**Training.** We used the two-layer nonlinear CNN in Section 3.1 with \(J=40\) filters. For GD, we set the learning rate to \(\eta=0.1\) and did not use momentum. For SAM, we used the same base GD optimizer and chose a smaller value of the inner step, \(\rho=0.02\), than other experiments to satisfy the constraint in Theorem 3.4. We trained the model for 600 iterations till convergence for GD and SAM.

**Results.** Figure 2(a) illustrates that both GD (blue) and SAM (orange) first learn the fast-learnable feature. In particular, the blue dotted line (\(G_{e}\)) accelerates quickly at around epoch 250 while the orange dotted line (\(S_{e}\)) increases drastically much later at around epoch 450. That is: _(1) GD learns the fast

Figure 4: **Test classification error of ResNet18 on CIFAR10, STL10, TinyImageNet and ResNet34 on CIFAR100. The numbers below bars indicate the approximate training cost and the tick on top shows the std over three runs. USEFUL enhances the performance of SGD and SAM on all 5 datasets. TrivialAugment (TA) further boosts SAM’s performance (except for CINIC10). Remarkably, USEFUL consistently boosts the performance across all scenarios and achieves (to our knowledge) SOTA performance for ResNet18 and ResNet34 on the selected datasets when combined with SAM and TA.**_learnable feature very early in training._ This is well-aligned with our Theorems 3.2 and 3.3 and their discussion. Furthermore, the gap between contribution of fast-learnable and slow-learnable features towards the model output in SAM \((S_{e}^{(t)}-S_{d}^{(t)})\) is much smaller than that of GD \((G_{e}^{(t)}-G_{d}^{(t)})\). That is: _(2) fast-learnable and slow-learnable features are learned more evenly in SAM._ This validates our Theorem 3.4. From around epoch 500 onwards, the contribution of the slow-learnable feature in SAM surpasses the level of that in GD while the contribution of the fast-learnable feature in SAM is still lower than the counterpart in GD. When increasing the slow-learnable feature strength \(\beta_{d}\) from 0.2 to 0.4 in Figure 2(b), the same conclusion for the growth speed of fast-learnable and slow-learnable features holds. Notably, there is a clear increase in the classification accuracy of the model trained with either GD or SAM by increasing \(\beta_{d}\), as can be seen in Figure 2(c). That is: _(3) amplifying the strength of the slow-learnable feature improves the generalization performance._ Effectively, this enables the model successfully predict examples in which the fast-learnable feature is missing.

### USEFUL is Effective across Datasets

Figure 4 illustrates the performance of models trained with SGD and SAM on original vs modified data distribution by USEFUL. We see that USEFUL effectively reduces the test classification error of both SGD and SAM. Interestingly, USEFUL further improves SAM's generalization performance by reducing its simplicity bias. Notably, on the STL10 dataset, USEFUL boosts the performance of SGD to surpass that of SAM. The percentages of examples found for upsampling by USEFUL for CIFAR10, CIFAR100, STL10, CINIC10, and Tiny-ImageNet are roughly 30%, 50%, 50%, 45%, and 60%, respectively. Thus, training SGD on the modified data distribution only incurs a cost of 1.3x, 1.5x, 1.5x, 1.45x, and 1.6x compared to 2x of SAM.

**USEFUL+TA is particularly effective.** Stacking strong augmentation methods e.g. TrivialAugment [50] further improves the performance, achieving state-of-the-art for ResNet on all datasets. When strong augmentation is combined with USEFUL, it makes more variations of the (upsampled) slow-learnable features and enhances their learning. Hence, it further boost the performance.

### USEFUL is Effective across Architectures & Settings

**Model architectures: CNN, ViT, MLP.** Next, we confirm the versatility of our method, by applying it to different model architectures including 3-layer MLP, CNNs (ResNet18, VGG19, DenseNet121), and Transformers (ViT-S). Figure 5 shows that USEFUL is effective across different model architectures. Remarkably, when applying to non-CNN architectures, it reduces the test error of SGD to a lower level than that of SAM alone. Detailed results for 3-layer MLP is given in Appendix D.2.

**Settings: batch-size, learning rate, and SAM variants.** In Appendix D, we confirm the effectiveness of USEFUL for different batch sizes of 128, 256, 512, and different initial learning rates of 0.1, 0.2, 0.4. In Appendix D.4, we confirm that USEFUL applied to ASAM [42]--a SAM variant which uses a scale-invariant sharpness measure--further reduces the test error.

**USEFUL vs Random Upsampling.** Fig. 6 shows that USEFUL considerably outperforms SGD and SAM on randomly upsampled CIFAR10 & CIFAR100. This confirms that the main benefit of USEFUL is due to the modified distribution and not longer training time. In Appendix D.8, we also confirm that upsampling outperforms upweighting for SAM & SGD.

Figure 5: **Test classification errors of different architectures on CIFAR10. USEFUL improves the performance of SGD and SAM when training different architectures. TrivialAugment (TA) further boosts SAM’s capabilities. The results for 3-layer MLP can be found in Figure 9.**

### USEFUL's Solution has Similar Properties to SAM

**SAM & USEFUL Find Sparser Solutions than SGD.**[3] showed that SAM's solution has a better sparsity-inducing property indicated by the L1 norm than the standard ERM. Fig. 10 shows the L1 norm of ResNet18 trained on CIFAR10 and ResNet34 trained on CIFAR100 at the end of training. We see that USEFUL drives both SGD and SAM to find solutions with smaller L1 norms.

**SAM & USEFUL Find Less Sharp Solutions than SGD.** While our goal is not to directly find a flatter minimum or the same solution as SAM, we showed that USEFUL finds flatter minima. Following [3], we used the maximum Hessian eigenvalue (\(\lambda_{max}\)) and the bulk of the spectrum (\(\lambda_{max}/\lambda_{5}\)) [30], which are commonly used metrics for sharpness [10; 31; 37; 74]. Table 1 illustrates that SGD+USEFUL on CIFAR10 reduces sharpness metrics significantly compared to SGD, proving that USEFUL successfully reduces the sharpness of the solution. We note that to capture the sharpness/flatness, multiple different criteria have been proposed (largest Hessian eigenvalue and bulk of Hessian), and one criterion is not enough to accurately capture the sharpness. While the solution of SGD+USEFUL has a higher largest Hessian eigenvalue than SAM, it achieves the smallest bulk.

**SAM & USEFUL Reduce Forgetting Scores.** Forgetting scores [69] count the number of times an example is misclassified after being correctly classified during training and is an indicator of the learning speed and difficulty of examples. We show in Appendix D.3 that both SAM and USEFUL successfully reduce the forgetting scores, thus learn slow-learnable features faster than SGD. This aligns with our Theorem 3.4 and results on the toy datasets. By upsampling slow-learnable examples in the dataset, they contribute more to learning and hence SGD+USEFUL learns them faster than SGD.

**USEFUL also Benefits Distribution Shift.** While our main contribution is providing a novel and effective method to improve the in-distribution generalization performance, we conduct experiments confirming the benefits of our method to distribution shift. We discuss this experiment and its results in Appendix D.5. On Waterbirds dataset [61] with strong spurious correlation (95%), both SAM and USEFUL successfully improve the performance on the balanced test set by 6.21% and 5.8%, respectively. We also show the applicability of USEFUL to fine-tuning a ResNet50 pre-trained on ImageNet.

## 6 Conclusion

In this paper, we made the first attempt to improve the in-distribution generalization performance of machine learning methods by modifying the distribution of training data. We first analyzed learning dynamics of sharpness-aware minimization (SAM), and attributed its superior performance over GD to mitigating the simplicity bias, and learning features at a more speed. Inspired by SAM, we upsampled the examples that contain slow-learnable features to alleviate the simplicity bias. This allows learning features more uniformly, thus improving the performance. Our method boosts the performance of image classifiers trained with SGD or SAM and easily stacks with data augmentation.

## Acknowledgments

This research was partially supported by the National Science Foundation CAREER Award 2146492, National Science Foundation 2421782 and Simons Foundation, Cisco Systems, Optum AI, and a UCLA Hellman Fellowship.

Figure 6: USEFUL vs. Random Upsampling, when training ResNet18 on CIFAR10 and CIFAR100.

## References

* [1] Amro Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. _arXiv preprint arXiv:2303.09540_, 2023.
* [2] Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. _arXiv preprint arXiv:2012.09816_, 2020.
* [3] Maksym Andriushchenko and Nicolas Flammarion. Towards understanding sharpness-aware minimization. In _International Conference on Machine Learning_, pages 639-668. PMLR, 2022.
* [4] Maksym Andriushchenko, Dara Bahri, Hossein Mobahi, and Nicolas Flammarion. Sharpness-aware minimization leads to low-rank features. _arXiv preprint arXiv:2305.16292_, 2023.
* [5] Robert Baldock, Hartmut Maennel, and Behnam Neyshabur. Deep learning through the lens of example difficulty. _Advances in Neural Information Processing Systems_, 34:10876-10889, 2021.
* [6] Peter L Bartlett, Philip M Long, and Olivier Bousquet. The dynamics of sharpness-aware minimization: Bouncing across ravines and drifting towards wide minima. _Journal of Machine Learning Research_, 24(316):1-36, 2023.
* [7] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice and the classical bias-variance trade-off. _Proceedings of the National Academy of Sciences_, 116(32):15849-15854, 2019.
* [8] Yuan Cao, Zixiang Chen, Misha Belkin, and Quanquan Gu. Benign overfitting in two-layer convolutional neural networks. _Advances in neural information processing systems_, 35:25237-25250, 2022.
* [9] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and Sungrae Park. Swad: Domain generalization by seeking flat minima. _Advances in Neural Information Processing Systems_, 34:22405-22418, 2021.
* [10] Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent into wide valleys. _Journal of Statistical Mechanics: Theory and Experiment_, 2019(12):124018, 2019.
* [11] Zixiang Chen, Yihe Deng, Yue Wu, Quanquan Gu, and Yuanzhi Li. Towards understanding the mixture-of-experts layer in deep learning. _Advances in neural information processing systems_, 35:23049-23062, 2022.
* [12] Zixiang Chen, Junkai Zhang, Yiwen Kou, Xiangning Chen, Cho-Jui Hsieh, and Quanquan Gu. Why does sharpness-aware minimization generalize better than sgd? _arXiv preprint arXiv:2310.07269_, 2023.
* [13] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In _Proceedings of the fourteenth international conference on artificial intelligence and statistics_, pages 215-223. JMLR Workshop and Conference Proceedings, 2011.
* [14] Elliot Creager, Jorn-Henrik Jacobsen, and Richard Zemel. Environment inference for invariant learning. In _International Conference on Machine Learning_, pages 2189-2200. PMLR, 2021.
* [15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on effective number of samples. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 9268-9277, 2019.
* [16] Luke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not imagenet or cifar-10. _arXiv preprint arXiv:1810.03505_, 2018.
* [17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* [18] Yihe Deng, Yu Yang, Baharan Mirzasoleiman, and Quanquan Gu. Robust learning with progressive data expansion against spurious correlation. _arXiv preprint arXiv:2306.04949_, 2023.

* [19] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In _International Conference on Machine Learning_, pages 1019-1028. PMLR, 2017.
* [20] Jiawei Du, Hanshu Yan, Jiashi Feng, Joey Tianyi Zhou, Liangli Zhen, Rick Siow Mong Goh, and Vincent YF Tan. Efficient sharpness-aware minimization for improved training of neural networks. _arXiv preprint arXiv:2110.03141_, 2021.
* [21] Ayoub El Hanchi, David Stephens, and Chris Maddison. Stochastic reweighted gradient descent. In _International Conference on Machine Learning_, pages 8359-8374. PMLR, 2022.
* [22] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. _arXiv preprint arXiv:2010.01412_, 2020.
* [23] Samir Yitzhak Gadre, Gabriel Illarco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. _arXiv preprint arXiv:2304.14108_, 2023.
* [24] Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net optimization via hessian eigenvalue density. In _International Conference on Machine Learning_, pages 2232-2241. PMLR, 2019.
* [25] Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. _Advances in neural information processing systems_, 30, 2017.
* [26] Katherine Hermann and Andrew Lampinen. What shapes feature representations? exploring datasets, architectures, and training. _Advances in Neural Information Processing Systems_, 33:9995-10006, 2020.
* [27] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_, 2022.
* [28] Wei Hu, Lechao Xiao, Ben Adlam, and Jeffrey Pennington. The surprising simplicity of the early-time learning dynamics of neural networks. _Advances in Neural Information Processing Systems_, 33:17116-17128, 2020.
* [29] Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al. _An introduction to statistical learning_, volume 112. Springer, 2013.
* [30] Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek Tabor, Kyunghyun Cho, and Krzysztof Geras. The break-even point on optimization trajectories of deep neural networks. _arXiv preprint arXiv:2002.09572_, 2020.
* [31] Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey. Three factors influencing minima in sgd. _arXiv preprint arXiv:1711.04623_, 2017.
* [32] Samy Jelassi and Yuanzhi Li. Towards understanding how momentum improves generalization in deep learning. In _International Conference on Machine Learning_, pages 9965-10040. PMLR, 2022.
* [33] Tyler B Johnson and Carlos Guestrin. Training deep models faster with robust, approximate importance sampling. _Advances in Neural Information Processing Systems_, 31, 2018.
* [34] Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz Barak, and Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity. _Advances in neural information processing systems_, 32, 2019.
* [35] Angelos Katharopoulos and Francois Fleuret. Not all samples are created equal: Deep learning with importance sampling. In _International conference on machine learning_, pages 2525-2534. PMLR, 2018.
* [36] Simran Kaur, Jeremy Cohen, and Zachary Chase Lipton. On the maximum hessian eigenvalue and generalization. In _Proceedings on_, pages 51-65. PMLR, 2023.
* [37] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. _arXiv preprint arXiv:1609.04836_, 2016.

* [38] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [39] Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training is sufficient for robustness to spurious correlations. _arXiv preprint arXiv:2204.02937_, 2022.
* [40] Yiwen Kou, Zixiang Chen, Yuanzhou Chen, and Quanquan Gu. Benign overfitting in two-layer relu convolutional neural networks. In _International Conference on Machine Learning_, pages 17615-17659. PMLR, 2023.
* [41] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [42] Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks. In _International Conference on Machine Learning_, pages 5905-5914. PMLR, 2021.
* [43] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. _CS 231N_, 7(7):3, 2015.
* [44] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. _arXiv preprint arXiv:2107.06499_, 2021.
* [45] Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix: Learning with noisy labels as semi-supervised learning. _arXiv preprint arXiv:2002.07394_, 2020.
* [46] Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large learning rate in training neural networks. _Advances in neural information processing systems_, 32, 2019.
* [47] Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In _Proceedings of the European conference on computer vision (ECCV)_, pages 19-34, 2018.
* [48] Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training group information. In _International Conference on Machine Learning_, pages 6781-6792. PMLR, 2021.
* [49] Song Mei and Andrea Montanari. The generalization error of random features regression: Precise asymptotics and the double descent curve. _Communications on Pure and Applied Mathematics_, 75(4):667-766, 2022.
* [50] Samuel G Muller and Frank Hutter. Trivialaugment: Tuning-free yet state-of-the-art data augmentation. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 774-782, 2021.
* [51] Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double descent: Where bigger models and more data hurt. _Journal of Statistical Mechanics: Theory and Experiment_, 2021(12):124003, 2021.
* [52] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. _arXiv preprint arXiv:1412.6614_, 2014.
* [53] Vardan Papyan. The full spectrum of deepnet hessians at scale: Dynamics with sgd training and sample size. _arXiv preprint arXiv:1811.07062_, 2018.
* [54] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.
* [55] Mohammad Pezeshki, Oumar Kaba, Yoshua Bengio, Aaron C Courville, Doina Precup, and Guillaume Lajoie. Gradient starvation: A learning proclivity in neural networks. _Advances in Neural Information Processing Systems_, 34:1256-1272, 2021.
* [56] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient neural architecture search via parameters sharing. In _International conference on machine learning_, pages 4095-4104. PMLR, 2018.

* [57] Aahlad Manas Puli, Lily Zhang, Yoav Wald, and Rajesh Ranganath. Don't blame dataset shift! shortcut learning due to gradients and cross entropy. _Advances in Neural Information Processing Systems_, 36:71874-71910, 2023.
* [58] Aahlad Manas Puli, Lily Zhang, Yoav Wald, and Rajesh Ranganath. Don't blame dataset shift! shortcut learning due to gradients and cross entropy. _Advances in Neural Information Processing Systems_, 36, 2024.
* [59] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of machine learning research_, 21(140):1-67, 2020.
* [60] Nicolas Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an exponential convergence _rate for finite training sets. _Advances in neural information processing systems_, 25, 2012.
* [61] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. _arXiv preprint arXiv:1911.08731_, 2019.
* [62] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic average gradient. _Mathematical Programming_, 162:83-112, 2017.
* [63] Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The pitfalls of simplicity bias in neural networks. _Advances in Neural Information Processing Systems_, 33:9573-9585, 2020.
* [64] Vatsal Shah, Anastasios Kyrillidis, and Sujay Sanghavi. Minimum norm solutions do not always generalize well for over-parameterized problems. _stat_, 1050:16, 2018.
* [65] Jacob Mitchell Springer, Vaishnavh Nagarajan, and Aditi Raghunathan. Sharpness-aware minimization enhances feature quality via balanced learning. In _The Twelfth International Conference on Learning Representations_, 2024.
* [66] Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. Certified defenses for data poisoning attacks. _Advances in neural information processing systems_, 30, 2017.
* [67] Damien Teney, Ehsan Abbasnejad, Simon Lucey, and Anton Van den Hengel. Evading the simplicity bias: Training a diverse set of models discovers solutions with superior ood generalization. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 16761-16772, 2022.
* [68] Rishabh Tiwari and Pradeep Shenoy. Overcoming simplicity bias in deep networks using a feature sieve. In _International Conference on Machine Learning_, pages 34330-34343. PMLR, 2023.
* [69] Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network learning. _arXiv preprint arXiv:1812.05159_, 2018.
* [70] Guillermo Valle-Perez, Chico Q Camargo, and Ard A Louis. Deep learning generalizes because the parameter-function map is biased towards simple functions. In _International Conference on Learning Representations_, 2018.
* [71] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 8769-8778, 2018.
* [72] Pengfei Wang, Zhaoxiang Zhang, Zhen Lei, and Lei Zhang. Sharpness-aware gradient matching for domain generalization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3769-3778, 2023.
* [73] Kaiyue Wen, Tengyu Ma, and Zhiyuan Li. How does sharpness-aware minimization minimize sharpness? _arXiv preprint arXiv:2211.05729_, 2022.
* [74] Yeming Wen, Kevin Luk, Maxime Gazeau, Guodong Zhang, Harris Chan, and Jimmy Ba. An empirical study of large-batch stochastic gradient descent with structured covariance noise. _arXiv preprint arXiv:1902.08234_, 2019.

* [75] Yu Yang, Eric Gan, Gintare Karolina Dziugaite, and Baharan Mirzasoleiman. Identifying spurious biases early in training through the lens of simplicity bias. _arXiv preprint arXiv:2305.18761_, 2023.
* [76] Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, and Michael Mahoney. Adahesian: An adaptive second order optimizer for machine learning. In _proceedings of the AAAI conference on artificial intelligence_, volume 35, pages 10665-10673, 2021.
* [77] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 558-567, 2021.
* [78] Runtian Zhai, Chen Dan, Zico Kolter, and Pradeep Ravikumar. Understanding why generalized reweighting does not improve over erm. _arXiv preprint arXiv:2201.12293_, 2022.
* [79] Hongyi Zhang. mixup: Beyond empirical risk minimization. _arXiv preprint arXiv:1710.09412_, 2017.
* [80] Peilin Zhao and Tong Zhang. Stochastic optimization with importance sampling for regularized loss minimization. In _international conference on machine learning_, pages 1-9. PMLR, 2015.
* [81] Yaowei Zheng, Richong Zhang, and Yongyi Mao. Regularizing neural networks via adversarial model perturbation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8156-8165, 2021.
* [82] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. _arXiv preprint arXiv:1611.01578_, 2016.

## Appendix A Formal Proofs

### Proof of Theorem 3.2

**Notation.** In this paper, we use lowercase letters, lowercase boldface letters, and uppercase boldface letters to respectively denote scalars \((a)\), vectors \((\bm{v})\), and matrices \((\bm{W})\). For a vector \(\bm{v}\), we use \(\left\|\bm{v}\right\|_{2}\) to denote its Euclidean norm. Given two sequence \(\{x_{n}\}\) and \(\{y_{n}\}\), we denote \(x_{n}=O(y_{n})\) if \(|x_{n}|\leq C_{1}|y_{n}|\) for some absolute positive constant \(C_{1}\), \(x_{n}=\Omega(y_{n})\) if \(|x_{n}|\geq C_{2}|y_{n}|\) for some absolute positive constant \(C_{2}\), and \(x_{n}=\Theta(y_{n})\) if \(C_{3}|y_{n}|\leq|x_{n}|\leq C_{4}|y_{n}|\) for some absolute constant \(C_{3},C_{4}>0\). In addition, we use \(\tilde{O}(\cdot),\tilde{\Omega}(\cdot),\) and \(\tilde{\Theta}(\cdot)\) to hide logarithmic factors in these notations. Furthermore, we denote \(x_{n}=\text{poly}(y_{n})\) if \(x_{n}=O(y_{n}^{D})\) for some positive constant D, and \(x_{n}=\text{polylog}(y_{n})\) if \(x_{n}=\text{poly}(\log(y_{n}))\).

First, we have the following assumption for the model weight initialization.

**Assumption A.1** (Weight initialization).: Assume that we initialize \(\bm{W}^{(0)}\sim\mathcal{N}(0,\sigma_{0}^{2})\) such that for all \(j\in[J]\), \(\langle\bm{w}_{j}^{(0)},\bm{v}_{e}\rangle,\langle\bm{w}_{j}^{(0)},\bm{v}_{d} \rangle\geq\rho>0\).

The above assumption is reasonable because we later show that both sequences \(\langle\bm{w}_{j}^{(t)},\bm{v}_{e}\rangle\) and \(\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle\) are non-decreasing. So, we can obtain the above initialization by training the model for several iterations. For simplicity of the notation, we assume that \(\alpha N\) is an integer and the first \(\alpha N\) data examples have the fast-learnable feature while the rest do not. Before going into the analysis, we denote the derivative of a data example \(i\) at iteration \(t\) to be

\[l_{i}^{(t)}=\frac{\exp(-y_{i}f(\bm{x}_{i};\bm{W}^{(t)}))}{1+\exp(-y_{i}f(\bm{ x}_{i};\bm{W}^{(t)}))}=\text{sigmoid}(-y_{i}f(\bm{x}_{i};\bm{W}^{(t)})).\] (7)

**Lemma A.2** (Gradient).: _Let the loss function \(\mathcal{L}\) be as defined in Equation 2. For \(t\geq 0\) and \(j\in[J]\), the gradient of the loss \(\mathcal{L}(\bm{W}^{(t)})\) with regard to neuron \(\bm{w}_{j}^{(t)}\) is_

\[\nabla_{\bm{w}_{j}^{(t)}}\mathcal{L}(\bm{W}^{(t)}) =-\frac{3}{N}\sum_{i=1}^{\alpha N}l_{i}^{(t)}\left(\beta_{d}^{3} \langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle^{2}\bm{v}_{d}+\beta_{e}^{3}\langle \bm{w}_{j}^{(t)},\bm{v}_{e}\rangle^{2}\bm{v}_{e}+y_{i}\langle\bm{w}_{j}^{(t)},\bm{\xi}_{i}\rangle^{2}\bm{\xi}_{i}\right)-\] \[\quad\frac{3}{N}\sum_{i=\alpha N+1}^{N}l_{i}^{(t)}\left(\beta_{d} ^{3}\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle^{2}\bm{v}_{d}+y_{i}\langle\bm{ w}_{j}^{(t)},\bm{\xi}_{i}\rangle^{2}\bm{\xi}_{i}\right).\] (8)

Proof.: We have the following gradient

\[\nabla_{\bm{w}_{j}^{(t)}}\mathcal{L}(\bm{W}^{(t)}) =-\frac{1}{N}\sum_{i=1}^{N}\frac{\exp(-y_{i}f(\bm{x}_{i};\bm{W}^{( t)}))}{1+\exp(-y_{i}f(\bm{x}_{i};\bm{W}^{(t)}))}\cdot y_{i}f^{\prime}(\bm{x}_{i}; \bm{W}^{(t)})\] \[=-\frac{3}{N}\sum_{i=1}^{N}l_{i}^{(t)}y_{i}\sum_{p=1}^{P}\langle \bm{w}_{j}^{(t)},\bm{x}^{(p)}\rangle^{2}\cdot\bm{x}^{(p)}\] \[=-\frac{3}{N}\sum_{i=1}^{N}l_{i}^{(t)}\left(\beta_{d}^{3}\langle \bm{w}_{j}^{(t)},\bm{v}_{d}\rangle^{2}\bm{v}_{d}+\beta_{e}^{3}\langle\bm{w}_{ j}^{(t)},\bm{v}_{e}\rangle^{2}\bm{v}_{e}+y_{i}\langle\bm{w}_{j}^{(t)},\bm{\xi}_{i} \rangle^{2}\bm{\xi}_{i}\right)-\] \[\quad\frac{3}{N}\sum_{i=\alpha N+1}^{N}l_{i}^{(t)}\left(\beta_{d} ^{3}\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle^{2}\bm{v}_{d}+y_{i}\langle\bm{ w}_{j}^{(t)},\bm{\xi}_{i}\rangle^{2}\bm{\xi}_{i}\right)\]

With the above formula of gradient, we have the following equations:

**Fast-learnable feature gradient.** The projection of the gradient on \(\bm{v}_{e}\) is

\[\langle\nabla_{\bm{w}_{j}^{(t)}}\mathcal{L}(\bm{W}^{(t)}),\bm{v}_{e}\rangle=- \frac{3\beta_{e}^{3}}{N}\sum_{i=1}^{\alpha N}l_{i}^{(t)}\langle\bm{w}_{j}^{(t)}, \bm{v}_{e}\rangle^{2}\] (9)

**Slow-learnable feature gradient.** The projection of the gradient on \(\bm{v}_{d}\) is

\[\langle\nabla_{\bm{w}_{j}^{(t)}}\mathcal{L}(\bm{W}^{(t)}),\bm{v}_{d}\rangle=- \frac{3\beta_{d}^{3}}{N}\sum_{i=1}^{N}l_{i}^{(t)}\langle\bm{w}_{j}^{(t)},\bm{v }_{d}\rangle^{2}\] (10)

**Noise gradient.** The projection of the gradient on \(\bm{\xi}_{i}\) is

\[\langle\nabla_{\bm{w}_{j}^{(t)}}\mathcal{L}(\bm{W}^{(t)}),\bm{\xi}_{i}\rangle= -\frac{3}{N}\left(l_{i}^{(t)}y_{i}\langle\bm{w}_{j}^{(t)},\bm{\xi}_{i}\rangle^{ 2}\left\|\bm{\xi}_{i}\right\|_{2}^{2}+\sum_{k=1,k\neq i}^{N}l_{k}^{(t)}y_{k} \langle\bm{w}_{j}^{(t)},\bm{\xi}_{k}\rangle^{2}\langle\bm{\xi}_{k},\bm{\xi}_{ i}\rangle\right)\] (11)

**Derivative of data example \(i\).** For \(1\leq i\leq\alpha N\), \(l_{i}^{(t)}\) can be rewritten as

\[l_{i}^{(t)}=\text{sigmoid}\left(\sum_{j=1}^{J}-\beta_{d}^{3}\langle\bm{w}_{j} ^{(t)},\bm{v}_{d}\rangle^{3}-\beta_{e}^{3}\langle\bm{w}_{j}^{(t)},\bm{v}_{e} \rangle^{3}-y_{i}\langle\bm{w}_{j}^{(t)},\bm{\xi}_{i}\rangle^{3}\right)\] (12)

while for \(\alpha N+1\leq i\leq N\), \(l_{i}^{(t)}\) can be rewritten as

\[l_{i}^{(t)} =\text{sigmoid}\left(\sum_{j=1}^{J}-\beta_{d}^{3}\langle\bm{w}_{ j}^{(t)},\bm{v}_{d}\rangle^{3}-y_{i}\langle\bm{w}_{j}^{(t)},\bm{\xi}_{i} \rangle^{3}\right)\] \[\geq\text{sigmoid}\left(\sum_{j=1}^{J}-\beta_{d}^{3}\langle\bm{ w}_{j}^{(t)},\bm{v}_{d}\rangle^{3}-\beta_{e}^{3}\langle\bm{w}_{j}^{(t)},\bm{v}_{e} \rangle^{3}-y_{i}\langle\bm{w}_{j}^{(t)},\bm{\xi}_{i}\rangle^{3}\right)\] (13)

Note that \(0<l_{i}^{(t)}<1\) due to the property of the sigmoid function. Furthermore, we similarly consider that the sum of the sigmoid terms for all time steps is bounded up to a logarithmic dependence [11]. The sigmoid term is considered small for a \(\kappa\) such that

\[\sum_{t=0}^{T}\frac{1}{1+\exp(\kappa)}\leq\tilde{O}(1),\] (14)

which implies \(\kappa\geq\tilde{\Omega}(1)\).

We present the detailed proofs that build up to Theorem A.8. We begin by considering the update for the fast-learnable and slow-learnable features.

**Lemma A.3** (Fast-learnable feature update.).: _For all \(t\geq 0\) and \(j\in[J]\), the fast-learnable feature update is_

\[\langle\bm{w}_{j}^{(t+1)},\bm{v}_{e}\rangle=\langle\bm{w}_{j}^{(t)},\bm{v}_{e }\rangle+\tilde{\Theta}(\eta)\alpha\beta_{e}^{3}g_{1}(t)\langle\bm{w}_{j}^{(t) },\bm{v}_{e}\rangle^{2},\] (15)

_where \(g_{1}(t)=\text{sigmoid}\left(\sum_{j=1}^{J}-\beta_{d}^{3}\langle\bm{w}_{j}^{( t)},\bm{v}_{d}\rangle^{3}-\beta_{e}^{3}\langle\bm{w}_{j}^{(t)},\bm{v}_{e} \rangle^{3}\right)\)._

Proof.: Plugging the update rule of GD, we have

\[\langle\bm{w}_{j}^{(t+1)},\bm{v}_{e}\rangle =\langle\bm{w}_{j}^{(t)}-\eta\nabla_{\bm{w}_{j}^{(t)}}\mathcal{L} (\bm{W}^{(t)}),\bm{v}_{e}\rangle\] \[=\langle\bm{w}_{j}^{(t)},\bm{v}_{e}\rangle+\frac{3\eta\beta_{e}^{ 3}}{N}\sum_{i=1}^{\alpha N}l_{i}^{(t)}\langle\bm{w}_{j}^{(t)},\bm{v}_{e} \rangle^{2}\] \[=\langle\bm{w}_{j}^{(t)},\bm{v}_{e}\rangle+\tilde{\Theta}(\eta) \alpha\beta_{e}^{3}g_{1}(t)\langle\bm{w}_{j}^{(t)},\bm{v}_{e}\rangle^{2},\]

where the last equality holds due to Lemma B.5. 

Similarly, we obtain the following update rule for slow-learnable features.

**Lemma A.4** (Slow-learnable feature update.).: _For all \(t\geq 0\) and \(j\in[J]\), the fast-learnable feature update is_

\[\langle\bm{w}_{j}^{(t+1)},\bm{v}_{d}\rangle=\langle\bm{w}_{j}^{(t)},\bm{v}_{d} \rangle+\frac{3\eta\beta_{d}^{3}}{N}\sum_{i=1}^{N}l_{i}^{(t)}\langle\bm{w}_{j}^ {(t)},\bm{v}_{d}\rangle^{2},\] (16)

_which gives_

\[\tilde{\Theta}(\eta)\beta_{d}^{3}g_{1}(t)\langle\bm{w}_{j}^{(t)},\bm{v}_{d} \rangle^{2}\leq\langle\bm{w}_{j}^{(t+1)},\bm{v}_{d}\rangle-\langle\bm{w}_{j}^ {(t)},\bm{v}_{d}\rangle\leq\tilde{\Theta}(\eta)\beta_{d}^{3}(\alpha g_{1}(t)+ 1-\alpha)\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle^{2}\] (17)

_where \(g_{1}(t)=\text{sigmoid}\left(\sum_{j=1}^{J}-\beta_{d}^{3}\langle\bm{w}_{j}^{ (t)},\bm{v}_{d}\rangle^{3}-\beta_{e}^{3}\langle\bm{w}_{j}^{(t)},\bm{v}_{e} \rangle^{3}\right)\)._

Proof.: Plugging the update rule of GD, we have

\[\langle\bm{w}_{j}^{(t+1)},\bm{v}_{d}\rangle =\langle\bm{w}_{j}^{(t)}-\eta\nabla_{\bm{w}_{j}^{(t)}}\mathcal{L }(\bm{W}^{(t)}),\bm{v}_{d}\rangle\] \[=\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle+\frac{3\eta\beta_{d}^{ 3}}{N}\sum_{i=1}^{N}l_{i}^{(t)}\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle^{2}\]

From Lemma B.5, we have for \(1\leq i\leq\alpha N,l_{i}^{(t)}=\Theta(1)g_{1}(t)\) and for \(\alpha N+1\leq i\leq N,\Theta(1)g_{1}(t)\leq l_{i}^{(t)}\leq 1\). Combining with the above equality, we obtain the desired inequalities. 

Next, we simplify the two above update rules in the early training stage.

**Lemma A.5** (Fast-learnable feature update in early iterations).: _Let \(T_{0}>0\) be such that \(\max_{j\in[J]}\langle\bm{w}_{j}^{(T_{0})},\bm{v}_{e}\rangle\geq\tilde{\Omega} (1/\beta_{e})\). For \(t\in[0,T_{0}]\), the fast-learnable feature update has the following rule_

\[\langle\bm{w}_{j}^{(t+1)},\bm{v}_{e}\rangle=\langle\bm{w}_{j}^{(t)},\bm{v}_{ e}\rangle+\tilde{\Theta}(\eta)\alpha\beta_{e}^{3}\langle\bm{w}_{j}^{(t)},\bm{v}_{ e}\rangle^{2},\] (18)

Proof.: Let \(T_{0}>0\) be such that either \(\max_{j\in[J]}\langle\bm{w}_{j}^{(T_{0})},\bm{v}_{e}\rangle\geq\tilde{\Omega} (1/\beta_{e})\) or \(\max_{j\in[J]}\langle\bm{w}_{j}^{(T_{0})},\bm{v}_{d}\rangle\geq\tilde{\Omega} (1/\beta_{d})\). We will show later that the first condition will be met and we have \(\max_{j\in[J]}\langle\bm{w}_{j}^{(T_{0})},\bm{v}_{d}\rangle\leq\tilde{\Omega} (1/\beta_{d})\) for all \(j\in[J]\) and \(t\in[0,T_{0}]\).

Recall that \(g_{1}(t)=\text{sigmoid}\left(\sum_{j=1}^{J}-\beta_{d}^{3}\langle\bm{w}_{j}^{(t )},\bm{v}_{d}\rangle^{3}-\beta_{e}^{3}\langle\bm{w}_{j}^{(t)},\bm{v}_{e} \rangle^{3}\right)\). Then, for \(t\in[0,T_{0}]\), we have

\[g_{1}(t) =\frac{1}{1+\exp(\sum_{j=1}^{J}-\beta_{d}^{3}\langle\bm{w}_{j}^{( t)},\bm{v}_{d}\rangle^{3}-\beta_{e}^{3}\langle\bm{w}_{j}^{(t)},\bm{v}_{e} \rangle^{3})}\] \[\geq\frac{1}{1+\exp(\kappa+\kappa)}\] \[=\frac{1}{1+\exp(\tilde{\Omega}(1))},\]

where the first inequality holds due to \(\langle\bm{w}_{j}^{(t)},\bm{v}_{e}\rangle\leq\kappa/(J^{1/3}\beta_{e})\) and \(\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle\leq\kappa/(J^{1/3}\beta_{d})\) for \(t\in[0,T_{0}]\)[18][Lemma E.3]. Therefore, similar to [18, 32], we have \(g_{1}(t)=\Theta(1)\) in the early iterations. This implies the result in Lemma A.3 as

\[\langle\bm{w}_{j}^{(t+1)},\bm{v}_{e}\rangle=\langle\bm{w}_{j}^{(t)},\bm{v}_{e }\rangle+\tilde{\Theta}(\eta)\alpha\beta_{e}^{3}\langle\bm{w}_{j}^{(t)},\bm{v}_ {e}\rangle^{2}.\] (19)

Similarly, we obtain the following simplified update rule for slow-learnable features in the early iterations.

**Lemma A.6** (Slow-learnable feature update in early iterations).: _Let \(T_{0}>0\) be such that \(\max_{j\in[J]}\langle\bm{w}_{j}^{(T_{0})},\bm{v}_{e}\rangle\geq\tilde{\Omega} (1/\beta_{e})\). For \(t\in[0,T_{0}]\), the fast-learnable feature update has the following rule_

\[\langle\bm{w}_{j}^{(t+1)},\bm{v}_{d}\rangle=\langle\bm{w}_{j}^{(t)},\bm{v}_{d} \rangle+\tilde{\Theta}(\eta)\beta_{d}^{3}\langle\bm{w}_{j}^{(t)},\bm{v}_{d} \rangle^{2},\] (20)We next show that GD will learn the fast-learnable feature quicker than learning the slow-learnable feature.

**Lemma A.7**.: _Assume \(\eta=\tilde{o}(\beta_{d}\sigma_{0})\). Let \(T_{0}\) be the iteration number that \(\max_{j\in[J]}\langle\bm{w}_{j}^{(T_{0})},\bm{v}_{e}\rangle\) reaches \(\tilde{\Omega}(1/\beta_{e})=\tilde{\Theta(1)}\). Then, we have for all \(t\leq T_{0}\), it holds that \(\max_{j\in[J]}\langle\bm{w}_{j}^{(T_{0})},\bm{v}_{d}\rangle=\tilde{O}(\sigma_ {0})\)._

Proof.: Among all the possible indices \(j\in[J]\), we focus on the index \(j^{\star}=\arg\max_{j\in[J]}\langle\bm{w}_{j}^{(0)},\bm{v}_{e}\rangle\). Therefore, for \(C_{t}=\alpha\beta_{e}^{3}=\Theta(1)\), we apply Lemma B.2 with two positive sequences \(\langle\bm{w}_{j^{\star}}^{(t)},\bm{v}_{e}\rangle\) and \(\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle\) defined in Lemmas A.5 and A.6 and get

\[\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle\leq O(\langle\bm{w}_{j}^{(0)},\bm{ v}_{d}\rangle)=\tilde{O}(\sigma_{0})\] (21)

for all \(j\in[J]\). 

**Theorem A.8** (Restatement of Theorem 3.2).: _We consider training a two-layer nonlinear CNN model initialized with \(\bm{W}^{(0)}\sim\mathcal{N}(0,\sigma_{0}^{2})\) on the training dataset \(D=\{(\bm{x}_{i},y_{i})\}_{i=1}^{N}\) that follows the data distribution \(\mathcal{D}(\beta_{e},\beta_{d},\alpha)\) with \(\alpha^{1/3}\beta_{e}>\beta_{d}\). After training with GD in Equation 3 for \(T_{\text{GD}}\) iterations where_

\[T_{\text{GD}}=\frac{\tilde{\Theta}(1)}{\eta\alpha\beta_{e}^{3}\sigma_{0}}+ \tilde{\Theta}(1)\Big{[}\frac{-\log(\sigma_{0}\beta_{e})}{\log(2)}\Big{]},\] (22)

_for all \(j\in[J]\) and \(t\in[0,T_{\text{GD}})\), we have_

\[\langle\bm{w}_{j}^{(t+1)},\bm{v}_{e}\rangle=\langle\bm{w}_{j}^{( t)},\bm{v}_{e}\rangle+\tilde{\Theta}(\eta)\alpha\beta_{e}^{3}\langle\bm{w}_{j}^ {(t)},\bm{v}_{e}\rangle^{2}.\] (23) \[\langle\bm{w}_{j}^{(t+1)},\bm{v}_{d}\rangle=\langle\bm{w}_{j}^{( t)},\bm{v}_{d}\rangle+\tilde{\Theta}(\eta)\beta_{d}^{3}\langle\bm{w}_{j}^{(t)}, \bm{v}_{d}\rangle^{2}\] (24)

_After training for \(T_{\text{GD}}\) iterations, with high probability, the learned weight has the following properties: (1) it learns the fast-learnable feature \(\bm{v}_{e}:\max_{j\in[J]}\langle\bm{w}_{j}^{(T_{\text{GD}})},\bm{v}_{e}\rangle \geq\tilde{\Omega}(1/\beta_{e})\); (2) it does not learn the slow-learnable feature \(\bm{v}_{d}:\max_{j\in[J]}\langle\bm{w}_{j}^{(T_{\text{GD}})},\bm{v}_{d}\rangle =\tilde{O}(\sigma_{0})\)._

Proof.: From the results of Lemmas A.5- A.7, it remains to calculate the time \(T_{\text{GD}}\). Plugging \(v=\tilde{\Omega}(1/\beta_{e}),m=M=\tilde{\Theta}(\eta)\alpha\beta_{e}^{3},z_{ 0}=\tilde{O}(\sigma_{0})\) into Lemma B.3, we have \(T_{\text{GD}}\) as

\[T_{\text{GD}}=\frac{\tilde{\Theta}(1)}{\eta\alpha\beta_{e}^{3}\sigma_{0}}+ \tilde{\Theta}(1)\Big{[}\frac{-\log(\sigma_{0}\beta_{e})}{\log(2)}\Big{]}\] (25)

### Proof of Theorem 3.3

Before going into the analysis, we denote the derivative of a data example \(i\) at iteration \(t\) to be

\[l_{i,\bm{\epsilon}}^{(t)}=\frac{\exp(-y_{i}f(\bm{x}_{i};\bm{W}^{(t)}+\bm{ \epsilon}^{(t)})))}{1+\exp(-y_{i}f(\bm{x}_{i};\bm{W}^{(t)}+\bm{\epsilon}^{(t)}) )}=\text{sigmoid}(-y_{i}f(\bm{x}_{i};\bm{W}^{(t)}+\bm{\epsilon}^{(t)}),\] (26)

where \(\bm{\epsilon}^{(t)}=\rho^{(t)}\nabla\mathcal{L}(\bm{W}^{(t)})\) is the weighted ascent direction at the current parameter \(\bm{W}^{(t)}\). We denote the weight vector of the \(j\)-th filter after being perturbed by SAM as

\[\bm{w}_{j,\bm{\epsilon}}^{(t)}=\bm{w}_{j}^{(t)}+\bm{\epsilon}_{j}^{(t)}=\bm{w} _{j}^{(t)}+\rho^{(t)}\nabla_{\bm{w}_{j}^{(t)}}\mathcal{L}(\bm{W}^{(t)}),\] (27)

where \(\rho^{(t)}=\rho/\left\|\nabla\mathcal{L}(\bm{W}^{(t)})\right\|_{F}\).

First, we have the following inequalities regarding the gradient norm:

\[\left\|\nabla\mathcal{L}(\bm{W}^{(t)})\right\|_{F} \geq\left\|\nabla_{\bm{w}_{j}^{(t)}}\mathcal{L}(\bm{W}^{(t)})\right\|\] (28) \[=\langle\nabla_{\bm{w}_{j}^{(t)}}\mathcal{L}(\bm{W}^{(t)}),\nabla_ {\bm{w}_{j}^{(t)}}\mathcal{L}(\bm{W}^{(t)})\rangle^{1/2}\] (29) \[=\left[\left(\frac{3\beta_{d}^{3}}{N}\sum_{i=1}^{N}l_{i}^{(t)} \langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle^{2}\right)^{2}+\left(\frac{3\beta_{e }^{3}}{N}\sum_{i=1}^{\alpha N}l_{i}^{(t)}\langle\bm{w}_{j}^{(t)},\bm{v}_{e} \rangle^{2}\right)^{2}\right.\] \[\left.+\left\|\frac{3}{N}\sum_{i=1}^{N}l_{i}^{(t)}y_{i}\langle \bm{w}_{j}^{(t)},\bm{\xi}_{i}\rangle^{2}\bm{\xi}_{i}\right\|\right]^{1/2}\] (30)

Thus,

\[\left\|\nabla\mathcal{L}(\bm{W}^{(t)})\right\|_{F} \geq\frac{3\beta_{d}^{3}}{N}\sum_{i=1}^{N}l_{i}^{(t)}\langle\bm{ w}_{j}^{(t)},\bm{v}_{d}\rangle^{2}\] (31) \[\left\|\nabla\mathcal{L}(\bm{W}^{(t)})\right\|_{F} \geq\frac{3\beta_{e}^{3}}{N}\sum_{i=1}^{\alpha N}l_{i}^{(t)}\langle \bm{w}_{j}^{(t)},\bm{v}_{e}\rangle^{2}\] (32)

**Lemma A.9** (Gradient).: _Let the loss function \(\mathcal{L}\) be as defined in Equation 2. For \(t\geq 0\) and \(j\in[J]\), the gradient of the loss \(\mathcal{L}(\bm{W}^{(t)}+\bm{\epsilon}^{(t)})\) with regard to neuron \(\bm{w}_{j,\bm{\epsilon}}^{(t)}\) is_

\[\nabla_{\bm{w}_{j,\bm{\epsilon}}^{(t)}}\mathcal{L}(\bm{W}^{(t)}+ \bm{\epsilon}^{(t)}) =-\frac{3}{N}\sum_{i=1}^{\alpha N}l_{i,\bm{\epsilon}}^{(t)}\left( \beta_{d}^{3}\langle\bm{w}_{j,\bm{\epsilon}}^{(t)},\bm{v}_{d}\rangle^{2}\bm{ v}_{d}+\beta_{e}^{3}\langle\bm{w}_{j,\bm{\epsilon}}^{(t)},\bm{v}_{e}\rangle^{2} \bm{v}_{e}+y_{i}\langle\bm{w}_{j,\bm{\epsilon}}^{(t)},\bm{\xi}_{i}\rangle^{2} \bm{\xi}_{i}\right)-\] \[\quad\frac{3}{N}\sum_{i=\alpha N+1}^{N}l_{i,\bm{\epsilon}}^{(t)} \left(\beta_{d}^{3}\langle\bm{w}_{j,\bm{\epsilon}}^{(t)},\bm{v}_{d}\rangle^{2 }\bm{v}_{d}+y_{i}\langle\bm{w}_{j,\bm{\epsilon}}^{(t)},\bm{\xi}_{i}\rangle^{2 }\bm{\xi}_{i}\right)\] (33)

Proof.: We have the following gradient

\[\nabla_{\bm{w}_{j,\bm{\epsilon}}^{(t)}}\mathcal{L}(\bm{W}^{(t)}+ \bm{\epsilon}^{(t)}) =-\frac{1}{N}\sum_{i=1}^{N}\frac{\exp(-y_{i}f(\bm{x}_{i};\bm{W}^{ (t)}+\bm{\epsilon}^{(t)}))}{1+\exp(-y_{i}f(\bm{x}_{i};\bm{W}^{(t)}+\bm{ \epsilon}^{(t)}))}\cdot y_{i}f^{\prime}(\bm{x}_{i};\bm{W}^{(t)}+\bm{\epsilon} ^{(t)})\] \[=-\frac{3}{N}\sum_{i=1}^{N}l_{i,\bm{\epsilon}}^{(t)}y_{i}\sum_{p= 1}^{P}\langle\bm{w}_{j,\bm{\epsilon}}^{(t)},\bm{x}^{(p)}\rangle^{2}\cdot\bm{ x}^{(p)}\] \[=-\frac{3}{N}\sum_{i=1}^{\alpha N}l_{i,\bm{\epsilon}}^{(t)}\left( \beta_{d}^{3}\langle\bm{w}_{j,\bm{\epsilon}}^{(t)},\bm{v}_{d}\rangle^{2}\bm{ v}_{d}+\beta_{e}^{3}\langle\bm{w}_{j,\bm{\epsilon}}^{(t)},\bm{v}_{e}\rangle^{2} \bm{v}_{e}+y_{i}\langle\bm{w}_{j,\bm{\epsilon}}^{(t)},\bm{\xi}_{i}\rangle^{2} \bm{\xi}_{i}\right)-\] \[\quad\frac{3}{N}\sum_{i=\alpha N+1}^{N}l_{i,\bm{\epsilon}}^{(t)} \left(\beta_{d}^{3}\langle\bm{w}_{j,\bm{\epsilon}}^{(t)},\bm{v}_{d}\rangle^{2 }\bm{v}_{d}+y_{i}\langle\bm{w}_{j,\bm{\epsilon}}^{(t)},\bm{\xi}_{i}\rangle^{2 }\bm{\xi}_{i}\right)\]

With the above formula of gradient, we have the projection of perturbed weight on \(\bm{v}_{e}\) is

\[\langle\bm{w}_{j,\bm{\epsilon}}^{(t)},\bm{v}_{e}\rangle =\langle\bm{w}_{j}^{(t)},\bm{v}_{e}\rangle+\langle\bm{\epsilon}_ {j}^{(t)},\bm{v}_{e}\rangle\] \[=\langle\bm{w}_{j}^{(t)},\bm{v}_{e}\rangle+\langle\rho^{(t)} \nabla_{\bm{w}_{j}^{(t)}}\mathcal{L}(\bm{W}^{(t)}),\bm{v}_{e}\rangle\] \[=\langle\bm{w}_{j}^{(t)},\bm{v}_{e}\rangle-\frac{3\rho^{(t)} \beta_{e}^{3}}{N}\sum_{i=1}^{\alpha N}l_{i}^{(t)}\langle\bm{w}_{j}^{(t)},\bm{ v}_{e}\rangle^{2}\] (34)

From Equations 32 and 34, we have

\[0\leq\langle\bm{w}_{j}^{(t)},\bm{v}_{e}\rangle-\rho\leq\langle\bm{w}_{j,\bm{ \epsilon}}^{(t)},\bm{v}_{e}\rangle\leq\langle\bm{w}_{j}^{(t)},\bm{v}_{e}\rangle\] (35)Similarly, the projection of perturbed weight on \(\bm{v}_{d}\) is

\[\langle\bm{w}_{j,\bm{\epsilon}}^{(t)},\bm{v}_{d}\rangle =\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle-\frac{3\rho^{(t)}\beta_{ d}^{3}}{N}\sum_{i=1}^{N}l_{i}^{(t)}\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle^{2}\] (36) \[0 \leq\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle-\rho\leq\langle\bm {w}_{j,\bm{\epsilon}}^{(t)},\bm{v}_{d}\rangle\leq\langle\bm{w}_{j}^{(t)},\bm{ v}_{d}\rangle\] (37)

**Fast-learnable feature gradient.** The projection of the gradient on \(\bm{v}_{e}\) is

\[\langle\nabla_{\bm{w}_{j,\bm{\epsilon}}^{(t)}}\mathcal{L}(\bm{W}^{(t)}+\bm{ \epsilon}^{(t)}),\bm{v}_{e}\rangle=-\frac{3\beta_{e}^{3}}{N}\sum_{i=1}^{\alpha N }l_{i,\bm{\epsilon}}^{(t)}\langle\bm{w}_{j,\bm{\epsilon}}^{(t)},\bm{v}_{e} \rangle^{2}\] (38)

**Slow-learnable feature gradient.** The projection of the gradient on \(\bm{v}_{d}\) is

\[\langle\nabla_{\bm{w}_{j,\bm{\epsilon}}^{(t)}}\mathcal{L}(\bm{W}^{(t)}+\bm{ \epsilon}^{(t)}),\bm{v}_{d}\rangle=-\frac{3\beta_{d}^{3}}{N}\sum_{i=1}^{N}l_{ i,\bm{\epsilon}}^{(t)}\langle\bm{w}_{j,\bm{\epsilon}}^{(t)},\bm{v}_{d}\rangle^{2}\] (39)

**Noise gradient.** The projection of the gradient on \(\bm{\xi}_{i}\) is

\[\langle\nabla_{\bm{w}_{j,\bm{\epsilon}}^{(t)}}\mathcal{L}(\bm{W}^{(t)}+\bm{ \epsilon}^{(t)}),\bm{\xi}_{i}\rangle=-\frac{3}{N}\left(l_{i,\bm{\epsilon}}^{( t)}y_{i}\langle\bm{w}_{j,\bm{\epsilon}}^{(t)},\bm{\xi}_{i}\rangle^{2}\left\|\bm{ \xi}_{i}\right\|_{2}^{2}+\sum_{k=1,k\neq i}^{N}l_{k,\bm{\epsilon}}^{(t)}y_{k} \langle\bm{w}_{j,\bm{\epsilon}}^{(t)},\bm{\xi}_{k}\rangle^{2}\langle\bm{\xi}_ {k},\bm{\xi}_{i}\rangle\right)\] (40)

**Derivative of data example \(i\).** For \(1\leq i\leq\alpha N\), \(l_{i,\bm{\epsilon}}^{(t)}\) can be rewritten as

\[l_{i,\bm{\epsilon}}^{(t)} =\text{sigmoid}\left(\sum_{j=1}^{J}-\beta_{d}^{3}\langle\bm{w}_{ j,\bm{\epsilon}}^{(t)},\bm{v}_{d}\rangle^{3}-\beta_{e}^{3}\langle\bm{w}_{j, \bm{\epsilon}}^{(t)},\bm{v}_{e}\rangle^{3}-y_{i}\langle\bm{w}_{j,\bm{\epsilon }}^{(t)},\bm{\xi}_{i}\rangle^{3}\right)\] (41)

while for \(\alpha N+1\leq i\leq N\), \(l_{i}^{(t)}\) can be rewritten as

\[l_{i,\bm{\epsilon}}^{(t)} =\text{sigmoid}\left(\sum_{j=1}^{J}-\beta_{d}^{3}\langle\bm{w}_ {j,\bm{\epsilon}}^{(t)},\bm{v}_{d}\rangle^{3}-y_{i}\langle\bm{w}_{j,\bm{ \epsilon}}^{(t)},\bm{\xi}_{i}\rangle^{3}\right)\] \[\geq\text{sigmoid}\left(\sum_{j=1}^{J}-\beta_{d}^{3}\langle\bm{w} _{j,\bm{\epsilon}}^{(t)},\bm{v}_{d}\rangle^{3}-\beta_{e}^{3}\langle\bm{w}_{j, \bm{\epsilon}}^{(t)},\bm{v}_{e}\rangle^{3}-y_{i}\langle\bm{w}_{j,\bm{\epsilon }}^{(t)},\bm{\xi}_{i}\rangle^{3}\right)\] (42)

We present the detailed proofs that build up to Theorem A.15. We begin by considering the update for the fast-learnable and slow-learnable features.

**Lemma A.10** (Fast-learnable feature update.).: _For all \(t\geq 0\) and \(j\in[J]\), the fast-learnable feature update is_

\[\langle\bm{w}_{j}^{(t)},\bm{v}_{e}\rangle+\bar{\Theta}(\eta) \alpha\beta_{e}^{3}g_{2}(t)(\langle\bm{w}_{j}^{(t)},\bm{v}_{e}\rangle-\rho)^{2} \leq\langle\bm{w}_{j}^{(t+1)},\bm{v}_{e}\rangle=\langle\bm{w}_{j} ^{(t)},\bm{v}_{e}\rangle+\bar{\Theta}(\eta)\alpha\beta_{e}^{3}g_{2}(t)\langle \bm{w}_{j,\bm{\epsilon}}^{(t)},\bm{v}_{e}\rangle^{2}\] \[\leq\langle\bm{w}_{j}^{(t)},\bm{v}_{e}\rangle+\bar{\Theta}(\eta )\alpha\beta_{e}^{3}g_{2}(t)(\langle\bm{w}_{j}^{(t)},\bm{v}_{e}\rangle)^{2}\] (43)

_where \(g_{2}(t)=\text{sigmoid}\left(\sum_{j=1}^{J}-\beta_{d}^{3}\langle\bm{w}_{j,\bm{ \epsilon}}^{(t)},\bm{v}_{d}\rangle^{3}-\beta_{e}^{3}\langle\bm{w}_{j,\bm{ \epsilon}}^{(t)},\bm{v}_{e}\rangle^{3}\right)\)._

Proof.: Plugging the update rule of SAM, we have

\[\langle\bm{w}_{j}^{(t+1)},\bm{v}_{e}\rangle =\langle\bm{w}_{j}^{(t)}-\eta\nabla_{\bm{w}_{j,\bm{\epsilon}}^{( t)}}\mathcal{L}(\bm{W}^{(t)}+\bm{\epsilon}^{(t)}),\bm{v}_{e}\rangle\] \[=\langle\bm{w}_{j}^{(t)},\bm{v}_{e}\rangle+\frac{3\eta\alpha \beta_{e}^{3}}{N}\sum_{i=1}^{N}l_{i,\bm{\epsilon}}^{(t)}\langle\bm{w}_{j,\bm{ \epsilon}}^{(t)},\bm{v}_{e}\rangle^{2}\] \[=\langle\bm{w}_{j}^{(t)},\bm{v}_{e}\rangle+\bar{\Theta}(\eta) \alpha\beta_{e}^{3}g_{2}(t)\langle\bm{w}_{j,\bm{\epsilon}}^{(t)},\bm{v}_{e} \rangle^{2},\]

where the last equality holds due to Lemma B.10. Combining with Equation 35, we obtain the desired inequalities.

Similarly, we obtain the following update rule for slow-learnable features.

**Lemma A.11** (Slow-learnable feature update.).: _For all \(t\geq 0\) and \(j\in[J]\), the slow-learnable feature update is_

\[\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle+\tilde{\Theta}(\eta) \beta_{d}^{3}g_{2}(t)(\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle-\rho)^{2} \leq\langle\bm{w}_{j}^{(t+1)},\bm{v}_{d}\rangle=\langle\bm{w}_{j} ^{(t)},\bm{v}_{d}\rangle+\frac{3\eta\beta_{d}^{3}}{N}\sum_{i=1}^{N}l_{i}^{(t)} \langle\bm{w}_{j,\bm{\epsilon}}^{(t)},\bm{v}_{d}\rangle^{2}\] (44) \[\leq\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle+\tilde{\Theta}(\eta )\beta_{d}^{3}(\alpha g_{2}(t)+1-\alpha)(\langle\bm{w}_{j}^{(t)},\bm{v}_{d} \rangle)^{2}\]

_where \(g_{2}(t)=\text{sigmoid}\left(\sum_{j=1}^{J}-\beta_{d}^{3}\langle\bm{w}_{j,\bm {\epsilon}}^{(t)},\bm{v}_{d}\rangle^{3}-\beta_{e}^{3}\langle\bm{w}_{j,\bm{ \epsilon}}^{(t)},\bm{v}_{e}\rangle^{3}\right)\)._

Proof.: Plugging the update rule of GD, we have

\[\langle\bm{w}_{j}^{(t+1)},\bm{v}_{d}\rangle =\langle\bm{w}_{j}^{(t)}-\eta\nabla_{\bm{w}_{j,\bm{\epsilon}}^{( t)}}\mathcal{L}(\bm{W}^{(t)}),\bm{v}_{d}\rangle\] \[=\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle+\frac{3\eta\beta_{d}^ {3}}{N}\sum_{i=1}^{N}l_{i}^{(t)}\langle\bm{w}_{j,\bm{\epsilon}}^{(t)},\bm{v}_{ d}\rangle^{2}\]

From Lemma B.5, we have for \(1\leq i\leq\alpha N,l_{i}^{(t)}=\Theta(1)g_{1}(t)\) and for \(\alpha N+1\leq i\leq N,\Theta(1)g_{1}(t)\leq l_{i}^{(t)}\leq 1\). Combining with the above equality and Equation 37, we obtain the desired inequalities. 

Next, we simplify the two above update rules in the early training stage.

**Lemma A.12** (Fast-learnable feature update in early iterations).: _Let \(T_{0}>0\) be such that \(\max_{j\in[J]}\langle\bm{w}_{j}^{(T_{0})},\bm{v}_{e}\rangle\geq\tilde{\Omega} (1/\beta_{e})\). For \(t\in[0,T_{0}]\), the fast-learnable feature update has the following rule_

\[\tilde{\Theta}(\eta)\alpha\beta_{e}^{3}(\langle\bm{w}_{j}^{(t)}, \bm{v}_{e}\rangle-\rho)^{2}\leq\langle\bm{w}_{j}^{(t+1)},\bm{v}_{e}\rangle- \langle\bm{w}_{j}^{(t)},\bm{v}_{e}\rangle\leq\langle\bm{w}_{j}^{(t)},\bm{v}_{ e}\rangle+\tilde{\Theta}(\eta)\alpha\beta_{e}^{3}(\langle\bm{w}_{j}^{(t)},\bm{v}_{e} \rangle)^{2}\] (45)

Proof.: Let \(T_{0}>0\) be such that either \(\max_{j\in[J]}\langle\bm{w}_{j}^{(T_{0})},\bm{v}_{e}\rangle\geq\tilde{\Omega} (1/\beta_{e})\) or \(\max_{j\in[J]}\langle\bm{w}_{j}^{(T_{0})},\bm{v}_{d}\rangle\geq\tilde{\Omega} (1/\beta_{d})\). We will show later that the first condition will be met and we have \(\max_{j\in[J]}\langle\bm{w}_{j}^{(T_{0})},\bm{v}_{d}\rangle\leq\tilde{\Omega} (1/\beta_{d})\) for all \(j\in[J]\) and \(t\in[0,T_{0}]\).

Recall that \(g_{2}(t)=\text{sigmoid}\left(\sum_{j=1}^{J}-\beta_{d}^{3}\langle\bm{w}_{j, \bm{\epsilon}}^{(t)},\bm{v}_{d}\rangle^{3}-\beta_{e}^{3}\langle\bm{w}_{j,\bm {\epsilon}}^{(t)},\bm{v}_{e}\rangle^{3}\right)\). Then, for \(t\in[0,T_{0}]\), we have

\[g_{2}(t) =\frac{1}{1+\exp(\sum_{j=1}^{J}-\beta_{d}^{3}\langle\bm{w}_{j, \bm{\epsilon}}^{(t)},\bm{v}_{d}\rangle^{3}-\beta_{e}^{3}\langle\bm{w}_{j,\bm{ \epsilon}}^{(t)},\bm{v}_{e}\rangle^{3})}\] \[\geq\frac{1}{1+\exp(\kappa+\kappa)}\] \[=\frac{1}{1+\exp(\tilde{\Omega}(1))},\]

where the first inequality holds due to \(\langle\bm{w}_{j,\bm{\epsilon}}^{(t)},\bm{v}_{e}\rangle\leq\langle\bm{w}_{j}^{(t )},\bm{v}_{e}\rangle\leq\kappa/(J^{1/3}\beta_{e})\) and \(\langle\bm{w}_{j,\bm{\epsilon}}^{(t)},\bm{v}_{d}\rangle\leq\langle\bm{w}_{j}^{ (t)},\bm{v}_{d}\rangle\leq\kappa/(J^{1/3}\beta_{d})\) for \(t\in[0,T_{0}]\). Therefore, we have \(g_{2}(t)=\Theta(1)\) in the early iterations. Replacing \(g_{2}(t)=\Theta(1)\) into the results of Lemma A.10, we obtain the desired results. 

Similarly, we obtain the following simplified update rule for slow-learnable features in the early iterations.

**Lemma A.13** (Slow-learnable feature update in early iterations).: _Let \(T_{0}>0\) be such that \(\max_{j\in[J]}\langle\bm{w}_{j}^{(T_{0})},\bm{v}_{e}\rangle\geq\tilde{\Omega} (1/\beta_{e})\). For \(t\in[0,T_{0}]\), the fast-learnable feature update has the following rule_

\[\tilde{\Theta}(\eta)\beta_{d}^{3}(\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle- \rho)^{2}\leq\langle\bm{w}_{j}^{(t+1)},\bm{v}_{d}\rangle-\langle\bm{w}_{j}^{(t )},\bm{v}_{d}\rangle\leq\tilde{\Theta}(\eta)\beta_{d}^{3}(\langle\bm{w}_{j}^{(t )},\bm{v}_{d}\rangle)^{2}\] (46)We next show that SAM will learn the fast-learnable feature quicker than the slow-learnable one.

**Lemma A.14**.: _Assume \(\eta=\tilde{o}(\beta_{d}\sigma_{0})\). Let \(T_{0}\) be the iteration number that \(\max_{j\in[J]}\langle\bm{w}_{j}^{(T_{0})},\bm{v}_{e}\rangle\) reaches \(\tilde{\Omega}(1/\beta_{e})=\tilde{\Theta(1)}\). Then, we have for all \(t\leq T_{0}\), it holds that \(\max_{j\in[J]}\langle\bm{w}_{j}^{(T_{0})},\bm{v}_{d}\rangle=\tilde{O}(\sigma_ {0})\)._

Proof.: Among all the possible indices \(j\in[J]\), we focus on the index \(j^{\star}=\arg\max_{j\in[J]}\langle\bm{w}_{j}^{(0)},\bm{v}_{e}\rangle\). Therefore, for \(C_{t}=\alpha\beta_{e}^{3}=\Theta(1)\), we apply Lemma B.2 with two positive sequences \(\langle\bm{w}_{j^{\star}}^{(t)},\bm{v}_{e}\rangle\) and \(\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle\) defined in Lemmas A.12 and A.13 and get

\[\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle\leq O(\langle\bm{w}_{j}^{(0)},\bm{ v}_{d}\rangle)=\tilde{O}(\sigma_{0})\] (47)

for all \(j\in[J]\). 

**Theorem A.15** (Restatement of Theorem 3.3).: _We consider training a two-layer nonlinear CNN model initialized with \(\bm{W}^{(0)}\sim\mathcal{N}(0,\sigma_{0}^{2})\) on the training dataset \(D=\{(\bm{x}_{i},y_{i})\}_{i=1}^{N}\) that follows the data distribution \(\mathcal{D}(\beta_{e},\beta_{d},\alpha)\) with \(\alpha^{1/3}\beta_{e}>\beta_{d}\). After training with SAM in Equation 3 for \(T_{\text{SAM}}\) iterations where_

\[T_{\text{SAM}}=\frac{\tilde{\Theta}(\sigma_{0})}{\eta\alpha\beta_{e}^{3}( \sigma_{0}-\rho)^{2}}+\frac{\tilde{\Theta}(\sigma_{0}^{2})}{(\sigma_{0}-\rho)^ {2}}\Big{\lceil}\frac{-\log(\sigma_{0}\beta_{e})}{\log(2)}\Big{\rceil},\] (48)

_for all \(j\in[J]\) and \(t\in[0,T_{\text{SAM}})\), we have_

\[\langle\bm{w}_{j}^{(t)},\bm{v}_{e}\rangle+\tilde{\Theta}(\eta) \alpha\beta_{e}^{3}(\langle\bm{w}_{j}^{(t)},\bm{v}_{e}\rangle-\rho)^{2} \leq\langle\bm{w}_{j}^{(t+1)},\bm{v}_{e}\rangle=\langle\bm{w}_{j} ^{(t)},\bm{v}_{e}\rangle+\tilde{\Theta}(\eta)\alpha\beta_{e}^{3}\langle\bm{w}_ {j,\bm{\epsilon}}^{(t)},\bm{v}_{e}\rangle^{2}\] \[\leq\langle\bm{w}_{j}^{(t)},\bm{v}_{e}\rangle+\tilde{\Theta}( \eta)\alpha\beta_{e}^{3}(\langle\bm{w}_{j}^{(t)},\bm{v}_{e}\rangle)^{2}\] (49) \[\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle+\tilde{\Theta}(\eta) \beta_{d}^{3}(\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle-\rho)^{2} \leq\langle\bm{w}_{j}^{(t+1)},\bm{v}_{d}\rangle=\langle\bm{w}_{j} ^{(t)},\bm{v}_{d}\rangle+\tilde{\Theta}(\eta)\beta_{d}^{3}\langle\bm{w}_{j,\bm {\epsilon}}^{(t)},\bm{v}_{d}\rangle^{2}\] \[\leq\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle+\tilde{\Theta}( \eta)\beta_{d}^{3}(\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle)^{2}\] (50)

_After training for \(T_{\text{SAM}}\) iterations, with high probability, the learned weight has the following properties: (1) it learns the fast-learnable feature \(\bm{v}_{e}:\max_{j\in[J]}\langle\bm{w}_{j}^{(T_{\text{SAM}})},\bm{v}_{e}\rangle \geq\tilde{\Omega}(1/\beta_{e})\); (2) it does not learn the slow-learnable feature \(\bm{v}_{d}:\max_{j\in[J]}\langle\bm{w}_{j}^{(T_{\text{SAM}})},\bm{v}_{d} \rangle=\tilde{O}(\sigma_{0})\)._

Proof.: With the results of Lemmas A.12- A.14, it remains to calculate the time \(T_{\text{SAM}}\). Plugging \(v=\tilde{\Omega}(1/\beta_{e}),m=M=\tilde{\Theta}(\eta)\alpha\beta_{e}^{3},z_{ 0}=\tilde{O}(\sigma_{0})\) into Lemma B.3, we have \(T_{\text{SAM}}\) as

\[T_{\text{SAM}}=\frac{\tilde{\Theta}(\sigma_{0})}{\eta\alpha\beta_{e}^{3}( \sigma_{0}-\rho)^{2}}+\frac{\tilde{\Theta}(\sigma_{0}^{2})}{(\sigma_{0}-\rho)^ {2}}\Big{\lceil}\frac{-\log(\sigma_{0}\beta_{e})}{\log(2)}\Big{\rceil}\] (51)

Comparing Eq. 22 and 48, we can see that SAM learns the fast-learnable features later than GD. Particularly, if we remove the approximate notations, we have the following inequality

\[\frac{1}{\eta\beta_{e}^{3}\sigma_{0}}+\Big{\lceil}\frac{-\log(\sigma_{0}\beta_{e })}{\log(2)}\Big{\rceil}\geq\frac{\sigma_{0}}{\eta\beta_{e}^{3}(\sigma_{0}- \rho)^{2}}+\frac{\sigma_{0}^{2}}{(\sigma_{0}-\rho)^{2}}\Big{\lceil}\frac{- \log(\sigma_{0}\beta_{e})}{\log(2)}\Big{\rceil},\] (52)

which holds due to Assumption A.1 about weight initialization in Appendix A.1, i.e., (\(\sigma_{0}\geq\rho\geq 0\)). 

### Proof of Theorem 3.4

In this section, we show that SAM learns fast-learnable and slow-learnable features at a more uniform speed. To ease the notation, we denote \(G_{e}^{(t)}=\max_{j\in[J]}\langle\bm{w}_{j}^{(t)},\bm{v}_{e}\rangle\) and \(G_{d}^{(t)}=\max_{j\in[J]}\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle\) for model weights trained with GD. Similarly, we denote \(S_{e}^{(t)}\) and \(S_{d}^{(t)}\) for model weights trained with SAM. We use \(\hat{S}_{e}^{(t)}\) and \(\hat{S}_{d}^{(t)}\) to denote the inner products with perturbed weights. We simplify Equation 34 and 36 for early iterations \(t\leq T_{0}\) as

\[\hat{S}_{e}^{(t)} =S_{e}^{(t)}-\tilde{\Theta}(1)\rho^{(t)}\alpha\beta_{e}^{3}(S_{e}^{ (t)})^{2}\] (53) \[\hat{S}_{d}^{(t)} =S_{d}^{(t)}-\tilde{\Theta}(1)\rho^{(t)}\beta_{d}^{3}(S_{d}^{(t)})^{2}\] (54)Before introducing the theorem, we assume that the model is initialized in favor of the fast-learnable feature, i.e. \(G_{e}^{(0)}-G_{d}^{(0)}\geq\rho\). This is reasonable as a consequence of Theorem A.8 because we can just train the model for several iterations to achieve this initialization (similar argument for Assumption A.1).

**Theorem A.16** (Restatement of Theorem 3.4).: _Consider the training dataset \(D=\{(\bm{x}_{i},y_{i})\}_{i=1}^{N}\) that follows the data distribution \(\mathcal{D}(\beta_{e},\beta_{d},\alpha)\) using the two-layer nonlinear CNN model initialized with \(\bm{W}^{(0)}\sim\mathcal{N}(0,\sigma_{0}^{2})\). Assume that the fast-learnable feature strength is significantly larger \(\alpha^{1/3}\beta_{e}>\beta_{d}\). Training the same model initialization, we have that for every iteration \(t\leq T_{0}\)_

\[\rho+S_{d}^{(t)} \leq S_{e}^{(t)}\] (55) \[\hat{S}_{d}^{(t)} <\hat{S}_{e}^{(t)}\] (56) \[S_{e}^{(t)} \leq G_{e}^{(t)}\] (57) \[S_{d}^{(t)} \leq G_{d}^{(t)}\] (58) \[S_{e}^{(t)}-S_{d}^{(t)} \leq G_{e}^{(t)}-G_{d}^{(t)}\] (59)

Proof.: We prove this by induction. For \(t=0\), the above hypotheses immediately hold because we use train two methods from the same initialization. Particularly, we have \(0<S_{d}^{(0)}=G_{d}^{(0)}<G_{e}^{(0)}=S_{e}^{(0)}\) and \(\hat{S}_{e}^{(0)}-\hat{S}_{d}^{(0)}\geq S_{e}^{(0)}-\rho-S_{d}^{(0)}\geq(G_{e}^ {(0)}-G_{d}^{(0)})-\rho\geq 0\).

Assume that the induction hypotheses hold for \(t\), i.e.

\[\rho+S_{d}^{(t)} \leq S_{e}^{(t)}\] (60) \[\hat{S}_{d}^{(t)} <\hat{S}_{e}^{(t)}\] (61) \[S_{e}^{(t)} \leq G_{e}^{(t)}\] (62) \[S_{d}^{(t)} \leq G_{d}^{(t)}\] (63) \[S_{e}^{(t)}-S_{d}^{(t)} \leq G_{e}^{(t)}-G_{d}^{(t)}\] (64)

We need to prove that they also hold for \(t+1\). From Lemma A.15 and the first two induction hypotheses,

\[\rho+S_{d}^{(t+1)}=\rho+S_{d}^{(t)}+\tilde{\Theta}(\eta)\beta_{d}^{3}(\hat{S}_ {d}^{(t)})^{2}<S_{e}^{(t)}+\tilde{\Theta}(\eta)\alpha\beta_{e}^{3}(\hat{S}_{e} ^{(t)})^{2}=S_{e}^{(t+1)}\] (65)

Then, \(\hat{S}_{e}^{(t+1)}-\hat{S}_{d}^{(t+1)}\geq S_{e}^{(t+1)}-\rho-S_{d}^{(t+1)}\geq 0\). From Equation 35 and Lemma A.12,

\[S_{e}^{(t+1)}\leq S_{e}^{(t)}+\tilde{\Theta}(\eta)\alpha\beta_{e}^{3}(S_{e}^{( t)})^{2}\leq G_{e}^{(t)}+\tilde{\Theta}(\eta)\alpha\beta_{e}^{3}(G_{e}^{(t)})^{2} \leq G_{e}^{(t+1)}.\] (66)

Similarly, we have \(S_{d}^{(t+1)}\leq G_{d}^{(t+1)}\). From Equations 53 and 54,

\[S_{d}^{(t)}-\hat{S}_{d}^{(t)}=\tilde{\Theta}(1)\rho^{(t)}\beta_{ d}^{3}(S_{d}^{(t)})^{2} ) <\tilde{\Theta}(1)\rho^{(t)}\alpha\beta_{e}^{3}(S_{e}^{(t)})^{2}=S_{ e}^{(t)}-\hat{S}_{e}^{(t)}\] (67) \[0\leq\hat{S}_{e}^{(t)}-\hat{S}_{d}^{(t)}<S_{e}^{(t)}-S_{d}^{(t)} \leq G_{e}^{(t)}-G_{d}^{(t)}\] (68)

Combining with Equations 35 and 37, we have

\[(\hat{S}_{e}^{(t)})^{2}-(\hat{S}_{d}^{(t)})^{2} <(S_{e}^{(t)})^{2}-(S_{d}^{(t)})^{2} \leq(G_{e}^{(t)})^{2}-(G_{d}^{(t)})^{2}\] (69) \[(G_{d}^{(t)})^{2}-(\hat{S}_{d}^{(t)})^{2} <(G_{e}^{(t)})^{2}-(\hat{S}_{e}^{(t)})^{2}\] (70) \[\tilde{\Theta}(\eta)\beta_{d}^{3}((G_{e}^{(t)})^{2}-(\hat{S}_{d}^ {(t)})^{2}) <\tilde{\Theta}(\eta)\alpha\beta_{e}^{3}((G_{e}^{(t)})^{2}-(\hat{S}_ {e}^{(t)})^{2})\] (71) \[G_{d}^{(t)}-S_{d}^{(t)}+\tilde{\Theta}(\eta)\beta_{d}^{3}((G_{d }^{(t)})^{2}-(\hat{S}_{d}^{(t)})^{2}) <G_{e}^{(t)}-S_{e}^{(t)}+\tilde{\Theta}(\eta)\alpha\beta_{e}^{3}((G_{e }^{(t)})^{2}-(\hat{S}_{e}^{(t)})^{2})\] (72) \[G_{d}^{(t+1)}-S_{d}^{(t+1)} <G_{e}^{(t+1)}-S_{e}^{(t+1)}\] (73) \[S_{e}^{(t+1)}-S_{d}^{(t+1)} <G_{e}^{(t+1)}-G_{d}^{(t+1)}\] (74)

Therefore, the induction hypotheses hold for \(t+1\).

### Proof of Theorem 3.5

From Theorem A.16, we have the following result for switching between SAM and GD during training.

**Lemma A.17**.: _Consider the training dataset \(D=\{(\bm{x}_{i},y_{i})\}_{i=1}^{N}\) that follows the data distribution \(\mathcal{D}(\beta_{e},\beta_{d},\alpha)\) using the two-layer nonlinear CNN model initialized with \(\bm{W}^{(0)}\sim\mathcal{N}(0,\sigma_{0}^{2})\). Assume that the noise is sufficiently small (ref. Lemmas B.4 and B.9) and the fast-learnable feature strength is significantly larger \(\alpha^{1/3}\beta_{e}>\beta_{d}\). From any iteration \(t\) during early training, the normalized gradient of the one-step SAM update has a larger weight on the slow-learnable feature compared to that of GD._

Proof.: First, recall the gradients of GD and SAM are as follows.

\[\nabla_{\bm{w}_{j}^{(t)}}\mathcal{L}(\bm{W}^{(t)}) =-\frac{3}{N}\sum_{i=1}^{\alpha N}l_{i}^{(t)}\left(\beta_{d}^{3} \langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle^{2}\bm{v}_{d}+\beta_{e}^{3}\langle \bm{w}_{j}^{(t)},\bm{v}_{e}\rangle^{2}\bm{v}_{e}+y_{i}\langle\bm{w}_{j}^{(t)}, \bm{\xi}_{i}\rangle^{2}\bm{\xi}_{i}\right)-\] \[\quad\frac{3}{N}\sum_{i=\alpha N+1}^{N}l_{i}^{(t)}\left(\beta_{d} ^{3}\langle\bm{w}_{j}^{(t)},\bm{v}_{d}\rangle^{2}\bm{v}_{d}+y_{i}\langle\bm{w }_{j}^{(t)},\bm{\xi}_{i}\rangle^{2}\bm{\xi}_{i}\right)\] (75) \[\nabla_{\bm{w}_{j,\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ \bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ }}}}}}}}}}}}}}}}(t) \]\] \] \[\quad\frac{3}{N}\sum_{i=\alpha N+1}^{N}l_{i,\bm{\bm{\bm{\bm{ \bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ }}}}}}}}}}}}}}}}}} \left(\beta_{d}^{3}\langle\bm{w}_{j,\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ \bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bmbm{\bmbm{ }}}}}}}}}}}}}}}}}}} \rangle}\]\]\]\] \] \[\quad\frac{3}{N}\sum_{i=\alpha N+1}^{N}l_{i,\bm{\bm{\bm{ \bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ }}}}}}}}}}}}}}}}}}} \left(\beta_{d}^{3}\langle\bm{w}_{j,\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ \bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ }}}}}}}}}}}}}}}}}}} \rangle}\right\]\] \[\quad\frac{3}{N}\sum_{i=\alpha N+1}^{N}l_{i,\bm{\bm{\bm{\bm{ \bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ }}}}}}}}}}}}}}}}}}} \left(\beta_{d}^{3}\langle\bm{w}_{j,\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ \bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ }}}}}}}}}}}}}}}}}}} \rangle}\left(\bm{v}_{d}^{(t)},\bm{v}_{d} \rangle^{2}\bm{v}_{d}+\beta_{e}^{3}\langle\bm{w}_{j,\bm{\bm{\bm{\bm{\bm{\bm{\bm{ \bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ }}}}}}}}}}}}}}}}}} \rangle}\left(\bm{v}_{d}^{(t)},\bm{v}_{d} \rangle^{2}\bm{v}_{d}+y_{i}\langle\bm{w}_{j,\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ \bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bmbm{\bmbmbmbmbmbmbm{bmbmbmbmbmbmbm         }}}}}}}}}}}}}}}}}\right)\left(\bm{v}\bm{v}\bm{v}\bm{v}\bm{v}\bm{v}}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\{v}\}{v}\{v}\{v}\{v}\{v}\}\{v}\{v}\{v}\{v}\{v}\{v}\}{v}\{v}\{v}\{v}\{v}\{v}\}{v}\{v}\{v}\{v}\}{v}\{v}\{v}\{v}\{v}\}{v}\{v}\{v}\{v}\}{v}\{v}\{v}\{v}\{v}\}{v}\{v}\{v}\{v}\}{v}\{v}\{v}\{v}\}{v}\{v}\{v}\{v}\}{v}\{v}\{v}\{v}\}{v}\{v}\{v}\{v}\}{v}\{v}\{v}\{v}\{v}\}{v}\{v}\{v}\}{v}\{v}\{v}\{v}\}{v}\{v}\{v}\{v}\}{v}\{v}\{v}\{v}\{v}}\{v}\{v}\{v}\}{v}\{v}\{v}\{v}\{v}\}{v}\{v}\{v}\}{v}\{v}\{v}\}{v}\{v}\{v}\{v}\}{v}\{v}\{v}\}{\{v}\{v}}\{v}\{v}\{v}\{v}\}{v}\{v}\{v}\{v}\}{v}\{v}\{v}\{v}}{\v}\{v}\{v}\}{v}\{v}\{v}\{v}\}{\{v}}\{v}\{v}\{v}\}{v}\{v}\{v}\{v}\}{v}\{v}\{v}\}{\v}{\v}\{v}\{v}\{v}\}{\v}{\v}{v}\{v}\}{\v}\{v}\{v}\}{v}\{v}\{v}\{v}\}{\{v}\{v}\}{v}\{v}\{v}\}{\{v}}\{v}\{v}}{\v}\{v}\{v}\{v}\}{\v}{\v}\{v}\}{\v}{\v}{\v}\{v}{\v}\}{\{v}}\{v}\{v}{\v}\}{\{v}}\{v}\{v}\{v}\}{\v}{\v}{\v}{\v}\{}\{v}\{v}}{\v}\{v}{\}}{\v}{\{v}}\{v}\{v}\}{\v}{\v}{\v}\{v}\{v}\{v}}{\v}{\v}{\}{\v}\{v}}{\v}{\v}\{v}}{\{v}\}{\v}{\v}{\v}{\}}\{v}\{v}{\v}\{v}\{}\}{\v}{\v}\{v}{\v}{\v}\}{\v}{\{v}}\{\v}{\}}{\v}{\vFrom Equation 81 in the above proof, it can be seen clearly that amplifying the slow-learnable feature strength in either GD or SAM, i.e., increasing \(\beta_{d}\), places a larger weight on the slow-learnable feature. Thus, we have the next theorem.

**Theorem A.18** (Restatement of Theorem 3.5).: _Consider the training dataset \(D=\{(\bm{x}_{i},y_{i})\}_{i=1}^{N}\) that follows the data distribution \(\mathcal{D}(\beta_{e},\beta_{d},\alpha)\) using the two-layer nonlinear CNN model initialized with \(\bm{W}^{(0)}\sim\mathcal{N}(0,\sigma_{0}^{2})\). Assume that the noise is sufficiently small (ref. Lemmas B.4 and B.9) and the fast-learnable feature strength is significantly larger \(\alpha^{1/3}\beta_{e}>\beta_{d}\). We have the following results for one-step upsampling, i.e. increasing \(\beta_{d}\) from any iteration \(t\) during early training_

1. _The normalized gradient of the one-step SAM update has a larger weight on the slow-learnable feature compared to that of GD._
2. _Amplifying the slow-learnable feature strength puts a larger weight on the slow-learnable feature in the normalized gradients of GD and SAM._
3. _There exists an upsampling factor_ \(k\) _such that the normalized gradient of the one-step GD update on_ \(\mathcal{D}(\beta_{e},k\beta_{d},\alpha)\) _recovers the normalized gradient of the one-step SAM update on_ \(\mathcal{D}(\beta_{e},\beta_{d},\alpha)\)_._

Proof.: The first result has already been proved in Lemma A.17. Now, consider increasing the slow-learnable feature strength from \(\beta_{d}\) to \(\beta_{d}^{\prime}\). Similar to the proof of Corollary A.17, to verify that the normalized of the new normalized gradient of GD favors the slow-learnable feature, it is sufficient to show

\[\frac{(\beta_{d}^{\prime})^{3}\langle\bm{w}^{(t)},\bm{v}_{d}\rangle^{2}}{ \beta_{e}^{3}\langle\bm{w}^{(t)},\bm{v}_{e}\rangle^{2}}\geq\frac{\beta_{d}^{ 3}\langle\bm{w}^{(t)},\bm{v}_{d}\rangle^{2}}{\beta_{e}^{3}\langle\bm{w}^{(t)}, \bm{v}_{e}\rangle^{2}}\] (86)

which is trivial because \(\beta_{d}^{\prime}>\beta_{d}\). Similarly, we can verify the result for SAM. Now, let's find the new coefficient \(\beta_{d}^{\prime}=k\beta_{d}(k>1)\) such that training one-step GD on \(\mathcal{D}(\beta_{e},\beta_{d}^{\prime},\alpha)\) can recover the normalized gradient of the one-step SAM update on the original data distribution \(\mathcal{D}(\beta_{e},\beta_{d},\alpha)\). Using Equation 81, we have

\[\frac{\beta_{d}^{3}\langle\bm{w}_{\bm{\epsilon}}^{(t)},\bm{v}_{d }\rangle^{2}}{\beta_{e}^{3}\langle\bm{w}_{\bm{\epsilon}}^{(t)},\bm{v}_{e} \rangle^{2}} =\frac{(\beta_{d}^{\prime})^{3}\langle\bm{w}^{(t)},\bm{v}_{d} \rangle^{2}}{\beta_{e}^{3}\langle\bm{w}^{(t)},\bm{v}_{e}\rangle^{2}}\] (87) \[\frac{\langle\bm{w}_{\bm{\epsilon}}^{(t)},\bm{v}_{d}\rangle}{ \langle\bm{w}_{\bm{\epsilon}}^{(t)},\bm{v}_{e}\rangle} =k^{3/2}\frac{\langle\bm{w}^{(t)},\bm{v}_{d}\rangle}{\langle\bm{w} ^{(t)},\bm{v}_{e}\rangle}\] (88) \[\frac{\langle\bm{w}^{(t)},\bm{v}_{d}\rangle-3\rho^{(t)}\beta_{d}^ {3}\langle\bm{w}^{(t)},\bm{v}_{d}\rangle^{2}}{\langle\bm{w}^{(t)},\bm{v}_{e} \rangle-3\rho^{(t)}\alpha\beta_{e}^{3}\langle\bm{w}^{(t)},\bm{v}_{e}\rangle^{2}} =k^{3/2}\frac{\langle\bm{w}^{(t)},\bm{v}_{d}\rangle}{\langle\bm{w} ^{(t)},\bm{v}_{e}\rangle}\] (89) \[k^{3/2} =\frac{1-3\rho^{(t)}\beta_{d}^{3}\langle\bm{w}^{(t)},\bm{v}_{d} \rangle}{1-3\rho^{(t)}\alpha\beta_{e}^{3}\langle\bm{w}^{(t)},\bm{v}_{e}\rangle}\] (90) \[k =\left(\frac{1-3\rho^{(t)}\beta_{d}^{3}\langle\bm{w}^{(t)},\bm{v} _{d}\rangle}{1-3\rho^{(t)}\alpha\beta_{e}^{3}\langle\bm{w}^{(t)},\bm{v}_{e} \rangle}\right)^{2/3}.\] (91)

Therefore, with \(\beta_{d}^{\prime}=\left(\frac{1-3\rho^{(t)}\beta_{d}^{3}\langle\bm{w}^{(t)}, \bm{v}_{d}\rangle}{1-3\rho^{(t)}\alpha\beta_{e}^{3}\langle\bm{w}^{(t)},\bm{v} _{e}\rangle}\right)^{2/3}\beta_{d}\), the normalized gradient of the one-step GD update on \(\mathcal{D}(\beta_{e},\beta_{d}^{\prime},\alpha)\) is similar to that of the one-step SAM update on \(\mathcal{D}(\beta_{e},\beta_{d},\alpha)\). 

## Appendix B Auxiliary Lemmas

**Lemma B.1** (Claim D.20, [2]).: _Considering an increasing sequence \(x_{t}\geq 0\) defined as \(x_{t+1}=x_{t}+\eta C_{t}(x_{t}-\rho)^{2}\) for some \(C_{t}=\Theta(1),0\leq\rho\leq x_{0}\), then we have for every \(A>x_{0}\), every \(\delta\in(0,1)\), and every \(\eta\in(0,1]\):_

\[\sum_{t\geq 0,x_{t}\leq A}\eta C_{t} \leq\frac{1+\delta}{x_{0}}+\frac{O(\eta(A-\rho)^{2})}{x_{0}^{2}} \frac{\log(A/x_{0})}{\log(1+\delta)}\] (92) \[\sum_{t\geq 0,x_{t}\leq A}\eta C_{t} \geq\frac{1-\frac{(1+\delta)x_{0}}{A}}{x_{0}(1+\delta)}-\frac{O( \eta(A-\rho)^{2})}{x_{0}^{2}}\frac{\log(A/x_{0})}{\log(1+\delta)}\] (93)Proof.: For every \(g=0,1,\ldots\), let \(T_{g}\) be the first iteration such that \(x_{t}\geq(1+\delta)^{g}x_{0}\). Let \(b\) be the smallest integer such that \((1+\delta)^{b}x_{0}\geq A\). Suppose for notation simplicity that we replace \(x_{t}\) with exactly \(A\) whenever \(x_{t}\geq A\). By the definition of \(T_{g}\), we have

\[\sum_{t\in[T_{g},T_{g+1})}\eta C_{t}[(1+\delta)^{g}x_{0}]^{2} \leq x_{T_{g+1}}-x_{T_{g}}\leq\delta(1+\delta)^{g}x_{0}+O(\eta(A-\rho)^{2})\] (94) \[\sum_{t\in[T_{g},T_{g+1})}\eta C_{t}[(1+\delta)^{g+1}x_{0}]^{2} \geq x_{T_{g+1}}-x_{T_{g}}\geq\delta(1+\delta)^{g}x_{0}-O(\eta(A-\rho)^{2})\] (95)

These imply that

\[\sum_{t\in[T_{g},T_{g+1})}\eta C_{t} \leq\frac{\delta}{(1+\delta)^{g}x_{0}}+\frac{O(\eta(A-\rho)^{2}) }{x_{0}^{2}}\] (97) \[\sum_{t\in[T_{g},T_{g+1})}\eta C_{t} \geq\frac{\delta}{(1+\delta)^{g+2}x_{0}}-\frac{O(\eta(A-\rho)^{2} )}{x_{0}^{2}}\] (98)

Recall \(b\) is the smallest integer such that \((1+\delta)^{b}x_{0}\geq A\), so we can calculate

\[\sum_{t\geq 0,x_{t}\leq A}\eta C_{t} \leq\sum_{g=0}^{b-1}\frac{\delta}{(1+\delta)^{g}x_{0}}+\frac{O( \eta(A-\rho)^{2})}{x_{0}^{2}}b\] (99) \[=\frac{\delta}{1-\frac{1}{1+\delta}}\frac{1}{x_{0}}+\frac{O(\eta( A-\rho)^{2})}{x_{0}^{2}}b\] (100) \[=\frac{1+\delta}{x_{0}}+\frac{O(\eta(A-\rho)^{2})}{x_{0}^{2}} \frac{\log(A/x_{0})}{\log(1+\delta)}\] (101) \[\sum_{t\geq 0,x_{t}\leq A}\eta C_{t} \geq\sum_{g=0}^{b-2}\frac{\delta}{(1+\delta)^{g+2}x_{0}}-\frac{O (\eta(A-\rho)^{2})}{x_{0}^{2}}b\] (102) \[=\frac{\delta(1+\delta)^{-1}(1-\frac{1}{(1+\delta)^{(b-1)}})}{1- \frac{1}{1+\delta}}\frac{1}{x_{0}}-\frac{O(\eta(A-\rho)^{2})}{x_{0}^{2}}b\] (103) \[=\frac{1-\frac{(1+\delta)x_{0}}{A}}{x_{0}(1+\delta)}-\frac{O( \eta(A-\rho)^{2})}{x_{0}^{2}}\frac{\log(A/x_{0})}{\log(1+\delta)}\] (104)

Thus, the two desired inequalities are proved. 

**Lemma B.2** (Lemma D.19, [2].).: _Let \(\{x_{t},y_{t}\}_{t=1,\ldots}\) be two positive sequences that satisfy_

\[x_{t+1}\geq x_{t}+\eta\cdot C_{t}(x_{t}-\rho)^{2},\] \[y_{t+1}\leq y_{t}+S\eta\cdot C_{t}y_{t}^{2},\]

_for some \(C_{t}=\Theta(1)\). Suppose \(x_{0}\geq y_{0}S\frac{1+2G}{1-3G}\) where \(S\in(0,1),G\in(0,1/3)\) and \(0<\eta\leq\min\{\frac{G^{2}x_{0}}{\log(A/x_{0})},\frac{G^{2}y_{0}}{\log(1/G)} \},0\leq\rho<O(x_{0})\), and for all \(A\in(x_{0},O(1)]\), let \(T_{x}\) be the first iteration such that \(x_{t}\geq A\). Then, we have \(y_{T_{x}}\leq O(G^{-1}y_{0})\)._

Proof.: Let \(T_{x}\) be the first iteration \(t\) in which \(x_{t}\geq A\). Apply Lemma B.1 for the \(x_{t}\) sequence with \(C_{t}=C_{t}\) and threshold \(A\), we have

\[\sum_{t=0}^{T_{x}}\eta C_{t} \leq\frac{1+\delta}{x_{0}}+\frac{O(\eta(A-\rho)^{2})}{x_{0}^{2}} \frac{\log(A/x_{0})}{\log(1+\delta)}\] (105) \[=\frac{1+\delta}{x_{0}}+O\left(\frac{\eta(A-\rho)^{2}\log(A/x_{0 })}{\delta x_{0}^{2}}\right)\] (106) \[\leq\frac{1+\delta}{x_{0}}+O\left(\frac{\eta\log(A/x_{0})}{ \delta x_{0}^{2}}\right)\] (107)Let \(T_{y}\) be the first iteration \(t\) in which \(y_{t}\geq A\). Apply Lemma B.1 for the \(y_{t}\) sequence with \(\eta=S\eta,C_{t}=C_{t},\rho=0\) and threshold \(A^{\prime}=G^{-1}y_{0}\), we have

\[\sum_{t=0}^{T_{y}}S\eta C_{t} \geq\frac{1-\frac{(1+\delta)y_{0}}{A^{\prime}}}{y_{0}(1+\delta)}- \frac{O(S\eta(A^{\prime})^{2})}{y_{0}^{2}}\frac{\log(A^{\prime}/y_{0})}{\log(1+ \delta)}\] (108) \[\geq\frac{1-O(\delta+G)}{y_{0}}-O\left(\frac{S\eta(A^{\prime})^{2 }\log(1/G)}{\delta y_{0}^{2}}\right)\] (109) \[\geq\frac{1-O(\delta+G)}{y_{0}}-O\left(\frac{S\eta\log(1/G)}{ \delta y_{0}^{2}}\right)\] (110)

Compare Equation 107 and 110. Choosing \(\delta=G\) and \(\eta\leq\min\{\frac{G^{2}x_{0}}{\log(A/x_{0})},\frac{G^{2}y_{0}}{\log(1/G)}\}\), together with \(x_{0}\geq y_{0}S\frac{1+2G}{1-3G}\) we have \(T_{x}\leq T_{y}\). 

**Lemma B.3** (Lemma K.15, [32].).: _Let \(\{z_{t}\}_{t=0}^{T}\) be a positive sequence defined by the following recursions_

\[z_{t+1} \geq z_{t}+m(z_{t}-\rho)^{2},\] \[z_{t+1} \leq z_{t}+M(z_{t})^{2},\]

_where \(z_{0}>\rho\geq 0\) is the initialization and \(m,M>0\) are some constants. Let \(v>z_{0}\), then the time \(T_{v}\) such that \(z_{T_{v}}\geq v\) for all \(t\geq T_{v}\) is_

\[T_{v}=\frac{2z_{0}}{m(z_{0}-\rho)^{2}}+\frac{4Mz_{0}^{2}}{m(z_{0}-\rho)^{2}} \Big{\lceil}\frac{\log(v/z_{0})}{\log(2)}\Big{\rceil}.\] (111)

Proof.: Let \(n\in\mathbb{N}^{\star}\). Let \(T_{n}\) be the first time that \(z_{t}\geq 2^{n}z_{0}\). We want to find an upper bound of \(T_{n}\). We start with the case \(n=1\). By summing the recursion, we have:

\[z_{T_{1}}\geq z_{0}+m\sum_{t=0}^{T_{1}-1}(z_{t}-\rho)^{2}\] (112)

Because \(z_{t}\geq z_{0}\), we obtain

\[T_{1}\leq\frac{z_{T_{1}}-z_{0}}{m(z_{0}-\rho)^{2}}\] (113)

Now, we want to bound \(z_{T_{1}}-z_{0}\). Using again the recursion and \(z_{T_{1}-1}\leq 2z_{0}\), we have

\[z_{T_{1}}\leq z_{T_{1}-1}+M(z_{T_{1}-1})^{2}\leq 2z_{0}+4Mz_{0}^{2}.\] (114)

Combining Equation 113 and 114, we get a bound on \(T_{1}\) as

\[T_{1}\leq\frac{z_{0}+4Mz_{0}^{2}}{m(z_{0}-\rho)^{2}}=\frac{z_{0}}{m(z_{0}-\rho )^{2}}+\frac{4Mz_{0}^{2}}{m(z_{0}-\rho)^{2}}\] (115)

Now, let's find a bound for \(T_{n}\). Starting from the recursion and using the fact that \(z_{t}\geq 2^{n-1}z_{0}\) for \(t\geq T_{n-1}\) we have

\[z_{T_{n}} \geq z_{T_{n-1}}+m\sum_{t=T_{n-1}}^{T_{n}-1}(z_{t}-\rho)^{2}\] (116) \[\geq z_{T_{n-1}}+(2^{n-1})^{2}m(z_{0}-\rho)^{2}(T_{n}-T_{n-1})\] (117)

On the other hand, by using \(z_{T_{n}-1}\leq 2^{n}z_{0}\) we upper bound \(z_{T_{n}}\) as

\[z_{T_{n}}\leq z_{T_{n}-1}+M(z_{T_{n}-1})^{2}\leq 2^{n}z_{0}+M2^{2n}z_{0}^{2}\] (118)

Besides, we know that \(z_{T_{n-1}}\geq 2^{n-1}z_{0}\). Therefore, we upper bound \(z_{T_{n}}-z_{T_{n-1}}\) as

\[z_{T_{n}}-z_{T_{n-1}}\leq 2^{n-1}z_{0}+M2^{2n}z_{0}^{2}\] (119)Combining Equations 116 and 119 yields

\[T_{n} \leq T_{n-1}+\frac{2^{n-1}z_{0}+M2^{2n}z_{0}^{2}}{(2^{n-1})^{2}m(z_{ 0}-\rho)^{2}}\] (120) \[=T_{n-1}+\frac{z_{0}}{2^{n-1}m(z_{0}-\rho)^{2}}+\frac{4Mz_{0}^{2}} {m(z_{0}-\rho)^{2}}\] (121)

Summing Equation 120 for \(n=2,\ldots,n\) we have

\[T_{n} \leq\sum_{i=1}^{n}\frac{z_{0}}{2^{i-1}m(z_{0}-\rho)^{2}}+\frac{4 Mnz_{0}^{2}}{m(z_{0}-\rho)^{2}}\leq\frac{2z_{0}}{m(z_{0}-\rho)^{2}}+\frac{4Mnz_ {0}^{2}}{m(z_{0}-\rho)^{2}}\] (122)

Lastly, we know that \(2^{n}z_{0}\geq v\) this implies that we can set \(n=\left\lceil\frac{\log(v/z_{0})}{\log(2)}\right\rceil\) in Equation 122. 

We make the following assumptions for every \(t\leq T\) as the same in [32].

**Lemma B.4** (Induction hypothesis D.1, [32]).: _Throughout the training process using GD for \(t\leq T\), we maintain that, for every \(i\) and \(j\in[J]\),_

\[|\langle\bm{w}_{j}^{(t)},\bm{\xi}_{i}\rangle|\leq\tilde{O}(\sigma _{0}\sigma_{p}\sqrt{d}).\] (123)

**Lemma B.5** (Lemma G.4, [18]).: _For every \(i\), we have \(l_{i}^{(t)}=\Theta(1)g_{1}(t)\), where_

\[g_{1}(t)=\text{sigmoid}\left(\sum_{j=1}^{J}-\beta_{d}^{3}\langle \bm{w}_{j}^{(t)},\bm{v}_{d}\rangle^{3}-\beta_{e}^{3}\langle\bm{w}_{j}^{(t)}, \bm{v}_{e}\rangle^{3}\right).\] (124)

**Lemma B.6** (Lemma K.5, [32]).: _Let \(X\in\mathbb{R}^{d}\) be a Gaussian random vector, \(X\sim\mathcal{N}(0,\sigma^{2}I_{d})\). Then with probability at least \(1-o(1)\), we have \(\|X\|_{2}^{2}=\Theta(\sigma^{2}\sqrt{d})\)._

**Lemma B.7** (Lemma K.7, [32]).: _Let \(X\) and \(Y\) be independent Gaussian random vectors on \(\mathbb{R}^{d}\) and \(X\sim\mathcal{N}(0,\sigma^{2}\bm{I}_{d})\), \(Y\sim\mathcal{N}(0,\sigma_{0}^{2}\bm{I}_{d})\). Assume that \(\sigma\sigma_{0}\leq\frac{1}{d}\). Then, with probability at least \(1-\delta\), we have_

\[|\langle X,Y\rangle|\leq\sigma\sigma_{0}\sqrt{2d\log\frac{2}{ \delta}}\]

**Lemma B.8** (Bound on noise inner products).: _Let \(N=O(poly(d))\). The following hold with probability at least \(1-o(1)\):_

\[\max\left\{|\langle\bm{w}_{j,\bm{\epsilon}}^{(0)},\bm{\xi}_{i} \rangle|\right\}=\tilde{O}(\sigma\sigma_{0}\sqrt{d})\] \[\max_{i}\left\{\frac{1}{n}\sum_{k=1}^{n}|\langle\bm{\xi}_{k},\bm {\xi}_{i}\rangle|\right\}=\tilde{O}(\frac{\sigma^{2}d}{N}+\sigma^{2}\sqrt{d})\]

Proof.: For the first inequality, Lemma B.7 implies that with probability at least \(1-\frac{1}{dN}\),

\[|\langle\bm{w}_{j,\bm{\epsilon}}^{(0)},\bm{\xi}_{i}\rangle|\leq \sigma\sigma_{0}\sqrt{2d\log(\frac{2}{dN})}=\tilde{O}(\sigma\sigma_{0}\sqrt{d})\] (125)

Taking a union bound over \(n=1,\ldots,N\) gives the result.

The second statement is proved similarly. 

**Lemma B.9** (Bound on the noise component for SAM).: _Assume that \(\rho=o(\sigma_{0})\) and \(\omega(1)\leq N\leq O(poly(d))\). Throughout the training process using SAM for \(t\leq T\), we maintain that, for every \(i\) and \(j\in[J]\),_

\[|\langle\bm{w}_{j,\bm{\epsilon}}^{(t)},\bm{\xi}_{i}\rangle|\leq \tilde{O}(\sigma\sigma_{0}\sqrt{d})\] (126)Proof.: Let \(\chi_{t}=\max\{|\langle\bm{w}_{j,\bm{\xi}_{i}}^{(t)},\bm{\xi}_{i}\rangle|\}\), \(\alpha=\max_{i}\{\frac{1}{n}\sum_{k=1}^{n}|\langle\bm{\xi}_{k},\bm{\xi}_{i} \rangle|\}\). Combined with \(l_{k,\epsilon}^{(t)}\leq 1\), the noise gradient update rule can be bounded as

\[\chi_{t+1}\leq\chi_{t}+3\eta\alpha\chi_{t}^{2}\]

Suppose that \(a(t)\) satisfies the differential equation

\[a^{\prime} =3\alpha\eta a^{2}\] \[a(0) =\chi_{0}\]

Observe that \(a(t)\) is increasing so, by the Mean Value Theorem there exists \(\tau\in(t,t+1)\) such that

\[a(t+1)-a(t) =a^{\prime}(\tau)\] \[=3\alpha\eta a(\tau)^{2}\] \[\geq 3\alpha\eta a(t)^{2}\]

So an fast-learnable induction shows that \(a(t)\geq\chi_{t}\).

Now solving for \(a(t)\),

\[a(t)=\frac{1}{\frac{1}{a(0)}-3\alpha\eta t},\qquad t\leq\frac{1}{3\alpha\eta a (0)}.\]

Using the high probability tail bounds B.8,

\[a(0) =\chi_{0}=\tilde{O}(\sigma\sigma_{0}\sqrt{d})\] \[\alpha =\tilde{O}(\frac{\sigma^{2}d}{N}+\sigma^{2}\sqrt{d})\]

where \(\sigma=\frac{\sigma_{p}}{\sqrt{d}}\). Substituting these bounds gives

\[a(t)=\frac{1}{\tilde{O}(\frac{1}{\sigma\sigma_{0}\sqrt{d}})-\tilde{O}(\eta t (\frac{\sigma^{2}d}{N}+\sigma^{2}\sqrt{d}))}\]

Now Theorem A.15 and \(\rho=o(\sigma_{0})\) implies that \(\eta T_{0}=\tilde{\Theta}(\frac{1}{\sigma_{0}\beta_{0}^{3}})\). Combined with the assumption that \(N=\Omega(1)\), the second term in the denominator is of lower order than the first term, so

\[a(T_{0})=\tilde{O}(\sigma\sigma_{0}\sqrt{d}).\]

We conclude that

\[|\langle\bm{w}_{j}^{(t)},\bm{\xi}_{i}\rangle|\leq\tilde{O}(\sigma_{0}\sigma_{ p}\sqrt{d}).\] (127)

**Lemma B.10**.: _For every \(i\), we have \(l_{i}^{(t)}=\Theta(1)g_{1}(t)\) and \(l_{i,\bm{\epsilon}}^{(t)}=\Theta(1)g_{2}(t)\), where_

\[g_{1}(t) =\text{sigmoid}\left(\sum_{j=1}^{J}-\beta_{d}^{3}\langle\bm{w}_{ j}^{(t)},\bm{v}_{d}\rangle^{3}-\beta_{e}^{3}\langle\bm{w}_{j}^{(t)},\bm{v}_{e} \rangle^{3}\right),\] (128) \[g_{2}(t) =\text{sigmoid}\left(\sum_{j=1}^{J}-\beta_{d}^{3}\langle\bm{w}_{ j,\bm{\epsilon}}^{(t)},\bm{v}_{d}\rangle^{3}-\beta_{e}^{3}\langle\bm{w}_{j,\bm{ \epsilon}}^{(t)},\bm{v}_{e}\rangle^{3}\right).\] (129)

The proof is the same as [18][Lemma G.4].

## Appendix C Additional Experimental Settings

### Datasets and Training Details

**Datasets.** The CIFAR10 dataset [41] consists of 60,000 32 x 32 color images in 10 classes, with 6000 images per class. The CIFAR100 dataset [41] is just like the CIFAR10, except it has 100 classes containing 600 images each. For both of these datasets, the training set has 50,000 images (5,000 per class for CIFAR10 and 500 per class for CIFAR100) with the test set having 10,000 images. CINIC10 [16] represents an image classification dataset consisting of 270,000 images, which is 4.5 times larger than CIFAR10. The dataset is created by merging CIFAR10 with images extracted from the ImageNet database, specifically selecting and downsampling images from the same 10 classes present in CIFAR10. Tiny-ImageNet [43] comprises 100,000 images distributed across 200 classes of ImageNet [17], with each class containing 500 images. These images have been resized to 64x64 dimensions and are in color. The dataset consists of 500 training images, 50 validation images, and 50 test images per class. The STL10 dataset [13] includes 5000 96x96 training labeled images, 500 per CIFAR10 class. The test set consists of 800 images per class, this counts up to 8,000 images in total.

**Training on different datasets.** Follow the setting from [3; 4], we trained Pre-Activation ResNet18 on all datasets except for CIFAR100 which was trained with ResNet34. We trained our models for 200 epochs with a batch size of 128 and used basic data augmentations such as random mirroring and random crop. We used SGD with the momentum parameter of 0.9 and set weight decay to 0.0005. We also fixed \(\rho=0.1\) for SAM unless further specified. For all datasets, we used a learning rate schedule where we set the initial learning rate to 0.1. The learning rate is decayed by a factor of 10 after 50% and 75% epochs, i.e., we set the learning rate to 0.01 after 100 epochs and to 0.001 after 150 epochs.

**Training with different architectures.** We used the same training procedures for Pre-Activation ResNet18, VGG19, and DenseNet121. We directly used the official Pytorch [54] implementation for VGG19 and DenseNet121. For 3-layer MLPs, we used a hidden size of 512 with a dropout of 0.1 to avoid overfitting and set \(\rho=0.01\). For ViT-S [77], we adopted a Pytorch implementation at https://github.com/lucidrains/vit-pytorch. In particular, the hidden size, the depth, the number of attention heads, and the MLP size are set to 768, 8, 8, and 2304, respectively. We adjusted the patch size to 4 to fit the resolution of CIFAR10 and set both the initial learning rate and \(\rho\) to 0.01.

**Computational resources.** Each model is trained on 1 NVIDIA RTX A5000 GPU.

### Other Implementation Details

**When to separate the examples?** We selected the best-separating epoch \(t\) in the set of \(\{4,5,6,7,8,10\}\) for CIFAR10 and \(\{12,14,16,18,20,22\}\) for CIFAR100. Particularly, we separated examples of CIFAR10 at around epoch 8 while that of CIFAR100 is near epoch 20. Near this point, the gain in training error diminishes significantly as shown in Figure 14(a), which shows a sign that the model successfully learns fast-learnable features. In addition, we reported the results for different separating epochs in Appendix D.8. In Figure 4, the best separating epochs for STL10, CINIC10, and Tiny-ImageNet are 11, 4, and 10, respectively. The separating epoch for Waterbirds is 5.

**Forgetting score.** To compute forgetting scores of training examples in each dataset, we collected the same statistics as in [69] but computed at the end of each epoch. The reason is to make the statistics consistent between two versions of the same slow-learnable example which is repeated in the upsampled dataset.

**Hessian spectra.** We approximated the density of the Hessian spectrum using the Lanczos algorithm [24; 53]. The Hessian matrix is approximated by 1000 examples (100 per class of CIFAR10). Then we extract the top eigenvalues to calculate the maximum Hessian eigenvalue \((\lambda_{\text{max}})\) and the bulk of spectra \((\lambda_{\text{max}}/\lambda_{5})\)[30].

Figure 7: USEFUL first trains the model for a few epochs \(t\), which in practice is around 5-10% of the total training epochs. It then clusters examples in every class into 2 groups and upsamples the cluster with higher average loss. Finally, the base model is retrained from scratch on the modified data distribution.

## Appendix D Additional Results

### An Extreme Case of Toy Datasets

We consider an extreme setting when two features have identical strength and no missing fast-learnable features, i.e., \(\beta_{e}=\beta_{d}=\alpha=1\), the gap between LHS and RHS in Equation 5 is not small as shown in the figure 8. The gap is consistently around 0.2-0.3 from epoch 250 onwards.

### USEFUL is Useful for MLP

Figure 9 shows that for 3-layer MLP, USEFUL successfully reduces the test error of both SGD and SAM by nearly 1%. Additionally, SGD+USEFUL yields better performance than SAM alone.

Figure 8: The gap between contribution of fast-learnable and slow-learnable features towards the model output in SAM and GD. The toy datasets is generated from the distribution in Definition 3.1 with \(\beta_{d}=\beta_{e}=\alpha=1\).

Figure 10: **L1 norm of ResNet18 trained on CIFAR10 and ResNet34 trained on CIFAR100.** Lower L1 norm indicates a sparser solution and stronger implicit regularization properties [3, Section 4.2]. SAM has a lower L1 norm than SGD, and USEFUL further reduces the L1 norm of SGD and SAM.

Figure 9: **Test classification errors of 3-layer MLP on CIFAR10.** The number below each bar indicates the estimated cost to train the model and the tick on top shows the standard deviation over three runs. USEFUL improves the performance of SGD and SAM when training with 3-layer MLP.

### SAM & USEFUL Reduce Forgetting Scores

We used forgetting scores [69] to partition examples in CIFAR10 into different groups. Forgetting scores count the number of times an example is misclassified after being correctly classified during training and is an indicator of the learning-speed of examples. Figure 11 illustrates that SGD+USEFUL and SAM have fewer examples with high forgetting scores than SGD does. This aligns with our theoretical analysis in Theorem 3.4 and results on the toy datasets. By upsampling slow-learnable examples in the dataset, they contribute more to learning and hence SGD+USEFUL learns slow-learnable features faster than SGD.

Furthermore, Tables 2 and 3 illustrate the average forgetting score, first learned iteration (i.e., at this epoch, the model predicts correctly for the first time) and iteration learned (i.e., after this epoch, the prediction is always correct) of examples in fast-learnable and slow-learnable clusters. Iteration learned is highly correlated with prediction depth [5], which is another notion of data difficulty. It can be seen clearly that examples in fast-learnable clusters have a lower difficulty score for every metric, indicating that USEFUL successfully identifies fast-learnable examples early in training.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Metric & SGD & SGD+USEFUL & SAM \\ \hline \(\lambda_{\text{max}}\) & 53.8 & 41.8 & 12.4 \\ \(\lambda_{\text{max}}/\lambda_{5}\) & 3.8 & 1.5 & 2.4 \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Sharpness of solution at convergence.** We train ResNet18 on CIFAR10 and measure the maximum Hessian eigenvalue \(\lambda_{\text{max}}\) and the bulk spectra measured as \(\lambda_{\text{max}}/\lambda_{5}\).

\begin{table}
\begin{tabular}{l|c c} \hline \hline Metric & Fast-learnable & Slow-learnable \\ \hline Forgetting score & 3.8 \(\pm\) 6.1 & 14.7 \(\pm\) 9.0 \\ \hline First learned iteration & 0.9 \(\pm\) 1.2 & 3.7 \(\pm\) 8.0 \\ \hline Iteration learned & 45.6 \(\pm\) 50.0 & 105.8 \(\pm\) 41.0 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Average scores for two clusters on CIFAR10.

Figure 11: **Forgetting scores for training ResNet18 on CIFAR10.** Forgetting scores measure the learning speed of examples in training data. USEFUL approaches the training dynamics of SAM, with more examples being forgotten infrequently and fewer examples being forgotten frequently.

\begin{table}
\begin{tabular}{l|c c} \hline \hline Metric & Fast-learnable & Slow-learnable \\ \hline Forgetting score & 10.2 \(\pm\) 8.6 & 16.8 \(\pm\) 7.4 \\ \hline First learned iteration & 4.7 \(\pm\) 6.8 & 9.8 \(\pm\) 13.7 \\ \hline Iteration learned & 86.6 \(\pm\) 46.1 & 115.7 \(\pm\) 31.0 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Average scores for two clusters on CIFAR100.

### USEFUL generalizes to other SAM variants

In this experiment, we show that SAM can also generalize to other variants of SAM. We chose ASAM, which is proposed by Kwon et al. to address the sensitivity of parameter re-scaling [19]. Following the recommended settings in ASAM, we trained it with a perturbation radius \(\rho=1.0\), which is 10 times that of SAM. Other settings are identical to the standard settings in Appendix C. Table 4 demonstrates the results for training ResNet18 on CIFAR10. Both SAM and ASAM can be combined with USEFULto improve the test classification error. When using TA, ASAM shows a slightly better performance than SAM.

### USEFUL shows promising results for the OOD settings

While our main contribution is providing a novel and effective method to improve the in-distribution generalization performance, we conducted new experiments confirming the benefits of our method to improving out-of-distribution (OOD) generalization performance. As a few very recent works [9; 72] showed the benefit of SAM in improving OOD performance, it is expected that USEFUL also extend to this setting.

**Spurious correlation (Waterbirds).** As can be seen in Figure 12, both SAM and USEFUL effectively improve the performance on the balanced test set though the model is trained on the spurious training

\begin{table}
\begin{tabular}{l c c} \hline \hline  & SAM & ASAM \\ \hline + & 4.23 \(\pm\) 0.08 & 4.33 \(\pm\) 0.19 \\ + USEFUL & **4.04 \(\pm\) 0.06** & **4.09 \(\pm\) 0.10** \\ \hline + TA & 4.06 \(\pm\) 0.08 & 3.93 \(\pm\) 0.11 \\ + TA + USEFUL & **3.49 \(\pm\) 0.09** & **3.46 \(\pm\) 0.01** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Test classification errors for training SAM and ASAM on the original CIFAR10 and modified datasets by USEFUL. Results are averaged over 3 seeds.

Figure 12: **Comparing test classification errors on Waterbirds. The number below each bar indicates the approximated cost to train the model and the tick on top shows the standard deviation over three runs. USEFUL boosts the performance of SGD and SAM on the balanced test set, showing its generalization to the OOD setting. In addition, the success of USEFUL in fine-tuning reveals its new application to the transfer learning setting.**

\begin{table}
\begin{tabular}{l c c|c c} \hline \hline Method & \multicolumn{2}{c}{SGD} & \multicolumn{2}{c}{SAM} \\ \cline{2-5}  & CIFAR10 & CIFAR100 & CIFAR10 & CIFAR100 \\ \hline Upweighting loss & 5.33 \(\pm\) 0.09 & 26.70 \(\pm\) 3.25 & 4.28 \(\pm\) 0.02 & 21.75 \(\pm\) 0.25 \\ \hline USEFUL & **4.79 \(\pm\) 0.05** & **22.58 \(\pm\) 0.08** & **4.04 \(\pm\) 0.06** & **20.40 \(\pm\) 0.36** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparison between USEFUL and upweighting loss regarding test classification errors. Upweighting loss doubled the loss for all examples in the slow-learnable clusters found by USEFUL, which is different from dynamically upweighting examples during the training [78]. Results are averaged over 3 seeds.

set. When training ResNet18 with SGD from scratch, USEFUL decreases the classification errors by 5.8%. In addition, it can be successfully applied to the pre-trained ResNet50 (on ImageNet), opening up a promising application to the transfer learning setting.

**Long-tail distribution (Long-tail CIFAR10).** We conducted new experiments on long-tail CIFAR10 [15] with an imbalance ratio of 10. Table 6 shows that USEFUL can also improve the performance of SGD and SAM, by reducing the simplicity bias on the long-tail data. Figure 13 visualizes the distribution of classes before and after upsampling by USEFUL. Interestingly, we see that USEFUL upsamples more examples from some of the larger classes and still improves the accuracy on the balanced test set. This improvement is attributed to the more uniform speed of feature learning, and not balancing the training data distribution. Notably, USEFUL outperforms class balancing to address long tail distribution. Besides, USEFUL can be stacked with methods to address long-tail data to further improve performance, as we confirmed in the last row.

### USEFUL is also effective for noisy label data

Our method and analysis consider a clean dataset. But, as we confirmed in Table 8, USEFUL can easily stack with MixUp [79], a robust method for learning against noisy labels, to reduce the simplicity bias and improve their performance. Applying USEFUL on top of MixUp to CIFAR10 with 10-20% random label flip successfully boosts the performance of both SGD and SAM when training on data with corrupt labels.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Ratio & SGD & SGD+USEFUL & SAM & SAM+USEFUL \\ \hline
1:10 & 10.01 \(\pm\) 0.21 & 9.53 \(\pm\) 0.13 & 8.85 \(\pm\) 0.08 & 8.22 \(\pm\) 0.04 \\ Balancing & 9.77 \(\pm\) 0.17 & 9.25 \(\pm\) 0.11 & 8.31 \(\pm\) 0.11 & 7.93 \(\pm\) 0.02 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Test error on long-tailed CIFAR10. Balancing means that we upsampled small classes to make the classes balanced. Results are averaged over 3 seeds.

Figure 13: Class count before and after upsampling by USEFUL on long-tail CIFAR10 dataset.

\begin{table}
\begin{tabular}{l c c} \hline \hline Partition method & CIFAR10 & CIFAR100 \\ \hline Quantile & 5.27 \(\pm\) 0.10 & 23.49 \(\pm\) 0.82 \\ \hline Misclassification & 4.98 \(\pm\) 0.17 & 23.86 \(\pm\) 0.70 \\ \hline USEFUL & **4.79 \(\pm\) 0.05** & **22.58 \(\pm\) 0.08** \\ \hline \hline \end{tabular}
\end{table}
Table 7: **Test classification errors of SGD for different partition methods. Results are averaged over 3 seeds.**

### Comparison with simplicity bias mitigation methods

While previous works show that reducing simplicity bias benefits the OOD settings, we show that reducing the simplicity bias also benefits the ID settings. To confirm our hypothesis on our simplicity bias mitigation baselines, we applied EIIL [14] and JTT [48] to train ResNet18 on CIFAR10. The choice of the baselines is because they have publicly available code and fewer hyperparameters to tune in our limited rebuttal time. For EIIL, we tuned lr \(\in\){1e-1, 1e-2, 1e-3, 5e-4, 1e-4}, number of epochs \(\in\){1, 2, 4, 8} for training the reference model, and the weight decay \(\in\){1e-3, 5e-4, 1e-4} for training GroupDRO. For JTT, we tuned the separating epoch \(\in\){4, 5, 6, 7, 8, 10} and upsampling factor \(\in\){2, 3, 4, 5}, while lr and weight decay follow the standard training of ResNet18 on CIFAR10. Table 9 shows that all methods successfully reduce the simplicity bias, yielding an improvement over SGD. While EIIL requires tuning 3 hyperparameters with a total of 60 combinations, USEFUL only requires one hyperparameter, which is the separating epoch within a small range (around the time when the slope of the training loss curve diminishes).

\begin{table}
\begin{tabular}{c c c c} \hline \hline Rate & SGD & SGD+USEFUL & SAM & SAM+USEFUL \\ \hline
10\% & 7.20 \(\pm\) 0.17 & 6.64 \(\pm\) 0.10 & 5.15 \(\pm\) 0.05 & 4.75 \(\pm\) 0.09 \\
20\% & 9.26 \(\pm\) 0.23 & 8.88 \(\pm\) 0.07 & 6.08 \(\pm\) 0.06 & 5.82 \(\pm\) 0.05 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Test error on label noise CIFAR10 (all methods are with MixUp). Results are averaged over 3 seeds.

Figure 14: **Ablation studies of training ResNet18 on CIFAR10.** In each experiment, we used the standard training settings while (left) varying training batch size or (middle) varying learning rate, or (right) varying upsampling factor.

\begin{table}
\begin{tabular}{c c c c} \hline \hline SGD & EIIL & JTT & SGD+USEFUL \\ \hline
5.07 \(\pm\) 0.04 & 5.04 \(\pm\) 0.04 & 4.89 \(\pm\) 0.03 & **4.79 \(\pm\) 0.05** \\ \hline \hline \end{tabular}
\end{table}
Table 9: Test errors of different simplicity bias reduction methods on CIFAR10. Results are averaged over 3 seeds.

Figure 15: **Separating epoch analysis.** (left) Red lines indicate our optimal choice of \(t\) to separate examples at and restart training. The early epoch that can best separate the examples is when the change in training error starts to shrink. (right) Red points indicate our optimal choice of \(t\).

### Ablation studies

**USEFUL vs Upweighting loss.** We compare USEFUL with upweighting loss of examples in the slow-learnable clusters. As can be seen in Table 5, when coupling with either SGD or SAM, USEFUL clearly outperforms upweighting loss on both CIFAR datasets. It is worth mentioning that upweighting loss is different from iteratively importance sampling methods such as GRW [78], which dynamically upweights examples during the training by factors that depend on the loss value. In addition, GRW is dedicated to the distribution shift setting while our paper considers the in-distribution setting.

**Data selection method.** In this experiment, we compare clustering with other methods for partitioning data. The first baseline is to upsample misclassified examples (Misclassification) while the second baseline is to upsample all examples whose training errors are larger than the median value (Quantile). All the three methods are performed at the same epoch \(t\). Table 7 shows that USEFUL selects a better set of upsampling examples, leading to the best model performance.

**Training batch size.** Figure 14 left shows the gap between USEFULand SGD when changing the batch size. Our method consistently improves the performance, proving its effectiveness is not simply captured by the gradient variance caused by the training batch size.

**Learning rate.** The small learning rate is a nuance in our theoretical results to guarantee that fast-learnable and slow-learnable features are separable in early training. In general, a small learning rate is required for most theoretical results on gradient descent and its convergence and is a standard theoretical assumption [12, 60, 62, 73]. In practice, both for separating fast-learnable vs slow-learnable examples and for training on the upsampled data, we used the standard learning rate that results in the best generalization performance for both SGD and SAM following prior work [3, 81, 42]. While the theoretical requirement for the learning rate is always smaller than the one that is used in practice, empirically a larger learning rate does not yield better generalization for the problems we considered in contrast to other settings [46, 58]. As shown in Figure 14 middle, increasing the learning rate has an adverse effect on the model performance. Indeed, for a fair comparison, the algorithms should be trained with hyperparameters that empirically yield the best performance; otherwise, the conclusions are not valid. Nevertheless, USEFUL always improves the test error across different learning rates.

**Upsampling factor.** We empirically found the upsampling factor of 2 to consistently work well across different datasets and architectures. Using larger upsampling factors results in a too-large discrepancy between the training and test distribution and does not work better, as is expected and discussed in Section 1. As illustrated in Figure 14 right, while all factors from 2 to 5 bring performance improvement, the upsampling factor of 2 yields the best performance, as it reduces the simplicity bias with minimum change to the rest of the data distribution.

**Separating epoch.** Fig 15a shows that the early epoch that can best separate the examples is when the change in training error starts to shrink. At this time, examples that are learned can be separated from the rest by clustering model outputs as analyzed in our theoretical results in Section 3.3. Figure 15b demonstrates the performance of USEFUL when separating examples at different epochs in early training. Too early or too late epochs do not cluster examples well, i.e., some examples with fast-learnable features fall into the slow-learnable clusters and vice versa. This ablation study shows that upsampling correct examples and with enough amount is important for our method to achieve its best. Note that there is no universal separating epoch. This is reasonable because each data has a different data distribution, i.e. slow-learnable and fast-learnable features, thus, a different theoretical time \(T\) in Theorems 3.2 and 3.3.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our claims match the theoretical and experimental results Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We do not have theoretical results for non-CNN architectures. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We provide full assumptions and proofs in Appendix.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide detailed experimental settings in the Experiment section and in Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: We use open datasets, which have been cited and described in the paper. We also provide our code for reproducing the experimental results. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide detailed experimental settings in the Experiment section and in Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report the error bar for averaging multiple seeds in our experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * The results of the experiment are summarized in Table 10.

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide the information on our computational resources in Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our paper conform to every aspect in the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: There are no potential harms to our models. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite all data, code, and models used properly. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.