# Gradient Estimation For Exactly-\(k\) Constraints

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The exactly-\(k\) constraint is ubiquitous in machine learning and scientific applications, such as ensuring that the sum of electric charges in a neutral atom is zero. However, enforcing such constraints in machine learning models while allowing differentiable learning is challenging. In this work, we aim to provide a "cookbook" for seamlessly incorporating exactly-\(k\) constraints into machine learning models by extending a recent gradient estimator from Bernoulli variables to Gaussian and Poisson variables, utilizing constraint probabilities. We show the effectiveness of our proposed gradient estimators in synthetic experiments, and further demonstrate the practical utility of our approach by training neural networks to predict partial charges for metal-organic frameworks, aiding virtual screening in chemistry. Our proposed method not only enhances the capability of learning models but also expands their applicability to a wider range of scientific domains where satisfaction of constraints is crucial.

## 1 Introduction

The exactly-\(k\) constraint, that is, the sum of \(n\) variables is equal to \(k\), is not only ubiquitous in machine learning such as learning sparse features (Chen et al., 2018) and discrete variational auto-encoders (Rolfe, 2016), but also critical to scientific applications such as charge-neutral scenarios in computational chemistry (Raza et al., 2020) and count-aware cell type deconvolution (Liu et al., 2023). In the former cases, the variables are binary while in the latter cases, the variables are continuous or integer, depending on the applications. Such tasks can involve optimizing the expectation of an objective function with respect to variables satisfying the exactly-\(k\) constraint, whose distributions are parameterized by neural networks. This optimization problem is challenging since the expectation can be intractable and thus gradient estimation is required. Existing estimators include score-function-based ones that suffer from high variance and reparameterization-based ones that require relaxation and can be highly biased Xie and Ermon (2019). A recently proposed gradient estimator (Ahmed et al., 2023) outperforms the aforementioned estimators by leveraging constraint probability and avoiding relaxations. Still, it is limited to the exactly-\(k\) constraint on Bernoulli variables.

In this work, we aim to carry out a systematic study of gradient estimation for exactly-\(k\) constraints over Bernoulli, Gaussian, and Poisson variables, the three most commonly used distributions in modeling. We show that on the forward pass, the constrained distributions have closed-form representations, and thus exact sampling from the constrained distribution can be achieved. On the backward pass, we reparameterize the gradient of the loss function with respect to the samples as a function of the expected marginals of the constrained distributions. Further, we find that under certain loss functions, the expected loss under the constrained distribution has a closed-form solution. That is, in such cases, we are able to train models under the exactly-\(k\) constraint without any gradientestimations. We include synthetic experiments to evaluate the bias and variance of our proposed gradient estimation on Gaussian and Poisson variables. We also include an experiment on predicting partial charges for metal-organic frameworks, where our gradient estimation, when combined with an ensemble method, achieves state-of-the-art prediction performance.

## 2 Problem Statement and Motivation

We consider models described by the equations

\[\bm{\theta}=h_{\bm{v}}(\bm{x}),\qquad\bm{z}\sim p_{\bm{\theta}}(\bm{z}\mid\sum_{ i}z_{i}=k),\qquad\hat{\bm{y}}=f_{\bm{u}}(\bm{z},\bm{x}),\] (1)

where \(\bm{x}\in\mathcal{X}\) and \(\hat{\bm{y}}\in\mathcal{Y}\) denote feature inputs and target outputs, respectively, \(h_{\bm{v}}:\mathcal{X}\to\Theta\) and \(f_{\bm{u}}:\mathcal{Z}\times\mathcal{X}\to\mathcal{Y}\) are smooth, parameterized maps. \(\bm{\theta}\) are parameters inducing a distribution over the latent vector \(\bm{z}\) and the induced distribution \(p_{\bm{\theta}}(\bm{z})\) is defined as \(p_{\bm{\theta}}(\bm{z})=\prod_{i=1}^{n}p_{\theta_{i}}(z_{i})\), with \(p_{\theta_{i}}(z_{i})\) as defined in Table 1, where \(\mathcal{N}(z;\mu,\sigma^{2})\) denotes the density of a Gaussian distribution with mean \(\mu\) and variance \(\sigma^{2}\) at \(z\). An exactly-\(k\) constraint is enforced over the distribution \(p_{\bm{\theta}}(\bm{z})\), inducing a conditional distribution \(p_{\bm{\theta}}(\bm{z}\mid\sum_{i}z_{i}=k):=p_{\bm{\theta}}(\bm{z})\cdot[\sum _{i}z_{i}=k]/p_{\bm{\theta}}(\sum_{i}z_{i}=k)\) where the denominator denotes the constraint probability \(p_{\bm{\theta}}(\sum_{i}z_{i}=k)\). This formulation is general and it can subsume neural network models that integrate the exactly-\(k\) constraint in the input, output, or latent space, which we visualize in Figure 1.

The training of such models is performed by optimizing an expected loss to learn parameters \(\bm{\omega}=(\bm{v},\bm{u})\) in Equation 1 as below,

\[L(\bm{x},\bm{y};\bm{\omega})=\mathbb{E}_{\bm{z}\sim p_{\bm{\theta}}(\bm{z}| \sum_{i}z_{i}=k)}[\ell(f_{\bm{u}}(\bm{z},\bm{x}),\bm{y})]\qquad\text{ \emph{with} }\bm{\theta}=h_{\bm{v}}(\bm{x}),\] (2)

where \(\ell:\mathcal{Y}\times\mathcal{Y}\to\mathbb{R}^{+}\) is a point-wise loss function. However, the standard auto-differentiation can not be directly applied to the expected loss due to two main obstacles. First, for the gradient of \(L\) w.r.t. parameters \(\bm{u}\) in the decoder network \(f_{\bm{u}}\) defined as

\[\nabla_{\bm{u}}L(\bm{x},\bm{y};\bm{\omega})=\mathbb{E}_{\bm{z}\sim p_{\bm{ \theta}}(\bm{z}|\sum_{i}z_{i}=k)}[\partial_{\bm{u}}f_{\bm{u}}(\bm{z},\bm{x})^{ \top}\nabla_{\hat{\bm{y}}}\ell(\hat{\bm{y}},\bm{y})]\] (3)

with \(\hat{y}=f_{\bm{u}}(\bm{z},\bm{x})\) being decoding of a latent sample \(\bm{z}\), the expectation does not allow closed-form solution in general and requires Monte-Carlo estimations by sampling \(\bm{z}\) from the constrained distribution \(p_{\bm{\theta}}(\bm{z}\mid\sum_{i}z_{i}=k)\). The same issue arises in the gradient of \(L\) w.r.t. parameters \(\bm{v}\) in the encoder network defined as

\[\nabla_{\bm{v}}L(\bm{x},\bm{y};\bm{\omega})=\partial_{\bm{v}}h_{\bm{v}}(\bm{x} )^{\top}\nabla_{\bm{\theta}}L(\bm{x},\bm{y};\bm{\omega}).\] (4)

The second obstacle lies in the computation of the gradient of \(L\) w.r.t. the encoder as in Equation 4 defined as that requires to compute \(\partial_{\bm{\theta}}\bm{z}\), a derivative that is not well-defined and requires gradient estimation for updating \(\bm{\theta}\). In a recent work [1], a gradient estimator called SIMPLE is proposed to tackle these two issues by _exactly sampling_ from the constrained distribution and using _marginals_ as a proxy to samples respectively, where SIMPLE is able to outperform both score-function-based gradient estimators and reparameterization-based ones. However, SIMPLE is limited to Bernoulli variables and whether the same gradient estimation can be extended to a larger distribution family remains underexplored.

\begin{table}
\begin{tabular}{c c} \hline
**Variable** & **Parameterized Distribution** \\ \hline Bernoulli & \(p_{\theta_{i}}(z_{i}=1)=\mathrm{sigmoid}(\theta_{i})\) \\ \(p_{\theta_{i}}(z_{i}=0)=1-\mathrm{sigmoid}(\theta_{i})\) \\ Gaussian & \(p_{\theta_{i}}(z_{i})=\mathcal{N}(z_{i};\mu_{i},\sigma_{i}^{2})\) with \(\theta_{i}=(\mu_{i},\sigma_{i})\) \\ Poisson & \(p_{\theta_{i}}(z_{i})=\theta_{i}^{z_{i}}e^{-\theta_{i}}/z_{i}!\) \\ \hline \end{tabular}
\end{table}
Table 1: Parameterization of the three distribution settings.

Figure 1: Model formulation under an exactly-\(k\) constraint.

Gradient Estimation for Exactly-\(k\)

We tackle the gradient estimation for the exactly-\(k\) constraints by solving the aforementioned two subproblems: (**P1**) how to sample exactly from the constrained distribution \(p_{\bm{\theta}}(\bm{z}\mid\sum_{i}z_{i}=k)\) and (**P2**) how to estimate \(\nabla_{\bm{\theta}}L(\bm{x},\bm{y};\bm{\omega})\). By combining solutions to these two problems, we manage to train the constrained model in an end-to-end manner. Table 3 in the Appendix presents a summary of the key components in the proposed gradient estimation.

### Exact Sampling

For both Gaussian and Poisson variables, we find that their constrained distributions conform to commonly seen closed-form distributions and thus allow efficient sampling by using built-in sampling algorithms in deep learning frameworks. We formally state our findings below.

**Proposition 1** (Gaussian Constrained Distribution).: _Given \(\bm{z}=(z_{1},\dots,z_{n})^{T}\) with \(z_{i}\sim\mathcal{N}(\mu_{i},\sigma_{i}^{2})\), the constrained distribution \(p(\bm{z}\mid\sum_{j=1}^{n}z_{j}=k)\) is equivalent to an \(n-1\) dimensional multivariate normal distribution with mean \(\overline{\mu}\in\mathbb{R}^{n-1}\) and covariance matrix \(\overline{\bm{\Sigma}}\in\mathbb{R}^{(n-1)\times(n-1)}\) with their entries defined as below,_

\[\overline{\mu}_{i}=\sum_{j=1}^{n-1}\left(\mathbbm{1}\left[i=j\right]\sigma_{i }^{2}-\frac{\sigma_{i}^{2}\sigma_{j}^{2}}{\sum_{i=1}^{n}\sigma_{i}^{2}} \right)\left(c+\frac{\mu_{j}}{\sigma_{j}^{2}}\right)\ \text{ and }\ \overline{\bm{\Sigma}}_{i,j}=\begin{cases}\sigma_{i}^{2}-\frac{(\sigma_{i}^{ 2})^{2}}{\sum_{j=1}^{n}\sigma_{i}^{2}}&i=j\\ -\frac{\sigma_{i}^{2}\sigma_{i}^{2}}{\sum_{i=1}^{n}\sigma_{i}^{2}}&i\neq j \end{cases}.\]

**Proposition 2** (Poisson Constrained Distribution).: _Given \(\bm{z}=(z_{1},\dots,z_{n})^{T}\) with \(z_{i}\sim Poisson(\theta_{i})\), the constrained distribution \(p(\bm{z}\mid\sum_{j=1}^{n}z_{n}=k)\) is equivalent to a multinomial distribution with parameter \(k\) and probabilities \(\frac{\theta_{1}}{\sum_{j=1}^{n}\delta_{j}},\dots,\frac{\theta_{n}}{\sum_{j=1} ^{n}\delta_{j}}\)._

### Conditional Marginals as Proxy

For estimating gradient \(\nabla_{\bm{\theta}}L(\bm{x},\bm{y};\bm{\omega})\), we follow an approximation adopted by Ahmed et al. (2023); Niepert et al. (2021) where the main intuition is to use the conditional marginals \(\bm{\mu}\coloneqq\mu(\bm{\theta})\coloneqq\{p_{\bm{\theta}}(z_{j}\mid\sum_{i} z_{i}=k)\}_{j=1}^{n}\) as a proxy for samples \(\bm{z}\), that is,

\[\nabla_{\bm{\theta}}L(\bm{x},\bm{y};\bm{\omega})\approx\partial_{\bm{\theta} }\mu(\bm{\theta})\nabla_{\bm{z}}\ell(\bm{x},\bm{y};\bm{\omega}),\] (5)

where the sample \(\bm{z}\) is reparameterized to be a function of the conditional marginals and is assumed to be \(\partial_{\bm{\mu}}\bm{z}\approx\bm{\mathrm{I}}\). In the case of Gaussian and Poisson variables, the reparameterization is achieved by using the expected marginals conditioning on the exactly-\(k\) constraint, that is, \(\bm{\mu}\coloneqq\mu(\bm{\theta})\) with \(\bm{\mu}_{j}=\mathbb{E}_{p_{\bm{\theta}}(z_{j}\mid\sum_{i}z_{i}=k)}[z_{j}]\) as a function of the parameters \(\bm{\theta}\). For succinctness, we refer to \(\bm{\mu}\) as expected marginals. The remaining question is how to obtain the expected marginals \(\bm{\mu}\). We find that the expected marginals in both cases have closed-form solutions.

**Proposition 3** (Gaussian Conditional Marginal).: _Given \(\bm{z}=(z_{1},\dots,z_{n})^{T}\) with \(z_{i}\sim\mathcal{N}(\mu_{i},\sigma_{i}^{2})\), the conditional marginal \(p(z_{i}\mid\sum_{j=1}^{n}z_{j}=k)\) follows a univariate Gaussian distribution with mean \(\tilde{\mu}_{i}=\mu_{i}+\frac{\sigma_{i}^{2}}{\sum_{j=1}^{n}\sigma_{j}^{2}}(k -\sum_{j=1}^{n}\mu_{j})\) and variance \(\tilde{\sigma}_{i}^{2}=\sigma_{i}^{2}-\frac{(\sigma_{i}^{2})^{2}}{\sum_{j=1}^{ n}\sigma_{j}^{2}}\), that is, \(\bm{\mu}_{i}=\tilde{\mu}_{i}\)._

**Proposition 4** (Poisson Conditional Marginal).: _Given \(\bm{z}=(z_{1},\dots,z_{n})^{T}\) with \(z_{i}\sim Poisson(\theta_{i})\), the conditional marginal of \(p(z_{i}\mid\sum_{j=1}^{n}z_{n}=k)\) follows a binomial distribution with parameter \(k\) and probability \(\frac{\theta_{i}}{\sum_{j=1}^{n}\theta_{j}}\), with \(\bm{\mu}_{i}=\frac{k\theta_{i}}{\sum_{j=1}^{n}\theta_{j}}\)._

### Closed-form Expected Loss

This section focuses on some special cases where the expected loss in Equation 2 has a closed-form solution and thus no gradient estimation is needed. We find that when the decoder \(f_{\bm{u}}\) is an identity function, that is, \(\bm{y}=\bm{z}\), the expected loss defined over Gaussian variables has a closed-form solution when the element-wise loss is L1 loss or L2 loss. The same conclusion holds for Poisson variables with the element-wise loss being L2 loss. We refer the readers to Proposition 5 and Proposition 6 respectively in Appendix for details.

## 4 Experiments

We evaluate our proposed gradient estimation on both synthetic settings and a scientific application.

Synthetic Experiments.We analyze our proposed gradient estimators for Gaussian and Poisson variables using three metrics, bias, variance, and averaged error, in synthetic settings where the ground truth gradients can be obtained by taking derivatives of the closed-form expected loss as stated in Section 3.3. The distance between the estimated and the ground truth gradient vectors is measured by the cousin distance defined as 1 - cosine similarity. We further compare with a random estimation as a baseline. Bias and variance results are presented in Figure 2 with additional details and results presented in Section C in the Appendix, where our proposed gradient estimator is able to achieve significantly lower bias, variances as well as averaged errors than the baseline, indicating its effectiveness.

Partial Charge Predictions for Metal-Organic Frameworks.Metal-organic frameworks (MOFs) represent a class of materials with a wide range of applications in chemistry and materials science. Predicting properties of MOFs, such as partial charges on metal ions, is essential for understanding their reactivity and performance in chemical processes. However, it is challenging due to the complex interactions between metal ions and ligands and the requirement that the predictions need to satisfy the charge neutral constraint, that is, an exactly-zero constraint.

We adopt the same model as in Raza et al. (2020) where the charges are assumed to be Gaussian variables and the element loss is L1 loss, and address this problem by training the model leveraging our observation in Section 3.3 and using gradients of the expected loss. We further observe that using an ensemble of such models gives predictions that also satisfy the charge-neutral constraint. The prediction performance of our two proposed approaches is presented in Table 2, compared with baseline approaches reported by Raza et al. (2020). Results show that training using closed-form expected loss achieves the same performance as MPNN(variance) which is considered to be the strongest baseline approach, and when further combined with the ensemble method, our approach achieves significantly better predictions.

## 5 Conclusion

In this work, we provide an extensive study on differentiable learning under exactly-\(k\) constraints given various distribution families. We further provide empirical studies of our proposed gradient estimation on both synthetic experiments and a scientific application.

\begin{table}
\begin{tabular}{c c} \hline \hline
**Method** & **MAD** \\ (charge neutrality enforcement) & mean (std) \\ \hline Constant Prediction & 0.324 (7e-3) \\ Element-mean (uniform) & 0.154 (2e-3) \\ Element-mean (variance) & 0.153 (2e-3) \\ MPNN (uniform) & 0.026 (8e-4) \\ MPNN (variance, reproduced) & 0.0251 (8e-4) \\ Closed-form (ours) & 0.0251 (6e-4) \\ Closed-form + Ensemble (ours) & **0.0235 (5e-4)** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison on prediction performance.

Figure 2: A comparison of our gradient estimation and random estimations on bias and variance.

## References

* Ahmed et al. [2023] Kareem Ahmed, Zhe Zeng, Mathias Niepert, and Guy Van den Broeck. Simple: A gradient estimator for k-subset sampling. In _Proceedings of the International Conference on Learning Representations (ICLR)_, may 2023.
* Altmann et al. [2014] Yoann Altmann, Steve McLaughlin, and Nicolas Dobigeon. Sampling from a multivariate gaussian distribution truncated on a simplex: a review. In _2014 IEEE Workshop on Statistical Signal Processing (SSP)_, pages 113-116. IEEE, 2014.
* Chen et al. [2018] Jianbo Chen, Le Song, Martin Wainwright, and Michael Jordan. Learning to explain: An information-theoretic perspective on model interpretation. In _International conference on machine learning_, pages 883-892. PMLR, 2018.
* Cong et al. [2017] Yulai Cong, Bo Chen, and Mingyuan Zhou. Fast simulation of hyperplane-truncated multivariate normal distributions. 2017.
* Grover et al. [2019] Aditya Grover, Eric Wang, Aaron Zweig, and Stefano Ermon. Stochastic optimization of sorting networks via continuous relaxations. _arXiv preprint arXiv:1903.08850_, 2019.
* Gut [2009] Allan Gut. _An Intermediate Course in Probability_. Springer, 2009.
* Holt and Nguyen [2023] William Holt and Duy Nguyen. _Introduction to Bayesian Data Imputation_. SSRN, 2023.
* Jang et al. [2016] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. _arXiv preprint arXiv:1611.01144_, 2016.
* Kim et al. [2016] Carolyn Kim, Ashish Sabharwal, and Stefano Ermon. Exact sampling with integer linear programs and random perturbations. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 30, 2016.
* Liu et al. [2023] Zhiyuan Liu, Dafei Wu, Weiwei Zhai, and Liang Ma. Sonar enables cell type deconvolution with spatially weighted poisson-gamma model for spatial transcriptomics. _Nature Communications_, 14(1):4727, 2023.
* Maatouk et al. [2022] Hassan Maatouk, Xavier Bay, and Didier Rulliere. A note on simulating hyperplane-truncated multivariate normal distributions. _Statistics & Probability Letters_, 191:109650, 2022.
* Maddison et al. [2016] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. _arXiv preprint arXiv:1611.00712_, 2016.
* Miller [1981] Kenneth S Miller. On the inverse of the sum of matrices. _Mathematics magazine_, 54(2):67-72, 1981.
* Niepert et al. [2021] Mathias Niepert, Pasquale Minervini, and Luca Franceschi. Implicit mle: backpropagating through discrete exponential family distributions. _Advances in Neural Information Processing Systems_, 34:14567-14579, 2021.
* Raza et al. [2020] Ali Raza, Arni Sturluson, Cory M Simon, and Xiaoli Fern. Message passing neural networks for partial charge assignment to metal-organic frameworks. _The Journal of Physical Chemistry C_, 124(35):19070-19082, 2020.
* Rolfe [2016] Jason Tyler Rolfe. Discrete variational autoencoders. _arXiv preprint arXiv:1609.02200_, 2016.
* Xie and Ermon [2019] Sang Michael Xie and Stefano Ermon. Reparameterizable subset sampling via continuous relaxations. In _Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19_, pages 3919-3925. International Joint Conferences on Artificial Intelligence Organization, 7 2019. doi: 10.24963/ijcai.2019/544.

## Appendix A Related Work

A substantial amount of research has been devoted to estimating gradients for categorical random variables. Maddison et al. (2016) Jang et al. (2016) proposed to refactor the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution, which enables automatic differentiation. This paper investigates a more complex distribution, \(k\)-subset distribution. Gradient estimation under exactly-\(k\) constraints has been widely studied. Existing methods either employ the score function and straight-through estimator or suggest custom relaxation (Kim et al., 2016; Chen et al., 2018; Grover et al., 2019; Xie and Ermon, 2019). Xie and Ermon (2019) extends the Gumbel-softmax technique to k-subsets, enabling backpropagation for k-subset sampling. However, this comes at the trade-off of introducing some bias in the learning process due to the use of relaxed samples. While score function estimators offer a seemingly simple solution, it is widely acknowledged that they are prone to exhibiting exceedingly high variance. A recently introduced gradient estimator known as SIMPLE (Ahmed et al., 2023) surpasses its predecessors but is constrained to Bernoulli random variables.

Extensive research has been conducted on numerical sampling from multivariate normal distributions while adhering to various constraints. Altmann et al. (2014) reviewed classical Gibbs Sampling on a standard simplex (samples are positive and sum to one) and proposed using Hamiltonian Monte Carlo(HMC) methods. Efficient sampling method for multivariate normal distribution truncated by hyperplanes(\(\mathbf{A}\mathbf{x}=\mathbf{b}\), where \(dim(\mathbf{x})=N\) and \(rank(\mathbf{A})=n<N\)) were investigated by Maatouk et al. (2022) and Cong et al. (2017). These studies focus on numerical simulations, whereas our approach aims to derive a closed-form solution for the multivariate normal distribution subject to the exactly-\(k\) constraint.

## Appendix B Theoretical Results

**Proposition 5** (Closed-form Expected Loss under Gaussian).: _Let \(\boldsymbol{z}=\left(z_{1},\ldots,z_{n}\right)^{T}\), where \(z_{i}\sim\mathcal{N}(\mu_{i},\sigma_{i}^{2})\). Let \(\mathbf{b}=\left(b_{1},b_{2},\ldots,b_{n}\right)^{T}\) be the ground truth vector subject to the equality constraint \(\sum_{j=1}^{n}b_{j}=k\). The L1 loss of \(\boldsymbol{z}\) subject to the constraint \(\sum_{j=1}^{n}z_{j}=k\) is given by_

\[L(\theta)=\sum_{i=1}^{n}\tilde{\sigma}_{i}\sqrt{\frac{2}{\pi}}\exp\left(\frac {-\left(\tilde{\mu}_{i}-b_{i}\right)^{2}}{2\tilde{\sigma}_{i}^{2}}\right)+ \left(\tilde{\mu}_{i}-b_{i}\right)erf\left(\frac{\tilde{\mu}_{i}-b_{i}}{ \sqrt{2\tilde{\sigma}_{i}^{2}}}\right),\]

_where \(\tilde{\mu}_{i}\) and \(\tilde{\sigma}_{i}^{2}\) are the mean and variance of the conditional marginal of \(z_{i}\) subject to the constraint. \(\tilde{\mu}_{i}=\mu_{i}+\frac{\sigma_{i}^{2}}{\sum_{j=1}^{n}\sigma_{j}^{2}}(k -\sum_{j=1}^{n}\mu_{j})\) and \(\tilde{\sigma}_{i}^{2}=\sigma_{i}^{2}-\frac{(\sigma_{i}^{2})^{2}}{\sum_{j=1}^ {n}\sigma_{j}^{2}}\). Further, the L2 loss of \(\boldsymbol{z}\) subject to the constraint \(\sum_{j=1}^{n}z_{j}=k\) is given by_

\[L(\theta)=\sum_{i=1}^{n}\left[\left(\mu_{i}-\frac{\sigma_{i}^{2}\sum_{j=1}^{n }\mu_{j}}{\sum_{j=1}^{n}\sigma_{j}^{2}}\right)^{2}+\sigma_{i}^{2}-\frac{( \sigma_{i}^{2})^{2}}{\sum_{j=1}^{n}\sigma_{j}^{2}}-2b_{i}\left(\mu_{i}-\frac{ \sigma_{i}^{2}\sum_{j=1}^{n}\mu_{j}}{\sum_{j=1}^{n}\sigma_{j}^{2}}\right)+b_{i} ^{2}\right].\]

**Proposition 6** (Closed-form Expected Loss under Poisson).: _Let \(\boldsymbol{z}=(z_{1},\ldots,z_{n})^{T}\), where \(z_{i}\sim Poisson(\theta_{i})\). Let \(\mathbf{b}=\left(b_{1},b_{2},\ldots,b_{n}\right)^{T}\) be the ground truth vector subject to the equality constraint

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Variable** & **Sampling** & **Expected Marginals** & **Expected Loss** \\ \hline Bernoulli & Proposition 2 & Theorem 1 & — \\  & in Ahmed et al. (2023) & in Ahmed et al. (2023) & — \\ Gaussian & Proposition 1 & Proposition 3 & Proposition 5 \\ Poisson & Proposition 2 & Proposition 4 & Proposition 6 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Summary of exact sampling, expected marginals, and closed-form expected loss.

\(\sum_{j=1}^{n}b_{j}=k\). The L2 loss of \(\bm{z}\) subject to the constraint \(\sum_{j=1}^{n}z_{j}=k\) is given by_

\[L(\theta)=\sum_{i=1}^{n}\left[k\left(\frac{\theta_{i}}{\sum_{j=1}^{n}\theta_{j}} \right)\left(1-\frac{\theta_{i}}{\sum_{j=1}^{n}\theta_{j}}\right)+k^{2}\left( \frac{\theta_{i}}{\sum_{j=1}^{n}\theta_{j}}\right)^{2}-2kb_{i}\left(\frac{ \theta_{i}}{\sum_{j=1}^{n}\theta_{j}}\right)+b_{i}^{2}\right].\]

## Appendix C Additional Experiment Results in Synthetic Settings

We carried out a series of experiments to analyze the effectiveness of our gradient estimator from Gaussian and Poisson variables. Our focus lies on three pivotal metrics: bias, variance, and the average error. Since, we only care about the direction of the gradients, we employed the cosine distance, namely 1 - cosine similarity, to measure the deviation of our gradient estimators from the ground truth vector. The ground truth logits, \(\mathbf{n}\), are sampled from \(\mathcal{N}(\mathbf{0},\mathbf{I})\) satisfying the constraint. We plotted the three metrics against the dimension of \(\bm{z}\), namely \(n\), and graphed the standard deviations. For each \(n\), we randomly generated \(10\) sets of parameters and calculated the metrics for each set. Then, we take average of these \(10\) repeats and computed their standard deviations. We compare our results with random guess. The randomly generated gradients are sampled from \(\mathcal{N}(\mathbf{0},\mathbf{I})\).

GaussianWe use the L1 loss function \(L(\theta)=\mathbf{E}_{\bm{z}\sim p_{\theta}(\bm{z}\|\sum_{i}z_{i}=0)}[\|\ \mathbf{z}-\mathbf{b}\ \|_{1}]\). The constraint, \(k\), is set to \(0\). We observe that the bias and average error remain relatively stable across various values of \(n\), with biases hovering around 0.1 and average errors hovering around 0.3. The variance steadily decreases and converges to a relatively low value. Our estimator outperforms the baseline across all dimensions in all three metrics.

PoissonWe use the L2 loss function \(L(\theta)=\mathbf{E}_{\bm{z}\sim p_{\theta}(\bm{z}\|\sum_{i}z_{i}=0)}[\|\ \mathbf{z}-\mathbf{b}\ \|_{2}^{2}]\). The constraint is set to \(k=n\). Since, the bias, variance, and average error for our estimators are very small, we opt to take their logarithms. In all dimensions and using all three metrics, our estimator surpasses the baseline.

Figure 4: Synthetic Experiment with Poisson Variables.

Figure 3: Synthetic Experiment with Gaussian Variables.

Additional Experimental Details for Partial Charge Predictions

Model ArchitectureOur model architecture extends the Message Passing Neural Network (MPNN) Raza et al. (2020) framework and incorporates exact-k constraint for Gaussian variables, ensuring strict adherence to the critical constraint. The core innovation involves replacing the conventional L1 loss with the closed-form Gaussian loss function 5. This loss function penalizes deviations from the exact-k constraint while considering the probabilistic nature of Gaussian variables. This comprehensive approach not only enables our model to capture complex structural relationships in MOFs but also ensures accurate predictions of partial charges while respecting the crucial exact-k constraint, enhancing its applicability in a wide range of graph-based applications, including those pertaining to metal-organic frameworks.

Additionally, we also devise an ensemble methodology to enhance the predictive performance and robustness of our exact-k constrained MPNN model. To achieve this, we adopt a systematic approach encompassing model variability, aggregation strategies and cross-validation. Two instances of the exact-k constrained MPNN model are trained with variations in initialization. We apply the averaging aggregation technique to combine the predictions from these models. Performance assessment is conducted through cross-validation techniques. The ensemble's performance is evaluated on a separate test dataset to ascertain its generalization ability. This ensemble approach not only elevates predictive accuracy but also fortifies the model's resilience, rendering it highly effective for complex tasks, including those pertaining to metal-organic frameworks.

TrainingHere, we describe our training and evaluation process for the exact-k constrained MPNN. We conducted a random partitioning of the dataset containing 2266 charge-labeled MOFs, creating distinct training, validation, and test sets (70/10/20%). We use the training set for direct model parameter tuning, while the validation set aids in hyperparameter selection to prevent overfitting. The test set plays a crucial role in providing an unbiased assessment of the final model's performance.

Hyperparameter TuningTo optimize our model's performance, we conduct a systematic hyperparameter tuning process, sequentially optimizing six key hyperparameters: Learning rate, Batch size, Time steps, Embedding size, Hidden Feature size, and Patience Threshold. After thorough tuning, we set the hyperparameters to their optimal values: lr = 0.005, batch size = 64, time steps = 4, embedding size = 20, hidden feature size = 40, and patience threshold = 150, achieving peak model performance.

## Appendix E Proofs

### Proposition 1

Proof.: Let \(\bm{z}=(z_{1},\ldots,z_{n})^{T}\), where \(z_{i}\sim\mathcal{N}(\mu_{i},\sigma_{i}^{2})\). We attempt to compute a closed-form solution for the conditional distribution \(p\left(\bm{z}\mid\sum_{j=1}^{n}z_{j}=k\right)\).

\[p\left(\bm{z}\mid\sum_{j=1}^{n}z_{j}=k\right) =\frac{p\left(\bm{z}\cap\sum_{j=1}^{n}z_{j}=k\right)}{p\left( \sum_{j=1}^{n}z_{j}=k\right)}\] \[=\frac{p\left(\bm{z}\right)\cdot\left[\sum_{j=1}^{n}z_{j}=k\right] }{p\left(\sum_{j=1}^{n}z_{j}=k\right)}\]

where \(\left[\sum z_{i}=k\right]\) is an indicator function. Notice that the denominator \(p(\sum_{j=1}^{n}z_{j}=k)\) is the probability distribution function of \(Y=\sum_{j=1}^{n}z_{j}\) evaluated at \(k\). Since \(Y\) is a linear combination of independent Gaussian random variables, \(Y\sim\mathcal{N}(\sum_{j=1}^{n}\mu_{j},\sum_{j=1}^{n}\sigma_{j}^{2})\). Thus,

\[p\left(\sum_{j=1}^{n}z_{j}=k\right) =\frac{1}{\sqrt{2\pi\sum_{j=1}^{n}\sigma_{j}^{2}}}\exp\left[- \frac{1}{2\sum_{j=1}^{n}\sigma_{j}^{2}}\left(k-\sum_{j=1}^{n}\mu_{j}\right)^{2}\right]\]The joint distribution function \(p\left(\bm{z}\right)\), the numerator, follows a multivariate normal distribution with mean \(\mu=\left(\mu_{1},\mu_{2},\ldots,\mu_{n}\right)^{T}\) and variance \(\bm{\Sigma}=diag\left(\sigma_{i}^{2}\right)\) Thus, the conditional distribution can be rewritten as

\[p\left(\bm{z}\mid\sum_{j=1}^{n}z_{j}=k\right)=\frac{\prod_{i=1}^{n}\frac{1}{ \sqrt{2\pi\sigma_{i}^{2}}}\exp\left[-\frac{1}{2\sigma_{i}^{2}}\left(z_{i}-\mu_ {i}\right)^{2}\right]}{\frac{1}{\sqrt{2\pi\sum_{j=1}^{n}\sigma_{j}^{2}}}\exp \left[-\frac{1}{2\sum_{j=1}^{n}\sigma_{j}^{2}}\left(k-\sum_{j=1}^{n}\mu_{j} \right)^{2}\right]}[\sum_{j=1}^{n}z_{j}=k]\]

Let \(C=\left(\frac{1}{\sqrt{2\pi\sum_{j=1}^{n}\sigma_{j}^{2}}}\exp\left[-\frac{1}{2 \sum_{j=1}^{n}\sigma_{j}^{2}}\left(k-\sum_{j=1}^{n}\mu_{j}\right)^{2}\right] \right)^{-1}\). We can express our result as

\[p\left(\bm{z}\mid\sum_{j=1}^{n}z_{j}=k\right) =C\cdot\left[\sum_{j=1}^{n}z_{j}=k\right]\cdot\prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma_{i}^{2}}}\exp\left[-\frac{1}{2\sigma_{i}^{2}}\left( z_{i}-\mu_{i}\right)^{2}\right]\] \[=C\cdot f\left(\bm{z}\right)\]

where \(f\left(\bm{z}\right)\) is the joint p.d.f. of the multivariate normal distribution \(\bm{z}\) To deal with the indicator function, let's assume \(z_{n}=k-\sum_{j=1}^{n-1}z_{j}\). Then, the joint p.d.f. of \(\bm{z}\) becomes

\[f\left(\bm{z}\right)=\frac{1}{\sqrt{2\pi\sigma_{n}^{2}}}\exp \left[-\frac{1}{2\sigma_{n}^{2}}\left(k-\sum_{i=1}^{n-1}z_{i}-\mu_{n}\right) ^{2}\right]\cdot\prod_{i=1}^{n-1}\frac{1}{\sqrt{2\pi\sigma_{i}^{2}}}\exp \left[-\frac{1}{2\sigma_{i}^{2}}\left(z_{i}-\mu_{i}\right)^{2}\right]\] \[=\left(2\pi\right)^{-\frac{n}{2}}\left(\prod_{i=1}^{n}\sigma_{i} \right)^{-1}\] \[\exp\left[-\frac{\left(\frac{k^{2}-2k\sum_{i=1}^{n-1}z_{i}-2k\mu _{n}+\left(\sum_{i=1}^{n-1}z_{i}\right)^{2}+2\mu_{n}\sum_{i=1}^{n-1}z_{i}+\mu _{n}^{2}}+\sum_{i=1}^{n-1}\frac{z_{i}^{2}-2z_{i}\mu_{i}+\mu_{n}^{2}}{\sigma_{ i}^{2}}\right)}{2}\right]\]

Now, we only consider the terms in the exponential function without \(-\frac{1}{2}\).

\[\sum_{i=1}^{n-1}\frac{z_{i}^{2}}{\sigma_{i}^{2}}+\sum_{i=1}^{n-1} \left(-\frac{2k}{\sigma_{n}^{2}}+\frac{2\mu_{n}}{\sigma_{n}^{2}}-\frac{2\mu_{ i}}{\sigma_{i}^{2}}\right)z_{i}+\left(-\frac{2k\mu_{n}}{\sigma_{n}^{2}}+\frac{k^{2}}{ \sigma_{n}^{2}}+\frac{\mu_{n}^{2}}{\sigma_{n}^{2}}+\sum_{i=1}^{n-1}\frac{\mu_ {i}^{2}}{\sigma_{i}^{2}}\right)+\frac{(\sum_{i=1}^{n-1}z_{i})^{2}}{\sigma_{n}^ {2}}\]

Notice that \((\sum_{i=1}^{n-1}z_{i})^{2}=\sum_{i=1}^{n-1}z_{i}^{2}+\sum_{i=1}^{n-1}\sum_{j=1,j\neq i}^{n-1}z_{i}z_{j}\). Then, our equation becomes

\[\sum_{i=1}^{n-1}\frac{z_{i}^{2}}{\sigma_{i}^{2}}+\sum_{i=1}^{n-1} \left(-\frac{2k}{\sigma_{n}^{2}}+\frac{2\mu_{n}}{\sigma_{n}^{2}}-\frac{2\mu_{ i}}{\sigma_{i}^{2}}\right)z_{i}+\left(-\frac{2k\mu_{n}}{\sigma_{n}^{2}}+\frac{k^{2}}{ \sigma_{n}^{2}}+\frac{\mu_{n}^{2}}{\sigma_{n}^{2}}+\sum_{i=1}^{n-1}\frac{\mu_ {i}^{2}}{\sigma_{i}^{2}}\right)\] \[\quad+\frac{\sum_{i=1}^{n-1}z_{i}^{2}+\sum_{i=1}^{n-1}\sum_{j=1,j\neq i}^{n-1}z_{i}z_{j}}{\sigma_{n}^{2}}\] \[= \sum_{i=1}^{n-1}\left(\frac{1}{\sigma_{i}^{2}}+\frac{1}{\sigma_{n} ^{2}}\right)z_{i}^{2}+\sum_{i=1}^{n-1}\left(-\frac{2k}{\sigma_{n}^{2}}+\frac{2 \mu_{n}}{\sigma_{n}^{2}}-\frac{2\mu_{i}}{\sigma_{i}^{2}}\right)z_{i}+\left(- \frac{2k\mu_{n}}{\sigma_{n}^{2}}+\frac{k^{2}}{\sigma_{n}^{2}}+\frac{\mu_{n}^{2} }{\sigma_{n}^{2}}+\sum_{i=1}^{n-1}\frac{\mu_{i}^{2}}{\sigma_{i}^{2}}\right)\] \[\quad+\frac{\sum_{i=1}^{n-1}\sum_{j=1,j\neq i}^{n-1}z_{i}z_{j}}{ \sigma_{n}^{2}}\] \[= \sum_{i=1}^{n-1}\left[\left(\frac{1}{\sigma_{i}^{2}}+\frac{1}{ \sigma_{n}^{2}}\right)z_{i}^{2}+\frac{\sum_{j=1,j\neq i}^{n-1}z_{j}}{\sigma_{n}^ {2}}z_{i}+\left(-\frac{2k}{\sigma_{n}^{2}}+\frac{2\mu_{n}}{\sigma_{n}^{2}}-\frac{2 \mu_{i}}{\sigma_{i}^{2}}\right)z_{i}\right]\] \[\quad+\left(-\frac{2k\mu_{n}}{\sigma_{n}^{2}}+\frac{k^{2}}{ \sigma_{n}^{2}}+\frac{\mu_{n}^{2}}{\sigma_{n}^{2}}+\sum_{i=1}^{n-1}\frac{\mu_{ i}^{2}}{\sigma_{i}^{2}}\right)\]

Then, we consider an arbitrary \(n-1\) dimensional multivariate normal distribution with mean \(\overline{\mu}\) and variance \(\overline{\bm{\Sigma}}\).It's p.d.f. is given by

\[(2\pi)^{-\frac{n-1}{2}}\det\overline{\bm{\Sigma}}^{-\frac{1}{2}}\exp\left(- \frac{1}{2}(\overline{\bm{z}}-\overline{\mu})^{T}\overline{\bm{\Sigma}}^{-1}( \overline{\bm{z}}-\overline{\mu})\right)\]

[MISSING_PAGE_FAIL:10]

Consider the following Lemma Miller [1981]

**Lemma 1**.: _Let \(\mathbf{G}\) and \(\mathbf{H}\) be arbitrary square matrices of the same dimension. If \(\mathbf{G}\) and \(\mathbf{G}+\mathbf{H}\) are nonsigular and \(\mathbf{H}\) has rank one, then_

\[(\mathbf{G}+\mathbf{H})^{-1}=\mathbf{G}^{-1}-\frac{1}{1+g}\mathbf{G}^{-1} \mathbf{H}\mathbf{G}^{-1}\]

_where \(g=tr\left(\mathbf{H}\mathbf{G}^{-1}\right)\)_

Since \(\det\mathbf{A}\) and \(\det(\mathbf{A}+\mathbf{B})\) are nonzero, we know that \(\mathbf{A}\) and \(\mathbf{A}+\mathbf{B}\) are nonsigular. \(\mathbf{B}\) is a rank \(1\) matrix. By the above lemma, we have

\[(\mathbf{A}+\mathbf{B})^{-1}=\mathbf{A}^{-1}-\frac{1}{1+g}\mathbf{A}^{-1} \mathbf{B}\mathbf{A}^{-1}\]

where \(g=tr\left(\mathbf{B}\mathbf{A}^{-1}\right)\) This is equivalent to

\[\overline{\mathbf{\Sigma}}=\mathbf{A}^{-1}-\frac{1}{1+tr(\mathbf{B}\mathbf{A} ^{-1})}\mathbf{A}^{-1}\mathbf{B}\mathbf{A}^{-1}\]

Equation (6) and (7) imply that \(\overline{\mathbf{\Sigma}}^{-1}\) is a symmetric and positive definite matrix. Its inverse \(\overline{\mathbf{\Sigma}}\) is also a symmetric and positive definite matrix. We attempt to find an expression for each element of \(\overline{\mathbf{\Sigma}}\). We first consider \(\mathbf{B}\mathbf{A}^{-1}\).

\[\mathbf{B}\mathbf{A}^{-1} =\begin{pmatrix}\frac{1}{\sigma_{1}^{2}}&\frac{1}{\sigma_{1}^{2} }&\frac{1}{\sigma_{1}^{2}}&\cdots&\frac{1}{\sigma_{1}^{2}}\\ \frac{1}{\sigma_{1}^{2}}&\frac{1}{\sigma_{1}^{2}}&\frac{1}{\sigma_{1}^{2}}& \cdots&\frac{1}{\sigma_{1}^{2}}\\ \frac{1}{\sigma_{1}^{2}}&\frac{1}{\sigma_{1}^{2}}&\frac{1}{\sigma_{1}^{2}}& \cdots&\frac{1}{\sigma_{1}^{2}}\\ \frac{1}{\sigma_{1}^{2}}&\frac{1}{\sigma_{1}^{2}}&\frac{1}{\sigma_{1}^{2}}& \cdots&\frac{1}{\sigma_{1}^{2}}\\ \end{pmatrix}\begin{pmatrix}\sigma_{1}^{2}&0&0&\ldots&0\\ 0&\sigma_{2}^{2}&0&\ldots&0\\ 0&0&\sigma_{3}^{2}&\ldots&0\\ \vdots&\vdots&\vdots&\vdots\\ 0&0&0&\ldots&\sigma_{n-1}^{2}\\ \end{pmatrix}\] \[=\begin{pmatrix}\frac{\sigma_{1}^{2}}{\sigma_{1}^{2}}&\frac{ \sigma_{1}^{2}}{\sigma_{1}^{2}}&\frac{\sigma_{1}^{2}}{\sigma_{1}^{2}}&\cdots& \frac{\sigma_{1}^{2}-1}{\sigma_{1}^{2}}\\ \frac{\sigma_{1}^{2}}{\sigma_{1}^{2}}&\frac{\sigma_{1}^{2}}{\sigma_{1}^{2}}& \cdots&\frac{\sigma_{n-1}^{2}}{\sigma_{1}^{2}}\\ \frac{\sigma_{1}^{2}}{\sigma_{n}^{2}}&\frac{\sigma_{1}^{2}}{\sigma_{1}^{2}}& \cdots&\frac{\sigma_{n-1}^{2}}{\sigma_{n}^{2}}\\ \vdots&\vdots&\vdots&\vdots\\ 0&0&0&\ldots&\sigma_{n-1}^{2}\\ \end{pmatrix}\]

Notice that \(tr(\mathbf{B}\mathbf{A}^{-1})=\sum_{i=1}^{n-1}\frac{\sigma_{i}^{2}}{\sigma_{n} ^{2}}\), so \(1+tr(\mathbf{B}\mathbf{A}^{-1})=\sum_{i=1}^{n}\frac{\sigma_{i}^{2}}{\sigma_{n} ^{2}}\). Then we compute \(\mathbf{A}^{-1}\mathbf{B}\mathbf{A}^{-1}\)

\[\mathbf{A}^{-1}\mathbf{B}\mathbf{A}^{-1} =\begin{pmatrix}\sigma_{1}^{2}&0&0&\ldots&0\\ 0&\sigma_{2}^{2}&0&\ldots&0\\ 0&0&\sigma_{3}^{2}&\ldots&0\\ \vdots&\vdots&\vdots&\vdots\\ 0&0&0&\ldots&\sigma_{n-1}^{2}\\ \end{pmatrix}\begin{pmatrix}\frac{\sigma_{1}^{2}}{\sigma_{n}^{2}}&\frac{ \sigma_{2}^{2}}{\sigma_{n}^{2}}&\frac{\sigma_{3}^{2}}{\sigma_{n}^{2}}&\cdots& \frac{\sigma_{n-1}^{2}}{\sigma_{n}^{2}}\\ \frac{\sigma_{1}^{2}}{\sigma_{n}^{2}}&\frac{\sigma_{1}^{2}}{\sigma_{n}^{2}}& \frac{\sigma_{1}^{2}}{\sigma_{n}^{2}}&\cdots&\frac{\sigma_{n-1}^{2}}{\sigma_{n}^{2}} \\ \vdots&\vdots&\vdots&\vdots\\ \frac{\sigma_{1}^{2}}{\sigma_{n}^{2}}&\frac{\sigma_{1}^{2}}{\sigma_{n}^{2}}& \frac{\sigma_{1}^{2}}{\sigma_{n}^{2}}&\cdots&\frac{\sigma_{n-1}^{2}\sigma_{n}^{2}} {\sigma_{n}^{2}}\\ \vdots&\vdots&\vdots&\vdots\\ \frac{\sigma_{1}^{2}\sigma_{n-1}^{2}}{\sigma_{n}^{2}}&\frac{\sigma_{2}^{2}\sigma_{ n-1}^{2}}{\sigma_{n}^{2}}&\frac{\sigma_{1}^{2}\sigma_{n-1}^{2}}{\sigma_{n}^{2}}& \cdots&\frac{(\sigma_{n-1}^{2})^{2}}{\sigma_{n}^{2}}\\ \end{pmatrix}\]The variance and covariance matrix \(\overline{\bm{\Sigma}}\) becomes

\[\begin{pmatrix}\sigma_{1}^{2}&0&0&\dots&0\\ 0&\sigma_{2}^{2}&0&\dots&0\\ 0&0&\sigma_{3}^{2}&\dots&0\\ \vdots&\vdots&\vdots&&\vdots\\ 0&0&0&\dots&\sigma_{n-1}^{2}\end{pmatrix}-\frac{1}{\sum_{i=1}^{n}\sigma_{i}^{2 }}\begin{pmatrix}\frac{(\sigma_{1}^{2})^{2}}{\sigma_{n}^{2}}&\frac{\sigma_{2}^ {2}\sigma_{1}^{2}}{\sigma_{n}^{2}}&\frac{\sigma_{2}^{2}\sigma_{1}^{2}}{\sigma _{n}^{2}}&\dots&\frac{\sigma_{n-1}^{2}\sigma_{1}^{2}}{\sigma_{n}^{2}}\\ \frac{\sigma_{1}^{2}\sigma_{2}^{2}}{\sigma_{n}^{2}}&\frac{(\sigma_{2}^{2})^{2} }{\sigma_{n}^{2}}&\frac{\sigma_{2}^{2}\sigma_{2}^{2}}{\sigma_{n}^{2}}&\dots& \frac{\sigma_{n-1}^{2}\sigma_{2}^{2}}{\sigma_{n}^{2}}\\ \vdots&\vdots&\vdots&&\vdots\\ \frac{\sigma_{1}^{2}\sigma_{n-1}^{2}}{\sigma_{n}^{2}}&\frac{\sigma_{2}^{2} \sigma_{n-1}^{2}}{\sigma_{n}^{2}}&\frac{\sigma_{2}^{2}\sigma_{n-1}^{2}}{\sigma _{n}^{2}}&\dots&\frac{(\sigma_{n-1}^{2})^{2}}{\sigma_{n}^{2}}\end{pmatrix}\]

Thus, we have the following result:

\[\overline{\bm{\Sigma}}_{i,j}=\begin{cases}\sigma_{i}^{2}-\frac{(\sigma_{i}^{2 })^{2}}{\sum_{i=1}^{n}\sigma_{i}^{2}}&i=j\\ -\frac{\sigma_{i}^{2}\sigma_{i}^{2}}{\sum_{i=1}^{n}\sigma_{i}^{2}}&i\neq j \end{cases}\]

Next, we derive an expression for \(\overline{\mu}\). Since \(\overline{\bm{\Sigma}}^{-1}\) is symmetric, Equation (10) can be transformed into

\[-\sum_{j=1}^{n-1}2a_{i,j}u_{j} =\left(-\frac{2k}{\sigma_{n}^{2}}+\frac{2\mu_{n}}{\sigma_{n}^{2} }-\frac{2\mu_{i}}{\sigma_{i}^{2}}\right)\] \[\sum_{j=1}^{n-1}a_{i,j}u_{j} =\left(\frac{k}{\sigma_{n}^{2}}+\frac{\mu_{i}}{\sigma_{i}^{2}}- \frac{\mu_{n}}{\sigma_{n}^{2}}\right)\]

This is equivalent to

\[\overline{\bm{\Sigma}}^{-1}\overline{\mu}=c\bm{1}+\mu_{\mathbf{reduced}}\oslash \sigma_{\mathbf{reduced}}\]

where \(c=\frac{k-\mu_{n}}{\sigma_{n}^{2}}\), \(\mu_{\mathbf{reduced}}=(\mu_{1},\dots,\mu_{n-1})^{T}\), \(\sigma_{\mathbf{reduced}}=(\sigma_{1}^{2},\dots,\sigma_{n-1}^{2})^{T}\), and \(\oslash\) denotes element-wise division of vectors. The mean \(\mu\) is expressed as

\[\overline{\mu}=\overline{\bm{\Sigma}}(c\bm{1}+\mu_{\mathbf{reduced}}\oslash \sigma_{\mathbf{reduced}})\] (11)

We also attempt to find an element-wise expression for the mean \(\overline{\mu}\) Let's define \(s_{i,j}=\overline{\bm{\Sigma}}_{i,j}\). Then we have

\[s_{i,j}=\mathbb{1}\left[i=j\right]\sigma_{i}^{2}-\frac{\sigma_{i}^{2}\sigma_{j }^{2}}{\sum_{i=1}^{n}\sigma_{i}^{2}}\]

From the equation for \(\overline{\mu}\), we know that

\[\overline{\mu}_{i} =\sum_{j=1}^{n-1}s_{i,j}(c+\frac{\mu_{i}}{\sigma_{i}^{2}})\] \[=\sum_{j=1}^{n-1}\left(\mathbb{1}\left[i=j\right]\sigma_{i}^{2}- \frac{\sigma_{i}^{2}\sigma_{j}^{2}}{\sum_{i=1}^{n}\sigma_{i}^{2}}\right)\left( c+\frac{\mu_{j}}{\sigma_{j}^{2}}\right)\]

Finally, we deal with the constant terms in the exponent.

\[-\frac{2k\mu_{n}}{\sigma_{n}^{2}}+\frac{k^{2}}{\sigma_{n}^{2}}+\frac{\mu_{n}^{ 2}}{\sigma_{n}^{2}}+\sum_{i=1}^{n-1}\frac{\mu_{i}^{2}}{\sigma_{i}^{2}}\] (12)

\[\sum_{i=1}^{n-1}\overline{\mu}_{i}\sum_{j=1}^{n-1}a_{i,j}\overline{\mu}_{j}\] (13)

Equation (12) is the constant term in the exponential function in the probability distribution function derived by taking the cross section of our \(n\) dimensional multivariate normal distribution and a hyperplane. Equation (13) is the constant term in the exponential function in the probability distribution function of an arbitrary \(n-1\) dimensional multivariate normal distribution. The scaling term from the exponential term is given by

\[-\frac{2k\mu_{n}}{\sigma_{n}^{2}}+\frac{k^{2}}{\sigma_{n}^{2}}+\frac {\mu_{n}^{2}}{\sigma_{n}^{2}}+\sum_{i=1}^{n-1}\frac{\mu_{i}^{2}}{\sigma_{i}^{2} }-\sum_{i=1}^{n-1}\overline{\mu}_{i}\sum_{j=1}^{n-1}a_{i,j}\overline{\mu_{j}}\] \[= \frac{(\mu_{n}-k)^{2}}{\sigma_{n}^{2}}+\mathbf{1}^{T}(\mu_{\mathbf{ reduced},\mathbf{squared}}\oslash\sigma_{\mathbf{reduced}})-\overline{\mu}^{T} \overline{\mathbf{\Sigma}}^{-1}\overline{\mu}\]

where \(\mu_{\mathbf{reduced},\mathbf{squared}}=(\mu_{1}^{2},\dots,\mu_{n-1}^{2})^{T}\). We define

\[D=\exp\left[-\frac{1}{2}\left(\frac{(\mu_{n}-k)^{2}}{\sigma_{n}^{2}}+ \mathbf{1}^{T}\left(\mu_{\mathbf{reduced},\mathbf{squared}}\oslash\sigma_{ \mathbf{reduced}}\right)-\overline{\mu}^{T}\overline{\mathbf{\Sigma}}^{-1} \overline{\mu}\right)\right]\]

This is our scaling term from the exponent. Finally, we consider the constant term in the front.

\[(2\pi)^{-\frac{n}{2}}\left(\prod_{i=1}^{n}\sigma_{i}\right)^{-1}=(2\pi)^{- \frac{1}{2}}\frac{\left(\prod_{i=1}^{n}\sigma_{i}\right)^{-1}}{\det\overline{ \mathbf{\Sigma}}^{-\frac{1}{2}}}\cdot(2\pi)^{-\frac{n-1}{2}}\det\overline{ \mathbf{\Sigma}}^{-\frac{1}{2}}\]

\((2\pi)^{-\frac{n}{2}}\left(\prod_{i=1}^{n}\sigma_{i}\right)^{-1}\) is the constant term of the multivariate normal truncated by the hyperplane, and \((2\pi)^{-\frac{n-1}{2}}\det\overline{\mathbf{\Sigma}}^{-\frac{1}{2}}\) is the constant term of an arbitrary \(n-1\) dimensional multivariate normal. The scaling term is \(E=(2\pi)^{-\frac{1}{2}}\frac{\left(\prod_{i=1}^{n}\sigma_{i}\right)^{-1}}{\det \overline{\mathbf{\Sigma}}^{-\frac{1}{2}}}\). Thus, our conditional distribution is a \(n-1\) dimensional multivariate normal distribution with p.d.f. given by

\[p\left(\mathbf{z}\mid\sum_{j=1}^{n}z_{j}=k\right)=C\cdot D\cdot E\cdot(2\pi)^{ -\frac{n-1}{2}}\det\overline{\mathbf{\Sigma}}^{-\frac{1}{2}}\exp\left(-\frac{ 1}{2}(\overline{\mathbf{z}}-\overline{\mu})^{T}\overline{\mathbf{\Sigma}}^{-1 }(\overline{\mathbf{z}}-\overline{\mu})\right)\]

where \(\overline{\mathbf{z}}=(z_{1},\dots,z_{n-1})^{T}\). 

### Proposition 2

Proof.: Let \(\bm{z}=(z_{1},\dots,z_{n})^{T}\), where \(z_{i}\sim Poisson(\theta_{i})\). We attempt to compute a closed-form solution for the conditional probability \(p\left(\bm{z}\mid\sum_{j=1}^{n}z_{j}=k\right)\).

\[p\left((\bm{z}|\sum_{j=1}^{n}z_{j}=k\right)=\frac{p\left(\bm{z}\cap\sum z_{i}= k\right)}{p\left(\sum_{j=1}^{n}z_{j}=k\right)}\]

Let \(Y=\sum_{j=1}^{n}z_{j}\). The denominator is the p.d.f. of \(Y\) evaluated at \(k\). Since \(Y\) is a linear combination of independent Poisson random variables, we know \(Y\sim Poisson(\sum_{j=1}^{n}\theta_{j})\). Thus,

\[p\left(\sum_{j=1}^{n}z_{j}=k\right)=\frac{e^{-\sum_{j=1}^{n}\theta_{j}}\left( \sum_{j=1}^{n}\theta_{j}\right)^{k}}{k!}\]

Next, let's consider the numerator.

\[p(\mathbf{z}\cap\sum_{j=1}^{n}z_{j}=k)=\begin{cases}p(\mathbf{z})&\sum_{j=1}^ {n}z_{j}=k\\ 0&\sum_{j=1}^{n}z_{j}\neq k\end{cases}\]where \(p(\bm{z})=\prod_{i=1}^{n}f(z_{i})=\prod_{i=1}^{n}\frac{e^{-\varepsilon^{\prime}_{i} \theta_{i}^{\varepsilon_{i}}}}{z_{i}!}\). Thus, our conditional distribution is given by

\[p(\mathbf{z}|\sum_{j=1}^{n}z_{j}=k) =\begin{cases}\frac{e^{-\sum_{i=1}^{n}\theta_{i}^{\varepsilon_{i }}}\frac{1}{\prod_{i=1}^{n}\theta_{i}^{\varepsilon_{i}}}}{\frac{e^{-\sum_{i=1} ^{n}\theta_{i}^{\varepsilon_{i}}}(\sum_{i=1}^{n}\theta_{i})^{k}}{k^{k}}}&\sum_ {j=1}^{n}z_{j}=k\\ 0&\sum_{j=1}^{n}z_{j}\neq k\end{cases}\] \[=\begin{cases}\frac{k!\prod_{i=1}^{n}\theta_{i}^{\varepsilon_{i}}}{ (\sum_{i=1}^{n}\theta_{i})^{k}\prod_{i=1}^{n}z_{i}!}&\sum_{j=1}^{n}z_{j}=k\\ 0&\sum_{j=1}^{n}z_{j}\neq k\end{cases}\] \[=\begin{cases}\frac{(\sum_{i=1}^{n}\theta_{i})^{k}}{\sum_{i=1}^{ n}z_{i}!}\prod_{i=1}^{n}\theta_{i}^{\varepsilon_{i}}&\sum_{j=1}^{n}z_{j}=k\\ 0&\sum_{j=1}^{n}z_{j}\neq k\end{cases}\] \[=\begin{cases}\frac{k!}{\prod_{i=1}^{n}z_{i}!}\prod_{i=1}^{n} \left(\frac{\theta_{i}}{\sum_{j=1}^{n}\theta_{j}}\right)^{z_{i}}&\sum_{j=1}^{ n}z_{j}=k\\ 0&\sum_{j=1}^{n}z_{j}=k\\ 0&\sum_{j=1}^{n}z_{j}\neq k\end{cases}\] \[=f\left(\mathbf{z};k,\frac{\theta_{1}}{\sum_{j=1}^{n}\theta_{j}}, \ldots,\frac{\theta_{n}}{\sum_{j=1}^{n}\theta_{j}}\right)\]

where \(f\left(\mathbf{z};k,\frac{\theta_{1}}{\sum_{j=1}^{n}\theta_{j}},\ldots,\frac{ \theta_{n}}{\sum_{j=1}^{n}\theta_{j}}\right)\) is the probability mass function of a multinomial distribution with parameter \(k\) and \(\frac{\theta_{1}}{\sum_{j=1}^{n}\theta_{j}},\ldots,\frac{\theta_{n}}{\sum_{j=1 }^{n}\theta_{j}}\). 

### Proposition 3

Proof.: Let \(\bm{z}=(z_{1},\ldots,z_{n})^{T}\), where \(z_{i}\sim\mathcal{N}(\mu_{i},\sigma_{i}^{2})\). We attemp to compute a closed-form solution for the conditional marginal of \(z_{i}\), \(p(z_{i}\mid\sum_{j=1}^{n}z_{j}=k)\). We first derive the joint distribution of \(z_{i}\) and \(\sum_{j=1}^{n}z_{j}\). Consider the following affine transformation

\[\mathbf{A}\mathbf{z}=\begin{pmatrix}0&\ldots&1&\ldots&0\\ 1&\ldots&1&\ldots&1\end{pmatrix}\begin{pmatrix}z_{1}\\ \vdots\\ z_{i}\\ \vdots\\ z_{n}\end{pmatrix}=\begin{pmatrix}z_{i}\\ \sum_{j=1}^{n}z_{j}\end{pmatrix}\]

The first row of matrix \(\mathbf{A}\) has \(1\) at i-th column and \(0\) everywhere, and the last row of matrix \(\mathbf{A}\) has \(1\) everywhere.

**Theorem 2**.: _Let \(\mathbf{Y}\sim\mathcal{N}_{n}(\mu,\bm{\Sigma})\), and let \(A\) be an \(m\times n\) matrix of rank \(m\). Then, \(\mathbf{A}\mathbf{Y}\sim\mathcal{N}_{m}(\mathbf{A}\mu,\mathbf{A}\bm{\Sigma} \mathbf{A}^{T})\) Gut [2009]_

Since matrix \(\mathbf{A}\) is full rank, by Theorem 2, \((z_{i},\sum_{j=1}^{n}z_{j})^{T}\) follows a \(2\) dimensional multivariate normal distribution with mean and variance computed as follows.

\[\mathbf{A}\mu=\begin{pmatrix}0&\ldots&1&\ldots&0\\ 1&\ldots&1&\ldots&1\end{pmatrix}\begin{pmatrix}\mu_{1}\\ \vdots\\ \mu_{i}\\ \vdots\\ \mu_{n}\end{pmatrix}=\begin{pmatrix}\mu_{i}\\ \sum_{j=1}^{n}\mu_{j}\end{pmatrix}\]

\[\mathbf{A}\bm{\Sigma}\mathbf{A}^{T}=\begin{pmatrix}\sigma_{1}^{2}&\sigma_{1}^ {2}\\ \sigma_{1}^{2}&\sum_{j=1}^{n}\sigma_{j}^{2}\end{pmatrix}\]

**Theorem 3**.: _Suppose that \(\mathbf{Y}\), \(\mu\), and \(\bm{\Sigma}\) are partitioned as \(\mathbf{Y}=\begin{pmatrix}\mathbf{Y_{1}}\\ \mathbf{Y_{2}}\end{pmatrix}\),\(\mu=\begin{pmatrix}\mu_{1}\\ \mu_{2}\end{pmatrix},\bm{\Sigma}=\begin{pmatrix}\bm{\Sigma_{11}}&\bm{\Sigma_{12}} \\ \bm{\Sigma_{21}}&\bm{\Sigma_{22}}\end{pmatrix}\), and \(\mathbf{Y}\sim\mathcal{N}(\mu,\bm{\Sigma})\). It can be shown that the conditional distribution of \(\mathbf{Y_{1}}\) given _is also multivariate normal, \(\mathbf{Y_{1}}\mid\mathbf{Y_{2}}\sim N(\mu_{\mathbf{1}\mid\mathbf{2}},\mathbf{ \Sigma}_{\mathbf{1}\mid\mathbf{2}})\), where \(\mu_{\mathbf{1}\mid\mathbf{2}}=\mu_{\mathbf{1}}+\mathbf{\Sigma_{12}\Sigma_{22}}^ {-1}(\mathbf{Y_{2}}-\mu_{\mathbf{2}})\), and \(\mathbf{\Sigma_{1}\mid\mathbf{2}}=\mathbf{\Sigma_{11}}-\mathbf{\Sigma_{12} \Sigma_{22}}^{-1}\mathbf{\Sigma_{21}}\) Holt and Nguyen [2023]_

We apply Theorem 3 to derive the conditional distribution. \(z_{i}\mid\sum_{j=1}^{n}z_{j}\sim\mathcal{N}(\tilde{\mu}_{i},\tilde{\sigma}_{i} ^{2})\), where the mean and variance are computed as follows:

\[\tilde{\mu}_{i}=\mu_{i}+\frac{\sigma_{i}^{2}}{\sum_{j=1}^{n}\sigma_{j}^{2}}(k- \sum_{j=1}^{n}\mu_{j})\]

\[\tilde{\sigma}_{i}^{2}=\sigma_{i}^{2}-\sigma_{i}^{2}\frac{1}{\sum_{j=1}^{n} \sigma_{j}^{2}}\sigma_{i}^{2}=\sigma_{i}^{2}-\frac{(\sigma_{i}^{2})^{2}}{\sum_ {j=1}^{n}\sigma_{j}^{2}}\]

### Proposition 4

Proof.: Let \(\bm{z}=(z_{1},\dots,z_{n})^{T}\), where \(z_{i}\sim Poisson(\theta_{i})\). We attempt to compute a closed-form solution for the conditional marginal \(p(z_{i}\mid\sum_{j=1}^{n}z_{n}=k)\).

\[p\left(z_{i}\mid\sum_{j=1}^{n}z_{j}=k\right) =\sum\cdots\sum(z_{(z_{1},\dots,z_{i-1},z_{i+1},\dots,z_{n});\sum_ {j=1}^{n}z_{j}=k}p(\mathbf{z}\mid\sum z_{i}=k)\] \[=\sum\cdots\sum(z_{1},\dots,z_{i-1},z_{i+1},\dots,z_{n});\sum_{j=1 }^{n}z_{j}=k}^{f}\left(\mathbf{z};k,\frac{\theta_{1}}{\sum_{j=1}^{n}\theta_{j }},\dots,\frac{\theta_{n}}{\sum_{j=1}^{n}\theta_{j}}\right)\]

Since the marginal of each variable of a multinomial distribution is a binomial distribution, then the conditional marginal is

\[p\left(z_{i}\mid\sum_{j=1}^{n}z_{j}=k\right)=\binom{k}{z_{i}}\left(\frac{ \theta_{i}}{\sum_{j=1}^{n}\theta_{j}}\right)^{z_{i}}\left(1-\frac{\theta_{i}}{ \sum_{j=1}^{n}\theta_{j}}\right)^{n-z_{i}}\]

This is the probability mass function of a binomial distribution with parameter \(k\) and probability \(\frac{\theta_{i}}{\sum_{j=1}^{n}\theta_{j}}\). 

### Proposition 5

Proof.: Let \(\bm{z}=(z_{1},\dots,z_{n})^{T}\), where \(z_{i}\sim\mathcal{N}(\mu_{i},\sigma_{i}^{2})\). Let \(\mathbf{b}=\left(b_{1},b_{2},\dots,b_{n}\right)^{T}\) be the ground truth logits subject to the equality constraint \(\sum_{j=1}^{n}b_{j}=k\). We attempt to derive a closed-form solution for the L1 loss of \(\bm{z}\) subject to the constraint \(\sum_{j=1}^{n}z_{j}=k\).

\[L(\theta) =\mathrm{E}_{\mathbf{z}\sim p_{\theta}(\mathbf{z}\mid\sum_{i}z_ {i}=0)}[\parallel\mathbf{z}-\mathbf{b}\parallel_{1}]\] \[=\sum_{i=1}^{n}\mathrm{E}_{\mathbf{z}\sim p_{\theta}(\mathbf{z} \mid\sum_{i}z_{i}=0)}[\parallel z_{i}-b_{i}\parallel_{1}]\]

From previous derivation, we know that the conditional distribution of \(z_{i}\) subject to the equality constraint is an univariate normal distribution wit mean \(\tilde{\mu}_{i}=\mu_{i}+\frac{\sigma_{i}^{2}}{\sum_{j=1}^{n}\sigma_{j}^{2}}(k- \sum_{j=1}^{n}\mu_{j})\) and variance \(\tilde{\sigma}_{i}^{2}=\sigma_{i}^{2}-\frac{(\sigma_{i}^{2})^{2}}{\sum_{j=1}^{n }\sigma_{j}^{2}}\). Let's define \(y_{i}=z_{i}-b_{i}\). Then, \(y_{i}\sim N\left(\tilde{\mu}_{i}-b_{i},\tilde{\sigma_{i}^{2}}\right)\). Thus, \(\mathrm{E}_{\mathbf{z}\sim p_{\theta}(\mathbf{z}\mid\sum_{i}z_{i}=0)}[\parallel y _{i}\parallel]\) is the mean of a folded normal distribution.

\[\sum_{i=1}^{n}\mathrm{E}_{\mathbf{z}\sim p_{\theta}(\mathbf{z} \mid\sum_{i}z_{i}=0)}[\parallel y_{i}\parallel] =\sum_{i=1}^{n}\sigma_{y_{i}}\sqrt{\frac{2}{\pi}}\exp\left(\frac{ -\mu_{y_{i}}^{2}}{2\sigma_{y_{i}}^{2}}\right)+\mu_{y_{i}}erf\left(\frac{\mu_{y_ {i}}}{\sqrt{2\sigma_{y_{i}}^{2}}}\right)\] \[=\sum_{i=1}^{n}\overline{\sigma_{i}}\sqrt{\frac{2}{\pi}}\exp\left( \frac{-\left(\overline{\mu_{i}}-b_{i}\right)^{2}}{2\overline{\sigma_{i}^{2}}} \right)+\left(\overline{\mu_{i}}-b_{i}\right)erf\left(\frac{\overline{\mu_{i}}-b_ {i}}{\sqrt{2\overline{\sigma_{i}^{2}}}}\right)\]We also attempt to derive a closed-form solution for the L2 loss of \(\bm{z}\) subject to the constraint \(\sum_{j=1}^{n}z_{j}=k\).

\[L(\theta) =\mathds{E}_{\bm{z}\sim p_{\theta}(\bm{x}|\sum_{i}z_{i}=0)}[\parallel \bm{z}-\bm{b}\parallel_{2}^{2}]\] \[=\sum_{i=1}^{n}\mathds{E}_{\bm{z}\sim p_{\theta}(\bm{x}|\sum_{i}z _{i}=0)}[z_{i}^{2}]-2\sum_{i=1}^{n}\mathds{E}_{\bm{z}\sim p_{\theta}(\bm{x}| \sum_{i}z_{i}=0)}[z_{i}b_{i}]+\sum_{i=1}^{n}\mathds{E}_{\bm{z}\sim p_{\theta}( \bm{x}|\sum_{i}z_{i}=0)}[b_{i}^{2}]\]

Since we assume \(z_{i}\) and \(b_{i}\) are independent, and \(\bm{b}\) is the constant ground truth vector.

\[L(\theta) =\sum_{i=1}^{n}\mathds{E}_{\bm{z}\sim p_{\theta}(\bm{x}|\sum_{i}z _{i}=0)}[z_{i}^{2}]-2\sum_{i=1}^{n}b_{i}\mathds{E}_{\bm{z}\sim p_{\theta}(\bm{ x}|\sum_{i}z_{i}=0)}[z_{i}]+\sum_{i=1}^{n}b_{i}^{2}\] \[=\sum_{i=1}^{n}\mathds{E}_{z_{i}\sim p_{\theta}(z_{i}|\sum_{i}z _{i}=0)}[z_{i}^{2}]-2\sum_{i=1}^{n}b_{i}\mathds{E}_{z_{i}\sim p_{\theta}(z_{i }|\sum_{i}z_{i}=0)}[z_{i}]+\sum_{i=1}^{n}b_{i}^{2}\]

From previous derivation, we know that the conditional distribution of \(z_{i}\) is \(p\left(z_{i}\mid\sum_{j=1}^{n}z_{j}=k\right)=f\left(z_{i};\tilde{\mu}_{i}=\mu_ {i}+\frac{\sigma_{i}^{2}}{\sum_{j=1}^{n}\sigma_{j}^{2}}(k-\sum_{j=1}^{n}\mu_{ j}),\tilde{\sigma}_{i}^{2}=\sigma_{i}^{2}-\frac{(\sigma_{i}^{2})^{2}}{\sum_{j=1}^{n} \sigma_{j}^{2}}\right)\). The expectation in the first term is the second moment of this gaussian distribution.

\[\sum_{i=1}^{n}\mathds{E}_{z_{i}\sim p(z_{i}|\sum_{i}z_{i}=0)}[z_{i}^{2}]=\sum _{i=1}^{n}\left[\left(\mu_{i}-\frac{\sigma_{i}^{2}\sum_{j=1}^{n}\mu_{j}}{\sum_ {j=1}^{n}\sigma_{j}^{2}}\right)^{2}+\sigma_{i}^{2}-\frac{(\sigma_{i}^{2})^{2}} {\sum_{j=1}^{n}\sigma_{j}^{2}}\right]\]

Likewise, the expectation in the second term is the mean of this gassuain distribution.

\[\sum_{i=1}^{n}b_{i}\mathds{E}_{z_{i}\sim p_{\theta}(z_{i}|\sum_{i}z_{i})}=\sum _{i=1}^{n}b_{i}\left(\mu_{i}-\frac{\sigma_{i}^{2}\sum_{j=1}^{n}\mu_{j}}{\sum_ {j=1}^{n}\sigma_{j}^{2}}\right)\]

### Proposition 6

Proof.: Let \(\bm{z}=(z_{1},\ldots,z_{n})^{T}\), where \(z_{i}\sim Poisson(\theta_{i})\). Let \(\bm{b}=(b_{1},b_{2},\ldots,b_{n})^{T}\) be the ground truth vector subject to the equality constraint \(\sum_{j=1}^{n}b_{j}=k\). We attempt to derive a closed-form solution for the L2 loss of \(\bm{z}\) subject to the constraint \(\sum_{j=1}^{n}z_{j}=k\).

\[L(\theta) =\mathds{E}_{\bm{z}\sim p_{\theta}(\bm{x}|\sum_{i}z_{i}=0)}[ \parallel\bm{z}-\bm{b}\parallel_{2}^{2}]\] \[=\sum_{i=1}^{n}\mathds{E}_{z_{i}\sim p_{\theta}(z_{i}|\sum_{j}z_ {j}=0)}[z_{i}^{2}]-2\sum_{i=1}^{n}b_{i}\mathds{E}_{z_{i}\sim p_{\theta}(z_{i}| \sum_{j}z_{j}=0)}[z_{i}]+\sum_{i=1}^{n}b_{i}^{2}\]

Since the conditional marginal distribution is a binomial distribution, it's second moment is given by

\[\sum_{i=1}^{n}\mathds{E}_{z_{i}\sim p_{\theta}(z_{i}|\sum_{j}z_{j}=0)}[z_{i}^{2 }]=\sum_{i=1}^{n}\left[k\left(\frac{\theta_{i}}{\sum_{j=1}^{n}\theta_{j}} \right)\left(1-\frac{\theta_{i}}{\sum_{j=1}^{n}\theta_{j}}\right)+k^{2}\left( \frac{\theta_{i}}{\sum_{j=1}^{n}\theta_{j}}\right)^{2}\right]\]

It's first moment(mean) is given by

\[-2\sum_{i=1}^{n}b_{i}\mathds{E}_{z_{i}\sim p_{\theta}(z_{i}|\sum_{j}z_{j}=0)}[ z_{i}]=-2k\sum_{i=1}^{n}b_{i}\left(\frac{\theta_{i}}{\sum_{j=1}^{n}\theta_{j}}\right)\]

Thus, we have

\[\sum_{i=1}^{n}\mathds{E}_{z_{i}\sim p_{\theta}(z_{i}|\sum_{j}z_{j} =0)}[z_{i}^{2}]=\sum_{i=1}^{n}\left[k\left(\frac{\theta_{i}}{\sum_{j=1}^{n} \theta_{j}}\right)\left(1-\frac{\theta_{i}}{\sum_{j=1}^{n}\theta_{j}}\right)+k^ {2}\left(\frac{\theta_{i}}{\sum_{j=1}^{n}\theta_{j}}\right)^{2}\right]\] \[-2k\sum_{i=1}^{n}b_{i}\left(\frac{\theta_{i}}{\sum_{j=1}^{n} \theta_{j}}\right)+\sum_{i=1}^{n}b_{i}^{2}\]