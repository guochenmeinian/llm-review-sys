# Towards Anytime Classification in Early-Exit Architectures by Enforcing Conditional Monotonicity

Metod Jazbec

UvA-Bosch Delta Lab

University of Amsterdam

m.jazbec@uva.nl

&James Urquhart Allingham

University of Cambridge

jua23@cam.ac.uk

Dan Zhang

Bosch Center for AI &

University of Tubingen

dan.zhang2@de.bosch.com

&Eric Nalisnick

UvA-Bosch Delta Lab

University of Amsterdam

e.t.nalisnick@uva.nl

###### Abstract

Modern predictive models are often deployed to environments in which computational budgets are dynamic. Anytime algorithms are well-suited to such environments as, at any point during computation, they can output a prediction whose quality is a function of computation time. Early-exit neural networks have garnered attention in the context of anytime computation due to their capability to provide intermediate predictions at various stages throughout the network. However, we demonstrate that current early-exit networks are not directly applicable to anytime settings, as the quality of predictions for individual data points is not guaranteed to improve with longer computation. To address this shortcoming, we propose an elegant post-hoc modification, based on the Product-of-Experts, that encourages an early-exit network to become gradually confident. This gives our deep models the property of _conditional monotonicity_ in the prediction quality--an essential stepping stone towards truly anytime predictive modeling using early-exit architectures. Our empirical results on standard image-classification tasks demonstrate that such behaviors can be achieved while preserving competitive accuracy on average.

## 1 Introduction

When deployed in the wild, predictive models are often subject to dynamic constraints on their computation. Consider a vision system for an autonomous vehicle: the same system should perform well in busy urban environments and on deserted rural roads. Yet the former environment often requires quick decision-making, unlike the latter. Hence, it would be beneficial for our models to be allowed to 'think' as long as its environment allows. Deploying models to resource-constrained devices has similar demands. For example, there are roughly \(25,000\) different computing configurations of Android devices (Yu et al., 2019), and developers often need to balance poor user experience in low-resource settings with under-performance in high-resource settings.

_Anytime algorithms_ can ease, if not resolve, these issues brought about by dynamic constraints on computations. An algorithm is called _anytime_ if it can quickly produce an approximate solution and subsequently employ additional resources, if available, to incrementally decrease the approximation error (Dean and Boddy, 1988; Zilberstein, 1996).1 Anytime computation is especially valuable when resources can fluctuate during test time. Returning to the example of mobile devices, an anytimealgorithm would be able to gracefully adapt to the computational budget of whatever class of device it finds itself on: low-performing devices will depend on intermediate (but still good) solutions, while high-performing devices can exploit the model's maximum abilities.

_Early-exit neural networks_ (EENNs) (Teerapititayanon et al., 2016) have attracted attention as anytime predictors, due to their architecture that can generate predictions at multiple depths (Huang et al., 2018). However, as we demonstrate, EENNs do not exhibit _anytime_ behavior. Specifically, current EENNs do not possess _conditional monotonicity_, meaning that the prediction quality for individual data points can deteriorate with longer computation at test time. This lack of monotonicity leads to a decoupling of computation time and output quality--behavior that is decisively not _anytime_.

In this work, we improve EENNs' ability to perform anytime predictive modeling. We propose a lightweight transformation that strongly encourages monotonicity in EENNs' output probabilities as they evolve with network depth. Our method is based on a Product-of-Experts formulation (Hinton, 1999) and exploits the fact that a product of probability distributions approximates an 'and' operation. Thus by taking the product over all early-exits computed thus-far, the aggregated predictive distribution is gradually refined over time (exits) to have a non-increasing support. This transformation has an inductive bias that encourages _anytime uncertainty_: the first exits produce high-entropy predictive distributions--due to their large support--and the EENN becomes progressively confident at each additional exit (by decreasing the support and reallocating probability).

In our experiments, we demonstrate across a range of vision classification tasks that our method leads to significant improvements in conditional monotonicity in a variety of backbone models _without re-training_. Moreover, our transformation preserves the original model's average accuracy and even improves it in some cases (e.g. CIFAR-100). Thus our work allows the large pool of efficient architectures to be endowed with better anytime properties, all with minimal implementation overhead.

## 2 Background & Setting of Interest

DataOur primary focus is multi-class classification.2 Let \(\mathcal{X}\subseteq\mathbb{R}^{d}\) denote the feature space, and let \(\mathcal{Y}\) denote the label space, which we assume to be a categorical encoding of multiple (\(K\)) classes. \(\bm{x}\in\mathcal{X}\) denotes a feature vector, and \(y\in\mathcal{Y}\) denotes the class (\(1\) of \(K\)). We assume \(\bm{x}\) and \(y\) are realizations of the random variables \(\mathbf{x}\) and \(\mathbf{y}\) with the (unknown) data distribution \(p(\mathbf{x},\mathbf{y})=p(\mathbf{y}|\mathbf{x})\ p(\mathbf{x})\). The training data sample is \(\mathcal{D}=\{(\bm{x}_{n},y_{n})\}_{n=1}^{N}\). Our experiments focus on vision tasks in which \(\bm{x}\) is a natural image; in the appendix, we also report results on natural language.

Footnote 2: See Appendix B.6 for discussion of anytime regression.

Early-exit Neural Networks: Model and TrainingOur model of interest is the _early-exit neural network_(Teerapititayanon et al., 2016) (EENN). For a given data-point \(\bm{x}\), an EENN defines \(M\) total models--\(p\left(\mathbf{y}\,|\,\mathbf{x}=\bm{x},\,\bm{\phi}_{m},\,\bm{\theta}_{m} \right),\ m=1,\ldots,M\), where \(\bm{\phi}_{m}\) denotes the parameters for the \(m\)th classifier head and \(\bm{\theta}_{m}\) are the parameters of the backbone architecture. Each set of parameters \(\bm{\theta}_{m}\) contains the parameters of all previous models: \(\bm{\theta}_{1}\subset\bm{\theta}_{2}\subset\ldots\subset\bm{\theta}_{M}\). Due to this nested construction, each model can be thought of as a preliminary 'exit' to the full architecture defined by \(\bm{\theta}_{M}\). This enables the network's computation to be'short-circuited' and terminated early at exit \(m\), without evaluating later models. Training EENNs is usually done by fitting all exits simultaneously:

\[\ell\left(\bm{\phi}_{1},\ldots,\bm{\phi}_{M},\bm{\theta}_{1},\ldots,\bm{\theta }_{M};\mathcal{D}\right)=-\sum_{n=1}^{N}\sum_{m=1}^{M}w_{m}\cdot\log p\left( \mathbf{y}=y_{n}\,|\,\mathbf{x}=\bm{x}_{n},\,\bm{\phi}_{m},\,\bm{\theta}_{m}\right)\] (1)

where \(w_{m}\in\mathbb{R}^{+}\) are weights that control the degree to which each exit is trained. The weights are most often set to be uniform across all exits. For notational brevity, we will denote \(p\left(\mathbf{y}=y\,|\,\mathbf{x}=\bm{x},\,\,\bm{\phi}_{m},\,\bm{\theta}_{m}\right)\) as \(p_{m}\left(y\,|\,\bm{x}\right)\) henceforth.

Setting of InterestAt test time, there are various ways to exploit the advantages of an EENN. For example, computation could be halted if an input is deemed sufficiently easy such that a weaker model (e.g. \(p_{1}\) or \(p_{2}\)) can already make an accurate prediction, thus accelerating inference time. Such _efficient_ computation has been the central focus of the EENNs's literature thus far (Matsubara et al., 2023) and is sometimes referred to as _budgeted batch classification_(Huang et al., 2018).

However, in this work we focus exclusively on the related, yet distinct, _anytime_ setting motivated in the Introduction. We assume computation is terminated when the current environment cannot support computing and evaluating the \((m+1)\)th model. These constraints originate solely from the environment, meaning the user or model developer has no control over them. We assume the model will continue to run until it either receives such an external signal to halt or the final--i.e., \(M\)th--model is computed (Grubb and Bagnell, 2012). Furthermore, in line with previous literature on anytime algorithms (Huang et al., 2018; Hu et al., 2019), we assume the model sees one data point at a time when deployed, i.e., the batch size is one at test time.

Anytime Predictive ModelsTo have the potential to be called 'anytime,' an algorithm must produce a sequence of intermediate solutions. In the case of classification, an anytime predictive model must hence output a series of probability distributions over \(\mathcal{Y}\) with the goal of forming better and better approximations of the true conditional data distribution \(p\left(\mathbf{y}\,|\,\mathbf{x}\right)\). Specifically, we are interested in the evolution of the _conditional error_ for a given \(\boldsymbol{x}\in\mathcal{X}\), defined here as \(\epsilon_{t}(\boldsymbol{x}):=D\big{(}p\left(\mathbf{y}\,|\,\boldsymbol{x} \right),\;p_{t}\left(\mathbf{y}\,|\,\boldsymbol{x}\right)\big{)}\) where \(D\) denotes a suitable distance or divergence (e.g., KL-divergence or total-variation), and \(p_{t}\) represents a model's predictive distribution after computing for time \(t\in\mathbb{R}_{\geq 0}\). In addition to conditional, i.e., instance-level, error we can also consider the evolution of a _marginal error_ defined as \(\bar{\epsilon}_{t}:=\mathbb{E}_{\boldsymbol{x}\sim p\left(\mathbf{x}\right)} \big{[}\epsilon_{t}(\boldsymbol{x})\big{]}\). For the remainder of this section, we will use \(\epsilon_{t}\) to refer to both versions of the error and revisit their difference in Section 3. Since this paper focuses on anytime EENNs,3 we use the early-exit index \(m\) as the time index \(t\).

Footnote 3: While EENNs are the focus of our work, they are not the only model class that can be used for anytime prediction. In theory, any model that yields a sequence of predictions could be considered.

Properties of Anytime ModelsZilberstein (1996) enumerates properties that an algorithm should possess to be _anytime_. Below we summarize Zilberstein (1996)'s points that are relevant to predictive modeling with EENNs, ordering them in a hierarchy of necessity:

1. _Interruptibility_: The most basic requirement is that the model can be stopped and produce an answer at any time. Traditional neural networks do not satisfy interruptibility since they produce a single output after evaluating all layers. However, EENNs do by returning the softmax probabilities for the \(m\)th model if the \((m+1)\)th model has not been computed yet.
2. _Monotonicity_: The quality of the intermediate solutions should not decrease with computation time. Or equivalently, expending more compute time should guarantee performance does not degrade. For EENNs, monotonicity is achieved if the error \(\epsilon_{m}\) is non-increasing across early-exits: \(\epsilon_{1}\geq\ldots\geq\epsilon_{M}\).
3. _Consistency_: The quality of the intermediate solutions should be correlated with computation time. For EENNs, consistency is of most concern when the amount of computation varies across sub-models. For example, if computing the \((m+1)\)th model requires evaluating \(10\times\) more residual blocks than the \(m\)th model required, then a consistent architecture should produce a roughly \(10\times\) better solution (\(\epsilon_{m+1}\approx\epsilon_{m}/10\)).
4. _Diminishing Returns_: The difference in quality between consecutive solutions is gradually decreasing, with the largest jumps occurring early, e.g. \((\epsilon_{1}-\epsilon_{2})\gg(\epsilon_{M-1}-\epsilon_{M})\). Monotonicity and consistency need to be satisfied in order for this stronger property to be guaranteed.

For a traditional EENN (trained with Equation 1), property #1 is satisfied by construction (by the presence of early-exits). Property #2 is not explicitly encouraged by the typical EENN architecture or training objective. Hence, we begin the next section by investigating if current EENNs demonstrate any signs of monotonicity. Properties #3 and #4 are left to future work since, as we will demonstrate, achieving property #2 is non-trivial and is thus the focus of the remainder of this paper.

Estimating Anytime PropertiesThe error sequence \(\{\epsilon_{m}\}_{m=1}^{M}\) is essential for studying the anytime properties of EENNs. We can equivalently consider the evolution of _prediction quality_ measures \(\gamma_{m}\) and aim for a model that exhibits increasing quality, i.e., \(\gamma_{m}\leq\gamma_{m+1},\forall m\). For the remainder of the paper, we will work with the notion of prediction quality unless otherwise stated. Observe that each \(\gamma_{m}\) (and equivalently, \(\epsilon_{m}\)) is always unknown, as it depends on the unknown data distribution \(p\left(\mathbf{y}\,|\,\mathbf{x}\right)\). Consequently, we must rely on estimates \(\hat{\gamma}_{m}\). In the presence of a labeled hold-out dataset, we can use ground-truth labels \(y^{*}\in\mathcal{Y}\) and consider the correctness of the prediction as our estimator:\(\hat{\gamma}_{m}^{c}(\bm{x}):=[\arg\max_{y}p_{m}\left(y\,|\,\bm{x}\right)=y^{*}]\) where \([\cdot]\) is the Iverson bracket.4 However, such an estimator provides a crude signal (0 or 1) on a conditional level (per data point). As such, it is not congruent with conditional monotonicity or consistency. Due to this, we focus primarily on the probability of the ground-truth class \(\hat{\gamma}_{m}^{p}(\bm{x}):=p_{m}\left(y^{*}\,|\,\bm{x}\right)\) when examining anytime properties at the conditional level. Finally, estimating marginal prediction quality (error) requires access to \(p\left(\mathbf{x}\right)\). Following common practice, we approximate \(\mathbb{E}_{\bm{x}\sim p\left(\mathbf{x}\right)}\) by averaging over the hold-out dataset. Averaging \(\hat{\gamma}_{m}^{c}(\bm{x})\) and \(\hat{\gamma}_{m}^{p}(\bm{x})\) corresponds to the test accuracy and the average ground-truth probability at the \(m\)th exit, respectively.

Footnote 4: The Iverson bracket \([\texttt{cond}]\) is 1 if \(\texttt{cond}\) is true and 0 otherwise.

## 3 Checking for Monotonicity in Early-Exit Neural Networks

In this section, we take a closer look at _state-of-the-art_ early-exit neural networks and their monotonicity, a key property of anytime models. Our results show that EENNs are marginally monotone but lack conditional monotonicity in the prediction quality. For ease of presentation, we focus here on a particular EENN (_Multi-Scale Dense Net_ (MSDNet); Huang et al. (2018)) and dataset (CIFAR-100). Section 6 shows that this observation generalizes across other EENNs and datasets.

Marginal MonotonicityThe left plot of Figure 1 reports MSDNet's test accuracy (blue) on CIFAR-100 as a function of the early exits. Accuracy has been the primary focus in the anytime EENN literature Huang et al. (2018); Hu et al. (2019), and as expected, it increases at each early exit \(m\). Thus MSDNet clearly exhibits _marginal monotonicity_: monotonicity on average over the test set. The same figure also shows that the average ground-truth probability (orange) increases at each \(m\), meaning that MSDNet demonstrates marginal monotonicity in this related quantity as well.

Conditional MonotonicityYet for real-time decision-making systems, such as autonomous driving, it is usually not enough to have marginal monotonicity. Rather, we want _conditional monotonicity_: that our predictive model will improve instep with computation _for every input_. If conditional monotonicity is achieved, then marginal monotonicity is also guaranteed. However, the converse is not true, making conditional monotonicity the stronger property.

To check if MSDNet demonstrates conditional monotonicity, examine the middle plot of Figure 1. Here we plot the probability of the true label, i.e., \(\hat{\gamma}_{m}^{p}(\bm{x})\), as a function of the early exits for \(10\) randomly sampled test points (again from CIFAR-100). We see that--despite MSDNet being marginally monotonic--it is quite unstable in the probabilities it produces per exit. For example, the purple line exhibits confidence in the correct answer (\(\sim 75\%\)) at the second exit, drops to near zero by the fourth exit, becomes confident again (\(\sim 80\%\)) at exits \(5\) and \(6\), and finally falls to near zero again at the last exit. Thus MSDNet clearly does not exhibit the stronger form of monotonicity.

We next quantify the degree to which MSDNet is violating conditional monotonicity. First, we define the _maximum decrease_ in the ground-truth probability as \(\text{MPD}^{*}(\bm{x}):=\max_{m^{\prime}>m}\big{\{}\max(\hat{\gamma}_{m}^{p}( \bm{x})-\hat{\gamma}_{m^{\prime}}^{p}(\bm{x}),0)\big{\}}\). For various thresholds \(\tau\in[0,1]\), we count how many test instances exhibit a probability decrease that exceeds the given threshold: \(N_{\tau}:=\sum_{m}[\text{MPD}^{*}(\bm{x}_{n})\geq\tau]\). The results are depicted in the right panel of Figure 1. We see that for \(\sim 25\%\) of test points, the ground-truth probability decreases by at least \(0.5\) at some exit. In a separate analysis (see Appendix B.1.2), when examining the correctness of prediction measure \(\hat{\gamma}_{m}^{c}(\bm{x})\), we find that \(\sim 30\%\) of test examples exhibit decreasing trajectories across early exits.5 A similar finding was reported by Kaya et al. (2019). Both observations demonstrate that MSDNet, a state-of-the-art EENN, does not exhibit conditional monotonicity nor is it close to achieving it.

Footnote 5: This implies that the model incorrectly classifies the data point after correctly predicting it at some of the earlier exits: \(\hat{\gamma}_{m}^{c}(\bm{x})=1\) and \(\hat{\gamma}_{m^{\prime}}^{c}(\bm{x})=0\) for some \(m^{\prime}>m\).

## 4 Transformations for Conditional Monotonicity

In light of the aforementioned challenges with current EENNs in the anytime setting, we explore potential lightweight modifications that could encourage, or even enforce, conditional monotonicity. We first suggest a strong baseline for encouraging monotonicity, taking inspiration from Zilberstein (1996). We then introduce our primary methodology, which is based on product ensembles.

### Caching Anytime Predictors

As noted by Zilberstein (1996), conditional monotonicity can be encouraged by caching the best prediction made so far and returning it (instead of the latest prediction). Yet this requires the ability to measure the prediction quality of intermediate results. While previously introduced quantities, such as ground-truth probability \(\hat{\gamma}_{m}^{p}(\bm{x})\), can serve to examine anytime properties before deploying the model, their reliance on the true label \(y^{*}\) renders them unsuitable for measuring prediction quality at test time. As a proxy, we propose using the EENN's internal measure of confidence. We found that simply using the softmax confidence performs well in practice. At an exit \(m\), we compute \(C(p_{m},\bm{x}):=\max_{y}p_{m}(y|\bm{x})\) and overwrite the cache if \(C(p_{j},\bm{x})<C(p_{m},\bm{x})\), where \(j\) is the exit index of the currently cached prediction. We call this approach _Caching Anytime_ (CA) and consider it as a (strong) baseline. While CA is rather straightforward, it has been thus far overlooked in the anytime EENN literature, as the standard approach is to return the latest prediction Huang et al. (2018); Hu et al. (2019). Note that if the anytime model displays conditional monotonicity to begin with, applying CA has no effect since the most recent prediction is always at least as good as the cached one.

### Product Anytime Predictors

Our proposed method for achieving conditional monotonicity in EENNs is motivated by the following idealized construction.

Guaranteed Conditional Monotonicity via Hard Product-of-ExpertsConsider an EENN whose sub-classifiers produce 'hard' one-vs-rest probabilities: each class has either probability \(0\) or \(1\) (before normalization). Now define the predictive distribution at the \(m\)th exit to be a Product-of-Experts Hinton (1999) (PoE) ensemble involving the current and all preceding exits:

\[p_{\Pi,m}\left(y\,|\,\bm{x}\right):=\frac{1}{Z_{m}}\prod_{i=1}^{m}\left[f_{i} (\bm{x})_{y}>b\right]^{w_{i}},\quad Z_{m}=\sum_{y^{\prime}\in\{1,\dots,K\}} \prod_{i=1}^{m}\left[f_{i}(\bm{x})_{y^{\prime}}>b\right]^{w_{i}}\] (2)

where \(b\) is a threshold parameter, \([\cdot]\) is an Iverson bracket, \(f_{i}(\bm{x})\in\mathbb{R}^{K}\) represents a vector of logits at \(i\)th exit, and \(w_{i}\in\mathbb{R}^{+}\) are ensemble weights. Assuming that a class \(y\) is in the support of the final classifier for a given \(\bm{x}\), i.e. \(p_{\Pi,M}\left(y\,|\,\bm{x}\right)>0\), then such a model is _guaranteed_ to be monotone in the probability of \(y\):

\[p_{\Pi,m}\left(y\,|\,\bm{x}\right)\ \leq\ p_{\Pi,m+1}\left(y\,|\,\bm{x} \right),\ \forall\ m\.\] (3)

The formal proof can be found in Appendix A. Consequently, if the true class \(y^{*}\) is in the final support, an EENN with this construction is conditionally monotonic in the ground-truth probability \(\hat{\gamma}_{m}^{p}(\bm{x})\) (as defined in Section 2).

The intuition behind this construction is that at each exit, the newly computed ensemble member casts a binary vote for each class. If a class has received a vote at every exit thus far, then it remains in the support of \(p_{\Pi,m}\left(\mathbf{y}\,|\,\bm{x}\right)\). Otherwise, the label drops out of the support and never re-enters. At each

Figure 1: Multi-Scale Dense Network (MSDNet; Huang et al. (2018)) results on the test set of CIFAR-100. _Left_: test accuracy and average ground-truth probability at each early exit, the two measures of marginal monotonicity we study in this work. _Middle_: ground-truth probability trajectories (i.e., \(\{\hat{\gamma}_{m}^{p}(\bm{x})\}_{m=1}^{M}\)) for 10 random test data points. _Right_: aggregation of analysis on conditional monotonicity for MSDNet. Specifically, we compute the maximum decrease in ground-truth probability for each test data point and then plot the percentage of test examples where the drop exceeds various thresholds. A red dashed line denotes the performance of an oracle model with perfect conditional monotonicity, i.e., none of the data points exhibit any decrease in the ground-truth probability.

exit, the set of candidate labels can only either remain the same or be reduced, thus concentrating probability in a non-increasing subset of classes. This construction also has an _underconfidence bias_: the probability of all labels starts small due to many labels being in the support at the earliest exits. Consequently, the model is encouraged to exhibit high uncertainty early on and then to become progressively confident at subsequent exits.

A Practical Relaxation using ReLUAlthough the above construction provides perfect conditional monotonicity (for labels in the final support), it often results in a considerable decrease in marginal accuracy (see Appendix B.2). This is not surprising since the \(0-1\) probabilities result in 'blunt' predictions. At every exit, all in-support labels have equal probability, and thus the model must select a label from the support at random in order to generate a prediction. The support is often large, especially at the early exits, and in turn the model devolves to random guessing.

To overcome this limitation, first notice that using \(0-1\) probabilities is equivalent to using a Heaviside function to map logits to probabilities.6 In turn, we propose to relax the parameterization, replacing the Heaviside with the ReLU activation function. The ReLU has the nice property that it preserves the rank of the'surviving' logits while still 'nullifying' classes with low logits. Yet this relaxation comes at a price: using ReLUs allows conditional monotonicity to be violated. However, as we will show in Section 6, the violations are rare and well worth the improvements in accuracy. Moreover, the number of violations can be controlled by applying an upper limit to the logits, as detailed in Appendix B.8.

Footnote 6: In the most widely used softmax parameterization of the categorical likelihood, the exponential function is used when transforming logits into probabilities.

Therefore our final _Product Anytime_ (PA) method for conditional monotonicity is written as:

\[p_{\mathrm{PA},m}\left(y\,|\,\boldsymbol{x}\right):=\frac{1}{Z_{m}}\prod_{i=1 }^{m}\max\left(f_{i}(\boldsymbol{x})_{y},\;0\right)^{w_{i}},\quad Z_{m}=\sum_ {y^{\prime}\in\{1,\ldots,K\}}\prod_{i=1}^{m}\max\left(f_{i}(\boldsymbol{x})_{ y^{\prime}},\;0\right)^{w_{i}}\] (4)

where \(\max(\cdot,\cdot)\) returns the maximum of the two arguments. We set the ensemble weights to \(w_{i}=i/M\) since we expect later experts to be stronger predictors. Note that PA's predictive distribution may degenerate into a _zero_ distribution such that \(p_{\mathrm{PA},m}\left(y\,|\,\boldsymbol{x}\right)=0,\;\forall\;y\in\mathcal{Y}\). This occurs when the ensemble's experts have non-overlapping support and are thus unable to arrive at a consensus prediction. If a prediction is required, we fall back on the softmax predictive distribution based on the logits of the classifier at the current exit.

Learning and Post-Hoc ApplicationEquation 4 can be used for training an EENN directly via the negative log-likelihood. However, one can also take pre-trained EENNs (e.g., as open-sourced by the original authors) and apply Equation 4 _post-hoc_. For this post-hoc application, the EENN's logits are input into Equation 4 rather than into the usual softmax transformation. As we will demonstrate in the experiments, post-hoc application endows the model with conditional monotonicity without any substantial degradation to the original model's accuracy. In cases such as CIFAR-100, we find that our method actually _improves_ the accuracy of the pre-trained model. The only metric we find to be sensitive to training from scratch vs post-hoc application is confidence calibration, with the former providing better calibration properties. This result will be demonstrated in Figure 6. Thus, we currently recommend using our method via post-hoc application, as it requires minimal implementation overhead (about three lines of code) and allows the user to easily toggle between anytime and traditional behavior.

For a visualization of how our post-hoc PA affects probability trajectories, see Figure 2. PA manages to rectify the backbone EENN's non-monotone behavior to a large extent. As a concrete example of how this can help with anytime predictions, we can consider again the scenario of Android

Figure 2: Ground-truth probability trajectories (i.e., \(\{\hat{\gamma}_{m}^{p}(\boldsymbol{x})\}_{m=1}^{M}\)) for 10 random test data points after applying our PA method. For comparison, MSDNet’s trajectories from Figure 1 are reproduced in the background.

phones introduced in Section 1. Deploying the original MSDNet might result in certain data points receiving inferior predictions on a higher-spec device compared to a device with limited computational capabilities. However, if we apply our PA to an MSDNet, such inconsistencies become far less likely due to the model's monotonic behavior.

## 5 Related work

_Anytime Algorithms_(Dean and Boddy, 1988, Zilberstein, 1996) have long been studied outside the context of neural networks. For instance, Wellman and Liu (1994) considered Bayesian networks in anytime settings, while Grubb and Bagnell (2012) proposed an anytime Ada-Boost algorithm (Bishop, 2007) and also studied Haar-like features (Viola and Jones, 2001) for anytime object detection. More recently, modern neural networks have been used in models with only _marginal_ anytime properties (Huang et al., 2018, Hu et al., 2019, Liu et al., 2022). Our research improves upon this work by bringing these models a step closer to having per-data-point anytime prediction, a much stronger form of anytime computation.

_Early-Exit Neural Networks_(Teerapittayanon et al., 2016, Yang et al., 2020, Han et al., 2022) have been investigated for applications in computer vision (Huang et al., 2018, Kaya et al., 2019) and natural language processing (Schwartz et al., 2020, Schuster et al., 2021, Xu and McAuley, 2023). A wide variety of backbone architectures has been explored, encompassing recurrent neural networks (Graves, 2016), convolutional networks (Huang et al., 2018), and transformers (Wang et al., 2021). Furthermore, ensembling of EENNs intermediate predictions has been considered (Qendro et al., 2021, Wolczyk et al., 2021, Meronen et al., 2023), primarily to enhance their uncertainty quantification capabilities. Our objective is to leverage the recent advancements in early-exit networks and make them suitable for true (conditional) anytime predictive modeling.

Related to our critique of current EENNs, Kaya et al. (2019) and Wolczyk et al. (2021) also observed that prediction quality can decline at later exits, calling the phenomenon 'overthinking.' These works address the problem via methods for early termination. Our work aims to impose monotonicity on the ground-truth probabilities, which eliminates 'overthinking' by definition. We further examine the connection to Kaya et al. (2019) and Wolczyk et al. (2021) in Appendix B.5.

_Product-of-Experts_(Hinton, 1999, 2002) ensembles have been extensively studied in the generative modeling domain due to their ability to yield sharper distributions compared to Mixture-of-Experts ensembles (Wu and Goodman, 2018, Huang et al., 2022). However, in the supervised setting, PoE ensembles have received relatively little attention (Pignat et al., 2022). PoE is often applied only post-hoc, likely due to training difficulties (Cao and Fleet, 2014, Watson and Morimoto, 2022).

## 6 Experiments

We conduct two sets of experiments.7 First, in Section 6.1, we verify that our method (PA) maintains strong average performance while significantly improving conditional monotonicity in state-of-the-art EENNs making them more suitable for the anytime prediction task. To check that our findings generalize across data modalities, we also conduct NLP experiments and present the results in Appendix B.4. In the second set of experiments, detailed in Section 6.2, we move beyond prediction quality and shift focus to monotonicity's effect on uncertainty quantification in the anytime setting. Lastly, we point out some limitations in Section 6.3.

Footnote 7: Our code is publicly available at https://github.com/metodj/AnytimeClassification.

DatasetsWe consider CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), and ILSVRC 2012 (ImageNet; Deng et al. (2009)). The CIFAR datasets each contain \(50\)k training and \(10\)k test images, while ImageNet is a larger dataset with 1.2M training and \(50\)k test instances. The three datasets consist of 10, 100, and 1000 classes, respectively.

BaselinesWe use four state-of-the-art EENNs as baselines and backbone architectures. The first is the Multi-Scale Dense Network (MSDNet; Huang et al. (2018)), which consists of stacked convolutional blocks. To maximize performance at each early-exit, the authors introduce: (1) feature maps on multiple scales, ensuring that coarse-level features are available all throughout the network,

[MISSING_PAGE_FAIL:8]

to probabilities) when it comes to achieving more monotone behavior. We additionally explored if calibrating the backbone EENN would be enough to make the underlying model monotone. While post-hoc calibrating every exit does help with monotonicity, it does not perform as well as PA; see Appendix B.9.

Monotonicity in CorrectnessOur PA approach yields close to perfect conditional monotonicity when considering probability as the estimator of prediction quality. Yet it is also worth checking if other estimators are monotonic, such as the correctness of the prediction, i.e., \(\hat{\gamma}_{m}^{c}(\bm{x})\). For correctness, we observe similar improvements in conditional monotonicity; see Appendix B.1.2 for a detailed analysis. For example, on CIFAR-100, MSDNet 'forgets' the correct answer in \(\sim 30\%\) of test cases, whereas applying post-hoc PA reduces these monotonicity violations to \(\sim 13\%\) of cases.

### Anytime Uncertainty

We also examine the uncertainty quantification abilities of these anytime models. Again consider Figure 2, the probability trajectories obtained from post-hoc PA. The probabilities start low at first few exits, indicating high uncertainty in the early stages of anytime evaluation. The left plot of Figure 5 further supports this insight, showing that the average entropy of the PA predictive distributions is high and reduces at each early exit. Lastly, in the right plot, we present the average size of _conformal sets_8 at each exit. The set size reflects the model's predictive uncertainty, with larger sets indicating more uncertainty. Thus we see that PA is best at uncertainty quantification: it is quite uncertain at the earliest exits (set size of \(\sim 11\)) but grows twice as confident by the last exit (set size of \(\sim 4\)). CA and regular MSDNet keep a relatively constant set size by comparison, only varying between

Figure 4: The \(\%\) of test examples with a ground-truth probability drop exceeding a particular threshold. Our PA significantly improves the conditional monotonicity of ground-truth probabilities across various datasets and backbone models. To better illustrate the different behavior of various methods, a log scale is used for the y-axis. We report average monotonicity together with one standard deviation based on \(n=3\) independent runs. For DViT (Wang et al., 2021), we were unable to perform multiple independent runs, and report the results for various models instantiations instead, see Appendix B.11. Note that for PA, the monotonicity curves on ImageNet (almost perfectly) overlap at \(y=0\) for all backbone models considered. We also report results for the L2W model in Appendix B.11, which are largely the same as for MSDNet.

Figure 5: Uncertainty measures for the MSDNet model on the CIFAR-100. _Left_: average test entropy of (categorical) predictive distributions for each early exit. _Right_: mean size of conformal sets at each exit point. To obtain conformal scores, we used Regularized Adaptive Predictive Sets algorithm (RAPS; Angelopoulos et al., 2021). \(20\%\) of test examples were used as a hold-out set to calculate conformal quantiles at each early exit.

\(11\) and \(7.5\). Hence, the monotonicity properties of PA result in an appealing link between uncertainty and computation time; and we term this property as _anytime uncertainty_. This is an appropriate inductive bias for anytime predictive models since, intuitively, making quicker decisions should entail a higher level of uncertainty. Lastly, in Appendix B.7, we present results of conditional level experiments where we find that our PA yields significantly more monotone (i.e., non-increasing) uncertainty patterns.

### Limitations

Label Sets with Small CardinalityWe observe that PA's performance in terms of conditional monotonicity improves as the number of classes \(|\mathcal{Y}|\) increases. For instance, PA yields fully monotone trajectories on ImageNet, but not for datasets with smaller number of classes like CIFAR-10, where we detect small violations. This finding is further supported by our NLP experiments in Appendix B.4, where \(|\mathcal{Y}|\) is smaller compared to the image classification datasets considered in this section. Based on this finding, we recommend using the CA approach over PA for scenarios in which \(|\mathcal{Y}|<5\).

Calibration GapDue to its aforementioned underconfidence bias, post-hoc PA generates poorly calibrated probabilities during the initial exits, as illustrated in the left plot of Figure 6. Yet this is primarily observed in _expected calibration error_ (ECE) and related metrics. It disappears when considering some alternative metrics to ECE, as shown in the right plot of Figure 12. Finetuning with PA as an optimization objective, as opposed to applying it post-hoc, closes the calibration gap in most cases, albeit at some cost to accuracy and monotonicity. See Figure 6 (_right_) for concrete examples of how PA with finetuning affects ground-truth probability trajectories. In addition, we experimented with adaptive thresholding in PA, using different thresholds \(b\) for distinct data points, rather than employing 0 for all data points as with ReLU, and found that it can also be beneficial in terms of calibration. For more details on finetuning and adaptive thresholding, as well as a discussion on the relevance of ECE-like metrics in the anytime setting, see Appendix B.3.

## 7 Conclusion

We have demonstrated that expending more time during prediction in current early-exit neural networks can lead to reduced prediction quality, rendering these models unsuitable for direct application in anytime settings. To address this issue, we proposed a post-hoc modification based on Product-of-Experts and showed through empirical and theoretical analysis that it greatly improves conditional monotonicity with respect to prediction quality over time while preserving competitive overall performance.

In future work, we aim to further improve EENNs by imbuing them with other critical anytime properties, such as diminishing returns and consistency. Moreover, it would be interesting to study the impact of monotonicity in other settings, such as budgeted batch classification. We report some preliminary results in Appendix B.10, finding the monotonicity to be less beneficial there compared to the anytime setting, which was the focus of our work. We also plan to delve deeper into the concept of anytime uncertainty. Although we have (briefly) touched upon it in this study, we believe a more comprehensive and formal definition is necessary for the development of robust and reliable anytime models.

Figure 6: Calibration analysis for MSDNet on CIFAR-100. _Left_: expected calibration error (ECE) during anytime evaluation. _Right_: Ground-truth probability trajectories for 10 random test data points after applying our PA with finetuning (PA-FT) method. For comparison, MSDNet’s trajectories from Figure 1 are reproduced in the background.

## Acknowledgments and Disclosure of Funding

We thank Alexander Timans and Rajeev Verma for helpful discussions. We are also grateful to the anonymous reviewers who helped us improve our work with their constructive feedback. MJ and EN are generously supported by the Bosch Center for Artificial Intelligence. JUA acknowledges funding from the EPSRC, the Michael E. Fisher Studentship in Machine Learning, and the Qualcomm Innovation Fellowship.

## References

* Allingham and Nalisnick (2022) J. U. Allingham and E. Nalisnick. A Product of Experts Approach to Early-Exit Ensembles. Technical report, 2022.
* Angelopoulos et al. (2021) A. N. Angelopoulos, S. Bates, M. I. Jordan, and J. Malik. Uncertainty sets for image classifiers using conformal prediction. In _9th International Conference on Learning Representations, ICLR_, 2021.
* Antoran et al. (2020) J. Antoran, J. U. Allingham, and J. M. Hernandez-Lobato. Depth uncertainty in neural networks. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems_, 2020.
* Bishop (2007) C. M. Bishop. _Pattern recognition and machine learning, 5th Edition_. Information science and statistics. Springer, 2007. ISBN 9780387310732.
* Cao and Fleet (2014) Y. Cao and D. J. Fleet. Generalized product of experts for automatic and principled fusion of gaussian process predictions. _CoRR_, 2014. URL http://arxiv.org/abs/1410.7827.
* effortless bayesian deep learning. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems_, 2021.
* Dean and Boddy (1988) T. L. Dean and M. S. Boddy. An analysis of time-dependent planning. In _Proceedings of the 7th National Conference on Artificial Intelligence_. AAAI Press / The MIT Press, 1988.
* Deng et al. (2009) J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In _IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)_, 2009.
* Graves (2016) A. Graves. Adaptive computation time for recurrent neural networks. _CoRR_, 2016. URL http://arxiv.org/abs/1603.08983.
* Grubb and Bagnell (2012) A. Grubb and D. Bagnell. Speedboost: Anytime prediction with uniform near-optimality. In _Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics, AISTATS_, 2012.
* Guo et al. (2017) C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On calibration of modern neural networks. In _Proceedings of the 34th International Conference on Machine Learning, ICML_, 2017.
* Han et al. (2022a) Y. Han, G. Huang, S. Song, L. Yang, H. Wang, and Y. Wang. Dynamic neural networks: A survey. _IEEE Trans. Pattern Anal. Mach. Intell._, 2022a.
* ECCV_. Springer, 2022b.
* Hinton (1999) G. E. Hinton. Products of experts. Technical report, 1999.
* Hinton (2002) G. E. Hinton. Training products of experts by minimizing contrastive divergence. _Neural Comput._, 2002.
* Hu et al. (2019) H. Hu, D. Dey, M. Hebert, and J. A. Bagnell. Learning anytime predictions in neural networks via adaptive loss balancing. In _The Thirty-Third Conference on Artificial Intelligence, AAAI_. AAAI Press, 2019.
* Hu et al. (2019)G. Huang, D. Chen, T. Li, F. Wu, L. van der Maaten, and K. Q. Weinberger. Multi-scale dense networks for resource efficient image classification. In _6th International Conference on Learning Representations, ICLR_, 2018.
* ECCV_. Springer, 2022.
* Kaya et al. [2019] Y. Kaya, S. Hong, and T. Dumitras. Shallow-deep networks: Understanding and mitigating network overthinking. In _Proceedings of the 36th International Conference on Machine Learning, ICML_. PMLR, 2019.
* Kivlichan et al. [2021] I. D. Kivlichan, Z. Lin, J. Z. Liu, and L. Vasserman. Measuring and improving model-moderator collaboration using uncertainty estimation. _CoRR_, 2021. URL https://arxiv.org/abs/2107.04212.
* Krizhevsky et al. [2009] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* Lakshminarayanan et al. [2017] B. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems_, 2017.
* Li et al. [2019] H. Li, H. Zhang, X. Qi, R. Yang, and G. Huang. Improved techniques for training adaptive deep networks. In _IEEE/CVF International Conference on Computer Vision, ICCV_. IEEE, 2019.
* Liu et al. [2022] Z. Liu, Z. Xu, H. Wang, T. Darrell, and E. Shelhamer. Anytime dense prediction with confidence adaptivity. In _The Tenth International Conference on Learning Representations, ICLR_, 2022.
* Matsubara et al. [2023] Y. Matsubara, M. Levorato, and F. Restuccia. Split computing and early exiting for deep learning applications: Survey and research challenges. _ACM Comput. Surv._, 2023.
* Meronen et al. [2023] L. Meronen, M. Trapp, A. Pilzer, L. Yang, and A. Solin. Fixing overconfidence in dynamic neural networks. _CoRR_, 2023. doi: 10.48550/arXiv.2302.06359. URL https://doi.org/10.48550/arXiv.2302.06359.
* Pignat et al. [2022] E. Pignat, J. Silverio, and S. Calinon. Learning from demonstration using products of experts: Applications to manipulation and task prioritization. _Int. J. Robotics Res._, 2022.
* Qendro et al. [2021] L. Qendro, A. Campbell, P. Lio, and C. Mascolo. Early exit ensembles for uncertainty quantification. In _Machine Learning for Health, ML4H@NeurIPS 2021_, Proceedings of Machine Learning Research. PMLR, 2021.
* Schuster et al. [2021] T. Schuster, A. Fisch, T. S. Jaakkola, and R. Barzilay. Consistent accelerated inference via confident adaptive transformers. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP_, 2021.
* Schwartz et al. [2020] R. Schwartz, G. Stanovsky, S. Swayamdipta, J. Dodge, and N. A. Smith. The right tool for the job: Matching model and instance complexities. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL_, 2020.
* Shafer and Vovk [2008] G. Shafer and V. Vovk. A tutorial on conformal prediction. _J. Mach. Learn. Res._, 2008.
* Teerapittayanon et al. [2016] S. Teerapittayanon, B. McDanel, and H. T. Kung. Branchynet: Fast inference via early exiting from deep neural networks. In _23rd International Conference on Pattern Recognition, ICPR_. IEEE, 2016.
* Viola and Jones [2001] P. A. Viola and M. J. Jones. Rapid object detection using a boosted cascade of simple features. In _2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)_, 2001.
* Wang et al. [2021] Y. Wang, R. Huang, S. Song, Z. Huang, and G. Huang. Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems_, 2021.
* Watson and Morimoto [2022] C. Watson and T. K. Morimoto. Learning non-parametric models in real time via online generalized product of experts. _IEEE Robotics Autom. Lett._, 2022.
* Wandler et al. [2018]M. P. Wellman and C. Liu. State-space abstraction for anytime evaluation of probabilistic networks. In _UAI '94: Proceedings of the Tenth Annual Conference on Uncertainty in Artificial Intelligence_, 1994.
* Wolczyk et al. [2021] M. Wolczyk, B. Wojcik, K. Balazy, I. T. Podolak, J. Tabor, M. Smieja, and T. Trzcinski. Zero time waste: Recycling predictions in early exit neural networks. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems_, 2021.
* Wu and Goodman [2018] M. Wu and N. D. Goodman. Multimodal generative models for scalable weakly-supervised learning. In _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems_, 2018.
* Xu and McAuley [2023] C. Xu and J. J. McAuley. A survey on dynamic neural networks for natural language processing. In _Findings of the Association for Computational Linguistics: EACL_, 2023.
* Yang et al. [2020] L. Yang, Y. Han, X. Chen, S. Song, J. Dai, and G. Huang. Resolution adaptive networks for efficient inference. In _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR_, 2020.
* Yu et al. [2019] J. Yu, L. Yang, N. Xu, J. Yang, and T. S. Huang. Slimmable neural networks. In _7th International Conference on Learning Representations, ICLR_, 2019.
* Zilberstein [1996] S. Zilberstein. Using anytime algorithms in intelligent systems. _AI Magazine_, 1996.

The Appendix is structured as follows:

* We provide a proof of conditional guarantees in EENNs for (hard) PoE in Appendix A.
* We present conditional monotonicity results using alternative estimators of performance quality \(\gamma_{m}\) in Appendix B.1.
* We conduct an ablation study for our PA model in Appendix B.2.
* We propose two modifications to our PA that help with reducing the calibration gap in Appendix B.3.
* We report results of NLP experiments in Appendix B.4.
* We delve into the connections between conditional monotonicity and previously introduced concepts in EENNs literature in Appendix B.5.
* We discuss anytime regression and deep ensembles in Appendix B.6.
* We present conditional monotonicity results using uncertainty measures in Appendix B.7.
* We propose a technique for controlling the violations of conditional monotonicity in PA in Appendix B.8.
* We study the impact of calibration on the conditional monotonicity in Appendix B.9.
* We provide budgeted batch classification results in Appendix B.10.
* We provide additional plots in Appendix B.11.

## Appendix A Theoretical Results

In the following, we provide a proof for conditional monotonicity of (hard) Product-of-Experts ensembles presented in Section 4.2.

**Proposition**.: _Let \(p_{\Pi,m}\left(\mathbf{y}\mid\boldsymbol{x}\right)\) represent the Product-of-Experts ensemble of the first \(m\) early-exits. Furthermore, let \(y\in\mathcal{Y}\) denote any class that is present in the support of the full-NN product distribution, i.e., \(p_{\Pi,M}\left(y\mid\boldsymbol{x}\right)>0\). Under the assumption that the Heaviside activation function is used to map logits to probabilities in each \(p_{m}\left(y\mid\boldsymbol{x}\right)\), it follows that:_

\[p_{\Pi,m}\left(y\mid\boldsymbol{x}\right)\leq p_{\Pi,m+1}\left(y\mid \boldsymbol{x}\right)\]

_for every early-exit \(m\) and for every input \(\boldsymbol{x}\)._

Proof.: Using the definition of the PoE ensemble and that of the Heaviside activation function, the predictive likelihood at the \(m\)th early exit can be written as

\[p_{\Pi,m}\left(y\mid\boldsymbol{x}\right):=\frac{1}{Z_{m}}\prod_{i=1}^{m} \left[f_{i}(\boldsymbol{x})_{y}>b\right]^{w_{i}},\quad Z_{m}=\sum_{y^{\prime} \in\left\{1,\ldots,K\right\}}\prod_{i=1}^{m}\left[f_{i}(\boldsymbol{x})_{y^{ \prime}}>b\right]^{w_{i}}\]

where \(f_{i}(\boldsymbol{x})\in\mathbb{R}^{K}\) denotes a vector of logits at the \(i\)-th early-exit, \(b\) represents a chosen threshold, and \(w_{i}\in\mathbb{R}^{+}\) represent ensemble weights. Since we assume that class \(y\) is in the support of the full NN ensemble, i.e., \(p_{\Pi,M}\left(y\mid\boldsymbol{x}\right)>0\), it follows that

\[\prod_{i=1}^{m}\left[f_{i}(\boldsymbol{x})_{\tilde{y}}>b\right]^{w_{i}}=\prod _{i=1}^{m+1}\left[f_{i}(\boldsymbol{x})_{\tilde{y}}>b\right]^{w_{i}}=1,\ \ \forall m=1,\ldots,M-1\,.\]

It then remains to show that the normalization constant is non-increasing, i.e., \(Z_{m}\geq Z_{m+1},\forall m\). To this end observe that \(\left[f_{m+1}(\boldsymbol{x})_{y^{\prime}}>b\right]^{w_{m+1}}\in\left\{0,1 \right\},\ \forall y^{\prime}\in\mathcal{Y}\), using which we proceed as

\[Z_{m}=\sum_{y^{\prime}\in\left\{1,\ldots,K\right\}}\prod_{i=1}^{m}\left[f_{i} (\boldsymbol{x})_{y^{\prime}}>b\right]^{w_{i}}\geq\]

\[\sum_{y^{\prime}\in\left\{1,\ldots,K\right\}}\prod_{i=1}^{m}\left[f_{i}( \boldsymbol{x})_{y^{\prime}}>b\right]^{w_{i}}\cdot\left[f_{m+1}(\boldsymbol{x })_{y^{\prime}}>b\right]^{w_{m+1}}=Z_{m+1},\]

where \(Z_{m}=Z_{m+1}\) if \(\left[f_{m+1}(\boldsymbol{x})_{y^{\prime}}>b\right]^{w_{m+1}}=1\ \forall y^{\prime}\in \mathcal{Y}\) and \(Z_{m}>Z_{m+1}\) otherwise.

Additional Experiments

### Conditional Monotonicity

In the main paper, the ground-truth probability was our focus when studying conditional monotonicity in early-exit networks. Here, we supplement these findings with additional results, utilizing alternative estimators of prediction quality \(\gamma_{m}\).

#### b.1.1 Full-Model Prediction Probability

Suppose only an unlabeled hold-out dataset is available. In that case, the anytime properties of EENNs can still be investigated by relying on the full model prediction \(\hat{y}:=\arg\max_{y}p_{M}\left(y\,|\,\bm{x}\right)\), instead of \(y^{*}\). Naturally, the merit of this alternative hinges on the underlying model being accurate in most cases. Furthermore, besides serving as a substitute for the ground-truth probability in the absence of a labeled dataset, the evolution of the full-model prediction probability can be used as a stand-alone measure of how well early-exits in an EENN approximate the full model.

In Figure 7, we present conditional monotonicity results using \(\hat{y}\) probability as the prediction quality measure. Similar to the results presented in Section 6.1 for \(y^{*}\) probability, we conclude that our PA outperforms both the baseline models and CA approach across all datasets considered.

#### b.1.2 Correctness of Prediction

We next shift our focus to the correctness of prediction as an estimator of performance quality. Recall from Section 2 its definition: \(\hat{\gamma}_{m}^{c}(\bm{x}):=[\arg\max_{y}p_{m}\left(y\,|\,\bm{x}\right)=y^{*}]\). It is a binary measure that can only take on the values of 0 or 1, and thus provides a less nuanced signal about the evolution of the performance quality compared to probability-based measures that evolve more continuously in the interval \([0,1]\).

To aggregate the results of the conditional monotonicity analysis, we consider the following two quantities:

* \(\%_{\text{mono}}\): the percentage of test data points that exhibit fully monotone trajectories in the correctness of the prediction, i.e., \(\hat{\gamma}_{m}^{c}(\bm{x})\leq\hat{\gamma}_{m+1}^{c}(\bm{x}),\forall m\). This implies that if the model managed to arrive at the correct prediction at the \(m\)th exit, it will not forget it at later exits. An oracle model with perfect conditional monotonicity will result in \(\%_{\text{mono}}=100\%\).
* \(\%_{\text{zero}}\): the number of points for which the EENN model is wrong at all exits. This is used to account for trivial solutions of the monotonicity desideratum: \(\hat{\gamma}_{m}^{c}(\bm{x})=0,\;\forall m\). Naturally, the smaller the \(\%_{\text{zero}}\), the better.

Figure 7: The percentage of test examples with a full-model prediction probability drop exceeding a specified threshold. PA significantly improves the conditional monotonicity properties of ground-truth probabilities across various datasets and backbone models. To better illustrate the different behavior of various methods, a log scale is used for y-axis. We report average monotonicity together with one standard deviation based on \(n=3\) independent runs. For DiT [Wang et al., 2021], we were unable to perform multiple independent runs and report the results for various model instantiations instead; see Appendix B.11.

Results are displayed in Table 1. Comparing our PA model against the MSDNet baseline, we observe a significant improvement in terms of the percentage of test data points that exhibit perfect conditional monotonicity, i.e., \(\%_{\text{mono}}\). However, this improvement comes at a cost, as fewer points have a correct prediction for at least one exit. Despite this trade-off, the magnitude of improvement in \(\%_{\text{mono}}\) is larger compared to the degradation of \(\%_{\text{zero}}\) across all datasets considered, which demonstrates that our PA method improves the conditional monotonicity of \(\hat{\gamma}_{m}^{c}(\bm{x})\) in a non-trivial manner.

Although the Caching Anytime (CA) method outperforms PA in terms of \(\%_{\text{mono}}\) across all datasets, it is worth noting that a considerable portion of CA's outperformance can be attributed to trivial solutions (i.e., \(\hat{\gamma}_{m}^{c}(\bm{x})=0,\ \forall m\)), which is reflected by CA's poor performance in terms of \(\%\)zero. Furthermore, the fact that neither CA nor PA fully achieves conditional monotonicity in prediction correctness (i.e., \(\%_{\text{mono}}=100\)) highlights that there is still potential for further improvement in endowing EENNs with conditional monotonicity.

### Product Anytime (PA) Ablations

To better understand the role of different components in our proposed PA method, we conduct several ablations and present the results in Figure 8. In accordance with the theoretical results we presented in Section 4.2, we observe that the PoE ensemble combined with the Heaviside activation (\(\bm{-}\)) yields a model that exhibits complete conditional monotonicity in the full model prediction (i.e., \(\hat{y}\)) probability.9 However, this combination also leads to a model with inferior performance in terms of overall test accuracy (see left plot). As mentioned, this occurs because the model assigns an equal probability to all classes in its support. The ReLU activation resolves this issue by performing on par with Heaviside in terms of monotonicity and on par with exponential, i.e., softmax, in terms of accuracy, thus combining the best of both worlds.

\begin{table}
\begin{tabular}{l c c|c c|c c} \hline \hline  & \multicolumn{2}{c}{CIFAR-10} & \multicolumn{2}{c}{CIFAR-100} & \multicolumn{2}{c}{ImageNet} \\ \cline{2-7}  & \(\%_{\text{mono}}\) (\(\uparrow\)) & \(\%_{\text{zero}}\) (\(\downarrow\)) & \(\%_{\text{mono}}\) (\(\uparrow\)) & \(\%_{\text{zero}}\) (\(\downarrow\)) & \(\%_{\text{mono}}\) (\(\uparrow\)) & \(\%_{\text{zero}}\) (\(\downarrow\)) \\ \hline _MSDNet_ & 91.8 & **3.6** & 70.8 & **12.6** & 84.9 & **20.4** \\ _PA (ours)_ & 96.2 & 4.6 & 87.2 & 16.4 & 93.5 & 24.1 \\ _CA_ & **97.8** & 5.5 & **90.8** & 18.6 & **94.5** & 25.6 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Aggregated results of conditional monotonicity analysis based on correctness of prediction as a performance quality measure. MSDNet (Huang et al., 2018) is used as a backbone model for PA and CA for the results reported here. See Section B.1.2 for definition of \(\%_{\text{mono}}\) and \(\%_{\text{zero}}\).

Figure 8: Test accuracy and conditional monotonicity properties of various PA model instantiations for the CIFAR-100 dataset using MSDNet as a backbone. Specifically, we vary (1) the type of activation function for mapping logits to probabilities and (2) the type of ensembling. Our proposed PA method uses ReLU activation function with product ensembling (\(\bm{-}\)) and is the only method to perform well in terms of both accuracy and monotonicity.

Delving deeper into the ReLU activation function and examining various ensembling styles, we also observe in Figure 8 that Mixture-of-Experts with ReLU (MoE-ReLU;...) demonstrates performance comparable to our proposed PA (PoE-ReLU; \(\boldsymbol{\mathbin{\vbox{\hbox{\scalebox{.5}{$\bullet$}}}\hbox{\scalebox{.5 {$\bullet$}}}}}\)) in terms of both overall accuracy and conditional monotonicity. The MoE predictive distribution at the \(m\)th exit is defined as:

\[p_{\mathrm{MoE},m}\left(y\,|\,\boldsymbol{x}\right):=\sum_{i=1}^{m}w_{i}\cdot p _{i}\left(y\,|\,\boldsymbol{x}\right),\quad p_{i}\left(y\,|\,\boldsymbol{x} \right)=\frac{a\big{(}f_{i}(\boldsymbol{x})_{y}\big{)}}{\sum_{y^{\prime}\in\{1, \ldots,K\}}a\big{(}f_{i}(\boldsymbol{x})_{y^{\prime}}\big{)}}\]

where \(a:\mathbb{R}^{K}\to\mathbb{R}_{\geq 0}\) represents a chosen activation function, such as ReLU, and we utilize uniform weights \(w_{i}=1/m\). Given this observation, one might question why we prefer PoE over MoE in our final PA model. The rationale for this decision is two-fold. First, the proof presented in Appendix A, which serves as the inspiration for our method, is only valid when PoE ensembling is employed. Second, unlike the PoE-based model, the MoE-based model does not exhibit anytime uncertainty. Instead, it demonstrates constant underconfidence across all exits, as illustrated in Figure 9. Considering these factors, we find PoE to be superior to MoE for anytime prediction purposes and incorporate it into our final proposed solution.

Early-Exit Ensembles BaselinesMoE-exponential (\(\cdots\)) and PoE-exponential (\(\boldsymbol{\mathbin{\vbox{\hbox{\scalebox{.5}{$\bullet$}}}\hbox{\scalebox{.5 {$\bullet$}}}}}\)) can also be interpreted as _early-exit ensemble_ baselines for our PA approach. Instead of relying only on the current predictive distributions (as we do when reporting backbone EENNs results in Section 6), we ensemble distributions across all exits up to and including the current one. However, it is clear from Figure 8 that ensembling softmax predictions is insufficient for achieving conditional monotonicity, as both approaches significantly underperform compared to our PA. This underscores the importance of the activation function that maps logits to probabilities for improving monotonicity properties in EENNs. In Figure 10, we verify that these findings hold on the ImageNet dataset as well, where we again find that our PA is the only configuration that performs well both in terms of marginal accuracy and conditional monotonicity.

Sigmoid Activation FunctionSo far, we have discussed why the ReLU function is preferred over the Heaviside or exponential (softmax) functions for transforming logits into probabilities in our setting. We also considered the sigmoid activation function; however, upon further examination, we identified two drawbacks that make it less suitable in our context. Firstly, the sigmoid function lacks the "nullifying" effect of ReLU. As a result, the support for classes diminishes more slowly, and we do not observe the same anytime uncertainty behavior as we do with ReLU (see Section 6.2). Secondly, logits with larger values all map to the same value, approximately 1, under the sigmoid function. This can negatively impact accuracy, as it does not preserve the ranking. Figure 11 illustrates the results using the sigmoid activation function for the CIFAR-100 dataset with MSDNet as a backbone: our choice of ReLU clearly outperforms the sigmoid activation.

Figure 9: Expected Calibration Error (ECE; _left_) and average maximum class probability over the test dataset (_right_) for various instantiations of our PA model using the MSDNet model as a backbone on the CIFAR-100. Only product ensembling (PoE) gives rise to a model that gets progressively more confident with more early exits, as exemplified by decreasing ECE and increasing magnitude of probability. On the contrary, mixture ensembles (MoE) result in constant underconfidence.

### Closing the Calibration Gap

Here we outline two modifications to our proposed PA method aimed at reducing the calibration gap reported in Section 6.3.

#### b.3.1 Adaptive Thresholds

We first introduce an extension of our Product Anytime (PA) method, which enables navigating the monotonicity-calibration trade-off in scenarios when a hold-out validation dataset is available.

Recall that ReLU is used as an activation function in PA to map logits to probabilities. This implies that 0 is used as a threshold, determining which classes'survive' and get propagated further, and which do not. However, 0 is arguably an arbitrary threshold, and one could consider another positive constant \(b\in\mathbb{R}^{+}\) instead. It is important to note that the choice of \(b\) directly influences the size of the support of the predictive likelihood. As illustrated in Figure 13, a larger value of \(b\) results in a smaller number of classes with positive probability (as expected). This larger threshold does also lead to a reduction in the coverage of PA's predictive sets with respect to the ground-truth class. However, the

Figure 11: Test accuracy, conditional monotonicity and ECE results for MSDNet model () on CIFAR-100 dataset. Using sigmoid activation function () hurts both accuracy as well as calibration. Hence, we utilize ReLU activation in our proposed PA ().

Figure 12: Calibration analysis for MSDNet on CIFAR-100. _Left_: expected calibration error (ECE) during anytime evaluation. _Right_: Oracle-Collaborative AUROC with 0.5% deferral [OC AUROC; 1], which simulates a realistic scenario of human-algorithm collaboration by measuring the performance of a classifier when it is able to defer some examples to an oracle, based on its maximum class probability.

Figure 10: Test accuracy and conditional monotonicity properties of various PA model instantiations for the ImageNet dataset using MSDNet as a backbone. Our PA () is the only method to perform well in terms of both accuracy and monotonicity. Note that the monotonicity curves for ReLU and Heaviside activation functions (almost perfectly) overlap at \(y=0\).

decrease in coverage does not significantly affect the overall accuracy of the model, as shown in the far right plot.

Expanding on this concept, one can also consider adaptive thresholding: rather than using the same threshold value for all data points, the model could dynamically adjust the threshold in the activation function based on the current example. Ideally, when the model is confident in its prediction, a higher value of \(b\) is employed, resulting in smaller predictive sets. Conversely, when faced with a more challenging example, a lower threshold is utilized to capture the model's uncertainty.

Switching to ReLU with adaptive thresholding, the predictive likelihood at \(i\)th exit (before ensembling) is given by:

\[p_{i}\left(y\,|\,\bm{x}\right)=\frac{\max\left(f_{i}(\bm{x})_{y},\tau\big{(}f_ {i}(\bm{x})\big{)}\right)}{\sum_{y^{\prime}\in\{1,\ldots,K\}}\max\left(f_{i}( \bm{x})_{y^{\prime}},\tau\big{(}f_{i}(\bm{x})\big{)}\right)}\]

where \(\tau:\mathbb{R}^{K}\rightarrow\mathbb{R}^{+}\) maps the vector of logits \(f(\bm{x})\in\mathbb{R}^{K}\) to a non-negative threshold.10

Footnote 10: In general, \(\tau\) could depend on the datapoint \(\bm{x}\) itself; however, since we desire an efficient model (to avoid adding excessive overhead during inference time), we opt for a more lightweight option based on the processed input, i.e., logits.

Here, we consider threshold functions of the form \(\tau\big{(}f(\bm{x})\big{)}=C\cdot p_{\psi}\big{(}f(\bm{x})\big{)}\), where \(p_{\psi}:\mathbb{R}^{K}\rightarrow[0,1]\) is a model that estimates the probability of the datapoint \(\bm{x}\) being classified correctly, and \(C\) is a constant that ensures the outputs of \(\tau\) have a similar magnitude to that of logits. To estimate \(C\) and \(\psi\), we rely on the hold-out validation dataset. Specifically, we fit a binary logistic regression model using labels \(y_{n}^{\tau}:=[\hat{y}_{n}=y_{n}^{*}]\). Additionally, we only use the \(K=3\) largest logits as input to the regression model, as we have empirically found that excluding the rest does not significantly impact the performance. To find \(C\), we perform a simple grid search and select the value that yields the best monotonicity-calibration trade-off on the validation dataset. We fit a thresholding model only at the first exit and then reuse it at later exits, as this approach yielded satisfactory performance for the purposes of our analysis. It is likely that fitting a separate thresholding model at each exit would lead to further performance benefits.

In Figure 14, we observe that PA with adaptive thresholding still improves over baseline in terms of monotonicity (middle plot, note the log-scale of \(y\)-axis). Although the adaptive thresholding approach demonstrates worse monotonicity when compared to the PA algorithm with a static threshold (i.e., \(\tau=0\)), it outperforms the PA method in terms of calibration, as measured by the Expected Calibration Error (ECE). To further elucidate the influence of adaptive thresholding, Figure 15 presents the ground-truth probability trajectories for a random selection of test data points. In general, it is apparent

Figure 13: Static threshold analysis for PA using MSDNet as a backbone on CIFAR-100 dataset. PA’s predictive set is defined as: \(s_{m}(\bm{x}_{n}):=\{y\in\mathcal{Y}\,|\,p_{\mathrm{PA},m}\left(y\,|\,\bm{x}_{ n}\right)>0\}\). _Right_: Average size of PA’s predictive set at each exit. As expected, increasing \(b\) leads to smaller sets. _Middle_: (Empirical) Coverage of ground-truth class at each exit: \(c_{m}:=\sum_{n}\bigl{[}y_{n}\in s_{m}(\bm{x}_{n})\bigr{]}\,/\,N_{\text{test}}\). As \(b\) increases, it becomes more likely for the true class \(y^{*}\) to fall outside of the support giving rise to a smaller coverage. _Left_: The marginal test accuracy at each exit is shown as the threshold parameter \(b\) varies. Increasing \(b\) negatively affects accuracy, though the impact is minor for smaller values of \(b\).

that adaptive thresholding can serve as an effective strategy for improving the calibration of the PA method, albeit at some expense to both conditional monotonicity and overall accuracy.

#### b.3.2 PA Finetuning

The post-hoc nature of our proposed PA method is attractive due to its minimal implementation overhead. However, as mentioned in Section 6.3, it seems to adversely affect calibration, according to ECE-like metrics, when compared to baseline models. This discrepancy might be due to a mismatch between the objectives during training and testing in the PA method. To examine this hypothesis, we consider training our model using the PA predictive likelihood (as given by Equation 4) and assess its effect on calibration.

Given the product structure and the non-smooth activation function (ReLU) employed in PA's likelihood, the training can be quite challenging. To enhance the stability of the training process, we introduce two modifications. Firstly, instead of training the model from scratch, we begin by training with a standard EENN objective (see Equation 1) with a softmax likelihood and then _finetune_ the model using the PA likelihood at the end.11 Secondly, we substitute the ReLU activation function with a Softplus function12, which we found aids in stabilizing the model's convergence during training. Consequently, we also utilize Softplus to map logits to (unnormalized) probabilities during testing. Moreover, we found that using ensemble weights \(w_{m}=1\) is more effective for models finetuned

Figure 14: Results for CIFAR-100 using MSDNet as the backbone model. We see that adaptive thresholding can be effective in reducing PA’s calibration gap, though at some cost to both test accuracy and conditional monotonicity. Note that the baseline results in this figure differ slightly from those in Figures 3 and 4, as less data (90%) was used to fit the model, with a portion of the training dataset (10%) used to fit the thresholding model \(\tau\).

Figure 15: Ground-truth probability trajectories (i.e., \(\{\hat{\gamma}_{m}^{p}(\bm{x})\}_{m=1}^{M}\)) for 10 random test CIFAR-100 data points using MSDNet as a backbone. Both instantiations of PA yield more monotone ground-truth probability trajectories.

with the PA objective, as opposed to our default choice of \(w_{m}=m/M\) presented in the main paper for post-hoc PA.

The results are presented in Figure 16. Focusing initially on (marginal) test accuracy (_right_), we note that PA training (**--**) has a slight negative impact on performance, particularly when contrasted with the post-hoc PA (**--**). Similarly, it underperforms post-hoc PA in terms of conditional monotonicity (_middle_), but it considerably surpasses the baseline MSDNet model (**--**). When considering ECE, it becomes clear that fine-tuning with PA leads to significant improvements over post-hoc PA, thus substantiating our hypothesis that the calibration gap mainly arises from the discrepancy between train-time and test-time objectives. On the CIFAR-100 dataset, PA with finetuning even outperforms the baseline MSDNet in terms of ECE.

Based on these observations, we conclude that integrating PA during the training of EENNs is an effective strategy for bridging the calibration gap, though it incurs a minor cost in terms of overall accuracy and conditional monotonicity. We note that these costs could be mitigated (or ideally, completely eliminated) through further stabilization of product training. As such, we view this as a promising avenue for future research.

#### b.3.3 Relevance of ECE in Anytime Prediction Setting

In Section 6.3, we pointed out poor Expected Calibration Error (ECE) in the initial exits as one of the limitations of our proposed PA method. It is important to note, however, that no decision in the anytime setting is based on model (or prediction) confidence, i.e., \(\max_{y}p_{m}\left(y\,|\,\bm{x}\right)\). This contrasts with the _budgeted batch classification_ setting (Huang et al., 2018), where the decision on when to exit could be based on the model confidence. Thus, we believe that our PA potentially undermining

Figure 16: Results for various image classification datasets using MSDNet as the backbone model. We see that finetuning with PA (as opposed to only applying it post-hoc) can be effective in reducing PA’s calibration gap, though at some cost to both test accuracy and conditional monotonicity.

model confidence, as evident by poor ECE in the earlier exits, is not a limitation that would seriously impact the merit of our solution. Especially since we focus solely on the anytime setting, where, we would argue, non-monotone behavior in terms of performance quality is a much more serious issue.

However, it is true that model/prediction confidence, i.e. \(\max_{y}p_{m}\left(y\,|\,\bm{x}\right)\), could be useful in the anytime scenario as a proxy for model uncertainty. This allows the model--when prompted by its environment to make a prediction--to also offer an estimate of uncertainty alongside that prediction. However, given that our PA potentially adversely affects model confidence in earlier exits, we caution against its use. Instead, we recommend alternative measures of uncertainty that better align with our model, such as the conformal set size (c.f., Section 6.2).

### NLP Experiments

In this section, we conduct NLP experiments with the aim of demonstrating that our proposed approaches for adapting EENNs for anytime inference are generalizable across different data modalities.

Specifically, we employ an early-exit model from Schwartz et al. (2020), in which the BERT model is adapted for accelerated inference. The considered datasets are (1) IMDB, a binary sentiment classification dataset; (2) AG, a news topic identification dataset with \(|\mathcal{Y}|=4\); and (3) SNLI, where the objective is to predict whether a hypothesis sentence agrees with, contradicts, or is neutral in relation to a premise sentence (\(|\mathcal{Y}|=3\)). We adhere to the same data preprocessing and model training procedures as outlined in Schwartz et al. (2020).

To account for the smaller number of classes in these NLP datasets, we shift logits for a (small) constant prior to applying ReLU activation function. Hence the PA likelihood at \(m\)th exit has the form (before applying product ensembling):

\[p_{m}\left(y\,|\,\bm{x}\right)=\max\left(f_{m}(\bm{x})_{y}+1/C,\,0\right)^{w_ {i}},\] (5)

where \(C\) represents the cardinality of \(\mathcal{Y}\). This helps with reducing the possibility of PA collapsing to a zero distribution.

The results are depicted in Figure 17. Similar to our image classification experiments discussed in Section 6.1, we find that our proposed modifications yield comparable overall accuracy, as shown in the left plot. Considering conditional monotonicity (_middle_ plot), at least one of our two suggested methods outperforms the baseline model on every dataset. However, the degree of improvement in monotonicity is considerably smaller in comparison to our image classification experiments, with the PA model even failing to yield consistent improvement on certain datasets, such as IMDB. We attribute this not to the change in data modality, but rather to the fewer number of classes in the NLP datasets used in this section as compared to the image datasets used in Section 6.1.

Figure 17: NLP experiments results using the BERT model from Schwartz et al. (2020) as the backbone. Despite the findings being qualitatively similar to those presented in Figures 3 and 4 for image classification datasets, the enhancement in terms of conditional monotonicity provided by our proposed methods is less pronounced for the NLP classification tasks considered in this section.

### Overthinking and Hindsight Improvability

In this section, we delve deeper into concepts that have been previously introduced in EENN literature and that share a connection with the idea of conditional monotonicity, which is the focus of our study.

In Kaya et al. (2019), the authors discovered that EENNs, for some data points, yield correct predictions early on but then produce incorrect results at later exits13--a phenomenon they term _overthinking_. Consequently, the authors noted that early-exiting in deep neural networks is not only beneficial for speeding up inference but can sometimes even lead to improved overall test accuracy. To measure overthinking, they considered the difference in performance between an oracle model that exits at the first exit with the correct answer14 and the performance of the full EENN \(p_{M}\left(\mathbf{y}\,|\,\boldsymbol{x}\right)\). In the anytime setting, we desire a model with the lowest overthinking measure, i.e., \(0\), as this indicates that evaluating more layers in the early-exit network does not negatively impact performance.

Footnote 13: Using notation introduced in Section 2 this implies: \(\hat{\gamma}^{c}_{m}(\boldsymbol{x})=1\) and \(\hat{\gamma}^{c}_{m^{\prime}}(\boldsymbol{x})=0\) for some \(m^{\prime}>m\).

Footnote 14: Such a model cannot be implemented in practice, as it requires access to ground-truth values. Nevertheless, it is interesting to study it as a way to better understand the upper bound on the achievable performance of the underlying model.

In Figure 18, we present the results of the overthinking analysis for the MSDNet (Huang et al., 2018) model. We observe that both of our proposed methods significantly improve in terms of the overthinking measure (_left_), making them more applicable in anytime scenarios.

We also note that a decrease in the overthinking measure can result from either (1) a reduction in the (theoretical) performance of the oracle model or (2) an improvement in the (actual) performance of the full EENN. As can be seen in Figure 18 (_middle_ and _right_), our proposed modifications mainly improve the overthinking measure through (1). To explore this further, in Figure 19 we plot the number of test points that the model _learned_, i.e., correctly predicted for the first time, and the number of points that the model _forgot_, i.e., incorrectly predicted for the first time, at each early exit. Both PA and CA show a clear strength in minimizing forgetting, although they do so at the cost of diminishing the baseline model's ability to learn correct predictions. 15 These results indicate that there is further room for improvement in the adaptation of EENNs for anytime computation, as the ideal model would minimize forgetting without compromising its learning abilities.

Figure 18: Overthinking analysis based on Kaya et al. (2019) using the MSDNet model. Our methods substantially improve upon the baseline MSDNet in terms of the overthinking measure (_left_), making them more suitable for an anytime setting. However, most of these improvements can be attributed to a decrease in the theoretical performance of the oracle model (_middle_) rather than to the improvement of the full EENN performance (_right_). For a more detailed explanation of the quantities depicted, please refer to Section B.5.

In a follow-up study by Wolczyk et al. (2021), the hindsight improvability (HI) measure is introduced. For each early exit, this measure takes into account the set of incorrectly predicted points and computes the percentage of those that were accurately predicted at any of the previous exits. In an anytime setting, an ideal model would consistently exhibit an HI measure of \(0\). As evidenced by the results in Figure 20, our post-hoc modifications substantially enhance the HI measure (_right_ plot) while only causing a minor negative impact on overall accuracy (_left_ plot). Similar to the overthinking measure, the observation that \(HI>0\) for our proposed methods suggests a potential for further improvements.

Figure 19: _Learn-vs-forget_ analysis for the CIFAR-100 dataset using the MSDNet model. Specifically, we plot the number of data points that are correctly classified for the first time (_left_) and the number of data points that are incorrectly predicted for the first time (_right_) at each early exit. We observe that our modifications help the model ‘forget’ less, though this comes with a minor trade-off in the model’s ability to ‘learn’.

Figure 20: Results for CIFAR-100 using Zero-Time-Waste (ZTW; Wolczyk et al. (2021)) model. Applying our PA method to ZTW model leads to large improvements in terms of conditional monotonicity (_middle_) and hindsight improvability (_right_), while only causing a minor decrease in marginal accuracy (_left_).

### Deep Ensembles and Regression

Although we have presented our results for classification with early-exit neural networks, we can easily extend them to handle generic deep ensembles and regression settings.

Deep EnsemblesWhile applying our method to deep ensembles might seem strange since they have no implicit ordering as in the case of early-exit NNs, we note that ensembles are often evaluated sequentially in low resource settings where there is not enough memory to evaluate all of the ensemble members concurrently. In such settings, there is an implicit, if arbitrary, ordering, and we may still require anytime properties so that we can stop evaluating additional ensemble members at any point. Therefore, we can apply both Product Anytime and Caching Anytime--as in Section 4--to deep ensembles, where \(p_{m}\left(y\,|\,\bm{x}\right)\) now represents the \(m\)-th ensemble member, rather than the \(m\)-the early exit. An earlier version of this work (Allingham and Nalisnick, 2022) focused on product anytime in the deep ensembles case. However, in that work, the focus was also on training from scratch rather than the post-hoc setting we consider in this work. Training from scratch is challenging due to, among other things,16 instabilities caused by the Product-of-Experts and the bounded support of the likelihoods. Thus, the PA results from Allingham and Nalisnick (2022) are worse in terms of predictive performance than the standard ensemble baseline. Note that our theoretical results in Appendix A also apply in the deep ensembles case.17 Figure 21 shows qualitative results for our _anytime ensembles_ applied to a simple classification task. As expected, the support shrinks as we add more multiplicands to the product.

Footnote 16: For example, Allingham and Nalisnick (2022) use a mixture of the product and individual likelihoods as their loss function to encourage the individual ensemble members to fit the data well. However, they use the standard softmax likelihood—with an exponential activation function—for the individual ensemble members, which conflicts with the Heaviside activations in the product likelihood and results in reduced performance.

Footnote 17: We note that Allingham and Nalisnick (2022) neglect an important property required for monotonicity of the modal probability: \(p_{m}\left(y\,|\,\bm{x}\right)\) must be uniform in addition to having finite support. I.e., in the classification setting, the activation function used to map logits to probabilities must be the Heaviside function.

RegressionProduct Anytime can be applied to regression by using a uniform likelihood at each early-exit (or ensemble member)

\[p_{m}\left(y\,|\,\bm{x}\right)=\mathcal{U}(a_{m},\,b_{m})\]

which results in a product likelihood

\[p_{1:m}\left(y\,|\,\bm{x}\right)=\mathcal{U}(a_{\text{max}},\,b_{\text{min}})\]

where \(a_{\text{max}}=\text{max}(\{a_{m}\})\) and \(b_{\text{min}}=\text{min}(\{b_{m}\})\). That is, the lower and upper bounds of the product are, respectively, the maximum-lower and minimum-upper bounds of the multiplicands. As we add more multiplicands, the width of the product density can only get smaller or stay the same, and thus the probability of the modal prediction \(p_{1:m}\left(\tilde{y}\,|\,\bm{x}\right)\) can only increase or stay the same, as in the classification case. Figure 22 shows qualitative results for anytime ensembles applied to a simple regression task. We see that the order of evaluation is unimportant. Firstly, as expected, the final result is the same, and in both cases, the support size always decreases or is unchanged. Secondly, the fit has almost converged within a few evaluations, and the last few multiplicands tend to have smaller effects. Note that the prediction bounds cover the data well even for the poorer early fits. Finally, note that there is no support outside of the training data range--we consider this data as out-of-distribution.

Figure 21: Evolution of the support for each of 4-classes (\(\blacksquare\), \(\blacksquare\), \(\blacksquare\), \(\blacksquare\), \(\blacksquare\)) in a spiral-arm classification task with a 5-member anytime ensemble, as 1 to 5 (left to right) members are included in the prediction.

### Anytime Uncertainty: Conditional Monotonicity

In Section 6.2, we presented the results of our uncertainty experiments where we observed that, marginally, our PA model demonstrates an _anytime uncertainty_ pattern: the model starts with high uncertainty early on and then progressively becomes more confident at later exits.

In this section, we supplement these experiments with an analysis at the conditional level. Specifically, for a given \(\bm{x}\in\mathcal{X}\), we compute the uncertainty (for instance, entropy) at each early exit: \(\{U_{m}(\bm{x})\}_{m=1}^{M}\). Following the approach we used in Section 3 for performance quality sequences, we define a _maximum increase_ in uncertainty as \(\text{MUI}(\bm{x}):=\max_{m^{\prime}>m}\big{\{}\max(U_{m^{\prime}}(\bm{x})-U_{ m}(\bm{x}),0)\big{\}}\). Next, for a range of suitably chosen thresholds \(\tau\in\mathbb{R}_{\geq 0}\), we compute the number of test instances that exhibit an uncertainty increase surpassing a given threshold \(N_{\tau}:=\sum_{n}[\text{MUI}(\bm{x}_{n})\geq\tau]\). Note that a monotone oracle model would exhibit entirely non-increasing uncertainty trajectories, meaning \(N_{\tau}=0,\ \forall\tau\).

The results for entropy and conformal set size are depicted in Figures 23 and 24, respectively. We observe that an EANN like MSDNet exhibits non-monotone behavior also in its uncertainty estimates across different exits. This means that there are data points where the model starts with high confidence in the initial exits but becomes less certain in subsequent stages. Such a trend is problematic in the anytime setting, similar to the issues of non-monotonicity in performance quality (c.f., Section 3). It suggests that the model expends additional computational resources only to diminish its confidence in its response. Reassuringly, our PA aids in ensuring monotonicity not only

Figure 23: Conditional monotonicity using entropy as an uncertainty measure and MSDNet model as a backbone. To better illustrate the different behavior of various methods, a log scale is used for the y-axis. Our PA exhibits a significantly more monotone behavior compared to the baseline MSDNet (Huang et al., 2018) model.

Figure 22: Evolution of the the product support () and mode () for the _simple 1d_(Antorán et al., 2020) regression task with a 5-member anytime ensemble, as 1 to 5 (left to right) members are included in the prediction. In each column the support of the multiplicand to be included in the next column’s product are shown (). The top and bottom show two different arbitrary orderings of the ensemble members.

in terms of performance quality (e.g., ground-truth probability) but also in its uncertainty estimates (e.g., conformal set size) as evident in Figures 24 and 23.

### Controlling the Violations of Conditional Monotonicity

In Section 4.2, we introduced our Product Anytime (PA) method. It relies on the ReLU activation function to map logits to (unnormalized) probabilities, which unfortunately compromises the strong conditional monotonicity guarantees that are present when using the Heaviside activation function.

In this section, we propose a technique that allows for a more nuanced control of violations of conditional guarantees. To illustrate this, observe first that ReLU and Heaviside are specific cases of the following activation function:

\[a(x):=C_{b}\cdot\text{max}\big{(}\text{min}(x,b),\,0\big{)}\] (6)

where \(C_{b}:=\text{max}(1,1/b)\). Notice how \(\lim_{b\to\infty}a(x)=\text{ReLU}(x)\) and \(\lim_{b\to 0^{+}}a(x)=\text{Heaviside}(x)\). Therefore, by adjusting the clipping threshold \(b\), we can navigate the accuracy-monotonicity trade-off as needed. This is demonstrated on the CIFAR-10 dataset in Figure 25.

Figure 24: Conditional monotonicity using conformal set size as an uncertainty measure and MSDNet model as a backbone. To better illustrate the different behavior of various methods, a log scale is used for the y-axis. Same as in Figure 5, we used Regularized Adaptive Predictive Sets algorithm [RAPS; Angelopoulos et al., 2021] to compute conformal scores. \(20\%\) of test examples were used as a hold-out set to calculate conformal quantiles at each early exit. Our PA exhibits a significantly more monotone behavior compared to the baseline MSDNet [Huang et al., 2018] model.

Figure 25: Test accuracy (_left_) and conditional monotonicity (_middle_ and _right_) on CIFAR-10 dataset for various instantiations of the PA model with the MSDNet backbone. We vary the clipping threshold \(b\) used in the activation function when mapping logits to probabilities. When \(b\) is close to \(0\), the activation function approximates the Heaviside function, resulting in more monotone behavior, although this comes with a decrease in (marginal) accuracy. As \(b\) is increased, the activation function moves closer to ReLU, leading to more violations of monotonicity, but also enhancing accuracy.

### Impact of Calibration on Conditional Monotonicity

In this section, we study whether monotonicity could be achieved by calibrating the underlying EENN rather than employing our proposed PA. To answer this, we conducted an experiment where we calibrated MSDNet and analyzed its effect on monotonicity. For the calibration, we utilized three standard techniques: temperature scaling (Guo et al., 2017), deep ensembles (Lakshminarayanan et al., 2017), and last-layer Laplace (Daxberger et al., 2021). In Figure 26, we present results for MSDNet on CIFAR-100. While all three approaches lead to better calibration in terms of ECE (right plot), our PA significantly outperforms them in conditional monotonicity (middle plot). Moreover, all three baselines are (arguably) more complex compared to our PA: temperature scaling requires validation data, deep ensemble has M-times more parameters (M being the ensemble size), and Laplace is significantly slower compared to PA since we need test-time sampling to estimate the predictive posterior at each exit.

However, simply improving calibration might not be expected to improve monotonicity. Thus, in Figure 26, we also provide results for combining the above uncertainty calibration techniques with our CA baseline. We see that this combination provides further improvements w.r.t. monotonicity -- the improved calibration allows for better caching of the correct prediction. However, the monotonicity of these caching-and-calibrated methods still underperforms compared to PA despite being more complicated. This demonstrates that monotonicity can not be achieved via calibration alone and further underlines the value of our proposed approach.

### Budgeted Batch Classification

In this section, we conduct a budgeted batch classification experiment using MSDNet (Huang et al., 2018) as the backbone. In budgeted batch classification, a limited amount of compute is available to classify a batch of datapoints. The goal is to design a model that can allocate resources smartly between easy and hard examples to maximize the overall performance, such as accuracy.

The results are shown in Figure 27. Since MSDNet outperforms our PA at most computational budgets, we conclude that the monotonicity is less beneficial in budgeted batch classification. However, as seen on CIFAR datasets, the non-monotonicity of MSDNet is also concerning in this setting - for larger values of FLOPs, MSDNet performance starts to drop, while PA keeps monotonically increasing and surpasses MSDNet. Based on these results, we suggest "turning on" our PA only for anytime prediction and recommend that the user sticks to the original EENN in the budgeted batch classification. However, this certainly warrants further investigation (as seen by the performance benefits of our PA on CIFAR for larger FLOPs).

Figure 26: Comparison of our PA with calibrated baselines using MSDNet as a backbone on CIFAR-100 dataset. For calibration, we use last-layer Laplace, deep ensembles with \(M=5\), and temperature scaling. Additionally, we consider both caching (**—**) and non-caching (**- *)** versions of calibrated baselines. None of the considered calibrated baselines outperforms our PA in terms of monotonicity (middle), despite being more complex. The blue and purple solid lines overlap in the accuracy plot (_left_), indicating that the temperature scaling does not affect accuracy for the non-cached version, as expected.

[MISSING_PAGE_FAIL:29]

Figure 31: Test accuracy on the CIFAR and ImageNet datasets for L2W model (Han et al., 2022b). We plot the average accuracy from \(n=3\) independent runs.

Figure 30: The \(\%\) of test examples with a ground-truth probability drop exceeding a particular threshold for various instantiations of DViT model (Wang et al., 2021). Our PA significantly improves the conditional monotonicity of ground-truth probabilities across various datasets and backbone models. To better illustrate the different behavior of various methods, a log scale is used for the y-axis.

Figure 31: Test accuracy on the CIFAR and ImageNet datasets for L2W model (Han et al., 2022b). We plot the average accuracy from \(n=3\) independent runs.

Figure 29: Test accuracy on the CIFAR and ImageNet datasets for various instantiations of DViT model (Wang et al., 2021).

Figure 32: The \(\%\) of test examples with a ground-truth probability drop exceeding a particular threshold for L2W model (Han et al., 2022b). Results are averaged over \(n=3\) independent runs. To better illustrate the different behavior of various methods, a log scale is used for the y-axis.