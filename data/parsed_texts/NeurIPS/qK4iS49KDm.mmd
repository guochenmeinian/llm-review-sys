# Neural network learns low-dimensional polynomials with SGD near the information-theoretic limit

Jason D. Lee\({}^{1}\), Kazusato Oko\({}^{2,4}\), Taiji Suzuki\({}^{3,4}\), Denny Wu\({}^{5,6}\)

\({}^{1}\)Princeton University, \({}^{2}\)University of California, Berkeley, \({}^{3}\)University of Tokyo

\({}^{4}\)RIKEN AIP, \({}^{5}\)New York University, \({}^{6}\)Flatiron Institute

jasonlee@princeton.edu, oko@berkeley.edu,

taiji@mist.i.u-tokyo.ac.jp, dennywu@nyu.edu

###### Abstract

We study the problem of gradient descent learning of a single-index target function \(f_{*}(\bm{x})=\sigma_{*}(\langle\bm{x},\bm{\theta}\rangle)\) under isotropic Gaussian data in \(\mathbb{R}^{d}\), where the unknown link function \(\sigma_{*}:\mathbb{R}\to\mathbb{R}\) has information exponent \(p\) (defined as the lowest degree in the Hermite expansion). Prior works showed that gradient-based training of neural networks can learn this target with \(n\gtrsim d^{\Theta(p)}\) samples, and such complexity is predicted to be necessary by the correlational statistical query lower bound. Surprisingly, we prove that a two-layer neural network optimized by an SGD-based algorithm (on the squared loss) learns \(f_{*}\) with a complexity that is not governed by the information exponent. Specifically, for arbitrary polynomial single-index models, we establish a sample and runtime complexity of \(n\simeq T=\Theta(d\cdot\mathrm{polylog}d)\), where \(\Theta(\cdot)\) hides a constant only depending on the degree of \(\sigma_{*}\); this dimension dependence matches the information theoretic limit up to polylogarithmic factors. More generally, we show that \(n\gtrsim d^{(p_{*}-1)\lor 1}\) samples are sufficient to achieve low generalization error, where \(p_{*}\leq p\) is the _generative exponent_ of the link function. Core to our analysis is the reuse of minibatch in the gradient computation, which gives rise to higher-order information beyond correlational queries.

## 1 Introduction

Single-index models are a classical class of functions that capture low-dimensional structure in the learning problem. To efficiently estimate such functions, the learning algorithm should extract the relevant (one-dimensional) subspace from high-dimensional observations; hence this problem setting has been extensively studied in deep learning theory [1, 2, 3, 1, 1, 2, 1, 1, 2, 3, 4], to examine the adaptivity to low-dimensional targets and benefit of representation learning in neural networks (NNs) optimized by gradient descent (GD). In this work we study the learning of a single-index target function under isotropic Gaussian data:

\[y_{i}=f_{*}(\bm{x}_{i})+\varsigma_{i},\quad f_{*}(\bm{x}_{i})= \sigma_{*}(\langle\bm{x}_{i},\bm{\theta}\rangle),\quad\bm{x}_{i}\overset{ \text{i.i.d.}}{\sim}\mathcal{N}(0,\bm{I}_{d}),\] (1.1)

where \(\varsigma_{i}\) is i.i.d. label noise, \(\bm{\theta}\in\mathbb{R}^{d}\) is the direction of index features, and we assume the link function \(\sigma_{*}:\mathbb{R}\to\mathbb{R}\) has information exponent \(p\in\mathbb{N}_{+}\) defined as the index of the first non-zero coefficient in the Hermite expansion (see Definition 1).

Equation (1.1) requires the estimation of the one-dimensional link function \(\sigma_{*}\) and the relevant direction \(\bm{\theta}\); it is known that learning is information theoretically possible with \(n\gtrsim d\) training examples [1, 1]. Indeed, when \(\sigma_{*}\) is polynomial, such statistical complexity can be achieved up to logarithmic factors by a tailored algorithm that exploit the structure of low-dimensional target [1]. On the other hand, for gradient-based training of two-layer NNs, existing works established a sample complexity of \(n\gtrsim d^{\Theta(p)}\)[1, 2, 3], which presents a gapbetween the information theoretic limit and what is computationally achievable by (S)GD. Such a gap is also predicted by the correlational statistical query (CSQ) lower bound [10, 1], which roughly states that for a CSQ algorithm to learn (isotropic) Gaussian single-index models using less than exponential compute, a sample size of \(n\gtrsim d^{p/2}\) is necessary.

Although CSQ lower bounds are frequently cited to imply a fundamental barrier of learning via SGD (with the squared/correlation loss), strictly speaking, the CSQ model does not include empirical risk minimization with gradient descent, due to the non-adversarial noise and existence of non-correlational terms in the gradient computation. Very recently, [13] exploited higher-order terms in the gradient update arising from the reuse of the same training data, and showed that for certain link functions with high information exponent (\(p>2\)), two-layer NNs may still achieve weak recovery (i.e., nontrivial overlap with \(\bm{\theta}\)) after two GD steps with \(\Theta(d)\) batch size. While this presents evidence that GD-trained NNs can learn \(f_{*}\) beyond the sample complexity suggested by the CSQ lower bound, the weak recovery statement in [13] may not translate to statistical guarantees; moreover, the class of functions where SGD can achieve vanishing generalization error is not fully characterized, as only a few specific examples of link functions are discussed.

Given the existence of (non-NN) algorithms that learn any single-index polynomials in \(n=\tilde{O}(d)\) samples [1] regardless of the information exponent \(p\), and more generally, non-CSQ algorithms with a sample complexity surpassing the CSQ lower bound [12], it is natural to ask if gradient-based training of NNs can achieve similar statistical efficiency for this function class. Motivated by observations in [13] that SGD with reused data may break the "curse of information exponent", we aim to address the question:

_Can NN optimized by SGD with reused batch learn single-index \(f_{*}\) beyond the CSQ lower bound?_

_And for polynomial \(\sigma_{*}\), can learning succeed near the information-theoretic limit \(n\simeq d\)?_

Empirically, the separation between one-pass (online) and multi-pass SGD is clearly observed in Figure 1, where we trained the same two-layer ReLU neural network to learn a single-index polynomial with information exponent \(p=3\). We see that SGD with reused data (Figure 1(b)) reaches low test error using roughly \(n\simeq d\) samples, whereas online SGD fails to achieve even weak recovery with much larger sample size \(n=\Omega(d^{2})\). Our main contribution is to establish this improved statistical complexity for two-layer NNs trained by a variant of SGD with reused training data.

### Our Contributions

We answer the above question in the affirmative by showing that SGD training (with the squared loss) on a natural class of shallow NNs can achieve small generalization error using polynomial compute and a sample complexity that is not governed by the information exponent, if we employ a layer-wise optimization procedure (analogous to that in [11, 10, 1]) and reuse of the same minibatch. The core insight is that SGD can implement a full statistical query (SQ) algorithm that goes beyond CSQ, despite the correlational structure of the squared loss. Our main finding is summarized by the following theorem.

Figure 1: We train a ReLU NN (3.1) with \(N=1024\) neurons using SGD (squared loss) with step size \(\eta=1/d\) to learn a single-index target \(f_{*}(\bm{x})=\mathsf{He}_{3}(\langle\bm{x},\bm{\theta}\rangle)\); heatmaps are values averaged over 10 runs. \((a)\) online SGD with batch size \(B=8\); \((b)\) GD on the same batch of size \(n\) for \(T=2^{14}\) steps. For online SGD we only report weak recovery (i.e., averaged overlap between neuron \(\bm{w}\) and target \(\bm{\theta}\)) since the test error does not drop.

**Theorem** (informal).: _A shallow NN with \(N=\tilde{\Theta}_{d}(1)\) neurons can learn arbitrary single-index models up to small population loss: \(\mathbb{E}_{\bm{v}}[(f_{\bm{\Theta}}(\bm{x})-f_{*}(\bm{x}))^{2}]=o_{d,\mathbb{P} }(1)\), if we employ an SGD-based algorithm (with reused training data) to minimize the squared loss objective, with a sample and runtime complexity of \(n,T=\tilde{\Theta}_{d}(d^{(p_{*}-1)\lor 1})\), where \(p_{*}\) is the generative exponent of the link \(\sigma_{*}\)._

Note that the generative exponent [4] is defined as the _minimum_ information exponent of the link function \(\sigma_{*}\) after arbitrary \(L^{2}\) transformation, and hence by definition \(p_{*}\leq p\) (equality is achieved by the identity transformation). We make the following remarks on our main result.

* We know that \(p_{*}\leq 2\) for arbitrary _polynomial_ link functions. Therefore, the theorem suggests that NN + SGD with reused batch can learn single-index polynomials with a sample complexity \(n=\tilde{O}_{d}(d)\) which is information theoretically optimal up to polylogarithmic factors, hence matching the efficiency of SQ algorithms tailored for low-dimensional polynomial regression [1].
* For non-polynomial \(\sigma_{*}\) with high generative exponent \(p_{*}>2\), our sample complexity \(n\gtrsim d^{p_{*}-1}\) can be interpreted as an SQ version of the online SGD result in [1]. Since the information exponent \(p\) can be arbitrarily larger than the generative exponent \(p_{*}\), our main theorem disproves a conjecture in [1] stating that \(n\asymp d^{p/2}\) is the optimal sample complexity for empirical risk minimization with SGD on the squared loss / correlation loss.
* A key observation in our analysis is that with suitable activation function, SGD with reused batch can go beyond correlational queries and implement (a subclass of) SQ algorithms. This enables polynomial transformations to the labels that reduce the information exponent, and therefore optimization can escape the high-entropy "equator" at initialization in polylogarithmic time.

Upon completion of this work, we became aware of the preprint [1] showing weak recovery (for polynomial targets with \(p_{*}\leq 2\)) with similar sample complexity, also by exploiting the reuse of training data. Our work was conducted independently and simultaneously.

## 2 Problem Setting and Prior Works

Notations.\(\quad\left\|\,\cdot\,\right\|\) denotes the \(\ell_{2}\) norm for vectors and the \(\ell_{2}\to\ell_{2}\) operator norm for matrices. \(O_{d}(\cdot)\) and \(o_{d}(\cdot)\) stand for the big-O and little-o notations, where the subscript highlights the asymptotic variable \(d\) and suppresses dependence on \(p,q\); we write \(\tilde{O}(\cdot)\) when (poly-)logarithmic factors are ignored. \(\mathcal{O}_{d,\mathbb{P}}(\cdot)\) (resp. \(o_{d,\mathbb{P}}(\cdot)\)) represents big-O (resp. little-o) in probability as \(d\to\infty\). \(\Omega(\cdot),\Theta(\cdot)\) are defined analogously. \(\gamma\) is the standard Gaussian distribution in \(\mathbb{R}\). We denote the \(L^{2}\)-norm of a function \(f\) with respect to the data distribution (which will be specified) as \(\left\|f\right\|_{L^{2}}\). For \(g:\mathbb{R}\to\mathbb{R}\), we denote \(g^{i}\) as its \(i\)-th exponentiation, and \(g^{(i)}\) is the \(i\)-th derivative. We say an event happens _with high probability_ when the failure probability is bounded by \(\exp(-C\log d)\) for large constant \(C\).

### Complexity of Learning Single-index Models

We aim to learn a single-index model (1.1) where the link function \(\sigma_{*}:\mathbb{R}\to\mathbb{R}\) has information exponent \(p\) defined as follows [1, 1].

**Definition 1** (Information exponent).: _Let \(\{\mathsf{He}_{j}\}_{j=0}^{\infty}\) denote the normalized Hermite polynomials. The information exponent of \(g\in L^{2}(\gamma)\), denoted by \(\mathrm{IE}(g):=p\in\mathbb{N}_{+}\), is the index of the first non-zero Hermite coefficient of \(g\), that is, given \(g(z)=\sum_{i=0}^{\infty}\alpha_{i}\mathsf{He}_{i}(z)\), \(p:=\min\{i\!>\!0:\alpha_{i}\!\neq\!0\}\)._

By definition, when \(\sigma_{*}\) is a degree-\(q\) polynomial, we always have \(p\leq q\). Note that \(f_{*}\) contains \(\Theta(d)\) parameters to be estimated, and hence _information theoretically \(n\gtrsim d\)_ samples are both su

Figure 2: Complexity of learning single-index model where the link function \(\sigma_{*}\) is a degree-\(q\) polynomial with information exponent \(p\). For the CSQ lower bound, we translate the tolerance to sample complexity using the i.i.d. concentration heuristic \(\tau\approx n^{-1/2}\). We restrict ourselves to algorithms using polynomial compute; this excludes the sphere-covering procedure in [4] or exponential-width neural network in [1, 2].

ficient and necessary for learning [13, 1, 14]; however, the sample complexity achieved by different (polynomial time) algorithms depends on structure of the link function.

* **Kernel Methods.** Rotationally invariant kernels cannot adapt to the low-dimensional structure of single-index \(f_{*}\) and hence suffer from the curse of dimensionality [17, 13, 15, 16]. By a standard dimension argument [18, 19, 20], we know that in the isotropic data setting, kernel methods (including neural networks in the lazy regime [13, 14]) require \(n\gtrsim d^{q}\) samples to learn degree-\(q\) polynomials in \(\mathbb{R}^{d}\).
* **Gradient-based Training of NNs.** While NNs can easily approximate a single-index model [1], the sample complexity of gradient-based learning established in prior works typically scales as \(n\gtrsim d^{\Theta(p)}\): in the well-specified setting, [1] proved a sample complexity of \(n=\tilde{\Theta}(d^{p-1})\) for online SGD, which is later improved to \(\tilde{\Theta}(d^{p/2})\) by a smoothed objective [15]; as for the misspecified setting, [10, 11] showed that \(n\gtrsim d^{p}\) samples suffice, and in some cases a \(\tilde{\Theta}(d^{p-1})\) complexity is achievable [1, 1]. Consequently, at the information-theoretic limit (\(n\asymp d\)), existing results can only cover the learning of low information exponent targets [1, 13, 14]. This exponential dependence on \(p\) also appears in the CSQ lower bounds [10, 1], which is often considered to be indicative of the performance of SGD learning with the squared loss (see Section 2.2).

Statistical Query Learners.If we do not restrict ourselves to correlational queries, the sample complexity of learning (1.1) can be drastically improved. Specifically, for polynomial \(\sigma_{*}\), [10] gave an SQ algorithm that achieves low generalization error in \(n=\tilde{O}(d)\) samples, which is near the information-theoretic limit; the key ingredient is to construct nonlinear transformations to the labels that lowers the information exponent to \(2\); similar preprocessing also appeared in context of phase retrieval [13, 14]. Such transformations do not belong to CSQ, but can be utilized by a full SQ learner to enhance the statistical efficiency. Recently, [15] introduced the _generative exponent_ which governs the complexity of SQ algorithms.

**Definition 2** (Generative exponent).: _The generative exponent (GE) of \(g\in L^{2}(\gamma)\) is defined as the lowest information exponent (IE) after arbitrary \(L^{2}\) transformation, that is,_

\[p_{*}=:\mathrm{GE}(g)=\inf_{\mathcal{T}\in L^{2}(P_{y})}\mathrm{IE}(\mathcal{T }\circ g).\]

The generative exponent is the smallest information exponent obtained by all possible label transformations. By definition we always have \(p^{*}\leq p\), and the gap between the two indices can be arbitrarily large; for example, for the Hermite polynomials we have \(\mathrm{IE}(\mathsf{He}_{k})=k\) whereas \(\mathrm{GE}(\mathsf{He}_{k})\leq 2\).

[15] established a sample complexity lower bound of \(n=\Omega(d^{p_{*}/2\lor 1})\) for full SQ learners with polynomial compute (assuming \(\tau\approx n^{-1/2}\)), and obtained matching upper bound by a tensor partial-trace algorithm. Our goal is to show that SGD training of two-layer neural network can also achieve a sample and runtime complexity that scales with \(n\simeq d^{\Theta(p_{*})}\), where the dimension dependence is governed by the generative exponent \(p_{*}\) instead of the information exponent \(p\).

### Can Gradient Descent Go Beyond Correlational Queries?

Correlational statistical query.A statistical query (SQ) learner [11, 12] accesses the target \(f_{*}\) through noisy queries \(\tilde{\phi}\) with error tolerance \(\tau\): \(|\tilde{\phi}-\mathbb{E}_{\bm{x},y}[\phi(\bm{x},y)]|\leq\tau\). Lower bound on the performance of SQ algorithm is a classical measure of computational hardness. In the context of gradient-based optimization, an often-studied subclass of SQ is the _correlational_ statistical query (CSQ) [15] where the query is restricted to (noisy version of) \(\mathbb{E}_{\bm{x},y}[\phi(\bm{x})y]\). To see the connection between CSQ and SGD, consider the gradient of expected squared loss for one neuron \(f_{\bm{w}}(\bm{x})\):

\[\nabla_{\bm{w}}\mathbb{E}_{\bm{x},y}(f_{\bm{w}}(\bm{x})-y)^{2}\propto-\mathbb{ E}_{\bm{x},y}[\underbrace{\cdot\nabla_{\bm{w}}f_{\bm{w}}(\bm{x})}_{\text{ correlational query}}]+\mathbb{E}_{\bm{x}}[\underbrace{f_{\bm{w}}(\bm{x})\cdot\nabla_{\bm{w}}f_{\bm{w}}( \bm{x})}_{\text{can be evaluated without $y$}}].\]

One can see that information of the target function is encoded in the correlation term in the gradient. To infer the statistical efficiency of GD in the empirical risk minimization setting, we replace the population gradient with the empirical average \(\nabla_{\bm{w}}(\frac{1}{n}\sum_{i=1}^{n}(f_{\bm{w}}(\bm{x}_{i})-y_{i})^{2})\), and heuristically equate the CSQ tolerance \(\tau\) with the scale of i.i.d. concentration error \(n^{-1/2}\).

For the Gaussian single-index model class with information exponent \(p\), [10] proved a lower bound stating that a CSQ learner either has access to queries with tolerance \(\tau\lesssim d^{-p/4}\), or exponentially many queries are needed to learn \(f_{*}\) with small population loss. Using the heuristic \(\tau\approx n^{-1/2}\), this suggests a sample complexity lower bound \(n\gtrsim d^{p/2}\) for polynomial time CSQ algorithm. This lower bound can be achieved by a landscape smoothing procedure [11] (in the well-specified setting), and is conjectured to be optimal for empirical risk minimization with SGD [1].

SGD with reused data.As previously discussed, the gap between SQ and CSQ algorithms primarily stems from the existence of label transformations that decrease the information exponent. While such transformation cannot be utilized by a CSQ learner, [10] argued that they may arise from two consecutive gradient updates using the same minibatch. For illustrative purposes, consider one neuron \(f_{\bm{w}}(\bm{x})=\sigma(\langle\bm{x},\bm{w}\rangle)\) updated by two GD steps using the same data point \((\bm{x},y)\), starting from zero initialization \(\bm{w}^{0}=\bm{0}\) (we focus on the correlational term in the loss for simplicity):

\[\bm{w}^{2}=\bm{w}^{1}+\eta\cdot y\sigma^{\prime}(\langle\bm{x},\bm{w}^{1} \rangle)\bm{x}=\eta\sigma^{\prime}(0)\underbrace{y\cdot\bm{x}}_{\text{CSQ term}}+\eta \underbrace{y\sigma^{\prime}(\eta\sigma^{\prime}(0)\|\bm{x}\|^{2}\cdot y)\bm{x }}_{\text{non-CSQ term}}.\] (2.1)

Under appropriate learning rate scaling \(\eta\cdot\left\|\bm{x}\right\|^{2}=\Theta(1)\), one can see that in the second gradient step, the label \(y\) is transformed by the nonlinearity \(\sigma^{\prime}\), even though the loss function itself is not modified. Based on this observation, [10] showed that if the non-CSQ term in (2.1) reduces the information exponent to \(1\), then _weak recovery_ (i.e., nontrivial overlap between the first-layer parameters \(\bm{w}\) and index features \(\bm{\theta}\)) can be achieved after two GD steps with \(n=\Theta(d)\) samples.

### Challenges in Establishing Statistical Guarantees

Importantly, the analysis in [10] does not lead to concrete learnability guarantees for the class of single-index polynomials for the following reasons: \((i)\) it is not clear if an appropriate nonlinear transformation that lowers the information exponent can always be extracted from SGD with reused data, and \((ii)\) the weak recovery guarantee may not translate to a sample complexity for the trained NN to achieve small generalization error. We elaborate these technical challenges below.

SGD decreases information exponent.To show weak recovery, [10], Definition 3.1] assumed that the student activation \(\sigma\) can reduce the information exponent of the labels to \(1\); while a few examples are given, the existence of such transformations in SGD is not guaranteed:

* The label transformation employed in prior SQ algorithms [1] is based on thresholding, which reduces the information exponent to \(2\) for any polynomial \(\sigma_{*}\); however, isolating such function from SGD updates on the squared loss is challenging. Instead, we make use of monomial transformation which can be extracted from SGD via Taylor expansion.
* If the link function satisfies \(p_{*}\geq 2\), its information exponent after arbitrary nonlinear transformation is at least \(2\); such functions are predicted not be not learnable by SGD in the \(n\asymp d\) regime [10]. To handle this setting, we analyze the SGD update up to \(\operatorname{poly}(d)\) time, at which a non-trivial overlap can be established by a Gronwall-type argument similar to [1]. For \(p_{*}=2\), this recovers results on phase retrieval when \(\sigma_{*}(z)=z^{2}\) which requires \(n=\Omega(d\log d)\) samples.

From weak recovery to sample complexity.Note that weak recovery (i.e., \(|\langle\bm{w},\bm{\theta}\rangle|>\varepsilon\) for some small constant \(\varepsilon>0\)) is generally insufficient to establish low generalization error of the trained NN. Therefore, we need to show that starting from a nontrivial overlap, subsequent gradient steps can achieve _strong recovery_ of the index features (i.e., \(|\langle\bm{w},\bm{\theta}\rangle|>1-\varepsilon\)), despite the link misspecification. After the first-layer parameters align with the target function, we train the second-layer parameters with SGD to learn the link function \(\sigma_{*}\) with the aid of random bias units [10].

## 3 Learning Polynomial \(f_{*}\) in Linear Sample Complexity

We first consider the setting where \(\sigma_{*}\) is polynomial with degree \(q\) specified as follows.

**Assumption 1**.: _The target function is given as \(f_{*}(\bm{x})=\sigma_{*}(\langle\bm{x},\bm{\theta}\rangle)\), where the link function \(\sigma_{*}:\mathbb{R}\to\mathbb{R}\) admits the Hermite decomposition \(\sigma_{*}(z)=\sum_{i=p}^{q}\alpha_{i}\mathsf{He}_{i}(z)\)._

For single-index polynomials, we do not expect a computational-to-statistical gap under the SQ class [1] -- indeed, we will establish learning guarantees near the information theoretic limit \(n\asymp d\)

### Training Algorithm

We train the following two-layer network with \(N\) neurons using SGD to minimize the squared loss:

\[f_{\bm{\Theta}}(\bm{x})=\frac{1}{N}\sum_{j=1}^{N}a_{j}\sigma_{j}(\langle\bm{x}, \bm{w}_{j}\rangle+b_{j}),\] (3.1)

where \(\bm{\Theta}=(\bm{w}_{j},a_{j},b_{j})_{j=1}^{N}\) are trainable parameters, and \(\sigma_{j}:\mathbb{R}\rightarrow\mathbb{R}\) is the activation function defined as the sum of Hermite polynomials up to degree \(C_{\sigma}\): \(\sigma_{j}(z):=\sum_{i=0}^{C_{\sigma}}\beta_{j,i}\text{He}_{i}(z)\), where \(C_{\sigma}\) only depends on the degree of link function \(\sigma_{*}\). Note that we allow each neuron to have a different nonlinearity as indicated by the subscript in \(\sigma_{j}\); this subscript is omitted when we focus on the dynamics of one single neuron. Our SGD training procedure is described in Algorithm 1, and below we outline the key ingredients of the algorithm.

* Algorithm 1 employs a layer-wise training strategy common in the recent feature learning theory literature [22, 2, 3], where in the first stage, we optimize the first-layer parameters \(\{\bm{w}_{j}\}_{j=1}^{N}\) with normalized SGD to learn the low-dimensional latent representation (index features \(\bm{\theta}\)), and in the second phase, we train the second-layer \(\{a_{j}\}_{j=1}^{N}\) to fit the unknown link function \(\sigma_{*}\).
* The most crucial part in Phase I of Algorithm 1 is the reuse of the same minibatch in the gradient computation. Specifically, we sample a fresh batch of training examples in _every two GD steps_; this enables us to extract non-CSQ terms from two consecutive gradient updates outlined in (2.1).
* We introduce an _interpolation step_ between the current and previous iterates with hyperparameter \(\xi\) to stabilize the training dynamics; this resembles a negative momentum often seen in optimization algorithms [1, 1]; the role of this interpolation is discussed in Section 4.2. We use a projected gradient update \(\tilde{\nabla}_{\bm{w}}\mathcal{L}(\bm{w})=(\bm{I}_{d}-\bm{w}^{2t}\bm{w}^{2t} \,^{\top})\nabla_{\bm{w}}\mathcal{L}(\bm{w})\) for steps \(2t\) and \(2t+1\), where \(\nabla_{\bm{w}}\) is the Euclidean gradient; similar use of projection also appeared in [13, 1].

``` Input : Step sizes \(\eta^{t}\); momentum parameters \(\xi^{t}\); training time \(T_{1},T_{2}\); \(\ell_{2}\) regularization \(\lambda\). Initialize \(\bm{w}_{j}^{0}\sim\mathbb{S}^{d-1}(1)\), \(a_{j}\sim\text{Unif}\{\pm c_{\alpha}\}\). Phase I: normalized SGD on first-layer parameters for\(t=0\)to\(T_{1}\)do if\(t\) is eventhen \(\bm{x}\sim\mathcal{N}(0,\bm{I}_{d}),\ y=f_{*}(\bm{x})+\varsigma\) ; // Draw i.i.d. data \((\bm{x},y)\) \(\bm{w}_{j}^{t}\leftarrow\bm{w}_{j}^{t}-\xi_{j}^{t}(\bm{w}_{j}^{t}-\bm{w}_{j}^ {t-2})\), (when \(t>0\)) ; // Interpolation step \(\bm{w}_{j}^{t}\leftarrow\bm{w}_{j}^{t}/\|\bm{w}_{j}^{t}\|\) ; // Normalization  end if \(\bm{w}_{j}^{t+1}\leftarrow\bm{w}_{j}^{t}-\eta^{t}\tilde{\nabla}_{\bm{w}}(f_{ \bm{\Theta}}(\bm{x})-y)^{2}\), (\(j=1,\ldots,N\)) ; // SGD step  end for Initialize \(b_{j}\sim\text{Unif}([-C_{b},C_{b}])\). Phase II: SGD on second-layer parameters \(\hat{\bm{a}}\leftarrow\arg\!\min_{\bm{a}\in\mathbb{R}^{N}}\frac{1}{T_{2}}\sum_{ i=1}^{T_{2}}(f_{\bm{\Theta}}(\bm{x}_{i})-y_{i})^{2}+\lambda\|\bm{a}\|^{2}\) ; // Ridge regression Output: Prediction function \(\bm{x}\mapsto f_{\hat{\bm{\Theta}}}(\bm{x})\) with \(\hat{\bm{\Theta}}=(\hat{a}_{j},\bm{w}_{j}^{T_{1}},b_{j})_{j=1}^{N}\). ```

**Algorithm 1**Gradient-based training of two-layer neural network

### Convergence and Sample Complexity

Weak Recovery Guarantee.We first consider the "search phase" of SGD, and show that after running Phase I of Algorithm 1 for \(T=\text{polylog}(d)\) steps, a subset of parameters \(\bm{w}\) achieve non-trivial overlap with the target direction \(\bm{\theta}\). We denote \(H(g;j)\) as the \(j\)-th Hermite coefficient of some \(g\in L^{2}(\gamma)\). Our main theorems handle polynomial activations satisfying the following condition.

**Assumption 2**.: _We require the activation function to be a polynomial \(\sigma(z)=\sum_{i=0}^{C_{\sigma}}\beta_{i}\text{He}_{i}(z)\) and its degree \(C_{\sigma}\) to be sufficiently large so that \(C_{\sigma}\geq C_{\sigma}\) holds (\(C_{q}\) is defined in Proposition 6). For all \(2\leq\ell\leq C_{\sigma}\) and \(k=0,1\), we assume that \(H\big{(}\sigma^{(\ell)}(\sigma^{(1)})^{\ell-1};k\big{)}>0\)._As discussed in Appendix B.1, for a given \(\sigma_{*}\), the above assumption only needs to be met for one pair of \((k,\ell)\). Appendix B.1.3 states that \(H\big{(}\sigma^{(\ell)}(\sigma^{(1)})^{\ell-1};k\big{)}\neq 0\) also suffices if we set the momentum parameter \(\xi\) differently. Now we verify this condition for a wide range of polynomial activations.

**Lemma 3**.: _Given \(\ell\geq 2\) and \(k\geq 0\). For \(C_{\sigma}\geq\frac{2\ell+k-1}{\ell}\), if we choose \(\{\beta_{i}\}_{i=0}^{C_{\sigma}}\) where \(\beta_{i}\) is randomly drawn from some non-empty interval \([a_{i},b_{i}]\), then \(H(\sigma^{(\ell)}(\sigma^{(1)})^{\ell-1};k)\neq 0\) with probability 1._

The next theorem states that \(n=\tilde{\Theta}(d)\) samples are sufficient for SGD to achieve weak recovery.

**Theorem 1**.: _Under Assumptions 1 and 2, for suitable choices of hyperparameters \(\eta^{t}=\tilde{O}_{d}(Nd^{-1})\) and \(1-\xi^{t}=o_{d}(1)\), there exists constant \(C(q)\) such that after Phase I of Algorithm 1 is run for \(2T_{1,1}=C(q)\cdot d\mathrm{polylog}(d)\) steps, with high probability, there exists a subset of neurons \(\bm{w}_{j}^{2T_{1}}\in\mathcal{W}\) with \(|\mathcal{W}|=\tilde{\Theta}(N)\) such that \(\big{|}\langle\bm{w}_{j}^{2T_{1}},\bm{\theta}\rangle\big{|}>c\) for some \(c\gtrsim 1/\mathrm{polylog}(d)\)._

Recall that at random initialization we have \(\langle\bm{w},\bm{\theta}\rangle\approx d^{-1/2}\) with high probability. The theorem hence implies that SGD "escapes from mediocrity" after seeing \(n=\tilde{O}(d)\) samples, analogous to the information exponent \(p=2\) setting studied in [1]. We remark that due to the small second-layer initialization, the squared loss is dominated by the correlation loss, which allows us to track the evolution of each neuron independently; similar use of vanishing initialization also appeared in [1, 1].

Strong recovery and sample complexity.After weak recovery is achieved, we continue Phase I to amplify the alignment. Due to the nontrivial overlap between \(\bm{w}\) and \(\bm{\theta}\), the objective is no longer dominated by the lowest degree in the Hermite expansion. Therefore, to establish strong recovery (\(\langle\bm{w},\bm{\theta}\rangle>1-\varepsilon\)), we place an additional assumption on the activation function.

**Assumption 3**.: _Given the Hermite expansions \(\sigma_{*}(z)=\sum_{i=0}^{q}\alpha_{i}\mathsf{He}_{i}(z)\), \(\sigma_{j}(z)=\sum_{i=0}^{C_{\sigma}}\beta_{j,i}\mathsf{He}_{i}(z)\), we assume the coefficients satisfy \(\alpha_{i}\beta_{j,i}\geq 0\) for \(p\leq i\leq q\)._

This assumption is easily verified in the well-specified setting \(\sigma_{*}=\sigma\)[1] since \(\alpha_{i}=\beta_{i}\), and under link misspecification, it has been directly assumed in prior work [13]. We follow [13] and show that by randomizing the Hermite coefficients of the activation function, a subset of neurons satisfy the above assumption for any degree-\(q\) polynomial link function \(\sigma_{*}\).

**Lemma 4**.: _If we set \(\sigma_{j}(z)=\sum_{i=0}^{C_{\sigma}}\beta_{j,i}\mathsf{He}_{i}(z)\), where for each neuron we sample \(\beta_{j,i}\stackrel{{\mathrm{i.i.d.}}}{{\sim}}\mathrm{Unif}(\{ \pm r_{i}\})\) with appropriate constant \(r_{i}\), then Assumption 2 and 3 are satisfied in \(\exp(-\Theta(q))\)-fraction of neurons._

Note that in our construction of activation functions for both assumptions, we do not exploit knowledge of the link function \(\sigma_{*}\) other than its degree \(q\) which decides the constant \(C_{\sigma}\); see Appendix B.1 for more discussion of Assumption 3 and Lemma 4. The next theorem shows that by running Phase I for \(\tilde{\Theta}(d)\) more steps, a subset of neurons achieves sufficiently large overlap with the index features.

**Theorem 2**.: _For student neurons satisfying Assumptions 2, 3 and parameter \(\bm{w}_{j}\) starting from nontrivial overlap \(c>0\) specified in Theorem 1, if Phase I of Algorithm 1 continues for \(2T_{1,2}=\tilde{\Theta}_{d}(d\varepsilon^{-2})\) steps with hyperparameters \(\eta^{t}=\tilde{O}_{d}(Nd^{-1}\varepsilon)\), \(\xi^{t}=1\), we achieve \(\big{\langle}\bm{w}_{j}^{2(T_{1,1}+T_{1,2})},\bm{\theta}\big{\rangle}>1-\varepsilon\) with high probability._

The following proposition shows that after strong recovery, training the second-layer parameters in Phase II is sufficient for the NN model (3.1) to achieve small generalization error.

**Proposition 5**.: _After Phase I terminates, for suitable \(\lambda>0\), the output of Phase II satisfies_

\[\mathbb{E}_{\bm{x}}[(f_{\bm{\Theta}}(\bm{x})-f_{*}(\bm{x}))^{2}]\lesssim \varepsilon^{2}.\]

_with probability 1 as \(d\to\infty\), if we set \(T_{2}=C(q)N^{4}\mathrm{polylog}(d)\varepsilon^{-4}\), \(N=C(q)\mathrm{polylog}(d)\varepsilon^{-1}\) for some constant \(C(q)\) depending on the target degree \(q\)._

Putting things together.Combining the above theorems, we conclude that in order for two-layer NN (3.1) trained by Algorithm 1 to achieve \(\varepsilon\) population squared loss, it is sufficient to set

\[n=T_{1}+T_{2}\asymp C(q)\cdot(d\varepsilon^{-2}\lor\varepsilon^{-8})\mathrm{ polylog}(d),\quad N\asymp C(q)\cdot\varepsilon^{-1}\mathrm{polylog}(d),\]

where constant \(C(q)\) only depends on the target degree \(q\) (although exponentially). Hence we may set \(\varepsilon^{-1}\asymp\mathrm{polylog}d\) to conclude an almost-linear sample and computational complexity for learning arbitrary single-index polynomials up to \(o_{d}(1)\) population error.

Proof Sketch

In this section we outline the high-level ideas and key steps in our derivation.

### Monomial Transformation Reduces Information Exponent

To prove the main theorem, we first establish the existence of nonlinear label transformation that \((i)\) reduces the information exponent, and \((ii)\) can be easily extracted from SGD updates. If we ignore desideratum \((ii)\), then for polynomial link functions, transformations that decrease the information exponent to at most \(2\) have been constructed in [10, Section 2.1]. However, prior results are based on the thresholding function, and it is not clear if such function naturally arises from SGD with batch reuse. The following proposition shows that the effect of thresholding can also be achieved by a simple monomial transformation where the required degree can be uniformly upper bounded.

**Proposition 6**.: _Let \(g:\mathbb{R}\to\mathbb{R}\) be any polynomial with degree up to \(p\) and \(\|g\|_{L^{2}(\gamma)}^{2}=1\), then_

1. _There exists some_ \(i\leq C_{q}\in\mathbb{N}_{+}\) _such that_ \(\mathrm{IE}(g^{i})\leq 2\)_, where constant_ \(C_{q}\) _only depends on_ \(q\)_._
2. _Let_ \(g^{\mathrm{odd}}:\mathbb{R}\to\mathbb{R}\) _be the odd part of_ \(g\) _with_ \(\|g^{\mathrm{odd}}\|_{L^{2}(\gamma)}^{2}\geq\rho>0\)_. Then there exists some_ \(i\leq C_{q,\rho}\in\mathbb{N}_{+}\) _such that_ \(\mathrm{IE}(g^{i})=1\)_, where constant_ \(C_{q,\rho}\) _only depends on_ \(q\) _and_ \(\rho\)_._

The proof can be found in Appendix A. We make the following remarks.

* The proposition implies that for any polynomial link function that is not even, there exists some \(i\in\mathbb{N}_{+}\) only depending on the degree of \(\sigma_{*}\) such that raising the function to the \(i\)-th power reduces the information exponent to \(1\) (this implies the generative exponent \(p_{*}=1\)). For even \(\sigma_{*}\), the information exponent after arbitrary transformation is at least \(2\) (\(p_{*}=2\)), which can also be attained by monomial transformation. Furthermore, we provide a _uniform_ upper-bound on the required degree of transformation \(i\) via a compactness argument.
* The advantage of working with monomial transformations is that they can be obtained from two GD steps on the same training example, by Taylor expanding the activation \(\sigma^{\prime}\). In Section 4.2, we build upon this observation to show that Phase I of Algorithm 1 achieves weak recovery using \(n\gtrsim d\,\mathrm{polylog}(d)\) samples.

Intuition behind the analysis.Our proof is inspired by [10] which introduced a (non-polynomial) label transformation that reduces the information exponent of any degree-\(q\) polynomial to at most \(2\). To prove the existence of monomial transformation for the same purpose, we first show that for a fixed link function \(\sigma_{*}\), there exists some \(i\) such that the \(i\)-th power of the link function has information exponent \(2\), which mirrors the transformation used in [10]. Then, we make use of the compactness of the space of link functions to define a test function and obtain a uniform bound on \(i\). As for the polynomial transformation for non-even functions, we exploit the asymmetry of \(\sigma_{*}\) to further reduce the information exponent to 1.

### SGD with Batch Reuse Implements Polynomial Transformation

Now we present a more formal discussion of (2.1) to illustrate how polynomial transformation can be utilized in batch reuse SGD. We let \(\eta^{t}\equiv\eta\). When one neuron \(f_{\bm{w}}(\bm{x})=\sigma(\langle\bm{x},\bm{w}\rangle)\) is updated by two GD steps using the same sample \((\bm{x},y)\), starting from \(\bm{w}^{0}:=\bm{\omega}\), the alignment with \(\bm{\theta}\) becomes

\[\langle\bm{\theta},\bm{w}^{2}\rangle=\langle\bm{\theta},\big{[} \bm{w}^{1}+\eta\cdot y\sigma^{\prime}(\langle\bm{x},\bm{w}^{1}\rangle)\bm{x} \big{]}\rangle=\langle\bm{\theta},\bm{\omega}\rangle+\] \[\eta\bigg{[}y\sigma^{\prime}(\langle\bm{\omega},\bm{x}\rangle) \langle\bm{\theta},\bm{x}\rangle+\sum_{i=0}^{C_{q}-1}\underbrace{(\eta\|\bm{x} \|^{2})^{i}y^{i+1}(i!)^{-1}(\sigma^{\prime}(\langle\bm{\omega},\bm{x}\rangle)) ^{i}\sigma^{(i+1)}(\langle\bm{\omega},\bm{x}\rangle)\langle\bm{\theta},\bm{x }\rangle}_{=:\psi_{i}}\bigg{]}.\] (4.1)

We take \(\eta\leq c_{\eta}d^{-1}\) with a small constant \(c_{\eta}\) so that \(\eta\|\bm{x}\|^{2}\ll 1\) with high probability. Crucially, the strength of each term in (4.1) can vary depending on properties of the unknown link function \(\sigma_{*}\). Hence a careful analysis is required to ensure that a suitable monomial transformation is always singled out from the gradient. We establish the following lemma on the evolution of alignment.

**Lemma 7**.: _Under the assumptions per Theorem 1, the following holds for \(p_{*}=1,2\):_

\[\langle\bm{\theta},\bm{w}^{2(t+1)}\rangle\geq\langle\bm{\theta},\bm{w}^{2t} \rangle+c_{\eta}^{I}c_{\xi}c_{\sigma}d^{-\frac{p_{*}}{2}\lor 1}(\kappa^{2t})^{p_{*}-1}+c_{v }c_{\xi}d^{-\frac{p_{*}}{2}\lor 1}\nu^{2t}.\]See Lemma 16 for the formal version. For \(p_{*}=1\), taking expectation immediately yields that weak recovery within \((\eta(1-\xi)\gamma)^{-1}=O(d)\) steps. For \(p_{*}=2\), \(\langle\bm{\theta},\bm{w}_{j}^{2t}\rangle=:\kappa^{t}\) can be approximated by a differential equation \(\frac{\mathrm{d}\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ \bm{ }}}}}}}}}}}}}{4t}= \eta(1-\xi)\gamma\kappa^{t}\). Solving this yields \(\kappa^{t}=\kappa^{0}\exp(\eta(1-\xi)\gamma t)\approx d^{-\frac{1}{2}}\exp(\eta (1-\xi)\gamma t)\), and weak recovery is obtained within \(t\lesssim(\eta(1-\xi)\gamma)^{-1}\cdot\log d=O(d\log d)\) steps, similar to the analysis in [1].

Why interpolation is needed.In our setting, the signal strength may not dominate the error from discarding the effect of normalization. In prior analyses for online SGD, given the gradient \(-\bm{g}\) and projection \(P_{\bm{w}}=\bm{I}_{d}-\bm{w}\bm{w}^{\top}\), the spherical gradient changes the alignment as \(\langle\bm{\theta},\bm{w}^{t+1}\rangle=\big{\langle}\bm{\theta},\frac{\bm{w} ^{\prime}+\eta P_{\bm{w}}\bm{g}}{\|\bm{w}^{\prime}+\eta P_{\bm{w}}\bm{g}\|} \big{\rangle}\geq\langle\bm{\theta},\bm{w}^{t}\rangle+\eta\langle\bm{\theta}, \bm{g}\rangle-\frac{1}{2}\eta^{2}\|\bm{g}\|^{2}\langle\bm{\theta},\bm{w}^{t} \rangle+\text{(h.o.t.), see \@@cite[cite]{[\@@bibref{}{BAGJ21}{}{},DNGL23]}}\). Here \(\eta\langle\bm{\theta},\bm{g}\rangle\) corresponds to the signal, and \(-\frac{1}{2}\eta^{2}\|\bm{g}\|^{2}\langle\bm{\theta},\bm{w}^{t}\rangle\) comes from normalization. Thus, taking \(\eta\) sufficiently small, the normalization error shrinks faster than the signal. However, in our case the signal shrinks at the rate of \(c_{\eta}^{I}\) (recall that \(\eta=c_{\eta}d^{-1}\)), and hence taking a smaller step may not improve the signal-to-noise ratio when the degree of transformation \(I\) is large. The interpolation step in Algorithm 1 reduces the effect of normalization without shrinking the signal too much. In particular, by setting \(\xi=1-\tilde{\eta}\), we see that the signal is affected by a factor of \(\tilde{\eta}\) whereas the normalization error shrinks by \(\tilde{\eta}^{2}\); this allows us to boost the signal-to-noise ratio by taking \(\tilde{\eta}\) small.

### Analysis of Phase II and Statistical Guarantees

Once strong recovery is achieved for the first-layer parameters, we turn to Phase II and optimize the second-layer with \(\ell_{2}\) regularization. Since the objective is strongly convex, gradient-based optimization can efficiently minimize the empirical loss. In Appendix B.6, the learnability guarantee follows from standard analysis analogous to that in [1, 2, 1], where we construct a "certificate" second-layer \(\bm{a}^{*}\in\mathbb{R}^{N}\) that achieves small loss and small norm:

\[\mathbb{E}_{\bm{x}}\Big{(}f_{*}(\bm{x})-\tfrac{1}{N}\sum_{j=1}^{N}a_{j}^{*} \sigma_{j}\big{(}\langle\bm{w}_{j}^{T_{1}},\bm{x}\rangle+b_{j}\big{)}\Big{)}^{ 2}\leq\varepsilon^{*},\quad\|\bm{a}^{*}\|\lesssim r^{*},\]

from which the population loss of the regularized empirical risk minimizer can be bounded via standard Rademacher complexity argument. To construct such a certificate, we make use of the random bias units \(\{b_{j}\}_{j=1}^{N}\) to approximate the link function \(\sigma_{*}\) as done in [1, 1, 1].

## 5 Beyond Polynomial Link Functions

Thus far we have shown that for polynomial single-index target functions (which satisfy \(p_{*}\leq 2\)), SGD with data reuse can implement a polynomial transformation to the labels that reduces the information exponent to at most 2; consequently, the trained two-layer neural network can achieve small generalization error with \(n=d\operatorname{polylog}(d)\) samples. However, as shown in [1], there exists (non-polynomial) \(\sigma_{*}\) with generative exponent \(p_{*}>2\) (i.e., label transformations cannot lower the information exponent to 2) and thus not learnable by SQ algorithms in linear sample complexity.

Nevertheless, for a single-index model with generative exponent \(p_{*}\), we know there exists an "optimal" label transformation that reduces the information exponent to \(p_{*}\). If SGD can make use of such transformation, then from the arguments in [1], it is natural to conjecture that a sample size of \(n\simeq d^{p_{*}-1}\) is sufficient. In this section we confirm this intuition by proving that SGD with data reuse (Algorithm 1) indeed matches this complexity. The following lemma is an analogue of Proposition 6 stating that polynomial transformations are sufficient to lower the information exponent.

**Lemma 8**.: _Given \(\sigma_{*}\) with generative exponent \(p_{*}\in\mathbb{N}_{+}\). Suppose we can take an orthonormal polynomial basis \(\{\phi_{k}\}_{k}\) for the space \(L^{2}(P_{y})\) with inner product \(\langle f,g\rangle=\mathbb{E}_{y=\sigma_{*}(z)}[f(y)g(y)]\). Then there exists some degree of transformation \(I\in\mathbb{N}_{*}\) such that \(\operatorname{IE}(\sigma_{*}^{I})=p_{*}\)._

We outline the differences and additional technical challenges to handle the \(\operatorname{GE}(\sigma_{*})>2\) setting.

* For general \(L^{2}\) link functions \(\sigma_{*}\), we can no longer make use of the compactness argument (see proof of Proposition 6) to upper bound the degree of monomial transformation. Hence in Lemma 8 we do not state a uniform upper bound on the required degree \(I\), unlike the polynomial setting.
* Any link function with \(p_{*}>2\) cannot be polynomial, and hence we cannot achieve low generalization error using a neural network with polynomial nonlinearity. We therefore need to use an activation function with universal function approximation ability.

### Sample Complexity for Weak Recovery

We first show that Algorithm 1 achieves weak recovery (i.e., nontrivial overlap with the ground truth \(\bm{\theta}\)) with a complexity governed by the generative exponent of the link function \(p_{*}=\mathrm{GE}(\sigma_{*})\). Similar to Section 3.2, we make use of randomized activation functions to ensure the desired label transformation is encoded -- we defer the conditions on the student activation to Appendix B.1.2.

**Proposition 9**.: _Suppose the link function \(\sigma_{*}\) has generative exponent \(p_{*}\), and let \(I\in\mathbb{N}_{+}\) be the smallest degree of monomial transformation that lowers the information exponent to \(p_{*}\) (i.e., \(\mathrm{IE}(\sigma_{*}^{\prime})=p\)). We can find a student activation function \(\sigma\) depending only on \(p,p_{*}\) and \(I\), such that if we take \(\eta^{2t},\eta^{2t+1}=c_{\eta}Nd^{-1}\), \(\xi^{2(t+1)}=1-c_{\xi}d^{-(p_{*}-2)+/2}\) for small \(c_{\eta},c_{\xi}=o_{d}(1)\), and set_

\[T_{1,1}\simeq c_{\xi}^{-1}\begin{cases}d&(\text{if }p_{*}=1)\\ d(\log d)&(\text{if }p_{*}=2)\\ d^{p_{*}-1}&(\text{if }p_{*}\geq 3),\end{cases}\]

_then if the initial alignment \(\langle\bm{w}^{0},\bm{\theta}\rangle\geq 2c_{\eta}^{-1}d^{-\nicefrac{{1}}{{2}}}\), there exists \(\tau_{*}\leq T_{1,1}\) such that for all \(\tau\geq\tau_{*}\),_

\[\langle\bm{w}^{2\tau},\bm{\theta}\rangle\geq\tilde{\Theta}(1),\quad\text{ with probability }1-o_{d}(1).\]

Proposition 9 is a generalization of Theorem 1 beyond polynomial \(\sigma_{*}\) (the proof of both results are presented in Appendix B.3,B.4), and can be interpreted as an SQ counterpart to [1]: we establish a sufficient sample size of \(n\simeq d^{(p_{*}-1)\lor 1}\) for Algorithm 1 to exit the search phase, which is parallel to the \(n\simeq d^{(p-1)\lor 1}\) rate for one-pass SGD (note that our rates are slightly sharper due to logarithmic factors removed, since \(c_{\xi}^{-1}\) can grow arbitrarily slowly with \(d\)). For high generative exponent \(\sigma_{*}\) with \(p_{*}>2\), we no longer match the information theoretically optimal sample complexity \(n\asymp d\), which is consistent with the computational-to-statistical gap observed in [13].

### Generalization Error Guarantee

After Phase I of Algorithm 1, we learn the unknown link function \(\sigma_{*}\) via training the second-layer. To approximate non-polynomial functions, we introduce a ReLU component in the student nonlinearity \(\sigma\) (see Lemma 12 for discussions), and make use of the approximation result for the (univariate) ReLU kernel in [1], which handles general \(\sigma_{*}\) whose second derivative has bounded 4th moment. Combining the above, we arrive at the following end-to-end guarantee for learning single-index models with arbitrary generative exponent using SGD training of neural network.

**Proposition 10** (Informal).: _Suppose the link function \(\sigma_{*}\) has generative exponent \(p_{*}\in\mathbb{N}_{*}\) and satisfies \(\sigma_{*},\sigma_{*}^{\prime\prime}\in L^{4}(\gamma)\). For appropriately chosen activation function \(\sigma\) (see Appendix B.1.2), a neural network (3.1) with \(N=\tilde{\Theta}(1)\) neurons optimized by Algorithm 1 achieves small population loss \(\mathbb{E}_{\bm{x}}[(f_{\tilde{\bm{\Theta}}}(\bm{x})-f_{*}(\bm{x}))^{2}]=o_{d, \mathbb{P}}(1)\) with a sample complexity of \(n=\tilde{\Theta}(d^{(p_{*}-1)\lor 1})\)._

See Appendix B.6 for the full statement with \(\varepsilon\) dependence. This proposition confirms that the sample complexity for weak recovery (Proposition 9) is the bottleneck in single-index learning, as the total sample size required for Algorithm 1 to achieve low test error also scales with \(d^{(p_{*}-1)\lor 1}\).

## 6 Conclusion and Future Directions

We showed that a two-layer neural network (3.1) trained by SGD with reused batch can learn single-index model (with generative exponent \(p_{*}\)) using \(n\simeq d^{(p_{*}-1)\lor 1}\) samples and compute; in particular, when the link function \(\sigma_{*}\) is polynomial, we established a sample complexity of \(n=\tilde{O}(d\varepsilon^{-2})\) to achieve \(\varepsilon\) population loss, which is almost information theoretically optimal. Our analysis is based on the observation that by reusing the same training data twice in the gradient computation, a non-correlational term arises in the SGD update that transforms the labels (despite the loss function not modified). We proved that monomial transformations that lower the information exponent of \(\sigma_{*}\) can be extracted by Taylor-expanding the SGD update; then we showed via careful analysis of the trajectory that strong recovery and low population loss is achieved under suitable activation function.

Interesting future directions include extension to multi-index models [1, 1, 13, 14], hierarchical target functions [1, 12], and in-context learning [1]. Also, the SGD algorithm that we employ requires a layer-wise training procedure and a specific batch reuse schedule; one may therefore ask if standard multi-pass SGD training of all parameters simultaneously [1] (as reported in Figure 1) also achieves the same statistical efficiency.

## Acknowledgements

The authors thank Gerard Ben Arous, Joan Bruna, Alex Damian, Marco Mondelli, and Eshaan Nichani for the discussions and feedback on the manuscript. JDL acknowledges support of the ARO under MURI Award W911NF-11-1-0304, NSF CCF 2002272, NSF IIS 2107304, NSF CIF 2212262, ONR Young Investigator Award, and NSF CAREER Award 2144994. KO was partially supported by JST ACT-X (JPMJAX23C4). TS was partially supported by JSPS KAKENHI (24K02905) and JST CREST (JPMJCR2015). This research is unrelated to DW's work at xAI.

## References

* [AAM22] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks. In _Conference on Learning Theory_, pages 4782-4887. PMLR, 2022.
* [AAM23] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 2552-2623. PMLR, 2023.
* [ADK\({}^{+}\)24] Luca Arnaboldi, Yatin Dandi, Florent Krzakala, Luca Pesce, and Ludovic Stephan. Repetita iuvant: Data repetition allows sgd to learn high-dimensional multi-index functions. _arXiv preprint arXiv:2405.15459_, 2024.
* [AZ18] Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. _Journal of Machine Learning Research_, 18(221):1-51, 2018.
* [AZL19] Zeyuan Allen-Zhu and Yuanzhi Li. What can resnet learn efficiently, going beyond kernels? _Advances in Neural Information Processing Systems_, 32, 2019.
* [Bac17] Francis Bach. Breaking the curse of dimensionality with convex neural networks. _The Journal of Machine Learning Research_, 18(1):629-681, 2017.
* [BAGJ21] Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. Online stochastic gradient descent on non-convex losses from high-dimensional inference. _The Journal of Machine Learning Research_, 22(1):4788-4838, 2021.
* [BAGJ22] Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. High-dimensional limit theorems for sgd: Effective dynamics and critical scaling. _Advances in Neural Information Processing Systems_, 35:25349-25362, 2022.
* [BBPV23] Alberto Bietti, Joan Bruna, and Loucas Pillaud-Vivien. On learning Gaussian multi-index models with gradient flow. _arXiv preprint arXiv:2310.19793_, 2023.
* [BBSS22] Alberto Bietti, Joan Bruna, Clayton Sanford, and Min Jae Song. Learning single-index models with shallow neural networks. _Advances in Neural Information Processing Systems_, 35:9768-9783, 2022.
* [BES\({}^{+}\)22] Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. High-dimensional asymptotics of feature learning: How one gradient step improves the representation. _Advances in Neural Information Processing Systems_, 35:37932-37946, 2022.
* [BES\({}^{+}\)23] Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, and Denny Wu. Learning in the presence of low-dimensional structure: A spiked random matrix perspective. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [BF02] Nader H Bshouty and Vitaly Feldman. On using extended statistical queries to avoid membership queries. _Journal of Machine Learning Research_, 2(Feb):359-395, 2002.
* [BKM\({}^{+}\)19] Jean Barbier, Florent Krzakala, Nicolas Macris, Leo Miolane, and Lenka Zdeborova. Optimal errors and phase transitions in high-dimensional generalized linear models. _Proceedings of the National Academy of Sciences_, 116(12):5451-5460, 2019.

* [BL20] Yu Bai and Jason D. Lee. Beyond linearization: On quadratic and higher-order approximation of wide neural networks. In _International Conference on Learning Representations_, 2020.
* [BMZ23] Raphael Berthier, Andrea Montanari, and Kangjie Zhou. Learning time-scales in two-layers neural networks. _arXiv preprint arXiv:2303.00055_, 2023.
* [CCM11] Seok-Ho Chang, Pamela C Cosman, and Laurence B Milstein. Chernoff-type bounds for the Gaussian error function. _IEEE Transactions on Communications_, 59(11):2939-2944, 2011.
* [CM20] Sitan Chen and Raghu Meka. Learning polynomials in few relevant dimensions. In _Conference on Learning Theory_, pages 1161-1227. PMLR, 2020.
* [COB19] Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. _Advances in Neural Information Processing Systems_, 32, 2019.
* [CWPPS23] Elizabeth Collins-Woodfin, Courtney Paquette, Elliot Paquette, and Inbar Seroussi. Hitting the high-dimensional notes: An ode for sgd learning dynamics on glms and multi-index models. _arXiv preprint arXiv:2308.08977_, 2023.
* [DH18] Rishabh Dudeja and Daniel Hsu. Learning single-index models in gaussian space. In _Conference On Learning Theory_, pages 1887-1930. PMLR, 2018.
* [DH24] Rishabh Dudeja and Daniel Hsu. Statistical-computational trade-offs in tensor pca and related problems via communication complexity. _The Annals of Statistics_, 52(1):131-156, 2024.
* [DKL\({}^{+}\)23] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time. _arXiv preprint arXiv:2305.18270_, 2023.
* [DLS22] Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent. In _Conference on Learning Theory_, pages 5413-5452. PMLR, 2022.
* [DNGL23] Alex Damian, Eshaan Nichani, Rong Ge, and Jason D. Lee. Smoothing the landscape boosts the signal for SGD: Optimal sample complexity for learning single index models. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [DPVLB24] Alex Damian, Loucas Pillaud-Vivien, Jason D Lee, and Joan Bruna. The computational complexity of learning gaussian single-index models. _arXiv preprint arXiv:2403.05529_, 2024.
* [DTA\({}^{+}\)24] Yatin Dandi, Emanuele Troiani, Luca Arnaboldi, Luca Pesce, Lenka Zdeborova, and Florent Krzakala. The benefits of reusing batches for gradient descent in two-layer networks: Breaking the curse of information and leap exponents. _arXiv preprint arXiv:2402.03220_, 2024.
* [DWY21] Konstantin Donhauser, Mingqi Wu, and Fanny Yang. How rotational invariance of common kernels prevents generalization in high dimensions. In _International Conference on Machine Learning_, pages 2804-2814. PMLR, 2021.
* [Gla23] Margalit Glasgow. Sgd finds then tunes features in two-layer neural networks with near-optimal sample complexity: A case study in the xor problem. _arXiv preprint arXiv:2309.15111_, 2023.
* [GMMM21] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers neural networks in high dimension. _The Annals of Statistics_, 49(2):1029-1054, 2021.
* [HSSVG21] Daniel Hsu, Clayton H Sanford, Rocco Servedio, and Emmanouil Vasileios Vlatakis-Gkaragkounis. On the approximation power of two-layer networks of random relus. In _Conference on Learning Theory_, pages 2423-2461. PMLR, 2021.

* [JGH18] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In _Advances in neural information processing systems_, pages 8571-8580, 2018.
* [Kea98] Michael Kearns. Efficient noise-tolerant learning from statistical queries. _Journal of the ACM (JACM)_, 45(6):983-1006, 1998.
* [KMS20] Pritish Kamath, Omar Montasser, and Nathan Srebro. Approximate is good enough: Probabilistic variants of dimensional and margin complexity. In _Conference on Learning Theory_, pages 2236-2262. PMLR, 2020.
* [MHPG\({}^{+}\)23] Alireza Mousavi-Hosseini, Sejun Park, Manuela Giroti, Ioannis Mitliagkas, and Murat A Erdogdu. Neural networks efficiently learn low-dimensional representations with SGD. In _The Eleventh International Conference on Learning Representations_, 2023.
* [MHWSE23] Alireza Mousavi-Hosseini, Denny Wu, Taiji Suzuki, and Murat A. Erdogdu. Gradient-based feature learning under structured data. In _Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023)_, 2023.
* [MM18] Marco Mondelli and Andrea Montanari. Fundamental limits of weak recovery with applications to phase retrieval. In _Conference On Learning Theory_, pages 1445-1450. PMLR, 2018.
* [MZD\({}^{+}\)23] Arvind Mahankali, Haochen Zhang, Kefan Dong, Margalit Glasgow, and Tengyu Ma. Beyond ntk with vanilla gradient descent: A mean-field analysis of neural networks with polynomial width, samples, and time. _Advances in Neural Information Processing Systems_, 36, 2023.
* [NDL23] Eshaan Nichani, Alex Damian, and Jason D Lee. Provable guarantees for nonlinear feature learning in three-layer neural networks. _Advances in Neural Information Processing Systems_, 36, 2023.
* [O'D14] Ryan O'Donnell. _Analysis of Boolean Functions_. Cambridge University Press, 2014.
* [OSSW24a] Kazusato Oko, Yujin Song, Taiji Suzuki, and Denny Wu. Learning sum of diverse features: computational hardness and efficient gradient-based training for ridge combinations. In _Conference on Learning Theory_. PMLR, 2024.
* [OSSW24b] Kazusato Oko, Yujin Song, Taiji Suzuki, and Denny Wu. Pretrained transformer efficiently learns low-dimensional target functions in-context. _arXiv preprint arXiv:2411.02544_, 2024.
* [Rey20] Lev Reyzin. Statistical queries and statistical algorithms: Foundations and applications. _arXiv preprint arXiv:2004.00557_, 2020.
* [Sch80] Jacob T Schwartz. Fast probabilistic algorithms for verification of polynomial identities. _Journal of the ACM (JACM)_, 27(4):701-717, 1980.
* [TS24] Shokichi Takakura and Taiji Suzuki. Mean-field analysis on two-layer neural networks from a kernel perspective. _arXiv preprint arXiv:2403.14917_, 2024.
* [WWF24] Zhichao Wang, Denny Wu, and Zhou Fan. Nonlinear spiked covariance matrices and signal propagation in deep neural networks. In _Conference on Learning Theory_. PMLR, 2024.
* [YS19] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding neural networks. _Advances in Neural Information Processing Systems_, 32, 2019.
* [ZLBH19] Michael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps forward, 1 step back. _Advances in neural information processing systems_, 32, 2019.

## Table of Contents

* 1 Introduction
	* 1.1 Our Contributions
* 2 Problem Setting and Prior Works
	* 2.1 Complexity of Learning Single-index Models
	* 2.2 Can Gradient Descent Go Beyond Correlational Queries?
	* 2.3 Challenges in Establishing Statistical Guarantees
* 3 Learning Polynomial \(f_{*}\) in Linear Sample Complexity
	* 3.1 Training Algorithm
	* 3.2 Convergence and Sample Complexity
* 4 Proof Sketch
	* 4.1 Monomial Transformation Reduces Information Exponent
	* 4.2 SGD with Batch Reuse Implements Polynomial Transformation
	* 4.3 Analysis of Phase II and Statistical Guarantees
* 5 Beyond Polynomial Link Functions
	* 5.1 Sample Complexity for Weak Recovery
	* 5.2 Generalization Error Guarantee
* 6 Conclusion and Future Directions
* A Polynomial Transformation
* A.1 Proof for Even Functions \((i)\)
* A.2 Proof for Non-even Functions \((ii)\)
* A.3 Proof for Non-Polynomial Functions
* B SGD with Reused Batch
* B.1 Assumptions on Link Function
* B.2 Initialization
* B.3 Weak Recovery: Population Update
* B.4 Weak Recovery: Stochastic Update
* B.5 From Weak Recovery to Strong Recovery
* B.6 Second Layer Training
Polynomial Transformation

Proof of Proposition 6.We use a thresholding and compactness argument inspired by [13].

### Proof for Even Functions \((i)\)

We divide the analysis into the following steps.

**(i-1): Monomials reducing the information exponent.** Define \(\tau(f)=\max_{-2\leq t\leq 2}|f(t)|\). This entails that if \(|f(t)|\geq\tau(f)\), then we have \(|t|>2\).

Consider the following expectation:

\[\mathbb{E}_{t\sim\mathcal{N}(0,1)}\bigg{[}\bigg{(}\frac{f(t)}{2\tau(f)}\bigg{)} ^{i}(t^{2}-1)\bigg{]}.\] (A.1)

We evaluate the case when \(i\) is even. (A.1) can be lower bounded as

\[\text{(A.1)}=\mathbb{E}_{t\sim\mathcal{N}(0,1)}\bigg{[}\mathbbm{1} [|f(t)|\geq 2\tau(f)|\bigg{(}\frac{f(t)}{2\tau(f)}\bigg{)}^{i}(t^{2}-1) \bigg{]}\] \[\qquad+\mathbb{E}_{t\sim\mathcal{N}(0,1)}\bigg{[}\mathbbm{1}[|f(t )|<\tau(f)|\bigg{(}\frac{f(t)}{2\tau(f)}\bigg{)}^{i}(t^{2}-1)\bigg{]}\] \[\qquad+\mathbb{E}_{t\sim\mathcal{N}(0,1)}\bigg{[}\mathbbm{1}[|f(t )|<\tau(f)|\bigg{(}\frac{f(t)}{2\tau(f)}\bigg{)}^{i}(t^{2}-1)\bigg{]}\] \[\qquad\geq\mathbb{E}_{t\sim\mathcal{N}(0,1)}\bigg{[}\mathbbm{1}[| f(t)|\geq 2\tau(f)|\bigg{(}\frac{2\tau(f)}{2\tau(f)}\bigg{)}^{i}(2^{2}-1)\bigg{]}\] \[\qquad+\mathbb{E}_{t\sim\mathcal{N}(0,1)}\bigg{[}\mathbbm{1}[|f(t )|<2\tau(f)|\bigg{(}\frac{f(t)}{2\tau(f)}\bigg{)}^{i}(2^{2}-1)\bigg{]}\] \[\qquad+\mathbb{E}_{t\sim\mathcal{N}(0,1)}\bigg{[}\mathbbm{1}[|f(t )|<\tau(f)|\bigg{(}\frac{\tau(f)}{2\tau(f)}\bigg{)}^{i}(0^{2}-1)\bigg{]}\] \[\qquad\geq 3\mathbb{P}_{t\sim\mathcal{N}(0,1)}[|f(t)|\geq 2\tau(f) |-2^{-i}.\]

Note that \(\mathbb{P}[|f(t)|\geq 2\tau(f)]\) is positive (since \(f\) is polynomial) and independent of \(i\), while \(2^{-i}\) decays to \(0\) as \(i\) increases. Therefore, for sufficiently large \(i\in\mathbb{N}\), (A.1) is positive and hence \(\mathrm{IE}(f^{i})\leq 2\). The subsequent analysis aims to provide an upper bound on \(i\).

**(i-2): Construction of test function.** We introduce the notation \(H(\cdot;j)\) which takes any function (in \(L^{1}\)) and returns its \(j\)-th Hermite coefficient. We consider the following test function:

\[\mathscr{H}(f):=\sum_{i=2}^{\infty}\bigg{(}\frac{H(f^{i};2)}{2^{\frac{i}{2}} (2i-1)^{\frac{i}{2}}}\bigg{)}^{2}.\] (A.2)

**(i-3): Lower bound of test function via compactness.** Let \(\mathcal{F}_{q}\) be a set of polynomials with degree up to \(q\) with unit \(L^{2}\) norm. Because \(\mathscr{H}(f)\) is positive for any \(f\in\mathcal{F}_{q}\), \(H(f^{i};2)\) is continuous with respect to \(f\), and \(\mathcal{F}_{q}\) is a compact set, \(\inf_{f\in\mathcal{F}_{q}}\mathscr{H}(f)\) admits a minimum value \(\mathscr{H}_{0}\) which is positive.

**(i-4): Conclusion via hypercontractivity.** Because \(f\) is a polynomial with degree at most \(q\), Gaussian hypercontractivity [12] yields that

\[2H(f^{i};2)^{2}\leq\mathbb{E}_{t\sim\mathcal{N}(0,1)}\big{[}(f(t))^{2i}\big{]} \leq(2i-1)^{iq}\big{(}\mathbb{E}_{t\sim\mathcal{N}(0,1)}\big{[}f(t)^{2}\big{]} \big{)}^{i}=(2i-1)^{iq}.\]

Therefore, for all polynomials in \(\mathcal{F}_{q}\), a partial sum of (A.2) is uniformly bounded by

\[\bigg{|}\sum_{i=j}^{\infty}\bigg{(}\frac{H(f^{i};2)}{2^{\frac{i}{2}}(2i-1)^{ \frac{i}{2}}}\bigg{)}^{2}\bigg{|}\leq\sum_{i=j}^{\infty}2^{-i-1}=2^{-j}\to 0 \quad(j\to\infty).\]

Combining this with the fact that \(\mathscr{H}(f)\geq\mathscr{H}_{0}>0\), we know that there exists some \(C_{q}\leq 1+\log_{2}(\mathscr{H}_{0}^{-1})\) such that

\[\sum_{i=2}^{C_{q}}\bigg{(}\frac{H(f^{i};2)}{2^{\frac{i}{2}}(2i-1)^{\frac{i}{2} }}\bigg{)}^{2}>\frac{1}{2}\mathscr{H}_{0}>0,\]

for all polynomials in \(\mathcal{F}_{q}\). This means that there is at least one \(i\leq C_{q}\) such that \(H(f^{i};2)\neq 0\).

### Proof for Non-even Functions \((ii)\)

**(ii-1): Monomials reducing the information exponent.** We prove that some exponentiation of \(g:=f^{2}\) has non-zero first Hermite coefficient. Denote \(g^{\rm odd}\) as the odd part of \(g\), and similarly \(g^{\rm even}\). Let \(\upsilon(g)\in\mathbb{R}_{+}\) be the value at which the followings hold:

1. \(g^{\rm odd}(t)>0\) for all \(t\geq\upsilon(g)\) and \(g^{\rm odd}(t)<0\) for all \(t\leq-\upsilon(g)\).
2. \(g^{\rm even}(t)>|g^{\rm odd}(t)|\) for all \(t\geq\upsilon(g)\) and \(t\leq-\upsilon(g)\).
3. For for all \(t\geq\upsilon(g)\) and \(t\leq-\upsilon(g)\), \(g(s)=g(t)\) (as an equation of \(s\)) only has two real-valued solutions with opposing signs.

Such threshold \(\upsilon(g)\) exists because the tail of \(g=f^{2}\) is dominated by the highest degree which is even. Then, we let \(\tau(g)=\max_{-\upsilon(g)\leq t\leq\upsilon(g)}|g(t)|\).

Consider the following expectation:

\[\mathbb{E}_{t\sim\mathcal{N}(0,1)}\bigg{[}\bigg{(}\frac{g(t)}{2\tau(g)}\bigg{)} ^{i}t\bigg{]}.\] (A.3)

(A.3) is decomposed as

\[\eqref{eq:A.3}=\mathbb{E}_{t\sim\mathcal{N}(0,1)}\bigg{[}\mathbbm{1} \big{|}|g(t)|\geq 3\tau(f)\big{]}\bigg{(}\frac{g(t)}{3\tau(g)}\bigg{)}^{i}t \bigg{]}\] \[\qquad+\mathbb{E}_{t\sim\mathcal{N}(0,1)}\bigg{[}\mathbbm{1} \big{[}2\tau(g)\leq|g(t)|<3\tau(g)\big{]}\bigg{(}\frac{g(t)}{3\tau(f)}\bigg{)} ^{i}t\bigg{]}\] \[\qquad+\mathbb{E}_{t\sim\mathcal{N}(0,1)}\bigg{[}\mathbbm{1} \big{[}|g(t)|<2\tau(g)\big{]}\bigg{(}\frac{g(t)}{3\tau(g)}\bigg{)}^{i}t\bigg{]}.\] (A.4)

We first evaluate the first term. Because of (c), \(g(t)=3\tau(f)\) has two real-valued solutions \(\alpha<0<\beta\). Because of (a) and (b), \(g(\beta)=g^{\rm even}(\beta)+g^{\rm odd}(\beta)=3\tau(f)>g^{\rm even}(-\beta)+ g^{\rm odd}(-\beta)=g^{\rm odd}(-\beta)\). Because \(\lim_{t\to-\infty}g^{\rm odd}(t)=+\infty\), and \(\alpha\) is the only solution in \(t<0\), we have \(\alpha<-\beta\). Moreover, for all \(t>\beta\), we have \(g(t)=g^{\rm even}(t)+g^{\rm odd}(t)>g^{\rm even}(-t)+g^{\rm odd}(-t)=g^{\rm odd }(-t)\). Combining the above, the first term of (A.4) is bounded as

\[\mathbb{E}_{t\sim\mathcal{N}(0,1)}\bigg{[}\mathbbm{1}\big{[}|g(t) |\geq 3\tau(f)\big{]}\bigg{(}\frac{g(t)}{3\tau(g)}\bigg{)}^{i}t\bigg{]}\] \[\quad+\mathbb{E}_{t\sim\mathcal{N}(0,1)}\bigg{[}\mathbbm{1}\big{[} t\leq\alpha\big{]}\bigg{(}\frac{g(t)}{3\tau(g)}\bigg{)}^{i}t\bigg{]}\] \[\quad=\mathbb{E}_{t\sim\mathcal{N}(0,1)}\big{[}\mathbbm{1}\big{[} \beta\leq t\leq-\alpha\big{]}\bigg{.}.\]

Following the exact same reasoning, we know that the second term of (A.4) is positive. Finally, the third term which is bounded by

\[\mathbb{E}_{t\sim\mathcal{N}(0,1)}\big{[}\mathbbm{1}\big{[}|g(t)|<2\tau(g) \big{]}\bigg{(}\frac{g(t)}{3\tau(g)}\bigg{)}^{i}t\big{]}\geq-\mathbb{E}_{t\sim \mathcal{N}(0,1)}\big{[}\mathbbm{1}\big{[}|g(t)|<2\tau(g)\big{]}|t|\big{]}\bigg{(} \frac{2}{3}\bigg{)}^{i}.\]

Putting things together,

\[\eqref{eq:A.4}>\beta\mathbb{P}_{t\sim\mathcal{N}(0,1)}\big{[}\beta\leq t\leq- \alpha\big{]}-\mathbb{E}_{t\sim\mathcal{N}(0,1)}\big{[}\mathbbm{1}\big{[}|g(t)| <2\tau(g)\big{]}|t|\big{]}\bigg{(}\frac{2}{3}\bigg{)}^{i}.\]

The first term is independent of \(i\) and positive, while the second term goes to zero as \(i\) grows. Therefore, there exists some \(i\) such that \(\mathrm{IE}(g^{i};1)=1\).

**(ii-2): Construction of test function.** This time we consider the following function:

\[\mathscr{H}(f):=\sum_{i=2}^{\infty}\bigg{(}\frac{H(f^{i};1)}{2^{\frac{i}{2}}(2i-1) ^{\frac{iq}{2}}}\bigg{)}^{2}.\]

**(ii-3): Lower bound of test function via compactness.** Let \(\mathcal{F}_{q}\) be a set of unit \(L^{2}\)-norm polynomials with degree up to \(q\) and \(\mathbb{E}_{t\sim\mathcal{N}(0,1)}[f^{\mathrm{odd}}(t)^{2}]\geq c\). Since \(\mathscr{H}(f)\) is always positive for \(\mathcal{F}_{q}\), \(\mathscr{H}(f)\) is continuous with respect to \(f\), and \(\mathcal{F}_{q}\) is a compact set, \(\inf_{f\in\mathcal{F}_{q}}\mathscr{H}(f)\) has the minimum value \(\mathscr{H}_{0}\) that is positive. Note that \(\mathscr{H}(f)\) might depends on \(c\).

**(ii-4): Conclusion via hypercontractivity.** Using the same argument as in (i), we conclude that there exists some \(C_{q,c}\) such that

\[\sum_{i=2}^{C_{q}}\bigg{(}\frac{H(f^{i};1)}{2^{i}(2i-1)^{\frac{iq}{2}}}\bigg{)} ^{2}>\frac{1}{2}\mathscr{H}_{0}>0.\]

Because \(\mathscr{H}_{0}\) depends on \(c\), \(C_{q,c}\) depends on \(c\) as well as \(q\). 

### Proof for Non-Polynomial Functions

For non-polynomial link functions, we note that similar to [1], the existence of polynomial basis is needed to exclude extreme cases, and we cannot upper bound the required degree \(I\) because general link functions are not included in a compact space.

**Proof of Lemma 8.** The derivation is analogous to [1, Lemma F.14]. Let \(z\sim\mathcal{N}(0,1)\) and \(y=\sigma_{*}(z)\). We define \(\zeta_{p_{*}}(y)=\mathbb{E}[\frac{1}{\sqrt{p_{*}!}}]\mathrm{Re}_{p_{*}}(z)|y|\) and its basis expansion \(\zeta_{p_{*}}(y)=\sum_{k=0}^{\infty}v_{k}\phi_{k}\). Let \(K\) be a smallest integer such that \(v_{k}\neq 0\). Then, there exists an integer with \(I\leq K\) such that \(\mathrm{IE}(y^{I})=p_{*}\). Indeed,

\[\mathbb{E}[\phi_{K}(y)\mathrm{Re}_{p_{*}}(z)] =\mathbb{E}_{y}[\Phi_{K}(y)\mathbb{E}_{z|y}[\mathrm{Re}_{p_{*}}(z) |y]]\] \[=\mathbb{E}_{y}\bigg{[}\Phi_{K}(y)\sum_{k=0}^{K}v_{k}\phi_{k}(y) \bigg{]}=v_{K}\neq 0,\]

which means that at least one of \(y,y^{2},\cdots,y^{K}\) yields a non-zero \(p_{*}\)-th Hermite coefficient. 

## Appendix B SGD with Reused Batch

In this section we show that Algorithm 1 learns single-index models in \(\tilde{O}(d^{1\lor(p_{*}-1)})\) samples with high probability. The algorithm trains the first layer for \(T_{1}\) SGD steps, where we sample a new data point in every two steps. The first layer training is further divided into two phases: weak recovery (\(\bm{w}^{\top}\bm{\theta}\gtrsim 1\)) and strong recovery (\(\|\bm{w}-\bm{\theta}\|\lesssim\varepsilon\)). Then, we learn the second layer parameters.

Specifically, Section B.2 shows that at initialization, a (nearly) constant fraction of neurons has alignment \(\bm{w}^{\top}\bm{\theta}\) beyond a certain threshold. We focus on such neurons in the first phase of training. Section B.3 lower bounds the expected update of alignment \(\bm{w}^{\top}\bm{\theta}\) of two gradient steps, and Section B.4 establishes that the neurons achieve weak recovery within \(2T_{1,1}=\tilde{O}(d^{1\lor(p_{*}-1)})\) steps. Section B.5 discusses how to convert weak recovery to strong recovery using \(2T_{1,2}=\tilde{O}(d\varepsilon^{-2})\) more steps. We let \(T_{1}=2T_{1,1}+2T_{1,2}\). Finally, Section B.6 analyzes second layer training and concludes the proof.

In the following proofs, we use several constants, which depends on \(d\) at most at most polylogarithmically. Specifically, asymptotic strength of the constants is ordered as follows.

\[1\simeq c_{\sigma}\simeq C_{1}\lesssim\begin{cases}c_{\eta}^{-1} \simeq C_{2}\lesssim\mathrm{poly}(c_{\eta}^{-1})\lesssim\begin{cases}c_{1}^{- 1}\simeq C_{3}\\ c_{2}^{-1}\end{cases}\\ \delta^{-1}\end{cases}\begin{cases}\delta^{-1}\mathrm{poly}(c_{\eta}^{-1}) \lesssim c_{\xi}^{-1}\\ \mathrm{poly}(c_{1}^{-1})\lesssim\bar{c}_{\eta}^{-1}\end{cases}\\ \lesssim\mathrm{polylog}(d)=C_{4}.\]Here, \(c_{\eta}\) and \(\delta\) should satisfy \(\lim_{d\to\infty}c_{\eta}=\lim_{d\to\infty}\delta=0\), but the convergence can be arbitrarily slow, (e.g., as slow as \(1/\log\log\log\cdots\log d\)). This requirement comes from the fact that we do not know the exact value of \(H(\sigma_{s}^{t};p_{\star})\). To ensure that one signal term (from the Taylor series) is isolated, taking \(\eta\asymp d^{-1}\) with a sufficiently small constant is insufficient but \(\eta\asymp c_{\eta}d^{-1}\) with arbitrarily slow \(c_{\eta}\) suffices. Also, to guarantee that the failure probability is \(o_{d}(1)\), we require \(\delta\) to be \(o_{d}(1)\). \(c_{\xi}\) can also decay arbitrarily slowly, as long as it satisfies \(c_{\xi}\lesssim\delta\mathrm{poly}(c_{\eta}^{-1})\). \(C_{4}=\mathrm{polylog}(d)\) will be used to represent any polylogarithmic factor that comes from high probability bounds.

For the first-layer training, we can reduce the argument into training of one neuron using the correlation loss as follows. At each step, the gradient update (Line 8 of Algorithm 1) is written as

\[\bm{w}_{j}^{t+1} \leftarrow\bm{w}_{j}^{t}-\eta^{t}\tilde{\nabla}_{\bm{w}}\big{(}(f _{\bm{\Theta}}(\bm{x})-y)^{2}\big{)}\] \[=\bm{w}_{j}^{t}-\eta^{t}\tilde{\nabla}_{\bm{w}}\bigg{(}\frac{1}{ N}\sum_{j=1}^{N}a_{j}\sigma_{j}(\bm{w}_{j}^{t\top}\bm{x})\bigg{)}^{2}+2\eta_{j}^{t} \tilde{\nabla}_{\bm{w}}\bigg{(}y\frac{1}{N}\sum_{j=1}^{N}a_{j}\sigma_{j}(\bm{w }_{j}^{t\top}\bm{x})\bigg{)}\] \[=\bm{w}_{j}^{t}-\frac{2\eta^{t}c_{\alpha}^{2}}{N}\bigg{(}\frac{1} {N}\sum_{j=1}^{N}\sigma_{j}(\bm{w}_{j}^{t\top}\bm{x})\bigg{)}\big{(}\tilde{ \nabla}_{\bm{w}}\sigma_{j}(\bm{w}_{j}^{t\top}\bm{x})\big{)}+\frac{2\eta^{t}c_{ a}}{N}y\big{(}\tilde{\nabla}_{\bm{w}}\sigma_{j}(\bm{w}_{j}^{t\top}\bm{x}) \big{)}.\] (B.1)

While the second term scales with \(\eta^{t}c_{a}^{2}N^{-1}\), the third term scales with \(\eta^{t}c_{a}N^{-1}\). Thus, by setting \(c_{a}\) sufficiently small, we can ignore the interaction between neurons. We will show that the strength of the signal in the direction of \(\bm{\theta}\) is at least \(({\kappa}_{j}^{t})^{p_{\star}-1}\gtrsim d^{-\frac{p_{\star}-1}{2}}\) (up to a polylogarithmic factor, and \(p_{\star}=\mathrm{GE}(\sigma_{\star})\)). On the other hand, we can easily see that \(\bm{\theta}^{\top}\big{(}\frac{1}{N}\sum_{j=1}^{N}\sigma_{j}(\bm{w}_{j}^{t\top }\bm{x})\big{)}\big{(}\tilde{\nabla}_{\bm{w}}\sigma_{j}(\bm{w}_{j}^{t\top}\bm{ x})\big{)}\) is bounded by \(\tilde{O}(1)\) with high probability. Therefore, by simply letting \(c_{a}=\tilde{\Theta}(d^{-\frac{p_{\star}-1}{2}})\), we can ignore the effect of the second term in (B.1). Moreover, for simplicity, we will reparameterize \(\frac{2\eta^{t}c_{a}}{N}\) as \(\eta^{t}\) below. Consequently, we may analyze the following update

\[\bm{w}_{j}^{t+1}\leftarrow\bm{w}_{j}^{t}+\eta^{t}\tilde{\nabla}_{\bm{w}}\big{(} y\sigma_{j}(\bm{w}_{j}^{t\top}\bm{x})\big{)},\]

instead of Line 8 of Algorithm 1. Since there is no interaction between neurons now, we omit the subscript \(j\) when the context is clear.

### Assumptions on Link Function

The analysis consists of three different phases: weak recovery and strong recovery of the first-layer weights, and approximation of the link function (ridge regression of the second-layer). Each phase requires different assumptions on the activation functions, depending on the link function. Before starting the analysis, we decompose Assumptions 2 and 3 and clarify which conditions are needed in each phase. We prove that instead of using a specific activation function tailored to different link functions, a randomized activation function satisfies all required assumptions with probability \(\Omega(1)\). In the following, we write the student activation function as

\[\sigma_{j}(s):=\sum_{i=0}^{\infty}\beta_{j,i}\mathsf{He}_{i}(s)\]

with coefficients \(\{\beta_{j,i}\}_{i=0}^{C_{\sigma}}\) (sometimes the subscript \(j\), which is the index of the neurons, is omitted).

#### b.1.1 For polynomial link functions

In the following, we summarize the precise conditions to be satisfied by the activation functions (these conditions are weaker than Assumptions 2 and 3). For polynomial link functions, we focus on polynomial activation functions (with bounded degree) for simplicity, but non-polynomial activation functions would not change the proof significantly.

Let \(p\) and \(q\) be the minimum and maximum degree of non-zero Hermite coefficients of \(\sigma_{\star}\). Note that \(\mathrm{GE}(\sigma_{\star})=1\) or \(2\) holds (see Proposition 6). Let \(I\leq C_{q}\) (according to Proposition 6) be the smallest integer such that \(\mathrm{IE}(\sigma_{\star}^{I})=\mathrm{GE}(\sigma_{\star})=p_{\star}\) and \(C_{\sigma}\) be the degree of the activation function.

**(I) If \(I=1\Leftrightarrow\mathrm{IE}(\sigma_{\star})=\mathrm{GE}(\sigma_{\star})=p_{ \star}\).**

**Weak recovery:**\(\alpha_{p},\beta_{p_{\star}}>0\) (covered by Assumption 3).

**Strong recovery:**: \(\sum_{j=p_{*}}^{q}j!\alpha_{j}\beta_{j}s^{j-1}>0\) for all \(s>0\) (covered by Assumption 2).

**Approximation (ridge regression):**: \(\beta_{i}\neq 0\) for some \(i\geq q\) (covered by Assumption 3).
**(II) If \(2\leq I=\{\min\;i\;\,|\operatorname{IE}(\sigma_{*}^{I})=\operatorname{GE}( \sigma_{*})=p_{*}\}\leq C_{\sigma}\).**

**Weak recovery:**: \(H((\sigma_{*})^{I};p_{*})H(\sigma^{(I)}(\sigma^{(1)})^{I-1};p_{*}-1)>0\) (covered by Assumption 3).

**Strong recovery:**: \(\sum_{j=p_{*}+1}^{q}j!\alpha_{j}\beta_{j}s^{j-1}>0\) for all \(s>0\) (covered by Assumption 2).

**Approximation:**: \(\beta_{i}\neq 0\) for some \(i\geq q\) (covered by Assumption 3).

Note that it is difficult to construct a deterministic activation function that satisfies all of the assumptions for any link function \(\sigma_{*}\) (the simplest counterexample is to consider \(-\sigma_{*}\) which flips the Hermite coefficients). Instead, we show the existence of randomized construction of such an activation function that satisfies all of the assumptions on the activation function simultaneously with constant probability, which entails that a subset of neurons can achieve strong recovery. The construction does not depend on properties of the link function itself except for its degree \(q\).

**Lemma 11**.: _There exists a randomized activation function sampled from a discrete set such that the above conditions hold with constant probability._

**Proof.** Let \(c\) be a sufficiently small constant only used in this proof and \(C_{\sigma}\) be the minimum odd integer with \(C_{\sigma}\geq\max\{C_{q}+1,q+2,3\}\), where \(C_{q}\) was introduced in Proposition 6. With probability \(\frac{1}{2}\), we let \(\beta_{1}\sim\operatorname{Unif}(\{\pm 1\})\), and \(\beta_{j}\sim\operatorname{Unif}(\{\pm c\})\) for \(2\leq j\leq C_{\sigma}\). With probability \(\frac{1}{2}\), we let \(\beta_{j}\sim\operatorname{Unif}(\{\pm c\})\) for \(1\leq j\leq C_{\sigma}-2\) and \(\beta_{C_{\sigma}-1}=\beta_{C_{\sigma}}\sim\operatorname{Unif}(\{\pm 1\})\).

We first consider (I). When \(\beta_{1}\sim\operatorname{Unif}(\{\pm 1\})\), and \(\beta_{j}\sim\operatorname{Unif}(\{\pm c\})\) for \(2\leq j\leq C_{\sigma}\), it is easy to see \(\operatorname{sign}(\alpha_{j})=\operatorname{sign}(\beta_{j})\) for all \(j=1,\cdots,q\) hold with probability at least \(2^{-q}\), which is sufficient to satisfy (I).

We then consider (II). First focus on the case when \(p_{*}=1\) and \(I\) is even. When \(\beta_{1}\sim\operatorname{Unif}(\{\pm 1\})\) and \(\beta_{j}\sim\operatorname{Unif}(\{\pm c\})\) for \(2\leq j\leq C_{\sigma}\), by taking \(c\) sufficiently small, we have

\[H(\sigma^{(I)}(\sigma^{(1)})^{I-1};0)=\underbrace{I!\beta_{I}(\beta_{1})^{I-1 }}_{\widetilde{\times}\,c}+O(c^{2}).\] (B.2)

When \(I\) is even, by adjusting the sign of \(\beta_{1}\), \(H(\sigma^{(I)}(\sigma^{(1)})^{I-1};0)\) is non-zero and has the same sign as \(H((\sigma_{*})^{I};1)\) with probability \(\frac{1}{2}\). Note that the sign of \(\beta_{1}\) is independent from whether \(\sum_{j=2}^{q}j!\alpha_{j}\beta_{j}s^{j-1}>0\) for all \(s>0\) holds. This holds with probability at least \(2^{-q+1}\). Thus we verified (II) for \(p_{*}=1\) and even \(I\).

For \(p_{*}=1\) and odd \(I\), consider \(\beta_{j}\sim\operatorname{Unif}(\{\pm c\})\) for \(1\leq j\leq C_{\sigma}-2\) and \(\beta_{C_{\sigma}-1}=\beta_{C_{\sigma}}\sim\operatorname{Unif}(\{\pm 1\})\). Note that \(\sum_{j=2}^{q}j!\alpha_{j}\beta_{j}s^{j-1}>0\) for all \(s>0\) (this is the condition for strong recovery) and the condition for ridge regression also holds. Furthermore, the sign of \(H((\mathsf{H}\mathsf{e}_{C_{\sigma}}+\mathsf{H}\mathsf{e}_{C_{\sigma}-1})^{( I)}((\mathsf{H}\mathsf{e}_{C_{\sigma}}+\mathsf{H}\mathsf{e}_{C_{\sigma}-1})^{(1)})^{I-1} ;0)\) is \(\pm 1\) with equiprobability, independent of \(\beta_{2},\ldots,\beta_{q}\). Therefore, by taking \(c\) sufficiently small, we can obtain the desired sign of \(H(\sigma^{(I)}(\sigma^{(1)})^{I-1};0)\). Thus we proved (II) for \(p_{*}=1\) and odd \(I\).

Regarding (II) for \(p_{*}=2\) and even \(I\), when \(\beta_{1}\sim\operatorname{Unif}(\{\pm 1\})\) and \(\beta_{j}\sim\operatorname{Unif}(\{\pm c\})\) for \(2\leq j\leq C_{\sigma}\), we have

\[H(\sigma^{(I)}(\sigma^{(1)})^{I-1};1)=\underbrace{(I+1)!\beta_{I+1}(\beta_{1}) ^{I-1}}_{\widetilde{\times}\,c}+O(c^{2}).\]

Thus, similar to (II) with \(p_{*}=1\) and even \(I\), we get (II) for \(p_{*}=2\) and even \(I\).

Finally, consider (II) for \(p_{*}=2\) and odd \(I\). When \(\beta_{j}\sim\operatorname{Unif}(\{\pm c\})\) for \(1\leq j\leq C_{\sigma}-2\) and \(\beta_{C_{\sigma}-1}=\beta_{C_{\sigma}}\sim\operatorname{Unif}(\{\pm 1\})\), the sign of \(H((\mathsf{H}\mathsf{e}_{C_{\sigma}}+\mathsf{H}\mathsf{e}_{C_{\sigma}-1})^{(I)}(( \mathsf{H}\mathsf{e}_{C_{\sigma}}+\mathsf{H}\mathsf{e}_{C_{\sigma}-1})^{(1)})^{I- 1};1)\) is \(\pm 1\) with equiprobability when \(I\) is odd, and this term dominates the others in \(H(\sigma^{(I)}(\sigma^{(1)})^{I-1};1)\). Thus, (II) for \(p_{*}=2\) and odd \(I\) holds similarly to (II) for \(p_{*}=1\) and odd \(I\).

Now we have obtained the assertion for all cases. \(\square\)

#### b.1.2 For general link functions

Now we consider non-polynomial link functions with potentially large generative exponent \(p_{*}=\operatorname{GE}(\sigma_{*})\geq 2\). For weak and strong recovery to succeed, the conditions on the activation function are essentially the same as those for polynomial link functions:

**(I)**: **If** \(I=1\Leftrightarrow\mathrm{IE}(\sigma_{*})=\mathrm{GE}(\sigma_{*})=p_{*}\)**.**

**Weak recovery:** \(a_{p_{*}}\beta_{p_{*}}>0\)**.**

**Strong recovery:** \(\sum_{j=p_{*}}^{\infty}j!\alpha_{j}\beta_{j}s^{j-1}>0\) **for all** \(s>0\)**,**
**(II)**: **If** \(2\leq I=\{\min\ i\ |\ \mathrm{IE}(\sigma_{*}^{I})=\mathrm{GE}(\sigma_{*})=p_{*} \}\leq C_{\sigma}\)**.**

**Weak recovery:** \(H((\sigma_{*})^{I};p_{*})H(\sigma^{(I)}(\sigma^{(1)})^{I-1};p_{*}-1)>0\)**,**

**Strong recovery:** \(\sum_{j=p_{*}+1}^{\infty}j!\alpha_{j}\beta_{j}s^{j-1}>0\) **for all** \(s>0\)**.**

Due to the proof strategy (which uses Taylor expansion), we also require that all differentials and sum of expectations appearing in the following proofs are well-defined and bounded.

To approximate a non-polynomial \(\sigma_{*}\), we introduce the following condition on the activation function. We sample \(\sigma_{j}\) from a discrete set (with bounded cardinality). Let \(J\) be an index set such that the coefficients of \(\sigma_{j}\ (j\in J)\) satisfy the conditions above. Because we are selecting \(\sigma_{j}\) from a discrete set, \(|J|\simeq N\) holds. We introduce the following condition, which states that the target single-index model can be well-approximated by a linear combination of student neurons.

**Assumption 4**.: _When \(b_{j}\sim\mathrm{Unif}([-C_{b},C_{b}])\) where \((C_{b}=\mathrm{polylog}(d))\) and \(\bm{x}_{1},\ldots,\bm{x}_{T_{2}}\sim\mathcal{N}(0,\bm{I}_{d})\), there exists a set of coefficients \(a_{1},\ldots,a_{|J|}\) such that_

\[\frac{1}{T_{2}}\sum_{i=1}^{T_{2}}\left(\frac{1}{|J|}\sum_{j\in J}a_{j}\sigma_{ j}(\bm{\theta}_{j}^{\top}\bm{x}_{i}+b_{j})-\sigma_{*}(\bm{\theta}^{\top}\bm{x}_{ i})\right)^{2}\lesssim\varepsilon^{2},\]

_holds with coefficients of reasonable magnitudes \(\sum_{j\in J}a_{j}^{2}=\Theta(|J|)\) with high probability (w.r.t. the randomness of \(b_{j}\) and \(\bm{x}_{i}\)). Moreover, \(\mathbb{E}_{\bm{x}}[\sigma_{j}(\bm{\theta}_{j}^{\top}\bm{x}+b_{j})^{4}]\leq \mathrm{polylog}(d)\) for all \(j\) with high probability (w.r.t. the randomness of \(b_{j}\))._

The following lemma states that we can design a randomized activation function that satisfies all of the above assumptions with probability \(\Omega(1)\), as long as the link function \(\sigma\) satisfies Assumption 4 for \(\sigma=\mathrm{ReLU}\). In other words, we are able to cover the class of link functions \(\sigma\) that can be efficiently approximated by a two-layer ReLU network. Since the general link functions are not included in a compact space, we do not have an upper bound of exponent to obtain \(\mathrm{IE}(\sigma_{*}^{I})=\mathrm{GE}(\sigma_{*})\) as we had \(C_{q}\) in the polynomial case. Consequently, our student activation is not entirely agnostic to the link function \(\sigma_{*}\), as we require knowledge of \(p\) (information exponent), \(p_{*}\) and \(I\).

**Lemma 12**.: _Suppose the target link function \(\sigma_{*}\) satisfies Assumption 4 for \(\sigma_{j}=\mathrm{ReLU}\). There exists a randomized activation sampled from a discrete set such that the above conditions hold with constant probability._

Before we sketch the design of activation function, we present the following approximation result from [1], which establishes that Assumption 4 with \(\sigma_{j}=\mathrm{ReLU}\) is satisfied for broad class of functions, according to Lemma 4.4 and 4.5 of [1]. Specifically, taking \(\tau=1/2\) and \(\lambda=N^{-1}\) yields that \(\mathbb{E}_{\bm{x}}[(\frac{1}{|J|}\sum_{j\in J}a_{j}\sigma_{j}(\bm{\theta}^{ \top}\bm{x}+b_{j})-\sigma_{*}(\bm{\theta}^{\top}\bm{x}))^{2}]\leq N^{-\frac{ 2}{\sigma}}\). Although they sample \(b_{j}\) from Gaussian \(\mathcal{N}(0,2)\), the result translates to uniform sampling of biases from \([-C_{b},C_{b}]\) by introducing additional logarithmic factor.

**Lemma 13** (Lemma 4.4, 4.5 of [1]).: _Suppose that \(\mathbb{E}_{z\sim\mathcal{N}(0,1)}[\sigma_{*}(z)^{4}]\), \(\mathbb{E}_{z\sim\mathcal{N}(0,1)}[\sigma_{*}^{\prime\prime}(z)^{4}]<\infty\). Then, Assumption 4 with \(\sigma_{j}=\mathrm{ReLU}\) holds with \(\varepsilon=N^{-\frac{1}{\sigma}}\) and \(C_{b}\simeq\sqrt{\log d}\)._

**Proof of Lemma 12.** We show the existence of suitable \(\sigma\) in two steps: first we construct a randomized polynomial activation function that satisfies conditions (I)(II) with constant probability; then we add a small ReLU perturbation so that the activation can approximate non-polynomial \(\sigma_{*}\).

Recall \(p\in\mathbb{N}_{+}\) is the information exponent of \(\sigma_{*}\). We first show that there exists a randomized polynomial activation that satisfies the conditions for weak and strong recovery with probability \(\Omega(1)\). Note that the issue of differentiability and bounded moment is avoided when we focus on the polynomial activation functions. We specify the following two distributions. With probability \(\frac{1}{2}\), let \(\beta_{1}\sim\mathrm{Unif}(\{-1,1\})\), \(\beta_{j}\sim\mathrm{Unif}(\{-c,c\})\) for \(j=1,\cdots,p_{*}+I-1\) and \(\beta_{j}=0\) otherwise, where \(c>0\) is a sufficiently small constant. With probability \(\frac{1}{2}\), let \(\beta_{1}=\mathrm{Unif}(\{-1,1\})\), \(\beta_{2}=\mathrm{Unif}(\{-c,c\})\), \(\beta_{j}=\mathrm{Unif}(\{-c^{2},c^{2}\})\) for all \(2\leq j\leq(p_{*}+I)\lor p\) for a sufficiently small constant \(c>0\), and \(\beta_{j}=0\) otherwise.

Regarding (I), consider the case when the coefficients are sampled from the first distribution, and \(|\beta_{j}|\ll 1\) except for \(j=p_{*}\). Then, \(\sum_{j=p_{*}}^{\infty}\), \(j!\alpha_{j}\beta_{j}s^{j-1}\approx p_{*}!\alpha_{p_{*}}s^{p_{*}}\). Choosing the sign of \(\beta_{p_{*}}\), we have that the assumption holds with probability \(\Omega(1)\).

Regarding (II) with even \(I\), consider coefficients sampled from the first distribution, and \(\mathrm{Sign}(\beta_{j})=\mathrm{Sign}(\alpha_{j})\) for \(j\leq(p_{*}+I-1)\lor p\). Then, \(\sum_{j=p_{*}+1}^{\infty}j!\alpha_{j}\beta_{j}s^{j-1}>0\) for all \(s>0\). Also, similarly to (B.2),

\[H(\sigma^{(I)}(\sigma^{(1)})^{I-1};p_{*}-1)=\underbrace{i!\beta_{I+p_{*}-1}( \beta_{1})^{I-1}}_{\times\,c}+O(c^{2}).\]

By flipping the sign of \(\beta_{1}\), we can change the sign of \(H(\sigma^{(I)}(\sigma^{(1)})^{I-1};p_{*}-1)\). Thus, (II) for even \(I\) is satisfied by a randomized choice of \(\beta_{1}\).

For (II) with odd \(I\), consider coefficients sampled from the second distribution, and \(\mathrm{Sign}(\beta_{j})=\mathrm{Sign}(\alpha_{j})\) for \(j\leq(p_{*}+I)\lor p\). Then, \(\sum_{j=p_{*}+1}^{\infty}j!\alpha_{j}\beta_{j}s^{j-1}>0\) for all \(s>0\).

\[H(\sigma^{(I)}(\sigma^{(1)})^{I-1};p_{*}-1)=\frac{1}{(p_{*}-1)!} \mathbb{E}[\sigma^{(I)}(\sigma^{(1)})^{I-1}\mathsf{He}_{p_{*}-1}]\] \[=\frac{1}{(p_{*}-1)!}\mathbb{E}[(I-1)(\beta_{p_{*}+I}\mathsf{He}_ {p_{*}+I})^{(I)}(\beta_{2}\mathsf{He}_{2})^{(1)}(\beta_{1})^{I-2}\mathsf{He}_ {p_{*}-1}]+O(c^{4}).\] \[=\underbrace{\frac{2(I-1)\beta_{p_{*}+I}\beta_{2}(\beta_{1})^{I-2 }(p_{*}+I)!}{(p_{*}-1)!}}_{\times\,c^{3}}+O(c^{4}).\]

By flipping the sign of \(\beta_{1}\), we can change the sign of \(H(\sigma^{(I)}(\sigma^{(1)})^{I-1};p_{*}-1)\). Thus, (II) for odd \(I\) is satisfied by a randomized choice of \(\beta_{1}\).

Therefore, we have constructed a randomized polynomial activation \(\sigma\) that satisfies all of the conditions for weak and strong recovery. Now we provide a sketch of reasoning that when the link function \(\sigma_{*}\) is well-approximated by ReLU as specified in Assumption 4, we can find some \(\sigma\) that additionally satisfies Assumption 4 by introducing a small ReLU component. Specifically, we add \(c_{\mathrm{R}}\cdot\mathrm{ReLU}\) to the activation function with probability \(\frac{1}{2}\), with a sufficiently small \(c_{\mathrm{R}}=\tilde{\Omega}(1)\), e.g., \(c_{\mathrm{R}}=(\mathsf{log}d)^{-C}\) for some \(C>0\). When a two-layer ReLU network approximates \(\sigma_{*}\) that satisfies Assumption 4, by using the neurons with added ReLU component, \(\sigma_{*}\) can be approximated up to some polynomial residual with degree \((p_{*}+I)\lor p\). And by using the remaining polynomial neurons, we can approximate the additional polynomial terms in \(\sigma_{*}\) (see Lemma 22,23). Subtracting the latter from the former, we obtain the desired approximation result. When \(c_{\mathrm{R}}\) is sufficiently small, this additional term does not impact the conditions for weak and strong recovery and the moment calculations; similarly, since \(c_{\mathrm{R}}\ll 1\) we may discard this non-smooth term before Taylor expansion without affecting the analysis of optimization dynamics. We remark that to avoid such unnatural design of activation function, we can also train the first-layer parameters using a polynomial activation specified above, and then perturb it before the second-layer training to enhance the approximation ability -- such strategy has also been employed in prior layer-wise training analysis [1]. 

#### b.1.3 More Discussion on Assumption 2

Assumption 2 requires \(H(\sigma^{(I)}(\sigma^{(1)})^{I-1};p_{*}-1)\) is not zero and has the same sign as \(H(\sigma_{*}^{I};p_{*})\). We remark that if we allow a negative momentum parameter larger than \(1\), i.e., setting \(\xi^{2(t+1)}=1+c_{\xi}d^{-\frac{(p_{*}-2)}{2}}\), we can negate the opposite sign of \(H(\sigma^{(I)}(\sigma^{(1)})^{I-1};p_{*}-1)\) (see Lemma 16), and the subsequent analysis still holds. Therefore, what we essentially need is \(H(\sigma^{(i)}(\sigma^{(1)})^{i-1};k)\neq 0\). Lemma 3 confirms that it is satisfied by almost all polynomials:

**Proof of Lemma 3.** We note that \(H(\sigma^{(i)}(\sigma^{(1)})^{i-1};k)=\mathbb{E}[\sigma^{(i)}(\sigma^{(1)})^{i- 1}\mathsf{He}_{k}]\) is a polynomial of \(\{\beta_{j}\}_{j=0}^{C_{\sigma}}\). This polynomial is not identically equal to zero. To confirm this, consider \(\sigma=x^{C_{\sigma}}+x^{C_{\sigma}-1}\). Because \(\sigma^{(i)}(\sigma^{(1)})^{i-1}\) is expanded as a sum of \(x^{l}(i(C_{\sigma}-3)\leq l\leq i(C_{\sigma}-2)+1\) with positive coefficients and each \(x^{l}\) is a sum of \(\mathsf{He}_{l},\mathsf{He}_{l-2}\cdots\) with positive coefficients, \(\sigma^{(i)}(\sigma^{(1)})^{i-1}\) has all positive Hermite coefficients for degree \(0,1,\cdots,i(C_{\sigma}-2)+1\). If \(k\leq i(C_{\sigma}-2)+1\)this choice of \(\sigma\) yields \(H(\sigma^{(i)}(\sigma^{(1)})^{i-1};k)>0\), which confirms that \(H(\sigma^{(i)}(\sigma^{(1)})^{i-1};k)\) as a polynomial of \(\{\beta_{j}\}_{j=0}^{C_{\sigma}}\) is not identically equal to zero. Hence the assertion follows from so-called Schwartz-Zippel Lemma [13], or the fact that zeros of a non-zero polynomial form a measure-zero set. 

### Initialization

We first consider the initial alignment. In the following sections, we focus on the neurons that satisfy \(\kappa_{j}^{0}=\bm{\theta}^{\top}\bm{w}_{j}^{0}\geq 2c_{\eta}^{-1}d^{-\frac{1}{2}}\) at the initialization. The following lemma states that roughly a constant portion of the neurons satisfy the initial alignment condition upon random initialization. In particular, if we take \(c_{\eta}=\Omega((\log\log d)^{-\frac{1}{2}})\), the fraction of neurons that satisfy the initial alignment condition is at least \(e^{-16c_{\eta}^{-2}}=\tilde{\Omega}(1)\). Let us write \(C_{2}=c_{\eta}^{-1}\) for simplicity in the following.

**Lemma 14**.: _At the time of initialization, \(\kappa_{j}^{0}=\bm{\theta}^{\top}\bm{w}^{0}\) satisfies the following:_

\[\mathbb{P}[\kappa_{j}^{0}\geq 2C_{2}d^{-\frac{1}{2}}]=\mathbb{P}[\kappa_{j}^ {0}\leq-2C_{2}d^{-\frac{1}{2}}]\gtrsim e^{-16C_{2}^{2}}=\tilde{\Omega}(1).\]

We make use of the following lemma.

**Lemma 15** (Theorem 2 of [1]).: _For any \(\beta>1\) and \(s\in\mathbb{R}\), we have_

\[\frac{\sqrt{2e(\beta-1)}}{2\beta\sqrt{\pi}}e^{-\frac{\beta s^{2}}{2}}\leq\int_ {s}^{\infty}\frac{1}{\sqrt{2\pi}}e^{-\frac{s^{2}}{2}}\mathrm{d}t\]

Proof of Lemma 14.Because \(\kappa^{0}=\bm{v}^{\top}\bm{w}\stackrel{{\mathrm{d}}}{{=}}\frac{ \bm{e}_{\top}^{\top}\bm{g}}{\|\bm{g}\|}\), where \(\bm{g}\sim\mathcal{N}(0,\bm{I}_{d})\),

\[\mathbb{P}[\kappa_{j}^{0}\geq 2C_{2}d^{-\frac{1}{2}}] =\mathbb{P}_{\bm{g}\sim\mathcal{N}(0,\bm{I}_{d})}\bigg{[}\bm{e}_ {1}^{\top}\bm{g}\geq 4C_{2}\wedge\|\bm{g}\|\leq 2d^{\frac{1}{2}}\bigg{]}\] \[\geq\mathbb{P}_{\bm{g}\sim\mathcal{N}(0,\bm{I}_{d})}\bigg{[}\bm{ e}_{1}^{\top}\bm{g}\geq 4C_{2}\bigg{]}-\mathbb{P}_{\bm{g}\sim\mathcal{N}(0,\bm{I} _{d})}\bigg{[}\|\bm{g}\|\geq 2d^{\frac{1}{2}}\bigg{]}\] \[\gtrsim\frac{\sqrt{2e(\beta-1)}}{2\beta\sqrt{\pi}}e^{-8\beta C_{ 2}^{2}}-e^{-\Omega(d)},\]

where we used Lemma 15 for the final inequality. By letting \(\beta=2\), we have that \(\mathbb{P}[\kappa_{j}^{0}\geq C_{2}d^{-\frac{1}{2}}]\gtrsim e^{-16C_{2}^{2}}\). Because of the symmetry, \(\mathbb{P}[\kappa_{j}^{0}\leq 2C_{2}d^{-\frac{1}{2}}]=\mathbb{P}[\kappa_{j}^{0} \geq 2C_{2}d^{-\frac{1}{2}}]\). 

### Weak Recovery: Population Update

We divide the first layer training into the first phase (weak recovery) and the second phase (strong recovery). We first evaluate the expected update of two gradient steps with the same training example.

**Lemma 16**.: _Let \(\eta^{2t},\eta^{2t+1}=\eta=c_{\eta}d^{-1}\), \(\xi^{2(t+1)}=\xi=1-c_{\xi}d^{-\frac{(p_{*}-2)}{2}+}\). Suppose that the link function satisfies \(\mathrm{IE}(\sigma_{*}^{I})=\mathrm{GE}(\sigma_{*})=p_{*}\) (we choose the smallest such 1) and activation functions satisfy all of the assumptions in Section B.1 for weak recovery. Then, for \(\bm{w}^{2t}\) with \(c_{\eta}^{-1}d^{-\frac{1}{2}}\leq\bm{\theta}^{\top}\bm{w}^{2t}\leq c_{\eta}^{I}\), the alignment \(\bm{\theta}^{\top}\bm{w}^{2(t+1)}\) can be evaluated as,_

\[\bm{\theta}^{\top}\bm{w}^{2(t+1)}\geq\bm{\theta}^{\top}\bm{w}^{2t}+c_{\eta}^{ I}c_{\xi}c_{\sigma}d^{-\frac{p_{*}}{2}\lor 1}(\kappa^{2t})^{p_{*}-1}+c_{\eta}c_{\xi}d^{- \frac{p_{*}}{2}\lor 1}\nu^{2t}.\]

_Here \(c_{\sigma}=p_{*}!\alpha_{p_{*}}\beta_{p_{*}}\) (when \(\mathrm{IE}(\sigma_{*})=\mathrm{GE}(\sigma_{*})\)) or \(c_{\sigma}=\frac{p_{*}!H(\sigma_{*}^{I};p_{*})H(\sigma^{(I)}(\sigma^{(I)})^{I -1};p_{*}-1)}{2(I-1)!}\) (otherwise), and \(\nu^{2t}\) is a mean-zero sub-exponential random variable._

Proof.: The expected alignment \(\bm{\theta}^{\top}\bm{w}^{2(t+1)}\) after two gradient steps from \(\bm{w}^{2t}=\bm{\omega}\) using the same sample \((\bm{x},y)\), step size \(\eta^{2t}=\eta^{2t+1}=\eta=c_{\eta}d^{-1}\) and momentum parameter \(\xi^{2(t+1)}=\xi=\xi\)\(1-c_{\xi}d^{-\frac{(p_{*}-2)_{+}}{2}}\) is evaluated as follows. With a projection matrix \(\bm{P}_{\bm{\omega}}=\bm{I}-\bm{\omega}\bm{\omega}^{\top}\), the first step updates the weight as

\[\bm{w}^{2t+1}\leftarrow\bm{w}^{2t}+\eta\tilde{\nabla}_{\bm{w}}y \sigma(\bm{w}^{2t\top}\bm{x})=\bm{\omega}+\eta y\sigma^{\prime}(\bm{\omega}^{ \top}\bm{x})\bm{P}_{\bm{\omega}}\bm{x},\] (B.3)

and the next gradient step with the same sample is computed as

\[\tilde{\nabla}_{\bm{w}}y\sigma(\bm{w}^{2t+1\top}\bm{x}) =y\sigma^{\prime}(\bm{w}^{2t+1\top}\bm{x})\bm{x}\] \[=y\sigma^{\prime}\big{(}(\bm{\omega}+\eta y\sigma^{\prime}(\bm{ \omega}^{\top}\bm{x})\bm{P}_{\bm{\omega}}\bm{x})^{\top}\bm{x}\big{)}\bm{P}_{ \bm{\omega}}\bm{x}\] \[=y\sigma^{\prime}\big{(}\bm{\omega}^{\top}\bm{x}+\eta\|\bm{x}\|_ {\bm{P}_{\bm{\omega}}}^{2}\sigma^{\prime}(\bm{\omega}^{\top}\bm{x})y\big{)}\bm {P}_{\bm{\omega}}\bm{x},\] (B.4)

here we used the notation \(\|\bm{\theta}\|_{A}^{2}=\bm{\theta}^{\top}A\bm{\theta}\) for a vector \(\bm{\theta}\in\mathbb{R}^{d}\) and a positive symmetric matric \(A\in\mathbb{R}^{d\times d}\). From (B.3) and (B.4), the parameter after the two steps is obtained as

\[\bm{w}^{2(t+1)} \leftarrow\bm{w}^{2t+1}+\eta\tilde{\nabla}_{\bm{w}}y\sigma(\bm{w} ^{2t+1\top}\bm{x})\] \[=\bm{\omega}+\eta y\sigma^{\prime}(\bm{\omega}^{\top}\bm{x})\bm{P }_{\bm{\omega}}\bm{x}+\eta y\sigma^{\prime}\big{(}\bm{\omega}^{\top}\bm{x}+ \eta\|\bm{x}\|_{\bm{P}_{\bm{\omega}}}^{2}\sigma^{\prime}(\bm{\omega}^{\top}\bm{ x})y\big{)}\bm{P}_{\bm{\omega}}\bm{x}.\] \[=\bm{\omega}+\eta\bm{g}^{2t},\]

where

\[\bm{g}^{2t}=y\sigma^{\prime}(\bm{\omega}^{\top}\bm{x})\bm{P}_{ \bm{\omega}}\bm{x}+y\sigma^{\prime}\big{(}\bm{\omega}^{\top}\bm{x}+\eta\|\bm{ x}\|_{\bm{P}_{\bm{\omega}}}^{2}\sigma^{\prime}(\bm{\omega}^{\top}\bm{x})y \big{)}\bm{P}_{\bm{\omega}}\bm{x}.\]

Finally, the normalization step yields

\[\bm{w}^{2(t+1)}\leftarrow\frac{\bm{w}^{2(t+1)}-\xi^{2(t+1)}(\bm{w} ^{2(t+1)}-\bm{w}^{2t})}{\|\bm{w}^{2(t+1)}-\xi^{2(t+1)}(\bm{w}^{2(t+1)}-\bm{w}^ {2t})\|}=\frac{\bm{\omega}+\eta\xi\bm{g}^{2t}}{\|\bm{\omega}+\eta\xi\bm{g}^{2t }\|}=\frac{\bm{\omega}+c_{\eta}c_{\xi}d^{-\frac{p_{*}}{2}\lor 1}\bm{g}^{2t}}{\|\bm{ \omega}+c_{\eta}c_{\xi}d^{-\frac{p_{*}}{2}\lor 1}\bm{g}^{2t}\|}.\]

Therefore, by writing \(\bm{\theta}^{\top}\bm{w}^{2t}=\kappa^{2t}\), the update of the alignment is

\[\kappa^{2(t+1)}=\bm{\theta}^{\top}\bm{w}^{2(t+1)}\] \[=\frac{\kappa^{2t}+c_{\eta}c_{\xi}d^{-\frac{p_{*}}{2}\lor 1}\bm{ \theta}^{\top}\bm{g}^{2t}}{\|\bm{\omega}+c_{\eta}c_{\xi}d^{-\frac{p_{*}}{2} \lor 1}\bm{g}^{2t}\|}\] \[\geq\kappa^{2t}+c_{\eta}c_{\xi}d^{-\frac{p_{*}}{2}\lor 1}\bm{ \theta}^{\top}\bm{g}^{2t}-\frac{1}{2}\kappa^{2t}c_{\eta}^{2}c_{\xi}^{2}d^{-p_ {*}\lor 2}\|\bm{g}^{2t}\|^{2}-\frac{1}{2}c_{\eta}^{3}c_{\xi}^{3}d^{-\frac{3p_ {*}}{2}\lor 3}|\bm{\theta}^{\top}\bm{g}^{2t}\|\bm{\beta}^{2t}\|^{2}.\] (B.5)

We can easily see that \(\mathbb{E}[\|\bm{g}^{2t}\|^{2}]\lesssim d\) and \(\mathbb{E}[|\bm{\theta}^{\top}\bm{g}^{2t}|\|\bm{g}^{2t}\|^{2}]\lesssim d\), which implies that the expectation of the last two terms of (B.5) is bounded by \(\lesssim\kappa^{2t}c_{\eta}^{2}c_{\xi}^{2}d^{-(p_{*}-1)\lor 1}\lor c_{\eta}^{3}c_{ \xi}^{3}d^{-(\frac{3p_{*}}{2}-1)\lor 2}\leq c_{\eta}^{2}c_{\xi}^{2}d^{-(p_{*}-1)\lor 1} \kappa^{2t}\).

Now we bound \(\mathbb{E}[\bm{\theta}^{\top}\bm{g}^{2t}]\) by \(\gtrsim c_{\eta}^{I-1}\kappa^{p_{*}-1}\). Let \(C_{\sigma}\) be the maximum degree of the activation function with non-zero coefficients of Hermite expansion, which may be infinity when we consider general link functions, and there appear some infinite sums. For these cases we simply assume the sums converge - we discuss the validity of this condition in Section B.1.2. We omit the subscript \(2t\) in the following for simplicity. We divide the analysis into the two cases.

(I) If \(I=1\Leftrightarrow\mathrm{IE}(\sigma_{*})=\mathrm{GE}(\sigma_{*})=p_{*}\).For the first term of \(\mathbb{E}[\bm{\theta}^{\top}\bm{g}]\), we have

\[\bm{\theta}^{\top}\mathbb{E}[y\sigma^{\prime}(\bm{\omega}^{\top} \bm{x})\bm{P}_{\bm{\omega}}\bm{x}] =\bm{\theta}^{\top}\bm{P}_{\bm{\omega}}\mathbb{E}\bigg{[}\bigg{(} \sum_{j=p_{*}}^{C_{\sigma}}\alpha_{j}\mathsf{He}_{j}(\bm{\theta}^{\top}\bm{x}) \bigg{)}\bigg{(}\sum_{j=1}^{C_{\sigma}}j\beta_{j}\mathsf{He}_{j-1}\big{(}\bm{ \omega}^{\top}\bm{x}\big{)}\bigg{)}\bm{x}\bigg{]}\] \[=\bm{\theta}^{\top}\bm{P}_{\bm{\omega}}\sum_{j=p_{*}}^{\infty} \bigg{[}j!\alpha_{j}\beta_{j}\big{(}\bm{\theta}^{\top}\bm{\omega}\big{)}^{j-1}\bm{ \theta}+(j+2)!\alpha_{j}\beta_{j+2}\big{(}\bm{\theta}^{\top}\bm{\omega}\big{)}^{j} \bm{\omega}\bigg{]}\] \[=\sum_{j=p_{*}}^{C_{\sigma}}j!\alpha_{j}\beta_{j}\big{(}\bm{ \theta}^{\top}\bm{\omega}\big{)}^{j-1}\bm{\theta}^{\top}\bm{P}_{\bm{\omega}}\bm{\theta}\] \[=p_{*}!\alpha_{p_{*}}\beta_{p_{*}}\kappa^{p_{*}-1}+O(\kappa^{p_{*} }).\] (B.6)

[MISSING_PAGE_EMPTY:24]

[MISSING_PAGE_FAIL:25]

\[=c_{\eta}^{I-1}p_{*}!d^{-(I-1)}\mathbb{E}_{z\sim\chi_{d-2}^{2}}[z^{I-1}]H(\sigma_{ *}^{I};p_{*})H(\sigma^{(I)}(\sigma^{(1)})^{I-1};p_{*}-1)(1-\kappa^{2})\kappa^{p_{* }-1}+O(c_{\eta}^{I-1}\kappa^{p_{*}}).\]

Putting it all together (recovering the constants omitted in (B.12) again),

(B.11) \[=c_{\eta}^{I-1}\frac{p_{*}!d^{-(I-1)}\mathbb{E}_{z\sim\chi_{d-2}^{2}}[z^{I-1}]} {(I-1)!}H(\sigma_{*}^{I};p_{*})H(\sigma^{(I)}(\sigma^{(1)})^{I-1};p_{*}-1) \kappa^{p_{*}-1}+O(c_{\eta}^{I}\kappa^{p_{*}-1}+\kappa^{p_{*}}),\]

and

\[\mathbb{E}[\boldsymbol{\theta}^{\top}\boldsymbol{g}]=\eqref{eq:c_ \eta}+\eqref{eq:c_\eta}\] \[=c_{\eta}^{I-1}\underbrace{\frac{p_{*}!d^{-(I-1)}\mathbb{E}_{z \sim\chi_{d-2}^{2}}[z^{I-1}]}{(I-1)!}}_{=\Theta(1)}H(\sigma_{*}^{I};p_{*})H( \sigma^{(I)}(\sigma^{(1)})^{I-1};p_{*}-1)\kappa^{p_{*}-1}+O(c_{\eta}^{I}\kappa ^{p_{*}-1}+\kappa^{p_{*}}).\]

Combining (i) and (ii), we have

\[\mathbb{E}[\boldsymbol{\theta}^{\top}\boldsymbol{g}]\geq 2c_{\eta}^{I-1}c_{ \sigma}\kappa^{p_{*}-1}+O(c_{\eta}^{I}\kappa^{p_{*}-1}+\kappa^{p_{*}})\]

for a positive constant \(c_{\sigma}=\Theta(1)\). Here \(c_{\sigma}>0\) satisfies \(2c_{\sigma}=2p_{*}!\alpha_{p_{*}}\beta_{p_{*}}\) (for (i)) or \(2c_{\sigma}=\frac{p_{*}!H(\sigma_{*}^{I};p_{*})H(\sigma^{(I)}(\sigma^{(1)})^{I -1};p_{*}-1)}{(I-1)!}\) (for (ii)). Going back to (B.5), by setting \(\nu^{2t}=(\boldsymbol{\theta}^{\top}\boldsymbol{g}^{2t}-\mathbb{E}[\boldsymbol {\theta}^{\top}\boldsymbol{g}^{2t}])\), we have

\[\kappa^{2(t+1)} \geq\kappa^{2t}+2c_{\eta}c_{\xi}d^{-\frac{p_{*}}{2}\lor 1}\mathbb{ E}[\boldsymbol{\theta}^{\top}\boldsymbol{g}^{2t}]+c_{\eta}c_{\xi}d^{- \frac{p_{*}}{2}\lor 1}(\boldsymbol{\theta}^{\top}\boldsymbol{g}^{2t}- \mathbb{E}[\boldsymbol{\theta}^{\top}\boldsymbol{g}^{2t}])+O(c_{\eta}^{2}c_{ \xi}^{2}d^{-(p_{*}-1)\lor 1}\kappa^{2t})\] \[=\kappa^{2t}+2c_{\eta}^{I}c_{\xi}d^{-\frac{p_{*}}{2}\lor 1}c_{ \sigma}(\kappa^{2t})^{p_{*}-1}+c_{\eta}c_{\xi}d^{-\frac{p_{*}}{2}\lor 1}\nu^{2t}\] \[\qquad+O\Big{(}c_{\eta}^{2}c_{\xi}^{2}d^{-(p_{*}-1)\lor 1}( \kappa^{2t})^{2t}+c_{\eta}^{I+1}c_{\xi}d^{-\frac{p_{*}}{2}\lor 1}(\kappa^{2t})^{p_{* }-1}+c_{\eta}c_{\xi}d^{-\frac{p_{*}}{2}\lor 1}(\kappa^{2t})^{p_{*}}\Big{)}.\]

When \(c_{\xi}\leq c_{\eta}^{I}\) and \(c_{\eta}^{-1}d^{-\frac{1}{2}}\leq\kappa\leq c_{\eta}^{I}\), terms in the big-\(O\) notation is smaller than \(c_{\eta}^{I}c_{\xi}d^{-\frac{p_{*}}{2}\lor 1}c_{\sigma}(\kappa^{2t})^{p_{*}-1}\) and we have

\[\kappa^{2(t+1)}\geq\kappa^{2t}+c_{\eta}^{I}c_{\xi}c_{\sigma}d^{-\frac{p_{*}}{2 }\lor 1}(\kappa^{2t})^{p_{*}-1}+c_{\eta}c_{\xi}d^{-\frac{p_{*}}{2}\lor 1}\nu^{2t}.\]

It is straightforward to check \(\nu^{2t}\) has sub-Weibull tail. 

### Weak Recovery: Stochastic Update

This subsection proves weak recovery using the results on population update from the previous section. Specifically, from the previous section, we know that

\[\boldsymbol{\theta}^{\top}\boldsymbol{w}^{2(t+1)}\geq\boldsymbol{\theta}^{ \top}\boldsymbol{w}^{2t}+c_{\eta}^{I}c_{\xi}c_{\sigma}d^{-\frac{p_{*}}{2}\lor 1 }(\kappa^{2t})^{p_{*}-1}+c_{\eta}c_{\xi}d^{-\frac{p_{*}}{2}\lor 1}\nu^{2t},\]

with the mean-zero sub-Weibull random variable \(\nu^{2t}\) and a positive \(c_{\sigma}=\Theta(1)\). For notational simplicity we write \(c_{\eta}^{I}c_{\sigma}=c_{1}\). The following lemma is a detailed version of Proposition 9.

**Lemma 17**.: _Take \(\eta^{2t},\eta^{2t+1}=\eta=c_{\eta}d^{-1}\), \(\xi^{2(t+1)}=\xi=1-c_{\xi}d^{-\frac{(p_{*}-2)_{\pm}}{2}}\). Suppose that the link function satisfies \(\mathrm{IE}(\sigma_{*}^{I})=\mathrm{GE}(\sigma_{*})=p_{*}\) (we choose the smallest such \(I\)) and activation functions satisfy all of the assumptions in Section B.1 for weak recovery. Let_

\[T_{1,1}=C_{3}c_{\xi}^{-1}\begin{cases}d&(\text{if }p_{*}=\mathrm{GE}(\sigma_{*})=1) \\ d(\log d)&(\text{else if }p_{*}=\mathrm{GE}(\sigma_{*})=2)\\ d^{p_{*}-1}&(\text{else }p_{*}=\mathrm{GE}(\sigma_{*})\geq 3),\end{cases}\]

_and take \(c_{\xi}\lesssim\delta\mathrm{poly}(c_{\eta})\), \(c_{2}\gtrsim\mathrm{poly}(c_{\eta})\), and \(C_{3}\simeq c_{1}^{-1}\). If \(\kappa^{0}\geq 2c_{\eta}^{-1}d^{-\frac{1}{2}}\), there exists some \(\tau_{*}\leq T_{1,1}\) such that_

\[\kappa^{2\tau_{*}}\geq 2c_{2},\]

_with probability at least \(1-\delta\), and \(\kappa^{2\tau}\geq 2c_{2}\) for all \(\tau_{*}\leq\tau\leq T_{1,1}\), with high probability._We may take \(\delta=o_{d}(1)\) with arbitrarily slow decay. The proof is adapted from [1], but our bound on \(T_{1,1}\) is slightly sharper (by a \(\log d\) factor for \(p_{*}=2\) and by a \((\log d)^{2}\) factor for \(p_{*}\geq 3\)). For \(p_{*}=2\), this is because of a trick that we carefully "restart" the dynamics, whose failure probability exponentially decays.

**Proof.** We divide the proof into the following cases.

**(i) When \(p_{*}=1\).** Note that \(\{\sum_{s=0}^{\tau}\nu^{2s}\}_{\tau}\) is Martingale with \(\mathbb{E}[(\nu^{2s})^{2}]\lesssim 1\). By Doob's maximal inequality and Markov's inequality, with probability \(1-\delta\), we have

\[\max_{0\leq\tau\leq T}\bigg{|}\sum_{s=0}^{\tau}\nu^{2s}\bigg{|}^{2}\leq\delta^ {-1}\mathbb{E}[(\sum_{s=0}^{T}\nu^{2s})^{2}]\leq\delta^{-1}\sum_{s=0}^{T} \mathbb{E}[(\nu^{2s})^{2}]\leq C_{1}\delta^{-1}(T+1)\] (B.14)

for any fixed \(T\geq 0\), with a sufficiently large constant \(C_{1}=\Theta(1)\). In the following we consider the case when (B.14) holds for \(T=c_{1}^{-1}c_{\xi}^{-1}d-1\).

If \(c_{\eta}^{-1}d^{-\frac{1}{2}}\leq\kappa^{2t}\leq c_{\eta}^{I}\) for all \(t=0,1,\cdots,\tau\), we have

\[\kappa^{2(\tau+1)} \geq\kappa^{2\tau}+c_{1}c_{\xi}d^{-1}+c_{\eta}c_{\xi}d^{-1}\nu^{2\tau}\] \[\geq 2c_{\eta}^{-1}d^{-\frac{1}{2}}+c_{1}c_{\xi}(\tau+1)d^{-1} \gamma-c_{\eta}c_{\xi}d^{-1}\bigg{|}\sum_{s=0}^{\tau}\nu^{2s}\bigg{|}.\] (B.15)

Now, applying (B.14) to get

\[\kappa^{2(\tau+1)}\geq\text{(B.15)}\geq 2c_{\eta}^{-1}d^{-\frac{1}{2}}+c_{1}c _{\xi}(\tau+1)d^{-1}-c_{\eta}c_{\xi}^{\frac{1}{2}}c_{1}^{-\frac{1}{2}}C_{1}^{ \frac{1}{2}}\delta^{-\frac{1}{2}}d^{-\frac{1}{2}},\]

when \(\tau\leq c_{1}^{-1}c_{\xi}^{-1}d-1\). By letting \(c_{\xi}\leq c_{\eta}^{-4}c_{1}C_{1}^{-1}\delta\), we have \(c_{\eta}^{-1}d^{-\frac{1}{2}}\leq c_{\eta}c_{\xi}^{\frac{3}{2}}c_{1}^{-\frac{1 }{2}}C_{1}^{\frac{1}{2}}\delta^{-\frac{1}{2}}d^{-\frac{1}{2}}\), and

\[\kappa^{2(\tau+1)}\geq c_{\eta}^{-1}d^{-\frac{1}{2}}+c_{1}c_{\xi}(\tau+1)d^{-1},\]

which verifies \(c_{\eta}^{-1}d^{-\frac{1}{2}}\leq\kappa^{2t}\) for \(t=\tau+1\). Thus, there exists some \(\tau_{*}\leq c_{1}^{-1}c_{\xi}^{-1}d\) such that

\[\kappa^{2\tau_{*}}\geq 4c_{2},\]

for \(c_{1}\leq\frac{1}{4}c_{\eta}^{I}\), with probability \(1-\delta\).

Now we prove that \(\kappa^{2t}\geq 2c_{2}\) holds for all \(\tau_{*}\leq t\leq T_{1,1}=C_{3}c_{\xi}^{-1}d\). Because \(\nu^{2t}\) are mean-zero sub-Weibull random variables, we also have that \(|\sum_{s=\tau}^{\tau+\tau^{\prime}-1}\nu^{2s}|\leq C_{4}\sqrt{\tau^{\prime}}\) for all \(0\leq\tau,\tau^{\prime}\leq T_{1,1}\) with high probability. Also, because \(\eta^{t}\ll d^{-1}\) and \(|1-\xi^{t}|\ll 1\), we can easily see that \(|\kappa^{2(\tau+1)}-\kappa^{2\tau}|=\tilde{O}(d^{-1})\) for all \(\tau=0,1,\cdots,T_{1,1}-1\), with high probability. Thus, when there exists \(\tau\geq\tau_{*}\) such that \(\kappa^{2(\tau-1)}\geq 4c_{2}\) and \(\kappa^{2\tau}<4c_{2}\), we have \(\kappa^{2\tau}\geq 3c_{2}\) with high probability. Moreover, following the above argument, we can inductively show that

\[\kappa^{2(\tau+\tau^{\prime})} \geq 3c_{2}+c_{1}c_{\xi}\tau^{\prime}d^{-1}-c_{\eta}c_{\xi}d^{-1}C_{ 4}\sqrt{\tau^{\prime}}\] \[\geq 3c_{2}+c_{1}c_{\xi}\tau^{\prime}d^{-1}-\begin{cases}c_{2}&( \tau^{\prime}\leq c_{\eta}^{-2}c_{\xi}^{-2}C_{4}^{-2}c_{2}^{2}d^{2})\\ c_{1}c_{\xi}\tau^{\prime}d^{-1}&(\tau^{\prime}\leq c_{\eta}^{2}c_{1}^{-2}C_{4} ^{2})\end{cases}.\] \[\geq 2c_{2},\]

for \(\tau^{\prime}\leq T_{1,1}=C_{3}c_{\xi}^{-1}d\) or until \(\kappa^{2(\tau+\tau^{\prime})}\geq 4c_{2}\) holds again. By repeating this argument (if there are multiple such \(\tau\)), we see that \(\kappa^{2t}\geq 2c_{2}\) holds for all \(\tau_{*}\leq t\leq T_{1,1}=C_{3}c_{\xi}^{-1}d\) with high probability.

**(ii) When \(p_{*}=2\).** We define \(\iota_{0}=0,\iota_{1}=\log_{(1+c_{1}c_{\xi}d^{-1})}(4),t_{2}=2\log_{(1+c_{1}c_ {\xi}d^{-1})}(4),\dots\). We show that, for each \(i\), if \(\kappa^{2\iota_{i}}\geq 2c_{\eta}^{-1}d^{-\frac{1}{2}}\), we have \(\kappa^{2(\iota_{i+1})}\geq 2\kappa^{2\iota_{i}}\), with probability at least \(1-\delta 4^{-i}\), or there exists some \(t\) (\(\iota_{i}<t\leq\iota_{i+1}\)) with \(\kappa^{2t}>c_{\eta}^{I}\).

Assume that the above statement holds until some \(i-1\geq 0\) (we do not need to assume anything for \(i=0\)). Then, we have \(\kappa^{2\iota_{i}}\geq 2^{i}\kappa^{0}\geq 2c_{\eta}^{-1}d^{-\frac{1}{2}}\). Similarly to (B.15), if \(c_{\eta}^{-1}d^{-\frac{1}{2}}\leq\kappa^{2t}\leq c_{\eta}^{I}\) for all \(t=\iota_{i},\iota_{i}+1,\cdots,\tau\), we have

\[\kappa^{2(\tau+1)}\geq\kappa^{2\iota_{i}}+c_{1}c_{\xi}d^{-1}\sum_{s=\iota_{i}}^{ \tau}\kappa^{2s}-c_{\eta}c_{\xi}d^{-1}\bigg{|}\sum_{s=\iota_{i}}^{\tau}\nu^{2s} \bigg{|}.\]Applying (B.14) with \(\delta=\delta/4^{i}\) and \(T=\frac{1}{4}c_{\eta}^{-2}c_{\xi}^{-2}C^{-1}(\delta/4^{i})(\kappa^{2i_{\iota}})^ {2}d^{2}-1\) to get

\[\kappa^{2(\tau+1)} \geq\kappa^{2i_{\iota}}+c_{1}c_{\xi}d^{-1}\sum_{s=\iota_{i}}^{ \tau}\kappa^{2s}-c_{\eta}c_{\xi}d^{-1}C^{\frac{1}{2}}\delta^{-\frac{1}{2}} \sqrt{\tau+1-\iota_{i}}\] \[\geq\kappa^{2i_{\iota}}+c_{1}c_{\xi}d^{-1}\sum_{s=\iota_{i}}^{ \tau}\kappa^{2s}-\frac{1}{2}\kappa^{2\iota_{i}}\]

when \(\tau\leq\iota_{i}+\frac{1}{4}c_{\eta}^{-2}c_{\xi}^{-2}C^{-1}(\delta/4^{i})( \kappa^{2\iota_{i}})^{2}d^{2}-1\), which verifies \(c_{\eta}^{-1}d^{-\frac{1}{2}}\leq\frac{1}{2}\kappa^{2\iota_{i}}\leq\kappa^{2t}\) for \(t=\tau+1\).

This implies that, with probability \(1-\delta/4^{i}\), we have

\[\kappa^{2(\tau+1)}\geq\frac{1}{2}\kappa^{2\iota_{i}}+c_{1}c_{\xi}d^{-1}\sum_{ s=\iota_{i}}^{\tau}\kappa^{2s}\]

for all \(\tau=\frac{1}{4}c_{\eta}^{-2}c_{\xi}^{-2}C^{-1}(\delta/4^{i})(\kappa^{2\iota_{ i}})^{2}d^{2}-1\), which is equivalent to

\[\kappa^{2\tau}\geq(1+c_{1}c_{\xi}d^{-1})^{\tau-\iota_{i}}\frac{1}{2}\kappa^{2 \iota_{i}}\]

for all \(\tau=\iota_{i},\iota_{i}+1,\cdots,\iota_{i}+\frac{1}{4}c_{\eta}^{-2}c_{\xi}^{- 2}C^{-1}(\delta/4^{i})(\kappa^{2\iota_{i}})^{2}d^{2}\). By taking \(c_{\xi}\ll c_{1}c_{\eta}^{-2}C^{-1}(\delta/4^{i})(\kappa^{2\iota_{i}})^{2}d\), we have \(\frac{1}{4}c_{\eta}^{-2}c_{\xi}^{-2}C^{-1}(\delta/4^{i})(\kappa^{2\iota_{i}})^ {2}d^{2}\geq\log_{(1+c_{1}c_{\xi}d^{-1})}(4)\), and we get

\[\kappa^{2\iota_{i+1}}\geq 2\kappa^{2\iota_{i}}\]

with probability \(1-\delta/4^{i}\) (or there exists \(t\leq\iota_{i+1}\) such that \(\kappa^{2t}>c_{\eta}^{I}\)).

Thus, by induction, for all \(i\), we have that

\[\kappa^{2\iota_{i}}\geq 2^{i}\kappa^{0},\] (B.16)

or that there exists some \(t\leq\iota_{i}\) such that \(\kappa^{2t}\) is larger than \(c_{\eta}^{I}\), with probability \(1-\delta\).

The LHS of (B.16) becomes larger than \(c_{\eta}^{I}\) for some \(i\leq\log d\). Because \(\iota_{i}=\Theta(ic_{1}^{-1}c_{\xi}^{-1}d)\), within \(O(c_{1}^{-1}c_{\xi}d\log d)\) steps, there exists at least one \(\tau_{*}=O(c_{1}^{-1}c_{\xi}^{-1}d\log d)\) such that \(\kappa^{2\tau_{*}}\geq 4c_{2}\) for \(c_{2}\leq\frac{1}{4}c_{\eta}^{I}\), with probability \(1-\delta\).

Once such \(\tau_{*}\) is obtained, following the last paragraph of (i), we can see that \(\kappa^{2t}\geq 2c_{2}\) holds until \(t=T_{1,1}\) with high probability.

**(iii) When \(p_{*}\geq 3\).** We apply (B.14) with \(T=\frac{1}{p_{*}-2}c_{1}^{-1}c_{\xi}^{-1}d^{\frac{p_{*}}{2}}(\kappa^{0})^{-(p _{*}-2)}\) to obtain that

\[c_{\eta}c_{\xi}d^{-\frac{p_{*}}{2}}\bigg{|}\sum_{s=0}^{\tau}\nu^{2s}\bigg{|} \leq c_{\eta}c_{\xi}^{\frac{1}{2}}c_{1}^{-\frac{1}{2}}C^{\frac{1}{2}}\delta^{- \frac{1}{2}}d^{-\frac{p_{*}-2}{4}}(\kappa^{0})^{-\frac{p_{*}-2}{2}}\] (B.17)

for all \(\tau=0,1,\cdots,T-1\), with probability \(1-\delta\).

We take \(c_{\xi}\ll c_{\eta}^{-2}c_{1}C^{-1}\delta d^{\frac{p_{*}}{4}}(\kappa^{0})^{ \frac{p_{*}}{2}}\) so that (B.17) is bounded by \(c_{1}^{-1}d^{-\frac{1}{2}}\). Then,

\[\kappa^{2(\tau+1)} \geq\kappa^{0}+c_{1}c_{\xi}d^{-\frac{p_{*}}{2}}\sum_{s=0}^{\tau} (\kappa^{2s})^{p_{*}-1}+c_{\eta}c_{\xi}d^{-\frac{p_{*}}{2}}\sum_{s=0}^{\tau} \nu^{2s}\] \[\geq c_{\eta}^{-1}d^{-\frac{1}{2}}+c_{1}c_{\xi}d^{-\frac{p_{*}}{2 }}\sum_{s=0}^{\tau}(\kappa^{2\tau})^{p_{*}-1}.\]

It is easy to see that \(\kappa^{2(\tau+1)}\) is lower bounded by \(a^{\tau+1}\), where \(a^{0}=c_{\eta}^{-1}d^{-\frac{1}{2}}\) and \(a^{\tau+1}=a^{\tau}+c_{1}c_{\xi}d^{-\frac{p_{*}}{2}}(a^{\tau})^{p_{*}-1}\). By applying Lemma 18, we have

\[\kappa^{2\tau}\geq\frac{\kappa^{0}}{\big{(}1-c_{1}c_{\xi}d^{-\frac{p_{*}}{2}}(p _{*}-2)(\kappa^{0})^{(p_{*}-2)}t\big{)}^{\frac{1}{p_{*}-2}}}.\]Thus, until \(\tau\leq\big{(}c_{1}c_{\ell}d^{-\frac{\kappa}{2}}(p_{*}-2)(\kappa^{0})^{(p_{*}-2)} \big{)}^{-1}\leq T+1\ll d^{p_{*}-1}\), with probability at least \(1-\delta\), there exists some \(\tau_{*}\) such that

\[\kappa^{2\tau_{*}}\geq 4c_{2}\geq c_{\eta}^{I}\]

when \(c_{2}\leq\frac{1}{4}c_{\eta}^{I}\).

Once such \(\tau_{*}\) is obtained, following the last paragraph of (i), we can see that \(\kappa^{2t}\geq 2c_{2}\) holds until \(t=T_{1,1}\) with high probability. 

In the above proof we used the (discrete version of) Bihari-LaSalle inequality from [1].

**Lemma 18**.: _For \(p\geq 3\) and \(c>0\), consider a positive sequence \((a^{t})_{t\geq 0}\) such that_

\[a^{t+1}=a^{t}+c(a^{t})^{p-1}.\]

_Then, we have_

\[a^{t}\geq\frac{a^{0}}{\big{(}1-c(p-2)(a^{0})^{(p-2)}t\big{)}^{ \frac{1}{p-2}}}.\]

Proof.: From definition, we have

\[c=\frac{a^{t+1}-a^{t}}{(a^{t})^{p-1}}\leq\int_{t=a^{t}}^{a^{t+1} }\frac{1}{x^{p-1}}\leq\frac{1}{p-2}\bigg{[}\frac{1}{(a^{t})^{p-2}}-\frac{1}{(a ^{t+1})^{p-2}}\bigg{]}.\]

Taking the summation and re-arranging the terms yield

\[(a^{t})^{-(p-2)}\leq(a^{0})^{-(p-2)}-c(p-2)t,\] \[\therefore a^{t}\geq\frac{a^{0}}{\big{(}1-c(p-2)(a^{0})^{(p-2)}t \big{)}^{\frac{1}{p-2}}},\]

which gives the lower bound. 

### From Weak Recovery to Strong Recovery

In the previous subsection, we proved that after \(t=2T_{1,1}=\tilde{\Theta}(d)\) steps, with probability \(\tilde{\Omega}(1)\) over the randomness of initialization, we obtain nontrivial alignment \(\kappa_{j}^{2T_{1,1}}\geq 2c_{2}\). This subsection discusses how to convert the weak recovery into the strong recovery.

**Lemma 19**.: _Suppose the neuron satisfies \(\kappa^{2T_{1,1}}\geq 2c_{2}\). Take \(\eta^{2t}=\eta=\bar{c}_{\eta}\varepsilon d^{-1}\), \(\eta^{2t+1}=0\), \(\xi^{2(t+1)}=0\) for all \(t\geq T_{1,1}\), where \(\bar{c}_{\eta}\lesssim\mathrm{poly}(c_{1})\). If the activation functions satisfy all of the assumptions in Section B.1 for strong recovery, then we have_

\[\boldsymbol{\theta}^{\top}\boldsymbol{w}^{2(T_{1,1}+\tau_{*})} \geq 1-\varepsilon,\]

_with high probability, where \(\tau_{*}\leq T_{1,2}=C_{3}d\varepsilon^{-2}\). Moreover, \(\boldsymbol{\theta}^{\top}\boldsymbol{w}^{2(T_{1,1}+t)}\geq 1-\varepsilon\) for all \(\tau_{*}\leq t\leq T_{1,2}=C_{3}d\varepsilon^{-2}\), with high probability._

Proof.: Consider the Hermite expansions of \(\sigma_{*}\) and \(\sigma\). Let \(p\) be the smallest degree that both \(\sigma_{*}\) and \(\sigma\) have non-zero coefficients. First we compute the population gradient (of the correlation term) as

\[\mathbb{E}\big{[}\tilde{\nabla}_{\boldsymbol{w}}y\sigma( \boldsymbol{w}^{2t}{}^{\top}\boldsymbol{x})\big{]} =\mathbb{E}\bigg{[}\tilde{\nabla}_{\boldsymbol{w}}\bigg{(}\sum_{ j=p}^{\infty}\alpha_{j}\mathsf{H}\mathsf{e}_{j}(\boldsymbol{\theta}^{\top} \boldsymbol{x})\bigg{)}\bigg{(}\sum_{j=0}^{\infty}\beta_{j}\mathsf{H}\mathsf{e }_{j}(\boldsymbol{w}^{2t}{}^{\top}\boldsymbol{x})\bigg{)}\bigg{]}\] \[=\sum_{j=p}^{\infty}\big{[}j!\alpha_{j}\beta_{j}(\boldsymbol{ \theta}^{\top}\boldsymbol{w}^{2t})^{j-1}\boldsymbol{\theta}+(j+2)!\alpha_{j} \beta_{j+2}(\boldsymbol{\theta}^{\top}\boldsymbol{w}^{2t})^{j}\boldsymbol{w} ^{2t}\big{]}.\] (B.18)

Applying \(P_{\boldsymbol{w}^{2t}}\), we have

\[\mathbb{E}\big{[}P_{\boldsymbol{w}^{2t}}\tilde{\nabla}_{ \boldsymbol{w}}y\sigma(\boldsymbol{w}^{2t}{}^{\top}\boldsymbol{x})\big{]}=( \boldsymbol{\theta}-(\boldsymbol{w}^{2t}{}^{\top}\boldsymbol{\theta})\boldsymbol {w}^{2t})\sum_{j=p}^{\infty}j!\alpha_{j}\beta_{j}(\boldsymbol{\theta}^{\top} \boldsymbol{w}^{2t})^{j-1}.\] (B.19)Thus, the update of the alignment \(\kappa^{2t}=\bm{\theta}^{\top}\bm{w}^{2t}\) is

\[\kappa^{2(t+1)}\geq\kappa^{2t}+\eta\bm{\theta}^{\top}\bm{g}-\frac{1}{2}\eta^{2} \kappa^{2t}\|\bm{g}\|^{2}-\frac{1}{2}\eta^{3}\tilde{\eta}^{3}|\bm{\theta}^{\top }\bm{g}|\|\bm{g}\|^{2},\]

where

\[\bm{g}=P_{\bm{w}^{2t}}y\sigma^{\prime}({\bm{w}^{2t}}^{\top}\bm{x})\bm{x}.\]

From (B.18), the expectation of (B.19) is bounded by

\[\mathbb{E}[\kappa^{2(t+1)}] \geq\kappa^{2t}+\eta(1-(\kappa^{2t})^{2})\sum_{j=p}^{\infty}j! \alpha_{j}\beta_{j}(\bm{\theta}^{\top}\bm{w}^{2t})^{j-1}-\eta^{2}C_{4}d(\kappa^ {2t}+\eta)\] \[\geq\kappa^{2t}+\eta(1-(\kappa^{2t})^{2})\sum_{j=p}^{\infty}j! \alpha_{j}\beta_{j}(\kappa^{2t})^{p-1}-\eta^{2}C_{4}d(\kappa^{2t}+\eta).\]

By letting \(\eta\leq c_{1}^{p-1}\varepsilon d^{-1}\), when \(\kappa^{2t}\leq 1-\varepsilon\), we have

\[\mathbb{E}[\kappa^{2(t+1)}]\geq\kappa^{2t}+\frac{1}{2}\eta\varepsilon\sum_{j= p}^{\infty}j!\alpha_{j}\beta_{j}(\kappa^{2t})^{p-1}\geq\kappa^{2t}+\eta \varepsilon c_{1}^{p}.\]

It is easy to see that the noise \(\nu^{2t}\) has sub-Weibull tail and we obtain that

\[\kappa^{2(t+1)}\geq\kappa^{2t}+\frac{1}{2}\eta\varepsilon\sum_{j=p}^{\infty}j!\alpha_{j}\beta_{j}(\kappa^{2t})^{p-1}+\eta\nu^{2t}\geq\kappa^{2t}+\eta \varepsilon c_{1}^{p}+\eta\nu^{2t}.\] (B.20)

Suppose that \(2c_{2}\leq\kappa^{2(T1,1+\tau)}\leq 1-\varepsilon\) for all \(t=0,1,\ldots,\tau-1\). By taking the summation of (B.20), we have

\[\kappa^{2(T1,1+\tau)}\geq\kappa^{2T1,1}+\eta\varepsilon tc_{1}^{p}+\eta\sum_{ s=T1,1}^{T1,1+\tau-1}\nu^{2t}\geq 2c_{2}+\eta\varepsilon\tau c_{1}^{p}-C_{4} \eta\sqrt{\tau},\] (B.21)

with high probability. The third term is bounded by \(C_{4}\eta\sqrt{\tau}\leq c_{2}\) when \(\tau\leq c_{2}^{2}C_{4}^{-2}\eta^{-2}=c_{2}^{2}C_{4}^{-2}\bar{c}_{\eta}^{-2} \varepsilon^{-2}d^{2}\) and by \(\frac{1}{2}\eta\varepsilon\tau c_{1}^{p}\) when \(\tau\geq 4\varepsilon^{-2}c_{1}^{-2p}C_{4}^{2}\). Because \(c_{2}^{2}C_{4}^{-2}\bar{c}_{\eta}^{-2}\varepsilon^{-2}d^{2}\geq 4\varepsilon^{-2}c_{1}^ {-2p}C_{4}^{2}\), we can bound (B.21) by

\[\kappa^{2(T1,1+\tau)}\geq c_{2}+\frac{1}{2}\eta\varepsilon\tau c_{1}^{p},\] (B.22)

which verifies \(2c_{2}\leq\kappa^{2(T1,1+\tau)}\).

Therefore, by induction, until \(\kappa^{2t}\geq 1-\varepsilon\), we have the lower bound (B.22), whose RHS exceeds \(1-\varepsilon\) when \(\tau\geq 2\eta^{-1}\varepsilon^{-1}c_{1}^{-p}\leq C_{3}d\varepsilon^{-2}\). Thus, there exists \(\tau_{*}\leq T_{1,2}=C_{3}d\varepsilon^{-2}\) such that \(\kappa^{2(T1,1+\tau_{*})}\geq 1-\varepsilon\), with high probability.

Now, what remains is to prove that \(\kappa^{2(T1,1+\tau)}\geq 1-3\varepsilon\) holds for all \(\tau_{*}\leq t\leq T_{1,2}=C_{3}d\varepsilon^{-2}\). Because \(\nu^{2t}\) are mean-zero sub-Weibull random variables, we have that \(|\sum_{s=\tau}^{\tau+\tau^{\prime}-1}\nu^{2s}|\leq C_{4}\sqrt{\tau^{\prime}}\) for all \(0\leq\tau,\tau^{\prime}\leq T_{1,1}\) with high probability. Also, because \(\eta^{t}\ll\varepsilon d^{-1}\), we can easily see that \(|\kappa^{2(\tau+1)}-\kappa^{2\tau}|=\bar{O}(\varepsilon d^{-1})\) for all \(\tau=0,1,\cdots,T_{1,1}-1\), with high probability. Thus, when there exists \(\tau\geq\tau_{*}\) such that \(\kappa^{2(T_{1,1}+\tau-1)}\geq 1-\varepsilon\) and \(\kappa^{2(T_{1,1}+\tau)}<1-\varepsilon\), we have \(\kappa^{2(T_{1,1}+\tau)}\geq 1-2\varepsilon\) with high probability. Moreover, following the above argument, we can inductively show that

\[\kappa^{2(T1,1+\tau+\tau^{\prime})} \geq 1-2\varepsilon+\eta\varepsilon\tau^{\prime}c_{1}^{p}-C_{4} \eta\sqrt{\tau^{\prime}}\] \[\geq 1-2\varepsilon+\eta\varepsilon\tau^{\prime}c_{1}^{p}-\begin{cases} \varepsilon&(\tau^{\prime}\leq\bar{c}_{\eta}^{-2}C_{4}^{-2}d^{2})\\ \eta\varepsilon\tau^{\prime}c_{1}^{p}&(\tau^{\prime}\geq\varepsilon^{-2}C_{4}^ {2}c_{1}^{-2p})\end{cases}.\] \[\geq 1-3\varepsilon,\]

for \(\tau^{\prime}\leq T_{1,2}\) or until \(\kappa^{2(T1,1+\tau+\tau^{\prime})}\geq 1-\varepsilon\) holds again. Note that the last inequality follows from \(\bar{c}_{\eta}^{-2}C_{4}^{-2}d^{2}\geq\varepsilon^{-2}C_{4}^{2}c_{1}^{-2p}\). By repeating this argument (if there are multiple such \(\tau\)), we can see that \(\kappa^{2(T1,1+t)}\geq 1-\varepsilon\) holds for all \(\tau_{*}\leq t\leq T_{1,2}=C_{3}d\varepsilon^{-2}\) with high probability.

Adjusting hidden constants to remove a factor of \(3\) from \(3\varepsilon\) yields the desired result.

### Second Layer Training

From the previous analysis, we know that at least \(\Omega(1)\) portion of the neurons will satisfy the weak and strong recovery conditions (Appendix B.1), at least \(\tilde{\Omega}(1)\) portion of the neurons (independent from the choice of \(\sigma_{j}\)) satisfy initial alignment conditions (Appendix B.2), and at least \(1-o(1)\) fraction of them achieves strong recovery. This subsection proves a generalization error bound after second-layer training. Let \(f_{\bm{a}}(\bm{x})=f_{\bm{\Theta}}(\bm{x})\) for \(\bm{\Theta}=(\hat{\bm{w}}_{j},a_{j},\hat{b}_{j})_{j=1}^{N}\) where \(\bm{a}\in\mathbb{R}^{N}\) and \((\hat{\bm{w}}_{j},\hat{b}_{j})_{j=1}^{N}\) are the parameters trained in the first stage. Let \(\bm{a}^{*}\in\mathbb{R}^{N}\) be the "certificate" with \(\|\bm{a}^{*}\|^{2}=\tilde{O}(N)\) that is shown to exist in Lemma 22.

Polynomial Link Functions.The following lemma is a complete version of Proposition 5.

**Lemma 20**.: _There exists a \(4q\)-th order polynomial \(Q(R_{\bm{w}},b,q^{\prime})\) of \(R_{\bm{w}}=\max_{j}\|\bm{w}_{j}\|\), \(b=(b_{j})_{j=1}^{N}\) such that, if we set \(\lambda=\Theta\Big{(}\sqrt{\frac{2}{T_{2}\delta_{0}}N^{2}Q(R_{w},b,q^{\prime}) }\Big{)}\) for some \(\delta_{0}>0\), the ridge estimator \(\hat{\bm{a}}\) satisfies_

\[\|f_{\hat{\bm{a}}}-f_{*}\|_{L^{2}(P_{x})}^{2}\lesssim(N^{-2}+ \varepsilon^{2})+\frac{1}{T_{2}\lambda\delta_{0}}\big{(}2N^{2}Q(R_{w},b,q^{ \prime})+\mathbb{E}_{\bm{x}}[(f_{*})^{4}]\big{)}+\frac{3\lambda}{2}\|\bm{a}^{ *}\|^{2},\] (B.23)

_with probability \(1-\delta_{0}\). Hence taking \(T_{2}=\tilde{\Theta}((N^{4}Q^{2}(R_{w},b,q^{\prime})+\mathbb{E}[f_{*}(\bm{x}) ^{4}]^{2})\varepsilon^{-4})\) and \(N=\tilde{\Theta}(\varepsilon^{-1})\), we have_

\[\mathbb{E}_{\bm{x}}[(f_{\hat{\bm{a}}}(\bm{x})-f_{*}(\bm{x}))^{2}] \lesssim\varepsilon^{2}.\]

**Proof.** Let \(P_{T_{2}}\) be the empirical distribution of the second stage: \(P_{T_{2}}:=\frac{1}{T_{2}}\sum_{i=1}^{T_{2}}\delta_{\bm{x}_{i}}\). Let \(\psi(\bm{x})=(\sigma(\langle\bm{x},\hat{\bm{w}}_{j}\rangle)+b_{j}))_{j=1}^{N}\) so that \(f_{\bm{a}}(\bm{x})=\langle\bm{a},\psi(\bm{x})\rangle\).

Part (1).We first bound the term \(\|f_{\hat{\bm{a}}}-f_{*}\|_{L^{2}(P_{T_{2}})}\). Since \(\hat{\mathcal{L}}(f_{\hat{\bm{a}}})+\lambda\|\hat{\bm{a}}\|^{2}\leq\hat{ \mathcal{L}}(f_{\bm{a}^{*}})+\lambda\|\bm{a}^{*}\|^{2}\), we have

\[\|f_{\hat{\bm{a}}}-f_{*}\|_{L^{2}(P_{T_{2}})}^{2}+\lambda\|\hat{ \bm{a}}\|^{2}\] (B.24) \[\leq\|f_{\bm{a}^{*}}-f_{*}\|_{L^{2}(P_{T_{2}})}^{2}+\frac{2}{T_{2 }}\sum_{i=1}^{T_{2}}(f_{\bm{a}^{*}}(\bm{x}_{i})-f_{\hat{\bm{a}}}(\bm{x}_{i})) \varepsilon_{i}+\lambda\|\bm{a}^{*}\|^{2}.\]

Now, by the Cauchy-Schwarz inequality, we have

\[\frac{2}{T_{2}}\sum_{i=1}^{T_{2}}(f_{\bm{a}^{*}}(x_{i})-f_{\hat{ \bm{a}}}(x_{i}))\varepsilon_{i} =(\bm{a}^{*}-\hat{\bm{a}})^{\top}\frac{2}{T_{2}}\sum_{i=1}^{T_{2} }\psi(\bm{x}_{i})\varepsilon_{i}\] \[\leq 2\|\bm{a}^{*}-\hat{\bm{a}}\|\sqrt{\frac{\sum_{i,j}\varepsilon_ {i}\varepsilon_{j}\psi(\bm{x}_{i})^{\top}\psi(\bm{x}_{j})}{T_{2}^{2}}}.\]

By applying Markov's inequality to the right hand side, it can be further bounded by

\[\|\bm{a}^{*}-\hat{\bm{a}}\|\sqrt{\frac{\mathbb{E}_{\bm{x}}[\|\psi( \bm{x})\|^{2}]}{T_{2}\delta_{1}}}\leq\frac{\lambda}{2}\|\hat{\bm{a}}\|^{2}+ \frac{\lambda}{2}\|\bm{a}^{*}\|^{2}+\frac{\mathbb{E}_{\bm{x}}[\|\psi(\bm{x}) \|^{2}]}{T_{2}\delta_{1}\lambda},\]

with probability \(1-\delta_{1}\). Thus, by combining with (B.24), we arrive at

\[\|f_{\hat{\bm{a}}}-f_{*}\|_{L^{2}(P_{T_{2}})}^{2}+\frac{\lambda}{2}\|\hat{\bm{a }}\|^{2}\leq\|f_{\bm{a}^{*}}-f_{*}\|_{L^{2}(P_{T_{2}})}^{2}+\frac{\mathbb{E}_{ \bm{x}}[\|\psi(\bm{x})\|^{2}]}{T_{2}\delta_{1}\lambda}+\frac{3\lambda}{2}\|\bm{a }^{*}\|^{2}.\]

Here, by using the evaluation \(\|f_{\bm{a}^{*}}-f_{*}\|_{L^{2}(P_{T_{2}})}=\tilde{O}(N^{-1}+\varepsilon)\) in Lemma 22, the right hand side can be further bounded by

\[\|f_{\hat{\bm{a}}}-f_{*}\|_{L^{2}(P_{T_{2}})}^{2}+\frac{\lambda}{2}\|\hat{\bm{a}} \|^{2}\leq\tilde{O}(N^{-2}+\varepsilon^{2})+\frac{\mathbb{E}_{\bm{x}}[\|\psi(\bm{ x})\|^{2}]}{T_{2}\delta_{1}\lambda}+\frac{3\lambda}{2}\|\bm{a}^{*}\|^{2}.\]

[MISSING_PAGE_FAIL:32]

\[\|f_{\hat{\bm{a}}}-f_{*}\|_{L^{2}(P_{x})}^{2}\lesssim(N^{-2}+\varepsilon^{2})+ \frac{1}{T_{2}\lambda\delta_{0}}\big{(}2N^{2}Q(R_{w},b,q^{\prime})+\mathbb{E}_{ \bm{x}}[(f_{*}(\bm{x}))^{4}]\big{)}+\frac{3\lambda}{2}\|\bm{a}^{*}\|^{2},\]

with probability \(1-4\delta_{0}\). Thus, since \(\|\bm{a}^{*}\|^{2}=\tilde{O}(N)\), by setting \(T_{2}=\tilde{\Theta}((N^{4}Q^{2}(R_{w},b,q^{\prime})+\mathbb{E}[f_{*}(\bm{x}) ^{4}]^{2})\varepsilon^{-4})\), and \(N=\tilde{\Theta}(\varepsilon^{-1})\), we obtain that (B.23) \(\lesssim\varepsilon^{2}\). 

Higher Generative Exponent Functions.For general link functions, under Assumption 4 and the bounded fourth moment of the link function, we have the following counterpart of Lemma 20, which provides the formal statement of Proposition 10.

**Lemma 21**.: _Suppose that \(\mathbb{E}[\sigma_{*}(\bm{\theta}^{\top}x)^{4}]<\infty\) and Assumption 4 hold. Then, by setting \(\lambda=\tilde{\Theta}\Big{(}\sqrt{\frac{N^{2}}{T_{2}\delta_{0}}}\Big{)}\) for some \(\delta_{0}>0\), the ridge estimator \(\hat{\bm{a}}\) satisfies_

\[\|f_{\hat{\bm{a}}}-f_{*}\|_{L^{2}(P_{x})}^{2}\lesssim\varepsilon^{2}+\frac{1} {\sqrt{T_{2}\delta_{0}}}\big{(}N^{2}C_{4}+\mathbb{E}_{\bm{x}}[(f_{*})^{4}] \big{)}+\frac{1}{\sqrt{T_{2}\delta_{0}}}\|\bm{a}^{*}\|^{2},\]

_with probability \(1-\delta_{0}\). By taking \(T_{2}=\tilde{\Theta}((N^{4}+N^{2})\varepsilon^{-4})\), we have_

\[\mathbb{E}_{\bm{x}}[(f_{\hat{\bm{a}}}(\bm{x})-f_{*}(\bm{x}))^{2}]\lesssim \varepsilon^{2}.\]_Furthermore, applying Lemma 12 and 13 yields that, when \(\sigma_{*}=\sum_{j=0}^{\infty}\alpha_{j}\mathsf{He}_{j}\) satisfies \(\sum_{j=0}^{\infty}j^{2}j!\alpha_{j}^{2}\) and \(\mathbb{E}[\sigma_{*}(\boldsymbol{\theta}^{\top}x)^{4}]\) are bounded, with a properly designed randomized activation in Lemma 12, by taking \(N=\tilde{\Theta}(\varepsilon^{-7})\) and \(T_{2}=\tilde{\Theta}(\varepsilon^{-32})\), Algorithm 1 yields_

\[\mathbb{E}_{\boldsymbol{x}}[(f_{\boldsymbol{\hat{a}}}(\boldsymbol{x})-f_{*}( \boldsymbol{x}))^{2}]\lesssim\varepsilon^{2},\]

_with probability \(1-o_{d}(1)\)._

**Proof.** The proof is identical to that of Lemma 20, with the difference being that we replace the bounded moment assumptions with \(\mathbb{E}[\sigma_{*}(\boldsymbol{\theta}^{\top}\boldsymbol{x})^{4}]<\infty\) or Assumption 4. \(\Box\)

Approximation Guarantee.Note that for non-polynomial link function with generative exponent \(p_{*}\geq 3\), the approximation error is already controlled in Assumption 4 based on [1, Lemma 4.4, 4.5] (using activation function with a ReLU component). If \(\sigma_{*}\) is a degree-\(q\) polynomial, we have the following approximation result using polynomial activation, which follows Lemmas 29 and 30 of [13].

**Lemma 22**.: _Suppose that there exist at least \(N^{\prime}=\tilde{\Theta}(N)\) neurons that satisfy \(\|\boldsymbol{w}_{j}^{2T_{1}}-\boldsymbol{\theta}\|\leq\varepsilon\) and \(\sigma\) is a polynomial link function with degree at least \(q\). Let \(b_{j}\sim\mathrm{Unif}([-C_{b},C_{b}])\) with \(C_{b}=\tilde{O}(1)\), and consider approximation of a ridge function \(h(\boldsymbol{\theta}^{\top}\boldsymbol{x})\) with its degree at most \(q\). Then, there exists \(a_{1},\ldots,a_{N}\) such that_

\[\left|\frac{1}{N}\sum_{j=1}^{N}a_{j}\sigma_{j}\big{(}\boldsymbol{w}_{j}^{2T_{ 1}}{}^{\top}\boldsymbol{x}+b_{j}\big{)}-h(\boldsymbol{\theta}^{\top} \boldsymbol{x})\right|=\tilde{O}(N^{-1}+\varepsilon)\]

_with high probability, where \((\boldsymbol{x},y)\) is a random sample, and we omit dependence on the degree \(q\) in the big-\(O\) notation. Moreover, we have \(\sum_{j=1}^{N}a_{j}^{2}=\tilde{O}(N)\)._

Lemma 22 can be established from the following result in [13].

**Lemma 23**.: _Suppose that \(C_{b}\geq q\). For any polynomial \(h(s)\) with its degree at most \(q\), there exists \(\bar{v}(b;h)\) with \(|\bar{v}(b;h)|\lesssim C_{b}\) such that for all \(s\),_

\[\mathbb{E}[\bar{v}(b;h)\sigma(\delta s+b)]=h(s).\]

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist",**
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The paper's main contribution is studying the computational complexity of learning single-index models with polynomial link functions by training neural networks with stochastic gradient descent, which is what we claim in the abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]Justification: We discussed the limitations of our results in Section 4. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: All assumptions required in our work are stated in Assumptions 1,2, and 3. All proofs are presented in the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: This paper is theoretical and experiments are only used for the purpose of illustration in the Introduction.

Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: This paper is theoretical and experiments are only used for the purpose of illustration in the Introduction. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.

* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: This is a theory paper. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: This is a theory paper. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: This is a theory paper. Guidelines: * The answer NA means that the paper does not include experiments.

* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The authors have read and followed the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This paper carries out a theoretical study, and we believe our work does not have specific societal impacts that require a discussion. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA]Justification: This paper is theoretical and the experiment is only about two-layer neural network and synthetic data, which we do not have such risks. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: This paper is theoretical and does not use any assets. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper is theoretical and does not introduce new assets. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects**Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper is theoretical and does not include crowdsourcing experiments nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper is theoretical and does not include crowdsourcing experiments nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.