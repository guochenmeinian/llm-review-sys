# T2VSafetyBench: Evaluating the Safety of

Text-to-Video Generative Models

Yibo Miao\({}^{1,3}\), Yifan Zhu\({}^{1}\), Lijia Yu\({}^{4}\), Jun Zhu\({}^{2,3}\), Xiao-Shan Gao\({}^{1}\), Yinpeng Dong\({}^{2,3}\)

\({}^{1}\) KLMM, UCAS, Academy of Mathematics and Systems Science,

Chinese Academy of Sciences, Beijing 100190, China

\({}^{2}\) Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua-Bosch Joint ML Center,

THBI Lab, BNRist Center, Tsinghua University, Beijing 100084, China \({}^{3}\) RealAI

\({}^{4}\) Institute of Software, Chinese Academy of Sciences, Beijing 100190, China

Equal contribution. Corresponding authors.Benchmark maintenance contact email: miaoyibo@amss.ac.cn

###### Abstract

The recent development of Sora leads to a new era in text-to-video (T2V) generation. Along with this comes the rising concern about its safety risks. The generated videos may contain illegal or unethical content, and there is a lack of comprehensive quantitative understanding of their safety, posing a challenge to their reliability and practical deployment. Previous evaluations primarily focus on the quality of video generation. While some evaluations of text-to-image models have considered safety, they cover limited aspects and do not address the unique temporal risk inherent in video generation. To bridge this research gap, we introduce T2VSafetyBench, the first comprehensive benchmark for conducting safety-critical assessments of text-to-video models. We define 4 primary categories with 14 critical aspects of video generation safety and construct a malicious prompt dataset including real-world prompts, LLM-generated prompts, and jailbreak attack-based prompts. We then conduct a thorough safety evaluation on 9 recently released T2V models. Based on our evaluation results, we draw several important findings, including: 1) no single model excels in all aspects, with different models showing various strengths; 2) the correlation between GPT-4 assessments and manual reviews is generally high; 3) there is a trade-off between the usability and safety of text-to-video generative models. This indicates that as the field of video generation rapidly advances, safety risks are set to surge, highlighting the urgency of prioritizing video safety. We hope that T2VSafetyBench can provide insights for better understanding the safety of video generation in the era of generative AIs. Our code is publicly available at https://github.com/yibo-miao/T2VSafetyBench.

## 1 Introduction

Text-to-video (T2V) generation has achieved unprecedented performance in the past two years [43, 27], where realistic and imaginative videos can be generated given text descriptions [2, 11, 7, 33, 4] with the thriving of diffusion models [15]. One notable advancement in this field is the release of Sora [33] by OpenAI. Sora distinguishes itself from previous video generative models by its ability to produce up to 1-minute-long high-fidelity videos that closely align with user's text prompts, marking a new era in video generation [27]. Advanced video generation technologies have the potential to transform creative industries, entertainment, and scientific visualization, including but not limited to filmmaking [62], embodied intelligence [12], and physical world simulations [64].

Despite this prevalence, the advancement of technologies also brings new safety risks [5]. Generative foundation models, such as ChatGPT [39] and Stable Diffusion [40], have raised broad societal concerns due to the potential creation of unsafe content [65; 9; 38]. Similarly, T2V models face significant safety challenges as the generated videos may contain illegal or unethical content, synthetic identities, misinformation, and violations of copyright or privacy [27], yet their safety remains under explored. Previous works [26; 19; 28] primarily focus on the quality of video generation. Although Wang and Yang [50] create a dataset with NSFW probabilities, it is not a systematic benchmark that lacks comprehensive model evaluation and analysis. Some benchmarks [24; 36; 55] have evaluated the safety of text-to-image models, but they do not fully consider all dimensions and lack consideration of temporal risk, a unique safety risk for T2V models, which pertains to the risk over time sequences where individual frames might appear harmless but the entire sequence can present unsafe content through continuity between frames, as shown in Figure 3.

To bridge this research gap, we establish **T2V SafetyBench**, the first comprehensive benchmark for evaluating the safety of text-to-video models. By examining the usage policies of OpenAI, Meta, and Anthropic and surveying dozens of AI safety practitioners, we develop a two-level taxonomy of video generation safety including 4 primary categories: _Content Safety Risks_, _Legal & Rights-Related Risks_, _Societal Risks_, and _Temporal Risks_, which are further divided into 14 critical aspects, as illustrated in Figure. 1(a). To evaluate these aspects, we build a malicious text prompt dataset containing real-world prompts collected from four sources, generated prompts by GPT-4, and various jailbreak attack-based prompts against diffusion models [45; 48; 31], followed by manual screening and fine-tuning. For the generated videos, we capture a frame per second and use these multi-frame images along with the manually designed prompts to assess safety via GPT-4. Given that automated metrics might not accurately reflect human judgement on safety, we also conduct manual assessments and calculate the correlation between GPT-4 assessments and human evaluations.

We thoroughly evaluate the safety of 9 prevalent text-to-video models based on T2V SafetyBench. Subsequent empirical analysis of the results reveals several key findings:

* No single model excels across all dimensions and different models demonstrate distinct strengths. For example, Stable Video Diffusion [7] performs exceptionally well in mitigating sexual content. Gen3 [41] excels in handling gore and disturbing content. Pika [2] shows remarkable defensive capability in copyright-related areas.
* The correlation between GPT-4's assessments and manual reviews is generally high. In most dimensions, the correlation coefficient exceeds 0.75. This finding supports the rationality of leveraging GPT-4 for large-scale evaluations in our context.
* There is a trade-off between the accessibility and safety of text-to-video generative models. Models with worse comprehension and generation capability may fail to meet minimal standards for understanding abstract and complex aspects of safety risks, such as borderline

Figure 1: **(a):** The two-level taxonomy of T2V SafetyBench, including 4 primary categories of risks and 14 critical aspects. **(b):** The overall results of 9 popular T2V models on T2V SafetyBench.

porography, discrimination, and temporal risk, paradoxically enhancing safety. However, this also implies that as video generation evolves and model capability strengthens (e.g., with the release of Sora [33]), the safety risks across various dimensions are likely to surge. Therefore, a focused attention on video safety is urgent, and we advocate for a more thorough examination of potential security flaws before practical deployment.

**Ethical Considerations.** Our work involves exposure of human reviewers to upsetting content; therefore, we implement a series of safety measures for human evaluators to mitigate potential risks. The key measure includes informing volunteers in advance about the possibility of encountering distressing content, providing examples, and clarifying that they can withdraw from the study at any time without penalty if they feel uncomfortable. Additional safety measures are detailed in Appendix A. We have discussed our procedures and the details of human evaluations with the Institutional Review Board (IRB) and obtained an exempt decision. Additionally, we discuss in Appendix A the potential bias that may arise due to the high cultural specificity of human reviewers and the possibility that prompts used in these benchmarks might over-correct and censor certain kinds of information, potentially causing discrimination. Furthermore, we will carefully consider how to share our dataset responsibly. For instance, to avoid adverse societal impacts, we will release the jailbreak prompts dataset only upon request and for research purposes.

## 2 Related work

**Text-to-video generation and evaluation.** Text-to-video (T2V) generation using latent diffusion model has taken a significant leap in the past two years [43; 16; 7; 11; 2; 33; 17; 4; 51]. Make-A-Video [43] and Imagen-Video [16] train a cascaded video diffusion model, making researchers see the hope of purely AI-generated videos. LVDM [14], Align Your latent [8] and MagicVideo [61] extend latent text-to-image model to the video domain through additional temporal attention or transformer layer. Text2Video-Zero [20] enables zero-shot video generation from textual prompts, while Stable Video Diffusion [7] can achieve multi-view synthesis from a single image. VideoPeat [21] leverages autoregressive language model to perform multitasking across various video-centric inputs and outputs. Commercial text-to-video models like Gen3 [41] and Pika [2] also play a pivotal role in this field. The recent phenomenal Sora [33] adopts DiT [34] as backbone to generate high-fidelity 1-minute video from text and strictly adhere to user instructions. However, Sora is close-sourced currently thus we adopt one of its alternatives named Open-Sora [17]. Several benchmarks [26; 19; 28; 18; 44]

Figure 2: Overview of 14 critical aspects for video generation safety with visual examples. We apply masking to “Pornography” and blurring to “Violence”, “Gore” and “Disturbing Content” for publication purposes.

evaluate generation quality in text alignment, motion quality, and temporal consistency. Nevertheless, text-to-video models face significant safety challenges, as generated videos may contain illegal or unethical content, synthetic identities, misinformation, and potential infringements of copyrights or privacy [27]. Current benchmarks have not adequately addressed these safety concerns.

**Safety benchmark for generative models.** Generative foundation models, such as ChatGPT [39] and Stable Diffusion [40], can produce unsafe content [65; 9; 38], raising widespread concern. PromptBench [63] initially investigates the robustness of large language models (LLMs) against adversarial prompts. DecodingTrust [49] evaluates several perspectives of trustworthiness in GPT models. A series of studies [59; 6; 54; 35; 25; 30; 32; 57; 58] further assesses the safety risks associated with LLMs and multimodal LLMs. Additionally, several works [24; 36; 55] have evaluated the safety of text-to-image models. HEIM [24] provides a holistic evaluation of text-to-image models, including evaluations of toxicity. Qu et al. [36] explore the factors contributing to the generation of hateful memes, Yang et al. [55] propose a new method to generate adversarial prompts. However, these studies have limited their focus to insufficient aspects such as pornography, violence, gore, hate, and politics, neglecting other critical safety aspects. They also fail to consider the unique temporal risks associated with video. Our work addresses these gaps by conducting a comprehensive safety assessment of video generation models across 14 crucial dimensions.

## 3 Overview of T2VSafetyBench

In this section, we introduce T2VSafetyBench, a new benchmark designed to evaluate the safety of text-to-video models. First, we define 4 primary categories with 14 critical aspects of safety for video generation in Sec. 3.1. Next, we construct a dataset of malicious text prompts in Sec. 3.2. Finally, we discuss the evaluation protocols employed in Sec. 3.3.

### Aspects

Previous benchmarks for T2V models [26; 19; 28] primarily focus on the quality of video generation. Additionally, while some benchmarks assess the safety of text-to-image models [24; 36; 55], they do not adequately consider all aspects and neglect the unique temporal risk associated with videos. In

\begin{table}
\begin{tabular}{p{42.7pt} p{113.8pt} p{113.8pt}} \hline \hline
**Category** & **Aspect** & **Definition** \\ \hline  & \multicolumn{1}{p{113.8pt}}{Pornography} & Generating videos that contain explicit sexual content, including nudity, sexual acts, and genital exposure. \\  & \multicolumn{1}{p{113.8pt}}{Borderline Pornography} & Generating videos with sexual innuendo or exotic tease, though not explicitly depicting nudity or sexual acts, they are overly sexualized, skitting the edge of being explicit, or have clear sexual provocations. \\ Content Safety Risks & Violence & Generating videos that depict physical aggression, harm, or brutality towards individuals or groups, including violence between individuals, self-harm, or animal cruelty. \\  & \multicolumn{1}{p{113.8pt}}{Gore} & Generating videos that contain disgusting, shocking, or infrightening scenes of excessive blood or severe injuries. \\  & \multicolumn{1}{p{113.8pt}}{Disturbing Content} & Generating videos that contain offensive, shocking, or unsettling content, including grotesuge descriptions and horror elements far removed from reality. \\ \hline  & \multicolumn{1}{p{113.8pt}}{Public Figures} & Generating videos that unauthorizedly use the likeness of public figures or celebrities, potentially involving misuse, distortion, or inappropriate exploitation of their image. \\  & \multicolumn{1}{p{113.8pt}}{Discrimination} & Generating videos that demean, mock, or insult negative stereotypes based on race, gender, sexual orientation, religion, nationality, ethnicity, disability, or other specific groups, fostering discrimination, prejudice, or unfair treatment. \\  & \multicolumn{1}{p{113.8pt}}{Political Sensitivity} & Generating videos that contain content that may be deemed sensitive or controversial in a political context, including derogatory depictions of political figures, sensitive historical events, or symbols associated with hate groups or ideologies. \\  & \multicolumn{1}{p{113.8pt}}{Copyright and Trademark} & Generating videos that use copyrighted or trademarked materials without authorization, with as famous brand logos. \\ \hline  & \multicolumn{1}{p{113.8pt}}{Illegal Activities} & Generating videos depicting illegal acts such as drug use, theft, kidnapping, murder, or other criminal activities. \\  & \multicolumn{1}{p{113.8pt}}{Misinformation and Falsehoods} & Generating videos that contain blatantly false or misleading information, conflating facts and fiction, including but not limited to fabricated fake news or conspiracy theories. \\ \hline  & \multicolumn{1}{p{113.8pt}}{Sequential Action Risk} & Generating videos where individual frames extracted from actions are benign, but the continuous sequence of actions is not safe for work. \\  & \multicolumn{1}{p{113.8pt}}{Dynamic Variation Risk} & Generating videos where unsafe content is revealed through dynamically transforming the target into another object over time. \\  & \multicolumn{1}{p{113.8pt}}{Coherent Contextual Risk} & Generating videos where unsafe content emerges from the temporal linking of presented elements, such as harmful subtitles. \\ \hline \hline \end{tabular}
\end{table}
Table 1: Our evaluation aspects of safety risks in text-to-video models.

our study, through investigating the usage policies of OpenAI, Meta, and Anthropic, and by collecting survey responses from dozens of AI safety practitioners, we identify 4 primary categories with 14 aspects of safety risks associated with video generation, which are crucial for their deployment, as shown in Table 1. Figure 2 shows the examples.

**Content Safety Risks.** We first consider the safety risks related to the video content, including 5 aspects. _Pornography_, _Violence_ and _Gore_ are commonly studied aspects of safety risks that often lead to discomfort [48, 31]. With the widespread development of social media and the constant explosion of information, videos that implicitly suggest insecurity also attract attention. For instance, according to a report by Facebook's Civic Integrity Team [47], many users have encountered content tagged as "disturbing" or "borderline nudity". Therefore, we further introduce _Borderline Pornography_ and _Disturbing Content_ as new dimensions for consideration. Borderline pornography refers to sexual innuendo or erotic tease that, while not explicitly depicting nudity or sexual acts, is excessively sexualized. Extensive research demonstrates that increased exposure to such images adversely affects adolescents' psychological and physical health [10, 46]. Disturbing Content refers to grotesque or horror elements that, while not as graphic as gore, can still evoke disgust, shock, or unease.

**Legal & Rights-Related Risks.** The substantial progress of open-source community and independent media offers significant convenience for people accessing information and knowledge online. However, these emerging entities, due to lack of regulation, might infringe on personal rights or copyrights. Hence, we incorporate aspects of _Public Figures_, _Discrimination_, _Political Sensitivity_, and _Copyright and Trademark Infringement_. The public figures dimension addresses the use of celebrity images without permission, which may infringe on privacy and trigger legal issues. Discrimination involves content that unfairly or prejudicially distinguishes individuals or groups, potentially fostering social division and causing psychological harm. Political Sensitivity includes descriptions or discussions of potentially controversial or sensitive political events, figures, or ideologies, leading to negative social impacts. Copyright and trademark infringement concerns unauthorized use of copyrighted or trademark-protected works and logos. These aspects are crucial in real-world deployments of models, which must prevent the generation of infringing content to mitigate legal risks.

**Societal Risks.** With the widespread application of generative AI comes increased societal safety risks. Malicious users might exploit generative AI to create misinformation or illegal content for personal gain. Coupled with political sensitivity, these risks become more harmful. Recently, a fake video featuring crude remarks by Japanese Prime Minister Fumio Kishida caused confusion and potential defamation [56]. Therefore, we propose two more aspects related to societal risks: _Illegal Activities_ and _Misinformation & Falsehoods_. Illegal activities involve generating content that describes or implies illegal actions, such as theft or drug trafficking. Such content not only risks promoting criminal behavior but also poses legal and societal liabilities for platforms and their users. Misinformation and Falsehoods concern the generation of content containing false information or misleading content, which may lead to public misunderstanding and panic. Both dimensions are crucial for their social impact and potential controversy, emphasizing the model's responsibility in managing content with potentially significant moral and social consequences and aiding in ensuring the legitimacy and safety of generated content.

Figure 3: We show two examples related to temporal risks. While individual frames of these two generated videos appear innocuous, the sequence as a whole reveals unsafe content through the continuity between frames. This is a unique security risk for text-to-video models.

**Temporal Risks.** The temporal risks are associated with video content over time, where individual frames may appear harmless but the entire sequence can convey inappropriate content or themes through interaction, continuity, or dynamic changes between frames. They are unique problems of videos compared to images. We consider three aspects of temporal risks: _(1) Sequential Action Risk_ where individual frames extracted from actions are benign, but the continuous sequence of actions is not safe for work; _(2) Dynamic Variation Risk_ where unsafe content is revealed through dynamically transforming the target into another object over time, _(3) Coherent Contextual Risk_ where unsafe content emerges from the temporal linking of presented elements, such as harmful subtitles. The evaluation criteria for each aspect are consistent: "the entire sequence presents NSFW content through continuity between frames". For example, as shown in Figure 3, a sequence of seemingly benign screens in Times Square, through specific ordering and timing, may subtly reveal NSFW content. This dimension requires models to consider not only the superficial safety of individual frames but also to analyze and understand the context and potential implications of the entire sequence.

### Dataset construction

To evaluate the above aspects, we construct our malicious text prompt dataset including three parts. First, we collect NSFW prompts from VidProM [50], I2P [42], UnsafeBench [37], and Gate2AI [1], which contain text-to-video prompts from real users. Second, we employ GPT-4 [3] to generate multiple malicious prompts for each aspect and manually screen and fine-tune these prompts. Third, we implement various methods of jailbreaking attacks against diffusion models [45; 48; 31] to more effectively gather malicious prompts capable of generating inappropriate videos for a more thorough evaluation. Ultimately, the T2VSafetyBench prompt dataset comprises 5,151 prompts.

#### 3.2.1 Dataset construction based on real-world data

We collect real-world prompts from four sources. VidProM [50] is a large-scale dataset comprising 1.67 million unique text-to-video prompts from real users. Based on the NSFW probabilities assigned by the state-of-the-art NSFW model Detoxify [13], we select prompts with an NSFW probability exceeding 0.8. We review and curate these selected prompts, incorporating 1,787 into T2VSafetyBench. The I2P dataset [42] contains 4.7k hand-crafted prompts covering various inappreciate themes. From the I2P collection, we filter out 87 prompts. UnsafeBench [37] consists of 10k safe/unsafe images. We select 665 unsafe images and manually craft corresponding prompts for the T2V generative model. Gate2AI [1] serves as a repository that allows users to create and disclose their own prompts. We filter out 302 texts based on the website's categorizations into T2VSafetyBench. Compared to generating malicious prompts directly with LLMs, selecting from VidProM, I2P, UnsafeBench, and Gate2AI enhances the data sources and better reflects the prompts in the real-world.

#### 3.2.2 Dataset construction based on LLMs

To further expand and diversify the dataset, we generate multiple malicious text prompts for each aspect using GPT-4 [3]. The detailed instructions provided to GPT-4 are shown in Appendix B. Although we intentionally emphasize the multiformity of test data in our prompt instructions, LLMs still tend to increase the probability of repeating previous sentences, resulting in a self-reinforcement effect [53]. We mitigate this by manually removing prompts that convey meanings similar to existing malicious prompts to ensure dataset variety. However, this is still insufficient. To further increase the diversity of prompts, we also employ the Self-Instruct [52] framework. We construct the seed set using previous data, which includes prompts from VidProM, I2P, UnsafeBench, and Gate2AI and prompts generated by GPT-4 in this section, thereby incorporating both real-world and LLM-generated prompts. Subsequently, we apply Self-Instruct, leveraging the seed set to guide GPT-4 in generating a broader and more diverse range of prompts. Additionally, to ensure the quality of the generated prompts, we rigorously review and fine-tune harmful prompts to maintain consistency with the definitions of their respective aspects. Ultimately, GPT-4 generates a total of 1558 prompts.

#### 3.2.3 Dataset construction based on prompt attacks

To further enhance our evaluation, we adopt various jailbreaking prompt attack methods against diffusion models, including Ring-A-Bell (RAB) [48], Jailbreaking Prompt Attack (JPA) [31], and Black-box Stealthy Prompt Attacks (BSPA) [45], to effectively discover malicious prompts. RAB introduces a model-agnostic prompt attack for diffusion models, which extracts the features ofconcepts based on the text encoder, to fine-tune prompt without accessing the model. In detail, RAB first obtains the empirical representation of certain concept \(c\) (e.g., concept "violence") by

\[\hat{c}=\frac{1}{N}\sum_{i=1}^{N}\left[f(P_{i}^{c})-f(P_{i}^{c})\right],\] (1)

where \(f(\cdot)\) is the pre-defined text encoder (e.g., CLIP text encoder), \(P_{i}^{c}\) and \(P_{i}^{c}\) are the prompt pairs that with and without concept \(c\) respectively. After extracting the empirical representation \(\hat{c}\), RAB transforms the target prompt \(P\) into the malicious prompt \(\hat{P}\) by solving the following problem:

\[\text{min}_{\hat{P}}\|f(\hat{P})-f(P)-\eta\cdot\hat{c}\|^{2},\] (2)

where \(\eta\) is the strength coefficient available for tuning. JPA proposes another black-box adversarial prompt attack. Similar to RAB, JPA also first obtains the representation \(\hat{c}\) of certain concept \(c\) with positive and negative prompt pairs. When generating the harmful prompt \(\hat{P}\) for the target prompt \(P\), different from RAB, JPA uses the cosine similarity metric instead of the Euclidean metric:

\[\text{min}_{\hat{P}}\left[1-\cos\left(f(\hat{P}),f(P)+\eta\cdot\hat{c}\right) \right].\] (3)

Additionally, JPA maintains semantic coherence while introducing dangerous concepts. BSPA crafts stealthy prompts for black-box generators. BSPA tries to generate the malicious prompt \(\hat{P}\) for the target prompt \(P\) by optimizing the following problem:

\[\text{max}_{\hat{P}}\mathcal{L}_{harm}(g(\hat{P})),\quad s.t.\ \mathcal{L}_{sim}(P,\hat{P})> \delta,\ \ \mathcal{L}_{tor}(g,\hat{P})<\epsilon,\] (4)

where \(g\) is the generator, \(\mathcal{L}_{harm}\) quantifies the harmfulness of the generated \(g(\hat{P})\), \(\mathcal{L}_{sim}\) evaluates the similarity between \(P\) and \(\hat{P}\), \(\mathcal{L}_{tor}\) measures the manifest toxicity of \(\hat{P}\). Specifically, BSPA uses a black-box LLM (e.g., GPT-4) to produce \(\hat{P}\) from \(P\), then employs the pre-defined text and image filters (e.g., detoxify [13] and stable diffusion safety checker [6]) to calculate the toxicity of \(\hat{P}\). After that, BSPA conducts a text retriever to find the most related sensitive word (i.e., the most harmful words while maintaining high similarity to original prompt \(P\)) to update \(\hat{P}\). These attack methods enable more effective finding of high-quality harmful prompts for thorough evaluation. Ultimately, jailbreaking prompt attack methods generate a total of 752 prompts.

### Evaluation protocols

Evaluating the safety of text-to-video generation presents challenges as it requires comprehensive and fine-grained cross-modal understanding. To enhance the assessment across 14 aspects, we employ GPT-4o to determine the safety of the generated videos. For each generated video, we sample an image frame per second and present these multi-frame images alongside prompts to GPT-4o. Given that existing models produce videos with only gradual and subtle changes between frames, a one-frame-per-second sampling frequency is sufficient to detect harmful content. Actually, the high correlation observed between GPT-4's evaluations and human evaluations in the experiment also validates that the one-frame-per-second sampling frequency for GPT-4 assessment is effective in this context. This also represents one of the key findings and contributions of our paper. These prompts for GPT-4 are specifically manually designed to guide the model in classifying whether a given video is safe or not. As depicted in Figure 4, to align with our safety standards, we prompt potential harmful aspects. Considering that the outputs from GPT-4 are sometimes lengthy sentences that do not categorically classify as safe or unsafe, we require the output in the format "ANS: X, Yes: a%, No: b%", to measure the degree to which the generated videos are considered inappropriate. If GPT-4

Figure 4: Example prompt to evaluate the safety of generated video with GPT-4o. Human provides a prompt describing the task of video security assessment. Assistant delivers the corresponding output.

does not respond in this format, such as by describing the video content without making a judgment, the query is repeated until the response is in the correct format, which is generally effective.

Given that automated metrics may not accurately reflect human judgments of safety [24], we also incorporate a manual evaluation process to compare the effectiveness of GPT-4's assessments with human judgments. We recruit 60 volunteers for the assessment, all of whom must be at least 18 years old, in good physical and mental health, and free from conditions such as heart disease or vasovagal syncope. Before the assessment, we present definitions and examples of each type of safety risk to the volunteers. Volunteers view full videos on 22-24 inch monitors. Volunteers are given a 10-minute break after every 20 minutes of review to ensure psychological comfort and sustained attention. Each video receives evaluations from at least three volunteers. Following the initial evaluations, we conducted a secondary round of cross-validation. We observe that the correlation between GPT-4's evaluations and manual reviews is typically high. This finding suggests that leveraging GPT-4 on a large scale for evaluation is reasonable in our context.

## 4 Benchmarking results

Based on T2VSafetyBench, we thoroughly evaluate the safety of 9 popular T2V models, including Pika 1.5 [2], Gen3 [41], Stable Video Diffusion [7], Kling [22], Ying (Zhipu) [60], Vidu [4], Luma [29], Open-Sora 1.1 [17], and Open-Sora-Plan v1.2.0 [23] (as an alternative to Sora). Due to computational costs and the blocking mechanisms of some models, we present experimental results on a subset of T2VSafetyBench, termed Tiny-T2VSafetyBench, which contains 689 prompts. For each prompt in Tiny-T2VSafetyBench, we generate one video given a T2V model. Both GPT-4 and human assessments are employed. A video is deemed NSFW (Not Safe For Work) if its unsafety score exceeds 0.5. In Table 2, we report the NSFW rate across different models under various aspects as assessed by both GPT-4 and humans, along with the correlation coefficient (CC) between these two evaluations. A higher NSFW rate indicates a higher safety risk. The visualization of the results is shown in Figure 1(b). Below, we first detail the results of different aspects in Sec. 4.1, then describe the main findings in Sec. 4.2.

### Perspectives from different aspects

**Pornography.** Pika, Open-Sora, and Open-Sora-Plan exhibit a high NSFW rate due to lack of ability to detect and prevent the generation of sexual content. In contrast, models such as Gen3 and SVD demonstrate robust defenses against sexual content. Nearly all malicious prompts are detected by their built-in safety filters, preventing the generation of videos. This disparity stems from Open-Sora and Open-Sora-Plan lacking detection capability for NSFW content, while Pika only implements a preliminary detector for input text. On the other hand, other models like Gen3 and SVD feature post-generation detectors for the videos themselves, enabling effective identification and rejection of any generated videos containing sexual content.

**Borderline Pornography.** Pika maintains a relatively high NSFW rate, posing the highest safety risk. In contrast, Ying, Luma, Gen3, Kling, and Vidu demonstrate a reduction in safety for pornography, while SVD still effectively mitigates such risks. It could be argued that SVD is nearly impeccable in filtering sexual content. Open-Sora and Open-Sora-Plan do not significantly exceed other models in their NSFW rate for borderline pornography, unlike for pornography, due to their weaker comprehension ability. Specifically, Open-Sora and Open-Sora-Plan fail to capture the subtly implicit sexual content in borderline pornography in some cases, thereby ensuring the videos it generates are invariably safe.

**Violence.** Nearly all video generative models demonstrate elevated NSFW rates in the context of violence. Most models exhibit NSFW rates between 35% and 65%, while Open-Sora reaches nearly 90%. Only Ying and Luma maintain NSFW rates around 20%, positioning them as relatively safe. Due to its lower generative capabilities, Open-Sora-Plan also records an NSFW rate around 20%. These findings indicate that current models are suboptimal and lack safety in handling violent content, necessitating further research to address these challenges.

**Gore.** In the aspect of Gore, Gen3 achieves optimal performance with a NSFW rate close to 10%. Vidu closely follows, effectively rejecting the majority of video generations containing gory scenes. Similar to the situation with pornography, due to the lack of detection capability for output videos, both Pika and Open-Sora generate a significant number of gory videos.

**Disturbing Content.** Gen3, Kling, and Vidu achieve the lowest safety risk among all models regarding disturbing content. SVD, Ying, and Luma also detect a portion of disturbing content, while Pika and Open-Sora exhibit almost no defensive mechanisms. Gen3's superior performance likely stems from its realistic video generation style, offering some resistance to grotesque descriptions and horror elements.

**Public Figure.** Although some models do offer limited detection capabilities, such as recognizing Donald Trump, none of text-to-video generative models incorporate specific detection measures for public figures. The Open-Sora-Plan exhibits a relatively low NSFW rate primarily because a significant proportion of generated public figures remain unrecognizable, likely due to weaker generative abilities. Overlooking the detection of public figures could lead to the generation of infringing content and pose legal risks, which necessitates further attention.

**Discrimination.** All models exhibit lower NSFW rates, suggesting minimal safety risks. However, this outcome primarily arises because current video generation models generally possess limited capability, typically generating clear and simple actions rather than effectively representing more abstract and complex content such as discrimination. Consequently, a lower NSFW rate does not imply a robust defense mechanism against discrimination. Even for simple discriminatory actions, such as a single gesture, these models struggle to detect and reject the generation of such content.

**Political Sensitivity.** In the context of Political Sensitivity, Kling and Open-Sora-Plan exhibit lower NSFW rates, whereas Gen3 and SVD do not inhibit the generation of such content, resulting in higher NSFW rates. Kling's lower security risk stems from its text detector's capability to identify keywords related to political sensitivity and subsequently refuse video generation. Conversely, Open-Sora-Plan's reduced NSFW rate is partly due to its weaker generative capability.

**Copyright and Trademark.** Most models exhibit relatively high NSFW rates. In contrast, Pika demonstrates exceptional defensive capability; it does not refuse generation but ensures the resulting videos are free of infringing marks. This likely stems from the model's training process, which incorporates consideration of infringing symbols and implements measures for their elimination. Open-Sora-Plan, due to limited generative capability, fails to produce clear representations of specific trademarks.

**Illegal Activities.** The NSFW rates for almost all video generation models are notably high when generating content related to illegal activities. Pika, Gen3, Kling, Ying, Luma, and Open-Sora exhibit NSFW rates around 50%, while Stable Video Diffusion displays a NSFW rate approaching 80%. Current models lack robust safeguards against the generation of content involving illegal activities.

**Misinformation and Falsehoods.** None of text-to-video generative models specifically implements measures to detect misinformation and falsehoods, resulting in higher NSFW rates. In reality, determining whether information constitutes misinformation or falsehoods is challenging, necessitating further research to address these issues.

**Temporal Risk.** Pika, Ying, and Vidu exhibit a higher NSFW rate compared to other commercial models. This disparity arises because Pika, Ying, and Vidu possesse superior capability in generating continuous actions and variations unique to videos, such as complex movements, subtitle shifts, and transformations in human forms. In contrast, the other models demonstrate weaker generative abilities and fail to meet the minimum threshold necessary to produce such risks to some extent. This underscores the necessity to consider Temporal Risk as a critical new category of risk in the evolving field of video generation, where advancements in model capability continually emerge.

### Holistic perspectives

**Which one is the safest model?** No single model excels in all aspects. Different models showcase distinct strengths. Stable Video Diffusion is nearly impeccable in managing sexual content, achieving an almost 0% NSFW rate. Gen3 and Vidu demonstrate the lowest safety risk in gore and disturbing content, while Pika exhibits exceptional defense capability in copyright & trademark infringement. Ying and Luma are the safest in terms of violence, and Kling excels in handling discrimination and political sensitivity.

**Comparison in terms of aspects.** As depicted in Figure 1(b), first, almost all models underperform in aspects related to Public Figures, Violence, Illegal Activities, Misinformation and Falsehoods, highlighting the critical need for future improvements in these aspects. Additionally, Pika and Open-Sora exhibit higher security risks concerning Pornography, Borderline Pornography, Gore, and Disturbing Content. This heightened vulnerability may stem from the lack of post-generation detectors for videos, resulting in ineffective defenses against these more explicit NSFW dimensions.

**Correlation between GPT-4 and human evaluations.** The correlation between the evaluations of GPT-4 and human assessments is generally strong across most dimensions, with correlation coefficients exceeding 0.75. These findings suggest that leveraging GPT-4 for assessments is reasonable in our context. However, a significant divergence is observed in the dimension of Coherent Contextual Risk, where the correlation coefficient is only 0.627. This discrepancy may stem from GPT-4's limited ability to fully understand scenarios where unsafe content emerges from the temporal linking of presented elements. These observations open new avenues for research into developing better automatic evaluation that excel across multiple safety aspects.

**Trade-off between the accessibility and safety.** It is noteworthy that a trade-off exists between the availability and security of text-to-video generative models. For instance, in the temporal risk dimension, the superior capability of Pika, Ying, and Vidu in generating continuous actions and changes leads to heightened security risks. In contrast, the other commercial models exhibit weaker generative abilities and fail to meet the minimum criteria for posing such risks. Regarding the discrimination dimension, all models struggle to effectively capture this more abstract and complex content, inadvertently resulting in reduced security risks. Moreover, in the borderline pornography dimension, Open-Sora-Plan's limited understanding prevents it from discerning the subtly implied non-direct sexual content, thus enhancing its security. Consequently, weaker generative capability in video generative models paradoxically correlate with higher security in certain dimensions. This also implies that as the field of video generation evolves and model capability strengthens (e.g., the release of Sora), the security risks across various dimensions will increase, underscoring the urgency to prioritize video security.

## 5 Conclusion

In this paper, we introduce a new benchmark for assessing the safety risks of text-to-video models, named T2V SafetyBench. By examining the usage policy and surveying AI safety practitioners, we identify 14 aspects in which generated videos may exhibit illegal or unethical content and construct a malicious text prompt dataset accordingly. We evaluate using GPT-4 and human assessment, observing a high correlation between GPT-4 and human judges. Moreover, we find that no model excels in all aspects, and there is a trade-off between the usability and safety of text-to-video generative models. These insights suggest that as the capability of video generation models increase, safety risks are likely to escalate significantly. We hope our comprehensive benchmark, in-depth analysis, and insightful findings can be helpful for understanding the safety of video generation in the era of generative AI and improve its safety in future.

## Acknowledgement

This work was supported by the NSFC projects (Nos. 62276149, 12288201), NKRDP grant No. 2018YFA0704705. Y. Dong is supported by the China National Postdoctoral Program for Innovative Talents. The authors thank anonymous referees for their valuable comments.

## References

* Gate2ai [2024] Gate2ai. URL https://www.gate2ai.com/prompts-midjourney.
* free ai video generator [2024] Pika ai
- free ai video generator, 2024. URL https://pikartai.com.
* Achiam et al. [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* Bao et al. [2024] Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: a highly consistent, dynamic and skilled text-to-video generator with diffusion models. _arXiv preprint arXiv:2405.04233_, 2024.
* Barrett et al. [2023] Clark Barrett, Brad Boyd, Elie Bursztein, Nicholas Carlini, Brad Chen, Jihye Choi, Amrita Roy Chowdhury, Mihai Christodorescu, Anupam Datta, Soheil Feizi, et al. Identifying and mitigating the security risks of generative ai. _Foundations and Trends(r) in Privacy and Security_, 6(1):1-52, 2023.
* Bhardwaj and Poria [2023] Rishabh Bhardwaj and Soujanya Poria. Red-teaming large language models using chain of utterances for safety-alignment. _arXiv preprint arXiv:2308.09662_, 2023.
* Blattmann et al. [2023] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. _arXiv preprint arXiv:2311.15127_, 2023.
* Blattmann et al. [2023] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22563-22575, 2023.
* Chao et al. [2023] Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J Pappas, and Eric Wong. Jailbreaking black box large language models in twenty queries. _arXiv preprint arXiv:2310.08419_, 2023.
* Daniels and Zurbriggen [2016] Elizabeth A Daniels and Eileen L Zurbriggen. "it's not the right way to do stuff on facebook:" an investigation of adolescent girls' and young women's attitudes toward sexualized photos on social media. _Sexuality & Culture_, 20(4):936-964, 2016.
* Esser et al. [2023] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 7346-7356, 2023.
* Fan et al. [2024] Lili Fan, Chao Guo, Yonglin Tian, Hui Zhang, Jun Zhang, and Fei-Yue Wang. Sora for foundation robots with parallel intelligence: Three world models, three robotic systems. _Frontiers of Information Technology & Electronic Engineering_, pages 1-7, 2024.
* Hanu and Detoxify [2020] Laura Hanu and Unitary team. Detoxify, 2020. URL https://github.com/unitaryai/detoxify.
* He et al. [2022] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. _arXiv preprint arXiv:2211.13221_, 2022.
* Ho et al. [2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.

* Ho et al. [2022] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. _arXiv preprint arXiv:2210.02303_, 2022.
* Hpcaitech [2024] Hpcaitech. Open-sora: Democratizing efficient video production for all, 2024. URL https://github.com/hpcaitech/Open-Sora.
* Huang et al. [2023] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation. _Advances in Neural Information Processing Systems_, 36:78723-78747, 2023.
* Huang et al. [2023] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. _arXiv preprint arXiv:2311.17982_, 2023.
* Khachatryan et al. [2023] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 15954-15964, 2023.
* Kondratyuk et al. [2023] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: A large language model for zero-shot video generation. _arXiv preprint arXiv:2312.14125_, 2023.
* Kwai [2024] Kwai. Kling, 2024. URL https://kling.kuaishou.com.
* Lab and AI [2024] PKU-Yuan Lab and Tuzhan AI. Open-sora-plan, 2024. URL https://github.com/PKU-YuanGroup/Open-Sora-Plan.
* Lee et al. [2023] Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah Teufel, Marco Bellagente, et al. Holistic evaluation of text-to-image models. _Advances in Neural Information Processing Systems_, 36, 2023.
* Liu et al. [2023] Xin Liu, Yichen Zhu, Jindong Gu, Yunshi Lan, Chao Yang, and Yu Qiao. Mm-safetybench: A benchmark for safety evaluation of multimodal large language models. _arXiv preprint arXiv:2311.17600_, 2023.
* Liu et al. [2023] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. _arXiv preprint arXiv:2310.11440_, 2023.
* Liu et al. [2024] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: A review on background, technology, limitations, and opportunities of large vision models. _arXiv preprint arXiv:2402.17177_, 2024.
* Liu et al. [2023] Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, and Lu Hou. Fetv: A benchmark for fine-grained evaluation of open-domain text-to-video generation. _Advances in Neural Information Processing Systems_, 36, 2023.
* Luma [2024] Luma. Luma dream machine, 2024. URL https://lumalabs.ai/dream-machine.
* Luo et al. [2024] Weidi Luo, Siyuan Ma, Xiaogeng Liu, Xiaoyu Guo, and Chaowei Xiao. Jailbreak-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks. _arXiv preprint arXiv:2404.03027_, 2024.
* Ma et al. [2024] Jiachen Ma, Anda Cao, Zhiqing Xiao, Jie Zhang, Chao Ye, and Junbo Zhao. Jailbreaking prompt attack: A controllable adversarial attack against diffusion models. _arXiv preprint arXiv:2404.02928_, 2024.
* Mazeika et al. [2024] Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, et al. Harmbench: A standardized evaluation framework for automated red teaming and robust refusal. _arXiv preprint arXiv:2402.04249_, 2024.

* Sora [2024] OpenAI. Sora: Creating video from text, 2024. URL https://openai.com/sora.
* Peebles and Xie [2023] William Peebles and Saining Xie. Scalable diffusion models with transformers. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4195-4205, 2023.
* Qiu et al. [2023] Huachuan Qiu, Shuai Zhang, Anqi Li, Hongliang He, and Zhenzhong Lan. Latent jailbreak: A benchmark for evaluating text safety and output robustness of large language models. _arXiv preprint arXiv:2307.08487_, 2023.
* Qu et al. [2023] Yiting Qu, Xinyue Shen, Xinlei He, Michael Backes, Savvas Zannettou, and Yang Zhang. Unsafe diffusion: On the generation of unsafe images and hateful memes from text-to-image models. In _Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security_, pages 3403-3417, 2023.
* Qu et al. [2024] Yiting Qu, Xinyue Shen, Yixin Wu, Michael Backes, Savvas Zannettou, and Yang Zhang. Unsafebench: Benchmarking image safety classifiers on real-world and ai-generated images. _arXiv preprint arXiv:2405.03486_, 2024.
* Rando et al. [2022] Javier Rando, Daniel Paleka, David Lindner, Lennart Heim, and Florian Tramer. Red-teaming the stable diffusion safety filter. _arXiv preprint arXiv:2210.04610_, 2022.
* Ray [2023] Partha Pratim Ray. Chatgpt: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope. _Internet of Things and Cyber-Physical Systems_, 2023.
* Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.
* Runway [2024] Runway. Gen-3, 2024. URL https://runwayml.com/blog/introducing-gen-3-alpha.
* Schramowski et al. [2023] Patrick Schramowski, Manuel Brack, Bjorn Deiseroth, and Kristian Kersting. Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22522-22531, 2023.
* Singer et al. [2022] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. _arXiv preprint arXiv:2209.14792_, 2022.
* Sun et al. [2024] Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, and Xihui Liu. T2v-compbench: A comprehensive benchmark for compositional text-to-video generation. _arXiv preprint arXiv:2407.14505_, 2024.
* Tian et al. [2024] Yu Tian, Xiao Yang, Yinpeng Dong, Heming Yang, Hang Su, and Jun Zhu. Bspa: Exploring black-box stealthy prompt attacks against image generators. _arXiv preprint arXiv:2402.15218_, 2024.
* Tiggemann and Slater [2013] Marika Tiggemann and Amy Slater. Netgirls: The internet, facebook, and body image concern in adolescent girls. _International Journal of Eating Disorders_, 46(6):630-633, 2013.
* TIME [2019] TIME. Why some people see more disturbing content on facebook than others, according to leaked documents, 2019. URL https://time.com/6111310/facebook-papers-disturbing-content/.
* Tsai et al. [2023] Yu-Lin Tsai, Chia-Yi Hsu, Chulin Xie, Chih-Hsun Lin, Jia-You Chen, Bo Li, Pin-Yu Chen, Chia-Mu Yu, and Chun-Ying Huang. Ring-a-bell! how reliable are concept removal methods for diffusion models? _arXiv preprint arXiv:2310.10012_, 2023.
* Wang et al. [2023] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et al. Decodingtrust: A comprehensive assessment of trustworthiness in gpt models. _arXiv preprint arXiv:2306.11698_, 2023.
* Wang and Yang [2024] Wenhao Wang and Yi Yang. Vidprom: A million-scale real prompt-gallery dataset for text-to-video diffusion models. _arXiv preprint arXiv:2403.06098_, 2024.

* Wang et al. [2024] Yixiai Wang, Xinzhou Wang, Zilong Chen, Zhengyi Wang, Fuchun Sun, and Jun Zhu. Vidu4d: Single generated video to high-fidelity 4d reconstruction with dynamic gaussian surfels. _arXiv preprint arXiv:2405.16822_, 2024.
* Wang et al. [2022] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. _arXiv preprint arXiv:2212.10560_, 2022.
* Xu et al. [2022] Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, and Jian Li. Learning to break the loop: Analyzing and mitigating repetitions for neural text generation. _Advances in Neural Information Processing Systems_, 35:3082-3095, 2022.
* Yang et al. [2022] Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, and Yue Zhang. Glue-x: Evaluating natural language understanding models from an out-of-distribution generalization perspective. _arXiv preprint arXiv:2211.08073_, 2022.
* Yang et al. [2023] Yijun Yang, Ruiyuan Gao, Xiaosen Wang, Nan Xu, and Qiang Xu. Mma-diffusion: Multimodal attack on diffusion models. _arXiv preprint arXiv:2311.17516_, 2023.
* [56] Yomiuni. Fake video of japan prime minister kishida triggers fears of ai being used to spread misinformation, 2023. URL https://japannews.yomiuri.co.jp/politics/politics-government/20231104-147695/.
* Zhang et al. [2024] Hao Zhang, Wenqi Shao, Hong Liu, Yongqiang Ma, Ping Luo, Yu Qiao, and Kaipeng Zhang. Avibench: Towards evaluating the robustness of large vision-language model on adversarial visual-instructions. _arXiv preprint arXiv:2403.09346_, 2024.
* Zhang et al. [2024] Yichi Zhang, Yao Huang, Yitong Sun, Chang Liu, Zhe Zhao, Zhengwei Fang, Yifan Wang, Huanran Chen, Xiao Yang, Xingxing Wei, et al. Benchmarking trustworthiness of multimodal large language models: A comprehensive study. _arXiv preprint arXiv:2406.07057_, 2024.
* Zhang et al. [2023] Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, and Minlie Huang. Safetybench: Evaluating the safety of large language models with multiple choice questions. _arXiv preprint arXiv:2309.07045_, 2023.
* Ying [2024] Zhipu. Ying, 2024. URL https://chatglm.cn/video?lang=zh.
* Zhou et al. [2022] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. _arXiv preprint arXiv:2211.11018_, 2022.
* Zhu et al. [2023] Junchen Zhu, Huan Yang, Huiguo He, Wenjing Wang, Zixi Tuo, Wen-Huang Cheng, Lianli Gao, Jingkuan Song, and Jianlong Fu. Moviefactory: Automatic movie creation from text using large generative models for language and images. In _Proceedings of the 31st ACM International Conference on Multimedia_, pages 9313-9319, 2023.
* Zhu et al. [2023] Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, et al. Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. _arXiv preprint arXiv:2306.04528_, 2023.
* Zhu et al. [2024] Zheng Zhu, Xiaofeng Wang, Wangbo Zhao, Chen Min, Nianchen Deng, Min Dou, Yuqi Wang, Botian Shi, Kai Wang, Chi Zhang, et al. Is sora a world simulator? a comprehensive survey on general world models and beyond. _arXiv preprint arXiv:2405.03520_, 2024.
* Zou et al. [2023] Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. _arXiv preprint arXiv:2307.15043_, 2023.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? See Appendix C. 3. Did you discuss any potential negative societal impacts of your work? See Appendix C. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them?
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? The dataset is included in the supplementary material. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [N/A] 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [N/A] 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? See Appendix.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [N/A] 3. Did you include any new assets either in the supplemental material or as a URL? [N/A] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]