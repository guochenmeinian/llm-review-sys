Fast Exact Leverage Score Sampling from Khatri-Rao Products with Applications to Tensor Decomposition

Vivek Bharadwaj 1,2, Osman Asif Malik 2, Riley Murray 3,2,1,

**Laura Grigori 4, Aydin Buluc 2,1, James Demmel 1**

1Electrical Engineering and Computer Science Department, UC Berkeley

2Computational Research Division, Lawrence Berkeley National Lab

3International Computer Science Institute

4 Institute of Mathematics, EPFL & Lab for Simulation and Modelling, Paul Scherrer Institute

###### Abstract

We present a data structure to randomly sample rows from the Khatri-Rao product of several matrices according to the exact distribution of its leverage scores. Our proposed sampler draws each row in time logarithmic in the height of the Khatri-Rao product and quadratic in its column count, with persistent space overhead at most the size of the input matrices. As a result, it tractably draws samples even when the matrices forming the Khatri-Rao product have tens of millions of rows each. When used to sketch the linear least squares problems arising in CANDECOMP / PARAFAC tensor decomposition, our method achieves lower asymptotic complexity per solve than recent state-of-the-art methods. Experiments on billion-scale sparse tensors validate our claims, with our algorithm achieving higher accuracy than competing methods as the decomposition rank grows.

## 1 Introduction

The Khatri-Rao product (KRP, denoted by \(\odot\)) is the column-wise Kronecker product of two matrices, and it appears in diverse applications across numerical analysis and machine learning [16]. We examine overdetermined linear least squares problems of the form \(\min_{X}\left\|AX-B\right\|_{F}\), where the design matrix \(A=U_{1}\odot...\odot U_{N}\) is the Khatri-Rao product of matrices \(U_{j}\in\mathbb{R}^{I_{j}\times R}\). These problems appear prominently in signal processing [23], compressed sensing [31], inverse problems related to partial differential equations [5], and alternating least squares (ALS) CANDECOMP / PARAFAC (CP) tensor decomposition [13]. In this work, we focus on the case where \(A\) has moderate column count (several hundred at most). Despite this, the problem remains formidable because the height of \(A\) is \(\prod_{j=1}^{N}I_{j}\). For row counts \(I_{j}\) in the millions, it is intractable to even materialize \(A\) explicitly.

Several recently-proposed randomized sketching algorithms can approximately solve least squares problems with Khatri-Rao product design matrices [4, 12, 15, 18, 29]. These methods apply a sketching operator \(S\) to the design and data matrices to solve the reduced least squares problem \(\min_{\tilde{X}}\left\|SA\tilde{X}-SB\right\|_{F}\), where \(S\) has far fewer rows than columns. For appropriately chosen \(S\), the residual of the downsampled system falls within a specified tolerance \(\varepsilon\) of the optimal residual with high probability \(1-\delta\). In this work, we constrain \(S\) to be a _sampling matrix_ that selects and reweights a subset of rows from both \(A\) and \(B\). When the rows are selected according to the distribution of _statistical leverage scores_ on the design matrix \(A\), only \(\tilde{O}\left(R/(\varepsilon\delta)\right)\) samples are required (subject to the assumptions at the end of section 2.1). The challenge, then, is to efficiently sample according to the leverage scores when \(A\) has Khatri-Rao structure.

We propose a leverage-score sampler for the Khatri-Rao product of matrices with tens of millions of rows each. After construction, our sampler draws each row in time quadratic in the column count, but logarithmic in the total row count of the Khatri-Rao product. Our core contribution is the following theorem.

**Theorem 1.1** (Efficient Khatri-Rao Product Leverage Sampling).: _Given \(U_{1},...,U_{N}\) with \(U_{j}\in\mathbb{R}^{I_{j}\times R}\), there exists a data structure satisfying the following:_

1. _The data structure has construction time_ \(O\left(\sum_{j=1}^{N}I_{j}R^{2}\right)\) _and requires additional storage space_ \(O\left(\sum_{j=1}^{N}I_{j}R\right).\) _If a single entry in a matrix_ \(U_{j}\) _changes, it can be updated in time_ \(O(R\log\left(I_{j}/R\right))\)_. If the entire matrix_ \(U_{j}\) _changes, it can be updated in time_ \(O\left(I_{j}R^{2}\right)\)_._
2. _The data structure produces_ \(J\) _samples from the Khatri-Rao product_ \(U_{1}\odot...\odot U_{N}\) _according to the exact leverage score distribution on its rows in time_ \[O\left(NR^{3}+J\sum_{k=1}^{N}R^{2}\log\max\left(I_{k},R\right)\right)\] _using_ \(O(R^{3})\) _scratch space. The structure can also draw samples from the Khatri-Rao product of any subset of_ \(U_{1},...,U_{N}\)_._

The efficient update property and ability to exclude one matrix are important in CP decomposition. When the inputs \(U_{1},...,U_{N}\) are sparse, an analogous data structure with \(O\left(R\sum_{j=1}^{N}\text{nnz}(U_{j})\right)\) construction time and \(O\left(\sum_{j=1}^{N}\text{nnz}(U_{j})\right)\) storage space exists with identical sampling time. Since the output factor matrices \(U_{1},...,U_{N}\) are typically dense, we defer the proof to Appendix A.8. Combined with error guarantees for leverage-score sampling, we achieve an algorithm for alternating least squares CP decomposition with asymptotic complexity lower than recent state-of-the-art methods (see Table 1).

Our method provides the most practical benefit on sparse input tensors, which may have dimension lengths in the tens of millions (unlike dense tensors that quickly incur intractable storage costs at large dimension lengths) [25]. On the Amazon and Reddit tensors with billions of nonzero entries, our algorithm STS-CP can achieve 95% of the fit of non-randomized ALS between 1.5x and 2.5x faster than a high-performance implementation of the state-of-the-art CP-ARLS-LEV algorithm [15]. Our algorithm is significantly more sample-efficient; on the Enron tensor, only \(\sim 65,000\) samples per solve were required to achieve the 95% accuracy threshold above a rank of 50, which could not be achieved by CP-ARLS-LEV with even 54 times as many samples.

## 2 Preliminaries and Related Work

Notation.We use \([N]\) to denote the set \(\{1,...,N\}\) for a positive integer \(N\). We use \(\tilde{O}\) notation to indicate the presence of multiplicative terms polylogarithmic in \(R\) and \((1/\delta)\) in runtime complexities. For the complexities of our methods, these logarithmic factors are no more than \(O(\log(R/\delta))\). We

\begin{table}
\begin{tabular}{l l} \hline \hline Algorithm & Complexity per Iteration \\ \hline CP-ALS [13] & \(N(N+I)I^{N-1}R\) \\ CP-ARLS-LEV [15] & \(N(R+I)R^{N}/(\varepsilon\delta)\) \\ TNS-CP [19] & \(N^{3}IR^{3}/(\varepsilon\delta)\) \\ Gaussian TNE [17] & \(N^{2}(N^{1.5}R^{3.5}/\varepsilon^{3}+IR^{2})/\varepsilon^{2}\) \\
**STS-CP (ours)** & \(N(NR^{3}\log I+IR^{2})/(\varepsilon\delta)\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Asymptotic Complexity to decompose an \(N\)-dimensional \(I\times...\times I\) dense tensor via CP alternating least squares. For randomized algorithms, each approximate least-squares solution has residual within \((1+\varepsilon)\) of the optimal value with high probability \(1-\delta\). Factors involving \(\log R\) and \(\log(1/\delta)\) are hidden (\(\tilde{O}\) notation). See A.1 for details.

use Matlab notation \(A\left[i,:\right]\), \(A\left[:,i\right]\) to index rows, resp. columns, of matrices. For consistency, we use the convention that \(A\left[i,:\right]\) is a row vector. We use \(\cdot\) for standard matrix multiplication, \(\rightharpoonup\) as the elementwise product, \(\otimes\) to denote the Kronecker product, and \(\odot\) for the Khatri-Rao product. See Appendix A.2 for a definition of each operation. Given matrices \(A\in\mathbb{R}^{m_{1}\times n}\), \(B\in\mathbb{R}^{m_{2}\times n}\), the \(j\)-th column of the Khatri-Rao product \(A\odot B\in\mathbb{R}^{m_{1}m_{2}\times n}\) is the Kronecker product \(A\left[:,j\right]\otimes B\left[:,j\right]\).

We use angle brackets \(\langle\cdot,...,\cdot\rangle\) to denote a **generalized inner product**. For identically-sized vectors / matrices, it returns the sum of all entries in their elementwise product. For \(A,B,C\in\mathbb{R}^{m\times n}\),

\[\langle A,B,C\rangle:=\sum_{i=1,j=1}^{m,n}A\left[i,j\right]B\left[i,j\right]C \left[i,j\right].\]

Finally, \(M^{+}\) denotes the pseudoinverse of matrix \(M\).

### Sketched Linear Least Squares

A variety of random sketching operators \(S\) have been proposed to solve overdetermined least squares problems \(\min_{X}\left\|AX-B\right\|_{F}\) when \(A\) has no special structure [30, 2]. When \(A\) has Khatri-Rao product structure, prior work has focused on _sampling_ matrices [6, 15], which have a single nonzero entry per row, operators composed of fast Fourier / trigonometric transforms [12], or Countsketch-type operators [27, 1]. For tensor decomposition, however, the matrix \(B\) may be sparse or implicitly specified as a black-box function. When \(B\) is sparse, Countsketch-type operators still require the algorithm to iterate over all nonzero values in \(B\). As Larsen and Kolda [15] note, operators similar to the FFT induce fill-in when applied to a sparse matrix \(B\), destroying the benefits of sketching. Similar difficulties arise when \(B\) is implicitly specified. This motivates our decision to focus on row sampling operators, which only touch a subset of entries from \(B\). Let \(\hat{x}_{1},...,\hat{x}_{J}\) be a selection of \(J\) indices for the rows of \(A\in\mathbb{R}^{I\times R}\), sampled i.i.d. according to a probability distribution \(q_{1},...,q_{I}\). The associated sampling matrix \(S\in\mathbb{R}^{J\times I}\) is specified by

\[S\left[j,i\right]=\begin{cases}\frac{1}{\sqrt{Jq_{i}}},&\text{if }\hat{x}_{j}=i\\ 0,&\text{otherwise}\end{cases}\]

where the weight of each nonzero entry corrects bias induced by sampling. When the probabilities \(q_{j}\) are proportional to the _leverage scores_ of the rows of \(A\), strong guarantees apply to the solution of the downsampled problem.

Leverage Score Sampling.The leverage scores of a matrix assign a measure of importance to each of its rows. The leverage score of row \(i\) from matrix \(A\in\mathbb{R}^{I\times R}\) is given by

\[\ell_{i}=A\left[i,:\right](A^{\top}A)^{+}A\left[i,:\right]^{\top}\] (1)

for \(1\leq i\leq I\). Leverage scores can be expressed equivalently as the squared row norms of the matrix \(Q\) in any reduced \(QR\) factorization of \(A\)[8]. The sum of all leverage scores is the rank of \(A\)[30]. Dividing the scores by their sum, we induce a probability distribution on the rows used to generate a sampling matrix \(S\). The next theorem has appeared in several works, and we take the form given by Malik et al. [19]. For an appropriate sample count, it guarantees that the residual of the downsampled problem is close to the residual of the original problem.

**Theorem 2.1** (Guarantees for Leverage Score Sampling).: _Given \(A\in\mathbb{R}^{I\times R}\) and \(\varepsilon,\delta\in(0,1)\), let \(S\in\mathbb{R}^{J\times I}\) be a leverage score sampling matrix for \(A\). Further define \(\tilde{X}=\operatorname*{arg\,min}_{X}\left\|SAX-SB\right\|_{F}\). If \(J\gtrsim R\max\left(\log\left(R/\delta\right),1/(\varepsilon\delta)\right)\), then with probability at least \(1-\delta\) it holds that_

\[\left\|A\tilde{X}-B\right\|_{F}\leq(1+\varepsilon)\min_{X}\left\|AX-B\right\| _{F}.\]

For the applications considered in this work, \(R\) ranges up to a few hundred. As \(\varepsilon\) and \(\delta\) tend to 0 with fixed \(R\), \(1/(\varepsilon\delta)\) dominates \(\log(R/\delta)\). Hence, we assume that the minimum sample count \(J\) to achieve the guarantees of the theorem is \(\Omega(R/(\varepsilon\delta))\).

### Prior Work

Khatri-Rao Product Leverage Score Sampling.Well-known sketching algorithms exist to quickly estimate the leverage scores of dense matrices [8]. These algorithms are, however, intractable for \(A=U_{1}\odot...\odot U_{N}\) due to the height of the Khatri-Rao product. Cheng et al. [6] instead approximate each score as a product of leverage scores associated with each matrix \(U_{j}\). Larsen and Kolda [15] propose CP-ARLS-LEV, which uses a similar approximation and combines random sampling with a deterministic selection of high-probability indices. Both methods were presented in the context of CP decomposition. To sample from the Khatri-Rao product of \(N\) matrices, both require \(O(R^{N}/(\varepsilon\delta))\) samples to achieve the \((\varepsilon,\delta)\) guarantee on the residual of each least squares solution. These methods are simple to implement and perform well when the Khatri-Rao product has column count up to 20-30. On the other hand, they suffer from high sample complexity as \(R\) and \(N\) increase. The TNS-CP algorithm by Malik et al. [19] samples from the exact leverage score distribution, thus requiring only \(O(R/(\varepsilon\delta))\) samples per least squares solve. Unfortunately, it requires time \(O\left(\sum_{j=1}^{N}I_{j}R^{2}\right)\) to draw each sample.

Comparison to Woodruff and Zandieh.The most comparable results to ours appear in work by Woodruff and Zandieh [29], who detail an algorithm for approximate ridge leverage-score sampling for the Khatri-Rao product in near input-sparsity time. Their work relies on a prior oblivious method by Ahle et al. [1], which sketches a Khatri-Rao product using a sequence of Countsketch / OSNAP operators arranged in a tree. Used in isolation to solve a linear least squares problem, the tree sketch construction time scales as \(O\left(\frac{1}{\varepsilon}\sum_{j=1}^{N}\text{nnz}(U_{j})\right)\) and requires an embedding dimension quadratic in \(R\) to achieve the \((\varepsilon,\delta)\) solution-quality guarantee. Woodruff and Zandieh use a collection of these tree sketches, each with carefully-controlled approximation error, to design an algorithm with linear runtime dependence on the column count \(R\). On the other hand, the method exhibits \(O(N^{7})\) scaling in the number of matrices involved, has \(O(\varepsilon^{-4})\) scaling in terms of the desired accuracy, and relies on a sufficiently high ridge regularization parameter. Our data structure instead requires construction time quadratic in \(R\). In exchange, we use distinct methods to design an efficiently-updatable sampler with runtime linear in both \(N\) and \(\varepsilon^{-1}\). These properties are attractive when the column count \(R\) is below several thousand and when error as low as \(\epsilon\approx 10^{-3}\) is needed in the context of an iterative solver (see Figure 5). Moreover, the term \(O(R^{2}\sum_{j=1}^{N}I_{j})\) in our construction complexity arises from symmetric rank-\(k\) updates, a highly-optimized BLAS3 kernel on modern CPU and GPU architectures. Appendix A.3 provides a more detailed comparison between the two approaches.

Kronecker Regression.Kronecker regression is a distinct (but closely related) problem to the one we consider. There, \(A=U_{1}\otimes...\otimes U_{N}\) and the matrices \(U_{i}\) have potentially distinct column counts \(R_{1},...,R_{N}\). While the product distribution of leverage scores from \(U_{1},...,U_{N}\) provides only an approximation to the leverage score distribution of the Khatri-Rao product [6; 15], it provides the _exact_ leverage distribution for the Kronecker product. Multiple works [7; 9] combine this property with other techniques, such as dynamically-updatable tree-sketches [21], to produce accurate and updatable Kronecker sketching methods. None of these results apply directly in our case due to the distinct properties of Kronecker and Khatri-Rao products.

## 3 An Efficient Khatri-Rao Leverage Sampler

Without loss of generality, we will prove part 2 of Theorem 1.1 for the case where \(A=U_{1}\odot...\odot U_{N}\); the case that excludes a single matrix follows by reindexing matrices \(U_{k}\). We further assume that \(A\) is a nonzero matrix, though it may be rank-deficient. Similar to prior sampling works [18; 29], our algorithm will draw one sample from the Khatri-Rao product by sampling a row from each of \(U_{1},U_{2},....\) in sequence and computing their Hadamard product, with the draw from \(U_{j}\) conditioned on prior draws from \(U_{1},...,U_{j-1}\).

Let us index each row of \(A\) by a tuple \((i_{1},...,i_{N})\in[I_{1}]\times...\times[I_{N}]\). Equation (1) gives

\[\ell_{i_{1},...,i_{N}}=A\left[(i_{1},...,i_{N}),:]\left(A^{\top}A\right)^{+}A \left[(i_{1},...,i_{N}),:\right]^{\top}.\] (2)

For \(1\leq k\leq N\), define \(G_{k}:=U_{k}^{\top}U_{k}\in\mathbb{R}^{R\times R}\) and \(G:=\left(\hat{\Re}_{k=1}^{N}G_{k}\right)\in\mathbb{R}^{R\times R}\); it is a well-known fact that \(G=A^{\top}A\)[13]. For a single row sample from \(A\), let \(\hat{s}_{1},...,\hat{s}_{N}\) be random variables for thedraws from multi-index set \([I_{1}]\times...\times[I_{N}]\) according to the leverage score distribution. Assume, for some \(k\), that we have already sampled an index from each of \([I_{1}]\,,...,[I_{k-1}]\), and that the first \(k-1\) random variables take values \(\hat{s}_{1}=s_{1},...,\hat{s}_{k-1}=s_{k-1}\). We abbreviate the latter condition as \(\hat{s}_{<k}=s_{<k}\). To sample from \(I_{k}\), we seek the distribution of \(\hat{s}_{k}\) conditioned on \(\hat{s}_{1},...\hat{s}_{k-1}\). Define \(h_{<k}\) as the transposed elementwise product1 of rows already sampled:

Footnote 1: For \(a>b\), assume that \(\bigcirc_{i=a}^{b}(...)\) produces a vector / matrix filled with ones.

\[h_{<k}:=\bigotimes_{i=1}^{k-1}U_{i}\left[s_{i},:\right]^{\top}.\] (3)

Also define \(G_{>k}\) as

\[G_{>k}:=G^{+}\;\;\bigotimes_{i=k+1}^{N}G_{i}.\] (4)

Then the following theorem provides the conditional distribution of \(\hat{s}_{k}\).

**Theorem 3.1** (Malik 2022, [18], Adapted).: _For any \(s_{k}\in[I_{k}]\),_

\[p(\hat{s}_{k}=s_{k}\mid\hat{s}_{<k}=s_{<k}) =C^{-1}\langle h_{<k}h_{<k}^{\top},U_{k}\left[s_{k},:\right]^{ \top}U_{k}\left[s_{k},:\right],G_{>k}\rangle\] (5) \[:=q_{h_{<k},U_{k},G_{>k}}\left[s_{k}\right]\]

_where \(C=\langle h_{<k}h_{<k}^{\top},U_{k}^{\top}U_{k},G_{>k}\rangle\) is nonzero._

We include the derivation of Theorem 3.1 from Equation (2) in Appendix A.4. Computing all entries of the probability vector \(q_{h_{<k},U_{k},G_{>k}}\) would cost \(O(I_{j}R^{2})\) per sample, too costly when \(U_{j}\) has millions of rows. It is likewise intractable (in preprocessing time and space complexity) to precompute probabilities for every possible conditional distribution on the rows of \(U_{j}\), since the conditioning random variable has \(\prod_{k=1}^{j-1}I_{k}\) potential values. Our key innovation is a data structure to sample from a discrete distribution of the form \(q_{h_{<k},U_{k},G_{>k}}\)_without_ materializing all of its entries or incurring superlinear cost in either \(N\) or \(\varepsilon^{-1}\). We introduce this data structure in the next section and will apply it twice in succession to get the complexity in Theorem 1.1.

### Efficient Sampling from \(q_{h,U,Y}\)

We introduce a slight change of notation in this section to simplify the problem and generalize our sampling lemma. Let \(h\in\mathbb{R}^{R}\) be a vector and let \(Y\in\mathbb{R}^{R\times R}\) be a positive semidefinite (p.s.d.) matrix, respectively. Our task is to sample \(J\) rows from a matrix \(U\in\mathbb{R}^{I\times R}\) according to the distribution

\[q_{h,U,Y}\left[s\right]:=C^{-1}\langle hh^{\top},U^{\top}\left[s,:\right]U\left[ s,:\right],Y\rangle\] (6)

provided the normalizing constant \(C=\langle hh^{\top},U^{\top}U,Y\rangle\), is nonzero. We impose that all \(J\) rows are drawn with the same matrices \(Y\) and \(U\), but potentially distinct vectors \(h\). The following lemma establishes that an efficient sampler for this problem exists.

**Lemma 3.2** (Efficient Row Sampler).: _Given matrices \(U\in\mathbb{R}^{I\times R},Y\in\mathbb{R}^{R\times R}\) with \(Y\) p.s.d., there exists a data structure parameterized by positive integer \(F\) that satisfies the following:_

1. _The structure has construction time_ \(O\left(IR^{2}\right)\) _and storage requirement_ \(O\left(R^{2}\lceil I/F\rceil\right)\)_. If_ \(I<F\)_, the storage requirement drops to_ \(O(1)\)_._
2. _After construction, the data structure can produce a sample according to the distribution_ \(q_{h,U,Y}\) _in time_ \(O(R^{2}\log\lceil I/F\rceil+FR^{2})\) _for any vector_ \(h\)_._
3. _If_ \(Y\) _is a rank-1 matrix, the time per sample drops to_ \(O(R^{2}\log\lceil I/F\rceil+FR)\)_._

This data structure relies on an adaptation of a classic binary-tree inversion sampling technique [22]. Consider a partition of the interval \([0,1]\) into \(I\) bins, the \(i\)-th having width \(q_{h,U,Y}\left[i\right]\). We sample \(d\sim\text{Uniform}\left[0,1\right]\) and return the index of the containing bin. We locate the bin index through a binary search terminated when at most \(F\) bins remain in the search space, which are then scanned in linear time. Here, \(F\) is a tuning parameter that we will use to control sampling complexity and space usage.

We can regard the binary search as a walk down a full, complete binary tree \(T_{I,F}\) with \(\left[I/F\right]\) leaves, the nodes of which store contiguous, disjoint segments \(S(v)=\left\{S_{0}(v)..S_{1}(v)\right\}\subseteq\left[I\right]\) of size at most \(F\). The segment of each internal node is the union of segments held by its children, and the root node holds \(\left\{1,...,I\right\}\). Suppose that the binary search reaches node \(v\) with left child \(L(v)\) and maintains the interval \(\left[\text{low},\text{high}\right]\subseteq\left[0,1\right]\) as the remaining search space to explore. Then the search branches left in the tree iff \(d<\text{low}+\sum_{i\in S(L(v))}q_{h,U,Y}\left[i\right].\)

This branching condition can be evaluated efficiently if appropriate information is stored at each node of the segment tree. Excluding the offset "low", the branching threshold takes the form

\[\sum_{i\in S(v)}q_{h,U,Y}\left[i\right]=C^{-1}\langle hh^{\top},\sum_{i\in S(v )}U\left[i,\cdot\right]^{\top}U\left[i,\cdot\right],Y\rangle:=C^{-1}\langle hh ^{\top},G^{v},Y\rangle.\] (7)

Here, we call each matrix \(G^{v}\in\mathbb{R}^{R\times R}\) a _partial Gram matrix_. In time \(O(IR^{2})\) and space \(O(R^{2}\lceil I/F\rceil)\), we can compute and cache \(G^{v}\) for each node of the tree to construct our data structure. Each subsequent binary search costs \(O(R^{2})\) time to evaluate Equation (7) at each of \(\log\lceil I/F\rceil\) internal nodes and \(O(FR^{2})\) to evaluate \(q_{h,U,Y}\) at the \(F\) indices held by each leaf, giving point 2 of the lemma. This cost at each leaf node reduces to \(O(FR)\) in case \(Y\) is rank-1, giving point 3. A complete proof of this lemma appears in Appendix A.5.

### Sampling from the Khatri-Rao Product

We face difficulties if we directly apply Lemma 3.2 to sample from the conditional distribution in Theorem 3.1. Because \(G_{>k}\) is not rank-1 in general, we must use point 2 of the lemma, where no selection of the parameter \(F\) allows us to simultaneously satisfy the space and runtime constraints of Theorem 1.1. Selecting \(F=R\) results in cost \(O(R^{3})\) per sample (violating the runtime requirement in point 2), whereas \(F=1\) results in a superlinear storage requirement \(O(IR^{2})\) (violating the space requirement in point 1, and becoming prohibitively expensive for \(I\geq 10^{6}\)). To avoid these extremes, we break the sampling procedure into two stages. The first stage selects a 1-dimensional subspace spanned by an eigenvector of \(G_{>k}\), while the second samples according to Theorem 3.1 after projecting the relevant vectors onto the selected subspace. Lemma 3.2 can be used for _both_ stages, and the second stage benefits from point 3 to achieve better time and space complexity.

Below, we abbreviate \(q=q_{h_{<k},U_{k},G_{>k}}\) and \(h=h_{<k}\). When sampling from \(I_{k}\), observe that \(G_{>k}\) is the same for all samples. We compute a symmetric eigendecomposition \(G_{>k}=V\Lambda V^{\top}\), where each column of \(V\) is an eigenvector of \(G_{>k}\) and \(\Lambda=\operatorname{diag}((\lambda_{u})_{u=1}^{R})\) contains the eigenvalues along the diagonal. This allows us to rewrite entries of \(q\) as

\[q\left[s\right]=C^{-1}\sum_{u=1}^{R}\lambda_{u}\langle hh^{\top},U_{k}\left[s, :\right]^{\top}U_{k}\left[s,:\right],V\left[:,u\right]V\left[:,u\right]^{\top }\rangle.\] (8)

Define matrix \(W\in\mathbb{R}^{I_{k}\times R}\) elementwise by

\[W\left[t,u\right]:=\langle hh^{\top},U_{k}\left[t,:\right]^{\top}U_{k}\left[t,:\right],V\left[:,u\right]V\left[:,u\right]^{\top}\rangle\]

and observe that all of its entries are nonnegative. Since \(\lambda_{u}\geq 0\) for all \(u\) (\(G_{>k}\) is p.s.d.), we can write \(q\) as a mixture of probability distributions given by the normalized columns of \(W\):

\[q=\sum_{u=1}^{R}w\left[u\right]\frac{W\left[:,u\right]}{\left\|W\left[:,u \right]\right\|_{1}},\]

where the vector \(w\) of nonnegative weights is given by \(w\left[u\right]=\left(C^{-1}\lambda_{u}\left\|W\left[:,u\right]\right\|_{1}\right)\). Rewriting \(q\) in this form gives us the two stage sampling procedure: first sample a component \(u\) of the mixture according to the weight vector \(w\), then sample an index in \(\left\{1..I_{k}\right\}\) according to the probability vector

Figure 1: A segment tree \(T_{8,2}\) and probability distribution \(\left\{q_{1},...,q_{8}\right\}\) on \(\left[1,...,8\right]\).

[MISSING_PAGE_FAIL:7]

in the Khatri-Rao product to match the ordering of rows in the matricized tensor (see Appendix A.9 for an explicit formula for the matricization). These problems are ideal candidates for randomized sketching [4; 12; 15], and applying the data structure in Theorem 1.1 gives us the **STS-CP** algorithm.

**Corollary 3.3** (Sts-Cp).: _Suppose \(\mathcal{T}\) is dense, and suppose we solve each least squares problem in ALS with a randomized sketching algorithm. A leverage score sampling approach as defined in section 2 guarantees that with \(\tilde{O}(R/(\varepsilon\delta))\) samples per solve, the residual of each sketched least squares problem is within \((1+\varepsilon)\) of the optimal residual with probability \((1-\delta)\). The efficient sampler from Theorem 1.1 brings the complexity of ALS to_

\[\tilde{O}\left(\frac{\#\text{it}}{\varepsilon\delta}\cdot\sum_{j=1}^{N}\left( NR^{3}\log I_{j}+I_{j}R^{2}\right)\right)\]

_where "\(\#\)it" is the number of ALS iterations, and with any term \(\log I_{j}\) replaced by \(\log R\) if \(I_{j}<R\)._

The proof appears in Appendix A.9 and combines Theorem 1.1 with Theorem 2.1. STS-CP also works for sparse tensors and likely provides a greater advantage here than the dense case, as sparse tensors tend to have much larger mode size [25]. The complexity for sparse tensors depends heavily on the sparsity structure and is difficult to predict. Nevertheless, we expect a significant speedup based on prior works that use sketching to accelerate CP decomposition [6; 15].

## 4 Experiments

Experiments were conducted on CPU nodes of NERSC Perlmutter, an HPE Cray EX supercomputer, and our code is available at https://github.com/vbharadwaj-bk/fast_tensor_leverage.git. On tensor decomposition experiments, we compare our algorithms against the random and hybrid versions of CP-ARLS-LEV proposed by Larsen and Kolda [15]. These algorithms outperform uniform sampling and row-norm-squared sampling, achieving excellent accuracy and runtime relative to exact ALS. In contrast to TNS-CP and the Gaussian tensor network embedding proposed by Ma and Solomonik (see Table 1), CP-ARLS-LEV is one of the few algorithms that can practically decompose sparse tensors with mode sizes in the millions. In the worst case, CP-ARLS-LEV requires \(\tilde{O}(R^{N-1}/(\varepsilon\delta))\) samples per solve for an \(N\)-dimensional tensor to achieve solution guarantees like those in Theorem 2.1, compared to \(\tilde{O}(R/(\varepsilon\delta))\) samples required by STS-CP. Appendices A.10, A.11, and A.13 provide configuration details and additional results.

### Runtime Benchmark

Figure 2 shows the time to construct our sampler and draw 50,000 samples from the Khatri-Rao product of i.i.d. Gaussian initialized factor matrices. We quantify the runtime impacts of varying \(N\), \(R\), and \(I\). The asymptotic behavior in Theorem 1.1 is reflected in our performance measurements, with the exception of the plot that varies \(R\). Here, construction becomes disproportionately cheaper than sampling due to cache-efficient BLAS3 calls during construction. Even when the full Khatri-Rao product has \(\approx 3.78\times 10^{22}\) rows (for \(I=2^{25},N=3,R=32\)), we require only \(0.31\) seconds on average for sampling (top plot, rightmost points).

### Least Squares Accuracy Comparison

We now test our sampler on least squares problems of the form \(\min_{x}\|Ax-b\|\), where \(A=U_{1}\odot...\odot U_{N}\) with \(U_{j}\in\mathbb{R}^{I\times R}\) for all \(j\). We initialize all matrices \(U_{j}\) entrywise i.i.d. from a standard normal distribution and randomly multiply 1% of all entries by 10. We choose \(b\) as a Kronecker product \(c_{1}\otimes...\otimes c_{N}\), with each vector \(c_{j}\in\mathbb{R}^{I}\) also initialized entrywise from a Gaussian distribution. We assume this form for \(b\) to tractably compute the exact solution to the linear least squares problem and evaluate the accuracy of our randomized methods. We **do not** give our algorithms access to the Kronecker form of \(b\); they are only permitted on-demand, black-box access to its entries.

For each problem instance, define the distortion of our sampling matrix \(S\) with respect to the column space of \(A\) as

\[D(S,A)=\frac{\kappa(SQ)-1}{\kappa(SQ)+1}\] (12)

where \(Q\) is an orthonormal basis for the column space of \(A\) and \(\kappa(SQ)\) is the condition number of \(SQ\). A higher-quality sketch \(S\) exhibits lower distortion, which quantifies the preservation of distances from the column space of \(A\) to the column space of \(SA\)[20]. For details about computing \(\kappa(SQ)\) efficiently when \(A\) is a Khatri-Rao product, see Appendix A.12. Next, define \(\varepsilon=\frac{\text{residual}_{\text{approx}}}{\text{residual}_{ \text{gt}}}-1\), where \(\text{residual}_{\text{approx}}\) is the residual of the randomized least squares algorithm. \(\varepsilon\) is nonnegative and (similar to its role in Theorem 2.1) quantifies the quality of the randomized algorithm's solution.

For varying \(N\) and \(R\), Figure 3 shows the average values of \(D\) and \(\varepsilon\) achieved by our algorithm against the leverage product approximation used by Larsen and Kolda [15]. Our sampler exhibits nearly constant distortion \(D\) for fixed rank \(R\) and varying \(N\), and it achieves \(\varepsilon\approx 10^{-2}\) even when \(N=9\). The product approximation increases both the distortion and residual error by at least an order of magnitude.

### Sparse Tensor Decomposition

We next apply STS-CP to decompose several large sparse tensors from the FROSTT collection [25] (see Appendix A.11 for more details on the experimental configuration). Our accuracy metric is the tensor fit. Letting \(\tilde{\mathcal{T}}\) be our low-rank CP approximation, the fit with respect to ground-truth tensor \(\mathcal{T}\) is \(\text{fit}(\tilde{\mathcal{T}},\mathcal{T})=1-\left\|\tilde{\mathcal{T}}- \mathcal{T}\right\|_{F}/\left\|\mathcal{T}\right\|_{F}\).

Table 4 in Appendix A.13.1 compares the runtime per round of our algorithm against the Tensorly Python package [14] and Matlab Tensor Toolbox [3], with dramatic speedup over both. As Figure 4 shows, the fit achieved by CP-ARLS-LEV compared to STS-CP degrades as the rank increases forfixed sample count. By contrast, STS-CP improves the fit consistently, with a significant improvement at rank 125 over CP-ARLS-LEV. Timings for both algorithms are available in Appendix A.13.5. Figure 6 explains the higher fit achieved by our sampler on the Uber and Amazon tensors. In the first 10 rounds of ALS, we compute the exact solution to each least squares problem before updating the factor matrix with a randomized algorithm's solution. Figure 6 plots \(\varepsilon\) as ALS progresses for hybrid CP-ARLS-LEV and STS-CP. The latter consistently achieves lower residual per solve. We further observe that CP-ARLS-LEV exhibits an oscillating error pattern with period matching the number of modes \(N\).

To assess the tradeoff between sampling time and accuracy, we compare the fit as a function of ALS update time for STS-CP and random CP-ARLS-LEV in Figure 6 (time to compute the fit excluded). On the Reddit tensor with \(R=100\), we compared CP-ARLS-LEV with \(J=2^{16}\) against CP-ARLS-LEV with progressively larger sample count. Even with \(2^{18}\) samples per randomized least squares solve, CP-ARLS-LEV cannot achieve the maximum fit of STS-CP. Furthermore, STS-CP makes progress more quickly than CP-ARLS-LEV. See Appendix A.13.4 for similar plots for other datasets.

## 5 Discussion and Future Work

Our method for exact Khatri-Rao leverage score sampling enjoys strong theoretical guarantees and practical performance benefits. Especially for massive tensors such as Amazon and Reddit, our randomized algorithm's guarantees translate to faster progress to solution and higher final accuracies. The segment tree approach described here can be applied to sample from tensor networks besides the Khatri-Rao product. In particular, modifications to Lemma 3.2 permit efficient leverage sampling from a contraction of 3D tensor cores in ALS tensor train decomposition. We leave the generalization of our fast sampling technique as future work.

## Acknowledgements, Funding, and Disclaimers

We thank the referees for valuable feedback which helped improve the paper.

V. Bharadwaj was supported by the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Department of Energy Computational Science Graduate Fellowship under Award Number DE-SC0022158. O. A. Malik and A. Buluc were supported by the Office of Science of the DOE under Award Number DE-AC02-05CH11231. L. Grigori was supported by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program through grant agreement 810367. R. Murray was supported by Laboratory Directed Research and Development (LDRD) funding from Berkeley Lab, provided by the Director, Office of Science, of the U.S. DOE under Contract No. DE-AC02-05CH11231. R. Murray was also funded by an NSF Collaborative Research Framework under NSF Grant Nos. 2004235 and 2004763. This research used resources of the National Energy Research Scientific Computing Center, a DOE Office of Science User Facility, under Contract No. DE-AC02-05CH11231 using NERSC award ASCR-ERCAP0024170.

This report was prepared as an account of work sponsored by an agency of the United States Government. Neither the United States Government nor any agency thereof, nor any of their employees, makes any warranty, express or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights. Reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government or any agency thereof. The views and opinions of authors expressed herein do not necessarily state or reflect those of the United States Government or any agency thereof.

## References

* Ahle et al. [2020] Thomas D. Ahle, Michael Kapralov, Jakob B. T. Knudsen, Rasmus Pagh, Ameya Velingker, David P. Woodruff, and Amir Zandieh. _Oblivious Sketching of High-Degree Polynomial Kernels_, pages 141-160. Society for Industrial and Applied Mathematics, 2020. doi: 10.1137/1.9781611975994.9.
* Ailon and Chazelle [2009] Nir Ailon and Bernard Chazelle. The fast Johnson-Lindenstrauss transform and approximate nearest neighbors. _SIAM Journal on computing_, 39(1):302-322, 2009.
* Bader and Kolda [2008] Brett W. Bader and Tamara G. Kolda. Efficient matlab computations with sparse and factored tensors. _SIAM Journal on Scientific Computing_, 30(1):205-231, 2008. doi: 10.1137/060676489.
* Battaglino et al. [2018] Casey Battaglino, Grey Ballard, and Tamara G. Kolda. A practical randomized CP tensor decomposition. _SIAM Journal on Matrix Analysis and Applications_, 39(2):876-901, 2018. doi: 10.1137/17M1112303.
* Chen et al. [2020] Ke Chen, Qin Li, Kit Newton, and Stephen J. Wright. Structured random sketching for PDE inverse problems. _SIAM Journal on Matrix Analysis and Applications_, 41(4):1742-1770, 2020. doi: 10.1137/20M1310497.
* Cheng et al. [2016] Dehua Cheng, Richard Peng, Yan Liu, and Ioakeim Perros. SPALS: Fast alternating least squares via implicit leverage scores sampling. In _Advances in Neural Information Processing Systems_, volume 29. Curran Associates, Inc., 2016.
* Diao et al. [2019] Huaian Diao, Rajesh Jayaram, Zhao Song, Wen Sun, and David Woodruff. Optimal sketching for Kronecker product regression and low rank approximation. In _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* Drineas et al. [2012] Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, and David P. Woodruff. Fast approximation of matrix coherence and statistical leverage. _J. Mach. Learn. Res._, 13(1):3475-3506, dec 2012. ISSN 1532-4435.
* Fahrbach et al. [2022] Matthew Fahrbach, Gang Fu, and Mehrdad Ghadiri. Subquadratic Kronecker regression with applications to tensor decomposition. In _Advances in Neural Information Processing Systems_, volume 35, pages 28776-28789. Curran Associates, Inc., 2022.

* Haidar et al. [2015] Azzam Haidar, Tingxing Dong, Stanimire Tomov, Piotr Luszczek, and Jack Dongarra. Framework for batched and GPU-resident factorization algorithms to block Householder transformations. In _ISC High Performance_, Frankfurt, Germany, 07-2015 2015. Springer, Springer.
* Halko et al. [2011] N. Halko, P. G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. _SIAM Review_, 53(2):217-288, 2011. doi: 10.1137/090771806.
* Jin et al. [2020] Ruhui Jin, Tamara G Kolda, and Rachel Ward. Faster Johnson-Lindenstrauss transforms via Kronecker products. _Information and Inference: A Journal of the IMA_, 10(4):1533-1562, October 2020. ISSN 2049-8772. doi: 10.1093/imaiai/iaaa028.
* Kolda and Bader [2009] Tamara G. Kolda and Brett W. Bader. Tensor decompositions and applications. _SIAM Review_, 51(3):455-500, August 2009. ISSN 0036-1445. doi: 10.1137/07070111X. Publisher: Society for Industrial and Applied Mathematics.
* Kossaifi et al. [2019] Jean Kossaifi, Yannis Panagakis, Anima Anandkumar, and Maja Pantic. Tensorly: Tensor learning in python. _Journal of Machine Learning Research (JMLR)_, 20(26), 2019.
* Larsen and Kolda [2022] Brett W. Larsen and Tamara G. Kolda. Practical leverage-based sampling for low-rank tensor decomposition. _SIAM J. Matrix Analysis and Applications_, 43(3):1488-1517, August 2022. doi: 10.1137/21M1441754.
* Liu and Trenkler [2008] Shuangzhe Liu and Gotz Trenkler. Hadamard, Khatri-Rao, Kronecker and other matrix products. _International Journal of Information and Systems Sciences_, 4(1):160-177, 2008.
* Ma and Solomonik [2022] Linjian Ma and Edgar Solomonik. Cost-efficient gaussian tensor network embeddings for tensor-structured inputs. In _Advances in Neural Information Processing Systems_, volume 35, pages 38980-38993. Curran Associates, Inc., 2022.
* Malik [2022] Osman Asif Malik. More efficient sampling for tensor decomposition with worst-case guarantees. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 14887-14917. PMLR, 17-23 Jul 2022.
* Malik et al. [2022] Osman Asif Malik, Vivek Bharadwaj, and Riley Murray. Sampling-based decomposition algorithms for arbitrary tensor networks, October 2022. arXiv:2210.03828 [cs, math].
* Murray et al. [2023] Riley Murray, James Demmel, Michael W. Mahoney, N. Benjamin Erichson, Maksim Melnichenko, Osman Asif Malik, Laura Grigori, Piotr Luszczek, Michal Derezinski, Miles E. Lopes, Tianyu Liang, Hengrui Luo, and Jack Dongarra. Randomized numerical linear algebra : A perspective on the field with an eye to software, 2023.
* Reddy et al. [2022] Aravind Reddy, Zhao Song, and Lichen Zhang. Dynamic tensor product regression. In _Advances in Neural Information Processing Systems_, volume 35, pages 4791-4804. Curran Associates, Inc., 2022.
* Saad et al. [2020] Feras A. Saad, Cameron E. Freer, Martin C. Rinard, and Vikash K. Mansinghka. Optimal approximate sampling from discrete probability distributions. _Proceedings of the ACM on Programming Languages_, 4(POPL):1-31, January 2020. ISSN 2475-1421. doi: 10.1145/3371104.
* Sidiropoulos and Budampati [2002] N.D. Sidiropoulos and R.S. Budampati. Khatri-Rao space-time codes. _IEEE Transactions on Signal Processing_, 50(10):2396-2407, 2002. doi: 10.1109/TSP.2002.803341.
* Smith and Karypis [2016] Shaden Smith and George Karypis. SPLATT: The Surprisingly ParalleL spArse Tensor Toolkit. https://github.com/ShadenSmith/splatt, 2016.
* Smith et al. [2017] Shaden Smith, Jee W. Choi, Jiajia Li, Richard Vuduc, Jongsoo Park, Xing Liu, and George Karypis. FROSTT: The formidable repository of open sparse tensors and tools, 2017. URL http://frostt.io/.

* Song et al. [2022] Linghao Song, Yuze Chi, Atefeh Sohrabizadeh, Young-kyu Choi, Jason Lau, and Jason Cong. Sextans: A streaming accelerator for general-purpose sparse-matrix dense-matrix multiplication. In _Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays_, FPGA '22, page 65-77, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450391498. doi: 10.1145/3490422.3502357.
* Wang et al. [2015] Yining Wang, Hsiao-Yu Tung, Alexander J Smola, and Anima Anandkumar. Fast and guaranteed tensor decomposition via sketching. _Advances in neural information processing systems_, 28, 2015.
* Wijeratne et al. [2023] Sasindu Wijeratne, Ta-Yang Wang, Rajgopal Kannan, and Viktor Prasanna. Accelerating sparse MTTKRP for tensor decomposition on FPGA. In _Proceedings of the 2023 ACM/SIGDA International Symposium on Field Programmable Gate Arrays_, FPGA '23, page 259-269, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9781450394178. doi: 10.1145/3543622.3573179.
* Woodruff and Zandieh [2022] David Woodruff and Amir Zandieh. Leverage score sampling for tensor product matrices in input sparsity time. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 23933-23964. PMLR, 17-23 Jul 2022.
* Woodruff et al. [2014] David P Woodruff et al. Sketching as a tool for numerical linear algebra. _Foundations and Trends(r) in Theoretical Computer Science_, 10(1-2):1-157, 2014.
* Yu et al. [2010] Yao Yu, Athina P. Petropulu, and H. Vincent Poor. MIMO radar using compressive sampling. _IEEE Journal of Selected Topics in Signal Processing_, 4(1):146-163, February 2010. doi: 10.1109/JSTSP.2009.2038973.

Appendix

### Details about Table 1

CP-ALS [13] is the standard, non-randomized alternating least squares method given by Algorithm 6 in Appendix A.9. The least squares problems in the algorithm are solved by exact methods. CP-ARLS-LEV is the algorithm proposed by Larsen and Kolda [15] that samples rows from the Khatri-Rao product according to a product distribution of leverage scores on each factor matrix. The per-iteration runtimes for both algorithms are re-derived in Appendix C.3 of the work by Malik [18] from their original sources. Malik [18] proposed the CP-ALS-ES algorithm (not listed in the table), which is superseded by the TNS-CP algorithm [19]. We report the complexity from Table 1 of the latter work. The algorithm by Ma and Solomonik [17] is based on a general method to sketch tensor networks. Our reported complexity is listed in Table 1 for Algorithm 1 in their work.

Table 1 does not list the one-time initialization costs for any of the methods. All methods require at least \(O(NIR)\) time to randomly initialize factor matrices, and CP-ALS requires no further setup. CP-ARLS-LEV, TNS-CP, and STS-CP all require \(O(NIR^{2})\) initialization time. CP-ARLS-LEV uses the initialization phase to compute the initial leverage scores of all factor matrices. TNS-CP uses the initialization step to compute and cache Gram matrices of all factors \(U_{j}\). STS-CP must build the efficient sampling data structure described in Theorem 1.1. The algorithm from Ma and Solomonik requires an initialization cost of \(O(I^{N}m)\), where \(m\) is a sketch size parameter that is \(O(NR/\varepsilon^{2})\) to achieve the \((\varepsilon,\delta)\) accuracy guarantee for each least squares solve.

### Definitions of Matrix Products

Table 2 defines the standard matrix product, Hadamard product, Kronecker product \(\otimes\), and Khatri-Rao product \(\odot\), as well as the dimensions of their operands.

### Further Comparison to Prior Work

In this section, we provide a more detailed comparison of our sampling algorithm with the one proposed by Woodruff and Zandieh [29]. Their work introduces a ridge leverage-score sampling algorithm for Khatri-Rao products with the attractive property that the sketch can be formed in input-sparsity time. For constant failure probability \(\delta\), the runtime to produce a \((1\pm\epsilon)\)\(\ell_{2}\)-subspace embedding for \(A=U_{1}\odot...\odot U_{N}\) is given in Appendix B of their work (proof of Theorem 2.7). Adapted to our notation, **their runtime** is

\[O\left(\log^{4}R\log N\sum_{i=1}^{N}\text{nnz}(U_{i})+\frac{N^{7}s_{\lambda}^ {2}R}{\varepsilon^{4}}\log^{5}R\log N\right)\]

where \(s_{\lambda}=\sum_{i=1}^{R}\frac{\lambda_{i}}{\lambda_{i}+\lambda}\), \(\lambda_{1},...,\lambda_{R}\) are the eigenvalues of the Gram matrix \(G\) of matrix \(A\), and \(\lambda\geq 0\) is a regularization parameter. For comparison, **our runtime** for constant failure probability \(\delta\) is

\[O\left(R\sum_{i=1}^{N}\text{nnz}(U_{i})+\frac{R^{3}}{\varepsilon}\log\left( \prod_{i=1}^{N}I_{i}\right)\log R\right).\]

Woodruff and Zandieh's method provides a significant advantage for large column count \(R\) or high regularization parameter \(\lambda\). As a result, it is well-suited to the problem of regularized low-rank approximation when the column count \(R\) is given by the number of data points in a dataset. On the

\begin{table}
\begin{tabular}{l l l l l} \hline \hline Operation & Size of \(A\) & Size of \(B\) & Size of \(C\) & Definition \\ \hline \(C=A\cdot B\) & \((m,k)\) & \((k,n)\) & \((m,n)\) & \(C[i,j]=\sum_{i=1}^{k}A\left[i,a\right]B\left[a,j\right]\) \\ \(C=A\oplus B\) & \((m,n)\) & \((m,n)\) & \((m,n)\) & \(C\left[i,j\right]=A\left[i,j\right]B\left[i,j\right]\) \\ \(C=A\oplus B\) & \((m_{1},n_{1})\) & \((m_{2},n_{2})\) & \((m_{2}m_{1},n_{2}n_{1})\) & \(C\left[(i_{2},i_{1}),(j_{2},j_{1})\right]=A\left[i_{1},j_{1}\right]B\left[i_{2 },j_{2}\right]\) \\ \(C=A\ominus B\) & \((m_{1},n)\) & \((m_{2},n)\) & \((m_{2}m_{1},n)\) & \(C\left[(i_{2},i_{1}),j\right]=A\left[i_{1},j\right]B\left[i_{2},j\right]\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Matrix product definitions.

other hand, the algorithm has poor dependence on the matrix count \(N\) and error parameter \(\varepsilon\). For tensor decomposition, \(R\) is typically no larger than a few hundred, while high accuracy (\(\epsilon\approx 10^{-3}\)) is required for certain tensors to achieve a fit competitive with non-randomized methods (see section 4.3, Figures 4 and 5). When \(\lambda\) is small, we have \(s_{\lambda}\approx R\). Here, Woodruff and Zandieh's runtime has an \(O(R^{3})\) dependence similar to ours. When \(R\leq\log^{4}R\log N\), our sampler has faster construction time as well.

Finally, we note that our sampling data structure can be constructed using highly cache-efficient, parallelizable symmetric rank-\(R\) updates (BLAS3 operation dSYRR). As a result, the quadratic dependence on \(R\) in our algorithm can be mitigated by dense linear algebra accelerators, such as GPUs or TPUs.

### Proof of Theorem 3.1

Theorem 3.1 appeared in a modified form as Lemma 10 in the work by Malik [18]. This original version used the definition in place of \(G_{>k}\) defined in Equation (4), where \(\Phi\) was a sketched approximation of \(G^{+}\). Woodruff and Zandieh [29] exhibit a version of the theorem with similar modifications. We prove the version stated in our work below.

Proof of Theorem 3.1.: We rely on the assumption that the Khatri-Rao product \(A\) is a nonzero matrix (but it may be rank-deficient). We begin by simplifying the expression for the leverage score of a row of \(A\) corresponding to multi-index \((i_{1},...,i_{N})\). Beginning with Equation (2), we derive

\[\ell_{i_{1},...,i_{N}} =A\left[(i_{1},...,i_{N}),:\right]G^{+}A\left[(i_{1},...,i_{N}),: \right]^{\top}\] (13) \[=\langle A\left[(i_{1},...,i_{N}),:\right]^{\top}A\left[(i_{1},...,i_{N}),:\right],G^{+}\rangle\] \[=\langle\left(\mathop{\bigotimes_{a=1}^{N}}_{a=1}U_{a}\left[i_{a},:\right]\right)^{\top}\left(\mathop{\bigotimes_{a=1}^{N}}_{a=1}U_{a}\left[i_{ a},:\right]\right),G^{+}\rangle\] \[=\langle\mathop{\bigotimes_{a=1}^{N}}_{a=1}U_{a}\left[i_{a},: \right]^{\top}U_{a}\left[i_{a},:\right],U_{k}\left[i_{k},:\right]^{\top}U_{k} \left[i_{k},:\right]\mathop{\bigotimes_{a=k+1}^{N}}_{a=k+1}U_{a}\left[i_{a},: \right]^{\top}U_{a}\left[i_{a},:\right],G^{+}\rangle\] \[=\langle\mathop{\bigotimes_{a=1}^{N}}_{a=1}U_{a}\left[i_{a},: \right]^{\top}U_{a}\left[i_{a},:\right],U_{k}\left[i_{k},:\right]^{\top}U_{k} \left[i_{k},:\right],G^{+}\mathop{\bigotimes_{a=k+1}^{N}}_{a=k+1}U_{a}\left[i_{ a},:\right]^{\top}U_{a}\left[i_{a},:\right]\rangle.\]

We proceed to the main proof of the theorem. To compute \(p(\hat{s}_{k}=s_{k}\mid\hat{s}_{<k}=s_{<k})\), we marginalize over random variables \(\hat{s}_{k+1}...\hat{s}_{N}\). Recalling the definition of \(h_{<k}\) from Equation (3), we have

\[p(\hat{s}_{k}=s_{k}\mid\hat{s}_{<k}=s_{<k}) \propto\sum_{i_{k+1},...,i_{N}}p\left((\hat{s}_{<k}=s_{<k})\wedge (\hat{s}_{k}=s_{k})\wedge\bigwedge_{u=k+1}^{N}(\hat{s}_{u}=i_{u})\right)\] (14) \[\propto\sum_{i_{k+1},...,i_{N}}\ell_{s_{1},...,s_{k},i_{k+1},...,i_{N}}.\]

The first line above follows by marginalizing over \(\hat{s}_{k+1},...,\hat{s}_{N}\). The second line follows because the joint random variable \((\hat{s}_{1},...,\hat{s}_{N})\) follows the distribution of statistical leverage scores on the rows of \(A\). We now plug in Equation (13) to get

\[\sum_{i_{k}+1,\ldots,i_{N}}\ell_{s_{1},\ldots,s_{k},i_{k+1},\ldots,i _{N}}\] (15) \[=\sum_{i_{k}+1,\ldots,i_{N}}\langle\big{(}\cstsa_{4}=1\big{)}\,U_{a }\left[s_{a},:\right]^{\top}U_{a}\left[s_{a},:\right],U_{k}\left[s_{k},:\right] ^{\top}U_{k}\left[s_{k},:\right],G^{+}\cstsa_{a=k+1}^{N}U_{a}\left[i_{a},: \right]^{\top}U_{a}\left[i_{a},:\right]\rangle\] \[=\sum_{i_{k}+1,\ldots,i_{N}}\langle h_{<k}h_{<k}^{\top},U_{k} \left[s_{k},:\right]^{\top}U_{k}\left[s_{k},:\right],G^{+}\cstsa_{a=k+1}^{N}U_{ a}\left[i_{a},:\right]^{\top}U_{a}\left[i_{a},:\right]\rangle\] \[=\langle h_{<k}h_{<k}^{\top},U_{k}\left[s_{k},:\right]^{\top}U_{k }\left[s_{k},:\right],G^{+}\cstsa_{a=k+1}^{N}G_{a}\rangle\] \[=\langle h_{<k}h_{<k}^{\top},U_{k}\left[s_{k},:\right]^{\top}U_{k }\left[s_{k},:\right],G_{>k}\rangle.\]

We now compute the normalization constant \(C\) for the distribution by summing the last line of Equation (15) over all possible values for \(\hat{s}_{k}\):

\[C =\sum_{s_{k}=1}^{I_{k}}\langle h_{<k}h_{<k}^{\top},U_{k}\left[s_{k },:\right]^{\top}U_{k}\left[s_{k},:\right],G_{>k}\rangle\] (16) \[=\langle h_{<k}h_{<k}^{\top},\sum_{s_{k}=1}^{I_{k}}U_{k}\left[s_{ k},:\right]^{\top}U_{k}\left[s_{k},:\right],G_{>k}\rangle\] \[=\langle h_{<k}h_{<k}^{\top},G_{k},G_{>k}\rangle.\]

For \(k=1\), we have \(h_{<k}=[1,...,1]^{\top}\), so \(C=\langle G_{k},G_{>k}\rangle\). Then \(C\) is the sum of all leverage scores, which is known to be the rank of \(A\)[30]. Since \(A\) was assumed nonzero, \(C\neq 0\). For \(k>1\), assume that the conditioning event \(\hat{s}_{<k}=s_{<k}\) occurs with nonzero probability. This is a reasonable assumption, since our sampling algorithm will never select prior values \(\hat{s}_{1},...,\hat{s}_{k-1}\) that have 0 probability of occurrence. Let \(\tilde{C}\) be the normalization constant for the conditional distribution on \(\hat{s}_{k-1}\). Then we have

\[0 <p(\hat{s}_{k-1}=s_{k-1}\mid\hat{s}_{<k-1}=s_{<k-1})\] (17) \[=\tilde{C}^{-1}\langle h_{<k-1}h_{<k-1}^{\top},U_{k-1}\left[s_{k- 1},:\right]^{\top}U_{k-1}\left[s_{k-1},:\right],G_{>k-1}\rangle\] \[=\tilde{C}^{-1}\langle h_{<k}h_{<k}^{\top},G_{>k-1}\rangle\] \[=\tilde{C}^{-1}\langle h_{<k}h_{<k}^{\top},G_{k}\cstsa_{>k}\rangle\] \[=\tilde{C}^{-1}\langle h_{<k}h_{<k}^{\top},G_{k},G_{>k}\rangle\] \[=\tilde{C}^{-1}C\]

Since \(\tilde{C}>0\), we must have \(C>0\). 

### Proof of Lemma 3.2

We detail the construction procedure, sampling procedure, and correctness of our proposed data structure. Recall that \(T_{I,F}\) denotes the collection of nodes in a full, complete binary tree with \(\lceil I/F\rceil\) leaves. Each leaf \(v\in T_{I,F}\) holds a segment \(S(v)=\{S_{0}(v)..S_{1}(v)\}\subseteq\{1..I\}\), with \(|S(v)|\leq F\) and \(S(u)\cap S(v)=\varnothing\) for distinct leaves \(u,v\). For each internal node \(v\), \(S(v)=S(L(v))\cup S(R(v))\), where \(L(v)\) and \(R(v)\) denote the left and right children of node \(v\). The root node \(r\) satisfies \(S(r)=\{1..I\}\).

**Construction:** Algorithm 3 gives the procedure to build the data structure. We initialize a segment tree \(T_{I,F}\) and compute \(G^{v}\) for all leaf nodes \(v\in T_{I,F}\) as a sum of outer products of rows from \(U\) (lines 1-3). Starting at the level above the leaves, we then compute \(G^{v}\) for each internal node as the sum of \(G^{L(v)}\) and \(G^{R(v)}\), the partial Gram matrices of its two children. Runtime \(O(IR^{2})\) is required to compute \(I\) outer products across all iterations of the loop on line 3. Our segment tree has \(\lceil I/F\rceil-1\) internal nodes, and the addition in line 6 contributes runtime \(O(R^{2})\) for each internal node. This adds complexity \(O(R^{2}(\lceil I/F\rceil-1))\leq O(IR^{2})\), for total construction time \(O(IR^{2})\).

To analyze the space complexity, observe that we store a matrix \(G^{v}\in\mathbb{R}^{R\times R}\) at all \(2\lceil I/F\rceil-1\) nodes of the segment tree, for asymptotic space usage \(O(\lceil I/F\rceil R^{2})\). We can cut the space usage in half by only storing \(G^{v}\) when \(v\) is either the root or a left child in our tree, since the sampling procedure in Algorithm 4 only accesses the partial Gram matrix stored by left children. We can cut the space usage in half again by only storing the upper triangle of each symmetric matrix \(G^{v}\). Finally, in the special case that \(I<F\), the segment tree has depth 1 and the initial binary search can be eliminated entirely. As a result, the data structure has \(O(1)\) space overhead, since we can avoid storing any partial Gram matrices \(G^{v}\). This proves the complexity claims in point 1 of Lemma 3.2.

```
1:Build tree \(T_{I,F}\) with depth \(d=\lceil\log\lceil I/F\rceil\rceil\)
2:for\(v\in\text{leaves}(T_{I,F})\)do
3:\(G^{v}:=\sum_{i\in S(v)}U\left[i,:\right]^{\top}U\left[i,:\right]\)
4:for\(u=d-2...0\)do
5:for\(v\in\text{level}(T_{I,F},u)\)do
6:\(G^{v}:=G^{L(v)}+G^{R(v)}\) ```

**Algorithm 3** BuildSampler(\(U\in\mathbb{R}^{I\times R}\), \(F\), \(Y\))

**Sampling:** Algorithm 4 gives the procedure to draw a sample from our proposed data structure. It is easy to verify that the normalization constant \(C\) for \(q_{h,U,Y}\) is \(\langle hh^{\top},G^{\text{root}(T_{I,F})},Y\rangle\), since \(G^{\text{root}(T_{I,F})}=U^{\top}U\). Lines 8 and 9 initialize a pair of templated procedures \(\tilde{m}\) and \(\tilde{q}\), each of which accepts a node from the segment tree. The former is used to compute the branching threshold at each internal node, and the latter returns the probability vector \(q_{h,U,Y}\left[S_{0}(v):S_{1}(v)\right]\) for the segment \(\{S_{0}(v)..S_{1}(v)\}\) maintained by a leaf node. To see this last fact, observe for \(i\in[I]\) that

\[\begin{split}&\tilde{q}(v)\left[i-S_{0}(v)\right]\\ &=C^{-1}U\left[i,:\right]\cdot\left(hh^{\top}\righre Y\right) \cdot U\left[i,:\right]^{\top}\\ &=C^{-1}\langle hh^{\top},U\left[i,:\right]^{\top}U\left[i,: \right],Y\rangle\\ &=q_{h,U,Y}\left[i\right].\end{split}\] (18)

The loop on line 12 performs the binary search using the two templated procedures. Line 18 uses the procedure \(\tilde{q}\) to scan through at most \(F\) bin endpoints after the binary search finishes early.

The depth of segment tree \(T_{I,F}\) is \(\log\lceil I/F\rceil\). As a result, the runtime of the sampling procedure is dominated by \(\log\lceil I/F\rceil\) evaluations of \(\tilde{m}\) and a single evaluation of \(\tilde{q}\) during the binary search. Each execution of procedure \(\tilde{m}\) requires time \(O(R^{2})\), relying on the partial Gram matrices \(G^{v}\) computed during the construction phase. When \(Y\) is a general p.s.d. matrix, the runtime of \(\tilde{q}\) is \(O(FR^{2})\). This complexity is dominated by the matrix multiplication \(W\cdot(hh^{\top}\righre Y)\) on line 5. In this case, the runtime of the "RowSampler" procedure to draw one sample is \(O(R^{2}\log\lceil I/F\rceil+FR^{2})\), satisfying the complexity claims in point 2 of the lemma.

Now suppose \(Y\) is a rank-1 matrix with \(Y=uu^{\top}\) for some vector \(u\). We have \(hh^{\top}\righre Y=(h\righre u)(h\righre u)^{\top}\). This gives

\[\tilde{q}_{p}(h,C,v)=\text{diag}(W\cdot(hh^{\top}\righre uu^{\top})\cdot W)=(W \cdot(h\righre u))^{2}\]

where the square is elementwise. The runtime of the procedure \(\tilde{q}\) is now dominated by a matrix-vector multiplication that costs time \(O(FR)\). In this case, we have per-sample complexity \(O(R^{2}\log\lceil I/F\rceil+FR)\), matching the complexity claim in point 3 of the lemma.

**Correctness:** Recall that the inversion sampling procedure partitions the interval \([0,1]\) into \(I\) bins, the \(i\)-th bin having width \(q_{h,U,Y}\left[i\right]\). The goal of our procedure is to find the bin that contains the uniform random draw \(d\). Since procedure \(\tilde{m}\) correctly returns the branching threshold (up to the offset "low") given by Equation (7), the loop on line 12 correctly implements a binary search on the list of bin endpoints specified by the vector \(q_{h,U,Y}\). At the end of the loop, \(c\) is a leaf node that maintains a collection \(S(c)\) of bins, one of which contains the random draw \(d\). Since the procedure \(\tilde{q}\) correctly returns probabilities \(q_{h,U,Y}\left[i\right]\) for \(i\in S(c)\) for leaf node \(c\), (see Equation (18)), line 18 finds the bin that contains the random draw \(d\). The correctness of the procedure follows from the correctness of inversion sampling [22]. 

### Cohesive Proof of Theorem 1.1

In this proof, we fully explain Algorithms 1 and 2 in the context of the sampling procedure outlined in section 3.2. We verify the complexity claims first and then prove correctness.

**Construction and Update:** For each matrix \(U_{j}\), Algorithm 1 builds an efficient row sampling data structure \(Z_{j}\) as specified by Lemma 3.2. We let the p.s.d. matrix \(Y\) that parameterizes each sampler be a matrix of ones, and we set \(F=R\). From Lemma 3.2, the time to construct sampler \(Z_{j}\) is \(O(I_{j}R^{2})\). The space used by sampler \(Z_{j}\) is \(O(\lceil I_{j}/F\rceil R^{2})=O(I_{j}R)\), since \(F=R\). In case \(I_{j}<R\), we use the special case described in Appendix A.5 to get a space overhead \(O(1)\), avoiding a term \(O(R^{2})\) in the space complexity.

Summing the time and space complexities over all \(j\) proves part 1 of the theorem. To update the data structure if matrix \(U_{j}\) changes, we only need to rebuild sampler \(Z_{j}\) for a cost of \(O(I_{j}R^{2})\). The construction phase also computes and stores the Gram matrix \(G_{j}\) for each matrix \(U_{j}\). We defer the update procedure in case a single entry of matrix \(U_{j}\) changes to Appendix A.7.

**Sampling:** For all indices \(k\) (except possibly \(j\)), lines 1-5 from Algorithm 2 compute \(G_{>k}\) and its eigendecomposition. Only a single pass over the Gram matrices \(G_{k}\) is needed, so these steps cost \(O(R^{3})\) for each index \(k\). Line 5 builds an efficient row sampler \(E_{k}\) for the matrix of scaled eigenvectors \(\sqrt{\Lambda_{k}}\cdot V_{k}\). For sampler \(k\), we set \(Y=G_{k}\) with cutoff parameter \(F=1\). From Lemma 3.2, the construction cost is \(O(R^{3})\) for each index \(k\), and the space required by each sampler is \(O(R^{3})\). Summing these quantities over all \(k\neq j\) gives asymptotic runtime \(O(NR^{3})\) for lines 2-5.

The loop spanning lines 6-12 draws \(J\) row indices from the Khatri-Rao product \(U_{\neq j}\). For each sample, we maintain a "history vector" \(h\) to write the variables \(h_{<k}\) from Equation (3). For each index \(k\neq j\), we draw random variable \(\hat{u}_{k}\) using the row sampler \(E_{k}\). This random draw indexes a scaled eigenvector of \(G_{>k}\). We then use the history vector \(h\) multiplied by the eigenvector to sample a row index \(\hat{t}_{k}\) using data structure \(Z_{k}\). The history vector \(h\) is updated, and we proceed to draw the next index \(\hat{t}_{k}\).

As written, lines 2-5 also incur scratch space usage \(O(NR^{3})\). The scratch space can be reduced to \(O(R^{3})\) by exchanging the order of loops on line 6 and line 8 and allocating \(J\) separate history vectors \(h\), once for each draw. Under this reordering, we perform all \(J\) draws for each variable \(\hat{u}_{k}\) and \(\hat{t}_{k}\) before moving to \(\hat{u}_{k+1}\) and \(\hat{t}_{k+1}\). In this case, only a single data structure \(E_{k}\) is required at each iteration of the outer loop, and we can avoid building all the structures in advance on line 5. We keep the algorithm in the form written for simplicity, but we implemented the memory-saving approach in our code.

From Lemma 3.2, lines 9 and 10 cost \(O(R^{2}\log R)\) and \(O\left(R^{2}\log\lceil I_{k}/R\rceil\right)\), respectively. Line 11 costs \(O(R)\) and contributes a lower-order term. Summing over all \(k\neq j\), the runtime to draw a single sample is

\[O\left(\sum_{k\neq j}(R^{2}\log\lceil I_{k}/R\rceil+R^{2}\log R)\right)=O \left(\sum_{k\neq j}R^{2}\log\max\left(I_{k},R\right)\right).\]

Adding the runtime for all \(J\) samples to the runtime of the loop spanning lines 2-6 gives runtime \(O\left(NR^{3}+J\sum_{k\neq j}R^{2}\log\max\left(I_{k},R\right)\right)\), and the complexity claims have been proven.

**Correctness:** We show correctness for the case where \(j=-1\) and we sample from the Khatri-Rao product of all matrices \(U_{k}\), since the proof for any other value of \(j\) requires a simple reindexing of matrices. To show that our sampler is correct, it is enough to prove the condition that for \(1\leq k\leq N\),

\[p(\hat{t}_{k}=t_{k}\mid h_{<k})=q_{h_{<k},U_{k},G_{>k}}\left[t_{k}\right],\] (19)

since, by Theorem 3.1, \(p(\hat{s}_{k}=s_{k}\mid\hat{s}_{<k}=s_{<k})=q_{h_{<k},U_{k},G_{>k}}\left[s_{k}\right]\). This would imply that the joint random variable \((\hat{t}_{1},...,\hat{t}_{N})\) has the same probability distribution as \((\hat{s}_{1},...,\hat{s}_{N})\), which by definition follows the leverage score distribution on \(U_{1}\odot...\odot U_{N}\). To prove the condition in Equation (19), we apply Equations (9) and (11) derived earlier:

\[p(\hat{t}_{k}=t_{k}\mid h_{<k})\] \[\quad=\sum_{u_{k}=1}^{R}p(\hat{t}_{k}=t_{k}\mid\hat{u}_{k}=u_{k}, h_{<k})p(\hat{u}_{k}=u_{k}\mid h_{<k})\] (Bayes' Rule) \[\quad=\sum_{u_{k}=1}^{R}w\left[u_{k}\right]\frac{W\left[t_{k},u_ {k}\right]}{\left\|W\left[:,u_{k}\right]\right\|_{1}}\] (Equations ( 9 ) and ( 11 ), in reverse) \[\quad=q_{h_{<k},U_{k},G_{>k}}\left[t_{k}\right].\] (20)

### Efficient Single-Element Updates

Applications such as CP decomposition typically change all entries of a single matrix \(U_{j}\) between iterations, incurring an update cost \(O(I_{j}R^{2})\) for our data structure from Theorem 1.1. In case only a single element of \(U_{j}\) changes, our data structure can be updated in time \(O\left(R\log I_{j}\right)\).

Proof.: Algorithm 5 gives the procedure when the update \(U_{j}\left[r,c\right]:=\hat{u}\) is performed. The matrices \(G^{v}\) refer to the partial Gram matrices maintained by each node \(v\) of the segment trees in our data structure, and the matrix \(\tilde{U}_{j}\) refers to the matrix \(U_{j}\) before the update operation.

Let \(T_{I_{j},R}\) be the segment tree corresponding to matrix \(U_{j}\) in the data structure, and let \(v\in T_{I_{j},R}\) be the leaf whose segment contains \(r\). Lines 3-5 of the algorithm update the row and column indexed by \(c\) in the partial Gram matrix held by the leaf node.

The only other nodes requiring an update are ancestors of \(v\), each holding a partial Gram matrix that is the sum of its two children. Starting from the direct parent \(A(v)\), the loop on line 6 performs these ancestor updates. The addition on line 8 only requires time \(O(R)\), since only row and column \(c\) change between the old value of \(G^{v}\) and its updated version. Thus, the runtime of this procedure is \(O(R\log\lceil I_{j}/R\rceil)\) from multiplying the cost to update a single node by the depth of the tree.

### Extension to Sparse Input Matrices

Our data structure is designed to sample from Khatri-Rao products \(U_{1}\odot...\odot U_{N}\) where the input matrices \(U_{1},...,U_{N}\) are dense, a typical situation in tensor decomposition. Slight modifications to the construction procedure permit our data structure to handle sparse matrices efficiently as well. The following corollary states the result as a modification to Theorem 1.1.

**Corollary A.1** (Sparse Input Modification).: _When input matrices \(U_{1},...,U_{N}\) are sparse, point 1 of Theorem 1.1 can be modified so that the proposed data structure has \(O\left(R\sum_{j=1}^{N}\text{nnz}(U_{j})\right)\) construction time and \(O\left(\sum_{j=1}^{N}\text{nnz}(U_{j})\right)\) storage space. The sampling time and scratch space usage in point 2 of Theorem 1.1 does not change. The single-element update time in point 1 is likewise unchanged._

Proof.: We will modify the data structure in Lemma 3.2. The changes to its construction and storage costs will propagate to our Khatri-Rao product sampler, which maintains one of these data structures for each input matrix.

Let us restrict ourselves to the case \(F=R,Y=[1]\) in relation to the data structure in Lemma 3.2. These choices for \(F\) and \(Y\) are used in the construction phase given by Algorithm 1. The proof in Appendix A.5 constrains each leaf \(v\) of a segment tree \(T_{I,F}\) to hold a contiguous segment \(S(v)\subseteq[I]\) of cardinality at most \(F\). Instead, choose each segment \(S(v)=\{S_{0}(v)..S_{1}(v)\}\) so that \(U\left[S_{0}(v):S_{1}(v),:\right]\) has at most \(R^{2}\) nonzeros, and the leaf count of the tree is at most \(\lceil\text{nnz}(U)/R^{2}\rceil+1\) for input matrix \(U\in\mathbb{R}^{I\times R}\). Assuming the nonzeros of \(U\) are sorted in row-major order, we can construct such a partition of \([I]\) into segments in time \(O(\text{nnz}(U))\) by iterating in order through the nonzero rows and adding each of them to a "current" segment. We shift to a new segment when the current segment cannot hold any more nonzeros.

This completes the modification to the data structure in Lemma 3.2, and we now analyze its updated time / space complexity.

**Updated Construction / Update Complexity of Lemma 3.2, \(F=R,Y=[1]\)**: Algorithm 3 constructs the partial Gram matrix for each leaf node \(v\) in the segment tree. Each nonzero in the segment \(U\left[S_{0}(v):S_{1}(v),:\right]\) contributes time \(O(R)\) during line 3 of Algorithm 3 to update a single row and column of \(G^{v}\). Summed over all leaves, the cost of line 3 is \(O(\text{nnz}(U)R)\). The remainder of the construction procedure updates the partial Gram matrices of all internal nodes. Since there are at most \(O\left(\lceil\text{nnz}(U)/R^{2}\rceil\right)\) internal nodes and the addition on line 6 costs \(O(R^{2})\) per node, the remaining steps of the construction procedure cost \(O(\text{nnz}(U))\), a lower-order term. The construction time is therefore \(O(\text{nnz}(U)R)\).

Since we store a single partial Gram matrix of size \(R^{2}\) at each of \(O\left(\lceil\text{nnz}(U)/R^{2}\rceil\right)\) internal nodes, the space complexity of our modified data structure is \(O(\text{nnz}(U))\).

Finally, the data structure update time in case a single element of \(U\) is modified does not change from Theorem 1.1. Since the depth of the segment tree \(\lceil\text{nnz}(U)/R^{2}\rceil+1\) is upper-bounded by \(\lceil I/R\rceil+1\), the runtime of the update procedure in Algorithm 5 stays the same.

**Updated Sampling Complexity of Lemma 3.2, \(F=R,Y=[1]\)**: The procedure "RowSample" in Algorithm 4 now conducts a traversal of a tree of depth \(O(\lceil\text{nnz}(U)/R^{2}\rceil)\). As a result, wecan still upper-bound the number of calls to procedure \(\tilde{m}\) as \(\lceil I/F\rceil\). The runtime of procedure \(\tilde{m}\) is unchanged. The runtime of procedure \(\tilde{q}\) for leaf node \(c\) is dominated by the matrix-vector multiplication \(U\left[S_{0}(c):S_{1}(c),:\right]\cdot h\). This runtime is \(O\left(\text{nnz}\left(U\left[S_{0}(c):S_{1}(c),:\right]\right)\right)\leq O \left(R^{2}\right)\). Putting these facts together, the sampling complexity of the data structure in Lemma 3.2 does not change under our proposed modifications for \(F=R,Y=[1]\).

Updated Construction Complexity of Theorem 1.1: Algorithm 1 now requires \(O\left(R\sum_{j=1}^{N}\text{nnz}(U_{j})\right)\) construction time and \(O\left(\sum_{j=1}^{N}\text{nnz}(U_{j})\right)\) storage space, summing the costs for the updated structure from Lemma 3.2 over all matrices \(U_{1},...,U_{N}\). The sampling complexity of these data structures is unaffected by the modifications, which completes the proof of the corollary. 

### Alternating Least Squares CP Decomposition

CP Decomposition.CP decomposition represents an \(N\)-dimensional tensor \(\tilde{\mathcal{T}}\in\mathbb{R}^{I_{1}\times...\times I_{n}}\) as a weighted sum of generalized outer products. Formally, let \(U_{1},...,U_{N}\) with \(U_{j}\in\mathbb{R}^{I_{j}\times R}\) be factor matrices with each column having unit norm, and let \(\sigma\in\mathbb{R}^{R}\) be a nonnegative coefficient vector. We call \(R\) the rank of the decomposition. The tensor \(\tilde{\mathcal{T}}\) that the decomposition represents is given elementwise by

\[\tilde{\mathcal{T}}\left[i_{1},...,i_{N}\right]:=\langle\sigma^{\top},U_{1} \left[i_{1},:\right],...,U_{N}\left[i_{N},:\right]\rangle=\sum_{r=1}^{R}\sigma \left[r\right]U_{1}[i_{1},r]\cdots U_{N}[i_{N},r],\]

which is a generalized inner product between \(\sigma^{\top}\) and rows \(U_{j}\left[i_{j},:\right]\) for \(1\leq j\leq N\). Given an input tensor \(\mathcal{T}\) and a target rank \(R\), the goal of approximate CP decomposition is to find a rank-\(R\) representation \(\tilde{\mathcal{T}}\) that minimizes the Frobenius norm \(\left\|\mathcal{T}-\tilde{\mathcal{T}}\right\|_{F}\).

Definition of Matricization.The matricization \(\text{mat}(\mathcal{T},j)\) flattens tensor \(\mathcal{T}\in\mathbb{R}^{I_{1}\times...\times I_{N}}\) into a matrix and isolates mode \(j\) along the row axis of the output. The output of matricization has dimensions \(I_{j}\times\prod_{k\neq j}I_{k}\). We take the formal definition below from a survey by Kolda and Bader [13]. The tensor entry \(\mathcal{T}\left[i_{1},...,i_{N}\right]\) is equal to the matricization entry \(\text{mat}(\mathcal{T},j)\left[i_{N},u\right]\), where

\[u=1+\sum_{\begin{subarray}{c}k=1\\ k\neq j\end{subarray}}^{N}(i_{k}-1)\prod_{\begin{subarray}{c}m=1\\ m\neq j\end{subarray}}^{k-1}I_{m}.\]

Details about Alternating Least Squares.Let \(U_{1},...,U_{N}\) be factor matrices of a low-rank CP decomposition, \(U_{k}\in\mathbb{R}^{I_{k}\times R}\). We use \(U_{\neq j}\) to denote \(\bigodot_{k=N,k\neq j}^{k=1}U_{k}\). Note the inversion of order here to match indexing in the definition of matricization above. Algorithm 6 gives the non-randomized alternating least squares algorithm CP-ALS that produces a decomposition of target rank \(R\) given input tensor \(\mathcal{T}\in\mathbb{R}^{I_{1}\times...\times I_{N}}\) in general format. The random initialization on line 1 of the algorithm can be implemented by drawing each entry of the factor matrices \(U_{j}\) according to a standard normal distribution, or via a randomized range finder [11]. The vector \(\sigma\) stores the generalized singular values of the decomposition. At iteration \(j\) within a round, ALS holds all factor matrices except \(U_{j}\) constant and solves a linear-least squares problem on line 6 for a new value for \(U_{j}\). In between least squares solves, the algorithm renormalizes the columns of each matrix \(U_{j}\) to unit norm and stores their original norms in the vector \(\sigma\). Appendix A.11 contains more details about the randomized range finder and the convergence criteria used to halt iteration.

We obtain a randomized algorithm for sparse tensor CP decomposition by replacing the exact least squares solve on line 6 with a randomized method according to Theorem 2.1. Below, we prove Corollary 3.3, which derives the complexity of the randomized CP decomposition algorithm.

Proof of Corollary 3.3.: The design matrix \(U_{\neq j}\) for optimization problem \(j\) within a round of ALS has dimensions \(\prod_{k\neq j}I_{k}\times R\). The observation matrix \(\text{mat}(\mathcal{T},j)^{\top}\) has dimensions \(\prod_{k\neq j}I_{k}\times I_{j}\). To achieve error threshold \(1+\varepsilon\) with probability \(1-\delta\) on each solve, we draw \(J=\tilde{O}\left(R/(\varepsilon\delta)\right)\) rows from both the design and observation matrices and solve the downsampled problem (Theorem 2.1).

These rows are sampled according to the leverage score distribution on the rows of \(U_{\neq j}\), for which we use the data structure in Theorem 1.1. After a one-time initialization cost \(O(\sum_{j=1}^{N}I_{j}R^{2}))\) before the ALS iteration begins, the complexity to draw \(J\) samples (assuming \(I_{j}\geq R\)) is

\[O\left(NR^{3}+J\sum_{k\neq j}R^{2}\log I_{k}\right)=\tilde{O}\left(NR^{3}+ \frac{R}{\varepsilon\delta}\sum_{k\neq j}R^{2}\log I_{k}\right).\]

The cost to assemble the corresponding subset of the observation matrix is \(O(JI_{j})=\tilde{O}(RI_{j}/(\varepsilon\delta))\). The cost to solve the downsampled least squares problem is \(O(JR^{2})=\tilde{O}(I_{j}R^{2}/(\varepsilon\delta))\), which dominates the cost of forming the subset of the observation matrix. Finally, we require additional time \(O(I_{j}R^{2})\) to update the sampling data structure (Theorem 1.1 part 1). Adding these terms together and summing over \(1\leq j\leq N\) gives

\[\tilde{O}\left(\frac{1}{\varepsilon\delta}\cdot\sum_{j=1}^{N} \left[I_{j}R^{2}+\sum_{k\neq j}R^{3}\log I_{k}\right]\right)\] (21) \[=\tilde{O}\left(\frac{1}{\varepsilon\delta}\cdot\sum_{j=1}^{N} \left[I_{j}R^{2}+(N-1)R^{3}\log I_{j}\right]\right).\]

Rounding \(N-1\) to \(N\) and multiplying by the number of iterations gives the desired complexity. When \(I_{j}<R\) for any \(j\), the complexity changes in Theorem 1.1 propagate to the equation above. The column renormalization on line 8 of the CP-ALS algorithm contributes additional time \(O\left(\sum_{j=1}^{N}I_{j}R\right)\) per round, a lower-order term.

### Experimental Platform and Sampler Parallelism

We provide two implementations of our sampler. The first is a slow reference implementation written entirely in Python, which closely mimics our pseudocode and can be used to test correctness. The second is an efficient implementation written in C++, parallelized in shared memory with OpenMP and Intel Thread Building Blocks.

Each Perlmutter CPU node (our experimental platform) is equipped with two sockets, each containing an AMD EPYC 7763 processor with 64 cores. All benchmarks were conducted with our efficient C++ implementation using 128 OpenMP threads. We link our code against Intel Thread Building blocks to call a multithreaded sort function when decomposing sparse tensors. We use OpenBLAS 0.3.21 to handle linear algebra with OpenMP parallelism enabled, but our code links against any linear algebra library implementing the CBLAS and LAPACKE interfaces.

Our proposed data structure samples from the exact distribution of leverage scores of the Khatri-Rao product, thereby enjoying better sample efficiency than alternative approaches such as CP-ARLS-LEV [15]. The cost to draw each sample, however, is \(O(R^{2}\log H)\), where \(H\) is the number of rows in the Khatri-Rao product. Methods such as row-norm-squared sampling or CP-ARLS-LEV can draw each sample in time \(O(\log H)\) after appropriate preprocessing. Therefore, efficient parallelization of our sampling procedure is required for competitive performance, and we present two strategies below.

1. **Asynchronous Thread Parallelism**: The KRPSampleDraw procedure in Algorithm 2 can be called by multiple threads concurrently without data races. The simplest parallelization strategy divides the \(J\) samples equally among the threads in a team, each of which makes calls to KRPSampleDraw asynchronously. This strategy works well on a CPU, but is less attractive on a SIMT processor like a GPU where instruction streams cannot diverge without significant performance penalties.
2. **Synchronous Batch Parallelism** As an alternative to the asynchronous strategy, suppose for the moment that all leaves have the same depth in each segment tree. Then for every sample, STSample makes a sequence of calls to \(\tilde{m}\), each updating the current node by branching left or right in the tree. The length of this sequence is the depth of the tree, and it is followed by a single call to the function \(\tilde{q}\). Observe that procedure \(\tilde{m}\) in Algorithm 4 can be computed with a matrix-vector multiplication followed by a dot product. The procedure \(\tilde{q}\) of Algorithm 4 requires the same two operations if \(F=1\) or \(Y=[1]\). Thus, we can create a batched version of our sampling procedure that makes a fixed length sequence of calls to batched gemv and dot routines. All processors march in lock-step down the levels of each segment tree, each tracking the branching paths of a distinct set of samples. The MAGMA linear algebra library provides a batched version of gemv[10], while a batched dot product can be implemented with an ad hoc kernel. MAGMA also offers a batched version of the symmetric rank-\(k\) update routine syrk, which is helpful to parallelize row sampler construction (Algorithm 3). When all leaves in the tree are not at the same level, the the bottom level of the tree can be handled with a special sequence of instructions making the required additional calls to \(\tilde{m}\).

Our CPU code follows the batch synchronous design pattern. To avoid dependency on GPU-based MAGMA routines in our CPU prototype, portions of the code that should be batched BLAS calls are standard BLAS calls wrapped in a for loop. These sections can be easily replaced when the appropriate batched routines are available.

### Sparse Tensor CP Experimental Configuration

Table 3 lists the nonzero counts and dimensions of sparse tensors in our experiments [25]. We took the log of all values in the Enron, NELL-2, and Reddit-2015 tensors. Consistent with established practice, this operation damps the effect of a few high magnitude tensor entries on the fit metric [15].

The factor matrices for the Uber, Amazon, NELL-2, and Reddit experiments were initialized with i.i.d. entries from the standard normal distribution. As suggested by Larsen and Kolda [15], the Enron tensor's factors were initialized with a randomized range finder [11]. The range finder algorithm initializes each factor matrix \(U_{j}\) as \(\text{mat}(\mathcal{T},j)S\), a sketch applied to the mode-\(j\) matricization of \(\mathcal{T}\) with \(S\in\mathbb{R}^{\prod_{k\neq j}I_{k}\times R}\). Larsen and Kolda chose \(S\) as a sparse sampling matrix to select a random subset of fibers along each mode. We instead used an i.i.d. Gaussian sketching matrix that was not materialized explicitly. Instead, we exploited the sparsity of \(\mathcal{T}\) and noted that at most \(\text{nnz}\left(\mathcal{T}\right)\) columns of \(\text{mat}(\mathcal{T},j)\) were nonzero. Thus, we computed at most \(\text{nnz}\left(\mathcal{T}\right)\) rows of the random sketching matrix \(S\), which were lazily generated and discarded during the matrix multiplication without incurring excessive memory overhead.

ALS was run for a maximum of 40 rounds on all tensors except for Reddit, which was run for 80 rounds. The exact fit was computed every 5 rounds (defined as 1 epoch), and we used an early

\begin{table}
\begin{tabular}{l r r r r} \hline \hline Tensor & Dimensions & Nonzeros & Prep. & Init. \\ \hline Uber Pickups & 183 \(\times\) 24 \(\times\) 1,140 \(\times\) 1,717 & 3,309,490 & None & IID \\ Enron Emails & 6,066 \(\times\) 5,699 \(\times\) 244,268 \(\times\) 1,176 & 54,202,099 & log & RRF \\ NELL-2 & 12,092 \(\times\) 9,184 \(\times\) 28,818 & 76,879,419 & log & IID \\ Amazon Reviews & 4,821,207 \(\times\) 1,774,269 \(\times\) 1,805,187 & 1,741,809,018 & None & IID \\ Reddit-2015 & 8,211,298 \(\times\) 176,962 \(\times\) 8,116,559 & 4,687,474,081 & log & IID \\ \hline \hline \end{tabular}
\end{table}
Table 3: Sparse Tensors from FROSTT collection.

stopping condition to terminate runs before the maximum round count. The algorithm was terminated at epoch \(T\) if the maximum fit in the last 3 epochs did not exceed the maximum fit from epoch 1 through epoch \(T-3\) by tolerance \(10^{-4}\).

Hybrid CP-ARLS-LEV deterministically includes rows from the Khatri-Rao product whose probabilities exceed a threshold \(\tau\). The ostensible goal of this procedure is to improve diversity in sample selection, as CP-ARLS-LEV may suffer from many repeat draws of high probability rows. We replicated the conditions proposed in the original work by selecting \(\tau=1/J\)[15].

Individual trials of non-randomized (exact) ALS on the Amazon and Reddit tensors required several hours on a single Perlmutter CPU node. To speed up our experiments, accuracy measurements for exact ALS in Figure 3 were carried out using multi-node SPLATT, The Surprisingly ParalleL spArse Tensor Toolkit [24], on four Perlmutter CPU nodes. The fits computed by SPLATT agree with those computed by our own non-randomized ALS implementation. As a result, Figure 3 verifies that our randomized algorithm STS-CP produces tensor decompositions with accuracy comparable to those by highly-optimized, state-of-the-art CP decomposition software. We leave a distributed-memory implementation of our _randomized_ algorithms to future work.

### Efficient Computation of Sketch Distortion

The definition of \(\sigma\) in this section is different from its definition in the rest of this work. The condition number \(\kappa\) of a matrix \(M\) is defined as

\[\kappa(M):=\frac{\sigma_{\text{max}}(M)}{\sigma_{\text{min}}(M)}\]

where \(\sigma_{\text{min}}(M)\) and \(\sigma_{\text{max}}(M)\) denote the minimum and maximum nonzero singular values of \(M\). Let \(A\) be a Khatri-Rao product of \(N\) matrices \(U_{1},...,U_{N}\) with \(\prod_{j=1}^{N}I_{j}\) rows, \(R\) columns, and rank \(r\leq R\). Let \(A=Q\Sigma V^{\top}\) be its reduced singular value decomposition with \(Q\in\mathbb{R}^{\prod_{j}I_{j}\times r},\Sigma\in\mathbb{R}^{r\times r}\), and \(V\in\mathbb{R}^{r\times R}\). Finally, let \(S\in\mathbb{R}^{J\times\prod_{j}I_{j}}\) be a leverage score sampling matrix for \(A\). Our goal is to compute \(\kappa(SQ)\) without fully materializing either \(A\) or its QR decomposition. We derive

\[\kappa(SQ) =\kappa(SQ\Sigma V^{\top}\Sigma^{-1})\] (22) \[=\kappa(SAV\Sigma^{-1})\]

The matrix \(SA\in\mathbb{R}^{J\times R}\) is efficiently computable using our leverage score sampling data structure. We require time \(O(JR^{2})\) to multiply it by \(V\Sigma^{-1}\) and compute the singular value decomposition of the product to get the condition number. Next observe that \(A^{\top}A=V\Sigma^{2}V^{\top}\), so we can recover \(V\) and \(\Sigma^{-1}\) by eigendecomposition of \(A^{\top}A\in\mathbb{R}^{R\times R}\) in time \(O(R^{3})\). Finally, recall the formula

\[A^{\top}A=\bigotimes_{j=1}^{N}U_{j}^{\top}U_{j}\]

used at the beginning of Section 3 that enables computation of \(A^{\top}A\) in time \(O\left(\sum_{j=1}^{N}I_{j}R^{2}\right)\) without materializing the full Khatri-Rao product. Excluding the time to form \(SA\) (which is given by Theorem 1.1), \(\kappa(SQ)\) is computable in time

\[O\left(JR^{2}+R^{3}+\sum_{j=1}^{N}I_{j}R^{2}\right).\]

Plugging \(\kappa(SQ)\) into Equation (12) gives an efficient method to compute the distortion.

### Supplementary Results

#### a.13.1 Comparison Against Standard CP Decomposition Packages

Table 4 compares the runtime per ALS round for our algorithm against existing common software packages for sparse tensor CP decomposition. We compared our algorithm against Tensorly version 0.81 [14] and Matlab Tensor Toolbox version 3.5 [3]. We compared our algorithm against both non-randomized ALS and a version of CP-ARLS-LEV in Tensor Toolbox.

As demonstrated by Table 4, our implementation exhibits more than 1000x speedup over Tensorly and 295x over Tensor Toolbox (non-randomized) for the NELL-2 tensor. STS-CP enjoys a dramatic speedup over Tensorly because the latter explicitly materializes the Khatri-Rao product, which is prohibitively expensive given the large tensor dimensions (see Table 3).

STS-CP consistently exhibits at least 2.5x speedup over the version of CP-ARLS-LEV in Tensor Toolbox, with more than 10x speedup on the Amazon tensor. To ensure a fair comparison with CP-ARLS-LEV, we wrote an improved implementation in C++ that was used for all other experiments.

#### a.13.2 Probability Distribution Comparison

Figure 7 provides confirmation on a small test problem that our sampler works as expected. For the Khatri-Rao product of three matrices \(A=U_{1}\odot U_{2}\odot U_{3}\), it plots the true distribution of leverage scores against a normalized histogram of 50,000 draws from the data structure in Theorem 1.1. We choose \(U_{1},U_{2},U_{3}\in\mathbb{R}^{8\times 8}\) initialized i.i.d. from a standard normal distribution with 1% of all entries multiplied by 10. We observe excellent agreement between the histogram and the true distribution.

#### a.13.3 Fits Achieved for \(J=2^{16}\)

Table 5 gives the fits achieved for sparse tensor decomposition for varying rank and algorithm (presented graphically in Figure 4). Uncertainties are one standard deviation across 8 runs of ALS.

#### a.13.4 Fit as a Function of Time

Figures 7(a) and 7(b) shows the fit as a function of time for the Amazon Reviews and NELL2 tensors. The hybrid version of CP-ARLS-LEV was used for comparison in both experiments. As in section 4.3, thick lines are averages of the running max fit across 4 ALS trials, shown by the thin dotted lines. For Amazon, the STS-CP algorithm makes faster progress than CP-ARLS-LEV at all tested sample counts.

For the NELL-2 tensor, STS-CP makes slower progress than CP-ARLS-LEV for sample counts up to \(J=163,840\). On average, these trials with CP-ARLS-LEV do not achieve the same final fit as STS-CP. CP-ARLS-LEV finally achieves a comparable fit to STS-CP when the former uses \(J=196,608\) samples, compared to \(J=65,536\) for our method.

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline Method & Uber & Enron & NELL-2 & Amazon & Reddit \\ \hline Tensorly, Sparse Backend & 64.2 & OOM & 759.6 & OOM & OOM \\ Matlab Ttoolbox Standard & 11.6 & 294.4 & 177.4 & \textgreater{}3600 & OOM \\ Matlab Ttoolbox CP-ARLS-LEV & 0.5 & 1.4 & 1.9 & 34.2 & OOM \\
**STS-CP (ours)** & **0.2** & **0.5** & **0.6** & **3.4** & **26.0** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Average time (seconds) per ALS round for our method vs. standard CP decomposition packages. OOM indicates an out-of-memory error. All experiments were conducted on a single LBNL Perlmutter CPU node. Randomized algorithms were benchmarked with \(2^{16}\) samples per least-squares solve.

Figure 7: Comparison of true leverage score distribution with histogram of 50,000 samples drawn from \(U_{1}\odot U_{2}\odot U_{3}\).

#### a.13.5 Speedup of STS-CP and Practical Usage Guide

Timing Comparisons.For each tensor, we now compare hybrid CP-ARLS-LEV and STS-CP on the time required to achieve a fixed fraction of the fit achieved by non-randomized ALS. For each tensor and rank in the set \(\{25,50,75,100,125\}\), we ran both algorithms using a range of sample counts. We tested STS-CP on values of \(J\) from the set \(\{2^{15}x\mid 1\leq x\leq 4\}\) for all tensors. CP-ARLS-LEV required a sample count that varied significantly between datasets to hit the required thresholds, and we report the sample counts that we tested in Table 6. Because CP-ARLS-LEV has poorer sample complexity than STS-CP, we tested a wider range of sample counts for the former algorithm.

\begin{table}
\begin{tabular}{c c c c|c} \hline \hline Tensor & \(R\) & CP-ARLS-LEV & CP-ARLS-LEV-H & STS-CP (ours) & Exact \\ \hline \multirow{5}{*}{Uber} & 25 &.187 \(\pm\) 2.30e-03 &.188 \(\pm\) 2.11e-03 & **.189**\(\pm\) 1.52e-03 &.190 \(\pm\) 1.41e-03 \\  & 50 &.211 \(\pm\) 1.72e-03 &.212 \(\pm\) 1.27e-03 & **.216**\(\pm\) 1.18e-03 &.218 \(\pm\) 1.61e-03 \\  & 75 &.218 \(\pm\) 1.76e-03 &.218 \(\pm\) 2.05e-03 & **.230**\(\pm\) 9.24e-04 &.232 \(\pm\) 9.29e-04 \\  & 100 &.217 \(\pm\) 3.15e-03 &.217 \(\pm\) 1.69e-03 & **.237**\(\pm\) 2.12e-03 &.241 \(\pm\) 1.00e-03 \\  & 125 &.213 \(\pm\) 1.96e-03 &.213 \(\pm\) 2.47e-03 & **.243**\(\pm\) 1.78e-03 &.247 \(\pm\) 1.52e-03 \\ \hline \multirow{5}{*}{Enron} & 25 &.0881 \(\pm\) 1.02e-02 &.0882 \(\pm\) 9.01e-03 & **.0955**\(\pm\) 1.19e-02 &.0978 \(\pm\) 8.50e-03 \\  & 50 &.0883 \(\pm\) 1.72e-02 &.0920 \(\pm\) 6.32e-03 & **.125**\(\pm\) 1.03e-02 &.132 \(\pm\) 1.51e-02 \\  & 75 &.0899 \(\pm\) 6.10e-03 &.0885 \(\pm\) 6.39e-03 & **.149**\(\pm\) 1.25e-02 &.157 \(\pm\) 4.87e-03 \\  & 100 &.0809 \(\pm\) 1.26e-02 &.0787 \(\pm\) 1.00e-02 & **.164**\(\pm\) 5.90e-03 &.176 \(\pm\) 4.12e-03 \\  & 125 &.0625 \(\pm\) 1.52e-02 &.0652 \(\pm\) 1.00e-02 & **.182**\(\pm\) 1.04e-02 &.190 \(\pm\) 4.35e-03 \\ \hline \multirow{5}{*}{NELL-2} & 25 &.0465 \(\pm\) 9.52e-04 &.0467 \(\pm\) 4.61e-04 & **.0470**\(\pm\) 4.69e-04 &.0478 \(\pm\) 7.20e-04 \\  & 50 &.0590 \(\pm\) 5.33e-04 &.0593 \(\pm\) 4.34e-04 & **.0608**\(\pm\) 5.44e-04 &.0618 \(\pm\) 4.21e-04 \\  & 75 &.0658 \(\pm\) 6.84e-04 &.0660 \(\pm\) 3.95e-04 & **.0694**\(\pm\) 2.96e-04 &.0708 \(\pm\) 3.11e-04 \\  & 100 &.0700 \(\pm\) 4.91e-04 &.0704 \(\pm\) 4.48e-04 & **.0760**\(\pm\) 6.52e-04 &.0779 \(\pm\) 5.09e-04 \\  & 125 &.0729 \(\pm\) 8.56e-04 &.0733 \(\pm\) 7.22e-04 & **.0814**\(\pm\) 5.03e-04 &.0839 \(\pm\) 8.47e-04 \\ \hline \multirow{5}{*}{Amazon} & 25 &.338 \(\pm\) 6.63e-04 &.339 \(\pm\) 6.99e-04 & **.340**\(\pm\) 6.61e-04 &.340 \(\pm\) 5.78e-04 \\  & 50 &.359 \(\pm\) 1.09e-03 &.360 \(\pm\) 8.04e-04 & **.366**\(\pm\) 7.22e-04 &.366 \(\pm\) 1.01e-03 \\ \cline{1-1}  & 7For each configuration of tensor, target rank \(R\), sampling algorithm, and sample count \(J\), we ran 4 trials using the configuration and stopping criteria in Appendix A.11. The result of each trial was a set of (time, fit) pairs. For each configuration, we linearly interpolated the pairs for each trial and averaged the resulting continuous functions over all trials. The result for each configuration was a function \(f_{\mathcal{T},R,A,J}:\mathbb{R}^{+}\rightarrow[0,1]\). The value \(f_{\mathcal{T},R,A,J}(t)\) is the average fit at time \(t\) achieved by algorithm \(A\) to decompose tensor \(\mathcal{T}\) with target rank \(R\) using \(J\) samples per least squares solve. Finally, let

\[\text{Speedup}_{\mathcal{T},R,M}:=\frac{\min_{J}\text{argmin}_{t\geq 0}\left[f_{ \mathcal{T},R,\text{CP-ARLS-LEV},J}(t)>P\right]}{\min_{J}\text{argmin}_{t\geq 0 }\left[f_{\mathcal{T},R,\text{STS-CP},J}(t)>P\right]}\]

be the speedup of STS-CP to over CP-ARLS-LEV (hybrid) to achieve a threshold fit \(P\) on tensor \(\mathcal{T}\) with target rank \(R\). We let the threshold \(P\) for each tensor \(\mathcal{T}\) be a fixed fraction of the fit achieved by non-randomized ALS (see Table 5).

Figure 9 reports the speedup of STS-CP over hybrid CP-ARLS-LEV for \(P=0.95\) on all tensors except Enron. For large tensors with over one billion nonzeros, we report a significant speedup anywhere from 1.4x to 2.0x for all tested ranks. For smaller tensors with less than 100 million nonzeros, the lower cost of each least squares solve lessens the impact of the expensive, more accurate sample selection phase of STS-CP. Despite this, STS-CP performs comparably to CP-ARLS-LEV at most ranks, with significant slowdown only at rank 25 on the smallest tensor Uber.

On the Enron tensor, hybrid CP-ARLS-LEV could not achieve the 95% accuracy threshold for any rank above 25 for the sample counts tested in Table 6. **STS-CP achieved the threshold accuracy for all ranks tested**. Instead, Figure 10 reports the speedup to achieve 85% of the fit of non-randomized ALS on the Enron. Beyond rank 25, our method consistently exhibits more than 2x speedup to reach the threshold.

Guide to Sampler Selection.Based on the performance comparisons in this section, we offer the following guide to CP decomposition algorithm selection. Our experiments demonstrate that **STS-CP offers the most benefit on sparse tensors with billions of nonzeros (Amazon and Reddit) at high target decomposition rank**. Here, the runtime of our more expensive sampling procedure is offset by reductions in the least squares solve time. For smaller tensors, our sampler may still offer significant performance benefits (Enron). In other cases (Uber, NELL-2), CP-ARLS-LEV exhibits better performance, but by small margins for rank beyond 50.

\begin{table}
\begin{tabular}{l c} \hline \hline Tensor & Values of \(J\) Tested \\ \hline Uber & \(\{2^{15}x\mid x\in\{1..13\}\}\) \\ Enron & \(\{2^{15}x\mid x\in\{1..7\}\cup\{10,12,14,16,18,20,22,26,30,34,38,42,46,50,54\}\}\) \\ NELL-2 & \(\{2^{15}x\mid x\in\{1..7\}\}\) \\ Amazon & \(\{2^{15}x\mid x\in\{1..7\}\}\) \\ Reddit & \(\{2^{15}x\mid x\in\{1..12\}\}\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Tested Sample Counts for hybrid CP-ARLS-LEV

Figure 9: Speedup of STS-CP over CP-ARLS-LEV hybrid (LK) to reach 95% of the fit achieved by non-randomized ALS. Large tensors have more than 1 billion nonzero entries.

STS-CP reduces the cost of each least squares solve through a sample selection process that relies on dense linear algebra primitives (see Algorithms 3 and 4). Because these operations can be expressed as standard BLAS calls and can be carried out in parallel (see Appendix A.10, we hypothesize that STS-CP is favorable when GPUs or other dense linear algebra accelerators are available.

Because our target tensor is sparse, the least squares solve during each ALS iteration requires a sparse matricized-tensor times Khatri-Rao product (spMTTKRP) operation. After sampling, this primitive can reduced to sparse-matrix dense-matrix multiplication (SpMM). Development of accelerators for these primitives is an active area of research [28, 26]. When such accelerators are available, the lower cost of the spMTTKRP operation reduces the relative benefit provided by the STS-CP sample selection method. We hypothesize that CP-ARLS-LEV, with its faster sample selection process but lower sample efficiency, may retain its benefit in this case. We leave verification of these two hypotheses as future work.

Figure 10: Speedup of STS-CP over CP-ARLS-LEV hybrid (LK) to reach 85% of the fit achieved by non-randomized ALS on the Enron Tensor.