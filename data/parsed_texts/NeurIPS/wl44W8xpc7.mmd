# Learning Infinitesimal Generators of Continuous Symmetries from Data

 Gyeonghoon Ko, Hyunsu Kim, Juho Lee

Kim Jaechul Graduate School of AI

KAIST

Seoul, South Korea

{kog, kim.hyunsu, juholee}@kaist.ac.kr

###### Abstract

Exploiting symmetry inherent in data can significantly improve the sample efficiency of a learning procedure and the generalization of learned models. When data clearly reveals underlying symmetry, leveraging this symmetry can naturally inform the design of model architectures or learning strategies. Yet, in numerous real-world scenarios, identifying the specific symmetry within a given data distribution often proves ambiguous. To tackle this, some existing works learn symmetry in a data-driven manner, parameterizing and learning expected symmetry through data. However, these methods often rely on explicit knowledge, such as pre-defined Lie groups, which are typically restricted to linear or affine transformations. In this paper, we propose a novel symmetry learning algorithm based on transformations defined with one-parameter groups, continuously parameterized transformations flowing along the directions of vector fields called infinitesimal generators. Our method is built upon minimal inductive biases, encompassing not only commonly utilized symmetries rooted in Lie groups but also extending to symmetries derived from nonlinear generators. To learn these symmetries, we introduce a notion of a validity score that examine whether the transformed data is still valid for the given task. The validity score is designed to be fully differentiable and easily computable, enabling effective searches for transformations that achieve symmetries innate to the data. We apply our method mainly in two domains: image data and partial differential equations, and demonstrate its advantages. Our codes are available at https://github.com/kogyeonghoon/learning-symmetry-from-scratch.git.

## 1 Introduction

Symmetry is fundamental in many scientific disciplines, crucial for understanding the structure and dynamics of physical systems, datasets, and mathematical models. The ability to uncover and leverage symmetries has become increasingly important in machine learning and scientific research due to its potential to improve model efficiency, generalization, and interpretability. By capturing inherent symmetrical properties, models can learn more compact and informative representations, leading to improved performance in tasks like supervised learning [31, 29, 4, 7, 33], self-supervised learning [8, 15, 23], and generative models [19, 11, 18].

Previous methods for learning symmetry have often relied on the explicit parameterization of group representations based on predefined generators, which can be limited in capturing various symmetries, including transformations that do not align along the generators. For example, when searching for Lie group symmetries in images or physics data, existing methods [3, 34] parameterize a group action \(g\) as the matrix exponential of a linear combination of linear or affine Lie algebra generators \(L_{i}\) with their learnable coefficients \(w_{i}\) as \(g=\exp\left(\sum_{i}w_{i}L_{i}\right)\). In the affine transformations of images in\((x_{1},x_{2})\)-coordinates, there are six generators, each corresponding to translation, scaling, and shearing operations with respect to the \(x_{1}\)-axis and \(x_{2}\)-axis. Although there exist some methods that directly learn the generators, they are either bound to the general linear group \(GL(n)\), which cannot account for non-affine or non-linear transformations [24], or are not guaranteed to find the correct symmetry in real-world image datasets [9; 13].

When searching for symmetries in high-dimensional real-world datasets, we can take advantage of the fact that the data can be interpreted as a function \(f:\mathcal{X}\rightarrow\mathcal{Y}\), such as images, which are functions from the 2D Euclidean space to the color space. Another notable example of such data is partial differential equations (PDEs), where the data take the form \(\bm{u}:\mathcal{X}\rightarrow\mathcal{U}\) and the Lie symmetries are defined as transformations on the space \(\mathcal{X}\times\mathcal{U}\). There have been significant advances in Lie symmetry analysis in recent years, for both academic and industrial purposes, mostly involving extensive symbolic calculations and relying on computer algebra systems [26]. Discovering Lie symmetries of PDEs from data without prior knowledge is an unexplored topic, except for the work of Gabel et al. [14], which learns the symmetry generators of various PDEs in a supervised learning setup.

In this work, we propose a novel method for learning continuous symmetries, including non-affine transformations, from data without prior knowledge. By modeling one-parameter groups using Neural Ordinary Differential Equation (Neural ODE) [6], we establish a learnable infinitesimal generator capable of producing a sequence of transformed data through ODE integration. We design an appropriate _validity score_ function that measures how much the transformation violates the invariance to certain criteria defined depending on the target task, and learn the generators by optimizing towards the validity score of the data transformed through ODE integration. For example, in an image classification dataset, we use a pre-trained feature extractor and define the validity score to be the cosine similarity between the features extracted from the original image and the transformed image. For PDEs, the validity score is defined by the numerical errors of the original equations after the transform. The validity scores are chosen based on the characteristics of the target tasks, and designed to be fully differentiable, so that the symmetry can be learned via gradient descent in an end-to-end fashion. We also incorporate two regularizations, orthonormality and Lipschitz loss, which prevent the learned generators from converging to a trivial solution.

Subsequently, we demonstrate that our method indeed discovers the correct symmetries in both image and PDE datasets. To the best of our knowledge, our research is the first to retrieve affine symmetry in the entire space of continuous transformations using the CIFAR-10 classification dataset, as shown in Figure 1. Moreover, our method excels in identifying non-affine symmetries and approximate symmetries in PDE tasks. We further demonstrate that the learned generators can be leveraged to develop automatic augmentation generators, which can be used to produce augmented training data for both image classification tasks and neural operator learning tasks of PDEs [21]. We provide empirical evidence that the models trained with data augmented by our learned generators perform competitively with those trained with traditional closed-form transforms such as Lie point symmetry (LPS) [4]. Moreover, we show that the _approximate symmetries_ discovered by our method, which cannot be found by classical methods, can also boost the performance of the models, especially when the size of the training data is small.

Figure 1: (a) Examples of the vector fields. \(V_{3}\) is a learned symmetry which is approximately a rotation, while \(V_{7}\) is not a symmetry, thus having a high validity score. (b) Transformed CIFAR-10 images using the learned generators. All the vector fields and transformations learned from CIFAR-10 are presented in Figure 8 of Appendix C. (c) Transformation of PDEs (KS equation) with learned symmetries: time translation (t-tsl) and Galilean boost (gal).

## 2 Preliminaries: One-parameter Group

In this section, we present the basic definitions of a one-parameter group, which we use to parameterize the symmetric transformations learned from the data.

Consider an unknown Euclidean domain \(\mathcal{Z}\subseteq\mathbb{R}^{n}\) and a smooth vector field \(V:\mathcal{Z}\to\mathbb{R}^{n}\). A path \(\gamma:I=(a,b)\subseteq\mathbb{R}\to\mathcal{Z}\) satisfying \(\frac{d}{ds}\gamma(s)=V(\gamma(s))\) for all \(s\in I\) is a curve that travels around the domain \(\mathcal{Z}\) with a velocity given by the vector field \(V\). Along the curve \(\gamma\), a point \(\bm{x}_{0}=\gamma(a_{0})\) can be transported to \(\gamma(a_{0}+s)\) by flowing along the vector field \(V\) by time \(s\). We define the flow \(\vartheta_{s}^{V}\) by \(\vartheta_{s}^{V}(\bm{x}_{0})=\gamma(a_{0}+s)\) of \(V\) as in Figure 2. This flow is computed by solving an ODE

\[\frac{d}{ds}\vartheta_{s}^{V}(\bm{x})=V(\bm{x})\] (1)

with initial condition \(\vartheta_{0}^{V}(\bm{x})=\bm{x}\) for all \(\bm{x}\in\mathcal{Z}\)[20].

The flow \(\vartheta^{V}\) is governed by an autonomous ODE, i.e., an ODE independent of the temporal variable \(s\). Due to properties of autonomous ODEs, the flow \(\vartheta^{V}\) exists uniquely and it is smooth in both variables \(s\) and \(\bm{x}\). Assuming a mild condition on \(V\), such as \(V\) extends to a compactly supported vector field \(\tilde{V}\) on \(\mathbb{R}^{n}\), the ODE does not terminate in \(\mathbb{R}^{n}\) in finite time and hence \(\vartheta_{s}^{V}\) is defined for all \(s\in\mathbb{R}\). In that case, the flow satisfies a group equation

\[\vartheta_{s_{1}+s_{2}}^{V}(\bm{x})=\vartheta_{s_{1}}^{V}\circ\vartheta_{s_{2 }}^{V}(\bm{x})\] (2)

for all \(s_{1},s_{2}\in\mathbb{R}\). It means that the flow can be regarded as a group action of \(\mathbb{R}\) on \(\mathbb{R}^{n}\), transforming elements of \(\mathcal{Z}\subseteq\mathbb{R}^{n}\). For this reason, \(\vartheta_{s}^{V}\) is also called a _one-parameter group_, and the vector field \(V\) is called an _infinitesimal generator_ of the one-parameter group.

On \(\mathcal{Z}=\mathbb{R}^{n}\), a constant vector field \(V(\bm{x})=\bm{v}\in\mathbb{R}^{n}\) gives rise to a translation \(\bm{x}\mapsto\bm{x}+s\bm{v}\). For a matrix \(\bm{A}\in\mathbb{R}^{n\times n}\), a vector field \(V(\bm{x})=\bm{A}\bm{x}\) gives rise to an affine transformation \(\bm{x}\mapsto\exp(s\bm{A})\bm{x}\), where \(\exp\) is the matrix exponentiation. Multiple infinitesimal generators may span a vector space \(\mathfrak{g}\), and if \(\mathfrak{g}\) satisfies some algebraic condition (closure under the Lie bracket), then \(\mathfrak{g}\) forms a _Lie algebra_. Composing the elements of one-parameter groups of elements in \(\mathfrak{g}\) gives rise to a _Lie group_\(G\). The correspondence between \(G\) and \(\mathfrak{g}\) is called Lie group-Lie algebra correspondence.

_Continuous symmetries_ are commonly defined by a Lie group \(G\), acting on some domain and keeping the transformed objects _invariant_ with respect to some criterion. We model symmetries by specifying their infinitesimal generators whose composition of one-parameter groups comprises the symmetries of that domain.

Below, we describe two representative examples that will be discussed extensively in the remainder of the paper: images (interpreted as functions on 2D planes) and PDEs.

### Images and Their Symmetries

Consider a rescaled image of the form \(f:\mathcal{X}=[-1,1]^{2}\subseteq\mathbb{R}^{2}\to\mathcal{Y}=[0,1]^{3} \in\mathbb{R}^{3}\). The affine transformations on \(\mathbb{R}^{2}\) have the form \(\bm{x}=(x_{1},x_{2})\mapsto\bm{A}\bm{x}+\bm{b}\) for a matrix \(\bm{A}\in\mathbb{R}^{2\times 2}\) and a vector \(\bm{b}\in\mathbb{R}^{2}\). The Affine transformations form the 6-dimensional Affine group \(\mathrm{Aff}(2)\), and it has a corresponding 6-dimensional Lie algebra having a basis \(\{L_{1},\ldots,L_{6}\}\) given as in Table 1. The symmetries of images are often exploited as a data augmentation strategy for learning image classifiers, under an assumption that the transforms do not alter the identity or semantics of the images to be classified.

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Generator** & **Expression** & **One-Parameter Group** & **Description** \\ \hline \(L_{1}\) & \((1,0)\) & \(\bm{x}\mapsto\bm{x}+s(1,0)\) & translation in \(x_{1}\)-axis \\ \(L_{2}\) & \((0,1)\) & \(\bm{x}\mapsto\bm{x}+s(0,1)\) & translation in \(x_{2}\)-axis \\ \(L_{3}\) & \((\bm{x}_{1},0)\) & \(\bm{x}\mapsto\bm{x}+\left(\vartheta_{0}^{V}\right)\bm{x}\) & scaling of \(x_{1}\)-axis \\ \(L_{4}\) & \((0,x_{2})\) & \(\bm{x}\mapsto\left(\frac{1}{2}\right)^{2}\bm{x}\) & scaling of \(x_{1}\)-axis \\ \(L_{5}\) & \((x_{2},0)\) & \(\bm{x}\mapsto\left(\frac{1}{2}\right)^{2}\bm{x}\) & shear parallel to \(x_{1}\)-axis \\ \(L_{6}\) & \((0,x_{1})\) & \(\bm{x}\mapsto\left(\frac{1}{2}\right)^{2}\bm{x}\) & shear parallel to \(x_{2}\)-axis \\ \hline \(L_{3}+L_{4}\) & \((x_{1},x_{2})\) & \(\bm{x}\mapsto\left(\frac{1}{2}\right)^{2}\bm{x}\) & uniform scaling \\ \(L_{6}-L_{5}\) & \((-2x_{2},x_{1})\) & \(\bm{x}\mapsto\left(\frac{1}{2}\right)^{2}\bm{x}\) & rotation \\ \hline \hline \end{tabular}
\end{table}
Table 1: Infinitesimal generators of the Affine group \(\mathrm{Aff}(2)\). The set of six generators \(\{L_{1},\cdots,L_{6}\}\) forms a basis of the corresponding Lie algebra.

### PDEs and Their Symmetries

Given an \(n\)-dimensional independent variable \(\bm{x}=(x_{1},\cdots,x_{n})\in\mathcal{X}\subseteq\mathbb{R}^{n}\) and an \(m\)-dimensional dependent variable \(\bm{u}=\bm{u}(\bm{x})=(u_{1}(\bm{x}),\cdots,u_{m}(\bm{x}))\in\mathcal{U}\subseteq \mathbb{R}^{m}\), we denote by \(\bm{u}^{(i)}\) the collection of all \(i\)-th partial derivatives of \(\bm{u}\) with respect to \(\bm{x}\). A _partial differential equation_\(\bm{\Delta}\) on \(\bm{u}(\bm{x})\) of order \(k\) is defined by a set of algebraic equations \(\bm{\Delta}(\bm{x},\bm{u},\bm{u}^{(1)},\cdots,\bm{u}^{(k)})=0\) involving all the variables and their partial derivatives. For example, two scalar independent variables \(x,t\in\mathbb{R}\) and one scalar dependent variable \(u(x,t)\in\mathbb{R}\) governed by equation \(\Delta=u_{t}+uu_{x}+u_{xxx}=0\) gives the 1-dimensional Korteweg-de Vries (KdV) equation, where we denote partials using subscripts, e.g. \(u_{t}=\frac{\partial u}{\partial t}\) and \(u_{xxx}=\frac{\partial^{3}u}{\partial x^{3}}\). The KdV equation is commonly used to model the dynamics of solitons, e.g. shallow water waves [36]. The KdV equation described above is an example of 1-dimensional scalar-valued evolution equation. Such an equation takes the form \(u=u(x,t)\in\mathbb{R}\) with its governing equation of the form

\[u_{t}=F(x,t,u,u_{x},u_{xx},u_{xxx},\cdots)\] (3)

for some function \(F\). In this paper, we only deal with 1D scalar-valued evolution equation on a fixed periodic domain \(x\in[0,L]\).

Continuous symmetries of PDEs are commonly parametrized by a one-parameter group on \(\mathcal{X}\times\mathcal{U}\). Denote \((\bm{\xi},\bm{\mu})=(\bm{\xi}[\bm{x},\bm{u}],\bm{\mu}[\bm{x},\bm{u}])\) an infinitesimal generator defined on \(\mathcal{X}\times\mathcal{U}\). Then the PDE \(\bm{\Delta}\) possesses the infinitesimal generator of symmetry \((\bm{\xi},\bm{\mu})\) if the equation is still satisfied after transforming both the independent variable \(\bm{x}\) and the dependent variable \(\bm{u}\)[27; 5; 26]. Symmetries of PDEs are categorized by how the generators \((\bm{\xi},\bm{\mu})\) depend on \((\bm{x},\bm{u})\). The symmetry is a Lie point symmetry (LPS) if the value of \((\bm{\xi},\bm{\mu})\) at each point \((\bm{x},\bm{u}(\bm{x}))\) depends only on the point value \((\bm{x},\bm{u}(\bm{x}))\) itself. If \((\bm{\xi},\bm{\mu})\) also depends on the derivatives \(\bm{u}^{(1)},\cdots,\bm{u}^{(k)}\) at that point, it is called a Lie-Backlund symmetry or generalized symmetry. If \((\bm{\xi},\bm{\mu})\) depends on integrals of \(\bm{u}\), then it is called a nonlocal symmetry. Finding an LPS of a PDE \(\bm{\Delta}\) can be done algorithmically under some mild assumptions on \(\bm{\Delta}\). However, there is no general recipe of finding Lie-Backlund symmetries or nonlocal symmetries, and discovering such symmetries remains an active area of research.

## 3 Related Work

Symmetry discovery.Approaches to learning symmetries can be categorized by addressing two questions: (a) _where do they search for symmetries_, and (b) _what are they aiming to learn_. One line of research aims to learn ranges, focusing on determining the ranges of transformation scales that enhance learning when employed as augmentation techniques. For example, Benton et al. [3] learns transformation ranges of predefined transformations by treating them as learnable parameters and backpropagating through differentiable transformations.

Another line of research aim to learn subgroups of bigger candidate groups, typically a linear group \(GL(n)\) or an affine group \(\mathrm{Aff}(n)\). For example, Desai et al. [10] use the Generative Adversarial Network (GAN) to search for symmetries, with the generator transforming data by group elements sampled from the candidate group and the discriminator verifying whether the transformed data still lies in the data distribution. Similarly, Yang et al. [34] employ the GAN approach, but generator of GAN models infinitesimal generators instead of the subgroup itself, and learns affine symmetries such as rotation of images and Lorentz symmetry of high-energy particles. As an alternative, Moskalev et al. [24] proposed an idea of extracting symmetries from learned neural network by differentiating through it, and retrieved 2D rotation in the linear group using the rotation MNIST dataset.

Finally, _learning symmetries with minimal assumption_, i.e. without assuming the infinitesimal generators are linear or affine, is an area of large interest. An early attempt of Rao & Ruderman [28] models infinitesimal generator by a learnable matrix from the pixel space to the pixel space, and learn 2D rotation by solving a task that compares original images and rotated ones, where the images are \(5\times 5\) random pixels. Sohl-Dickstein et al. [32] takes the similar approach with eigen-decomposing the learnable matrix. Dehmamy et al. [9] builds a convolution operation whose kernel encodes learnable infinitesimal generators, and retrieved 2D rotation from random \(7\times 7\) images by comparing original and transformed ones, and Yang et al. [35] uses an autoencoder to simplify nonlinear symmetries into linear ones. Our work closely aligns with Liu & Tegmark [22] and Forestano et al. [13], which model one-parameter groups by an MLP and learn the symmetries from an invariant scalar quantity. To thebest of our knowledge, learning correct symmetries with minimal assumption was only achieved with toy datasets, far from real-world datasets such as CIFAR-10.

Utilizing symmetries in deep learning.An effective method for leveraging symmetries in deep learning is data augmentation [31]. In the image domain, there are numerous augmentation techniques available [29], most of which are based on geometric properties of images. Although data augmentation techniques have been primarily explored in the context of images, recent studies by [4; 23] have demonstrated that symmetries can also be used for augmenting data in the training of neural PDE solvers. In addition to data augmentation, some approaches involve designing new neural network architectures that inherently reflect the group symmetries of the input data [7]. Wang et al. [33] applied a similar strategy within the PDE domain.

## 4 Learning Continuous Symmetries with One-Parameter Groups

### Training Process

Given a learning task with a dataset \(\mathcal{D}\subset\mathcal{A}\) in an underlying space \(\mathcal{A}=\{f|f:\mathcal{X}\rightarrow\mathcal{Y}\}\), we aim to model symmetry by a one-parameter group \(\vartheta_{s}\) acting on \(\mathcal{A}\), as explained in SS 4.3. We define a continuous symmetry by stating that \(\vartheta\) is a symmetry of this task if there exists some \(\sigma>0\) such that for any data point \(f\in\mathcal{D}\) and transformation scale \(s\in[-\sigma,\sigma]\), the transformed data point \(\vartheta_{s}(f)\) remains valid for this task. We assume the existence of a differentiable _validity score_\(S(\vartheta_{s},f)\in\mathbb{R}\), such that \(\vartheta_{s}(f)\in\mathcal{A}\) is valid if \(S(\vartheta_{s},f)<C\) for a certain threshold \(C\in\mathbb{R}\). Then, a one-parameter group \(\vartheta\) is a symmetry of the task if \(S(\vartheta_{s},f)<C\) for all \(f\in\mathcal{D}\).

The validity score depends on the nature of the target task, though no strict criterion exists. As long as it is differentiable and the valid data aids learning, it is considered acceptable. For instance, we can define the validity based on a negative log-likelihood of a probabilistic model. In SS 4.2, we discuss the validity scores to be used for image and PDE data.

Once a validity score is defined, we learn a symmetry \(\vartheta^{*}\) by minimizing the validity scores of transformed data,

\[\vartheta^{*}=\operatorname*{arg\,min}_{\vartheta}\mathbb{E}_{f\sim\mathcal{D},s\sim\operatorname{Unif}([-\sigma,\sigma])}\left[S(\vartheta_{s},f)\right],\] (4)

where the \(\operatorname*{arg\,min}\) is taken over the entire class of smooth one-parameter groups. Since the learning is performed in function space, we appropriately constrain the function space using a regularizer, as described in SS 4.4. Once symmetries are learned, they reveal the symmetrical properties of the target task, which can then be exploited to augment the training data.

### Task-specific Definition of Validity Score \(S\)

Images.In image-related tasks, we define a validity score using a pre-trained neural network. Let \(\mathcal{D}\) be an image classification dataset consisting of data of the form \((f,y)\in\mathcal{A}\times\mathbb{R}\), where \(f\) is an image and \(y\) is a label. Also let \(H_{\mathrm{cls}}\circ H_{\mathrm{fext}}:\mathcal{A}\rightarrow\mathbb{R}\) be a learned neural network, where we denote by \(H_{\mathrm{fext}}:\mathcal{A}\rightarrow\mathbb{R}^{k}\) the feature extractor and \(H_{\mathrm{cls}}:\mathbb{R}^{k}\rightarrow\mathbb{R}\) the classifier. We define the validity score \(S(\vartheta_{s},f)\) as the cosine similarity between the features before and after the transformation:

\[S(\vartheta_{s},f)=\operatorname*{sim}\left(H_{\mathrm{fext}}(\vartheta_{s}(f) ),H_{\mathrm{fext}}(f)\right),\] (5)

where \(\operatorname*{sim}\) is the cosine similarity defined as \(\operatorname*{sim}(\boldsymbol{v}_{1},\boldsymbol{v}_{2})=\frac{|\boldsymbol{ v}_{1},\boldsymbol{v}_{2}|}{\|\boldsymbol{v}_{1}\|\|\boldsymbol{v}_{2}\|}\) for all \(\boldsymbol{v}_{1},\boldsymbol{v}_{2}\in\mathbb{R}^{k}\setminus\{\boldsymbol{0}\}\).

PDEs.Let \(\boldsymbol{u}(\boldsymbol{x})\) be a solution of a given PDE \(\boldsymbol{\Delta}\), discretized on a rectangular grid \(\mathcal{X}_{\mathrm{grid}}=\{\boldsymbol{x}_{i}\}_{i=1}^{N_{\mathrm{grid}}}\). For a transformed data \(\vartheta_{s}(\boldsymbol{u})\), we measure the violation of the equality \(\boldsymbol{\Delta}=0\) to assess whether the transformed data is still a valid solution. Using an appropriate numerical differentiation method, we directly compute the value of the PDE, denoted as \(\boldsymbol{\Delta}(\vartheta_{s}(\boldsymbol{u}))\), which represents the error of \(\vartheta_{s}(\boldsymbol{u})\)

Figure 3: Process of learning symmetry.

as a solution of \(\bm{\Delta}\), taking a value \(\bm{\Delta}(\vartheta_{s}(\bm{u}))_{i}\) at grid point \(\bm{x}_{i}\). The validity score is defined by the summation of all PDE errors across the grid points:

\[S(\vartheta_{s},\bm{u})=\sum_{i}\left|\bm{\Delta}(\vartheta_{s}(\bm{u}))_{i} \right|.\] (6)

For example, for a solution \(u(x)\) of the 1D KdV equation, we examine whether the transformed solution \(\tilde{u}=\vartheta_{s}(u)\) satisfies \(\tilde{u}_{t}+\tilde{u}\tilde{u}_{x}+\tilde{u}_{xxx}=0\) where the partials are computed using a numerical differentiation method.

### Parametrization of One-Parameter Groups using Neural ODE

On a Euclidean domain \(\mathcal{Z}\subseteq\mathbb{R}^{n}\), we model an infinitesimal generator with an MLP \(\bm{h}_{\bm{\theta}}:\mathcal{Z}\subseteq\mathbb{R}^{n}\rightarrow\mathbb{R}^ {n}\). The infinitesimal generator \(\bm{h}_{\bm{\theta}}\) gives rise to a one-parameter group \(\vartheta_{s}^{\bm{h}_{\bm{\theta}}}\). We sample a transformation scale \(\alpha\sim\mathrm{Unif}([-\sigma,\sigma])\) for a predefined hyperparameter \(\sigma\in\mathbb{R}_{>0}\). To transform a point \(x\in\mathcal{Z}\) along this one-parameter group by an amount \(\alpha\geq 0\), we use a numerical ODE solver to solve the ODE for \(\gamma:[0,\alpha]\rightarrow\mathcal{Z}\) satisfying

\[\gamma^{\prime}(s)=\bm{h}_{\bm{\theta}}(\gamma(s)),\quad\forall s\in[0,\alpha ],\quad\gamma(0)=x\] (7)

and obtain a transformed data point \(\tilde{\bm{x}}=\vartheta_{\alpha}^{\bm{h}_{\bm{\theta}}}(\bm{x})=\gamma(\alpha)\). If \(\alpha<0\), we compute \(\vartheta_{\alpha}^{\bm{h}_{\bm{\theta}}}(\bm{x})=\vartheta_{-\alpha}^{-\bm{ h}_{\bm{\theta}}}(\bm{x})\) by integrating \(-\bm{h}_{\bm{\theta}}\) instead of \(\bm{h}_{\bm{\theta}}\) using the ODE solver. We can backpropagate through the numerical ODE solver using the adjoint method [6] to learn \(\theta\).

Let \(f:\mathcal{X}\rightarrow\mathcal{Y}\) be a data point on a domain \(\mathcal{A}\). As \(\mathcal{A}\) is a space of functions, naively modeling symmetry on \(\mathcal{A}\) may ignore the geometry implied in the input space \(\mathcal{X}\). Instead, we define two transformations: \(\vartheta_{\mathcal{X}}\) on \(\mathcal{X}\) and \(\vartheta_{\mathcal{Y}}\) on \(\mathcal{Y}\), and induce a transformation of \(f\) by

\[(\vartheta_{\mathcal{X}}(f))(x)=f(\vartheta_{\mathcal{X}}^{-1}(x)),\quad( \vartheta_{\mathcal{Y}}(f))(x)=\vartheta_{\mathcal{Y}}(f(x)),\] (8)

where we abuse notation and write the transformed function as \(\vartheta_{\mathcal{X}}(f)\) and \(\vartheta_{\mathcal{Y}}(f)\). For an image represented as a discretized function \(f:\mathcal{X}\rightarrow\mathcal{Y}\) from \(\mathcal{X}=[-1,1]^{2}\) and \(\mathcal{Y}=[0,1]^{3}\), \(\vartheta_{\mathcal{X}}\) corresponds to spatial transformations such as translation or rotation, and \(\vartheta_{\mathcal{Y}}\) corresponds to color space transformations. For a PDE, a 1D scalar-valued evolution equation on a fixed periodic domain takes the form \(u(x,t)\in\mathcal{U}=\mathbb{R}\) with \((x,t)\in[0,L]\times[0,T]=\mathcal{X}\subseteq\mathbb{R}^{2}\), and we parameterize an infinitesimal generator on a product space \(\mathcal{X}\times\mathcal{U}\subseteq\mathbb{R}^{3}\) by an MLP. Then, a transformation on \((x,t,u)\in\mathcal{X}\times\mathcal{U}\) induces a transformation on the solution of the PDE \(u(x,t)\).

### Objective Functions

Symmetry loss.Let \(N_{\mathrm{sym}}\) be the number of symmetries to be learned. Let \((\bm{h}_{\bm{\theta}}^{(a)})_{a=1}^{N_{\mathrm{sym}}}\) be the infinitesimal generators computed from a single MLP. For each \(a\in\{1,\dots,N_{\mathrm{sym}}\}\), we sample a transformation scale \(s_{a}\sim\mathrm{Unif}([-\sigma,\sigma])\) to transform \(f\) via numerical integration. The parameter \(\bm{\theta}\) is optimized by minimizing the average validity score over the training data,

\[\mathcal{L}_{\mathrm{sym}}(\bm{\theta})=\sum_{a=1}^{N_{\mathrm{sym}}}\mathbb{E }_{f\sim\mathcal{D},s_{a}\sim\mathrm{Unif}([-\sigma,\sigma])}\left[S\Big{(} \vartheta_{s_{a}}^{\bm{h}_{\bm{\theta}}^{(a)}},f\Big{)}\right].\] (9)

Orthonormality loss.Learning only with the symmetry loss may result in trivial solutions such as the zero vector field or the same vector field repeated in multiple slots. To prevent this, we introduce the orthonormality loss to regularize the model towards learning orthonomral vector fields. Specifically, given two vector fields \(V_{1},V_{2}:\mathcal{Z}\rightarrow\mathbb{R}^{n}\), we define an inner product as,

\[\langle V_{1},V_{2}\rangle=\frac{1}{\text{vol}(\mathcal{Z})}\int_{\mathcal{Z}} \omega(\bm{x})(V_{1}(\bm{x})\cdot V_{2}(\bm{x}))d\bm{x}\approx\frac{1}{| \mathcal{Z}_{\text{grid}}|}\sum_{\bm{x}_{i}\in\mathcal{Z}_{\text{grid}}} \omega(\bm{x}_{i})(V_{1}(\bm{x}_{i})\cdot V_{2}(\bm{x}_{i})),\] (10)

with a suitable weight function \(\omega(x):\mathcal{Z}\rightarrow\mathbb{R}\) and a discretized grid \(\mathcal{Z}_{\text{grid}}\) of \(\mathcal{Z}\) of size \(|\mathcal{Z}_{\text{grid}}|\). Given this definition, we first normalize each generator by its norm to ensure \(\|\bm{h}_{\bm{\theta}}^{(a)}\|^{2}=1\). Then we compute the orthonormality loss as,

\[\mathcal{L}_{\text{ortho}}(\bm{\theta})=\sum_{1\leq a<b\leq N_{\mathrm{sym}}} \Big{\langle}\mathrm{sg}(\bm{h}_{\bm{\theta}}^{(a)}),\bm{h}_{\bm{\theta}}^{(b)} \Big{\rangle},\] (11)where \(\mathrm{sg}(\cdot)\) denotes the stop-gradient operation to ensure that the constraint \(\langle\bm{h}_{\bm{\theta}}^{(a)},\bm{h}_{\bm{\theta}}^{(b)}\rangle=0\) only affects the latter slot (\(b\)). By doing this, if the true number of symmetries \(N_{\mathrm{sym}}^{*}\) is less than or equal to the assumed number of symmetries \(N_{\mathrm{sym}}\), the learned symmetries will be aligned in the first \(N_{\mathrm{sym}}^{*}\) slots.

Lipschitz loss.We further introduce inductive biases to the infinitesimal generators we aim to learn. For instance, an infinitesimal generator moving only a single pixel near the boundary by a large scale would be undesirable. This idea can be implemented using Lipschitz continuity. For a grid point \(\bm{x}_{i}\in\mathcal{Z}_{\text{grid}}\) and its neighboring point \(\bm{x}_{j}\in\mathrm{hhd}(\bm{x}_{i})\subset\mathcal{Z}_{\text{grid}}\), we expect the vector field \(V\) to satisfy the Lipschitz condition,

\[\mathrm{Lips}(V;\bm{x}_{i},\bm{x}_{j})<\tau\text{ where }\mathrm{Lips}(V;\bm{x} _{i},\bm{x}_{j})=\frac{\|V(\bm{x}_{i})-V(\bm{x}_{j})\|}{\|\bm{x}_{i}-\bm{x}_{j }\|}.\] (12)

To regularize the model toward the Lipschitz condition, we introduce the Lipshitz loss,

\[\mathcal{L}_{\text{Lips}}(\bm{\theta})=\sum_{a=1}^{N_{\mathrm{sym}}}\sum_{ \bm{x}_{i}\in\mathcal{Z}_{\text{grid}},\bm{x}_{j}\in\mathrm{hhdhd}(\bm{x}_{i}) }\max(\mathrm{Lips}(\bm{h}_{\bm{\theta}}^{(a)};\bm{x}_{i},\bm{x}_{j})-\tau,0).\] (13)

Total loss and loss-scale-independent learning.We jointly minimize the three loss functions with suitable weights \(w_{\mathrm{sym}},w_{\text{ortho}},w_{\text{Lips}}>0\) and learn the weights \(\bm{\theta}\) of MLP using a stochastic gradient descent:

\[\bm{\theta}^{*}=\operatorname*{arg\,min}_{\bm{\theta}}w_{\text{sym}}\mathcal{ L}_{\text{sym}}(\bm{\theta})+w_{\text{ortho}}\mathcal{L}_{\text{ortho}}(\bm{ \theta})+w_{\text{Lips}}\mathcal{L}_{\text{Lips}}(\bm{\theta}).\] (14)

To minimize the computational burden of hyperparameter tuning, we ensure that all the loss terms have a _natural scale_, i.e. a dimensionless scale independent of the context. For example, when penalizing the inner product in Equation 11, we apply \(\arccos\) to the normalized inner product to ensure the loss term lies in \([0,\pi/2)\). Similarly, the scale of the PDE validity score \(S(\vartheta_{s},\bm{u})\) in Equation 9 depends on the scale of the data \(\bm{u}\). When penalizing it, we apply the log function so that the gradients are scaled automatically as \(\nabla_{\bm{\theta}}\log(S(\vartheta_{s},\bm{u}))=\nabla_{\bm{\theta}}S( \vartheta_{s},\bm{u})/S(\vartheta_{s},\bm{u})\).

Here we describe the generic training process, but the actual implementation requires non-trivial task-specific designs, such as the choice of the weighting function \(w(\bm{x})\) or the method for locating the transformed data on the target grid. We defer these details for image and PDE tasks to Appendix A.

### Comparison With Other Methods

Here, we compare our method with other recent symmetry discovery methods. The differences mainly arise from (a) what they aim to learn (e.g., transformation scales or subgroups from a larger group) and (b) their assumptions about prior knowledge (e.g., complete, partial, or no knowledge of symmetry generators). Another important distinction is the viewpoint on symmetry: some methods learn symmetries that raw datasets inherently possess (implicit), while others learn symmetries from datasets explicitly designed to carry such symmetries (explicit).

Some recent symmetry discovery works are listed in Table 2. We emphasize that our method excels in two key aspects: (a) our learning method reduces infinitely many degrees of freedom, (b) our method works with high-dimensional real-world datasets. For example, while LieGAN [33] and LieGG [24] reduce a 6-dim space (affine) to a 3-dim space (translation and rotation) in an image dataset, ours reduces an \(\infty\)-dim space to a finite one. L-conv [9] also does not assume any prior knowledge, but it is limited in finding rotation in a toy task, where it learns rotation angles of rotated images by comparing them with the original ones, which are 7x7 random pixel images.

## 5 Experiments

### Images

We use images of size \(32\times 32\) from the CIFAR-10 classification task. Since our method does not model discrete symmetry, we use horizontal flip with 50% probability by default. We train a ResNet-18 model, which will be used as the feature extractor \(H_{\text{text}}\) in Equation 5. The weight function on the pixels is computed as explained in Appendix A.1. We expect to find 6 affine generators and we use an MLP modeling 10 vector fields in the pixel space \([-1,1]^{2}\subseteq\mathbb{R}^{2}\), expecting the first six learned vector fields to be the affine generators. We learn the Equation 14 using stochastic gradient descent with \(w_{\text{sym}}=1\) and \(w_{\text{ortho}},w_{\text{Lips}}=10\). The parameter \(\sigma\), which controls the scale of transformation, is set to \(\sigma=0.4\), and the Lipschitz threshold \(\tau\) is set to \(\tau=0.5\). Other details are described in Appendix B.1. We conducted three trials with random initializations and report the full results in Appendix C.1. Furthermore, we also learn symmetries in the _color space_, and their results are shown in Appendix G.

Learned symmetries.Since we expect to learn affine symmetries, we compare the results with the affine basis \(\{L_{1},\cdots,L_{6}\}\) defined in Table 1. We compute the inner products \(\langle V,L_{i}\rangle\) of the learned vector field \(V\) with \(L_{i}\) for \(i=1,\cdots,6\) to measure how much the learned vector fields contain the affine basis and measure the affine-ness of vector field by \(\text{Affine-ness}(V)^{2}=\sum_{i=1}^{6}\langle V,L_{i}\rangle^{2}\).

In all experiments, we successfully retrieve six linearly independent affine generators in the first six slots. Figure 3(a) shows that the learned generators are orthogonal to each other, as desired. Figure 3(b) shows the inner product between the learned generators and the affine generators. Since the affine-ness measure of the first 6 learned generators in Figure 3(c) is almost close to \(1\), we can read out the affine components in Figure 3(b) and say that e.g., \(V_{1}\approx(0,-0.98+0.12x_{2})\). Notably, two translation generators are found in the first two slots, indicating that the two translations are _the most invariant_ one-parameter group among the entire class of one-parameter groups on the pixel space. After the two translation generators, four affine generators are learned, indicating that affine transformations are _the next most invariant_ transformations. In particular, the third and fourth generators are close to the rotation generator and the scaling generator, respectively. The remaining four generators fix pixels close to the center and transform boundary pixels by a large magnitude.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline  & **Augerino** & **LieGAN** & **LieGG** & **L-conv** & **Forestano et al.** & **Ours** \\ \hline
**Symmetry** & completely & partially & partially & completely & completely & completely \\
**generators** & known & known (affine) & known (affine) & unknown & unknown & unknown \\ \hline
**Learn what?** & transformation & symmetry & symmetry & symmetry & symmetry & symmetry & symmetry \\  & scales & (rotation / & generator & generator & generator & generator \\  & Lorentz) & & (rotation) & (rotation) & (in low-dim task) & (affine) \\ \hline
**Verified** & raw & rotation & random & & & raw \\
**with what?** & CIFAR-10 & MNIST / & rotation & \(7\times 7\) pixel & toy data & CIFAR-10 \\  & & Top tagging & MNIST & image & (dim \(\leq 10\)) & \& PDEs \\ \hline
**Implicit or** & implicit & explicit & explicit & explicit & explicit & implicit \\ \hline
**How?** & optimize & compare & extracts from & compare & extracts from & extracts from \\  & while training & fake/true data & learned NN & rotated and & invariant oracle & validity score \\  & downstream & in GAN & using Lie & original & using Lie & using ODE \\  & task & framework & derivative & images & derivative & integration \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison with other symmetry discovery methods.

Figure 4: (a) Self inner-products of the learned generators. (b) Inner product comparison of the learned generators with the affine generators. (c) Affine-ness of learned generators.

[MISSING_PAGE_FAIL:9]

Interestingly, for the Burgers' equation and the cKdV equation, we additionally found the \(u\)-axis rescaling operation \((0,0,u)\) and the time translation \((0,1,0)\) respectively. Applying the \(u\)-axis rescaling \(u\mapsto cu\) for \(c\approx 1\) to the Burgers' equation \(u_{t}+uu_{x}-\nu u_{x}x=0\) gives \(cu_{t}+c^{2}uu_{x}-\nu cu_{x}x=c(c-1)uu_{x}\), leaving only the \(uu_{x}\) term. The \(uu_{x}\) term in Burgers' is called the _convection term_, and it is approximately zero in most region and spikes in some small region. Similarly, the time translation \(t\mapsto t+c\) for \(c\approx 0\) fixes the first three terms in the cKdV equation \(u_{t}+uu_{x}+u_{xxx}+u/(2(t+1))=0\) and only changes the last term \(u/(2(t+1))\) by a negligible amount. These are not LPSs of the given equations, but the error of the PDE after transformation is smaller than the error of the numerical differentiation method. These are _approximate symmetries_ (AS), and the theory of AS is also of great interest in the symmetry analysis of PDEs [2; 1].

Augmentation results.We use the learned symmetries as data augmentation and train Fourier Neural Operators (FNOs). The detailed experiment setting is described in Appendix B.2. Since FNOs are extremely sensitive to numerical error, we employ Whittaker-Shannon interpolation, explained in Appendix D.2, to resample the transformed results. The results are depicted in Figure 6. In all cases, data augmentation using the learned symmetries improve the performance, almost close to the results using the ground truth symmetries. The detailed results are in Appendix C.2. Additionally, we verify that the approximate symmetry of cKdV is also beneficial to training, as shown in Table 6, especially when the numbers of data points is low, proving the effectiveness of symmetries extracted from data.

Ablations.Additional ablation studies on numerical methods, such as numerical differentiation and interpolation, and hyperparameter sensitivity are conducted, and their results are presented in Appendices E and F.

## 6 Conclusion

We have introduced a novel method for learning continuous symmetries, including non-affine transformations, from data without prior knowledge. By leveraging Neural ODE, our approach models one-parameter groups to generate a sequence of transformations, guided by a task-specific validity score function. This approach captures both affine and non-affine symmetries in image and PDE datasets, enhancing automatic data augmentation in image classification and neural operator learning for PDEs. The learned generators produce augmented training data that improve model performance, particularly with limited training data.

Limitation.However, despite its flexibility, our method requires careful selection of numerical methods, such as numerical differentiation and interpolation, to ensure stable training and the ODE integration can be computationally large for augmentation generation compared to other augmentation methods. While we focus on image symmetries and LPSs of PDEs, the method could potentially model other symmetries and domains with proper validity scores, suggesting future applications in learning complex symmetries, including conditional and non-local symmetries, in various data types.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**\# Data** & **None** & **LPS** & **AS** & **LPS+AS** \\ \hline \(2^{7}\) & \((3.70\pm 0.09)\times 10^{-6}\) & \((3.49\pm 1.06)\times 10^{-6}\) & \((3.20\pm 0.94)\times 10^{-6}\) & \((2.66\pm 0.36)\times 10^{-6}\) \\ \(2^{5}\) & \((7.90\pm 1.21)\times 10^{-4}\) & \((5.90\pm 0.17)\times 10^{-4}\) & \((4.45\pm 0.15)\times 10^{-4}\) & \((3.70\pm 0.15)\times 10^{-4}\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Test NMSE comparison of augmentation using LPS and AS in FNO learning for cKdV.

Figure 6: Comparison of augmentation performances using the ground truth symmetries and the learned symmetries with various numbers of data. The symbol \(\emptyset\) stands for no-augmentation.

## Acknowledgments and Disclosure of Funding

This work was partly supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (No.RS-2019-II190075, Artificial Intelligence Graduate School Program(KAIST), No.2022-0-00713, Meta-learning Applicable to Real-world Problems, No.2022-0-00184, Development and Study of AI Technologies to Inexpensively Conform to Evolving Policy on Ethics, No.RS-2024-00509279, Global AI Frontier Lab)

## References

* [1] Baikov, V., Gazizov, R., and Ibragimov, N. K. Approximate symmetry and formal linearization. _Journal of Applied Mechanics and Technical Physics_, 30(2):204-212, 1989.
* [2] Baumann, G. Approximate symmetries of partial differential equations. _Symmetry Analysis of Differential Equations with Mathematica(r)_, pp. 404-423, 2000.
* [3] Benton, G. W., Finzi, M., Izmailov, P., and Wilson, A. G. Learning invariances in neural networks from training data. In _Advances in Neural Information Processing Systems 35 (NeurIPS 2022)_, 2020.
* [4] Brandstetter, J., Welling, M., and Worrall, D. E. Lie point symmetry data augmentation for neural pde solvers. In _International Conference on Machine Learning_, pp. 2241-2256. PMLR, 2022.
* [5] Cantwell, B. J. _Introduction to symmetry analysis_. Cambridge University Press, 2002.
* [6] Chen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud, D. K. Neural ordinary differential equations. _Advances in neural information processing systems_, 31, 2018.
* [7] Cohen, T. and Welling, M. Group equivariant convolutional networks. In _International conference on machine learning_, pp. 2990-2999. PMLR, 2016.
* [8] Dangovski, R., Jing, L., Loh, C., Han, S., Srivastava, A., Cheung, B., Agrawal, P., and Soljacic, M. Equivariant self-supervised learning: Encouraging equivariance in representations. In _International Conference on Learning Representations (ICLR)_, 2022.
* [9] Dehmamy, N., Walters, R., Liu, Y., Wang, D., and Yu, R. Automatic symmetry discovery with lie algebra convolutional network. _Advances in Neural Information Processing Systems_, 34:2503-2515, 2021.
* [10] Desai, K., Nachman, B., and Thaler, J. Symmetry discovery with deep learning. _Physical Review D_, 105(9):096031, 2022.
* [11] Dey, N., Chen, A., and Ghafurian, S. Group equivariant generative adversarial networks. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_, 2021.
* [12] Dumbser, M. and Kaser, M. Arbitrary high order non-oscillatory finite volume schemes on unstructured meshes for linear hyperbolic systems. _Journal of Computational Physics_, 221(2):693-723, 2007.
* [13] Forestano, R. T., Matchev, K. T., Matcheva, K., Roman, A., Unlu, E. B., and Verner, S. Deep learning symmetries and their lie groups, algebras, and subalgebras from first principles. _Machine Learning: Science and Technology_, 4(2):025027, 2023.
* [14] Gabel, A., Quax, R., and Gavves, E. Data-driven lie point symmetry detection for continuous dynamical systems. _Machine Learning: Science and Technology_, 2024.
* [15] Garrido, Q., Najman, L., and LeCun, Y. Self-supervised learning of split invariant equivariant representations. In _Proceedings of The 40th International Conference on Machine Learning (ICML 2023)_, 2023.

* [16] Gruver, N., Finzi, M. A., Goldblum, M., and Wilson, A. G. The lie derivative for measuring learned equivariance. In _The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023_, 2023.
* [17] Hoarau, E. and David, C. Lie group computation of finite difference schemes. _arXiv preprint math/0611895_, 2006.
* [18] Hoogeboom, E., Satorras, V. G., Vignac, C., and Welling, M. Equivariant diffusion for molecule generation in 3d. In _Proceedings of The 39th International Conference on Machine Learning (ICML 2022)_, 2022.
* [19] Huang, Y., Peng, X., Ma, J., and Zhang, M. 3dlinker: An E(3) equivariant variational autoencoder for molecular linker design. In _Proceedings of The 39th International Conference on Machine Learning (ICML 2022)_, 2022.
* [20] Lee, J. M. _Smooth manifolds_. Springer, 2012.
* [21] Li, Z., Kovachki, N. B., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart, A. M., and Anandkumar, A. Fourier neural operator for parametric partial differential equations. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_, 2021.
* [22] Liu, Z. and Tegmark, M. Machine learning hidden symmetries. _Physical Review Letters_, 128(18):180201, 2022.
* [23] Mialon, G., Garrido, Q., Lawrence, H., Rehman, D., LeCun, Y., and Kiani, B. Self-supervised learning with lie symmetries for partial differential equations. _Advances in Neural Information Processing Systems_, 36:28973-29004, 2023.
* [24] Moskalev, A., Sepliarskaia, A., Sosnovik, I., and Smeulders, A. W. M. Liegg: Studying learned lie group generators. In _Advances in Neural Information Processing Systems 35 (NeurIPS 2022)_, 2022.
* May 3, 2018, Conference Track Proceedings_, 2018.
* [26] Oliveri, F. Lie symmetries of differential equations: Classical results and recent contributions. _Symmetry_, 2(2):658-706, 2010. ISSN 2073-8994. doi: 10.3390/sym2020658. URL https://www.mdpi.com/2073-8994/2/2/658.
* [27] Olver, P. J. _Applications of Lie groups to differential equations_, volume 107. Springer Science & Business Media, 1993.
* [28] Rao, R. and Ruderman, D. Learning lie groups for invariant visual perception. _Advances in neural information processing systems_, 11, 1998.
* [29] Shorten, C. and Khoshgoftaar, T. M. A survey on image data augmentation for deep learning. _Journal of big data_, 6(1):1-48, 2019.
* [30] Sil, S. Nonclassical symmetries, nonlinear self-adjointness, conservation laws and some new exact solutions of cylindrical kdv equation. _International Journal of Applied and Computational Mathematics_, 9(5):69, 2023.
* [31] Simard, P. Y., Steinkraus, D., Platt, J. C., et al. Best practices for convolutional neural networks applied to visual document analysis. In _Icdar_, volume 3. Edinburgh, 2003.
* [32] Sohl-Dickstein, J., Wang, C. M., and Olshausen, B. A. An unsupervised algorithm for learning lie group transformations. _arXiv preprint arXiv:1001.1027_, 2010.
* [33] Wang, R., Walters, R., and Yu, R. Incorporating symmetry into deep dynamics models for improved generalization. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_, 2021.

* [34] Yang, J., Walters, R., Dehmamy, N., and Yu, R. Generative adversarial symmetry discovery. In _Proceedings of The 40th International Conference on Machine Learning (ICML 2023)_, 2023.
* [35] Yang, J., Dehmamy, N., Walters, R., and Yu, R. Latent space symmetry discovery. In _International Conference on Machine Learning_, 2023.
* [36] Zabusky, N. J. and Kruskal, M. D. Interaction of" solitons" in a collisionless plasma and the recurrence of initial states. _Physical review letters_, 15(6):240, 1965.
* [37] Zhang, Y.-T. and Shu, C.-W. Eno and weno schemes. In _Handbook of numerical analysis_, volume 17, pp. 103-122. Elsevier, 2016.

Implementation Details

### Image Dataset

Notation.Images are functions of the form \(f:\mathcal{X}\rightarrow\mathcal{Y}\), where \(\mathcal{X}=[-1,1]^{2}\) is the spatial domain and \(\mathcal{Y}=[0,1]^{3}\) is the normalized RGB domain. Pixels \(\mathcal{X}_{\text{grid}}=\{\bm{x}_{i}\}_{i=1}^{N_{\text{grid}}}\) are discretized through a rectangular grid of \(\mathcal{X}\), and images \(f\) are discretized on the pixel space by \(f_{i}=f(\bm{x}_{i})\) for all \(i\).

\(\omega(\bm{x})\) in orthonormality loss.We first learn a symmetry on the spatial domain \(\mathcal{X}\) using a feature extractor \(H_{\text{hidden}}\) taken from a pre-trained neural network. One obstacle is that in most image datasets, the main subjects of images are mostly located around the centers, and the regions close to the boundary are filled with backgrounds. In other words, each pixel has a different level of importance, and we may end up learning infinitesimal generators that only move boundary pixels and fix the center.

We take into account the importance of pixels using the weight function \(\omega(x):\mathcal{X}\rightarrow\mathbb{R}_{\geq 0}\). For a discretized image \(f=\{f_{i}\}\) and for each pixel \(\bm{x}_{i}\), we measure a _pixel sensitivity_ of the image \(f\) at the \(i\)-th pixel up to the feature extractor \(H_{\text{feat}}\) as

\[\mathrm{Sensitivity}(f,\bm{x}_{i})=\left\|\frac{\partial H_{\text{feat}}(f)}{ \partial\bm{x}_{i}}\right\|=\left\|\frac{\partial f_{i}}{\partial\bm{x}_{i}} \frac{\partial H_{\text{feat}}(f)}{\partial f_{i}}\right\|,\] (15)

which can be computed by querying a Jacobian-vector product of \(H_{\text{feat}}\) with respect to the image gradient at each pixel \(\frac{\partial f_{i}}{\partial\bm{x}_{i}}\). We define the weight \(\omega(x)\) as the average of pixel sensitivity across the dataset:

\[\omega(\bm{x}_{i})=\mathbb{E}_{f\sim\mathcal{D}}[\mathrm{Sensitivity}(f,\bm{x} _{i})]=\mathbb{E}_{f\sim\mathcal{D}}\left[\left\|\frac{\partial H_{\text{feat} }(f)}{\partial\bm{x}_{i}}\right\|\right].\] (16)

However, computing this weight function \(\omega\) needs \(N_{\text{grid}}\cdot N_{\text{data}}\) times of computation of Jacobian-vector products. Instead, we use a Gaussian kernel \(\kappa(\bm{x};\hat{\bm{x}},\sigma)=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{ \left\|\bm{x}-\hat{\bm{x}}\right\|^{2}}{2\sigma^{2}})\), with a fixed \(\sigma=0.1\) and the center \(\hat{\bm{x}}\) sampled uniformly on \(\mathcal{X}\), and approximate the weight function by

\[\omega(\bm{x}_{i})\approx\mathbb{E}_{f\sim\mathcal{D},\hat{\bm{x}}\sim\mathrm{ Unif}(\mathcal{X})}\left[\kappa(\bm{x}_{i};\hat{\bm{x}},\sigma)\right\|\sum_{j} \frac{\partial H_{\text{feat}}(f)}{\partial\bm{x}_{j}}\kappa(\bm{x}_{j};\hat{ \bm{x}},\sigma)\right]\] (17)

so that we compute the weight in a stochastic manner. Intuitively, it computes Jacobian-vector product for each larger pseudo-pixel represented by the Gaussian kernels instead of each individual pixel. We iterate over the dataset 20 times, and the computed weight function is shown in Figure 7.

Training details.The infinitesimal generator \(\bm{h}_{\bm{\theta}}^{(a)}\) transforms each pixel \(\bm{x}_{i}\) into \(\tilde{\bm{x}}_{i}\) via ODE integration. In this process, the transformed pixels \(\{\tilde{\bm{x}}_{i}\}\) may no longer be located on the rectangular grid \(\mathcal{X}_{\text{grid}}\), so we use the bilinear interpolation method to resample the transformed image \(\vartheta_{s}(f)\) on \(\mathcal{X}_{\text{grid}}\). Note that the bilinear interpolation operation is differentiable, thereby we can train the MLP by minimizing the validity score \(S(\vartheta_{s},f)\), which is the cosine similarity between the features of \(f\) and \(\vartheta_{s}(f)\):

\[S\left(\vartheta_{s},f\right)=\mathrm{sim}\left(H_{\text{feat}}(\vartheta_{s} (f)),H_{\text{feat}}(f)\right).\] (18)

### PDE Dataset

Notation.Solutions of a 1D scalar-valued evolution equation on a periodic domain take the form \(u(x,t):\mathcal{X}\rightarrow\mathcal{U}\) for \(\mathcal{X}=[0,L]\times[0,T]\) and \(\mathcal{U}=\mathbb{R}\), where \([0,L]\) is the spatial domain and \([0,T]\) is the temporal domain. Similar to the image task, the domain \(\mathcal{X}\) is discretized by the rectangular grid \(\mathcal{X}_{\text{grid}}=\{\bm{x}_{i}\}_{i=1}^{N_{\text{grid}}}=\{(x_{i},t_{i} )\}_{i=1}^{N_{\text{grid}}}\). A discretized solution is a set of tuples \((x_{i},t_{i},u_{i})\) where \(u_{i}=u(x_{i},t_{i})\).

Figure 7: The weight function \(\omega(\bm{x})\).

\(\omega(\bm{x})\) in orthonormality loss.Unlike in images, we assume that all discretized grid points hold equal importance, so we set \(\omega(\bm{x})=1\) for all \(\bm{x}\in\mathcal{X}\).

Training details.The infinitesimal generators on the product space \(\mathcal{X}\times\mathcal{U}\) transform a point \(\bm{u}=\{(x_{i},t_{i},u_{i})\}\) into \(\tilde{\bm{u}}=\{(\tilde{x}_{i},\tilde{t}_{i},\tilde{u}_{i})\}\). Therefore, the transformed solutions are no longer on the rectangular grid. To compute the PDE value \(\Delta(\tilde{\bm{u}})\) for \(\tilde{\bm{u}}=\vartheta_{s}(\bm{u})\) in Equation 6, we need to compute partial derivatives numerically such as \(u_{x}(\tilde{x}_{i},\tilde{t}_{i})\), \(u_{t}(\tilde{x}_{i},\tilde{t}_{i})\) or \(u_{xx}(\tilde{x}_{i},\tilde{t}_{i})\). We use the _finite difference method_, in which the numerical derivative is approximated by finite differences, e.g.,

\[u_{x}(x_{i},t_{i})=\frac{u(x_{i}+\Delta x,t_{i})-u(x_{i}-\Delta x,t_{i})}{2 \Delta x}.\] (19)

for some small \(\Delta x\).

In particular, we use the weighted essentially non-oscillating (WENO) scheme as a numerical differentiation method [37], with a careful choice of parameters as in Dumbser & Kaser [12]. In the WENO method, multiple estimates for derivatives are made using multiple sets of neighboring gridpoints (called _stencils_). The multiple estimates are then averaged with weights (called _smoothness indicator_) that approximate how stable the derivative estimates are. We implement the WENO method working on a nonuniform grid, and the model learns symmetries by backpropagating through it. A detailed description on the WENO scheme is in Appendix D.1.

## Appendix B Experiment Details

In this section, we present the detailed experimental settings of the experiments in SS 5.

### Images

Training ResNet-18.When training the ResNet-18 with CIFAR-10, both the feature extractor \(H_{\text{feat}}\) and models after augmentation, we train the model in 200 epochs with a batch size 128. The learning rate is set to \(10^{-1}\) and decreases by a factor of \(0.2\) at the 60th, 120th, and 160th epoch. The model is trained by SGD optimizer with Nesterov momentum 0.9 and weight decay 0.4.

Learning symmetries.To learn the symmetry generators, we train the MLP using two shared hidden layers, each with a width of 256, followed by a hidden layer of width 32 for each output vector field. We use the swish activation function to ensure the learned vector fields are smooth. The MLP is trained for 50 epochs with a batch size 128 and fixed learning rate of \(10^{-4}\) using the Adam optimizer. The learning process takes less than 10 hours on a GeForce RTX 2080 Ti GPU.

### PDEs

Data generation.We follow the data generation method of Brandstetter et al. [4]. Given an 1D evolution equation \(u_{t}=F(x,t,u,u_{x},u_{xx},\cdots)\) for \(u=u(x,t)\) on periodic domain \([0,L]\), we start with an initial condition \(u(x,0)\) by random Fourier series as

\[u(x,0)=\sum_{p=1}^{P}A_{p}\sin(2\pi l_{p}x/L+\phi_{p})\] (20)

where \(P\) is the number of Fourier modes and \((A_{p},l_{p},\phi_{p})\) are random coefficients. For time-stepping, we compute \(x\)-derivatives \(u_{x},u_{xx},\cdots\) using pseudospectral method, which computes derivatives in Fourier domain and converts them back to the original domain. We use an ODE solver to compute the time evolution of \(u(x,t)\) from \(t=0\) to \(t=T\). The solution is discretized on regular grid of size \(N_{x}\times N_{t}\) on \([0,L]\times[0,T]\), where \(N_{x}=256\) and \(N_{t}=140\) by default.

When simulating Burgers' equation, we instead solve the PDE for the heat equation \(\phi_{t}=\phi_{xx}\) for \(\phi=\nu\phi(x,t)\). The Burgers' equation \(u(x,t)\) and the heat equation \(\phi(x,t)\) is related via the Cole-Hopf transformation:

\[u=2\nu\frac{\partial}{\partial x}\log(x).\] (21)

After data generation, we transform the heat equation back to the Burgers' equation.

Learning symmetries.We use an MLP with the same architecture as described in Appendix B.1. Since all the tuples \((x,t,u)\) must pass through the MLP, totaling \(N_{x}\times N_{t}\) for each data instance, we set a small batch size 4. Symmetries are learned using 1024 data instances over 50 epochs. We use the Adam optimizer and train the network with a learning rate of \(10^{-4}\) in the first 25 epochs and \(10^{-5}\) for the remaining 25 epochs. The learning process also takes less than 10 hours on a GeForce RTX 2080 Ti GPU.

Sobolev regularization.To ensure smoothness in the vector fields, we apply additional regularization using the Sobolev norm of order 2 in the \(x\)-domain. For a vector field \(V(x,t)\), the Sobolev norm can be efficiently computed in Fourier domain:

\[\|V(\cdot,t)\|_{2,2}^{2}=\sum_{i=0}^{2}\left\|\frac{\partial^{i}V(\cdot,t)}{ \partial x^{i}}\right\|_{2}^{2}=\sum_{n=0}^{N_{x}-1}\left(1+|\frac{n_{\text{ freq}}}{L}|\right)^{2}\hat{V}(n,t)\] (22)

where \(n_{\text{freq}}=\min(n,N_{x}-n)\) and \(\hat{V}(\cdot,t)\) is the discrete Fourier transformation of \(V(\cdot,t)\). Since we already enforce \(||V||=1\), we penalize towards the Sobolev norm excluding the zeroth order term:

\[\|V(\cdot,t)\|_{2,2}^{2}-\|V(\cdot,t)\|_{2}^{2}=\sum_{i=1}^{2}\left\|\frac{ \partial^{i}V(\cdot,t)}{\partial x^{i}}\right\|_{2}^{2}=\sum_{n=0}^{N_{x}-1} \left(\left(1+|\frac{n_{\text{freq}}}{L}|\right)^{2}-1\right)\hat{V}(n,t).\] (23)

We apply the Sobolev regularization during the final 10 epochs.

Training FNOs.For the KdV and KS equations, we train an autoregressive FNO solver, which takes 20 timesteps as input and predicts the subsequent 20 timesteps, following the experimental setup of Brandstetter et al. [4]. For the nKdV and cKdV equations, which are time-dependent and contain explicit \(t\) terms, we train FNOs as single-time neural operators. These models utilizes the initial conditions of the equations to predict the states after 70 timesteps. We train the FNO over 40 epochs, with each epoch comprising 280 iterations across the dataset for the KdV and KS equation and 100 iterations for the nKdV and cKdV equations. The learning rate begins at \(10^{-4}\) and decreases by a factor of \(0.4\) every 10 epochs.

## Appendix C Experiment Results

In this section, we provide detailed results of the experiments outlined in SS 5. For symmetry learning tasks, we conducted each experiment three times and randomly selected one for reporting in SS 5. The complete set of results is provided here. Also, we report the detailed evaluation metrics in augmentation tasks.

### Images

Figure 8 is a visualizations of the learned symmetries discussed in SS 5.1. Figure 9 displays results from experiments conducted under the same settings but with different model initializations compared to Figure 4 in SS 5.1. It is notable that affine symmetries consistently occupy the first six slots across all experiments.

### PDEs

We report experiments results for learning symmetries, conducting three trials for each equation in figure Figure 10. The ground truth symmetries and the approximate symmetries are consistently found in the former slots in all the experiments. The remaining fourth slots occasionally converge to learned symmetries despite the orthonormality loss or converge to some unknown vector fields with high validity scores. The results from the first trials are used in the augmentation experiments.

In Table 7 and Table 8, we provide the augmentation results of FNOs using the learned symmetries, which are illustrated in Figure 6.

## Appendix D Technical Details

### Weighted Essentially Non-Oscillating (WENO) Scheme

This section is largely based on Zhang & Shu [37], Dumbser & Kaser [12].

WENO scheme in 1D.Consider a smooth function \(f:\mathcal{X}\subseteq\mathbb{R}\rightarrow\mathbb{R}\), where we only have access to the function on the grid \(\mathcal{X}_{\text{grid}}=\{x_{1},\cdots,x_{N_{\text{\mu i}}}\},x_{1}<\cdots<x _{N_{\text{\mu i}}}\). To estimate the derivatives \(f^{(k)}(x)=\frac{d^{k}f}{dx^{k}}(x)\) for some \(x\in\mathcal{X}\), we use \(k+1\) neighboring points in \(I=\{x_{i},\cdots,x_{i+k}\}\) and compute the unique \(k\)-th order polynomial \(p_{I}(x)\) that interpolates the function \(f\) on the points in \(I\). In other words, we ensure \(p_{I}(x_{i})=f(x_{i}),\cdots,p_{I}(x_{i+k})=f(x_{i+k})\). Then we can pick up the estimates of the derivatives using the polynomial as \(f^{(k)}(x)\approx p_{I}^{(k)}(x)\). We call the set of neighboring points \(I\) a _stencil_, and the polynomial \(p_{I}\) the _reconstruction polynomial_. Usage of reconstruction polynomials is a fundamental concept in the numerical differentiation method known as the _finite difference method_.

In the weighted essentially non-oscillating (WENO) scheme, we compute multiple estimates \(p_{I_{1}}^{(k)}(x),\cdots,p_{I_{N_{s}}}^{(k)}(x)\) for the derivatives \(f^{(k)}(x)\) using multiple stencils \(I_{1},\cdots,I_{N_{s}}\). The estimates

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{3}{c}{**KdV**} & \multicolumn{3}{c}{**KS**} \\ \cline{2-7} \# **Data** & **None** & **Ground-truth** & **Ours** & **None** & **Ground-truth** & **Ours** \\ \hline \(2^{9}\) & \(0.398\pm 0.013\) & \(0.0640\pm 0.0050\) & \(0.0824\pm 0.0052\) & \(0.00693\pm 0.00039\) & \(0.00229\pm 0.00014\) & \(0.00614\pm 0.00051\) \\ \(2^{7}\) & \(1.42\pm 0.05\) & \(0.246\pm 0.012\) & \(0.283\pm 0.018\) & \(0.324\pm 0.031\) & \(0.0241\pm 0.0001\) & \(0.0422\pm 0.0002\) \\ \(2^{5}\) & \(4.47\pm 0.26\) & \(0.980\pm 0.039\) & \(1.12\pm 0.10\) & \(5.78\pm 0.13\) & \(1.14\pm 0.10\) & \(1.37\pm 0.04\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Comparison of augmentation methods with different numbers of data for KdV and KS

Figure 8: (a) The learned vector fields. (b) Transformed images using the learned generators. The images with transformation scale 0 are the original images.

are then averaged with weights \(\omega_{1},\cdots,\omega_{N_{s}}\) which take account of the quality of each estimation. In the WENO scheme, we assume the grid points are sufficiently dense, hence the estimates are more accurate when the reconstruction polynomials \(p_{I_{m}}\) are smooth. The smoothness of \(p_{I_{m}}\) is measured by the _smoothness indicator_, defined as:

\[\mathrm{IS}_{I_{m}}=\sum_{\alpha=1}^{k}\int_{\Delta}|\Delta|^{\alpha-1}(p_{I_{ m}}^{(\alpha)}(x))^{2}dx\] (24)

where \(\Delta=(x_{j},x_{j+1})\subset\mathbb{R}\) is an interval between two grid points containing \(x\), and \(|\Delta|\) is the length of \(\Delta\). The weights \(\omega_{1},\cdots,\omega_{N_{s}}\) are defined as:

\[\omega_{m}=\frac{\alpha_{m}}{\sum_{m^{\prime}}\alpha_{m^{\prime}}},\quad \alpha_{m}=\frac{\gamma_{m}}{(\epsilon+\mathrm{IS}_{I_{m}})^{b}}\] (25)

where \(\gamma_{m}\) is called the _linear weight_, usually chosen heuristically, \(b\) is a positive integer usually set to 2 or 4, and \(\epsilon>0\) is a small positive number preventing the denominator from being zero. The final estimate for the derivative \(f^{(k)}(x)\) is computed by averaging the estimates \(p_{I_{1}}^{(k)}(x),\cdots,p_{I_{N_{s}}}^{(k)}(x)\) with the weights \(\omega_{1},\cdots,\omega_{N_{s}}\):

\[\hat{f}^{(k)}(x)=\sum_{m=1}^{N_{s}}\omega_{m}\cdot p_{I_{m}}^{(k)}(x).\] (26)

WENO scheme for multivariate function.The WENO scheme extends smoothly to multidimensional function \(f:\mathbb{R}^{n}\rightarrow\mathbb{R}\). For \(\bm{k}=(k_{1},\cdots,k_{n})\in\mathbb{Z}_{\geq 0}^{n}\), we denote \(f^{(\bm{k})}(x)=\frac{\partial^{\bm{k}}f}{\partial x_{1}^{\kappa_{1}}\cdots \partial x_{n}^{\kappa_{n}}}(x)\). On a stencil \(I\) with an appropriate number of grid points, we compute the reconstruction polynomial \(p_{I}(\bm{x})\) with a nonzero \(\bm{k}\)-degree term, and the smoothness indicator is defined similarly as:

\[\mathrm{IS}_{I}=\sum_{1\leq|\bm{\alpha}|\leq k}\int_{\Delta}|\Delta|^{|\bm{ \alpha}|-1}(p_{I_{m}}^{(\bm{\alpha})}(\bm{x}))^{2}d\bm{x}\] (27)

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & \multicolumn{2}{c}{**nKdV**} & \multicolumn{2}{c}{**c}{**cKdV**} \\ \cline{2-5} \# Data & **None** & **Ours** & **None** & **Ours** \\ \hline \(2^{9}\) & \((3.52\pm 0.11)\times 10^{-3}\) & \((2.58\pm 0.17)\times 10^{-3}\) & \((3.72\pm 0.33)\times 10^{-6}\) & \((3.44\pm 0.37)\times 10^{-6}\) \\ \(2^{7}\) & \((3.16\pm 0.06)\times 10^{-2}\) & \((2.96\pm 0.50)\times 10^{-2}\) & \((3.71\pm 0.09)\times 10^{-5}\) & \((1.86\pm 0.10)\times 10^{-5}\) \\ \(2^{5}\) & \((4.79\pm 0.09)\times 10^{-1}\) & \((2.44\pm 0.11)\times 10^{-1}\) & \((7.90\pm 1.21)\times 10^{-4}\) & \((2.99\pm 0.37)\times 10^{-4}\) \\ \hline \hline \end{tabular}
\end{table}
Table 8: Comparison of augmentation methods with different numbers of data for nKdV and cKdV

Figure 9: The same results (learned symmetries of images) under different initializations compared to Figure 4.

where \(\bm{\alpha}=(\alpha_{1},\cdots,\alpha_{n})\in\mathbb{Z}_{\geq 0}^{n}\), \(|\bm{\alpha}|=\sum_{i=1}^{n}\alpha_{i}\) and the interval \(\Delta\) is now a \(n\)-dimensional cell containing the point \(\bm{x}\) with volume \(|\Delta|\). Like the 1D case, the weights are defined using the linear weights and smoothness indicator.

Choice of parameters.We follow the choice of parameters of Dumbser & Kaser [12]. The linear weight \(\gamma_{m}\) is chosen to be 100 if \(\bm{x}\) is inside the center cell of \(I_{m}\) and 1 otherwise, reflecting the fact that the estimates are accurate as much as the point \(\bm{x}\) is close to the center of \(I_{m}\). The parameters \(b\) and \(\epsilon\) are chosen as \(b=4\) and \(\epsilon=10^{-6}\).

### Whittaker-Shannon Interpolation on a Periodic Domain

Let \(\cdots,f[-1],f[0],f[1],\cdots\) be a discretization of a continuous signal on a real line. The Whittaker-Shannon interpolation recovers the original signal \(f\) as:

\[f(t)=\sum_{n=-\infty}^{\infty}f[n]\mathrm{sinc}(t-n)\] (28)

where \(\mathrm{sinc}\) is the normalized sinc function \(\mathrm{sinc}(t)=\frac{\mathrm{sinc}(\pi x)}{\pi x}\). The _Nyquist-Shannon sampling theorem_ states that states that \(f(t)\) is the _perfect reconstruction_ of \(f\), in a sense that if the original function does not contain any frequencies higher than a certain threshold, called the _Nyquist frequency_, then \(f\) is perfectly reconstructs the original signal.

Figure 10: Inner product comparison of the learned symmetry generators with the ground truth symmetries. (a), (b), and (c) are conducted with different random initializations.

Similarly, if \(f[0],f[1],\cdots,f[N-1]\) is a discretization of a continuous signal on a periodic domain \([0,N]\) for some _even_ positive integer \(N\), the Whittaker-Shannon interpolation is given as:

\[f(t)=\sum_{n=-\infty}^{\infty}f[n\text{ mod }N]\mathrm{sinc}(t-n)=\sum_{n=0}^{N-1 }f[n]\sum_{n=-\infty}^{\infty}\mathrm{sinc}(t-mN)=\sum_{n=0}^{N-1}f[n]D_{N}(t)\] (29)

where \(D_{N}(t)=\sum_{n=-\infty}^{\infty}\mathrm{sinc}(t-mN)\) is the _Dirichlet kernel_ with period \(N\). The Dirichlet kernel \(D_{N}(t)\) has a closed-form expression

\[D_{N}(t)=\frac{\sin(\pi t)}{N\tan(\pi t/N)}.\] (30)

### Nonlinear Time Transformation of the KdV Equation

We apply a nonlinear time transformation \(t\) defined as \(t=t_{0}(e^{\frac{\dot{t}}{t_{0}}}-1)\), where \(t_{0}\) is a constant scaling factor, to the KdV equation \(u_{t}+uu_{x}+u_{xxx}\). The derivative with respect to \(t\) is transformed to

\[\frac{\partial}{\partial t}=\frac{\partial\dot{t}}{\partial t}\frac{\partial }{\partial\dot{t}}=e^{\frac{\dot{t}}{t_{0}}}\frac{\partial}{\partial\dot{t}}\] (31)

hence the KdV equation with nonlinear time transformation (nKdV) becomes

\[e^{\frac{\dot{t}}{t_{0}}}u_{\dot{t}}u_{t}+uu_{x}+u_{xxx}=0.\] (32)

The three symmetries of the KdV changes accordingly. The space translation is left untouched, and the other two infinitesimal generators are tranformed as:

\[\frac{\partial}{\partial t} =e^{\frac{\dot{t}}{t_{0}}}\frac{\partial}{\partial\dot{t}}\] (33) \[t\frac{\partial}{\partial x}+\frac{\partial}{\partial u} =t_{0}(e^{\frac{\dot{t}}{t_{0}}}-1)\frac{\partial}{\partial x}+ \frac{\partial}{\partial u}\] (34)

where we used the derivative notation \((1,0,0)=\frac{\partial}{\partial x}\), \((0,1,0)=\frac{\partial}{\partial t}\) and \((0,0,1)=\frac{\partial}{\partial u}\).

## Appendix E Ablation Studies

### Numerical differentiation.

When searching for symmetries of PDEs, we use the WENO scheme as a numerical differentiation method. We have found that the choice of numerical differentiator is critical, as gradients must flow through numerical differntiation during the backpropagation step. To compare different methods, we experimented with using the WENO scheme and another method: bilinear interpolation followed by discrete numerical differentiation on a regular rectangular grid.. When using bilinear interpolation followed by discrete differentiation, the loss failed to converge well, even with very small learning rate \(10^{-6}\). The learned vector fields were not orthogonal, and they only spanned the time translation and the space translation at best, as in Figure 11. These experiments were conducted using the KS equation.

We hypothesize that the bilinear interpolation distributes each transformed point into two adjacent grid points, but discrete differentiation is done by repeatedly subtracting values of two adjacent grid points, so the gradient may not flow well during the backpropagation.

### Whittaker-Shannon interpolation.

In the PDE augmentation task, when the transformed PDEs are resampled into the regular rectangular grid, we use Whittaker-Shannon instead of more commonly used bilinear interpolation. This step is also crucial for training FNOs with transformed data, since FNOs are extremely sensitive to numerical error and hence any aliasing effects must be avoided. We compared the augmentation results using Whittaker-Shannon interpolation with those using bilinear interpolation, using the KS equation with 512 training data and report the result in Table 9. The results clearly demonstrate that bilinear interpolation adversely affects the training FNO models.

## Appendix F Hyperparameter analysis

In this section, we analyze the roles of various hyperparameters in our learning scheme. We use experiment setting of the KS equation in SS 5.2. We selected the KS equation for analysis as it proved to be the most challenging in learning symmetries, possibly due to its fourth-order derivative term \(u_{xxxx}\). In particular, among the three symmetries (space translation, time translation, Galilean boost) the algorithm easily learns the space and time translation in a few epochs, but learning the Galilean boost takes more epochs as in Figure 12. We evaluate the results by examining whether the three symmetries are correctly found in the first three slots.

### Lipschitz Threshold

The Lipschitz threshold \(\tau\) in Equation 13 is the only parameter that constrains the function space in which we search for the symmetries. The threshold \(\tau\) is set to \(\tau=3\) in the main experiments. We conduct additional experiments with \(\tau=1,2,4,8,16,32\). Surprisingly, in all experiments, regardless of the value of \(\tau\), we found the three ground truth symmetries.

### Transformation Scale

The transformation scale \(\sigma\) controls how much we transform the data when searching for symmetries. The tuple \((x,t,u)\) in \(\mathcal{X}\times\mathcal{U}\) is scaled so that \(x\) and \(t\) form a uniform grid on \([0,1]^{2}\), and \(u\) is scaled so that the standard deviation of \(u\) closely matches that of \(x\) and \(t\), which is approximately \(0.29\). Hence, the default transformation scale \(\sigma=0.4\) transforms data with slightly more than its standard deviation at most. We conduct experiments with various transformation scales \(\sigma=0.1,0.2,0.4,0.6,0.8\). The ground truth symmetries were found when \(\sigma=0.1,0.2,0.4\). When \(\sigma=0.6\), the Galilean boost was found but allocated in the fourth slot, meaning that the learning was unstable. When \(\sigma=0.8\), the model only found space and time translation correctly.

Figure 11: Experiment using using bilinear interpolation and discrete differentiation. (a) Learning curve. (b) Learned symmetries.

\begin{table}
\begin{tabular}{c c c} \hline \hline  & **No-aug** & **Whittaker-Shannon** & **Bilinear** \\ \hline Test NMSE & \(0.00693\pm 0.00039\) & \(0.006143\pm 0.00051\) & \(0.542\pm 0.051\) \\ \hline \hline \end{tabular}
\end{table}
Table 9: Test NMSE comparison of augmentation using Whittaker-Shannon interpolation and bilinear interpolation.

Figure 12: Learning curves of four symmetry generators.

### Weights of Three Losses

Although the three loss terms Equation 14 are designed to have dimensionless scales, tuning the three loss terms in appropriate ranges is inevitable.We perform a grid search over the three weights \(w_{\text{sym}},w_{\text{ortho}},w_{\text{Lipschitz}}\) of the total loss in a fixed grid \([1,3,10]^{3}\). We report the number of correctly learned symmetries across different values of \(w_{\text{sym}}\), \(w_{\text{Lips}}\), and \(w_{\text{ortho}}\) in Table 10. The number 3 is the maximum number of symmetries to be discovered. We found that the ratio between \(w_{\text{sym}}\) and \(w_{\text{ortho}}\) significantly affects the results. In this case, the weight \(w_{\text{ortho}}\) should be larger than \(w_{\text{sym}}\) to prevent the learning of redundant infinitesimal generators.

## Appendix G Color-space Results

Recall that in our formulation, images take the form \(\mathcal{X}\rightarrow\mathcal{Y}\) where \(\mathcal{X}=[-1,1]^{2}\) is the pixel space and \(\mathcal{Y}\in\mathbb{R}^{3}\) is the RGB color space. Our formulation is not limited to searching for the symmetries on the 2D plane \(\mathcal{X}\), but is capable of learning symmetries in the color space \(\mathcal{Y}\). In this section, we elaborate how we modeled learning scheme for color-space symmetries and present the results.

Figure 14: Experiments with various values of hyperparameter \(\sigma\).

Figure 13: Experiments with various values of hyperparameter \(\tau\).

### Learning Scheme

Similar to other cases, we model one-parameter groups on \(\mathcal{Y}\) by a neural ODE, integrating over 3D vector fields \(\mathcal{Y}\to\mathbb{R}^{3}\) parametrized by a neural network. Colors are non-uniform - for example, black, brown and gray appear much more frequently than pink or yellow in CIFAR-10 dataset. To address this, we first form a grid \(\mathcal{Y}_{\text{grid}}\) on the color space \(\mathcal{Y}\) by randomly sampling 1024 colors from the CIFAR-10 dataset. Then we define the weight function \(w:\mathcal{Y}\to\mathbb{R}\), like the weight function in Equation17, by the _color sensitivity_ of the neural network \(H_{\text{text}}\). For a grid point \(\bm{y}_{i}\in\mathcal{Y}_{\text{grid}}\), the weight \(\omega(\bm{y}_{i})\) is defined as:

\[\omega(\bm{y}_{i})=\mathbb{E}_{f\sim\mathcal{D}}\left[\left\|\frac{\partial H_ {\text{text}}(f)}{\partial\bm{y}_{i}}\right\|\right].\] (35)

To estimate the magnitude of the gradient with respect to the change of \(\bm{y}_{i}\), we use neighboring points \(\bm{y}_{j}\in\mathrm{nbhd}(\bm{y}_{i})\subset\mathcal{Y}_{\text{grid}}\). For a discretized image \(\{\bm{x}_{i},f(\bm{x}_{i})\}_{x_{i}\in\mathcal{X}_{\text{grid}}}\), we use the nearest neighbor

\begin{table}

\end{table}
Table 10: The number of correctly learned symmetries across values of \(w_{\text{sym}}\), \(w_{\text{Lips}}\), and \(w_{\text{ortho}}\).

Figure 15: Learned color-space transformations.

algorithm to collect \(f(\bm{x}_{i})\in\mathrm{Nearest}(\bm{y}_{i})\) close to \(\bm{y}_{i}\), and measure how much \(H_{\text{text}}\) changes when the color \(f(\bm{x}_{i})\) shifts towards the direction \(\bm{y}_{j}-\bm{y}_{i}\):

\[\left|\left|\frac{\partial H_{\text{text}}(f)}{\partial\bm{y}_{i}}\right| \right|=\frac{1}{|\mathrm{nbhd}(\bm{y}_{i})|}\sum_{\bm{y}_{j}\in\mathrm{nbhd}( \bm{y}_{j})}\left\|(\bm{y}_{j}-\bm{y}_{i})\cdot\nabla_{\mathrm{1}_{\mathrm{Nearest }(\bm{y}_{i})}}H_{\text{text}}(f)\right\|\] (36)

where \(\mathbbm{1}_{\mathrm{Nearest}(\bm{y}_{i})}\) is a vector whose \(i\)-th entry is 1 if \(f(\bm{x}_{i})\in\mathrm{Nearest}(\bm{y}_{i})\) and 0 otherwise, and \(\nabla_{\mathrm{1}_{\mathrm{Nearest}(\bm{y}_{i})}}H_{\text{text}}(f)\) is computed by Jacobian-vector product.

Once the weight function is computed, we learn symmetries by optimizing through the validity score \(S\), defined by cosine similarity of features of the learned neural network. Since there are (batch size) \(\cdot\) (number of pixels) number of colors in a single batch of images, it's impractical to feed all of them directly into the MLP. Instead, we use the grid \(\mathcal{Y}_{\text{grid}}\), and compute the vector field values on the grid and interpolate them using the nearest neighbor algorithm.

### Results

We open three slots for color-space symmetries and train the model with \(w_{\text{sym}},w_{\text{Lips}}=1\) and \(w_{\text{ortho}}=3\). We found the brightness control, color contrast and blue-yellow shift respectively as in Figure 15.

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We clarified our contribution in the last two sentences in the abstract and the last paragraph in the introduction. Both sections mentioned that our scope is learning continuous symmetry with minimal prior knowledge. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]Justification: We mentioned our limitations in the last two sentences in the conclusion. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Although we did not provide any theorems or propositions, which requires proofs, all formulas are numbered and cross-referenced. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We checked the reproducibility of the reported experiments and we provided the hyperparameter analysis in the paper and we shared our source code. Guidelines: ** The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We attached our source code and we used public datasets, which are freely accessed by anyone. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.

* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All hyperparameters used in the experiments are provided in the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We measured the performance with 3-5 repeats. In Table 1 and Table 4, we reported the standard deviation, and in Figure 6(B), we described it using the error bar. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We also mentioned such information in the appendix. Guidelines: * The answer NA means that the paper does not include experiments.

* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We checked the code of ethics and our paper does not cause any hazard because it only explores theoretical symmetries. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Since we explore only theoretical symmetries present in mathematical space, our work is far from the societal impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?Answer: [NA] Justification: We do not have such risks. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We properly credited when adopting the existing codes and properly cited the original paper that produced the code and dataset. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We shared our code with anovmization and documentation. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects**Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve any potential risks incurred by study participants Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.