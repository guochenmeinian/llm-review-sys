# Implicit Causal Representation Learning via

Switchable Mechanisms

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Learning causal representations from observational and interventional data in the absence of known ground-truth graph structures necessitates implicit latent causal representation learning. Implicit learning of causal mechanisms typically involves two categories of interventional data: hard and soft interventions. In real-world scenarios, soft interventions are often more realistic than hard interventions, as the latter require fully controlled environments. Unlike hard interventions, which directly force changes in a causal variable, soft interventions exert influence indirectly by affecting the causal mechanism. However, the subtlety of soft interventions impose several challenges for learning causal models. One challenge is that soft intervention's effects are ambiguous, since parental relations remain intact. In this paper, we tackle the challenges of learning causal models using soft interventions while retaining implicit modeling. Our approach models the effects of soft interventions by employing a _causal mechanism switch variable_ designed to toggle between different causal mechanisms. In our experiments, we consistently observe improved learning of identifiable, causal representations, compared to baseline approaches.

## 1 Introduction

One of the long-standing challenges in causal representation learning is how to recover the ground-truth causal graph of a system solely from observations. Termed the _identifiability of causal models_ problem, this endeavor is crucial. Without achieving identifiability, we risk erroneously attributing causal relationships to learned representations. Furthermore, statistical models can masquerade as Directed Acyclic Graphs (DAGs) where edges lack causal significance, further complicating our pursuit.

When considering the challenge of identifying causal models, it is known that the Markov condition in graphs is insufficient for this task [26]. Thus, without additional assumptions or data, we find ourselves limited to learning only a _Markov Equivalence Class_ (MEC) of the causal model. Existing works have made different assumptions about availability of ground-truth causal variables labels [34], model parameters [1], availability of paired interventional data [3, 31], and availability of intervention targets [17] to ensure identifiability of causal models.

Figure 1: Difference between hard interventions and soft interventions: As seen in the middle row, hard interventions sever connections with parents. Therefore, an object’s class cannot have any effect on the object’s color when we intervene on color. On the other hand, soft interventions, as shown in the bottom row, allow for such effects.

Interventional data are usually obtained through _soft_ or _hard_ interventions. Hard interventions usually involve controlled experiments and they severe the connection of an intervened variable with its parents [24]. In terms of Structural Causal Models (SCM), hard interventions set the causal mechanism relating a causal variable to its parents, to a constant. Due to ethical or safety reasons, it may not be possible to perform hard interventions in many real-world applications. On the other hand, the effects of soft interventions are more subtle since parent variables can still affect their children. These effects can be modeled by a change in the set of parents, the causal mechanisms, and the exogenous variables [7]. Consequently, hard interventions can also be seen as a special case of soft interventions where the causal mechanism is set to a constant. Illustrated in Figure 1, a prominent challenge in causal representation learning lies in dealing with the ambiguity surrounding the effects of soft interventions. The observed alterations in object colors fail to distinctly elucidate whether they stem from parental influences or the applied interventions.

Additionally, a lack of comprehension regarding causal graphs can pose significant challenges in causal representation learning. In certain applications, the causal graph can be constructed using domain knowledge, allowing us to subsequently learn the causal variables [2; 18; 20]. However, this is not universally applicable, necessitating the direct learning of the causal graph itself. In a Variational AutoEncoder (VAE) framework, there are generally two approaches for causal representation learning: Explicit Latent Causal Models (ELCMs) [34; 1; 35; 37; 17; 15] and Implicit Latent Causal Models (ILCMs) [3]. In ELCMs, the latents are the causal variables and the adjacency matrix of the causal graph is parameterized and integrated into the prior of the latents such that the prior of latents is factorized according to the Causal Markov Condition [27]. This approach to causal representation learning is highly susceptible to becoming stuck in local minima as it is hard to learn representations without knowing the graph, and it is hard to learn the graph without knowing the representations. ILCMs [3] were introduced to circumvent this "chicken-and-egg" problem by using _solution functions_, which can implicitly model edges in the causal graph rather than explicitly modeling the entire adjacency matrix of the causal model. In ILCMs _the latents are the exogenous variables_ and the there is no explicit parameterization for the graph.

In implicit causal representation learning, the task involves recovering the exogenous variables \(\mathcal{E}\) from observed variables \(\mathcal{X}\) and learning solution functions. In [3], interventions are assumed to be hard, but this is often unrealistic and does not align with real-world problems. **In this paper, we propose a novel approach for Implicit Causal Representation Learning via Switchable Mechanisms (ICRL-SM).** We will introduce the _causal mechanism switch variable_ as a way of modeling the effect of soft interventions and identifying the causal variables. Our experiments on both synthetic and large real-world datasets, highlight the efficacy of proposed method in identifying causal variables and promising future directions in implicit causal representation learning. Our key contributions can be summarized as follows:

**I.** A novel approach for implicit causal representation learning with soft interventions.

**II.** Employing causal mechanisms switch variable to model the effect of soft interventions.

**III.** Theory for identifiability up to reparameterization from soft interventions.

## 2 Related Work

Causal representation learning has recently garnered significant attention [27; 14]. The primary challenge in this problem lies in achieving identifiability beyond the Markov equivalence class [26]. Solely relying on observational data necessitates additional assumptions regarding causal mechanisms, decoders, latent structure, and the availability of interventional data [22; 28; 36; 25; 15; 1; 40; 13; 34]. Recent works have focused on identifying causal models from collected interventional data instead of making strong assumptions about functions of the causal model. Interventional data facilitates identifiability based on relatively weak assumptions [1; 6; 3; 39; 33]. This type of data can be further categorized based on whether it involves soft or hard interventions, and whether the manipulated variables are observed and specified or latent. Our focus in this paper is on examining soft interventions, encompassing both observed and unobserved variables.

### Explicit models vs. Implicit models

Table 1 presents a comparison of the assumptions and identifiability results between our proposed theory and other related works on causal representation learning with interventions. In causal representation learning with interventions, one approach assumes a given causal graph and concentrates on identifying causal mechanisms and mixing functions. For instance, Causal Component Analysis (CauCA) [33] explores soft interventions with a known graph. Alternatively, when the graph is

not provided, explicit models seek to reconstruct it from interventional data [6; 17], potentially resulting in a chicken-and-egg problem in causal representation learning [3]. Current methods face the challenge of simultaneously learning the causal graph and other network parameters, especially in the absence of information about causal variables or the graph. Addressing these challenges, [3] recently introduced ILCM, which performs _implicit_ causal representation learning exclusively using _hard_ intervention data. In contrast, our approach introduces a novel method for learning an implicit model from _soft_ interventions. [3] describes methods for extracting a causal graph from a learned implicit model, which could be applied to our method as well. In our experiments, we will compare our method with ILCM and dVAE [21], given their implicit nature and similar experimental settings and assumptions. Additionally, to showcase the superiority of our method over explicit models, we will employ explicit causal model discovery methods like ENCO [16] and DDS [5], in conjunction with various variants of \(\beta\)-VAE.

### Hard interventions vs Soft interventions

The identification of explicit causal models from hard interventions has been extensively explored. [29] investigate causal disentanglement in linear causal models with linear mixing functions under hard interventions. Similarly, [4] focus on identifying causal models with linear causal mechanisms and nonlinear mixing functions, also utilizing hard interventions. In a more general setting with non-parametric causal mechanisms and mixing functions, [32] examine the identifiability of causal models, utilizing multi-environment data from unknown interventions. Similarly, [2] explore identifiability of causal models using multi-environment data from unknown interventions. [30] investigate the identifiability of causal models with nonlinear causal mechanisms and linear mixing functions, considering both hard and soft interventions.

Recent work has expanded the concept of explicit hard interventions to include soft interventions. In their study, [38] address the identification of causal models from soft interventions, leveraging the sparsity of the adjacency matrix as an inductive bias. However, when dealing with implicit models, soft interventions introduce new complexities. Identifiability becomes more challenging, as the causal effect of variables on observed variables is less apparent. This ambiguity arises from the dual possibility of effects originating from interventions or influences from parent variables on the causal variables. Moreover, in scenarios where implicit modeling is retained, the absence of knowledge about parent variables further complicates identifiability. While [3] theoretically establishes identifiability for hard interventions, practical experiments involving complex causal models with over 10 variables reveal increased ambiguity and confounding factors. Consequently, model identification becomes less straightforward.

## 3 Methodology

### Data Generating Process

A structural causal model (Definition A.1.1) is used to understand and describe the relationships between different variables and how they influence each other through causal mechanisms. A **decoder function**, \(g(\mathbf{z})=\mathbf{x}\), maps a vector of causal values \(z\) to observed values \(x\). The causal variables \(\mathcal{Z}\) are unobserved and the goal is to infer them from interventional data. For each causal variable, a **diffeomorphic solution function**, \(s_{i}:\mathcal{E}_{i}\rightarrow\mathcal{Z}_{i}\), deterministically maps a value for exogenous variable \(\mathcal{E}_{i}\) to a value for causal variable \(\mathcal{Z}_{i}\). _In implicit modeling, we learn the solution functions \(s_{i}\) directly,_ rather than defining them through local mechanisms \(f_{i}\). We write \(\mathcal{S}\) for the set of all solution functions \(s_{i}\in\mathcal{S}\), so \(\mathcal{S}:\mathcal{E}\rightarrow\mathcal{Z}\).

Identifying causal models from data can be complex and is often studied within classes of models such as those identifiable up to affine transformations. For example, in the context of nonlinear _Independent Component Analysis (ICA)_, the generative process also involves a mixture function \(g\) of latent causal variables \(\mathcal{Z}\in\mathbb{R}^{n}\), resulting in observations \(\mathcal{X}\in\mathbb{R}^{n}\)[41; 15]. However, a significant distinction between causal representation learning and nonlinear-ICA is that in the former, the causal

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline
**Methods** & **Causal Mechanisms** & **Missing functions** & **Interventions** & **Explicit/Implicit** & **Identifiability** \\ \hline CausalDisexpancy [38] & Nonlinear & Full row rank polynomial & Soft & Explicit & Permutation and Affine \\ Conc(A [33]) & Nonlinear & Diffeomorphism & Soft & Explicit & Different based on assumptions \\ Linear-CD [29] & Linear & Linear & Hard & Explicit & Permutation \\ Scale-[30] & Nonlinear & Linear & Hard/Soft & Explicit & Scale/Mixed \\ ILCM [3] & Nonlinear & Diffeomorphism & Hard & Implicit & Permutation and reparameterization \\ dVAE [21] & Nonlinear & Diffeomorphism & Hard & Implicit & Permutation and reparameterization \\ ICRL-SM (ours) & Nonlinear & Diffeomorphism & Soft & Implicit & Repparameterization \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of proposed method with other recent related work on causal learning from interventional data variables \(\mathcal{Z}\) may have complex dependencies. Our objective in this paper is to recover \(\mathcal{E}\) from \(\mathcal{X}\) and eventually map \(\mathcal{E}\) to \(\mathcal{Z}\) using solution functions.

Identifying a causal model from observational data is not trivial and requires assumptions on the parameters of the model [1]. Adding information about interventions in addition to observations, helps to identify causal variables by exhibiting the effect of changing a causal variable on the observed variables. An interventional data point \((x,\tilde{x},i)\) includes the pre-intervention observation \(x\), the post-intervention observation \(\tilde{x}\), and intervention target \(i\in\mathcal{I}\) where \(\mathcal{I}\) is the set of intervention targets selected from the causal variables. The post-intervention data \(\tilde{x}\) is generated by a _soft intervention_ that targets one of the causal variables in \(\mathcal{Z}\). To achieve identifiability up to reparametrization, we rely on a series of assumptions within the data generation process, outlined as follows:

**Assumption 3.1**.: _(Data generating assumptions)_

_1. Atomic Interventions: For every sample \((x,\tilde{x},i)\), only one causal variable is targeted by an intervention._

_2. Known Targets: Targets of soft interventions are known._

_3. Post-intervention Exogenous Variables: The exogenous variables' values change only for the corresponding intervened causal variable, while the others maintain their pre-intervention values, thus \(e_{i}\neq\tilde{e}_{i}\) if \(i\in\mathcal{I}\),and \(e_{i}=\tilde{e}_{i}\) otherwise._

_4. Sufficient Variability: Soft interventions alter causal mechanisms to introduce sufficient variability [15]. These interventions should modify causal mechanisms to ensure non-overlapping conditional distributions of causal variables (refer to Figure 1)._

_5. Diffeomorphic decoder and causal mechanisms: Diffeomorphism guarantees no information loss and avoids abrupt changes in the function's image._

The **known targets** assumption can be relaxed in applications where such data is not available and the same procedure in [3] can be used to infer the intervention targets. In fact, in our real-world experiments, intervention targets are not available and based on the nature of the datasets, we hypothesize our causal variables to be object attributes and actions to be intervention targets.

### Causal Mechanisms Switch Variable

The major difference of soft intervention with hard intervention is that post-intervention causal variable \(\tilde{\mathcal{Z}}_{i}\) is no longer disconnected from its parents and its causal mechanism \(\tilde{s}_{i}\) is affected by the intervention. This is why identifying the causal mechanisms is more difficult for soft interventions. Soft intervention data yield fewer constraints on the causal graph structure than hard intervention data. For more details refer to string diagrams of soft and hard interventions depicted in Figure 1(b). Figure 1(b) shows our main generative model. It includes a data augmentation step that adds the intervention displacement \(\tilde{x}-x\) as an observed feature that directly represents the effect of a soft intervention in observation space.

**Augmented implicit causal model** To model the effect of soft interventions, we introduce the causal mechanism switch variable \(\mathcal{V}\)[26]. By leveraging \(\mathcal{V}\), we can effectively switch to the pre-intervention causal mechanisms within post-intervention data. This facilitates the model's ability to solely focus on discerning alterations in the intrinsic characteristics of each causal variable. These changes are encapsulated within their respective exogenous variables, aiding the model in learning the causal relationships more accurately. We propose to use a modulated form of \(\mathcal{V}\) to model the soft intervention effects on each causal variable as an additive effect with a nonlinear function \(h_{i}\) such that \(\forall i,\ \tilde{\mathcal{Z}}_{i}=\tilde{s}_{i}(\tilde{\mathcal{E}}_{i}; \tilde{\mathcal{E}}_{/i})=s_{i}(\tilde{\mathcal{E}}_{i};\mathcal{E}_{/i},h_{i }(\mathcal{V}))\). As the parental set for each causal variable is not known, we have to use a modulated form of \(\mathcal{V}\) in every causal variable's solution function and the inclusion of \(h_{i}(\mathcal{V})\) enables the model to encompass variations in the parental sets of all causal variables in \(\mathcal{V}\). Therefore, there is a switch variable \(\mathcal{V}_{i}\) for each causal variable \(\mathcal{Z}_{i}\). Adding switch variables to solution functions leads to the concept of an _augmented implicit causal model_.

**Definition 3.2**.: _(Augmented Implicit Causal Models) An Augmented Implicit Causal Models (AICMs) is defined as \(\mathcal{A}=(\mathcal{S},\mathcal{Z},\mathcal{E},\mathcal{V})\) where \(\mathcal{V}\in\mathbb{R}^{n}\) is the causal mechanism switch variable which models the effect of soft interventions on solution functions \(\mathcal{S}\):_

\[\forall i,\ \tilde{\mathcal{Z}}_{i}=\tilde{s}_{i}(\tilde{\mathcal{E}}_{i}; \tilde{\mathcal{E}}_{/i})=s_{i}(\tilde{\mathcal{E}}_{i};\mathcal{E}_{/i},h_{i }(\mathcal{V})),\] (1)

_where \(\tilde{s_{i}}\) is the new solution function resulting from the soft intervention, \(\tilde{\mathcal{E}}_{/i}\) is the altered set of all exogenous variables except \(i\), including the ancestral exogenous variables, due to intervention, and \(\tilde{\mathcal{E}}_{i}\) is the post-intervention exogenous variable._The usage of \(\mathcal{V}\) in soft interventions is analogous to augmented networks in [23] which were mainly designed for hard interventions. Pearl [23] even foresaw this possibility by saying: "One advantage of the augmented network representation is that it is applicable to any change in the functional relationship \(f_{i}\) and not merely to the replacement of \(f_{i}\) by a constant."

By using Taylor's expansion, we can expand the solution functions as follows:

\[s_{i}(\tilde{\mathcal{E}}_{i};\mathcal{E}_{/i},h_{i}(\mathcal{V}))=s_{i}( \tilde{\mathcal{E}}_{i};\mathcal{E}_{/i},h_{i}(v_{0}))+\sum_{n=1}^{\infty} \frac{1}{n!}\left|\frac{\partial^{n}s_{k}}{\partial h_{i}^{k}}\right|_{h_{i}=h _{i}(v_{0})}(h_{i}(\mathcal{V})-h_{i}(v_{0}))^{n}\right)=s_{i}(\tilde{ \mathcal{E}}_{i};\mathcal{E}_{/i},h_{i}(v_{0}))+R_{i}\] (2)

where we'll use \(R_{i}\) as a short-hand for Equation 2. We define the **separable dependence** property for solution functions as \(\exists h_{i}(v_{0}):s_{i}(\tilde{\mathcal{E}}_{i};\mathcal{E}_{/i},h_{i}(v_{ 0}))=s_{i}(\tilde{\mathcal{E}}_{i};\mathcal{E}_{/i})\). An example of such a scenario could be in location-scale noise models such as, \(s_{i}(\tilde{e}_{i};e_{/i},h_{i}(v))=\tilde{e}_{i}+loc(e_{/i})+h_{i}(v)=\tilde {e_{i}}+loc(e_{/i})+v^{2}+v\) where \(v_{0}\) would be zero. By assuming the separable dependence property, we can write the solution function in Equation 2 as:

\[s_{i}(\tilde{\mathcal{E}}_{i};\mathcal{E}_{/i},h_{i}(\mathcal{V}))=s_{i}( \tilde{\mathcal{E}}_{i};\mathcal{E}_{/i})+R_{i}=s_{i}(\tilde{\mathcal{E}}_{i} ;\mathcal{E}_{/i})+\textit{soft intervention effect}\] (3)

As a result, we can switch to pre-intervention solution functions. Subsequently, by modeling soft intervention effects using \(h_{i}(\mathcal{V})\), we can recover pre-intervention solution functions. During inference, we simply disregard the \(h_{i}(\mathcal{V})\) term in the solution functions. Nonetheless, it is possible to train the prior \(p(\mathcal{V})\) to ensure that the separable dependence property is maintained for pre-intervention data.

**Observability of switch variable** The intuition behind using \(\mathcal{V}\) is to separate the effect of soft intervention on \(\tilde{\mathcal{E}}_{i}\) into two: (1) The effect on causal mechanisms and parents, and (2) The effect on exogenous variable \(\mathcal{E}_{i}\). For example, we can say that causal variables in images of objects are the objects' attributes such as shape, color, and size, and performing actions like "Fold" change these attributes. Furthermore, it can be asserted that the camera angle within a given image may influence the shape of the object. If the images were generated from a hard intervention, the camera angle remains fixed between pre and post intervention. However, the camera angle changes along with the performed actions indicating that the interventions are soft. In this case, if we had a knowledge of how the camera angle affects the attributes of objects, then we could separate the effect of soft intervention. In other words, if \(\mathcal{V}\) is observed, then we can extract the effect of the intervention that we are interested in (i.e., the effect on the causal variable itself). For more details, refer to Figure A4.

Lacking an understanding of how soft intervention influences the causal model, a more complex model becomes necessary. Consequently, the term \(R_{i}\) in Equation 2 would involve a higher order of \(h_{i}(\mathcal{V})\). Therefore, we assume the observability of \(\mathcal{V}\):

**Assumption 3.3**.: _(Observability of \(\mathcal{V}\)) Given an intervention sample \((x,\tilde{x},i)\) and linear decoders, we can approximate the soft intervention effects \(h_{i}(\mathcal{V})\) as follows:_

\[\tilde{z}-z=\Delta e_{i}+R\quad\text{(using Equation 2)},\quad\tilde{x}-x=g( \tilde{z})-g(z)\approx g(\tilde{z}-z)=g(\Delta e_{i}+R),\]

_where \(R=[R_{0},R_{1},...,R_{n}]\) and \(n\) is the number of causal variables. \(R\) and \(\Delta e_{i}\) are the vectors indicating the soft intervention effects and change in effect of the exogenous variable of the intervened causal variable, respectively. Note that elements of \(R\) will be all zero except for the intervened causal variable. Consequently, with linear mixing functions and some pre-processing on observed samples (here subtraction), we can observe \(R_{i}\)._

Our synthetic data is generated using a linear decoder, however, the decoder for the real-world datasets is not necessarily linear. Therefore, we do not observe \(\mathcal{V}\) from \(\tilde{x}-x\) in the real-world dataset. Nevertheless, our findings suggest that incorporating soft interventions through \(\mathcal{V}\) leads to superior performance compared to other implicit modeling approaches. Clearly, understanding the impact of soft interventions on the generative system of the dataset would result in improved outcomes.

### Identifiability Theorem for Implicit SCMs with Soft Interventions

In this paper, our focus lies in identifying the causal variables up to reparameterization through soft interventions. We first define identifiability up to reparameterization (Definition 3.4) and subsequently introduce the identifiability theorem 3.5. The proof of theorem is extensive and is available in full in Appendix A1.

We establish identifiability up to reparameterization, allowing for the mapping of causal variables \(\mathcal{Z}\) and \(\mathcal{Z}^{\prime}\) between two Latent Causal Models (\(\mathcal{M}\) and \(\mathcal{M}^{\prime}\)) through component-wise transformations(Definition A1.2). Given our implicit modeling approach, lacking knowledge of the causal graph, we include all exogenous variables in the solution functions, as depicted in Equation 1. Notably, **the causal graph remains unaltered during learning**. To illustrate, we contrast hard interventions, which neglect parent influences, with soft interventions that acknowledge parental effects in a simple example. Consider a basic causal model \(Z_{1}\to Z_{2}\) alongside a location-scale noise model [12] for the solution function, given by \(\tilde{z}_{2}=\frac{\tilde{e}_{2}-\widetilde{loc}(e_{1})}{scale(e_{1})}\). The distribution \(p(\tilde{\mathcal{Z}}_{2})\) mean is \(\frac{1}{scale(e_{1})}\times\text{mean}(\tilde{\mathcal{E}}_{2})-\frac{ \widetilde{loc}(e_{1})}{scale(e_{1})}\) In the context of hard interventions, we can assume \(p(\tilde{\mathcal{Z}}_{2}|\mathcal{Z}_{1})=p(\tilde{\mathcal{Z}}_{2})=N(0,1)\) as there are no parental effects. Consequently, the location and scale networks within the solution function tend to dampen parental effects, given the absence of parental influence in the ground-truth data. Contrarily, soft interventions exhibit parental influence in the ground-truth data, thus \(p(\tilde{\mathcal{Z}}_{2}|\mathcal{Z}_{1})\neq N(0,1)\). Due to the lack of parental knowledge in implicit modeling, we model \(p(\tilde{\mathcal{Z}}_{2}|\mathcal{Z}_{1})=p(\tilde{\mathcal{Z}}_{2}|\mathcal{ E}_{2})\), as \(\mathcal{E}_{2}\) is a known parent of \(\tilde{\mathcal{Z}}_{2}\). Consequently, parental effects are propagated to \(\mathcal{E}_{i}\) (the corresponding exogenous variable of each causal variable), violating identifiability up to reparameterization. By leveraging \(\mathcal{V}\), we allow parental effects to propagate to \(\mathcal{V}\) instead of \(\mathcal{E}_{i}\).

**Definition 3.4**.: _(Equivalence up to component-wise reparameterization) Let \(\mathcal{M}=(\mathcal{A},\mathcal{X},g,\mathcal{I})\) and \(\mathcal{M}^{\prime}=(\mathcal{A}^{\prime},\mathcal{X},g^{\prime},\mathcal{I})\) be two Latent Causal Models (LCM) based on AICMs \(\mathcal{A},\mathcal{A}^{\prime}\) with shared observation space \(\mathcal{X}\), shared intervention targets \(\mathcal{I}\), and respective decoders \(g\) and \(g^{\prime}\). We say that \(\mathcal{M}\) and \(\mathcal{M}^{\prime}\) are equivalent up to component-wise reparameterization \(\mathcal{M}\sim_{r}\mathcal{M}^{\prime}\) if there exists a component-wise transformation (Definition A1.2) \(\phi_{\mathcal{Z}}\) from the causal variables \(\mathcal{Z}\) to the causal variables \(\mathcal{Z}^{\prime}\) and a component-wise transformation \(\phi_{\mathcal{E}}\) between \(\mathcal{E}\) and \(\mathcal{E}^{\prime}\) such that:_

_1. Indices are preserved (i.e., \(\phi_{i}(z_{i})=z_{i}^{\prime}\) and \(\phi_{i}(e_{i})=e_{i}^{\prime}\)). Corresponding edges are preserved (i.e., \(\mathcal{Z}_{i}\rightarrow\mathcal{Z}_{j}\) holds in \(\mathcal{G}\) iff \(\mathcal{Z}_{i}^{\prime}\rightarrow\mathcal{Z}_{j}^{\prime}\) holds in \(\mathcal{G}^{\prime}\). Edges \(\mathcal{E}_{i}\rightarrow\mathcal{Z}_{i}\) should be preserved as well.)_

_2. The exogenous transformation preserves the probability measure on exogenous variables \(p_{\mathcal{E}^{\prime}}=(\phi_{\mathcal{E}})_{*}p_{\mathcal{E}}\) (Definition A1.4)._

**3. The causal transformation preserves the probability measure on causal variables \(p_{\mathcal{Z}^{\prime}}=(\phi_{\mathcal{Z}})_{*}p_{\mathcal{Z}}\) (Definition A1.4)._**

**Theorem 3.5**.: _(Identifiability of latent causal models.) Let \(\mathcal{M}=(\mathcal{A},\mathcal{X},g,\mathcal{I})\) and \(\mathcal{M}^{\prime}=(\mathcal{A}^{\prime},\mathcal{X},g^{\prime},\mathcal{I})\) be two LCMs with shared observation space \(\mathcal{X}\) and shared intervention targets \(\mathcal{I}\). Suppose the following conditions are satisfied:_

_1. Data generating assumptions explained in Assumption 3.1._

_2. Soft interventions satisfy Assumption 3.3._

_3. The causal and exogenous variables are real-valued._

_4. The causal and exogenous variables follow a multivariate normal distribution._

_Then the following statements are equivalent:_

_Two LCMs \(\mathcal{M}\) and \(\mathcal{M}^{\prime}\) assign the same likelihood to interventional and observational data i.e., \(p_{\mathcal{M}}^{\mathcal{X},\mathcal{I}}(x,\tilde{x},i)=p_{\mathcal{M}^{ \prime}}^{\mathcal{X},\mathcal{I}}(x,\tilde{x},i)\)._

_- \(\mathcal{M}\) and \(\mathcal{M}^{\prime}\) are disentangled, that is \(\mathcal{M}\sim_{r}\mathcal{M}^{\prime}\) according to Definition 3.4._

### Training Objective

Consequently, there will be three latent variables in ICRL-SM:

**1.** A causal mechanism switch variable \(\mathcal{V}\).

**2.** The pre-intervention exogenous variables \(\mathcal{E}\).

**3.** The post-intervention exogenous variables \(\tilde{\mathcal{E}}\).

As the data log-likelihood \(\log p(x,\tilde{x},x-\tilde{x})\equiv\log p(x,\tilde{x})\) is intractable, we utilize an ELBO approximation as training objective:

\[\begin{split}\log p(x,\tilde{x})\geq& E_{g(e,\tilde{x},|e,\tilde{x}|,\tilde{x})}\big{[}\log p (x,\tilde{x}|e,\tilde{e},v)\big{]}-KLD(q(e,\tilde{e},v|x,\tilde{x})\|p(e, \tilde{e},x))\\ &=E_{q(v|\tilde{x}-\tilde{x})\cdot q(e|x)\cdot q(e|x)}\big{[}\log p (x|e)p(\tilde{x}|\tilde{e})p(\tilde{x}-x|v)\big{]}-KLD(q(v|\tilde{x}-x)\cdot q (e|x)\cdot q(e|\tilde{x})||p(e|e,v)p(v)p(e)).\end{split}\] (4)

The observations are encoded and decoded independently. The KLD term regularizes the encodings to share the latent _intervention model_\(p(\tilde{e}|e,v)p(v)p(e)\) that is shared across all data points. The components of this model can be interpreted as follows:

**1.**\(p(e)\) is the prior distribution over exogenous variables \(e\).

**2.**\(p(v)\) is the prior distribution over switch variables \(v\).

**3.**\(p(\tilde{e}|e,v)\) is a transition model that shows how the exogeneous variables change as a function of the intervention.

We factorize the posterior with a mean-field approximation \(q(v,e,\tilde{e}|x,\tilde{x})=q(v|\tilde{x}-x)\cdot q(e|x)\cdot q(\tilde{e}| \tilde{x})\) and, following our data generation model (Figure 1(b)), the reconstruction probability as \(p(x,\tilde{x}|e,\tilde{e},v)=p(x|e)p(\tilde{x}|\tilde{e})p(\tilde{x}-x|v)\). The prior over latent variables is factorized as \(p(\tilde{e},e,v)=p(\tilde{e}|e,v)p(v)p(e)\)(Figure 1(b)). Pre-intervention exogenous variables are mutually independent, hence, \(p(e)=\Pi_{i}p(e_{i})\) and \(p(v)=\Pi_{i}p(v_{i})\). We assume \(p(e_{i})\) and \(p(v_{i})\) to be standard Gaussian. Furthermore, as we assume \(e_{i}=\tilde{e}_{i}\) for all non-intervened variables, the \(p(\tilde{e}|e,v)\) will be as follows:

\[p(\tilde{e}|e,v)=\Pi_{i\notin I}\delta(\tilde{e}_{i}-e_{i})\Pi_{i\in I}p( \tilde{e}_{i}|e,v)=\Pi_{i\notin I}\delta(\tilde{e}_{i}-e_{i})\Pi_{i\in I}p( \tilde{z}_{i}|e_{i})\left|\frac{\partial\tilde{z}_{i}}{\partial\tilde{e}_{i}}\right|\] (5)

The last equality is obtained from the Change of Variable Rule in probability theory, applied to the solution function \(\tilde{z}_{i}=s_{i}(\tilde{e}_{i};e_{/i},h_{i}(v))\). Furthermore, we write \(p(\tilde{z}_{i}|e,v)=p(\tilde{z}_{i}|e_{i})\) since only \(e_{i}\) is a known parent of \(\tilde{z}_{i}\) in implicit modeling. We assume \(p(\tilde{z}_{i}|e_{i})\) to be a Gaussian whose mean is determined by \(e_{i}\). We implement the solution function using a location-scale noise models [12] as also practiced in [3], which defines an invertible diffeomorphism. For simplicity, in our experiments, we are only going to change the \(loc\) network in post-intervention. Therefore, \(h_{i}(v)\) will be used as:

\[\tilde{z}_{i}=\tilde{s}_{i}(\tilde{e}_{i};e_{/i},h_{i}(v))=\frac{\tilde{e}_{i }-(loc_{i}(e_{/i})+h_{i}(v))}{scale_{i}(e_{/i})},\] (6)

where \(loc_{i}:\mathbb{R}^{n-1}\rightarrow\mathbb{R}\) and \(scale_{i}:\mathbb{R}^{n-1}\rightarrow\mathbb{R}\) are fully connected networks calculating the first and second moments, respectively. The general overview of the model is illustrated in Figure 1(a).

## 4 Experiments and Results

The experiments conducted in this paper address two downstream tasks; (1) Causal Disentanglement to identify the true causal graph from pairs of observations \((x,\tilde{x},i)\), and (2) Action Inference to make supervised inferences about actions generated from the post-intervention samples using information about the values of the manipulated causal variables. Moreover, we conducted additional experiments designed as an ablation study, the results of which are presented in A4. All models are trained using the same setting and data with known intervention targets.

### Datasets

**Synthetic Dataset** We generate simple synthetic datasets with \(\mathcal{X}=\mathcal{Z}=\mathbb{R}^{n}\). For each value of \(n\), we generate ten random DAGs, a random location-scale SCM, then a random dataset from the parameterized SCM. To generate random DAGs, each edge is sampled in a fixed topological order from a Bernoulli distribution with probability 0.5. The pre-intervention and post-intervention causal variables are obtained as:

\[z_{i}=scale(z_{pa_{i}})e_{i}+loc(z_{pa_{i}})\xrightarrow{\text{Soft- Intervention}}\tilde{z}_{i}=scale(z_{pa_{i}})\tilde{e_{i}}+\widetilde{loc}(z_{pa_{i}}),\] (7)

where the \(loc\) and \(scale\) networks are changed in post intervention. The pre-intervention \(loc\) and post-intervention \(\widetilde{loc}\) network weights are initialized with samples drawn from \(\mathcal{N}(0,1)\) and \(\mathcal{N}(3,1)\), respectively. The \(scale\) is constant 1 for both pre-intervention and post-intervention samples. Both \(e_{i}\) and \(\tilde{e_{i}}\) are sampled from a standard Gaussian. The causal variables are mapped to the data space through a randomly sampled \(SO(n)\) rotation. For each dataset, we generate 100,000 training samples, 10,000 validation samples, and 10,000 test samples.

Action DatasetsCausal-Triplet datasets tailored for _actionable_ counterfactuals [19] feature paired images where several global scene properties may vary including camera view and object occlusions. Thus, the images can be viewed as outcomes of soft interventions, wherein actions affect objects alongside subtle alterations. These datasets [19] consist of: images obtained from a photo-realistic simulator of embodied agents, ProcTHOR [9], and the other contains images repurposed from a real-world video dataset of human-object interactions [8]. The former one contains 100 k images in which 7 types of actions manipulate 24 types of objects in 10 k distinct ProcTHOR indoor environments. The latter consists of 2,632 image pairs, collected under a similar setup from the Epic-Kitchens dataset with 97 actions manipulating 277 objects.Based on the nature of actions in this dataset, the causal variables should represent attributes of objects such as shape and color. As the dataset consists of images we train all the methods with ResNet encoder and decoder. For the ProcThor dataset the number of causal variables are 7. For the Epic-Kitchens dataset, we randomly chose 20 actions from the dataset as 97 causal variables will be too complex in a VAE setup.

### Metrics

For the causal disentanglement task, we are going to use the DCI scores [10]. Causal disentanglement score quantifies the degree to which \(\mathcal{Z}_{i}\) factorises or disentangles the \(\mathcal{Z}^{*}\). Causal disentanglement \(D_{i}\) for \(\mathcal{Z}_{i}\) is calculated as \(D_{i}=(1-H_{K}(P_{i.}))=(1+\sum_{k=0}^{K-1}P_{ik}\log_{K}P_{ik})\) where \(P_{ij}=\frac{R_{ij}}{\sum_{k=0}^{K-1}R_{ik}}\) and \(R_{ij}\) denotes the probability of \(\mathcal{Z}_{i}\) being important for predicting \(\mathcal{Z}_{j}^{*}\). Total causal disentanglement is the weighted average \(\sum_{i}\rho_{i}D_{i}\) where \(\rho_{i}=\frac{\sum_{j}R_{ij}}{\sum_{ij}R_{ij}}\). Causal Completeness quantifies the degree to which each \(\mathcal{Z}_{i}^{*}\) is captured by a single \(\mathcal{Z}_{i}\). Causal completeness is calculated as \(C_{j}=(1-H_{D}(\tilde{P}_{j}))=(1+\sum_{d=0}^{D-1}\tilde{P}_{dj}\log_{D}\tilde {P}_{ij})\). \(D\) and \(K\) here are equal to the dimension of \(\mathcal{Z}^{*}\) and \(\mathcal{Z}\) which is \(n\). For the action inference task, we will use classification accuracy as a metric. As we assume intervention targets are known, we train all models using known intervention targets for a fair comparison.

## 5 Results

### Causal Disentanglement

We generated a dataset for the soft interventions and trained the models of ICRL-SM, ILCM, \(\beta\)-VAE and D-VAE for 10 different seeds, which generated 10 different causal graphs. We selected 4 causal variables to encompass complex causal structures, including forks, chains, and colliders. Table 2 displays the Causal Disentanglement and Causal Completeness scores for all models, computed on the test data.

The results in Table 2 indicate that our method ICRL-SM can identify the true causal graph in most cases. The worst results are seen for graphs \(G5\) and \(G10\). As mentioned in [27; 25], causal graphs are sparse and in the \(G5\) case, where the graph is fully connected, the proposed method cannot identify the causal variables well. Furthermore, in the next experiment we are going to examine the factors affecting causal disentanglement such as the number of edges in the graph and the intensity of soft intervention effect. These findings can explain why ICRL-SM cannot identify causal variables in \(G10\) despite its sparsity.

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline \multicolumn{2}{c}{**Graph**} & \multicolumn{3}{c}{**Causal Disentanglement**} & \multicolumn{3}{c}{**Causal Completeness**} \\ \hline
**Model** & **Name** & \(\beta\)**-VAE & \(\beta\)**-VAE & ILCM & ICRL-SM & \(\beta\)**-VAE & \(\beta\)**-VAE & ILCM & ICRL-SM \\ \hline \multirow{3}{*}{**T**} & G1 & 0.88 & 0.54 & 0.71 & **0.82** & 0.51 & 0.69 & 0.78 & **0.87** \\  & G2 & 0.30 & 0.72 & 0.75 & **0.83** & 0.49 & 0.77 & 0.80 & **0.87** \\  & G3 & 0.28 & 0.51 & 0.68 & **0.98** & 0.49 & 0.56 & 0.78 & **0.98** \\  & G4 & 0.16 & 0.50 & 0.65 & **0.68** & 0.38 & 0.69 & 0.77 & **0.78** \\  & G5 & 0.27 & 0.44 & **0.53** & 0.42 & 0.45 & 0.54 & **0.66** & 0.50 \\  & G6 & 0.52 & 0.62 & 0.71 & **0.98** & 0.66 & 0.69 & 0.36 & **0.98** \\  & G7 & 0.39 & 0.49 & 0.71 & **0.75** & 0.70 & 0.73 & 0.89 & **0.89** \\  & G8 & 0.47 & 0.54 & 0.50 & **0.59** & 0.6 & 0.63 & 0.62 & **0.68** \\  & G9 & 0.30 & 0.68 & 0.83 & **0.85** & 0.40 & 0.76 & 0.86 & **0.87** \\  & G10 & 0.39 & 0.39 & **0.52** & 0.32 & 0.53 & 0.56 & **0.82** & 0.70 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison of identifiability results

### Factors Affecting Causal Disentanglement

In this experiment, we consider the graph \(G3\), which has the best identifiability, and change the intensity of soft intervention and number of edges in its data generation process. To change the intensity, the post-intervention \(\widetilde{loc}\) network weights are initialized with samples drawn from \(N(1,1)\) (almost similar to \(loc\)) and \(N(10,1)\) (significantly different from \(loc\)). To change the number of edges, we consider a chain and fully-connected graph.

The results in Table 4 further confirms the sparsity of causal graphs as the causal disentanglement is much worse in the fully-connected graph than the default graph of \(G3\). The result for significantly different post-intervention causal mechanisms indicate that the switch variable cannot approximate intense effects of soft intervention and more supervision is required to observe \(\mathcal{V}\). Similar post-intervention causal mechanisms also do not have sufficient variability to disentangle the causal variables as mentioned in Theory 3.5.

### Action Inference

In this experiment, we show the performance of ICRL-SM in the real-world Causal-Triplet datasets. In these datasets \(\mathcal{V}\) i.e., soft intervention effects, are not directly observable. Nevertheless, our findings suggest that incorporating soft interventions through \(\mathcal{V}\) leads to superior performance compared to other implicit modeling approaches. Clearly, understanding the impact of soft interventions on the generative system of the dataset would result in improved outcomes.

The results in Table 3 indicate that when including all causal variables to predict actions, ICRL-SM performs at par with the baseline methods. However, including all causal variables in the action or object inference may cause spurious correlations. Therefore, we have also experimented with including only the related causal variable in action and object inference. In this setting, ICRL-SM significantly outperforms the baseline methods which means that it can better disentangle the causal variables. We have also compared ICRL-SM with explicit causal representation learning methods. ENCO [16] and DDS [5] have variable topological order of causal variables during training. Furthermore, we have included a specific setting where the topological order is fixed during training. As shown in Table 4, our proposed method has superior performance to explicit models as well.

## 6 Conclusion

ICRL-SM, our novel model, enhances implicit causal representation learning during soft interventions by introducing a causal mechanism switch variable. Evaluations on synthetic and real-world datasets demonstrate ICRL-SM's superiority over state-of-the-art methods, highlighting its practical effectiveness. Our findings emphasize ICRL-SM's ability to discern causal models from soft interventions, marking it as a promising avenue for future research.

\begin{table}
\begin{tabular}{l l c c} \hline \hline
**Datasets** & **Methods** & **Action Accuracy** & **Object Accuracy** \\ \hline Epic-Krdenes & ENCO [16] & 0.69 & 0.13 \\  & DDS [5] & 0.44 & 0.09 \\  & Fixed-order & **0.79** & 0.14 \\  & **ICRL-SM (ours)** & **0.86** & **0.18** \\ \hline PrecHOR & ENCO [16] & 0.45 & 0.53 \\  & DDS [5] & 0.64 & 0.67 \\  & Fixed-order & 0.65 & 0.54 \\  & **ICRL-SM (ours)** & **0.93** & **0.82** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Left table depicts the action and object accuracy of three explicit models, with experiments conducted applying an image with resolution of \(R_{64}\) as the input to the Resnet50 encoder with the intervened causal variable (\(z_{i}\)). Right table shows the comparison of ICRL-SM performance on different configurations of \(G5\)

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{**Epic-Kitchens**} & \multicolumn{4}{c}{**ProcTHOR**} \\ \cline{2-9}  & \multicolumn{2}{c}{**Action Accuracy**} & \multicolumn{2}{c}{**Object Accuracy**} & \multicolumn{2}{c}{**Action Accuracy**} & \multicolumn{2}{c}{**Object Accuracy**} \\ \cline{2-9}
**Method** & \(Z_{i}R_{64}\) & \(z_{i}R_{64}\) & \(Z_{i}R_{64}\) & \(z_{i}R_{64}\) & \(Z_{i}R_{64}\) & \(z_{i}R_{64}\) & \(z_{i}R_{64}\) & \(z_{i}R_{64}\) \\ \hline \(\beta-VAE\)[11] & **0.27** & 0.18 & 0.19 & 0.06 & **0.39** & 0.30 & **0.44** & 0.37 \\ \(d-VAE\)[21] & 0.19 & 0.69 & **0.20** & 0.17 & 0.35 & 0.81 & 0.40 & 0.78 \\ IECM[3] & 0.21 & 0.59 & 0.14 & 0.14 & 0.30 & 0.70 & 0.41 & 0.76 \\
**ICRL-SM (ours)** & 0.16 & **0.86** & 0.16 & **0.18** & 0.28 & **0.93** & 0.40 & **0.82** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Table comparing action and object accuracy across various methods on Causal-Triplet datasets under different settings. \(Z\) and \(z_{i}\) show whether all causal variables (\(Z\)), or only the intervened casual variable (\(z_{i}\)) are used for the prediction task. \(R_{64}\) denote images with resolutions \(64\times 64\).

## References

* Ahuja et al. [2023] Kartik Ahuja, Divyat Mahajan, Yixin Wang, and Yoshua Bengio. Interventional causal representation learning. In _International Conference on Machine Learning, ICML_, volume 202 of _Proceedings of Machine Learning Research_, pages 372-407. PMLR, 2023.
* Bagi et al. [2023] Shayan Shirahmad Gale Bagi, Zahra Gharaee, Oliver Schulte, and Mark Crowley. Generative causal representation learning for out-of-distribution motion forecasting. In _International Conference on Machine Learning, ICML_, volume 202 of _Proceedings of Machine Learning Research_, pages 31596-31612. PMLR, 2023.
* Brehmer et al. [2022] Johann Brehmer, Pim de Haan, Phillip Lippe, and Taco S. Cohen. Weakly supervised causal representation learning. In _NeurIPS_, 2022.
* Buchholz et al. [2023] Simon Buchholz, Goutham Rajendran, Elan Rosenfeld, Bryon Aragam, Bernhard Scholkopf, and Pradeep Ravikumar. Learning linear causal representations from interventions under general nonlinear mixing, 2023.
* Charpentier et al. [2022] Bertrand Charpentier, Simon Kibler, and Stephan Gunnemann. Differentiable DAG sampling. In _The Tenth International Conference on Learning Representations, ICLR_. OpenReview.net, 2022.
* Cooper and Yoo [2013] Gregory F. Cooper and Changwon Yoo. Causal discovery from a mixture of experimental and observational data, 2013.
* Correa and Bareinboim [2020] Juan D. Correa and Elias Bareinboim. General transportability of soft interventions: Completeness results. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems, NeurIPS_, 2020.
* Damen et al. [2022] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Rescaling egocentric vision: Collection, pipeline and challenges for EPIC-KITCHENS-100. _Int. J. Comput. Vis._, 130(1):33-55, 2022.
* Deitke et al. [2022] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Kiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. Prochtor: Large-scale embodied ai using procedural generation. _Advances in Neural Information Processing Systems_, 35:5982-5994, 2022.
* Eastwood and Williams [2018] Cian Eastwood and Christopher K. I. Williams. A framework for the quantitative evaluation of disentangled representations. In _6th International Conference on Learning Representations, ICLR_, 2018.
* Higgins et al. [2017] Irina Higgins, Loic Matthey, Arka Pal, Christopher P. Burgess, Xavier Glorot, Matthew M. Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. In _5th International Conference on Learning Representations, ICLR_, 2017.
* Immer et al. [2023] Alexander Immer, Christoph Schultheis, Julia E. Vogt, Bernhard Scholkopf, Peter Buhlmann, and Alexander Marx. On the identifiability and estimation of causal location-scale noise models. In _International Conference on Machine Learning, ICML_, volume 202 of _Proceedings of Machine Learning Research_, pages 14316-14332. PMLR, 2023.
* Jaber et al. [2020] Amin Jaber, Murat Kocaoglu, Karthikeyan Shanmugam, and Elias Bareinboim. Causal discovery from soft interventions with unknown targets: Characterization and learning. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems, NeurIPS_, 2020.
* Kaddour et al. [2022] Jean Kaddour, Aengus Lynch, Qi Liu, Matt J. Kusner, and Ricardo Silva. Causal machine learning: A survey and open problems. _CoRR_, abs/2206.15475, 2022.
* Lachapelle et al. [2022] Sebastien Lachapelle, Pau Rodriguez, Yash Sharma, Katie Everett, Remi Le Priol, Alexandre Lacoste, and Simon Lacoste-Julien. Disentanglement via mechanism sparsity regularization: A new principle for nonlinear ICA. In _1st Conference on Causal Learning and Reasoning, CLeaR_, volume 177 of _Proceedings of Machine Learning Research_, pages 428-484. PMLR, 2022.
* Lippe et al. [2022] Phillip Lippe, Taco Cohen, and Efstratios Gavves. Efficient neural causal discovery without acyclicity constraints. In _The Tenth International Conference on Learning Representations, ICLR_. OpenReview.net, 2022.
* Lippe et al. [2022] Phillip Lippe, Sara Magliacane, Sindy Lowe, Yuki M. Asano, Taco Cohen, and Stratis Gavves. CITRIS: causal identifiability from temporal intervened sequences. In _International Conference on Machine Learning, ICML_, volume 162 of _Proceedings of Machine Learning Research_, pages 13557-13603. PMLR, 2022.
* Liu et al. [2021] Chang Liu, Xinwei Sun, Jindong Wang, Haoyue Tang, Tao Li, Tao Qin, Wei Chen, and Tie-Yan Liu. Learning causal semantic representation for out-of-distribution prediction. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Worthman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 6155-6170. Curran Associates, Inc., 2021.
* Liu et al. [2023] Yuejiang Liu, Alexandre Alahi, Chris Russell, Max Horn, Dominik Zietlow, Bernhard Scholkopf, and Francesco Locatello. Causal triplet: An open challenge for intervention-centric causal representation learning. In _Conference on Causal Learning and Reasoning, CLeaR_, volume 213 of _Proceedings of Machine Learning Research_, pages 553-573. PMLR, 2023.
* Liu et al. [2022] Yuejiang Liu, Riccardo Cadei, Jonas Schweizer, Sherwin Bahmani, and Alexandre Alahi. Towards robust and adaptive motion forecasting: A causal representation perspective. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pages* [466] 17060-17071. IEEE, 2022.
* [467] Francesco Locatello, Ben Poole, Gunnar Ratsch, Bernhard Scholkopf, Olivier Bachem, and Michael Tschannen. Weakly-supervised disentanglement without compromises. In _Proceedings of the 37th International Conference on Machine Learning,ICML_, volume 119 of _Proceedings of Machine Learning Research_, pages 6348-6359. PMLR, 2020.
* [468] Chaochao Lu, Yuhuai Wu, Jose Miguel Hernandez-Lobato, and Bernhard Scholkopf. Invariant causal representation learning for out-of-distribution generalization. In _The Tenth International Conference on Learning Representations, ICLR_, 2022.
* [469] Judea Pearl. _Causality_, cambridge university press (2000). _Artif. Intell._, 169(2):174-179, 2005.
* [470] Judea Pearl, Madelyn Glymour, and Nicholas P. Jewell. _Causal inference in statistics: A primer_. John Wiley and Sons, 2016.
* [471] Ronan Perry, Julius von Kugelgen, and Bernhard Scholkopf. Causal discovery in heterogeneous environments under the sparse mechanism shift hypothesis. In _NeurIPS_, 2022.
* [472] Bernhard Scholkopf. Causality for machine learning. _CoRR_, abs/1911.10500, 2019.
* [473] Bernhard Scholkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. _Proceedings of the IEEE_, 109(5):612-634, 2021.
* [474] Xinwei Shen, Furui Liu, Hanze Dong, Qing Lian, Zhitang Chen, and Tong Zhang. Weakly supervised disentangled generative causal representation learning. _J. Mach. Learn. Res._, 23:241:1-241:55, 2022.
* [475] Chandler Squires, Anna Seigal, Salil Bhate, and Caroline Uhler. Linear causal disentanglement via interventions, 2023.
* [476] Burak Varici, Emre Acarturk, Karthikeyan Shanmugam, Abhishek Kumar, and Ali Tajer. Score-based causal representation learning with interventions, 2023.
* [477] Julius von Kugelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Scholkopf, Michel Besserve, and Francesco Locatello. Self-supervised learning with data augmentations provably isolates content from style. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 16451-16467. Curran Associates, Inc., 2021.
* [478] Julius von Kugelgen, Michel Besserve, Liang Wendong, Luigi Gresele, Armin Kekic, Elias Bareniboin, David M. Blei, and Bernhard Scholkopf. Nonparametric identifiability of causal representations from unknown interventions, 2023.
* [479] Liang Wendong, Armin Kekic, Julius von Kugelgen, Simon Buchholz, Michel Besserve, Luigi Gresele, and Bernhard Scholkopf. Causal component analysis, 2023.
* [480] Mengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun Wang. Causalvae: Disentangled representation learning via neural structural causal models. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR_, pages 9593-9602. Computer Vision Foundation / IEEE, 2021.
* [481] Shuai Yang, Kui Yu, Fuyuan Cao, Lin Liu, Hao Wang, and Jiuyong Li. Learning causal representations for robust domain adaptation. _IEEE Transactions on Knowledge and Data Engineering_, pages 1-1, 2021.
* [482] Kui Yu, Xianjie Guo, Lin Liu, Jiuyong Li, Hao Wang, Zhaolong Ling, and Xindong Wu. Causality-based feature selection: Methods and evaluations. _ACM Comput. Surv._, 53(5), 2020.
* [483] Yue Yu, Jie Chen, Tian Gao, and Mo Yu. DAG-GNN: DAG structure learning with graph neural networks. In _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 7154-7163. PMLR, 2019.
* [484] Jiaqi Zhang, Chandler Squires, Kristjan Greenewald, Akash Srivastava, Karthikeyan Shanmugam, and Caroline Uhler. Identifiability guarantees for causal disentanglement from soft interventions, 2023.
* [485] Jiaqi Zhang, Chandler Squires, Kristjan H. Greenewald, Akash Srivastava, Karthikeyan Shanmugam, and Caroline Uhler. Identifiability guarantees for causal disentanglement from soft interventions. _CoRR_, abs/2307.06250, 2023.
* [486] Xun Zheng, Bryon Aragam, Pradeep Ravikumar, and Eric P. Xing. Dogs with NO TEARS: continuous optimization for structure learning. In _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems NeurIPS_, pages 9492-9503, 2018.
* [487] Yujia Zheng, Ignavier Ng, and Kun Zhang. On the identifiability of nonlinear ICA: sparsity and beyond. In _NeurIPS_, 2022.

## Appendix A1 Proof of Identifiability Theorem

In order to prove our model is identifiable we need a two additional definitions and some previously stated assumptions.

**Definition A1.1**.: _Structural Causal Models_

_A structural causal model (SCM) is a tuple \(\mathcal{C}=(\mathcal{F},\mathcal{Z},\mathcal{E},\mathcal{G})\) with the following components:_

_1. The domain of causal variables \(\mathcal{Z}=\mathcal{Z}_{1}\times\mathcal{Z}_{2}\times\ldots\times\mathcal{Z} _{n}\)._

_2. The domain of exogenous variables \(\mathcal{E}=\mathcal{E}_{1}\times\mathcal{E}_{2}\times\ldots\times\mathcal{E} _{n}\)._

_3. A directed acyclic graph \(\mathcal{G}(\mathcal{C})\) over the causal and exogenous variables._

_4. A causal mechanism \(f_{i}\in\mathcal{F}\) which maps an assignment of parent values for the parents \(\mathcal{Z}_{pa_{i}}\) plus an exogenous variable value for \(\mathcal{E}_{i}\) to a value of causal variable \(Z_{i}\)._

**Definition A1.2**.: _(Component-wise Transformation) Let \(\phi\) be a transformation (1-1 onto mapping) between product spaces \(\phi:\Pi_{i=1}^{n}\mathcal{X}_{i}\rightarrow\Pi_{i=1}^{n}\mathcal{Y}_{i}\). If there exist local transformations \(\phi_{i}\) such that \(\forall i,j,\,\forall x,\,\phi(x_{1},x_{2},...,x_{n})_{i}=\phi_{i}(x_{j})\), then \(\phi\) is a component-wise transformation._

**Definition A1.3**.: _(Diffeomorphism) A diffeomorphism between smooth manifolds \(M\) and \(N\) is a bijective map \(f:M\to N\), which is smooth and has a smooth inverse. Diffeomorphisms preserve information as they are invertible transformations without discontinuous changes in their image._

**Definition A1.4**.: _(Pushforward measure) Given a measurable function \(f:A\to B\) between two measurable spaces \(A\) and \(B\), and a measure \(p\) defined on \(A\), the pushforward measure \(f_{*}p\) on \(B\) is defined for measurable sets \(E\) in \(B\) as:_

\((f_{*}p)(E)=p(f^{-1}(E))\)__

_where \({}_{*}\) denotes the pushforward operation. In other words, the pushforward measure \(f_{*}p\) assigns a measure to a set in \(B\) by measuring the pre-image of that set under \(f\) in the space \(A\)._

**Lemma A1.5**.: _The transformation \(\phi_{\mathcal{Z}}:\mathcal{Z}\rightarrow\mathcal{Z}^{\prime}\) between the causal variable of two LCMs \(\mathcal{M}\) and \(\mathcal{M}^{\prime}\) defined in Definition 3.4 is a component-wise transformation, if \(\forall i,j,i\neq j\quad\tilde{\mathcal{E}}^{\prime}_{i}\perp\!\!\!\perp \tilde{\mathcal{E}}^{\prime}_{j}\) and the causal variables follow a multivariate normal distribution conditional on the pre-intervention exogenous variables where \(\tilde{E}^{\prime}_{i}\) denote the post-intervention exogenous variable of causal variable \(i\) in \(\mathcal{M}^{\prime}\)._

_proof: We consider the case where the exogenous variables are mapped to causal variables by a location-scale noise model such that \(\tilde{z}_{i}=\frac{\tilde{e}_{i}-\bar{loc}(e_{/i})}{scale(e_{/i})}\)._

\[\forall i,j,i\neq j\quad\tilde{\mathcal{E}}^{\prime}_{i}\perp\!\!\!\perp \tilde{\mathcal{E}}^{\prime}_{j}\to E[\tilde{\mathcal{E}}^{\prime}_{i} \tilde{\mathcal{E}}^{\prime}_{j}]=E[\tilde{\mathcal{E}}^{\prime}_{i}]E[\tilde{ \mathcal{E}}^{\prime}_{j}]\]_let's add these three constants \(-E[\tilde{\mathcal{E}}_{i}^{\prime}]\widetilde{co_{j}^{\prime}}(e_{j}^{\prime}),\ -E[ \tilde{\mathcal{E}}_{j}^{\prime}]\widetilde{loc_{i}^{\prime}}(e_{i}^{\prime}),\ \widetilde{loc_{i}^{\prime}}(e_{i}^{\prime})\widetilde{loc_{j}^{\prime}}(e_{j }^{\prime})\) to the both sides of the equality and then divide both sides by \(\widetilde{scale_{i}^{\prime}}(e_{i}^{\prime})\widetilde{scale_{j}^{\prime}}(e_{ j}^{\prime})\):_

\[E\left[\frac{\tilde{\mathcal{E}}_{i}^{\prime}\tilde{\mathcal{E} }_{j}^{\prime}-\tilde{\mathcal{E}}_{i}^{\prime}\widetilde{loc_{j}^{\prime}}(e_ {j}^{\prime})-\tilde{\mathcal{E}}_{j}^{\prime}\widetilde{loc_{i}^{\prime}}(e_ {i}^{\prime})+\widetilde{loc_{i}^{\prime}}(e_{i}^{\prime})\widetilde{loc_{j}^ {\prime}}(e_{j}^{\prime})}{\widetilde{scale_{i}^{\prime}}(e_{i}^{\prime}) \widetilde{scale_{j}^{\prime}}(e_{j}^{\prime})}\right]=\] \[\frac{E[\tilde{\mathcal{E}}_{i}^{\prime}]E[\tilde{\mathcal{E}}_{ j}^{\prime}]-E[\tilde{\mathcal{E}}_{i}^{\prime}]\widetilde{loc_{j}^{\prime}}(e_ {j}^{\prime})-E[\tilde{\mathcal{E}}_{j}^{\prime}]\widetilde{loc_{i}^{\prime}}(e _{i}^{\prime})+\widetilde{loc_{i}^{\prime}}(e_{i}^{\prime})\widetilde{loc_{j}^ {\prime}}(e_{j}^{\prime})}{\widetilde{scale_{i}^{\prime}}(e_{i}^{\prime}) \widetilde{scale_{j}^{\prime}}(e_{j}^{\prime})}\] \[\to E\left[(\frac{\tilde{\mathcal{E}}_{i}^{\prime}-\widetilde{ loc_{i}^{\prime}}(e_{i}^{\prime})}{\widetilde{scale_{i}^{\prime}}(e_{i}^{\prime})})( \frac{\tilde{\mathcal{E}}_{j}^{\prime}-\widetilde{loc_{j}^{\prime}}(e_{j}^{ \prime})}{\widetilde{scale_{j}^{\prime}}(e_{j}^{\prime})})\right]=(\frac{E[ \tilde{\mathcal{E}}_{i}^{\prime}]-\widetilde{loc_{i}^{\prime}}(e_{j}^{\prime })}{\widetilde{scale_{i}^{\prime}}(e_{j}^{\prime})})(\frac{E[\tilde{ \mathcal{E}}_{j}^{\prime}]-\widetilde{loc_{j}^{\prime}}(e_{j}^{\prime})}{ \widetilde{scale_{j}^{\prime}}(e_{j}^{\prime})})\] \[\to E[\tilde{\mathcal{E}}_{i}^{\prime}\tilde{\mathcal{E}}_{j}^{ \prime}]\mathcal{E}^{\prime}]=E[\tilde{\mathcal{E}}_{i}^{\prime}|\mathcal{E}|E[ \tilde{\mathcal{E}}_{j}^{\prime}]\mathcal{E}^{\prime}]\] \[\to E[\tilde{\mathcal{E}}_{i}^{\prime}\tilde{\mathcal{E}}_{j}^{ \prime}]\mathcal{E}^{\prime}]-E[\tilde{\mathcal{E}}_{i}^{\prime}|\mathcal{E}|E[ \tilde{\mathcal{E}}_{j}^{\prime}]\mathcal{E}^{\prime}]-E[\tilde{\mathcal{E}}_{ i}^{\prime}|\mathcal{E}|E[\tilde{\mathcal{E}}_{j}^{\prime}]\mathcal{E}^{\prime}]-E[ \tilde{\mathcal{E}}_{i}^{\prime}|\mathcal{E}|E[\tilde{\mathcal{E}}_{j}^{\prime }]\mathcal{E}^{\prime}]+E[\tilde{\mathcal{E}}_{i}^{\prime}|\mathcal{E}^{ \prime}]E[\tilde{\mathcal{E}}_{j}^{\prime}|\mathcal{E}^{\prime}]=0\] \[\to E\left[(\tilde{\mathcal{E}}_{i}^{\prime}-E[\tilde{\mathcal{E} }_{i}^{\prime}]\mathcal{E}^{\prime})(\tilde{\mathcal{E}}_{j}^{\prime}-E[ \tilde{\mathcal{E}}_{j}^{\prime}]\mathcal{E}^{\prime})|\mathcal{E}^{\prime} \right]=0\] \[\to cov(\tilde{\mathcal{E}}_{i}^{\prime},\tilde{\mathcal{E}}_{j}^{ \prime}|\mathcal{E}^{\prime})=0\]

_Typically, the aforementioned equalities would be valid for any diffeomorphic solution function \(\tilde{s}_{i}:\tilde{\mathcal{E}}_{i}\to\tilde{\mathcal{E}}_{i}\). However, in this paper, we specifically focus on solution functions represented by a location-scale noise model._

_Assuming that the causal variables follow a **multivariate normal distribution conditional on the pre-intervention exogenous variables**, \(cov(\tilde{\mathcal{Z}}_{i}^{\prime},\tilde{\mathcal{Z}}_{j}^{\prime}| \mathcal{E}^{\prime})=0\) would imply that \(\tilde{\mathcal{Z}}_{i}^{\prime}\perp\!\!\!\perp\tilde{\mathcal{Z}}_{j}^{ \prime}|\mathcal{E}^{\prime}\). Let's define \(\phi_{\mathcal{E}}=g^{\prime-1}\circ g:\mathcal{E}\to\mathcal{E}^{\prime}\) where \(g\) and \(g^{\prime}\) are the decoders in \(\mathcal{M}\) and \(\mathcal{M}^{\prime}\). As stated in Assumption 3.1, the decoders are diffeomorphism, hence, \(\phi_{\mathcal{E}}\) is a diffeomorphism. Furthermore, let's denote \(\tilde{s}\) as the set of all solution functions in post-intervention which are also diffeomorphism as stated in Assumption 3.1. Consequently:_

\[(\phi_{\mathcal{E}}^{-1}\text{ is diffeomorphic})\ \forall i,j,i\neq j\quad\tilde{ \mathcal{Z}}_{i}^{\prime}\perp\!\!\!\perp\tilde{\mathcal{Z}}_{j}^{\prime}| \mathcal{E}^{\prime}\to\tilde{\mathcal{Z}}_{i}^{\prime}\perp\!\!\!\perp \tilde{\mathcal{Z}}_{j}^{\prime}|\phi_{\mathcal{E}}^{-1}(\mathcal{E}^{\prime}) \to\tilde{\mathcal{Z}}_{i}^{\prime}\perp\!\!\!\perp\tilde{\mathcal{Z}}_{j}^{ \prime}|\mathcal{E}\] \[\to p(\tilde{\mathcal{Z}}_{i}^{\prime}|\mathcal{E})p(\tilde{ \mathcal{Z}}_{j}^{\prime}|\mathcal{E})=p(\tilde{\mathcal{Z}}_{i}^{\prime},\tilde{ \mathcal{Z}}_{j}^{\prime}|\mathcal{E})\] \[(\text{all functions in }\tilde{s}\text{ are diffeomorphism})\to p(\tilde{ \mathcal{Z}}_{i}^{\prime}|\tilde{s}(\mathcal{E}))p(\tilde{\mathcal{Z}}_{j}^{ \prime}|\tilde{s}(E))=p(\tilde{\mathcal{Z}}_{i}^{\prime},\tilde{\mathcal{Z}}_{j}^ {\prime}|\tilde{s}(\mathcal{E}))\] \[\to p(\tilde{\mathcal{Z}}_{i}^{\prime}|\tilde{\mathcal{Z}})p( \tilde{\mathcal{Z}}_{j}^{\prime}|\tilde{\mathcal{Z}})=p(\tilde{\mathcal{Z}}_{i}^{ \prime},\tilde{\mathcal{Z}}_{j}^{\prime}|\tilde{\mathcal{Z}})\]

_The association between \(\tilde{\mathcal{Z}}^{\prime}\) and \(\tilde{\mathcal{Z}}\) arises from their shared observation space. We know that every causal variable in \(\mathcal{M}^{\prime}\) depends at least on one of the causal variables in \(\mathcal{M}\). If one of the causal variables in \(\mathcal{M}^{\prime}\) depended on more than one causal variable in \(\mathcal{M}\), it would create dependency between two variables in \(\mathcal{M}^{\prime}\) and violate the above equality. Therefore, no variable in \(\mathcal{M}^{\prime}\) depends on more than one causal variable in \(\mathcal{M}\). Consequently, the transformation \(\phi_{\mathcal{Z}}\) is a component-wise transformation._

**Theorem A.1.6**.: _(Identifiability of latent causal models.) Let \(\mathcal{M}=(\mathcal{A},\mathcal{X},g,\mathcal{I})\) and \(\mathcal{M}^{\prime}=(\mathcal{A}^{\prime},\mathcal{X},g^{\prime},\mathcal{I})\) be two LCMs with shared observation space \(\mathcal{X}\) and shared intervention targets \(\mathcal{I}\). Suppose the following conditions are satisfied: **1.**: _Identical correspondence assumptions explained in 3.1._
**2.**: _Soft interventions satisfy Assumption 3.3._
**3.**: _The causal and exogenous variables are real-valued._
**4.**: _The causal and exogenous variables follow a multivariate normal distribution._
**_Then the following statements are equivalent: **-Two LCMs \(\mathcal{M}\) and \(\mathcal{M}^{\prime}\) assign the same likelihood to interventional and observational data i.e.,_\(p_{\mathcal{M}}^{\mathcal{X}}(x,\tilde{x})=p_{\mathcal{M}^{\prime}}^{\mathcal{X}}(x,\tilde{x})\).

- \(\mathcal{M}\) and \(\mathcal{M}^{\prime}\) are disentangled, that is \(\mathcal{M}\sim_{r}\mathcal{M}^{\prime}\) according to Definition 3.4.

**Proof** We will proceed to prove the equivalence between statements 1 and 2 by showing the implication is true in each direction.

**A1.1**: \(\mathcal{M}\sim_{r}\mathcal{M}^{\prime}\Rightarrow p_{\mathcal{M}}^{\mathcal{X }}(x,\tilde{x})=p_{\mathcal{M}^{\prime}}^{\mathcal{X}}(x,\tilde{x})\)

This direction is fairly straightforward. According to Definition 3.4, the fact that \(M\sim_{r}M^{\prime}\) implies that \(\phi_{\mathcal{E}}\) is measure preserving. Therefore, \(p_{\mathcal{M}^{\prime}}^{\mathcal{E}}(e^{\prime},\tilde{e}^{\prime})=(\phi_{ \mathcal{E}})_{*}p_{\mathcal{M}}^{\mathcal{E}}(e,\tilde{e})\). Furthermore, considering that ancestry is preserved, \(\phi_{\mathcal{Z}}\) is measure preserving, and that causal variables are obtained from their ancestral exogenous variables in implicit models, we have \(p_{\mathcal{M}^{\prime}}^{\mathcal{Z}}(z^{\prime},\tilde{z}^{\prime})=(\phi_{ \mathcal{Z}})_{*}p_{\mathcal{M}}^{\mathcal{Z}}(z,\tilde{z})\). Since models are trained to maximize the log likelihood of \(p(x,\tilde{x},\tilde{x}-x)\) and the latent spaces in \(M\) and \(M^{\prime}\) have the same distribution, the decoders should yield the same observational distributions \(p_{\mathcal{M}}^{\mathcal{X}}(x,\tilde{x})=p_{\mathcal{M}^{\prime}}^{\mathcal{ X}}(x,\tilde{x})\).

**A1.2**: \(p_{\mathcal{M}}^{\mathcal{X}}(x,\tilde{x})=p_{\mathcal{M}^{\prime}}^{\mathcal{ X}}(x,\tilde{x})\Rightarrow\mathcal{M}\sim_{r}\mathcal{M}^{\prime}\)

Let's define \(\phi_{\mathcal{E}}=g^{\prime-1}\circ g:\mathcal{E}\rightarrow\mathcal{E}^{\prime}\). Since we can express \(e=s^{-1}(z)\), we can now define \(\phi_{\mathcal{Z}}\) as

\[\phi_{\mathcal{Z}}=s^{\prime}\circ g^{\prime-1}\circ g\circ s^{-1}:\mathcal{Z }\rightarrow\mathcal{Z}^{\prime}.\] (8)

Therefore, \(\phi_{\mathcal{E}}=s^{\prime-1}\circ\phi_{\mathcal{Z}}\circ s\). Because \(g\) and \(g^{\prime}\) are **diffeomorphisms**, \(\phi_{\mathcal{E}}\) is a diffeomorphism as well. Furthermore, since \(p_{\mathcal{M}}^{\mathcal{X}}=p_{\mathcal{M}^{\prime}}^{\mathcal{X}}\) and \(\phi_{\mathcal{E}}\) is a diffeomorphism, then \(p_{\mathcal{M}^{\prime}}^{\mathcal{E}}=(\phi_{\mathcal{E}})_{*}p_{\mathcal{M}}^ {\mathcal{E}}\). Consequently, \(\phi_{\mathcal{E}}\) is measure-preserving. Similarly, \(\phi_{\mathcal{E}}\) is measure-preserving as well since causal mechanisms are **diffeomorphisms**.

**Step 1: Identical correspondence of edges and nodes** Let's define the set \(U\) as \(U=\{\mathcal{E}\times\mathcal{E}|\forall I,J\in\mathcal{I}:\;supp\;p_{ \mathcal{M}}^{\mathcal{E},\mathcal{I}}(e,\tilde{e}|I)\cap supp\;p_{\mathcal{M}} ^{\mathcal{E},\mathcal{I}}(e,\tilde{e}|J)\}\). Then, assuming **atomic** interventions and **counterfactual exogenous variables**, \(p_{\mathcal{M}}^{\mathcal{E},\mathcal{I}}(U|I)=p_{\mathcal{M}}^{\mathcal{E}, \mathcal{I}}(U|J)=0\). Therefore, we can say that \(p_{\mathcal{M}}^{\mathcal{E}}(e,\tilde{e})=\sum_{I\in\mathcal{I}}p_{\mathcal{M }}^{\mathcal{E},\mathcal{I}}(e,\tilde{e}|I)p_{\mathcal{M}}^{\mathcal{I}}(I)\) is a discrete mixture of non-overlapping distributions \(p_{\mathcal{M}}^{\mathcal{E},\mathcal{I}}(e,\tilde{e}|I)\). Similarly, we can say that \(p_{\mathcal{M}^{\prime}}^{\mathcal{E}}(e,\tilde{e})\) is a discrete mixture of non-overlapping distributions. It can be concluded that as \(\phi_{\mathcal{E}}\) must map between these distributions, there exists a bijection that also induces a permutation \(\psi:[n]\rightarrow[n]\). Note: If we had non-atomic interventions or non-counterfactual exogenous variables, then these distributions would have some overlapping. With overlapping distributions, we can no longer claim there is a bijection mapping between these distributions.

In space \(\mathcal{Z}\), the interventions should also be **sufficiently variable** in order to have non-overlapping \(p_{\mathcal{M}}^{\mathcal{Z},\mathcal{I}}(z,\tilde{z}|I)\) distributions. In the case of soft interventions, \(\tilde{z}\) is affected by all ancestral exogenous variables which could be ancestors of other causal variables as well. Consequently, if the changes in causal mechanisms are not sufficient, the effect of ancestral exogenous variables on causal variables will share some similarities and create overlapping distributions. Similar to \(p_{\mathcal{M}}^{\mathcal{E}}(e,\tilde{e}|I)\), we can say that there is a permutation between \(p_{\mathcal{M}}^{\mathcal{Z}}(z,\tilde{z}|I)\) as well. Furthermore, as we assume the target of interventions are known we have:

\[\forall I\in\mathcal{I}:\;p_{\mathcal{M}}^{\mathcal{Z}}(z,\tilde{z}|I)=p_{ \mathcal{M}^{\prime}}^{\mathcal{Z}}(z,\tilde{z}|I)\] (9)

Consequently, the permutation \(\psi\) is an identity transformation. The effect of soft intervention with known targets on these conditional distributions is shown in Figure A1.

**Step 2: Component-wise \(\phi_{\mathcal{Z}}\)**

According to Lemma A1.5, in order to prove that \(\phi_{\mathcal{Z}}\) is a component-wise transformation, we need to prove that \(\tilde{\mathcal{E}}_{i}^{\prime}\) and \(\tilde{\mathcal{E}}_{j}^{\prime}\) are independent \(\forall i,j,i\neq j\). In implicit modeling we do not know the parents of each causal variable, hence, we assume the distribution of \(\tilde{\mathcal{Z}}_{i}^{\prime}\) to be conditioned only on \(\mathcal{E}_{i}^{\prime}\) as in Equation 5 since \(\mathcal{E}_{i}^{\prime}\) is a known parent of \(\tilde{\mathcal{Z}}_{i}^{\prime}\). The mean of a conditional distribution can be calculated as:

\[E[\tilde{z}_{i}^{\prime}|e_{i}^{\prime}]=\mu_{z_{i}^{\prime}}+\rho\frac{\sigma _{\tilde{z}_{i}^{\prime}}}{\sigma_{e_{i}^{\prime}}}(e_{i}^{\prime}-\mu_{e_{i}^{ \prime}})\] (10)

where \(\rho\) and \(\sigma\) are the correlation coefficient and variance of the random variables, respectively. On the other hand, we model \(\tilde{\mathcal{Z}}_{i}^{\prime}\) using switch mechanisms as:\[\tilde{z}_{i}^{\prime}=s_{i}(\tilde{e}_{i}^{\prime};e_{/i}^{\prime},h(v^{\prime}))\]

By using Taylor's expansion we can write above equation as:

\[s_{i}(\tilde{e}_{i}^{\prime};e_{/i}^{\prime},h_{i}(v^{\prime}))=s_ {i}(\tilde{e}_{i}^{\prime};e_{/i}^{\prime},h_{i}(v^{\prime}_{0}))++\sum_{n=1}^{ \infty}\frac{1}{n!}\left(\frac{\partial^{n}s_{i}}{\partial h_{i}^{n}}\right|_ {h_{i}=h_{i}(v^{\prime}_{0})}(h_{i}(v^{\prime})-h_{i}(v^{\prime}_{0}))^{n}\right)\] \[=s_{i}(\tilde{e}_{i}^{\prime};e_{/i}^{\prime},h_{i}(v^{\prime}_{0 }))+R_{i}\]

Furthermore, we assume **separable dependence** such that:

\[\exists v^{\prime}_{0}\text{ such that }\forall i\quad s_{i}(\tilde{e}_{i}^{ \prime};e_{/i}^{\prime},h_{i}(v^{\prime}_{0}))=s_{i}(\tilde{e}_{i}^{\prime};e _{/i}^{\prime})\]

An example of such a scenario could be in location-scale noise models, where a soft intervention changes the location parameter of the model as:

\[s_{i}(e_{i}^{\prime};e_{/i}^{\prime})=e_{i}^{\prime}+loc(e_{/i}^ {\prime})\rightarrow\tilde{s}_{i}(\tilde{e}_{i}^{\prime};e_{/i}^{\prime})=s_ {i}(\tilde{e}_{i}^{\prime};e_{/i}^{\prime},h_{i}(v^{\prime}))\] \[=\tilde{e}_{i}^{\prime}+loc(e_{/i}^{\prime})+h_{i}(v^{\prime})= \tilde{e}_{i}^{\prime}+loc(e_{/i}^{\prime})+v^{\prime 2}+v^{\prime}\]In this example, for \(v^{\prime}_{0}=0\), \(s_{i}(\tilde{e}^{\prime}_{i};e^{\prime}_{/i},h_{i}(v^{\prime}_{0}))=s_{i}(\tilde{e }^{\prime}_{i};e^{\prime}_{/i})\).

Consequently, we can write the following equality from Equation 10:

\[E[\tilde{\mathcal{Z}}^{\prime}_{i}|e^{\prime}_{i}]=E[s_{i}(\tilde{\mathcal{E}} ^{\prime}_{i};\mathcal{E}^{\prime}_{/i})+R_{i}|e^{\prime}_{i}]=\mu_{\tilde{ \mathcal{Z}}^{\prime}_{i}}+\rho\frac{\sigma_{\tilde{\mathcal{Z}}^{\prime}_{i}} }{\sigma_{\mathcal{E}^{\prime}_{i}}}(e^{\prime}_{i}-\mu_{\tilde{\mathcal{E}}^ {\prime}_{i}})\]

By taking the partial derivative of both side with respect to \(\tilde{\mathcal{E}}^{\prime}_{j}\) we have:

\[\forall j\neq i\quad E[\frac{\partial s_{i}(\tilde{\mathcal{E}}^{\prime}_{i}; \mathcal{E}^{\prime}_{/i})}{\partial\tilde{\mathcal{E}}^{\prime}_{i}}\cdot \frac{\partial\tilde{\mathcal{E}}^{\prime}_{i}}{\partial\tilde{\mathcal{E}}^ {\prime}_{j}}+\frac{\partial s_{i}(\tilde{\mathcal{E}}^{\prime}_{i};\mathcal{E }^{\prime}_{/i})}{\partial\mathcal{E}^{\prime}_{/i}}\cdot\frac{\partial \mathcal{E}^{\prime}_{/i}}{\partial\tilde{\mathcal{E}}^{\prime}_{j}}+\frac{ \partial R_{i}}{\partial\tilde{\mathcal{E}}^{\prime}_{j}}|e^{\prime}_{i}]=0\]

If we did not have the causal mechanism switch variable \((h_{i}(\mathcal{V}^{\prime}))\), the equation above would only hold if \(s_{i}\) was constant in parents, which is not the case due to the presence of soft interventions, or if \(\frac{\partial s_{i}(\tilde{\mathcal{E}}^{\prime}_{i};\mathcal{E}^{\prime}_{/ i})}{\partial\tilde{\mathcal{E}}^{\prime}_{i}}\cdot\frac{\partial \mathcal{E}^{\prime}_{i}}{\partial\tilde{\mathcal{E}}^{\prime}_{j}}=-\frac{ \partial s_{i}(\tilde{\mathcal{E}}^{\prime}_{i};\mathcal{E}^{\prime}_{/i})}{ \partial\mathcal{E}^{\prime}_{/i}}\cdot\frac{\partial\mathcal{E}^{\prime}_{/ i}}{\partial\tilde{\mathcal{E}}^{\prime}_{j}}\). The latter scenario would imply that \(\frac{\partial\tilde{\mathcal{E}}^{\prime}_{i}}{\partial\tilde{\mathcal{E}}^ {\prime}_{j}}\neq 0\), hence, \(\tilde{\mathcal{E}}^{\prime}_{i}\not\perp\tilde{\mathcal{E}}^{\prime}_{j}\). However, by introducing the causal mechanism switch variable \(\mathcal{V}\) and assuming it is observed, we can account for the effects of soft interventions through \(h_{i}(\mathcal{V}^{\prime})\). In this case, \(\frac{\partial\mathcal{E}^{\prime}_{i}}{\partial\mathcal{E}^{\prime}_{j}}=0\) as exogenous variables are commonly assumed to be independent in practice. Consequently:

\[\forall i,j\quad\tilde{\mathcal{E}}^{\prime}_{i}\perp\tilde{ \mathcal{E}}^{\prime}_{j}\] \[\rightarrow\forall i,j\quad p(\tilde{\mathcal{E}}^{\prime}_{i}, \tilde{\mathcal{E}}^{\prime}_{j}|\tilde{\mathcal{E}}_{i},\tilde{\mathcal{Z}}_ {j})=p(\tilde{\mathcal{Z}}^{\prime}_{i}|\tilde{\mathcal{Z}}_{i})p(\tilde{ \mathcal{Z}}^{\prime}_{j}|\tilde{\mathcal{Z}}_{j})\] \[\rightarrow\phi_{\mathcal{Z}}\text{ is a component-wise transformation.}\]

**Step 3: Component-wise \(\phi_{\mathcal{Z}}\)**

Using the result from previous step that \(\phi_{\mathcal{Z}}\) is a component-wise transformation, the string diagrams for connections between \(\mathcal{E}\) and \(\mathcal{E}^{\prime}\) will be as shown in Figure 10. \(\phi_{\mathcal{E}_{i}}\) will only depend on \(\mathcal{E}_{A}\), where \(A=anc_{i}\) is the ancestors of variable \(i\), and \(e_{i}\). Because \(s(e)_{anc_{i}}\), \(s(e)_{i}\), and \(s^{\prime-1}(z^{\prime})_{i}\) only depend on ancestors and \(\phi_{\mathcal{Z}}\) is a component-wise transformation. The first equality in Figure 10 follows from the definition of \(\phi_{\mathcal{E}_{i}}\). The second equality holds when we first apply \(\phi_{\mathcal{Z}_{A}}\) and then apply the causal mechanisms. It can be concluded from the most right-hand side diagram in Figure 10 that the transformation from \(\mathcal{E}^{\prime}_{i}\times\mathcal{E}_{A}\rightarrow\mathcal{E}^{\prime}_{i}\) is constant in \(\mathcal{E}_{A}\). Therefore, \(\phi_{\mathcal{E}_{i}}\) is a component-wise transformation.

### Soft vs. Hard intervention

In a causal model, an intervention refers to a deliberate action taken to manipulate or change one or more variables in order to observe its impact on other variables within the causal model. Interventions help to study how changes in one variable directly cause changes in another, thereby revealing causal relationships.

Based on the levels of control and manipulation in a causal intervention, we can have soft vs. hard interventions. A hard intervention involves directly manipulating the variables of interest in a controlled manner such as Randomized Controlled Trials (RCTs). In other words, a hard intervention sets the value of a causal variable \(Z\) to a certain value denoted as \(do(Z=z)\)[24].

On the other hand, soft intervention involves more subtle or less controlled manipulation of variables and changes the conditional distribution of the causal variable \(p(Z|Z_{pa})\to\tilde{p}(Z|Z_{pa})\) which can be modeled as \(\tilde{z_{i}}=\tilde{f}_{i}(z_{pa_{i}},\tilde{e_{i}})\)[7].

Looking at interventions from a graphical standpoint, a hard intervention entails that the intervened node is solely impacted by the intervention itself, with no influence coming from its ancestral nodes. Conversely, in the context of a soft intervention, the representation of the intervened node can be influenced not only by the intervention but also by its parent nodes.

As an example, suppose we are trying to understand the causal relationship between different types of diets and weight loss. The _soft intervention_ in this scenario could be a switch from a regular diet to a low-carb diet. Switching to a low-carb diet is a voluntary choice made by the individual and there are no external forces or regulations compelling them to make this change (non-coercive).

The intervention involves a modification of the individual's diet rather than a complete disruption since they are adjusting the proportion of macronutrients (fats, proteins, and carbs) they consume, which is less disruptive than a radical change in eating habits (gradual modification). The individual has autonomy to choose and tailor their diet according to their preferences and health goals so they are empowered to make informed decisions about their dietary choices (behaviour empowerment).

Conversely, if the government or an authority were to intervene and enforce a mandatory low-carb diet through legal means, this would constitute a _hard intervention_. In this scenario, regulations would be implemented, prohibiting the consumption of specific carbohydrate-containing foods. Regulatory agencies would be established to oversee and ensure adherence to the low-carb diet mandate, taking actions such as removing prohibited foods from the market, restricting their import and production, and so on. Individuals caught consuming banned foods would be subject to fines, legal repercussions, or other penalties.

## Appendix A3 Experiments

This section contains additional details about ICRL-SM design architectures, datasets, and experiments settings.

### Datasets

#### Synthetic

We generate simple synthetic datasets with \(\mathcal{X}=\mathcal{Z}=\mathbb{R}^{n}\). For each value of \(n\), we generate ten random DAGs, a random location-scale SCM, then a random dataset from the parameterized SCM. To generate random DAGs, each edge is sampled in a fixed topological order from a Bernoullidistribution with probability 0.5. The pre-intervention and post-intervention causal variables are obtained as:

\[z_{i}=scale(z_{pa_{i}})e_{i}+loc(z_{pa_{i}})\xrightarrow{\text{Soft- intervention}}\tilde{z}_{i}=scale(z_{pa_{i}})\tilde{e_{i}}+\widetilde{loc}(z_{pa_{i}}),\] (11)

where the \(loc\) and \(scale\) networks are changed in post intervention. The pre-intervention \(loc\) and post-intervention \(\widetilde{loc}\) network weights are initialized with samples drawn from \(\mathcal{N}(0,1)\) and \(\mathcal{N}(3,1)\), respectively. For ablation studies, we change the mean of these Normal distributions. The \(scale\) is constant 1 for both pre-intervention and post-intervention samples. Both \(e_{i}\) and \(\tilde{e_{i}}\) are sampled from a standard Gaussian. The causal variables are mapped to the data space through a randomly sampled \(SO(n)\) rotation. For each dataset, we generate 100,000 training samples, 10,000 validation samples, and 10,000 test samples.

#### a3.1.2 Causal-Triplet

The Causal-Triplet datasets are consisted of images containing objects in which an action is manipulating the objects shown in Figure A4. Examples of actions and objects in these datasets are given in Table A1 and A2.

Based on the actions and objects, we treat our causal variables as attributes of objects which can be changed by actions. Therefore, actions in these datasets are considered as interventions. Assume that \(z_{1}\) corresponds to the attributes of an object, e.g. a door, the target of opening or closing (action's target) is \(z_{1}\).

We use actions' labels in these datasets to detect the targets of interventions to determine which causal variable has been intervened upon. Note that informing the model about the target of intervention is not same as informing about the action itself (See Table 3). We use 5000 images of these datasets to train all models.

#### a3.2 Architecture Design

Based on the ICRL-SM architecture depicted in Figure 2a, we devised a location-scale solution function (Equation 6) in which the \(loc_{i}\) and \(scale_{i}\), and \(h_{i}\) networks each comprise of fully connected networks. These networks consist of two layers each, with 64 hidden units per layer and ReLU activation functions. The encoder and decoder parameters for latents \(\mathcal{E}\) and \(\mathcal{\hat{E}}\) are shared and we use a separate encoder and decoder with the same architecture for the latent \(\mathcal{V}\). For our synthetic dataset experiments, the encoder and decoder are consisted of fully connected networks with 2 hidden layers and 64 units in each hidden layer. For the Causal-Triplet datasets, we utilized ResNet-based networks. The same encoder and decoder architectures are used for all baseline models in the experiments. ResNet50 encoder, ResNet50 decoder, and classifiers with 1 hidden layer and 64 hidden units are used for predicting actions and objects for experiments in Table 4 and Table 3. ResNet18 encoder, ResNet18 decoder, and classifiers with 2 hidden layer and 2 hidden units are used for predicting actions and objects for experiments in Table A4 and Table A3.

#### a3.3 Training

To enforce the condition described in Equation 5 for \(i\notin\mathcal{I}\), we assign the post-intervention exogenous variables the same value as the pre-intervention exogenous variables. In mathematical terms, this translates to \(\forall i\notin\mathcal{I}\), we set \(\tilde{e}_{i}=e_{i}\).

In our experiments, we do not pretrain the networks, however, for the baseline models we follow the training procedure in [3]. We also use consistency in our experiments to ensure that the encoder and decoder are inverse of each other. Consistency regularizer is used as \(\sum_{i}E_{\hat{x}\sim p(\hat{x}|e),x\sim p(x)}[(x-\hat{x})^{2}]\) where \(\hat{x}\) are the reconstructed samples.

For optimization, Adam optimizer is used with default hyperparamters. In the synthetic experiments, learning rate changes from \(3e-4\) to \(1e-8\) with a cosine scheduler. In the Causal-Triplet experiments in Table 4 and Table 3 learning rate changes from \(0.002\) to \(1e-8\) with a cosine scheduler. For Table A4 and Table A3 experiments earning rate changes from \(0.0001\) to \(1e-8\) with a cosine scheduler. In all experiments the batch size is set to 64. In the main Causal-Triplet experiments we train the models for 2000 epochs, and for 400 epochs, in the appendix Causal-Triplet experiments we train the models for 100 epochs. In the appendix experiments, the graph parameters for explicit models are frozen after 1000 epochs.

All models are trained using Nvidia GeForce RTX4090 GPUs. Each of the Causal-Triplet experiments takes 3-8 hours to train the models and each of the synthetic experiments takes 2-3 hours to train the models.

We save the models' weights with best validation loss and evaluate them using those weights with test data.

## A4 Ablation study

### Scalability

While our primary research objective centered on addressing identifiability challenges in implicit causal models under soft interventions, we also conducted an investigation into the scalability of our proposed model. To comprehensively assess its performance, we designed experiments covering a range of causal graphs, featuring 5 to 10 variables, with 10 different seeds for each variable, following a similar experimental setup as our 4-variable causal graph experiments. The outcomes of these experiments, comparing ICRL-SM and ILCM, are presented in Figure A6. By increasing the number of variables in the graph, confounding factors and ambiguities of causal relations increase as well. Consequently, more supervision on \(\mathcal{V}\) is required to better separate the effect of causal variables themselves on the observed variables.

### Backbone model

We trained the models using a simpler backbone model, ResNet18, to see how it affects performance. The input image resolution is \(64\times 64\) and we use the intervened causal variables to predict action 

[MISSING_PAGE_EMPTY:21]

NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our contributions include identifiability of causal models with soft interventions. In the proposed methods section we give the theory and assumptions for the identifiability result and in our experiments we evaluate our method using datasets generated by soft interventions. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have some strict assumptions on data generation process and model which are given in Assumptions 3.3 and 3.1 which may not be plausible to satisfy in some applications. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?

Answer: [Yes]

Justification: We give the full set of our assumptions in the proposed method section and the detailed proof in Appendix A1.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer:[Yes] Justification: We provide the full details of our model architecture and training settings in Appendix A3 and in Section 5. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide our anonymized codes which contains the necessary scripts and instructions to run the experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide the full details of our model architecture and training settings in Appendix A3 and in Section 5. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: In our synthetic experiments we initialized the causal graph in the dataests with different seeds. The results of these different seeds are provided in Table 2 and Figure A6. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ** The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The details are given in Appendix A3. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We only have a code repository for replicating experiments and we have submitted the anonymized zip file with our submission. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.