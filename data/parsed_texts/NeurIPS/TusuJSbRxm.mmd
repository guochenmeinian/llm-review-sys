Trajectory Data Suffices for Statistically Efficient Learning in Offline RL with Linear \(q^{\pi}\)-Realizability and Concentrability

Volodymyr Tkachuk

University of Alberta, Edmonton, Canada

&Gellert Weisz

Google DeepMind, London, UK

Csaba Szepesvari

Google DeepMind, Edmonton, Canada

University of Alberta, Edmonton, Canada

###### Abstract

We consider offline reinforcement learning (RL) in \(H\)-horizon Markov decision processes (MDPs) under the linear \(q^{\pi}\)-realizability assumption, where the action-value function of every policy is linear with respect to a given \(d\)-dimensional feature function. The hope in this setting is that learning a good policy will be possible without requiring a sample size that scales with the number of states in the MDP. Foster et al. (2021) have shown this to be impossible even under _concentrability_, a data coverage assumption where a coefficient \(C_{\text{cone}}\) bounds the extent to which the state-action distribution of any policy can veer off the data distribution. However, the data in this previous work was in the form of a sequence of individual transitions. This leaves open the question of whether the negative result mentioned could be overcome if the data was composed of sequences of full trajectories. In this work we answer this question positively by proving that with trajectory data, a dataset of size \(\text{poly}(d,H,C_{\text{cone}})/\epsilon^{2}\) is sufficient for deriving an \(\epsilon\)-optimal policy, regardless of the size of the state space. The main tool that makes this result possible is due to Weisz et al. (2023), who demonstrate that linear MDPs can be used to approximate linearly \(q^{\pi}\)-realizable MDPs. The connection to trajectory data is that the linear MDP approximation relies on "skipping" over certain states. The associated estimation problems are thus easy when working with trajectory data, while they remain nontrivial when working with individual transitions. The question of computational efficiency under our assumptions remains open.

## 1 Introduction

We study the offline reinforcement learning (RL) setting, where the objective is to derive a near-optimal policy for an \(H\)-horizon Markov decision process (MDP) using _offline data_. This contrasts with the online RL paradigm, where learners interact directly with an MDP - or its simulator - to collect new data. Offline RL is especially relevant when acquiring new data guided by the learner is infeasible or ill-advised for safety reasons.

Deriving a near-optimal policy is only possible from offline data that covers the MDP well enough. One way to formalize this as an assumption, which we adopt for this work, is called _concentrability_. This assumption posits that the offline data sufficiently covers the distribution of state-action pairs that are accessible through running any policy. Challenges also arise in MDPs characterized by large or infinite state spaces. In such scenarios, an efficient learner's data requirements cannot scale with the state space size. An approach to remove state space dependence is to assume that the state-action value function of any policy can be linearly represented using a \(d\)-dimensional feature map, an assumption known as _linear \(q^{\pi}\)-realizability_.

While linear \(q^{\pi}\)-realizability facilitates efficient online RL (Weisz et al., 2023), its applicability has been limited in offline contexts. For instance, Foster et al. (2021) proves that no learning algorithm can derive an \(\epsilon\)-optimal policy under linear \(q^{\pi}\)-realizability and concentrability bounded by \(C_{\text{conc}}\), with a \(\mathrm{poly}(d,H,C_{\text{conc}},\epsilon^{-1})\) number of samples. However, their result does not apply to _trajectory data_, where the offline data contains full sequences of state, action, and reward tuples obtained by following some policy from the initial state to the terminal state. The following problem is left open:

_"Does there exist an efficient learner that outputs an \(\epsilon\)-optimal policy, under the assumptions of linear \(q^{\pi}\)-realizability, concentrability, and trajectory data?"_

Our findings affirmatively answer this question in terms of sample complexity, highlighting a notable distinction in the requirements for trajectory data versus general offline data for effective learning. This underscores the practical value of accumulating trajectory data whenever feasible.

## 2 Related Works

In Table 1 we provide a comparison of our result to the other works in offline RL discussed below.

**Lower bounds:** As we have already discussed in Section 1, the work of Foster et al. (2021) shows a lower bound that depends on the size of the state space (in the same setting as ours), except they do not assume access to trajectory data. The work by Jia et al. (2024) is perhaps the most relevent to ours. They show an exponential lower bound in the horizon for policy evaluation, under the assumptions of trajectory data, concentrability, and a _restricted_ linear \(q^{\pi}\)-realizability where the value function of only the target policy is linear. While we anticipate that evaluating policies (their focus) is no more difficult than optimizing policies (our focus), our \(q^{\pi}\) realizability is for all memoryless policies (Assumption 1), while theirs is restricted to the target policy. Zanette (2021) shows an exponential lower bound in terms of the feature dimension \(d\), under linear \(q^{\pi}\)-realizability, and various other structural assumptions; however, their setting would result in a concentrability coefficient larger than the size of the state space. Wang et al. (2020), Amortila et al. (2020) show a lower bound that is exponential in the horizon, under linear \(q^{\pi}\)-realizability. However, they use a \(\lambda_{\text{min}}\)_lower bound_ condition, which requires a lower bound on the minimum eigenvalue \(\lambda_{\text{min}}\) of the expected covariance matrix used for least-squares estimation. This is seen as a weaker condition than ours, as it only posits good coverage in terms of the feature space, not the (possibly much richer) state-action space.

**Upper bounds:**Chen and Jiang (2019), Munos and Szepesvari (2008) present efficient algorithms under concentrability and _Bellman completeness_, an assumption that the Bellman optimality operator outputs a linearly realizable function when its input is linearly realizable. As linear \(q^{\pi}\)-realizability does not imply Bellman completeness (Zanette et al., 2020), these results do not transfer

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline  & & \multicolumn{3}{c|}{Assumptions} & \\ Work & Task & Data & Structural & Result \\ \hline \hline (Xiong et al., 2022) & \(\pi\)-opt & \(\lambda_{\text{min}}\) lower bound & Linear MDP & ✓ \\ \hline (Chen and Jiang, 2019) & \(\pi\)-opt & Conc & Bellman complete & ✓ \\ \hline (Xie and Jiang, 2021) & \(\pi\)-opt & Strong Conc & \(q^{\pi}\) & ✓ \\ \hline (**This work**) & \(\pi\)-opt & Conc \& Traj\_data & \(q^{\pi}\) & ✓ \\ \hline (Foster et al., 2021) & \(\pi\)-opt or \(\pi\)-eval & Conc & \(q^{\pi}\) & x \\ \hline (Wang et al., 2020) & \(\pi\)-eval & \(\lambda_{\text{min}}\) lower bound & \(q^{\pi}\) & x \\ \hline (Jia et al., 2024) & \(\pi\)-eval & Conc & Restricted \(q^{\pi}\) & x \\ \hline \end{tabular}
\end{table}
Table 1: Notation is defined as: \(\pi\)-opt = policy optimization, \(\pi\)-eval = policy evaluation, Conc = Concentrability, \(q^{\pi}\) = linear \(q^{\pi}\)-realizability, Traj = Trajectory, ✓ = \(\mathrm{poly}(d,H,C_{\text{conc}},1/\epsilon)\) sample complexity, x = exponential lower bound in terms of one of \(d,H,C_{\text{conc}}\).

to our setting. Xie and Jiang (2021) show an upper bound under linear \(q^{\pi}\)-realizability, albeit, using a stronger notion of data coverage than concentrability, which we call _strong concentrability_. The work of Xie et al. (2021, 2022) give data-dependent sample complexity bounds that hold under both Bellman completeness and linear \(q^{\pi}\)-realizability even in the absence of explicit data coverage assumptions. Jin et al. (2021) assume a general function approximation setting and also provide data-dependent bounds, while Duan et al. (2020), Xiong et al. (2022) show upper bounds for _linear MDPs_ (a stronger assumption than linear \(q^{\pi}\)-realizability (Zanette et al., 2020)) with the \(\lambda_{\text{min}}\) lower bound condition.

## 3 Setting

Throughout we fix the integer \(d\geq 1\). Let \(\vec{0}\in\mathbb{R}^{d}\) be the \(d\)-dimensional, all zero vector. For \(L>0\), let \(\mathcal{B}(L)=\{x\in\mathbb{R}^{d}:\left\|x\right\|_{2}\leq L\}\) denote the \(d\)-dimensional Euclidean ball of radius \(L\) centered at the origin, where \(\|\cdot\|_{2}\) denotes the Euclidean norm. The inner product \(\langle x,y\rangle\) for \(x,y\in\mathbb{R}^{d}\) is defined as the dot product \(x^{\top}y\). Let \(\mathbbm{1}\{B\}\) be the indicator function of a boolean-valued (possibly random) variable \(B\), taking the value \(1\) if \(B\) is true and \(0\) if false. Let \(\mathcal{M}_{1}(X)\) denote the set of probability distributions over the set \(X\). Let \(\mathbb{E}_{B\sim\mathcal{P}}\) denote the expectation of random variable \(B\) under distribution \(\mathcal{P}\). For integers \(i,j\), let \([i]=\{1,2,\ldots,i\}\) and \([i:j]=\{i,\ldots,j\}\). For a symmetric matrix \(M\in\mathbb{R}^{d\times d}\) we write \(\lambda_{\text{min}}(M)\) and \(\lambda_{\text{max}}(M)\) for its minimum and maximum eigenvalue.

The environment is modeled by a finite horizon Markov decision process (MDP). Fix the horizon to \(H\). This MDP is defined by a tuple \((\mathcal{S},\mathcal{A},P,\mathcal{R})\). Here, the state space \(\mathcal{S}\) is finite1, and organized by stages: \(\mathcal{S}=\bigcup_{h\in[H+1]}\mathcal{S}_{h}\), starting from a designated initial state \(s_{1}\) (\(\mathcal{S}_{1}=\{s_{1}\}\))2, and culminating in a designated terminal state \(s_{\top}\) (\(\mathcal{S}_{H+1}=\{s_{\top}\}\))3. Without loss of generality, we assume \(\mathcal{S}_{h}\) and \(\mathcal{S}_{h^{\prime}}\) for \(h\neq h^{\prime}\) are disjoint sets. Define the function \(\operatorname{stage}:\mathcal{S}\to[H+1]\), such that \(\operatorname{stage}(s)=h\) if \(s\in\mathcal{S}_{h}\). The action space \(\mathcal{A}\) is finite. The transition kernel is \(P:(\bigcup_{h\in[H]}\mathcal{S}_{h})\times\mathcal{A}\to\mathcal{M}_{1}( \mathcal{S})\), with the property that transitions occur between successive stages. Specifically, for any \(h\in[H]\), state \(s_{h}\in\mathcal{S}_{h}\), and action \(a\in\mathcal{A}\), \(P(s_{h},a)\in\mathcal{M}_{1}(\mathcal{S}_{h+1})\). The reward kernel is \(\mathcal{R}:\mathcal{S}\times\mathcal{A}\to\mathcal{M}_{1}([0,1])\). So that the terminal state \(s_{\top}\) has no influence on the learner we force the reward kernel to deterministically give zero reward for all actions \(a\in\mathcal{A}\) in \(s_{\top}\) (i.e. \(\mathcal{R}(s_{\top},a)(r)=\mathbbm{1}\{0=r\}\)). An agent interacts with this environment sequentially across an episode of \(H+1\) stages, by selecting an action \(a\in\mathcal{A}\) in the current state. The environment (except at stage \(H+1\)) then transitions to a subsequent state according to \(P\) and provides a reward in \([0,1]\) as specified by \(\mathcal{R}^{4}\).

Footnote 1: The state space is assumed to be finite to simplify presentation. Our results extend to infinite state spaces.

Footnote 2: A deterministic start state \(s_{1}\) is added for simplicity of presentation. It is easy to show that adding an additional stage to the MDP allows for the transition dynamics to encode an arbitrary start state distribution.

Footnote 3: A terminal state \(s_{\top}\) is added purely as a technical convenience for the analysis. We will focus on the interaction of learners for stages \(h\in[H]\) (not \([H+1]\)), since the terminal state will have no affect on the learner.

We define an agent's interaction with the MDP through a _policy_\(\pi\), which assigns a probability distribution over actions based on the history of interactions (including states, actions, and rewards). For this work, we restrict policies to be _memoryless_, that is, their action distribution depends solely on the most recent state in the history. The set of all memoryless policies is \(\Pi=\{\pi:\pi:\mathcal{S}\to\mathcal{M}_{1}(\mathcal{A})\}\). For \(\pi\in\Pi\), we write \(\pi(a|s)\) to denote the probability \(\pi(s)\) assigns to action \(a\). For deterministic policies only (i.e., those that for each state place a unit probability mass on some action) we sometimes abuse notation by writing \(\pi(s)\) to denote \(\operatorname{arg\,max}_{a\in\mathcal{A}}\pi(a|s)\). Starting from any state \(s\) within the MDP and using a policy \(\pi\) induces a probability distribution over trajectories, denoted as \(\mathbb{P}_{\pi,s}\). For any \(a\in\mathcal{A}\), \(\mathbb{P}_{\pi,s,a}\) is the distribution over the trajectories when first action \(a\) is used in state \(s\), after which policy \(\pi\) is followed. Specifically, for some \(h\in[H+1]\) and \((s,a)\in\mathcal{S}_{h}\times\mathcal{A}\), we write \(\text{Traj}\sim\mathbb{P}_{\pi,s,a}\) to denote that \(\text{Traj}=(S_{h},A_{h},R_{h},\ldots,S_{H+1},A_{H+1},R_{H+1})\) for a random trajectory that follows the distribution specified by \(\mathbb{P}_{\pi,s,a}\), that is, \(S_{h}=s\), \(A_{h}=a\), \(A_{i}\sim\pi(S_{i})\) for \(i\in[h+1:H+1]\), \(S_{i+1}\sim P(S_{i},A_{i})\) for \(i\in[h:H]\), and \(R_{i}\sim\mathcal{R}(S_{i},A_{i})\) for \(i\in[h:H+1]\). Writing \(\text{Traj}\sim\mathbb{P}_{\pi,s}\) has an analogous meaning, with the only difference being that \(A_{h}\) is not fixed,and instead \(A_{h}\sim\pi(S_{h})\). For \(h\in[H+1]\), we write \(\mathbb{P}_{\pi,s}^{h}\) (and \(\mathbb{P}_{\pi,s,a}^{h}\)) for the marginal distribution of \((S_{h},A_{h})\) (i.e., the state-action pair of stage \(h\)) under the joint distribution of \(\mathbb{P}_{\pi,s}\) (and \(\mathbb{P}_{\pi,s,a}\)).

For \(1\leq t\leq t^{\prime}\leq H+1\), we use the notation \(x_{t:t^{\prime}}=(x_{u})_{u\in[t:t^{\prime}]}\) throughout, except when \((x_{u})_{u\in[t:t^{\prime}]}\) are a sequence of scalar rewards. In that case, for convenience, we write \(r_{t:t^{\prime}}=\sum_{u=t}^{t^{\prime}}r_{u}\) and \(R_{t:t^{\prime}}=\sum_{u=t}^{t^{\prime}}R_{u}\). The state-value and action-value functions \(v^{\pi}\) and \(q^{\pi}\) are defined as the expected total reward along the rest of the trajectory while \(\pi\) is used:

\[v^{\pi}(s)=\underset{\text{Traj}\sim\mathbb{P}_{\pi,s,a}}{\mathbb{E}}R_{\text {stage}(s):H}\text{ for }s\in\mathcal{S}\quad\text{and}\quad q^{\pi}(s,a)=\underset{\text{Traj}\sim \mathbb{P}_{\pi,s,a}}{\mathbb{E}}R_{\text{stage}(s):H}\text{ for }(s,a)\in\mathcal{S} \times\mathcal{A}\,.\]

Let \(\pi^{\star}\in\Pi\) be an optimal policy, satisfying \(q^{\pi^{\star}}(s,a)=\sup_{\pi\in\Pi}q^{\pi}(s,a)\) for all \((s,a)\in\mathcal{S}\times\mathcal{A}\). Let \(q^{\star}(s,a)=q^{\pi^{\star}}(s,a)\) and \(v^{\star}(s)=\max_{a\in\mathcal{A}}q^{\star}(s,a)\) for all \((s,a)\in\mathcal{S}\times\mathcal{A}\). By definition, we have

\[v^{\star}(s_{\top})=v^{\pi}(s_{\top})=0\quad\text{and}\quad q^{\star}(s_{\top},a)=q^{\pi}(s_{\top},a)=0\qquad\text{ for all }\pi\in\Pi,a\in\mathcal{A}\,.\] (1)

### Assumptions and Problem Statement

A feature map is defined as \(\phi:\mathcal{S}\times\mathcal{A}\to\mathcal{B}(L_{1})\) for some \(L_{1}>0\). The representative power of a feature map for an MDP is described by the following assumption:

**Assumption 1** (\((\eta,L_{2})\)-Approximately Linear \(q^{\pi}\)-Realizable MDP).: _For some \(\eta\geq 0,L_{2}>0\), assume that the MDP (together with a feature map \(\phi\)) is such that_

\[\sup_{\pi\in\Pi}\min_{\theta_{h}\in\mathcal{B}(L_{2})}\max_{(s_{h},a_{h})\in \mathcal{S}_{h}\times\mathcal{A}}|q^{\pi}(s_{h},a_{h})-\langle\phi(s_{h},a_{h} ),\theta_{h}\rangle|\leq\eta\qquad\text{ for all }h\in[H+1]\,.\]

_For any \(h\in[H+1]\), let \(\psi_{h}:\Pi\to\mathcal{B}(L_{2})\) be a mapping from policies to parameter values \(\theta_{h}\) that attain the \(\min\) in the above display. For \(h=H+1\), we restrict this mapping to \(\psi_{H+1}(\cdot)=\vec{0}\), which satisfies the above display by definition. We write \(\psi_{h:t}(\pi)\) for \((\psi_{h}(\pi),\ldots,\psi_{t}(\pi))\)._

We also make the following assumptions on the offline data (the relationship to non-trajectory data and the negative result by Foster et al. (2021) is discussed in Appendix B) :

**Assumption 2** (Full Length Trajectory Data).: _Assume the learner is given a dataset of full length trajectories5 and corresponding features of size \(n\geq 1\):_

Footnote 5: Our learner does not require explicit knowledge of the states within each trajectory; the features alone are sufficient.

\[\left(\text{traj}^{1},\ldots,\text{traj}^{n}\right)\quad\text{and}\quad\left(( \phi(s_{h}^{1},a))_{h\in[H],a\in\mathcal{A}},\ldots,(\phi(s_{h}^{n},a))_{h\in[ H],a\in\mathcal{A}}\right),\]

_where for some "data collection policy"\(\pi^{0}\in\Pi\) unknown to the learner, \((\text{traj}^{j})_{j=1}^{n}\) are independent samples from \(\mathbb{P}_{\pi^{0},s_{1}}\) where \(\text{traj}^{j}=(s_{i}^{j},a_{i}^{j},r_{i}^{j})_{t\in[H+1]}\). To simplify notation we write_

\[\phi_{h}^{j}=\phi(s_{h}^{j},a_{h}^{j})\quad\text{for all }h\in[H],j\in[n]\,.\]

**Definition 1** (Admissible Distribution).: _A sequence of \(H\) state-action distributions \(\nu=(\nu_{h})_{h\in[H]}\in(\mathcal{M}_{1}(\mathcal{S}_{h}\times\mathcal{A})) ^{H}\) is admissible for an MDP if there exists a policy \(\pi\in\Pi\) such that_

\[\nu_{h}(s_{h},a_{h})=\mathbb{P}_{\pi,s_{1}}^{h}(s_{h},a_{h})\quad\text{for all }(s_{h},a_{h})\in\mathcal{S}_{h}\times\mathcal{A},\ h\in[H]\,.\]

Define the state-action occupancy measure of the data collection policy \(\pi^{0}\) as \(\mu=(\mu_{h})_{h\in[H]}\) such that

\[\mu_{h}(s_{h},a_{h})=\mathbb{P}_{\pi^{0},s_{1}}^{h}(s_{h},a_{h})\quad\text{for all }(s_{h},a_{h})\in\mathcal{S}_{h}\times\mathcal{A},\ h\in[H]\,.\]

**Assumption 3** (Concentability).: _Assume there exists a constant \(C_{\text{conc}}\geq 1\), such that for all admissible distributions \(\nu=(\nu_{h})_{h\in[H]}\)_

\[\max_{h\in[H]}\max_{(s_{h},a_{h})\in\mathcal{S}_{h}\times\mathcal{A}}\left\{ \frac{\nu_{h}(s_{h},a_{h})}{\mu_{h}(s_{h},a_{h})}\right\}\leq C_{\text{conc}}\,.\]

**Problem 1**.: _Let \(\epsilon>0\). Under Assumptions 1 to 3, does there exist a learner, with access to only \(n=\operatorname{poly}(1/\epsilon,H,d,C_{\text{conc}},\log(1/\delta),\log(1/L_{1}),\log(1/L_{2}))\) full length trajectories (as defined in Assumption 2), that outputs a policy \(\pi\) such that, with probability at least \(1-\delta\),_

\[v^{\star}(s_{1})-v^{\pi}(s_{1})\leq\epsilon\,?\]Result

We resolve Problem 1 in the positive by defining a learner (Algorithm 1) that: selects parameters optimistically from modified MDPs that "skip over" certain states while preserving tight \(q\)-value estimation guarantees (achieved by solving Optimization Problem 1); then, outputs a greedy policy \(\pi^{\prime}\) (defined in line 3) over the selected parameters. This result is made formal in following theorem (proof in Section 5):

**Theorem 1**.: _Let \(\epsilon\in(0,H]\). Under Assumptions 1 to 3, if the number of full length trajectories \(n=\tilde{\Theta}(C^{4}_{\text{\rm conc}}H^{7}d^{4}/\epsilon^{2})\) and \(\eta=\tilde{\mathcal{O}}\big{(}\epsilon^{2}/(C^{2}_{\text{\rm conc}}H^{5}d^{2}) \big{)}\)6, then, with probability at least \(1-\delta\), the policy \(\pi^{\prime}\) output by our learner (Algorithm 1) satisfies_

Footnote 6: The bound on \(\eta\) is assumed for clarity of presentation, to avoid presenting two error terms in the final bound.

\[v^{\star}(s_{1})-v^{\pi^{\prime}}(s_{1})\leq\epsilon\,,\]

where \(\tilde{\Omega},\tilde{\mathcal{O}}\) and \(\tilde{\Theta}\) are the counterparts of \(\Omega,\mathcal{O}\) and \(\Theta\) from the big-Oh notation that hide polylogarithmic factors of the problem parameters \((1/\epsilon,1/\delta,H,d,C_{\text{\rm conc}},L_{1},L_{2})\). The following subsections focus on introducing the theory needed to formally present our learner, giving intuition behind our learner, and presenting our learner.

### Background Theory

Our learner relies on the observation due to Weisz et al. (2023) that linearly \(q^{\pi}\)-realizable MDPs are linear MDPs, as long as they contain no low-range states. The _range_ of a state is the largest possible regret from that state, that is, the largest difference in action-value that the choice of action in that state can make (up to misspecification):

\[\text{range}(s)=\sup_{\pi\in\Pi}\max_{a,a^{\prime}\in\mathcal{A}}\bigl{\langle} \phi(s,a,a^{\prime}),\psi_{\text{stage}(s)}(\pi)\bigr{\rangle}\quad\text{ for all }s\in\mathcal{S}\,,\] (2)

where \(\phi(s,a,a^{\prime})=\phi(s,a)-\phi(s,a^{\prime})\) is a notation we use to denote feature differences. Intuitively, the choice of actions in low-range states are unimportant, as

\[|v^{\pi}(s)-q^{\pi}(s,a)|\leq\text{range}(s)+2\eta\qquad\text{ for any }\pi\in\Pi\text{ and all }(s,a)\in\mathcal{S}\times\mathcal{A}.\] (3)

As a warm-up, consider the example MDPs shown in Fig. 1. We will transform the linearly \(q^{\pi}\)-realizable MDP on the left into a linear MDP on the right by "skipping over" the red low-range states. Let the features for both MDPs be \(\phi(s_{1},\cdot)=(1),\phi(s_{3},\cdot)=(0.5),\phi(\cdot,\cdot)=(0)\) otherwise. Then the left MDP is \((0,1)\)-approximately \(q^{\pi}\)-realizable, with realizability parameter \(\psi_{h}(\pi)=(1)\) for all \(h\in[H+1],\pi\in\Pi\). However, it is not a linear MDP, since the rewards cannot be represented by a linear function of the features. To see this, notice that there exists no \(\theta\) such that \(\langle\phi(s_{1},a_{1}),\theta\rangle=r(s_{1},a_{1})=1\) and \(\langle\phi(s_{1},a_{2}),\theta\rangle=r(s_{1},a_{2})=0.5\), since \(\phi(s_{1},a_{1})=\phi(s_{1},a_{2})=(1)\). We modify this MDP on the left to "skip over" low-range red states, by automatically taking the first available action at such states, and summing up the rewards along skipped paths. This turns the MDP into the one on the right of Fig. 1, which is a linear MDP.

The key fact about linear MDPs that we will use is that for any function \(f:\mathcal{S}\to[0,H]\) (e.g., \(v\)-value approximators), and any \(h\in[H]\), there is some parameter \(\theta_{h}\in\mathbb{R}^{d}\) so that for _any_\((s,a)\in\mathcal{S}_{h}\times\mathcal{A}\), \(\langle\phi(s,a),\theta_{h}\rangle\) gives the expectation of the reward plus \(f\)'s value on the next state. In our modified

Figure 1: The features for both MDPs are \(\phi(s_{1},\cdot)=(1),\phi(s_{3},\cdot)=(0.5),\phi(\cdot,\cdot)=(0)\) otherwise. **Left:** A \((0,1)\)-approximately \(q^{\pi}\)-realizable MDP. **Right:** Linear MDP, obtained by skipping low range (red) states in the left MDP. Source: Figure 1 from (Weisz et al., 2023).

MDP this result transfers to the fact that the expected sum of rewards along a skipped path, plus \(f\)'s value on the next state after the skipped path, is linearly realizable. Before making this result formal in Lemma 4.2, we clarify the skipping behavior.

First, we address the fact that we need an approximate, parametric bound on \(\text{range}(\cdot)\) with a parameter count that is independent of \(|\mathcal{S}|\). For \(h\in[2:H]\), let \(\Psi_{h}=\{\psi_{h}(\pi)\,:\,\pi\in\Pi\}\subseteq\mathcal{B}(L_{2})\) be the (compact) set of parameter values corresponding to all policies. For all \(h\in[2:H]\), fix a subset \(\bar{G}_{h}\subset\Psi_{h}\) of size \(|\bar{G}_{h}|=d_{0}:=\lceil 4d\log\log(d)+16\rceil\) that is the basis of a near-optimal design for \(\Psi_{h}\) (more precisely, satisfying Definition 3). The existence of such a near-optimal design follows from (Todd, 2016, Part (ii) of Lemma 3.9). Let \(\bar{G}=\bar{G}_{2:H}\), which we call the _true guess_. Now notice that \(\bar{G}\in\mathbf{G}\) where

\[\mathbf{G}=(\mathcal{B}(L_{2}))^{[2:H]\times[d_{0}]}\,.\] (4)

For \(G\in\mathbf{G}\) we will use the notation that \(G=G_{2:H}\), where \(G_{h}=(\vartheta_{h}^{i})_{i\in[d_{0}]}\in\mathcal{B}(L_{2})^{d_{0}}\). Any \(G=G_{2:H}\in\mathbf{G}\) can be used to define an approximate, low parameter-count "version" of range that is completely specified by \(\tilde{O}(Hd^{2})\) parameters:

\[\text{range}^{G}(s)=\max_{\vartheta\in G_{h}}\max_{a,a^{\prime}\in\mathcal{A}} \langle\phi(s,a,a^{\prime}),\vartheta\rangle\qquad\text{for all }h\in[2:H],s\in \mathcal{S}_{h}\,.\] (5)

As shown in Proposition 4.5 of (Weisz et al., 2023), \(\text{range}^{\bar{G}}\) can be used to bound the true range:

**Lemma 4.1**.: _For all \(h\in[2:H]\) and \(s\in\mathcal{S}_{h}\), \(\text{range}(s)\leq\sqrt{2d}\cdot\text{range}^{\tilde{G}}(s)\)._

Based on any \(G\in\mathbf{G}\), we are interested in simulating a modified MDP that "skips over" states \(s\) that have a low \(\text{range}^{G}(s)\), by taking an action according to \(\pi^{0}\), and presenting as the reward the summed up rewards along paths consisting of skipped states. This "modified MDP" only serves as intuition, and will not be formally defined or used in our formal arguments. Instead, we define the "skipping probability" parameter at state \(s\in\mathcal{S}\), with \(\alpha>0\) (set later in Eq. (35)), as

\[\omega_{G}(s)=\begin{cases}1&\text{if }s\not\in\mathcal{S}_{1}\cup\mathcal{S} _{H+1}\text{ and }\text{range}^{G}(s)\leq\alpha/\sqrt{2d}\\ 2-\sqrt{2d}\cdot\text{range}^{G}(s)/\alpha&\text{if }s\not\in\mathcal{S}_{1}\cup \mathcal{S}_{H+1}\text{ and }\alpha/\sqrt{2d}\leq\text{range}^{G}(s)\leq 2\alpha/\sqrt{2d}\\ 0&\text{otherwise.}\end{cases}\] (6)

The skipping behavior is probabilistic7: it never skips for stages \(1\) and \(H+1\) (where \(\text{range}^{G}\) is not defined); it always skips for ranges lower than some threshold, never skips for ranges higher than twice this threshold, and linearly interpolates between the two in between the thresholds. For \(h\in[H]\), and \(1\leq l\leq h\), let \(\text{traj}=(s_{t},a_{t},r_{t})_{l\leq t\leq H+1}\) be any fixed trajectory that starts from some stage \(l\). Let \(\tau\sim F_{G,\text{traj},h+1}\in\mathcal{M}_{1}([h+1:H+1])\) be the random stopping stage, when starting from state \(s_{h}\) and skipping subsequent states with probability \(\omega_{G}(\cdot)\). Formally, for \(t\in[h+1:H+1]\) let \(F_{G,\text{traj},h+1}(\tau=t)=(1-\omega_{G}(s_{t}))\prod_{u=h+1}^{t-1}\omega_{G }(s_{u})\). We will often write \(F_{G,h+1}^{j}\) to denote \(F_{G,\text{traj}^{j},h+1}\) where \(\text{traj}^{j}=(s_{t}^{j},a_{t}^{j},r_{t}^{j})_{t\in[H+1]},j\in[n]\).

Footnote 7: The resulting smoothness of skipping behavior is beneficial for a later technical covering argument (Eq. (90) in Lemma I.4).

Next, we present a key tool derived from results of Weisz et al. (2023): as long as the skips are informed by the true guess, the resulting MDP is approximately linear (proof in Appendix C):

**Lemma 4.2** (Approximate Linear MDP under the true guess).: _Let \(\eta\geq 0,L_{2}>0\). Let \(M\) be an \((\eta,L_{2})\)-approximately linear \(q^{\pi}\)-realizable MDP (Assumption 1) with corresponding feature map \(\phi\). Let \(\tilde{L}_{2}=L_{2}(8H^{2}d_{0}/\alpha+1)\). Then, for each \(f:\mathcal{S}\to[0,H]\) with \(f(s_{\top})=0\), policy \(\pi\in\Pi\), and stage \(h\in[H]\), there exists a parameter \(\rho_{h}^{\pi}(f)\in\mathcal{B}(\tilde{L_{2}})\) such that for all \((s,a)\in\mathcal{S}_{h}\times\mathcal{A}\),_

\[\left|\operatorname*{\mathbb{E}}_{\text{traj}\sim\mathbb{P}_{\pi,s,a}} \operatorname*{\mathbb{E}}_{\tau\sim F_{G,\text{traj},h+1}}[R_{h:\tau-1}+f(S_{ \tau})]-\langle\phi(s,a),\rho_{h}^{\pi}(f)\rangle\right|\leq\tilde{\eta}\,,\]

_where \(\tilde{\eta}=\eta(10H^{2}d_{0}/\alpha+1)\)._

### The Benefit of Trajectory Data

Our learner will heavily rely on the result presented in Lemma 4.2. We will need to learn good estimates of the parameters \(\rho_{h}^{\pi^{0}}(f)\), for any \(f:\mathcal{S}\to[0,H],h\in[H]\). However, to estimate a \(\rho_{h}^{\pi^{0}}(f)\)parameter well we will require least-squares targets that have bounded noise and expectation equal to \(\langle\phi(s,a),\rho_{h}^{\pi}(f)\rangle\) for all \((s,a)\in\mathcal{S}_{h}\times\mathcal{A}\). Full trajectory data (Assumption 2) makes this possible. Each full length trajectory \(\text{traj}^{j}=(s_{i}^{j},a_{i}^{j},r_{i}^{j})_{t\in[H+1]}j\in[n]\) can be used to create the following least-squares target (which has the desired properties):

\[\underset{\tau\sim F_{G,h+1}^{j}}{\mathbb{E}}\left[r_{h:\tau-1}^{j}+f\big{(}s_ {\tau}^{j}\big{)}\right].\]

Importantly, it is because we have full length trajectories that we can transform the data available to simulate arbitrary length skipping mechanisms.

### Intuition Behind our Learner

Next, we describe the high-level intuition and ideas behind our learner. Consider the "modified" MDP where low-range states are skipped. As the learner has access to trajectory data (Assumption 2), it can transform this data accordingly to simulate trajectories from the modified MDP. Any near-optimal policy for the modified MDP is also near-optimal for the original MDP (due to Eq. (3)). Thus, our previous linear realizability property (Lemma 4.2) allows for an offline RL version of the algorithm Eleanor(Zanette et al., 2020) to statistically efficiently derive a near-optimal policy for the modified MDP. Indeed, the optimization problem underlying Eleanor serves as a starting point for Optimization Problem 1, which is at the heart of our learner.

The challenge is that the true guess \(\bar{G}\) that Lemma 4.2 relies upon is not known to the learner. This means that the learner is not given any explicit information of what states to "skip over". To overcome this, we design a learner to output the policy \(\pi^{\prime}\) (defined in Algorithm 1) based on Optimization Problem 1, where the optimization problem considers all guesses for the possible values of \(\bar{G}\). For each \(G\in\mathbf{G}\), it considers the MDP that skips over low-range states when the range is calculated according to \(G\). It then calculates sets \(\Theta_{G,h}\) for each stage \(h\), that are guaranteed (with high probability) to include the parameter \(\psi_{h}(\pi^{\star}_{G})\) realizing \(q^{\pi^{\star}_{G}}\) (where \(\pi^{\star}_{G}\), defined in Eq. (15), is the optimal policy in the MDP with skipping based on \(G\)). We achieve this by defining \(\Theta_{G,h}\) backwards for \(h=H,H-1,\ldots,1\). By induction, if \(\Theta_{G,h+1},\ldots,\Theta_{G,H}\) all contain the desired parameter for their stage, then _some_ parameter sequence in the Cartesian product \(\Theta_{G,h+1}\times\cdots\times\Theta_{G,H}\) allows us to near-perfectly (up to some misspecification error) compute \(q^{\pi^{\star}_{G}}\)-values of stages \(>h\). Therefore, the least-squares parameter based on this sequence will be near the true parameter for stage \(h\). Defining \(\hat{\Theta}_{G,h}\) to be all least-squares predictors for sequences in the aforementioned Cartesian product, and \(\Theta_{G,h}\) to be unions of the confidence ellipsoids around these predictors ensures the true parameter \(\psi_{h}(\pi^{\star}_{G})\) realizing \(q^{\pi^{\star}_{G}}\) for stage \(h\) is included in \(\Theta_{G,h}\). This argument is made precise in Lemma D.2.

There are two problems remaining. One is that some values of \(G\) considered by Optimization Problem 1 lead to skipping over important large-value states, degrading the performance of the best policy \(\pi^{\star}_{G}\) available under that skipping. The other problem is that at the expense of making sure the true parameters are included in the sets \(\Theta_{G,h}\), these sets might become large, in the sense of containing parameters that lead to very different predictions. Avoiding the first problem would make \(v^{\pi^{\star}_{G}}(s_{1})\) nearly as large as \(v^{\star}(s_{1})\). Avoiding the second problem would lead to tight \(q\)-value estimators, and therefore to \(v^{\pi^{\prime}_{G}}(s_{1})\) being nearly as large as \(v^{\pi^{\star}_{G}}(s_{1})\), for a policy \(\pi^{\prime}_{G}\) that is greedy with respect to our hypothetically tight \(q\)-value estimator. A key idea is to reject from consideration any \(G\in\mathbf{G}\) that leads to \(q\)-estimations that are not sufficiently tight (Eq. (14)). The reason we can do this is because for \(G=\bar{G}\) we can show that this condition passes (with high probability), and therefore we do not reject \(\bar{G}\) (precise statement in Lemma D.3). We can show this since we have trajectory data (Assumption 2), allowing us to use least-squares targets of the form used in Eq. (12), which we know are linearly realizable when \(G=\bar{G}\) (discussed in Section 4.2). Finally, we resolve the first problem by selecting among these tight estimators the one that guarantees the highest policy value from \(s_{1}\), which can be no worse than the value guaranteed by the choice of \(G=\bar{G}\), which itself can be seen to be close to \(v^{\star}(s_{1})\).

### Learner

Next, we formally introduce our learner, at the heart of which lies Optimization Problem 1. We define various \(q\) and \(v\)-value estimators that we use. For \(x\in\mathbb{R}\), let \(\operatorname{clip}_{[0,H]}x=\max\{0,\min\{H,x\}\}\).

[MISSING_PAGE_FAIL:8]

Proof.: \(v^{\star}(s_{1})-v^{\pi^{\prime}}(s_{1})\) can be decomposed into the following error terms.

\[v^{\star}(s_{1})-v^{\pi^{\prime}}(s_{1})=\underbrace{v^{\star}(s_{1})-v^{\pi^{ \star}_{\bar{G}}}(s_{1})}_{\text{(I)}}+\underbrace{v^{\pi^{\star}_{\bar{G}}}(s_ {1})-v_{\theta^{\prime}_{1}}(s_{1})}_{\text{(II)}}+\underbrace{v_{\theta^{ \prime}_{1}}(s_{1})-v^{\pi^{\prime}}(s_{1})}_{\text{(III)}}\,.\]

The remainder of the proof focuses on bounding these error terms. Following the intuition described in Section 4, showing that terms \(\text{(I)}\) and \(\text{(II)}\) are small can be seen as addressing the first problem of potentially skipping over large-value states, while showing that term \(\text{(III)}\) is small can be seen as addressing the second problem of \(\pi^{\prime}\) being greedy w.r.t. to a potentially inaccurate estimates \(\theta^{\prime}_{1:H+1}\).

**Bounding \(\text{(I)}=v^{\star}(s_{1})-v^{\pi^{\star}_{\bar{G}}}(s_{1})\)**: This term cannot be too large since the range\({}^{\bar{G}}\) function is approximately correct (Lemma 4.1), and we only skip over states with low range\({}^{\bar{G}}\) (Eq. (6)), implying the action we take doesn't affect the value function much (Eq. (3)). In Appendix D.1 we formalize this intuition, and show the following result.

\[\text{(I)}=v^{\star}(s_{1})-v^{\pi^{\star}_{\bar{G}}}(s_{1})\leq H(2\alpha+2 \eta)\,.\] (16)

**Bounding \(\text{(II)}=v^{\pi^{\star}_{\bar{G}}}(s_{1})-\bar{v}_{\theta^{\prime}_{1}}(s _{1})\)**: This term can be bounded by approximately zero due to Optimization Problem 1 being optimistic from the start state. First, note that \(v^{\pi^{\star}_{\bar{G}}}\) is approximately equal to \(\bar{v}_{\psi_{1}(\pi^{\star}_{\bar{G}})}\) (Assumption 1). Then, in Lemma D.2 we show that \(\psi_{1}(\pi^{\star}_{\bar{G}})\in\Theta_{\bar{G},1}\), and in Lemma D.3 we show that \(\bar{G}\) is a feasible solution to Optimization Problem 1. Since \((G^{\prime},\theta^{\prime}_{1:H+1})\) is the solution to Optimization Problem 1, it holds that \(\bar{v}_{\theta^{\prime}_{1}}(s_{1})\geq\bar{v}_{\theta}(s_{1})\) for any \(\theta\in\Theta_{G,1}\) where \(G\) is a feasible solution to Optimization Problem 1. Thus \(\bar{v}_{\theta^{\prime}_{1}}(s_{1})\geq v_{\psi_{1}(\pi^{\star}_{\bar{G}})}(s _{1})\). In Appendix D.2 we formalize this intuition, and show that with probability at least \(1-\delta\),

\[\text{(II)}=v^{\pi^{\star}_{\bar{G}}}(s_{1})-\bar{v}_{\theta^{\prime}_{1}}(s_ {1})\leq\eta\,.\] (17)

**Bounding \(\text{(III)}=\bar{v}_{\theta^{\prime}_{1}}(s_{1})-v^{\pi^{\prime}}(s_{1})\)**: To bound term \(\text{(III)}\) we will first show in Lemma 5.1 that value estimates in terms of \(\theta^{\prime}_{h}\) and \(\psi_{h}(\pi^{\star}_{G^{\prime}})\) are close with high probability for all \(h\in[H+1]\). This lemma allows us to relate \(\bar{v}_{\theta^{\prime}_{1}}\) to \(\bar{v}_{\psi_{1}(\pi^{\star}_{G^{\prime}})}\) and then Assumption 1 relates \(\bar{v}_{\psi_{1}(\pi^{\star}_{G^{\prime}})}\) to \(v^{\pi^{\star}_{\bar{G}^{\prime}}}\). We are then left with relating \(v^{\pi^{\star}_{G^{\prime}}}\) to \(v^{\pi^{\prime}}\). To do this we claim that \(\pi^{\prime}\) is an approximate policy improvement step w.r.t. \(v^{\pi^{\star}_{G^{\prime}}}\), which can be seen by recalling that \(\pi^{\prime}\) is greedy w.r.t. \(\bar{q}_{\theta^{\prime}_{1:H+1}}\), and as we mentioned a couple sentences ago, \(\bar{v}_{\theta^{\prime}_{h}}\) and \(\bar{v}_{\psi_{h}(\pi^{\star}_{G^{\prime}})}\) are close for all \(h\in[H+1]\).

To formalize this intuition we begin by decomposing \(\bar{v}_{\theta^{\prime}_{1}}(s_{1})-v^{\pi^{\prime}}(s_{1})\) into the following error terms

\[\bar{v}_{\theta^{\prime}_{1}}(s_{1})-v^{\pi^{\prime}}(s_{1})=\bar{v}_{\theta^{ \prime}_{1}}(s_{1})-\bar{q}_{\psi_{1}(\pi^{\star}_{G^{\prime}})}(s_{1},\pi^{ \prime}(s_{1}))+\bar{q}_{\psi_{1}(\pi^{\star}_{G^{\prime}})}(s_{1},\pi^{ \prime}(s_{1}))-v^{\pi^{\prime}}(s_{1})\,.\] (18)

To bound \(\bar{v}_{\theta^{\prime}_{1}}(s_{1})-\bar{q}_{\psi_{1}(\pi^{\star}_{G^{\prime}} )}(s_{1},\pi^{\prime}(s_{1}))\) we introduce a useful lemma (proof in Appendix F).

**Lemma 5.1**.: _There is an event \(\mathcal{E}_{2}\), that occurs with probability at least \(1-\delta/3\), such that under event \(\mathcal{E}_{2}\), for all \(G\in\mathbf{G}\) that are feasible solutions to Optimization Problem 1, for all \(h\in[H]\), for all \((\theta_{s,a})_{(s,a)\in\mathcal{S}_{h}\times\mathcal{A}}\) and \((\bar{\theta}_{s,a})_{(s,a)\in\mathcal{S}_{h}\times\mathcal{A}}\in\Theta_{G,h}^ {\mathcal{S}_{h}\times\mathcal{A}}\), and for all admissible distributions \(\nu=(\nu_{t})_{t\in[H]}\), it holds that_

\[\operatorname*{\mathbb{E}}_{(S,A)\sim\nu_{h}}\Bigl{[}\bar{q}_{\theta_{S,A}}(S,A )-\bar{q}_{\theta_{S,A}}(S,A)\Bigr{]}\leq\tilde{\epsilon}\qquad\qquad\qquad \tilde{\epsilon}\text{ defn Eq. \eqref{It is left to bound \(\bar{q}_{\psi_{1}(\pi^{*}_{G^{\prime}})}(s_{1},\pi^{\prime}(s_{1}))-v^{\pi^{\prime }}(s_{1})\) in Eq. (18). To do this, first note that

\[\bar{q}_{\psi_{1}(\pi^{*}_{G^{\prime}})}(s_{1},\pi^{\prime}(s_{1}))-v^{\pi^{ \prime}}(s_{1})\leq q^{\pi^{*}_{G^{\prime}}}(s_{1},\pi^{\prime}(s_{1}))-v^{\pi ^{\prime}}(s_{1})+\eta\,.\]

where the inequality holds since we have approximate linear \(q^{\pi}\)-realizability (Assumption 1). To bound \(q^{\pi^{*}_{G^{\prime}}}(s_{1},\pi^{\prime}(s_{1}))-v^{\pi^{\prime}}(s_{1})\) notice that \(v^{\pi^{\prime}}(s_{1})=q^{\pi^{\prime}}(s_{1},\pi^{\prime}(s_{1}))\), which implies that

\[q^{\pi^{*}_{G^{\prime}}}(s_{1},\pi^{\prime}(s_{1}))-v^{\pi^{\prime}}(s_{1})=q^ {\pi^{*}_{G^{\prime}}}(s_{1},\pi^{\prime}(s_{1}))-q^{\pi^{\prime}}(s_{1},\pi^ {\prime}(s_{1}))=\underset{\text{Tari}\sim\mathbb{P}_{\pi^{\prime},s_{1}}}{ \mathbb{E}}\Big{[}v^{\pi^{*}_{G^{\prime}}}(S_{2})-v^{\pi^{\prime}}(S_{2})\Big{]}\,.\]

Next, we give a bound on \(\mathbb{E}_{\text{Tari}\sim\mathbb{P}_{\pi^{\prime},s_{1}}}\Big{[}v^{\pi^{*}_{G ^{\prime}}}(S_{2})-v^{\pi^{\prime}}(S_{2})\Big{]}\) (proof in Appendix D.3):

**Lemma 5.2**.: _Under event \(\mathcal{E}_{1}\cap\mathcal{E}_{2}\), for any \(h\in[2:H+1]\), it holds that_

\[\underset{\text{Tari}\sim\mathbb{P}_{\pi^{\prime},s_{1}}}{\mathbb{E}}\Big{[}v ^{\pi^{*}_{G^{\prime}}}(S_{h})-v^{\pi^{\prime}}(S_{h})\Big{]}\leq 2(H-h+2)( \eta+\tilde{\epsilon})\,.\]

Intuitively, the above lemma holds since \(\pi^{\prime}\) can be thought of as an approximate policy improvement step w.r.t. \(v^{\pi^{*}_{G^{\prime}}}\). To see this, recall that \(\pi^{\prime}\) is greedy w.r.t. \(\bar{q}_{\theta^{*}_{1:H+1}}\) (line 3). Then, with Lemma 5.1, we can show \(\bar{v}_{\theta^{*}_{h}}\) and \(\bar{v}_{\psi_{h}(\pi^{*}_{G^{\prime}})}\)(which is close to \(v^{\pi^{*}_{G^{\prime}}}\) (Assumption 1)) are close for all \(h\in[H+1]\). The above bounds imply that under event \(\mathcal{E}_{1}\cap\mathcal{E}_{2}\), which occurs with probability at least \(1-2\delta/3\),

\[(\text{III})=\bar{v}_{\theta^{*}_{1}}(s_{1})-v^{\pi^{\prime}}(s_{1})\leq 2H( \eta+\tilde{\epsilon})+\tilde{\epsilon}+\eta\,.\] (19)

**Combining the Bounds:** To finish the proof we combine the bounds on all three terms (Eqs. (16), (17) and (19)), to get that under event \(\mathcal{E}_{1}\cap\mathcal{E}_{2}\cap\mathcal{E}_{3}\), which occurs with probability at least \(1-\delta\),

\[v^{*}(s_{1})-v^{\pi^{\prime}}(s_{1})\leq H(2\alpha+2\eta)+\eta+2H(\eta+\tilde{ \epsilon})+\tilde{\epsilon}+\eta\leq 4(H+1)(\alpha+\eta+\tilde{\epsilon})\,.\]

To bound the above display by \(\epsilon\) we set \(\alpha=\epsilon/(12(H+1))<1\). If \(n=\tilde{\Theta}\big{(}C^{4}_{\text{conc}}H^{7}d^{4}/\epsilon^{2}\big{)}\) and \(\eta=\tilde{\mathcal{O}}\Big{(}\alpha/\sqrt{nH}\Big{)}\) (Eq. (26)), we show that \(\tilde{\epsilon}=\tilde{\mathcal{O}}\big{(}C^{2}_{\text{conc}}H^{5/2}d^{2}/ \sqrt{n}\big{)}\) (Eq. (44)). This implies that

\[4(H+1)(\alpha+\eta+\tilde{\epsilon})\leq\epsilon\,.\qed\]

## 6 Limitations and Conclusions

In this work we resolved an open problem in the positive, by presenting the first statistically efficient learner (Section 4.4) that outputs a near optimal policy in the offline RL setting with approximate linear \(q^{\pi}\)-realizability (Assumption 1), trajectory data (Assumption 2), and concentrability (Assumption 3). One limitation of this work is that we are not aware of any computationally efficient implementation of Optimization Problem 1, which is at the heart of our learner. As such, it is left as an open problem whether computationally efficient learning is possible in the setting we considered. Another limitation is that we are not sure if our statistical rate in Theorem 1 is optimal. Showing a matching lower bound or improving the rate is left for future work.

Another limitation of our work originates from our setting underpinning our result (Section 4), namely the three assumptions: approximate linear \(q^{\pi}\)-realizability, trajectory data, and concentrability. Approximate linear \(q^{\pi}\)-realizability requires the value function of all memoryless policies to be linear in a fixed and known \(d\)-dimensional feature map. While strictly weaker than the linear MDP assumption (Zanette et al., 2020), this assumption is still strong. Trajectory data requires full sequences of interactions with an environment to be collected by a single policy. For long horizon problems this can be practically challenging. Concentrability requires the state and action spaces to be well-covered. This can be challenging to guarantee since often the state and action spaces are unknown at the time of data collection. Further, since we require the trajectory data to be collected by a single policy, it may be the case that no single policy exists that covers the state and action spaces well, and a mixture of policies must be considered, which our current result does not immediately hold for. Although the assumptions appear strong, a justification for them is that under many variations of weaker assumptions (for instance: general data, or linear \(q^{\pi}\)-realizability of only one policy, or only coverage of the feature space), polynomial statistical rates have been shown to be impossible to achieve by any learner (Table 1).

Since this work is focused on foundational theoretical research it is unlikely to have any direct and immediate societal impacts.

## References

* Abbasi-Yadkori et al. (2011) Y. Abbasi-Yadkori, D. Pal, and C. Szepesvari. Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_, 24, 2011.
* Amortila et al. (2020) P. Amortila, N. Jiang, and T. Xie. A variant of the wang-foster-kakade lower bound for the discounted setting. _arXiv preprint arXiv:2011.01075_, 2020.
* Cai et al. (2020) Q. Cai, Z. Yang, C. Jin, and Z. Wang. Provably efficient exploration in policy optimization. In _International Conference on Machine Learning_, pages 1283-1294. PMLR, 2020.
* Chen and Jiang (2019) J. Chen and N. Jiang. Information-theoretic considerations in batch reinforcement learning. In _International Conference on Machine Learning_, pages 1042-1051. PMLR, 2019.
* Duan et al. (2020) Y. Duan, Z. Jia, and M. Wang. Minimax-optimal off-policy evaluation with linear function approximation. In _International Conference on Machine Learning_, pages 2701-2709. PMLR, 2020.
* Foster et al. (2021) D. J. Foster, A. Krishnamurthy, D. Simchi-Levi, and Y. Xu. Offline reinforcement learning: Fundamental barriers for value function approximation. _arXiv preprint arXiv:2111.10919_, 2021.
* Hoeffding (1994) W. Hoeffding. Probability inequalities for sums of bounded random variables. _The collected works of Wassily Hoeffding_, pages 409-426, 1994.
* Jia et al. (2024) Z. Jia, A. Rakhlin, A. Sekhari, and C.-Y. Wei. Offline reinforcement learning: Role of state aggregation and trajectory data. _arXiv preprint arXiv:2403.17091_, 2024.
* Jin et al. (2021) Y. Jin, Z. Yang, and Z. Wang. Is pessimism provably efficient for offline rl? In _International Conference on Machine Learning_, pages 5084-5096. PMLR, 2021.
* Lattimore and Szepesvari (2020) T. Lattimore and C. Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* Munos and Szepesvari (2008) R. Munos and C. Szepesvari. Finite-time bounds for fitted value iteration. _Journal of Machine Learning Research_, 9(5), 2008.
* Todd (2016) M. J. Todd. _Minimum-volume ellipsoids: Theory and algorithms_. SIAM, 2016.
* Vershynin (2018) R. Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* Wang et al. (2020) R. Wang, D. P. Foster, and S. M. Kakade. What are the statistical limits of offline rl with linear function approximation? _arXiv preprint arXiv:2010.11895_, 2020.
* Weisz et al. (2023) G. Weisz, A. Gyorgy, and C. Szepesvari. Online rl in linearly qpi-realizable mdps is as easy as in linear mdps if you learn what to ignore. _arXiv preprint arXiv:2310.07811_, 2023.
* Xie and Jiang (2021) T. Xie and N. Jiang. Batch value-function approximation with only realizability. In _International Conference on Machine Learning_, pages 11404-11413. PMLR, 2021.
* Xie et al. (2021) T. Xie, C.-A. Cheng, N. Jiang, P. Mineiro, and A. Agarwal. Bellman-consistent pessimism for offline reinforcement learning. _Advances in neural information processing systems_, 34:6683-6694, 2021.
* Xie et al. (2022) T. Xie, M. Bhardwaj, N. Jiang, and C.-A. Cheng. Armor: A model-based framework for improving arbitrary baseline policies with offline data. _arXiv preprint arXiv:2211.04538_, 2022.
* Xiong et al. (2022) W. Xiong, H. Zhong, C. Shi, C. Shen, L. Wang, and T. Zhang. Nearly minimax optimal offline reinforcement learning with linear function approximation: Single-agent mdp and markov game. _arXiv preprint arXiv:2205.15512_, 2022.
* Zanette (2021) A. Zanette. Exponential lower bounds for batch reinforcement learning: Batch rl can be exponentially harder than online rl. In _International Conference on Machine Learning_, pages 12287-12297. PMLR, 2021.
* Zanette et al. (2020) A. Zanette, A. Lazaric, M. Kochenderfer, and E. Brunskill. Learning near optimal policies with low inherent bellman error. In _International Conference on Machine Learning_, pages 10978-10989. PMLR, 2020.

## Appendix A Parameter Settings and Notation

\[n =\tilde{\Theta}\bigg{(}\frac{C_{\text{conc}}^{4}H^{7}d^{4}}{ \epsilon^{2}}\bigg{)}\] Set at the end of Section 5 \[d_{0} =\lceil 4d\log\log(d)+16\rceil\] Defined above Eq. (4) (21) \[L_{1} =\text{Upper bound on $2$-norm of features $\phi$}\] Defined above Assumption 1 \[L_{2} =\text{Upper bound on $2$-norm of true parameters $\psi_{h},h\in[H]$}\] Assumption 1 \[\tilde{L_{2}} =L_{2}(8H^{2}d_{0}/\alpha+1)\] Defined in Lemma 4.2 (24) \[\sqrt{\lambda} =H^{3/2}d/\tilde{L_{2}}\] Defined in Eq. (57) (25) \[\eta \leq\frac{H^{3/2}d}{\sqrt{n}(10H^{2}d_{0}/\alpha+1)}=\tilde{ \Theta}\bigg{(}\frac{\alpha}{\sqrt{nH}}\bigg{)}=\tilde{\Theta}\bigg{(}\frac{ \epsilon^{2}}{C_{\text{conc}}^{2}H^{5}d^{2}}\bigg{)}\] Defined in Eq. (59) (26) \[\tilde{\eta} =\eta(10H^{2}d_{0}/\alpha+1)=\tilde{\mathcal{O}}\bigg{(}\frac{H ^{3/2}d}{\sqrt{n}}\bigg{)}\] Defined in Lemma 4.2 (27) \[\tilde{\epsilon} =\tilde{\mathcal{O}}\big{(}d/\sqrt{n}\big{)}\] Defined in Eq. (72) (28) \[\tilde{\epsilon} =\tilde{\mathcal{O}}\bigg{(}\frac{C_{\text{conc}}H^{5/2}d^{2}}{ \sqrt{n}}\bigg{)}\] Defined in Eq. (51) (29) \[\tilde{\epsilon} =\tilde{\mathcal{O}}\bigg{(}\frac{C_{\text{conc}}^{2}H^{5/2}d^{ 2}}{\sqrt{n}}\bigg{)}\] Defined in Eq. (44) (30) \[\bar{\beta} =\tilde{\mathcal{O}}\Big{(}H^{3/2}d\Big{)}\] Defined in Eq. (77) (31) \[\beta =\tilde{\mathcal{O}}\Big{(}H^{3/2}d\Big{)}\] Defined in Eq. (58) (32) \[|C_{\xi}^{\mathbf{G}}| \leq(1+2L_{2}/\xi))^{dHQ_{0}},\,\xi>0\] Defined in Lemma I.4 (33) \[L_{\xi} =12\sqrt{2d}H^{2}L_{1}\xi\alpha^{-1}\Big{(}2\sqrt{n}L_{1}\tilde{ L_{2}}/(H^{3/2}d)\Big{)}^{H},\,\xi>0\] Defined in Eq. (88) (34) \[\alpha =\frac{\epsilon}{12(H+1)}<1\] Defined at the end of Section 5 (35)Trajectory Data vs. Non-Trajectory Data

To compare the two types of data, we first define "non-trajectory" data, which consist of individual transitions without any guarantees of them coming from complete MDP trajectories.

**Assumption 4** (Non-Trajectory Data).: _Assume that for each \(h\in[H]\) the learner is given a dataset of transition tuples and corresponding features of size \(n\geq 1\):_

\[(s^{j}_{h},a^{j}_{h},r^{j}_{h},\bar{s}^{j}_{h+1})_{j\in[n]}\quad\text{and}\quad ((\phi(s^{j}_{h},a))_{a\in\mathcal{A}},(\phi(\bar{s}^{j}_{h+1},a))_{a\in \mathcal{A}})_{j\in[n]}\,,\]

_where \((s^{j}_{h},a^{j}_{h})\sim\mu^{\prime}_{h},r^{j}_{h}\sim\mathcal{R}(s^{j}_{h},a ^{j}_{h}),\bar{s}^{j}_{h+1}\sim P(s^{j}_{h},a^{j}_{h})\), for all \(j\in[n]\), and \(\mu^{\prime}_{h}\in\mathcal{M}_{1}(\mathcal{S}_{h}\times\mathcal{A})\) is a "data collection distribution" unknown to the learner._

There are two differences between the above non-trajectory data and trajectory data as defined in Assumption 2. In particular, if \(\mu^{\prime}_{h}=\mathbb{P}^{h}_{\pi^{0},s_{1}}\) and \(\bar{s}^{j}_{h+1}=s^{j}_{h+1}\), then we get back the trajectory data assumption.

Foster et al. (2021) showed a negative result under Assumption 4. Our method addresses the hard instance from Foster et al. (2021) if the data is given as complete trajectories. Below we explain why the lower bound constructions from Foster et al. (2021) break down if they need to use trajectory data, and why our algorithm breaks down if it doesn have trajectory data.

The lower bound constructions in Theorems 1.1 and 1.2 of Foster et al. (2021) were both made hard because the data collection distributions of individual transition tuples \((s,a,r,\bar{s})\) were selected such that they reveal no (or almost no) information about the MDP instance. In both cases, receiving samples from the joint distribution of the entire trajectory \(\mathbb{P}_{\pi^{0},s_{1}}\) makes the problem easy. In the case of Theorem 1.1, one would simply observe which states are reachable from the start state (the planted states). For Theorem 1.2, some information on whether any next-state \(\bar{s}\) is planted or not would be leaking in each trajectory, in the form of being able to observe the next-state transition from exactly \(\bar{s}\).

A simpler example showing the root of the problem with non-trajectory data is as follows. Consider the toy problem of learning the value of some policy \(\pi\) after taking action \(a\) in state \(s_{1}\) in a 2-stage MDP. The data is given as tuples of the form \((s_{1},a,r^{1}_{1},\bar{s}^{1}_{2},\ldots,s_{1},a,r^{n}_{1},\bar{s}^{2}_{2})\) for the first stage and \((s^{1}_{2},a^{1}_{1},r^{1}_{2},\bar{s}^{1}_{3},\ldots,\bar{s}^{n}_{2},a^{n}_{ 2},r^{n}_{2},\bar{s}^{n}_{3})\) for the second stage. Notice there is no guarantee that \(s^{j}_{2}\sim P(s_{1},a)\) with \(j\in[n]\). We cannot infer what the rewards from the second-stage states distributed as \(P(s_{1},a)\) might look like from the data, making this problem hopelessly hard. In the extreme, the MDP might have infinitely many second-stage states, with the probability of any \(\bar{s}^{j}_{2}=s^{k}_{2}\) (for any \(j\) and \(k\)) being 0, highlighting that one cannot just connect and importance weight matching next-states \(\bar{s}^{j}_{2}\) of the first-stage transitions with matching start-states \(s^{k}_{2}\) of the second-stage transitions. In contrast, if we assume the data is such that \(\bar{s}^{j}_{2}=s^{j}_{2}\), this problem is immediately avoided as samples from \(P(s_{1},a)\), along with rewards from those states are directly handed to the learner. The learner can then simply use all of the rewards \(r^{j}_{2}\) from tuples that contain the action \(\pi(s^{j}_{2})\) (which we have on average at least \(\,1/C_{\text{conc}}\) of, due to concentrability) to estimate the value of policy \(\pi\) after taking \(s_{1},a\) (solving the toy problem).

Now consider our algorithm if we do not have trajectory data. In this case we are no longer able to construct least squares targets of the form needed to make use of Lemma 4.2 (discussed in Section 4.2). This means that we would not be able to guarantee that our targets are linear, even under the true skipping mechanism \(\bar{G}\), implying that \(\bar{G}\) might not be a feasible solution to our optimization problem. Then our optimism argument that the output of the optimization problem has a value estimate at least as large as the value estimate based on \(\bar{G}\) would no longer hold, causing our whole proof strategy to break down.

Proof of Lemma 4.2

Proof.: We follow a proof technique introduced in [Weisz et al., 2023]. We start by quoting their definition of admissible functions and their admissible realizability lemma.

**Definition 2** (Definition 4.6 in [Weisz et al., 2023]).: _For any \(h\in[H]\), \(f:\mathcal{S}_{h}\to\mathbb{R}\) is \(\alpha^{\prime}\)-admissible for some \(\alpha^{\prime}>0\) if for all \(s\in\mathcal{S}_{h}\), \(|f(s)|\leq\text{range}(s)/\alpha^{\prime}\)._

**Lemma C.1** (Admissible-realizability (Lemma 4.7 in [Weisz et al., 2023])).: _If \(f:\mathcal{S}_{h}\to\mathbb{R}\) is \(\alpha^{\prime}\)-admissible then it is realizable, that is, for all \(t\in[h-1]\) and \(\pi\in\Pi\), there exists some \(\tilde{\theta}\in\mathbb{R}^{d}\) with \(\left\|\tilde{\theta}\right\|_{2}\leq 4d_{0}L_{2}/\alpha^{\prime}\) such that for all \((s,a)\in\mathcal{S}_{t}\times\mathcal{A}\),_

\[\left|\operatorname*{\mathbb{E}}_{\text{Taj}\sim\mathbb{P}_{\pi,s,a}}f(S_{h}) -\left\langle\phi(s,a),\tilde{\theta}\right\rangle\right|\leq\eta_{0} \text{where }\eta_{0}=5d_{0}\eta/\alpha^{\prime}.\]

Next, we fix some \(f:\mathcal{S}\to[0,H]\) with \(f(s_{\top})=0\), and policy \(\pi\in\Pi\). For \(2\leq h\leq H+1\), define \(\tilde{g}_{h}:\mathcal{S}_{h}\to[-H,H]\) as \(\tilde{g}_{H+1}(\cdot)=0\), and

\[\tilde{g}_{h}(s) =\operatorname*{\mathbb{E}}_{\text{Taj}\sim\mathbb{P}_{\pi,s,a}} \operatorname*{\mathbb{E}}_{\tau\sim F_{G,\text{Taj},h}}[-R_{\tau:H}+f(S_{ \tau})]\] \[=\operatorname*{\mathbb{E}}_{\text{Taj}\sim\mathbb{P}_{\pi,s}} \sum_{t=h}^{H}[-R_{t:H}+f(S_{t})](1-\omega_{\tilde{G}}(S_{t}))\prod_{u=h}^{t- 1}\omega_{\tilde{G}}(S_{u})\,.\]

Notice that for any \(h\in[H]\), \((s,a)\in\mathcal{S}_{h}\times\mathcal{A}\), the function of \((s,a)\) we aim to linearly realize can be written as

\[\operatorname*{\mathbb{E}}_{\text{Taj}\sim\mathbb{P}_{\pi,s,a}} \operatorname*{\mathbb{E}}_{\tau\sim F_{G,\text{Taj},h}}[R_{h:\tau-1}+f(S_{ \tau})] =\operatorname*{\mathbb{E}}_{\text{Taj}\sim\mathbb{P}_{\pi,s,a}} \tilde{g}_{h+1}(S_{h+1})+R_{h:H}\] (36) \[=\operatorname*{\mathbb{E}}_{S_{h+1}\sim P(s,a)}\tilde{g}_{h+1}(S _{h+1})+q^{\pi}(s,a)\,.\]

The second term of the sum, \(q^{\pi}(s,a)\) is linearly realizable by Assumption 1 with parameter \(\psi_{h}(\pi)\). The first term needs more work before Lemma C.1 can be applied. To this end, for \(h\in[2:H]\) we define \(g_{h}:\mathcal{S}_{h}\to\mathbb{R}\) as

\[g_{h}(s)=(1-\omega_{\tilde{G}}(s))\operatorname*{\mathbb{E}}_{\text{Taj}\sim \mathbb{P}_{\pi,s}}[-R_{h:H}+f(s)-\tilde{g}_{h+1}(S_{h+1})]\,.\]

Notice that \(\tilde{g}_{h}\) can be decomposed into a sum of \(g_{t}\) functions as for all \(h\in[2:H]\), \(s\in\mathcal{S}_{h}\),

\[\tilde{g}_{h}(s)=\operatorname*{\mathbb{E}}_{\text{Taj}\sim\mathbb{P}_{\pi,s} }\sum_{t=h}^{H}g_{t}(S_{t})\,.\] (37)

The benefit of decomposing \(\tilde{g}_{h}\) into \(g_{t}\) functions is that \(g_{t}\) are \(\alpha^{\prime}\)-admissible under Definition 2 for \(\alpha^{\prime}=\alpha/(2H)\). To see this, note that for any trajectory and \(s\), \(-R_{h:H}+f(s)-\tilde{g}_{h+1}(S_{h+1})\in[-2H,2H]\). \(g_{t}(s)\) multiplies this by \(1-\omega_{\tilde{G}}(s)\) which by Eq. (6) is between \(0\) and \(1\), and satisfies \(1-\omega_{\tilde{G}}(s)=0\) if \(\text{range}^{\tilde{G}}(s)\leq\alpha/\sqrt{2d}\). By Lemma 4.1, \(\text{range}(s)\leq\sqrt{2d}\cdot\text{range}^{\tilde{G}}(s)\), so \(g_{t}(s)=0\) for any \(s\) with \(\text{range}(s)\leq\alpha\). On any other \(s\), \(|g_{t}(s)|\leq 2H\). Therefore \(g_{t}\) is \(\alpha^{\prime}\)-admissible. This allows us to use Lemma C.1 to get that for any \(h\in[H-1]\), there exist \(\tilde{\theta}_{h+1:H}\in\mathcal{B}(8Hd_{0}L_{2}/\alpha)^{H-h}\), such that for any stage \(t\in[h+1:H]\), for all \((s,a)\in\mathcal{S}_{h}\times\mathcal{A}\),

\[\left|\operatorname*{\mathbb{E}}_{\text{Taj}\sim\mathbb{P}_{\pi,s,a}}g_{t}(S_{t })-\left\langle\phi(s,a),\tilde{\theta}_{t}\right\rangle\right|\leq 10Hd_{0}\eta/ \alpha\,.\]

By combining this with Eq. (37), for all \(h\in[H]\) there exists \(\tilde{\theta}=\sum_{t=h+1}^{H}\tilde{\theta}_{t}\) with \(\tilde{\theta}\in\mathcal{B}(8H^{2}d_{0}L_{2}/\alpha)\) such that for all \((s,a)\in\mathcal{S}_{h}\times\mathcal{A}\),

\[\left|\operatorname*{\mathbb{E}}_{S_{h+1}\sim P(s,a)}\tilde{g}_{h+1}(S_{h+1})- \left\langle\phi(s,a),\tilde{\theta}\right\rangle\right|\leq 10H^{2}d_{0}\eta/ \alpha\,.\]

Combined with Eq. (36) and the parameter \(\psi_{h}(\pi)\) from Assumption 1, there exists \(\theta=\tilde{\theta}+\psi_{h}(\pi)\) with \(\theta\in\mathcal{B}(L_{2}(8H^{2}d_{0}/\alpha+1))\) such that for all \((s,a)\in\mathcal{S}_{h}\times\mathcal{A}\),

\[\left|\operatorname*{\mathbb{E}}_{\text{Taj}\sim\mathbb{P}_{\pi,s,a}} \operatorname*{\mathbb{E}}_{\tau\sim F_{G,\text{Taj},h}}[R_{h:\tau-1}+f(S_{\tau} )]-\left\langle\phi(s,a),\theta\right\rangle\right|\leq\eta(10H^{2}d_{0}/ \alpha+1)=\tilde{\eta}\,.\]

To finish the proof, we define \(\rho_{h}^{\pi}(f)=\theta\) for the arbitrary \(h\in[H]\), \(\pi\), and \(f\) picked above.

Results used in Section 5

### Bounding term \((\mathbf{I})\)

We begin by defining an alternative policy \(\pi_{G}^{\dagger}\) as

\[\pi_{G}^{\dagger}(a|s)=\pi^{0}(a|s)\omega_{\bar{G}}(s)+\pi^{\star}(a|s)(1-\omega_ {\bar{G}}(s))\,.\]

This policy can only be worse in value than \(\pi_{G}^{\star}\):

**Lemma D.1**.: _For all \(s\in\mathcal{S}\),_

\[v^{\pi_{G}^{\star}}(s)\geq v^{\pi_{G}^{\dagger}}(s)\,.\]

Proof.: We prove by induction for \(h=H+1,H,\ldots,1\) that for all \(s\in\mathcal{S}_{h}\), \(v^{\pi_{G}^{\star}}(s)\geq v^{\pi_{G}^{\dagger}}(s)\). The base case of \(h=H+1\) is immediately true by definition as \(v\)-values are \(0\) on \(s_{\top}\), regardless of the policy. Assuming the inductive hypothesis holds for \(h+1\), we continue by proving it for \(h\). Let \((s,a)\in\mathcal{S}_{h}\times\mathcal{A}\) be arbitrary. Notice that

\[q^{\pi_{G}^{\star}}(s,a)-q^{\pi_{G}^{\dagger}}(s,a)=\mathop{\mathbb{E}}_{S^{ \prime}\sim P(s,a)}v^{\pi_{G}^{\star}}(S^{\prime})-v^{\pi_{G}^{\dagger}}(S^{ \prime})\geq 0\,,\]

where the inequality is due to the inductive hypothesis. Next, for any \(s\in\mathcal{S}_{h}\), by the above and the definition of the policies,

\[v^{\pi_{G}^{\star}}(s) =\omega_{\bar{G}}(s)\mathop{\mathbb{E}}_{A\sim\pi^{0}(s)}q^{\pi_ {G}^{\star}}(s,A)+(1-\omega_{\bar{G}}(s))\max_{a\in\mathcal{A}}q^{\pi_{G}^{ \star}}(s,a)\] \[\geq\omega_{\bar{G}}(s)\mathop{\mathbb{E}}_{A\sim\pi^{0}(s)}q^{ \pi_{G}^{\dagger}}(s,A)+(1-\omega_{\bar{G}}(s))\max_{a\in\mathcal{A}}q^{\pi_{G }^{\dagger}}(s,a)\] \[\geq\omega_{\bar{G}}(s)\mathop{\mathbb{E}}_{A\sim\pi^{0}(s)}q^{ \pi_{G}^{\dagger}}(s,A)+(1-\omega_{\bar{G}}(s))\mathop{\mathbb{E}}_{A\sim\pi_{ G}^{\star}}q^{\pi_{G}^{\dagger}}(s,A)=v^{\pi_{G}^{\dagger}}(s)\,,\]

finishing the induction. 

Due to Lemma D.1, \(v^{\star}(s_{1})-v^{\pi_{G}^{\star}}(s_{1})\leq v^{\star}(s_{1})-v^{\pi_{G}^ {\dagger}}(s_{1})\). We continue by bounding \(v^{\star}(s_{1})-v^{\pi_{G}^{\dagger}}(s_{1})\). We first decompose it using the performance difference lemma (Lemma J.3) to get that

\[v^{\star}(s_{1})-v^{\pi_{G}^{\dagger}}(s_{1})=\sum_{h=2}^{H}\mathop{\mathbb{E }}_{(S_{h},A_{h})\sim\mathbb{P}^{h}_{\pi^{\star},s_{1}}}\Big{(}q^{\pi_{G}^{ \dagger}}(S_{h},A_{h})-v^{\pi_{G}^{\dagger}}(S)\Big{)}\,,\] (38)

where we only have the sum from \(h=2\) to \(H\), since for \(h=1\) and \(h=H+1\), for any \(s\in\mathcal{S}_{h}\), \(\omega_{\bar{G}}(s)=0\) and therefore \(\pi_{G}^{\dagger}(s)=\pi^{\star}(s)\). By the definition of \(\omega_{\bar{G}}\) (Eq. (6)), we can see that for \(s\in\mathcal{S},\omega_{\bar{G}}(s)\neq 0\) only when \(\text{range}^{\bar{G}}(s)\leq 2\alpha/\sqrt{2d}\) Thus, the policies \(\pi^{\star}(s)\) and \(\pi_{G}^{\dagger}(s)\) are equal for all \(s\in\mathcal{S}\) that satisfy \(\text{range}^{\bar{G}}(s)\geq 2\alpha/\sqrt{2d}\). Making use of this result in Eq. (38), we get that

\[\sum_{h=2}^{H}\mathop{\mathbb{E}}_{(S_{h},A_{h})\sim\mathbb{P}^{ h}_{\pi^{\star},s_{1}}}\Big{(}q^{\pi_{G}^{\dagger}}(S_{h},A_{h})-v^{\pi_{G}^{ \dagger}}(S)\Big{)}\] \[=\sum_{h=2}^{H}\mathop{\mathbb{E}}_{(S_{h},A_{h})\sim\mathbb{P}^{ h}_{\pi^{\star},s_{1}}}\Big{[}\mathbbm{1}\Big{\{}\text{range}^{\bar{G}}(S_{h})\leq 2 \alpha/\sqrt{2d}\Big{\}}\Big{(}q^{\pi_{G}^{\dagger}}(S_{h},A_{h})-v^{\pi_{G}^{ \dagger}}(S_{h})\Big{)}\Big{]}\,.\]

By Eq. (3), we know that

\[q^{\pi_{G}^{\dagger}}(s,a)-v^{\pi_{G}^{\dagger}}(s)\leq\text{range}(s)+2\eta\,.\]

Then, we can use Lemma 4.1 to get that for all \(h\in[2:H]\) and \((s,a)\in\mathcal{S}_{h}\times\mathcal{A}\)

\[q^{\pi_{G}^{\dagger}}(s,a)-v^{\pi_{G}^{\dagger}}(s)\leq\sqrt{2d}\cdot\text{range }^{\bar{G}}(s)+2\eta\,.\]Putting things together we get the following bound.

\[(\mathrm{I}) \leq v^{\star}(s_{1})-v^{\pi^{*}_{O}}(s_{1})\] \[\leq v^{\star}(s_{1})-v^{\pi^{\dagger}_{O}}(s_{1})\] \[=\sum_{h=2}^{H}\operatorname*{\mathbb{E}}_{(S_{h},A_{h})\sim \mathbb{P}^{h^{\star}}_{\eta^{\star},s_{1}}}\Bigl{[}\mathbbm{1}\Bigl{\{}\text{ range}^{\bar{G}}(S)\leq 2\alpha/\sqrt{2d}\Bigr{\}}\Bigl{(}q^{\pi^{\dagger}_{O}}(S,A)-v^{ \pi^{\dagger}_{O}}(S)\Bigr{)}\Bigr{]}\] \[\leq H(2\alpha+2\eta)\,.\]

### Bounding term \((\mathbf{II})\)

To bound \(v^{\pi^{*}_{O}}(s_{1})-\bar{v}_{\theta^{\prime}_{1}}(s_{1})\) we decompose it into the following error terms

\[v^{\pi^{*}_{O}}(s_{1})-\bar{v}_{\theta^{\prime}_{1}}(s_{1}) =v^{\pi^{*}_{O}}(s_{1})-\bar{v}_{\psi_{1}(\pi^{*}_{O})}(s_{1})+ \bar{v}_{\psi_{1}(\pi^{*}_{O})}(s_{1})-\bar{v}_{\theta^{\prime}_{1}}(s_{1})\] \[\leq\bar{v}_{\psi_{1}(\pi^{*}_{O})}(s_{1})-\bar{v}_{\theta^{ \prime}_{1}}(s_{1})+\eta\,,\] (39)

where the inequality holds since we have approximate linear \(q^{\pi}\)-realizability (Assumption 1), which implies that

\[v^{\pi^{*}_{O}}(s_{1})-\bar{v}_{\psi_{1}(\pi^{*}_{O})}(s_{1})\leq\max_{a\in \mathcal{A}}\Bigl{(}q^{\pi^{*}_{O}}(s_{1},a)-\bar{q}_{\psi_{1}(\pi^{*}_{O})}( s_{1},a)\Bigr{)}\leq\eta\,.\]

To help us bound \(\bar{v}_{\psi_{1}(\pi^{*}_{O})}(s_{1})-\bar{v}_{\theta^{\prime}_{1}}(s_{1})\) in Eq. (39) we make use of two lemmas. The first is the following (proof in Appendix E).

**Lemma D.2**.: _There is an event \(\mathcal{E}_{1}\), which occurs with probability at least \(1-\delta/3\), such that under \(\mathcal{E}_{1}\), for all \(G\in\mathbf{G}\) and \(h\in[H+1]\), it holds that \(\psi_{h}(\pi^{*}_{G})\in\Theta_{G,h}\)._

Lemma D.2 tells us that under event \(\mathcal{E}_{1}\), \(\psi_{1}(\pi^{*}_{\bar{G}})\in\Theta_{\bar{G},1}\). The second lemma is the following (proof in Appendix G).

**Lemma D.3** (Feasibility).: _There is an event \(\mathcal{E}_{1}\cap\mathcal{E}_{2}\cap\mathcal{E}_{3}\), which occurs with probability at least \(1-\delta\), such that under event \(\mathcal{E}_{1}\cap\mathcal{E}_{2}\cap\mathcal{E}_{3}\), the true guess \(\bar{G}\) is a feasible solution to Optimization Problem 1._

Notice that since \((G^{\prime},\theta^{\prime}_{1:H+1})\) is the solution to Optimization Problem 1, it holds that \(\bar{v}_{\theta^{\prime}_{1}}(s_{1})\geq\bar{v}_{\theta}(s_{1})\) for any \(\theta\in\Theta_{G,1}\) where \(G\) is a feasible solution to Optimization Problem 1. Thus, we get that under event \(\mathcal{E}_{1}\cap\mathcal{E}_{2}\cap\mathcal{E}_{3}\), since \(\psi_{1}(\pi^{*}_{\bar{G}})\in\Theta_{\bar{G},1}\) (by Lemma D.3), and \(\bar{G}\) is a feasible solution to Optimization Problem 1 (by Lemma D.3), it holds that

\[\bar{v}_{\psi_{1}(\pi^{*}_{O})}(s_{1})-\bar{v}_{\theta^{\prime}_{1}}(s_{1})\leq \eta\,,\]

which together with Eq. (39), implies that

\[(\mathrm{II})=v^{\pi^{*}_{O}}(s_{1})-\bar{v}_{\theta^{\prime}_{1}}(s_{1})\leq \eta\,.\]

### Proof of Lemma 5.2

Proof.: Recall that \((G^{\prime},\theta^{\prime}_{1:H+1})\) is the solution to Optimization Problem 1. We aim to show that under event \(\mathcal{E}_{1}\cap\mathcal{E}_{2}\), for any \(h\in[2:H+1]\), it holds that

\[\operatorname*{\mathbb{E}}_{\text{\rm{Trail}}\sim\mathbb{P}_{\pi^{\prime},s_{1} }}\Bigl{[}v^{\pi^{*}_{G^{\prime}}}(S_{h})-v^{\pi^{\prime}}(S_{h})\Bigr{]}\leq 2 (H-h+2)(\eta+\tilde{\epsilon})\,.\] (40)

To prove Eq. (40) we will use induction. The base case is when \(h=H+1\), which trivially holds, since \(v^{\pi}(s_{\top})=r_{H+1}(s,a)=0\) for all \(\pi\in\Pi,s_{\top}\in\mathcal{S}_{H+1},a\in\mathcal{A}\).

Now, we show the inductive step. Let \(h\in[2:H]\) be arbitrary. Assume that Eq. (40) holds for any \(t\in[h+1:H+1]\). We prove that Eq. (40) also holds for \(h\). For any \((s,a)\in(\mathcal{S}\backslash\mathcal{S}_{1})\times\mathcal{A}\), let

\[\tilde{\pi}_{h}(a|s)=\begin{cases}\pi^{\star}_{G^{\prime}}(a|s)&\text{if }\operatorname {stage}(s)=h\\ \pi^{\prime}(a|s)&\text{if }\operatorname{stage}(s)\neq h\,.\end{cases}\]Then

\[\mathop{\mathbb{E}}_{\text{Traj}\sim\mathbb{P}_{x^{\prime},s_{1}}} \left[v^{\pi^{*}_{G^{\prime}}}(S_{h})-v^{\pi^{\prime}}(S_{h})\right]\] \[=\mathop{\mathbb{E}}_{\text{Traj}\sim\mathbb{P}_{\bar{u}_{h,h},1}} q^{\pi^{*}_{G^{\prime}}}(S_{h},A_{h})-\mathop{\mathbb{E}}_{\text{Traj}\sim \mathbb{P}_{x^{\prime},s_{1}}}q^{\pi^{\prime}}(S_{h},A_{h})\] \[=\mathop{\mathbb{E}}_{\text{Traj}\sim\mathbb{P}_{\bar{u}_{h,h},1} }\left[q^{\pi^{*}_{G^{\prime}}}(S_{h},A_{h})-\bar{q}_{\theta^{\prime}_{h}}(S_{ h},A_{h})\right]+\mathop{\mathbb{E}}_{\text{Traj}\sim\mathbb{P}_{\bar{u}_{h,h},1}}\bar{q}_{\theta^{\prime}_{h}}(S_{h},A_{h})-\mathop{\mathbb{E}}_{\text{Traj} \sim\mathbb{P}_{x^{\prime},s_{1}}}\bar{q}_{\theta^{\prime}_{h}}(S_{h},A_{h})\] \[\quad+\mathop{\mathbb{E}}_{\text{Traj}\sim\mathbb{P}_{x^{\prime},s_{1}}}\left[\bar{q}_{\theta^{\prime}_{h}}(S_{h},A_{h})-q^{\pi^{*}_{G^{\prime }}}(S_{h},A_{h})\right]+\mathop{\mathbb{E}}_{\text{Traj}\sim\mathbb{P}_{x^{ \prime},s_{1}}}\left[q^{\pi^{*}_{G^{\prime}}}(S_{h},A_{h})-q^{\pi^{\prime}}(S_{ h},A_{h})\right]\] \[\leq\mathop{\mathbb{E}}_{\text{Traj}\sim\mathbb{P}_{\bar{u}_{h,h},1}}\left[\bar{q}_{\psi_{h}(\pi^{*}_{G^{\prime}})}(S_{h},A_{h})-\bar{q}_{\theta ^{\prime}_{h}}(S_{h},A_{h})\right]+\mathop{\mathbb{E}}_{\text{Traj}\sim\mathbb{ P}_{\bar{u}_{h,h},1}}\bar{q}_{\theta^{\prime}_{h}}(S_{h},A_{h})-\mathop{ \mathbb{E}}_{\text{Traj}\sim\mathbb{P}_{x^{\prime},s_{1}}}\bar{q}_{\theta^{ \prime}_{h}}(S_{h},A_{h})\] \[\quad+\mathop{\mathbb{E}}_{\text{Traj}\sim\mathbb{P}_{x^{\prime },s_{1}}}\left[\bar{q}_{\theta^{\prime}_{h}}(S_{h},A_{h})-\bar{q}_{\psi_{h}(\pi^ {*}_{G^{\prime}})}(S_{h},A_{h})\right]+\mathop{\mathbb{E}}_{\text{Traj}\sim \mathbb{P}_{x^{\prime},s_{1}}}\left[q^{\pi^{*}_{G^{\prime}}}(S_{h},A_{h})-q^{ \pi^{\prime}}(S_{h},A_{h})\right]+2\eta\,,\] (41)

where the inequality holds since we have approximate linear \(q^{\pi}\)-realizability (Assumption 1). To bound the first and third error terms above notice that under event \(\mathcal{E}_{1}\), by Lemma D.2, we know that \(\psi_{h}(\pi^{*}_{G^{\prime}})\in\Theta_{G^{\prime},h}\). We also know that \(\theta^{\prime}_{h}\in\Theta_{G^{\prime},h}\), by definition. Let \(\tilde{\nu}_{u}(s_{u},a_{u})=\mathbb{P}^{u}_{\bar{u}_{h,h},s_{1}}(s_{u},a_{u})\) and \(\nu^{\prime}_{u}(s_{u},a_{u})=\mathbb{P}^{u}_{\pi^{\prime},s_{1}}(s_{u},a_{u})\) for all \(u\in[H],(s_{u},a_{u})\in\mathcal{S}_{u}\times\mathcal{A}\). Clearly, \(\tilde{\nu}=(\tilde{\nu}_{u})_{u\in[H]}\) and \(\nu^{\prime}=(\nu^{\prime}_{u})_{u\in[H]}\) are admissible distributions, by Definition 1. Notice that we have satisfied all the conditions to make use of Lemma 5.1. Thus, under event \(\mathcal{E}_{1}\cap\mathcal{E}_{2}\) it holds that

\[\mathop{\mathbb{E}}_{\text{Traj}\sim\mathbb{P}_{\bar{u}_{h,h},1}} \left[\bar{q}_{\psi_{h}(\pi^{*}_{G^{\prime}})}(S_{h},A_{h})-\bar{q}_{\theta^{ \prime}_{h}}(S_{h},A_{h})\right]=\mathop{\mathbb{E}}_{(S_{h},A_{h})\sim\tilde {\nu}_{h}}\left[\bar{q}_{\psi_{h}(\pi^{*}_{G^{\prime}})}(S_{h},A_{h})-\bar{q}_{ \theta^{\prime}_{h}}(S_{h},A_{h})\right]\leq\tilde{\epsilon}\,,\]

and

\[\mathop{\mathbb{E}}_{\text{Traj}\sim\mathbb{P}_{x^{\prime},s_{1}}} \left[\bar{q}_{\theta^{\prime}_{h}}(S_{h},A_{h})-\bar{q}_{\psi_{h}(\pi^{*}_{G^{ \prime}})}(S_{h},A_{h})\right]=\mathop{\mathbb{E}}_{(S_{h},A_{h})\sim\nu^{ \prime}_{h}}\left[\bar{q}_{\theta^{\prime}_{h}}(S_{h},A_{h})-\bar{q}_{\psi_{h} (\pi^{*}_{G^{\prime}})}(S_{h},A_{h})\right]\leq\tilde{\epsilon}\,.\]

The term \(\mathop{\mathbb{E}}_{\text{Traj}\sim\mathbb{P}_{\bar{u}_{h,h},1}}\bar{q}_{\theta^ {\prime}_{h}}(S_{h},A_{h})-\mathop{\mathbb{E}}_{\text{Traj}\sim\mathbb{P}_{x^{ \prime},s_{1}}}\bar{q}_{\theta^{\prime}_{h}}(S_{h},A_{h})\) in Eq. (41) can be bounded by recalling the definition of \(\pi^{\prime}(s)\) (line 3), to get that

\[\mathop{\mathbb{E}}_{\text{Traj}\sim\mathbb{P}_{\bar{u}_{h,h},1}} \bar{q}_{\theta^{\prime}_{h}}(S_{h},A_{h})\leq\max_{a\in\mathcal{A}}\bar{q}_{ \theta^{\prime}_{h}}(S_{h},a)=\mathop{\mathbb{E}}_{\text{Traj}\sim\mathbb{P}_{x^{ \prime},s_{1}}}\bar{q}_{\theta^{\prime}_{h}}(S_{h},A_{h})\,.\]

Under event \(\mathcal{E}_{1}\cap\mathcal{E}_{2}\), after plugging the above bounds into Eq. (41), we have that

\[\mathop{\mathbb{E}}_{\text{Traj}\sim\mathbb{P}_{x^{\prime},s_{1}}} \left[v^{\pi^{*}_{G^{\prime}}}(S_{h})-v^{\pi^{\prime}}(S_{h})\right] \leq\mathop{\mathbb{E}}_{\text{Traj}\sim\mathbb{P}_{x^{\prime},s_ {1}}}\left[q^{\pi^{*}_{G^{\prime}}}(S_{h},A_{h})-q^{\pi^{\prime}}(S_{h},A_{h}) \right]+2\eta+2\tilde{\epsilon}\] \[=\mathop{\mathbb{E}}_{\text{Traj}\sim\mathbb{P}_{x^{\prime},s_{1}}} \left[v^{\pi^{*}_{G^{\prime}}}(S_{h+1})-v^{\pi^{\prime}}(S_{h+1})\right]+2 \eta+2\tilde{\epsilon}\] \[\leq 2(H-h+2)(\eta+\tilde{\epsilon})\,,\]

where the last inequality holds by the inductive hypothesis for \(h+1\) (Eq. (40)), completing the proof of the claim.

Proof of Lemma d.2

Proof.: We will prove the claim using induction. The base case is when \(h=H+1\), for which \(\psi_{H+1}(\pi)=\vec{0}\) for all \(\pi\in\Pi\) by definition. Thus, for all \(G\in\mathbf{G}\), it holds that \(\psi_{H+1}(\pi_{G}^{\star})\in\Theta_{G,H+1}=\{\vec{0}\}\).

Now, we show the inductive step. Let \(h\in[H]\) be arbitrary. Assume Lemma D.2 holds for any \(t\in[h+1:H+1]\). We prove that it also holds for \(h\). Define

\[\hat{\psi}_{h}(\pi_{G}^{\star})=X_{h}^{-1}\sum_{j\in[n]}\phi_{h}^{j}\mathop{ \mathbb{E}}_{\tau\sim F_{G,h+1}^{\prime}}\Bigl{[}r_{h:\tau-1}^{j}+\bar{v}_{ \psi_{h+1:H+1}(\pi_{G}^{\star})}\bigl{(}s_{\tau}^{j}\bigr{)}\Bigr{]}\,.\]

By the inductive hypothesis we know that for any \(G\in\mathbf{G}\)

\[\psi_{h+1:H+1}(\pi_{G}^{\star})\in\Theta_{G,h+1}\times\cdots\times\Theta_{G,H+1}\]

Thus, \(\hat{\psi}_{h}(\pi_{G}^{\star})\in\hat{\Theta}_{G,h}\). It is left to show that \(\Bigl{\|}\hat{\psi}_{h}(\pi_{G}^{\star})-\psi_{h}(\pi_{G}^{\star})\Bigr{\|}_{ X_{h}}\leq\beta\), which together with the fact that \(\psi_{h}(\pi_{G}^{\star})\in\mathcal{B}(L_{2})\), implies the desired result, \(\psi_{h}(\pi_{G}^{\star})\in\Theta_{G,h}\).

We would like to make use of Lemma H.1 to bound \(\Bigl{\|}\hat{\psi}_{h}(\pi_{G}^{\star})-\psi_{h}(\pi_{G}^{\star})\Bigr{\|}_{ X_{h}}\). To do so, we map the terms used in the Lemma H.1 to our terms as follows

\[n=n,\lambda=\lambda,\theta_{\star}=\psi_{h}(\pi_{G}^{\star}),V=X_{h},\hat{ \theta}=\hat{\psi}_{h}(\pi_{G}^{\star}),A=\Bigl{(}\phi_{h}^{j}\Bigr{)}_{j\in[n]}\,,\]

\[Y=\tilde{Y}+\Delta=\Bigl{(}\Bigl{\langle}\phi_{h}^{j},\psi_{h}(\pi_{G}^{\star} )\Bigr{\rangle}\Bigr{)}_{j\in[n]}+\gamma+\Delta=\Biggl{(}\mathop{\mathbb{E}}_{ \tau\sim F_{G,h+1}^{j}}\Bigl{[}r_{h:\tau-1}^{j}+\bar{v}_{\psi_{h+1:H+1}(\pi_{ G}^{\star})}\bigl{(}s_{\tau}^{j}\bigr{)}\Bigr{]}\Biggr{)}_{j\in[n]}\,,\]

\[\gamma=\Biggl{(}\mathop{\mathbb{E}}_{\tau\sim F_{G,h+1}^{j}}\Bigl{[}r_{h:\tau -1}^{j}+\bar{v}_{\psi_{h+1:H+1}(\pi_{G}^{\star})}\bigl{(}s_{\tau}^{j}\bigr{)} \Bigr{]}-\mathop{\mathbb{E}}_{\text{Taj}\sim\mathbb{P}_{\pi^{0},\psi_{h}^{j}, \alpha_{h}^{j}}}\mathop{\mathbb{E}}_{\tau\sim F_{G,\text{Taj},h+1}}\Bigl{[}R_ {h:\tau-1}+\bar{v}_{\psi_{h+1:H+1}(\pi_{G}^{\star})}(S_{\tau})\Bigr{]}\Biggr{)} _{j\in[n]}\,,\]

\[\alpha=\Biggl{(}\mathop{\mathbb{E}}_{\text{Taj}\sim\mathbb{P}_{\pi^{0},\psi_{h }^{j},\alpha_{h}^{j}}}\mathop{\mathbb{E}}_{\tau\sim F_{G,\text{Taj},h+1}} \Bigl{[}R_{h:\tau-1}+\bar{v}_{\psi_{h+1:H+1}(\pi_{G}^{\star})}(S_{\tau})\Bigr{]} -\Bigl{\langle}\phi_{h}^{j},\psi_{h}(\pi_{G}^{\star})\Bigr{\rangle}\Biggr{)}_{j \in[n]}\,,\]

\[\iota=\sum_{j\in[n]}\phi_{h}^{j}\Biggl{(}\mathop{\mathbb{E}}_{\tau\sim F_{G, h+1}^{j}}\Bigl{[}r_{h:\tau-1}^{j}+\bar{v}_{\psi_{h+1:H+1}(\pi_{G}^{\star})} \bigl{(}s_{\tau}^{j}\bigr{)}\Bigr{]}-\mathop{\mathbb{E}}_{\text{Taj}\sim \mathbb{P}_{\pi^{0},\psi_{h}^{j},\alpha_{h}^{j}}}\mathop{\mathbb{E}}_{\tau\sim F _{G,\text{Taj},h+1}}\Bigl{[}R_{h:\tau-1}+\bar{v}_{\psi_{h+1:H+1}(\pi_{G}^{ \star})}(S_{\tau})\Bigr{]}\Biggr{)}\,.\]

With the definitions as above, applying Lemma H.1 we get

\[\Bigl{\|}\hat{\psi}_{h}(\pi_{G}^{\star})-\psi_{h}(\pi_{G}^{\star})\Bigr{\|}_{ X_{h}}\leq\sqrt{\lambda}\|\psi_{h}(\pi_{G}^{\star})\|_{2}+\|\Delta\|_{\infty}\sqrt{n}+\| \iota\|_{X_{h}^{-1}}\,.\] (42)

The first term in Eq. (42) can be bounded by \(H^{3/2}d\) by recalling that \(\sqrt{\lambda}=H^{3/2}d/\tilde{L_{2}}\) (Eq. (25)) and \(\|\psi_{h}(\pi_{G}^{\star})\|_{2}\leq L_{2}\leq\tilde{L_{2}}\).

The \(\left\|\Delta\right\|_{\infty}\) in the second term in Eq. (42) can be bounded by first decomposing the error, and using a triangle inequality as follows.

\[\left\|\Delta\right\|_{\infty}\leq\left\|\Biggl{(}\mathop{\mathbb{E}}_{ \text{Taj}\sim\mathbb{P}_{\pi^{0},\psi_{h}^{j},\alpha_{h}^{j}}}\mathop{\mathbb{ E}}_{\tau\sim F_{G,\text{Taj},h+1}}\biggl{[}\bar{v}_{\psi_{h+1:H+1}(\pi_{G}^{ \star})}(S_{\tau})-\mathop{\max}_{a^{\prime}\in\mathcal{A}}q^{\pi_{G}^{ \star}}(S_{\tau},a^{\prime})\biggr{]}\Biggr{)}_{j\in[n]}\right\|_{\infty}\] \[\qquad\qquad+\left\|\Biggl{(}\mathop{\mathbb{E}}_{\text{Taj}\sim \mathbb{P}_{\pi^{0},\psi_{h}^{j},\alpha_{h}^{j}}}\mathop{\mathbb{E}}_{\tau\sim F _{G,\text{Taj},h+1}}\biggl{[}R_{h:\tau-1}+\mathop{\max}_{a^{\prime}\in \mathcal{A}}q^{\pi_{G}^{\star}}(S_{\tau},a^{\prime})\biggr{]}-\Bigl{\langle}\phi_ {h}^{j},\psi_{h}(\pi_{G}^{\star})\Bigr{\rangle}\Biggr{)}_{j\in[n]}\right\|_{ \infty}.\] (43)

For the first term in Eq. (43), notice that

\[\bar{v}_{\psi_{h+1:H+1}(\pi_{G}^{\star})}(S_{\tau})-\mathop{\max}_{a^{\prime}\in \mathcal{A}}q^{\pi_{G}^{\star}}(S_{\tau},a^{\prime})\leq\mathop{\max}_{a^{ \prime}\in\mathcal{A}}\Bigl{(}\bigl{\langle}\phi(S_{\tau},a^{\prime}),\psi_{ \text{stage}(S_{\tau})}(\pi_{G}^{\star})\bigr{\rangle}-q^{\pi_{G}^{\star}}(S_{ \tau},a^{\prime})\Bigr{)}\leq\eta\,,\]where the last inequality holds since we have approximate linear \(q^{\pi}\)-realizability (Assumption 1). To bound the second term in Eq. (43) notice that by the definition of \(\pi^{\star}_{G}\) (Eq. (15)), \(\arg\max_{a^{\prime}\in\mathcal{A}}q^{\pi^{\star}_{G}}(S_{\tau},a^{\prime})\) is exactly the action \(\pi^{\star}_{G}\) would take at the stopping stage \(\tau\), and that the distribution of Traj under policy \(\pi^{0}\) until stopping stage \(\tau\) is same as the distribution of Traj under policy \(\pi^{\star}_{G}\) until stopping stage \(\tau\). This implies that

\[\left\|\left(\underset{\mathbb{E}_{\mathrm{Traj}\sim\mathbb{P}_{a ^{\prime}},\rho^{j}_{h},a^{\prime}_{h},a^{\prime}_{h}}}{\mathbb{E}_{\mathrm{ Traj}_{h},h+1}}\bigg{[}R_{h:\tau-1}+\max_{a^{\prime}\in\mathcal{A}}q^{\pi^{ \star}_{G}}(S_{\tau},a^{\prime})\bigg{]}-\left\langle\phi^{j}_{h},\psi_{h}( \pi^{\star}_{G})\right\rangle\right)_{j\in[n]}\right\|_{\infty}\] \[=\left\|\left(q^{\pi^{\star}_{G}}(s^{j}_{h},a^{\prime}_{h})- \left\langle\phi^{j}_{h},\psi_{h}(\pi^{\star}_{G})\right\rangle\right)_{j\in [n]}\right\|_{\infty}\leq\eta\,,\]

where the last inequality holds since we have approximate linear \(q^{\pi}\)-realizability (Assumption 1). Plugging the above bounds into Eq. (43), we get that \(\left\|\Delta\right\|_{\infty}\leq 2\eta\).

To bound the third term in Eq. (42), let the event \(\mathcal{E}_{1}\) be as defined in the proof of Lemma H.2, which occurs with probability at least \(1-\delta/3\). Then, under event \(\mathcal{E}_{1}\), the third term in Eq. (42) can be bounded by \(\bar{\beta}\) (Eq. (31)), by applying Lemma H.2 since \(\psi_{h+1:H+1}(\pi^{\star}_{G})\in\mathcal{B}(L_{2})^{H-h+1}\subset\mathcal{B} (\tilde{L_{2}})^{H-h+1}\).

Plugging the three bounds above back into Eq. (42) we get that, under event \(\mathcal{E}_{1}\), it holds that

\[\left\|\hat{\psi}_{h}(\pi^{\star}_{G})-\psi_{h}(\pi^{\star}_{G})\right\|_{X_{ h}}\leq H^{3/2}d+2\eta\sqrt{n}+\bar{\beta}\leq\beta\,,\]

where \(\beta\) is defined in Eq. (32), and the last inequality can be seen to hold by plugging in parameter values according to Appendix A. Thus, under event \(\mathcal{E}_{1}\), which occurs with probability at least \(1-\delta/3\), for any \(G\in\mathbf{G}\) and \(h\in[H]\) it holds that \(\psi_{h}(\pi^{\star}_{G})\in\Theta_{G,h}\), which completes the proof.

Proof of Lemma 5.1

Proof.: Let \(G\in\mathbf{G}\) be a feasible solution to Optimization Problem 1. By Lemma I.1, there is an event \(\mathcal{E}_{2}\), that occurs with probability at least \(1-\delta/3\), such that under event \(\mathcal{E}_{2}\), for all \(G\in\mathbf{G}\), and for all \(h\in[H]\), it holds that

\[\leq\frac{H}{\sqrt{n}}\sqrt{\log\!\left(\frac{6H|C_{\xi}^{\mathbf{G}}|}{ \delta}\right)}+2L_{\xi}\,.\]

where \(|C_{\xi}^{\mathbf{G}}|,L_{\xi}\), are defined in Eqs. (33) and (34). Let \(h\in[H]\). Recall that since \(G\) is a feasible solution to Optimization Problem 1 we know that Eq. (14) passed for \(h\). Thus,

\[\frac{1}{n}\sum_{i\in[n]}\!\left(\!\max_{\theta\in\Theta_{G,h}}\bar{q}_{ \theta}(s^{i}_{h},a^{i}_{h})-\min_{\theta\in\Theta_{G,h}}\bar{q}_{\theta}(s^{ i}_{h},a^{i}_{h})\right)\leq\bar{\epsilon}\,.\]

Combining the above two results, we have that under event \(\mathcal{E}_{2}\), for any \(h\in[H]\), it holds that

\[\underset{(S,A)\sim\mu_{h}}{\mathbb{E}}\!\left[\max_{\theta\in\Theta_{G,h}} \bar{q}_{\theta}(S,A)-\min_{\theta\in\Theta_{G,h}}\bar{q}_{\theta}(S,A)\right] \leq\frac{H}{\sqrt{n}}\sqrt{\log\!\left(\frac{6H|C_{\xi}^{\mathbf{G}}|}{\delta }\right)}+2L_{\xi}+\bar{\epsilon}\,.\]

Now we will relate the data collecting distribution \(\mu\) to any admissible distribution \(\nu\). Notice that by the definition of \(\max\) and \(\min\), for any \((s,a)\in\mathcal{S}\times\mathcal{A}\), it holds that

\[\underset{\theta\in\Theta_{G,h}}{\max}\bar{q}_{\theta}(s,a)-\min_{\theta\in \Theta_{G,h}}\bar{q}_{\theta}(s,a)\geq 0\,.\]

Thus, we can apply Lemma G.4 to get that under event \(\mathcal{E}_{2}\), for all \(h\in[H]\), and admissible distribution \(\nu=(\nu_{t})_{t\in[H]}\), it holds that

\[\underset{(S,A)\sim\nu_{h}}{\mathbb{E}}\!\left[\max_{\theta\in\Theta_{G,h}} \bar{q}_{\theta}(S,A)-\min_{\theta\in\Theta_{G,h}}\bar{q}_{\theta}(S,A)\right] \leq C_{\text{conc}}\!\left(\frac{H}{\sqrt{n}}\sqrt{\log\!\left(\frac{6H|C_{ \xi}^{\mathbf{G}}|}{\delta}\right)}+2L_{\xi}+\bar{\epsilon}\right).\]

To conclude, we have that under event \(\mathcal{E}_{2}\), for any \(G\in\mathbf{G}\) that is a feasible solution to Optimization Problem 1, for any \(h\in[H]\), for any \((\theta_{s,a})_{(s,a)\in\mathcal{S}_{h}\times\mathcal{A}}\) and \((\tilde{\theta}_{s,a})_{(s,a)\in\mathcal{S}_{h}\times\mathcal{A}}\) with \(\theta_{s,a},\tilde{\theta}_{s,a}\in\Theta_{G,h}\) for all \((s,a)\in\mathcal{S}_{h}\times\mathcal{A}\), and for any admissible distribution \(\nu=(\nu_{t})_{t\in[H]}\), it holds that

\[\underset{(S,A)\sim\nu_{h}}{\mathbb{E}}\!\left[\bar{q}_{\theta_{ S,A}}(S,A)-\bar{q}_{\theta_{S,A}}(S,A)\right]\] \[\leq C_{\text{conc}}\!\left(\frac{H}{\sqrt{n}}\sqrt{\log\!\left( \frac{6H(1|C_{\xi}^{\mathbf{G}}|}{\delta}\right)}+2L_{\xi}+\bar{\epsilon}\right)\] \[\leq C_{\text{conc}}\!\left(\frac{H}{\sqrt{n}}\sqrt{\log\!\left( \frac{6H(1+2L_{2}/\xi))^{dH_{0}}}{\delta}\right)}+24\sqrt{2d}H^{2}L_{1}\xi \alpha^{-1}\!\left(2\sqrt{n}L_{1}\tilde{L_{2}}/(H^{3/2}d)\right)^{H}+\bar{ \epsilon}\right)\] \[\leq C_{\text{conc}}\!\left(\frac{H}{\sqrt{n}}\sqrt{dH^{2}d_{0} \log\!\left(1+96\sqrt{2d}H^{2}L_{1}L_{2}\alpha^{-1}\sqrt{n}L_{1}\tilde{L_{2}} /(H^{3/2}d)\right)+\log\!\left(\frac{6H}{\delta}\right)}+\frac{1}{\sqrt{n}}+ \bar{\epsilon}\right)\] \[=\tilde{\epsilon}=\tilde{\mathcal{O}}\!\left(\frac{C_{\text{conc}} H^{2}d}{\sqrt{n}}+\frac{C_{\text{conc}}}{\sqrt{n}}+\frac{C_{\text{conc}}^{2}H^{5/2}d^{2}}{ \sqrt{n}}\right)=\tilde{\mathcal{O}}\!\left(\frac{C_{\text{conc}}^{2}H^{5/2}d^ {2}}{\sqrt{n}}\right).\] (44)

The third inequality holds by plugging in the values of \(|C_{\xi}^{\mathbf{G}}|,L_{\xi}\), as defined in Eqs. (33) and (34). The last inequality holds by setting \(\xi^{-1}=24\sqrt{2d}\sqrt{n}H^{2}L_{1}\alpha^{-1}\!\left(2\sqrt{n}L_{1} \tilde{L_{2}}/(H^{3/2}d)\right)^{H}\). The last two equalities hold by plugging in parameter values according to Appendix A.

[MISSING_PAGE_FAIL:21]

expectation taken w.r.t. the distribution \(\mu\) (similar to the first term, which we claim we know how to bound). To this end, for any \((s^{\prime},a^{\prime})\in\mathcal{S}\times\mathcal{A}\), define the policy \(\check{\pi}_{h}\) as

\[\check{\pi}_{h}(a^{\prime}|s^{\prime})=\begin{cases}\pi^{0}(a^{\prime}|s^{ \prime})&\text{if }\operatorname{stage}(s^{\prime})\leq h\\ \check{\pi}(a^{\prime}|s^{\prime})&\text{if }\operatorname{stage}(s^{\prime})>h \,.\end{cases}\]

Notice that for any \(t\in[H],u\in[t+1:H+1]\), \(\operatorname{\mathbbm{E}}_{(S_{t},A_{t})\sim\mu_{t}}\operatorname{ \mathbbm{E}}_{(S_{u},A_{u})\sim\mathbb{P}^{u}_{s,S_{t},A_{t}}}=\operatorname{ \mathbbm{E}}_{(S_{u},A_{u})\sim\mathbb{P}^{u}_{t_{t},s}}\). Let \(\check{\nu}_{u}(s_{u},a_{u})=\mathbb{P}^{u}_{\check{\pi}_{h},s_{1}}(s_{u},a_{ u})\) for all \(u\in[H],(s_{u},a_{u})\in\mathcal{S}_{u}\times\mathcal{A}\). Clearly, \(\check{\nu}=(\check{\nu}_{u})_{n\in[H]}\) is an admissible distribution by Definition 1. Thus, with the definition of \(\check{\nu}\) and using Lemma G.4, we get that Eq. (49) is

\[=4\beta\operatorname*{\mathbbm{E}}_{(S_{h},A_{h})\sim\mu_{h}} \min\Bigl{\{}1,\|\phi(S_{h},A_{h})\|_{X_{h}^{-1}}\Bigr{\}}+4\beta\sum_{t=h+1} ^{H}\operatorname*{\mathbbm{E}}_{(S_{t},A_{t})\sim\check{\nu}_{h}}\min\Bigl{\{} 1,\|\phi(S_{t},A_{t})\|_{X_{t}^{-1}}\Bigr{\}}+(H-h+1)\check{\eta}\] \[\leq 4\beta\operatorname*{\mathbbm{E}}_{(S_{h},A_{h})\sim\mu_{h}} \min\Bigl{\{}1,\|\phi(S_{h},A_{h})\|_{X_{h}^{-1}}\Bigr{\}}+4C_{\text{conc}} \beta\sum_{t=h+1}^{H}\operatorname*{\mathbbm{E}}_{(S_{t},A_{t})\sim\mu_{t}} \min\Bigl{\{}1,\|\phi(S_{t},A_{t})\|_{X_{t}^{-1}}\Bigr{\}}+(H-h+1)\check{\eta}\] \[\leq 4C_{\text{conc}}\beta\sum_{t=h}^{H}\operatorname*{\mathbbm{E}} _{(S_{t},A_{t})\sim\mu_{t}}\min\Bigl{\{}1,\|\phi(S_{t},A_{t})\|_{X_{t}^{-1}} \Bigr{\}}+(H-h+1)\check{\eta}\,.\]

The last inequality used that \(C_{\text{conc}}\geq 1\). Finally, we can apply Lemma G.3 to bound \(\operatorname{\mathbbm{E}}_{(S_{t},A_{t})\sim\mu_{t}}\min\Bigl{\{}1,\|\phi(S_ {t},A_{t})\|_{X_{t}^{-1}}\Bigr{\}}\). In particular, let \(\mathcal{E}_{3}\) be as defined in the proof of Lemma G.3. Then, by Lemma G.3, under event \(\mathcal{E}_{3}\), it holds that

\[4C_{\text{conc}}\beta\sum_{t=h}^{H}\operatorname*{\mathbbm{E}}_{(S_{t},A_{t}) \sim\mu_{t}}\min\Bigl{\{}1,\|\phi(S_{t},A_{t})\|_{X_{t}^{-1}}\Bigr{\}}\leq 4C_{ \text{conc}}\beta\sum_{t=h}^{H}\check{\epsilon}\leq 4HC_{\text{conc}}\check{ \epsilon}\beta\,.\]

Putting all of the bound after Eq. (49) together and plugging them into Eq. (49), we have that, under event \(\mathcal{E}_{1}\cap\mathcal{E}_{2}\), it holds that

\[\operatorname*{\mathbbm{E}}_{(S_{h},A_{h})\sim\mu_{h}}\biggl{[}\max_{\theta\in \Theta_{\mathcal{O},h}}\bar{q}_{\theta}(S_{h},A_{h})-\min_{\theta\in\Theta_{ \mathcal{O},h}}\bar{q}_{\theta}(S_{h},A_{h})\biggr{]}\leq(H-h+1)\check{\eta}+4 HC_{\text{conc}}\check{\epsilon}\beta\,.\] (50)

We are now ready to state the final result. Noting that \(h\in[H]\) was arbitrary, by combining Eqs. (45) and (50), we have that, under event \(\mathcal{E}_{1}\cap\mathcal{E}_{2}\cap\mathcal{E}_{3}\), for all \(h\in[H]\), it holds that

\[\frac{1}{n}\sum_{i\in[n]}\biggl{(}\max_{\theta\in\Theta_{\mathcal{ O},h}}\bar{q}_{\theta}(s_{h}^{i},a_{h}^{i})-\min_{\theta\in\Theta_{\mathcal{O},h}} \bar{q}_{\theta}(s_{h}^{i},a_{h}^{i})\biggr{)}\] \[\leq\frac{H}{\sqrt{n}}\sqrt{\log\biggl{(}\frac{6H|C_{\xi}^{ \mathbf{G}}|}{\delta}\biggr{)}}+2L_{\xi}+(H-h+1)\check{\eta}+4HC_{\text{conc}} \check{\epsilon}\beta\] \[=\frac{H}{\sqrt{n}}\sqrt{\log\biggl{(}\frac{6H(1+2L_{2}/\xi))^{dHd_ {0}}}{\delta}}+24\sqrt{2d}H^{2}L_{1}\xi\alpha^{-1}\Bigl{(}2\sqrt{n}L_{1} \tilde{L_{2}}/(H^{3/2}d)\Bigr{)}^{H}\] \[\quad\quad+(H-h+1)\check{\eta}+4HC_{\text{conc}}\check{\epsilon}\beta\] \[\leq\frac{H}{\sqrt{n}}\sqrt{dH^{2}d_{0}\log\Bigl{(}1+96\sqrt{n} \sqrt{2d}H^{2}L_{1}L_{2}\alpha^{-1}\sqrt{n}L_{1}\tilde{L_{2}}/(H^{3/2}d)\Bigr{)} +\log\biggl{(}\frac{6H}{\delta}\biggr{)}}+\frac{1}{\sqrt{n}}\] \[\quad\quad+(H-h+1)\check{\eta}+4HC_{\text{conc}}\check{\epsilon}\beta\] \[=\bar{\epsilon}=\tilde{\mathcal{O}}\biggl{(}\frac{H^{2}d}{\sqrt{n}} +\frac{1}{\sqrt{n}}+\frac{H^{5/2}d}{\sqrt{n}}+\frac{C_{\text{conc}}H^{5/2}d^{2 }}{\sqrt{n}}\biggr{)}=\tilde{\mathcal{O}}\biggl{(}\frac{C_{\text{conc}}H^{5/2}d^{ 2}}{\sqrt{n}}\biggr{)}\,.\] (51)

The first equality holds by plugging in the values of \(|C_{\xi}^{\mathbf{G}}|,L_{\xi}\), as defined in Eqs. (33) and (34). The second equality holds by setting \(\xi^{-1}=24\sqrt{n}\sqrt{2d}H^{2}L_{1}\alpha^{-1}\Bigl{(}2\sqrt{n}L_{1}\tilde{L _{2}}/(H^{3/2}d)\Bigr{)}^{H}\). The last two equalities hold by plugging in parameter values according to Appendix A.

Noticing that this is exactly the condition (Eq. (14)) in Optimization Problem 1 that needs to be satisfied by any feasible solution, we conclude that, under event \(\mathcal{E}_{1}\cap\mathcal{E}_{2}\cap\mathcal{E}_{3}\), the true guess \(\bar{G}\) is a feasible solution to Optimization Problem 1.

### Proof of Lemma g.1

Proof.: To prove Eq. (46) we will use induction. The base case is when \(h=H+1\), for which \(\bar{v}_{\theta_{H+1}}(s)=v^{\pi_{\widehat{G}}^{\circ}}(s)\) for all \(s\in\mathcal{S}_{H+1}\). This holds, since for all \((s,a)\in\mathcal{S}_{H+1}\times\mathcal{A}\), \(\bar{q}_{\theta_{H+1}}(s,a)=0\) for all \(\theta_{H+1}\in\Theta_{\bar{G},H+1}\), by the definition of \(\Theta_{\bar{G},H+1}\) (Eq. (13)), and \(v^{\pi_{\widehat{G}}^{\circ}}(s)=0\), by Eq. (1), since \(\mathcal{S}_{H+1}=\{s\top\}\).

Now, we show the inductive step. Let \(h\in[H]\) be arbitrary. Assume that Eq. (46) holds for any \(t\in[h+1:H+1]\). We prove that Eq. (46) holds for \(h\). Let \((s,a)\in\mathcal{S}_{h}\times\mathcal{A},\theta_{h}\in\Theta_{\bar{G},h}\). Then,

\[\left|\bar{q}_{\theta_{h}}(s,a)-q^{\pi_{\widehat{G}}^{\circ}}(s, a)\right| \leq\left|\min\Bigl{\{}H,\left\langle\phi(s,a),\theta_{h}\right\rangle-q^{\pi_{ \widehat{G}}^{\circ}}(s,a)\Bigr{\}}\right|\] \[=\min\Bigl{\{}H,\left|\left\langle\phi(s,a),\theta_{h}\right\rangle -q^{\pi_{\widehat{G}}^{\circ}}(s,a)\right|\Bigr{\}}\,,\] (52)

where the last equality holds since \(\bigl{(}\langle\phi(s,a),\theta_{h}\rangle-q^{\pi_{\widehat{G}}^{\circ}}(s, a)\bigr{)}\in[-H,H]\). We will focus on bounding \(\bigl{|}\langle\phi(s,a),\theta_{h}\rangle-q^{\pi_{\widehat{G}}^{\circ}}(s,a) \bigr{|}\). By the definition of the set \(\Theta_{\bar{G},h}\) (Eq. (13)) we know that there exists a \(\hat{\theta}_{h}\in\hat{\Theta}_{\bar{G},h}\) such that \(\left\|\theta_{h}-\hat{\theta}_{h}\right\|_{X_{h}}\leq\beta\). Thus, by the Cauchy-Schwarz inequality, we have that

\[\left|\left\langle\phi(s,a),\theta_{h}-\hat{\theta}_{h}\right\rangle\right| \leq\left\|\phi(s,a)\right\|_{X_{h}^{-1}}\left\|\theta_{h}-\hat{\theta}_{h} \right\|_{X_{h}}\leq\beta||\phi(s,a)||_{X_{h}^{-1}}\,.\]

This implies that

\[\left|\left\langle\phi(s,a),\theta_{h}\right\rangle-q^{\pi_{\widehat{G}}^{ \circ}}(s,a)\right|\leq\left|\left\langle\phi(s,a),\hat{\theta}_{h}\right\rangle -q^{\pi_{\widehat{G}}^{\circ}}(s,a)\right|+\beta||\phi(s,a)||_{X_{h}^{-1}}\,.\] (53)

Since we know \(\hat{\theta}_{h}\in\hat{\Theta}_{\bar{G},h}\), by the definition of \(\hat{\Theta}_{\bar{G},h}\) (Eq. (12)), there exists a \(\theta_{h+1:H+1}\in\Theta_{\bar{G},h+1}\times\cdots\times\Theta_{\bar{G},H+1}\), such that

\[\hat{\theta}_{h}=X_{h}^{-1}\sum_{j\in[n]}\phi_{h}^{j}\mathop{\mathbb{E}}_{ \tau\sim F_{\bar{G},h+1}^{j}}\Bigl{[}r_{h;\tau-1}^{j}+\bar{v}_{\theta_{h+1:H +1}}\bigl{(}s_{\tau}^{j}\bigr{)}\Bigr{]}\,.\]

By Lemma 4.2, we know there exists a parameter \(\rho_{h}^{\pi^{0}}(f)\in\mathcal{B}(\tilde{L_{2}})\), such that for all \((s,a)\in\mathcal{S}_{h}\times\mathcal{A}\),

\[\left|\mathop{\mathbb{E}}_{\text{Tri}\sim\mathcal{P}_{\pi^{0},s,a}}\mathop{ \mathbb{E}}_{\bar{G},\text{Tri},h+1}\bigl{[}R_{h:\tau-1}+\bar{v}_{\theta_{h+1 :H+1}}(S_{\tau})\bigr{]}-\left\langle\phi(s,a),\rho_{h}^{\pi^{0}}\bigl{(}\bar{v }_{\theta_{h+1:H+1}}\bigr{)}\right\rangle\right|\leq\tilde{\eta}\,.\] (54)

Let \(\rho_{h}^{\pi^{0}}\) be as defined above. Next, we will show a bound on \(\Bigl{|}\left\langle\phi(s,a),\hat{\theta}_{h}-\rho_{h}^{\pi^{0}}\bigl{(}\bar{v }_{\theta_{h+1:H+1}}\bigr{)}\right\rangle\Bigr{|}\) and \(\Bigl{|}\left\langle\phi(s,a),\rho_{h}^{\pi^{0}}\bigl{(}\bar{v}_{\theta_{h+1:H+ 1}}\bigr{)}\right\rangle-q^{\pi_{\widehat{G}}^{\circ}}(s,a)\Bigr{|}\), which together will give us a bound on \(\Bigl{|}\left\langle\phi(s,a),\hat{\theta}_{h}\right\rangle-q^{\pi_{\widehat{G} }^{\circ}}(s,a)\Bigr{|}\), as desired. The following result gives us a bound on \(\left\|\hat{\theta}_{h}-\rho_{h}^{\pi^{0}}\bigl{(}\bar{v}_{\theta_{h+1:H+1}} \bigr{)}\right\|_{X_{h}}\).

**Lemma G.2**.: _There is an event \(\mathcal{E}_{1}\), which occurs with probability at least \(1-\delta/3\), such that under event \(\mathcal{E}_{1}\), for all \(G\in\mathbf{G}\) for all \(h\in[H]\), and for all \(\hat{\theta}_{h}\in\hat{\Theta}_{\bar{G},h}\), it holds that_

\[\left\|\hat{\theta}_{h}-\theta_{h}^{\star}\right\|_{X_{h}}\leq\beta\,,\]

_where_

\[\hat{\theta}_{h}=X_{h}^{-1}\sum_{j\in[n]}\phi_{h}^{j}\mathop{\mathbb{E}}_{\tau \sim F_{\bar{G},h+1}^{j}}\Bigl{[}r_{h;\tau-1}^{j}+\bar{v}_{\theta_{h+1:H+1}} \bigl{(}s_{\tau}^{j}\bigr{)}\Bigr{]}\quad\text{for some $\theta_{h+1:H+1}\in\Theta_{G,h+1}\times\cdots\times\Theta_{G,H+1}$}\,,\]

_and \(\theta_{h}^{\star}\in\mathcal{B}(\tilde{L_{2}})\) is such that, for all \((s,a)\in\mathcal{S}_{h}\times\mathcal{A}\), it satisfies_

\[\left|\mathop{\mathbb{E}}_{\text{Tri}\sim\mathcal{P}_{\pi^{0},s,a}}\mathop{ \mathbb{E}}_{\tau\sim F_{\bar{G},\text{Tri},h+1}}\bigl{[}R_{h:\tau-1}+\bar{v}_{ \theta_{h+1:H+1}}(S_{\tau})\bigr{]}-\left\langle\phi(s,a),\theta_{h}^{\star} \right\rangle\right|\leq\tilde{\eta}\,.\]Proof.: Fix \(G\in\mathbf{G}\), \(h\in[H]\), and \(\hat{\theta}_{h}\in\hat{\Theta}_{G,h}\), such that

\[\hat{\theta}_{h}=X_{h}^{-1}\sum_{j\in[n]}\phi_{h}^{j}\,\underset{\tau\sim F_{G,h +1}^{j}}{\mathbb{E}}\Big{[}r_{h:\tau-1}^{j}+\bar{v}_{\theta_{h+1:H+1}}\big{(}s_ {\tau}^{j}\big{)}\Big{]}\quad\text{for some }\theta_{h+1:H+1}\in\Theta_{G,h+1} \times\cdots\times\Theta_{G,H+1}\,.\]

Fix \(\theta_{h}^{\star}\in\mathcal{B}(\tilde{L_{2}})\), such that, for all \((s,a)\in\mathcal{S}_{h}\times\mathcal{A}\), it satisfies

\[\left|\underset{\text{Tinj}\sim\mathbb{P}_{\pi_{0},s,a}}{\mathbb{E}}\, \underset{\tau\sim F_{G,\text{Tinj},h+1}}{\mathbb{E}}\big{[}R_{h:\tau-1}+\bar{ v}_{\theta_{h+1:H+1}}\big{(}S_{\tau}\big{)}\big{]}-\langle\phi(s,a),\theta_{h}^{ \star}\rangle\right|\leq\tilde{\eta}\,.\] (55)

We would like to make use of Lemma H.1 to bound \(\left\|\hat{\theta}_{h}-\theta_{h}^{\star}\right\|_{X_{h}}\). To do so, we map the terms used in the Lemma H.1 to our terms as follows

\[n=n,\lambda=\lambda,\theta_{\star}=\theta_{h}^{\star},V=X_{h},\hat{\theta}= \hat{\theta}_{h},A=\left(\phi_{h}^{j}\right)_{j\in[n]},\]

\[\gamma=\left(\underset{\tau\sim F_{G,h+1}^{j}}{\mathbb{E}}\Big{[}r_{h:\tau-1 }^{j}+\bar{v}_{\theta_{h+1:H+1}}\big{(}s_{\tau}^{j}\big{)}\Big{]}-\underset{ \text{Tinj}\sim\mathbb{P}_{\pi_{0},s_{h}^{j},s_{h}^{j}}}{\mathbb{E}}\, \underset{\tau\sim F_{G,\text{Tinj},h+1}}{\mathbb{E}}\big{[}R_{h:\tau-1}+\bar {v}_{\theta_{h+1:H+1}}(S_{\tau})\big{]}\right)_{j\in[n]},\]

\[\Delta=\left(\underset{\text{Tinj}\sim\mathbb{P}_{\pi_{0},s_{h}^{j},s_{h}^{j},\sigma_{h}^{j}}}{\mathbb{E}}\,\underset{\tau\sim F_{G,\text{Tinj},h+1}}{ \mathbb{E}}\big{[}R_{h:\tau-1}+\bar{v}_{\theta_{h+1:H+1}}(S_{\tau})\big{]}- \left\langle\phi_{h}^{j},\theta_{h}^{\star}\right\rangle\right)_{j\in[n]},\]

\[\iota=\sum_{j\in[n]}\phi_{h}^{j}\bigg{(}\underset{\tau\sim F_{G,h+1}^{j}}{ \mathbb{E}}\Big{[}r_{h:\tau-1}^{j}+\bar{v}_{\theta_{h+1:H+1}}\big{(}s_{\tau}^{ j}\big{)}\Big{]}-\underset{\text{Tinj}\sim\mathbb{P}_{\pi_{0},s_{h}^{j},s_{h}^{j},\sigma_{h}^{j}}}{\mathbb{E}}\,\underset{\tau\sim F_{G,\text{Tinj},h+1}}{ \mathbb{E}}\big{[}R_{h:\tau-1}+\bar{v}_{\theta_{h+1:H+1}}(S_{\tau})\big{]}\bigg{)}\,.\]

With the definitions as above, applying Lemma H.1 we get

\[\left\|\hat{\theta}_{h}-\theta_{h}^{\star}\right\|_{X_{h}}\leq\sqrt{\lambda} \|\theta_{h}^{\star}\|_{2}+\left\|\Delta\right\|_{\infty}\sqrt{n}+\left\|\iota \right\|_{X_{h}^{-1}}.\] (56)

The first term can be bounded by \(H^{3/2}d\), by noting that \(\left\|\theta_{h}^{\star}\right\|_{2}\leq\tilde{L_{2}}\), and setting

\[\sqrt{\lambda}=H^{3/2}d/\tilde{L_{2}}\,.\] (57)

The second term can be bounded by \(\tilde{\eta}\sqrt{n}\), by using Eq. (55).

To bound the third term let the event \(\mathcal{E}_{1}\) be as defined in the proof of Lemma H.2, which occurs with probability at least \(1-\delta/3\). Then, under event \(\mathcal{E}_{1}\), the third term in Eq. (56) can be bounded by \(\tilde{\beta}\) (Eq. (31)), by applying Lemma H.2, since \(\theta_{h+1:H+1}\in\Theta_{G,h+1}\times\cdots\times\Theta_{G,H+1}\subset \mathcal{B}(\tilde{L_{2}})^{H-h+1}\).

Plugging the three bounds above back into Eq. (56) we get that under event \(\mathcal{E}_{1}\), it holds that

\[\left\|\hat{\theta}_{h}-\theta_{h}^{\star}\right\|_{X_{h}} \leq H^{3/2}d+\tilde{\eta}\sqrt{n}+\tilde{\beta}\] \[=\beta=\tilde{\mathcal{O}}\Big{(}H^{3/2}d\Big{)}\,.\] (58)

The last equality holds by setting

\[\eta\leq\frac{H^{3/2}d}{\sqrt{n}(10H^{2}d_{0}/\alpha+1)}\implies\tilde{\eta} \leq H^{3/2}d/\sqrt{n}\,,\] (59)

and the values of \(n,\bar{\beta}\) are set according to Appendix A. 

We return back to the proof of Lemma G.1. Let \(\mathcal{E}_{1}\) be as defined in the proof of Lemma G.2. For the remainder of the proof, operate under event \(\mathcal{E}_{1}\). By Lemma G.2 (with \(\rho_{h}^{\pi^{0}}\big{(}\bar{v}_{\theta_{h+1:H+1}}\big{)}=\theta_{h}^{\star}\)), we have that

\[\left\|\hat{\theta}_{h}-\rho_{h}^{\pi^{0}}\big{(}\bar{v}_{\theta_{h+1:H+1}} \big{)}\right\|_{X_{h}}\leq\beta\,.\] (60)Returning to \(\left|\left\langle\phi(s,a),\hat{\theta}_{h}-\rho_{h}^{\pi^{0}}\big{(}\bar{v}_{ \theta_{h+1:H+1}}\big{)}\right\rangle\right|\), we can now bound it as follows.

\[\left|\left\langle\phi(s,a),\hat{\theta}_{h}-\rho_{h}^{\pi^{0}}\big{(}\bar{v}_{ \theta_{h+1:H+1}}\big{)}\right\rangle\right|\leq\left|\left\lvert\phi(s,a) \right\rvert\right\rvert_{X_{h}^{-1}}\left\lVert\hat{\theta}_{h}-\rho_{h}^{ \pi^{0}}\big{(}\bar{v}_{\theta_{h+1:H+1}}\big{)}\right\rVert_{X_{h}}\leq\beta \left\lVert\phi(s,a)\right\rVert_{X_{h}^{-1}}.\] (61)

The first inequality used the Cauchy-Schwarz inequality. The second inequality used Eq. (60). Now, we bound \(\left|\left\langle\phi(s,a),\rho_{h}^{\pi^{0}}\big{(}\bar{v}_{\theta_{h+1:H+1}} \big{)}\right\rangle-q^{\pi^{\underline{\zeta}}}(s,a)\right|\), by making use of Eq. (54), to get that

\[\leq\left|\operatorname*{\mathbb{E}}_{\operatorname*{\mathbb{T} \mathrm{z}}\sim\mathbb{P}_{\pi^{0},s,a}}\operatorname*{\mathbb{E}}_{\tau\sim F _{G,\mathrm{z}\mathrm{z}\mathrm{z}},h+1}\big{[}R_{h:\tau-1}+\bar{v}_{\theta_{h +1:H+1}}(S_{\tau})\big{]}-q^{\pi^{\underline{\zeta}}}(s,a)\right|+\tilde{\eta}\] \[=\left|\operatorname*{\mathbb{E}}_{\operatorname*{\mathbb{T} \mathrm{z}}\sim\mathbb{P}_{\pi^{0},s,a}}\operatorname*{\mathbb{E}}_{\tau\sim F _{G,\mathrm{z}\mathrm{z}\mathrm{z}},h+1}\bigg{[}R_{h:\tau-1}+\max_{a^{\prime} \in\mathcal{A}}q^{\pi^{\underline{\zeta}}_{\partial}}(S_{\tau},a^{\prime})- \max_{a^{\prime}\in\mathcal{A}}q^{\pi^{\underline{\zeta}}_{\partial}}(S_{ \tau},a^{\prime})+\max_{a^{\prime}\in\mathcal{A}}q^{\pi^{\underline{\zeta}}_{ \partial}}(S_{\tau},a^{\prime})\bigg{]}-q^{\pi^{\underline{\zeta}}_{\partial}} (s,a)\bigg{|}+\tilde{\eta}\] \[\leq\left|\operatorname*{\mathbb{E}}_{\operatorname*{\mathbb{T} \mathrm{z}}\sim\mathbb{P}_{\pi^{0},s,a}}\operatorname*{\mathbb{E}}_{\tau\sim F _{G,\mathrm{z}\mathrm{z}\mathrm{z}},h+1}\bigg{[}R_{h:\tau-1}+\max_{a^{\prime} \in\mathcal{A}}q^{\pi^{\underline{\zeta}}_{\partial}}(S_{\tau},a^{\prime}) \bigg{]}-q^{\pi^{\underline{\zeta}}_{\partial}}(S_{\tau},a)\right|\] \[\qquad+\left|\operatorname*{\mathbb{E}}_{\operatorname*{\mathbb{T }\mathrm{z}}\sim\mathbb{P}_{\pi^{0},s,a}}\operatorname*{\mathbb{E}}_{\tau\sim F _{G,\mathrm{z}\mathrm{z}\mathrm{z}},h+1}\alpha^{\pi^{\underline{\zeta}}_{ \partial}}(S_{\tau},a^{\prime})-q^{\pi^{\underline{\zeta}}_{\partial}}(S_{ \tau},a^{\prime})\right|+\tilde{\eta}\,.\] (62)

The equality used the definition of \(\bar{v}\) (Eq. (10)). To bound the first term in Eq. (62) notice that by the definition of \(\pi^{\underline{\zeta}}_{\partial}\) (Eq. (15)), \(\arg\max_{a^{\prime}\in\mathcal{A}}q^{\pi^{\underline{\zeta}}_{\partial}}(S_{ \tau},a^{\prime})\) is exactly the action \(\pi^{\underline{\zeta}}_{G}\) would take at the stopping stage \(\tau\), and that the distribution of Traj under policy \(\pi^{0}\) until stopping stage \(\tau\) is same as the distribution of Traj under policy \(\pi^{\underline{\tau}}_{G}\) until stopping stage \(\tau\). This gives that

\[\left|\operatorname*{\mathbb{E}}_{\operatorname*{\mathbb{T}\mathrm{z}}\sim \mathbb{P}_{\pi^{0},s,a}}\operatorname*{\mathbb{E}}_{\tau\sim F_{G,\mathrm{z} \mathrm{z}\mathrm{z}},h+1}\bigg{[}R_{h:\tau-1}+\max_{a^{\prime}\in\mathcal{A}} q^{\pi^{\underline{\zeta}}_{\partial}}(S_{\tau},a^{\prime})\bigg{]}-q^{\pi^{ \underline{\zeta}}_{\partial}}(S_{\tau},a)\right|=\left|q^{\pi^{\underline{ \zeta}}_{\partial}}(s,a)-q^{\pi^{\underline{\zeta}}_{\partial}}(s,a)\right|= 0\,.\]

To bound the second term in Eq. (62) we can use the inductive hypothesis (Eq. (46)). Defining notation that will be needed for the below display, for any \((s^{\prime},a^{\prime})\in\mathcal{S}\times\mathcal{A}\) we will write \(\operatorname*{\mathrm{T}\mathrm{z}}^{\prime}\sim\mathbb{P}_{\bar{\pi},s,a}\) to have the usual definition \(\operatorname*{\mathrm{T}\mathrm{z}}^{\prime}=(s^{\prime},a^{\prime},R_{h}^{ \prime},\ldots,S_{H+1}^{\prime},A_{H+1}^{\prime},R_{H+1}^{\prime})\), except with a superscript \((\cdot)^{\prime}\) added to all of the random elements. Then, by letting \(a_{\tau}=\arg\max_{a^{\prime}\in\mathcal{A}}\big{(}\bar{q}_{\theta_{\mathrm{ stage}(S_{\tau})}}(S_{\tau},a^{\prime})-q^{\pi^{\underline{\zeta}}_{\partial}}(S_{\tau},a^{ \prime})\big{)}\), applying the inductive hypothesis, and a triangle inequality, we have that

\[\leq\left|\operatorname*{\mathbb{E}}_{\operatorname*{\mathbb{T} \mathrm{z}}\sim\mathbb{P}_{\pi^{0},s,a}}\operatorname*{\mathbb{E}}_{\tau\sim F _{G,\mathrm{z}\mathrm{z}\mathrm{z}},h+1}\left[2\beta\operatorname*{\mathbb{E}}_{ \operatorname*{\mathbb{T}\mathrm{z}}^{\prime}\sim\mathbb{P}_{\pi,S_{\tau},a^{ \prime}}}\sum_{t=\tau}^{H}\min\Bigl{\{}1,\left\lVert\phi(S_{t}^{\prime},A_{t}^ {\prime})\right\rVert_{X_{t}^{-1}}\Bigr{\}}\right|+\left|\operatorname*{ \mathbb{E}}_{\operatorname*{\mathbb{T}\mathrm{z}}\sim\mathbb{P}_{\pi^{0},s,a}} \operatorname*{\mathbb{E}}_{\tau\sim F_{G,\mathrm{z}\mathrm{z},h+1}}(H-\tau+1) \tilde{\eta}\right]\right|.\]

We bound each of the terms in the expectation separately. For the first term, we first recall the definition of \(g^{\bar{\pi}}\) (Eq. (48)) and upper bound the term inside the expectation as follows, which will help us relate things to \(\bar{\pi}\) as we shall see soon.

\[\operatorname*{\mathbb{E}}_{\operatorname*{\mathrm{T}\mathrm{z}}\sim\mathbb{P}_{ \bar{\pi},s,a}}\sum_{t=\tau}^{H}\min\Bigl{\{}1,\left\lVert\phi(S_{t}^{\prime},A_{t }^{\prime})\right\rVert_{X_{t}^{-1}}\Bigr{\}}=g^{\bar{\pi}}(S_{\tau},a_{\tau}) \leq\max_{a^{\prime}\in\mathcal{A}}g^{\bar{\pi}}(S_{\tau},a^{\prime})\,.\]

Along with the above result, notice that by the definition of \(\bar{\pi}\) (Eq. (47)), \(\arg\max_{a^{\prime}\in\mathcal{A}}g^{\bar{\pi}}(S_{\tau},a^{\prime})\) is exactly the action \(\bar{\pi}\) would take at the stopping stage \(\tau\), and that the distribution of Traj under policy \(\pi^{0}\) until stopping stage \(\tau\) is same as the distribution of Traj under policy \(\bar{\pi}\) until stopping stage \(\tau\). Thus,

\[2\beta\bigg{|}\operatorname*{\mathbb{E}}_{\operatorname*{\mathbb{T} \mathrm{z}}\sim\mathbb{P}_{\pi^{0},s,a

[MISSING_PAGE_FAIL:26]

for the operator norm,

\[\left\|X-Y\right\|_{\text{op}} =\left\|\sum_{i=1}^{d}x_{i}x_{i}^{\top}-y_{i}y_{i}^{\top}\right\|_{ \text{op}}\] \[=\left\|\sum_{i=1}^{d}(x_{i}-y_{i})(x_{i}-y_{i})^{\top}+y_{i}(x_{i }-y_{i})^{\top}+(x_{i}-y_{i})y_{i}^{\top}\right\|_{\text{op}}\] \[\leq\sum_{i=1}^{d}\left\|(x_{i}-y_{i})(x_{i}-y_{i})^{\top}\right\| _{\text{op}}+\left\|y_{i}(x_{i}-y_{i})^{\top}\right\|_{\text{op}}+\left\|(x_{ i}-y_{i})y_{i}^{\top}\right\|_{\text{op}}\] \[\leq\sum_{i=1}^{d}\lVert(x_{i}-y_{i})\rVert_{2}\lVert(x_{i}-y_{i })\rVert_{2}+\lVert y_{i}\rVert_{2}\lVert(x_{i}-y_{i})\rVert_{2}+\lVert(x_{i }-y_{i})\rVert_{2}\lVert y_{i}\rVert_{2}\] \[\leq\sum_{i=1}^{d}\xi^{2}+\frac{2\xi}{\sqrt{\lambda}}=d\xi^{2}+ \frac{2d\xi}{\sqrt{\lambda}}\,.\]

Then, for any \(u\in\mathcal{B}(L_{1})\)

\[\left|\left\|u\right\|_{X}^{2}-\left\|u\right\|_{Y}^{2}\right|=\left|u^{\top} (X-Y)u\right|\leq\left\|u\right\|_{2}^{2}\lVert X-Y\rVert_{\text{op}}\leq L_{ 1}^{2}\bigg{(}d\xi^{2}+\frac{2d\xi}{\sqrt{\lambda}}\bigg{)}\,,\]

which implies that (since for non-negative \(a,b,\sqrt{a+b}\leq\sqrt{a}+\sqrt{b}\))

\[\left\|u\right\|_{X} \leq\sqrt{\left\|u\right\|_{Y}^{2}+L_{1}^{2}\bigg{(}d\xi^{2}+ \frac{2d\xi}{\sqrt{\lambda}}\bigg{)}}\leq\left\|u\right\|_{Y}+\sqrt{L_{1}^{2} \bigg{(}d\xi^{2}+\frac{2d\xi}{\sqrt{\lambda}}\bigg{)}},\] \[\left\|u\right\|_{Y} \leq\sqrt{\left\|u\right\|_{X}^{2}+L_{1}^{2}\bigg{(}d\xi^{2}+ \frac{2d\xi}{\sqrt{\lambda}}\bigg{)}}\leq\left\|u\right\|_{X}+\sqrt{L_{1}^{2} \bigg{(}d\xi^{2}+\frac{2d\xi}{\sqrt{\lambda}}\bigg{)}}\,.\]

Thus, for any \(u\in\mathcal{B}(L_{1})\)

\[\left\|u\right\|_{X}-\left\|u\right\|_{Y}|\leq L_{1}\sqrt{d\xi^{2}+\frac{2d \xi}{\sqrt{\lambda}}}\,.\] (64)

Eq. (64) is the first useful result that we alluded to at the beginning of the proof.

Now, we will show the second useful result. For any \(Y\in\mathbb{Y}\) and \(h\in[H]\), define the event

\[\mathcal{E}_{3}^{Y,h}=\left\{\left|\underset{(S,A)\sim\mu_{h}}{\mathbb{E}} \min\{1,\left\|\phi(S,A)\right\|_{Y}\}-\frac{1}{n}\sum_{j\in[n]}\min\Bigl{\{} 1,\left\|\phi_{h}^{j}\right\|_{Y}\Bigr{\}}\right|\leq\frac{1}{\sqrt{n}}\sqrt{ \log\biggl{(}\frac{6H|\mathbb{Y}|}{\delta}\biggr{)}}\right\}.\]

Since \(\min\{1,\left\|u\right\|_{Y}\}\in[0,1]\) for all \(u\in\mathcal{B}(L_{1}),Y\in\mathbb{Y}\), we can use Hoeffding's inequality (Lemma J.1) to get that, for any \(Y\in\mathbb{Y},h\in[H]\), event \(\mathcal{E}_{3}^{Y,h}\) occurs with probability at least \(1-\delta/(3H|\mathbb{Y}|)\). Let

\[\mathcal{E}_{3}=\bigcap_{Y\in\mathbb{Y},h\in[H]}\mathcal{E}_{3}^{Y,h}\,.\] (65)

Then, by applying a union bound over \(Y,h\) we have that the event \(\mathcal{E}_{3}\) occurs with probability at least \(1-\delta/3\), and under event \(\mathcal{E}_{3}\), for all \(Y\in\mathbb{Y},h\in[H]\), it holds that

\[\underset{(S,A)\sim\mu_{h}}{\mathbb{E}}\min\{1,\left\|\phi(S,A)\right\|_{Y}\} \leq\frac{1}{n}\sum_{j\in[n]}\min\Bigl{\{}1,\left\|\phi_{h}^{j}\right\|_{Y} \Bigr{\}}+\frac{1}{\sqrt{n}}\sqrt{\log\left(\frac{6H|\mathbb{Y}|}{\delta} \right)}\,.\] (66)

Eq. (66) is the second useful result that we alluded to at the beginning of the proof.

Now, we turn to proving Lemma G.3. Let \(h\in[H]\). Let \(X\in\mathbb{X}\), and select \(Y\in\mathbb{Y}\) such that, for any \(u\in\mathcal{B}(L_{1})\)

\[\left|\left\|u\right\|_{X}-\left\|u\right\|_{Y}\right|\leq L_{1} \sqrt{d\xi^{2}+\frac{2d\xi}{\sqrt{\lambda}}}\,,\] (67)

which we know exists by Eq. (64). By using Eq. (67) we get that

\[\mathop{\mathbb{E}}_{(S,A)\sim\mu_{h}}\min\{1,\left\|\phi(S,A) \right\|_{X}\}\leq\mathop{\mathbb{E}}_{(S,A)\sim\mu_{h}}\min\{1,\left\|\phi(S, A)\right\|_{Y}\}+L_{1}\sqrt{d\xi^{2}+\frac{2d\xi}{\sqrt{\lambda}}}\,.\] (68)

To bound the first term on the RHS in Eq. (68) we can use Eq. (66), to get that under event \(\mathcal{E}_{3}\)

\[\mathop{\mathbb{E}}_{(S,A)\sim\mu_{h}}\min\{1,\left\|\phi(S,A) \right\|_{Y}\}\leq\frac{1}{n}\sum_{j\in[n]}\min\Bigl{\{}1,\left\|\phi_{h}^{j} \right\|_{Y}\Bigr{\}}+\frac{1}{\sqrt{n}}\sqrt{\log\!\left(\frac{6H|\mathbb{Y} |}{\delta}\right)}\,.\] (69)

We can bound the first term on the RHS of Eq. (69), by again using Eq. (67), to get that

\[\frac{1}{n}\sum_{j\in[n]}\min\Bigl{\{}1,\left\|\phi_{h}^{j} \right\|_{Y}\Bigr{\}}\leq\frac{1}{n}\sum_{j\in[n]}\min\Bigl{\{}1,\left\|\phi_{ h}^{j}\right\|_{X}\Bigr{\}}+L_{1}\sqrt{d\xi^{2}+\frac{2d\xi}{\sqrt{\lambda}}}\,.\] (70)

Then, by Jensen's inequality we have that

\[\frac{1}{n}\sum_{j\in[n]}\min\Bigl{\{}1,\left\|\phi_{h}^{j} \right\|_{X}\Bigr{\}}=\sqrt{\left(\frac{1}{n}\sum_{j\in[n]}\min\Bigl{\{}1, \left\|\phi_{h}^{j}\right\|_{X}\Bigr{\}}\right)^{2}}\leq\sqrt{\frac{1}{n} \sum_{j\in[n]}\min\Bigl{\{}1,\left\|\phi_{h}^{j}\right\|_{X}^{2}\Bigr{\}}}\,.\] (71)

Putting Eqs. (68) to (71) together and noting that \(X,h\) were arbitrary, we get that, under event \(\mathcal{E}_{3}\), for any \(X\in\mathbb{X},h\in[H]\), it holds that

\[\mathop{\mathbb{E}}_{(S,A)\sim\mu_{h}}\min\{1,\left\|\phi(S,A) \right\|_{X}\}\leq\sqrt{\frac{1}{n}\sum_{j\in[n]}\min\!\left\{1,\left\|\phi_{ h}^{j}\right\|_{X}^{2}\right\}}+2L_{1}\sqrt{d\xi^{2}+\frac{2d\xi}{\sqrt{\lambda}}}+ \frac{1}{\sqrt{n}}\sqrt{\log\!\left(\frac{6H|\mathbb{Y}|}{\delta}\right)}\,.\]

We can now introduce \(X_{h}\) and make use of the above result. Notice that for any \(h\in[H],X_{h}^{-1}=(\lambda I+\sum_{j\in[n]}\phi_{h}^{j}(\phi_{h}^{j})^{\top} )^{-1}\) is such that \(\lambda_{\text{max}}(X_{h}^{-1})\leq 1/\lambda\), since \(\lambda_{\text{min}}(X_{h})\geq\lambda\). Thus, \(X_{h}^{-1}\in\mathbb{X}\). For any \(t\in[n],h\in[H]\), define \(X_{t,h}=\lambda I+\sum_{j\in[t]}\phi_{h}^{j}(\phi_{h}^{j})^{\top}\), and notice that \(X_{n,h}^{-1}=X_{h}^{-1}\), and that \(X_{t,h}^{-1}-X_{n,h}^{-1}\) is positive semidefinite. This implies that, for all \(h\in[H]\)

\[\frac{1}{n}\sum_{j\in[n]}\min\!\left\{1,\left\|\phi_{h}^{j}\right\| _{X_{h}^{-1}}^{2}\right\}\leq\frac{1}{n}\sum_{j\in[n]}\min\!\left\{1,\left\| \phi_{h}^{j}\right\|_{X_{j-1,h}^{-1}}^{2}\right\}.\]

Now, we can use the elliptical potential lemma (Lemma J.4), to conclude that, for all \(h\in[H]\)

\[\frac{1}{n}\sum_{j\in[n]}\min\!\left\{1,\left\|\phi_{h}^{j}\right\| _{X_{j-1,h}^{-1}}^{2}\right\}\leq\frac{2d}{n}\log\!\left(\frac{d\lambda+nL_{1} ^{2}}{d\lambda}\right).\]

Putting everything together, we get that, under event \(\mathcal{E}_{3}\), for all \(h\in[H]\), it holds that

\[\mathop{\mathbb{E}}_{(S,A)\sim\mu_{h}}\min\!\left\{1,\left\|\phi(S,A)\right\|_{X_{h}^{-1}}\right\}\] \[\leq 2L_{1}\sqrt{d\xi^{2}+\frac{2d\xi}{\sqrt{\lambda}}}+\frac{1}{ \sqrt{n}}\sqrt{\log\!\left(\frac{6H|\mathbb{Y}|}{\delta}\right)}+\sqrt{\frac{2d }{n}\log\!\left(\frac{d\lambda+nL_{1}^{2}}{d\lambda}\right)}\] \[\leq 2L_{1}\sqrt{d\xi^{2}+\frac{2\xi\tilde{L_{2}}}{H^{3/2}}}+\frac{1} {\sqrt{n}}\sqrt{\log\!\left(\frac{3H(1+2\tilde{L_{2}}^{2}/\xi)^{d^{2}}}{\delta }\right)}+\sqrt{\frac{2d}{n}\log\!\left(\frac{d\lambda+nL_{1}^{2}}{d\lambda} \right)}\] \[=\frac{\sqrt{d}}{\sqrt{n}}+\frac{1}{\sqrt{n}}\sqrt{d^{2}\log\! \left(1+16nL_{1}^{2}\tilde{L_{2}}^{3}\right)+\log\!\left(\frac{3H}{\delta} \right)}+\sqrt{\frac{2d}{n}\log\!\left(\frac{d\lambda+nL_{1}^{2}}{d\lambda} \right)}\] \[=\check{\epsilon}=\tilde{\mathcal{O}}\!\left(d/\sqrt{n}\right).\] (72)The second inequality used that \(|\mathbb{Y}|=(1+2/(\lambda\xi))^{d^{2}}\). The first equality holds by setting \(\xi^{-1}=8\tilde{L_{2}}L_{1}^{2}n\). The last equality holds by plugging in parameter values according to Appendix A. 

**Lemma G.4**.: _If Assumption 3 holds, then for any non-negative function \(f:\mathcal{S}\times\mathcal{A}\to[0,\infty)\), for any admissible distribution \(\nu=(\nu_{t})_{t\in[H]}\), and for any \(h\in[H]\), it holds that_

\[\operatorname*{\mathbb{E}}_{(S,A)\sim\nu_{h}}f(S,A)\leq C_{\text{\sf conc}} \operatorname*{\mathbb{E}}_{(S,A)\sim\mu_{h}}f(S,A)\,.\]

Proof.: Let \(f:\mathcal{S}\times\mathcal{A}\to[0,\infty)\) be any non-negative function. Let \(h\in[H]\) be any stage. Then,

\[\operatorname*{\mathbb{E}}_{(S,A)\sim\nu_{h}}f(S,A) =\int_{z\in\mathcal{S}_{h}\times\mathcal{A}}f(z)\nu_{h}(z)dz\] \[=\int_{z\in\mathcal{S}_{h}\times\mathcal{A}}f(z)\frac{\nu_{h}(z )}{\mu_{h}(z)}\mu_{h}(z)dz\] \[\leq\int_{z\in\mathcal{S}_{h}\times\mathcal{A}}f(z)C_{\text{\sf conc }}\mu_{h}(z)dz\] \[=C_{\text{\sf conc}}\operatorname*{\mathbb{E}}_{(S,A)\sim\mu_{h} }f(S,A)\,,\]

where the inequality holds by applying Assumption 3, and noting that \(f\) is non-negative. This implies the desired result, since \(f\) and \(h\) were arbitrary.

Lemmas Related to Least-squares

**Lemma H.1** (Least-squares Error Decomposition).: _Let \(\lambda>0,\theta_{\star}\in\mathbb{R}^{d}\) and \(n\in\mathbb{N}^{+}\), For all \(k\in[n]\), let_

\[A_{k}\in\mathbb{R}^{d},\ \gamma_{k}\in\mathbb{R},\ \tilde{Y}_{k}= \langle A_{k},\theta_{\star}\rangle+\gamma_{k},\ Y_{k}=\tilde{Y}_{k}+\Delta_{k},\] \[V=\lambda I+\sum_{t=1}^{n}A_{t}A_{t}^{\top},\ \hat{\theta}=V^{-1}\sum_{t=1}^{n}A_{t}Y_{t},\ \iota=\sum_{t=1}^{n}A_{t}\gamma_{t},\ \Delta=(\Delta_{t})_{t\in[n]}\,.\]

_Then,_

\[\left\|\hat{\theta}-\theta_{\star}\right\|_{V}\leq\sqrt{\lambda}\|\theta_{ \star}\|_{2}+\left\|\Delta\right\|_{\infty}\sqrt{n}+\left\|\iota\right\|_{V^{ -1}}.\]

Proof.: We begin by decomposing the targets used in \(\hat{\theta}\) as follows

\[\hat{\theta} =V^{-1}\sum_{t=1}^{n}A_{t}Y_{t}\] \[=V^{-1}\sum_{t=1}^{n}A_{t}(\langle A_{t},\theta_{\star}\rangle+ \gamma_{t}+\Delta_{t})\] \[=\left(\lambda I+\sum_{t=1}^{n}A_{t}A_{t}^{\top}\right)^{-1}\left( \sum_{t=1}^{n}A_{t}A_{t}^{\top}\theta_{\star}+\lambda I\theta_{\star}-\lambda I \theta_{\star}\right)+V^{-1}\sum_{t=1}^{n}A_{t}(\gamma_{t}+\Delta_{t})\] \[=\theta_{\star}+\lambda V^{-1}\theta_{\star}+V^{-1}\sum_{t=1}^{n} A_{t}\gamma_{t}+V^{-1}\sum_{t=1}^{n}A_{t}\Delta_{t}\,.\]

Then, subtracting \(\theta_{\star}\) from both sides and taking the matrix \(V\) weighted norm of both sides gives us that

\[\left\|\hat{\theta}-\theta_{\star}\right\|_{V} =\left\|\lambda V^{-1}\theta_{\star}+V^{-1}\sum_{t=1}^{n}A_{t} \gamma_{t}+V^{-1}\sum_{t=1}^{n}A_{t}\Delta_{t}\right\|_{V}\] \[\leq\lambda\|\theta_{\star}\|_{V^{-1}}+\left\|\sum_{t=1}^{n}A_{t} \gamma_{t}\right\|_{V^{-1}}+\left\|\sum_{t=1}^{n}A_{t}\Delta_{t}\right\|_{V^{ -1}}\] \[\leq\frac{\lambda}{\lambda_{\text{min}}(V)}\|\theta_{\star}\|_{2 }+\left\|\iota\right\|_{V^{-1}}+\left\|\Delta\right\|_{\infty}\sqrt{n}\] \[\leq\sqrt{\lambda}\|\theta_{\star}\|_{2}+\left\|\iota\right\|_{V ^{-1}}+\left\|\Delta\right\|_{\infty}\sqrt{n}\,,\]

where the second inequality used that \(\left\|V^{-1}\right\|\leq\lambda_{\text{max}}(V^{-1})=1/\lambda_{\text{min}}(V)\) and Lemma J.5 to bound \(\left\|\sum_{t=1}^{n}A_{t}\Delta_{t}\right\|_{V^{-1}}\). 

**Lemma H.2** (Least-squares Noise Bound).: _There is an event \(\mathcal{E}_{1}\), which occurs with probability at least \(1-\delta/3\), such that under event \(\mathcal{E}_{1}\), for all \(h\in[H]\), for all \(G\in\mathbf{G}\), and \(\theta_{h+1:H+1}\in\mathcal{B}(\tilde{\mathcal{L}_{2}})^{H-h+1}\), it holds that_

\[\left\|\sum_{j\in[n]}\phi_{h}^{j}\left(\underset{\tau\sim F_{G,h+1}^{j}}{ \mathbb{E}}\Big{[}r_{h:\tau-1}^{j}+\bar{v}_{\theta_{h+1:H+1}}\big{(}s_{\tau}^ {j}\big{)}\Big{]}-\underset{\text{\rm Traj}\sim\mathbb{P}_{\pi^{0},s_{h}^{j}, \kappa_{h}^{j}}}{\mathbb{E}}\underset{\tau\sim F_{G,\text{\rm Imj},h+1}}{ \mathbb{E}}\big{[}R_{h:\tau-1}+\bar{v}_{\theta_{h+1:H+1}}(S_{\tau})\big{]} \right)\right\|_{X_{h}^{-1}}\leq\bar{\beta}\,,\]

_where \(\bar{\beta}\) is defined in Eq. (31)._

Proof.: We begin the proof by showing two useful results (namely Eq. (74) and Eq. (75)), which will be needed later in the proof.

By Lemma J.2, we know there exists a set \(C_{\xi}\subset\mathcal{B}(a),a,\xi>0\) with \(|C_{\xi}|=(1+2a/\xi)^{d}\) such that for any \(x\in\mathcal{B}(a)\) there exists a \(y\in C_{\xi}\) such that \(\left\|x-y\right\|_{2}\leq\xi\). Define the set 

[MISSING_PAGE_FAIL:31]

Now with all of the above results in hand, we turn to finally proving Lemma H.2. Let \(G\in\mathbf{G}\) as in the lemma statement. Then, by Eq. (74) we know that there exists a \((\tilde{G},\theta_{\widetilde{\alpha}:H+1}^{\sim})\in C_{\xi}^{\mathcal{G}} \times C_{\xi}^{\Theta}\) such that, for any \(h\in[H]\), for any \(u\in[h]\) and trajectory \(\text{traj}=(s_{t},a_{t},r_{t})\in[u:H+1]\), it holds that

\[\left|\underset{\tau\sim F_{G,\text{\tiny{min}},h+1}}{\mathbb{E}}\big{[}r_{h: \tau-1}+\bar{v}_{\theta_{h+1:H+1}}(s_{\tau})\big{]}-\underset{\tau\sim F_{G, \text{\tiny{min}},h+1}}{\mathbb{E}}\Big{[}r_{h:\tau-1}+\bar{v}_{\theta_{h+1:H +1}}(s_{\tau})\Big{]}\right|\leq 7\sqrt{2d}H^{2}L_{1}\xi/\alpha\,.\] (76)

Let \(\tilde{G},\theta_{2:H+1}^{\sim}\) be as defined above. Then, for any \(h\in[H]\), by using the triangle inequality we can write

\[\left\|\sum_{j\in[n]}\phi_{h}^{j}\bigg{(}\underset{\tau\sim F_{G, h+1}^{j}}{\mathbb{E}}\Big{[}r_{h:\tau-1}^{j}+\bar{v}_{\theta_{h+1:H+1}}\big{(}s_{ \tau}^{j}\big{)}\Big{]}-\underset{\text{Traj}\sim\mathbb{P}_{u^{0},s^{j}_{h},a^{j}_{h}}}{\mathbb{E}}\underset{\tau\sim F_{G,\text{\tiny{min}},h+1}}{ \mathbb{E}}\big{[}R_{h:\tau-1}+\bar{v}_{\theta_{h+1:H+1}}(S_{\tau})\big{]} \bigg{)}\right\|_{X_{h}^{-1}}\] \[\leq\left\|\sum_{j\in[n]}\phi_{h}^{j}\bigg{(}\underset{\tau\sim F _{G,h+1}^{j}}{\mathbb{E}}\Big{[}r_{h:\tau-1}^{j}+\bar{v}_{\theta_{h+1:H+1}} \big{(}s_{\tau}^{j}\big{)}\Big{]}-\underset{\text{Traj}\sim\mathbb{P}_{u^{0}, s^{j}_{h},a^{j}_{h}}}{\mathbb{E}}\underset{\tau\sim F_{G,\text{\tiny{min}},h+1}}{ \mathbb{E}}\Big{[}R_{h:\tau-1}+\bar{v}_{\theta_{h+1:H+1}}(S_{\tau})\Big{]} \bigg{)}\right\|_{X_{h}^{-1}}\] \[\quad+\left\|\sum_{j\in[n]}\phi_{h}^{j}\bigg{(}\underset{\tau\sim F _{G,h+1}^{j}}{\mathbb{E}}\Big{[}r_{h:\tau-1}^{j}+\bar{v}_{\theta_{h+1:H+1}} \big{(}s_{\tau}^{j}\big{)}\Big{]}-\underset{\tau\sim F_{G,\text{\tiny{min}},h +1}}{\mathbb{E}}\Big{[}r_{h:\tau-1}^{j}+\bar{v}_{\theta_{h+1:H+1}}(s_{\tau}^{ j})\Big{]}\bigg{)}\right\|_{X_{h}^{-1}}\] \[\quad+\left\|\sum_{j\in[n]}\phi_{h}^{j}\bigg{(}\underset{\text{ Traj}\sim\mathbb{P}_{u^{0},s^{j}_{h},a^{j}_{h}}}{\mathbb{E}}\Big{[}\underset{ \tau\sim F_{G,\text{\tiny{min}},h+1}}{\mathbb{E}}\Big{[}R_{h:\tau-1}+\bar{v}_ {\theta_{h+1:H+1}}(S_{\tau})\Big{]}-\underset{\tau\sim F_{G,\text{\tiny{min}},h+1}}{\mathbb{E}}\big{[}R_{h:\tau-1}+\bar{v}_{\theta_{h+1:H+1}}(S_{\tau}) \big{]}\bigg{]}\right)\right\|_{X_{h}^{-1}}.\]

The first term can be bounded by Eq. (75), if we are under event \(\mathcal{E}_{1}\). For the second and third term we make use of Lemma J.5, which ensures that for any sequence \((b_{j})_{j\in[n]}\) such that \(|b_{j}|\leq c\in\mathbb{R}\) the following holds

\[\left\|\sum_{j\in[n]}\phi_{h}^{j}b_{j}\right\|_{X_{h}^{-1}}\leq c\sqrt{n}\,.\]

For the second term and third term the respective \(b_{j}\) terms can be bounded by using Eq. (76), giving us \(c=7\sqrt{2d}H^{2}L_{1}\xi/\alpha\).

Putting the above three bounds together we have that under event \(\mathcal{E}_{1}\), which occurs with probability at least \(1-\delta/3\), for all \(h\in[H]\), for all \(G\in\mathbf{G}\), and \(\theta_{h+1:H+1}\in\mathcal{B}(\tilde{L}_{2})^{H-h+1}\), it holds that

\[\left\|\sum_{j\in[n]}\phi_{h}^{j}\bigg{(}\underset{\tau\sim F_{G,h+1}^{j}}{\mathbb{E}}\Big{[}r_{h:\tau-1}^{j}+\bar{v}_{\theta_{h+1:H+1}} \big{(}s_{\tau}^{j}\big{)}\Big{]}-\underset{\text{Traj}\sim\mathbb{P}_{u^{0},s^ {j}_{h},a^{j}_{h}}}{\mathbb{E}}\underset{\tau\sim F_{G,\text{\tiny{min}},h+1}}{ \mathbb{E}}\big{[}R_{h:\tau-1}+\bar{v}_{\theta_{h+1:H+1}}(S_{\tau})\big{]} \bigg{)}\right\|_{X_{h}^{-1}}\] \[\leq\sqrt{2H^{2}\log\bigg{(}\frac{3H(1+2L_{2}\tilde{L_{2}}/\xi)^{ dH(d_{0}+1)}}{\delta}\bigg{)}}+\log\bigg{(}\sqrt{\frac{\det(X_{h})}{\det(\lambda I )}}\bigg{)}+14\sqrt{n}\sqrt{2d}H^{2}L_{1}\xi/\alpha\] \[=H\sqrt{2dH(d_{0}+1)\log\Bigl{(}1+2L_{2}\tilde{L_{2}}/\xi\Bigr{)} +\log(\det(X_{h}))-d\log(\lambda)+\log\biggl{(}\frac{3H}{\delta}\bigg{)}}+14 \sqrt{n}\sqrt{2d}H^{2}L_{1}\xi/\alpha\] \[\leq H\sqrt{2dH(d_{0}+1)\log\Bigl{(}1+2L_{2}\tilde{L_{2}}/\xi \Bigr{)}+d\log(\lambda+nL_{1}^{2}/d)-d\log(\lambda)+\log\biggl{(}\frac{3H}{ \delta}\bigg{)}}+14\sqrt{n}\sqrt{2d}H^{2}L_{1}\xi/\alpha\,,\]

where the last inequality used the Determinant-Trace Inequality (see Lemma 10 in [1]). Setting \(\xi^{-1}=14\sqrt{n}\sqrt{2d}H^{2}L_{1}\alpha^{-1}\), we get that the above display is

\[\leq H\sqrt{2dH(d_{0}+1)\log\Bigl{(}1+28\sqrt{2d}H^{2}L_{2}\tilde{L_{2}}L_{1} \alpha^{-1}\Bigr{)}}+d\log(\lambda+nL_{1}^{2}/d)-d\log(\lambda)+\log\biggl{(} \frac{3H}{\delta}\bigg{)}+1\] \[=\bar{\beta}=\tilde{\mathcal{O}}\Big{(}H^{3/2}d\Big{)}\,.\] (77)

The last equality holds by plugging in parameter values according to Appendix A.

Lemmas Related to Covering \(\mathbf{G}\)

**Lemma I.1**.: _There is an event \(\mathcal{E}_{2}\), that occurs with probability at least \(1-\delta/3\), such that under event \(\mathcal{E}_{2}\), for all \(G\in\mathbf{G}\), and for all \(h\in[H]\), it holds that_

\[\left|\underset{(S,A)\sim\mu_{h}}{\mathbb{E}}\!\!\left[\max_{ \theta\in\Theta_{G,h}}\bar{q}_{\theta}(S,A)-\min_{\theta\in\Theta_{G,h}}\bar{ q}_{\theta}(S,A)\right]-\frac{1}{n}\sum_{i\in[n]}\!\left(\max_{\theta\in\Theta_{G,h}} \bar{q}_{\theta}(s^{i}_{h},a^{i}_{h})-\min_{\theta\in\Theta_{G,h}}\bar{q}_{ \theta}(s^{i}_{h},a^{i}_{h})\right)\right|\] \[\leq\frac{H}{\sqrt{n}}\sqrt{\log\!\left(\frac{6H|C^{\mathbf{G}}_{ \xi}|}{\delta}\right)}+2L_{\xi}\,,\]

_where \(|C^{\mathbf{G}}_{\xi}|,L_{\xi}\), are defined in Eqs. (33) and (34)._

Proof.: Let \(G\in\mathbf{G}\) be a feasible solution to Optimization Problem 1. By the first result in Lemma 1.2, there exists a set \(C^{\mathbf{G}}_{\xi}\subset\mathbf{G}\) such that, there exists a \(\tilde{G}\in C^{\mathbf{G}}_{\xi}\) such that, for any \(h\in[H],(s,a)\in\mathcal{S}_{h}\times\mathcal{A}\), it holds that

\[\left|\left(\max_{\theta\in\Theta_{G,h}}\bar{q}_{\theta}(s,a)-\min_{\theta\in \Theta_{G,h}}\bar{q}_{\theta}(s,a)\right)-\left(\max_{\theta\sim\Theta_{G,h}} \bar{q}_{\theta^{\sim}}(s,a)-\min_{\theta\in\Theta_{G,h}}\bar{q}_{\theta^{ \sim}}(s,a)\right)\right|\leq L_{\xi}\,.\] (78)

Select \(\tilde{G}\) as defined above. Let \(h\in[H]\). Using Eq. (78), we know that

\[\left|\underset{(S,A)\sim\mu_{h}}{\mathbb{E}}\!\!\left[\max_{\theta\in\Theta _{G,h}}\bar{q}_{\theta}(S,A)-\min_{\theta\in\Theta_{G,h}}\bar{q}_{\theta}(S,A) \right]-\underset{(S,A)\sim\mu_{h}}{\mathbb{E}}\!\!\left[\max_{\theta\in\Theta _{G,h}}\bar{q}_{\theta^{\sim}}(S,A)-\min_{\theta\in\Theta_{G,h}}\bar{q}_{ \theta^{\sim}}(S,A)\right]\right|\leq L_{\xi}\,.\] (79)

To bound the second term in the absolute value of Eq. (79) to its empirical mean, we can use the second result in Lemma I.2, which gives us that under event \(\mathcal{E}_{2}\)

\[\left|\underset{(S,A)\sim\mu_{h}}{\mathbb{E}}\!\!\left[\max_{ \theta^{\sim}\in\Theta_{G,h}}\bar{q}_{\theta^{\sim}}(S,A)-\min_{\theta^{\sim} \in\Theta_{G,h}}\bar{q}_{\theta^{\sim}}(S,A)\right]-\frac{1}{n}\sum_{i\in[n]} \!\left(\max_{\theta^{\sim}\in\Theta_{G,h}}\bar{q}_{\theta^{\sim}}(s^{i}_{h},a^{i}_{h})-\min_{\theta^{\sim}\in\Theta_{G,h}}\bar{q}_{\theta^{\sim}}(s^{i}_{h },a^{i}_{h})\right)\right|\] \[\leq\frac{H}{\sqrt{n}}\sqrt{\log\!\left(\frac{6H|C^{\mathbf{G}}_{ \xi}|}{\delta}\right)}\,.\] (80)

We can relate the \(\tilde{G}\) in the second term of the absolute value in Eq. (80) back to \(G\), by once again using Eq. (78), to get that

\[\left|\frac{1}{n}\sum_{i\in[n]}\!\left(\max_{\theta^{\sim}\in \Theta_{G,h}}\bar{q}_{\theta^{\sim}}(s^{i}_{h},a^{i}_{h})-\min_{\theta^{\sim} \in\Theta_{G,h}}\bar{q}_{\theta^{\sim}}(s^{i}_{h},a^{i}_{h})\right)-\frac{1}{ n}\sum_{i\in[n]}\!\left(\max_{\theta\in\Theta_{G,h}}\bar{q}_{\theta}(s^{i}_{h},a^{i}_{h})- \min_{\theta\in\Theta_{G,h}}\bar{q}_{\theta}(s^{i}_{h},a^{i}_{h})\right)\right|\] \[\leq L_{\xi}\,.\] (81)

Putting together Eqs. (79) to (81), and noting that \(h\) was arbitrary, gives the desired result. 

**Lemma I.2**.: _Let \(\xi>0\). There exists a set \(C^{\mathbf{G}}_{\xi}\subset\mathbf{G}\) such that, for any \(G\in\mathbf{G}\), there exists a \(\tilde{G}\in C^{\mathbf{G}}_{\xi}\) such that, for any \(h\in[H],(s,a)\in\mathcal{S}_{h}\times\mathcal{A}\), it holds that_

\[\left|\left(\max_{\theta\in\Theta_{G,h}}\bar{q}_{\theta}(s,a)-\min_{\theta\in \Theta_{G,h}}\bar{q}_{\theta}(s,a)\right)-\left(\max_{\theta^{\sim}\in\Theta_ {G,h}}\bar{q}_{\theta^{\sim}}(s,a)-\min_{\theta^{\sim}\in\Theta_{\tilde{G},h}} \bar{q}_{\theta^{\sim}}(s,a)\right)\right|\leq L_{\xi}\,,\]

_where \(|C^{\mathbf{G}}_{\xi}|,L_{\xi}\), are defined in Eqs. (33) and (34). Furthermore, there is an event \(\mathcal{E}_{2}\), which occurs with probability at least \(1-\delta/3\), such that under event \(\mathcal{E}_{2}\), for any \(\tilde{G}\in C^{\mathbf{G}}_{\xi}\), and \(h\in[H]\), it holds that_

\[\left|\underset{(S,A)\sim\mu_{h}}{\mathbb{E}}\!\!\left[\max_{ \theta^{\sim}\in\Theta_{\tilde{G},h}}\bar{q}_{\theta^{\sim}}(S,A)-\min_{ \theta^{\sim}\in\Theta_{\tilde{G},h}}\bar{q}_{\theta^{\sim}}(S,A)\right]- \frac{1}{n}\sum_{i\in[n]}\!\left(\max_{\theta^{\sim}\in\Theta_{\tilde{G},h}} \bar{q}_{\theta^{\sim}}(s^{i}_{h},a^{i}_{h})-\min_{\theta^{\sim}\in\Theta_{ \tilde{G},h}}\bar{q}_{\theta^{\sim}}(s^{i}_{h},a^{i}_{h})\right)\right|\] \[\leq\frac{H}{\sqrt{n}}\sqrt{\log\!\left(\frac{6H|C^{\mathbf{G}}_{ \xi}|}{\delta}\right)}\,.\]Proof.: Let \(\xi>0,\kappa_{t}\geq 0,\forall t\in[2:H+1]\). By Lemma I.4, there exists a set \(C_{\xi}^{\mathbf{G}}\subset\mathbf{G}\) with \(|C_{\xi}^{\mathbf{G}}|\leq(1+2L_{2}/\xi))^{dHd_{0}}\) such that, for any \(G\in\mathbf{G}\), there exists a \(\tilde{G}\in C_{\xi}^{\mathbf{G}}\) such that, for any \(h\in[H]\), for any \(\theta_{h+1:H+1},\theta_{h+1:H+1}^{\sim}\in\mathcal{B}(\tilde{L}_{2})^{H-h+1}\), such that for all \(t\in[h+1:H+1],s\in\mathcal{S}_{t},\left|\bar{v}_{\theta_{t}}(s)-\bar{v}_{\bar{v }_{\bar{v}_{\bar{v}}}}(s)\right|\leq\kappa_{t}\), and for any \(j\in[n]\), it holds that

\[\left|\mathop{\mathbb{E}}_{\tau\sim F_{G,h+1}^{j}}\left[r_{h:\tau -1}+\bar{v}_{\theta_{h+1:H+1}}(s_{\tau})\right]-\mathop{\mathbb{E}}_{\tau\sim F _{G,h+1}^{j}}\left[r_{h:\tau-1}+\bar{v}_{\theta_{h+1:H+1}^{\sim}}(s_{\tau}) \right]\right|\] \[\leq(H-h+1)6\sqrt{2d}HL_{1}\xi/\alpha+\sum_{t=h}^{H}\kappa_{t+1}\,.\] (82)

For the remainder of the proof let \(G,\tilde{G}\) be as described above.

We first show the following intermediate result.

**Lemma I.3**.: _For all \(h\in[H]\), it holds that:_

1. _For any_ \(\hat{\theta}_{h}\in\hat{\Theta}_{G,h},\theta_{h}\in\Theta_{G,h}\)_, there exists_ \(\hat{\theta}_{h}^{\sim}\in\hat{\Theta}_{\tilde{G},h},\theta_{h}^{\sim}\in \Theta_{\tilde{G},h}\) _such that_ \[\left\|\hat{\theta}_{h}-\hat{\theta}_{h}^{\sim}\right\|_{X_{h}} \leq c_{h}^{\xi},\quad\left\|\theta_{h}-\theta_{h}^{\sim}\right\|_{X_{h}}\leq c _{h}^{\xi}\,.\] (83)
2. _For any_ \(\hat{\theta}_{h}^{\sim}\in\hat{\Theta}_{\tilde{G},h},\theta_{h}^{\sim}\in \Theta_{\tilde{G},h}\)_, there exists_ \(\hat{\theta}_{h}\in\hat{\Theta}_{G,h},\theta_{h}\in\Theta_{G,h}\) _such that_ \[\left\|\hat{\theta}_{h}-\hat{\theta}_{h}^{\sim}\right\|_{X_{h}} \leq c_{h}^{\xi},\quad\left\|\theta_{h}-\theta_{h}^{\sim}\right\|_{X_{h}}\leq c _{h}^{\xi}\,,\] (84)

_where,_

\[c_{h}^{\xi}=6\sqrt{n}\sqrt{2d}H^{2}L_{1}\xi\alpha^{-1}\Big{(}1+ \sqrt{n}L_{1}\tilde{L_{2}}/(H^{3/2}d)\Big{)}^{H-h}\,.\] (85)

Proof.: **Proof of result \(1\).:** To show Eq. (83) we will use induction. The base case is when \(h=H\), for which

\[\hat{\Theta}_{G,H}=\hat{\Theta}_{\tilde{G},H}=\Bigg{\{}X_{H}^{-1} \sum_{j\in[n]}\phi_{H}^{j}r_{H}^{j}\Bigg{\}},\implies\Theta_{G,H}=\Theta_{ \tilde{G},H}\,.\]

Thus, for any \(\hat{\theta}_{H}\in\hat{\Theta}_{G,H},\theta_{H}\in\Theta_{G,H}\), select \(\hat{\theta}_{H}^{\sim}=\hat{\theta}_{H}\in\hat{\Theta}_{\tilde{G},H},\theta_ {H}^{\sim}=\theta_{H}\in\Theta_{\tilde{G},H}\). Then

\[\left\|\hat{\theta}_{H}-\hat{\theta}_{H}^{\sim}\right\|_{X_{H}} \leq 0,\quad\left\|\theta_{H}-\theta_{H}^{\sim}\right\|_{X_{H}}\leq 0\,.\]

Now, we show the inductive step. Let \(h\in[H-1]\) be arbitrary. Assume Eq. (83) holds for any \(t\in[h+1:H]\). We prove that Eq. (83) also holds for \(h\). Let \(\hat{\theta}_{h}\in\hat{\Theta}_{G,h}\) be arbitrary. Notice that \(\hat{\theta}_{h}\) must have the following form.

\[\hat{\theta}_{h}=X_{h}^{-1}\sum_{j\in[n]}\phi_{h}^{j}\mathop{ \mathbb{E}}_{\tau\sim F_{G,h+1}^{j}}\left[r_{h:\tau-1}^{j}+\bar{v}_{\theta_{h +1:H+1}}\big{(}s_{\tau}^{j}\big{)}\right]\in\hat{\Theta}_{G,h},\text{ for some }\theta_{h+1:H+1}\in\Theta_{G,h+1}\times\cdots\times\Theta_{G,H+1}\,.\]

Select \(\theta_{h+1:H+1}^{\sim}\in\Theta_{\tilde{G},h+1}\times\cdots\times\Theta_{ \tilde{G},H+1}\) such that for all \(t\in[h+1:H+1]\)

\[\left\|\theta_{t}-\theta_{t}^{\sim}\right\|_{X_{t}}\leq c_{t}^{ \xi}\,,\] (86)

which exists by the inductive hypothesis (Eq. (83)) and since \(\Theta_{G,H+1}=\Theta_{\tilde{G},H+1}=\{\vec{0}\}\). Define

\[\hat{\theta}_{h}^{\sim}=X_{h}^{-1}\sum_{j\in[n]}\phi_{h}^{j}\mathop{\mathbb{E}} _{\tau\sim F_{G,h+1}^{j}}\left[r_{h:\tau-1}^{j}+\bar{v}_{\theta_{h+1:H+1}^{\sim }}\big{(}s_{\tau}^{j}\big{)}\right]\in\hat{\Theta}_{\tilde{G},h}\,.\]Recall that we aim to bound \(\left\|\hat{\theta}_{h}-\hat{\theta}_{h}^{\sim}\right\|_{X_{h}}\). Plugging in the expressions for \(\hat{\theta}_{h},\hat{\theta}_{h}^{\sim}\), as defined above, we get that

\[\left\|\hat{\theta}_{h}-\hat{\theta}_{h}^{\sim}\right\|_{X_{h}} =\left\|X_{h}^{-1}\sum_{j\in[n]}\phi_{h}^{j}\Bigg{(}\underset{\tau \sim F_{G,h+1}^{j}}{\mathbb{E}}\Big{[}r_{h:\tau-1}^{j}+\bar{v}_{\theta_{h+1:H+ 1}}\big{(}s_{\tau}^{j}\big{)}\Big{]}-\underset{\tau\sim F_{G,h+1}^{j}}{\mathbb{ E}}\Big{[}r_{h:\tau-1}^{j}+\bar{v}_{\theta_{h+1:H+1}}\big{(}s_{\tau}^{j} \big{)}\Big{]}\right)\right\|_{X_{h}}\] \[=\left\|X_{h}^{-1}\sum_{j\in[n]}\phi_{h}^{j}b_{j}\right\|_{X_{h}} =\left\|\sum_{j\in[n]}\phi_{h}^{j}b_{j}\right\|_{X_{h}^{-1}}\,,\]

where in the second equality we let \(b_{j}=\mathbb{E}_{\tau\sim F_{G,h+1}^{j}}\Big{[}r_{h:\tau-1}^{j}+\bar{v}_{ \theta_{h+1:H+1}}\big{(}s_{\tau}^{j}\big{)}\Big{]}\)\(-\)\(\mathbb{E}_{\tau\sim F_{G,h+1}^{j}}\Big{[}r_{h:\tau-1}^{j}+\bar{v}_{\theta_{h+1:H+1}} \big{(}s_{\tau}^{j}\big{)}\Big{]}\). To bound the above term we can make use of Lemma J.5, which ensures that for any sequence \((b_{j})_{j\in[n]}\) such that \(|b_{j}|\leq a\in\mathbb{R}\) the following holds

\[\left\|\sum_{j\in[n]}\phi_{h}^{j}b_{j}\right\|_{X_{h}^{-1}}\leq a\sqrt{n}\,.\]

Thus, we are left to bound \(|b_{j}|\). To do so, we can make use of Eq. (82), which requires us to bound \(\left|\bar{v}_{\theta_{t}}(s)-\bar{v}_{\theta_{t}^{\sim}}(s)\right|\) for all \(t\in[h+1:H],s\in\mathcal{S}_{t}\), which can be done as follows.

\[\left|\bar{v}_{\theta_{t}}(s)-\bar{v}_{\theta_{t}^{\sim}}(s)\right| =\left|\operatorname{clip}_{[0,H]}\max_{a\in\mathcal{A}}\langle \phi(s,a),\theta_{t}\rangle-\operatorname{clip}_{[0,H]}\max_{a\in\mathcal{A}} \langle\phi(s,a),\theta_{t}^{\sim}\rangle\right|\] \[\leq\max_{a\in\mathcal{A}}\langle\phi(s,a),\theta_{t}-\theta_{t}^ {\sim}\rangle\Big{|}\] \[\leq\frac{L_{1}}{\sqrt{\lambda}}c_{t}^{\xi}=L_{1}\tilde{L_{2}}c_{ t}^{\xi}/(H^{3/2}d)\,.\] (87)

The second inequality uses the Cauchy-Schwarz inequality. The third inequality used Eq. (22), that \(\lambda_{\text{max}}(X_{h}^{-1})\leq 1/\lambda\) (by definition of \(X_{h}\) Eq. (11)), and Eq. (86). The last equality used that \(\sqrt{\lambda}=H^{3/2}d/\tilde{L_{2}}\) (Eq. (25)). Plugging Eq. (87) into Eq. (82) we get that for any \(j\in[n]\)

\[|b_{j}| =\left|\underset{\tau\sim F_{G,h+1}^{j}}{\mathbb{E}}\big{[}r_{h: \tau-1}+\bar{v}_{\theta_{h+1:H+1}}(s_{\tau})\big{]}-\underset{\tau\sim F_{G,h+ 1}^{j}}{\mathbb{E}}\Big{[}r_{h:\tau-1}+\bar{v}_{\theta_{h+1:H+1}}(s_{\tau}) \Big{]}\right|\] \[\leq(H-h+1)6\sqrt{2d}HL_{1}\xi/\alpha+\sum_{t=h}^{H}L_{1}\tilde{ L_{2}}c_{t+1}^{\xi}/(H^{3/2}d)\,.\]

Thus,

\[\left\|\hat{\theta}_{h}-\hat{\theta}_{h}^{\sim}\right\|_{X_{h}} =\left\|\sum_{j\in[n]}\phi_{h}^{j}b_{j}\right\|_{X_{h}^{-1}} \leq\Bigg{(}6\sqrt{2d}H^{2}L_{1}\xi\alpha^{-1}+\frac{L_{1}\tilde{ L_{2}}}{H^{3/2}d}\sum_{t=h}^{H}c_{t+1}^{\xi}\Bigg{)}\sqrt{n}\] \[=6\sqrt{n}\sqrt{2d}H^{2}L_{1}\xi\alpha^{-1}+\sqrt{n}\frac{L_{1} \tilde{L_{2}}}{H^{3/2}d}\sum_{t=h}^{H}c_{t+1}^{\xi}\] \[=6\sqrt{n}\sqrt{2d}H^{2}L_{1}\xi\alpha^{-1}\Big{(}1+\sqrt{n}L_{1} \tilde{L_{2}}/(H^{3/2}d)\Big{)}^{H-h}\] \[=c_{h}^{\xi}\,.\]

To see why the second last equality is true, let \(x=6\sqrt{n}\sqrt{2d}H^{2}L_{1}\xi\alpha^{-1}\) and \(y=\sqrt{n}L_{1}\tilde{L_{2}}/(H^{3/2}d)\). Then, for any \(t\in[h:H]\), \(c_{t}^{\xi}=x(1+y)^{H-t}\) (by Eq. (85)) and

\[6\sqrt{n}\sqrt{2d}H^{2}L_{1}\xi\alpha^{-1}+\sqrt{n}\frac{L_{1}\tilde{L_{2}}}{H^{ 3/2}d}\sum_{t=h}^{H}c_{t+1}^{\xi}=x+y\sum_{t=h+1}^{H}x(1+y)^{H-t}\,.\]Notice that the sum can be rewritten as a finite geometric series.

\[\sum_{t=h+1}^{H}x(1+y)^{H-t}=x\sum_{k=0}^{H-h-1}(1+y)^{k}=x\frac{(1+y)^{H-h}-1}{y}\,.\]

Thus,

\[x+y\sum_{t=h+1}^{H}x(1+y)^{H-t}=x+y\cdot x\frac{(1+y)^{H-h}-1}{y}=x(1+y)^{H-h}=c _{h}^{\xi}\,.\]

This proves the first result in Eq. (83).

Next, we show the second result in Eq. (83). Let \(\theta_{h}\in\Theta_{G,h}\). By the definition of the set \(\Theta_{G,h}\), there exists a \(\hat{\theta}_{h}\in\hat{\Theta}_{G,h}\) such that \(\left\|\theta_{h}-\hat{\theta}_{h}\right\|_{X_{h}}\leq\beta\). Then, by the first result in Eq. (83) (which we have shown holds for \(h\) above), there exists a \(\hat{\theta}_{h}^{\sim}\in\hat{\Theta}_{\tilde{G},h}\), such that \(\left\|\hat{\theta}_{h}-\hat{\theta}_{h}^{\sim}\right\|_{X_{h}}\leq c_{h}^{\xi}\). Let \(\hat{\theta}_{h},\hat{\theta}_{h}^{\sim}\) be as defined above, and select \(\theta_{h}^{\sim}=\theta_{h}-\hat{\theta}_{h}+\hat{\theta}_{h}^{\sim}\), which is an element of \(\Theta_{\tilde{G},h}\) since

\[\left\|\theta_{h}^{\sim}-\hat{\theta}_{h}^{\sim}\right\|_{X_{h}}=\left\| \theta_{h}-\hat{\theta}_{h}+\hat{\theta}_{h}^{\sim}-\hat{\theta}_{h}^{\sim} \right\|_{X_{h}}=\left\|\theta_{h}-\hat{\theta}_{h}\right\|_{X_{h}}\leq\beta\,.\]

Then,

\[\left\|\theta_{h}-\theta_{h}^{\sim}\right\|_{X_{h}}=\left\|\theta_{h}-\theta_{ h}-\hat{\theta}_{h}+\hat{\theta}_{h}^{\sim}\right\|_{X_{h}}=\left\|\hat{ \theta}_{h}^{\sim}-\hat{\theta}_{h}\right\|_{X_{h}}\leq c_{h}^{\xi}\,.\]

This completes the proof of the second result in Eq. (83).

**Proof of result \(2.\):** The proof is identical to that of the proof of result \(1.\), except swapping the roles of \(\hat{\theta}_{h},\theta_{h},G\) and \(\hat{\theta}_{h}^{\sim},\theta_{h}^{\sim},\tilde{G}\). \(\Box\)

Now, with the results of Eqs. (83) and (84) in hand, we return to proving Lemma I.2. For any \(h\in[H]\) let \(\theta_{h}=\arg\max_{\theta\in\Theta_{G,h}}\langle\phi(s,a),\theta\rangle\) then, by Eq. (83), there exists a \(\theta_{h}^{\sim}\in\Theta_{\tilde{G},h}\) such that \(\left\|\theta_{h}-\theta_{h}^{\sim}\right\|_{X_{t}}\leq c_{h}^{\xi}\). This gives that, for all \(h\in[H],(s,a)\in\mathcal{S}_{h}\times\mathcal{A}\)

\[\max_{\theta\in\Theta_{G,h}}\langle\phi(s,a),\theta\rangle-\max_{ \theta^{\sim}\in\Theta_{\tilde{G},h}}\langle\phi(s,a),\theta^{\sim}\rangle =\langle\phi(s,a),\theta_{h}\rangle-\langle\phi(s,a),\theta_{h} ^{\sim}\rangle+\langle\phi(s,a),\theta_{h}^{\sim}\rangle-\max_{\theta^{\sim} \in\Theta_{\tilde{G},h}}\langle\phi(s,a),\theta^{\sim}\rangle\] \[\leq\left\|\phi(s,a)\right\|_{X_{h}^{-1}}\|\theta_{h}-\theta_{h}^ {\sim}\|_{X_{h}}\] \[\leq\frac{L_{1}}{\sqrt{\lambda}}c_{h}^{\xi}=L_{1}\tilde{L_{2}}c_{h }^{\xi}/(H^{3/2}d)\,.\]

The second inequality used Eq. (22), that \(\lambda_{\text{max}}(X_{h}^{-1})\leq 1/\lambda\) (by definition of \(X_{h}\) Eq. (11)), and Eq. (86). The last equality used that \(\sqrt{\lambda}=H^{3/2}d/\tilde{L_{2}}\) (Eq. (25)). Now, for the other direction, using similar steps as above, for any \(h\in[H]\), let \(\theta_{h}^{\sim}=\arg\max_{\theta^{\sim}\in\Theta_{\tilde{G},h}}\langle\phi(s,a ),\theta^{\sim}\rangle\) then, by Eq. (84), there exists a \(\theta_{h}\in\Theta_{G,h}\) such that \(\left\|\theta_{h}-\theta_{h}^{\sim}\right\|_{X_{h}}\leq c_{h}^{\xi}\). This gives that, for all \(h\in[H],(s,a)\in\mathcal{S}_{h}\times\mathcal{A}\)

\[\max_{\theta^{\sim}\in\Theta_{\tilde{G},h}}\langle\phi(s,a),\theta^{ \sim}\rangle-\max_{\theta\in\Theta_{G,h}}\langle\phi(s,a),\theta\rangle =\langle\phi(s,a),\theta_{h}^{\sim}\rangle-\langle\phi(s,a), \theta_{h}\rangle+\langle\phi(s,a),\theta_{h}\rangle-\max_{\theta\in\Theta_{G,h }}\langle\phi(s,a),\theta\rangle\] \[\leq\left\|\phi(s,a)\right\|_{X_{h}^{-1}}\|\theta_{h}^{\sim}- \theta_{h}\|_{X_{h}}\] \[\leq\frac{L_{1}}{\sqrt{\lambda}}c_{h}^{\xi}=L_{1}\tilde{L_{2}}c_{h }^{\xi}/(H^{3/2}d)\,.\]

The above two results together imply that, for all \(h\in[H],(s,a)\in\mathcal{S}_{h}\times\mathcal{A}\)

\[\left|\max_{\theta\in\Theta_{G,h}}\langle\phi(s,a),\theta\rangle-\max_{\theta ^{\sim}\in\Theta_{\tilde{G},h}}\langle\phi(s,a),\theta^{\sim}\rangle\right|\leq L _{1}\tilde{L_{2}}c_{h}^{\xi}/(H^{3/2}d)\,.\]

Following the same steps as above for \(\min\) we can get that, for all \(h\in[H],(s,a)\in\mathcal{S}_{h}\times\mathcal{A}\)

\[\left|\min_{\theta\in\Theta_{G,h}}\langle\phi(s,a),\theta\rangle-\min_{\theta^{ \sim}\in\Theta_{\tilde{G},h}}\langle\phi(s,a),\theta^{\sim}\rangle\right|\leq L _{1}\tilde{L_{2}}c_{h}^{\xi}/(H^{3/2}d)\,.\]The above two results together imply that, for all \(h\in[H],(s,a)\in\mathcal{S}_{h}\times\mathcal{A}\),

\[\left|\left(\max_{\theta\in\Theta_{G,h}}\bar{q}_{\theta}(s,a)-\min_{\theta\in \Theta_{G,h}}\bar{q}_{\theta}(s,a)\right)-\left(\max_{\theta^{\sim}\in\Theta_{ \tilde{G},h}}\bar{q}_{\theta^{\sim}}(s,a)-\min_{\theta^{\sim}\in\Theta_{ \tilde{G},h}}\bar{q}_{\theta^{\sim}}(s,a)\right)\right|=L_{\xi}\,,\]

where (by recalling Eq. (85)),

\[L_{\xi}=12\sqrt{2d}H^{2}L_{1}\xi\alpha^{-1}\Big{(}2\sqrt{n}L_{1}\tilde{L_{2}}/ (H^{3/2}d)\Big{)}^{H}\geq 2L_{1}\tilde{L_{2}}c_{h}^{\xi}/(H^{3/2}d)\,.\] (88)

This concludes the proof of the first result in Lemma I.2.

Now we prove the second result in Lemma I.2. For any \(\tilde{G}\in C_{\xi}^{\mathbf{G}},h\in[H]\), define the event

\[\mathcal{E}_{2}^{\tilde{G},h} =\left\{\left|\begin{aligned} &\operatorname*{\mathbb{E}}_{(S,A) \sim\mu_{h}}\!\!\left[\max_{\theta^{\sim}\in\Theta_{\tilde{G},h}}\bar{q}_{ \theta^{\sim}}(S,A)-\min_{\theta^{\sim}\in\Theta_{\tilde{G},h}}\bar{q}_{ \theta^{\sim}}(S,A)\right]\\ &\quad-\frac{1}{n}\sum_{i\in[n]}\!\!\left(\max_{\theta^{\sim}\in \Theta_{\tilde{G},h}}\bar{q}_{\theta^{\sim}}(s_{h}^{i},a_{h}^{i})-\min_{\theta ^{\sim}\in\Theta_{\tilde{G},h}}\bar{q}_{\theta^{\sim}}(s_{h}^{i},a_{h}^{i}) \right)\right|\leq\frac{H}{\sqrt{n}}\sqrt{\log\!\left(\frac{6H|C_{\xi}^{ \mathbf{G}}|}{\delta}\right)}\,.\end{aligned}\right.\]

Then, since \(\max_{\theta^{\sim}\in\Theta_{\tilde{G},h}}\bar{q}_{\theta^{\sim}}(s,a)-\min _{\theta^{\sim}\in\Theta_{\tilde{G},h}}\bar{q}_{\theta^{\sim}}(s,a)\in[0,H]\) for all \((s,a)\in\mathcal{S}_{h}\times\mathcal{A}\), by Hoeffding's inequality (Lemma J.1), we have that, for any \(\tilde{G}\in C_{\xi}^{\mathbf{G}},h\in[H]\), event \(\mathcal{E}_{2}^{\tilde{G},h}\) occurs with probability at least \(1-\delta/(3H|C_{\xi}^{\mathbf{G}}|)\). Let

\[\mathcal{E}_{2}=\bigcap_{\tilde{G}\in C_{\xi}^{\mathbf{G}},h\in[H]}\mathcal{E }_{2}^{\tilde{G},h}\,.\] (89)

Then, by applying a union bound over \(\tilde{G},h\) we have that the event \(\mathcal{E}_{2}\) occurs with probability at least \(1-\delta/3\). 

**Lemma I.4**.: _Let \(\xi>0,\kappa_{t}\geq 0,\forall t\in[2:H+1]\). Then, there exists a set \(C_{\xi}^{\mathbf{G}}\subset\mathbf{G}\) with \(|C_{\xi}^{\mathbf{G}}|\leq(1+2L_{2}/\xi))^{dHd_{0}}\) such that, for any \(G\in\mathbf{G}\), there exists a \(\tilde{G}\in C_{\xi}^{\mathbf{G}}\) such that, for any \(h\in[H]\), for any \(u\in[h]\) and trajectory \(\text{traj}=(s_{t},a_{t},r_{t})_{t\in[u,H+1]}\), and for any \(\theta_{h+1:H+1},\theta_{h+1:H+1}^{\kappa}\in\mathcal{B}(\tilde{L_{2}})^{H-h+1}\) that are close in predictions, that is, such that for all \(t\in[h+1:H+1],s\in\mathcal{S}_{t},\left|\bar{v}_{\theta_{t}}(s)-\bar{v}_{ \theta_{t}^{\sim}}(s)\right|\leq\kappa_{t}\), it holds that_

\[\left|\begin{aligned} &\operatorname*{\mathbb{E}}_{(G,\text{ alg}),h+1}\!\left[r_{h:\tau-1}+\bar{v}_{\theta_{h+1:H+1}}(s_{\tau})\right]- \operatorname*{\mathbb{E}}_{\tau\sim\tilde{F}_{G,\text{ alg},h+1}}\!\left[r_{h:\tau-1}+\bar{v}_{\theta_{h+1:H+1}}(s_{ \tau})\right]\right|\\ &\leq(H-h+1)6\sqrt{2d}HL_{1}\xi/\alpha+\sum_{t=h}^{H}\kappa_{t+ 1}\,.\end{aligned}\right.\]

Proof.: Recall that

\[\mathbf{G}=\left\{(G_{h})_{h\in[2:H]}=(\vartheta_{h}^{i})_{h\in[2:H],i\in[d_{ 0}]}:\text{ for all }h\in[2:H],i\in[d_{0}],\vartheta_{h}^{i}\in\mathcal{B}(L_{2})\right\}.\]

By Lemma J.2, we know there exists a set \(C_{\xi}\subset\mathcal{B}(a),a,\xi>0\) with \(|C_{\xi}|=(1+2a/\xi)^{d}\) such that for any \(x\in\mathcal{B}(a)\) there exists a \(y\in C_{\xi}\) such that \(\left\|x-y\right\|_{2}\leq\xi\). Define the set \(C_{\xi}^{\mathbf{G}}=\bigtimes_{h\in[2:H],i\in[d_{0}]}C_{\xi}\subset\mathbf{G}\) with \(|C_{\xi}^{\mathbf{G}}|\leq(1+2L_{2}/\xi))^{dHd_{0}}\). Then, for any \(G=(\vartheta_{h}^{i})_{h\in[2:H],i\in[d_{0}]}\in\mathbf{G}\), there exists a \(\tilde{G}=(\tilde{\vartheta}_{h}^{i})_{h\in[2:H],i\in[d_{0}]}\in C_{\xi}^{ \mathbf{G}}\) such that

\[\left\|\vartheta_{h}^{i}-\tilde{\vartheta}_{h}^{i}\right\|_{2}\leq\xi\quad\text{ for all }h\in[2:H],i\in[d_{0}]\,.\]

Let \(G,\tilde{G}\) be as defined above. Then, for all and, since we have by definition (Eq. (6)) that \(\omega_{G}\) is a smooth function in terms of range\({}^{G}\), we get that

\[|\omega_{G}(s)-\omega_{\tilde{G}}(s)|\leq\left|2-\frac{\sqrt{2d}\cdot\text{range }^{G}(s)}{\alpha}-\left(2-\frac{\sqrt{2d}\cdot\text{range}^{\tilde{G}}(s)}{ \alpha}\right)\right|\leq\frac{2\sqrt{2d}L_{1}\xi}{\alpha}\,.\] (90)

For all \(h\in[H]\), let \(\theta_{h+1:H+1},\theta_{h+1:H+1}^{\sim}\in\mathcal{B}(\tilde{L_{2}})^{H-h+1}\), such that for all \(t\in[h+1:H+1],s\in\mathcal{S}_{t},\left|\bar{v}_{\theta_{t}}(s)-\bar{v}_{ \theta_{t}^{\sim}}(s)\right|\leq\kappa_{t}\). Then, for any \(h\in[H],u\in[h]\), and trajectory \(\text{traj}=(s_{t},a_{t},r_{t})_{t\in[u:H+1]}\), it holds that

\[\mathop{\mathbb{E}}_{\tau\sim F_{G,\text{adj},h+1}}\bigl{[}r_{h: \tau-1}+\bar{v}_{\theta_{h+1:H+1}}(s_{\tau})\bigr{]}\] \[=r_{h}+(1-\omega_{G}(s_{h+1}))\bar{v}_{\theta_{h+1}}(s_{h+1})+ \omega_{G}(s_{h+1})\bigl{(}r_{h+1}+(1-\omega_{G}(s_{h+2}))v_{\theta_{h+2}}(s_{ h+2})\bigr{)}+\ldots\] \[=r_{h}+(1-\omega_{G}(s_{h+1}))\bar{v}_{\theta_{h+1}}(s_{h+1})+ \omega_{G}(s_{h+1})\mathop{\mathbb{E}}_{\tau\sim F_{G,\text{adj},h+2}}\bigl{[} r_{h+1:\tau-1}+\bar{v}_{\theta_{h+2:H+1}}(s_{\tau})\bigr{]}\,.\] (91)

Using similar steps to above it can be shown that

\[\mathop{\mathbb{E}}_{\tau\sim F_{G,\text{adj},h+1}}\Bigl{[}r_{h: \tau-1}+\bar{v}_{\theta_{h+1:H+1}}(s_{\tau})\Bigr{]}\] \[=r_{h}+(1-\omega_{\tilde{G}}(s_{h+1}))\bar{v}_{\theta_{h+1}}(s_{h +1})+\omega_{\tilde{G}}(s_{h+1})\mathop{\mathbb{E}}_{\tau\sim F_{\tilde{G}, \text{adj},h+2}}\Bigl{[}r_{h+1:\tau-1}+\bar{v}_{\theta_{h+2:H+1}}(s_{\tau}) \Bigr{]}\,.\] (92)

We claim that for any \(h\in[H],u\in[h]\), and trajectory \(\text{traj}=(s_{t},a_{t},r_{t})_{t\in[u:H+1]}\)

\[\left|\mathop{\mathbb{E}}_{\tau\sim F_{G,\text{adj},h+1}}\bigl{[} r_{h:\tau-1}+\bar{v}_{\theta_{h+1:H+1}}(s_{\tau})\bigr{]}-\mathop{\mathbb{E}}_{ \tau\sim F_{G,\text{adj},h+1}}\Bigl{[}r_{h:\tau-1}+\bar{v}_{\theta_{h+1:H+1}}( s_{\tau})\Bigr{]}\right|\] \[\leq(H-h+1)6\sqrt{2d}HL_{1}\xi/\alpha+\sum_{t=h}^{H}\kappa_{t+1}\,.\] (93)

To show Eq. (93) we will use induction on \(h\). The base case is when \(h=H\), for which

\[\left|\mathop{\mathbb{E}}_{\tau\sim F_{G,\text{adj},H+1}}\bigl{[} r_{H:\tau-1}+\bar{v}_{\theta_{H+1:H+1}}(s_{\tau})\bigr{]}-\mathop{\mathbb{E}}_{ \tau\sim F_{G,\text{adj},H+1}}\Bigl{[}r_{H:\tau-1}+\bar{v}_{\theta_{\tilde{H} +1:H+1}}(s_{\tau})\Bigr{]}\right|\] \[=\left|\mathop{\mathbb{E}}_{\tau\sim F_{G,\text{adj},H+1}}[r_{H: \tau-1}]-\mathop{\mathbb{E}}_{\tau\sim F_{G,\text{adj},H+1}}[r_{H:\tau-1}] \right|=0\,,\]

where the first equality holds since \(\Theta_{G,H+1}=\Theta_{\tilde{G},H+1}=\{\vec{0}\}\) (defined in Eq. (13)). The second equality holds since for any \(u\in[H+1]\), and trajectory \(\text{traj}=(s_{t},a_{t},r_{t})_{t\in[u:H+1]}\), \(F_{G,\text{traj},H+1}(\tau=H+1)=F_{\tilde{G},\text{traj},H+1}(\tau=H+1)=1\).

Now, we show the inductive step. Let \(h\in[H-1]\) be arbitrary. Assume Eq. (93) holds for any \(t\in[h+1,H]\). We prove that Eq. (93) also holds for \(h\). Fix any \(u\in[H+1]\), and trajectory \(\text{traj}=(s_{t},a_{t},r_{t})_{t\in[u:H+1]}\). To shorten notation let \(E_{G,\theta,h}=\mathop{\mathbb{E}}_{\tau\sim F_{G,\text{adj},h+1}}[r_{h:\tau- 1}+\bar{v}_{\theta_{h+1:H+1}}(s_{\tau})\bigr{]}\) (and similar for \(E_{\tilde{G},\theta^{\sim},h}\)). Then using the assumption that for all \(u\in[2:H+1],s\in\mathcal{S}_{u},\left|\bar{v}_{\theta_{u}}(s)-\bar{v}_{\theta_{u }^{\sim}}(s)\right|\leq\kappa_{u}\) along with Eqs. (90) to (92) and noting that for any \[[H],E_{G,\theta,h},E_{\tilde{G},\theta^{\sim},h}\in[0,2H],\bar{v}_{\theta_{h+1:H+1} },\bar{v}_{\theta^{\sim}_{h+1:H+1}}\in[0,H],\omega_{G},\omega_{\tilde{G}}\in[0,1],\]

\[\Big{|}_{\tau\sim F_{G,\mu,h+1}}\big{[}r_{h:\tau-1}+\bar{v}_{\theta_{h+1:H+1}}( s_{\tau})\big{]}-\underset{\tau\sim F_{G,\mu,h+1}}{\mathbb{E}}\Big{[}r_{h: \tau-1}+\bar{v}_{\theta^{\sim}_{h+1:H+1}}(s_{\tau})\Big{]}\Big{|}\] \[=\left|(1-\omega_{G}(s_{h+1}))\bar{v}_{\theta_{h+1}}(s_{h+1})-(1- \omega_{\tilde{G}}(s_{h+1}))\bar{v}_{\theta^{\sim}_{h+1}}(s_{h+1})+\omega_{G} (s_{h+1})E_{G,\theta,h+1}-\omega_{\tilde{G}}(s_{h+1})E_{\tilde{G},\theta^{ \sim},h+1}\right|\] \[\leq|(1-\omega_{G}(s_{h+1})-(1-\omega_{\tilde{G}}(s_{h+1}))|| \bar{v}_{\theta_{h+1}}(s_{h+1})|+|1-\omega_{\tilde{G}}(s_{h+1})||\bar{v}_{ \theta_{h+1}}(s_{h+1})-\bar{v}_{\theta^{\sim}_{h+1}}(s_{h+1})|\] \[\qquad+|\omega_{G}(s_{h+1})-\omega_{\tilde{G}}(s_{h+1})||E_{G, \theta,h}|+|\omega_{\tilde{G}}(s_{h+1})||E_{G,\theta,h+1}-E_{\tilde{G},\theta^ {\sim},h+1}|\] \[\leq 2\sqrt{2d}HL_{1}\xi/\alpha+\kappa_{h+1}+4\sqrt{2d}HL_{1}\xi/ \alpha+\Big{|}E_{G,\theta,h+1}-E_{\tilde{G},\theta^{\sim},h+1}\Big{|}\] \[\leq 6\sqrt{2d}HL_{1}\xi/\alpha+\kappa_{h+1}+(H-h)6\sqrt{2d}HL_{1} \xi/\alpha+\sum_{t=h+1}^{H+1}\kappa_{t+1}\] \[\leq(H-h+1)6\sqrt{2d}HL_{1}\xi/\alpha+\sum_{t=h}^{H}\kappa_{t+1}\,,\]

where the second last inequality holds by the inductive hypothesis.

Other Useful Results and Definitions

**Definition 3**.: _A finite set \(G\subset\mathbb{R}^{d}\) is the basis of a near-optimal design for a set \(\Theta\subseteq\mathbb{R}^{d}\), if there exists a probability distribution \(\rho\) over elements of \(G\), such that for any \(\theta\in\Theta\),_

\[\langle v,\theta\rangle=0\quad\text{for all }v\in\mathrm{Ker}(V(G, \rho)),\text{ and}\] (94) \[\left\|\theta\right\|_{V(G,\rho)^{\dagger}}^{2}\leq 2d,\] (95) \[\text{where }V(G,\rho)=\sum_{x\in G}\rho(x)xx^{\top}\,,\] (96)

_where for some matrix \(X\), \(X^{\dagger}\) denotes the Moore-Penrose inverse of some, and \(\mathrm{Ker}(X)\) its kernel (or null space)._

**Lemma J.1** (Hoeffding's Inequality (Theorem 2 in [Hoeffding, 1994])).: _Let \((X_{i})_{i\in\mathbb{N}}\) be independent random variables such that \(X_{i}\in[a,b]\) for some \(a,b\in\mathbb{R}\), and let \(S_{n}=\frac{1}{n}\sum_{i=1}^{n}X_{i}\). Then, with probability at least \(1-\zeta\) it holds that_

\[\left|\operatorname{\mathbbm{E}}S_{n}-S_{n}\right|\leq\frac{(b-a)}{\sqrt{n}} \sqrt{\log\!\left(\frac{2}{\zeta}\right)}\,.\]

**Lemma J.2** (Covering number of the Euclidean ball).: _Let \(a>0,\epsilon>0,d\geq 1\), and \(\mathcal{B}_{d}(a)=\{x\in\mathbb{R}^{d}:\left\|x\right\|_{2}\leq a\}\) denote the \(d\)-dimensional Euclidean ball of radius \(a\) centered at the origin. The covering number of \(\mathcal{B}_{d}(a)\) is upper bounded by \(\left(1+\frac{2a}{\epsilon}\right)^{d}\)._

Proof.: Same as the proof of Corollary 4.2.13 in [Vershynin, 2018] with \(\mathcal{B}(1)\) replaced with \(\mathcal{B}(a)\). 

**Lemma J.3** (Performance Difference Lemma (Lemma 3.2 in [Cai et al., 2020])).: _For any policies \(\pi,\bar{\pi}\), it holds that_

\[v^{\pi}(s_{1})-v^{\bar{\pi}}(s_{1})=\sum_{h=1}^{H}\operatorname{\mathbbm{E}} \limits_{(S_{h},A_{h})\sim\mathbb{P}_{\pi,s_{1}}^{h}}\left(q^{\bar{\pi}}(S_{h},A_{h})-v^{\bar{\pi}}(S)\right).\]

**Lemma J.4** (Elliptical Potential Lemma (Lemma 19.4 in [Lattimore and Szepesvari, 2020])).: _Let \(V_{0}\in\mathbb{R}^{d\times d}\) be positive definite and \(a_{1},\dots,a_{n}\in\mathbb{R}^{d}\) be a sequence of vectors with \(\left\|a_{t}\right\|_{2}\leq L\leq\infty\) for all \(t\in[n],V_{t}=V_{0}+\sum_{s\leq t}a_{s}a_{s}^{\top}\). then,_

\[\sum_{t=1}^{n}\min\Bigl{\{}1,\left\|a_{t}\right\|_{V_{t-1}^{-1}}^{2}\Bigr{\}} \leq 2\log\!\left(\frac{\mathrm{det}\,V_{n}}{\mathrm{det}\,V_{0}}\right) \leq 2d\log\!\left(\frac{\mathrm{Tr}\,V_{0}+nL^{2}}{d\det(V_{0})^{1/d}} \right)\leq 2d\log\!\left(\frac{d\lambda+nL^{2}}{d\lambda}\right).\]

**Lemma J.5** (Projection Bound (Lemma 8 in [Zanette et al., 2020])).: _Let \((a_{i})_{i\in[n]}\) be any sequence of vectors in \(\mathbb{R}^{d}\) and \((b_{i})_{i\ in[n]}\) be any sequence of scalars such that \(|b_{i}|\leq c\). For any \(\lambda\geq 0\) and \(k\in\mathbb{N}\) we have_

\[\left\|\sum_{i=1}^{n}a_{i}b_{i}\right\|_{(\sum_{i=1}^{n}a_{i}a_{i}^{\top}+ \lambda I)^{-1}}^{2}\leq nc^{2}\,.\]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: All claims explained and shown in Section 4. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Discussed in Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: The result is Theorem 1, which lists all the assumptions it uses (which are presented in Section 3.1), and its proof is presented in Section 5. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: The paper does not include experiments requiring code. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: To the best of our knowledge our research does not violate anything in the Code of Ethics upon our careful review. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Discussed at the end of Section 6. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks since we do not use any data or models. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.