# ProbTS: Benchmarking Point and Distributional Forecasting across Diverse Prediction Horizons

 Jiawen Zhang

HKUST (GZ)

Guangzhou, China

jiawe.zh@gmail.com

&Xumeng Wen

Microsoft Research Asia

Beijing, China

xumengwen@microsoft.com

&Zhenwei Zhang

Tsinghua University

Beijing, China

zzw20@mails.tsinghua.edu.cn

Shun Zheng

Microsoft Research Asia

Beijing, China

shun.zheng@microsoft.com

&Jia Li

HKUST (GZ)

Guangzhou, China

jialee@ust.hk

&Jiang Bian

Microsoft Research Asia

Beijing, China

jiang.bian@microsoft.com

This work was done during the internship at Microsoft Research Asia.

###### Abstract

Delivering precise point and distributional forecasts across a spectrum of prediction horizons represents a significant and enduring challenge in the application of time-series forecasting within various industries. Prior research on developing deep learning models for time-series forecasting has often concentrated on isolated aspects, such as long-term point forecasting or short-term probabilistic estimations. This narrow focus may result in skewed methodological choices and hinder the adaptability of these models to uncharted scenarios. While there is a rising trend in developing universal forecasting models, a thorough understanding of their advantages and drawbacks, especially regarding essential forecasting needs like point and distributional forecasts across short and long horizons, is still lacking. In this paper, we present ProbTS, a benchmark tool designed as a unified platform to evaluate these fundamental forecasting needs and to conduct a rigorous comparative analysis of numerous cutting-edge studies from recent years. We dissect the distinctive data characteristics arising from disparate forecasting requirements and elucidate how these characteristics can skew methodological preferences in typical research trajectories, which often fail to fully accommodate essential forecasting needs. Building on this, we examine the latest models for universal time-series forecasting and discover that our analyses of methodological strengths and weaknesses are also applicable to these universal models. Finally, we outline the limitations inherent in current research and underscore several avenues for future exploration. 1

Footnote 1: Project repository: https://github.com/microsoft/ProbTS

## 1 Introduction

Time-series forecasting has extensive applications in various industries, including traffic flow forecasting [43], renewable energy forecasting [67], and diverse forecasting demands in retail [8], finance [29], physical system [39], and climate [48]. It is crucial to provide forecasts across different prediction horizons, addressing both short- and long-term planning needs [13, 26, 4, 61]. Moreover, modern decision-making processes typically require not only point forecasts to quantify planning efficiency but also robust distributional estimations to manage uncertainty effectively [24, 30]. The fundamental need to produce accurate point and distributional forecasts across various horizons presents significant challenges to existing forecasting approaches.

Nevertheless, much of the previous research on developing deep learning models for time-series forecasting has often focused on isolated aspects, such as long-term point forecasting or short-term distribution estimations. This narrow focus may result in skewed methodological choices and hinder the adaptability of these models to rarely evaluated scenarios. For example, studies such as [78, 72, 38, 76, 11, 71, 49, 73, 40] have primarily explored neural architecture designs tailored for long-term point forecasting with strong trending and seasonal patterns. However, it remains unclear how these advancements can be effectively extended to capture complicated distributions and whether these designs maintain their effectiveness in short-term scenarios. Conversely, research such as [58, 57, 62, 7, 33] adapts deep generative models [17, 28] for probabilistic forecasting, specializing in characterizing complex data distributions. Yet, these models have mainly been developed and evaluated in short-term scenarios, leaving questions about their effectiveness in long-term forecasting and their ability to preserve point forecasting performance.

Despite the recent surge in building time-series foundation models over the past year [18, 56, 14, 10, 15, 25, 41, 20, 70, 2, 23, 74], our understanding of their advantages and limitations, especially regarding essential forecasting needs like point and distributional forecasts across various horizons, is still limited. Many of these models claim to support arbitrary prediction horizons, employing different mechanisms that come with their own set of advantages and drawbacks. Among them, a select few offer capabilities for distributional forecasting, which, however, are typically confined to predefined closed-form distributions [56, 70] or discrete distributions with value quantization [2]. The emergence of these foundation models has brought about unprecedented zero-shot forecasting capabilities. Consequently, it is both timely and crucial to delve into an evaluation of their strengths and weaknesses, especially in relation to the fundamental forecasting needs mentioned earlier.

In this study, we present ProbTS, a benchmark tool crafted to serve as a comprehensive platform for assessing those key forecasting needs and for performing a detailed comparison of several state-of-the-art models developed in recent years. To address the core forecasting requirements, ProbTS includes a broad array of datasets and spans various forecasting horizons. It also utilizes both point and distributional metrics to facilitate a thorough performance evaluation.

Our research reveals that the specific data characteristics inherent to different forecasting requirements often play a crucial role in guiding the selection of model designs. As a result, it is crucial to have a comprehensive view of the essential forecasting needs. To aid in the analysis and interpretation of performance, we measure three essential data characteristics in ProbTS: the strength of trends and seasonality, and the complexity of the data distribution. Moreover, we have explicitly distinguished three fundamental methodological aspects within ProbTS that differentiate the existing forecasting models, largely influencing their pros and cons. The first aspect involves the approach to distributional forecasting, ranging from models focused on point forecasts [49, 40] to those using pre-defined distribution heads based on specific data assumptions [56, 70]. The second aspect is the decoding scheme used to generate multi-step forecasts, which can be either autoregressive (AR) or non-autoregressive (NAR). The third aspect pertains to the normalization choice, where the long-term point forecasting models typically employ reversible instance normalization (RevIN) [32] while short-term probabilistic ones often use mean scaling strategies [58, 57].

By utilizing ProbTS, we conduct a systematic comparison between studies that focus on long-term point forecasting and those aimed at short-term distributional estimation, employing various forecasting horizons and evaluation metrics. Our overarching finding is that the strengths of these methods tend to diminish in scenarios they are rarely evaluated in, highlighting several important but unresolved research questions. Notably, while recent probabilistic forecasting approaches have shown proficiency in short-term distribution estimation, we find that long-term distributional forecasting remains a significant challenge. This challenge stems from achieving distribution estimation that remains both efficient and effective as the prediction horizon extends--a topic that has not been thoroughly investigated in existing literature. Additionally, our analysis uncovers a clear divide in the choice of decoding schemes: most long-term point forecasting methods opt for NAR, whereas choices in short-term forecasting studies are more evenly split. Further investigation suggests that the preference for NAR methods stems from existing AR models' difficulty in managing error accumulation, particularly over extended horizons with strong trends. However, we observe that a proper normalization strategy can significantly improve AR models in long-term forecasting, opening new possibilities for AR-based approaches. Moreover, AR decoding performs better in scenarios with pronounced seasonality, indicating potential for refining these strategies, particularly for long term forecasts. Given the inefficiency of existing NAR-based probabilistic methods like CSDI, our comparison highlights the need for further exploration of decoding strategies in future research.

Furthermore, we have expanded the analytical framework of ProbTS to include an examination of several very recently developed time-series foundation models, which has allowed us to re-validate some of our earlier findings. Interestingly, there appears to be a relatively even split in their preference for AR and NAR decoding schemes. Our analysis reaffirms the limitation of AR in handling time-series data, as we observe that AR-based foundation models tend to excel at shorter horizons. However, their performance advantages often significantly diminish over longer forecasting periods. This underscores the critical need for future research to focus on addressing the issue of error accumulation in AR-based foundation models. Besides, our exploration reveals that current probabilistic foundation models may face challenges when dealing with complex data distributions. This observation suggests that the integration of more sophisticated distribution estimation techniques could enhance the development of time-series foundation models.

In summary, we have made the following contributions.

* Introduction of ProbTS, a benchmark tool designed for a thorough evaluation of essential forecasting needs, towards precise point and probabilistic forecasting across varied horizons.
* Comprehensive analysis of methodological variations within forecasting models, especially regarding distributional estimation methods and decoding schemes (AR vs. NAR), which illuminates significant yet previously underexplored research challenges.
* Extension of our analytical framework to include the latest time-series foundation models, providing insights into the implications of their methodological choices and underscoring important directions for future research endeavors.

## 2 Related Work

Classical Time-series Forecasting ModelsIn recent years, classical research in time-series forecasting has bifurcated into two distinct but complementary streams. The first stream has concentrated on refining neural architecture designs for long-term forecasting, primarily employing non-autoregressive decoding schemes to address scenarios with pronounced trend and seasonality. This stream has evolved from enhancing multi-layer perceptrons [53; 76] to developing specialized recurrent or convolutional neural networks [34; 37], and introducing Transformer-based models [66; 49; 40]. Despite achieving advancements in point forecasts, these efforts mainly capture average future changes, with only a few adopting approaches like quantile regression to partially overcome this limitation [69; 36]. On the other hand, the second stream, probabilistic time-series forecasting specializes in capturing the intricate data distribution of future time series. It encompasses a spectrum of techniques, from utilizing predefined likelihood functions [55; 60] and Gaussian copulas [59; 19] to exploring advanced deep generative models [58; 7]. Unlike the first stream, this branch employs both AR [58; 57] and NAR decoding schemes [62; 7; 33], often utilizing standard neural network architectures to represent time series [16; 58; 57; 7; 19], though some studies propose customized designs [62; 35; 5]. Together, these streams highlight the diverse approaches to forecasting, ranging from point predictions focusing on the mean future variations to probabilistic forecasts that capture the full distribution of future values. In Appendix A.1, we summarize a comparison of these models on the coverage of essential forecasting needs and their methodological preferences.

Universal Time-series Foundation ModelsOver the past year, the development of time-series foundation models has greatly accelerated, driven by the success of language foundation models [9]. This wave has seen models such as Lag-Llama [56], TimesFM [15], Timer [41], and Chronos [2] adopting the decoder-only Transformer architecture with an AR decoding scheme. Conversely, models like Forecast PFN [18], MOIRAI [70], TTM [20], and UniTS [23] employ the NAR decoding, often using variable-length placeholders to indicate prediction positions for different horizons. Probabilistic forecasting is less common, with MOIRAI and Lag-Llama integrating pre-defined distribution heads (Student-t for Lag-Llama and a mixture for MOIRAI) while Chronos uses quantized bins to accommodate time-series values and adopts Softmax outputs for distribution approximation. The strategic choice between AR and NAR decoding and the method for distributional estimation highlight distinct trade-offs. For an extensive comparison, see Appendix A.2.

Toolkits for Time-series Forecasting.We observe a plethora of toolkits that have been developed for time-series forecasting. These range from those primarily designed for point forecasting, such as Prophet [63], sktime [42], tasi [52], and TSIibb [71], to others that incorporate probabilistic forecasting, including GluonTS [1], PyTorchTS [58], PyTorchForecasting2, and NeuralForecast3. In creating ProbTS, we built upon the foundations laid by tools like PyTorchTS, GluonTS, and TSIibb. Our unique contribution is a detailed approach that supports both precise point and probabilistic forecasting over various horizons, and examines methodological differences in forecasting models, especially regarding distributional estimation and decoding schemes (AR vs. NAR). Additionally, ProbTS integrates cutting-edge time-series foundation models, making it a comprehensive benchmark tool for tackling current and future challenges in time-series forecasting. A comparison of ProbTS with existing toolkits, focusing on functionalities and features, is provided in Appendix A.3.

Footnote 2: github.com/jdb78/pytorch-forecasting

Footnote 3: github.com/Nixtla/neuralforecast

## 3 The ProbTS Tool

This section offers a concise overview of the ProbTS tool's design and implementation. The core modules and the primary pipeline of ProbTS are depicted in Figure 1.

DataWe aggregate publicly accessible datasets used for both short-term and long-term forecasting. Initial data visualization analyses reveal that the data domains and forecasting horizons significantly influence specific data characteristics within a given forecasting horizon. For instance, many long-term forecasting scenarios exhibit clear trend and seasonality patterns within a forecasting window, while numerous short-term forecasting cases display irregular variations within a short sliding window. Consequently, we have developed quantified indicators, such as trend and seasonality strengths, along with _non-Gaussianity_ to indicate the complexity of data distribution within a forecasting window. Detailed information about dataset statistics, visualization analyses, and quantified measures can be found in Appendix B.1.1, B.1.2, B.1.3, and B.1.4. The quantified measurements for all forecasting scenarios are compiled in Table 1.

MetricsProbTS incorporates a broad range of evaluation metrics to enable a thorough assessment of both point and distributional forecasts. These metrics are elaborated in detail in Appendix B.2. In this paper, we primarily use the normalized mean absolute error (NMAE) for point forecasts and the continuous ranked probability score (CRPS) for distributional forecasts to succinctly communicate the critical insights discovered. It is noteworthy that some methods reproduced in ProbTS, their original papers reported certain point forecast metrics before de-normalizing forecasts to the initial scale [75; 71; 49] or primarily reveal aggregated distributional metrics over all time-series variates, namely CRPS-sum [59; 57; 58]. We have verified our reproduced results align with their reported results and utilized the unified metrics in this study to offer a comprehensive and fair comparison of these studies from different research threads.

Figure 1: An overview of ProbTS.

ModelThe model module in ProbTS explicitly differentiates critical methodological decisions, especially the decoding scheme (AR vs NAR) and the distributional estimation approach. Specifically, we employ the following mathematical formulation. We denote an element of a multivariate time series as \(x_{t}^{k}\in\mathbb{R}\), where \(k\) represents the variate index and \(t\) denotes the time index. At time step \(t\), we have a multivariate vector \(\bm{x}_{t}\in\mathbb{R}^{K}\). Each \(x_{t}^{k}\) is associated with covariates \(\bm{c}_{t}^{k}\in\mathbb{R}^{N}\), which encapsulates auxiliary information about the observations. Given a length-\(T\) forecast horizon, a length-\(L\) observation history \(\bm{x}_{t-L:t}\) and corresponding covariates \(\bm{c}_{t-L:t}\), the objective in time series forecasting is to generate the vector of future values \(\bm{x}_{t+1:t+T}\). Based on established conventions, we categorize forecast as short-term if the horizon \(T\leq\mathcal{F}\)[57; 62], and long-term if \(T\gg\mathcal{F}\)[75; 49; 40], where \(\mathcal{F}\) represents the primary periodicity of the data (e.g., 24 for hourly frequency). To represent point and distributional forecasting in a unified way, here we divide a model into an encoder \(f_{\phi}\) and a forecaster \(p_{\theta}\). An encoder is tasked with generating expressive hidden states \(\bm{h}\in\mathbb{R}^{D}\). Under _autoregressive_ decoding scheme, encoder forecasts variates using their past values: \(\bm{h}_{t}=f_{\phi}(\bm{x}_{t-1},\bm{c}_{t},\bm{h}_{t-1})\). Under the _non-autoregressive_ scheme, the encoder generates all the forecasts in one step: \(\bm{h}_{t+1:t+T}=f_{\phi}(\bm{x}_{t-L:t},\bm{c}_{t-L:t+T})\). A forecaster \(p_{\theta}\) is employed either to directly estimate _point forecasts_ as \(\hat{\bm{x}}_{t}=p_{\theta}(\bm{h}_{t})\), or to perform sampling based on the estimated _probabilistic distributions_ as \(\hat{\bm{x}}_{t}\sim p_{\theta}(\bm{x}_{t}|\bm{h}_{t})\). In addition, the normalization choices utilized by different research branches vary, with a detailed analysis provided in Appendix D.1.

## 4 Results and Analyses

Utilizing ProbTS, we conducted a comprehensive benchmarking and analysis of a diverse range of state-of-the-art models from different strands of research. We mainly assessed these models using NAME and CRPS metrics across multiple forecasting horizons, repeating each experiment five times with different seeds to ensure result reliability.

Selected Models for Comparison.Our selection criteria for models focused on a balance of performance, reproducibility, and simplicity. For long-term point forecasting, we included models like iTransformer [40], PatchTST [49], TimesNet [71], N-HiTS [11], and LTSF-Linear [75]. Probabilistic forecasting methods selected include GRU NVP, GRU MAF, Trans MAF [58], TimeGrad [57], and CSDI [62]. Additionally, general architectures like Linear, GRU [12], and Transformer [66], along with simple non-parametric baselines, were evaluated as a reference. For foundation models, reproducible methods such as Lag-Llama [56], TimesFM [15], Timer [41], MOIRAI [70], Chronos [2], and UniTS [23] were included. Detailed implementation specifics are in Appendix B.3.

Due to space constraints, comprehensive comparison results are placed in Appendix C, with detailed results for short-term and long-term forecasting in Tables 9 and 10, respectively. Zero-shot evaluations of pre-trained time-series foundation models are detailed in Tables 11 and 12. Our evaluation highlights the critical relationship between forecasting requirements, data properties, and modeling strategies. It aims to shed light on the strengths and limitations of current approaches, paving the way for uncovering novel research avenues.

\begin{table}
\begin{tabular}{l|c c c c c c c} \hline \hline
**Dataset-Horizon** & **Exchange-S** & **Solar-S** & **Electricity-S** & **Traffic-S** & **Wikipedia-S** & **ETTm1-L** & **ETTm2-L** \\ \hline Trend \(F_{T}\) & 0.9982 & 0.1688 & 0.6443 & 0.2880 & 0.5253 & 0.9462 & 0.9770 \\ Seasonality \(F_{S}\) & 0.1256 & 0.8592 & 0.8323 & 0.6656 & 0.2234 & 0.0105 & 0.0612 \\ \hline Non-Gaussianity & 0.2967 & 0.5004 & 0.3579 & 0.2991 & 0.2751 & 0.0833 & 0.1701 \\ \hline \hline
**Dataset-Horizon** & **ETTm1-L** & **ETTm2-L** & **Electricity-L** & **Traffic-L** & **Weather-L** & **Exchange-L** & **ILI-L** \\ \hline Trend \(F_{T}\) & 0.7728 & 0.9412 & 0.6476 & 0.1632 & 0.9612 & 0.9978 & 0.5438 \\ Seasonality \(F_{S}\) & 0.4772 & 0.3608 & 0.8344 & 0.6798 & 0.2657 & 0.1349 & 0.6075 \\ \hline Non-Gaussianity & 0.0719 & 0.1422 & 0.1533 & 0.1378 & 0.1727 & 0.1082 & 0.1112 \\ \hline \hline \end{tabular}
\end{table}
Table 1: This table includes a quantitative assessment of the inherent characteristics for all forecasting scenarios, each corresponding to a dataset with a specific forecasting horizon. We use the suffixes ”-S” and ”-L” to differentiate between short-term and long-term scenarios. Quantified indicators encompass trend and seasonality strengths, as well as non-Gaussianity, where a higher value signifies a greater deviation from a Gaussian distribution.

### Analyzing Classical Models for Time-series Forecasting

We examine traditional non-universal time-series models from distinct research branches: one branch focuses on developing customized neural architectures tailored for long-term point forecasting, while the other branch concentrates on creating advanced probabilistic methods for short-term distributional forecasting. Our investigation confirms the effectiveness of these models for their intended purposes. However, we observe a notable trend: the strengths of these methods tend to diminish in scenarios where they are seldom tested.

**Diminishing Advantages of Customized Architectures in Short-term Forecasting Scenarios** The comparative analysis presented in Figures 1(a) and 1(c) showcases the performance of point and probabilistic forecasting methods with respect to the NMAE metric. These figures also illustrate how NMAE values correlate with non-Gaussianity, a measure we employ to evaluate the complexity of data distributions. It becomes evident that customized architectures, originally crafted for long-term forecasting, tend to lose their competitive performance in short-term scenarios. This phenomenon could be attributed to the increased importance of accurately characterizing complex data distributions within shorter forecasting windows, where higher non-Gaussianity scores are indicative of this necessity. A closer look at Figures 1(c) and 1(d) further reveals that the performance disparity measured with CRPS becomes even more pronounced for datasets characterized by significant non-Gaussianity, such as Solar-S. This observation underscores the critical need for incorporating short-term patterns and distributional estimation capabilities into the design of new forecasting architectures.

**Significant Performance Degradation for Existing Probabilistic Methods in Long-term Distribution Forecasting** The performance of current probabilistic forecasting models in long-term scenarios, even when assessed using distributional metrics such as CRPS, reveals notable limitations. This is highlighted by the comparison between Figures 1(b) and 1(d), which shows a significant drop in performance for models like TimeGrad on ETTm1-L, GRU NVP on ETTh2-L, and Weather-L datasets. The decline in performance can be attributed to the fact that these probabilistic models were not specifically designed with the unique challenges of long-term forecasting in mind. This oversight has mixed influences. On the positive side, the design of these methods has led to a more balanced approach in the choice between AR and NAR decoding schemes, providing a versatile

Figure 2: We present a comprehensive comparison between classical models designed for long-term point forecasting and short-term distributional forecasting across various prediction horizons. It utilizes a non-Gaussianity score to highlight the complexity of the data distribution across different datasets. The aggregated performance metrics are derived from Tables 9 and 10.

foundation for probabilistic forecasting. However, the downside is more significant: no matter using which decoding schemes, existing probabilistic models face considerable challenges when applied to long-term distributional forecasting. We will dive deeper into the specific challenges associated with each decoding scheme next. Here the performance gap underscores the need for future research to systematically investigate long-term distributional forecasting.

Different Decoding Schemes & Challenges in Long-term Distributional ForecastingExisting probabilistic forecasting methods exhibit a balanced preference for both AR and NAR decoding schemes. For instance, TimeGrad employs an AR decoding scheme, whereas CSDI utilizes an NAR approach. This contrasts starkly with the aforementioned customized architectures, which solely opt for NAR decoding. These two types of decoding schemes, however, confront distinctive challenges when applied to long-term probabilistic forecasting. With original normalization strategy, AR probabilistic models like TimeGrad struggle with error accumulation, particularly as the forecast horizon extends or trends strengthen, the performance gap widens, as shown in Figures 2(a) and 2(c). On the other hand, NAR models such as CSDI encounter memory constraints in long-term forecasts (detailed in Appendix D.7). Moreover, Table 10 reveals that even on smaller datasets, such as ETTm2 and ETTh1, CSDI's performance in long-term scenarios is less than optimal, indicating reduced learning efficiency by the extension of the forecasting horizon.

The Unexpected Superiority of AR Decoding in Addressing Strong SeasonalityDespite its drawbacks, AR-based models, as used in TimeGrad, excels in capturing strong seasonality, outperforming models like PatchTST in scenarios such as the Traffic dataset (Table 10). This advantage is further analyzed in Figures 2(c) and 2(d), showing AR's increasing benefit with stronger seasonal patterns. This suggests AR's potential in long-term forecasting could be revitalized with solutions to its error accumulation challenge in long horizons.

Figure 3: We explore the challenges faced by current models in conducting long-term distributional forecasting, with insights drawn from Table 10 and Table 16. Subplot (a) shows significant error increases in AR-based models, averaged across all datasets except Traffic. Subplot (b) demonstrates how the instance-level normalization impacts performance in long-term forecasting. Subplots (c) examine how trends and seasonality impact performance across all long-term forecasting datasets and horizons.

RevIN's Effectiveness in Long-term Forecasting with Exceptions.RevIN significantly enhances AR-based models in long-term forecasting, as shown in Figures 2(b) and 7. Notably, on the ETTh1 dataset, GRU NVP (w/ RevIN) even outperforms PatchTST (w/ RevIN). While RevIN offers substantial improvements for most models across most datasets, it brings negative impact on the Traffic dataset. The Traffic dataset features strong seasonality but minimal trends, thus we speculate that the major distribution shift addressed by RevIN is related to normalizing the effect of trending. These findings indicate that normalizing the trending effect could be a direction to alleviate error accumulation of AR-based models in long-term forecasting. However, we also observe that RevIN does not seem to be an ideal match for the NAR probabilistic model. For instance, CSDI (w/ RevIN) performs worse than CSDI (w/ Scaling) on the Weather dataset. Further research in developing effective normalization strategies for NAR probabilistic models is necessary.

No Dominating Normalization Strategies in Short-term Forecasting.While RevIN is effective for long-term scenarios, it does not adequately address the challenges faced by short-term probabilistic models. As shown in Table 14, RevIN fails to consistently deliver significant improvements for models such as CSDI, TimeGrad, and GRU NVP in short-term forecasting. The mean scaling strategy has proven to be the most reliable option for these models, explaining its widespread use. Although omitting instance-level normalization is occasionally acceptable, it can lead to significant issues, as seen with TimeGrad (without normalization) on the Wikipedia and Solar datasets, and GRU NVP (without normalization) on Electricity. We provide detailed analysis in Appendix D.1.

### Analyzing Foundation Models for Universal Time-series Forecasting

We next explore the capabilities of recent foundation models in universal time-series forecasting, focusing on their performance across different prediction horizons and their ability to estimate distributions, especially regarding their zero-shot transfer capabilities on unseen datasets. Table 11 showcases their significant progress, sometimes outperforming traditional models without re-training. Using the analytic framework of ProbTS, we delve into their methodological pros and cons.

Navigating the AR Decoding Challenge over Extended Forecasting HorizonsFigure 3(a) illustrates how the performance of various time-series foundation models evolves in relation to expanding forecasting horizons. It is evident that for shorter horizons, AR-based foundation models such as TimesFM and Timer exhibit highly competitive performance, on par with NAR-based models like MOIRAI. However, the advantage of NAR-based decoding becomes increasingly apparent as the forecasting horizon lengthens, as demonstrated by the widening performance gap between TimesFM and MOIRAI. This trend is consistent with our earlier observation that without proper normalization strategies, AR-based methods could suffer from significant error accumulation when applied to long-term time-series forecasting. Given the inherent strengths of AR decoding, such as its superiority at capturing strong seasonality and its robust performance in certain short-term forecasting

Figure 4: We evaluate the efficacy of time-series foundation models for various forecasting horizons and distributional estimation. Subplot (a), derived from Table 11 and excluding results from the Electricity dataset, demonstrates the short-term forecasting capabilities and long-term error accumulation of AR-based models. Subplot (b), draw from Table 12, investigates short-term distributional estimation, highlighting the performance challenges of foundation models compared to CSDI in handling complex data distributions. Note that we include MOIRAI with two different context lengths, 96 and 5000, as context length significantly affects its transfer performance.

scenarios, it is clear that further research is warranted to explore ways to overcome its limitations in long-term forecasting contexts. This could potentially unlock new avenues for enhancing the versatility and effectiveness of AR-based time-series foundation models across a broader range of forecasting horizons.

The Critical Role of Addressing Complex Data DistributionsFigure 3(a) illustrates the incremental changes in CRPS among leading probabilistic time-series foundation models, such as MOIRAI and Chronos, compared to the best-performing short-term probabilistic model, CSDI. It becomes apparent that in scenarios characterized by complex data distributions, indicated by higher non-Gaussianity scores, the performance decline of MOIRAI in relation to CSDI becomes notably more pronounced. This phenomenon may be attributed to MOIRAI's approach to supporting distributional forecasting, which involves utilizing a mixture of predefined distribution heads. While this method is efficient and effective for certain applications, it may lack the expressiveness required to accurately model more complex data distributions. Furthermore, these observations underscore that, in specific contexts, foundation models might not be able to fully replace traditional models that have been specifically tailored and trained for particular domains. Additionally, the prospect of fine-tuning these foundation models as a remedy is less economically viable, primarily due to their significantly larger size. This highlights the importance of not only continuing to refine foundation models to enhance their adaptability and performance across a wide spectrum of data distributions but also recognizing the continued relevance of domain-specific models, especially for handling intricate data distributions where a more nuanced approach may be necessary.

## 5 Conclusion

In this study, we introduced ProbTS, a benchmark tool tailored for evaluating essential forecasting needs, which facilitates a detailed comparison of various state-of-the-art models in the context of time-series forecasting. Through our comprehensive analysis, we identified significant challenges and opportunities in the realm of time-series forecasting, particularly highlighting the need for models that can effectively address both point and probabilistic forecasting across diverse horizons.

LimitationsWhile our study represents a significant step forward in understanding and evaluating time-series forecasting models, it does come with many limitations. A predominant focus of our work is on empirical analysis, relying heavily on intuitions and experimental observations, which may lack the depth that theoretical foundations could provide. Additionally, our exploration, though extensive, might not encompass all the nuanced factors that influence model performance. By concentrating on major methodological decisions such as AR versus NAR decoding schemes, we may inadvertently overlook other critical aspects that could play a decisive role in forecasting accuracy. Moreover, the datasets employed for evaluation, despite their diversity and relevance to current research threads, may not fully capture the vast spectrum of real-world forecasting challenges. This limitation is particularly pronounced when comparing different foundation models, as their pre-training often involves an even broader array of data, potentially skewing the comparative analysis.

Future DirectionsThe insights derived from our study open the door to numerous promising research directions. Addressing the shortcomings of AR and NAR decoding schemes, especially in their application across varying forecasting horizons, emerges as a critical area for future exploration. Innovating effective architecture designs that can navigate the intricacies of short-term forecasting challenges and devising efficient methods for long-term probabilistic forecasting stand out as urgent needs. For AR-based models, reducing error accumulation remains essential, with ReVIN-style normalization showing potential for improving long-term forecasting. Additionally, exploring effective normalization strategies for NAR-based probabilistic models is an underdeveloped yet promising area. Equally important is the enhancement of models' abilities to characterize complex data distributions, which could significantly improve the adaptability and effectiveness of foundation models. Beyond these technical endeavors, expanding the scope of datasets used for evaluation to encompass a wider range of real-world scenarios will be crucial for validating the robustness and versatility of future forecasting models. Lastly, integrating theoretical insights with empirical findings could provide a more holistic understanding of model behaviors, contributing to the development of more sophisticated and nuanced forecasting solutions.

## References

* [1]
* Alexandrov et al. [2020] Alexander Alexandrov, Konstantinos Benidis, Michael Bohlke-Schneider, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, Danielle C. Maddix, Syama Rangapuram, David Salinas, Jasper Schulz, Lorenzo Stella, Ali Caner Turkmen, and Yuyang Wang. 2020. GluonTS: Probabilistic and Neural Time Series Modeling in Python. _JMLR_ 21, 116 (2020), 1-6.
* Ansari et al. [2024] Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin Shen, Oleksandr Shchur, Syama Syndar Rangapuram, Sebastian Pineda Arango, Shubham Kapoor, Jasper Zschiegner, Danielle C. Maddix, Hao Wang, Michael W. Mahoney, Kari Torkkola, Andrew Gordon Wilson, Michael Bohlke-Schneider, and Yuyang Wang. 2024. Chronos: Learning the Language of Time Series. _arXiv preprint arXiv:2403.07815_ (2024).
* Athanasopoulos et al. [2011] George Athanasopoulos, Rob J Hyndman, Haiyan Song, and Doris C Wu. 2011. The tourism forecasting competition. _International Journal of Forecasting_ 27, 3 (2011), 822-844.
* Barros et al. [2015] Joaquim Barros, Miguel Araujo, and Rosaldo JF Rosseti. 2015. Short-term real-time traffic prediction methods: A survey. In _International Conference on Models and Technologies for Intelligent Transportation Systems_.
* Bergsma et al. [2022] Shane Bergsma, Timothy Zeyl, Javad Rahimipour Anaraki, and Lei Guo. 2022. C2FAR: Coarse-to-Fine Autoregressive Networks for Precise Probabilistic Forecasting. In _NeurIPS_.
* Bhatnagar et al. [2021] Aadyot Bhatnagar, Paul Kassianik, Chenghao Liu, Tian Lan, Wenzhuo Yang, Rowan Cassius, Doyen Sahoo, Devansh Arpit, Sri Subramanian, Gerald Woo, et al. 2021. Merlion: A machine learning library for time series. _arXiv preprint arXiv:2109.09265_ (2021).
* Bilos et al. [2023] Marin Bilos, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka, and Stephan Gunnemann. 2023. Modeling Temporal Data as Continuous Functions with Stochastic Process Diffusion. In _ICML_. 2452-2470.
* Bose et al. [2017] Joos-Hendrik Bose, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, Dustin Lange, David Salinas, Sebastian Schelter, Matthias Seeger, and Yuyang Wang. 2017. Probabilistic demand forecasting at scale. _VLDB_ (2017).
* Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In _NeurIPS_.
* Cao et al. [2024] Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan Liu. 2024. TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting. In _ICLR_.
* Challu et al. [2023] Cristian Challu, Kin G. Olivares, Boris Oreshkin, Federico Ramirez, Max Canseco, and Artur Dubrawski. 2023. NHITS: Neural Hierarchical Interpolation for Time Series Forecasting. In _AAAI_. 6989-6997.
* Chung et al. [2014] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. _arXiv preprint arXiv:1412.3555_ (2014).
* Cramer et al. [2022] Estee Y Cramer, Evan L Ray, Velma K Lopez, Johannes Bracher, Andrea Brennen, Alvaro J Castro Rivadeneira, Aaron Gerding, Tilmann Gneiting, Katie H House, Yuxin Huang, et al. 2022. Evaluation of individual and ensemble probabilistic forecasts of COVID-19 mortality in the United States. _Proceedings of the National Academy of Sciences_ (2022).
* Darlow et al. [2024] Luke Nicholas Darlow, Qiwen Deng, Ahmed Hassan, Martin Asenov, Rajkarn Singh, Artijom Joosen, Adam Barker, and Amos Storkey. 2024. DAM: Towards a Foundation Model for Forecasting. In _ICLR_.

* Das et al. [2024] Abhimanyu Das, Weihao Kong, Rajat Sen, and Yichen Zhou. 2024. A decoder-only foundation model for time-series forecasting. In _ICML_.
* de Bezenac et al. [2020] Emmanuel de Bezenac, Syama Sundar Rangapuram, Konstantinos Benidis, Michael Bohlke-Schneider, Richard Kurle, Lorenzo Stella, Hilaf Hasson, Patrick Gallinari, and Tim Januschowski. 2020. Normalizing Kalman Filters for Multivariate Time Series Analysis. In _NeurIPS_. 2995-3007.
* Dinh et al. [2017] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. 2017. Density Estimation using Real NVP. In _ICLR_.
* Dooley et al. [2023] Samuel Dooley, Gurnoor Singh Khurana, Chirag Mohapatra, Siddartha V Naidu, and Colin White. 2023. ForecastPFN: Synthetically-trained zero-shot forecasting. In _NeurIPS_.
* Drouin et al. [2022] Alexandre Drouin, Etienne Marcotte, and Nicolas Chapados. 2022. TACTiS: Transformer-Attentional Copulas for Time Series. In _ICML_. 5447-5493.
* Ekambaram et al. [2024] Vijay Ekambaram, Arindam Jati, Nam H Nguyen, Pankaj Dayama, Chandra Reddy, Wesley M Gifford, and Jayant Kalagnanam. 2024. Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series. _arXiv preprint arXiv:2401.03955_ (2024).
* Elsayed et al. [2021] Shereen Elsayed, Daniela Thyssens, Ahmed Rashed, Hadi Samer Jomaa, and Lars Schmidt-Thieme. 2021. Do we really need deep learning models for time series forecasting? _arXiv preprint arXiv:2101.02118_ (2021).
* Falcon and Lightning team [2019] William Falcon and The PyTorch Lightning team. 2019. PyTorch Lightning. https://doi.org/10.5281/zenodo.3828935
* Gao et al. [2024] Shanghua Gao, Teddy Koker, Owen Queen, Thomas Hartvigsen, Theodoros Tsiligkaridis, and Marinka Zitnik. 2024. UniTS: Building a Unified Time Series Model. _arXiv preprint arXiv:2403.00131_ (2024).
* Gneiting and Katzfuss [2014] Tilmann Gneiting and Matthias Katzfuss. 2014. Probabilistic Forecasting. _Annual Review of Statistics and Its Application_ (2014).
* Goswami et al. [2024] Mononito Goswami, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, and Artur Dubrawski. 2024. MOMENT: A Family of Open Time-series Foundation Models. In _ICML_.
* Hernandez et al. [2014] Luis Hernandez, Carlos Baladron, Javier M Aguiar, Belen Carro, Antonio J Sanchez-Esguevillas, Jaime Lloret, and Joaquim Massana. 2014. A survey on electric power demand forecasting: Future trends in smart grids, microgrids and smart buildings. _IEEE Communications Surveys & Tutorials_ (2014).
* Herzen et al. [2022] Julien Herzen, Francesco LASSig, Samuele Giuliano Piazzetta, Thomas Neuer, LAeo Tafti, Guillaume Raille, Tomas Van Pottelbergh, Marek Pasieka, Andrzej Skrodzki, Nicolas Huguenin, Maxime Dumonal, Jan KO-cisz, Dennis Bader, Fr-Orick Gusset, Mounir Benheddi, Camila Williamson, Michal Kosinski, Matej Petrik, and G-Grosch. 2022. Darts: User-Friendly Modern Machine Learning for Time Series. _JMLR_ 23, 124 (2022), 1-6.
* Ho et al. [2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising Diffusion Probabilistic Models. In _NeurIPS_.
* Hou et al. [2021] Min Hou, Chang Xu, Yang Liu, Weiqing Liu, Jiang Bian, Le Wu, Zhi Li, Enhong Chen, and Tie-Yan Liu. 2021. Stock trend prediction with multi-granularity data: A contrastive learning approach with adaptive fusion. In _CIKM_.
* Hyndman and Athanasopoulos [2018] Rob J Hyndman and George Athanasopoulos. 2018. _Forecasting: principles and practice_.
* Jiang et al. [2021] Xiaodong Jiang, Sudeep Srivastava, Sourav Chatterjee, Yang Yu, Jeffrey Handler, Peiyi Zhang, Rohan Bopardikar, Dawei Li, Yanjun Lin, Uttam Thakore, Michael Brundage, Ginger Holt, Caner Komurlu, Rakshita Nagalla, Zhichao Wang, Hechao Sun, Peng Gao, Wei Cheung, Jun Gao, Qi Wang, Marius Guerard, Morteza Kazemi, Yulin Chen, Chong Zhou, Sean Lee, Nikolay Laptev, Tihamer Levendovszky, Jake Taylor, Huijun Qian, Jian Zhang, Aida Shoydokova, TrishaSingh, Chengjun Zhu, Zeynep Baz, Christoph Bergmeir, Di Yu, Ahmet Koylan, Kun Jiang, Ploy Temiyasathit, and Emre Yurtbay. 2022. _Kats_. https://github.com/facebookresearch/Kats
* Kim et al. [2022] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. 2022. Reversible Instance Normalization for Accurate Time-Series Forecasting against Distribution Shift. In _ICLR_.
* Kollovich et al. [2023] Marcel Kollovich, Abdul Fatir Ansari, Michael Bohlke-Schneider, Jasper Zschiegner, Hao Wang, and Yuyang Wang. 2023. Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting. In _NeurIPS_.
* Lai et al. [2018] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. 2018. Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks. In _SIGIR_. 95-104.
* Li et al. [2022] Yan Li, Xinjiang Lu, Yaqing Wang, and Dejing Dou. 2022. Generative Time Series Forecasting with Diffusion, Denoise, and Disentanglement. In _NeurIPS_.
* Lim et al. [2021] Bryan Lim, Sercan O Arik, Nicolas Loeff, and Tomas Pfister. 2021. Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting. _International Journal of Forecasting_ (2021).
* LIU et al. [2022] Minhao LIU, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia LAI, Lingna Ma, and Qiang Xu. 2022. SCINet: Time Series Modeling and Forecasting with Sample Convolution and Interaction. In _NeurIPS_.
* Liu et al. [2022] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X. Liu, and Schahram Dustdar. 2022. Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting. In _ICLR_.
* Liu et al. [2024] Yang Liu, Jiashun Cheng, Haihong Zhao, Tingyang Xu, Peilin Zhao, Fugee Tsung, Jia Li, and Yu Rong. 2024. SEGNO: Generalizing Equivariant Graph Neural Networks with Physical Inductive Biases. In _ICLR_.
* Liu et al. [2024] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. 2024. itransformer: Inverted transformers are effective for time series forecasting. In _ICLR_.
* Liu et al. [2024] Yong Liu, Haoran Zhang, Chenyu Li, Xiangdong Huang, Jianmin Wang, and Mingsheng Long. 2024. Timer: Transformers for Time Series Analysis at Scale. In _ICML_.
* Loning et al. [2019] Markus Loning, Anthony Bagnall, Sajaysurya Ganesh, Viktor Kazakov, Jason Lines, and Franz J Kiraly. 2019. sktime: A unified interface for machine learning with time series. _arXiv preprint arXiv:1909.07872_ (2019).
* Lv et al. [2014] Yisheng Lv, Yanjie Duan, Wenwen Kang, Zhengxi Li, and Fei-Yue Wang. 2014. Traffic flow prediction with big data: A deep learning approach. _IEEE Transactions on Intelligent Transportation Systems_ (2014).
* Makridakis and Hibon [1997] Spyros Makridakis and Michele Hibon. 1997. ARMA models and the Box-Jenkins methodology. _Journal of forecasting_ 16, 3 (1997), 147-163.
* Makridakis et al. [2020] Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. 2020. The M4 Competition: 100,000 time series and 61 forecasting methods. _International Journal of Forecasting_ 36, 1 (2020), 54-74.
* Makridakis et al. [2022] Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. 2022. The M5 competition: Background, organization, and implementation. _International Journal of Forecasting_ 38, 4 (2022), 1325-1336.
* Matheson and Winkler [1976] James E Matheson and Robert L Winkler. 1976. Scoring Rules for Continuous Probability Distributions. _Management Science_ 22, 10 (1976), 1087-1096.
* Mudelsee [2019] Manfred Mudelsee. 2019. Trend analysis of climate time series: A review of methods. _Earth-science reviews_ (2019).
* Mussel et al. [2019]* Nie et al. [2023] Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2023. A Time Series is Worth 64 Words: Long-term Forecasting with Transformers. In _ICLR_.
* Nielsen [2019] Frank Nielsen. 2019. On the Jensen-Shannon symmetrization of distances relying on abstract means. _Entropy_ 21, 5 (2019), 485.
* Nixtla [2024] Nixtla. 2024. NeuralForecast. https://github.com/Nixtla/neuralforecast. GitHub repository.
* A State-of-the-art Deep Learning Library for Time Series and Sequential Data. Github. https://github.com/timeseriesAI/tsai
* Oreshkin et al. [2020] Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. 2020. N-BEATS: Neural basis expansion analysis for interpretable time series forecasting. In _ICLR_.
* Qiu et al. [2024] Xiangfei Qiu, Jilin Hu, Lekui Zhou, Xingjian Wu, Junyang Du, Buang Zhang, Chenjuan Guo, Aoying Zhou, Christian S. Jensen, Zhenli Sheng, and Bin Yang. 2024. TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods. _VLDB_ 17, 9 (2024), 2363-2377.
* Rangapuram et al. [2018] Syama Sundar Rangapuram, Matthias W. Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and Tim Januschowski. 2018. Deep State Space Models for Time Series Forecasting. In _NeurIPS_. 7796-7805.
* Rasul et al. [2023] Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Hena Ghonia, Rishika Bhagwatkar, Arian Khorasani, Mohammad Javad Darvishi Bayazi, George Adamopoulos, Roland Riachi, Nadhir Hassen, et al. 2023. Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting. _arXiv preprint arXiv:2310.08278_ (2023).
* Rasul et al. [2021] Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. 2021. Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting. In _ICML_. 8857-8868.
* Rasul et al. [2021] Kashif Rasul, Abdul-Saboor Sheikh, Ingmar Schuster, Urs M. Bergmann, and Roland Vollgraf. 2021. Multivariate Probabilistic Time Series Forecasting via Conditioned Normalizing Flows. In _ICLR_.
* Salinas et al. [2019] David Salinas, Michael Bohlke-Schneider, Laurent Callot, Roberto Medico, and Jan Gasthaus. 2019. High-dimensional Multivariate Forecasting with Low-rank Gaussian Copula Processes. In _NeurIPS_. 6824-6834.
* Salinas et al. [2020] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. 2020. DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks. _International Journal of Forecasting_ 36, 3 (2020), 1181-1191.
* Su et al. [2016] Fei Su, Honghui Dong, Limin Jia, Yong Qin, and Zhao Tian. 2016. Long-term forecasting oriented to urban expressway traffic situation. _Advances in mechanical engineering_ (2016).
* Tashiro et al. [2021] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. 2021. CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation. In _NeurIPS_. 24804-24816.
* Taylor and Letham [2018] Sean J Taylor and Benjamin Letham. 2018. Forecasting at Scale. _The American Statistician_ (2018).
* Team [2024] Pytorch Forecasting Team. 2024. PyTorch Forecasting. http://github.com/jdb78/pytorch-forecasting. GitHub repository.
* Team [2024] Pytorch Transformer TS Team. 2024. Pytorch Transformer based Time Series Models. https://github.com/kashif/pytorch-transformer-ts. GitHub repository.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In _NeurIPS_. 5998-6008.
* Wang et al. [2019] Huaizhi Wang, Zhenxing Lei, Xian Zhang, Bin Zhou, and Jianchun Peng. 2019. A review of deep learning for renewable energy forecasting. _Energy Conversion and Management_ (2019).

* Wang et al. (2006) Xiaozhe Wang, Kate Smith, and Rob Hyndman. 2006. Characteristic-based Clustering for Time Series Data. _Data Mining and Knowledge Discovery_ 13 (2006), 335-364.
* Wen et al. (2017) Ruofeng Wen, Kari Torkkola, Balakrishnan Narayanaswamy, and Dhruv Madeka. 2017. A Multi-horizon Quantile Recurrent Forecaster. _arXiv preprint arXiv:1711.11053_ (2017).
* Woo et al. (2024) Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, and Doyen Sahoo. 2024. Unified Training of Universal Time Series Forecasting Transformers. In _ICML_.
* Wu et al. (2023) Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. 2023. TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis. In _ICLR_.
* Wu et al. (2021) Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting. In _NeurIPS_. 22419-22430.
* Xu et al. (2024) Zhijian Xu, Ailing Zeng, and Qiang Xu. 2024. FITS: Modeling Time Series with $10k$ Parameters. In _ICLR_.
* Ye et al. (2024) Jiexia Ye, Weiqi Zhang, Ke Yi, Yongzi Yu, Ziyue Li, Jia Li, and Fugee Tsung. 2024. A Survey of Time Series Foundation Models: Generalizing Time Series Representation with Large Language Mode. _arXiv preprint arXiv:2405.02358_ (2024).
* Zeng et al. (2023) Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. 2023. Are Transformers Effective for Time Series Forecasting?. In _AAAI_. 11121-11128.
* Zhang et al. (2022) Tianping Zhang, Yizhuo Zhang, Wei Cao, Jiang Bian, Xiaohan Yi, Shun Zheng, and Jian Li. 2022. Less is More: Fast Multivariate Time Series Forecasting with Light Sampling-Oriented MLP Structures. _arXiv preprint arXiv:2207.01186_ (2022).
* Zhang et al. (2023) Weiqi Zhang, Jianfeng Zhang, Jia Li, and Fugee Tsung. 2023. A co-training approach for noisy time series learning. In _CIKM_. 3308-3318.
* Zhou et al. (2021) Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wan-cai Zhang. 2021. Informer: Beyond Efficient Transformer for Long Sequence Time-series Forecasting. In _AAAI_. 11106-11115.

Additional Related Work

A Comparison with Traditional Models on Covering Essential Forecasting Needs and methodological decisions

Table 2 presents a comparative summary of our approach, which adopts an integrated perspective, and representative studies from the existing literature.

### A Comparison of Pre-trained Time-series Foundation Models

We have incorporated eight recently emerged time series foundation models, namely Lag-Llama [56], Chronos [2], TimesFM [15], Timer [41], MOIRAI [70], UniTS [23], ForecastPFN [18], and TTM [20], into our framework. These foundation models are categorized based on their capabilities, such as zero-shot forecasting, adaptability to varying prediction lengths, and support for probabilistic predictions, as well as their architectural designs, including whether they are auto-regressive and the nature of their backbone networks. Additionally, we have detailed their training processes, including the lengths of prediction horizons used during pre-training and the sizes of look-back windows. These details are summarized in Table 3.

Furthermore, we have compiled a summary of these foundation models' pre-training and evaluation on several classical time series forecasting datasets. This compilation is presented in Table 4.

### A Comparison with Existing Libraries on the Coverage of Data, Model, and Metrics

ProbTS is a research toolkit designed to advance forecasting research across varied horizons, focusing on both point and distributional forecasting. To achieve these objectives, ProbTS includes state-of-the-art models, comprehensive evaluation protocols (point vs. distributional), and explores different methodological aspects of forecasting models, particularly in terms of distributional estimation

\begin{table}
\begin{tabular}{l|c c|c c|c c|c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{2}{c}{**Pred. Horizon**} & \multicolumn{2}{c}{**Paradigm**} & \multicolumn{2}{c}{**Arch. Design**} & \multicolumn{2}{c}{**Dec. Scheme**} \\  & Short & Long & Point & Distr. & General & Customized & AR & Non-AR \\ \hline N-BEATS [53] & \(\nearrow\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\times\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\ Autoformer [72] & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\times\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\ Informer [78] & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\times\) & \(\times\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\ Pyraformer [38] & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\times\) & \(\times\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\ N-HiTS [11] & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\times\) & \(\times\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\ LTSF-Linear [75] & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\times\) & \(\times\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\ PatchTST [49] & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\times\) & \(\times\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\ TimesNet [71] & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\times\) & \(\times\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\ iTransformer [40] & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\times\) & \(\times\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\ \hline DeepAR [60] & \(\checkmark\) & \(\times\) & \(\times\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\ GP-copula [59] & \(\checkmark\) & \(\times\) & \(\times\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\ LSTM NVP [58] & \(\checkmark\) & \(\times\) & \(\times\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\ LSTM MAF [58] & \(\checkmark\) & \(\times\) & \(\times\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\ Trans MAF [58] & \(\checkmark\) & \(\times\) & \(\times\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\ TimeGrad [57] & \(\checkmark\) & \(\times\) & \(\times\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\ CSDI [62] & \(\checkmark\) & \(\times\) & \(\times\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\ SPD [7] & \(\checkmark\) & \(\times\) & \(\times\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\ TSDiff [33] & \(\checkmark\) & \(\times\) & \(\times\) & \(\checkmark\) & \(\times\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\ \hline This Study & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: We provide a concise comparison between the methodologies presented in this paper and those from two distinct research branches. The comparison is based on data scenarios (short-term versus long-term forecasting), primary evaluation metrics (point versus distributional forecasts), and key methodological choices (general or customized neural architecture designs, and autoregressive or non-autoregressive decoding schemes).

methods and decoding schemes (AR vs. NAR). In Table 5, we provide a comprehensive comparison of ProbTS with existing libraries in terms of toolkit functionalities and the benchmarking aspects we aim to investigate.

## Appendix B More Details on ProbTS

### Data

The data module unifies varied data scenarios to facilitate thorough evaluation and implements standardized pre-processing techniques to ensure fair comparison.

Moreover, we utilize a quantitative approach to visually delineate datasets' intrinsic characteristics, which employs decomposition to assess trends and seasonality in a time series and evaluate the similarity between data distribution and a Gaussian to depict the complexity of data distribution.

#### b.1.1 Time-series Forecasting Datasets

Table 6 provides a summary of the public datasets employed in our study. These datasets have been sourced from recent research studies in the field of deep time-series forecasting.

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c} \hline \hline
**Model** & **Solar** & **Wikipedia** & **ETTm1** & **ETTm2** & **ETTh1** & **ETTh2** & **Electricity** & **Traffic** & **Weather** & **Exchange** & **ILI** \\ \hline MOIRAI & \(\bigcirc\) & ✓ & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) & ✓ & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) \\ Lag-Llama & ✓ * & \(\bigcirc\) & ✓ & \(\bigcirc\) & ✓ & ✓ & ✓ & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) \\ TimesFM & \(\bigcirc\)* & ✓* & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) & ✓ & ✓ & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) \\ Chronos & ✓ & ✓* & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) & ✓ & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) \\ TTM & \(\bigtimes\) & \(\bigtimes\) & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) \\ UniTS & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) & \(\bigcirc\) \\ Timer & \(\bigtimes\) & \(\bigtimes\) & _few_ & _few_ & _few_ & _few_ & _few_ & _few_ & _\(\bigtimes\)_ & \(\bigtimes\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Evaluation Datasets for Time-series Foundation Models. We selected several popular datasets to evaluate time-series foundation models. \(\bigvee\) indicates pre-training on the dataset, \(\bigcirc\) indicates zero-shot evaluation on the dataset, few indicates few-shot evaluation on the dataset, and \(\bigtimes\) indicates the dataset is not mentioned in the paper or documentation. *’ indicates that the data comes from the same source but may be processed differently.

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c|c} \hline \hline
**Model** & **Zero-shot** & **Any-horizon** & **AR** & **Prob.** & **Arch.** & **Multi-variate** & **Pre-train** & **Look-back** \\  & **shot** & **horizon** & & & & **variate** & **Horizons** & **Window** \\ \hline Lag-Llama & ✓ & ✓ & ✓ & ✓ & D-O & \(\bigtimes\) & 24\(\sim\)60 & 32\(\sim\)1024 \\ Chronos & ✓ & ✓ & ✓ & ✓ & E-D & \(\bigtimes\) & 64 & 512 \\ TimesFM & ✓ & ✓ & ✓ & ✓ & D-O & \(\bigtimes\) & – & 512 \\ Timer & ✓ & ✓ & ✓ & ✓ & D-O & \(\bigtimes\) & up to 1440 & 672 \\ \hline MOIRAI & ✓ & ✓ & ✗ & ✓ & E-O & \(\bigcirc\) & varying & 100\(\sim\)5000 \\ UniTS & ✓ & ✓ & ✗ & ✗ & E-O & \(\bigtimes\) & – & 60\(\sim\)720 \\ ForecastPFN & ✓ & ✓ & ✗ & ✗ & E-O & \(\bigtimes\) & 0 50 & 50 500 \\ TTM & ✓ & ✗ & ✗ & ✗ & Unique & ✓ & 96 & 512 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Foundation Models for Time Series. **Zero-shot** indicates whether the original work tests zero-shot capabilities. **Any-horizon** indicates if the same pre-trained model can adapt to prediction tasks of varying lengths. **AR** denotes if the model performs auto-regressive forecasting. **Prob.** indicates if the model natively supports probabilistic forecasting. **Arch.** denotes the model’s backbone architecture: D-O for decoder-only transformer, E-O for encoder-only transformer, E-O for encoder-decoder transformer, and unique for specially designed backbones. **Multi-variate** indicates if the model explicitly handles multivariate relationships. **Pre-train Horizons** specifies the forecasting task horizons during pre-training. **Look-back Window** specifies the context history length settings used in the original experiments.

[MISSING_PAGE_FAIL:17]

#### b.1.3 Quantifying Trend and Seasonality Strengths

Based on the intuition obtained from data visualization, we would like to quantify the strengths of trend and seasonality for a time-series segment with a predefined window size (corresponding to the prediction horizon). Then we can quantify the trend and seasonality strengths at the dataset level by averaging over all time-series segments of a dataset.

To quantify the strengths of trend and seasonality for a fixed-length time-series segment, we draw upon methodologies outlined in the work of [68]. In particular, we employed a time series decomposition

Figure 6: We have also sampled and visualized multiple time-series segments from the long-term forecasting datasets, where the size of the segment window matches the prediction horizon.

model expressed as:

\[y_{t}=T_{t}+S_{t}+R_{t},\]

where \(T_{t}\) represents the smoothed trend component, \(S_{t}\) signifies the seasonal component, and \(R_{t}\) denotes the remainder component. In order to obtain each component, we followed the STL decomposition approach 4.

Footnote 4: https://otexts.com/fpp2/stl.html

In the case of strongly trended data, the variation within the seasonally adjusted data should considerably exceed that of the remainder component. Consequently, the ratio \(\text{Var}(R_{t})/\text{Var}(T_{t}+R_{t})\) is expected to be relatively small. As such, the measure of trend strength can be formulated as:

\[F_{T}=\max\left(0,1-\frac{\text{Var}(R_{t})}{\text{Var}(T_{t}+R_{t})}\right).\]

The quantified trend strength, ranging from \(0\) to \(1\), characterizes the degree of trend presence. Similarly, the evaluation of seasonal intensity employs the detrended data:

\[F_{S}=\max\left(0,1-\frac{\text{Var}(R_{t})}{\text{Var}(S_{t}+R_{t})}\right).\]

A series with \(F_{S}\) near \(0\) indicates minimal seasonality, while strong seasonality is indicated by \(F_{S}\) approaching \(1\) due to the considerably smaller variance of \(\text{Var}(R_{t})\) in comparison to \(\text{Var}(S_{t}+R_{t})\).

Tables 1 depict the results for each dataset. Notably, the ETT datasets and the Exchange dataset manifest conspicuous trends, whereas the Electricity, Solar, and Traffic datasets showcase marked seasonality. Additionally, the Exchange dataset stands out with distinctive features. Figure 6 also illustrates that with shorter prediction windows, the Exchange dataset sustains comparatively minor fluctuations, almost forming a linear trajectory. This enables effective forecasting through a straightforward batch mean approach. As the forecasting horizon extends, the dataset appears a more pronounced trend while retaining minimal seasonality.

#### b.1.4 Quantifying Data Distribution Complexity

To differentiate between methods optimized for point or distributional forecasts, we aim to quantify the complexity of data distribution within a time-series segment whose window size equals the prediction horizon length. Such complexities may arise from the unpredictability of the data itself or from noises accidentally introduced during the data collection process [77].

We propose that assessing non-Gaussianity, i.e., how closely the distribution of time-series values within that window resembles a Gaussian distribution, could serve as a meaningful measure. This is because point forecasting methods, optimized with mean squared loss, are essentially equivalent to probabilistic counterparts that include a Gaussian output head and employ maximum a posteriori estimation. This suggests that point forecasting methods inherently assume that time-series values adhere to a Gaussian distribution. In contrast, advanced probabilistic methods, which do not make prior assumptions about data distribution, can adapt to complex data distributions in a data-driven manner.

Hence, we use the Jensen-Shannon divergence [50] to measure the similarity between the actual value distribution of a time-series segment and a Gaussian distribution fitted to the observed values. Short-term datasets used a window size of 30, while long-term datasets used a size of 336. By averaging the calculated divergence values across all time-series segments of a dataset, we obtain a dataset-level measure of non-Gaussianity. A larger divergence value indicates a larger deviation from a Gaussian distribution in the data.

### Metrics

In ProbTS, we integrate an extensive variety of metrics that take into account both point and distributional forecasts, thereby providing a comprehensive and multifaceted assessment of forecasting models.

#### b.2.1 Metrics for Point Forecasts

Mean Absolute Error (MAE)The Mean Absolute Error (MAE) quantifies the average absolute deviation between the forecasts and the true values. Since it averages the absolute errors, MAE is robust to outliers. Its mathematical formula is given by:

\[\text{MAE}=\frac{1}{K\times T}\sum_{k=1}^{K}\sum_{t=1}^{T}|x_{t}^{k}-\hat{x}_{t} ^{k}|,\]

where \(K\) is the number of variates, \(T\) is the length of series, \(x_{t}^{k}\) and \(\hat{x}_{t}^{k}\) denotes the ground-truth value and the predicted value, respectively. For multivariate time series, we also provide the aggregated version:

\[\text{MAE}_{\text{sum}}=\frac{1}{T}\sum_{t=1}^{T}|x_{t}^{\text{sum}}-\hat{x}_{ t}^{\text{sum}}|,\]

where \(x_{t}^{\text{sum}}\) and \(\hat{x}_{t}^{\text{sum}}\) are the summation across the dimension \(K\) of \(x_{t}^{k}\) and \(\hat{x}_{t}^{k}\), respectively.

Normalized Mean Absolute Error (NMAE)The Normalized Mean Absolute Error (NMAE) is a normalized version of the MAE, which is dimensionless and facilitates the comparability of the error magnitude across different datasets or scales. The mathematical representation of NMAE is given by:

\[\text{NMAE}=\frac{\sum_{k=1}^{K}\sum_{t=1}^{T}|x_{t}^{k}-\hat{x}_{t}^{k}|}{ \sum_{k=1}^{K}\sum_{t=1}^{T}|x_{t}^{k}|}.\]

Its aggregated version is:

\[\text{NMAE}_{\text{sum}}=\frac{\sum_{t=1}^{T}|x_{t}^{\text{sum}}-\hat{x}_{t}^ {\text{sum}}|}{\sum_{t=1}^{T}|x_{t}^{\text{sum}}|}.\]

Mean Squared Error (MSE)The Mean Squared Error (MSE) is a quantitative metric used to measure the average squared difference between the observed actual value and forecasts. It is defined mathematically as follows:

\[\text{MSE}=\frac{1}{K\times T}\sum_{k=1}^{K}\sum_{t=1}^{T}(x_{t}^{k}-\hat{x}_ {t}^{k})^{2}.\]

For multivariate time series, we also provide the aggregated version:

\[\text{MSE}_{\text{sum}}=\frac{1}{T}\sum_{t=1}^{T}(x_{t}^{\text{sum}}-\hat{x} _{t}^{\text{sum}})^{2}.\]

Normalized Root Mean Squared Error (NRMSE)The Normalized Root Mean Squared Error (NRMSE) is a normalized version of the Root Mean Squared Error (RMSE), which quantifies the average squared magnitude of the error between forecasts and observations, normalized by the expectation of the observed values. It can be formally written as:

\[\text{NRMSE}=\frac{\sqrt{\frac{1}{T}\sum_{k=1}^{T}(x_{t}^{\text{sum}}-\hat{x }_{t}^{\text{sum}})^{2}}}{\frac{1}{T}\sum_{t=1}^{T}|x_{t}^{\text{sum}}|}.\]

For multivariate time series, we also provide the aggregated version:

\[\text{NRMSE}_{\text{sum}}=\frac{\sqrt{\frac{1}{T}\sum_{t=1}^{T}(x_{t}^{ \text{sum}}-\hat{x}_{t}^{\text{sum}})^{2}}}{\frac{1}{T}\sum_{t=1}^{T}|x_{t}^{ \text{sum}}|}.\]

Mean Absolute Scaled Error (MASE)The Mean Absolute Scaled Error (MASE) divides the MAE of forecasted values by MAE of the in-sample one-step naive forecast, which is a scale-invariant metrics:

\[\text{MASE}=\frac{\frac{1}{K\times T}\sum_{k=1}^{K}\sum_{t=1}^{T}|x_{t}^{k}- \hat{x}_{t}^{k}|}{\frac{1}{K\times T}\sum_{k=1}^{K}\sum_{t=1}^{T}|x_{t}^{k}-x_ {t-1}^{k}|}.\]

#### b.2.2 Metrics for Distributional Forecasts

Continuous Ranked Probability Score (CRPS)The Continuous Ranked Probability Score (CRPS) [47] quantifies the agreement between a cumulative distribution function (CDF) \(F\) and an observation \(x\), represented as:

\[\text{CRPS}=\int_{\mathds{R}}(F(z)-\mathds{I}\{x\leq z\})^{2}dz,\]

where \(\mathds{I}\{x\leq z\}\) denotes the indicator function, equating to one if \(x\leq z\) and zero otherwise.

Being a proper scoring function, CRPS reaches its minimum when the predictive distribution \(F\) coincides with the data distribution. When using the empirical CDF of \(F\), denoted as \(\hat{F}(z)=\frac{1}{n}\sum_{i=1}^{n}\mathds{I}\{X_{i}\leq z\}\), where \(n\) represents the number of samples \(X_{i}\sim F\), CRPS can be precisely calculated from the simulated samples of the conditional distribution \(p_{\theta}(\bm{x}_{t}|\bm{h}_{t})\). In our practice, 100 samples are employed to estimate the empirical CDF.

For multivariate time series, the aggregate CRPS, denoted as \(\text{CRPS}_{\text{sum}}\), is derived by summing across the \(K\) time series, both for the ground-truth data and sampled data, and subsequently averaging over the forecasting horizon. Formally, it is represented as:

\[\text{CRPS}_{\text{sum}}=\mathds{E}_{t}\left[\text{CRPS}\left(\hat{F}_{\text{ sum}}(t),\sum_{i=1}^{K}x_{i,t}^{0}\right)\right].\]

### Baselines

To ensure the integrity of the results, ProbTS adheres to a standard implementation process, employing unified data splitting, standardization techniques, and adopting fair settings for hyperparameter tuning across all methods.

Implementation DetailsProbTS was developed using PyTorch Lightning [22]. During training, we sampled 100 batches per epoch and limited training to 50 epochs, using the CRPS metric for checkpointing. All experiments employed the Adam optimizer and were run on single NVIDIA Tesla V100 GPUs with CUDA 11.3. To enable evaluation of distribution-level metrics, we conducted 100 samplings to calculate metrics on the test set.

Following the most commonly adopted settings [75, 49, 71], in the long-term forecasting context, all of the models are following the same experimental setup with prediction length \(T\in\{24,36,48,60\}\) for ILI-L dataset and \(T\in\{96,192,336,720\}\) for other datasets. Note that the lookback window here is 96 for all the models, to ensure a fair comparison. In the short-term forecasting context, the length of the lookback window is the same as the forecasting horizons, which are 30 for Exchange-S dataset and Wikipedia-S dataset, and 24 for the rest, the same as [59].

Hyper-parameter TuningFor a fair comparison, we conducted a comprehensive grid search for critical hyperparameters across all models in this study. Table 7 details the shared hyperparameters tuned within the ProbTS pipeline, along with those kept constant. Due to the vast array of model-specific hyperparameters, we present an example configuration in Table 8. Complete hyperparameter configurations for each model, identified through this process, will be made available in a public GitHub repository for transparency and reproducibility.

Implementation Details on Foundation ModelsWe used reference implementations of eight time series foundation models into ProbTS.

For Lag-Llama [56], we use its official code5 and integrate the LagLlamaEstimator with its pre-trained checkpoint. The look-back window is uniformly set to 512, irrespective of the forecast horizon. The other hyper-parameters are aligned with the recommended settings.

Footnote 5: https://github.com/time-series-foundation-models/lag-llama

For Chronos [2], we use its official code6 and integrate the ChronosPipeline into ProbTS using amazon/chronos-t5 checkpoints (three models were tested: small, base and large). Two look-backwindows are used during evaluation: 96 and 512. We set limit_prediction_length=False to enable it to predict horizons longer than 64. However, this may potentially lead to a decrease in predictive performance since the model was only trained to consider prediction lengths of 64 or less during pre-training.

For TimesFM [15], we modify its official code7 into ProbTS and load checkpoints from google/timesfm-1.0-200m. Look-back window is set to 96 for a fair comparison.

Footnote 7: https://github.com/google-research/timesfm

For timer [41], we modify its official code8 into ProbTS and Timer_67M_UTSD_4G checkpoint downloaded from its repo. Look-back window is also set to 96 for a fair comparison.

Footnote 8: https://github.com/thuml/Large-Time-Series-Model

Footnote 9: https://github.com/SalesforceAIResearch/uni2ts

For MOIRAI [70], we employ its official code9 and load checkpoints from Salesforce/moirai-1.0-R-base. Two look-back windows are utilized during evaluation: 96 and 5000. The original experiments suggest that MOIRAI's forecasting capability can be consistently enhanced by increasing the look-back window. Consequently, we have included a 5000 look-back window to test the model's performance.

Footnote 9: https://github.com/abacusai/ForecastPFN

For UniTS [23], we have adapted its official code10 into our ProbTS framework and loaded the saved_weights from its repo. The look-back window is set to 96 to ensure a fair comparison. It is important to note that this checkpoint was originally used for the Zero-Shot New-length Forecasting experiment in the original work (where models are challenged to predict new lengths by adjusting from the trained length, with offsets ranging from 0 to 384), which differs from the objectives of our experiments.

Footnote 10: https://github.com/SalesforceAIResearch/uni2ts

For ForecastPFN [18], we have integrated its official code11 into our ProbTS framework and have utilized the units_x128_pretrain_checkpoint. However, we have encountered some challenges

\begin{table}
\begin{tabular}{l|c} \hline \hline
**Hyper-parameter** & **Value or Range Searched** \\ \hline learning rate & [1e-4, 1e-3, 1e-2] \\ dropout & [0, 0.1, 0.2] \\ batch\_size & [8, 16, 32, 64] \\ use\_lags & [True, False] \\ use\_feat\_idx\_emb & [True, False] \\ use\_time\_feat & [True, False] \\ autoregressive & [True, False] \\ scaler & [Standard, Scaling, None] \\ limit\_train\_batches & 100 \\ num\_samples & 100 \\ quantiles\_num & 20 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Hyper-parameters values fixed or range searched in hyper-parameter tuning.

\begin{table}
\begin{tabular}{l|l} \hline \hline Model & Hyperparameter \\ \hline DLinear & learning\_rate=0.01, kernel\_size=3, f\_hidden\_size=40 \\ \hline PatchTST & learning\_rate=0.0001, stride=3, patch\_len=6, n\_layers=3, n\_heads=8, dropout=0.1, kernel\_size=3, f\_hidden\_size=32 \\ \hline TimesNet & learning\_rate=0.001, n\_layers=2, num\_kernels=6, top\_k=5, f\_hidden\_size=64, d\_ff=64 \\ \hline GRU NVP & learning\_rate=0.001, f\_hidden\_size=40, num\_layers=2, n\_blocks=3, hidden\_size=100, conditional\_length=200 \\ \hline GRU MAF & learning\_rate=0.001, f\_hidden\_size=40, num\_layers=2, n\_blocks=4, hidden\_size=100, conditional\_length=200 \\ \hline Trans MAF & learning\_rate=0.001, f\_hidden\_size=32, num\_heads=8, n\_blocks=4, hidden\_size=100, conditional\_length=200 \\ \hline TimeGrad & learning\_rate=0.001, f\_hidden\_size=128, num\_layers=4, conditional\_length=100, beta\_end=0.1, diff\_steps=100 \\ \hline CSDI & learning\_rate=0.001, channels=64, emb\_time\_dim=128, emb\_feature\_dim=16, num\_steps=50, num\_heads=8, n\_layers=4 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Hyperparameter settings for Electricity-S dataset.

[MISSING_PAGE_FAIL:23]

[MISSING_PAGE_EMPTY:24]

[MISSING_PAGE_FAIL:25]

## Appendix D Additional Results and Experiments

### The Impact of Normalization

Normalization is a crucial aspect of time-series models, with different research branches adopting distinct strategies that can affect model performance across various data scenarios. Motivated by this, we provide a detailed analysis of different normalization methods in this section. Unless stated otherwise, our benchmarking follows the default normalization methods used by each model.

#### d.1.1 Different Normalization Choice Between Distinct Research Branches

In time series forecasting, normalization typically occurs in two stages. First, during preprocessing, _dataset-level normalization_ is applied, where global statistics (e.g., mean and standard deviation) from the training set are used to normalize all time-series values. Then, a local normalization module might be used to perform _instance-level normalization_ when feeding a batch of time-series segments into the model.

Different research branches prefer distinct instance-level normalization strategies. Long-term point forecasting models [49, 40] typically adopt the RevIN [32]. Given a batch of time-series segments within a lookback window, RevIN applies a per-series z-score normalization, augmented with learnable affine parameters. Its main advantage is its effectiveness in addressing distribution shifts, particularly in long-term forecasting.

In contrast, most short-term probabilistic forecasting models [57, 58] employ an ad-hoc but still effective normalization strategy. For example, given a batch of time-series segments \(X\in\mathbb{R}^{K\times L}\) (where \(K\) is the number of variables and \(L\) is the length of the lookback window), a per-series scaling is applied as \(X_{i}^{\text{norm}}=\frac{X_{i}}{\sum_{t=1}^{L}|X_{i,t}|/L},\ i=1,\dots,K\) to stabilize value ranges. For simplicity, we refer to this type of normalization as _Mean Scaling_.

We summarize the instance-level normalization choices originally used by each model in the Table 13.

Existing probabilistic models rarely use RevIN and are seldom combined with AR-based models that employ RevIN-style normalization. Similarly, the mean scaling strategy, commonly used in probabilistic forecasting models, is rarely applied to models designed for long-term forecasting. To better understand the effects of different normalization strategies, we selected representative models from both categories and combined them with three normalization methods: _RevIN_, _Scaling_ (i.e., mean scaling), and _w/o Norm_ (no instance-level normalization, using time-series values as provided by the dataset-level preprocessing). The results of these experiments are presented in Table 14, 15, 16, and 17.

\begin{table}
\begin{tabular}{l|l} \hline \hline
**Normalization Choice** & **Model** \\ \hline ReVIN & iTransformer, PatchTST \\ Mean Scaling & TimeGrad, GRU NVP \\ w/o Norm & GRU, CSDI, DLinear \\ \hline \hline \end{tabular}
\end{table}
Table 13: Original instance-level normalization choices of each model.

\begin{table}
\begin{tabular}{l|c c|c c|c c|c c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{2}{c}{Exchange-S} & \multicolumn{2}{c}{Solar-S} & \multicolumn{2}{c}{Electricity-S} & \multicolumn{2}{c}{Traffic-S} \\  & CRPS & NMAE & CRPS & NMAE & CRPS & NMAE & CRPS & NMAE \\ \hline CSDI & \(0.008_{.000}\) & \(0.011_{.000}\) & \(\mathbf{0.366_{.005}}\) & \(\mathbf{0.484_{.008}}\) & \(\mathbf{0.050_{.001}}\) & \(\mathbf{0.065_{.001}}\) & \(\mathbf{0.146_{.012}}\) & \(\mathbf{0.176_{.013}}\) \\ \hline Chronos & \(0.007\) & \(\underline{0.010}\) & – & – & – & – & \(\underline{0.178}\) & \(\underline{0.211}\) \\ MOIRAI-96 & \(\mathbf{0.007}\) & \(\mathbf{0.010}\) & \(0.502\) & \(\underline{0.681}\) & \(0.069\) & \(0.084\) & – & – \\ MOIRAI-5000 & \(0.007\) & \(0.010\) & \(0.521\) & \(0.702\) & \(0.059\) & \(0.079\) & – & – \\ \hline \hline \end{tabular}
\end{table}
Table 12: Results of probabilistic foundation models on short-term distributional forecasting. For every model, we exclude the evaluation results on its pre-trained datasets.

#### d.1.2 Analysis of Instance-level Normalization

**RevIN Significantly Improves Most Models in Long-term Forecasting Scenarios, with Some Exceptions.** RevIN's ability to mitigate the effects of data distribution shifts leads to significant performance improvements in most models for long-term forecasting, as shown in Table 16. This benefit extends beyond models like PatchTST and iTransformer, which originally employed RevIN, to others such as DLinear that do not inherently use this approach. Notably, RevIN has greatly enhanced AR-based models in long-term scenarios. For instance, on the ETT datasets, GRU NVP (w/ RevIN) outperforms even PatchTST (w/ RevIN), suggesting that normalizing trend effects can help reduce error accumulation in AR-based models.

However, RevIN can have a negative impact in certain cases. On the Traffic dataset, GRU (w/ ReVIN) and GRU NVP (w/ ReVIN) perform worse than without normalization, as shown in Table 16. Interestingly, this aligns with our analysis of data characteristics: the Traffic dataset displays strong seasonality but less trending. We speculate that RevIN's effectiveness in other datasets stems from its ability to normalize trend-related distribution shifts, which is less relevant for the Traffic dataset. Additionally, RevIN appears less suited for NAR probabilistic models. For instance, CSDI (w/ RevIN) performs worse than CSDI (w/ Scaling) on the Weather, Electricity, Exchange, and ILI dataset. Further research is needed to develop more effective normalization strategies for NAR probabilistic models.

**No Dominating Normalization Strategies in Short-term Forecasting.** As shown in Table 14, RevIN does not consistently provide robust or significant improvements for models such as CSDI, TimeGrad, and GRU NVP in short-term forecasting. The Mean Scaling strategy, though empirical, proves to be the most reliable choice for these probabilistic models, likely explaining its widespread use. In some cases, instance-level normalization can be omitted, but this approach can lead to serious issues, as seen with TimeGrad (w/o normalization) on the Wikipedia and Solar datasets, and GRU NVP (w/o normalization) on the Electricity dataset. Developing effective instance-level normalization methods for complex data distributions in short-term forecasting remains an important yet often overlooked research direction.

### The Impact of Data Scale

To further explore critical characteristics of time-series forecasting, we have examined the correlation between model performance gains, relative to the baseline model (GRU), and dataset dimensions, length, and volume (see Table 18). However, our analysis does not identify a significant correlation between these factors and model performance.

### Statistical and Gradient Boosting Decision Tree Baselines

To enhance the empirical robustness of our study, we integrate classical statistical models, including ARIMA [44] and ETS [30], along with the Gradient Boosting Decision Tree (GBDT) model, XGBoost, into the ProbTS framework. The results in Table 19 clearly demonstrate the superior performance of deep learning methods over simple statistical baselines, emphasizing the importance

\begin{table}
\begin{tabular}{c|c c c c c c c|c c c c c} \hline \hline Model & \multicolumn{4}{c|}{PatchTST} & \multicolumn{3}{c|}{CSDI} & \multicolumn{3}{c|}{TimeGrad} & \multicolumn{3}{c}{GRU NVP} \\  & RevIN & Scaling & w/o Norm & ReVIN & Scaling & w/o Norm & ReVIN & Scaling & w/o Norm & ReVIN & Scaling & w/o Norm \\ \hline Electricity-S & 0.0659 & **0.0645** & 0.0660 & 0.0666 & - & **0.0648** & 0.0852 & **0.0710** & 0.9742 & 1.0881 & **0.0929** & 0.2246 \\ Exchange Rate-S & **0.0102** & 0.0108 & 0.0111 & **0.0096** & 0.0111 & 0.0151 & **0.0115** & 0.0118 & 0.0220 & **0.0114** & 0.0189 & 0.0170 \\ Solar-S & **0.6275** & 0.7105 & 0.7160 & 0.5988 & **0.5616** & 0.5580 & **0.6041** & 0.7011 & 0.9162 & 1.291 & 0.7244 & **0.5893** \\ Traffic-S & **0.2001** & 0.2036 & 0.2168 & 0.1752 & 0.1877 & **0.1669** & 0.2167 & **0.1516** & 0.1683 & 0.2577 & 0.2216 & 0.2877 \\ Wikipedia-S & **0.2529** & 0.3245 & 0.3695 & 0.2585 & **0.2437** & 0.2609 & 0.3278 & **0.3257** & 0.9968 & 0.4041 & **0.3559** & 0.5131 \\ \hline \hline \end{tabular}
\end{table}
Table 15: The impact of different normalization methods in short-term forecasting scenarios (NMAE).

\begin{table}
\begin{tabular}{c|c c c c c c c c|c c c c} \hline \hline Model & \multicolumn{4}{c|}{PatchTST} & \multicolumn{3}{c|}{CSDI} & \multicolumn{3}{c}{TimeGrad} & \multicolumn{3}{c}{GRU NVP} \\  & RevIN & Scaling & w/o Norm & ReVIN & Scaling & w/o Norm & ReVIN & Scaling & w/o Norm & ReVIN & Scaling & w/o Norm \\ \hline Electricity-S & 0.0659 & **0.0645** & 0.0660 & 0.0666 & - & **0.0652** & 0.0673 & **0.0563** & 0.9681 & 0.0659 & **0.0706** & 0.1607 \\ Exchange Rate-S & **0.0102** & 0.0108 & 0.0111 & **0.0070** & 0.0083 & 0.0110 & 0.0100 & **0.0093** & 0.0170 & **0.0090** & 0.0147 & 0.0133 \\ Solar-S & **0.6275** & 0.7105 & 0.7166 & 0.4903 & **0.4342** & 0.4603 & **0.4945** & 0.5455 & 0.8362 & 0.9529 & 0.5293 & **0.4393** \\ Traffic-S & **0.2001** & 0.2036 & 0.2168 & 0.1505 & **0.1535** & **0.1389** & 0.1806 & **0.1280** & 0.1400 & 0.1257 & **0.1770** & 0.2277 \\ Wikipedia-S & **0.2529** & 0.3245 & 0.3695 & 0.2164 & **0.2060** & 0.2276 & **0.2757** & 0.2773 & 0.9969 & 0.3317 & **0.3187** & 0.4561 \\ \hline \hline \end{tabular}
\end{table}
Table 14: The impact of different normalization methods in short-term forecasting scenarios (CRPS).

## Appendix A

## Appendix Aof capturing non-linear dependencies for accurate forecasts. Notably, ARIMA and ETS exhibit varied performance across different data characteristics. ARIMA struggles with datasets like Solar, characterized by weak trending and strong seasonality, while ETS shows better adaptability. Conversely, in cases of strong trending and weak seasonality, as observed in the 'Wikipedia' dataset, ARIMA significantly outperforms ETS.

Utilizing the implementation from [21], we find that XGBoost competes well, even surpassing neural network models in certain scenarios. However, for datasets with more complex distributions like 'Solar' and 'Electricity,' advanced probabilistic estimation methods demonstrate a substantial advantage over traditional learning methods and point estimation techniques. This highlights the adaptability and strength of advanced probabilistic methods in handling intricate forecasting scenarios.

### Experiments on Univariate Datasets

In pursuit of a comprehensive analysis spanning univariate and multivariate scenarios, we examined a subset of M4 [45], M5 [46], and TOURISM datasets [3]--crucial datasets for univariate time-series forecasting. Table 20 provides a quantitative assessment of the intrinsic characteristics of these new datasets, focusing on trending strength, seasonality, and data distribution complexity, as detailed in

\begin{table}
\begin{tabular}{l|c c c c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{2}{c}{DLinear} & \multicolumn{2}{c}{PatchTST} & \multicolumn{2}{c}{GRU NP} & \multicolumn{2}{c}{TimeGrad} & \multicolumn{2}{c}{CSDI} \\  & CRPS & NMAE & CRPS & NMAE & CRPS & NMAE & CRPS & NMAE & CRPS & NMAE \\ \hline \# Var. & 0.2422 & 0.2422 & -0.2676 & -0.2676 & -0.1856 & -0.2136 & -0.1665 & -0.1793 & -0.2315 & -0.2592 \\ \# Total timestep & -0.1422 & -0.1422 & 0.3821 & 0.3821 & 0.3072 & 0.3329 & 0.2860 & 0.2971 & 0.3542 & 0.3826 \\ \# Var. \(\times\) Timestep & 0.0162 & 0.0162 & 0.0166 & 0.0166 & -0.0068 & -0.0011 & 0.0082 & 0.0117 & -0.0053 & -0.0133 \\ \hline \hline \end{tabular}
\end{table}
Table 18: The correlation coefficient between the data volume and the relative performance improvement compared to the baseline model (GRU).

Figure 7: Impact of different instance-level normalization methods on model performance.

[MISSING_PAGE_FAIL:31]

### Experiments on Synthetic Datasets

To enhance the rigor of the insights presented, we employ synthetic datasets created with the GluonTS library13, encompassing a baseline dataset and variants with pronounced trends, strong seasonality, and complex data distribution (see Table 22). Specifically, we generate these datasets by superimposing four components - trend, seasonality, noise, and anomaly - each with adjustable intensity parameters. The seasonality component is defined by period hyper-parameters and intensity coefficients; the trend by slope intensity; the noise by Gaussian distribution sampling with adjustable intensity; and the anomaly by occurrence probability and maximum intensity.

\begin{table}
\begin{tabular}{l|c c c c c c c c} \hline \hline \multirow{2}{*}{Dataset} & \multicolumn{2}{c}{DLinear} & \multicolumn{2}{c}{PatchTST} & \multicolumn{2}{c}{GRU NVP} & \multicolumn{2}{c}{TimeGrad} \\  & CRPS & NMAE & CRPS & NMAE & CRPS & NMAE & CRPS & NMAE \\ \hline M4-Weekly & 0.081 & 0.081 & 0.089 & 0.089 & 0.066 & 0.077 & 0.055 & 0.065 \\ M4-Daily & 0.034 & 0.034 & 0.035 & 0.035 & 0.030 & 0.038 & 0.026 & 0.032 \\ M5 & 0.891 & 0.891 & 0.898 & 0.898 & 0.679 & 0.864 & - & - \\ TOURISM-Monthly & 0.168 & 0.168 & 0.136 & 0.136 & 0.171 & 0.223 & 0.152 & 0.191 \\ \hline \hline \end{tabular}
\end{table}
Table 21: Results on M4, M5, and TOURISM datasets. We utilize a lookback window of 3H, with ‘H’ denoting the forecasting horizon.

Figure 9: Impact of different instance-level normalization methods on model performance.

Subsequent experiments on these synthetic datasets (refer to Table 23), using representative models, validate the empirical findings established on other datasets with ProbTS. Key observations include the declining performance of autoregressive decoding models, such as TimeGrad, in the presence of increasing trends, improved performance for models using autoregressive decoding with intensifying seasonality, and the competitive performance of probabilistic methods like CSDI in handling more complex data distributions.

### Case Study

To intuitively demonstrate the distinct characteristics of point and probabilistic estimations, a case study was conducted on short-term datasets. Figure 10 illustrates that point estimation yields single-valued, deterministic estimates, in contrast to probabilistic methods, which model continuous data distributions as depicted in Figure 11. This modeling of data distributions captures the uncertainty in forecasts, aiding decision-makers in fields such as weather and finance to make more informed choices. It is also observed that while both methods align well with ground truth values in short-term forecasting datasets, they struggle to accurately capture outliers, particularly noted in the Wikipedia dataset.

### Model Efficiency

For reference, detailed results regarding memory usage and time efficiency for five representative models on long-term forecasting datasets are provided here. Table 24 displays the computation memory of various models with a forecasting horizon set to 96. Additionally, Table 25 compares the inference time of these models on long-term forecasting datasets, illustrating the impact of changes in the forecasting horizon.

\begin{table}
\begin{tabular}{l|c c|c c|c c|c c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{2}{c}{Normal} & \multicolumn{2}{c}{Strong Trend} & \multicolumn{2}{c}{Strong Seasonality} & \multicolumn{2}{c}{Complex Distribution} \\  & CRPS & NMAE & CRPS & NMAE & CRPS & NMAE & CRPS & NMAE \\ \hline DLinear & \(0.013\) & \(0.013\) & \(\mathbf{0.001}\) & \(\mathbf{0.001}\) & \(0.014\) & \(0.014\) & \(0.301\) & \(0.301\) \\ PatchTST & \(\mathbf{0.012}\) & \(\mathbf{0.012}\) & \(\mathbf{0.001}\) & \(\mathbf{0.001}\) & \(\mathbf{0.012}\) & \(\mathbf{0.012}\) & \(0.275\) & \(\mathbf{0.275}\) \\ TimeGrad & \(0.024\) & \(0.032\) & \(0.042\) & \(0.048\) & \(0.022\) & \(0.028\) & \(0.283\) & \(0.338\) \\ CSDI & \(0.013\) & \(0.014\) & \(0.010\) & \(0.007\) & \(0.020\) & \(0.027\) & \(\mathbf{0.269}\) & \(0.301\) \\ \hline \hline \end{tabular}
\end{table}
Table 23: Results on synthetic datasets. The look-back window and forecasting horizon are 30.

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline Metric & Dataset & DLinear & PatchTST & LSTM NVP & TimeGrad & CSDI \\ \hline \multirow{4}{*}{NPARAMS (MB)} & ETT\(n1\) & 0.075 & 2.145 & 1.079 & 1.233 & 1.720 \\  & Electricity-L & 0.076 & 2.146 & 3.680 & 3.472 & 1.370 \\  & Traffic-L & 0.078 & 2.149 & 15.926 & 8.298 & 1.390 \\  & Weather-L & 0.075 & 2.145 & 3.085 & 0.574 & 1.721 \\  & Exchange-L & 0.075 & 0.135 & 1.979 & 0.488 & 1.720 \\ \hline \multirow{4}{*}{Max GPU Mem. (GB)} & ETT\(n1\) & 0.002 & 0.009 & 0.010 & 0.012 & 0.027 \\  & Electricity-L & 0.060 & 0.068 & 0.129 & 0.128 & 1.411 \\ \cline{1-1}  & Traffic-L & 0.161 & 0.168 & 0.361 & 0.333 & 9.102 \\ \cline{1-1}  & Weather-L & 0.004 & 0.012 & 0.021 & 0.012 & 0.070 \\ \cline{1-1}  & Exchange-L & 0.002 & 0.002 & 0.013 & 0.008 & 0.030 \\ \hline \hline \end{tabular}
\end{table}
Table 24: Computation memory. The batch size is 1 and the prediction horizon is set to 96.

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline
**Dataset** & **Normal** & **Strong Trend** & **Strong Seasonality** & **Complex Distribution** \\ \hline Trend \(F_{T}\) & 0.105 & 0.554 & 0.105 & 0.064 \\ Seasonality \(F_{S}\) & 0.302 & 0.302 & 0.791 & 0.190 \\ \hline JS Div. & 0.261 & 0.248 & 0.272 & 0.469 \\ \hline \hline \end{tabular}
\end{table}
Table 22: Quantitative assessment of intrinsic characteristics for synthetic datasets. The JS Div denotes Jensen–Shannon divergence, where a lower score indicates closer approximations to a Gaussian distribution.

### Further discussion on uni- / multi-variate modeling

Our experiments indicate that the choice between univariate and multivariate modeling is not a primary factor in fulfilling the essential forecasting needs considered in this paper.

#### d.8.1 Discussion

We discuss the differences between univariate and multivariate modeling from two perspectives:

* Dataset Perspective: Whether the dataset is prepared for univariate or multivariate benchmarking.
* Model Perspective: How the model handles multivariate data, treating each variable channel independently or not.

Dataset PerspectiveAll datasets listed in Table 5 are typically referred to as multivariate datasets, indicating that there may be strong connections across different variables. Despite this implication, when developing forecasting models, we can treat each variable channel independently, essentially turning a multivariate dataset into a univariate setup. In contrast, some datasets, like M4, M5, and

Figure 10: Point forecasts from the PatchTST model and the ground-truth value on short-term forecasting datasets.

TOURISM listed in Table 20, explicitly serve univariate modeling. We rarely see multivariate models being developed for these univariate cases.

Model PerspectiveWe have observed different preferences for univariate and multivariate modeling. Existing models can be categorized into three groups:

* **Native Univariate Models*
* Classical models like N-BEATS and N-HiTS.
* Most time-series foundation models, such as TimesFM and Chronos.
* **Native Multivariate Models*
* Most probabilistic models, such as CSDI and TimeGrad.
* Some point forecasting models, such as Informer and Autoformer.
* **Hybrid Models of Univariate and Multivariate Modeling*
* Some classical models, such as PatchTST.
* Some time-series foundation models, such as MOIRAI.

Figure 11: Forecasting intervals from the TimeGrad model and the ground-truth value on short-term forecasting datasets.

Native univariate models can also be applied to multivariate datasets by treating them as univariate cases. Similarly, native multivariate models can be applied to univariate datasets by setting the variable dimension to 1. Hybrid models typically include specific modes to activate univariate and multivariate functionalities. For example, in PatchTST, we can use a shared forecasting head for univariate modeling or assign a specific forecasting head for each variable channel to differentiate different variables.

We compile a summary table (Table 26) delineating how models from each branch address the multivariate aspect. Despite a thorough investigation, we have not identified a clear pattern linking the modeling of cross-channel interactions to overall model performance. A notable trend is the prevalent use of a channel-mixing approach in most studies. However, findings are diverse; models like DLinear and PatchTST suggest that processing channels independently can yield superior results, while others like CSDI indicate that explicit modeling of cross-channel interactions offers significant advantages. This diversity underscores the ongoing exploration of the impact of cross-channel interactions on forecasting performance.

Figure 12: Comparison of computational efficiency. The forecasting horizon is set to 96 for calculating memory usage.

\begin{table}
\begin{tabular}{l|c|c c c c c} \hline \hline Dataset & Pred len & DLinear & PatchTST & LSTM NVP & TimeGrad & CSDI \\ \hline \multirow{4}{*}{ETTm1-L} & 96 & \(0.0003\pm 0.0000\) & \(0.0003\pm 0.0000\) & \(0.0352\pm 0.0007\) & \(4.1067\pm 0.0504\) & \(16.3280\pm 0.0747\) \\  & 192 & \(0.0003\pm 0.0000\) & \(0.0003\pm 0.0000\) & \(0.0697\pm 0.0020\) & \(7.8979\pm 0.0403\) & \(25.8378\pm 0.3124\) \\  & 336 & \(0.0003\pm 0.0000\) & \(0.0003\pm 0.0000\) & \(0.1221\pm 0.0044\) & \(13.6197\pm 0.1023\) & \(39.8832\pm 0.2157\) \\  & 720 & \(0.0004\pm 0.0000\) & \(0.0003\pm 0.0000\) & \(0.2603\pm 0.0020\) & \(28.6074\pm 1.1346\) & \(86.1862\pm 0.1863\) \\ \hline \multirow{4}{*}{Electricity-L} & 96 & \(0.0004\pm 0.0000\) & \(0.0045\pm 0.0001\) & \(0.1783\pm 0.0006\) & \(13.8439\pm 0.0054\) & \(388.3150\pm 0.2155\) \\  & 192 & \(0.0006\pm 0.0000\) & \(0.00046\pm 0.0000\) & \(0.3700\pm 0.0010\) & \(27.6683\pm 0.0368\) & \(659.4284\pm 0.2003\) \\  & 336 & \(0.0008\pm 0.0000\) & \(0.0049\pm 0.0000\) & \(0.7157\pm 0.0028\) & \(48.4456\pm 0.0279\) & - \\  & 720 & \(0.0015\pm 0.0000\) & \(0.0057\pm 0.0000\) & \(2.0785\pm 0.0186\) & \(104.1473\pm 0.1465\) & - \\ \hline \multirow{4}{*}{Traffic-L} & 96 & \(0.0010\pm 0.0001\) & \(0.0102\pm 0.0000\) & \(3.695\pm 0.0022\) & \(31.7644\pm 0.0101\) & - \\  & 192 & \(0.0013\pm 0.0000\) & \(0.0106\pm 0.0000\) & \(0.8287\pm 0.0094\) & \(63.5832\pm 0.0060\) & - \\  & 336 & \(0.0020\pm 0.0000\) & \(0.0114\pm 0.0001\) & \(1.6945\pm 0.0026\) & \(111.4147\pm 0.0169\) & - \\  & 720 & \(0.0039\pm 0.0000\) & \(0.0137\pm 0.0000\) & \(5.0963\pm 0.0018\) & \(258.1274\pm 0.6088\) & - \\ \hline \multirow{4}{*}{Weather-L} & 96 & \(0.0002\pm 0.0000\) & \(0.0004\pm 0.0000\) & \(0.0800\pm 0.0016\) & \(4.1261\pm 0.0812\) & \(37.8984\pm 0.0782\) \\  & 192 & \(0.0003\pm 0.0000\) & \(0.0004\pm 0.0000\) & \(0.1568\pm 0.0008\) & \(8.2913\pm 0.5544\) & \(62.0223\pm 0.2329\) \\  & 336 & \(0.0003\pm 0.0000\) & \(0.0004\pm 0.0000\) & \(0.2482\pm 0.0297\) & \(14.24391\pm 0.4891\) & \(96.8704\pm 0.2258\) \\  & 720 & \(0.0003\pm 0.0000\) & \(0.0005\pm 0.0000\) & \(0.5447\pm 0.0249\) & \(29.4407\pm 0.3519\) & \(216.6044\pm 0.4253\) \\ \hline \multirow{4}{*}{Exchange-L} & 96 & \(0.0006\pm 0.0000\) & \(0.0004\pm 0.0000\) & \(0.0284\pm 0.0001\) & \(4.1069\pm 0.0981\) & \(17.8655\pm 0.1282\) \\  & 192 & \(0.0007\pm 0.0000\) & \(0.0004\pm 0.0000\) & \(0.0563\pm 0.0008\) & \(8.1576\pm 0.0911\) & \(28.5456\pm 0.0873\) \\  & 336 & \(0.0007\pm 0.0000\) & \(0.0004\pm 0.0000\) & \(0.0966\pm 0.0007\) & \(14.4593\pm 0.4466\) & \(44.9733\pm 0.3820\) \\  & 720 & \(0.0007\pm 0.0000\) & \(0.0004\pm 0.0000\) & \(0.2085\pm 0.0046\) & \(30.1443\pm 0.5378\) & \(97.7417\pm 0.2606\) \\ \hline \multirow{4}{*}{ILI-L} & 24 & \(0.0002\pm 0.0000\) & \(0.0008\pm 0.0001\) & \(0.0080\pm 0.0001\) & \(1.0427\pm 0.0190\) & \(12.4308\pm 0.1681\) \\  & 192 & \(0.0002\pm 0.0000\) & \(0.0008\pm 0.0000\) & \(0.0121\pm 0.0003\) & \(1.5762\pm 0.0282\) & \(12.7187\pm 0.1344\) \\ \cline{1-1}  & 336 & \(0.0002\pm 0.0000\) & \(0.0008\pm 0.0000\) & \(0.0155\pm 0.0002\) & \(2.1344\pm 0.0660\) & \(12.7386\pm 0.1868\) \\ \cline{1-1}  & 720 & \(0.0002\pm 0.0000\) & \(0.0008\pm 0.0000\) & \(0.0196\pm 0.0004\) & \(2.5787\pm 0.0594\) & \(12.5407\pm 0.0481\) \\ \hline \hline \end{tabular}
\end{table}
Table 25: Comparison of inference time (sec./sample).

#### d.8.2 Additional Experiments on uni- / multi-variate modeling

In Table 27, we include additional experiments comparing univariate and multivariate modeling of PatchTST across different datasets. Our observation is that there is no definitive answer as to which approach is superior; it depends on the nature of the dataset. Some datasets benefit from modeling variable correlations, while others perform better with independent modeling. The performance gaps are not significant.

Similar observations have been reported in MOIRAI, which allows either univariate or multivariate modes by controlling its cross-variate attention masks. When applied to a downstream forecasting scenario, it can search over the validation set to determine which configurations to activate. We believe such a design could serve as a good example of unifying univariate and multivariate modeling.

\begin{table}
\begin{tabular}{l|l|c} \hline \hline Model & Research branch & Process channels independently \\ \hline \multirow{6}{*}{Customized neural architectures} & N-BEATS [53] & ✓ \\  & N-HiTS [11] & ✓ \\  & Autoformer [72] & ✗ \\  & Informer [78] & ✗ \\  & LTSF-Linear [75] & ✗ \\  & PatchTST [49] & ✗ \\  & TimesNet [71] & ✗ \\ \hline \multirow{6}{*}{Probabilistic estimation} & DeepAR [60] & ✓ \\  & GP-copula [59] & ✗ \\  & LSTM NVP [58] & ✗ \\ \cline{1-1}  & LSTM MAF [58] & ✗ \\ \cline{1-1}  & Trans MAF [58] & ✗ \\ \cline{1-1}  & TimeGrad [57] & ✗ \\ \cline{1-1}  & CSDI [62] & ✗ \\ \cline{1-1}  & SPD [7] & ✗ \\ \hline \hline \end{tabular}
\end{table}
Table 26: Summary of how existing models handle multivariate time series.

\begin{table}
\begin{tabular}{l|c|c|c} \hline \hline
**Dataset** & **Pred. Horizon** & **PatchTST (Multivariate)** & **PatchTST (Univariate)** \\ \hline \multirow{3}{*}{ETTh1} & 96 & 0.3239 & 0.3212 \\  & 192 & 0.3609 & **0.3562** \\  & 336 & 0.3763 & **0.3737** \\  & 720 & **0.3882** & 0.3909 \\ \hline \multirow{3}{*}{ETTm1} & 96 & **0.2652** & 0.2739 \\  & 192 & **0.2926** & 0.2961 \\  & 336 & 0.3101 & 0.3188 \\  & 720 & 0.345 & 0.3463 \\ \hline \multirow{3}{*}{Electricity} & 96 & **0.0832** & 0.0857 \\  & 192 & **0.0899** & 0.0912 \\  & 336 & **0.0995** & 0.1001 \\  & 720 & 0.1183 & 0.116 \\ \hline \multirow{3}{*}{Exchange} & 96 & 0.0243 & **0.0235** \\  & 192 & 0.0348 & **0.0336** \\  & 336 & 0.0471 & **0.0462** \\  & 720 & 0.0787 & **0.0777** \\ \hline \multirow{3}{*}{Weather} & 96 & 0.0872 & **0.0837** \\  & 192 & 0.0924 & **0.0858** \\ \cline{1-1}  & 336 & 0.0934 & **0.0903** \\ \cline{1-1}  & 720 & 0.0993 & **0.0953** \\ \hline \hline \end{tabular}
\end{table}
Table 27: The comparison of PatchTST on univariate and multivariate modeling.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] See Sections 5. 3. Did you discuss any potential negative societal impacts of your work? [N/A] 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] We release the ProbTS toolkit, documentation, and running scripts at https://github.com/microsoft/ProbTS. The repository includes parameter configurations for benchmarking experiments, ensuring reproducibility of all results presented in the paper. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] All datasets in this paper are split according to standard practices in previous works. We have specified the hyperparameters in Appendix B.3 The detailed configuration file for each baseline is available on our GitHub page. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] We specify a fixed set of random seeds for each experiment to indicate the error bars. We show the mean and standard deviation of evaluation scores collected from multiple runs. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix B.3.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [Yes] See Appendix B.4. 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] The data utilized in this study is open-source and does not necessitate consent. Acquisition details are available on our GitHub page. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] The data we are using contains no personally identifiable information or offensive content.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]