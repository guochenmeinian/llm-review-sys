# Identifiable Shared Component Analysis of Unpaired Multimodal Mixtures

 Subash Timilsina

School of EECS

Oregon State University

Corvallis, OR 97331

timilsis@oregonstate.edu

&Sagar Shrestha

School of EECS

Oregon State University

Corvallis, OR 97331

shressag@oregonstate.edu

&Xiao Fu

School of EECS

Oregon State University

Corvallis, OR 97331

xiao.fu@oregonstate.edu

###### Abstract

A core task in multi-modal learning is to integrate information from multiple feature spaces (e.g., text and audio), offering modality-invariant essential representations of data. Recent research showed that, classical tools such as _canonical correlation analysis_ (CCA) provably identify the shared components up to minor ambiguities, when samples in each modality are generated from a linear mixture of shared and private components. Such identifiability results were obtained under the condition that the cross-modality samples are aligned/paired according to their shared information. This work takes a step further, investigating shared component identifiability from multi-modal linear mixtures where cross-modality samples are unaligned. A distribution divergence minimization-based loss is proposed, under which a suite of sufficient conditions ensuring identifiability of the shared components are derived. Our conditions are based on cross-modality distribution discrepancy characterization and density-preserving transform removal, which are much milder than existing studies relying on independent component analysis. More relaxed conditions are also provided via adding reasonable structural constraints, motivated by available side information in various applications. The identifiability claims are thoroughly validated using synthetic and real-world data.

## 1 Introduction

The same data entities can often be represented in different feature spaces (e.g., audio, text and image), due to the variety of sensing modalities or domains. Learning common latent components of data from multiple modalities is well-motivated in representation learning. The shared components are considered modality-invariant essential representations of data, which can often enhance performance of downstream tasks by shedding modality-specific noise [1, 2, 3, 4] and avoiding over-fitting [5, 6, 7].

A prominent theoretical aspect of shared component learning lies in _identifiability_ of the components of interest. The literature posed an intriguing theoretical question [1, 2, 8]: If every modality of data is represented by a linear mixture of shared and private components with an unknown mixing system, are the shared components identifiable (up to acceptable ambiguities)? Such component identification problems are often nontrivial due to the ill-posed nature of any linear mixture model (see, e.g., [9, 10, 11, 12, 13, 14]). Interestingly, the work [1] showed that using the classical _canonical correlationanalysis_ (CCA) provably find the shared components up to rotation and scaling. In fact, shared component identification from multimodal/multiview linear mixtures were considered in various contexts (see, e.g., [15; 16; 17; 18]), although some of these works did not model private components. The identifiability results in [1; 2] were generalized to nonlinear mixture models as well [4; 19]. The shared component identification perspective was also related to the success of representation learning in self-supervised learning (SSL) [5; 6; 7].

Nonetheless, the treatment in [1; 2] and the related works [15; 16; 17] all assumed that the cross-modality data are aligned (i.e., paired) according to their shared components. In many applications, such as cross-language information retrieval [20; 21; 22], domain adaptation [23; 24; 25], and biological data translation [26; 27], aligned cross-modality data are hard to acquire, if not outright unavailable. A natural question is: _When the multimodal linear mixtures are unaligned, can the shared latent components still be provably identified under reasonably mild conditions?_

**Existing Studies.** Theoretical characteristics of unaligned multimodal learning were studied under various settings. The work [28] considered a case where one modality is a linear transform of another modality, and showed that the linear transformation is potentially identifiable. The recent work [29] extended this model to a nonlinear transform setting. However, these works did not consider _latent_ component models--yet the latter are more versatile in many ways, e.g., facilitating one-to-many cross-domain translations [30; 31]. The work [32] considered unaligned mixtures of shared and private components, but the assumptions (e.g., the availability of a large amount of modalities) to ensure identifiability may not be easy to satisfy. The most related work is perhaps [8]. But their approach also relied on somewhat stringent assumptions, e.g., that all the latent components are element-wise statistically independent with at most one component being Gaussian. This is because their procedure had to invoke the classical _independent component analysis_ (ICA) [33].

**Contributions.** In this work, we provide a suite of sufficient conditions under which the shared components can be provably identified from unaligned multimodal linear mixtures up to reasonable ambiguities. The model and identification problem are referred to as _unaligned shared component analysis_ (unaligned SCA) in the sequel.

_(i) An Identifiable Learning Loss for Unaligned SCA._ We propose to tackle the unaligned SCA problem by matching the probability distributions of linearly embedded multi-modal data. We show that under reasonable conditions, the linear transformations identifies the shared components up to the same ambiguities as those in the aligned case [1; 2]. The conditions are considerably milder compared to the existing unaligned SCA work [8].

_(ii) Enhanced Identifiability via Structural Constraints._ We come up with two types of structural constraints, motivated by available side information in applications, to further relax the identifiability conditions. Specifically, we look into cases where the multi-modal data have similar linear mixing systems and cases where a few cross-domain aligned samples available. We show that by adding constraints accordingly, unaligned SCA are identifiable under much milder conditions.

Our contributions primarily lie in identifiability analysis. Nonetheless, we also show the usefulness of our results in real-world applications, namely, _cross-lingual word retrieval_, _genetic information alignment_ and _image data domain adaptation_. Particularly, it shows that our succinct multimodal linear mixture model can effectively post-process outputs of pre-trained encoders, e.g., those in [34; 35], to improve data representations and enhance downstream task performance.

**Notation.** Notation definitions can be found in Appendix A.

## 2 Background

**Generative Model of Interest.** Following the classical settings in [1; 2; 15; 16; 18], we consider modeling the multi-modal data as linear mixtures. More specifically, we adopt the model in [1; 2]that splits the latent representation of data into shared components and private components:

\[\bm{x}^{(q)}=\bm{A}^{(q)}\bm{z}^{(q)},\quad\bm{z}^{(q)}=[\bm{c}^{\top},(\bm{p }^{(q)})^{\top}]^{\top},\ q=1,2,\] (1)

where \(\bm{x}^{(q)}\in\mathbb{R}^{d^{(q)}}\) represents the data from the \(q\)th modality, \(\bm{z}^{(q)}\in\mathbb{R}^{d_{\mathrm{C}}+d_{\mathrm{P}}^{(q)}}\) represents the corresponding latent code, \(\bm{c}\in\mathbb{R}^{d_{\mathrm{C}}}\) and \(\bm{p}^{(q)}\in\mathbb{R}^{d_{\mathrm{P}}^{(q)}}\) stand for the shared components and the private components, respectively. The data \(\bm{x}^{(q)}\)'s are assumed to be zero-mean, which can be enforced by centering. Note that the positions of \(\bm{c}\) and \(\bm{p}_{q}\) are not necessarily arranged as \([\bm{c}^{\top},(\bm{p}^{(q)})^{\top}]^{\top}\) (more generally, \(\bm{z}^{(q)}=\bm{\Pi}^{(q)}[\bm{c}^{\top},(\bm{p}^{(q)})^{\top}]^{\top}\) with an unknown permutation matrix \(\bm{\Pi}^{(q)}\)). However, the representation in (1) is without loss of generality as one can define \(\bm{A}^{(q)}:=\bm{A}^{(q)}(\bm{\Pi}^{(q)})^{\top}\) to reach the representation in (1). For all the domains, we have

\[\bm{c}\sim\mathbb{P}_{\bm{c}},\quad\bm{p}^{(q)}\sim\mathbb{P}_{\bm{p}^{(q)}},\] (2)

where \(\mathbb{P}_{\bm{c}}\) and \(\mathbb{P}_{\bm{p}^{(q)}}\) represent the distributions of the shared components and the domain-private components, respectively. Under (1), the two different range spaces \(\operatorname{range}(\bm{A}^{(q)})\) for \(q=1,2\) represent two feature spaces. Then latent \(\bm{p}^{(q)}\) further distinguishes the modalities and often has interesting physical interpretation. For example, some vision literature use \(\bm{c}\) to model "content" and \(\bm{p}^{(q)}\) "style" of the images [31, 36]. In cross-lingual word embedding retrieval [2], \(\bm{c}\) represents the semantic meaning of the words, while \(\bm{p}^{(q)}\) represents the language-specific components. The goal of SCA boils down to finding linear operators to recover \(\bm{c}\) to a reasonable extent.

**Aligned SCA: Identifiability of CCA and Extensions.** Learning \(\bm{c}\) without knowing \(\bm{A}^{(q)}\) is a typical component analysis problem. Learning latent components from _linear mixture models_ (LMMs) like \(\bm{x}=\bm{A}\bm{z}\) lacks identifiability in general, due to the bilinear nature of the models. This is because one can find an infinite number of invertible matrices \(\bm{B}\) such that \(\bm{x}=\bm{A}\bm{B}\bm{B}^{-1}\bm{z}\). Then, both \((\bm{A},\bm{z})\) and \((\bm{A}\bm{B},\bm{B}^{-1}\bm{z})\) can fit to the data \(\bm{x}\), making the problem ill-posed in tens of solution uniqueness; see, e.g., [9, 37] and more discussions in Sec. 5. Nonetheless, the works [1, 2] studied the identifiability of \(\bm{c}\) under the model (1), using the assumption that the cross-modality samples share the same \(\bm{c}\) are aligned. In particular, [1] formulated the \(\bm{c}\)-identification problem as a CCA problem:

\[\underset{\{\bm{Q}^{(q)}\}_{q=1}^{2}}{\text{minimize}} \mathbb{E}\left[\left\|\bm{Q}^{(1)}\bm{x}^{(1)}-\bm{Q}^{(2)}\bm{x }^{(2)}\right\|_{2}^{2}\right]\] (3a) \[\operatorname{subject\ to} \bm{Q}^{(q)}\mathbb{E}\left[\bm{x}^{(q)}(\bm{x}^{(q)})^{\top} \right](\bm{Q}^{(q)})^{\top}=\bm{I}\quad q=1,2,\] (3b)

where \(\bm{Q}^{(q)}\in\mathbb{R}^{d_{C}\times d^{(q)}}\). The expectation in (3a) is taken from the joint distribution of the _aligned pairs_\(\mathbb{P}_{\bm{x}^{(1)},\bm{x}^{(2)}}\), where every pair \((\bm{x}^{(1)},\bm{x}^{(2)})\) shares the same \(\bm{c}\). The formulation aims to find \(\bm{Q}^{(q)}\) such that the transformed representations of the aligned pairs \(\bm{Q}^{(1)}\bm{x}^{(1)}\) and \(\bm{Q}^{(2)}\bm{x}^{(2)}\) are equal. In [1], it was shown that

\[\widehat{\bm{Q}}^{(q)}\bm{x}^{(q)}=\bm{\Theta}\bm{c}\] (4)

under mild conditions (see Appendix E.1 for details), where \((\widehat{\bm{Q}}^{(1)},\widehat{\bm{Q}}^{(2)})\) is an optimal solution of the CCA formulation and \(\bm{\Theta}\) is a certain non-singular matrix. Eq. (4) means that \(\widehat{\bm{Q}}^{(q)}\) finds the range space where \(\bm{c}\) lives in, i.e., \(\operatorname{range}(\bm{A}^{(q)}_{1:d_{C}})\) under our notation.

**Unaligned SCA: Existing Result and Theoretical Gap.** The work in [8] studied the identifiability of \(\bm{c}\) under (1) when \(\bm{x}^{(1)}\) and \(\bm{x}^{(2)}\) are _unaligned_. Their approach works under the condition _that the elements of \(\bm{z}^{(q)}=[\bm{c}^{\top},(\bm{p}^{(q)})^{\top}]^{\top}\) are mutually statistically independent_. There, \(\widehat{\bm{z}}^{(q)}=\bm{\Pi}^{(q)}\bm{\Sigma}^{(q)}\bm{z}^{(q)}\) is assumed to have been estimated by ICA, where \(\bm{\Pi}^{(q)}\) and \(\bm{\Sigma}^{(q)}\) represent the scaling and permutation ambiguities, respectively, which cannot be removed by ICA. The work [8] assumed \(\bm{\Sigma}^{(q)}=\bm{I}\) by imposing a unit-variance assumption on all the \(z^{(q)}_{i}\)'s. Then, a cross-domain matching algorithm is used to match the shared elements in \(\widehat{\bm{z}}^{(1)}\) and \(\widehat{\bm{z}}^{(2)}\). The formulation can be summarized as finding \(d_{C}\) pairs of non-repetitive \((i,j)\) such that \(\bm{e}_{i}^{\top}\widehat{\bm{z}}^{(1)}\) and \(\bm{e}_{j}^{\top}\widehat{\bm{z}}^{(2)}\) have identical distributions, where \(\bm{e}_{i}\) is the \(i\)th unit vector. Denote \(\widehat{c}_{m}^{(1)}=\bm{e}_{i_{m}}^{\top}\widehat{\bm{z}}^{(1)}\) and \(\widehat{c}_{m}^{(2)}=\bm{e}_{j_{m}}^{\top}\widehat{\bm{z}}^{(2)}\) for \(m\in[d_{C}]\). It can be shown that

\[\widehat{c}_{m}^{(q)}=kc_{\bm{\pi}(m)}^{(q)},\ m\in[d_{C}],\] (5)

where \(k\in\{+1,-1\}\) and \(\bm{\pi}\) is a permutation of \(\{1,\ldots,d_{C}\}\) (see details in Appendix E.2 summarized from [8]). This method effectively applies ICA to each modality, and thus the ICA identifiability conditions [33] have to met by \(\bm{x}^{(1)}\) and \(\bm{x}^{(2)}\) individually. However, if one only aims to extract \(\bm{\Theta}\bm{c}\) as in CCA, these assumptions appear to be overly stringent.

## 3 Proposed Approach

**Unaligned SCA: Problem Formulation** We assume that \(\bm{x}^{(q)}\)'s are zero-mean. We use the notation from CCA in (3a). However, since no aligned samples are available, we replace the sample-level matching objective with a distribution matching (DM) module, as DM can be carried out without sample level alignment:

\[\mathrm{find} \bm{Q}^{(q)}\in\mathbb{R}^{d_{C}\times d^{(q)}},\;q=1,2,\] (6a) subject to \[\bm{Q}^{(1)}\bm{x}^{(1)}\stackrel{{(\mathrm{d})}}{ {=}}\bm{Q}^{(2)}\bm{x}^{(2)},\] (6b) \[\bm{Q}^{(q)}\mathbb{E}\left[\bm{x}^{(q)}(\bm{x}^{(q)})^{\top} \right](\bm{Q}^{(q)})^{\top}=\bm{I}\quad q=1,2.\] (6c)

where "\(\bm{u}\stackrel{{(\mathrm{d})}}{{=}}\bm{v}\)" means the distributions of \(\bm{u}\) and \(\bm{v}\) are the same.

The formulation in (6) can be realized using various distribution matching tools, e.g., _maximum mean discrepancy_ (MMD) [38] and _Wasserstein distance_[39]. We use the adversarial loss:

\[\min_{\bm{Q}^{(1)},\bm{Q}^{(2)}}\max_{f}\,\mathbb{E}_{\bm{x}^{(1)}}\log\left( f(\bm{Q}^{(1)}\bm{x}^{(1)})\right)+\mathbb{E}_{\bm{x}^{(2)}}\log\left(1-f(\bm{Q} ^{(2)}\bm{x}^{(2)})\right)+\lambda\sum_{q=1}^{2}\mathcal{R}\left(\bm{Q}^{(q)} \right),\] (7)

The first and second terms comprise the adversarial loss from GAN [40]. It finds \(\bm{Q}^{(q)}\) to confuse the best-possible discriminator \(f:\mathbb{R}^{d_{C}}\rightarrow\mathbb{R}\), where \(f\) is represented by a neural network in practice. It is well known that the minimax optimal point of the first two terms is attained when (6b) is met [40]. We use \(\mathcal{R}(\bm{Q}^{(q)})=\|\bm{Q}^{(q)}\mathbb{E}[\bm{x}^{(q)}(\bm{x}^{(q)}) ^{\top}](\bm{Q}^{(q)})^{\top}-\bm{I}\|_{\mathrm{F}}^{2}\) to "lift" the constraints. This way, the learning criterion in (7) can be readily handled by any off-the-shelf adversarial learning tools.

**Identifability of Unaligned SCA** As we saw in Theorem 4, CCA identifies \(\widehat{\bm{Q}}^{(q)}\bm{x}^{(q)}=\bm{\Theta}\bm{c}\) where \(\bm{\Theta}\in\mathbb{R}^{d_{C}\times d_{C}}\) under the settings of aligned SCA. Establishing a similar result for unaligned SCA is much more challenging. First, it is unclear if (6b) could disentangle \(\bm{c}\) from \(\bm{p}^{(q)}\). In general, \(\bm{Q}^{(q)}\bm{x}^{(q)}\) could still be a mixture of \(\bm{c}\) and \(\bm{p}^{(q)}\) yet (6b) still holds (e.g., when both \(\bm{c}\) and \(\bm{p}^{(q)}\) are Gaussian.)

Second, even when the disentanglement is attained via enforcing (6b) and we have \(\bm{Q}^{(q)}\bm{x}^{(q)}=\bm{\Theta}^{(q)}\bm{c}\), in general it does not hold that \(\bm{\Theta}^{(1)}=\bm{\Theta}^{(2)}\). This is because \(\bm{\Theta}^{(1)}\bm{c}\stackrel{{(\mathrm{d})}}{{=}}\bm{\Theta}^ {(2)}\bm{c}\) where \(\bm{\Theta}^{(1)}\neq\bm{\Theta}^{(2)}\) can still be perfectly met (e.g., when \(\mathbb{P}_{\bm{\Theta}^{(q)}\bm{c}}\) is symmetric Gaussian in Fig. 2 ). However, \(\bm{\Theta}^{(1)}\neq\bm{\Theta}^{(2)}\) means that the extracted representations from the two modalities are not matched. This creates challenges for applications like cross-domain information retrieval, language translation, or domain adaptation.

Our intuition is as follows: If the two distributions \(\mathbb{P}_{\bm{c},\bm{p}^{(1)}}\) and \(\mathbb{P}_{\bm{c},\bm{p}^{(2)}}\) are very different, then \(\bm{Q}^{(1)}\bm{x}^{(1)}\stackrel{{(\mathrm{d})}}{{=}}\bm{Q}^{(2 )}\bm{x}^{(2)}\) cannot hold unless \(\bm{Q}^{(q)}\bm{A}^{(q)}=[\bm{\Theta}^{(q)},\bm{0}]\). We use the following to characterize such difference between the joint distributions:

**Assumption 1** (Modality Variability).: _For any two linear subspaces \(\mathcal{P}^{(q)}\subset\mathbb{R}^{d_{C}+d_{\mathbf{P}}^{(q)}},\;q=1,2,\) with \(\dim(\mathcal{P}^{(q)})=d_{\mathbf{P}}^{(q)}\), \(\mathcal{P}^{(q)}\neq\mathbf{0}\times\mathbb{R}^{d_{\mathbf{P}}^{(q)}}\) and linearly independent vectors \(\{\boldsymbol{y}_{i}^{(q)}\in\mathbb{R}^{c_{\mathbf{C}}+d_{\mathbf{P}}^{(q)} }\}_{i=1}^{d_{\mathbf{C}}},\;q=1,2\), the sets \(\mathcal{A}^{(q)}=\mathrm{conv}\{\mathbf{0},\boldsymbol{y}_{1}^{(q)},\ldots, \boldsymbol{y}_{d_{C}}^{(q)}\}+\mathcal{P}^{(q)},\;q=1,2,\) are such that if \(\mathbb{P}_{\mathbf{c},\boldsymbol{p}^{(q)}}[\mathcal{A}^{(q)}]>0\) for \(q=1\) or \(q=2\), then there exists a \(k\in\mathbb{R}\) such that the joint distributions \(\mathbb{P}_{\mathbf{c},\boldsymbol{p}^{(1)}}[k\mathcal{A}^{(1)}]\neq\mathbb{P }_{\mathbf{c},\boldsymbol{p}^{(2)}}[k\mathcal{A}^{(2)}]\), where \(k\mathcal{A}^{(q)}=\{ka\;|\;a\in\mathcal{A}^{(q)}\}\)._

The condition in Assumption 1 is a geometric way to characterize the difference between \(\mathbb{P}_{\mathbf{c},\boldsymbol{p}^{(1)}}\) and \(\mathbb{P}_{\mathbf{c},\boldsymbol{p}^{(2)}}\)--if the joint distributions have different measures for all possible "stripes", each being a direct sum of a subspace and a convex hull (see Fig. 2), then \(\mathbb{P}_{\mathbf{c},\boldsymbol{p}^{(1)}}\) and \(\mathbb{P}_{\mathbf{c},\boldsymbol{p}^{(2)}}\) must be very different. Note that the difference is contributed by the modality-specific term \(\boldsymbol{p}^{(q)}\), and thus we call this condition "modality variability". Modality variability is similar to the "domain variability" used in [32, 41]--both characterize the discrepancy of the joint probabilities \(\mathbb{P}_{\mathbf{c},\boldsymbol{p}^{(1)}}\) and \(\mathbb{P}_{\mathbf{c},\boldsymbol{p}^{(2)}}\). However, there are key differences: The domain variability was defined in a unified latent domain over _arbitrary_ sets \(\mathcal{A}\), which could be stringent. Instead, we use the fact that (6) relies on linear operations to construct \(\mathcal{A}^{(q)}\), which makes the condition defined over a much smaller class of sets--thereby largely relaxing the requirements. Restricting \(\mathcal{A}^{(q)}\) to be stripes also makes the modality variability condition much more relaxed compared to the domain variability condition.

We show the following:

**Theorem 1**.: _Under Assumption 1 and the generative model in (1), denote any solution of (6) as \(\widehat{\boldsymbol{Q}}^{(q)}\)\(q=1,2\). Then, if the mixing matrices \(\boldsymbol{A}^{(q)}\) are full column ranks and \(\mathbb{E}[\boldsymbol{c}\boldsymbol{c}^{\top}])\) is full rank, we have \(\widehat{\boldsymbol{Q}}^{(q)}\boldsymbol{x}^{(q)}=\boldsymbol{\Theta}^{(q)} \boldsymbol{c}\). In addition, assume that either of the following is satisfied:_

* _The individual elements of the content components are statistically independent and non-Gaussian. In addition,_ \(c_{i}\stackrel{{(\mathsf{d})}}{{\neq}}kc_{j},\forall i\neq j, \forall k\in\mathbb{R}\) _and_ \(c_{i}\stackrel{{(\mathsf{d})}}{{\neq}}-c_{i},\forall i\)_, i.e., the marginal distributions of the content elements cannot be matched with each other by mere scaling._
* _The support of_ \(\mathbb{P}_{\mathbf{c}}\)_, denoted by_ \(\mathcal{C}\)_, is a hyper-rectangle, i.e.,_ \(\mathcal{C}=[-a_{1},a_{1}]\times\cdots\times[-a_{d_{C}},a_{d_{C}}]\)_. Further, suppose that_ \(c_{i}\stackrel{{(\mathsf{d})}}{{\neq}}kc_{j},\forall i\neq j, \forall k\in\mathbb{R}\) _and_ \(c_{i}\stackrel{{(\mathsf{d})}}{{\neq}}-c_{i},\forall i\)_._

_Then, we have \(\widehat{\boldsymbol{Q}}^{(q)}\boldsymbol{x}^{(q)}=\boldsymbol{\Theta} \boldsymbol{c}\), i.e., \(\boldsymbol{\Theta}^{(q)}=\boldsymbol{\Theta}\) for all \(q=1,2\), where \(\boldsymbol{\Theta}^{(q)}\)._

In Theorem 1, Assumption 1 is used to guarantee \(\widehat{\boldsymbol{Q}}^{(q)}\boldsymbol{x}^{(q)}=\boldsymbol{\Theta}^{(q)} \boldsymbol{c}\) and either of conditions (a) or (b) is used to make sure \(\boldsymbol{\Theta}^{(1)}=\boldsymbol{\Theta}^{(2)}\). Note that both (a) and (b) are milder than those in [8] (cf. Theorem 5), where the element-wise statistical independence of \(\boldsymbol{z}^{(q)}\) was relied on to find shared representation of \(\boldsymbol{x}^{(1)}\) and \(\boldsymbol{x}^{(2)}\). The proof is in Appendix B.

**Numerical Validation.** In Fig. 3, the top and bottom rows validate Theorem 1 under the assumptions in (a) and (b), respectively. In the top row, we set \(\boldsymbol{c}\in\mathbb{R}^{2}\), where \(c_{1}\) is sampled from Gaussian mixtures with three components and \(c_{2}\) is sampled from a Gamma distribution (and \(c_{1}\perp\!\!\!\perp c_{2}\)). We set \(p^{(1)}\) and \(p^{(2)}\) as one-dimensional Laplacian and uniform distributions. In the bottom row, the dimensions of \(\boldsymbol{c}\) and \(\boldsymbol{p}^{(q)}\) for \(q=1,2\) are unchanged, but their distributions are replaced in order to satisfy conditions in (b) (see details in Appendix F). One can see that clearly \(\widehat{\boldsymbol{c}}^{(q)}=\boldsymbol{\Theta}\boldsymbol{c}\); i.e., the learned \(\widehat{\boldsymbol{c}}^{(q)}\) for \(q=1,2\) are identically rotated and scaled versions of \(\boldsymbol{c}\).

A remark is that our framework still allows to identify individual \(c_{i}\)'s as in [8].

**Corollary 1**.: _Under the conditions in Theorem 1 (a), Assume that at most one \(c_{i}\) for \(i\in[d_{\mathrm{C}}]\) is Gaussian. Then, the components of \(\boldsymbol{c}\) are identifiable up to permutation and scaling ambiguities by applying ICA to \(\widehat{\boldsymbol{c}}^{(q)}=\widehat{\boldsymbol{Q}}^{(q)}\boldsymbol{x}^{ (q)}\) for either \(q=1\) or \(q=2\)._

The corollary means that to identify individual \(c_{i}\), using our formulation still enjoys much milder conditions relative to [8]. Specifically, our condition only specifies the independence among elements of \(\boldsymbol{c}\), but the condition in [8] needs that all the elements in \(\boldsymbol{z}^{(q)}=[\boldsymbol{c}^{\top},(\boldsymbol{p}^{(q)})^{\top}]^{\top}\) are independent.

## 4 Enhanced Identifiability via Structural Constraints

Theorem 1 was well-supported by the synthetic data experiments. However, our experiments found that the learning criterion (6) often struggles to produce sensible results in some applications. Our conjecture is that the Assumptions in Theorem 1 (a) and (b) might not have been satisfied by the real data under our tests. Although they are not necessary conditions for identifiability, these conditions do indicate that the requirements to guarantee identifiability of unaligned SCA using (6) are nontrivial to meet. In this section, we explore a couple of structural constraints arising from side information in applications to remove the need for the relatively stringent assumptions on \(\bm{c}\).

**Homogeneous Domains.** The first structural constraint that we consider is \(\bm{A}^{(q)}=\bm{A}\) for \(q=1,2\). This model is motivated by the fact that advanced representation learning tools, e.g., self-supervised learning tools (e.g., SimCLR [42]) and foundation models (e.g., CLIP [35]), are already capable of mapping the data clusters to a shared linearly separable space--which indicates that the representations share a subspace, i.e., \(\bm{x}^{(q)}\approx\bm{A}\bm{z}^{(q)}\). Under such circumstances, the proposed model and method can be used to further process the data by discarding the private components in the latent representation.

Here, we consider the special case of generative process in (1) where,

\[\bm{x}^{(q)}=\bm{A}[\bm{c}^{\top},(\bm{p}^{(q)})^{\top}]^{\top}.\] (8)

Under this model, we look for the shared components by solving (6) with a single \(\bm{Q}=\bm{Q}^{(1)}=\bm{Q}^{(2)}\). We use the following version of the modality variability condition:

**Assumption 2**.: _For any linear subspace \(\mathcal{P}\subset\mathbb{R}^{d_{\mathrm{C}}+d_{\mathrm{P}}},\;d_{\mathrm{P}}= d_{\mathrm{P}}^{(1)}=d_{\mathrm{P}}^{(2)}\), with \(\dim(\mathcal{P})=d_{\mathrm{P}}\), \(\mathcal{P}\neq\bm{0}\times\mathbb{R}^{d_{\mathrm{P}}}\) and linearly independent vectors \(\{\bm{y}_{i}\in\mathbb{R}^{d_{\mathrm{C}}+d_{\mathrm{P}}}\}_{i=1}^{d_{\mathrm{ C}}},\;q=1,2\), the sets \(\mathcal{A}=\mathrm{conv}\{\bm{0},\bm{y}_{1},\dots,\bm{y}_{d_{\mathrm{C}}}\} +\mathcal{P},\;q=1,2.\) are such that if \(\mathbb{P}_{\bm{c},\bm{p}^{(q)}}[\mathcal{A}]>0\) for \(q=1\) or \(q=2\), then the joint distributions \(\mathbb{P}_{\bm{c},\bm{p}^{(1)}}[k\mathcal{A}]\neq\mathbb{P}_{\bm{c},\bm{p}^{( 2)}}[k\mathcal{A}]\) for some \(k\in\mathbb{R}\)._

**Theorem 2**.: _Consider the mixture model in (8). Assume that \(\mathrm{rank}(\bm{A})=d_{\mathrm{C}}+d_{\mathrm{P}}\) and \(\mathrm{rank}(\mathbb{E}[\bm{c}\bm{c}^{\top}])=d_{\mathrm{C}}\), and that Assumption 2 holds. Denote \(\widehat{\bm{Q}}\) as any solution of (6) by constraining \(\bm{Q}=\bm{Q}^{(1)}=\bm{Q}^{(2)}\). Then, we have \(\widehat{\bm{Q}}\bm{x}^{(q)}=\bm{\Theta}\bm{c}\)._

One can see that the conditions (a) and (b) in Theorem 1 are completely removed, if the structure \(\bm{A}^{(1)}=\bm{A}^{(2)}\) is imposed. In fact, the result in Theorem 2 is expected and readily seen from the proof of Theorem 1, as the cause for \(\bm{\Theta}^{(1)}\neq\bm{\Theta}^{(2)}\) is the use of two different \(\bm{Q}^{(q)}\)'s. Nonetheless, this simple variation will prove useful in a series of real-data experiments.

**The Weakly Supervised Case.** Another way to add structural constraints is to use available auxiliary information. For example, some datasets have weak annotations and selected pairs; see, e.g., [43; 44].

**Assumption 3** (Weak Supervision).: _There exist a set of available aligned samples \((\bm{x}_{\ell}^{(1)},\bm{x}_{\ell}^{(2)})\) for \(\ell\in\mathcal{L}\) such that \(\bm{x}_{\ell}^{(q)}=\bm{A}^{(q)}\bm{z}_{\ell}^{(q)},\;\bm{z}_{\ell}^{(q)}=[ \bm{c}_{\ell}^{\top},(\bm{p}_{\ell}^{(q)})^{\top}]^{\top};\) i.e., \((\bm{x}_{\ell}^{(1)},\bm{x}_{\ell}^{(2)})\) share the same \(\bm{c}_{\ell}\)._

Figure 3: Validation of Theorem 1. Top row: results under assumption (a). Bottom row: results under assumption (b).

The condition can be added into our formulation in (6) as a constraint, i.e.,

\[\bm{Q}^{(1)}\bm{x}_{\ell}^{(1)}=\bm{Q}^{(2)}\bm{x}_{\ell}^{(2)},\ \forall\ell\in \mathcal{L}.\] (9)

In the next theorem, we show that the incorporation of aligned samples helps relax conditions (a) and (b) in Theorem 1:

**Theorem 3**.: _Assume that Assumption 1 is satisfied, that \(|\mathcal{L}|\geq d_{\mathrm{C}}\) paired samples \((\bm{x}_{\ell}^{(1)},\bm{x}_{\ell}^{(2)})\) are available, that \(\bm{A}^{(q)}\) for \(q=1,2\) have full column rank, and that \(\mathbb{P}_{\bm{c}}\) is absolutely continuous. Denote \((\widehat{\bm{Q}}^{(1)},\widehat{\bm{Q}}^{(2)})\) as any optimal solution of (6) under the constraint (9). Then, we have \(\widehat{\bm{Q}}^{(q)}\bm{x}^{(q)}=\bm{\Theta}\bm{c}\)._

The proof and synthetic data validation can be found in Appendices D and F, respectively. Note that to realize (9), one only needs to add a regularization term \(\beta\sum_{\ell\in\mathcal{L}}\|\bm{Q}^{(1)}\bm{x}_{\ell}^{(1)}-\bm{Q}^{(2)} \bm{x}_{\ell}^{(2)}\|_{2}^{2}\) to the loss in (7), where \(\beta\geq 0\) is a tunable parameter. The overall loss is still differentiable and thus can be easily handled by gradient based approaches.

A remark is that our weakly supervised formulation can use as few as \(d_{\mathrm{C}}\) pairs of \((\bm{x}_{\ell}^{(1)},\bm{x}_{\ell}^{(2)})\) to establish identifiability of shared component. In contrast, CCA requires at least \(d_{\mathrm{C}}+d_{\mathrm{P}}^{(1)}+d_{\mathrm{P}}^{(2)}\) pairs to attain the same identifiability (cf. Appendix. E.1).

**Private Component Identifiability.** Although our focus is shared component identification, we show that private components are also identifiable with additional assumptions; see Appendix H.

## 5 Related Works

_Identifiability of Component Analysis under Linear Mixture Models._ Various component analysis models were studied in the past several decades, e.g., principal component analysis [45], independent component analysis [33], sparse component analysis [10; 12], bounded component analysis [13], simplex component analysis [46; 47], and polytopic component analysis [14]--motivated by their applications in dimensionality reduction, representation learning, and latent variable identification (see, e.g., topic mining [48; 49], hyperspectral unmixing [46; 47], audio/speech separation [33] and community detection [50]). The classical component analysis tools mostly study a single modality. The identifiability results under these models are well developed and documented.

_Identifiability of Shared Components from Aligned Modalities._ Modeling multimodal data as two or more linear/nonlinear mixtures of latent components was considered in CCA-related works [1; 2; 15; 19], _independent vector analysis_ (IVA) works [17; 18], multiview ICA works [16; 51], and SSL works [52; 5; 6; 7]. Partitioning the latent components into shared and private blocks was considered in [1; 2; 4; 5; 7; 52]. Shared component identifiability was established at the block level (see, e.g., [1; 2; 5]) and the individual component level (e.g., [51]) in these works. Nonetheless, they all rely on completely paired/aligned cross-modality samples, which we do not use in this work.

_Distribution Matching and Unaligned Multimodal Analysis._ Using distribution matching in unaligned multimodal data analytics for different purpose also has a long history; see applications in image-to-image translation [53], domain adaptation [54], cross-platform image super-resolution [55], and cross-domain information retrieval [21]. The recent works [56] and [57] pointed out the identifiability challenge and the existence of density-preserving transforms. The works in [28; 29] started studying the uniqueness issues in distribution matching. However, the latent mixture models were not studied in this line of work.

_Identifiability of Unaligned SCA._ The works in [32; 41] investigated the shared component identifiability when the multimodal data are nonlinear mixtures of content and style (which are shared and private components, respectively) under the same mixing system. Hence, our identical linear mixing case in Theorem 2 can be understood as a special case of theirs. But their analysis relies on the assumption that all the latent components are statistically independent, which is much stronger than our conditions in Theorem 2. Their results also require that there are a large amount of modalities available. But our proof works for just two modalities. The most related work is [8], which uses the model in (1) in the context of multi-view causal graph learning. As discussed before, their assumptions on the latent components are much stronger than ours (see Corollary 1 and Appendix E.2).

## 6 Numerical Validation

**More Synthetic-Data Validation.** We first validate our proposed method on synthetic data that follows our model; see Appendix F for details.

**Application (i) - Domain Adaptation.** We first test the proposed methods over a number of domain adaptation (DA) tasks. In DA, we have the source domain data \(\{\bm{x}^{(1)}\}\) and the target domain \(\{\bm{x}^{(2)}\}\), respectively. Only the source domain data have labels and the two domains are unaligned. We hope to use our method to find shared representations of source and target, and thus the classifier trained using source data can also work well on the target data.

_Dataset_: We use two standard benchmarks of DA, i.e., _Office-31_[58] and _Office-Home_[59]. The _Office-31_ dataset has 4652 images and 31 categories from three domains, namely, Amazon images (**A**), Webcam images (**W**) and DSLR images (**D**). The _Office-Home_ dataset contains 15,500 images with 65 object classes from four domains, i.e., Artistic images (**Ar**), Clip art images (**Cl**), Product images (**Pr**), and Real-world images (**Rw**).

_Setup_: We first test the homogeneous domain model in Sec. 4. The images are pre-processed using a ResNet50-based image encoder pre-trained over ImageNet1k [42]. As mentioned, it was observed that self-supervised representation encoders find embeddings that are linearly separable [42], which justifies the use of the model \(\bm{x}^{(q)}\approx\bm{A}\bm{z}^{(q)}\) in the embedding domain. After pre-processing, each image is represented by \(d^{(q)}=2048\) features for \(q=1,2\). We set \(d_{\mathrm{C}}=256\) for _Office-31_ and \(d_{\mathrm{C}}=512\) for _Office-Home_. More detailed settings are in Appendix G.

_Baselines and Training Setup_: The baselines are representative DA methods, namely, DANN[25], MDD[60], MCC[61], SDAT[62], and ELS[63]. All the baselines use the same encoder-produced embeddings as inputs; see Appendix G.1 for their configurations. We also use ResNet encoder's outputs as an extra baseline as it learns informative and transferable features from the ImageNet-1K dataset. We follow the training strategies adopted by the baselines [25; 60; 62] to learn a classifier jointly with the shared latent components. This strategy arguably regularizes towards more classification-friendly geometry of the shared features. Therefore we append a cross-entropy (CE) based classifier training module to our loss in (7) that learns our feature extractor \(\bm{Q}\). More details are in Appendix G.1.

_Metric_: The evaluation metric is the classification accuracy in the target domain \(\{\bm{x}^{(2)}\}\). The classifier is trained with the projected source domain \(\widehat{\bm{Q}}\bm{x}^{(1)}\) and the associated labels.

_Result_: Table 1 and Table 2 show the classification accuracy (mean\(\pm\)std) on _Office-31_ and _Office-Home_, respectively. The results are averaged over 5 runs. One can observe that the proposed method offers the best and second best performance in most of the cases. In some tasks (e.g.,"**A\(\rightarrow\)W**", "**Ar\(\rightarrow\)CI**", "**Ar\(\rightarrow\)Pr**" and "**Rw\(\rightarrow\)CI**"), the proposed method outperforms the best-performing baselines by at least 2% in accuracy.

More results on the DA task can be found in Appendix G.1.

**Application (ii) - Single Cell Sequence Analysis.** In biomedical research, it is desired to fuse measurements from multiple sensorial modalities of the same cells, in order to have better characterizations of the cells. However, obtaining multimodal data of the same cells simultaneously is almost impossible, due to the sensing limitations. Therefore, many methods are proposed in the literature for aligning unpaired multi-modal single cell data [27; 64; 65]. We focus on the following two modalities of single-cell data [66]: (1) the RNA sequences \(\{\bm{x}^{(1)}\}\) and (2) the ATAC sequences \(\{\bm{x}^{(2)}\}\).

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|c|c|} \hline
**source \(\rightarrow\) target** & ResNet & DANN & MDD & MCC & SDAT & ELS & Proposed \\ \hline \hline \(\bm{A}\rightarrow\bm{W}\) & 85.2 \(\pm\) 0.2 & 86.3 \(\pm\) 0.3 & 86.4 \(\pm\) 0.4 & 88.3 \(\pm\) 0.3 & 88.6 \(\pm\) 0.4 & 87.2 \(\pm\) 0.3 & **94.4**\(\pm\) 0.4 \\ \(\bm{D}\rightarrow\bm{W}\) & 97.5 \(\pm\) 0.1 & 97.4 \(\pm\) 0.3 & 97.7 \(\pm\) 0.1 & 96.9 \(\pm\) 0.1 & 97.6 \(\pm\) 0.1 & 97.7 \(\pm\) 0.1 & **97.8**\(\pm\) 0.2 \\ \(\bm{W}\rightarrow\bm{D}\) & 99.5 \(\pm\) 0.3 & 98.7 \(\pm\) 0.2 & **99.7**\(\pm\) 0.1 & 97.4 \(\pm\) 0.2 & 99.1 \(\pm\) 0.2 & 99.3 \(\pm\) 0.2 & 99.5 \(\pm\) 0.3 \\ \(\bm{A}\rightarrow\bm{D}\) & 89.4 \(\pm\) 0.2 & 84.3 \(\pm\) 0.4 & 89.9 \(\pm\) 0.2 & 87.4 \(\pm\) 0.5 & 86.3 \(\pm\) 0.4 & 87.1 \(\pm\) 0.2 & **90.1**\(\pm\) 0.3 \\ \(\bm{D}\rightarrow\bm{A}\) & 71.4 \(\pm\) 0.3 & 71.7 \(\pm\) 0.4 & 70.6 \(\pm\) 0.3 & **74.9**\(\pm\) 0.4 & 72.3 \(\pm\) 0.4 & 71.6 \(\pm\) 0.3 & 71.9 \(\pm\) 0.1 \\ \(\bm{W}\rightarrow\bm{A}\) & 73.1 \(\pm\) 0.2 & 73.5 \(\pm\) 0.2 & 72.3 \(\pm\) 0.4 & 73.0 \(\pm\) 0.4 & 73.6 \(\pm\) 0.3 & 73.7 \(\pm\) 0.3 & **74.6**\(\pm\) 0.1 \\ \hline
**Average** & 86.0 \(\pm\) 0.2 & 85.3 \(\pm\) 0.3 & 86.1 \(\pm\) 0.2 & 86.3 \(\pm\) 0.3 & 86.2 \(\pm\) 0.3 & 86.1 \(\pm\) 0.2 & **87.3**\(\pm\) 0.2 \\ \hline \end{tabular}
\end{table}
Table 1: Classification accuracy on the target domain of _office-31_ dataset (ResNet50 embedding).

_Dataset_: We use human lung adenocarcinoma A549 cells data from [66]. The dataset contains 1,874 samples of RNA sequences \(\{\bm{x}^{(1)}\}\) and ATAC sequences \(\{\bm{x}^{(2)}\}\). Each data set is split into 1534 training samples and 340 testing samples as in [27]. The data have labeled associations between the two domains--part of which will be used to test our weakly supervised formulation. For this experiment, features of RNA sequence and the ATAC sequence have dimensions of \(d^{(1)}=815\) and \(d^{(2)}=2613\), respectively. We set \(d_{\mathrm{C}}=256\). We use our weakly supervised formulation as shown in (9). We uniformly sampled a set of indices from the training set to serve as \(\mathcal{L}\).

_Baseline and Metric_: We use weakly supervised algorithm, namely, cross-modal autoencoder (CM-AE) work in [27], as a baseline, which also learns the shared representation between unaligned RNA and ATAC sequences. We use the \(K\)-nearest neighbor (\(k\)-NN) accuracy to evaluate the performance as suggested in [27].

_Result_: The plot in Fig. 4 shows the \(k\)-NN accuracy of the methods on the test set. Results show the mean and standard deviation over 10 runs, each having a different random initialization. For the proposed method, we vary the number of available paired samples from \(0\) (cf. Theorem 1) to \(d_{\mathrm{C}}=256\) (cf. Theorem 3). Note that the baseline uses more (i.e., \(256\) and \(770\)) paired samples. It also needs additional class labels, i.e., \(y_{i}^{(q)}\) for the \(i\)th sample \(\bm{x}_{i}^{(q)}\). Here, \(y_{i}^{(q)}\) represents the number of hours (\(0\), \(1\) or \(3\)) of cell treatment [27; 66]. The proposed method without any supervision (i.e., \(0\) paired samples) already exhibits around 3 times greater \(k\)-NN accuracy compared to the baseline for all \(k\). Moreover, including just one paired sample boosts the \(k\)-NN accuracy of the proposed method to around 5 times higher than the baseline for all \(k\). Finally, one can observe a steadily increasing \(k\)-NN accuracy with respect to the number of available paired samples. This corroborates with our Theorem 3.

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|c|} \hline
**source \(\rightarrow\) target** & **ResNet** & **DANN** & **MDD** & **MCC** & **SDAT** & **ELS** & **Proposed** \\ \hline \(\bm{\mathrm{Ar}}\rightarrow\bm{\mathrm{Cl}}\) & 42.0 \(\pm\) 0.2 & 46.7 \(\pm\) 0.2 & 47.4 \(\pm\) 0.3 & 44.4 \(\pm\) 0.3 & 47.3 \(\pm\) 0.4 & 48.5 \(\pm\) 0.2 & **51.0 \(\pm\) 0.3** \\ \(\bm{\mathrm{Ar}}\rightarrow\bm{\mathrm{Pr}}\) & 69.2 \(\pm\) 0.1 & 70.2 \(\pm\) 0.4 & 72.8 \(\pm\) 0.4 & 72.4 \(\pm\) 0.2 & 71.1 \(\pm\) 0.3 & 71.0 \(\pm\) 0.3 & **75.8 \(\pm\) 0.1** \\ \(\bm{\mathrm{Ar}}\rightarrow\bm{\mathrm{Rw}}\) & 80.2 \(\pm\) 0.3 & 81.2 \(\pm\) 0.4 & 81.2 \(\pm\) 0.1 & 80.3 \(\pm\) 0.3 & 80.5 \(\pm\) 0.1 & 80.8 \(\pm\) 0.4 & **82.5 \(\pm\) 0.2** \\ \(\bm{\mathrm{Cl}}\rightarrow\bm{\mathrm{Ar}}\) & 60.7 \(\pm\) 0.4 & 60.8 \(\pm\) 0.3 & 62.4 \(\pm\) 0.1 & 59.2 \(\pm\) 0.4 & 57.6 \(\pm\) 0.2 & 59.8 \(\pm\) 0.1 & **62.7 \(\pm\) 0.4** \\ \(\bm{\mathrm{Cl}}\rightarrow\bm{\mathrm{Pr}}\) & 71.0 \(\pm\) 0.1 & 69.8 \(\pm\) 0.3 & 70.0 \(\pm\) 0.4 & **71.1 \(\pm\) 0.4** & 66.5 \(\pm\) 0.1 & 68.5 \(\pm\) 0.2 & 72.5 \(\pm\) 0.3 \\ \(\bm{\mathrm{Cl}}\rightarrow\bm{\mathrm{Rw}}\) & 74.8 \(\pm\) 0.2 & 73.3 \(\pm\) 0.1 & 74.1 \(\pm\) 0.1 & **76.2 \(\pm\) 0.2** & 70.7 \(\pm\) 0.1 & 71.7 \(\pm\) 0.1 & 75.8 \(\pm\) 0.1 \\ \(\bm{\mathrm{Pr}}\rightarrow\bm{\mathrm{Ar}}\) & 60.6 \(\pm\) 0.2 & 62.2 \(\pm\) 0.1 & 64.3 \(\pm\) 0.1 & 59.2 \(\pm\) 0.1 & 62.5 \(\pm\) 0.4 & 60.9 \(\pm\) 0.2 & **64.4 \(\pm\) 0.3** \\ \(\bm{\mathrm{Pr}}\rightarrow\bm{\mathrm{Cl}}\) & 44.8 \(\pm\) 0.1 & 48.8 \(\pm\) 0.1 & 48.0 \(\pm\) 0.3 & 46.2 \(\pm\) 0.2 & 49.0 \(\pm\) 0.3 & 49.6 \(\pm\) 0.3 & **50.4 \(\pm\) 0.1** \\ \(\bm{\mathrm{Pr}}\rightarrow\bm{\mathrm{Rw}}\) & 79.6 \(\pm\) 0.1 & 80.3 \(\pm\) 0.4 & 79.6 \(\pm\) 0.3 & 80.3 \(\pm\) 0.2 & 80.0 \(\pm\) 0.1 & 79.2 \(\pm\) 0.1 & **81.7 \(\pm\) 0.2** \\ \(\bm{\mathrm{Rw}}\rightarrow\bm{\mathrm{Ar}}\) & 70.1 \(\pm\) 0.2 & 71.5 \(\pm\) 0.1 & 71.4 \(\pm\) 0.3 & 67.8 \(\pm\) 0.2 & 71.6 \(\pm\) 0.4 & 71.3 \(\pm\) 0.4 & **72.6 \(\pm\) 0.1** \\ \(\bm{\mathrm{Rw}}\rightarrow\bm{\mathrm{Cl}}\) & 45.8 \(\pm\) 0.2 & 50.9 \(\pm\) 0.2 & 50.3 \(\pm\) 0.1 & 50.0 \(\pm\) 0.2 & 51.4 \(\pm\) 0.1 & 50.7 \(\pm\) 0.1 & **53.2 \(\pm\) 0.1** \\ \(\bm{\mathrm{Rw}}\rightarrow\bm{\mathrm{Pr}}\) & 80.7 \(\pm\) 0.1 & 80.6 \(\pm\) 0.4 & 81.1 \(\pm\) 0.1 & 81.2 \(\pm\) 0.1 & 80.7 \(\pm\) 0.1 & 79.8 \(\pm\) 0.3 & **82.9 \(\pm\) 0.3** \\ \hline
**Average** & 64.9 \(\pm\) 0.1 & 66.3 \(\pm\) 0.2 & 66.8 \(\pm\) 0.2 & 65.6 \(\pm\) 0.2 & 65.7 \(\pm\) 0.2 & 65.9 \(\pm\) 0.2 & **68.7 \(\pm\) 0.2** \\ \hline \end{tabular}
\end{table}
Table 2: Classification accuracy on the target domain of _office-Home_ dataset (ResNet50 embedding).

Figure 4: \(k\)-NN accuracy for single-cell sequence alignment.

**Application (iii) - Multi-lingual Information Retrieval.** We also evaluated our method on a word embedding association problem from the natural language processing literature [20; 21]. This task aims to associate high-dimensional word embeddings across different languages according to their semantic meaning. The word embeddings in two languages are represented using two sets of vectors, i.e., \(\{\bm{x}_{i}^{(1)}\}_{i=1}^{I}\) and \(\{\bm{x}_{i}^{(2)}\}_{j=1}^{J}\). The postulate is that if \(\bm{x}_{i}^{(1)}\) and \(\bm{x}_{j}^{(2)}\) have the same meaning (e.g., both representing "cat") in two languages (e.g., English and German), they should share a latent components \(\bm{c}\).

_Dataset_: We use the word embeddings from the MUSE dataset (https://github.com/facebookresearch/MUSE) [21]. These monolingual word embedding are generated using fastText [67] and has dimensions of \(d^{(q)}=300\) for \(q=1,2\). The training dataset include 200,000 word embeddings in each language. In our experiment we set \(d_{C}=256\). We follow the generative model under (8) and run the formulation in (7) to learn the linear transformation \(\bm{Q}\).

_Baseline_: We use Adv[21] as the baseline which also uses distribution matching between two language domains. Unlike our method, Adv does not use linear mixture models.

_Metric_: We follow [21] to use the average precision score calculated based on _nearest neighbor_ (NN) and _cross domain similarity local scaling_ (CSLS). Precision at \(k\) ("\(k\) precision") is computed by the number of times that one of the correct translations of source word is retrieved at top-\(k\) results (\(k\) ={1, 5, 10}). The final score is normalized to be in the range of 0 to 100, with 100 being the highest score indicating the best performance. To evaluate the performance, we use the same test data as in [21]. For each source and target language pair, this dataset includes 1,500 source word embeddings. The source embeddings are used to retrieve corresponding embeddings from a pool of 200,000 target word embeddings.

_Result_: Table 3 reports the P@1 scores over the test data calculated for each source and target language pair. The languages are denoted as as **en** - English, **es** - Spanish, **it** - Italian, **fr** - French, **de** - Germany, **ru** - Russian, **ar** - Arabic and **vi** - Vietnamese. One can observe that the proposed method exhibits a better precision performance than that of Adv in most of the translation tasks. In particular, the proposed method significantly outperforms the baseline on the tasks **en\(\rightarrow\)ar**, **ar\(\rightarrow\)en**, **en\(\rightarrow\)vi** and **vi\(\rightarrow\)en**, showing at least \(10\%\) precision gains. Similarly, our method shows at least \(5\%\) improvements in both NN and CSLS based precision metrics in **en\(\rightarrow\)es** and **es\(\rightarrow\)en** tasks.

More details and additional experiments can be found in Appendix G.3.

## 7 Conclusion

In this work, we considered the problem of identifying shared components from unaligned multi-domain mixtures. We proposed a learning loss that matches the distributions of linearly transformed data. Based on this loss, we came up with a suite of sufficient conditions to ensure the identifiability of shared components. Furthermore, we proposed modified models and losses that enjoy more relaxed conditions for shared component identifiability. This was achieved via introducing structural constraints, namely, the homogeneity of the mixing systems and the existence of weak supervision. Our theoretical claims were validated with both synthetic and real-world data, demonstrating soundness of the theorems and usefulness of the models/algorithms.

**Limitations.** First, our conditions for shared component identification are sufficient. The necessary conditions are not underpinned, but necessary conditions assist understanding the limitations of the models and algorithms. Second, our methods were developed under the linear mixture model, which has limited expressiveness, and thus often requires pre-processing to approximately meet the model specification. We expect that results with similar flavors to be derived for nonlinear models in the future. Third, the results were derived under an unlimited data assumption. It would be interesting have a finite sample analysis. Finally, optimizing GAN-based losses is sensitive to hyperparameter settings. Back-propagation based minimax optimization occasionally fails to converge. More optimization-friendly losses and more stable algorithms are desirable in the context of distribution matching.

## Acknowledgment

This work is supported in part by the Army Research Office (ARO) under Project ARO W911NF-21-1-0227, and in part by the National Science Foundation (NSF) CAREER Award ECCS-2144889.

## References

* [1]M. S. Ibrahim, A. S. Zamzam, A. Konar, and N. D. Sidiropoulos (2021) Cell-edge detection via selective cooperation and generalized canonical correlation. IEEE Transactions on Wireless Communications20 (11), pp. 7431-7444. Cited by: SS1.
* [2]M. Sorensen, C. I. Kanatsoulis, and N. D. Sidiropoulos (2021) Generalized canonical correlation analysis: a subspace intersection approach. IEEE Transactions on Signal Processing69 (24), pp. 2452-2467. Cited by: SS1.
* [3]P. Rastogi, B. Van Durme, and R. Arora (2015) Multiview LSA: representation learning via generalized CCA. In Proc. Conference of the North American chapter of the Association for Computational Linguistics: human language technologies, pp. 556-566. Cited by: SS1.
* [4]P. Comon and C. Jutten (2010) Handbook of blind source separation: independent component analysis and applications. Academic press. Cited by: SS1.
* [5]J. Fu and K. Huang (2024) Global identifiability of \(\ell_{1}\)-based dictionary learning via matrix volume optimization. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 36, pp. 36. Cited by: SS1.
* [6]J. Sun, Q. Qu, and J. Wright (2016) Complete dictionary recovery over the sphere i: overview and the geometric picture. IEEE Transactions on Information Theory63 (2), pp. 853-884. Cited by: SS1.
* [7]J. Sun, Q. Qu, and J. Wright (2016) Complete dictionary recovery over the sphere i: overview and the geometric picture. IEEE Transactions on Information Theory63 (2), pp. 853-884. Cited by: SS1.
* [8]J. Sun, Q. Qu, and J. Wright (2016) Complete dictionary recovery over the sphere i: overview and the geometric picture. IEEE Transactions on Information Theory63 (2), pp. 853-884. Cited by: SS1.
* [9]J. Sun, Q. Qu, and J. Wright (2016) Complete dictionary recovery over the sphere i: overview and the geometric picture. IEEE Transactions on Information Theory63 (2), pp. 853-884. Cited by: SS1.
* [10]J. Sun, Q. Qu, and J. Wright (2016) Complete dictionary recovery over the sphere i: overview and the geometric picture. IEEE Transactions on Information Theory63 (2), pp. 853-884. Cited by: SS1.
* [11]J. Sun, Q. Qu, and J. Wright (2016) Complete dictionary recovery over the sphere i: overview and the geometric picture. IEEE Transactions on Information Theory63 (2), pp. 853-884. Cited by: SS1.
* [12]J. Sun, Q. Qu, and J. Wright (2016) Complete dictionary recovery over the sphere i: overview and the geometric picture. IEEE Transactions on Information Theory63 (2), pp. 853-884. Cited by: SS1.
* [13]J. Sun, Q. Qu, and J. Wright (2016) Complete dictionary recovery over the sphere i: overview and the geometric picture. IEEE Transactions on Information Theory63 (2), pp. 853-884. Cited by: SS1.
* [14]J. Sun, Q. Qu, and J. Wright (2016) Complete dictionary recovery over the sphere i: overview and the geometric picture. IEEE Transactions on Information Theory63 (2), pp. 853-884. Cited by: SS1.
* [15]J. Sun, Q. Qu, and J. Wright (2016) Complete dictionary recovery over the sphere i: overview and the geometric picture. IEEE Transactions on Information Theory63 (2), pp. 853-884. Cited by: SS1.
* [16]H. Richard, P. Ablin, B. Thirion, A. Gramfort, and A. Hyvarinen (2021) Shared independent component analysis for multi-subject neuroimaging. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 34, pp. 29 962-29 971. Cited by: SS1.
* [17]J. Von Kugelgen, Y. Sharma, L. Gresele, W. Brendel, B. Scholkopf, M. Besserve, and F. Locatello (2021) Self-supervised learning with data augmentations provably isolates content from style. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 34, pp. 16 451-16 467. Cited by: SS1.
* [18]J. Von Kugelgen, Y. Sharma, L. Gresele, W. Brendel, B. Scholkopf, M. Besserve, and F. Locatello (2021) Self-supervised learning with data augmentations provably isolates content from style. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 34, pp. 16 451-16 467. Cited by: SS1.
* [19]J. Von Kugelgen, Y. Sharma, L. Gresele, W. Brendel, B. Scholkopf, M. Besserve, and F. Locatello (2021) Self-supervised learning with data augmentations provably isolates content from style. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 34, pp. 16 451-16 467. Cited by: SS1.
* [20]J. Von Kugelgen, Y. Sharma, L. Gresele, W. Brendel, B. Scholkopf, M. Besserve, and F. Locatello (2021) Self-supervised learning with data augmentations provably isolates content from style. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 34, pp. 16 451-16 467. Cited by: SS1.
* [21]J. Von Kugelgen, Y. Sharma, L. Gresele, W. Brendel, B. Scholkopf, M. Besserve, and F. Locatello (2021) Self-supervised learning with data augmentations provably isolates content from style. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 34, pp. 16 451-16 467. Cited by: SS1.
* [22]J. Von Kugelgen, Y. Sharma, L. Gresele, W. Brendel, B. Scholkopf, M. Besserve, and F. Locatello (2021) Self-supervised learning with data augmentations provably isolates content from style. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 34, pp. 16 451-16 467. Cited by: SS1.
* [23]J. Von Kugelgen, Y. Sharma, L. Gresele, W. Brendel, B. Scholkopf, M. Besserve, and F. Locatello (2021) Self-supervised learning with data augmentations provably isolates content from style. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 34, pp. 16 451-16 467. Cited by: SS1.
* [24]J. Von Kugelgen, Y. Sharma, L. Gresele, W. Brendel, B. Scholkopf, M. Besserve, and F. Locatello (2021) Self-supervised learning with data augmentations provably isolates content from style. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 34, pp. 16 451-16 467. Cited by: SS1.
* [25]J. Von Kugelgen, Y. Sharma, L. Gresele, W. Brendel, B. Scholkopf, M. Besserve, and F. Locatello (2021) Self-supervised learning with data augmentations provably isolates content from style. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 34, pp. 16 451-16 467. Cited by: SS1.
* [26]J. Von Kugelgen, Y. Sharma, L. Gresele, W. Brendel, B. Scholkopf, M. Besserve, and F. Locatello (2021) Self-supervised learning with data augmentations provably isolates content from style. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 34, pp. 16 451-16 467. Cited by: SS1.
* [27]J. Von Kugelgen, Y. Sharma, L. Gresele, W. Brendel, B. Scholkopf, M. Besserve, and F. Locatello (2021) Self-supervised learning with data augmentations provably isolates content from style. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 34, pp. 16 451-16 467. Cited by: SS1.
* [28]J. Von Kugelgen, Y. Sharma, L. Gresele, W. Brendel, B. Scholkopf, M. Besserve, and F. Locatello (2021) Self-supervised learning with data augmentations provably isolates content from style. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 34, pp. 16 451-16 467. Cited by: SS1.
* [29]J. Von Kugelgen, Y. Sharma, L. Gresele, W. Brendel, B. Scholkopf, M. Besserve, and F. Locatello (2021) Self-supervised learning with data augmentations provably isolates content from style. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 34, pp. 16 451-16 467. Cited by: SS1.
* [30]J. Von Kugelgen, Y. Sharma, L. Gresele, W. Brendel, B. Scholkopf, M. Besserve, and F. Locatello (2021) Self-supervised learning with data augmentations provably isolates content from style. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 34, pp. 16 451-16 467. Cited by: SS1.
* [31]J. Von Kugelgen, Y. Sharma, L. Gresele, W. Brendel, B. Scholkopf, M. Besserve, and F. Locatello (2021) Self-supervised learning with data augmentations provably isolates content from style. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 34, pp. 16 451-16 467. Cited by: SS1.
* [32]J. Von Kugelgen, Y. Sharma, L. Gresele, W. Brendel, B. Scholkopf, M. Besserve, and F. Locatello (2021) Self-supervised learning with data augmentations provably isolates content from style. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 34, pp. 16 451-16 467. Cited by: SS1.
* [33]J. Von Kugelgen, Y. Sharma, L. Gresele, W. Brendel, B. Scholkopf, M. Besserve, and F. Locatello (2021) Self-supervised learning with data augmentations provably isolates content from style. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 34, pp. 16 451-16 467. Cited by: SS1.
* [34]J. Von Kugelgen, Y. Sharma, L. Gresele, W. Brendel, B. Scholkopf, M. Besserve, and F. Locatello (2021) Self-supervised learning with data augmentations provably isolates content from style. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 34, pp. 16 451-16 467. Cited by: SS1.
* [35]J. Von Kugelgen, Y. Sharma, L. Gresele, W. Brendel, B. Scholkopf, M. Besserve, and F. Locatello (2021) Self-supervised learning with data augmentations provably isolates content from style. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 34, pp. 16 451-16 467. Cited by: SS1.
* [36]J. Von Kugelgen, Y. Sharma, L. Gresele, W. Brendel, B. Scholkopf, M. Besserve, and F. Locatello (2021) Self-supervised learning with data augmentations provably isolates content from style. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 34, pp. 16 451-16 467. Cited by: SS1.
* [37]J. Von Kugelgen, Y. Sharma, L. Gresele, W. Brendel, B. Scholkopf, M. Besserve, and F. Locatello (2021) Self-supervised learning with data augmentations provably isolates content from style. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 34, pp. 16 451-16 4* [18] M. Anderson, G.-S. Fu, R. Phlypo, and T. Adali, "Independent vector analysis: Identification conditions and performance bounds," _IEEE Transactions on Signal Processing_, vol. 62, no. 17, pp. 4399-4410, 2014.
* [19] P. A. Karakasis and N. D. Sidiropoulos, "Revisiting deep generalized canonical correlation analysis," _IEEE Transactions on Signal Processing_, vol. 71, pp. 4392-4406, 2023.
* [20] G. Lample, A. Conneau, L. Denoyer, and M. Ranzato, "Unsupervised machine translation using monolingual corpora only," in _Proc. International Conference on Learning Representations (ICLR)_, 2018.
* [21] G. Lample, A. Conneau, M. Ranzato, L. Denoyer, and H. Jegou, "Word translation without parallel data," in _Proc. International Conference on Learning Representations (ICLR)_, 2018.
* [22] E. Grave, A. Joulin, and Q. Berthet, "Unsupervised alignment of embeddings with wasserstein procrustes," in _Proc. International Conference on Artificial Intelligence and Statistics (AISTATS)_. PMLR, 2019, pp. 1880-1890.
* [23] S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira, "Analysis of representations for domain adaptation," in _Advances in Neural Information Processing Systems (NeurIPS)_, vol. 19, 2006.
* [24] M. Long, Y. Cao, J. Wang, and M. Jordan, "Learning transferable features with deep adaptation networks," in _Proc. International Conference on Machine Learning (ICML)_. PMLR, 2015, pp. 97-105.
* [25] Y. Ganin and V. Lempitsky, "Unsupervised domain adaptation by backpropagation," in _Proc. International Conference on Machine Learning (ICML)_. PMLR, 2015, pp. 1180-1189.
* [26] S. Waaijenborg, P. C. Verselewel de Witt Hamer, and A. H. Zwinderman, "Quantifying the association between gene expressions and dna-markers by penalized canonical correlation analysis," _Statistical applications in genetics and molecular biology_, vol. 7, no. 1, 2008.
* [27] K. D. Yang, A. Belyaeva, S. Venkatachalapathy, K. Damodaran, A. Katcoff, A. Radhakrishnan, G. Shivashankar, and C. Uhler, "Multi-domain translation between single-cell imaging and sequencing data using autoencoders," _Nature communications_, vol. 12, no. 1, p. 31, 2021.
* [28] I. Gulrajani and T. Hashimoto, "Identifiability conditions for domain adaptation," in _Proc. International Conference on Machine Learning (ICML)_, 2022, pp. 7982-7997.
* [29] S. Shrestha and X. Fu, "Towards identifiable unsupervised domain translation: A diversified distribution matching approach," in _Proc. International Conference on Learning Representations (ICLR)_, 2024.
* [30] Y. Choi, Y. Uh, J. Yoo, and J.-W. Ha, "StarGAN v2: Diverse image synthesis for multiple domains," in _Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2020, pp. 8188-8197.
* [31] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz, "Multimodal unsupervised image-to-image translation," in _Proc. European Conference on Computer Vision (ECCV)_, 2018, pp. 172-189.
* [32] S. Xie, L. Kong, M. Gong, and K. Zhang, "Multi-domain image generation and translation with identifiability guarantees," in _Proc. International Conference on Learning Representations (ICLR)_, 2023.
* [33] P. Comon, "Independent component analysis, a new concept?" _Signal processing_, vol. 36, no. 3, pp. 287-314, 1994.
* [34] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in _Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 770-778.
* [35] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark _et al._, "Learning transferable visual models from natural language supervision," in _Proc. International Conference on Machine Learning (ICML)_. PMLR, 2021, pp. 8748-8763.
* [36] Y. Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, and J. Choo, "StarGAN: Unified generative adversarial networks for multi-domain image-to-image translation," in _Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2018, pp. 8789-8797.
* [37] X. Fu, K. Huang, N. D. Sidiropoulos, and W.-K. Ma, "Nonnegative matrix factorization for signal and data analytics: Identifiability, algorithms, and applications," _IEEE Signal Processing Magazine_, vol. 36, no. 2, pp. 59-80, 2019.

* [38] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Scholkopf, and A. Smola, "A kernel two-sample test," _The Journal of Machine Learning Research_, vol. 13, no. 1, pp. 723-773, 2012.
* [39] C. Villani _et al._, _Optimal transport: old and new_. Springer, 2009, vol. 338.
* [40] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, "Generative adversarial networks," in _Advances in Neural Information Processing Systems (NeurIPS)_, 2014.
* [41] L. Kong, S. Xie, W. Yao, Y. Zheng, G. Chen, P. Stojanov, V. Akinwande, and K. Zhang, "Partial disentanglement for domain adaptation," in _Proc. International Conference on Machine Learning (ICML)_, 2022, pp. 11 455-11 472.
* [42] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, "A simple framework for contrastive learning of visual representations," in _Proc. International Conference on Machine Learning (ICML)_. PMLR, 2020, pp. 1597-1607.
* [43] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra _et al._, "Matching networks for one shot learning," in _Advances in Neural Information Processing Systems (NeurIPS)_, vol. 29, 2016.
* [44] E. Triantafillou, T. Zhu, V. Dumoulin, P. Lamblin, U. Evci, K. Xu, R. Goroshin, C. Gelada, K. Swersky, P.-A. Manzagol _et al._, "Meta-dataset: A dataset of datasets for learning to learn from few examples," in _Proc. International Conference on Learning Representations (ICLR)_, 2019.
* [45] S. Wold, K. Esbensen, and P. Geladi, "Principal component analysis," _Chemometrics and intelligent laboratory systems_, vol. 2, no. 1-3, pp. 37-52, 1987.
* [46] J. Li and J. M. Bioucas-Dias, "Minimum volume simplex analysis: A fast algorithm to unmix hyperspectral data," in _IEEE International Geoscience and Remote Sensing Symposium_, vol. 3, 2008, pp. III-250.
* [47] X. Fu, K. Huang, B. Yang, W.-K. Ma, and N. D. Sidiropoulos, "Robust volume minimization-based matrix factorization for remote sensing and document clustering," _IEEE Transactions on Signal Processing_, vol. 64, no. 23, pp. 6254-6268, 2016.
* [48] S. Arora, R. Ge, Y. Halpern, D. Mimno, A. Moitra, D. Sontag, Y. Wu, and M. Zhu, "A practical algorithm for topic modeling with provable guarantees," in _Proc. International Conference on Machine Learning (ICML)_. PMLR, 2013, pp. 280-288.
* [49] K. Huang, X. Fu, and N. D. Sidiropoulos, "Anchor-free correlated topic modeling: Identifiability and algorithm," _Advances in Neural Information Processing Systems_, vol. 29, 2016.
* [50] X. Mao, P. Sarkar, and D. Chakrabarti, "Estimating mixed memberships with sharp eigenvector deviations," _Journal of the American Statistical Association_, vol. 116, no. 536, pp. 1928-1940, 2021.
* [51] L. Gresele, P. K. Rubenstein, A. Mehrjou, F. Locatello, and B. Scholkopf, "The incomplete Rosetta stone problem: Identifiability results for multi-view nonlinear ICA," in _Proc. Uncertainty in Artificial Intelligence_, 2020, pp. 217-227.
* [52] C. Eastwood, J. von Kugelgen, L. Ericsson, D. Bouchacourt, P. Vincent, M. Ibrahim, and B. Scholkopf, "Self-supervised disentanglement by leveraging structure in data augmentations," in _Causal Representation Learning Workshop at NeurIPS 2023_, 2023.
* [53] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, "Unpaired image-to-image translation using cycle-consistent adversarial networks," in _Proc. the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2017, pp. 2223-2232.
* [54] M. Long, H. Zhu, J. Wang, and M. I. Jordan, "Deep transfer learning with joint adaptation networks," in _Proc. International Conference on Machine Learning (ICML)_. PMLR, 2017, pp. 2208-2217.
* [55] W. Wang, H. Zhang, Z. Yuan, and C. Wang, "Unsupervised real-world super-resolution: A domain adaptation perspective," in _Proc. IEEE International Conference on Computer Vision (ICCV)_, 2021, pp. 4318-4327.
* [56] T. Galanti, L. Wolf, and S. Benaim, "The role of minimal complexity functions in unsupervised learning of semantic mappings," in _Proc. International Conference on Learning Representations (ICLR)_, 2018.

* [57] N. Moriakov, J. Adler, and J. Teuwen, "Kernel of CycleGAN as a principle homogeneous space," in _Proc. International Conference on Learning Representations (ICLR)_, 2020.
* [58] K. Saenko, B. Kulis, M. Fritz, and T. Darrell, "Adapting visual category models to new domains," in _Proc. European Conference on Computer Vision (ECCV)_. Springer, 2010, pp. 213-226.
* [59] H. Venkateswara, J. Eusebio, S. Chakraborty, and S. Panchanathan, "Deep hashing network for unsupervised domain adaptation," in _Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2017, pp. 5018-5027.
* [60] Y. Zhang, T. Liu, M. Long, and M. Jordan, "Bridging theory and algorithm for domain adaptation," in _Proc. International Conference on Machine Learning (ICML)_, 2019.
* [61] Y. Jin, X. Wang, M. Long, and J. Wang, "Minimum class confusion for versatile domain adaptation," in _Proc. IEEE International Conference on Computer Vision (ICCV)_. Springer, 2020, pp. 464-480.
* [62] H. Rangwani, S. K. Aithal, M. Mishra, A. Jain, and V. B. Radhakrishnan, "A closer look at smoothness in domain adversarial training," in _Proc. International Conference on Machine Learning (ICML)_. PMLR, 2022, pp. 18 378-18 399.
* [63] Y. Zhang, J. Liang, Z. Zhang, L. Wang, R. Jin, T. Tan _et al._, "Free lunch for domain adversarial training: Environment label smoothing," in _Proc. International Conference on Learning Representations (ICLR)_, 2023.
* [64] L. Eyring, D. Klein, T. Uscidda, G. Palla, N. Kilbertus, Z. Akata, and F. Theis, "Unbalancedness in neural monge maps improves unpaired domain translation," _arXiv preprint arXiv:2311.15100_, 2023.
* [65] M. Amodio and S. Krishnaswamy, "MAGAN: Aligning biological manifolds," in _Proc. International Conference on Machine Learning (ICML)_. PMLR, 2018, pp. 215-223.
* [66] J. Cao, D. A. Cusanovich, V. Ramani, D. Aghamirzaie, H. A. Pliner, A. J. Hill, R. M. Daza, J. L. McFaline-Figueroa, J. S. Packer, L. Christiansen _et al._, "Joint profiling of chromatin accessibility and gene expression in thousands of single cells," _Science_, vol. 361, no. 6409, pp. 1380-1385, 2018.
* [67] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. Jegou, and T. Mikolov, "Fasttext.zip: Compressing text classification models," _arXiv preprint arXiv:1612.03651_, 2016.
* [68] K. B. Athreya and S. N. Lahiri, _Measure theory and probability theory_. Springer, 2006, vol. 19.
* [69] J.-F. Cardoso, "Super-symmetric decomposition of the fourth-order cumulant tensor. blind identification of more sources than sensors." in _Proc. International Conference on Acoustics, Speech, and Signal Processing (ICASSP)_, vol. 91, 1991, pp. 3109-3112.
* [70] D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," in _Proc. ICLR_, 2015.
* [71] A. L. Maas, A. Y. Hannun, and A. Y. Ng, "Rectifier nonlinearities improve neural network acoustic models," in _Proc. International Conference on Machine Learning (ICML)_, vol. 30, no. 1, 2013, p. 3.
* [72] K. Ghasedi Dizaji, A. Herandi, C. Deng, W. Cai, and H. Huang, "Deep clustering via joint convolutional autoencoder embedding and relative entropy minimization," in _Proc. the IEEE International Conference on Computer Vision (ICCV)_, 2017, pp. 5736-5745.
* [73] J. Xie, R. Girshick, and A. Farhadi, "Unsupervised deep embedding for clustering analysis," in _Proc. International Conference on Machine Learning (ICML)_. PMLR, 2016, pp. 478-487.
* [74] W. Wang, X. Yan, H. Lee, and K. Livescu, "Deep variational canonical correlation analysis," _arXiv preprint arXiv:1610.03454_, 2016.
* [75] A. Gretton, K. Fukumizu, C. Teo, L. Song, B. Scholkopf, and A. Smola, "A kernel statistical test of independence," in _Advances in Neural Information Processing Systems (NeurIPS)_, vol. 20, 2007.
* [76] T. M. Cover, _Elements of information theory_. John Wiley & Sons, 1999.

**Supplementary Material of "Identifiable Shared Component Analysis of Unpaired Multimodal Mixtures"**

## Appendix A Notation

The notations used throughout the paper are summarized in the Table 4.:

\begin{table}
\begin{tabular}{l|l} \hline Notation & Definition \\ \hline \hline \(x\), \(\bm{x}\), \(\bm{X}\) & scalar, vector and matrix \\ \hline \(\bm{x}^{(q)}\) & variable from q-th domain \\ \hline \(x_{i}\), \(\bm{x}_{i}\) & both represents i-th element of vector \(\bm{x}\) \\ \hline \(\bm{X}_{ij}\) & represents the element of i-th row and j-th column of matrix \(\bm{X}\) \\ \hline \(\bm{x}^{\top}\), \(\bm{X}^{\top}\) & transpose of \(\bm{x}\) and \(\bm{X}\) \\ \hline \(|\mathcal{X}|\) & represents the cardinality of set \(\mathcal{X}\) \\ \hline \(\text{Null}(\bm{X})\) & represents the null space of matrix \(\bm{X}\) \\ \hline \(\text{conv}(\cdot)\) & returns the convex hull of the given set \\ \hline \(\dim(\mathcal{X})\) & denotes the dimension of subspace \(\mathcal{X}\) \\ \hline \(k\mathcal{A}\) & \(\{k\bm{a}\mid\bm{a}\in\mathcal{A},\ k\in\mathcal{R}\}\) \\ \hline \(\bm{x}+\mathcal{X}\) & \(\{\bm{x}+\bm{z}\mid\bm{z}\in\mathcal{X}\}\) \\ \hline \(\mathcal{X}+\mathcal{Y}\) & \(\{\bm{x}+\bm{y}\mid\bm{x}\in\mathcal{X},\bm{y}\in\mathcal{Y}\}\) \\ \hline \(\bm{A}_{\text{PreImg}}(\mathcal{X})\) & preimage of \(\mathcal{X}\); \(\{\bm{x}\mid\bm{Ax}\in\mathcal{X}\}\) \\ \hline \([N]\) & set of whole numbers up to \(N\); \(\{1\dots N\}\) \\ \hline \(\bm{I}\) & identity matrix \\ \hline \(\mathbb{P}_{\bm{x}}\) & probability distribution of random variable \(\bm{x}\) \\ \hline \(\mathbb{P}_{\bm{x},\bm{y}}\) & joint probability distribution of random variable \(\bm{x}\) and \(\bm{y}\) \\ \hline \(\bm{\mathbb{L}}[\cdot]\) & expectation \\ \hline \(\bm{x}\)\(\stackrel{{(\mathsf{d})}}{{\underset{\bm{x}\longleftarrow}{\bm{y}}}}\) & \(\bm{x}\) and \(\bm{y}\) random vectors have the same distribution \\ \hline \(\bm{x}\)\(\stackrel{{(\mathsf{d})}}{{\neq}}\)\(\bm{y}\) & \(\bm{x}\) and \(\bm{y}\) random vectors have different distributions \\ \hline \(\bm{x}\)\(\stackrel{{}}{{\perp}}\)\(\bm{y}\) & \(\bm{x}\) and \(\bm{y}\) random vectors are statistically independent \\ \hline \([a,b]\) & represents continuous interval between \(a\) and \(b\) \\ \hline \(\mathcal{N}(\mu,\sigma^{2})\) & normal distribution with mean \(\mu\) and variance \(\sigma^{2}\) \\ \hline Uniform\([a,b]\) & uniform distribution with interval \(a\) and \(b\) \\ \hline Gamma\((\alpha,\ \theta)\) & gamma distribution with the shape parameter \(\alpha\) and scale parameter \(\theta\) \\ \hline Laplace\((\mu,\ b)\) & Laplace distribution with location \(\mu\) and diversity or scale parameter \(b\) \\ \hline VonMises\((\mu,\kappa)\) & von Mises distribution with location \(\mu\) and \(\kappa\) concentration parameter. \\ \hline Beta\((\alpha,\beta)\) & beta distribution with the shape parameters \(\alpha\) and \(\beta\) \\ \hline \end{tabular}
\end{table}
Table 4: Definition of notations.

Proof of Theorem 1

We restate the theorem here:

**Theorem** 1 Under Assumption 1 and the generative model in (1), denote any solution of (6) as \(\widehat{\bm{Q}}^{(q)}\)\(q=1,2\). Then, if the mixing matrices \(\bm{A}^{(q)}\) are full column ranks and \(\mathbb{E}[\bm{c}\bm{c}^{\top}])\) is full rank, we have \(\widehat{\bm{Q}}^{(q)}\bm{x}^{(q)}=\bm{\Theta}^{(q)}\bm{c}\). In addition, assume that either of the following is satisfied:

1. The individual elements of the content components are statistically independent and non-Gaussian. In addition, \(c_{i}\overset{(\mathsf{d})}{\neq}kc_{j},\forall i\neq j,\forall k\in\mathbb{R}\) and \(c_{i}\overset{(\mathsf{d})}{\neq}-c_{i},\forall i\), i.e., the marginal distributions of the content elements cannot be matched with each other by mere scaling.
2. The support \(\mathcal{C}\) is a hyper-rectangle, i.e., \(\mathcal{C}=[-a_{1},a_{1}]\times\cdots\times[-a_{d_{\mathcal{C}}},a_{d_{ \mathcal{C}}}]\). Further, suppose that \(c_{i}\overset{(\mathsf{d})}{\neq}kc_{j},\forall i\neq j,\forall k\in\mathbb{R}\) and \(c_{i}\overset{(\mathsf{d})}{\neq}-c_{i},\forall i\).

Then, we have \(\widehat{\bm{Q}}^{(q)}\bm{x}^{(q)}=\bm{\Theta}\bm{c}\), i.e., \(\bm{\Theta}^{(q)}=\bm{\Theta}\) for all \(q=1,2\), where \(\bm{\Theta}^{(q)}\).

We will prove the theorem in following two steps. For the first step we will prove \(\widehat{\bm{Q}}^{(q)}\bm{x}^{(q)}=\bm{\Theta}^{(q)}\bm{c}\) and for second step we will employ either assumption (a) or (b) to prove that \(\bm{\Theta}^{(q)}=\bm{\Theta},\ \forall q=1,2\).

### Linearly transformed content identification

Let us define

\[\bm{H}^{(q)}=\bm{Q}^{(q)}\bm{A}^{(q)}\in\mathbb{R}^{d_{\mathcal{C}}\times(d_{ \mathcal{C}}+d_{\mathcal{P}}^{(q)})}.\]

We want to show that

\[\mathrm{Null}(\bm{H}^{(q)})=\bm{0}\times\mathbb{R}^{d_{\mathcal{ P}}^{(q)}},\] (10)

since this will imply that that \(\bm{H}^{(q)}\) does not depend upon the style component. Combined with the fact that \(\mathrm{rank}(\bm{H}^{(q)})=d_{\mathcal{C}}\), this will imply that \(\bm{H}^{(q)}\) is an invertible function of the content component. To that end, consider the following line of arguments.

Since the objective in (6) matches the distribution for latent random variables \(\widehat{\bm{c}}^{(1)}=\bm{Q}^{(1)}\bm{x}^{(1)}\) and \(\widehat{\bm{c}}^{(2)}=\bm{Q}^{(2)}\bm{x}^{(2)}\), the following holds for any \(\mathcal{R}_{c}\subseteq\mathbb{R}^{d_{\mathcal{C}}},\forall k\in\mathbb{R}\),

\[\mathbb{P}_{\widehat{\varepsilon}^{(1)}}[k\mathcal{R}_{c}] =\mathbb{P}_{\widehat{\varepsilon}^{(2)}}[k\mathcal{R}_{c}],\] (11) \[\overset{(a)}{\Longleftrightarrow}\ \mathbb{P}_{\bm{z}^{(1)}}[\bm{H}^{(1)}_{ \mathrm{PreImg}}(k\mathcal{R}_{c})] =\mathbb{P}_{\bm{z}^{(2)}}[\bm{H}^{(2)}_{\mathrm{PreImg}}(k \mathcal{R}_{c})]\] \[\overset{(b)}{\Longleftrightarrow}\ \mathbb{P}_{\bm{z}^{(1)}}[k\bm{H}^{(1)}_{ \mathrm{PreImg}}(\mathcal{R}_{c})] =\mathbb{P}_{\bm{z}^{(2)}}[k\bm{H}^{(2)}_{\mathrm{PreImg}}( \mathcal{R}_{c})],\]

where, \(\bm{H}^{(q)}_{\mathrm{PreImg}}(\mathcal{R}_{c}):=\{\bm{z}^{(q)}\mid\bm{H}^{(q )}\bm{z}^{(q)}\in\mathcal{R}_{c}\}\) is the pre-image of \(\bm{H}^{(q)}\). \((a)\) follows because \(\mathbb{P}_{\widehat{\varepsilon}^{(q)}}[k\mathcal{R}_{c}]=\mathbb{P}_{\bm{H}^ {(q)}\bm{z}^{(q)}}[k\mathcal{R}_{c}]=\mathbb{P}_{\bm{z}^{(q)}}[\bm{H}^{(q)}_{ \mathrm{PreImg}}(k\mathcal{R}_{c})]\)[68, Section 2.2]. \((b)\) follows because \(\bm{H}^{(q)}\) is a linear operator.

Although (11) holds for any \(\mathcal{R}_{c}\), we will see that it is sufficient to consider a special \(\mathcal{R}_{c}\) to prove (10). To that end, take \(\mathcal{R}_{c}=\mathrm{conv}\{\bm{0},\bm{a}_{1},\ldots,\bm{a}_{d_{\mathcal{C} }}\}\), where \(\bm{a}_{i}\in\mathbb{R}^{d_{\mathcal{C}}}\) such that \(\mathbb{P}_{\widehat{\varepsilon}^{(q)}}[\mathcal{R}_{c}]>0\). Let us take \(\bm{y}_{i}^{(q)}\in\mathbb{R}^{d_{\mathcal{C}}+d_{\mathcal{P}}^{(q)}}\), such that \(\bm{H}^{(q)}\bm{y}_{i}^{(q)}=\bm{a}_{i}\). For reasons that will be clear later, we hope to show that

\[\bm{H}^{(q)}_{\mathrm{PreImg}}(\mathcal{R}_{c})=\mathrm{conv}\{\bm{0},\bm{y}_ {1}^{(q)},\ldots,\bm{y}_{d_{\mathcal{C}}}^{(q)}\}+\mathrm{Null}(\bm{H}^{(q)}).\]

To that end, observe that for any \(\bm{r}\in\mathcal{R}_{c}\), we can represent \(\bm{r}\) as,

\[\bm{r}=\frac{1}{d_{\mathcal{C}}+1}\sum_{i=1}^{d_{\mathcal{C}}}w_{i}\bm{a}_{i}, \text{ for some }\{w_{i}\}_{i=1}^{d_{\mathcal{C}}}\ s.t.\ \sum_{i=1}^{d_{\mathcal{C}}}w_{i}\leq 1,\ \forall i.\]For both view \(q=1,2\), we get,

\[\bm{r} =\frac{1}{d_{\mathrm{C}}+1}\sum_{i=1}^{d_{\mathrm{C}}}w_{i}\bm{H}^{( q)}\bm{y}_{i}^{(q)}\] \[\implies\bm{r} =\bm{H}^{(q)}\left(\frac{1}{d_{\mathrm{C}}+1}\sum_{i=1}^{d_{ \mathrm{C}}}w_{i}\bm{y}_{i}^{(q)}\right)\] \[\bm{H}_{\mathrm{Prelmg}}^{(q)}\left(\frac{1}{d_{\mathrm{C}}+1}\sum_ {i=1}^{d_{\mathrm{C}}}w_{i}\bm{a}_{i}\right)=\frac{1}{d_{\mathrm{C}}+1}\sum_{ i=1}^{d_{\mathrm{C}}}w_{i}\bm{y}_{i}^{(q)}+\mathrm{Null}(\bm{H}^{(q)})\] (12)

We can write,

\[\bm{H}_{\mathrm{Prelmg}}^{(q)}(\mathcal{R}_{c})=\mathrm{conv}\{\bm{0},\bm{y}_{ 1}^{(q)},\ldots,\bm{y}_{d_{\mathrm{C}}}^{(q)}\}+\mathrm{Null}(\bm{H}^{(q)})\] (13)

We have that \(\mathrm{Null}(\bm{H}^{(q)})\subset\mathbb{R}^{d_{\mathrm{C}}+d_{\mathrm{P}}^{ (q)}}\) is a linear subspace with \(\dim(\mathrm{Null}(\bm{H}^{(q)}))=d_{\mathrm{P}}^{(q)}\). Let \(\mathcal{A}^{(q)}=\bm{H}_{\mathrm{Prelmg}}^{(q)}(\mathcal{R}_{c})\). Note that \(\mathbb{P}_{\bm{z}^{(1)}}[k\mathcal{A}^{(1)}]=\mathbb{P}_{\bm{z}^{(2)}}[k \mathcal{A}^{(2)}],\forall k\in\mathbb{R}\) (from (11), and \(\mathbb{P}_{\bm{z}^{(q)}}[\mathcal{A}^{(q)}]>0\) (by the construction of \(\mathcal{R}_{c}\)). Further, the set \(\mathcal{A}^{(q)}\) is of the form

\[\mathrm{conv}\{\bm{0},\bm{y}_{1}^{(q)},\ldots,\bm{y}_{d_{\mathrm{C}}}^{(q)}\}+ \mathcal{P}^{(q)},\]

because \(\mathrm{Null}(\bm{H}^{(q)})\) is a subspace of dimension \(d_{\mathrm{P}}^{(q)}\), hence it satisfies the definition of \(\mathcal{P}^{(q)}\). Hence, Assumption 1 implies that

\[\mathrm{Null}(\bm{H}^{(q)})=\bm{0}\times\mathbb{R}^{d_{\mathrm{P}}^{(q)}}.\]

Denoting the \(N\)th to \(M\)th columns of \(\bm{H}^{(q)}\) by \(\bm{H}^{(q)}(N:M)\), the above is equivalent to saying

\[\bm{H}^{(q)}(d_{\bm{C}}+1:d_{\bm{C}}+d_{\bm{P}}^{(q)})=0.\] (14)

Denote,

\[\bm{\Theta}^{(q)}=\bm{H}^{(q)}(1:d_{\bm{C}})\;\forall\;q=1,2.\]

Then, we can write,

\[\bm{Q}^{(q)}\bm{x}^{(q)}=\bm{\Theta}^{(q)}\bm{c},\;\forall\;q=1,2.\] (15)

Next, we use Assumption (a) or (b) to show that \(\bm{\Theta}^{(1)}=\bm{\Theta}^{(2)}=\bm{\Theta}\). To that end, note that the distribution matching constraint implies that

\[\bm{\Theta}^{(1)}\bm{c} \xRightarrow{\bm{\Theta}^{(2)}}\bm{c}\] \[\implies\bm{c} \xRightarrow{\bm{\Theta}^{(1)}}-1\bm{\Theta}^{(2)}\bm{c}.\]

Hence \(\bm{M}=(\bm{\Theta}^{(1)})^{-1}\bm{\Theta}^{(2)}\) is an invertible matrix such that \(\bm{c}\xRightarrow{\bm{\Theta}^{(d)}}\bm{M}\bm{c}\). However, in the following, we will show that if either Assumption (a) or (b) is satisfied, then \(\bm{M}=\bm{I}\), and thus \(\bm{\Theta}^{(1)}=\bm{\Theta}^{(2)}\).

### Considering Assumption (a)

We want to show that when Assumption (a) is satisfied, if \(\bm{M}\bm{c}\xRightarrow{\bm{c}}\) for any invertible \(\bm{M}\), then \(\bm{M}=\bm{I}\).

Note that \(\bm{M}\bm{c}=[\bm{m}_{1}\ldots\bm{m}_{d_{\mathrm{C}}}]\begin{bmatrix}c_{1}\\ \vdots\\ c_{d_{\mathrm{C}}}\end{bmatrix}\). By Assumption (a), we have that the components of content are statistically independent \(c_{i}\xRightarrow{\bm{\perp}}c_{j},\;i\neq j\), non-Gaussian, and has non-zero kurtosis. Then, according to cumulant multilinearity and additivity properties, the fourth order cumulant tensor \(\mathrm{Cum}(\bm{M}\bm{c})\) of \(\bm{M}\bm{c}\) has the following unique decomposition [69],

\[\mathrm{Cum}(\bm{M}\bm{c})=\sum_{i=1}^{d_{\mathrm{C}}}\kappa_{i}\bm{m}_{i} \circ\bm{m}_{i}\circ\bm{m}_{i}\circ\bm{m}_{i}\] (16)

[MISSING_PAGE_EMPTY:18]

Since the support of \(\bm{M}\bm{c}\) is \(\mathcal{C}\), this implies that \(\forall\bm{c}\in\mathcal{C}\),

\[\bm{c}=\sum_{i=1}^{d_{\mathcal{C}}}\alpha_{i}(\bm{M}\bm{v}_{i}), \qquad\text{for some }-1\leq\alpha_{i}\leq 1\]

The last equation implies that the set of points \(\bm{M}\bm{v}_{i},\forall i\in[d_{\mathcal{C}}]\) also satisfy property (19). Hence, for each \(i\in[d_{\mathcal{C}}]\), \(\bm{M}\bm{v}_{i}=\pm\bm{v}_{j}\) for some unique \(j\in[d_{\mathcal{C}}]\). Note that \(j\) should be unique for each \(i\) because \(\bm{M}\) is invertible, hence \(\bm{M}\) cannot map two orthogonal vectors \(\bm{v}_{i}\) and \(\bm{v}_{k}\), \(i\neq k\), to the same vector \(\pm\bm{v}_{j}\) with same or different signs.

Let \(\bm{V}=[\bm{v}_{1},\dots,\bm{v}_{d_{\mathcal{C}}}]^{T}\). Then one can write

\[\bm{M}\bm{V}=\bm{V}\bm{\Sigma}\bm{\Pi},\]

where \(\bm{\Sigma}\) is some diagonal matrix with diagonal entries from \(\{+1,-1\}\) and \(\bm{\Pi}\) is a permutation matrix. Then the above implies

\[\bm{M}\bm{\Lambda}\bm{I} =\bm{\Lambda}\bm{I}\bm{\Sigma}\bm{\Pi}\] \[\implies\bm{M} =\bm{\Lambda}\bm{\Sigma}\bm{\Pi}\bm{\Lambda}^{-1}.\]

Hence \(\bm{M}\) is a permutation and scaling matrix.

Finally, by the same argument presented in last paragraph of Sec. B.2 (i.e., proof with Assumption (a)), we conclude that \(\bm{M}\) is an identity matrix.

## Appendix C Proof of Theorem 2

We restate the theorem here:

**Theorem 2** Consider the mixture model in (8). Assume that \(\mathrm{rank}(\bm{A})=d_{\mathcal{C}}+d_{\mathcal{P}}\) and \(\mathrm{rank}(\mathbb{E}[\bm{c}\bm{c}^{\top}])=d_{\mathcal{C}}\), and that Assumption 2 holds. Denote \(\bm{\widehat{Q}}\) as any solution of (6) by constraining \(\bm{Q}=\bm{Q}^{(1)}=\bm{Q}^{(2)}\). Then, we have \(\bm{\widehat{Q}}\bm{x}^{(q)}=\bm{\Theta}\bm{c}\).

One can follow the same argument as in the step 1 of proof in B.

Let us define

\[\bm{H}=\bm{Q}\bm{A}\in\mathbb{R}^{d_{\mathcal{C}}\times(d_{\mathcal{C}}+d_{ \mathcal{P}})}.\]

We want to show that

\[\mathrm{Null}(\bm{H})=\bm{0}\times\mathbb{R}^{d_{\mathcal{P}}},\] (20)

since this will imply that that \(\bm{H}\) does not depend upon the style component. Combined with the fact that \(\mathrm{rank}(\bm{H})=d_{\mathcal{C}}\), this will imply that \(\bm{H}\) is an invertible function of the content component. To that end, consider the following line of arguments.

Since the objective in (6) matches the distribution for latent random variables \(\bm{\widehat{c}}^{(1)}=\bm{Q}\bm{x}^{(1)}\) and \(\bm{\widehat{c}}^{(2)}=\bm{Q}\bm{x}^{(2)}\), the following holds for any \(\mathcal{R}_{c}\subseteq\mathbb{R}^{d_{\mathcal{C}}},\supseteq k\in\mathbb{R}\)

\[\mathbb{P}_{\bm{\widehat{c}}^{(1)}}[k\mathcal{R}_{c}] =\mathbb{P}_{\bm{\widehat{c}}^{(2)}}[k\mathcal{R}_{c}],\] \[\overset{(a)}{\iff}\ \mathbb{P}_{\bm{z}^{(1)}}[\bm{H}_{ \mathrm{PreImg}}(k\mathcal{R}_{c})] =\mathbb{P}_{\bm{z}^{(2)}}[\bm{H}_{\mathrm{PreImg}}(k\mathcal{R }_{c})]\] (21) \[\overset{(b)}{\iff}\ \mathbb{P}_{\bm{z}^{(1)}}[k\bm{H}_{ \mathrm{PreImg}}(\mathcal{R}_{c})] =\mathbb{P}_{\bm{z}^{(2)}}[k\bm{H}_{\mathrm{PreImg}}(\mathcal{R }_{c})],\] (22)

where, \(\bm{H}_{\mathrm{PreImg}}(\mathcal{R}_{c}):=\{\bm{z}\mid\bm{H}\bm{z}\in \mathcal{R}_{c}\}\) is the pre-image of \(\bm{H}\). \((a)\) follows because \(\mathbb{P}_{\bm{\widehat{c}}^{(q)}}[k\mathcal{R}_{c}]=\mathbb{P}_{\bm{H}\bm{z}^ {(q)}}[k\mathcal{R}_{c}]=\mathbb{P}_{\bm{z}^{(q)}}[\bm{H}_{\mathrm{PreImg}}(k \mathcal{R}_{c})]\)[68, Section 2.2]. \((b)\) follows because \(\bm{H}\) is a linear operation.

Although (21) holds for any \(\mathcal{R}_{c}\), we will see that it is sufficient to consider a special \(\mathcal{R}_{c}\) to prove (20). To that end, take \(\mathcal{R}_{c}=\mathrm{conv}\{\bm{0},\bm{a}_{1},\dots,\bm{a}_{d_{\mathcal{C}}}\}\), where \(\bm{a}_{i}\in\mathbb{R}^{d_{\mathcal{C}}}\) such that \(\mathbb{P}_{\bm{\widehat{c}}^{(q)}}[\mathcal{R}_{c}]>0\). Let us take \(\bm{y}_{i}\in\mathbb{R}^{d_{\mathcal{C}}+d_{\mathcal{P}}}\), such that \(\bm{H}\bm{y}_{i}=\bm{a}_{i}\). For reasons that will be clear later, we hope to show that

\[\bm{H}_{\mathrm{PreImg}}(\mathcal{R}_{c})=\mathrm{conv}\{\bm{0},\bm{y}_{1}, \dots,\bm{y}_{d_{\mathcal{C}}}\}+\mathrm{Null}(\bm{H}).\]

[MISSING_PAGE_FAIL:20]

for some invertible matrices \(\bm{\Theta}^{(q)},\forall q\). Hence,

\[\bm{\Theta}^{(1)}\bm{c} \stackrel{{(\mathsf{d})}}{{\Longrightarrow}}\bm{ \Theta}^{(2)}\bm{c}\] (28) \[\implies\bm{c} \stackrel{{(\mathsf{d})}}{{\Longrightarrow}}(\bm{ \Theta}^{(1)})^{-1}\bm{\Theta}^{(2)}\bm{c}.\] (29)

Hence we can have linear transformation \(\bm{M}:=(\bm{\Theta}^{(1)})^{-1}\bm{\Theta}^{(2)}\) which has same probability density as \(\mathbb{P}_{\bm{c}}\). However, the sample matching constraint (9), for \(\ell-\)th sample implies that

\[\bm{Q}^{(1)}\bm{x}_{\ell}^{(1)} =\bm{Q}^{(2)}\bm{x}_{\ell}^{(2)}\] \[\implies\bm{\Theta}^{(1)}\bm{c}_{\ell} =\bm{\Theta}^{(2)}\bm{c}_{\ell}\] \[\implies\bm{c}_{\ell} =(\bm{\Theta}^{(1)})^{-1}\bm{\Theta}^{(2)}\bm{c}_{\ell}\] \[\implies\bm{c}_{\ell} =\bm{M}\bm{c}_{\ell}.\]

Let \(\bm{C}=[\bm{c}_{1}\dots\bm{c}_{N_{p}}]\). Then the above implies:

\[\bm{C} =\bm{M}\bm{C}\] \[\implies(\bm{M}-\bm{I})\bm{C} =\bm{0}.\]

Now we show that \(\bm{C}\) is a full row rank matrix, which implies that \(\bm{M}-\bm{I}=\bm{0}\implies\bm{M}=\bm{I}\). To that end, note that random variables \(\bm{x}_{i}^{(1)}\) and \(\bm{x}_{i}^{(2)}\) being i.i.d implies that \(\bm{c}^{(i)}\) are i.i.d from \(\mathbb{P}_{\bm{c}}\). This implies that for any \(1\leq i\leq|\mathcal{L}|\),

\[\Pr[\bm{c}_{i}\in\mathrm{span}(\{\bm{c}_{n_{1}},\dots,\bm{c}_{n_{d_{\mathrm{C} }}-1}\})]=0.\] (30)

This is because \(\mathrm{span}(\{\bm{c}_{n_{1}},\dots,\bm{c}_{n_{d_{\mathrm{C}}}-1}\})\) for \(n_{j}\in[|\mathcal{L}|]\), is a lower dimensional subspace in \(\mathbb{R}^{d_{\mathrm{C}}}\), which has zero probability under absolutely continuous distribution \(\mathbb{P}_{\bm{c}}\). Hence any \(d_{\mathrm{C}}\) out of \(|\mathcal{L}|\) column vectors in \(\bm{C}\) are linearly independent with probability 1.

This concludes the proof.

## Appendix E Detailed Identifiability Conditions of Existing Results

### Identifiability of CCA

 **Theorem 4** (Identifiability of Aligned SCA via CCA [1]).: _Under (1), assume that every aligned pair \((\bm{x}^{(1)},\bm{x}^{(2)})\) share the same \(\bm{c}\), and that \(\bm{A}^{(q)}\) has full column rank. Also assume that there exists an \(N\)-sample set \(\{\ell_{1},\dots,\ell_{N}\}\) such that \([\bm{C}^{\top},(\bm{P}^{(1)})^{\top},(\bm{P}^{(2)})^{\top}]\in\mathbb{R}^{N \times(d_{\mathrm{C}}+d_{\mathrm{P}}^{(1)}+d_{\mathrm{P}}^{(2)})}\) has full column rank, where \(\bm{C}=[\bm{c}_{\ell_{1}},\dots\bm{c}_{\ell_{N}}]\in\mathbb{R}^{d_{\mathrm{C} }\times N}\) and \(\bm{P}^{(q)}=[\bm{p}_{\ell_{1}}^{(q)}\dots\bm{p}_{\ell_{N}}^{(q)}]\in\mathbb{R }^{d_{\mathrm{P}}^{(q)}\times N}\) for \(q=1,2\). Denote \((\widehat{\bm{Q}}^{(1)},\widehat{\bm{Q}}^{(2)})\) as an optimal solution of the CCA formulation. Then, we we have_

\[\widehat{\bm{Q}}^{(q)}\bm{x}^{(q)}=\bm{\Theta}\bm{c},\]

_where \(\bm{\Theta}\) is nonsingular._

In the above theorem, one can see that \(N\geq(d_{\mathrm{C}}+d_{\mathrm{P}}^{(1)}+d_{\mathrm{P}}^{(2)})\) is a _necessary condition_ for the identifiability of \(\bm{\Theta}\bm{c}\). Hence, CCA needs at least \(d_{\mathrm{C}}+d_{\mathrm{P}}^{(1)}+d_{\mathrm{P}}^{(2)}\) paired samples for identifiability.

### Identifiability of Unaligned SCA in [8]

We summarize the result in [8] in the following

**Theorem 5** (Identifiability of Unaligned SCA via ICA [8]).: _Under (1), assume that the following are met: (i) The conditions for ICA identifiability [33] is met by each modality, including that the components of \(\bm{z}^{(q)}=[\bm{c}^{\top},(\bm{p}^{(q)})^{\top}]^{\top}\) are mutually statistically independent and contain at most one Gaussian variable. In addition, each \(\bm{z}_{i}^{(q)}\) has unit variance; (ii) \(\mathbb{P}_{z_{i}^{(q)}}\neq\mathbb{P}_{z_{j}^{(q)}},\mathbb{P}_{z_{i}^{(q)}} \neq\mathbb{P}_{-z_{j}^{(q)}}\ \forall i,j\in[d_{\mathrm{C}}+d_{\mathrm{P}}^{(q)}],\ i\neq j\). Then, assume that \((i_{m},j_{m})\) are obtained by ICA followed by cross domain matching (see the part on Unaligned SCA in Section 2 ) for \(m=1,\dots,d_{\mathrm{C}}\).__Denote \(\widehat{c}^{(1)}_{m}=\bm{e}^{\mathsf{T}}_{i_{m}}\widehat{\bm{z}}^{(1)}\) and \(\widehat{c}^{(2)}_{m}=\bm{e}^{\mathsf{T}}_{j_{m}}\widehat{\bm{z}}^{(2)}\). We have the following:_

\[\widehat{c}^{(q)}_{m}=kc^{(q)}_{\bm{\pi}(m)},\ m\in[d_{\mathrm{C}}],\] (31)

_where \(k\in\{+1,-1\}\) and \(\bm{\pi}\) is a permutation of \(\{1,\ldots,d_{\mathrm{C}}\}\)._

## Appendix F Additional Synthetic Data Experiments

Hyperparameter Settings:We use Adam optimizer [70] to solve (7) and learn matrices \(\bm{Q}^{(q)},\ q=1,2\) and the discriminator \(f\). We set the initial learning rate of matrix and discriminator to be \(0.009\) and \(0.0008\) respectively. We set the \(\lambda=0.1\) in (7) to enforce (6c). For weak supervision experiment in F, we set \(\beta=0.01\) in (9). We generate total of 100,000 samples in each domain. For our experiment we set the batch size to be 1,000 and run (7) for 50 epochs. Our discriminator is a 6-layer multilayer perceptron (MLP) with hidden units { 1024, 521, 512, 256, 128, 64 } in each layer. All the layers use leaky ReLU activation functions [71] with a slope of 0.2 except for the last layer which has sigmoid activations. We include a label smoothing coefficient of 0.2 in the discriminator predictions as suggested in [40].

Additional Details for Validation of Theorem 1 in Sec. 3:Here we explain the data generation details of the result shown in Fig. 3. For the result in top row, we sample \(c_{1}\) from a Gaussian mixture with three Gaussian components. Each component follows a normal distribution \(\mathcal{N}(\mu,\ 2)\) where \(\mu\sim\mathcal{N}(0,\ 10)\). The second component, i.e., \(c_{2}\), is independently sampled from the gamma distribution \(\texttt{Gamma}(1,\ 3)\). The private components are sampled from \(p^{(1)}\ \sim\texttt{Laplace}(1.0,\ 6.5)\) and \(p^{(2)}\ \sim\texttt{Uniform}[-10,\ 10]\), both only having one dimension. In the bottom row, we sample \(\bm{c}\in\mathbb{R}^{2}\sim\texttt{VonMises}(2.5,\ 2.0)\) distribution. The private components satisfy \(p^{(1)}\sim\texttt{Laplace}(1.0,\ 6.5)\) and \(p^{(2)}\ \sim\texttt{Gamma}(0.5,\ 3.0)\). Each element of mixing matrices are sampled from \(\bm{A}^{(q)}_{ij}\sim\mathcal{N}(0,1),\ q=1,2\). The readers are referred to Table 4 for the definition of notations used for distributions.

Validation of Theorem 1 under different sample sizes and imbalanced data:Here we observe the shared component identification performance of the proposed method numerically. We conducted two experiments in different settings. First, we vary the sample sizes in both modalities, but the two modalities have the same sample size. Second, we only vary the sample size of modality 2 while keeping the sample size of modality 1 fixed. This way, we create the data imbalance between modalities. Note that the shared components are identified if the following two conditions are met:

1. \(\bm{Q}^{(q)}\bm{A}^{(q)}=[\bm{\Theta},\bm{0}]\), i.e., \(\widehat{\bm{\Theta}}^{(1)}=\widehat{\bm{\Theta}}^{(2)}=\bm{\Theta}\) and
2. \(\|\bm{Q}^{(q)}\bm{A}^{(q)}(d_{\mathrm{C}}:d_{\mathrm{C}}+d_{\mathrm{P}}^{(q)}) \|_{\mathrm{F}}=\bm{0}\).

Therefore, we use the above as our performance evaluation metrics. For the following experiments (Table 5 and 6), we generate the data for the two modalities by sampling a two-dimensional content \(\bm{c}\sim\texttt{VonMises}(2.5,\ 2.0)\) and private components from \(p^{(1)}\sim\texttt{Laplace}(1.0,\ 6.5)\) and \(p^{(2)}\ \sim\texttt{Gamma}(0.5,\ 3.0)\). The elements of the mixing matrices are sampled as \(\bm{A}^{(q)}_{ij}\sim\mathcal{N}(0,1),\ q=1,2\). We report the mean and standard deviation of \(\|\widehat{\bm{\Theta}}^{(1)}(1:d_{\mathrm{C}})-\widehat{\bm{\Theta}}^{(2)}(1: d_{\mathrm{C}})\|_{\mathrm{F}}\) and \(1/2\sum_{q=1}^{2}\|\widehat{\bm{\Theta}}^{(q)}(d_{\mathrm{C}}:d_{\mathrm{C}}+d_ {\mathrm{P}}^{(q)})\|_{\mathrm{F}}\) obtained from 5 different runs.

Table 5 shows the performance of SCA and CCA under different sample sizes (i.e., \(N\)). One can see that the proposed method (SCA) clearly identifies the shared components even when only 100 samples are available. The performance starts to deteriorate when \(N\leq 50\), probably because the min-max optimization problem is difficult to solve with very few samples. CCA does not really work under this setting as it needs aligned cross-domain samples.

Table 6 shows the performance of SCA in the cases where two modalities have unbalanced data sizes. The number of samples in the first modality is fixed to 100,000 while the second modality's data size varies from 10,000 to 10 samples. The data generation process remains the same as in the previous experiment. One can see that even under obvious cross-domain data size imbalance (e.g., 100,000 to 1,000), the proposed method performs reasonably well in terms of shared component identification.

[MISSING_PAGE_EMPTY:23]

**Baselines and Training Setup**: The baselines are representative DA methods, namely, DANN[25], MDD[60], MCC[61], SDAT[62], and ELS[63]. We use the implementations of DANN, MDD, and MCC from the https://github.com/thuml/Transfer-Learning-Library, while SDAT and ELS are taken from https://github.com/yfzhang114/Environment-Label-Smoothing. In all the baselines, the classifier is jointly optimized with the feature extractor \(\bm{Q}\) which arguably regularizes towards more classification-friendly geometry of the shared features; see [72, 73]. Following their training strategy, we also append a cross-entropy (CE) based classifier training module to our loss in (7) (which learns our feature extractor \(\bm{Q}\)). The CE part uses \(\bm{Q}\bm{x}^{(1)}\) and the labels of the sources as inputs to learn the classifier, i.e.,

\[\mathcal{L}_{\mathrm{CE}}=-\gamma\ \sum_{\ell=1}^{N}\sum_{k=1}^{K}\mathbb{I}[y_{ \ell}=k]\log\bm{r_{\theta}}([\bm{Q}\bm{x}_{\ell}^{(1)}]_{k}),\] (32)

where \(\bm{r_{\theta}}(\cdot):\mathbb{R}^{d_{\mathrm{CE}}}\rightarrow\mathbb{R}^{K}\) is the classifier that aims to map the learned feature vector \(\bm{Q}\bm{x}_{\ell}^{(1)}\) to a \(K\)-dimensional probability mass function (i.e., the distribution of the ground-truth label over \(K\) classes), \(y_{\ell}\in[K]\) represents the label of the \(\ell\)th sample in source domain, and the indicator function \(\mathbb{I}[y_{\ell}=k]=1\) only when the event \(y_{\ell}=k\) happens (other wise \(\mathbb{I}[y_{\ell}=k]=0\). The \(\gamma\geq 0\) is the tunable parameter. The joint loss is still differentiable, and thus we still use the Adam optimizer to jointly optimize \(\bm{Q}\) and \(\bm{\theta}\).

Additional domain adaptation experiment using CLIP features:In this experiment, we use CLIP as an image encoder as it learns informative and transferable features from very large datasets [35]. Table 8 and Table 9 show the results on _Office-31_ and _Office-Home_ datasets, respectively, using CLIP embeddings. Compared to the results on ResNet50 embeddings in Table 1 and Table 2, one can observe that all the methods, including proposed method, gains an advantage. This is likely because CLIP was trained on a large and diverse dataset [35], which may have include similar content to the _Office-31_ and _Office-Home_ datasets.

The results show that, as a foundation model, CLIP can already unify the embeddings of the source and target domains to a reasonable extent. In addition, our model and algorithm when combined with regularization techniques like MCC, can still further enhance performance, even with simple post-processing of CLIP embeddings.

\begin{table}
\begin{tabular}{|l|c|} \hline
**Parameter** & **Value** \\ \hline Optimizer & Adam \\ Learning rate of \(\bm{Q}\) & \(0.0002\) \\ Learning rate of \(f\) & \(0.00002\) \\ Learning rate of classifier & \(0.02\) \\ Learning rate decay of classifier & \(0.75\) \\ \(\lambda\) (see Eq. (7)) & 1.0 \\ \(\gamma\) (see Eq. (32)) & 0.1 \\ Batch size & 64 \\ Number of epochs & 20 \\ Discriminator; \(f\) architecture & 6 layers, hidden units {1024, 521, 512, 256, 128, 64} \\ Activation functions of \(f\) & Leaky ReLU (slope 0.2), Sigmoid (final layer) \\ Label smoothing coefficient in \(f\) & 0.2 \\ \hline \end{tabular}
\end{table}
Table 7: Hyperparameter settings for domain adaptation.

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|} \hline
**source \(\rightarrow\) target** & CLIP & DANN & MDD & MCC & SDAT & SDAT+MOC & ELS & ELS+MOC & Proposed & Proposed+MOC \\ \hline \(\bm{A}\)\(\rightarrow\) W & 93.4 & 93.7 & 94.1 & 95.9 & 95.0 & 98.1 & 96.8 & **98.7** & 95.3 & 98.3 \\ \(\bm{D}\)\(\rightarrow\) W & 99.1 & **100.0** & 99.3 & **100.0** & **100.0** & **100.0** & **100.0** & 99.7 & **100.0** \\ \(\bm{W}\)\(\rightarrow\) D & **100.0** & 99.5 & 99.8 & 94.9 & 95.9 & 99.5 & **100.0** & **100.0** & 99.9 \\ \(\bm{A}\)\(\rightarrow\) D & 91.9 & 92.1 & 94.2 & 97.7 & 95.7 & 97.7 & 95.0 & 97.7 & 93.7 & **99.1** \\ \(\bm{D}\)\(\rightarrow\) A & 81.4 & 81.9 & 79.2 & 85.7 & 81.2 & 84.6 & 81.3 & 83.0 & 83.9 & **85.9** \\ \(\bm{W}\)\(\rightarrow\) A & 81.7 & 83.0 & 82.2 & 84.7 & 84.7 & 86.7 & 82.6 & 86.3 & 85.8 & **87.1** \\ \hline
**Average** & 91.2 & 91.6 & 91.4 & 93.7 & 92.6 & 94.4 & 92.5 & 94.2 & 93.0 & **95.0** \\ \hline \end{tabular}
\end{table}
Table 8: Classification accuracy on the target domain of _office-31_ dataset using CLIP embeddings.

Visualization Result:Fig. 6 shows the 2-dimensional visualization of the CLIP-learned features (\(d=256\)) from two domains, namely, DSLR and Amazon images (_Office-31_), using t-SNE. One can see that CLIP could roughly group the same classes from the two domains together. But the proposed method can further pull the circles and the triangle markers together--meaning that the \(\bm{Q}\) really learns shared representations of the same data in the DSLR and Amazon domains.

### Single-cell Sequence Analysis

Hyperparameter Settings:The hyperparameter settings for single-cell sequence analysis is presented in Table. 10.

Baseline:For more details on baseline refer to the implementation in https://github.com/uhlerlab/cross-modal-autoencoders.

### Multi-lingual Information Retrieval

Hyperparameter Settings:The hyperparameter settings for multi-lingual information retrieval is described in the Table. 11.

Figure 6: _Office-31_ dataset: DSLR images features represented as circle markers, Amazon images features represented as triangle markers. Different color represent different classes.

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|} \hline
**source \(\rightarrow\) target** & CLIP & DANN & MDD & MCC & SDAT+ACC & SLLS & ELS+ACC & Proposed & Proposed+RCC \\ \hline \(\mathbf{Ar}\rightarrow\mathbf{Cl}\) & 78.0 & 80.4 & 80.2 & 80.9 & 79.6 & 80.7 & 80.0 & 81.3 & 82.0 & **83.2** \\ \(\mathbf{Ar}\rightarrow\mathbf{Pr}\) & 88.7 & 91.7 & 88.9 & 93.3 & 89.4 & 94.3 & 91.2 & 93.9 & 91.4 & **95.2** \\ \(\mathbf{Ar}\rightarrow\mathbf{Rw}\) & 90.6 & 90.2 & 91.0 & 92.8 & 90.1 & 92.1 & 89.4 & 92.1 & 91.9 & **93.8** \\ \(\mathbf{Cl}\rightarrow\mathbf{Ar}\) & 85.2 & 83.2 & 85.1 & 87.4 & 83.1 & 86.1 & 84.4 & 87.2 & 85.4 & **87.7** \\ \(\mathbf{Cl}\rightarrow\mathbf{Pr}\) & 89.0 & 89.7 & 90.1 & 93.4 & 90.2 & 93.5 & 89.7 & 93.5 & 91.1 & **94.9** \\ \(\mathbf{Cl}\rightarrow\mathbf{Rw}\) & 89.8 & 88.1 & 89.4 & 89.3 & 87.9 & 90.5 & 88.3 & 90.6 & 90.4 & **92.0** \\ \(\mathbf{Pr}\rightarrow\mathbf{Ar}\) & 78.2 & 80.4 & 81.8 & 83.7 & 81.0 & 85.0 & 81.8 & 86.1 & 83.0 & **86.6** \\ \(\mathbf{Pr}\rightarrow\mathbf{Cl}\) & 72.7 & 75.8 & 75.8 & 78.4 & 75.4 & 78.5 & 75.7 & 78.3 & 77.7 & **81.3** \\ \(\mathbf{Pr}\rightarrow\mathbf{Rw}\) & 89.0 & 90.4 & 90.3 & 92.6 & 90.8 & 92.1 & 90.0 & 92.3 & 91.0 & **93.6** \\ \(\mathbf{Rw}\rightarrow\mathbf{Ar}\) & 86.6 & 84.9 & 85.9 & 85.3 & 85.3 & 86.3 & 85.4 & 87.0 & 87.7 & **88.2** \\ \(\mathbf{Rw}\rightarrow\mathbf{Cl}\) & 78.1 & 79.4 & 79.8 & 79.0 & 78.6 & 79.8 & 79.1 & 79.8 & 81.3 & **81.8** \\ \(\mathbf{Rw}\rightarrow\mathbf{Pr}\) & 94.3 & 94.6 & 93.9 & 95.9 & 94.8 & 95.4 & 94.0 & 95.2 & 94.7 & **96.0** \\ \hline
**Average** & 85.0 & 85.7 & 86.0 & 87.6 & 85.5 & 87.8 & 85.7 & 88.1 & 87.3 & **89.5** \\ \hline \end{tabular}
\end{table}
Table 9: Classification accuracy on the target domain of _office-Home_ dataset using CLIP embeddings.

Additional Results:Table 12 reports the P@5 and P@10 scores over the test data, calculated for different source and target language pairs. It can be observed that the proposed method achieves higher precision than Adv in most of the translation tasks (e.g., by at least \(1\%\) in the **en\(\rightarrow\)es** and **es\(\rightarrow\)en** tasks) when considering both P@5 and P@10 scores.

### Computation resources

All the experiments were run on Nvidia H100 GPU. The approximate runtime for a single run of the algorithm is 20 minutes for multi-lingual information retrieval, 15 minutes for domain adaptation, and 3 minutes for single-cell sequence analysis.

Complexity Analysis:Since the proposed objective is tackled using stochastic gradient (SG)-based first-order iterative method, the computational complexity of the proposed algorithm depends upon the per-iteration complexity.

For each sample, the per-iteration complexity is composed of a forward pass and a backward pass.

Note that the problem size depends upon \(d^{(q)}\) (the data dimension), \(d_{C}\), and the batch size \(B\). We assume that the network architecture of \(\bm{f}\) (the number of layers and hidden units in each layer) is fixed, represented by \(\bm{f}=\bm{\sigma}\circ\bm{F}_{L}\circ\cdots\circ\bm{\sigma}\circ\bm{F}_{1}\), where \(\bm{F}_{\ell}\) and \(\bm{\sigma}\) are the linear layer (matrix) and activation function corresponding to the \(\ell\)th layer. Only the input dimension \(d_{\mathrm{C}}\) of first matrix \(\bm{F}_{1}\), varies with the problem size.

The forward pass involves computing \(\widehat{\bm{c}}^{(q)}=\bm{Q}^{(q)}\bm{x}^{(q)}\) and \(\bm{f}^{(q)}(\widehat{\bm{c}}^{(q)})\), both of which scale linearly with \(d_{\mathrm{C}},d^{(q)}\) and the batch size \(B\). Hence, the forward pass time complexity is \(O(Bd_{\mathrm{C}}(d^{(1)}+d^{(2)}))\).

Similarly, the backward pass requires computing of \(\frac{\partial L}{\partial\widehat{\bm{c}}^{(q)}_{\mathrm{C}}},\forall i\in[d_ {\mathrm{C}}]\) and \(\frac{\partial\widehat{\bm{c}}^{(q)}_{\mathrm{C}}}{\partial\bm{Q}^{(q)}{}_{jk }},\forall i\in[d_{\mathrm{C}}],j,k\in[d_{\mathrm{C}}\times d^{(q)}]\), where \(L\) is the loss function. The first gradient computation is linear in \(Bd_{\mathrm{C}}\), while the second gradient computation has a complexity of \(O(Bd_{\mathrm{C}}(d^{(1)}+d^{(2)}))\). Hence the computational complexity of our method is \(O(Bd_{\mathrm{C}}(d^{(1)}+d^{(2)}))\).

\begin{table}
\begin{tabular}{|l|c|} \hline
**Parameter** & **Value** \\ \hline Optimizer & Adam \\ Learning rate of \(\bm{Q}^{(q)}\) & \(0.001\) \\ Learning rate of \(f\) & \(0.0001\) \\ \(\lambda\) (see Eq. (7)) & 1.0 \\ \(\beta\) (see Eq. (9)) & 10.0 \\ Batch size & 32 \\ Number of epochs & 75 \\ Discriminator; \(f\) architecture & 6 layers, hidden units {1024, 521, 512, 256, 128, 64} \\ Activation functions of \(f\) & Leaky ReLU (slope 0.2), Sigmoid (final layer) \\ Label smoothing coefficient in \(f\) & 0.2 \\ \hline \end{tabular}
\end{table}
Table 10: Hyperparameter settings for single-cell sequence analysis.

\begin{table}
\begin{tabular}{|l|c|} \hline
**Parameter** & **Value** \\ \hline Optimizer & Adam \\ Learning rate of \(\bm{Q}\) & \(0.0001\) \\ Learning rate of \(f\) & \(0.00001\) \\ \(\lambda\) (see Eq. (7)) & 1.0 \\ Batch size & 32 \\ Number of epochs & 5 \\ Discriminator; \(f\) (similar as in [21]) & 2 layers, 2048 hidden units each \\ Activation functions of \(f\) & Leaky ReLU (slope 0.2), Sigmoid (final layer) \\ Dropout rate (Input) in \(f\) & 0.1 \\ Label smoothing coefficient in \(f\) & 0.2 \\ \hline \end{tabular}
\end{table}
Table 11: Hyperparameter settings for multi-lingual information retrieval.

The memory complexity involves storing the network parameters and the aforementioned gradients. Hence, only the size of \(\bm{Q}^{(q)}\), \(\bm{F}_{1}\), and \(\bm{c}^{(q)}\) changes with the problem dimension. The size of \(\bm{Q}^{(q)}\), \(\bm{F}_{1}\), and \(\bm{c}^{(q)}\) are \(d_{\mathrm{C}}d^{(q)}\), \(O(d_{\mathrm{C}})\) and \(d_{\mathrm{C}}\), respectively. Therefore, the space complexity is \(O(Bd_{\mathrm{C}}(d^{(1)}+d^{(2)}))\).

In summary, both the memory and computational complexities of the proposed method scales linearly with \(d_{\mathrm{C}}\).

## Appendix H Extension: Private Component Identification

Theorems 1-3 are concerned with learning the shared component \(\bm{c}\). The goal, there, was to ensure that \(\bm{Q}_{\mathrm{C}}^{(q)}\bm{x}^{(q)}\bm{\Theta}\bm{c},\forall q\). In some cases, the private components \(\bm{p}^{(q)}\) is also of interest [6, 31, 74]. To learn \(\bm{p}^{(q)}\), we propose to solve the following learning criterion:

\[\mathrm{find} \bm{Q}_{\mathrm{C}}^{(q)}\in\mathbb{R}^{d_{\mathrm{C}}\times d^{ (q)}},\bm{Q}_{\mathrm{P}}^{(q)}\in\mathbb{R}^{d_{\mathrm{P}}^{(q)}\times d^{(q )}}\ q=1,2,\] (33a) subject to \[\bm{Q}_{\mathrm{C}}^{(1)}\bm{x}^{(1)}\overset{(\ref{eq:d})}{ \rightleftharpoons}\bm{Q}_{\mathrm{C}}^{(2)}\bm{x}^{(2)},\] (33b) \[\bm{Q}_{\mathrm{C}}^{(q)}\bm{x}^{(0)}\perp\bm{Q}_{\mathrm{P}}^{(q )}\bm{x}^{(q)}\quad q=1,2,\] (33c) \[\bm{Q}_{\mathrm{C}}^{(q)}\mathbb{E}\left[\bm{x}^{(q)}(\bm{x}^{(q )})^{\top}\right](\bm{Q}_{\mathrm{C}}^{(q)})^{\top}=\bm{I}\quad q=1,2,\] (33d) \[\bm{Q}_{\mathrm{P}}^{(q)}\mathbb{E}\left[\bm{x}^{(q)}(\bm{x}^{(q )})^{\top}\right](\bm{Q}_{\mathrm{P}}^{(q)})^{\top}=\bm{I}\quad q=1,2,\] (33e)

where \(\bm{u}\perp\!\!\!\perp\bm{v}\) means that the random vectors \(\bm{u}\) and \(\bm{v}\) are independent with each other.

For implementation we use following criterion,

\[\min_{\bm{Q}_{\mathrm{C}}^{(1)},\bm{Q}_{\mathrm{C}}^{(2)}\bm{Q}_ {\mathrm{P}}^{(1)},\bm{Q}_{\mathrm{P}}^{(1)}}\max_{f}\ \mathbb{E}_{\bm{x}^{(1)}}\log\left(f(\bm{Q}_{\mathrm{C}} ^{(1)}\bm{x}^{(1)})\right)+\mathbb{E}_{\bm{x}^{(2)}}\log\left(1-f(\bm{Q}_{ \mathrm{C}}^{(2)}\bm{x}^{(2)})\right)\] \[+\lambda\sum_{q=1}^{2}\mathcal{R}\left(\bm{Q}_{\mathrm{C}}(^{(q) })\right)+\omega\sum_{q=1}^{2}\mathcal{R}\left(\bm{Q}_{\mathrm{P}}(^{(q)}) \right)+\rho\sum_{q=1}^{2}\mathrm{HSIC}(\bm{Q}_{\mathrm{C}}^{(q)}\bm{x}^{(q)},\bm{Q}_{\mathrm{P}}^{(q)}\bm{x}^{(q)}),\] (34)

where, first two term are adversarial loss for distribution matching. The constraint on (33d) and (33e) are enforced as \(\mathcal{R}(\bm{Q}_{\mathrm{C}}^{(q)})\) and \(\mathcal{R}(\bm{Q}_{\mathrm{C}}^{(q)})\) respectively, where \(\mathcal{R}(\bm{Q}^{(q)})=\|\bm{Q}^{(q)}\mathbb{E}[\bm{x}^{(q)}(\bm{x}^{(q)})^ {\top}](\bm{Q}^{(q)})^{\top}-\bm{I}\|_{\mathrm{F}}^{2}\). The constraint on (33c) is realized with Hilbert-Schmidt Independence Criterion (HSIC) [75]. HSIC measures the independence between two distribution. So, we minimize HSIC between estimated shared component and estimated private component to promote independence between shared and private components.

We show that under some reasonable conditions the block \(\bm{p}^{(q)}\) can also be learned up to a matrix multiplication:

\begin{table}
\begin{tabular}{|l|l||l|l||l|l|} \hline P@k & source \(\rightarrow\) target & Adv - NN & proposed - NN & Adv - CSLS & proposed - CSLS \\ \hline \hline \multirow{6}{*}{P@5} & **en\(\rightarrow\)es** & 77.9 & **78.8** & 83.6 & **85.2** \\  & **es\(\rightarrow\)en** & 73.2 & **79.0** & 83.2 & **86.0** \\ \cline{2-6}  & **en\(\rightarrow\)it** & 68.0 & **70.8** & 76.8 & **82.4** \\  & **it\(\rightarrow\)en** & **79.0** & 77.6 & **71.2** & 66.9 \\ \cline{2-6}  & **en\(\rightarrow\)fr** & **79.2** & 75.2 & **85.7** & 85.2 \\  & **fr\(\rightarrow\)en** & 69.8 & **73.5** & 77.2 & **83.5** \\ \hline \hline \multirow{6}{*}{P@10} & **en\(\rightarrow\)es** & 82.4 & **82.6** & 87.0 & **87.8** \\  & **es\(\rightarrow\)en** & 79.0 & **82.3** & 87.2 & **88.8** \\ \cline{2-6}  & **en\(\rightarrow\)it** & 74.1 & **75.6** & 81.7 & **85.6** \\ \cline{1-1}  & **it\(\rightarrow\)en** & 71.5 & **75.8** & 82.0 & **82.9** \\ \cline{1-1} \cline{2-6}  & **en\(\rightarrow\)fr** & **83.4** & 79.1 & 88.0 & **88.4** \\ \cline{1-1}  & **fr\(\rightarrow\)en** & 73.8 & **77.4** & 80.6 & **86.6** \\ \hline \end{tabular}
\end{table}
Table 12: Average precision P@k of cross-language information retrieval 

**Theorem 6**.: _Assume that the blocks \(\bm{c}\), \(\bm{p}^{(1)}\) and \(\bm{p}^{(2)}\) are statistically independent, i.e., \(p(\bm{c},\bm{p}^{(1)},\bm{p}^{(2)})=p(\bm{c})p(\bm{p}^{(1)})p(\bm{p}^{(2)})\). Then, if one of the following holds:_

* _Assumption_ 1 _and assumptions in Theorem_ 1 _are satisfied, and (_33_) is solved yielding solutions_ \(\widehat{\bm{Q}}_{\mathrm{C}}^{(q)}\) _and_ \(\widehat{\bm{Q}}_{\mathrm{P}}^{(q)}\)__
* _Assumption_ 2 _is satisfied and has same mixing matrix_ \(\bm{A}^{(q)}=\bm{A}\) _and (_33_) with_ \(\bm{Q}_{\mathrm{P}}^{(q)}=\bm{Q}_{\mathrm{P}}\) _and_ \(\bm{Q}_{\mathrm{C}}^{(q)}=\bm{Q}_{\mathrm{C}}\) _is solved yielding_ \(\widehat{\bm{Q}}_{\mathrm{C}}^{(q)}\) _and_ \(\widehat{\bm{Q}}_{\mathrm{P}}^{(q)}\) _as the solutions._
* _Assumption_ 1 _is satisfied and_ \(d_{\mathrm{C}}\) _paired samples_ \((\bm{x}_{\ell}^{(1)},\bm{x}_{\ell}^{(2)})\) _are available (weak supervision), and denote_ \(\widehat{\bm{Q}}_{\mathrm{C}}^{(q)}\) _and_ \(\widehat{\bm{Q}}_{\mathrm{P}}^{(q)}\) _as the solutions after solving (_33_)._

_Then, we have \(\widehat{\bm{Q}}_{\mathrm{C}}^{(q)}\bm{x}^{(q)}=\bm{\Theta}\bm{c}\) and \(\widehat{\bm{Q}}_{\mathrm{P}}^{(q)}\bm{x}^{(q)}=\bm{\Xi}^{(q)}\bm{p}^{(q)},\;\) for some invertible \(\bm{\Xi}^{(q)}\) for all \(q=1,2.\)_

Proof.: For each case in Theorem. 6 (i) - (iii), we can prove

\[\widehat{\bm{c}}^{(q)}=\widehat{\bm{Q}}_{\mathrm{C}}^{(q)}\bm{x}^{(q)}=\bm{ \Theta}\bm{c},\;q=1,2\] (35)

using Theorems 1-3. The proofs are referred to Appendix B-D.

Let us denote

\[\widehat{\bm{p}}^{(q)}=\widehat{\bm{Q}}_{\mathrm{P}}^{(q)}\bm{x}^ {(q)}=\widehat{\bm{Q}}_{\mathrm{P}}^{(q)}\bm{A}^{(q)}\begin{bmatrix}\bm{c}\\ \bm{p}^{(q)}\end{bmatrix}=\bm{H}^{(q)}\begin{bmatrix}\bm{c}\\ \bm{p}^{(q)}\end{bmatrix},\]

where \(\bm{H}^{(q)}=\widehat{\bm{Q}}_{\mathrm{P}}^{(q)}\bm{A}^{(q)}\in\mathbb{R}^{d _{\mathrm{P}}^{(q)}\times(d_{\mathrm{C}}+d_{\mathrm{P}}^{(q)})}\). Note that the constraint (33c) implies that the mutual information between \(\widehat{\bm{p}}^{(q)}\) and \(\widehat{\bm{c}}^{(q)}\) is zero, i.e.,

\[I(\widehat{\bm{p}}^{(q)};\widehat{\bm{c}}^{(q)})=0.\]

Note that \(\widehat{\bm{p}}^{(q)}\to\widehat{\bm{c}}^{(q)}\to\bm{\Theta}^{-1}\widehat{ \bm{c}}^{(q)}=\bm{c}\) is a Markov chain. This is because when conditioned on \(\widehat{\bm{c}}^{(q)}\), \(\bm{\Theta}^{-1}\widehat{\bm{c}}^{(q)}\) becomes constant, making it independent of \(\widehat{\bm{p}}^{(q)}.\) This allows us to use the data processing inequality [76, Theorem 2.8.1], which results in the following:

\[I(\widehat{\bm{p}}^{(q)};\widehat{\bm{c}}^{(q)})\geq I(\widehat{\bm{p}}^{(q)}; \bm{\Theta}^{-1}\widehat{\bm{c}}^{(q)})=I(\widehat{\bm{p}}^{(q)};\bm{c})).\]

Since mutual information is always non-negative, the above implies that \(I(\widehat{\bm{p}}^{(q)};\bm{c})=0\). This implies that \(\widehat{\bm{p}}^{(q)}=\bm{H}^{(q)}\begin{bmatrix}\bm{c}\\ \bm{p}^{(q)}\end{bmatrix}\) is independent of \(\bm{c}\). Hence, \(\bm{H}^{(q)}[1:d_{\mathrm{C}}]=0,\forall q\).

Therefore \(\widehat{\bm{p}}^{(q)}=\bm{H}^{(q)}[d_{\mathrm{C}}+1:d_{\mathrm{C}}+d_{ \mathrm{P}}^{(q)}]\bm{p}^{(q)}=\bm{\Xi}^{(q)}\bm{p}^{(q)},\forall q\), where \(\bm{\Xi}^{(q)}=\bm{H}^{(q)}[d_{\mathrm{C}}+1:d_{\Data Generation:We set \(d_{\mathrm{C}}=2\) and \(d_{\mathrm{P}}^{(q)}=1\) as in the previous synthetic experiments. We sample \(\bm{c}\sim\texttt{VonMises}(2.5,\ 2.0)\) The private components are sampled from \(p^{(1)}\sim\texttt{Beta}(1.0,\ 3.0)\) and \(p^{(2)}\sim\texttt{Gamma}(0.5,\ 3.0)\) distributions. Each element of mixing matrices are sampled from \(A_{ij}^{(q)}\sim\mathcal{N}(0,1),\ q=1,2\).

Result:Fig. 7 shows the result for proposed method for private component identification. The first column shows the data domain, the second column shows the true and extracted shared component, and the third and fourth columns shows the true and extracted private components. Especially, the last row of the third and fourth columns shows the plot of ground truth \(\bm{p}^{(q)}\) on \(x-\)axis and \(\widehat{\bm{p}}^{(q)}\) on the y-axis. The plot is approximately a straight line which indicates that the estimated private components \(\widehat{p}^{(q)}\) are scaled version (i.e., invertible linear transformations) of ground truth private components. This verifies our Theorem 6.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: See Section 3, 4 and 6. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Section 7. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: See Appendix B, C, D and H. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: See Appendix F and G. * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes]Justification: Yes the code is provided in the supplemental material. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See Appendix F and G. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We have provided the error bars for the experiment that is computationally less demanding. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean.

* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Appendix G.4. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have followed the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The contribution of this paper is on theoretical aspects of machine learning. We don't foresee any immediate societal impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Theoretical paper. So not applicable in our case. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: See Appendix G. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We donot release any new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Not applicable. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Not Applicable. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.