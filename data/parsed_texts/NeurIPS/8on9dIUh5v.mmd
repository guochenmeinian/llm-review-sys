# Provable Benefit of Cutout and CutMix

for Feature Learning

Junsoo Oh

KAIST AI

junsoo.oh@kaist.ac.kr &Chulhee Yun

KAIST AI

chulhee.yun@kaist.ac.kr

###### Abstract

Patch-level data augmentation techniques such as Cutout and CutMix have demonstrated significant efficacy in enhancing the performance of vision tasks. However, a comprehensive theoretical understanding of these methods remains elusive. In this paper, we study two-layer neural networks trained using three distinct methods: vanilla training without augmentation, Cutout training, and CutMix training. Our analysis focuses on a feature-noise data model, which consists of several label-dependent features of varying rarity and label-independent noises of differing strengths. Our theorems demonstrate that Cutout training can learn low-frequency features that vanilla training cannot, while CutMix training can learn even rarer features that Cutout cannot capture. From this, we establish that CutMix yields the highest test accuracy among the three. Our novel analysis reveals that CutMix training makes the network learn all features and noise vectors "evenly" regardless of the rarity and strength, which provides an interesting insight into understanding patch-level augmentation.

## 1 Introduction

Data augmentation is a crucial technique in deep learning, particularly in the image domain. It involves creating additional training examples by applying various transformations to the original data, thereby enhancing the generalization performance and robustness of deep learning models. Traditional data augmentation techniques typically focus on geometric transformations such as random rotations, horizontal and vertical flips, and cropping (Krizhevsky et al., 2012), or color-based adjustments such as color jittering (Simonyan and Zisserman, 2014).

In recent years, several new data augmentation techniques have appeared. Among them, patch-level data augmentation techniques like Cutout (DeVries and Taylor, 2017) and CutMix (Yun et al., 2019) have received considerable attention for their effectiveness in improving generalization. Cutout is a straightforward method where random rectangular regions of an image are removed during training. In comparison, CutMix adopts a more complex strategy by cutting and pasting sections from different images and using mixed labels, encouraging the model to learn from blended contexts. The success of Cutout and CutMix has triggered the development of numerous variants including Random Erasing (Zhong et al., 2020), GridMask (Chen et al., 2020), CutBlur (Yoo et al., 2020), Puzzle Mix (Kim et al., 2020), and Co-Mixup (Kim et al., 2021). However, despite the empirical success of these patch-level data augmentation techniques in various image-related tasks, a lack of comprehensive theoretical understanding persists: _why and how do they work?_

In this paper, we aim to address this gap by offering a theoretical analysis of two important patch-level data augmentation techniques: Cutout and CutMix. Our theoretical framework draws inspiration from a study by Shen et al. (2022), which explores a data model comprising multiple label-dependent feature vectors and label-independent noises of varying frequencies and intensities. The key idea of this work is that learning features with low frequency can be challenging due to strong noises (i.e., low signal-to-noise ratio). We focus on how Cutout and CutMix can aid in learning such rare features.

### Our Contributions

In this paper, we consider a patch-wise data model consisting of features and noises, and use two-layer convolutional neural networks as learner networks. We focus on three different training methods: vanilla training without any augmentation, Cutout training, and CutMix training. We refer to these training methods in our problem setting as ERM, Cutout, and CutMix. We investigate how these methods affect the network's ability to learn features. We summarize our contributions below:

* We analyze ERM, Cutout, and CutMix, revealing that Cutout outperforms ERM since it enables the learning of rarer features compared to ERM (Theorem 3.1 and Theorem 3.2). Furthermore, CutMix demonstrates almost perfect performance (Theorem 3.3) by learning all features.
* Our main intuition behind the negative result for ERM is that ERM learns to classify training samples by memorizing noise vectors instead of learning meaningful features if the features do not appear frequently enough. Hence, ERM suffers low test accuracy because it cannot learn rare features. However, Cutout alleviates this challenge by removing some of the strong noise patches, allowing it to learn rare features to some extent.
* We prove the near-perfect performance of CutMix based on a novel technique that views the non-convex loss as a composition of a convex function and reparameterization. This enables us to characterize the global minimum of the loss and show that CutMix forces the model to activate almost uniformly across every patch of inputs, allowing it to learn all features.

### Related Works

Feature Learning Theory.Our work aligns with a recent line of studies investigating how training methods and neural network architectures influence feature learning. These studies focus on a specific data distribution composed of two components: label-dependent features and label-independent noise. The key contribution of this body of work is the exploration of which training methods or neural networks are most effective at learning meaningful features and achieving good generalization performance. Allen-Zhu and Li (2020) demonstrate that an ensemble model can achieve near-perfect performance by learning diverse features, while a single model tends to learn only certain parts of the feature space, leading to lower test accuracy. In other works, Cao et al. (2022); Kou et al. (2023a) explore the phenomenon of benign overfitting when training a two-layer convolutional neural network. The authors identify the specific conditions under which benign overfitting occurs, providing valuable insights into how these networks behave during training. Several other studies seek to understand various aspects of deep learning through the lens of feature learning (Zou et al., 2021; Jelassi and Li, 2022; Chen et al., 2022, 2023; Li and Li, 2023; Huang et al., 2023a, 2023).

Theoretical Analysis of Data Augmentation.Several works aim to analyze traditional data augmentation from different perspectives, including kernel theory (Dao et al., 2019), margin-based approach (Rajput et al., 2019), regularization effects (Wu et al., 2020), group invariance (Chen et al., 2020b), and impact on optimization (Hanin and Sun, 2021). Moreover, many papers have explored various aspects of a recent technique called Mixup (Zhang et al., 2017). For example, studies have explored its regularization effects (Carratino et al., 2020; Zhang et al., 2020), its role in improving calibration (Zhang et al., 2022), its ability to find optimal decision boundaries (Oh and Yun, 2023) and its potential negative effects (Chidambaram et al., 2021; Chidambaram and Ge, 2024). Some works investigate the broader framework of Mixup, including CutMix, which aligns with the scope of our work. Park et al. (2022) study the regularization effect of mixed-sample data augmentation within a unified framework that contains both Mixup and CutMix. In Oh and Yun (2023), the authors analyze masking-based Mixup, which is a class of Mixup variants that also includes CutMix. In their context, they show that masking-based Mixup can deviate from the Bayes optimal classifier but require less training sample complexity. However, neither work provides a rigorous explanation for why CutMix has been successful. The studies most closely related to our work include Shen et al. (2022); Chidambaram et al. (2023); Zou et al. (2023). Shen et al. (2022) regard traditional data augmentation as a form of feature manipulation and investigate its advantages from a feature learning perspective. Both Chidambaram et al. (2023) and Zou et al. (2023) analyze Mixup within a feature learning framework. However, patch-level data augmentation such as Cutout and CutMix, which are the focus of our work, have not yet been explored within this context.

Problem Setting

In this section, we introduce the data distribution and neural network architecture, and formally describe the three training methods considered in this paper.

### Data Distribution

We consider a binary classification problem on structured data, consisting of patches of label-dependent vectors (referred to as _features_) and label-independent vectors (referred to as _noise_).

**Definition 2.1** (Feature Noise Patch Data).: We define a data distribution \(\mathcal{D}\) on \(\mathbb{R}^{d\times P}\times\{-1,1\}\) such that \((\bm{X},y)\sim\mathcal{D}\) where \(\bm{X}=\big{(}\bm{x}^{(1)},\dots,\bm{x}^{(P)}\big{)}\in\mathbb{R}^{d\times P}\) and \(y\in\{\pm 1\}\) is constructed as follows.

1. Choose the _label_\(y\in\{\pm 1\}\) uniformly at random.
2. Let \(\{\bm{v}_{s,k}\}_{s\in\{\pm 1\},k\in[K]}\subset\mathbb{R}^{d}\) be a set of orthonormal _feature vectors_. Choose the feature vector \(\bm{v}\in\mathbb{R}^{d}\) for data point \(\bm{X}\) as \(\bm{v}=\bm{v}_{y,k}\) with probability \(\rho_{k}\) from \(\{\bm{v}_{y,k}\}_{k\in[K]}\subset\mathbb{R}^{d}\), where \(\rho_{1}+\dots+\rho_{K}=1\) and \(\rho_{1}\geq\dots\geq\rho_{K}\). In our setting, there are three types of features with significantly different frequencies: _common features_, _rare features_, and _extremely rare features_, ordered from most to least frequent. The indices of these features partition \([K]\) into \((\mathcal{K}_{C},\mathcal{K}_{R},\mathcal{K}_{E})\).
3. We construct \(P\) patches of \(\bm{X}\) as follows. * **Feature Patch**: Choose \(p^{*}\) uniformly from \([P]\) and we set \(\bm{x}^{(p^{*})}=\bm{v}\). * **Dominant Noise Patch**: Choose \(\tilde{p}\) uniformly from \([P]\setminus\{p^{*}\}\). We construct \(\bm{x}^{(\tilde{p})}=\alpha\bm{u}+\xi^{(\tilde{p})}\) where \(\alpha\bm{u}\) is _feature noise_ drawn uniformly from \(\{\alpha\bm{v}_{1,1},\alpha\bm{v}_{-1,1}\}\) with \(0<\alpha<1\) and \(\xi^{(\tilde{p})}\) is Gaussian _dominant noise_ drawn from \(N(\bm{0},\sigma_{\mathrm{d}}^{2}\bm{\Lambda})\). * **Background Noise Patch**: The remaining patches \(p\in[P]\setminus\{p^{*},\tilde{p}\}\) consist of Gaussian _background noise_, i.e., we set \(\bm{x}^{(p)}=\xi^{(p)}\) where \(\xi^{(p)}\sim N(\bm{0},\sigma_{\mathrm{d}}^{2}\bm{\Lambda})\).

Here, the noise covariance matrix is defined as \(\bm{\Lambda}:=\bm{I}-\sum_{s,k}\bm{v}_{s,k}\bm{v}_{s,k}^{\top}\) which ensures that Gaussian noises are orthogonal to all features. We assume that the dominant noise is stronger than the background noise, i.e., \(\sigma_{\mathrm{b}}<\sigma_{\mathrm{d}}\).

Our data distribution captures characteristics of image data, where the input consists of several patches. Some patches contain information relevant to the image labels, such as cat faces for the label "cat," while other patches contain information irrelevant to the labels, such as the background. Intuitively, there are two ways to fit the given data: learning features or memorizing noise. If a model fits the data by learning features, it can correctly classify test data having the same features. However, if a model fits the data by memorizing noise, it cannot generalize to unseen data because noise patches are not relevant to labels. Thus, learning more features is crucial for achieving better generalization.

In real-world scenarios, different features may appear with varying frequencies. For instance, the occurrences of cat's faces and cat's tails in a dataset might differ significantly, although both are relevant to the "cat" label. Our data distribution reflects these characteristics by considering features with varying frequencies. To emphasize the distinctions between the three training methods we analyze, we categorize features into three groups: common, rare, and extremely rare. We refer to data points containing these features as _common data_, _rare data_, and _extremely rare data_, respectively. We emphasize that these terminologies are chosen merely to distinguish the three different levels of rarity, and even "extremely rare" features appear in a nontrivial fraction of the training data with high probability (see our assumptions in Section 2.4).

Comparison to Previous Work.Our data distribution is similar to those considered in Shen et al. (2022) and Zou et al. (2023), which investigate the benefits of standard data augmentation methods and Mixup by comparing them to vanilla training without any augmentation. These results consider two types of features--common and rare--with different levels of rarity, along with two types of noise: feature noise and Gaussian noise. However, we consider three types of features: common, rare, and extremely rare, and three types of noise: feature noise, dominant noise, and background noise. This distinction allows us to compare three distinct methods and demonstrate the differences between them, whereas Shen et al. (2022) and Zou et al. (2023) compared only two methods.

### Neural Network Architecture

For the prediction model, we focus on the following two-layer convolutional neural network where the weights in the second layer are fixed at \(1\) and \(-1\), with only the first layer being trainable. Several works including Shen et al. (2022) and Zou et al. (2023) also focus on similar two-layer convolutional neural networks.

**Definition 2.2** (2-Layer CNN).: We define 2-layer CNN \(f_{\bm{W}}:\mathbb{R}^{d\times P}\rightarrow\mathbb{R}\) parameterized by \(\bm{W}=\{\bm{w}_{1},\bm{w}_{-1}\}\in\mathbb{R}^{d\times 2}\). For each input \(\bm{X}=\left(\bm{x}^{(1)},\dots,\bm{x}^{(P)}\right)\in\mathbb{R}^{d\times P}\), we define

\[f_{\bm{W}}(\bm{X}):=\sum_{p\in[P]}\phi\left(\left\langle\bm{w}_{1},\bm{x}^{(p)} \right\rangle\right)-\sum_{p\in[P]}\phi\left(\left\langle\bm{w}_{-1},\bm{x}^{( p)}\right\rangle\right),\]

where \(\phi(\cdot)\) is a smoothed version of leaky ReLU activation, defined as follows.

\[\phi(z):=\begin{cases}z-\frac{(1-\beta)r}{2}&z\geq r\\ \frac{1-\beta}{2r}z^{2}+\beta z&0\leq z\leq r\\ \beta z&z\leq 0\end{cases},\]

where \(0<\beta\leq 1\) and \(r>0\).

Previous works on the theory of feature learning often consider neural networks with (smoothed) ReLU or polynomial activation functions. However, we adopt a smoothed leaky ReLU activation, which always has a positive slope, to exclude the possibility of neurons "dying" during the complex optimization trajectory. Using smoothed leaky ReLU to analyze the learning dynamics of neural networks is not entirely new; there is a body of work that studies phenomena such as benign overfitting (Frei et al., 2022) and implicit bias (Frei et al., 2022; Kou et al., 2023) by analyzing neural networks with (smoothed) leaky ReLU activation.

A key difference between ReLU and leaky ReLU lies in the possibility of ReLU neurons "dying" in the negative region, where some negatively initialized neurons remain unchanged throughout training. As a result, using ReLU activation requires multiple neurons to ensure the survival of neurons at initialization, which becomes increasingly probable as the number of neurons increases. In contrast, the derivative of leaky ReLU is always positive, ensuring that a single neuron is often sufficient. Therefore, for mathematical simplicity, we consider the case where the network has a single neuron for each positive and negative output. We believe that our analysis can be extended to the multi-neuron case as we validate numerically in Appendix A.2.

### Training Methods

Using a training set sampled from the distribution \(\mathcal{D}\), we would like to train our network \(f_{\bm{W}}\) to learn to correctly classify unseen data points from \(\mathcal{D}\). We consider three learning methods: vanilla training without any augmentation, Cutout, and CutMix. We first introduce necessary notation for our data and parameters, and then formalize training methods within our framework.

Training Data.We consider a training set \(\mathcal{Z}=\{(\bm{X}_{i},y_{i})\}_{i\in[n]}\) comprising \(n\) data points, each independently drawn from \(\mathcal{D}\). For each \(i\in[n]\), we denote \(\bm{X}_{i}=(\bm{x}_{i}^{(1)},\dots,\bm{x}_{i}^{(P)})\).

Initialization.We initialize the model parameters in our neural network using random initialization. Specifically, we initialize the model parameter \(\bm{W}^{(0)}=\{\bm{w}_{1}^{(0)},\bm{w}_{-1}^{(0)}\}\), where \(\bm{w}_{1}^{(0)},\bm{w}_{-1}^{(0)}\stackrel{{\text{i.i.d.}}}{{ \sim}}N(\bm{0},\sigma_{0}^{2}\bm{I}_{d})\). Let us denote updated model parameters at iteration \(t\) as \(\bm{W}^{(t)}=\{\bm{w}_{1}^{(t)},\bm{w}_{-1}^{(t)}\}\).

#### 2.3.1 Vanilla Training

The vanilla approach to training a model \(f_{\bm{W}}\) is solving the empirical risk minimization problem using gradient descent. We refer to this method as ERM. Then, ERM updates parameters \(\bm{W}^{(t)}\) of a model using the following rule.

\[\bm{W}^{(t+1)}=\bm{W}^{(t)}-\eta\nabla_{\bm{W}}\mathcal{L}_{\mathrm{ERM}} \left(\bm{W}^{(t)}\right),\]where \(\eta\) is a learning rate and \(\mathcal{L}_{\mathrm{ERM}}(\cdot)\) is the ERM training loss defined as

\[\mathcal{L}_{\mathrm{ERM}}(\bm{W}):=\frac{1}{n}\sum_{i\in[n]}\ell(y_{i}f_{\bm{W} }(\bm{X}_{i})),\] (1)

where \(\ell(\cdot)\) is the logistic loss \(\ell(z)=\log(1+e^{-z})\).

#### 2.3.2 Cutout Training.

Cutout (DeVries and Taylor, 2017) is a data augmentation technique that randomly cuts out rectangular regions of image inputs. In our patch-wise data, we regard Cutout training as using inputs with masked patches from the original data. For each subset \(\mathcal{C}\) of \([P]\) and \(i\in[n]\), we define augmented data \(\bm{X}_{i,\mathcal{C}}\in\mathbb{R}^{d\times P}\) as a data point generated by cutting the patches with indices in \(\mathcal{C}\) out of \(\bm{X}_{i}\). We can represent \(\bm{X}_{i,\mathcal{C}}\) as

\[\bm{X}_{i,\mathcal{C}}=\left(\bm{x}_{i,\mathcal{C}}^{(1)},\ldots,\bm{x}_{i, \mathcal{C}}^{(P)}\right),\text{ where }\bm{x}_{i,\mathcal{C}}^{(p)}=\begin{cases}\bm{x}_{i}^{(p)}& \text{if }p\notin\mathcal{C},\\ \bm{0}&\text{otherwise}.\end{cases}\]

Note that the output of the model \(f_{\bm{W}}(\cdot)\) on this augmented data point \(\bm{X}_{i,\mathcal{C}}\) is

\[f_{\bm{W}}(\bm{X}_{i,\mathcal{C}})=\sum_{p\notin\mathcal{C}}\phi\left(\left< \bm{w}_{1},\bm{x}_{i}^{(p)}\right>\right)-\sum_{p\notin\mathcal{C}}\phi\left( \left<\bm{w}_{-1},\bm{x}_{i}^{(p)}\right>\right).\]

Then, the objective function for Cutout training can be defined as

\[\mathcal{L}_{\mathrm{Cutout}}(\bm{W}):=\frac{1}{n}\sum_{i\in[n]}\mathbb{E}_{ \mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}[\ell(y_{i}f_{\bm{W}}(\bm{X}_{i, \mathcal{C}}))],\]

where \(\mathcal{D}_{\mathcal{C}}\) is a uniform distribution on the collection of subsets of \([P]\) with cardinality \(C\), where \(C\) is a hyperparameter satisfying \(1\leq C<\frac{P}{2}\).1 We refer to the process of training our model using gradient descent on Cutout loss \(\mathcal{L}_{\mathrm{Cutout}}(\bm{W})\) as Cutout, and its update rule is

Footnote 1: DeVries and Taylor (2017) also employ a moderate size of cutting, such as cutting \(16\times 16\) pixels on CIFAR-10 data, which originally has \(32\times 32\) pixels.

\[\bm{W}^{(t+1)}=\bm{W}^{(t)}-\eta\nabla_{\bm{W}}\mathcal{L}_{\mathrm{Cutout}} \left(\bm{W}^{(t)}\right),\] (2)

where \(\eta\) is a learning rate.

#### 2.3.3 CutMix Training.

CutMix (Yun et al., 2019) involves not only cutting parts of images, but also pasting them into different images as well as assigning them mixed labels. For each subset \(\mathcal{S}\) of \([P]\) and \(i,j\in[n]\), we define the augmented data point \(\bm{X}_{i,j,\mathcal{S}}\in\mathbb{R}^{d\times P}\) as the data obtained by cutting patches with indices in \(\mathcal{S}\) from data \(\bm{X}_{i}\) and pasting them into \(\bm{X}_{j}\) at the same indices \(\mathcal{S}\). We can write \(\bm{X}_{i,j,\mathcal{S}}\) as

\[\bm{X}_{i,j,\mathcal{S}}=\left(\bm{x}_{i,j,\mathcal{S}}^{(1)},\ldots,\bm{x}_{i,j,\mathcal{S}}^{(P)}\right),\text{ where }\bm{x}_{i,j,\mathcal{S}}^{(p)}= \begin{cases}\bm{x}_{i}^{(p)}&\text{if }p\in\mathcal{S},\\ \bm{x}_{j}^{(p)}&\text{otherwise}.\end{cases}\]

The one-hot encoding of the labels \(y_{i}\) and \(y_{j}\) are also mixed with proportions \(\frac{|\mathcal{S}|}{P}\) and \(1-\frac{|\mathcal{S}|}{P}\), respectively. This mixed label results in the loss of the form

\[\frac{|\mathcal{S}|}{P}\ell(y_{i}f_{\bm{W}}(\bm{X}_{i,j,\mathcal{S}}))+\left( 1-\frac{|\mathcal{S}|}{P}\right)\ell(y_{j}f_{\bm{W}}(\bm{X}_{i,j,\mathcal{S}})).\]

From this, the CutMix training loss \(\mathcal{L}_{\mathrm{CutMix}}(\bm{W})\) can be defined as

\[\mathcal{L}_{\mathrm{CutMix}}(\bm{W}):=\frac{1}{n^{2}}\sum_{i,j\in[n]} \mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S}}}\Bigg{[}\frac{|\mathcal{S }|}{P}\ell(y_{i}f_{\bm{W}}(\bm{X}_{i,j,\mathcal{S}}))+\left(1-\frac{|\mathcal{ S}|}{P}\right)\ell(y_{j}f_{\bm{W}}(\bm{X}_{i,j,\mathcal{S}}))\Bigg{]},\]

where \(\mathcal{D}_{\mathcal{S}}\) is a probability distribution on the set of subsets of \([P]\) which samples \(\mathcal{S}\sim\mathcal{D}_{\mathcal{S}}\) as follows.21. Choose the cardinality \(s\) of \(\mathcal{S}\) uniformly at random from \(\{0,1,\ldots,P\}\), and
2. Choose \(\mathcal{S}\) uniformly at random from the collection of subsets of \([P]\) with cardinality \(s\).

We refer to the process of training our network using gradient descent on CutMix loss \(\mathcal{L}_{\mathrm{CutMix}}(\bm{W})\) as \(\mathsf{CutMix}\), and its update rule is

\[\bm{W}^{(t+1)}=\bm{W}^{(t)}-\eta\nabla_{\bm{W}}\mathcal{L}_{\mathrm{CutMix}} \left(\bm{W}^{(t)}\right),\] (3)

where \(\eta\) is a learning rate.

### Assumptions on the Choice of Problem Parameters

To control the quantities that appear in the analysis of training dynamics, we make assumptions on several quantities in our problem setting. For simplicity, we use choices of problem parameters as a function of the dimension of patches \(d\) and consider sufficiently large \(d\).

We use the standard asymptotic notation \(\mathcal{O}(\cdot),\Omega(\cdot),\Theta(\cdot),o(\cdot),\omega(\cdot)\) to express the dependency on \(d\). We also use \(\widetilde{\mathcal{O}}(\cdot),\widetilde{\Omega}(\cdot),\widetilde{\Theta}(\cdot)\) to hide logarithmic factors of \(d\). Additionally, \(\mathrm{poly}(d)\) (or \(\mathrm{polylog}(d)\)) represents quantities that increase faster than \(d^{c_{1}}\)(or \((\log d)^{c_{1}}\)) and slower than \(d^{c_{2}}\) (or \((\log d)^{c_{2}}\)) for some constant \(0<c_{1}<c_{2}\). Similarly, \(o(1/\mathrm{poly}(d))\) (or \(o(1/\mathrm{polylog}(d))\)) denotes some quantities that decrease faster than \(1/d^{c}\) (or \(1/(\log d)^{c}\)) for any constant \(c\). Finally, we use \(f(d)=o(g(d)/\mathrm{polylog}(d))\) when \(f(d)/g(d)=o(1/\mathrm{polylog}(d))\) for some function \(f\) and \(g\) of \(d\).

Assumptions.We assume that \(P=\Theta(1)\) and \(P\geq 8\) for simplicity. Additionally, we consider a high-dimensional regime where the number of data points is much smaller than the dimension \(d\), which is expressed as \(n=o\left(\alpha\beta\sigma_{\mathrm{d}}^{-1}\sigma_{\mathrm{b}}d^{\frac{1}{2} }/\mathrm{polylog}(d)\right)\). We also assume that \(\rho_{k}n=\omega\left(n^{\frac{1}{2}}\log d\right)\) for all \(k\in[K]\), which ensures the sufficiency of data points with each feature.

In addition, as we will describe in Section 4, the relative scales between the frequencies of features and the strengths of noises play crucial roles in our analysis, as they serve as a proxy for the "learning speed" in the initial phase. For common features \(k\in\mathcal{K}_{C}\), we assume \(\rho_{k}=\Theta(1)\) and the learning speed of common features is much faster than that of dominant noise, which translates into the assumption \(\sigma_{\mathrm{d}}^{2}d=o(\beta n)\). For rare features \(k\in\mathcal{K}_{R}\), we assume \(\rho_{k}=\Theta(\rho_{R})\) for some \(\rho_{R}\), and we consider the case where the learning speed of rare features is much slower than that of dominant noise but faster than background noise, which is expressed as \(\rho_{R}n=o\left(\alpha^{2}\sigma_{\mathrm{d}}^{2}d/\mathrm{polylog}(d)\right)\) and \(\sigma_{\mathrm{b}}^{2}d=o(\beta\rho_{R}n)\). Finally, for extremely rare features \(k\in\mathcal{K}_{E}\), we say \(\rho_{k}=\Theta(\rho_{E})\) for some \(\rho_{E}\) and their learning is even slower than that of background noises, which can be expressed as \(\rho_{E}n=o\left(\alpha^{2}\sigma_{\mathrm{b}}^{2}d/\mathrm{polylog}(d)\right)\).

Lastly, we assume the strength of feature noise satisfies \(\alpha=o\left(n^{-1}\beta\sigma_{\mathrm{d}}^{2}d/\mathrm{polylog}(d)\right)\), and \(r,\sigma_{0},\eta>0\) are sufficiently small so that \(\sigma_{0},r=o\left(\alpha/\mathrm{polylog}(d)\right)\), \(\eta=o\left(r\sigma_{\mathrm{d}}^{-2}d^{-1}/\mathrm{polylog}(d)\right)\).

We list our assumptions in Assumption B.1 and there are many choices of parameters satisfying the set of assumptions, including:

\[P=8,C=2,n=\Theta\left(d^{0.4}\right),\alpha=\Theta\left(d^{-0.02 }\right),\beta=\frac{1}{\mathrm{polylog}(d)},\sigma_{0}=\Theta(d^{-0.2}),r= \Theta(d^{-0.2}),\] \[\sigma_{\mathrm{d}}=\Theta\left(d^{-0.305}\right),\sigma_{\mathrm{ b}}=\Theta\left(d^{-0.375}\right),\rho_{R}=\Theta\left(d^{-0.1}\right),\rho_{E}= \Theta\left(d^{-0.195}\right),\eta=\Theta(d^{-1}).\]

## 3 Main Results

In this section, we provide a characterization of the high probability guarantees for the behavior of models trained using three distinct methods we have introduced. We denote by \(T^{*}\) the maximum admissible training iterates and we assume \(T^{*}=\frac{\mathrm{poly}(d)}{\eta}\) with a sufficiently large polynomial in \(d\). In all of our theorem statements, the randomness is over the sampling of training data and the initialization of models and all results hold under the condition that \(d\) is sufficiently large.

The following theorem characterizes training accuracy and test accuracy achieved by \(\mathsf{ERM}\).

[MISSING_PAGE_EMPTY:7]

To provide the proof overview, let us introduce some additional notation. For each \(i\in[n]\), recall that the corresponding input point can be written as \(\bm{X}_{i}=(\bm{x}_{i}^{(1)},\ldots,\bm{x}_{i}^{(P)})\). We use \(p_{i}^{*}\) and \(\tilde{p}_{i}\) to denote the indices of its feature patch and dominant noise patch, respectively. For each feature vector \(\bm{v}_{s,k}\), where \(s\in\{\pm 1\}\) and \(k\in[K]\), let \(\mathcal{V}_{s,k}\subset[n]\) represent the set of indices of data points having the feature vector \(\bm{v}_{s,k}\), and \(\mathcal{V}_{s}=\bigcup_{k=1}^{K}\mathcal{V}_{s,k}\) denotes the set of indices of data with label \(s\). For each data point \(i\in[n]\) and dominant or background noise patch \(p\in[P]\setminus\{p_{i}^{*}\}\), we refer to the Gaussian noise inside \(\bm{x}_{i}^{(p)}\) as \(\xi_{i}^{(p)}\).

### Vanilla Training and Cutout Training

We now explain why ERM fails to learn (extremely) rare features, while Cutout can learn rare features but not extremely rare features. Let us consider ERM. From (1), for \(s,s^{\prime}\in\{\pm 1\},k\in[K],i\in[n]\) and \(p\in[P]\setminus\{p_{i}^{*}\}\), the component of \(\bm{w}_{s}\) in the feature vector \(\bm{v}_{s^{\prime},k}\)'s direction is updated as

\[\left\langle\bm{w}_{s}^{(t+1)},\bm{v}_{s^{\prime},k}\right\rangle=\left\langle \bm{w}_{s}^{(t)},\bm{v}_{s^{\prime},k}\right\rangle-\frac{ss^{\prime}\eta}{n} \sum_{j\in\mathcal{V}_{s^{\prime},k}}\ell^{\prime}(y_{j}f_{\bm{W}^{(t)}}(\bm{X }_{j}))\phi^{\prime}\left(\left\langle\bm{w}_{s}^{(t)},\bm{v}_{s^{\prime},k} \right\rangle\right),\] (4)

and similarly, the "update" of inner product of \(\bm{w}_{s}\) with a noise patch \(\xi_{i}^{(p)}\) can be written as

\[\left\langle\bm{w}_{s}^{(t+1)},\xi_{i}^{(p)}\right\rangle\approx\left\langle \bm{w}_{s}^{(t)},\xi_{i}^{(p)}\right\rangle-\frac{sy_{i}\eta}{n}\ell^{\prime} (y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i}))\phi^{\prime}\left(\left\langle\bm{w}_{s}^ {(t)},\xi_{i}^{(p)}\right\rangle\right)\left\|\xi_{i}^{(p)}\right\|^{2},\] (5)

where the approximation is due to the near-orthogonality of Gaussian random vectors in the high-dimensional regime. This approximation shows that \(\left\langle\bm{w}_{s}^{(t+1)},\bm{v}_{s^{\prime},k}\right\rangle\)'s and \(\left\langle\bm{w}_{s}^{(t)},\xi_{i}^{(p)}\right\rangle\)'s are almost monotonically increasing or decreasing. We address the approximation errors using a variant of the technique introduced by Cao et al. (2022), as detailed in Appendix B.3.

From (4) and (5), we can observe that in the early phase of training satisfying \(-\ell^{\prime}(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i}))=\Theta(1)\), the main factor for the speed of learning features and noises are the number of feature occurrence \(|\mathcal{V}_{s^{\prime},k}|\) and the strength of noise \(\|\xi_{i}^{(p)}\|^{2}\). From our assumptions introduced in Section 2.4, if we compare the learning speed of different components, we have

common features \(\gg\) dominant noises \(\gg\) rare features \(\gg\) background noises \(\gg\) extremely rare features,

in terms of "learning speed." Based on this observation, we conduct a three-phase analysis for ERM.

* **Phase 1**: Learning common features quickly.
* **Phase 2**: Fitting (extremely) rare data by memorizing dominant noises instead of learning features.
* **Phase 3**: A model cannot learn (extremely) rare features since gradients of all data are small.

The main intuition behind why ERM cannot learn (extremely) rare features is that the gradients of all data containing these features become small after quickly memorizing dominant noise patches. In contrast, since Cutout randomly cuts some patches out, there exist augmented data points that do not contain dominant noises and have only features and background noises. This allows Cutout to learn rare features, thanks to these augmented data. However, extremely rare features cannot be learned since the learning speed of background noise is much faster and there are too many background noise patches to cut them all out.

_Remark 4.1_.: Shen et al. (2022) conduct analysis on vanilla training and training using standard data augmentation, sharing the same intuition in similar but different data models and neural networks. Also, we emphasize that we proved the model cannot learn (extremely) rare features even if we run \(\frac{\mathrm{poly}(d)}{\eta}\) iterations of GD, whereas Shen et al. (2022) only consider the first iteration that achieves perfect training accuracy.

Practical Insights.In practice, images contain features and noise across several patches. A larger cutting size can be more effective in removing noise but may also remove important features that the model needs to learn. Thus, there is a trade-off in choosing the optimal cutting size, a trend also observed in DeVries and Taylor (2017). One limitation of Cutout is that it may not effectively remove dominant noise. Thus, dominant noise can persist in the augmented data, leading to potential noise memorization. We believe that developing strategies that can more precisely detect and remove these noise components from the image input could enhance the effectiveness of these methods.

### CutMix Training

In learning dynamics of ERM and Cutout, inner products between weight and data patches evolve (approximately) monotonically, which makes the analysis much more feasible. However, analyzing the learning dynamics of CutMix involves non-monotone change of inner products, which is inevitable since CutMix uses mixed labels; this is also demonstrated in our experimental results (Section 5,especially the leftmost plot in Figure 1). Non-monotonicity and non-convexity of the problem necessitates novel proof strategies.

Let us define \(\bm{Z}:=\{z_{s,k}\}_{s\in\{\pm 1\},k\in[K]}\cup\{z_{i}^{(p)}\}_{i\in[n],p\in[P] \setminus\{p_{i}^{*}\}}\) as a function of \(\bm{W}\) as follows,

\[z_{i}^{(p)}:=\phi\left(\left\langle\bm{w}_{1},\xi_{i}^{(p)}\right\rangle \right)-\phi\left(\left\langle\bm{w}_{-1},\xi_{i}^{(p)}\right\rangle\right), \quad z_{s,k}:=\phi(\left\langle\bm{w}_{1},\bm{v}_{s,k}\right\rangle)-\phi( \left\langle\bm{w}_{-1},\bm{v}_{s,k}\right\rangle).\]

Then, \(\bm{Z}\) represents the contribution of each noise patch and feature vector to the neural network output, and the nonconvex function \(\mathcal{L}_{\mathrm{CutMix}}(\bm{W})\) can be viewed as the composition of \(\bm{Z}(\bm{W})\) and a convex function \(h(\bm{Z})\). By using the convexity of \(h(\bm{Z})\), we can characterize the global minimum of \(\mathcal{L}_{\mathrm{CutMix}}(\bm{W})\). Surprisingly, we show that any global minimizer \(\bm{W}^{*}=\{\bm{w}_{1}^{*},\bm{w}_{-1}^{*}\}\) satisfies

\[\phi\left(\left\langle\bm{w}_{s}^{*},\bm{x}_{i}^{(p)}\right\rangle\right)- \phi\left(\left\langle\bm{w}_{-s}^{*},\bm{x}_{i}^{(p)}\right\rangle\right)=C _{s},\]

for all \(s\in\{\pm 1\},i\in\mathcal{V}_{s}\), and \(p\in[P]\), with some constants \(C_{1},C_{-1}=\Theta(1)\). In other words, at the global minimum, the output of model on each patch of the training data is uniform across the set of data with the same labels. We also prove that CutMix can achieve a point close to the global minimum within \(\frac{\mathrm{poly}(d)}{\eta}\) iterations. As a result, the model trained by CutMix can learn all features including extremely rare features. The complete proof of Theorem 3.3 appears in Appendix E.2.

_Remark 4.2_.: Zou et al. (2023) investigate Mixup in a similar feature-noise model and show that Mixup can learn rarer features than vanilla training, with its benefits emerging from the early dynamics of training. However, our characterization of the global minimum of \(\mathcal{L}_{\mathrm{CutMix}}(\bm{W})\) and experimental results in our setting (Section 5, Figure 1) suggest that the benefits of CutMix, especially for learning extremely rare features, arise from the later stages of training. This suggests that Mixup and CutMix have different underlying mechanisms for promoting feature learning.

Practical Insights.The main underlying mechanism of CutMix is that it learns information almost uniformly from all patches in the training data. However, this approach also involves memorizing noise, which can potentially degrade performance in real-world scenarios. We believe that a more sophisticated strategy such as considering the positional information of patches as used in Puzzle Mix (Kim et al., 2020) or Co-Mixup (Kim et al., 2021) could improve the ability to learn more from patches containing features and reduce the impact of noise.

## 5 Experiments

We conduct experiments both in our setting and real-world data CIFAR-10 to support our theoretical findings and intuition. We defer CIFAR-10 experiment results to Appendix A.1.

For the numerical experiments on our setting, we set the number of patches \(P=3\), dimension \(d=2000\), number of data points \(n=300\), dominant noise strength \(\sigma_{\mathrm{d}}=0.25\), background noise strength \(\sigma_{\mathrm{b}}=0.15\), and feature noise strength \(\alpha=0.005\). The feature vectors are given as the standard basis \(\bm{e}_{1},\bm{e}_{2},\bm{e}_{3},\bm{e}_{4},\bm{e}_{5},\bm{e}_{6}\in\mathbb{R} ^{d}\), where \(\bm{e}_{1},\bm{e}_{2},\bm{e}_{3}\) are features for the positive label \(y=1\) and \(\bm{e}_{4},\bm{e}_{5},\bm{e}_{6}\) are features for the negative label \(y=-1\). We categorize \(\bm{e}_{1}\) and \(\bm{e}_{4}\) as common features with a frequency of \(0.8\), \(\bm{e}_{2}\) and \(\bm{e}_{5}\) as rare features with a frequency of \(0.15\), and lastly, \(\bm{e}_{3}\) and \(\bm{e}_{6}\) as extremely rare features with a frequency of \(0.05\). For the learner network, we set the slope of negative regime \(\beta=0.1\) and the length of the smoothed interval \(r=1\). We train models using three methods: ERM, Cutout, and CutMix with a learning rate \(\eta=1\). For Cutout, we cut a single patch of data (\(C=1\)). We apply full-batch gradient descent for all methods; for Cutout and CutMix, we utilize all possible augmented data points.3 We note that this choice of problem parameters does not exactly match the technical assumptions in Section 2.4. However, we empirically observe the same conclusions, which suggests that our analysis could be extended beyond our assumptions.

Footnote 3: For CutMix, this may induce different choices of \(\mathcal{D}_{\mathcal{S}}\) from those assumed in our analysis, but we mention that other general choices of \(\mathcal{D}_{\mathcal{S}}\) do not alter the conclusions in our analysis.

For each feature vector \(\bm{v}\) of the positive label, we plot the output of the learned filters for the feature vector \(\phi(\langle\bm{w}_{1}^{(\ell)},\bm{v}\rangle)-\phi(\langle\bm{w}_{-1}^{(\ell)},\bm{v}\rangle)\) throughout training in Figure 1. Our numerical findings confirm that ERM can only learn common features, CutOut can learn common and rare features but cannot learn extremely rare features, and CutMix can learn all types of features. Especially, CutMix learn common features, rare features, and extremely rare features almost evenly. Also, we observed non-monotone behavior of the output in the case of CutMix, which motivated our novel proof technique. The same trends are observed with different architectures, such as a smoothed (leaky) ReLU network with multiple neurons, as detailed in Appendix A.2.

## 6 Conclusion

We studied how Cutout and CutMix influence the ability to learn features in a patch-wise feature-noise data model learning with two-layer convolutional neural networks by comparing them with vanilla training. We showed that Cutout enables the learning of rare features that cannot be learned through vanilla training by mitigating the problem of memorizing label-independent noises instead of learning label-dependent features. Surprisingly, we further proved that CutMix can learn extremely rare features that Cutout cannot learn. We also present our theoretical insights on the underlying mechanism of these methods and provide experimental support.

Limitation and Future Work.Our work has some limitations related to the neural network architecture, specifically, the use of a 2-layer two-neuron smoothed leaky ReLU network. Extending our results to neural networks with deeper, wider, and more general activation functions is a direction for future work. Another future direction is to develop patch-level data augmentation based on our theoretical findings. Also, it would be interesting to perform theoretical analysis on state-of-the-art patch-level data augmentation such as Puzzle Mix (Kim et al., 2020) or Co-Mixup (Kim et al., 2021). These methods utilize patch location information, thus it may require the development of a theoretical framework capturing more complex characteristics of image data.

## Acknowledgement

This work was supported by three Institute of Information & communications Technology Planning & Evaluation (IITP) grants (No. RS-2019-II190075, Artificial Intelligence Graduate School Program (KAIST); No. RS-2022-II220184, Development and Study of AI Technologies to Inexpensively Conform to Evolving Policy on Ethics; No. RS-2024-00457882, AI Research Hub Project) funded by the Korean government (MSIT), and a National Research Foundation of Korea (NRF) grant (No. RS-2019-NR040050) funded by the Korean government (MSIT). CY acknowledges support from a grant funded by Samsung Electronics Co., Ltd.

Figure 1: Numerical results on our problem setting. We validate our findings on the trends of ERM, CutOut, and CutMix in learning common feature (Left), rare feature (Center), and extremely rare feature (Right). The output of the common feature trained by CutMix shows non-monotone behavior.

## References

* Allen-Zhu and Li (2020) Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. _arXiv preprint arXiv:2012.09816_, 2020.
* Bubeck et al. (2015) Sebastien Bubeck et al. Convex optimization: Algorithms and complexity. _Foundations and Trends(r) in Machine Learning_, 8(3-4):231-357, 2015.
* Cao et al. (2022) Yuan Cao, Zixiang Chen, Misha Belkin, and Quanquan Gu. Benign overfitting in two-layer convolutional neural networks. _Advances in neural information processing systems_, 35:25237-25250, 2022.
* Carratino et al. (2020) Luigi Carratino, Moustapha Cisse, Rodolphe Jenatton, and Jean-Philippe Vert. On mixup regularization. _arXiv preprint arXiv:2006.06049_, 2020.
* Chen et al. (2020a) Pengguang Chen, Shu Liu, Hengshuang Zhao, and Jiaya Jia. Gridmask data augmentation. _arXiv preprint arXiv:2001.04086_, 2020a.
* Chen et al. (2020b) Shuxiao Chen, Edgar Dobriban, and Jane H Lee. A group-theoretic framework for data augmentation. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_, pages 21321-21333, 2020b.
* Chen et al. (2022) Zixiang Chen, Yihe Deng, Yue Wu, Quanquan Gu, and Yuanzhi Li. Towards understanding the mixture-of-experts layer in deep learning. _Advances in neural information processing systems_, 35:23049-23062, 2022.
* Chen et al. (2023) Zixiang Chen, Junkai Zhang, Yiwen Kou, Xiangning Chen, Cho-Jui Hsieh, and Quanquan Gu. Why does sharpness-aware minimization generalize better than sgd? In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* Chidambaram and Ge (2024) Muthu Chidambaram and Rong Ge. For better or for worse? learning minimum variance features with label augmentation. _arXiv preprint arXiv:2402.06855_, 2024.
* Chidambaram et al. (2021) Muthu Chidambaram, Xiang Wang, Yuzheng Hu, Chenwei Wu, and Rong Ge. Towards understanding the data dependency of mixup-style training. _arXiv preprint arXiv:2110.07647_, 2021.
* Chidambaram et al. (2023) Muthu Chidambaram, Xiang Wang, Chenwei Wu, and Rong Ge. Provably learning diverse features in multi-view data with midpoint mixup. In _International Conference on Machine Learning_, pages 5563-5599. PMLR, 2023.
* Dao et al. (2019) Tri Dao, Albert Gu, Alexander Ratner, Virginia Smith, Chris De Sa, and Christopher Re. A kernel theory of modern data augmentation. In _International conference on machine learning_, pages 1528-1537. PMLR, 2019.
* DeVries and Taylor (2017) Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. _arXiv preprint arXiv:1708.04552_, 2017.
* Frei et al. (2022a) Spencer Frei, Niladri S Chatterji, and Peter Bartlett. Benign overfitting without linearity: Neural network classifiers trained by gradient descent for noisy linear data. In _Conference on Learning Theory_, pages 2668-2703. PMLR, 2022a.
* Frei et al. (2022b) Spencer Frei, Gal Vardi, Peter L Bartlett, Nathan Srebro, and Wei Hu. Implicit bias in leaky relu networks trained on high-dimensional data. _arXiv preprint arXiv:2210.07082_, 2022b.
* Hanin and Sun (2021) Boris Hanin and Yi Sun. How data augmentation affects optimization for linear regression. _Advances in Neural Information Processing Systems_, 34:8095-8105, 2021.
* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* Huang et al. (2023a) Wei Huang, Yuan Cao, Haonan Wang, Xin Cao, and Taiji Suzuki. Graph neural networks provably benefit from structural information: A feature learning perspective. _arXiv preprint arXiv:2306.13926_, 2023a.
* Huang et al. (2023b)Wei Huang, Ye Shi, Zhongyi Cai, and Taiji Suzuki. Understanding convergence and generalization in federated learning through feature learning theory. In _The Twelfth International Conference on Learning Representations_, 2023b.
* Jelassi and Li (2022) Samy Jelassi and Yuanzhi Li. Towards understanding how momentum improves generalization in deep learning. In _International Conference on Machine Learning_, pages 9965-10040. PMLR, 2022.
* Jiang et al. (2021) Ziheng Jiang, Chiyuan Zhang, Kunal Talwar, and Michael C Mozer. Characterizing structural regularities of labeled data in overparameterized models. In _International Conference on Machine Learning_, pages 5034-5044. PMLR, 2021.
* Kim et al. (2020) Jang-Hyun Kim, Wonho Choo, and Hyun Oh Song. Puzzle mix: Exploiting saliency and local statistics for optimal mixup. In _International Conference on Machine Learning_, pages 5275-5285. PMLR, 2020.
* Kim et al. (2021) Jang-Hyun Kim, Wonho Choo, Hosan Jeong, and Hyun Oh Song. Co-mixup: Saliency guided joint mixup with supermodular diversity. _arXiv preprint arXiv:2102.03065_, 2021.
* Kou et al. (2023a) Yiwen Kou, Zixiang Chen, Yuanzhou Chen, and Quanquan Gu. Benign overfitting in two-layer relu convolutional neural networks. In _International Conference on Machine Learning_, pages 17615-17659. PMLR, 2023a.
* Kou et al. (2023b) Yiwen Kou, Zixiang Chen, and Quanquan Gu. Implicit bias of gradient descent for two-layer relu and leaky relu networks on nearly-orthogonal data. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023b.
* Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. _Advances in neural information processing systems_, 25, 2012.
* Li and Li (2023) Binghui Li and Yuanzhi Li. Why clean generalization and robust overfitting both happen in adversarial training. _arXiv preprint arXiv:2306.01271_, 2023.
* Oh and Yun (2023) Junsoo Oh and Chulhee Yun. Provable benefit of mixup for finding optimal decision boundaries. In _International Conference on Machine Learning_, pages 26403-26450. PMLR, 2023.
* Park et al. (2022) Chanwoo Park, Sangdoo Yun, and Sanghyuk Chun. A unified analysis of mixed sample data augmentation: A loss function perspective. _Advances in Neural Information Processing Systems_, 35:35504-35518, 2022.
* Rajput et al. (2019) Shashank Rajput, Zhili Feng, Zachary Charles, Po-Ling Loh, and Dimitris Papailiopoulos. Does data augmentation lead to positive margin? In _International Conference on Machine Learning_, pages 5321-5330. PMLR, 2019.
* Shen et al. (2022) Ruoqi Shen, Sebastien Bubeck, and Suriya Gunasekar. Data augmentation as feature manipulation. In _International conference on machine learning_, pages 19773-19808. PMLR, 2022.
* Simonyan and Zisserman (2014) Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. _arXiv preprint arXiv:1409.1556_, 2014.
* Vershynin (2018) Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* Wu et al. (2020) Sen Wu, Hongyang Zhang, Gregory Valiant, and Christopher Re. On the generalization effects of linear transformations in data augmentation. In _International Conference on Machine Learning_, pages 10410-10420. PMLR, 2020.
* Yoo et al. (2020) Jaejun Yoo, Namhyuk Ahn, and Kyung-Ah Sohn. Rethinking data augmentation for image super-resolution: A comprehensive analysis and a new strategy. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 8375-8384, 2020.
* Yun et al. (2019) Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 6023-6032, 2019.
* Yang et al. (2019)Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. _arXiv preprint arXiv:1710.09412_, 2017.
* Zhang et al. (2020) Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou. How does mixup help with robustness and generalization? _arXiv preprint arXiv:2010.04819_, 2020.
* Zhang et al. (2022) Linjun Zhang, Zhun Deng, Kenji Kawaguchi, and James Zou. When and how mixup improves calibration. In _International Conference on Machine Learning_, pages 26135-26160. PMLR, 2022.
* Zhong et al. (2020) Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 13001-13008, 2020.
* Zou et al. (2021) Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu. Understanding the generalization of adam in learning neural networks with proper regularization. _arXiv preprint arXiv:2108.11371_, 2021.
* Zou et al. (2023) Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu. The benefits of mixup for feature learning. In _International Conference on Machine Learning_, pages 43423-43479. PMLR, 2023.

###### Contents

* 1 Introduction
	* 1.1 Our Contributions
	* 1.2 Related Works
* 2 Problem Setting
	* 2.1 Data Distribution
	* 2.2 Neural Network Architecture
	* 2.3 Training Methods
	* 2.4 Assumptions on the Choice of Problem Parameters
* 3 Main Results
* 4 Overview of Analysis
	* 4.1 Vanilla Training and Cutout Training
	* 4.2 CutMix Training
* 5 Experiments
* 6 Conclusion
* A Additional Experimental Results
* A.1 Experiments on CIFAR-10 Dataset
* A.2 Additional Experimental Results on Our Data Distribution
* B Proof Preliminaries
* B.1 Properties of the Choice of Problem Parameters
* B.2 Quantities at the Beginning
* B.3 Feature Noise Decomposition
* C Proof for ERM
* C.1 Proof of Lemma B.3 for ERM
* C.2 Proof of Theorem 3.1
* D Proof for Cutout
* D.1 Proof of Lemma B.3 for Cutout
* D.2 Proof of Theorem 3.2
* E Proof for CutMix
* E.1 Proof of Lemma B.3 for CutMix
* E.2 Proof of Theorem 3.3
* F Technical Lemmas

Additional Experimental Results

For all experiments described in this section and in Section 5, we use NVIDIA RTX A6000 GPUs.

### Experiments on CIFAR-10 Dataset

#### a.1.1 Experimental Support for Our Intuition

We compare three methods, ERM training, Cutout training, and CutMix training on CIFAR-10 classification. For ERM training, we apply only random cropping and random horizontal flipping on train dataset. In comparison, for Cutout training and CutMix training, we additionally apply Cutout and CutMix, respectively, on training data. For Cutout training, we randomly cut \(16\times 16\) pixels of input images, and for CutMix training, we sample the mixing ratio from a beta distribution \(\mathrm{Beta}(0.5,0.5)\). We train ResNet-18 (He et al., 2016) for 200 epochs with a batch size of 128 using SGD with a learning rate \(0.1\), momentum \(0.9\), and weight decay \(5\times 10^{-4}\). Trained models using ERM, Cutout, and CutMix achieve test accuracy \(95.16\%\), \(96.05\%\), and \(96.29\%\), respectively.

We randomly generate augmented data using CutMix from pairs of cat images and dog images in CIFAR-10 with varying mixing ratios \(\lambda=1,0.8,0.6\) (Dog:Cat = \(\lambda:1-\lambda\)). We randomly make \(5,000\) (cat, dot)-pairs in CIFAR-10 training set and apply CutMix randomly \(10\) times. By repeating this procedure \(10\) times, we generate total \(5,000\times 10\times 10=500,000\) augmented samples for each mixing ratio \(\lambda\). We plot a histogram of dog prediction output subtracted by cat prediction output (before applying the softmax function), evaluated on \(500,000\) augmented data in Figure 2.

The leftmost plot represents the evaluation results for original dog images, as it uses a mixing ratio of \(\lambda=1\). We can observe that the output of the model trained using Cutout is skewed toward higher values compared to the output of the model trained using other methods. We believe this aligns with the theoretical intuition that Cutout learns more information from the original image using augmented data.

The remaining two plots show the output for randomly augmented data using CutMix. We observe that the models trained with CutMix exhibit a shorter tail, supporting our intuition from the CutMix analysis that the models learn uniformly across all patches.

#### a.1.2 Experimental Support for Our Findings

We train ResNet-18 using ERM training, Cutout training, and CutMix training following the same experimental details described in Appendix A.1.1, except using only 10% of the training set. This data-hungry setting is intended to highlight the benefits of Cutout and CutMix. We then evaluated the trained models on the remaining 90% of the CIFAR-10 training dataset. The reason for evaluating the remaining training dataset is to analyze the misclassified data using C-score (Jiang et al., 2021), which is publicly available only for the training dataset.

Figure 2: Histogram of dog prediction output subtracted by cat prediction output evaluated on data points augmented by CutMix data using cat data and dog data with varying mixing ratio \(\lambda\) (Dog : Cat \(=\lambda:1-\lambda\)) (Left) \(\lambda=1\), (Center) \(\lambda=0.8\), (Right) \(\lambda=0.6\)

C-score measures the structural regularity of data, with lower values indicating examples that are more difficult to classify correctly. In our framework, data with harder-to-learn features (corresponding to rarer features) would likely have lower C-scores. Since directly extracting and quantitatively evaluating features learned by the models is challenging, we use the C-score as a proxy to evaluate the misclassified data across models trained by ERM, Cutout, and CutMix.

Table 1 illustrates that Cutout tends to misclassify data with lower C-scores compared to ERM, indicating that Cutout learns more hard-to-learn features than vanilla training. Furthermore, the data misclassified by CutMix has even lower C-scores than those misclassified by Cutout, suggesting that CutMix is effective at learning features that are the most challenging to classify. This observation aligns with our theoretical findings, demonstrating that CutMix captures even more difficult features compared to both ERM and Cutout.

Since directly visualizing features learned by a model is challenging, we present data that were misclassified by the model trained with ERM but correctly classified by the model trained with Cutout instead. In Figure 3, we show 7 samples per class with the lowest C-scores, which are considered to have rare features. Similarly, we also visualize data misclassified by the model trained with Cutout but correctly classified by the model trained with CutMix to represent extremely rare data in Figure 4. This approach allows us to interpret some (extremely) rare features in CIFAR-10, such as frogs with unusual colors.

\begin{table}
\begin{tabular}{|l|c|c|c|c|} \hline Method & Mean & Q1 & Q2 & Q3 \\ \hline
**ERM** & 0.687 & 0.615 & 0.782 & 0.841 \\ \hline
**Cutout** & 0.679 & 0.599 & 0.775 & 0.837 \\ \hline
**CutMix** & 0.670 & 0.575 & 0.767 & 0.835 \\ \hline \end{tabular}
\end{table}
Table 1: Mean and quantiles of the C-score on misclassified data across models trained with ERM, Cutout, and CutMix. The results indicate that Cutout tends to misclassify data with lower C-scores compared to ERM, while CutMix exhibits even lower C-scores.

Figure 3: Examples of rare data in CIFAR-10

Figure 4: Examples of extreme data in CIFAR-10

[MISSING_PAGE_FAIL:19]

Proof Preliminaries

### Properties of the Choice of Problem Parameters

In our analysis, we consider the choice of problem parameters as a function of the dimension of patches \(d\) and consider sufficiently large \(d\). Let us summarize the assumptions on the parameters for the problem setting and assume they hold.

**Assumption B.1**.: The following conditions hold.

1. (The number of patches) \(P=\Theta(1)\) and \(P\geq 8\).
2. (Overparameterized regime): \(n=o\left(\alpha\beta\sigma_{\mathrm{d}}^{-1}\sigma_{\mathrm{b}}d^{\frac{1}{2}} /\mathrm{polylog}(d)\right)\).
3. (Sufficient feature data): For all \(k\in[K]\), \(\rho_{k}n=\omega\left(n^{\frac{1}{2}}\log d\right)\).
4. (Common feature vs dominant noise): For all \(k\in\mathcal{K}_{C},\rho_{k}=\Theta(1)\) and \(\sigma_{\mathrm{d}}^{2}d=o(\beta n)\).
5. (Rare feature vs noise): For all \(k\in\mathcal{K}_{R},\rho_{k}=\Theta(\rho_{R})\) with \(\rho_{R}n=o\left(\alpha^{2}\sigma_{\mathrm{d}}^{2}d/\mathrm{polylog}(d)\right)\) and \(\sigma_{\mathrm{b}}^{2}d=o(\beta\rho_{R}n)\).
6. (Extremely rare feature vs background noise) For all \(k\in\mathcal{K}_{E},\rho_{k}=\Theta(\rho_{E})\) with \(\rho_{E}n=o\left(\alpha^{2}\sigma_{\mathrm{b}}^{2}d/\mathrm{polylog}(d)\right)\).
7. (Strength of feature noise) \(\alpha=o\left(n^{-1}\beta\sigma_{\mathrm{d}}^{2}d/\mathrm{polylog}(d)\right)\).
8. \(\sigma_{0}\sigma_{\mathrm{d}}^{2}d,r=o\left(\alpha/\mathrm{polylog}(d)\right), \eta=o\left(r\sigma_{\mathrm{d}}^{-2}d^{-1}/\mathrm{polylog}(d)\right)\)

We now present some properties derived from Assumption B.1, which are frequently used throughout our proof.

From (A3), for all \(k\in[K]\), we have the following inequality:

\[n\geq\rho_{1}n\geq\rho_{k}^{2}n=\omega\left(\log^{2}d\right)\] (6)

From (A1) and (A2), and given that \(\beta<1,\sigma_{\mathrm{b}}<\sigma_{\mathrm{d}}\), we have

\[d>(\beta\sigma_{\mathrm{d}}^{-1}\sigma_{\mathrm{b}}d^{\frac{1}{2}})^{2}>n^{2}P >nP.\] (7)

From (A2), (A3), and (A6), and given that \(\alpha,\beta<1\), we have

\[\sigma_{\mathrm{d}}^{2}d>\sigma_{\mathrm{b}}^{2}d=\omega(\rho_{E}n)=\omega(1).\] (8)

From (A1), (A2) and the fact that \(0<\alpha<1\), we have

\[nP\beta^{-1}\sigma_{\mathrm{d}}\sigma_{\mathrm{b}}^{-1}d^{-\frac{1}{2}}=o \left(\frac{\alpha}{\mathrm{polylog}(d)}\right)=o\left(\frac{1}{\mathrm{polylog }(d)}\right)\] (9)

From (A7) and (A4), we have

\[\alpha\beta^{-1}<\alpha\beta^{-2}=o\left(\frac{n^{-1}\beta^{-1}\sigma_{ \mathrm{d}}^{2}d}{\mathrm{polylog}(d)}\right)=o\left(\frac{1}{\mathrm{polylog }(d)}\right)\] (10)

From (8) and (A8), \(\eta=o(1)\) and then we have

\[\eta\leq\frac{\log(\eta T^{*})}{2}.\] (11)

From (A2), (A3), (A4), and (A5) we have

\[\alpha^{-2}=o\left(\frac{\sigma_{\mathrm{d}}^{2}d}{\rho_{R}n}\right)=o\left( \rho_{R}^{-1}\right)=o\left(n^{\frac{1}{2}}\right)=o\left(d^{\frac{1}{4}} \right).\] (12)

### Quantities at the Beginning

We characterize some quantities at the beginning of training.

**Lemma B.2**.: _Let \(E_{\mathrm{init}}\) the event such that all the following holds:_

* \(\frac{25}{52}n\leq\left|\mathcal{V}_{1}\right|,\left|\mathcal{V}_{-1}\right| \leq\frac{27}{52}n\)__
* _For each_ \(s\in\{\pm 1\}\) _and_ \(k\in[K]\)_,_ \(\frac{\rho_{k}n}{4}\leq\left|\mathcal{V}_{s,k}\right|\leq\frac{3\rho_{k}n}{4}\)__
* \(\cup_{i\in\mathcal{V}_{1,1}}\{p_{i}^{*}\}=[P]\)__
* _For any_ \(s,s^{\prime}\in\{\pm 1\}\) _and_ \(k\in[K]\)_,_ \(\left|\left\langle\bm{w}_{s}^{(0)},\bm{v}_{s^{\prime},k}\right\rangle\right| \leq\sigma_{0}\log d\)_._
* _For any_ \(s\in\{\pm 1\}\) _and_ \(i\in[n]\)_,_ \(\left|\left\langle\bm{w}_{s}^{(0)},\xi_{i}^{(\tilde{p}_{i})}\right\rangle \right|\leq\sigma_{0}\sigma_{\mathrm{d}}d^{\frac{1}{2}}\log d\)_._
* _For any_ \(s\in\{\pm 1\},i\in[n]\) _and_ \(p\in[P]\setminus\{p_{i}^{*},\tilde{p}_{i}\}\)_,_ \(\left|\left\langle\bm{w}_{s}^{(0)},\xi_{i}^{(p)}\right\rangle\right|\leq \sigma_{0}\sigma_{\mathrm{b}}d^{\frac{1}{2}}\log d\)_._
* _For any_ \(i,j\in[n]\) _with_ \(i\neq j\)_,_ \(\frac{1}{2}\sigma_{\mathrm{d}}^{2}d\leq\left\|\xi_{i}^{(\tilde{p}_{i})}\right\| ^{2}\leq\frac{3}{2}\sigma_{\mathrm{d}}^{2}d\) _and_ \(\left|\left\langle\xi_{i}^{(\tilde{p}_{i})},\xi_{j}^{(\tilde{p}_{j})}\right\rangle \right|\leq\sigma_{\mathrm{d}}^{2}d^{\frac{1}{2}}\log d\)_._
* _For any_ \(i,j\in[n]\) _and_ \(p\in[P]\setminus\{p_{j}^{*},\tilde{p}_{j}\}\)_,_ \(\left|\left\langle\xi_{i}^{(\tilde{p}_{i})},\xi_{j}^{(p)}\right\rangle\right| \leq\sigma_{\mathrm{d}}\sigma_{\mathrm{b}}d^{\frac{1}{2}}\log d\)_._
* _For any_ \(i,j\in[n]\) _and_ \(p\in[P]\setminus\{p_{i}^{*},\tilde{p}_{i}\},q\in[P]\setminus\{p_{j}^{*}, \tilde{p}_{j}\}\) _with_ \((i,p)\neq(j,q)\)_,_ \(\frac{1}{2}\sigma_{\mathrm{b}}^{2}d\leq\left\|\xi_{i}^{(p)}\right\|^{2}\leq \frac{3}{2}\sigma_{\mathrm{b}}^{2}d\) _and_ \(\left|\left\langle\xi_{i}^{(p)},\xi_{j}^{(q)}\right\rangle\right|\leq\sigma_{ \mathrm{b}}^{2}d^{\frac{1}{2}}\log d\)_._
* \(\{\bm{v}_{s,k}\}_{s\in\{\pm 1\},k\in[K]}\cup\{\bm{x}_{i}^{(p)}\}_{i\in[n],p\in[P] \setminus\{p_{i}^{*}\}}\) _is linearly independent._

_Then, the event \(E_{\mathrm{init}}\) occurs with probability at least \(1-o\left(\frac{1}{\mathrm{poly}(d)}\right)\). Also, if \(\xi\ \sim N(\bm{0},\sigma^{2}\Lambda)\) is independent of \(\bm{w}_{1}^{(0)},\bm{w}_{-1}^{(0)}\) and \(\{(\bm{X}_{i},y_{i})\}_{i\in[n]}\), we have_

\[\left|\left\langle\bm{w}_{1}^{(0)},\xi\right\rangle\right|,\left|\left\langle \bm{w}_{-1}^{(0)},\xi\right\rangle\right|\leq\sigma_{0}\sigma d^{\frac{1}{2}} \log d,\text{ and }\left|\left\langle\xi,\xi_{i}^{(p)}\right\rangle\right|\leq \sigma\sigma_{\mathrm{d}}d^{\frac{1}{2}}\log d,\]

_for all \(i\in[n]\) and \(p\in[P]\setminus\{p_{i}^{*}\}\), with probability at least \(1-o\left(\frac{1}{\mathrm{poly}(d)}\right)\)._

Proof of Lemma B.2.: Let us prove the first three points hold with probability at least \(1-o\left(\frac{1}{\mathrm{poly}(d)}\right)\). By Hoeffding's inequality,

\[\mathbb{P}\left[\left||\mathcal{V}_{1}|-\frac{n}{2}\right|>\frac{ n}{52}\right] =\mathbb{P}\left[\left|\sum_{i\in[n]}\left(\mathbbm{1}_{y_{i}=1}- \mathbb{E}[\mathbbm{1}_{y_{i}=1}]\right)\right|>\frac{n}{52}\right]\] \[\leq 2\exp\left(-\frac{2}{52^{2}}n\right)=o\left(\frac{1}{ \mathrm{poly}(d)}\right),\]

where the last equality is due to (6). In addition, for each \(s\in\{\pm 1\},k\in[K]\), by Hoeffding's inequality

\[\mathbb{P}\left[\left||\mathcal{V}_{s,k}|-\frac{\rho_{k}}{2}n \right|>\frac{\rho_{k}}{4}n\right] =\mathbb{P}\left[\left|\sum_{i\in[n]}\left(\mathbbm{1}_{i\in \mathcal{V}_{s,k}}-\mathbb{E}[\mathbbm{1}_{i\in\mathcal{V}_{s,k}}]\right)\right| >\frac{\rho_{k}}{4}n\right]\] \[\leq 2\exp\left(-\frac{\rho_{k}^{2}}{8}n\right)=o\left(\frac{1}{ \mathrm{poly}(d)}\right),\]

where the last equality is due to (6). Also, for each \(i\in[n]\) and \(p\in[P]\),

\[\mathbb{P}[\left\{i\in\mathcal{V}_{1,1}\right\}\cap\{p_{i}^{*}=p\}]=\frac{\rho_ {1}}{P}.\]Hence,

\[\mathbb{P}\left[\cup_{i\in\mathcal{V}_{1,1}}\{p_{i}^{*}\}\neq[P]\right] \leq\sum_{p\in[P]}\mathbb{P}\left[\cap_{i\in[n]}\left(\left(\{i\in \mathcal{V}_{1,1}\}\cap\{p_{i}^{*}=p\}\right)^{\complement}\right)\right]\] \[=P\left(1-\frac{\rho_{1}}{P}\right)^{n}\leq P\exp\left(-\frac{ \rho_{1}}{P}n\right)\] \[=o\left(\frac{1}{\mathrm{poly}(d)}\right).\]

Next, we will prove the remaining. Let us refer to the standard deviation of the Gaussian noise vector in \(p\)-th patch of \(i\)-th data as \(\sigma_{i,p}\). In other words, for each \(i\in[n]\) and \(p\in[P]\setminus\{p_{i}^{*}\}\),

\[\sigma_{i,p}=\begin{cases}\sigma_{\mathrm{d}}&\text{if }p=\tilde{p}_{i},\\ \sigma_{\mathrm{b}}&\text{otherwise}.\end{cases}\]

For each \(s,s^{\prime}\in\{\pm 1\}\) and \(k\in[K]\), \(\left\langle\bm{w}_{s}^{(0)},\bm{v}_{s^{\prime},k}\right\rangle\sim N(0, \sigma_{0})\). Hence, by Hoeffding's inequality, we have

\[\mathbb{P}\left[\left|\left\langle\bm{w}_{s}^{(0)},\bm{v}_{s^{\prime},k} \right\rangle\right|>\sigma_{0}\log d\right]\leq 2\exp\left(-\frac{\left(\sigma_{0} \log d\right)^{2}}{2\sigma_{0}^{2}}\right)=o\left(\frac{1}{\mathrm{poly}(d)} \right).\]

Let \(\{\bm{u}_{l}\}_{l\in[d-2K]}\) be an orthonormal basis of the orthogonal complement of \(\mathrm{Span}(\{\bm{v}_{s,k}\}_{s\in\{\pm 1\},k\in[K]})\). Note that for each \(s\in\{\pm 1\},i\in[n]\) and \(p\in[P]\setminus\{p_{i}^{*}\}\), we can write \(\xi_{i}^{(p)}\) and \(\xi\) as

\[\bm{w}_{s}(0)=\sigma_{0}\sum_{l\in[d-2K]}\mathbf{z}_{s,l}\bm{u}_{l},\quad\xi_{ i}^{(p)}=\sigma_{i,p}\sum_{l\in[d-2K]}\mathbf{z}_{i,l}^{(p)}\bm{u}_{l},\quad\xi= \sigma\sum_{l\in[d-2K]}\mathbf{z}_{l}\bm{u}_{l}\]

where \(\mathbf{z}_{s,l},\mathbf{z}_{i,l}^{(p)},\mathbf{z}_{l}\stackrel{{ i.d.}}{{\sim}}N(0,1)\). The sub-gaussian norm of standard normal distribution \(N(0,1)\) is \(\sqrt{\frac{8}{3}}\). Then \(\left(\mathbf{z}_{i,l}^{(p)}\right)^{2}-1\)'s are mean zero sub-exponential with sub-exponential norm \(\frac{8}{3}\) (Lemma 2.7.6 in Vershynin (2018)). In addition, \(\mathbf{z}_{s,l}\mathbf{z}_{i,l}^{(p)}\)'s, \(\mathbf{z}_{i,l}^{(p)}\mathbf{z}_{j,l}^{(q)}\)'s and \(\mathbf{z}_{i,l}^{(p)}\mathbf{z}_{l}\)'s are mean zero sub-exponential with sub-exponential norm less than or equal to \(\frac{8}{3}\) (Lemma 2.7.7 in Vershynin (2018)). We use Bernstein's inequality (Theorem 2.8.1 in Vershynin (2018)), with \(c\) being the absolute constant stated therein. We then have the following:

\[1-\mathbb{P}\left[\frac{1}{2}\sigma_{i,p}^{2}d\leq\left\|\xi_{i }^{(p)}\right\|^{2}\leq\frac{3}{2}\sigma_{i,p}^{2}d\right] \leq\mathbb{P}\left[\left|\left\|\xi_{i}^{(p)}\right\|^{2}- \sigma_{i,p}^{2}(d-2K)\right|\geq\sigma_{i,p}^{2}d^{\frac{1}{2}}\log d\right]\] \[=\mathbb{P}\left[\left|\sum_{l\in[d-2K]}\left(\left(\mathbf{z}_{i,l}^{(p)}\right)^{2}-1\right)\right|\geq d^{\frac{1}{2}}\log d\right]\] \[\leq 2\exp\left(-\frac{9cd\log^{2}d}{64(d-2K)}\right)\] \[\leq 2\exp\left(-\frac{9c\log^{2}d}{64}\right)=o\left(\frac{1}{ \mathrm{poly}(d)}\right),\]

in addition,

\[\mathbb{P}\left[\left|\left\langle\xi_{i}^{(p)},\xi_{j}^{(q)} \right\rangle\right|\geq\sigma_{i,p}\sigma_{j,q}d^{\frac{1}{2}}\log d\right] =\mathbb{P}\left[\left|\sum_{l\in[d-2K]}\mathbf{z}_{i,l}^{(p)} \mathbf{z}_{j,l}^{(q)}\right|\geq d^{\frac{1}{2}}\log d\right]\] \[\leq 2\exp\left(-\frac{9cd\log^{2}d}{64(d-2K)}\right)\] \[\leq 2\exp\left(-\frac{9c\log^{2}d}{64}\right)=o\left(\frac{1}{ \mathrm{poly}(d)}\right).\]Similarly, we have

\[\mathbb{P}\left[\left|\left\langle\bm{w}_{s}^{(0)},\xi_{i}^{(p)}\right\rangle \right|\geq\sigma_{0}\sigma_{i,p}d^{\frac{1}{2}}\log d\right]\leq 2\exp\left(- \frac{9c\log^{2}d}{64}\right)=o\left(\frac{1}{\operatorname{poly}(d)}\right).\]

Lastly, the last result holds almost surely due to (7). Applying the union bound to all events, each of which is at most \(\operatorname{poly}(d)\) due to (7), leads us to our first conclusion.

In addition, for each \(s\in\{\pm 1\},i\in[n]\) and \(p\in[P]\setminus\{p_{i}^{*}\}\),

\[\mathbb{P}\left[\left|\left\langle\bm{w}_{s}^{(0)},\xi\right\rangle\right| \geq\sigma_{0}\sigma d^{\frac{1}{2}}\log d\right]\leq 2\exp\left(-\frac{9c\log^{2}d}{64} \right)=o\left(\frac{1}{\operatorname{poly}(d)}\right),\]

and

\[\mathbb{P}\left[\left|\left\langle\xi_{i}^{(p)},\xi\right\rangle\right|\geq \sigma_{i,p}\sigma d^{\frac{1}{2}}\log d\right]\leq 2\exp\left(-\frac{9c\log^{2}d}{64} \right)=o\left(\frac{1}{\operatorname{poly}(d)}\right).\]

Applying the union bound to all events, each of which is at most \(\operatorname{poly}(d)\) due to (7), leads us to our second conclusion. 

### Feature Noise Decomposition

In our analysis, we use a technique that analyzes the coefficients of linear combinations of feature and noise vectors. A similar technique in a different data and network setting is introduced by Cao et al. (2022).

**Lemma B.3**.: _If we run one of ERM, Cutout, and CutMix training to update parameters \(\bm{W}^{(t)}\) of a model \(f_{\bm{W}^{(t)}}\), then there exist coefficients (corresponding to each method) \(\gamma_{s}^{(t)}(s^{\prime},k)\)'s and \(\rho_{s}^{(t)}(i,p)\)'s so that we can write \(\bm{W}^{(t)}=\{\bm{w}_{1}^{(t)},\bm{w}_{-1}^{(t)}\}\) as_

\[\bm{w}_{s}^{(t)} =\bm{w}_{s}^{(0)}+\sum_{k\in[K]}\gamma_{s}^{(t)}(s,k)\bm{v}_{s,k}- \sum_{k\in[K]}\gamma_{s}^{(t)}(-s,k)\bm{v}_{-s,k}\] \[\quad+\sum_{i\in Y_{s},p\in[P]\setminus\{p_{i}^{*}\}}\rho_{s}^{(t )}(i,p)\frac{\xi_{i}^{(p)}}{\left\|\xi_{i}^{(p)}\right\|^{2}}-\sum_{i\in\mathcal{ V}_{-s},p\in[P]\setminus\{p_{i}^{*}\}}\rho_{s}^{(t)}(i,p)\frac{\xi_{i}^{(p)}}{ \left\|\xi_{i}^{(p)}\right\|^{2}}\] \[\quad+\alpha\left(\sum_{i\in\mathcal{F}_{s}}sy_{i}\rho_{s}^{(t)}( i,\tilde{p}_{i})\frac{\bm{v}_{s,1}}{\left\|\xi_{i}^{(\tilde{p}_{i})}\right\|^{2}}+ \sum_{i\in\mathcal{F}_{-s}}sy_{i}\rho_{s}^{(t)}(i,\tilde{p}_{i})\frac{\bm{v}_ {-s,1}}{\left\|\xi_{i}^{(\tilde{p}_{i})}\right\|^{2}}\right)\]

_where \(\mathcal{F}_{s}\) denotes the set of indices of data with feature noise \(\bm{v}_{s,1}\). Furthermore, if we run one of ERM and Cutout, the coefficients \(\gamma_{s}^{(t)}(s^{\prime},k)\)'s and \(\rho_{s}^{(t)}(i,p)\)'s are monotone increasing._

We provide proof of Lemma B.3 for ERM in Appendix C.1, for Cutout in Appendix D.1 and for CutMix in Appendix E.1.

Since Gaussian vectors in a high-dimensional regime are nearly orthogonal, we can use the coefficients to approximate the inner products or outputs of neurons. The following lemma quantifies the approximation error.

**Lemma B.4**.: _Suppose the event \(E_{\mathrm{init}}\) occurs and \(0\leq\gamma_{s}^{(t)}(s^{\prime},k),\rho_{s}^{(t)}(i,p)\leq\widetilde{\mathcal{ O}}(\beta^{-1})\) for all \(s,s^{\prime}\in\{\pm 1\},k\in[K],i\in[n]\) and \(p\in[P]\setminus\{p_{i}^{*}\}\) at iteration \(t\). Then, for each \(s\in\{\pm 1\},k\in[K],i\in[n],\) and \(p\in[P]\setminus\{p_{i}^{*}\}\), the following holds:_

* \(\left|\left\langle\bm{w}_{s}^{(t)},\bm{v}_{s,k}\right\rangle-\gamma_{s}^{(t)}( s,k)\right|,\left|\phi\left(\left\langle\bm{w}_{s}^{(t)},\bm{v}_{s,k}\right\rangle \right)-\gamma_{s}^{(t)}(s,k)\right|=o\left(\frac{1}{\operatorname{polylog}(d)}\right)\)__
* \(\left|\left\langle\bm{w}_{s}^{(t)},\bm{v}_{-s,k}\right\rangle+\gamma_{s}^{(t)} (-s,k)\right|,\left|\phi\left(\left\langle\bm{w}_{s}^{(t)},\bm{v}_{-s,k}\right \rangle\right)+\beta\gamma_{s}^{(t)}(-s,k)\right|=o\left(\frac{1}{\operatorname {polylog}(d)}\right)\)__
* \(\left|\left\langle\bm{w}_{y_{i}}^{(t)},\xi_{i}^{(p)}\right\rangle-\rho_{y_{i} }^{(t)}(i,p)\right|,\left|\phi\left(\left\langle\bm{w}_{y_{i}}^{(t)},\xi_{i}^{ (p)}\right\rangle\right)-\rho_{y_{i}}^{(t)}(i,p)\right|=o\left(\frac{1}{ \operatorname{polylog}(d)}\right)\)__
* \(\left|\left\langle\bm{w}_{-y_{i}}^{(t)},\xi_{i}^{(p)}\right\rangle+\rho_{-y_{i} }^{(t)}(i,p)\right|,\left|\phi\left(\left\langle\bm{w}_{-y_{i}}^{(t)},\xi_{i}^{ (p)}\right\rangle\right)+\beta\rho_{-y_{i}}^{(t)}(i,p)\right|=o\left(\frac{1}{ \operatorname{polylog}(d)}\right)\)__* \(\left|\phi\left(\left\langle\bm{w}_{y_{i}}^{(t)},\bm{x}_{i}^{(\tilde{p}_{i})}\right\rangle \right\rangle-\rho_{y_{i}}^{(t)}(i,\tilde{p}_{i})\right|,\left|\phi\left( \left\langle\bm{w}_{-y_{i}}^{(t)},\bm{x}_{i}^{(\tilde{p}_{i})}\right\rangle \right)+\beta\rho_{-y_{i}}^{(t)}(i,\tilde{p}_{i})\right|=o\left(\frac{1}{\mathrm{ polylog}(d)}\right)\)

Proof of Lemma B.4.: For each \(s\in\{\pm 1\},k\in[K]\setminus\{1\}\), by (A8) and (8), we have

\[\left|\left\langle\bm{w}_{s}^{(t)},\bm{v}_{s,k}\right\rangle-\gamma_{s}^{(t)}( s,k)\right|=\left|\left\langle\bm{w}_{s}^{(0)},\bm{v}_{s,k}\right\rangle \right|=\widetilde{\mathcal{O}}(\sigma_{0})=o\left(\frac{1}{\mathrm{polylog}(d) }\right).\]

Similarly, by (A8) and (8),

\[\left|\left\langle\bm{w}_{s}^{(t)},\bm{v}_{-s,k}\right\rangle+\gamma_{s}^{(t) }(-s,k)\right|=\left|\left\langle\bm{w}_{s}^{(0)},\bm{v}_{-s,k}\right\rangle \right|=\widetilde{\mathcal{O}}(\sigma_{0})=o\left(\frac{1}{\mathrm{polylog}(d) }\right),\]

Next, we will consider the case of \(\bm{v}_{1,1}\) and \(\bm{v}_{-1,1}\). For each \(s\in\{\pm 1\}\), we have

\[\left|\left\langle\bm{w}_{s}^{(t)},\bm{v}_{s,1}\right\rangle- \gamma_{s}^{(t)}(s,1)\right|\] \[\leq\left|\left\langle\bm{w}_{s}^{(0)},\bm{v}_{s,1}\right\rangle \right|+\alpha\sum_{i\in[n]}\rho_{s}^{(t)}(i,\tilde{p}_{i})\left\|\xi_{i}^{( \tilde{p}_{i})}\right\|^{-2}\] \[\leq\widetilde{\mathcal{O}}(\sigma_{0})+\widetilde{\mathcal{O}} \left(\alpha n\beta^{-1}\sigma_{\mathrm{d}}^{-2}d^{-1}\right)\] \[=o\left(\frac{1}{\mathrm{polylog}(d)}\right),\]

where the last equality is due to (8) and (A7). Similarly, we have

\[\left|\left\langle\bm{w}_{s}^{(t)},\bm{v}_{-s,1}\right\rangle+ \gamma_{s}^{(t)}(-s,1)\right|\] \[\leq\left|\left\langle\bm{w}_{s}^{(0)},\bm{v}_{-s,1}\right\rangle \right|+\alpha\sum_{i\in[n]}\rho_{s}^{(t)}(i,\tilde{p}_{i})\left\|\xi_{i}^{( \tilde{p}_{i})}\right\|^{-2}\] \[\leq\widetilde{\mathcal{O}}(\sigma_{0})+\widetilde{\mathcal{O}} \left(\alpha n\beta^{-1}\sigma_{\mathrm{d}}^{-2}d^{-1}\right)\] \[=o\left(\frac{1}{\mathrm{polylog}(d)}\right).\]

Hence, from (A8) and the fact that \(|\phi(z)-z|\leq\frac{(1-\beta)r}{2}\) for any \(z\geq 0\), we have

\[\left|\phi\left(\left\langle\bm{w}_{s}^{(t)},\bm{v}_{s,k}\right\rangle \right)-\gamma_{s}^{(t)}(s,k)\right|\] \[\leq\left|\phi\left(\left\langle\bm{w}_{s}^{(t)},\bm{v}_{s,k} \right\rangle\right)-\phi\left(\gamma_{s}^{(t)}(s,k)\right)\right|+\left|\phi \left(\gamma_{s}^{(t)}(s,k)\right)-\gamma_{s}^{(t)}(s,k)\right|\] \[\leq\left|\left\langle\bm{w}_{s}^{(t)},\bm{v}_{s,k}\right\rangle- \gamma_{s}^{(t)}(s,k)\right|+\frac{(1-\beta)r}{2}\] \[=o\left(\frac{1}{\mathrm{polylog}(d)}\right).\]

and

\[\left|\phi\left(\left\langle\bm{w}_{s}^{(t)},\bm{v}_{-s,k}\right \rangle\right)+\beta\gamma_{s}^{(t)}(-s,k)\right| =\left|\phi\left(\left\langle\bm{w}_{s}^{(t)},\bm{v}_{-s,k}\right \rangle\right)-\phi\left(-\gamma_{s}^{(t)}(-s,k)\right)\right|\] \[\leq\left|\left\langle\bm{w}_{s}^{(t)},\bm{v}_{-s,k}\right\rangle +\gamma_{s}^{(t)}(-s,k)\right|\] \[=o\left(\frac{1}{\mathrm{polylog}(d)}\right).\]

For each \(i\in[n],\) and \(p\in[P]\setminus\{p_{i}^{*}\}\), we have

\[\left|\left\langle\bm{w}_{y_{i}}^{(t)},\xi_{i}^{(p)}\right\rangle-\rho_{y_{i}}^ {(t)}(i,p)\right|\leq\left|\left\langle\bm{w}_{y_{i}}^{(0)},\xi_{i}^{(p)} \right\rangle\right|+\sum_{\begin{subarray}{c}j\in[n],q\in[P]\setminus\{p_{i}^{ \prime}\}\\ (j,q)\neq(i,p)\end{subarray}}\rho_{y_{i}}^{(t)}(j,q)\frac{\left|\left\langle\xi_{i }^{(p)},\xi_{j}^{(q)}\right\rangle\right|}{\left\|\xi_{j}^{(q)}\right\|^{2}}\]\[\leq\left|\left\langle\boldsymbol{w}_{-y_{i}}^{(t)},\xi_{i}^{(p)} \right\rangle\right\rangle-\rho_{y_{i}}^{(t)}(i,p)\Big{|}+\left|\phi\left( \left\langle\boldsymbol{w}_{y_{i}}^{(t)},\boldsymbol{x}_{i}^{(\bar{p}_{i})} \right\rangle\right)-\phi\left(\left\langle\boldsymbol{w}_{y_{i}}^{(t)}, \xi_{i}^{(\bar{p}_{i})}\right\rangle\right)\right|\] \[=o\left(\frac{1}{\mathrm{polylog}(d)}\right),\]

where we apply the triangular inequality, the fact that \(\phi^{\prime}\leq 1\), the triangular inequality again, \(\rho_{y_{i}}^{(t)}(s,1)=\widetilde{\mathcal{O}}(\beta^{-1})\) and (10) sequentially.

Similarly,

\[\left|\left\langle\boldsymbol{w}_{-y_{i}}^{(t)},\xi_{i}^{(p)} \right\rangle+\rho_{-y_{i}}^{(t)}(i,p)\right| \leq\left|\left\langle\boldsymbol{w}_{-y_{i}}^{(0)},\xi_{i}^{(p) }\right\rangle\right|+\sum_{\begin{subarray}{c}j\in[n],q\in[P]\setminus\{p_{ i}^{*}\}\\ (j,q)\neq(i,p)\end{subarray}}\rho_{-y_{i}}^{(t)}(j,q)\frac{\left|\left\langle \xi_{i}^{(p)},\xi_{j}^{(q)}\right\rangle\right|}{\left\|\xi_{j}^{(q)}\right\| ^{2}}\] \[\leq\widetilde{\mathcal{O}}(\sigma_{0}\sigma_{d}d^{\frac{1}{2}}) +\widetilde{\mathcal{O}}\left(nP\beta^{-1}\sigma_{d}\sigma_{\mathrm{b}}^{-1} d^{-\frac{1}{2}}\right)\] \[=o\left(\frac{1}{\mathrm{polylog}(d)}\right),\]

and

\[\left|\phi\left(\left\langle\boldsymbol{w}_{-y_{i}}^{(t)},\xi_{i }^{(p)}\right\rangle\right)+\beta\rho_{-y_{i}}^{(t)}(i,p)\right| =\left|\phi\left(\left\langle\boldsymbol{w}_{-y_{i}}^{(t)},\xi_{ i}^{(p)}\right\rangle\right)-\phi\left(-\rho_{-y_{i}}^{(t)}(i,p)\right)\right|\] \[\leq\left|\left\langle\boldsymbol{w}_{-y_{i}}^{(t)},\xi_{i}^{(p) }\right\rangle+\rho_{-y_{i}}^{(t)}(i,p)\right|\] \[=o\left(\frac{1}{\mathrm{polylog}(d)}\right),\]

Also, if \(i\in\mathcal{F}_{s}\) for some \(s\in\{\pm 1\}\),

\[\left|\phi\left(\left\langle\boldsymbol{w}_{-y_{i}}^{(t)},\boldsymbol {x}_{i}^{(\bar{p}_{i})}\right\rangle\right)+\beta\rho_{-y_{i}}^{(t)}(i,\bar{ p}_{i})\right|\] \[=\left|\phi\left(\left\langle\boldsymbol{w}_{-y_{i}}^{(t)},\xi_{i }^{(\bar{p}_{i})}\right\rangle\right)+\beta\rho_{-y_{i}}^{(t)}(i,\bar{p}_{i}) \right|+\left|\phi\left(\left\langle\boldsymbol{w}_{-y_{i}}^{(t)},\boldsymbol {x}_{i}^{(\bar{p}_{i})}\right\rangle\right)-\phi\left(\left\langle \boldsymbol{w}_{-y_{i}}^{(t)},\xi_{i}^{(\bar{p}_{i})}\right\rangle\right)\right|\] \[=o\left(\frac{1}{\mathrm{polylog}(d)}\right).\]
\[Q^{(s^{*},k^{*})}(\bm{w})=s{s^{*}}\gamma_{s}(s^{*},k^{*})\bm{v}_{s^{*},k^{* }}+s{s^{*}}\sum_{i\in\mathcal{V}_{s^{*},k^{*}},p\in[P]\setminus\{p_{i}^{*}\}} \rho_{s}(i,p)\frac{\xi_{i}^{(p)}}{\left\|\xi_{i}^{(p)}\right\|^{2}}\] \[\qquad\qquad+\alpha\left(\sum_{i\in\mathcal{F}_{s}\cap\mathcal{V} _{s^{*},k^{*}}}s{s^{*}}\rho_{s}(i,\tilde{p}_{i})\frac{\bm{v}_{s,1}}{\left\|\xi _{i}^{(\tilde{p}_{i})}\right\|^{2}}+\sum_{i\in\mathcal{F}_{-s}\cap\mathcal{V} _{s^{*},k^{*}}}s{s^{*}}\rho_{s}(i,\tilde{p}_{i})\frac{\bm{v}_{-s,1}}{\left\|\xi _{i}^{(\tilde{p}_{i})}\right\|^{2}}\right).\]

The function \(Q^{(s^{*},k^{*})}\) plays a crucial role in Section C.2.4 and Section D.2.4. The key intuition behind our definition of \(Q^{(s^{*},k^{*})}\) is that \(Q^{(s^{*},k^{*})}(\bm{W}^{(t)})\) represents the term updated by the data having the feature vector \(\bm{v}_{s^{*},k^{*}}\), where \(\bm{W}^{(t)}\) are the iterates of either ERM or Cutout. As expected from this intuition, if we sum all \(Q^{(s^{*},k^{*})}_{1}(\bm{w}_{1})\) and \(Q^{(s^{*},k^{*})}_{-1}(\bm{w}_{-1})\) over all \(s^{*}\in\{\pm 1\}\) and \(k^{*}\in[K]\), the result will be equal to \(\bm{w}_{1}-\bm{w}_{1}^{(0)}\) and \(\bm{w}_{-1}-\bm{w}_{-1}^{(0)}\), respectively.

Proof.: From linear independency of \(\{\bm{v}_{s,k}\}_{s\in\{\pm 1\},k\in[K]}\cup\left\{\bm{x}_{i}^{(p)}\right\}_{i\in[n],p \in[P]\setminus\{p_{i}^{*}\}}\), we can express any element \(\bm{W}=\{\bm{w}_{1},\bm{w}_{-1}\}\in\mathcal{W}\) as

\[\bm{w}_{s}=\bm{w}_{s}^{(0)}+\sum_{k\in[K]}\tilde{\gamma}_{s}(s,k)\bm{v}_{s,k}- \sum_{k\in[K]}\tilde{\gamma}_{s}(-s,k)\bm{v}_{-s,k}\]\[+\sum_{\begin{subarray}{c}i\in\mathcal{V}_{s}\\ p\in[P]\setminus\{p_{i}^{*}\}\end{subarray}}\rho_{s}(i,p)\frac{\xi_{i}^{(p)}}{ \left\|\xi_{i}^{(p)}\right\|^{2}}-\sum_{\begin{subarray}{c}i\in\mathcal{V}_{-s }\\ p\in[P]\setminus\{p_{i}^{*}\}\end{subarray}}\rho_{s}(i,p)\frac{\xi_{i}^{(p)}}{ \left\|\xi_{i}^{(p)}\right\|^{2}}\] (13)

with unique \(\{\tilde{\gamma}_{s}(s,k),\tilde{\gamma}_{s}(-s,k)\}_{s\in\{\pm 1\},k\in[K]}\) and \(\{\rho_{s}(i,p)\}_{s\in\{\pm 1\},i\in[n],p\in[P]\setminus\{i^{*}\}}\). If we define \(\gamma_{s}(s,k)\) and \(\gamma_{s}(-s,k)\) as \(\gamma_{s}(s,k)=\tilde{\gamma}_{s}(s,k),\gamma_{s}(-s,k)=\tilde{\gamma}_{s}(-s,k)\) for \(k\neq 1\), and

\[\gamma_{s}(s,1) =\tilde{\gamma}_{s}(s,1)-\alpha\sum_{i\in\mathcal{F}_{s}}sy_{i} \rho_{s}(i,\tilde{p}_{i})\left\|\xi_{i}^{(\tilde{p}_{i})}\right\|^{-2},\] \[\gamma_{s}(-s,1) =\tilde{\gamma}_{s}(-s,1)+\alpha\sum_{i\in\mathcal{F}_{-s}}sy_{i }\rho_{s}(i,\tilde{p}_{i})\left\|\xi_{i}^{(\tilde{p}_{i})}\right\|^{-2},\]

then we have

\[\boldsymbol{w}_{s} =\boldsymbol{w}_{s}^{(0)}+\sum_{k\in[K]}\gamma_{s}(s,k) \boldsymbol{v}_{s,k}-\sum_{k\in[K]}\gamma_{s}(-s,k)\boldsymbol{v}_{-s,k}\] \[+\sum_{\begin{subarray}{c}i\in\mathcal{V}_{s}\\ p\in[P]\setminus\{p_{i}^{*}\}\end{subarray}}\rho_{s}(i,p)\frac{\xi_{i}^{(p)}}{ \left\|\xi_{i}^{(p)}\right\|^{2}}-\sum_{\begin{subarray}{c}i\in\mathcal{V}_{- s}\\ p\in[P]\setminus\{p_{i}^{*}\}\end{subarray}}\rho_{s}(i,p)\frac{\xi_{i}^{(p)}}{ \left\|\xi_{i}^{(p)}\right\|^{2}}\] \[+\alpha\left(\sum_{i\in\mathcal{F}_{s}}sy_{i}\rho_{s}(i,\tilde{p }_{i})\frac{\boldsymbol{v}_{s,1}}{\left\|\xi_{i}^{(\tilde{p}_{i})}\right\|^{2 }}+\sum_{i\in\mathcal{F}_{-s}}sy_{i}\rho_{s}(i,\tilde{p}_{i})\frac{ \boldsymbol{v}_{-s,1}}{\left\|\xi_{i}^{(\tilde{p}_{i})}\right\|^{2}}\right).\]

Next, we want to show the uniqueness part. Suppose \(\{\hat{\gamma}_{s}(s,k),\hat{\gamma}_{s}(-s,k)\}_{s\in\{\pm 1\},k\in[K]}\) and \(\{\hat{\rho}_{s}(i,p)\}_{s\in\{\pm 1\},i\in[n],p\in[P]\setminus\{i^{*}\}}\) satisfies

\[\boldsymbol{w}_{s} =\boldsymbol{w}_{s}^{(0)}+\sum_{k\in[K]}\hat{\gamma}_{s}(s,k) \boldsymbol{v}_{s,k}-\sum_{k\in[K]}\hat{\gamma}_{s}(-s,k)\boldsymbol{v}_{-s,k}\] \[+\sum_{\begin{subarray}{c}i\in\mathcal{V}_{s}\\ p\in[P]\setminus\{p_{i}^{*}\}\end{subarray}}\hat{\rho}_{s}(i,p)\frac{\xi_{i}^ {(p)}}{\left\|\xi_{i}^{(p)}\right\|^{2}}-\sum_{\begin{subarray}{c}i\in \mathcal{V}_{-s}\\ p\in[P]\setminus\{p_{i}^{*}\}\end{subarray}}\hat{\rho}_{s}(i,p)\frac{\xi_{i}^{(p )}}{\left\|\xi_{i}^{(p)}\right\|^{2}}\] \[+\alpha\left(\sum_{i\in\mathcal{F}_{s}}sy_{i}\hat{\rho}_{s}(i, \tilde{p}_{i})\frac{\boldsymbol{v}_{s,1}}{\left\|\xi_{i}^{(\tilde{p}_{i})} \right\|^{2}}+\sum_{i\in\mathcal{F}_{-s}}sy_{i}\hat{\rho}_{s}(i,\tilde{p}_{i}) \frac{\boldsymbol{v}_{-s,1}}{\left\|\xi_{i}^{(\tilde{p}_{i})}\right\|^{2}} \right).\]

We have

\[\boldsymbol{w}_{s} =\boldsymbol{w}_{s}^{(0)}+\sum_{k\in[K]\setminus\{1\}}\hat{ \gamma}_{s}(s,k)\boldsymbol{v}_{s,k}-\sum_{k\in[K]\setminus\{1\}}\hat{\gamma}_ {s}(-s,k)\boldsymbol{v}_{-s,k}\] \[+\left(\hat{\gamma}_{s}(s,1)+\alpha\sum_{i\in\mathcal{F}_{s}}sy_{i }\hat{\rho}_{s}(i,\tilde{p}_{i})\left\|\xi_{i}^{(\tilde{p}_{i})}\right\|^{-2} \right)\boldsymbol{v}_{s,1}\] \[-\left(\hat{\gamma}_{s}(-s,1)-\alpha\sum_{i\in\mathcal{F}_{-s}}sy_ {i}\hat{\rho}_{s}(i,\tilde{p}_{i})\left\|\xi_{i}^{(\tilde{p}_{i})}\right\|^{-2 }\right)\boldsymbol{v}_{-s,1}\] \[+\sum_{\begin{subarray}{c}i\in\mathcal{V}_{s}\\ p\in[P]\setminus\{p_{i}^{*}\}\end{subarray}}\hat{\rho}_{s}(i,p)\frac{\xi_{i}^{(p )}}{\left\|\xi_{i}^{(p)}\right\|^{2}}-\sum_{\begin{subarray}{c}i\in\mathcal{V}_{- s}\\ p\in[P]\setminus\{p_{i}^{*}\}\end{subarray}}\hat{\rho}_{s}(i,p)\frac{\xi_{i}^{(p)}}{ \left\|\xi_{i}^{(p)}\right\|^{2}}.\]

From the uniqueness of (13), we have

\[\hat{\gamma}_{s}(s,k)=\tilde{\gamma}_{s}(s,k)=\gamma_{s}(s,k),\quad\hat{\gamma}_ {s}(-s,k)=\tilde{\gamma}_{s}(-s,k)=\gamma_{s}(-s,k),\]

for each \(s\in\{\pm 1\},k\in[K]\setminus\{1\}\), and \(\hat{\rho}_{s}(i,p)=\rho_{s}(i,p)\) for each \(i\in[n],p\in[P]\setminus\{p_{i}^{*}\}\). Furthermore,

\[\hat{\gamma}_{s}(s,1)+\alpha\sum_{i\in\mathcal{F}_{s}}sy_{i}\hat{\rho}_{s}(i, \tilde{p}_{i})\left\|\xi_{i}^{(\tilde{p}_{i})}\right\|^{-2}=\tilde{\gamma}_{s}(s,1 )=\gamma_{s}(s,1)+\alpha\sum_{i\in\mathcal{F}_{s}}sy_{i}\rho_{s}(i,\tilde{p}_{i}) \left\|\xi_{i}^{(\hat{p}_{i})}\right\|^{-2},\]\[\hat{\gamma}_{s}(-s,1)-\alpha\sum_{i\in\mathcal{F}_{-s}}sy_{i}\hat{\rho}_{s}(i, \tilde{p}_{i})\left\|\xi_{i}^{(\tilde{p}_{i})}\right\|^{-2}=\tilde{\gamma}_{s}(- s,1)=\gamma_{s}(-s,1)-\alpha\sum_{i\in\mathcal{F}_{-s}}sy_{i}\rho_{s}(i,\tilde{p}_{ i})\left\|\xi_{i}^{(\tilde{p}_{i})}\right\|^{-2}.\]

Hence, we obtain the uniqueness of the expression and \(Q^{(s^{*},k^{*})}\) is well defined for each \(s^{*}\in\{\pm 1\}\) and \(k^{*}\in[K]\).

[MISSING_PAGE_EMPTY:29]

\[+\alpha\left(\sum_{i\in\mathcal{F}_{s}}sy_{i}\rho_{s}^{(t)}(i,\tilde{p}_{i}) \frac{\bm{v}_{s,1}}{\left\|\xi_{i}^{(\tilde{p}_{i})}\right\|^{2}}+\sum_{i\in \mathcal{F}_{-s}}sy_{i}\rho_{s}^{(t)}(i,\tilde{p}_{i})\frac{\bm{v}_{-s,1}}{ \left\|\xi_{i}^{(\tilde{p}_{i})}\right\|^{2}}\right),\]

for each \(s\in\{\pm 1\}\). Furthermore, \(\gamma_{s}^{(t)}(s^{\prime},k)\)'s and \(\rho_{s}^{(t)}(i,p)\)'s are monotone increasing. 

### Proof of Theorem 3.1

To show Theorem 3.1, we present a structured proof comprising the following five steps:

1. Establish upper bounds on \(\gamma_{s}^{(t)}(s^{\prime},k)\)'s and \(\rho_{s}^{(t)}(i,p)\)'s to apply Lemma B.4 (Section C.2.1).
2. Demonstrate that the model learns common features quickly (Section C.2.2).
3. Show that the model overfits dominant noise in (extremely) rare data instead of learning its feature (Section C.2.3).
4. Confirm the persistence of this tendency until \(T^{*}\) iterates (Section C.2.4).
5. Characterize train accuracy and test accuracy (Section C.2.5).

#### c.2.1 Bounds on the Coefficients in Feature Noise Decomposition

The following lemma provides upper bounds on Lemma B.3 during \(T^{*}\) iterations.

**Lemma C.1**.: _Suppose the event \(E_{\mathrm{init}}\) occurs. For any \(t\in[0,T^{*}]\), we have_

\[0\leq\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)\leq 4\log(\eta T^{*}), \quad 0\leq\rho_{y_{i}}^{(t)}(i,p)+\beta\rho_{-y_{i}}^{(t)}(i,p)\leq 4\log \left(\eta T^{*}\right),\]

_for all \(s\in\{\pm 1\},k\in[K],i\in[n]\) and \(p\in[P]\setminus\{p_{i}^{*}\}\). Consequently, \(\gamma_{s}^{(t)}(s^{\prime},k),\rho_{s}^{(t)}(i,p)=\widetilde{\mathcal{O}}( \beta^{-1})\) for all \(s,s^{\prime}\in\{\pm 1\},k\in[K],i\in[n]\) and \(p\in[P]\setminus\{p_{i}^{*}\}\)._

Proof of Lemma c.1.: The first argument implies the second argument since \(\log(\eta T^{*})=\mathrm{polylog}(d)\) and

\[\gamma_{s}^{(t)}(s^{\prime},k)\leq\beta^{-1}\left(\gamma_{s^{\prime}}^{(t)}( s^{\prime},k)+\beta\gamma_{s^{\prime}}^{(t)}(s^{\prime},k)\right),\quad\rho_{s}^{(t )}(i,p)\leq\beta^{-1}\left(\rho_{y_{i}}^{(t)}(i,p)+\beta\rho_{-y_{i}}^{(t)}(i, p)\right),\]

for all \(s,s^{\prime}\in\{\pm 1\},k\in[K],i\in[n]\) and \(p\in[P]\setminus\{p_{i}^{*}\}\).

We will prove this by using induction on \(t\). The initial case \(t=0\) is trivial. Suppose the given statement holds at \(t=T\) and consider the case \(t=T+1\).

Let \(\tilde{T}_{s,k}\leq T\) denote the smallest iteration where \(\gamma_{s}^{(\tilde{T}_{s,k}+1)}(s,k)+\beta\gamma_{-s}^{(\tilde{T}_{s,k}+1)}( s,k)>2\log(\eta T^{*})\). We assume the existence of \(\tilde{T}_{s,k}\), as its absence would directly lead to our desired conclusion; to see why, note that the following holds, due to (14) and (11):

\[\gamma_{s}^{(T+1)}(s,k)+\beta\gamma_{-s}^{(T+1)}(s,k)\] \[=\gamma_{s}^{(T)}(s,k)+\beta\gamma_{-s}^{(T)}(s,k)+\frac{\eta}{n} \sum_{i\in\mathcal{V}_{s,k}}g_{i}^{(T)}\bigg{(}\phi^{\prime}\left(\left<\bm{w} _{s}^{(T)},\bm{v}_{s,k}\right>\right)+\beta\phi^{\prime}\left(\left<\bm{w}_{- s}^{(T)},\bm{v}_{s,k}\right>\right)\bigg{)}\] \[\leq 2\log(\eta T^{*})+2\eta\leq 4\log(\eta T^{*})\]

Now suppose there exists such \(\tilde{T}_{s,k}\leq T\). By (14), we have

\[\gamma_{s}^{(T+1)}(s,k)+\beta\gamma_{-s}^{(T+1)}(s,k)\] \[=\gamma_{s}^{(\tilde{T}_{s,k})}(s,k)+\beta\gamma_{-s}^{(\tilde{T}_ {s,k})}(s,k)\] \[\quad+\sum_{t=\tilde{T}_{s,k}}^{T}\left(\gamma_{s}^{(t+1)}(s,k)+ \beta\gamma_{-s}^{(t+1)}(s,k)-\gamma_{s}^{(t)}(s,k)-\beta\gamma_{-s}^{(t)}(s,k)\right)\] \[\leq 2\log(\eta T^{*})+\log(\eta T^{*})+\frac{\eta}{n}\sum_{t= \tilde{T}_{s,k}+1}^{T}\sum_{i\in\mathcal{V}_{s,k}}g_{i}^{(t)}\bigg{(}\phi^{ \prime}\left(\left<\bm{w}_{s}^{(t)},\bm{v}_{s,k}\right>\right)+\beta\phi^{ \prime}\left(\left<\bm{w}_{-s}^{(t)},\bm{v}_{s,k}\right>\right)\bigg{)}.\]The inequality is due to \(\gamma_{s}^{(\tilde{T}_{s,k})}(s,k)+\beta\gamma_{-s}^{(\tilde{T}_{s,k})}(s,k)\leq 2 \log(\eta T^{*})\) from our choice of \(\tilde{T}_{s,k}\) and

\[\frac{\eta}{n}\sum_{i\in\mathcal{V}_{s,k}}g_{i}^{(\tilde{T}_{s,k})}\bigg{(}\phi ^{\prime}\left(\left\langle\bm{w}_{s}^{(\tilde{T}_{s,k})},\bm{v}_{s,k}\right \rangle\right)+\beta\phi^{\prime}\left(\left\langle\bm{w}_{-s}^{(\tilde{T}_{s,k})},\bm{v}_{s,k}\right\rangle\right)\bigg{)}\leq 2\eta\leq\log(\eta T^{*}),\]

from (11).

For each \(t=\tilde{T}_{s,k}+1,\ldots T\), and \(i\in\mathcal{V}_{s,k}\), we have

\[y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i})\] \[\geq\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)+\sum_{p\in [P]\setminus\{p_{i}^{*}\}}\Big{(}\rho_{s}^{(t)}(i,p)+\beta\rho_{-s}^{(t)}(i,p )\Big{)}-2P\cdot o\left(\frac{1}{\mathrm{polylog}(d)}\right)\] \[\geq\frac{3}{2}\log(\eta T^{*})\]

The first inequality is due to Lemma B.4 and the second inequality holds due to (A7), (8), and our choice of \(t\), \(\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)\geq 2\log(\eta T^{*})\).

Hence, we obtain

\[\frac{\eta}{n}\sum_{t=\tilde{T}_{s,k}}^{T}\sum_{i\in\mathcal{V}_{s,k}}g_{i}^{(t)}\bigg{(}\phi^{\prime}\left(\left\langle\bm{w}_{s}^{(t)},\bm{v} _{s,k}\right\rangle\right)+\beta\phi^{\prime}\left(\left\langle\bm{w}_{-s}^{( t)},\bm{v}_{s,k}\right\rangle\right)\bigg{)}\] \[\leq\frac{2\eta}{n}\sum_{t=\tilde{T}_{s,k}}^{T}\sum_{i\in\mathcal{ V}_{s,k}}\exp\left(-y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i})\right)\] \[\leq\frac{2|\mathcal{V}_{s,k}|}{n}(\eta T^{*})\exp\left(-\frac{3} {2}\log(\eta T^{*})\right)\] \[\leq\frac{2}{\sqrt{\eta T^{*}}}\leq\log(\eta T^{*}),\]

where the last inequality holds for any reasonably large \(T^{*}\). Merging all inequalities together, we have \(\gamma_{s}^{(T+1)}(s,k)+\beta\gamma_{-s}^{(T+1)}(s,k)\leq 4\log(\eta T^{*})\).

Next, we will follow similar arguments to show that

\[\rho_{y_{i}}^{(T+1)}(i,p)+\beta\rho_{-y_{i}}^{(T+1)}(i,p)\leq 4\log(\eta T^{*})\]

for each \(i\in[n]\) and \(p\in[P]\setminus\{p_{i}^{*}\}\).

Let \(\tilde{T}_{i}^{(p)}\leq T\) be the smallest iteration such that \(\rho_{y_{i}}^{(\tilde{T}^{(p)}+1)}(i,p)+\beta\rho_{-y_{i}}^{(\tilde{T}^{(p)}+ 1)}(i,p)>2\log(\eta T^{*})\). We assume the existence of \(\tilde{T}_{i}^{(p)}\), as its absence would directly lead to our desired conclusion; to see why, note that the following holds, due to (15) and (11):

\[\rho_{y_{i}}^{(T+1)}(i,p)+\beta\rho_{-y_{i}}^{(T+1)}(i,p)\] \[=\rho_{y_{i}}^{(T)}(i,p)+\beta\rho_{-y_{i}}^{(T)}(i,p)+\frac{\eta }{n}g_{i}^{(T)}\bigg{(}\phi^{\prime}\left(\left\langle\bm{w}_{s}^{(t)},\bm{x}_ {i}^{(p)}\right\rangle\right)+\beta\phi^{\prime}\left(\left\langle\bm{w}_{s}^{ (t)},\bm{x}_{i}^{(p)}\right\rangle\right)\bigg{)}\left\|\xi_{i}^{(p)}\right\|^{2}\] \[\leq 2\log(\eta T^{*})+2\eta\leq 4\log(\eta T^{*}),\]

where the first inequality is due to \(\left\|\xi_{i}^{(p)}\right\|\leq\frac{3}{2}\sigma_{\mathrm{d}}^{2}d\) and (A4), and the last inequality is due to (11).

Now suppose there exists such \(\tilde{T}_{i}^{(p)}\leq T\). By (15), we have

\[\rho_{y_{i}}^{(T+1)}(i,p)+\beta\rho_{-y_{i}}^{(T+1)}(i,p)\]\[=\rho_{y_{i}}^{(\tilde{T}_{i}^{(p)})}(i,p)+\beta\rho_{-y_{i}}^{( \tilde{T}_{i}^{(p)})}(i,p)\] \[\quad+\sum_{t=\tilde{T}_{i}^{(p)}}^{T}\left(\rho_{y_{i}}^{(t+1)}(i, p)+\beta\rho_{-y_{i}}^{(t+1)}(i,p)-\rho_{y_{i}}^{(t)}(i,p)-\beta\rho_{-y_{i}}^{(t)}(i,p)\right)\] \[\leq 2\log(\eta T^{*})+\log(\eta T^{*})\] \[\quad+\frac{\eta}{n}\sum_{t=\tilde{T}_{i}^{(p)}+1}^{T}g_{i}^{(t)} \bigg{(}\phi^{\prime}\left(\left\langle\bm{w}_{s}^{(t)},\bm{x}_{i}^{(p)} \right\rangle\right)+\beta\phi^{\prime}\left(\left\langle\bm{w}_{-s}^{(t)}, \bm{x}_{i}^{(p)}\right\rangle\right)\bigg{)}\left\|\xi_{i}^{(p)}\right\|^{2}\]

The inequality is due to \(\rho_{y_{i}}^{(\tilde{T}_{i}^{(p)})}(i,p)+\beta\rho_{-y_{i}}^{(\tilde{T}_{i}^ {(p)})}(i,p)\leq 2\log(\eta T^{*})\) from our choice of \(\tilde{T}_{i}^{(p)}\) and

from \(\left\|\xi_{i}^{(p)}\right\|^{2}\leq\frac{3}{2}\sigma_{\mathrm{d}}^{2}d\), (A4), and (11).

For each \(t=\tilde{T}_{i}^{(p)}+1,\ldots,T\), if \(i\in\mathcal{V}_{s,k}\), then we have

\[\quad y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i})\] \[=\phi\left(\left\langle\bm{w}_{y_{i}}^{(t)},\bm{x}_{i}^{(p)} \right\rangle\right)-\phi\left(\left\langle\bm{w}_{-y_{i}}^{(t)},\bm{x}_{i}^{ (p)}\right\rangle\right)+\sum_{q\in[P]\setminus\{p\}}\left(\phi\left(\left\langle \bm{w}_{y_{i}}^{(t)},\bm{x}_{i}^{(p)}\right\rangle\right)-\phi\left(\left\langle \bm{w}_{-y_{i}}^{(t)},\bm{x}_{i}^{(p)}\right\rangle\right)\right)\] \[\geq\rho_{y_{i}}^{(t)}(i,p)+\beta\rho_{-y_{i}}^{(t)}(i,p)+\gamma_ {y_{i}}^{(t)}(s,k)+\beta\gamma_{-y_{i}}^{(t)}(s,k)\] \[\quad+\sum_{q\in[P]\setminus\{p,p_{i}^{*}\}}\left(\rho_{y_{i}}^{ (t)}(i,q)+\beta\rho_{-y_{i}}^{(t)}(i,q)\right)-2P\cdot o\left(\frac{1}{\mathrm{ polylog}(d)}\right)\] \[\geq\frac{3}{2}\log(\eta T^{*}).\]

The first inequality is due to Lemma B.4 and the second inequality holds because from our choice of \(t\), \(\rho_{y_{i}}^{(t)}(i,p)+\beta\rho_{-y_{i}}^{(t)}(i,p)\geq 2\log(\eta T^{*})\).

Therefore, we have

\[\frac{\eta}{n}\sum_{t=\tilde{T}_{i}^{(p)}+1}^{T}g_{i}^{(t)}\bigg{(} \phi^{\prime}\left(\left\langle\bm{w}_{y_{i}}^{(t)},\bm{x}_{i}^{(p)}\right\rangle \right)+\beta\phi^{\prime}\left(\left\langle\bm{w}_{-y_{i}}^{(t)},\bm{x}_{i}^{ (p)}\right\rangle\right)\bigg{)}\left\|\xi_{i}^{(p)}\right\|^{2}\] \[\leq\eta\sum_{t=\tilde{T}_{i}^{(p)}+1}^{T}\exp\left(-y_{i}f_{\bm{W }^{(t)}}(\bm{X}_{i})\right)\leq(\eta T^{*})\exp\left(-\frac{3}{2}\log(\eta T^{ *})\right)\] \[\leq\frac{1}{\sqrt{\eta}T^{*}}\leq\log(\eta T^{*}),\]

where the first inequality is due to \(\left\|\xi_{i}^{(p)}\right\|^{2}\leq\frac{3}{2}\sigma_{\mathrm{d}}^{2}\), (A4) and the last inequality holds for any reasonably large \(T^{*}\). Merging all inequalities together, we conclude \(\rho_{y_{i}}^{(T+1)}(i,p)+\beta\rho_{-y_{i}}^{(T+1)}(i,p)\leq 4\log(\eta T^{*})\). 

#### c.2.2 Learning Common Features

In the initial stages of training, the model quickly learns common features while exhibiting minimal overfitting to Gaussian noise.

First, we establish lower bounds on the number of iterations ensuring that noise coefficients \(\rho_{s}^{(t)}(i,p)\) remain small, up to the order of \(\frac{1}{P}\).

**Lemma C.2**.: _Suppose the event \(E_{\mathrm{init}}\) occurs. There exists \(\tilde{T}>\frac{n}{6\eta P\sigma_{\mathrm{d}}^{2}d}\) such that \(\rho_{s}^{(t)}(i,p)\leq\frac{1}{4P}\) for all \(0\leq t<\tilde{T},s\in\{\pm 1\},i\in[n]\) and \(p\in[P]\setminus\{p_{i}^{*}\}\)._Proof of Lemma c.2.: Let \(\tilde{T}\) be the smallest iteration such that \(\rho_{s}^{(\tilde{T})}(i,p)\geq\frac{1}{4P}\) for some \(s\in\{\pm 1\},i\in[n]\) and \(p\in[P]\setminus\{p_{i}^{*}\}\). We assume the existence of \(\tilde{T}\), as its absence would directly lead to our conclusion. Then, for any \(0\leq t<\tilde{T}\), we have

\[\rho_{s}^{(t+1)}(i,p)=\rho_{s}^{(t)}(i,p)+\frac{\eta}{n}g_{i}^{(t)}\phi^{\prime }\left(\left<\bm{w}_{s}^{(t)},\bm{x}_{i}^{(p)}\right>\right)\left\|\xi_{i}^{(p )}\right\|^{2}\leq\rho_{s}^{(t)}(i,p)+\frac{3\eta\sigma_{\mathrm{d}}^{2}d}{2n},\]

where the inequality is due to \(g_{i}^{(t)}<1\), \(\phi^{\prime}\leq 1\), and \(\left\|\xi_{i}^{(p)}\right\|^{2}\leq\frac{3}{2}\sigma_{\mathrm{d}}^{2}d\). Hence, we have

\[\frac{1}{4P}\leq\rho_{s}^{(\tilde{T})}(i,p)=\sum_{t=0}^{\tilde{T}-1}\left(\rho _{s}^{(t+1)}(i,p)-\rho_{s}^{(t)}(i,p)\right)<\frac{3\eta\sigma_{\mathrm{d}}^{ 2}d}{2n}\tilde{T},\]

and we conclude \(\tilde{T}>\frac{n}{6\eta P\sigma_{\mathrm{d}}^{2}d}\) which is the desired result. 

Next, we will show that the model learns common features in at least constant order within \(\tilde{T}\) iterates.

**Lemma C.3**.: _Suppose the event \(E_{\mathrm{init}}\) occurs and \(\rho_{k}=\omega\left(\frac{\sigma_{\mathrm{d}}^{2}d}{\beta n}\right)\) for some \(k\in[K]\). Then, for each \(s\in\{\pm 1\}\), there exists \(T_{s,k}\leq\frac{9n}{\eta\beta|V_{s,k}|}\) such that \(\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)\geq 1\) for any \(t>T_{s,k}\)._

Proof of Lemma c.3.: Suppose \(\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)<1\) for all \(0\leq t\leq\frac{n}{6\eta P\sigma_{\mathrm{d}}^{2}d}\). For each \(i\in\mathcal{V}_{s,k}\), we have

\[y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i})\] \[=\phi\left(\left<\bm{w}_{s}^{(t)},\bm{v}_{s,k}\right>\right)- \phi\left(\left<\bm{w}_{-s}^{(t)},\bm{v}_{s,k}\right>\right)+\sum_{p\in[P] \setminus\{p_{i}^{*}\}}\left(\phi\left(\left<\bm{w}_{s}^{(t)},\bm{x}_{i}^{(p) }\right>\right)-\phi\left(\left<\bm{w}_{-s}^{(t)},\bm{x}_{i}^{(p)}\right> \right)\right)\right)\] \[\leq\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)+\sum_{p\in[ P]\setminus\{p_{i}^{*}\}}\left(\rho_{s}^{(t)}(i,p)+\beta\rho_{-s}^{(t)}(i,p) \right)+2P\cdot o\left(\frac{1}{\mathrm{polylog}(d)}\right)\] \[\leq 1+2P\cdot\frac{1}{4P}+2P\cdot o\left(\frac{1}{\mathrm{ polylog}(d)}\right)\] \[\leq 2.\]

The first inequality is due to Lemma B.4, the second inequality holds since we can apply Lemma C.2, and the last inequality is due to (A1). Thus, \(g_{i}^{(t)}=\frac{1}{1+\exp\left(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i})\right)}>\frac {1}{9}\) and we have

\[\gamma_{s}^{(t+1)}(s,k)+\beta\gamma_{-s}^{(t+1)}(s,k)\] \[=\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)+\frac{\eta}{n }\sum_{i\in\mathcal{V}_{s,k}}g_{i}^{(t)}\bigg{(}\phi^{\prime}\left(\left<\bm{ w}_{s}^{(t)},\bm{v}_{s,k}\right>\right)+\beta\phi^{\prime}\left(\left<\bm{w}_{-s}^{(t) },\bm{v}_{s,k}\right>\right)\bigg{)}\] \[\geq\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)+\frac{\eta \beta|\mathcal{V}_{s,k}|}{9n}.\]

Notice that \(|\mathcal{V}_{s,k}|=\rho_{k}n\). From the condition in the lemma statement, we have \(\frac{9n}{\eta\beta|\mathcal{V}_{s,k}|}=o\left(\frac{n}{6\eta P\sigma_{\mathrm{ d}}^{2}d}\right)\). If we choose \(t_{0}\in\left[\frac{9n}{\eta\beta|\mathcal{V}_{s,k}|},\frac{n}{6\eta P\sigma_{ \mathrm{d}}^{2}d}\right]\), then

\[1>\gamma_{s}^{(t_{0})}(s,k)+\beta\gamma_{-s}^{(t_{0})}(s,k)\geq\frac{\eta \beta|\mathcal{V}_{s,k}|}{9n}t_{0}\geq 1,\]

and this is contradictory; therefore, it cannot hold that \(\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)<1\) for all \(0\leq t\leq\frac{n}{6\eta P\sigma_{\mathrm{d}}^{2}d}\). Hence, there exists \(0\leq T_{s,k}<\frac{n}{6\eta P\sigma_{\mathrm{d}}^{2}d}\) such that \(\gamma_{s}^{(T_{s,k}+1)}(s,k)+\beta\gamma_{-s}^{(T_{s,k}+1)}(s,k)\geq 1\) and choose the smallest one. Then we obtain

\[1>\gamma_{s}^{(T_{s,k})}(s,k)+\beta\gamma_{-s}^{(T_{s,k})}(s,k)\geq\frac{\eta \beta|\mathcal{V}_{s,k}|}{9n}T_{s,k}.\]

Therefore, \(T_{s,k}<\frac{9n}{\eta\beta|\mathcal{V}_{s,k}|}\) and this is what we desired.

What We Have So Far.For any common feature \(\bm{v}_{s,k}\) with \(s\in\{\pm 1\}\) and \(k\in\mathcal{K}_{C}\), it satisfies \(\rho_{k}=w\left(\frac{\sigma_{2}^{2}d}{\beta n}\right)\) due to (A4). By Lemma C.3, at any iterate \(t\in\left[\tilde{T}_{1},T^{*}\right]\) with \(\tilde{T}_{1}:=\max_{s\in\{\pm 1\},k\in\mathcal{K}_{C}}T_{s,k}\), the following properties hold if the event \(E_{\mathrm{init}}\) occurs:

* (Learn common features): For any \(s\in\{\pm 1\}\) and \(k\in\mathcal{K}_{C}\), \[\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)=\Omega(1).\]
* For any \(s\in\{\pm 1\},i\in[n],\) and \(p\in[P]\setminus\{p_{i}^{*}\},\rho_{s}^{(t)}(i,p)=\widetilde{\mathcal{O}} \left(\beta^{-1}\right)\).

#### c.2.3 Overfitting (extremely) Rare Data

In the previous step, we have shown that common data can be well-classified by learning common features. In this step, we will show that the model correctly classifies (extremely) rare data by overfitting dominant noise instead of learning its features.

We first introduce lower bounds on the number of iterates such that feature coefficients \(\gamma_{s}^{(t)}(s^{\prime},k)\) remain small, up to the order of \(\alpha^{2}\beta^{-1}\). This lemma holds for any kind of features, but we will focus on (extremely) rare features. This does not contradict the results from Section C.2.2 for common features since the upper bound on the number of iterations in Lemma C.3 is larger than the lower bound on the number of iterations in this lemma.

**Lemma C.4**.: _Suppose the event \(E_{\mathrm{init}}\) occurs. For each \(s\in\{\pm 1\}\) and \(k\in[K]\), there exists \(\tilde{T}_{s,k}>\frac{n\alpha^{2}}{\eta\beta|\mathcal{V}_{s,k}|}\) such that \(\gamma_{s^{\prime}}^{(t)}(s,k)\leq\alpha^{2}\beta^{-1}\) for any \(0\leq t<\tilde{T}_{s,k}\) and \(s^{\prime}\in\{\pm 1\}\)._

Proof of Lemma c.4.: Let \(\tilde{T}_{s,k}\) be the smallest iterate such that \(\gamma_{s^{\prime}}^{(\tilde{T}_{s,k})}(s,k)>\alpha^{2}\beta^{-1}\) for some \(s^{\prime}\in\{\pm 1\}\). We assume the existence of \(\tilde{T}_{s,k}\), as its absence would directly lead to our conclusion.

For any \(0\leq t<\tilde{T}_{s,k}\),

\[\gamma_{s^{\prime}}^{(t+1)}(s,k)=\gamma_{s^{\prime}}^{(t)}(s,k)+\frac{\eta}{n} \sum_{i\in\mathcal{V}_{s,k}}g_{i}^{(t)}\phi^{\prime}\left(\left\langle\bm{w}_{ s^{\prime}}^{(t)},\bm{v}_{s,k}\right\rangle\right)\leq\gamma_{s^{\prime}}^{(t)}(s,k)+\frac{\eta|\mathcal{V}_{s,k}|}{n},\]

and we have

\[\alpha^{2}\beta^{-1}<\gamma_{s^{\prime}}^{(\tilde{T}_{s,k})}(s,k)=\sum_{t=0}^ {\tilde{T}_{s,k}-1}\left(\gamma_{s^{\prime}}^{(t+1)}(s,k)-\gamma_{s^{\prime}} ^{(t)}(s,k)\right)\leq\frac{\eta|\mathcal{V}_{s,k}|}{n}\tilde{T}_{s,k}.\]

We conclude \(\tilde{T}_{s,k}>\frac{n\alpha^{2}}{\eta\beta|\mathcal{V}_{s,k}|}\) which is the desired result. 

Next, we will show that the model overfits (extremely) rare data by memorizing dominant noise patches in at least constant order within \(\tilde{T}_{s,k}\) iterates.

**Lemma C.5**.: _Suppose the event \(E_{\mathrm{init}}\) occurs and \(\rho_{k}=o\left(\frac{\alpha^{2}\sigma_{0}^{2}d}{n}\right)\). Then, for each \(i\in\mathcal{V}_{s,k}\), there exists \(T_{i}\in\left[\tilde{T}_{1},\frac{18n}{\eta\beta\sigma_{0}^{2}d}\right]\) such that_

\[\sum_{p\in[P]\setminus\{p_{i}^{*}\}}\left(\rho_{s}^{(t)}(i,p)+\beta\rho_{-s}^ {(t)}(i,p)\right)\geq 1,\]

_for any \(t>T_{i}\)._

Proof of Lemma c.5.: Suppose \(\sum_{p\in[P]\setminus\{p_{i}^{*}\}}\left(\rho_{s}^{(t)}(i,p)+\beta\rho_{-s}^ {(t)}(i,p)\right)<1\) for \(0\leq t\leq\frac{n\alpha^{2}}{\eta\beta|\mathcal{V}_{s,k}|}\).

From Lemma B.4 and Lemma C.4, we have

\[y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i})\]\[=\phi\left(\left\langle\bm{w}_{s}^{(t)},\bm{v}_{s,k}\right\rangle \right)-\phi\left(\left\langle\bm{w}_{-s}^{(t)},\bm{v}_{s,k}\right\rangle\right)+ \sum_{p\in[P]\setminus\{p_{i}^{*}\}}\left(\phi\left(\left\langle\bm{w}_{s}^{(t) },\bm{x}_{i}^{(p)}\right\rangle\right)-\phi\left(\left\langle\bm{w}_{-s}^{(t) },\bm{x}_{i}^{(p)}\right\rangle\right)\right)\] \[\leq\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)+\sum_{p\in[ P]\setminus\{p_{i}^{*}\}}\left(\rho_{s}^{(t)}(i,p)+\beta\rho_{-s}^{(t)}(i,p) \right)+2P\cdot o\left(\frac{1}{\mathrm{polylog}(d)}\right)\] \[\leq(1+\beta)\alpha^{2}\beta^{-1}+1+2P\cdot o\left(\frac{1}{ \mathrm{polylog}(d)}\right)\] \[\leq 2,\]

where the last inequality is due to (10). Thus, we have \(g_{i}^{(t)}=\frac{1}{1+\exp\left(y_{i}f_{\bm{w}^{(t)}}(\bm{X}_{i})\right)}\geq \frac{1}{9}\). Also,

\[\rho_{s}^{(t+1)}(i,\tilde{p}_{i})+\beta\rho_{-s}^{(t+1)}(i,\tilde {p}_{i})\] \[=\rho_{s}^{(t)}(i,\tilde{p}_{i})+\beta\rho_{-s}^{(t)}(i,\tilde{p }_{i})+\frac{\eta}{n}g_{i}^{(t)}\bigg{(}\phi^{\prime}\left(\left\langle\bm{w} _{s}^{(t)},\bm{x}_{i}^{(\tilde{p}_{i})}\right\rangle\right)+\beta\phi^{\prime} \left(\left\langle\bm{w}_{-s}^{(t)},\bm{x}_{i}^{(\tilde{p}_{i})}\right\rangle \right)\bigg{)}\left\|\xi_{i}^{(\tilde{p}_{i})}\right\|^{2}\] \[\geq\rho_{s}^{(t)}(i,\tilde{p}_{i})+\beta\rho_{-s}^{(t)}(i,\tilde {p}_{i})+\frac{\eta\beta\sigma_{\mathrm{d}}^{2}d}{18n},\]

where the last inequality is due to \(\left\|\xi_{i}^{(\tilde{p}_{i})}\right\|^{2}\geq\frac{1}{2}\sigma_{\mathrm{d }}^{2}d\) and \(\phi^{\prime}\geq\beta\).

Notice that \(|\mathcal{V}_{s,k}|=\rho_{k}n\). From the given condition in the lemma statement, we have \(\frac{18n}{\eta\beta\sigma_{\mathrm{d}}^{2}d}=o\left(\frac{n\alpha^{2}}{\eta \beta|\mathcal{V}_{s,k}|}\right)\). If we choose \(t_{0}\in\left[\frac{18n}{\eta\beta\sigma_{\mathrm{d}}^{2}d},\frac{n\alpha^{2}} {\eta\beta|\mathcal{V}_{s,k}|}\right]\), then we have

\[1>\sum_{p\in[P]\setminus\{p_{i}^{*}\}}\left(\rho_{s}^{(t_{0})}(i,p)+\beta \rho_{-s}^{(t_{0})}(i,p)\right)\geq\rho_{s}^{(t_{0})}(i,\tilde{p}_{i})+\beta \rho_{-s}^{(t_{0})}(i,\tilde{p}_{i})\geq\frac{\eta\beta\sigma_{\mathrm{d}}^{2} d}{18n}t_{0}\geq 1.\]

This is a contradiction; therefore it cannot hold that \(\sum_{p\in[P]\setminus\{p_{i}^{*}\}}\left(\rho_{s}^{(t)}(i,p)+\beta\rho_{-s}^{ (t)}(i,p)\right)<1\) for all \(0\leq t\leq\frac{n\alpha^{2}}{\eta\beta|\mathcal{V}_{s,k}|}\). Hence, we can choose the smallest \(0\leq T_{i}<\frac{n\alpha^{2}}{\eta\beta|\mathcal{V}_{s,k}|}\) such that \(\sum_{p\in[P]\setminus\{p_{i}^{*}\}}\left(\rho_{s}^{(t+1)}(i,p)+\beta\rho_{-s} ^{(T_{i}+1)}(i,p)\right)\geq 1\).

For any \(0\leq t<T_{i}\),

\[1\geq\sum_{p\in[P]\setminus\{p_{i}^{*}\}}\left(\rho_{s}^{(T_{i})}(i,p)+\beta \rho_{-s}^{(T_{i})}(i,p)\right)\geq\rho_{s}^{(T_{i})}(i,\tilde{p}_{i})+\beta \rho_{-s}^{(T_{i})}(i,\tilde{p}_{i})\geq\frac{\eta\beta\sigma_{\mathrm{d}}^{2} d}{18n}T_{i},\]

and we conclude that \(T_{i}\leq\frac{18n}{\eta\beta\sigma_{\mathrm{d}}^{2}d}\).

Lastly, we move on to prove \(T_{i}>\bar{T}_{1}\). Combining Lemma C.2 and Lemma C.3 leads to

\[\sum_{p\in[P]\setminus\{p_{i}^{*}\}}\left(\rho_{s}^{(\bar{T}_{1})}(i,p)+\beta \rho_{-s}^{(\bar{T}_{1})}(i,p)\right)\leq\frac{1}{2}.\]

Thus, we have \(T_{i}>\bar{T}_{1}\) and this is what we desired. 

What We Have So Far.For any \(k\in\mathcal{K}_{K}\cup\mathcal{K}_{E}\), it satisfies \(\rho_{k}=o\left(\frac{\alpha^{2}\sigma_{\mathrm{d}}^{2}d}{n}\right)\) due to (A5). By Lemma C.5 at iterate \(t\in[T_{\mathrm{ERM}},T^{*}]\) with

\[T_{\mathrm{ERM}}:=\max_{\begin{subarray}{c}s\in\{\pm 1\}\\ k\in\mathcal{K}_{R}\cup\mathcal{K}_{E}\end{subarray}}\max_{i\in\mathcal{V}_{s,k }}T_{i}\quad\in\left[\bar{T}_{1},T^{*}\right]\]

the following properties hold if the event \(E_{\mathrm{init}}\) occurs:

* (Learn common features): For \(s\in\{\pm 1\}\) and \(k\in\mathcal{K}_{C}\), \[\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)=\Omega(1),\]* (Overfit (extremely) rare data): For any \(s\in\{\pm 1\}\), \(k\in\mathcal{K}_{R}\cup\mathcal{K}_{E}\), and \(i\in\mathcal{V}_{s,k}\), \[\sum_{p\in[P]\setminus\{p_{i}^{*}\}}\left(\rho_{s}^{(t)}(i,p)+\beta\rho_{-s}^{( t)}(i,p)\right)=\Omega(1),\]
* (Do not learn (extremely) rare features at \(T_{\mathrm{ERM}}\)): For any \(s,s^{\prime}\in\{\pm 1\}\) and \(k\in\mathcal{K}_{R}\cup\mathcal{K}_{E}\), \(\gamma_{s^{\prime}}^{(T_{\mathrm{ERM}})}(s,k)\leq\alpha^{2}\beta^{-1}\).
* For any \(s\in\{\pm 1\},i\in[n],\) and \(p\in[P]\setminus\{p_{i}^{*}\}\), \(\rho_{s}^{(t)}(i,p)=\widetilde{\mathcal{O}}\left(\beta^{-1}\right)\).

#### c.2.4 ERM cannot Learn (extremely) Rare Features Within Polynomial Times

In this step, we will show that ERM cannot learn (extremely) rare features within the maximum admissible iterations \(T^{*}=\frac{\mathrm{poly}(d)}{\eta}\).

From now on, we fix any \(s^{*}\in\{\pm 1\}\) and \(k^{*}\in\mathcal{K}_{R}\cup\mathcal{K}_{E}\). Recall that we defined the set \(\mathcal{W}\) and the function \(Q^{(s^{*},k^{*})}:\mathcal{W}\rightarrow\mathbb{R}^{d\times 2}\) in Lemma B.5. Let us omit superscripts for simplicity. For each iteration \(t\), \(Q(\bm{W}^{(t)})\) represents the cumulative updates contributed by data points with feature vector \(\bm{v}_{s^{*},k^{*}}\) until \(t\)-th iteration. We will sequentially introduce several technical lemmas and by combining these lemmas, quantify update by data with feature vector \(\bm{v}_{s^{*},k^{*}}\) after \(T_{\mathrm{ERM}}\) and derive our conclusion.

Let us define \(\bm{W}^{*}=\{\bm{w}_{1}^{*},\bm{w}_{-1}^{*}\}\), where

\[\bm{w}_{s}^{*}=\bm{w}_{s}^{(T_{\mathrm{ERM}})}+M\sum_{i\in\mathcal{V}_{s^{*}, k^{*}}}\frac{\xi_{i}^{(\tilde{p}_{i})}}{\left\|\xi_{i}^{(\tilde{p}_{i})} \right\|^{2}},\]

for each \(s\in\{\pm 1\}\) with \(M=4\beta^{-1}\log\left(\frac{2\eta\beta^{2}T^{*}}{\alpha^{2}}\right)\). Note that (12), \(\beta<1\), and \(T^{*}=\frac{\mathrm{poly}(d)}{\eta}\) together imply \(M=\widetilde{\mathcal{O}}\left(\beta^{-1}\right)\). Note that \(\bm{W}^{(t)},\bm{W}^{*}\in\mathcal{W}\) for any \(t\geq 0\).

**Lemma C.6**.: _Suppose the event \(E_{\mathrm{init}}\) occurs. Then,_

\[\left\|Q\left(\bm{W}^{(T_{\mathrm{ERM}})}\right)-Q(\bm{W}^{*})\right\|^{2}\leq 1 2M^{2}|\mathcal{V}_{s^{*},k^{*}}|\sigma_{\mathrm{d}}^{-2}d^{-1},\]

_where \(\left\|\cdot\right\|\) denotes the Frobenius norm._

Proof of Lemma c.6.: For each \(s\in\{\pm 1\}\),

\[ss^{*}\left(Q_{s}\left(\bm{w}_{s}^{*}\right)-Q_{s}\left(\bm{w}_{ s}^{(T_{\mathrm{ERM}})}\right)\right)\] \[=Q_{s}\left(ss^{*}M\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\frac{\xi_ {i}^{(\tilde{p}_{i})}}{\left\|\xi_{i}^{(\tilde{p}_{i})}\right\|}\right)\] \[=M\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\frac{\xi_{i}^{(\tilde{p}_ {i})}}{\left\|\xi_{i}^{(\tilde{p}_{i})}\right\|^{2}}+\alpha M\left(\sum_{i\in \mathcal{F}_{s}\cap\mathcal{V}_{s^{*},k^{*}}}\frac{\bm{v}_{s,1}}{\left\|\xi_{ i}^{(\tilde{p}_{i})}\right\|^{2}}+\sum_{i\in\mathcal{F}_{-s}\cap\mathcal{V}_{s^{*},k^{*}}} \frac{\bm{v}_{-s,1}}{\left\|\xi_{i}^{(\tilde{p}_{i})}\right\|^{2}}\right),\]

and we have

\[\left\|Q\left(\bm{W}^{(T_{\mathrm{ERM}})}\right)-Q(\bm{W}^{*}) \right\|^{2}\] \[=\left\|Q_{1}(\bm{w}_{1}^{*})-Q_{1}\left(\bm{w}_{1}^{(T_{\mathrm{ ERM}})}\right)\right\|^{2}+\left\|Q_{-1}(\bm{w}_{-1}^{*})-Q_{-1}\left(\bm{w}_{-1}^{ (T_{\mathrm{ERM}})}\right)\right\|^{2}\] \[\leq 2M^{2}\left(\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\left\|\xi_{i} ^{(\tilde{p}_{i})}\right\|^{-2}+\sum_{i,j\in\mathcal{V}_{s^{*},k^{*}},i\neq j} \frac{\left|\left\langle\xi_{i}^{(\tilde{p}_{i})},\xi_{j}^{(\tilde{p}_{j})} \right\rangle\right|}{\left\|\xi_{i}^{(\tilde{p}_{i})}\right\|^{2}\left\|\xi_ {j}^{(\tilde{p}_{j})}\right\|^{2}}\right)\]\[\leq\sum_{i\in\mathcal{F}_{s}\cap\mathcal{V}_{s^{*},k^{*}}}\left\| \xi_{i}^{(\tilde{p}_{i})}\right\|^{-2}+\sum_{i\in\mathcal{F}_{-s}\cap\mathcal{V} _{s^{*},k^{*}}}\left\|\xi_{i}^{(\tilde{p}_{i})}\right\|^{-2}\] \[=\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\left\|\xi_{i}^{(\tilde{p}_{ i})}\right\|^{-2},\]

where the first inequality is due to \(\alpha<1\) and the second inequality is due to \(\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\left\|\xi_{i}^{(\tilde{p}_{i})}\right\|^{- 2}\leq 2|\mathcal{V}_{s^{*},k^{*}}|\sigma_{\mathrm{d}}^{-2}d^{-1}<1\) from (A5). Hence, from \(E_{\mathrm{init}}\), we obtain

\[\left\|Q\left(\bm{W}^{(T_{\mathrm{ERM}})}\right)-Q(\bm{W}^{*})\right\|^{2}\leq 6 M^{2}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\left\|\xi_{i}^{( \tilde{p}_{i})}\right\|^{-2}\leq 12M^{2}|\mathcal{V}_{s^{*},k^{*}}|\sigma_{ \mathrm{d}}^{-2}d^{-1}.\]

**Lemma C.7**.: _Suppose the \(E_{\mathrm{init}}\) occurs. For any \(t\geq T_{\mathrm{ERM}}\) and \(i\in\mathcal{V}_{s^{*},k^{*}}\), it holds that_

\[\left\langle y_{i}\nabla_{\bm{W}}f_{\bm{W}^{(t)}}(\bm{X}_{i}),Q(\bm{W}^{*}) \right\rangle\geq\frac{M\beta}{2}.\]

Proof of Lemma c.7.: We have

\[\left\langle y_{i}\nabla_{\bm{W}}f_{\bm{W}^{(t)}}(\bm{X}_{i}),Q(\bm{W}^{*})\right\rangle\]

For any \(s\in\{\pm 1\}\) and \(p\in[P]\setminus\{p_{i}^{*},\tilde{p}_{i}\}\),

\[ss^{*}\left\langle Q_{s}(\bm{w}_{s}^{*}),\xi_{i}^{(p)}\right\rangle\]

\[=\rho_{s}^{(T_{\mathrm{ERM}})}(i,p)+\sum_{\begin{subarray}{c}j\in\mathcal{V} _{s^{*},k^{*}},q\in[P]\setminus\{p_{j}^{*}\}\\ (j,q)\neq(i,p)\end{subarray}}\rho_{s}^{(T_{\mathrm{ERM}})}(j,q)\frac{\left\langle \xi_{i}^{(p)},\xi_{j}^{(q)}\right\rangle}{\left\|\xi_{j}^{(q)}\right\|^{2}}+ \sum_{j\in\mathcal{V}_{s^{*},k^{*}}}M\frac{\left\langle\xi_{i}^{(p)},\xi_{j}^ {(\tilde{p}_{j})}\right\rangle}{\left\|\xi_{j}^{(\tilde{p}_{j})}\right\|^{2}}\]\[\geq-\widetilde{\mathcal{O}}\left(nP\beta^{-1}\sigma_{\mathrm{d}} \sigma_{\mathrm{b}}^{-1}d^{-\frac{1}{2}}\right)-\widetilde{\mathcal{O}}\left( nM\sigma_{\mathrm{b}}\sigma_{\mathrm{d}}^{-1}d^{-\frac{1}{2}}\right)\] \[=-o\left(\frac{1}{\mathrm{polylog}(d)}\right),\] (16)

where the last equality is due to (9) and \(M=\widetilde{\mathcal{O}}\left(\beta^{-1}\right)\). Also, for any \(s\in\{\pm 1\}\), \(ss^{*}\left\langle Q_{s}(\bm{w}_{s}^{*}),\bm{v}_{s^{*},k^{*}}\right\rangle= \gamma_{s}^{(T_{\mathrm{EMM}})}(s^{*},k^{*})\geq 0\). In addition,

\[ss^{*}\left\langle Q_{s}(\bm{w}_{s}^{*}),\bm{x}_{i}^{(\tilde{p} _{i})}\right\rangle\] \[=ss^{*}\left\langle Q_{s}(\bm{w}_{s}^{*}),\xi_{i}^{(\tilde{p}_{i })}\right\rangle+ss^{*}\left\langle Q_{s}(\bm{w}_{s}^{*}),\bm{x}_{i}^{(\tilde {p}_{i})}-\xi_{i}^{(\tilde{p}_{i})}\right\rangle\] \[\geq ss^{*}\left\langle Q_{s}(\bm{w}_{s}^{*}),\xi_{i}^{(\tilde{p} _{i})}\right\rangle-\widetilde{\mathcal{O}}\left(\alpha^{2}\beta^{-1}\rho_{k^{ *}}n\sigma_{\mathrm{d}}^{-2}d^{-1}\right)\] \[=M+\rho_{s}^{(T_{\mathrm{EMM}})}(i,\tilde{p}_{i})+\sum_{ \begin{subarray}{c}j\in\mathcal{V}_{s^{*},k^{*}},q\in[P]\setminus\{p_{s}^{*} \}\\ (j,q)\neq(i,\tilde{p}_{i})\end{subarray}}\rho_{s}^{(T_{\mathrm{EMM}})}(j,q) \frac{\left\langle\xi_{i}^{(\tilde{p}_{i})},\xi_{j}^{(q)}\right\rangle}{\left \lVert\xi_{j}^{(q)}\right\rVert^{2}}\] \[\quad+\sum_{j\in\mathcal{V}_{s^{*},k^{*}}\setminus\{i\}}M\frac{ \left\langle\xi_{i}^{(\tilde{p}_{i})},\xi_{j}^{(\tilde{p}_{j})}\right\rangle} {\left\lVert\xi_{j}^{(\tilde{p}_{j})}\right\rVert^{2}}-\widetilde{\mathcal{O} }\left(\alpha^{2}\beta^{-1}\rho_{k^{*}}n\sigma_{\mathrm{d}}^{-2}d^{-1}\right)\] \[\geq M-\widetilde{\mathcal{O}}\left(nP\beta^{-1}\sigma_{\mathrm{d }}\sigma_{\mathrm{b}}^{-1}d^{-\frac{1}{2}}\right)-\widetilde{\mathcal{O}} \left(\alpha^{2}\beta^{-1}\rho_{k^{*}}n\sigma_{\mathrm{d}}^{-2}d^{-1}\right)\] \[=M-o\left(\frac{1}{\mathrm{polylog}(d)}\right)\] \[\geq\frac{M}{2},\] (17)

where the first inequality is due to the definition of \(Q\) and the second-to-last line is due to (9) and (A7).

Hence, applying (16) and (17) for \(s=s^{*},-s^{*}\) and combining with \(\phi^{\prime}\geq\beta\), we have

\[\left\langle y_{i}\nabla_{\bm{W}}f_{\bm{W}^{(t)}}(\bm{X}_{i}),Q(\bm{W}^{*}) \right\rangle\geq M\beta-o\left(\frac{1}{\mathrm{polylog}(d)}\right)\geq\frac {M\beta}{2}.\]

By combining Lemma C.6 and Lemma C.7, we can obtain the following result.

**Lemma C.8**.: _Suppose the event \(E_{\mathrm{init}}\) occurs._

\[\frac{\eta}{n}\sum_{t=T_{\mathrm{EMM}}}^{T^{*}}\sum_{i\in\mathcal{V}_{s^{*},k ^{*}}}\ell\left(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i})\right)\leq\left\lVert Q\left( \bm{W}^{(T_{\mathrm{EMM}})}\right)-Q(\bm{W}^{*})\right\rVert^{2}+2\eta T^{*}e^ {-\frac{M\beta}{4}},\]

_where \(\left\lVert\cdot\right\rVert\) denotes the Frobenius norm._

Proof of Lemma c.8.: Note that for any \(T_{\mathrm{ERM}}\leq t<T^{*}\),

\[Q\left(\bm{W}^{(t+1)}\right)=Q\left(\bm{W}^{(t)}\right)-\frac{\eta}{n}\nabla_{ \bm{W}}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\ell\left(y_{i}f_{\bm{W}^{(t)}}(\bm {X}_{i})\right),\]

and thus

\[\left\lVert Q\left(\bm{W}^{(t)}\right)-Q\left(\bm{W}^{*}\right) \right\rVert^{2}-\left\lVert Q\left(\bm{W}^{(t+1)}\right)-Q\left(\bm{W}^{*} \right)\right\rVert^{2}\] \[=\frac{2\eta}{n}\left\langle\nabla_{\bm{W}}\sum_{i\in\mathcal{V}_ {s^{*},k^{*}}}\ell\left(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i})\right),Q\left(\bm{W }^{(t)}\right)-Q\left(\bm{W}^{*}\right)\right\rangle-\frac{\eta^{2}}{n^{2}} \left\lVert\nabla_{\bm{W}}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\ell\left(y_{i}f _{\bm{W}^{(t)}}(\bm{X}_{i})\right)\right\rVert^{2}\]\[=\frac{2\eta}{n}\left\langle\nabla_{\bm{W}}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\ell(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i})),Q\left(\bm{W}^{(t)}\right)\right\rangle\] \[\quad-\frac{2\eta}{n}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\ell^{ \prime}(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i}))\left\langle\nabla_{\bm{W}}y_{i}f_{ \bm{W}^{(t)}}(\bm{X}_{i}),Q\left(\bm{W}^{*}\right)\right\rangle-\frac{\eta^{2} }{n^{2}}\left\|\nabla_{\bm{W}}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\ell\left(y_ {i}f_{\bm{W}^{(t)}}(\bm{X}_{i})\right)\right\|^{2}\] \[\geq\frac{2\eta}{n}\left\langle\nabla_{\bm{W}}\sum_{i\in\mathcal{V }_{s^{*},k^{*}}}\ell(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i})),Q\left(\bm{W}^{(t)} \right)\right\rangle\] \[\quad-\frac{M\beta\eta}{n}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}} \ell^{\prime}(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i}))-\frac{\eta^{2}}{n^{2}}\left\| \nabla_{\bm{W}}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\ell\left(y_{i}f_{\bm{W}^ {(t)}}(\bm{X}_{i})\right)\right\|^{2},\]

where the last inequality is due to Lemma C.7. By the chain rule, we have

\[\left\langle\nabla_{\bm{W}}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}} \ell(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i})),Q\left(\bm{W}^{(t)}\right)\right\rangle\] \[=\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\Bigg{[}\ell^{\prime}(y_{i}f _{\bm{W}^{(t)}}(\bm{X}_{i}))\] \[\quad\times\sum_{p\in[P]}\Bigg{(}\phi^{\prime}\left(\left\langle \bm{w}_{s^{*}}^{(t)},\bm{x}_{i}^{(p)}\right\rangle\right)\left\langle Q_{s^{* }}\left(\bm{w}_{s^{*}}^{(t)}\right),\bm{x}_{i}^{(p)}\right\rangle-\phi^{ \prime}\left(\left\langle\bm{w}_{-s^{*}}^{(t)},\bm{x}_{i}^{(p)}\right\rangle \right)\left\langle Q_{-s^{*}}\left(\bm{w}_{-s^{*}}^{(t)}\right),\bm{x}_{i}^{( p)}\right\rangle\Big{)}\Bigg{]}.\]

For each \(s\in\{\pm 1\}\), \(i\in\mathcal{V}_{s^{*},k^{*}}\), and \(p\in[P]\),

\[\left|\left\langle\bm{w}_{s}^{(t)},\bm{x}_{i}^{(p)}\right\rangle -\left\langle Q_{s}\left(\bm{w}_{s}^{(t)}\right),\bm{x}_{i}^{(p)}\right\rangle\right|\] \[=\left|\left\langle\bm{w}_{s}^{(t)}-Q_{s}\left(\bm{w}_{s}^{(t)} \right),\bm{x}_{i}^{(p)}\right\rangle\right|\] \[\leq\sum_{j\in[n]\setminus\mathcal{V}_{s^{*},k^{*}},q\in[P] \setminus\{p_{s}^{*}\}}\left|\left\langle\rho_{s}^{(t)}(j,q)\frac{\xi_{j}^{(q) }}{\left\|\xi_{j}^{(q)}\right\|^{2}},\bm{x}_{i}^{(p)}\right\rangle\right|\] \[\quad+\alpha\sum_{j\in\mathcal{F}_{1}\setminus\mathcal{V}_{s^{*}, k^{*}}}\rho_{s}^{(t)}(j,\tilde{p}_{j})\left\|\xi_{j}^{(\tilde{p}_{j})}\right\|^{-2} \left|\left\langle\bm{v}_{1,1},\bm{x}_{i}^{(p)}\right\rangle\right|\] \[\leq\widetilde{\mathcal{O}}\left(nP\beta^{-1}\sigma_{\mathrm{d}} \sigma_{\mathrm{b}}^{-1}d^{-\frac{1}{2}}\right)+\widetilde{\mathcal{O}}\left( \alpha^{2}\beta^{-1}n\sigma_{\mathrm{d}}^{-2}d^{-1}\right)\] \[=o\left(\frac{1}{\mathrm{polylog}(d)}\right),\]

where the last inequality is due to Lemma C.1 and the event \(E_{\mathrm{init}}\). By Lemma F.1,

\[\sum_{p\in[P]}\Bigg{(}\phi^{\prime}\left(\left\langle\bm{w}_{s^{ *}}^{(t)},\bm{x}_{i}^{(p)}\right\rangle\right)\left\langle Q_{s^{*}}\left(\bm{ w}_{s^{*}}^{(t)}\right),\bm{x}_{i}^{(p)}\right\rangle-\phi^{\prime}\left(\left\langle \bm{w}_{-s^{*}}^{(t)},\bm{x}_{i}^{(p)}\right\rangle\right)\left\langle Q_{-s^ {*}}\left(\bm{w}_{s^{*}}^{(t)}\right),\bm{x}_{i}^{(p)}\right\rangle\bigg{)}\] \[\leq\sum_{p\in[P]}\Bigg{(}\phi\left(\left\langle\bm{w}_{s^{*}}^{ (t)},\bm{x}_{i}^{(p)}\right\rangle\right)-\phi\left(\left\langle\bm{w}_{-s^{*} }^{(t)},\bm{x}_{i}^{(p)}\right\rangle\right)\bigg{)}+rP+o\left(\frac{1}{ \mathrm{polylog}(d)}\right)\] \[=y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i})+o\left(\frac{1}{\mathrm{polylog }(d)}\right)\]

where the last equality is due to \(r=o\left(\frac{1}{\mathrm{polylog}(d)}\right)\). Therefore, we have \[\geq\frac{2\eta}{n}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\ell^{\prime} \left(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i})\right)\left(y_{i}f_{\bm{W}^{(t)}}(\bm{X} _{i})+o\left(\frac{1}{\mathrm{polylog}(d)}\right)-\frac{M\beta}{2}\right)\] \[\geq\frac{2\eta}{n}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\ell^{ \prime}(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i}))\left(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{ i})-\frac{M\beta}{4}\right)\] \[\geq\frac{2\eta}{n}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\ell^{ \prime}(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i}))\left(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{ i})-\frac{M\beta}{4}\right)\] \[\quad-\frac{\eta^{2}}{n^{2}}\left\|\nabla_{\bm{W}}\sum_{i\in \mathcal{V}_{s^{*},k^{*}}}\ell(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i}))\right\|^{2}.\]

From the convexity of \(\ell(\cdot)\),

\[\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\ell^{\prime}(y_{i}f_{\bm{W} ^{(t)}}(\bm{X}_{i}))\left(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i})-\frac{M\beta}{4} \right) \geq\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\left(\ell(y_{i}f_{\bm{W} ^{(t)}}(\bm{X}_{i}))-\ell\left(\frac{M\beta}{4}\right)\right)\] \[\geq\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\ell(y_{i}f_{\bm{W}^{(t)} }(\bm{X}_{i}))-ne^{-\frac{M\beta}{4}}.\]

In addition, by Lemma F.2,

\[\frac{\eta^{2}}{n^{2}}\left\|\nabla\sum_{i\in\mathcal{V}_{s^{*}, k^{*}}}\ell\left(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i})\right)\right\|^{2} \leq\frac{8\eta^{2}P^{2}\sigma_{d}^{2}d|\mathcal{V}_{s^{*},k^{*}} |}{n^{2}}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\ell(y_{i}f_{\bm{W}^{(t)}}(\bm{X} _{i}))\] \[\leq\frac{\eta}{n}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\ell(y_{i}f _{\bm{W}^{(t)}}(\bm{X}_{i})),\]

where the last inequality is due to (A8), and we have

\[\left\|Q\left(\bm{W}^{(t)}\right)-Q(\bm{W}^{*})\right\|^{2}-\left\| Q\left(\bm{W}^{(t+1)}\right)-Q(\bm{W}^{*})\right\|^{2}\] \[\geq\frac{\eta}{n}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\ell(y_{i}f _{\bm{W}^{(t)}}(\bm{X}_{i}))-2\eta e^{-\frac{M\beta}{4}}.\]

From telescoping summation, we have

\[\frac{\eta}{n}\sum_{t=T_{\mathrm{EM}}}^{T^{*}}\sum_{i\in\mathcal{V}_{s^{*},k^{ *}}}\ell\left(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i})\right)\leq\left\|Q\left(\bm{W }^{(T_{\mathrm{EM}})}\right)-Q\left(\bm{W}^{*}\right)\right\|^{2}+2\eta T^{*} e^{-\frac{M\beta}{4}}.\]

Finally, we can prove that the model cannot learn (extremely) rare features within \(T^{*}\) iterations.

**Lemma C.9**.: _Suppose the event \(E_{\mathrm{init}}\) occurs. For any \(T\in[T_{\mathrm{ERM}},T^{*}]\), we have \(\gamma_{s}^{(T)}(s^{*},k^{*})=\widetilde{\mathcal{O}}\left(\alpha^{2}\beta^{- 2}\right)\) for each \(s\in\{\pm 1\}\)._

Proof of Lemma c.9.: For any \(T\in[T_{\mathrm{ERM}},T^{*}]\), we have

\[\gamma_{s}^{(T)}(s,k) =\gamma_{s}^{(T_{\mathrm{ERM}})}(s^{*},k^{*})+\frac{\eta}{n}\sum_{ t=T_{\mathrm{ERM}}}^{T-1}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}g_{i}^{(t)}\phi^{ \prime}\left(\left\langle\bm{w}_{s}^{(t)},\bm{v}_{s^{*},k^{*}}\right\rangle\right)\] \[\leq\gamma_{s}^{(T_{\mathrm{ERM}})}(s^{*},k^{*})+\frac{\eta}{n} \sum_{t=T_{\mathrm{ERM}}}^{T-1}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}g_{i}^{(t)}\]\[\leq\gamma_{s}^{(T_{\text{ERM}})}(s^{*},k^{*})+\frac{\eta}{n}\sum_{t=T_{\text{ ERM}}}^{T-1}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\ell\left(y_{i}f_{\bm{W}^{(t)}}( \bm{X}_{i})\right),\]

where the first inequality is due to \(\phi^{\prime}\leq 1\) and the second inequality is due to \(-\ell^{\prime}\leq\ell\). From the result of Section C.2.3 we know \(\gamma_{s}^{(T_{\text{ERM}})}(s^{*},k^{*})\leq\alpha^{2}\beta^{-1}\). Additionally, by Lemma C.8 and Lemma C.6, we have

\[\frac{\eta}{n}\sum_{t=T_{\text{ERM}}}^{(T-1)}\sum_{i\in\mathcal{V} _{s^{*},k^{*}}}\ell\left(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i})\right) \leq\frac{\eta}{n}\sum_{t=T_{\text{ERM}}}^{(T^{*})}\sum_{i\in \mathcal{V}_{s^{*},k^{*}}}\ell\left(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i})\right)\] \[\leq\left\|Q\left(\bm{W}^{(T_{\text{ERM}})}\right)-Q(\bm{W}^{*}) \right\|^{2}+2\eta T^{*}e^{-\frac{M\beta}{4}}\] \[\leq 12M^{2}|\mathcal{V}_{s^{*},k^{*}}|\sigma_{\text{d}}^{-2}d^{ -1}+2\eta T^{*}e^{-\frac{M\beta}{4}}\] \[=\widetilde{\mathcal{O}}\left(\alpha^{2}\beta^{-2}\right).\]

The last line is due to (A5) and our choice \(M=4\beta^{-1}\log\left(\frac{2\eta\beta^{2}T^{*}}{\alpha^{2}}\right)\). Thus, we have our conclusion. 

What We Have So Far.Suppose the event \(E_{\text{init}}\) occurs. For any \(t\in[T_{\text{ERM}},T^{*}]\), we have

* (Learn common features): For each \(s\in\{\pm 1\}\) and \(k\in\mathcal{K}_{C}\), \[\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)=\Omega(1).\]
* (Overfit (extremely) rare data): For each \(s\in\{\pm 1\},k\in\mathcal{K}_{R}\cup\mathcal{K}_{E}\) and \(i\in\mathcal{V}_{s,k}\), \[\sum_{p\in[P]\setminus\{p_{i}^{*}\}}\left(\rho_{s}^{(t)}(i,p)+\beta\rho_{-s} ^{(t)}(i,p)\right)=\Omega(1).\]
* (Cannot learn (extremely) rare features): For each \(s\in\{\pm 1\}\) and \(k\in\mathcal{K}_{R}\cup\mathcal{K}_{E}\), \[\gamma_{s}^{(t)}(s,k),\gamma_{-s}^{(t)}(s,k)=\mathcal{O}\left(\alpha^{2}\beta ^{-2}\right).\]
* For any \(s\in\{\pm 1\},i\in[n],\) and \(p\in[P]\setminus\{p_{i}^{*}\},\rho_{s}^{(t)}(i,p)=\widetilde{\mathcal{O}} \left(\beta^{-1}\right)\),

#### c.2.5 Train and Test Accuracy

In this step, we will prove that the model trained by ERM has perfect training accuracy but has near-random guesses on (extremely) rare data.

For any \(i\in\mathcal{V}_{s,k}\) with \(s\in\{\pm 1\}\) and \(k\in\mathcal{K}_{C}\), by Lemma B.4, we have

\[y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i})\] \[=\sum_{p\in[P]}\left(\phi\left(\left\langle\bm{w}_{s}^{(t)},\bm{x} _{i}^{(p)}\right\rangle\right)-\phi\left(\left\langle\bm{w}_{-s}^{(t)},\bm{x} _{i}^{(p)}\right\rangle\right)\right)\] \[\geq\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)+\sum_{p\in[P ]\setminus\{p_{i}^{*}\}}\left(\rho_{s}^{(t)}(i,p)+\beta\rho_{-s}^{(t)}(i,p) \right)-2P\cdot o\left(\frac{1}{\operatorname{polylog}(d)}\right)\] \[\geq\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)-o\left( \frac{1}{\operatorname{polylog}(d)}\right)\] \[=\Omega(1)-o\left(\frac{1}{\operatorname{polylog}(d)}\right)\] \[>0,\]

for any \(t\in[T_{\text{ERM}},T^{*}]\). In addition, for any \(i\in\mathcal{V}_{s,k}\) with \(s\in\{\pm 1\}\) and \(k\in\mathcal{K}_{R}\cup\mathcal{K}_{E}\), we have

\[y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i})\]\[=\sum_{p\in[P]}\left(\phi\left(\left\langle\bm{w}_{s}^{(t)},\bm{x}_{i} ^{(p)}\right\rangle\right)-\phi\left(\left\langle\bm{w}_{-s}^{(t)},\bm{x}_{i}^{( p)}\right\rangle\right)\right)\] \[=\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)+\sum_{p\in[P] \setminus\{p_{i}^{*}\}}\left(\rho_{s}^{(t)}(i,p)+\beta\rho_{-s}^{(t)}(i,p) \right)-2P\cdot o\left(\frac{1}{\mathrm{polylog}(d)}\right)\] \[\geq\sum_{p\in[P]\setminus\{p_{i}^{*}\}}\left(\rho_{s}^{(t)}(i,p) +\beta\rho_{-s}^{(t)}(i,p)\right)-o\left(\frac{1}{\mathrm{polylog}(d)}\right)\] \[=\Omega(1)-o\left(\frac{1}{\mathrm{polylog}(d)}\right)\] \[>0,\]

for any \(t\in[T_{\mathrm{ERM}},T^{*}]\). We can conclude that \(\mathsf{ERM}\) with \(t\in[T_{\mathrm{ERM}},T^{*}]\) iterates achieve perfect training accuracy.

Next, let us move on to the test accuracy part. Let \((\bm{X},y)\sim\mathcal{D}\) be a test data with \(\bm{X}=\left(\bm{x}^{(1)},\ldots,\bm{x}^{(P)}\right)\in\mathbb{R}^{d\times P}\) having feature patch index \(p^{*}\), dominant noise patch index \(\tilde{p}\), and feature vector \(\bm{v}_{y,k}\). We have \(\bm{x}^{(p)}\sim N(\bm{0},\sigma_{\mathrm{b}}^{2}\bm{\Lambda})\) for each \(p\in[P]\setminus\{p^{*},\tilde{p}\}\) and \(\bm{x}^{(\tilde{p})}-\alpha\bm{v}_{s,1}\sim N(\bm{0},\sigma_{\mathrm{d}}^{2} \bm{\Lambda})\) for some \(s\in\{\pm 1\}\). Therefore, for all \(t\in[T_{\mathrm{ERM}},T^{*}]\) and \(p\in[\tilde{P}]\setminus\{p^{*},\tilde{p}\}\),

\[\left|\phi\left(\left\langle\bm{w}_{1}^{(t)},\bm{x}^{(p)} \right\rangle\right)-\phi\left(\left\langle\bm{w}_{-1}^{(t)},\bm{x}^{(p)} \right\rangle\right)\right|\] \[\leq\left|\left\langle\bm{w}_{1}^{(t)}-\bm{w}_{-1}^{(t)},\bm{x}^ {(p)}\right\rangle\right|\] \[\leq\left|\left\langle\bm{w}_{1}^{(t)}-\bm{w}_{-1}^{(0)},\bm{x}^ {(p)}\right\rangle\right|+\sum_{i\in[n],q\in[P]\setminus\{p_{i}^{*}\}}\left| \rho_{1}^{(t)}(i,q)-\rho_{-1}^{(t)}(i,q)\right|\frac{\left|\left\langle\xi_{i }^{(q)},\bm{x}^{(p)}\right\rangle\right|}{\left\|\xi_{i}^{(q)}\right\|^{2}}\] \[\leq\widetilde{\mathcal{O}}\left(\sigma_{0}\sigma_{\mathrm{b}}d^{ \frac{1}{2}}\right)+\widetilde{\mathcal{O}}\left(nP\beta^{-1}\sigma_{\mathrm{d }}\sigma_{\mathrm{b}}^{-1}d^{-\frac{1}{2}}\right)\] \[=o\left(\frac{\alpha}{\mathrm{polylog}(d)}\right),\] (18)

with probability at least \(1-o\left(\frac{1}{\mathrm{polylog}(d)}\right)\) due to Lemma B.2, (A8), (8), and (9). In addition, for any \(s^{\prime}\in\{\pm 1\}\), we have

\[\left|\left\langle\bm{w}_{s^{\prime}}^{(t)},\bm{x}^{(\tilde{p})} -\alpha\bm{v}_{s,1}\right\rangle\right|\] \[\leq\left|\left\langle\bm{w}_{s^{\prime}}^{(0)},\bm{x}^{(\tilde{p}) }-\alpha\bm{v}_{s,1}\right\rangle\right|+\sum_{i\in[n],q\in[P]\setminus\{p_{i }^{*}\}}\rho_{s^{\prime}}^{(t)}(i,q)\frac{\left|\left\langle\xi_{i}^{(q)},\bm{x }^{(\tilde{p})}-\alpha\bm{v}_{s,1}\right\rangle\right|}{\left\|\xi_{i}^{(q)} \right\|^{2}}\] \[=\widetilde{\mathcal{O}}\left(\sigma_{0}\sigma_{\mathrm{d}}d^{ \frac{1}{2}}\right)+\widetilde{\mathcal{O}}\left(nP\beta^{-1}\sigma_{\mathrm{d }}\sigma_{\mathrm{b}}^{-1}d^{-\frac{1}{2}}\right)\] \[=o\left(\frac{\alpha}{\mathrm{polylog}(d)}\right),\] (19)

with probability at least \(1-o\left(\frac{1}{\mathrm{polylog}(d)}\right)\) due to Lemma B.2, (A8), (8), and (9).

**Case 1: \(k\in\mathcal{K}_{C}\)**

By Lemma B.2, (A8), and (10),

\[\left|\phi\left(\left\langle\bm{w}_{1}^{(t)},\bm{x}^{(\tilde{p}) }\right\rangle\right)-\phi\left(\left\langle\bm{w}_{-1}^{(t)},\bm{w}^{( \tilde{p})}\right\rangle\right)\right|\] \[\leq\left|\left\langle\bm{w}_{1}^{(t)}-\bm{w}_{-1}^{(t)},\bm{x}^ {(\tilde{p})}\right\rangle\right|\] \[\leq\alpha\left|\left\langle\bm{w}_{1}^{(t)}-\bm{w}_{-1}^{(t)},\bm {v}_{s,1}\right\rangle\right|+\left|\left\langle\bm{w}_{1}^{(t)}-\bm{w}_{-1}^{(t) },\bm{x}^{(p)}-\alpha\bm{v}_{s,1}\right\rangle\right|\]\[\leq\alpha\left(\gamma_{1}^{(t)}(s,1)+\gamma_{-1}^{(t)}(s,1)\right)+ \alpha\left|\left\langle\bm{w}_{1}^{(0)},\bm{v}_{s,1}\right\rangle\right|+ \alpha\left|\left\langle\bm{w}_{-1}^{(0)},\bm{v}_{s,1}\right\rangle\right|+o \left(\frac{1}{\mathrm{polylog}(d)}\right)\] \[\leq\widetilde{\mathcal{O}}\left(\alpha\beta^{-1}\right)+ \widetilde{\mathcal{O}}\left(\alpha\sigma_{0}\right)+o\left(\frac{1}{\mathrm{ polylog}(d)}\right)\] \[=o\left(\frac{1}{\mathrm{polylog}(d)}\right),\] (20)

with probability at least \(1-o\left(\frac{1}{\mathrm{poly}(d)}\right)\). Suppose (18) and (20) holds. By Lemma B.4, we have

\[yf_{\bm{W}^{(t)}}(\bm{X})\] \[=\left(\phi\left(\left\langle\bm{w}_{y}^{(t)},\bm{v}_{y,k} \right\rangle\right)\right.-\phi\left(\left\langle\bm{w}_{-y}^{(t)},\bm{v}_{ y,k}\right\rangle\right)\left.\right)\] \[\quad+\sum_{p\in[P]\setminus\{p^{*}\}}\left(\phi\left(\left\langle \bm{w}_{y}^{(t)},\bm{x}^{(p)}\right\rangle\right)-\phi\left(\left\langle\bm{ w}_{-y}^{(t)},\bm{x}^{(p)}\right\rangle\right)\right)\] \[=\gamma_{y}^{(t)}(y,k)+\beta\gamma_{-y}^{(t)}(y,k)-o\left(\frac{1 }{\mathrm{polylog}(d)}\right)\] \[=\Omega(1)-o\left(\frac{1}{\mathrm{polylog}(d)}\right)\] \[>0.\]

Therefore, we have

\[\mathbb{P}_{(\bm{X},y)\sim\mathcal{D}}\left[yf_{\bm{W}^{(t)}}(\bm{X})>0\mid\bm {x}^{(p^{*})}=\bm{v}_{y,k},k\in\mathcal{K}_{C}\right]\geq 1-o\left(\frac{1}{ \mathrm{poly}(d)}\right).\] (21)

Case 2: \(k\in\mathcal{K}_{R}\cup\mathcal{K}_{E}\)By triangular inequality and \(\phi^{\prime}\leq 1\), we have

\[\phi\left(\left\langle\bm{w}_{s}^{(t)},\bm{x}^{(\bar{p})} \right\rangle\right)-\phi\left(\left\langle\bm{w}_{-s}^{(t)},\bm{x}^{(\bar{p} )}\right\rangle\right)\] \[=\phi\left(\left\langle\bm{w}_{s}^{(t)},\alpha\bm{v}_{s,1} \right\rangle\right)-\phi\left(\left\langle\bm{w}_{-s}^{(t)},\alpha\bm{v}_{s,1 }\right\rangle\right)\] \[\quad+\left(\phi\left(\left\langle\bm{w}_{s}^{(t)},\bm{x}^{( \bar{p})}\right\rangle\right)-\phi\left(\left\langle\bm{w}_{s}^{(t)},\alpha \bm{v}_{s,1}\right\rangle\right)\right)-\left(\phi\left(\left\langle\bm{w}_{- s}^{(t)},\bm{x}^{(\bar{p})}\right\rangle\right)-\phi\left(\left\langle\bm{w}_{- s}^{(t)},\alpha\bm{v}_{s,1}\right\rangle\right)\right)\] \[\geq\phi\left(\left\langle\bm{w}_{s}^{(t)},\alpha\bm{v}_{s,1} \right\rangle\right)-\phi\left(\left\langle\bm{w}_{-s}^{(t)},\alpha\bm{v}_{s,1}\right\rangle\right)\] \[\quad-\left|\left\langle\bm{w}_{s}^{(t)},\bm{x}^{(\bar{p})}- \alpha\bm{v}_{s,1}\right\rangle\right|-\left|\left\langle\bm{w}_{-s}^{(t)}, \bm{x}^{(\bar{p})}-\alpha\bm{v}_{s,1}\right\rangle\right|.\]

In addition,

\[\phi\left(\left\langle\bm{w}_{s}^{(t)},\alpha\bm{v}_{s,1}\right \rangle\right)-\phi\left(\left\langle\bm{w}_{-s}^{(t)},\alpha\bm{v}_{s,1} \right\rangle\right)\] \[=\left(\phi\left(\alpha\gamma_{s}^{(t)}(s,1)\right)-\phi\left(- \alpha\gamma_{-s}^{(t)}(s,1)\right)\right)\] \[\quad+\left(\phi\left(\left\langle\bm{w}_{s}^{(t)},\alpha\bm{v}_{ s,1}\right\rangle\right)-\phi\left(\alpha\gamma_{s}^{(t)}(s,1)\right)\right)\] \[\quad-\left(\phi\left(\left\langle\bm{w}_{-s}^{(t)},\alpha\bm{v}_{ s,1}\right\rangle\right)-\phi\left(-\alpha\gamma_{-s}^{(t)}(s,1)\right)\right)\] \[\geq\left(\phi\left(\alpha\gamma_{s}^{(t)}(s,1)\right)-\phi\left(- \alpha\gamma_{-s}^{(t)}(s,1)\right)\right)\] \[\quad-\alpha\left|\left\langle\bm{w}_{s}^{(t)},\bm{v}_{s,1}\right \rangle-\gamma_{s}^{(t)}(s,1)\right|-\alpha\left|\left\langle\bm{w}_{-s}^{(t)}, \bm{v}_{s,1}\right\rangle+\gamma_{-s}^{(t)}(s,1)\right|\] \[=\alpha\left(\gamma_{s}^{(t)}(s,1)+\beta\gamma_{-s}^{(t)}(s,1) \right)-\alpha\cdot o\left(\frac{1}{\mathrm{polylog}(d)}\right)\]\[=\Omega(\alpha),\]

where the second equality is due to Lemma B.4 and (A8). If (19) holds, we have

\[\phi\left(\left\langle\bm{w}_{s}^{(t)},\bm{x}^{(\tilde{p})}\right\rangle\right)- \phi\left(\left\langle\bm{w}_{-s}^{(t)},\bm{x}^{(\tilde{p})}\right\rangle\right) =\Omega(\alpha)-o\left(\frac{\alpha}{\mathrm{polylog}(d)}\right)=\Omega(\alpha).\] (22)

Note that

\[yf_{\bm{W}^{(t)}}(\bm{X})\] \[=\phi\left(\left\langle\bm{w}_{y}^{(t)},\bm{v}_{y,k}\right\rangle \right)-\phi\left(\left\langle\bm{w}_{-y}^{(t)},\bm{v}_{y,k}\right\rangle \right)+\phi\left(\left\langle\bm{w}_{y}^{(t)},\bm{x}^{(\tilde{p})}\right\rangle \right)-\phi\left(\left\langle\bm{w}_{-y}^{(t)},\bm{x}^{(\tilde{p})}\right\rangle\right)\] \[\quad+\sum_{p\in[P]\setminus\{p^{*},\tilde{p}\}}\bigg{(}\phi\left( \left\langle\bm{w}_{y}^{(t)},\bm{x}^{(p)}\right\rangle\right)-\phi\left( \left\langle\bm{w}_{-y}^{(t)},\bm{x}^{(p)}\right\rangle\right)\bigg{)},\]

and

\[\left|\phi\left(\left\langle\bm{w}_{y}^{(t)},\bm{v}_{y,k}\right\rangle \right)-\phi\left(\left\langle\bm{w}_{-y}^{(t)},\bm{v}_{y,k}\right\rangle \right)\right|\] \[\leq\left|\left\langle\bm{w}_{y}^{(t)}-\bm{w}_{-y}^{(t)},\bm{v}_ {y,k}\right\rangle\right|+o\left(\frac{\alpha}{\mathrm{polylog}(d)}\right)\] \[\leq\gamma_{1}^{(t)}(y,k)+\gamma_{-1}^{(t)}(y,k)+\left|\left\langle \bm{w}_{y}^{(0)}-\bm{w}_{-y}^{(0)},\bm{v}_{y,k}\right\rangle\right|+o\left( \frac{\alpha}{\mathrm{polylog}(d)}\right)\] \[\leq\mathcal{O}(\alpha^{2}\beta^{-2})+\widetilde{\mathcal{O}}( \sigma_{0})+o\left(\frac{\alpha}{\mathrm{polylog}(d)}\right)\] \[=o\left(\frac{\alpha}{\mathrm{polylog}(d)}\right)\] \[<\phi\left(\left\langle\bm{w}_{s}^{(t)},\bm{x}^{(\tilde{p})} \right\rangle\right)-\phi\left(\left\langle\bm{w}_{-s}^{(t)},\bm{x}^{(\tilde {p})}\right\rangle\right),\]

where the first inequality is due to (18), second-to-last line is due to (A8), (8) and (10), and the last inequality is due to (22). Therefore, we have \(yf_{\bm{W}^{(t)}}(\bm{X})>0\) if \(y=s\). Otherwise, \(yf_{\bm{W}^{(t)}}(\bm{X})<0\). Therefore, we have

\[\mathbb{P}_{(\bm{X},y)\sim\mathcal{D}}\left[yf_{\bm{W}^{(t)}}(\bm{X})>0\mid\bm {x}^{(p^{*})}=\bm{v}_{y,k},k\in\mathcal{K}_{R}\cup\mathcal{K}_{E}\right]=\frac {1}{2}\pm o\left(\frac{1}{\mathrm{poly}(d)}\right).\] (23)

Hence, combining (21) and (23) implies

\[\mathbb{P}_{(\bm{X},y)\sim\mathcal{D}}\left[yf_{\bm{W}^{(t)}}(\bm {X})>0\right] =\sum_{k\in\mathcal{K}_{C}}\rho_{k}+\frac{1}{2}\left(1-\sum_{k\in \mathcal{K}_{C}}\rho_{k}\right)\pm o\left(\frac{1}{\mathrm{poly}(d)}\right)\] \[=1-\frac{1}{2}\sum_{k\in\mathcal{K}_{R}\cup\mathcal{K}_{E}}\rho_{k }\pm o\left(\frac{1}{\mathrm{poly}(d)}\right).\]Proof for Cutout

In this section, we use \(g_{i,\mathcal{C}}^{(t)}:=\frac{1}{1+\exp\left(y_{i}f_{\bm{W}^{(t)}(\bm{X}_{i}, \mathcal{C})}\right)}\) for each data \(i\), \(\mathcal{C}\subset[P]\) with \(|\mathcal{C}|=C\) and iteration \(t\), for simplicity.

### Proof of Lemma b.3 for Cutout

For \(s\in\{\pm 1\}\) and iterate \(t\),

\[\bm{w}_{s}^{(t+1)}-\bm{w}_{s}^{(t)}\] \[=-\eta\nabla_{\bm{w}_{s}}\mathcal{L}_{\text{Cutout}}\left(\bm{W }^{(t)}\right)\] \[=\frac{\eta}{n}\sum_{i\in[n]}sy_{i}\mathbb{E}_{\mathcal{C} \sim\mathcal{D}_{\mathcal{C}}}\left[g_{i,\mathcal{C}}^{(t)}\sum_{p\notin \mathcal{C}}\phi^{\prime}\left(\left\langle\bm{w}_{s}^{(t)},\bm{x}_{i}^{(p)} \right\rangle\right)\bm{x}_{i}^{(p)}\right]\] \[=\frac{\eta}{n}\left(\sum_{i\in\mathcal{V}_{s}}\mathbb{E}_{ \mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}\left[g_{i,\mathcal{C}}^{(t)}\sum_{ p\notin\mathcal{C}}\phi^{\prime}\left(\left\langle\bm{w}_{s}^{(t)},\bm{x}_{i}^{(p)} \right\rangle\right)\bm{x}_{i}^{(p)}\right]\right.\] \[\qquad\qquad\left.-\sum_{i\in\mathcal{V}_{-s}}\mathbb{E}_{ \mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}\left[g_{i,\mathcal{C}}^{(t)}\sum_{ p\notin\mathcal{C}}\phi^{\prime}\left(\left\langle\bm{w}_{s}^{(t)},\bm{x}_{i}^{(p)} \right\rangle\right)\bm{x}_{i}^{(p)}\right]\right),\]

and we have

\[\sum_{i\in\mathcal{V}_{s}}\mathbb{E}_{\mathcal{C}\sim\mathcal{D}_ {\mathcal{C}}}\left[g_{i,\mathcal{C}}^{(t)}\sum_{p\notin\mathcal{C}}\phi^{ \prime}\left(\left\langle\bm{w}_{s}^{(t)},\bm{x}_{i}^{(p)}\right\rangle\right) \bm{x}_{i}^{(p)}\right]\] \[=\sum_{k\in[K]}\sum_{i\in\mathcal{V}_{s,k}}\mathbb{E}_{\mathcal{C }\sim\mathcal{D}_{\mathcal{C}}}\left[g_{i,\mathcal{C}}^{(t)}\phi^{\prime} \left(\left\langle\bm{w}_{s}^{(t)},\bm{v}_{s,k}\right\rangle\right)\cdot \mathbbm{1}_{p;\notin\mathcal{C}}\right]\bm{v}_{s,k}\] \[\quad+\sum_{i\in\mathcal{V}_{s}\cap\mathcal{V}_{s}}\mathbb{E}_{ \mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}\left[g_{i,\mathcal{C}}^{(t)}\phi^{ \prime}\left(\left\langle\bm{w}_{s}^{(t)},\alpha\bm{v}_{-s,1}+\xi_{i}^{(\bar{p }_{i})}\right\rangle\right)\cdot\mathbbm{1}_{p;\notin\mathcal{C}}\right]\left( \alpha\bm{v}_{-s,1}+\xi_{i}^{(\bar{p}_{i})}\right),\]

and

\[\sum_{i\in\mathcal{V}_{-s}}\mathbb{E}_{\mathcal{C}\sim\mathcal{D} _{\mathcal{C}}}\left[g_{i,\mathcal{C}}^{(t)}\sum_{p\notin\mathcal{C}}\phi^{ \prime}\left(\left\langle\bm{w}_{s}^{(t)},\bm{x}_{i}^{(p)}\right\rangle\right) \bm{x}_{i}^{(p)}\right]\] \[=\sum_{k\in[K]}\sum_{i\in\mathcal{V}_{-s,k}}\mathbb{E}_{\mathcal{ C}\sim\mathcal{D}_{\mathcal{C}}}\left[g_{i,\mathcal{C}}^{(t)}\phi^{\prime} \left(\left\langle\bm{w}_{s}^{(t)},\bm{v}_{-s,k}\right\rangle\right)\cdot \mathbbm{1}_{p;\notin\mathcal{C}}\right]\bm{v}_{-s,k}\] \[\quad+\sum_{i\in\mathcal{V}_{-s}\cap\mathcal{V}_{s}}\sum_{p\in[P] \setminus\{p_{i}^{\prime},\bar{p}_{i}\}}\mathbb{E}_{\mathcal{C}\sim\mathcal{D} _{\mathcal{C}}}\left[g_{i,\mathcal{C}}^{(t)}\phi^{\prime}\left(\left\langle\bm {w}_{s}^{(t)},\xi_{i}^{(p)}\right\rangle\right)\cdot\mathbbm{1}_{p\notin \mathcal{C}}\right]\xi_{i}^{(p)}\] \[\quad+\sum_{i\in\mathcal{V}_{-s}\cap\mathcal{V}_{s}}\mathbb{E}_{ \mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}\left[g_{i,\mathcal{C}}^{(t)}\phi^{ \prime}\left(\left\langle\bm{w}_{s}^{(t)},\alpha\bm{v}_{s,1}+\xi_{i}^{(\bar{p} _{i})}\right\rangle\right)\cdot\mathbbm{1}_{\bar{p}_{i}\notin\mathcal{C}}\right] \left(\alpha\bm{v}_{s,1}+\xi_{i}^{(\bar{p}_{i})}\right)\] \[\quad+\sum_{i\in\mathcal{V}_{-s}\cap\mathcal{V}_{-s}}\mathbb{E}_{ \mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}\left[g_{i,\mathcal{C}}^{(t)}\phi^{ \prime}\left(\left\langle\bm{w}_{s}^{(t)},\alpha\bm{v}_{-s,1}+\xi_{i}^{(\bar{p} _{i})}\right\rangle\right)\cdot\mathbbm{1}_{\bar{p}_{i}\notin\mathcal{C}}\right] \left(\alpha\bm{v}_{-s,1}+\xi_{i}^{(\bar{p}_{i})}\right).\]

Hence, if we define \(\gamma_{s}^{(t)}(s^{\prime},k)\)'s and \(\rho_{s}^{(t)}(i,p)\)'s recursively by using the rule

\[\gamma_{s}^{(t+1)}(s^{\prime},k)=\gamma_{s}^{(t)}(s^{\prime},k)+\frac{\eta}{n} \sum_{i\in\mathcal{V}_{s^{\prime},k}}\mathbb{E}_{\mathcal{C}\sim\mathcal{D}_{ \mathcal{C}}}\left[g_{i,\mathcal{C}}^{(t)}\phi^{\prime}\left(\left\langle\bm{w}_{s }^{(t)},\bm{v}_{s^{\prime},k}\right\rangle\right)\cdot\mathbbm{1}_{p;\notin \mathcal{C}}\right],\] (24)\[\rho_{s}^{(t+1)}(i,p)=\rho_{s}^{(t)}(i,p)+\frac{\eta}{n}\mathbb{E}_{\mathcal{C} \sim\mathcal{D}_{\mathcal{C}}}\left[g_{i,\mathcal{C}}^{(t)}\phi^{\prime}\left( \left\langle\bm{w}_{s}^{(t)},\bm{x}_{s}^{(p)}\right\rangle\right)\cdot\mathbb{1 }_{p\notin\mathcal{C}}\right]\left\|\xi_{i}^{(p)}\right\|^{2},\] (25)

starting from \(\gamma_{s}^{(0)}(s^{\prime},k)=\rho_{s}^{(0)}(i,p)=0\) for each \(s,s^{\prime}\in\{\pm 1\},k\in[K],i\in[n]\) and \(p\in[P]\setminus\{p_{i}^{*}\}\), then we have

\[\bm{w}_{s}^{(t)} =\bm{w}_{s}^{(0)}+\sum_{k\in[K]}\gamma_{s}^{(t)}(s,k)\bm{v}_{s,k} -\sum_{k\in[K]}\gamma_{s}^{(t)}(-s,k)\bm{v}_{-s,k}\] \[\quad+\sum_{i\in\mathcal{V}_{s},p\in[P]\setminus\{p_{i}^{*}\}} \rho_{s}^{(t)}(i,p)\frac{\xi_{i}^{(p)}}{\left\|\xi_{i}^{(p)}\right\|^{2}}- \sum_{i\in\mathcal{V}_{-s},p\in[P]\setminus\{p_{i}^{*}\}}\rho_{s}^{(t)}(i,p) \frac{\xi_{i}^{(p)}}{\left\|\xi_{i}^{(p)}\right\|^{2}}\] \[\quad+\alpha\left(\sum_{i\in\mathcal{F}_{s}}sy_{i}\rho_{s}^{(t)}(i,\tilde{p}_{i})\frac{\bm{v}_{s,1}}{\left\|\xi_{i}^{(\tilde{p}_{i})}\right\|^{2 }}+\sum_{i\in\mathcal{F}_{-s}}sy_{i}\rho_{s}^{(t)}(i,\tilde{p}_{i})\frac{\bm{ v}_{-s,1}}{\left\|\xi_{i}^{(\tilde{p}_{i})}\right\|^{2}}\right),\]

for each \(s\in\{\pm 1\}\). Furthermore, \(\gamma_{s}^{(t)}(s^{\prime},k)\)'s and \(\rho_{s}^{(t)}(i,p)\)'s are monotone increasing. 

### Proof of Theorem 3.2

To show Theorem 3.2, we present a structured proof comprising the following five steps:

1. Establish upper bounds on \(\gamma_{s}^{(t)}(s^{\prime},k)\)'s and \(\rho_{s}^{(t)}(i,p)\)'s to apply Lemma B.4 (Section D.2.1).
2. Demonstrate that the model quickly learns common and rare features (Section D.2.2).
3. Show that the model overfits augmented data if it does not contain common or rare features (Section D.2.3).
4. Confirm the persistence of this tendency until \(T^{*}\) iterates (Section D.2.4).
5. Characterize train accuracy and test accuracy (Section D.2.5).

#### d.2.1 Bounds on the Coefficients in Feature Noise Decomposition

The following lemma provides upper bounds on Lemma B.3 during \(T^{*}\) iterations.

**Lemma D.1**.: _Suppose the event \(E_{\mathrm{init}}\) occurs. For any \(0\leq t\leq T^{*}\), we have_

\[0\leq\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)\leq 4\log(\eta T^{*}), \quad 0\leq\rho_{y_{i}}^{(t)}(i,p)+\beta\rho_{-y_{i}}^{(t)}(i,p)\leq 4\log \left(\eta T^{*}\right),\]

_for all \(s\in\{\pm 1\},k\in[K],i\in[n]\) and \(p\in[P]\setminus\{p_{i}^{*}\}\). Consequently, \(\gamma_{s}^{(t)}(s^{\prime},k),\rho_{s}^{(t)}(i,p)=\widetilde{\mathcal{O}}( \beta^{-1})\) for all \(s,s^{\prime}\in\{\pm 1\},k\in[K],i\in[n]\) and \(p\in[P]\setminus\{p_{i}^{*}\}\)._

Proof of Lemma D.1.: The first argument implies the second argument since \(\log(\eta T^{*})=\mathrm{polylog}(d)\) and

\[\gamma_{s}^{(t)}(s^{\prime},k)\leq\beta^{-1}\left(\gamma_{s^{\prime}}^{(t)}(s^ {\prime},k)+\beta\gamma_{s^{\prime}}^{(t)}(s^{\prime},k)\right),\quad\rho_{s}^ {(t)}(i,p)\leq\beta^{-1}\left(\rho_{y_{i}}^{(t)}(i,p)+\beta\rho_{-y_{i}}^{(t)}( i,p)\right),\]

for all \(s,s^{\prime}\in\{\pm 1\},k\in[K],i\in[n]\) and \(p\in[P]\setminus\{p_{i}^{*}\}\).

We will prove the first argument by using induction on \(t\). The initial case \(t=0\) is trivial. Suppose the statement holds at \(t=T\) and consider the case \(t=T+1\).

Let \(\tilde{T}_{s,k}\leq T\) denote the smallest iteration where \(\gamma_{s}^{(\tilde{T}_{s,k}+1)}(s,k)+\beta\gamma_{-s}^{(\tilde{T}_{s,k}+1)}(s, k)>2\log(\eta T^{*})\). We assume the existence of \(\tilde{T}_{s,k}\), as its absence would directly lead to our desired conclusion; to see why, note that the following holds, due to (24) and (11):

\[\gamma_{s}^{(T+1)}(s,k)+\beta\gamma_{-s}^{(T+1)}(s,k)\] \[=\gamma_{s}^{(T)}(s,k)+\beta\gamma_{-s}^{(T)}(s,k)\] \[\quad+\frac{\eta}{n}\sum_{i\in\mathcal{V}_{s,k}}\mathbb{E}_{ \mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}\left[g_{i,\mathcal{C}}^{(T)}\cdot \mathbb{1}_{p_{i}^{*}\notin\mathcal{C}}\right]\left(\phi^{\prime}\left(\left\langle \bm{w}_{s}^{(T)},\bm{v}_{s,k}\right\rangle\right)+\beta\phi^{\prime}\left( \left\langle\bm{w}_{-s}^{(T)},\bm{v}_{s,k}\right\rangle\right)\right)\]\[\leq 2\log(\eta T^{*})+2\eta\leq 4\log(\eta T^{*})\]

By (24), we have

\[\gamma_{s}^{(T+1)}(s,k)+\beta\gamma_{-s}^{(T+1)}(s,k)\] \[=\gamma_{s}^{(\tilde{T}_{s,k})}(s,k)+\beta\gamma_{-s}^{(\tilde{T}_ {s,k})}(s,k)\] \[\quad+\sum_{t=\tilde{T}_{s,k}}^{T}\left(\gamma_{s}^{(t+1)}(s,k)+ \beta\gamma_{-s}^{(t+1)}(s,k)-\gamma_{s}^{(t)}(s,k)-\beta\gamma_{-s}^{(t)}(s,k )\right)\] \[\leq 2\log(\eta T^{*})+\log(\eta T^{*})\]

The inequality is due to \(\gamma_{s}^{(\tilde{T}_{s,k})}(s,k)+\beta\gamma_{-s}^{(\tilde{T}_{s,k})}(s,k) \leq 2\log(\eta T^{*})\) and

\[\frac{\eta}{n}\sum_{i\in\mathcal{V}_{s,k}}\mathbb{E}_{\mathcal{C }\sim\mathcal{D}_{\mathcal{C}}}\left[g_{i,\mathcal{C}}^{(\tilde{T}_{s,k})} \bigg{(}\phi^{\prime}\left(\left<\boldsymbol{w}_{s}^{(\tilde{T}_{s,k})}, \boldsymbol{v}_{s,k}\right>\right)+\beta\phi^{\prime}\left(\left<\boldsymbol{ w}_{-s}^{(\tilde{T}_{s,k})},\boldsymbol{v}_{s,k}\right>\right)\bigg{)}\cdot \mathbbm{1}_{p_{i}^{*}\notin\mathcal{C}}\right]\] \[\leq 2\eta\leq\log(\eta T^{*}),\]

from our choice of \(\tilde{T}_{s,k}\) and \(\eta\).

For each \(t=\tilde{T}_{s,k}+1,\ldots T\), \(i\in\mathcal{V}_{s,k}\), and \(\mathcal{C}\subset[P]\) such that \(|\mathcal{C}|=C\) and \(p_{i}^{*}\notin\mathcal{C}\), we have

\[y_{i}f_{\boldsymbol{W}^{(t)}}(\boldsymbol{X}_{i,\mathcal{C}})\] \[=\phi\left(\left<\boldsymbol{w}_{s}^{(t)},\boldsymbol{v}_{s,k} \right>\right)-\phi\left(\left<\boldsymbol{w}_{-s}^{(t)},\boldsymbol{v}_{s,k} \right>\right)+\sum_{p\notin\mathcal{C}\cup\{p_{i}^{*}\}}\left(\phi\left( \left<\boldsymbol{w}_{s}^{(t)},\boldsymbol{x}_{i}^{(p)}\right>\right)-\phi \left(\left<\boldsymbol{w}_{-s}^{(t)},\boldsymbol{x}_{i}^{(p)}\right>\right)\right)\] \[\geq\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)+\sum_{p \notin\mathcal{C}\cup\{p_{i}^{*}\}}\left(\rho_{s}^{(t)}(i,p)+\beta\rho_{-s}^{ (t)}(i,p)\right)-2P\cdot o\left(\frac{1}{\operatorname{polylog}(d)}\right)\] \[\geq\frac{3}{2}\log(\eta T^{*})\]

The first inequality is due to Lemma B.4 and the second inequality holds due to (A7), (8), and our choice of \(t\), \(\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)\geq 2\log(\eta T^{*})\).

Hence, we obtain

\[\frac{\eta}{n}\sum_{t=\tilde{T}_{s,k}}^{T}\sum_{i\in\mathcal{V}_{ s,k}}\mathbb{E}_{\mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}\left[g_{i,\mathcal{C}}^ {(t)}\bigg{(}\phi^{\prime}\left(\left<\boldsymbol{w}_{s}^{(t)},\boldsymbol{v} _{s,k}\right>\right)+\beta\phi^{\prime}\left(\left<\boldsymbol{w}_{-s}^{(t)}, \boldsymbol{v}_{s,k}\right>\right)\bigg{)}\cdot\mathbbm{1}_{p_{i}^{*}\notin \mathcal{C}}\right]\] \[\leq\frac{2\eta}{n}\sum_{t=\tilde{T}_{s,k}}^{T}\sum_{i\in\mathcal{ V}_{s,k}}\mathbb{E}_{\mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}\left[\exp\left(-y_{i}f_{ \boldsymbol{W}^{(t)}}(\boldsymbol{X}_{i,\mathcal{C}})\right)\cdot\mathbbm{1}_ {p_{i}^{*}\notin\mathcal{C}}\right]\] \[\leq\frac{2|\mathcal{V}_{s,k}|}{n}(\eta T^{*})\exp\left(-\frac{3} {2}\log(\eta T^{*})\right)\] \[\leq\frac{2}{\sqrt{\eta T^{*}}}\leq\log(\eta T^{*}),\]

where the last inequality holds for any reasonably large \(T^{*}\). Merging all inequalities together, we have \(\gamma_{s}^{(T+1)}(s,k)+\beta\gamma_{-s}^{(T+1)}(s,k)\leq 4\log(\eta T^{*})\).

Next, we will follow similar arguments to show that

\[\rho_{y_{i}}^{(T+1)}(i,p)+\beta\rho_{-y_{i}}^{(T+1)}(i,p)\leq 4\log(\eta T^{*})\]

for each \(i\in[n]\) and \(p\in[P]\setminus\{p_{i}^{*}\}\).

Let \(\tilde{T}_{i}^{(p)}\leq T\) be the smallest iteration such that \(\rho_{y_{i}}^{(\tilde{T}_{i}^{(p)}+1)}(i,p)+\beta\rho_{-y_{i}}^{(\tilde{T}_{i}^{ (p)}+1)}(i,p)>2\log(\eta T^{*})\). We assume the existence of \(\tilde{T}_{i}^{(p)}\),, as its absence would directly lead to our desired conclusion; to see why, note that the following holds, due to (25) and (11):

\[\rho_{y_{i}}^{(T+1)}(i,p)+\beta\rho_{-y_{i}}^{(T+1)}(i,p)\] \[=\rho_{y_{i}}^{(T)}(i,p)+\beta\rho_{-y_{i}}^{(T)}(i,p)\] \[\quad+\frac{\eta}{n}\mathbb{E}_{\mathcal{C}\sim\mathcal{D}_{ \mathcal{C}}}\left[g_{i,\mathcal{C}}^{(T)}\cdot\mathbbm{1}_{p\notin\mathcal{C}} \right]\left(\phi^{\prime}\left(\left\langle\bm{w}_{s}^{(t)},\bm{x}_{i}^{(p)} \right\rangle\right)+\beta\phi^{\prime}\left(\left\langle\bm{w}_{s}^{(t)},\bm{ x}_{i}^{(p)}\right\rangle\right)\right)\left\|\xi_{i}^{(p)}\right\|^{2}\] \[\leq 2\log(\eta T^{*})+2\eta\leq 4\log(\eta T^{*}),\]

where the first inequality is due to \(\left\|\xi_{i}^{(p)}\right\|\leq\frac{3}{2}\sigma_{\mathrm{d}}^{2}d\) and (A4), and the last inequality is due to (11).

Now we suppose there exists such \(\tilde{T}_{i}\leq T\). By (25), we have

\[\rho_{y_{i}}^{(T+1)}(i,p)+\beta\rho_{-y_{i}}^{(T+1)}(i,p)\] \[=\rho_{y_{i}}^{(\tilde{T}_{i}^{(p)})}(i,p)+\beta\rho_{-y_{i}}^{( \tilde{T}_{i}^{(p)})}(i,p)\] \[\quad+\sum_{t=\tilde{T}_{i}^{(p)}}^{T}\left(\rho_{y_{i}}^{(t+1)}( i,p)+\beta\rho_{-y_{i}}^{(t+1)}(i,p)-\rho_{y_{i}}^{(t)}(i,p)-\beta\rho_{-y_{i}}^{(t )}(i,p)\right)\] \[\leq 2\log(\eta T^{*})+\log(\eta T^{*})\]

The inequality is due to \(\rho_{y_{i}}^{(t)}(i,p)+\beta\rho_{-y_{i}}^{(t)}(i,p)\leq 2\log(\eta T^{*})\) our choice of \(\tilde{T}_{i}^{(p)}\) and

\[\frac{\eta}{n}\mathbb{E}_{\mathcal{C}\sim\mathcal{D}_{\mathcal{C }}}\left[g_{i,\mathcal{C}}^{(\tilde{T}_{i}^{(p)})}\cdot\mathbbm{1}_{p\notin \mathcal{C}}\right]\left(\phi^{\prime}\left(\left\langle\bm{w}_{s}^{(\tilde{T}_ {i}^{(p)})},\bm{x}_{i}^{(p)}\right\rangle\right)+\beta\phi^{\prime}\left( \left\langle\bm{w}_{-s}^{(t)},\bm{x}_{i}^{(p)}\right\rangle\right)\right) \left\|\xi_{i}^{(p)}\right\|^{2}\] \[\leq 2\eta\leq\log(\eta T^{*}),\]

from \(\left\|\xi_{i}^{(p)}\right\|^{2}\leq\frac{3}{2}\sigma_{\mathrm{d}}^{2}d\), (A4), and (11).

For each \(t=\tilde{T}_{i}^{(p)}+1,\ldots,T\), and \(\mathcal{C}\subset[P]\) such that \(|\mathcal{C}|=C\) and \(p\notin\mathcal{C}\), we have

\[y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i,\mathcal{C}})\] \[=\phi\left(\left\langle\bm{w}_{y_{i}}^{(t)},\bm{x}_{i}^{(p)} \right\rangle\right)-\phi\left(\left\langle\bm{w}_{-y_{i}}^{(t)},\bm{x}_{i}^{( p)}\right\rangle\right)+\sum_{q\notin\mathcal{C}\cup\{p\}}\left(\phi\left(\left\langle \bm{w}_{y_{i}}^{(t)},\bm{x}_{i}^{(q)}\right\rangle\right)-\phi\left(\left\langle \bm{w}_{-y_{i}}^{(t)},\bm{x}_{i}^{(q)}\right\rangle\right)\right)\] \[\geq\rho_{y_{i}}^{(t)}(i,p)+\beta\rho_{-y_{i}}^{(t)}(i,p)-2P\cdot o \left(\frac{1}{\mathrm{polylog}(d)}\right)\] \[\geq\frac{3}{2}\log(\eta T^{*}).\]

The first inequality is due to Lemma B.4 and the second inequality holds since from our choice of \(t\), \(\rho_{y_{i}}^{(t)}(i,p)+\beta\rho_{-y_{i}}^{(t)}(i,p)\geq 2\log(\eta T^{*})\).

Therefore, we have

\[\frac{\eta}{n}\sum_{t=\tilde{T}_{i}^{(p)}+1}^{T}\mathbb{E}_{ \mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}\left[g_{i,\mathcal{C}}^{(t)}\cdot \mathbbm{1}_{p\notin\mathcal{C}}\right]\left(\phi^{\prime}\left(\left\langle\bm {w}_{y_{i}}^{(t)},\bm{x}_{i}^{(p)}\right\rangle\right)+\beta\phi^{\prime} \left(\left\langle\bm{w}_{-y_{i}}^{(t)},\bm{x}_{i}^{(p)}\right\rangle\right) \right)\left\|\xi_{i}^{(p)}\right\|^{2}\] \[\leq\eta\sum_{t=\tilde{T}_{i}^{(p)}+1}^{T}\mathbb{E}_{\mathcal{C} \sim\mathcal{D}_{\mathcal{C}}}\left[\exp\left(-y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i,\mathcal{C}})\right)\mathbbm{1}_{p\notin\mathcal{C}}\right]\leq(\eta T^{*}) \exp\left(-\frac{3}{2}\log(\eta T^{*})\right)\]\[\leq\frac{1}{\sqrt{\eta T^{*}}}\leq\log(\eta T^{*}),\]

where the first inequality is due to \(\left\|\xi_{i}^{(p)}\right\|^{2}\leq\frac{3}{2}\sigma_{\mathrm{d}}^{2}d\) and (A4). Hence, we conclude \(\rho_{y_{i}}^{(T+1)}(i,p)+\beta\rho_{-y_{i}}^{(T+1)}(i,p)\leq 4\log(\eta T^{*})\). 

#### d.2.2 Learning Common Features and Rare Features

In the initial stages of training, the model quickly learns common features while exhibiting minimal overfitting to Gaussian noise.

First, we establish lower bounds on the number of iterations, ensuring that background noise coefficients \(\rho_{s}^{(t)}(i,p)\) for \(p\neq p_{i}^{*},\tilde{p}_{i}\) remain small, up to the order of \(\frac{1}{P}\).

**Lemma D.2**.: _Suppose the event \(E_{\mathrm{init}}\) occurs. There exists \(\tilde{T}>\frac{n}{6\eta P\sigma_{\mathrm{b}}^{2}d}\) such that \(\rho_{s}^{(t)}(i,p)\leq\frac{1}{4P}\) for all \(0\leq t<\tilde{T},s\in\{\pm 1\},i\in[n]\) and \(p\in[P]\setminus\{p_{i}^{*},\tilde{p}_{i}\}\)._

Proof of Lemma d.2.: Let \(\tilde{T}\) be the smallest iteration such that \(\rho_{s}^{(\tilde{T})}(i,p)\geq\frac{1}{4P}\) for some \(s\in\{\pm 1\},i\in[n]\) and \(p\in[P]\setminus\{p_{i}^{*}\}\). We assume the existence of \(\tilde{T}\), as its absence would directly lead to our conclusion. Then, for any \(0\leq t<\tilde{T}\), we have

\[\rho_{s}^{(t+1)}(i,p)=\rho_{s}^{(t)}(i,p)+\frac{\eta}{n}\mathbb{E}_{\mathcal{C }\sim\mathcal{D}_{\mathrm{C}}}\left[g_{i,\mathcal{C}}^{(t)}\phi^{\prime} \left(\left\langle\bm{w}_{s}^{(t)},\bm{x}_{i}^{(p)}\right\rangle\right)\cdot \mathbbm{1}_{p\notin\mathcal{C}}\right]\left\|\xi_{i}^{(p)}\right\|^{2}<\rho_{s }^{(t)}(i,p)+\frac{3\eta\sigma_{\mathrm{b}}^{2}d}{2n},\]

where the inequality is due to \(g_{i,\mathcal{C}}^{(t)}<1\), \(\phi^{\prime}\leq 1\), and \(\left\|\xi_{i}^{(p)}\right\|^{2}\leq\frac{3}{2}\sigma_{\mathrm{b}}^{2}d\). Hence, we have

\[\frac{1}{4P}\leq\rho_{s}^{(\tilde{T})}(i,p)=\sum_{t=0}^{\tilde{T}-1}\left( \rho_{s}^{(t+1)}(i,p)-\rho_{s}^{(t)}(i,p)\right)<\frac{3\eta\sigma_{\mathrm{b }}^{2}d}{2n}\tilde{T},\]

and we conclude \(\tilde{T}>\frac{n}{6\eta P\sigma_{\mathrm{b}}^{2}d}\) which is the desired result. 

Next, we will show that the model learns common features in at least constant order within \(\tilde{T}\) iterates.

**Lemma D.3**.: _Suppose the event \(E_{\mathrm{init}}\) occurs and \(\rho_{k}=\omega\left(\frac{\sigma_{\mathrm{b}}^{2}d}{\beta n}\right)\) for some \(k\in[K]\). Then, for each \(s\in\{\pm 1\}\). there exists \(T_{s,k}\leq\frac{9\eta P}{\eta\beta\left\|Y_{s,k}\right\|}\) such that \(\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)\geq 1\) for any \(t>T_{s,k}\)._

Proof of Lemma d.3.: Suppose \(\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)<1\) for all \(0\leq t\leq\frac{n}{6\eta P\sigma_{\mathrm{b}}^{2}d}\). For each \(i\in\mathcal{V}_{s,k}\) and \(\mathcal{C}\subset[P]\) with \(|\mathcal{C}|=C\) such that \(p_{i}^{*}\notin\mathcal{C}\) and \(\tilde{p}_{i}\in\mathcal{C}\), we have

\[y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i,\mathcal{C}})\] \[=\phi\left(\left\langle\bm{w}_{s}^{(t)},\bm{v}_{s,k}\right\rangle \right)-\phi\left(\left\langle\bm{w}_{-s}^{(t)},\bm{v}_{s,k}\right\rangle \right)+\sum_{p\notin\mathcal{C}\cup[p_{i}^{*}]}\left(\phi\left(\left\langle \bm{w}_{s}^{(t)},\bm{x}_{i}^{(p)}\right\rangle\right)-\phi\left(\left\langle \bm{w}_{-s}^{(t)},\bm{x}_{i}^{(p)}\right\rangle\right)\right)\] \[\leq\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)+\sum_{p \notin\mathcal{C}\cup\{p_{i}^{*}\}}\left(\rho_{s}^{(t)}(i,p)+\beta\rho_{-s}^ {(t)}(i,p)\right)+2P\cdot o\left(\frac{1}{\mathrm{polylog}(d)}\right)\] \[\leq 1+2P\cdot\frac{1}{4P}+2P\cdot o\left(\frac{1}{\mathrm{polylog}(d) }\right)\] \[\leq 2.\]

The first inequality is due to Lemma B.4, the second inequality holds since we can apply Lemma D.2, and the last inequality is due to (A1). Thus, \(g_{i,\mathcal{C}}^{(t)}=\frac{1}{1+\exp\left(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i, \mathcal{C}})\right)}>\frac{1}{9}\) and we have

\[\gamma_{s}^{(t+1)}(s,k)+\beta\gamma_{-s}^{(t+1)}(s,k)\]\[=\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)\] \[\quad+\frac{\eta}{n}\sum_{i\in\mathcal{V}_{s,k}}\mathbb{E}_{\mathcal{ C}\sim\mathcal{D}_{c}}\left[g_{i,\mathcal{C}}^{(t)}\phi^{\prime}\left(\left\langle \boldsymbol{w}_{s}^{(t)},\boldsymbol{v}_{s,k}\right\rangle\right)+\beta\phi^{ \prime}\left(\left\langle\boldsymbol{w}_{-s}^{(t)},\boldsymbol{v}_{s,k} \right\rangle\right)\right)\cdot\mathbbm{1}_{p_{i}^{*}\notin\mathcal{C}}\] \[\geq\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)+\frac{\eta \beta}{9n}\sum_{i\in\mathcal{V}_{s,k}}\mathbb{E}_{\mathcal{C}\sim\mathcal{D}_{c }}[\mathbbm{1}_{p_{i}^{*}\notin\mathcal{C}\wedge\tilde{p}_{i}\in\mathcal{C}}]\] \[=\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)+\frac{\eta \beta|\mathcal{V}_{s,k}|C(P-C)}{9nP(P-1)}\] \[\geq\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)+\frac{\eta \beta|\mathcal{V}_{s,k}|}{9nP}.\]

From the given condition in the lemma statement, we have \(\frac{9nP}{\eta\beta|\mathcal{V}_{s,k}|}=o\left(\frac{n}{6\eta P\sigma_{0}^{ \prime}d}\right)\). If we choose \(t_{0}\in\left[\frac{9nP}{\eta\beta|\mathcal{V}_{s,k}|},\frac{n}{6\eta P\sigma_ {0}^{\prime}d}\right]\), then

\[1>\gamma_{s}^{(t_{0})}(s,k)+\beta\gamma_{-s}^{(t_{0})}(s,k)\geq\frac{\eta \beta|\mathcal{V}_{s,k}|}{9nP}t_{0}\geq 1,\]

and this is contradictory; therefore, it cannot hold that \(\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)<1\) for all \(0\leq t\leq\frac{n}{6\eta P\sigma_{0}^{\prime}d}\). Hence, there exists \(0\leq T_{s,k}<\frac{n}{6\eta P\sigma_{0}^{\prime}d}\) such that \(\gamma_{s}^{(T_{s,k}+1)}(s,k)+\beta\gamma_{-s}^{(T_{s,k}+1)}(s,k)\geq 1\) and choose the smallest one. Then we obtain

\[1\geq\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)\geq\frac{\eta\beta| \mathcal{V}_{s,k}|}{9nP}T_{s,k}.\]

Therefore, \(T_{s,k}\leq\frac{9nP}{\eta\beta|\mathcal{V}_{s,k}|}\) and this is what we desired. 

What We Have So Far.For any common feature or rare feature \(\boldsymbol{v}_{s,k}\) with \(s\in\{\pm 1\}\) and \(k\in\mathcal{K}_{C}\cup\mathcal{K}_{R}\), it satisfies \(\rho_{k}=\omega\left(\frac{\sigma_{0}^{\prime}d}{\beta n}\right)\) due to (A5). By Lemma D.3, at any iterate \(t\in\left[\tilde{T}_{1},T^{*}\right]\) with \(\tilde{T}_{1}:=\max_{s\in\{\pm 1\},k\in\mathcal{C}}T_{s,k}\), the following properties hold if the event \(E_{\mathrm{init}}\) occurs:

* (Learn common/rare features): For \(s\in\{\pm 1\}\) and \(k\in\mathcal{K}_{C}\cup\mathcal{K}_{R}\), \(\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)=\Omega(1)\),
* For any \(s\in\{\pm 1\},i\in[n],\) and \(p\in[P]\setminus\{p_{i}^{*}\},\rho_{s}^{(t)}(i,p)=\widetilde{\mathcal{O}} \left(\beta^{-1}\right)\).

#### d.2.3 Overfitting Augmented Data

In the previous step, we have shown that data containing common or rare features can be well-classified by learning common and rare features. In this step, we will show that the model correctly classifies the remaining training data by overfitting background noise instead of learning its features.

We first introduce lower bounds on the number of iterates such that feature coefficients \(\gamma_{s}^{(t)}(s^{\prime},k)\) remain small, up to the order of \(\alpha^{2}\beta^{-1}\). This lemma holds to any kind of features, but we will focus on extremely rare features. This does not contradict the results from Section D.2.2 for common features and rare features since the upper bound on the number of iterations in Lemma D.3 is larger than the lower bound on the number of iterations in this lemma.

**Lemma D.4**.: _Suppose the event \(E_{\mathrm{init}}\) occurs. For each \(s\in\{\pm 1\}\) and \(k\in[K]\), there exists \(\tilde{T}_{s,k}\geq\frac{n\alpha^{2}}{\eta\beta|\mathcal{V}_{s,k}|}\) such that \(\gamma_{s^{\prime}}^{(t)}(s,k)\leq\alpha^{2}\beta^{-1}\) for any \(0\leq t<\tilde{T}_{s,k}\) and \(s^{\prime}\in\{\pm 1\}\)._

Proof of Lemma D.4.: Let \(\tilde{T}_{s,k}\) be the smallest iterate such that \(\gamma_{s^{\prime}}^{(t)}(s,k)>\alpha^{2}\beta^{-1}\) for some \(s^{\prime}\in\{\pm 1\}\). We assume the existence of \(\tilde{T}_{s,k}\), as its absence would directly lead to our conclusion.

For any \(0\leq t<\tilde{T}_{s,k}\),

\[\gamma_{s^{\prime}}^{(t+1)}(s,k)=\gamma_{s^{\prime}}^{(t)}(s,k)+\frac{\eta}{n} \sum_{i\in\mathcal{V}_{s,k}}\mathbb{E}_{\mathcal{C}\sim\mathcal{D}_{c}}\left[g_ {i,\mathcal{C}}^{(t)}\phi^{\prime}\left(\left\langle\boldsymbol{w}_{s^{\prime}}^ {(t)},\boldsymbol{v}_{s,k}\right\rangle\right)\cdot\mathbbm{1}_{p_{i}^{*} \notin\mathcal{C}}\right]\leq\gamma_{s^{\prime}}^{(t)}(s,k)+\frac{\eta| \mathcal{V}_{s,k}|}{n},\]and we have

\[\alpha^{2}\beta^{-1}\leq\gamma_{s^{\prime}}^{(\tilde{T}_{s,k})}(s,k)=\sum_{t=0}^{ \tilde{T}_{s,k}-1}\left(\gamma_{s^{\prime}}^{(t+1)}(s,k)-\gamma_{s^{\prime}}^{(t )}(s,k)\right)\leq\frac{\eta|\mathcal{V}_{s,k}|}{n}\tilde{T}_{s,k}.\]

We conclude \(\tilde{T}_{s,k}\geq\frac{n\alpha^{2}}{\eta\beta|\mathcal{V}_{s,k}|}\) which is the desired result. 

Next, we will show that the model overfits data augmented not containing common or rare features in at least constant order within \(\tilde{T}_{s,k}\) iterates.

**Lemma D.5**.: _Suppose the event \(E_{\mathrm{init}}\) occurs and \(\rho_{k}=o\left(\frac{\alpha^{2}\sigma_{\mathrm{b}}^{2}d}{n}\right)\). For each \(i\in[n]\) and \(\mathcal{C}\subset[P]\) with \(|\mathcal{C}|=C\), if (1) \(i\in\mathcal{V}_{y_{i},k}\) and \(p_{i}^{*}\notin\mathcal{C}\) or (2) \(i\in[n]\) and \(p_{i}^{*}\in\mathcal{C}\), then there exists \(T_{i,\mathcal{C}}\in\left[\bar{T}_{1},\frac{18n\binom{P}{C}}{\eta\beta\sigma _{\mathrm{b}}^{2}d}\right]\) such that_

\[\sum_{p\notin\mathcal{C}\cup\{p_{i}^{*}\}}\left(\rho_{y_{i}}^{(t)}(i,p)+\beta \rho_{-y_{i}}^{(t)}(i,p)\right)\geq 1,\]

_for any \(t>T_{i,\mathcal{C}}\)._

Proof of Lemma D.5.: We can address both cases in the statement simultaneously. Suppose \(\sum_{p\notin\mathcal{C}\cup\{p_{i}^{*}\}}\left(\rho_{y_{i}}^{(t)}(i,p)+\beta \rho_{-y_{i}}^{(t)}(i,p)\right)<1\) for all \(0\leq t\leq\frac{n\alpha^{2}}{\eta\beta|\mathcal{V}_{y_{i},k}|}\).

From Lemma B.4 and Lemma D.4, we have

\[y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i,\mathcal{C}})\] \[=\sum_{p\notin\mathcal{C}}\left(\phi\left(\left\langle\bm{w}_{y_ {i}}^{(t)},\bm{x}_{i}^{(p)}\right\rangle\right)-\phi\left(\left\langle\bm{w} _{-y_{i}}^{(t)},\bm{x}_{i}^{(p)}\right\rangle\right)\right)\] \[\leq\gamma_{y_{i}}^{(t)}(y_{i},k)+\beta\gamma_{-y_{i}}^{(t)}(y_{ i},k)+\sum_{p\notin\mathcal{C}\cup\{p_{i}^{*}\}}\left(\rho_{y_{i}}^{(t)}(i,p)+ \beta\rho_{-y_{i}}^{(t)}(i,p)\right)+2P\cdot o\left(\frac{1}{\mathrm{polylog}(d )}\right)\] \[\leq(1+\beta)\alpha^{2}\beta^{-1}+1+2P\cdot o\left(\frac{1}{ \mathrm{polylog}(d)}\right)\] \[\leq 2,\]

and \(g_{i,\mathcal{C}}^{(t)}=\frac{1}{1+\exp\left(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i,\mathcal{C}})\right)}\geq\frac{1}{9}\). Also, for each \(p\notin\mathcal{C}\cup\{p_{i}^{*}\}\), we have

\[\rho_{s}^{(t+1)}(i,p)+\beta\rho_{-s}^{(t+1)}(i,p)\] \[\geq\rho_{s}^{(t)}(i,p)+\beta\rho_{-s}^{(t)}(i,p)\] \[\geq\rho_{s}^{(t)}(i,p)+\beta\rho_{-s}^{(t)}(i,p)+\frac{\eta\beta \sigma_{\mathrm{b}}^{2}d}{18n\binom{P}{C}},\]

where the last inequality is due to \(\left\|\xi_{i}^{(p)}\right\|^{2}\geq\frac{1}{2}\sigma_{\mathrm{b}}^{2}d\) and \(\phi^{\prime}\geq\beta\). We also have

\[\sum_{p\notin\mathcal{C}\cup\{p_{i}^{*}\}}\left(\rho_{s}^{(t+1)}(i,p)+\beta \rho_{-s}^{(t+1)}(i,p)\right)\geq\sum_{p\notin\mathcal{C}\cup\{p_{i}^{*}\}} \left(\rho_{s}^{(t)}(i,p)+\beta\rho_{-s}^{(t)}(i,p)\right)+\frac{\eta\beta \sigma_{\mathrm{b}}^{2}d}{18n\binom{P}{C}}\]

From the given condition in the lemma statement, we have \(\frac{18n\binom{P}{C}}{\eta\beta\sigma_{\mathrm{b}}^{2}d}=o\left(\frac{n\alpha ^{2}}{\eta\beta|\mathcal{V}_{s,k}|}\right)\). If we choose \(t_{0}\in\left[\frac{18n\binom{P}{C}}{\eta\beta\sigma_{\mathrm{b}}^{2}d},\frac {n\alpha^{2}}{\eta\beta|\mathcal{V}_{s,k}|}\right]\), then we have

\[1>\sum_{p\notin\mathcal{C}\cup\{p_{i}^{*}\}}\left(\rho_{s}^{(t_{0})}(i,p)+ \beta\rho_{-s}^{(t_{0})}(i,p)\right)\geq\frac{\eta\beta\sigma_{\mathrm{b}}^{2} d}{18n\binom{P}{C}}t_{0}\geq 1,\]and this is a contradiction; therefore, it cannot hold that \(\sum_{p\notin\mathcal{C}\cup\{p_{i}^{*}\}}\left(\rho_{y_{i}}^{(t)}(i,p)+\beta\rho_{ -y_{i}}^{(t)}(i,p)\right)<1\) for all \(0\leq t\leq\frac{n\alpha^{2}}{\eta\beta|\mathcal{V}_{y_{i},k}|}\). Thus, there exists \(0\leq T_{i,\mathcal{C}}<\frac{n\alpha^{2}}{\eta\beta|\mathcal{V}_{y_{i},k}|}\) satisfying \(\sum_{p\notin\mathcal{C}\cup\{p_{i}^{*}\}}\left(\rho_{s}^{(T_{i,\mathcal{C}}+ 1)}(i,p)+\beta\rho_{-s}^{(T_{i,\mathcal{C}}+1)}(i,p)\right)\geq 1\) and let us choose the smallest one.

For any \(0\leq t<T_{i,\mathcal{C}}\), we have

\[1\geq\sum_{p\notin\mathcal{C}\cup\{p_{i}^{*}\}}\left(\rho_{s}^{(T_{i,\mathcal{C }})}(i,p)+\beta\rho_{-s}^{(T_{i,\mathcal{C}})}(i,p)\right)\geq\frac{\eta\sigma _{b}^{2}d}{18n\binom{P}{C}}T_{i},\]

and we conclude that \(T_{i,\mathcal{C}}\leq\frac{18n\binom{P}{C}}{\eta\beta\sigma_{b}^{2}d}\).

Lastly, we move on to prove \(T_{i,\mathcal{C}}>\bar{T}_{1}\). Combining Lemma D.2 and Lemma D.3 leads to

\[\sum_{p\notin\mathcal{C}\cup\{p_{i}^{*}\}\setminus\{p_{i}^{*}\}}\left(\rho_{ s}^{(\bar{T}_{1})}(i,p)+\beta\rho_{-s}^{(T_{1})}(i,p)\right)\leq\frac{1}{2}.\]

Thus, we have \(T_{i,\mathcal{C}}>\bar{T}_{1}\) and this is what we desired. 

What We Have So Far.For any \(k\in\mathcal{K}_{E}\), it satisfies \(\rho_{k}=o\left(\frac{\alpha^{2}n}{\sigma_{b}^{2}d}\right)\) due to (A6). By Lemma D.5 at iterate \(t\in[T_{\mathrm{Cutout}},T^{*}]\) with

\[T_{\mathrm{Cutout}}:=\max\left\{\max_{k\in\mathcal{K}_{E},i\in\mathcal{V}_{y _{i},k},p_{i}^{*}\notin\mathcal{C}}T_{i,\mathcal{C}},\max_{i\in[n],p_{i}^{*} \in\mathcal{C}}T_{i,\mathcal{C}}\right\}\in\left[\bar{T}_{1},T^{*}\right]\]

the following properties hold if the event \(E_{\mathrm{init}}\) occurs:

* (Learn common/rare features): For any \(s\in\{\pm 1\}\) and \(k\in\mathcal{K}_{C}\cup\mathcal{K}_{R}\), \[\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)=\Omega(1),\]
* (Overfit augmented data with extremely rare features or no feature): For each \(i\in[n],k\in\mathcal{K}_{E}\), \(\mathcal{C}\subset[P]\) with \(|\mathcal{C}|=C\) such that (1) \(i\in\mathcal{V}_{y_{i},k}\) and \(p_{i}^{*}\notin\mathcal{C}\) or (2) \(i\in[n]\) and \(p_{i}^{*}\in\mathcal{C}\) \[\sum_{p\notin\mathcal{C}\cup\{p_{i}^{*}\}}\left(\rho_{y_{i}}^{(t)}(i,p)+\beta \rho_{-y_{i}}^{(t)}(i,p)\right)=\Omega(1).\]
* (Do not learn extremely rare features at \(T_{\mathrm{Cutout}}\)): For any \(s,s^{\prime}\in\{\pm 1\}\) and \(k\in\mathcal{K}_{E}\), \[\gamma_{s^{\prime}}^{(T_{\mathrm{Cutout}})}(s,k)\leq\alpha^{2}\beta^{-1}.\]
* For any \(s\in\{\pm 1\},i\in[n],\) and \(p\in[P]\setminus\{p_{i}^{*}\},\rho_{s}^{(t)}(i,p)=\widetilde{\mathcal{O}} \left(\beta^{-1}\right)\).

#### d.2.4 Cutout cannot Learn Extremely Rare Features Within Polynomial Times

In this step, We will show that Cutout cannot learn extremely rare features within the maximum admissible iterate \(T^{*}=\frac{\mathrm{poly}(d)}{\eta}\).

we fix any \(s^{*}\in\{\pm 1\}\) and \(k^{*}\in\mathcal{K}_{E}\). Recall the function \(Q^{(s^{*},k^{*})}:\mathcal{W}\rightarrow\mathbb{R}^{d\times 2}\), defined in Lemma B.5 and omit superscripts for simplicity. For each iteration \(t\), \(Q(\bm{W}^{(t)})\) represents quantities updates by data with feature vector \(\bm{v}_{s^{*},k^{*}}\) until \(t\)-th iteration. We will sequentially introduce several technical lemmas and by combining these lemmas, quantify update by data with feature vector \(\bm{v}_{s^{*},k^{*}}\) after \(T_{\mathrm{Cutout}}\) and derive our conclusion.

Let us define \(\bm{W}^{*}=\{\bm{w}_{1}^{*},\bm{w}_{-1}^{*}\}\), where

\[\bm{w}_{s}^{*}=\bm{w}_{s}^{(T_{\mathrm{Cutout}})}+M\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\sum_{p\in[P]\setminus\{p_{i}^{*},\bar{p}_{i}\}}\frac{\xi_{i}^{(p)}}{ \left\|\xi_{i}^{(p)}\right\|^{2}},\]

where \(M=4\beta^{-1}\log\left(\frac{2\eta\beta^{2}T^{*}}{\alpha^{2}}\right)\). Note that (12), \(\beta<1\), and \(T^{*}=\frac{\mathrm{poly}(d)}{\eta}\) together imply \(M=\widetilde{\mathcal{O}}\left(\beta^{-1}\right)\). Note that \(\bm{W}^{(t)},\bm{W}^{*}\in\mathcal{W}\) for any \(t\geq 0\).

**Lemma D.6**.: _Suppose the event \(E_{\mathrm{init}}\) occurs. Then,_

\[\left\|Q\left(\bm{W}^{(T_{\mathrm{Cutout}})}\right)-Q(\bm{W}^{*})\right\|^{2} \leq 8M^{2}P|\mathcal{V}_{s^{*},k^{*}}|\sigma_{\mathrm{b}}^{-2}d^{-1}.\]

_where \(\left\|\cdot\right\|\) denotes the Frobenius norm._

Proof of Lemma D.6.: For each \(s\in\{\pm 1\}\),

\[ss^{*}\left(Q_{s}\left(\bm{w}_{s}^{*}\right)-Q_{s}\left(\bm{w}_{ s}^{(T_{\mathrm{Cutout}})}\right)\right)\] \[=Q_{s}\left(ss^{*}M\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\sum_{p\in[ P]\setminus\{p_{i}^{*},\tilde{p}_{i}\}}\frac{\xi_{i}^{(p)}}{\left\|\xi_{i}^{(p)} \right\|}\right)\] \[=M\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\sum_{p\in[P]\setminus\{p_{ i}^{*},\tilde{p}_{i}\}}\frac{\xi_{i}^{(p)}}{\left\|\xi_{i}^{(p)}\right\|^{2}},\]

and we have

\[\left\|Q\left(\bm{W}^{(T_{\mathrm{Cutout}})}\right)-Q(\bm{W}^{*} )\right\|^{2}\] \[=\left\|Q_{1}(\bm{w}_{1}^{*})-Q_{1}\left(\bm{w}_{1}^{(T_{\mathrm{ Cutout}})}\right)\right\|^{2}+\left\|Q_{-1}(\bm{w}_{-1}^{*})-Q_{-1}\left(\bm{w}_{-1} ^{(T_{\mathrm{Cutout}})}\right)\right\|^{2}\] \[\leq 2M^{2}\left(\sum_{i\in\mathcal{V}_{s^{*},k^{*}},p\in[P] \setminus\{p_{i}^{*},\tilde{p}_{i}\}}\left\|\xi_{i}^{(p)}\right\|^{-2}+\sum_{ \begin{subarray}{c}i,j\in\mathcal{V}_{s^{*},k^{*}}\\ p\in[P]\setminus\{p_{i}^{*},\tilde{p}_{i}\},q\in[P]\setminus\{p_{j}^{*},\tilde{p} _{j}\}\end{subarray}}\frac{\left|\left\langle\xi_{i}^{(p)},\xi_{j}^{(q)} \right\rangle\right|}{\left\|\xi_{i}^{(p)}\right\|^{2}\left\|\xi_{j}^{(q)} \right\|^{2}}\right).\]

From \(E_{\mathrm{init}}\) and (A2), we have

\[\sum_{\begin{subarray}{c}i,j\in\mathcal{V}_{s^{*},k^{*}}\\ p\in[P]\setminus\{p_{i}^{*},\tilde{p}_{i}\},q\in[P]\setminus\{p_{j}^{*}, \tilde{p}_{j}\}\\ (i,p)\neq(j,q)\end{subarray}}\frac{\left|\left\langle\xi_{i}^{(p)},\xi_{j}^{(q) }\right\rangle\right|^{2}}{\left\|\xi_{i}^{(p)}\right\|^{2}\left\|\xi_{j}^{(q) }\right\|^{2}} \leq\sum_{\begin{subarray}{c}i\in\mathcal{V}_{s^{*},k^{*}}\\ p\in[P]\setminus\{p_{i}^{*},\tilde{p}_{i}\}\end{subarray}}\sum_{\begin{subarray}{ c}j\in\mathcal{V}_{s^{*},k^{*}}\\ p\in[P]\setminus\{p_{j}^{*},\tilde{p}_{j}\}\end{subarray}}\left\|\xi_{i}^{(p) }\right\|^{-2}\widetilde{\mathcal{O}}\left(d^{-\frac{1}{2}}\right)\] \[\leq\sum_{\begin{subarray}{c}i\in\mathcal{V}_{s^{*},k^{*}}\\ p\in[P]\setminus\{p_{i}^{*},\tilde{p}_{i}\}\end{subarray}}\left\|\xi_{i}^{(p) }\right\|^{-2}\]

From the event \(E_{\mathrm{init}}\) defined in Lemma B.2, we have

\[\sum_{\begin{subarray}{c}i\in\mathcal{V}_{s^{*},k^{*}}\\ p\in[P]\setminus\{p_{i}^{*},\tilde{p}_{i}\}\end{subarray}}\left\|\xi_{i}^{(p) }\right\|^{-2}\leq 2P|\mathcal{V}_{s^{*},k^{*}}|\sigma_{\mathrm{d}}^{-2}d^{-1},\]

and we obtain

\[\left\|Q\left(\bm{W}^{(T_{\mathrm{Cutout}})}\right)-Q(\bm{W}^{*})\right\|^{2} \leq 4M^{2}\sum_{i\in\mathcal{V}_{s^{*},k^{*}},p\in[P]\setminus\{p_{i}^{*}\}} \left\|\xi_{i}^{(p)}\right\|^{-2}\leq 8M^{2}P|\mathcal{V}_{s^{*},k^{*}}|\sigma_{ \mathrm{b}}^{-2}d^{-1}.\]

**Lemma D.7**.: _Suppose the \(E_{\mathrm{init}}\) occurs. For any \(t\geq T_{\mathrm{Cutout}}\), \(i\in\mathcal{V}_{s^{*},k^{*}}\) and any \(\mathcal{C}\subset[P]\) with \(|\mathcal{C}|=C\), it holds that_

\[\langle y_{i}\nabla_{\bm{W}}f_{\bm{W}^{(t)}}(\bm{X}_{i,\mathcal{C}}),Q(\bm{W}^ {*})\rangle\geq\frac{M\beta}{2}.\]Proof of Lemma D.7.: We have

\[\left\langle y_{i}\nabla_{\bm{W}}f_{\bm{W}^{(t)}}(\bm{X}_{i,\mathcal{C }}),Q(\bm{W}^{*})\right\rangle\] \[=\sum_{p\notin\mathcal{C}}\bigg{(}\phi^{\prime}\left(\left\langle \left\langle\bm{w}_{s^{*}}^{(t)},\bm{x}_{i}^{(p)}\right\rangle\right\rangle \left\langle Q_{s^{*}}(\bm{w}_{s^{*}}^{*}),\bm{x}_{i}^{(p)}\right\rangle-\phi^ {\prime}\left(\left\langle\bm{w}_{-s^{*}}^{(t)},\bm{x}_{i}^{(p)}\right\rangle \right)\left\langle Q_{-s^{*}}(\bm{w}_{-s^{*}}^{*}),\bm{x}_{i}^{(p)}\right\rangle \bigg{)}.\]

For any \(s\in\{\pm 1\}\) and \(p\in[P]\setminus\{p_{i}^{*},\tilde{p}_{i}\}\),

\[ss^{*}\left\langle Q_{s}(\bm{w}_{s}^{*}),\xi_{i}^{(p)}\right\rangle\] \[\geq M+\rho_{s}^{(T_{\text{Coutout}})}(i,p)-\sum_{\begin{subarray} {c}j\in[n],q\in[P]\setminus\{p_{j}^{*}\}\\ (j,q)\neq(i,p)\end{subarray}}\rho_{s}^{(T_{\text{Coutout}})}(j,q)\frac{\left| \left\langle\xi_{i}^{(p)},\xi_{j}^{(q)}\right\rangle\right|}{\left\|\xi_{j}^ {(q)}\right\|^{2}}\] \[\geq M-\widetilde{\mathcal{O}}\left(nP\beta^{-1}\sigma_{\text{d} }\sigma_{\text{b}}^{-1}d^{-\frac{1}{2}}\right)=M-o\left(\frac{1}{\operatorname {polylog}(d)}\right)\] \[\geq\frac{M}{2},\] (26)

where the last equality is due to (9). Also, for any \(s\in\{\pm 1\}\), \(ss^{*}\left\langle Q_{s}(\bm{w}_{s}^{*}),\bm{v}_{s^{*},k^{*}}\right\rangle= \gamma_{s}^{(T_{\text{Coutout}})}(s^{*},k^{*})\geq 0\). In addition,

\[ss^{*}\left\langle Q_{s}(\bm{w}_{s}^{*}),\bm{x}_{i}^{(\tilde{p} _{i})}\right\rangle\] \[=ss^{*}\left\langle Q_{s}(\bm{w}_{s}^{*}),\xi_{i}^{(\tilde{p}_{ i})}\right\rangle+ss^{*}\left\langle Q_{s}(\bm{w}_{s}^{*}),\bm{x}_{i}^{( \tilde{p}_{i})}-\xi_{i}^{(\tilde{p}_{i})}\right\rangle\] \[=ss^{*}\left\langle Q_{s}(\bm{w}_{s}^{*}),\xi_{i}^{(\tilde{p}_{i} )}\right\rangle-\widetilde{\mathcal{O}}\left(\alpha^{2}\beta^{-1}\rho_{k^{*}} n\sigma_{\text{d}}^{-2}d^{-1}\right)\] \[=\rho_{s}^{(T_{\text{Coutout}})}(i,\tilde{p}_{i})+\sum_{ \begin{subarray}{c}j\in[n],q\in[P]\setminus\{p_{i}^{*}\}\\ (j,q)\neq(i,\tilde{p}_{i})\end{subarray}}\rho_{s}^{(T_{\text{Coutout}})}(j,q) \frac{\left\langle\xi_{i}^{(\tilde{p}_{i})},\xi_{j}^{(q)}\right\rangle}{ \left\|\xi_{j}^{(q)}\right\|^{2}}-\widetilde{\mathcal{O}}\left(\alpha^{2} \beta^{-1}\rho_{k^{*}}n\sigma_{\text{d}}^{-2}d^{-1}\right)\] \[\geq-\widetilde{\mathcal{O}}\left(nP\beta^{-1}\sigma_{\text{d}} \sigma_{\text{b}}^{-1}d^{-\frac{1}{2}}\right)-\widetilde{\mathcal{O}}\left( \alpha^{2}\beta^{-1}\rho_{k^{*}}n\sigma_{\text{d}}^{-2}d^{-1}\right)\] \[=-o\left(\frac{1}{\operatorname{polylog}(d)}\right),\] (27)

where the last equality is due to (9) and (A7).

For any \(\mathcal{C}\subset[P]\) with \(|\mathcal{C}|=C\), there exists \(p\in[P]\setminus\{p_{i}^{*},\tilde{p}_{i}\}\) such that \(p\neq\mathcal{C}\) since \(C<\frac{P}{2}\). By applying (26) and (27) for \(s=s^{*},-s^{*}\) and combining with \(\phi^{\prime}\geq\beta\), we have

\[\left\langle y_{i}\nabla_{\bm{W}}f_{\bm{W}^{(t)}}(\bm{X}_{i, \mathcal{C}}),Q(\bm{W}^{*})\right\rangle\] \[\geq\left(\phi^{\prime}\left(\left\langle\bm{w}_{s^{*}}^{(t)},\bm {x}_{i}^{(p)}\right\rangle\right)\left\langle Q_{s^{*}}(\bm{w}_{s^{*}}^{*}), \bm{x}_{i}^{(p)}\right\rangle-\phi^{\prime}\left(\left\langle\bm{w}_{-s^{*}}^{(t )},\bm{x}_{i}^{(p)}\right\rangle\right)\left\langle Q_{-s^{*}}(\bm{w}_{-s^{*}}^{ *}),\bm{x}_{i}^{(p)}\right\rangle\right)\] \[\geq M\beta-o\left(\frac{1}{\operatorname{polylog}(d)}\right)\] \[\geq\frac{M\beta}{2}.\]By combining Lemma D.6 and Lemma D.7, we can obtain the following result.

**Lemma D.8**.: _Suppose the event \(E_{\mathrm{init}}\) occurs._

\[\frac{\eta}{n}\sum_{t=T_{\mathrm{Cutout}}}^{T^{*}}\sum_{i\in\mathcal{V}_{s^{*}, h^{*}}}\mathbb{E}_{\mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}[\ell\left(y_{i}f_{ \boldsymbol{W}^{(t)}}(\boldsymbol{X}_{i,\mathcal{C}})\right)]\leq\left\|Q \left(\boldsymbol{W}^{(T_{\mathrm{Cutout}})}\right)-Q(\boldsymbol{W}^{*}) \right\|^{2}+2\eta T^{*}e^{-\frac{M\beta}{4}},\]

_where \(\left\|\cdot\right\|\) denotes the Frobenius norm._

Proof of Lemma D.8.: Note that for any \(T_{\mathrm{Cutout}}\leq t<T^{*}\),

\[Q\left(\boldsymbol{W}^{(t+1)}\right)=Q\left(\boldsymbol{W}^{(t)}\right)-\frac {\eta}{n}\nabla_{\boldsymbol{W}}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\mathbb{E} _{\mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}\left[\ell\left(y_{i}f_{ \boldsymbol{W}^{(t)}}(\boldsymbol{X}_{i,\mathcal{C}})\right)\right].\]

Therefore, we have

\[\left\|Q\left(\boldsymbol{W}^{(t)}\right)-Q\left(\boldsymbol{W}^{ *}\right)\right\|^{2}-\left\|Q\left(\boldsymbol{W}^{(t+1)}\right)-Q\left( \boldsymbol{W}^{*}\right)\right\|^{2}\] \[=\frac{2\eta}{n}\left\langle\nabla_{\boldsymbol{W}}\sum_{i\in \mathcal{V}_{s^{*},k^{*}}}\mathbb{E}_{\mathcal{C}\sim\mathcal{D}_{\mathcal{C} }}\left[\ell\left(y_{i}f_{\boldsymbol{W}^{(t)}}(\boldsymbol{X}_{i,\mathcal{C}} )\right)\right],Q\left(\boldsymbol{W}^{(t)}\right)-Q\left(\boldsymbol{W}^{*} \right)\right\rangle\] \[\quad-\frac{\eta^{2}}{n^{2}}\left\|\nabla_{\boldsymbol{W}}\sum_{i \in\mathcal{V}_{s^{*},k^{*}}}\mathbb{E}_{\mathcal{C}\sim\mathcal{D}_{\mathcal{ C}}}\left[\ell\left(y_{i}f_{\boldsymbol{W}^{(t)}}(\boldsymbol{X}_{i,\mathcal{C}}) \right)\right]\right\|^{2}\] \[\quad-\frac{\eta^{2}}{n^{2}}\left\|\nabla_{\boldsymbol{W}}\sum_{i \in\mathcal{V}_{s^{*},k^{*}}}\mathbb{E}_{\mathcal{C}\sim\mathcal{D}_{\mathcal{ C}}}\left[\ell\left(y_{i}f_{\boldsymbol{W}^{(t)}}(\boldsymbol{X}_{i,\mathcal{C}}) \right)\right]\right\|^{2}\] \[\geq\frac{2\eta}{n}\left\langle\nabla_{\boldsymbol{W}}\sum_{i\in \mathcal{V}_{s^{*},k^{*}}}\mathbb{E}_{\mathcal{C}\sim\mathcal{D}_{\mathcal{C} }}\left[\ell(y_{i}f_{\boldsymbol{W}^{(t)}}(\boldsymbol{X}_{i,\mathcal{C}})) \right],Q\left(\boldsymbol{W}^{(t)}\right)\right\rangle\] \[\quad-\frac{M\beta\eta}{n}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}} \mathbb{E}_{\mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}\left[\ell^{\prime}(y_{i} f_{\boldsymbol{W}^{(t)}}(\boldsymbol{X}_{i,\mathcal{C}}))\right]-\frac{\eta^{2}}{n^{2}} \left\|\nabla_{\boldsymbol{W}}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\mathbb{E} _{\mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}\left[\ell\left(y_{i}f_{\boldsymbol{ W}^{(t)}}(\boldsymbol{X}_{i,\mathcal{C}})\right)\right]\right\|^{2},\]

where the last inequality is due to Lemma D.7. By the chain rule, for each \(\mathcal{C}\subset[P]\) with \(|\mathcal{C}|=C\), we have

\[\left\langle\nabla_{\boldsymbol{W}}\sum_{i\in\mathcal{V}_{s^{*}, k^{*}}}\ell(y_{i}f_{\boldsymbol{W}^{(t)}}(\boldsymbol{X}_{i,\mathcal{C}})),Q \left(\boldsymbol{W}^{(t)}\right)\right\rangle\] \[=\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\left[\ell^{\prime}(y_{i}f_ {\boldsymbol{W}^{(t)}}(\boldsymbol{X}_{i,\mathcal{C}}))\right.\] \[\quad\times\sum_{p\notin\mathcal{C}}\left(\phi^{\prime}\left( \left\langle\boldsymbol{w}_{s^{*}}^{(t)},\boldsymbol{x}_{i}^{(p)}\right\rangle \right)\left\langle Q_{s^{*}}\left(\boldsymbol{w}_{s^{*}}^{(t)}\right), \boldsymbol{x}_{i}^{(p)}\right\rangle-\phi^{\prime}\left(\left\langle \boldsymbol{w}_{-s^{*}}^{(t)},\boldsymbol{x}_{i}^{(p)}\right\rangle\right)\left \langle Q_{-s^{*}}\left(\boldsymbol{w}_{-s^{*}}^{(t)}\right),\boldsymbol{x}_{ i}^{(p)}\right\rangle\right)\right].\]

For each \(s\in\{\pm 1\}\), \(i\in\mathcal{V}_{s^{*},k^{*}}\), and \(p\in[P]\),

\[\left|\left\langle\boldsymbol{w}_{s}^{(t)},\boldsymbol{x}_{i}^{(p)} \right\rangle-\left\langle Q_{s}\left(\boldsymbol{w}_{s}^{(t)}\right), \boldsymbol{x}_{i}^{(p)}\right\rangle\right|\] \[=\left|\left\langle\boldsymbol{w}_{s}^{(t)}-Q_{s}\left( \boldsymbol{w}_{s}^{(t)}\right),\boldsymbol{x}_{i}^{(p)}\right\rangle\right|\]\[\leq\sum_{j\in[n]\setminus\mathcal{V}_{s^{*},k^{*}},q\in[P]\setminus \{p_{i}^{*}\}}\left|\left\langle\rho_{s}^{(t)}(j,q)\frac{\xi_{j}^{(q)}}{\left\| \xi_{j}^{(q)}\right\|^{2}},\bm{x}_{i}^{(p)}\right\rangle\right|\] \[\quad+\alpha\sum_{j\in\mathcal{F}_{\downarrow}\setminus\mathcal{V} _{s^{*},k^{*}}}\rho_{s}^{(t)}(j,\tilde{p}_{j})\left\|\xi_{j}^{(\tilde{p}_{j})} \right\|^{-2}\left|\left\langle\bm{v}_{1,1},\bm{x}_{i}^{(p)}\right\rangle\right|\] \[\quad+\alpha\sum_{j\in\mathcal{F}_{-\downarrow}\setminus\mathcal{V }_{s^{*},k^{*}}}\rho_{s}^{(t)}(j,\tilde{p}_{j})\left\|\xi_{j}^{(\tilde{p}_{j})} \right\|^{-2}\left|\left\langle\bm{v}_{-1,1},\bm{x}_{i}^{(p)}\right\rangle\right|\] \[\leq\widetilde{\mathcal{O}}\left(nP\beta^{-1}\sigma_{\mathrm{d}} \sigma_{\mathrm{b}}^{-1}d^{-\frac{1}{2}}\right)+\widetilde{\mathcal{O}}\left( \alpha^{2}\beta^{-1}n\sigma_{\mathrm{d}}^{-2}d^{-1}\right)\] \[=o\left(\frac{1}{\mathrm{polylog}(d)}\right),\]

where the last inequality is due to Lemma D.1 and the event \(E_{\mathrm{init}}\). By Lemma F.1,

\[\sum_{p\notin\mathcal{C}}\left(\phi^{\prime}\left(\left\langle \bm{w}_{s^{*}}^{(t)},\bm{x}_{i}^{(p)}\right\rangle\right)\left\langle Q_{s^{* }}\left(\bm{w}_{s^{*}}^{(t)}\right),\bm{x}_{i}^{(p)}\right\rangle-\phi^{\prime }\left(\left\langle\bm{w}_{-s^{*}}^{(t)},\bm{x}_{i}^{(p)}\right\rangle\right) \left\langle Q_{-s^{*}}\left(\bm{w}_{s^{*}}^{(t)}\right),\bm{x}_{i}^{(p)} \right\rangle\right)\] \[\leq\sum_{p\notin\mathcal{C}}\left(\phi\left(\left\langle\bm{w} _{s^{*}}^{(t)},\bm{x}_{i}^{(p)}\right\rangle\right)-\phi\left(\left\langle\bm {w}_{-s^{*}}^{(t)},\bm{x}_{i}^{(p)}\right\rangle\right)\right)+rP+o\left( \frac{1}{\mathrm{polylog}(d)}\right)\] \[=y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i,\mathcal{C}})+o\left(\frac{1}{ \mathrm{polylog}(d)}\right),\]

where the last equality is due to \(r=o\left(\frac{1}{\mathrm{polylog}(d)}\right)\). Therefore, we have

\[\left\|Q\left(\bm{W}^{(t)}\right)-Q\left(\bm{W}^{*}\right)\right\| ^{2}-\left\|Q\left(\bm{W}^{(t+1)}\right)-Q(\bm{W}^{*})\right\|^{2}\] \[\geq\frac{2\eta}{n}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\mathbb{E} _{\mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}\left[\ell^{\prime}\left(y_{i}f_{ \bm{W}^{(t)}}(\bm{X}_{i,\mathcal{C}})\right)\left(y_{i}f_{\bm{W}^{(t)}}(\bm {X}_{i,\mathcal{C}})+o\left(\frac{1}{\mathrm{polylog}(d)}\right)-\frac{M \beta}{2}\right)\right]\] \[\geq\frac{2\eta}{n}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\mathbb{E} _{\mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}\left[\ell^{\prime}(y_{i}f_{\bm{W} ^{(t)}}(\bm{X}_{i,\mathcal{C}}))\left(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i, \mathcal{C}})-\frac{M\beta}{4}\right)\right]\] \[\quad-\frac{\eta^{2}}{n^{2}}\left\|\nabla_{\bm{W}}\sum_{i\in \mathcal{V}_{s^{*},k^{*}}}\mathbb{E}_{\mathcal{C}\sim\mathcal{D}_{\mathcal{C}}} \left[\ell(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i,\mathcal{C}}))\right]\right\|^{2}.\]

From the convexity of \(\ell(\cdot)\),

\[\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\mathbb{E}_{\mathcal{C}\sim \mathcal{D}_{\mathcal{C}}}\left[\ell^{\prime}(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i, \mathcal{C}}))\left(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i,\mathcal{C}})-\frac{M \beta}{4}\right)\right]\] \[\geq\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\mathbb{E}_{\mathcal{C} \sim\mathcal{D}_{\mathcal{C}}}\left[\left(\ell(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i, \mathcal{C}}))-\ell\left(\frac{M\beta}{4}\right)\right)\right]\] \[\geq\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\mathbb{E}_{\mathcal{C} \sim\mathcal{D}_{\mathcal{C}}}\left[\ell(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i, \mathcal{C}}))\right]-ne^{-\frac{M\beta}{4}}.\]

In addition, by Lemma F.3,

\[\frac{\eta^{2}}{n^{2}}\left\|\nabla\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\mathbb{E }_{\mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}[\ell\left(y_{i}f_{\bm{W}^{(t)}}( \bm{X}_{i,\mathcal{C}}))\right)\right\|^{2}\]\[\leq\frac{8\eta^{2}P^{2}\sigma_{0}^{2}d|V_{s^{*},k^{*}}|}{n^{2}}\sum_{ i\in\mathcal{V}_{s^{*},k^{*}}}\mathbb{E}_{\mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}[ \ell(y_{i}f_{\boldsymbol{W}^{(t)}}(\boldsymbol{X}_{i,\mathcal{C}}))]\] \[\leq\frac{\eta}{n}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\mathbb{E}_ {\mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}[\ell(y_{i}f_{\boldsymbol{W}^{(t)}}( \boldsymbol{X}_{i,\mathcal{C}}))],\]

where the last inequality is due to (A8), and we have

\[\left\|Q\left(\boldsymbol{W}^{(t)}\right)-Q(\boldsymbol{W}^{*}) \right\|^{2}-\left\|Q\left(\boldsymbol{W}^{(t+1)}\right)-Q(\boldsymbol{W}^{*}) \right\|^{2}\] \[\geq\frac{\eta}{n}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}}\mathbb{E}_ {\mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}[\ell(y_{i}f_{\boldsymbol{W}^{(t)}} (\boldsymbol{X}_{i,\mathcal{C}}))]-2\eta e^{-\frac{M\beta}{4}}.\]

From telescoping summation, we have

\[\frac{\eta}{n}\sum_{t=T_{\mathrm{Cutout}}}^{T^{*}}\sum_{i\in\mathcal{V}_{s^{* },k^{*}}}\mathbb{E}_{\mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}[\ell\left(y_{ i}f_{\boldsymbol{W}^{(t)}}(\boldsymbol{X}_{i,\mathcal{C}})\right)]\leq\left\|Q \left(\boldsymbol{W}^{(T_{\mathrm{Cutout}})}\right)-Q\left(\boldsymbol{W}^{*} \right)\right\|^{2}+2\eta T^{*}e^{-\frac{M\beta}{4}}.\]

Finally, we can prove that the model cannot learn extremely rare features within \(T^{*}\) iterations.

**Lemma D.9**.: _Suppose the event \(E_{\mathrm{init}}\) occurs. For any \(T\in[T_{\mathrm{Cutout}},T^{*}]\), we have \(\gamma_{s}^{(T)}(s^{*},k^{*})=\widetilde{\mathcal{O}}(\alpha^{2}\beta^{-2})\) for each \(s\in\{\pm 1\}\)._

Proof of Lemma D.9.: For any \(T\in[T_{\mathrm{Cutout}},T^{*}]\), we have

\[\gamma_{s}^{(T)}(s^{*},k^{*}) =\gamma_{s}^{(T_{\mathrm{Cutout}})}(s^{*},k^{*})+\frac{\eta}{n} \sum_{t=T_{\mathrm{Cutout}}}^{T-1}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}} \mathbb{E}_{\mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}\left[g_{i,\mathcal{C}}^{ (t)}\cdot\mathbf{1}_{p\notin\mathcal{C}}\right]\phi^{\prime}\left(\left\langle \boldsymbol{w}_{s}^{(t)},\boldsymbol{v}_{s^{*},k^{*}}\right\rangle\right)\] \[\leq\gamma_{s}^{(T_{\mathrm{Cutout}})}(s^{*},k^{*})+\frac{\eta}{n} \sum_{t=T_{\mathrm{Cutout}}}^{T-1}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}} \mathbb{E}_{\mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}\left[g_{i,\mathcal{C}}^{ (t)}\right]\] \[\leq\gamma_{s}^{(T_{\mathrm{Cutout}})}(s^{*},k^{*})+\frac{\eta}{n} \sum_{t=T_{\mathrm{Cutout}}}^{T-1}\sum_{i\in\mathcal{V}_{s^{*},k^{*}}} \mathbb{E}_{\mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}\left[\ell\left(y_{i}f_{ \boldsymbol{W}^{(t)}}(\boldsymbol{X}_{i,\mathcal{C}})\right)\right],\]

where the first inequality is due to \(\phi^{\prime}\leq 1\) and the second inequality is due to \(-\ell^{\prime}\leq\ell\). From the result of Section D.2.3, \(\gamma_{s}^{(T_{\mathrm{Cutout}})}(s^{*},k^{*})\leq\alpha^{2}\beta^{-1}\) and by Lemma D.8 and Lemma D.6, we have

\[\frac{\eta}{n}\sum_{t=T_{\mathrm{Cutout}}}^{(T-1)}\sum_{i\in \mathcal{V}_{s^{*},k^{*}}}\mathbb{E}_{\mathcal{C}\sim\mathcal{D}_{\mathcal{C}} }[\ell\left(y_{i}f_{\boldsymbol{W}^{(t)}}(\boldsymbol{X}_{i,\mathcal{C}})\right)] \leq\frac{\eta}{n}\sum_{t=T_{\mathrm{Cutout}}}^{(T^{*})}\sum_{i \in\mathcal{V}_{s^{*},k^{*}}}\mathbb{E}_{\mathcal{C}\sim\mathcal{D}_{\mathcal{C} }}[\ell\left(y_{i}f_{\boldsymbol{W}^{(t)}}(\boldsymbol{X}_{i,\mathcal{C}}) \right)]\] \[\leq\left\|Q\left(\boldsymbol{W}^{(T_{\mathrm{Cutout}})}\right)-Q( \boldsymbol{W}^{*})\right\|^{2}+2\eta T^{*}e^{-\frac{M\beta}{2}}\] \[\leq 8M^{2}P|\mathcal{V}_{s^{*},k^{*}}|\sigma_{\mathrm{b}}^{-2}d^{-1} +2\eta T^{*}e^{-\frac{M\beta}{4}}\] \[=\widetilde{\mathcal{O}}\left(\alpha^{2}\beta^{-2}\right).\]

The last line is due to (A6) and \(M=4\beta^{-1}\log\left(\frac{2\eta\beta^{2}T^{*}}{\alpha^{2}}\right)\). This finishes the proof. 

What We Have So Far.Suppose the event \(E_{\mathrm{init}}\) occurs. For any \(t\in[T_{\mathrm{Cutout}},T^{*}]\), we have

* (Learn common/rare features): \(\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)=\Omega(1)\) for each \(s\in\{\pm 1\}\) and \(k\in\mathcal{K}_{C}\cup\mathcal{K}_{R}\)
* (Overfit augmented data with extremely rare features or no feature): For each \(i\in[n],k\in\mathcal{K}_{E},\mathcal{C}\subset[P]\) with \(|\mathcal{C}|=C\) such that (1) \(i\in\mathcal{V}_{y_{i},k}\) and \(p_{i}^{*}\notin\mathcal{C}\) or (2) \(i\in[n]\) and \(p_{i}^{*}\in\mathcal{C}\) \[\sum_{p\notin\mathcal{C}\cup\{p_{i}^{*}\}}\left(\rho_{y_{i}}^{(t)}(i,p)+ \beta\rho_{-y_{i}}^{(t)}(i,p)\right)=\Omega(1).\]* (Cannot learn extreme features): \(\gamma_{s}^{(t)}(s,k),\gamma_{-s}^{(t)}(s,k)=\mathcal{O}\left(\alpha^{2}\beta^{-2}\right)\) for each \(s\in\{\pm 1\}\) and \(k\in\mathcal{K}_{E}\).
* For any \(s\in\{\pm 1\},i\in[n],\) and \(p\in[P]\setminus\{p_{i}^{*}\},\rho_{s}^{(t)}(i,p)=\widetilde{\mathcal{O}} \left(\beta^{-1}\right)\),

#### d.2.5 Train and Test Accuracy

In this step, we will prove that the model trained by Cutout has perfect training accuracy on both augmented data and original data but has near-random guesses on test data with extremely rare data.

For any \(i\in\mathcal{V}_{s,k}\) with \(s\in\{\pm 1\}\), \(k\in\mathcal{K}_{C}\cup\mathcal{K}_{\mathcal{R}}\) and \(\mathcal{C}\subset[P]\) with \(|\mathcal{C}|=C\) and \(p_{i}^{*}\notin\mathcal{C}\),

\[y_{i}f_{\boldsymbol{W}^{(t)}}(\boldsymbol{X}_{i,\mathcal{C}})\] \[=\sum_{p\notin\mathcal{C}}\left(\phi\left(\left\langle\boldsymbol {w}_{s}^{(t)},\boldsymbol{x}_{i}^{(p)}\right\rangle\right)-\phi\left(\left\langle \boldsymbol{w}_{-s}^{(t)},\boldsymbol{x}_{i}^{(p)}\right\rangle\right)\right)\] \[=\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)+\sum_{p\notin \mathcal{C}\cup\{p_{i}^{*}\}}\left(\rho_{s}^{(t)}(i,p)+\beta\rho_{-s}^{(t)}(i,p)\right)-2(P-C)\cdot o\left(\frac{1}{\operatorname{polylog}(d)}\right)\] \[\geq\gamma_{s}^{(t)}(s,k)+\beta\gamma_{-s}^{(t)}(s,k)-2(P-C) \cdot o\left(\frac{1}{\operatorname{polylog}(d)}\right)\] \[=\Omega(1)-o\left(\frac{1}{\operatorname{polylog}(d)}\right)\] \[=\Omega(1),\]

for any \(t\in[T_{\operatorname{Cutout}},T^{*}]\). In addition, for any \(i\in[n]\) and \(\mathcal{C}\subset[P]\) with \(|\mathcal{C}|=C\) that does not correspond to the case above, by Lemma D.5 and Lemma B.4, we have

\[y_{i}f_{\boldsymbol{W}^{(t)}}(\boldsymbol{X}_{i,\mathcal{C}})\] \[=\sum_{p\notin\mathcal{C}}\left(\phi\left(\left\langle \boldsymbol{w}_{y_{i}}^{(t)},\boldsymbol{x}_{i}^{(p)}\right\rangle\right)- \phi\left(\left\langle\boldsymbol{w}_{-y_{i}}^{(t)},\boldsymbol{x}_{i}^{(p)} \right\rangle\right)\right)\] \[\geq\sum_{p\notin\mathcal{C}\cup\{p_{i}^{*}\}}\left(\rho_{y_{i}} ^{(t)}(i,p)+\beta\rho_{-y_{i}}^{(t)}(i,p)\right)-2(P-C)\cdot o\left(\frac{1}{ \operatorname{polylog}(d)}\right)\] \[=\Omega(1)-o\left(\frac{1}{\operatorname{polylog}(d)}\right)\] \[=\Omega(1),\]

for any \(t\in[T_{\operatorname{Cutout}},T^{*}]\). We can conclude that Cutout with \(t\in[T_{\operatorname{Cutout}},T^{*}]\) iterates achieve perfect training accuracy on augmented data.

Next, we will show that Cutout achieves perfect training accuracy on the original data. For any \(i\in[n]\), let us choose \(\mathcal{C}\subset[P]\) with \(|\mathcal{C}|=C\) such that \(p_{i}^{*}\in\mathcal{C}\). Then, from the result above, we have

\[y_{i}f_{\boldsymbol{W}^{(t)}}(\boldsymbol{X}_{i}) =y_{i}f_{\boldsymbol{W}^{(t)}}(\boldsymbol{X}_{i,\mathcal{C}})+ \sum_{p\in\mathcal{C}}\left(\phi\left(\left\langle\boldsymbol{w}_{y_{i}}^{(t) },\boldsymbol{x}_{i}^{(p)}\right\rangle\right)-\phi\left(\left\langle \boldsymbol{w}_{-y_{i}}^{(t)},\boldsymbol{x}_{i}^{(p)}\right\rangle\right)\right)\] \[\geq y_{i}f_{\boldsymbol{W}^{(t)}}(\boldsymbol{X}_{i,\mathcal{C}})+ \sum_{p\in\mathcal{C}\setminus\{p_{i}^{*}\}}\left(\rho_{y_{i}}^{(t)}(i,p)+ \beta\rho_{-y_{i}}^{(t)}(i,p)\right)-C\cdot o\left(\frac{1}{\operatorname{ polylylog}(d)}\right)\] \[\geq\Omega(1),\]

for any \(t\in[T_{\operatorname{Cutout}},T^{*}]\) and we conclude that Cutout with \(t\in[T_{\operatorname{Cutout}},T^{*}]\) iterates achieve perfect training accuracy on original data.

Lastly, let us move on to the test accuracy part. Let \((\boldsymbol{X},y)\sim\mathcal{D}\) be a test data with \(\boldsymbol{X}=\left(\boldsymbol{x}^{(1)},\ldots,\boldsymbol{x}^{(P)}\right) \in\mathbb{R}^{d\times P}\) having feature patch \(p^{*}\), dominant noise patch \(\tilde{p}\), and feature vector \(\boldsymbol{v}_{y,k}\). We have \(\boldsymbol{x}^{(p)}\sim N(\boldsymbol{0},\sigma_{\mathrm{b}}^{2}\boldsymbol{ \Lambda})\) for each \(p\in[P]\setminus\{p^{*},\tilde{p}\}\) and \(\boldsymbol{x}^{(\tilde{p})}-\alpha\boldsymbol{v}_{s,1}\sim N(\boldsymbol{0}, \sigma_{\mathrm{d}}^{2}\boldsymbol{\Lambda})\) for some \(s\in\{\pm 1\}\). Therefore, for all \(t\in[T_{\operatorname{Cutout}},T^{*}]\) and \(p\in[P]\setminus\{p^{*},\tilde{p}\}\),

\[\left|\phi\left(\left\langle\boldsymbol{w}_{1}^{(t)},\boldsymbol{x}^{(p)} \right\rangle\right)-\phi\left(\left\langle\boldsymbol{w}_{-1}^{(t)}, \boldsymbol{x}^{(p)}\right\rangle\right)\right|\]\[\leq\left|\left\langle\bm{w}_{1}^{(t)}-\bm{w}_{-1}^{(t)},\bm{x}^{(p)} \right\rangle\right|\] \[\leq\left|\left\langle\bm{w}_{1}^{(0)}-\bm{w}_{-1}^{(0)},\bm{x}^{( p)}\right\rangle\right|+\sum_{i\in[n],q\in[P]\setminus\{p_{i}^{*}\}}\left|\rho_{1}^{(t)} (i,q)-\rho_{-1}^{(t)}(i,q)\right|\frac{\left|\left\langle\xi_{i}^{(q)},\bm{x}^ {(p)}\right\rangle\right|}{\left\|\xi_{i}^{(q)}\right\|^{2}}\] \[\leq\widetilde{\mathcal{O}}\left(\sigma_{0}\sigma_{\mathrm{b}}d^{ \frac{1}{2}}\right)+\widetilde{\mathcal{O}}\left(nP\beta^{-1}\sigma_{\mathrm{d }}\sigma_{\mathrm{b}}^{-1}d^{-\frac{1}{2}}\right)\] \[=o\left(\frac{\alpha}{\mathrm{polylog}(d)}\right),\] (28)

with probability at least \(1-o\left(\frac{1}{\mathrm{poly}(d)}\right)\) due to Lemma B.2, (A8), (8), and (9).. In addition, for any \(s^{\prime}\in\{\pm 1\}\), we have

\[\left|\left\langle\bm{w}_{s^{\prime}}^{(t)},\bm{x}^{(\tilde{p}) }-\alpha\bm{v}_{s,1}\right\rangle\right|\] \[\leq\left|\left\langle\bm{w}_{s^{\prime}}^{(0)},\bm{x}^{(\tilde{p })}-\alpha\bm{v}_{s,1}\right\rangle\right|+\sum_{i\in[n],q\in[P]\setminus\{p_{ i}^{*}\}}\rho_{s^{\prime}}^{(t)}(i,q)\frac{\left|\left\langle\xi_{i}^{(q)},\bm{x}^ {(\tilde{p})}-\alpha\bm{v}_{s,1}\right\rangle\right|}{\left\|\xi_{i}^{(q)} \right\|^{2}}\] \[=\widetilde{\mathcal{O}}\left(\sigma_{0}\sigma_{\mathrm{d}}d^{ \frac{1}{2}}\right)+\widetilde{\mathcal{O}}\left(nP\beta^{-1}\sigma_{\mathrm{d }}\sigma_{\mathrm{b}}^{-1}d^{-\frac{1}{2}}\right)\] \[=o\left(\frac{\alpha}{\mathrm{polylog}(d)}\right),\] (29)

with probability at least \(1-o\left(\frac{1}{\mathrm{poly}(d)}\right)\) due to Lemma B.2, (A8), (8), and (9).

**Case 1:**\(k\in\mathcal{K}_{C}\cup\mathcal{K}_{R}\)

By Lemma B.2, (A7), and (10),

\[\left|\phi\left(\left\langle\bm{w}_{1}^{(t)},\bm{x}^{(\tilde{p}) }\right\rangle\right)-\phi\left(\left\langle\bm{w}_{-1}^{(t)},\bm{x}^{( \tilde{p})}\right\rangle\right)\right|\] \[\leq\left|\left\langle\bm{w}_{1}^{(t)}-\bm{w}_{-1}^{(t)},\bm{x}^{( \tilde{p})}\right\rangle\right|\] \[\leq\alpha\left|\left\langle\bm{w}_{1}^{(t)}-\bm{w}_{-1}^{(t)}, \bm{v}_{s,1}\right\rangle\right|+\left|\left\langle\bm{w}_{1}^{(t)}-\bm{w}_{-1 }^{(t)},\bm{x}^{(p)}-\alpha\bm{v}_{s,1}\right\rangle\right|\] \[\leq\alpha\left(\gamma_{1}^{(t)}(s,1)+\gamma_{-1}^{(t)}(s,1) \right)+\alpha\left|\left\langle\bm{w}_{1}^{(0)},\bm{v}_{s,1}\right\rangle \right|+\alpha\left|\left\langle\bm{w}_{-1}^{(0)},\bm{v}_{s,1}\right\rangle \right|+o\left(\frac{1}{\mathrm{polylog}(d)}\right)\] \[\leq\widetilde{\mathcal{O}}\left(\alpha\beta^{-1}\right)+ \widetilde{\mathcal{O}}\left(\alpha\sigma_{0}\right)+o\left(\frac{1}{\mathrm{ polylylog}(d)}\right)\] \[=o\left(\frac{1}{\mathrm{polylog}(d)}\right),\] (30)

with probability at least \(1-o\left(\frac{1}{\mathrm{poly}(d)}\right)\). Suppose (28) and (30) holds. By Lemma B.4, we have

\[yf_{\bm{W}^{(t)}}(\bm{X})\] \[=\left(\phi\left(\left\langle\bm{w}_{y}^{(t)},\bm{v}_{y,k}\right \rangle\right)\right.-\phi\left(\left\langle\bm{w}_{-y}^{(t)},\bm{v}_{y,k} \right\rangle\right)\right)\] \[\quad+\sum_{p\in[P]\setminus\{p^{*}\}}\left(\phi\left(\left\langle \bm{w}_{y}^{(t)},\bm{x}^{(p)}\right\rangle\right)-\phi\left(\left\langle\bm{ w}_{-y}^{(t)},\bm{x}^{(p)}\right\rangle\right)\right)\] \[=\gamma_{y}^{(t)}(y,k)+\beta\gamma_{-y}^{(t)}(y,k)-o\left(\frac{1 }{\mathrm{polylog}(d)}\right)\] \[=\Omega(1)-o\left(\frac{1}{\mathrm{polylog}(d)}\right)\]\[>0.\]

Therefore, we have

\[\mathbb{P}_{(\bm{X},y)\sim\mathcal{D}}\left[yf_{\bm{W}^{(t)}}(\bm{X})>0\mid\bm{x}^ {(p^{*})}=\bm{v}_{y,k},k\in\mathcal{K}_{C}\cup\mathcal{K}_{R}\right]\geq 1-o \left(\frac{1}{\mathrm{poly}(d)}\right).\] (31)

**Case 2:**\(k\in\mathcal{K}_{E}\)

By triangular inequality and \(\phi^{\prime}\leq 1\), we have

\[\phi\left(\left\langle\bm{w}_{s}^{(t)},\bm{x}^{(\bar{p})} \right\rangle\right)-\phi\left(\left\langle\bm{w}_{-s}^{(t)},\bm{x}^{(\bar{p}) }\right\rangle\right)\] \[=\phi\left(\left\langle\bm{w}_{s}^{(t)},\alpha\bm{v}_{s,1} \right\rangle\right)-\phi\left(\left\langle\bm{w}_{-s}^{(t)},\alpha\bm{v}_{s,1}\right\rangle\right)\] \[\geq\phi\left(\left\langle\bm{w}_{s}^{(t)},\alpha\bm{v}_{s,1} \right\rangle\right)-\phi\left(\left\langle\bm{w}_{-s}^{(t)},\alpha\bm{v}_{s,1}\right\rangle\right)\] \[\quad-\left|\left\langle\bm{w}_{s}^{(t)},\bm{x}^{(\bar{p})}- \alpha\bm{v}_{s,1}\right\rangle\right|-\left|\left\langle\bm{w}_{-s}^{(t)}, \bm{x}^{(\bar{p})}-\alpha\bm{v}_{s,1}\right\rangle\right|.\]

In addition,

\[\phi\left(\left\langle\bm{w}_{s}^{(t)},\alpha\bm{v}_{s,1} \right\rangle\right)-\phi\left(\left\langle\bm{w}_{-s}^{(t)},\alpha\bm{v}_{s,1}\right\rangle\right)\] \[=\left(\phi\left(\alpha\gamma_{s}^{(t)}(s,1)\right)-\phi\left(- \alpha\gamma_{-s}^{(t)}(s,1)\right)\right)\] \[\quad+\left(\phi\left(\left\langle\bm{w}_{s}^{(t)},\alpha\bm{v}_ {s,1}\right\rangle\right)-\phi\left(\alpha\gamma_{s}^{(t)}(s,1)\right)\right)\] \[\quad-\left(\phi\left(\left\langle\bm{w}_{-s}^{(t)},\alpha\bm{v }_{s,1}\right\rangle\right)-\phi\left(-\alpha\gamma_{-s}^{(t)}(s,1)\right)\right)\] \[\geq\left(\phi\left(\alpha\gamma_{s}^{(t)}(s,1)\right)-\phi\left(- \alpha\gamma_{-s}^{(t)}(s,1)\right)\right)\] \[\quad-\alpha\left|\left\langle\bm{w}_{s}^{(t)},\bm{v}_{s,1} \right\rangle-\gamma_{s}^{(t)}(s,1)\right|-\alpha\left|\left\langle\bm{w}_{-s }^{(t)},\bm{v}_{s,1}\right\rangle+\gamma_{-s}^{(t)}(s,1)\right|\] \[=\alpha\left(\gamma_{s}^{(t)}(s,1)+\beta\gamma_{-s}^{(t)}(s,1) \right)-\alpha\cdot o\left(\frac{1}{\mathrm{polylog}(d)}\right)\] \[=\Omega(\alpha),\]

where the second equality is due to Lemma B.4 and (A8). If (29) holds, we have

\[\phi\left(\left\langle\bm{w}_{s}^{(t)},\bm{x}^{(\bar{p})}\right\rangle\right) -\phi\left(\left\langle\bm{w}_{-s}^{(\bar{p})},\bm{x}^{(\bar{p})}\right\rangle \right)=\Omega(\alpha)-o\left(\frac{\alpha}{\mathrm{polylog}(d)}\right)= \Omega(\alpha).\] (32)

Note that

\[yf_{\bm{W}^{(t)}}(\bm{X})\] \[=\phi\left(\left\langle\bm{w}_{y}^{(t)},\bm{v}_{y,k}\right\rangle \right)-\phi\left(\left\langle\bm{w}_{-y}^{(t)},\bm{v}_{y,k}\right\rangle \right)+\phi\left(\left\langle\bm{w}_{y}^{(t)},\bm{x}^{(\bar{p})}\right\rangle \right)-\phi\left(\left\langle\bm{w}_{-y}^{(t)},\bm{x}^{(\bar{p})}\right\rangle\right)\] \[\quad+\sum_{p\in[P]\setminus\{p^{*},\bar{p}\}}\left(\phi\left( \left\langle\bm{w}_{y}^{(t)},\bm{x}^{(p)}\right\rangle\right)-\phi\left( \left\langle\bm{w}_{-y}^{(t)},\bm{x}^{(p)}\right\rangle\right)\right),\]

and

\[\left|\phi\left(\left\langle\bm{w}_{y}^{(t)},\bm{v}_{y,k}\right\rangle \right)-\phi\left(\left\langle\bm{w}_{-y}^{(t)},\bm{v}_{y,k}\right\rangle \right)\right|\] \[\quad+\left|\sum_{p\in[P]\setminus\{p^{*},\bar{p}\}}\left(\phi \left(\left\langle\bm{w}_{y}^{(t)},\bm{x}^{(p)}\right\rangle\right)-\phi \left(\left\langle\bm{w}_{-y}^{(t)},\bm{x}^{(p)}\right\rangle\right)\right)\right|\] \[\leq\left|\left\langle\bm{w}_{y}^{(t)}-\bm{w}_{-y}^{(t)},\bm{v}_{y,k}\right\rangle\right|+o\left(\frac{\alpha}{\mathrm{polylog}(d)}\right)\]\[\leq\gamma_{1}^{(t)}(y,k)+\gamma_{-1}^{(t)}(y,k)+\left|\left\langle \boldsymbol{w}_{y}^{(0)}-\boldsymbol{w}_{-y}^{(0)},\boldsymbol{v}_{y,k}\right\rangle \right|+o\left(\frac{\alpha}{\operatorname{polylog}(d)}\right)\] \[\leq\mathcal{O}(\alpha^{2}\beta^{-2})+\widetilde{\mathcal{O}}( \sigma_{0})+o\left(\frac{\alpha}{\operatorname{polylog}(d)}\right)\] \[=o\left(\frac{\alpha}{\operatorname{polylog}(d)}\right)\] \[<\phi\left(\left\langle\boldsymbol{w}_{s}^{(t)},\boldsymbol{x}^ {(\bar{p})}\right\rangle\right)-\phi\left(\left\langle\boldsymbol{w}_{-s}^{( t)},\boldsymbol{x}^{(\bar{p})}\right\rangle\right),\]

where the first inequality is due to (28), the second-to-last line is due to (A8), (8), and (10), and the last inequality is due to (32). Therefore, we have \(yf_{\boldsymbol{W}^{(t)}}(\boldsymbol{X})>0\) if \(y=s\). Otherwise, \(yf_{\boldsymbol{W}^{(t)}}(\boldsymbol{X})<0\).

\[\mathbb{P}_{(\boldsymbol{X},y)\sim\mathcal{D}}\left[yf_{\boldsymbol{W}^{(t)}} (\boldsymbol{X})>0\mid\boldsymbol{x}^{(p^{*})}=\boldsymbol{v}_{y,k},k\in \mathcal{K}_{E}\right]=\frac{1}{2}\pm o\left(\frac{1}{\operatorname{poly}(d)} \right).\] (33)

Hence, combining (31) and (33) implies

\[\mathbb{P}_{(\boldsymbol{X},y)\sim\mathcal{D}}\left[yf_{\boldsymbol {W}^{(t)}}(\boldsymbol{X})>0\right] =\sum_{k\in\mathcal{K}_{c}\cup\mathcal{K}_{R}}\rho_{k}+\frac{1}{ 2}\left(1-\sum_{k\in\mathcal{K}_{c}\cup\mathcal{K}_{R}}\rho_{k}\right)\pm o \left(\frac{1}{\operatorname{poly}(d)}\right)\] \[=1-\frac{1}{2}\sum_{k\in\mathcal{K}_{E}}\rho_{k}\pm o\left(\frac {1}{\operatorname{poly}(d)}\right).\]

\(\Box\)Proof for CutMix

### Proof of Lemma b.3 for CutMix

For each \(i,j\in[n]\) and \(\mathcal{S}\subset[P]\), let

\[g_{i,j,\mathcal{S}}^{(t)}:=-\frac{|\mathcal{S}|}{P}y_{i}\ell^{\prime}\big{(}y_{i }f_{\boldsymbol{W}^{(t)}}(\boldsymbol{X}_{i,j,\mathcal{S}})\big{)}-\left(1- \frac{|\mathcal{S}|}{P}\right)y_{j}\ell^{\prime}\big{(}y_{j}f_{\boldsymbol{W}^ {(t)}}(\boldsymbol{X}_{i,j,\mathcal{S}})\big{)}.\]

For \(s\in\{\pm 1\}\) and iterate \(t\),

\[\boldsymbol{w}_{s}^{(t+1)}-\boldsymbol{w}_{s}^{(t)}\] \[=-\eta\nabla_{\boldsymbol{w}_{s}}\mathcal{L}_{\mathrm{CutMix}} \left(\boldsymbol{W}^{(t)}\right)\] \[=\frac{\eta}{n^{2}}\sum_{i,j\in[n],p\in[P]\setminus\{p_{i}^{*}\}} \mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S}}}\left[sg_{i,j,\mathcal{S }}^{(t)}\left(\sum_{p\in\mathcal{S}}\phi^{\prime}\left(\left\langle\boldsymbol {w}_{s}^{(t)},\boldsymbol{x}_{i}^{(p)}\right\rangle\right)\boldsymbol{x}_{i}^ {(p)}+\sum_{p\notin\mathcal{S}}\phi^{\prime}\left(\left\langle\boldsymbol{w}_ {s}^{(t)},\boldsymbol{x}_{i}^{(p)}\right\rangle\right)\boldsymbol{x}_{j}^{(p) }\right)\right]\] \[=\frac{s\eta}{n^{2}}\sum_{s^{\prime}\in\{\pm 1\},k\in[K]}\sum_{i \in\mathcal{V}_{s^{\prime},k},j\in[n]}\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_ {\mathcal{S}}}\left[g_{i,j,\mathcal{S}}^{(t)}\mathbbm{1}_{p_{i}^{*}\in \mathcal{S}}+g_{j,i,\mathcal{S}}^{(t)}\mathbbm{1}_{p_{i}^{*}\notin\mathcal{S}} \right]\phi^{\prime}\left(\left\langle\boldsymbol{w}_{s}^{(t)},\boldsymbol{v}_ {s^{\prime},k}\right\rangle\right)\boldsymbol{v}_{s^{\prime},k}\] \[\quad+\frac{s\eta}{n^{2}}\sum_{i,j\in[n],p\in[P]\setminus\{p_{i} ^{*}\}}\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S}}}\left[g_{i,j, \mathcal{S}}^{(t)}\mathbbm{1}_{p\in\mathcal{S}}+g_{j,i,\mathcal{S}}^{(t)} \mathbbm{1}_{p\notin\mathcal{S}}\right]\phi^{\prime}\left(\left\langle \boldsymbol{w}_{s}^{(t)},\boldsymbol{x}_{i}^{(p)}\right\rangle\right) \boldsymbol{x}_{i}^{(p)}.\]

Hence, if we define \(\gamma_{s}^{(t)}(s^{\prime},k)\)'s and \(\rho_{s}^{(t)}(i,p)\)'s recursively by using the rule

\[\gamma_{s}^{(t+1)}(s^{\prime},k)=\gamma_{s}^{(t)}(s^{\prime},k)+ \frac{s\prime^{\prime}\eta}{n^{2}}\sum_{i\in\mathcal{V}_{s^{\prime},k},j\in[n ]}\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S}}}\left[g_{i,j,\mathcal{ S}}^{(t)}\mathbbm{1}_{p_{i}^{*}\in\mathcal{S}}+g_{j,i,\mathcal{S}}^{(t)} \mathbbm{1}_{p_{i}^{*}\notin\mathcal{S}}\right]\phi^{\prime}\left(\left\langle \boldsymbol{w}_{s}^{(t)},\boldsymbol{v}_{s^{\prime},k}\right\rangle\right),\] \[\rho_{s}^{(t+1)}(i,p)=\rho_{s}^{(t)}(i,p)+\frac{s\!y_{i}\eta}{n^ {2}}\sum_{j\in[n]}\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S}}}\left[g _{i,j,\mathcal{S}}^{(t)}\mathbbm{1}_{p\in\mathcal{S}}+g_{j,i,\mathcal{S}}^{(t)} \mathbbm{1}_{p\notin\mathcal{S}}\right]\phi^{\prime}\left(\left\langle \boldsymbol{w}_{s}^{(t)},\boldsymbol{x}_{i}^{(p)}\right\rangle\right)\left\| \xi_{i}^{(p)}\right\|^{2},\]

starting from \(\gamma_{s}^{(0)}(s^{\prime}k)=\rho_{s}^{(0)}(i,p)=0\) for each \(s,s^{\prime}\in\{\pm 1\},k\in[K],i\in[n]\) and \(p\in[P]\setminus\{p_{i}^{*}\}\), then we have

\[\boldsymbol{w}_{s}^{(t)} =\boldsymbol{w}_{s}^{(0)}+\sum_{k\in[K]}\gamma_{s}^{(t)}(s,k) \boldsymbol{v}_{s,k}-\sum_{k\in[K]}\gamma_{s}^{(t)}(-s,k)\boldsymbol{v}_{-s,k}\] \[\quad+\sum_{\begin{subarray}{c}i\in\mathcal{V}_{s}\\ p\in[P]\setminus\{\tilde{p}_{i}\}\end{subarray}}\rho_{s}^{(t)}(i,p)\frac{\xi_{i }^{(p)}}{\left\|\xi_{i}^{(p)}\right\|^{2}}-\sum_{\begin{subarray}{c}i\in \mathcal{V}_{s^{\prime},s}\\ p\in[P]\setminus\{\tilde{p}_{i}\}\end{subarray}}\rho_{s}^{(t)}(i,p)\frac{\xi_{i }^{(p)}}{\left\|\xi_{i}^{(p)}\right\|^{2}}\] \[\quad+\alpha\left(\sum_{i\in\mathcal{F}_{s}}sy_{i}\rho_{s}^{(t)} (i,\tilde{p}_{i})\frac{\boldsymbol{v}_{s,1}}{\left\|\xi_{i}^{(\tilde{p}_{i})} \right\|^{2}}+\sum_{i\in\mathcal{F}_{-s}}sy_{i}\rho_{s}^{(t)}(i,\tilde{p}_{i}) \frac{\boldsymbol{v}_{-s,1}}{\left\|\xi_{i}^{(\tilde{p}_{i})}\right\|^{2}}\right),\]

for each \(s\in\{\pm 1\}\). 

### Proof of Theorem 3.3

We will prove that the conclusion of Theorem 3.3 holds when the event \(E_{\mathrm{init}}\) occurs. The proof of Theorem 3.3 is structured into the following six steps:

1. Introduce a reparametrization of the \(\mathsf{CutMix}\) loss \(\mathcal{L}_{\mathrm{CutMix}}(\boldsymbol{W})\) to a convex function \(h(\boldsymbol{Z})\) for ease of analysis (Section E.2.1).
2. Characterize a global minimum of \(h(\boldsymbol{Z})\) (Section E.2.2).
3. Evaluate strong convexity constant in the region near the global minimum of \(h(\boldsymbol{Z})\) (Section E.2.3).
4. Show that near stationary point of \(h(\boldsymbol{Z})\) is close to a global minimum (Section E.2.4).
5. Prove that gradient descent on the \(\mathsf{CutMix}\) loss \(\mathcal{L}_{\mathrm{CutMix}}(\boldsymbol{W})\) achieves a near-stationary point of the reparametrized function \(h(\boldsymbol{Z})\) and perfect accuracy on original training data (Section E.2.5).
6. Evaluate the test accuracy of a model in near-stationary point (Section E.2.6).

#### e.2.1 Reparametrization of CutMix Loss

It is complicated to characterize the stationary points of \(\mathsf{CutMix}\) loss \(\mathcal{L}_{\mathrm{CutMix}}(\bm{W})\) due to its non-convexity. We will overcome this problem by introducing reparameterization of the objective function. Let us define

\[z_{i}^{(p)}:=\phi\left(\left\langle\bm{w}_{1},\bm{x}_{i}^{(p)}\right\rangle \right)-\phi\left(\left\langle\bm{w}_{-1},\bm{x}_{i}^{(p)}\right\rangle\right),\]

for \(i\in[n],p\in[P]\) and

\[z_{s,k}:=\phi(\left\langle\bm{w}_{1},\bm{v}_{s,k}\right\rangle)-\phi(\left\langle \bm{w}_{-1},\bm{v}_{s,k}\right\rangle),\]

for each \(s\in\{\pm 1\},k\in[K]\). We can rewrite \(\mathsf{CutMix}\) loss \(\mathcal{L}_{\mathrm{CutMix}}(\bm{W})\) as a function \(h(\bm{Z})\) of the defined variables \(\bm{Z}:=\{z_{s,k}\}_{s\in\{\pm 1\},k\in[K]}\cup\{z_{i}^{(p)}\}_{i\in[n],p\in[P] \setminus\{p_{i}^{*}\}}\) as follows.

\[h(\bm{Z}):=\frac{1}{n^{2}}\sum_{i,j\in[n]}\mathbb{E}_{\mathcal{S }\sim\mathcal{D}_{S}}\left[\frac{|\mathcal{S}|}{P}\ell\left(y_{i}\left(\sum_{ p\in\mathcal{S}}z_{i}^{(p)}+\sum_{p\notin\mathcal{S}}z_{j}^{(p)}\right)\right)\right.\] \[\left.\qquad\qquad\qquad\qquad\qquad\qquad+\left(1-\frac{|\mathcal{ S}|}{P}\right)\ell\left(y_{j}\left(\sum_{p\in\mathcal{S}}z_{i}^{(p)}+\sum_{p \notin\mathcal{S}}z_{j}^{(p)}\right)\right)\,\right]\,,\]

where we write \(z_{i}^{(p_{i}^{*})}=z_{s,k}\) if \(i\in\mathcal{V}_{s,k}\). For notational simplicity, let us consider \(\bm{Z}\) as vectors in \(\mathbb{R}^{2K+n(P-1)}\) with the standard orthonormal basis \(\left\{\bm{e}_{s,k}\right\}_{s\in\{\pm 1\},k\in[K]}\cup\left\{\bm{e}_{i}^{(p)} \right\}_{i\in[n],p\in[P]\setminus\{p_{i}^{*}\}}\) which means

\[\bm{Z} =\{z_{s,k}\}_{s\in\{\pm 1\},k\in[K]}\cup\left\{z_{i}^{(p)} \right\}_{i\in[n],p\in[P]\setminus\{p_{i}^{*}\}}\] \[=\sum_{s\in\{\pm 1\},k\in[K]}z_{s,k}\bm{e}_{s,k}+\sum_{i\in[n],p \in[P]\setminus\{p_{i}^{*}\}}z_{i}^{(p)}\bm{e}_{i}^{(p)}.\]

If there is no confusion, we will use \(\bm{e}_{i}^{(p_{i}^{*})}\) to represent \(\bm{e}_{s,k}\), for \(i\in\mathcal{V}_{s,k}\).

By the chain rule,

\[\nabla_{\bm{W}}\mathcal{L}_{\mathrm{CutMix}}(\bm{W})=\bm{J}(\bm{W})\nabla_{\bm {Z}}h(\bm{Z}),\]

where each column of Jacobian matrix \(\bm{J}(\bm{W})\in\mathbb{R}^{2d\times(n(P-1)+2K)}\) is

\[\nabla_{\bm{W}}z_{s,k}=\begin{pmatrix}\phi^{\prime}(\left\langle\bm{w}_{1}, \bm{v}_{s,k}\right\rangle)\bm{v}_{s,k}\\ -\phi^{\prime}(\left\langle\bm{w}_{-1},\bm{v}_{s,k}\right\rangle)\bm{v}_{s,k} \end{pmatrix}\in\mathbb{R}^{2d},\nabla_{\bm{W}}z_{i}^{(p)}=\begin{pmatrix}\phi^ {\prime}\left(\left\langle\bm{w}_{1},\bm{x}_{i}^{(p)}\right\rangle\right)\bm{x }_{i}^{(p)}\\ -\phi^{\prime}\left(\left\langle\bm{w}_{-1},\bm{x}_{i}^{(p)}\right\rangle \right)\bm{x}_{i}^{(p)}\end{pmatrix}\in\mathbb{R}^{2d}.\]

Let us characterize the smallest singular value \(\sigma_{\min}(\bm{J}(\bm{W}))\) of the Jacobian matrix \(\bm{J}(\bm{W})\). For any unit vector \(\bm{c}=\{c_{s,k}\}_{s\in\{\pm 1\},k\in[K]}\cup\left\{c_{i}^{(p)}\right\}_{i\in[n],p\in[P] \setminus\{p_{i}^{*}\}}\in\mathbb{R}^{2K+n(P-1)}\), we have

\[\|\bm{J}(\bm{W})\bm{c}\|^{2} =\sum_{s\in\{\pm 1\},k\in[K]}c_{s,k}^{2}\left\|\nabla_{\bm{W}}z_{s,k} \right\|^{2}+\sum_{i\in[n],p\in[P]\setminus\{p_{i}^{*}\}}\left(c_{i}^{(p)} \right)^{2}\left\|\nabla_{\bm{W}}z_{i}^{(p)}\right\|^{2}\] \[+\sum_{\begin{subarray}{c}s_{1},s_{2}\in\{\pm 1\},k_{1},k_{2}\in[K] \\ (s_{1},k_{1})\neq(s_{2},k_{2})\end{subarray}}c_{s_{1},k_{1}}c_{s_{2},k_{2}} \langle\nabla_{\bm{W}}z_{s_{1},k_{1}},\nabla_{\bm{W}}z_{s_{2},k_{2}}\rangle\] \[+2\sum_{\begin{subarray}{c}s\in\{\pm 1\},k\in[K]\\ i\in[n],p\in[P]\setminus\{p_{i}^{*}\}\end{subarray}}c_{s,k}c_{i}^{(p)}\left\langle \nabla_{\bm{W}}z_{s,k},\nabla_{\bm{W}}z_{i}^{(p)}\right\rangle\] \[+\sum_{\begin{subarray}{c}i\in[n],p\in[P]\setminus\{p_{i}^{*}\} \\ j\in[n],q\in[P]\setminus\{p_{j}^{*}\}\\ (i,p)\neq(j,q)\end{subarray}}c_{i}^{(p)}c_{j}^{(q)}\left\langle\nabla_{\bm{W}}z_{ i}^{(p)},\nabla_{\bm{W}}z_{j}^{(q)}\right\rangle.\]For each \(s_{1},s_{2}\in\{\pm 1\},k_{1},k_{2}\in[K]\) such that \((s_{1},k_{1})\neq(s_{2},k_{2})\), and \(i\in[n],p\in[P]\setminus\{p_{i}^{*},\tilde{p}_{i}\}\),

\[\left\langle\nabla_{\boldsymbol{W}}z_{s_{1},k_{1}},\nabla_{\boldsymbol{W}}z_{s _{2},k_{2}}\right\rangle=\left\langle\nabla_{\boldsymbol{W}}z_{s_{1},k_{1}}, \nabla_{\boldsymbol{W}}z_{i}^{(p)}\right\rangle=0,\]

and if \(k_{1}>1\)

since \(\left\langle\boldsymbol{v}_{s_{1},k_{1}},\boldsymbol{v}_{s_{2},k_{2}}\right\rangle =\left\langle\boldsymbol{v}_{s_{1},k_{1}},\xi_{i}^{(p)}\right\rangle=\left\langle \boldsymbol{v}_{s_{1},k_{1}},\xi_{i}^{(\tilde{p}_{i})}\right\rangle=0\). Also, for each \(s\in\{\pm 1\}\) and \(i\in\mathcal{F}_{s}\), then

\[2\left|c_{s,1}c_{i}^{(\tilde{p}_{i})}\left\langle\nabla_{ \boldsymbol{W}}z_{s,1},\nabla_{\boldsymbol{W}}z_{i}^{(\tilde{p}_{i})}\right\rangle\right|\] \[=2\left|c_{s,1}c_{i}^{(\tilde{p}_{i})}\right|\left(\phi^{\prime}( \left\langle\boldsymbol{w}_{1},\boldsymbol{v}_{s,1}\right\rangle)\phi^{\prime} \left(\left\langle\boldsymbol{w}_{1},\boldsymbol{x}_{i}^{(\tilde{p}_{i})} \right\rangle\right)+\phi^{\prime}(\left\langle\boldsymbol{w}_{-1}, \boldsymbol{v}_{s,1}\right\rangle)\phi^{\prime}\left(\left\langle\boldsymbol{w }_{-1},\boldsymbol{x}_{i}^{(\tilde{p}_{i})}\right\rangle\right)\right)\] \[\leq 4c_{s,1}^{2}\left(\phi^{\prime}(\left\langle\boldsymbol{w}_{1}, \boldsymbol{v}_{s,1}\right\rangle)^{2}+\phi^{\prime}(\left\langle\boldsymbol{w }_{-1},\boldsymbol{v}_{s,1}\right\rangle)^{2}\right)\frac{\alpha^{2}}{\left\| \boldsymbol{x}_{i}^{(\tilde{p}_{i})}\right\|^{2}}\] \[\leq\frac{1}{2n}c_{s,1}^{2}\left(\phi^{\prime}(\left\langle \boldsymbol{w}_{1},\boldsymbol{v}_{s,1}\right\rangle)^{2}+\phi^{\prime}( \left\langle\boldsymbol{w}_{-1},\boldsymbol{v}_{s,1}\right\rangle)^{2}\right)\]

where the last inequality holds since

\[\left\|\boldsymbol{x}_{i}^{(\tilde{p}_{i})}\right\|^{2}=\alpha^{2}+\left\| \xi_{i}^{(\tilde{p}_{i})}\right\|^{2}\geq\frac{1}{2}\sigma_{\mathrm{d}}^{2}d =\omega(n\alpha^{2}),\]

where we apply the fact from the event \(E_{\mathrm{init}}\) defined in Lemma B.2 and (A7). Also,

\[\left\langle\nabla_{\boldsymbol{W}}z_{-s,1},\nabla_{\boldsymbol{W}}z_{i}^{( \tilde{p}_{i})}\right\rangle=0.\]

Furthermore, for each \(i,j\in[n],p\in[P]\setminus\{p_{i}^{*}\},q\in[P]\setminus\{p_{j}^{*}\}\) with \((i,p)\neq(j,q)\) satisfies

\[\left|c_{i}^{(p)}c_{j}^{(q)}\left\langle\nabla_{\boldsymbol{W}}z_{ i}^{(p)},\nabla_{\boldsymbol{W}}z_{j}^{(q)}\right\rangle\right|\] \[=\left|c_{i}^{(p)}c_{j}^{(q)}\right|\left(\phi^{\prime}\left( \left\langle\boldsymbol{w}_{1},\boldsymbol{x}_{i}^{(p)}\right\rangle\right) \phi^{\prime}\left(\left\langle\boldsymbol{w}_{1},\boldsymbol{x}_{j}^{(q)} \right\rangle\right)+\phi^{\prime}\left(\left\langle\boldsymbol{w}_{-1}, \boldsymbol{x}_{i}^{(p)}\right\rangle\right)\phi^{\prime}\left(\left\langle \boldsymbol{w}_{-1},\boldsymbol{x}_{j}^{(q)}\right\rangle\right)\phi^{\prime} \left(\left\langle\boldsymbol{w}_{-1},\boldsymbol{x}_{j}^{(q)}\right\rangle \right)\right)\left|\left\langle\boldsymbol{x}_{i}^{(p)},\boldsymbol{x}_{j}^{( q)}\right\rangle\right|\] \[\leq\frac{1}{4Pn}\left(c_{i}^{(p)}\right)^{2}\left(\phi^{\prime} \left(\left\langle\boldsymbol{w}_{1},\boldsymbol{x}_{i}^{(q)}\right\rangle \right)^{2}+\phi^{\prime}\left(\left\langle\boldsymbol{w}_{-1},\boldsymbol{x}_ {i}^{(p)}\right\rangle\right)^{2}\right)\left\|\boldsymbol{x}_{i}^{(p)}\right\| ^{2}\] \[\quad+\frac{1}{4Pn}\left(c_{j}^{(q)}\right)^{2}\left(\phi^{\prime} \left(\left\langle\boldsymbol{w}_{1},\boldsymbol{x}_{j}^{(q)}\right\rangle \right)^{2}+\phi^{\prime}\left(\left\langle\boldsymbol{w}_{-1},\boldsymbol{x}_ {j}^{(q)}\right\rangle\right)^{2}\right)\left\|\boldsymbol{x}_{j}^{(q)}\right\| ^{2}\] \[=\frac{1}{4Pn}\left(\left(c_{i}^{(p)}\right)^{2}\left\| \nabla_{\boldsymbol{W}}z_{i}^{(p)}\right\|^{2}+\left(c_{j}^{(q)}\right)^{2} \left\|\nabla_{\boldsymbol{W}}z_{j}^{(q)}\right\|^{2}\right)\]

where the last inequality is due to AM-GM inequality and

\[\left\|\boldsymbol{x}_{i}^{(p)}\right\|\cdot\left\|\boldsymbol{x}_{j}^{(q)} \right\|\geq 2nP\left|\left\langle\boldsymbol{x}_{i}^{(p)},\boldsymbol{x}_{j}^{( q)}\right\rangle\right|,\]

which we show through a case analysis. For the case \(p=\tilde{p}_{i}\) and \(q=\tilde{p}_{j}\), this inequality holds since

\[\left\|\boldsymbol{x}_{i}^{(p)}\right\|\cdot\left\|\boldsymbol{x}_{j}^{(q)} \right\|\geq\left\|\xi_{i}^{(p)}\right\|\cdot\left\|\xi_{j}^{(q)}\right\|\geq 2nP \left(\left|\left\langle\xi_{i}^{(p)},\xi_{j}^{(q)}\right\rangle\right|+\alpha^{2} \right)\geq 2nP\left|\left\langle\boldsymbol{x}_{i}^{(p)},\boldsymbol{x}_{j}^{(q)} \right\rangle\right|,\]

where the second inequality is due to

\[\frac{1}{2}\left\|\xi_{i}^{(p)}\right\|\cdot\left\|\xi_{j}^{(q)}\right\|\geq 2nP \left|\left\langle\xi_{i}^{(p)},\xi_{j}^{(q)}\right\rangle\right|,\quad\frac{1}{2} \left\|\xi_{i}^{(p)}\right\|\cdot\left\|\xi_{j}^{(q)}\right\|\geq 2nP\alpha^{2}.\]

which is implied by the fact from the event \(E_{\mathrm{init}}\) defined in Lemma B.2, (A1), (A2), and (A7). In the remaining case,

\[\left\|\boldsymbol{x}_{i}^{(p)}\right\|\cdot\left\|\boldsymbol{x}_{j}^{(q)} \right\|\geq\left\|\xi_{i}^{(p)}\right\|\cdot\left\|\xi_{j}^{(q)}\right\|\geq 2nP \left|\left\langle\xi_{i}^{(p)},\xi_{j}^{(q)}\right\rangle\right|=2nP\left| \left\langle\boldsymbol{x}_{i}^{(p)},\boldsymbol{x}_{j}^{(q)}\right\rangle \right|,\]where the second inequality is due to the fact from event \(E_{\mathrm{init}}\) defined in Lemma B.2, (A1), and (A2). For \(s\in\{\pm 1\},k\in[K]\) and \(i\in[n],p\in[P]\setminus\{p_{i}^{*}\}\),

\[\|\nabla_{\bm{W}}z_{s,k}\|^{2}=\phi^{\prime}(\left<\bm{w}_{1},\bm{ v}_{s,k}\right>)^{2}+\phi^{\prime}(\left<\bm{w}_{-1},\bm{v}_{s,k}\right>)^{2} \geq 2\beta^{2},\]

and

\[\left\|\nabla_{\bm{W}}z_{i}^{(p)}\right\|^{2} =\left(\phi^{\prime}\left(\left<\bm{w}_{1},\bm{x}_{i}^{(p)} \right>\right)^{2}+\phi^{\prime}\left(\left<\bm{w}_{-1},\bm{x}_{i}^{(p)} \right>\right)^{2}\right)\left\|\bm{x}_{i}^{(p)}\right\|^{2}\] \[\geq\beta^{2}\sigma_{i,p}^{2}d\] \[\geq\beta^{2},\]

where the last inequality is due to (8). By merging all inequalities together, we have

\[\|\bm{J}(\bm{W})\bm{c}\|^{2}\] \[=\sum_{s\in\{\pm 1\},k\in[K]}c_{s,k}^{2}\|\nabla_{\bm{W}}z_{s,k}\|^ {2}+\sum_{i\in[n],p\in[P]\setminus\{p_{i}^{*}\}}\left(c_{i}^{(p)}\right)^{2} \left\|\nabla_{\bm{W}}z_{i}^{(p)}\right\|^{2}\] \[\geq\sum_{s\in\{\pm 1\},k\in[K]}c_{s,k}^{2}\|\nabla_{\bm{W}}z_{s,k} \|^{2}+\sum_{i\in[n],p\in[P]\setminus\{p_{i}^{*}\}}\left(c_{i}^{(p)}\right)^{2 }\left\|\nabla_{\bm{W}}z_{i}^{(p)}\right\|^{2}\] \[-\sum_{s\in\{\pm 1\},i\in\mathcal{F}_{s}}\left(\frac{1}{2n}c_{s,1}^{2 }\|\nabla_{\bm{W}}z_{s,1}\|^{2}+\frac{1}{4}\left(c_{i}^{(\tilde{p}_{i})}\right) ^{2}\left\|\nabla_{\bm{W}}z_{i}^{(\tilde{p}_{i})}\right\|^{2}\right)\] \[-\frac{1}{4Pn}\sum_{\begin{subarray}{c}i\in[n],p\in[P]\setminus\{ p_{i}^{*}\}\\ j\in[n],q\in[P]\setminus\{p_{j}^{*}\}\\ (i,p)\neq(j,q)\end{subarray}}\left(\left(c_{i}^{(p)}\right)^{2}\left\|\nabla_{ \bm{W}}z_{i}^{(p)}\right\|^{2}+\left(c_{j}^{(q)}\right)^{2}\left\|\nabla_{\bm{ W}}z_{j}^{(q)}\right\|^{2}\right)\] \[>\frac{1}{4}\sum_{s\in\{\pm 1\},k\in[K]}c_{s,k}^{2}\|\nabla_{\bm{W}}z _{s,k}\|^{2}+\frac{1}{4}\sum_{i\in[n],p\in[P]\setminus\{p_{i}^{*}\}}\left(c_{ i}^{(p)}\right)^{2}\left\|\nabla_{\bm{W}}z_{i}^{(p)}\right\|^{2}\geq\frac{\beta^{2}}{4},\]

and we conclude \(\sigma_{\min}(\bm{J}(\bm{W}))\geq\frac{\beta}{2}\) for any \(\bm{W}\).

#### e.2.2 Characterization of a Global Minimum of CutMix Loss

In this section, we will check that \(h(\bm{Z})\) is strictly convex and it has a global minimum.

For each \(i,j\in[n]\) and \(\mathcal{S}\subset[P]\) let us define \(\bm{a}_{i,j,\mathcal{S}}\in\mathbb{R}^{2K+n(P-1)}\) as

\[\bm{a}_{i,j,\mathcal{S}}=\sum_{p\in\mathcal{S}}\bm{e}_{i}^{(p)}+\sum_{p\notin \mathcal{S}}\bm{e}_{j}^{(p)},\]

and then

\[h(\bm{Z})=\frac{1}{n^{2}}\sum_{i,j\in[n]}\mathbb{E}_{\mathcal{S} \sim\mathcal{D}_{\mathcal{S}}}\left[\frac{|\mathcal{S}|}{P}\ell\left(y_{i} \langle\bm{a}_{i,j,\mathcal{S}},\bm{Z}\rangle\right)+\left(1-\frac{|\mathcal{ S}|}{P}\right)\ell\left(y_{j}\langle\bm{a}_{i,j,\mathcal{S}},\bm{Z}\rangle \right)\right].\]

Since \(\ell(\cdot)\) is convex, \(h(\bm{Z})\) is also convex. Note that

\[\nabla h(\bm{Z})=\frac{1}{n^{2}}\sum_{i,j\in[n]}\mathbb{E}_{ \mathcal{S}\sim\mathcal{D}_{\mathcal{S}}}\left[\left(\frac{|\mathcal{S}|}{P}y_ {i}\ell^{\prime}(y_{i}\langle\bm{a}_{i,j,\mathcal{S}},\bm{Z}\rangle)+\left( 1-\frac{|\mathcal{S}|}{P}\right)y_{j}\ell^{\prime}(y_{j}\langle\bm{a}_{i,j, \mathcal{S}},\bm{Z}\rangle)\right)\bm{a}_{i,j,\mathcal{S}}\right],\]

and

\[\nabla^{2}h(\bm{Z})\]\[=\frac{1}{n^{2}}\sum_{i,j\in[n]}\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_ {\mathcal{S}}}\left[\left(\frac{|\mathcal{S}|}{P}\ell^{\prime\prime}(y_{i} \langle\bm{a}_{i,j,\mathcal{S}},\bm{Z}\rangle)+\left(1-\frac{|\mathcal{S}|}{P} \right)\ell^{\prime\prime}(y_{j}\langle\bm{a}_{i,j,\mathcal{S}},\bm{Z}\rangle) \right)\bm{a}_{i,j,\mathcal{S}}\bm{a}_{i,j,\mathcal{S}}^{\top}\right]\] \[=\frac{1}{n^{2}}\sum_{i,j\in[n]}\mathbb{E}_{\mathcal{S}\sim \mathcal{D}_{\mathcal{S}}}\left[\ell^{\prime\prime}(\langle\bm{a}_{i,j, \mathcal{S}},\bm{Z}\rangle)\bm{a}_{i,j,\mathcal{S}}\bm{a}_{i,j,\mathcal{S}}^{ \top}\right],\]

where the last equality holds since \(\ell^{\prime\prime}(z)=\ell^{\prime\prime}(-z)\) for any \(z\in\mathbb{R}\). From the equation above, it suffices to show that \(\{\bm{a}_{i,j,\mathcal{S}}\}_{i,j\in[n],\mathcal{S}\subset[P]}\) spans \(\mathbb{R}^{2K+n(P-1)}\) to show strict convexity of \(h(\bm{Z})\).

We define a function \(I:[P]\rightarrow[n]\) such that for each \(p\in[P]\), \(p_{I(p)}^{*}=p\) with \(\bm{x}_{I(p)}^{(p)}=\bm{v}_{1,1}\), where the existence is guaranteed by Lemma B.2 (but not necessarily unique). Then for any \(i\in[n]\) and \(p\in[p]\), we have

\[\bm{a}_{i,i,\emptyset}+\sum_{q\in[P]\setminus\{p\}}\bm{a}_{I(q),i, \{q\}}-(P-1)\bm{a}_{I(p),i,\{p\}}\] \[=\sum_{p^{\prime}\in[P]}\bm{e}_{i}^{(p^{\prime})}+\sum_{q\in[P] \setminus\{p\}}\left(\bm{e}_{1,1}+\sum_{p^{\prime}\in[P]\setminus\{q\}}\bm{e} _{i}^{(p^{\prime})}\right)-(P-1)\left(\bm{e}_{1,1}+\sum_{p^{\prime}\in[P] \setminus\{p\}}\bm{e}_{i}^{(p^{\prime})}\right)\] \[=\sum_{p^{\prime}\in[P]}\bm{e}_{i}^{(p^{\prime})}+\left((P-1)\bm {e}_{i}^{(p)}+(P-2)\sum_{p^{\prime}\in[P]\setminus\{p\}}\bm{e}_{i}^{(p^{ \prime})}\right)-(P-1)\sum_{p^{\prime}\in[P]\setminus\{p\}}\bm{e}_{i}^{(p^{ \prime})}\] \[=P\bm{e}_{i}^{(p)}.\] (34)

Hence, \(\{\bm{a}_{i,j,\mathcal{S}}\}_{i,j\in[n],\mathcal{S}\subset[P]}\) spans \(\mathbb{R}^{2K+n(P-1)}\) and \(h(\bm{Z})\) is strictly convex. Thus, it can have at most one global minimum. We want to show the existence of the global minimum and characterize it.

\[n^{2}\nabla h(\bm{Z})\] \[=\sum_{i,j\in[n]}\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{ \mathcal{S}}}\left[\left(\frac{|\mathcal{S}|}{P}y_{i}\ell^{\prime}(y_{i} \langle\bm{a}_{i,j,\mathcal{S}},\bm{Z}\rangle)+\left(1-\frac{|\mathcal{S}|}{P} \right)y_{j}\ell^{\prime}(y_{j}\langle\bm{a}_{i,j,\mathcal{S}},\bm{Z}\rangle) \right)\bm{a}_{i,j,\mathcal{S}}\right]\] \[=2\sum_{\begin{subarray}{c}i,j\in[n]\\ p\in[P]\end{subarray}}\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S}}} \left[\left(\frac{|\mathcal{S}|}{P}y_{i}\ell^{\prime}(y_{i}\langle\bm{a}_{i,j,\mathcal{S}},\bm{Z}\rangle)+\left(1-\frac{|\mathcal{S}|}{P}\right)y_{j}\ell^ {\prime}(y_{j}\langle\bm{a}_{i,j,\mathcal{S}},\bm{Z}\rangle)\right)\mathds{1} _{p\in\mathcal{S}}\right]\bm{e}_{i}^{(p)}.\]

We can simplify terms as

\[\sum_{j\in[n]}\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S} }}\left[\left(\frac{|\mathcal{S}|}{P}y_{i}\ell^{\prime}(y_{i}\langle\bm{a}_{i,j,\mathcal{S}},\bm{Z}\rangle)+\left(1-\frac{|\mathcal{S}|}{P}\right)y_{j}\ell^ {\prime}(y_{j}\langle\bm{a}_{i,j,\mathcal{S}},\bm{Z}\rangle)\right)\mathds{1} _{p\in\mathcal{S}}\right]\] \[=\sum_{j\in\mathcal{V}_{y_{i}}}\mathbb{E}_{\mathcal{S}\sim \mathcal{D}_{\mathcal{S}}}\left[\left(\frac{|\mathcal{S}|}{P}y_{i}\ell^{ \prime}(y_{i}\langle\bm{a}_{i,j,\mathcal{S}},\bm{Z}\rangle)+\left(1-\frac{| \mathcal{S}|}{P}\right)y_{j}\ell^{\prime}(y_{j}\langle\bm{a}_{i,j,\mathcal{S}},\bm{Z}\rangle)\right)\mathds{1}_{p\in\mathcal{S}}\right]\] \[=y_{i}\sum_{j\in\mathcal{V}_{y_{i}}}\mathbb{E}_{\mathcal{S}\sim \mathcal{D}_{\mathcal{S}}}[\ell^{\prime}(y_{i}\langle\bm{a}_{i,j,\mathcal{S}}, \bm{Z}\rangle)\mathds{1}_{p\in\mathcal{S}}]\] \[\quad+y_{i}\sum_{j\in\mathcal{V}_{-y_{i}}}\mathbb{E}_{\mathcal{S} \sim\mathcal{D}_{\mathcal{S}}}\left[\left(\ell^{\prime}(y_{i}\langle\bm{a}_{i,j,\mathcal{S}},\bm{Z}\rangle)+\left(1-\frac{|\mathcal{S}|}{P}\right)\right) \mathds{1}_{p\in\mathcal{S}}\right]\] \[=y_{i}|\mathcal{V}_{-y_{i}}|\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{ \mathcal{S}}}\left[\left(1-\frac{|\mathcal{S}|}{P}\right)\mathds{1}_{p\in \mathcal{S}}\right]+y_{i}\sum_{j\in[n]}\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{ \mathcal{S}}}[\ell^{\prime}(y_{i}\langle\bm{a}_{i,j,\mathcal{S}},\bm{Z}\rangle) \mathds{1}_{p\in\mathcal{S}}],\]

where the second equality holds since \(\ell^{\prime}(z)+\ell^{\prime}(-z)=-1\). Also, for any \(p\in[P]\),

\[\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S}}}\left[\left(1-\frac{| \mathcal{S}|}{P}\right)\mathds{1}_{p\in\mathcal{S}}\right]=\frac{1}{P}\sum_{q\in[P ]}\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S}}}\left[\left(1-\frac{| \mathcal{S}|}{P}\right)\mathds{1}_{q\in\mathcal{S}}\right]\]\[=\frac{1}{P}\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S}}} \left[\left(1-\frac{|\mathcal{S}|}{P}\right)\sum_{q\in\mathcal{S}}\mathbbm{1}_{q \in\mathcal{S}}\right]\] \[=\frac{1}{P}\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S}}} \left[\left(1-\frac{|\mathcal{S}|}{P}\right)|\mathcal{S}|\right]=\frac{P-1}{6P}.\]

Hence, if

\[\sum_{j\in[n]}\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S}}}[\ell^{ \prime}(y_{i}\langle\bm{a}_{i,j,\mathcal{S}},\bm{Z}\rangle)\mathbbm{1}_{p\in \mathcal{S}}]+\frac{P-1}{6P}|\mathcal{V}_{-y_{i}}|=0,\]

for all \(i\in[n]\) and \(p\in[P]\), then we have \(\nabla h(\bm{Z})=0\). Let us consider a specific \(\bm{Z}\) parameterized by \(z_{1},z_{-1}\), of the form \(z_{i}^{(p)}=y_{i}z_{y_{i}}\) for all \(i\in[n]\) and \(p\in[P]\). We will find a stationary point with this specific form and then it should be the unique global minimum in the entire domain. Then for each \(i\in[n]\) and \(p\in[P]\), we have

\[\sum_{j\in[n]}\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S} }}[\ell^{\prime}(y_{i}\langle\bm{a}_{i,j,\mathcal{S}},\bm{Z}\rangle)\mathbbm{1 }_{p\in\mathcal{S}}]\] \[=\sum_{j\in\mathcal{V}_{y_{i}}}\mathbb{E}_{\mathcal{S}\sim \mathcal{D}_{\mathcal{S}}}[\ell^{\prime}(y_{i}\langle\bm{a}_{i,j,\mathcal{S}},\bm{Z}\rangle)\mathbbm{1}_{p\in\mathcal{S}}]+\sum_{j\in\mathcal{V}_{-y_{i}}} \mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S}}}[\ell^{\prime}(y_{i} \langle\bm{a}_{i,j,\mathcal{S}},\bm{Z}\rangle)\mathbbm{1}_{p\in\mathcal{S}}]\] \[=|\mathcal{V}_{y_{i}}|\cdot\mathbb{E}_{\mathcal{S}\sim\mathcal{D} _{\mathcal{S}}}[\ell^{\prime}(Pz_{y_{i}})\mathbbm{1}_{p\in\mathcal{S}}]+| \mathcal{V}_{-y_{i}}|\cdot\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S} }}[\ell^{\prime}(|\mathcal{S}|z_{y_{i}}-(P-|\mathcal{S}|)z_{-y_{i}})\mathbbm{ 1}_{p\in\mathcal{S}}]\] \[=\frac{1}{P}\left(|\mathcal{V}_{y_{i}}|\cdot\mathbb{E}_{\mathcal{ S}\sim\mathcal{D}_{\mathcal{S}}}\left[\ell^{\prime}(Pz_{y_{i}})\sum_{q\in \mathcal{S}}\mathbbm{1}_{q\in\mathcal{S}}\right]\right.\] \[\qquad\qquad+|\mathcal{V}_{-y_{i}}|\cdot\mathbb{E}_{\mathcal{S} \sim\mathcal{D}_{\mathcal{S}}}\left[\ell^{\prime}(|\mathcal{S}|z_{y_{i}}-(P-| \mathcal{S}|)z_{-y_{i}})\sum_{q\in\mathcal{S}}\mathbbm{1}_{q\in\mathcal{S}} \right]\right)\] \[=\frac{1}{P}\Big{(}|\mathcal{V}_{y_{i}}|\cdot\mathbb{E}_{\mathcal{ S}\sim\mathcal{D}_{\mathcal{S}}}[|\mathcal{S}|\ell^{\prime}(Pz_{y_{i}})]+| \mathcal{V}_{-y_{i}}|\cdot\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S} }}[|\mathcal{S}|\ell^{\prime}(|\mathcal{S}|z_{y_{i}}-(P-|\mathcal{S}|)z_{-y_{ i}})]\Big{)}\] \[=\frac{|\mathcal{V}_{y_{i}}|}{2}\ell^{\prime}(Pz_{y_{i}})+\frac{| \mathcal{V}_{-y_{i}}|}{P}\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S}}}[| \mathcal{S}|\ell^{\prime}(|\mathcal{S}|z_{y_{i}}-(P-|\mathcal{S}|)z_{-y_{i}})].\]

From Lemma F.4, there exists a unique minimizer \(\hat{\bm{Z}}=\{\hat{z}_{s,k}\}_{s\in\{\pm 1\},k\in[K]}\cup\left\{\hat{z}_{i}^{(p)} \right\}_{i\in[n],p\in[P]\setminus\{p_{i}^{*}\}}\) of \(h(\bm{Z})\) and it satisfies \(s\hat{z}_{s,k}=z_{s}^{*}=\Theta(1)\) for all \(k\in[K]\) and \(y_{i}\hat{z}_{i}^{(p)}=z_{y_{i}}^{*}=\Theta(1)\) for all \(i\in[n]\) and \(p\in[P]\setminus\{p_{i}^{*}\}\) due to (A1).

#### e.2.3 Strong Convexity Near Global Minimum

We will show that \(h(\bm{Z})\) is strongly convex in a set \(\mathcal{G}\) containing a global minimum \(\hat{\bm{Z}}\) where \(\mathcal{G}\) is defined as follows.

\[\mathcal{G}:=\left\{\bm{Z}\in\mathbb{R}^{2K+n(P-1)}:\|\bm{Z}-\hat{\bm{Z}}\|_{ \infty}<\|\hat{\bm{Z}}\|_{\infty}\right\},\]

here \(\|\cdot\|_{\infty}\) is \(\ell_{\infty}\) norm. For any \(\bm{Z}\in\mathcal{G}\) and a unit vector \(\bm{c}\in\mathbb{R}^{2K+n(P-1)}\) with \(\bm{c}=\sum_{s\in\{\pm 1\},k\in[K]}c_{s,k}\bm{e}_{s,k}+\sum_{i\in[n],p\in[P] \setminus\{p_{i}^{*}\}}c_{i}^{(p)}\bm{e}_{i}^{(p)}\), we have

\[\bm{c}^{\top}\nabla^{2}h(\bm{Z})\bm{c} =\frac{1}{n^{2}}\sum_{i,j\in[n]}\mathbb{E}_{\mathcal{S}\sim\mathcal{D }_{\mathcal{S}}}\left[\ell^{\prime\prime}(\langle\bm{a}_{i,j,\mathcal{S}},\bm{Z} \rangle)\langle\bm{a}_{i,j,\mathcal{S}},\bm{c}\rangle^{2}\right]\] \[\geq\frac{\ell^{\prime\prime}(2P\|\hat{\bm{Z}}\|_{\infty})}{n^{2}} \sum_{i,j\in[n]}\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S}}}[\langle \bm{a}_{i,j,\mathcal{S}},\bm{c}\rangle^{2}].\]Note that for each \(i\in[n],p\in[P]\), from (34), we have

\[c_{i}^{(p)}=\left\langle\bm{c},\bm{e}_{i}^{(p)}\right\rangle=\frac{1}{P}\langle \bm{c},\bm{a}_{i,i,\emptyset}\rangle+\frac{1}{P}\sum_{q\in[P]\setminus\{p\}} \langle\bm{c},\bm{a}_{I(q),i,\{q\}}\rangle-\frac{P-1}{P}\left\langle\bm{c},\bm {a}_{I(p),i,\{p\}}\right\rangle,\]

where we use the notational convention \(c_{i}^{(p_{i}^{*})}=c_{s,k}\) for \(s\in\{\pm 1\},k\in[K]\) and \(i\in\mathcal{V}_{s,k}\). By Cauchy-Schwartz inequality and the fact that \(\mathbb{P}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S}}}[\mathcal{S}=\emptyset], \mathbb{P}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S}}}[\mathcal{S}=\{q\}]\geq \frac{1}{P(P+1)}\) for all \(q\in[P]\),

\[\left(c_{i}^{(p)}\right)^{2}\] \[=\left(\frac{1}{P}\left\langle\bm{c},\bm{a}_{i,i,\emptyset} \right\rangle+\frac{1}{P}\sum_{q\in[P]\setminus\{p\}}\left\langle\bm{c},\bm{a }_{I(q),i,\{q\}}\right\rangle-\frac{P-1}{P}\left\langle\bm{c},\bm{a}_{I(p),i, \{p\}}\right\rangle\right)^{2}\] \[\leq\left(\frac{1}{P^{2}}+\frac{P-1}{P^{2}}+\left(-\frac{P-1}{P} \right)^{2}\right)P(P+1)\sum_{i,j\in[n]}\mathbb{E}_{\mathcal{S}\sim\mathcal{D} _{\mathcal{S}}}[\langle\bm{c},\bm{a}_{i,j,\mathcal{S}}\rangle^{2}]\] \[\leq 2P^{2}\sum_{i,j,\in[n]}\mathbb{E}_{\mathcal{S}\sim\mathcal{D} _{\mathcal{S}}}\left[\langle\bm{c},\bm{a}_{i,j,\mathcal{S}}\rangle^{2}\right].\]

Hence, we have

\[\bm{c}^{\top}\nabla^{2}h(\bm{Z})\bm{c} \geq\frac{\ell^{\prime\prime}(2P\|\hat{Z}\|_{\infty})}{(4K+2n(P-1 ))P^{2}n^{2}}(4K+2n(P-1))P^{2}\sum_{i,j\in[n]}\mathbb{E}_{\mathcal{S}\sim \mathcal{D}_{\mathcal{S}}}\left[\langle\bm{c},\bm{a}_{i,j,\mathcal{S}}\rangle ^{2}\right]\] \[\geq\frac{\ell^{\prime\prime}(2P\|\hat{Z}\|_{\infty})}{(4K+2n(P-1 ))P^{2}n^{2}}\left(\sum_{s\in\{\pm 1\},k\in[K]}c_{s,k}^{2}+\sum_{i\in[n],q\in[P] \setminus\{p_{i}^{\prime}\}}\left(c_{i}^{(q)}\right)^{2}\right)\] \[=\frac{\ell^{\prime\prime}(2P\|\hat{Z}\|_{\infty})}{(4K+2n(P-1))P^ {2}n^{2}},\]

and we conclude \(h(\bm{Z})\) is \(\mu\)-strongly convex in \(\mathcal{G}\) where \(\mu:=\frac{\ell^{\prime\prime}(2P\|\hat{Z}\|_{\infty})}{(4K+2n(P-1))P^{2}n^{ 2}}\). Due to (A1), (A2), and the fact that \(\|\hat{Z}\|_{\infty}=\Theta(1)\), we have \(\mu\geq\frac{1}{\mathrm{poly}(d)}\).

#### e.2.4 Near Stationary Points are Close to Global Minimum

In this step, we want to show that near stationary points of \(h(\bm{Z})\) are close to a global minimum \(\hat{\bm{Z}}\).

**Lemma E.1**.: _Suppose \(\bm{Z}\in\mathbb{R}^{2K+n(P-1)}\) satisfies \(\|\nabla h(\bm{Z})\|<\mu\epsilon\) with some \(0<\epsilon<\frac{\|\hat{\bm{Z}}\|_{\infty}}{2}\). Then, we have \(\left\|\bm{Z}-\hat{\bm{Z}}\right\|<\epsilon\)._

Proof of Lemma e.1.: If \(\bm{Z}=\hat{\bm{Z}}\), we immediately have our conclusion. We may assume \(\bm{Z}\neq\hat{\bm{Z}}\).

Let us define a function \(g:\mathbb{R}\rightarrow\mathbb{R}\) as \(g(t)=h\left(\hat{\bm{Z}}+t(\bm{Z}-\hat{\bm{Z}})\right)\). Then \(g\) is convex and

\[g^{\prime}(t) =\left\langle\nabla h\left(\hat{\bm{Z}}+t(\bm{Z}-\hat{\bm{Z}}) \right),\bm{Z}-\hat{\bm{Z}}\right\rangle,\] \[g^{\prime\prime}(t) =\left(\bm{Z}-\hat{\bm{Z}}\right)^{\top}\nabla^{2}h\left(\hat{ \bm{Z}}+t(\bm{Z}-\hat{\bm{Z}})\right)\left(\bm{Z}-\hat{\bm{Z}}\right).\]

Furthermore, for \(0\leq t\leq t_{0}\) where \(t_{0}:=\frac{\|\hat{\bm{Z}}\|}{2\|\bm{Z}-\hat{\bm{Z}}\|_{\infty}}\),

\[\hat{\bm{Z}}+t(\bm{Z}-\hat{\bm{Z}})\in\mathcal{G},\qquad\because g^{\prime \prime}(t)\geq\mu\left\|\bm{Z}-\hat{\bm{Z}}\right\|^{2}.\]We can conclude \(g\) is \(\mu\left\|\bm{Z}-\hat{\bm{Z}}\right\|^{2}\)-strongly convex in \([0,t_{0}]\). From strong convexity in \([0,t_{0}]\) and convexity in \(\mathbb{R}\), we have

\[(g^{\prime}(t_{0})-g^{\prime}(0))t_{0}=g^{\prime}(t_{0})t_{0}\geq\mu\left\|\bm{Z }-\hat{\bm{Z}}\right\|^{2}t_{0}^{2},\quad(g^{\prime}(1)-g^{\prime}(t_{0}))(1-t _{0})\geq 0.\]

If \(t_{0}<1\), we have

\[\left\|\nabla h(\bm{Z})\right\|\left\|\bm{Z}-\hat{\bm{Z}}\right\|\geq\left\langle \nabla h(\bm{Z}),\bm{Z}-\hat{\bm{Z}}\right\rangle=g^{\prime}(1)\geq g^{\prime} (t_{0})\geq\mu\left\|\bm{Z}-\hat{\bm{Z}}\right\|^{2}t_{0},\]

and

\[\left\|\nabla h(\bm{Z})\right\|\geq\mu\left\|\bm{Z}-\hat{\bm{Z}}\right\|t_{0} =\frac{\mu\left\|\bm{Z}-\hat{\bm{Z}}\right\|\left\|\hat{\bm{Z}}\right\|_{ \infty}}{2\left\|\bm{Z}-\hat{\bm{Z}}\right\|_{\infty}}\geq\frac{\mu\left\| \hat{\bm{Z}}\right\|_{\infty}}{2},\]

this is contradictory. Thus, we have \(t_{0}\geq 1\) and \(\bm{Z}\in\mathcal{G}\). From the strong convexity of \(h(\bm{Z})\) in \(\mathcal{G}\), we have

\[\mu\left\|\bm{Z}-\hat{\bm{Z}}\right\|\leq\left\|\nabla h(\bm{Z})-\nabla h(\hat {\bm{Z}})\right\|=\left\|\nabla h(\bm{Z})\right\|<\mu\epsilon,\]

and we have our conclusion \(\left\|\bm{Z}-\hat{\bm{Z}}\right\|<\epsilon\). 

#### e.2.5 Gradient Descent Achieves a Near Stationary Point

We will show that \(\mathcal{L}_{\mathrm{CutMix}}(\bm{W})\) is a smooth function.

**Lemma E.2**.: _Suppose the event \(E_{\mathrm{init}}\) occurs. CutMix Loss \(\mathcal{L}_{\mathrm{CutMix}}(\bm{W})\) is \(L\)-smooth with \(L=9r^{-1}P\sigma_{\mathrm{d}}^{2}d\)._

Proof of Lemma e.2.: Note that

\[\nabla_{\bm{w}_{1}}\mathcal{L}_{\mathrm{CutMix}}(\bm{W})\] \[=\frac{1}{n^{2}}\sum_{i,j\in[n]}\mathbb{E}_{\mathcal{S}\sim \mathcal{D}_{\mathcal{S}}}\Bigg{[}\left(\frac{|\mathcal{S}|}{P}y_{i}\ell^{ \prime}(y_{i}f_{\bm{W}}(\bm{X}_{i,j,\mathcal{S}}))+\left(1-\frac{|\mathcal{S} |}{P}\right)y_{j}\ell^{\prime}(y_{j}f_{\bm{W}}(\bm{X}_{i,j,\mathcal{S}}))\right)\] \[\quad\times\left(\sum_{p\in\mathcal{S}}\phi^{\prime}\left(\left\langle \bm{w}_{1},\bm{x}_{i}^{(p)}\right\rangle\right)\bm{x}_{i}^{(p)}+\sum_{p\notin \mathcal{S}}\phi^{\prime}\left(\left\langle\bm{w}_{1},\bm{x}_{j}^{(p)} \right\rangle\right)\bm{x}_{j}^{(p)}\right)\Bigg{]}\,.\]

Let \(\widetilde{\bm{W}}=\{\widetilde{\bm{w}}_{1},\widetilde{\bm{w}}_{-1}\}\) and \(\overline{\bm{W}}=\{\overline{\bm{w}}_{1},\overline{\bm{w}}_{-1}\}\) be any parameters of the neural network \(f_{\bm{W}}\). For any \(i,j\in[n]\) and \(\mathcal{S}\subset[P]\),

\[\left(\frac{|\mathcal{S}|}{P}y_{i}\ell^{\prime}(y_{i}f_{\widetilde{\bm{W}}}( \bm{X}_{i,j,\mathcal{S}}))+\left(1-\frac{|\mathcal{S}|}{P}\right)y_{j}\ell^{ \prime}(y_{j}f_{\widetilde{\bm{W}}}(\bm{X}_{i,j,\mathcal{S}}))\right)\] \[\qquad\qquad\qquad\qquad\times\left(\sum_{p\in\mathcal{S}}\phi^{ \prime}\left(\left\langle\overline{\bm{w}}_{1},\bm{x}_{i}^{(p)}\right\rangle \right)\bm{x}_{i}^{(p)}+\sum_{p\notin\mathcal{S}}\phi^{\prime}\left(\left\langle \overline{\bm{w}}_{1},\bm{x}_{j}^{(p)}\right\rangle\right)\bm{x}_{j}^{(p)}\right)\] \[= \left(\frac{|\mathcal{S}|}{P}y_{i}\ell^{\prime}(y_{i}f_{\widetilde{ \bm{W}}}(\bm{X}_{i,j,\mathcal{S}}))+\left(1-\frac{|\mathcal{S}|}{P}\right)y_{j} \ell^{\prime}(y_{j}f_{\widetilde{\bm{W}}}(\bm{X}_{i,j,\mathcal{S}}))\right)\] \[\qquad\qquad\qquad\qquad\times\left(\sum_{p\in\mathcal{S}}\phi^{ \prime}\left(\left\langle\widetilde{\bm{w}}_{1},\bm{x}_{i}^{(p)}\right\rangle \right)\bm{x}_{i}^{(p)}+\sum_{p\notin\mathcal{S}}\phi^{\prime}\left(\left\langle \widetilde{\bm{w}}_{1},\bm{x}_{j}^{(p)}\right\rangle\right)\bm{x}_{j}^{(p)}\right)\] \[-\left(\frac{|\mathcal{S}|}{P}y_{i}\ell^{\prime}(y_{i}f_{ \widetilde{\bm{W}}}(\bm{X}_{i,j,\mathcal{S}}))+\left(1-\frac{|\mathcal{S}|}{P} \right)y_{j}\ell^{\prime}(y_{j}f_{\widetilde{\bm{W}}}(\bm{X}_{i,j,\mathcal{S}}))\right)\] \[\qquad\qquad\qquad\qquad\qquad\times\left(\sum_{p\in\mathcal{S}} \phi^{\prime}\left(\left\langle\overline{\bm{w}}_{1},\bm{x}_{i}^{(p)}\right \rangle\right)\bm{x}_{i}^{(p)}+\sum_{p\notin\mathcal{S}}\phi^{\prime}\left( \left\langle\overline{\bm{w}}_{1},\bm{x}_{j}^{(p)}\right\rangle\right)\bm{x}_{j}^{( p)}\right)\]\[+\left(\frac{|\mathcal{S}|}{P}y_{i}\ell^{\prime}(y_{i}f_{\widetilde{ \boldsymbol{W}}}(\boldsymbol{X}_{i,j,\mathcal{S}}))+\left(1-\frac{|\mathcal{S}|}{P }\right)y_{i}\ell^{\prime}(y_{j}f_{\widetilde{\boldsymbol{W}}}(\boldsymbol{X}_ {i,j,\mathcal{S}}))\right)\] \[\qquad\qquad\qquad\qquad\qquad\qquad\times\left(\sum_{p\in \mathcal{S}}\phi^{\prime}\left(\left\langle\widetilde{\boldsymbol{w}}_{1}, \boldsymbol{x}_{i}^{(p)}\right\rangle\right)\boldsymbol{x}_{i}^{(p)}+\sum_{p \notin\mathcal{S}}\phi^{\prime}\left(\left\langle\widetilde{\boldsymbol{w}}_{1 },\boldsymbol{x}_{j}^{(p)}\right\rangle\right)\boldsymbol{x}_{j}^{(p)}\right)\] \[-\left(\frac{|\mathcal{S}|}{P}y_{i}\ell^{\prime}(y_{i}f_{ \widetilde{\boldsymbol{W}}}(\boldsymbol{X}_{i,j,\mathcal{S}}))+\left(1-\frac {|\mathcal{S}|}{P}\right)y_{i}\ell^{\prime}(y_{j}f_{\widetilde{\boldsymbol{W }}}(\boldsymbol{X}_{i,j,\mathcal{S}}))\right)\] \[\qquad\qquad\qquad\qquad\qquad\qquad\times\left(\sum_{p\in \mathcal{S}}\phi^{\prime}\left(\left\langle\widetilde{\boldsymbol{w}}_{1}, \boldsymbol{x}_{i}^{(p)}\right\rangle\right)\boldsymbol{x}_{i}^{(p)}+\sum_{p \notin\mathcal{S}}\phi^{\prime}\left(\left\langle\widetilde{\boldsymbol{w}}_{ 1},\boldsymbol{x}_{j}^{(p)}\right\rangle\right)\boldsymbol{x}_{j}^{(p)}\right).\]

Since \(|\ell^{\prime}|\leq 1\),

\[\left|\frac{|\mathcal{S}|}{P}y_{i}\ell^{\prime}\left(y_{i}f_{\widetilde{ \boldsymbol{W}}}(\boldsymbol{X}_{i,j,\mathcal{S}})\right)+\left(1-\frac{| \mathcal{S}|}{P}\right)y_{i}\ell^{\prime}\left(y_{j}f_{\widetilde{ \boldsymbol{W}}}(\boldsymbol{X}_{i,j,\mathcal{S}}))\right|\leq 1,\]

and since \(|\phi^{\prime}|\leq 1\),

\[\left\|\sum_{p\in\mathcal{S}}\phi^{\prime}\left(\left\langle\widetilde{ \boldsymbol{w}}_{1},\boldsymbol{x}_{i}^{(p)}\right\rangle\right)\boldsymbol{x} _{i}^{(p)}+\sum_{p\notin\mathcal{S}}\phi^{\prime}\left(\left\langle\widetilde{ \boldsymbol{w}}_{1},\boldsymbol{x}_{j}^{(p)}\right\rangle\right)\boldsymbol{x} _{j}^{(p)}\right)\right\|\leq P\max_{i\in[n],p\in[P]}\left\|\boldsymbol{x}_{i}^ {(p)}\right\|.\]

In addition, since \(\phi\) is \(r^{-1}\)-smooth,

\[\left\|\left(\sum_{p\in\mathcal{S}}\phi^{\prime}\left(\left\langle \widetilde{\boldsymbol{w}}_{1},\boldsymbol{x}_{i}^{(p)}\right\rangle\right) \boldsymbol{x}_{i}^{(p)}+\sum_{p\notin\mathcal{S}}\phi^{\prime}\left(\left\langle \widetilde{\boldsymbol{w}}_{1},\boldsymbol{x}_{j}^{(p)}\right\rangle\right) \boldsymbol{x}_{j}^{(p)}\right)\right.\] \[\leq\sum_{p\in\mathcal{S}}\left|\phi^{\prime}\left(\left\langle \widetilde{\boldsymbol{w}}_{1},\boldsymbol{x}_{i}^{(p)}\right\rangle\right)- \phi^{\prime}\left(\left\langle\widetilde{\boldsymbol{w}}_{1},\boldsymbol{x}_{j }^{(p)}\right\rangle\right)\right|\left\|\boldsymbol{x}_{i}^{(p)}\right\|\] \[\quad+\sum_{p\notin\mathcal{S}}\left|\phi^{\prime}\left(\left\langle \widetilde{\boldsymbol{w}}_{1},\boldsymbol{x}_{j}^{(p)}\right\rangle\right)- \phi^{\prime}\left(\left\langle\widetilde{\boldsymbol{w}}_{1},\boldsymbol{x}_{j }^{(p)}\right\rangle\right)\right|\left\|\boldsymbol{x}_{j}^{(p)}\right\|\] \[\leq r^{-1}\sum_{p\in\mathcal{S}}\left|\left\langle\widetilde{ \boldsymbol{w}}_{1}-\overline{\boldsymbol{w}}_{1},\boldsymbol{x}_{i}^{(p)} \right\rangle\right|\left\|\boldsymbol{x}_{i}^{(p)}\right\|+r^{-1}\sum_{p\notin \mathcal{S}}\left|\left\langle\widetilde{\boldsymbol{w}}_{1}-\overline{ \boldsymbol{w}}_{1},\boldsymbol{x}_{j}^{(p)}\right\rangle\right|\left\| \boldsymbol{x}_{j}^{(p)}\right\|\] \[\leq r^{-1}P\left(\max_{i\in[n],p\in[P]}\left\|\boldsymbol{x}_{i} ^{(p)}\right\|\right)^{2}\left\|\widetilde{\boldsymbol{w}}_{1}-\overline{ \boldsymbol{w}}_{1}\right\|,\]

and since \(\ell^{\prime}\) and \(\phi\) are \(1\)-Lipschitz, we have

\[\left|\left(\frac{|\mathcal{S}|}{P}y_{i}\ell^{\prime}(y_{i}f_{ \widetilde{\boldsymbol{W}}}(\boldsymbol{X}_{i,j,\mathcal{S}}))+\left(1-\frac{| \mathcal{S}|}{P}\right)y_{j}\ell^{\prime}(y_{j}f_{\widetilde{\boldsymbol{W}}}( \boldsymbol{X}_{i,j,\mathcal{S}}))\right)\right.\] \[\left.-\left(\frac{|\mathcal{S}|}{P}y_{i}\ell^{\prime}(y_{i}f_{ \widetilde{\boldsymbol{W}}}(\boldsymbol{X}_{i,j,\mathcal{S}}))+\left(1-\frac{| \mathcal{S}|}{P}\right)y_{j}\ell^{\prime}(y_{j}f_{\widetilde{\boldsymbol{W}}}( \boldsymbol{X}_{i,j,\mathcal{S}}))\right)\right|\] \[\leq|f_{\widetilde{\boldsymbol{W}}}(\boldsymbol{X}_{i,j,\mathcal{S }})-f_{\widetilde{\boldsymbol{W}}}(\boldsymbol{X}_{i,j,\mathcal{S}})|\] \[\leq\sum_{p\in\mathcal{S}}\left(\left|\left\langle\widetilde{ \boldsymbol{w}}_{1}-\overline{\boldsymbol{w}}_{1},\boldsymbol{x}_{i}^{(p)} \right\rangle\right|+\left|\left\langle\widetilde{\boldsymbol{w}}_{-1}- \overline{\boldsymbol{w}}_{-1},\boldsymbol{x}_{i}^{(p)}\right\rangle\right|\right)\] \[\quad+\sum_{p\notin\mathcal{S}}\left(\left|\left\langle \widetilde{\boldsymbol{w}}_{1}-\overline{\boldsymbol{w}}_{1},\boldsymbol{x}_{j}^{(p)} \right\rangle\right|+\left|\left\langle\widetilde{\boldsymbol{w}}_{-1}-\overline{ \boldsymbol{w}}_{-1},\boldsymbol{x}_{j}^{(p)}\right\rangle\right|\right)\] \[\leq P\max_{i\in[n],j\in[P]}\left\|\boldsymbol{x}_{i}^{(p)} \right\|\left(\left\|\widetilde{\boldsymbol{w}}_{1}-\overline{\boldsymbol{w}}_{1} \right\|+\left\|\widetilde{\boldsymbol{w}}_{-1}-\overline{\boldsymbol{w}}_{-1} \right\|\right)\] \[\leq\sqrt{2}P\max_{i\in[n],j\in[P]}\left\|\boldsymbol{x}_{i}^{(p)} \right\|\left\|\boldsymbol{\widetilde{\boldsymbol{W}}}-\overline{\boldsymbol{W}} \right\|.\]Therefore,

\[\left\|\nabla_{\bm{w}_{1}}\mathcal{L}_{\mathrm{CutMix}}(\bm{ \widetilde{W}})-\nabla_{\bm{w}_{1}}\mathcal{L}_{\mathrm{CutMix}}(\bm{ \widetilde{W}})\right\|\] \[\leq r^{-1}P\left(\max_{i\in[n],p\in[P]}\left\|\bm{x}_{i}^{(p)} \right\|\right)^{2}\left\|\bm{\widetilde{w}}_{1}-\bm{\overline{w}}_{1}\right\| +\sqrt{2}P^{2}\left(\max_{i\in[n],p\in[P]}\left\|\bm{x}_{i}^{(p)}\right\| \right)^{2}\left\|\bm{\widetilde{W}}-\bm{\overline{W}}\right\|\] \[\leq 2r^{-1}P\left(\max_{i\in[n],p\in[P]}\left\|\bm{x}_{i}^{(p)} \right\|\right)^{2}\left\|\bm{\widetilde{W}}-\bm{\overline{W}}\right\|,\]

where the last equality is due to (A1) and (A8). In the same way, we can obtain

\[\left\|\nabla_{\bm{w}_{-1}}\mathcal{L}_{\mathrm{CutMix}}(\bm{ \widetilde{W}})-\nabla_{\bm{w}_{-1}}\mathcal{L}_{\mathrm{CutMix}}(\bm{ \overline{W}})\right\|\leq 2r^{-1}P\left(\max_{i\in[n],p\in[P]}\left\|\bm{x}_{i} ^{(p)}\right\|\right)^{2}\left\|\bm{\widetilde{W}}-\bm{\overline{W}}\right\|,\]

and

\[\left\|\nabla\mathcal{L}_{\mathrm{CutMix}}(\bm{\widetilde{W}})- \nabla\mathcal{L}_{\mathrm{CutMix}}(\bm{\overline{W}})\right\| \leq 4r^{-1}P\left(\max_{i\in[n],p\in[P]}\left\|\bm{x}_{i}^{(p)} \right\|\right)^{2}\left\|\bm{\widetilde{W}}-\bm{\overline{W}}\right\|\] \[\leq 9r^{-1}P\sigma_{\mathrm{d}}^{2}d\left\|\bm{\widetilde{W}}- \bm{\overline{W}}\right\|,\]

where the last inequality holds since \(\left\|\xi_{i}^{(p)}\right\|^{2}<\frac{3}{2}\sigma_{\mathrm{d}}^{2}d\) and \(\alpha^{2}\leq\frac{3}{4}\sigma_{\mathrm{d}}^{2}d\) due to (A7). Hence, \(\mathcal{L}_{\mathrm{CutMix}}(\bm{W})\) is \(L\)-smooth with \(L:=9r^{-1}P\sigma_{\mathrm{d}}^{2}d\). 

Since our objective function \(\mathcal{L}_{\mathrm{CutMix}}(\bm{W})\) is \(L\)-smooth and \(\eta\leq\frac{1}{L}\) due to (A8), descent lemma (see Lemma 3.4 in Bubeck et al. (2015)) implies

\[\mathcal{L}_{\mathrm{CutMix}}\left(\bm{W}^{(t+1)}\right)-\mathcal{L}_{ \mathrm{CutMix}}\left(\bm{W}^{(t)}\right)\leq-\frac{\eta}{2}\left\|\nabla \mathcal{L}_{\mathrm{CutMix}}\left(\bm{W}^{(t)}\right)\right\|^{2},\]

and by telescoping sum, we have

\[\frac{1}{T}\sum_{t=0}^{T-1}\left\|\nabla\mathcal{L}_{\mathrm{CutMix}}\left( \bm{W}^{(t)}\right)\right\|^{2}\leq\frac{2\mathcal{L}_{\mathrm{CutMix}}\left( \bm{W}^{(0)}\right)}{\eta T}=\frac{\Theta(1)}{\eta T},\] (35)

for any \(T>0\).

Choose \(\epsilon=\frac{\mu\beta\|\hat{\bm{Z}}\|_{\infty}}{\mathrm{polylog}(d)}\). Then from (35), there exists \(T_{\mathrm{CutMix}}\leq\frac{\mathrm{poly}(d)}{\eta}\) such that

\[\left\|\nabla\mathcal{L}_{\mathrm{CutMix}}\left(\bm{W}^{(T_{\mathrm{CutMix}} )}\right)\right\|\leq\epsilon.\]

From characterization of \(\sigma_{\min}(\bm{J}(\bm{W}))\) in Section E.2.1,

\[\epsilon\geq\left\|\nabla\mathcal{L}_{\mathrm{CutMix}}\left(\bm{ W}^{(T_{\mathrm{CutMix}})}\right)\right\| \geq\sigma_{\min}\left(\bm{J}\left(\bm{W}^{(T_{\mathrm{CutMix}})} \right)\right)\left\|\nabla h\left(\bm{Z}^{(T_{\mathrm{CutMix}})}\right)\right\|\] \[\geq\frac{\beta}{2}\left\|\nabla h\left(\bm{Z}^{(T_{\mathrm{CutMix }})}\right)\right\|,\]

and thus

\[\left\|\nabla h\left(\bm{Z}^{(T_{\mathrm{CutMix}})}\right)\right\|\leq 2\beta^{-1} \epsilon=\mu\cdot\frac{2\|\hat{\bm{Z}}\|_{\infty}}{\mathrm{polylog}(d)}.\]

For sufficiently large \(d\), the RHS becomes smaller than \(\mu\cdot\frac{\|\hat{\bm{Z}}\|_{\infty}}{4}\). Then, by Lemma E.1 we have seen in Section E.2.4,

\[\left\|\bm{Z}^{(T_{\mathrm{CutMix}})}-\hat{\bm{Z}}\right\|\leq\frac{\|\hat{ \bm{Z}}\|_{\infty}}{4},\]

and thus

\[\phi\left(\left\langle\bm{w}_{y_{i}}^{(T_{\mathrm{CutMix}})},\bm{x}_{i}^{(p)} \right\rangle\right)-\phi\left(\left\langle\bm{w}_{-y_{i}}^{(T_{\mathrm{CutMix} })},\bm{x}_{i}^{(p)}\right\rangle\right)=\Theta(1),\]

for all \(i\in[n]\) and \(p\in[P]\), and therefore it reaches perfect training accuracy.

#### e.2.6 Test Accuracy of Solution Found by Gradient Descent

The final step is showing that \(\bm{W}^{(T_{\text{CutMix}})}\) reaches almost perfect test accuracy.

From the results of Section E.2.5, we have

\[\phi\left(\left\langle\bm{w}_{s}^{(T_{\text{CutMix}})},\bm{v}_{s,k }\right\rangle\right)-\phi\left(\left\langle\bm{w}_{-s}^{(T_{\text{CutMix}})}, \bm{v}_{s,k}\right\rangle\right) =\Theta(1),\] \[\phi\left(\left\langle\bm{w}_{y_{i}}^{(T_{\text{CutMix}})}, \xi_{i}^{(p)}\right\rangle\right)-\phi\left(\left\langle\bm{w}_{-y_{i}}^{(T_{ \text{CutMix}})},\xi_{i}^{(p)}\right\rangle\right) =\Theta(1),\]

for each \(s\in\{\pm 1\},k\in[K],i\in[n]\) and \(p\in[P]\setminus\{p_{i}^{*}\}\).

For any \(u>v\), by the mean value theorem, we have

\[\beta(u-v)\leq\phi(u)-\phi(v)=(u-v)\frac{\phi(u)-\phi(v)}{u-v}\leq(u-v).\]

Hence, we have

\[\phi\left(\left\langle\bm{w}_{s}^{(T_{\text{CutMix}})},\bm{v}_{s,k }\right\rangle\right)-\phi\left(\left\langle\bm{w}_{-s}^{(T_{\text{CutMix}})}, \bm{v}_{s,k}\right\rangle\right)\leq\left\langle\bm{w}_{s}^{(T_{\text{CutMix} })}-\bm{w}_{-s}^{(T_{\text{CutMix}})},\bm{v}_{s,k}\right\rangle,\] \[\left\langle\bm{w}_{s}^{(T_{\text{CutMix}})}-\bm{w}_{-s}^{(T_{ \text{CutMix}})},\bm{v}_{s,k}\right\rangle\leq\beta^{-1}\left(\phi\left( \left\langle\bm{w}_{s}^{(T_{\text{CutMix}})},\bm{v}_{s,k}\right\rangle\right) -\phi\left(\left\langle\bm{w}_{-s}^{(T_{\text{CutMix}})},\bm{v}_{s,k}\right \rangle\right)\right),\]

and

\[\Omega(1)\leq\left\langle\bm{w}_{s}^{(T_{\text{CutMix}})}-\bm{w}_{-s}^{(T_{ \text{CutMix}})},\bm{v}_{s,k}\right\rangle\leq\mathcal{O}(\beta^{-1}),\]

for each \(s\in\{\pm 1\}\) and \(k\in[K]\). Similarly, for all \(i\in[n]\) and \(p\in[P]\setminus\{p_{i}^{*}\}\),

\[\phi\left(\left\langle\bm{w}_{y_{i}}^{(T_{\text{CutMix}})},\xi_{i}^{(p)} \right\rangle\right)-\phi\left(\left\langle\bm{w}_{-y_{i}}^{(T_{\text{CutMix} })},\xi_{i}^{(p)}\right\rangle\right)\leq\left\langle\bm{w}_{y_{i}}^{(T_{\text {CutMix}})}-\bm{w}_{-y_{i}}^{(T_{\text{CutMix}})},\xi_{i}^{(p)}\right\rangle,\]

and

\[\Omega(1)\leq\left\langle\bm{w}_{y_{i}}^{(T_{\text{CutMix}})}-\bm{w}_{-y_{i}}^ {(T_{\text{CutMix}})},\xi_{i}^{(p)}\right\rangle\leq\mathcal{O}(\beta^{-1}).\]

By Lemma B.3,

\[\bm{w}_{1}^{(T_{\text{CutMix}})}-\bm{w}_{-1}^{(T_{\text{CutMix} })}\] \[=\bm{w}_{1}^{(0)}-\bm{w}_{-1}^{(0)}+\sum_{s\in\{\pm 1\},k\in[K]}s \gamma(s,k)\bm{v}_{s,k}+\sum_{i\in[n],p\in[P]\setminus\{p_{i}^{*}\}}y_{i}\rho( i,p)\frac{\xi_{i}^{(p)}}{\left\|\xi_{i}^{(p)}\right\|^{2}},\]

where for each \(s\in\{\pm 1\}\),

\[\gamma(s,1) =\gamma_{1}^{(T_{\text{CutMix}})}(s,1)+\gamma_{-1}^{(T_{\text{ CutMix}})}(s,1)\] \[\quad+\alpha\sum_{i\in\mathcal{F}_{s}}y_{i}\left(\rho_{1}^{(T_{ \text{CutMix}})}(i,\tilde{p}_{i})+\rho_{-1}^{(T_{\text{CutMix}})}(i,\tilde{p }_{i})\right)\left\|\xi_{i}^{(\tilde{p}_{i})}\right\|^{-2},\]

and

\[\gamma(s,k)=\gamma_{1}^{(T_{\text{CutMix}})}(s,k)+\gamma_{-1}^{(T_{\text{ CutMix}})}(s,k),\]

\[\rho(i,p)=\rho_{1}^{(T_{\text{CutMix}})}(i,p)+\rho_{-1}^{(T_{\text{CutMix}})}(i,p),\]

for each \(s\in\{\pm 1\},k\in[K]\setminus\{1\},i\in[n]\) and \(p\in[P]\setminus\{p_{i}^{*}\}\). If we choose \(j\in[n],q\in[P]\setminus\{p_{j}^{*}\}\) such that \(\rho(j,q)=\max_{i\in[n],p\in[P]\setminus\{p_{i}^{*}\}}\rho(i,p)\), then we have

\[\left\langle\bm{w}_{y_{j}}^{(T_{\text{CutMix}})}-\bm{w}_{-y_{j}}^{(T_{\text{ CutMix}})},\xi_{j}^{(q)}\right\rangle\] \[=\left\langle\bm{w}_{y_{j}}^{(0)}-\bm{w}_{-y_{j}}^{(0)},\xi_{j}^{( q)}\right\rangle+\rho(j,q)+y_{j}\sum_{\begin{subarray}{c}i\in[n],p\in[P]\setminus\{p_{i}^{*}\} \\ (i,p)\neq(j,q)\end{subarray}}y_{i}\rho(i,p)\frac{\left\langle\xi_{i}^{(p)},\xi_{j}^ {(q)}\right\rangle}{\left\|\xi_{i}^{(p)}\right\|^{2}}.\]From the event defined in Lemma B.2, (A8), and (8),

\[\left|\left\langle\bm{w}_{y_{j}}^{(0)}-\bm{w}_{-y_{j}}^{(0)},\xi_{j}^{(q)}\right \rangle\right|=o\left(\frac{1}{\mathrm{polylog}(d)}\right)\leq\frac{1}{2}\left \langle\bm{w}_{y_{j}}^{(T_{\mathrm{CutMix}})}-\bm{w}_{-y_{j}}^{(T_{\mathrm{ CutMix}})},\xi_{j}^{(q)}\right\rangle,\]

where the inequality holds since \(\left\langle\bm{w}_{y_{j}}^{(T_{\mathrm{CutMix}})}-\bm{w}_{-y_{j}}^{(T_{ \mathrm{CutMix}})},\xi_{j}^{(q)}\right\rangle=\Omega(1)\). In addition, by triangular inequality, we have

\[\left|\sum_{\begin{subarray}{c}i\in[n],p\in[P]\setminus\{p_{i}^{ \ast}\}\\ (i,p)\neq(j,q)\end{subarray}}y_{i}\rho(i,p)\frac{\left\langle\xi_{i}^{(p)}, \xi_{j}^{(q)}\right\rangle}{\left\|\xi_{i}^{(p)}\right\|^{2}}\right| \leq\sum_{\begin{subarray}{c}i\in[n],p\in[P]\setminus\{p_{i}^{ \ast}\}\\ (i,p)\neq(j,q)\end{subarray}}\rho(i,p)\frac{\left|\left\langle\xi_{i}^{(p)}, \xi_{j}^{(q)}\right\rangle\right|}{\left\|\xi_{i}^{(p)}\right\|^{2}}\] \[\leq\rho(j,q)\widetilde{\mathcal{O}}\left(nP\sigma_{\mathrm{d}} \sigma_{\mathrm{b}}^{-1}d^{-\frac{1}{2}}\right)\leq\frac{\rho(j,q)}{2},\]

where the last inequality is due to (9). Hence,

\[\frac{1}{3}\rho(j,q)\leq\left|\left\langle\bm{w}_{y_{j}}^{(T_{\mathrm{CutMix }})}-\bm{w}_{-y_{j}}^{(T_{\mathrm{CutMix}})},\xi_{j}^{(q)}\right\rangle\right| \leq 3\rho(j,q)\]

and we have \(\rho(j,q)=\widetilde{\mathcal{O}}(\beta^{-1})\).

Let \((\bm{X},y)\sim\mathcal{D}\) be a test data with \(\bm{X}=\left(\bm{x}^{(1)},\ldots,\bm{x}^{(P)}\right)\in\mathbb{R}^{d\times P}\) having feature patch \(p^{\ast}\), dominant noise patch \(\tilde{p}\), and feature vector \(\bm{v}_{y,k}\). We have \(\bm{x}^{(p)}\sim N(\bm{0},\sigma_{\mathrm{b}}^{2}\bm{\Lambda})\) for each \(p\in[P]\setminus\{p^{\ast},\tilde{p}\}\) and \(\bm{x}^{(\tilde{p})}-\alpha\bm{v}_{s,1}\sim N(\bm{0},\sigma_{\mathrm{d}}^{2} \bm{\Lambda})\) for some \(s\in\{\pm 1\}\). Therefore, for all \(p\in[P]\setminus\{p^{\ast},\tilde{p}\}\)

\[\left|\phi\left(\left\langle\bm{w}_{1}^{(T_{\mathrm{CutMix}})}, \bm{x}^{(p)}\right\rangle\right)-\phi\left(\left\langle\bm{w}_{-1}^{(T_{ \mathrm{CutMix}})},\bm{x}^{(p)}\right\rangle\right)\right|\] \[\leq\left|\left\langle\bm{w}_{1}^{(T_{\mathrm{CutMix}})}-\bm{w}_ {-1}^{(T_{\mathrm{CutMix}})},\bm{x}^{(p)}\right\rangle\right|\] \[=\left|\left\langle\bm{w}_{1}^{(0)}-\bm{w}_{-1}^{(0)},\bm{x}^{(p) }\right\rangle\right|+\sum_{i\in[n],q\in[P]\setminus\{p_{i}^{\ast}\}}\rho(i,q) \frac{\left|\left\langle\xi_{i}^{(q)},\bm{x}^{(p)}\right\rangle\right|}{\left\| \xi_{i}^{(q)}\right\|^{2}}\] \[\leq\widetilde{\mathcal{O}}\left(\sigma_{0}\sigma_{\mathrm{b}}d^ {\frac{1}{2}}\right)+\widetilde{\mathcal{O}}\left(nP\beta^{-1}\sigma_{\mathrm{ d}}\sigma_{\mathrm{b}}^{-1}d^{-\frac{1}{2}}\right)\] \[=o\left(\frac{1}{\mathrm{polylog}(d)}\right),\] (36)

with probability at least \(1-o\left(\frac{1}{\mathrm{polylog}(d)}\right)\) due to Lemma B.2. In addition,

\[\left|\phi\left(\left\langle\bm{w}_{1}^{(T_{\mathrm{CutMix}})}, \bm{x}^{(\tilde{p})}\right\rangle\right)-\phi\left(\left\langle\bm{w}_{-1}^{(T_ {\mathrm{CutMix}})},\bm{x}^{(\tilde{p})}\right\rangle\right)\right|\] \[\leq\left|\left\langle\bm{w}_{1}^{(T_{\mathrm{CutMix}})}-\bm{w}_ {-1}^{(T_{\mathrm{CutMix}})},\bm{x}^{(\tilde{p})}\right\rangle\right|\] \[\leq\alpha\left|\left\langle\bm{w}_{1}^{(T_{\mathrm{CutMix}})}- \bm{w}_{-1}^{(T_{\mathrm{CutMix}})},\bm{v}_{s,1}\right\rangle\right|+\left| \left\langle\bm{w}_{1}^{(T_{\mathrm{CutMix}})}-\bm{w}_{-1}^{(T_{\mathrm{CutMix }})},\bm{x}^{(\tilde{p})}-\alpha\bm{v}_{s,1}\right\rangle\right|\] \[\leq\alpha\beta^{-1}\left|\phi\left(\left\langle\bm{w}_{1}^{(T_{ \mathrm{CutMix}})},\bm{v}_{s,1}\right\rangle\right)-\phi\left(\left\langle\bm {w}_{-1}^{(T_{\mathrm{CutMix}})},\bm{v}_{s,1}\right\rangle\right)\right|\] \[\quad+\left|\left\langle\bm{w}_{1}^{(0)}-\bm{w}_{-1}^{(0)},\bm{x }^{(\tilde{p})}-\alpha\bm{v}_{s,1}\right\rangle\right|+\sum_{i\in[n],q\in[P] \setminus\{p_{i}^{\ast}\}}\rho(i,q)\frac{\left|\left\langle\xi_{i}^{(q)},\bm{x }^{(\tilde{p})}-\alpha\bm{v}_{s,1}\right\rangle\right|}{\left\|\xi_{i}^{(q)} \right\|^{2}}\] \[\leq\widetilde{\mathcal{O}}\left(\alpha\beta^{-1}\right)+\widetilde {\mathcal{O}}\left(\sigma_{0}\sigma_{\mathrm{d}}d^{\frac{1}{2}}\right)+ \widetilde{\mathcal{O}}\left(nP\beta^{-1}\sigma_{\mathrm{d}}\sigma_{\mathrm{b}}^ {-1}d^{-\frac{1}{2}}\right)\] \[=o\left(\frac{1}{\mathrm{polylog}(d)}\right),\] (37)

with probability at least \(1-o\left(\frac{1}{\mathrm{polylog}(d)}\right)\), where the last equality is due to (8), (9), (10), and (A8).

Suppose (36) and \((\ref{eq:37})\) holds. Then,

\[\begin{split}& yf_{\bm{W}^{(T_{\text{CutMix}})}}(\bm{X})\\ &=\left(\phi\left(\left<\bm{w}_{y}^{(T_{\text{CutMix}})},\bm{v}_{ y,k}\right>\right)-\phi\left(\left<\bm{w}_{-y}^{(T_{\text{CutMix}})},\bm{v}_{y,k} \right>\right)\right)\\ &\quad+\sum_{p\in[P]\setminus\{p^{*}\}}\left(\phi\left(\left<\bm {w}_{y}^{(T_{\text{CutMix}})},\bm{x}^{(p)}\right>\right)-\phi\left(\left<\bm {w}_{-y}^{(T_{\text{CutMix}})},\bm{x}^{(p)}\right>\right)\right)\\ &=\Omega(1)-o\left(\frac{1}{\operatorname{polylog}(d)}\right) \\ &>0.\end{split}\]

Hence, we have our conclusion.

Technical Lemmas

In this section, we introduce technical lemmas that are used for proving the main theorems. We present their proofs here for better readability.

The following lemma is used in Section C.2.4 and Section D.2.4:

**Lemma F.1**.: _For any \(z,\delta\in\mathbb{R}\),_

\[|\phi(z)-(z+\delta)\phi^{\prime}(z)|\leq r+|\delta|.\]

Proof of Lemma F.1.: \[\phi(z)-z\phi^{\prime}(z)=\begin{cases}z-\frac{1-\beta}{2}r-z=-\frac{1-\beta} {2}r=-\frac{1-\beta}{2}r&\text{if }z\geq r\\ \frac{1-\beta}{2r}z^{2}+\beta z-\left(\frac{1-\beta}{r}z+\beta\right)z=\frac{ 1-\beta}{2r}z^{2}&\text{if }0\leq z\leq r\\ \beta z-\beta z=0&\text{if }z<0\end{cases},\]

and we obtain

\[|\phi(z)-(z+\delta)\phi^{\prime}(z)|\leq|\phi(z)-z\phi^{\prime}(z)|+|\delta| \phi^{\prime}(z)\leq\frac{1-\beta}{2}r+|\delta|\leq r+|\delta|.\]

The following lemma is used in Section C.2.4.

**Lemma F.2**.: _Suppose \(E_{\mathrm{init}}\) occurs. Then, for any model parameter \(\bm{W}=\{\bm{w}_{1},\bm{w}_{-1}\}\), we have_

\[\left\|\nabla_{\bm{W}}\sum_{i\in\mathcal{V}_{s,k}}\ell\left(y_{i}f_{\bm{W}}( \bm{X}_{i})\right)\right\|^{2}\leq 8P^{2}\sigma_{\mathrm{d}}^{2}d|\mathcal{V}_{s,k }|\sum_{i\in\mathcal{V}_{s,k}}\ell(y_{i}f_{\bm{W}}(\bm{X}_{i})),\]

_for each \(s\in\{\pm 1\}\) and \(k\in[K]\)._

Proof of Lemma F.2.: For each \(s\in\{\pm 1\}\) and \(i\in[n]\), we have

\[\left\|\nabla_{\bm{w}_{s}}f_{\bm{W}}(\bm{X}_{i})\right\|=\left\|\sum_{p\in[P] }\phi^{\prime}\left(\left\langle\bm{w}_{s},\bm{x}_{i}^{(p)}\right\rangle \right)\bm{x}_{i}^{(p)}\right\|\leq P\max_{p\in[P]}\left\|\bm{x}_{i}^{(p)} \right\|\leq 2P\sigma_{\mathrm{d}}d^{\frac{1}{2}},\]

where the inequality is due to the condition from the event \(E_{\mathrm{init}}\) defined in Lemma B.2 and (A7).. Therefore, for each \(s\in\{\pm 1\}\), we have

\[\left\|\nabla_{\bm{w}_{s}}\sum_{i\in\mathcal{V}_{s,k}}\ell\left( y_{i}f_{\bm{W}}(\bm{X}_{i})\right)\right\|^{2} =\left\|\sum_{i\in\mathcal{V}_{s,k}}\ell^{\prime}\left(y_{i}f_{ \bm{W}}(\bm{X}_{i})\right)\nabla_{\bm{w}_{s}}f_{\bm{W}}(\bm{X}_{i})\right\|^{2}\] \[\leq\left(\sum_{i\in\mathcal{V}_{s,k}}\ell^{\prime}\left(y_{i}f_{ \bm{W}}(\bm{X}_{i})\right)\left\|\nabla_{\bm{w}_{s}}f_{\bm{W}}(\bm{X}_{i}) \right\|\right)^{2}\] \[\leq 4P^{2}\sigma_{\mathrm{d}}^{2}d\left(\sum_{i\in\mathcal{V}_{ s,k}}\ell^{\prime}\left(y_{i}f_{\bm{W}}(\bm{X}_{i})\right)\right)^{2}\] \[\leq 4P^{2}\sigma_{\mathrm{d}}^{2}d|\mathcal{V}_{s,k}|\sum_{i\in \mathcal{V}_{s,k}}\left(\ell^{\prime}\left(y_{i}f_{\bm{W}}(\bm{X}_{i})\right) \right)^{2}\] \[\leq 4P^{2}\sigma_{\mathrm{d}}^{2}d|\mathcal{V}_{s,k}|\sum_{i\in \mathcal{V}_{s,k}}\ell\left(y_{i}f_{\bm{W}}(\bm{X}_{i})\right).\]

The first inequality is due to triangular inequality, the third inequality is due to Cauchy-Schwartz inequality and the last inequality is due to \(0\leq-\ell^{\prime}\leq 1\), which can be used to show \((\ell^{\prime})^{2}\leq-\ell^{\prime}\leq\ell\). As a result, we have our conclusion:

\[\left\|\nabla_{\bm{W}}\sum_{i\in\mathcal{V}_{s,k}}\ell\left(y_{i}f_{\bm{W}}( \bm{X}_{i})\right)\right\|^{2}=\left\|\nabla_{\bm{w}_{1}}\sum_{i\in\mathcal{V} _{s,k}}\ell\left(y_{i}f_{\bm{W}}(\bm{X}_{i})\right)\right\|^{2}+\left\|\nabla _{\bm{w}_{-1}}\sum_{i\in\mathcal{V}_{s,k}}\ell\left(y_{i}f_{\bm{W}}(\bm{X}_{i}) \right)\right\|^{2}\]\[\leq 8P^{2}\sigma_{\mathrm{d}}^{2}d|\mathcal{V}_{s,k}|\sum_{i\in \mathcal{V}_{s,k}}\ell(y_{i}f_{\bm{W}}(\bm{X}_{i})).\]

The following lemma is used in Section D.2.4.

**Lemma F.3**.: _Suppose \(E_{\mathrm{init}}\) occurs. Then, for any model parameter \(\bm{W}=\{\bm{w}_{1},\bm{w}_{-1}\}\), we have_

\[\left\|\nabla\sum_{i\in\mathcal{V}_{s,k}}\mathbb{E}_{\mathcal{C} \sim\mathcal{D}_{\mathcal{C}}}[\ell\left(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i, \mathcal{C}})\right)]\right\|^{2}\leq 8P^{2}\sigma_{\mathrm{d}}^{2}d| \mathcal{V}_{s,k}|\sum_{i\in\mathcal{V}_{s,k}}\mathbb{E}_{\mathcal{C}\sim \mathcal{D}_{\mathcal{C}}}[\ell(y_{i}f_{\bm{W}^{(t)}}(\bm{X}_{i,\mathcal{C}}))]\]

_for each \(s\in\{\pm 1\}\) and \(k\in[K]\)._

Proof of Lemma f.3.: For each \(s\in\{\pm 1\}\), \(i\in[n]\) and \(\mathcal{C}\subset[P]\) with \(|\mathcal{C}|=C\), we have

\[\left\|\nabla_{\bm{w}_{s}}f_{\bm{W}}(\bm{X}_{i,\mathcal{C}})\right\|=\left\| \sum_{p\notin\mathcal{C}}\phi^{\prime}\left(\left<\bm{w}_{s},\bm{x}_{i}^{(p)} \right>\right)\bm{x}_{i}^{(p)}\right\|\leq P\max_{p\in[P]}\left\|\bm{x}_{i}^{( p)}\right\|\leq 2P\sigma_{\mathrm{d}}d^{\frac{1}{2}},\]

where the inequality is due to the condition from the event \(E_{\mathrm{init}}\) defined in Lemma B.2 and (A7). Therefore, for any \(s\in\{\pm 1\}\), we have

\[\left\|\nabla_{\bm{w}_{s}}\sum_{i\in\mathcal{V}_{s,k}}\mathbb{E} _{\mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}[\ell\left(y_{i}f_{\bm{W}}(\bm{X}_{i,\mathcal{C}})\right)]\right\|^{2}\] \[=\left\|\sum_{i\in\mathcal{V}_{s,k}}\mathbb{E}_{\mathcal{C}\sim \mathcal{D}_{\mathcal{C}}}[\ell^{\prime}\left(y_{i}f_{\bm{W}}(\bm{X}_{i, \mathcal{C}})\right)\nabla_{\bm{w}_{s}}f_{\bm{W}}(\bm{X}_{i,\mathcal{C}})] \right\|^{2}\] \[\leq 4P^{2}\sigma_{\mathrm{d}}^{2}d\left(\sum_{i\in\mathcal{V}_{s,k}}\mathbb{E}_{\mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}\left[\ell^{\prime} \left(y_{i}f_{\bm{W}}(\bm{X}_{i,\mathcal{C}})\right)\right]\right)^{2}\] \[\leq 4P^{2}\sigma_{\mathrm{d}}^{2}d|\mathcal{V}_{s,k}|\sum_{i\in \mathcal{V}_{s,k}}\mathbb{E}_{\mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}[\ell \left(y_{i}f_{\bm{W}}(\bm{X}_{i,\mathcal{C}})\right)].\]

The first inequality is due to triangular inequality, the third inequality is due to Cauchy-Schwartz inequality and the last inequality is due to \(0\leq-\ell^{\prime}\leq 1\), which can be used to show \((\ell^{\prime})^{2}\leq-\ell^{\prime}\leq\ell\). As a result, we have our conclusion:

\[\left\|\nabla_{\bm{W}}\sum_{i\in\mathcal{V}_{s,k}}\mathbb{E}_{ \mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}\left[\ell\left(y_{i}f_{\bm{W}}(\bm{X }_{i,\mathcal{C}})\right)\right]\right\|^{2}\] \[=\left\|\nabla_{\bm{w}_{1}}\sum_{i\in\mathcal{V}_{s,k}}\mathbb{E }_{\mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}\left[\ell\left(y_{i}f_{\bm{W}}( \bm{X}_{i,\mathcal{C}})\right)\right]\right\|^{2}+\left\|\nabla_{\bm{w}_{-1}} \sum_{i\in\mathcal{V}_{s,k}}\mathbb{E}_{\mathcal{C}\sim\mathcal{D}_{\mathcal{C} }}\left[\ell\left(y_{i}f_{\bm{W}}(\bm{X}_{i,\mathcal{C}})\right)\right]\right\| ^{2}\] \[\leq 8P^{2}\sigma_{\mathrm{d}}^{2}d|\mathcal{V}_{s,k}|\sum_{i\in \mathcal{V}_{s,k}}\mathbb{E}_{\mathcal{C}\sim\mathcal{D}_{\mathcal{C}}}\left[ \ell(y_{i}f_{\bm{W}}(\bm{X}_{i,\mathcal{C}}))\right].\]The following lemma guarantees the existence and characterizes the minimum of the CutMix loss in Section E.2.2.

**Lemma F.4**.: _Suppose the event \(E_{\mathrm{init}}\) occurs. Let \(g_{1},g_{-1}:\mathbb{R}\times\mathbb{R}\to\mathbb{R}\) be defined as_

\[g_{s}(z_{1},z_{-1}):=\frac{|\mathcal{V}_{s}|}{|\mathcal{V}_{-s}|}\ell^{\prime}( Pz_{s})+\frac{2}{P}\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S}}}\left[| \mathcal{S}|\ell^{\prime}(|\mathcal{S}|z_{s}-(P-|\mathcal{S}|)z_{-s})\right]+ \frac{P-1}{3P},\]

_for each \(s\in\{\pm 1\}\). There exist unique \(z_{1}^{*},z_{-1}^{*}>0\) such that \(g_{1}(z_{1}^{*},z_{-1}^{*})=g_{-1}(z_{1}^{*},z_{-1}^{*})=0\). Furthermore, we have \(z_{1}^{*},z_{-1}^{*}=\Theta(1)\)._

Proof of Lemma F.4.: For each \(z_{1}>0\),

\[g_{-1}(z_{1},0) =\left(\frac{|\mathcal{V}_{-1}|}{|\mathcal{V}_{1}|}+1\right) \cdot\left(-\frac{1}{2}\right)+\frac{2}{P}\mathbb{E}_{\mathcal{S}\sim\mathcal{ D}_{\mathcal{S}}}[|\mathcal{S}|\ell^{\prime}(-(P-|\mathcal{S}|)z_{1})]+\frac{P-1}{3P}\] \[<\left(\frac{|\mathcal{V}_{-1}|}{|\mathcal{V}_{1}|}+1\right) \cdot\left(-\frac{1}{2}\right)+\frac{P-1}{3P}<0,\]

since \(\ell^{\prime}(z)\leq-\frac{1}{2}\) for any \(z\leq 0\) and we use \(\frac{25}{52}n\leq|\mathcal{V}_{1}|,|\mathcal{V}_{-1}|\leq\frac{27}{52}n\) from the event \(E_{\mathrm{init}}\) defined in Lemma B.2. In addition,

\[g_{-1}(z_{1},Pz_{1}+\log 9)\] \[=\frac{|\mathcal{V}_{-1}|}{|\mathcal{V}_{1}|}\ell^{\prime}(P^{2}z _{1}+P\log 9)+\frac{2}{P}\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S}}}[| \mathcal{S}|\ell^{\prime}(|\mathcal{S}|Pz_{1}+|\mathcal{S}|\log 9-(P-|\mathcal{S}|)z_{1}) ]+\frac{P-1}{3P}\] \[\geq\left(\frac{|\mathcal{V}_{-1}|}{|\mathcal{V}_{1}|}+1\right) \ell^{\prime}(\log 9)+\frac{P-1}{3P}\] \[>0,\]

where we use \(\frac{25}{52}n\leq|\mathcal{V}_{1}|,|\mathcal{V}_{-1}|\leq\frac{27}{52}n\) from the event \(E_{\mathrm{init}}\) defined in Lemma B.2 and (A1) for the last inequality.

Since \(z\mapsto g_{-1}(z_{1},z)\) is strictly increasing and by intermediate value theorem, there exists \(S:(0,\infty)\to(0,\infty)\) such that \(z=S(z_{1})\) is a unique solution of \(g_{-1}(z_{1},z)=0\) and \(S(z_{1})<Pz_{1}+\log 9\). Note that \(S\) is strictly increasing since \(g_{-1}(z_{1},z_{-1})\) is strictly decreasing with respect to \(z_{1}\) and strictly increasing with respect to \(z_{-1}\). Also, if \(S(z)\) is bounded above, i.e., there exists some \(U>0\) such that \(S(z)\leq U\) for any \(z>0\),

\[\lim_{z\to\infty}g_{-1}(z,S(z))\] \[=\lim_{z\to\infty}\left(\frac{|\mathcal{V}_{-1}|}{|\mathcal{V}_{1 }|}\ell^{\prime}\left(PS(z)\right)+\frac{2}{P}\mathbb{E}_{\mathcal{S}\sim \mathcal{D}_{\mathcal{S}}}\left[|\mathcal{S}|\ell^{\prime}\big{(}|\mathcal{S}| S(z)-(P-|\mathcal{S}|)z\big{)}\right]+\frac{P-1}{3P}\right)\] \[\leq\lim_{z\to\infty}\left(\frac{|\mathcal{V}_{-1}|}{|\mathcal{ V}_{1}|}\ell^{\prime}\left(PU\right)+\frac{2}{P}\mathbb{E}_{\mathcal{S}\sim \mathcal{D}_{\mathcal{S}}}\left[|\mathcal{S}|\ell^{\prime}\big{(}|\mathcal{S}| U-(P-|\mathcal{S}|)z\big{)}\right]+\frac{P-1}{3P}\right)\] \[\leq-\frac{2}{P}\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{ \mathcal{S}}}\left[|\mathcal{S}|\cdot\mathbbm{1}_{|\mathcal{S}|\neq P}\right] +\frac{P-1}{3P}=-\frac{P-1}{P+1}+\frac{P-1}{3P}\] \[<0,\]

and it is contradictory. Hence, we have \(\lim_{z\to\infty}S(z)=\infty\).

Let us choose \(\underline{z}>0\) such that

\[\underline{z}=\frac{1}{P}\log\left(\frac{3P\left(1+\frac{|\mathcal{V}_{1}|}{| \mathcal{V}_{-1}|}\right)}{P-1}-1\right),\]

and thus

\[\ell^{\prime}(P\underline{z})=-\frac{P-1}{3P\left(1+\frac{|\mathcal{V}_{1}|}{| \mathcal{V}_{-1}|}\right)}.\]

We have

\[g_{1}(\underline{z},S(\underline{z}))=\frac{|\mathcal{V}_{1}|}{|\mathcal{V}_{-1 }|}\ell^{\prime}(P\underline{z})+\frac{2}{P}\mathbb{E}_{\mathcal{S}\sim\mathcal{ D}_{\mathcal{S}}}\left[|\mathcal{S}|\ell^{\prime}\big{(}|\mathcal{S}| \underline{z}-(P-|\mathcal{S}|)S(\underline{z})\big{)}\right]+\frac{P-1}{3P}\]\[=\frac{1}{P+1}\left(P\ell^{\prime}\big{(}PS(z^{*})\big{)}+(P-1)\ell^{ \prime}\big{(}(P-1)S(z^{*})-z^{*}\big{)}+\sum_{m=0}^{P-2}m\ell^{\prime}\big{(}mS( z^{*})-(P-m)z^{*}\big{)}\right)\] \[<\frac{1}{P+1}\left(-\frac{(P-1)(P-2)}{2}(1-\epsilon)-\frac{P-1}{ 2}\right),\]where the last inequality is due to (39). This is contradictory to (38), especially the second term inside the maximum, and we have \((P-1)S(z^{*})-z^{*}>0\). Note that we have

\[-\frac{\epsilon}{2}=\ell^{\prime}\left(\frac{1}{2}\min\{z^{*},S(z^{*})\}\right) \geq\ell^{\prime}\left(\frac{z^{*}}{2P}\right),\]

and since \(\epsilon=\Theta(1)\) in (38), we have \(z^{*}\leq 2P\log\left(\frac{2}{\epsilon}-1\right)=\mathcal{O}(1)\).

Thus, we have \(S(z^{*})-(P-1)z^{*}<0<(P-1)S(z^{*})-z^{*}<PS(z^{*})\). One can consider dividing the interval \([S(z^{*})-(P-1)z^{*},PS(z^{*})]\) into a grid of length \(z^{*}+S(z^{*})\). Then, the interval is equally divided into \(P-1\) sub-intervals and \(0\) belongs to one of them. In other words, there exists \(k\in[P-2]\) such that

\[kS(z^{*})-(P-k)z^{*}\leq 0<(k+1)S(z^{*})-(P-k-1)z^{*},\]

and note that if \(P=3\), then \(k=1\). The rest of the proof is divided into two cases: \((k+1)S(z^{*})-(P-k-1)z^{*}\geq\frac{1}{2}(z^{*}+S(z^{*}))\) or \((k+1)S(z^{*})-(P-k-1)z^{*}<\frac{1}{2}(z^{*}+S(z^{*}))\). In both cases, we show that \(g_{1}(z^{*},S(z^{*}))>0\).

**Case 1:**\((k+1)S(z^{*})-(P-k-1)z^{*}\geq\frac{1}{2}(z^{*}+S(z^{*}))\)

From (39), we have

\[-1<\ell^{\prime}(-Pz^{*})<\cdots<\cdots<\ell^{\prime}\big{(}(k-1)S(z^{*})-(P- k+1)z^{*}\big{)}<-1+\epsilon,\]

and

\[-\epsilon<\ell^{\prime}\big{(}(k+1)S(z^{*})-(P-k-1)z^{*}\big{)}<\cdots<\ell^ {\prime}\big{(}PS(z^{*})\big{)}<0.\]

Thus, we have

\[\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S}}}\left[| \mathcal{S}|\ell^{\prime}\big{(}|\mathcal{S}|S(z^{*})-(P-|\mathcal{S}|)z^{*} \big{)}\right]\big{|}\] \[>\frac{1}{P+1}\left(k\ell^{\prime}\big{(}kS(z^{*})-(P-k)z^{*} \big{)}-\frac{k(k-1)}{2}\right)-\frac{P}{2}\epsilon.\]

and we obtain \(k>\frac{P-1}{2}\) since

\[\frac{k(k+1)}{2}\] \[=\frac{k(k-1)}{2}+k\] \[>-\frac{P(P+1)}{2}\epsilon-(P+1)\mathbb{E}_{\mathcal{S}\sim \mathcal{D}_{\mathcal{S}}}[|\mathcal{S}|\ell^{\prime}(|\mathcal{S}|S(z^{*})-(P -|\mathcal{S}|)z^{*})]\] \[\quad+k\ell^{\prime}(kS(z^{*})-(P-k)z^{*})+k\] \[>-\frac{P(P+1)}{2}\left(1+\frac{|\mathcal{V}_{-1}|}{|\mathcal{V} _{1}|}\right)\epsilon+\frac{(P-1)(P+1)}{6}\] \[\geq\frac{\frac{P-1}{2}\left(\frac{P-1}{2}+1\right)}{2},\]

where the second inequality is due to (40) and the fact that \(\ell^{\prime}\geq-1\), and the last inequality is due to (38), especially the third term inside the maximum. Note that since \(k\in\mathbb{N}\), \(k\geq\frac{P}{2}\).

Note that from (39), we have

\[-1<\ell^{\prime}\big{(}-PS(z^{*})\big{)}<\cdots<\ell^{\prime}\big{(}(P-k-1)z^{ *}-(k+1)S(z^{*})\big{)}<-1+\epsilon,\]

and

\[-\epsilon<\ell^{\prime}\big{(}(P-k+1)z^{*}-(k-1)z^{*}\big{)}<\cdots<\ell^{ \prime}(Pz^{*})<0.\]

Hence, we obtain

\[\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S}}}\left[| \mathcal{S}|\ell^{\prime}\big{(}|\mathcal{S}|z^{*}-(P-|\mathcal{S}|)S(z^{*}) \big{)}\right]\] \[\geq\frac{1}{P+1}\left(-\frac{(P-k-1)(P-k)}{2}-\frac{1}{2}(P-k) -\left((P-k+1)+\cdots+P\right)\epsilon\right)\]\[\geq-\frac{(P-k)^{2}}{2(P+1)}-\frac{1}{P+1}\cdot\frac{P(P+1)}{2}\epsilon\] \[\geq-\frac{P^{2}}{8(P+1)}-\frac{P}{2}\epsilon,\]

where we use \(k\geq\frac{P}{2}\) for the last inequality. Therefore, we have

\[g_{1}(z^{*},S(z^{*})) =\frac{|\mathcal{V}_{1}|}{|\mathcal{V}_{-1}|}\ell^{\prime}(Pz^{*}) +\frac{2}{P}\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S}}}[|\mathcal{S}| \ell^{\prime}(|\mathcal{S}|z^{*}-(P-|\mathcal{S}|)S(z^{*}))]+\frac{P-1}{3P}\] \[\geq-\left(\frac{|\mathcal{V}_{1}|}{|\mathcal{V}_{-1}|}+1\right) \epsilon-\frac{P}{4(P+1)}+\frac{P-1}{3P}\] \[>0,\]

where the last inequality is due to (38), especially the fourth term inside the maximum.

**Case 2:**\((k+1)S(z^{*})-(P-k-1)z^{*}<\frac{1}{2}(z^{*}+S(z^{*}))\)

In this case, we have \(kS(z^{*})-(P-k)z^{*}\leq-\frac{1}{2}(z^{*}+S(z^{*}))\). From (39), we have

\[-1<\ell^{\prime}(-Pz^{*})<\cdots<\ell^{\prime}\big{(}kS(z^{*})-(P-k)z^{*} \big{)}<-1+\epsilon,\]

and

\[-\epsilon<\ell^{\prime}\big{(}(k+2)S(z^{*})-(P-k-2)z^{*}\big{)}<\cdots<\ell^{ \prime}\big{(}PS(z^{*})\big{)}<0.\]

Thus, we have

\[\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S}}}[|\mathcal{S}| \ell^{\prime}(|\mathcal{S}|S(z^{*})-(P-|\mathcal{S}|)z^{*}))|]\] \[>\frac{1}{P+1}\left((k+1)\ell^{\prime}\big{(}(k+1)S(z^{*})-(P-k- 1)z^{*}\big{)}-\frac{k(k+1)}{2}\right)-\frac{P}{2}\epsilon,\]

and we obtain \(k>\frac{P-1}{2}\) since

\[\frac{(k+1)^{2}}{2}\] \[=\frac{k(k+1)}{2}+\frac{k+1}{2}\] \[>-\frac{P(P+1)}{2}\epsilon-(P+1)\mathbb{E}_{\mathcal{S}\sim \mathcal{D}_{\mathcal{S}}}[|\mathcal{S}|\ell^{\prime}(|\mathcal{S}|S(z^{*})-(P -|\mathcal{S}|)z^{*})]\] \[\quad+(k+1)\ell^{\prime}((k+1)S(z^{*})-(P-k-1)z^{*})+\frac{k+1}{2}\] \[>-\frac{P(P+1)}{2}\left(1+\frac{|\mathcal{V}_{-1}|}{|\mathcal{V} _{1}|}\right)\epsilon+\frac{(P-1)(P+1)}{6}\] \[>\frac{\left(\frac{P-1}{2}+1\right)^{2}}{2},\]

where the second inequality is due to (40) and the fact that \(\ell^{\prime}(z)\geq-\frac{1}{2}\quad\forall z\geq 0\), and the last inequality is due to our (38), especially the third term inside the maximum. Note that since \(k\in\mathbb{N}\), we have \(k\geq\frac{P}{2}\).

Note that from (39), we have

\[-1<\ell^{\prime}\big{(}-PS(z^{*})\big{)}<\cdots<\ell^{\prime}\big{(}(P-k-2)z^{ *}-(k+2)S(z^{*})\big{)}<-1+\epsilon,\]

and

\[-\epsilon<\ell^{\prime}\big{(}(P-k)z^{*}-kz^{*}\big{)}<\cdots<\ell^{\prime}(Pz ^{*})<0.\]

Hence, we obtain

\[\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S}}}[|\mathcal{S}| \ell^{\prime}(|\mathcal{S}|z^{*}-(P-|\mathcal{S}|)S(z^{*})]\] \[\geq\frac{1}{P+1}\left(-\frac{(P-k-1)(P-k)}{2}-\left((P-k)+ \cdots+P\right)\epsilon\right)\]\[\geq-\frac{(P-k)(P-k-1)}{2(P+1)}-\frac{1}{P+1}\cdot\frac{P(P+1)}{2}\epsilon\] \[\geq-\frac{(P-k)^{2}}{2(P+1)}-\frac{1}{P+1}\cdot\frac{P(P+1)}{2}\epsilon\] \[\geq-\frac{P^{2}}{8(P+1)}-\frac{P}{2}\epsilon,\]

where we use \(k\geq\frac{P}{2}\) for the last inequality. Therefore, we have

\[g_{1}(z^{*},S(z^{*})) =\frac{|\mathcal{V}_{1}|}{|\mathcal{V}_{-1}|}\ell^{\prime}(Pz^{*} )+\frac{2}{P}\mathbb{E}_{\mathcal{S}\sim\mathcal{D}_{\mathcal{S}}}[|\mathcal{ S}|\ell^{\prime}(|\mathcal{S}|z^{*}-(P-|\mathcal{S}|)S(z^{*}))]+\frac{P-1}{3P}\] \[\geq-\left(\frac{|\mathcal{V}_{1}|}{|\mathcal{V}_{-1}|}+1\right) \epsilon-\frac{P}{4(P+1)}+\frac{P-1}{3P}\] \[>0,\]

where the last inequality is due to (38), especially the fourth term inside the maximum.

In both cases, we have \(g_{1}(z^{*},S(z^{*}))>0\). By intermediate value theorem, there exist unique \(z_{1}^{*},z_{-1}^{*}>0\) such that \(g_{1}(z_{1}^{*},z_{-1}^{*})=g_{-1}(z_{1}^{*},z_{-1}^{*})=0\). In addition, \(\underline{z}\leq z_{1}^{*}\leq z^{*}\) and we have \(z_{1}=\Theta(1)\) since \(\underline{z}=\Omega(1)\) and \(z^{*}=\mathcal{O}(1)\). By using a similar argument, we can show that \(z_{-1}^{*}=\Theta(1)\), and we have our conclusion.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction accurately reflect the paper's contributions. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitation on our problem setting and theoretical framework in Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We provide the full set of assumptions and a complete proof in Appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide detailed descriptions of the experimental setting in Section 5 and Appendix A. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [No] Justification: We do not provide open access to the data and code since our main focus is theory. Guidelines: The main focus of this paper is theory. * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide detailed descriptions of the experimental setting in Section 5 and Appendix A. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: The main focus of this paper is theory. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide the type of compute workers used (NVIDIA RTX A6000) in Appendix A. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research address to the NeurIPS Code of Ethics, ensuring ethical conduct throughout the study. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The paper mainly focuses on establishing a theoretical understanding of existing data augmentation techniques. Thus, there are no direct societal implications arising from the research. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper does not involve the release of data or models that pose a high risk for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not introduce any new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with Human subjects Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.