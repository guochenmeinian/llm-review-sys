# Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation

Sebastien Lachapelle\({}^{*,1}\)

Ioannis Mitliagkas\({}^{\dagger}\)

&Divyat Mahajan\({}^{*}\)

Correspondence to: {lachaseb, divyat.mahajan}@mila.quebec

Mila & DIRO, Universite de Montreal

\({}^{1}\)Samsung - SAIT AI Lab, Montreal

###### Abstract

We tackle the problems of latent variables identification and "out-of-support" image generation in representation learning. We show that both are possible for a class of decoders that we call _additive_, which are reminiscent of decoders used for object-centric representation learning (OCRL) and well suited for images that can be decomposed as a sum of object-specific images. We provide conditions under which exactly solving the reconstruction problem using an additive decoder is guaranteed to identify the blocks of latent variables up to permutation and block-wise invertible transformations. This guarantee relies only on very weak assumptions about the distribution of the latent factors, which might present statistical dependencies and have an almost arbitrarily shaped support. Our result provides a new setting where nonlinear independent component analysis (ICA) is possible and adds to our theoretical understanding of OCRL methods. We also show theoretically that additive decoders can generate novel images by recombining observed factors of variations in novel ways, an ability we refer to as _Cartesian-product extrapolation_. We show empirically that additivity is crucial for both identifiability and extrapolation on simulated data.

## 1 Introduction

The integration of connectionist and symbolic approaches to artificial intelligence has been proposed as a solution to the lack of robustness, transferability, systematic generalization and interpretability of current deep learning algorithms [53, 4, 13, 25, 21] with justifications rooted in cognitive sciences [20, 28, 43] and causality [57, 63]. However, the problem of extracting meaningful symbols grounded in low-level observations, e.g. images, is still open. This problem is sometime referred to as _disentanglement_[4, 48] or _causal representation learning_[63]. The question of _identifiability_ in representation learning, which originated in works on _nonlinear independent component analysis_ (ICA) [65, 31, 33, 36], has been the focus of many recent efforts [49, 66, 26, 47, 3, 9, 41]. The mathematical results of these works provide rigorous explanations for when and why symbolic representations can be extracted from low-level observations. In a similar spirit, _Object-centric representation learning_ (OCRL) aims to learn a representation in which the information about different objects are encoded separately [19, 22, 11, 24, 18, 51, 14]. These approaches have shown impressive results empirically, but the exact reason why they can perform this form of segmentation without any supervision is poorly understood.

### Contributions

Our first contribution is an analysis of the identifiability of a class of decoders we call _additive_ (Definition 1). Essentially, a decoder \(\bm{f}(\bm{z})\) acting on a latent vector \(\bm{z}\in\mathbb{R}^{d_{z}}\) to produce an observation \(\bm{x}\) is said to be additive if it can be written as \(\bm{f}(\bm{z})=\sum_{B\in\mathcal{B}}\bm{f}^{(B)}(\bm{z}_{B})\) where \(\mathcal{B}\) is a partition of \(\{1,\dots,d_{z}\}\), \(\bm{f}^{(B)}(\bm{z}_{B})\) are "block-specific" decoders and the \(\bm{z}_{B}\) are non-overlapping subvectors of \(\bm{z}\). This class of decoder is particularly well suited for images \(x\) that can be expressed as a sum of images corresponding to different objects (left of Figure 1). Unsurprisingly, this class of decoder bears similarity with the decoding architectures used in OCRL (Section 2), which already showed important successes at disentangling objects without any supervision. Our identifiability results provide conditions under which exactly solving the reconstruction problem with an additive decoder identifies the latent blocks \(\bm{z}_{B}\) up to permutation and block-wise transformations (Theorems 1 & 2). We believe these results will be of interest to both the OCRL community, as they partly explain the empirical success of these approaches, and to the nonlinear ICA and disentanglement community, as it provides an important special case where identifiability holds. This result relies on the block-specific decoders being "sufficiently nonlinear" (Assumption 2) and requires only very weak assumptions on the distribution of the ground-truth latent factors of variations. In particular, these factors can be statistically dependent and their support can be (almost) arbitrary.

Our second contribution is to show theoretically that additive decoders can generate images never seen during training by recombining observed factors of variations in novel ways (Corollary 3). To describe this ability, we coin the term "Cartesian-product extrapolation" (right of Figure 1). We believe the type of identifiability analysis laid out in this work to understand "out-of-support" generation is novel and could be applied to other function classes or learning algorithms such as DALLE-2 [59] and Stable Diffusion [61] to understand their apparent creativity and hopefully improve it.

Both latent variables identification and Cartesian-product extrapolation are validated experimentally on simulated data (Section 4). More specifically, we observe that additivity is crucial for both by comparing against a non-additive decoder which fails to disentangle and extrapolate.

**Notation.** Scalars are denoted in lower-case and vectors in lower-case bold, e.g. \(x\in\mathbb{R}\) and \(\bm{x}\in\mathbb{R}^{n}\). We maintain an analogous notation for scalar-valued and vector-valued functions, e.g. \(f\) and \(\bm{f}\). The \(i\)th coordinate of the vector \(\bm{x}\) is denoted by \(\bm{x}_{i}\). The set containing the first \(n\) integers excluding \(0\) is denoted by \([n]\). Given a subset of indices \(S\subseteq[n]\), \(\bm{x}_{S}\) denotes the subvector consisting of entries \(\bm{x}_{i}\) for \(i\in S\). Given a function \(\bm{f}(\bm{x}_{S})\in\mathbb{R}^{m}\) with input \(\bm{x}_{S}\), the derivative of \(\bm{f}\) w.r.t. \(\bm{x}_{i}\) is denoted by \(D_{i}\bm{f}(\bm{x}_{S})\in\mathbb{R}^{m}\) and the second derivative w.r.t. \(\bm{x}_{i}\) and \(\bm{x}_{i^{\prime}}\) is \(D_{i,i^{\prime}}^{2}\bm{f}(\bm{x}_{S})\in\mathbb{R}^{m}\). See Table 2 in appendix for more.

**Code:** Our code repository can be found at this link.

## 2 Background & Literature review

**Identifiability of latent variable models.** The problem of latent variables identification can be best explained with a simple example. Suppose observations \(\bm{x}\in\mathbb{R}^{d_{x}}\) are generated i.i.d. by first sampling a latent vector \(\bm{z}\in\mathbb{R}^{d_{z}}\) from a distribution \(\mathbb{P}_{\bm{z}}\) and feeding it into a decoder function \(\bm{f}:\mathbb{R}^{d_{z}}\to\mathbb{R}^{d_{x}}\)

Figure 1: **Left: Additive decoders model the additive structure of scenes composed of multiple objects. Right: Additive decoders allow to generate novel images never seen during training via Cartesian-product extrapolation (Corollary 3). Purple regions correspond to latents/observations seen during training. The blue regions correspond to the Cartesian-product extension. The middle set is the manifold of images of balls. In this example, the learner never saw both balls high, but these can be generated nevertheless thanks to the additive nature of the scene. Details in Section 3.2.**i.e. \(\bm{x}=\bm{f}(\bm{z})\). By choosing an alternative model defined as \(\hat{\bm{f}}:=\bm{f}\circ\bm{v}\) and \(\hat{\bm{z}}:=\bm{v}^{-1}(\bm{z})\) where \(\bm{v}:\mathbb{R}^{d_{z}}\to\mathbb{R}^{d_{z}}\) is some bijective transformation, it is easy to see that the distributions of \(\hat{\bm{x}}=\hat{\bm{f}}(\hat{\bm{z}})\) and \(\bm{x}\) are the same since \(\hat{\bm{f}}(\hat{\bm{z}})=\bm{f}\circ\bm{v}(\bm{v}^{-1}(\bm{z}))=\bm{f}(\bm{z})\). The problem of identifiability is that, given only the distribution over \(\bm{x}\), it is impossible to distinguish between the two models \((\bm{f},\bm{z})\) and \((\hat{\bm{f}},\hat{\bm{z}})\). This is problematic when one wants to discover interpretable factors of variations since \(\bm{z}\) and \(\hat{\bm{z}}\) could be drastically different. There are essentially two strategies to go around this problem: (i) restricting the hypothesis class of decoders \(\hat{\bm{f}}\)[65, 26, 44, 54, 9, 73], and/or (ii) restricting/adding structure to the distribution of \(\hat{\bm{z}}\)[33, 50, 42, 47]. By doing so, the hope is that the only bijective mappings \(\bm{v}\) keeping \(\hat{\bm{f}}\) and \(\hat{\bm{z}}\) into their respective hypothesis classes will be trivial indeterminacies such as permutations and element-wise rescalings. Our contribution, which is to restrict the decoder function \(\hat{\bm{f}}\) to be additive (Definition 1), falls into the first category. Other restricted function classes for \(\bm{f}\) proposed in the literature include post-nonlinear mixtures [65], local isometries [16, 15, 29], conformal and orthogonal maps [26, 60, 9] as well as various restrictions on the sparsity of \(\bm{f}\)[54, 73, 7, 71]. Methods that do not restrict the decoder must instead restrict/structure the distribution of the latent factors by assuming, e.g., sparse temporal dependencies [31, 38, 42, 40], conditionally independent latent variables given an observed auxiliary variable [33, 36], that interventions targeting the latent factors are observed [42, 47, 46, 8, 2, 3, 64, 10, 67, 72, 34], or that the support of the latents is a Cartesian-product [68, 62]. In contrast, our result makes very mild assumptions about the distribution of the latent factors, which can present statistical dependencies, have an almost arbitrarily shaped support and does not require any interventions. Additionally, none of these works provide extrapolation guarantees as we do in Section 3.2.

**Relation to nonlinear ICA.** Hyvarinen and Pajunen [32] showed that the standard nonlinear ICA problem where the decoder \(\bm{f}\) is nonlinear and the latent factors \(\bm{z}_{i}\) are _statistically independent_ is unidentifiable. This motivated various extensions of nonlinear ICA where more structure on the factors is assumed [30, 31, 33, 36, 37, 27]. Our approach departs from the standard nonlinear ICA problem along three axes: (i) we restrict the mixing function to be additive, (ii) the factors do not have to be necessarily independent, and (iii) we can identify only the blocks \(\bm{z}_{B}\) as opposed to each \(\bm{z}_{i}\) individually up to element-wise transformations, unless \(\mathcal{B}=\{\{1\},...,\{d_{z}\}\}\) (see Section 3.1).

**Object-centric representation learning (OCRL).** Lin et al. [45] classified OCRL methods in two categories: _scene mixture models_[22, 23, 24, 51] & _spatial-attention models_[19, 12, 11, 18]. Additive decoders can be seen as an approximation to the decoding architectures used in the former category, which typically consist of an object-specific decoder \(\bm{f}^{\text{(obj)}}\) acting on object-specific latent blocks \(\bm{z}_{B}\) and "mixed" together via a masking mechanism \(\bm{m}^{(B)}(\bm{z})\) which selects which pixel belongs to which object. More precisely,

\[\bm{f}(\bm{z})=\sum_{B\in\mathcal{B}}\bm{m}^{(B)}(\bm{z})\odot\bm{f}^{\text{ (obj)}}(\bm{z}_{B})\;\text{, where }\bm{m}^{(B)}_{k}(\bm{z})=\frac{\exp(\bm{a}_{k}(\bm{z}_{B}))}{\sum_{B^{ \prime}\in\mathcal{B}}\exp(\bm{a}_{k}(\bm{z}_{B^{\prime}}))}\;,\] (1)

and where \(\mathcal{B}\) is a partition of \([d_{z}]\) made of equal-size blocks \(B\) and \(\bm{a}:\mathbb{R}^{|B|}\to\mathbb{R}^{d_{x}}\) outputs a score that is normalized via a softmax operation to obtain the masks \(\bm{m}^{(B)}(\bm{z})\). Many of these works also present some mechanism to select dynamically how many objects are present in the scene and thus have a variable-size representation \(\bm{z}\), an important technical aspect we omit in our analysis. Empirically, training these decoders based on some form of reconstruction objective, probabilistic or not, yields latent blocks \(\bm{z}_{B}\) that represent the information of individual objects separately. We believe our work constitutes a step towards providing a mathematically grounded explanation for why these approaches can perform this form of disentanglement without supervision (Theorems 1 & 2). Many architectural innovations in scene mixture models concern the encoder, but our analysis focuses solely on the structure of the decoder \(\bm{f}(\bm{z})\), which is a shared aspect across multiple methods. Generalization capabilities of object-centric representations were studied empirically by Dittadi et al. [14] but did not cover Cartesian-product extrapolation (Corollary 3) on which we focus here.

**Diagonal Hessian penalty [58].** Additive decoders are also closely related to the penalty introduced by Peebles et al. [58] which consists in regularizing the Hessian of the decoder to be diagonal. In Appendix A.2, we show that "additivity" and "diagonal Hessian" are equivalent properties. They showed empirically that this penalty can induce disentanglement on datasets such as CLEVR [35], which is a standard benchmark for OCRL, but did not provide any formal justification. Our work provides a rigorous explanation for these successes and highlights the link between the diagonal Hessian penalty and OCRL.

**Compositional decoders [7].** Compositional decoders were recently introduced by Brady et al. [7] as a model for OCRL methods with identifiability guarantees. A decoder \(\bm{f}\) is said to be _compositional_ when its Jacobian \(D\bm{f}\) satisfies the following property everywhere: For all \(i\in[d_{z}]\) and \(B\in\mathcal{B}\), \(D_{B}\bm{f}_{i}(\bm{z})\neq\bm{0}\implies D_{B^{c}}\bm{f}_{i}(\bm{z})=\bm{0}\), where \(B^{c}:=[d_{z}]\setminus B\). In other words, each \(\bm{x}_{i}\) can _locally_ depend solely on one block \(\bm{z}_{B}\) (this block can change for different \(\bm{z}\)). In Appendix A.3, we show that compositional \(C^{2}\) decoders are additive. Furthermore, Example 3 shows a decoder that is additive but not compositional, which means that additive \(C^{2}\) decoders are strictly more expressive than compositional \(C^{2}\) decoders. Another important distinction with our work is that we consider more general supports for \(\bm{z}\) and provide a novel extrapolation analysis. That being said, our identifiability result does not supersede theirs since they assume only \(C^{1}\) decoders while our theory assumes \(C^{2}\).

**Extrapolation.** Du and Mordatch [17] studied empirically how one can combine energy-based models for what they call _compositional generalization_, which is similar to our notion of Cartesian-product extrapolation, but suppose access to datasets in which only one latent factor varies and do not provide any theory. Webb et al. [70] studied extrapolation empirically and proposed a novel benchmark which does not have an additive structure. Besserve et al. [5] proposed a theoretical framework in which out-of-distribution samples are obtained by applying a transformation to a single hidden layer inside the decoder network. Krueger et al. [39] introduced a domain generalization method which is trained to be robust to tasks falling outside the convex hull of training distributions. Extrapolation in text-conditioned image generation was recently discussed by Wang et al. [69].

## 3 Additive decoders for disentanglement & extrapolation

Our theoretical results assume the existence of some data-generating process describing how the observations \(\bm{x}\) are generated and, importantly, what are the "natural" factors of variations.

**Assumption 1** (Data-generating process).: _The set of possible observations is given by a lower dimensional manifold \(\bm{f}(\mathcal{Z}^{\text{test}})\) embedded in \(\mathbb{R}^{d_{x}}\) where \(\mathcal{Z}^{\text{test}}\) is an open set of \(\mathbb{R}^{d_{z}}\) and \(\bm{f}:\mathcal{Z}^{\text{test}}\to\mathbb{R}^{d_{x}}\) is a \(C^{2}\)-diffeomorphism onto its image. We will refer to \(\bm{f}\) as the ground-truth decoder. At training time, the observations are i.i.d. samples given by \(\bm{x}=\bm{f}(\bm{z})\) where \(\bm{z}\) is distributed according to the probability measure \(\mathbb{F}_{\bm{z}}^{\text{train}}\) with support \(\mathcal{Z}^{\text{train}}\subseteq\mathcal{Z}^{\text{test}}\). Throughout, we assume that \(\mathcal{Z}^{\text{train}}\) is regularly closed (Definition 6)._

Intuitively, the ground-truth decoder \(\bm{f}\) is effectively relating the "natural factors of variations" \(\bm{z}\) to the observations \(\bm{x}\) in a one-to-one fashion. The map \(\bm{f}\) is a \(C^{2}\)-diffeomorphism onto its image, which means that it is \(C^{2}\) (has continuous second derivative) and that its inverse (restricted to the image of \(\bm{f}\)) is also \(C^{2}\). Analogous assumptions are very common in the literature on nonlinear ICA and disentanglement [33; 36; 42; 1]. Mansouri et al. [52] pointed out that the injectivity of \(\bm{f}\) is violated when images show two objects that are indistinguishable, an important practical case that is not covered by our theory.

We emphasize the distinction between \(\mathcal{Z}^{\text{train}}\), which corresponds to the observations seen during training, and \(\mathcal{Z}^{\text{test}}\), which corresponds to the set of all possible images. The case where \(\mathcal{Z}^{\text{train}}\neq\mathcal{Z}^{\text{test}}\) will be of particular interest when discussing extrapolation in Section 3.2. The "regularly closed" condition on \(\mathcal{Z}^{\text{train}}\) is mild, as it is satisfied as soon as the distribution of \(\bm{z}\) has a density w.r.t. the Lebesgue measure on \(\mathbb{R}^{d_{z}}\). It is violated, for example, when \(\bm{z}\) is a discrete random vector. Figure 2 illustrates this assumption with simple examples.

**Objective.** Our analysis is based on the simple objective of reconstructing the observations \(\bm{x}\) by learning an encoder \(\hat{\bm{g}}:\mathbb{R}^{d_{x}}\to\mathbb{R}^{d_{z}}\) and a decoder \(\hat{\bm{f}}:\mathbb{R}^{d_{z}}\to\mathbb{R}^{d_{x}}\). Note that we assumed implicitly that the dimensionality of the learned representation matches the dimensionality of the ground-truth. We define the set of latent codes the encoder can output when evaluated on the training distribution:

\[\hat{\mathcal{Z}}^{\text{train}}:=\hat{\bm{g}}(\bm{f}(\mathcal{Z}^{\text{train} }))\,.\] (2)

When the images of the ground-truth and learned decoders match, i.e. \(\bm{f}(\mathcal{Z}^{\text{train}})=\hat{\bm{f}}(\hat{\mathcal{Z}}^{\text{ train}})\), which happens when the reconstruction task is solved exactly, one can define the map \(\bm{v}:\mathcal{Z}^{\text{train}}\to\mathcal{Z}^{\text{train}}\) as

\[\bm{v}:=\bm{f}^{-1}\circ\hat{\bm{f}}\,.\] (3)

This function is going to be crucial throughout the work, especially to define \(\mathcal{B}\)-disentanglement (Definition 3), as it relates the learned representation to the ground-truth representation.

Before introducing our formal definition of additive decoders, we introduce the following notation: Given a set \(\mathcal{Z}\subseteq\mathbb{R}^{d_{z}}\) and a subset of indices \(B\subseteq[d_{z}]\), let us define \(\mathcal{Z}_{B}\) to be the projection of \(\mathcal{Z}\) onto dimensions labelled by the index set \(B\). More formally,

\[\mathcal{Z}_{B}:=\{\bm{z}_{B}\mid\bm{z}\in\mathcal{Z}\}\subseteq\mathbb{R}^{|B |}\,.\] (4)

Intuitively, we will say that a decoder is _additive_ when its output is the summation of the outputs of "object-specific" decoders that depend only on each latent block \(\bm{z}_{B}\). This captures the idea that an image can be seen as the juxtaposition of multiple images which individually correspond to objects in the scene or natural factors of variations (left of Figure 1).

**Definition 1** (Additive functions).: _Let \(\mathcal{B}\) be a partition of \([d_{z}]\)1. A function \(\bm{f}:\mathcal{Z}\to\mathbb{R}^{d_{x}}\) is said to be **additive** if there exist functions \(\bm{f}^{(B)}:\mathcal{Z}_{B}\to\mathbb{R}^{d_{x}}\) for all \(B\in\mathcal{B}\) such that_

Footnote 1: Without loss of generality, we assume that the partition \(\mathcal{B}\) is contiguous, i.e. each \(B\in\mathcal{B}\) can be written as \(B=\{i+1,i+2,\ldots,i+|B|\}\).

\[\forall\bm{z}\in\mathcal{Z},\bm{f}(\bm{z})=\sum_{B\in\mathcal{B}}\bm{f}^{(B)} (\bm{z}_{B})\,.\] (5)

This additivity property will be central to our analysis as it will be the driving force of identifiability (Theorem 1 & 2) and Cartesian-product extrapolation (Corollary 3).

**Remark 1**.: _Suppose we have \(\bm{x}=\sigma(\sum_{B\in\mathcal{B}}\bm{f}^{(B)}(\bm{z}_{B}))\) where \(\sigma\) is a known bijective function. For example, if \(\sigma(\bm{y}):=\exp(\bm{y})\) (component-wise), the decoder can be thought of as being multiplicative. Our results still apply since we can simply transform the data doing \(\tilde{\bm{x}}:=\sigma^{-1}(\bm{x})\) to recover the additive form \(\tilde{\bm{x}}=\sum_{B\in\mathcal{B}}\bm{f}^{(B)}(\bm{z}_{B})\)._

**Differences with OCR in practice.** We point out that, although the additive decoders make intuitive sense for OCRL, they are not expressive enough to represent the "masked decoders" typically used in practice (Equation (1)). The lack of additivity stems from the normalization in the masks \(\bm{m}^{(B)}(\bm{z})\). We hypothesize that studying the simpler additive decoders might still reveal interesting phenomena present in modern OCRL approaches due to their resemblance. Another difference is that, in practice, the same object-specific decoder \(\bm{f}^{(\text{obj})}\) is applied to every latent block \(\bm{z}_{B}\). Our theory allows for these functions to be different, but also applies when functions are the same. Additionally, this parameter sharing across \(\bm{f}^{(B)}\) enables modern methods to have a variable number of objects across samples, an important practical point our theory does not cover.

### Identifiability analysis

We now study the identifiability of additive decoders and show how they can yield disentanglement. Our definition of disentanglement will rely on _partition-respecting permutations_:

**Definition 2** (Partition-respecting permutations).: _Let \(\mathcal{B}\) be a partition of \(\{1,...,d_{z}\}\). A permutation \(\pi\) over \(\{1,...,d_{z}\}\) respects \(\mathcal{B}\) if, for all \(B\in\mathcal{B},\;\pi(B)\in\mathcal{B}\)._

Essentially, a permutation that respects \(\mathcal{B}\) is one which can permute blocks of \(\mathcal{B}\) and permute elements within a block, but cannot "mix" blocks together. We now introduce \(\mathcal{B}\)-disentanglement.

**Definition 3** (\(\mathcal{B}\)-disentanglement).: _A learned decoder \(\hat{\bm{f}}:\mathbb{R}^{d_{z}}\to\mathbb{R}^{d_{x}}\) is said to be \(\mathcal{B}\)**-disentangled** w.r.t. the ground-truth decoder \(\bm{f}\) when \(\bm{f}(\mathcal{Z}^{\text{train}})=\hat{\bm{f}}(\hat{\mathcal{Z}}^{\text{ train}})\) and the mapping \(\bm{v}:=\bm{f}^{-1}\circ\hat{\bm{f}}\) is a diffeomorphism from \(\hat{\mathcal{Z}}^{\text{train}}\) to \(\mathcal{Z}^{\text{train}}\) satisfying the following property: there exists a permutation \(\pi\) respecting \(\mathcal{B}\) such that, for all \(B\in\mathcal{B}\), there exists a function \(\bar{\bm{v}}_{\pi(B)}:\hat{\mathcal{Z}}^{\text{train}}_{B}\to\mathcal{Z}^{ \text{train}}_{\pi(B)}\) such that, for all \(\bm{z}\in\hat{\mathcal{Z}}^{\text{train}}\), \(\bm{v}_{\pi(B)}(\bm{z})=\bar{\bm{v}}_{\pi(B)}(\bm{z}_{B})\). In other words, \(\bm{v}_{\pi(B)}(\bm{z})\) depends only on \(\bm{z}_{B}\)._

Thus, \(\mathcal{B}\)-disentanglement means that the blocks of latent dimensions \(\bm{z}_{B}\) are disentangled from one another, but that variables within a given block might remain entangled. Note that, unless the partition is \(\mathcal{B}=\{\{1\},\ldots,\{d_{z}\}\}\), this corresponds to a weaker form of disentanglement than what is typically seeked in nonlinear ICA, i.e. recovering each variable individually.

**Example 1**.: _To illustrate \(\mathcal{B}\)-disentanglement, imagine a scene consisting of two balls moving around in 2D where the "ground-truth" representation is given by \(\bm{z}=(x^{1},y^{1},x^{2},y^{2})\) where \(\bm{z}_{B_{1}}=(x^{1},y^{1})\) and \(\bm{z}_{B_{2}}=(x^{2},y^{2})\) are the coordinates of each ball (here, \(\mathcal{B}:=\{\{1,2\},\{3,4\}\}\)). In that case, a learned representation is \(\mathcal{B}\)-disentangled when the balls are disentangled from one another. However, the basis in which the position of each ball is represented might differ in both representations._Our first result (Theorem 1) shows a weaker form of disentanglement we call _local_\(\mathcal{B}\)-disentanglement. This means the Jacobian matrix of \(\bm{v}\), \(D\bm{v}\), has a "block-permutation" structure everywhere.

**Definition 4** (Local \(\mathcal{B}\)-disentanglement).: _A learned decoder \(\hat{\bm{f}}:\mathbb{R}^{d_{z}}\to\mathbb{R}^{d_{x}}\) is said to be **locally \(\mathcal{B}\)-disentangled** w.r.t. the ground-truth decoder \(\bm{f}\) when \(\bm{f}(\mathcal{Z}^{\mathrm{train}})=\hat{\bm{f}}(\hat{\mathcal{Z}}^{\mathrm{ train}})\) and the mapping \(\bm{v}:=\bm{f}^{-1}\circ\hat{\bm{f}}\) is a diffeomorphism from \(\hat{\mathcal{Z}}^{\mathrm{train}}\) to \(\mathcal{Z}^{\mathrm{train}}\) with a mapping \(\bm{v}:\hat{\mathcal{Z}}^{\mathrm{train}}\to\mathcal{Z}^{\mathrm{train}}\) satisfying the following property: for all \(\bm{z}\in\hat{\mathcal{Z}}^{\mathrm{train}}\), there exists a permutation \(\pi\) respecting \(\mathcal{B}\) such that, for all \(B\in\mathcal{B}\), the columns of \(D\bm{v}_{\pi(B)}(\bm{z})\in\mathbb{R}^{|B|\times d_{z}}\) outside block \(B\) are zero._

In Appendix A.4, we provide three examples where local disentanglement holds but not global disentanglement. The first one illustrates how having a disconnected support can allow for a permutation \(\pi\) (from Definition 4) that changes between disconnected regions of the support. The last two examples show how, even if the permutation stays the same throughout the support, we can still violate global disentanglement, even with a connected support.

We now state the main identifiability result of this work which provides conditions to guarantee _local_ disentanglement. We will then see how to go from local to _global_ disentanglement in the subsequent Theorem 2. For pedagogical reasons, we delay the formalization of the sufficient nonlinearity Assumption 2 on which the result crucially relies.

**Theorem 1** (Local disentanglement via additive decoders).: _Suppose that the data-generating process satisfies Assumption 1, that the learned decoder \(\hat{\bm{f}}:\mathbb{R}^{d_{z}}\to\mathbb{R}^{d_{x}}\) is a \(C^{2}\)-diffeomorphism, that the encoder \(\hat{\bm{g}}:\mathbb{R}^{d_{x}}\to\mathbb{R}^{d_{z}}\) is continuous, that both \(\bm{f}\) and \(\hat{\bm{f}}\) are additive (Definition 1) and that \(\bm{f}\) is sufficiently nonlinear as formalized by Assumption 2. Then, if \(\hat{\bm{f}}\) and \(\hat{\bm{g}}\) solve the reconstruction problem on the training distribution, i.e. \(\mathbb{E}^{\mathrm{train}}||\bm{x}-\hat{\bm{f}}(\hat{\bm{g}}(\bm{x}))||^{2}=0\), we have that \(\hat{\bm{f}}\) is locally \(\mathcal{B}\)-disentangled w.r.t. \(\bm{f}\) (Definition 4)._

The proof of Theorem 1, which can be found in Appendix A.5, is inspired from Hyvarinen et al. [33]. The essential differences are that (i) they leverage the additivity of the conditional log-density of \(\bm{z}\) given an auxiliary variable \(\bm{u}\) (i.e. conditional independence) instead of the additivity of the decoder function \(\bm{f}\), (ii) we extend their proof techniques to allow for "block" disentanglement, i.e. when \(\mathcal{B}\) is not the trivial partition \(\{\{1\},\ldots,\{d_{z}\}\}\), (iii) the asssumption "sufficient variability" of the prior \(p(\bm{z}\mid\bm{u})\) of Hyvarinen et al. [33] is replaced by an analogous assumption of "sufficient nonlinearity" of the decoder \(\bm{f}\) (Assumption 2), and (iv) we consider much more general supports \(\mathcal{Z}^{\mathrm{train}}\) which makes the jump from local to global disentanglement less direct in our case.

**The identifiability-expressivity trade-off.** The level of granularity of the partition \(\mathcal{B}\) controls the trade-off between identifiability and expressivity: the finer the partition, the tighter the identifiability guarantee but the less expressive is the function class. The optimal level of granularity is going to dependent on the application at hand. Whether \(\mathcal{B}\) could be learned from data is left for future work.

**Sufficient nonlinearity.** The following assumption is key in proving Theorem 2, as it requires that the ground-truth decoder is "sufficiently nonlinear". This is reminiscent of the "sufficient variability" assumptions found in the nonlinear ICA litterature, which usually concerns the distribution of the latent variable \(\bm{z}\) as opposed to the decoder \(\bm{f}\)[30, 31, 33, 36, 37, 42, 73]. We clarify this link in Appendix A.6 and provide intuitions why sufficient nonlinearity can be satisfied when \(d_{x}\gg d_{z}\).

**Assumption 2** (Sufficient nonlinearity of \(\bm{f}\)).: _Let \(q:=d_{z}+\sum_{B\in\mathcal{B}}\frac{|B(|B|+1)}{2}\). For all \(\bm{z}\in\mathcal{Z}^{\mathrm{train}}\), \(\bm{f}\) is such that the following matrix has linearly independent columns (i.e. full column-rank):_

\[\bm{W}(\bm{z}):=\left[\left[D_{i}\bm{f}^{(B)}(\bm{z}_{B})\right]_{i\in B}\, \left[D_{i,i^{\prime}}^{2}\bm{f}^{(B)}(\bm{z}_{B})\right]_{(i,i^{\prime})\in B _{\leq}^{2}}\right]_{B\in\mathcal{B}}\in\mathbb{R}^{d_{x}\times q}\,,\] (6)

_where \(B_{\leq}^{2}:=B^{2}\cap\{(i,i^{\prime})\mid i^{\prime}\leq i\}\). Note this implies \(d_{x}\geq q\)._

The following example shows that Theorem 1 does not apply if the ground-truth decoder \(\bm{f}\) is linear. If that was the case, it would contradict the well known fact that linear ICA with independent Gaussian factors is unidentifiable.

**Example 2** (Importance of Assumption 2).: _Suppose \(\bm{x}=\bm{f}(\bm{z})=\bm{A}\bm{z}\) where \(\bm{A}\in\mathbb{R}^{d_{x}\times d_{z}}\) is full rank. Take \(\hat{\bm{f}}(\bm{z}):=\bm{A}\bm{V}\bm{z}\) and \(\hat{\bm{g}}(\bm{x}):=\bm{V}^{-1}\bm{A}^{\dagger}\bm{x}\) where \(\bm{V}\in\mathbb{R}^{d_{z}\times d_{z}}\) is invertible and \(\bm{A}^{\dagger}\) is the left pseudo inverse of \(\bm{A}\). By construction, we have that \(\mathbb{E}[\bm{x}-\hat{\bm{f}}(\hat{\bm{g}}(\bm{x}))]=0\) and \(\bm{f}\) and \(\hat{\bm{f}}\) are \(\mathcal{B}\)-additive because \(\bm{f}(\bm{z})=\sum_{B\in\mathcal{B}}\bm{A}_{\cdot,B}\bm{z}_{B}\) and \(\hat{\bm{f}}(\bm{z})=\sum_{B\in\mathcal{B}}(\bm{AV})_{\cdot,B}\bm{z}_{B}\). However, we still have that \(\bm{v}(\bm{z}):=\bm{f}^{-1}\circ\hat{\bm{f}}(\bm{z})=\bm{V}\bm{z}\) where \(\bm{V}\) does not necessarily have a block-permutation structure, i.e. no disentanglement. The reason we cannot apply Theorem 1 here is because Assumption 2 is not satisfied. Indeed, the second derivatives of \(\bm{f}^{(B)}(\bm{z}_{B}):=\bm{A}_{\cdot,B}\bm{z}_{B}\) are all zero and hence \(\bm{W}(\bm{z})\) cannot have full column-rank._

**Example 3** (A sufficiently nonlinear \(\bm{f}\)).: _In Appendix A.7 we show numerically that the function_

\[\bm{f}(\bm{z}):=[\bm{z}_{1},\bm{z}_{1}^{2},\bm{z}_{1}^{3},\bm{z}_{1}^{4}]^{ \top}+[(\bm{z}_{2}+1),(\bm{z}_{2}+1)^{2},(\bm{z}_{2}+1)^{3},(\bm{z}_{2}+1)^{4} ]^{\top}\] (7)

_is a diffeomorphism from the square \([-1,0]\times[0,1]\) to its image that satisfies Assumption 2._

**Example 4** (Smooth balls dataset is sufficiently nonlinear).: _In Appendix A.7 we present a simple synthetic dataset consisting of images of two colored balls moving up and down. We also verify numerically that its underlying ground-truth decoder \(\bm{f}\) is sufficiently nonlinear._

#### 3.1.1 From local to global disentanglement

The following result provides additional assumptions to guarantee _global_ disentanglement (Definition 3) as opposed to only local disentanglement (Definition 4). See Appendix A.8 for its proof.

**Theorem 2** (From local to global disentanglement).: _Suppose that all the assumptions of Theorem 1 hold. Additionally, assume \(\mathcal{Z}^{\mathrm{train}}\) is path-connected (Definition 8) and that the block-specific decoders \(\bm{f}^{(B)}\) and \(\hat{\bm{f}}^{(B)}\) are injective for all blocks \(B\in\mathcal{B}\). Then, if \(\hat{\bm{f}}\) and \(\hat{\bm{g}}\) solve the reconstruction problem on the training distribution, i.e. \(\mathbb{E}^{\mathrm{train}}||\bm{x}-\hat{\bm{f}}(\hat{\bm{g}}(\bm{x}))||^{2}=0\), we have that \(\hat{\bm{f}}\) is (globally) \(\mathcal{B}\)-disentangled w.r.t. \(\bm{f}\) (Definition 3) and, for all \(B\in\mathcal{B}\),_

\[\hat{\bm{f}}^{(B)}(\bm{z}_{B})=\bm{f}^{(\pi(B))}(\bar{\bm{v}}_{\pi(B)}(\bm{z} _{B}))+\bm{c}^{(B)},\,\text{for all }\bm{z}_{B}\in\hat{\mathcal{Z}}_{B}^{\mathrm{ train}}\,,\] (8)

_where the functions \(\bar{\bm{v}}_{\pi(B)}\) are from Defition 3 and the vectors \(\bm{c}^{(B)}\in\mathbb{R}^{d_{x}}\) are constants such that \(\sum_{B\in\mathcal{B}}\bm{c}^{(B)}=0\). We also have that the functions \(\bar{\bm{v}}_{\pi(B)}:\hat{\mathcal{Z}}_{B}^{\mathrm{train}}\to\mathcal{Z}_{ \pi(B)}^{\mathrm{train}}\) are \(C^{2}\)-diffeomorphisms and have the following form:_

\[\bar{\bm{v}}_{\pi(B)}(\bm{z}_{B})=(\bm{f}^{\pi(B)})^{-1}(\hat{\bm{f}}^{(B)}( \bm{z}_{B})-\bm{c}^{(B)}),\text{ for all }\bm{z}_{B}\in\hat{\mathcal{Z}}_{B}^{ \mathrm{train}}\,.\] (9)

Equation (8) in the above result shows that each block-specific learned decoder \(\hat{\bm{f}}^{(B)}\) is "imitating" a block-specific ground-truth decoder \(\bm{f}^{\pi(B)}\). Indeed, the "object-specific" image outputted by the decoder \(\hat{\bm{f}}^{(B)}\) evaluated at some \(\bm{z}_{B}\in\hat{\mathcal{Z}}_{B}^{\mathrm{train}}\) is the same as the image outputted by \(\bm{f}^{(B)}\) evaluated at \(\bm{v}(\bm{z}_{B})\in\mathcal{Z}_{B}^{\mathrm{train}}\), _up to an additive constant vector \(\bm{c}^{(B)}\)_. These constants cancel each other out when taking the sum of the block-specific decoders.

Equation (9) provides an explicit form for the function \(\bar{\bm{v}}_{\pi(B)}\), which is essentially the learned block-specific decoder composed with the inverse of the ground-truth block-specific decoder.

**Additional assumptions to go from local to global.** Assuming that the support of \(\mathbb{E}^{\mathrm{train}}\), \(\mathcal{Z}^{\mathrm{train}}\), is **path-connected** (see Definition 8 in appendix) is useful since it prevents the permutation \(\pi\) of Definition 4 from changing between two disconnected regions of \(\hat{\mathcal{Z}}^{\mathrm{train}}\). See Figure 2 for an illustration. In Appendix A.9, we discuss the additional assumption that each \(\bm{f}^{(B)}\) must be injective and show that, in general, it is not equivalent to the assumption that \(\sum_{B\in\mathcal{B}}\bm{f}^{(B)}\) is injective.

### Cartesian-product extrapolation

In this section, we show how a learned additive decoder can be used to generate images \(\bm{x}\) that are "out of support" in the sense that \(\bm{x}\not\in\bm{f}(\mathcal{Z}^{\mathrm{train}})\), but that are still on the manifold of "reasonable" images, i.e. \(\bm{x}\in\bm{f}(\mathcal{Z}^{\mathrm{test}})\). To characterize the set of images the learned decoder can generate, we will rely on the notion of "cartesian-product extension", which we define next.

Figure 2: Illustrating regularly closed sets (Definition 6) and path-connected sets (Definition 8). Theorem 2 requires \(\mathcal{Z}^{\mathrm{train}}\) to satisfy both properties.

**Definition 5** (Cartesian-product extension).: _Given a set \(\mathcal{Z}\subseteq\mathbb{R}^{d_{z}}\) and partition \(\mathcal{B}\) of \([d_{z}]\), we define the Cartesian-product extension of \(\mathcal{Z}\) as_

\[\text{\rm CPE}_{\mathcal{B}}(\mathcal{Z}):=\prod_{B\in\mathcal{B}}\mathcal{Z}_{ B}\,,\text{where }\mathcal{Z}_{B}:=\{\bm{z}_{B}\mid\bm{z}\in\mathcal{Z}\}.\]

_It is indeed an extension of \(\mathcal{Z}\) since \(\mathcal{Z}\subseteq\prod_{B\in\mathcal{B}}\mathcal{Z}_{B}\)._

Let us define \(\bar{\bm{v}}:\text{\rm CPE}_{\mathcal{B}}(\hat{\mathcal{Z}}^{\text{train}}) \to\text{\rm CPE}_{\mathcal{B}}(\mathcal{Z}^{\text{train}})\) to be the natural extension of the function \(\bm{v}:\hat{\mathcal{Z}}^{\text{train}}\to\mathcal{Z}^{\text{train}}\). More explicitly, \(\bar{\bm{v}}\) is the "concatenation" of the functions \(\bar{\bm{v}}_{B}\) given in Definition 3:

\[\bar{\bm{v}}(\bm{z})^{\top}:=[\bar{\bm{v}}_{B_{1}}(\bm{z}_{\pi^{-1}(B_{1})})^{ \top}\cdots\bar{\bm{v}}_{B_{\ell}}(\bm{z}_{\pi^{-1}(B_{\ell})})^{\top}]\,,\] (10)

where \(\ell\) is the number of blocks in \(\mathcal{B}\). This map is a diffeomorphism because each \(\bar{\bm{v}}_{\pi(B)}\) is a diffeomorphism from \(\hat{\mathcal{Z}}_{B}^{\text{train}}\) to \(\mathcal{Z}_{\pi(B)}^{\text{train}}\) by Theorem 2.

We already know that \(\hat{\bm{f}}(\bm{z})=\bm{f}\circ\bar{\bm{v}}(\bm{z})\) for all \(\bm{z}\in\hat{\mathcal{Z}}^{\text{train}}\). The following result shows that this equality holds in fact on the larger set \(\text{\rm CPE}_{\mathcal{B}}(\hat{\mathcal{Z}}^{\text{train}})\), the Cartesian-product extension of \(\hat{\mathcal{Z}}^{\text{train}}\). See right of Figure 1 for an illustration of the following corollary.

**Corollary 3** (Cartesian-product extrapolation).: _Suppose the assumptions of Theorem 2 holds. Then,_

\[\text{for all }\bm{z}\in\text{\rm CPE}_{\mathcal{B}}(\hat{\mathcal{Z}}^{ \text{train}}),\;\sum_{B\in\mathcal{B}}\hat{\bm{f}}^{(B)}(\bm{z}_{B})=\sum_{B \in\mathcal{B}}\bm{f}^{(\pi(B))}(\bar{\bm{v}}_{\pi(B)}(\bm{z}_{B}))\,.\] (11)

_Furthermore, if \(\text{\rm CPE}_{\mathcal{B}}(\mathcal{Z}^{\text{train}})\subseteq\mathcal{Z}^ {\text{test}}\), then \(\hat{\bm{f}}(\text{\rm CPE}_{\mathcal{B}}(\hat{\mathcal{Z}}^{\text{train}})) \subseteq\bm{f}(\mathcal{Z}^{\text{test}})\)._

Equation (11) tells us that the learned decoder \(\hat{\bm{f}}\) "imitates" the ground-truth \(\bm{f}\) not just over \(\hat{\mathcal{Z}}^{\text{train}}\), but also over its Cartesian-product extension. This is important since it guarantees that we can generate observations never seen during training as follows: Choose a latent vector \(\bm{z}^{\text{new}}\) that is in the Cartesian-product extension of \(\hat{\mathcal{Z}}^{\text{train}}\), but not in \(\hat{\mathcal{Z}}^{\text{train}}\) itself, i.e. \(\bm{z}^{\text{new}}\in\text{\rm CPE}_{\mathcal{B}}(\hat{\mathcal{Z}}^{\text{ train}})\setminus\hat{\mathcal{Z}}^{\text{train}}\). Then, evaluate the learned decoder on \(\bm{z}^{\text{new}}\) to get \(\bm{x}^{\text{new}}:=\hat{\bm{f}}(\bm{z}^{\text{new}})\). By Corollary 3, we know that \(\bm{x}^{\text{new}}=\bm{f}\circ\bar{\bm{v}}(\bm{z}^{\text{new}})\), i.e. it is the observation one would have obtain by evaluating the ground-truth decoder \(\bm{f}\) on the point \(\bar{\bm{v}}(\bm{z}^{\text{new}})\in\text{\rm CPE}_{\mathcal{B}}(\mathcal{Z}^ {\text{train}})\). In addition, this \(\bm{x}^{\text{new}}\) has never been seen during training since \(\bar{\bm{v}}(\bm{z}^{\text{new}})\not\in\bar{\bm{v}}(\hat{\mathcal{Z}}^{\text{ train}})=\mathcal{Z}^{\text{train}}\). The experiment of Figure 4 illustrates this procedure.

**About the extra assumption "\(\text{\rm CPE}_{\mathcal{B}}(\mathcal{Z}^{\text{train}})\subseteq\mathcal{Z}^ {\text{test}}\)".** Recall that, in Assumption 1, we interpreted \(\bm{f}(\mathcal{Z}^{\text{test}})\) to be the set of "reasonable" observations \(\bm{x}\), of which we only observe a subset \(\bm{f}(\mathcal{Z}^{\text{train}})\). Under this interpretation, \(\mathcal{Z}^{\text{test}}\) is the set of reasonable values for the vector \(\bm{z}\) and the additional assumption that \(\text{\rm CPE}_{\mathcal{B}}(\mathcal{Z}^{\text{train}})\subseteq\mathcal{Z}^ {\text{test}}\) in Corollary 3 requires that the Cartesian-product extension of \(\mathcal{Z}^{\text{train}}\) consists only of reasonable values of \(\bm{z}\). From this assumption, we can easily conclude that \(\hat{\bm{f}}(\text{\rm CPE}_{\mathcal{B}}(\hat{\mathcal{Z}}^{\text{train}})) \subseteq\bm{f}(\mathcal{Z}^{\text{test}})\), which can be interpreted as: "The novel observations \(\bm{x}^{\text{new}}\) obtained via Cartesian-product extrapolation are _reasonable_". Appendix A.11 describes an example where the assumption is violated, i.e. \(\text{\rm CPE}_{\mathcal{B}}(\mathcal{Z}^{\text{train}})\not\subseteq\mathcal{Z} ^{\text{test}}\). The practical implication of this is that the new observations \(\bm{x}^{\text{new}}\) obtained via Cartesian-product extrapolation might not always be reasonable.

**Disentanglement is not enough for extrapolation.** To the best of our knowledge, Corollary 3 is the first result that formalizes how disentanglement can induce extrapolation. We believe it illustrates the fact that disentanglement alone is not sufficient to enable extrapolation and that one needs to restrict the hypothesis class of decoders in some way. Indeed, given a learned decoder \(\hat{\bm{f}}\) that is disentangled w.r.t. \(\bm{f}\) on the training support \(\mathcal{Z}^{\text{train}}\), one cannot guarantee both decoders will "agree" outside the training domain without further restricting \(\hat{\bm{f}}\) and \(\bm{f}\). This work has focused on "additivity", but we believe other types of restriction could correspond to other types of extrapolation.

## 4 Experiments

We now present empirical validations of the theoretical results presented earlier. To achieve this, we compare the ability of additive and non-additive decoders to both identify ground-truth latent factors (Theorems 1 & 2) and extrapolate (Corollary 3) when trained to solve the reconstruction task on simple images (\(64\times 64\times 3\)) consisting of two balls moving in space [2]. See Appendix B.1for training details. We consider two datasets: one where the two ball positions can only vary along the \(y\)-axis (**ScalarLatents**) and one where the positions can vary along both the \(x\) and \(y\) axes (**BlockLatents**).

**ScalarLatents:** The ground-truth latent vector \(\bm{z}\in\mathbb{R}^{2}\) is such that \(\bm{z}_{1}\) and \(\bm{z}_{2}\) corresponds to the height (y-coordinate) of the first and second ball, respectively. Thus the partition is simply \(\mathcal{B}=\{\{1\},\{2\}\}\) (each object has only one latent factor). This simple setting is interesting to study since the low dimensionality of the latent space (\(d_{z}=2\)) allows for exhaustive visualizations like Figure 4. To study Cartesian-product extrapolation (Corollary 3), we sample \(\bm{z}\) from a distribution with a L-shaped support given by \(\mathcal{Z}^{\text{train}}:=[0,1]\times[0,1]\setminus[0.5,1]\times[0.5,1]\), so that the training set does not contain images where both balls appear in the upper half of the image (see Appendix B.2).

**BlockLatents:** The ground-truth latent vector \(\bm{z}\in\mathbb{R}^{4}\) is such that \(\bm{z}_{\{1,2\}}\) and \(\bm{z}_{\{3,4\}}\) correspond to the \(x,y\) position of the first and second ball, respectively (the partition is simply \(\mathcal{B}=\{\{1,2\},\{3,4\}\}\), i.e. each object has two latent factors). Thus, this more challenging setting illustrates "block-disentanglement". The latent \(\bm{z}\) is sampled uniformly from the hypercube \([0,1]^{4}\) but the images presenting occlusion (when a ball is behind another) are rejected from the dataset. We discuss how additive decoders cannot model images presenting occlusion in Appendix A.12. We also present an additional version of this dataset where we sample from the hypercube \([0,1]^{4}\) with dependencies. See Appendix B.2 for more details about data generation.

**Evaluation metrics:** To evaluate disentanglement, we compute a matrix of scores \((s_{B,B^{\prime}})\in\mathbb{R}^{\ell\times\ell}\) where \(\ell\) is the number of blocks in \(\mathcal{B}\) and \(s_{B,B^{\prime}}\) is a score measuring how well we can predict the ground-truth block \(\bm{z}_{B}\) from the learned latent block \(\hat{\bm{z}}_{B^{\prime}}=\hat{\bm{g}}_{B^{\prime}}(\bm{x})\) outputted by the encoder. The final Latent Matching Score (LMS) is computed as \(\text{LMS}=\arg\max_{\pi\in\mathfrak{S}_{B}}\frac{1}{f}\sum_{B\in\mathcal{B}} s_{B,\pi(B)}\), where \(\mathfrak{S}_{\mathcal{B}}\) is the set of permutations respecting \(\mathcal{B}\) (Definition 2). When \(\mathcal{B}:=\{\{1\},\dots,\{d_{z}\}\}\) and the score used is the absolute value of the correlation, LMS is simply the _mean correlation coefficient_ (MCC), which is widely used in the nonlinear ICA literature [30, 31, 33, 42]. Because our theory guarantees recovery of the latents only up to invertible and potentially nonlinear transformations, we use the Spearman correlation, which can capture nonlinear relationships unlike the Pearson correlation. We denote this score by \(\text{LMS}_{\text{Sper}}\) and will use it in the dataset **ScalarLatents**. For the **BlockLatents** dataset, we cannot use Spearman correlation (because \(\bm{z}_{B}\) are two dimensional). Instead, we take the score \(s_{B,B^{\prime}}\) to be the \(R^{2}\) score of a regression tree. We denote this score by \(\text{LMS}_{\text{tree}}\). There are subtleties to take care of when one wants to evaluate \(\text{LMS}_{\text{tree}}\) on a non-additive model due to the fact that the learned representation does not have a natural partition \(\mathcal{B}\). We must thus search over partitions. We discuss this and provide further details on the metrics in Appendix B.3.

### Results

**Additivity is important for disentanglement.** Table 1 shows that the additive decoder obtains a much higher \(\text{LMS}_{\text{Sper}}\ \&\ \text{LMS}_{\text{Tree}}\) than its non-additive counterpart on all three datasets considered, even if both decoders have very small reconstruction errors. This is corroborated by the visualizations of Figures 4 & 5. Appendix B.5 additionally shows object-specific reconstructions for the **BlockLatents** dataset. We emphasize that disentanglement is possible even when the latent factors are dependent (or causally related), as shown on the **ScalarLatents** dataset (L-shaped support implies dependencies) and on the **BlockLatents** dataset with dependencies (Table 1). Note that prior works have relied on interventions [3, 2, 8] or Cartesian-product supports [68, 62] to deal with dependencies.

\begin{table}
\begin{tabular}{l c c c c||c c c||c c} \hline \hline  & \multicolumn{3}{c||}{**ScalarLatents**} & \multicolumn{3}{c||}{**BlockLatents**} & \multicolumn{2}{c}{**BlockLatents**} \\  & & & & & \multicolumn{2}{c}{(independent \(\bm{z}\))} & \multicolumn{2}{c}{(dependent \(\bm{z}\))} \\ \hline Decoders & RMSE & \(\text{LMS}_{\text{Sper}}\) & \(\text{RMSE}^{\text{OOS}}\) & \(\text{LMS}_{\text{Sper}}^{\text{OOS}}\) & RMSE & \(\text{LMS}_{\text{Tree}}\) & RMSE & \(\text{LMS}_{\text{Tree}}\) \\ \hline Non-add. & 06 \(\pm\).002 & 70.6\(\pm\) 5.21 &.18\(\pm\).012 & 73.7\(\pm\) 4.64 &.02\(\pm\).001 & 53.9\(\pm\) 7.58 &.02\(\pm\).001 & 78.1\(\pm\).292 \\ Additive &.06\(\pm\).002 & **91.5\(\pm\).357** &.11\(\pm\).018 & **89.5\(\pm\).62** &.03\(\pm\).012 & **92.2\(\pm\).491** &.01\(\pm\).002 & **99.9\(\pm\).02** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Reporting reconstruction mean squared error (RMSE \(\downarrow\)) and the Latent Matching Score (LMS \(\uparrow\)) for the three datasets considered: **ScalarLatents** and **BlockLatents** with independent and dependent latents. Runs were repeated with 10 random initializations. \(\text{RMSE}^{\text{OOS}}\) and \(\text{LMS}_{\text{Sper}}^{\text{OOS}}\) are the same metric but evaluated out of support (see Appendix B.3 for details). While the standard error is high, the differences are still clear as can be seen in their box plot version in Appendix B.4.

**Additivity is important for Cartesian-product extrapolation.** Figure 4 illustrates that the additive decoder can generate images that are outside the training domain (both balls in upper half of the image) while its non-additive counterpart cannot. Furthermore, Table 1 also corroborates this showing that the "out-of-support" (OOS) reconstruction MSE and \(\text{LMS}_{\text{Spar}}\) (evaluated only on the samples never seen during training) are significantly better for the additive than for the non-additive decoder.

**Importance of connected support.** Theorem 2 required that the support of the latent factors, \(\mathcal{Z}^{\text{train}}\), was path-connected. Appendix B.6 shows experiments where this assumption is violated, which yields lower \(\text{LMS}_{\text{Spar}}\) for the additive decoder, thus highlighting the importance of this assumption.

## 5 Conclusion

We provided an in-depth identifiability analysis of _additive decoders_, which bears resemblance to standard decoders used in OCRL, and introduced a novel theoretical framework showing how this architecture can generate reasonable images never seen during training via "Cartesian-product extrapolation". We validated empirically both of these results and confirmed that additivity was indeed crucial. By studying rigorously how disentanglement can induce extrapolation, our work highlighted the necessity of restricting the decoder to extrapolate and set the stage for future works to explore disentanglement and extrapolation in other function classes such as masked decoders typically used in OCRL. We postulate that the type of identifiability analysis introduced in this work has the potential of expanding our understanding of creativity in generative models, ultimately resulting in representations that generalize better.

Figure 4: Figure (a) shows latent representation outputted by the encoder \(\hat{\bm{g}}(\bm{x})\) over the _training_ dataset, and the corresponding reconstructed images of the additive decoder with median \(\text{LMS}_{\text{Spar}}\) among runs performed on the **ScalarLatents** dataset. Figure (b) shows the same thing for the non-additive decoder. The color gradient corresponds to the value of one of the ground-truth factor, the red dots correspond to factors used to generate the images and the yellow dashed square highlights extrapolated images.

Figure 5: Latent responses for the case of independent latents in the **BlockLatent** dataset. In each plot, we report the latent factors predicted from multiple images where one ball moves along only one axis at a time. For the additive case, at most two latents change, as it should, while more than two latents change for the non-additive case. See Appendix B.5 for details.

## Acknowledgements

This research was partially supported by the Canada CIFAR AI Chair Program, by an IVADO excellence PhD scholarship and by Samsung Electronics Co., Ldt. The experiments were in part enabled by computational resources provided by Calcul Quebec (calculquebec.ca) and the Digital Research Alliance of Canada (allianeccan.ca). Simon Lacoste-Julien is a CIFAR Associate Fellow in the Learning in Machines & Brains program.

## References

* Ahuja et al. [2022] K. Ahuja, J. Hartford, and Y. Bengio. Properties from mechanisms: an equivariance perspective on identifiable representation learning. In _International Conference on Learning Representations_, 2022.
* Ahuja et al. [2022] K. Ahuja, J. Hartford, and Y. Bengio. Weakly supervised representation learning with sparse perturbations, 2022.
* Ahuja et al. [2023] K. Ahuja, D. Mahajan, Y. Wang, and Y. Bengio. Interventional causal representation learning. In _Proceedings of the 40th International Conference on Machine Learning_, 2023.
* Bengio et al. [2013] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. _IEEE transactions on pattern analysis and machine intelligence_, 2013.
* Besserve et al. [2021] M. Besserve, R. Sun, D. Janzing, and B. Scholkopf. A theory of independent mechanisms for extrapolation in generative models. In _Proceedings of the 35th AAAI Conference on Artificial Intelligence (AAAI)_, 2021.
* Bradbury et al. [2018] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.
* Brady et al. [2023] J. Brady, R. S. Zimmermann, Y. Sharma, B. Scholkopf, J. von Kugelgen, and W. Brendel. Provably learning object-centric representations. In _International Conference on Machine Learning_, 2023.
* Brehmer et al. [2022] J. Brehmer, P. De Haan, P. Lippe, and T. Cohen. Weakly supervised causal representation learning. In _Advances in Neural Information Processing Systems_, 2022.
* Buchholz et al. [2022] S. Buchholz, M. Besserve, and B. Scholkopf. Function classes for identifiable nonlinear independent component analysis. In _Advances in Neural Information Processing Systems_, 2022.
* Buchholz et al. [2023] S. Buchholz, G. Rajendran, E. Rosenfeld, B. Aragam, B. Scholkopf, and P. Ravikumar. Learning linear causal representations from interventions under general nonlinear mixing, 2023.
* Burgess et al. [2019] C. P. Burgess, L. Matthey, N. Watters, R. Kabra, I. Higgins, M. Botvinick, and A. Lerchner. Monet: Unsupervised scene decomposition and representation, 2019.
* Crawford and Pineau [2019] E. Crawford and J. Pineau. Spatially invariant unsupervised object detection with convolutional neural networks. _Proceedings of the AAAI Conference on Artificial Intelligence_, 2019.
* d'Avila Garcez and Lamb [2020] A. S. d'Avila Garcez and L. Lamb. Neurosymbolic AI: The 3rd wave. _ArXiv_, abs/2012.05876, 2020.
* Dittadi et al. [2022] A. Dittadi, S. S. Papa, M. De Vita, B. Scholkopf, O. Winther, and F. Locatello. Generalization and robustness implications in object-centric learning. In _Proceedings of the 39th International Conference on Machine Learning_, 2022.
* Donoho and Grimes [2003] D. Donoho and C. Grimes. Image manifolds which are isometric to euclidean space. _Journal of Mathematical Imaging and Vision_, 2003.
* Donoho and Grimes [2003] D. L. Donoho and C. Grimes. Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data. _Proceedings of the National Academy of Sciences_, 2003.

* [17] Y. Du and I. Mordatch. Implicit generation and modeling with energy based models. In _Advances in Neural Information Processing Systems_, 2019.
* [18] M. Engelcke, A. R. Kosiorek, O. P. Jones, and I. Posner. Genesis: Generative scene inference and sampling with object-centric latent representations. In _International Conference on Learning Representations_, 2020.
* [19] S. M. A. Eslami, N. Heess, T. Weber, Y. Tassa, D. Szepesvari, K. Kavukcuoglu, and G. E. Hinton. Attend, infer, repeat: Fast scene understanding with generative models. In _Advances in Neural Information Processing Systems_, 2016.
* [20] J. A. Fodor and Z. W. Pylyshyn. Connectionism and cognitive architecture: A critical analysis. _Cognition_, 1988.
* [21] A. Goyal and Y. Bengio. Inductive biases for deep learning of higher-level cognition. _Proc. R. Soc. A 478: 20210068_, 2022.
* [22] K. Greff, A. Rasmus, M. Berglund, T. Hao, H. Valpola, and J. Schmidhuber. Tagger: Deep unsupervised perceptual grouping. In _Advances in Neural Information Processing Systems_, 2016.
* [23] K. Greff, S. van Steenkiste, and J. Schmidhuber. Neural expectation maximization. In _Advances in Neural Information Processing Systems_, 2017.
* [24] K. Greff, R. L. Kaufman, R. Kabra, N. Watters, C. Burgess, D. Zoran, L. Matthey, M. Botvinick, and A. Lerchner. Multi-object representation learning with iterative variational inference. In _Proceedings of the 36th International Conference on Machine Learning_, 2019.
* [25] K. Greff, S. van Steenkiste, and J. Schmidhuber. On the binding problem in artificial neural networks. _ArXiv_, abs/2012.05208, 2020.
* [26] L. Gresele, J. V. Kugelgen, V. Stimper, B. Scholkopf, and M. Besserve. Independent mechanism analysis, a new concept? In _Advances in Neural Information Processing Systems_, 2021.
* [27] H. Halva, S. L. Corff, L. Lehericy, J. So, Y. Zhu, E. Gassiat, and A. Hyvarinen. Disentangling identifiable features from noisy data with structured nonlinear ICA. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021.
* [28] S. Harnad. The symbol grounding problem. _Physica D: Nonlinear Phenomena_, 1990.
* [29] D. Horan, E. Richardson, and Y. Weiss. When is unsupervised disentanglement possible? In _Advances in Neural Information Processing Systems_, 2021.
* [30] A. Hyvarinen and H. Morioka. Unsupervised feature extraction by time-contrastive learning and nonlinear ica. In _Advances in Neural Information Processing Systems_, 2016.
* [31] A. Hyvarinen and H. Morioka. Nonlinear ICA of Temporally Dependent Stationary Sources. In _Proceedings of the 20th International Conference on Artificial Intelligence and Statistics_, 2017.
* [32] A. Hyvarinen and P. Pajunen. Nonlinear independent component analysis: Existence and uniqueness results. _Neural Networks_, 1999.
* [33] A. Hyvarinen, H. Sasaki, and R. E. Turner. Nonlinear ica using auxiliary variables and generalized contrastive learning. In _AISTATS_. PMLR, 2019.
* [34] Y. Jiang and B. Aragam. Learning nonparametric latent causal graphs with unknown interventions, 2023.
* [35] J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. L. Zitnick, and R. B. Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016.

* [36] I. Khemakhem, D. Kingma, R. Monti, and A. Hyvarinen. Variational autoencoders and nonlinearica: A unifying framework. In _Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics_, 2020.
* [37] I. Khemakhem, R. Monti, D. Kingma, and A. Hyvarinen. Ice-beem: Identifiable conditional energy-based deep models based on nonlinear ica. In _Advances in Neural Information Processing Systems_, 2020.
* [38] D. A. Klindt, L. Schott, Y. Sharma, I. Ustyuzhaninov, W. Brendel, M. Bethge, and D. M. Paiton. Towards nonlinear disentanglement in natural data with temporal sparse coding. In _9th International Conference on Learning Representations_, 2021.
* [39] D. Krueger, E. Caballero, J.-H. Jacobsen, A. Zhang, J. Binas, D. Zhang, R. Le Priol, and A. Courville. Out-of-distribution generalization via risk extrapolation (rex). In _Proceedings of the 38th International Conference on Machine Learning_, 2021.
* [40] S. Lachapelle and S. Lacoste-Julien. Partial disentanglement via mechanism sparsity. In _UAI 2022 Workshop on Causal Representation Learning_, 2022.
* [41] S. Lachapelle, T. Deleu, D. Mahajan, I. Mitliagkas, Y. Bengio, S. Lacoste-Julien, and Q. Bertrand. Synergies between disentanglement and sparsity: a multi-task learning perspective, 2022.
* [42] S. Lachapelle, P. Rodriguez Lopez, Y. Sharma, K. E. Everett, R. Le Priol, A. Lacoste, and S. Lacoste-Julien. Disentanglement via mechanism sparsity regularization: A new principle for nonlinear ICA. In _First Conference on Causal Learning and Reasoning_, 2022.
* [43] B. M. Lake, T. D. Ullman, J. B. Tenenbaum, and S. J. Gershman. Building machines that learn and think like people. _Behavioral and Brain Sciences_, 2017.
* [44] F. Leeb, G. Lanzillotta, Y. Annadani, M. Besserve, S. Bauer, and B. Scholkopf. Structure by architecture: Disentangled representations without regularization, 2021.
* [45] Z. Lin, Y. Wu, S. V. Peri, W. Sun, G. Singh, F. Deng, J. Jiang, and S. Ahn. Space: Unsupervised object-oriented scene representation via spatial attention and decomposition. In _International Conference on Learning Representations_, 2020.
* [46] P. Lippe, S. Magliacane, S. Lowe, Y. M. Asano, T. Cohen, and E. Gavves. iCITRIS: Causal representation learning for instantaneous temporal effects. In _UAI 2022 Workshop on Causal Representation Learning_, 2022.
* [47] P. Lippe, S. Magliacane, S. Lowe, Y. M. Asano, T. Cohen, and E. Gavves. CITRIS: Causal identifiability from temporal intervened sequences, 2022.
* [48] F. Locatello, S. Bauer, M. Lucic, G. Raetsch, S. Gelly, B. Scholkopf, and O. Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. In _Proceedings of the 36th International Conference on Machine Learning_, 2019.
* [49] F. Locatello, B. Poole, G. Raetsch, B. Scholkopf, O. Bachem, and M. Tschannen. Weakly-supervised disentanglement without compromises. In _Proceedings of the 37th International Conference on Machine Learning_, 2020.
* [50] F. Locatello, M. Tschannen, S. Bauer, G. Ratsch, B. Scholkopf, and O. Bachem. Disentangling factors of variations using few labels. In _International Conference on Learning Representations_, 2020.
* [51] F. Locatello, D. Weissenborn, T. Unterthiner, A. Mahendran, G. Heigold, J. Uszkoreit, A. Dosovitskiy, and T. Kipf. Object-centric learning with slot attention. In _Advances in Neural Information Processing Systems_, 2020.
* [52] A. Mansouri, J. Hartford, K. Ahuja, and Y. Bengio. Object-centric causal representation learning. In _NeurIPS 2022 Workshop on Symmetry and Geometry in Neural Representations_, 2022.
* [53] G. F. Marcus. The algebraic mind : integrating connectionism and cognitive science, 2001.

* [54] G. E. Moran, D. Sridhar, Y. Wang, and D. Blei. Identifiable deep generative models via sparse decoding. _Transactions on Machine Learning Research_, 2022.
* [55] J. Munkres. _Analysis On Manifolds_. Basic Books, 1991.
* [56] J. R. Munkres. _Topology_. Prentice Hall, Inc., 2 edition, 2000.
* [57] J. Pearl. The seven tools of causal inference, with reflections on machine learning. _Commun. ACM_, 2019.
* [58] W. Peebles, J. Peebles, J.-Y. Zhu, A. A. Efros, and A. Torralba. The hessian penalty: A weak prior for unsupervised disentanglement. In _Proceedings of European Conference on Computer Vision (ECCV)_, 2020.
* [59] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.
* [60] P. Reizinger, L. Gresele, J. Brady, J. V. Kugelgen, D. Zietlow, B. Scholkopf, G. Martius, W. Brendel, and M. Besserve. Embrace the gap: VAEs perform independent mechanism analysis. In _Advances in Neural Information Processing Systems_, 2022.
* [61] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.
* [62] K. Roth, M. Ibrahim, Z. Akata, P. Vincent, and D. Bouchacourt. Disentanglement of correlated factors via hausdorff factorized support. In _The Eleventh International Conference on Learning Representations_, 2023.
* Advances in Machine Learning and Deep Neural Networks_, 2021.
* [64] C. Squires, A. Seigal, S. Bhate, and C. Uhler. Linear causal disentanglement via interventions. In _Proceedings of the 40th International Conference on Machine Learning_, 2023.
* [65] A. Taleb and C. Jutten. Source separation in post-nonlinear mixtures. _IEEE Transactions on Signal Processing_, 1999.
* [66] J. Von Kugelgen, Y. Sharma, L. Gresele, W. Brendel, B. Scholkopf, M. Besserve, and F. Locatello. Self-supervised learning with data augmentations provably isolates content from style. In _Thirty-Fifth Conference on Neural Information Processing Systems_, 2021.
* [67] J. von Kugelgen, M. Besserve, W. Liang, L. Gresele, A. Kekic, E. Bareinboim, D. M. Blei, and B. Scholkopf. Nonparametric identifiability of causal representations from unknown interventions, 2023.
* [68] Y. Wang and M. I. Jordan. Desiderata for representation learning: A causal perspective, 2022.
* [69] Z. Wang, L. Gui, J. Negrea, and V. Veitch. Concept algebra for text-controlled vision models, 2023.
* [70] T. W. Webb, Z. Dulbecco, S. M. Frankland, A. A. Petrov, R. C. O'Reilly, and J. D. Cohen. Learning representations that support extrapolation. In _Proceedings of the 37th International Conference on Machine Learning_, 2020.
* [71] Q. Xi and B. Bloem-Reddy. Indeterminacy in generative models: Characterization and strong identifiability. In _Proceedings of The 26th International Conference on Artificial Intelligence and Statistics_, 2023.
* [72] J. Zhang, C. Squires, K. Greenewald, A. Srivastava, K. Shanmugam, and C. Uhler. Identifiability guarantees for causal disentanglement from soft interventions, 2023.
* [73] Y. Zheng, I. Ng, and K. Zhang. On the identifiability of nonlinear ICA: Sparsity and beyond. In _Advances in Neural Information Processing Systems_, 2022.

## Appendix

### Table of Contents

* 1 Identifiability and Extrapolation Analysis
	* 1.1 Useful definitions and lemmas
	* 1.2 Relationship between additive decoders and the diagonal Hessian penalty
	* 1.3 Additive decoders form a superset of compositional decoders [7]
	* 1.4 Examples of local but non-global disentanglement
	* 1.5 Proof of Theorem 1
	* 1.6 Sufficient nonlinearity v.s. sufficient variability in nonlinear ICA with auxiliary variables
	* 1.7 Examples of sufficiently nonlinear additive decoders
	* 1.8 Proof of Theorem 2
	* 1.9 Injectivity of object-specific decoders v.s. injectivity of their sum
* 1.10 Proof of Corollary 3
* 1.11 Will all extrapolated images make sense?
* 1.12 Additive decoders cannot model occlusion
* 2 Experiments
	* 2.1 Training Details
	* 2.2 Datasets Details
	* 2.3 Evaluation Metrics
	* 2.4 Boxplots for main experiments (Table 1)
	* 2.5 Additional Results: BlockLatents Dataset
	* 2.6 Disconnected Support Experiments
	* 2.7 Additional Results: ScalarLatents Dataset

## Appendix A Identifiability and Extrapolation Analysis

### Useful definitions and lemmas

We start by recalling some notions of general topology that are going to be used later on. For a proper introduction to these concepts, see for example Munkres [56].

**Definition 6** (Regularly closed sets).: _A set \(\mathcal{Z}\subseteq\mathbb{R}^{d_{z}}\) is regularly closed if \(\mathcal{Z}=\overline{\mathcal{Z}^{\circ}}\), i.e. if it is equal to the closure of its interior (in the standard topology of \(\mathbb{R}^{n}\))._

**Definition 7** (Connected sets).: _A set \(\mathcal{Z}\subseteq\mathbb{R}^{d_{z}}\) is connected if it cannot be written as a union of non-empty and disjoint open sets (in the subspace topology)._

**Definition 8** (Path-connected sets).: _A set \(\mathcal{Z}\subseteq\mathbb{R}^{d_{z}}\) is path-connected if for all pair of points \(\boldsymbol{z}^{0},\boldsymbol{z}^{1}\in\mathcal{Z}\), there exists a continuous map \(\boldsymbol{\phi}:[0,1]\to\mathcal{Z}\) such that \(\boldsymbol{\phi}(0)=\boldsymbol{z}^{0}\) and \(\boldsymbol{\phi}(1)=\boldsymbol{z}^{1}\). Such a map is called a path between \(\boldsymbol{z}^{0}\) and \(\boldsymbol{z}^{1}\)._

\begin{table}
\begin{tabular}{c l} \multicolumn{2}{c}{Calligraphic \& indexing conventions} \\ \([n]\) & \(:=\) & \(\{1,2,\ldots,n\}\) \\ \(x\) & Scalar (random or not, depending on context) \\ \(\boldsymbol{x}\) & Vector (random or not, depending on context) \\ \(\boldsymbol{X}\) & Matrix \\ \(\mathcal{X}\) & Set/Support \\ \(f\) & Scalar-valued function \\ \(\boldsymbol{f}\) & Vector-valued function \\ \(f\big{|}_{A}\) & Restriction of \(f\) to the set \(A\) \\ \(Df\), \(D\boldsymbol{f}\) & Jacobian of \(f\) and \(\boldsymbol{f}\) \\ \(D^{2}f\) & Hessian of \(f\) \\ \(B\subseteq[n]\) & Subset of indices \\ \(|B|\) & Cardinality of the set \(B\) \\ \(\boldsymbol{x}_{B}\) & Vector formed with the \(i\)th coordinates of \(\boldsymbol{x}\), for all \(i\in B\) \\ \(\boldsymbol{X}_{B,B^{\prime}}\) & Matrix formed with the entries \((i,j)\in B\times B^{\prime}\) of \(\boldsymbol{X}\). \\ \(\text{Given }\mathcal{X}\subseteq\mathbb{R}^{n},\mathcal{X}_{B}\) & \(:=\) & \(\{\boldsymbol{x}_{B}\mid\boldsymbol{x}\in\mathcal{X}\}\) (projection of \(\mathcal{X}\)) \\ \multicolumn{2}{c}{Recurrent notation} \\ \(\boldsymbol{x}\in\mathbb{R}^{d_{z}}\) & Observation \\ \(\boldsymbol{z}\in\mathbb{R}^{d_{z}}\) & Vector of latent factors of variations \\ \(\mathcal{Z}\subseteq\mathbb{R}^{d_{z}}\) & Support of \(\boldsymbol{z}\) \\ \(\boldsymbol{f}\) & Ground-truth decoder function \\ \(\hat{\boldsymbol{f}}\) & Learned decoder function \\ \(\mathcal{B}\) & A partition of \([d_{z}]\) (assumed contiguous w.l.o.g.) \\ \(B\in\mathcal{B}\) & A block of the partition \(\mathcal{B}\) \\ \(B(i)\in\mathcal{B}\) & The unique block of \(\mathcal{B}\) that contains \(i\) \\ \(\pi:[d_{z}]\to[d_{z}]\) & A permutation \\ \(S_{\mathcal{B}}\) & \(:=\) & \(\bigcup_{B\in\mathcal{B}}B^{2}\) \\ \(S_{\mathcal{B}}^{c}\) & \(:=\) & \([d_{z}]^{2}\setminus S_{\mathcal{B}}\) \\ \(\mathbb{R}^{d_{z}\times d_{z}}_{S_{\mathcal{B}}}\) & \(:=\) & \(\{\boldsymbol{M}\in\mathbb{R}^{d_{z}\times d_{z}}\mid(i,j)\not\in S_{\mathcal{ B}}\implies\boldsymbol{M}_{i,j}=0\}\) \\ \multicolumn{2}{c}{General topology} \\ \(\overline{\mathcal{X}}\) & Closure of the subset \(\mathcal{X}\subseteq\mathbb{R}^{n}\) in the standard topology of \(\mathbb{R}^{n}\) \\ \(\mathcal{X}^{\circ}\) & Interior of the subset \(\mathcal{X}\subseteq\mathbb{R}^{n}\) in the standard topology of \(\mathbb{R}^{n}\) \\ \multicolumn{2}{c}{} \\ \end{tabular}
\end{table}
Table 2: Table of Notation.

**Definition 9** (Homeomorphism).: _Let \(A\) and \(B\) be subsets of \(\mathbb{R}^{n}\) equipped with the subspace topology. A function \(\bm{f}:A\to B\) is an homeomorphism if it is bijective, continuous and its inverse is continuous._

The following technical lemma will be useful in the proof of Theorem 1. For it, we will need additional notation: Let \(S\subseteq A\subseteq\mathbb{R}^{n}\). We already saw that \(\overline{S}\) refers to the closure \(S\) in the \(\mathbb{R}^{n}\) topology. We will denote by \(\mathrm{cl}_{A}(S)\) the closure of \(S\) in the subspace topology of \(A\) induced by \(\mathbb{R}^{n}\), which is not necessarily the same as \(\overline{S}\). In fact, both can be related via \(\mathrm{cl}_{A}=\overline{S}\cap A\) (see Munkres [56, Theorem 17.4, p.95]).

**Lemma 4**.: _Let \(A,B\subseteq\mathbb{R}^{n}\) and suppose there exists an homeomorphism \(\bm{f}:A\to B\). If \(A\) is regularly closed in \(\mathbb{R}^{n}\), we have that \(B\subseteq\overline{B^{\circ}}\)._

Proof.: Note that \(\bm{f}\big{|}_{A^{\circ}}\) is a continuous injective function from the open set \(A^{\circ}\) to \(\bm{f}(A^{\circ})\). By the "invariance of domain" theorem [56, p.381], we have that \(\bm{f}(A^{\circ})\) must be open in \(\mathbb{R}^{n}\). Of course, we have that \(\bm{f}(A^{\circ})\subseteq B\), and thus \(\bm{f}(A^{\circ})\subseteq B^{\circ}\) (the interior of \(B\) is the largest open set contained in \(B\)). Analogously, \(\bm{f}^{-1}\big{|}_{B^{\circ}}\) is a continuous injective function from the open set \(B^{\circ}\) to \(\bm{f}^{-1}(B^{\circ})\). Again, by "invariance of domain", \(\bm{f}^{-1}(B^{\circ})\) must be open in \(\mathbb{R}^{n}\) and thus \(\bm{f}^{-1}(B^{\circ})\subseteq A^{\circ}\). We can conclude that \(\bm{f}(A^{\circ})=B^{\circ}\).

We can conclude as follow:

\[B=\bm{f}(A)=\bm{f}(\overline{A^{\circ}})=\bm{f}(\overline{A^{\circ}}\cap A)= \bm{f}(\mathrm{cl}_{A}(A^{\circ}))\subseteq\mathrm{cl}_{B}(\bm{f}(A^{\circ})) =\mathrm{cl}_{B}(B^{\circ})=\overline{B^{\circ}}\cap B\subseteq\overline{B^{ \circ}}\,,\]

where the first inclusion holds by continuity of \(\bm{f}\)[56, Thm.18.1 p.104]. 

This lemma is taken from [42].

**Lemma 5** (Sparsity pattern of an invertible matrix contains a permutation).: _Let \(\bm{L}\in\mathbb{R}^{m\times m}\) be an invertible matrix. Then, there exists a permutation \(\sigma\) such that \(\bm{L}_{i,\sigma(i)}\neq 0\) for all \(i\)._

Proof.: Since the matrix \(\bm{L}\) is invertible, its determinant is non-zero, i.e.

\[\det(\bm{L}):=\sum_{\pi\in\mathfrak{S}_{m}}\text{sign}(\pi)\prod_{i=1}^{m}\bm {L}_{i,\pi(i)}\neq 0\,,\] (12)

where \(\mathfrak{S}_{m}\) is the set of \(m\)-permutations. This equation implies that at least one term of the sum is non-zero, meaning there exists \(\pi\in\mathfrak{S}_{m}\) such that for all \(i\in[m]\), \(\bm{L}_{i,\pi(i)}\neq 0\). 

**Definition 10** (Aligned subspaces of \(\mathbb{R}^{m\times n}\)).: _Given a subset \(S\subseteq\{1,...,m\}\times\{1,...,n\}\), we define_

\[\mathbb{R}_{S}^{m\times n}:=\{\bm{M}\in\mathbb{R}^{m\times n}\mid(i,j)\not \in S\implies\bm{M}_{i,j}=0\}\,.\] (13)

**Definition 11** (Useful sets).: _Given a partition \(\mathcal{B}\) of \([d]\), we define_

\[S_{\mathcal{B}}:=\bigcup_{B\in\mathcal{B}}B^{2}\quad S_{\mathcal{B}}^{c}:=\{1, \ldots,d_{z}\}^{2}\setminus S_{\mathcal{B}}\] (14)

**Definition 12** (\(C^{k}\)-diffeomorphism).: _Let \(A\subseteq\mathbb{R}^{n}\) and \(B\subseteq\mathbb{R}^{m}\). A map \(\bm{f}:A\to B\) is said to be a \(C^{k}\)-diffeomorphism if it is bijective, \(C^{2}\) and has a \(C^{2}\) inverse._

**Remark 2**.: _Differentiability is typically defined for functions that have an open domain in \(\mathbb{R}^{n}\). However, in the definition above, the set \(A\) might not be open in \(\mathbb{R}^{n}\) and \(B\) might not be open in \(\mathbb{R}^{m}\). In the case of an arbitrary domain \(A\), it is customary to say that a function \(\bm{f}:A\subseteq\mathbb{R}^{n}\to\mathbb{R}^{m}\) is \(C^{k}\) if there exists a \(C^{k}\) function \(\bm{g}\) defined on an open set \(U\subseteq\mathbb{R}^{n}\) that contains \(A\) such that \(\bm{g}\big{|}_{A}=\bm{f}\) (i.e. \(\bm{g}\) extends \(\bm{f}\)). With this definition, we have that a composition of \(C^{k}\) functions is \(C^{k}\), as usual. See for example p.199 of Munkres [55]._

The following lemma allows us to unambiguously define the \(k\) first derivatives of a \(C^{k}\) function \(\bm{f}:A\to\mathbb{R}^{m}\) on the set \(\overline{A^{\circ}}\).

**Lemma 6**.: _Let \(A\subseteq\mathbb{R}^{n}\) and \(\bm{f}:A\to\mathbb{R}^{m}\) be a \(C^{k}\) function. Then, its \(k\) first derivatives is uniquely defined on \(\overline{A^{\circ}}\) in the sense that they do not depend on the specific choice of \(C^{k}\) extension._Proof.: Let \(\bm{g}:U\to\mathbb{R}^{n}\) and \(\bm{h}:V\to\mathbb{R}^{n}\) be two \(C^{k}\) extensions of \(\bm{f}\) to \(U\subseteq\mathbb{R}^{n}\) and \(V\subseteq\mathbb{R}^{n}\) both open in \(\mathbb{R}^{n}\). By definition,

\[\bm{g}(\bm{x})=\bm{f}(\bm{x})=\bm{h}(\bm{x}),\;\forall\bm{x}\in A\,.\] (15)

The usual derivative is uniquely defined on the interior of the domain, so that

\[D\bm{g}(\bm{x})=D\bm{f}(\bm{x})=D\bm{h}(\bm{x}),\;\forall\bm{x}\in A^{\circ}\,.\] (16)

Consider a point \(\bm{x}_{0}\in\overline{A^{\circ}}\). By definition of closure, there exists a sequence \(\{\bm{x}_{k}\}_{k=1}^{\infty}\subseteq A^{\circ}\) s.t. \(\lim_{k\to\infty}\bm{x}_{k}=\bm{x}_{0}\). We thus have that

\[\lim_{k\to\infty}D\bm{g}(\bm{x}_{k}) =\lim_{k\to\infty}D\bm{h}(\bm{x}_{k})\] (17) \[D\bm{g}(\bm{x}_{0}) =D\bm{h}(\bm{x}_{0})\,,\] (18)

where we used the fact that the derivatives of \(\bm{g}\) and \(\bm{h}\) are continuous to go to the second line. Thus, all the \(C^{k}\) extensions of \(\bm{f}\) must have equal derivatives on \(\overline{A^{\circ}}\). This means we can unambiguously define the derivative of \(\bm{f}\) everywhere on \(\overline{A^{\circ}}\) to be equal to the derivative of one of its \(C^{k}\) extensions.

Since \(\bm{f}\) is \(C^{k}\), its derivative \(D\bm{f}\) is \(C^{k-1}\), we can thus apply the same argument to get that the second derivative of \(\bm{f}\) is uniquely defined on \(\overline{A^{\circ}}^{\circ}\). It can be shown that \(\overline{\overline{A^{\circ}}^{\circ}}=\overline{A^{\circ}}\). One can thus apply the same argument recursively to show that the first \(k\) derivatives of \(\bm{f}\) are uniquely defined on \(\overline{A^{\circ}}\). 

**Definition 13** (\(C^{k}\)-diffeomorphism onto its image).: _Let \(A\subseteq\mathbb{R}^{n}\). A map \(\bm{f}:A\to\mathbb{R}^{m}\) is said to be a \(C^{k}\)-diffeomorphism onto its image if the restriction \(\bm{f}\) to its image \(\tilde{\bm{f}}:A\to\bm{f}(A)\) is a \(C^{k}\)-diffeomorphism._

**Remark 3**.: _If \(S\subseteq A\subseteq\mathbb{R}^{n}\) and \(\bm{f}:A\to\mathbb{R}^{m}\) is a \(C^{k}\)-diffeomorphism on its image, then the restriction of \(\bm{f}\) to \(S\), i.e. \(\bm{f}\big{|}_{S}\), is also a \(C^{k}\) diffeomorphism on its image. That is because \(\bm{f}\big{|}_{S}\) is clearly bijective, is \(C^{k}\) (simply take the \(C^{k}\) extension of \(\bm{f}\)) and so is its inverse (simply take the \(C^{k}\) extension of \(\bm{f}^{-1}\))._

### Relationship between additive decoders and the diagonal Hessian penalty

**Proposition 7** (Equivalence between additivity and diagonal Hessian).: _Let \(\bm{f}:\mathbb{R}^{d_{x}}\to\mathbb{R}^{d_{x}}\) be a \(C^{2}\) function. Then,_

\[\begin{array}{ll}\forall\bm{z}\in\mathbb{R}^{d_{z}},\;\bm{f}(\bm{z})=\sum_{B \in\mathcal{B}}\bm{f}^{(B)}(\bm{z}_{B})&\iff\forall k\in[d_{x}],\;\bm{z}\in \mathbb{R}^{d_{z}},\;\;D^{2}\bm{f}_{k}(\bm{z})\;\mbox{is}\\ \mbox{where }\bm{f}^{(B)}:\mathbb{R}^{|B|}\to\mathbb{R}^{d_{x}}\mbox{ is }C^{2}.&\mbox{block diagonal with blocks in }\mathcal{B}.\end{array}\] (19)

Proof.: We start by showing the "\(\implies\)" direction. Let \(B\) and \(B^{\prime}\) be two distinct blocks of \(\mathcal{B}\). Let \(i\in B\) and \(i^{\prime}\in B^{\prime}\). We can compute the derivative of \(\bm{f}_{k}\) w.r.t. \(\bm{z}_{i}\):

\[D_{i}\bm{f}_{k}(\bm{z})=\sum_{B\in\mathcal{B}}D_{i}\bm{f}_{k}^{(B)}(\bm{z}_{B}) =D_{i}\bm{f}_{k}^{(B)}(z_{B})\,,\] (20)

where the last equality holds because \(i\in B\) and not in any other block \(\bar{B}\). Furthermore,

\[D_{i,i^{\prime}}^{2}\bm{f}_{k}(\bm{z})=D_{i,i^{\prime}}^{2}\bm{f}_{k}^{(B)}( \bm{z}_{B})=0\,,\] (21)

where the last equality holds because \(i^{\prime}\not\in B\). This shows that \(D^{2}\bm{f}_{k}(\bm{z})\) is block diagonal.

We now show the "\(\iff\) " direction. Fix \(k\in[d_{x}]\), \(B\in\mathcal{B}\). We know that \(D_{B,B^{c}}^{2}\bm{f}_{k}(\bm{z})=0\) for all \(\bm{z}\in\mathbb{R}^{d_{z}}\). Fix \(\bm{z}\in\mathbb{R}^{d_{z}}\). Consider a continuously differentiable path \(\bm{\phi}:[0,1]\to\mathbb{R}^{|B^{c}|}\) such that \(\bm{\phi}(0)=0\) and \(\bm{\phi}(1)=\bm{z}_{B^{c}}\). As \(D_{B,B^{c}}^{2}\bm{f}_{k}(\bm{z})\) is a continuous function of \(\bm{z}\), we can use the fundamental theorem of calculus for line integrals to get that

\[D_{B}\bm{f}_{k}(\bm{z}_{B},\bm{z}_{B^{c}})-D_{B}\bm{f}_{k}(\bm{z}_{B},0)=\int_{0 }^{1}\underbrace{D_{B,B^{c}}^{2}\bm{f}_{k}(\bm{z}_{B},\bm{\phi}(t))}_{=0} \bm{\phi}^{\prime}(t)dt=0\,,\] (22)

(where \(D_{B,B^{c}}^{2}\bm{f}_{k}(\bm{z}_{B},\bm{\phi}(t))\bm{\phi}^{\prime}(t)\) denotes a matrix-vector product) which implies that

\[D_{B}\bm{f}_{k}(\bm{z})=D_{B}\bm{f}_{k}(\bm{z}_{B},0)\,.\] (23)And the above equality holds for all \(B\in\mathcal{B}\) and all \(\bm{z}\in\mathbb{R}^{d_{z}}\).

Choose an arbitrary \(\bm{z}\in\mathbb{R}^{d_{z}}\). Consider a continously differentiable path \(\bm{\psi}:[0,1]\to\mathbb{R}^{d_{z}}\) such that \(\bm{\psi}(0)=0\) and \(\bm{\psi}(1)=\bm{z}\). By applying the fundamental theorem of calculus for line integrals once more, we have that

\[\bm{f}_{k}(\bm{z})-\bm{f}_{k}(0) =\int_{0}^{1}D\bm{f}_{k}(\bm{\psi}(t))\bm{\psi}^{\prime}(t)dt\] (24) \[=\int_{0}^{1}\sum_{B\in\mathcal{B}}D_{B}\bm{f}_{k}(\bm{\psi}(t)) \bm{\psi}^{\prime}_{B}(t)dt\] (25) \[=\sum_{B\in\mathcal{B}}\int_{0}^{1}D_{B}\bm{f}_{k}(\bm{\psi}(t)) \bm{\psi}^{\prime}_{B}(t)dt\] (26) \[=\sum_{B\in\mathcal{B}}\int_{0}^{1}D_{B}\bm{f}_{k}(\bm{\psi}_{B}( t),0)\bm{\psi}^{\prime}_{B}(t)dt\,,\] (27)

where the last equality holds by (23). We can further apply the fundamental theorem of calculus for line integrals to each term \(\int_{0}^{1}D_{B}\bm{f}_{k}(\bm{\psi}_{B}(t),0)\bm{\psi}^{\prime}_{B}(t)dt\) to get

\[\bm{f}_{k}(\bm{z})-\bm{f}_{k}(0) =\sum_{B\in\mathcal{B}}(\bm{f}_{k}(\bm{z}_{B},0)-\bm{f}_{k}(0,0))\] (28) \[\implies\bm{f}_{k}(\bm{z}) =\bm{f}_{k}(0)+\sum_{B\in\mathcal{B}}(\bm{f}_{k}(\bm{z}_{B},0)- \bm{f}_{k}(0))\] (29) \[=\sum_{B\in\mathcal{B}}\underbrace{\left(\bm{f}_{k}(\bm{z}_{B},0) -\frac{|\mathcal{B}|-1}{|\mathcal{B}|}\bm{f}_{k}(0)\right)}_{\bm{f}_{k}^{(B)}( \bm{z}_{B}):=}\,.\] (30)

and since \(\bm{z}\) was arbitrary, the above holds for all \(\bm{z}\in\mathbb{R}^{d_{z}}\). Note that the functions \(\bm{f}_{k}^{(B)}(\bm{z}_{B})\) must be \(C^{2}\) because \(\bm{f}_{k}\) is \(C^{2}\). This concludes the proof. 

### Additive decoders form a superset of compositional decoders [7]

Compositional decoders were introduced by Brady et al. [7] as a suitable class of functions to perform object-centric representation learning with identifiability guarantees. They are also interested in block-disentanglement, but, contrarily to our work, they assume that the latent vector \(\bm{z}\) is fully supported, i.e. \(\mathcal{Z}=\mathbb{R}^{d_{z}}\). We now rewrite the definition of compositional decoders in the notation used in this work:

**Definition 14** (Compositional decoders, adapted from [7]).: _Given a partition \(\mathcal{B}\), a differentiable decoder \(\bm{f}:\mathbb{R}^{d_{z}}\to\mathbb{R}^{d_{z}}\) is said to be compositional w.r.t. \(\mathcal{B}\) whenever the Jacobian \(D\bm{f}(\bm{z})\) is such that for all \(i\in[d_{z}],B\in\mathcal{B},\bm{z}\in\mathbb{R}^{d_{z}}\), we have_

\[D_{B}\bm{f}_{i}(\bm{z})\neq\bm{0}\implies D_{B^{c}}\bm{f}_{i}(\bm{z})=\bm{0}\,,\]

_where \(B^{c}\) is the complement of \(B\in\mathcal{B}\)._

In other words, each line of the Jacobian can have nonzero values only in one block \(B\in\mathcal{B}\). Note that this nonzero block can change with different values of \(\bm{z}\).

The next result shows that additive decoders form a superset of \(C^{2}\) compositional decoders (Brady et al. [7] assumed only \(C^{1}\)). Note that additive decoders are _strictly_ more expressive than \(C^{2}\) compositional decoders because some additive functions are not compositional, like Example 3 for instance.

**Proposition 8** (Compositional implies additive).: _Given a partition \(\mathcal{B}\), if \(\bm{f}:\mathbb{R}^{d_{z}}\to\mathbb{R}^{d_{x}}\) is compositional (Definition 14) and \(C^{2}\), then it is also additive (Definition 1)._

Proof.: Choose any \(i\in[d_{x}]\). Our strategy will be to show that \(D^{2}\bm{f}_{i}\) is block diagonal everywhere on \(\mathbb{R}^{d_{z}}\) and use Proposition 7 to conclude that \(\bm{f}_{i}\) is additive.

Choose an arbitrary \(\bm{z}_{0}\in\mathbb{R}^{d_{z}}\). By compositionality, there exists a block \(B\in\mathcal{B}\) such that \(D_{B^{c}}\bm{f}_{i}(\bm{z}_{0})=\bm{0}\). We consider two cases separately:

**Case 1** Assume \(D_{B}\bm{f}_{i}(\bm{z}_{0})\neq\bm{0}\). By continuity of \(D_{B}\bm{f}_{i}\), there exists an open neighborhood of \(\bm{z}_{0}\), \(U\), s.t. for all \(\bm{z}\in U,\;D_{B}\bm{f}_{i}(\bm{z})\neq\bm{0}\). By compositionality, this means that, for all \(\bm{z}\in U,\;D_{B^{c}}\bm{f}_{i}(\bm{z})=\bm{0}\). When a function is zero on an open set, its derivative must also be zero, hence \(DD_{B^{c}}\bm{f}_{i}(\bm{z}_{0})=\bm{0}\). Because \(\bm{f}\) is \(C^{2}\), the Hessian is symmetric so that we also have \(D_{B^{c}}D\bm{f}_{i}(\bm{z}_{0})=\bm{0}\). We can thus conclude that the Hessian \(D^{2}\bm{f}_{i}(\bm{z}_{0})\) is such that all entries are zero except possibly for \(D^{2}\bm{f}_{i}(\bm{z}_{0})_{B,B}\). Hence, \(D^{2}\bm{f}_{i}(\bm{z}_{0})\) is block diagonal with blocks in \(\mathcal{B}\).

**Case 2:** Assume \(D_{B}\bm{f}_{i}(\bm{z}_{0})=\bm{0}\). This means the whole row of the Jacobian is zero, i.e. \(D\bm{f}_{i}(\bm{z}_{0})=\bm{0}\). By continuity of \(D\bm{f}_{i}\), we have that the set \(V:=(D\bm{f}_{i})^{-1}(\{0\})\) is closed. Thus this set decomposes as \(V=V^{\circ}\cup\partial V\) where \(V^{\circ}\) and \(\partial V\) are the interior and boundary of \(V\), respectively.

**Case 2.1:** Suppose \(\bm{z}_{0}\in V^{\circ}\). Then we can take a derivative so that \(D^{2}\bm{f}_{i}(\bm{z}_{0})=\bm{0}\), which of course means that \(D^{2}\bm{f}_{i}(\bm{z}_{0})\) is diagonal.

**Case 2.2:** Suppose \(\bm{z}_{0}\in\partial V\). By the definition of boundary, for all open set \(U\) containing \(\bm{z}_{0}\), \(U\) intersects with the complement of \(V\), i.e. \((D\bm{f}_{i})^{-1}(\mathbb{R}^{d_{z}}\setminus\{0\})\). This means we can construct a sequence \(\{\bm{z}_{k}\}_{k=1}^{\infty}\subseteq V^{c}\) which converges to \(\bm{z}_{0}\). By **Case 1**, we have that for all \(k\geq 1\), \(D^{2}\bm{f}_{i}(\bm{z}_{k})\) is block diagonal. This means that \(\lim_{k\to\infty}D^{2}\bm{f}_{i}(\bm{z}_{k})\) is block diagonal. Moreover, by continuity of \(D^{2}\bm{f}_{i}\), we have that \(\lim_{k\to\infty}D^{2}\bm{f}_{i}(\bm{z}_{k})=D^{2}\bm{f}_{i}(\bm{z}_{0})\). Hence \(D^{2}\bm{f}_{i}(\bm{z}_{0})\) is block diagonal.

We showed that for all \(\bm{z}_{0}\in\mathbb{R}^{d_{z}}\), \(D^{2}\bm{f}_{i}(\bm{z}_{0})\) is block diagonal. Hence, \(\bm{f}\) is additive by Proposition 7. 

### Examples of local but non-global disentanglement

In this section, we provide examples of mapping \(\bm{v}:\hat{\mathcal{Z}}^{\text{train}}\to\mathcal{Z}^{\text{train}}\) that satisfy the _local_ disentanglement property of Definition 4, but not the _global_ disentanglement property of Definition 3. Note that these notions are defined for pairs of decoders \(\bm{f}\) and \(\hat{\bm{f}}\), but here we construct directly the function \(\bm{v}\) which is usually defined as \(\bm{f}^{-1}\circ\hat{\bm{f}}\). However, given \(\bm{v}\) we can always define \(\bm{f}\) and \(\hat{\bm{f}}\) to be such that \(\bm{f}^{-1}\circ\hat{\bm{f}}=\bm{v}\): Simply take \(\bm{f}(\bm{z}):=[\bm{z}_{1},\ldots,\bm{z}_{d_{z}},0,\ldots,0]^{\top}\in\mathbb{R }^{d_{z}}\) and \(\hat{\bm{f}}:=\bm{f}\circ\bm{v}\). This construction however yields a decoder \(\bm{f}\) that is not sufficiently nonlinear (Assumption 2). Clearly the mappings \(\bm{v}\) that we provide in the following examples cannot be written as compositions of decoders \(\bm{f}^{-1}\circ\hat{\bm{f}}\) where \(\bm{f}\) and \(\hat{\bm{f}}\) satisfy all assumptions of Theorem 2, as this would contradict the theorem. In Examples 5 & 6, the path-connected assumption of Theorem 2 is violated. In Example 7, it is less obvious to see which assumptions would be violated.

**Example 5** (Disconnected support with changing permutation).: _Let \(\bm{v}:\hat{\mathcal{Z}}\to\mathbb{R}^{2}\) s.t. \(\hat{\mathcal{Z}}=\hat{\mathcal{Z}}^{(1)}\cup\hat{\mathcal{Z}}^{(2)}\subseteq \mathbb{R}^{2}\) where \(\hat{\mathcal{Z}}^{(1)}=\{\bm{z}\in\mathbb{R}^{2}\mid\bm{z}_{1}\leq 0\) and \(\bm{z}_{2}\leq 0\}\) and \(\hat{\mathcal{Z}}^{(2)}=\{\bm{z}\in\mathbb{R}^{2}\mid\bm{z}_{1}\geq 1\) and \(\bm{z}_{2}\geq 1\}\). Assume_

\[\bm{v}(\bm{z}):=\begin{cases}(\bm{z}_{1},\bm{z}_{2}),&\text{if }\bm{z}\in\hat{ \mathcal{Z}}^{(1)}\\ (\bm{z}_{2},\bm{z}_{1}),&\text{if }\bm{z}\in\hat{\mathcal{Z}}^{(2)}\end{cases}.\] (31)

_Step 1: \(\bm{v}\) is a diffeomorphism. Note that \(\bm{v}\) is its own inverse. Indeed,_

\[\bm{v}(\bm{v}(\bm{z}))=\begin{cases}\bm{v}(\bm{z}_{1},\bm{z}_{2})=(\bm{z}_{1}, \bm{z}_{2}),&\text{if }\bm{z}\in\hat{\mathcal{Z}}^{(1)}\\ \bm{v}(\bm{z}_{2},\bm{z}_{1})=(\bm{z}_{1},\bm{z}_{2}),&\text{if }\bm{z}\in\hat{\mathcal{Z}}^{(2)}\end{cases}.\]

_Thus, \(\bm{v}\) is bijective on its image. Clearly, \(\bm{v}\) is \(C^{2}\), thus \(\bm{v}^{-1}=\bm{v}\) is also \(C^{2}\). Hence, \(\bm{v}\) is a \(C^{2}\)-diffeomorphism._

_Step 2: \(\bm{v}\) is locally disentangled. The Jacobian of \(\bm{v}\) is given by_

\[D\bm{v}(\bm{z}):=\begin{cases}\begin{bmatrix}1&0\\ 0&1\end{bmatrix},&\text{if }\bm{z}\in\hat{\mathcal{Z}}^{(1)}\\ \begin{bmatrix}0&1\\ 1&0\end{bmatrix},&\text{if }\bm{z}\in\hat{\mathcal{Z}}^{(2)}\end{cases},\] (32)_which is everywhere a permutation matrix, hence \(\bm{v}\) is locally disentangled._

_Step 3: \(\bm{v}\) is not globally disentangled. That is because \(\bm{v}_{1}(\bm{z}_{1},\bm{z}_{2})\) depends on both \(\bm{z}_{1}\) and \(\bm{z}_{2}\). Indeed, if \(\bm{z}_{2}=0\), we have that \(\bm{v}_{1}(-1,0)=-1\neq 0=\bm{v}_{1}(0,0)\). Also, if \(\bm{z}_{1}=1\), we have that \(\bm{v}_{1}(1,1)=1\neq 2=\bm{v}_{1}(1,2)\)._

**Example 6** (Disconnected support with fixed permutation).: _Let \(\bm{v}:\hat{\mathcal{Z}}\to\mathbb{R}^{2}\) s.t. \(\hat{\mathcal{Z}}=\hat{\mathcal{Z}}^{(1)}\cup\hat{\mathcal{Z}}^{(2)}\subseteq \mathbb{R}^{2}\) where \(\hat{\mathcal{Z}}^{(1)}=\{\bm{z}\in\mathbb{R}^{2}\mid\bm{z}_{2}\leq 0\}\) and \(\hat{\mathcal{Z}}^{(2)}=\{\bm{z}\in\mathbb{R}^{2}\mid\bm{z}_{2}\geq 1\}\). Assume \(\bm{v}(\bm{z}):=\bm{z}+1(\bm{z}\in\hat{\mathcal{Z}}^{(2)})\)._

_Step 1: \(\bm{v}\) is a diffeomorphism. The image of \(\bm{v}\) is the union of the following two sets: \(\mathcal{Z}^{(1)}:=\bm{v}(\hat{\mathcal{Z}}^{(1)})=\hat{\mathcal{Z}}^{(1)}\) and \(\mathcal{Z}^{(2)}:=\bm{v}(\hat{\mathcal{Z}}^{(2)})=\{\bm{z}\in\mathbb{R}^{2} \mid\bm{z}_{2}\geq 2\}\). Consider the map \(\bm{w}:\mathcal{Z}^{(1)}\cup\mathcal{Z}^{(2)}\to\hat{\mathcal{Z}}\) defined as \(\bm{w}(\bm{z}):=\bm{z}-1(\bm{z}\in\mathcal{Z}^{(2)})\). We now show that \(\bm{w}\) is the inverse of \(\bm{v}\):_

\[\bm{w}(\bm{v}(\bm{z})) =\bm{v}(\bm{z})-1(\bm{v}(\bm{z})\in\mathcal{Z}^{(2)})\] (33) \[=\bm{z}+1(\bm{z}\in\hat{\mathcal{Z}}^{(2)})-1(\bm{z}+1(\bm{z}\in \hat{\mathcal{Z}}^{(2)})\in\mathcal{Z}^{(2)})\,.\] (34)

_If \(\bm{z}\in\hat{\mathcal{Z}}^{(2)}\), we have_

\[\bm{w}(\bm{v}(\bm{z})) =\bm{z}+1-1(\bm{z}+1\in\mathcal{Z}^{(2)})\] (35) \[=\bm{z}+1-1(\bm{z}\in\hat{\mathcal{Z}}^{(2)})=\bm{z}\,.\] (36)

_If \(\bm{z}\in\hat{\mathcal{Z}}^{(1)}\), we have_

\[\bm{w}(\bm{v}(\bm{z}))=\bm{z}-1(\bm{z}\in\mathcal{Z}^{(2)})=\bm{z}\,.\] (37)

_A similar argument can be made to show that \(\bm{v}(\bm{w}(\bm{z}))=\bm{z}\). Thus \(\bm{w}\) is the inverse of \(\bm{v}\). Both \(\bm{v}\) and its inverse \(\bm{w}\) are \(C^{2}\), thus \(\bm{v}\) is a \(C^{2}\)-diffeomorphism on its image._

_Step 2: \(\bm{v}\) is locally disentangled. This is clear since \(D\bm{v}(\bm{z})=\bm{I}\) everywhere._

_Step 3: \(\bm{v}\) is not globally disentangled. Indeed, the function \(\bm{v}_{1}(\bm{z}_{1},\bm{z}_{2})=\bm{z}_{1}+1(\bm{z}\in\hat{\mathcal{Z}}^{(2 )})\) is not constant in \(\bm{z}_{2}\)._

**Example 7** (Connected support).: _Let \(\bm{v}:\hat{\mathcal{Z}}\to\mathbb{R}^{2}\) s.t. \(\hat{\mathcal{Z}}=\hat{\mathcal{Z}}^{(b)}\cup\hat{\mathcal{Z}}^{(o)}\) where \(\hat{\mathcal{Z}}^{(b)}\) and \(\hat{\mathcal{Z}}^{(o)}\) are respectively the blue and orange regions of Figure 6. Both regions contain their boundaries. The function \(\bm{v}\) is defined as follows:_

\[\bm{v}_{1}(\bm{z}) :=\bm{z}_{1}\] (38) \[\bm{v}_{2}(\bm{z}) :=\begin{cases}\frac{(\bm{z}_{2}+1)^{2}+1}{2},&\text{if }\bm{z}\in\hat{ \mathcal{Z}}^{(b)}\\ e^{\bm{z}_{2}},&\text{if }\bm{z}\in\hat{\mathcal{Z}}^{(o)}\end{cases}\,.\] (39)

_Step 1: \(\bm{v}\) is a diffeomorphism. Clearly, \(\bm{v}_{1}\) is \(C^{2}\). To show that \(\bm{v}_{2}\) also is, we must verify that \(\bm{v}_{2}(\bm{z})\) is \(C^{2}\) at the frontier between \(\hat{\mathcal{Z}}^{(b)}\) and \(\hat{\mathcal{Z}}^{(o)}\), i.e. when \(\bm{z}\in[1/4,1]\times\{0\}\). \(\bm{v}_{2}(\bm{z})\) is continuous since_

\[\frac{(\bm{z}_{2}+1)^{2}+1}{2}\bigg{|}_{\bm{z}_{2}=0}=1=\left.e^{\bm{z}_{2}} \right|_{\bm{z}_{2}=0}\,.\] (40)\(\bm{v}_{2}(\bm{z})\) is \(C^{1}\) since_

\[\left.\left(\frac{(\bm{z}_{2}+1)^{2}+1}{2}\right)^{\prime}\right|_{\bm{z}_{2}=0}= \left.(\bm{z}_{2}+1)\right|_{\bm{z}_{2}=0}=1=\left.e^{\bm{z}_{2}}\right|_{\bm{z }_{2}=0}=\left.(e^{\bm{z}_{2}})^{\prime}\right|_{\bm{z}_{2}=0}\,.\] (41)

\(\bm{v}_{2}(\bm{z})\) is \(C^{2}\) since

\[\left.\left(\frac{(\bm{z}_{2}+1)^{2}+1}{2}\right)^{\prime\prime}\right|_{\bm{z }_{2}=0}=\left.1\right|_{\bm{z}_{2}=0}=1=\left.e^{\bm{z}_{2}}\right|_{\bm{z}_{ 2}=0}=\left.(e^{\bm{z}_{2}})^{\prime\prime}\right|_{\bm{z}_{2}=0}\,.\] (42)

_We will now find an explicit expression for the inverse of \(\bm{v}\). Define_

\[\bm{w}_{1}(\bm{z}) :=\bm{z}_{1}\] (43) \[\bm{w}_{2}(\bm{z}) :=\begin{cases}\sqrt{2\bm{z}_{2}-1}-1,&\text{if }\bm{z}\in\bm{v}( \hat{\mathcal{Z}}^{(b)})\\ \log(\bm{z}_{2}),&\text{if }\bm{z}\in\bm{v}(\hat{\mathcal{Z}}^{(o)})\end{cases}\,.\] (44)

_It is straightforward to see that \(\bm{w}(\bm{v}(\bm{z}))=\bm{z}\) for all \(\bm{z}\in\hat{\mathcal{Z}}\). One can also show that \(\bm{w}\) is \(C^{2}\) at the boundary between both regions \(\bm{v}(\hat{\mathcal{Z}}^{(b)})\) and \(\bm{v}(\hat{\mathcal{Z}}^{(o)})\), i.e. when \(\bm{z}\in[1/4,1]\times\{1\}\)._

_Since both \(\bm{v}\) and its inverse \(\bm{w}\) are \(C^{2}\), \(\bm{v}\) is a \(C^{2}\)-diffeomorphism._

_Step 2: \(\bm{v}\) is locally disentangled._ _The Jacobian of_ \(\bm{v}\) _is_

\[D\bm{v}(\bm{z}):=\begin{cases}\begin{bmatrix}1&0\\ 0&\bm{z}_{2}+1\end{bmatrix},\text{ if }\bm{z}\in\hat{\mathcal{Z}}^{(b)}\\ \begin{bmatrix}1&0\\ 0&e^{\bm{z}_{2}}\end{bmatrix},\text{ if }\bm{z}\in\hat{\mathcal{Z}}^{(o)}\end{cases}\,,\] (45)

_which is a permutation-scaling matrix everywhere on \(\hat{\mathcal{Z}}\). Thus local disentanglement holds._

_Step 3: \(\bm{v}\) is not globally disentangled._ _However,_ \(\bm{v}_{2}(\bm{z}_{1},\bm{z}_{2})\) _is not constant in_ \(\bm{z}_{1}\)_. Indeed,_

\[\bm{v}_{2}(-\frac{1}{2},-\frac{1}{2})=\left.\frac{(\bm{z}_{2}+1)^{2}+1}{2} \right|_{\bm{z}_{2}=-1/2}=\frac{5}{8}\neq e^{-1/2}=\bm{v}_{2}(\frac{1}{2},- \frac{1}{2})\,.\] (46)

_Thus global disentanglement does not hold._

### Proof of Theorem 1

**Proposition 9**.: _Suppose that the data-generating process satisfies Assumption 1, that the learned decoder \(\hat{\bm{f}}:\mathbb{R}^{d_{x}}\to\mathbb{R}^{d_{x}}\) is a \(C^{2}\)-diffeomorphism onto its image and that the encoder \(\hat{\bm{g}}:\mathbb{R}^{d_{x}}\to\mathbb{R}^{d_{z}}\) is continuous. Then, if \(\hat{\bm{f}}\) and \(\hat{\bm{g}}\) solve the reconstruction problem on the training distribution, i.e. \(\mathbb{E}^{\rm train}||\bm{x}-\hat{\bm{f}}(\hat{\bm{g}}(\bm{x}))||^{2}=0\), we have that \(\bm{f}(\mathcal{Z}^{\rm train})=\hat{\bm{f}}(\hat{\mathcal{Z}}^{\rm train})\) and the map \(\bm{v}:=\bm{f}^{-1}\circ\hat{\bm{f}}\) is a \(C^{2}\)-diffeomorphism from \(\hat{\mathcal{Z}}^{\rm train}\) to \(\mathcal{Z}^{\rm train}\)._

Proof.: First note that

\[\mathbb{E}^{\rm train}||\bm{x}-\hat{\bm{f}}(\hat{\bm{g}}(\bm{x}))||^{2}= \mathbb{E}^{\rm train}||\bm{f}(\bm{z})-\hat{\bm{f}}(\hat{\bm{g}}(\bm{f}(\bm{ z})))||^{2}=0\,,\] (47)

which implies that, for \(\mathbb{P}^{\rm train}_{\bm{z}}\)-almost every \(\bm{z}\in\mathcal{Z}^{\rm train}\),

\[\bm{f}(\bm{z})=\hat{\bm{f}}(\hat{\bm{g}}(\bm{f}(\bm{z})))\,.\]

But since the functions on both sides of the equations are continuous, the equality holds for all \(\bm{z}\in\mathcal{Z}^{\rm train}\). This implies that \(\bm{f}(\mathcal{Z}^{\rm train})=\hat{\bm{f}}\circ\hat{\bm{g}}\circ\bm{f}( \mathcal{Z}^{\rm train})=\hat{\bm{f}}(\hat{\mathcal{Z}}^{\rm train})\).

By Remark 3, the restrictions \(\bm{f}:\mathcal{Z}^{\rm train}\to\bm{f}(\mathcal{Z}^{\rm train})\) and \(\hat{\bm{f}}:\hat{\mathcal{Z}}^{\rm train}\to\hat{\bm{f}}(\hat{\mathcal{Z}}^ {\rm train})\) are \(C^{2}\)-diffeomorphisms and, because \(\bm{f}(\mathcal{Z}^{\rm train})=\hat{\bm{f}}(\hat{\mathcal{Z}}^{\rm train})\), their composition \(\bm{v}:=\bm{f}^{-1}\circ\hat{\bm{f}}:\hat{\mathcal{Z}}^{\rm train}\to\mathcal{Z }^{\rm train}\) is a well defined \(C^{2}\)-diffeomorphism (since \(C^{2}\)-diffeomorphisms are closed under composition).

**Theorem 1** (Local disentanglement via additive decoders).: _Suppose that the data-generating process satisfies Assumption 1, that the learned decoder \(\hat{\bm{f}}:\mathbb{R}^{d_{z}}\to\mathbb{R}^{d_{z}}\) is a \(C^{2}\)-diffeomorphism, that the encoder \(\hat{\bm{g}}:\mathbb{R}^{d_{x}}\to\mathbb{R}^{d_{z}}\) is continuous, that both \(\bm{f}\) and \(\hat{\bm{f}}\) are additive (Definition 1) and that \(\bm{f}\) is sufficiently nonlinear as formalized by Assumption 2. Then, if \(\hat{\bm{f}}\) and \(\hat{\bm{g}}\) solve the reconstruction problem on the training distribution, i.e. \(\mathbb{E}^{\rm train}||\bm{x}-\hat{\bm{f}}(\hat{\bm{g}}(\bm{x}))||^{2}=0\), we have that \(\hat{\bm{f}}\) is locally \(\mathcal{B}\)-disentangled w.r.t. \(\bm{f}\) (Definition 4)._

Proof.: We can apply Proposition 9 and have that the map \(\bm{v}:=\bm{f}^{-1}\circ\hat{\bm{f}}\) is a \(C^{2}\)-diffeomorphism from \(\hat{\mathcal{Z}}^{\rm train}\) to \(\mathcal{Z}^{\rm train}\). This allows one to write

\[\bm{f}\circ\bm{v}(\bm{z}) =\hat{\bm{f}}(\bm{z})\;\forall\bm{z}\in\hat{\mathcal{Z}}^{\rm train}\] (48) \[\sum_{B\in\mathcal{B}}\bm{f}^{(B)}(\bm{v}_{B}(\bm{z})) =\sum_{B\in\mathcal{B}}\hat{\bm{f}}^{(B)}(\bm{z}_{B})\;\forall \bm{z}\in\hat{\mathcal{Z}}^{\rm train}\,.\] (49)

Since \(\mathcal{Z}^{\rm train}\) is regularly closed and is diffeomorphic to \(\hat{\mathcal{Z}}^{\rm train}\), by Lemma 4, we must have that \(\hat{\mathcal{Z}}^{\rm train}\subseteq\overline{(\hat{\mathcal{Z}}^{\rm train })^{\circ}}\). Moreover, the left and right hand side of (49) are \(C^{2}\), which means they have uniquely defined first and second derivatives on \(\overline{(\hat{\mathcal{Z}}^{\rm train})^{\circ}}\) by Lemma 6. This means the derivatives are uniquely defined on \(\hat{\mathcal{Z}}^{\rm train}\).

Let \(\bm{z}\in\hat{\mathcal{Z}}^{\rm train}\). Choose some \(J\in\mathcal{B}\) and some \(j\in J\). Differentiate both sides of the above equation with respect to \(\bm{z}_{j}\), which yields:

\[\sum_{B\in\mathcal{B}}\sum_{i\in B}D_{i}\bm{f}^{(B)}(\bm{v}_{B}(\bm{z}))D_{j} \bm{v}_{i}(\bm{z})=D_{j}\hat{\bm{f}}^{(J)}(\bm{z}_{J})\,.\] (50)

Choose \(J^{\prime}\in\mathcal{B}\setminus\{J\}\) and \(j^{\prime}\in J^{\prime}\). Differentiating the above w.r.t. \(\bm{z}_{j^{\prime}}\) yields

\[\sum_{B\in\mathcal{B}}\sum_{i\in B}\left[D_{i}\bm{f}^{(B)}(\bm{v} _{B}(\bm{z}))D_{j,j^{\prime}}^{2}\bm{v}_{i}(\bm{z})+\sum_{i^{\prime}\in B}D_{i,i^{\prime}}^{2}\bm{f}^{(B)}(\bm{v}_{B}(\bm{z}))D_{j^{\prime}}\bm{v}_{i^{\prime }}(\bm{z})D_{j}\bm{v}_{i}(\bm{z})\right]=0\] \[\sum_{B\in\mathcal{B}}\left[\sum_{i\in B}\Big{[}D_{i}\bm{f}^{(B)}( \bm{v}_{B}(\bm{z}))D_{j,j^{\prime}}^{2}\bm{v}_{i}(\bm{z})+D_{i,i}^{2}\bm{f}^{( B)}(\bm{v}_{B}(\bm{z}))D_{j^{\prime}}\bm{v}_{i}(\bm{z})D_{j}\bm{v}_{i}(\bm{z}) \Big{]}+\right.\] \[\left.\sum_{(i,i^{\prime})\in B_{\leq}^{2}}D_{i,i^{\prime}}^{2} \bm{f}^{(B)}(\bm{v}_{B}(\bm{z}))(D_{j^{\prime}}\bm{v}_{i^{\prime}}(\bm{z})D_{j} \bm{v}_{i}(\bm{z})+D_{j^{\prime}}\bm{v}_{i}(\bm{z})D_{j}\bm{v}_{i^{\prime}}(\bm{ z}))\right]=0\,,\] (51)

where \(B_{<}^{2}:=B^{2}\cap\{(i,i^{\prime})\mid i^{\prime}<i\}\). For the sake of notational conciseness, we are going to refer to \(S_{\mathcal{B}}\) and \(S_{\mathcal{B}}^{S}\) as \(S\) and \(S^{c}\) (Definition 11). Also, define

\[S_{<}:=\bigcup_{B\in\mathcal{B}}B_{<}^{2}\,.\] (52)

Let us define the vectors

\[\forall i\in\{1,...d_{z}\},\;\vec{a}_{i}(\bm{z}) :=(D_{j,j^{\prime}}^{2}\bm{v}_{i}(\bm{z}))_{(j,j^{\prime})\in S^{c}}\] (53) \[\forall i\in\{1,...d_{z}\},\;\vec{b}_{i}(\bm{z}) :=(D_{j^{\prime}}\bm{v}_{i}(\bm{z})D_{j}\bm{v}_{i}(\bm{z}))_{(j,j^{ \prime})\in S^{c}}\] (54) \[\forall B\in\mathcal{B},\;\forall(i,i^{\prime})\in B_{<}^{2},\; \vec{c}_{i,i^{\prime}}(\bm{z}) :=(D_{j^{\prime}}\bm{v}_{i^{\prime}}(\bm{z})D_{j}\bm{v}_{i}(\bm{z})+D_{j^{ \prime}}\bm{v}_{i}(\bm{z})D_{j}\bm{v}_{i^{\prime}}(\bm{z}))_{(j,j^{\prime})\in S ^{c}}\] (55)

This allows us to rewrite, for all \(k\in\{1,...,d_{x}\}\)

\[\sum_{B\in\mathcal{B}}\left[\sum_{i\in B}\Big{[}D_{i}\bm{f}_{k}^{(B)}(\bm{v}_{B} (\bm{z}))\vec{a}_{i}(\bm{z})+D_{i,i}^{2}\bm{f}_{k}^{(B)}(\bm{v}_{B}(\bm{z})) \vec{b}_{i}(\bm{z})\Big{]}+\sum_{(i,i^{\prime})\in B_{<}^{2}}D_{i,i^{\prime}}^{2 }\bm{f}_{k}^{(B)}(\bm{v}_{B}(\bm{z}))\vec{c}_{i,i^{\prime}}(\bm{z})\right]=0\,.\] (56)

We define

\[\bm{w}(\bm{z},k) :=((D_{i}\bm{f}_{k}^{(B)}(\bm{z}_{B}))_{i\in B},(D_{i,i}^{2}\bm{f} _{k}^{(B)}(\bm{z}_{B}))_{i\in B},(D_{i,i^{\prime}}^{2}\bm{f}_{k}^{(B)}(\bm{z}_{B})) _{(i,i^{\prime})\in B_{<}^{2}})_{B\in\mathcal{B}}\] (57) \[\bm{M}(\bm{z}) :=[[\vec{a}_{i}(\bm{z})]_{i\in B},[\vec{b}_{i}(\bm{z})]_{i\in B},[ \vec{c}_{i,i^{\prime}}(\bm{z})]_{(i,i^{\prime})\in B_{<}^{2}}]_{B\in \mathcal{B}}\,,\] (58)which allows us to write, for all \(k\in\{1,...,d_{z}\}\)

\[\bm{M}(\bm{z})\bm{w}(\bm{v}(\bm{z}),k)=0\,.\] (59)

We can now recognize that the matrix \(\bm{W}(\bm{v}(\bm{z}))\) of Assumption 2 is given by

\[\bm{W}(\bm{v}(\bm{z}))^{\top}=[\bm{w}(\bm{v}(\bm{z}),1)\ \ldots\ \bm{w}(\bm{v}(\bm{z}),d_{x})]\] (60)

which allows us to write

\[\bm{M}(\bm{z})\bm{W}(\bm{v}(\bm{z}))^{\top}=0\] (61) \[\bm{W}(\bm{v}(\bm{z}))\bm{M}(\bm{z})^{\top}=0\] (62)

Since \(\bm{W}(\bm{v}(\bm{z}))\) has full column-rank (by Assumption 2 and the fact that \(\bm{v}(\bm{z})\in\mathcal{Z}^{\text{train}}\)), there exists \(q\) rows that are linearly independent. Let \(K\) be the index set of these rows. This means \(\bm{W}(\bm{v}(\bm{z}))_{K,\cdot}\) is an invertible matrix. We can thus write

\[\bm{W}(\bm{v}(\bm{z}))_{K,\cdot}\bm{M}(\bm{z})^{\top} =0\] (63) \[(\bm{W}(\bm{v}(\bm{z}))_{K,\cdot})^{-1}\bm{W}(\bm{v}(\bm{z}))_{K,\cdot}\bm{M}(\bm{z})^{\top} =(\bm{W}(\bm{v}(\bm{z}))_{K,\cdot})^{-1}0\] (64) \[\bm{M}(\bm{z})^{\top} =0\,,\] (65)

which means, in particular, that, \(\forall i\in\{1,\ldots,d_{z}\}\), \(\vec{b}_{i}(\bm{z})=0\), i.e.,

\[\forall i\in\{1,\ldots,d_{z}\},\forall(j,j^{\prime})\in S^{c},D_{j}\bm{v}_{i} (\bm{z})D_{j^{\prime}}\bm{v}_{i}(\bm{z})=0\] (66)

Since the \(\bm{v}\) is a diffeomorphism, its Jacobian matrix \(D\bm{v}(\bm{z})\) is invertible everywhere. By Lemma 5, this means there exists a permutation \(\pi\) such that, for all \(j\), \(D_{j}\bm{v}_{\pi(j)}(\bm{z})\neq 0\). This and (66) imply that

\[\forall(j,j^{\prime})\in S^{c},\ \ D_{j}\bm{v}_{\pi(j^{\prime})}(\bm{z}) \underbrace{D_{j^{\prime}}\bm{v}_{\pi(j^{\prime})}(\bm{z})}_{\neq 0}=0,\] (67)

\[\implies\forall(j,j^{\prime})\in S^{c},\ \ D_{j}\bm{v}_{\pi(j^{\prime})}(\bm{z}) =0\,.\] (68)

To show that \(D\bm{v}(\bm{z})\) is a \(\mathcal{B}\)-block permutation matrix, the only thing left to show is that \(\pi\) respects \(\mathcal{B}\). For this, we use the fact that, \(\forall B\in\mathcal{B},\forall(i,i^{\prime})\in B^{2}_{<}\), \(\vec{c}_{i,i^{\prime}}(\bm{z})=0\) (recall \(\bm{M}(\bm{z})=0\)). Because \(\vec{c}_{i,i^{\prime}}(\bm{z})=\vec{c}_{i^{\prime},i}(\bm{z})\), we can write

\[\forall(i,i^{\prime})\in S\ \text{s.t.}\ i\neq i^{\prime},\forall(j,j^{\prime}) \in S^{c},D_{j^{\prime}}\bm{v}_{i^{\prime}}(\bm{z})D_{j}\bm{v}_{i}(\bm{z})+D_ {j^{\prime}}\bm{v}_{i}(\bm{z})D_{j}\bm{v}_{i^{\prime}}(\bm{z})=0\,.\] (69)

We now show that if \((j,j^{\prime})\in S^{c}\) (indices belong to different blocks), then \((\pi(j),\pi(j^{\prime}))\in S^{c}\) (they also belong to different blocks). Assume this is false, i.e. there exists \((j_{0},j^{\prime}_{0})\in S^{c}\) such that \((\pi(j_{0}),\pi(j^{\prime}_{0}))\in S\). Then we can apply (69) (with \(i:=\pi(j_{0})\) and \(i^{\prime}:=\pi(j^{\prime}_{0})\)) and get

\[\underbrace{D_{j^{\prime}_{0}}\bm{v}_{\pi(j^{\prime}_{0})}(\bm{z})D_{j_{0}} \bm{v}_{\pi(j_{0})}(\bm{z})}_{\neq 0}+D_{j^{\prime}_{0}}\bm{v}_{\pi(j_{0})}(\bm{z})D_{j_{0}} \bm{v}_{\pi(j^{\prime}_{0})}(\bm{z})=0\,,\] (70)

where the left term in the sum is different of \(0\) because of the definition of \(\pi\). This implies that

\[D_{j^{\prime}_{0}}\bm{v}_{\pi(j_{0})}(\bm{z})D_{j_{0}}\bm{v}_{\pi(j^{\prime}_{0 })}(\bm{z})\neq 0\,,\] (71)

otherwise (70) cannot hold. But (71) contradicts (68). Thus, we have that,

\[(j,j^{\prime})\in S^{c}\implies(\pi(j),\pi(j^{\prime}))\in S^{c}\,.\] (72)

The contraposed is

\[(\pi(j),\pi(j^{\prime}))\in S\implies(j,j^{\prime})\in S\] (73) \[(j,j^{\prime})\in S\implies(\pi^{-1}(j),\pi^{-1}(j^{\prime})) \in S\,.\] (74)

From the above, it is clear that \(\pi^{-1}\) respects \(\mathcal{B}\) which implies that \(\pi\) respects \(\mathcal{B}\) (Lemma 10). Thus \(D\bm{v}(\bm{z})\) is a \(\mathcal{B}\)-block permutation matrix.

**Lemma 10** (\(\mathcal{B}\)-respecting permutations form a group).: _Let \(\mathcal{B}\) be a partition of \(\{1,\ldots,d_{z}\}\) and let \(\pi\) and \(\bar{\pi}\) be a permutation of \(\{1,\ldots,d_{z}\}\) that respect \(\mathcal{B}\). The following holds:_

1. _The identity permutation_ \(e\) _respects_ \(\mathcal{B}\)_._
2. _The composition_ \(\pi\circ\bar{\pi}\) _respects_ \(\mathcal{B}\)_._
3. _The inverse permutation_ \(\pi^{-1}\) _respects_ \(\mathcal{B}\)_._

Proof.: The first statement is trivial, since for all \(B\in\mathcal{B}\), \(e(B)=B\in\mathcal{B}\).

The second statement follows since for all \(B\in\mathcal{B}\), \(\bar{\pi}(B)\in\mathcal{B}\) and thus \(\pi(\bar{\pi}(B))\in\mathcal{B}\).

We now prove the third statement. Let \(B\in\mathcal{B}\). Since \(\pi\) is surjective and respects \(\mathcal{B}\), there exists a \(B^{\prime}\in\mathcal{B}\) such that \(\pi(B^{\prime})=B\). Thus, \(\pi^{-1}(B)=\pi^{-1}(\pi(B^{\prime}))=B^{\prime}\in\mathcal{B}\). 

### Sufficient nonlinearity v.s. sufficient variability in nonlinear ICA with auxiliary variables

In Section 3.1, we introduced the "sufficient nonlinearity" condition (Assumption 2) and highlighted its resemblance to the "sufficient variability" assumptions often found in the nonlinear ICA literature [30, 31, 33, 36, 37, 42, 73]. We now clarify this connection. To make the discussion more concrete, we consider the sufficient variability assumption found in Hyvarinen et al. [33]. In this work, the latent variable \(\bm{z}\) is assumed to be distributed according to

\[p(\bm{z}\mid\bm{u}):=\prod_{i=1}^{d_{z}}p_{i}(\bm{z}_{i}\mid\bm{u})\,.\] (75)

In other words, the latent factors \(\bm{z}_{i}\) are mutually conditionally independent given an observed auxiliary variable \(\bm{u}\). Define

\[\bm{w}(\bm{z},\bm{u}):=\left(\left(\frac{\partial}{\partial\bm{z}_{i}}\log p _{i}(\bm{z}_{i}\mid\bm{u})\right)_{i\in[d_{z}]}\,\left(\frac{\partial^{2}}{ \partial\bm{z}_{i}^{2}}\log p_{i}(\bm{z}_{i}\mid\bm{u})\right)_{i\in[d_{z}]} \right)\in\mathbb{R}^{2d_{z}}\,.\] (76)

We now recall the assumption of sufficient variability of Hyvarinen et al. [33]:

**Assumption 3** (Assumption of variability from Hyvarinen et al. [33, Theorem 1]).: _For any \(\bm{z}\in\mathbb{R}^{d_{z}}\), there exists \(2d_{z}+1\) values of \(\bm{u}\), denoted by \(\bm{u}^{(0)},\bm{u}^{(1)},\ldots,\bm{u}^{(2d_{z})}\) such that the \(2d_{z}\) vectors_

\[\bm{w}(\bm{z},\bm{u}^{(1)})-\bm{w}(\bm{z},\bm{u}^{(0)}),\ldots,\bm{w}(\bm{z}, \bm{u}^{(2d_{z})})-\bm{w}(\bm{z},\bm{u}^{(0)})\] (77)

_are linearly independent._

To emphasize the resemblance with our assumption of sufficient nonlinearity, we rewrite it in the special case where the partition \(\mathcal{B}:=\{\{1\},\ldots,\{d_{z}\}\}\). Note that, in that case, \(q:=d_{z}+\sum_{B\in\mathcal{B}}\frac{|B|(|B|+1)}{2}=2d_{z}\).

**Assumption 4** (Sufficient nonlinearity (trivial partition)).: _For all \(\bm{z}\in\mathcal{Z}^{\text{train}}\), \(\bm{f}\) is such that the following matrix has independent columns (i.e. full column-rank):_

\[\bm{W}(\bm{z}):=\left[\left[D_{i}\bm{f}^{(i)}(\bm{z}_{i})\right]_{i\in[d_{z}] }\,\left[D_{i,i}^{2}\bm{f}^{(i)}(\bm{z}_{i})\right]_{i\in[d_{z}]}\right]\in \mathbb{R}^{d_{x}\times 2d_{z}}\,.\] (78)

One can already see the resemblance between Assumptions 3 & 4, e.g. both have something to do with first and second derivatives. To make the connection even more explicit, define \(\bm{w}(\bm{z},k)\) to be the \(k\)th row of \(\bm{W}(\bm{z})\) (do not conflate with \(\bm{w}(\bm{z},\bm{u})\)). Also, recall the basic fact from linear algebra that the column-rank is always equal to the row-rank. This means that \(\bm{W}(\bm{z})\) is full column-rank if and only if there exists \(k_{1}\),..., \(k_{2d_{z}}\in[d_{x}]\) such that the vectors \(\bm{w}(\bm{z},k_{1}),\ldots,\bm{w}(\bm{z},k_{2d_{z}})\) are linearly independent. It is then easy to see the correspondance between \(\bm{w}(\bm{z},k)\) and \(\bm{w}(\bm{z},\bm{u})-\bm{w}(\bm{z},\bm{u}^{(0)})\) (from Assumption 3) and between the pixel index \(k\in[d_{x}]\) and the auxiliary variable \(\bm{u}\).

We now look at why Assumption 2 is likely to be satisfied when \(d_{x}>>d_{z}\). Informally, one can see that when \(d_{x}\) is much larger than \(2d_{z}\), the matrix \(\bm{W}(z)\) has much more rows than columns and thus it becomes more likely that we will find \(2d_{z}\) rows that are linearly independent, thus satisfying Assumption 2.

### Examples of sufficiently nonlinear additive decoders

**Example 8** (A sufficiently nonlinear \(\bm{f}\) - Example 3 continued).: _Consider the additive function_

\[\bm{f}(\bm{z}):=\begin{bmatrix}\bm{z}_{1}\\ \bm{z}_{1}^{\bm{z}_{1}}\\ \bm{z}_{1}^{\bm{z}_{1}}\\ \bm{z}_{1}^{\bm{z}_{1}}\end{bmatrix}+\begin{bmatrix}(\bm{z}_{2}+1)\\ (\bm{z}_{2}+1)^{2}\\ (\bm{z}_{2}+1)^{3}\\ (\bm{z}_{2}+1)^{4}\end{bmatrix}\,.\] (79)

_We will provide a numerical verification that this function is a diffeomorphism from the square \([-1,0]\times[0,1]\) to its image that satisfies Assumption 2._

_The Jacobian of \(\bm{f}\) is given by_

\[D\bm{f}(\bm{z})=\begin{bmatrix}1&1\\ 2\bm{z}_{1}&2(\bm{z}_{2}+1)\\ 3\bm{z}_{1}^{2}&3(\bm{z}_{2}+1)^{2}\\ 4\bm{z}_{1}^{3}&4(\bm{z}_{2}+1)^{3}\end{bmatrix}\,,\] (80)

_and the matrix \(\bm{W}(\bm{z})\) from Assumption 2 is given by_

\[\bm{W}(\bm{z})=\begin{bmatrix}1&0&1&0\\ 2\bm{z}_{1}&2&2(\bm{z}_{2}+1)&2\\ 3\bm{z}_{1}^{2}&6\bm{z}_{1}&3(\bm{z}_{2}+1)^{2}&6(\bm{z}_{2}+1)\\ 4\bm{z}_{1}^{3}&12\bm{z}_{1}^{2}&4(\bm{z}_{2}+1)^{3}&12(\bm{z}_{2}+1)^{2} \end{bmatrix}\,.\] (81)

_Figure 7 presents a numerical verification that \(\bm{f}\) is injective, has a full rank Jacobian and satisfies Assumption 2. Injective \(\bm{f}\) with full rank Jacobian is enough to conclude that \(\bm{f}\) is a diffeomorphism onto its image._

**Example 9** (Smooth balls dataset is sufficiently nonlinear - Example 4 continued).: _We implemented a ground-truth additive decoder \(\bm{f}:[0,5]^{2}\to\mathbb{R}^{64\leqslant 64\leqslant 3}\) which maps to 64x64 RGB images consisting of two colored balls where \(\bm{z}_{1}\) and \(\bm{z}_{2}\) control their respective heights (Figure 7(a)). The analytical form of \(\bm{f}\) can be found in our code base. The decoder \(\bm{f}\) is implemented in JAX [6] which allows for its automatic differentiation to compute \(D\bm{f}\) and \(D^{2}\bm{f}\) (Figures 7(b) & 7(c)). This allows us to verify numerically that \(\bm{f}\) is sufficiently nonlinear (Assumption 2). Recall that this assumption requires that \(\bm{W}(\bm{z})\) (defined in Assumption 2) has independent columns everywhere. To test this, we compute \(\text{Vol}(\bm{z}):=\sqrt{|\det(\bm{W}(\bm{z})^{\top}\bm{W}(\bm{z}))|}\) over a grid of values of \(\bm{z}\) and verify that \(\text{Vol}(\bm{z})>0\) everywhere (Figure 7(d)). Note that \(\text{Vol}(\bm{z})\) corresponds to the \(4D\) volume of the parallelepiped embedded in \(\mathbb{R}^{64\leqslant 64\leqslant 3}\) spanned by the four columns of \(\bm{W}(\bm{z})\). This volume is \(>0\) if and only if the columns are linearly independent. Note that we normalize the columns of \(\bm{W}(\bm{z})\) so that they have a norm

Figure 7: Numerical verification that \(\bm{f}:[-1,0]\times[0,1]\to\mathbb{R}^{4}\) from Example 8 is injective (**left**), has a full rank Jacobian (**middle**) and satisfies Assumption 2 (**right**). The **left** figure shows that \(\bm{f}\) is injective on the square \([-1,0]\times[0,1]\) since one can recover \(\bm{z}\) uniquely by knowing the values of \(\bm{f}_{1}(\bm{z})\) and \(\bm{f}_{2}(\bm{z})\), i.e. knowing the level sets. The **middle** figure reports the \(\det(D\bm{f}(\bm{z})^{\top}D\bm{f}(\bm{z}))\) (columns of the Jacobian are normalized to have norm 1) and shows that it is nonzero in the square \([-1,0]\times[0,1]\), which means the Jacobian is full rank. The **right** figure shows the determinant of the matrix \(\bm{W}(\bm{z})\) (from Assumption 2, but with normalized columns), we can see that it is nonzero everywhere on the square \([-1,0]\times[0,1]\). We normalized the columns of \(D\bm{f}\) and \(\bm{W}\) so that the determinant is between 0 and 1.

of one. It follows that \(\text{Vol}(\bm{z})\) is between \(0\) and \(1\) where \(1\) means the vectors are orthogonal, i.e. maximally independent. The minimal value of \(\text{Vol}(\bm{z})\) over the domain of \(\bm{f}\) is \(\approx 0.97\), indicating that Assumption 2 holds._

### Proof of Theorem 2

We start with a simple definition:

**Definition 15** (\(\mathcal{B}\)-block permutation matrices).: _A matrix \(\bm{A}\in\mathbb{R}^{d\times d}\) is a \(\mathcal{B}\)-block permutation matrix if it is invertible and can be written as \(\bm{A}=\bm{C}\bm{P}_{\pi}\) where \(\bm{P}_{\pi}\) is the matrix representing the \(\mathcal{B}\)-respecting permutation \(\pi\) (\(P_{\pi}\bm{e}_{i}=\bm{e}_{\pi(i)}\)) and \(\bm{C}\in\mathbb{R}^{d\times d}_{\mathcal{S}\mathbb{R}}\) (See Definitions 10 & 11)._

The following technical lemma leverages continuity and path-connectedness to show that the block-permutation structure must remain the same across the whole domain. It can be skipped at first read.

**Lemma 11**.: _Let \(\mathcal{C}\) be a connected topological space and let \(\bm{M}:\mathcal{C}\rightarrow\mathbb{R}^{d\times d}\) be a continuous function. Suppose that, for all \(c\in\mathcal{C}\), \(\bm{M}(c)\) is an invertible \(\mathcal{B}\)-block permutation matrix (Definition 15). Then, there exists a \(\mathcal{B}\)-respecting permutation \(\pi\) such that for all \(c\in\mathcal{C}\) and all distinct \(B,B^{\prime}\in\mathcal{B}\), \(\bm{M}(c)_{\pi(B^{\prime}),B}=0\)._

Proof.: The reason this result is not trivial, is that, even if \(\bm{M}(c)\) is a \(\mathcal{B}\)-block permutation for all \(c\), the permutation might change for different \(c\). The goal of this lemma is to show that, if \(\mathcal{C}\) is connected and the map \(\bm{M}(\cdot)\) is continuous, then one can find a single permutation that works for all \(c\in\mathcal{C}\).

First, since \(\mathcal{C}\) is connected and \(\bm{M}\) is continuous, its image, \(\bm{M}(\mathcal{C})\), must be connected (by [56, Theorem 23.5]).

Figure 8: Figure (a) shows an image the synthetic dataset of Example 9. Figure (b) shows the derivative of the image w.r.t. \(\bm{z}_{1}\) (the height of the left ball) where the color intensity of each pixel corresponds to the Euclidean norm along the RGB axis. Figure (c) similarly shows the second derivative of the image w.r.t. \(\bm{z}_{1}\). Figure (d) is a contour plot of the function \(\sqrt{|\det(\bm{W}(\bm{z})^{\top}\bm{W}(\bm{z}))|}\) where \(\bm{W}(\bm{z})\) is defined in Assumption 2 (here columns are normalized to have unit norm). The smallest value of \(\sqrt{|\det(\bm{W}(\bm{z})^{\top}\bm{W}(\bm{z}))|}\) across domain is \(\approx 0.97\), indicating that Assumption 2 is satisfied. See Example 9 and code for details. Figure (e)e is a higher resolution rendering of the red region of Figure (d)d (to make sure there is no singularity there).

Second, from the hypothesis of the lemma, we know that

\[\bm{M}(\mathcal{C})\subseteq\mathcal{A}:=\left(\bigcup_{\pi\in\mathfrak{S}( \mathcal{B})}\mathbb{R}_{S_{\mathcal{B}}}^{d\times d}\bm{P}_{\pi}\right)\setminus \left\{\text{singular matrices}\right\},\] (82)

where \(\mathfrak{S}(\mathcal{B})\) is the set of \(\mathcal{B}\)-respecting permutations and \(\mathbb{R}_{S_{\mathcal{B}}}^{d\times d}\bm{P}_{\pi}=\{\bm{M}\bm{P}_{\pi}\mid \bm{M}\in\mathbb{R}_{S_{\mathcal{B}}}^{d\times d}\}\). We can rewrite the set \(\mathcal{A}\) above as

\[\mathcal{A}=\bigcup_{\pi\in\mathfrak{S}(\mathcal{B})}\left(\mathbb{R}_{S_{ \mathcal{B}}}^{d\times d}\bm{P}_{\pi}\setminus\left\{\text{singular matrices}\right\}\right)\,,\] (83)

We now define an equivalence relation \(\sim\) over \(\mathcal{B}\)-respecting permutation: \(\pi\sim\pi^{\prime}\) iff for all \(B\in\mathcal{B}\), \(\pi(B)=\pi^{\prime}(B)\). In other words, two \(\mathcal{B}\)-respecting permutations are equivalent if they send every block to the same block (note that they can permute elements of a given block differently). We notice that

\[\pi\sim\pi^{\prime}\implies\mathbb{R}_{S_{\mathcal{B}}}^{d\times d}\bm{P}_{ \pi}=\mathbb{R}_{S_{\mathcal{B}}}^{d\times d}\bm{P}_{\pi^{\prime}}\,.\] (84)

Let \(\mathfrak{S}(\mathcal{B})/\sim\) be the set of equivalence classes induce by \(\sim\) and let \(\Pi\) stand for one such equivalence class. Thanks to (84), we can define, for all \(\Pi\in\mathfrak{S}(\mathcal{B})/\sim\), the following set:

\[V_{\Pi}:=\mathbb{R}_{S_{\mathcal{B}}}^{d\times d}\bm{P}_{\pi}\setminus\left\{ \text{singular matrices}\right\},\text{ for some }\pi\in\Pi\,,\] (85)

where the specific choice of \(\pi\in\Pi\) is arbitrary (any \(\pi^{\prime}\in\Pi\) would yield the same definition, by (84)). This construction allows us to write

\[\mathcal{A}=\bigcup_{\Pi\in\mathfrak{S}(\mathcal{B})/\sim}V_{\Pi}\,,\] (86)

We now show that \(\{V_{\Pi}\}_{\Pi\in\mathfrak{S}(\mathcal{B})/\sim}\) forms a partition of \(\mathcal{A}\). Choose two distinct equivalence classes of permutations \(\Pi\) and \(\Pi^{\prime}\) and let \(\pi\in\Pi\) and \(\pi^{\prime}\in\Pi^{\prime}\) be representatives. We note that

\[\mathbb{R}_{S_{\mathcal{B}}}^{d\times d}\bm{P}_{\pi}\cap\mathbb{R}_{S_{ \mathcal{B}}}^{d\times d}\bm{P}_{\pi^{\prime}}\subseteq\left\{\text{ singular matrices}\right\},\] (87)

since any matrix that is both in \(\mathbb{R}_{S_{\mathcal{B}}}^{d\times d}\bm{P}_{\pi}\) and \(\mathbb{R}_{S_{\mathcal{B}}}^{d\times d}\bm{P}_{\pi^{\prime}}\) must have at least one row filled with zeros. This implies that

\[V_{\Pi}\cap V_{\Pi^{\prime}}=\emptyset\,,\] (88)

which shows that \(\{V_{\Pi}\}_{\Pi\in\mathfrak{S}(\mathcal{B})/\sim}\) is indeed a partition of \(\mathcal{A}\).

Each \(V_{\Pi}\) is closed in \(\mathcal{A}\) (wrt the relative topology) since

\[V_{\Pi}=\mathbb{R}_{S_{\mathcal{B}}}^{d\times d}\bm{P}_{\pi}\setminus\left\{ \text{singular matrices}\right\}=\mathcal{A}\cap\underbrace{\mathbb{R}_{S_{ \mathcal{B}}}^{d\times d}\bm{P}_{\pi}}_{\text{closed in }\mathbb{R}^{d\times d}}\,.\] (89)

Moreover, \(V_{\Pi}\) is open in \(\mathcal{A}\), since

\[V_{\Pi}=\mathcal{A}\setminus\bigcup_{\begin{subarray}{c}\Pi^{\prime}\neq\Pi \\ \text{closed in }\mathcal{A}\end{subarray}}V_{\Pi^{\prime}}\,.\] (90)

Thus, for any \(\Pi\in\mathfrak{S}(\mathcal{B})/\sim\), the sets \(V_{\Pi}\) and \(\bigcup_{\Pi^{\prime}\neq\Pi}V_{\Pi^{\prime}}\) forms a _separation_ (see [56, Section 23]). Since \(\bm{M}(\mathcal{C})\) is a connected subset of \(\mathcal{A}\), it must lie completely in \(V_{\Pi}\) or \(\bigcup_{\Pi^{\prime}\neq\Pi}V_{\Pi^{\prime}}\), by [56, Lemma 23.2]. Since this is true for all \(\Pi\), it must follow that there exists a \(\Pi^{*}\) such that \(\bm{M}(\mathcal{C})\subseteq V_{\Pi^{*}}\), which completes the proof. 

**Theorem 2** (From local to global disentanglement).: _Suppose that all the assumptions of Theorem 1 hold. Additionally, assume \(\mathcal{Z}^{\text{train}}\) is path-connected (Definition 8) and that the block-specific decoders \(\bm{f}^{(B)}\) and \(\hat{\bm{f}}^{(B)}\) are injective for all blocks \(B\in\mathcal{B}\). Then, if \(\hat{\bm{f}}\) and \(\hat{\bm{g}}\) solve the reconstruction problem on the training distribution, i.e. \(\mathbb{E}^{\text{train}}||\bm{x}-\hat{\bm{f}}(\hat{\bm{g}}(\bm{x}))||^{2}=0\), we have that \(\hat{\bm{f}}\) is (globally) \(\mathcal{B}\)-disentangled w.r.t. \(\bm{f}\) (Definition 3) and, for all \(B\in\mathcal{B}\),_

\[\hat{\bm{f}}^{(B)}(\bm{z}_{B})=\bm{f}^{(\pi(B))}(\bar{\bm{v}}_{\pi(B)}(\bm{z}_{B }))+\bm{c}^{(B)},\text{ for all }\bm{z}_{B}\in\hat{\mathcal{Z}}_{B}^{\text{train}}\,,\] (8)_where the functions \(\bar{\bm{v}}_{\pi(B)}\) are from Definition 3 and the vectors \(\bm{c}^{(B)}\in\mathbb{R}^{d_{x}}\) are constants such that \(\sum_{B\in\mathcal{B}}\bm{c}^{(B)}=0\). We also have that the functions \(\bar{\bm{v}}_{\pi(B)}:\hat{\mathcal{Z}}_{B}^{\text{train}}\to\mathcal{Z}_{\pi( B)}^{\text{train}}\) are \(C^{2}\)-diffeomorphisms and have the following form:_

\[\bar{\bm{v}}_{\pi(B)}(\bm{z}_{B})=(\bm{f}^{\pi(B)})^{-1}(\hat{\bm{f}}^{(B)}( \bm{z}_{B})-\bm{c}^{(B)}),\text{ for all }\bm{z}_{B}\in\hat{\mathcal{Z}}_{B}^{\text{ train}}\,.\] (9)

Proof.: **Step 1 - Showing the permutation \(\pi\) does not change for different \(\bm{z}\).** Theorem 1 showed local \(\mathcal{B}\)-disentanglement, i.e. for all \(\bm{z}\in\hat{\mathcal{Z}}^{\text{train}}\), \(D\bm{v}(\bm{z})\) has a \(\mathcal{B}\)-block permutation structure. The first step towards showing global disentanglement is to show that this block structure is the same for all \(\bm{z}\in\hat{\mathcal{Z}}^{\text{train}}\) (_a priori_, \(\pi\) could be different for different \(\bm{z}\)). Since \(\bm{v}\) is \(C^{2}\), its Jacobian \(D\bm{v}(\bm{z})\) is continuous. Since \(\mathcal{Z}^{\text{train}}\) is path-connected, \(\hat{\mathcal{Z}}^{\text{train}}\) must also be since both sets are diffeomorphic. By Lemma 11, this means the \(\mathcal{B}\)-block permutation structure of \(D\bm{v}(\bm{z})\) is the same for all \(\bm{z}\in\hat{\mathcal{Z}}^{\text{train}}\) (implicitly using the fact that path-connected implies connected). In other words, there exists a permutation \(\pi\) respecting \(\mathcal{B}\) such that, for all \(\bm{z}\in\hat{\mathcal{Z}}^{\text{train}}\) and all distinct \(B,B^{\prime}\in\mathcal{B}\), \(D_{B}\bm{v}_{\pi(B^{\prime})}(\bm{z})=0\).

**Step 2 - Linking object-specific decoders.** We now show that, for all \(B\in\mathcal{B}\), \(\hat{\bm{f}}^{(B)}(\bm{z}_{B})=\bm{f}^{(\pi(B))}(\bm{v}_{\pi(B)}(\bm{z}))+\bm {c}^{(B)}\) for all \(\bm{z}\in\hat{\mathcal{Z}}^{\text{train}}\). To do this, we rewrite (50) as

\[D\hat{\bm{f}}^{(J)}(\bm{z}_{J})=\sum_{B\in\mathcal{B}}D\bm{f}^{(B)}(\bm{v}_{B} (\bm{z}))D_{J}\bm{v}_{B}(\bm{z})\,,\] (91)

but because \(B\neq\pi(J)\implies D_{J}\bm{v}_{B}(\bm{z})=0\) (block-permutation structure), we get

\[D\hat{\bm{f}}^{(J)}(\bm{z}_{J})=D\bm{f}^{(\pi(J))}(\bm{v}_{\pi(J)}(\bm{z}))D_{ J}\bm{v}_{\pi(J)}(\bm{z})\,.\] (92)

The above holds for all \(J\in\mathcal{B}\). We simply change \(J\) by \(B\) in the following equation.

\[D\hat{\bm{f}}^{(B)}(\bm{z}_{B})=D\bm{f}^{(\pi(B))}(\bm{v}_{\pi(B)}(\bm{z}))D_{ B}\bm{v}_{\pi(B)}(\bm{z})\,.\] (93)

Now notice that the r.h.s. of the above equation is equal to \(D(\bm{f}^{(\pi(B))}\circ\bm{v}_{\pi(B)})\). We can thus write

\[D\hat{\bm{f}}^{(B)}(\bm{z}_{B})=D(\bm{f}^{(\pi(B))}\circ\bm{v}_{\pi(B)})(\bm{z} )\,,\text{for all }\bm{z}\in\hat{\mathcal{Z}}^{\text{train}}\,.\] (94)

Now choose distinct \(\bm{z},\bm{z}^{0}\in\hat{\mathcal{Z}}^{\text{train}}\). Since \(\mathcal{Z}^{\text{train}}\) is path-connected, \(\hat{\mathcal{Z}}^{\text{train}}\) also is since they are diffeomorphic. Hence, there exists a continuously differentiable function \(\bm{\phi}:[0,1]\to\hat{\mathcal{Z}}^{\text{train}}\) such that \(\bm{\phi}(0)=\bm{z}^{0}\) and \(\bm{\phi}(1)=\bm{z}\). We can now use (94) together with the gradient theorem, a.k.a. the fundamental theorem of calculus for line integrals, to show the following

\[\int_{0}^{1}D\hat{\bm{f}}^{(B)}(\bm{\phi}_{B}(\bm{z}))\cdot\bm{ \phi}_{B}(t)dt =\int_{0}^{1}D(\bm{f}^{(\pi(B))}\circ\bm{v}_{\pi(B)})(\bm{\phi}( \bm{z}))\cdot\bm{\phi}(t)dt\] (95) \[\hat{\bm{f}}^{(B)}(\bm{z}_{B})-\hat{\bm{f}}^{(B)}(\bm{z}_{B}^{0}) =\bm{f}^{(\pi(B))}\circ\bm{v}_{\pi(B)}(\bm{z})-\bm{f}^{(\pi(B))} \circ\bm{v}_{\pi(B)}(\bm{z}^{0})\] (96) \[\hat{\bm{f}}^{(B)}(\bm{z}_{B}) =\bm{f}^{(\pi(B))}\circ\bm{v}_{\pi(B)}(\bm{z})+(\underbrace{\hat{ \bm{f}}^{(B)}(\bm{z}_{B}^{0})-\bm{f}^{(\pi(B))}\circ\bm{v}_{\pi(B)}(\bm{z}^{0}) }_{\text{constant in }\bm{z}})\] (97) \[\hat{\bm{f}}^{(B)}(\bm{z}_{B}) =\bm{f}^{(\pi(B))}\circ\bm{v}_{\pi(B)}(\bm{z})+\bm{c}^{(B)}\,,\] (98)

which holds for all \(\bm{z}\in\hat{\mathcal{Z}}^{\text{train}}\).

We now show that \(\sum_{B\in\mathcal{B}}\bm{c}^{(B)}=0\). Take some \(\bm{z}^{0}\in\hat{\mathcal{Z}}^{\text{train}}\). Equations (49) & (98) tell us that

\[\sum_{B\in\mathcal{B}}\bm{f}^{(B)}(\bm{v}_{B}(\bm{z}^{0})) =\sum_{B\in\mathcal{B}}\hat{\bm{f}}^{(B)}(\bm{z}_{B}^{0})\] (99) \[=\sum_{B\in\mathcal{B}}\bm{f}^{(\pi(B))}(\bm{v}_{\pi(B)}(\bm{z}^{0}) )+\sum_{B\in\mathcal{B}}\bm{c}^{(B)}\] (100) \[=\sum_{B\in\mathcal{B}}\bm{f}^{(B)}(\bm{v}_{B}(\bm{z}^{0}))+\sum_{B \in\mathcal{B}}\bm{c}^{(B)}\] (101) \[\implies 0 =\sum_{B\in\mathcal{B}}\bm{c}^{(B)}\] (102)

**Step 3 - From local to global disentanglement.** By assumption, the functions \(\bm{f}^{(B)}:\mathcal{Z}_{B}^{\text{train}}\to\mathbb{R}^{d_{x}}\) are injective. This will allow us to show that \(\bm{v}_{\pi(B)}(\bm{z})\) depends only on \(\bm{z}_{B}\). We proceed by contradiction. Suppose there exists \((\bm{z}_{B},\bm{z}_{B^{c}})\in\hat{\mathcal{Z}}^{\text{train}}\) and \(\bm{z}_{B^{c}}^{0}\) such that \((\bm{z}_{B},\bm{z}_{B^{c}}^{0})\in\hat{\mathcal{Z}}^{\text{train}}\) and \(\bm{v}_{\pi(B)}(\bm{z}_{B},\bm{z}_{B^{c}})\neq\bm{v}_{\pi(B)}(\bm{z}_{B},\bm{z}_ {B^{c}}^{0})\). This means

\[\bm{f}^{(\pi(B))}\circ\bm{v}_{\pi(B)}(\bm{z}_{B},\bm{z}_{B^{c}})+ \bm{c}^{(B)}=\hat{\bm{f}}^{(B)}(\bm{z}_{B})=\bm{f}^{(\pi(B))}\circ\bm{v}_{\pi(B )}(\bm{z}_{B},\bm{z}_{B^{c}}^{0})+\bm{c}^{(B)}\] \[\bm{f}^{(\pi(B))}(\bm{v}_{\pi(B)}(\bm{z}_{B},\bm{z}_{B}))=\bm{f}^{ (\pi(B))}(\bm{v}_{\pi(B)}(\bm{z}_{B},\bm{z}_{B}^{0}))\]

which is a contradiction with the fact that \(\bm{f}^{(\pi(B))}\) is injective. Hence, \(\bm{v}_{\pi(B)}(\bm{z})\) depends only on \(\bm{z}_{B}\). We also get an explicit form for \(\bm{v}_{\pi(B)}\):

\[(\bm{f}^{\pi(B)})^{-1}(\hat{\bm{f}}^{(B)}(\bm{z}_{B})-\bm{c}^{(B)})=\bm{v}_{ \pi(B)}(\bm{z})\text{ for all }\bm{z}\in\mathcal{Z}^{\text{train}}\,.\] (103)

We define the map \(\bar{\bm{v}}_{\pi(B)}(\bm{z}_{B}):=(\bm{f}^{\pi(B)})^{-1}(\hat{\bm{f}}^{(B)}( \bm{z}_{B})-\bm{c}^{(B)})\) which is from \(\hat{\mathcal{Z}}_{B}^{\text{train}}\) to \(\mathcal{Z}_{\pi(B)}^{\text{train}}\). This allows us to rewrite (98) as

\[\hat{\bm{f}}^{(B)}(\bm{z}_{B})=\bm{f}^{(\pi(B))}\circ\bar{\bm{v}}_{\pi(B)}(\bm {z}_{B})+\bm{c}^{(B)}\,,\text{ for all }\bm{z}_{B}\in\mathcal{Z}_{B}^{\text{train}}\,.\] (104)

Because \(\hat{\bm{f}}^{(B)}\) is also injective, we must have that \(\bar{\bm{v}}_{\pi(B)}:\hat{\mathcal{Z}}_{B}^{\text{train}}\to\mathcal{Z}_{\pi( B)}^{\text{train}}\) is injective as well.

We now show that \(\bar{\bm{v}}_{\pi(B)}\) is surjective. Choose some \(\bm{z}_{\pi(B)}\in\mathcal{Z}_{\pi(B)}^{\text{train}}\). We can always find \(\bm{z}_{\pi(B)^{c}}\) such that \((\bm{z}_{\pi(B)},\bm{z}_{\pi(B)^{c}})\in\mathcal{Z}^{\text{train}}\). Because \(\bm{v}:\hat{\mathcal{Z}}^{\text{train}}\to\mathcal{Z}^{\text{train}}\) is surjective (it is a diffeomorphism), there exists a \(\bm{z}^{0}\in\hat{\mathcal{Z}}^{\text{train}}\) such that \(\bm{v}(\bm{z}^{0})=(\bm{z}_{\pi(B)},\bm{z}_{\pi(B)^{c}})\). By (103), we have that

\[\bar{\bm{v}}_{\pi(B)}(\bm{z}_{B}^{0})=\bm{v}_{\pi(B)}(\bm{z}^{0})\,.\] (105)

which means \(\bar{\bm{v}}_{\pi(B)}(\bm{z}_{B}^{0})=\bm{z}_{\pi(B)}\).

We thus have that \(\bar{\bm{v}}_{\pi(B)}\) is bijective. It is a diffeomorphism because

\[\det D\bar{\bm{v}}_{\pi(B)}(\bm{z}_{B})=\det D_{B}\bm{v}_{\pi(B)}(\bm{z})\neq 0 \ \forall\bm{z}\in\hat{\mathcal{Z}}^{\text{train}}\] (106)

where the first equality holds by (103) and the second holds because \(\bm{v}\) is a diffeomorphism and has block-permutation structure, which means it has a nonzero determinant everywhere on \(\hat{\mathcal{Z}}^{\text{train}}\) and is equal to the product of the determinants of its blocks, which implies each block \(D_{B}\bm{v}_{\pi(B)}\) must have nonzero determinant everywhere.

Since \(\bar{\bm{v}}_{\pi(B)}:\hat{\mathcal{Z}}_{B}^{\text{train}}\to\mathcal{Z}_{\pi( B)}^{\text{train}}\) bijective and has invertible Jacobian everywhere, it must be a diffeomorphism. 

### Injectivity of object-specific decoders v.s. injectivity of their sum

We want to explore the relationship between the injectivity of individual object-specific decoders \(\bm{f}^{(B)}\) and the injectivity of their sum, i.e. \(\sum_{B\in B}\bm{f}^{(B)}\).

We first show the simple fact that having each \(\bm{f}^{(B)}\) injective is not sufficient to have \(\sum_{B\in\mathcal{B}}\bm{f}^{(B)}\) injective. Take \(\bm{f}^{(B)}(\bm{z}_{B})=\bm{W}^{(B)}\bm{z}_{B}\) where \(\bm{W}^{(B)}\in\mathbb{R}^{d_{x}\times|B|}\) has full column-rank for all \(B\in\mathcal{B}\). We have that

\[\sum_{B\in\mathcal{B}}\bm{f}^{(B)}(\bm{z}_{B})=\sum_{B\in\mathcal{B}}\bm{W}^{(B)} \bm{z}_{B}=[\bm{W}^{(B_{1})}\ \cdots\ \bm{W}^{(B_{\ell})}]\bm{z}\,\] (107)

where it is clear that the matrix \([\bm{W}^{(B_{1})}\ \cdots\ \bm{W}^{(B_{\ell})}]\in\mathbb{R}^{d_{x}\times d_{z}}\) is not necessarily injective even if each \(\bm{W}^{(B)}\) is. This is the case, for instance, if all \(\bm{W}^{(B)}\) have the same image.

We now provide conditions such that \(\sum_{B\in\mathcal{B}}\bm{f}^{(B)}\) injective implies each \(\bm{f}^{(B)}\) injective. We start with a simple lemma:

**Lemma 12**.: _If \(g\circ h\) is injective, then \(h\) is injective._

Proof.: By contradiction, assume that \(h\) is not injective. Then, there exists distinct \(x_{1},x_{2}\in\text{Dom}(h)\) such that \(h(x_{1})=h(x_{2})\). This implies \(g\circ h(x_{1})=g\circ h(x_{2})\), which violates injectivity of \(g\circ h\)The following Lemma provides a condition on the domain of the function \(\sum_{B\in\mathcal{B}}\bm{f}^{(B)}\), \(\mathcal{Z}^{\text{train}}\), so that its injectivity implies injectivity of the functions \(\bm{f}^{(B)}\).

**Lemma 13**.: _Assume that, for all \(B\in\mathcal{B}\) and for all distinct \(\bm{z}_{B},\bm{z}_{B}^{\prime}\in\mathcal{Z}_{B}^{\text{train}}\), there exists \(\bm{z}_{B^{c}}\) such that \((\bm{z}_{B},\bm{z}_{B^{c}}),(\bm{z}_{B}^{\prime},\bm{z}_{B^{c}})\in\mathcal{Z}^ {\text{train}}\). Then, whenever \(\sum_{B\in\mathcal{B}}\bm{f}^{(B)}\) is injective, each \(\bm{f}^{(B)}\) must be injective._

Proof.: Notice that \(\bm{f}(\bm{z}):=\sum_{B\in\mathcal{B}}\bm{f}^{(B)}(\bm{z}_{B})\) can be written as \(\bm{f}:=\text{SumBlocks}\circ\bar{\bm{f}}(\bm{z})\) where

\[\bar{\bm{f}}(\bm{z}):=\begin{bmatrix}\bm{f}^{(B_{1})}(\bm{z}_{B_{1}})\\ \vdots\\ \bm{f}^{(B_{\ell})}(\bm{z}_{B_{\ell}})\end{bmatrix}\,,\,\text{and SumBlocks}(\bm{x}^{(B_{1})},\ldots,\bm{x}^{(B_{\ell})}):= \sum_{B\in\mathcal{B}}\bm{x}^{(B)}\] (108)

Since \(\bm{f}\) is injective, by Lemma 12\(\bar{\bm{f}}\) must be injective.

We now show that each \(\bm{f}^{(B)}\) must also be injective. Take \(\bm{z}_{B},\bm{z}_{B}^{\prime}\in\mathcal{Z}_{B}^{\text{train}}\) such that \(\bm{f}^{(B)}(\bm{z}_{B})=\bm{f}^{(B)}(\bm{z}_{B}^{\prime})\). By assumption, we know there exists a \(\bm{z}_{B^{c}}\) s.t. \((\bm{z}_{B},\bm{z}_{B^{c}})\) and \((\bm{z}_{B}^{\prime},\bm{z}_{B^{c}})\) are in \(\mathcal{Z}^{\text{train}}\). By construction, we have that \(\bar{\bm{f}}((\bm{z}_{B},\bm{z}_{B^{c}}))=\bar{\bm{f}}((\bm{z}_{B}^{\prime}, \bm{z}_{B^{c}}))\). By injectivity of \(\bar{\bm{f}}\), we have that \((\bm{z}_{B},\bm{z}_{B^{c}})\neq(\bm{z}_{B}^{\prime},\bm{z}_{B^{c}})\), which implies \(\bm{z}_{B}\neq\bm{z}_{B}^{\prime}\), i.e. \(\bm{f}^{(B)}\) is injective. 

### Proof of Corollary 3

**Corollary 3** (Cartesian-product extrapolation).: _Suppose the assumptions of Theorem 2 holds. Then,_

\[\text{for all }\bm{z}\in\text{CPE}_{\mathcal{B}}(\hat{\mathcal{Z}}^{\text{ train}})\,,\,\sum_{B\in\mathcal{B}}\hat{\bm{f}}^{(B)}(\bm{z}_{B})=\sum_{B\in \mathcal{B}}\bm{f}^{(\pi(B))}(\bar{\bm{v}}_{\pi(B)}(\bm{z}_{B}))\,.\] (11)

_Furthermore, if \(\text{CPE}_{\mathcal{B}}(\mathcal{Z}^{\text{train}})\subseteq\mathcal{Z}^{ \text{test}}\), then \(\hat{\bm{f}}(\text{CPE}_{\mathcal{B}}(\hat{\mathcal{Z}}^{\text{train}})) \subseteq\bm{f}(\mathcal{Z}^{\text{test}})\)._

Proof.: Pick \(\bm{z}\in\text{CPE}(\hat{\mathcal{Z}}^{\text{train}})\). By definition, this means that, for all \(B\in\mathcal{B}\), \(\bm{z}_{B}\in\hat{\mathcal{Z}}_{B}^{\text{train}}\). We thus have that, for all \(B\in\mathcal{B}\),

\[\hat{\bm{f}}^{(B)}(\bm{z}_{B})=\bm{f}^{(\pi(B))}\circ\bar{\bm{v}}_{\pi(B)}(\bm {z}_{B})+\bm{c}^{(B)}\,.\] (109)

We can thus sum over \(B\) to obtain

\[\sum_{B\in\mathcal{B}}\hat{\bm{f}}^{(B)}(\bm{z}_{B})=\sum_{B\in\mathcal{B}} \bm{f}^{(\pi(B))}\circ\bar{\bm{v}}_{\pi(B)}(\bm{z}_{B})+\underbrace{\sum_{B\in \mathcal{B}}\bm{c}^{(B)}}_{=0}\,.\] (110)

Since \(\bm{z}\in\text{CPE}(\hat{\mathcal{Z}}^{\text{train}})\) was arbitrary, we have

\[\text{for all }\bm{z}\in\text{CPE}(\hat{\mathcal{Z}}^{\text{ train}}),\,\sum_{B\in\mathcal{B}}\hat{\bm{f}}^{(B)}(\bm{z}_{B})=\sum_{B\in\mathcal{B}}\bm{f}^{( \pi(B))}\circ\bar{\bm{v}}_{\pi(B)}(\bm{z}_{B})\] (111) \[\hat{\bm{f}}(\bm{z})=\bm{f}\circ\bar{\bm{v}}(\bm{z})\,,\] (112)

where \(\bar{\bm{v}}:\text{CPE}_{\mathcal{B}}(\hat{\mathcal{Z}}^{\text{train}})\to\text {CPE}_{\mathcal{B}}(\mathcal{Z}^{\text{train}})\) is defined as

\[\bar{\bm{v}}(\bm{z}):=\begin{bmatrix}\bar{\bm{v}}_{B_{1}}(\bm{z}_{\pi^{-1}(B_{1 })})\\ \vdots\\ \bar{\bm{v}}_{B_{\ell}}(\bm{z}_{\pi^{-1}(B_{\ell})})\end{bmatrix}\,,\] (113)

The map \(\bar{\bm{v}}\) is a diffeomorphism since each \(\bar{\bm{v}}_{\pi(B)}\) is a diffeomorphism from \(\hat{\mathcal{Z}}_{B}^{\text{train}}\) to \(\mathcal{Z}_{\pi(B)}^{\text{train}}\).

By (112) we get

\[\hat{\bm{f}}(\text{CPE}_{\mathcal{B}}(\hat{\mathcal{Z}}^{\text{train}}))=\bm{f} \circ\bar{\bm{v}}(\text{CPE}_{\mathcal{B}}(\hat{\mathcal{Z}}^{\text{train}}))\,,\] (114)

and since the map \(\bar{\bm{v}}\) is surjective we have \(\bar{\bm{v}}(\text{CPE}_{\mathcal{B}}(\hat{\mathcal{Z}}^{\text{train}}))=\text{ CPE}_{\mathcal{B}}(\mathcal{Z}^{ \text{train}})\) and thus

\[\hat{\bm{f}}(\text{CPE}_{\mathcal{B}}(\hat{\mathcal{Z}}^{\text{train}}))=\bm{f}( \text{CPE}_{\mathcal{B}}(\mathcal{Z}^{\text{train}}))\,.\] (115)

Hence if \(\text{CPE}_{\mathcal{B}}(\mathcal{Z}^{\text{train}})\subseteq\mathcal{Z}^{\text{ test}}\), then \(\bm{f}(\text{CPE}_{\mathcal{B}}(\mathcal{Z}^{\text{train}}))\subseteq\bm{f}(\mathcal{Z}^ {\text{test}})\). 

### Will all extrapolated images make sense?

Here is a minimal example where the assumption \(\text{CPE}_{\mathcal{B}}(\mathcal{Z}^{\text{train}})\not\subseteq\mathcal{Z}^{ \text{test}}\) is violated.

**Example 10** (Violation of \(\text{CPE}_{\mathcal{B}}(\mathcal{Z}^{\text{train}})\not\subseteq\mathcal{Z}^{ \text{test}}\)).: _Imagine \(\bm{z}=(\bm{z}_{1},\bm{z}_{2})\) where \(\bm{z}_{1}\) and \(\bm{z}_{2}\) are the \(x\)-positions of two distinct balls. It does not make sense to have two balls occupying the same location in space and thus whenever \(\bm{z}_{1}=\bm{z}_{2}\) we have \((\bm{z}_{1},\bm{z}_{2})\not\in\mathcal{Z}^{\text{test}}\). But if \((1,2)\) and \((2,1)\) are both in \(\mathcal{Z}^{\text{train}}\), it implies that \((1,1)\) and \((2,2)\) are in \(\text{CPE}(\mathcal{Z}^{\text{train}})\), which is a violation of \(\text{CPE}_{\mathcal{B}}(\mathcal{Z}^{\text{train}})\subseteq\mathcal{Z}^{ \text{test}}\)._

### Additive decoders cannot model occlusion

We now explain why additive decoders cannot model occlusion. Occlusion occurs when an object is partially hidden behind another one. Intuitively, the issue is the following: Consider two images consisting of two objects, A and B (each image shows both objects). In both images, the position of object A is the same and in exactly one of the images, object B partially occludes object A. Since the position of object \(A\) did not change, its corresponding latent block \(\bm{z}_{A}\) is also unchanged between both images. However, the pixels occupied by object A do change between both images because of occlusion. The issue is that, because of additivity, \(\bm{z}_{A}\) and \(\bm{z}_{B}\) cannot interact to make some pixels that belonged to object A "disappear" to be replaced by pixels of object B. In practice, object-centric representation learning methods rely a masking mechanism which allows interactions between \(\bm{z}_{A}\) and \(\bm{z}_{B}\) (See Equation 1 in Section 2). This highlights the importance of studying this class of decoders in future work.

## Appendix B Experiments

### Training Details

Loss Function.We use the standard reconstruction objective of mean squared error loss between the ground truth data and the reconstructed/generated data.

Hyperparameters.For both the ScalarLatents and the BlockLatents dataset, we used the Adam optimizer with the hyperparameters defined below. Note that we maintain consistent hyperparameters across both the Additive decoder and the Non-Additive decoder method.

_ScalarLatents Dataset._

* Batch Size: \(64\)
* Learning Rate: \(1\times 10^{-3}\)
* Weight Decay: \(5\times 10^{-4}\)
* Total Epochs: \(4000\)

_BlockLatents Dataset._

* Batch Size: \(1024\)
* Learning Rate: \(1\times 10^{-3}\)
* Weight Decay: \(5\times 10^{-4}\)
* Total Epochs: \(6000\)

Model Architecture.We use the following architectures for Encoder and Decoder across both the datasets (ScalarLatents, BlockLatents). Note that for the ScalarLatents dataset we train with latent dimension \(d_{z}=2\), and for the BlockLatents dataset we train with latent dimension \(d_{z}=4\), which corresponds to the dimensionalities of the ground-truth data generating process for both datasets.

_Encoder Architecture:_

* RestNet-18 Architecture till the penultimate layer (\(512\) dimensional feature output)
* Stack of 5 fully-connected layer blocks, with each block consisting of Linear Layer ( dimensions: \(512\times 512\)), Batch Normalization layer, and Leaky ReLU activation (negative slope: \(0.01\)).

* Final Linear Layer (dimension: \(512\times d_{z}\)) followed by Batch Normalization Layer to output the latent representation.

_Decoder Architecture (Non-additive):_

* Fully connected layer block with input as latent representation, consisting of Linear Layer (dimension: \(d_{z}\times 512\)), Batch Normalization layer, and Leaky ReLU activation (negative slope: \(0.01\)).
* Stack of 5 fully-connected layer blocks, with each block consisting of Linear Layer ( dimensions: \(512\times 512\)), Batch Normalization layer, and Leaky ReLU activation (negative slope: \(0.01\)).
* Series of DeConvolutional layers, where each DeConvolutional layer is foll\[\bm{z}_{4}\sim\begin{cases}\text{Uniform}(0.5,1)&\text{if }1.25\times(\bm{z}_{1}^{ 2}+\bm{z}_{2}^{2})\geq 1.0\\ \text{Uniform}(0,0.5)&\text{if }1.25\times(\bm{z}_{1}^{2}+\bm{z}_{2}^{2})<1.0 \end{cases}\]

Intuitively, this means the second ball will be placed in either the top-left or the bottom-right quadrant based on the position of the first ball. We also exclude from the dataset the images presenting occlusion.

Note that our dependent BlockLatent setup is same as the non-linear SCM case from Ahuja et al. [3].

We use \(50k\) samples for both the train and the test dataset, along with \(12.5k\) samples (\(25\%\) of the train sample size) for the validation dataset.

Disconnected Support Dataset.For this dataset, we have setup similar to the **ScalarLatents** dataset; we fix the x-coordinates of both balls to \(0.25\) and \(0.75\) and only vary the y-coordinates so that \(\bm{z}\in\mathbb{R}^{2}\). We sample the y-coordinate of the first ball (\(\bm{z}_{1}\)) from Uniform(0, 1). Then we sample the y-coordinate of the second ball (\(\bm{z}_{2}\)) from either of the following continuous uniform distribution with equal probability; Uniform(0, 0.25) and Uniform(0.75, 1). This leads to a disconnected support given by \(\mathcal{Z}^{\text{train}}:=[0,1]\times[0,1]\setminus[0.25,0.75]\times[0.25,0. 75]\).

We use \(50k\) samples for the test dataset, while we use \(20k\) samples for the train dataset along with \(5k\) samples (\(25\%\) of the train sample size) for the validation dataset.

### Evaluation Metrics

Recall that, to evaluate disentanglement, we compute a matrix of scores \((s_{B,B^{\prime}})\in\mathbb{R}^{\ell\times\ell}\) where \(\ell\) is the number of blocks in \(\mathcal{B}\) and \(s_{B,B^{\prime}}\) is a score measuring how well we can predict the ground-truth block \(\bm{z}_{B}\) from the learned latent block \(\hat{\bm{z}}_{B^{\prime}}=\hat{\bm{g}}_{B^{\prime}}(\bm{x})\) outputted by the encoder. The final Latent Matching Score (LMS) is computed as \(\text{LMS}=\operatorname*{arg\,max}_{\tau\in\mathfrak{S}_{\mathcal{B}}}\frac {1}{\tau}\sum_{B\in\mathcal{B}}s_{B,\pi(B)}\), where \(\mathfrak{S}_{\mathcal{B}}\) is the set of permutations respecting \(\mathcal{B}\) (Definition 2). These scores are always computed on the test set.

Metric \(\text{LMS}_{\text{Spar}}\):As mentioned in the main paper, this metric is used for the **ScalarLatents** dataset where each block is 1-dimensional. Hence, this metric is almost the same as the mean correlation coefficient (MCC), which is widely used in the nonlinear ICA literature [30, 31, 33, 36, 42], with the only difference that we use Spearman correlation instead of Pearson correlation as a score \(s_{B,B^{\prime}}\). The Spearman correlation can capture nonlinear monotonous relations, unlike Pearson which can only capture linear dependencies. We favor Spearman over Pearson because our identifiability result (Theorem 2) guarantees we can recover the latents only up to permutation and element-wise invertible transformations, which can be nonlinear.

Metric \(\text{LMS}_{\text{tree}}\):This metric is used for the **BlockLatents** dataset. For this metric, we take \(s_{B,B^{\prime}}\) to be the \(R^{2}\) score of a Regression Tree with maximal depth of \(10\). For this, we used the class sklearn.tree.DecisionTreeRegressor from the sklearn library. We learn the parameters of the Decision Tree using the train dataset and then use it to evaluate \(\text{LMS}_{\text{tree}}\) metric on the test dataset. For the additive decoder, it is easy to compute this metric since the additive structure already gives a natural partition \(\mathcal{B}\) which matches the ground-truth. However, for the non-additive decoder, there is no natural partition and thus we cannot compute \(\text{LMS}_{\text{tree}}\) directly. To go around this problem, for the non-additive decoder, we compute \(\text{LMS}_{\text{tree}}\) for all possible partitions of \(d_{z}\) latent variables into blocks of size \(|B|=2\) (assuming all blocks have the same dimension), and report the best \(\text{LMS}_{\text{tree}}\). This procedure is tractable in our experiments due to the small dimensionality of the problem we consider.

### Boxplots for main experiments (Table 1)

Since the standard error in the main results (Table 1) was high, we provide boxplots in Figures 9 & 10 to have a better visibility on what is causing this. We observe that the high standard error for the Additive approach was due to bad performance for a few bad random initializations for the ScalarLatents dataset; while we have nearly perfect latent identification for the others. Figure 14e shows the latent space learned by the worst case seed, which somehow learned a disconnected support even if the ground-truth support was connected. Similarly, for the case of Independent BlockLatents, there are only a couple of bad random initializations and the rest of the cases have perfect identification.

### Additional Results: BlockLatents Dataset

To get a qualitative understanding of latent identification in the BlockLatents dataset, we plot the response of each predicted latent as we change a particular ground-truth latent factor. We describe the following cases of changing the ground-truth latents:

* **Ball 1 moving along x-axis:** We sample 10 equally spaced points for \(\bm{z}_{1}\) from \([0,1]\); while keeping other latents fixed as follows: \(\bm{z}_{2}=0.25,\bm{z}_{3}=0.50,\bm{z}_{4}=0.75\). We will never have occlusion since the balls are separated along the y-axis \(\bm{z}_{4}-\bm{z}_{2}>0\).
* **Ball 2 moving along x-axis:** We sample 10 equally spaced points for \(\bm{z}_{3}\) from \([0,1]\); while keeping other latents fixed as follows: \(\bm{z}_{1}=0.50,\bm{z}_{2}=0.25,\bm{z}_{4}=0.75\). We will never have occlusion since the balls are separated along the y-axis \(\bm{z}_{4}-\bm{z}_{2}>0\).
* **Ball 1 moving along y-axis:** We sample 10 equally spaced points for \(\bm{z}_{2}\) from \([0,1]\); while keeping other latents fixed as follows: \(\bm{z}_{1}=0.25,\bm{z}_{3}=0.75,\bm{z}_{4}=0.50\). We will never have occlusion since the balls are separated along the x-axis \(\bm{z}_{3}-\bm{z}_{1}>0\).
* **Ball 2 moving along y-axis:** We sample 10 equally spaced points for \(\bm{z}_{4}\) from \([0,1]\); while keeping other latents fixed as follows: \(\bm{z}_{1}=0.25,\bm{z}_{2}=0.50,\bm{z}_{3}=0.75\). We will never have occlusion since the balls are separated along the x-axis \(\bm{z}_{3}-\bm{z}_{1}>0\).

Figure 10: Reconstruction mean squared error (MSE) (\(\downarrow\)) and Latent Matching Score (LMS) (\(\uparrow\)) for 10 different initializations for **BlockLatents** dataset.

Figure 9: Reconstruction mean squared error (MSE) (\(\downarrow\)) and Latent Matching Score (LMS) (\(\uparrow\)) over 10 different random initializations for **ScalarLatents** dataset.

Figure 5 in the main paper presents the latent responses plot for the median \(\text{LMS}_{\text{tree}}\) case among random initializations. In Figure 11, we provide the results for the case of best and the worst \(\text{LMS}_{\text{tree}}\) among random seeds. We find that Additive Decoder fails for only for the worst case random seed, while Non-Additive Decoder fails for all the cases.

Additionally, we provide the object-specific reconstructions for the Additive Decoder in Figure 12. This helps us better understand the failure of Additive Decoder for the worst case random seed (Figure 12c), where the issue arises due to bad reconstruction error.

Figure 11: Latent responses for the cases with the **best/median/worst**\(\text{LMS}_{\text{Tree}}\) among runs performed on the **BlockLatent** dataset with independent latents. In each plot, we report the latent factors predicted from multiple images where one ball moves along only one axis at a time.

### Disconnected Support Experiments

Since path-connected latent support is an important assumption for latent identification with additive decoders (Theorem 2), we provide results for the case where the assumption is not satisfied. We experiment with the **Disconnected Support** dataset (Section B.2) and find that we obtain much worse \(\text{LMS}_{\text{Spear}}\) as compared to the case of training with L-shaped support in the **ScalarLatents** dataset. Over 10 different random initializations, we find mean \(\text{LMS}_{\text{Spear}}\) performance of \(69.5\) with standard error of \(6.69\).

For better qualitative understanding, we provide visualization of the latent support and the extrapolated images for the median \(\text{LMS}_{\text{Spear}}\) among 10 random seeds in Figure 13. Somewhat surprisingly, the representation appears to be aligned in the sense that the first predicted latent corresponds to the blue ball while the second predicted latent correspond to the red ball. Also surprisingly, extrapolation

Figure 12: Object-specific renderings with the **best/median/worst**\(\text{LMS}_{\text{tree}}\) among runs performed on the **BlockLatents** dataset with independent latents. In each plot, the first row is the original image, the second row is the reconstruction and the third and fourth rows are the output of the object-specific decoders. In the best and median cases, each object-specific decoder corresponds to one and only one object, e.g. the third row of the best case always corresponds to the red ball. However, in the worst case, there are issues with reconstruction as only one of the balls is generated. Note that the visual artefacts are due to the additive constant indeterminacy we saw in Theorem 2, which cancel each other as is suggested by the absence of artefacts in the reconstruction.

[MISSING_PAGE_FAIL:38]

Figure 14: Figure (a, c, e) shows the learned latent space, \(\hat{\mathcal{Z}}^{\text{train}}\), and the corresponding reconstructed images of the additive decoder with the **best/median/worst**\(\text{LMS}_{\text{Spar}}\) among runs performed on the **ScalarLatents** dataset. Figure (b, d, f) shows the same thing for the non-additive decoder. The red dots correspond to latent factors used to generate the images and the yellow square highlights extrapolated images.