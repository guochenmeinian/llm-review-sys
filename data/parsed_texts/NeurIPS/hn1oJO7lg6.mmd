# Computing Approximate \(\ell_{p}\) Sensitivities

Swati Padmanabhan

Massachusetts Institute of Technology

pswt@mit.edu

&David P. Woodruff

Carnegie Mellon University

dwoodruf@cs.cmu.edu

&Qiuyi (Richard) Zhang

Google Research

qiuyiz@google.com

###### Abstract

Recent works in dimensionality reduction for regression tasks have introduced the notion of sensitivity, an estimate of the importance of a specific datapoint in a dataset, offering provable guarantees on the quality of the approximation after removing low-sensitivity datapoints via subsampling. However, fast algorithms for approximating sensitivities, which we show is equivalent to approximate regression, are known for only the \(\ell_{2}\) setting, in which they are popularly termed leverage scores. In this work, we provide the first efficient algorithms for approximating \(\ell_{p}\) sensitivities and related summary statistics of a given matrix. In particular, for a given \(n\times d\) matrix, we compute \(\alpha\)-approximation to its \(\ell_{1}\) sensitivities at the cost of \(O(n/\alpha)\) sensitivity computations. For estimating the total \(\ell_{p}\) sensitivity (i.e. the sum of \(\ell_{p}\) sensitivities), we provide an algorithm based on importance sampling of \(\ell_{p}\) Lewis weights, which computes a constant factor approximation to the total sensitivity at the cost of roughly \(O(\sqrt{d})\) sensitivity computations. Furthermore, we estimate the maximum \(\ell_{1}\) sensitivity, up to a \(\sqrt{d}\) factor, using \(O(d)\) sensitivity computations. We generalizeall these results to \(\ell_{p}\) norms for \(p>1\). Lastly, we experimentally show that for a wide class of matrices in real-world datasets, the total sensitivity can be quickly approximated and is significantly smaller than the theoretical prediction, demonstrating that real-world datasets have low intrinsic effective dimensionality.

## 1 Introduction

Many modern large-scale machine learning datasets comprise tall matrices \(\mathbf{A}\in\mathbb{R}^{n\times d}\) with \(d\) features and \(n\gg d\) training examples, often making it computationally expensive to run them through even basic algorithmic primitives. Therefore, many applications spanning dimension reduction, privacy, and fairness aim to estimate the importance of a given datapoint as a first step. Such importance estimates afford us a principled approach to focus our attention on a subset of only the most important examples.

In view of this benefit, several sampling approaches have been extensively developed in the machine learning literature. One of the simplest such methods prevalent in practice is uniform sampling. Prominent examples of this technique include variants of stochastic gradient descent methods [1, 2, 3, 4] such as [5, 6, 7, 8, 9, 10, 11, 12, 13, 14] developed for the "finite-sum minimization" problem

\[\text{minimize}_{\bm{\theta}\in\mathcal{X}}\quad\sum_{i=1}^{n}f_{i}(\bm{ \theta}).\] (1.1)

where, in the context of empirical risk minimization, each \(f_{i}:\mathcal{X}\mapsto\mathbb{R}_{\geq 0}\) in (1.1) measures the loss incurred by the \(i\)-th data point from the training set, and in generalized linear models, each \(f_{i}\) represents a link function applied to a linear predictor evaluated at the \(i\)-th data point. The aforementioned algorithms randomly sample a function \(f_{i}\) and make progress via gradient estimation techniques.

However, uniform sampling can lead to significant information loss when the number of important training examples is relatively small. Hence, _importance sampling_ methods that assign higher sampling probabilities to more important examples have emerged both in theory [15; 16] and practice [17; 18; 19; 20; 21]. One such technique is based on _sensitivity scores_, defined [15] for each coordinate \(i\in[n]\) as:

\[\boldsymbol{\sigma}_{i}\stackrel{{\mbox{\tiny def}}}{{=}} \max_{\mathbf{x}\in\mathcal{X}}\frac{f_{i}(\mathbf{x})}{\sum_{j=1}^{n}f_{j}( \mathbf{x})}.\] (1.2)

Sampling each \(f_{i}\) independently with probability \(p_{i}\propto\sigma_{i}\) and scaling the sampled row with \(1/p_{i}\) preserves the objective in (1.1) in expectation for every \(\mathbf{x}\in\mathcal{X}\). Further, one gets a \((1\pm\varepsilon)\)-approximation to this objective by sampling \(\widetilde{O}(\mathfrak{S}d\varepsilon^{-2})\) functions [22, Theorem \(1.1\)], where \(\mathfrak{S}=\sum_{i=1}^{n}\boldsymbol{\sigma}_{i}\) is called the _total sensitivity_, and \(d\) is an associated VC dimension.

Numerous advantages of sensitivity sampling over \(\ell_{p}\) Lewis weight sampling [23] were highlighted in the recent work [24], prominent among which is that a sampling matrix built using the \(\ell_{p}\) sensitivities of \(\mathbf{A}\) is significantly smaller than one built using its \(\ell_{p}\) Lewis weights in many important cases, e.g., when the total sensitivity \(\mathfrak{S}\) is small for \(p>2\) (which is seen in many structured matrices, such as combinations of Vandermonde, sparse, and low-rank matrices as studied in [25] as well as those in many naturally occuring datasets (cf. Section4)). Additionally, sensitivity sampling has found use in constructing \(\epsilon\)-approximators for shape-fitting functions, also called strong coresets [15; 26; 27; 28; 16], clustering [16; 30; 29], logistic regression [31; 32], and least squares regression [33; 34].

Despite their benefits, the computational burden of estimating these sensitivities has been known to be significant, requiring solving an expensive maximization problem for each datapoint, which is a limiting factor in their use [35]. A key exception is for \(f_{i}(\mathbf{x})=(\mathbf{a}_{i}^{\top}\mathbf{x})^{2}\), when the sensitivity score of \(f_{i}\) is exactly the leverage score of \(\mathbf{a}_{i}\), which can be computed quickly in \(O(\log(n))\) sensitivity calculations, as opposed to the naive \(O(n)\) sensitivity calculations (cf. Section2).

### Our contributions

In this work, we initiate a systematic study of algorithms for approximately computing the \(\ell_{p}\) sensitivities of a matrix, generally up to a constant factor. We first define \(\ell_{p}\) sensitivities:

**Definition 1.1**.: _[_15_]_ _Given a full-rank matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\) with \(n\geq d\) and a scalar \(p\in(0,\infty)\), let \(\mathbf{a}_{i}\) denote the \(i\)'th row of matrix \(\mathbf{A}\). Then the vector of \(\ell_{p}\) sensitivities of \(\mathbf{A}\) is defined as1 (cf. Section2 for the notation)_

Footnote 1: From hereon, for notational conciseness, we omit stating \(\mathbf{A}\mathbf{x}\neq\mathbf{0}\) in the problem constraint.

\[\boldsymbol{\sigma}_{p}(\mathbf{a}_{i})\stackrel{{\mbox{\tiny def }}}{{=}}\max_{\mathbf{x}\in\mathbb{R}^{d},\mathbf{A}\mathbf{x}\neq\mathbf{0}} \frac{\left|\mathbf{a}_{i}^{\top}\mathbf{x}\right|^{p}}{\left\|\mathbf{A} \mathbf{x}\right\|^{p}_{p}},\text{ for all }i\in[n],\] (1.3)

_and the total \(\ell_{p}\) sensitivity is defined as \(\mathfrak{S}_{p}(\mathbf{A})\stackrel{{\mbox{\tiny def}}}{{=}} \sum_{i\in[n]}\boldsymbol{\sigma}_{p}(\mathbf{a}_{i})\)._

These \(\ell_{p}\) sensitivities yield sampling probabilities for \(\ell_{p}\) regression, a canonical optimization problem that captures least squares regression, maximum flow, and linear programming, in addition to appearing in applications like low rank matrix approximation [36], sparse recovery [37], graph-based semi-supervised learning [38; 39; 40], and data clustering [41]. However, algorithms for approximating \(\ell_{p}\) sensitivities were known for only \(p=2\) (when they are called "leverage scores"). As is typical in large-scale machine learning, we assume \(n\gg d\). We now state our contributions, which are three-fold.

**(1) Approximation algorithms.** We design algorithms for three prototypical computational tasks associated with \(\ell_{p}\) sensitivities of a matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\). Specifically, for an approximation parameter \(\alpha>1\), we can simultaneously estimate all \(\ell_{p}\) sensitivities up to an additive error of \(O\left(\frac{\alpha^{p}}{n}\mathfrak{S}_{p}(\mathbf{A})\right)\) via \(O(n/\alpha)\) individual sensitivity calculations, reducing our runtime by sacrificing some accuracy. We limit most of our results in the main text to \(\ell_{1}\) sensitivities, but extend these to all \(p\geq 1\) in AppendixC. To state our results for the \(\ell_{p}\) setting, we use \(\mathbf{n}\mathbf{n}\mathbf{z}(\mathbf{A})\) to denote the number of non-zero entries of the matrix \(\mathbf{A}\), \(\omega\) for the matrix multiplication constant, and we introduce the following piece of notation.

**Definition 1.2** (Notation for \(\ell_{p}\) Results).: _We introduce the notation \(\mathsf{LP}(m,d,p)\) to denote the cost of approximating one \(\ell_{p}\) sensitivity of an \(m\times d\) matrix up to an accuracy of a given constant factor._

**Theorem 1.3** (**Estimating all sensitivities**; informal version of Theorem 3.1 and Theorem C.2).: _Given a matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\) and \(1\leq\alpha\ll n\), there exists an algorithm (Algorithm 1) which returns a vector \(\widetilde{\boldsymbol{\sigma}}\in\mathbb{R}^{n\times d}_{\geq 0}\) such that, with high probability, for each \(i\in[n]\), we have_

\[\boldsymbol{\sigma}_{1}(\mathbf{a}_{i})\leq\widetilde{\boldsymbol{\sigma}}_{i }\leq O(\boldsymbol{\sigma}_{1}(\mathbf{a}_{i})+\tfrac{\alpha}{n}\mathfrak{S}_ {1}(\mathbf{A})).\]

_Our algorithm's runtime is \(\widetilde{O}\left(\mathbf{nnz}(\mathbf{A})+\tfrac{n}{\alpha}\cdot d^{\omega}\right)\). More generally, for \(p\geq 1\), we can compute (via Algorithm 6) an estimate \(\widetilde{\boldsymbol{\sigma}}\in\mathbb{R}^{n\times d}_{\geq 0}\) satisfying \(\boldsymbol{\sigma}_{p}(\mathbf{a}_{i})\leq\widetilde{\boldsymbol{\sigma}}_{i }\leq O(\alpha^{p-1}\boldsymbol{\sigma}_{p}(\mathbf{a}_{i})+\tfrac{\alpha^{p} }{n}\mathfrak{S}_{p}(\mathbf{A}))\) with high probability for each \(i\in[n]\), at a cost of \(O\left(\mathbf{nnz}(\mathbf{A})+\tfrac{n}{\alpha}\cdot\mathsf{LP}(O(d^{\max(1,p/2)}),d,p)\right)\)._

Two closely related properties arising widely in algorithms research are the _total sensitivity_\(\mathfrak{S}_{1}(\mathbf{A})\) and the _maximum sensitivity_\(\|\boldsymbol{\sigma}_{1}(\mathbf{A})\|_{\infty}\). As stated earlier, one important feature of the total sensitivity is that it determines the total sample size needed for function approximation, thus making its efficient estimation crucial for fast \(\ell_{1}\) regression. Additionally, it was shown in [24] that the sample complexity of sensitivity sampling is much lower than that of Lewis weight sampling in many practical applications such as when the total sensitivity is small; therefore, an approximation of the total sensitivity can be used as a fast test for whether or not to proceed with sensitivity sampling (involving the costly task of calculating all sensitivities).

While the total sensitivity helps us characterize the whole dataset (as explained above), the _maximum sensitivity_ captures the importance of the most important datapoint and is used in, e.g., experiment design and in reweighting matrices for low coherence [42]. Additionally, it captures the maximum extent to which a datapoint can influence the objective function and is therefore used in differential privacy [43]. While one could naively estimate the total and maximum sensitivities by computing all \(n\) sensitivities using Theorem 1.3, we give faster algorithms that, strikingly, have no polynomial dependence on \(n\). In the assumed regime of \(n\gg d\), this is a significant runtime improvement.

**Theorem 1.4** (**Estimating the total sensitivity**; informal version of Theorem 3.5).: _Given a matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\), a scalar \(p\geq 1\), and a small constant \(\gamma<1\), there exists an algorithm (Algorithm 2), which returns a positive scalar \(\widehat{s}\) such that, with a probability of at least \(0.99\), we have_

\[\mathfrak{S}_{p}(\mathbf{A})\leq\widehat{s}\leq(1+O(\gamma))\mathfrak{S}_{p} (\mathbf{A}).\]

_Our algorithm's runtime is \(\widetilde{O}\left(\mathbf{nnz}(\mathbf{A})+\tfrac{1}{\gamma^{2}}\cdot d^{ \lfloor p/2-1\rfloor}\mathsf{LP}(O(d^{\max(1,p/2)},d,p)\right)\)._

In Algorithm 4 (presented in Appendix B.2), we give an alternate method to estimate \(\mathfrak{S}_{1}(\mathbf{A})\) based on recursive leverage score sampling, without \(\ell_{p}\) Lewis weight calculations, which we believe to be of independent interest. All of our algorithms utilize approximation properties of \(\ell_{p}\) Lewis weights and subspace embeddings and recent developments in fast computation of these quantities.

**Theorem 1.5** (**Estimating the maximum sensitivity**; informal version of Theorem 3.7 and Theorem C.3).: _Given a matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\), there exists an algorithm (Algorithm 3) which returns a scalar \(\widetilde{\boldsymbol{\sigma}}>0\) such that_

\[\|\boldsymbol{\sigma}_{1}(\mathbf{A})\|_{\infty}\leq\widetilde{\boldsymbol{ \sigma}}\leq O(\sqrt{d}\|\boldsymbol{\sigma}_{1}(\mathbf{A})\|_{\infty}).\]

_Our algorithm's runtime is \(\widetilde{O}(\mathbf{nnz}(\mathbf{A})+d^{\omega+1})\). Furthermore, via Algorithm 7, this result holds for all \(p\in[1,2]\) and for \(p>2\), this guarantee costs \(\widetilde{O}\left(\mathbf{nnz}(\mathbf{A})+d^{p/2}\cdot\mathsf{LP}(O(d^{p/2} ),d,p)\right)\)._

**(2) Hardness results.** In the other direction, while it is known that \(\ell_{p}\) sensitivity calculation reduces to \(\ell_{p}\) regression, we show the converse. Specifically, we establish hardness of computing \(\ell_{p}\) sensitivities in terms of the cost of solving multiple corresponding \(\ell_{p}\) regression problems, by showing that a \((1+\varepsilon)\)-approximation to \(\ell_{p}\)-sensitivities solves \(\ell_{p}\) multi-regression up to an accuracy factor of \(1+\varepsilon\).

**Theorem 1.6** (**Leave-One-Out \(\ell_{p}\) Multiregression Reduces to \(\ell_{p}\) Sensitivities**; informal2).: _Suppose that we are given an sensitivity approximation algorithm \(\boldsymbol{A}\), which for any matrix \(\mathbf{A}^{\prime}\in\mathbb{R}^{n^{\prime}\times d^{\prime}}\) and accuracy parameter \(\varepsilon^{\prime}\in(0,1)\), computes \((1\pm\varepsilon^{\prime})\boldsymbol{\sigma}_{p}(\mathbf{A}^{\prime})\) in time \(\mathcal{T}(n^{\prime},d^{\prime},\mathbf{nnz}(\mathbf{A}^{\prime}),\varepsilon^ {\prime})\). Given a matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\) with \(n\geq d\), let \(\text{OPT}_{i}:=\min_{\mathbf{y}\in\mathbb{R}^{d-1}}\|\mathbf{A}_{\cdot-i} \mathbf{y}+\mathbf{A}_{\cdot:i}\|_{p}^{p}\) and \(\mathbf{y}_{i}^{*}:=\arg\min_{\mathbf{y}\in\mathbb{R}^{d-1}}\|\mathbf{A}_{\cdot -i}\mathbf{y}+\mathbf{A}_{\cdot:i}\|_{p}\) for all the \(i\in[d]\). Then, there exists an algorithm that takes \(\mathbf{A}\in\mathbb{R}^{n\times d}\) and computes \((1\pm\varepsilon)\text{OPT}_{i}\) for all \(i\) in time \(\mathcal{T}(n+d,d,\mathbf{nnz}(\mathbf{A}),\varepsilon)\)._For \(p\neq 2\), approximating all \(\ell_{p}\) sensitivities to a high accuracy would therefore require improving \(\ell_{p}\) multi-regression algorithms with \(\Omega(d)\) instances, for which the current best high-accuracy algorithms take \(\text{poly}(d)\,\mathbf{nnz}(\mathbf{A})\) time [44; 45]. More concretely, our hardness results imply that in general one cannot compute all sensitivities as quickly as leverage scores unless there is a major breakthrough in multi-response regression. In order to show this, we introduce a reduction algorithm that efficiently regresses columns of \(\mathbf{A}\) against linear combinations of other columns of \(\mathbf{A}\), by simply adding a row to \(\mathbf{A}\) capturing the desired linear combination and then computing sensitivities, which will reveal the cost of the corresponding regression problem. We can solve \(\Theta(d)\) of these problems by augmenting the matrix to increase the rows and columns by at most a constant factor. We defer these details to Appendix D.

**(3) Numerical experiments.** We consolidate our theoretical contributions with a demonstration of the empirical advantage of our approximation algorithms over the naive approach of estimating these sensitivities using the UCI Machine Learning Dataset Repository [46] in Section4. We found that many datasets have extremely small total sensitivities; therefore, fast sensitivity approximation algorithms utilize this small intrinsic dimensionality of real-world data far better than Lewis weights sampling, with sampling bounds often a factor of \(2\)-\(5\)x better. We also found these sensitivities to be easy to approximate, with the accuracy-runtime tradeoff much better than our theory suggests, with up to \(40\)x faster in runtime. Lastly, we show that our algorithm for estimating the total sensitivity produces accurate estimates, up to small constant factors, with a runtime speedup of at least \(4\)x.

### Related work

Introduced in the pioneering work of [15], sensitivity sampling falls under the broad framework of "importance sampling". It has found use in constructing \(\epsilon\)-approximators of numerical integrands of several function classes in numerical analysis, statistics (e.g., in the context of VC dimension), and computer science (e.g., in coresets for clustering). The results from [15] were refined and extended by [16], with more advances in the context of shape-fitting problems in [26; 27; 28], clustering [16; 29; 30], logistic regression [31; 32], least squares regression [33; 47; 34], principal component analysis [48], reinforcement learning [49; 50], and pruning of deep neural networks [51; 52; 53]. Sampling algorithms for \(\ell_{p}\) subspace embeddings have also been extensively studied in functional analysis [54; 55; 56; 57; 58; 59] as well as in theoretical computer science [23; 60; 44; 45].

Another notable line of work in importance sampling uses Lewis weights to determine sampling probabilities. As we explain in Section2, Lewis weights are closely related to sensitivities. Many modern applications of Lewis weights in theoretical computer science we introduced in [23], which gave input sparsity time algorithms for approximating Lewis weights and used them to obtain fast algorithms for solving \(\ell_{p}\) regression. They have subsequently been used in row sampling algorithms for data pre-processing [33; 61; 62; 23; 60], computing dimension-free strong coresets for \(k\)-median and subspace approximation [63], and fast tensor factorization in the streaming model [64]. Lewis weights are also used for \(\ell_{1}\) regression in: [65] for stochastic gradient descent pre-conditioning, [66] for quantile regression, and [67] to provide algorithms for linear algebraic problems in the sliding window model. Lewis weights have also become widely used in convex geometry [68], randomized numerical linear algebra [23; 69; 64; 70; 42], and machine learning [71; 72; 73].

## 2 Notation and preliminaries

We use boldface uppercase and lowercase letters for matrices and vectors respectively. We denote the \(i^{\rm th}\) row vector of a matrix \(\mathbf{A}\) by \(\mathbf{a}_{i}\). Given a matrix \(\mathbf{M}\), we use \(\operatorname{Tr}(\mathbf{M})\) for its trace, \(\operatorname{rank}(\mathbf{M})\) for its rank, \(\mathbf{M}^{\dagger}\) for its Moore-Penrose pseudoinverse, and \(|\mathbf{M}|\) for the number of its rows. When \(x\) is an integer, we use \([x]\) for the set of integers \(1,2,\ldots,x\). For two positive scalars \(x\) and \(y\) and a scalar \(\alpha\in(0,1)\), we use \(x\approx_{\alpha}y\) to denote \((1-\alpha)y\leq x\leq(1+\alpha)y\); we sometimes use \(x\approx y\) to mean \((1-c)y\leq x\leq(1+c)y\) for some appropriate universal constant \(c\). We use \(\widetilde{O}\) to hide dependence on \(n^{o(1)}\). We acknowledge that there is some notation overloading between \(p\) (when used to denote the scalar in, for example, \(\ell_{p}\) norms) and \(p_{i}\) (when used to denote some probabilities) but the distinction is clear from context. We defer the proofs of facts stated in this section to Appendix A.

Notation for sensitivities.We start with a slightly general definition of sensitivities: \(\boldsymbol{\sigma}_{p}^{\rm B}(\mathbf{a}_{i})\) to denote the \(\ell_{p}\) sensitivity of \(\mathbf{a}_{i}\) with respect to \(\mathbf{B}\); when computing the sensitivity with respect to the same matrix that the row is drawn from, we omit the matrix superscript:

\[\bm{\sigma}_{p}^{\text{B}}(\mathbf{a}_{i}):=\max_{\mathbf{x}}\frac{|\mathbf{a}_{i }^{\top}\mathbf{x}|^{p}}{\|\mathbf{B}\mathbf{x}\|_{p}^{p}}\text{ and }\bm{\sigma}_{p}(\mathbf{a}_{i}):=\max_{\mathbf{x}}\frac{|\mathbf{a}_{i}^{ \top}\mathbf{x}|^{p}}{\|\mathbf{A}\mathbf{x}\|_{p}^{p}}.\] (2.1)

When referring to the vector of \(\ell_{p}\) sensitivities of all the rows of a matrix, we omit the subscript in the argument: so, \(\bm{\sigma}_{p}^{\text{B}}(\mathbf{A})\) is the vector of \(\ell_{p}\) sensitivities of rows of the matrix \(\mathbf{A}\), each computed with respect to the matrix \(\mathbf{B}\) (as in Equation (2.1)), and analogously, \(\bm{\sigma}_{p}(\mathbf{A})\) is the vector of \(\ell_{p}\) sensitivities of the rows of matrix \(\mathbf{A}\), each computed with respect to itself. Similarly, the total sensitivity is the sum of \(\ell_{p}\) sensitivities of the rows of \(\mathbf{A}\) with respect to matrix \(\mathbf{B}\), for which we use the following notation:

\[\mathfrak{S}_{p}^{\text{B}}(\mathbf{A}):=\sum_{i\in[|\mathbf{A}|]}\bm{\sigma}_ {p}^{\text{B}}(\mathbf{a}_{i})\text{ and }\mathfrak{S}_{p}(\mathbf{A}):=\sum_{i\in[| \mathbf{A}|]}\bm{\sigma}_{p}(\mathbf{a}_{i}),\] (2.2)

where, analogous to the rest of the notation regarding sensitivities, we omit the superscript when the sensitivities are being computed with respect to the input matrix itself.

While \(\bm{\sigma}_{p}^{\text{B}}(\mathbf{a}_{i})\) could be infinite, by appending \(\mathbf{a}_{i}\) to \(\mathbf{B}\), the generalized sensitivity becomes once again contained in \([0,1]\). Specifically, a simple rearrangement of the definition gives us the identity \(\bm{\sigma}_{p}^{\text{B}\cup\mathbf{a}_{i}}(\mathbf{a}_{i})=1/(1+1/\bm{\sigma }_{p}^{\text{B}}(\mathbf{a}_{i}))\). We now define leverage scores, which are a special type of sensitivities and also amenable to efficient computation [60], which makes them an ideal "building block" in algorithms for approximating sensitivities.

Leverage scores are \(\ell_{2}\) sensitivities.The leverage score of the \(i^{\mathrm{th}}\) row \(\mathbf{a}_{i}\) of \(\mathbf{A}\) is \(\tau_{i}(\mathbf{A})\stackrel{{\mathrm{def}}}{{=}}\mathbf{a}_{i} ^{\top}(\mathbf{A}^{\top}\mathbf{A})^{\dagger}\mathbf{a}_{i}\). The \(i^{\mathrm{th}}\) leverage score is also the \(i^{\mathrm{th}}\) diagonal entry of the orthogonal projection matrix \(\mathbf{A}(\mathbf{A}^{\top}\mathbf{A})^{\dagger}\mathbf{A}^{\top}\). Since the eigenvalues of an orthogonal projection matrix are zero or one, it implies \(\tau_{i}(\mathbf{A})=\mathbf{1}_{i}^{\top}\mathbf{A}(\mathbf{A}^{\top}\mathbf{ A})^{\dagger}\mathbf{A}^{\top}\mathbf{1}_{i}\leq 1\) for all \(i\in[n]\) and \(\sum_{i=1}^{n}\tau_{i}(\mathbf{A})\leq d\) (see [74]).

It turns out that \(\tau_{i}(\mathbf{A})=\bm{\sigma}_{2}(\mathbf{a}_{i})\) (see [75]). This may be verified by applying a change of basis to the variable \(\mathbf{y}=\mathbf{A}\mathbf{x}\) in the definition of sensitivity and working out that the maximizer is the vector parallel to \(\mathbf{y}=\mathbf{A}^{\dagger}\mathbf{a}_{i}\). This also gives an explicit formula for the total \(\ell_{2}\) sensitivity: \(\mathfrak{S}_{2}(\mathbf{A})=\operatorname{rank}(\mathbf{A})\).

The fastest algorithm for constant factor approximation to leverage scores is by [60], as stated next.

**Fact 2.1** ([60, Lemma 7]).: _Given a matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\), we can compute constant-factor approximations to all its leverage scores in time \(O(\mathbf{n}\mathbf{n}\mathbf{z}(\mathbf{A})\log(n)+d^{\omega}\log(d))\)._

In this paper, the above is the result we use to compute leverage scores (to a constant accuracy). Indeed, since the error accumulates multiplicatively across different steps of our algorithms, a high-accuracy algorithm for leverage score computation does not serve any benefit over this constant-accuracy one. Leverage scores approximate \(\ell_{1}\) sensitivities up to a distortion factor of \(\sqrt{n}\), as seen from the below known fact we prove, for completeness, in Appendix A.

**Fact 2.2** (Crude sensitivity approximation via leverage scores).: _Given a matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\), let \(\bm{\sigma}_{1}(\mathbf{a}_{i})\) denote the \(i^{\mathrm{th}}\)\(\ell_{1}\) sensitivity of \(\mathbf{A}\) with respect to \(\mathbf{A}\), and let \(\tau_{i}(\mathbf{A})\) denote its \(i^{\mathrm{th}}\) leverage score with respect to \(\mathbf{A}\). Then we have \(\sqrt{\frac{\tau_{i}(\mathbf{A})}{n}}\leq\bm{\sigma}_{1}(\mathbf{a}_{i})\leq \sqrt{\tau_{i}(\mathbf{A})}\)._

In general, \(\ell_{p}\) sensitivities of rows of a matrix suffer a distortion factor of at most \(n^{|1/2-1/p|}\) from corresponding leverage scores, which follows from Holder's inequality. The \(\ell_{p}\) generalizations of leverages scores are called \(\ell_{p}\) Lewis weights [54, 23], which satisfy the recurrence \(\mathbf{w}_{i}=\tau_{i}(\mathbf{W}^{1/2-1/p}\mathbf{A})\). However, unlike when \(p=2\), \(\ell_{p}\) Lewis weights are _not_ equal to sensitivities, and from an approximation perspective, they provide only a one-sided bound on the sensitivities \(\bm{\sigma}_{p}^{\mathbf{A}}(\mathbf{a}_{i})\leq d^{\max(0,p/2-1)}\,\mathbf{w}_ {p}^{\mathbf{A}}(\mathbf{a}_{i})\)[75, 76]. While we have algorithms for efficiently computing leverage scores [60] and Lewis weights [23, 77], extensions for utilizing these ideas for fast and accurate sensitivity approximation algorithms for all \(p\) have been limited, which is the gap this paper aims to fill. A key regime of interest for many downstream algorithms is a small constant factor approximation to true sensitivities, such as for subsampling [78].

Subspace embeddings.We use sampling-based constant-approximation \(\ell_{p}\) subspace embeddings that we denote by \(\mathbf{S}_{p}\). When the type of embedding is clear from context, we omit the subscript.

**Definition 2.3** (\(\ell_{p}\) Subspace Embedding).: _Given \(\mathbf{A}\in\mathbb{R}^{n\times d}\) (typically \(n\gg d\)), we call \(\mathbf{S}\in\mathbb{R}^{r\times n}\) (typically \(d\leq r\ll n\)) an \(\epsilon\)-approximate \(\ell_{p}\) subspace embedding of \(\mathbf{A}\) if \(\|\mathbf{SAx}\|_{p}\mathbf{\approx}_{\varepsilon}\|\mathbf{Ax}\|_{p}\ \forall \mathbf{x}\in\mathbb{R}^{d}\)._

When the diagonal matrix \(\mathbf{S}\) is defined as \(\mathbf{S}_{ii}=1/\sqrt{p_{i}}\) with probability \(p_{i}=\Omega(\tau_{i}(\mathbf{A}))\) (and \(0\) otherwise), then with high probability \(\mathbf{SA}\) is an \(\ell_{2}\) subspace embedding for \(\mathbf{A}\)[33; 79]; further, \(\mathbf{S}\) has at most \(\widetilde{O}(d\varepsilon^{-2})\) non-zero entries. For general \(p\), we use Lewis weights to construct \(\ell_{p}\) subspace embeddings fast [23; Theorem 7.1]. This fast construction has been made possible by the reduction of Lewis weight computation to a few (polynomial in \(p\)) leverage score computations for \(p<4\)[23] as well as \(p\geq 4\)[77]. This efficient construction is the reason for our choice of this subspace embedding. We note that there has been an extensive amount of literature on designing \(\ell_{1}\) subspace embeddings for example using Cauchy [80] and exponential [81] random variables.

For \(p\leq 2\), it is known that the expected sample complexity of rows is \(\sum_{i=1}^{n}p_{i}=O(d)\)[34], and for \(p>2\), this is known to be at most \(O(d^{p/2})\)[35]. While these bounds are optimal in the worst case, one important application of faster sensitivity calculations is to compute the minimal subspace embedding dimension for average case applications, particularly those seen in practice. This gap is captured by the total sensitivity \(\mathfrak{S}_{p}(\mathbf{A})\), which has been used to perform more sample efficient subspace embeddings [67; 27; 28], as sampling \(\widetilde{O}(\mathfrak{S}_{p}(\mathbf{A})d/\epsilon^{2})\) rows suffice to provide an \(\ell_{p}\) subspace embedding.

Accurate sensitivity calculations.While leverage scores give a crude approximation, constant-factor approximations of \(\ell_{p}\) sensitivities seem daunting since we must compute worst-case ratios over the input space. However, we can compute the cost of one sensitivity specific row by reducing it to \(\ell_{p}\) regression since by scale invariance, we have \(\frac{1}{\boldsymbol{\sigma}_{p}^{\frac{1}{\boldsymbol{\sigma}_{p}(\mathbf{a} _{i})}}}=\min_{\mathbf{a}_{i}^{T}\mathbf{x}=1}\|\mathbf{B}\mathbf{x}\|_{p}^{p}\).

This reduction allows us to use recent developments in fast approximate algorithms for \(\ell_{p}\) regression [44; 25; 45], which utilize subspace embeddings to first reduce the number of rows of the matrix to \(O(\text{poly}(d))\). For the specific case of \(p=1\), which is the focus of the main body of our paper, we may reduce to approximate \(\ell_{1}\) regression on a \(O(d)\times d\) matrix, which is a form of linear programming.

**Fact 2.4**.: _Given an \(n\times d\) matrix, the cost of computing \(k\) of its \(\ell_{1}\) sensitivities is \(\widetilde{O}(\mathbf{n}\mathbf{nz}(\mathbf{A})+k\cdot d^{\omega})\)._

## 3 Approximating functions of sensitivities

We first present a constant-probability algorithm approximating the \(\ell_{1}\) sensitivities in Algorithm1. Our algorithm is a natural one: Since computing the \(\ell_{1}\) sensitivities of all the rows simultaneously is computationally expensive, we instead hash \(\alpha\)-sized blocks of random rows into smaller (constant-sized) buckets and compute the sensitivities of the smaller, \(O(n/\alpha)\)-sized matrix \(\mathbf{P}\) so generated, computing the sensitivities with respect to \(\mathbf{SA}\), the \(\ell_{1}\) subspace embedding of \(\mathbf{A}\). Running this algorithm multiple times gives our high-probability guarantee via the standard median trick.

**Theorem 3.1**.: _Given a full-rank matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\) and an approximation factor \(1<\alpha\ll n\), let \(\boldsymbol{\sigma}_{1}(\mathbf{a}_{i})\) be the \(\ell_{1}\) sensitivity of the \(i^{\text{th}}\) row of \(\mathbf{A}\). Then there exists an algorithm that, in time \(\widetilde{O}\left(\mathbf{n}\mathbf{nz}(\mathbf{A})+\frac{n}{\alpha}\cdot d ^{\omega}\right)\), returns \(\widetilde{\boldsymbol{\sigma}}\in\mathbb{R}_{>0}^{n}\) such that with high probability, for each \(i\in[n]\),_

\[\boldsymbol{\sigma}_{1}(\mathbf{a}_{i})\leq\widetilde{\boldsymbol{\sigma}}_{i} \leq O(\boldsymbol{\sigma}_{1}(\mathbf{a}_{i})+\tfrac{\alpha}{n}\mathfrak{S}_{ 1}(\mathbf{A})).\] (3.1)

Proof Sketch of Theorem3.1; full proof in SectionB.1.: We achieve our guarantee via Algorithm1. To see this, first note that \(\|\mathbf{SAx}\|_{1}\approx\Theta(\|\mathbf{Ax}\|_{1})\) (since as per Line1, \(\mathbf{SA}\) is an \(\ell_{1}\) subspace embedding of \(\mathbf{A}\)). Combining this with the fact that every row in \(\mathbf{A}\) is mapped, via the designed randomness, to some row in the matrix \(\mathbf{P}\) helps establish the desired approximation guarantee. The runtime follows from using Fact2.4 to compute \(\ell_{1}\) sensitivities with respect to \(\mathbf{SA}\) of \(|\mathbf{P}|=O(n/\alpha)\) rows. 

As seen in SectionC.1, our techniques described above also generalize to the \(p\geq 1\) case. Specifically, in SectionC.2, we show that reducing \(\ell_{p}\) sensitivity calculations by an \(\alpha\) factor gives an approximation guarantee of the form \(O(\alpha^{p-1}\boldsymbol{\sigma}_{p}(\mathbf{a}_{i}))+\tfrac{\alpha^{p}}{n} \mathfrak{S}_{p}(\mathbf{A}))\).

**Remark 3.2**.: _The sensitivities returned by our algorithm are approximate, with relative and additive error, but are useful in certain settings as they preserve \(\ell_{p}\) regression approximation guarantees while increasing total sample complexity by only a small \(\text{poly}(d)\) factor compared to true sensitivities while still keeping it much smaller than that obtained via Lewis weights. To see this with a simple toy

[MISSING_PAGE_FAIL:7]

**Fact 3.4** (Sampling via Lewis Weights [23, 35]).: _Given \(\mathbf{A}\in\mathbb{R}^{n\times d}\) and \(p>0\). Consider a random diagonal \(\ell_{p}\) sampling matrix \(\mathbf{S}\in\mathbb{R}^{n\times n}\) with sampling probabilities \(\{p_{i}\}\) proportional to the \(\ell_{p}\) Lewis weight of \(\mathbf{A}\), i.e., for each \(i\in[n]\), the \(i^{\mathrm{th}}\) diagonal entry is independently set to be_

\[\mathbf{S}_{i,i}=\left\{\begin{array}{ll}1/p_{i}^{1/p}&\text{ with probability }p_{i}\\ 0&\text{ otherwise}\end{array}\right..\]

_Then, with high probability, \(\mathbf{S}\) with \(O(\varepsilon^{-2}d^{\max(1,p/2)}(\log d)^{2}\log(d/\varepsilon))\) rows is an \(\ell_{p}\) subspace embedding for \(\mathbf{A}\) (cf. Definition 2.3)._

**Theorem 3.5**.: _Given a matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\) and an approximation factor \(\gamma\in(0,1)\), there exists an algorithm, which returns a positive scalar \(\widehat{s}\) such that, with a probability \(0.99\), we have_

\[\mathfrak{S}_{p}(\mathbf{A})\leq\widehat{s}\leq(1+O(\gamma))\mathfrak{S}_{p} (\mathbf{A}).\]

_Our algorithm's runtime is \(\widetilde{O}\left(\mathbf{n}\mathbf{n}\mathbf{z}(\mathbf{A})+\frac{1}{ \gamma^{2}}\cdot d^{|p/2-1|}\mathsf{LP}(O(d^{\max(1,p/2)},d,p)\right)\)._

Proof.: Without loss of generality, we may assume \(\mathbf{A}\) to be full rank. Then, its Lewis weights satisfy \(\sum_{i=1}^{n}\mathbf{w}_{p}(\mathbf{a}_{i})=d\). Per Line 2 of Algorithm 2, our sampling distribution is chosen to be \(v_{i}=\frac{\mathbf{w}_{p}(\mathbf{a}_{i})}{d}\) for all \(i\in[n]\). We sample from \(\mathbf{A}\) rows with replacement, with row \(i\) picked with a probability of \(v_{i}\). From the definition of \(v_{i}\) in Line 2 and \(r_{j}\) in Line 5 and the fact that \(\mathbf{S}_{p}\mathbf{A}\) is a constant factor \(\ell_{p}\) subspace embedding of \(\mathbf{A}\), our algorithm's output satisfies the following unbiasedness condition:

\[\mathbb{E}\left(\frac{1}{m}\sum_{j\in[m]}r_{j}\right)=\mathbb{E}\left(\frac{1} {m}\sum_{j=1}^{m}\sum_{i_{j}=1}^{n}\frac{\boldsymbol{\sigma}_{p}^{\mathbf{S}_ {p}\mathbf{A}}(\mathbf{a}_{i_{j}})}{v_{i_{j}}}\cdot v_{i_{j}}\right)=\mathfrak{ S}_{p}^{\mathbf{S}_{p}\mathbf{A}}(\mathbf{A})\leq 2\mathfrak{S}_{p}(\mathbf{A}).\]

By independence, we also have the following bound on the variance of \(\frac{1}{m}\sum_{j\in[m]}r_{j}\):

\[\text{Var}\left(\frac{1}{m}\sum_{j\in[m]}r_{j}\right)\leq\frac{1}{m}\sum_{i=1 }^{n}\frac{\boldsymbol{\sigma}_{p}^{\mathbf{S}_{p}\mathbf{A}}(\mathbf{a}_{i})^ {2}}{v_{i}}=\frac{d}{m}\cdot\sum_{i=1}^{n}\frac{\boldsymbol{\sigma}_{p}^{ \mathbf{S}_{p}\mathbf{A}}(\mathbf{a}_{i})^{2}}{\mathbf{w}_{p}(\mathbf{a}_{i})},\]

with the final step holding by the choice of \(v_{i}\) in Line 2. In the case that \(p\geq 2\), we have

\[\text{Var}\left(\frac{1}{m}\sum_{j\in[m]}r_{j}\right)\leq\frac{d}{m}\cdot\sum _{i=1}^{n}\frac{\boldsymbol{\sigma}_{p}^{\mathbf{S}_{p}\mathbf{A}}(\mathbf{a}_ {i})^{2}}{\mathbf{w}_{p}(\mathbf{a}_{i})}\leq\frac{d}{m}\cdot\sum_{i\in[n]} \boldsymbol{\sigma}_{p}^{\mathbf{S}_{p}\mathbf{A}}(\mathbf{a}_{i})\cdot d^{p/2- 1}\leq\frac{2d^{p/2}}{m}\mathfrak{S}_{p}(\mathbf{A}),\]

where the first inequality uses Fact 3.3. Therefore, applying Chebyshev's inequality on \(\frac{1}{m}\sum_{j\in[m]}r_{j}\) (as defined in Line 5) with \(m=O\left(\frac{d^{p/2}}{\mathfrak{S}_{p}(\mathbf{A})\gamma^{2}}\right)\) gives a a \(\gamma\)-multiplicative accuracy in approximating \(\mathfrak{S}_{p}(\mathbf{A})\) (with the desired constant probability). For \(p\geq 2\), we additionally have [24, Theorem \(1.7\)] the lower bound \(\mathfrak{S}_{p}(\mathbf{A})\geq d\), which when plugged into the value of \(m\) gives the claimed sample complexity. A similar argument may be applied for the case \(p<2\); specifically, we have that

\[\text{Var}\left(\frac{1}{m}\sum_{j\in[m]}r_{j}\right)\leq\frac{d}{m}\cdot\sum _{i=1}^{n}\frac{\boldsymbol{\sigma}_{p}^{\mathbf{S}_{p}\mathbf{A}}(\mathbf{a}_ {i})^{2}}{\mathbf{w}_{p}(\mathbf{a}_{i})}\leq\frac{d}{m}\cdot\sum_{i\in[n]} \boldsymbol{\sigma}_{p}^{\mathbf{S}_{p}\mathbf{A}}(\mathbf{a}_{i})\leq\frac{2d }{m}\mathfrak{S}_{p}(\mathbf{A}),\]

where the second step is by \(\mathbf{w}_{p}(\mathbf{a}_{i})\geq\boldsymbol{\sigma}_{p}(\mathbf{a}_{i})\) from Fact 3.3. For \(p<2\), we also have \(\mathfrak{S}_{p}(\mathbf{A})\geq d^{p/2}\) from [24, Theorem \(1.7\)]. Applying Chebyshev's inequality with this fact gives a sample complexity of \(m=O\left(d^{1-p/2}/\gamma^{2}\right)\). This completes the correctness guarantee.

**Runtime.** We first compute all \(\ell_{p}\) Lewis weights of \(\mathbf{A}\in\mathbb{R}^{n\times d}\) up to a constant multiplicative accuracy, the cost of which is \(O\left(\frac{1}{1-|1-p/2|}\log(\log(n))\right)\) leverage score computations for \(p<4\)[23] and \(O(p^{3}\log(np))\) leverage score computations for \(p\geq 4\)[77]. Next, we compute \(m=O(d^{|1-p/2|})\) sensitivities with respect to \(\mathbf{S}_{p}\mathbf{A}\). From [23, 35], \(\mathbf{S}_{p}\) has, with high probability, \(O(d^{p/2}\log d)\) rows when \(p>2\) and \(O(d\log d)\) rows when \(p\leq 2\). Summing the cost of computing these \(m\) sensitivities and that of computing the Lewis weights gives the claimed runtime.

**Remark 3.6**.: _We present in Appendix B.2 an alternate algorithm, Algorithm 4, for estimating the total \(\ell_{p}\) sensitivity for \(p=1\). Algorithm 4 uses recursive computations of leverage scores, in contrast to Algorithm 2 which uses Lewis weights in a one-shot manner, and may be of independent interest._

### Approximating the maximum \(\ell_{1}\) sensitivity

In this section, we present an algorithm that approximates \(\|\bm{\sigma}_{1}(\mathbf{A})\|_{\infty}=\max_{i\in\|\mathbf{A}\|}\bm{\sigma}_{1} (\mathbf{a}_{i})\), the maximum of the \(\ell_{1}\) sensitivities of the rows of a matrix \(\mathbf{A}\). As alluded to in Section1, a first approach to this problem would be to simply estimate all \(n\) sensitivities and compute their maximum. To do better than this, one idea, inspired by the random hashing approach of Algorithm1, is as follows.

If the matrix has a large number of high-sensitivity rows, then, intuitively, the appropriately scaled maximum sensitivity of a uniformly sampled subset of these rows should approximate \(\|\bm{\sigma}_{1}(\mathbf{A})\|_{\infty}\). Specifically, assume that the matrix has at least \(\alpha\) rows of sensitivity at least \(\|\bm{\sigma}_{1}(\mathbf{A})\|_{\infty}/\sqrt{\alpha}\); uniformly sample \(n/\alpha\) rows and return \(\widetilde{\bm{\sigma}}_{a}\), the (appropriately scaled) maximum of \(\bm{\sigma}_{1}^{\mathbf{S_{1}}\mathbf{A}}(\mathbf{a}_{i})\), for the sampled rows \(i\). Then, \(\|\bm{\sigma}_{1}(\mathbf{A})\|_{\infty}\leq\widetilde{\bm{\sigma}}_{a}\leq O (\sqrt{\alpha}\|\bm{\sigma}_{1}(\mathbf{A})\|_{\infty})\) with a constant probability. Here the upper bound guarantee is without any condition on the number of rows with large \(\ell_{1}\) sensitivities.

In the other case, if the matrix does _not_ have too many high-sensitivity rows, we could estimate the maximum sensitivity by hashing rows into small buckets via Rademacher combinations of uniformly sampled blocks of \(\alpha\) rows each (just like in Algorithm1). Then, \(\widetilde{\bm{\sigma}}_{b}\), the scaled maximum of the \(\ell_{1}\) sensitivities of these rows satisfies \(\|\bm{\sigma}_{1}(\mathbf{A})\|_{\infty}\leq\widetilde{\bm{\sigma}}_{b}\leq O (\sqrt{\alpha}\|\bm{\sigma}_{1}(\mathbf{A})\|_{\infty})\). Here, it is the _lower_ bound that comes for free (i.e., without any condition on the number of rows with large \(\ell_{1}\) sensitivities).

Despite the above strategies working for each case, there is no way to combine these approaches without knowledge of whether the input matrix has a enough "high-sensitivity" rows or not. We therefore avoid this approach and instead present Algorithm3, where we make use of \(\mathbf{S}_{\infty}\mathbf{A}\), an \(\ell_{\infty}\) subspace embedding of \(\mathbf{A}\). Our algorithm hinges on the recent development [75, Theorem 1.3] on efficient construction of such an embedding with simply a subset of \(\widetilde{O}(d)\) rows of \(\mathbf{A}\).

```
0: Matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\)
0: Scalar \(\widehat{s}\in\mathbb{R}_{\geq 0}\) that satisfies \(\|\bm{\sigma}_{1}(\mathbf{A})\|_{\infty}\leq\widehat{s}\leq C\cdot\sqrt{d} \cdot\|\bm{\sigma}_{1}(\mathbf{A})\|_{\infty}\)
1: Compute, for \(\mathbf{A}\), an \(\ell_{\infty}\) subspace embedding \(\mathbf{S}_{\infty}\mathbf{A}\in\mathbb{R}^{O(d\log^{2}(d))\times d}\) such that \(\mathbf{S}_{\infty}\mathbf{A}\) is a subset of the rows of \(\mathbf{A}\)[75, Theorem 1.3]
2: Compute, for \(\mathbf{A}\), an \(\ell_{1}\) subspace embedding \(\mathbf{S}_{1}\mathbf{A}\in\mathbb{R}^{O(d)\times d}\)
3: Return \(\sqrt{d}\|\bm{\sigma}_{1}^{\mathbf{S_{1}}\mathbf{A}}(\mathbf{S}_{\infty} \mathbf{A})\|_{\infty}\) ```

**Algorithm 3** Approximating the Maximum of \(\ell_{1}\)-Sensitivities

**Theorem 3.7** (**Approximating the Maximum of \(\ell_{1}\) Sensitivities**).: _Given a matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\), there exists an algorithm, which in time \(\widetilde{O}(\mathbf{n}\mathbf{n}\mathbf{z}(\mathbf{A})+d^{\omega+1})\), outputs a positive scalar \(\widehat{s}\) that satisfies_

\[\Omega(\|\bm{\sigma}_{1}(\mathbf{A})\|_{\infty})\leq\widehat{s}\leq O(\sqrt{d} \|\bm{\sigma}_{1}(\mathbf{A})\|_{\infty}).\]

Proof Sketch of Theorem3.7; full proof in AppendixB.3.: We achieve our guarantee via Algorithm3. Define \(\mathbf{x}^{\star}\) and \(\mathbf{a}_{i^{\star}}\) as: \(\mathbf{x}^{\star}\), \(i^{\star}=\arg\max_{\mathbf{x}\in\mathbb{R}^{d},i\in[|\mathbf{A}|]}\frac{\| \mathbf{a}_{i^{\star}}^{\top}\mathbf{x}\|_{1}}{\|\mathbf{A}\mathbf{x}\|_{1}}\). Since \(\mathbf{S}_{1}\mathbf{A}\) is an \(\ell_{1}\) subspace embedding of \(\mathbf{A}\), we have, for any \(\mathbf{x}\in\mathbb{R}^{d}\), that \(\|\mathbf{S}_{1}\mathbf{A}\mathbf{x}\|_{1}=\Theta(\|\mathbf{A}\mathbf{x}\|_{1})\). If \(\mathbf{S}_{\infty}\mathbf{A}\) contains the row \(\mathbf{a}_{i^{\star}}\), then \(\|\mathbf{S}_{1}\mathbf{A}\mathbf{x}\|_{1}{=}\Theta(\|\mathbf{A}\mathbf{x}\|_{1})\) implies \(\|\bm{\sigma}_{1}^{\mathbf{S_{1}}\mathbf{A}}(\mathbf{S}_{\infty}\mathbf{A})\|_ {\infty}=\Theta(1)\|\bm{\sigma}_{1}(\mathbf{A})\|_{\infty}\). In the other case, suppose \(\mathbf{a}_{i^{\star}}\) is not included in \(\mathbf{S}_{\infty}\mathbf{A}\). Then we observe that

\[\|\bm{\sigma}_{1}^{\mathbf{S_{1}}\mathbf{A}}(\mathbf{S}_{\infty}\mathbf{A})\|_ {\infty}=\max_{\mathbf{x}\in\mathbb{R}^{d},\mathbf{c}_{j}\in\mathbf{S}_{ \infty}\mathbf{A}}\frac{\mathbf{c}_{j}^{\top}\mathbf{x}}{\|\mathbf{S}_{1} \mathbf{A}\mathbf{x}\|_{1}}\geq\max_{\mathbf{x}\in\mathbb{R}^{d}}\frac{\| \mathbf{S}_{\infty}\mathbf{A}\mathbf{x}\|_{\infty}}{\|\mathbf{S}_{1}\mathbf{A} \mathbf{x}\|_{1}}=\Theta(1)\max_{\mathbf{x}\in\mathbb{R}^{d}}\frac{\|\mathbf{S }_{\infty}\mathbf{A}\mathbf{x}\|_{\infty}}{\|\mathbf{A}\mathbf{x}\|_{1}},\] (3.2)

where the the second step is by choosing a specific vector in the numerator and the third step uses \(\|\mathbf{S}_{1}\mathbf{A}\mathbf{x}\|_{1}=\Theta(\|\mathbf{A}\mathbf{x}\|_{1})\). We further have,

\[\max_{\mathbf{x}\in\mathbb{R}^{d}}\frac{\|\mathbf{S}_{\infty}\mathbf{A}\mathbf{x} \|_{\infty}}{\|\mathbf{A}\mathbf{x}\|_{1}}\geq\frac{\|\mathbf{S}_{\infty}\mathbf{A} \mathbf{x}^{\star}\|_{\infty}}{\|\mathbf{A}\mathbf{x}^{\star}\|_{1}}\geq\frac{\| \mathbf{A}\mathbf{x}^{\star}\|_{\infty}}{\sqrt{d}\|\mathbf{A}\mathbf{x}^{\star} \|_{1}}\geq\frac{|\mathbf{a}_{i^{\star}}^{\top}\mathbf{x}^{\star}|}{\sqrt{d}\| \mathbf{A}\mathbf{x}^{\star}\|_{1}}=\frac{1}{\sqrt{d}}\|\bm{\sigma}_{1}(\mathbf{A}) \|_{\infty},,\] (3.3)

where the first step is by choosing \(\mathbf{x}=\mathbf{x}^{\star}\), the second step is by the guarantee of \(\ell_{\infty}\) subspace embedding, and the final step is by definition of \(\bm{\sigma}_{1}(\mathbf{a}_{i^{\star}})\). Combining Inequality3.2 and Inequality3.3 gives the claimed lower bound on \(\|\bm{\sigma}_{1}^{\mathbf{S_{1}}\mathbf{A}}(\mathbf{S}_{\infty}\mathbf{A})\|_ {\infty}\). The runtime follows from the cost of computing \(\mathbf{S}_{\infty}\mathbf{A}\) from [75] and that of computing \(\widetilde{O}(d)\) of \(\ell_{1}\) sensitivities with respect to \(\mathbf{S}_{1}\mathbf{A}\).

## 4 Numerical experiments

We demonstrate our fast sensitivity approximations on multiple real-world datasets in the UCI Machine Learning Dataset Repository [46], such as the wine and fires datasets, for different \(p\) and varying approximation parameters \(\alpha\). We focus on the wine dataset here, with full details in Appendix E.

Experimental Setup.For each dataset and each \(p\), we first apply the Lewis weight subspace embedding to compute a smaller matrix \(\mathbf{SA}\). Then, we run our sensitivity computation Algorithm 1 and measure the average and maximum absolute log ratio \(|\log(\sigma_{\text{approx}}/\sigma_{\text{true}})|\) to capture the relative error of the sensitivity approximation. Note that this is stricter than our guarantees, which give only an \(O(\alpha)\) additive error; therefore we have no real upper bound on the maximum absolute log ratio due to small sensitivities. We plot an upper bound of \(\log(\alpha^{p})\), which is the worst case additive error approximation although it does provide relative error guarantees with respect to the average sensitivity. We compare our fast algorithm for computing the total sensitivity with the naive brute-force method by approximating all sensitivities and then averaging.

Analysis.In practice, we found most real-world datasets have easy-to-approximate sensitivities with total \(\ell_{p}\) sensitivity about \(2\)-\(5\) times lower than the theoretical upper bound. Figure 1 shows that the average absolute log ratios for approximating the \(\ell_{p}\) sensitivities are much smaller than those suggested by the theoretical upper bound, even for large \(\alpha\). Specifically, when \(\alpha=40\), we incur only a \(16\)x accuracy deterioration in exchange for a \(40\)x faster algorithm. This is significantly better than the worst-case accuracy guarantee which for \(p=3\) would be \(\alpha^{p}=40^{3}\).

More importantly, we find that empirical total sensitivities are much lower than the theoretical upper bounds suggest, often by a factor of at least \(5\), especially for \(p>2\). This implies that real-world structure can be utilized for improved data compression and justifies the importance of sensitivity estimation for real-world datasets. Furthermore, our novel algorithm approximates the total sensitivity up to an overestimate within a factor of \(1.3\), in less than a quarter of the time of the brute-force algorithm. Our observation generalizes to other real-world datasets (see Appendix E).

## Acknowledgements

We are very grateful to: Sagi Perel, Arun Jambulapati, and Kevin Tian for helpful discussions about related work; Taisuke Yasuda for his generous help discussing results in \(\ell_{p}\) sensitivity sampling; and our NeurIPS reviewers for their time, effort, and many constructive suggestions. Swati gratefully acknowledges funding for her student researcher position from Google Research and research

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline \multirow{2}{*}{\(p\)} & Total Sensitivity & \multirow{2}{*}{Brute-Force/True} & \multirow{2}{*}{Approximation} & \multirow{2}{*}{Brute-Force Runtime (s)} & \multirow{2}{*}{Approximate Runtime (s)} \\  & Upper Bound & & & & \\ \hline
1 & 14 & 5.2 & 6.4 & 667 & 105 \\
1.5 & 14 & 11.6 & 14.2 & 673 & 131 \\
2.5 & 27.1 & 13.8 & 14.9 & 693 & 209 \\
3 & 52.4 & 7.2 & 8.9 & 686 & 192 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Runtime comparison for computing total sensitivities for the wine dataset, which has matrix shape \((177,\,14)\).

Figure 1: Average absolute log ratios for all \(\ell_{p}\) sensitivity approximations for wine, with the theoretical additive bound (dashed).

assistantship at UW from the NSF award CCF-\(1749609\) (via Yin Tat Lee). Part of this work was done while D. Woodruff was at Google Research. D. Woodruff also acknowledges support from a Simons Investigator Award.

## References

* [1] Herbert Robbins and Sutton Monro. A stochastic approximation method. _The annals of mathematical statistics_, 1951.
* [2] Leon Bottou and Yann Cun. Large scale online learning. _Advances in neural information processing systems_, 16, 2003.
* [3] Tong Zhang. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In _Proceedings of the twenty-first international conference on Machine learning_, page 116, 2004.
* [4] Leon Bottou. Stochastic gradient descent tricks. In _Neural networks: Tricks of the trade_, pages 421-436. Springer, 2012.
* [5] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. _ACM Transactions on Intelligent Systems and Technology_, 2:27:1-27:27, 2011. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.
* [6] Nicolas Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an exponential convergence rate for finite training sets. _Advances in neural information processing systems_, 25, 2012.
* [7] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss minimization. _Journal of Machine Learning Research_, 14(2), 2013.
* [8] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. _Advances in neural information processing systems_, 26, 2013.
* [9] Mehrdad Mahdavi, Lijun Zhang, and Rong Jin. Mixed optimization for smooth functions. _Advances in neural information processing systems_, 26, 2013.
* [10] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. _Advances in neural information processing systems_, 27, 2014.
* [11] Julien Mairal. Incremental majorization-minimization optimization with application to large-scale machine learning. _SIAM Journal on Optimization_, 25(2), 2015.
* [12] Zeyuan Allen-Zhu and Yang Yuan. Improved svrg for non-strongly-convex or sum-of-non-convex objectives. In _International conference on machine learning_, 2016.
* [13] Elad Hazan and Haipeng Luo. Variance-reduced and projection-free stochastic optimization. In _International Conference on Machine Learning_, 2016.
* [14] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic average gradient. _Mathematical Programming_, 162(1), 2017.
* [15] Michael Langberg and Leonard J Schulman. Universal \(\varepsilon\)-approximators for integrals. In _Proceedings of the twenty-first annual ACM-SIAM symposium on Discrete Algorithms_, 2010.
* [16] Dan Feldman and Michael Langberg. A unified framework for approximating and clustering data. In _Proceedings of the forty-third annual ACM symposium on Theory of computing_, 2011.
* [17] Deanna Needell, Rachel Ward, and Nati Srebro. Stochastic gradient descent, weighted sampling, and the randomized kaczmarz algorithm. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 27. Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper_files/paper/2014/file/f29c21d4897f78948b91f03172341b7b-Paper.pdf.
* [18] Peilin Zhao and Tong Zhang. Stochastic optimization with importance sampling for regularized loss minimization. In _Proceedings of the 32nd International Conference on Machine Learning_, pages 1-9, 2015.

* [19] Angelos Katharopoulos and Francois Fleuret. Biased importance sampling for deep neural network training. _arXiv preprint arXiv:1706.00043_, 2017.
* [20] Tyler B. Johnson and Carlos Guestrin. Training deep models faster with robust, approximate importance sampling. In _Advances in Neural Information Processing Systems 31_, 2018.
* [21] Soren Mindermann, Jan M Brauner, Muhammed T Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu, Benedikt Holten, Aidan N Gomez, Adrien Morisot, Sebastian Farquhar, et al. Prioritized training on points that are learnable, worth learning, and not yet learnt. In _International Conference on Machine Learning_, 2022.
* [22] Vladimir Braverman, Dan Feldman, Harry Lang, Adiel Statman, and Samson Zhou. New frameworks for offline and streaming coreset constructions. _arXiv preprint arXiv:1612.00889_, 2016.
* [23] Michael B Cohen and Richard Peng. Lp row sampling by lewis weights. In _Proceedings of the forty-seventh annual ACM symposium on Theory of computing_, 2015.
* [24] David Woodruff and Taisuke Yasuda. Sharper bounds for \(\ell_{p}\) sensitivity sampling. In _International Conference on Machine Learning_, pages 37238-37272. PMLR, 2023.
* [25] Raphael Meyer, Cameron Musco, Christopher Musco, David P Woodruff, and Samson Zhou. Fast regression for structured inputs. In _International Conference on Learning Representations (ICLR)_, 2022.
* [26] Kasturi Varadarajan and Xin Xiao. On the sensitivity of shape fitting problems. In _32nd International Conference on Foundations of Software Technology and Theoretical Computer Science_, 2012.
* [27] Vladimir Braverman, Dan Feldman, Harry Lang, Adiel Statman, and Samson Zhou. Efficient coreset constructions via sensitivity sampling. _Proceedings of Machine Learning Research_, 157 (2021), 2021.
* [28] Murad Tukan, Xuan Wu, Samson Zhou, Vladimir Braverman, and Dan Feldman. New coresets for projective clustering and applications. In _International Conference on Artificial Intelligence and Statistics_, 2022.
* [29] Olivier Bachem, Mario Lucic, and Andreas Krause. Coresets for nonparametric estimation-the case of dp-means. In _International Conference on Machine Learning_, 2015.
* [30] Mario Lucic, Olivier Bachem, and Andreas Krause. Strong coresets for hard and soft bregman clustering with applications to exponential family mixtures. In _Artificial intelligence and statistics_, 2016.
* [31] Jonathan Huggins, Trevor Campbell, and Tamara Broderick. Coresets for scalable bayesian logistic regression. _Advances in Neural Information Processing Systems_, 29, 2016.
* [32] Alexander Munteanu, Chris Schwiegelshohn, Christian Sohler, and David Woodruff. On coresets for logistic regression. _Advances in Neural Information Processing Systems_, 31, 2018.
* [33] Petros Drineas, Michael W Mahoney, and Shan Muthukrishnan. Sampling algorithms for \(\ell_{2}\) regression and applications. In _Proceedings of the seventeenth annual ACM-SIAM symposium on Discrete algorithm_, 2006.
* [34] Michael W Mahoney et al. Randomized algorithms for matrices and data. _Foundations and Trends(r) in Machine Learning_, 3(2), 2011.
* [35] David P Woodruff and Taisuke Yasuda. Online lewis weight sampling. In _Proceedings of the 2023 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, 2023.
* [36] Flavio Chierichetti, Sreenivas Gollapudi, Ravi Kumar, Silvio Lattanzi, Rina Panigrahy, and David P Woodruff. Algorithms for \(\ell_{p}\) low-rank approximation. In _International Conference on Machine Learning_, 2017.

* [37] Emmanuel J Candes and Terence Tao. Decoding by linear programming. _IEEE transactions on information theory_, 51(12), 2005.
* [38] Morteza Alamgir and Ulrike Luxburg. Phase transition in the family of \(p\)-resistances. _Advances in neural information processing systems_, 24, 2011.
* [39] Jeff Calder. Consistency of lipschitz learning with infinite unlabeled data and finite labeled data. _SIAM Journal on Mathematics of Data Science_, 1(4), 2019.
* [40] Mauricio Flores, Jeff Calder, and Gilad Lerman. Analysis and algorithms for \(\ell_{p}\)-based semi-supervised learning on graphs. _Applied and Computational Harmonic Analysis_, 60, 2022.
* [41] Abderrahim Elmoataz, Matthieu Toutain, and Daniel Tenbrinck. On the \(p\)-laplacian and \(\infty\)-laplacian on graphs with applications in image and data processing. _SIAM Journal on Imaging Sciences_, 8(4), 2015.
* [42] Kenneth Clarkson, Ruosong Wang, and David Woodruff. Dimensionality reduction for tukey regression. In _International Conference on Machine Learning_, 2019.
* [43] Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. _Foundations and Trends(r) in Theoretical Computer Science_, 9(3-4):211-407, 2014.
* [44] Deeksha Adil, Richard Peng, and Sushant Sachdeva. Fast, provably convergent irls algorithm for p-norm linear regression. _Advances in Neural Information Processing Systems_, 32, 2019.
* [45] Arun Jambulapati, Yang P Liu, and Aaron Sidford. Improved iteration complexities for over-constrained p-norm regression. In _Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing_, 2022.
* [46] Arthur Asuncion and David Newman. Uci machine learning repository, 2007.
* [47] Anant Raj, Cameron Musco, and Lester Mackey. Importance sampling via local sensitivity. In _International Conference on Artificial Intelligence and Statistics_, 2020.
* [48] Alaa Maalouf, Adiel Statman, and Dan Feldman. Tight sensitivity bounds for smaller coresets. In _Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 2051-2061, 2020.
* [49] Ruosong Wang, Russ R Salakhutdinov, and Lin Yang. Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension. _Advances in Neural Information Processing Systems_, 33:6123-6135, 2020.
* [50] Dingwen Kong, Ruslan Salakhutdinov, Ruosong Wang, and Lin F Yang. Online sub-sampling for reinforcement learning with general function approximation. _arXiv preprint arXiv:2106.07203_, 2021.
* [51] Lucas Liebenwein, Cenk Baykal, Harry Lang, Dan Feldman, and Daniela Rus. Provable filter pruning for efficient neural networks. In _International Conference on Learning Representations_, 2019.
* [52] Murad Tukan, Loay Mualem, and Alaa Maalouf. Pruning neural networks via coresets and convex geometry: Towards no assumptions. _Advances in Neural Information Processing Systems_, 2022.
* [53] Ben Mussay, Dan Feldman, Samson Zhou, Vladimir Braverman, and Margarita Osadchy. Data-independent structured pruning of neural networks via coresets. _IEEE Transactions on Neural Networks and Learning Systems_, 33(12):7829-7841, 2021.
* [54] D Lewis. Finite dimensional subspaces of \(\ell_{p}\). _Studia Mathematica_, 1978.
* [55] Gideon Schechtman. More on embedding subspaces of \(\ell_{p}\) in \(\ell_{r}^{n}\). _Compositio Mathematica_, 1987.

* [56] Jean Bourgain, Joram Lindenstrauss, and Vitali Milman. Approximation of zonoids by zonotopes. 1989.
* [57] Michel Talagrand. _Proceedings of the American Mathematical Society_, 1990.
* [58] Michel Ledoux and Michel Talagrand. _Probability in Banach Spaces: isoperimetry and processes_, volume 23. Springer Science & Business Media, 1991.
* [59] Gideon Schechtman and Artem Zvavitch. Embedding subspaces of \(\ell_{p}\) into \(\ell_{p}^{n}\), \(0<p<1\). _Mathematische Nachrichten_, 2001.
* [60] Michael B Cohen, Yin Tat Lee, Cameron Musco, Christopher Musco, Richard Peng, and Aaron Sidford. Uniform sampling for matrix approximation. In _Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science_, 2015.
* [61] Petros Drineas, Malik Magdon-Ismail, Michael W Mahoney, and David P Woodruff. Fast approximation of matrix coherence and statistical leverage. _The Journal of Machine Learning Research_, 13(1), 2012.
* [62] Mu Li, Gary L Miller, and Richard Peng. Iterative row sampling. In _2013 IEEE 54th Annual Symposium on Foundations of Computer Science_, 2013.
* [63] Christian Sohler and David P Woodruff. Strong coresets for k-median and subspace approximation: Goodbye dimension. In _2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS)_, 2018.
* [64] Rachit Chhaya, Jayesh Choudhari, Anirban Dasgupta, and Supratim Shit. Streaming coresets for symmetric tensor factorization. In _International Conference on Machine Learning_, 2020.
* [65] David Durfee, Kevin A Lai, and Saurabh Sawlani. \(\ell_{1}\) regression using lewis weights preconditioning and stochastic gradient descent. In _Conference On Learning Theory_, 2018.
* [66] Yi Li, Ruosong Wang, Lin Yang, and Hanrui Zhang. Nearly linear row sampling algorithm for quantile regression. In _Proceedings of the 37th International Conference on Machine Learning_, 2020.
* [67] Vladimir Braverman, Petros Drineas, Cameron Musco, Christopher Musco, Jalaj Upadhyay, David P Woodruff, and Samson Zhou. Near optimal linear algebra in the online and sliding window models. In _2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS)_, 2020.
* [68] Aditi Laddha, Yin Tat Lee, and Santosh Vempala. Strong self-concordance and sampling. In _Proceedings of the 52nd annual ACM SIGACT symposium on theory of computing_, 2020.
* [69] Yi Li, Ruosong Wang, Lin Yang, and Hanrui Zhang. Nearly linear row sampling algorithm for quantile regression. In _International Conference on Machine Learning_, 2020.
* [70] Arvind V Mahankali and David P Woodruff. Optimal \(\ell_{1}\) column subset selection and a fast ptas for low rank approximation. In _Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA)_, 2021.
* [71] Tung Mai, Cameron Musco, and Anup Rao. Coresets for classification-simplified and strengthened. _Advances in Neural Information Processing Systems_, 34, 2021.
* [72] Xue Chen and Michal Derezinski. Query complexity of least absolute deviation regression via robust uniform convergence. In _Conference on Learning Theory_, 2021.
* [73] Aditya Parulekar, Advait Parulekar, and Eric Price. L1 regression with lewis weights subsampling. _Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques_, 2021.
* [74] Frederic G Foster. On the stochastic matrices associated with certain queuing processes. _The Annals of Mathematical Statistics_, 24(3), 1953.

* [75] David P Woodruff and Taisuke Yasuda. High-dimensional geometric streaming in polynomial space. In _2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS)_, pages 732-743, 2022.
* [76] David P Woodruff Cameron Musco, Christopher Musco, David P Woodruff, and Taisuke Yasuda. Active sampling for linear regression beyond the l2 norm. _CoRR_, 2021.
* [77] Maryam Fazel, Yin Tat Lee, Swati Padmanabhan, and Aaron Sidford. Computing lewis weights to high precision. In _Proceedings of the 2022 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, 2022.
* [78] David P Woodruff et al. Sketching as a tool for numerical linear algebra. _Foundations and Trends(r) in Theoretical Computer Science_, 10(1-2), 2014.
* [79] Mark Rudelson and Roman Vershynin. Sampling from large matrices: An approach through geometric functional analysis. _Journal of the ACM (JACM)_, 2007.
* [80] Christian Sohler and David P Woodruff. Subspace embeddings for the \(\ell\_1\)-norm with applications. In _Proceedings of the forty-third annual ACM symposium on Theory of computing_, pages 755-764, 2011.
* [81] David Woodruff and Qin Zhang. Subspace embeddings and \(\ell\_p\)-regression using exponential random variables. In _Conference on Learning Theory_, pages 546-567. PMLR, 2013.
* [82] Cameron Musco, Christopher Musco, David P Woodruff, and Taisuke Yasuda. Active linear regression for \(\ell_{p}\) norms and beyond. In _2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS)_, 2022.
* [83] Michael B Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the current matrix multiplication time. _Journal of the ACM (JACM)_, 68(1), 2021.
* [84] Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_. Cambridge university press, 2020.

## Appendix A Omitted proofs: general technical results

**Fact A.1** (Crude sensitivity approximation via leverage scores).: _Given a matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\), let \(\boldsymbol{\sigma}_{1}(\mathbf{a}_{i})\) denote the \(i^{\mathrm{th}}\)\(\ell_{1}\) sensitivity of \(\mathbf{A}\) with respect to \(\mathbf{A}\), and let \(\tau_{i}(\mathbf{A})\) denote its \(i^{\mathrm{th}}\) leverage score with respect to \(\mathbf{A}\). Then we have \(\sqrt{\frac{\tau_{i}(\mathbf{A})}{n}}\leq\boldsymbol{\sigma}_{1}(\mathbf{a}_ {i})\leq\sqrt{\tau_{i}(\mathbf{A})}\)._

Proof.: The proof of this claim follows from a simple application of standard norm inequalities, and we present one here for completeness. For any \(\mathbf{u}\in\mathbb{R}^{n}\), we have \(\|\mathbf{u}\|_{2}\leq\|\mathbf{u}\|_{1}\leq\sqrt{n}\|\mathbf{u}\|_{2}\). Therefore, for any \(\mathbf{x}\in\mathbb{R}^{d}\), we have

\[\frac{|\mathbf{x}^{\top}\mathbf{a}_{i}|}{\sqrt{n}\|\mathbf{A}\mathbf{x}\|_{2} }\leq\frac{|\mathbf{x}^{\top}\mathbf{a}_{i}|}{\|\mathbf{A}\mathbf{x}\|_{1}} \leq\frac{|\mathbf{x}^{\top}\mathbf{a}_{i}|}{\|\mathbf{A}\mathbf{x}\|_{2}}.\] (A.1)

Let \(\mathbf{x}_{\tau}\) be the vector that realizes the \(i^{\mathrm{th}}\) leverage score, and \(\mathbf{x}_{\sigma}\) be the vector that realizes the \(i^{\mathrm{th}}\)\(\ell_{1}\) sensitivity (i.e. they are the maximizers of sensitivity). Then, we may conclude from Inequality (A.1) that

\[\sqrt{\frac{|\mathbf{x}_{\tau}^{\top}\mathbf{a}_{i}|^{2}}{n\|\mathbf{A} \mathbf{x}_{\tau}\|_{2}^{2}}}\leq\frac{|\mathbf{x}_{\tau}^{\top}\mathbf{a}_{i }|}{\|\mathbf{A}\mathbf{x}_{\tau}\|_{1}}\leq\frac{|\mathbf{x}_{\sigma}^{\top} \mathbf{a}_{i}|^{2}}{\|\mathbf{A}\mathbf{x}_{\sigma}\|_{1}}\leq\sqrt{\frac{| \mathbf{x}_{\sigma}^{\top}\mathbf{a}_{i}|^{2}}{\|\mathbf{A}\mathbf{x}_{\sigma }\|_{2}^{2}}}\leq\sqrt{\frac{|\mathbf{x}_{\tau}^{\top}\mathbf{a}_{i}|^{2}}{\| \mathbf{A}\mathbf{x}_{\tau}\|_{2}^{2}}},\]

which gives the claimed result. 

**Fact A.2**.: _Given an \(n\times d\) matrix, the cost of computing \(k\) of its \(\ell_{1}\) sensitivities is \(\widetilde{O}(\mathbf{n}\mathbf{n}\mathbf{z}(\mathbf{A})+k\cdot d^{\omega})\)._

Proof.: Given an \(n\times d\) matrix \(\mathbf{A}\), we first compute \(\mathbf{S}\mathbf{A}\), its \(\ell_{1}\) subspace embedding of size \(O(d)\times d\), and compute the \(i^{\mathrm{th}}\) sensitivity of \(\mathbf{A}\) as follows.

\[\boldsymbol{\sigma}_{1}^{\mathbf{A}}(\mathbf{a}_{i})=\max_{\mathbf{x}\in \mathbb{R}^{d}}\frac{|\mathbf{a}_{i}^{\top}\mathbf{x}|}{\|\mathbf{A}\mathbf{x} \|_{1}}=\max_{\mathbf{x}\in\mathbb{R}^{d}}\frac{|\mathbf{a}_{i}^{\top}\mathbf{ x}|}{\Theta(\|\mathbf{S}\mathbf{A}\mathbf{x}\|_{1})}\approx_{O(1)}\boldsymbol{\sigma}_{1}^{ \mathbf{S}\mathbf{A}}(\mathbf{a}_{i}),\]

where the second step is by the definition of \(\ell_{1}\) subspace embedding (cf. Definition 2.3). To compute \(\boldsymbol{\sigma}_{1}^{\mathbf{S}\mathbf{A}}(\mathbf{a}_{i})\), we need to compute \(\min_{\mathbf{a}_{i}^{\top}\mathbf{x}=1}\|\mathbf{S}\mathbf{A}\mathbf{x}\|_{1}\), which, by introducing a new variable for each of the rows of \(\mathbf{S}\mathbf{A}\mathbf{x}\), can be transformed into a linear program with \(O(d)\) variables and \(d\) constraints. To see the claim on runtime, the cost of computing the subspace embedding is \(\mathbf{n}\mathbf{n}\mathbf{z}(\mathbf{A})\). Having computed this once, we can then solve \(k\) linear programs, each at cost \(d^{\omega}\) (which is the current fastest LP solver [83]). Putting together these two costs gives the claimed runtime. 

**Lemma A.3**.: _Given positive numbers \(a_{1},a_{2},\ldots,a_{m}\), let \(r:=\frac{\max_{i\in[m]}a_{i}}{\min_{i\in[m]}a_{i}}\) and \(A^{\text{(true)}}:=\sum_{i\in[m]}a_{i}\). Suppose we sample, uniformly at random with replacement, a set \(S\in[m]\) of these numbers, and let \(A^{\text{(ext)}}:=\frac{m}{|S|}\cdot\sum_{:a_{i}\in S}a_{i}\). Then, if \(|S|\geq 10\left(\frac{r(1+\gamma)}{\gamma^{2}}\log\left(\frac{1}{\delta} \right)\right)\) for a large enough absolute constant \(C\), we can ensure with at least a probability of \(1-\delta\), that \(|A^{\text{(ext)}}-A^{\text{(true)}}|\leq\gamma A^{\text{(true)}}\)._

Proof.: By construction, the expected value of a sample \(a_{i}\) in \(S\) is \(\frac{1}{m}A^{\text{(true)}}\). Denoting by \(\mathcal{P}\) the uniform distribution over \(a_{1},a_{2},\ldots,a_{m}\), and let \(\sigma(\mathcal{P})\) be the standard deviation of this distribution \(\mathcal{P}\). Then, we can apply Bernstein's inequality [84, Theorem \(2.8.1\)] to see that the absolute error \(|A^{\text{(ext)}}-A^{\text{(true)}}|\) satisfies the following guarantee for all \(t>0\):

\[\Pr\left\{|A^{\text{(est)}}-A^{\text{(true)}}|\geq t\right\} =\Pr\left\{\left|\sum_{:a_{i}\in S}\left(a_{i}-\frac{1}{m}A^{ \text{(true)}}\right)\right|\geq t\cdot\frac{|S|}{m}\right\}\] \[\leq e^{-\left\{\frac{1}{2}t^{2}\cdot\frac{|S|^{2}}{m^{2}}\cdot \frac{1}{3}\cdot\frac{|S|}{m}\cdot\max_{i\in[m]}a_{i}}\right\}\] \[=e^{-\left\{\frac{1}{2}t^{2}\cdot|S|}{m^{2}\cdot\sigma(\mathcal{P}) ^{2}+\frac{1}{3}\cdot m\cdot t\cdot\max_{i\in[m]}a_{i}}\right\}.\] (A.2)Since each of the \(a_{i}\)s is positive, we note that

\[\sigma(\mathcal{P})^{2}=\frac{1}{m}\sum_{i\in[m]}\left(a_{i}-\frac{A^{\text{(true )}}}{m}\right)^{2}\leq\frac{1}{m}\sum_{i\in[m]}a_{i}^{2}\leq\frac{1}{m}\cdot \max_{i\in[m]}a_{i}\cdot\sum_{i\in[m]}a_{i}=\frac{1}{m}\cdot\max_{i\in[m]}a_{i} \cdot A^{\text{(true)}}.\] (A.3)

Combining Inequality (A.2) and Inequality (A.3) for \(t=\gamma A^{\text{(true)}}\) implies that

\[\Pr\left\{\left|A^{\text{(est)}}-A^{\text{(true)}}\right|\geq\gamma A^{\text{ (true)}}\right\}\leq e^{-\left\{\frac{\frac{1}{2}\gamma^{2}A^{\text{(true)}} \left|S\right|}{m(1+\frac{1}{3}\cdot\gamma)\cdot\max_{i\in[m]}a_{i}}\right\}}\leq\delta\] (A.4)

holds for the choice of \(|S|\geq 10\left(\frac{m}{\gamma^{2}}(1+\gamma)\frac{\max_{i\in[m]}a_{i}}{A^{\text {(true)}}}\log\left(\frac{1}{\delta}\right)\right)\) yields the claimed error guarantee. 

## Appendix B Omitted proofs: \(\ell_{1}\) sensitivities

### Estimating all \(\ell_{1}\) sensitivities

**Theorem 3.1**.: _Given a full-rank matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\) and an approximation factor \(1<\alpha\ll n\), let \(\boldsymbol{\sigma}_{1}(\mathbf{a}_{i})\) be the \(\ell_{1}\) sensitivity of the \(i^{\text{th}}\) row of \(\mathbf{A}\). Then there exists an algorithm that, in time \(\widetilde{O}\left(\mathbf{n}\mathbf{n}\mathbf{z}(\mathbf{A})+\frac{n}{ \alpha}\cdot d^{\omega}\right)\), returns \(\widetilde{\boldsymbol{\sigma}}\in\mathbb{R}_{\geq 0}^{n}\) such that with high probability, for each \(i\in[n]\),_

\[\boldsymbol{\sigma}_{1}(\mathbf{a}_{i})\leq\widetilde{\boldsymbol{\sigma}}_{i }\leq O(\boldsymbol{\sigma}_{1}(\mathbf{a}_{i})+\frac{\alpha}{n}\mathfrak{S}_{ 1}(\mathbf{A})).\] (3.1)

Proof.: Note that in Line 2 of Algorithm 1, the rows of \(\mathbf{A}\) are partitioned into randomly created \(n/\alpha\) blocks. Suppose \(\mathbf{a}_{i}\), the \(i^{\text{th}}\) row of \(\mathbf{A}\), falls into the block \(\mathbf{B}_{\ell}\). Recall, that in Line 5, the rows from \(\mathbf{B}_{\ell}\) are mapped to those in \(\mathbf{P}\) with row indices in the set \(J\). Then, we compute the \(i^{\text{th}}\)\(\ell_{1}\) sensitivity estimate as \(\widetilde{\boldsymbol{\sigma}}_{i}=\max_{j\in J}\boldsymbol{\sigma}_{1}^{ \mathbf{SA}}(\mathbf{p}_{j})\). We observe that for all \(\mathbf{x}\in\mathbb{R}^{d}\),

\[\|\mathbf{SA}\mathbf{x}\|_{1}=\Theta(\|\mathbf{A}\mathbf{x}\|_{1}).\] (B.1)

We use Equation (B.1) below to establish the claimed bounds. Let \(\mathbf{x}^{\star}=\arg\max_{\mathbf{x}\in\mathbb{R}^{d}}\frac{|\mathbf{a}_{i} ^{\top}\mathbf{x}|}{\|\mathbf{Ax}\|_{1}}\) be the vector that realizes the \(i^{\text{th}}\)\(\ell_{1}\) sensitivity of \(\mathbf{A}\). Further, let the \(j^{\text{th}}\) row of \(\mathbf{P}\) be \(\mathbf{r}_{k}^{(\ell)}\mathbf{B}_{\ell}\). Then, with a probability of at least \(1/2\), we have

\[\boldsymbol{\sigma}_{1}^{\mathbf{SA}}(\mathbf{p}_{j})=\max_{ \mathbf{x}\in\mathbb{R}^{d}}\frac{|\mathbf{r}_{k}^{(\ell)}\mathbf{B}_{\ell} \mathbf{x}|}{\|\mathbf{SA}\mathbf{x}\|_{1}}\geq\frac{|\mathbf{r}_{k}^{(\ell)} \mathbf{B}_{\ell}\mathbf{x}^{\star}|}{\|\mathbf{SA}\mathbf{x}^{\star}\|_{1}}= \Theta(1)\frac{|\mathbf{r}_{k}^{(\ell)}\mathbf{B}_{\ell}\mathbf{x}^{\star}|}{ \|\mathbf{Ax}^{\star}\|_{1}}\geq\Theta(1)\frac{|\mathbf{a}_{i}^{\top}\mathbf{x }^{\star}|}{\|\mathbf{Ax}^{\star}\|_{1}}=\Theta(1)\boldsymbol{\sigma}_{1}( \mathbf{a}_{i}),\] (B.2)

where the first step is by definition of \(\boldsymbol{\sigma}_{1}^{\mathbf{SA}}(\mathbf{p}_{j})\), the second is by evaluating the function being maximized at a specific choice of \(\mathbf{x}\), the third step is by applying \(\|\mathbf{SA}\mathbf{x}\|_{1}=\Theta(\|\mathbf{Ax}\|_{1})\) from Equation (B.1), and the final step is by definition of \(\mathbf{x}^{\star}\) and \(\boldsymbol{\sigma}_{1}(\mathbf{a}_{i})\). For the fourth step, we use the fact that \(|\mathbf{r}_{k}^{\ell}\mathbf{B}_{\ell}\mathbf{x}^{\star}|=|\mathbf{r}_{k,i}^{ \ell}\mathbf{a}_{i}^{\star}\mathbf{x}^{\star}+\sum_{j\neq i}\mathbf{r}_{k,j}^{ \ell}(\mathbf{B}_{\ell}\mathbf{x}^{\star})_{j}|\geq|\mathbf{a}_{i}^{\top} \mathbf{x}^{\star}|\) with a probability of at least \(1/2\) since the vector \(\mathbf{r}_{k}^{\ell}\) has coordinates that are \(+1\) or \(-1\) with equal probability. By a union bound over the \(|J|\) independent rows that block \(\mathbf{B}_{\ell}\) is mapped to, we establish the claimed lower bound in Inequality (B.2) with a probability of at least \(0.9\). To show an upper bound on \(\boldsymbol{\sigma}_{1}^{\mathbf{SA}}(\mathbf{p}_{j})\), we observe that

\[\boldsymbol{\sigma}_{1}^{\mathbf{SA}}(\mathbf{p}_{j})=\max_{ \mathbf{x}\in\mathbb{R}^{d}}\frac{|\mathbf{r}_{k}^{(\ell)}\mathbf{B}_{\ell} \mathbf{x}|}{\|\mathbf{SA}\mathbf{x}\|_{1}}\leq\max_{\mathbf{x}\in\mathbb{R}^ {d}}\frac{\|\mathbf{B}_{\ell}\mathbf{x}\|_{1}}{\|\mathbf{SA}\mathbf{x}\|_{1}}= \Theta(1)\max_{\mathbf{x}\in\mathbb{R}^{d}}\frac{\|\mathbf{B}_{\ell}\mathbf{x}\|_{1 }}{\|\mathbf{Ax}\|_{1}}\leq\Theta(1)\sum_{j:\mathbf{a}_{j}\in\mathbf{B}_{ \ell}}\max_{\mathbf{x}\in\mathbb{R}^{d}}\frac{|\mathbf{a}_{j}^{\top}\mathbf{x}| }{\|\mathbf{Ax}\|_{1}},\] (B.3)

where the second step is by Holder inequality and the fact that the entries of \(\mathbf{r}_{k}^{(\ell)}\) are all bounded between \(-1\) and \(+1\), and the third step is by Equation (B.1). The final term of the above chain of inequalities may be rewritten as \(\Theta(1)\sum_{j:\mathbf{a}_{j}\in\mathbf{B}_{\ell}}s_{j}(\mathbf{A})\), by the definition of \(\mathbf{B}_{\ell}\). Because \(\mathbf{B}_{\ell}\) is a group of \(\alpha\) rows selected uniformly at random out of \(n\) rows and contains the row \(\mathbf{a}_{i}\), we have:

\[\mathbb{E}\left\{\sum_{\begin{subarray}{c}j:\mathbf{a}_{j}\in\mathbf{B}_{\ell}, \\ j\neq i\end{subarray}}\boldsymbol{\sigma}_{1}(\mathbf{a}_{j})\right\}=\frac{ \alpha-1}{n-1}\sum_{j\neq i}\boldsymbol{\sigma}_{1}(\mathbf{a}_{j}).\]Therefore, Markov inequality gives us that with a probability of at least \(0.9\), we have

\[\bm{\sigma}_{1}^{\mathbf{SA}}(\mathbf{p}_{j})\leq\bm{\sigma}_{1}(\mathbf{a}_{j})+ \mathfrak{S}_{1}(\mathbf{A})O(\tfrac{\alpha}{n}).\] (B.4)

The median trick then establishes the bounds in Inequality (B.2) and Inequality (B.4) with high probability. The claimed runtime is obtained by the cost of constructing \(\mathbf{SA}\) plus \(O(n/\alpha)\) computations of \(\ell_{1}\) sensitivities with respect to \(\mathbf{SA}\), for which we may use Fact 2.4. 

### Estimating the sum of \(\ell_{1}\) sensitivities

In this section, we present a randomized algorithm that approximates the total sensitivity \(\mathfrak{S}_{1}(\mathbf{A})\), up to a \(\gamma\)-multiplicative approximation for some \(\gamma\in(0,1)\) that is not too small. This algorithm is significantly less general than Algorithm 2 (which holds for all \(p\geq 1\)) but it uses extremely simple facts about leverage scores and what we think is a new recursion technique in this context, which we believe could be of potential independent interest.

**Theorem B.1**.: _Given a matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\) and an approximation factor \(\gamma\in(0,1)\) such that \(\gamma\geq\Omega\left(\frac{1}{n^{3}}\right)\), there exists an algorithm, which in time \(\widetilde{O}\left(\mathbf{nnz}(\mathbf{A})+\frac{d^{\omega}}{\gamma^{2}} \cdot\max\left(\sqrt{d},\tfrac{1}{\gamma^{2}}\right)\right)\), returns a positive scalar \(\widehat{s}\) such that, with a probability of \(0.99\), we have_

\[\mathfrak{S}_{1}(\mathbf{A})\leq\widehat{s}\leq(1+O(\gamma))\mathfrak{S}_{1}( \mathbf{A}).\]

```
0: Matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\) and approximation factor \(\gamma\in(0,1)\)
0: A positive scalar that satisfies, with probability \(0.99\), that \[\mathfrak{S}_{1}(\mathbf{A})\leq\widehat{s}\leq(1+O(\gamma))\mathfrak{S}_{1}( \mathbf{A})\]
1: Compute \(\mathbf{SA}\in\mathbb{R}^{O(d)\times d}\), a \(1/2\)-approximate \(\ell_{1}\) subspace embedding of \(\mathbf{A}\) (cf. Definition 2.3)
2: Set \(\rho=O(\gamma/\log(\log(n+d)))\). Compute \(\mathbf{S}^{\prime}\mathbf{A}\), a \(\rho\)-approximate \(\ell_{1}\) subspace embedding of \(\mathbf{A}\)
3: Construct a matrix \(\widehat{\mathbf{A}}\) comprising only those rows of \(\mathbf{A}\) with leverage scores at least \(1/n^{10}\)
4: Let \(s\) be the output of Algorithm 5 with inputs \(\widehat{\mathbf{A}}\), \(\mathbf{SA}\), \(\mathbf{S}^{\prime}\mathbf{A}\), \(n\), and \(\rho\)
5: Return \(\widehat{s}:=(1+\gamma)\left(s+\frac{1}{n^{5}}(|\mathbf{A}|-|\widehat{ \mathbf{A}}|)\right)\) ```

**Algorithm 4** Approximating the Sum of \(\ell_{1}\)-Sensitivities

#### b.2.1 Proof sketch.

We achieve our guarantee via Algorithm 4 and Algorithm 5, which are based on the following key principles. The first is Fact 2.2: the \(i^{\mathrm{th}}\) sensitivity and \(i^{\mathrm{th}}\) leverage score of \(\mathbf{A}\in\mathbb{R}^{n\times d}\) satisfy \(\sqrt{\frac{\tau_{i}(\mathbf{A})}{n}}\leq\bm{\sigma}_{1}(\mathbf{a}_{i})\leq \sqrt{\tau_{i}(\mathbf{A})}\). The second idea, derived from Bernstein's inequality (and formalized in Lemma A.3), is: the sum of a set of positive numbers with values bounded between \(a\) and \(b\) can be approximated to a \(\gamma\)-multiplicative factor using \(\widetilde{O}((b/a)\gamma^{-2})\) uniformly sampled (and appropriately scaled) samples. The third principle is that \(\bm{\sigma}_{1}(\mathbf{a}_{i})\approx\bm{\sigma}_{1}^{\mathbf{SA}}(\mathbf{ a}_{i})\) when \(\mathbf{SA}\) is a constant-approximation \(\ell_{1}\) subspace embedding of \(\mathbf{A}\).

Based on these ideas, we first divide the rows of \(\mathbf{A}\) into \(\mathcal{B}=\Theta(\log n)\) submatrices based on which of the \(\mathcal{B}\) buckets \([1/2,1],[1/4,1/2],[1/8,1/4]\), etc. their leverage scores (computed with respect to \(\mathbf{A}\)) fall into. Since each of these \(\mathcal{B}\) submatrices comprises rows with leverage scores in the range \([a,2a]\) for some \(a\), per Fact 2.2, we have \(\sqrt{\frac{a}{n}}\leq\bm{\sigma}_{1}(\mathbf{a}_{i})\leq\sqrt{2a}\). Therefore, by Lemma A.3, the total sensitivity of this submatrix may be approximated by the appropriately scaled total sensitivity of \(O(\sqrt{n})\) of its uniformly sampled rows. Our design choice to split rows based on leverage scores is based on the fact that leverage scores (unlike sensitivities) are efficiently computable (using [60]). Computing the total sensitivity of an \(\widetilde{O}(\sqrt{n})\)-sized matrix thus costs \(\widetilde{O}(\mathbf{nnz}(\mathbf{A})+\sqrt{n}\cdot d^{\omega})\) under this scheme.

Applying the idea sketched above recursively further reduces the cost. Suppose, after dividing the input matrix \(\mathbf{A}\) into \(\mathcal{B}\) submatrices \(\mathbf{M}_{i}\) and subsampling \(\widetilde{\mathbf{M}}_{i}\) from these matrices, we try to recurse on each of these \(\mathcal{B}\) submatrices \(\widetilde{\mathbf{M}}_{i}\). A key requirement for this idea to work is that the sum of the \(\ell_{1}\) sensitivities of the rows of \(\widetilde{\mathbf{M}}_{i}\) in isolation be close enough to the true sum of \(\ell_{1}\) sensitivities of those rows with respect to the original matrix \(\mathbf{A}\). In general, _this is not true_. However, we can meet our requirement using the \(\ell_{1}\) subspace embedding \(\mathbf{SA}\) of the (original) matrix \(\mathbf{A}\). Specifically, if we vertically concatenate a matrix \(\widetilde{\mathbf{M}}_{i}\) with \(\mathbf{SA}\) and call this matrix \(\mathbf{C}\), then for each row \(\widetilde{\mathbf{M}}_{i}[j]\in\widetilde{\mathbf{M}}_{i}\), we have \(\boldsymbol{\sigma}_{1}^{\mathbf{C}}(\widetilde{\mathbf{M}}_{i}[j])\approx \boldsymbol{\sigma}_{1}^{\mathbf{A}}(\widetilde{\mathbf{M}}_{i}[j])\). Further, \(|\mathbf{C}|\leq O(\sqrt{n})\), and so in the step subsampling the rows of \(\widetilde{\mathbf{M}}_{i}\), we choose \(O(n^{1/4})\) samples. Recursing this exponentially decreases the row dimension of the input matrix at each level, with \((\log n)^{k}\) matrices at the \(k^{\mathrm{th}}\) level of recursion.

#### b.2.2 Helper lemmas for proving Theorem b.1

For our formal proof, we need Definition b.2, Fact B.3, and Definition b.4.

**Definition B.2** (Notation for recursion tree of Algorithm 5).: _Recall that \(\mathcal{B}=\Theta(\log(n))\) as in Line 1 of Algorithm 5. Then we have the following notation corresponding to Algorithm 5._

* _We define a rooted_ \(\mathcal{B}\)_-ary tree_ \(\mathcal{T}\) _corresponding to the execution of Algorithm_ 5_._
* _We use_ \(\mathcal{T}_{j}\) _to denote the subtree starting from the root node with all the nodes up to and including those at the_ \(j^{\mathrm{th}}\) _level. We denote the_ \(i^{\mathrm{th}}\) _node at the_ \(j^{\mathrm{th}}\) _level by_ \(\mathcal{T}_{(j,i)}\)_. Thus, every node is uniquely specified by its level in the tree and its index within that level._
* _The root node_ \(\mathcal{T}_{(1,1)}\) _at level_ \(1\) _corresponds to the first call to Algorithm_ 5 _from Algorithm_ 4_._
* _For the node_ \(\mathcal{T}_{(j,i)}\)_, we denote by_ \(\mathbf{M}^{(j,i)}\) _the input matrix to the corresponding recursive call; note that all the other inputs are global and remain the same throughout the recursive call._
* _Analogously, we use_ \(\widetilde{s}_{(j,i)}\) _to denote our estimate of_ \(\mathfrak{S}_{1}^{{}^{\prime}\mathbf{A}}(\mathbf{M}^{(j,i)})\)_, the sum of_ \(\ell_{1}\) _sensitivities of rows of the matrix_ \(\mathbf{M}^{(j,i)}\) _in the node_ \(\mathcal{T}_{(j,i)}\)_, defined with respect to the sketched matrix_ \(\mathbf{S}^{\prime}\mathbf{A}\)_._

**Fact B.3**.: _If event \(A\) happens with probability \(1-\tau_{1}\) and event \(B\) happens with probability \(1-\tau_{2}\) conditioned on event \(A\), then the probability of both events \(A\) and \(B\) happening is at least \(1-\tau_{1}-\tau_{2}\)._

**Definition B.4**.: _We say that a node \(\mathcal{N}\) with matrix \(\mathbf{M}\) satisfies the \((\widehat{\rho},\widehat{\delta})\)-approximation guarantee if \(\widehat{s}\), the output of Algorithm 5 on \(\mathbf{M}\), satisfies the following guarantee for the true sum \(\mathfrak{S}_{1}^{{}^{\prime}\mathbf{A}}(\mathbf{M})\) of \(\ell_{1}\) sensitivities of the rows in \(\mathbf{M}\):_

\[\widehat{s}\approx_{\widehat{\rho}}\mathfrak{S}_{1}^{{}^{\prime}\mathbf{A}}( \mathbf{M})\text{ with a probability at least }1-\widehat{\delta}\]_for some approximation parameter \(\widehat{\rho}\in(0,1)\) and an error probability parameter \(\widehat{\delta}\in(0,1)\)._

**Lemma B.5** (Partitioning of \(\mathbf{M}\)).: _In Algorithm 5, even though we ignore the range \(\left[0,\frac{1}{n^{20}}\right]\) when creating the submatrices \(\mathbf{M}_{1},\mathbf{M}_{2},\ldots,\mathbf{M}_{\mathcal{B}}\) from \(\mathbf{M}\), every row in \(\mathbf{M}\) is part of exactly one of the submatrices \(\mathbf{M}_{1},\mathbf{M}_{2},\ldots,\mathbf{M}_{\mathcal{B}}\). In other words, the matrix \(\mathbf{M}\) is partitioned into the matrices \(\mathbf{M}_{1},\mathbf{M}_{2},\ldots,\mathbf{M}_{\mathcal{B}}\), with no row missed out._

Proof.: Our proof strategy is to show that for all row indices \(k\in[|\mathbf{M}|]\), each of the leverage scores \(\tau_{k}(\mathbf{C})\) satisfy \(\tau_{k}(\mathbf{C})\geq\frac{1}{n^{20}}\). Therefore, creating buckets of rows with the range of leverage score starting at \(\frac{1}{n^{20}}\) (instead of \((0)\) does not miss out any rows.

To this end, we note that for every row \(\mathbf{a}_{k}\) in the input \(\mathbf{M}\) to Algorithm 5, we have:

\[\sqrt{\frac{1}{n^{11}}}\leq\sqrt{\frac{\tau_{k}(\mathbf{A})}{|\mathbf{A}|}} \leq\boldsymbol{\sigma}_{1}^{\mathbf{A}}(\mathbf{a}_{k})=\max_{\mathbf{x}\in \mathbb{R}^{d}}\frac{|\mathbf{x}^{\top}\mathbf{a}_{k}|}{\|\mathbf{A}\mathbf{x} \|_{1}}\leq\max_{\mathbf{x}\in\mathbb{R}^{d}}3\frac{|\mathbf{x}^{\top}\mathbf{ a}_{k}|}{\|\mathbf{C}\mathbf{x}\|_{1}}=3\boldsymbol{\sigma}_{1}^{\mathbf{C}}( \mathbf{a}_{k})\leq 3\sqrt{\tau_{k}(\mathbf{C})},\] (B.5)

where the first step is because in Line 3 of Algorithm 4, we discard those rows \(\mathbf{a}_{k}\) of \(\mathbf{A}\) with leverage scores smaller than \(1/n^{10}\) and there are \(n\) rows in \(\mathbf{A}\), so every row \(\mathbf{a}_{k}\) in the input matrix \(\mathbf{M}\) to Algorithm 5 satisfies \(\tau_{k}(\mathbf{A})\geq\frac{1}{n^{10}}\); the second step is by Fact 2.2 applied to \(\mathbf{a}_{k}\); the third step is by the definition of the \(\ell_{1}\) sensitivity of \(\mathbf{a}_{k}\) with respect to \(\mathbf{A}\); the fourth step is by observing that \(\|\mathbf{C}\mathbf{x}\|_{1}=\|\mathbf{S}\mathbf{A}\mathbf{x}\|_{1}+\|\mathbf{ M}\mathbf{x}\|_{1}\leq 2\|\mathbf{A}\mathbf{x}\|_{1}+\|\mathbf{A}\mathbf{x}\|_{1}= 3\|\mathbf{A}\mathbf{x}\|_{1}\) since \(\mathbf{S}\mathbf{A}\) is a constant-factor (with the constant assumed \(1/2\)) \(\ell_{1}\) subspace embedding of \(\mathbf{A}\) and because \(\mathbf{M}\) is a submatrix of \(\mathbf{A}\); the fifth step is by the definition of \(\ell_{1}\) sensitivity of \(\mathbf{a}_{k}\) with respect to \(\mathbf{C}\); the final step is by Fact 2.2 applied to \(\mathbf{a}_{k}\) within \(\mathbf{C}\). Observing the first and last terms of this inequality chain, we have \(\tau_{k}(\mathbf{C})\geq\frac{1}{3n^{17}}\) for all \(k\in[|\mathbf{C}|]\). This finishes the proof of the claim. 

**Lemma B.6**.: _For the recursion tree \(\mathcal{T}\) corresponding to Algorithm 5, let the size \(b\) of the input matrices at the leaf nodes be \(b=\frac{50D^{3}\log(100\mathcal{B})}{\gamma^{2}}\cdot\left(\frac{2D^{3}\log(1 00\mathcal{B})}{\gamma^{2}}+\sqrt{d\log(d)}\right)\). Then, the depth of the recursion tree is less than \(D:=1+\log(\log(2n+2d\log(d)))\)._

Proof.: For now, let \(\beta\) be arbitrary but less than \(\beta^{2}\leq n+d^{\prime}\), where \(d^{\prime}\) is the row dimension of the subspace embedding \(\mathbf{S}\mathbf{A}\). Then, the size of the input matrix to the nodes of the recursion tree as we increase in depth evolves as

\[\beta\sqrt{d^{\prime}+n},\;\beta\sqrt{d^{\prime}+\beta\sqrt{d^{\prime}+n}},\; \beta\sqrt{d^{\prime}+\beta\sqrt{d^{\prime}+\beta\sqrt{d^{\prime}+n}}}.\ldots.\]

Thus, this sequence evolves as

\[x_{t+1}=\beta\sqrt{d^{\prime}+x_{t}},\text{ with }x_{0}=n+d^{\prime}.\]

To see the limit of this sequence, we set \(x=\beta\sqrt{d^{\prime}+x}\), and solve for this quadratic equation as

\[x=\frac{\beta^{2}+\sqrt{\beta^{4}+4\beta^{2}d^{\prime}}}{2}\leq\beta(\beta+ \sqrt{d^{\prime}}).\] (B.6)

We set the base case to be \(b^{\star}:=7\beta(\beta+\sqrt{d^{\prime}})\), a constant times larger than this asymptotic limit. We now compute the depth \(t^{\star}\) at which the recursion attains the value \(b^{\star}\) is attained. To do so, we consider a sequence \(\{y_{t}\}\) which overestimates the sequence \(\{x_{t}\}\) of matrix sizes for a big range of \(t\). We define \(\{y_{t}\}\) as

\[y_{0}=n+d^{\prime},\text{ and }y_{t+1}=\beta\sqrt{2y_{t}},\]

and it can be checked that for \(t\) such that \(y_{t}\geq d^{\prime}\), we have \(y_{t+1}\geq x_{t+1}\). Moreover, for all \(t\geq 1\), \(y_{t}=\beta^{1+1/2+\cdots+1/2^{t-1}}(2y_{0})^{1/2^{t}}=\beta^{2-2^{1-t}}(2y_{0} )^{1/2^{t}}\). The limit of this sequence can easily be seen to \(\beta^{2}\). We now split our analysis in two cases based on the limits of these two sequences.

Case 1: \(\beta^{2}\geq d^{\prime}\).Since the limit of the sequence \(\{y_{t}\}\) is \(\beta^{2}\), it is easy to see that \(y_{t}\) is always greater \(d^{\prime}\) (since \(y_{t}\) is monotonically decreasing as \(y_{0}\geq\beta^{2}\)). Thus, \(x_{t}\) will always be less than \(y_{t}\) in this case. We will now show that \(y_{t}\) (and thus \(x_{t}\)) will be less than \(b^{\star}\) in \(t_{0}=2\log\log(2y_{0})\) steps. Solving the inequality for \(t\) as follows: \(y_{t}\leq\beta^{2}(2y_{0})^{1/2^{t}}\leq 4\beta^{2}\leq b^{\star}\), it suffices to ensure that \((2y_{0})^{1/2^{t}}\leq 4\), which happens if \(t\geq t_{0}\). Thus, \(t^{\star}\leq 2\log\log 2y_{0}\).

Case \(2\): \(\beta^{2}<d^{\prime}\).In this case, the sequence \(y_{t}\) will eventually go below \(d^{\prime}\) and thus the inequality \(y_{t}\geq x_{t}\) might eventually break down. Let \(t=t^{\prime}\) denote the first time step at which \(y_{t}\leq 5d^{\prime}\) (which will be finite in this case). Then, by definition, \(y_{t}\geq x_{t}\) for all \(t<t^{\prime}\). We will now upper bound the value of \(t^{\prime}\) as follows: \(y_{t}\leq\beta^{2}(2y_{0})^{1/2^{t}}\leq d^{\prime}(2y_{0})^{1/2^{t}}\leq 4d^{ \prime}\) ; Thus, it suffices to ensure that \((2y_{0})^{1/2^{t}}\leq 4\), which happens if \(t\geq t_{0}:=2\log\log(2y_{0})\). Hence, \(t^{\prime}\leq t_{0}\). Moreover, the value of \(y_{t^{\prime}}\leq 5d^{\prime}\), implies that \(x_{t^{\prime}-1}\leq y_{t^{\prime}-1}\leq\frac{(5d^{\prime})^{2}}{2\beta^{2}}\). We will now show that \(x_{t^{\prime}+1}\leq b^{\star}\) using the evolution of \(x_{t+1}=\beta\sqrt{d^{\prime}+x_{t}}\) as follows: First, \(x_{t^{\prime}}\leq\beta\sqrt{d^{\prime}}+\beta\sqrt{x_{t^{\prime}-1}}\leq \beta\sqrt{d^{\prime}}+5d^{\prime}\leq 6d^{\prime}\); Then, \(x_{t^{\prime}+1}\leq\beta\sqrt{d^{\prime}}+\beta\sqrt{6d^{\prime}}\leq 7 \beta\sqrt{d^{\prime}}\leq b^{\star}\). Thus, \(t^{\star}\leq 1+2\log\log 2y_{0}\).

Therefore, in both of these cases, we have that after at most \(D=1+\log\log(2n+2d^{\prime})\) depth, the base case of \(b^{\star}=7\beta(\beta+\sqrt{d^{\prime}})\) is achieved at the leaf node. Recall that the algorithm sets \(D=2\log\log(n+d^{\prime})+1\) and \(\rho=\gamma/D\) for \(d^{\prime}=d\log(d)\) which implies that \(\beta:=C\frac{(1+\rho)}{\rho^{2}}\log(\delta^{-1})=CD\frac{(D+\gamma)}{\gamma^ {2}}\log(100\mathcal{B}^{D})\leq\frac{2D^{3}\log(100\mathcal{B})}{\gamma^{2}}\). Since Algorithm5 sets \(b=\frac{50D^{3}\log(100\mathcal{B})}{\gamma^{2}}\cdot\left(\frac{2D^{3}\log(1 00\mathcal{B})}{\gamma^{2}}+\sqrt{d\log(d)}\right)\), which is larger than \(b^{\star}\), we may conclude that the choice of \(b\) implies that the recursion tree \(\mathcal{T}\) of Algorithm5 has a depth less than \(D=1+\log(\log(2n+2d\log(d)))\).

**Lemma B.7**.: _Suppose the root node \(\mathcal{T}_{(1,1)}\) of Algorithm5 satisfies a \((\widehat{\rho},\widehat{\delta})\)-approximation guarantee as defined in DefinitionB.4, and denote by \(\widehat{s}\) the output of Algorithm4. Then, with a probability of at least \(1-\widehat{\delta}\), we have_

\[\frac{1}{1+\gamma}\widehat{s}\approx_{\widehat{\rho}+\rho}\mathfrak{S}_{1}^{ \mathbf{A}}(\mathbf{A}).\]

As a result of this lemma, in the final proof of correctness of Algorithm4 (appearing later as proof of TheoremB.1), it suffices to show that \(\widehat{s}\) satisfies a \((\widehat{\rho},\widehat{\delta})\)-approximation guarantee for some appropriate choices of these parameters.

Proof of LemmaB.7.: Suppose the root node satisfies DefinitionB.4 with some \((\widehat{\rho},\widehat{\delta})\)-approximation guarantee. This means that there exists an \(\widetilde{s}\) such that the output of Algorithm5 on the input matrix \(\widehat{\mathbf{A}}\) satisfies

\[\widetilde{s}\approx_{\widehat{\rho}}\mathfrak{S}_{1}^{\mathbf{S}^{\prime} \mathbf{A}}(\widehat{\mathbf{A}})\text{ with a probability at least }1-\widehat{\delta}.\] (B.7)

Since \(\mathbf{S}^{\prime}\mathbf{A}\) is, by design, a \(\rho\)-approximation \(\ell_{1}\) subspace embedding for \(\mathbf{A}\), it means for all \(\mathbf{x}\in\mathbb{R}^{d}\) we have

\[(1-\rho)\|\mathbf{A}\mathbf{x}\|_{1}\leq\|\mathbf{S}^{\prime}\mathbf{A} \mathbf{x}\|_{1}\leq(1+\rho)\|\mathbf{A}\mathbf{x}\|_{1}.\] (B.8)

Therefore, for every row \(\mathbf{a}_{i}\in\widehat{\mathbf{A}}\), we have by the definition of \(\ell_{1}\) sensitivity and Inequality(B.8) that the \(\ell_{1}\) sensitivity of \(\mathbf{a}_{i}\) computed with respect to \(\mathbf{S}^{\prime}\mathbf{A}\) is a \(\rho\)-approximation of the \(\ell_{1}\) sensitivity of \(\mathbf{a}_{i}\) with respect to \(\mathbf{A}\):

\[\boldsymbol{\sigma}_{1}^{\mathbf{S}^{\prime}\mathbf{A}}(\mathbf{a}_{i})=\max_{ \mathbf{x}\in\mathbb{R}^{d}}\frac{|\mathbf{a}_{i}^{\top}\mathbf{x}|}{\| \mathbf{S}^{\prime}\mathbf{A}\mathbf{x}\|_{1}}\approx_{\rho}\max_{\mathbf{x }\in\mathbb{R}^{d}}\frac{|\mathbf{a}_{i}^{\top}\mathbf{x}|}{\|\mathbf{A}\mathbf{ x}\|_{1}}=\boldsymbol{\sigma}_{1}^{\mathbf{A}}(\mathbf{a}_{i}).\]

Therefore, by linearity, this \(\rho\)-approximation factor transfers over in connecting the total sensitivity of \(\widehat{\mathbf{A}}\) with respect to \(\mathbf{S}^{\prime}\mathbf{A}\) to the total sensitivity of \(\widehat{\mathbf{A}}\) with respect to \(\mathbf{A}\):

\[\mathfrak{S}_{1}^{\mathbf{S}^{\prime}\mathbf{A}}(\widehat{\mathbf{A}})=\sum_{i \in\|\widehat{\mathbf{A}}\|}\boldsymbol{\sigma}_{1}^{\mathbf{S}^{\prime} \mathbf{A}}(\mathbf{a}_{i})\approx_{\rho}\sum_{i\in\|\widehat{\mathbf{A}}\|} \boldsymbol{\sigma}_{1}^{\mathbf{A}}(\mathbf{a}_{i})=\mathfrak{S}_{1}^{\mathbf{A} }(\widehat{\mathbf{A}}).\] (B.9)

Therefore, chaining EquationB.7 and EquationB.9 yields the following approximation guarantee:

\[\widetilde{s}\approx_{\widehat{\rho}+\rho}\mathfrak{S}_{1}^{\mathbf{A}}( \widehat{\mathbf{A}})\text{ with a probability at least }1-\widehat{\delta}.\] (B.10)

There is no change in the error probability above from EquationB.7 because EquationB.9 holds deterministically. Next, since in Line3 of Algorithm4, we dropped rows of \(\mathbf{A}\) that have leverage scores less than \(\frac{1}{n^{10}}\), for each of the dropped rows \(\mathbf{a}_{i}\), we have

\[\boldsymbol{\sigma}_{1}^{\mathbf{A}}(\mathbf{a}_{i})\leq\sqrt{\tau_{i}(\mathbf{A })}\leq\frac{1}{n^{5}}.\] (B.11)Therefore, the quantity we return, \(\widehat{s}\), satisfies the following approximation guarantee:

\[\frac{1}{1+\gamma}\widehat{s}:=\widetilde{s}+\frac{1}{n^{5}}(|\mathbf{A}|-| \widehat{\mathbf{A}}|)\geq(1-(\widehat{\rho}+\rho))\mathfrak{S}_{1}^{\mathbf{A }}(\widehat{\mathbf{A}})+\sum_{i:\mathbf{a}_{i}\notin\widehat{\mathbf{A}}} \boldsymbol{\sigma}_{1}^{\mathbf{A}}(\mathbf{a}_{i})\geq(1-(\widehat{\rho}+ \rho))\mathfrak{S}_{1}^{\mathbf{A}}(\mathbf{A}),\] (B.12)

where the first step is by definition of \(\widehat{s}\) in Algorithm 4; the second step is by Equation (B.10); the third step is by applying Inequality (B.11) to each of the rows not present in \(\widehat{\mathbf{A}}\). In the other direction,

\[\frac{1}{1+\gamma}\widehat{s}=\widetilde{s}+\frac{1}{n^{5}}(|\mathbf{A}|-| \widehat{\mathbf{A}}|)\leq(1+(\widehat{\rho}+\rho))\mathfrak{S}_{1}^{\mathbf{ A}}(\widehat{\mathbf{A}})+\frac{1}{n^{4}}\leq(1+(\widehat{\rho}+\rho))\mathfrak{S}_{1}^ {\mathbf{A}}(\mathbf{A}),\] (B.13)

where the first step is by definition of \(\widehat{s}\) in Algorithm 4; the second step is by the fact that \(|\mathbf{A}|\leq n\); the final step is by combining the assumption \(\rho\geq\frac{1}{n^{4}}\) and the facts that all sensitivities are non-negative and that the total \(\ell_{1}\) sensitivity is at least one. Combining Inequality (B.12) and Inequality (B.13) finishes the proof. 

**Lemma B.8** (Single-Level Computation in Recursion Tree).: _Consider a node \(\mathcal{N}\) in the recursion tree corresponding to Algorithm 5, and let \(\mathbf{M}\) be the input matrix for \(\mathcal{N}\). Further assume that \(\mathcal{N}\) is not a leaf node. Denote by \(\mathcal{N}_{1},\mathcal{N}_{2},\ldots,\mathcal{N}_{\mathcal{B}}\) (where \(\mathcal{B}=\Theta(\log(n))\) is the number of recursive calls at each node in Algorithm 5) the children of node \(\mathcal{N}\), with each node \(\mathcal{N}_{i}\) with the input matrix \(\widetilde{\mathbf{M}}_{i}\) (as constructed in Algorithm 5). Suppose each of these child nodes \(\mathcal{N}_{i}\) satisfies a \((\widehat{\rho},\widehat{\delta})\)-approximation guarantee (cf. Definition B.4). Then \(\mathcal{N}\) satisfies a \((\widehat{\rho}+\rho,(\widehat{\delta}+\delta)\cdot\mathcal{B})\)-approximation guarantee._

Proof.: We first reiterate the steps of Algorithm 5 that are crucial to this proof. As proved in Lemma B.5, Algorithm 5 partitions the rows of the matrix \(\mathbf{M}\) in node \(\mathcal{N}\) into \(\mathcal{B}\) submatrices \(\mathbf{M}_{1},\mathbf{M}_{2},\ldots,\mathbf{M}_{\mathcal{B}}\) based on their leverage scores. Next, for each matrix \(\mathbf{M}_{i}\), we uniformly sample with replacement \(O(\sqrt{|\mathbf{C}|}(1+\rho)\rho^{-2}\log(\delta^{-1}))\) rows and term this set of rows as matrix \(\widetilde{\mathbf{M}}_{i}\), which we then pass to Algorithm 5 recursively in node \(\mathcal{N}_{i}\) (recall that we assume that the node \(\mathcal{N}\) is not a leaf node). In other words, each child node \(\mathcal{N}_{i}\) has as its input matrix \(\widetilde{\mathbf{M}}_{i}\).

Since we assume that all the child nodes \(\mathcal{N}_{1},\mathcal{N}_{2},\ldots,\mathcal{N}_{\mathcal{B}}\) satisfy a \((\widehat{\rho},\widehat{\delta})\)-approximation guarantee as in Definition B.4, it means that, for all \(i\in[\mathcal{B}]\), the output \(\widehat{s}_{i}\) of Algorithm 5 on node \(\mathcal{N}_{i}\) satisfies the following guarantee:

\[\widehat{s}_{i}\approx_{\widehat{\rho}}\mathfrak{S}_{1}^{\mathbf{S}^{ \prime}\mathbf{A}}(\widetilde{\mathbf{M}}_{i})\text{ with a probability at least }1- \widehat{\delta}.\] (B.14)

Next, we recall that each \(\mathbf{M}_{i}\) is composed of only those rows of \(\mathbf{M}\) whose leverage scores (computed with respect to \(\mathbf{C}\)) lie in the \(i^{\mathrm{th}}\) bucket \([2^{-i},2^{-i+1}]\). Combining this with creftype 2.2, this implies that the \(\ell_{1}\) sensitivity of the \(j^{\mathrm{th}}\) row \(\mathbf{M}_{i}[j]\) of the matrix \(\mathbf{M}_{i}\), computed with respect to \(\mathbf{C}\), satisfies

\[\sqrt{\frac{2^{-i}}{|\mathbf{C}|}}\leq\sqrt{\frac{\tau_{j}^{\mathbf{C}}( \mathbf{M}_{i})}{|\mathbf{C}|}}\leq\boldsymbol{\sigma}_{1}^{\mathbf{C}}( \mathbf{M}_{i}[j])\leq\sqrt{\tau_{j}^{\mathbf{C}}(\mathbf{M}_{i})}\leq\sqrt{ 2^{-i+1}}.\] (B.15)

Additionally, we claim that

\[\frac{1}{3(1+\rho)}\boldsymbol{\sigma}_{1}^{\mathbf{C}}(\mathbf{M}_{i}[j]) \leq\boldsymbol{\sigma}_{1}^{\mathbf{S}^{\prime}\mathbf{A}}(\mathbf{M}_{i}[j ])\leq 3(1+\rho)\boldsymbol{\sigma}_{1}^{\mathbf{C}}(\mathbf{M}_{i}[j]).\] (B.16)

To see this, we observe that

\[\boldsymbol{\sigma}_{1}^{\mathbf{C}}(\mathbf{M}_{i}[j])=\max_{\mathbf{x}\in \mathbb{R}^{d}}\frac{|\mathbf{x}^{\top}\mathbf{M}_{i}[j]|}{\|\mathbf{C}\mathbf{ x}\|_{1}}\geq\max_{\mathbf{x}\in\mathbb{R}^{d}}\frac{|\mathbf{x}^{\top} \mathbf{M}_{i}[j]|}{3\|\mathbf{A}\mathbf{x}\|_{1}}\geq\max_{\mathbf{x}\in \mathbb{R}^{d}}\frac{|\mathbf{x}^{\top}\mathbf{M}_{i}[j]|}{3(1+\rho)\| \mathbf{S}^{\prime}\mathbf{A}\mathbf{x}\|_{1}}=\frac{\boldsymbol{\sigma}_{1}^{ \mathbf{S}^{\prime}\mathbf{A}}(\mathbf{M}_{i}[j])}{3(1+\rho)},\] (B.17)

where the first step is by the definition of \(\ell_{1}\) sensitivity of \(\mathbf{M}_{i}[j]\) (i.e., the \(j^{\mathrm{th}}\) row of matrix \(\mathbf{M}_{i}\)) with respect to \(\mathbf{C}\); the second step is by the definition of \(\mathbf{C}\), the fact that \(\mathbf{SA}\) is a \(1/2\)-factor \(\ell_{1}\) subspace embedding of \(\mathbf{A}\), and because \(\mathbf{M}\) is a subset of rows of \(\mathbf{A}\); the third step is because \(\mathbf{S}^{\prime}\mathbf{A}\) is a \(\rho\)-approximate \(\ell_{1}\) subspace embedding for \(\mathbf{A}\), and the final step is by the definition of \(\boldsymbol{\sigma}_{1}^{\mathbf{A}}(\mathbf{M}_{i}[j])\). In the other direction, we have

\[\boldsymbol{\sigma}_{1}^{\mathbf{C}}(\mathbf{M}_{i}[j])=\max_{\mathbf{x}\in \mathbb{R}^{d}}\frac{|\mathbf{x}^{\top}\mathbf{M}_{i}[j]|}{\|\mathbf{C}\mathbf{ x}\|_{1}}\leq\max_{\mathbf{x}\in\mathbb{R}^{d}}\frac{|\mathbf{x}^{\top} \mathbf{M}_{i}[j]|}{0.5\|\mathbf{A}\mathbf{x}\|_{1}}\leq\max_{\mathbf{x}\in \mathbb{R}^{d}}\frac{(1+\rho)|\mathbf{x}^{\top}\mathbf{M}_{i}[j]|}{0.5\| \mathbf{S}^{\prime}\mathbf{A}\mathbf{x}\|_{1}}\leq 3(1+\rho)\boldsymbol{\sigma}_{1}^{\mathbf{S}^{\prime}\mathbf{A}}( \mathbf{M}_{i}[j]),\] (B.18)where the second step is by the definition of \(\mathbf{C}\) as a vertical concatenation of \(\mathbf{SA}\) and \(\mathbf{M}\) and further using that \(\mathbf{SA}\) is a constant factor \(\ell_{1}\) subspace embedding of \(\mathbf{A}\) and dropping the non-negative term \(\|\mathbf{M}\mathbf{x}\|_{1}\). Combining Inequality (B.15) and Inequality (B.16), we see that for all rows \(j\in\|\mathbf{M}_{i}\|\), we have

\[\frac{1}{3(1+\rho)}\sqrt{\frac{2^{-i}}{|\mathbf{C}|}}\leq\bm{\sigma}_{1}^{ \mathbf{S}^{\prime}\mathbf{A}}(\mathbf{M}_{i}[j])\leq 3(1+\rho)\sqrt{2^{-i+1}}.\] (B.19)

Since Inequality (B.19) shows upper and lower bounds on each sensitivity \(\bm{\sigma}_{1}^{\mathbf{S}^{\prime}\mathbf{A}}(\mathbf{M}_{i})\) of the matrix \(\mathbf{M}_{i}\), we may use Lemma A.3 to approximate the sum of these sensitivities. In particular, by Lemma A.3, we may construct a matrix \(\widetilde{\mathbf{M}}_{i}\) composed of \(\frac{20\sqrt{|C|}(1+2\rho)\log(\delta^{-1})}{\rho^{2}}\) rows of \(\mathbf{M}_{i}\) sampled uniformly at random with replacement, and we are guaranteed

\[(1+\rho)\frac{|\mathbf{M}_{i}|}{|\widetilde{\mathbf{M}}_{i}|}\mathfrak{S}_{1}^ {\mathbf{S}^{\prime}\mathbf{A}}(\widetilde{\mathbf{M}}_{i})\approx_{\rho} \mathfrak{S}_{1}^{\mathbf{S}^{\prime}\mathbf{A}}(\mathbf{M}_{i})\text{ with a probability at least }1-\delta.\] (B.20)

Note that the output \(\widehat{s}\) of Algorithm 5 when applied to node \(\mathcal{N}\) is defined as

\[\widehat{s}:=(1+\rho)\sum_{i\in[\mathcal{B}]}\frac{|\mathbf{M}_{i}|}{| \widetilde{\mathbf{M}}_{i}|}\widehat{s}_{i}.\] (B.21)

Therefore, by union bound over the failure probability (cf. Fact B.3), we can combine Equation (B.14) and sum Equation (B.20) over all \(i\in\mathcal{B}\) child nodes to conclude that \(\widehat{s}\) from Equation (B.21) satisfies a \((\widehat{\rho}+\rho,(\widehat{\delta}+\delta)\mathcal{B})\)-approximation guarantee of Definition B.4. 

#### b.2.3 Proof of Theorem b.1

Proof of Theorem b.1.: We first sketch our strategy to prove Theorem B.1's correctness guarantee. Essentially, our goal is to establish Definition B.4 for the root node \(\mathcal{T}_{(1,1)}\). That this goal suffices to prove the theorem is justified in Lemma B.7. We now show that the node \(\mathcal{T}_{(1,1)}\) satisfies a \((\widehat{\rho},\widehat{\delta})\)-approximation guarantee for some \(\widehat{\rho}\) and \(\widehat{\delta}\). We use the method of induction.

Correctness guarantee of Theorem b.1.We know that the leaf nodes all satisfy the \((\rho,0)\)-approximation guarantee in Definition B.4 since at this level, the base case is triggered, where we compute a \(\rho\)-approximation estimate of the total \(\ell_{1}\) sensitivity with respect to the subspace embedding \(\mathbf{S}^{\prime}\mathbf{A}\). Having established this approximation guarantee at the leaf nodes, we can use Lemma B.8 to inductively propagate the property of \((\rho,\delta)\)-approximation guarantee up the recursion tree \(\mathcal{T}\). From Lemma B.6, we have that the recursion tree \(\mathcal{T}\) of Algorithm 5 is of depth at most \(D=O(\log\log(n+d))\), and the number of child nodes at any given node is at most \(\mathcal{B}=\Theta(\log(n))\); we factor these into the approximation guarantee and error probability.

Base case.At the leaf nodes of the recursion tree, we directly compute the total sensitivities with respect to \(\mathbf{S}^{\prime}\mathbf{A}\). Recall that we denote the depth of the tree by \(D\). Then, because \(\mathbf{S}^{\prime}\mathbf{A}\) is, by definition, a \(\rho\)-approximate subspace embedding for \(\mathbf{A}\), we have, for each \(i\) that indexes a leaf node,

\[\widehat{s}_{(D,i)}\approx_{\rho}\mathfrak{S}_{1}^{\mathbf{S}^{\prime}\mathbf{ A}}(\mathbf{M}^{(D,i)})\text{ with a probability of }1.\] (B.22)

Induction proof.Consider the subtree \(\mathcal{T}_{j}\) truncated at (but including) the nodes at level \(j\) in the recursion tree \(\mathcal{T}\) corresponding to Algorithm 5. Assume the induction hypothesis that all the leaf nodes in the subtree \(\mathcal{T}_{j}\) satisfy Definition B.4 with parameters \(\left(\rho\cdot(D+1-j),\delta\cdot\left(\frac{\mathcal{B}^{D-j+1}-\mathcal{B }}{\mathcal{B}-1}\right)\right)\). That is, at the matrix of each such node \(i\), we have an estimate \(\widehat{s}_{(j,i)}\) of its total sensitivity \(\mathfrak{S}_{1}^{\mathbf{S}^{\prime}\mathbf{A}}(\mathbf{M}^{(j,i)})\) such that

\[\widehat{s}_{(j,i)}\approx_{\rho\cdot(D+1-j)}\mathfrak{S}_{1}^{\mathbf{S}^{ \prime}\mathbf{A}}(\mathbf{M}^{(j,i)})\text{ with a probability at least }1-\delta\cdot\left(\frac{\mathcal{B}^{D-j+1}-\mathcal{B}}{\mathcal{B}-1} \right).\] (B.23)

Then, each of the leaf nodes of the subtree \(\mathcal{T}_{j-1}\) (i.e., the subtree that stops one level above \(\mathcal{T}_{j}\)) satisfies the assumption in Lemma B.8 since its child nodes are either leaf nodes of the entire recursion tree \(\mathcal{T}\) (for which this statement has been shown in Inequality (B.22)) or, if they are not leaf nodes of the recursion tree, they satisfy Definition B.4 with parameters \(\widehat{\rho}=\rho\cdot(D+1-j)\) and \(\widehat{\delta}=\delta\cdot\left(\frac{\mathcal{B}^{D-j+1}-\mathcal{B}}{ \mathcal{B}-1}\right)\). Therefore, Lemma B.8 implies that each of the leaf nodes of \(\mathcal{T}_{j-1}\) satisfies Definition B.4 with approximation parameter \(\widehat{\rho}\) and error probability \(\widehat{\delta}\) defined as follows:

\[\widehat{\rho}=\rho+\rho\cdot(D+1-j)=\rho\cdot(D+2-j)\text{ and }\widehat{ \delta}=\mathcal{B}\cdot\left(\delta+\delta\cdot\left(\frac{\mathcal{B}^{D-j+ 1}-\mathcal{B}}{\mathcal{B}-1}\right)\right)=\delta\cdot\left(\frac{\mathcal{ B}^{D-j+2}-\mathcal{B}}{\mathcal{B}-1}\right).\]

In other words, the outputs \(\widehat{s}_{(j-1,k)}\) of Algorithm 5 on the matrices \(\mathbf{M}^{(j-1,k)}\) in the leaf nodes of \(\mathcal{T}_{j-1}\) each satisfy:

\[\widehat{s}_{(j-1,k)}\approx_{O(\rho\cdot(D+2-j))}\mathfrak{S}_{1}^{\mathbf{ S}^{\prime}\mathbf{A}}(\mathbf{M}^{(j-1,k)})\text{ with a probability at least }1-\delta\cdot\left(\frac{\mathcal{B}^{D-j+2}-\mathcal{B}}{\mathcal{B}-1}\right).\] (B.24)

The base case in Inequality (B.22) and the satisfaction of the induction hypothesis in Inequality (B.24) together finish the proof of the induction hypothesis.

Finishing the proof of correctness.In light of the above conclusion, for the root node \(\mathcal{T}_{(1,1)}\), which is at the level \(j=1\), we may plug in \(j=1\) in Inequality (B.23) and obtain that the output \(\widehat{s}_{(1,1)}\) of Algorithm 5 at the root node satisfies

\[\widehat{s}_{(1,1)}\approx_{O(D\rho)}\mathfrak{S}_{1}^{\mathbf{S}^{\prime} \mathbf{A}}(\mathbf{M}^{(1,1)})\text{ with a probability of at least }1-\delta\left(\frac{\mathcal{B}^{D}- \mathcal{B}}{\mathcal{B}-1}\right).\] (B.25)

In Algorithm 5, we choose the parameters \(\delta=\frac{0.01}{\mathcal{B}^{D}}\) and \(\rho=O\left(\frac{\gamma}{D}\right)\). Plugging these parameters into Inequality (B.25), we conclude that the root node satisfies Definition B.4 with a \((\gamma,0.01)\)-approximation guarantee. This finishes the proof of correctness of the claim.

Proof of runtime.Let \(b\) denote the maximum number of rows of a matrix at a leaf node of \(\mathcal{T}\). Further, recall from Fact 2.4 that the cost of computing one \(\ell_{1}\) sensitivity of an \(n\times d\) matrix is,

\[L=\widetilde{O}(\mathbf{n}\mathbf{n}\mathbf{z}(\mathbf{A})+d^{\omega}).\] (B.26)

Then as stated in Line 1, the algorithm computes the \(\rho\)-approximate total sensitivity at the leaf nodes; Fact 2.4, this incurs a cost of

\[\mathcal{C}(b)=\widetilde{O}(\mathbf{n}\mathbf{n}\mathbf{z}(\mathbf{A})+b \cdot d^{\omega}).\] (B.27)

At any other node, the computational cost is the work done at that level plus the total cost of the recursive calls at that level. Let \(\mathcal{C}(r)\) denote the runtime of Algorithm 5 when the input is an \(r\times d\) matrix. Then,

\[\mathcal{C}(r)=\left\{\begin{array}{ll}\mathcal{B}\cdot\mathcal{C}\left( \frac{\sqrt{r}(1+\rho)}{\rho^{2}}\log(\delta^{-1})\right)+L&r>b\\ \mathcal{C}(b)&r\leq b\end{array}\right.,\]

where \(b\) is the number of rows in the base case, and \(L\) is the cost of computing leverage scores and equals \(L=\widetilde{O}(\mathbf{n}\mathbf{n}\mathbf{z}(\mathbf{A})+d^{\omega})\) from Fact 2.1. For the depth of the recursion \(D\) (where the top level is \(D=1\)), plugging in our choices of \(\delta=\frac{0.01}{\mathcal{B}^{D}}\) and \(\rho=O\left(\frac{\gamma}{D}\right)\), this may be expressed as via expanding the recursion as:

\[\mathcal{C}(n)=\mathcal{B}^{D}\cdot\mathcal{C}(b)+L\cdot\left(\frac{\mathcal{B }^{D}-1}{\mathcal{B}-1}\right)\] (B.28)

for \(n\geq b\). We plug into this expression the values of \(b\) and \(D\) from Lemma B.6, \(L\) from Equation (B.26), \(\mathcal{C}(b)\) from Equation (B.27), and \(\mathcal{B}=\Theta(\log(n))\) to get the claimed runtime. 

### Estimating the maximum \(\ell_{1}\) sensitivity

**Theorem 3.7** (**Approximating the Maximum of \(\ell_{1}\) Sensitivities**).: _Given a matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\), there exists an algorithm, which in time \(\widetilde{O}(\mathbf{n}\mathbf{n}\mathbf{z}(\mathbf{A})+d^{\omega+1})\), outputs a positive scalar \(\widehat{s}\) that satisfies_

\[\Omega(\|\boldsymbol{\sigma}_{1}(\mathbf{A})\|_{\infty})\leq\widehat{s}\leq O (\sqrt{d}\|\boldsymbol{\sigma}_{1}(\mathbf{A})\|_{\infty}).\]Proof.: First, we have

\[\|\mathbf{S}_{1}\mathbf{A}\mathbf{x}\|_{1}=\Theta(\|\mathbf{A}\mathbf{x}\|_{1}).\] (B.29)

We use EquationB.29 to establish the claimed bounds below. First we set some notation. Define \(\mathbf{x}^{\star}\) and \(\mathbf{a}_{i^{\star}}\) as follows:

\[\mathbf{x}^{\star},i^{\star}=\arg\max_{\mathbf{x}\in\mathbb{R}^{d},i\in\| \mathbf{A}\|}\frac{|\mathbf{a}_{i}^{\top}\mathbf{x}|}{\|\mathbf{A}\mathbf{x}\| _{1}}\] (B.30)

Thus, \(\mathbf{x}^{\star}\) is the vector that realizes the maximum sensitivity of the matrix \(\mathbf{A}\), and the row \(\mathbf{a}_{i^{\star}}\) is the row of \(\mathbf{A}\) with maximum sensitivity with respect to \(\mathbf{A}\). Suppose the matrix \(\mathbf{S}_{\infty}\mathbf{A}\) contains the row \(\mathbf{a}_{i^{\star}}\). Then we have

\[\max_{i:\mathbf{c}_{i}\in\mathbf{S}_{\infty}\mathbf{A}}\boldsymbol{\sigma}_{1 }(\mathbf{c}_{i})=\max_{\mathbf{x}\in\mathbb{R}^{d},\mathbf{c}_{k}\in\mathbf{ S}_{\infty}\mathbf{A}}\frac{|\mathbf{c}_{k}^{\top}\mathbf{x}|}{\|\mathbf{S}_{1} \mathbf{A}\mathbf{x}\|_{1}}=\Theta(1)\max_{\mathbf{x}\in\mathbb{R}^{d}, \mathbf{c}_{k}\in\mathbf{S}_{\infty}\mathbf{A}}\frac{|\mathbf{c}_{k}^{\top} \mathbf{x}|}{\|\mathbf{A}\mathbf{x}\|_{1}}=\Theta(1)\max_{\mathbf{x}\in \mathbb{R}^{d}}\frac{|\mathbf{a}_{i}^{\top}\mathbf{x}|}{\|\mathbf{A}\mathbf{x} \|_{1}},\] (B.31)

where the first step is by definition of \(\ell_{1}\) sensitivity of \(\mathbf{S}_{\infty}\mathbf{A}\) with respect to \(\mathbf{S}_{1}\mathbf{A}\), the second step is by EquationB.29, and the third step by noting that the matrix \(\mathbf{S}_{\infty}\mathbf{A}\) is a subset of the rows of \(\mathbf{A}\) (which includes \(\mathbf{a}_{i^{\star}}\)). By definition of \(\boldsymbol{\sigma}_{1}(\mathbf{a}_{i^{\star}})\) in EquationB.30, we have

\[\max_{\mathbf{x}\in\mathbb{R}^{d}}\frac{|\mathbf{a}_{i^{\star}}^{\top} \mathbf{x}|}{\|\mathbf{A}\mathbf{x}\|_{1}}=\|\boldsymbol{\sigma}_{1}(\mathbf{A })\|_{\infty},.\] (B.32)

Then, combining EquationB.31 and InequalityB.32 gives the guarantee in this case. In the other case, suppose \(\mathbf{a}_{i^{\star}}\) is not included in \(\mathbf{S}_{\infty}\mathbf{A}\). Then we observe that the upper bound from the preceding inequalities still holds. For the lower bound, we observe that

\[\|\boldsymbol{\sigma}_{1}^{\mathbf{S}_{1}\mathbf{A}}(\mathbf{S}_{\infty} \mathbf{A})\|_{\infty}=\max_{\mathbf{x}\in\mathbb{R}^{d},\mathbf{c}_{j}\in \mathbf{S}_{\infty}\mathbf{A}}\frac{\mathbf{c}_{j}^{\top}\mathbf{x}}{\| \mathbf{S}_{1}\mathbf{A}\mathbf{x}\|_{1}}\geq\max_{\mathbf{x}\in\mathbb{R}^{d} }\frac{\|\mathbf{S}_{\infty}\mathbf{A}\mathbf{x}\|_{\infty}}{\|\mathbf{S}_{1} \mathbf{A}\mathbf{x}\|_{1}}=\Theta(1)\max_{\mathbf{x}\in\mathbb{R}^{d}}\frac {\|\mathbf{S}_{\infty}\mathbf{A}\mathbf{x}\|_{\infty}}{\|\mathbf{A}\mathbf{x} \|_{1}},\] (B.33)

where the the second step is by choosing a specific vector in the numerator and the third step uses \(\|\mathbf{S}_{1}\mathbf{A}\mathbf{x}\|_{1}=\Theta(\|\mathbf{A}\mathbf{x}\|_{1})\). We further have,

\[\max_{\mathbf{x}\in\mathbb{R}^{d}}\frac{\|\mathbf{S}_{\infty}\mathbf{A} \mathbf{x}\|_{\infty}}{\|\mathbf{A}\mathbf{x}\|_{1}}\geq\frac{\|\mathbf{S}_{ \infty}\mathbf{A}\mathbf{x}^{\star}\|_{\infty}}{\|\mathbf{A}\mathbf{x}^{\star }\|_{1}}\geq\frac{\|\mathbf{A}\mathbf{x}^{\star}\|_{\infty}}{\sqrt{d}\| \mathbf{A}\mathbf{x}^{\star}\|_{1}}\geq\frac{|\mathbf{a}_{i^{\star}}^{\top} \mathbf{x}^{\star}|}{\sqrt{d}\|\mathbf{A}\mathbf{x}^{\star}\|_{1}}=\frac{1}{ \sqrt{d}}\|\boldsymbol{\sigma}_{1}(\mathbf{A})\|_{\infty},\] (B.34)

where the first step is by choosing \(\mathbf{x}=\mathbf{x}^{\star}\), the second step is by the distortion guarantee of \(\ell_{\infty}\) subspace embedding [35], and the final step is by definition of \(\boldsymbol{\sigma}_{1}(\mathbf{a}_{i^{\star}})\). Combining InequalityC.10 and InequalityC.11 gives the claimed lower bound on \(\|\boldsymbol{\sigma}_{1}^{\mathbf{S}_{1}\mathbf{A}}(\mathbf{S}_{\infty} \mathbf{A})\|_{\infty}\). The runtime follows from the computational cost and row dimension of \(\mathbf{S}_{\infty}\mathbf{A}\) from [35] and the cost of computing \(\ell_{1}\) sensitivities with respect to \(\mathbf{S}_{1}\mathbf{A}\) as per Fact2.4. 

## Appendix C Omitted proofs: \(\ell_{p}\) sensitivities

We recall the following notation that we use for stating our results in this setting.

**Definition C.1** (Notation for \(\ell_{p}\) Results).: _We introduce the notation \(\mathsf{LP}(m,d,p)\) to denote the cost of approximating one \(\ell_{p}\) sensitivity of an \(m\times d\) matrix up to an accuracy of a given constant factor._

### Estimating all \(\ell_{p}\) sensitivities

We generalize Algorithm1 to the case \(p\geq 1\) below.

**Theorem C.2** (Approximating all \(\ell_{p}\) sensitivities).: _Given a full-rank matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\), an approximation factor \(1<\alpha\ll n\), and a scalar \(p\geq 1\), let \(\boldsymbol{\sigma}_{p}(\mathbf{a}_{i})\) be the \(i^{\mathrm{th}}\)\(\ell_{p}\) sensitivity. Then there exists an algorithm that returns a vector \(\widetilde{\boldsymbol{\sigma}}\in\mathbb{R}_{\geq 0}^{n}\) such that with high probability, for each \(i\in[n]\), we have_

\[\Omega(\boldsymbol{\sigma}_{p}(\mathbf{a}_{i}))\leq\widetilde{\boldsymbol{ \sigma}}_{i}\leq O(\alpha^{p-1}\boldsymbol{\sigma}_{p}(\mathbf{a}_{i}))+\tfrac{ \alpha^{p}}{n}\mathfrak{S}_{p}(\mathbf{A})).\] (C.1)

_Our algorithm runs in time \(\widetilde{O}\left(\mathbf{n}\mathbf{n}\mathbf{z}(\mathbf{A})+\tfrac{n}{\alpha} \cdot\mathsf{LP}(d^{\max(1,p/2)},d,p)\right)\) (cf. Definition1.2)._Proof.: Suppose the \(i^{\mathrm{th}}\) row of \(\mathbf{A}\) falls into the bucket \(\mathbf{B}_{\ell}\). Suppose, further, that the rows from \(\mathbf{B}_{\ell}\) are mapped to those in \(\mathbf{P}\) with row indices in the set \(J\). Then, Algorithm 6 returns a vector of sensitivity estimates \(\widetilde{\boldsymbol{\sigma}}\in\mathbb{R}^{n}\) with the \(i^{\mathrm{th}}\) coordinate defined as \(\widetilde{\boldsymbol{\sigma}}_{i}=\max_{j\in J}\boldsymbol{\sigma}_{p}( \mathbf{c}_{j})\). For any \(\mathbf{x}\in\mathbb{R}^{d}\), we have

\[\|\mathbf{S}_{p}\mathbf{A}\mathbf{x}\|_{p}^{p}\approx\|\mathbf{A}\mathbf{x}\|_ {p}^{p}.\] (C.2)

We use Equation (C.2) to establish the claimed bounds below. Let \(\mathbf{x}^{\star}=\arg\max_{\mathbf{x}\in\mathbb{R}^{d}}\frac{|\mathbf{a}_{i} ^{\top}\mathbf{x}|_{p}^{p}}{\|\mathbf{A}\mathbf{x}\|_{p}^{p}}\) be the vector that realizes the \(i^{\mathrm{th}}\)\(\ell_{p}\) sensitivity of \(\mathbf{A}\). Further, let the \(j^{\mathrm{th}}\) row of \(\mathbf{P}\) be \(\mathbf{r}_{k}^{(\ell)}\mathbf{B}_{\ell}\). Then, with probability of at least \(1/2\), we have

\[\boldsymbol{\sigma}_{p}^{\mathbf{S}_{p}\mathbf{A}}(\mathbf{p}_{j})=\max_{ \mathbf{x}\in\mathbb{R}^{d}}\frac{|\mathbf{r}_{k}^{(\ell)}\mathbf{B}_{\ell} \mathbf{x}|^{p}}{\|\mathbf{S}_{p}\mathbf{A}\mathbf{x}^{\star}\|_{p}^{p}} \geq\frac{|\mathbf{r}_{k}^{(\ell)}\mathbf{B}_{\ell}\mathbf{x}^{\star}|^{p}}{ \|\mathbf{S}_{p}\mathbf{A}\mathbf{x}^{\star}\|_{p}^{p}}=\Theta(1)\frac{| \mathbf{r}_{k}^{(\ell)}\mathbf{B}_{\ell}\mathbf{x}^{\star}|^{p}}{\|\mathbf{A} \mathbf{x}^{\star}\|_{p}^{p}}\geq\Theta(1)\frac{|\mathbf{a}_{i}^{\top}\mathbf{ x}^{\star}|^{p}}{\|\mathbf{A}\mathbf{x}^{\star}\|_{p}^{p}}=\Theta(1)\frac{| \mathbf{a}_{i}^{\top}\mathbf{x}^{\star}|^{p}}{\|\mathbf{A}\mathbf{x}^{\star} \|_{p}^{p}}=\Theta(1)\boldsymbol{\sigma}_{p}(\mathbf{a}_{i}),\] (C.3)

where the first step is by definition of \(\boldsymbol{\sigma}_{p}(\mathbf{p}_{j})\), the second is by evaluating the function being maximized at a specific choice of \(\mathbf{x}\), the third step is by Equation (C.2), and the final step is by definition of \(\mathbf{x}^{\star}\) and \(\boldsymbol{\sigma}_{p}(\mathbf{a}_{i})\). For the fourth step, we use the fact that \(|\mathbf{r}_{k}^{(\ell)}\mathbf{B}_{\ell}\mathbf{x}^{\star}|^{p}=|\mathbf{r}_ {k,i}^{(\ell)}\mathbf{a}_{i}^{\top}\mathbf{x}^{\star}+\sum_{j\neq i}\mathbf{r}_ {k,j}^{(\ell)}(\mathbf{B}\mathbf{x}^{\star})_{j}|^{p}\geq|\mathbf{a}_{i}^{ \top}\mathbf{x}^{\star}|^{p}\) with a probability of at least \(1/2\) since the vector \(\mathbf{r}_{k}^{(\ell)}\) has coordinates that are \(-1\) or \(+1\) with equal probability. By a union bound over \(j\in J\) independent rows that block \(\mathbf{B}_{\ell}\) is mapped to, we establish the claimed lower bound in Inequality (C.3) with probability at least \(0.9\). To show an upper bound on \(\boldsymbol{\sigma}_{p}^{\mathbf{S}_{p}\mathbf{A}}(\mathbf{p}_{j})\), we observe that

\[\max_{\mathbf{x}\in\mathbb{R}^{d}}\frac{|\mathbf{r}_{k}^{(\ell)}\mathbf{B}_{ \ell}\mathbf{x}|^{p}}{\|\mathbf{S}_{p}\mathbf{A}\mathbf{x}\|_{p}^{p}}\leq \alpha^{p-1}\max_{\mathbf{x}\in\mathbb{R}^{d}}\frac{\|\mathbf{B}_{\ell}\mathbf{ x}\|_{p}^{p}}{\|\mathbf{S}_{p}\mathbf{A}\mathbf{x}\|_{p}^{p}}\leq\Theta( \alpha^{p-1})\sum_{j:\mathbf{a}_{j}\in\mathbf{B}_{\ell}}\max_{\mathbf{x}\in \mathbb{R}^{d}}\frac{|\mathbf{a}_{j}^{\top}\mathbf{x}|^{p}}{\|\mathbf{A} \mathbf{x}\|_{p}^{p}}=\Theta(\alpha^{p-1})\sum_{j:\mathbf{a}_{j}\in\mathbf{B}_ {\ell}}\boldsymbol{\sigma}_{p}(\mathbf{a}_{j}),\] (C.4)

where the second step is by Equation (C.2) and opening up \(\|\mathbf{B}_{\ell}\mathbf{x}\|_{p}^{p}\) in terms of the rows in \(\mathbf{B}_{\ell}\), and the final step is by definition of \(\mathbf{B}_{\ell}\). To see the first step, we observe that

\[|\mathbf{r}_{k}^{(\ell)}\mathbf{B}_{\ell}\mathbf{x}|^{p}\leq\|\mathbf{B}_{\ell }\mathbf{x}\|_{p}^{p}\cdot\|\mathbf{r}_{k}^{(\ell)}\|_{q}^{p}\leq\|\mathbf{B}_ {\ell}\mathbf{x}\|_{p}^{p}\cdot\alpha^{p/q}=\|\mathbf{B}_{\ell}\mathbf{x}\|_{p} ^{p}\cdot\alpha^{p-1},\]

where \(\frac{1}{p}+\frac{1}{q}=1\), the first step is by Holder's inequality, the second is because each entry of \(\mathbf{r}_{k}^{(\ell)}\) is either \(+1\) or \(-1\) and \(\mathbf{r}_{k}^{(\ell)}\in\mathbb{R}^{\alpha}\), and the third is because \(\frac{p}{q}=p-1\). Because \(\mathbf{B}_{\ell}\) is a group of \(\alpha\) rowsselected uniformly at random out of \(n\) rows and contains the row \(\mathbf{a}_{i}\), we have:

\[\mathbb{E}\left\{\sum_{j:\mathbf{a}_{j}\in\mathbf{B}_{t},j\neq i}\boldsymbol{ \sigma}_{p}(\mathbf{a}_{j})\right\}=\frac{\alpha-1}{n-1}\sum_{j\neq i} \boldsymbol{\sigma}_{p}(\mathbf{a}_{i}).\]

Therefore, Markov inequality gives us that with a probability of at least \(0.9\), we have

\[\boldsymbol{\sigma}_{p}^{\mathbf{S}_{p}\mathbf{A}}(\mathbf{p}_{j})\leq O( \alpha^{p-1}\boldsymbol{\sigma}_{p}(\mathbf{a}_{i})+\tfrac{\alpha^{p}}{n} \mathfrak{S}_{p}(\mathbf{A})).\] (C.5)

The median trick then establishes Inequality (C.4) and Inequality (C.5) with high probability. To establish the runtime, note that we first construct the subspace embedding \(\mathbf{S}_{p}\mathbf{A}\) and then compute the \(\ell_{p}\) sensitivities of \(\mathbf{P}\) (with \(O(n/\alpha)\) rows) with respect to \(\mathbf{S}_{p}\mathbf{A}\). The size of the subspace embedding \(\mathbf{S}_{p}\mathbf{A}\) is \(O(d^{1\lor p/2}\times d)\) (see [75, Table 1]). This completes the runtime claim. 

### Estimating the maximum of \(\ell_{p}\) sensitivities

In Section3.2 and AppendixB.3, we showed our result for estimating the maximum \(\ell_{1}\) sensitivity. In this section, we show how to extend this result to all \(p\geq 1\). Algorithm7 generalizes Algorithm3.

``` Input: Matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\) and \(p\geq 1\) (with \(p\neq 2\)) Output: Scalar \(\widehat{s}\in\mathbb{R}_{\geq 0}\) that satisfies that \[\|\boldsymbol{\sigma}_{p}(\mathbf{A})\|_{\infty}\leq\widehat{s}\leq C\cdot d^ {p/2}\cdot\|\boldsymbol{\sigma}_{p}(\mathbf{A})\|_{\infty}\]
1: Compute, for \(\mathbf{A}\), an \(\ell_{\infty}\) subspace embedding \(\mathbf{S}_{\infty}\mathbf{A}\in\mathbb{R}^{O(d\log^{2}(d))\times d}\) such that \(\mathbf{S}_{\infty}\mathbf{A}\) is a subset of the rows of \(\mathbf{A}\)[35]
2: Compute, for \(\mathbf{A}\), an \(\ell_{p}\) subspace embedding \(\mathbf{S}_{p}\mathbf{A}\in\left\{\begin{array}{ll}\mathbb{R}^{O(d)\times d }&p\in[1,2)\\ \mathbb{R}^{O(d^{p/2})\times d}&p>2\end{array}\right.\)
3: Return \(d^{p/2}\|\boldsymbol{\sigma}_{p}^{\mathbf{S}_{p}\mathbf{A}}(\mathbf{S}_{\infty }\mathbf{A})\|_{\infty}\) ```

**Algorithm 7** Approximating the Maximum of \(\ell_{p}\)-Sensitivities

**Theorem C.3** (Approximating the maximum of \(\ell_{p}\) sensitivities).: _Given a matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\) and \(p\geq 1\) (with3\(p\neq 2\)), there exists an algorithm, which outputs a positive scalar \(\widehat{s}\) that satisfies_

Footnote 3: For \(p=2\), the result of [23] gives a constant factor approximation to all leverage scores in \(\mathbf{nnz}(\mathbf{A})+d^{\nu}\) time.

\[\Omega(\|\boldsymbol{\sigma}_{p}(\mathbf{A})\|_{\infty})\leq\widehat{s}\leq O (d^{p/2}\|\boldsymbol{\sigma}_{p}(\mathbf{A})\|_{\infty}).\]

_The runtime of the algorithm is \(\widetilde{O}(\mathbf{nnz}(\mathbf{A})+d^{\nu+1})\) for \(p\in[1,2)\) and \(\widetilde{O}(\mathbf{nnz}(\mathbf{A})+d^{p/2}\). \(\mathsf{LP}(O(d^{p/2}),d,p))\) for \(p>2\)._

Proof.: First, we have

\[\|\mathbf{S}_{p}\mathbf{A}\mathbf{x}\|_{p}^{p}=\Theta(\|\mathbf{A}\mathbf{x} \|_{p}^{p}).\] (C.6)

We use EquationC.6 to establish the claimed bounds below. First we set some notation. Define \(\mathbf{x}^{\star}\) and \(\mathbf{a}_{i^{\star}}\) as follows:

\[\mathbf{x}^{\star},i^{\star}=\arg\max_{\mathbf{x}\in\mathbb{R}^{d},i\in\| \mathbf{A}\|}\frac{|\mathbf{a}_{i}^{\top}\mathbf{x}|^{p}}{\|\mathbf{A}\mathbf{x} \|_{p}^{p}}\] (C.7)

Thus, \(\mathbf{x}^{\star}\) is the vector that realizes the maximum sensitivity of the matrix \(\mathbf{A}\), and the row \(\mathbf{a}_{i^{\star}}\) is the row of \(\mathbf{A}\) with maximum sensitivity with respect to \(\mathbf{A}\). Suppose the matrix \(\mathbf{S}_{\infty}\mathbf{A}\) contains the row \(\mathbf{a}_{i^{\star}}\). Then we have

\[\max_{i:\mathbf{e}_{i}\in\mathbf{S}_{\infty}\mathbf{A}}\boldsymbol{\sigma}_{p}^ {\mathbf{S}_{p}\mathbf{A}}(\mathbf{e}_{i})=\max_{\mathbf{x}\in\mathbb{R}^{d}, \mathbf{e}_{i}\in\mathbf{S}_{\infty}\mathbf{A}}\frac{|\mathbf{c}_{k}^{\top} \mathbf{x}|^{p}}{\|\mathbf{S}_{p}\mathbf{A}\mathbf{x}\|_{p}^{p}}=\Theta(1) \max_{\mathbf{x}\in\mathbb{R}^{d},\mathbf{e}_{k}\in\mathbf{S}_{\infty}\mathbf{A }}\frac{|\mathbf{c}_{k}^{\top}\mathbf{x}|^{p}}{\|\mathbf{A}\mathbf{x}\|_{p}^{p}}= \Theta(1)\max_{\mathbf{x}\in\mathbb{R}^{d}}\frac{|\mathbf{a}_{i^{\star}}^{\top }\mathbf{x}|^{p}}{\|\mathbf{A}\mathbf{x}\|_{p}^{p}},\] (C.8)where the first step is by definition of \(\ell_{p}\) sensitivity of \(\mathbf{S}_{\infty}\mathbf{A}\) with respect to \(\mathbf{S}_{p}\mathbf{A}\), the second step is by Equation3, and the third step by noting that the matrix \(\mathbf{S}_{\infty}\mathbf{A}\) is a subset of the rows of \(\mathbf{A}\) (which includes \(\mathbf{a}_{i^{*}}\)). By definition of \(\boldsymbol{\sigma}_{p}(\mathbf{a}_{i^{*}})\) in Equation3, we have

\[\max_{\mathbf{x}\in\mathbb{R}^{d}}\frac{|\mathbf{a}_{i^{*}}^{\top}\mathbf{x} |^{p}}{\|\mathbf{A}\mathbf{x}\|_{p}^{p}}=\|\boldsymbol{\sigma}_{p}(\mathbf{A} )\|_{\infty}.\] (C.9)

Then, combining Equation3 and Equation3 gives the guarantee in this case. In the other case, suppose \(\mathbf{a}_{i^{*}}\) is not included in \(\mathbf{S}_{\infty}\mathbf{A}\). Then we observe that the upper bound from the preceding inequalities still holds. For the lower bound, we observe that

\[\|\boldsymbol{\sigma}_{p}^{\mathbf{S}_{p}\mathbf{A}}(\mathbf{S}_{\infty} \mathbf{A})\|_{\infty}=\max_{\mathbf{x}\in\mathbb{R}^{d},\varepsilon_{p} \in\mathbf{S}_{\infty}\mathbf{A}}\frac{|\mathbf{c}_{j}^{\top}\mathbf{x}|^{p}} {\|\mathbf{S}_{p}\mathbf{A}\mathbf{x}\|_{p}^{p}}\geq\max_{\mathbf{x}\in \mathbb{R}^{d}}\frac{\|\mathbf{S}_{\infty}\mathbf{A}\mathbf{x}\|_{\infty}^{p} }{\|\mathbf{S}_{p}\mathbf{A}\mathbf{x}\|_{p}^{p}}=\Theta(1)\max_{\mathbf{x} \in\mathbb{R}^{d}}\frac{\|\mathbf{S}_{\infty}\mathbf{A}\mathbf{x}\|_{\infty}^{ p}}{\|\mathbf{A}\mathbf{x}\|_{p}^{p}},\] (C.10)

where the the second step is by choosing a specific vector in the numerator and the third step uses \(\|\mathbf{S}_{p}\mathbf{A}\mathbf{x}\|_{p}^{p}=\Theta(\|\mathbf{A}\mathbf{x}\|_ {p}^{p})\). We further have,

\[\max_{\mathbf{x}\in\mathbb{R}^{d}}\frac{\|\mathbf{S}_{\infty}\mathbf{A} \mathbf{x}\|_{\infty}^{p}}{\|\mathbf{A}\mathbf{x}\|_{p}^{p}}\geq\frac{\| \mathbf{S}_{\infty}\mathbf{A}\mathbf{x}^{\star}\|_{\infty}^{p}}{\|\mathbf{A} \mathbf{x}^{\star}\|_{p}^{p}}\geq\frac{\|\mathbf{A}\mathbf{x}^{\star}\|_{ \infty}^{p}}{dp^{/2}\|\mathbf{A}\mathbf{x}^{\star}\|_{p}^{p}}\geq\frac{| \mathbf{a}_{i^{*}}^{\top}\mathbf{x}^{\star}|^{p}}{dp^{/2}\|\mathbf{A}\mathbf{ x}^{\star}\|_{p}^{p}}=\frac{1}{dp^{/2}}\|\boldsymbol{\sigma}_{p}(\mathbf{A})\|_{ \infty},\] (C.11)

where the first step is by choosing \(\mathbf{x}=\mathbf{x}^{\star}\), the second step is by the guarantee of \(\ell_{\infty}\) subspace embedding, and the final step is by definition of \(\boldsymbol{\sigma}_{p}(\mathbf{a}_{i^{*}})\). Combining Inequality3 and Inequality3 gives the claimed lower bound on \(\|\boldsymbol{\sigma}_{p}^{\mathbf{S}_{p}\mathbf{A}}(\mathbf{S}_{\infty} \mathbf{A})\|_{\infty}\). The runtime follows from the cost of computing \(\mathbf{S}_{\infty}\mathbf{A}\) from [35] and the cost of computing and size of \(\mathbf{S}_{p}\mathbf{A}\) from [23, Figure 1] 

## Appendix D Lower Bounds

**Theorem D.1** (\(\ell_{p}\) Regression Reduces to \(\ell_{p}\) Sensitivities).: _Suppose that we are given an algorithm \(\mathcal{A}\), which for any matrix \(\mathbf{A}^{\prime}\in\mathbb{R}^{n^{\prime}\times d^{\prime}}\) and accuracy parameter \(\varepsilon^{\prime}\in(0,1)\), computes \((1\pm\varepsilon^{\prime})\boldsymbol{\sigma}_{p}(\mathbf{A}^{\prime})\) in time \(\mathcal{T}(n^{\prime},d^{\prime},\mathbf{nnz}(\mathbf{A}^{\prime}), \varepsilon^{\prime})\). Then, there exists an algorithm that takes \(\mathbf{A}\in\mathbb{R}^{n\times d}\) and \(\mathbf{b}\in\mathbb{R}^{n}\) as inputs and computes \((1\pm\varepsilon)\min_{\mathbf{y}\in\mathbb{R}^{d}}\|\mathbf{A}\mathbf{y}- \mathbf{b}\|_{p}^{p}\pm\lambda^{p}\varepsilon\) in time \(\mathcal{T}(n+1,d+1,\mathbf{nnz}(\mathbf{A})+\mathbf{nnz}(\mathbf{b})+1, \varepsilon)\) for any \(\lambda>0\)._

Proof.: Given \(\mathbf{A}\in\mathbb{R}^{n\times d}\) and \(\mathbf{b}\in\mathbb{R}^{d}\), consider the matrix \(\mathbf{A}^{\prime}:=\begin{bmatrix}\mathbf{A}&-\mathbf{b}\\ \mathbf{0}^{\top}&-\lambda\end{bmatrix}\in\mathbb{R}^{(n+1)\times d}\). Then the \(n+1\)-th \(\ell_{p}\) sensitivity of \(\mathbf{A}^{\prime}\) is

\[\boldsymbol{\sigma}_{p}(\mathbf{a}_{n+1}^{\prime})=\max_{\mathbf{x}\in\mathbb{R }^{d}}\frac{|\langle\mathbf{e}_{d},\mathbf{x}\rangle|^{p}}{\|\mathbf{A}^{ \prime}\mathbf{x}\|_{p}^{p}}=\max_{\mathbf{x}\in\mathbb{R}^{d}:x_{d}=\lambda ^{-1}}\frac{1}{\|\mathbf{A}^{\prime}\mathbf{x}\|_{p}^{p}}=\frac{1}{1+\min_{ \mathbf{y}\in\mathbb{R}^{d-1}}\{\|\mathbf{A}\mathbf{y}-\lambda^{-1}\mathbf{ b}\|_{p}^{p}\}},\]

where the second step is by the scale-invariance of the definition of sensitivity with respect to the variable of optimization. By scale-invariance we see that for any \(c>0\), and \(\min_{\mathbf{y}}\|\mathbf{A}_{\cdot\shortmid i}\mathbf{y}-c\mathbf{A}_{\cdot \shortmid i}\|_{p}^{p}=c^{p}\min_{\mathbf{y}}\|\mathbf{A}_{\cdot \shortmid i}\mathbf{y}-\mathbf{A}_{\cdot\shortmid i}\|_{p}^{p}\).

Therefore, note that rearranging gives \(\min_{\mathbf{y}\in\mathbb{R}^{d-1}}\|\mathbf{A}\mathbf{y}-\mathbf{b}\|_{p}^{p}= \frac{\lambda^{p}}{\boldsymbol{\sigma}_{p}(\mathbf{a}_{n+1}^{\prime})}-\lambda^ {p}\), which is how we can estimate the cost of the regression. Then computing \(\boldsymbol{\sigma}_{p}(\mathbf{a}_{n+1}^{\prime})\) to a multiplicative accuracy of \(\varepsilon^{\prime}\) gives the value of \(\min_{\mathbf{y}\in\mathbb{R}^{d-1}}\|\mathbf{A}\mathbf{y}-\mathbf{b}\|_{p}^{p}\) up to error \(\varepsilon(\frac{\lambda^{p}}{\boldsymbol{\sigma}_{p}(\mathbf{a}_{n+1}^{\prime})})= \varepsilon(\lambda^{p}+\min_{\mathbf{y}\in\mathbb{R}^{d-1}}\|\mathbf{A} \mathbf{y}-\mathbf{b}\|_{p}^{p})\), giving our final bounds.

Using the same technique as in TheoremD.1, we can similarly extend the reduction to a multiple regression task. In particular, we show that computing the values of a family of certain regularized leave-one-out regression problems for a matrix may be obtained by simply computing leverage scores of an associated matrix; therefore, this observation demonstrates that finding fast algorithms for sensitivities is as hard as multiple regression tasks. Specifically, for some matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\), we denote \(\mathbf{A}_{\cdot\shortmid i}\in\mathbb{R}^{n\times d-1}\) as the submatrix of \(\mathbf{A}\) with its \(i\)-th column, denoted as \(\mathbf{A}_{\cdot\shortmid i}\), removed. We show that approximate sensitivity calculations can solve \(\min_{\mathbf{y}\in\mathbb{R}^{d-1}}\|\mathbf{A}_{\cdot\shortmid i}\mathbf{y}+ \mathbf{A}_{\cdot\shortmid i}\|_{2}\) approximately for all \(i\).

**Lemma D.2** (Regularized Leave-One-Out \(\ell_{p}\) Multiregression Reduces to \(\ell_{p}\) Sensitivities).: _Suppose that we are given an algorithm \(\mathcal{A}\), which for any matrix \(\mathbf{A}^{\prime}\in\mathbb{R}^{n^{\prime}\times d^{\prime}}\) and accuracy parameter \(\varepsilon^{\prime}\in(0,1)\), computes \((1\pm\varepsilon^{\prime})\boldsymbol{\sigma}_{p}(\mathbf{A}^{\prime})\) in time \(\mathcal{T}(n^{\prime},d^{\prime},\mathbf{nnz}(\mathbf{A}^{\prime}),\varepsilon ^{\prime})\). Given a matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\) with \(n\geq d\), let \(\text{OPT}_{i}:=\min_{\mathbf{y}\in\mathbb{R}^{d-1}}\|\mathbf{A}_{:-i} \mathbf{y}+\mathbf{A}_{:i}\|_{p}^{p}\) and \(\mathbf{y}_{i}^{*}:=\arg\min_{\mathbf{y}\in\mathbb{R}^{d-1}}\|\mathbf{A}_{:-i }\mathbf{y}+\mathbf{A}_{:i}\|_{p}\) for all the \(i\in[d]\). Then, there exists an algorithm that takes \(\mathbf{A}\in\mathbb{R}^{n\times d}\) and computes \((1\pm\varepsilon)\text{OPT}_{i}\pm\lambda^{p}(1+\|\mathbf{y}_{i}^{*}\|_{p}^{p})\) in time \(\mathcal{T}(n+d,d,\mathbf{nnz}(\mathbf{A}),\varepsilon)\) for any \(\lambda>0\)._

Proof.: Given \(\mathbf{A}\in\mathbb{R}^{n\times d}\), consider the matrix \(\mathbf{A}^{\prime}:=\begin{bmatrix}\mathbf{A}\\ \lambda\mathbf{I}\end{bmatrix}\in\mathbb{R}^{(n+d)\times d}\) formed by vertically appending a scaled identity matrix to \(\mathbf{A}\), with \(\lambda>0\). By the definition of \(\ell_{p}\) sensitivities, we have the following.

\[\boldsymbol{\sigma}_{p}(\mathbf{a}_{n+i}^{\prime}) =\max_{\mathbf{x}\in\mathbb{R}^{d}}\frac{|\mathbf{x}^{\top} \mathbf{a}_{n+i}^{\prime}|^{p}}{\|\mathbf{A}^{\prime}\mathbf{x}\|_{p}^{p}}= \max_{\mathbf{x}\in\mathbb{R}^{d}:x_{i}=\lambda^{-1}}\frac{1}{\|\mathbf{A}^{ \prime}\mathbf{x}\|_{p}^{p}}=\max_{\mathbf{x}\in\mathbb{R}^{d}:x_{i}=\lambda^{ -1}}\frac{1}{\|\lambda^{-1}\mathbf{A}_{:i}^{\prime}+\mathbf{A}_{-:i}^{\prime} \mathbf{x}_{-i}\|_{p}^{p}}\] \[=\frac{1}{\min_{\mathbf{y}\in\mathbb{R}^{d-1}}\left\{1+\lambda^{p }\|\mathbf{y}\|_{p}^{p}+\|\mathbf{A}_{:-i}\mathbf{y}+\lambda^{-1}\mathbf{A}_{: i}\|_{p}^{p}\right\}}\]

Recall that \(\mathbf{y}_{i}^{*}:=\arg\min_{\mathbf{y}\in\mathbb{R}^{d-1}}\|\mathbf{A}_{:-i }\mathbf{y}+\mathbf{A}_{:i}\|_{p}\), and since the power is a monotone transform, \(\text{OPT}_{i}:=\min_{\mathbf{y}\in\mathbb{R}^{d-1}}\|\mathbf{A}_{:-i} \mathbf{y}+\mathbf{A}_{:i}\|_{p}=\|\mathbf{A}_{:-i}\mathbf{y}_{i}^{*}+\mathbf{ A}_{:i}\|_{p}\). By scale-invariance we see that for any \(c>0\), \(c\mathbf{y}_{i}^{*}=\arg\min_{\mathbf{y}\in\mathbb{R}^{d-1}}\|\mathbf{A}_{:-i }\mathbf{y}+c\mathbf{A}_{:i}\|_{p}^{p}\) and \(\min_{\mathbf{y}}\|\mathbf{A}_{:-i}\mathbf{y}+c\mathbf{A}_{:i}\|_{p}^{p}=c^{p} \text{OPT}_{i}\). Therefore, by plugging \(\mathbf{y}=\lambda^{-1}\mathbf{y}_{i}^{*}\), note that

\[\min_{\mathbf{y}\in\mathbb{R}^{d-1}}\left\{1+\lambda^{p}\|\mathbf{y}\|_{p}^{p} +\|\mathbf{A}_{:-i}\mathbf{y}+\lambda^{-1}\mathbf{A}_{:i}\|_{p}^{p}\right\}\leq 1 +\|\mathbf{y}^{*}\|_{p}^{p}+\lambda^{-p}\text{OPT}_{i}.\]

However, we also note that by non-negativity,

\[\min_{\mathbf{y}\in\mathbb{R}^{d-1}}\left\{1+\lambda^{p}\|\mathbf{y}\|_{p}^{p} +\|\mathbf{A}_{:-i}\mathbf{y}+\lambda^{-1}\mathbf{A}_{:i}\|_{p}^{p}\right\}\geq \lambda^{-p}\text{OPT}_{i}.\]

Combining this all together gives the claimed approximation guarantee by noting that

\[\text{OPT}_{i}+\lambda^{p}\|\mathbf{y}^{*}\|_{p}^{p}+\lambda^{p}\geq\frac{ \lambda^{p}}{\boldsymbol{\sigma}_{p}(\mathbf{a}_{n+i}^{\prime})}\geq\text{OPT }_{i}\]

Therefore, our algorithm is to simply return \(\lambda^{p}/s_{p}(\mathbf{a}_{n+i}^{\prime})\), where \(s\) is the \(\epsilon\)-approximate estimate of the sensitivity and our bound follows. The runtime guarantees follow immediately from assumption that the runtime of calculating the \(\ell_{p}\) sensitivities of \(\mathbf{A}^{\prime}\). 

**Corollary D.3**.: _Assuming that leave-one-out \(\ell_{p}\) multi-regression with \(\Omega(d)\) instances takes \(\text{poly}(d)\,\mathbf{nnz}(\mathbf{A})+\text{poly}(1/\varepsilon)\) time [44, 45], computing \(\ell_{p}\) sensitivities of an \(n\times d\) matrix \(\mathbf{A}\) costs at least \(\text{poly}(d)\,\mathbf{nnz}(\mathbf{A})+\text{poly}(1/\varepsilon)\)._

## Appendix E Additional Experiments

Here we include additional experiments on other similar datasets.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline \multirow{2}{*}{\(p\)} & Total Sensitivity & \multirow{2}{*}{Brute-Force} & \multirow{2}{*}{Approximation} & \multirow{2}{*}{Brute-Force Runtime (s)} & \multirow{2}{*}{Approximate Runtime (s)} \\  & Upper Bound & & & & \\ \hline
1 & 11 & 4.7 & 3.9 & 1940 & 303 \\
1.5 & 11 & 8.3 & 10.3 & 1980 & 280 \\
2.5 & 20 & 10.3 & 11.8 & 1970 & 326 \\
3 & 36.4 & 6.9 & 7.3 & 1970 & 371 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Runtime comparison for computing total sensitivities for the fires dataset, which has matrix shape \((517,11)\). We include the theoretical upper bound for the total sensitivities using lewis weights calculations.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline \multirow{2}{*}{\(p\)} & Total Sensitivity & \multirow{2}{*}{Brute-Force} & \multirow{2}{*}{Approximation} & \multirow{2}{*}{Brute-Force Runtime (s)} & \multirow{2}{*}{Approximate Runtime (s)} \\  & Upper Bound & & & & \\ \hline
1 & 11 & 4.1 & 5.2 & 540 & 154 \\
1.5 & 11 & 10.1 & 6.7 & 560 & 201 \\
2.5 & 20 & 19.9 & 12.2 & 390 & 117 \\
3 & 36.4 & 8.8 & 6.3 & 354 & 136 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Runtime comparison for computing total sensitivities for the concrete dataset, which has matrix shape \((101,11)\). We include the theoretical upper bound for the total sensitivities using lewis weights calculations.

Figure 2: Average absolute log ratios for all \(\ell_{p}\) sensitivity approximations for fires.