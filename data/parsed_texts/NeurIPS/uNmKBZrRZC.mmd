# Adaptive Linear Estimating Equations

Mufang Ying

Department of Statistics

Rutgers University - New Brunswick

my426@scarletmail.rutgers.edu &Koulik Khamaru

Department of Statistics

Rutgers University - New Brunswick

k1241@stat.rutgers.edu &Cun-Hui Zhang

Department of Statistics

Rutgers University - New Brunswick

czhang@stat.rutgers.edu

###### Abstract

Sequential data collection has emerged as a widely adopted technique for enhancing the efficiency of data gathering processes. Despite its advantages, such data collection mechanism often introduces complexities to the statistical inference procedure. For instance, the ordinary least squares (OLS) estimator in an adaptive linear regression model can exhibit non-normal asymptotic behavior, posing challenges for accurate inference and interpretation. In this paper, we propose a general method for constructing debiased estimator which remedies this issue. It makes use of the idea of adaptive linear estimating equations, and we establish theoretical guarantees of asymptotic normality, supplemented by discussions on achieving near-optimal asymptotic variance. A salient feature of our estimator is that in the context of multi-armed bandits, our estimator retains the non-asymptotic performance of the least squares estimator while obtaining asymptotic normality property. Consequently, this work helps connect two fruitful paradigms of adaptive inference: a) non-asymptotic inference using concentration inequalities and b) asymptotic inference via asymptotic normality.

## 1 Introduction

Adaptive data collection arises as a common practice in various scenarios, with a notable example being the use of (contextual) bandit algorithms. Algorithms like these aid in striking a balance between exploration and exploitation trade-offs within decision-making processes, encompassing domains such as personalized healthcare and web-based services [35; 24; 3; 22]. For instance, in personalized healthcare, the primary objective is to choose the most effective treatment for each patient based on their individual characteristics, such as medical history, genetic profile, and living environment. Bandit algorithms can be used to allocate treatments based on observed response, and the algorithm updates its probability distribution to incorporate new information as patients receive treatment and their response is observed. Over time, the algorithm can learn which treatments are the most effective for different types of patients.

Although the adaptivity in data collection improves the quality of data, the sequential nature (non-iid) of the data makes the inference procedure quite challenging [34; 26; 5; 28; 27; 10; 30; 29]. There is a lengthy literature on the problem of parameter estimation in the adaptive design setting. In a series of work [15; 19; 17], the authors studied the consistency of the least squares estimator for an adaptivelinear model. In a later work, Lai [14] studied the consistency of the least squares estimator in a nonlinear regression model. The collective wisdom of these papers is that, for adaptive data collection methods, standard estimators are consistent under a mild condition on the maximum and minimum eigenvalues of the covariance matrix [19; 14]. In a more recent line of work [1; 2], the authors provide a high probability upper bound on the \(\ell_{2}\)-error of the least squares estimator for a linear model. We point out that, while the high probability bounds provide a quantitative understanding of OLS, these results assume a stronger sub-Gaussian assumption on the noise variables.

The problem of inference, i.e. constructing valid confidence intervals, with adaptively collected data is much more delicate. Lai and Wei [19] demonstrated that for a unit root autoregressive model, which is an example of adaptive linear regression models, the least squares estimator doesn't achieve asymptotic normality. Furthermore, the authors showed that for a linear regression model, the least squares estimator is asymptotically normal when the data collection procedure satisfies a stability condition. Concretely, letting \(\bm{x}_{i}\) denote the covariate associated with \(i\)-th sample, the authors require

\[\mathbf{B}_{n}^{-1}\mathbf{S}_{n}\stackrel{{ p}}{{\longrightarrow}} \mathbf{I}\] (1)

where \(\mathbf{S}_{n}=\sum_{i=1}^{n}\bm{x}_{i}\bm{x}_{i}^{\top}\) and \(\{\mathbf{B}_{n}\}_{n\geq 1}\) is a sequence of _non-random_ positive definite matrices. Unfortunately, in many scenarios, the stability condition (1) is violated [38; 19]. Moreover, in practice, it might be difficult to verify whether the stability condition (1) holds or not. In another line of research [10; 36; 37; 4; 9; 28; 31; 25; 38], the authors assume knowledge of the underlying data collection algorithm and provide asymptotically valid confidence intervals. While this approach offers intervals under a much weaker assumption on the underlying model, full knowledge of the data collection algorithm is often unavailable in practice.

Online debiasing based methods:In order to produce valid statistical inference when the stability condition (1) does not hold, some authors [8; 7; 13] utilize the idea of online debiasing. At a high level, the online debiased estimator reduces bias from an initial estimate (usually the least squares estimate) by adding some correction terms, and the online debiasing procedure does not require the knowledge of the data generating process. Although this procedure guarantees asymptotic reduction of bias to zero, the bias term's convergence rate can be quite slow.

In this work, we consider estimating the unknown parameter in an adaptive linear model by using a set of adaptive linear estimating equations (ALEE). We show that our proposed ALEE estimator achieves asymptotic normality without knowing the exact data collection algorithm while addressing the slowly decaying bias problem in online debiasing procedure.

## 2 Background and problem set-up

In this section, we provide the background for our problem and set up a few notations. We begin by defining the adaptive data collection mechanism for linear models.

### Adaptive linear model

Suppose a scalar response variable \(y_{t}\) is linked to a covariate vector \(\bm{x}_{t}\in\mathbb{R}^{d}\) at time \(t\) via the linear model:

\[y_{t}=\bm{x}_{t}^{\top}\bm{\theta}^{*}+\epsilon_{t}\qquad\text{for }t\in[n],\] (2)

where \(\bm{\theta}^{*}\in\mathbb{R}^{d}\) is the unknown parameter of interest.

In an adaptive linear model, the regressor \(\bm{x}_{t}\) at time \(t\) is assumed to be a (unknown) function of the prior data point \(\{\bm{x}_{1},y_{1},\ldots,\bm{x}_{t-1},y_{t-1}\}\) as well as additional source of randomness that may be present in the data collection process. Formally, we assume there is an increasing sequence of \(\sigma\)-fields \(\{\mathcal{F}_{t}\}_{t\geq 0}\) such that

\[\sigma(\bm{x}_{1},y_{1},\ldots,\bm{x}_{t-1},y_{t-1},\bm{x}_{t})\in\mathcal{F} _{t-1}\qquad\text{for }t\in[n].\]

For the noise variables \(\{\epsilon_{t}\}_{t\geq 1}\) appearing in equation (2), we impose the following conditions

\[\mathbb{E}[\epsilon_{t}|\mathcal{F}_{t-1}]=0,\quad\mathbb{E}[\epsilon_{t}^{2} |\mathcal{F}_{t-1}]=\sigma^{2},\quad\text{and}\quad\sup_{t\geq 1}\mathbb{E}[| \epsilon_{t}/\sigma|^{2+\delta}|\mathcal{F}_{t-1}]<\infty,\] (3)

for some \(\delta>0\). The above condition is relatively mild compared to a sub-Gaussian condition.

Examples of adaptive linear model arise in various problems, including multi-armed and contextual bandit problems, dynamical input-output systems, adaptive approximation schemes and time series models. For instance, in the context of the multi-armed bandit problem, the design vector \(\bm{x}_{t}\) is one of the basis vectors \(\{\bm{e}_{k}\}_{k\in[d]}\), representing an arm being pulled, while \(\bm{\theta}^{*},y_{t}\) represent the true mean reward vector and reward at time \(t\), respectively.

### Adaptive linear estimating equations

As we mentioned earlier, the OLS estimator can fail to achieve asymptotic normality due to the instability of the covariance matrix with adaptively collected data. To get around this issue, we consider a different approach ALEE (adaptive linear estimating equations). Namely, we obtain an estimate by solving a system of linear estimating equations with adaptive weights,

\[\texttt{ALEE}\colon\qquad\sum_{t=1}^{n}\bm{w}_{t}(y_{t}-\bm{x}_{t}^{\top} \widehat{\bm{\theta}}_{\text{ALEE}})=\bm{0}.\] (4)

Here the weight \(\bm{w}_{t}\in\mathbb{R}^{d}\) is chosen in a way that \(\bm{w}_{t}\in\mathcal{F}_{t-1}\) for \(t\in[n]\). Let us now try to gain some intuition behind the construction of ALEE. Rewriting equation (4), we have

\[\{\sum_{t=1}^{n}\bm{w}_{t}\bm{x}_{t}\}\cdot(\widehat{\bm{\theta}}_{\text{ALEE }}-\bm{\theta}^{*})=\sum_{t=1}^{n}\bm{w}_{t}\epsilon_{t}.\] (5)

Notably, the choice of \(\bm{w}_{t}\in\mathcal{F}_{t-1}\) makes \(\sum_{t=1}^{n}\bm{w}_{t}\epsilon_{t}\) the sum of a martingale difference sequence. Our first theorem postulates conditions on the weight vectors \(\{\bm{w}_{t}\}_{t\geq 1}\) such that the right-hand side of (5) converges to normal distribution asymptotically. Throughout the paper, we use the shorthand \(\mathbf{W}_{t}=(\bm{w}_{1},\ldots,\bm{w}_{t})^{\top}\in\mathbb{R}^{t\times d}\), \(\mathbf{X}_{t}=(\bm{x}_{1},\ldots,\bm{x}_{t})^{\top}\in\mathbb{R}^{t\times d}\).

**Proposition 2.1**.: _Suppose condition (3) holds and the predictable sequence \(\{\bm{w}_{t}\}_{1\leq t\leq n}\) satisfies_

\[\max_{1\leq t\leq n}\|\bm{w}_{t}\|_{2}=o_{p}(1)\qquad\text{and}\qquad\big{\|} \mathbf{I}_{d}-\mathbf{W}_{n}^{\top}\mathbf{W}_{n}\big{\|}_{\text{op}}=o_{p}( 1).\] (6)

_Let \(\mathbf{A}_{w}=\mathbf{V}_{w}\mathbf{U}_{w}^{\top}\mathbf{X}_{n}\) with \(\mathbf{W}_{n}=\mathbf{U}_{w}\mathbf{\Lambda}_{w}\mathbf{V}_{w}^{\top}\) being the SVD of \(\mathbf{W}_{n}\). Then,_

\[\mathbf{A}_{w}(\widehat{\bm{\theta}}_{\text{ALEE}}-\bm{\theta}^{*})/\widehat {\sigma}\stackrel{{ d}}{{\longrightarrow}}\mathcal{N}\big{(} \bm{0},\mathbf{I}_{d}\big{)},\] (7)

_where \(\widehat{\sigma}\) is any consistent estimator for \(\sigma\)._

**Proof.** Invoking the second part of the condition (6), we have that \(\mathbf{\Lambda}_{w}\) is invertible for large \(n\), and \(\|\mathbf{V}_{w}\mathbf{\Lambda}_{w}^{-1}\mathbf{V}_{w}^{\top}-\mathbf{I}_{d} \|_{\text{op}}=o_{p}(1)\). Utilizing the expression (5), we have

\[\mathbf{A}_{w}(\widehat{\bm{\theta}}_{\text{ALEE}}-\bm{\theta}^{*})/\sigma= \mathbf{V}_{w}\mathbf{\Lambda}_{w}^{-1}\mathbf{V}_{w}^{\top}\mathbf{W}_{n}^{ \top}\mathbf{X}_{n}(\widehat{\bm{\theta}}_{\text{ALEE}}-\bm{\theta}^{*})/ \sigma=\mathbf{V}_{w}\mathbf{\Lambda}_{w}^{-1}\mathbf{V}_{w}^{\top}\sum_{t=1}^ {n}\bm{w}_{t}\epsilon_{t}/\sigma.\]

Invoking the stability condition on the weights \(\{\bm{w}_{t}\}\) and using the fact that \(\sum_{t=1}^{n}\bm{w}_{t}\epsilon_{t}\) is a martingale difference sequence, we conclude from martingale central limit theorem [11, Theorem 2.1] that

\[\sum_{t=1}^{n}\bm{w}_{t}\epsilon_{t}/\sigma\stackrel{{ d}}{{ \longrightarrow}}\mathcal{N}\big{(}\bm{0},\mathbf{I}_{d}\big{)}.\]

Combining the last equation with \(\|\mathbf{V}_{w}\mathbf{\Lambda}_{w}^{-1}\mathbf{V}_{w}^{\top}-\mathbf{I}_{d} \|_{\text{op}}=o_{p}(1)\) and using Slutsky's theorem yield

\[\mathbf{A}_{w}(\widehat{\bm{\theta}}_{\text{ALEE}}-\bm{\theta}^{*})/\sigma \stackrel{{ d}}{{\longrightarrow}}\mathcal{N}\big{(}\bm{0}, \mathbf{I}_{d}\big{)}.\]

The claim of Proposition 2.1 now follows from Slutsky's theorem.

A few comments regarding the Proposition 2.1 are in order. Straightforward calculation shows

\[\mathbf{A}_{w}^{\top}\mathbf{A}_{w}=\mathbf{X}_{n}^{\top}\mathbf{P}_{w} \mathbf{X}_{n}\preceq\mathbf{S}_{n},\qquad\text{where}\qquad\mathbf{P}_{w}= \mathbf{W}_{n}(\mathbf{W}_{n}^{\top}\mathbf{W}_{n})^{-1}\mathbf{W}_{n}^{\top}.\] (8)

In words, the volume of the confidence region based on (7) is always larger than the confidence region generated by the least squares estimate. Therefore, the ALEE-based inference, which is consistently valid, exhibits a reduced efficiency in cases where both types of confidence regions are valid. Compared with the confidence regions based on OLS, the advantage of the ALEE approach is to provide flexibility in the choice of weights to guarantee the validity of the CLT conditions (6).

Next, note that the matrix \(\mathbf{A}_{w}\) is asymptotically equivalent to the matrix \(\mathbf{W}_{n}^{\top}\mathbf{X}_{n}\) (see equation (5)) under the stability condition (6). The benefit of this reformulation is that it helps us better understand efficiency of ALEE compared with the OLS. This has led us to define a notion of _affinity_ between the weights \(\{\bm{w}_{t}\}_{t\geq 1}\) and covariates \(\{\bm{x}_{t}\}_{t\geq 1}\) for better understanding of the efficiency of ALEE and ways to design nearly optimal weights, as it will be clear in the next section.

Finally, it is straightforward to obtain a consistent estimate for \(\sigma\). For instance, assuming \(\log(\lambda_{\max}(\mathbf{X}_{n}^{\top}\mathbf{X}_{n}))/n\stackrel{{ a.s.}}{{ \longrightarrow}}0\) and the noise condition (3), we have

\[\widehat{\sigma}^{2}:=\frac{1}{n}\sum_{t=1}^{n}(y_{t}-\bm{x}_{t}^{\top} \widehat{\bm{\theta}}_{\text{LS}})^{2}\stackrel{{ a.s.}}{{ \longrightarrow}}\sigma^{2}.\] (9)

Here, \(\widehat{\bm{\theta}}_{\text{LS}}\) refers to the least squares estimate. See [19, Lemma 3] for a detailed proof of equation (9).

## 3 Main results

In this section, we propose methods to construct weights \(\{\bm{w}_{t}\}_{t\geq 1}\) which satisfy the stability property (6), and study the resulting ALEE. Section 3.1 is devoted to the multi-armed bandit case, Section 3.2 to an autoregressive model, and Section 3.3 to the contextual bandit case. Before delving into details, let us try to understand intuitively how to construct weights that have desirable properties.

The expression (8) reveals that the efficiency of ALEE depends on the projection of the data matrix \(\mathbf{X}_{n}\) on \(\mathbf{W}_{n}\). Thus, the efficiency of the ALEE approach can be measured by the principal angles between the random projections \(\mathbf{P}_{w}\) in (8) and \(\mathbf{P}_{x}=\mathbf{X}_{n}\mathbf{S}_{n}^{-1}\mathbf{X}_{n}^{\top}\). Accordingly, we define the _affinity_\(\mathcal{A}(\mathbf{W}_{n},\mathbf{X}_{n})\) of the weights \(\{\bm{w}_{t}\}_{t\geq 1}\) as the cosine of the largest principle angle, or equivalently

\[\mathcal{A}(\mathbf{W}_{n},\mathbf{X}_{n})=\sigma_{d}(\mathbf{P}_{w}\mathbf{P }_{x})=\sigma_{d}\big{(}\mathbf{U}_{w}^{\top}\mathbf{X}_{n}\mathbf{S}_{n}^{-1 /2}\big{)}\] (10)

as the \(d\)-th largest singular value of \(\mathbf{P}_{w}\mathbf{P}_{x}\). Formally, the above definition captures the cosine of the angle between the two subspaces spanned by the columns of \(\mathbf{X}_{n}\) and \(\mathbf{W}_{n}\), respectively [12]. Good weights \(\{\bm{w}_{t}\}_{t\geq 1}\) are those with relatively large affinity or

\[\mathbf{U}_{w}\propto\mathbf{X}_{n}\mathbf{S}_{n}^{-1/2}\qquad\text{(approximately)}.\] (11)

### Multi-armed bandits

In the context of the \(K\)-arm bandit problem, the Gram matrix has a diagonal structure, which means that we can focus on constructing weights \(\{\bm{w}_{t}\}_{t\geq 1}\) for each coordinate independently. For an arm \(k\in[K]\) and round \(t\geq 1\), define

\[s_{t,k}=s_{0}+\sum_{i=1}^{t}x_{i,k}^{2}\qquad\text{for some positive $s_{0}\in\mathcal{F}_{0}$}.\] (12)

Define the \(k\)-th coordinate of the weight \(\bm{w}_{t}\) as

\[w_{t,k}=f\Big{(}\frac{s_{t,k}}{s_{0}}\Big{)}\cdot\frac{x_{t,k}}{ \sqrt{s_{0}}}\quad\text{with}\quad f(x)=\sqrt{\frac{\log 2}{x\cdot\log(e^{2}x) \cdot(\log\log(e^{2}x))^{2}}}.\] (13)

The intuition behind the above construction is as follows. The discussion at near equation (11) indicates that the \(k\)-th coordinate of \(\bm{w}_{t}\) should be proportional to \(x_{t,k}/(\sum_{i\leq n}x_{i,k}^{2})^{1/2}\). However, the weight \(\bm{w}_{t}\) is required to be predictable, which can only depend on the data points 1 up to time \(t\). Consequently, we approximate the sum \(\sum_{i\leq n}x_{i,k}^{2}\) by the partial sum \(s_{t,k}\) in (12). Finally, note that

Footnote 1: Note that \(\bm{x}_{t,k}\in\mathcal{F}_{t-1}\) can be used to construct \(\bm{w}_{t}\)

\[w_{t,k}=f\Big{(}\frac{s_{t,k}}{s_{0}}\Big{)}\cdot\frac{x_{t,k}}{ \sqrt{s_{0}}}\approx\frac{x_{t,k}}{\sqrt{s_{t,k}}}.\] (14)

The logarithmic factors in (13) ensure that the stability conditions (6) hold. In the following theorem, we generalize the above method as a general strategy for constructing weights \(\{\bm{w}_{t}\}_{t\geq 1}\) satisfying the stability condition (6).

#### 3.1.1 Stable weight construction strategy

Consider a positive decreasing function \(f(x)\) on the interval \([1,\infty)\) with an increasing derivative \(f^{\prime}(x)\). Let \(f\) satisfy the conditions: \(f^{\prime}/f\) is increasing,

\[\int_{1}^{\infty}f^{2}(x)dx=1,\quad\text{and}\quad\int_{1}^{\infty}f(x)dx=\infty.\] (15)

With \(s_{0}\in\mathcal{F}_{0}\), we define weight \(w_{t,k}\) as

\[w_{t,k}=f\Big{(}\frac{s_{t,k}}{s_{0}}\Big{)}\frac{x_{t,k}}{\sqrt{s_{0}}}\qquad \text{with}\qquad s_{t,k}=s_{0}+\sum_{i=1}^{t}x_{i,k}^{2}.\] (16)

A key condition that ensures the weights \(\{w_{t,k}\}_{t\geq 1}\) satisfy the desirable stability property (6) is

\[\max_{1\leq t\leq n}f^{2}\Big{(}\frac{s_{t,k}}{s_{0}}\Big{)}\frac{x_{t,k}^{2}} {s_{0}}+\max_{1\leq t\leq n}\left(1-\frac{f(s_{t,k}/s_{0})}{f(s_{t-1,k}/s_{0} )}\right)+\int_{s_{n,k}/s_{0}}^{\infty}f^{2}(x)dx=o_{p}(1).\] (17)

For multi-armed bandits, this condition is automatically satisfied when both quantities \(1/s_{0}\) and \(s_{0}/s_{n,k}\) converge to zero in probability. Putting together the pieces, we have the following result for multi-armed bandits.

**Theorem 3.1**.: _Suppose condition (3) holds and \(1/s_{0}+s_{0}/s_{n,k}=o_{p}(1)\) for some \(k\in[K]\). Then, the \(k\)-th coordinate \(\widehat{\theta}_{\textsc{ALEE},k}\), obtained using weights from equation (16), satisfies_

\[(\widehat{\theta}_{\textsc{ALEE},k}-\theta_{k}^{*})\cdot\int_{1}^{s_{n,k}/s_{ 0}}\frac{\sqrt{s_{0}}}{\widehat{\sigma}}f(x)dx\stackrel{{ d}}{{ \longrightarrow}}\mathcal{N}(0,1),\] (18)

_where \(\widehat{\sigma}\) is a consistent estimate of \(\sigma\). Equivalently,_

\[\frac{(\widehat{\theta}_{\textsc{ALEE},k}-\theta_{k}^{*})}{\widehat{\sigma} \sqrt{\sum_{1\leq t\leq n}w_{t,k}^{2}}}\cdot\bigg{(}\sum_{t=1}^{n}w_{t,k}x_{t, k}\bigg{)}\stackrel{{ d}}{{\longrightarrow}}\mathcal{N}(0,1).\] (19)

The proof of Theorem 3.1 can be found in Section A.1 of the Appendix. A few comments regarding Theorem 3.1 are in order.

First, the above theorem enables us to construct valid CI in the estimation of the mean \(\theta_{k}^{*}\) for a sub-optimal arm \(k\) when employing an asymptotically optimal allocation rule to achieve the optimal regret in [18] with sample size \(\sum_{t\leq n}x_{t,k}\asymp\log n\), or when using a sub-optimal rule to achieve \(\text{polylog}(n)\). On the other hand, the classical martingale CLT is applicable to the optimal arm (if unique) under such asymptotically optimal or sub-optimal allocation rules. Consequently, one may obtain a valid CI for the optimal arm from the standard OLS estimate [19]. However, it is important to note that such CIs are not guaranteed for sub-optimal arms.

Figure 1: Empirical distribution of the standardized estimation errors from OLS and ALEE approach. Results are obtained with a dataset of size \(n=1000\) and \(3000\) independent replications. Left: AR\((1)\) model \(y_{t}=y_{t-1}+\epsilon_{t}\) with independent errors \(\epsilon_{t}\sim\mathcal{N}(0,1)\). Right: Two-armed bandit problem with equal arm mean \(\theta_{1}^{*}=\theta_{2}^{*}=0.3\) and independent noise \(\epsilon_{t}\sim\mathcal{N}(0,1)\). Figure 4 in Section C.3 considers the same setting with centered Poisson noise, which is not sub-Gaussian.

Next, while Theorem 3.1 holds for any \(s_{0}\) diverging to infinity but of smaller order than \(s_{n,k}\) ( which may depend on \(k\)), the convergence rate of \(\sum_{1\leq t\leq n}w_{t,k}\epsilon_{t}\) to normality is enhanced by choosing a large value for \(s_{0}\). In practical terms, it is advisable to choose an \(s_{0}\) that is slightly smaller than the best-known lower bound for \(s_{n,k}\).

Finally, the choice of function \(f\) determines the efficiency of ALEE estimator. For instance, taking function \(f(x)=1/x\), we obtain an estimator with asymptotic variance of order \(1/\{s_{0}\log^{2}(s_{n,k}/s_{0})\}\), which is only better than what one would get using stopping time results by a logarithmic factor. In the next Corollary, an improved choice of \(f\) yields near optimal variance up to logarithmic terms.

**Corollary 3.2**.: _Consider the same set of assumptions as stated in Theorem 3.1. The ALEE estimator \(\widehat{\theta}_{\textsc{ALEE},k}\), obtained by using \(f(x)=(\beta\log^{\beta}2)^{1/2}\{x(\log e^{2}x)(\log\log e^{2}x)^{1+\beta}\}^ {-1/2}\) for any \(\beta>0\) satisfies_

\[\sqrt{\frac{4\beta(\log 2)^{\beta}}{\log(s_{n,k}/s_{0})\{\log\log(s_{n,k}/s_{0}) \}^{1+\beta}}}\cdot\frac{\sqrt{s_{n,k}}(\widehat{\theta}_{\textsc{ALEE},k}- \theta_{k}^{*})}{\widehat{\sigma}}\stackrel{{ d}}{{\longrightarrow }}\mathcal{N}(0,1).\]

The proof of this corollary follows directly from Theorem 3.1. For \(s_{0}=\log n/\log\log n\) in multi-armed bandits with asymptotically optimal allocations, \(\log(s_{n,k}/s_{0})=(1+o(1))\log\log s_{n,k}\).

#### 3.1.2 Finite sample bounds for ALEE estimators

One may also construct finite sample confidence intervals for each arm via applying concentration bounds. Indeed, for any arm \(k\in K\), we have

\[\{\sum_{t=1}^{n}w_{t,k}x_{t,k}\}\cdot(\widehat{\theta}_{\textsc{ALEE},k}- \theta_{k}^{*})=\sum_{t=1}^{n}w_{t,k}\epsilon_{t}.\] (20)

Following the construction of \(w_{t,k}\in\mathcal{F}_{t-1}\), the term \(\sum_{t=1}^{n}w_{t,k}\epsilon_{t}\) is amenable to concentration inequalities if we assume that the noise \(\epsilon_{t}\) is sub-Gaussian conditioned on \(\mathcal{F}_{t-1}\), i.e.

\[\forall\lambda\in\mathbb{R}\qquad\qquad\mathbb{E}[e^{\lambda\epsilon_{t}}\mid \mathcal{F}_{t-1}]\leq e^{\sigma_{\beta}^{2}\lambda^{2}/2}.\] (21)

**Corollary 3.3** (Theorem 1 in [1]).: _Suppose the sub-Gaussian noise condition (21) is in force. Then for any \(\delta>0\) and \(\lambda_{0}>0\), the following bound holds with probability at least \(1-\delta\)_

\[\left|\sum_{t=1}^{n}w_{t,k}x_{t,k}\right|\cdot|\widehat{\theta}_{\textsc{ALEE },k}-\theta_{k}^{*}|\leq\sigma_{\beta}\sqrt{(\lambda_{0}+\sum_{t=1}^{n}w_{t,k }^{2})\cdot\log\left(\frac{\lambda_{0}+\sum_{t=1}^{n}w_{t,k}^{2}}{\delta^{2} \lambda_{0}}\right)}.\] (22)

**Remark 3.4**.: _In the context of multi-armed bandit, by considering the function \(f\) in Corollary 3.2 with \(\beta=1\) and Corollary 3.3 with \(\lambda_{0}=1\), we derive that with probability at least \(1-\delta\)_

\[|\widehat{\theta}_{\textsc{ALEE},k}-\theta_{k}^{*}|\leq\sigma_{g}\sqrt{\log(2 /\delta^{2})}\frac{\sqrt{2+\log(s_{n,k}/s_{0})}\log\{2+\log(s_{n,k}/s_{0})\}} {\sqrt{s_{n,k}}-\sqrt{s_{0}}}\] (23)

_provided \(s_{0}>1\). See Section A.2 of the Appendix for a proof of this argument. Recall that \(\sqrt{s_{n,k}}=(s_{0}+\sum_{i\leq n}x_{i,k}^{2})^{1/2}\), the bound is in the same spirit as existing finite sample bounds for the OLS estimator for arm means [1, 21]. In simple terms, the ALEE estimator behaves similarly to the OLS estimator in a non-asymptotic setting while still maintaining asymptotic normality._

### Autoregressive time series

Next, we focus on an autoregressive time series model

\[y_{t}=\theta^{*}y_{t-1}+\epsilon_{t}\quad\text{ for }t\in[n],\] (24)

where \(y_{0}=0\). Note that the above model is a special case of the adaptive linear model (2). It is well-known that when \(\theta^{*}\in(-1,1)\), the time series model (24) satisfies a stability assumption (1). Consequently, one might use the OLS estimate based confidence intervals [19] for \(\theta^{*}\). However, when \(\theta^{*}=1\) -- also known as the _unit root_ case -- stability condition (1) does not hold, and the least squares estimator is not asymptotically normal [19]. In other words, when \(\theta^{*}=1\), the least squares based intervals do not provide correct coverage.

In this section, we apply ALEE-based approach to construct confidence intervals that are valid for \(\theta^{*}\in[-1,1]\). Similar to previous sections, let \(s_{0}\in\mathcal{F}_{0}\) and denote \(s_{t}=s_{0}+\sum_{1\leq i\leq t}y_{i-1}^{2}\). Following a construction similar to the last section, we have the following corollary.

**Corollary 3.5**.: _Assume the noise variables \(\{\epsilon_{t}\}_{t}\) are i.i.d with mean zero, variance \(\sigma^{2}\) and sub-Gaussian parameter \(\sigma_{g}^{2}\). Then, for any \(\theta^{*}\in[-1,1]\), the ALEE estimator, obtained using \(w_{t}=f(s_{t}/s_{0})y_{t-1}/\sqrt{s_{0}}\) with function \(f\) from Corollary 3.2 and \(s_{0}=n/\log\log(n)\), satisfies_

\[\sqrt{\frac{4\beta(\log 2)^{\beta}}{\log(s_{n}/s_{0})\{\log\log(s_{n}/s_{0 })\}^{1+\beta}}}\cdot\frac{\sqrt{s_{n}}(\widehat{\theta}_{\textsc{ALE}}- \theta^{*})}{\widehat{\sigma}}\stackrel{{ d}}{{\longrightarrow}} \mathcal{N}(0,1).\] (25)

The proof of Corollary 3.5 can be found in Section A.3 of the Appendix.

### Contextual bandits

In contextual bandit problems, the task of defining adaptive weights that satisfy the stability condition (6) while maintaining a large affinity is challenging. Without loss of generality, we assume that \(\|\bm{x}_{t}\|_{2}\leq 1\). Following the discussion around (11) and using \(\mathbf{S}_{t}\) as an approximation of \(\mathbf{S}_{n}\), we see that a good choice for the weight is \(\bm{w}_{t}\approx\mathbf{S}_{t}^{-\frac{1}{2}}\bm{x}_{t}\). However, it is not all clear at the moment why the above choice produces \(d-\)dimensional weights \(\bm{w}_{t}\) satisfying the stability condition (6). It turns out that the success of our construction is based on the variability of certain matrix \(\mathbf{V}_{t}\). For a \(\mathcal{F}_{0}\)-measurable \(d\times d\) symmetric matrix \(\bm{\Sigma}_{0}\succeq\mathbf{I}_{d}\) and \(t\in[n]\), we define

\[\bm{\Sigma}_{t}=\bm{\Sigma}_{0}+\sum_{i=1}^{t}\bm{x}_{i}\bm{x}_{i }^{\top}\qquad\text{and}\qquad\bm{z}_{t}=\bm{\Sigma}_{t-1}^{-\frac{1}{2}}\bm{ x}_{t}.\] (26)

For \(t\in[n]\), we define the variability matrix \(\mathbf{V}_{t}\) as

\[\mathbf{V}_{t}=\left(\mathbf{I}_{d}+\sum_{i=1}^{t}\bm{z}_{i}\bm{ z}_{i}^{\top}\right)^{-1}\qquad\text{(Variability)}.\] (27)

The variability matrix \(\mathbf{V}_{t}\) comes up frequently in finite sample analysis of the least squares estimator [16; 19; 2], the generalized linear models with adaptive data [23], and in online optimization [6]; see comments after Theorem 3.6 for a more detailed discussion on the matrix \(\mathbf{V}_{t}\). Now, we define weights \(\{\bm{w}_{t}\}_{t\geq 1}\) as

\[\bm{w}_{t}=\sqrt{1+\bm{z}_{t}^{\top}\mathbf{V}_{t-1}\bm{z}_{t}} \cdot\mathbf{V}_{t}\bm{z}_{t}.\] (28)

**Theorem 3.6**.: _Suppose condition (3) holds and \(\|\bm{\Sigma}_{0}^{-1}\|_{\textnormal{op}}+\|\mathbf{V}_{n}\|_{\textnormal{ op}}=o_{p}(1)\). Then, the ALEE estimator \(\widehat{\bm{\theta}}_{\textsc{ALEE}}\), obtained using the weights \(\{\bm{w}_{t}\}_{1\leq t\leq n}\) from (28), satisfies_

\[\left(\sum_{t=1}^{n}\bm{w}_{t}\bm{x}_{t}\right)\cdot(\widehat{ \bm{\theta}}_{\textsc{ALEE}}-\bm{\theta}^{*})\stackrel{{ d}}{{ \longrightarrow}}\mathcal{N}\big{(}\bm{0},\sigma^{2}\mathbf{I}_{d}\big{)}.\]

The proof of Theorem 3.6 can be found in Section A.4 of the Appendix. In Theorem B.4 in the appendix, we establish the asymptotic normality of a modified version of the ALEE estimator, which has the same asymptotic variance as the one in Theorem 3.6 under the assumption \(\|\bm{\Sigma}_{0}^{-1}\|_{\textnormal{op}}=o_{p}(1)\). In other words, the modified theorem B.4 does not assume any condition on the \(\|\mathbf{V}_{n}\|_{\textnormal{op}}\).

To better convey the idea of our construction, we provide a lemma that may be of independent interest. This lemma applies to weights \(\bm{w}_{t}\) generated by (27) and (28) with general \(\bm{z}_{t}\).

**Lemma 3.7**.: _Let \(\bm{w}_{t}\) be as in (28) with the variability matrix \(\mathbf{V}_{t}\) in (27). Then,_

\[\sum_{t=1}^{n}\bm{w}_{t}\bm{w}_{t}^{\top}=\mathbf{I}_{d}-\mathbf{ V}_{n},\quad\max_{1\leq t\leq n}\|\bm{w}_{t}\|_{2}=\max_{1\leq t\leq n}\| \mathbf{V}_{t-1}\bm{z}_{t}\|_{2}/(1+\bm{z}_{t}^{\top}\mathbf{V}_{t-1}\bm{z}_{ t})^{1/2}.\] (29)

_For \(\bm{z}_{t}\in\mathcal{F}_{t-1}\), the stability condition (6) holds when \(\max_{1\leq t\leq n}\bm{z}_{t}^{\top}\mathbf{V}_{t}\bm{z}_{t}+\|\mathbf{V}_{n}\|_ {\textnormal{op}}=o_{p}(1)\)._Proof.For any \(t\geq 1\), \(\mathbf{V}_{t}=\mathbf{V}_{t-1}-\mathbf{V}_{t-1}\boldsymbol{z}_{t}\boldsymbol{z}_{ t}^{\top}\mathbf{V}_{t-1}/(1+\boldsymbol{z}_{t}^{\top}\mathbf{V}_{t-1} \boldsymbol{z}_{t})\). It follows that \(\mathbf{V}_{t}\boldsymbol{z}_{t}=\mathbf{V}_{t-1}\boldsymbol{z}_{t}/(1+ \boldsymbol{z}_{t}^{\top}\mathbf{V}_{t-1}\boldsymbol{z}_{t})\) and \(\sum_{t=1}^{n}\boldsymbol{w}_{t}\boldsymbol{w}_{t}^{\top}=\sum_{t=1}^{n} \mathbf{V}_{t-1}(\mathbf{V}_{t}^{-1}-\mathbf{V}_{t-1}^{-1})\mathbf{V}_{t}= \mathbf{I}_{d}-\mathbf{V}_{n}\).

Comments on Theorem 3.6 conditions:It is instructive to compare the conditions of Theorem 3.1 and Theorem 3.6. The condition \(\|\boldsymbol{\Sigma}_{0}^{-1}\|_{\text{op}}=o_{p}(1)\) is an analogue of the condition \(1/s_{0}=o_{p}(1)\). The condition \(\|\mathbf{V}_{n}\|_{\text{op}}=o_{p}(1)\) is a bit more subtle. This condition is an analogue of the condition \(s_{0}/s_{n,k}=o_{p}(1)\). Indeed, applying elliptical potential lemma [2, Lemma 4] yields

\[\frac{\log(\det(\boldsymbol{\Sigma}_{0}+\mathbf{S}_{n}))}{\log(\det( \boldsymbol{\Sigma}_{0}))}\leq\text{trace}(\mathbf{V}_{n}^{-1})-d=\sum_{t=1}^{ n}\boldsymbol{x}_{t}^{\top}\boldsymbol{\Sigma}_{t-1}^{-1}\boldsymbol{x}_{t} \leq 2\cdot\frac{\log(\det(\boldsymbol{\Sigma}_{0}+\mathbf{S}_{n}))}{\log(\det( \boldsymbol{\Sigma}_{0}))}\] (30)

where \(\mathbf{S}_{n}=\sum_{i=1}^{n}\boldsymbol{x}_{i}\boldsymbol{x}_{i}^{\top}\) is the Gram matrix. We see that for \(\|\mathbf{V}_{n}\|_{\text{op}}=o_{p}(1)\), it is necessary that the eigenvalues of \(\mathbf{S}_{n}\) grow to infinity at a faster rate than the eigenvalues of \(\boldsymbol{\Sigma}_{0}\). Moreover, in the case of dimension \(d=1\), the condition \(\|\mathbf{V}_{n}\|_{\text{op}}=o_{p}(1)\) is equivalent to \(s_{0}/s_{n,k}=o_{p}(1)\).

## 4 Numerical experiments

In this section, we consider three settings: two-armed bandit setting, first order auto-regressive model setting and contextual bandit setting. In two-armed bandit setting, the rewards are generated with same arm mean \((\theta_{1}^{*},\theta_{2}^{*})=(0.3,0.3)\), and noise is generated from a normal distribution with mean \(0\) and variance \(1\). To collect two-armed bandit data, we use \(\epsilon\)-Greedy algorithm with decaying exploration rate \(\sqrt{\log(t)/t}\). The rate is designed to make sure the number of times each armed is pulled has order greater than \(\log(n)\) up to time \(n\). In the second setting, we consider the time series model,

\[y_{t}=\theta^{*}y_{t-1}+\epsilon_{t},\] (31)

where \(\theta^{*}=1\) and noise \(\epsilon_{t}\) is drawn from a normal distribution with mean \(0\) and variance \(1\). In the contextual bandit setting, we consider the true parameter \(\boldsymbol{\theta}^{*}\) to be \(0.3\) times the all-one vector. In the initial iterations, a random context \(\boldsymbol{x}_{t}\) is generated from a uniform distribution in \(\mathcal{S}^{d-1}\). Then, we apply \(\epsilon\)-Greedy algorithm to these pre-selected contexts with decaying exploration rate \(\log^{2}(t)/t\). For all of the above three settings, we run \(1000\) independent replications.

To analyze the data we collect for these settings, we apply ALEE approach with weights specified in Corollary 3.2, 3.5 and Theorem B.4, respectively. More specifically, in the first two settings, we consider \(\beta=1\) in Corollary 3.2. For two-armed bandit example, we set \(s_{0}=e^{2}\log(n)\), which is known to be a lower bound for \(s_{n,1}\). For AR(1) model, we consider \(s_{0}=e^{2}n/\log\log(n)\). For the contextual bandit example, we consider \(\boldsymbol{\Sigma}_{0}=\log(n)\cdot\mathbf{I}_{d}\). In the simulations, we also compare ALEE approach to the normality based confidence interval for OLS estimator [19] (which may be incorrect), the concentration bounds for the OLS estimator based on self-normalized martingale sequence [1], and W-decorrelation [8]. Detailed implementations about these methods can be found in Appendix C.1.

In Figure 2, we display results for two-armed bandit example, providing the empirical coverage plots for the first arm mean \(\theta_{1}^{*}\) as well as average width for two-sided CIs. We observe that CIs based on

Figure 2: Two-armed bandit problem with equal arm mean \(\theta_{1}^{*}=\theta_{2}^{*}=0.3\). Error bars plotted are \(\pm\) standard errors.

OLS undercover \(\theta_{1}^{*}\) while other methods provide satisfactory coverage. Notably, from the average CI width plot, we can see that W-decorrelation and concentration methods have relatively large CI widths. On the contrary, ALEE-based CIs achieve target coverage while keeping the width of CIs relatively small.

For AR(1) model, we display the results in Figure 3. For the context bandit example, we consider \(d=20\) and summarize the empirical coverage probability and the logarithm of the volume of the confidence regions in Table 1, along with corresponding standard deviations. See Appendix C.2 for experiments with dimension \(d=10\) and \(d=50\).

## 5 Discussion

In this paper, we study the parameter estimation problem in an adaptive linear model. We propose to use ALEE (adaptive linear estimation equations) to obtain point and interval estimates. Our main contribution is to propose an estimator which is asymptotically normal without requiring any stability condition on the sample covariance matrix. Unlike the concentration based confidence regions, our proposed confidence regions allow for heavy tailed noise variables. We demonstrate the utiltity of our method by comparing our method with existing methods.

Our work leaves several questions open for future research. For example, it would be interesting to characterize the variance of the ALEE estimator compared to the best possible variance[13; 20] for \(d>1\). It would also be interesting to know if such results can be extended to non-linear adaptive models, e.g., to an adaptive generalized linear model [23]. Furthermore, our paper assumes a fixed dimension \(d\) for the problem while letting \(n\rightarrow\infty\). It would be interesting to explore whether we can allow the dimension to grow with the number of samples at a specific rate.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Method & \multicolumn{4}{c}{Level of confidence} \\ \hline  & 0.8 & \multicolumn{2}{c}{0.85} & \multicolumn{2}{c}{0.9} \\ \hline  & Avg. Coverage & Avg. Log(Volume) & Avg. Coverage & Avg. Log(Volume) & Avg. Coverage & Avg. log(Volume) \\ ALEE & 0.805 (\(\pm\) 0.396) & 6.541 (\(\pm\) 0.528) & 0.861 (\(\pm\) 0.346) & 7.108 (\(\pm\) 0.528) & 0.910 (\(\pm\) 0.286) & 7.806 (\(\pm\) 0.528) \\ OLS & 0.776 (\(\pm\) 0.417) & -2.079 (\(\pm\) 0.525) & 0.830 (\(\pm\) 0.376) & -1.513 (\(\pm\) 0.525) & 0.881 (\(\pm\) 0.324) & -0.815 (\(\pm\) 0.525) \\ W-Decorrelation & 0.777 (\(\pm\) 0.416) & 25.727 (\(\pm\) 0.518) & 0.829 (\(\pm\) 0.377) & 26.294 (\(\pm\) 0.518) & 0.870 (\(\pm\) 0.336) & 26.992 (\(\pm\) 0.518) \\ Concentration & 1.000 (\(\pm\) 0.000) & 17.374 (\(\pm\) 0.506) & 1.000 (\(\pm\) 0.000) & 17.408(\(\pm\) 0.506) & 1.000 (\(\pm\) 0.000) & 17.455 (\(\pm\) 0.506) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Contextual bandit: d = 20

Figure 3: AR(1) with model coefficient \(\theta^{*}=1\) and \(s_{0}=e^{2}n/\log\log(n)\). Error bars plotted are \(\pm\) standard errors.

## Acknowledgments

This work was partially supported by the National Science Foundation Grants DMS-2311304, CCF-1934924, DMS-2052949 and DMS-2210850.

## References

* [1] Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_, 24, 2011.
* [2] Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Online least squares estimation with self-normalized processes: An application to bandit problems. _arXiv preprint arXiv:1102.2670_, 2011.
* [3] Deepak Agarwal, Bee-Chung Chen, Pradheep Elango, Nitin Motgi, Seung-Taek Park, Raghu Ramakrishnan, Scott Roy, and Joe Zachariah. Online models for content optimization. _Advances in Neural Information Processing Systems_, 21, 2008.
* [4] Aurelien Bibaut, Maria Dimakopoulou, Nathan Kallus, Antoine Chambaz, and Mark van Der Laan. Post-contextual-bandit inference. _Advances in neural information processing systems_, 34:28548-28559, 2021.
* [5] Jack Bowden and Lorenzo Trippa. Unbiased estimation for response adaptive clinical trials. _Statistical methods in medical research_, 26(5):2376-2388, 2017.
* [6] Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit feedback. 2008.
* [7] Yash Deshpande, Adel Javanmard, and Mohammad Mehrabi. Online debiasing for adaptively collected high-dimensional data with applications to time series analysis. _Journal of the American Statistical Association_, pages 1-14, 2021.
* [8] Yash Deshpande, Lester Mackey, Vasilis Syrgkanis, and Matt Taddy. Accurate inference for adaptive linear models. In _International Conference on Machine Learning_, pages 1194-1203. PMLR, 2018.
* [9] Maria Dimakopoulou, Zhimei Ren, and Zhengyuan Zhou. Online multi-armed bandits with adaptive inference. _Advances in Neural Information Processing Systems_, 34:1939-1951, 2021.
* [10] Vitor Hadad, David A Hirshberg, Ruohan Zhan, Stefan Wager, and Susan Athey. Confidence intervals for policy evaluation in adaptive experiments. _Proceedings of the National Academy of Sciences_, 118(15):e2014602118, 2021.
* [11] Inge S Helland. Central limit theorems for martingales with discrete or continuous time. _Scandinavian Journal of Statistics_, pages 79-94, 1982.
* [12] Ilse CF Ipsen and Carl D Meyer. The angle between complementary subspaces. _The American mathematical monthly_, 102(10):904-911, 1995.
* [13] Koulik Khamaru, Yash Deshpande, Lester Mackey, and Martin J Wainwright. Near-optimal inference in adaptive linear regression. _arXiv preprint arXiv:2107.02266_, 2021.
* [14] Tze Leung Lai. Asymptotic properties of nonlinear least squares estimates in stochastic regression models. _The Annals of Statistics_, pages 1917-1930, 1994.
* [15] Tze Leung Lai and Herbert Robbins. Adaptive design and stochastic approximation. _The annals of Statistics_, pages 1196-1221, 1979.
* [16] Tze Leung Lai and Herbert Robbins. Adaptive design and stochastic approximation. _The annals of Statistics_, pages 1196-1221, 1979.
* [17] Tze Leung Lai and Herbert Robbins. Consistency and asymptotic efficiency of slope estimates in stochastic approximation schemes. _Zeitschrift fur Wahrscheinlichkeitstheorie und verwandte Gebiete_, 56(3):329-360, 1981.
* [18] Tze Leung Lai, Herbert Robbins, et al. Asymptotically efficient adaptive allocation rules. _Advances in applied mathematics_, 6(1):4-22, 1985.
* [19] Tze Leung Lai and Ching Zong Wei. Least squares estimates in stochastic regression models with applications to identification and control of dynamic systems. _The Annals of Statistics_, 10(1):154-166, 1982.

* Lattimore [2023] Tor Lattimore. A lower bound for linear and kernel regression with adaptive covariates. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 2095-2113. PMLR, 2023.
* Lattimore and Szepesvari [2017] Tor Lattimore and Csaba Szepesvari. The end of optimism? an asymptotic analysis of finite-armed linear bandits. In _Artificial Intelligence and Statistics_, pages 728-737. PMLR, 2017.
* Li et al. [2010] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation. In _Proceedings of the 19th international conference on World wide web_, pages 661-670, 2010.
* Li et al. [2017] Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contextual bandits. In _International Conference on Machine Learning_, pages 2071-2080. PMLR, 2017.
* Liao et al. [2020] Peng Liao, Kristjan Greenewald, Predrag Klasnja, and Susan Murphy. Personalized heartsteps: A reinforcement learning algorithm for optimizing physical activity. _Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies_, 4(1):1-22, 2020.
* Lin et al. [2023] Licong Lin, Koulik Khamaru, and Martin J Wainwright. Semi-parametric inference based on adaptively collected data. _arXiv preprint arXiv:2303.02534_, 2023.
* Luedtke and Van Der Laan [2016] Alexander R Luedtke and Mark J Van Der Laan. Statistical inference for the mean outcome under a possibly non-unique optimal treatment strategy. _Annals of statistics_, 44(2):713, 2016.
* Neel and Roth [2018] Seth Neel and Aaron Roth. Mitigating bias in adaptive data gathering via differential privacy. In _International Conference on Machine Learning_, pages 3720-3729. PMLR, 2018.
* Nie et al. [2018] Xinkun Nie, Xiaoying Tian, Jonathan Taylor, and James Zou. Why adaptively collected data have negative bias and how to correct for it. In _International Conference on Artificial Intelligence and Statistics_, pages 1261-1269. PMLR, 2018.
* Shin et al. [2019] Jaehyeok Shin, Aaditya Ramdas, and Alessandro Rinaldo. Are sample means in multi-armed bandits positively or negatively biased? _Advances in Neural Information Processing Systems_, 32, 2019.
* Shin et al. [2019] Jaehyeok Shin, Aaditya Ramdas, and Alessandro Rinaldo. On the bias, risk and consistency of sample means in multi-armed bandits. _arXiv preprint arXiv:1902.00746_, 2019.
* Singh et al. [2020] Krishna Kumar Singh, Dhruv Mahajan, Kristen Grauman, Yong Jae Lee, Matt Feiszli, and Deepti Ghadiyaram. Don't judge an object by its context: Learning to overcome contextual bias. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11070-11078, 2020.
* Vershynin [2018] Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* Wainwright [2019] Martin J Wainwright. _High-dimensional statistics: A non-asymptotic viewpoint_, volume 48. Cambridge university press, 2019.
* Xu et al. [2013] Min Xu, Tao Qin, and Tie-Yan Liu. Estimation bias in multi-armed bandit algorithms for search advertising. _Advances in Neural Information Processing Systems_, 26, 2013.
* Yom-Tov et al. [2017] Elad Yom-Tov, Guy Feraru, Mark Kozdoba, Shie Mannor, Moshe Tennenholtz, and Irit Hochberg. Encouraging physical activity in patients with diabetes: intervention using a reinforcement learning system. _Journal of medical Internet research_, 19(10):e338, 2017.
* Zhan et al. [2021] Ruohan Zhan, Vitor Hadad, David A Hirshberg, and Susan Athey. Off-policy evaluation via adaptive weighting with data from contextual bandits. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 2125-2135, 2021.
* Zhan et al. [2023] Ruohan Zhan, Zhimei Ren, Susan Athey, and Zhengyuan Zhou. Policy learning with adaptively collected data. _Management Science_, 2023.
* Zhang et al. [2020] Kelly Zhang, Lucas Janson, and Susan Murphy. Inference for batched bandits. _Advances in neural information processing systems_, 33:9818-9829, 2020.

## Appendix A Proof

In Theorem 3.1, Corollary 3.3 and Remark 3.4, we deal with an arm with index \(k\in[K]\). To simplify notations, we drop the subscript \(k\) in \(s_{t,k}\), \(w_{t,k}\), \(x_{t,k}\), \(\widehat{\theta}_{\textsc{\tiny{ALEE}},k}\) and \(\theta_{k}^{*}\) throughout the proof, and use \(s_{t}\), \(w_{t}\), \(x_{t}\), \(\widehat{\theta}_{\textsc{\tiny{ALEE}}}\) and \(\theta^{*}\), respectively.

### Proof of Theorem 3.1

Condition (17) serves as an important role in proving (18). Therefore, we start our proof by verifying the condition (17). Since function \(f\) is a positive decreasing function, we first have

\[\max_{1\leq t\leq n}f^{2}(\frac{s_{t}}{s_{0}})\frac{x_{t}^{2}}{s_{0}}\leq f^{ 2}(1)\frac{1}{s_{0}}.\] (32)

Furthermore, since function \(f^{\prime}/f\) is increasing, we have

\[\begin{split}&\max_{1\leq t\leq n}\left(1-\frac{f(s_{t}/s_{0})}{f(s _{t-1}/s_{0})}\right)=\max_{1\leq t\leq n}\frac{f(s_{t-1}/s_{0})-f(s_{t}/s_{0} )}{f(s_{t-1}/s_{0})}\\ &\overset{(i)}{\leq}\frac{1}{s_{0}}\max_{1\leq t\leq n}\frac{-f ^{\prime}(s_{t-1}/s_{0})}{f(s_{t-1}/s_{0})}=\frac{1}{s_{0}}\frac{-f^{\prime}(1 )}{f(1)},\end{split}\] (33)

where inequality \((i)\) follows from mean value theorem and the monotonicity of the function \(f^{\prime}/f\). Thus, by assuming \(1/s_{0}=o_{p}(1)\) and \(s_{0}/s_{n}=o_{p}(1)\), condition (17) follows directly from equation (32) and equation (33).

By the construction of ALEE estimator, we have

\[\bigg{\{}\sum_{t=1}^{n}w_{t}x_{t}\bigg{\}}\cdot(\widehat{\theta}_{\textsc{ \tiny{ALEE}}}-\theta^{*})=\sum_{t=1}^{n}w_{t}\epsilon_{t}.\] (34)

Note that

\[\sum_{t=1}^{n}w_{t}x_{t}=\sqrt{s_{0}}\sum_{t=1}^{n}f(s_{t}/s_{0})\frac{x_{t}^ {2}}{s_{0}}=\sqrt{s_{0}}\int_{1}^{s_{n}/s_{0}}f(x)dx\cdot\frac{\sum_{t\leq n}f (s_{t}/s_{0})x_{t}^{2}/s_{0}}{\int_{1}^{s_{n}/s_{0}}f(x)dx}.\] (35)

By the mean value theorem, we have that for \(t\in[n]\), \(\xi_{t}\in[s_{t-1},s_{t}]\)

\[\int_{s_{t-1}/s_{0}}^{s_{t}/s_{0}}f(x)dx=f(\xi_{t}/s_{0})\frac{x_{t}^{2}}{s_{0 }}.\]Therefore, we have

\[\frac{\sum_{t\leq n}f(s_{t}/s_{0})x_{t}^{2}/s_{0}}{\int_{1}^{s_{n}/s_{0}}f(x)dx}=1+ \underbrace{\frac{\sum_{t\leq n}(\frac{f(s_{t}/s_{0})}{f(t_{t}/s_{0})}-1)f(\xi_{t }/s_{0})x_{t}^{2}/s_{0}}{\sum_{t\leq n}f(\xi_{t}/s_{0})x_{t}^{2}/s_{0}}}_{ \stackrel{{\Delta}}{{=}}R}.\]

Observe that

\[|R| \leq\frac{\sum_{t\leq n}\frac{|f(s_{t}/s_{0})}{f(\xi_{t}/s_{0})}-1| f(\xi_{t}/s_{0})x_{t}^{2}/s_{0}}{\sum_{t\leq n}f(\xi_{t}/s_{0})x_{t}^{2}/s_{0}}\] \[\leq\frac{\sum_{t\leq n}|\frac{f(s_{t}/s_{0})}{f(s_{t-1}/s_{0})}- 1|f(\xi_{t}/s_{0})x_{t}^{2}/s_{0}}{\sum_{t\leq n}f(\xi_{t}/s_{0})x_{t}^{2}/s_{0 }}\] \[\leq\max_{1\leq t\leq n}\left(1-\frac{f(s_{t}/s_{0})}{f(s_{t-1}/s_ {0})}\right)\stackrel{{(ii)}}{{=}}o_{p}(1).\]

Equality \((ii)\) follows from condition (17). Consequently, applying Slutsky's theorem yields

\[\frac{\sum_{i=1}^{n}w_{t}x_{t}}{\sqrt{s_{0}}\int_{1}^{s_{n}/s_{0}}f(x)dx} \stackrel{{ p}}{{\longrightarrow}}1.\]

Similarly, we can derive

\[\sum_{t=1}^{n}w_{t}^{2}=\sum_{t=1}^{n}f^{2}(s_{t}/s_{0})\frac{x_{t}^{2}}{s_{0 }}=(1+o_{p}(1))\int_{1}^{s_{n}/s_{0}}f^{2}(x)dx=1+o_{p}(1).\] (36)

Knowing \(\max_{1\leq t\leq n}w_{t}^{2}=\max_{1\leq t\leq n}f^{2}(s_{t}/s_{0})x_{t}^{2} /s_{0}=o_{p}(1)\), which is a consequence of equation (17), martingale central limit theorem together with an application of Slutsky's theorem yields

\[(\widehat{\theta}_{\textsc{{\tiny ALEE}}}-\theta^{*})\cdot\int_{1}^{s_{n}/s_ {0}}\frac{\sqrt{s_{0}}}{\widehat{\sigma}}f(x)dx\stackrel{{ d}}{{ \longrightarrow}}\mathcal{N}(0,1).\]

Lastly, we recall that

\[\frac{\widehat{\theta}_{\textsc{{\tiny ALEE}}}-\theta^{*}}{\widehat{\sigma} \sqrt{\sum_{t\leq n}w_{t}^{2}}}\cdot\bigg{(}\sum_{t=1}^{n}w_{t}x_{t}\bigg{)}= \frac{1}{\widehat{\sigma}\sqrt{\sum_{t\leq n}w_{t}^{2}}}\sum_{t=1}^{n}w_{t} \epsilon_{t}.\]

Therefore, equation (19) follows from martingale central limit theorem and Slutsky's theorem.

**Remark A.1**.: _Equation (18) sheds light on the asymptotic variance of the ALEE estimator, thereby aiding in the selection of a suitable function \(f\) to improve the efficiency of ALEE estimator. On the other hand, equation (19) offers a practical approach to obtaining an asymptotically precise confidence interval._

**Remark A.2**.: _Condition (17) is a general requirement that governs equation (18), and is not specific to bandit problems. However, the difficulty in verifying (17) can vary depending on the problem at hand._

### Proof of Remark 3.4

Corollary 3.3 follows directly from Theorem 1 in [1]. In this section, we provide a proof of Remark 3.4. By considering \(\lambda_{0}=1\) in Corollary 3.3, we have with probability at least \(1-\delta\)

\[\bigg{|}\sum_{t=1}^{n}w_{t}x_{t}\bigg{|}\cdot|\widehat{\theta}_{\textsc{{ \tiny ALEE}}}-\theta^{*}|\leq\sigma_{g}\sqrt{(1+\sum_{t=1}^{n}w_{t}^{2})\cdot \log\bigg{(}\frac{1+\sum_{t=1}^{n}w_{t}^{2}}{\delta^{2}}\bigg{)}}.\] (37)

By the construction of the weights in Corollary 3.2, we have

\[\sum_{t=1}^{n}w_{t}^{2}=\sum_{t=1}^{n}f^{2}(\frac{s_{t}}{s_{0}})\frac{x_{t}^{2} }{s_{0}}\leq\int_{1}^{\infty}f^{2}(x)dx=1.\] (38)Therefore, to complete the proof, it suffices to characterize a lower bound for \(\sum_{1\leq t\leq n}w_{t}x_{t}\). By definition, we have

\[\begin{split}\sum_{t=1}^{n}w_{t}x_{t}&=\sum_{t=1}^{n}f( s_{t}/s_{0})\frac{x_{t}^{2}}{\sqrt{s_{0}}}\\ &\stackrel{{(i)}}{{=}}\sum_{t=1}^{n}\frac{x_{t}^{2}} {(s_{t}\log(e^{2}s_{t}/s_{0}))^{1/2}\log\log(e^{2}s_{t}/s_{0})}\\ &\geq\frac{1}{(2+\log(s_{n}/s_{0}))^{1/2}\log(2+\log(s_{n}/s_{0} ))}\sum_{t=1}^{n}\frac{x_{t}^{2}}{\sqrt{s_{t}}}\\ &\stackrel{{(ii)}}{{\geq}}\frac{1}{(2+\log(s_{n}/s_{ 0}))^{1/2}\log(2+\log(s_{n}/s_{0}))}\cdot 2(\sqrt{s_{n}}-\sqrt{s_{0}})\sqrt{ \frac{s_{0}}{1+s_{0}}}\\ &\stackrel{{(iii)}}{{\geq}}\frac{1}{(2+\log(s_{n}/s_{ 0}))^{1/2}\log(2+\log(s_{n}/s_{0}))}\cdot\sqrt{2}(\sqrt{s_{n}}-\sqrt{s_{0}}). \end{split}\] (39)

In equation \((i)\), we plug in the expression of function \(f\) and hence \(\sqrt{s_{0}}\) cancels out. Since \(x_{t}\) is either \(0\) or \(1\), inequality \((ii)\) follows from the integration of the function \(h(x)=1/\sqrt{x}\). Inequality \((iii)\) follows from \(s_{0}>1\). Putting things together, we have

\[\begin{split}|\widehat{\theta}_{\textsc{ALEE}}-\theta^{*}|& \leq\sigma_{g}\frac{\sqrt{2\log(2/\delta^{2})}}{\sum_{1\leq t\leq n }w_{t}x_{t}}\\ &\leq\sigma_{g}\sqrt{\log(2/\delta^{2})}\frac{\sqrt{2+\log(s_{n} /s_{0})}\log\{2+\log(s_{n}/s_{0})\}}{\sqrt{s_{n}}-\sqrt{s_{0}}}.\end{split}\] (40)

This completes our proof of Remark 3.4.

### Proof of Corollary 3.5

Note that it suffices to verify the following condition (41)

\[\max_{1\leq t\leq n}f^{2}\Big{(}\frac{s_{t}}{s_{0}}\Big{)}\frac{y_{t-1}^{2}}{ s_{0}}+\max_{1\leq t\leq n}\left(1-\frac{f(s_{t}/s_{0})}{f(s_{t-1}/s_{0})} \right)+\int_{s_{n}/s_{0}}^{\infty}f^{2}(x)dx=o_{p}(1)\] (41)

for \(\theta^{*}\in[-1,1]\) in order to complete the proof of Corollary 3.5. The other part of the proof can be adapted from the proof of Theorem 3.1. To simplify notations, we let

\[T_{1}\stackrel{{\Delta}}{{=}}\max_{1\leq t\leq n}f^{2}\Big{(} \frac{s_{t}}{s_{0}}\Big{)}\frac{y_{t-1}^{2}}{s_{0}},\quad T_{2}\stackrel{{ \Delta}}{{=}}\max_{1\leq t\leq n}\left(1-\frac{f(s_{t}/s_{0})}{f(s_{t-1}/s_{0 })}\right),\quad\text{and}\quad T_{3}\stackrel{{\Delta}}{{=}}\int _{s_{n}/s_{0}}^{\infty}f^{2}(x)dx.\]

Therefore, proving equation (41) is equivalent to showing that \(T_{1}\), \(T_{2}\), and \(T_{3}\) converge to zero in probability. We will now demonstrate the convergence of each of these three terms to zero in probability.

\(T_{1}\) with \(\theta^{*}\) = 1:To prove \(T_{1}=o_{p}(1)\), we make use of a result in [19, Equation 3.23], which is

\[\mathbb{P}\left(\liminf_{n\to\infty}n^{-2}(\log\log n)\sum_{t=1}^{n}y_{t-1}^{ 2}=\sigma^{2}/4\right)=1.\] (42)

Observe that

\[\begin{split} T_{1}=\max_{1\leq t\leq n}f^{2}(\frac{s_{t}}{s_{0} })\frac{y_{t-1}^{2}}{s_{0}}&=\max_{1\leq t\leq n}\frac{y_{t-1}^{ 2}}{s_{t}\log(e^{2}s_{t}/s_{0})\{\log\log(e^{2}s_{t}/s_{0})\}^{1+\beta}}\\ &\leq\max_{1\leq t\leq n}\frac{y_{t-1}^{2}}{s_{t}\log(e^{2})\{ \log\log(e^{2})\}^{1+\beta}}\\ &=\frac{1}{2(\log 2)^{1+\beta}}\max_{1\leq t\leq n}\frac{y_{t-1}^{2}}{s_ {t}}\\ &\leq\frac{1}{2(\log 2)^{1+\beta}}\max\{\max_{1\leq t\leq n^{2 /3}}\frac{y_{t-1}^{2}}{s_{0}},\max_{\lfloor n^{2/3}\rfloor+1\leq t\leq n} \frac{y_{t-1}^{2}}{s_{\lfloor n^{2/3}\rfloor-s_{0}}}\}\end{split}\] (43)

In equation (43), we split the sequence into two parts and set different lower bounds for \(s_{t}\). The major benefit of this step is to help us derive a better choice of \(s_{0}\). Now we bound \(\max_{1\leq t\leq\lfloor n^{2/3}\rfloor}y_{t-1}^{2}\) and \(\max_{\lfloor n^{2/3}\rfloor+1\leq t\leq n}y_{t-1}^{2}\). Note that

\[\mathbb{P}\left(\max_{1\leq t\leq\lfloor n^{2/3}\rfloor}y_{t-1}^{2 }\geq\epsilon\right)\] (44) \[= \mathbb{P}\left(\max\{\max_{1\leq t\leq\lfloor n^{2/3}\rfloor}y_{t -1},\max_{1\leq t\leq\lfloor n^{2/3}\rfloor}-y_{t-1}\}\geq\sqrt{\epsilon}\right)\] \[\leq \frac{\mathbb{E}\left[\max\{\max_{1\leq t\leq\lfloor n^{2/3} \rfloor}y_{t-1},\max_{1\leq t\leq\lfloor n^{2/3}\rfloor}-y_{t-1}\}\right]}{ \sqrt{\epsilon}}\] \[\overset{(i)}{\leq} \sqrt{\frac{2n^{2/3}\sigma_{g}^{2}\log(2n^{2/3})}{\epsilon}},\]

where inequality is derived from (33, Exercise 2.12) and the fact that \(y_{i}\) is sub-Gaussian with sub-Gaussian parameter \(\sigma_{g}^{2}n^{2/3}\) for \(i\leq\lfloor n^{2/3}\rfloor\). Therefore, we conclude that

\[\max_{1\leq t\leq\lfloor n^{2/3}\rfloor}y_{t-1}^{2}=O_{p}(n^{2/3}\log n).\]

Consequently, we have

\[\max_{1\leq t\leq\lfloor n^{2/3}\rfloor}\frac{y_{t-1}^{2}}{s_{0}}=\frac{n^{2/ 3}\log n}{n/\log\log n}\cdot O_{p}(1)=o_{p}(1).\] (45)

By applying the same trick to \(\max_{\lfloor n^{2/3}\rfloor+1\leq t\leq n}y_{t-1}^{2}=O_{p}(n\log n)\).

Hence we have

\[\max_{\lfloor n^{2/3}\rfloor+1\leq t\leq n}\frac{y_{t-1}^{2}}{s_{\lfloor n^{2/ 3}\rfloor}-s_{0}}=\frac{O_{p}(n\log n)}{n^{4/3}/\log\log n^{2/3}}\cdot\frac{n ^{4/3}/\log\log n^{2/3}}{s_{\lfloor n^{2/3}\rfloor}-s_{0}}\overset{(ii)}{=}o_ {p}(1)\cdot O_{p}(1)=o_{p}(1).\] (46)

Equality \((ii)\) makes use of equation (42). Combining equation (44) with equations (45) and (46), we conclude that \(T_{1}=o_{p}(1)\).

\(T_{1}\) with \(\theta^{*}=-1\):When \(\theta^{*}=-1\), the proof is essentially the same as the case when \(\theta^{*}=-1\). The only difference lies in the order of \(\sum_{1\leq i\leq n}y_{i-1}^{2}\). However, by pairing \(\epsilon_{2t-1}\) with \(\epsilon_{2t}\), for \(t\in\mathbb{N}^{+}\), we can arrive at the same result. Specifically, for \(t\in\mathbb{N}^{+}\), we let \(\epsilon_{t}^{\prime}=\epsilon_{2t}-\epsilon_{2t-1}\) and define

\[y_{t}^{\prime}=\sum_{k=1}^{t}\epsilon_{k}^{\prime}\]

where \(y_{0}^{\prime}\overset{\Delta}{=}0\) and \(\{\epsilon_{t}^{\prime}\}_{t\geq 1}\) are random variables with mean zero, variance \(2\sigma^{2}\) and sub-Gaussian parameter \(2\sigma_{g}^{2}\). Therefore, applying equation (44) yields

\[\liminf_{n\rightarrow\infty}n^{-2}(\log\log n)\sum_{t=1}^{n}(y_{t-1}^{\prime} )^{2}=\sigma^{2}.\] (47)

Setting \(n_{0}=\lfloor(\lfloor n^{2/3}\rfloor-1)/2\rfloor\), we have

\[s_{\lfloor n^{2/3}\rfloor}-s_{0}=\sum_{t=1}^{\lfloor n^{2/3}\rfloor-1}y_{t}^{2 }\geq\sum_{t=1}^{n_{0}}(y_{t}^{\prime})^{2}=\sum_{t=1}^{n_{0}+1}(y_{t-1}^{ \prime})^{2}.\] (48)

According to equation (47) and equation (48), we have

\[\max_{\lfloor n^{2/3}\rfloor+1\leq t\leq n}\frac{y_{t-1}^{2}}{s_{ \lfloor n^{2/3}\rfloor}-s_{0}} \leq\frac{\max_{\lfloor n^{2/3}\rfloor+1\leq t\leq n}y_{t-1}^{2}} {\sum_{1\leq t\leq n_{0}+1}(y_{t-1}^{\prime})^{2}}\] (49) \[=\frac{\max_{\lfloor n^{2/3}\rfloor+1\leq t\leq n}y_{t-1}^{2}}{(n _{0}+1)^{2}/\log\log(n_{0}+1)}\cdot\frac{(n_{0}+1)^{2}/\log\log(n_{0}+1)}{ \sum_{1\leq t\leq n_{0}+1}(y_{t-1}^{\prime})^{2}}\] \[=o_{p}(1)\cdot O_{p}(1)=o_{p}(1),\]

which completes the proof of \(T_{1}=o_{p}(1)\) for the case when \(\theta^{*}=-1\).

\(T_{1}\) with \(\theta^{*}\in(-1,1)\):Given \(\theta^{*}\in(-1,1)\), we observe that \(y_{t}\) is a sub-Gaussian random variable with sub-Gaussian parameter \(\frac{\sigma_{0}^{2}}{1-(\theta^{*})^{2}}\) for any \(t\in\mathbb{N}^{+}\). Therefore, following equation (43), we have

\[T_{1}\leq\frac{1}{2(\log 2)^{1+\beta}}\max_{1\leq t\leq n}\frac{y_{t-1}^{2}}{s_{0}}\] (50)

where in the above inequality we use \(s_{0}\) as a lower bound for \(s_{t}\). By applying [33, Exercise 2.12], we have

\[\max_{1\leq t\leq n}y_{t-1}^{2}=O_{p}(\log n),\] (51)

leading to the conclusion that \(T_{1}=o_{p}(1)\).

\(T_{2}\) with \(\theta^{*}\in[-1,1]\):Similar to equation (33), we have

\[T_{2}=\max_{1\leq t\leq n}\left(1-\frac{f(s_{t}/s_{0})}{f(s_{t-1 }/s_{0})}\right) =\max_{1\leq t\leq n}\frac{f(s_{t-1}/s_{0})-f(s_{t}/s_{0})}{f(s_{ t-1}/s_{0})}\] \[\leq\max_{1\leq t\leq n}\frac{-f^{\prime}(s_{t-1}/s_{0})}{f(s_{ t-1}/s_{0})}\frac{y_{t-1}^{2}}{s_{0}}.\]

Define \(g(x)=-f^{\prime}(x)/f(x)\) and we can compute that

\[\int g(x)dx=-\int\frac{f^{\prime}(x)}{f(x)}dx=-\int\frac{1}{f}df=-\log f+C,\]

where \(C\) is some constant. Doing some calculation yields

\[g(x) =\frac{d}{dx}-\log f=\frac{d}{dx}\left\{\frac{1}{2}(\log(x)+\log \log(e^{2}x))+(1+\beta)\log\log(e^{2}x))\right\}\] \[=\frac{1}{2x}\left\{1+\frac{1}{\log(e^{2}x)}+\frac{1+\beta}{\log( e^{2}x)}\cdot\frac{1}{\log\log(e^{2}x)}\right\}.\]

Therefore, we have

\[T_{2} \leq\max_{1\leq t\leq n}\frac{-f^{\prime}(s_{t-1}/s_{0})}{f(s_{t- 1}/s_{0})}\frac{y_{t-1}^{2}}{s_{0}}=\max_{1\leq t\leq n}g(s_{t-1}/s_{0})\frac{ y_{t-1}^{2}}{s_{0}}\] \[\leq\frac{1}{2}\left(\frac{3}{2}+\frac{1+\beta}{2\log 2}\right) \max_{1\leq t\leq n}\frac{y_{t-1}^{2}}{s_{t-1}}.\]

We note that demonstrating \(\max_{1\leq t\leq n}y_{t-1}^{2}/s_{t-1}=o_{p}(1)\) follows the same approach as the proof of \(\max_{1\leq t\leq n}y_{t-1}^{2}/s_{t}=o_{p}(1)\). Hence, we omit it. To conclude, we show that \(T_{2}=o_{p}(1)\) for \(\theta^{*}\in[-1,1]\).

\(T_{3}\) with \(\theta^{*}\in[-1,1]\):To prove \(T_{3}=o_{p}(1)\), it suffices to verify that

\[\frac{s_{0}}{\sum_{1\leq t\leq n}y_{t}^{2}}=o_{p}(1).\] (52)

For convenience, in equation (52) we use \(y_{t}\) instead of \(y_{t-1}\). Note that when \(\theta^{*}=1\) or \(\theta^{*}=-1\), we have provided almost sure lower bounds for \(\sum_{1\leq t\leq n}y_{t}^{2}\) in the proof of \(T_{1}=o_{p}(1)\). Therefore, equation (52) follows from these lower bounds. To prove equation (52) when \(\theta^{*}\in(-1,1)\), we begin by rewriting \(\sum_{1\leq t\leq n}y_{t}^{2}\) in quadratic form. Without confusion and loss of generality, we replace \(\theta^{*}\) by \(\theta\), consider \(\operatorname{Var}(\epsilon_{t})=1\), and set \(\boldsymbol{\varepsilon}_{n}=(\epsilon_{1},\epsilon_{2},\ldots,\epsilon_{n})^ {\top}\). For \(t\in[n]\), we have

\[y_{t}=\sum_{k=1}^{t}\theta^{t-k}\epsilon_{k}=\boldsymbol{a}_{t}^{\top} \boldsymbol{\varepsilon}_{n},\]

where \(\boldsymbol{a}_{t}\in\mathbb{R}^{n}\) and \(a_{t,j}=\theta^{t-j}\) for \(j\leq t\) and \(a_{t,j}=0\) for \(j>t\). Therefore, \(\sum_{1\leq t\leq n}y_{t}^{2}\) can be written as

\[\sum_{1\leq t\leq n}y_{t}^{2}=\sum_{1\leq t\leq n}\boldsymbol{\varepsilon}_{n}^ {\top}\boldsymbol{a}_{t}\boldsymbol{a}_{t}^{\top}\boldsymbol{\varepsilon}_{n}= \boldsymbol{\varepsilon}_{n}^{\top}\boldsymbol{\Lambda}\boldsymbol{\varepsilon}_{n},\] (53)

where \(\boldsymbol{\Lambda}=\sum_{1\leq t\leq n}\boldsymbol{a}_{t}\boldsymbol{a}_{t}^{\top}\). Applying Hanson-Wright inequality (e.g. see [32]), we have

\[\mathbb{P}\left(|\boldsymbol{\varepsilon}_{n}^{\top}\boldsymbol{\Lambda} \boldsymbol{\varepsilon}_{n}-\mathbb{E}\boldsymbol{\varepsilon}_{n}^{\top} \boldsymbol{\Lambda}\boldsymbol{\varepsilon}_{n}|>t\right)\leq 2\exp\left[-c\min\left(\frac{t^{2}}{K^{4} \|\boldsymbol{\Lambda}\|_{\mathrm{F}}^{2}},\frac{t}{K^{2}\|\boldsymbol{\Lambda} \|_{\mathrm{F}}}\right)\right],\] (54)where \(c\) and \(K\) are some universal constants. Observe that

\[\mathbb{E}_{n}^{\top}\mathbf{A}\bm{\varepsilon}_{n} =\operatorname{trace}(A)=\operatorname{trace}(\sum_{1\leq t\leq n }\bm{a}_{t}\bm{a}_{t}^{\top})=\operatorname{trace}(\sum_{1\leq t\leq n}\bm{a}_ {t}^{\top}\bm{a}_{t})\] \[=\sum_{1\leq t\leq n}(1+\theta^{2}+\cdots+\theta^{2(t-1)})\] \[=\sum_{1\leq t\leq n}\frac{1-\theta^{2t}}{1-\theta^{2}}\] \[=\frac{n}{1-\theta^{2}}-\frac{\theta^{2}(1-\theta^{2n})}{(1- \theta^{2})^{2}}.\]

Furthermore, we have

\[\|\mathbf{A}\|_{\mathbb{F}}^{2} =\operatorname{trace}(\mathbf{A}^{\top}\mathbf{A})=\operatorname {trace}(\sum_{1\leq t\leq n}\bm{a}_{i}\bm{a}_{t}^{\top}\cdot\sum_{1\leq j\leq n }\bm{a}_{j}\bm{a}_{j}^{\top})\] (55) \[=\sum_{1\leq t\leq n}\sum_{1\leq j\leq n}(\bm{a}_{i}^{\top}\bm{a} _{j})^{2}\] \[=\sum_{1\leq t\leq n}\|\bm{a}_{i}\|_{2}^{4}+2\sum_{1\leq t\leq j \leq n}\|\bm{a}_{i}\|_{2}^{4}\cdot\theta^{2(j-i)}.\]

Subsequently, we have

\[\sum_{1\leq t\leq n}\|\bm{a}_{i}\|_{2}^{4}\leq\|\mathbf{A}\|_{\mathbb{F}}^{2} \leq(1+\frac{2}{1-\theta^{2}})\sum_{1\leq t\leq n}\|\bm{a}_{i}\|_{2}^{4},\] (56)

where

\[\sum_{1\leq t\leq n}\|\bm{a}_{i}\|_{2}^{4}=\frac{n}{(1-\theta^{2})^{2}}-\frac {2\theta^{2}(1-\theta^{2n})}{(1-\theta^{2})^{3}}+\frac{\theta^{4}(1-\theta^{4n })}{(1-\theta^{2})^{2}(1-\theta^{4})}.\]

Assuming \(\delta\leq 2e^{-c}\) and \(t=\frac{1}{c}K^{2}\|\mathbf{A}\|_{\mathbb{F}}\log(\frac{2}{\delta})\), we have with probability at least \(1-\delta\),

\[\bm{\varepsilon}_{n}^{\top}\mathbf{A}\bm{\varepsilon}_{n}\geq\mathbb{E}\bm{ \varepsilon}_{n}^{\top}\mathbf{A}\bm{\varepsilon}_{n}-\frac{1}{c}K^{2}\| \mathbf{A}\|_{\mathbb{F}}\log(\frac{2}{\delta}).\] (57)

We note that the term on the right hand side of equation (57) has order \(n\). For any \(\epsilon>0\), consider the following probability

\[\limsup_{n\to\infty}\mathbb{P}\left(\frac{s_{0}}{\bm{\varepsilon}_ {n}^{\top}\mathbf{A}\bm{\varepsilon}_{n}}>\epsilon\right) \leq\limsup_{n\to\infty}\mathbb{P}\left(\frac{s_{0}}{\bm{ \varepsilon}_{n}^{\top}\mathbf{A}\bm{\varepsilon}_{n}}>\epsilon,\bm{ \varepsilon}_{n}^{\top}\mathbf{A}\bm{\varepsilon}_{n}\geq\mathbb{E}\bm{ \varepsilon}_{n}^{\top}\mathbf{A}\bm{\varepsilon}_{n}-\frac{1}{c}K^{2}\| \mathbf{A}\|_{\mathbb{F}}\log(\frac{2}{\delta})\right)\] (58) \[+\limsup_{n\to\infty}\mathbb{P}\left(\bm{\varepsilon}_{n}^{\top} \mathbf{A}\bm{\varepsilon}_{n}<\mathbb{E}\bm{\varepsilon}_{n}^{\top}\mathbf{A} \bm{\varepsilon}_{n}-\frac{1}{c}K^{2}\|\mathbf{A}\|_{\mathbb{F}}\log(\frac{2} {\delta})\right)\] \[\leq\limsup_{n\to\infty}\mathbb{P}\left(\frac{s_{0}}{\mathbb{E} \bm{\varepsilon}_{n}^{\top}\mathbf{A}\bm{\varepsilon}_{n}-\frac{1}{c}K^{2}\| \mathbf{A}\|_{\mathbb{F}}\log(\frac{2}{\delta})}>\epsilon\right)+\delta.\]

By fixing \(\delta\) and comparing the order of \(s_{0}\) with the order of \(\bm{\varepsilon}_{n}^{\top}\mathbf{A}\bm{\varepsilon}_{n}-\frac{1}{c}K^{2}\| \mathbf{A}\|_{\mathbb{F}}\log(\frac{2}{\delta})\), we have

\[\limsup_{n\to\infty}\mathbb{P}\left(\frac{s_{0}}{\mathbb{E}\bm{\varepsilon} _{n}^{\top}\mathbf{A}\bm{\varepsilon}_{n}-\frac{1}{c}K^{2}\|\mathbf{A}\|_{ \mathbb{F}}\log(\frac{2}{\delta})}>\epsilon\right)=0.\]

Since \(\delta\) can be arbitrarily small, we conclude that

\[\frac{s_{0}}{\bm{\varepsilon}_{n}^{\top}\mathbf{A}\bm{\varepsilon}_{n}}=o_{p}( 1),\] (59)

which completes the proof of \(T_{3}=o_{p}(1)\).

### Proof of Theorem 3.6

Note that for any \(t\geq 1\), we have

\[\|\mathbf{V}_{t}\|_{\mathbf{0}\mathbf{P}}\leq 1\quad\text{and}\quad\mathbf{V}_{t}= \mathbf{V}_{t-1}-\mathbf{V}_{t-1}\bm{z}_{t}\bm{z}_{t}^{\top}\mathbf{V}_{t-1}/(1+ \bm{z}_{t}^{\top}\mathbf{V}_{t-1}\bm{z}_{t}).\] (60)

The second part of equation (60) follows from the Sherman-Morrison formula. Let \(\bm{u}_{t}=\mathbf{V}_{t}\bm{z}_{t}\) and we adopt the notation \(\mathbf{V}_{0}=\mathbf{I}_{d}\). By multiplying \(\bm{z}_{t}\) on the right hand side of \(\mathbf{V}_{t}\), we have

\[\begin{split}\mathbf{V}_{t}\bm{z}_{t}&=\mathbf{V}_{t- 1}\bm{z}_{t}-\mathbf{V}_{t-1}\bm{z}_{t}\bm{z}_{t}^{\top}\mathbf{V}_{t-1}\bm{z}_{ t}/(1+\bm{z}_{t}^{\top}\mathbf{V}_{t-1}\bm{z}_{t})\\ &=\mathbf{V}_{t-1}\bm{z}_{t}\bigg{(}1-\frac{\bm{z}_{t}^{\top} \mathbf{V}_{t-1}\bm{z}_{t}}{1+\bm{z}_{t}^{\top}\mathbf{V}_{t-1}\bm{z}_{t}} \bigg{)}=\frac{\mathbf{V}_{t-1}\bm{z}_{t}}{1+\bm{z}_{t}^{\top}\mathbf{V}_{t-1} \bm{z}_{t}}.\end{split}\] (61)Therefore, following the definition of \(\bm{u_{t}}\), we have \((1+\bm{z}_{t}^{\top}\mathbf{V}_{t-1}\bm{z}_{t})\bm{u}_{t}=\mathbf{V}_{t-1}\bm{z}_{t}\). Consequently,

\[\sum_{t=1}^{n}(1+\bm{z}_{t}^{\top}\mathbf{V}_{t-1}\bm{z}_{t})\bm{u}_{t}\bm{u}_{t }^{\top}=\sum_{t=1}^{n}\mathbf{V}_{t-1}(\mathbf{V}_{t}^{-1}-\mathbf{V}_{t-1}^{ -1})\mathbf{V}_{t}=\mathbf{I}_{d}-\mathbf{V}_{n}.\] (62)

By recognizing \(\bm{w}_{t}=\sqrt{1+\bm{z}_{t}^{\top}\mathbf{V}_{t-1}\bm{z}_{t}}\cdot\bm{u}_{t}\), we come to

\[\sum_{t=1}^{n}\bm{w}_{t}\bm{w}_{t}^{\top}=\sum_{t=1}^{n}\mathbf{V}_{t-1}( \mathbf{V}_{t}^{-1}-\mathbf{V}_{t-1}^{-1})\mathbf{V}_{t}=\mathbf{I}_{d}- \mathbf{V}_{n}.\]

What remains now is to verify conditions in (6). Notably, assumption \(\|\mathbf{V}_{n}\|_{\text{op}}=o_{p}(1)\) implies

\[\sum_{t=1}^{n}\bm{w}_{t}\bm{w}_{t}^{\top}\overset{p}{\longrightarrow}\mathbf{ I}_{d}.\] (63)

Since \(\|\bm{\Sigma}_{0}^{-1}\|_{\text{op}}=o_{p}(1)\), \(\|\mathbf{V}_{t}\|_{\text{op}}\leq 1\) and \(\|\bm{x}_{t}\|_{2}\leq 1\), we can show

\[\max_{1\leq t\leq n}\bm{z}_{t}^{\top}\mathbf{V}_{t}\bm{z}_{t}=\max_{1\leq t \leq n}\bm{x}_{t}^{\top}\bm{\Sigma}_{t-1}^{-\frac{1}{2}}\mathbf{V}_{t}\bm{ \Sigma}_{t-1}^{-\frac{1}{2}}\bm{x}_{t}=o_{p}(1).\] (64)

Besides, equation (61) together with equation (64) implies

\[\max_{1\leq t\leq n}\bm{z}_{t}^{\top}\mathbf{V}_{t-1}\bm{z}_{t}=\max_{1\leq t \leq n}\frac{\bm{z}_{t}^{\top}\mathbf{V}_{t}\bm{z}_{t}}{1-\bm{z}_{t}^{\top} \mathbf{V}_{t}\bm{z}_{t}}=o_{p}(1).\] (65)

Thus, it follows that

\[\max_{1\leq t\leq n}\|\bm{w}_{t}\|_{2} =\max_{1\leq t\leq n}\left\|\sqrt{1+\bm{z}_{t}^{\top}\mathbf{V}_{ t-1}\bm{z}_{t}}\cdot\mathbf{V}_{t}\bm{z}_{t}\right\|_{2}\] (66) \[\leq\max_{1\leq t\leq n}\left(\sqrt{1+\bm{z}_{t}^{\top}\mathbf{V} _{t-1}\bm{z}_{t}}\cdot\|\mathbf{V}_{t}^{\frac{1}{2}}\|_{\text{op}}\cdot\| \mathbf{V}_{t}^{\frac{1}{2}}\bm{x}_{t}\|_{2}\right)\] \[\leq\sqrt{\left(1+\max_{1\leq t\leq n}\bm{z}_{t}^{\top}\mathbf{V} _{t-1}\bm{z}_{t}\right)}\cdot\max_{1\leq t\leq n}\bm{z}_{t}^{\top}\mathbf{V}_ {t}\bm{z}_{t}=o_{p}(1).\]

Combining equations (66) and (63) yields (6). Hence we complete the proof by applying Proposition 2.1.

**Remark A.3**.: _The detailed proof of Lemma 3.7 can be found in the proof of Theorem 3.6._

## Appendix B Generalized Theorem 3.6

In Theorem 3.6, we impose the following condition (67) so that the ALEE estimator with weights specified in equation (28) achieves asymptotic normality:

\[\|\mathbf{V}_{n}\|_{\text{op}}=o_{p}(1).\] (67)

However, it is typically difficult to directly verify the above condition in practice. To tackle this problem, in this section, we provide a modified version of ALEE estimator which achieves asymptotic normality without requiring condition (67). In this section, we use the same notations \(\bm{\Sigma}_{t},\bm{z}_{t},\mathbf{V}_{t},\) and \(\bm{w}_{t}\) as defined in equations (26), (27) and (28), respectively. Furthermore, we let \(\lambda_{1}\geq\ldots\geq\lambda_{n}\) be the eigenvalues of the matrix \(\mathbf{V}_{n}^{-1}\) and \(\bm{a}_{1},\ldots,\bm{a}_{n}\) be the corresponding eigenvectors.

At a high level, we construct additional \(m_{n}\) vectors \(\{\bm{z}_{t}\}_{n+1\leq t\leq n+m_{n}}\) so that the minimum eigenvalue of the resulting matrix \(\mathbf{V}_{n+m_{n}}^{-1}\) is greater than a pre-specified constant \(\kappa_{n}\), which satisfies \(\lim_{n\to\infty}\kappa_{n}=\infty\). It is easy to see that by construction (see Algorithm 1), the matrix \(\mathbf{V}_{n+m_{n}}\) satisfies

\[\|\mathbf{V}_{n+m_{n}}\|_{\text{op}}\leq\frac{1}{\kappa_{n}}\overset{p}{ \longrightarrow}0\qquad\text{where}\quad m_{n}=\sum_{k=1}^{d}n_{k}.\] (69)

**Remark B.1**.: _Parameter \(\kappa_{n}\) is set to ensure condition (69) holds. In practice, we set \(\kappa_{n}=d\log(n)\)._

**Remark B.2**.: _It's worth mentioning that the number of extra \(\{\bm{z}_{t}\}_{t>n}\) is a random variable. Therefore, in order to prove a similar asymptotic normality theorem to Theorem 3.6, we have to apply martingale central limit theorem with stopping times [11, Theorem 2.1]._

**Theorem B.3** (Theorem 2.1 in [11]).: _Let \(\{\xi_{n,k}\}_{k\geq 1,n\geq 1}\) be an array of random variables defined on a probability space \((\Omega,\mathcal{F},P)\) and let \(\{\mathcal{F}_{n,k}\}_{n\geq 1,k\geq 0}\) be an array of \(\sigma\)-fields such that \(\xi_{n,k}\) is \(\mathcal{F}_{n,k}\)-measurable and \(\mathcal{F}_{n,k-1}\subset\mathcal{F}_{n,k}\subset\mathcal{F}\) for each \(n\) and \(k\geq 1\). For each \(n\), let \(k_{n}\) be a stopping time with respect to \(\{\mathcal{F}_{n,k}\}_{k\geq 0}\). Suppose that_\[\sum_{k=1}^{k_{n}}\mathbb{E}\left[\xi_{n,k}\mid\mathcal{F}_{n,k-1} \right]\overset{p}{\longrightarrow}0,\] (70a) \[\sum_{k=1}^{k_{n}}\operatorname{Var}\left[\xi_{n,k}\mid\mathcal{F }_{n,k-1}\right]\overset{p}{\longrightarrow}1,\] (70b) \[\sum_{k=1}^{k_{n}}\mathbb{E}\left[\left|\xi_{n,k}\right|^{2+\delta} \mid\mathcal{F}_{n,k-1}\right]\overset{p}{\longrightarrow}0\quad\text{for some }\delta>0,\] (70c)

_then \(\sum_{k=1}^{k_{n}}\xi_{n,k}\overset{d}{\longrightarrow}\mathcal{N}(0,1)\)._

With this setup, we are now ready to prove the asymptotic normality of \(\widehat{\boldsymbol{\theta}}_{\text{\rm{ALE}}}\) from (68).

**Theorem B.4** (Generalized Theorem 3.6).: _Suppose condition (3) holds. Then, for any tuning parameters \(\boldsymbol{\Sigma}_{0}\) and \(\kappa_{n}\) that satisfy \(\|\boldsymbol{\Sigma}_{0}^{-1}\|_{\text{\rm{op}}}=o_{p}(1)\) and \(\lim_{n\to\infty}\kappa_{n}=\infty\), the ALEE estimator \(\widehat{\boldsymbol{\theta}}_{\text{\rm{ALE}}}\) obtained from equation (68) satisfies_

\[\left(\sum_{t=1}^{n}\boldsymbol{w}_{t}\boldsymbol{x}_{t}\right)\cdot\frac{ \widehat{\boldsymbol{\theta}}_{\text{\rm{ALE}}}-\boldsymbol{\theta}^{\star}} {\widehat{\sigma}}\overset{d}{\longrightarrow}\mathcal{N}\big{(}\boldsymbol{ 0},\mathbf{I}_{d}\big{)},\]

_where \(\widehat{\sigma}\) is a consistent estimator of \(\sigma\)._

**Remark B.5**.: _We would like to reiterate that the asymptotic variance of of the modified ALEE estimator obtained from (68) is the same as the one mentioned in Theorem 3.6. Additionally, this modified version does not require the condition \(\|\mathbf{V}_{n}\|_{\text{\rm{op}}}=o_{p}(1)\) hold and hence is more applicable in practice with theoretical guarantee._

Proof.: Rewriting equation (68), we have

\[\sum_{t=1}^{n}\boldsymbol{w}_{t}\boldsymbol{x}_{t}^{\top}(\widehat{ \boldsymbol{\theta}}_{\text{\rm{ALE}}}-\boldsymbol{\theta}^{\star})=\sum_{t= 1}^{n+m_{n}}\boldsymbol{w}_{t}\epsilon_{t}.\] (71)Therefore, by Cramer-Wold theorem, it suffices to show that for any unit vector \(\bm{v}\),

\[\sum_{t=1}^{n+m_{n}}\bm{v}^{\top}\bm{w}_{t}\epsilon_{t}\stackrel{{ d}}{{\longrightarrow}}\mathcal{N}(0,\sigma^{2}).\] (72)

The proof now follows by verifying the conditions (70a)-(70c) of Theorem B.3 with \(\xi_{n,k}=\bm{v}^{\top}\bm{w}_{k}\epsilon_{k}\). We begin by verifying conditions (70a)-(70c). By Lemma 3.7, we have

\[\sum_{t=1}^{n+m_{n}}\bm{w}_{t}\bm{w}_{t}^{\top}=\mathbf{I}_{d}-\mathbf{V}_{n+m _{n}}.\] (73)

Note that

\[\sum_{t=1}^{n+m_{n}}\mathrm{Var}[\bm{w}_{t}\epsilon_{t}\mid\mathcal{F}_{t-1}]= \sum_{t=1}^{n+m_{n}}\sigma^{2}\bm{w}_{t}\bm{w}_{t}^{\top}+\sigma^{2}\left( \frac{\widehat{\sigma}^{2}}{\sigma^{2}}-1\right)\sum_{t=n+1}^{n+m_{n}}\bm{w}_ {t}\bm{w}_{t}^{\top}.\] (74)

By equation (73) and the fact that \(\widehat{\sigma}^{2}\) is consistent, we have

\[\sum_{t=n+1}^{n+m_{n}}\bm{w}_{t}\bm{w}_{t}^{\top}\preceq\mathbf{I}_{d}\qquad \text{and}\qquad\frac{\widehat{\sigma}^{2}}{\sigma^{2}}-1\stackrel{{ p}}{{\longrightarrow}}0.\] (75)

Combining equations (69), (73), (74) and (75), we conclude

\[\sum_{t=1}^{n+m_{n}}\mathrm{Var}[\bm{w}_{t}\epsilon_{t}\mid\mathcal{F}_{t-1}] \stackrel{{ p}}{{\longrightarrow}}\sigma^{2}\mathbf{I}_{d}.\] (76)

On the other hand, we have

\[\max_{1\leq t\leq n+m_{n}}\|\bm{w}_{t}\|_{2} \stackrel{{(i)}}{{\leq}} \max_{1\leq t\leq n+m_{n}}\left(\sqrt{1+\bm{z}_{t}^{\top}\mathbf{V}_ {t-1}\bm{z}_{t}}\cdot\|\mathbf{V}_{t}\|_{\text{op}}\cdot\|\bm{z}_{t}\|_{2}\right)\] \[\stackrel{{(ii)}}{{\leq}} \max_{1\leq t\leq n+m_{n}}\sqrt{2}\|\bm{z}_{t}\|_{2}\] \[\stackrel{{(iii)}}{{\leq}} \sqrt{2}\|\bm{\Sigma}_{0}^{-1/2}\|_{\text{op}}.\]

Inequality \((i)\) follows from the definition of \(\bm{w}_{t}\). In inequality \((ii)\), we use the assumption that \(\bm{\Sigma}_{0}\succeq\mathbf{I}_{d}\) and the fact that \(\|\bm{z}_{t}\|_{2}\leq 1\) and \(\|\mathbf{V}_{t}\|_{\text{op}}\leq 1\). The last inequality \((iii)\) follows from the definition of \(\bm{z}_{t}\) and the condition that \(\|\bm{\Sigma}_{0}^{-1}\|_{\text{op}}=o_{p}(1)\). Hence, we can see that

\[\max_{1\leq t\leq n+m_{n}}\|\bm{w}_{t}\|_{2}\stackrel{{ p}}{{ \longrightarrow}}0.\] (77)

Therefore, we have

\[\max_{1\leq t\leq n+m_{n}}|\bm{v}^{\top}\bm{w}_{t}|\stackrel{{ p}}{{\longrightarrow}}0\qquad\text{and}\qquad\sum_{t=1}^{n+m_{n}} \mathrm{Var}[\bm{v}^{\top}\bm{w}_{t}\epsilon_{t}\mid\mathcal{F}_{t-1}]\stackrel{{ p}}{{\longrightarrow}}\sigma^{2}.\] (78)

Note that condition (70a) holds because \(\{\bm{v}^{\top}\bm{w}_{k}\epsilon_{k}\}_{k\geq 1}\) is a martingale difference sequence by construction. Condition (70b) follows from statement (78). It remains to verify condition (70c). Observe that

\[\sum_{t=1}^{n+m_{n}}\mathbb{E}[|\bm{v}^{\top}\bm{w}_{t}\epsilon_{ t}|^{2+\delta}\mid\mathcal{F}_{t-1}]=\sum_{t=1}^{n+m_{n}}|\bm{v}^{\top}\bm{w}_{t}|^ {2+\delta}\mathbb{E}[|\epsilon_{t}|^{2+\delta}\mid\mathcal{F}_{t-1}]\] \[\leq \left(\max_{1\leq t\leq n+m_{n}}|\bm{v}^{\top}\bm{w}_{t}|^{\delta }\right)\cdot\left(\sup_{t\geq 1}\mathbb{E}[|\epsilon_{t}|^{2+\delta}\mid \mathcal{F}_{t-1}]\right)\cdot\max\{\frac{1}{\sigma^{2}},\frac{1}{\widehat{ \sigma}^{2}}\}\sum_{t=1}^{n+m_{n}}\mathrm{Var}[\bm{v}^{\top}\bm{w}_{t}\epsilon_{ t}\mid\mathcal{F}_{t-1}]\] \[\stackrel{{(iv)}}{{=}} o_{p}(1)\cdot O_{p}(1)\cdot O_{p}(1)=o_{p}(1).\]

Equation \((iv)\) follows from condition (3), equation (78) and the fact that \(\widehat{\sigma}^{2}\) is a consistent estimator. Lastly, by applying Slutsky's theorem, we prove that

\[\frac{1}{\widehat{\sigma}}\sum_{t=1}^{n}\bm{w}_{t}\bm{x}_{t}^{\top}(\widehat{ \bm{\theta}}_{\text{ALE}}-\theta^{*})\stackrel{{ d}}{{ \longrightarrow}}\mathcal{N}(\bm{0},\mathbf{I}_{d}).\] (79)Simulation

In this section, we provide additional comparisons among the ALEE method, the OLS, the W-decorrelation [8], and the concentration inequality based bounds [1]. The code can be found at https://github.com/mufangying/ALEE.

### Simulation details

Throughout our experiments, we utilize \(\widehat{\sigma}^{2}\) from equation (9) as an (consistent) estimate of of \(\sigma^{2}\)[19].

OLS:When data are i.i.d, the least squares estimator satisfies the following condition

\[\frac{1}{\sigma^{2}}(\widehat{\bm{\theta}}_{\mathrm{LS}}-\bm{\theta}^{\ast})^ {\top}\mathbf{S}_{n}(\widehat{\bm{\theta}}_{\mathrm{LS}}-\bm{\theta}^{\ast}) \stackrel{{ d}}{{\longrightarrow}}\chi_{d}^{2}.\]

Therefore, we consider \(1-\alpha\) confidence region to be

\[\mathbf{C}_{\mathrm{LS}}=\bigg{\{}\bm{\theta}\in\mathbb{R}^{d}:\frac{1}{ \widehat{\sigma}^{2}}(\widehat{\bm{\theta}}_{\mathrm{LS}}-\bm{\theta})^{\top }\mathbf{S}_{n}(\widehat{\bm{\theta}}_{\mathrm{LS}}-\bm{\theta})\leq\chi_{d,1 -\alpha}^{2}\bigg{\}}.\] (80)

We point out that the above confidence region is not guaranteed to be accurate when the data is collected in an adaptive manner, as will also be highlighted in our experiments.

W-decorrelation:The W-decorrelation method is borrowed from Algorithm 1 in [8]. Specifically, the estimator takes the form

\[\widehat{\bm{\theta}}_{\mathrm{W}}=\widehat{\bm{\theta}}_{\mathrm{LS}}+\sum_ {t=1}^{n}\bm{w}_{t}(y_{t}-\bm{x}_{t}^{\top}\widehat{\bm{\theta}}_{\mathrm{LS }}).\] (81)

Given a parameter \(\lambda\), weights \(\{\bm{w}_{t}\}_{1\leq t\leq n}\) are set as follows

\[\bm{w}_{t}=\bigg{(}\mathbf{I}_{d}-\sum_{i=1}^{t-1}\bm{w}_{t}\bm{x}_{t}^{\top} \bigg{)}\bm{x}_{t}/(\lambda+\|\bm{x}_{t}\|_{2}^{2}).\] (82)

Following the recommendations from the paper [8], in order to set \(\lambda\) appropriately, we first run the bandit algorithm or time series with \(N\) replications and record the corresponding minimum eigenvalues \(\{\lambda_{\min}(\mathbf{S}_{n}^{(1)}),\ldots,\lambda_{\min}(\mathbf{S}_{n}^{ (N)})\}\). We choose \(\lambda\) to be the 0.1-quantile of \(\{\lambda_{\min}(\mathbf{S}_{n}^{(1)}),\ldots,\lambda_{\min}(\mathbf{S}_{n}^{ (N)})\}\). Finally, we obtain a \(1-\alpha\) confidence region for \(\bm{\theta}^{\ast}\) as

\[\mathbf{C}_{\mathrm{W}}=\bigg{\{}\bm{\theta}\in\mathbb{R}^{d}:\frac{1}{ \widehat{\sigma}^{2}}(\widehat{\bm{\theta}}_{\mathrm{W}}-\bm{\theta})^{\top} \mathbf{W}^{\top}\mathbf{W}(\widehat{\bm{\theta}}_{\mathrm{W}}-\bm{\theta}) \leq\chi_{d,1-\alpha}^{2}\bigg{\}},\] (83)

where \(\mathbf{W}=(\bm{w}_{1},\ldots,\bm{w}_{n})^{\top}\).

Concentration based on self-normalized martingales:We consider [1, Theorem 1] for a single coordinate in two-armed bandit problem and AR(1) model. For contextual bandits, we apply [1, Theorem 2]. Applying concentration bounds requires a sub-Gaussian parameter, for which we use \(\widehat{\sigma}\) from equation (9) as an estimate. We point out that this estimate of the sub-Gaussian parameter is conservative, as the sub-Gaussian parameter of a sub-Gaussian random variable is always lower bounded by its variance [33, Chapter 2]. This variance estimate is accurate for Gaussian noise random variables.

* For one dimensional examples, we have that for any \(\lambda>0\), with probability at least \(1-\alpha\): \[|\widehat{\theta}_{\mathrm{LS}}-\theta^{\ast}|\leq\frac{\widehat{\sigma}\sqrt{ \lambda+\sum_{t=1}^{n}x_{t}^{2}}}{\sum_{t=1}^{n}x_{t}^{2}}\sqrt{\log\bigg{(} \frac{\lambda+\sum_{t=1}^{n}x_{t}^{2}}{\lambda\alpha^{2}}\bigg{)}}.\] (84) In two-armed bandit problem, \(x_{t}\) is simply \(x_{t,1}\) for \(\theta_{1}^{\ast}\) or \(x_{t,2}\) for \(\theta_{2}^{\ast}\). Here we consider \(\lambda=1\).
* For the contextual bandit examples, we apply Theorem 2 from [1], and set \(S=\sqrt{d}\); we set a small value of \(\lambda=0.01\) to mimic the performance of an OLS estimators. Specifically, we utilize the following \(1-\alpha\) confidence region \[\mathbf{C}_{\text{con}}=\bigg{\{}\bm{\theta}\in\mathbb{R}^{d}:(\widehat{\bm{ \theta}}_{r}-\bm{\theta})^{\top}(\lambda\mathbf{I}_{d}+\mathbf{S}_{n})( \widehat{\bm{\theta}}_{r}-\bm{\theta})\leq\bigg{(}\widehat{\sigma}\sqrt{\log \bigg{(}\frac{\det(\lambda\mathbf{I}_{d}+\mathbf{S}_{n})}{\lambda^{d}\alpha^{ 2}}\bigg{)}}+\lambda^{\frac{1}{2}}S\bigg{)}^{2}\bigg{\}},\] (85) where \(\widehat{\bm{\theta}}_{r}=(\mathbf{X}_{n}^{\top}\mathbf{X}_{n}+\lambda\mathbf{ I}_{d})^{-1}\mathbf{X}_{n}^{\top}\mathbf{Y}_{n}\) and \(\mathbf{Y}_{n}=(y_{1},\ldots,y_{n})^{\top}\).

[MISSING_PAGE_EMPTY:22]