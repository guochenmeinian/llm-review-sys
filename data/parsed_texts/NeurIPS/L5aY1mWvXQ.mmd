# Rethinking Evaluation Strategy for Temporal Link Prediction through Counterfactual Analysis

Aniq Ur Rahman\({}^{1}\) Alexander Modell\({}^{2}\) Justin P. Coon\({}^{1}\)

\({}^{1}\)University of Oxford, U.K. \({}^{2}\)Imperial College London, U.K.

aniq.rahman@eng.ox.ac.uk, a.modell@imperial.ac.uk, justin.coon@eng.ox.ac.uk

###### Abstract

In response to critiques of existing evaluation methods for Temporal Link Prediction (TLP) models, we propose a novel approach to verify if these models truly capture temporal patterns in the data. Our method involves a sanity check formulated as a counterfactual question: "What if a TLP model is tested on a temporally distorted version of the data instead of the real data?" Ideally, a TLP model that effectively learns temporal patterns should perform worse on temporally distorted data compared to real data. We provide an in-depth analysis of this hypothesis and introduce two data distortion techniques to assess well-known TLP models. Our contributions are threefold: (1) We introduce simple techniques to distort temporal patterns within a graph, generating temporally distorted test splits of well-known datasets for sanity checks. These distortion methods are applicable to any temporal graph dataset. (2) We perform counterfactual analysis on TLP models such as JODIE, TGAT, TGN, and CAWN to evaluate their capability in capturing temporal patterns across different datasets. (3) We propose an alternative evaluation strategy for TLP, addressing the limitations of binary classification and ranking methods, and introduce two metrics - average time difference (ATD) and average count difference (ACD) - to provide a comprehensive measure of a model's predictive performance. The code and datasets are available at: https://github.com/Aniq55/TLPCF.git.

## 1 Introduction

In static graphs, link prediction refers to the task of predicting whether an edge exists between two nodes after having observed other edges in the graph. Temporal link prediction (TLP) is a dynamic extension of link prediction wherein the task is to predict whether a link (edge) exists between any two nodes in the future based on the historical observations (Qin and Yeung, 2023). The predictive capability of TLP models make them useful in applications pertaining to dynamic graphs, such as product recommendations (Qin et al., 2024; Fan et al., 2021), social network content or account recommendation (Fan et al., 2019; Daud et al., 2020), fraud detection in financial networks (Kim et al., 2024), and resource allocation, to name a few.

In the TLP literature (Kumar et al., 2019; Trivedi et al., 2019; Xu et al., 2020; Rossi et al., 2020; Wang et al., 2020; Cong et al., 2023), the TLP task is treated as a binary classification problem where the query

\[\text{q}_{1}:\text{``Does an edge exist between the nodes $u$ and $v$ at time $t$?}\]

is processed by a model and then compared with the ground truth following which metrics such as area under the receiver operating characteristic curve (AU-ROC), and average precision (AP) are reported. The ground truth consists of positive samples, and a fixed number of random negative samples. There are a couple of issues in the binary classification approach. Firstly, the timestamps in the query are restricted to the timestamps present in the ground truth, which makes the evaluationbiased and does not test the model's performance in the continuous time range. Secondly, checking for the existence of an edge at a specific timestamp is an ill-posed question, and instead the existence of an edge should be queried within a finite time-interval. Lastly, the negative edge sampling strategy, and the number of negative samples per positive sample impact the performance metrics as seen in EXH(Poursafaei and Rabbany, 2023).

Alternatively, in a rank-based approach, the query is formulated as:

\[\text{q}_{2}:\text{``Which nodes are likely to have an edge with node $u$ at time $t$?''}\]

In this case, the model returns an ordered list of nodes arranged from most likely to least likely. Then, the rank of the ground truth edge is returned if a match is found, and if not, a high number is reported. For all the edges in the test data, metrics such as Mean Average Rank (MAR) or Mean Reciprocal Rank (MRR) can be reported to assess the performance of the model (Huang et al., 2024). While the rank-based metrics are more intuitive than AU-ROC and AP, the issues regarding binary classification mentioned above still remain unaddressed. To give a true picture of the predictive power of the TLP models, a penalty term should be introduced to account for the nodes that are incorrectly estimated to form an edge with node \(u\) at time \(t\).

In a recent work, Poursafaei et al. (2022) highlighted that the state-of-the-art (SoTA) performance of some TLP models on the standard benchmark datasets is near-perfect. This is counterintuitive because TLP is a challenging task, even more challenging than link prediction of static graphs, due to the additional degree of freedom in the data induced by the temporal dimension. The flaw in the evaluation method is attributed to the limited negative sampling strategy, and the authors propose a new negative edge sampling strategy which results in a different ranking of the baselines.

Inspired by the critique of the evaluation method, we propose a method to conduct sanity check of the TLP models to determine if they truly capture the temporal patterns in the data. The sanity check is formulated as the counterfactual question (Pearl, 2019):

"What if a TLP model which is trained on a temporal graph is tested on _temporally distorted_ version of the data instead of the real data?"

Ideally, a TLP model which is capable of learning the temporal patterns should perform worse on temporally distorted data compared to the real data. We conduct an in-depth analysis of this argument and introduce various data distortion techniques to assess well-known TLP models.

ContributionsThe contributions of our work can be summarised as follows:

* We introduce simple **techniques** to distort the temporal patterns within a graph. These techniques are then used to generate temporally distorted version of the test split of some famous datasets which can be used for **sanity check**. Moreover, the distortion methods can be applied to any temporal graph dataset.
* We perform **counterfactual analysis** on TLP models such as JODIE(Kumar et al., 2019), TGAT(Xu et al., 2020), TGN(Rossi et al., 2020), and CAWN(Wang et al., 2020) to check whether they are capable of capturing the temporal patters within various datasets.
* We propose an alternative **evaluation strategy** for TLP through which the existing pitfalls of binary classification and ranking methods can be avoided. We also propose two **metrics**: average time difference (ATD), and average count difference (ACD) to measure the performance of TLP models. These metrics can provide a holistic picture of a model's predictive performance.

OrganizationIn Sec. 2, we define temporal graphs and the associated notations. We also provide a brief overview of interpreting temporal graphs as point processes, which forms the theoretical foundation of TLP. In Sec. 3, we formalize the counterfactual analysis through logical arguments, and also propose data distortion techniques. The results of the counterfactual analysis are presented in Sec. 4 along with the details of the datasets and TLP models used for evaluation. In Sec. 5, we suggest a generative evaluation approach for TLP, and discuss the broader impact and limitations of our work.

Preliminaries

### Definitions

In TLP literature, continuous-time temporal graphs with _ephemeral edges_ are often considered, where edges represent interaction events between two nodes at a specific point in time. Alternatively, temporal graphs can be defined with edges that appear at a certain time and either persist for a duration (Celikkanat et al., 2024; Farzaneh and Coon, 2023) or accumulate indefinitely. In this work, we focus on the ephemeral edge temporal graph, also known as interaction graphs (Qin et al., 2024) or unevenly sampled edge sequence (Qin and Yeung, 2023).

**Definition 2.1**.: A **temporal graph** with \(m\in\mathbb{N}\) ephemeral edges formed between nodes in \(\mathcal{U}\) and \(\mathcal{V}\) is defined as \(\mathcal{G}=(\mathcal{U},\mathcal{V},\mathcal{E})\), where \(\mathcal{E}\triangleq\{(u_{i},v_{i},t_{i}):i\in[m],u_{i}\in\mathcal{U},v_{i} \in\mathcal{V},t_{i}\in\mathbb{R}\}\) denotes the set of edges. The tuple \((u,v,t)\) is referred to as an edge event.

While the definition caters to bipartite structure, with \(\mathcal{U}=\mathcal{V}\), it can also represent general graphs.

**Definition 2.2**.: The occurrences of a particular edge \((u,v)\) in \(\mathcal{E}\) is denoted as \(\mathcal{E}_{(u,v)}\) and defined as \(\mathcal{E}_{(u,v)}\triangleq\{(u,v,t):(u,v,t)\in\mathcal{E}\}\).

**Definition 2.3**.: The slice of edges in \(\mathcal{E}\) with timestamps in the range \((t_{1},t_{2})\) is denoted as \(\mathcal{E}(t_{1},t_{2})\), and defined as \(\mathcal{E}(t_{1},t_{2})\triangleq\{(u,v,t):(u,v,t)\in\mathcal{E},t\in(t_{1}, t_{2})\}\).

**Definition 2.4**.: The timestamps in \(\mathcal{E}\) consisting of \(m\in\mathbb{N}\) edges can be extracted through a function \(\mathscr{T}:(\mathcal{U}\times\mathcal{V}\times\mathbb{R})^{m}\to\mathbb{R}^{m}\) as \(\mathscr{T}(\mathcal{E})\triangleq\{t:(u,v,t)\in\mathcal{E}\}\).

### Point Process

Perry and Wolfe (2013) modelled the interaction events of a directed edge \((u,v)\) as an inhomoegenous Poisson point process. In a recent work on continuous-time representation learning on temporal graphs, Modell et al. (2024) followed suit, and assumed \(\mathcal{E}_{(u,v)}\) to be sampled from an independent inhomogenous Poisson point process with intensity \(\lambda_{(u,v)}(t)\). The number of edge events \((u,v)\) between timestamps \(t_{1}\) and \(t_{2}\) follow a Poisson distribution with rate \(\int_{t_{1}}^{t_{2}}\lambda_{(u,v)}(t)\,dt\), i.e.,

\[|\mathcal{E}_{(u,v)}(t_{1},t_{2})|\sim\mathrm{Poisson}\left(\int_{t_{1}}^{t_{2 }}\lambda_{(u,v)}(t)\,dt\right).\] (1)

To connect the present to the past, Du et al. (2016) view the intensity function \(\lambda^{\star}_{(u,v)}(t)\) as a nonlinear function of the sample history, where \(\star\) indicates that the function is conditioned on the history. The conditional density function for edge \((u,v)\) is written as

\[p^{\star}_{(u,v)}(t)=\lambda^{\star}_{(u,v)}(t)\exp\left(-\int_{t^{\prime}}^{ t}\lambda^{\star}_{(u,v)}(\tau)\,d\tau\right),\] (2)

where \(t^{\prime}<t\) is the last time when edge \((u,v)\) was observed. The goal is to find the parameters \(\lambda^{\star}_{(u,v)}(t):0<t\leq T\) which can describe the observation \(\mathcal{E}_{(u,v)}\). This is done by minimizing the negative log likelihood (NLL) at the timestamps of edge occurrence (Shchur et al., 2021):

\[\min_{\lambda^{\star}_{(u,v)}(t):\,0<t\leq T}-\sum_{t\in\mathscr{T}\left( \mathcal{E}_{(u,v)}\right)}\log\left(\lambda^{\star}_{(u,v)}(t)\right)+\int_{0 }^{T}\lambda^{\star}_{(u,v)}(\tau)\,d\tau,\quad T=\max\mathscr{T}\left( \mathcal{E}_{(u,v)}\right).\] (3)

In (Shchur et al., 2021), the operation of a neural temporal point process is summarized as:

* The edge events in \(\{(u,v,t_{i}):i\in[m]\}\) are represented as feature vectors \(\bm{x}_{i}=f_{\epsilon}(u,v,t_{i})\),
* The historical feature vectors are encoded into a state vector \(\bm{h}_{i}=f_{\text{b}}(\bm{x}_{1},\cdots\bm{x}_{i-1})\),
* The distribution of \(t_{i}\) conditioned on the past is simply conditioned on \(\bm{h}_{i}\).

The functions \(f_{\epsilon}\) and \(f_{\text{b}}\), as well as the conditioning on \(\bm{h}_{i}\), can be implemented using neural networks.

**Conjecture 2.1**.: _The samples from a neural temporal point process are **learnable**, i.e., a model exists which can perform temporal link predictions based on the past observations._

## 3 Counterfactual Analysis

Experiment SetupA model \(f\) is trained on a temporal graph \(\mathcal{E}_{\mathrm{train}}\) and tested on \(\mathcal{E}_{\mathrm{test}}\) through the binary classification approach resulting in metrics such as AU-ROC, and AP. In general, \(\mathcal{E}_{\mathrm{train}}=\mathcal{E}(0,\tau_{0})\), and \(\mathcal{E}_{\mathrm{test}}=\mathcal{E}(\tau_{0},T)\), i.e., the train and test data are chronologically split from the same temporal graph which is assumed to be generated through a common causal mechanism.

In light of the experimental setup, we ask the question: "Would the model \(f\) which is trained on \(\mathcal{E}_{\mathrm{train}}\) perform well if tested on a distorted version of \(\mathcal{E}_{\mathrm{test}}\) instead of \(\mathcal{E}_{\mathrm{test}}\)?" To formalise the question in the counterfactual framework proposed by Pearl (2019), we consider the following statements:

* The test data is \(\mathcal{E}_{\mathrm{test}}\).
* The test data is a temporally distorted version of \(\mathcal{E}_{\mathrm{test}}\).
* The performance metric is in the range \((\alpha-\epsilon,\min\{1,\alpha+\epsilon\})\).
* The performance metric is strictly less than \(\alpha-\epsilon\).

Then, the counterfactual question can be framed as \(P\left(y_{x}\mid x^{\prime},y^{\prime}\right)\) which stands for:

* The probability that the prediction accuracy would be less than \(\alpha-\epsilon\) had the test data been a temporally distorted version of \(\mathcal{E}_{\mathrm{test}}\), given the prediction accuracy was observed to be approximately \(\alpha\) when the model was tested on \(\mathcal{E}_{\mathrm{test}}\).

We link the counterfactual question to our hypothesis in the following proposition:

**Proposition 3.1**.: _If \(P\left(y_{x}\mid x^{\prime},y^{\prime}\right)\approx 0\implies\text{model $f$ cannot learn the temporal patterns in $\mathcal{E}_{\mathrm{train}}$}\)._

Proof.: Consider the set of logical statements:

* The temporal graph \(\mathcal{E}\) contains patterns that allow future edge predictions to be made based on past information, i.e., \(\mathcal{G}\) is learnable.
* The model \(f\) is _capable_ of learning the patterns in a learnable temporal graph.
* \(\mathcal{E}_{\mathrm{train}}=\mathcal{E}(0,\tau_{0}),\mathcal{E}_{\mathrm{ test}}=\mathcal{E}(\tau_{0},T)\).
* \(\mathcal{G}^{\prime}=\mathscr{D}(\mathcal{E}_{\mathrm{test}})\), where \(\mathscr{D}(\cdot)\) is the temporal distortion function.
* The model \(f\) is trained on \(\mathcal{E}_{\mathrm{train}}\).
* The prediction metric reported by \(f\) on the real test data \(\mathcal{E}_{\mathrm{test}}\) is higher than the prediction metric on the distorted data \(\mathcal{G}^{\prime}\). \[\begin{array}{rcl}\mathsf{s}_{1}\wedge\mathsf{s}_{2}\wedge\mathsf{s}_{3} \wedge\mathsf{s}_{4}&\wedge\mathsf{s}_{5}&\implies&\mathsf{s}_{6}\\ \neg\mathsf{s}_{6}&\implies&\neg\mathsf{s}_{1}\vee\neg\mathsf{s}_{2}\vee \neg\mathsf{s}_{3}\vee\neg\mathsf{s}_{4}\vee\neg\mathsf{s}_{5}&\text{( contraposition)}\end{array}\] (4)

For the experimental setup \(\mathsf{s}_{3}=1\), and \(\mathsf{s}_{5}=1\). Assuming that the temporal graph \(\mathcal{G}\) is learnable \(\mathsf{s}_{1}=1\), and that the function \(\mathscr{D}(\mathcal{E}_{\mathrm{test}})\) results in a temporally distorted version of \(\mathcal{E}_{\mathrm{test}}\), i.e., \(\mathsf{s}_{4}=1\), we get \(\neg\mathsf{s}_{6}\implies\neg\mathsf{s}_{2}\). Alternatively, \(\neg\mathsf{s}_{6}\equiv\mathbb{I}(P\left(y_{x}\mid x^{\prime},y^{\prime} \right)\approx 0)\), and \(\neg\mathsf{s}_{2}\) is interpreted as "model \(f\) is _incapable_ of learning the temporal patterns in \(\mathcal{G}^{\prime}\). 

ExampleIn Fig. 1, we show that \(\mathcal{E}_{\mathrm{train}}\cup\mathcal{E}_{\mathrm{test}}\) is sampled from a point process with intensity \(\lambda^{*}(t),t\in[0,T]\). We generate \(\mathcal{E}^{\prime}\) from another point process with intensity \(\lambda^{\prime}(t),t\in[\tau_{0},T]\). We depict the intensity functions as two sinusoidal waves with different frequency and phase. If a model \(f\) learns this intensity function by observing \(\mathcal{E}_{\mathrm{train}}\), and then generates samples for prediction, they would be more similar to \(\mathcal{E}_{\mathrm{test}}\) than \(\mathcal{E}^{\prime}\).

Figure 1: Example of Temporal distortion.

### Temporal Distortion Techniques

Let \(\mathcal{E}\) be a temporal graph sampled from a temporal point process with intensity \(\lambda^{\star}(t)\) for \(t\in[0,T]\). Let \(\mathcal{E}^{\prime}\) be data sampled from another point process with intensity \(\lambda^{\prime}(t)\) for \(t\in[0,T]\).

**Definition 3.1**.: The temporal graph \(\mathcal{E}^{\prime}\) is \(\delta\)**-temporally distorted** w.r.t. \(\mathcal{E}\) if for some \(\delta>0\),

\[\frac{1}{T}\int_{0}^{T}|\lambda^{\star}(t)-\lambda^{\prime}(t)|\,dt>\delta.\] (5)

In practice, we do not have access to the true intensity functions, and have to compare the realisations instead. Let \(\mathcal{E}\) and \(\mathcal{E}^{\prime}\) be two temporal graphs, then we measure the difference in their characteristics through the following two metrics.

**Definition 3.2**.: The average time difference (ATD) between \(\mathcal{E}\) and \(\mathcal{E}^{\prime}\) is defined as:

\[\textsf{ATD}(\mathcal{E},\mathcal{E}^{\prime})\triangleq\frac{1}{T|\mathcal{ E}|}\sum_{(u,v,t)\in\mathcal{E}}\min_{\mathcal{E}^{\prime}\in\mathscr{T} \left(\mathcal{E}^{\prime}_{(u,v)}\right)\cup\{T\}}|t-t^{\prime}|,\] (6)

where \(T=\max\mathscr{T}\left(\mathcal{E}\right)-\min\mathscr{T}\left(\mathcal{E}\right)\).

**Definition 3.3**.: The average count difference (ACD) between \(\mathcal{E}\) and \(\mathcal{E}^{\prime}\) is defined as:

\[\textsf{ACD}(\mathcal{E},\mathcal{E}^{\prime})\triangleq\frac{1}{|\mathcal{E }|}\sum_{(u,v,t)\in\mathcal{E}}\left||\mathcal{E}_{(u,v)}(t-\bar{\tau},t+\bar {\tau})|-|\mathcal{E}^{\prime}_{(u,v)}(t-\bar{\tau},t+\bar{\tau})|\right||,\] (7)

where \(\bar{\tau}=\frac{\max\mathscr{T}\left(\mathcal{E}\right)-\min\mathscr{T} \left(\mathcal{E}\right)}{|\mathcal{E}|}\).

Now that we are equipped with metrics to measure the difference between two temporal graphs, we device distortion functions \(\mathscr{D}(\cdot)\) which can enable us to investigate the counterfactual question posed earlier. We propose two distortion techniques \(\mathscr{D}_{\textsc{Intense}}(\cdot,K)\) which creates \(K\) time-perturbed copies of each edge events, and \(\mathscr{D}_{\textsc{Shuffle}}(\cdot)\) wherein the timestamps of different edge events are shuffled.

IntenseLet the real temporal graph data be denoted by \(\mathcal{E}=\cup_{(u,v)\in\mathcal{U}\times\mathcal{V}}\mathcal{E}_{(u,v)}\), and the distorted version be denoted by \(\mathcal{E}^{\prime}=\cup_{(u,v)\in\mathcal{U}\times\mathcal{V}}\mathcal{E}^{ \prime}_{(u,v)}\). then, for each edge event \((u,v,t)\) in the real data \(\mathcal{E}\), we create \(K\) edge events \((u,v,t+\tau)\) with \(\tau\) sampled uniformly from \((-\bar{\tau},\bar{\tau})\) for some \(\bar{\tau}>0\). Alternatively, if it is known that \(\mathcal{E}_{(u,v)}\) is sampled from a point process with intensity \(\lambda^{\star}_{(u,v)}(t)\), then we can generate \(\mathcal{E}^{\prime}_{(u,v)}\) by sampling from another point process with intensity \(\lambda^{\prime}_{(u,v)}(t)\), such that

\[\lambda^{\prime}_{(u,v)}(t)=K\lambda^{\star}_{(u,v)}(t),\,\forall(u,v)\in \mathcal{U}\times\mathcal{V}.\]

The operation of \(\mathscr{D}_{\textsc{Intense}}\) is described in Algorithm 1.

ShuffleFor any two edge events \((u,v,t),(u^{\prime},v^{\prime},t^{\prime})\in\mathcal{E}\), we shuffle the timestamps in the distorted version, i.e. \((u,v,t^{\prime}),(u^{\prime},v^{\prime},t)\in\mathcal{E}^{\prime}\). The shuffling process is also called label permutation (Chatterjee, 2018). In terms of the point process, we can explain shuffling as follows. If \(\mathcal{E}_{(u,v)}\) is known to be sampled from a point process with intensity \(\lambda^{\star}_{(u,v)}(t)\), then \(\mathcal{E}^{\prime}_{(u,v)}\) can be generated by sampling from an inhomogenous Poisson point process with intensity \(\lambda^{\prime}_{(u,v)}(t)\), where

\[\lambda^{\prime}_{(u,v)}(t)=\frac{\int_{0}^{T}\lambda^{\star}_{(u,v)}(t)\,dt} {\int_{0}^{T}\lambda^{\star}(t)\,dt}\lambda^{\star}(t),\,\forall(u,v)\in \mathcal{U}\times\mathcal{V}.\]

We describe the operation of \(\mathscr{D}_{\textsc{Shuffle}}\) in Algorithm 2.

```
0:\(\mathcal{E},K\in\mathbb{N}\)
0:\(\mathcal{E}^{\prime}=\varnothing\)
1:\(\mathcal{E}^{\prime}=\varnothing\)
2:\(\tau_{0}\leftarrow\min\mathscr{T}(\mathcal{E})\)
3:\(T\leftarrow\max\mathscr{T}(\mathcal{E})\)
4:\(\bar{\tau}\leftarrow\frac{T-\tau_{0}}{|\mathcal{E}|}\)
5:for\((u,v,t)\in\mathcal{E}\)do
6:for\(k\in[K]\)do
7:\(\tau\sim\textsc{Uniform}(-\bar{\tau},\bar{\tau})\)
8:\(\mathcal{E}^{\prime}\leftarrow\mathcal{E}^{\prime}\cup\{(u,v,t+\tau)\}\)
9:endfor
10:endfor ```

**Algorithm 1**\(\mathscr{D}_{\textsc{Intense}}\)

```
0:\(\mathcal{E}^{\prime}\leftarrow\varnothing\)
1:\(\mathcal{E}^{\prime}=\varnothing\)
2:\(\tau_{0}\leftarrow\min\mathscr{T}(\mathcal{E})\)
3:\(T\leftarrow\max\mathscr{T}(\mathcal{E})\)
4:\(\bar{\tau}\leftarrow\frac{T-\tau_{0}}{|\mathcal{E}|}\)
5:for\((u,v,t)\in\mathcal{E}\)do
6:for\(k\in[K]\)do
7:\(\tau\sim\textsc{Uniform}(-\bar{\tau},\bar{\tau})\)
8:\(\mathcal{E}^{\prime}\leftarrow\mathcal{E}^{\prime}\cup\{(u,v,t+\tau)\}\)
9:endfor
10:endfor ```

**Algorithm 2**\(\mathscr{D}_{\textsc{Shuffle}}\)

```
0:\(\mathcal{E}^{\prime}\leftarrow\varnothing\)
1:\(\mathcal{E}^{\prime}\leftarrow\varnothing\)
2:\(\mathcal{E}^{\prime}\leftarrow\mathcal{E}^{\prime}\cup\{(u,v,t)\in\mathcal{E}\}\)
3:for\((u,v,t)\in\mathcal{E}\)do
4:\(\tau\sim\mathcal{T}\)
5:\(\mathcal{E}^{\prime}\leftarrow\mathcal{E}^{\prime}\cup\{(u,v,\tau)\}\)
6:\(\mathcal{T}\leftarrow\mathcal{T}\setminus\{\tau\}\)
7:endfor ```

**Algorithm 2**\(\mathscr{D}_{\textsc{Shuffle}}\)

```
0:\(\mathcal{E}^{\prime}\leftarrow\mathcal{E}^{\prime}\cup\{(u,v,\tau)\}\)
1:\(\mathcal{E}^{\prime}\leftarrow\mathcal{E}^{\prime}\cup\{(u,v,\tau)\}\)
2:\(\mathcal{E}^{\prime}\leftarrow\mathcal{E}^{\prime}\cup\{(u,v,\tau)\}\)
3:\(\mathcal{E}^{\prime}\leftarrow\mathcal{E}^{\prime}\cup\{(u,v,t+\tau)\}\)
4:endfor ```

**Algorithm 3**\(\mathscr{D}_{\textsc{Shuffle}}\)

```
0:\(\mathcal{E}^{\prime}\leftarrow\mathcal{E}^{\prime}\cup\{(u,v,\tau)\}\)
4:\(\mathcal{E}^{\prime}\leftarrow\mathcal{E}^{\prime}\cup\{(u,v,\tau)\}\)
5:\(\mathcal{E}^{\prime}\leftarrow\mathcal{E}^{\prime}\cup\{(u,v,\tau)\}\)
6:\(\mathcal{E}^{\prime}\leftarrow\mathcal{E}^{\prime}\cup\{(u,v,\tau)\}\)
7:\(\mathcal{E}^{\prime}\leftarrow\mathcal{E}^{\prime}\cup\{(u,v,\tau)\}\)
8:endfor ```

**Algorithm 4**\(\mathscr{D}_{\textsc{Intense}}\)

### Temporal Distortion Techniques

Let \(\mathcal{E}\) be a temporal graph sampled from a temporal point process with intensity \(\lambda^{\star}(t)\) for \(t\in[0,T]\). Let \(\mathcal{E}^{\prime}\) be data sampled from another point process with intensity \(\lambda^{\star}(t)\) for \(t\in[0,T]\).

**Definition 3.1**.: The temporal graph \(\mathcal{E}^{\prime}\) is \(\delta\)**-temporally distorted** w.r.t. \(\mathcal{E}\) if for some \(\delta>0\),

\[\frac{1}{T}\int_{0}^{T}|\lambda^{\star}(t)-\lambda^{\prime}(t)|\,dt>\delta.\] (5)

In practice, we do not have access to the true intensity functions, and have to compare the realisations instead. Let \(\mathcal{E}\) and \(\mathcal{E}^{\prime}\) be two temporal graphs, then we measure the difference in their characteristics through the following two metrics.

**Definition 3.2**.: The average time difference (ATD) between \(\mathcal{E}\) and \(\mathcal{E}^{\prime}\) is defined as:

\[\textsf{ATD}(\mathcal{E},\mathcal{E}^{\prime})\triangleq\frac{1}{T|\mathcal{E}|} \sum_{(u,v,t)\in\mathcal{E}}\sum_{t^{\prime}\in\mathscr{T}\left(\mathcal{E}^{\prime} _{(Experiment

DatasetsWe use the following datasets1 to perform counterfactual analysis:

Footnote 1: The datasets can be downloaded from https://zenodo.org/records/7213796

* wikipedia(Kumar et al., 2019) describes a dynamic graph of interaction between the editors and Wikipedia pages over a span of one month. The entries consist of the user ID, page ID, and timestamp. The edge features are LIWC-feature vectors (Pennebaker et al., 2001) of the edit text. The edge feature dimension is \(172\).
* reddit(Kumar et al., 2019) describes a bipartite interaction graph between the users and subreddits. The interaction event is recorded with the IDs of the user, subreddit and timestamp. Similar to wikipedia, the post content is converted into a LIWC-feature vector of dimension \(172\) which serves as the edge feature.
* uci(Panzarasa et al., 2009) is a dynamic graph describing message-exchange among the students at University of California at Irvine (UCI) from April to October 2004. The interaction event consists of the user IDs, and timestamp.

The scale of the datasets are presented in Table 1. The datasets are chronologically split in the ratio \(0.7:0.15:0.15\) into train, validation, and test sets, respectively.

Next, we use \(\mathscr{D}_{\textsc{INTENSE}}(\cdot,5)\) and \(\mathscr{D}_{\textsc{SHUPLE}}(\cdot)\) to create \(10\) temporally distorted samples of the test splits of each dataset. In Table 2, we present the \(\mathsf{ATD}\), and \(\mathsf{ACD}\) by comparing the distorted samples with the original test data of different datasets. Through \(\mathscr{D}_{\textsc{INTENSE}}(\cdot,5)\), the \(\mathsf{ATD}\) is negligible, however, the \(\mathsf{ACD}\) is close to \(5\). Through \(\mathscr{D}_{\textsc{SHUPLE}}(\cdot)\), the \(\mathsf{ACD}\) is approximately \(1\) for wikipedia and reddit, and close to \(2\) for uci. We also see an increase in \(\mathsf{ATD}\) which is close to \(0.1\) for all datasets. Therefore, the metrics \(\mathsf{ATD}\) and \(\mathsf{ACD}\) should be considered in conjunction to measure the dissimilarity of two temporal graphs.

ModelsWe evaluate2 the performance of the following TLP models3 in light of Proposition 3.1:

Footnote 2: \(\mathsf{GPU}\): NVIDIA GeForce RTXTM 3060. \(\mathbf{CPU}\): 12th Gen Intel® CoreTM i7-12700 × 20; 16.0 GiB.

Footnote 3: The optimal hyper-parameters reported by the models are used.

* JODIE(Kumar et al., 2019) uses a recurrent neural network (RNN) to generate node embeddings for each interaction event. The future embedding of a node is estimated through a novel projection operator which is turn in used to predict future edge events.
* TGAT(Xu et al., 2020) relies on self-attention mechanism to generate node embeddings to capture the temporal evolution of the graph structure.
* TGN(Rossi et al., 2020) combine memory modules with graph-based operators to create an encoder-decoder pair capable of creating temporal node embeddings.
* CAWN(Wang et al., 2020) propose a novel strategy based on the law of triadic closure, where temporal walks retrieve the dynamic graph motifs without explicitly counting and selecting the motifs. The node IDs are replaced with the hitting counts to facilitate inductive inference.

For all the models we have forked the main branch of their original Github repositories, and added additional arguments to account for the distortion technique, as well as more focused logging. We wanted to evaluate GraphMixer(Cong et al., 2023) as it claims superior performance, however the distorted datasets we generated were not compatible with the dataloader used in their codebase.

\begin{table}
\begin{tabular}{l r r r} \hline \hline Dataset & \(|\mathcal{U}\cup\mathcal{V}|\) & \(|\mathcal{E}|\) \\ \hline wikipedia & 9227 & 157474 \\ reddit & 10984 & 672447 \\ nici & 1899 & 59835 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Number of nodes and edges in temporal graph datasets.

\begin{table}
\begin{tabular}{l r r r r r} \hline \hline  & \multicolumn{2}{c}{wikipedia} & \multicolumn{2}{c}{reddit} & \multicolumn{2}{c}{uci} \\ \cline{2-6}  & \(\mathsf{ATD}\) & \(\mathsf{ACD}\) & \(\mathsf{ATD}\) & \(\mathsf{ACD}\) & \(\mathsf{ATD}\) & \(\mathsf{ACD}\) \\ \hline Intense & 6.9e-6 \(\pm\) 2e-8 & 4.479 \(\pm\) 1.9e-3 & 1.6e-6 \(\pm\) 2e-9 & 4.112 \(\pm\) 3e-8 & 1.6e-5 \(\pm\) 1.2e-7 & 7.214 \(\pm\) 1.2e-2 \\ Shuffle & 0.078 \(\pm\) 5.7e-4 & 1.093 \(\pm\) 3.4e-4 & 0.099 \(\pm\) 3e-4 & 1.033 \(\pm\) 8e-5 & 0.132 \(\pm\) 8.4e-4 & 1.877 \(\pm\) 3.3e-3 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Distortion measures on different datasets.

ResultsThe models are evaluated under two settings: _transductive_, and _inductive_. In transductive TLP, the nodes \(u,v\) in the positive sample \((u,v,t)\in\mathcal{E}_{\text{test}}\) were observed during training. In contrast, in inductive TLP, at least one node in \(u,v\) is novel, and was not observed during training.

From Table 3 it is evident that none of the models are capable of distinguishing between the real data, and data sampled from a five-times more intense version. However, we see that TGN is fairly robust when the timestamps of the test data are shuffled, as its performance worsens the most compared to other models. The performance gap between the real and distorted versions decrease as the dataset size increases (see Table. 1).

In Fig. 2, and Fig. 3 we present the metric gap \(\mathbb{E}[y_{x}-y^{\prime}]\) for \(x\sim\mathscr{D}_{\text{INENSE}}(x^{\prime},5)\), and \(x\sim\mathscr{D}_{\text{SHUPLE}}(x^{\prime})\), respectively, for different models in categorical bar plots grouped by the dataset. We

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline \multirow{2}{*}{JODIE} & \multicolumn{2}{c}{wikipedia} & \multicolumn{2}{c}{reddit} & \multicolumn{2}{c}{uci} \\ \cline{2-7}  & AU-ROC & AP & AU-ROC & AP & AU-ROC & AP \\ \hline _transductive_ & 0.9170 \(\pm\) 3e-3 & 0.9137 \(\pm\) 5e-3 & 0.9679 \(\pm\) 4e-3 & 0.9654 \(\pm\) 5e-3 & 0.8950 \(\pm\) 3e-3 & 0.8726 \(\pm\) 3e-3 \\ \hline Intense & 0.9177 \(\pm\) 7e-3 & 0.9078 \(\pm\) 1e-2 & 0.9619 \(\pm\) 9e-3 & 0.9567 \(\pm\) 1e-2 & 0.9244 \(\pm\) 2e-3 & 0.9129 \(\pm\) 8e-3 \\ Shuffle & 0.9097 \(\pm\) 2e-2 & 0.8962 \(\pm\) 4e-2 & 0.9661 \(\pm\) 1e-2 & 0.9613 \(\pm\) 4e-2 & 0.8852 \(\pm\) 3e-3 & 0.8509 \(\pm\) 3e-3 \\ \hline _inductive_ & 0.8941 \(\pm\) 4e-3 & **0.8970** \(\pm\) 5e-3 & **0.9343** \(\pm\) 9e-3 & **0.9138** \(\pm\) 2e-2 & **0.7546** \(\pm\) 8e-3 & **0.7310** \(\pm\) 2e-2 \\ \hline Intense & 0.9036 \(\pm\) 1e-2 & 0.8972 \(\pm\) 1e-2 & 0.9457 \(\pm\) 1e-2 & 0.9308 \(\pm\) 4e-2 & 0.8384 \(\pm\) 3e-3 & 0.8332 \(\pm\) 8e-3 \\ Shuffle & 0.9157 \(\pm\) 1e-2 & 0.9078 \(\pm\) 2e-2 & 0.9419 \(\pm\) 3e-2 & 0.9251 \(\pm\) 6e-3 & 0.7368 \(\pm\) 5e-3 & 0.6994 \(\pm\) 8e-3 \\ \hline \hline \multirow{2}{*}{TGAT} & \multicolumn{2}{c}{wikipedia} & \multicolumn{2}{c}{reddit} & \multicolumn{2}{c}{uci} \\ \cline{2-7}  & AU-ROC & AP & AU-ROC & AP & AU-ROC & AP \\ \hline _transductive_ & 0.9499 \(\pm\) 2e-3 & 0.9528 \(\pm\) 2e-3 & 0.9806 \(\pm\) 6e-4 & **0.9818** \(\pm\) 6e-4 & **0.7885** \(\pm\) 1e-2 & **0.7694** \(\pm\) 7e-3 \\ \hline Intense & 0.9680 \(\pm\) 2e-3 & 0.9691 \(\pm\) 2e-3 & 0.9821 \(\pm\) 6e-4 & 0.9825 \(\pm\) 6e-4 & 0.8707 \(\pm\) 1e-2 & 0.8637 \(\pm\) 2e-2 \\ Shuffle & 0.9492 \(\pm\) 5e-3 & 0.9532 \(\pm\) 5e-3 & 0.9814 \(\pm\) 7e-3 & 0.9826 \(\pm\) 6e-3 & 0.7719 \(\pm\) 1e-2 & 0.7336 \(\pm\) 2e-2 \\ \hline _inductive_ & **0.9353** \(\pm\) 2e-3 & 0.9401 \(\pm\) 2e-3 & 0.9641 \(\pm\) 1e-3 & **0.9658** \(\pm\) 1e-3 & 0.7020 \(\pm\) 8e-3 & 0.7008 \(\pm\) 1e-2 \\ \hline Intense & 0.9604 \(\pm\) 2e-3 & 0.9621 \(\pm\) 2e-3 & 0.9676 \(\pm\) 8e-4 & 0.9676 \(\pm\) 1e-3 & 0.8019 \(\pm\) 2e-2 & 0.8095 \(\pm\) 2e-2 \\ Shuffle & 0.9257 \(\pm\) 7e-3 & 0.9304 \(\pm\) 7e-3 & 0.9644 \(\pm\) 7e-3 & 0.9664 \(\pm\) 3e-3 & 0.6558 \(\pm\) 7e-3 & 0.6324 \(\pm\) 1e-2 \\ \hline \hline \multirow{2}{*}{TGN} & \multicolumn{2}{c}{wikipedia} & \multicolumn{2}{c}{reddit} & \multicolumn{2}{c}{uci} \\ \cline{2-7}  & AU-ROC & AP & AU-ROC & AP & AU-ROC & AP \\ \hline Intense & 0.9898 \(\pm\) 1e-3 & 0.9911 \(\pm\) 6e-4 & 0.9723 \(\pm\) 2e-3 & 0.9744 \(\pm\) 2e-3 & 0.9653 \(\pm\) 3e-3 & 0.9709 \(\pm\) 3e-3 \\ Shuffle & 0.8310 \(\pm\) 3e-2 & 0.8487 \(\pm\) 3e-2 & 0.9533 \(\pm\) 2e-3 & 0.9563 \(\pm\) 2e-3 & 0.6722 \(\pm\) 6e-2 & 0.6520 \(\pm\) 4e-2 \\ \hline _inductive_ & **0.9374** \(\pm\) 1e-3 & **0.9463** \(\pm\) 1e-3 & **0.9299** \(\pm\) 1e-3 & **0.9346** \(\pm\) 1e-3 & **0.7714** \(\pm\) 6e-3 & **0.7948** \(\pm\) 6e-3 \\ \hline Intense & 0.9903 \(\pm\) 1e-3 & 0.9908 \(\pm\) 6e-4 & 0.9617 \(\pm\) 3e-3 & 0.9645 \(\pm\) 3e-3 & 0.9592 \(\pm\) 3e-3 & 0.9650 \(\pm\) 2e-3 \\ Shuffle & 0.8194 \(\pm\) 2e-2 & 0.8376 \(\pm\) 3e-2 & 0.9266 \(\pm\) 4e-3 & 0.9299 \(\pm\) 3e-3 & 0.6245 \(\pm\) 2e-2 & 0.6193 \(\pm\) 9e-3 \\ \hline \hline \multirow{2}{*}{CAWN} & \multicolumn{2}{c}{wikipedia} & \multicolumn{2}{c}{reddit} & \multicolumn{2}{c}{uci} \\ \cline{2-7}  & AU-ROC & AP & AU-ROC & AP & AU-ROC & AP \\ \hline _transductive_ & 0.9886 \(\pm\) 1e-4 & 0.9901 \(\pm\) 1e-4 & **0.9864** \(\pm\) 4e-3 & **0.9884** \(\pm\) 3e-3 & **0.9162** \(\pm\) 9e-4 & **0.9397** \(\pm\) 8e-4 \\ \hline Intense & 0.9977 \(\pm\) 9e-5 & 0.9975 \(\pm\) 8e-5 & 0.9931 \(\pm\) 8e-5 & 0.9942 \(\pm\) 7e-5 & 0.9848 \(\pm\) 6e-4 & 0.9889 \(\pm\) 7e-4 \\ Shuffle & 0.9868 \(\pm\) 3e-4 & 0.9887 \(\pm\) 3e-4 & 0.9859 \(\pm\) 6e-4 & 0.9880 \(\pm\) 2e-3 & 0.8495 \(\pm\) 7e-3 & 0.8866 \(\pm\) 2e-3 \\ \hline _inductive_ & 0.9877 \(\pm\) 5e-4 & **0.9896** \(\pm\) 4e-4 & **0.9833** \(\pm\) 5e-3 & **0.9859** \(\pm\) 3e-check whether \(\max{y_{x}}<\min{y^{\prime}}\) in an empirical way by checking if \(\mathbb{E}[y_{x}]+\texttt{CI}<\mathbb{E}[y^{\prime}]-\texttt{CI}^{\prime} \implies\mathbb{E}[y_{x}]-\mathbb{E}[y^{\prime}]<-(\texttt{CI}+\texttt{CI}^{ \prime})\). Therefore, we plot \(\mathbb{E}[y_{x}]-\mathbb{E}[y^{\prime}]\) as coloured bars, and \(-(\texttt{CI}+\texttt{CI}^{\prime})\) as black diamonds. Moreover, we indicate \(\epsilon=0.05\) as the dashed black line passing through \(-0.05\).

The models evaluated in this work form the set of baselines to validate the performance of new models. However, as we demonstrate, a higher metric alone is not indicative of good performance without sanity checks. The counterfactual question helps make the evaluation more explainable, as models that perform worse on temporally distorted data with high ATD and ACD can claim superiority over modes that do not. An ideal TLP model should be able to capture the difference in the count of edge events, as well as temporal shifts in the edge events.

Discussion

Moving away from the binary classification approach to assess the performance of temporal link prediction, the research should explore a generative approach where after observing a temporal graph from time \(t\in(0,\tau_{0})\), the model can generate a temporal graph in \(t\in(\tau_{0},T)\). This generated temporal graph should be compared with the ground truth for similarity to assess the performance of the model. The metrics \(\mathsf{ATD}\) and \(\mathsf{ACD}\) can be used to measure the difference in the timestamps, as well as the edge counts along the time axis.

We showed that the performance gap in light of Proposition 3.1 decreases with increasing size of the temporal graph, focus should be establish TLP models on smaller datasets, first in the transductive setting, and then progress to inductive setting. In the generative method of evaluation, we can also make use of other metrics that characterise a network, or a point process to add additional constraints.

Broader ImpactWe presented a framework, wherein we asked a counterfactual question, and then designed intervention mechanisms by generating temporally distorted test sets. In the future, researchers can devise their own temporal distortion techniques to assess the performance of a TLP model, if they follow the binary classification approach to evaluation. Our aim is also to encourage researchers to explore the generative evaluation strategy, and design TLP models which can generate temporal graphs after observing the edge events in the past. While our work focused on temporal graphs with ephemeral edges (see Definition 2.1), distortion techniques can also be designed for interval graphs, where the edge events persist for a duration. In this work, rather than introducing novel datasets, we present techniques for generating temporally distorted versions of any temporal graph dataset. This makes the contribution relevant even for datasets which will be introduced in the future.

LimitationsDue to resource constraints, we could not evaluate the models on more datasets. However, we aim to get additional results by the rebuttal period on the datasets used in Poursafaei and Rabbany (2023). We also wanted to measure the performance of the models through ranking metrics like \(\mathsf{MRR}\) or \(\mathsf{MAR}\), but the distorted datasets were not compatible with the dataloader used by Temporal Graph Benchmark (TGB) (Huang et al., 2024).

## References

* Celikkanat et al. (2024) A. Celikkanat, N. Nakis, and M. Morup. Continuous-time graph representation with sequential survival process. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 11177-11185, 2024.
* Chatterjee (2018) S. Chatterjee. Learning and memorization. In _International conference on machine learning_, pages 755-763. PMLR, 2018.
* Cong et al. (2023) W. Cong, S. Zhang, J. Kang, B. Yuan, H. Wu, X. Zhou, H. Tong, and M. Mahdavi. Do We Really Need Complicated Model Architectures For Temporal Networks? In _The Eleventh International Conference on Learning Representations_, Sept. 2023.
* Daud et al. (2020) N. N. Daud, S. H. Ab Hamid, M. Saadoon, F. Sahran, and N. B. Anuar. Applications of link prediction in social networks: A review. _Journal of Network and Computer Applications_, 166:102716, 2020.
* Du et al. (2016) N. Du, H. Dai, R. Trivedi, U. Upadhyay, M. Gomez-Rodriguez, and L. Song. Recurrent marked temporal point processes: Embedding event history to vector. In _Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining_, pages 1555-1564, 2016.
* Fan et al. (2019) W. Fan, Y. Ma, Q. Li, Y. He, E. Zhao, J. Tang, and D. Yin. Graph neural networks for social recommendation. In _The world wide web conference_, pages 417-426, 2019.
* Fan et al. (2021) Z. Fan, Z. Liu, J. Zhang, Y. Xiong, L. Zheng, and P. S. Yu. Continuous-time sequential recommendation with temporal graph collaborative transformer. In _Proceedings of the 30th ACM international conference on information & knowledge management_, pages 433-442, 2021.
* Farzaneh and Coon (2023) A. Farzaneh and J. P. Coon. An information-theoretic analysis on temporal graph evolution. In _Temporal Graph Learning Workshop@ NeurIPS 2023_, 2023.
* Fan et al. (2019)S. Huang, F. Poursafaei, J. Danovitch, M. Fey, W. Hu, E. Rossi, J. Leskovec, M. Bronstein, G. Rabusseau, and R. Rabbany. Temporal graph benchmark for machine learning on temporal graphs. _Advances in Neural Information Processing Systems_, 36, 2024.
* Kim et al. (2024) Y. Kim, Y. Lee, M. Choe, S. Oh, and Y. Lee. Temporal graph networks for graph anomaly detection in financial networks. _arXiv preprint arXiv:2404.00060_, 2024.
* Kumar et al. (2019) S. Kumar, X. Zhang, and J. Leskovec. Predicting dynamic embedding trajectory in temporal interaction networks. In _Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 1269-1278, 2019.
* Modell et al. (2024) A. Modell, I. Gallagher, E. Ceccherini, N. Whiteley, and P. Rubin-Delanchy. Intensity profile projection: A framework for continuous-time representation learning for dynamic networks. _Advances in Neural Information Processing Systems_, 36, 2024.
* Panzarasa et al. (2009) P. Panzarasa, T. Opsahl, and K. M. Carley. Patterns and dynamics of users' behavior and interaction: Network analysis of an online community. _Journal of the American Society for Information Science and Technology_, 60(5):911-932, 2009.
* Pearl (2019) J. Pearl. The seven tools of causal inference, with reflections on machine learning. _Communications of the ACM_, 62(3):54-60, 2019.
* Pennebaker et al. (2001) J. W. Pennebaker, M. E. Francis, and R. J. Booth. Linguistic inquiry and word count: Liwc 2001. _Mahway: Lawrence Erlbaum Associates_, 71(2001):2001, 2001.
* Perry and Wolfe (2013) P. O. Perry and P. J. Wolfe. Point process modelling for directed interaction networks. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 75(5):821-849, 2013.
* Poursafaei and Rabbany (2023) F. Poursafaei and R. Rabbany. Exhaustive Evaluation of Dynamic Link Prediction. In _2023 IEEE International Conference on Data Mining Workshops (ICDMW)_, pages 1121-1130, Shanghai, China, Dec. 2023. IEEE. ISBN 9798350381641. doi: 10.1109/ICDMW60847.2023.00147.
* Poursafaei et al. (2022) F. Poursafaei, S. Huang, K. Pelrine, and R. Rabbany. Towards better evaluation for dynamic link prediction. _Advances in Neural Information Processing Systems_, 35:32928-32941, 2022.
* Qin and Yeung (2023) M. Qin and D.-Y. Yeung. Temporal Link Prediction: A Unified Framework, Taxonomy, and Review, June 2023.
* Qin et al. (2024) Y. Qin, W. Ju, H. Wu, X. Luo, and M. Zhang. Learning graph ode for continuous-time sequential recommendation. _IEEE Transactions on Knowledge and Data Engineering_, 2024.
* Rossi et al. (2020) E. Rossi, B. Chamberlain, F. Frasca, D. Eynard, F. Monti, and M. Bronstein. Temporal graph networks for deep learning on dynamic graphs. _arXiv preprint arXiv:2006.10637_, 2020.
* Shchur et al. (2021) O. Shchur, A. C. Turkmen, T. Januschowski, and S. Gunnemann. Neural temporal point processes: A review. _arXiv preprint arXiv:2104.03528_, 2021.
* Trivedi et al. (2019) R. Trivedi, M. Farajtabar, P. Biswal, and H. Zha. Dyrep: Learning representations over dynamic graphs. In _International conference on learning representations_, 2019.
* Wang et al. (2020) Y. Wang, Y.-Y. Chang, Y. Liu, J. Leskovec, and P. Li. Inductive representation learning in temporal networks via causal anonymous walks. In _International Conference on Learning Representations_, 2020.
* Xu et al. (2020) D. Xu, C. Ruan, E. Korpeoglu, S. Kumar, and K. Achan. Inductive representation learning on temporal graphs. In _International Conference on Learning Representations_, 2020.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? We have mentioned the contributions of our work in the **Contributions** paragraph, and also in the abstract. 2. Did you describe the limitations of your work? In Sec. 5, we have discussed the limitations of our work. 3. Did you discuss any potential negative societal impacts of your work? [NA] 4. Have you read the ethics review guidelines and ensured that your paper conforms to them?
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? See Sec. 3. 2. Did you include complete proofs of all theoretical results? The proof of Proposition 3.1 is given in Sec. 3.
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? The URL is provided in the abstract. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? We have mentioned this as a footnote in Sec. 4. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? We reported the 95% confidence intervals for the metrics along with the means, which were run multiple times for random seeds, and on multiple samples. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? This is mentioned as a footnote in Sec. 4.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? We have cited the original sources, as well as the link where they are available. 2. Did you mention the license of the assets? The license of the previous datasets can be found at the link. 3. Did you include any new assets either in the supplemental material or as a URL? We have provided our code as a link in the abstract. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? The datasets are already released with a public license. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? The datasets do not contain any personally identifiable information or offensive content.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [NA] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [NA] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?