# Understanding Multi-phase Optimization Dynamics

and Rich Nonlinear Behaviors of ReLU Networks

 Mingze Wang

School of Mathematical Sciences

Peking University

Beijing, 100081, P.R. China

mingzewang@stu.pku.edu.cn &Chao Ma

Department of Mathematics

Stanford University

Stanford, CA 94305

chaoma@stanford.edu

###### Abstract

The training process of ReLU neural networks often exhibits complicated nonlinear phenomena. The nonlinearity of models and non-convexity of loss pose significant challenges for theoretical analysis. Therefore, most previous theoretical works on the optimization dynamics of neural networks focus either on local analysis (like the end of training) or approximate linear models (like Neural Tangent Kernel). In this work, we conduct a complete theoretical characterization of the training process of a two-layer ReLU network trained by Gradient Flow on a linearly separable data. In this specific setting, our analysis captures the whole optimization process starting from random initialization to final convergence. Despite the relatively simple model and data that we studied, we reveal four different phases from the whole training process showing a general simplifying-to-complicating learning trend. Specific nonlinear behaviors can also be precisely identified and captured theoretically, such as initial condensation, saddle-to-plateau dynamics, plateau escape, changes of activation patterns, learning with increasing complexity, etc.

## 1 Introduction

Deep learning shows its remarkable capabilities across various fields of applications. However, the theoretical understanding of its great success still has a long way to go. Among all theoretical topics, one of the most crucial aspect is the understanding of the optimization dynamics of deep neural network (NN), particularly the dynamics produced by Gradient Descent (GD) and its variants. This topic is highly challenging due to the highly non-convex loss landscape and existing works usually work with settings that do not align well with realistic practices. For instance, the extensively studied Neural Tangent Kernel (NTK) theory (Jacot et al., 2018; Du et al., 2018, 2019; Zou et al., 2018; Allen-Zhu et al., 2019) proves the global convergence of Stochastic gradient descent (SGD) to zero training error for highly over-parameterized neural networks; however, the optimization behaviors are similar to kernel methods and do not exhibit nonlinear behaviors, because neurons remain close to their initialization throughout training.

In reality, however, the training of practical networks can exhibit plenty of nonlinear behaviors (Chizat and Bach, 2018; Mei et al., 2019; Woodworth et al., 2020). In the initial stage of the training, a prevalent nonlinear phenomenon induced by small initialization is _initial condensation_(Maennel et al., 2018; Luo et al., 2021), where neurons condense onto a few isolated orientations. At the end of training, NNs trained by GD can _directionally converge_ to the KKT points of some constrained max-margin problem (Nacson et al., 2019; Lyu and Li, 2019; Ji and Telgarsky, 2020). However, KKT points are not generally unique, and determining which direction GD converges to can be challenging. Nonlinear training behaviors besides initial and terminating stages of optimization are also numerous. For example, for square loss, Jacot et al. (2021) investigates the _saddle-to-saddle_ dynamics where GD traverses a sequence of saddles during training, but it is unclear whether similar behavior canoccur for classification tasks using exp-tailed loss. Moreover, while in lazy regime, most activation patterns do not change during training ReLU networks, it remains uncertain when and how _activation patterns evolve_ beyond lazy regime. Additionally, while it is generally conjectured that GD _learns functions of increasing complexity_(Nakkiran et al., 2019), this perspective has yet to be proven.

As reviewed in Section 2, works have been done to analyze and explain the nonlinear training behaviors listed above. However, due to the complexity of the training dynamics, most existing works only focus on one phenomenon and conduct analysis on a certain stage of the training process. Few attempts have been done to derive a full characterization of the whole training dynamics from the initialization to convergence, and the settings adopted by these works are usually too simple to capture many nonlinear behaviors (Phuong and Lampert, 2021; Lyu et al., 2021; Wang and Ma, 2022; Boursier et al., 2022).

**In this work**, we make an attempt to theoretically describe the whole neural network training dynamics beyond the linear regime, in a setting that many nonlinear behaviors manifest. Specifically, We analyze the training process of a two-layer ReLU network trained by Gradient Flow (GF) on a linearly separable data. In this setting, our analysis captures the whole optimization process starting from random initialization to final convergence. Despite the relatively simple model and data that we studied, we reveal multiple phases in training process, and show a general simplifying-to-complicating learning trend by detailed analysis of each phase. Specifically, by our meticulous theoretical analysis of the whole training process, we precisely identify **four different phases** that exhibit **numerous nonlinear behaviors**. In Phase I, _initial condensation and simplification_ occur as living neurons rapidly condense in two different directions. Meanwhile, GF _escapes from the saddle_ around initialization. In Phase II, GF _gets stuck into the plateau_ of training accuracy for a long time, then _escapes_. In Phase III, a significant number of neurons are _deactivated_, leading to _self-simplification_ of the network, then GF tries to learn using the almost simplest network. In Phase IV, a considerable number of neurons are _reactivated_, causing _self-complication_ of the network. Finally, GF _converges towards an initialization-dependent direction_, and this direction is _not even a local max margin direction_. Overall, the whole training process exhibits a remarkable _simplifying-to-complicating_ behavior.

## 2 Other Related Works

Initial condensation phenomenon are studied in (Maennel et al., 2018; Luo et al., 2021; Zhou et al., 2022; Abbe et al., 2022; Chen et al., 2023; Liu et al., 2021; Boursier et al., 2022). Theoretically, Lyu et al. (2021); Boursier et al. (2022) analyze the condensation directions under their settings, which are some types of data average. Additionally, Atanasov et al. (2022) demonstrates that NNs in the rich feature learning regime learn a kernel machine due to the silent alignment phenomenon, similar to the initial condensation.

The end of training is extensively studied for classification tasks. Specifically, for classification with exponentially-tailed loss functions, if all the training data can be classified correctly, NNs trained by GD converge to the KKT directions of some constrained max-margin problem (Nacson et al., 2019; Lyu and Li, 2019; Chizat and Bach, 2020; Ji and Telgarsky, 2020; Kunin et al., 2023). In (Phuong and Lampert, 2021; Lyu et al., 2021), they analyze entire training dynamics and derive specific convergent directions that only depend on the data. Furthermore, another famous phenomenon in the end of training is the neural collapse (Papyan et al., 2020; Fang et al., 2021; Zhu et al., 2021; Han et al., 2021), which says the features represented by over-parameterized neural networks for data in a same class will collapse to one point, and such points for all classes converge to a simplex equiangular tight frame.

Saddle-to-saddle dynamics are explored for square loss in (Jacot et al., 2021; Zhang et al., 2022; Boursier et al., 2022; Pesme and Flammarion, 2023; Abbe et al., 2023). Furthermore, learning of increasing complexity, also called simplifying-to-complicating or frequency-principle, is investigated in (Arpit et al., 2017; Nakkiran et al., 2019; Xu et al., 2019; Rahaman et al., 2019).

Beyond lazy regime and local analysis, Phuong and Lampert (2021); Lyu et al. (2021); Wang and Ma (2022); Boursier et al. (2022) also characterize the whole training dynamics and exhibit a few of nonlinear behaviors. Specifically, Lyu et al. (2021) studies the training dynamics of GF on Leaky ReLU networks, which differ from ReLU networks because Leaky ReLU is always activated on any data. In (Safran et al., 2022), they studies the dynamics of GF on one dimensional dataset, and characterizes the effective number of linear regions. In (Brutzkus et al., 2017), they studies the dynamics of SGD on Leaky ReLU networks and linearly separable dataset. Moreover, Boursier et al. (2022) characterizes the dynamics on orthogonally data for square loss. The studies closest to our work are Phuong and Lampert (2021); Wang and Ma (2022), exploring the complete dynamics on classifying orthogonally separable data. However, this data is easy to learn, and all the features can be learned rapidly (accuracy=\(100\%\)) in initial training, followed by lazy training (activation patterns do not change). Unfortunately, this simplicity does not hold true for actual tasks on much more complex data, and NNs can only learn some features in initial training, which complicates the overall learning process. Furthermore, we provide a detailed comparison between our results and these works in Section 5. Another related work (Saxe et al., 2022) introduces a novel bias of learning dynamics: toward shared representations. This idea and the view of gating networks are enlightening for extending our two-layer theory to deep ReLU neural networks.

Our work also investigates the max-margin implicit bias of ReLU neural networks, and related works have been listed above. Although in homogenized neural networks such as ReLU, GD implicitly converges to a KKT point of the max-margin problem, it is still unclear where it is an actual optimum. A recent work (Vardi et al., 2022) showed that in many cases, the converged KKT point is not even a local optimum of the max margin problem. Besides, there are many other attempts to explain the implicit bias of deep learning (Vardi, 2023). Another popular implicit bias is the flat minima bias (Hochreiter and Schmidhuber, 1997; Keskar et al., 2016). Recent studies (Wu et al., 2018; Blanc et al., 2020; Ma and Ying, 2021; Li et al., 2021; Mulayoff et al., 2021; Wu et al., 2022; Wu and Su, 2023) provided explanations for why SGD favors flat minima and flat minima generalize well.

## 3 Preliminaries

**Basic Notations.** We use bold letters for vectors or matrices and lowercase letters for scalars, e.g. \(\bm{x}=\left(x_{1},\cdots,x_{d}\right)^{\top}\in\mathbb{R}^{d}\) and \(\bm{P}=\left(P_{ij}\right)_{m_{1}\times m_{2}}\in\mathbb{R}^{m_{1}\times m_{2}}\). We use \(\langle\cdot,\cdot\rangle\) for the standard Euclidean inner product between two vectors, and \(\lVert\cdot\rVert\) for the \(l_{2}\) norm of a vector or the spectral norm of a matrix. We use progressive representation \(\mathcal{O},\Omega,\Theta\) to hide absolute positive constants. For any positive integer \(n\), let \([n]=\{1,\cdots,n\}\). Denote by \(\mathcal{N}(\bm{\mu},\bm{\Sigma})\) the Gaussian distribution with mean \(\bm{\mu}\) and covariance matrix \(\bm{\Sigma}\), \(\mathbb{U}(\mathcal{S})\) the uniform distribution on a set \(\mathcal{S}\). Denote by \(\mathbb{I}\{E\}\) the indicator function for an event \(E\).

### Binary Classification with Two-layer ReLU Networks

**Binary classification.** In this paper, we consider the binary classification problem. We are given \(n\) training data \(\mathcal{S}=\{(\bm{x}_{i},y_{i})\}_{i=1}^{n}\subset\mathbb{R}^{d}\times\{\pm 1\}\). Let \(f(\cdot;\bm{\theta})\) be a neural network model parameterized by \(\bm{\theta}\), and aim to minimize the empirical risk given by:

\[\mathcal{L}(\bm{\theta})=\frac{1}{n}\sum_{i=1}^{n}\ell(y_{i}f(\bm{x}_{i};\bm{ \theta})),\] (1)

where \(\ell(\cdot):\mathbb{R}\rightarrow\mathbb{R}\) is the exponential-type loss function (Soudry et al., 2018; Lyu and Li, 2019) for classification tasks, including the most popular classification losses: exponential loss, logistic loss, and cross-entropy loss. Our analysis focuses on the exponential loss \(\ell(z)=e^{-z}\), while our method can be extended to logistic loss and cross-entropy loss.

**Two-layer ReLU Network.** Throughout the following sections, we consider two-layer ReLU neural networks comprising \(m\) neurons defined as

\[f(\bm{x};\bm{\theta})=\sum_{k=1}^{m}a_{k}\sigma(\bm{b}_{k}^{\top}\bm{x}),\]

where \(\sigma(z)=\max\{z,0\}\) is the ReLU activation function, \(\bm{b}_{1}\cdots,\bm{b}_{m}\in\mathbb{R}^{d}\) are the weights in the first layer, \(a_{1},\cdots,a_{m}\) are the weights in the second layer. And we consider the case that the weights in the second layer are fixed, which is a common setting used in previous studies (Arora et al., 2019; Chatterji et al., 2021). We use \(\bm{\theta}=(\bm{b}_{1}^{\top},\cdots,\bm{b}_{m}^{\top})^{\top}\in\mathbb{R}^{md}\) to denote the concatenation of all trainable weights.

### Gradient Flow Starting from Random Initialization

**Gradient Flow.** As the limiting dynamics of (Stochastic) Gradient Descent with infinitesimal learning rate (Li et al., 2017, 2019), we study the following Gradient Flow (GF) on the objective function (1):

\[\frac{\mathrm{d}\boldsymbol{\theta}(t)}{\mathrm{d}t}\in-\partial^{\circ} \mathcal{L}(\boldsymbol{\theta}(t)),\quad t\geq 0.\] (2)

Notice that the ReLU is not differentiable at \(0\), and therefore, the dynamics is defined as a subgradient inclusion flow (Bolte et al., 2010). Here, \(\partial^{\circ}\) denotes the Clarke subdifferential, which is a generalization of the derivative for non-differentiable functions. Additionally, to address the potential non-uniqueness of gradient flow trajectories, we adopt the definition of solutions for discontinuous systems (Filippov, 2013). For formal definitions, please refer to Appendix B, G, and H.

**Random Initialization.** We consider GF (2) starting from the following initialization:

\[\boldsymbol{b}_{k}(0) \stackrel{{\mathrm{i.i.d.}}}{{\sim}}\frac{\kappa_{1 }}{\sqrt{m}}\mathbb{U}(\mathbb{S}^{d-1})\text{ and }a_{k}=s_{k}\frac{\kappa_{2}}{\sqrt{m}}\text{ for }k \in[m];\] \[s_{k}=1\text{ for }k\in[m/2];\;s_{k}=-1\text{ for }k\in[m]-[m/2].\]

Here, \(0<\kappa_{1}<\kappa_{2}\leq 1\) control the initialization scale. It is worth noting that since the distribution \(\mathcal{N}(\boldsymbol{0},\boldsymbol{I}_{d}/d)\) is close to \(\mathbb{U}(\mathbb{S}^{d-1})\) in high-dimensional settings, our result can be extended to the initialization \(\boldsymbol{b}_{k}\stackrel{{\mathrm{i.i.d.}}}{{\sim}}\mathcal{N }(\boldsymbol{0},\kappa_{1}^{2}\boldsymbol{I}_{d}/md)\) with high probability guarantees.

### Linearly Separable Data beyond Orthogonally Separable

In previous works (Phuong and Lampert, 2021; Wang and Ma, 2022), a special case of the linearly separable dataset was investigated, namely "orthogonally separable". A training dataset is orthogonally separable when \(\left\langle\boldsymbol{x}_{i},\boldsymbol{x}_{j}\right\rangle\geq 0\) for \(i,j\) in the same class, and \(\left\langle\boldsymbol{x}_{i},\boldsymbol{x}_{j}\right\rangle\leq 0\) for \(i,j\) in different classes. As mentioned in Section 2, in this case, GF can learn all features and achieve \(100\%\) training accuracy quickly, followed by lazy training. In this work, we consider data that is more difficult to learn, which leads to more complicated optimization dynamics. Specifically, we consider the following data.

**Assumption 3.1**.: Consider the linearly separable dataset \(\mathcal{S}=\{(\boldsymbol{x}_{i},y_{i})\}_{i\in[n]}\subset\mathbb{R}^{d} \times\mathbb{R}\) such that

\[(\boldsymbol{x}_{i},y_{i})=\begin{cases}(\boldsymbol{x}_{+},1),\;i\in[n_{+}]\\ (\boldsymbol{x}_{-},-1),\;i\in[n]-[n_{+}]\end{cases}\quad\text{, where }\boldsymbol{x}_{+}, \boldsymbol{x}_{-}\in\mathbb{S}^{d-1}\text{ are two data points with a small angle }\Delta\in(0,\pi/2)\text{, and }n_{+},n_{-}\text{ are the numbers of positive and negative samples, respectively, with }n=n_{+}+n_{-}\text{.}\) We also use \(p:=n_{+}/n_{-}\) to denote the ratio of \(n_{+}\) and \(n_{-}\), which measures the class imbalance. Furthermore, we assume \(p\cos\Delta>1\).

**Remark 3.2**.: We focus on the training dataset satisfying Assumption 3.1 with a small \(\Delta\ll 1\). The margin of the dataset is \(\sin(\Delta/2)\), which implies that the separability of this data is much weaker than that of orthogonal separable data. Additionally, the condition \(p\cos\Delta>1\) merely requires a slight imbalance in the data. These two properties work together to produce rich nonlinear behaviors during training.

## 4 Characterization of Four-phase Optimization Dynamics

In this section, we study the whole optimization dynamics of GF (2) starting from random initialization when training the two-layer ReLU network on linearly separable dataset satisfying Assumption 3.1 and using the loss function (1). To begin with, we introduce some additional notations.

**Additional Notations.** First, we identify several crucial data-dependent directions under Assumption 3.1. These include two directions that are orthogonal to the data, defined as \(\boldsymbol{x}_{+}^{\perp}:=\frac{\boldsymbol{x}_{-}-(\boldsymbol{x}_{-}, \boldsymbol{x}_{+})\boldsymbol{x}_{+}}{\|\boldsymbol{x}_{-}-(\boldsymbol{x}_{- },\boldsymbol{x}_{+})\boldsymbol{x}_{+}\|}\) and \(\boldsymbol{x}_{-}^{\perp}:=\frac{\boldsymbol{x}_{+}-(\boldsymbol{x}_{+}, \boldsymbol{x}_{-})\boldsymbol{x}_{-}}{\|\boldsymbol{x}_{+}-(\boldsymbol{x}_{ +},\boldsymbol{x}_{-})\boldsymbol{x}_{-}\|}\), which satisfy \(\left\langle\boldsymbol{x}_{+},\boldsymbol{x}_{+}^{\perp}\right\rangle=\left \langle\boldsymbol{x}_{-},\boldsymbol{x}_{-}^{\perp}\right\rangle=0\). Additionally, we define the label-average data direction as \(\boldsymbol{\mu}:=\frac{\boldsymbol{x}}{\|\boldsymbol{z}\|}\) where \(\boldsymbol{z}=\frac{1}{n}\sum_{i=1}^{n}y_{i}\boldsymbol{x}_{i}\). One can verify that \(\left\langle\boldsymbol{\mu},\boldsymbol{x}_{+}\right\rangle>0\) and \(\left\langle\boldsymbol{\mu},\boldsymbol{x}_{-}\right\rangle>0\) under the condition \(p\cos\Delta>1\). In Figure 3, we visualize these directions.

Second, we use the following notations to denote important quantities during the GF training process. We denote the prediction on \(\boldsymbol{x}_{+}\) and \(\boldsymbol{x}_{-}\) by \(f_{+}(t):=f(\boldsymbol{x}_{+};\boldsymbol{\theta}(t)),f_{-}(t):=f(\boldsymbol{ x}_{-};\boldsymbol{\theta}(t))\). We use\(\mathrm{Acc}(t):=\frac{1}{n}\sum_{i=1}^{n}\mathbb{I}\{y_{i}f(\bm{x}_{i};\bm{\theta} (t))>0\}\) to denote the training accuracy at time \(t\). For each neuron \(k\in[m]\), we use \(\bm{w}_{k}(t):=\bm{b}(t)/\left\lVert\bm{b}(t)\right\rVert\) and \(\rho_{k}(t):=\left\lVert\bm{b}(t)\right\rVert\) to denote its direction and norm, respectively. To capture the activation dynamics of each neuron \(k\in[m]\) on each data, we use \(\texttt{sgn}_{k}^{+}(t):=\texttt{sgn}(\langle\bm{b}_{k}(t),\bm{x}_{+}\rangle)\) to record whether the \(k\)-th neuron is activated with respect to \(\bm{x}_{+}\), and \(\texttt{sgn}_{k}^{-}(t):=\texttt{sgn}(\langle\bm{b}_{k}(t),\bm{x}_{-}\rangle)\) defined similarly, which we call ReLU activation patterns.

### A Brief Overview of four-phase Optimization Dynamics

We illustrate different phases in the training dynamics by a numerical example. Specifically, we train a network on the dataset that satisfies Assumption 3.1 with \(p=4\) and \(\Delta=\pi/15\). The directions and magnitudes of the neurons at some important times are shown in Figure 1, reflecting four different phases on the training behavior and activation patterns. More experiment details and results can be found in Appendix A.1.

From Fig 1(a) to (b) is the **Phase I** of the dynamics, marked by a condensation of neurons. Although the initial directions are random, we see that all neurons are rapidly divided into three categories: living positive neurons \((k\in\mathcal{K}_{+})\) and living negative neurons \((k\in\mathcal{K}_{-})\) condense in one direction each (\(\bm{\mu}\) and \(\bm{x}_{+}^{\perp}\)), while other neurons \((k\notin\mathcal{K}_{+}\cup\mathcal{K}_{-})\) are deactivated forever. From the perspective of loss landscape, GF rapidly escapes from the saddle near \(\bm{0}\) where the loss gradient vanishes.

From Fig 1(b) to (c) is the **Phase II** of the dynamics, in which GF gets stuck into a plateau with training accuracy \(\frac{p}{1+p}\) for a long time \(T_{\mathrm{plat}}\) before escaping. Once the dynamics escapes from the plateau, the training accuracy rises to a perfect \(100\%\). Moreover, activation patterns do not change in this phase.

From Fig 1(c) to (d) is the **Phase III** of the dynamics. The phase transition from phase II to phase III sees a rapid deactivation of all the living positive neurons \(k\in\mathcal{K}_{+}\) on \(\bm{x}_{-}\) rapidly, while other activation patterns are unchanged. This leads to a simpler network in phase III, in which only living positive neurons (in \(\mathcal{K}_{+}\)) predict \(\bm{x}_{+}\), and only living negative neurons (in \(\mathcal{K}_{-}\)) predict \(\bm{x}_{-}\). Hence, in this phase the GF tries to learn the training data using almost the simplest network by only changing the norms of the neurons.

Finally, Fig 1(d) to (e) shows **Phase IV**, starting from another "phase transition" when all the living negative neurons (\(k\in\mathcal{K}_{-}\)) reactivate simultaneously on \(\bm{x}_{+}\). This leads to a more complicated network. After the phase transition, the activation patterns no longer change, and the neurons eventually converges towards some specific directions dependent on both data and initialization. Additionally, this direction is not even the local optimal max margin direction.

**Overall**, the whole dynamics exhibit a simplifying-to-complicating learning trend.

In the following four subsections, we present a meticulously detailed and comprehensive depiction of the whole optimization dynamics and nonlinear behaviors. For clarity, in Figure 2, we first display the timeline of our dynamics and some nonlinear behaviors.

In Appendix A.2, we further validate our theoretical bounds on the key time points in Figure 2 numerically. Additionally, in Appendix A.3, we relax the data Assumption 3.1 by perturbing the

Figure 1: These figures visualize (in polar coordinates) the projections of all neurons \(\{\bm{b}_{k}(t)\}_{k\in[m]}\) onto the \(2\)d subspace \(\mathrm{span}\{\bm{x}_{+},\bm{x}_{-}\}\) during training. Each purple star represents a positive neuron \((k\in[m/2])\), while each brown star represents a negative neuron \((k\in[m]-[m/2])\). Additionally, the directions of \(\bm{x}_{+},\bm{x}_{-},\bm{x}_{+}^{\perp},\bm{x}_{-}^{\perp},\bm{\mu}\) are plotted in blue, orange, green, red and pink colors, respectively. The complete version of these figures is Figure 4 in Appendix A.1.

data with random noise, and our experimental results illustrate that similar four-phase dynamics and nonlinear behaviors persist.

### Phase I. Initial Condensation and Saddle Escape

Let \(T_{\text{I}}=10\sqrt{\frac{\kappa_{1}}{\kappa_{2}}}\), and we call \(t\in[0,T_{\text{I}}]\) Phase I. The theorem below is our main result in Phase I.

**Theorem 4.1** (Initial Condensation).: _Let the width \(m=\Omega\left(\log(1/\delta)\right)\), the initialization \(\kappa_{1},\kappa_{2}=\mathcal{O}(1)\) and \(\kappa_{1}/\kappa_{2}=\mathcal{O}(\Delta^{8})\). Then with probability at least \(1-\delta\), the following results hold at \(T_{\text{I}}\):_

**(S1)** _Let \(\mathcal{K}_{+}\) be the index set of living positive neurons at \(T_{\text{I}}\), i.e. \(\mathcal{K}_{+}:=\{k\in[m/2]:\text{sgn}_{k}^{+}(T_{\text{I}})=1\text{ or }\text{sgn}_{k}^{-}(T_{\text{I}})=1\}\). Then,_ **(i)**_\(0.21m\leq|\mathcal{K}_{+}(T_{\text{I}})|\leq 0.29m\). Moreover, for any \(k\in\mathcal{K}_{+}\),_ **(ii)** _its norm is small but significant: \(\rho_{k}(T_{\text{I}})=\Theta\left(\sqrt{\frac{\kappa_{1}\kappa_{2}}{\kappa_{ 2}}}\right)\);_ **(iii)** _Its direction is strongly aligned with \(\bm{\mu}\): \(\left\langle\bm{w}_{k}(T_{\text{I}}),\bm{\mu}\right\rangle\geq 1-\mathcal{O} \left(\sqrt{\kappa_{1}\kappa_{2}}\right)-\mathcal{O}\left((\kappa_{1}/\kappa_ {2})^{0.55}\right)\);_ **(iv)** _\(\text{sgn}_{k}^{+}(T_{\text{I}})=\text{sgn}_{k}^{-}(T_{\text{I}})=1\)._

**(S2)** _Let \(\mathcal{K}_{-}\) be the index set of living negative neurons at \(T_{\text{I}}\), i.e. \(\mathcal{K}_{-}:=\{k\in[m]-[m/2]:\text{sgn}_{k}^{+}(T_{\text{I}})=1\text{ or }\text{sgn}_{k}^{-}(T_{\text{I}})=1\}\). Then,_ **(i)**_\(0.075m\leq|\mathcal{K}_{-}|\leq 0.205m\). Moreover, for any \(k\in\mathcal{K}_{-}\),_ **(ii)** _its norm is tiny: \(\rho_{k}(T_{\text{I}})=\mathcal{O}\left(\frac{\sqrt{\kappa_{1}\kappa_{2}}}{ \sqrt{m}}\big{(}\sqrt{\frac{\kappa_{1}}{\kappa_{2}}}+\frac{\Delta}{p}\big{)}\right)\);_ **(iii)** _its direction is aligned with \(\bm{x}_{+}^{\perp}\): \(\left\langle\bm{w}_{k}(T_{\text{I}}),\bm{x}_{+}^{\perp}\right\rangle\geq 1- \mathcal{O}\left((\sqrt{\frac{\kappa_{1}}{\kappa_{2}}}\frac{p}{\Delta})^{1.6}\right)\);_ **(iv)** _\(\text{sgn}_{k}^{-}(T_{\text{I}})=1\), but \(\text{sgn}_{k}^{+}(T_{\text{I}})=0\)._

**(S3)** _For other neuron \(k\notin\mathcal{K}_{+}\cup\mathcal{K}_{-}\), it dies and remains unchanged during the remaining training process: \(\text{sgn}_{k}^{+}(t)\leq 0\), \(\text{sgn}_{k}^{-}(t)\leq 0\), \(\bm{b}_{k}(t)\equiv\bm{b}_{k}(T_{\text{I}}),\ \forall t\geq T_{\text{I}}\)._

**(S4).**\(f_{+}(T_{\text{I}})=\Theta\left(\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}\right),f_{-} (T_{\text{I}})=\Theta\left(\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}\right)\)_, and \(\operatorname{Acc}(T_{\text{I}})=\frac{p}{1+p}\)._

**Initial condensation and simplification.** Theorem 4.1 (S1)(S2)(S3) show that, after a short time \(T_{\text{I}}=\Theta(\sqrt{\kappa_{1}/\kappa_{2}})\), all neurons are implicitly simplified to three categories: \(\mathcal{K}_{+}\), \(\mathcal{K}_{-}\) and others. The living positive neurons \(k\in\mathcal{K}_{+}\) align strongly with \(\bm{\mu}\), and the living negative neurons \(k\in\mathcal{K}_{-}\) align with \(\bm{x}_{+}^{\perp}\) and lie on the manifold orthogonal to \(\bm{x}_{+}\). Other neurons die and remain unchanged during the remaining training process. Moreover, we also estimate tight bounds for \(|\mathcal{K}_{+}|\) and \(|\mathcal{K}_{+}|\). Actually, \(\kappa_{1}/\kappa_{2}=\mathcal{O}(1)\) can ensure Theorem 4.1 and initial condensation hold (please refer to Appendix C), and we write \(\kappa_{1}/\kappa_{2}=\mathcal{O}(\Delta^{8})\) here to ensure that the dynamics of later phases hold. In Phase I, the dynamics exhibit a fast condensation phenomenon, i.e., in addition to dead neurons, living positive and negative neurons condense in one direction each.

**Saddle-to-Plateau.** The network is initially close to the saddle point at \(\bm{0}\) (where the loss gradient vanishes). However, Theorem 4.1 (S1) reveals that despite being small, there is a significant growth in the norm of living positive neuron \(k\in\mathcal{K}_{+}\) from \(\Theta(\kappa_{1}/\sqrt{m})\) to \(\Theta(\sqrt{\kappa_{1}\kappa_{2}}/\sqrt{m})\) and the predictions also experience substantial growth (S4). This means that GF escapes from this saddle rapidly. Furthermore, it is worth noting that initial training accuracy can simply be \(0\), \(\frac{1}{1+p}\), \(\frac{p}{1+p}\), or \(1\). However, after Phase I, the training accuracy reaches \(\operatorname{Acc}(T_{\text{I}})=\frac{p}{1+p}\) which we will prove as a plateau in the next subsection. Therefore, Phase I exhibits saddle-to-plateau dynamics.

**Remark 4.2**.: Throughout the following subsections, we call the neuron \(k\in\mathcal{K}_{+}\) the "living positive neuron", the neuron \(k\in\mathcal{K}_{-}\) the "living negative neuron", and the neuron \(k\notin\mathcal{K}_{+}\cup\mathcal{K}_{-}\) the "dead neuron". Moreover, we denote \(m_{+}:=|\mathcal{K}_{+}|\), \(m_{-}:=|\mathcal{K}_{-}|\), and \(\alpha:=\frac{m_{-}}{m_{+}}\). Notice that Theorem 4.1(S1)(S2) guarantee that \(0<\frac{0.075}{0.29}\leq\alpha\leq\frac{0.205}{0.21}<1\).

**Remark 4.3**.: The results in the following subsections are all based on the occurrence of the events in Theorem 4.1 and with the same settings as Theorem 4.1. So they all hold with probability at least \(1-\delta\).

Please refer to Appendix C for the proof of Phase I.

### Phase II. Getting Stuck in and Escaping from Plateau

In this phase, we study the dynamics before the patterns of living neurons change again after Phase I. Specifically, we define

\[T_{\rm II}:=\inf\{t>T_{\rm I}:\exists k\in\mathcal{K}_{+}\cup\mathcal{K}_{-}, \,\mathsf{sgn}_{k}^{+}(t)\neq\mathsf{sgn}_{k}^{+}(T_{\rm I})\text{ or }\mathsf{sgn}_{k}^{-}(t)\neq\mathsf{sgn}_{k}^{-}(T_{\rm I})\},\]

and call \(t\in(T_{\rm I},T_{\rm II}]\) Phase II.

**Theorem 4.4** (End of Phase II).: **(S1)**__\(T_{\rm II}=\Theta\left(\frac{1}{\kappa_{2}^{2}\Delta^{2}}\right)\)_. **(S2)**__\(\mathcal{L}(\bm{\theta}(T_{\rm II}))=\Theta\left(p^{-\frac{1}{1-\alpha \cos\Delta}}\right)\)._

**(S3)** _One of living positive neuron \(k_{0}\in\mathcal{K}_{+}\) precisely changes its pattern on \(\bm{x}_{-}\) at \(T_{\rm II}\): \(\lim\limits_{t\to T_{\rm II}}\mathsf{sgn}_{k_{0}}^{-}(t)=1\) and \(\lim\limits_{t\to T_{\rm II}^{+}}\mathsf{sgn}_{k_{0}}^{-}(t)=0\), while all other activation patterns remain unchanged._

Recalling the results in Theorem 4.1, during Phase II, the activation patterns do not change with \(\mathsf{sgn}_{k}^{+}(t)=\mathsf{sgn}_{k}^{-}(t)=1\) for \(k\in\mathcal{K}_{+}\) and \(\mathsf{sgn}_{k}^{+}(t)=0,\mathsf{sgn}_{k}^{-}(t)=1\) for \(k\in\mathcal{K}_{-}\). Theorem 4.4 demonstrates that at the end of Phase II, except for one of living positive neuron \(k_{0}\in\mathcal{K}_{+}\) precisely changes its pattern on \(\bm{x}_{-}\), all other activation patterns remain unchanged.

**Theorem 4.5** (Plateau).: _We define the hitting time \(T_{\rm plat}:=\inf\{t\in[T_{\rm I},T_{\rm II}]:\mathrm{Acc}(t)=1\}\). Then,_ **(S1)**__\(T_{\rm plat}=\Theta\left(\frac{p}{\kappa_{2}^{2}\Delta^{2}}\right)\);_ **(S2)**__\(\forall t\in[T_{\rm I},T_{\rm plat}]\), \(\mathrm{Acc}(t)\equiv\frac{p}{1+p}\);_ **(S3)**__\(\forall t\in(T_{\rm plat},T_{\rm II}]\), \(\mathrm{Acc}(t)\equiv 1\)._

**Plateau of training accuracy.** According to Theorem 4.5, during Phase II, the training accuracy gets stuck in a long plateau \(\frac{p}{1+p}\), which lasts for \(\Theta\big{(}\frac{p}{\kappa_{2}^{2}\Delta^{2}}\big{)}\) time. However, once escaping from this plateau, the training accuracy rises to \(100\%\). It is worth noting that this plateau is essentially induced by the dataset. All that's required is only mild imbalance (\(p\) is slightly greater than 1 such that \(p\cos\Delta>1\)) and a small margin \(\sin(\Delta/2)\) of two data classes. Notably, if the dataset has an extremely tiny margin \((\Delta\to 0)\), then the length of this plateau will be significantly prolonged \((T_{\rm plat}\to+\infty)\), which implies how the data separation can affect the training dynamics. Additionally, using a smaller initialization scale \(\kappa_{1}\) of the input layers cannot avoid this plateau.

Please refer to Appendix D for the proof of Phase II.

### Phase III. Simplifying by Neuron Deactivation, and Trying to Learn by Simplest Network

Building upon Phase II, we demonstrate that within a short time, all the living positive neurons \(\mathcal{K}_{+}\) change their activation patterns, corresponding to a "phase transition". Specifically, we define

\[T_{\rm II}^{\rm PT}:=\inf\{t>T_{\rm II}:\forall k\in\mathcal{K}_{+},\mathsf{ sgn}_{k}^{-}(t)=0\},\]

and we call \(t\in(T_{\rm II},T_{\rm II}^{\rm PT}]\) the phase transition from Phase II to Phase III.

**Theorem 4.6** (Phase Transition).: **(S1)**__\(T_{\rm II}^{\rm PT}=\left(1+\mathcal{O}\left(\sqrt{\kappa_{1}\kappa_{2}^{2}} \right)\right)T_{\rm II}\);_ **(S2)**__\(\mathsf{sgn}_{k}^{+}(T_{\rm II}^{\rm PT})=1\) and \(\mathsf{sgn}_{k}^{-}(T_{\rm II}^{\rm PT})=0\) for any \(k\in\mathcal{K}_{+}\); \(\mathsf{sgn}_{k}^{+}(T_{\rm II}^{\rm PT})=0\) and \(\mathsf{sgn}_{k}^{-}(T_{\rm II}^{\rm PT})=1\) for any \(k\in\mathcal{K}_{-}\)._

**Neuron deactivation.** As shown in Theorem 4.6 (S2), after the phase transition, _all_ the living positive neurons \(k\in\mathcal{K}_{+}\) undergo deactivation for \(\bm{x}_{-}\), i.e., \(\mathsf{sgn}_{k}^{-}(t)\) changes from 1 to 0, while other activation patterns remain unchanged. Furthermore, Theorem 4.6 (S1) reveals that the phase transition is completed quite quickly by using sufficiently small initialization value \(\kappa_{1},\kappa_{2}\). A smaller initialization value leads to a more precise initial condensation \(\bm{w}_{k}(T_{\rm I})\approx\bm{\mu}\), causing all living positive neurons to remain closer together before \(T_{\rm II}\) and thus changing their patterns nearly simultaneously.

Self-simplifying.As a result, the network implicitly simplifies itself through the deactivation behavior. At \(T_{\mathrm{III}}^{\mathrm{PT}}\), only living negative neurons \(k\in\mathcal{K}_{+}\) are used for predicting on \(\bm{x}_{-}\), i.e., \(f_{-}(T_{\mathrm{II}}^{\mathrm{PT}})=\frac{\kappa_{2}}{\sqrt{m}}\sum_{k\in \mathcal{K}_{-}}\sigma(\langle\bm{b}_{k}(T_{\mathrm{II}}^{\mathrm{PT}}),\bm{x} _{-}\rangle)\). In contrast, during Phase II, both living positive and living negative neurons jointly predict on \(\bm{x}_{-}\), i.e., \(f_{-}(t)=\frac{\kappa_{2}}{\sqrt{m}}\sum_{k\in\mathcal{K}_{+}}\sigma(\langle \bm{b}_{k}(t),\bm{x}_{-}\rangle)-\frac{\kappa_{2}}{\sqrt{m}}\sum_{k\in\mathcal{ K}_{-}}\sigma(\langle\bm{b}_{k}(t),\bm{x}_{-}\rangle)\). As indicated in Table 1, two classes of activation patterns are simplified from \((1,0)\) to \((0,0)\), while others do not change.

After this phase transition, we study the dynamics before the patterns of living neurons change again. Specifically, we define

\[T_{\mathrm{III}}:=\inf\{t>T_{\mathrm{II}}^{\mathrm{PT}}:\exists k\in\mathcal{ K}_{+}\cup\mathcal{K}_{-},\mathsf{sgn}_{k}^{+}(t)\neq\mathsf{sgn}_{k}^{+}(T_{ \mathrm{II}}^{\mathrm{PT}})\text{ or }\mathsf{sgn}_{k}^{-}(t)\neq\mathsf{sgn}_{k}^{-}(T_{ \mathrm{II}}^{\mathrm{PT}})\},\]

and call \(t\in(T_{\mathrm{II}},T_{\mathrm{III}}]\) Phase III.

**Theorem 4.7** (End of Phase III).: \(T_{\mathrm{III}}=\big{(}1+\Theta(\Delta^{2})\big{)}T_{\mathrm{II}}\)_._

Learning by simplest network.During \(t\in(T_{\mathrm{II}}^{\mathrm{PT}},T_{\mathrm{III}})\), all activation patterns do not change. This ensures that \(f_{+}(t)=\frac{\kappa_{2}}{\sqrt{m}}\sum_{k\in\mathcal{K}_{+}}\sigma(\langle \bm{b}_{k}(t),\bm{x}_{+}\rangle)\), while \(f_{-}(t)=-\frac{\kappa_{2}}{\sqrt{m}}\sum_{k\in\mathcal{K}_{-}}\sigma(\langle \bm{b}_{k}(t),\bm{x}_{-}\rangle)\). Additionally, by using sufficiently small \(\kappa_{1}\), the neurons in \(\mathcal{K}_{+}\) and \(\mathcal{K}_{-}\) keep close together respectively before \(T_{\mathrm{III}}\), making the network close to a simple two-neuron network consisting of one positive neuron and one negative neuron. Please refer to Appendix E for more details. Furthermore, this pattern scheme is almost the "simplest" way to ensure binary classification: the living positive neurons only predict positive data \(\bm{x}_{+}\) while the living negative neurons only predict negative data \(\bm{x}_{-}\). Therefore, GF tries to learn by this almost simplest network in this phase.

Please refer to Appendix E for the proof of Phase III.

### Phase IV. Complicating by Neuron Reactivation, and Directional Convergence

Phase IV begins with an instantaneous phase transition at time \(T_{\mathrm{III}}\).

**Theorem 4.8** (Phase Transition).: _All living negative neuron \(k\in\mathcal{K}_{-}\) simultaneously change their patterns on \(\bm{x}_{+}\) at \(T_{\mathrm{III}}\): \(\lim\limits_{t\to T_{\mathrm{III}}^{\mathrm{PT}}}\mathsf{sgn}_{k}^{+}(t)=0\), \(\lim\limits_{t\to T_{\mathrm{III}}^{\mathrm{PT}}}\mathsf{sgn}_{k}^{+}(t)=1\), while others remain unchanged._

Neuron reactivation.According to Theorem 4.8, _all_ of living negative neurons \(k\in\mathcal{K}_{-}\) reactivate _simultaneously_ on \(\bm{x}_{+}\) at \(T_{\mathrm{III}}\): \(\mathsf{sgn}_{k}^{+}(t)\) changes from 0 to 1, while other activation patterns remain unchanged.

Self-Complicating.Along with the reactivation behavior, GF implicitly complicates itself. In Phase III, only living negative neurons \(k\in\mathcal{K}_{+}\) are used to predict on \(\bm{x}_{+}\), i.e., \(f_{+}(t)=\frac{\kappa_{2}}{\sqrt{m}}\sum_{k\in\mathcal{K}_{+}}\sigma(\langle \bm{b}_{k}(t),\bm{x}_{+}\rangle)\). In contrast, after the phase transition at \(T_{\mathrm{III}}\), both living positive and living negative neurons jointly predict on \(\bm{x}_{+}\), i.e. \(f_{+}(t)=\frac{\kappa_{2}}{\sqrt{m}}\sum_{k\in\mathcal{K}_{+}}\sigma(\langle \bm{b}_{k}(t),\bm{x}_{+}\rangle)-\frac{\kappa_{2}}{\sqrt{m}}\sum_{k\in\mathcal{ K}_{-}}\sigma(\langle\bm{b}_{k}(t),\bm{x}_{+}\rangle)\). As indicated in Table 1, two classes of activation patterns are complicated from \((0,0)\) to \((0,1)\), while others do not change.

In this phase, we study the dynamics before activation patterns change again after the phase transition in Theorem 4.8. We define the hitting time:

\[T_{\mathrm{IV}}:=\inf\{t>T_{\mathrm{III}}:\exists k\in\mathcal{K}_{+}\cup \mathcal{K}_{-},\mathsf{sgn}_{k}^{+}(t)\neq\lim\limits_{s\to T_{\mathrm{III}}^{ +}}\mathsf{sgn}_{k}^{+}(s)\text{ or }\mathsf{sgn}_{k}^{-}(t)\neq\lim\limits_{s\to T_{ \mathrm{III}}^{+}}\mathsf{sgn}_{k}^{-}(s)\},\]

and we call \(t\in(T_{\mathrm{III}},T_{\mathrm{IV}}]\) Phase IV.

**Theorem 4.9** (Phase IV).: **(S1)**\(T_{\mathrm{IV}}=+\infty\)_. Moreover, for any \(t>T_{\mathrm{III}}\),_ **(S2)** _all activation patterns do not change;_ **(S3)** _the loss converges with \(\mathcal{L}(\bm{\theta}(t))=\Theta\left(\frac{1}{p^{1-\alpha\cos\Delta}+\kappa_{ 2}^{2}\Delta^{2}(t-T_{\mathrm{III}})}\right)\)._

Theorem 4.9 illustrates that all activation patterns never change again after the phase transition at \(T_{\mathrm{III}}\) with \(\mathsf{sgn}_{k}^{+}(t)=1\), \(\mathsf{sgn}_{k}^{-}(t)=0\) for any \(k\in\mathcal{K}_{+}\) and \(\mathsf{sgn}_{k}^{+}(t)=\mathsf{sgn}_{k}^{-}(t)=1\) for any \(k\in\mathcal{K}_{-}\). Additionally, the loss converges with the polynomial rate \(\Theta(1/\kappa_{2}^{2}\Delta^{2}t)\). Furthermore, we present the following theorem about the convergent direction of each neuron.

**Theorem 4.10** (Directional Convergence).: _The limit \(\lim\limits_{t\rightarrow+\infty}\frac{\bm{\theta}(t)}{\left\|\bm{\theta}(t)\right\| _{2}}\) exists and denoted by \(\overline{\bm{\theta}}=(\overline{b}_{1}^{\top},\cdots,\overline{b}_{m}^{\top}) ^{\top}\in\mathbb{S}^{md-1}\). Moreover, (i) for any \(k\notin\mathcal{K}_{+}\cup\mathcal{K}_{-},\overline{b}_{k}=\bm{0}\); (ii) for any \(k\in\mathcal{K}_{+},\overline{b}_{k}\equiv\bm{v}_{+}=C\big{(}\bm{x}_{+}-\bm{x} _{-}\cos\Delta\big{)}\); (iii) for any \(k\in\mathcal{K}_{-},\overline{b}_{k}\equiv\bm{v}_{-}=C\big{(}\big{(}1+\frac{ \sin^{2}\Delta}{\alpha(1+\cos\Delta)}\big{)}\bm{x}_{-}-\bm{x}_{+}\big{)}\), where \(C>0\) is a scaling constant such that \(\left\|\overline{\bm{\theta}}\right\|_{2}=1\). (iv) Additionally, \(f_{-}(\overline{\bm{\theta}})=-f_{+}(\overline{\bm{\theta}})>0\)._

**Initialization-dependent Directional Convergence.** As an asymptotic result, Theorem 4.10 provides the final convergent direction of GF. All living positive neurons (\(k\in\mathcal{K}_{+}\)) directionally converge to \(\bm{v}_{+}\parallel\bm{x}_{-}^{\perp}\) with \(\langle\bm{v}_{+},\bm{x}_{+}\rangle>0\) and \(\langle\bm{v}_{+},\bm{x}_{-}\rangle=0\), while all living negative neurons (\(k\in\mathcal{K}_{-}\)) directionally converge to \(\bm{v}_{-}\in\mathrm{span}\{\bm{x}_{+},\bm{x}_{-}\}\) with \(\langle\bm{v}_{-},\bm{x}_{+}\rangle>0\) and \(\langle\bm{v}_{-},\bm{x}_{-}\rangle>0\). It is worth noting that \(\bm{v}_{-}\) directly depends not only on the data but also on the ratio \(\alpha=|\mathcal{K}_{-}|/|\mathcal{K}_{+}|\) (defined in Remark 4.2). Recalling the results in Phase I, \(\alpha\) lies in a certain range with high probability; but it is still a random variable due to its dependence on random initialization. Different initializations may lead to different values \(|\mathcal{K}_{-}|/|\mathcal{K}_{+}|\) at the end of Phase I, eventually causing different convergent directions in Phase IV.

**Non-(Local)-Max-Margin Direction.** Lastly, we study the implicit bias of the final convergence rate. According to Lyu and Li (2019); Ji and Telgarsky (2020) and our results above, \(\overline{\bm{\theta}}\) in Theorem 4.10 must be a KKT direction of some max-margin optimization problem. However, it is not clear whether the direction \(\overline{\bm{\theta}}\) is actually an actual optimum of this problem. Surprisingly, in next Theorem, we demonstrate that the final convergent direction is **not even a local optimal** direction of this problem, which enlightens us to rethink the max margin bias of ReLU neural networks.

**Theorem 4.11** (Implicit Bias).: _The final convergent direction \(\overline{\bm{\theta}}\) (in Theorem 4.10) is a KKT direction of the max-margin problem \(\min:\frac{1}{2}\left\|\bm{\theta}\right\|^{2}\ \mathrm{s.t.}\)\(y_{i}f(\bm{x}_{i};\bm{\theta})\geq 1,i\in[n]\). However, \(\overline{\bm{\theta}}\) is not even a local optimal direction of this problem._

Please refer to Appendix F for the proof of Phase IV.

## 5 Discussion and Comparison on Nonlinear Behaviors

Throughout the whole training process in Section 4, we divide the phases based on the evolution of ReLU activation patterns. During Phase I, as well as the beginning of Phase II and III, numerous activation patterns undergo rapid changes. Table 1 summarizes the evolution of activation patterns for all living neurons after Phase I. These results are also numerically validated in Figure 1.

**Simplifying-to-Complicating.** In phase I, GF simplifies all the neurons from random directions into three categories: living positive neurons \(\mathcal{K}_{+}\) and living negative neurons \(\mathcal{K}_{-}\) condense in one direction each, which other neurons are deactivated forever. After Phase I, as shown in Table 1, the two classes of activation patterns change from \((1,0)\stackrel{{\text{simplify}}}{{\rightarrow}}(0,0)\stackrel{{ \text{complicate}}}{{\rightarrow}}(0,1)\), while other patterns remain unchanged. Therefore, the evolution of activation patterns exhibits a simplifying-to-complicating learning trend, which also implies that the network trained by GF learn features in increasing complexity.

**Comparison with NTK.** In the lazy regime such as NTK, most neurons keep close to the initialization and most activation patterns do not change during training. Specifically, for any training data \(\bm{x}_{i}\), \(\frac{1}{m}\sum_{k\in[m]}\mathbb{I}\{\mathsf{sgn}(\langle\bm{b}_{k}(t),\bm{x} _{i}\rangle)\neq\mathsf{sgn}(\langle\bm{b}_{k}(0),\bm{x}_{i}\rangle)\}=o(1), \forall t>0\)(Du et al., 2018). However, our work stands out from lazy regime as activation patterns undergo numerous changes during training. In Phase I, initial condensation causes substantial changes in activation patterns, which is similarly observed in (Phuong and Lampert, 2021). Furthermore, even after Phase I, there are notable modifications in activation patterns. As shown in Table 1, the proportion of changes in

\begin{table}
\begin{tabular}{l|c|c|c|c} \hline \hline  & \(t\in(T_{1},T_{11})\) & \(t\in(T_{11},T_{11}^{\mathrm{PT}})\) & \(t\in(T_{11}^{\mathrm{PT}},T_{11})\) & \(t\in(T_{11},+\infty)\) \\ \hline \(\mathsf{sgn}_{1}^{-}(t)\) (\(k\in\mathcal{K}_{+}\)) & 1 & 1 or 0 & 0 & 0 \\ \hline \(\mathsf{sgn}_{k}^{+}(t)\) (\(k\in\mathcal{K}_{-}\)) & 0 & 0 & 0 & 1 \\ \hline \hline \end{tabular}
\end{table}
Table 1: The evolution of two classes of activation patterns of living neurons after Phase I. As for other two classes, \(\mathsf{sgn}_{k}^{+}(t)\) (\(k\in\mathcal{K}_{+}\)) and \(\mathsf{sgn}_{k}^{-}(t)\) (\(k\in\mathcal{K}_{-}\)), they remain equal to \(1\) after Phase I.

activation patterns for any given training data is the \(\Theta(1)\), as compared with the \(o(1)\) in NTK regime. Specifically, at any \(t>T_{\text{III}}\), \(\frac{1}{m}\sum_{k\in[m]}\mathbb{I}\{\text{sgn}_{k}^{+}(t)\neq\text{sgn}_{k}^{ +}(T_{\text{I}})\}=\frac{1}{m}|\mathcal{K}_{-}|=\Theta(1)\) and \(\frac{1}{m}\sum_{k\in[m]}\mathbb{I}\{\text{sgn}_{k}^{-}(t)\neq\text{sgn}_{k}^ {-}(T_{\text{I}})\}=\frac{1}{m}|\mathcal{K}_{+}|=\Theta(1)\). On the other hand, in our analysis, the requirement on the network's width \(m\) is only \(m=\Omega(\log(1/\delta))\) (Theorem 4.1), regardless of data parameters \(p,\Delta\), while NTK regime requires a much larger width \(m=\Omega(\log(p/\delta)/\Delta^{6})\)(Ji and Telgarsky, 2019).

**Comparison with**Phuong and Lampert (2021); Lyu et al. (2021); Wang and Ma (2022); Boursier et al. (2022). Beyond lazy regime and local analysis, these works also characterize the entire training dynamics and analyze a few nonlinear behaviors. Now we compare our results with these works in detail. (i) While Lyu et al. (2021) focuses on training Leaky ReLU NNs, our work and the other three papers study ReLU NNs. It is worth noting that the dynamics of Leaky ReLU NNs differ from ReLU due to the permanent activation of Leaky ReLU (\(\sigma^{\prime}(\cdot)\geq\alpha>0\)). (ii) Initial condensation is also proven in (Lyu et al., 2021; Boursier et al., 2022), and the condensation directions are some types of data averages. In our work, neurons can aggregate towards not only the average direction \(\bm{\mu}\), but also another direction \(\bm{x}_{+}^{\perp}\). Moreover, we also estimate the number of neurons that condense into two directions. (iii) Saddle-to-saddle dynamics are proven in (Phuong and Lampert, 2021) for square loss, where the second saddle is about training loss and caused by incomplete fitting. However, our work focus on classification with exponential loss and exhibit a similar saddle-to-plateau dynamics, where the plateau is about training accuracy, caused by incomplete feature learning. (iv) Phuong and Lampert, 2021; Wang and Ma, 2022), all features can be rapidly learned in Phase I (accuracy\(=100\%\)), followed by lazy training. However, for practical tasks on more complex data, NNs can hardly learn all features in such short time. In our work, the data is more difficult to learn, resulting in incomplete feature learning in Phase I (accuracy\(<100\%\)). Subsequently, NNs experience a long time to learn other features completely. Such multi-phase feature leaning dynamics are closer to practical training process. (v) Neuron reactivation and deactivation. For ReLU NNs, The evolution of activation patterns is one of the essential causes of nonlinear dynamics. In (Phuong and Lampert, 2021; Wang and Ma, 2022), activation patterns only change rapidly in Phase I, after which they remain unchanged. In (Boursier et al., 2022), their lemma 6 shows that their dynamics lack neuron reactivation. However, in our dynamics, even after Phase I, our dynamics exhibit significant neuron deactivation and reactivation as discussed in Table 1. (vi) The final convergent directions are also derived in (Phuong and Lampert, 2021; Lyu et al., 2021; Boursier et al., 2022), which only depend on the data. However, in our setting, the convergent direction is more complicated, determined by both data and random initialization. (vii) Furthermore, our four-phase dynamics demonstrate the whole evolution of activation patterns during training and reveal a general simplifying-to-complicating learning trend.

In summary, our whole four-phase optimization dynamics capture more nonlinear behaviors than these works. Furthermore, we conduct a more thorough and detailed theoretical analysis of these nonlinear behaviors, providing a more systematic and comprehensive understanding.

## 6 Conclusion and Future Work

In this work, we study the optimization dynamics of ReLU neural networks trained by GF on a linearly separable data. Our analysis captures the whole optimization process starting from random initialization to final convergence. Throughout the whole training process, we reveal four different phases and identify rich nonlinear behaviors theoretically. However, theoretical understanding of the training of NNs still has a long way to go. For instance, although we conduct a fine-grained analysis of GF, the dynamics of GD are more complex and exhibit other nonlinear behaviors such as progressive sharpening and edge of stability (Wu et al., 2018; Jastrzebski et al., 2019; Cohen et al., 2021; Ma et al., 2022; Li et al., 2022; Damian et al., 2022; Zhu et al., 2022; Ahn et al., 2022; Abh et al., 2022; Abh et al., 2022; Abh et al., 2022; Abh et al., 2022; Feng and Tu, 2021; Liu et al., 2021; Ziyin et al., 2022; Wu et al., 2022; Wojtowytsch, 2023; Wang and Wu, 2023) in each iteration, which can have a pronounced impact on the optimization dynamics and nonlinear behaviors. Better understanding of the nonlinear behaviors during GD or SGD training is an important direction of future work.

## Acknowledgments and Disclosure of Funding

We thank Prof. Weinan E, Prof. Lei Wu, Prof. Zhi-Qin John Xu and anonymous reviewers for helpful suggestions. Mingze Wang is supported in part by the National Key Basic Research Program of China: 2015CB856000.

## References

* Abbe et al. (2022) Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks. In _Conference on Learning Theory_, pages 4782-4887. PMLR, 2022a.
* Abbe et al. (2022b) Emmanuel Abbe, Elisabetta Cornacchia, Jan Hazla, and Christopher Marquis. An initial alignment between neural network and target is needed for gradient descent to learn. In _International Conference on Machine Learning_, pages 33-52. PMLR, 2022b.
* Abbe et al. (2023) Emmanuel Abbe, Enric Boix-Adsera, and Theodor Misiakiewicz. Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics. _arXiv preprint arXiv:2302.11055_, 2023.
* Ahn et al. (2022a) Kwangjun Ahn, Sebastien Bubeck, Sinho Chewi, Yin Tat Lee, Felipe Suarez, and Yi Zhang. Learning threshold neurons via the" edge of stability". _arXiv preprint arXiv:2212.07469_, 2022a.
* Ahn et al. (2022b) Kwangjun Ahn, Jingzhao Zhang, and Suvrit Sra. Understanding the unstable convergence of gradient descent. In _International Conference on Machine Learning_, pages 247-257. PMLR, 2022b.
* Allen-Zhu et al. (2019) Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In _International Conference on Machine Learning_, pages 242-252. PMLR, 2019.
* Arora et al. (2019) Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In _International Conference on Machine Learning_, pages 322-332. PMLR, 2019.
* Arpit et al. (2017) Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In _International conference on machine learning_, pages 233-242. PMLR, 2017.
* Atanasov et al. (2022) Alexander Atanasov, Blake Bordelon, and Cengiz Pehlevan. Neural networks as kernel learners: The silent alignment effect. _International Conference on Learning Representations_, 2022.
* Blanc et al. (2020) Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant. Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process. In _Conference on learning theory_, pages 483-513. PMLR, 2020.
* Bolte et al. (2010) Jerome Bolte, Aris Daniilidis, Olivier Ley, and Laurent Mazet. Characterizations of lojasiewicz inequalities: subgradient flows, talweg, convexity. _Transactions of the American Mathematical Society_, 362(6):3319-3363, 2010.
* Boursier et al. (2022) Etienne Boursier, Loucas Pillaud-Vivien, and Nicolas Flammarion. Gradient flow dynamics of shallow relu networks for square loss and orthogonal inputs. _Advances in Neural Information Processing Systems_, 2022.
* Brutzkus et al. (2017) Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns over-parameterized networks that provably generalize on linearly separable data. _arXiv preprint arXiv:1710.10174_, 2017.
* Chatterji et al. (2021) Niladri S Chatterji, Philip M Long, and Peter L Bartlett. When does gradient descent with logistic loss find interpolating two-layer networks? _Journal of Machine Learning Research_, 22(159):1-48, 2021.
* Chen et al. (2023) Zhengan Chen, Yuqing Li, Tao Luo, Zhangchen Zhou, and Zhi-Qin John Xu. Phase diagram of initial condensation for two-layer neural networks. _arXiv preprint arXiv:2303.06561_, 2023.
* Chen et al. (2020)* Chizat and Bach (2018) Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. _Advances in neural information processing systems_, 31, 2018.
* Chizat and Bach (2020) Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss. In _Conference on Learning Theory_, pages 1305-1338. PMLR, 2020.
* Clarke et al. (2008) Francis H Clarke, Yuri S Ledyaev, Ronald J Stern, and Peter R Wolenski. _Nonsmooth analysis and control theory_, volume 178. Springer Science & Business Media, 2008.
* Cohen et al. (2021) Jeremy M Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. Gradient descent on neural networks typically occurs at the edge of stability. _arXiv preprint arXiv:2103.00065_, 2021.
* Damian et al. (2022) Alex Damian, Eshaan Nichani, and Jason D Lee. Self-stabilization: The implicit bias of gradient descent at the edge of stability. _arXiv preprint arXiv:2209.15594_, 2022.
* Du et al. (2019) Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In _International Conference on Machine Learning_, pages 1675-1685. PMLR, 2019.
* Du et al. (2018) Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. _arXiv preprint arXiv:1810.02054_, 2018.
* Dutta et al. (2013) Joydeep Dutta, Kalyanmoy Deb, Rupesh Tulshyan, and Ramnik Arora. Approximate kkt points and a proximity measure for termination. _Journal of Global Optimization_, 56(4):1463-1499, 2013.
* Fang et al. (2021) Cong Fang, Hangfeng He, Qi Long, and Weijie J Su. Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training. _Proceedings of the National Academy of Sciences_, 118(43):e2103091118, 2021.
* Feng and Tu (2021) Yu Feng and Yuhai Tu. The inverse variance-flatness relation in stochastic gradient descent is critical for finding flat minima. _Proceedings of the National Academy of Sciences_, 118(9), 2021.
* Filippov (2013) Aleksei Fedorovich Filippov. _Differential equations with discontinuous righthand sides: control systems_, volume 18. Springer Science & Business Media, 2013.
* Han et al. (2021) XY Han, Vardan Papyan, and David L Donoho. Neural collapse under mse loss: Proximity to and dynamics on the central path. _arXiv preprint arXiv:2106.02073_, 2021.
* Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jurgen Schmidhuber. Flat minima. _Neural computation_, 9(1):1-42, 1997.
* Jacot et al. (2018) Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. _arXiv preprint arXiv:1806.07572_, 2018.
* Jacot et al. (2021) Arthur Jacot, Francois Ged, Berlin Simsek, Clement Hongler, and Franck Gabriel. Saddle-to-saddle dynamics in deep linear networks: Small initialization training, symmetry, and sparsity. _arXiv preprint arXiv:2106.15933_, 2021.
* Jastrzebski et al. (2019) Stanislaw Jastrzebski, Zachary Kenton, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey. On the relation between the sharpest directions of dnn loss and the sgd step length. _International Conference on Learning Representations_, 2019.
* Ji and Telgarsky (2019) Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow relu networks. _arXiv preprint arXiv:1909.12292_, 2019.
* Ji and Telgarsky (2020) Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. _Advances in Neural Information Processing Systems_, 33:17176-17186, 2020.
* Keskar et al. (2016) Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In _International Conference on Learning Representations_, 2016.
* Kunin et al. (2023) Daniel Kunin, Atsushi Yamamura, Chao Ma, and Surya Ganguli. The asymmetric maximum margin bias of quasi-homogeneous neural networks. _International Conference on Learning Representations_, 2023.
* Krizhevsky et al. (2014)* Li et al. (2017) Qianxiao Li, Cheng Tai, and Weinan E. Stochastic modified equations and adaptive stochastic gradient algorithms. In _International Conference on Machine Learning_, pages 2101-2110. PMLR, 2017.
* Li et al. (2019) Qianxiao Li, Cheng Tai, and Weinan E. Stochastic modified equations and dynamics of stochastic gradient algorithms i: Mathematical foundations. _The Journal of Machine Learning Research_, 20(1):1474-1520, 2019.
* Li et al. (2021) Zhiyuan Li, Tianhao Wang, and Sanjeev Arora. What happens after sgd reaches zero loss?-a mathematical framework. _arXiv preprint arXiv:2110.06914_, 2021.
* Li et al. (2022) Zhouzi Li, Zixuan Wang, and Jian Li. Analyzing sharpness along gd trajectory: Progressive sharpening and edge of stability. _arXiv preprint arXiv:2207.12678_, 2022.
* Liu et al. (2021) Kangqiao Liu, Liu Ziyin, and Masahito Ueda. Noise and fluctuation of finite learning rate stochastic gradient descent. In _International Conference on Machine Learning_, pages 7045-7056. PMLR, 2021.
* Luo et al. (2021) Tao Luo, Zhi-Qin John Xu, Zheng Ma, and Yaoyu Zhang. Phase diagram for two-layer relu neural networks at infinite-width limit. _The Journal of Machine Learning Research_, 22(1):3327-3373, 2021.
* Lyu and Li (2019) Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. _arXiv preprint arXiv:1906.05890_, 2019.
* Lyu et al. (2021) Kaifeng Lyu, Zhiyuan Li, Runzhe Wang, and Sanjeev Arora. Gradient descent on two-layer nets: Margin maximization and simplicity bias. _Advances in Neural Information Processing Systems_, 34, 2021.
* Ma and Ying (2021) Chao Ma and Lexing Ying. On linear stability of sgd and input-smoothness of neural networks. _Advances in Neural Information Processing Systems_, 34:16805-16817, 2021.
* Ma et al. (2022) Chao Ma, Daniel Kunin, Lei Wu, and Lexing Ying. Beyond the quadratic approximation: the multiscale structure of neural network loss landscapes. _arXiv preprint arXiv:2204.11326_, 2022.
* Maennel et al. (2018) Hartmut Maennel, Olivier Bousquet, and Sylvain Gelly. Gradient descent quantizes relu network features. _arXiv preprint arXiv:1803.08367_, 2018.
* Mei et al. (2019) Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit. In _Conference on Learning Theory_, pages 2388-2464. PMLR, 2019.
* Mulayoff et al. (2021) Rotem Mulayoff, Tomer Michaeli, and Daniel Soudry. The implicit bias of minima stability: A view from function space. _Advances in Neural Information Processing Systems_, 34:17749-17761, 2021.
* Nacson et al. (2019) Mor Shpigel Nacson, Suriya Gunasekar, Jason Lee, Nathan Srebro, and Daniel Soudry. Lexicographic and depth-sensitive margins in homogeneous and non-homogeneous deep models. In _International Conference on Machine Learning_, pages 4683-4692. PMLR, 2019.
* Nakkiran et al. (2019) Preetum Nakkiran, Gal Kaplun, Dimitris Kalimeris, Tristan Yang, Benjamin L Edelman, Fred Zhang, and Boaz Barak. Sgd on neural networks learns functions of increasing complexity. _arXiv preprint arXiv:1905.11604_, 2019.
* Papyan et al. (2020) Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the terminal phase of deep learning training. _Proceedings of the National Academy of Sciences_, 117(40):24652-24663, 2020.
* Pesme and Flammarion (2023) Scott Pesme and Nicolas Flammarion. Saddle-to-saddle dynamics in diagonal linear networks. _arXiv preprint arXiv:2304.00488_, 2023.
* Phuong and Lampert (2021) Mary Phuong and Christoph H Lampert. The inductive bias of relu networks on orthogonally separable data. In _International Conference on Learning Representations_, 2021.
* Pogogor et al. (2019)Aristide Rahaman, Nasim xd Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio, and Aaron Courville. On the spectral bias of neural networks. In _International Conference on Machine Learning_, pages 5301-5310. PMLR, 2019.
* Safran et al. (2022) Itay Safran, Gal Vardi, and Jason D Lee. On the effective number of linear regions in shallow univariate relu networks: Convergence guarantees and implicit bias. _Advances in Neural Information Processing Systems_, 35:32667-32679, 2022.
* Saxe et al. (2022) Andrew Saxe, Shagun Sodhani, and Sam Jay Lewallen. The neural race reduction: Dynamics of abstraction in gated networks. In _International Conference on Machine Learning_, pages 19287-19309. PMLR, 2022.
* Soudry et al. (2018) Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. _The Journal of Machine Learning Research_, 19(1):2822-2878, 2018.
* Thomas et al. (2020) Valentin Thomas, Fabian Pedregosa, Bart Merrienboer, Pierre-Antoine Manzagol, Yoshua Bengio, and Nicolas Le Roux. On the interplay between noise and curvature and its effect on optimization and generalization. In _International Conference on Artificial Intelligence and Statistics_, pages 3503-3513. PMLR, 2020.
* Vardi (2023) Gal Vardi. On the implicit bias in deep-learning algorithms. _Communications of the ACM_, 66(6):86-93, 2023.
* Vardi et al. (2022) Gal Vardi, Ohad Shamir, and Nati Srebro. On margin maximization in linear and relu networks. _Advances in Neural Information Processing Systems_, 35:37024-37036, 2022.
* Wang and Ma (2022) Mingze Wang and Chao Ma. Early stage convergence and global convergence of training mildly parameterized neural networks. _Advances in Neural Information Processing Systems_, 2022.
* Wang and Wu (2023) Mingze Wang and Lei Wu. The noise geometry of stochastic gradient descent: A quantitative and analytical characterization. _arXiv preprint arXiv:2310.00692_, 2023.
* Wojtowytsch (2023) Stephan Wojtowytsch. Stochastic gradient descent with noise of machine learning type part i: Discrete time analysis. _Journal of Nonlinear Science_, 33(3):45, 2023.
* Woodworth et al. (2020) Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In _Conference on Learning Theory_, pages 3635-3673. PMLR, 2020.
* Wu and Su (2023) Lei Wu and Weijie J Su. The implicit regularization of dynamical stability in stochastic gradient descent. In _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 37656-37684. PMLR, 2023.
* Wu et al. (2018) Lei Wu, Chao Ma, and Weinan E. How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective. _Advances in Neural Information Processing Systems_, 31:8279-8288, 2018.
* Wu et al. (2022) Lei Wu, Mingze Wang, and Weijie Su. When does sgd favor flat minima? a quantitative characterization via linear stability. _Advances in Neural Information Processing Systems_, 2022.
* Xu et al. (2019) Zhi-Qin John Xu, Yaoyu Zhang, Tao Luo, Yanyang Xiao, and Zheng Ma. Frequency principle: Fourier analysis sheds light on deep neural networks. _arXiv preprint arXiv:1901.06523_, 2019.
* Zhang et al. (2022) Yaoyu Zhang, Yuqing Li, Zhongwang Zhang, Tao Luo, and Zhi-Qin John Xu. Embedding principle: A hierarchical structure of loss landscape of deep neural networks. _Journal of Machine Learning_, 1(1):60-113, 2022. ISSN 2790-2048.
* Zhou et al. (2022a) Hanxu Zhou, Zhou Qixuan, Zhenyuan Jin, Tao Luo, Yaoyu Zhang, and Zhi-Qin Xu. Empirical phase diagram for three-layer neural networks with infinite width. _Advances in Neural Information Processing Systems_, 35:26021-26033, 2022a.
* Zhou et al. (2022b) Hanxu Zhou, Zhou Qixuan, Tao Luo, Yaoyu Zhang, and Zhi-Qin Xu. Towards understanding the condensation of neural networks at initial training. _Advances in Neural Information Processing Systems_, 35:2184-2196, 2022b.

Xingyu Zhu, Zixuan Wang, Xiang Wang, Mo Zhou, and Rong Ge. Understanding edge-of-stability training dynamics with a minimalist example. _arXiv preprint arXiv:2210.03294_, 2022.
* Zhu et al. (2019) Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects. In _International Conference on Machine Learning_, pages 7654-7663. PMLR, 2019.
* Zhu et al. (2021) Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. A geometric analysis of neural collapse with unconstrained features. _Advances in Neural Information Processing Systems_, 34:29820-29834, 2021.
* Ziyin et al. (2022) Liu Ziyin, Kangqiao Liu, Takashi Mori, and Masahito Ueda. Strength of minibatch noise in SGD. In _International Conference on Learning Representations_, 2022.
* Zou et al. (2018) Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes over-parameterized deep relu networks. _arXiv preprint arXiv:1811.08888_, 2018.

**Appendix**

###### Contents

* A Experimental Details
* B Proof Preparation
* C Proofs of Optimization Dynamics in Phase I
* D Proofs of Optimization Dynamics in Phase II
* E Proofs of Optimization Dynamics in Phase III
* F Proofs of Optimization Dynamics in Phase IV
* G Clarke Subdifferential and KKT Conditions for Non-smooth Optimization
* H Solution of Discontinuous System
* I Some Basic InequalitiesExperimental Details

All experiments are conducted on a MacBook pro 13 (M2) only using CPU. See the code at https://github.com/wmz9/Understanding_Multi-phase_Optimization_NeurIPS2023.

### Experiments on standard Dataset

We train the two-layer network on the dataset that satisfies Assumption 3.1 with \(d=20\), \(p=4\) and \(\Delta=\pi/15\). Specifically, we choose the network width \(m=100\); the initialization scale \(\kappa_{1}=0.1,\kappa_{2}=1\); the small learning rate \(\eta=0.01\).

In Figure 3, we show some key data directions in this dataset, as well as the training accuracy, which contains a long plateau. Furthermore, in Figure 4, we provide the evolution of all neurons during training from \(t=0\) to \(t=150000\), which is a more complete version of Figure 1.

Figure 4: (A more complete version of Figure 1) These figures visualize (in polar coordinates) the projections of all neurons \(\{\bm{b}_{k}(t)\}_{k\in[m]}\) onto the \(2\)d subspace \(\mathrm{span}\{\bm{x}_{+},\bm{x}_{-}\}\) during training (from \(t=0\) to \(t=150000\)). Each purple star represents a positive neuron \((k\in[m/2])\), while each brown star represents a negative neuron \((k\in[m]-[m/2])\).

Figure 3: (left) Some key data directions: the directions of \(\bm{x}_{+},\bm{x}_{-},\bm{x}_{+}^{\perp},\bm{x}_{-}^{\perp},\bm{\mu}\) are plotted in blue, orange, green, red and pink colors, respectively; (right) The training accuracy.

### Numerical validation on our theoretical bounds

We conduct experiments to validate our theoretical bounds on \(T_{\mathrm{plat}}\) and \(T_{\mathrm{III}}\) under different \(p\) and \(\Delta\), and the results are shown in 2.

In the first experiment (1st and 2nd subtable), we fix \(p=4\) for change \(\Delta\); in the second experiment (3rd and 4th subtable), we fix \(\Delta=\pi/15\) and change \(p\). As for other hyperparameters (such as \(\kappa_{1},\kappa_{2},d,m\)), we keep the same scales as our setups in Appendix A.1.

We have two main conclusions: (1) four training phases in our theory persistently exist; (the same as Fig 1 in Appendix A, and be omitted due to the limited space) (2) our theoretical estimates on \(T_{\mathrm{plat}}\) and \(T_{\mathrm{III}}\) are relatively tight, basically in the same magnitude as the realistic time.

### Experiments on Noisy Dataset

We conduct numerical experiments on the setting of adding small stochastic noise on top of \(\bm{x}_{+}\) and \(\bm{x}_{-}\), a little bit more realistic setting. Specifically, in \(\mathrm{span}\{\bm{x}_{+},\bm{x}_{-}\}\), we perturb the angles of \(n_{+}-1\) instances of \(\bm{x}_{+}\) and \(n_{-}-1\) instances of \(\bm{x}_{-}\) using stochastic noise \(\xi\sim\mathrm{Unif}([0,\Delta/4])\).

In Figure 5, we visualize (i) the evolution of each neuron throughout the training process; (ii) some key data directions; (iii) the evolution of training accuracy.

From the numerical results in Figure 5, we have two main conclusions: (1) we ascertain that the same four-phase optimization dynamics and nonlinear behaviors persist, even for our dataset with small stochastic noise; (2) a slight difference is that there is more than one plateau of training accuracy in Phase II. The reason is that for noisy data, GF needs to learn negative data one by one in Phase II. For example, three distinct negative data are employed in this experiment, so three plateaus of training accuracy emerge (\(12/15\), \(13/15\), and \(14/15\)).

Figure 5: The experimental results for noisy data with \(\Delta=\pi/15\), \(n_{+}=12\), \(n_{-}=3\). Fig (a): Some key data directions, including \(\bm{x}_{+}\), \(\bm{x}_{+}\)+noise, \(\bm{x}_{-}\), \(\bm{x}_{+}\)+noise, \(\bm{\mu}\), \(\bm{x}_{+}^{\perp}\), and \(\bm{x}_{-}^{\perp}\). Fig (b): The evolution of training accuracy. Figs (c)-(j): the evolution of the projections of all neurons \(\{\bm{b}_{k}(t)\}_{k\in[m]}\) onto the \(2\)d subspace \(\mathrm{span}\{\bm{x}_{+},\bm{x}_{-}\}\) during training (from \(t=0\) to \(t=400000\)). Each purple star represents a positive neuron (\(k\in[m/2]\)), while each brown star represents a negative neuron (\(k\in[m]-[m/2]\)). **Four-phase dynamics**: from Fig (c) to (d) is Phase I; from Fig (d) to (g) is Phase II; from Fig (g) to (h) is Phase III; from Fig (h) to (j) is Phase IV. To compare these results with noiseless data, please refer to Figures 3 and 4.

Proof Preparation

**Selection of initialization parameters.**

For the data satisfying Assumption 3.1, we consider the regime that \(\Delta\ll 1\) with \(p\cos\Delta>1\). During the entire proof, we select the initialization scale \(\kappa_{1},\kappa_{2}\) as follows:

\[\kappa_{2}=\mathcal{O}(1),\quad\frac{\kappa_{1}}{\kappa_{2}}=\mathcal{O}\left( \Delta^{8}\right).\] (3)

**Gradient Flow.** In general, for any \(k\in[m]\), the GF dynamics of \(\bm{b}_{k}(t)\) can be written as

\[\begin{split}&\frac{\mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t}\in \frac{\partial^{\circ}\mathcal{L}(\bm{\theta})}{\partial\bm{b}_{k}}=\frac{ \mathrm{s}_{k}\kappa_{2}}{\sqrt{m}}\frac{1}{n}\sum_{i=1}^{n}e^{-y_{i}f_{i}(t)} \partial^{\circ}\sigma(\langle\bm{b}_{k}(t),\bm{x}_{i}\rangle)y_{i}\bm{x}_{i} \\ =&\frac{\mathrm{s}_{k}\kappa_{2}}{\sqrt{m}}\Big{(} \frac{p}{1+p}e^{-f_{+}(t)}\partial^{\circ}\sigma(\langle\bm{b}_{k}(t),\bm{x}_ {+}\rangle)\bm{x}_{+}-\frac{1}{1+p}e^{f_{-}(t)}\partial^{\circ}\sigma(\langle \bm{b}_{k}(t),\bm{x}_{-}\rangle)\bm{x}_{-}\Big{)},\end{split}\] (4)

where \(\partial^{\circ}\) is Clarke's subdifferential defined in Definition G.1. Notice that if \(\mathcal{L}\) is continuously differentiable at \(\bm{\theta}\), then \(\partial^{\circ}\mathcal{L}(\bm{\theta})=\{\nabla\mathcal{L}(\bm{\theta})\}\) is unique.

However, for discontinuous differentiable points of \(\mathcal{L}\), the differential inclusion flow \(\frac{\mathrm{d}\bm{\theta}}{\mathrm{d}t}\in\partial^{\circ}\mathcal{L}(\bm{ \theta})\) may not be unique. To study a more specific dynamics, we also utilize Definition H.1 to determine GF at some of such points, which overcomes non-uniqueness of GF trajectories to some extent. It is worth noting that Definition H.1 and Definition G.1 are compatible and specifically, the dynamics defined in Definition H.1(Case I, III) lie in the convex hull defined in Definition G.1.

**Remark B.1**.: In (Lyu et al., 2021), the non-branching starting point Assumption is employed to address the technical challenge of non-uniqueness in GF trajectories. By comparison, in this work, we do not need this assumption. We adopt Definition H.1 to uniquely determine the Gradient Flow trajectories theoretically near some discontinuous differential regions, such as "Ridge", "Valley", and "Refraction edge" discussed in Section I.2 in (Lyu et al., 2021).

Additionally, in the following sections, we may rewrite this dynamics accordingly, such as specific forms and dynamics decomposition.

**Additional Notations.** As a similar description to \(\text{sgn}_{k}^{+}(\cdot)\) and \(\text{sgn}_{k}^{-}(\cdot)\), we also employ the following six manifolds to characterize activation patterns (by judging which manifold the neuron \(\bm{w}_{k}\) belongs to):

\[\begin{split}\mathcal{M}_{+}^{+}&:=\{\bm{w}\in \mathbb{S}^{d-1}:\langle\bm{w},\bm{x}_{+}\rangle>0\},\quad\mathcal{M}_{-}^{+} :=\{\bm{w}\in\mathbb{S}^{d-1}:\langle\bm{w},\bm{x}_{-}\rangle>0\},\\ \mathcal{M}_{+}^{0}&:=\{\bm{w}\in\mathbb{S}^{d-1}: \langle\bm{w},\bm{x}_{+}\rangle=0\},\quad\mathcal{M}_{-}^{0}:=\{\bm{w}\in \mathbb{S}^{d-1}:\langle\bm{w},\bm{x}_{-}\rangle=0\},\\ \mathcal{M}_{+}^{-}&:=\{\bm{w}\in\mathbb{S}^{d-1}: \langle\bm{w},\bm{x}_{+}\rangle\leq 0\},\quad\mathcal{M}_{-}^{-}:=\{\bm{w}\in \mathbb{S}^{d-1}:\langle\bm{w},\bm{x}_{-}\rangle\leq 0\}.\end{split}\]

As one of our interested manifolds, the border \(\partial(\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{+})\) can be divided into

\[\begin{split}\partial(\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{+} )=&\{\bm{w}\in\mathbb{S}^{d-1}:\langle\bm{w},\bm{x}_{+}\rangle=0 \text{ or }\langle\bm{w},\bm{x}_{-}\rangle=0\}\\ =&\{\bm{w}\in\mathbb{S}^{d-1}:\langle\bm{w},\bm{x}_ {+}\rangle=0,\langle\bm{w},\bm{x}_{-}\rangle>0\}\bigcup\{\bm{w}\in\mathbb{S}^{d -1}:\langle\bm{w},\bm{x}_{+}\rangle>0,\langle\bm{w},\bm{x}_{-}\rangle=0\}\\ &\quad\bigcup\{\bm{w}\in\mathbb{S}^{d-1}:\langle\bm{w},\bm{x}_ {+}\rangle=0,\langle\bm{w},\bm{x}_{-}\rangle=0\}\\ =&\big{(}\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^{+} \big{)}\cup\big{(}\mathcal{M}_{-}^{0}\cap\mathcal{M}_{+}^{+}\big{)}\cup\big{(} \mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^{0}\big{)}.\end{split}\]

Furthermore, we also utilize the following notations:

\[\begin{split}\mathcal{P}_{+}^{+}&:=\{\bm{b}\in \mathbb{R}^{d}:\langle\bm{b},\bm{x}_{+}\rangle>0\},\quad\mathcal{P}_{-}^{+}:=\{ \bm{b}\in\mathbb{R}^{d}:\langle\bm{b},\bm{x}_{-}\rangle>0\},\\ \mathcal{P}_{+}^{0}&:=\{\bm{b}\in\mathbb{R}^{d}: \langle\bm{b},\bm{x}_{+}\rangle=0\},\quad\mathcal{P}_{-}^{0}:=\{\bm{b}\in \mathbb{R}^{d-1}:\langle\bm{b},\bm{x}_{-}\rangle=0\},\\ \mathcal{P}_{-}^{-}&:=\{\bm{b}\in\mathbb{R}^{d}: \langle\bm{b},\bm{x}_{+}\rangle\leq 0\},\quad\mathcal{P}_{-}^{-}:=\{\bm{b}\in \mathbb{R}^{d}:\langle\bm{b},\bm{x}_{-}\rangle\leq 0\}.\end{split}\]

Notice that for the direction of a neuron \(\bm{b}_{k}\neq\bm{0}\), using \(\mathcal{P}_{+}^{+},\mathcal{P}_{+}^{0},\mathcal{P}_{+}^{+},\mathcal{P}_{-}^{+}, \mathcal{P}_{-}^{0},\mathcal{P}_{-}^{-}\) to describe \(\bm{b}_{k}\) is equivalent to using \(\mathcal{M}_{+}^{+},\mathcal{M}_{+}^{0},\mathcal{M}_{+}^{-},\mathcal{M}_{-}^{+}, \mathcal{M}_{-}^{0},\mathcal{M}_{-}^{-}\) to describe \(\bm{w}_{k}\).

**Lemma B.2** (Dead neurons keep dead).: _For the \(k\)-th neuron, if there exists a \(t_{0}\geq 0\), s.t. \(\bm{w}_{k}(t_{0})\in\mathcal{M}_{+}^{-}\cap\mathcal{M}_{-}^{-}\), then it dies and remains unchanged during the remaining training: \(\bm{w}_{k}(t)\in\mathcal{M}_{+}^{-}\cap\mathcal{M}_{-}^{-}\) and \(\bm{b}_{k}(t)\equiv\bm{b}_{k}(t_{0})\) for any \(t\geq t_{0}\)._

Proof of Lemma b.2.: A straightforward calculation. 

The above fact is a basic fact in our setting, which illustrates that if the neuron \(\bm{b}_{k}\) is deactivated for both data \(\bm{x}_{+}\) and \(\bm{x}_{-}\) at some time, then it remains "dead" forever.

It is worth mentioning that if the neuron \(\bm{b}_{k}\) is deactivated on \(\bm{x}_{+}\) but activated on \(\bm{x}_{-}\) at some time, it can still reactivate on \(\bm{x}_{+}\) later, which is one of the important reasons why our ReLU optimization dynamics are complicated.

Proofs of Optimization Dynamics in Phase I

In this section, we conduct a detailed analysis of the training dynamics of each neuron in Phase I. The main proof idea is to decompose neurons' dynamics into tangential and radial dynamics. For small initialization, in Phase I, the radial increasing of neurons is much slower than their tangential velocity, which can result in condensation. However, the main challenges arise from the initial direction's randomness and ReLU's discontinuous derivative, leading to eight categories of neuron dynamics. Moreover, it is also nontrivial and requires meticulous analysis to estimate the number of two classes of living neurons at the end of Phase I.

We define the time

\[T_{\text{I}}:=10\sqrt{\frac{\kappa_{1}}{\kappa_{2}}},\] (5)

and call \(t\in[0,T_{\text{I}}]\) "Phase I".

Recall that the selection (3) about initialization parameters can guarantee the **whole** four-phase optimization dynamics. Nevertheless, when we focus on Phase I, \(\kappa_{2}=\mathcal{O}(1)\) and \(\kappa_{1}/\kappa_{2}=\mathcal{O}(1)\) suffice to ensure the dynamics in Phase I. Specifically, during the proof of Phase I, we can use the following selection (6) on \(\kappa_{1},\kappa_{2}\), which is much weaker than (3):

\[\kappa_{2}=\mathcal{O}(1),\quad\frac{\kappa_{1}}{\kappa_{2}}=\mathcal{O}(1).\] (6)

For simplicity, we assume \(\Delta\leq\frac{1}{\bar{\bar{\bar{\bar{\bar{\bar{\bar{\bar{\bar{\bar{\bar{\bar{ \bar{\bar{\bar{\bar{\bar{\bar{\bar{\barbarbar{        }}}}}}}}}}}}}}}}\). And for convenience, we assume \(p\geq 5\). It is worth mentioning that our proof approach also applies for \(p=2,3,4\), with at most one absolute constant difference.

Prepared for the analysis in Phase I, we decompose the dynamics of \(\bm{b}_{k}(t)\) into the radial movement \(\rho_{k}(t)\in\mathbb{R}\) and the tangential movement \(\bm{w}_{k}(t)\in\mathbb{S}^{d-1}\) satisfied to \(\bm{b}_{k}(t)=\rho_{k}(t)\bm{w}_{k}(t)\).

**Lemma C.1** (Dynamics decomposition).: _For any \(k\in[m]\), the dynamics of \(\bm{b}_{k}(t)\) can be decomposed into the radial movement \(\rho_{k}(t)\in\mathbb{R}\) and the tangential movement \(\bm{w}_{k}(t)\in\mathbb{S}^{d-1}\):_

\[\frac{\mathrm{d}\bm{w}_{k}(t)}{\mathrm{d}t}\in\frac{\mathrm{s}_{k}\kappa_{2}}{ \sqrt{m}\rho_{k}(t)}\Big{(}\bm{F}_{k}(t)-\left\langle\bm{F}_{k}(t),\bm{w}_{k}( t)\right\rangle\bm{w}_{k}(t)\Big{)},\] (7)

_where \(\rho_{k}(t)=\|\bm{b}_{k}(t)\|\), \(\bm{w}_{k}(t)=\bm{b}_{k}(t)/\left\|\bm{b}_{k}(t)\right\|\) and_

\[\bm{F}_{k}(t)=\frac{1}{n}\sum_{i=1}^{n}e^{-y_{i}f_{i}(t)}\partial^{\circ} \sigma\left(\left\langle\bm{w}_{k}(t),\bm{x}_{i}\right\rangle\right)y_{i}\bm{ x}_{i}.\] (8)

Proof of Lemma C.1.: From the dynamics of \(\bm{b}_{k}(t)\):

\[\frac{\mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t}\in\frac{\mathrm{s}_{k}\kappa_{2}}{ \sqrt{m}}\bm{F}_{k}(t)=\frac{\mathrm{s}_{k}\kappa_{2}}{\sqrt{m}}\frac{1}{n} \sum_{i=1}^{n}e^{-y_{i}f_{i}(t)}\partial^{\circ}\sigma\left(\left\langle\bm{w }_{k}(t),\bm{x}_{i}\right\rangle\right)y_{i}\bm{x}_{i},\]

we have

\[\frac{\mathrm{d}\rho_{k}(t)}{\mathrm{d}t}=\frac{1}{2\left\|\bm{b} _{k}(t)\right\|}\frac{\mathrm{d}\left\|\bm{b}_{k}(t)\right\|^{2}}{\mathrm{d}t} =\frac{1}{\left\|\bm{b}_{k}(t)\right\|}\left\langle\bm{b}_{k}(t),\frac{ \mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t}\right\rangle\] \[\in \frac{1}{\left\|\bm{b}_{k}(t)\right\|}\left\langle\bm{b}_{k}(t), \frac{\mathrm{s}_{k}\kappa_{2}}{\sqrt{m}}\bm{F}_{k}(t)\right\rangle=\frac{ \mathrm{s}_{k}\kappa_{2}}{\sqrt{m}}\left\langle\bm{F}_{k}(t),\bm{w}_{k}(t) \right\rangle,\]

\[\frac{\mathrm{d}\bm{w}_{k}(t)}{\mathrm{d}t}=\frac{\left\|\bm{b}_{k}(t)\right\| \frac{\mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t}-\frac{\mathrm{d}\left\|\bm{b}_{k}(t )\right\|}{\mathrm{d}t}\bm{b}_{k}(t)}{\left\|\bm{b}_{k}(t)\right\|^{2}}=\frac{ 1}{\rho_{k}(t)}\left(\frac{\mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t}-\frac{ \mathrm{d}\rho_{k}(t)}{\mathrm{d}t}\bm{w}_{k}(t)\right)\] \[\in \frac{1}{\rho_{k}(t)}\Big{(}\frac{\mathrm{s}_{k}\kappa_{2}}{\sqrt {m}}\bm{F}_{k}(t)-\frac{\mathrm{s}_{k}\kappa_{2}}{\sqrt{m}}\left\langle\bm{F}_ {k}(t),\bm{w}_{k}(t)\right\rangle\bm{w}_{k}(t)\Big{)}=\frac{\mathrm{s}_{k} \kappa_{2}}{\sqrt{m}\rho_{k}(t)}\Big{(}\bm{F}_{k}(t)-\left\langle\bm{F}_{k}(t ),\bm{w}_{k}(t)\right\rangle\bm{w}_{k}(t)\Big{)}.\]Prepared for the analysis of the neurons' dynamics, we establish a rough estimate about the norm and prediction growth of each neuron in Phase I, and we will improve it later.

**Lemma C.2** (A Rough Estimate of Norm and Prediction in Phase I).: _For any \(t\leq T_{\text{I}}\), \(k\in[m]\), and \(i\in[n]\), we have the following estimates:_

\[\rho_{k}(t)\leq\frac{\kappa_{1}+1.1\kappa_{2}t}{\sqrt{m}},\] \[|f_{i}(t)|\leq\sqrt{\kappa_{1}\kappa_{2}},\] \[\Big{|}e^{-y_{i}f_{i}(t)}-1\Big{|}\leq 1.1\sqrt{\kappa_{1} \kappa_{2}}.\]

Proof of Lemma c.2.: First, we define the hitting time

\[T_{\sqrt{\kappa_{1}\kappa_{2}}}:=\inf\Big{\{}t>0:\max_{i\in[n]}|f_{i}(t)|>\sqrt {\kappa_{1}\kappa_{2}}\Big{\}}.\]

From \(|f_{i}(0)|\leq m\frac{\kappa_{2}}{\sqrt{m}}\frac{\kappa_{1}}{\sqrt{m}}=\kappa_ {1}\kappa_{2}<\sqrt{\kappa_{1}\kappa_{2}}\) and the continuity of \(f_{i}(\cdot)\), we know \(T_{\sqrt{\kappa_{1}\kappa_{2}}}>0\). Then we will prove \(T_{\text{I}}=10\sqrt{\frac{\kappa_{1}}{\kappa_{2}}}\leq T_{\sqrt{\kappa_{1} \kappa_{2}}}\).

For any \(k\in[m]\), \(i\in[n]\), and \(t\leq T_{\sqrt{\kappa_{1}\kappa_{2}}}\), we have the following estimates.

Recalling the definition of \(\boldsymbol{F}_{k}(t)\) (8), we have

\[\|\boldsymbol{F}_{k}(t)\|= \left\|\frac{1}{n}\sum_{i=1}^{n}e^{-y_{i}f_{i}(t)}\partial^{ \circ}\sigma\left(\left(\boldsymbol{w}_{k}(t),\boldsymbol{x}_{i}\right) \right)y_{i}\boldsymbol{x}_{i}\right|\leq\max_{i\in[n]}\Big{|}e^{-y_{i}f_{i}( t)}\Big{\|}\] \[\leq e^{\max[f_{i}(t)]}\leq e^{\sqrt{\kappa_{1}\kappa_{2}}}\leq 1.1.\]

Recalling the dynamics (7), for any \(t\leq T_{\sqrt{\kappa_{1}\kappa_{2}}}\),

\[\frac{\mathrm{d}\rho_{k}(t)}{\mathrm{d}t}\leq\frac{\kappa_{2}}{\sqrt{m}}\left\| \langle\boldsymbol{F}_{k}(t),\boldsymbol{w}_{k}(t)\rangle\right\|\leq\frac{ \kappa_{2}}{\sqrt{m}}\left\|\boldsymbol{F}_{k}(t)\right\|\leq\frac{1.1\kappa_ {2}}{\sqrt{m}}.\]

Then combining \(\rho_{k}(0)=\frac{\kappa_{1}}{\sqrt{m}}\), for any \(t\leq T_{\sqrt{\kappa_{1}\kappa_{2}}}\),

\[\rho_{k}(t)\leq\frac{\kappa_{1}+1.1\kappa_{2}t}{\sqrt{m}},\] (9) \[|f_{i}(t)|\leq\sum_{k=1}^{m}|a_{k}|\rho_{k}(t)\leq m\frac{\kappa_ {2}}{\sqrt{m}}\frac{\kappa_{1}+1.1\kappa_{2}t}{\sqrt{m}}\leq\kappa_{2}\big{(} \kappa_{1}+1.1\kappa_{2}t\big{)}.\]

So for any \(t\leq T_{\text{I}}\), we have

\[|f_{i}(t)|\leq\kappa_{2}\big{(}\kappa_{1}+1.1\kappa_{2}T_{\text{I}}\big{)}\leq \kappa_{2}\Big{(}\kappa_{1}+11\sqrt{\kappa_{1}\kappa_{2}}\Big{)}\leq 12\kappa_{2} \sqrt{\kappa_{1}\kappa_{2}}\leq\sqrt{\kappa_{1}\kappa_{2}}.\] (10)

From the definition of \(T_{\sqrt{\kappa_{1}\kappa_{2}}}\), we have proved \(T_{\text{I}}\leq T_{\sqrt{\kappa_{1}\kappa_{2}}}\).

Moreover, from this proof, we know (9) holds for any \(t\leq T_{\text{I}}\). Moreover, by Mean Value Theorem,

\[\Big{|}e^{-y_{i}f_{i}(t)}-1\Big{|}\leq\big{(}\max_{z\in[0,\sqrt{\kappa_{1} \kappa_{2}}]}e^{z}\big{)}|-y_{i}f_{i}(t)|\leq 1.1|f_{i}(t)|\leq 1.1\sqrt{ \kappa_{1}\kappa_{2}}.\]

Now we delve into the optimization dynamics of all neurons in Phase I in the following Section C.1 and C.2.

### The Dynamics of Positive Neurons

According to the initial direction, all positive neurons (\(\mathrm{s}_{k}=1\)) can be divided into the following four classes.

\[[m/2]= \left\{k\in[m/2]:\bm{w}_{k}(0)\in\mathcal{M}_{+}^{+}\cap\mathcal{M }_{-}^{+}\right\}\bigcup\left\{k\in[m/2]:\bm{w}_{k}(0)\in\mathcal{M}_{+}^{-} \cap\mathcal{M}_{-}^{+}\right\}\] \[\bigcup\left\{k\in[m/2]:\bm{w}_{k}(0)\in\mathcal{M}_{+}^{+}\cap \mathcal{M}_{-}^{-}\right\}\bigcup\left\{k\in[m/2]:\bm{w}_{k}(0)\in\mathcal{M}_ {+}^{-}\cap\mathcal{M}_{-}^{-}\right\}.\]

In the following four lemmas, we will prove the dynamics for these four classes of positive neurons. In summary, in Phase I (\(t<T_{\text{I}}\)), some of positive neurons align well with the direction \(\bm{\mu}\), and their norms experiment a small but significant increase, while other positive neurons go dead.

**Lemma C.3** (Positive, \(\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{+}\)).:

_For positive neuron \(k\in\{k\in[m/2]:\bm{w}_{k}(0)\in\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{+}\}\), at the end of Phase I, it holds that_

**(Direction).** _It is aligned with \(\bm{\mu}:\langle\bm{w}_{k}(T_{\text{I}}),\bm{\mu}\rangle\geq\left(1-\mathcal{ O}(\sqrt{\kappa_{1}\kappa_{2}})\right)\left(1-\mathcal{O}\Big{(}(\frac{ \kappa_{1}}{\kappa_{2}})^{0.55}\Big{)}\right);\)__

**(Norm).** _It has a small but significant norm \(:\rho_{k}(T_{\text{I}})=\Theta\Big{(}\frac{\sqrt{\kappa_{1}\kappa_{2}}}{\sqrt {m}}\Big{)}.\)_

Proof of Lemma C.3.: We do the following analysis for any \(k\in\{k\in[m/2]:\bm{w}_{k}(0)\in\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{+}\}\), i.e. \(\mathrm{s}_{k}=1\), \(\langle\bm{w}_{k}(0),\bm{x}_{+}\rangle>0\), and \(\langle\bm{w}_{k}(0),\bm{x}_{-}\rangle>0\).

**Step I.** The neuron stays in \(\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{+}\) for any \(t\leq T_{\text{I}}\).

First, we define the hitting time

\[T_{\text{hit}}:=\inf\Big{\{}t\in(0,T_{\text{I}}]:\bm{w}_{k}(t)\notin\partial( \mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{+})\Big{\}},\]

and we aim to prove that \(T_{\text{hit}}\) does not exist. From the definition of \(T_{\text{hit}}\) and (4), the dynamics of the neuron is:

\[\frac{\mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t}=\frac{\kappa_{2}}{\sqrt{m}}\Big{(} \frac{p}{1+p}e^{-f_{+}(t)}\bm{x}_{+}-\frac{1}{1+p}e^{f_{-}(t)}\bm{x}_{-}\Big{)},\;t\leq T_{\text{hit}}.\]

From \(T_{\text{hit}}\leq T_{\text{I}}\) and Lemma C.2, we have \(\big{|}e^{-y_{i}f_{i}(t)}-1\big{|}\leq 1.1\sqrt{\kappa_{1}\kappa_{2}}\) for any \(t\leq T_{\text{hit}}\). Then we have:

\[\frac{\mathrm{d}\,\langle\bm{b}_{k}(t),\bm{x}_{+}\rangle}{\mathrm{ d}t}=\frac{\kappa_{2}}{\sqrt{m}}\Big{(}\frac{pe^{-f_{+}(t)}}{1+p}-\frac{e^{f_{-}(t)} }{1+p}\cos\Delta\Big{)}\] \[\geq \frac{\kappa_{2}}{\sqrt{m}}\Big{(}\frac{p(1-1.1\sqrt{\kappa_{1} \kappa_{2}})}{1+p}-\frac{(1+1.1\sqrt{\kappa_{1}\kappa_{2}})\cos\Delta}{1+p} \Big{)}{>}0,\]

\[\frac{\mathrm{d}\,\langle\bm{b}_{k}(t),\bm{x}_{-}\rangle}{\mathrm{ d}t}=\frac{\kappa_{2}}{\sqrt{m}}\Big{(}\frac{pe^{-f_{+}(t)}\cos\Delta}{1+p}- \frac{e^{f_{-}(t)}}{1+p}\Big{)}\] \[\geq \frac{\kappa_{2}}{\sqrt{m}}\Big{(}\frac{p(1-1.1\sqrt{\kappa_{1} \kappa_{2}})\cos\Delta}{1+p}-\frac{1+1.1\sqrt{\kappa_{1}\kappa_{2}}}{1+p} \Big{)}{>}0.\]

Hence, for any \(t\leq T_{\text{hit}}\), \(\langle\bm{b}_{k}(t),\bm{x}_{+}\rangle>\langle\bm{b}_{k}(0),\bm{x}_{+}\rangle>0\) and \(\langle\bm{b}_{k}(t),\bm{x}_{-}\rangle>\langle\bm{b}_{k}(0),\bm{x}_{-}\rangle>0\). According to the definition of \(T_{\text{hit}}\), we have proved that \(T_{\text{hit}}\) does not exist, which means the neuron stays in \(\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{+}\) for any \(t\leq T_{\text{I}}\).

**Step II.** Estimate the evolution of \(\langle\bm{w}_{k}(t),\bm{\mu}\rangle\).

With the help of **Step I**, we were able to determine the dynamics for \(t\leq T_{\text{I}}\). For any \(t\leq T_{\text{I}}\), we can do the following estimate.

From (7), the tangential dynamics of the neuron is

\[\frac{\mathrm{d}\bm{w}_{k}(t)}{\mathrm{d}t}=\frac{\kappa_{2}}{\sqrt{m} \rho_{k}(t)}\Big{(}\bm{F}_{k}(t)-\left\langle\bm{F}_{k}(t),\bm{w}_{k}(t)\right \rangle\bm{w}_{k}(t)\Big{)},\] \[\text{where}\quad\bm{F}_{k}(t):=\frac{p}{1+p}e^{-f_{+}(t)}\bm{x}_ {+}-\frac{1}{1+p}e^{f_{-}(t)}\bm{x}_{-}.\]

Recalling the definitions of \(\bm{\mu}\) and \(\bm{z}\), we can estimate the difference between \(\bm{F}_{k}(t)\) and \(\bm{z}\):

\[\left\langle\bm{F}_{k}(t),\bm{z}\right\rangle=\left\langle\bm{z} +\frac{p}{1+p}\big{(}e^{-f_{+}(t)}-1\big{)}\bm{x}_{+}-\frac{1}{1+p}\big{(}e^{f_ {-}(t)}-1\big{)}\bm{x}_{-},\bm{z}\right\rangle\] \[= \left\|\bm{z}\right\|^{2}+\left\langle\frac{p}{1+p}\big{(}e^{-f_ {+}(t)}-1\big{)}\bm{x}_{+}-\frac{1}{1+p}\big{(}e^{f_{-}(t)}-1\big{)}\bm{x}_{-},\frac{p}{1+p}\bm{x}_{+}-\frac{1}{1+p}\bm{x}_{-}\right\rangle\] \[\geq \left\|\bm{z}\right\|^{2}-\frac{p^{2}}{(1+p)^{2}}\left|e^{-f_{+} (t)}-1\right|-\frac{1}{(1+p)^{2}}\left|e^{f_{-}(t)}-1\right|-\frac{p\cos\Delta} {(1+p)^{2}}\Big{(}\left|e^{-f_{+}(t)}-1\right|+\left|e^{f_{-}(t)}-1\right| \Big{)}\] \[\overset{\text{Lemma C\ref{lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lemlem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lemlem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lemlem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lemlem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lemlem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lemlem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lem:lemlem:lem:lem:lem:lem:lem:lem:lemlem:lem:lem:lem:lem:lem:lemlem:lem:lem:lem:lem:lemlem:lem:lemlem:lemlem:lem:lemlem:lem:lem:lemlem:lem:lem:lem:lemlem:lem:lemlem:lem:lem:lem:lem:lemlem:lemlem:lem:lem:lemlem:lem:lemlem:lem:lemlem:lem:lemlem:lem:lemlem:lem:lem:lemlem:lemlem:lemlem:lem:lem:lemlem:lem:lem:lem:lemlem:lemlem:lemlem:lem:lemlem:lem:lem:lemlem:lem:lemlem:lem:lem:lemlem:lem:lemlem:lem:lemlem:lem:lem:lem:lemlem:lem:lem:lem:lemlem:lem:lem:lemlem:lem:lemlem:lem:lemlem:lem:lemlemlem:lem:lem:lemlem:lem:lemlem:lem:lemlem:lem:lemlem:lem:lemlem:lem:lemlem:lem:lemlem:lem:lem:lemlem:lem:lemlem:lem:lemlem:lem:lemlem:lem:lem:lemlem:lem:lem:lemlem:lemlem:lem:lemlem:lemlem:lem:lemlem:lemlem:lemlem:lem:lemlem:lem:lemlem:lemlem:lem:lemlem:lemlem:lemlem:lemlem:lemlem:lemlem:lem:lemlem:lemlem:lem:lem:lemlem:lemlem:lemlem:lemlem:lem:lemlem:lem:lemlem:lemlem:lem:lemlem:lemlem:lemlem:lemlem:lem:lemlem:lemlem:lem:lemlem:lemlem:lem:lemlem:lemlem:lemlem:lemlem:lemlem:lemlem:lem:lemlemlem:lem:lemlem:lemlem:lemlem:lem:lemlem:lemlem:lemlem:lemlem:lemlem:lemlem:lem:lemlem:lemlem:lemlemlem:lemlem:lemlem:lemlem:lemlem:lemlem:lemlem:lemlem:lemlemlem:lemlem:lemlem:lemlem:lemlemlem:lemlem:lemlem:lemlem:lemlem:lemlem:lemlem:lemlem:lemlem:lemlem:lemlem:lemlem:lemlem:lemlem:lemlem:lemlem:lemlem:lemlem:lemlem:lemlem:lemlem:lemlemlem:lem:lemlem:lemlem:lemlem:lemlem:lemlem:lemlemlem:lemlem:lemlem:lemlemlem:lemlem:lemlem:lemlem:lemlem:lemlem:lemlemlem:lemlem:lemlemlem:lemlem:lemlem:lemlem:lemlemlem:lemlemlem:lemlem:lemlemlem:lemlemlem:lemlemlem:lemlemlem:lemlemlem:lemlemlemlem:lemlemlem:lemNow we consider the following auxiliary ODE:

\[\begin{cases}\frac{\mathrm{d}U(t)}{\mathrm{d}t}=\frac{2}{3\left(\frac{ \varepsilon_{1}}{\kappa_{2}}+1.1t\right)}\Big{(}1-4.2\sqrt{\kappa_{1}\kappa_{2} }-U^{2}(t)\Big{)}\\ U(0)=-\frac{1}{1+p}\end{cases},\] (13)

and let \(U(t)\) is the solution of (13). Due to (11) (12), we know that \(\langle\bm{w}_{k}(t),\bm{\mu}\rangle\) is an upper solution of ODE (13). From the Comparison Principle of ODEs, we know this means:

\[\langle\bm{w}_{k}(t),\bm{\mu}\rangle>U(t),\text{ for any }t\leq T_{\text{I}}.\]

Hence, in order to estimate \(\langle\bm{w}_{k}(t),\bm{\mu}\rangle\), we only need to study the solution of ODE (13). It is easy to verify that the solution of (13) satisfies

\[\log\left(\frac{1-4.2\sqrt{\kappa_{1}\kappa_{2}}+U(t)}{1-4.2\sqrt{ \kappa_{1}\kappa_{2}}-U(t)}\right)-\log\left(\frac{1-4.2\sqrt{\kappa_{1} \kappa_{2}}-\frac{1}{1+p}}{1-4.2\sqrt{\kappa_{1}\kappa_{2}}+\frac{1}{1+p}} \right)=\frac{4(1-4.2\sqrt{\kappa_{1}\kappa_{2}})}{3.3}\log\left(1+\frac{1.1 \kappa_{2}}{\kappa_{1}}t\right).\]

Then we have:

\[\log\left(\frac{1-4.2\sqrt{\kappa_{1}\kappa_{2}}+U(t)}{1-4.2\sqrt {\kappa_{1}\kappa_{2}}-U(t)}\right)\geq\log\left(\frac{1-4.2\sqrt{\kappa_{1} \kappa_{2}}-\frac{1}{6}}{1-4.2\sqrt{\kappa_{1}\kappa_{2}}+\frac{1}{6}}\right) +\frac{4(1-4.2\sqrt{\kappa_{1}\kappa_{2}})}{3.3}\log\left(1+\frac{1.1\kappa_ {2}}{\kappa_{1}}t\right)\] \[\geq \log\left(0.7\left(1+\frac{1.1\kappa_{2}}{\kappa_{1}}t\right)^{1. 15}\right)\geq\log\left(0.7\left(1+\frac{1.1\kappa_{2}}{\kappa_{1}}t\right)^{ 1.15}\right),\]

which means

\[U(t)\geq\left(1-4.2\sqrt{\kappa_{1}\kappa_{2}}\right)\left(1-\frac{2}{1+0.7 \left(1+\frac{1.1\kappa_{2}}{\kappa_{1}}t\right)^{1.15}}\right).\]

Hence, we have the estimate of \(\langle\bm{w}_{k}(t),\bm{\mu}\rangle\):

\[\langle\bm{w}_{k}(t),\bm{\mu}\rangle\geq\left(1-4.2\sqrt{\kappa_{1}\kappa_{2} }\right)\left(1-\frac{2}{1+0.7\left(1+\frac{1.1\kappa_{2}}{\kappa_{1}}t \right)^{1.15}}\right).\] (14)

Specifically, we have:

\[\langle\bm{w}_{k}(T_{\text{I}}),\bm{\mu}\rangle\geq\left(1-4.2\sqrt{\kappa_{1} \kappa_{2}}\right)\left(1-\frac{2}{1+0.7\left(1+11\sqrt{\frac{\kappa_{2}}{ \kappa_{1}}}\right)^{1.15}}\right).\]

**Step III.** A finer estimate of \(\rho_{k}(T_{\text{I}})\).

In this step, we will estimate the lower bound and upper bound for \(\rho_{k}(T_{\text{I}})\).

First, lemma C.2 gives us the upper bound for \(\rho_{k}(T_{\text{I}})\):

\[\rho_{k}(T_{\text{I}})\leq\frac{\kappa_{1}+1.1\kappa_{2}T_{\text{I}}}{\sqrt{m} }\leq\frac{\kappa_{1}+11\sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}\leq\frac{12 \sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}.\]

Now we focus on the estimate of the lower bound. Recalling the dynamics of \(\rho_{k}(t)\) (7), for any \(t\leq T_{\text{I}}\),

\[\frac{\mathrm{d}\rho_{k}(t)}{\mathrm{d}t}=\frac{\kappa_{2}}{\sqrt{ m}}\left\langle\bm{F}_{k}(t),\bm{w}_{k}(t)\right\rangle=\frac{\kappa_{2}}{\sqrt{m}} \Big{(}\left\langle\bm{z},\bm{w}_{k}(t)\right\rangle+\left\langle\bm{F}_{k}(t)- \bm{z},\bm{w}_{k}(t)\right\rangle\Big{)}\] \[\geq \frac{\kappa_{2}}{\sqrt{m}}\Big{(}\left\langle\bm{z},\bm{w}_{k}(t )\right\rangle-\left\|\bm{F}_{k}(t)-\bm{z}\right\|\Big{)}\geq\frac{\kappa_{2}}{ \sqrt{m}}\Big{(}\left\|\bm{z}\right\|\left\langle\bm{w}_{k}(t),\bm{\mu}\right\rangle -1.1\sqrt{\kappa_{1}\kappa_{2}}\Big{)}\]

[MISSING_PAGE_EMPTY:27]

then it holds:

\[T_{\rm hit}=T_{\rm hit,-}.\]

So we only need to estimate \(T_{\rm hit,-}\). Due to \(T_{\rm hit}\leq T_{1}\leq T_{\rm init}\) and Lemma C.2, for any \(t\leq T_{\rm hit}\), we have \(\left|e^{-y_{i}f_{i}(t)}-1\right|\leq 0.11\). Then for any \(t\leq T_{\rm hit}\), we have:

\[\frac{{\rm d}\left\langle\bm{b}_{k}(t),\bm{x}_{-}\right\rangle}{{\rm d}t}\geq \frac{0.89\kappa_{2}p\cos\Delta}{\sqrt{m}(1+p)}.\]

Recalling \(\left\langle\bm{w}_{k}(0),\bm{x}_{+}\right\rangle>0\) and \(\left\langle\bm{w}_{k}(0),\bm{x}_{-}\right\rangle<0\), with the help of Lemma I.2, we have \(\left\langle\bm{w}_{k}(0),\bm{x}_{-}\right\rangle>-\sin\Delta\). Combining the two estimate, we have:

\[\left\langle\bm{b}_{k}(t),\bm{x}_{-}\right\rangle\geq\left\langle \bm{b}_{k}(0),\bm{x}_{-}\right\rangle+\int_{0}^{t}\frac{0.89\kappa_{2}p\cos \Delta}{\sqrt{m}(1+p)}{\rm d}t\] \[> -\rho_{k}(0)\sin\Delta+\frac{0.89\kappa_{2}p\cos\Delta}{\sqrt{m} (1+p)}t=-\frac{\kappa_{1}\sin\Delta}{\sqrt{m}}+\frac{0.89\kappa_{2}p\cos \Delta}{\sqrt{m}(1+p)}t.\]

Hence,

\[T_{\rm hit}=T_{\rm hit,-}\leq\frac{(1+p)\tan\Delta}{0.89p}\frac{\kappa_{1}}{ \kappa_{2}}\leq 2\Delta\frac{\kappa_{1}}{\kappa_{2}}.\]

**Step II.** Dynamics after arriving in the manifold \(\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{0}\).

In this step, we analyze the training dynamics after \(\bm{w}_{k}(T_{\rm hit})\in\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{0}\).

First, we will prove \(\bm{w}_{k}(t)\) passes immediately from one side of the surface \(\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{0}\) to the other, i.e. \(\bm{w}_{k}(t)\) enters into \(\mathcal{M}_{-}^{+}\cap\mathcal{M}_{-}^{+}\) at time \(T_{\rm hit}\). Equivalently, we only need to prove \(\bm{b}_{k}(t)\) passes immediately from one side of the surface \(\mathcal{P}_{+}^{+}\cap\mathcal{P}_{-}^{0}\) to the other, i.e. \(\bm{b}_{k}(t)\) enters into \(\mathcal{P}_{+}^{+}\cap\mathcal{P}_{-}^{+}\) at time \(T_{\rm hit}\).

For any \(\tilde{\bm{b}}\in\mathcal{P}_{+}^{+}\cap\mathcal{P}_{-}^{0}\) and \(0<\delta_{0}\ll 1\), we know that \(\mathcal{P}_{+}^{+}\cap\mathcal{P}_{-}^{0}\) separates its neighborhood \(\mathcal{B}(\tilde{\bm{b}},\delta_{0})\) into two domains \(\mathcal{G}_{-}=\{\bm{b}\in\mathcal{B}(\tilde{\bm{b}},\delta_{0}):\left\langle \bm{b},\bm{x}_{-}\right\rangle<0\}\) and \(\mathcal{G}_{+}=\{\bm{b}\in\mathcal{B}(\tilde{\bm{b}},\delta_{0}):\left\langle \bm{b},\bm{x}_{-}\right\rangle>0\}\). Following Definition H.1, we calculate the limited vector field on \(\tilde{\bm{b}}\) from \(\mathcal{G}_{-}\) and \(\mathcal{G}_{+}\).

(i) The limited vector field \(\bm{F}^{-}\) on \(\tilde{\bm{b}}\) (from \(\mathcal{G}_{-}\)):

\[\frac{{\rm d}\bm{b}}{{\rm d}t}=\bm{F}^{-},\text{ where }\bm{F}^{-}=\frac{ \kappa_{2}}{\sqrt{m}}\left(\frac{p}{1+p}e^{-f_{+}(t)}\bm{x}_{+}\right).\]

(ii) The limited vector field \(\bm{F}^{+}\) on \(\tilde{\bm{b}}\) (from \(\mathcal{G}_{+}\)):

\[\frac{{\rm d}\bm{b}}{{\rm d}t}=\bm{F}^{+},\text{ where }\bm{F}^{+}=\frac{ \kappa_{2}}{\sqrt{m}}\left(\frac{pe^{-f_{+}(t)}}{1+p}\bm{x}_{+}-\frac{e^{f_{-} (t)}}{1+p}\bm{x}_{-}\right).\]

(iii) Then we calculate the projections of \(\bm{F}^{-}\) and \(\bm{F}^{+}\) onto \(\bm{x}_{-}\) (the normal to the surface \(\mathcal{P}_{+}^{+}\cap\mathcal{P}_{-}^{0}\)):

\[F_{N}^{-}=\left\langle\bm{F}^{-},\bm{x}_{-}\right\rangle=\frac{\kappa_{2}pe^{- f_{+}(t)}}{\sqrt{m}(1+p)}\cos\Delta,\]

\[F_{N}^{+}=\left\langle\bm{F}^{+},\bm{x}_{-}\right\rangle=\frac{\kappa_{2}pe^{- f_{+}(t)}}{\sqrt{m}(1+p)}\cos\Delta-\frac{\kappa_{2}e^{f_{-}(t)}}{\sqrt{m}(1+p)}.\]

From \(T_{1}<T_{\rm init}\) and Lemma C.2, we know \(|e^{-y_{i}f_{i}(t)}-1|\leq 0.11\), so \(pe^{-f_{+}(t)}\cos\Delta-e^{f_{-}(t)}\geq 0.89p\cos\Delta-1.11\)\(>\)0, which means \(F_{N}^{+}>0\). And it is clear that \(F_{N}^{-}>0\). Hence, the dynamics corresponds to Case (II) in Definition H.1 (\(F_{N}^{-}>0\) and \(F_{N}^{+}>0\)).

(iv) Hence, \(\bm{b}_{k}(t)\) passes immediately from one side of the surface \(\mathcal{P}_{+}^{+}\cap\mathcal{P}_{-}^{0}\) to the other, i.e. \(\bm{b}_{k}(t)\) enters into \(\mathcal{P}_{+}^{+}\cap\mathcal{P}_{-}^{+}\) at time \(T_{\rm hit}\).

Second, proceeding as in the proof of **Step I\(\sim\)III** of the Proof of Theorem C.3, we have the results:

\[\left\langle\bm{w}_{k}(T_{1}),\bm{\mu}\right\rangle\geq\left(1-4.2\sqrt{\kappa_{ 1}\kappa_{2}}\right)\left(1-\frac{2}{1+0.7\left(1+1.1(T_{1}-T_{\rm hit}) \right)^{1.15}}\right)\]

[MISSING_PAGE_EMPTY:29]

Recalling \(\langle\bm{w}_{k}(0),\bm{x}_{+}\rangle\leq 0\) and \(\langle\bm{w}_{k}(0),\bm{x}_{-}\rangle>0\), with the help of Lemma I.2, we have \(\langle\bm{w}_{k}(0),\bm{x}_{-}\rangle\leq\sin\Delta\). Combining the two estimate, we have:

\[\langle\bm{b}_{k}(t),\bm{x}_{-}\rangle\leq\langle\bm{b}_{k}(0), \bm{x}_{-}\rangle-\int_{0}^{t}\frac{0.89\kappa_{2}}{\sqrt{m}(1+p)}\mathrm{d}t\] \[\leq \rho_{k}(0)\sin\Delta-\frac{0.89\kappa_{2}}{\sqrt{m}(1+p)}t=\frac {\kappa_{1}\sin\Delta}{\sqrt{m}}-\frac{0.89\kappa_{2}}{\sqrt{m}(1+p)}t.\]

Hence,

\[T_{\mathrm{hit}}=T_{\mathrm{hit},+}\leq\frac{(1+p)\sin\Delta}{0.89}\frac{ \kappa_{1}}{\kappa_{2}}<T_{\mathrm{I}}=10\sqrt{\frac{\kappa_{1}}{\kappa_{2}}}.\]

Moreover, the analysis gives us \(\bm{w}_{k}(T_{\mathrm{hit}})\in\mathcal{M}_{+}^{-}\cap\mathcal{M}_{-}^{-}\). By Lemma B.2, we obtain:

\[\bm{w}_{k}(t)\in\mathcal{M}_{+}^{-}\cap\mathcal{M}_{-}^{-},\text{ for any }t\geq T_{\mathrm{hit}}.\]

**Lemma C.6** (Positive, \(\mathcal{M}_{+}^{-}\cap\mathcal{M}_{-}^{-}\)).: _For positive neuron \(k\in\{k\in[m/2]:\bm{w}_{k}(0)\in\mathcal{M}_{+}^{-}\cap\mathcal{M}_{-}^{-}\}\), it keeps dead forever._

Proof of Lemma c.6.: Due to Lemma B.2, this lemma is trivial. 

### The Dynamics of Negative Neurons

According to the initial direction, all negative neurons \((\mathrm{d}_{k}=-1)\) can be divided into the following four classes.

\[[m]-[m/2]\\ =\] \[\quad\bigcup\left\{k\in[m]-[m/2]:\bm{w}_{k}(0)\in\mathcal{M}_{+}^{ +}\cap\mathcal{M}_{-}^{-}\right\}\bigcup\left\{k\in[m]-[m/2]:\bm{w}_{k}(0)\in \mathcal{M}_{+}^{-}\cap\mathcal{M}_{-}^{-}\right\}.\]

In the following four lemmas, we will prove the dynamics of these four classes of negative neurons. In summary, in Phase I (\(t<T_{\mathrm{I}}\)), some of the negative neurons move to the manifold \(\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^{+}\) in a shorter time and then remain on this manifold, and their norms grow slowly, while other negative neurons go dead.

**Lemma C.7** (Negative, \(\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{+}\)).: _For negative neuron \(k\in\{k\in[m]-[m/2]:\bm{w}_{k}(0)\in\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{+}\}\), in Phase I \((t\leq T_{\mathrm{I}})\), it's dynamics must belong to one of the following two cases:_

**(i. Living)**_. (S1). \(\bm{w}_{k}(t)\in\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^{+}\) for any \(t\geq\mathcal{O}(\frac{\kappa_{1}}{\kappa_{2}}),\)_

_(S2). It has a small norm \(:\rho_{k}(T_{\mathrm{I}})=\mathcal{O}\Big{(}\frac{\sqrt{\kappa_{1} \kappa_{2}}}{\sqrt{m}}\Big{(}\sqrt{\frac{\kappa_{1}}{\kappa_{2}}}+\frac{ \Delta}{p}\Big{)}\Big{)},\)_

_(S3). It is weakly aligned with \(\bm{x}_{+}^{\perp}:\big{\langle}\bm{w}_{k}(T_{\mathrm{I}}),\bm{x}_{+}^{\perp} \big{\rangle}\geq 1-\mathcal{O}\Big{(}(\sqrt{\frac{\kappa_{1}}{\kappa_{2}}} \frac{p}{\Delta})^{1.6}\Big{)};\)_

**(ii. Dead)**_. \(\bm{w}_{k}(t)\in\mathcal{M}_{+}^{-}\cap\mathcal{M}_{-}^{-}\) for any \(t\geq\mathcal{O}(\frac{\kappa_{1}}{\kappa_{2}}).\)_

**Moreover**_, if \(\langle\bm{w}_{k}(0),\bm{x}_{-}\rangle>\frac{(1+\mathcal{O}(\kappa_{1}\kappa_ {2}))p\cos\Delta-(1-\mathcal{O}(\kappa_{1}\kappa_{2}))}{(1-\mathcal{O}(\kappa_ {1}\kappa_{2}))p-(1+\mathcal{O}(\kappa_{1}\kappa_{2}))\cos\Delta}\langle\bm{ w}_{k}(0),\bm{x}_{+}\rangle\), it must belongs to Case_ (i)_; _if \(\langle\bm{w}_{k}(0),\bm{x}_{+}\rangle>\frac{(1+\mathcal{O}(\kappa_{1}\kappa_ {2}))p-(1-\mathcal{O}(\kappa_{1}\kappa_{2}))\cos\Delta}{(1-\mathcal{O}(\kappa_ {1}\kappa_{2}))p\cos\Delta-(1+\mathcal{O}(\kappa_{1}\kappa_{2}))}\langle\bm{ w}_{k}(0),\bm{x}_{-}\rangle\), it must belongs to Case_ (ii)_._

Proof of Lemma c.7.: We do the following analysis for any \(k\in\{k\in[m]-[m/2]:\bm{w}_{k}(0)\in\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{+}\}\), i.e. \(s_{k}=-1\), \(\langle\bm{w}_{k}(0),\bm{x}_{+}\rangle>0\), and \(\langle\bm{w}_{k}(0),\bm{x}_{-}\rangle>0\).

**Step I.** Neuron must arrives in the border \(\partial(\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{+})\) in \(\mathcal{O}(\frac{\kappa_{1}}{\kappa_{2}})\) time.

First, we define the hitting time

\[T_{\mathrm{hit}}:=\inf\Big{\{}t\in(0,T_{\mathrm{I}}]:\bm{w}_{k}(t)\in\partial( \mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{+})\Big{\}},\]

and we aim to prove \(T_{\mathrm{hit}}\) exists and estimate \(T_{\mathrm{hit}}\).

Recalling the decoupling \(\partial(\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{+})=\big{(}\mathcal{M}_{+}^{ 0}\cap\mathcal{M}_{-}^{+}\big{)}\cup\big{(}\mathcal{M}_{-}^{0}\cap\mathcal{M}_ {+}^{+}\big{)}\cup\big{(}\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^{0}\big{)}\), we only need to focus on the dynamics of \(\langle\bm{b}_{k}(t),\bm{x}_{+}\rangle\) and \(\langle\bm{b}_{k}(t),\bm{x}_{-}\rangle\).

From the definition of \(T_{\mathrm{hit}}\) and (4), the dynamics of the neuron is:

\[\frac{\mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t}=-\frac{\kappa_{2}}{\sqrt{m}}\frac {1}{n}\sum_{i=1}^{n}e^{-y_{i}f_{i}(t)}y_{i}\bm{x}_{i}=-\frac{\kappa_{2}}{\sqrt {m}}\Big{(}\frac{p}{1+p}e^{-f_{+}(t)}\bm{x}_{+}-\frac{1}{1+p}e^{f_{-}(t)}\bm{x }_{-}\Big{)},\;t\leq T_{\mathrm{hit}}.\]

Then we have

\[\frac{\mathrm{d}\,\langle\bm{b}_{k}(t),\bm{x}_{+}\rangle}{\mathrm{ d}t}= \left\langle-\frac{\kappa_{2}}{\sqrt{m}}\Big{(}\frac{p}{1+p}e^{-f_{+}(t)}\bm{x}_{ +}-\frac{1}{1+p}e^{f_{-}(t)}\bm{x}_{-}\Big{)},\bm{x}_{+}\right\rangle\] \[= -\frac{\kappa_{2}}{\sqrt{m}}\Big{(}\frac{p}{1+p}e^{-f_{+}(t)}- \frac{1}{1+p}e^{f_{-}(t)}\cos\Delta\Big{)},\]

\[\frac{\mathrm{d}\,\langle\bm{b}_{k}(t),\bm{x}_{-}\rangle}{\mathrm{ d}t}= \left\langle-\frac{\kappa_{2}}{\sqrt{m}}\Big{(}\frac{p}{1+p}e^{-f_{+}(t)}\bm{x}_{ +}-\frac{1}{1+p}e^{f_{-}(t)}\bm{x}_{-}\Big{)},\bm{x}_{-}\right\rangle\] \[= -\frac{\kappa_{2}}{\sqrt{m}}\Big{(}\frac{p}{1+p}e^{-f_{+}(t)}\cos \Delta-\frac{1}{1+p}e^{f_{-}(t)}\Big{)}.\]

Due to \(T_{\mathrm{hit}}\leq T_{\mathrm{I}}\leq T_{\mathrm{init}}\) and Lemma C.2, for any \(t\leq T_{\mathrm{hit}}\), we have \(\left|e^{-y_{i}f_{i}(t)}-1\right|\leq 1.1\sqrt{\kappa_{1}\kappa_{2}}\). Then for any \(t\leq T_{\mathrm{hit}}\), we have:

\[\frac{\mathrm{d}\,\langle\bm{b}_{k}(t),\bm{x}_{+}\rangle}{\mathrm{ d}t}\leq-\frac{\kappa_{2}}{\sqrt{m}}\Big{(}\frac{(1-1.1\sqrt{\kappa_{1} \kappa_{2}})p}{1+p}-\frac{1+1.1\sqrt{\kappa_{1}\kappa_{2}}}{1+p}\cos\Delta \Big{)}\] \[\leq -\frac{\kappa_{2}\Big{(}(1-1.1\sqrt{\kappa_{1}\kappa_{2}})p-(1+1. 1\sqrt{\kappa_{1}\kappa_{2}})\cos\Delta\Big{)}}{\sqrt{m}(1+p)}\leq-\frac{ \kappa_{2}(0.98p-1.02)}{\sqrt{m}(1+p)},\]

\[\frac{\mathrm{d}\,\langle\bm{b}_{k}(t),\bm{x}_{-}\rangle}{\mathrm{ d}t}\leq-\frac{\kappa_{2}}{\sqrt{m}}\Big{(}\frac{(1-1.1\sqrt{\kappa_{1} \kappa_{2}})p}{1+p}\cos\Delta-\frac{1+1.1\sqrt{\kappa_{1}\kappa_{2}}}{1+p}\Big{)}\] \[\leq -\frac{\kappa_{2}\Big{(}(1-1.1\sqrt{\kappa_{1}\kappa_{2}})p\cos \Delta-(1+1.1\sqrt{\kappa_{1}\kappa_{2}})\Big{)}}{\sqrt{m}(1+p)}\leq-\frac{0.98 \kappa_{2}(p\cos\Delta-1)}{2\sqrt{m}(1+p)}.\]

Now we consider the time

\[T_{\mathrm{test}}:=\frac{3\kappa_{1}}{\kappa_{2}}.\]

If we assume \(T_{\mathrm{test}}<T_{\mathrm{hit}}\), then we have the estimate:

\[\langle\bm{b}_{k}(T_{\mathrm{test}}),\bm{x}_{+}\rangle\leq\langle \bm{b}_{k}(0),\bm{x}_{+}\rangle-\int_{0}^{T_{\mathrm{test}}}\frac{\kappa_{2}(0.9 8p-1.02)}{\sqrt{m}(1+p)}\mathrm{d}t\] \[\leq \frac{\kappa_{1}}{\sqrt{m}}-\frac{\kappa_{2}(0.98p-1.02)}{\sqrt{m }(1+p)}\frac{3\kappa_{1}}{\kappa_{2}}<0,\]

which is contradict to the definition of \(T_{\mathrm{hit}}\). Hence, we have:

\[T_{\mathrm{hit}}\leq T_{\mathrm{test}}\leq\frac{3\kappa_{1}}{\kappa_{2}},\]

which means neurons must arrive in the border \(\partial(\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{+})\) in \(\mathcal{O}(\frac{\kappa_{1}}{\kappa_{2}})\) time.

Because \(\partial(\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{+})=\big{(}\mathcal{M}_{+}^{0} \cap\mathcal{M}_{-}^{+}\big{)}\cup\big{(}\mathcal{M}_{-}^{0}\cap\mathcal{M}_{+}^ {+}\big{)}\cup\big{(}\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^{0}\big{)}\), the neuron must arrives in \(\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^{+}\) or \(\mathcal{M}_{-}^{0}\cap\mathcal{M}_{+}^{+}\) or \(\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^{0}\). If the neuron arrives in \(\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^{0}\), it goes dead forever (Lemma B.2). We will analyze the training dynamics after arriving in \(\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^{+}\) or \(\mathcal{M}_{-}^{0}\cap\mathcal{M}_{+}^{+}\) in the following Step II and Step III.

**Step II.** Dynamics after arriving in the manifold \(\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^{+}\).

In this step, we will analyze the training dynamics after \(\bm{w}_{k}(T_{\mathrm{hit}})\in\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^{+}\), i.e. after \(\bm{b}_{k}(T_{\mathrm{hit}})\in\mathcal{P}_{+}^{0}\cap\mathcal{P}_{-}^{+}\).

We first analysis the vector field around the manifold \(\mathcal{P}_{+}^{0}\cap\mathcal{P}_{-}^{+}\) for \(T_{\mathrm{hit}}\leq t\leq T_{\mathrm{I}}\).

For any \(\tilde{\bm{b}}\in\mathcal{P}_{+}^{0}\cap\mathcal{P}_{-}^{+}\) and \(0<\delta_{0}\ll 1\), we know that \(\mathcal{P}_{+}^{0}\cap\mathcal{P}_{-}^{+}\) separates its neighborhood \(\mathcal{B}(\tilde{\bm{b}},\delta_{0})\) into two domains \(\mathcal{G}_{-}=\{\bm{b}\in\mathcal{B}(\tilde{\bm{b}},\delta_{0}):\langle\bm{b },\bm{x}_{+}\rangle<0\}\) and \(\mathcal{G}_{+}=\{\bm{b}\in\mathcal{B}(\tilde{\bm{b}},\delta_{0}):\langle\bm{b },\bm{x}_{+}\rangle>0\}\). Following Definition H.1, we calculate the limited vector field on \(\tilde{\bm{b}}\) from \(\mathcal{G}_{-}\) and \(\mathcal{G}_{+}\).

(i) The limited vector field \(\bm{F}^{-}\) on \(\tilde{\bm{b}}\) (from \(\mathcal{G}_{-}\)):

\[\frac{\mathrm{d}\bm{b}}{\mathrm{d}t}=\bm{F}^{-},\text{ where }\bm{F}^{-}=\frac{ \kappa_{2}}{\sqrt{m}}\frac{1}{1+p}e^{f_{-}(t)}\bm{x}_{-}.\]

(ii) The limited vector field \(\bm{F}^{+}\) on \(\tilde{\bm{b}}\) (from \(\mathcal{G}_{+}\)):

\[\frac{\mathrm{d}\bm{b}}{\mathrm{d}t}=\bm{F}^{+},\text{ where }\bm{F}^{+}=-\frac{ \kappa_{2}}{\sqrt{m}}\left(\frac{pe^{-f_{+}(t)}}{1+p}\bm{x}_{+}-\frac{e^{f_{-}( t)}}{1+p}\bm{x}_{-}\right).\]

(iii) Then we calculate the projections of \(\bm{F}^{-}\) and \(\bm{F}^{+}\) onto \(\bm{x}_{+}\) (the normal to the surface \(\mathcal{P}_{+}^{0}\cap\mathcal{P}_{-}^{+}\)):

\[F_{N}^{-}=\big{\langle}\bm{F}^{-},\bm{x}_{+}\big{\rangle}=\frac{ \kappa_{2}e^{f_{-}(t)}}{\sqrt{m}(1+p)}\cos\Delta,\] \[F_{N}^{+}=\big{\langle}\bm{F}^{+},\bm{x}_{+}\big{\rangle}=\frac{ \kappa_{2}e^{f_{-}(t)}}{\sqrt{m}(1+p)}\cos\Delta-\frac{\kappa_{2}pe^{-f_{+}(t) }}{\sqrt{m}(1+p)}.\]

From \(T_{\mathrm{I}}<T_{\mathrm{init}}\) and Lemma C.2, we know \(|e^{-y_{i}f_{i}(t)}-1|\leq 0.11\), so \(pe^{-f_{+}(t)}\cos\Delta-e^{f_{-}(t)}\geq 0.89p\cos\Delta-1.11{>}0\), which means \(F_{N}^{+}<0\). And it is clear that \(F_{N}^{-}>0\). Hence, the dynamics corresponds to Case (I) in Definition H.1 (\(F_{N}^{-}>0\) and \(F_{N}^{+}<0\)).

(iv) Hence, \(\bm{b}_{k}(t)\) can not leave \(\mathcal{P}_{+}^{0}\cap\mathcal{P}_{-}^{+}\) for \(T_{\mathrm{hit}}\leq t\leq T_{\mathrm{I}}\).

(v) Moreover, the dynamics of \(\bm{b}_{k}\) on \(\mathcal{P}_{+}^{0}\cap\mathcal{P}_{-}^{+}\) satisfies:

\[\frac{\mathrm{d}\bm{b}}{\mathrm{d}t}=\alpha\bm{F}^{+}+(1-\alpha)\bm{F}^{-}, \quad\alpha=\frac{f_{N}^{-}}{f_{N}^{-}-f_{N}^{+}},\]

which is

\[\frac{\mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t}\frac{\kappa_{2}e^{f_{-}(t)}}{ \sqrt{m}(1+p)}\Big{(}\bm{x}_{-}-\bm{x}_{+}\cos\Delta\Big{)}.\]

By Lemma C.1, we know that the dynamics of \(\bm{w}_{k}(t)\) on \(\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^{+}\) and the dynamics of \(\rho_{k}(t)\) are:

\[\frac{\mathrm{d}\bm{w}_{k}(t)}{\mathrm{d}t}=\frac{\kappa_{2}e^{f_{-}(t)}}{ \rho_{k}(t)\sqrt{m}(1+p)}\Big{(}\bm{x}_{-}-\langle\bm{w}_{k}(t),\bm{x}_{-} \rangle\,\bm{w}_{k}-\bm{x}_{+}\cos\Delta\Big{)}.\] (15)

\[\frac{\mathrm{d}\rho_{k}(t)}{\mathrm{d}t}=\frac{\kappa_{2}e^{f_{-}(t)}}{ \sqrt{m}(1+p)}\left\langle\bm{w}_{k}(t),\bm{x}_{-}\right\rangle.\] (16)

(vi) In this step, we aim to estimate \(\rho_{k}(T_{\mathrm{I}})\) and \(\left\langle\bm{w}_{k}(t),\bm{x}_{+}^{\perp}\right\rangle\).

From \(\bm{w}_{k}(T_{\mathrm{hit}})\in\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^{+}\), it holds that \(\langle\bm{w}_{k}(T_{\mathrm{hit}}),\bm{x}_{+}\rangle=0\) and \(\langle\bm{w}_{k}(T_{\mathrm{hit}}),\bm{x}_{-}\rangle>0\). Using lemma I.2, we have \(0<\langle\bm{w}_{k}(T_{\mathrm{hit}}),\bm{x}_{-}\rangle\leq\sin\Delta\).

Recalling Lemma C.2 and the estimate of \(T_{\rm hit}\) in **Step I**, we have:

\[0\leq\rho_{k}(T_{\rm hit})\leq\frac{\kappa_{1}+1.1\kappa_{2}T_{\rm hit}}{\sqrt{ m}},\]

and we can estimate the dynamics for \(T_{\rm hit}\leq t\leq T_{\rm I}\) by (v)(vi):

\[0\leq\frac{\mathrm{d}\rho_{k}(t)}{\mathrm{d}t}=\frac{\kappa_{2}e^ {f_{-}(t)}}{\sqrt{m}(1+p)}\left<\bm{w}_{k}(t),\bm{x}_{-}\right>\leq\frac{\kappa _{2}e^{f_{-}(t)}}{\sqrt{m}(1+p)}\sin\Delta,\] \[e^{f_{-}(t)}\leq 1+0.11=1.11.\]

Then we obtain the estimate of \(\rho_{k}(t)\) for any \(T_{\rm hit}<t\leq T_{\rm I}\):

\[\rho_{k}(t)=\rho_{k}(T_{\rm hit})+\int_{T_{\rm hit}}^{t}\frac{ \mathrm{d}\rho_{k}(s)}{\mathrm{d}t}\mathrm{d}s\leq\frac{\kappa_{1}+1.1\kappa_{ 2}T_{\rm hit}}{\sqrt{m}}+\frac{\kappa_{2}e^{f_{-}(t)}\sin\Delta}{\sqrt{m}(1+p )}(t-T_{\rm hit})\] \[\leq \frac{\kappa_{1}+1.1\kappa_{2}T_{\rm hit}}{\sqrt{m}}+\frac{1.11 \kappa_{2}\sin\Delta}{\sqrt{m}(1+p)}(T_{\rm I}-T_{\rm hit})\leq\frac{\kappa_{ 1}+1.1\kappa_{2}\frac{3\kappa_{1}}{\kappa_{2}}}{\sqrt{m}}+\frac{1.11\kappa_{ 2}\sin\Delta}{\sqrt{m}(1+p)}(T_{\rm I}-\frac{3\kappa_{1}}{\kappa_{2}})\] \[\leq \frac{4.3\kappa_{1}}{\sqrt{m}}+\frac{1.11\kappa_{2}\sin\Delta}{ \sqrt{m}(1+p)}t.\]

Specifically, we have:

\[\rho_{k}(T_{\rm I})\leq\frac{4.3\kappa_{1}}{\sqrt{m}}+\frac{1.11 \kappa_{2}\sin\Delta}{\sqrt{m}(1+p)}T_{\rm I}\leq\frac{4.3\kappa_{1}}{\sqrt{ m}}+\frac{11.1\sqrt{\kappa_{1}\kappa_{2}}\sin\Delta}{\sqrt{m}(1+p)}=\frac{ \sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}\Big{(}4.3\sqrt{\frac{\kappa_{1}}{ \kappa_{2}}}+\frac{11.1\sin\Delta}{1+p}\Big{)}.\]

For any \(\bm{w}\in\mathcal{M}^{0}_{+}\cap\mathcal{M}^{+}_{-}\), we have \(\left<\bm{w},\bm{x}_{+}\right>=0\), so

\[\left<\bm{w},\bm{x}_{+}^{\perp}\right>=\left<\bm{w},\frac{\bm{x}_{-}-\bm{x}_{+ }\cos\Delta}{\|\bm{x}_{-}-\bm{x}_{+}\cos\Delta\|}\right>=\frac{\left<\bm{w},\bm {x}_{-}\right>}{\|\bm{x}_{-}-\bm{x}_{+}\cos\Delta\|}=\frac{1}{\sin\Delta} \left<\bm{w},\bm{x}_{-}\right>.\]

So we only need to focus on the dynamics of \(\left<\bm{w}_{k}(t),\bm{x}_{-}\right>\) to derive the dynamics of \(\left<\bm{w}_{k}(t),\bm{x}_{+}^{\perp}\right>\).

By (15) and the estimate of \(\rho_{k}(t)\), for any \(T_{\rm hit}\leq t\leq T_{\rm I}\) we have:

\[\frac{\mathrm{d}\left<\bm{w}_{k}(t),\bm{x}_{-}\right>}{\mathrm{d} t}=\left<\frac{e^{f_{-}(t)}}{\rho_{k}(t)\sqrt{m}(1+p)}\Big{(}\bm{x}_{-}- \left<\bm{w}_{k}(t),\bm{x}_{-}\right>\bm{w}_{k}(t)-\bm{x}_{+}\cos\Delta\Big{)},\bm{x}_{-}\right>\] \[= \frac{\kappa_{2}e^{f_{-}(t)}}{\rho_{k}(t)\sqrt{m}(1+p)}\Big{(} \sin^{2}\Delta-\left<\bm{w}_{k}(t),\bm{x}_{-}\right>^{2}\Big{)}\geq\frac{(1-0.11)\kappa_{2}}{(1+p)\Big{(}4.3\kappa_{1}+\frac{1.11\kappa_{2}\sin\Delta}{(1+p )}t\Big{)}}\Big{(}\sin^{2}\Delta-\left<\bm{w}_{k}(t),\bm{x}_{-}\right>^{2}\Big{)}\] \[\geq \frac{0.89\kappa_{2}}{4.3(1+p)\kappa_{1}+1.11\kappa_{2}t\sin \Delta}\Big{(}\sin^{2}\Delta-\left<\bm{w}_{k}(t),\bm{x}_{-}\right>^{2}\Big{)}.\]

And we have \(0<\left<\bm{w}_{k}(\frac{3\kappa_{1}}{\kappa_{2}}),\bm{x}_{-}\right><\sin\Delta\).

Now we consider the following auxiliary ODE:

\[\begin{cases}\frac{\mathrm{d}U(t)}{\mathrm{d}t}=\frac{0.89\kappa_{2}}{4.3(1+p) \kappa_{1}+1.11\kappa_{2}t\sin\Delta}\Big{(}\sin^{2}\Delta-U^{2}(t)\Big{)}\\ U(0)=0\end{cases},\] (17)

and let \(U(t)\) is the solution of (13). We know that \(\left<\bm{w}_{k}(t),\bm{x}_{-}\right>\) is an upper solution of ODE (17). From the Comparison Principle of ODEs, we know this means:

\[\left<\bm{w}_{k}(t),\bm{x}_{-}\right>>U(t),\text{ for any }t\leq T_{\rm I}.\]

In order to estimate \(\left<\bm{w}_{k}(t),\bm{x}_{-}\right>\), we only need to study the solution of ODE (17). It is easy to verify that the solution of (17) satisfies

\[\log\left(\frac{\sin\Delta+U(t)}{\sin\Delta-U(t)}\right)-\log\left(\frac{\sin \Delta}{\sin\Delta}\right)=\frac{1.78\kappa_{2}\Delta}{1.11\kappa_{2}\sin \Delta}\log\left(\frac{4.3(1+p)\kappa_{1}+1.11\kappa_{2}t\sin\Delta}{4.3(1+p) \kappa_{1}+3.33\kappa_{1}\sin\Delta}\right)\]Then we have:

\[\log\left(\frac{\sin\Delta+U(T_{\text{I}})}{\sin\Delta-U(T_{\text{I} })}\right)\geq\frac{1.78}{1.11}\log\left(\frac{4.3(1+p)\kappa_{1}+11.1\sqrt{ \kappa_{1}\kappa_{2}}\sin\Delta}{4.3(1+p)\kappa_{1}+3.33\kappa_{1}\sin\Delta}\right)\] \[> 1.6\log\left(1+\frac{\left(11.1\sqrt{\frac{\kappa_{2}}{\kappa_{1} }}-3.33\right)\frac{\sin\Delta}{1+p}}{4.3+3.33\frac{\sin\Delta}{1+p}}\right) >1.6\log\left(1+\frac{\left(11.1\sqrt{\frac{\kappa_{2}}{\kappa_{1}}}-3.33 \right)\frac{\sin\Delta}{1+p}}{4.6}\right)\] \[> 1.6\log\left(1+\frac{10.7}{4.6}\sqrt{\frac{\kappa_{2}}{\kappa_{1} }}\frac{\sin\Delta}{1+p}\right),\]

which means

\[U(T_{\text{I}})>\left(1-\frac{2}{\left(1+\frac{10.7}{4.6}\sqrt{\frac{\kappa_{ 2}}{\kappa_{1}}\frac{\sin\Delta}{1+p}}\right)^{1.6}+1}\right)\sin\Delta.\]

Hence, we have the estimate of \(\left\langle\bm{w}_{k}(t),\bm{x}_{+}^{\perp}\right\rangle\):

\[\left\langle\bm{w}_{k}(T_{\text{I}}),\bm{x}_{+}^{\perp}\right\rangle=\frac{1} {\sin\Delta}\left\langle\bm{w}_{k}(T_{\text{I}}),\bm{x}_{-}\right\rangle>\frac {1}{\sin\Delta}U(T_{\text{I}})>1-\frac{2}{\left(1+2.32\sqrt{\frac{\kappa_{2}} {\kappa_{1}}\frac{\sin\Delta}{1+p}}\right)^{1.6}+1}.\]

**Step III.** Dynamics after arriving in the manifold \(\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{0}\).

In this step, we analyze the training dynamics after \(\bm{w}_{k}(T_{\text{hit}})\in\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{0}\), i.e. \(\bm{b}_{k}(T_{\text{hit}})\in\mathcal{P}_{+}^{+}\cap\mathcal{P}_{-}^{0}\).

For any \(\tilde{\bm{b}}\in\mathcal{P}_{+}^{+}\cap\mathcal{P}_{-}^{0}\) and \(0<\delta_{0}\ll 1\), we know that \(\mathcal{P}_{+}^{+}\cap\mathcal{P}_{-}^{0}\) separates its neighborhood \(\mathcal{B}(\tilde{\bm{b}},\delta_{0})\) into two domains \(\mathcal{G}_{-}=\{\bm{b}\in\mathcal{B}(\tilde{\bm{b}},\delta_{0}):\left\langle \bm{b},\bm{x}_{-}\right\rangle<0\}\) and \(\mathcal{G}_{+}=\{\bm{b}\in\mathcal{B}(\tilde{\bm{b}},\delta_{0}):\left\langle \bm{b},\bm{x}_{-}\right\rangle>0\}\). Following Definition H.1, we calculate the limited vector field on \(\tilde{\bm{b}}\) from \(\mathcal{G}_{-}\) and \(\mathcal{G}_{+}\).

For any \(\tilde{\bm{b}}\in\mathcal{P}_{+}^{+}\cap\mathcal{P}_{-}^{0}\) and \(0<\delta_{0}\ll 1\), we know that \(\mathcal{P}_{+}^{+}\cap\mathcal{P}_{-}^{0}\) separates its neighborhood \(\mathcal{B}(\tilde{\bm{b}},\delta_{0})\) into two domains \(\mathcal{G}_{-}=\{\bm{b}\in\mathcal{B}(\tilde{\bm{b}},\delta_{0}):\left\langle \bm{b},\bm{x}_{-}\right\rangle<0\}\) and \(\mathcal{G}_{+}=\{\bm{b}\in\mathcal{B}(\tilde{\bm{b}},\delta_{0}):\left\langle \bm{b},\bm{x}_{-}\right\rangle>0\}\). Following Definition H.1, we calculate the limited vector field on \(\tilde{\bm{b}}\) from \(\mathcal{G}_{-}\) and \(\mathcal{G}_{+}\).

(i) The limited vector field \(\bm{F}^{-}\) on \(\tilde{\bm{b}}\) (from \(\mathcal{G}_{-}\)):

\[\frac{\mathrm{d}\bm{b}}{\mathrm{d}t}=\bm{F}^{-},\text{ where }\bm{F}^{-}=-\frac{ \kappa_{2}}{\sqrt{m}}\frac{p}{1+p}e^{-f_{+}(t)}\bm{x}_{+}.\]

(ii) The limited vector field \(\bm{F}^{+}\) on \(\tilde{\bm{b}}\) (from \(\mathcal{G}_{+}\)):

\[\frac{\mathrm{d}\bm{b}}{\mathrm{d}t}=\bm{F}^{+},\text{ where }\bm{F}^{+}=-\frac{\kappa_{2}}{\sqrt{m}}\left(\frac{ pe^{-f_{+}(t)}}{1+p}\bm{x}_{+}-\frac{e^{f_{-}(t)}}{1+p}\bm{x}_{-}\right).\]

(iii) Then we calculate the projections of \(\bm{F}^{-}\) and \(\bm{F}^{+}\) onto \(\bm{x}_{-}\) (the normal to the surface \(\mathcal{P}_{+}^{+}\cap\mathcal{P}_{-}^{0}\)):

\[F_{N}^{-}=\left\langle\bm{F}^{-},\bm{x}_{-}\right\rangle=-\frac{\kappa_{2}pe^{- f_{+}(t)}}{\sqrt{m}(1+p)}\cos\Delta,\]

\[F_{N}^{+}=\left\langle\bm{F}^{+},\bm{x}_{-}\right\rangle=-\left(\frac{\kappa_{2} pe^{-f_{+}(t)}}{\sqrt{m}(1+p)}\cos\Delta-\frac{\kappa_{2}e^{f_{-}(t)}}{\sqrt{m}(1+p)} \right).\]

From \(T_{\text{I}}<T_{\text{init}}\) and Lemma C.2, we know \(|e^{-y_{i}f_{i}(t)}-1|\leq 0.11\), so \(pe^{-f_{+}(t)}\cos\Delta-e^{f_{-}(t)}\geq 0.89p\cos\Delta-1.11\)\(>\)0, which means \(F_{N}^{+}>0\). And it is clear that \(F_{N}^{-}>0\). Hence, the dynamics corresponds to Case (II) in Definition H.1 (\(F_{N}^{-}>0\) and \(F_{N}^{+}>0\)).

(iv) Hence, \(\bm{b}_{k}(t)\) passes immediately from one side of the surface \(\mathcal{P}_{+}^{+}\cap\mathcal{P}_{-}^{0}\) to the other, i.e. \(\bm{b}_{k}(t)\) enters into \(\mathcal{P}_{+}^{+}\cap\mathcal{P}_{-}^{+}\) at time \(T_{\text{hit}}\).

Then the dynamics of \(\bm{b}_{k}\) in \(\mathcal{P}_{+}^{\mathcal{P}}\cap\mathcal{P}_{-}^{\mathcal{P}}\) satisfies:

\[\frac{\mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t}=-\frac{\kappa_{2}}{\sqrt{m}}\frac{p} {1+p}e^{-f_{+}(t)}\bm{x}_{+}.\]

(v) We define the following time, and our aim is to estimate \(T_{\mathrm{test},1}\):

\[T_{\mathrm{test},1}: =\inf\Big{\{}t\in(T_{\mathrm{hit}},T_{\mathrm{I}}]:\langle\bm{w} _{k}(t),\bm{x}_{+}\rangle\leq 0\text{ or }\langle\bm{w}_{k}(t),\bm{x}_{-}\rangle\geq 0 \Big{\}},\] \[T_{\mathrm{test},2}: =\inf\Big{\{}t\in(T_{\mathrm{hit}},T_{\mathrm{I}}]:\langle\bm{w} _{k}(t),\bm{x}_{+}\rangle\leq 0\Big{\}}\]

It is clear \(T_{\mathrm{test},1}\leq T_{\mathrm{test},2}\). Moreover, due to \(\frac{\mathrm{d}\langle\bm{b}_{k}(t),\bm{x}_{-}\rangle}{\mathrm{d}t}=-\frac{ \kappa_{2}}{\sqrt{m}}\frac{p}{1+p}e^{-f_{+}(t)}\cos\Delta<0\) and \(\langle\bm{b}_{k}(T_{\mathrm{hit}}),\bm{x}_{-}\rangle=0\), we know \(\langle\bm{b}_{k}(t),\bm{x}_{-}\rangle<0\) holds for any \(t\leq T_{\mathrm{test},1}\). Hence, we have

\[T_{\mathrm{test},1}=T_{\mathrm{test},2},\quad\langle\bm{w}_{k}(T_{\mathrm{ test},1}),\bm{x}_{+}\rangle=0,\quad\langle\bm{w}_{k}(T_{\mathrm{test},1}),\bm{x}_{-} \rangle<0.\]

And we only need to estimate \(T_{\mathrm{test},2}\). For any \(T_{\mathrm{hit}}<t\leq T_{\mathrm{test},1}=T_{\mathrm{test},2}\), we have

\[\frac{\mathrm{d}\langle\bm{b}_{k}(t),\bm{x}_{+}\rangle}{\mathrm{d}t}=-\frac{ \kappa_{2}}{\sqrt{m}}\frac{p}{1+p}e^{-f_{+}(t)}\stackrel{{\text{ Lemma C.2}}}{{\leq}}-\frac{\kappa_{2}}{\sqrt{m}}\frac{p}{1+p}(1-0.11)=-\frac{0.89 \kappa_{2}p}{\sqrt{m}(1+p)}.\]

Recalling Lemma C.2 and the estimate of \(T_{\mathrm{hit}}\) in **Step I**, we have:

\[\langle\bm{b}_{k}(T_{\mathrm{hit}}),\bm{x}_{+}\rangle\leq\|\rho_{k}(t)\|\leq \frac{\kappa_{1}+1.1\kappa_{2}T_{\mathrm{hit}}}{\sqrt{m}}\leq\|\rho_{k}(t)\| \leq\frac{\kappa_{1}+1.1\kappa_{2}\frac{3\kappa_{1}}{\kappa_{2}}}{\sqrt{m}}= \frac{4.3\kappa_{1}}{\sqrt{m}}.\]

Then for any \(T_{\mathrm{hit}}<t\leq T_{\mathrm{test},2}\), we have:

\[\langle\bm{b}_{k}(t),\bm{x}_{+}\rangle\leq\langle\bm{b}_{k}(T_{\mathrm{hit}}), \bm{x}_{+}\rangle-\int_{T_{\mathrm{hit}}}^{t}\frac{0.89\kappa_{2}p}{\sqrt{m}( 1+p)}\mathrm{d}s\leq\frac{4.3\kappa_{1}}{\sqrt{m}}-\frac{0.89\kappa_{2}p(t-T_{ \mathrm{hit}})}{\sqrt{m}(1+p)}.\]

So we have the estimate

\[T_{\mathrm{test},1}=T_{\mathrm{test},2}\leq T_{\mathrm{hit}}+\frac{4.3\kappa_{ 1}(1+p)}{0.89\kappa_{2}p}\leq\Big{(}3+\frac{4.3\cdot 6}{0.89\cdot 5}\Big{)}\frac{ \kappa_{1}}{\kappa_{2}}\leq\frac{9\kappa_{1}}{\kappa_{2}}<T_{\mathrm{I}}.\]

Recalling \(\bm{w}_{k}(T_{\mathrm{test},1})\in\mathcal{M}_{+}^{-}\cap\mathcal{M}_{-}^{-}\) and Lemma B.2, the neuron \(\bm{b}_{k}(t)\) keeps dead for any \(t\geq T_{\mathrm{I}}\).

**Step IV.** Which subspace does the neuron select?

From **Step II**, we know that the neuron \(\bm{w}_{k}(t)\) must arrives in \(\mathcal{M}_{+}^{0}\cap\mathcal{M}_{+}^{-}\) or \(\mathcal{M}_{-}^{0}\cap\mathcal{M}_{+}^{+}\) or \(\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^{0}\). In this step, we will analyze which subspace does the neuron select.

We only need to compare the following two times:

\[T_{\mathrm{hit},+}:=\inf\Big{\{}t\in(0,T_{\mathrm{I}}]:\langle\bm{b}_{k}(t), \bm{x}_{+}\rangle\leq 0\Big{\}},\]

\[T_{\mathrm{hit},-}:=\inf\Big{\{}t\in(0,T_{\mathrm{I}}]:\langle\bm{b}_{k}(t), \bm{x}_{-}\rangle\leq 0\Big{\}}.\]

From the definition of \(T_{\mathrm{hit}}\), we know \(T_{\mathrm{hit},+}=T_{\mathrm{hit}}\) or \(T_{\mathrm{hit},-}=T_{\mathrm{hit}}\).

Recalling the proof in **Step II**, we compare the following two dynamics for \(t<T_{\mathrm{hit}}\):

\[\frac{\mathrm{d}\,\langle\bm{b}_{k}(t),\bm{x}_{+}\rangle}{\mathrm{d}t} =-\frac{\kappa_{2}}{\sqrt{m}}\Big{(}\frac{p}{1+p}e^{-f_{+}(t)}- \frac{1}{1+p}e^{f_{-}(t)}\cos\Delta\Big{)},\] \[\frac{\mathrm{d}\,\langle\bm{b}_{k}(t),\bm{x}_{-}\rangle}{\mathrm{d }t} =-\frac{\kappa_{2}}{\sqrt{m}}\Big{(}\frac{p}{1+p}e^{-f_{+}(t)}\cos\Delta-\frac{1} {1+p}e^{f_{-}(t)}\Big{)}.\]

With the help of Lemma C.2 and the estimate of \(T_{\mathrm{hit}}\), for any \(t\leq T_{\mathrm{hit}}\),

\[|e^{-y_{t}f_{+}(t)}-1|\leq 1.1\kappa_{2}(\kappa_{1}+1.1\kappa_{2}T_{\mathrm{hit}}) \leq 1.1\kappa_{2}(\kappa_{1}+3.3\kappa_{1})=4.73\kappa_{1}\kappa_{2}.\]

Hence, we have the estimate of the dynamics:

\[-\frac{\kappa_{2}\Big{(}(1+4.73\kappa_{1}\kappa_{2})p-(1-4.73\kappa_{1}\kappa_{2}) \cos\Delta\Big{)}}{\sqrt{m}(1+p)}\leq\frac{\mathrm{d}\,\langle\bm{b}_{k}(t), \bm{x}_{+}\rangle}{\mathrm{d}t}\leq-\frac{\kappa_{2}\Big{(}(1-4.73\kappa_{1} \kappa_{2})p-(1+4.73\kappa_{1}\kappa_{2})\cos\Delta\Big{)}}{\sqrt{m}(1+p)},\]\[-\frac{\kappa_{2}\Big{(}(1+4.73\kappa_{1}\kappa_{2})p\cos\Delta-(1-4.73\kappa_{1} \kappa_{2})\Big{)}}{\sqrt{m}(1+p)}\leq\frac{\mathrm{d}\left\langle\bm{b}_{k}(t), \bm{x}_{-}\right\rangle}{\mathrm{d}t}\leq-\frac{\kappa_{2}\Big{(}(1-4.73\kappa _{1}\kappa_{2})p\cos\Delta-(1+4.73\kappa_{1}\kappa_{2})\Big{)}}{\sqrt{m}(1+p)}.\]

(i) If the initialization satisfies \(\left\langle\bm{b}_{k}(0),\bm{x}_{-}\right\rangle>\frac{(1+4.73\kappa_{1} \kappa_{2})p\cos\Delta-(1-4.73\kappa_{1}\kappa_{2})}{(1-4.73\kappa_{1}\kappa_ {2})p-(1+4.73\kappa_{1}\kappa_{2})\cos\Delta}\left\langle\bm{b}_{k}(0),\bm{x}_ {+}\right\rangle\), we will prove that the neuron selects \(\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^{+}\) at \(T_{\mathrm{hit}}\).

For any \(t<T_{\mathrm{hit}}\), we have the estimate:

\[\left\langle\bm{b}_{k}(t),\bm{x}_{-}\right\rangle\geq\left\langle\bm{b}_{k}(0 ),\bm{x}_{-}\right\rangle-\frac{\kappa_{2}\Big{(}(1+4.73\kappa_{1}\kappa_{2} )p\cos\Delta-(1-4.73\kappa_{1}\kappa_{2})\Big{)}}{\sqrt{m}(1+p)}t\]

Comparing these two inequalities, we have:

\[T_{\mathrm{hit},+}=T_{\mathrm{hit}}<T_{\mathrm{hit},-},\]

which means

\[\left\langle\bm{b}_{k}(T_{\mathrm{hit}}),\bm{x}_{+}\right\rangle=0,\quad \left\langle\bm{b}_{k}(T_{\mathrm{hit}}),\bm{x}_{-}\right\rangle>0.\]

So the neuron \(\bm{w}_{k}(T_{\mathrm{hit}})\in\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^{+}\).

(ii) If the initialization satisfies \(\left\langle\bm{b}_{k}(0),\bm{x}_{+}\right\rangle>\frac{(1+4.73\kappa_{1} \kappa_{2})p-(1-4.73\kappa_{1}\kappa_{2})\cos\Delta}{(1-4.73\kappa_{1}\kappa _{2})p\cos\Delta-(1+4.73\kappa_{1}\kappa_{2})}\left\langle\bm{b}_{k}(0),\bm{x} _{-}\right\rangle\), we will prove that the neuron selects \(\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}\) at \(T_{\mathrm{hit}}\).

For any \(t<T_{\mathrm{hit}}\), we have the estimate:

\[\left\langle\bm{b}_{k}(t),\bm{x}_{-}\right\rangle\leq\left\langle\bm{b}_{k}(0 ),\bm{x}_{-}\right\rangle-\frac{\kappa_{2}\Big{(}(1-4.73\kappa_{1}\kappa_{2}) p\cos\Delta-(1+4.73\kappa_{1}\kappa_{2})\Big{)}}{\sqrt{m}(1+p)}t,\]

\[\left\langle\bm{b}_{k}(t),\bm{x}_{+}\right\rangle\geq\left\langle\bm{b}_{k}(0 ),\bm{x}_{+}\right\rangle-\frac{\kappa_{2}\Big{(}(1+4.73\kappa_{1}\kappa_{2}) p-(1-4.73\kappa_{1}\kappa_{2})\cos\Delta\Big{)}}{\sqrt{m}(1+p)}t\]

Comparing these two inequalities, we have:

\[T_{\mathrm{hit},-}=T_{\mathrm{hit}}<T_{\mathrm{hit},+},\]

which means

\[\left\langle\bm{b}_{k}(T_{\mathrm{hit}}),\bm{x}_{-}\right\rangle=0,\quad \left\langle\bm{b}_{k}(T_{\mathrm{hit}}),\bm{x}_{+}\right\rangle>0.\]

So the neuron \(\bm{w}_{k}(T_{\mathrm{hit}})\in\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{0}\).

**Lemma C.8** (Negative, \(\mathcal{M}_{+}^{-}\cap\mathcal{M}_{-}^{+}\)).:

_For negative neuron \(k\in\{k\in[m]-[m/2]:\bm{w}_{k}(0)\in\mathcal{M}_{+}^{-}\cap\mathcal{M}_{-}^{+}\}\), in Phase I \((t\leq T_{\mathrm{I}})\), we have:_

_(S1)._ \(\bm{w}_{k}(t)\in\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^{+}\) _for any_ \(t\leq\mathcal{O}\Big{(}\frac{\kappa_{1}}{\kappa_{2}}p\Delta\Big{)},\)__

_(S2). It has a small norm_ \(:\rho_{k}(T_{\mathrm{I}})=\mathcal{O}\Big{(}\frac{\sqrt{\kappa_{1}\kappa_{2}}}{ \sqrt{m}}\Big{(}\sqrt{\frac{\kappa_{1}}{\kappa_{2}}}+\frac{\Delta}{p}\Big{)} \Big{)},\)__

_(S3). It is aligned with_ \(\bm{x}_{+}^{\perp}:\left\langle\bm{w}_{k}(T_{\mathrm{I}}),\bm{x}_{+}^{\perp} \right\rangle\geq 1-\mathcal{O}\Big{(}(\sqrt{\frac{\kappa_{1}}{\kappa_{2}}}\frac{p}{ \Delta})^{1.6}\Big{)}.\)__Proof of Lemma c.8.: We do the following analysis for any \(k\in\{k\in[m]-[m/2]:\bm{w}_{k}(0)\in\mathcal{M}_{+}^{-}\cap\mathcal{M}_{-}^{+}\}\), i.e. \(\mathrm{s}_{k}=-1\), \(\langle\bm{w}_{k}(0),\bm{x}_{+}\rangle\leq 0\), and \(\langle\bm{w}_{k}(0),\bm{x}_{-}\rangle>0\).

**Step I.** The neuron must arrives in \(\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^{+}\) in \(\mathcal{O}\Big{(}\frac{\kappa_{1}p\Delta}{\kappa_{2}}\Big{)}\) time.

The case \(\langle\bm{w}_{k}(0),\bm{x}_{+}\rangle=0\) is trivial. Then we only need to consider the case \(\langle\bm{w}_{k}(0),\bm{x}_{+}\rangle<0\).

First, we define the hitting time

\[T_{\mathrm{hit}}:=\inf\Big{\{}t\in(0,T_{\mathrm{I}}]:\bm{w}_{k}(t)\notin \mathcal{M}_{+}^{-}\cap\mathcal{M}_{-}^{+}\Big{\}},\]

and we aim to estimate \(T_{\mathrm{hit}}\) and prove \(\bm{w}_{k}(T_{\mathrm{hit}})\in\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^{+}\).

We focus on the dynamics of \(\langle\bm{b}_{k}(t),\bm{x}_{+}\rangle\) and \(\langle\bm{b}_{k}(t),\bm{x}_{-}\rangle\).

From the definition of \(T_{\mathrm{hit}}\) and (4), the dynamics of the neuron is:

\[\frac{\mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t}=\frac{\kappa_{2}}{\sqrt{m}}\frac{ 1}{1+p}e^{f_{-}(t)}\bm{x}_{-},\;t\leq T_{\mathrm{hit}}.\]

Then we have

\[\frac{\mathrm{d}\,\langle\bm{b}_{k}(t),\bm{x}_{+}\rangle}{\mathrm{ d}t}= \left\langle\frac{\kappa_{2}}{\sqrt{m}}\frac{1}{1+p}e^{f_{-}(t)}\bm{x}_{-},\bm{x}_{ +}\right\rangle=\frac{\kappa_{2}\cos\Delta}{\sqrt{m}(1+p)}e^{f_{-}(t)},\] \[\frac{\mathrm{d}\,\langle\bm{b}_{k}(t),\bm{x}_{-}\rangle}{\mathrm{ d}t}= \left\langle\frac{\kappa_{2}}{\sqrt{m}}\frac{1}{1+p}e^{f_{-}(t)}\bm{x}_{-},\bm{x}_{ -}\right\rangle=\frac{\kappa_{2}}{\sqrt{m}(1+p)}e^{f_{-}(t)}.\]

It is clear \(\frac{\mathrm{d}\,\langle\bm{b}_{k}(t),\bm{x}_{-}\rangle}{\mathrm{d}t}>0\), so \(\langle\bm{b}_{k}(t),\bm{x}_{-}\rangle>\langle\bm{b}_{k}(0),\bm{x}_{-}\rangle>0\) for any \(t\leq T_{\mathrm{hit}}\). If we denote

\[T_{\mathrm{hit},+}:=\inf\Big{\{}t\in(0,T_{\mathrm{I}}]:\langle\bm{w}_{k}(t),\bm {x}_{+}\rangle\leq 0\Big{\}},\]

then it holds:

\[T_{\mathrm{hit}}=T_{\mathrm{hit},+}.\]

So we only need to estimate \(T_{\mathrm{hit},+}\). Due to \(T_{\mathrm{hit}}\leq T_{\mathrm{I}}\leq T_{\mathrm{init}}\) and Lemma c.2, for any \(t\leq T_{\mathrm{hit}}\), we have \(|e^{-y_{i}f_{i}(t)}-1|\leq 0.11\). Then for any \(t\leq T_{\mathrm{hit}}\), we have:

\[\frac{\mathrm{d}\,\langle\bm{b}_{k}(t),\bm{x}_{+}\rangle}{\mathrm{d}t}\geq \frac{0.89\kappa_{2}\cos\Delta}{\sqrt{m}(1+p)}.\]

Recalling \(\langle\bm{w}_{k}(0),\bm{x}_{+}\rangle<0\) and \(\langle\bm{w}_{k}(0),\bm{x}_{-}\rangle>0\), with the help of Lemma I.2, we have \(\langle\bm{w}_{k}(0),\bm{x}_{+}\rangle>-\sin\Delta\) and \(\langle\bm{w}_{k}(0),\bm{x}_{-}\rangle<\sin\Delta\). Combining the two estimate, we have:

\[\langle\bm{b}_{k}(t),\bm{x}_{+}\rangle\geq\langle\bm{b}_{k}(0), \bm{x}_{+}\rangle+\int_{0}^{t}\frac{0.89\kappa_{2}\cos\Delta}{\sqrt{m}(1+p)} \mathrm{d}t\] \[> -\rho_{k}(0)\sin\Delta+\frac{0.89\kappa_{2}\cos\Delta}{\sqrt{m}(1 +p)}t=-\frac{\kappa_{1}\sin\Delta}{\sqrt{m}}+\frac{0.89\kappa_{2}\cos\Delta}{ \sqrt{m}(1+p)}t.\]

Hence,

\[T_{\mathrm{hit}}=T_{\mathrm{hit},+}\leq\frac{(1+p)\tan\Delta}{0.89}\frac{ \kappa_{1}}{\kappa_{2}}\leq 2p\Delta\frac{\kappa_{1}}{\kappa_{2}}<T_{\mathrm{I}}=10 \sqrt{\frac{\kappa_{1}}{\kappa_{2}}}.\]

Moreover, we can estimate of \(\rho_{k}(T_{\mathrm{hit}})\).

Since \(\langle\bm{w}_{k}(t),\bm{x}_{+}\rangle<0\) and \(\langle\bm{w}_{k}(t),\bm{x}_{-}\rangle>0\) hold for any \(t\leq T_{\mathrm{hit}}\), with the help of Lemma I.2, we have \(\langle\bm{w}_{k}(t),\bm{x}_{-}\rangle<\sin\Delta\). Combining (7), for any \(t\leq T_{\mathrm{hit}}\), we have

\[\rho_{k}(t)\leq\rho_{k}(0)+\int_{0}^{t}\frac{\kappa_{2}}{\sqrt{m }(1+p)}e^{f_{-}(t)}\,\langle\bm{w}_{k}(t),\bm{x}_{-}\rangle\,\mathrm{d}t\] \[\leq \frac{\kappa_{1}}{\sqrt{m}}+\int_{0}^{t}\frac{1.11\kappa_{2}}{ \sqrt{m}(1+p)}\sin\Delta\mathrm{d}t\leq\frac{\kappa_{1}}{\sqrt{m}}+\frac{1.11 \kappa_{2}\sin\Delta}{\sqrt{m}(1+p)}t.\]

**Step II.** Dynamics after arriving in the manifold \(\mathcal{M}^{0}_{+}\cap\mathcal{M}^{+}_{-}\).

Proceeding as in the proof of **Step II** in the Proof of Theorem C.7, we have:

\(\bm{w}_{k}(t)\) can not leave \(\mathcal{M}^{0}_{+}\cap\mathcal{M}^{+}_{-}\) for \(T_{\rm hit}\leq t\leq T_{\rm I}\). Moreover, the dynamics of \(\bm{w}_{k}\) on \(\mathcal{M}^{0}_{+}\cap\mathcal{M}^{+}_{-}\) satisfies:

\[\frac{\mathrm{d}\bm{w}_{k}(t)}{\mathrm{d}t}= \frac{\kappa_{2}e^{f_{-}(t)}}{\rho_{k}(t)\sqrt{m}(1+p)}\Big{(}\bm {x}_{-}-\left\langle\bm{w}_{k},\bm{x}_{-}\right\rangle\bm{w}_{k}-\bm{x}_{+} \cos\Delta\Big{)},\] \[\frac{\mathrm{d}\rho_{k}(t)}{\mathrm{d}t}= \frac{\kappa_{2}e^{f_{-}(t)}}{\sqrt{m}(1+p)}\left\langle\bm{w}_{k }(t),\bm{x}_{-}\right\rangle,\] \[\frac{\mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t}= \frac{\kappa_{2}e^{f_{-}(t)}}{\sqrt{m}(1+p)}\Big{(}\bm{x}_{-}- \bm{x}_{+}\cos\Delta\Big{)}.\]

Recalling the estimate of \(T_{\rm hit}\) in **Step I**, we have

\[\rho_{k}(T_{\rm hit})\leq\frac{\kappa_{1}}{\sqrt{m}}+\frac{1.11\kappa_{2}\sin \Delta}{\sqrt{m}(1+p)}T_{\rm hit}.\]

As the proof of **Step II** in the Proof of Theorem C.7, for any \(T_{\rm hit}<t\leq T_{\rm I}\), we have

\[\rho_{k}(t)=\rho_{k}(T_{\rm hit})+\int_{T_{\rm hit}}^{t}\frac{ \mathrm{d}\rho_{k}(s)}{\mathrm{d}s}\mathrm{d}s\leq\frac{\kappa_{1}}{\sqrt{m}} +\frac{1.11\kappa_{2}\sin\Delta}{\sqrt{m}(1+p)}T_{\rm hit}+\frac{\kappa_{2}e^ {f_{-}(t)}\sin\Delta}{\sqrt{m}(1+p)}(t-T_{\rm hit})\] \[\leq \frac{\kappa_{1}}{\sqrt{m}}+\frac{1.11\kappa_{2}\sin\Delta}{\sqrt {m}(1+p)}T_{\rm hit}+\frac{1.11\kappa_{2}\sin\Delta}{\sqrt{m}(1+p)}(t-T_{\rm hit })=\frac{\kappa_{1}}{\sqrt{m}}+\frac{1.11\kappa_{2}\sin\Delta}{\sqrt{m}(1+p)}t.\]

Combining the estimate in **Step I**, for any \(0<t\leq T_{\rm I}\), we have:

\[\rho_{k}(t)\leq\frac{\kappa_{1}}{\sqrt{m}}+\frac{1.11\kappa_{2}\sin\Delta}{ \sqrt{m}(1+p)}t.\]

Specifically, we have:

\[\rho_{k}(T_{\rm I})\leq\frac{\kappa_{1}}{\sqrt{m}}+\frac{11.1\sqrt{\kappa_{1} \kappa_{2}}\sin\Delta}{\sqrt{m}(1+p)}\leq\frac{\sqrt{\kappa_{1}\kappa_{2}}}{ \sqrt{m}}\Big{(}\sqrt{\frac{\kappa_{1}}{\kappa_{2}}}+11.1\frac{\Delta}{1+p} \Big{)}.\]

Similar to the proof of **Step II** in the Proof of Theorem C.7, we have the estimate of the dynamics of \(\left\langle\bm{w}_{k}(t),\bm{x}_{-}\right\rangle\):

\[\frac{\mathrm{d}\left\langle\bm{w}_{k}(t),\bm{x}_{-}\right\rangle}{\mathrm{d} t}\geq\frac{0.89\kappa_{2}}{(1+p)\kappa_{1}+1.11\kappa_{2}t\sin\Delta} \Big{(}\sin^{2}\Delta-\left\langle\bm{w}_{k}(t),\bm{x}_{-}\right\rangle^{2} \Big{)},\quad 0<t\leq T_{\rm I},\]

\[0<\left\langle\bm{w}_{k}(0),\bm{x}_{-}\right\rangle<\sin\Delta.\]

In the same way, we can derive

\[\left\langle\bm{w}_{k}(T_{\rm I}),\bm{x}_{-}\right\rangle>\left(1-\frac{2}{ \left(1+11.1\sqrt{\frac{\kappa_{2}}{\kappa_{1}}\sin\Delta}\right)^{1.6}+1} \right)\sin\Delta\]

Hence, we have the estimate of \(\left\langle\bm{w}_{k}(t),\bm{x}_{+}^{\perp}\right\rangle\):

\[\left\langle\bm{w}_{k}(T_{\rm I}),\bm{x}_{+}^{\perp}\right\rangle=\frac{1}{ \sin\Delta}\left\langle\bm{w}_{k}(T_{\rm I}),\bm{x}_{-}\right\rangle>1-\frac{2 }{\left(1+11.1\sqrt{\frac{\kappa_{2}}{\kappa_{1}}\sin\Delta}{1+p}\right)^{1.6 }+1}.\]

**Lemma C.9** (Negative, \(\mathcal{M}^{+}_{+}\cap\mathcal{M}^{-}_{-}\)).:

_For negative neuron \(k\in\{k\in[m]-[m/2]:\bm{w}_{k}(0)\in\mathcal{M}^{+}_{+}\cap\mathcal{M}^{-}_{-}\}\), it keeps dead:_

\[\bm{w}_{k}(t)\in\mathcal{M}^{-}_{+}\cap\mathcal{M}^{-}_{-},\text{ for any }t\geq T_{\rm I}>\mathcal{O}\Big{(}\frac{\kappa_{1}\Delta}{\kappa_{2}}\Big{)}.\]Proof of Lemma c.9.: We do the following analysis for any \(k\in\{k\in[m]-[m/2]:\bm{w}_{k}(0)\in\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{-}\}\), i.e. \(\mathrm{s}_{k}=-1\), \(\langle\bm{w}_{k}(0),\bm{x}_{+}\rangle>0\), and \(\langle\bm{w}_{k}(0),\bm{x}_{-}\rangle\leq 0\).

First, we define the hitting time

\[T_{\mathrm{hit}}:=\inf\Big{\{}t\in(0,T_{\mathrm{I}}]:\bm{w}_{k}(t)\notin \mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{-}\Big{\}},\]

and we aim to estimate \(T_{\mathrm{hit}}\) and prove \(\bm{w}_{k}(T_{\mathrm{hit}})\in\mathcal{M}_{+}^{-}\cap\mathcal{M}_{-}^{-}\).

From the definition of \(T_{\mathrm{hit}}\) and (4), the dynamics of the neuron is:

\[\frac{\mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t}=-\frac{\kappa_{2}}{\sqrt{m}}\frac{ p}{1+p}e^{-f_{+}(t)}\bm{x}_{+},\;t\leq T_{\mathrm{hit}}.\]

Then we have

\[\frac{\mathrm{d}\,\langle\bm{b}_{k}(t),\bm{x}_{+}\rangle}{\mathrm{ d}t}= \left\langle-\frac{\kappa_{2}}{\sqrt{m}}\frac{p}{1+p}e^{-f_{+}(t)} \bm{x}_{+},\bm{x}_{+}\right\rangle=-\frac{\kappa_{2}}{\sqrt{m}}\frac{p}{1+p}e ^{-f_{+}(t)},\] \[\frac{\mathrm{d}\,\langle\bm{b}_{k}(t),\bm{x}_{-}\rangle}{\mathrm{ d}t}= \left\langle-\frac{\kappa_{2}}{\sqrt{m}}\frac{p}{1+p}e^{-f_{+}(t)}\bm{x}_{+}, \bm{x}_{-}\right\rangle=-\frac{\kappa_{2}}{\sqrt{m}}\frac{p}{1+p}e^{-f_{+}(t)} \cos\Delta.\]

It is clear \(\frac{\mathrm{d}\,\langle\bm{b}_{k}(t),\bm{x}_{-}\rangle}{\mathrm{d}t}<0\), so \(\langle\bm{b}_{k}(t),\bm{x}_{-}\rangle<\langle\bm{b}_{k}(0),\bm{x}_{-}\rangle\leq 0\) for any \(t\leq T_{\mathrm{hit}}\). If we denote

\[T_{\mathrm{hit},+}:=\inf\Big{\{}t\in(0,T_{\mathrm{I}}]:\langle\bm{w}_{k}(t), \bm{x}_{+}\rangle\leq 0\Big{\}},\]

then it holds:

\[T_{\mathrm{hit}}=T_{\mathrm{hit},+}.\]

So we only need to estimate \(T_{\mathrm{hit},+}\). Due to \(T_{\mathrm{hit}}\leq T_{\mathrm{I}}\leq T_{\mathrm{init}}\) and Lemma C.2, for any \(t\leq T_{\mathrm{hit}}\), we have \(\left|e^{-y_{i}f_{i}(t)}-1\right|\leq 0.11\). Then for any \(t\leq T_{\mathrm{hit}}\), we have:

\[\frac{\mathrm{d}\,\langle\bm{b}_{k}(t),\bm{x}_{+}\rangle}{\mathrm{d}t}\leq- \frac{0.89\kappa_{2}p}{\sqrt{m}(1+p)}.\]

Recalling \(\langle\bm{w}_{k}(0),\bm{x}_{+}\rangle>0\) and \(\langle\bm{w}_{k}(0),\bm{x}_{-}\rangle\leq 0\), with the help of Lemma I.2, we have \(\langle\bm{w}_{k}(0),\bm{x}_{+}\rangle\leq\sin\Delta\). Combining the two estimate, we have:

\[\langle\bm{b}_{k}(t),\bm{x}_{+}\rangle\leq\langle\bm{b}_{k}(0), \bm{x}_{+}\rangle-\int_{0}^{t}\frac{0.89\kappa_{2}p}{\sqrt{m}(1+p)}\mathrm{d}t\] \[\leq \rho_{k}(0)\sin\Delta-\frac{0.89\kappa_{2}p}{\sqrt{m}(1+p)}t= \frac{\kappa_{1}\sin\Delta}{\sqrt{m}}-\frac{0.89\kappa_{2}p}{\sqrt{m}(1+p)}t.\]

Hence,

\[T_{\mathrm{hit}}=T_{\mathrm{hit},+}\leq\frac{(1+p)\sin\Delta}{0.89p}\frac{ \kappa_{1}}{\kappa_{2}}<T_{\mathrm{I}}=10\sqrt{\frac{\kappa_{1}}{\kappa_{2}}}.\]

Moreover, the analysis gives us \(\bm{w}_{k}(T_{\mathrm{hit}})\in\mathcal{M}_{+}^{-}\cap\mathcal{M}_{-}^{-}\). By Lemma B.2, we obtain:

\[\bm{w}_{k}(t)\in\mathcal{M}_{+}^{-}\cap\mathcal{M}_{-}^{-},\;\text{for any}\;t\geq T_{ \mathrm{hit}}.\]

**Lemma C.10** (Negative, \(\mathcal{M}_{+}^{-}\cap\mathcal{M}_{-}^{-}\)).: _For negative neuron \(k\in\{k\in[m]-[m/2]:\bm{w}_{k}(0)\in\mathcal{M}_{+}^{-}\cap\mathcal{M}_{-}^{-}\}\), it keeps dead: \(\bm{w}_{k}(t)\in\mathcal{M}_{+}^{-}\cap\mathcal{M}_{-}^{-}\) for any \(t\geq 0\)._

Proof of Lemma c.10.: Due to Lemma B.2, this lemma is trivial.

### Initialization Estimation and Proof of Theorem 4.1

To get the number of neurons in the eight classes in the subsection above, we also need to estimate the initial positions of these neurons under the random initialization.

**Lemma C.11** (Initialization Estimation).:

_If \(m=\Omega\big{(}\log(1/\delta)\big{)}\), then with probability at least \(1-\delta\), we have:_

\[\left|\#\Big{\{}k\in[m/2]:\left<\bm{w}_{k}(0),\bm{x}_{+}\right>>0 \Big{\}}-\frac{m}{4}\right|\leq 0.04m,\] \[\#\Big{\{}k\in[m]-[m/2]:\left<\bm{w}_{k}(0),\bm{x}_{-}\right>0, \left<\bm{w}_{k}(0),\bm{x}_{-}\right>A\left<\bm{w}_{k}(0),\bm{x}_{+}\right> \Big{\}}\geq 0.075m,\] \[\#\Big{\{}k\in[m]-[m/2]:\left<\bm{w}_{k}(0),\bm{x}_{-}\right>>0, \left<\bm{w}_{k}(0),\bm{x}_{+}\right>\leq B\left<\bm{w}_{k}(0),\bm{x}_{-} \right>\Big{\}}\leq 0.205m.\]

_where \(A=\frac{(1+4.73\kappa_{1}\kappa_{2})p\cos\Delta-(1-4.73\kappa_{1}\kappa_{2})} {(1-4.73\kappa_{1}\kappa_{2})p-(1+4.73\kappa_{1}\kappa_{2})\cos\Delta}\) and \(B=\frac{(1+4.73\kappa_{1}\kappa_{2})p-(1-4.73\kappa_{1}\kappa_{2})\cos\Delta}{ (1-4.73\kappa_{1}\kappa_{2})p\cos\Delta-(1+4.73\kappa_{1}\kappa_{2})}\) (mentioned in Lemma C.7)._

Proof of Lemma c.11.:

(i) By Hoeffding's Inequality (Lemma I.1), for any \(\epsilon>0\) we have:

\[\mathbb{P}\Bigg{(}\left|\#\Big{\{}k\in[m/2]:\left<\bm{w}_{k}(0), \bm{x}_{+}\right>>0\Big{\}}-\frac{m}{4}\right|\geq\frac{m\epsilon}{2}\Bigg{)} =\mathbb{P}\Bigg{(}\left|\frac{2}{m}\sum_{k\in[m/2]}\mathbb{I}\Big{\{} \left<\bm{w}_{k}(0),\bm{x}_{+}\right>>0\Big{\}}-\frac{1}{2}\right|\geq\epsilon \Bigg{)}\] \[= \mathbb{P}\Bigg{(}\left|\frac{2}{m}\sum_{k\in[m/2]}\mathbb{I} \big{\{}\left<\bm{w}_{k}(0),\bm{x}_{+}\right>>0\big{\}}-\mathbb{E}\Big{[} \mathbb{I}\big{\{}\left<\bm{w}_{1}(0),\bm{x}_{+}\right>>0\Big{\}}\Bigg{]} \right|\geq\epsilon\Bigg{)}\] \[\leq 2\exp\Big{(}-\frac{2(\frac{m}{2})^{2}\epsilon^{2}}{\frac{m}{2}} \Big{)}=2\exp(-m\epsilon^{2}).\]

(ii) From \(\bm{w}_{k}(0)\sim\mathbb{U}(\mathbb{S}^{d-1})\), without loss of generality, we can let \(\bm{x}_{-}=\bm{e}_{1}\) and \(\bm{x}_{+}=\bm{e}_{1}\cos\Delta+\bm{e}_{2}\sin\Delta\).

So we have:

\[\Big{\{}k\in[m]-[m/2]:\left<\bm{w}_{k}(0),\bm{x}_{-}\right>0, \left<\bm{w}_{k}(0),\bm{x}_{-}\right>>A\left<\bm{w}_{k}(0),\bm{x}_{+}\right> \Big{\}}\] \[= \Big{\{}k\in[m]-[m/2]:w_{k,1}(0)>0,w_{k,1}(0)>A\Big{(}w_{k,1}(0) \cos\Delta+w_{k,2}(0)\sin\Delta\Big{)}\Big{\}}\] \[= \Big{\{}k\in[m]-[m/2]:w_{k,1}(0)>0,(1-A\cos\Delta)w_{k,1}(0)>Aw_{ k,2}(0)\sin\Delta\Big{\}}.\]

From (6), we have \(A>0\) and

\[A=1+\frac{\Big{(}(1+4.73\kappa_{1}\kappa_{2})\cos\Delta-(1-4.73 \kappa_{1}\kappa_{2})\Big{)}(p+1)}{(1-4.73\kappa_{1}\kappa_{2})p-(1+4.73\kappa _{1}\kappa_{2})\cos\Delta}\] \[\leq 1+\frac{4.73\kappa_{1}\kappa_{2}(1+\cos\Delta)}{1-4.73\kappa_{1 }\kappa_{2}}\frac{p+1}{p-\frac{10}{9}}\leq 1+\frac{9.46\kappa_{1}\kappa_{2}}{1- \frac{1}{19}}\frac{90}{71}\leq 1+12.66\kappa_{1}\kappa_{2},\]

\[A=1+\frac{\Big{(}(1+4.73\kappa_{1}\kappa_{2})\cos\Delta-(1-4.73 \kappa_{1}\kappa_{2})\Big{)}(p+1)}{(1-4.73\kappa_{1}\kappa_{2})p-(1+4.73\kappa _{1}\kappa_{2})\cos\Delta}\] \[\geq 1-\frac{4.73\kappa_{1}\kappa_{2}(1+\cos\Delta)+(1-\cos\Delta)}{1 -4.73\kappa_{1}\kappa_{2}}\frac{p+1}{p-\frac{10}{9}}\geq 1-\frac{\frac{2}{19}+\frac{ \Delta^{2}}{2}}{1-\frac{1}{19}}\frac{90}{71}\] \[\geq 1-\frac{\frac{2}{19}+\frac{1}{19}}{\frac{18}{19}}\frac{90}{71} \geq 0.78,\]\[\frac{1-A\cos\Delta}{A\sin\Delta}\geq\frac{1-A}{A\sin\Delta}\geq-\frac{12.66\kappa_ {1}\kappa_{2}}{A\sin\Delta}\geq-\frac{12.66\kappa_{1}\kappa_{2}}{0.78\sin\Delta} \geq-\frac{12.66\kappa_{1}\kappa_{2}}{0.78\frac{2}{\pi}\Delta}\geq-\frac{25.5 \kappa_{1}\kappa_{2}}{\Delta}\geq-\frac{1}{100}.\]

For simplicity, we denote the event

\[A_{k}:=\Big{\{}w_{k,1}(0)>0,-\frac{1}{100}w_{k,1}(0)>w_{k,2}(0) \Big{\}},\;k\in[m]-[m/2].\]

Then we have the estimate:

\[\#\Big{\{}k\in[m]-[m/2]:w_{k,1}(0)>0,(1-A\cos\Delta)w_{k,1}(0)>Aw_ {k,2}(0)\sin\Delta\Big{\}}\] \[\geq \#\Big{\{}k\in[m]-[m/2]:w_{k,1}(0)>0,-\frac{1}{100}w_{k,1}(0)>w_{ k,2}(0)\Big{\}}=\sum_{k\in[m]-[m/2]}\mathbb{I}\{A_{k}\}.\]

We first estimate the lower bound for \(\mathbb{E}[\mathbb{I}\{A_{m}\}]\):

\[\mathbb{E}\Big{[}\mathbb{I}\{A_{m}\}\Big{]}=\mathbb{P}(A_{m})= \mathbb{P}\Big{(}w_{m,1}(0)>0,-\frac{1}{100}w_{m,1}(0)>w_{k,2}(0)\Big{)}\] \[\stackrel{{\bm{g}\sim\mathcal{N}(\mathbf{0},\mathbf{ I})}}{{=}} \mathbb{P}\Bigg{(}\frac{g_{1}}{\|\bm{g}\|}>0,-\frac{1}{100}\frac{g_{1}}{\|\bm{g}\| }>\frac{g_{2}}{\|\bm{g}\|}\Bigg{)}=\mathbb{P}\Big{(}g_{1}>0,g_{1}<-100g_{2} \Big{)}=\mathbb{P}\Big{(}g_{1}>0,g_{1}<100g_{2}\Big{)}\] \[= \mathbb{P}\Big{(}100g_{2}>g_{1}>0\Big{)}\geq\sup_{t>0}\mathbb{P} \Big{(}g_{2}>t,100t>g_{1}>0\Big{)}\stackrel{{ g\sim\mathcal{N}(0,1)}}{{=}}\sup_{t>0}\mathbb{P}\Big{(}g>t \Big{)}\mathbb{P}\Big{(}100t>g>0\Big{)}\] \[\geq \mathbb{P}\Big{(}g>\frac{1}{10}\Big{)}\mathbb{P}\Big{(}10>g>0 \Big{)}\geq 0.23.\]

Secondly, by Hoeffding's inequality (Lemma I.1), for any \(\epsilon>0\), we have

\[\mathbb{P}\Bigg{(}\sum_{k\in[m]-[m/2]}\mathbb{I}\{A_{k}\}-0.115m \leq-\frac{m}{2}\epsilon\Bigg{)}\leq\mathbb{P}\Bigg{(}\sum_{k\in[m]-[m/2]} \mathbb{I}\{A_{k}\}-\frac{m}{2}\mathbb{E}\Big{[}\mathbb{I}\{A_{m}\}\Big{]}\leq -\frac{m}{2}\epsilon\Bigg{)}\] \[= \mathbb{P}\Bigg{(}\frac{2}{m}\sum_{k\in[m]-[m/2]}\mathbb{I}\{A_{k }\}-\mathbb{E}\Big{[}\mathbb{I}\{A_{m}\}\Big{]}\leq-\epsilon\Bigg{)}\leq\exp \Big{(}-\frac{2\frac{(\frac{m}{2})}{2}\epsilon^{2}}{\frac{m}{2}}\Big{)}=\exp(- m\epsilon^{2})\]

(iii) This proof is similar to (ii). From \(\bm{w}_{k}(0)\sim\mathbb{U}(\mathbb{S}^{d-1})\), without loss of generality, we can let \(\bm{x}_{-}=\bm{e}_{1}\) and \(\bm{x}_{+}=\bm{e}_{1}\cos\Delta+\bm{e}_{2}\sin\Delta\).

so we have:

\[\Big{\{}k\in[m]-[m/2]:\langle\bm{w}_{k}(0),\bm{x}_{-}\rangle>0, \langle\bm{w}_{k}(0),\bm{x}_{+}\rangle\leq B\,\langle\bm{w}_{k}(0),\bm{x}_{-} \rangle\,\Big{\}}\] \[= \Big{\{}k\in[m]-[m/2]:w_{k,1}(0)>0,w_{k,1}(0)\cos\Delta+w_{k,2}(0 )\sin\Delta\leq Bw_{k,1}(0)\Big{\}}\] \[= \Big{\{}k\in[m]-[m/2]:w_{k,1}(0)>0,(B-\cos\Delta)w_{k,1}(0)>w_{k,2 }(0)\sin\Delta\Big{\}}.\]

From (6), we have \(B>0\) and

\[B-\cos\Delta=\frac{(1+4.73\kappa_{1}\kappa_{2})p-(1-4.73\kappa_{1} \kappa_{2})\cos\Delta}{(1-4.73\kappa_{1}\kappa_{2})p\cos\Delta-(1+4.73\kappa_{1 }\kappa_{2})}-\cos\Delta\] \[= \frac{(1+4.73\kappa_{1}\kappa_{2})(p+\cos\Delta)-(1-4.73\kappa_{1 }\kappa_{2})(1+p\cos\Delta)\cos\Delta}{(1-4.73\kappa_{1}\kappa_{2})p\cos \Delta-(1+4.73\kappa_{1}\kappa_{2})}\] \[= \frac{p\sin^{2}\Delta+4.73\kappa_{1}\kappa_{2}(p+2\cos\Delta+p\cos ^{2}\Delta)}{(1-4.73\kappa_{1}\kappa_{2})p\cos\Delta-(1+4.73\kappa_{1} \kappa_{2})}\leq\frac{\sin^{2}\Delta+9.46\kappa_{1}\kappa_{2}}{1-4.73\kappa_{1 }\kappa_{2}}\frac{p+1}{p\cos\Delta-\frac{10}{9}}\] \[\leq \frac{\sin^{2}\Delta+9.46\frac{\Delta}{2550}}{1-\frac{1}{19}} \frac{p+1}{\frac{9}{10}p-\frac{10}{9}}\leq\frac{\sin^{2}\Delta+9.46\frac{\pi \sin\Delta}{5100}}{1-\frac{1}{19}}\frac{p+1}{\frac{9}{10}p-\frac{10}{9}}\] \[\leq \frac{0.315+\frac{9.467}{5100}}{\frac{18}{19}}\frac{10}{\frac{81} {10}-\frac{10}{9}}\sin\Delta\leq\frac{\sin\Delta}{2},\]For simplicity, we denote the event

\[B_{k}:=\Big{\{}w_{k,1}(0)>0,\frac{1}{2}w_{k,1}(0)>w_{k,2}(0)\Big{\}},\;k\in[m]-[m/2].\]

Then we have the estimate:

\[\#\Big{\{}k\in[m]-[m/2]:w_{k,1}(0)>0,(B-\cos\Delta)w_{k,1}(0)>w_{k,2 }(0)\sin\Delta\Big{\}}\] \[\leq \#\Big{\{}k\in[m]-[m/2]:w_{k,1}(0)>0,\frac{1}{2}w_{k,1}(0)>w_{k,2} (0)\Big{\}}=\sum_{k\in[m]-[m/2]}\mathbb{I}\{B_{k}\}.\]

We first estimate the lower bound for \(\mathbb{E}[\mathbb{I}\{B_{m}\}]\):

\[\mathbb{E}\Big{[}\mathbb{I}\{B_{m}\}\Big{]}=\mathbb{P}(B_{m})= \mathbb{P}\Big{(}w_{m,1}(0)>0,\frac{1}{2}w_{m,1}(0)>w_{k,2}(0)\Big{)}\] \[\stackrel{{ g\sim\mathcal{N}(\mathbf{0},\mathbf{I}_ {d})}}{{=}} \mathbb{P}\bigg{(}\frac{g_{1}}{\|\bm{g}\|}>0,\frac{1}{2}\frac{g_ {1}}{\|\bm{g}\|}>\frac{g_{2}}{\|\bm{g}\|}\bigg{)}=\mathbb{P}\Big{(}g_{1}>0,g_ {1}>2g_{2}\Big{)}=\mathbb{P}\Big{(}g_{1}>0,g_{2}\leq 0\Big{)}+\mathbb{P} \Big{(}g_{1}>2g_{2}>0\Big{)}\] \[= \frac{1}{4}+\mathbb{P}\Big{(}g_{1}>2g_{2}>0\Big{)}\leq\frac{1}{4 }+\frac{1}{2\pi}\int_{0}^{+\infty}e^{-\frac{x^{2}}{2}}\int_{0}^{\frac{x}{2}}e ^{-\frac{y^{2}}{2}}\mathrm{d}y\mathrm{d}x\leq\frac{1}{4}+\frac{1}{2\pi}\int_{ 0}^{+\infty}\frac{x}{2}e^{-\frac{x^{2}}{2}}\mathrm{d}x\] \[\leq \frac{1}{4}+\frac{1}{4\pi}.\]

Secondly, by Hoeffding's inequality (Lemma I.1), for any \(\epsilon>0\), we have

\[\mathbb{P}\Bigg{(}\sum_{k\in[m]-[m/2]}\mathbb{I}\{B_{k}\}-\Big{(} \frac{1}{8}+\frac{1}{8\pi}\Big{)}m\geq\frac{m}{2}\epsilon\Bigg{)}\leq\mathbb{P }\Bigg{(}\sum_{k\in[m]-[m/2]}\mathbb{I}\{B_{k}\}-\frac{m}{2}\mathbb{E}\Big{[} \mathbb{I}\{B_{m}\}\Big{]}\geq\frac{m}{2}\epsilon\Bigg{)}\] \[= \mathbb{P}\Bigg{(}\frac{2}{m}\sum_{k\in[m]-[m/2]}\mathbb{I}\{B_{k} \}-\mathbb{E}\Big{[}\mathbb{I}\{B_{m}\}\Big{]}\geq\epsilon\Bigg{)}\leq\exp \Big{(}-\frac{2\big{(}\frac{m}{2}\big{)}^{2}\epsilon^{2}}{\frac{m}{2}}\Big{)}= \exp(-m\epsilon^{2}).\]

let \(\epsilon=0.08\) and \(\delta=4\exp(-m\epsilon^{2}/2)\). Combining the uniform bounds in (i)(ii)(iii), we obtain this theorem:

If \(m\geq\frac{2\log(4/\delta)}{0.08^{2}}\), then with probability at least \(1-\delta\), we have:

\[\Big{|}\#\Big{\{}k\in[m/2]:\langle\bm{w}_{k}(0),\bm{x}_{+}\rangle> 0\Big{\}}-\frac{m}{4}\Big{|}\leq 0.04m,\] \[\#\Big{\{}k\in[m]-[m/2]:w_{k,1}(0)>0,(1-A\cos\Delta)w_{k,1}(0)> Aw_{k,2}(0)\sin\Delta\Big{\}}\geq 0.075m,\] \[\#\Big{\{}k\in[m]-[m/2]:w_{k,1}(0)>0,(B-\cos\Delta)w_{k,1}(0)>w_{k,2}(0)\sin\Delta\Big{\}}\leq 0.205m.\]

So far, Lemma C.3, C.4, C.5, C.6, C.7, C.8, C.9, C.10 characterize the training dynamics of each neuron in Phase I, and Lemma C.11 estimate the initial positions of the neurons. Now we can prove our main theorem in Phase I.

**Theorem C.12** (Restatement of Theorem 4.1).: _Under the data Assumption 3.1, let the two-layer network trained by Gradient Flow (2) starting from random initialization. Let the width \(m=\Omega\left(\log(1/\delta)\right)\), the initialization scales satisfy (6). Then with probability at least \(1-\delta\), the following results_ **(S1)\(\sim\)(S5)** _hold at the end of Phase I (\(T_{\text{I}}=10\sqrt{\frac{\kappa_{1}}{\kappa_{2}}}\)):_

**(S1).** _For positive neurons \(k\in[m/2]\)\((s_{\text{k}}=1)\), let \(\mathcal{K}_{+}\) be the index set of living neurons, i.e. \(\mathcal{K}_{+}:=\{k\in[m/2]:\bm{w}_{k}(T_{\text{I}})\in\mathcal{M}_{+}^{+} \cup\mathcal{M}_{-}^{+}\}\). Then \(0.21m\leq|\mathcal{K}_{+}|\leq 0.29m\). Moreover, for any neuron \(k\in\mathcal{K}_{+}\), it has the following properties_ **(P1)**_**(P2)**_._

**(P1).** _Its norm is small but significant \(:\frac{4.66\sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}\leq\rho_{k}(T_{\text{I}})\leq \frac{12\sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}\)._

**(P2).**_Its direction is strongly aligned with \(\bm{\mu}:\)_

\[\langle\bm{w}_{k}(T_{\mathrm{I}}),\bm{\mu}\rangle\geq\left(1-4.2\sqrt{\kappa_{1} \kappa_{2}}\right)\left(1-\frac{2}{1+0.7\left(1+9.9\sqrt{\frac{\kappa_{2}}{ \kappa_{1}}}\right)^{1.15}}\right).\]

**(S2).**_For negative neurons \(k\in[m]-[m/2]\)\((\mathrm{s}_{k}=-1)\), let \(\mathcal{K}_{-}\) be the index set of living neurons, i.e. \(\mathcal{K}_{-}:=\{k\in[m]-[m/2]:\bm{w}_{k}(T_{\mathrm{I}})\in\mathcal{M}_{+} ^{+}\cup\mathcal{M}_{-}^{+}\}\). Then \(0.075m\leq|\mathcal{K}_{-}|\leq 0.205m\). Moreover, for any neuron \(k\in\mathcal{K}_{-}\), it has the following properties_ **(N1)(N2)(N3).**__

**(N1).** _Its norm is tiny \(:\rho_{k}(T_{\mathrm{I}})\leq\frac{\sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}} \Big{(}4.3\sqrt{\frac{\kappa_{1}}{\kappa_{2}}}+\frac{11.1\sin\Delta}{1+p} \Big{)}\)._

**(N2).** _It lies on a manifold perpendicular to \(\bm{x}_{+}:\bm{w}_{k}(t)\in\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^{+}\)._

**(N3).** _Its direction is weakly aligned with \(\bm{x}_{+}^{\perp}:\big{\langle}\bm{w}_{k}(T_{\mathrm{I}}),\bm{x}_{+}^{\perp} \big{\rangle}>1-\frac{2}{\Big{(}1+2.32\sqrt{\frac{\kappa_{2}}{\kappa_{1}}\sin \Delta}{1+p}\Big{)}^{1.6}+1}\)._

**(S3).** _For other neurons \(k\notin\mathcal{K}_{+}\cup\mathcal{K}_{-}\), it will remain dead forever:_

\[\bm{w}_{k}(T_{\mathrm{I}})\in\mathcal{M}_{+}^{-}\cap\mathcal{M}_{-}^{-},\quad \bm{b}_{k}(t)\equiv\bm{b}_{k}(T_{\mathrm{I}}),\quad\forall t\in[T_{\mathrm{I} },+\infty)\]

**(S4).** _The predictions for \(\bm{x}_{+}\) and \(\bm{x}_{-}\) have the estimate:_

\[0.978\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}\Big{(}(\frac{p-1}{p+1 })^{2}-0.11\Big{)}\leq f_{+}(T_{\mathrm{I}})\leq 3.85\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}},\] \[0.947\left((\frac{p-1}{p+1})^{2}\cos\Delta-0.2\right)\leq f_{-}(T_{\mathrm{I}})\leq 3.85\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}},\]

_and the training accuracy is \(\mathrm{Acc}(T_{\mathrm{I}})=\frac{p}{1+p}\)._

**(S5).** \(0<0.258\leq\frac{0.075}{0.29}\leq\frac{|\mathcal{K}_{-}|}{|\mathcal{K}_{+}|} \leq\frac{0.205}{0.21}\leq 0.977<1\).

_Proof of Theorem c.12._

This theorem is a corollary of Lemma C.3, Lemma C.4, Lemma C.5, Lemma C.6, Lemma C.7, Lemma C.8, Lemma C.9, Lemma C.10, and Lemma C.11. We focus on the end of Phase I: \(T_{\mathrm{I}}=10\sqrt{\frac{\kappa_{1}}{\kappa_{2}}}\).

Proof of **(S1)(S2).** From Lemma C.11, we know that: if \(m=\Omega\left(\log(1/\delta)\right)\), then with probability at least \(1-\delta\), we have:

\[\Big{|}\#\Big{\{}k\in[m/2]:\langle\bm{w}_{k}(0),\bm{x}_{+}\rangle >0\Big{\}}-\frac{m}{4}\Big{|}\leq 0.04m,\] \[\#\Big{\{}k\in[m]-[m/2]:\langle\bm{w}_{k}(0),\bm{x}_{-}\rangle>0, \langle\bm{w}_{k}(0),\bm{x}_{-}\rangle>A\,\langle\bm{w}_{k}(0),\bm{x}_{+} \rangle\Big{\}}\geq 0.075m,\] \[\#\Big{\{}k\in[m]-[m/2]:\langle\bm{w}_{k}(0),\bm{x}_{-}\rangle>0, \langle\bm{w}_{k}(0),\bm{x}_{+}\rangle\leq B\,\langle\bm{w}_{k}(0),\bm{x}_{-} \rangle\Big{\}}\leq 0.205m.\]

where \(A=\frac{(1+4.73\kappa_{1}\kappa_{2})p\cos\Delta-(1-4.73\kappa_{1}\kappa_{2})} {(1-4.73\kappa_{1}\kappa_{2})p-(1+4.73\kappa_{1}\kappa_{2})\cos\Delta}\) and \(B=\frac{(1+4.73\kappa_{1}\kappa_{2})p-(1-4.73\kappa_{1}\kappa_{2})\cos\Delta}{( 1-4.73\kappa_{1}\kappa_{2})p\cos\Delta-(1+4.73\kappa_{1}\kappa_{2})}\).

Recalling the dynamics analysis in Lemma C.3, C.4, C.5, and C.6, we have:

\[0.21m\leq|\mathcal{K}_{+}|=\#\Big{\{}k\in[m/2]:\langle\bm{w}_{k}(0),\bm{x}_{+} \rangle>0\Big{\}}\leq 0.29m.\]

Recalling the dynamics analysis in Lemma C.7, C.8 C.9, and C.10, we have:

\[|\mathcal{K}_{-}|\geq\#\Big{\{}k\in[m]-[m/2]:\langle\bm{w}_{k}(0),\bm{x}_{-} \rangle>0,\langle\bm{w}_{k}(0),\bm{x}_{-}\rangle>A\,\langle\bm{w}_{k}(0),\bm{x} _{+}\rangle\Big{\}}\geq 0.075m,\] \[|\mathcal{K}_{-}|\leq\#\Big{\{}k\in[m]-[m/2]:\langle\bm{w}_{k}(0), \bm{x}_{-}\rangle>0,\langle\bm{w}_{k}(0),\bm{x}_{+}\rangle\leq B\,\langle\bm{w} _{k}(0),\bm{x}_{-}\rangle\Big{\}}\leq 0.205m.\]Moreover, the estimates in Lemma C.3, C.4, C.5, and C.6 ensure that for any \(k\in\mathcal{K}_{+}\), the following results hold:

\[\left\langle\bm{w}_{k}(T_{\mathrm{I}}),\bm{\mu}\right\rangle\geq \left(1-4.2\sqrt{\kappa_{1}\kappa_{2}}\right)\left(1-\frac{2}{1+0.7\left(1+9.9 \sqrt{\frac{\kappa_{2}}{\kappa_{1}}}\right)^{1.15}}\right);\] \[\frac{4.66\sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}\leq\rho_{k}(T_{ \mathrm{I}})\leq\frac{12\sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}.\]

Similarly, the estimates in Lemma C.7, C.8 C.9, and C.10 ensure that for any \(k\in\mathcal{K}_{-}\), the following results hold:

\[\rho_{k}(T_{\mathrm{I}})\leq\frac{\sqrt{\kappa_{1}\kappa_{2}}}{ \sqrt{m}}\Big{(}4.3\sqrt{\frac{\kappa_{1}}{\kappa_{2}}}+\frac{11.1\sin\Delta}{ 1+p}\Big{)};\] \[\bm{w}_{k}(T_{\mathrm{I}})\in\mathcal{M}_{+}^{0}\cap\mathcal{M}_ {+}^{+};\] \[\left\langle\bm{w}_{k}(T_{\mathrm{I}}),\bm{x}_{+}^{\perp}\right\rangle >1-\frac{2}{\left(1+2.32\sqrt{\frac{\kappa_{2}}{\kappa_{1}}}\frac{\sin\Delta}{ 1+p}\right)^{1.6}+1}.\]

Proof of **(S3)**. A direct corollary of Lemma C.3, C.4, C.5, C.6, C.7, C.8, C.9, C.10.

Proof of **(S4)**. **(S4)** are direct corollaries of **(S1)(S2)**.

For \(f_{+}(T_{\mathrm{I}})\), we have the following estimate:

\[f_{+}(T_{\mathrm{I}})=\sum_{k\in\mathcal{K}_{+}}a_{k}\sigma\left( \bm{b}_{k}(T_{\mathrm{I}})^{\top}\bm{x}_{+}\right)+\sum_{k\in\mathcal{K}_{-}} a_{k}\sigma\left(\bm{b}_{k}(T_{\mathrm{I}})^{\top}\bm{x}_{+}\right)\] \[=\sum_{k\in\mathcal{K}_{+}}a_{k}\sigma\left(\bm{b}_{k}(T_{\mathrm{ I}})^{\top}\bm{x}_{+}\right)+0=\sum_{k\in\mathcal{K}_{+}}\frac{\kappa_{2}}{ \sqrt{m}}\rho_{k}(T_{\mathrm{I}})\sigma\left(\bm{w}_{k}(T_{\mathrm{I}})^{\top} \bm{x}_{+}\right)\] \[\geq\sum_{k\in\mathcal{K}_{+}}\frac{\kappa_{2}}{\sqrt{m}}\frac{4. 66\sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}\left\langle\bm{w}_{k}(T_{\mathrm{I}}),\bm{x}_{+}\right\rangle\geq\sum_{k\in\mathcal{K}_{+}}\frac{\kappa_{2}}{\sqrt {m}}\frac{4.66\sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}\Big{(}\left\langle\bm{ \mu},\bm{x}_{+}\right\rangle-\left\|\bm{w}_{k}(T_{\mathrm{I}})-\bm{\mu}\right\| \Big{)}\] \[=\sum_{k\in\mathcal{K}_{+}}\frac{\kappa_{2}}{\sqrt{m}}\frac{4.66 \sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}\Big{(}\left\|\bm{z}\right\|\left\langle \bm{z},\bm{x}_{+}\right\rangle-2+2\left\langle\bm{w}_{k}(T_{\mathrm{I}}),\bm {\mu}\right\rangle\Big{)}\] \[\geq\] \[\geq 0.978\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}\Bigg{(}\left(\frac{p- 1}{p+1}\right)^{2}-8.4\sqrt{\kappa_{1}\kappa_{2}}-\frac{4}{1+0.7\left(1+9.9 \sqrt{\frac{\kappa_{2}}{\kappa_{1}}}\right)^{1.15}}\Bigg{)}\] \[\overset{\eqref{eq:f_+}}{\geq} 0.978\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}\Big{(}(\frac{p-1}{p+1}) ^{2}-0.11\Big{)};\] \[\overset{\eqref{eq:f_+}}{\geq} \sum_{k\in\mathcal{K}_{+}}\frac{\kappa_{2}}{\sqrt{m}}\rho_{k}(T_{ \mathrm{I}})\sigma\left(\bm{w}_{k}(T_{\mathrm{I}})^{\top}\bm{x}_{+}\right)\] \[\leq\sum_{k\in\mathcal{K}_{+}}\frac{\kappa_{2}}{\sqrt{m}}\frac{12 \sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}\left\langle\bm{w}_{k}(T_{\mathrm{I}}), \bm{x}_{+}\right\rangle\leq\sum_{k\in\mathcal{K}_{+}}\frac{\kappa_{2}}{\sqrt{m} }\frac{12\sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}\Big{(}\left\langle\bm{\mu},\bm{ x}_{+}\right\rangle+\left\|\bm{w}_{k}(T_{\mathrm{I}})-\bm{\mu}\right\| \Big{)}\] \[=\sum_{k\in\mathcal{K}_{+}}\frac{\kappa_{2}}{\sqrt{m}}\frac{12 \sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}\Big{(}\left\|\bm{z}\right\|\left\langle \bm{z},\bm{x}_{+}\right\rangle+2-2\left\langle\bm{w}_{k}(T_{\mathrm{I}}),\bm {\mu}\right\rangle\Big{)}\]\[\leq |\mathcal{K}_{+}|\frac{12\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}}{m} \Bigg{(}1\cdot\frac{p-\cos\Delta}{p+1}+2-2+8.4\sqrt{\kappa_{1}\kappa_{2}}+\frac {4}{1+0.7\left(1+9.9\sqrt{\frac{\kappa_{2}}{\kappa_{1}}}\right)^{1.15}}\Bigg{)}\] \[\leq 0.29\cdot 12\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}\Bigg{(}1\cdot\frac{ p-\cos\Delta}{p+1}+8.4\sqrt{\kappa_{1}\kappa_{2}}+\frac{4}{1+0.7\left(1+9.9 \sqrt{\frac{\kappa_{2}}{\kappa_{1}}}\right)^{1.15}}\Bigg{)}\] \[\stackrel{{\eqref{eq:2.1}}}{{\leq}} 3.48\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}\Big{(}1+0.084+\frac{4} {1+0.7(1+99)^{1.15}}\Big{)}\leq 3.85\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}.\]

Then we have:

\[0.978\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}\Big{(}(\frac{p-1}{p+1})^{2}-0.11 \Big{)}\leq f_{+}(T_{\rm l})\leq 3.85\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}.\]

In the same way, we can estimate \(f_{-}(T_{\rm l})\):

\[f_{-}(T_{\rm l})=\sum_{k\in\mathcal{K}_{+}}a_{k}\sigma\left( \boldsymbol{b}_{k}(T_{\rm l})^{\top}\boldsymbol{x}_{-}\right)+\sum_{k\in \mathcal{K}_{-}}a_{k}\sigma\left(\boldsymbol{b}_{k}(T_{\rm l})^{\top} \boldsymbol{x}_{-}\right)\] \[\geq \sum_{k\in\mathcal{K}_{+}}\frac{\kappa_{2}}{\sqrt{m}}\frac{4.66 \sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}\left\langle\boldsymbol{w}_{k}(T_{\rm l }),\boldsymbol{x}_{-}\right\rangle-\sum_{k\in\mathcal{K}_{-}}\frac{\kappa_{2} }{\sqrt{m}}\frac{\sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}\Big{(}4.3\sqrt{\frac{ \kappa_{1}}{\kappa_{2}}}+\frac{11.1\sin\Delta}{1+p}\Big{)}\left\langle \boldsymbol{w}_{k}(T_{\rm l}),\boldsymbol{x}_{-}\right\rangle\] \[\geq \sum_{k\in\mathcal{K}_{+}}\frac{\kappa_{2}}{\sqrt{m}}\frac{4.66 \sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}\Big{(}\left\langle\boldsymbol{\mu}, \boldsymbol{x}_{-}\right\rangle-\left\|\boldsymbol{w}_{k}(T_{\rm l})- \boldsymbol{\mu}\right\|\Big{)}-\sum_{k\in\mathcal{K}_{-}}\frac{\kappa_{2}}{ \sqrt{m}}\frac{\sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}\Big{(}0.43+\frac{11.1}{ 12}\Big{)}\sin\Delta\] \[\geq |\mathcal{K}_{+}|\frac{4.66\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}} {m}\Bigg{(}\frac{p-1}{p+1}\frac{p\cos\Delta-1}{p+1}-2+2\Big{(}1-4.2\sqrt{ \kappa_{1}\kappa_{2}}-\frac{2}{1+0.7\left(1+9.9\sqrt{\frac{\kappa_{2}}{\kappa _{1}}}\right)^{1.15}}\Big{)}\Bigg{)}\] \[-|\mathcal{K}_{-}|\frac{\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}}{m} \cdot 1.355\sin\Delta\] \[\geq 0.21\cdot 4.66\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}\Bigg{(}\frac{p-1}{ p+1}\frac{p\cos\Delta-1}{p+1}-0.11\Bigg{)}-0.205\cdot 1.355\sin\Delta\kappa_{2}\sqrt{ \kappa_{1}\kappa_{2}}\] \[\geq \kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}\left(0.978(\frac{p-1}{p+1} )^{2}\cos\Delta-0.11-0.28\sin\Delta\right)\geq \kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}\left(0.947(\frac{p-1}{p+1})^{2}\cos \Delta-0.11-0.07\right)\] \[\geq 0.947\left((\frac{p-1}{p+1})^{2}\cos\Delta-0.2\right);\]

\[f_{-}(T_{\rm l})=\sum_{k\in\mathcal{K}_{+}}a_{k}\sigma\left( \boldsymbol{b}_{k}(T_{\rm l})^{\top}\boldsymbol{x}_{-}\right)+\sum_{k\in \mathcal{K}_{-}}a_{k}\sigma\left(\boldsymbol{b}_{k}(T_{\rm l})^{\top} \boldsymbol{x}_{-}\right)\] \[\leq \sum_{k\in\mathcal{K}_{+}}\frac{\kappa_{2}}{\sqrt{m}}\frac{12 \sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}\left\langle\boldsymbol{w}_{k}(T_{\rm l }),\boldsymbol{x}_{-}\right\rangle-0\] \[= \sum_{k\in\mathcal{K}_{+}}\frac{\kappa_{2}}{\sqrt{m}}\frac{12 \sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}\Big{(}\left\|\boldsymbol{z}\right\| \left\langle\boldsymbol{z},\boldsymbol{x}_{-}\right\rangle+2-2\left\langle \boldsymbol{w}_{k}(T_{\rm l}),\boldsymbol{\mu}\right\rangle\Big{)}\] \[\leq |\mathcal{K}_{+}|\frac{12\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}}{m} \Bigg{(}1\cdot\frac{p\cos\Delta-1}{p+1}+2-2+8.4\sqrt{\kappa_{1}\kappa_{2}}+ \frac{4}{1+0.7\left(1+9.9\sqrt{\frac{\kappa_{2}}{\kappa_{1}}}\right)^{1.15}} \Bigg{)}\] \[\leq 0.29\cdot 12\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}\Bigg{(}1\cdot\frac{ p\cos\Delta-1}{p+1}+8.4\sqrt{\kappa_{1}\kappa_{2}}+\frac{4}{1+0.7\left(1+9.9 \sqrt{\frac{\kappa_{2}}{\kappa_{1}}}\right)^{1.15}}\Bigg{)}\] \[\stackrel{{\eqref{eq:2.1}}}{{\leq}} 3.48\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}\Big{(}1+0.084+\frac{4} {1+0.7(1+99)^{1.15}}\Big{)}\leq 3.85\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}.\]Them we have:

\[0.947\left((\frac{p-1}{p+1})^{2}\cos\Delta-0.2\right)\leq f_{-}(T_{\text{I}})\leq 3.85\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}.\]

Due to \(f_{+}(T_{\text{I}})>0\) and \(f_{-}(T_{\text{I}})>0\), we obtain \(\operatorname{ACC}(T_{\text{I}})=\frac{p}{1+p}\).

Moreover, from Theorem C.12 (S1)(S2), we have

\[0<0.258\leq\frac{0.075m}{0.29m}\leq\alpha=\frac{|\mathcal{K}_{-}|}{|\mathcal{ K}_{+}|}=\frac{m_{-}}{m_{+}}\leq\frac{0.205m}{0.21m}\leq 0.977<1.\] (18)

**Remark C.13**.: The results in the following proofs are all based on the occurrence of the events in Theorem C.12. All of these results use the same settings as Theorem 4.1, except using a stronger condition on the initialization parameters (3) than (6) in Theorem C.12. So they all hold with probability at least \(1-\delta\).

Proofs of Optimization Dynamics in Phase II

In this phase, we study the dynamics before the patterns of living neurons change again after Phase I. Specifically, we define

\[T_{\Pi}:=\inf\{t>T_{\mathrm{I}}:\exists k\in\mathcal{K}_{+}\cup\mathcal{K}_{-}, \,\mathsf{sgn}_{k}^{+}(t)\neq\mathsf{sgn}_{k}^{+}(T_{\mathrm{I}})\text{ or }\mathsf{sgn}_{k}^{-}(t)\neq\mathsf{sgn}_{k}^{-}(T_{ \mathrm{I}})\},\]

and call \(t\in(T_{\mathrm{I}},T_{\mathrm{II}}]\) Phase II.

Recalling the results in Theorem 4.1, during Phase II, the activation patterns do not change with \(\mathsf{sgn}_{k}^{+}(t)=\mathsf{sgn}_{k}^{-}(t)=1\) for \(k\in\mathcal{K}_{+}\) and \(\mathsf{sgn}_{k}^{+}(t)=0,\mathsf{sgn}_{k}^{-}(t)=1\) for \(k\in\mathcal{K}_{-}\). Theorem 4.4 demonstrates that at the end of Phase II, except for one of living positive neuron \(k_{0}\in\mathcal{K}_{+}\) precisely changes its pattern on \(\bm{x}_{-}\), all other activation patterns remain unchanged.

Recall that at the end of Phase I, (i) the neuron \(k\notin\mathcal{K}_{+}\cup\mathcal{K}_{-}\) is dead forever; (ii) as for the living neuron \(k\in\mathcal{K}_{+}\cup\mathcal{K}_{-}\), the activation patterns are:

\[\mathsf{sgn}_{k}^{+}(t)=\mathsf{sgn}_{k}^{-}(t)=1\text{ for }k \in\mathcal{K}_{+};\] \[\mathsf{sgn}_{k}^{+}(t)=0,\mathsf{sgn}_{k}^{-}(t)=1\text{ for }k \in\mathcal{K}_{-}.\]

In this section, we will focus on the Phase when the negative neuron \(k\in\mathcal{K}_{-}\) still stays on the manifold \(\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^{+}\) (hence is still dead for \(\bm{x}_{+}\)) and the positive neuron \(k\in\mathcal{K}_{+}\) is still activated for \(\bm{x}_{-}\). In general, we aim to estimate the

As stated in the main text, we define as following hitting time:

\[\begin{split} T_{\Pi}&:=\inf\Big{\{}t>T_{\mathrm{I} }:\exists k\in\mathcal{K}_{+}\cup\mathcal{K}_{-},\mathsf{sgn}_{k}^{+}(t)\neq \mathsf{sgn}_{k}^{+}(T_{\mathrm{I}})\text{ or }\mathsf{sgn}_{k}^{-}(t)\neq\mathsf{ sgn}_{k}^{-}(T_{\mathrm{I}})\Big{\}}\\ &=\inf\Big{\{}t>T_{\mathrm{I}}:\exists k\in\mathcal{K}_{+},\text{ s.t. }\left\langle\bm{w}_{k}(t),\bm{x}_{+}\right\rangle\leq 0\text{ or }\left\langle\bm{w}_{k}(t),\bm{x}_{-}\right\rangle\leq 0,\\ \text{ or }\exists k\in\mathcal{K}_{-},\text{ s.t. }\left\langle\bm{w}_{k}(t),\bm{x}_{+}\right\rangle\neq 0 \text{ or }\left\langle\bm{w}_{k}(t),\bm{x}_{-}\right\rangle\leq 0\Big{\}},\end{split}\] (19)

and we call \(T_{\mathrm{I}}\leq t\leq T_{\mathrm{I}\mathrm{I}}\) "Phase II".

First, we define a more relaxed hitting time than \(T_{\mathrm{II}}\), only about the change of living positive neurons:

\[T_{\Pi}^{+}:=\inf\Big{\{}t>T_{\mathrm{I}}:\ \exists k\in\mathcal{K}_{+},\text{ s.t. }\left\langle\bm{w}_{k}(t),\bm{x}_{+}\right\rangle\leq 0\text{ or }\left\langle\bm{w}_{k}(t),\bm{x}_{-}\right\rangle\leq 0\Big{\}}.\] (20)

Noticing that the changes in activation partitions are essentially caused by the change of discontinuous vector fields, we first define the following auxiliary hitting time:

\[\begin{split} T_{\Pi}^{*}&:=T_{\Pi}^{+}\wedge\inf \big{\{}t>T_{\mathrm{I}}:\left\langle\bm{F}_{+}(t),\bm{x}_{+}\right\rangle\leq 0 \big{\}},\\ \text{ where }\bm{F}_{+}(t)&=\frac{p}{1+p}e^{-f_{+}(t)} \bm{x}_{+}-\frac{1}{1+p}e^{f_{-}(t)}\bm{x}_{-}.\end{split}\] (21)

We call \(T_{\mathrm{I}}\leq t\leq T_{\mathrm{II}}^{*}\) "Phase II".

In the subsequent proof, we first meticulously characterize the optimization dynamics in Phase II* and then prove \(T_{\mathrm{II}}=T_{\mathrm{II}}^{+}=T_{\mathrm{II}}^{*}\). The crucial proof technique is fine-grained prior estimations for \(2\)d ODEs on \(f_{+}(t)\) and \(f_{-}(t)\), leading to the vector field estimation.

To begin with, we establish the following lemma about the optimization dynamics of living neurons.

**Lemma D.1** (Dynamics of living neurons in Phase II*).:

_In Phase II*, \(t\in[T_{\mathrm{I}},T_{\mathrm{II}}^{*}]\), we have the following dynamics for each neuron \(k\in\mathcal{K}_{-}\cup\mathcal{K}_{+}\)._

_(S1) For positive neuron \(k\in\mathcal{K}_{+}\), we have:_

\[\begin{split}&\bm{w}_{k}(t)\in\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^ {+},\\ &\frac{\mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t}=\frac{\kappa_{2}}{ \sqrt{m}}\bm{F}_{+}(t)=\frac{\kappa_{2}}{\sqrt{m}}\left(\frac{p}{1+p}e^{-f_{+ }(t)}\bm{x}_{+}-\frac{1}{1+p}e^{f_{-}(t)}\bm{x}_{-}\right).\end{split}\]

_(S2). For negative neuron \(k\in\mathcal{K}_{-}\), we have:_

\[\begin{split}&\bm{w}_{k}(t)\in\mathcal{M}_{+}^{0}\cap\mathcal{M}_{ -}^{+},\\ &\frac{\mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t}=\frac{\kappa_{2}e^{f _{-}(t)}}{\sqrt{m}(1+p)}\Big{(}\bm{x}_{-}-\bm{x}_{+}\cos\Delta\Big{)}.\end{split}\]Proof of Lemma D.1.:

(S1) Let \(k\in\mathcal{K}_{+}\). Recalling the definition of \(T_{\Pi\!\!1}^{*}\), it holds that \(\langle\bm{w}_{k}(t),\bm{x}_{+}\rangle>0\) and \(\langle\bm{w}_{k}(t),\bm{x}_{-}\rangle>0\) for any \(T_{\mathrm{I}}\leq t\leq T_{\mathrm{I}\!\!1}^{*}\), so the dynamics holds.

(S2) Let \(k\in\mathcal{K}_{-}\). Recalling the definition of \(T_{\mathrm{I}\!\!1}^{*}\), it holds \(\langle\bm{F}_{+}(t),\bm{x}_{+}\rangle>0\) for any \(T_{\mathrm{I}}\leq t\leq T_{\mathrm{I}\!\!1}^{*}\). Due to \(\bm{w}_{k}(T_{\mathrm{I}\!\!1})\in\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^{+}\), we first analysis the vector field around the manifold \(\mathcal{P}_{+}^{0}\cap\mathcal{P}_{-}^{+}\) for \(T_{\mathrm{I}}\leq t\leq T_{\mathrm{I}\!\!1}^{*}\).

For any \(\tilde{\bm{b}}\in\mathcal{P}_{+}^{0}\cap\mathcal{P}_{-}^{+}\) and \(0<\delta_{0}\ll 1\), we know that \(\mathcal{P}_{+}^{0}\cap\mathcal{P}_{-}^{+}\) separates its neighborhood \(\mathcal{B}(\tilde{\bm{b}},\delta_{0})\) into two domains \(\mathcal{G}_{-}=\{\bm{b}\in\mathcal{B}(\tilde{\bm{b}},\delta_{0}):\langle\bm{b},\bm{x}_{+}\rangle<0\}\) and \(\mathcal{G}_{+}=\{\bm{b}\in\mathcal{B}(\tilde{\bm{b}},\delta_{0}):\langle\bm{b },\bm{x}_{+}\rangle>0\}\). Following Definition H.1, we calculate the limited vector field on \(\tilde{\bm{b}}\) from \(\mathcal{G}_{-}\) and \(\mathcal{G}_{+}\).

(i) The limited vector field \(\bm{F}^{-}\) on \(\tilde{\bm{b}}\) (from \(\mathcal{G}_{-}\)):

\[\frac{\mathrm{d}\bm{b}}{\mathrm{d}t}=\bm{F}^{-},\text{ where }\bm{F}^{-}= \frac{\kappa_{2}}{\sqrt{m}}\frac{1}{1+p}e^{f_{-}(t)}\bm{x}_{-}.\]

(ii) The limited vector field \(\bm{F}^{+}\) on \(\tilde{\bm{b}}\) (from \(\mathcal{G}_{+}\)):

\[\frac{\mathrm{d}\bm{b}}{\mathrm{d}t}=\bm{F}^{+},\text{ where }\bm{F}^{+}=- \frac{\kappa_{2}}{\sqrt{m}}\left(\frac{pe^{-f_{+}(t)}}{1+p}\bm{x}_{+}-\frac{e^{ f_{-}(t)}}{1+p}\bm{x}_{-}\right).\]

(iii) Then we calculate the projections of \(\bm{F}^{-}\) and \(\bm{F}^{+}\) onto \(\bm{x}_{+}\) (the normal to the surface \(\mathcal{P}_{+}^{0}\cap\mathcal{P}_{-}^{+}\)):

\[F_{N}^{-}=\big{\langle}\bm{F}^{-},\bm{x}_{+}\big{\rangle}=\frac{ \kappa_{2}e^{f_{-}(t)}}{\sqrt{m}(1+p)}\cos\Delta,\] \[F_{N}^{+}=\big{\langle}\bm{F}^{+},\bm{x}_{+}\big{\rangle}=\frac{ \kappa_{2}e^{f_{-}(t)}}{\sqrt{m}(1+p)}\cos\Delta-\frac{\kappa_{2}pe^{-f_{+}(t )}}{\sqrt{m}(1+p)}.\]

We further define the hitting time to check whether \(\bm{w}_{k}(t)\in\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^{+}\) for \(T_{\mathrm{I}}\leq t\leq T_{\mathrm{I}\!\!1}^{*}\).

\[\tau_{-}^{+}:=T_{\mathrm{I}\!\!1}^{*}\wedge\inf\{t>T_{\mathrm{I}\!\!1}:\exists k \in\mathcal{K}_{-},\text{ s.t. }\langle\bm{w}_{k}(t),\bm{x}_{-}\rangle\leq 0\}.\]

From the definition of \(T_{\mathrm{I}\!\!1}^{*}\), we know \(\langle\bm{F}_{+}(t),\bm{x}_{+}\rangle>0\) for any \(T_{\mathrm{I}}\leq t\leq T_{\mathrm{I}\!\!1}^{*}\), so \(F_{N}^{+}<0\). And it is clear that \(F_{N}^{-}>0\). Hence, the dynamics corresponds to Case (I) in Definition H.1 (\(F_{N}^{-}>0\) and \(F_{N}^{+}<0\)), which means \(\bm{b}_{k}(t)\) can not leave \(\mathcal{P}_{+}^{0}\) (i.e., \(\langle\bm{b}_{k}(t),\bm{x}_{+}\rangle=0\)) for \(t\in[T_{\mathrm{I}\!\!1},\tau_{-}^{+}]\), and the dynamics of \(\bm{b}_{k}(t)\) satisfies:

\[\frac{\mathrm{d}\bm{b}}{\mathrm{d}t}=\alpha\bm{F}^{+}+(1-\alpha)\bm{F}^{-}, \quad\alpha=\frac{f_{-}^{-}}{f_{N}^{-}-f_{N}^{+}},\,t\in[T_{\mathrm{I}\!\!1}, \tau_{-}^{+}],\]

which is

\[\frac{\mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t}\frac{\kappa_{2}e^{f_{-}(t)}}{ \sqrt{m}(1+p)}\Big{(}\bm{x}_{-}-\bm{x}_{+}\cos\Delta\Big{)},\,\,t\in[T_{ \mathrm{I}\!\!1},\tau_{-}^{+}].\]

By Lemma C.1, we know that the dynamics of \(\bm{w}_{k}(t)\) on \(\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^{+}\) and the dynamics of \(\rho_{k}(t)\) are:

\[\frac{\mathrm{d}\bm{w}_{k}(t)}{\mathrm{d}t}=\frac{\kappa_{2}e^{f_{-}(t)}}{ \rho_{k}(t)\sqrt{m}(1+p)}\Big{(}\bm{x}_{-}-\langle\bm{w}_{k}(t),\bm{x}_{-} \rangle\,\bm{w}_{k}-\bm{x}_{+}\cos\Delta\Big{)}.\]

From this dynamics, for any \(t\in[T_{\mathrm{I}\!\!1},\tau_{-}^{+}]\), we have

\[\langle\bm{b}_{k}(t),\bm{x}_{-}\rangle=\langle\bm{b}_{k}(0),\bm{x}_ {-}\rangle+\int_{0}^{t}\left\langle\frac{\mathrm{d}\bm{b}_{k}(s)}{\mathrm{d}s}, \bm{x}_{-}\right\rangle\mathrm{d}s\] \[=\langle\bm{b}_{k}(0),\bm{x}_{-}\rangle+\int_{0}^{t}\frac{\kappa_{ 2}e^{f_{-}(s)}}{\sqrt{m}(1+p)}\sin^{2}\Delta\mathrm{d}s>\langle\bm{b}_{k}(0), \bm{x}_{-}\rangle>0,\]

which means that \(T_{\mathrm{I}\!\!1}^{*}=\tau_{-}^{+}\).

Noticing that Lemma D.1 determines the activation patterns for living neurons in Phase II*, the next lemma gives the first-order dynamics of \(f_{+}(t)\) and \(f_{-}(t)\).

**Lemma D.2** (First-order dynamics of predictions in Phase II*).: _In Phase II* \((T_{\rm I}\leq t\leq T_{\rm II}^{*})\), we have the following dynamics for \(f_{+}(t)\) and \(f_{-}(t)\):_

\[\frac{{\rm d}f_{+}(t)}{{\rm d}t} =\kappa_{2}^{2}\frac{m_{+}}{m}\Big{(}\frac{pe^{-f_{+}(t)}}{1+p}- \frac{e^{f_{-}(t)}}{1+p}\cos\Delta\Big{)},\] \[\frac{{\rm d}f_{-}(t)}{{\rm d}t} =\kappa_{2}^{2}\frac{m_{+}}{m}\Big{(}\frac{pe^{-f_{+}(t)}}{1+p} \cos\Delta-\frac{e^{f_{-}(t)}}{1+p}\Big{)}-\kappa_{2}^{2}\frac{m_{-}}{m}\frac{ e^{f_{-}(t)}}{1+p}\sin^{2}\Delta.\]

Proof of Lemma D.2.: From the definition of \(T_{\rm II}^{*}\), for any \(T_{\rm I}\leq t\leq T_{\rm II}^{*}\), we have

\[f_{+}(t)=\sum_{k\in\mathcal{K}_{+}}\frac{\kappa_{2}}{\sqrt{m}} \sigma(\bm{b}_{k}^{\top}(t)\bm{x}_{+})-\sum_{k\in\mathcal{K}_{-}}\frac{\kappa_ {2}}{\sqrt{m}}\sigma(\bm{b}_{k}^{\top}(t)\bm{x}_{+})=\sum_{k\in\mathcal{K}_{+} }\frac{\kappa_{2}}{\sqrt{m}}\bm{b}_{k}^{\top}(t)\bm{x}_{+},\] \[f_{-}(t)=\sum_{k\in\mathcal{K}_{+}}\frac{\kappa_{2}}{\sqrt{m}} \sigma(\bm{b}_{k}^{\top}(t)\bm{x}_{-})-\sum_{k\in\mathcal{K}_{-}}\frac{\kappa _{2}}{\sqrt{m}}\sigma(\bm{b}_{k}^{\top}(t)\bm{x}_{-})=\sum_{k\in\mathcal{K}_{+ }}\frac{\kappa_{2}}{\sqrt{m}}\bm{b}_{k}^{\top}(t)\bm{x}_{-}-\sum_{k\in\mathcal{ K}_{-}}\frac{\kappa_{2}}{\sqrt{m}}\bm{b}_{k}^{\top}(t)\bm{x}_{-}.\]

With the help of Lemma D.1, we have the dynamics of predictions:

\[\frac{{\rm d}f_{+}(t)}{{\rm d}t}=\sum_{k\in\mathcal{K}_{+}}\frac {\kappa_{2}}{\sqrt{m}}\left\langle\frac{{\rm d}\bm{b}_{k}(t)}{{\rm d}t},\bm{x}_ {+}\right\rangle=\frac{\kappa_{2}^{2}}{m}\sum_{k\in\mathcal{K}_{+}}\Big{(} \frac{p}{1+p}e^{-f_{+}(t)}-\frac{1}{1+p}e^{f_{-}(t)}\cos\Delta\Big{)}\] \[= \frac{m_{+}}{m}\kappa_{2}^{2}\Big{(}\frac{pe^{-f_{+}(t)}}{1+p}- \frac{e^{f_{-}(t)}}{1+p}\cos\Delta\Big{)}.\]

Due to the specificity of the first-order dynamics, the following lemma gives an second-order **autonomous** dynamics of predictions, which is is the core dynamics in this phase.

**Lemma D.3** (Second-order Autonomous Dynamics of predictions in Phase II*).: _Consider the following two variables:_

\[\begin{cases}\mathcal{U}(t):=\kappa_{2}^{2}\frac{m_{+}}{m}\frac{p}{1+p}e^{-f_ {+}(t)},\\ \mathcal{V}(t):=\kappa_{2}^{2}\frac{m_{+}}{m}\frac{1}{1+p}e^{f_{-}(t)}.\end{cases}\]

_Then the following autonomous dynamics of \(\mathcal{U}(t)\) and \(\mathcal{V}(t)\) hold in Phase II* \((T_{\rm I}\leq t\leq T_{\rm II}^{*})\):_

\[\begin{cases}\frac{{\rm d}\mathcal{U}(t)}{{\rm d}t}=\mathcal{U}(t)\mathcal{V}( t)\cos\Delta-\mathcal{U}^{2}(t),\\ \frac{{\rm d}\mathcal{V}(t)}{{\rm d}t}=\mathcal{U}(t)\mathcal{V}(t)\cos\Delta- \mathcal{V}^{2}(t)\left(1+\alpha\sin^{2}\Delta\right).\end{cases}\]

Proof of Lemma D.3.: Recall the first-order dynamics in Lemma D.2:

\[\begin{cases}\frac{{\rm d}f_{+}(t)}{{\rm d}t}&=\kappa_{2}^{2}\frac{m_{+}}{m} \left(\frac{pe^{-f_{+}(t)}}{1+p}-\frac{e^{f_{-}(t)}}{1+p}\cos\Delta\right),\\ \frac{{\rm d}f_{-}(t)}{{\rm d}t}&=\kappa_{2}^{2}\frac{m_{+}}{m}\left(\frac{pe^{- f_{+}(t)}}{1+p}\cos\Delta-\frac{e^{f_{-}(t)}}{1+p}\right)-\kappa_{2}^{2}\frac{m_{-}}{m} \frac{e^{f_{-}(t)}}{1+p}\sin^{2}\Delta.\end{cases}\]Then this proof is a straight-forward calculation:

\[\frac{\mathrm{d}\mathcal{U}(t)}{\mathrm{d}t}= \kappa_{2}^{2}\frac{m_{+}}{m}\frac{p}{1+p}\frac{\mathrm{d}e^{-f_{+ }(t)}}{\mathrm{d}t}=-\kappa_{2}^{2}\frac{m_{+}}{m}\frac{p}{1+p}e^{-f_{+}(t)} \frac{\mathrm{d}f_{+}(t)}{\mathrm{d}t}\] \[= -\mathcal{U}(t)\frac{\mathrm{d}f_{+}(t)}{\mathrm{d}t}=\mathcal{U} (t)\mathcal{V}(t)\cos\Delta-\mathcal{U}^{2}(t),\] \[\frac{\mathrm{d}\mathcal{V}(t)}{\mathrm{d}t}= \kappa_{2}^{2}\frac{m_{+}}{m}\frac{1}{1+p}\frac{\mathrm{d}e^{f_{-} (t)}}{\mathrm{d}t}=\kappa_{2}^{2}\frac{m_{+}}{m}\frac{1}{1+p}e^{f_{-}(t)} \frac{\mathrm{d}f_{-}(t)}{\mathrm{d}t}\] \[= \mathcal{V}(t)\frac{\mathrm{d}f_{-}(t)}{\mathrm{d}t}=\mathcal{U} (t)\mathcal{V}(t)\cos\Delta-\mathcal{V}^{2}(t)\left(1+\alpha\sin^{2}\Delta \right).\]

Lemma D.3 enlighten us that we only need to study the dynamics of \(\mathcal{U}(t)\) and \(\mathcal{V}(t)\) to study the dynamics in Phase II, where \(\mathcal{U}(t),\mathcal{V}(t)\) satisfies the following autonomous dynamics:

\[\begin{cases}\frac{\mathrm{d}t(t)}{\mathrm{d}t}=\mathcal{U}(t) \mathcal{V}(t)\cos\Delta-\mathcal{U}^{2}(t);\\ \frac{\mathrm{d}\mathcal{V}(t)}{\mathrm{d}t}=\mathcal{U}(t)\mathcal{V}(t)\cos \Delta-\mathcal{V}^{2}(t)\left(1+\alpha\sin^{2}\Delta\right),\end{cases}\qquad t \geq T_{\mathrm{I}};\\ \begin{cases}\mathcal{U}(T_{\mathrm{I}})=\kappa_{2}^{2}\frac{m_{+}}{m}\frac{ p}{1+p}e^{-f_{+}(T_{\mathrm{I}})},\\ \mathcal{V}(T_{\mathrm{I}})=\kappa_{2}^{2}\frac{m_{+}}{m}\frac{1}{1+p}e^{f_{-} (T_{\mathrm{I}})}.\end{cases}\] (22)

The next lemma provides a fine-grained prior estimate of the dynamics (22).

**Lemma D.4** (Fine-grained prior estimate of the dynamics (22)).: _For the dynamics (22), then we have the following results:_

**(S1).**__\(\mathcal{U}(T_{\mathrm{I}})=\Theta(\kappa_{2}^{2})\) _and_ \(\mathcal{V}(T_{\mathrm{I}})=\Theta\Big{(}\frac{\kappa_{2}^{2}}{p}\Big{)}\)_._

**(S2).** _For any_ \(t\geq T_{\mathrm{I}}\)_, we have_ \(\mathcal{U}(t)>\mathcal{V}(t)>0\)_._

**(S3).** _If we define the hitting time_ \(\tau_{1}:=\inf\left\{t\geq T_{\mathrm{I}}:\mathcal{U}(t)\cos\Delta\leq \mathcal{V}(t)\left(1+\alpha\sin^{2}\Delta\right)\right\}\)_, then_

\[\mathcal{U}(\tau_{1})=\frac{1+\alpha\sin^{2}\Delta}{\cos\Delta} \mathcal{V}(\tau_{1}),\quad\mathcal{V}(\tau_{1})=\Theta\left(\kappa_{2}^{2}p^{ -\frac{1}{1+\cos\Delta}}\right),\] \[\tau_{1}=\mathcal{O}\left(\frac{p^{\frac{1}{1+\cos\Delta}}\log(1/ \Delta)}{\kappa_{2}^{2}}\right)=\Omega\left(\frac{p^{\frac{1}{1+\cos\Delta}}}{ \kappa_{2}^{2}}\right)=\tilde{\Theta}\left(\frac{p^{\frac{1}{1+\cos\Delta}}}{ \kappa_{2}^{2}}\right).\]

**(S4).** _For any_ \(t\geq\tau_{1}\)_, we have_

\[1+\frac{m_{-}}{2m_{+}}\sin^{2}\Delta<\frac{\mathcal{U}(t)}{\mathcal{V}(t)}< \frac{1+2\alpha\sin^{2}\Delta}{\cos\Delta},\]

\[\mathcal{U}(t)=\Theta\left(\frac{1}{\frac{p^{\frac{1}{1+\cos\Delta}}}{\kappa_{ 2}^{2}}+\Delta^{2}(t-\tau_{1})}\right),\quad\mathcal{V}(t)=\Theta\left(\frac{ 1}{\frac{p^{\frac{1}{1+\cos\Delta}}}{\kappa_{2}^{2}}+\Delta^{2}(t-\tau_{1})} \right).\]

**(S5).** _For any_ \(t\geq\tau_{1}\)_, we have_ \(\mathcal{U}(t)-\mathcal{V}(t)\cos\Delta=\Theta\left(\Delta^{2}\mathcal{V}(t) \right)>0\)_._

**(S6).** _For any_ \(t\geq\tau_{2}=\Theta\left(\frac{p^{\frac{1}{1+\cos\Delta}}\log(1/\Delta)}{ \kappa_{2}^{2}}\right)\geq 2\tau_{1}\)_, we have_

\[\mathcal{U}(t)\cos\Delta-\mathcal{V}(t)=-\Theta\left(\Delta^{2}\mathcal{V}(t) \right)<0.\]

Proof of Lemma D.4.: For simplicity, in this proof, we denote

\[\epsilon:=\alpha\sin^{2}\Delta.\]Step I. Preparation. From Theorem C.12 (S4), we know \(0<f_{+}(T_{\rm I}),f_{-}(T_{\rm I})\leq 3.85\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}} \leq 0.04\log(1.1)\), so

\[1<e^{f_{-}(T_{\rm I})}\leq 1+e^{0.04\log(1.1)}3.85\kappa_{2}\sqrt{\kappa_{1} \kappa_{2}}\leq 1+1.004\cdot 3.85\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}\leq 1+3.8 7\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}},\]

\[1>e^{-f_{+}(T_{\rm I})}\geq 1-e^{0.04\log(1.1)}3.85\kappa_{2}\sqrt{\kappa_{1} \kappa_{2}}\geq 1-1.004\cdot 3.85\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}\geq 1-3.8 7\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}.\]

Notice that \(\mathcal{U}(T_{\rm I})=\kappa_{2}^{2}\frac{m_{+}}{m}\frac{p}{1+p}e^{-f_{+}(T_{ \rm I})}\) and \(\mathcal{V}(T_{\rm I})=\kappa_{2}^{2}\frac{m_{+}}{m}\frac{1}{1+p}e^{f_{-}(T_{ \rm I})}\). Then we have the estimate:

\[1-3.87\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}\leq \frac{\mathcal{U}(T_{\rm I})}{\kappa_{2}^{2}\frac{m_{+}}{m}\frac{ p}{1+p}}\leq 1,\] \[1\leq \frac{\mathcal{V}(T_{\rm I})}{\kappa_{2}^{2}\frac{m_{+}}{m}\frac{ 1}{1+p}}\leq 1+3.87\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}.\]

From Theorem C.12 (S1)(S2), we have

\[0.258\leq\frac{0.075m}{0.29m}\leq\alpha\leq\frac{0.205m}{0.21m}\leq 0.977.\] (23)

For \(t=T_{\rm I}\), it holds that

\[\mathcal{U}(T_{\rm I})\mathcal{V}(T_{\rm I})\cos\Delta-\mathcal{U }^{2}(T_{\rm I})<0,\] \[\mathcal{U}(T_{\rm I})\mathcal{V}(T_{\rm I})\cos\Delta-\mathcal{V }^{2}(T_{\rm I})\left(1+\epsilon\right)>0.\]

Step II. A rough estimate on \(\mathcal{U}(t)\) and \(\mathcal{V}(t)\). In this step, we aim to prove:

\[\mathcal{U}(t)>\mathcal{V}(t)>0,\quad,\mathcal{U}(t)+\mathcal{V}(t)\leq \mathcal{U}(T_{\rm I})+\mathcal{V}(T_{\rm I}),\quad\forall t\in[T_{\rm I}, \infty).\]

First, from the definition of \(\mathcal{U}(t)\) and \(\mathcal{V}(t)\), we have \(\mathcal{U}(t)>0\) and \(\mathcal{V}(t)>0\).

Then we consider the dynamics of \(\mathcal{U}(t)+\mathcal{V}(t)\). From

\[\frac{\mathrm{d}}{\mathrm{d}t}\Big{(}\mathcal{U}(t)+\mathcal{V}(t)\Big{)}= \mathcal{U}(t)\mathcal{V}(t)\cos\Delta-\mathcal{U}^{2}(t)-\mathcal{V}^{2}(t) \left(1+\epsilon\right)\]

we have

\[\mathcal{U}(t)+\mathcal{V}(t)\leq\mathcal{U}(T_{\rm I})+\mathcal{V}(T_{\rm I} ),\quad\forall t\geq T_{\rm I}.\]

Then we consider the dynamics of \(\mathcal{U}(t)-\mathcal{V}(t)\). We define the hitting time

\[\tau_{\mathcal{U}-\mathcal{V}}:=\inf\Big{\{}t\geq T_{\rm I}:\mathcal{U}(t) \leq\mathcal{V}(t)\Big{\}}.\]

For any \(t\in[T_{\rm I},\tau_{\mathcal{U}-\mathcal{V}})\), we have:

\[\frac{\mathrm{d}}{\mathrm{d}t}\Big{(}\mathcal{U}(t)-\mathcal{V}( t)\Big{)}=-\mathcal{U}^{2}(t)+\mathcal{V}^{2}(t)\left(1+\epsilon\right)=-( \mathcal{U}(t)+\mathcal{V}(t))(\mathcal{U}(t)-\mathcal{V}(t))+\epsilon \mathcal{V}^{2}(t)\] \[> -(\mathcal{U}(t)+\mathcal{V}(t))(\mathcal{U}(t)-\mathcal{V}(t)) \geq-(\mathcal{U}(T_{\rm I})+\mathcal{V}(T_{\rm I}))(\mathcal{U}(t)-\mathcal{V }(t)),\]

We consider the auxiliary ODE: \(\frac{d}{\mathrm{d}t}\mathcal{P}(t)=-(\mathcal{U}(T_{\rm I})+\mathcal{V}(T_{ \rm I}))\mathcal{P}(t)\), where \(\mathcal{P}(T_{\rm I})=\mathcal{U}(T_{\rm I})-\mathcal{V}(T_{\rm I})>0\). From the Comparison Principle of ODEs, we have:

\[\mathcal{U}(t)-\mathcal{V}(t)\geq\mathcal{P}(t)=(\mathcal{U}(T_{\rm I})- \mathcal{V}(T_{\rm I}))\exp\Big{(}-(\mathcal{U}(T_{\rm I})+\mathcal{V}(T_{ \rm I}))(t-T_{\rm I})\Big{)}>0,\;\forall t\in[T_{\rm I},\tau_{\mathcal{U}- \mathcal{V}}).\]

From the definition of \(\tau_{\mathcal{U}-\mathcal{V}}\), we have proved

\[\tau_{\mathcal{U}-\mathcal{V}}=+\infty;\] \[\mathcal{U}(t)>\mathcal{V}(t),\;\forall t\in[T_{\rm I},+\infty).\]

Step III. Finer estimate in the early Phase \(t\in[T_{\rm I},\tau_{\rm I}]\). Define the following hitting time

\[\tau_{1}:=\inf\Big{\{}t\geq T_{\rm I}:\mathcal{U}(t)\cos\Delta\leq\mathcal{V}( t)\left(1+\epsilon\right)\Big{\}}.\]From Step I, we know \(\tau_{1}\) exists and \(\tau_{1}>T_{\rm I}\). From (22), we have \(\frac{{\rm d}{\cal U}(t)}{{\rm d}t}<0\) and \(\frac{{\rm d}{\cal V}(t)}{{\rm d}t}>0\) when \(t\in[T_{\rm I},\tau_{1})\). Moreover, we have the following dynamics for \(t\in[T_{\rm I},\tau_{1})\):

\[\frac{{\rm d}{\cal U}}{{\rm d}{\cal V}}=\frac{{\cal U}{\cal V}\cos\Delta-{\cal U }^{2}}{{\cal U}{\cal V}\cos\Delta-{\cal V}^{2}\left(1+\epsilon\right)}=\frac{ \frac{{\cal U}}{{\cal V}}\cos\Delta-\left(\frac{{\cal U}}{{\cal V}}\right)^{2} }{\frac{{\cal U}}{{\cal V}}\cos\Delta-(1+\epsilon)}.\]

If we define \({\cal Z}(t):=\frac{{\cal U}(t)}{{\cal V}(t)}\), then we have \({\rm d}{\cal U}={\cal Z}{\rm d}{\cal V}+{\cal V}{\rm d}{\cal Z}\).

The dynamics above can be transformed to:

\[{\cal V}\frac{{\rm d}{\cal Z}}{{\rm d}{\cal V}}=\frac{{\cal Z}\cos\Delta-{\cal Z }^{2}}{{\cal Z}\cos\Delta-(1+\epsilon)}-{\cal Z},\]

which means

\[\frac{1}{{\cal V}}{\rm d}{\cal V}=-\frac{1}{1+\cos\Delta+\epsilon}\left(\frac {1+\epsilon}{{\cal Z}}+\frac{\sin^{2}\Delta+\epsilon}{(1+\cos\Delta+\epsilon)- {\cal Z}(1+\cos\Delta)}\right){\rm d}{\cal Z}.\]

Integrating this equation from \(T_{\rm I}\) to \(t\in[T_{\rm I},\tau_{1})\), we have:

\[\log\left(\frac{{\cal V}(t)}{{\cal V}(T_{\rm I})}\right)= -\frac{1+\epsilon}{1+\cos\Delta+\epsilon}\log\left(\frac{{\cal Z} (t)}{{\cal Z}(T_{\rm I})}\right)\] \[+\frac{\sin^{2}\Delta+\epsilon}{(1+\cos\Delta+\epsilon)(1+\cos \Delta)}\log\left(\frac{(1+\cos\Delta){\cal Z}(t)-(1+\cos\Delta+\epsilon)}{( 1+\cos\Delta){\cal Z}(T_{\rm I})-(1+\cos\Delta+\epsilon)}\right),\;t\in[T_{ \rm I},\tau_{1}).\] (24)

From the continuity of \({\cal U}(t)\), \({\cal V}(t)\) and \({\cal Z}(t)\), we have

\[\tau_{1}=\inf\Big{\{}t\geq T_{\rm I}:{\cal Z}(t)\leq\frac{1+\epsilon}{\cos \Delta}\Big{\}}.\] (25)

Combining (25) and (24), let \(t\to\tau_{1}^{-}\). Then we have:

\[{\cal Z}(\tau_{1})=\frac{1+\epsilon}{\cos\Delta};\]

\[{\cal V}(\tau_{1})={\cal V}(T_{\rm I})\left(\frac{{\cal Z}(\tau_{1})}{{\cal Z} (T_{\rm I})}\right)^{-\frac{1+\epsilon}{1+\cos\Delta+\epsilon}}\left(\frac{( 1+\cos\Delta){\cal Z}(\tau_{1})-(1+\cos\Delta+\epsilon)}{(1+\cos\Delta){\cal Z }(T_{\rm I})-(1+\cos\Delta+\epsilon)}\right)^{\frac{\sin^{2}\Delta+\epsilon}{ (1+\cos\Delta+\epsilon)(1+\cos\Delta)}}>0,\]

where

\[\left(\frac{1-3.87\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}}{1+3.87\kappa_{2} \sqrt{\kappa_{1}\kappa_{2}}}\right)p\leq{\cal Z}(T_{\rm I})=\frac{{\cal U}(T_{ \rm I})}{{\cal V}(T_{\rm I})}\leq p.\]

Therefore,

\[\frac{{\cal V}(\tau_{1})}{{\cal V}(T_{\rm I})}\leq\left(\frac{p\cos\Delta}{1+ \epsilon}\right)^{\frac{1+\epsilon}{1+\cos\Delta+\epsilon}}\left(\frac{\sin^{2 }\Delta+\epsilon}{((1+\cos\Delta)p-(1+\cos\Delta+\epsilon))\cos\Delta}\right)^ {\frac{\sin^{2}\Delta+\epsilon}{(1+\cos\Delta+\epsilon)(1+\cos\Delta)}}\]

\[\frac{{\cal V}(\tau_{1})}{{\cal V}(T_{\rm I})}\geq\left(\frac{(1-3.87\kappa_{2} \sqrt{\kappa_{1}\kappa_{2}})p\cos\Delta}{(1+3.87\kappa_{2}\sqrt{\kappa_{1} \kappa_{2}})(1+\epsilon)}\right)^{\frac{1+\epsilon}{1+\cos\Delta+\epsilon}} \left(\frac{\sin^{2}\Delta+\epsilon}{\left(\frac{(1-3.87\kappa_{2}\sqrt{ \kappa_{1}\kappa_{2}})(1+\cos\Delta)}{1+3.87\kappa_{2}\sqrt{\kappa_{1}\kappa_{2 }}}p-(1+\cos\Delta+\epsilon)\right)\cos\Delta}\right)^{\frac{\sin^{2}\Delta+ \epsilon}{(1+\cos\Delta+\epsilon)(1+\cos\Delta+\epsilon)}}\]

and

\[{\cal U}(\tau_{1})=\frac{1+\epsilon}{\cos\Delta}{\cal V}(\tau_{1}),\]

where \(1\leq\frac{{\cal V}(T_{\rm I})}{\kappa_{2}^{2}\frac{\tau_{1}}{\tau_{1}}}\leq 1+3.87\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}\) is estimated in Step I.

Step IV. Nearly tight bounds for \(\tau_{1}\)

From the definition of \(\tau_{1}\), we have \(\frac{{\rm d}{\cal V}(t)}{{\rm d}t}>0\) for any \(t\in[T_{\rm I},\tau_{1})\), thus \({\cal V}(T_{\rm I})<{\cal V}(t)<{\cal V}(\tau_{1})\),

\(\forall t\in(T_{\rm I},\tau_{1})\). So we have

\[{\cal U}(t){\cal V}(T_{\rm I})\cos\Delta-{\cal U}^{2}(t)<\frac{{\rm d}{\cal U}(t )}{{\rm d}t}<{\cal U}(t){\cal V}(\tau_{1})\cos\Delta-{\cal U}^{2}(t),\;\forall t \in(T_{\rm I},\tau_{1}).\]We first estimate the upper bound for \(\tau_{1}\). Consider the following dynamics and the hitting time

\[\begin{cases}\frac{\mathrm{d}\phi(t)}{\mathrm{d}t}=\phi(t)\mathcal{V}( \tau_{1})\cos\Delta-\phi^{2}(t),\quad t\geq T_{\mathrm{I}},\\ \phi(T_{\mathrm{I}})=\mathcal{U}(T_{\mathrm{I}}).\end{cases}\]

\[\tau_{1}^{\mathrm{u}}:=\inf\left\{t>T_{\mathrm{I}}:\phi(t)\leq\mathcal{U}( \tau_{1})\right\}\]

Then \(\tau_{1}<\tau_{1}^{\mathrm{u}}\). From the dynamics of \(\phi(t)\), for any \(t\in(T_{\mathrm{I}},\tau_{1}^{\mathrm{u}}]\), it holds

\[\log\left(\frac{\phi(t)}{\phi(t)-\mathcal{V}(\tau_{1})\cos\Delta}\right) \Bigg{|}_{T_{\mathrm{I}}}^{t}=(t-T_{\mathrm{I}})\mathcal{V}(\tau_{1})\cos\Delta.\]

Therefore,

\[\tau_{1}^{\mathrm{u}}-T_{\mathrm{I}}= \frac{1}{\mathcal{V}(\tau_{1})\cos\Delta}\log\left(\frac{\phi( \tau_{1}^{\mathrm{u}})}{\phi(T_{\mathrm{I}})}\frac{(\phi(T_{\mathrm{I}})- \mathcal{V}(\tau_{1})\cos\Delta)}{(\phi(\tau_{1}^{\mathrm{u}})-\mathcal{V}( \tau_{1})\cos\Delta)}\right)\] \[= \frac{1}{\mathcal{V}(\tau_{1})\cos\Delta}\log\left(\frac{ \mathcal{U}(\tau_{1})}{\mathcal{U}(T_{\mathrm{I}})}\frac{(\mathcal{U}(T_{ \mathrm{I}})-\mathcal{V}(\tau_{1})\cos\Delta)}{(\mathcal{U}(\tau_{1})- \mathcal{V}(\tau_{1})\cos\Delta)}\right).\]

With the help of Theorem C.12, we have \(\frac{m_{+}}{m}=\Theta(1)\) and \(\frac{m_{-}}{m}=\Theta(1)\). From Step I, we have \(\mathcal{C}=\Theta\left(\frac{\kappa_{2}^{2}}{\Delta^{2}}\right)\), \(\mathcal{U}(T_{\mathrm{I}})=\Theta(\kappa_{2}^{2})\) and \(\mathcal{V}(T_{\mathrm{I}})=\Theta\Big{(}\frac{\kappa_{2}^{2}}{p}\Big{)}\). Moreover, it holds

\[\frac{\mathcal{V}(\tau_{1})}{\mathcal{V}(T_{\mathrm{I}})}=\Theta \left(p^{\frac{1+\epsilon}{1+\cos\Delta+\epsilon}}\Big{(}\frac{\Delta^{2}}{p} \Big{)}^{\frac{8\ln^{2}\Delta+\epsilon}{(1+\cos\Delta+\epsilon)(1+\cos\Delta )}}\right)=\Theta\left(p^{\frac{\cos\Delta}{1+\cos\Delta}}\frac{2\cdot\sin^{2 \Delta+\epsilon}}{(1+\cos\Delta+\epsilon)(1+\cos\Delta)}\right)=\Theta\left(p ^{\frac{\cos\Delta}{1+\cos\Delta}}\right),\] \[\mathcal{V}(\tau_{1})=\Theta\left(\mathcal{V}(T_{\mathrm{I}})p^{ \frac{\cos\Delta}{1+\cos\Delta}}\right)=\Theta\left(\kappa_{2}^{2}p^{-\frac{1 }{1+\cos\Delta}}\right).\]

It is easy to verify

\[\frac{\mathcal{U}(\tau_{1})}{\mathcal{U}(T_{\mathrm{I}})}=\frac{1+\epsilon}{ \cos\Delta}\frac{\mathcal{V}(\tau_{1})}{\mathcal{U}(T_{\mathrm{I}})}=\Theta \left(\frac{\mathcal{V}(T_{\mathrm{I}})}{\mathcal{U}(T_{\mathrm{I}})}p^{\frac {\cos\Delta}{1+\cos\Delta}}\right)=\Theta\left(p^{-\frac{1}{1+\cos\Delta}} \right);\]

Hence, we obtain the upper bound for \(\tau_{1}\):

\[\tau_{1}\leq\tau_{1}^{\mathrm{u}}=T_{\mathrm{I}}+\Theta\left( \frac{1}{\kappa_{2}^{2}p^{-\frac{1}{1+\cos\Delta}}}\log\left(p^{-\frac{1}{1+ \cos\Delta}}\frac{p^{\frac{1}{1+\cos\Delta}}}{\Delta^{2}}\right)\right)\] \[= \mathcal{O}\left(\sqrt{\frac{\kappa_{1}}{\kappa_{2}}}\right)+\Theta \left(\frac{p^{\frac{1}{1+\cos\Delta}}\log(1/\Delta)}{\kappa_{2}^{2}}\right)= \Theta\left(\frac{p^{\frac{1}{1+\cos\Delta}}\log(1/\Delta)}{\kappa_{2}^{2}} \right).\]

In a similar way, we can derive the lower bound for \(\tau_{1}\). Consider the following dynamics and the hitting time

\[\begin{cases}\frac{\mathrm{d}\psi(t)}{\mathrm{d}t}=\psi(t)\mathcal{V}(T_{ \mathrm{I}})\cos\Delta-\psi^{2}(t),\quad t\geq T_{\mathrm{I}},\\ \psi(T_{\mathrm{I}})=\mathcal{U}(T_{\mathrm{I}}).\end{cases}\]

\[\tau_{1}^{1}:=\inf\left\{t>T_{\mathrm{I}}:\psi(t)\leq\mathcal{U}(\tau_{1})\right\}\]

Then \(\tau_{1}>\tau_{1}^{1}\). From the dynamics of \(\phi(t)\), for any \(t\in(T_{\mathrm{I}},\tau_{1}^{1}]\), it holds

\[\log\left(\frac{\psi(t)}{\psi(t)-\mathcal{V}(T_{\mathrm{I}})\cos\Delta}\right) \Bigg{|}_{T_{\mathrm{I}}}^{t}=(t-T_{\mathrm{I}})\mathcal{V}(T_{\mathrm{I}})\cos\Delta.\]

Therefore,

\[\tau_{1}^{1}-T_{\mathrm{I}}= \frac{1}{\mathcal{V}(T_{\mathrm{I}})\cos\Delta}\log\left(\frac{\phi (\tau_{1}^{\mathrm{u}})}{\phi(T_{\mathrm{I}})}\frac{(\phi(T_{\mathrm{I}})- \mathcal{V}(T_{\mathrm{I}})\cos\Delta)}{(\phi(\tau_{1}^{\mathrm{u}})-\mathcal{ V}(T_{\mathrm{I}})\cos\Delta)}\right)\]\[= \frac{1}{\mathcal{V}(T_{\rm I})\cos\Delta}\log\left(\frac{\mathcal{U}( \tau_{\rm I})}{\mathcal{U}(T_{\rm I})}\left(\frac{\mathcal{U}(T_{\rm I})- \mathcal{V}(T_{\rm I})\cos\Delta}{\mathcal{U}(\tau_{\rm I})-\mathcal{V}(T_{\rm I })\cos\Delta}\right)\right)\] \[= \frac{1}{\mathcal{V}(T_{\rm I})\cos\Delta}\log\left(1+\frac{ \mathcal{V}(T_{\rm I})}{\mathcal{U}(T_{\rm I})}\frac{\left(\mathcal{U}(T_{\rm I })-\mathcal{U}(\tau_{\rm I})\right)\cos\Delta}{\left(\mathcal{U}(\tau_{\rm I} )-\mathcal{V}(T_{\rm I})\cos\Delta\right)}\right).\]

It is easy to verify

\[\frac{\mathcal{V}(T_{\rm I})}{\mathcal{U}(T_{\rm I})}\frac{\left(\mathcal{U}( T_{\rm I})-\mathcal{U}(\tau_{\rm I})\right)\cos\Delta}{\left(\mathcal{U}(\tau_{ \rm I})-\mathcal{V}(T_{\rm I})\cos\Delta\right)}=\Theta\left(\frac{1}{p}\frac {\kappa_{2}^{2}(1-p^{-\frac{1}{1+\cos\Delta}})}{\kappa_{2}^{2}(p^{-\frac{1}{ 1+\cos\Delta}}-p^{-1})}\right)=\Theta\left(p^{-\frac{\cos\Delta}{1+\cos\Delta }}\right),\]

thus

\[\log\left(1+\frac{\mathcal{V}(T_{\rm I})}{\mathcal{U}(T_{\rm I})}\frac{ \left(\mathcal{U}(T_{\rm I})-\mathcal{U}(\tau_{\rm I})\right)\cos\Delta}{ \left(\mathcal{U}(\tau_{\rm I})-\mathcal{V}(T_{\rm I})\cos\Delta\right)} \right)=\Theta\left(p^{-\frac{\cos\Delta}{1+\cos\Delta}}\right),\]

Hence, we obtain the lower bound for \(\tau_{1}\):

\[\tau_{1}\geq\tau_{1}^{1}=T_{\rm I}+\Theta\left(\frac{1}{\frac{\kappa_{2}^{2} }{p}}p^{-\frac{\cos\Delta}{1+\cos\Delta}}\right)=\mathcal{O}\left(\sqrt{\frac {\kappa_{1}}{\kappa_{2}}}\right)+\Theta\left(\frac{p^{\frac{1}{1+\cos\Delta} }}{\kappa_{2}^{2}}\right)=\Theta\left(\frac{p^{\frac{1}{1+\cos\Delta}}}{ \kappa_{2}^{2}}\right).\]

Step V. Finer Estimate in the late Phase \(t>\tau_{1}\).

In this step, we focus on the dynamics when \(t>\tau_{1}\).

First, we will prove the following nearly tight bound about the ratio of \(\mathcal{U}(t)\) to \(\mathcal{V}(t)\):

\[1+\frac{\epsilon}{2}<\frac{\mathcal{U}(t)}{\mathcal{V}(t)}<\frac{1+2\epsilon} {\cos\Delta},\quad\forall t\in[\tau_{1},+\infty).\]

For the right inequality, we define the hitting time

\[\tau_{\mathcal{U}/\mathcal{V}}^{r}:=\inf\Big{\{}t>\tau_{1}:\mathcal{U}(t)\geq \frac{1+2\epsilon}{\cos\Delta}\mathcal{V}(t)\Big{\}}.\]

From \(\frac{\mathcal{U}(\tau_{1})}{\mathcal{V}(\tau_{1})}=\frac{1+\epsilon}{\cos \Delta}<\frac{1+2\epsilon}{\cos\Delta}\), we know \(\tau_{\mathcal{U}/\mathcal{V}}^{r}\) exists and \(\tau_{\mathcal{U}/\mathcal{V}}^{r}>\tau_{1}\).

For any \(t\in(\tau_{1},\tau_{\mathcal{U}/\mathcal{V}}^{r})\), consider

\[\frac{\mathrm{d}}{\mathrm{d}t}\left(\mathcal{U}(t)-\frac{1+2 \epsilon}{\cos\Delta}\mathcal{V}(t)\right)=\left(1-\frac{1+2\epsilon}{\cos \Delta}\right)\mathcal{U}(t)\mathcal{V}(t)\cos\Delta-\mathcal{U}^{2}(t)+\frac {1+2\epsilon}{\cos\Delta}(1+\epsilon)\mathcal{V}^{2}(t)\] \[= -\left(\mathcal{U}(t)-\frac{1+2\epsilon}{\cos\Delta}\mathcal{V}(t )\right)\left(\mathcal{U}(t)+\left((1+2\epsilon)(1+\frac{1}{\cos\Delta})-\cos \Delta\right)\mathcal{V}(t)\right)\] \[\quad+\left(\cos\Delta-(1+2\epsilon)(1+\frac{1}{\cos\Delta})+ \frac{1+2\epsilon}{\cos\Delta}(1+\epsilon)\right)\mathcal{V}^{2}(t)\] \[= -\left(\mathcal{U}(t)-\frac{1+2\epsilon}{\cos\Delta}\mathcal{V}(t )\right)\left(\mathcal{U}(t)+\left((1+2\epsilon)(1+\frac{1}{\cos\Delta})-\cos \Delta\right)\mathcal{V}(t)\right)\] \[\quad+\left((\cos\Delta-1)+(\frac{1+2\epsilon}{\cos\Delta} \epsilon-2\epsilon)\right)\mathcal{V}^{2}(t)\] \[< -\left(\mathcal{U}(t)-\frac{1+2\epsilon}{\cos\Delta}\mathcal{V}(t )\right)\left(\mathcal{U}(t)+\left((1+2\epsilon)(1+\frac{1}{\cos\Delta})-\cos \Delta\right)\mathcal{V}(t)\right)\] \[\stackrel{{\text{Step II}}}{{<}} -\left((1+2\epsilon)(1+\frac{1}{\cos\Delta})-\cos\Delta\right) \left(\mathcal{U}(t)+\mathcal{V}(t)\right)\left(\mathcal{U}(t)-\frac{1+2 \epsilon}{\cos\Delta}\mathcal{V}(t)\right)\] \[\stackrel{{\text{Step II}}}{{\leq}} -\left((1+2\epsilon)(1+\frac{1}{\cos\Delta})-\cos\Delta\right) \left(\mathcal{U}(T_{\rm I})+\mathcal{V}(T_{\rm I})\right)\left(\mathcal{U}(t)- \frac{1+2\epsilon}{\cos\Delta}\mathcal{V}(t)\right).\]

For simplicity, we denote \(C_{1}:=\left((1+2\epsilon)(1+\frac{1}{\cos\Delta})-\cos\Delta\right)\left( \mathcal{U}(T_{\rm I})+\mathcal{V}(T_{\rm I})\right)>0\). We consider the auxiliary ODE: \(\frac{d}{dt}\mathcal{P}(t)=-C_{1}\mathcal{P}(t)\), where \(\mathcal{P}(\tau_{1})=\mathcal{U}(\tau_{1})-\frac{1+2\epsilon}{\cos\Delta} \mathcal{V}(\tau_{1})<0\). From the Comparison Principle of ODEs, we have:

\[\mathcal{U}(t)-\frac{1+2\epsilon}{\cos\Delta}\mathcal{V}(t)\leq\mathcal{P}(t)= \mathcal{P}(\tau_{1})e^{-C_{1}(t-\tau_{1})}<0,\;\forall t\in(\tau_{1},\tau_{ \mathcal{U}/\mathcal{V}}^{r}).\]From the definition of \(\tau^{r}_{\mathcal{U}/\mathcal{V}}\), we have proved

\[\tau^{r}_{\mathcal{U}/\mathcal{V}}=+\infty;\] \[\mathcal{U}(t)<\frac{1+2\epsilon}{\cos\Delta}\mathcal{V}(t),\; \forall t\in[\tau_{1},+\infty).\]

In the same way, it can be proved that

\[\mathcal{U}(t)>(1+\frac{\epsilon}{2})\mathcal{V}(t),\;\forall t\in[\tau_{1},+ \infty).\]

Moreover, we also need to derive a tight bound for \(\mathcal{U}(t)\) and \(\mathcal{V}(t)\) when \(t>\tau_{1}\), respectively.

For any \(t>\tau_{1}\), we have

\[\frac{\mathrm{d}}{\mathrm{d}t}\mathcal{V}(t)=\mathcal{V}(t)\Big{(} \mathcal{U}(t)\cos\Delta-(1+\epsilon)\mathcal{V}(t)\Big{)}\] \[> \mathcal{V}(t)\Big{(}(1+\frac{\epsilon}{2})\mathcal{V}(t)\cos \Delta-(1+\epsilon)\mathcal{V}(t)\Big{)}=-\Big{(}(1+\epsilon)-(1+\frac{ \epsilon}{2})\cos\Delta\Big{)}\mathcal{V}^{2}(t).\]

We consider the auxiliary ODE: \(\frac{d}{\mathrm{d}t}\mathcal{P}(t)=-\Big{(}(1+\epsilon)-(1+\frac{\epsilon}{ 2})\cos\Delta\Big{)}\mathcal{P}^{2}(t)\), where \(\mathcal{P}(\tau_{1})=\mathcal{V}(\tau_{1})\). From the Comparison Principle of ODEs, we have the lower bound for \(\mathcal{V}(t)\):

\[\mathcal{V}(t)\geq\mathcal{P}(t)=\frac{1}{\frac{1}{\mathcal{V}( \tau_{1})}+\Big{(}(1+\epsilon)-(1+\frac{\epsilon}{2})\cos\Delta\Big{)}(t-\tau _{1})},\;\forall t\in(\tau_{1},+\infty).\]

In the same way, for any \(t>\tau_{1}\), we have

\[\frac{\mathrm{d}}{\mathrm{d}t}\mathcal{U}(t)=\mathcal{U}(t)\Big{(} \mathcal{V}(t)\cos\Delta-\mathcal{U}(t)\Big{)}\] \[< \mathcal{U}(t)\Big{(}\mathcal{U}(t)\frac{\cos\Delta}{1+\frac{ \epsilon}{2}}-\mathcal{U}(t)\Big{)}=-\Big{(}1-\frac{\cos\Delta}{1+\frac{ \epsilon}{2}}\Big{)}\mathcal{U}^{2}(t).\]

We consider the auxiliary ODE: \(\frac{d}{\mathrm{d}t}\mathcal{P}(t)=-\Big{(}1-\frac{\cos\Delta}{1+\frac{ \epsilon}{2}}\Big{)}\mathcal{P}^{2}(t)\), where \(\mathcal{P}(\tau_{1})=\mathcal{U}(\tau_{1})\). From the Comparison Principle of ODEs, we have the upper bound for \(\mathcal{U}(t)\):

\[\mathcal{U}(t)\leq\mathcal{P}(t)=\frac{1}{\frac{1}{\mathcal{U}(\tau_{1})}+ \Big{(}1-\frac{\cos\Delta}{1+\frac{\epsilon}{2}}\Big{)}(t-\tau_{1})},\;\forall t \in(\tau_{1},+\infty).\]

The upper bound for \(\mathcal{V}(t)\) and the lower bound for \(\mathcal{U}(t)\) can be estimated by:

\[\mathcal{V}(t)<\frac{\mathcal{U}(t)}{1+\frac{\epsilon}{2}}\leq \frac{1}{1+\frac{\epsilon}{2}}\frac{1}{\frac{1}{\mathcal{U}(\tau_{1})}+\Big{(} 1-\frac{\cos\Delta}{1+\frac{\epsilon}{2}}\Big{)}(t-\tau_{1})},\;\forall t\in( \tau_{1},+\infty),\] \[\mathcal{U}(t)>\Big{(}1+\frac{\epsilon}{2}\Big{)}\,\mathcal{V}(t )\geq\frac{1+\frac{\epsilon}{2}}{\frac{1}{\mathcal{V}(\tau_{1})}+\mathcal{C} \Big{(}(1+\epsilon)-(1+\frac{\epsilon}{2})\cos\Delta\Big{)}(t-\tau_{1})},\; \forall t\in(\tau_{1},+\infty).\]

Hence, we obtain the tight bound for \(\mathcal{U}(t)\) and \(\mathcal{V}(t)\):

\[\frac{1}{\frac{1}{(1+\frac{\epsilon}{2})\mathcal{V}(\tau_{1})}+ \Big{(}\frac{1+\epsilon}{1+\frac{\epsilon}{2}}-\cos\Delta\Big{)}(t-\tau_{1})}< \mathcal{U}(t)\leq\frac{1}{\frac{1}{\mathcal{U}(\tau_{1})}+\Big{(}1-\frac{\cos \Delta}{1+\frac{\epsilon}{2}}\Big{)}(t-\tau_{1})},\;\forall t\in(\tau_{1},+ \infty);\] \[\frac{1}{\frac{1}{\mathcal{V}(\tau_{1})}+\Big{(}(1+\epsilon)-(1+ \frac{\epsilon}{2})\cos\Delta\Big{)}(t-\tau_{1})}\leq\mathcal{V}(t)<\frac{1}{ \frac{1+\frac{\epsilon}{2}}{\mathcal{U}(\tau_{1})}+\Big{(}1+\frac{\epsilon}{2}- \cos\Delta\Big{)}(t-\tau_{1})},\;\forall t\in(\tau_{1},+\infty).\]

It means

\[\mathcal{U}(t)=\Theta\left(\frac{1}{\frac{1}{\frac{1+\cos\Delta}{\kappa_{2}^{2} }}+\Delta^{2}(t-\tau_{1})}\right),\;\forall t\geq\tau_{1}=\mathcal{O}\left( \frac{p^{\frac{1}{1+\cos\Delta}}\log(1/\Delta)}{\kappa_{2}^{2}}\right);\]\[\mathcal{V}(t)=\Theta\left(\frac{1}{\frac{p^{\frac{1}{1+\cos\Delta}{\Delta}}}{ \kappa_{2}^{2}}+\Delta^{2}(t-\tau_{1})}\right),\;\forall t\geq\tau_{1}=\mathcal{ O}\left(\frac{p^{\frac{1}{1+\cos\Delta}}\log(1/\Delta)}{\kappa_{2}^{2}}\right).\]

Step VI. The tight bound for \(\mathcal{U}(t)-\mathcal{V}(t)\cos\Delta\).

From \(1+\frac{\epsilon}{2}<\frac{\mathcal{U}(t)}{\mathcal{V}(t)}<\frac{1+2\epsilon} {\cos\Delta}\) proved in Step V, the two-side bound is straight-forward: for any \(t\geq\tau_{1}\),

\[\mathcal{U}(t)-\mathcal{V}(t)\cos\Delta >\Big{(}1+\frac{\epsilon}{2}-\cos\Delta\Big{)}\mathcal{V}(t)= \Theta\left(\Delta^{2}\mathcal{V}(t)\right),\] \[\mathcal{U}(t)-\mathcal{V}(t)\cos\Delta <\Big{(}\frac{1+2\epsilon}{\cos\Delta}-\cos\Delta\Big{)}\mathcal{V} (t)=\Theta\left(\Delta^{2}\mathcal{V}(t)\right).\]

Then we obtain

\[\mathcal{U}(t)-\mathcal{V}(t)\cos\Delta=\Theta\left(\Delta^{2}\mathcal{V}(t) \right)=\Theta\left(\frac{1}{\frac{p^{\frac{1}{1+\cos\Delta}}}{\kappa_{2}^{2} \Delta^{2}}+(t-\tau_{1})}\right),\quad\forall t\geq\tau_{1}.\]

Step VII. The tight bound of \(\mathcal{U}(t)\cos\Delta-\mathcal{V}(t)\).

If we follow the proof in Step VI, \(1+\frac{\epsilon}{2}<\frac{\mathcal{U}(t)}{\mathcal{V}(t)}<\frac{1+2\epsilon} {\cos\Delta}\) can only gives us a loose two-side bound for \(\mathcal{U}(t)\cos\Delta-\mathcal{V}(t)\):

\[\mathcal{U}(t)\cos\Delta-\mathcal{V}(t)>\Big{(}\cos\Delta+\frac{ m_{-}}{2m_{+}}\cos\Delta-1\Big{)}\mathcal{V}(t)\stackrel{{\eqref{eq:2}}}{{>}}- \Theta\left(\Delta^{2}\mathcal{V}(t)\right),\] \[\mathcal{U}(t)\cos\Delta-\mathcal{V}(t)\cos\Delta<\Big{(}1+2 \epsilon-1\Big{)}\mathcal{V}(t)=\Theta\left(\Delta^{2}\mathcal{V}(t)\right).\]

Hence, we need more fine-grained analysis to derive its sharper bounds.

We first focus on its sharper upper bound. From the dynamics (22), for any \(t\geq T_{\text{I}}\), we have

\[\frac{\mathrm{d}}{\mathrm{d}t}\Big{(}\mathcal{U}(t)\cos\Delta- \mathcal{V}(t)\Big{)}\] \[= \left(-\Big{(}\mathcal{U}(t)\cos\Delta-\mathcal{V}(t)\Big{)} \Big{(}\mathcal{U}(t)+(\frac{1}{\cos\Delta}+1-\cos\Delta)\mathcal{V}(t)\Big{)} -\Big{(}\frac{1}{\cos\Delta}-\cos\Delta-\epsilon\Big{)}\mathcal{V}^{2}(t)\right)\] \[= \left(-\Big{(}\mathcal{U}(t)\cos\Delta-\mathcal{V}(t)\Big{)} \Big{(}\mathcal{U}(t)+(\frac{1}{\cos\Delta}+1-\cos\Delta)\mathcal{V}(t)\Big{)} -\mathcal{V}^{2}(t)\Big{(}\frac{1}{\cos\Delta}-\alpha\Big{)}\sin^{2}\Delta \right).\]

We define the hitting time

\[\tau_{\mathcal{U}/\mathcal{V}}^{+}:=\inf\Big{\{}t>\tau_{1}:\mathcal{U}(t)\cos \Delta-\mathcal{V}(t)\leq 0\Big{\}}.\]

From \(\frac{\mathcal{U}(\tau_{1})}{\mathcal{V}(\tau_{1})}=\frac{1+\epsilon}{\cos\Delta}\), we know \(\tau_{\mathcal{U}/\mathcal{V}}^{+}\) exists and \(\tau_{\mathcal{U}/\mathcal{V}}^{+}>\tau_{1}\).

Then for any \(t\in(\tau_{1},\tau_{\mathcal{U}/\mathcal{V}}^{+})\), we have \(\mathcal{U}(t)\cos\Delta-\mathcal{V}(t)>0\), so

\[\frac{\mathrm{d}}{\mathrm{d}t}\Big{(}\mathcal{U}(t)\cos\Delta- \mathcal{V}(t)\Big{)}\] \[= \left(-\Big{(}\mathcal{U}(t)\cos\Delta-\mathcal{V}(t)\Big{)} \Big{(}\mathcal{U}(t)+(\frac{1}{\cos\Delta}+1-\cos\Delta)\mathcal{V}(t)\Big{)} -\mathcal{V}^{2}(t)\Big{(}\frac{1}{\cos\Delta}-\alpha\Big{)}\sin^{2}\Delta\right)\] \[\stackrel{{\text{Step IV}}}{{<}} -\frac{\Big{(}\mathcal{U}(t)\cos\Delta-\mathcal{V}(t)\Big{)} \Big{(}\frac{1}{\cos\Delta}+2+\frac{\epsilon}{2}-\cos\Delta\Big{)}\mathcal{V}(t )-\mathcal{V}^{2}(t)\Big{(}\frac{1}{\cos\Delta}-\alpha\Big{)}\sin^{2}\Delta \Big{)}}{\frac{1}{\mathcal{V}(\tau_{1})}+\Big{(}(1+\epsilon)-(1+\frac{ \epsilon}{2})\cos\Delta\Big{)}(t-\tau_{1})}-\frac{\Big{(}\frac{1}{\cos\Delta}- \alpha\Big{)}\sin^{2}\Delta}{\Big{(}\frac{1}{\mathcal{V}(\tau_{1})}+\Big{(}(1+ \epsilon)-(1+\frac{\epsilon}{2})\cos\Delta\Big{)}(t-\tau_{1})\Big{)}^{2}}.\]For simplicity, we denote \(A=\frac{1}{V(\tau_{1})}\), \(B=(1+\epsilon)-(1+\frac{\epsilon}{2})\cos\Delta\), \(C_{1}=\frac{\sin^{2}\Delta}{\cos\Delta}+2+\frac{\epsilon}{2}\), \(C_{2}=\left(\frac{1}{\cos\Delta}-\alpha\right)\sin^{2}\Delta\). And we consider the auxiliary ODE:

\[\frac{\mathrm{d}\mathcal{E}(t)}{\mathrm{d}t}=-\frac{C_{1} \mathcal{E}(t)}{A+B(t-\tau_{1})}-\frac{C_{2}}{\left(A+B(t-\tau_{1})\right)^{2}},\] \[\text{where }\mathcal{E}(\tau_{1})=\mathcal{U}(\tau_{1})\cos \Delta-\mathcal{V}(\tau_{1})=\epsilon\mathcal{V}(\tau_{1}).\]

Its solution is

\[\mathcal{E}(t)=\left(1+\frac{B}{A}(t-\tau_{1})\right)^{-\frac{C_{1}}{B}}\left( \mathcal{E}(\tau_{1})-\frac{C_{2}}{A(C_{1}-B)}\left(\left(1+\frac{B}{A}(t- \tau_{1})\right)^{\frac{C_{1}}{B}-1}-1\right)\right).\]

From the Comparison Principle of ODEs, for any \(t\in(\tau_{1},\tau_{\mathcal{U}/\mathcal{V}}^{+})\), we have

\[\mathcal{U}(t)\cos\Delta-\mathcal{V}(t)\leq\mathcal{E}(t).\]

Let \(T_{\mathcal{E}}-\tau_{1}=\frac{A}{B}\left(\left(1+\frac{(C_{1}-B)\epsilon}{C_{ 2}}\right)^{\frac{1}{B}-1}-1\right)\), it is easy to verify \(\mathcal{E}(T_{\mathcal{E}})=0\). Moreover,

\[\frac{A}{B}=\Theta\left(\frac{1}{\mathcal{V}(\tau_{1})\Big{(}(1+ \epsilon)-(1+\frac{\epsilon}{2})\cos\Delta\Big{)}}\right)=\Theta\left(\frac{1 }{\kappa_{2}^{2}p^{-\frac{1}{1+\cos\Delta}\Delta^{2}}}\right)=\Theta\left( \frac{p^{\frac{1}{1+\cos\Delta}\Delta}}{\kappa_{2}^{2}\Delta^{2}}\right);\] \[\frac{1}{\frac{C_{1}}{B}-1}\log\left(1+\frac{(C_{1}-B)\epsilon}{C _{2}}\right)=\Theta\left(\Delta^{2}\log\left(1+\frac{\Theta(\Delta^{2})}{ \Theta(\Delta^{2})}\right)\right)=\Theta(\Delta^{2});\] \[\left(1+\frac{(C_{1}-B)\epsilon}{C_{2}}\right)^{\frac{1}{\mathcal{ U}_{B}-1}}-1=\exp\left(\frac{1}{\frac{C_{1}}{B}-1}\log\left(1+\frac{(C_{1}-B) \epsilon}{C_{2}}\right)\right)-1=\Theta(\Delta^{2}).\]

Therefore,

\[\tau_{\mathcal{U}/\mathcal{V}}^{+}\leq T_{\mathcal{E}}=\tau_{1}+ \Theta\left(\frac{p^{\frac{1}{1+\cos\Delta}}}{\kappa_{2}^{4}\Delta^{2}}\Delta^ {2}\right)\] \[= \mathcal{O}\left(\frac{p^{\frac{1}{1+\cos\Delta}}\log(1/\Delta)} {\kappa_{2}^{2}}\right)+\Theta\left(\frac{p^{\frac{1}{1+\cos\Delta}\Delta}}{ \kappa_{2}^{2}}\right)=\mathcal{O}\left(\frac{p^{\frac{1}{1+\cos\Delta}\log(1 /\Delta)}}{\kappa_{2}^{2}}\right).\]

Then we define the next hitting time

\[\tau_{\mathcal{U}/\mathcal{V}}^{-}:=\inf\Big{\{}t\geq\tau_{\mathcal{U}/ \mathcal{V}}^{+}:\mathcal{U}(t)\cos\Delta-\mathcal{V}(t)\geq 0\Big{\}}.\]

From \(\mathcal{U}(\tau_{\mathcal{U}/\mathcal{V}}^{+})\cos\Delta-\mathcal{V}(\tau_{ \mathcal{U}/\mathcal{V}}^{+})=0\) and \(\frac{\mathrm{d}}{\mathrm{d}t}\Big{(}\mathcal{U}(t)\cos\Delta-\mathcal{V}(t) \Big{)}\Big{|}_{t=\tau_{\mathcal{U}/\mathcal{V}}^{+}}<0\), we know \(\tau_{\mathcal{U}/\mathcal{V}}^{-}\) exists and \(\tau_{\mathcal{U}/\mathcal{V}}^{-}>\tau_{\mathcal{U}/\mathcal{V}}^{+}\).

For any \(t\in(\tau_{\mathcal{U}/\mathcal{V}}^{+},\tau_{\mathcal{U}/\mathcal{V}}^{-})\), we have \(\mathcal{U}(t)\cos\Delta-\mathcal{V}(t)<0\), so

\[\frac{\mathrm{d}}{\mathrm{d}t}\Big{(}\mathcal{U}(t)\cos\Delta- \mathcal{V}(t)\Big{)}\] \[= \left(-\Big{(}\mathcal{U}(t)\cos\Delta-\mathcal{V}(t)\Big{)} \Big{(}\mathcal{U}(t)+(\frac{1}{\cos\Delta}+1-\cos\Delta)\mathcal{V}(t)\Big{)} -\mathcal{V}^{2}(t)\Big{(}\frac{1}{\cos\Delta}-\alpha\Big{)}\sin^{2}\Delta \right)\] \[\overset{\text{Step IV}}{\leq} -\frac{\Big{(}1+\frac{\frac{1}{\cos\Delta}+1-\cos\Delta}{1+\frac{ \zeta}{2}}\Big{)}\Big{(}\mathcal{U}(t)\cos\Delta-\mathcal{V}(t)\Big{)}}{ \frac{1}{\mathcal{U}(\tau_{1})}+\Big{(}1-\frac{\cos\Delta}{1+\frac{\zeta}{2}} \Big{)}(t-\tau_{1})}-\frac{\Big{(}\frac{1}{\cos\Delta}-\alpha\Big{)}\sin^{2} \Delta}{\Big{(}\frac{1}{\mathcal{V}(\tau_{1})}+\Big{(}(1+\epsilon)-(1+\frac{ \epsilon}{2})\cos\Delta\Big{)}(t-\tau_{1})\Big{)}^{2}}\]\[\leq -\frac{\left(1+\frac{\frac{1}{\cos\Delta}+1-\cos\Delta}{1+\frac{1}{2}} \right)\left(\mathcal{U}(t)\cos\Delta-\mathcal{V}(t)\right)}{\frac{\cos\Delta} {1+\epsilon}\frac{1}{V(\tau_{1})}+\frac{1}{3}\Big{(}(1+\epsilon)-(1+\frac{ \epsilon}{2})\cos\Delta\Big{)}(t-\tau_{1})}-\frac{\Big{(}\frac{1}{\cos\Delta}- \alpha\Big{)}\sin^{2}\Delta}{\left(\frac{1}{V(\tau_{1})}+\Big{(}(1+\epsilon)-( 1+\frac{\epsilon}{2})\cos\Delta\Big{)}(t-\tau_{1})\Big{)}^{2}}\] \[\leq -\frac{3\Big{(}1+\frac{\frac{1}{\cos\Delta}+1-\cos\Delta}{1+\frac {1}{2}}\Big{)}\Big{(}\mathcal{U}(t)\cos\Delta-\mathcal{V}(t)\Big{)}}{\frac{1} {V(\tau_{1})}+\Big{(}(1+\epsilon)-(1+\frac{\epsilon}{2})\cos\Delta\Big{)}(t- \tau_{1})}-\frac{\Big{(}\frac{1}{\cos\Delta}-\alpha\Big{)}\sin^{2}\Delta}{ \left(\frac{1}{V(\tau_{1})}+\Big{(}(1+\epsilon)-(1+\frac{\epsilon}{2})\cos \Delta\Big{)}(t-\tau_{1})\Big{)}^{2}}.\]

For simplicity, we denote \(C_{3}=3\Big{(}1+\frac{1}{\frac{\cos\Delta}{1+\frac{1}{2}}}\Delta\Big{)}\). And we consider the auxiliary ODE:

\[\frac{\mathrm{d}\mathcal{F}(t)}{\mathrm{d}t}=-\frac{C_{3}\mathcal{ F}(t)}{A+B(t-\tau_{1})}-\frac{C_{2}}{\left(A+B(t-\tau_{1})\right)^{2}},\] \[\text{where }\mathcal{F}(\tau_{\mathcal{U}/\mathcal{V}}^{+})= \mathcal{U}(\tau_{\mathcal{U}/\mathcal{V}}^{+})\cos\Delta-\mathcal{V}(\tau_{ \mathcal{U}/\mathcal{V}}^{+})=0.\]

Its solution is

\[\mathcal{F}(t)=-\frac{C_{2}}{A(C_{3}-B)\Big{(}1+\frac{B}{A}(t- \tau_{1})\Big{)}^{\frac{C_{3}}{B}}}\left(\Big{(}1+\frac{B}{A}(t-\tau_{1}) \Big{)}^{\frac{C_{3}}{B}-1}-1\right)\] \[\leq -\frac{C_{2}}{AC_{3}\Big{(}1+\frac{B}{A}(t-\tau_{1})\Big{)}} \left(1-\frac{1}{\Big{(}1+\frac{B}{A}(t-\tau_{1})\Big{)}^{\frac{C_{3}}{B}-1}} \right).\]

Let \(\tau_{1}^{\prime}=\tau_{1}+\Theta\left(\frac{p^{\frac{1}{1+\cos\Delta}}\log(1/ \Delta)}{\kappa_{2}^{2}}\right)\geq 2\tau_{1}\). Then for any \(t\geq\tau_{1}^{\prime}\), it holds

\[\frac{1}{\Big{(}1+\frac{B}{A}(t-\tau_{1})\Big{)}^{\frac{C_{3}}{B }-1}}\leq\frac{1}{\Big{(}1+\frac{B}{A}(\tau_{1}^{\prime}-\tau_{1})\Big{)}^{ \frac{C_{3}}{B}-1}}=\exp\left(-\Big{(}\frac{C_{3}}{B}-1\Big{)}\log\Big{(}1+ \frac{B}{A}(\tau_{1}^{\prime}-\tau_{1})\Big{)}\right)\] \[= \exp\left(-\Theta\left(\frac{1}{\Delta^{2}}\right)\log\left(1+ \Theta\left(\frac{\kappa_{2}^{4}\Delta^{2}}{p^{\frac{1}{1+\cos\Delta}}}\frac{ p^{\frac{1}{1+\cos\Delta}}\log(1/\Delta)}{\kappa_{2}^{2}}\right)\right)\right)\] \[= \exp\left(-\Theta\left(\frac{1}{\Delta^{2}}\right)\log\left(1+ \Theta\left(\Delta^{2}\log(1/\Delta)\right)\right)\right)=\exp\left(-\Theta \left(\frac{1}{\Delta^{2}}\right)\Theta\left(\Delta^{2}\log(1/\Delta)\right)\right)\] \[= \exp\left(-\Theta\left(\log(1/\Delta)\right)\right)\leq\frac{1}{2},\]

thus,

\[\mathcal{F}(t)\leq-\frac{C_{2}}{AC_{3}\Big{(}1+\frac{B}{A}(t- \tau_{1})\Big{)}}\Big{(}1-\frac{1}{2}\Big{)}=-\Theta\left(\frac{\Delta^{2}}{ \frac{p^{\frac{1}{1+\cos\Delta}}}{\kappa_{2}^{2}}+\Delta^{2}(t-\tau_{1})} \right),\] \[\forall t\geq\tau_{1}^{\prime}=\tau_{1}+\Theta\left(\frac{p^{ \frac{1}{1+\cos\Delta}}\log(1/\Delta)}{\kappa_{2}^{2}}\right)\geq 2\tau_{1}.\]

If we let \(\tau_{2}=\Theta\left(\frac{p^{\frac{1}{1+\cos\Delta}}\log(1/\Delta)}{\kappa_{2} ^{2}}\right)\geq 2\tau_{1}^{\prime}\), then we have:

\[\mathcal{F}(t)\leq-\Theta\left(\frac{\Delta^{2}}{\frac{p^{\frac{1}{1+\cos\Delta }}}{\kappa_{2}^{2}}+\Delta^{2}(t-\tau_{1})}\right),\;\forall t\geq\tau_{2}.\]

From the Comparison Principle of ODEs, for any \(t\in(\tau_{\mathcal{U}/\mathcal{V}}^{+},\tau_{\mathcal{U}/\mathcal{V}}^{-})\), we have

\[\mathcal{U}(t)\cos\Delta-\mathcal{V}(t)\leq\mathcal{F}(t)<0.\]From the definition of \(\tau^{-}_{\mathcal{U}/\mathcal{V}}\), we obtain

\[\tau^{-}_{\mathcal{U}/\mathcal{V}}=+\infty.\]

Moreover,

\[\mathcal{U}(t)\cos\Delta-\mathcal{V}(t)\leq-\Theta\left(\frac{\Delta^{2}}{\frac{ \Gamma^{\frac{1}{1+\cos\Delta}\Delta}}{\kappa_{2}^{2}}+\Delta^{2}(t-\tau_{1}) }\right),\;\forall t\geq\tau_{2}=\Theta\left(\frac{p^{\frac{1}{1+\cos\Delta}} \log(1/\Delta)}{\kappa_{2}^{2}}\right).\]

Recalling the lower bound at the beginning of Step VII, we obtain the tight bound:

\[\mathcal{U}(t)\cos\Delta-\mathcal{V}(t)=-\Theta\left(\frac{\Delta^{2}}{\frac{ \Gamma^{\frac{1}{1+\cos\Delta}\Delta}}{\kappa_{2}^{2}}+\Delta^{2}(t-\tau_{1}) }\right)=-\Theta\left(\Delta^{2}\mathcal{V}(t)\right),\;\forall t\geq\tau_{2}= \Theta\left(\frac{p^{\frac{1}{1+\cos\Delta}}\log(1/\Delta)}{\kappa_{2}^{2}} \right).\]

\(\Box\)

**Lemma D.5** (Hitting time relationship).: \[T^{+}_{\rm II}=T^{*}_{\rm II}=\inf\left\{t\geq T_{\rm I}:\exists k \in\mathcal{K}_{+},\;{\rm s.t.}\left\langle\bm{b}_{k}(t),\bm{x}_{-}\right\rangle \leq 0\right\}\] \[= \inf\left\{t\geq T_{\rm I}:\exists k\in\mathcal{K}_{+},\;{\rm s.t.}\left\langle\bm{b}_{k}(T_{\rm I}),\bm{x}_{-}\right\rangle+\int_{\mathcal{T} _{\rm I}}^{t}\frac{\sqrt{m}}{\kappa_{2}m_{+}}\Big{(}\mathcal{U}(s)\cos\Delta- \mathcal{V}(s)\Big{)}{\rm d}s\leq 0\right\},\]

_where \(T^{+}_{\rm II}\) and \(T^{*}_{\rm II}\) are defined in (20)(21), and \(\mathcal{U}(t),\mathcal{V}(t)\) satisfy (22)._

_Proof of Lemma D.5._

Recall the definitions of \(T^{+}_{\rm II}\) and \(T^{*}_{\rm II}\):

\[T^{+}_{\rm II}=\inf\Big{\{}t>T_{\rm I}:\exists k\in\mathcal{K}_{ +},\;{\rm s.t.}\;\left\langle\bm{w}_{k}(t),\bm{x}_{+}\right\rangle\leq 0\;{ \rm or}\;\left\langle\bm{w}_{k}(t),\bm{x}_{-}\right\rangle\leq 0\Big{\}}\] \[\quad=\inf\Big{\{}t>T_{\rm I}:\exists k\in\mathcal{K}_{+},\;{\rm s.t.}\;\left\langle\bm{b}_{k}(t),\bm{x}_{+}\right\rangle\leq 0\;{\rm or}\;\left\langle\bm{b}_{k}(t), \bm{x}_{-}\right\rangle\leq 0\Big{\}},\] \[T^{*}_{\rm II}=\inf\Big{\{}t>T_{\rm I}:\left\langle\bm{F}_{+}(t), \bm{x}_{+}\right\rangle\leq 0\;{\rm or}\;\exists k\in\mathcal{K}_{+},\;{\rm s.t.}\;\left\langle\bm{b}_{k}(t),\bm{x}_{+}\right\rangle\leq 0\;{\rm or}\;\left\langle\bm{b}_{k}(t), \bm{x}_{-}\right\rangle\leq 0\Big{\}},\] \[\quad\text{where}\quad\bm{F}_{+}(t)= \frac{p}{1+p}e^{-f_{+}(t)}\bm{x}_{+}-\frac{1}{1+p}e^{f_{-}(t)}\bm{x }_{-}.\]

Notice

\[\left\langle\bm{F}_{+}(t),\bm{x}_{+}\right\rangle=\frac{p}{1+p}e^{-f_{+}(t)}- \frac{1}{1+p}e^{f_{-}(t)}\cos\Delta=\frac{m}{\kappa_{2}^{2}m_{+}}\Big{(} \mathcal{U}(t)-\mathcal{V}(t)\cos\Delta\Big{)}.\]

And for any \(k\in\mathcal{K}_{+}\),

\[\left\langle\bm{b}_{k}(t),\bm{x}_{+}\right\rangle=\left\langle \bm{b}_{k}(T_{\rm I}),\bm{x}_{+}\right\rangle+\int_{T_{\rm I}}^{t}\left\langle \frac{{\rm d}\bm{b}_{k}(s)}{{\rm d}s},\bm{x}_{+}\right\rangle{\rm d}s\] \[= \left\langle\bm{b}_{k}(T_{\rm I}),\bm{x}_{+}\right\rangle+\int_{T_ {\rm I}}^{t}\frac{\sqrt{m}}{\kappa_{2}m_{+}}\Big{(}\mathcal{U}(s)-\mathcal{V}(s )\cos\Delta\Big{)}{\rm d}s;\] \[\quad\left\langle\bm{b}_{k}(t),\bm{x}_{-}\right\rangle=\left\langle \bm{b}_{k}(T_{\rm I}),\bm{x}_{-}\right\rangle+\int_{T_{\rm I}}^{t}\left\langle \frac{{\rm d}\bm{b}_{k}(s)}{{\rm d}s},\bm{x}_{-}\right\rangle{\rm d}s\] \[= \left\langle\bm{b}_{k}(T_{\rm I}),\bm{x}_{-}\right\rangle+\int_{T_ {\rm I}}^{t}\frac{\sqrt{m}}{\kappa_{2}m_{+}}\Big{(}\mathcal{U}(s)\cos\Delta- \mathcal{V}(s)\Big{)}{\rm d}s.\]

So we have

\[T^{*}_{\rm II}=\sup\Big{\{}t>T_{\rm I}\;\mathcal{U}(t)-\mathcal{V}(t)\cos\Delta>0;\] \[\left\langle\bm{b}_{k}(T_{\rm I}),\bm{x}_{+}\right\rangle+\int_{T_ {\rm I}}^{t}\frac{\sqrt{m}}{\kappa_{2}m_{+}}\Big{(}\mathcal{U}(s)-\mathcal{V}(s )\cos\Delta\Big{)}{\rm d}s>0,\forall k\in\mathcal{K}_{+};\]\[\langle\bm{b}_{k}(T_{\mathrm{I}}),\bm{x}_{-}\rangle +\int_{T_{\mathrm{I}}}^{t}\frac{\sqrt{m}}{\kappa_{2}m_{+}}\Big{(} \mathcal{U}(s)\cos\Delta-\mathcal{V}(s)\Big{)}\mathrm{d}s>0,\forall k\in \mathcal{K}_{+}\Big{\}}.\]

With the help of Lemma D.4, we know that \(\mathcal{U}(t)-\mathcal{V}(t)\cos\Delta>0\) for any \(t\geq T_{\mathrm{I}}\). So \(\langle\bm{b}_{k}(T_{\mathrm{I}}),\bm{x}_{+}\rangle+\int_{T_{\mathrm{I}}}^{t }\frac{\sqrt{m}}{\kappa_{2}m_{+}\Delta}\Big{(}\mathcal{U}(s)-\mathcal{V}(s) \cos\Delta\Big{)}\mathrm{d}s>\langle\bm{b}_{k}(T_{\mathrm{I}}),\bm{x}_{+} \rangle>0,\forall k\in\mathcal{K}_{+},\forall t\geq T_{\mathrm{I}}\). Hence, we have the transformation of the hitting time:

\[T_{\mathrm{II}}^{*}=\inf\left\{t\geq T_{\mathrm{I}}:\exists k \in\mathcal{K}_{+},\;\mathrm{s.t.}\,\langle\bm{b}_{k}(T_{\mathrm{I}}),\bm{x}_ {-}\rangle+\int_{T_{\mathrm{I}}}^{t}\frac{\sqrt{m}}{\kappa_{2}m_{+}}\Big{(} \mathcal{U}(s)\cos\Delta-\mathcal{V}(s)\Big{)}\mathrm{d}s\leq 0\right\},\] \[T_{\mathrm{II}}^{+}=T_{\mathrm{II}}^{*}.\]

**Lemma D.6** (Time Estimate of Phase II).: \[T_{\mathrm{II}}=T_{\mathrm{II}}^{+}=T_{\mathrm{II}}^{*}=\Theta\left(\frac{p^{ \frac{1}{1-\alpha\cos\Delta}}}{\kappa_{2}^{2}\Delta^{2}}\right).\]

Proof of Lemma D.6.:

With the help of Theorem C.12 (S1), for any \(k\in\mathcal{K}_{+}\), we have:

\[\frac{4.66\sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}\leq\rho_{k}(T_{ \mathrm{I}})\leq\frac{12\sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}};\]

\[\langle\bm{w}_{k}(T_{\mathrm{I}}),\bm{\mu}\rangle\geq \Big{(}1-4.2\sqrt{\kappa_{1}\kappa_{2}}\Big{)}\left(1-\frac{2}{1+ 0.7\left(1+9.9\sqrt{\frac{\kappa_{2}}{\kappa_{1}}}\right)^{1.15}}\right)\] \[> 1-4.2\sqrt{\kappa_{1}\kappa_{2}}-\frac{2}{1+0.7\left(1+9.9\sqrt{ \frac{\kappa_{2}}{\kappa_{1}}}\right)^{1.15}}.\]

With the help of Lemma I.4, we have the estimate of \(\langle\bm{w}_{k}(T_{\mathrm{I}}),\bm{x}_{-}\rangle\):

\[-2\sqrt{\epsilon}\sin\Delta-\epsilon\leq\langle\bm{w}_{k}(T_{\mathrm{I}}),\bm{ x}_{-}\rangle-\frac{p\cos\Delta-1}{\sqrt{p^{2}+1-2p\cos\Delta}}\leq 2\sqrt{ \epsilon}\sin\Delta,\]

where

\[\epsilon=4.2\sqrt{\kappa_{1}\kappa_{2}}+\frac{2}{1+0.7\left(1+9.9\sqrt{\frac{ \kappa_{2}}{\kappa_{1}}}\right)^{1.15}}.\]

Then we have:

\[\langle\bm{b}_{k}(T_{\mathrm{I}}),\bm{x}_{-}\rangle \leq\frac{12\sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}\left(\frac{p \cos\Delta-1}{\sqrt{p^{2}+1-2p\cos\Delta}}+2\sqrt{\epsilon}\sin\Delta\right),\] \[\langle\bm{b}_{k}(T_{\mathrm{I}}),\bm{x}_{-}\rangle \geq\frac{4.66\sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}\left(\frac{p \cos\Delta-1}{\sqrt{p^{2}+1-2p\cos\Delta}}-2\sqrt{\epsilon}\sin\Delta- \epsilon\right),\]

which means

\[\langle\bm{b}_{k}(T_{\mathrm{I}}),\bm{x}_{-}\rangle=\Theta\left(\frac{\sqrt{ \kappa_{1}\kappa_{2}}}{\sqrt{m}}\right).\]

From the dynamics (22), we have

\[\mathcal{U}(t)\cos\Delta-\mathcal{V}(t)\] \[= \frac{m_{-}\cos\Delta}{m_{-}+m_{+}}\Big{(}\mathcal{U}(t)-\mathcal{V }(t)\cos\Delta\Big{)}+\frac{m_{+}}{m_{-}+m_{+}}\left(\mathcal{U}(t)\cos\Delta- \mathcal{V}(t)\left(1+\alpha\sin^{2}\Delta\right)\right)\]\[= \Theta\left(\frac{\sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}\right)+ \frac{\sqrt{m}}{\kappa_{2}m_{+}}\log\left(\frac{\mathcal{U}(T_{\rm I})^{\frac{m _{-}\cos\Delta}{(m_{-}+m_{+})}}}{\mathcal{U}(T_{\rm II}^{*})^{\frac{m_{-}\cos \Delta}{(m_{-}+m_{+})}}}\cdot\frac{\mathcal{V}(T_{\rm I}^{*})^{\frac{m_{+}}{m_{ -}+m_{+}}}}{\mathcal{V}(T_{\rm I})^{\frac{m_{-}+m_{+}}{m_{-}+m_{+}}}}\right)\] \[= \Theta\left(\frac{\sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}\right)+ \Theta\left(\frac{1}{\kappa_{2}\sqrt{m}}\right)\log\left(\frac{\mathcal{U}(T_{ \rm I})^{\frac{m_{-}\cos\Delta}{(m_{-}+m_{+})}}}{\mathcal{U}(T_{\rm II}^{*})^{ \frac{m_{-}\cos\Delta}{(m_{-}+m_{+})}}}\cdot\frac{\mathcal{V}(T_{\rm II}^{*})^{ \frac{m_{+}}{m_{-}+m_{+}}}}{\mathcal{V}(T_{\rm I})^{\frac{m_{-}+m_{+}}{m_{-} +m_{+}}}}\right),\]

we obtain

\[\frac{\mathcal{V}(T_{\rm II}^{*})^{\frac{m_{+}}{m_{-}+m_{+}}}}{\mathcal{U}(T_ {\rm II}^{*})^{\frac{m_{-}\cos\Delta}{(m_{-}+m_{+})}}}\cdot\frac{\mathcal{U}( T_{\rm I})^{\frac{m_{-}\cos\Delta}{(m_{-}+m_{+})}}}{\mathcal{V}(T_{\rm I})^{ \frac{m_{+}}{m_{-}+m_{+}}}}=\exp\left(-\Theta\left(\kappa_{2}\sqrt{\kappa_{1} \kappa_{2}}\right)\right)=\Theta(1).\]

A straight-forward calculation gives us:

\[\frac{\mathcal{V}(T_{\rm II}^{*})^{\frac{m_{+}}{m_{-}+m_{+}}}}{ \mathcal{U}(T_{\rm II}^{*})^{\frac{m_{-}\cos\Delta}{(m_{-}+m_{+})}}}\cdot \frac{\mathcal{U}(T_{\rm I})^{\frac{m_{-}\cos\Delta}{(m_{-}+m_{+})}}}{ \mathcal{V}(T_{\rm I})^{\frac{m_{+}}{m_{-}+m_{+}}}}\] \[= \Theta\left(\frac{1}{\left(\frac{p^{\frac{1+\cos\Delta}{\kappa_{2 }^{2}}}}{\kappa_{2}^{2}}+\Delta^{2}(T_{\rm II}^{*}-\tau_{1})\right)^{\frac{m_{ +}-\cos\Delta}{m_{+}+m_{-}}}}\cdot\frac{(\kappa_{2}^{2})^{\frac{m_{-}\cos \Delta}{m_{+}+m_{-}}}}{\left(\frac{\kappa_{2}^{2}}{p}\right)^{\frac{m_{+}}{m_{ +}+m_{-}}}}\right)\]\[= \Theta\left(\frac{p^{\frac{m_{+}}{m_{+}+m_{-}}}}{\left(p^{\frac{1}{1+ \cos\Delta}}+\kappa_{2}^{2}\Delta^{2}(T_{\Pi}^{*}-\tau_{1})\right)^{\frac{m_{+} -m_{-}\cos\Delta}{m_{+}+m_{-}}}}\right).\]

Hence, we get

\[T_{\Pi}^{*}-\tau_{1}=\Theta\left(\frac{1}{\kappa_{2}^{2}\Delta^{2}}\left(p^{ \frac{m_{+}}{m_{+}-m_{-}\cos\Delta}}-\Theta\left(p^{\frac{1}{1+\cos\Delta}} \right)\right)\right)=\Theta\left(\frac{p^{\frac{1}{1-\alpha\cos\Delta}}}{ \kappa_{2}^{2}\Delta^{2}}\right),\]

Combining Lemma D.4 (S3), we obtain

\[T_{\Pi}^{+}=T_{\Pi}^{*}=\tau_{1}+\Theta\left(\frac{p^{\frac{1}{1-\alpha\cos \Delta}}}{\kappa_{2}^{2}\Delta^{2}}\right)=\mathcal{O}\left(\frac{p^{\frac{1}{ 1+\cos\Delta}}\log(1/\Delta)}{\kappa_{2}^{2}}\right)+\Theta\left(\frac{p^{ \frac{1}{1-\alpha\cos\Delta}}}{\kappa_{2}^{2}\Delta^{2}}\right)=\Theta\left( \frac{p^{\frac{1}{1-\alpha\cos\Delta}}}{\kappa_{2}^{2}\Delta^{2}}\right).\]

Recall the relationship between \(T_{\Pi}\) and \(T_{\Pi}^{+}\) (19)(20):

\[T_{\Pi}=T_{\Pi}^{+}\wedge\inf\{t>T_{\Pi}:\exists k\in\mathcal{K}_{-},\text{ s. t. }\langle\bm{w}_{k}(t),\bm{x}_{+}\rangle\neq 0\text{ or }\langle\bm{w}_{k}(t),\bm{x}_{-}\rangle\leq 0\}.\]

Then using Lemma D.1 (S2), we obtain:

\[T_{\Pi}=T_{\Pi}^{+}=\Theta\left(\frac{p^{\frac{1}{1-\alpha\cos\Delta}}}{ \kappa_{2}^{2}\Delta^{2}}\right).\]

**Lemma D.7** (Length of Plateau).:

_If we define the hitting time \(T_{\mathrm{plat}}:=\inf\Big{\{}t\in[T_{\mathrm{I}},T_{\mathrm{II}}]:\mathrm{ Acc}(t)=1\Big{\}}\), then we have:_

**(S1).**__\(T_{\mathrm{plat}}=\Theta\left(\frac{p}{\kappa_{2}^{2}\Delta^{2}}\right)\)_._

**(S2).**__\(\forall t\in[T_{\mathrm{I}},T_{\mathrm{plat}}]\)_,_ \(\mathrm{Acc}(t)\equiv\frac{p}{1+p}\)_._

**(S3).**__\(\forall t\in(T_{\mathrm{plat}},T_{\mathrm{II}}]\)_,_ \(\mathrm{Acc}(t)\equiv 1\)_._

Proof of Lemma D.7.: It is easy to verify

\[T_{\mathrm{plat}}=\inf\Big{\{}t\in[T_{\mathrm{I}},T_{\mathrm{II}}]:f_{+}(t) \leq 0\text{ or }f_{-}(t)>0\Big{\}}.\]

From Theorem C.12 (S4), we know \(f_{+}(T_{\mathrm{I}})>0\) and \(f_{-}(T_{\mathrm{I}})>0\). From Lemma D.3, we have

\[\mathcal{U}(t)=\kappa_{2}^{2}\frac{m_{+}}{m}\frac{p}{1+p}e^{-f_{+}(t)},\quad \mathcal{V}(t)=\kappa_{2}^{2}\frac{m_{+}}{m}\frac{1}{1+p}e^{f_{-}(t)}.\]

From the proof of Lemma D.4, we know \(\frac{\mathrm{d}}{\mathrm{d}t}\mathcal{U}(t)<0,\;\forall t\in[T_{\mathrm{I}},T _{\mathrm{II}}]\), so

\[\mathcal{U}(t)\leq\mathcal{U}(T_{\mathrm{I}}),\quad f_{+}(t)\geq f_{+}(T_{ \mathrm{I}})>0,\;\forall t\in[T_{\mathrm{I}},T_{\mathrm{II}}].\]

Recall the definition of \(\tau_{1}\) and \(\tau_{2}\) in Lemma D.4. From the proof of Lemma D.4, we know \(\frac{\mathrm{d}}{\mathrm{d}t}\mathcal{V}(t)>0,\;\forall t\in[T_{\mathrm{I}}, \tau_{1})\), so

\[\mathcal{V}(t)\geq\mathcal{V}(T_{\mathrm{I}})\quad,f_{-}(t)\geq f_{-}(T_{ \mathrm{I}})>0,\;\forall t\in[T_{\mathrm{I}},\tau_{1}].\]

With the help of Lemma D.4 (S4), we know

\[\mathcal{V}(t)=\Theta\left(\frac{1}{\frac{p^{\frac{1}{1+\cos\Delta}}}{\kappa_ {2}^{2}}+\Delta^{2}(t-\tau_{1})}\right),\;\forall t\in(\tau_{1},+\infty).\]Because \(\mathcal{V}(T_{\rm plat})=\kappa_{2}^{2}\frac{m_{+}}{m}\frac{1}{1+p}=\Theta\left( \frac{\kappa_{2}^{2}}{p}\right)\), we have

\[\Theta\left(\frac{\kappa_{2}^{2}}{p}\right)=\Theta\left(\frac{1}{\frac{\frac{1} {p^{1+\cos\Delta}}}{\kappa_{2}^{2}}+\Delta^{2}(T_{\rm plat}-\tau_{1})}\right), \;\forall t\in(\tau_{1},+\infty).\]

Therefore,

\[T_{\rm plat}=\tau_{1}+\Theta\left(\frac{p}{\kappa_{2}^{2}\Delta^ {2}}\left(1-\Theta\left(\frac{1}{p^{1+\cos\Delta}}\right)\right)\right)=\tau_ {1}+\Theta\left(\frac{p}{\kappa_{2}^{2}\Delta^{2}}\right)\] \[= \mathcal{O}\left(\frac{p^{\frac{1}{1+\cos\Delta}}\log(1/\Delta)} {\kappa_{2}^{2}}\right)+\Theta\left(\frac{p}{\kappa_{2}^{2}\Delta^{2}}\right)= \Theta\left(\frac{p}{\kappa_{2}^{2}\Delta^{2}}\right).\]

It is easy to verify \(T_{\rm plat}=\Theta\left(\frac{p}{\kappa_{2}^{2}\Delta^{2}}\right)<T_{\rm II}= \Theta\left(\frac{p^{\frac{1}{1-\cos\Delta}}}{\kappa_{2}^{2}\Delta^{2}}\right)\).

Because \(\tau_{2}-\tau_{1}=\Theta\left(\frac{p^{\frac{1}{1+\cos\Delta}}\log(1/\Delta)} {\kappa_{2}^{2}}\right)\), for any \(t\in(\tau_{1},\tau_{2}]\), we have

\[\mathcal{V}(t)=\Theta\left(\frac{1}{\frac{p^{1+\cos\Delta}}{ \kappa_{2}^{2}}+\Delta^{2}(t-\tau_{1})}\right)=\Theta\left(\frac{1}{\frac{p^{ 1+\cos\Delta}}{\kappa_{2}^{2}}+\Delta^{2}(\tau_{2}-\tau_{1})}\right)\] \[= \Theta\left(\frac{\kappa_{2}^{2}}{p^{\frac{1}{1+\cos\Delta}}} \right)\gg\Theta\left(\frac{\kappa_{2}^{2}}{p}\right)=\mathcal{V}(T_{\rm plat}).\]

Thus,

\[f_{-}(t)>f_{-}(T_{\rm plat})=0,\;\forall t\in(\tau_{1},\tau_{2}].\]

From Lemma D.4 (S6), we know that \(\mathcal{U}(t)\cos\Delta-\mathcal{V}(t)<0,\,\forall t\in(\tau_{2},T_{\rm II}]\). Then \(\frac{{\rm d}V(t)}{{\rm d}t}<0\), \(\forall t\in(\tau_{2},T_{\rm II}]\). Thus,

\[f_{-}(t)>f_{-}(T_{\rm plat})=0,\;\forall t\in(\tau_{2},T_{\rm plat });\] \[f_{-}(t)<f_{-}(T_{\rm plat})=0,\;\forall t\in(T_{\rm plat},T_{\rm II }].\]

Hence, we know

\[f_{-}(t)\geq 0,\;\forall t\in[T_{\rm I},T_{\rm plat}];\] \[f_{-}(t)<0,\;\forall t\in(T_{\rm plat},T_{\rm II}].\]

In summary, we have proved (S1)(S2)(S3).

**Lemma D.8** (Prediction at end of Phase II).:

_(S1) For the predictions, we have:_

\[e^{-f_{+}(T_{\rm II})}=\Theta\left(p^{-\frac{1}{1-\alpha\cos \Delta}}\right),\quad e^{f_{-}(T_{\rm II})}=\Theta\left(p^{-\frac{\alpha\cos \Delta}{1-\alpha\cos\Delta}}\right),\quad\mathcal{L}(\bm{\theta}(T_{\rm II}) )=\Theta\left(p^{-\frac{1}{1-\alpha\cos\Delta}}\right);\] \[\frac{pe^{-f_{+}(T_{\rm II})}}{1+p}-\frac{e^{f_{-}(T_{\rm II})}} {1+p}\cos\Delta=\Theta\left(\Delta^{2}p^{-\frac{1}{1-\alpha\cos\Delta}}\right),\] \[\frac{pe^{-f_{+}(T_{\rm II})}}{1+p}\cos\Delta-\frac{e^{f_{-}(T_{ \rm II})}}{1+p}=-\Theta\left(\Delta^{2}p^{-\frac{1}{1-\alpha\cos\Delta}}\right).\]

_(S2). For any \(k\in\mathcal{K}_{+}\), we have:_

\[\langle\bm{b}_{k}(T_{\rm II}),\bm{x}_{-}\rangle=\mathcal{O}\left(\frac{\sqrt{ \kappa_{1}\kappa_{2}}}{\sqrt{m}}\right).\]

_(S3). For any \(k\in\mathcal{K}_{-}\), we have \(\langle\bm{b}_{k}(T_{\rm II}),\bm{x}_{+}\rangle=0\)._Proof of Lemma D.8.:

Proof of (S1).: Recall the definitions in Lemma D.3:

\[\begin{cases}\mathcal{U}(t)=\kappa_{2}^{2}\frac{m_{+}}{m}\frac{p}{1+p}e^{-f_{+}(t )},\\ \mathcal{V}(t)=\kappa_{2}^{2}\frac{m_{+}}{m}\frac{1}{1+p}e^{f_{-}(t)}.\end{cases}\]

From Lemma D.4 (S4) and Lemma D.6, we have

\[\mathcal{U}(t)=\Theta\left(\frac{1}{\frac{p^{\frac{1}{1+\cos \Delta}}}{\kappa_{2}^{2}}+\Delta^{2}(t-\tau_{1})}\right),\quad\mathcal{V}(t)= \Theta\left(\frac{1}{\frac{p^{\frac{1}{1+\cos\Delta}}}{\kappa_{2}^{2}}+\Delta^ {2}(t-\tau_{1})}\right),\] \[\tau_{1}=\mathcal{O}\left(\frac{p^{\frac{1}{1+\cos\Delta}}\log(1/ \Delta)}{\kappa_{2}^{2}}\right),\quad T_{\rm II}=\Theta\left(\frac{p^{\frac{1}{ 1-\alpha\cos\Delta}}}{\kappa_{2}^{2}\Delta^{2}}\right).\]

Therefore, we obtain the estimate:

\[e^{-f_{+}(T_{\rm II})}=\Theta\left(\frac{1}{\kappa_{2}^{2}}\frac{1}{\mathcal{ U}(T_{\rm II})}\right)=\Theta\left(\frac{1}{\kappa_{2}^{2}}\frac{1}{\frac{p^{ \frac{1}{1+\cos\Delta}}}{\kappa_{2}^{2}}+\Delta^{2}\frac{p^{\frac{1}{1-\alpha \cos\Delta}}}{\kappa_{2}^{2}\Delta^{2}}}\right)=\Theta\left(p^{-\frac{1}{1- \alpha\cos\Delta}}\right),\]

\[e^{f_{-}(T_{\rm II})}=\Theta\left(\frac{p}{\kappa_{2}^{2}}\frac{1}{\mathcal{V}( T_{\rm II})}\right)=\Theta\left(\frac{p}{\kappa_{2}^{2}}\frac{1}{\frac{p^{ \frac{1}{1+\cos\Delta}}}{\kappa_{2}^{2}}+\Delta^{2}\frac{p^{\frac{1}{1-\alpha \cos\Delta}}}{\kappa_{2}^{2}\Delta^{2}}}\right)=\Theta\left(p^{-\frac{\alpha \cos\Delta}{1-\alpha\cos\Delta}}\right).\]

Moreover, Lemma D.4 (S4)(S5)(S6) give us

\[\mathcal{U}(T_{\rm II})-\mathcal{V}(T_{\rm II})\cos\Delta=\Theta \left(\Delta^{2}\mathcal{V}(T_{\rm II})\right)=\Theta\left(\kappa_{2}^{2} \Delta^{2}p^{-\frac{1}{1-\alpha\cos\Delta}}\right),\] \[\mathcal{U}(T_{\rm II})\cos\Delta-\mathcal{V}(T_{\rm II})=-\Theta \left(\Delta^{2}\mathcal{V}(T_{\rm II})\right)=-\Theta\left(\kappa_{2}^{2} \Delta^{2}p^{-\frac{1}{1-\alpha\cos\Delta}}\right).\]

Hence,

\[\frac{pe^{-f_{+}(T_{\rm II})}}{1+p}-\frac{e^{f_{-}(T_{\rm II})}}{ 1+p}\cos\Delta=\frac{1}{\kappa_{2}^{2}}\Big{(}\mathcal{U}(T_{\rm II})- \mathcal{V}(T_{\rm II})\cos\Delta\Big{)}=\Theta\left(\Delta^{2}p^{-\frac{1}{1 -\alpha\cos\Delta}}\right),\] \[\frac{pe^{-f_{+}(T_{\rm II})}}{1+p}\cos\Delta-\frac{e^{f_{-}(T_{ \rm II})}}{1+p}=\frac{1}{\kappa_{2}^{2}}\Big{(}\mathcal{U}(T_{\rm II})\cos \Delta-\mathcal{V}(T_{\rm II})\Big{)}=-\Theta\left(\Delta^{2}p^{-\frac{1}{1- \alpha\cos\Delta}}\right).\]

Proof of (S2). Denote \(\mathcal{K}_{+}^{0}:=\Big{\{}k\in\mathcal{K}_{+}:\langle\bm{w}_{k}(T_{\rm II}), \bm{x}_{-}\rangle=0\Big{\}}\). From the definition of \(T_{\rm II}\) and the proof in Phase II, we know that \(\langle\bm{w}_{k}(T_{\rm II}),\bm{x}_{-}\rangle>0\) holds for any \(k\in\mathcal{K}_{+}-\mathcal{K}_{+}^{0}\).

From the proof in Lemma D.5, for any \(k\in\mathcal{K}_{+}\), it holds

\[\langle\bm{b}_{k}(t),\bm{x}_{-}\rangle=\langle\bm{b}_{k}(T_{\rm I }),\bm{x}_{-}\rangle+\int_{T_{\rm I}}^{t}\bigg{\langle}\frac{{\rm d}\bm{b}_{k} (s)}{{\rm d}s},\bm{x}_{-}\bigg{\rangle}\,{\rm d}s\] \[=\langle\bm{b}_{k}(T_{\rm I}),\bm{x}_{-}\rangle+\int_{T_{\rm I}}^ {t}\frac{\sqrt{m}}{\kappa_{2}m_{+}}\Big{(}\mathcal{U}(s)\cos\Delta-\mathcal{V} (s)\Big{)}{\rm d}s,\;\forall t\in[T_{\rm I},T_{\rm II}].\]

Thus for any \(k\in\mathcal{K}_{+}-\mathcal{K}_{+}^{0}\), we have

\[\langle\bm{b}_{k}(T_{\rm II}),\bm{x}_{-}\rangle-\langle\bm{b}_{k}(T_{\rm I}), \bm{x}_{-}\rangle=\langle\bm{b}_{k_{0}}(T_{\rm II}),\bm{x}_{-}\rangle-\langle \bm{b}_{k_{0}}(T_{\rm I}),\bm{x}_{-}\rangle=-\langle\bm{b}_{k_{0}}(T_{\rm I}), \bm{x}_{-}\rangle\,,\]

so

\[\langle\bm{b}_{k}(T_{\rm II}),\bm{x}_{-}\rangle=\langle\bm{b}_{k}(T_{\rm I}), \bm{x}_{-}\rangle-\langle\bm{b}_{k_{0}}(T_{\rm I}),\bm{x}_{-}\rangle\,.\]

From the proof of Lemma D.6, we know

\[\langle\bm{b}_{k}(T_{\rm I}),\bm{x}_{-}\rangle\leq\frac{12\sqrt{\kappa_{1} \kappa_{2}}}{\sqrt{m}}\left(\frac{p\cos\Delta-1}{\sqrt{p^{2}+1-2p\cos\Delta}}+2 \sqrt{\epsilon}\sin\Delta\right),\]\[\langle\bm{b}_{k}(T_{\rm I}),\bm{x}_{-}\rangle\geq\frac{4.66\sqrt{\kappa_{1}\kappa_ {2}}}{\sqrt{m}}\left(\frac{p\cos\Delta-1}{\sqrt{p^{2}+1-2p\cos\Delta}}-2\sqrt{ \epsilon}\sin\Delta-\epsilon\right),\]

where \(\epsilon=4.2\sqrt{\kappa_{1}\kappa_{2}}+\frac{2}{1+0.7\left(1+9.9\sqrt{\frac{ \kappa_{2}}{\kappa_{1}}}\right)^{1.15}}\). This means

\[\langle\bm{b}_{k}(T_{\rm I}),\bm{x}_{-}\rangle=\Theta\left(\frac{\sqrt{\kappa_ {1}\kappa_{2}}}{\sqrt{m}}\right).\]

Hence, for any \(k\in\mathcal{K}_{+}-\mathcal{K}_{+}^{0}\),

\[0<\langle\bm{b}_{k}(T_{\rm II}),\bm{x}_{-}\rangle= |\langle\bm{b}_{k}(T_{\rm I}),\bm{x}_{-}\rangle-\langle\bm{b}_{k _{0}}(T_{\rm I}),\bm{x}_{-}\rangle|\leq|\langle\bm{b}_{k}(T_{\rm I}),\bm{x}_ {-}\rangle|+|\langle\bm{b}_{k_{0}}(T_{\rm I}),\bm{x}_{-}\rangle|\] \[= \Theta\left(\frac{\sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}\right)+ \Theta\left(\frac{\sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}\right)=\Theta\left( \frac{\sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}\right).\]

Proof of (S3).: Due to the dynamics of the neuron \(k\in\mathcal{K}_{-}\) in Phase II, this conclusion is clear.

As simple corollaries of these lemmas, we can prove two theorems in Phase II.

Proof of Theorem 4.4 and 4.5.: Theorem 4.5 is Lemma D.7. Theorem 4.4 (S1) has been proven in Lemma D.6; Theorem 4.4 (S2) has been proven in Lemma D.8. Additionally, combining (i) \(T_{\rm II}=T_{\rm II}^{+}\) in Lemma D.6, (ii) the transformation in Lemma D.5, and (iii) the definition of \(T_{\rm II}\), we obtain Theorem 4.4 (S3).

Proofs of Optimization Dynamics in Phase III

### Optimization Dynamics during Phase Transition

Building upon Phase II, we will demonstrate that within a short time, all the living positive neurons \(\mathcal{K}_{+}\) change their activation patterns, corresponding to a "phase transition". After the phase transition, _all_ the living positive neurons \(k\in\mathcal{K}_{+}\) undergo deactivation for \(\bm{x}_{-}\), i.e., \(\mathsf{sgn}_{k}^{-}(t)\) changes from 1 to 0, while other activation patterns remain unchanged.

Specifically, we define the hitting time

\[\begin{split} T_{\rm II}^{\rm PT}:=&\inf\{t>T_{ \rm II}:\forall k\in\mathcal{K}_{+},\mathsf{sgn}_{k}^{-}(t)=0\}\\ =&\inf\big{\{}t>T_{\rm II}:\forall k\in\mathcal{K}_ {+},\langle\bm{w}_{k}(t),\bm{x}_{-}\rangle=0\big{\}},\end{split}\] (26)

and we call \(t\in(T_{\rm II},T_{\rm II}^{\rm PT}]\) "Phase Transition" from Phase II to Phase III.

Notice that the dynamics during phase transition is highly nonlinear with \(|\mathcal{K}_{+}|=\Theta(m)\) changes on activation partitions. Fortunately, we can keep the neurons of \(\mathcal{K}_{+}\) and \(\mathcal{K}_{-}\) close enough respectively in Phase I by using sufficiently small initialization \(\kappa_{1}\). Moreover, their differences do not enlarge in Phase II. As a result, the phase transition can be completed quickly without significant changes in the vector field.

In order to analyze the dynamics of neurons and vector fields, we introduce the auxiliary hitting time:

\[\begin{split} T_{\rm II}^{\rm PT*}:=T_{\rm II}^{\rm PT}\wedge \inf\Big{\{}t>T_{\rm II}:\langle\bm{F}_{+}(t),\bm{x}_{+}\rangle\leq 0\text{ or }\langle\bm{F}_{+}(t),\bm{x}_{-}\rangle\geq 0\Big{\}}; \\ \text{where }\quad\bm{F}_{+}(t)=&\frac{p}{1+p}e^{-f _{+}(t)}\bm{x}_{+}-\frac{1}{1+p}e^{f_{-}(t)}\bm{x}_{-}.\end{split}\] (27)

We call \(T_{\rm II}\leq t\leq T_{\rm II}^{\rm PT*}\) "Phase Transition*".

**Lemma E.1** (Dynamics of living neurons during Phase Transition*).:

_In Phase Transition*, i.e., \(t\in[T_{\rm II},T_{\rm II}^{\rm PT*}]\), we have the following dynamics for each neuron \(k\in\mathcal{K}_{-}\cup\mathcal{K}_{+}\)._

_(S1). For living negative neuron \(k\in\mathcal{K}_{-}\), we have:_

\[\begin{split}\bm{w}_{k}(t)\in\mathcal{M}_{+}^{0}\cap\mathcal{M} _{-}^{+},\\ \frac{\mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t}=\frac{\kappa_{2}e^{f _{-}(t)}}{\sqrt{m}(1+p)}\Big{(}\bm{x}_{-}-\bm{x}_{+}\cos\Delta\Big{)}.\end{split}\]

_(S2) For living positive neuron \(k\in\mathcal{K}_{+}\), we define the hitting time:_

\[T_{\rm II,k}^{\rm PT*}:=\inf\big{\{}t>T_{\rm II}:\langle\bm{w}_{k}(t),\bm{x}_ {-}\rangle=0\big{\}}\wedge\inf\big{\{}t>T_{\rm II}:\langle\bm{F}_{+}(t),\bm{x} _{+}\rangle\leq 0\text{ or }\langle\bm{F}_{+}(t),\bm{x}_{-}\rangle\geq 0\big{\}}.\]

_Then it holds that:_

_(P0)_ \(T_{\rm II}^{\rm PT*}=\max_{k\in\mathcal{K}_{+}}T_{\rm II,k}^{\rm PT*};\)__

_(P1) For any_ \(t\in[T_{\rm II},T_{\rm II,k}^{\rm PT*}],\) _we have_

\[\frac{\mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t}=\frac{\kappa_{2}}{\sqrt{m}}\bm{F} _{+}(t);\]

_(P2) If_ \(T_{\rm II,k}^{\rm PT*}<T_{\rm II}^{\rm PT*}\) _strictly, then for any_ \(t\in[T_{\rm II,k}^{\rm PT*},T_{\rm II}^{\rm PT*}],\) _we have_ \(\bm{w}_{k}(t)\in\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{0}\) _and_

\[\frac{\mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t}=\frac{\kappa_{2}pe^{-f _{+}(t)}}{\sqrt{m}(1+p)}\Big{(}\bm{x}_{+}-\bm{x}_{-}\cos\Delta\Big{)}.\]

_(P3) Regardless of the relationship between_ \(T_{\rm II,k}^{\rm PT*}\) _and_ \(T_{\rm II}^{\rm PT*}\)_, for any_ \(t\in[T_{\rm II},T_{\rm II}^{\rm PT*}]\)_, we have_

\[\langle\bm{w}_{k}(t),\bm{x}_{+}\rangle>0,\quad\langle\bm{w}_{k}(t),\bm{x}_{-} \rangle\geq 0.\]

Proof of Lemma E.1.

Proof of (S1). Recalling the definition of \(T_{\rm II}^{\rm PT*}\), it holds \(\langle\bm{F}_{+}(t),\bm{x}_{+}\rangle>0\) for any \(T_{\rm II}\leq t\leq T_{\rm II}^{\rm PT*}\). So (S1) can be proved in the same way as employed in the proof of Lemma D.1 (S2) and is omitted.

Proof of (S2)(P0) and (S2)(P1). (S2)(P0) is obvious. Moreover, for any \(k\in\mathcal{K}_{+}\) and \(t\in[T_{\rm II},T^{\rm PT*}_{\rm II,k})\), we have \(\langle\bm{w}_{k}(t),\bm{x}_{+}\rangle>0\) and \(\langle\bm{w}_{k}(t),\bm{x}_{-}\rangle>0\) for any \(T_{\rm I}\leq t\leq T^{*}_{\rm II}\), so (S2)(P1) can be proved in the same method as shown in the proof of Lemma D.1 (S1), and we have the dynamics:

\[\frac{{\rm d}\bm{b}_{k}(t)}{{\rm d}t}=\frac{\kappa_{2}}{\sqrt{m}}\bm{F}_{+}(t), \quad t\in[T_{\rm II},T^{\rm PT*}_{\rm II,k}).\]

Additionally, recalling the definition of \(T^{\rm PT*}_{\rm II,k}\), we know \(\langle\bm{F}_{+}(t),\bm{x}_{+}\rangle>0\) holds for any \(t\in[T_{\rm II},T^{\rm PT*}_{\rm II,k})\). Combining the dynamics of \(\bm{b}_{k}(t)\), we further have:

\[\langle\bm{b}_{k}(t),\bm{x}_{+}\rangle =\langle\bm{b}_{k}(T_{\rm II}),\bm{x}_{+}\rangle+\frac{\kappa_{2 }}{\sqrt{m}}\int_{T_{\rm II}}^{t}\langle\bm{F}_{+}(s),\bm{x}_{+}\rangle\,{ \rm d}s\] \[>\langle\bm{b}_{k}(T_{\rm II}),\bm{x}_{+}\rangle>0,\quad\forall t \in[T_{\rm II},T^{\rm PT*}_{\rm II,k}].\]

Proof of (S2)(P2). Let \(k\in\mathcal{K}_{+}\). If \(T^{\rm PT*}_{\rm II,k}<T^{\rm PT*}_{\rm II}\), we have the following results:

Step I. \(\bm{w}_{k}(T^{\rm PT*}_{\rm II,k})\in\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{0}\).

Recalling the definition of \(T^{\rm PT*}_{\rm II,k}\) and \(T^{\rm PT*}_{\rm II}\), \(T^{\rm PT*}_{\rm II,k}<T^{\rm PT*}_{\rm II}\) implies that \(\left\langle\bm{w}_{k}(T^{\rm PT*}_{\rm II,k}),\bm{x}_{-}\right\rangle=0\).

Then recalling our proof of (S2)(P1), we obtain \(\left\langle\bm{b}_{k}(T^{\rm PT*}_{\rm II,k}),\bm{x}_{+}\right\rangle>0\).

Hence, we obtain \(\bm{w}(T^{\rm PT*}_{\rm II,k})\in\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{0}\).

Step II. Dynamics after \(t=T^{\rm PT*}_{\rm II,k}\).

In this step, we will analyze the training dynamics after \(\bm{w}_{k}(T^{\rm PT*}_{\rm II,k})\in\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{0}\), i.e. \(\bm{b}_{k}(T^{\rm PT*}_{\rm II,k})\in\mathcal{P}_{+}^{+}\cap\mathcal{P}_{-}^{0}\). We first analysis the vector field around the manifold \(\mathcal{P}_{+}^{+}\cap\mathcal{P}_{-}^{0}\). For any \(\tilde{\bm{b}}\in\mathcal{P}_{+}^{+}\cap\mathcal{P}_{-}^{0}\) and \(0<\delta_{0}\ll 1\), we know that \(\mathcal{P}_{+}^{0}\cap\mathcal{P}_{-}^{+}\) separates its neighborhood \(\mathcal{B}(\tilde{\bm{b}},\delta_{0})\) into two domains \(\mathcal{G}_{-}=\{\bm{b}\in\mathcal{B}(\tilde{\bm{b}},\delta_{0}):\langle\bm{b },\bm{x}_{-}\rangle<0\}\) and \(\mathcal{G}_{+}=\{\bm{b}\in\mathcal{B}(\tilde{\bm{b}},\delta_{0}):\langle\bm{b },\bm{x}_{-}\rangle>0\}\). Following Definition H.1, we calculate the limited vector field on \(\tilde{\bm{b}}\) from \(\mathcal{G}_{-}\) and \(\mathcal{G}_{+}\).

(i) The limited vector field \(\bm{F}^{-}\) on \(\tilde{\bm{b}}\) (from \(\mathcal{G}_{-}\)):

\[\frac{{\rm d}\bm{b}}{{\rm d}t}=\bm{F}^{-},\text{ where }\bm{F}^{-}=\frac{\kappa_{2}}{ \sqrt{m}}\frac{p}{1+p}e^{-f_{+}(t)}\bm{x}_{+}.\]

(ii) The limited vector field \(\bm{F}^{+}\) on \(\tilde{\bm{b}}\) (from \(\mathcal{G}_{+}\)):

\[\frac{{\rm d}\bm{b}}{{\rm d}t}=\bm{F}^{+},\text{ where }\bm{F}^{+}=\frac{\kappa_{2}}{ \sqrt{m}}\left(\frac{pe^{-f_{+}(t)}}{1+p}\bm{x}_{+}-\frac{e^{f_{-}(t)}}{1+p} \bm{x}_{-}\right).\]

(iii) Then we calculate the projections of \(\bm{F}^{-}\) and \(\bm{F}^{+}\) onto \(\bm{x}_{-}\) (the normal to the surface \(\mathcal{P}_{+}^{+}\cap\mathcal{P}_{-}^{0}\)):

\[F_{N}^{-}=\left\langle\bm{F}^{-},\bm{x}_{-}\right\rangle=\frac{ \kappa_{2}pe^{-f_{+}(t)}}{\sqrt{m}(1+p)}\cos\Delta,\] \[F_{N}^{+}=\left\langle\bm{F}^{+},\bm{x}_{-}\right\rangle=\frac{ \kappa_{2}e^{f_{-}(t)}}{\sqrt{m}(1+p)}\cos\Delta-\frac{\kappa_{2}pe^{-f_{+}(t)} }{\sqrt{m}(1+p)}.\]

We further define the hitting time to check whether \(\bm{w}_{k}(t)\in\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{0}\) for \(T^{\rm PT*}_{\rm II,k}\leq t\leq T^{\rm PT*}_{\rm II}\).

\[\tau_{+,k}^{+}:=\inf\big{\{}t\in[T^{\rm PT*}_{\rm II,k},T^{\rm PT*}_{\rm II}]: \langle\bm{w}_{k}(t),\bm{x}_{+}\rangle\leq 0\big{\}}.\]

From the definition of \(T^{\rm PT*}_{\rm II}\), we know that \(\langle\bm{F}_{+}(t),\bm{x}_{-}\rangle=\frac{p}{1+p}e^{-f_{+}(t)}\cos\Delta- \frac{1}{1+p}e^{f_{-}(t)}<0\) for any \(t\in[T_{\rm II},T^{\rm PT*}_{\rm II}]\), which means \(F_{N}^{+}<0\). And it is clear that \(F_{N}^{-}>0\). Hence, the dynamics corresponds to Case (I) in Definition H.1 (\(F_{N}^{-}>0\) and \(F_{N}^{+}<0\)), which means that \(\bm{b}_{k}(t)\) can not leave \(\mathcal{P}_{-}^{0}\) for any \(t\in[T_{\mathrm{II},k}^{\mathrm{PT}*},\tau_{+,k}^{+}]\), and the dynamics of \(\bm{b}_{k}\) for \(t\in[T_{\mathrm{II},k}^{\mathrm{PT}*},\tau_{+,k}^{+}]\) satisfies:

\[\frac{\mathrm{d}\bm{b}}{\mathrm{d}t}=\alpha\bm{F}^{+}+(1-\alpha)\bm{F}^{-}, \quad\alpha=\frac{f_{N}^{-}}{f_{N}^{-}-f_{N}^{+}},\]

which is

\[\frac{\mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t}=\frac{\kappa_{2}pe^{-f_{+}(t)}}{ \sqrt{m}(1+p)}\Big{(}\bm{x}_{+}-\bm{x}_{-}\cos\Delta\Big{)},t\in[T_{\mathrm{II}, k}^{\mathrm{PT}*},\tau_{+,k}^{+}].\]

By Lemma C.1, we know that the dynamics of \(\bm{w}_{k}(t)\) on \(\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{0}\) and the dynamics of \(\rho_{k}(t)\) are:

\[\frac{\mathrm{d}\bm{w}_{k}(t)}{\mathrm{d}t}=\frac{\kappa_{2}pe^{-f_{+}(t)}}{ \rho_{k}(t)\sqrt{m}(1+p)}\Big{(}\bm{x}_{+}-\left<\bm{w}_{k},\bm{x}_{+}\right> \bm{w}_{k}-\bm{x}_{-}\cos\Delta\Big{)}.\]

Moreover, The dynamics above also ensures that:

\[\left<\bm{b}_{k}(t),\bm{x}_{+}\right>= \left<\bm{b}_{k}(T_{\mathrm{II},k}^{\mathrm{PT}*}),\bm{x}_{+} \right>+\int_{T_{\mathrm{II},k}^{\mathrm{PT}*}}^{t}\frac{\kappa_{2}pe^{-f_{+}( s)}}{\sqrt{m}(1+p)}\sin^{2}\Delta\mathrm{d}s\] \[> \left<\bm{b}_{k}(T_{\mathrm{II},k}^{\mathrm{PT}*}),\bm{x}_{+} \right>>0,\quad\forall t\in[T_{\mathrm{II},k}^{\mathrm{PT}*},\tau_{+,k}^{+}].\]

which means \(\tau_{+,k}^{+}=T_{\mathrm{II}}^{\mathrm{PT}*}\). Hence, we have proved (S2)(P2).

Proof of (S2)(P3). Our proof for (S2)(P1) and (S2)(P2) imply this result directly.

\(\square\)

**Lemma E.2** (Evolution of the prediction in Phase III*).: _For any \(t\in[T_{\mathrm{II}},T_{\mathrm{II}}^{\mathrm{PT}*}]\), we have_

\[\frac{e^{-C_{1}}}{1+C_{0}e^{f_{-}(T_{\mathrm{II}})}(t-T_{\mathrm{ II}})}\leq e^{f_{-}(t)-f_{-}(T_{\mathrm{II}})}\leq\frac{1}{1+C_{0}e^{(f_{-}(T_{ \mathrm{II}})-C_{1})}(t-T_{\mathrm{II}})},\] \[\exp\left(-C_{2}(t-T_{\mathrm{II}})\right)\leq e^{f_{+}(T_{\mathrm{II}})-f_{+}(t)}\leq 1,\]

_where_

\[C_{0}=\Theta\left(\frac{\kappa_{2}^{2}\Delta^{2}}{p}\right),\;C_{1}=\mathcal{O }\left(\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}\right),\;e^{f_{-}(T_{\mathrm{II} })}=\Theta\left(p^{-\frac{\alpha\cos\Delta}{1-\alpha\cos\Delta}}\right),\;C_{2} =\Theta\left(\kappa_{2}^{2}p^{-\frac{1}{1-\alpha\cos\Delta}}\right).\]

_Proof of Lemma E.2._

Step I. Preparation. With the help of Lemma E.1(S1) and (S2)(P3), we know that

(i) For \(k\in\mathcal{K}_{-}\), we have

\[\left<\bm{w}_{k}(t),\bm{x}_{+}\right>0,\quad\left<\bm{w}_{k}(t),\bm{x}_{-} \right>0,\quad\forall t\in[T_{\mathrm{II}},T_{\mathrm{II}}^{\mathrm{PT}*}].\]

(ii) For \(k\in\mathcal{K}_{+}\), we have

\[\left<\bm{w}_{k}(t),\bm{x}_{+}\right>0,\quad\left<\bm{w}_{k}(t),\bm{x}_{-} \right>\geq 0,\quad\forall t\in[T_{\mathrm{II}},T_{\mathrm{II}}^{\mathrm{PT}*}];\]

So \(f_{+}(t)\) and \(f_{-}(t)\) have the following representation for any \(t\in[T_{\mathrm{II}},T_{\mathrm{II}}^{\mathrm{PT}*}]\):

\[f_{+}(t)=\sum_{k\in\mathcal{K}_{+}}\frac{\kappa_{2}}{\sqrt{m}} \bm{b}_{k}^{\top}(t)\bm{x}_{+},\] \[f_{-}(t)=\sum_{k\in\mathcal{K}_{+}}\frac{\kappa_{2}}{\sqrt{m}} \bm{b}_{k}^{\top}(t)\bm{x}_{-}-\sum_{k\in\mathcal{K}_{-}}\frac{\kappa_{2}}{ \sqrt{m}}\bm{b}_{k}^{\top}(t)\bm{x}_{-}.\]

Step II. Evolution of \(f_{-}(t)\).

To begin with, we need to do a rough estimate of \(\sum_{k\in\mathcal{K}_{+}}\frac{\kappa_{2}}{\sqrt{m}}\bm{b}_{k}^{\top}(t)\bm{x}_{-}\). Let \(k\in\mathcal{K}_{+}\). For any \(t\in[T_{\Pi,k}^{\mathrm{PT}*},T_{\Pi}^{\mathrm{PT}*}]\), we have \(\langle\bm{b}_{k}(t),\bm{x}_{-}\rangle=0\). And for any \(t\in[T_{\Pi},T_{\Pi,k}^{\mathrm{PT}*})\), we have:

\[\frac{\mathrm{d}}{\mathrm{d}t}\,\langle\bm{b}_{k}(t),\bm{x}_{-}\rangle \stackrel{{\text{Lemma E.\ref{lem:2}}}}{{=}}\frac{\kappa_{2}}{ \sqrt{m}}\,\langle\bm{F}_{k}(t),\bm{x}_{-}\rangle\stackrel{{ \eqref{eq:2}}}{{<}}0.\]

Therefore, for any \(t\in[T_{\Pi},T_{\Pi}^{\mathrm{PT}*}]\), we have \(0\leq\langle\bm{b}_{k}(t),\bm{x}_{-}\rangle\leq\langle\bm{b}_{k}(T_{\Pi}),\bm {x}_{-}\rangle\), so

\[0\leq\sum_{k\in\mathcal{K}_{+}}\frac{\kappa_{2}}{\sqrt{m}}\bm{b }_{k}^{\top}(t)\bm{x}_{-}\leq\sum_{k\in\mathcal{K}_{+}}\frac{\kappa_{2}}{ \sqrt{m}}\bm{b}_{k}^{\top}(T_{\Pi})\bm{x}_{-},\] \[-\sum_{k\in\mathcal{K}_{-}}\frac{\kappa_{2}}{\sqrt{m}}\bm{b}_{k}^ {\top}(t)\bm{x}_{-}\leq f_{-}(t)\leq\sum_{k\in\mathcal{K}_{+}}\frac{\kappa_{2 }}{\sqrt{m}}\bm{b}_{k}^{\top}(T_{\Pi})\bm{x}_{-}-\sum_{k\in\mathcal{K}_{-}} \frac{\kappa_{2}}{\sqrt{m}}\bm{b}_{k}^{\top}(t)\bm{x}_{-}.\]

According to Lemma E.1, it follows that for any \(k\in\mathcal{K}_{-}\), its dynamics is \(\frac{\mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t}=\frac{\kappa_{2}e^{f_{-}(t)}}{ \sqrt{m(1+p)}}\Big{(}\bm{x}_{-}-\bm{x}_{+}\cos\Delta\Big{)}\), thus

\[\bm{b}_{k}^{\top}(t)\bm{x}_{-}=\bm{b}_{k}^{\top}(T_{\Pi})\bm{x}_{-}+\int_{T_{ \Pi}}^{t}\left\langle\frac{\mathrm{d}\bm{b}_{k}(s)}{\mathrm{d}s},\bm{x}_{-} \right\rangle\mathrm{d}s=\bm{b}_{k}^{\top}(T_{\Pi})\bm{x}_{-}+\frac{\kappa_{2} \sin^{2}\Delta}{\sqrt{m(1+p)}}\int_{T_{\Pi}}^{t}e^{f_{-}(s)}\mathrm{d}s,\]

Therefore, we have two-side bounds of \(f_{-}(t)\):

\[f_{-}(t)\leq-\frac{m_{-}\kappa_{2}^{2}\sin^{2}\Delta}{m(1+p)}\int_{T_{\Pi}}^{ t}e^{f_{-}(s)}\mathrm{d}s-\sum_{k\in\mathcal{K}_{-}}\frac{\kappa_{2}}{ \sqrt{m}}\bm{b}_{k}^{\top}(T_{\Pi})\bm{x}_{-}+\sum_{k\in\mathcal{K}_{+}}\frac{ \kappa_{2}}{\sqrt{m}}\bm{b}_{k}^{\top}(T_{\Pi})\bm{x}_{-},\]

\[f_{-}(t)\geq-\frac{m_{-}\kappa_{2}^{2}\sin^{2}\Delta}{m(1+p)}\int_{T_{\Pi}}^{ t}e^{f_{-}(s)}\mathrm{d}s-\sum_{k\in\mathcal{K}_{-}}\frac{\kappa_{2}}{ \sqrt{m}}\bm{b}_{k}^{\top}(T_{\Pi})\bm{x}_{-}.\]

For simplicity, we denote \(C_{0}:=\frac{m_{-}\kappa_{2}^{2}\sin^{2}\Delta}{m(1+p)}\), \(C_{-}^{-}:=\sum_{k\in\mathcal{K}_{-}}\frac{\kappa_{2}}{\sqrt{m}}\bm{b}_{k}^{ \top}(T_{\Pi})\bm{x}_{-}\) and \(C_{-}^{+}:=\sum_{k\in\mathcal{K}_{+}}\frac{\kappa_{2}}{\sqrt{m}}\bm{b}_{k}^{ \top}(T_{\Pi})\bm{x}_{-}\). Then we have:

\[-C_{0}\int_{T_{\Pi}}^{t}e^{f_{-}(s)}\mathrm{d}s-C_{-}^{-}\leq f_{-}(t)\leq-C_ {0}\int_{T_{\Pi}}^{t}e^{f_{-}(s)}\mathrm{d}s-C_{-}^{-}+C_{-}^{+}.\]

Let \(\Psi(t):=\int_{T_{\Pi}}^{t}e^{f_{-}(s)}\mathrm{d}s\), then \(\frac{\mathrm{d}\Psi(t)}{\mathrm{d}t}=e^{f_{-}(t)}\). So \(\Psi(T_{\Pi})=0\) and

\[-C_{0}\Psi(t)-C_{-}^{-}\leq\log\left(\frac{\mathrm{d}\Psi(t)}{ \mathrm{d}t}\right)\leq-C_{0}\Psi(t)-C_{-}^{-}+C_{-}^{+},\] \[e^{-C_{-}^{-}}e^{-C_{0}\Psi(t)}\leq\frac{\mathrm{d}\Psi(t)}{ \mathrm{d}t}\leq e^{-C_{-}^{-}+C_{-}^{+}}e^{-C_{0}\Psi(t)}\]

For the right hand, for any \(\epsilon\in(0,1)\), we consider the auxiliary ODE:

\[\begin{cases}\frac{\mathrm{d}\mathcal{P}(t)}{\mathrm{d}t}=e^{-C_{-}^{-}+(1+ \epsilon)C_{-}^{+}}e^{-C_{0}\mathcal{P}(t)},\\ \mathcal{P}(T_{\Pi})=0.\end{cases}\]

The solution of this ODE is \(\mathcal{P}(t)=\frac{1}{C_{0}}\log\left(1+C_{0}e^{-C_{-}^{-}+(1+\epsilon)C_{-}^ {+}}(t-T_{\Pi})\right)\). From the Comparison Principle of ODEs, we have the upper bound for \(\Psi(t)\):

\[\Psi(t)\leq\mathcal{P}(t)=\frac{1}{C_{0}}\log\left(1+C_{0}e^{-C_{-}^{-}+(1+ \epsilon)C_{-}^{+}}(t-T_{\Pi})\right).\]

Taking \(\epsilon\to 0\), we obtain

\[\Psi(t)\leq\frac{1}{C_{0}}\log\left(1+C_{0}e^{-C_{-}^{-}+C_{-}^{+}}(t-T_{\Pi}) \right).\]In the similar way, we can derive the lower bound for \(\Psi(t)\):

\[\Psi(t)\geq\frac{1}{C_{0}}\log\left(1+C_{0}e^{-C_{-}^{-}}(t-T_{\rm II})\right).\]

Consequently, we infer that

\[f_{-}(t)\leq-C_{0}\Psi(t)-C_{-}^{-}+C_{-}^{+}\leq-\log\left(1+C_{0}e^{-C_{-}^{- }}(t-T_{\rm II})\right)-C_{-}^{-}+C_{-}^{+},\]

\[f_{-}(t)\geq-C_{0}\Psi(t)-C_{-}^{-}\geq-\log\left(1+C_{0}e^{-C_{-}^{-}+C_{-}^{+ }}(t-T_{\rm II})\right)-C_{-}^{-}.\]

Noticing \(f_{-}(T_{\rm II})=C_{-}^{+}-C_{-}^{-}\), we obtain

\[-\log\left(1+C_{0}e^{-C_{-}^{-}+C_{-}^{+}}(t-T_{\rm II})\right)-C_{-}^{+}\leq f _{-}(t)-f_{-}(T_{\rm II})\leq-\log\left(1+C_{0}e^{-C_{-}^{-}}(t-T_{\rm II}) \right).\]

Noticing \(f_{-}(T_{\rm II})=C_{-}^{+}-C_{-}^{-}\), this inequality means

\[\frac{e^{-C_{+}^{+}}}{1+C_{0}e^{f_{-}(T_{\rm II})}(t-T_{\rm II})}\leq e^{f_{- }(t)-f_{-}(T_{\rm II})}\leq\frac{1}{1+C_{0}e^{(f_{-}(T_{\rm II})-C_{-}^{+})}(t -T_{\rm II})}.\]

where \(C_{0}=\frac{m_{-}\kappa_{2}^{2}\sin^{2}\Delta}{m(1+p)}=\Theta\left(\frac{ \kappa_{2}^{2}\Delta^{2}}{p}\right)\). Moreover, according to Lemma D.8 (S1)(S2), we have

\[e^{f_{-}(T_{\rm II})}=\Theta\left(p^{-\frac{\alpha\cos\Delta}{1-\alpha\cos \Delta}}\right),\]

\[C_{-}^{+}=\sum_{k\in\mathcal{K}_{+}}\frac{\kappa_{2}}{\sqrt{m}}\bm{b}_{k}^{\top }(T_{\rm II})\bm{x}_{-}=\mathcal{O}\left(m_{-}\frac{\kappa_{2}}{\sqrt{m}} \frac{\sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}\right)=\mathcal{O}\left(\kappa_{ 2}\sqrt{\kappa_{1}\kappa_{2}}\right).\]

Step III. Evolution of \(f_{+}(t)\).

Let \(k\in\mathcal{K}_{+}\). According to Lemma E.1 (S2)(P2) and (S2)(P3), it follows that for any \(k\in\mathcal{K}_{+}\), its dynamics during \(t\in[T_{\rm II},T_{\rm II}^{\rm PT*}]\) is

\[\frac{\mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t}=\frac{\kappa_{2}}{ \sqrt{m}}\left\langle\bm{F}_{+}(t),\bm{x}_{+}\right\rangle;\] \[\text{or}\;\frac{\mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t}=\frac{ \kappa_{2}pe^{-f_{+}(t)}}{\sqrt{m}(1+p)}\Big{(}\bm{x}_{+}-\bm{x}_{-}\cos\Delta \Big{)}.\]

Notice that

\[f_{+}(t)= \sum_{k\in\mathcal{K}_{+}}\frac{\kappa_{2}}{\sqrt{m}}\bm{b}_{k}^ {\top}(t)\bm{x}_{+}=\sum_{k\in\mathcal{K}_{+}}\frac{\kappa_{2}}{\sqrt{m}}\bm{b} _{k}^{\top}(T_{\rm II})\bm{x}_{+}+\sum_{k\in\mathcal{K}_{+}}\int_{T_{\rm II}}^ {t}\frac{\kappa_{2}}{\sqrt{m}}\left\langle\frac{\mathrm{d}\bm{b}_{k}(t)}{ \mathrm{d}s},\bm{x}_{+}\right\rangle\mathrm{d}s\] \[= f_{+}(T_{\rm II})+\frac{\kappa_{2}}{\sqrt{m}}\sum_{k\in\mathcal{ K}_{+}}\int_{T_{\rm II}}^{t}\left\langle\frac{\mathrm{d}\bm{b}_{k}(t)}{ \mathrm{d}s},\bm{x}_{+}\right\rangle\mathrm{d}s.\]

On the one hand, for any \(t\in[T_{\rm II},T_{\rm II}^{\rm PT*}]\), we have the lower bound:

\[f_{+}(t)\geq f_{+}(T_{\rm II})+\frac{\kappa_{2}^{2}}{m}\sum_{k\in\mathcal{K}_{+ }}\int_{T_{\rm II}}^{t}\min\Big{\{}\left\langle\bm{F}_{+}(s),\bm{x}_{+} \right\rangle,\frac{pe^{-f_{+}(s)}}{1+p}\sin^{2}\Delta\Big{\}}\mathrm{d}s\geq f _{+}(T_{\rm II}).\]

On the other hand, for any \(t\in[T_{\rm II},T_{\rm II}^{\rm PT*}]\), we can derive an upper bound:

\[f_{+}(t)\leq f_{+}(T_{\rm II})+\frac{\kappa_{2}^{2}}{m}\sum_{k\in\mathcal{K}_{+ }}\int_{T_{\rm II}}^{t}\max\Big{\{}\left\langle\bm{F}_{+}(s),\bm{x}_{+}\right\rangle,\frac{pe^{-f_{+}(s)}}{1+p}\sin^{2}\Delta\Big{\}}\mathrm{d}s\] \[\leq f_{+}(T_{\rm II})+\frac{\kappa_{2}^{2}}{m}\sum_{k\in\mathcal{K}_{+ }}\int_{T_{\rm II}}^{t}\max\Big{\{}\frac{pe^{-f_{+}(s)}}{1+p},\frac{pe^{-f_{+} (s)}}{1+p}\sin^{2}\Delta\Big{\}}\mathrm{d}s\] \[\leq f_{+}(T_{\rm II})+\frac{\kappa_{2}^{2}}{m}\sum_{k\in\mathcal{K}_{+ }}\int_{T_{\rm II}}^{t}\frac{pe^{-f_{+}(s)}}{1+p}\mathrm{d}s\leq f_{+}(T_{\rm II })+\frac{\kappa_{2}^{2}m_{+}}{m}\frac{pe^{-f_{+}(T_{\rm II})}}{1+p}(t-T_{\rm II }).\]Hence, we obtain

\[\exp\left(-\frac{\kappa_{2}^{2}m_{+}}{m}\frac{pe^{-f_{+}(T_{\rm II})}}{1+p}(t-T_{ \rm II})\right)\leq e^{f_{+}(T_{\rm II})-f_{+}(t)}\leq 1,\]

where

\[\frac{\kappa_{2}^{2}m_{+}}{m}\frac{pe^{-f_{+}(T_{\rm II})}}{1+p}\stackrel{{ \rm Lemma\,D.8}}{{=}}\Theta\left(\kappa_{2}^{2}e^{-f_{+}(T_{\rm II})} \right)=\Theta\left(\kappa_{2}^{2}p^{-\frac{1}{1-\alpha\cos\Delta}}\right).\]

**Lemma E.3** (Nearly fixed vector filed in Phase III*).: _There exist absolute constants \(Q_{1},Q_{2}>0\), such that: For any time \(T_{\rm fix}\in[T_{\rm II},+\infty)\), if we choose \(\kappa_{1},\kappa_{2}\) s.t._

\[\kappa_{2}^{2}\bigg{(}T_{\rm fix}\wedge T_{\rm II}^{\rm PT*}-T_{\rm II}\bigg{)} p^{-\frac{1}{1-\alpha\cos\Delta}}=\mathcal{O}(\Delta^{2}),\quad\kappa_{2}^{2} \sqrt{\frac{\kappa_{1}}{\kappa_{2}}}=\mathcal{O}(\Delta^{2}),\]

_then for any \(t\in[T_{\rm II},T_{\rm fix}\wedge T_{\rm II}^{\rm PT*}]\), we have_

\[\langle\boldsymbol{F}_{+}(t),\boldsymbol{x}_{+}\rangle\leq\frac{Q_{1}}{2} \Delta^{2}p^{-\frac{1}{1-\alpha\cos\Delta}},\quad\langle\boldsymbol{F}_{+}(t),\boldsymbol{x}_{-}\rangle\geq-\frac{Q_{2}}{2}\Delta^{2}p^{-\frac{1}{1-\alpha \cos\Delta}}.\]

Proof of Lemma E.3.: For simplicity, we denote \(\delta_{T}:=T_{\rm fix}\wedge T_{\rm II}^{\rm PT*}-T_{\rm II}\) From Lemma E.2, for any \(t\in[T_{\rm II},T_{\rm fix}\wedge T_{\rm II}^{\rm PT*}]\), we have

\[e^{f_{-}(t)-f_{-}(T_{\rm II})}-1 \leq\frac{1}{1+C_{0}e^{(f_{-}(T_{\rm II})-C_{1})}\delta_{T}}-1 \leq 0,\] \[e^{f_{-}(t)-f_{-}(T_{\rm II})}-1 \geq\frac{e^{-C_{1}}}{1+C_{0}e^{f_{-}(T_{\rm II})}\delta_{T}}-1 =\frac{e^{-C_{1}}-1-C_{0}e^{f_{-}(T_{\rm II})}\delta_{T}}{1+C_{0}e^{f_{-}(T_{ \rm II})}\delta_{T}}\geq-\frac{C_{1}+C_{0}e^{f_{-}(T_{\rm II})}\delta_{T}}{1+C _{0}e^{f_{-}(T_{\rm II})}\delta_{T}}\] \[e^{f_{+}(T_{\rm II})-f_{+}(t)}-1 \leq 0,\] \[e^{f_{+}(T_{\rm II})-f_{+}(t)}-1 \geq e^{-C_{2}\delta_{T}}-1\geq-C_{2}\delta_{T}.\]

Recalling Lemma D.8 (S1), there exists absolute constants \(Q_{1},Q_{2}>0\) such that

\[\langle\boldsymbol{F}_{+}(T_{\rm II}),\boldsymbol{x}_{+}\rangle =\frac{pe^{-f_{+}(T_{\rm II})}}{1+p}-\frac{e^{f_{-}(T_{\rm II})}}{1+p}\cos \Delta\geq Q_{1}\Delta^{2}p^{-\frac{1}{1-\alpha\cos\Delta}},\] \[\langle\boldsymbol{F}_{+}(T_{\rm II}),\boldsymbol{x}_{-}\rangle =\frac{pe^{-f_{+}(T_{\rm II})}}{1+p}\cos\Delta-\frac{e^{f_{-}(T_{ \rm II})}}{1+p}\leq-Q_{2}\Delta^{2}p^{-\frac{1}{1-\alpha\cos\Delta}}.\]

Step I. Bounding the term \(\langle\boldsymbol{F}_{+}(t),\boldsymbol{x}_{+}\rangle\).

\[\left|\langle\boldsymbol{F}_{+}(t),\boldsymbol{x}_{+}\rangle- \langle\boldsymbol{F}_{+}(T_{\rm II}),\boldsymbol{x}_{+}\rangle\right|\] \[= \left|\frac{p}{1+p}e^{-f_{+}(t)}-\frac{p}{1+p}e^{-f_{+}(T_{\rm II })}-\frac{e^{f_{-}(t)}}{1+p}\cos\Delta+\frac{e^{f_{-}(T_{\rm II})}}{1+p}\cos \Delta\right|\] \[\leq \left|\frac{p}{1+p}e^{-f_{+}(t)}-\frac{p}{1+p}e^{-f_{+}(T_{\rm II })}\right|+\left|\frac{e^{f_{-}(t)}}{1+p}\cos\Delta-\frac{e^{f_{-}(T_{\rm II })}}{1+p}\cos\Delta\right|\] \[\leq \frac{p}{1+p}e^{-f_{+}(T_{\rm II})}\left|e^{f_{+}(T_{\rm II})-f_ {+}(t)}-1\right|+\frac{e^{f_{-}(T_{\rm II})}}{1+p}\left|e^{f_{-}(t)-f_{-}(T_{ \rm II})}-1\right|\] \[\leq \frac{p}{1+p}e^{-f_{+}(T_{\rm II})}C_{2}\delta_{T}+\frac{e^{f_{-} (T_{\rm II})}}{1+p}\frac{C_{1}+C_{0}e^{f_{-}(T_{\rm II})}\delta_{T}}{1+C_{0}e^ {f_{-}(T_{\rm II})}\delta_{T}}\]

To ensure \(\left|\langle\boldsymbol{F}_{+}(t),\boldsymbol{x}_{+}\rangle-\langle \boldsymbol{F}_{+}(T_{\rm II}),\boldsymbol{x}_{+}\rangle\right|\leq\frac{1}{2}Q _{1}\Delta^{2}p^{-\frac{1}{1-\alpha\cos\Delta}}\), we need only select parameters such that

\[\frac{p}{1+p}e^{-f_{+}(T_{\rm II})}C_{2}\delta_{T}+\frac{e^{f_{-}(T_{\rm II})} }{1+p}\frac{C_{1}+C_{0}e^{f_{-}(T_{\rm II})}\delta_{T}}{1+C_{0}e^{f_{-}(T_{\rm II })}\delta_{T}}\leq\frac{1}{2}Q_{1}\Delta^{2}p^{-\frac{1}{1-\alpha\cos\Delta}}.\]From Lemma E.2 and Lemma D.8, we have:

\[C_{0}=\Theta\left(\frac{\kappa_{2}^{2}\Delta^{2}}{p}\right),\;C_{1}= \mathcal{O}\left(\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}\right),\;C_{2}=\Theta \left(\kappa_{2}^{2}p^{-\frac{1}{1-\alpha\cos\Delta}}\right),\] \[e^{-f_{+}(T_{\rm II})}=\Theta\left(p^{-\frac{1}{1-\alpha\cos \Delta}}\right),\quad e^{f_{-}(T_{\rm II})}=\Theta\left(p^{-\frac{\alpha\cos \Delta}{1-\alpha\cos\Delta}}\right).\]

Therefore, if we take

\[C_{0}e^{f_{-}(T_{\rm II})}\delta_{T}=\Theta\left(\kappa_{2}^{2}\Delta^{2}p^{- \frac{1}{1-\alpha\cos\Delta}}\delta_{T}\right)=\mathcal{O}(1),\]

then we have

\[\frac{p}{1+p}e^{-f_{+}(T_{\rm II})}C_{2}\delta_{T}+\frac{e^{f_{-}( T_{\rm II})}}{1+p}\frac{C_{1}+C_{0}e^{f_{-}(T_{\rm II})}\delta_{T}}{1+C_{0}e^{f_{-} (T_{\rm II})}\delta_{T}}\] \[= \Theta\left(\kappa_{2}^{2}p^{-\frac{2}{1-\alpha\cos\Delta}} \delta_{T}\right)+\Theta\left(p^{-\frac{1}{1-\alpha\cos\Delta}}\left(\mathcal{ O}(\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}})+\kappa_{2}^{2}\Delta^{2}p^{-\frac{1}{1- \alpha\cos\Delta}}\delta_{T}\right)\right)\] \[= \Theta\left(\kappa_{2}^{2}p^{-\frac{1}{1-\alpha\cos\Delta}}\left( \delta_{T}p^{-\frac{1}{1-\alpha\cos\Delta}}+\mathcal{O}(\sqrt{\frac{\kappa_{1 }}{\kappa_{2}}})\right)\right).\]

If we can take

\[\kappa_{2}^{2}\delta_{T}p^{-\frac{1}{1-\alpha\cos\Delta}}=\mathcal{O}(\Delta^{ 2}),\quad\kappa_{2}^{2}\sqrt{\frac{\kappa_{1}}{\kappa_{2}}}=\mathcal{O}( \Delta^{2}),\]

then \(\kappa_{2}^{2}\Delta^{2}p^{-\frac{1}{1-\alpha\cos\Delta}}\delta_{T}=\mathcal{O }(1)\) and

\[|\langle\bm{F}_{+}(t),\bm{x}_{+}\rangle-\langle\bm{F}_{+}(T_{\rm II}),\bm{x}_{ +}\rangle|=\mathcal{O}\left(\Delta^{2}p^{-\frac{1}{1-\alpha\cos\Delta}}\right) \leq\frac{1}{2}Q_{1}\Delta^{2}p^{-\frac{1}{1-\alpha\cos\Delta}},\]

Hence,

\[\langle\bm{F}_{+}(t),\bm{x}_{+}\rangle\geq \langle\bm{F}_{+}(T_{\rm II}),\bm{x}_{+}\rangle-|\langle\bm{F}_{+ }(t),\bm{x}_{+}\rangle-\langle\bm{F}_{+}(T_{\rm II}),\bm{x}_{+}\rangle|\] \[\geq \frac{1}{2}Q_{1}\Delta^{2}p^{-\frac{1}{1-\alpha\cos\Delta}}= \Omega\left(\Delta^{2}p^{-\frac{1}{1-\alpha\cos\Delta}}\right),\quad\forall t \in[T_{\rm II},T_{\rm fix}\wedge T_{\rm II}^{\rm PT*}].\]

Step II. Bounding the term \(\langle\bm{F}_{+}(t),\bm{x}_{-}\rangle\).

The proof can be completed by the method analogous to that used in Step I, and we omit it. The result is

\[\langle\bm{F}_{+}(t),\bm{x}_{-}\rangle\geq \langle\bm{F}_{+}(T_{\rm II}),\bm{x}_{-}\rangle+|\langle\bm{F}_{+ }(t),\bm{x}_{+}\rangle-\langle\bm{F}_{+}(T_{\rm II}),\bm{x}_{-}\rangle|\] \[\geq -\frac{1}{2}Q_{2}\Delta^{2}p^{-\frac{1}{1-\alpha\cos\Delta}}\equiv -\Omega\left(\Delta^{2}p^{-\frac{1}{1-\alpha\cos\Delta}}\right),\quad\forall t \in[T_{\rm II},T_{\rm fix}\wedge T_{\rm II}^{\rm PT*}].\]

**Lemma E.4** (The end of Phase Transition).: _If we choose \(\kappa_{1},\kappa_{2}\) s.t \(\kappa_{2}=\mathcal{O}(1)\) and \(\sqrt{\frac{\kappa_{1}}{\kappa_{2}}}=\mathcal{O}\left(\Delta^{4}\right)\) (3), then it holds that_

_(S1) (Time)._

\[T_{\rm II}^{\rm PT}=T_{\rm II}^{\rm PT*}=T_{\rm II}+\mathcal{O}\left(\sqrt{ \frac{\kappa_{1}}{\kappa_{2}}}\frac{p^{\frac{1}{1-\alpha\cos\Delta}}}{\Delta^{2 }}\right)=\left(1+\mathcal{O}\left(\sqrt{\kappa_{1}\kappa_{2}^{3}}\right) \right)T_{\rm II};\]

_(S2) (Prediction)._

\[e^{-f_{+}(T_{\rm II}^{\rm PT})}=\Theta\left(p^{-\frac{1}{1- \alpha\cos\Delta}}\right),\quad e^{f_{-}(T_{\rm II}^{\rm PT})}=\Theta\left(p^{ -\frac{\alpha\cos\Delta}{1-\alpha\cos\Delta}}\right);\] \[\frac{pe^{-f_{+}(T_{\rm II}^{\rm PT})}}{1+p}-\frac{e^{f_{-}(T_{ \rm II}^{\rm PT})}}{1+p}\cos\Delta=\Theta\left(\Delta^{2}p^{-\frac{1}{1- \alpha\cos\Delta}}\right),\] \[\frac{pe^{-f_{+}(T_{\rm II}^{\rm PT})}}{1+p}\cos\Delta-\frac{e^{f_ {-}(T_{\rm II}^{\rm PT})}}{1+p}=-\Theta\left(\Delta^{2}p^{-\frac{1}{1-\alpha \cos\Delta}}\right).\]

_(S3) (Activation patterns)._

\[\left\langle\bm{w}_{k}(T_{\rm II}^{\rm PT}),\bm{x}_{+}\right\rangle>0, \;\left\langle\bm{w}_{k}(T_{\rm II}^{\rm PT}),\bm{x}_{-}\right\rangle=0,\; \forall k\in\mathcal{K}_{+};\] \[\left\langle\bm{w}_{k}(T_{\rm II}^{\rm PT}),\bm{x}_{+}\right\rangle=0,\;\left\langle\bm{w}_{k}(T_{\rm II}^{\rm PT}),\bm{x}_{-}\right\rangle>0,\; \forall k\in\mathcal{K}_{-}.\]Proof of Lemma e.4.:

Step I. Time Estimate. Let \(k\in\mathcal{K}_{+}\).

Recalling the definition of \(T_{\Pi,k}^{\mathrm{PT}*}\) in Lemma E.1, Lemma E.1 (S2)(P0) also gives us

\[T_{\Pi}^{\mathrm{PT}*}=\max_{k\in\mathcal{K}_{+}}T_{\Pi,k}^{\mathrm{PT}*}.\]

From Lemma D.8 (S2), we know that there exists an absolute constant \(Q_{3}>0\), s.t. \(0\leq\langle\bm{b}_{k}(T_{\Pi}),\bm{x}_{-}\rangle\leq Q_{3}\frac{\sqrt{\kappa_ {1}\kappa_{2}}}{\sqrt{m}}\). And we let \(Q_{2}>0\) be the absolute constant \(Q_{2}\) in Lemma E.3.

First, we choose the time

\[T_{\mathrm{fix}}=T_{\Pi}+\frac{3Q_{3}}{Q_{2}\Delta^{2}}\sqrt{\frac{\kappa_{1} }{\kappa_{2}}}p^{\frac{1}{1-\alpha\cos\Delta}}.\]

then we choose \(\kappa_{1},\kappa_{2}\) s.t.

\[\kappa_{2}=\mathcal{O}(1),\quad\sqrt{\frac{\kappa_{1}}{\kappa_{2}}}=\mathcal{ O}\left(\Delta^{4}\right).\]

It can ensure

\[\kappa_{2}^{2}\Big{(}T_{\mathrm{fix}}\wedge T_{\Pi}^{\mathrm{PT}*}-T_{\Pi} \Big{)}p^{-\frac{1}{1-\alpha\cos\Delta}}=\Theta\left(\frac{\kappa_{2}^{2}}{ \Delta^{2}}\sqrt{\frac{\kappa_{1}}{\kappa_{2}}}\right)=\mathcal{O}(\Delta^{2} ),\quad\kappa_{2}^{2}\sqrt{\frac{\kappa_{1}}{\kappa_{2}}}=\mathcal{O}(\Delta^ {2}).\]

Then according to Lemma E.3, it follows that

\[\langle\bm{F}_{+}(t),\bm{x}_{-}\rangle\leq-\frac{Q_{2}}{2}\Delta^{2}p^{-\frac {1}{1-\alpha\cos\Delta}},\quad\forall t\in[T_{\Pi},T_{\Pi,k}^{\mathrm{PT}*} \wedge T_{\mathrm{fix}}).\]

Now we consider the dynamics for \(t\in[T_{\Pi},T_{\Pi,k}^{\mathrm{PT}*}\wedge T_{\mathrm{fix}})\).

Recalling lemma E.1, we have

\[\big{\langle}\bm{b}_{k}(T_{\Pi,k}^{\mathrm{PT}*}\wedge T_{\mathrm{ fix}}),\bm{x}_{-}\big{\rangle}=\langle\bm{b}_{k}(T_{\Pi}),\bm{x}_{-}\rangle+ \int_{T_{\Pi}}^{t}\left\langle\frac{\mathrm{d}\bm{b}_{k}(s)}{\mathrm{d}s},\bm{ x}_{+}\right\rangle\mathrm{d}s\] \[= \langle\bm{b}_{k}(T_{\Pi}),\bm{x}_{-}\rangle+\frac{\kappa_{2}}{ \sqrt{m}}\int_{T_{\Pi}}^{T_{\Pi,k}^{\mathrm{PT}*}\wedge T_{\mathrm{fix}}} \langle\bm{F}_{+}(s),\bm{x}_{-}\rangle\,\mathrm{d}s\] \[\leq Q_{3}\frac{\sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}-\frac{Q_{2}}{ 2}\Delta^{2}p^{-\frac{1}{1-\alpha\cos\Delta}}\Big{(}T_{\Pi,k}^{\mathrm{PT}*} \wedge T_{\mathrm{fix}}-T_{\Pi}\Big{)}\] \[\leq Q_{3}\frac{\sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}-\frac{Q_{2}}{ 2}\Delta^{2}p^{-\frac{1}{1-\alpha\cos\Delta}}\Big{(}(T_{\Pi,k}-T_{\Pi})\wedge \frac{3Q_{3}}{Q_{2}\Delta^{2}}\sqrt{\frac{\kappa_{1}}{\kappa_{2}}}p^{\frac{1} {1-\alpha\cos\Delta}}\Big{)}.\]

We claim \(T_{\Pi,k}^{\mathrm{PT}*}-T_{\Pi}\leq\frac{2Q_{3}}{Q_{2}\Delta^{2}}\sqrt{\frac {\kappa_{1}}{\kappa_{2}}}p^{\frac{1}{1-\alpha\cos\Delta}}\). If otherwise, then

\[\big{\langle}\bm{b}_{k}(T_{\Pi,k}^{\mathrm{PT}*}\wedge T_{\mathrm{fix}}),\bm{ x}_{-}\big{\rangle}<Q_{3}\frac{\sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}-Q_{3} \frac{\sqrt{\kappa_{1}\kappa_{2}}}{\sqrt{m}}=0.\]

From the definition of \(T_{\Pi,k}^{\mathrm{PT}*}\), we know \(T_{\Pi,k}^{\mathrm{PT}*}<T_{\Pi,k}^{\mathrm{PT}*}\wedge T_{\mathrm{fix}}\), which leads to a contradiction.

therefore, we have proved that for any \(k\in\mathcal{K}_{+}\),

\[T_{\Pi,k}^{\mathrm{PT}*}\wedge T_{\mathrm{fix}}=T_{\Pi,k}^{ \mathrm{PT}*};\] \[T_{\Pi,k}^{\mathrm{PT}*}\leq T_{\Pi}+\frac{2Q_{3}}{Q_{2}\Delta^{2}} \sqrt{\frac{\kappa_{1}}{\kappa_{2}}}p^{\frac{1}{1-\alpha\cos\Delta}}.\]

With the help of Lemma E.1 (S2)(P0), we obtain

\[T_{\Pi}^{\mathrm{PT}*}\wedge T_{\mathrm{fix}}=T_{\Pi}^{\mathrm{PT}*};\] \[T_{\Pi}^{\mathrm{PT}*}=\max_{k\in\mathcal{K}_{+}}T_{\Pi,k}^{ \mathrm{PT}*}\leq T_{\Pi}+\frac{2Q_{3}}{Q_{2}\Delta^{2}}\sqrt{\frac{\kappa_{1}} {\kappa_{2}}}p^{\frac{1}{1-\alpha\cos\Delta}}.\]Recalling Lemma E.3, \(\langle\bm{F}_{+}(t),\bm{x}_{+}\rangle>0\) and \(\langle\bm{F}_{+}(t),\bm{x}_{-}\rangle<0\) hold for any \(t\in[T_{\rm II},T_{\rm II}^{\rm PT*}\wedge T_{\rm fix}]=[T_{\rm II},T_{\rm II}^{ \rm PT*}]\). From the definitions of \(T_{\rm II}^{\rm PT}\) and \(T_{\rm II}^{\rm PT*}\) (26)(27), we obtain

\[T_{\rm II}^{\rm PT}=T_{\rm II}^{\rm PT*}.\]

In conclusion, we have proved:

\[T_{\rm II}^{\rm PT}=T_{\rm II}^{\rm PT*}=T_{\rm II}+\mathcal{O}\left(\sqrt{ \frac{\kappa_{1}}{\kappa_{2}}}\frac{p^{\frac{1}{1-\alpha\cos\Delta}}}{\Delta^{2 }}\right)\stackrel{{\rm Lemma\,D.6}}{{=}}\left(1+\mathcal{O} \left(\sqrt{\kappa_{1}\kappa_{2}^{3}}\right)\right)T_{\rm II}.\]

Step II. Prediction Estimate. Step I gives us the result:

\[\delta_{T}:=T_{\rm II}^{\rm PT}-T_{\rm II}=\mathcal{O}\left(\sqrt{\frac{ \kappa_{1}}{\kappa_{2}}}\frac{p^{\frac{1}{1-\alpha\cos\Delta}}}{\Delta^{2}} \right).\]

Recalling the proof of Lemma E.2, we know

\[-\frac{C_{1}+C_{0}e^{f_{-}(T_{\rm II})}\delta_{T}}{1+C_{0}e^{f_{- }(T_{\rm II})}\delta_{T}}\leq e^{f_{-}(t)-f_{-}(T_{\rm II})}-1\leq 0,\] \[-C_{2}\delta_{T}\leq e^{f_{+}(T_{\rm II})-f_{+}(t)}-1\leq 0.\]

where

\[C_{0}=\Theta\left(\frac{\kappa_{2}^{2}\Delta^{2}}{p}\right),\;C_{1}=\mathcal{O }\left(\kappa_{2}\sqrt{\kappa_{1}\kappa_{2}}\right),\;C_{2}=\Theta\left( \kappa_{2}^{2}p^{-\frac{1}{1-\alpha\cos\Delta}}\right).\]

Then a straightforward calculation gives us:

\[0\geq e^{f_{-}(t)-f_{-}(T_{\rm II})}-1=-\mathcal{O}\left(\kappa_{2}^{2}\sqrt {\frac{\kappa_{1}}{\kappa_{2}}}\right)-\mathcal{O}\left(\kappa_{2}^{2}\sqrt {\frac{\kappa_{1}}{\kappa_{2}}}\right)=-\mathcal{O}\left(\kappa_{2}^{2}\sqrt {\frac{\kappa_{1}}{\kappa_{2}}}\right)\]

\[0\geq e^{f_{+}(T_{\rm II})-f_{+}(T_{\rm II}^{\rm PT})}-1=-\mathcal{O}\left( \kappa_{2}^{2}\sqrt{\frac{\kappa_{1}}{\kappa_{2}}}\frac{1}{\Delta^{2}}\right).\]

With the help of Lemma D.8, we obtain the prediction estimate at the end of Phase III:

\[e^{-f_{+}(T_{\rm II}^{\rm PT})}= e^{f_{+}(T_{\rm II})}e^{f_{+}(T_{\rm II})-f_{+}(T_{\rm II}^{\rm PT })}=\Theta\left(e^{-f_{+}(T_{\rm II})}\right)=\Theta\left(p^{-\frac{1}{1- \alpha\cos\Delta}}\right),\] \[e^{f_{-}(T_{\rm II}^{\rm PT})}= e^{f_{-}(T_{\rm II})}e^{f_{-}(T_{\rm II}^{\rm PT})-f_{-}(T_{\rm II })}=\Theta\left(e^{f_{-}(T_{\rm II})}\right)=\Theta\left(p^{-\frac{\alpha \cos\Delta}{1-\alpha\cos\Delta}}\right).\]

Moreover,

\[\left|\left(\frac{pe^{-f_{+}(T_{\rm II}^{\rm PT})}}{1+p}-\frac{e ^{f_{-}(T_{\rm II}^{\rm PT})}}{1+p}\cos\Delta\right)-\left(\frac{pe^{-f_{+}(T_ {\rm II})}}{1+p}-\frac{e^{f_{-}(T_{\rm II})}}{1+p}\cos\Delta\right)\right|\] \[\leq \left|\frac{pe^{-f_{+}(T_{\rm II})}}{1+p}\right|\left|\frac{pe^{ -f_{+}(T_{\rm II}^{\rm PT})}}{\frac{1+p}{pe^{-f_{+}(T_{\rm II})}}}-1\right|+ \left|\frac{e^{f_{-}(T_{\rm II})}\cos\Delta}{1+p}\right|\left|\frac{e^{f_{-}( T_{\rm II}^{\rm PT})}\cos\Delta}{\frac{1+p}{1+p}}-1\right|\] \[= \mathcal{O}\left(p^{-\frac{1}{1-\alpha\cos\Delta}}\kappa_{2}^{2} \sqrt{\frac{\kappa_{1}}{\kappa_{2}}}\frac{1}{\Delta^{2}}\right)+\mathcal{O} \left(p^{-\frac{1}{1-\alpha\cos\Delta}}\kappa_{2}^{2}\sqrt{\frac{\kappa_{1}}{ \kappa_{2}}}\right)\] \[= \mathcal{O}\left(p^{-\frac{1}{1-\alpha\cos\Delta}}\kappa_{2}^{2} \sqrt{\frac{\kappa_{1}}{\kappa_{2}}}\frac{1}{\Delta^{2}}\right)^{\kappa_{1} \kappa_{2}^{2}}\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!
Step III. Activation Patterns.

Recall our proofs in Step I, we know that

\[\left\langle\bm{w}_{k}(T_{\mathrm{II}}^{\mathrm{PT}}),\bm{x}_{-}\right\rangle=0, \ \forall k\in\mathcal{K}_{+}.\]

Moreover, from the dynamics in Lemma E.1 (S1) and (S2)(P3), we obtain:

\[\left\langle\bm{w}_{k}(T_{\mathrm{II}}^{\mathrm{PT}}),\bm{x}_{+} \right\rangle>0,\ \forall k\in\mathcal{K}_{+};\] \[\left\langle\bm{w}_{k}(T_{\mathrm{II}}^{\mathrm{PT}}),\bm{x}_{+} \right\rangle=0,\ \left\langle\bm{w}_{k}(T_{\mathrm{II}}^{\mathrm{PT}}),\bm{x}_{-} \right\rangle>0,\ \forall k\in\mathcal{K}_{-}.\]

Proof of Theorem 4.6.: Theorem 4.6 (S1) has been proven in Lemma E.4 (S1), and Theorem 4.6 (S2) has been proven in Lemma E.4 (S3).

### Optimization Dynamics after Phase Transition

After Phase Transition (\(t>T_{\mathrm{II}}^{\mathrm{PT}}\)), we study the dynamics before the patterns of living neurons change again. Specifically, we define the following hitting time

\[T_{\mathrm{III}} :=\inf\Big{\{}t>T_{\mathrm{II}}^{\mathrm{PT}}:\exists k\in \mathcal{K}_{+}\cup\mathcal{K}_{-},\,\mathsf{sgn}_{k}^{+}(t)\neq\mathsf{sgn}_ {k}^{+}(T_{\mathrm{I}})\text{ or }\mathsf{sgn}_{k}^{-}(t)\neq\mathsf{sgn}_{k}^{-}(T_{ \mathrm{I}})\Big{\}}\] (28) \[=\inf\Big{\{}t>T_{\mathrm{II}}^{\mathrm{PT}}:\exists k\in \mathcal{K}_{+},\ \text{s.t.}\ \left\langle\bm{w}_{k}(t),\bm{x}_{+}\right\rangle\leq 0 \text{ or }\left\langle\bm{w}_{k}(t),\bm{x}_{-}\right\rangle\neq 0;\] \[\text{ or }\exists k\in\mathcal{K}_{-},\ \text{s.t.}\ \left\langle\bm{w}_{k}(t),\bm{x}_{+} \right\rangle\neq 0\text{ or }\left\langle\bm{w}_{k}(t),\bm{x}_{-}\right\rangle\leq 0\Big{\}},\]

and we call \(t\in(T_{\mathrm{II}}^{\mathrm{PT}},T_{\mathrm{III}})\) "L-Phase III".

Moreover, we call \(t\in[T_{\mathrm{II}},T_{\mathrm{III}})\) "Phase III", i.e.. "Phase Transition" + "L-Phase III".

In order to analyze the dynamics of neurons and vector fields, we introduce the auxiliary hitting time:

\[T_{\mathrm{III}}^{*}:=T_{\mathrm{III}}\wedge\inf\Big{\{}t>T_{ \mathrm{II}}^{\mathrm{PT}}: \left\langle\bm{F}_{+}(t),\bm{x}_{+}\right\rangle\leq 0\text{ or }\left\langle\bm{F}_{+}(t),\bm{x}_{-}\right\rangle\geq 0 \Big{\}},\] (29) \[\text{ where }\quad\bm{F}_{+}(t)= \frac{p}{1+p}e^{-f_{+}(t)}\bm{x}_{+}-\frac{1}{1+p}e^{f_{-}(t)}\bm{ x}_{-}.\]

We call \(t\in(T_{\mathrm{II}}^{\mathrm{PT}},T_{\mathrm{III}}^{*})\) "L-Phase III".

Due to the almost simplest activation patterns, this phase is easier to analyze, and we only need to estimate the time and size of the changes in the vector field. Nevertheless, our challenge is to prove that all living negative neurons simultaneously change their activation patterns at \(T_{\mathrm{III}}^{*}\), which also implies that \(T_{\mathrm{III}}=T_{\mathrm{III}}^{*}\).

**Lemma E.5** (Dynamics of activate neurons during L-Phase III*).:

_In L-Phase III* \((t\in[T_{\mathrm{II}}^{\mathrm{PT}},T_{\mathrm{III}}^{*}))\), we have the following dynamics for each neuron \(k\in\mathcal{K}_{-}\cup\mathcal{K}_{+}\)._

_(S1). For negative neuron_ \(k\in\mathcal{K}_{-}\)_, we have:_

\[\bm{w}_{k}(t)\in\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^{+},\] \[\frac{\mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t}=\frac{\kappa_{2}e^{f_{ -}(t)}}{\sqrt{m}(1+p)}\Big{(}\bm{x}_{-}-\bm{x}_{+}\cos\Delta\Big{)}.\]

_(S2) For positive neuron_ \(k\in\mathcal{K}_{+}\)_, we have:_

\[\bm{w}_{k}(t)\in\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^{0},\] \[\frac{\mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t}=\frac{\kappa_{2}pe^{-f _{+}(t)}}{\sqrt{m}(1+p)}\Big{(}\bm{x}_{+}-\bm{x}_{-}\cos\Delta\Big{)}.\]Proof of Lemma e.5.: From the definition of \(T_{\text{III}}^{*}\), we know that \(\langle\bm{F}_{+}(t),\bm{x}_{+}\rangle>0\) and \(\langle\bm{F}_{+}(t),\bm{x}_{-}\rangle<0\) hold for any \(t\in[T_{\text{II}}^{\text{PT}},T_{\text{III}}^{*})\). Moreover, Lemma E.4 ensures that for \(k\in\mathcal{K}_{+}\), \(\bm{w}_{k}(T_{\text{II}}^{\text{PT}})\in\mathcal{M}_{+}^{0}\cap\mathcal{M}_{-}^ {+}\); for \(k\in\mathcal{K}_{-}\), \(\bm{w}_{k}(T_{\text{II}}^{\text{PT}})\in\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-}^ {0}\). Hence, this lemma can be proved in the same way as shown in the proof of Lemma E.1 (S1) and (S2)(P2). We do not repeat it here.

**Lemma E.6** (Time and prediction estimate at the end of L-Phase III*).:

_(S1) (Time)._

\[T_{\text{III}}^{*}=T_{\text{II}}^{\text{PT}}+\Theta\left(\frac{p^{\frac{1}{1- \alpha\cos\Delta}}}{\kappa_{2}^{2}}\right)=\left(1+\Theta(\Delta^{2})\right)T _{\text{II}}^{\text{PT}}=\left(1+\Theta(\Delta^{2})\right)T_{\text{II}};\]

_(S2) (Prediction)._

\[e^{-f_{+}(T_{\text{III}}^{*})}=\Theta\left(p^{-\frac{1}{1-\alpha \cos\Delta}}\right),\quad e^{f_{-}(T_{\text{III}}^{*})}=\Theta\left(p^{-\frac{ \alpha\cos\Delta}{1-\alpha\cos\Delta}}\right);\] \[\frac{pe^{-f_{+}(T_{\text{III}}^{*})}}{1+p}-\frac{e^{f_{-}(T_{ \text{III}}^{*})}}{1+p}\cos\Delta=0,\] \[\frac{pe^{-f_{+}(T_{\text{II}}^{\text{PT}})}}{1+p}\cos\Delta- \frac{e^{f_{-}(T_{\text{III}}^{\text{PT}})}}{1+p}=-\Theta\left(\Delta^{2}p^{- \frac{1}{1-\alpha\cos\Delta}}\right).\]

Proof of Lemma e.6.: Step I. Explicit Solution to \(f_{+}(t)\) and \(f_{-}(t)\).

For any \(t\in[T_{\text{II}}^{\text{PT}},T_{\text{III}}^{*})\), we have:

\[f_{+}(t) =\frac{\kappa_{2}}{\sqrt{m}}\sum_{k\in\mathcal{K}_{+}}\bm{b}_{k}^ {\top}(t)\bm{x}_{+},\] \[f_{-}(t) =-\frac{\kappa_{2}}{\sqrt{m}}\sum_{k\in\mathcal{K}_{-}}\bm{b}_{k}^ {\top}(t)\bm{x}_{-}.\]

Let us consider the dynamics of \(f_{+}(t)\) and \(f_{-}(t)\). With the help of Lemma E.5, these two dynamics are nearly independent:

\[\frac{\mathrm{d}f_{+}(t)}{\mathrm{d}t}= =\frac{\kappa_{2}}{\sqrt{m}}\sum_{k\in\mathcal{K}_{+}}\left\langle \frac{\kappa_{2}pe^{-f_{+}(t)}}{\sqrt{m}(1+p)}\Big{(}\bm{x}_{+}-\bm{x}_{-}\cos \Delta\Big{)},\bm{x}_{+}\right\rangle=\frac{\kappa_{2}^{2}m_{+}p\sin^{2} \Delta}{m(1+p)}e^{-f_{+}(t)},\] \[\frac{\mathrm{d}f_{-}(t)}{\mathrm{d}t}= =-\frac{\kappa_{2}}{\sqrt{m}}\sum_{k\in\mathcal{K}_{-}}\left\langle \frac{\kappa_{2}e^{f_{-}(t)}}{\sqrt{m}(1+p)}\Big{(}\bm{x}_{-}-\bm{x}_{+}\cos \Delta\Big{)},\bm{x}_{-}\right\rangle=-\frac{\kappa_{2}^{2}m_{-}\sin^{2} \Delta}{m(1+p)}e^{f_{-}(t)}.\]

Their solutions are:

\[e^{-f_{+}(t)} =\frac{e^{-f_{+}(T_{\text{II}}^{\text{PT}})}}{1+e^{-f_{+}(T_{ \text{II}}^{\text{PT}})}\frac{\kappa_{2}^{2}m_{+}p\sin^{2}\Delta}{m(1+p)}(t-T_{ \text{II}}^{\text{PT}})},\] \[e^{f_{-}(t)} =\frac{e^{f_{-}(T_{\text{II}}^{\text{PT}})}}{1+e^{f_{-}(T_{\text{ II}}^{\text{PT}})}\frac{\kappa_{2}^{2}m_{-}\sin^{2}\Delta}{m(1+p)}(t-T_{\text{II}}^{ \text{PT}})}.\]

Step II. Time Estimate of \(T_{\text{III}}^{*}\).

For simplicity, we denote \(G_{+}:=\frac{\kappa_{2}^{2}m_{+}p\sin^{2}\Delta}{m(1+p)}\) and \(G_{-}:=\frac{\kappa_{2}^{2}m_{-}\sin^{2}\Delta}{m(1+p)}\).

First, we consider the evolution of the vector field \(\langle\bm{F}_{+}(t),\bm{x}_{-}\rangle\):

\[\langle\bm{F}_{+}(t),\bm{x}_{-}\rangle=\frac{pe^{-f_{+}(t)}}{1+p}\cos\Delta- \frac{e^{f_{-}(t)}}{1+p}\]\[= \frac{1}{1+p}\left(\frac{pe^{-f_{+}(T_{\rm II}^{\rm PT})}\cos\Delta}{1+e ^{-f_{+}(T_{\rm II}^{\rm PT})}G_{+}(t-T_{\rm II}^{\rm PT})}-\frac{e^{f_{-}(T_{ \rm II}^{\rm PT})}}{1+e^{f_{-}(T_{\rm II}^{\rm PT})}G_{-}(t-T_{\rm II}^{\rm PT })}\right)\] \[= \frac{(pe^{-f_{+}(T_{\rm II}^{\rm PT})}\cos\Delta-e^{f_{-}(T_{ \rm II}^{\rm PT})})+e^{f_{-}(T_{\rm II}^{\rm PT}-f_{+}(T_{\rm II}^{\rm PT})}( pG_{-}\cos\Delta-G_{+})(t-T_{\rm II}^{\rm PT})}{(1+p)(1+e^{-f_{+}(T_{\rm II}^{\rm PT })}G_{+}(t-T_{\rm II}^{\rm PT}))(1+e^{f_{-}(T_{\rm II}^{\rm PT})}G_{-}(t-T_{ \rm II}^{\rm PT}))}\] \[= \frac{(1+p)\left\langle\mbox{\boldmath$F$}_{+}(T_{\rm II}^{\rm PT }),\mbox{\boldmath$x$}_{-}\right\rangle+e^{f_{-}(T_{\rm II}^{\rm PT}-f_{+}(T _{\rm II}^{\rm PT})}(pG_{-}\cos\Delta-G_{+})(t-T_{\rm II}^{\rm PT})}{(1+p)(1+ e^{-f_{+}(T_{\rm II}^{\rm PT})}G_{+}(t-T_{\rm II}^{\rm PT}))(1+e^{f_{-}(T_{ \rm II}^{\rm PT})}G_{-}(t-T_{\rm II}^{\rm PT}))}<0.\]

Hence, the hitting time \(T_{\rm III}^{*}\) can be converted to the following \(T_{\rm III}^{**}\):

\[T_{\rm III}^{*}=T_{\rm III}^{**}:=T_{\rm III}\wedge\inf\Big{\{}t>T_{\rm II}^{ \rm PT}:\langle\mbox{\boldmath$F$}_{+}(t),\mbox{\boldmath$x$}_{+}\rangle\leq 0 \Big{\}}.\]

Then we consider \(\langle\mbox{\boldmath$F$}_{+}(t),\mbox{\boldmath$x$}_{+}\rangle\):

\[\langle\mbox{\boldmath$F$}_{+}(t),\mbox{\boldmath$x$}_{+}\rangle =\frac{pe^{-f_{+}(t)}}{1+p}-\frac{e^{f_{-}(t)}}{1+p}\cos\Delta\] \[= \frac{1}{1+p}\left(\frac{pe^{-f_{+}(T_{\rm II}^{\rm PT})}}{1+e^{ -f_{+}(T_{\rm II}^{\rm PT})}G_{+}(t-T_{\rm II}^{\rm PT})}-\frac{e^{f_{-}(T_{ \rm II}^{\rm PT})}\cos\Delta}{1+e^{f_{-}(T_{\rm II}^{\rm PT})}G_{-}(t-T_{\rm II }^{\rm PT})}\right)\] \[= \frac{(pe^{-f_{+}(T_{\rm II}^{\rm PT})}-e^{f_{-}(T_{\rm II}^{\rm PT })}\cos\Delta)+e^{f_{-}(T_{\rm II}^{\rm PT}-f_{+}(T_{\rm II}^{\rm PT})}(pG_{ -}-G_{+}\cos\Delta)(t-T_{\rm II}^{\rm PT})}{(1+p)(1+e^{-f_{+}(T_{\rm II}^{\rm PT })}G_{+}(t-T_{\rm II}^{\rm PT}))(1+e^{f_{-}(T_{\rm II}^{\rm PT})}G_{-}(t-T_{ \rm II}^{\rm PT}))}\] \[= \frac{(1+p)\left\langle\mbox{\boldmath$F$}_{+}(T_{\rm II}^{\rm PT }),\mbox{\boldmath$x$}_{+}\right\rangle+e^{f_{-}(T_{\rm II}^{\rm PT})-f_{+}(T _{\rm II}^{\rm PT})}(pG_{-}-G_{+}\cos\Delta)(t-T_{\rm II}^{\rm PT})}{(1+p)(1+ e^{-f_{+}(T_{\rm II}^{\rm PT})}G_{+}(t-T_{\rm II}^{\rm PT}))(1+e^{f_{-}(T_{\rm II}^{ \rm PT})}G_{-}(t-T_{\rm II}^{\rm PT}))}.\]

From Lemma E.4, we know

\[(1+p)\left\langle\mbox{\boldmath$F$}_{+}(T_{\rm II}^{\rm PT}), \mbox{\boldmath$x$}_{+}\right\rangle=(1+p)\left(\frac{pe^{-f_{+}(t)}}{1+p}- \frac{e^{f_{-}(t)}}{1+p}\cos\Delta\right)=\Theta\left(\Delta^{2}p^{-\frac{ \cos\Delta}{1-\alpha\cos\Delta}}\right),\] \[e^{f_{-}(T_{\rm II}^{\rm PT})-f_{+}(T_{\rm II}^{\rm PT})}= \Theta\left(e^{f_{-}(T_{\rm II+II})-f_{+}(T_{\rm II})}\right)=\Theta\left(p^{ -\frac{1+\alpha\cos\Delta}{1-\alpha\cos\Delta}}\right),\] \[pG_{-}-G_{+}\cos\Delta=\frac{\kappa_{2}^{2}p\sin^{2}\Delta}{1+p} \frac{(m_{-}-m_{+}\cos\Delta)}{m}=-\Theta\left(\kappa_{2}^{2}\Delta^{2} \right).\]

These imply the hitting time:

\[T_{\rm III}^{*}=T_{\rm III}^{**}=T_{\rm II}^{\rm PT}+\Theta\left(\frac{\Delta ^{2}p^{-\frac{\alpha\cos\Delta}{1-\alpha\cos\Delta}}}{p^{-\frac{1+\alpha\cos \Delta}{1-\alpha\cos\Delta}}\kappa_{2}^{2}\Delta^{2}}\right)=T_{\rm II}^{\rm PT }+\Theta\left(\frac{p^{\frac{1}{1-\alpha\cos\Delta}}}{\kappa_{2}^{2}}\right).\]

Step III. Prediction estimate.

From the explicit solution in Step I and the time estimate in Step II, it is easy to verify

\[e^{-f_{+}(T_{\rm III}^{*})}=\frac{e^{-f_{+}(T_{\rm II}^{\rm PT})}}{1+e^{-f_{+}( T_{\rm II}^{\rm PT})}\frac{\kappa_{2}^{2}m_{+}p\sin^{2}\Delta}{m(1+p)}(T_{\rm III}^{*}-T_{ \rm II}^{\rm PT})}=\Theta\left(p^{-\frac{1}{1-\alpha\cos\Delta}}\right),\]

\[e^{f_{-}(T_{\rm III}^{*})}=\frac{e^{f_{-}(T_{\rm II}^{\rm PT})}}{1+e^{f_{-}(T_{ \rm II}^{\rm PT})}\frac{\kappa_{2}^{2}m_{+}p\sin^{2}\Delta}{m(1+p)}(T_{\rm III }^{*}-T_{\rm II}^{\rm PT})}=\Theta\left(p^{-\frac{\alpha\cos\Delta}{1-\alpha \cos\Delta}}\right).\]

Recalling the calculation in Step II, we have:

\[\langle\mbox{\boldmath$F$}_{+}(T_{\rm III}^{*}),\mbox{\boldmath$x$}_{+}\rangle= \frac{pe^{-f_{+}(T_{\rm III}^{*})}}{1+p}-\frac{e^{f_{-}(T_{\rm III}^{*})}}{1+p} \cos\Delta=0,\]

\[\langle\mbox{\boldmath$F$}_{+}(T_{\rm III}^{*}),\mbox{\boldmath$x$}_{-}\rangle= \frac{pe^{-f_{+}(T_{\rm III}^{*})}}{1+p}\cos\Delta-\frac{e^{f_{-}(T_{\rm III}^{*})} }{1+p}\]\[\begin{split}& T^{\mathrm{N}}_{\mathrm{III}}=T^{\mathrm{F}}_{\mathrm{III}} \wedge T^{\mathrm{N}}_{\mathrm{III}}=\inf\big{\{}t>T^{\mathrm{PT}}_{\mathrm{II}} :\langle\bm{F}_{+}(t),\bm{x}_{+}\rangle\leq 0\text{ or }\exists k\in\mathcal{K}_{-},\text{ s.t. } \langle\bm{w}_{k}(t),\bm{x}_{+}\rangle\neq 0\big{\}}.\end{split}\]

It is obvious that \(T^{\mathrm{N}}_{\mathrm{III}}\geq T^{\mathrm{F}}_{\mathrm{III}}\wedge T^{ \mathrm{N}}_{\mathrm{III}}=T^{*}_{\mathrm{III}}\). Now we prove \(T^{\mathrm{N}}_{\mathrm{III}}=T^{*}_{\mathrm{III}}\).

If we assume \(T^{\mathrm{N}}_{\mathrm{III}}>T^{*}_{\mathrm{III}}\) strictly, then the dynamics about \(f_{+}(t)\) and \(f_{-}(t)\) in the proof (Step I) of Lemma E.6 still hold for any \(t\in[T^{*}_{\mathrm{III}},T^{\mathrm{N}}_{\mathrm{III}})\). Using the same calculate about \(\langle\bm{F}_{+}(t),\bm{x}_{+}\rangle\) in the proof (Step II, III) of Lemma E.6, we can obtain: \(\langle\bm{F}_{+}(t),\bm{x}_{+}\rangle<0,\;t\in[T^{*}_{\mathrm{III}},T^{*}_{ \mathrm{III}})\).

Then we consider the vector field around the manifold \(\mathcal{M}^{0}_{+}\cap\mathcal{M}^{+}_{-}\) for \(t\in[T^{*}_{\mathrm{III}},T^{\mathrm{N}}_{\mathrm{III}})\). In the same way as the proof of Lemma E.1 (S1), we can prove that the two-side projections onto \(\bm{x}_{+}\) (the normal to the surface \(\mathcal{M}^{+}_{+}\cap\mathcal{M}^{0}_{-}\)) satisfies \(f^{+}_{N}(t,\tilde{\bm{w}}),f^{-}_{N}(t,\tilde{\bm{w}})>0\) for any \(t\in[T^{*}_{\mathrm{III}},T^{*}_{\mathrm{III}}+\tau_{1})\), which satisfies (Case II) in Definition H.1. This implies that \(\bm{w}_{k}(t)\) enter the manifold \(\mathcal{M}^{+}_{+}\), i.e., \(\langle\bm{w}_{k}(t),\bm{x}_{+}\rangle>0\) for any \(t\in[T^{*}_{\mathrm{III}},T^{\mathrm{N}}_{\mathrm{III}})\), which is contradict to the definition of \(T^{\mathrm{N}}_{\mathrm{III}}\). Hence, we have proved

\[T^{\mathrm{N}}_{\mathrm{III}}=T^{\mathrm{N}}_{\mathrm{III}}\wedge T^{\mathrm{F}} _{\mathrm{III}}=T^{*}_{\mathrm{III}}.\]

Noticing that the change of activation patterns of \(\bm{\mathrm{sgn}}^{+}_{k}(t)\) (\(k\in\mathcal{K}_{-}\)) is due to the change of the vector field \(\langle\bm{F}_{+}(t),\bm{x}_{+}\rangle\), it is easy to verify that \(T^{\mathrm{F}}_{\mathrm{III}}=T^{\mathrm{N}}_{\mathrm{III}}\wedge T^{\mathrm{F} }_{\mathrm{III}}\ast\). Then we have \(T^{\mathrm{N}}_{\mathrm{III}}=T^{\mathrm{F}}_{\mathrm{III}}=T^{\mathrm{N}}_{ \mathrm{III}}\wedge T^{\mathrm{F}}_{\mathrm{III}}\ast\).

Moreover, noticing that \(T_{\mathrm{III}}\leq T^{\mathrm{N}}_{\mathrm{III}}\) and \(T^{*}_{\mathrm{III}}\leq T_{\mathrm{III}}\), we obtain \(T_{\mathrm{III}}=T^{*}_{\mathrm{III}}=T^{\mathrm{N}}_{\mathrm{III}}=T^{\mathrm{F}} _{\mathrm{III}}\).

Lastly, noticing that all living negative neurons (\(k\in\mathcal{K}_{-}\)) belong to \(\mathcal{M}^{0}_{+}\cap\mathcal{M}^{+}_{-}\) at time \(T_{\mathrm{III}}\). As discussed above, for each living negative neuron \(k\in\mathcal{K}_{-}\), the vector field near \(\bm{b}_{k}(T_{\mathrm{III}})\) is the same, with \(f^{-}_{N}>0\) and \(f^{+}_{N}=0\) in Definition H.1 (Case II). Hence, each living positive neuron \(\bm{w}_{k}\) leaves from \(\mathcal{M}^{0}_{+}\) and enter \(\mathcal{M}^{+}_{+}\) instantly at \(T_{\mathrm{III}}\), which means \(T^{\mathrm{W}}_{\mathrm{III}}=T^{\mathrm{N}}_{\mathrm{III}}\).

Hence, we have proved \(T_{\mathrm{III}}=T^{*}_{\mathrm{III}}=T^{\mathrm{W}}_{\mathrm{III}}=T^{\mathrm{N }}_{\mathrm{III}}=T^{\mathrm{F}}_{\mathrm{III}}\). 

Proof of Theorem 4.7.: Combining Lemma E.6 and E.7, we obtain \(T_{\mathrm{III}}=\left(1+\Theta(\Delta^{2})\right)T_{\mathrm{II}}\).

Proofs of Optimization Dynamics in Phase IV

Proof of Theorem 4.8.: From Lemma E.7, we know that all living negative neuron \(k\in\mathcal{K}_{-}\) simultaneously change their patterns on \(\bm{x}_{+}\) at \(T_{\mathrm{III}}\): \(\lim\limits_{t\to T_{\mathrm{III}}^{-}}\mathsf{sgn}_{k}^{+}(t)=0\), \(\lim\limits_{t\to T_{\mathrm{III}}^{+}}\mathsf{sgn}_{k}^{+}(t)=1\). Moreover, from our proof of Lemma E.7, we know that other activation patterns remain unchanged at \(T_{\mathrm{III}}\). 

In this phase, we study the dynamics before activation patterns change again after the phase transition in Theorem 4.8. We define the hitting time:

\[T_{\mathrm{IV}}:=\inf\{t>T_{\mathrm{III}}:\exists k\in\mathcal{K}_{+}\cup \mathcal{K}_{-},\mathsf{sgn}_{k}^{+}(t)\neq\lim\limits_{s\to T_{\mathrm{III}}^ {+}}\mathsf{sgn}_{k}^{+}(s)\text{ or }\mathsf{sgn}_{k}^{-}(t)\neq\lim\limits_{s\to T_{ \mathrm{III}}^{+}}\mathsf{sgn}_{k}^{-}(s)\},\]

and we call \(t\in(T_{\mathrm{III}},T_{\mathrm{IV}})\) Phase IV.

In order to analyze the dynamics of neurons and vector fields, we introduce the auxiliary hitting time:

\[\begin{split} T_{\mathrm{IV}}^{*}:=\inf\Big{\{}t>T_{\mathrm{III}} :&\langle\bm{F}_{+}(t),\bm{x}_{+}\rangle>0,\text{ or }\langle\bm{F}_{+}(t),\bm{x}_{-}\rangle>0\Big{\}},\\ \text{where}\quad\bm{F}_{+}(t)=&\frac{p}{1+p}e^{-f _{+}(t)}\bm{x}_{+}-\frac{1}{1+p}e^{f_{-}(t)}\bm{x}_{-},\end{split}\] (30)

and we call \(t\in(T_{\mathrm{III}},T_{\mathrm{IV}}^{*})\) Phase IV*.

First, we will provide meticulous prior estimations for \(2\mathrm{d}\) ODEs on \(f_{+}(t)\) and \(f_{-}(t)\), similar to Phase II, which can imply \(T_{\mathrm{IV}}^{*}=+\infty\). Additionally, we can prove \(T_{\mathrm{IV}}=T_{\mathrm{IV}}^{*}\). Lastly, with the help of our fine-grained analysis for the 2D dynamics and the results in (Lyu and Li, 2019; Ji and Telgarsky, 2020), we can determine the unique convergent direction from numerous KKT directions.

### Non-asymptotic Analysis of Optimization Dynamics in Phase IV*

**Lemma E.1** (Dynamics of activate neurons in Phase IV*).:

_In Phase IV* \((t\in(T_{\mathrm{III}},T_{\mathrm{IV}}^{*}))\), we have the following dynamics for each neuron \(k\in\mathcal{K}_{-}\cup\mathcal{K}_{+}\)._

_(S1). For negative neuron \(k\in\mathcal{K}_{-}\), we have:_

\[\begin{split}&\bm{w}_{k}(t)\in\mathcal{M}_{+}^{+}\cap\mathcal{M}_{-} ^{+},\\ &\frac{\mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t}=-\frac{\kappa_{2}}{ \sqrt{m}}\bm{F}_{+}(t)=-\frac{\kappa_{2}}{\sqrt{m}}\left(\frac{p}{1+p}e^{-f_{ +}(t)}\bm{x}_{+}-\frac{1}{1+p}e^{f_{-}(t)}\bm{x}_{-}\right).\end{split}\]

_(S2) For positive neuron \(k\in\mathcal{K}_{+}\), we have:_

\[\begin{split}&\bm{w}_{k}(t)\in\mathcal{M}_{+}^{+}\cap\mathcal{M}_{ -}^{0},\\ &\frac{\mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t}=\frac{\kappa_{2}pe^{- f_{+}(t)}}{\sqrt{m}(1+p)}\Big{(}\bm{x}_{+}-\bm{x}_{-}\cos\Delta\Big{)}.\end{split}\]

Proof of Lemma F.1.: Using the definition of \(T_{\mathrm{IV}}^{*}\), this lemma can be proved in the same way as shown in the proof of Lemma D.1, E.1 and E.5. 

The next lemma gives the first-order dynamics of \(f_{+}(t)\) and \(f_{-}(t)\).

**Lemma F.2** (First-order Dynamics of predictions in Phase IV*).:

_In Phase IV* \((T_{\mathrm{III}}\leq t\leq T_{\mathrm{IV}}^{*})\), we have the following dynamics for \(f_{+}(t)\) and \(f_{-}(t)\):_

\[\begin{split}&\frac{\mathrm{d}f_{+}(t)}{\mathrm{d}t}=\kappa_{2}^{2} \frac{m_{+}}{m}\frac{pe^{-f_{+}(t)}}{1+p}\sin^{2}\Delta+\kappa_{2}^{2}\frac{m_{ -}}{m}\left(\frac{pe^{-f_{+}(t)}}{1+p}-\frac{ef^{-}(t)\cos\Delta}{1+p}\right),\\ &\frac{\mathrm{d}f_{-}(t)}{\mathrm{d}t}=\kappa_{2}^{2}\frac{m_{- }}{m}\left(\frac{pe^{-f_{+}(t)}}{1+p}\cos\Delta-\frac{e^{f_{-}(t)}}{1+p} \right).\end{split}\]Proof of Lemma f.2.: From the definition of \(T_{\mathrm{IV}}\), for any \(T_{\mathrm{III}}\leq t\leq T_{\mathrm{IV}}\), we have

\[f_{+}(t) =\sum_{k\in\mathcal{K}_{+}}\frac{\kappa_{2}}{\sqrt{m}}\bm{b}_{k}^{ \top}(t)\bm{x}_{+}-\sum_{k\in\mathcal{K}_{-}}\frac{\kappa_{2}}{\sqrt{m}}\bm{b}_ {k}^{\top}(t)\bm{x}_{+},\] \[f_{-}(t) =-\sum_{k\in\mathcal{K}_{-}}\frac{\kappa_{2}}{\sqrt{m}}\bm{b}_{k} ^{\top}(t)\bm{x}_{-}.\]

With the help of Lemma F.1, we have the dynamics of predictions:

\[\frac{\mathrm{d}f_{+}(t)}{\mathrm{d}t} =\sum_{k\in\mathcal{K}_{+}}\frac{\kappa_{2}}{\sqrt{m}}\left\langle \frac{\mathrm{d}\bm{b}_{k}(t)}{\mathrm{d}t},\bm{x}_{+}\right\rangle-\sum_{k\in \mathcal{K}_{-}}\frac{\kappa_{2}}{\sqrt{m}}\left\langle\frac{\mathrm{d}\bm{b} _{k}(t)}{\mathrm{d}t},\bm{x}_{+}\right\rangle\] \[= \frac{\kappa_{2}^{2}}{m}\sum_{k\in\mathcal{K}_{+}}\frac{p}{1+p}e ^{-f_{+}(t)}\left(1-\cos^{2}\Delta\right)-\frac{\kappa_{2}^{2}}{m}\sum_{k\in \mathcal{K}_{-}}\left(\frac{\cos\Delta}{1+p}e^{f_{-}(t)}-\frac{p}{1+p}e^{-f_{+ }(t)}\right)\] \[= \frac{m_{+}}{m}\kappa_{2}^{2}\frac{p}{1+p}e^{-f_{+}(t)}\sin^{2} \Delta+\frac{m_{-}}{m}\kappa_{2}^{2}\left(\frac{pe^{-f_{+}(t)}}{1+p}-\frac{ \cos\Delta}{1+p}e^{f-(t)}\right).\]

Following the proof in Phase II, we focus on the dynamics about predictions. Due to the specificity of the first-order dynamics, the following lemma gives an second-order **autonomous** dynamics of predictions.

**Lemma F.3** (Second-order Autonomous Dynamics of predictions in Phase IV*).: _If we consider the following two variables:_

\[\begin{cases}\mathcal{I}(t):=\kappa_{2}^{2}\frac{m_{-}}{m}\frac{p}{1+p}e^{-f_{ +}(t)},\\ \mathcal{J}(t):=\kappa_{2}^{2}\frac{m_{-}}{m}\frac{1}{1+p}e^{f_{-}(t)},\end{cases}\]

_then the following autonomous dynamics of \(\mathcal{U}(t)\) and \(\mathcal{V}(t)\) hold in Phase IV* \((T_{\mathrm{III}}\leq t\leq T_{\mathrm{IV}}^{*})\):_

\[\begin{cases}\frac{\mathrm{d}\mathcal{I}(t)}{\mathrm{d}t}=\mathcal{I}(t) \mathcal{J}(t)\cos\Delta-\mathcal{I}^{2}(t)\left(1+\frac{m_{+}}{m_{-}}\sin^{2} \Delta\right),\\ \frac{\mathrm{d}\mathcal{J}(t)}{\mathrm{d}t}=\mathcal{I}(t)\mathcal{J}(t) \cos\Delta-\mathcal{J}^{2}(t).\end{cases}\]

Proof of Lemma f.3.: With the help of the first-order dynamics in Lemma F.2, the proof is straight-forward. 

Lemma F.3 enlighten us that we only need to study the dynamics of \(\mathcal{I}(t)\) and \(\mathcal{J}(t)\) to study the dynamics in Phase IV*, where \(\mathcal{I}(t),\mathcal{J}(t)\) satisfies the following autonomous dynamics:

\[\begin{cases}\frac{\mathrm{d}\mathcal{I}(t)}{\mathrm{d}t}=\mathcal{I}(t) \mathcal{J}(t)\cos\Delta-\mathcal{I}^{2}(t)\left(1+\frac{m_{+}}{m_{-}}\sin^{2} \Delta\right),\\ \frac{\mathrm{d}\mathcal{J}(t)}{\mathrm{d}t}=\mathcal{I}(t)\mathcal{J}(t) \cos\Delta-\mathcal{J}^{2}(t),\end{cases}\qquad t\geq T_{\mathrm{III}};\] (31)

The next lemma studies the dynamics (31) for any \(t\in[T_{\mathrm{III}},+\infty)\).

**Lemma F.4** (Fine-grained analysis of the dynamics (31)).: _For the dynamics (31), we have the following results:_

**(S1).**_Initialization._

\[\mathcal{I}(T_{\mathrm{III}})-\mathcal{J}(T_{\mathrm{III}})\cos \Delta=0,\quad\mathcal{I}(T_{\mathrm{III}})\cos\Delta-\mathcal{J}(T_{\mathrm{ III}})=-\Theta\left(\kappa_{2}^{2}2^{p-\frac{1}{1-\alpha\cos\Delta}}\right).\]

**(S2).**_Fine-grained two-side bound for \(\mathcal{I}(t)/\mathcal{J}(t)\)._

\[\frac{1+\cos\Delta}{1+\cos\Delta+\frac{m_{+}}{m_{-}}\sin^{2}\Delta}<\frac{ \mathcal{I}(t)}{\mathcal{J}(t)}<\cos\Delta,\quad\forall t\in[T_{\mathrm{III}}, +\infty).\]

**(S3).**_The limit of \(\mathcal{I}(t)/\mathcal{J}(t)\)._

\[\lim_{t\to\infty}\frac{\mathcal{I}(t)}{\mathcal{J}(t)}=\frac{1+\cos\Delta}{1+ \cos\Delta+\frac{m_{+}}{m_{-}}\sin^{2}\Delta}.\]

**(S4).**_Tight estimate of \(\mathcal{I}(t)\) and \(\mathcal{J}(t)\)._

\[\mathcal{I}(t)= \Theta\left(\frac{1}{p^{\frac{1-\alpha\cos\Delta}{\kappa_{2}^{2} }}+\Delta^{2}(t-T_{\mathrm{III}})}\right),\quad\forall t\in[T_{\mathrm{III}}, +\infty);\] \[\mathcal{J}(t)= \Theta\left(\frac{1}{\frac{1}{p^{\frac{1-\alpha\cos\Delta}{ \kappa_{2}^{2}}}}+\Delta^{2}(t-T_{\mathrm{III}})}\right),\quad\forall t\in[T_{ \mathrm{III}},+\infty).\]

Proof of Lemma F.4.: For simplicity, in this proof, we denote

\[\epsilon:=\frac{m_{+}}{m_{-}}\sin^{2}\Delta.\]

Step I. Preparation. Recalling Lemma E.6, we have:

\[\mathcal{I}(T_{\mathrm{III}})=\Theta\left(\kappa_{2}^{2}p^{-\frac{1}{1-\alpha \cos\Delta}}\right),\quad\mathcal{J}(T_{\mathrm{III}})=\Theta\left(\kappa_{2}^ {2}p^{-\frac{1}{1-\alpha\cos\Delta}}\right),\]

\[\mathcal{I}(T_{\mathrm{III}})-\mathcal{J}(T_{\mathrm{III}})\cos\Delta=0,\quad \mathcal{I}(T_{\mathrm{III}})\cos\Delta-\mathcal{J}(T_{\mathrm{III}})=-\Theta \left(\kappa_{2}^{2}\Delta^{2}p^{-\frac{1}{1-\alpha\cos\Delta}}\right).\]

Step II. A rough estimate on \(\mathcal{I}(t)\) and \(\mathcal{J}(t)\). In this step, we aim to prove:

\[\mathcal{J}(t)>\mathcal{I}(t)>0,\quad\mathcal{I}(t)+\mathcal{J}(t)\leq \mathcal{I}(T_{\mathrm{III}})+\mathcal{J}(T_{\mathrm{III}}),\quad\forall t\in[T _{\mathrm{III}},\infty).\]

First, from the definition of \(\mathcal{I}(t)\) and \(\mathcal{J}(t)\), we have \(\mathcal{I}(t)>0\) and \(\mathcal{J}(t)>0\).

Then we consider the dynamics of \(\mathcal{I}(t)+\mathcal{J}(t)\). From

\[= -(\mathcal{I}(t)-\mathcal{J}(t))^{2}\cos\Delta-(1-\cos\Delta) \mathcal{J}^{2}(t)-\mathcal{I}^{2}(t)\left(1+\epsilon-\cos\Delta\right)<0,\]

we have

\[\mathcal{I}(t)+\mathcal{J}(t)\leq\mathcal{I}(T_{\mathrm{III}})+\mathcal{J}(T_{ \mathrm{III}}),\quad\forall t\geq T_{\mathrm{III}}.\]

Then we consider the dynamics of \(\mathcal{J}(t)-\mathcal{I}(t)\). We define the hitting time

\[\tau_{\mathcal{J}-\mathcal{I}}:=\inf\Big{\{}t\geq T_{\mathrm{III}}:\mathcal{J}( t)\leq\mathcal{I}(t)\Big{\}}.\]

From Step I, we know \(\mathcal{J}(T_{\mathrm{III}})-\mathcal{I}(T_{\mathrm{III}})=(1-\cos\Delta) \mathcal{J}(T_{\mathrm{III}})>0\). From the continuity, \(\tau_{\mathcal{J}-\mathcal{I}}\) exists and \(\tau_{\mathcal{J}-\mathcal{I}}>T_{\mathrm{III}}\).

For any \(t\in[T_{\rm III},\tau_{\mathcal{J}-\mathcal{I}})\), we have \(\mathcal{J}(t)-\mathcal{I}(t)>0\) and

\[\frac{\mathrm{d}}{\mathrm{d}t}\Big{(}\mathcal{J}(t)-\mathcal{I}(t )\Big{)}=-\mathcal{J}^{2}(t)+\mathcal{I}^{2}(t)\left(1+\epsilon\right)=-( \mathcal{J}(t)+\mathcal{I}(t))(\mathcal{J}(t)-\mathcal{I}(t))+\epsilon \mathcal{I}^{2}(t)\] \[> -(\mathcal{J}(t)+\mathcal{I}(t))(\mathcal{J}(t)-\mathcal{I}(t)) \geq-(\mathcal{J}(T_{\rm III})+\mathcal{I}(T_{\rm III}))(\mathcal{J}(t)- \mathcal{I}(t)),\]

We consider the auxiliary ODE: \(\frac{\mathrm{d}}{\mathrm{d}t}\mathcal{P}(t)=-(\mathcal{J}(T_{\rm III})+ \mathcal{I}(T_{\rm III}))\mathcal{P}(t)\), where \(\mathcal{P}(T_{\rm III})=\mathcal{J}(T_{\rm III})-\mathcal{I}(T_{\rm III})>0\). From the Comparison Principle of ODEs, we have:

\[\mathcal{J}(t)-\mathcal{I}(t)\geq\mathcal{P}(t)=(\mathcal{J}(T_{\rm III})- \mathcal{I}(T_{\rm III}))\exp\Big{(}-(\mathcal{J}(T_{\rm III})+\mathcal{I}(T _{\rm III}))(t-T_{\rm III})\Big{)}>0,\ \forall t\in[T_{\rm I},\tau_{\mathcal{I}- \mathcal{V}}).\]

From the definition of \(\tau_{\mathcal{J}-\mathcal{I}}\), we have proved

\[\tau_{\mathcal{J}-\mathcal{I}}=+\infty;\] \[\mathcal{J}(t)>\mathcal{I}(t),\ \forall t\in[T_{\rm III},+\infty).\]

Step III. A rough two-side bound for \(\mathcal{I}(t)/\mathcal{J}(t)\).

In Step II, we have given a rough upper bound for \(\mathcal{I}(t)/\mathcal{J}(t)\): \(\mathcal{I}(t)/\mathcal{J}(t)<1\), \(\forall t\geq T_{\rm III}\). And we want to derive a lower bound for \(\mathcal{I}(t)/\mathcal{J}(t)\) in this step. We aim to prove:

\[\mathcal{I}(t)/\mathcal{J}(t)>\frac{1+\cos\Delta}{1+\cos\Delta+\epsilon},\quad \forall t\in[T_{\rm III},+\infty).\]

First, we define the hitting time:

\[\tau_{\mathcal{I}/\mathcal{J}}^{l}:=\inf\Big{\{}t\geq T_{\rm III}:\mathcal{I}( t)\leq\frac{1+\cos\Delta}{1+\cos\Delta+\epsilon}\mathcal{J}(t)\Big{\}}.\]

From Step I, we know

\[\mathcal{I}(T_{\rm III})-\frac{1+\cos\Delta}{1+\cos\Delta+\epsilon }\mathcal{J}(T_{\rm III})>\left(\cos\Delta-\frac{1+\cos\Delta}{1+\cos\Delta+ \epsilon}\right)\mathcal{J}(T_{\rm III})\] \[= \frac{\left(\frac{m_{+}}{m_{-}}\cos\Delta-1\right)\sin^{2}\Delta} {1+\cos\Delta+\epsilon}\mathcal{J}(T_{\rm III})\mathbf{\geq}\frac{\left(\frac {\cos\Delta}{0.977}-1\right)\sin^{2}\Delta}{1+\cos\Delta+\epsilon}\mathcal{J}(T _{\rm III})\] \[\geq \frac{\left(\frac{0.980}{0.977}-1\right)\sin^{2}\Delta}{1+\cos \Delta+\epsilon}\mathcal{J}(T_{\rm III})>0.\]

From the continuity, \(\tau_{\mathcal{I}/\mathcal{J}}^{l}\) exists and \(\tau_{\mathcal{I}/\mathcal{J}}^{l}>T_{\rm III}\).

For any \(t\in[T_{\rm III},\tau_{\mathcal{I}/\mathcal{J}}^{l})\), we have \(\mathcal{I}(t)-\frac{1+\cos\Delta}{1+\cos\Delta+\epsilon}\mathcal{J}(t)>0\) and

\[\frac{\mathrm{d}}{\mathrm{d}t}\left(\mathcal{I}(t)-\frac{1+\cos \Delta}{1+\cos\Delta+\epsilon}\mathcal{J}(t)\right)\] \[= \mathcal{I}(t)\mathcal{J}(t)\cos\Delta\Big{(}1-\frac{1+\cos\Delta} {1+\cos\Delta+\epsilon}\Big{)}-(1+\epsilon)\mathcal{I}^{2}(t)+\frac{1+\cos \Delta}{1+\cos\Delta+\epsilon}\mathcal{J}^{2}(t)\] \[= -\left(\mathcal{I}(t)-\frac{1+\cos\Delta}{1+\cos\Delta+\epsilon} \mathcal{J}(t)\right)\left((1+\epsilon)\mathcal{I}(t)+\mathcal{J}(t)\right)\] \[> -(1+\epsilon)\left(\mathcal{I}(t)-\frac{1+\cos\Delta}{1+\cos\Delta +\epsilon}\mathcal{J}(t)\right)\left(\mathcal{I}(t)+\mathcal{J}(t)\right)\] \[\geq -(1+\epsilon)\left(\mathcal{I}(T_{\rm III})+\mathcal{J}(T_{\rm III })\right)\left(\mathcal{I}(t)-\frac{1+\cos\Delta}{1+\cos\Delta+\epsilon} \mathcal{J}(t)\right).\]

We consider the auxiliary ODE: \(\frac{\mathrm{d}}{\mathrm{d}t}\mathcal{Q}(t)=-(1+\epsilon)(\mathcal{I}(T_{\rm III })+\mathcal{J}(T_{\rm III}))\mathcal{Q}(t)\), where \(\mathcal{Q}(T_{\rm III})=\mathcal{I}(T_{\rm III})-\frac{1+\cos\Delta}{1+\cos \Delta+\epsilon}\mathcal{J}(T_{\rm III})>0\). From the Comparison Principle of ODEs, we have:

\[\mathcal{I}(t)-\frac{1+\cos\Delta}{1+\cos\Delta+\epsilon}\mathcal{J}(t)\geq \mathcal{Q}(t)\]\[= \left(\mathcal{I}(T_{\rm III})-\frac{1+\cos\Delta}{1+\cos\Delta+ \epsilon}\mathcal{J}(T_{\rm III})\right)\exp\Big{(}-(1+\epsilon)(\mathcal{I}(T_{ \rm III})+\mathcal{J}(T_{\rm III}))(t-T_{\rm III})\Big{)}>0,\;\forall t\in[T_{ \rm III},\tau^{l}_{\mathcal{I}/\mathcal{J}}).\]

From the definition of \(\tau_{\mathcal{J}-\mathcal{I}}\), we have proved

\[\tau^{l}_{\mathcal{I}/\mathcal{J}}=+\infty;\] \[\mathcal{I}(t)/\mathcal{J}(t)>\frac{1+\cos\Delta}{1+\cos\Delta+ \epsilon},\quad\forall t\in[T_{\rm III},+\infty).\]

Hence, we obtain the two-side bound for \(\mathcal{I}(t)/\mathcal{J}(t)\):

\[\frac{1+\cos\Delta}{1+\cos\Delta+\epsilon}<\frac{\mathcal{I}(t)}{\mathcal{J}( t)}<1,\quad\forall t\in[T_{\rm III},+\infty).\]

Step IV. \(\mathcal{I}(t)\cos\Delta-\mathcal{J}(t)\) and \(\mathcal{I}(t)-\mathcal{J}(t)\cos\Delta\) are both negative.

The estimate on \(\mathcal{I}(t)\cos\Delta-\mathcal{J}(t)\) is straight-forward:

\[\mathcal{I}(t)\cos\Delta-\mathcal{J}(t)<\mathcal{I}(t)\cos\Delta-\mathcal{I}( t)<0.\]

As for \(\mathcal{I}(t)-\mathcal{J}(t)\cos\Delta\), we will actually prove a tighter upper bound:

\[\frac{\mathcal{I}(t)}{\mathcal{J}(t)}<\cos\Delta.\]

We need to do finer analysis using the specific dynamics (31). First, we define the following hitting time:

Define the following hitting time

\[\tau^{u}_{\mathcal{I}/\mathcal{J}}:=\inf\Big{\{}t>T_{\rm I}:\mathcal{I}(t)- \mathcal{J}(t)\cos\Delta\geq 0\}.\]

From \(\mathcal{I}(T_{\rm III})-\mathcal{J}(T_{\rm III})\cos\Delta=0\), \(\frac{{\rm d}}{{\rm d}t}(\mathcal{I}(T_{\rm III})-\mathcal{J}(T_{\rm III})\cos \Delta)<0\) and the continuity, we know \(\tau^{u}_{\mathcal{I}/\mathcal{J}}\) exists and \(\tau^{u}_{\mathcal{I}/\mathcal{J}}>T_{\rm III}\).

Recalling \(\mathcal{I}(t)\cos\Delta-\mathcal{J}(t)<0\), we have

\[\frac{{\rm d}}{{\rm d}t}\mathcal{J}(t)=\mathcal{J}(t)\left(\mathcal{I}(t) \cos\Delta-\mathcal{J}(t)\right)<0,\quad\forall t\geq T_{\rm III}.\]

So we can consider the following dynamics for \(t\in[T_{\rm III},\tau^{u}_{\mathcal{I}/\mathcal{J}}]\):

\[\frac{{\rm d}\mathcal{I}}{{\rm d}\mathcal{J}}=\frac{\mathcal{I}\mathcal{J} \cos\Delta-\mathcal{I}^{2}(1+\epsilon)}{\mathcal{I}\mathcal{J}\cos\Delta- \mathcal{J}^{2}}=\frac{\frac{\mathcal{I}}{\mathcal{J}}\cos\Delta-\left(\frac{ \mathcal{I}}{\mathcal{J}}\right)^{2}(1+\epsilon)}{\frac{\mathcal{I}}{ \mathcal{J}}\cos\Delta-1}.\]

If we define \(\mathcal{Z}(t):=\frac{\mathcal{I}(t)}{\mathcal{J}(t)}\), then we have \({\rm d}\mathcal{I}=\mathcal{Z}{\rm d}\mathcal{J}+\mathcal{J}{\rm d}\mathcal{Z}\).

The dynamics above can be transformed to:

\[\mathcal{J}\frac{{\rm d}\mathcal{Z}}{{\rm d}\mathcal{J}}=\frac{\mathcal{Z}\cos \Delta-\mathcal{Z}^{2}(1+\epsilon)}{\mathcal{Z}\cos\Delta-1}-\mathcal{Z}=\frac {\mathcal{Z}(1+\cos\Delta)-\mathcal{Z}^{2}(1+\cos\Delta+\epsilon)}{\mathcal{Z }\cos\Delta-1}.\]

Recalling the result in Step III, we have \((1+\cos\Delta)-(1+\cos\Delta+\epsilon)\mathcal{Z}(t)<0\) holds for any \(t\geq T_{\rm III}\). So the dynamics is equal to:

\[\frac{{\rm d}\mathcal{J}}{\mathcal{J}}= \left(\frac{\mathcal{Z}\cos\Delta-1}{\mathcal{Z}(1+\cos\Delta)- \mathcal{Z}^{2}(1+\cos\Delta+\epsilon)}\right){\rm d}\mathcal{Z}\] \[= -\frac{1}{1+\cos\Delta}\left(\frac{1}{\mathcal{Z}}+\frac{\sin^{2} \Delta+\epsilon}{1+\cos\Delta-\mathcal{Z}(1+\cos\Delta+\epsilon)}\right){\rm d }\mathcal{Z}.\]

Integrating this equation from \(T_{\rm III}\) to \(t\in[T_{\rm III},\tau^{u}_{\mathcal{I}/\mathcal{J}})\), we have:

\[\log\left(\frac{\mathcal{J}(t)}{\mathcal{J}(T_{\rm III})}\right)= -\frac{1}{1+\cos\Delta}\log\left(\frac{\mathcal{Z}(t)}{\mathcal{Z }(T_{\rm III})}\right)\] \[+\frac{\sin^{2}\Delta+\epsilon}{(1+\cos\Delta+\epsilon)(1+\cos \Delta)}\log\left(\frac{(1+\cos\Delta+\epsilon)\mathcal{Z}(t)-(1+\cos\Delta)}{ (1+\cos\Delta+\epsilon)\mathcal{Z}(T_{\rm I})-(1+\cos\Delta)}\right).\] (32)If we assume \(\tau^{u}_{\mathcal{I}/\mathcal{J}}<+\infty\), the continuity gives us

\[\lim_{t\rightarrow\tau^{u}_{\mathcal{I}/\mathcal{J}}}\mathcal{Z}(t)=\lim_{t \rightarrow\tau^{u}_{\mathcal{I}/\mathcal{J}}}\mathcal{I}(t)/\mathcal{J}(t)= \cos\Delta=\mathcal{I}(T_{\mathrm{III}})/\mathcal{J}(T_{\mathrm{III}})= \mathcal{Z}(T_{\mathrm{III}}).\]

Then letting \(t\rightarrow\tau^{u}_{\mathcal{I}/\mathcal{J}}\) in (32), we have

\[\lim_{t\rightarrow\tau^{u}_{\mathcal{I}/\mathcal{J}}}\log\left(\frac{ \mathcal{J}(t)}{\mathcal{J}(T_{\mathrm{III}})}\right)=0+0=0,\]

which means \(\mathcal{J}(\tau^{u}_{\mathcal{I}/\mathcal{J}})=\mathcal{J}(T_{\mathrm{III}})\).

But on the other hand, we have:

\[\mathcal{J}(\tau^{u}_{\mathcal{I}/\mathcal{J}})=\mathcal{J}(T_{ \mathrm{III}})+\int_{T_{\mathrm{III}}}^{\tau^{u}_{\mathcal{I}/\mathcal{J}}}( \mathcal{I}(t)\mathcal{J}(t)\cos\Delta-\mathcal{J}^{2}(t))\mathrm{d}t\] \[= \mathcal{J}(T_{\mathrm{III}})+\int_{T_{\mathrm{III}}}^{\tau^{u}_{ \mathcal{I}/\mathcal{J}}}\mathcal{J}(t)(\mathcal{I}(t)\cos\Delta-\mathcal{J}(t ))\mathrm{d}t\] \[< \mathcal{J}(T_{\mathrm{III}})+(\cos\Delta-1)\int_{T_{\mathrm{III}} }^{\tau^{u}_{\mathcal{I}/\mathcal{J}}}\mathcal{J}(t)\mathcal{I}(t)\mathrm{d}t <\mathcal{J}(T_{\mathrm{III}}),\]

which leads to a contradiction. Hence, we have proved

\[\tau^{u}_{\mathcal{I}/\mathcal{J}}=+\infty;\] \[\mathcal{I}(t)-\mathcal{J}(t)\cos\Delta<0,\quad\forall t\in[T_{ \mathrm{III}},+\infty).\]

Moreover, we obtain a sharper two-side bound for \(\mathcal{I}(t)/\mathcal{J}(t)\):

\[\frac{1+\cos\Delta}{1+\cos\Delta+\epsilon}<\frac{\mathcal{I}(t)}{\mathcal{J}(t )}<\cos\Delta,\quad\forall t\in[T_{\mathrm{III}},+\infty).\] (33)

Step V. Tight bound for \(\mathcal{I}(t)\) and \(\mathcal{J}(t)\).

In this step, we aim to give a tight bound for \(\mathcal{I}(t)+\mathcal{J}(t)\). With the help of the two-side bound (33), we have

\[\frac{\mathrm{d}}{\mathrm{d}t}\Big{(}\mathcal{I}(t)+\mathcal{J}(t )\Big{)}=2\mathcal{I}(t)\mathcal{J}(t)\cos\Delta-\mathcal{I}^{2}(t)\left(1+ \epsilon\right)-\mathcal{J}^{2}(t)\] \[= -\left(\mathcal{I}(t)-\mathcal{J}(t)\right)^{2}\cos\Delta-(1- \cos\Delta)\mathcal{J}^{2}(t)-\mathcal{I}^{2}(t)\left(1+\epsilon-\cos\Delta\right)\] \[< -\left(1-\cos\Delta\right)\left(\mathcal{J}^{2}(t)+\mathcal{I}^{2 }(t)\right)<-\frac{\left(1-\cos\Delta\right)\left(\mathcal{I}(t)+\mathcal{J}(t )\right)^{2}}{2}<-\frac{\Delta^{2}}{6}\left(\mathcal{I}(t)+\mathcal{J}(t) \right)^{2},\]

\[\frac{\mathrm{d}}{\mathrm{d}t}\Big{(}\mathcal{I}(t)+\mathcal{J}(t )\Big{)}=2\mathcal{I}(t)\mathcal{J}(t)\cos\Delta-\mathcal{I}^{2}(t)\left(1+ \epsilon\right)-\mathcal{J}^{2}(t)\] \[= -\left(\mathcal{I}(t)-\mathcal{J}(t)\right)^{2}\cos\Delta-(1- \cos\Delta)\mathcal{J}^{2}(t)-\mathcal{I}^{2}(t)\left(1+\epsilon-\cos\Delta\right)\] \[> -\left(1-\frac{1+\cos\Delta}{1+\cos\Delta+\epsilon}\right)^{2} \mathcal{J}^{2}(t)-(1-\cos\Delta)\mathcal{J}^{2}(t)-\mathcal{I}^{2}(t)\left( 1+\epsilon-\cos\Delta\right)\] \[> -\left(1+\epsilon-\cos\Delta\right)\left(\mathcal{I}^{2}(t)+ \mathcal{J}^{2}(t)\right)>-\left(\frac{2}{3}+\frac{m_{+}}{m_{-}}\right)\Delta^ {2}\left(\mathcal{I}(t)+\mathcal{J}(t)\right)^{2}>-2\Delta^{2}\left(\mathcal{ I}(t)+\mathcal{J}(t)\right)^{2}.\]

For the first inequality, we consider the auxiliary ODE: \(\frac{d}{\mathrm{d}t}\mathcal{P}(t)=-\frac{\Delta^{2}}{6}\mathcal{P}^{2}(t)\), where \(\mathcal{P}(T_{\mathrm{III}})=\mathcal{I}(T_{\mathrm{III}})+\mathcal{J}(T_{ \mathrm{III}})>0\). From the Comparison Principle of ODEs, we have:

\[\mathcal{I}(t)+\mathcal{J}(t)\leq\mathcal{P}(t)=\frac{1}{\frac{1}{\mathcal{I}(T _{\mathrm{III}})+\mathcal{J}(T_{\mathrm{III}})}+\frac{\Delta^{2}}{6}(t-T_{ \mathrm{III}})},\quad\forall t\geq T_{\mathrm{III}}.\]

In the same way, we can obtain the lower bound:

\[\mathcal{I}(t)+\mathcal{J}(t)\geq\frac{1}{\frac{1}{\mathcal{I}(T_{\mathrm{III}})+ \mathcal{J}(T_{\mathrm{III}})}+2\Delta^{2}(t-T_{\mathrm{III}})},\quad\forall t \geq T_{\mathrm{III}}.\]Recalling Step I, we have \(\frac{1}{\mathcal{I}(T_{\rm III})+\mathcal{J}(T_{\rm III})}=\Theta\left(p^{\frac{ 1}{1-\alpha\cos\Delta}}/\kappa_{2}^{2}\right)\). Hence, we obtain the tight bound:

\[\mathcal{I}(t)+\mathcal{J}(t)=\Theta\left(\frac{1}{\frac{p^{\frac{1-\alpha \cos\Delta}{\kappa_{2}^{2}}}}{\kappa_{2}^{2}}+\Delta^{2}(t-T_{\rm III})} \right),\quad\forall t\in[T_{\rm III},+\infty).\]

Taking (33) into the equation above, we have:

\[\mathcal{I}(t)= \Theta\left(\frac{1}{\frac{p^{\frac{1-\alpha\cos\Delta}{\kappa_{ 2}^{2}}}}{\kappa_{2}^{2}}+\Delta^{2}(t-T_{\rm III})}\right),\quad\forall t\in[T _{\rm III},+\infty);\] \[\mathcal{J}(t)= \Theta\left(\frac{1}{\frac{p^{\frac{1-\alpha\cos\Delta}{\kappa_{ 2}^{2}}}}{\kappa_{2}^{2}}+\Delta^{2}(t-T_{\rm III})}\right),\quad\forall t\in[ T_{\rm III},+\infty).\]

Step VI. The limit of \(\mathcal{I}(t)/\mathcal{J}(t)\).

By Step V and the proof of Step IV, we know \(\lim_{t\to\infty}\mathcal{J}(t)=0\) and \(\frac{\rm d}{\rm d}t\mathcal{J}(t)<0\) holds for any \(t>T_{\rm III}\). Then for any \(\epsilon^{\prime}>0\), there exists \(T^{\prime}>T_{\rm III}\) such that

\[\log\left(\frac{\mathcal{J}(t)}{\mathcal{J}(T_{\rm III})}\right)<\frac{1000}{ \Delta^{2}}\log(1000\epsilon\Delta^{2}),\quad\forall t>T^{\prime}.\]

Taking it into (32), we obtain that for any \(t>T^{\prime}\),

\[0<(1+\cos\Delta+\epsilon)\mathcal{Z}(t)-(1+\cos\Delta)<\epsilon^{\prime}.\]

By the definition of the limit, we get

\[\lim_{t\to\infty}\frac{\mathcal{I}(t)}{\mathcal{J}(t)}=\frac{1+\cos\Delta}{1+ \cos\Delta+\epsilon}=\frac{1+\cos\Delta}{1+\cos\Delta+\frac{m_{+}}{m_{-}}\sin ^{2}\Delta}.\]

**Lemma F.5** (Time and prediction estimate).: **(S1).** _For any \(t\in(T_{\rm III},+\infty)\)_

\[pe^{-f_{+}(t)}=\Theta\left(\frac{1}{p^{\frac{1}{1-\alpha\cos\Delta}}+\kappa_{ 2}^{2}\Delta^{2}(t-T_{\rm III})}\right),\;e^{f_{-}(t)}=\Theta\left(\frac{1}{p^ {\frac{1}{1-\alpha\cos\Delta}}+\kappa_{2}^{2}\Delta^{2}(t-T_{\rm III})} \right);\]

\[\mathcal{L}(\bm{\theta}(t))=\Theta\left(\frac{1}{p^{\frac{1}{1-\alpha\cos \Delta}}+\kappa_{2}^{2}\Delta^{2}(t-T_{\rm III})}\right).\]

**(S2).** _For any \(t\in(T_{\rm III},+\infty)\),_

\[\frac{1+\cos\Delta}{1+\cos\Delta+\frac{m_{+}}{m_{-}}\sin^{2}\Delta}<pe^{-(f_{ +}(t)+f_{-}(t))}<\cos\Delta.\]

_Moreover, \(pe^{-(f_{+}(T_{\rm III})+f_{-}(T_{\rm III}))}=\cos\Delta\) and \(\lim_{t\to\infty}pe^{-(f_{+}(t)+f_{-}(t))}=\frac{1+\cos\Delta}{1+\cos\Delta+ \frac{m_{+}}{m_{-}}\sin^{2}\Delta}\)._

**(S3).** _For any \(t\in(T_{\rm III},+\infty)\),_

\[\langle\bm{b}_{k}(t),\bm{x}_{+}\rangle>0,\;\langle\bm{b}_{k}(t),\bm {x}_{-}\rangle=0,\;k\in\mathcal{K}_{+};\] \[\langle\bm{b}_{k}(t),\bm{x}_{+}\rangle>0,\;\langle\bm{b}_{k}(t), \bm{x}_{-}\rangle>0,\;k\in\mathcal{K}_{-}.\]

**(S4) (Time).**__

\[T_{\rm IV}=T_{\rm IV}^{*}=+\infty.\]Proof of Lemma F.5.:

Notice the relationships: \(pe^{-f_{+}(t)}=\kappa_{2}^{2}\frac{m_{-}}{m}\frac{\mathcal{I}(t)}{1+p}\), \(e^{f_{-}(t)}=\kappa_{2}^{2}\frac{m_{-}}{m}\frac{\mathcal{J}(t)}{1+p}\) and \(pe^{-(f_{+}(t)+f_{-}(t))}=\mathcal{I}(t)/\mathcal{J}(t)\). Then Lemma F.4 implies that \(T_{\mathrm{IV}}^{*}=+\infty\). Recalling the dynamics in Lemma F.1, then lemma (S3)(S4) hold. Then using Lemma F.4 again, we obtain (S1)(S2). 

Proof of Theorem 4.9.:

Theorem 4.9 (S1), (S2), and (S3) are obtained in Lemma F.5 (S4), (S3), and (S1), respectively. 

### Asymptotic Directional Convergence

In this section, we will study the final convergence direction in our setting. It mainly depends on our prior fine-grained analysis of the training dynamics in Phase IV and the following result about the final convergence direction at the end of training.

**Lemma F.6**.: _Let \(f(\cdot;\bm{\theta})\) be a homogeneous neural network parameterized by \(\bm{\theta}\). Consider minimizing the exponential loss over a binary classification dataset \(\{(\bm{x}_{i},y_{i})\}_{i=1}^{n}\) (\(\left\lVert\bm{x}_{i}\right\rVert_{2}\leq 1,y_{i}\in\{\pm 1\}\)) using Gradient Flow. Assume that there exists time \(t_{0}\) such that \(\mathcal{L}(\bm{\theta}(t_{0}))<\frac{1}{n}\). Then,_

_(I) (Paraphrased from (Lyu and Li, 2019; Ji and Telgarsky, 2020))._\(\bm{\theta}(t)\) _converges in direction to a KKT point (Definition G.3) of the following maximum margin problem:_

\[\min: \frac{1}{2}\left\lVert\bm{\theta}\right\rVert_{2}^{2}\] \[\mathrm{s.t.}\ y_{i}f(\bm{x}_{i};\bm{\theta})\geq 1.\]

_(II) (Lyu and Li, 2019; Ji and Telgarsky, 2020))._\(\left\lVert\bm{\theta}(t)\right\rVert_{2}\to\infty\) _and_ \(\mathcal{L}(\bm{\theta}(t))\to 0\)_._

_(III) (Ji and Telgarsky (2020))._\(-\nabla\mathcal{L}(\bm{\theta}(t))\) _and_ \(\bm{\theta}(t)\) _converge to the same direction, meaning the angle between_ \(\bm{\theta}(t)\) _and_ \(-\nabla\mathcal{L}(\bm{\theta}(t))\) _converges to_ \(0\)_._

**Lemma F.7** (Final Convergence Direction).: _The limit \(\lim\limits_{t\to+\infty}\frac{\bm{\theta}(t)}{\left\lVert\bm{\theta}(t) \right\rVert_{2}}\) exists, and denoted by \(\overline{\bm{\theta}}=(\overline{b}_{1}^{\top},\cdots,\overline{b}_{m}^{\top })^{\top}\in\mathbb{S}^{md-1}\), then it satisfies_

\[\overline{\bm{b}}_{k}= \bm{0},\quad\forall k\notin\mathcal{K}_{+}\cup\mathcal{K}_{-};\] \[\overline{\bm{b}}_{k}= C\left(\bm{x}_{+}-\bm{x}_{-}\cos\Delta\right),\quad \forall k\in\mathcal{K}_{+};\] \[\overline{\bm{b}}_{k}= C\left(\left(1+\frac{m_{+}\sin^{2}\Delta}{m_{-}(1+\cos \Delta)}\right)\bm{x}_{-}-\bm{x}_{+}\right),\quad\forall k\in\mathcal{K}_{-};\]

_where \(C>0\) is a scaling constant such that \(\left\lVert\overline{\bm{\theta}}\right\rVert_{2}=1\). Moreover, \(f(\bm{x}_{+};\overline{\bm{\theta}})=-f(\bm{x}_{-};\overline{\bm{\theta}})>0\)._

Proof of Lemma F.7.:

Let \(\overline{\bm{\theta}}=(\overline{b}_{1}^{\top},\cdots,\overline{b}_{m}^{ \top})^{\top}\in\mathbb{S}^{md-1}\) be the limits point of \(\left\{\frac{\bm{\theta}(t)}{\left\lvert\overline{\bm{\theta}}(t)\right\rvert _{2}}:t\geq t_{0}\right\}\). From Lemma F.6 (I), we know that there exists a scaling factor \(\alpha>0\) such that \(\alpha\overline{\bm{\theta}}\) satisfies KKT conditions (Definition G.3) of the maximum-margin problem

\[\min: \frac{1}{2}\left\lVert\bm{\theta}\right\rVert_{2}^{2}\] (34) \[\mathrm{s.t.}\ f(\bm{x}_{+};\bm{\theta})\geq 1,\ f(\bm{x}_{-};\bm{ \theta})\leq-1.\]

For simplicity, we denote \(\bm{\theta}^{*}:=\alpha\bar{\bm{\theta}}\), where

\[\bm{\theta}^{*}=({b_{1}^{*}}^{\top},\cdots,{b_{m}^{*}}^{\top})^{\top}.\]

Moreover, let \(\lambda_{+}^{*},\lambda_{-}^{*}\geq 0\) be the corresponding Lagrange multipliers (with respect to \(\bm{\theta}^{*}\)) in Definition G.3.

Step I. The rough direction of each neuron.

Recalling the training dynamics about the dead neurons in Theorem 4.1 (S3),

\[\bm{b}_{k}(t)\equiv \bm{b}_{k}(T_{\mathrm{I}}),\ \langle\bm{b}_{k}(t),\bm{x}_{+}\rangle\leq 0,\ \langle\bm{b}_{k}(t),\bm{x}_{-}\rangle\leq 0,\ k\in[m/2]-\mathcal{K}_{+};\]\[\bm{b}_{k}(t)\equiv \bm{b}_{k}(T_{1}),\ \langle\bm{b}_{k}(t),\bm{x}_{+}\rangle\leq 0,\ \langle\bm{b}_{k}(t),\bm{x}_{-}\rangle\leq 0,\ k\in[m]-[m/2]-\mathcal{K}_{-}.\]

Noticing Lemma F.6 (II) or Lemma F.5 (S1), \(\left\|\bm{\theta}(t)\right\|_{2}\to\infty\), so

\[\bm{b}_{k}^{*}=\bm{0},\quad\forall k\notin\mathcal{K}_{+}\cup\mathcal{K}_{-}.\]

Then we only need to focus on \(\bm{\theta}_{k}^{*}\) for \(k\in\mathcal{K}_{+}\cup\mathcal{K}_{-}\).

Recalling in Lemma F.5 (S3), for any \(t>T_{\mathrm{IV}}\),

\[\langle\bm{b}_{k}(t),\bm{x}_{+}\rangle>0,\ \langle\bm{b}_{k}(t),\bm{x}_ {-}\rangle=0,\ k\in\mathcal{K}_{+};\] \[\langle\bm{b}_{k}(t),\bm{x}_{+}\rangle>0,\ \langle\bm{b}_{k}(t),\bm{x}_ {-}\rangle>0,\ k\in\mathcal{K}_{-}.\]

then we have

\[\langle\bm{b}_{k}^{*},\bm{x}_{+}\rangle\geq 0,\ \langle\bm{b}_{k}^{*}, \bm{x}_{-}\rangle=0,\quad k\in\mathcal{K}_{+};\] \[\langle\bm{b}_{k}^{*},\bm{x}_{+}\rangle\geq 0,\ \langle\bm{b}_{k}^{*}, \bm{x}_{-}\rangle\geq 0,\quad k\in\mathcal{K}_{-}.\]

Moreover,

\[f(\bm{x}_{+};\bm{\theta}^{*})= \frac{\kappa_{2}}{\sqrt{m}}\Big{(}\sum_{k\in\mathcal{K}_{+}} \sigma\left(\langle\bm{b}_{k}^{*},\bm{x}_{+}\rangle\right)-\sum_{k\in \mathcal{K}_{-}}\sigma\left(\langle\bm{b}_{k}^{*},\bm{x}_{+}\rangle\right) \Big{)},\] \[f(\bm{x}_{-};\bm{\theta}^{*})= \frac{\kappa_{2}}{\sqrt{m}}\Big{(}\sum_{k\in\mathcal{K}_{+}} \sigma\left(\langle\bm{b}_{k}^{*},\bm{x}_{-}\rangle\right)-\sum_{k\in \mathcal{K}_{-}}\sigma\left(\langle\bm{b}_{k}^{*},\bm{x}_{-}\rangle\right) \Big{)}=-\frac{\kappa_{2}}{\sqrt{m}}\sum_{k\in\mathcal{K}_{-}}\sigma\left( \langle\bm{b}_{k}^{*},\bm{x}_{-}\rangle\right).\]

Step II. Determine the direction of the neurons \(k\in\mathcal{K}_{+}\).

Since \(\bm{\theta}^{*}\) is a possible point, \(f(\bm{x}_{+};\bm{\theta}^{*})\geq 1\), which gives us

\[\sum_{k\in\mathcal{K}_{+}}\sigma(\langle\bm{b}_{k}^{*},\bm{x}_{+}\rangle)\geq \frac{\sqrt{m}}{\kappa_{2}}>0.\]

Hence, there exists \(k_{1}\in\mathcal{K}_{+}\), s.t. \(\langle\bm{b}_{k_{1}}^{*},\bm{x}_{+}\rangle>0\) strictly.

Then we study the neuron \(k_{2}\in\mathcal{K}_{+}\) (\(k_{2}\neq k_{1}\)). Lemma F.1 and Lemma F.5 (S3) give use that

\[\langle\bm{b}_{k_{1}}(t),\bm{x}_{+}\rangle= \langle\bm{b}_{k_{1}}(T_{\mathrm{IV}}),\bm{x}_{+}\rangle+\int_{T_ {\mathrm{IV}}}^{t}\frac{\kappa_{2}pe^{-f_{+}(t)}}{\sqrt{m}(1+p)}\sin^{2} \Delta\mathrm{d}t\] \[= \langle\bm{b}_{k_{2}}(T_{\mathrm{IV}}),\bm{x}_{+}\rangle+\int_{T_ {\mathrm{IV}}}^{t}\frac{\kappa_{2}pe^{-f_{+}(t)}}{\sqrt{m}(1+p)}\sin^{2} \Delta\mathrm{d}t+\Big{(}\,\langle\bm{b}_{k_{1}}(T_{\mathrm{IV}}),\bm{x}_{+} \rangle-\langle\bm{b}_{k_{2}}(T_{\mathrm{IV}}),\bm{x}_{+}\rangle\,\Big{)}\] \[= \langle\bm{b}_{k_{2}}(t),\bm{x}_{+}\rangle+\Big{(}\,\langle\bm{b} _{k_{1}}(T_{\mathrm{IV}}),\bm{x}_{+}\rangle-\langle\bm{b}_{k_{2}}(T_{\mathrm{ IV}}),\bm{x}_{+}\rangle\,\Big{)}.\]

Multiplying the above formula by \(c/\left\|\bm{\theta}(t)\right\|_{2}\) and taking \(t\) go to infinity, we obtain

\[\big{\langle}\bm{b}_{k_{1}}^{*},\bm{x}_{+}\big{\rangle}=\big{\langle}\bm{b}_{k _{2}}^{*},\bm{x}_{+}\big{\rangle}>0.\]

Due to the arbitrariness of \(k_{2}\), we know

\[\bm{b}_{k}^{*}\neq 0,\quad\langle\bm{b}_{k}^{*},\bm{x}_{+}\rangle>0,\quad\langle \bm{b}_{k}^{*},\bm{x}_{-}\rangle=0,\quad\forall k\in\mathcal{K}_{+}.\]

Then we can write the KKT condition about the gradient of \(\bm{b}_{k}^{*}\) (\(k\in\mathcal{K}_{+}\)) of Problem (34):

\[\bm{0}\in\bm{b}_{k}^{*}-\lambda_{+}^{*}\frac{\kappa_{2}}{\sqrt{m}}\bm{x}_{+}+ \lambda_{-}^{*}\frac{\kappa_{2}}{\sqrt{m}}\partial^{\circ}\sigma(0)\bm{x}_{-}.\]

It is clear that \(\bm{b}_{k}^{*}\in\mathrm{span}\{\bm{x}_{+},\bm{x}_{-}\}\). Then combining two formulations above, we obtain:

\[\bm{b}_{k}^{*}=\lambda_{+}^{*}\frac{\kappa_{2}}{\sqrt{m}}\left(\bm{x}_{+}-\bm{x} _{-}\cos\Delta\right),\quad\forall k\in\mathcal{K}_{+}.\]

Step III. Determine the direction of the neurons \(k\in\mathcal{K}_{-}\).

Since \(\bm{\theta}^{*}\) is a possible point, \(f(\bm{x}_{-};\bm{\theta}^{*})\leq-1\), which gives us

\[\sum_{k\in\mathcal{K}_{-}}\sigma(\left\langle\bm{b}_{k}^{*},\bm{x}_{-}\right\rangle )\geq\frac{\sqrt{m}}{\kappa_{2}}>0.\]

Hence, there exists \(k_{1}\in\mathcal{K}_{-}\), s.t. \(\left\langle\bm{b}_{k_{1}}^{*},\bm{x}_{-}\right\rangle>0\) strictly.

Then we study the neuron \(k_{2}\in\mathcal{K}_{-}\) (\(k_{2}\neq k_{1}\)). Lemma F.1 and Lemma F.5 (S3) give use that

\[\left\langle\bm{b}_{k_{1}}(t),\bm{x}_{-}\right\rangle=\left\langle \bm{b}_{k_{1}}(T_{\mathrm{IV}}),\bm{x}_{+}\right\rangle+\int_{T_{\mathrm{IV}}}^ {t}\frac{\kappa_{2}}{\sqrt{m}}\left(\frac{1}{1+p}e^{f_{-}(t)}-\frac{p}{1+p}e^ {-f_{+}(t)}\cos\Delta\right)\mathrm{d}t\] \[= \left\langle\bm{b}_{k_{2}}(T_{\mathrm{IV}}),\bm{x}_{-}\right\rangle +\int_{T_{\mathrm{IV}}}^{t}\frac{\kappa_{2}}{\sqrt{m}}\left(\frac{1}{1+p}e^{f _{-}(t)}-\frac{p}{1+p}e^{-f_{+}(t)}\cos\Delta\right)\mathrm{d}t+\Big{(}\left \langle\bm{b}_{k_{1}}(T_{\mathrm{IV}}),\bm{x}_{-}\right\rangle-\left\langle\bm {b}_{k_{2}}(T_{\mathrm{IV}}),\bm{x}_{-}\right\rangle\Big{)}\] \[= \left\langle\bm{b}_{k_{2}}(t),\bm{x}_{-}\right\rangle+\Big{(} \left\langle\bm{b}_{k_{1}}(T_{\mathrm{IV}}),\bm{x}_{-}\right\rangle-\left\langle \bm{b}_{k_{2}}(T_{\mathrm{IV}}),\bm{x}_{-}\right\rangle\Big{)}.\]

Multiplying the above formula by \(c/\left\|\bm{\theta}(t)\right\|_{2}\) and taking \(t\) go to infinity, we obtain

\[\left\langle\bm{b}_{k_{1}}^{*},\bm{x}_{-}\right\rangle=\left\langle\bm{b}_{k_ {2}}^{*},\bm{x}_{-}\right\rangle>0.\]

Due to the arbitrariness of \(k_{2}\), we know

\[\bm{b}_{k}^{*}\neq 0,\quad\left\langle\bm{b}_{k}^{*},\bm{x}_{-}\right\rangle>0, \quad\left\langle\bm{b}_{k}^{*},\bm{x}_{+}\right\rangle\geq 0,\quad\forall k \in\mathcal{K}_{-}.\]

The next difficulty in this step is to determine whether \(\left\langle\bm{b}_{k}^{*},\bm{x}_{+}\right\rangle\) can be \(0\). To prove this, we will use our fine-grained analysis of training dynamics (Lemma F.5) and Lemma F.6 (III).

Let \(k\in\mathcal{K}_{-}\). Recalling the dynamics of \(\bm{b}_{k}(t)\) in Lemma F.1, we know

\[-\frac{\partial\mathcal{L}(\bm{\theta}(t))}{\partial\bm{b}_{k}}= -\frac{\kappa_{2}}{\sqrt{m}}\left(\frac{p}{1+p}e^{-f_{+}(t)}\bm{x }_{+}-\frac{1}{1+p}e^{f_{-}(t)}\bm{x}_{-}\right)\] \[= \frac{\kappa_{2}}{\sqrt{m}}\frac{e^{f_{-}(t)}}{1+p}\left(\bm{x}_ {-}-pe^{-(f_{+}(t)+f_{-}(t))}\bm{x}_{+}\right).\]

Recalling Lemma F.5 (S2), \(\lim_{t\to\infty}pe^{-(f_{+}(t)+f_{-}(t))}=\frac{1+\cos\Delta}{1+\cos\Delta+ \frac{m_{+}}{m_{-}}\sin^{2}\Delta}\). Then using Lemma F.6 (III), there exists \(c_{1}>0\), s.t.

\[\bm{b}_{k}^{*}=c_{1}\left(\bm{x}_{-}-\frac{1+\cos\Delta}{1+\cos\Delta+\frac{m _{+}}{m_{-}}\sin^{2}\Delta}\bm{x}_{+}\right).\]

Hence, we have proved

\[\left\langle\bm{b}_{k}^{*},\bm{x}_{+}\right\rangle>0,\quad\forall k\in\mathcal{ K}_{-}.\]

Then writing the KKT condition about the gradient of \(\bm{b}_{k}^{*}\) of Problem (34):

\[\bm{0}=\bm{b}_{k}^{*}+\lambda_{+}^{*}\frac{\kappa_{2}}{\sqrt{m}}\bm{x}_{+}- \lambda_{-}^{*}\frac{\kappa_{2}}{\sqrt{m}}\bm{x}_{-},\quad\forall k\in\mathcal{ K}_{-}.\]

Combining the two equations about \(\bm{b}_{k}^{*}\), we obatin

\[\bm{b}_{k}^{*}=\lambda_{-}^{*}\frac{\kappa_{2}}{\sqrt{m}}\left( \bm{x}_{-}-\frac{1+\cos\Delta}{1+\cos\Delta+\frac{m_{+}}{m_{-}}\sin^{2}\Delta} \bm{x}_{+}\right),\quad\forall k\in\mathcal{K}_{-};\] \[\frac{\lambda_{+}^{*}}{\lambda_{-}^{*}}=\frac{1+\cos\Delta}{1+\cos \Delta+\frac{m_{+}}{m_{-}}\sin^{2}\Delta}.\]

In summary, we have proved the final convergence direction \(\overline{\bm{\theta}}=(\bm{b}_{1}^{\top},\cdots,\bm{b}_{m}^{\top})^{\top}\in \mathbb{S}^{md-1}\) satisfies

\[\overline{\bm{b}}_{k}= \bm{0},\quad\forall k\notin\mathcal{K}_{+}\cup\mathcal{K}_{-};\] \[\overline{\bm{b}}_{k}= C\Big{(}\bm{x}_{+}-\bm{x}_{-}\cos\Delta\Big{)},\quad\forall k\in \mathcal{K}_{+};\]\[\overline{\bm{b}}_{k}= C\left(\left(1+\frac{m_{+}\sin^{2}\Delta}{m_{-}(1+\cos\Delta)} \right)\bm{x}_{-}-\bm{x}_{+}\right),\quad\forall k\in\mathcal{K}_{-};\]

where \(C>0\) is a scaling constant such that \(\left\|\overline{\bm{\theta}}\right\|_{2}=1\).

Moreover, a straight-forward calculation gives us that

\[f(\bm{x}_{+};\overline{\bm{\theta}})=-f(\bm{x}_{-};\overline{\bm{\theta}})>0.\]

Proof of Theorem 4.10.: Lemma F.7 implies Theorem 4.10 directly. 

Proof of Theorem 4.11.: From Lemma F.7, the final convergence direction \(\overline{\bm{\theta}}=(\bar{\bm{b}}_{1}^{\top},\cdots,\bar{\bm{b}}_{m}^{\top })^{\top}\in\mathbb{S}^{md-1}\) satisfies

\[\overline{\bm{b}}_{k}= \bm{0},\quad\forall k\notin\mathcal{K}_{+}\cup\mathcal{K}_{-};\] \[\overline{\bm{b}}_{k}= C\Big{(}\bm{x}_{+}-\bm{x}_{-}\cos\Delta\Big{)},\quad \forall k\in\mathcal{K}_{+};\] \[\overline{\bm{b}}_{k}= C\left(\left(1+\frac{m_{+}\sin^{2}\Delta}{m_{-}(1+\cos \Delta)}\right)\bm{x}_{-}-\bm{x}_{+}\right),\quad\forall k\in\mathcal{K}_{-};\]

where \(C>0\) is a scaling constant such that \(\left\|\overline{\bm{\theta}}\right\|_{2}=1\) and \(f_{+}(\overline{\bm{\theta}})=-f_{-}(\overline{\bm{\theta}})>0\).

It is easy to verify that There exists a scaling factor \(C_{1}>0\) such that \(f_{+}(\hat{\bm{\theta}})=-f_{-}(\hat{\bm{\theta}})=1\), where \(\hat{\bm{\theta}}=C_{1}\overline{\bm{\theta}}\). For simplicity, we denote \(Q:=CC_{1}\), then \(\hat{\bm{\theta}}=(\hat{\bm{b}}_{1}^{\top},\cdots,\hat{\bm{b}}_{m}^{\top})^{\top} \in\mathbb{R}^{md}\) satisfies

\[\hat{\bm{b}}_{k}= \bm{0},\quad\forall k\notin\mathcal{K}_{+}\cup\mathcal{K}_{-};\] \[\hat{\bm{b}}_{k}= Q\Big{(}\bm{x}_{+}-\bm{x}_{-}\cos\Delta\Big{)},\quad \forall k\in\mathcal{K}_{+};\] \[\hat{\bm{b}}_{k}= Q\left(\left(1+\frac{m_{+}\sin^{2}\Delta}{m_{-}(1+\cos \Delta)}\right)\bm{x}_{-}-\bm{x}_{+}\right),\quad\forall k\in\mathcal{K}_{-};\] \[f_{+}(\hat{\bm{\theta}})= -f_{-}(\hat{\bm{\theta}})=1.\]

Therefore, \(\hat{\bm{\theta}}\) is a feasible point of Problem (34). Moreover, from

\[-1=-\sum_{k\in\mathcal{K}_{-}}\frac{\kappa_{2}}{\sqrt{m}}\left\langle\hat{\bm {b}}_{k},\bm{x}_{-}\right\rangle\] \[= -\frac{\kappa_{2}}{\sqrt{m}}m_{-}\left\langle Q\left(\left(1+ \frac{m_{+}\sin^{2}\Delta}{m_{-}(1+\cos\Delta)}\right)\bm{x}_{-}-\bm{x}_{+} \right),\bm{x}_{-}\right\rangle\] \[= -Q\frac{\kappa_{2}}{\sqrt{m}}m_{-}\left(1-\cos\Delta+\frac{m_{+} \sin^{2}\Delta}{m_{-}(1+\cos\Delta)}\right)\] \[= -Q\frac{\kappa_{2}}{\sqrt{m}}\left(m_{-}(1-\cos\Delta)+m_{+} \frac{\sin^{2}\Delta}{1+\cos\Delta}\right),\]

we have

\[\frac{\kappa_{2}}{\sqrt{m}}Q=\frac{1}{m_{-}(1-\cos\Delta)+m_{+} \frac{\sin^{2}\Delta}{1+\cos\Delta}}.\]

For any \(\epsilon\geq 0\), now we consider another solution \(\hat{\bm{\theta}}(\epsilon)\) near \(\hat{\bm{\theta}}\): \(\hat{\bm{\theta}}(\epsilon)=(\hat{\bm{b}}_{1}^{\top}(\epsilon),\cdots,\hat{\bm {b}}_{m}^{\top}(\epsilon))^{\top}\), where

\[\hat{\bm{b}}_{k}(\epsilon)= \bm{0},\quad\forall k\notin\mathcal{K}_{+}\cup\mathcal{K}_{-};\] \[\hat{\bm{b}}_{k}(\epsilon)= Q\Big{(}\bm{x}_{+}-\bm{x}_{-}\cos\Delta\Big{)}-Q\epsilon \Big{(}\bm{x}_{+}-\bm{x}_{-}\cos\Delta\Big{)},\quad\forall k\in\mathcal{K}_{+};\]\[\hat{\bm{b}}_{k}(\epsilon)= Q\left(\left(1+\frac{m_{+}\sin^{2}\Delta}{m_{-}(1+\cos\Delta)} \right)\bm{x}_{-}-\bm{x}_{+}\right)+Q\epsilon\Big{(}\bm{x}_{+}-\bm{x}_{-}\cos \Delta\Big{)},\quad\forall k\in\mathcal{K}_{-}.\]

and it holds that \(\hat{\bm{\theta}}(0)=\hat{\bm{\theta}}\). Moreover, it is easy to verify that \(\hat{\bm{\theta}}(\epsilon)\) is also a feasible point of Problem (34).

\[f_{-}(\hat{\bm{\theta}}(\epsilon))= f_{-}(\hat{\bm{\theta}}(0))=-1;\] \[f_{+}(\hat{\bm{\theta}}(\epsilon))= f_{+}(\hat{\bm{\theta}}(0))=1.\]

Then we compare the norm of \(\hat{\bm{\theta}}(\epsilon)\) and \(\hat{\bm{\theta}}(0)\).

\[\left\|\hat{\bm{\theta}}(\epsilon)\right\|^{2}= m_{+}\left(Q\Big{(}\bm{x}_{+}-\bm{x}_{-}\cos\Delta\Big{)}-Q\epsilon \Big{(}\bm{x}_{+}-\bm{x}_{-}\cos\Delta\Big{)}\right)^{2}\] \[+m_{-}\left(Q\left(\left(1+\frac{m_{+}\sin^{2}\Delta}{m_{-}(1+ \cos\Delta)}\right)\bm{x}_{-}-\bm{x}_{+}\right)+Q\epsilon\Big{(}\bm{x}_{+}- \bm{x}_{-}\cos\Delta\Big{)}\right)^{2},\]

At \(\epsilon=0\), we can calculate that

\[\left.\frac{\mathrm{d}\left\|\hat{\bm{\theta}}(\epsilon)\right\| ^{2}_{\Big{\|}}}{2m_{+}Q^{2}\mathrm{d}\epsilon}\right|_{\epsilon=0}\] \[= \frac{1}{m_{+}}\left(-m_{+}\left\|\bm{x}_{+}-\bm{x}_{-}\cos \Delta\right\|^{2}+m_{-}\left\langle\left(1+\frac{m_{+}\sin^{2}\Delta}{m_{-}( 1+\cos\Delta)}\right)\bm{x}_{-}-\bm{x}_{+},\bm{x}_{+}-\bm{x}_{-}\cos\Delta \right\rangle\right)\] \[= \left\langle\left(\frac{m_{-}}{m_{+}}+\frac{\sin^{2}\Delta}{1+ \cos\Delta}+\cos\Delta\right)\bm{x}_{-}-(1+\frac{m_{-}}{m_{+}})\bm{x}_{+},\bm{ x}_{+}-\bm{x}_{-}\cos\Delta\right\rangle\] \[= -\left(\alpha+\frac{\sin^{2}\Delta}{1+\cos\Delta}+\cos\Delta \right)\cos\Delta-(1+\alpha)\] \[+\left(\alpha+\frac{\sin^{2}\Delta}{1+\cos\Delta}+\cos\Delta \right)\cos\Delta+(1+\alpha)\cos^{2}\Delta\] \[= -(1+\alpha)\sin^{2}\Delta<0\]

Then combining the continuity of \(\frac{\mathrm{d}\left\|\hat{\bm{\theta}}(\epsilon)\right\|^{2}}{\mathrm{d} \epsilon}\), there exists \(\delta>0\) such that the following inequality holds:

\[\left\|\hat{\bm{\theta}}(\epsilon)\right\|^{2}<\left\|\hat{\bm{\theta}}(0) \right\|^{2},\;\forall\epsilon\in(0,\delta).\]

Hence, we have proved that \(\overline{\bm{\theta}}\) is not a local optimal direction of the max-margin problem (34).

Clarke Subdifferential and KKT Conditions for Non-smooth Optimization

**Definition G.1** (Clarke's Subdifferential (Clarke et al., 2008)).: For a locally Lipschitz function \(\mathcal{L}:\Omega\to\mathbb{R}\), the Clarke's subdifferential at \(\bm{\theta}\in\Omega\) is the convex set

\[\partial^{\circ}\mathcal{L}(\bm{\theta}):=\mathrm{conv}\left\{\lim_{i\to \infty}\nabla\mathcal{L}(\bm{\theta}_{i}):\lim_{i\to\infty}\bm{\theta}_{i}= \bm{\theta},\mathcal{L}\text{ is differential at }\bm{\theta}_{i}\right\}.\]

**Remark G.2**.: Notice that if \(\mathcal{L}\) is continuously differentiable at \(\bm{\theta}\), then \(\partial^{\circ}\mathcal{L}(\bm{\theta})=\{\nabla\mathcal{L}(\bm{\theta})\}\) is unique. However, for discontinuous differentiable points of \(\mathcal{L}\), the differential inclusion flow \(\frac{\mathrm{d}\bm{\theta}}{\mathrm{d}t}\in\partial^{\circ}\mathcal{L}(\bm{ \theta})\) defined by Definition G.1 may not be unique. To study a more specific dynamics, we also utilize Definition H.1 to determine GF at some of such points.

Now we review the definition of Karush-Kuhn-Tucker (KKT) conditions for non-smooth optimization problems (Dutta et al., 2013). Consider the following constrained optimization problem (P):

\[\min_{\bm{x}\in\mathbb{R}^{d}}: f(\bm{x})\] \[\mathrm{s.t.} g_{i}(\bm{x})\leq 0,\quad\forall i\in[N]\]

where \(f,g_{1},\cdots,g_{N}:\mathbb{R}^{d}\to\mathbb{R}\) are locally Lipschitz functions. We say that \(\bm{x}\in\mathbb{R}\) is a feasible point of (P) if \(\bm{x}\) satisfies \(g_{i}(\bm{x})\leq 0\) for all \(i\in[N]\).

**Definition G.3** (KKT Point for Non-smooth Optimization).: We say that a feasible point of (P) is a KKT point if there exists \(\lambda_{1},\cdots,\lambda_{N}\geq 0\) such that

\[1.\ \bm{0}\in\partial^{\circ}f(\bm{x})+\sum_{i\in[N]}\lambda_{i} \partial^{\circ}g_{i}(\bm{x});\] \[2.\ \forall i\in[N],\lambda_{i}g_{i}(\bm{x})=0.\]Solution of Discontinuous System

In this section, we add some supplements about the definitions of solutions of discontinuous systems, which can overcomes non-uniqueness of GF trajectories (2) to some extent. Many definitions of solutions of differential equations with discontinuous systems have been proposed. In this paper, we adopt a widely used definition of the solutions in Chapter 2.4 in (Filippov, 2013).

**Definition H.1** (Solutions of Discontinuous Systems, Chapter 2.4 in (Filippov, 2013)).: Consider a \(n\)-dimensional equation or a system \((\bm{x}\in\mathbb{R}^{n})\): \(\frac{\mathrm{d}\bm{x}}{\mathrm{d}t}=\bm{f}(\bm{x})\) with a piecewise continuous function \(\bm{f}\) in a domain \(G\). We aim to define the dynamics near some discontinuous regions.

Let the function \(\bm{f}\) be discontinuous on a smooth surface \(S\) given by the equation \(\phi(\bm{x})=0\). Let \(\bm{x}^{*}\in S\) and the surface \(S\) separate the neighborhood of \(\bm{x}^{*}\) into domains \(G^{-}\) and \(G^{+}\). Let the function \(\bm{f}(\bm{x})\) have the limit values:

\[\bm{f}^{-}(\bm{x}^{*}):=\lim_{\bm{x}\in G^{-},\bm{x}\to\bm{x}^{*}}\bm{f}(\bm{x }),\quad\bm{f}^{+}(\bm{x}^{*}):=\lim_{\bm{x}\in G^{+},\bm{x}^{*}\to\bm{x}^{*}} \bm{f}(\bm{x}).\]

Here one should distinguish between two main cases. Let \(f^{-}_{N}(\bm{x}^{*})\) and \(f^{+}_{N}(\bm{x}^{*})\) be projections of the vectors \(\bm{f}^{-}(\bm{x}^{*})\) and \(\bm{f}^{+}(\bm{x}^{*})\) onto the normal to the surface \(S\) at the point \(\bm{x}^{*}\), where the normal is directed towards the domain \(G^{+}\).

**(Case I)**. If the vectors \(\bm{f}(\bm{x}^{*})\) are directed to the surface \(S\) on both sides, i.e. \(f^{-}_{N}(\bm{x}^{*})>0\), \(f^{+}_{N}(\bm{x}^{*})<0\), then the solution the solution starting from \(\bm{x}^{*}\) can not leave \(S\) for some time. Moreover, its dynamics on \(S\) can be defined in the following way:

\[\frac{\mathrm{d}\bm{x}}{\mathrm{d}t}=\bm{f}^{0}(\bm{x}),\] \[\text{where }\bm{f}^{0}(\bm{x})=\alpha\bm{f}^{+}(\bm{x})+(1- \alpha)\bm{f}^{-}(\bm{x}),\quad\alpha=\frac{f^{-}_{N}(\bm{x})}{f^{-}_{N}(\bm {x})-f^{+}_{N}(\bm{x})}.\]

**(Case II)**. If \(f^{-}_{N}(\bm{x}^{*})\geq 0,f^{+}_{N}(\bm{x}^{*})\geq 0\), but \(f^{-}_{N}(\bm{x}^{*})\) and \(f^{+}_{N}(\bm{x}^{*})\) are not both 0, then the solution starting from \(\bm{x}^{*}\) passes from one side of the surface \(S\) to the other instantly.

**(Case III)**. If \(f^{-}_{N}(\bm{x}^{*})<0\), \(f^{+}_{N}(\bm{x}^{*})>0\), then the dynamics is defined in the similar way as (Case I).

**(Case IV)**. If \(f^{-}_{N}(\bm{x}^{*})\leq 0,f^{+}_{N}(\bm{x}^{*})\leq 0\), but \(f^{-}_{N}(\bm{x}^{*})\) and \(f^{+}_{N}(\bm{x}^{*})\) are not both \(0\), then the dynamics is defined in the similar way as (Case II).

**Remark H.2**.: Notice that Definition H.1 overcomes non-uniqueness of GF trajectories to some extent. It is worth noting that Definition H.1 and Definition G.1 are compatible and specifically, the dynamics defined in Definition H.1(Case I, III) lie in the convex hull defined in Definition G.1.

**Remark H.3**.: In (Lyu et al., 2021), the non-branching starting point Assumption is employed to address the technical challenge of non-uniqueness in GF trajectories. By comparison, in this work, we do not need this assumption. We adopt Definition H.1 to uniquely determine the Gradient Flow trajectories theoretically near some discontinuous differential regions, such as "Ridge", "Valley", and "Refraction edge" discussed in Section I.2 in (Lyu et al., 2021).

Some Basic Inequalities

**Lemma I.1** (Hoeffding's Inequality).: _Let \(X_{1},\cdots,X_{n}\) are independent random variables, and \(X_{i}\in[a_{i},b_{i}]\) for any \(i\in[n]\). Define \(\tilde{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i}\). Then for any \(\epsilon>0\), we have the following probability inequalities:_

\[\mathbb{P}\Big{(}\tilde{X}-\mathbb{E}[\tilde{X}]\geq\epsilon\Big{)} \leq\exp\Big{(}-\frac{2n^{2}\epsilon^{2}}{\sum_{i=1}^{n}(b_{i}-a_{i})^{2}} \Big{)},\] \[\mathbb{P}\Big{(}\tilde{X}-\mathbb{E}[\tilde{X}]\leq-\epsilon\Big{)} \leq\exp\Big{(}-\frac{2n^{2}\epsilon^{2}}{\sum_{i=1}^{n}(b_{i}-a_{i})^{2}} \Big{)}.\]

**Lemma I.2**.: _Consider \(\bm{x}_{1},\bm{x}_{2},\bm{y}\in\mathbb{S}^{d-1}\), where \(\langle\bm{x}_{1},\bm{x}_{2}\rangle=\cos\Delta\)\((\Delta\in(0,\pi/2))\). If \(\langle\bm{y},\bm{x}_{1}\rangle\geq 0\) and \(\langle\bm{y},\bm{x}_{2}\rangle\leq 0\), then we have \(0\leq\langle\bm{y},\bm{x}_{1}\rangle\leq\sin\Delta\) and \(-\sin\Delta\leq\langle\bm{y},\bm{x}_{2}\rangle\leq 0\)._

Proof of Lemma I.2.: Denote \(\mathcal{M}_{\bm{x}}:=\mathrm{span}\{\bm{x}_{1},\bm{x}_{2}\}\). We can do the orthogonal decomposition of \(\bm{y}\):

\[\bm{y}=\bm{y}_{\mathcal{M}}+\bm{y}_{\mathcal{M}}^{\perp},\]

where \(\bm{y}_{\mathcal{M}}\in\mathcal{M}_{\bm{x}}\) and \(\bm{y}_{\mathcal{M}}^{\perp}\perp\mathcal{M}_{\bm{x}}\). From \(\bm{y}\in\mathrm{span}\{\bm{x}_{1},\bm{x}_{2}\}\), there exist \(\alpha,\beta\in\mathbb{R}\), s.t. \(\bm{y}_{\mathcal{M}}=\alpha\bm{x}_{1}+\beta\bm{x}_{2}\).

Due to the orthogonal decomposition, we know \(\|\bm{y}_{\mathcal{M}}\|\leq 1\), which means \(\alpha^{2}+\beta^{2}+2\alpha\beta\cos\Delta\leq 1\).

Noticing \(\alpha^{2}+\beta^{2}+2\alpha\beta\cos\Delta=(\alpha+\beta\cos\Delta)^{2}+ \alpha^{2}\sin^{2}\Delta\), we know \(\alpha^{2}\sin^{2}\Delta\leq 1\).

Due to \(\langle\bm{y},\bm{x}_{1}\rangle\geq 0\) and \(\langle\bm{y},\bm{x}_{2}\rangle\leq 0\), we have \(\langle\bm{y}_{\mathcal{M}},\bm{x}_{1}\rangle\geq 0\) and \(\langle\bm{y}_{\mathcal{M}},\bm{x}_{2}\rangle\leq 0\), which means

\[\alpha+\beta\cos\Delta\geq 0,\quad\alpha\cos\Delta+\beta\leq 0.\]

So \(\alpha\geq 0\) and \(\alpha\sin\Delta\geq 0\). Recalling \(\alpha^{2}\cos^{2}\Delta\leq 1\), we know \(0\leq\alpha\sin\Delta\leq 1\). Hence, we have \(\alpha+\beta\cos\Delta\leq\alpha-\alpha\cos^{2}\Delta=\alpha\sin^{2}\Delta\leq \sin\Delta\), i.e. \(\langle\bm{y},\bm{x}_{1}\rangle\leq\sin\Delta\).

In the same way, we have \(-\sin\Delta\leq\langle\bm{y},\bm{x}_{2}\rangle\leq 0\).

**Lemma I.3**.: _If \(p\geq 5\), we have \(\|\bm{z}\|\geq\frac{p-1}{p+1}\geq\frac{2}{3}\)._

Proof of Lemma I.3.: \[\left\|\bm{z}\right\|^{2}= \left\|\frac{1}{n}\sum_{i=1}^{n}y_{i}\bm{x}_{i}\right\|^{2}= \left\|\frac{p}{1+p}\bm{x}_{+}-\frac{1}{1+p}\bm{x}_{-}\right\|^{2}=\left( \frac{p}{1+p}\right)^{2}+\left(\frac{1}{1+p}\right)^{2}-\frac{2p}{(1+p)^{2}} \left\langle\bm{x}_{+},\bm{x}_{-}\right\rangle\] \[= \left(\frac{p}{1+p}+\frac{1}{1+p}\right)^{2}-\frac{2p}{(1+p)^{2} }\left(\cos\Delta+1\right)\geq 1-\frac{2p}{(p+1)^{2}}\cdot 2=\left(\frac{p-1}{p+1} \right)^{2}.\]

**Lemma I.4**.: _Let \(\bm{w}\in\mathbb{S}^{d-1}\). If \(\langle\bm{w},\bm{\mu}\rangle\geq 1-\epsilon\)\((\epsilon\in(0,1))\), \(p\geq 5\) and \(\cos\Delta\geq 4/5\), then we have_

\[-2\sqrt{\epsilon}\sin\Delta-\epsilon\leq\langle\bm{w},\bm{x}_{-}\rangle-\frac{ p\cos\Delta-1}{\sqrt{p^{2}+1-2p\cos\Delta}}\leq 2\sqrt{\epsilon}\sin\Delta.\]

Proof of Lemma I.4.: \[\langle\bm{w},\bm{x}_{-}\rangle=\langle\bm{w},\bm{\mu}\rangle+\langle\bm{w}, \bm{x}_{-}-\bm{\mu}\rangle=\langle\bm{w},\bm{\mu}\rangle+\langle\bm{\mu},\bm{ x}_{-}-\bm{\mu}\rangle+\langle\bm{w}-\bm{\mu},\bm{x}_{-}-\bm{\mu}\rangle\,.\]

It is easy to verify

\[\langle\bm{\mu},\bm{x}_{-}-\bm{\mu}\rangle=\langle\bm{\mu},\bm{x}_{-}\rangle-1 =\left\langle\frac{p\bm{x}_{+}-\bm{x}_{-}}{\|p\bm{x}_{+}-\bm{x}_{-}\|},\bm{x}_{-} \right\rangle-1=\frac{p\cos\Delta-1}{\sqrt{p^{2}+1-2p\cos\Delta}}-1;\]

\[\|\bm{x}_{-}-\bm{\mu}\|=\sqrt{2-2\,\langle\bm{\mu},\bm{x}_{-}\rangle}=\sqrt{2-2 \frac{p\cos\Delta-1}{\sqrt{p^{2}+1-2p\cos\Delta}}};\]\[\|\bm{w}-\bm{\mu}\|=\sqrt{2-2\left\langle\bm{w},\bm{\mu}\right\rangle}.\]

Thus,

\[|\langle\bm{w}-\bm{\mu},\bm{x}_{-}-\bm{\mu}\rangle|\leq\|\bm{w}- \bm{\mu}\|\,\|\bm{x}_{-}-\bm{\mu}\|\] \[= \sqrt{2-2\frac{p\cos\Delta-1}{\sqrt{p^{2}+1-2p\cos\Delta}}\sqrt{ 2-2\left\langle\bm{w},\bm{\mu}\right\rangle}}\leq\sqrt{2-2\frac{p\cos\Delta-1 }{\sqrt{p^{2}+1-2p\cos\Delta}}}\sqrt{2\epsilon}\] \[\leq \frac{\sqrt{2}}{\sqrt[4]{p^{2}+1-2p\cos\Delta}}\sqrt{\frac{p^{2} \sin^{2}\Delta}{\sqrt{p^{2}+1-2p\cos\Delta}+p\cos\Delta-1}}\sqrt{2\epsilon}\] \[\leq \frac{2\sqrt{\epsilon}}{\sqrt{p-1}}\frac{p\sin\Delta}{\sqrt{p-1+ p\cos\Delta-1}}\leq 2\sqrt{\epsilon}\sin\Delta.\]

Then we have the bound:

\[\langle\bm{w},\bm{x}_{-}\rangle\leq\frac{p\cos\Delta-1}{\sqrt{p^{ 2}+1-2p\cos\Delta}}-1+\langle\bm{w},\bm{\mu}\rangle+|\langle\bm{w}-\bm{\mu}, \bm{x}_{-}-\bm{\mu}\rangle|\] \[\leq \frac{p\cos\Delta-1}{\sqrt{p^{2}+1-2p\cos\Delta}}+2\sqrt{ \epsilon}\sin\Delta,\]

\[\langle\bm{w},\bm{x}_{-}\rangle\geq\frac{p\cos\Delta-1}{\sqrt{p^{2}+1-2p\cos \Delta}}-1+\langle\bm{w},\bm{\mu}\rangle-|\langle\bm{w}-\bm{\mu},\bm{x}_{-}- \bm{\mu}\rangle|\] \[\geq \frac{p\cos\Delta-1}{\sqrt{p^{2}+1-2p\cos\Delta}}-\epsilon-2 \sqrt{\epsilon}\sin\Delta.\]