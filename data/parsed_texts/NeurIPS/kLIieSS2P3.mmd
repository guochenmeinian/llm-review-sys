# On the Choice of Perception Loss Function for Learned Video Compression

Sadaf Salehkalaibar

ECE Department

University of Toronto

sadafs@ece.utoronto.ca

&Buu Phan

ECE Department

University of Toronto

truong.phan@mail.utoronto.ca

&Jun Chen

ECE Department

McMaster University

chenjun@mcmaster.ca

&Wei Yu

ECE Department

University of Toronto

welyu@ece.utoronto.ca

&Ashish Khisti

ECE Department

University of Toronto

akhisti@ece.utoronto.ca

Equal Contribution

###### Abstract

We study causal, low-latency, sequential video compression when the output is subjected to both a mean squared-error (MSE) distortion loss as well as a perception loss to target realism. Motivated by prior approaches, we consider two different perception loss functions (PLFs). The first, PLF-JD, considers the joint distribution (JD) of all the video frames up to the current one, while the second metric, PLF-FMD, considers the framewise marginal distributions (FMD) between the source and reconstruction. Using information theoretic analysis and deep-learning based experiments, we demonstrate that the choice of PLF can have a significant effect on the reconstruction, especially at low-bit rates. In particular, while the reconstruction based on PLF-JD can better preserve the temporal correlation across frames, it also imposes a significant penalty in distortion compared to PLF-FMD and further makes it more difficult to recover from errors made in the earlier output frames. Although the choice of PLF decisively affects reconstruction quality, we also demonstrate that it may not be essential to commit to a particular PLF during encoding and the choice of PLF can be delegated to the decoder. In particular, encoded representations generated by training a system to minimize the MSE (without requiring either PLF) can be _near universal_ and can generate close to optimal reconstructions for either choice of PLF at the decoder. We validate our results using (one-shot) information-theoretic analysis, detailed study of the rate-distortion-perception tradeoff of the Gauss-Markov source model as well as deep-learning based experiments on moving MNIST and KTH datasets. Code will be available at https://github.com/truongbuu/URDP_flow.

## 1 Introduction

There is an increasing demand for video compression algorithms that are able to generate visually pleasing videos at low bitrates. Most of the current video codecs use distortion measures such as PSNR [1, 2, 3, 4], MSE and MS-SSIM [3, 4, 5] to generate reconstructions which tend to be blurry at extremely low bitrates. In recent years, there has been a growing interest (see e.g., [6, 7, 8, 9, 10]) in using deep generative models to make the reconstructions look more realistic. Such techniques introduce an additional perception loss function that measures a distance between distributions of the source and reconstruction, with _perfect_ perception corresponding requiring that the two distributions be identical.

In compression systems, improving realism comes at the price of increasing distortion. The work of Blau and Michaeli [11] establishes the theoretical rate-distortion-perception (RDP) tradeoff which has also been validated in [12; 13; 14; 15]. Furthermore _universal_ encoded representations were proposed in [16] where the representation is fixed at the encoder and the decoder is adapted to achieve a performance near the optimal RDP tradeoff curve. The extension of these works to video compression involves many challenges. First, the compression system must not only account for spatial redundancy as in image compression, but also exploit the temporal redundancy across video frames, making the system design more complex. Secondly, unlike the case of image compression, there may be no clear choice of the perception loss function (PLF). Indeed, some prior works [7] consider PLF that preserves framewise marginal distribution (PLF-FMD) between the source and reconstruction, while other works consider joint distribution (PLF-JD) across multiple frames [9].

As illustrated in Fig. 0(a), we study causal, low-latency, sequential video compression when the output is subjected to both a mean squared-error (MSE) distortion loss and either a PLF-JD or PLF-FMD metric for perception loss. Our contributions are as follows:

* _Differences in reconstruction quality based on the choice of PLF_: We demonstrate that the choice of PLF can decisively affect the reconstruction quality especially in the low bit-rate regime. We approximately characterize the operational RDP region on a per-frame basis for a first-order Markov source model and analyze the special case of Gauss-Markov sources in detail. We show that there is a significant penalty in distortion when using PLF-JD in the low-rate regime. On the experimental side, we demonstrate that while PLF-JD preserves better temporal consistency across video frames, it suffers from the _permanence of error_ phenomenon in which the mistakes in reconstructions propogate to future frames. On the other hand, the PLF-FMD metric shows

Figure 1: (a) Proposed System Model (b, c) Error permanence phenomenon under different PLFs. High fidelity but incorrect I-frame reconstruction propagates the error to subsequent P-frames in 0-PLF-JD reconstructions. The MMSE and 0-PLF-FMD reconstructions do not have this problem.

more capability in correcting mistakes across frames (see Fig. 1b for visualizations involving three-frame videos). On the other hand, if the first frame is transmitted at high bit-rate, we demonstrate that PLF-JD performs better than PLF-FMD.
* _Universality of minimum mean square error (MMSE) reconstructions_: We demonstrate that encoded representations generated from an encoder trained to minimize MSE reconstruction (without considering any PLF) suffice to produce close-to-optimal reconstructions for either choice of PLF. For general sources, we show that when using PLF-FMD, the MMSE reconstruction can be transformed to a reconstruction satisfying perfect perceptual quality by increasing the distortion at most by a _factor of two_. While a similar result does not hold for PLF-JD in general, it is satisfied for a special class of encoders which operate in the low-rate regime. For the Gauss-Markov source model we demonstrate exact universality i.e., encoded representation for the MMSE reconstruction can be adapted to achieve any other reconstruction in the RDP region. We also use deep learning based experiments to provide experimental evidence of these results. We note that the above notion of universal encoded representations based on MSE reconstruction can have significant advantages in practice. First, although the reconstructions associated with different choices of PLFs can be visually very different, universal representations delegate the choice of PLF to the decoder rather than requiring the encoder to commit to a specific PLF. Secondly, the universality of MMSE reconstructions is far more significant in the context of learned video compression. Given that perceptual reconstruction frequently generates novel details not present in the source frame, compressing motion flow vectors between the current frame and the prior perceptual reconstruction necessitates a higher bit allocation compared to utilizing the MMSE reconstruction. Hence, a recommended approach is to train end-to-end compression exclusively to minimize MSE and use our proposed scheme to achieve a (near-optimal) tradeoff between the distortion and perception losses as desired by the user.

The study of RDP region for learned video compression is considerably more challenging than the study of RDP function for a single frame in prior works ([11, 16]). This is because of the fact that the RDP region (for first-order Markov sources) involves a tradeoff between the compression rate assigned to each frame imposing a Markov structure on the reconstructions. For Gaussian sources, the proof of optimality of Gaussian reconstructions does not use the closed form of the RDP region due to its complexity. As a result, the study of RDP region for various operating regimes is quite more involved. Furthermore, for the proof of universality, one has to consider the achievability of the entire RDP region as opposed to just points on the boundary of RDP function in [16]. Finally the results on the fixed-encoder setup are more general than prior works ([11, 16]) that required a characterization of the information RDP region, which we do not require.

#### Related Work

_Perceptual Lossy Video Compression._ Distribution preserving framework using Generative Adversarial Networks (GAN) has been widely adopted as a surrogate metric for perceptual quality in image [17, 18, 19, 12] and, recently, video compression [6, 7, 8, 9]. Unlike image compression, where the choice of PLF is straightforward, there is currently no agreed-upon objective for lossy video compression. For instance, DVC-P [6] employs PLF-FMD to improve the visual quality per frame, ignoring the temporal coherence. Similarly, Mentzer et al. [7] utilize the per-frame metric with a conditional GAN model, and found no significant differences when using a GAN objective with multiple frames. Other works target temporal consistency by incorporating multiple frames in their GAN objective. This includes the work by Yang et al. [8], where every two consecutive frames are included, and by Veerabadran et al. [9], where they employ the PLF-JD metric in a non-causal setting. Unlike previous works, we study the impact of two different perception objectives, i.e. PLF-JD and PLF-FMD, on reconstructions in the causal setting, presenting theoretical properties that are verified by deep learning experiments. For the PLF-JD metric, we demonstrate the _error permanence phenomenon_, which, unlike the error propagation issue [18, 20], cannot be resolved by increasing the code rate assigned to the \(P\) frames.

_RDP Tradeoff and Principle of Universality._ Targeting the distribution preserving framework in lossy image compression, several theoretical works have shown the presence of RDP tradeoff [11, 21, 22, 23], where perfect perception comes at a cost of increasing distortion by at most a factor of 2. Furthermore, an encoder that generates universal representations exists [16, 24, 25], which enables the decoder to freely choose the level of distortion-perception tradeoff it desires. Our work explores these avenuesin the context of causal video compression. As discussed previously we show that the MMSE representation can be used as a universal representation for both perception metrics which has several advantages in the context of video compression..

## 2 System Model

Let \((X_{1},\ldots,X_{T})\in\mathcal{X}_{1}\times\ldots\times\mathcal{X}_{T}\) be \(T\) frames in a video (with each \(\mathcal{X}_{i}\subseteq\mathbb{R}^{d}\)) distributed according to \(P_{X_{1}\ldots X_{T}}\). The frames are available for encoding sequentially; \(X_{1}\) is available first, then \(X_{2}\) arrives, followed by \(X_{3}\) and so on. There is a shared randomness \(K\in\mathcal{K}\) which is available at all encoders and decoders. The following (possibly stochastic) mappings define the encoding and decoding functions:

\[f_{j} \colon\mathcal{X}_{1}\times\ldots\times\mathcal{X}_{j}\times \mathcal{K}\rightarrow\mathcal{M}_{j}, j=1,\ldots,T,\] (1) \[g_{j} \colon\mathcal{M}_{1}\times\mathcal{M}_{2}\times\ldots\times \mathcal{M}_{j}\times\mathcal{K}\rightarrow\hat{\mathcal{X}}_{j},\] (2)

where \(\mathcal{M}_{j}\in\{0,1\}^{\star}\) denotes the set of (variable-length) messages assigned by the \(j\)th encoder and \(\hat{\mathcal{X}}_{j}\subseteq\mathbb{R}^{d}\) is the \(j\)-th reconstruction alphabet (see Fig. 1a). Let \(P_{\hat{X}_{1}\ldots\hat{X}_{T}|X_{1}\ldots X_{T}}\) be the conditional distribution of the reconstructed video given the original video which is basically determined by the mappings \(\{f_{j}\}_{j=1}^{T}\) and \(\{g_{j}\}_{j=1}^{T}\). The above setting is a _one-shot_ setup as only a single source sample is compressed at a time. For each frame \(j\), a distortion metric is imposed on the output, which we assume throughout is the mean squared-error (MSE) function i.e. \(d(x_{j},\hat{x}_{j})=||x-\hat{x}_{j}||^{2}\), which is commonly used in many applications. From a perceptual point of view, for given probability distributions \(P_{X_{1}\ldots X_{j}}\) and \(P_{\hat{X}_{1}\ldots\hat{X}_{j}}\) on the original and reconstructed frame \(j\), let \(\phi_{j}(P_{X_{1}\ldots X_{j}},P_{\hat{X}_{1}\ldots\hat{X}_{j}})\) be the perception function capturing the difference between them. Note that the function \(\phi_{j}\) is defined based on the joint distribution of all first \(j\) frames. We call this metric as _perception loss function based on joint distribution (PLF-JD)_. Note that when \(\phi_{j}(P_{X_{1}\ldots X_{j}},P_{\hat{X}_{1}\ldots\hat{X}_{j}})=0\), we have:

\[P_{X_{1}\ldots X_{j}}=P_{\hat{X}_{1}\ldots\hat{X}_{j}},\qquad j=1,\ldots,T.\] (3)

We refer to this case as _zero-perception loss function based on joint distribution (\(0\)-PLF-JD)_. Alternatively, the _perception loss function based on framewise marginal distribution (PLF-FMD)_ is denoted by \(\xi_{j}(P_{X_{j}},P_{\hat{X}_{j}})\) and is based on only the marginal distribution of the \(j\)-th frame. In particular, note that \(0\)-PLF-FMD implies that \(P_{X_{j}}=P_{\hat{X}_{j}}\) for each \(j\). In most of the paper, for simplicity of presentation, we provide some of our results for \(T=3\) frames. In that case, we use the shorthand notation \(\mathsf{X}\) to denote the tuple \((X_{1},X_{2},X_{3})\), e.g., \(\mathsf{M}:=(M_{1},M_{2},M_{3})\), \(\mathsf{D}:=(D_{1},D_{2},D_{3})\), \(\mathsf{f}:=(f_{1},f_{2},f_{3})\).

## 3 Distortion Analysis for a Fixed Encoder and Zero-perception Loss

In this section, we assume that the encoding functions \(\mathsf{f}\) are fixed, but the decoding functions \(\mathsf{g}\) can be optimized to generate different reconstructions. Equivalently, the distribution \(P_{\mathsf{M}|\mathsf{X}K}\colon=1\{\mathsf{M}=\mathsf{f}(\mathsf{X},K)\}\) is fixed, while by varying the reconstruction distribution \(P_{\hat{\mathsf{X}}|\mathsf{M}K}\colon=1\{\hat{\mathsf{X}}=\mathsf{g}(\mathsf{ M},K)\}\), one attains different reconstructions \(\hat{\mathsf{X}}\), where \(1\{.\}\) denotes the indicator function. Furthermore defining \(D_{j}\colon=\mathbbm{E}_{P}[\|X_{j}-\hat{X}_{j}\|^{2}]\), we denote \(\mathsf{D}\) as the achievable distortion tuple associated with \(P_{\hat{\mathsf{X}}|\mathsf{M}K}\).

One natural choice of reconstructions is the minimum mean squared error (MMSE) reconstruction function. At step \(j\), the reconstruction, which we denote in this case by \(\hat{X}_{j}\), is obtained by taking the conditional expectation of \(X_{j}\) given all information at the decoder up to time \(j\) i.e., \(\hat{X}_{j}\colon=\mathbbm{E}_{P}[X_{j}|M_{1}\ldots M_{j},K]\) for each \(j=1,2,3\). It is well known that the MMSE reconstruction functions minimize the reconstruction distortion i.e., if we define the set

\[\Phi_{\mathsf{D}^{\min}}(P_{\mathsf{M}|\mathsf{X}K})=\{\mathsf{D}:D_{j}\geq \mathbbm{E}_{P}[\|X_{j}-\tilde{X}_{j}\|^{2}],\qquad j=1,2,3\}\] (4)

then the distortion tuple \(\mathsf{D}\) associated with any reconstruction \(P_{\hat{\mathsf{X}}|\mathsf{M}K}\) satisfies \(\mathsf{D}\in\Phi_{\mathsf{D}^{\min}}(P_{\mathsf{M}|\mathsf{X}K})\).

The main result of this section is that assuming fixed encoder, the achievable distortions under \(0\)-PLF-FMD is at most twice of that under the MMSE distortion loss alone. The same conclusion also holds for 0-PLF-JD for a class of encoders operating at low rate. We first consider the case of \(0\)-PLF-FMD.

**Definition 1** (\(0\)-Plf-Fmd Distortion): _For an encoder \(P_{\mathbb{M}|\mathsf{X}K}\), the set \(\Phi_{\mathbb{O}^{0}}(P_{\mathbb{M}|\mathsf{X}K})\) denotes the set of all distortion tuples \(\mathsf{D}\) for which there exists a reconstruction \(P_{\tilde{\mathsf{X}}|\mathsf{M}K}\) satisfying \(P_{X_{j}}=P_{\tilde{\mathsf{X}}_{j}}\) for each \(j\in\{1,2,3\}\)._

**Theorem 1**: _The set \(\Phi_{\mathbb{D}^{0}}(P_{\mathbb{M}|\mathsf{X}K})\) is characterized as follows:_

\[\Phi_{\mathbb{D}^{0}}(P_{\mathbb{M}|\mathsf{X}K})=\{\mathsf{D}:D_{j}\geq \mathbb{E}_{P}[\|X_{j}-\tilde{X}_{j}\|^{2}]+W_{2}^{2}(P_{\tilde{X}_{j}},P_{X_{ j}}),\ j=1,2,3\},\] (5)

_where \(W_{2}^{2}(P_{X_{j}},P_{\tilde{X}_{j}})\) denotes the Wasserstein-2 distance between the two distributions [26]. Furthermore, we also have that:_

\[\Phi_{\mathbb{D}^{0}}(P_{\mathbb{M}|\mathsf{X}K})\supseteq\{\mathsf{D}:D_{j} \geq 2\mathbb{E}_{P}[\|X_{j}-\tilde{X}_{j}\|^{2}],\ \ \ j=1,2,3\},\] (6)

_i.e., minimum achievable distortion with \(0\)-PLF-FMD is at most twice the MMSE distortion._

_Proof:_ See Appendix A.

We remark that the proof of Theorem 1, operationally demonstrates that the MMSE reconstruction can be converted to another reconstruction satisfying \(0\)-PLF-FMD with at-most a factor of \(2\) increase in distortion, generalizing the result in [16] for the single frame scenario (see also [21]).

We next consider the case when zero perception loss is satisfied under the PLF-JD metric. Analogous to \(\Phi_{\mathbb{D}^{0}}(P_{\mathbb{M}|\mathsf{X}K})\) in Definition 1, one can define \(\Phi_{\mathbb{D}^{0}}^{\text{joint}}(P_{\mathbb{M}|\mathsf{X}K})\) to be the set of distortions associated with reconstruction functions that satisfy (3). The analysis of \(\Phi_{\mathbb{D}^{0}}^{\text{joint}}(P_{\mathbb{M}|\mathsf{X}K})\) is discussed in Appendix B as it is more involved. In general, the _factor of two bound_ as in Theorem 1 cannot be realized in this case as demonstrated by a counter-example in Appendix B. Nevertheless, for a special family of encoders we can obtain a counterpart of Theorem 1. In this family of encoders, the source \(X_{j}\) at time \(j\) is nearly independent of the encoder outputs up to and including time \(j\), i.e., we can express:

\[P_{X_{j}|M_{1}\dots M_{j}K}^{\text{noisy}}=(1-\mu)P_{X_{j}}+\mu Q_{X_{j}|M_{1 }\dots M_{j}K}^{\text{noisy}},\qquad j=1,2,3.\] (7)

where \(\mu\) is a sufficiently small constant and the distribution \(Q^{\text{noisy}}(.)\) could be arbitrary conditional distribution with same marginal as \(P_{X_{j}}\). We note that such encoders are studied in a variety of problems in information theory (see e.g., [27]) that correspond to the low rate operating regime. The following result states that the factor-two bound holds approximately for such encoders.

**Theorem 2**: _For the class of encoders given by (7), we have_

\[\Phi_{\mathbb{D}^{0}}^{\text{joint}}(P_{\mathbb{M}|\mathsf{X}K}^{\text{noisy} })\supseteq\{\mathsf{D}:D_{j}\geq 2\mathbb{E}_{P^{\text{noisy}}}[\|X_{j}- \tilde{X}_{j}\|^{2}]+O(\mu),\ \ \ j=1,2,3\}.\] (8)

_Proof:_ See Appendix C.

We note that the low-rate operating regime is practically important, as at higher rates MMSE based reconstructions can suffice and the use of PLF metrics may be less relevant.

## 4 Rate-Distortion-Perception Region

In this section, we assume that both the encoder \(P_{\mathbb{M}|\mathsf{X}K}\) as well as the reconstruction \(P_{\tilde{\mathsf{X}}|\mathsf{M}K}\) can be optimized and study the associated rate-distortion-perception (RDP) tradeoff. We remind the reader that for PLF-JD and PLF-FMD, the PLFs are denoted by \(\phi_{j}(P_{X_{1}\dots X_{j}},P_{\tilde{\mathsf{X}}_{1}\dots\tilde{\mathsf{X}}_ {j}})\) and \(\xi_{j}(P_{X_{j}},P_{\tilde{\mathsf{X}}_{j}})\), respectively. In this case, the operational RDP region in the one-shot setting is defined as follows.

**Definition 2** ( Operational RDP region): _For a given \(P_{\mathbb{X}}\), an RDP tuple \((\mathsf{R},\mathsf{D},\mathsf{P})\) is said to be achievable for the one-shot setting if there exist an encoder \(P_{\mathbb{M}|\mathsf{X}K}\) and a reconstruction \(P_{\tilde{\mathsf{X}}|\mathsf{M}K}\) satisfying:_

\[\mathbb{E}[\ell(M_{j})]\leq R_{j},\ \ \ \mathbb{E}[\|X_{j}-\hat{X}_{j}\|^{2}]\leq D_{j},\ \ \ \phi_{j}(P_{X_{1}\dots X_{j}},P_{\tilde{X}_{1}\dots\tilde{X}_{j}})\leq P_{j},\ \ j=1,2,3,\] (9)

_where \(\ell(M_{j})\) denotes the length of the message \(M_{j}\). The closure of the set of all achievable tuples, denoted by \(\mathcal{C}_{\mathsf{RDP}}^{o}\), is the operational RDP region. Moreover, for a given \((\mathsf{D},\mathsf{P})\), the operational DP rate region, denoted by \(\mathcal{R}^{o}(\mathsf{D},\mathsf{P})\), is the closure of the set of all tuples \(\mathsf{R}\) such that \((\mathsf{R},\mathsf{D},\mathsf{P})\in\mathcal{C}_{\mathsf{RDP}}^{o}\)._

The region \(\mathcal{C}_{\mathsf{RDP}}^{o}\) cannot be directly computed as it involves all possible one-shot encoders/decoders. But for first-order Markov source, it has a tractable approximation in terms of mutual information.

### RDP Region of First-Order Markov Sources

We first define the first-order Markov sources and then introduce an iRDP region which is computable.

**Definition 3**: _We call \(\mathsf{X}\) as a first-order Markov source if the Markov chain \(X_{1}{\rightarrow}X_{2}{\rightarrow}X_{3}\) holds._

**Definition 4** (Information RDP region): _For first-order Markov sources, the information RDP (iRDP) region, denoted by \(\mathcal{C}_{\mathsf{RDP}}\), is the set of all tuples \((\mathsf{R},\mathsf{D},\mathsf{P})\) which satisfy the following_

\[R_{1}\geq I(X_{1};X_{r,1}),\qquad R_{2}\geq I(X_{2};X_{r,2}|X_{ r,1}),\qquad R_{3}\geq I(X_{3};X_{r,3}|X_{r,1},X_{r,2})\] (10) \[D_{j}\geq\mathbb{E}[\|X_{j}-\hat{X}_{j}\|^{2}],\qquad P_{j}\geq \phi_{j}(P_{X_{1}\ldots X_{j}},P_{\hat{X}_{1}\ldots\hat{X}_{j}}),\qquad j=1,2,3,\] (11)

_for auxiliary random variables \((X_{r,1},X_{r,2},X_{r,3})\) and \((\hat{X}_{1},\hat{X}_{2},\hat{X}_{3})\) satisfying the following_

\[\hat{X}_{1} = \eta_{1}(X_{r,1}),\ \ \hat{X}_{2}=\eta_{2}(X_{r,1},X_{r,2}),\ \ \hat{X}_{3}=X_{3,r},\] (12) \[X_{r,1} \rightarrow X_{1}\rightarrow(X_{2},X_{3}),\ \ X_{r,2}\rightarrow(X_{2},X_{r,1}) \rightarrow(X_{1},X_{3}),\] (13) \[X_{r,3} \rightarrow (X_{3},X_{r,1},X_{r,2})\rightarrow(X_{1},X_{2}),\] (14)

_for some deterministic functions \(\eta_{1}(.)\) and \(\eta_{2}(.,.)\). Moreover, for a given \((\mathsf{D},\mathsf{P})\), the information DP (iDP) rate region, denoted by \(\mathcal{R}(\mathsf{D},\mathsf{P})\), is the closure of the set of all tuples \(\mathsf{R}\) that \((\mathsf{R},\mathsf{D},\mathsf{P})\in\mathcal{C}_{\mathsf{RDP}}\)._

The expression for the iRDP region involves a search over auxiliary random variables \(\mathsf{X}_{r}\) and \(\hat{\mathsf{X}}\) that satisfy (12)-(14) subject to the constraints in (10)-(11). For first-order Markov sources, the following theorem states that the operational DP rate region can be approximated by the iDP rate region.

**Theorem 3**: _For first-order Markov sources, a given \((\mathsf{D},\mathsf{P})\) and \(\mathsf{R}\in\mathcal{R}(\mathsf{D},\mathsf{P})\), we have_

\[\mathsf{R}+\log(\mathsf{R}+1)+5\in\mathcal{R}^{o}(\mathsf{D}, \mathsf{P})\subseteq\mathcal{R}(\mathsf{D},\mathsf{P}).\] (15)

_Proof:_ See Appendix D. \(\blacksquare\)

From Theorem 3, it follows that \(\mathcal{R}(\mathsf{D},\mathsf{P})\) with overhead \(\log(\mathsf{R}+1)+5\) is an inner bound to \(\mathcal{R}^{o}(\mathsf{D},\mathsf{P})\). On the other hand, \(\mathcal{R}(\mathsf{D},\mathsf{P})\) provides an outer bound to \(\mathcal{R}^{o}(\mathsf{D},\mathsf{P})\). The two bounds match with each other in high rates. It can be shown that the overhead also vanishes in the large-blocklength setting where multiple symbols are encoded at a time. In the remainder of this paper, we will approximate \(\mathcal{R}^{o}(\mathsf{D},\mathsf{P})\) with \(\mathcal{R}(\mathsf{D},\mathsf{P})\) and use the latter region for our analysis.

**Remark 1**: _(Encoded Representations): The proof of the inner bound in Theorem 3 in Appendix D provides an operational interpretation to the auxiliary random variables \(\mathsf{X}_{r}=(X_{r,1},X_{r,2},X_{r,3})\) defined in iRDP region in Definition 4. In particular, \(X_{r,j}\) is a lossy version of the source sample \(X_{j}\) generated by the encoder in step \(j\). It is compressed and transmitted to the decoder at rate \(R_{j}\) in (10). We refer to \(\mathsf{X}_{r}\) as the encoded representation of the source \(\mathsf{X}\). The Markov chains (13)-(14) indicate that without loss of optimality, an encoded representation \(X_{r,j}\) can be computed from the source \(X_{j}\) and past reconstructions \(X_{r,1},\ldots,X_{r,j-1}\) without using past source samples \(X_{1},\ldots,X_{j-1}\)._

**Remark 2**: _(Deterministic Reconstructions): Note that the reconstruction functions generating \(\hat{\mathsf{X}}\) in Definition 4 are deterministic functions of the encoded representations (c.f. (12)). In particular, the shared randomness \(K\) is not required in the reconstruction functions. However, as the proof of the inner bound of Theorem 3 illustrates, the shared randomness is required in the compression and construction of \(X_{r,j}\). Moreover, by following the arguments in [28], one can set the reconstruction function of the last frame to be identity. Thus, in Definition 4, we have set \(\hat{X}_{3}=X_{r,3}\) in (12) where \(T=3\). In the sequel, for \(T\) frames we will set \(\hat{X}_{T}=X_{r,T}\)._

**Remark 3**: _The result in Theorem 3 also holds for the PLF-FMD. That is, one can replace the PLF in (9) and (11) by \(\xi_{j}(P_{X_{j}},P_{\hat{X}_{j}})\) and get a similar result (see Appendix D for the justification)._

### Gauss-Markov Source Model: RDP Region

In this section, we obtain practical insights through the analysis of the special case of first-order Gauss-Markov sources. We assume that \(X_{1}\sim\mathcal{N}(0,\sigma_{1}^{2})\),

\[X_{2}=\rho_{1}\frac{\sigma_{2}}{\sigma_{1}}X_{1}+N_{1},\qquad X_{3}=\rho_{2} \frac{\sigma_{3}}{\sigma_{2}}X_{2}+N_{2},\] (16)where \(N_{j}\) is independent of \(X_{j}\) with mean zero and variance \((1-\rho_{j}^{2})\sigma_{j+1}^{2}\) for \(j=1,2\). Note that the model extends naturally to the case of \(T\) time-steps. The perception metric is assumed to be the Wasserstein-2 distance, i.e., \(\phi_{j}(P_{X_{1}\ldots X_{j}},P_{\hat{X}_{1}\ldots\hat{X}_{j}}):=W_{2}^{2}(P_{ X_{1}\ldots X_{j}},P_{\hat{X}_{1}\ldots\hat{X}_{j}})\). For the PLF-FMD, the perception metric is given by \(\xi_{j}(P_{X_{j}},P_{\hat{X}_{j}}):=W_{2}^{2}(P_{X_{j}},P_{\hat{X}_{j}})\).

The following result states that for Gaussian sources, jointly Gaussian reconstructions are optimal. Thus, for a given tuple \((\mathsf{D},\mathsf{P})\), the characterization of \(\mathcal{R}(\mathsf{D},\mathsf{P})\) becomes computable.

**Theorem 4**: _For the Gauss-Markov source model, any tuple \((\mathsf{R},\mathsf{D},\mathsf{P})\in\mathcal{C}_{\mathsf{RDP}}\) can be attained by a jointly Gaussian distribution over \(\mathsf{X}_{r}\) and identity mappings for \(\eta_{j}(\cdot)\) in Definition 4._

_Proof:_ See Appendix E.

Generally, the optimized distribution in the above theorem may not admit a simple form. In the special case of \(T=2\) frames, the optimal reconstructions are given in Appendix E. To obtain practical insights, we consider various asymptotic operating regimes and provide a detailed analysis in Appendix F for the case of \(T=2\) frames and with \(\sigma_{1}^{2}=\sigma_{2}^{2}\). A summary of these results is provided in Table 2 in the same Appendix. We briefly summarize some of these results next.

### Gauss-Markov Source Model: Extremal Rates

One of the key observations of this paper is that the choice of PLF has implication on the rate allocation across different frames. Specifically, first consider the case when both \(R_{1}{=}R_{2}\) are small i.e., \(R_{1}{=}R_{2}{=}\epsilon\) (for small enough \(\epsilon\)). We discuss how each PLF affects the reconstruction in the second step. In the first step, we note that reconstruction in both cases must be identical and of the form \(\hat{X}_{1}^{G}{=}\sqrt{2\epsilon\ln 2}X_{1}{+}Z_{1}\) where \(Z_{1}{\sim}\mathcal{N}(0,(1{-}2\epsilon\ln 2)\sigma^{2})\) is independent of \(X_{1}\); the resulting distortion is given by \(D_{1}{=}2(1{-}\sqrt{2\epsilon\ln 2})\sigma^{2}\). However, the reconstructions in the second steps will be different for the two measures. For simplicity, we consider the extreme case when \(\rho{=}1\) (i.e., when \(X_{2}{=}X_{1}\)). Here, the PLF-JD metric is required to preserve perfect correlation and thus has to set \(\hat{X}_{2}^{G}{=}\hat{X}_{1}^{G}\) and results in \(D_{2}{=}D_{1}\). In other words, the decoder in the second step is unable to use any information transmitted in the second step as \(0\)-PLF-JD enforces the stringent constraint \(\hat{X}_{2}^{G}{=}\hat{X}_{1}^{G}\). In contrast, for the PLF-FMD metric, it can be shown that the reconstruction in the second step for \(\rho{=}1\) reduces to \(\hat{X}_{2}^{G}{=}\sqrt{2}\sqrt{2\epsilon\ln 2}X_{1}{+}Z_{\text{FMD}}\) and the associated distortion is given by \(D_{2}{=}2(1{-}\sqrt{4\epsilon\ln 2})\sigma^{2}\), which is lower than PLF-JD. Extending this example to \(T\) steps (with \(\rho{=}1\)), we note that PLF-JD will always be forced to output \(\hat{X}_{1}\), while the reconstruction using PLF-FMD will successively improve. The following theorem formalizes this observation.

**Theorem 5**: _For sufficiently small \(\epsilon\), let \(R_{j}=\epsilon\) and suppose that \(\rho_{j}=\rho\) and \(\sigma_{j}=\sigma\), for \(j=1,\ldots,T\). The achievable distortions \(D_{\text{FMD},j}\) (for \(0\)-PLF-FMD) and \(D_{\text{JD},j}\) (for \(0\)-PLF-JD) are:_

\[D_{\text{FMD},j}=2(1-\Delta_{\text{FMD},j}\sqrt{2\epsilon\ln 2})\sigma^{2}, \quad D_{\text{JD},j}=2(1-\Delta_{\text{JD},j}\sqrt{2\epsilon\ln 2})\sigma^{2},\] (17)

_where \(\Delta_{\text{FMD},j}:=\sqrt{1+\rho^{2}\frac{(2\rho^{2})^{j-1}-1}{2\rho^{2}-1}}\) and \(\Delta_{\text{JD},j}:=\rho^{2(j-1)}+1\{j\geq 2\}\cdot\sqrt{1-\rho^{2}}({\sum_{i=0}^{j-2}\rho^{2i}})\)._

_Proof:_ See Appendix G.

In particular, specializing to \(\rho=1\), \(\Delta_{\text{FMD},j}=2^{\frac{j-1}{2}}\) and \(\Delta_{\text{JD},j}=1\). This shows that the decrease in \(D_{\text{FMD},j}\) is exponential at each step which implies the ability of decoder based on \(0\)-PLF-FMD in correcting mistakes and not propagating them in future reconstructions. However, as discussed previously the decoder which uses \(0\)-PLF-JD is stuck at \(\hat{X}_{j}=\hat{X}_{1}\) when \(\rho=1\) and results in no subsequent improvement in the distortion. We call this behaviour as _permanence of error_. This phenomenon is magnified in the case when \(R_{1}\to 0\) and \(R_{2}\to\infty\), treated in Table 2 (Appendix F) as the PLF-JD severely constrains the decoder to copy the previous noisy reconstruction while the flexibility provided by PLF-FMD reduces the distortion in the second step.

The case when \(R_{1}\to\infty\) and \(R_{2}=\epsilon\) treated in Table 2 in Appendix F corresponds to the case when \(X_{1}\) is sent at a sufficiently high rate (as is the case with some I-frames) while \(X_{2}\) is sent at a low rate. Naturally, we have \(\hat{X}_{1}^{G}\approx X_{1}\) for both PLFs. On the other hand, we once again see a qualitatively different behaviour in the reconstruction of \(X_{2}\). For the case of \(0\)-PLF-FMD, we have \(\hat{X}_{2}^{G}\approx(1-O(\epsilon))\hat{X}_{1}^{G}+O(\epsilon)X_{2}\), i.e., the decoder essentially copies the previous frame with little contribution from the second step. In contrast, for the case of \(0\)-PLF-JD, it can be shown that \(\hat{X}_{2}^{G}\approx\rho_{1}\hat{X}_{1}^{G}+O(\sqrt{\epsilon})X_{2}+Z_{\text{JD}}\), where \(Z_{\text{JD}}\) is independent Gaussian noise with variance close to \(1-\rho^{2}\). We note that the PLF-JD metric prevents the decoder from simply "copying" the previous frame, but instead forces the decoder to generate a more diverse representation consistent with the joint distribution between the two frames.

### Universal Representations for Gauss-Markov Source Model

In this section, we show that the Gauss-Markov source model admits universal encoded representations. Such representations can be transformed through appropriate reconstruction functions to achieve the entire DP rate region. This is the counterpart of the result for general sources in Theorem 1 where it is shown that the MMSE reconstructions can be transformed to some target reconstructions satisfying the \(0\)-PLF-FMD with at most a factor-2 increase in distortion. In contrast, we demonstrate that the Gauss-Markov model admits _exact universality_ i.e., target reconstructions proposed in this section achieve all points in the iDP rate region. Interestingly, the transformation is linear with possibly some additive noise. First, we formalize the notion of universal representations.

**Definition 5** (iDP-Tradeoff): _For a given rate tuple \(\mathsf{R}\), the optimal iDP-tradeoff is the closure of the set of all tuples \((\mathsf{D},\mathsf{P})\) such that \((\mathsf{R},\mathsf{D},\mathsf{P})\in\mathcal{C}_{\mathsf{RDP}}\) and is denoted by \(\mathcal{DP}(\mathsf{R})\)._

**Definition 6** (Universal Representation): _A given encoded representation \(\mathsf{X}_{r}\) is called universal with respect to rate tuple \(\mathsf{R}\) if it satisfies the rate constraints (10) and the Markov chains in (13)-(14) and for each \((\mathsf{D},\mathsf{P})\in\mathcal{DP}(\mathsf{R})\), there exists a reconstruction \(\hat{\mathsf{X}}\) generated from \(P_{\hat{\mathsf{X}}|\mathsf{X}_{r}}\) achieving it._

For the Gauss-Markov source model, we show that the MMSE reconstruction admits a universal representation. We consider the reconstruction \(\hat{\mathsf{X}}_{r}\) that achieves minimum distortion in the \(\mathcal{DP}(\mathsf{R})\) region. This point is explicitly characterized in Appendix H.1. Furthermore, following Theorem 4, since the reconstruction functions \(\eta_{j}(\cdot)\) are identity, the MMSE reconstruction is equivalent to MMSE representation i.e., \(\hat{\mathsf{X}}_{r}=\mathsf{X}_{r}^{\text{RD}}\).The following theorem establishes that any point in \(\mathcal{DP}(\mathsf{R})\) can be achieved from \(\hat{\mathsf{X}}_{r}\).

**Theorem 6**: _For the Gauss-Markov source model and a given rate tuple \(\mathsf{R}\) with strictly positive components, let the MMSE representation be denoted as \(\mathsf{X}_{r}^{\text{RD}}=(X_{r,1}^{\text{RD}},X_{r,2}^{\text{RD}},X_{r,3}^{ \text{RD}})\). Let \((\mathsf{D},\mathsf{P})\in\mathcal{DP}(\mathsf{R})\) and let \(\hat{\mathsf{X}}=(\hat{X}_{1},\hat{X}_{2},\hat{X}_{3})\) be the corresponding reconstruction achieving it. Then there exist \(\kappa_{1}\), \(\theta_{1}\), \(\theta_{2}\), \(\psi_{1}\), \(\psi_{2}\) and \(\psi_{3}\) and noise variables \((Z_{1}\), \(Z_{2}\), \(Z_{3})\) independent of \((X_{r,1}^{\text{RD}},X_{r,2}^{\text{RD}},X_{r,3}^{\text{RD}})\), which satisfy the following_

\[\hat{X}_{1}=\kappa_{1}X_{r,1}^{\text{RD}}+Z_{1},\quad\hat{X}_{2}=\theta_{1}X_{ r,1}^{\text{RD}}+\theta_{2}X_{r,2}^{\text{RD}}+Z_{2},\quad\hat{X}_{3}=\psi_{1}X_{ r,1}^{\text{RD}}+\psi_{2}X_{r,2}^{\text{RD}}+\psi_{3}\hat{X}_{r,3}^{\text{RD}}+Z_{3}.\]

_Proof:_ See Appendix H.2. 

The above theorem indicates that the MMSE representation can be linearly transformed to achieve any point in \(\mathcal{DP}(\mathsf{R})\). In general the MMSE representation may have to be degraded through additional noise terms. In the proof of Theorem 6 we identify conditions when such degradation is not needed.

As discussed in Appendix H.2, Theorem 6 holds for both PLFs. This suggests the idea that one can train an encoder to get MMSE representations which are oblivious to the choice of PLF. Then, the decoder can generate a reconstruction which satisfies either of PLFs by simply applying a linear transformation to the MMSE representation. Thus, the task of choosing the right PLF can be assigned to the decoder based on distortion and perception requirements. We conclude by noting that Appendix H.3 provides an example where the coefficients in Theorem 6 can be computed explicitly.

## 5 Experimental Results

We conduct experiments on the MovingMNIST dataset [29] (with 1 digit) using Wasserstein GAN [30], to verify the implications of our theoretical claims to perceptual video compression. Additional results on the KTH dataset [31] are available in Appendix J.3. Our compression network is built on the scale-space flow model [32] and conditional module [33]. For a given rate and PLF, we obtain different distortion-perception tradeoff points by optimizing the weighted sum between distortion and perception losses. Details about the architecture and training procedure are available in the Appendix J.1. The experimental setup is focused on validating our theory, rather than proposing state-of-the-art neural network architectures. Accordingly, we begin by (1) validating Theorems 1 and 2, which characterize the factor-of-two bounds on the distortion of 0-PLF reconstructions (2) empirically demonstrating the _error permanence_ phenomenon of the PLF-JD in Section 4.3 and (3) computing the DP tradeoff function experimentally as well as confirming that the MMSE reconstruction provide near universal representations, as motivated by the results in Section 4.4.

As our first experimental result in Table 1, we validate the _factor of two bounds_ in Theorems 1 and 2. We consider the compression of two frames \(X_{1}\) and \(X_{2}\) at rates \(R_{1}\) and \(R_{2}\) respectively. The compression of \(X_{1}\) is performed without any prior reference and corresponds to the compression of the "I-frame", while the compression of \(X_{2}\) corresponds to the "P-frame", using \(X_{1}\) as the reference. We consider the cases when either \(R_{1}{=}\infty\) or \(R_{1}{=}12\) bits, where the former corresponds to lossless compression of \(X_{1}\) and the latter corresponds to the low rate regime (see Appendix I for a justification). The average distortion for the first frame when \(R_{1}=12\) is \(0.0124\) for the MMSE reconstruction and \(0.0235\) for the 0-PLF reconstruction, thus satisfying the factor of two bound. In compression of \(X_{2}\), we systematically vary the value of the rate \(R_{2}{\in}\{4,8,12,\infty\}\). Following Table 0(b), for 0-PLF-JD reconstruction, only \(R_{2}{=}4\) bits (low rate) satisfies the factor of two bounds as expected. Intuitively, even as more bits are acquired, the 0-PLF-JD criteria actively restricts improving the reconstructions, resulting in persistently higher distortion. Even when \(R_{2}=\infty\), the distortion remains non-zero as the decoder is forced to maintain temporal consistency with \(\hat{X}_{1}\). In contrast, for FMD, the factor of \(2\) bound holds at all rates, consistent with Theorem 1.

In Fig. 2, we present our experimental results with a group of pictures (GOP) of size 3 (i.e. one I-frame followed by two P-frames). In Fig. 1(a), we visualize sample reconstructions for MSE, 0-PLF-FMD and 0-PLF-JD cases when operating in the low-rate regime with \(R_{j}{=}12\) bits for \(j{=}1,2,3\). Note that given an incorrect digit reconstruction in \(\hat{X}_{1}\), the decoder with 0-PLF-JD consistently produces incorrect digits (or content) while the 0-PLF-FMD gradually "corrects" it, which confirms the _error permanence phenomenon_ discussed in the theoretical analysis in Section 4.3 and Table 2 in Appendix F. We also plot the framewise distortion in Fig. 1(b) to show the difference in achievable distortion two perception metrics across different values for \(R_{2}\) and \(R_{3}\) as a function of the frame index. Consistent with Theorem 5, the achievable distortion decreases much faster for 0-PLF-FMD than 0-PLF-JD for all selection of rates. Finally, we show similar results in Figure 3 for UVG dataset.

In Fig. 4, we plot the tradeoff curves between distortion and perception for the second reconstruction \(\hat{X}_{2}\) for both optimal (end-to-end) and universal representations for two cases: when \(R_{1}{=}\infty\) and \(R_{1}{=}12\) bits and for a range of values for \(R_{2}\). In general, the curves for both universal and optimal

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline \(R_{2}\) & MMSE & 0-PLF-FMD & 0-PLF-JD \\ \hline
1 & \(1.08\pm 0.01\) & \(1.74\pm 0.02\,\checkmark\) & \(2.05\pm 0.03\,\checkmark\) \\ \hline
2 & \(0.88\pm 0.01\) & \(1.39\pm 0.03\,\checkmark\) & \(1.46\pm 0.02\,\checkmark\) \\ \hline
3.17 & \(0.53\pm 0.01\) & \(0.76\pm 0.01\,\checkmark\) & \(0.79\pm 0.01\,\checkmark\) \\ \hline \multicolumn{4}{|c|}{**(a)** Case 1: \(R_{1}{=}\infty\) bits} \\ \hline \(R_{2}\) & MMSE & 0-PLF-FMD & 0-PLF-JD \\ \hline
4 & \(1.23\pm 0.01\) & \(2.21\pm 0.04\,\checkmark\) & \(2.36\pm 0.04\,\checkmark\) \\ \hline
8 & \(1.04\pm 0.01\) & \(1.78\pm 0.03\,\checkmark\) & \(2.28\pm 0.03\,\checkmark\) \\ \hline
12 & \(0.89\pm 0.02\) & \(1.43\pm 0.02\,\checkmark\) & \(2.26\pm 0.03\,\checkmark\) \\ \hline \(\infty\) & 0.0 & 0.0 \(\checkmark\) & \(2.18\pm 0.02\,\checkmark\) \\ \hline \end{tabular}
\end{table}
Table 1: Distortions of optimal reconstructions at different regime (\(\checkmark\) means factor of 2 holds and \(\checkmark\) means otherwise). Distortion is scaled by \(10^{-2}\).

Figure 2: Permanence of Error Phenomenon. In (a), we visually compare the reconstructions. Note that \(\hat{X}_{1}\) is the same for both 0-PLF-JD and 0-PLF-PMD. In (b), we show the framewise distortion for different \((R_{2},R_{3})\).

representations are relatively close to each other at all rate regimes. The general shape of every curve is relatively similar with the exception of the PLF-JD metric in Fig. 3(b), where the curves for different rates seemingly converge since increasing the rate does not significantly improve the distortion in this case as noted previously. Finally, as the universal encoders are derived from MMSE solutions, these results imply that one can simply send the MMSE representation to the decoder and the user can flexibly change the DP tradeoff up to their requirements. We further note that even when the end-to-end model targets an operating point different from the MMSE reconstruction, the latter is still required to estimate the motion flow vectors best. The universal representation provides a natural way to reconstruct the MMSE reconstruction from the encoder output. In the plots of Fig. 4, we leverage on established universality results for I-frame compression in prior works [16] to construct the MMSE representation for motion compensation as we have a GOP of size \(2\).

Finally in Appendix J.5, we consider the ability of the decoder to generate diverse reconstructions when operating under either PLF-JD or PLF-FMD. We focus on the case when \(X_{1}\) is transmitted losslessly and when \(X_{2}\) is compressed at low rates. Consistent with the theoretical analysis in Section 4.2 and Table 2 in Appendix F, the decoder optimized for PLF-JD is capable of producing diverse reconstructions by mimicking the actual motion between the frames. The PLF-FMD leads to reconstructions that are highly correlated and less desirable.

## 6 Conclusions

This work examines different perception loss functions for causal video coding, establishing its key theoretical properties such as the operational RDP region and universality principle. Our analysis highlights that while \(0\)-PLF-JD reconstruction preserves temporal correlation, it is susceptible to the error permanence phenomenon. Moreover, our investigation of universality reveals that the encoder can transform the MMSE representation to other points on the DP tradeoffs, irrespective of the PLF. We suggest future research directions such as exploring region-based perceptual metrics [34], incorporating image-aware bits allocation, and leveraging conditional perception metric [18].

Figure 4: RDP tradeoff curves for end-to-end and universal models. We plot the tradeoff for the two regimes: \(R_{1}{=}\infty\) and \(R_{1}{=}\epsilon\) in (a) and (b) respectively. The universal and optimal curves are close to each other.

Figure 3: Error permanence phenomenon on the UVG dataset. The PLF-JD reconstructions propagate the flaws in the color tone from the previous I-frame reconstruction while the PLF-FMD is able to fix these flaws. Compression rate for I-frame and P-frame are \(~{}0.14\)4bpp (low rate) and \(4.632\)bpp (high rate) respectively.

## References

* [1]E. Agustsson, D. Minnen, N. Johnston, J. Balle, S. J. Hwang, and G. Toderici (2020) Scale-space flow for end-to-end optimized video compression. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8503-8512. Cited by: SS1.

[MISSING_PAGE_POST]

. Agustsson, M. Tschannen, F. Mentzer, R. Timofte, and L. Van Gool (2019) Generative adversarial networks for extreme learned image compression. In Proceedings of the IEEE International Conference on Computer Vision, Cited by: SS1.
* [56]E. Agustsson, M. Tschannen, F. Mentzer, R. Timofte, and L. V. Gool (2020) Conditional* [21] Y. Blau and T. Michaeli, "The perception-distortion tradeoff," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, 2018, pp. 6228-6237.
* [22] D. Freirich, T. Michaeli, and R. Meir, "A theory of the distortion-perception tradeoff in wasserstein space," _Advances in Neural Information Processing Systems_, vol. 34, pp. 25 661-25 672, 2021.
* [23] N. Saldi, T. Linder, and S. Yuksel, "Randomized quantization and optimal design with a marginal constraint," in _2013 IEEE International Symposium on Information Theory_. IEEE, 2013, pp. 2349-2353.
* [24] Z. Yan, F. Wen, R. Ying, C. Ma, and P. Liu, "On perceptual lossy compression: The cost of perceptual reconstruction and an optimal training framework," in _International Conference on Machine Learning_. PMLR, 2021, pp. 11 682-11 692.
* [25] E. Agustsson, D. Minnen, G. Toderici, and F. Mentzer, "Multi-realism image compression with a conditional generator," _arXiv preprint arXiv:2212.13824_, 2022.
* [26] V. M. Panaretos and Y. Zemel, _An invitation to statistics in Wasserstein space_. Springer, 2020.
* [27] A. Makur, _Information contraction and decomposition_. PhD Thesis, MIT, 2019.
* [28] N. Ma and P. Ishwar, "On delayed sequential coding of correlated sources," _IEEE Trans. on Info. Theory_, vol. 57, no. 6, pp. 3763-3782, 2011.
* [29] N. Srivastava, E. Mansimov, and R. Salakhudinov, "Unsupervised learning of video representations using lstms," in _International conference on machine learning_. PMLR, 2015, pp. 843-852.
* [30] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville, "Improved training of wasserstein gans," _Advances in neural information processing systems_, vol. 30, 2017.
* [31] C. Schuldt, I. Laptev, and B. Caputo, "Recognizing human actions: a local svm approach," in _Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004._, vol. 3. IEEE, 2004, pp. 32-36.
* [32] E. Agustsson, D. Minnen, N. Johnston, J. Balle, S. J. Hwang, and G. Toderici, "Scale-space flow for end-to-end optimized video compression," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2020, pp. 8503-8512.
* [33] J. Li, B. Li, and Y. Lu, "Deep contextual video compression," _Advances in Neural Information Processing Systems_, vol. 34, pp. 18 114-18 125, 2021.
* [34] E. Pergament, P. Tandon, O. Rippel, L. Bourdev, A. G. Anderson, B. Olshausen, T. Weissman, S. Katti, and K. Tatwawadi, "Pim: Video coding using perceptual importance maps," _arXiv preprint arXiv:2212.10674_, 2022.
* [35] C. T. Li and A. El Gamal, "Strong functional representation lemma and applications to coding theorems," _IEEE Trans. on Info. Theory_, vol. 64, no. 11, pp. 6967-6978, 2018.
* [36] P. Stavrou, M. Skoglund, and T. Tanaka, "Sequential source coding for stochastic systems subject to finite rate constraints," _IEEE Trans. on Automatic Control_, vol. 67, no. 8, pp. 3822-3835, 2022.
* [37] A. Khina, V. Kostina, A. Khisti, and B. Hassibi, "Tracking and control of gauss-markov processes over packet-drop channels with acknowledgments," _IEEE Trans. on Cont. of Net. Systems_, vol. 6, no. 2, pp. 549-560, 2019.
* [38] A. El Gamal and Y. H. Kim, _Network Information Theory_. Cambridge University Press, 2011.
* [39] E. Denton and R. Fergus, "Stochastic video generation with a learned prior," in _International conference on machine learning_. PMLR, 2018, pp. 1174-1183.
* [40] Y.-H. Kwon and M.-G. Park, "Predicting future frames using retrospective cycle gan," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2019, pp. 1811-1820.
* [41] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, "The unreasonable effectiveness of deep features as a perceptual metric," in _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2018, pp. 586-595.

* [42] S. Hong, D. Yang, Y. Jang, T. Zhao, and H. Lee, "Diversity-sensitive conditional generative adversarial networks," in _7th International Conference on Learning Representations, ICLR 2019_. International Conference on Learning Representations, ICLR, 2019.
* [43] R. Yang and S. Mandt, "Lossy image compression with conditional diffusion models," _arXiv preprint arXiv:2209.06950_, 2022.

## Appendix A Distortion Analysis for \(0\)-Plf-Fmd

Recall the definition of Wasserstein-2 distance [26] as follows. For given distributions \(P_{X_{j}}\) and \(P_{\tilde{X}_{j}}\), let

\[W_{2}^{2}(P_{\tilde{X}_{j}},P_{X_{j}}):=\inf\mathbb{E}[\|X_{j}-\tilde{X}_{j}\|^ {2}],\] (18)

where the infimum is over all joint distributions of \((X_{j},\tilde{X}_{j})\) with marginals \(P_{X_{j}}\) and \(P_{\tilde{X}_{j}}\).

**Theorem 1**: _The set \(\Phi_{\mathsf{D}^{0}}(P_{\mathsf{M}|\mathsf{X}K})\) is characterized as follows:_

\[\Phi_{\mathsf{D}^{0}}(P_{\mathsf{M}|\mathsf{X}K})=\{\mathsf{D}:D_{j}\geq \mathbb{E}_{P}[\|X_{j}-\tilde{X}_{j}\|^{2}]+W_{2}^{2}(P_{\tilde{X}_{j}},P_{X_{ j}}),\ j=1,2,3\},\] (19)

_Furthermore, we also have that:_

\[\Phi_{\mathsf{D}^{0}}(P_{\mathsf{M}|\mathsf{X}K})\supseteq\{\mathsf{D}:D_{j} \geq 2\mathbb{E}_{P}[\|X_{j}-\tilde{X}_{j}\|^{2}],\ \ \ j=1,2,3\},\] (20)

_i.e., minimum achievable distortion with \(0\)-PLF-FMD is at most twice the MMSE distortion._

_Proof:_ Define

\[\mathcal{D}^{0}:=\{\mathsf{D}:D_{j}\geq\mathbb{E}[\|X_{j}-\tilde{X}_{j}\|^{2}] +W_{2}^{2}(P_{\tilde{X}_{j}},P_{X_{j}}),\ \ j=1,2,3\}.\] (21)

First, we show that \(\Phi_{\mathsf{D}^{0}}(P_{\mathsf{M}|\mathsf{X}K})\subseteq\mathcal{D}^{0}\). For any \(\mathsf{D}\in\Phi_{\mathsf{D}^{0}}(P_{\mathsf{M}|\mathsf{X}K})\), there exists \(\hat{\mathsf{X}}_{\mathsf{D}^{0}}=(\hat{X}_{D_{1}^{0}},\hat{X}_{D_{2}^{0}}, \hat{X}_{D_{3}^{0}})\) jointly distributed with \((\mathsf{M},\mathsf{X},K)\) such that

\[\mathbb{E}[\|X_{j}-\hat{X}_{D_{j}^{0}}\|^{2}] \leq D_{j},\ \ \ \ \ \ \ \ j=1,2,3,\] (22) \[P_{X_{j}} =P_{\hat{X}_{D_{j}^{0}}}.\] (23)

Then, for example, the analysis for the second frame is as follows

\[D_{2} \geq\mathbb{E}[\|X_{2}-\hat{X}_{D_{2}^{0}}\|^{2}]\] (24) \[=\mathbb{E}[\|(X_{2}-\tilde{X}_{2})-(\hat{X}_{D_{2}^{0}}-\tilde{ X}_{2})\|^{2}]\] (25) \[=\mathbb{E}[\|X_{2}-\tilde{X}_{2}\|^{2}]+\mathbb{E}[\|\tilde{X}_{ 2}-\hat{X}_{D_{2}^{0}}\|^{2}]\] (26) \[\geq\mathbb{E}[\|X_{2}-\tilde{X}_{2}\|^{2}]+W_{2}^{2}(P_{\tilde{X} _{2}},P_{\hat{X}_{D_{2}^{0}}})\] (27) \[=\mathbb{E}[\|X_{2}-\tilde{X}_{2}\|^{2}]+W_{2}^{2}(P_{\tilde{X}_{ 2}},P_{X_{2}}),\] (28)

where (26) holds because both \(\tilde{X}_{2}\) and \(\hat{X}_{D_{2}^{0}}\) are functions of \((M_{1},M_{2},K)\) and thus the MMSE \((X_{2}-\tilde{X}_{2})\) is uncorrelated with \((\hat{X}_{D_{2}^{0}}-\tilde{X}_{2})\); (28) follows because the \(0\)-PLF-FMD implies that \(P_{\tilde{X}_{D_{2}^{0}}}=P_{X_{2}}\). Following similar steps for other frames, we get \(\Phi_{\mathsf{D}^{0}}(P_{\mathsf{M}|\mathsf{X}K})\subseteq\mathcal{D}^{0}\).

Next, we show that \(\mathcal{D}^{0}\subseteq\Phi_{\mathsf{D}^{0}}(P_{\mathsf{M}|\mathsf{X}K})\). Assume that \(\mathsf{D}\in\mathcal{D}^{0}\). Let \(\hat{X}_{1}^{*}\) be an auxiliary random variable jointly distributed with \((M_{1},K)\) such that it satisfies the following conditions

\[P_{\hat{X}_{1}^{*}}=P_{X_{1}},\] (29)

and

\[P_{\tilde{X}_{1}\hat{X}_{1}^{*}}=\arg\inf_{\begin{subarray}{c}\tilde{P}_{ \tilde{X}_{1}\hat{X}_{1}^{*}}:\\ \tilde{P}_{\hat{X}_{1}^{*}}=P_{\tilde{X}_{1}}\\ \tilde{P}_{\hat{X}_{1}^{*}}=P_{\hat{X}_{1}^{*}}\end{subarray}}\mathbb{E}_{ \tilde{P}}[\|\tilde{X}_{1}-\hat{X}_{1}^{*}\|^{2}].\] (30)

Moreover, let \(\hat{X}_{2}^{*}\) be an auxiliary random variable jointly distributed with \((M_{1},M_{2},K)\) such that the following two conditions are satisfied

\[P_{\hat{X}_{2}^{*}}=P_{X_{2}},\] (31)

and

\[P_{\tilde{X}_{2}\hat{X}_{2}}=\arg\inf_{\begin{subarray}{c}\tilde{P}_{\tilde{X}_ {2}\hat{X}_{2}^{*}}:\\ \tilde{P}_{\hat{X}_{2}^{*}}=\tilde{P}_{\hat{X}_{2}}\\ \tilde{P}_{\hat{X}_{2}^{*}}=P_{\hat{X}_{2}^{*}}\end{subarray}}\mathbb{E}_{ \tilde{P}}[\|\tilde{X}_{2}-\hat{X}_{2}^{*}\|^{2}].\] (32)Similarly, we define \(\hat{X}_{3}^{*}\). Now, notice that since \(\mathsf{D}\in\mathcal{D}^{0}\), we have:

\[D_{2}\geq\mathbb{E}[\|X_{2}-\tilde{X}_{2}\|^{2}]+W_{2}^{2}(P_{\tilde{X}_{2}},P_{ X_{2}}).\] (33)

It then directly follows that

\[\mathbb{E}[\|X_{2}-\hat{X}_{2}^{*}\|^{2}] =\mathbb{E}[\|X_{2}-\tilde{X}_{2}\|^{2}]+\mathbb{E}[\|\tilde{X}_ {2}-\hat{X}_{2}^{*}\|^{2}]\] (34) \[=\mathbb{E}[\|X_{2}-\tilde{X}_{2}\|^{2}]+W_{2}^{2}(P_{\tilde{X}_{ 2}},P_{\tilde{X}_{2}^{*}})\] (35) \[=\mathbb{E}[\|X_{2}-\tilde{X}_{2}\|^{2}]+W_{2}^{2}(P_{\tilde{X}_{ 2}},P_{X_{2}})\] (36) \[\leq D_{2},\] (37)

where

* (34) follows because \(\tilde{X}_{2}\) and \(\hat{X}_{2}^{*}\) are functions of \((M_{1},M_{2},K)\) and thus the MMSE \((X_{2}-\tilde{X}_{2})\) is uncorrelated with \((\hat{X}_{2}^{*}-\tilde{X}_{2})\);
* (35) follows from (32);
* (36) follows because \(P_{\hat{X}_{2}^{*}}=P_{X_{2}}\).

Following similar steps for other frames, we get \(\mathsf{D}\in\Phi_{\mathsf{D}^{0}}(P_{\mathsf{X}_{\|}\mathsf{K}})\).

Now, notice that \(W_{2}^{2}(P_{\tilde{X}_{2}},P_{X_{2}})\leq\mathbb{E}[\|X_{2}-\tilde{X}_{2}\| ^{2}]\) since the Wasserstein-2 distance takes the infimum over all possible joint distributions \((X_{2},\tilde{X}_{2})\), but the expectation in \(\mathbb{E}[\|X_{2}-\tilde{X}_{2}\|^{2}]\) is taken over the given \(P_{X_{2}\hat{X}_{2}}\). Thus, we get

\[\mathbb{E}[\|X_{2}-\tilde{X}_{2}\|^{2}]+W_{2}^{2}(P_{\tilde{X}_{2}},P_{X_{2}} )\leq 2\mathbb{E}[\|X_{2}-\tilde{X}_{2}\|^{2}].\] (38)

This concludes the proof.

## Appendix B Distortion Analysis for \(0\)-Plf-Jd

Let \(\hat{X}_{1}^{*}\) be defined as in (29)-(30). Moreover, let \(\hat{X}_{2}^{*}\) be an auxiliary random variable jointly distributed with \((M_{1},M_{2},K)\) such that the following conditions are satisfied

\[P_{\hat{X}_{2}^{*}|\hat{X}_{1}^{*}=x_{1}}=P_{X_{2}|X_{1}=x_{1}},\qquad\forall x _{1}\in\mathcal{X}_{1},\] (39)

and

\[P_{\hat{X}_{2}\hat{X}_{2}^{*}|\hat{X}_{1}^{*}=x_{1}} =\arg\inf_{\begin{subarray}{c}\tilde{P}_{\hat{X}_{2}\hat{X}_{2}^{* }|\hat{X}_{1}^{*}=x_{1}}:\\ \tilde{P}_{\hat{X}_{2}|\hat{X}_{1}^{*}=x_{1}}=\tilde{P}_{\hat{X}_{2}|\hat{X}_ {1}^{*}=x_{1}}\\ \tilde{P}_{\hat{X}_{2}^{*}|\hat{X}_{1}^{*}=x_{1}}=P_{\hat{X}_{2}^{*}|\hat{X}_ {1}^{*}=x_{1}}\end{subarray}}\mathbb{E}_{P}[\|\tilde{X}_{2}-\hat{X}_{2}^{*}\|^ {2}|\hat{X}_{1}^{*}=x_{1}],\qquad\forall x_{1}\in\mathcal{X}_{1}.\] (40)

Then, the following result holds.

**Theorem 2**: _We have_

\[\Phi_{\mathsf{D}^{0}}^{\text{joint}}(P_{\mathsf{M}|\mathsf{K}K}) \supseteq\{\mathsf{D}:D_{1}\geq\mathbb{E}[\|X_{1}-\tilde{X}_{1}\|^{2}]+W_{2}^ {2}(P_{\tilde{X}_{1}},P_{X_{1}}),\] \[D_{2}\geq\mathbb{E}[\|X_{2}-\tilde{X}_{2}\|^{2}]+\sum_{x_{1}}P_{X _{1}}(x_{1})W_{2}^{2}(P_{\tilde{X}_{2}|\hat{X}_{1}^{*}=x_{1}},P_{X_{2}|X_{1}=x _{1}}),\] \[D_{3}\geq\mathbb{E}[\|X_{3}-\tilde{X}_{3}\|^{2}]+\sum_{x_{1},x_{ 2}}P_{X_{1}X_{2}}(x_{1},x_{2})W_{2}^{2}(P_{\tilde{X}_{3}|\hat{X}_{1}^{*}=x_{1},\hat{X}_{2}^{*}=x_{2}},P_{X_{3}|X_{1}=x_{1},X_{2}=x_{2}})\}.\] (41)

_Proof:_ Define

\[\mathcal{D}_{\text{joint}}^{0}:=\{\mathsf{D}:D_{1}\geq\mathbb{E}[ \|X_{1}-\tilde{X}_{1}\|^{2}]+W_{2}^{2}(P_{\tilde{X}_{1}},P_{X_{1}}),\] \[D_{2}\geq\mathbb{E}[\|X_{2}-\tilde{X}_{2}\|^{2}]+\sum_{x_{1}}P_{X _{1}}(x_{1})W_{2}^{2}(P_{\tilde{X}_{2}|\hat{X}_{1}^{*}=x_{1}},P_{X_{2}|X_{1}=x _{1}}),\] \[D_{3}\geq\mathbb{E}[\|X_{3}-\tilde{X}_{3}\|^{2}]+\sum_{x_{1},x_{ 2}}P_{X_{1}X_{2}}(x_{1},x_{2})W_{2}^{2}(P_{\tilde{X}_{3}|\hat{X}_{1}^{*}=x_{1}, \hat{X}_{2}^{*}=x_{2}},P_{X_{3}|X_{1}=x_{1},X_{2}=x_{2}})\}.\] (42)Now, assume that \(\mathsf{D}\in\mathcal{D}^{0}_{\text{joint}}\). For the first frame, recall that \(\hat{X}_{1}^{*}\) is an auxiliary random variable jointly distributed with \((M_{1},K)\) such that it satisfies (29)-(30). From similar steps to (34)-(36), it then follows that

\[\mathbb{E}[\|X_{1}-\hat{X}_{1}^{*}\|^{2}] =\mathbb{E}[\|X_{1}-\tilde{X}_{1}\|^{2}]+W_{2}^{2}(P_{\tilde{X}_{ 1}},P_{X_{1}})\] (43) \[\leq D_{1}.\] (44)

For the second frame, since \(\mathsf{D}\in\mathcal{D}^{0}_{\text{joint}}\), we have:

\[D_{2}\geq\mathbb{E}[\|X_{2}-\tilde{X}_{2}\|^{2}]+\sum_{x_{1}}P_{X_{1}}(x_{1}) W_{2}^{2}(P_{\tilde{X}_{2}|X_{1}=x_{1}},P_{X_{2}|X_{1}=x_{1}}).\] (45)

Recall that \(\hat{X}_{2}^{*}\) is an auxiliary random variable jointly distributed with \((M_{1},M_{2},K)\) such that (39)-(40) hold. It then directly follows that

\[\mathbb{E}[\|X_{2}-\hat{X}_{2}^{*}\|^{2}] =\mathbb{E}[\|X_{2}-\tilde{X}_{2}\|^{2}]+\mathbb{E}[\|\tilde{X}_ {2}-\hat{X}_{2}^{*}\|^{2}]\] (46) \[=\mathbb{E}[\|X_{2}-\tilde{X}_{2}\|^{2}]+\sum_{x_{1}}P_{\hat{X}_{ 1}^{*}}(x_{1})\mathbb{E}[\|\tilde{X}_{2}-\hat{X}_{2}^{*}\|^{2}|\hat{X}_{1}^{*} =x_{1}]\] (47) \[=\mathbb{E}[\|X_{2}-\tilde{X}_{2}\|^{2}]+\sum_{x_{1}}P_{\hat{X}_{ 1}^{*}}(x_{1})W_{2}^{2}(P_{\tilde{X}_{2}|\hat{X}_{1}^{*}=x_{1}},P_{\hat{X}_{2} ^{*}|\hat{X}_{1}^{*}=x_{1}})\] (48) \[=\mathbb{E}[\|X_{2}-\tilde{X}_{2}\|^{2}]+\sum_{x_{1}}P_{X_{1}}(x_{ 1})W_{2}^{2}(P_{\hat{X}_{2}|\hat{X}_{1}^{*}=x_{1}},P_{X_{2}|X_{1}=x_{1}}),\] (49)

where

* (46) follows because \(\tilde{X}_{2}\) and \(\hat{X}_{2}^{*}\) are functions of \((M_{1},M_{2},K)\) and thus the MMSE \((X_{2}-\tilde{X}_{2})\) is uncorrelated with \((\tilde{X}_{2}^{*}-\tilde{X}_{2})\),
* (48) follows from (40),
* (49) follows because \(P_{\hat{X}_{1}^{*}\hat{X}_{2}^{*}}=P_{X_{1}X_{2}}\).

Following similar steps for the third frame, we get \(\mathsf{D}\in\Phi_{\mathsf{D}^{0}}(P_{\mathsf{M}|\mathsf{X}K})\). This concludes the proof. 

### A Counterexample for Factor-Two Bound in Case of \(0\)-Plf-Jd

Assume that we have only two frames, i.e., \(D_{3}\to\infty\). Let \(M_{1}\) be independent of \(X_{1}\) and \(M_{2}=X_{2}\). Then, we have \(\tilde{X}_{1}=\emptyset\) and \(\tilde{X}_{2}=X_{2}\). Consider the achievable distortion region of Theorem 2. The distortion of the first step is given by the following

\[\mathbb{E}[\|X_{1}-\tilde{X}_{1}\|^{2}]+W_{2}^{2}(P_{\tilde{X}_{1}},P_{X_{1}}) =2\mathbb{E}[X_{1}^{2}].\] (50)

For the second frame, we have

\[\mathbb{E}[\|X_{2}-\tilde{X}_{2}\|^{2}]+\sum_{x_{1}}P_{X_{1}}(x_{ 1})W_{2}^{2}(P_{\tilde{X}_{2}|\hat{X}_{1}^{*}=x_{1}},P_{X_{2}|X_{1}=x_{1}})\] \[\quad=\sum_{x_{1}}P_{X_{1}}(x_{1})W_{2}^{2}(P_{X_{2}|\hat{X}_{1}^ {*}=x_{1}},P_{X_{2}|X_{1}=x_{1}})\] (51) \[\quad=\sum_{x_{1}}P_{X_{1}}(x_{1})W_{2}^{2}(P_{X_{2}},P_{X_{2}|X_{ 1}=x_{1}}),\] (52)

where (51) follows because \(\tilde{X}_{2}=X_{2}\) and (52) follows because \(X_{2}\) is independent of \(\hat{X}_{1}^{*}\) (\(M_{1}\) is independent of \(X_{1}\), then \(\hat{X}_{1}^{*}\), which is a function of \((M_{1},K)\), would be independent of \(X_{1}\) and hence independent of \(X_{2}\)).

Now, notice that the MMSE distortion of the second step is zero since \(\tilde{X}_{2}=X_{2}\). However, the achievable distortion of the second step for the reconstruction satisfying \(0\)-PLF JD is given in (52) which clearly does not satisfy the factor-two bound.

Fixed Encoders Operating at Low rate regime

We consider the class of noisy encoders where the encoder distribution can be written as follows

\[P^{\text{noisy}}_{X_{j}|M_{1}\ldots M_{j}K}=(1-\mu)P_{X_{j}}+\mu Q^{\text{ noisy}}_{X_{j}|M_{1}\ldots M_{j}K},\qquad j=1,2,3.\] (53)

where \(\mu\) is a sufficiently small constant and the distribution \(Q^{\text{noisy}}(\cdot)\) could be arbitrary conditional distribution with same marginal as \(P_{X_{j}}\).

**Theorem 3**: _For the class of encoders given by (53), we have_

\[\Phi^{\text{joint}}_{\text{D}^{0}}(P^{\text{noisy}}_{\text{M}|\mathcal{X}K}) \supseteq\{\text{D}:D_{j}\geq 2\mathbb{E}_{P^{\text{noisy}}}[\|X_{j}-\tilde{X}_{j} \|^{2}]+O(\mu),\quad j=2,\ldots,3\}.\] (54)

_Proof:_ We analyze the distortion for the second frame. A similar argument holds for other frames.

Denote the reconstruction of the second step by \(\hat{X}_{2}^{*}\) and consider the expected distortion. From a similar justification starting from (24) and leading to (26), we can write the distortion as follows

\[\mathbb{E}[\|X_{2}-\hat{X}_{2}^{*}\|^{2}]=\mathbb{E}[\|X_{2}-\tilde{X}_{2}\|^ {2}]+\mathbb{E}[\|\tilde{X}_{2}-\hat{X}_{2}^{*}\|^{2}].\] (55)

Now, we study the expected term \(\mathbb{E}[\|\tilde{X}_{2}-\hat{X}_{2}^{*}\|^{2}]\) as follows

\[\mathbb{E}[\|\tilde{X}_{2}-\hat{X}_{2}^{*}\|^{2}]=\sum_{x_{1}}P_{\hat{X}_{1}^{* }}(x_{1})\mathbb{E}[\|\tilde{X}_{2}-\hat{X}_{2}^{*}\|^{2}|\hat{X}_{1}^{*}=x_{1}].\] (56)

In order to analyze the above expression, we first approximate the MMSE reconstruction \(\tilde{X}_{2}\) as follows

\[\tilde{X}_{2} =\mathbb{E}_{P^{\text{noisy}}}[X_{2}|M_{1},M_{2},K]\] (57) \[=(1-\mu)\mathbb{E}_{P}[X_{2}]+\mu\mathbb{E}_{Q^{\text{noisy}}}[X_ {2}|M_{1},M_{2},K]\] (58) \[=\mathbb{E}[X_{2}]+O(\mu),\] (59)

where (58) follows from (53). Moreover, notice that (59) implies that

\[\mathbb{E}[\|X_{2}-\tilde{X}_{2}\|^{2}] =\mathbb{E}[\|X_{2}-\mathbb{E}[X_{2}]+\mu(\mathbb{E}_{Q^{\text{ noisy}}}[X_{2}|M_{1},M_{2},K]-\mathbb{E}[X_{2}])\|^{2}]\] (60) \[=\mathbb{E}[\|X_{2}-\mathbb{E}[X_{2}]\|^{2}]+O(\mu).\] (61)

Next, consider the expected term in (56) as follows

\[\sum_{x_{1}}P_{\hat{X}_{1}^{*}}(x_{1})\mathbb{E}[\|\tilde{X}_{2}- \hat{X}_{2}^{*}\|^{2}|\hat{X}_{1}^{*}=x_{1}] =\sum_{x_{1}}P_{\hat{X}_{1}^{*}}(x_{1})\mathbb{E}[\|\mathbb{E}[X_ {2}]-\hat{X}_{2}^{*}\|^{2}|\hat{X}_{1}^{*}=x_{1}]+O(\mu)\] (62) \[=\sum_{x_{1}}P_{\hat{X}_{1}^{*}}(x_{1})\mathbb{E}[\|\mathbb{E}[X_ {2}]-X_{2}\|^{2}|X_{1}=x_{1}]+O(\mu)\] (63) \[=\sum_{x_{1}}P_{X_{1}}(x_{1})\mathbb{E}[\|\mathbb{E}[X_{2}]-X_{2} \|^{2}|X_{1}=x_{1}]+O(\mu)\] (64) \[=\mathbb{E}[\|\mathbb{E}[X_{2}]-X_{2}\|^{2}]+O(\mu)\] (65) \[=\mathbb{E}[\|\tilde{X}_{2}-X_{2}\|^{2}]+O(\mu),\] (66)

where

* (62) follows from (59);
* (63) follows because the \(0\)-PLF-JD implies that \(P_{\hat{X}_{2}^{*}|\hat{X}_{1}^{*}}=P_{X_{2}|X_{1}}\) and \(\mathbb{E}[X_{2}]\) is just a constant;
* (64) follows from \(0\)-PLF-JD where \(P_{\hat{X}_{1}^{*}}=P_{X_{1}}\);
* (66) follows from (61).

Considering (55) and (66), we get

\[\mathbb{E}[\|X_{2}-\hat{X}_{2}^{*}\|^{2}]=2\mathbb{E}[\|X_{2}-\tilde{X}_{2}\|^ {2}]+O(\mu).\] (67)

The proof for the third frame follows similar steps.

## Appendix D Operational RDP Region

Recall the definition of iRDP region \(\mathcal{C}_{\text{RDP}}\) for first-order Markov sources (Definition 4) as follows. It is the set of all tuples \((\mathsf{R},\mathsf{D},\mathsf{P})\) satisfying

\[R_{1} \geq I(X_{1};X_{r,1}),\] (68) \[R_{2} \geq I(X_{2};X_{r,2}|X_{r,1}),\] (69) \[R_{3} \geq I(X_{3};X_{r,3}|X_{r,1},X_{r,2}),\] (70) \[D_{j} \geq\mathbb{E}[\|X_{j}-\hat{X}_{j}\|^{2}],\qquad\qquad j=1,2,3,\] (71) \[P_{j} \geq\phi_{j}(P_{X_{1}\ldots X_{j}},P_{\hat{X}_{1}\ldots\hat{X}_{ j}}),\qquad j=1,2,3,\] (72)

for auxiliary random variables \((X_{r,1},X_{r,2},X_{r,3})\) and \((\hat{X}_{1},\hat{X}_{2},\hat{X}_{3})\) such that

\[\hat{X}_{1} =\eta_{1}(X_{r,1}),\ \ \hat{X}_{2}=\eta_{2}(X_{r,1},X_{r,2}),\ \ \hat{X}_{3}=X_{r,3},\] (73) \[X_{r,1} \to X_{1}\to(X_{2},X_{3}),\] (74) \[X_{r,2} \to(X_{2},X_{r,1})\to(X_{1},X_{3}),\] (75) \[X_{r,3} \to(X_{3},X_{r,1},X_{r,2})\to(X_{1},X_{2}),\] (76)

for some deterministic functions \(\eta_{1}(.)\) and \(\eta_{2}(.,.)\).

**Theorem 4**: _For first-order Markov sources, a given \((\mathsf{D},\mathsf{P})\) and \(\mathsf{R}\in\mathcal{R}(\mathsf{D},\mathsf{P})\), we have_

\[\mathsf{R}+\log(\mathsf{R}+1)+5\in\mathcal{R}^{o}(\mathsf{D}, \mathsf{P}).\] (77)

_Moreover, the following holds:_

\[\mathcal{R}^{o}(\mathsf{D},\mathsf{P})\subseteq\mathcal{R}( \mathsf{D},\mathsf{P}).\] (78)

_Proof:_ Before stating the achievable scheme, we first discuss the strong functional representation lemma [35]. It states that for jointly distributed random variables \(X\) and \(Y\), there exists a random variable \(U\) independent of \(X\), and function \(\phi\) such that \(Y=\phi(X,U)\). Here, \(U\) is not necessarily unique. The strong functional representation lemma states further that there exists a \(U\) which has information of \(Y\) in the sense that

\[H(Y|U)\leq I(X;Y)+\log(I(X;Y)+1)+4.\] (79)

Notice that the strong functional representation lemma can be applied conditionally. Given \(P_{XY|W}\), we can represent \(Y\) as a function of \((X,W,U)\) such that \(U\) is independent of \((X,W)\) and

\[H(Y|W,U)\leq I(X;Y|W)+\log(I(X;Y|W)+1)+4.\] (80)

_Proof of (77) (Inner bound)_:

For a given \((\mathsf{D},\mathsf{P})\) and \(\mathsf{R}\in\mathcal{R}(\mathsf{D},\mathsf{P})\), let \(\mathsf{X}_{r}=(X_{r,1},X_{r,2},X_{r,3})\) be jointly distributed with \(\mathsf{X}=(X_{1},X_{2},X_{3})\) where the Markov chains (74)-(76) hold and the rate constraints in (68)-(70)

Figure 5: Encoded representations and reconstructions of the iRDP region \(\mathcal{C}_{\text{RDP}}\).

are satisfied such that there exist \((\hat{X}_{1},\hat{X}_{2},\hat{X}_{3})\) for which distortion-perception constraints (71)-(72) hold. Denote the joint distribution of \((\mathsf{X},\mathsf{X}_{r},\hat{\mathsf{X}})\) by \(P_{\mathsf{X}\mathsf{X}_{r},\hat{\mathsf{X}}}\) and notice that according to the Markov chains in (74)-(76), it factorizes as the following

\[P_{\mathsf{X}\mathsf{X}_{r},\hat{\mathsf{X}}}=P_{X_{1}X_{2}X_{3}} \cdot P_{X_{r,1}|X_{1}}\cdot P_{X_{r,2}|X_{r,1}X_{2}}\cdot P_{X_{r,3}|X_{r,2}X _{r,1}X_{3}}\] \[\cdot\mathbbm{1}\{\hat{X}_{1}=g_{1}(X_{r,1})\}\cdot\mathbbm{1}\{ \hat{X}_{2}=g_{2}(X_{r,1},X_{r,3})\}\cdot\mathbbm{1}\{\hat{X}_{3}=X_{r,3}\}.\] (81)

For an illustration of encoded representations \(\mathsf{X}_{r}\) and reconstructions \(\hat{\mathsf{X}}\) in \(\mathcal{R}(\mathsf{D},\mathsf{P})\) which are induced by distribution \(P_{\mathsf{X}\mathsf{X}_{r},\hat{\mathsf{X}}}\), see Fig. 5.

Now, we show that \(\mathsf{R}+\log(\mathsf{R}+1)+5\in\mathcal{R}(\mathsf{D},\mathsf{P})\). The achievable scheme is as follows. Fix the joint distribution \(P_{\mathsf{X}_{r}}\) according to (81) which constructs the codebook, given by

\[P_{\mathsf{X}_{r}}=P_{X_{r,1}}P_{X_{r,2}|X_{r,1}}P_{X_{r,3}|X_{r,2}X_{r,1}}.\] (82)

From the strong functional representation lemma [35], we know that

* there exist a random variable \(V_{1}\) independent of \(X_{1}\) and a deterministic function \(q_{1}\) such that \(X_{r,1}=q_{1}(X_{1},V_{1})\) and \[H(X_{r,1}|V_{1})\leq I(X_{1};X_{r,1})+\log(I(X_{1};X_{r,1})+1)+4,\] (83) which means that the first encoder observes the source \(X_{1}\) and applies the function \(q_{1}\) to get \(X_{r,1}\) whose distribution needs to be preserved according to (82) (see Fig. 6);
* according to the conditional strong functional representation lemma, there exist a random variable \(V_{2}\) independent of \((X_{2},X_{r,1})\) and a deterministic function \(q_{2}\) such that \(X_{r,2}=q_{2}(X_{r,1},X_{2},V_{2})\) and \[H(X_{r,2}|X_{r,1},V_{2})\leq I(X_{2};X_{r,2}|X_{r,1})+\log(I(X_{2};X_{r,2}|X_ {r,1})+1)+4.\] (84) At the second step, the representation \(X_{r,1}\) is available at the second encoder. So, upon observing the source \(X_{2}\), it applies the function \(q_{2}\) to get \(X_{r,2}\) whose conditional distribution given \(X_{r,1}\) needs to be preserved according to (82) (see Fig. 6);
* according to the conditional strong functional representation lemma, there exist a random variable \(V_{3}\) independent of \((X_{3},X_{r,1},X_{r,2})\) and a deterministic function \(q_{3}\) such that \(X_{r,3}=q_{3}(X_{r,1},X_{r,2},X_{3},V_{3})\) and \[H(X_{r,3}|X_{r,1},X_{r,2},V_{3})\leq I(X_{3};X_{r,3}|X_{r,1},X_{r,2})+\log(I( X_{3};X_{r,3}|X_{r,1},X_{r,2})+1)+4.\] (85)

Now, the encoding and decoding are as follows

* With \(V_{1}\) available at all encoders and decoders, we can have a class of prefix-free binary codes indexed by \(V_{1}\) with the expected codeword length not larger than \(I(X_{1};X_{r,1})+\log(I(X_{1};X_{r,1})+1)+5\) to represent \(X_{r,1}\), losslessly (see Fig. 6).
* With \(V_{2}\) available at the encoders and decoders, we can design a set of prefix-free binary codes indexed by \((V_{2},X_{r,1})\) with expected codeword length not larger than \(I(X_{2};X_{r,2}|X_{r,1})+\log(I(X_{2};X_{r,2}|X_{r,1})+1)+5\) to represent \(X_{r,2}\), losslessly(see Fig. 6).

Figure 6: Strong functional representation lemma for \(T=2\) frames.

* Similarly, one can represent \(X_{r,3}\) losslessly with \(V_{3}\) available at the third encoder and decoder.
* The decoders can use functions \(\hat{X}_{1}=\eta_{1}(X_{r,1})\), \(\hat{X}_{2}=\eta_{2}(X_{r,1},X_{r,2})\) and \(\hat{X}_{3}=X_{r,3}\) to get the reconstruction \(\hat{\mathsf{X}}\).

This shows that \(\mathsf{R}+\log(\mathsf{R}+1)+5\in\mathcal{R}^{o}(\mathsf{D},\mathsf{P})\).

_Proof of (78) (Outer Bound):_

For any \((\mathsf{D},\mathsf{P}),\mathsf{R}\in\mathcal{R}^{o}(\mathsf{D},\mathsf{P})\), shared randomness \(K\), encoding functions \(f_{j}\colon\mathcal{X}_{1}\times\ldots\times\mathcal{X}_{j}\times\mathcal{K} \to\mathcal{M}_{j}\) and decoding functions \(g_{j}\colon\mathcal{M}_{1}\times\mathcal{M}_{2}\times\ldots\times\mathcal{M} _{j}\times\mathcal{K}\to\hat{\mathcal{X}}_{j}\) such that

\[R_{j}\geq\mathbb{E}[\ell(M_{j})],\qquad j=1,2,3,\] (86)

and

\[D_{j} \geq\mathbb{E}[\|X_{j}-\hat{X}_{j}\|^{2}], j=1,2,3,\] (87) \[P_{j} \geq\phi_{j}(P_{X_{1}\ldots X_{j}},P_{\hat{X}_{1}\ldots\hat{X}_{ j}}), j=1,2,3,\] (88)

we lower bound the expected length of the messages. Define

\[X_{r,1} :=(M_{1},K),\] (89) \[X_{r,2} :=(M_{1},M_{2},K),\] (90)

and recall that according to the decoding functions, we have

\[\hat{X}_{j}=g_{j}(M_{1},\ldots,M_{j},K),\qquad j=1,2,3.\] (91)

We can write

\[R_{1}\geq\mathbb{E}[\ell(M_{1})] \geq H(M_{1}|K)\] (92) \[=I(X_{1};M_{1}|K)\] (93) \[=I(X_{1};M_{1},K)\] (94) \[=I(X_{1};X_{r,1}).\] (95)

Now, consider the following set of inequalities

\[R_{2}\geq\mathbb{E}[\ell(M_{2})] \geq H(M_{2}|M_{1},K)\] (96) \[=I(X_{1},X_{2};M_{2}|M_{1},K)\] (97) \[=I(X_{1},X_{2};X_{2,r}|X_{r,1}).\] (98)

Similarly, we have

\[R_{3}\geq\mathbb{E}[\ell(M_{3})] \geq H(M_{3}|M_{1},M_{2},K)\] (99) \[=I(X_{1},X_{2},X_{3};M_{3}|M_{1},M_{2},K)\] (100) \[\geq I(X_{1},X_{2},X_{3};\hat{X}_{3}|X_{r,1},X_{r,2}).\] (101)

Notice that the definitions in (89)-(90) imply the following Markov chains

\[X_{r,1} \to X_{1}\to(X_{2},X_{3}),\] (102) \[X_{r,2} \to(X_{1},X_{2},X_{r,1})\to X_{3}.\] (103)

On the other hand, the decoding functions of the first and second steps imply that

\[\hat{X}_{1} =g_{1}(M_{1},K),\] (104) \[\hat{X}_{2} =g_{2}(M_{1},M_{2},K),\] (105)

where together with definitions in (89) and (90), we can write

\[\hat{X}_{1} =g_{1}(M_{1},K):=\eta_{1}(X_{r,1}),\] (106) \[\hat{X}_{2} =g_{2}(M_{1},M_{2},K):=\eta_{2}(X_{r,1},X_{r,2}),\] (107)

such that \(\eta_{1}(.)\) and \(\eta_{2}(.,.)\) are deterministic functions.

Now, consider the fact that the set of constraints in (87)-(88), (95), (98), (101) with Markov chains in (102)-(103) and deterministic functions in (106)-(107) constitute an iRDP region, denoted by \(\bar{\mathcal{C}}_{\text{RDP}}\), which is the set of all tuples \((\mathsf{R},\mathsf{D},\mathsf{P})\) such that

\[R_{1} \geq I(X_{1};X_{r,1}),\] (108) \[R_{2} \geq I(X_{1},X_{2};X_{r,2}|X_{r,1}),\] (109) \[R_{3} \geq I(X_{1},X_{2},X_{3};\hat{X}_{3}|X_{r,1},X_{r,2}),\] (110) \[D_{j} \geq\mathbb{E}[\|X_{j}-\hat{X}_{j}\|^{2}],\qquad\qquad j=1,2,3,\] (111) \[P_{j} \geq\phi_{j}(P_{X_{1}\ldots X_{j}},P_{\hat{X}_{1}\ldots\hat{X}_{ j}}),\qquad j=1,2,3,\] (112)

for auxiliary random variables \((X_{r,1},X_{r,2})\) and \((\hat{X}_{1},\hat{X}_{2},\hat{X}_{3})\) satisfying the following

\[\hat{X}_{1} =\eta_{1}(X_{r,1}),\quad\hat{X}_{2}=\eta_{2}(X_{r,1},X_{r,2})\] (113) \[X_{r,1} \to X_{1}\to(X_{2},X_{3}),\] (114) \[X_{r,2} \to(X_{1},X_{2},X_{r,1})\to X_{3}.\] (115)

for some deterministic functions \(\eta_{1}(.)\) and \(\eta_{2}(.,.)\).

Comparing the two regions \(\bar{\mathcal{C}}_{\text{RDP}}\) and \(\mathcal{C}_{\text{RDP}}\), we identify the following differences. The Markov chain in (74) is more restricted comparing to (115). Moreover, the Markov chain (75) does not exist in \(\bar{\mathcal{C}}_{\text{RDP}}\). The following lemma states that \(\bar{\mathcal{C}}_{\text{RDP}}=\mathcal{C}_{\text{RDP}}\). Now, for a given \((\mathsf{D},\mathsf{P})\), let \(\bar{\mathcal{R}}(\mathsf{D},\mathsf{P})\) denote the set of rate tuples \(\mathsf{R}\) such \((\mathsf{R},\mathsf{D},\mathsf{P})\in\bar{\mathcal{C}}_{\text{RDP}}\), then this lemma implies that \(\bar{\mathcal{R}}(\mathsf{D},\mathsf{P})=\mathcal{R}(\mathsf{D},\mathsf{P})\) which completes the proof of the outer bound. Moreover, notice that the above proof only deals with the statistics of the representations and reconstructions and does not depend on the choice of the PLF. So, it holds for both PLF-FMD and PLF-JD. This concludes the proof.

We conclude this section by the following lemma.

**Lemma 1**: _For first-order Markov sources, we have_

\[\mathcal{C}_{\text{RDP}}=\bar{\mathcal{C}}_{\text{RDP}}.\] (116)

_Proof:_ This result for the scenario without perception constraint has been similarly observed in [36, Eq. (12)]. The proof in this section is provided for completeness.

First, notice that the set of Markov chains in (74)-(76) is more restricted than the ones in (114)-(115), hence \(\mathcal{C}_{\text{RDP}}\subseteq\bar{\mathcal{C}}_{\text{RDP}}\). Now, it remains to prove that \(\mathcal{C}_{\text{RDP}}\subseteq\mathcal{C}_{\text{RDP}}\). Consider the following facts

1. The distortion constraints in (111) depend only on the joint distribution of \((X_{j},\hat{X}_{j})\), and thus on the joint distribution of \((X_{j},X_{r,1},\ldots,X_{r,j})\). So, imposing the Markov chain \(X_{r,2}\to(X_{2},X_{r,1})\to X_{1}\) does not affect the expected distortion \(\mathbb{E}[\|X_{2}-\hat{X}_{2}\|^{2}]\) since it does not depend on the joint distribution of \(X_{1}\) with \((X_{r,1},X_{r,2},X_{2})\). A similar argument holds for other frames;
2. The perception constraints in (112) depend on the joint distributions \(P_{X_{1}\ldots X_{j}}\) and \(P_{\hat{X}_{1},\ldots,\hat{X}_{j}}\) (hence on \(P_{X_{r,1}\ldots X_{r,j}}\)). Thus, imposing \(X_{r,2}\to(X_{2},X_{r,1})\to X_{1}\) does not affect \(\phi_{2}(P_{X_{1}X_{2}},P_{\hat{X}_{1}\hat{X}_{2}})\) since it does not depend on the joint distribution of \(X_{1}\) with \((X_{r,1},X_{r,2},X_{2})\). A similar argument holds for other frames;
3. Moreover, the rate constraints in (109) and (110) would be further lower bounded by \[R_{2} \geq I(X_{1},X_{2};X_{r,2}|X_{r,1})\geq I(X_{2};X_{r,2}|X_{r,1}),\] (117) \[R_{3} \geq I(X_{1},X_{2},X_{3};\hat{X}_{3}|X_{r,1},X_{r,2})\geq I(X_{3} ;\hat{X}_{3}|X_{r,1},X_{r,2}).\] (118) Thus, the set of rate constraints is optimized by the set of Markov chains (74)-(76).
4. The mutual information terms \(I(X_{1};X_{r,1})\), \(I(X_{2};X_{r,2}|X_{r,1})\) and \(I(X_{3};\hat{X}_{3}|X_{r,1},X_{r,2})\) depend on distributions \(P_{X_{1}X_{r,1}}\), \(P_{X_{r,1}X_{r,2}X_{2}}\) and \(P_{X_{3}\hat{X}_{3}X_{r,1}X_{r,2}}\), respectively. So, these distributions should be preserved by the set of Markov chains. The first two distributions are preserved by the choice of (73)-(74). Now, since we have first-order Markov sources (see Definition 3), preserving the joint distributions of \(P_{X_{r,1}X_{1}}\) and \(P_{X_{r,1}X_{r,2}X_{2}}\) is sufficient to preserve the distribution \(P_{X_{r,1}X_{r,2}X_{3}}\). So, preserving the joint distribution of \(P_{\hat{X}_{3}X_{r,1}X_{r,2}}\) is sufficient to keep \(I(X_{3};\hat{X}_{3}|X_{r,1},X_{r,2})\) unchanged.

Considering the above four facts, without loss of optimality, one can impose the following Markov chains

\[X_{r,1} \to X_{1}\to(X_{2},X_{3}),\] (119) \[X_{r,2} \to(X_{2},X_{r,1})\to(X_{1},X_{3}),\] (120) \[\hat{X}_{3} \to(X_{3},X_{r,1},X_{r,2})\to(X_{1},X_{2}).\] (121)

This concludes the proof for the PLF-JD. For the PLF-FMD, notice that the only difference is the second fact stated above. But, this also holds since the perception constraints depend only on \(P_{X_{j}}\) and \(P_{\hat{X}_{j}}\) (hence on \(P_{X_{r,1}\ldots,X_{r,j}}\)).

## Appendix E Gauss-Markov Source Model

We first remark that the Wasserstein-2 distance can also be replaced by the KL-divergence in most of the following analysis. The common properties between these two measures are convexity and the fact that they both depend on only second-order statistics when restricted to Gaussian source model.

**Theorem 5**: _For the Gauss-Markov source model, any tuple \((\mathsf{R},\mathsf{D},\mathsf{P})\in\mathcal{C}_{\text{RDP}}\) can be attained by a jointly Gaussian distribution over \((X_{r,1},X_{r,2},X_{r,3})\) and identity mappings for \(\eta_{j}(\cdot)\) in Definition 4._

_Proof:_ First, notice that a proof for the setting without perception constraint is provided in [37]. The following proof is different from [37] in some steps and also involves the perception constraint.

For a given tuple \((\mathsf{R},\mathsf{D},\mathsf{P})\in\mathcal{C}_{\text{RDP}}\), let \(X_{r,1}^{*}\), \(X_{r,2}^{*}\), \(\hat{X}_{1}^{*}=\eta_{1}(X_{r,1}^{*})\), \(\hat{X}_{2}^{*}=\eta_{2}(X_{r,1}^{*},X_{r,2}^{*})\) and \(\hat{X}_{3}^{*}\) be random variables satisfying (73)-(75). Let \(\hat{P}_{\hat{X}_{1}^{G}|X_{1}}\), \(P_{\hat{X}_{2}^{G}|\hat{X}_{1}^{G}X_{2}}\) and \(P_{\hat{X}_{3}^{G}|\hat{X}_{1}^{G}\hat{X}_{2}^{G}X_{3}}\) be jointly Gaussian distributions such that the following conditions are satisfied.

\[\text{cov}(\hat{X}_{1}^{G},X_{1}) =\text{cov}(\hat{X}_{1}^{*},X_{1}),\] (122) \[\text{cov}(\hat{X}_{1}^{G},\hat{X}_{2}^{G},X_{2}) =\text{cov}(\hat{X}_{1}^{*},\hat{X}_{2}^{*},X_{2}),\] (123) \[\text{cov}(\hat{X}_{1}^{G},\hat{X}_{2}^{G},\hat{X}_{3}^{G},X_{3}) =\text{cov}(\hat{X}_{1}^{*},\hat{X}_{2}^{*},\hat{X}_{3}^{*},X_{3}),\] (124)

In general, the Gaussian random variables which satisfy the constraints in (122)-(124) can be written in the following format

\[X_{1} =\nu\hat{X}_{1}^{G}+Z_{1},\] (125) \[\hat{X}_{2}^{G} =\omega_{1}\hat{X}_{1}^{G}+\omega_{2}X_{2}+Z_{2},\] (126) \[\hat{X}_{3}^{G} =\tau_{1}\hat{X}_{1}^{G}+\tau_{2}\hat{X}_{2}^{G}+\tau_{3}X_{3}+Z_{ 3},\] (127)

for some real \(\nu\), \(\omega_{1}\), \(\omega_{2}\), \(\tau_{1}\), \(\tau_{2}\), \(\tau_{3}\) where \(\hat{X}_{1}^{G}\sim\mathcal{N}(0,\sigma_{\hat{X}_{1}^{G}}^{2})\), \(\hat{X}_{2}^{G}\sim\mathcal{N}(0,\sigma_{\hat{X}_{2}^{G}}^{2})\), \(Z_{1}\), \(Z_{2}\) and \(Z_{3}\) are Gaussian random variables with zero mean and variances \(\alpha_{1}^{2},\alpha_{2}^{2},\alpha_{3}^{2}\), independent of \(\hat{X}_{1}^{G}\), \((\hat{X}_{1}^{G},X_{2})\) and \((\hat{X}_{1}^{G},\hat{X}_{2}^{G},X_{3})\), respectively.

We explicitly derive the coefficients \(\nu\), \(\omega_{1}\), \(\omega_{2}\), \(\tau_{1}\), \(\tau_{2}\) and \(\tau_{3}\) in the following. Multiplying both sides of (125) by \(\hat{X}_{1}^{G}\) and taking an expectation, we get

\[\mathbb{E}[X_{1}\hat{X}_{1}^{G}]=\nu\sigma_{\hat{X}_{1}^{G}}^{2}.\] (128)

According to (122), the above equation can be written as follows

\[\mathbb{E}[X_{1}\hat{X}_{1}^{*}]=\nu\mathbb{E}[\hat{X}_{1}^{*2}].\] (129)

Multiplying both sides of (126) by the vector \([\hat{X}_{1}^{G}\;\;X_{2}]\) and taking an expectation, we have

\[[\mathbb{E}[\hat{X}_{1}^{G}\hat{X}_{2}^{G}]\;\;\mathbb{E}[X_{2}\hat{X}_{2}^{G} ]]=[\omega_{1}\;\;\omega_{2}]\begin{pmatrix}\sigma_{\hat{X}_{1}^{G}}^{2}& \mathbb{E}[X_{2}\hat{X}_{1}^{G}]\\ \mathbb{E}[X_{2}\hat{X}_{1}^{G}]&\sigma_{2}^{2}\end{pmatrix}\] (130)

Considering the fact that \(\mathbb{E}[X_{2}\hat{X}_{1}^{G}]=\rho_{1}\mathbb{E}[X_{1}\hat{X}_{1}^{G}]\) and according to (123), the above equation can be written as follows

\[[\mathbb{E}[\hat{X}_{1}^{*}\hat{X}_{2}^{*}]\;\;\mathbb{E}[X_{2}\hat{X}_{2}^{*} ]]=[\omega_{1}\;\;\omega_{2}]\begin{pmatrix}\mathbb{E}[\hat{X}_{1}^{*2}]&\rho_{ 1}\mathbb{E}[X_{1}\hat{X}_{1}^{*}]\\ \rho_{1}\mathbb{E}[X_{1}\hat{X}_{1}^{*}]&\sigma_{2}^{2}\end{pmatrix}.\] (131)Similarly, multiplying both sides of (127) by the vector \([\hat{X}_{1}^{G}\ \ \hat{X}_{2}^{G}\ \ X_{3}]\), taking an expectation and considering (124), we get

\[[\mathbb{E}[\hat{X}_{1}^{*}\hat{X}_{3}^{*}]\ \ \mathbb{E}[\hat{X}_{2}^{*}\hat{X}_{3}^{ *}]\ \ \mathbb{E}[X_{3}\hat{X}_{3}^{*}]]=[\tau_{1}\ \ \tau_{2}\ \ \tau_{3}]\begin{pmatrix} \mathbb{E}[\hat{X}_{1}^{*2}]&\mathbb{E}[\hat{X}_{1}^{*}\hat{X}_{2}^{*}]&\rho_{ 1}\rho_{2}\mathbb{E}[X_{1}\hat{X}_{1}^{*}]\\ \mathbb{E}[\hat{X}_{1}^{*}\hat{X}_{2}^{*}]&\mathbb{E}[\hat{X}_{2}^{*2}]&\rho_{ 2}\mathbb{E}[X_{2}\hat{X}_{2}^{*}]\\ \rho_{1}\rho_{2}\mathbb{E}[X_{1}\hat{X}_{1}^{*}]&\rho_{2}\mathbb{E}[X_{2}\hat {X}_{2}^{*}]&\mathbb{E}[\hat{X}_{3}^{*2}]\end{pmatrix}.\] (132)

Solving equations (129), (131) and (132), we get

\[\sigma_{\hat{X}_{1}^{G}}^{2} =\mathbb{E}[\hat{X}_{1}^{*2}],\] (133) \[\nu =\frac{\mathbb{E}[X_{1}\hat{X}_{1}^{*}]}{\mathbb{E}[\hat{X}_{1}^{ *2}]},\] (134) \[\alpha_{1}^{2} =\sigma_{1}^{2}-\frac{\mathbb{E}[X_{1}\hat{X}_{1}^{*}]}{\mathbb{E }[\hat{X}_{1}^{*2}]},\] (135) \[\omega_{1} =\frac{\nu\rho_{1}\mathbb{E}[\hat{X}_{1}^{*}\hat{X}_{2}^{*}]- \mathbb{E}[X_{2}\hat{X}_{2}^{*}]}{\nu^{2}\rho_{1}^{2}\sigma_{\hat{X}_{1}^{G}}^ {2}-\sigma_{2}^{2}},\] (136) \[\omega_{2} =\frac{\nu\rho_{1}\sigma_{\hat{X}_{1}^{G}}^{2}\mathbb{E}[X_{2} \hat{X}_{2}^{*}]-\sigma_{2}^{2}\mathbb{E}[\hat{X}_{1}^{*}\hat{X}_{2}^{*}]}{ \nu^{2}\rho_{1}^{2}\sigma_{\hat{X}_{1}^{G}}^{2}-\sigma_{2}^{2}\sigma_{\hat{X} _{1}^{G}}^{2}},\] (137) \[\alpha_{2}^{2} =\mathbb{E}[\hat{X}_{2}^{*2}]-\alpha_{2}^{2}\sigma_{\hat{X}_{1}^ {G}}^{2}-\omega_{2}^{2}\sigma_{2}^{2}-2\omega_{1}\omega_{2}\rho_{1}\nu\sigma_ {\hat{X}_{1}^{G}}^{2}.\] (138)

For the third step, the coefficients and noise variance of (127) are given as follows

\[[\tau_{1}\ \ \tau_{2}\ \ \tau_{3}]\] \[\alpha_{3}^{2} =\mathbb{E}[\hat{X}_{3}^{*2}]-\tau_{1}^{2}\mathbb{E}[\hat{X}_{1} ^{*2}]-\tau_{2}^{2}\mathbb{E}[\hat{X}_{2}^{*2}]-\tau_{3}^{2}\mathbb{E}[X_{3} ^{2}]\] \[-2\tau_{1}\tau_{2}\mathbb{E}[\hat{X}_{1}^{*}\hat{X}_{2}^{*}]-2 \tau_{1}\tau_{3}\rho_{1}\rho_{2}\mathbb{E}[X_{1}\hat{X}_{1}^{*}]-2\tau_{2}\tau _{3}\rho_{2}\mathbb{E}[X_{2}\hat{X}_{2}^{*}],\] (140)

where \((.)^{-1}\) denotes the inverse of a matrix.

Now, we look at the rate constraints.

_Rate Constraints_:

Consider the rate constraint of the first step as follows

\[R_{1} \geq I(X_{1};X_{r,1}^{*})\] (141) \[=H(X_{1})-H(X_{1}|X_{r,1}^{*})\] (142) \[\geq H(X_{1})-H(X_{1}|\hat{X}_{1}^{*})\] (143) \[=H(X_{1})-H(X_{1}-\mathbb{E}[X_{1}|\hat{X}_{1}^{*}]|\hat{X}_{1}^{ *})\] (144) \[\geq H(X_{1})-H(X_{1}-\mathbb{E}[X_{1}|\hat{X}_{1}^{*}])\] (145) \[\geq H(X_{1})-H(X_{1}-\mathbb{E}[X_{1}|\hat{X}_{1}^{G}])\] (146) \[=H(X_{1})-H(X_{1}-\mathbb{E}[X_{1}|\hat{X}_{1}^{G}]|\hat{X}_{1}^{ G})\] (147) \[=I(X_{1};\hat{X}_{1}^{G})\] (148)

where

* (143) follows because \(\hat{X}_{1}^{*}\) is a function of \(X_{r,1}^{*}\);
* (146) follows because for a given covariance matrix in (122), the Gaussian distribution maximizes the differential entropy;
* (147) follows because the MMSE is uncorrelated from the data and since the random variables are Gaussian, the MMSE would be independent of the data.

Next, consider the rate constraint of the second step as the following

\[R_{2} \geq I(X_{2};X_{r,2}^{*}|X_{r,1}^{*})\] (149) \[=H(X_{2}|X_{r,1}^{*})-H(X_{2}|X_{r,1}^{*},X_{r,2}^{*})\] (150) \[\geq H(X_{2}|X_{r,1}^{*})-H(X_{2}|\hat{X}_{1}^{*},\hat{X}_{2}^{*})\] (151) \[\geq H(X_{2}|X_{r,1}^{*})-H(X_{2}|\hat{X}_{1}^{G},\hat{X}_{2}^{G})\] (152) \[=H(\rho_{1}X_{1}+N_{1}|X_{r,1}^{*})-H(X_{2}|\hat{X}_{1}^{G},\hat{X }_{2}^{G})\] (153) \[\geq\frac{1}{2}\log\left(\rho_{1}^{2}2^{2H(X_{1}|X_{r,1}^{*})}+2 ^{2H(N_{1})}\right)-H(X_{2}|\hat{X}_{1}^{G},\hat{X}_{2}^{G})\] (154) \[\geq\frac{1}{2}\log\left(\rho_{1}^{2}2^{-2R_{1}}2^{2H(X_{1})}+2 ^{2H(N_{1})}\right)-H(X_{2}|\hat{X}_{1}^{G},\hat{X}_{2}^{G}),\] (155)

where

* (151) follows because \(\hat{X}_{1}^{*}\) and \(\hat{X}_{2}^{*}\) are deterministic functions of \(X_{r,1}^{*}\) and \((X_{r,1}^{*},X_{r,2}^{*})\), respectively;
* (152) follows because for a given covariance matrix in (123), the Gaussian distribution maximizes the differential entropy;
* (154) follows from entropy power inequality (EPI) [38, pp. 22];
* (155) follows from (142).

Similarly, consider the rate constraint of the third frame as the following,

\[R_{3} \geq I(X_{3};\hat{X}_{3}^{*}|X_{r,1}^{*},X_{r,2}^{*})\] (156) \[=H(X_{3}|X_{r,1}^{*},X_{r,2}^{*})-H(X_{3}|X_{r,1}^{*},X_{r,2}^{*},\hat{X}_{3}^{*})\] (157) \[\geq H(X_{3}|X_{r,1}^{*},X_{r,2}^{*})-H(X_{3}|\hat{X}_{1}^{*}, \hat{X}_{2}^{*},\hat{X}_{3}^{*})\] (158) \[\geq H(X_{3}|X_{r,1}^{*},X_{r,2}^{*})-H(X_{3}|\hat{X}_{1}^{G}, \hat{X}_{2}^{G},\hat{X}_{3}^{G})\] (159) \[=H(\rho_{2}X_{2}+N_{2}|X_{r,1}^{*},X_{r,2}^{*})-H(X_{3}|\hat{X}_{ 1}^{G},\hat{X}_{2}^{G},\hat{X}_{3}^{G})\] (160) \[\geq\frac{1}{2}\log\left(\rho_{2}^{2}2^{2H(X_{2}|X_{r,1}^{*},X_{ r,2}^{*})}+2^{2H(N_{2})}\right)-H(X_{3}|\hat{X}_{1}^{G},\hat{X}_{2}^{G},\hat{X }_{3}^{G})\] (161) \[\geq\frac{1}{2}\log\left(\rho_{2}^{2}2^{-2R_{2}}2^{2H(X_{2}|X_{r,1}^{*})}+2^{2H(N_{2})}\right)-H(X_{3}|\hat{X}_{1}^{G},\hat{X}_{2}^{G},\hat{X }_{3}^{G})\] (162) \[\geq\frac{1}{2}\log\left(\rho_{1}^{2}\rho_{2}^{2}2^{-2R_{1}-2R_{2 }}2^{2H(X_{1})}+\rho_{2}^{2}2^{-2R_{2}}2^{2H(N_{1})}+2^{2H(N_{2})}\right)-H(X_{ 3}|\hat{X}_{1}^{G},\hat{X}_{2}^{G},\hat{X}_{3}^{G})\] (163)

Next, we look at the distortion constraint.

_Distortion Constraint_: The choices in (122)-(124) imply that

\[D_{j}\geq\mathbb{E}[\|X_{j}-\hat{X}_{j}^{*}\|^{2}]=\mathbb{E}[\|X_{j}-\hat{X}_ {j}^{G}\|^{2}],\qquad j=1,2,3.\] (164)

Finally, we look at the perception constraint

_Perception Constraint_:

Define the following distribution

\[P_{U^{*}V^{*}}:=\arg\inf_{\begin{subarray}{c}\hat{P}_{UV^{*}}:\\ \hat{P}_{U}=P_{X_{1}}\\ \hat{P}_{V}=P_{\hat{X}_{1}^{*}}\end{subarray}}\mathbb{E}_{\hat{P}}[\|U-V\|^{2}].\] (165)

Now, define \(P_{U^{G}V^{G}}\) to be a Gaussian joint distribution with the following covariance matrix

\[\text{cov}(U^{G},V^{G})=\text{cov}(U^{*},V^{*}).\] (166)

Then, we have the following set of inequalities:\[P_{1}\geq W_{2}^{2}(P_{X_{1}},P_{\hat{X}_{1}^{*}}) =\inf_{\begin{subarray}{c}\hat{P}_{UV}:\\ \hat{P}_{UV}=P_{X_{1}}\\ \hat{P}_{V}=P_{X_{1}^{*}}\end{subarray}}\mathbb{E}_{\hat{P}}[\|U-V\|^{2}]\] (167) \[=\mathbb{E}[\|U^{*}-V^{*}\|^{2}]\] (168) \[=\mathbb{E}[\|U^{G}-V^{G}\|^{2}]\] (169) \[\geq W_{2}^{2}(P_{U^{G}},P_{V^{G}})\] (170) \[=\inf_{\begin{subarray}{c}\hat{P}_{UV}:\\ \hat{P}_{U}=P_{V^{G}}\\ \hat{P}_{V}=P_{V^{G}}\end{subarray}}\mathbb{E}_{\hat{P}}[\|U-V\|^{2}]\] (171) \[=\inf_{\begin{subarray}{c}\hat{P}_{UV}:\\ \hat{P}_{UV}=P_{X_{1}}\\ \hat{P}_{V}=P_{X_{1}^{G}}\end{subarray}}\mathbb{E}_{\hat{P}}[\|U-V\|^{2}]\] (172) \[=W_{2}^{2}(P_{X_{1}},P_{\hat{X}_{1}^{G}}),\] (173)

where

* (168) follows from the definition in (165);
* (169) follows from (166) which implies that \((U^{*},V^{*})\) and \((U^{G},V^{G})\) have the same second-order statistics;
* (172) follows because \(P_{V^{G}}=P_{\hat{X}_{1}^{G}}\) which is justified in the following. First, notice that both \(P_{V^{G}}\) and \(P_{\hat{X}_{1}^{G}}\) are Gaussian distributions. Denote the variance of \(V^{G}\) by \(\sigma_{V^{G}}^{2}\) and recall that the variance of \(\hat{X}_{1}^{G}\) is denoted by \(\sigma_{\hat{X}_{1}^{G}}^{2}\). According to (166), \(\sigma_{V^{G}}^{2}\) is equal to the variance of \(V^{*}\). Also, from (165), we know that \(P_{V^{*}}=P_{\hat{X}_{1}^{*}}\), hence the variances of \(V^{*}\) and \(\hat{X}_{1}^{*}\) are the same. On the other side, according to (122), we know that the variance of \(\hat{X}_{1}^{*}\) is equal to \(\sigma_{\hat{X}_{1}^{G}}^{2}\). Thus, we conclude that \(\sigma_{\hat{X}_{1}^{G}}^{2}=\sigma_{V^{G}}^{2}\), which yields \(P_{V^{G}}=P_{\hat{X}_{1}^{G}}\). A similar argument shows that \(P_{U^{G}}=P_{X_{1}}\).

A similar argument holds for the perception constraint of the second and third steps for both PLFs.

Thus, we have proved the set of Gaussian auxiliary random variables \((\hat{X}_{1}^{G},\hat{X}_{2}^{G},\hat{X}_{3}^{G})\) given in (125)-(127) where the coefficients are chosen according to distortion-perception constraints, provides an outer bound to \(\mathcal{C}_{\text{RDP}}\) which is the set of all tuples \((\mathsf{R},\mathsf{D},\mathsf{P})\) such that

\[R_{1} \geq I(X_{1};\hat{X}_{1}^{G}),\] (174) \[R_{2} \geq\frac{1}{2}\log\left(\rho_{1}^{2}2^{-2R_{1}}2^{2H(X_{1})}+2^ {2H(N_{1})}\right)-H(X_{2}|\hat{X}_{1}^{G},\hat{X}_{2}^{G}),\] (175) \[R_{3} \geq\frac{1}{2}\log\left(\rho_{1}^{2}\rho_{2}^{2}2^{-2R_{1}-2R_{ 2}}2^{2H(X_{1})}+\rho_{2}^{2}2^{-2R_{2}}2^{2H(N_{1})}+2^{2H(N_{2})}\right)-H(X _{3}|\hat{X}_{1}^{G},\hat{X}_{2}^{G},\hat{X}_{3}^{G}),\] (176) \[D_{j} \geq\mathbb{E}[\|X_{j}-\hat{X}_{j}^{G}\|^{2}],\qquad\qquad j=1,2,3\] (177) \[P_{j} \geq W_{2}^{2}(P_{X_{1}\ldots X_{j}},P_{\hat{X}_{1}^{G}\ldots \hat{X}_{j}^{G}}).\] (178)

Now, we need to show that the above RDP region is also an inner bound to \(\mathcal{C}_{\text{RDP}}\). This is simply verified by the following choice. In iRDP region of (68)-(76), choose the following:

\[X_{r,j}=\hat{X}_{j}=\hat{X}_{j}^{G},\qquad j=1,2,3,\] (179)

where \((\hat{X}_{1}^{G},\hat{X}_{2}^{G},\hat{X}_{3}^{G})\) satisfy (125)-(127) with coefficients chosen according to distortion-perception constraints. The lower bounds on distortion and perception constraints in (177) and (178) are immediately achieved by this choice. Now, we will look at the rate constraints. The achievable rate constraint of the first step can be written as follows

\[R_{1}\geq I(X_{1};\hat{X}_{1}^{G}),\] (180)which immediately coincides with (174). The achievable rate of the second step can be written as follows

\[R_{2} \geq I(X_{2};\hat{X}_{2}^{G}|\hat{X}_{1}^{G})\] (181) \[=H(X_{2}|\hat{X}_{1}^{G})-H(X_{2}|\hat{X}_{1}^{G},\hat{X}_{2}^{G})\] (182) \[=H(\rho_{1}X_{1}+N_{1}|\hat{X}_{1}^{G})-H(X_{2}|\hat{X}_{1}^{G}, \hat{X}_{2}^{G})\] (183) \[=\frac{1}{2}\log(\rho_{1}^{2}2^{2H(X_{1}|\hat{X}_{1}^{G})}+2^{2H( N_{1})})-H(X_{2}|\hat{X}_{1}^{G},\hat{X}_{2}^{G})\] (184) \[\geq\frac{1}{2}\log\left(\rho_{1}^{2}2^{-2R_{1}}2^{2H(X_{1})}+2^{ 2H(N_{1})}\right)-H(X_{2}|\hat{X}_{1}^{G},\hat{X}_{2}^{G}),\] (185)

where

* (184) follows because EPI holds with "equality" for jointly Gaussian distributions [38, pp. 22];
* (185) follows from (175).

Thus, the bound in (185) coincides with (155). A similar argument holds for the achievable rate of the third frame.

Notice that the above proof (both converse and achievability) can be extended to \(T\) frames using the sequential analysis that was presented. Thus, without loss of optimality, one can restrict to the jointly Gaussian distributions and identity functions \(\eta_{1}(.)\) and \(\eta_{2}(.,.)\) in iRDP region \(\mathcal{C}_{\text{RDP}}\). 

For a given rate \(\mathsf{R}\), the following corollary provides the optimization programs which lead to the characterization of the DP tradeoff \(\mathcal{DP}(\mathsf{R})\) for the Gauss-Markov source model.

**Corollary 1**: _For a given rate tuple \(\mathsf{R}\) and \(T=2\) frames, the optimal reconstructions of the DP-tradeoff \(\mathcal{DP}(\mathsf{R})\) can be written as follows_

\[\hat{X}_{1}^{G} =\nu X_{1}+Z_{1},\] (186) \[\hat{X}_{2}^{G} =\omega_{1}\hat{X}_{1}^{G}+\omega_{2}X_{2}+Z_{2},\] (187)

_where \(Z_{1}\) (resp \(Z_{2}\)) is a Gaussian random variable independent of \(X_{1}\) (resp \((\hat{X}_{1}^{G},X_{2})\)) and \(\hat{X}_{j}^{G}\sim\mathcal{N}(0,\hat{\sigma}_{j}^{2})\) for \(j=1,2\), and \(\nu,\omega_{1},\omega_{2},\hat{\sigma}_{1}^{2},\hat{\sigma}_{2}^{2}\) are the solutions of the following optimization program for the first step,_

\[\min_{\nu,\hat{\sigma}_{1}^{2}} \sigma_{1}^{2}+\hat{\sigma}_{1}^{2}-2\nu\sigma_{1}^{2},\] (188a) \[\text{s.t.} \nu^{2}\sigma_{1}^{2}\leq\hat{\sigma}_{1}^{2}(1-2^{-2R_{1}}),\] (188b) \[(\sigma_{1}-\hat{\sigma}_{1})^{2}\leq P_{1},\] (188c)

_and the following minimization problem for the second step and PLF-FMD,_

\[\min_{\omega_{1},\omega_{2},\hat{\sigma}_{2}^{2}} \sigma_{2}^{2}+\hat{\sigma}_{2}^{2}-2\nu\omega_{1}\rho_{1}\sigma_{1} \sigma_{2}-2\omega_{2}\sigma_{2}^{2},\] (189a) \[\text{s.t.} \omega_{2}^{2}\sigma_{2}^{2}(1-2^{-2R_{2}}\frac{\nu^{2}\rho_{1 }^{2}\sigma_{1}^{2}}{\hat{\sigma}_{1}^{2}})\leq(\hat{\sigma}_{2}^{2}-\omega_{ 1}^{2}\hat{\sigma}_{1}^{2}-2\omega_{1}\omega_{2}\nu\rho_{1}\sigma_{1}\sigma_{2 })(1-2^{-2R_{2}}),\] (189b) \[(\sigma_{2}-\hat{\sigma}_{2})^{2}\leq P_{2},\] (189c)

_or the following minimization problem for the second step and PLF-JD,_

\[\min_{\omega_{1},\omega_{2},\hat{\sigma}_{2}^{2}} \sigma_{2}^{2}+\hat{\sigma}_{2}^{2}-2\nu\omega_{1}\rho_{1}\sigma_{1} \sigma_{2}-2\omega_{2}\sigma_{2}^{2}\] (190a) \[\text{s.t.} \omega_{2}^{2}\sigma_{2}^{2}(1-2^{-2R_{2}}\frac{\nu^{2}\rho_{1} ^{2}\sigma_{1}^{2}}{\hat{\sigma}_{1}^{2}})\leq(\hat{\sigma}_{2}^{2}-\omega_{ 1}^{2}\hat{\sigma}_{1}^{2}-2\omega_{1}\omega_{2}\nu\rho_{1}\sigma_{1}\sigma_{2 })(1-2^{-2R_{2}}),\] (190b) \[\text{tr}(\Sigma_{12}+\hat{\Sigma}_{12}-2(\Sigma_{12}^{1/2}\hat {\Sigma}_{12}\Sigma_{12}^{1/2})^{1/2})\leq P_{2},\] (190c)

_where \(\text{tr}(.)\) denotes the trace of a matrix and_

\[\Sigma_{12} :=\begin{pmatrix}\sigma_{1}^{2}&\rho_{1}\sigma_{1}\sigma_{2}\\ \rho_{1}\sigma_{1}\sigma_{2}&\sigma_{2}^{2}\end{pmatrix},\] (191) \[\hat{\Sigma}_{12} :=\begin{pmatrix}\hat{\sigma}_{1}^{2}&\omega_{1}\hat{\sigma}_{1}^ {2}+\nu\omega_{2}\rho_{1}\sigma_{1}\sigma_{2}\\ \omega_{1}\hat{\sigma}_{1}^{2}+\nu\omega_{2}\rho_{1}\sigma_{1}\sigma_{2}&\hat{ \sigma}_{2}^{2}\end{pmatrix}.\] (192)_Proof:_ We obtain the optimization programs for \(T=2\) frames as follows.

For a given rate tuple \(\mathsf{R}\), the DP-tradeoff \(\mathcal{DP}(\mathsf{R})\) is given by the set of all tuples \((\mathsf{D},\mathsf{P})\) such that there exists \(\hat{\mathsf{X}}^{G}\) satisfying the following Markov chains

\[\hat{X}_{1}^{G} \to X_{1}\to X_{2},\] (193) \[\hat{X}_{2}^{G} \to(\hat{X}_{1}^{G},X_{2})\to X_{1},\] (194)

and the following conditions,

\[R_{1} \geq I(X_{1};\hat{X}_{1}^{G}),\] (195) \[R_{2} \geq I(X_{2};\hat{X}_{2}^{G}|\hat{X}_{1}^{G}),\] (196)

and

\[D_{j} \geq\mathbb{E}[\|X_{j}-\hat{X}_{j}^{G}\|^{2}],\qquad j=1,2,\] (197) \[P_{j} \geq W_{2}^{2}(P_{X_{1}\ldots X_{j}},P_{\hat{X}_{1}^{G}\ldots \hat{X}_{j}^{G}}).\] (198)

In general, the set of reconstructions that satisfy (193)-(194) can be written as follows

\[\hat{X}_{1}^{G} =\nu X_{1}+Z_{1},\] (199) \[\hat{X}_{2}^{G} =\omega_{1}\hat{X}_{1}^{G}+\omega_{2}X_{2}+Z_{2}.\] (200)

Plugging the above into (195) and (196) yields the following rate expressions

\[\frac{1}{2}\log\frac{\hat{\sigma}_{1}^{2}}{\hat{\sigma}_{1}^{2}- \nu^{2}\sigma_{1}^{2}} \leq R_{1},\] (201) \[\frac{1}{2}\log\frac{\hat{\sigma}_{2}^{2}-(\omega_{1}\hat{\sigma }_{1}+\frac{\omega_{2}\nu\rho_{1}\sigma_{1}\sigma_{2}}{\hat{\sigma}_{1}})^{2} }{\hat{\sigma}_{2}^{2}-\omega_{1}^{2}\hat{\sigma}_{1}^{2}-\omega_{2}^{2} \sigma_{2}^{2}-2\omega_{1}\omega_{2}\nu\rho_{1}\sigma_{1}\sigma_{2}} \leq R_{2}.\] (202)

Re-arranging the terms in the above constraints yields the conditions in (188b) and (190b). Considering (197) with (199)-(200) gives the following expressions for distortions

\[\mathbb{E}[\|X_{1}-\hat{X}_{1}^{G}\|^{2}] =\sigma_{1}^{2}+\hat{\sigma}_{1}^{2}-2\mathbb{E}[X_{1}\hat{X}_{ 1}^{G}]=\sigma_{1}^{2}+\hat{\sigma}_{1}^{2}-2\nu\sigma_{1}^{2},\] (203) \[\mathbb{E}[\|X_{2}-\hat{X}_{2}^{G}\|^{2}] =\sigma_{2}^{2}+\hat{\sigma}_{2}^{2}-2\mathbb{E}[X_{2}\hat{X}_{ 2}^{G}]=\sigma_{2}^{2}+\hat{\sigma}_{2}^{2}-2\omega_{1}\nu\rho_{1}\sigma_{1} \sigma_{2}-2\omega_{2}\sigma_{2}^{2},\] (204)

which are the objective functions in (188a) and (190a). Now, we evaluate the perception constraint. Notice that the covariance matrices of \((X_{1},X_{2})\) and \((\hat{X}_{1}^{G},\hat{X}_{2}^{G})\) are given by \(\Sigma_{12}\) and \(\hat{\Sigma}_{12}\) defined in (191) and (192), respectively. The Wasserstein-2 distance between two Gaussian distributions with covariance matrices \(\Sigma_{12}\) and \(\hat{\Sigma}_{12}\) is given in (190c) as discussed in [26, pp. 18].

Similarly, the expressions in (189) for the decoder based on PLF-FMD can be obtained.

## Appendix F Gauss-Markov Source Model: Extremal Rates

In this section, we derive the achievable reconstructions for some special cases. We assume that we have only two frames, i.e., \(D_{3},P_{3}\to\infty\). Moreover, let \(\sigma_{1}^{2}=\sigma_{2}^{2}:=\sigma^{2}\) for simplicity. In general, the reconstructions can be written as follows

\[\hat{X}_{1}^{G} =\nu X_{1}+Z_{1},\] (205) \[\hat{X}_{2}^{G} =\omega_{1}\hat{X}_{1}^{G}+\omega_{2}X_{2}+Z_{2},\] (206)

where \(\hat{X}_{j}^{G}\sim\mathcal{N}(0,\hat{\sigma}_{j}^{2})\) for \(j=1,2\). Recall the optimization program of the first step in (188) as follows

\[\min_{\nu,\hat{\sigma}_{1}^{2}}\,\sigma^{2}+\hat{\sigma}_{1}^{2}- 2\nu\sigma^{2},\] (207a) s.t. \[\nu^{2}\sigma^{2}\leq\hat{\sigma}_{1}^{2}(1-2^{-2R_{1}}),\] (207b) \[(\sigma-\hat{\sigma}_{1})^{2}\leq P_{1},\] (207c)For a given \(\hat{\sigma}_{1}^{2}\), the objective function in (207a) is a monotonically deacreasing function of \(\nu\), hence one can restrict \(\nu\) to be nonnegative, without loss of optimality. So, the above optimization program can be written as

\[\min_{\nu,\hat{\sigma}_{1}^{2}}\ \sigma^{2}+\hat{\sigma}_{1}^{2}-2\nu \sigma^{2},\] (208a) s.t. \[0\leq\nu\leq\frac{\hat{\sigma}_{1}}{\sigma}\sqrt{1-2^{-2R_{1}}},\] (208b) \[(\sigma-\hat{\sigma}_{1})^{2}\leq P_{1},\] (208c)

Optimizing with respect to \(\nu\) in the above program, we have

\[\nu=\frac{\hat{\sigma}_{1}}{\sigma}\sqrt{1-2^{-2R_{1}}},\] (209)

where the optimization program reduces to

\[\min_{\hat{\sigma}_{1}^{2}}\ \sigma^{2}+\hat{\sigma}_{1}^{2}-2 \sigma\hat{\sigma}_{1}\sqrt{1-2^{-2R_{1}}},\] (210a) s.t. \[(\sigma-\hat{\sigma}_{1})^{2}\leq P_{1}.\] (210b)

Next, recall the optimization program of the second step for PLF-FMD in (189) as follows

\[\min_{\omega_{1},\omega_{2},\hat{\sigma}_{2}^{2}}\ \sigma^{2}+\hat{ \sigma}_{2}^{2}-2\nu\omega_{1}\rho_{1}\sigma^{2}-2\omega_{2}\sigma^{2},\] (211a) s.t. \[\omega_{2}^{2}\sigma^{2}(1-2^{-2R_{2}}\frac{\nu^{2}\rho_{1}^{2} \sigma^{2}}{\hat{\sigma}_{1}^{2}})\leq(\hat{\sigma}_{2}^{2}-\omega_{1}^{2} \hat{\sigma}_{1}^{2}-2\omega_{1}\omega_{2}\nu\rho_{1}\sigma^{2})(1-2^{-2R_{2} }),\] (211b) \[(\sigma-\hat{\sigma}_{2})^{2}\leq P_{2},\] (211c)

Plugging (209) into the above program, we get

\[\min_{\omega_{1},\omega_{2},\hat{\sigma}_{2}^{2}}\ \sigma^{2}+ \hat{\sigma}_{2}^{2}-2\omega_{1}\rho_{1}\hat{\sigma}_{1}\sigma\sqrt{1-2^{-2R_{ 1}}}-2\omega_{2}\sigma^{2},\] (212a) s.t. \[\omega_{2}^{2}\sigma^{2}(1-\rho_{1}^{2}2^{-2R_{2}}(1-2^{-2R_{1} }))\leq(\hat{\sigma}_{2}^{2}-\omega_{1}^{2}\hat{\sigma}_{1}^{2}-2\omega_{1} \omega_{2}\rho_{1}\hat{\sigma}_{1}\sigma\sqrt{1-2^{-2R_{1}}})(1-2^{-2R_{2}}),\] (212b) \[(\sigma-\hat{\sigma}_{2})^{2}\leq P_{2},\] (212c)

The optimization program for the second step of PLF-JD is similar to the above program (212) when (212c) is replaced by (190c). In this section, we study different rate regimes and obtain the solutions of the above optimization programs. In particular, we are interested in two perception thresholds \(P_{2}\rightarrow\infty\) and \(P_{2}=0\) where the former corresponds to the classical rate-distortion region and the latter is the case of \(0\)-PLF. For the \(0\)-PLF-FMD, we have \(\hat{\sigma}_{1}=\hat{\sigma}_{2}=\sigma\). For the \(0\)-PLF-JD, in addition to preserving the marginals, the correlation \(\mathbbm{E}[\hat{X}_{1}^{G}\hat{X}_{2}^{G}]=\rho_{1}\sigma^{2}\) should be satisfied. For each of these cases, the optimization program in (212) is simplified in the following.

_Optimization Program of the Second Step for \(P\rightarrow\infty\)_: In this case, there is no perception constraint in the setting and the optimization program in (212) reduces to the following

\[\min_{\hat{\sigma}_{2}^{2},\omega_{1},\omega_{2}}\sigma^{2}+ \hat{\sigma}_{2}^{2}-2\omega_{1}\rho_{1}\hat{\sigma}_{1}\sigma\sqrt{1-2^{-2R_ {1}}}-2\omega_{2}\sigma^{2},\] (213a) s.t. \[\omega_{2}^{2}\sigma^{2}(1-\rho_{1}^{2}2^{-2R_{2}}(1-2^{-2R_{1} }))\leq(\hat{\sigma}_{2}^{2}-\omega_{1}^{2}\hat{\sigma}_{1}^{2}-2\omega_{1} \omega_{2}\rho_{1}\hat{\sigma}_{1}\sigma\sqrt{1-2^{-2R_{1}}})(1-2^{-2R_{2}}).\] (213b)

This case corresponds to the classical rate-distortion tradeoff where it is shown that for a given rate, the MMSE reconstructions are indeed optimal [28, 37]. The expressions for MMSE reconstructions are given in Appendix H.1.

_Optimization Program of the Second Step for \(0\)-PLF-FMD_: In this case, we have \(\hat{\sigma}_{1}=\hat{\sigma}_{2}=\sigma\). So, the optimization program in (212) reduces to the following

\[\min_{\omega_{1},\omega_{2}}\ \ 2\sigma^{2}-2\omega_{1}\rho_{1} \sigma^{2}\sqrt{1-2^{-2R_{1}}}-2\omega_{2}\sigma^{2},\] (214a) s.t \[\omega_{2}^{2}(1-\rho_{1}^{2}2^{-2R_{2}}(1-2^{-2R_{1}}))\leq(1- \omega_{1}^{2}-2\omega_{1}\omega_{2}\rho_{1}\sqrt{1-2^{-2R_{1}}})(1-2^{-2R_{2} }).\] (214b)Here, \(\omega_{1}\) and \(\omega_{2}\) only need to satisfy the rate constraint given in (214b) which represents a larger search space than that of \(0\)-PLF-JD which will be discussed in the following.

_Optimization Program of the Second Step for \(0\)-PLF-JD_: In this case, in addition to preserving marginals \(\hat{\sigma}_{1}=\hat{\sigma}_{2}=\sigma\), we need to satisfy the constraint \(\mathbb{E}[\hat{X}_{1}^{G}\hat{X}_{2}^{G}]=\rho_{1}\sigma^{2}\). Thus, the optimization program of this case has an extra condition \(\omega_{1}+\nu\omega_{2}\rho_{1}=\rho_{1}\) comparing to (214) and it is given as follows

\[\min_{\omega_{1},\omega_{2}} 2\sigma^{2}-2\omega_{1}\rho_{1}\sigma^{2}\sqrt{1-2^{-2R_{1}}}-2 \omega_{2}\sigma^{2},\] (215a) \[\text{s.t.} \omega_{2}^{2}(1-\rho_{1}^{2}2^{-2R_{2}}(1-2^{-2R_{1}}))\leq(1- \omega_{1}^{2}-2\omega_{1}\omega_{2}\rho_{1}\sqrt{1-2^{-2R_{1}}})(1-2^{-2R_{2 }}),\] (215b) \[\omega_{1}+\nu\omega_{2}\rho_{1}=\rho_{1}.\]

Comparing (215) with (214), we notice that the search space of the optimization program for \(0\)-PLF-JD is smaller than that of \(0\)-PLF-FMD. Thus, a larger distortion is expected for \(0\)-PLF-JD.

Before studying each case of extremal rates, we introduce another constraint in the optimization program of all above three cases of perception metrics. We restrict to nonnegative \(\omega_{1}\omega_{2}\rho_{1}\) and get an upper bound on the programs (213), (214) and (215). So, in further discussion on these programs, the constraint \(\omega_{1}\omega_{2}\rho_{1}\geq 0\) will be also considered.

_1) \(R_{1}=R_{2}=\epsilon\) for small \(\epsilon\)_:

In the low-rate regime, notice that we can approximate the rate term as follows

\[1-2^{-2\epsilon}=2\epsilon\ln 2+O(\epsilon^{2}).\] (216)

Plugging the above into (209), we have

\[\nu=\frac{\hat{\sigma}_{1}}{\sigma}\sqrt{2\epsilon\ln 2+O(\epsilon^{2})}.\] (217)

Also, inserting (216) into the rate constraint of the second step (211c) yields the following

\[\omega_{2}^{2}\sigma^{2}(1-\rho_{1}^{2}2\epsilon\ln 2+O(\epsilon^{2}))\leq( \hat{\sigma}_{2}^{2}-\omega_{1}^{2}\hat{\sigma}_{1}^{2}-2\omega_{1}\omega_{2} \rho_{1}\hat{\sigma}_{1}\sigma\sqrt{2\epsilon\ln 2+O(\epsilon^{2})})(2 \epsilon\ln 2+O(\epsilon^{2})).\lx@note{footnote}{The inequalities of the form $f(\epsilon)+O(\epsilon^{2})\leq g(\epsilon)+O(\epsilon^{2})$, where $f(\epsilon),g(\epsilon)=\Omega(\epsilon^{2})$, imply that $f(\epsilon)\leq g(\epsilon)$. So, in such inequalities, we work with dominant terms $(f(\epsilon),g(\epsilon))$ and ignore the small terms $O(\epsilon^{2})$. A similar argument holds if we have other orders of $\epsilon$ and the functions $f(.),g(.)$ approach zero slower than them.}\] (218)

Re-arranging the terms in the above inequality yields the following

\[\hat{\sigma}_{2}^{2} \geq\frac{\omega_{2}^{2}\sigma^{2}(1-\rho_{1}^{2}2\epsilon\ln 2+O( \epsilon^{2}))}{2\epsilon\ln 2+O(\epsilon^{2})}+\omega_{1}^{2}\hat{ \sigma}_{1}^{2}+2\omega_{1}\omega_{2}\rho_{1}\hat{\sigma}_{1}\sigma\sqrt{2 \epsilon\ln 2+O(\epsilon^{2})}\] (219) \[=\omega_{2}^{2}\sigma^{2}\left(\frac{1}{2\epsilon\ln 2}+O\left( 1\right)\right)+\omega_{1}^{2}\hat{\sigma}_{1}^{2}+2\omega_{1}\omega_{2}\rho_ {1}\hat{\sigma}_{1}\sigma\sqrt{2\epsilon\ln 2+O(\epsilon^{2})}\] (220)

So, in all of the optimization programs of the case \(R_{1}=R_{2}=\epsilon\), the above constraint (220) will replace the rate constraint of the second step.

Now, we consider different cases based on the perception measure.

_a) Without a perception constraint_: In this case, using (216), the optimization program of the first step in (210) simplifies to the following

\[D_{1}=\min_{\hat{\sigma}_{1}^{2}}\ \sigma^{2}+\hat{\sigma}_{1}^{2}-2\sigma \hat{\sigma}_{1}\sqrt{2\epsilon\ln 2+O(\epsilon^{2})},\] (221)

which gives us the following optimal solution

\[\hat{\sigma}_{1}=\sqrt{2\epsilon\ln 2+O(\epsilon^{2})}\sigma=\sqrt{2\epsilon\ln 2 }\sigma+O(\epsilon).\] (222)

Plugging the above solution into (217) and (221), we get

\[\nu=2\epsilon\ln 2+O(\epsilon^{2}),\] (223)\[D_{1}=(1-2\epsilon\ln 2)\sigma^{2}+O(\epsilon^{2}).\] (224)

Now, we look at the optimization program of the second step (213). For a given \(\omega_{1}\) and \(\omega_{2}\), the objective function is an increasing function of \(\hat{\sigma}_{2}^{2}\), so optimizing over \(\hat{\sigma}_{2}^{2}\) yields the following

\[\hat{\sigma}_{2}^{2}=\omega_{2}^{2}\sigma^{2}\left(\frac{1}{2 \epsilon\ln 2}+O\left(1\right)\right)+\omega_{1}^{2}\hat{\sigma}_{1}^{2}+2 \omega_{1}\omega_{2}\rho_{1}\hat{\sigma}_{1}\sigma\sqrt{2\epsilon\ln 2+O( \epsilon^{2})}.\] (225)

Thus, the optimization program (213) is further upper bounded by the following

\[\min_{\begin{subarray}{c}\hat{\sigma}_{2}^{2},\omega_{1},\omega_{ 2}:\\ \omega_{1}\omega_{2}\rho_{1}\geq 0\end{subarray}}\sigma^{2}+\omega_{2}^{2} \sigma^{2}\left(\frac{1}{2\epsilon\ln 2}+O\left(1\right)\right)+\omega_{1}^{2} \hat{\sigma}_{1}^{2}-2(1-\omega_{2})\omega_{1}\rho_{1}\hat{\sigma}_{1}\sigma \sqrt{2\epsilon\ln 2+O(\epsilon^{2})}-2\omega_{2}\sigma^{2}.\] (226)

The optimal solution of the above minimization is given by the following

\[\omega_{1} =\rho_{1}+O(\epsilon),\] (227) \[\omega_{2} =2\epsilon\ln 2+O(\epsilon^{2}).\] (228)

Thus, considering the dominant terms of (223), (227) and (228), we have

\[\hat{X}_{1}^{G} =(2\epsilon\ln 2)X_{1}+Z_{1},\] (229) \[\hat{X}_{2}^{G} =\rho_{1}\hat{X}_{1}^{G}+(2\epsilon\ln 2)X_{2}+Z_{2},\] (230)

and \(Z_{j}\sim\mathcal{N}(0,2\epsilon\sigma^{2}\ln 2)\) for \(j=1,2\). Notice that

\[D_{1} =(1-2\epsilon\ln 2)\sigma^{2},\] (231) \[D_{2} =(1-(1+\rho_{1}^{2})2\epsilon\ln 2)\sigma^{2}.\] (232)

_b) \(0\)-PLF-FMD_: In this case, we have \(\hat{\sigma}_{1}=\hat{\sigma}_{2}=\sigma\). For the optimization program of the first step, (209) reduces to the following

\[\nu=\sqrt{2\epsilon\ln 2}+O(\epsilon),\] (233)

and \(D_{1}\) is given in the following which is derived by (210)

\[D_{1}=2(1-\sqrt{2\epsilon\ln 2})\sigma^{2}+O(\epsilon).\] (234)

Now, we study the optimization program of the second step. The optimization program of (214) is further upper bounded by the following

\[\min_{\begin{subarray}{c}\omega_{1},\omega_{2}:\\ \omega_{1}\omega_{2}\rho_{1}\geq 0\end{subarray}}2\sigma^{2}-2\omega_{1} \rho_{1}\sigma^{2}\sqrt{2\epsilon\ln 2+O(\epsilon^{2})}-2\omega_{2}\sigma^{2},\] (235a) s.t. \[1\geq\sqrt{\omega_{2}^{2}\left(\frac{1}{2\epsilon\ln 2}+O \left(1\right)\right)+\omega_{1}^{2}+2\omega_{1}\omega_{2}\rho_{1}\sqrt{2 \epsilon\ln 2+O(\epsilon^{2})}}.\] (235b)

Now, we further simplify the inequality (235b) in the following. Considering the fact that \(\omega_{1}\omega_{2}\rho_{1}\geq 0\), this inequality implies that

\[\omega_{1}^{2} \leq 1,\] (236) \[\omega_{2}^{2} \leq 2\epsilon\ln 2+O(\epsilon^{2}).\] (237)

So, using the above inequalities, the RHS of (235b) can be upper bounded as follows

\[\sqrt{\omega_{2}^{2}\left(\frac{1}{2\epsilon\ln 2}+O\left(1 \right)\right)+\omega_{1}^{2}+2\omega_{1}\omega_{2}\rho_{1}\sqrt{2\epsilon\ln 2 +O(\epsilon^{2})}}\] \[\leq\sqrt{\omega_{2}^{2}\left(\frac{1}{2\epsilon\ln 2}+O \left(1\right)\right)+\omega_{1}^{2}+(\omega_{1}^{2}+\omega_{2}^{2})\rho_{1} \sqrt{2\epsilon\ln 2+O(\epsilon^{2})}}\] \[\leq\sqrt{\omega_{2}^{2}\left(\frac{1}{2\epsilon\ln 2}+O \left(1\right)\right)+\omega_{1}^{2}+O(\epsilon^{3/2})}.\] (238)Now, according to (238), the optimization program in (235) is further upper bounded by the following

\[\min_{\begin{subarray}{c}\omega_{1}\geq\omega_{2}:\\ \omega_{1}\geq\omega_{2}\rho_{1}\geq 0\end{subarray}} 2\sigma^{2}-2\omega_{1}\rho_{1}\sigma^{2}\sqrt{2\epsilon\ln 2+O( \epsilon^{2})}-2\omega_{2}\sigma^{2},\] (239a) s.t. \[1\geq\sqrt{\omega_{2}^{2}\left(\frac{1}{2\epsilon\ln 2}+O\left(1 \right)\right)+\omega_{1}^{2}+O(\epsilon^{3/2})}.\] (239b)

For a given \(\omega_{1}\) (resp \(\omega_{2}\)), the objective function (239a) is a monotonically decreasing function of \(\omega_{2}\) (resp \(\omega_{1}\)), so the optimal solution is attained on the boundary, i.e.,

\[1=\sqrt{\omega_{2}^{2}\left(\frac{1}{2\epsilon\ln 2}+O\left(1 \right)\right)+\omega_{1}^{2}+O(\epsilon^{3/2})}\] (240)

Thus, the program (239) further simplifies to the following

\[\min_{\begin{subarray}{c}\omega_{1}\geq\\ \omega_{1}\rho_{1}\geq 0\end{subarray}} 2\sigma^{2}-2\omega_{1}\rho_{1}\sigma^{2}\sqrt{2\epsilon\ln 2+O( \epsilon^{2})}-2\sigma^{2}\sqrt{(1-\omega_{1}^{2}-O(\epsilon^{3/2}))(2 \epsilon\ln 2+O(\epsilon^{2}))}.\] (241)

The optimal solution of the above program is given by

\[\omega_{1}=\frac{\rho_{1}}{\sqrt{1+\rho_{1}^{2}}}+O\left(\epsilon \right),\] (242)

which together with (240) yields

\[\omega_{2}=\sqrt{\frac{2\epsilon\ln 2}{1+\rho_{1}^{2}}}+O( \epsilon).\] (243)

Thus, considering dominant terms of (233), (242) and (243), we get

\[\hat{X}_{1}^{G}=\sqrt{2\epsilon\ln 2}X_{1}+Z_{1},\] (244) \[\hat{X}_{2}^{G}=\frac{\rho_{1}}{\sqrt{1+\rho_{1}^{2}}}\hat{X}_{1 }^{G}+\sqrt{\frac{2\epsilon\ln 2}{1+\rho_{1}^{2}}}X_{2}+Z_{2},\] (245)

where \(Z_{1}\sim\mathcal{N}(0,(1-2\epsilon\ln 2)\sigma^{2})\) and

\[Z_{2}\sim\mathcal{N}(0,(1-\frac{\rho_{1}^{2}}{1+\rho_{1}^{2}}- \frac{1+2\rho_{1}^{2}}{1+\rho_{1}^{2}}2\epsilon\ln 2)\sigma^{2}).\] (246)

Notice that

\[D_{1}=2(1-\sqrt{2\epsilon\ln 2})\sigma^{2},\] (247) \[D_{2}=2(1-\sqrt{(1+\rho_{1}^{2})2\epsilon\ln 2})\sigma^{2}.\] (248)

For the special case of \(\rho_{1}=1\), the expressions in (244) and (245) simplify as follows

\[\hat{X}_{1}^{G}=\sqrt{2\epsilon\ln 2}X_{1}+Z_{1},\] (249) \[\hat{X}_{2}^{G}=\sqrt{2}\sqrt{2\epsilon\ln 2}X_{1}+\frac{1}{ \sqrt{2}}Z_{1}+Z_{2}.\] (250)

Define \(Z_{\text{FMD}}:=\frac{1}{\sqrt{2}}Z_{1}+Z_{2}\) and notice that \(Z_{\text{FMD}}\sim\mathcal{N}(0,(1-4\epsilon\ln 2)\sigma^{2})\). Moreover, we have

\[D_{1}=2(1-\sqrt{2\epsilon\ln 2})\sigma^{2},\] (251) \[D_{2}=2(1-\sqrt{4\epsilon\ln 2})\sigma^{2}.\] (252)

_c) \(0\)-PLF-JD_: In this case, the optimization program of the first step is similar to the previous case. The optimization program of the second step is given in (215) where the condition \(\omega_{1}+\nu\omega_{2}\rho_{1}=\rho_{1}\) is introduced. According to (233), \(\nu=O(\sqrt{\epsilon})\) which suggests the following form for \(\omega_{1}\),

\[\omega_{1}=\rho_{1}-\delta_{\epsilon},\] (253)for some small \(\delta_{\epsilon}\) that goes to zero as \(\epsilon\to 0\). The parameter \(\delta_{\epsilon}\) will be determined later. Plugging \(\omega_{1}=\rho_{1}-\delta_{\epsilon}\) into (240), we find out that only the constant term of \(\omega_{1}\) contributes to a dominant term for \(\omega_{2}\) which yields the following

\[\omega_{2}=\sqrt{2\epsilon\ln 2(1-\rho_{1}^{2})}+O(\epsilon).\] (254)

Thus, we have

\[\hat{X}_{1}^{G} =\sqrt{2\epsilon\ln 2}X_{1}+Z_{1},\] (255) \[\hat{X}_{2}^{G} =(\rho_{1}-\delta_{\epsilon})\hat{X}_{1}^{G}+\sqrt{(1-\rho_{1}^{ 2})2\epsilon\ln 2}X_{2}+Z_{2},\] (256)

Now, applying the constraint \(\mathbb{E}[\hat{X}_{1}^{G}\hat{X}_{2}^{G}]=\rho_{1}\sigma^{2}\), we get

\[\delta_{\epsilon}=\rho_{1}\sqrt{1-\rho_{1}^{2}}(2\epsilon\ln 2).\] (257)

However, notice that since \(\delta_{\epsilon}=O(\epsilon)\), it does not contribute to dominant terms of distortion. So, we can simply represent \(\hat{X}_{1}^{G}\) and \(\hat{X}_{2}^{G}\) as follows

\[\hat{X}_{1}^{G} =\sqrt{2\epsilon\ln 2}X_{1}+Z_{1},\] (258) \[\hat{X}_{2}^{G} =\rho_{1}\hat{X}_{1}^{G}+\sqrt{(1-\rho_{1}^{2})2\epsilon\ln 2}X_{ 2}+Z_{2},\] (259)

where \(Z_{1}\sim\mathcal{N}(0,(1-2\epsilon\ln 2)\sigma^{2})\) and \(Z_{2}\sim\mathcal{N}(0,(1-\rho_{1}^{2}-(1-\rho_{1}^{2}+2\rho_{1}^{2}\sqrt{1- \rho_{1}^{2}})2\epsilon\ln 2)\sigma^{2})\). The following distortions are also achievable

\[D_{1} =2(1-\sqrt{2\epsilon\ln 2})\sigma^{2},\] (260) \[D_{2} =2(1-(\rho_{1}^{2}+\sqrt{1-\rho_{1}^{2}})\sqrt{2\epsilon\ln 2} )\sigma^{2}.\] (261)

For the special case of \(\rho=1\), according to (259) and (261), we have \(\hat{X}_{2}^{G}=\hat{X}_{1}^{G}\) and \(D_{2}=D_{1}\).

_2) \(R_{1}\to\infty\), \(R_{2}=\epsilon\) for small \(\epsilon\)_: In this case, since \(R_{1}\to\infty\), we have \(\hat{X}_{1}^{G}=X_{1}\), \(D_{1}=0\), and we only need to solve the optimization program of the second step. Also, we have the following approximation

\[1-2^{-2R_{2}}=1-2^{-2\epsilon}=2\epsilon\ln 2+O(\epsilon^{2}).\] (262)

We consider three different cases based on the perception constraint.

_a) Without a perception constraint_: In this case, consider the optimization program (213). For a given \(\omega_{1}\) and \(\omega_{2}\), the objective function is an increasing function of \(\hat{\sigma}_{2}^{2}\), hence optimizing over \(\hat{\sigma}_{2}^{2}\), we get

\[\hat{\sigma}_{2}^{2}=\frac{\omega_{2}^{2}\sigma^{2}(1-\rho_{1}^{2}+O(\epsilon) )}{2\epsilon\ln 2+O(\epsilon^{2})}+\omega_{1}^{2}\sigma^{2}+2\omega_{1} \omega_{2}\rho_{1}\sigma^{2}.\] (263)

The program in (213) is further upper bounded by the following

\[\min_{\begin{subarray}{c}\omega_{1},\omega_{2}:\\ \omega_{1}\omega_{2}\rho_{1}\geq 0\end{subarray}} \sigma^{2}+\frac{\omega_{2}^{2}\sigma^{2}(1-\rho_{1}^{2}+O( \epsilon))}{2\epsilon\ln 2+O(\epsilon^{2})}+\omega_{1}^{2}\sigma^{2}+2 \omega_{1}\omega_{2}\rho_{1}\sigma^{2}-2\omega_{1}\rho_{1}\sigma^{2}-2\omega_{ 2}\sigma^{2},\]

The solution of the above optimization program is given by the following

\[\omega_{1} =\rho_{1}-\rho_{1}(2\epsilon\ln 2),\] (265) \[\omega_{2} =2\epsilon\ln 2.\] (266)

Thus, we have

\[\hat{X}_{1}^{G} =X_{1},\] (267) \[\hat{X}_{2}^{G} =(\rho_{1}-\rho_{1}(2\epsilon\ln 2))X_{1}+(2\epsilon\ln 2)X_{2}+Z_{2},\] (268)

where \(Z_{2}\sim\mathcal{N}(0,(1-\rho_{1}^{2})\sigma^{2}2\epsilon\ln 2)\). So, the reconstruction of the second frame closely resembles the first frame. The distortions of the first and second frames are zero and \((1-\rho_{1}^{2}-(1-\rho_{1}^{2})2\epsilon\ln 2)\sigma^{2}\), respectively.

_b) \(0\)-PLF-FMD_: In this case, \(\hat{\sigma}_{1}=\hat{\sigma}_{2}=\sigma\). Thus, the optimization program in (214) is further upper bounded by the following

\[\min_{\begin{subarray}{c}\omega_{1},\omega_{2}:\\ \omega_{1}\omega_{2}\rho_{1}\geq 0\end{subarray}} 2\sigma^{2}-2\omega_{1}\rho_{1}\sigma^{2}-2\omega_{2}\sigma^{2},\] (269a) \[\text{s.t.} \omega_{2}^{2}(1-\rho_{1}^{2}+O(\epsilon))\leq(1-\omega_{1}^{2}-2 \omega_{1}\omega_{2}\rho_{1})(2\epsilon\ln 2+O(\epsilon^{2})).\] (269b)

For a given \(\omega_{1}\) (resp \(\omega_{2}\)), the objective function (269a) is a monotonically decreasing function of \(\omega_{2}\) (resp \(\omega_{1}\)). So, the optimal solution is attained on the boundary, i.e., (269b) is satisfied with equality given as follows

\[\omega_{2}^{2}(1-\rho_{1}^{2}+O(\epsilon))=(1-\omega_{1}^{2}-2\omega_{1}\omega _{2}\rho_{1})(2\epsilon\ln 2+O(\epsilon^{2})).\] (270)

It can be easily verified that the first-order terms of \(\omega_{1}\) and \(\omega_{2}\) which optimize the program are \(1\) and \(0\), respectively. So, we write \(\omega_{1}\) and \(\omega_{2}\) in the following form

\[\omega_{1} =1+(2\epsilon\ln 2)\delta_{1}+O(\epsilon^{2}),\] (271) \[\omega_{2} =(2\epsilon\ln 2)\delta_{2}+O(\epsilon^{2}),\] (272)

for some real \(\delta_{1}\) and \(\delta_{2}\). Plugging the above (271) and (272) into (270) and considering the dominant terms, we get

\[\delta_{2}^{2}(1-\rho_{1}^{2})=-2\delta_{1}-2\rho_{1}\delta_{2}.\] (273)

On the other side, we can write the objective function in (269) as follows

\[2\sigma^{2} -2\omega_{1}\rho_{1}\sigma^{2}-2\omega_{2}\sigma^{2}\] \[=2\sigma^{2}-2\rho_{1}\omega_{1}\sigma^{2}-2\omega_{2}\sigma^{2}+ O(\epsilon^{2})\] (274) \[=2\sigma^{2}-2\rho_{1}\sigma^{2}-2(\rho_{1}\delta_{1}\sigma^{2}+ \delta_{2}\sigma^{2})(2\epsilon\ln 2)+O(\epsilon^{2})\] (275) \[=2\sigma^{2}-2\rho_{1}\sigma^{2}-(-2\rho_{1}^{2}\delta_{2}\sigma ^{2}-\rho_{1}(1-\rho_{1}^{2})\delta_{2}^{2}+2\delta_{2}\sigma^{2})(2\epsilon \ln 2)+O(\epsilon^{2}).\] (276)

Differentiating the above expression with respect to \(\delta_{2}\) and letting it be zero, we have:

\[\delta_{2}=\frac{1}{\rho_{1}},\qquad\delta_{1}=-\frac{1+\rho_{1}^{2}}{2\rho_{ 1}^{2}}.\] (277)

Thus, we have

\[\hat{X}_{1}^{G} =X_{1},\] (278) \[\hat{X}_{2}^{G} =(1-\frac{(1+\rho_{1}^{2})2\epsilon\ln 2}{2\rho_{1}^{2}})\hat{X}_{1} ^{G}+\frac{2\epsilon\ln 2}{\rho_{1}}X_{2}+Z_{2},\] (279)

where \(Z_{2}\sim\mathcal{N}(0,(\frac{1-\rho_{1}^{2}}{\rho_{1}^{2}})2\epsilon\ln 2)\). Again, the reconstruction of the second frame is almost similar to the first frame and the distortion is \(2(1-\rho_{1}-(\frac{1-\rho_{1}^{2}}{2\rho_{1}})2\epsilon\ln 2)\sigma^{2}\).

_c) \(0\)-PLF-JD_: First consider the case where \(\rho_{1}\neq 1\). The optimization program is given in (215) where the constraint \(\omega_{1}+\nu\rho_{1}\omega_{2}=\rho_{1}\) is introduced. Notice that \(\omega_{1}\) can be written in the following form

\[\omega_{1}=\rho_{1}+\delta_{\epsilon},\] (280)

for some \(\delta_{\epsilon}\) that goes to zero as \(\epsilon\to 0\). The parameter \(\delta_{\epsilon}\) will be determined later. Plugging \(\omega_{1}=\rho_{1}+\delta_{\epsilon}\) into (270) yields the following

\[\omega_{2}=\sqrt{2\epsilon\ln 2}+O(\epsilon),\] (281)

which is derived only through the first-order term of \(\omega_{1}\) which is \(\rho_{1}\). Now, considering the fact that \(\mathbb{E}[\hat{X}_{1}^{G}\hat{X}_{2}^{G}]=\rho_{1}\sigma^{2}\), we obtain

\[\delta_{\epsilon}=-\rho_{1}\sqrt{2\epsilon\ln 2}.\] (282)

Thus, we have

\[\hat{X}_{1}^{G} =X_{1},\] (283) \[\hat{X}_{2}^{G} =(\rho_{1}-\rho_{1}\sqrt{2\epsilon\ln 2})\hat{X}_{1}^{G}+\sqrt{2 \epsilon\ln 2}X_{2}+Z_{2},\] (284)

where \(Z_{2}\sim\mathcal{N}(0,(1-\rho_{1}^{2})\sigma^{2})\). Here, the reconstruction of the second frame closely resembles the first frame. The distortion of the second frame is \(2(1-\rho_{1}^{2}-(1-\rho_{1}^{2})\sqrt{2\epsilon\ln 2})\sigma^{2}\).

If \(\rho_{1}=1\), we simply have \(\hat{X}_{2}^{G}=\hat{X}_{1}^{G}=X_{1}=X_{2}\) which can be derived from (283)-(284) by letting \(X_{1}=X_{2}\).

The analysis for the case of \(R_{1}=\epsilon\) and \(R_{2}\to\infty\) is similar and is omitted for brevity. The results of this section are summarized in Table 2.

[MISSING_PAGE_FAIL:34]

Evaluating the above constraint with the choice of random variables \((\hat{X}_{1}^{G},\hat{X}_{2}^{G},\hat{X}_{3}^{G})\) and re-arranging the terms, we get

\[\tau_{3}^{2}\sigma^{2}(1-2^{-2R_{3}}(\rho^{4}2^{-2R_{1}-2R_{2}}+ \rho^{2}(1-\rho^{2})2^{-2R_{2}}-\rho^{2}))\leq\] \[(1-2^{-2R_{3}})(1-\tau_{1}^{2}-\tau_{2}^{2}-2\tau_{1}\tau_{2} \omega_{1}\nu-2\tau_{1}\tau_{2}\omega_{2}\nu\rho-2\tau_{2}\tau_{3}\omega_{1}\nu \rho^{2}-2\tau_{2}\tau_{3}\omega_{2}\rho-2\tau_{1}\tau_{3}\nu\rho^{2})\sigma^{2}.\] (288)

Similar to (240), considering the dominant terms of the above rate constraint and the fact that the solution of the optimization problem is attained when the above inequality is satisfied with "equality", we get

\[(1-\tau_{1}^{2}-\tau_{2}^{2}+O(\epsilon^{3/2}))(2\epsilon\ln 2+O( \epsilon^{2}))=\tau_{3}^{2}(1+O(\epsilon)).\] (289)

The distortion can be written as follows

\[\mathbbm{E}[\|X_{3}-\hat{X}_{3}^{G}\|^{2}]=2\sigma^{2}-2\tau_{3} \sigma^{2}-2\tau_{2}\omega_{2}\rho\sigma^{2}-2\tau_{2}\omega_{1}\nu\rho^{2} \sigma^{2}-2\tau_{1}\nu\rho^{2}\sigma^{2}.\] (290)

So, the goal is to solve the following optimization problem for the third step

\[\min_{\tau_{1},\tau_{2},\tau_{3}} 2\sigma^{2}-2\tau_{3}\sigma^{2}-2\tau_{2}\omega_{2}\rho\sigma^{2 }-2\tau_{2}\omega_{1}\nu\rho^{2}\sigma^{2}-2\tau_{1}\nu\rho^{2}\sigma^{2}\] (291) s.t. \[(1-\tau_{1}^{2}-\tau_{2}^{2}+O(\epsilon^{3/2}))(2\epsilon\ln 2 +O(\epsilon^{2}))=\tau_{3}^{2}(1+O(\epsilon)).\] (292)

We restrict the search space to \(\tau_{1},\tau_{2},\tau_{3}\geq 0\) and get an upper bound to the above optimization program as follows

\[\min_{\tau_{1},\tau_{2},\tau_{3}\geq 0} 2\sigma^{2}-2\tau_{3}\sigma^{2}-2\tau_{2}\omega_{2}\rho\sigma^{2 }-2\tau_{2}\omega_{1}\nu\rho^{2}\sigma^{2}-2\tau_{1}\nu\rho^{2}\sigma^{2}\] (293) s.t. \[(1-\tau_{1}^{2}-\tau_{2}^{2}+O(\epsilon^{3/2}))(2\epsilon\ln 2 +O(\epsilon^{2}))=\tau_{3}^{2}(1+O(\epsilon)).\] (294)

The above optimization problem is equivalent to the following

\[\min_{\tau_{1},\tau_{2}\geq 0}\Bigg{(}2\sigma^{2}-2\sqrt{\frac{(2 \epsilon\ln 2+O(\epsilon^{2}))(1-\tau_{1}^{2}-\tau_{2}^{2}+O(\epsilon^{3/2}))}{1+O( \epsilon)}}\sigma^{2}\] \[-2\tau_{2}\omega_{2}\rho\sigma^{2}-2\tau_{2}\omega_{1}\nu\rho^{2} \sigma^{2}-2\tau_{1}\nu\rho^{2}\sigma^{2}\Bigg{)}.\] (295)

We proceed with solving the above optimization program. Taking the derivative of the objective function with respect to \(\eta_{1}\) and \(\eta_{2}\) yields the following:

\[\frac{\eta_{2}}{\sqrt{1-\eta_{1}^{2}-\eta_{2}^{2}}} =\rho\sqrt{1+\rho^{2}}+O(\epsilon),\] (296) \[\frac{\eta_{1}}{\sqrt{1-\eta_{1}^{2}-\eta_{2}^{2}}} =\rho^{2}+O(\epsilon).\] (297)

Solving the above set of equations, we get

\[\eta_{1} =\frac{\rho^{2}}{\sqrt{1+\rho^{2}+2\rho^{4}}}+O(\epsilon),\] (298) \[\eta_{2} =\frac{\rho\sqrt{1+\rho^{2}}}{\sqrt{1+\rho^{2}+2\rho^{4}}}+O( \epsilon).\] (299)

Thus, considering the dominant terms, we get the following reconstruction for the third frame

\[\hat{X}_{3}^{G}=\frac{\rho^{2}}{\sqrt{1+\rho^{2}+2\rho^{4}}}\hat{X }_{1}^{G}+\frac{\rho\sqrt{1+\rho^{2}}}{\sqrt{1+\rho^{2}+2\rho^{4}}}\hat{X}_{2}^ {G}+\frac{\sqrt{2\epsilon\ln 2}}{\sqrt{1+\rho^{2}+2\rho^{4}}}X_{3}+Z_{3}.\] (300)

The above reconstruction yields the following distortion for the third frame

\[\mathbbm{E}[\|X_{3}-\hat{X}_{3}^{G}\|^{2}]=2(1-\sqrt{2\epsilon\ln 2(1+ \rho^{2}+2\rho^{4})})\sigma^{2}.\] (301)

Finally, consider the reconstruction of the fourth frame as follows

\[\hat{X}_{4}^{G}=\lambda_{1}\hat{X}_{1}^{G}+\lambda_{2}\hat{X}_{2 }^{G}+\lambda_{3}\hat{X}_{3}^{G}+\lambda_{4}X_{4}+Z_{4},\] (302)where \(\hat{X}_{4}^{G}\sim\mathcal{N}(0,\sigma^{2})\). The rate constraint of the fourth step implies that

\[(1-\lambda_{1}^{2}-\lambda_{2}^{2}-\lambda_{3}^{2}+O(\epsilon))(2\epsilon\ln 2 +O(\epsilon))=\lambda_{4}^{2}(1+O(\epsilon)).\] (303)

The distortion can be written as follows

\[\mathbb{E}[\|X_{4}-\hat{X}_{4}^{G}\|^{2}] =2\sigma^{2}-2\lambda_{4}\sigma^{2}-2\lambda_{3}\rho\tau_{3} \sigma^{2}-2\lambda_{3}\rho^{2}\tau_{2}\omega_{2}\sigma^{2}-2\lambda_{3}\rho^{ 3}\tau_{2}\omega_{1}\nu\sigma^{2}\] \[\qquad-2\lambda_{3}\rho^{3}\tau_{1}\nu\sigma^{2}-2\lambda_{2}\rho ^{3}\omega_{1}\nu\sigma^{2}-2\lambda_{2}\rho^{2}\omega_{2}\sigma^{2}-2\lambda_ {1}\rho^{3}\nu\] (304) \[=2\sigma^{2}-2\sqrt{(2\epsilon\ln 2)(1-\lambda_{1}^{2}-\lambda_{ 2}^{2}-\lambda_{3}^{2})}\sigma^{2}-2\lambda_{3}\rho\tau_{3}\sigma^{2}\] \[\qquad-2\lambda_{3}\rho^{2}\tau_{2}\omega_{2}\sigma^{2}-2\lambda_ {3}\rho^{3}\tau_{2}\omega_{1}\nu\sigma^{2}-2\lambda_{3}\rho^{3}\tau_{1}\nu \sigma^{2}\] \[\qquad-2\lambda_{2}\rho^{3}\omega_{1}\nu\sigma^{2}-2\lambda_{2} \rho^{2}\omega_{2}\sigma^{2}-2\lambda_{1}\rho^{3}\nu+O(\epsilon).\] (305)

We take the derivative of the above expression with respect to \(\lambda_{1}\), \(\lambda_{2}\) and \(\lambda_{3}\) and we get

\[\frac{\lambda_{1}}{\sqrt{1-\lambda_{1}^{2}-\lambda_{2}^{2}- \lambda_{3}^{2}}} =\rho^{3}+O(\epsilon),\] (306) \[\frac{\lambda_{2}}{\sqrt{1-\lambda_{1}^{2}-\lambda_{2}^{2}- \lambda_{3}^{2}}} =\rho^{2}\sqrt{1+\rho^{2}}+O(\epsilon),\] (307) \[\frac{\lambda_{3}}{\sqrt{1-\lambda_{1}^{2}-\lambda_{2}^{2}- \lambda_{3}^{2}}} =\rho\sqrt{1+\rho^{2}+2\rho^{4}}+O(\epsilon).\] (308)

Solving the above set of equations yields the following

\[\lambda_{1} =\frac{\rho^{3}}{\sqrt{1+\rho^{2}+2\rho^{4}+4\rho^{6}}}+O( \epsilon),\] (309) \[\lambda_{2} =\frac{\rho^{2}\sqrt{1+\rho^{2}}}{\sqrt{1+\rho^{2}+2\rho^{4}+4 \rho^{6}}}+O(\epsilon),\] (310) \[\lambda_{3} =\frac{\rho\sqrt{1+\rho^{2}+2\rho^{4}}}{\sqrt{1+\rho^{2}+2\rho^{ 4}+4\rho^{6}}}+O(\epsilon).\] (311)

Thus, considering the dominant terms, we can write

\[\hat{X}_{4}^{G} =\frac{\rho^{3}}{\sqrt{1+\rho^{2}+2\rho^{4}+4\rho^{6}}}\hat{X}_ {1}^{G}+\frac{\rho^{2}\sqrt{1+\rho^{2}}}{\sqrt{1+\rho^{2}+2\rho^{4}+4\rho^{6} }}\hat{X}_{2}^{G}\] \[\qquad+\frac{\rho\sqrt{1+\rho^{2}+2\rho^{4}}}{\sqrt{1+\rho^{2}+ 2\rho^{4}+4\rho^{6}}}\hat{X}_{3}^{G}+\frac{\sqrt{2\epsilon\ln 2}}{\sqrt{1+\rho^{2}+2 \rho^{4}+4\rho^{6}}}X_{4}+Z_{4}.\] (312)

The distortion term then becomes:

\[\mathbb{E}[\|X_{4}-\hat{X}_{4}^{G}\|^{2}]=2(1-\sqrt{2\epsilon\ln 2(1+\rho^{2}+2 \rho^{4}+4\rho^{6})})\sigma^{2}.\] (313)

Now, we use induction to derive the terms for \(T\) frames. Define

\[\Delta_{\text{FMD},j} :=\sqrt{1+\sum_{i=1}^{j-1}2^{j-1-i}\rho^{2(j-i)}},\qquad j=2,\dots,T\] (314) \[=\sqrt{1+\rho^{2}\frac{(2\rho^{2})^{j-1}-1}{2\rho^{2}-1}}.\] (315)

Thus, we have

\[\hat{X}_{j}^{G}=\sum_{i=1}^{j-1}\frac{\Delta_{\text{FMD},i}\rho^{j-i}}{\Delta_{ \text{FMD},j}}\hat{X}_{i}^{G}+\frac{\sqrt{2\epsilon\ln 2}}{\Delta_{\text{FMD},j}}X_{j}+Z_{j}, \qquad j=2,\dots,T,\] (316)

where \(Z_{j}\) is a Gaussian random variable independent of \((\hat{X}_{1}^{G},\dots,\hat{X}_{j-1}^{G},X_{j})\) and its variance is such that \(\mathbb{E}[(\hat{X}_{j}^{G})^{2}]=\sigma^{2}\). The distortion is given by the following expression

\[D_{\text{FMD},j}=\mathbb{E}[\|X_{j}-\hat{X}_{j}\|^{2}]=2(1-\Delta_{\text{FMD},j }\sqrt{2\epsilon\ln 2})\sigma^{2},\qquad j=2,\dots,T.\] (317)For the special case where \(\rho=1\), then the distortion simplifies to the following

\[\mathbb{E}[\|X_{j}-\hat{X}_{j}\|^{2}]=2(1-2^{\frac{j-1}{2}}\sqrt{2 \epsilon\ln 2})\sigma^{2},\qquad j=2,\ldots,T,\] (318)

which shows an exponential decrease at each step.

_Distortion Analysis for \(0\)-PLF-JD_:

In this case, the proof for \(T\) frames is similar to (254)-(261). Thus, we have

\[\hat{X}_{j}^{G}=\rho\hat{X}_{j-1}^{G}+\sqrt{(1-\rho^{2})2\epsilon \ln 2}X_{j}+Z_{j},j=2,\ldots,T,\] (319)

where \(Z_{j}\) is a Gaussian random variable independent of \((\hat{X}_{j-1}^{G},X_{j})\) and its variance is such that \(\mathbb{E}[(\hat{X}_{T}^{G})^{2}]=\sigma^{2}\). It should be mentioned that preserving the correlation coefficients, e.g., \(\mathbb{E}[\hat{X}_{j}^{G}\hat{X}_{j-1}^{G}]=\rho\), needs some correction terms of \(O(\epsilon)\) as discussed in (257). However, as shown in (261), these correction terms do not contribute to dominant terms of distortion and hence, they can be ignored in the presentation of (319). Now, define

\[\Delta_{\text{JD},j}:=\rho^{2(j-1)}+\sqrt{1-\rho^{2}}(\sum_{i=0} ^{j-2}\rho^{2i}),\qquad j=2,\ldots,T,\] (320)

and notice that

\[D_{\text{JD},j} :=\mathbb{E}[\|X_{j}-\hat{X}_{j}\|^{2}]\] (321) \[=\,2\sigma^{2}-2\mathbb{E}[X_{j}\hat{X}_{j}]\] (322) \[=\,2\sigma^{2}-2\mathbb{E}[X_{j}(\rho\hat{X}_{j-1}^{G}+\sqrt{(1- \rho^{2})2\epsilon\ln 2}X_{j})]\] (323) \[=\,2\sigma^{2}-2\mathbb{E}[X_{j}(\rho^{j-1}X_{1}+\sqrt{1-\rho^{2} }(\rho^{j-2}X_{2}+\ldots+X_{j}))]\sqrt{2\epsilon\ln 2}\sigma^{2}\] (324) \[=\,2(1-\Delta_{\text{JD},j}\sqrt{2\epsilon\ln 2})\sigma^{2}.\] (325)

For the special case of \(\rho=1\), we get \(\Delta_{\text{JD},j}=1\) which remains a constant across different steps. 

## Appendix H Universality Statement for Gauss-Markov Source Model

### MMSE Representations for a Given Rate

For a given rate tuple R, the minimum distortions achievable by MMSE representations are derived in [28, 37] and are given by

\[D_{1}^{\min} =\sigma_{1}^{2}2^{-2R_{1}},\] (326) \[D_{2}^{\min} =(\rho_{1}^{2}\frac{\sigma_{2}^{2}}{\sigma_{1}^{2}}D_{1}^{\min}+ \sigma_{N_{1}}^{2})2^{-2R_{2}},\] (327) \[D_{3}^{\min} =(\rho_{2}^{2}\frac{\sigma_{3}^{2}}{\sigma_{2}^{2}}D_{2}^{\min}+ \sigma_{N_{2}}^{2})2^{-2R_{3}},\] (328)

where

\[\sigma_{N_{1}}^{2} :=(1-\rho_{1}^{2})\sigma_{2}^{2},\] (329) \[\sigma_{N_{2}}^{2} :=(1-\rho_{2}^{2})\sigma_{3}^{2}.\] (330)

The above distortions are achieved by the following optimal reconstructions \(\hat{\mathsf{X}}_{r}\) given in [28]. Notice that the MMSE representation is \(\mathsf{X}_{r}^{\text{RD}}=\hat{\mathsf{X}}_{r}\), i.e., the functions \(\eta_{1}(.)\) and \(\eta_{2}(.,.)\) of iRDP region \(\mathcal{C}_{\text{RDP}}\) (Definition 4) are identity functions (this statement follows from Theorem 5). Now, we choose the reconstruction \(\hat{\mathsf{X}}_{r}\) in the following.

The reconstruction \(\hat{X}_{r,1}\) is chosen such that \(\hat{X}_{r,1}\to X_{1}\to(X_{2},X_{3})\) holds a Markov chain and

\[X_{1}=\hat{X}_{r,1}+Z_{1},\] (331)

where \(\hat{X}_{r,1}\sim\mathcal{N}(0,\sigma_{1}^{2}-D_{1}^{\min})\) and \(Z_{1}\sim\mathcal{N}(0,D_{1}^{\min})\) are independent random variables. Then, the reconstruction \(\hat{X}_{r,2}\) is chosen as follows. Let

\[W_{2}:=\rho_{1}\frac{\sigma_{2}}{\sigma_{1}}Z_{1}+N_{1},\] (332)which is the innovation from \(\hat{X}_{r,1}\) to \(X_{2}\). Now, we find the random variables \(\hat{W}_{2}\) and \(Z_{2}\) such that

\[W_{2}=\hat{W}_{2}+Z_{2},\] (333)

where \(\hat{W}_{2}\sim\mathcal{N}(0,\rho_{1}^{2}\frac{\sigma_{2}^{2}}{\sigma_{1}^{2}}D _{1}^{\min}+\sigma_{N_{1}}^{2}-D_{2}^{\min})\) and \(Z_{2}\sim\mathcal{N}(0,D_{2}^{\min})\) are independent from each other, and the Markov chain \(\hat{W}_{2}\to(X_{2},\hat{X}_{r,1})\to(X_{1},X_{3})\) holds. Now, define

\[\hat{X}_{r,2}:=\rho_{1}\frac{\sigma_{2}}{\sigma_{1}}\hat{X}_{r,1}+\hat{W}_{2}.\] (334)

Finally, we choose the reconstruction \(\hat{X}_{r,3}\) as follows. Let

\[W_{3}:=\rho_{2}\frac{\sigma_{3}}{\sigma_{2}}Z_{2}+N_{2},\] (335)

which is the innovation from \(\hat{X}_{r,2}\) to \(X_{3}\). Now, we find random variables \(\hat{W}_{3}\) and \(Z_{3}\) such that

\[W_{3}=\hat{W}_{3}+Z_{3},\] (336)

where \(\hat{W}_{3}\sim\mathcal{N}(0,\rho_{2}^{2}\frac{\sigma_{2}^{2}}{\sigma_{2}^{2} }D_{2}^{\min}+\sigma_{N_{2}}^{2}-D_{3}^{\min})\) and \(Z_{2}\sim\mathcal{N}(0,D_{3}^{\min})\) are independent from each other, and the Markov chain \(\hat{W}_{3}\to(X_{3},\hat{X}_{r,1},\hat{X}_{r,2})\to(X_{1},X_{2})\) holds. Now, define

\[\hat{X}_{r,3}:=\rho_{1}\frac{\sigma_{3}}{\sigma_{2}}\hat{X}_{r,2}+\hat{W}_{3}.\] (337)

Thus, the optimal reconstruction \(\hat{\mathsf{X}}_{r}\) is chosen and it satisfies the rate constraint \(\mathsf{R}\).

### Universality Statement

**Theorem 7**: _For a given rate tuple \(\mathsf{R}\) with strictly positive components, let the MMSE representation be denoted as \(\mathsf{X}_{r}^{\mathsf{RD}}=(X_{r,1}^{\mathsf{RD}},X_{r,2}^{\mathsf{RD}},X_{r,3}^{\mathsf{RD}})\). Let \((\mathsf{D},\mathsf{P})\in\mathcal{DP}(\mathsf{R})\) and let \(\hat{\mathsf{X}}=(\hat{X}_{1},\hat{X}_{2},\hat{X}_{3})\) be the corresponding reconstruction achieving it. Then there exist \(\kappa_{1}\), \(\theta_{1}\), \(\theta_{2}\), \(\psi_{1}\), \(\psi_{2}\) and \(\psi_{3}\) and noise variables \((Z_{1}\), \(Z_{2}\), \(Z_{3})\) independent of \((X_{r,1}^{\mathsf{RD}},X_{r,2}^{\mathsf{RD}},X_{r,3}^{\mathsf{RD}})\), which satisfy the following_

\[\hat{X}_{1}=\kappa_{1}X_{r,1}^{\mathsf{RD}}+Z_{1},\quad\hat{X}_{2}=\theta_{1}X _{r,1}^{\mathsf{RD}}+\theta_{2}X_{r,2}^{\mathsf{RD}}+Z_{2},\quad\hat{X}_{3}= \psi_{1}X_{r,1}^{\mathsf{RD}}+\psi_{2}X_{r,2}^{\mathsf{RD}}+\psi_{3}\hat{X}_{r,3}^{\mathsf{RD}}+Z_{3}.\]

_For a given positive rate tuple \(\mathsf{R}\), let the MMSE representation \(\mathsf{X}_{r}^{\mathsf{RD}}\) be in the set \(\mathcal{P}^{\mathsf{RD}}(\mathsf{R})\). Also, let \((\mathsf{D},\mathsf{P})\in\mathcal{DP}(\mathsf{R})\) and \(\mathsf{X}_{r}\), \(\hat{\mathsf{X}}\) be the corresponding representation and reconstruction achieving it._

_Proof:_ First, notice that according to the proof of Theorem 5 for the Gauss-Markov source model, one can set \(\hat{\mathsf{X}}=\mathsf{X}_{r}\) in iRDP region of \(\mathcal{C}_{\mathsf{RDP}}\), without loss of optimality. So, in the following proof, the reconstruction \(\mathsf{X}_{r}\) and representation \(\hat{\mathsf{X}}\) are used interchangeably, in some places.

We show the following statement. If

\[R_{1} \geq I(X_{1};X_{r,1}),\] (338) \[R_{2} \geq I(X_{2};X_{r,2}|X_{r,1}),\] (339) \[R_{3} \geq I(X_{3};X_{r,3}|X_{r,1},X_{r,2}),\] (340)

then, there exist \(\kappa_{1}\), \(\theta_{1}\), \(\theta_{2}\), \(\psi_{1}\), \(\psi_{2}\) and \(\psi_{3}\) and noise variables \(Z_{1}\), \(Z_{2}\), \(Z_{3}\) independent of \(X_{r,1}^{\mathsf{RD}}\), \((X_{r,1}^{\mathsf{RD}},X_{r,2}^{\mathsf{RD}})\), \((X_{r,1}^{\mathsf{RD}},X_{r,2}^{\mathsf{RD}})\), respectively, which satisfy the following

\[\hat{X}_{1} =\kappa_{1}X_{r,1}^{\mathsf{RD}}+Z_{1},\] (341) \[\hat{X}_{2} =\theta_{1}X_{r,1}^{\mathsf{RD}}+\theta_{2}X_{r,2}^{\mathsf{RD}}+Z _{2},\] (342) \[\hat{X}_{3} =\psi_{1}X_{r,1}^{\mathsf{RD}}+\psi_{2}X_{r,2}^{\mathsf{RD}}+\psi_{ 3}\hat{X}_{r,3}^{\mathsf{RD}}+Z_{3}.\] (343)

If (338)-(340) are satisfied with equality, then the noise random variables in (341)-(343) do not exist and a linear combination is sufficient for converting \((X_{r,1}^{\mathsf{RD}},X_{r,2}^{\mathsf{RD}},X_{r,3}^{\mathsf{RD}})\) to \((\hat{X}_{1},\hat{X}_{2},\hat{X}_{3})\).

First, we prove the statement when all of inequalities in (338)-(340) hold with "equality". We provide the proof for \(T=2\) frames. The extension to arbitrary number of frames is straightforward. To that end, we first prove the following two lemmas.

**Lemma 2**: _Without loss of optimality, the reconstruction of the first step \(\hat{X}_{1}\) satisfies the following_

\[\gamma_{1}\hat{X}_{1}=W_{1},\] (344)

_where_

\[\gamma_{1}:=\frac{\mathbb{E}[X_{1}\hat{X}_{1}]}{\sigma_{\hat{X}_{1}}^{2}},\] (345)

_and \(W_{1}\) is a Gaussian random variable that its statistics do not depend on the pair \((D_{1},P_{1})\)._

_Proof:_ According to Theorem 5, we know that \((X_{1},\hat{X}_{1})\) are jointly Gaussian. So, we can write \(X_{1}\) as follows

\[X_{1}=\gamma_{1}\hat{X}_{1}+T_{1},\] (346)

where \(T_{1}\) is a Gaussian random variable independent of \(\hat{X}_{1}\) with a constant variance \(\sigma_{1}^{2}2^{-2R_{1}}\). Notice that (346) can be written as follows

\[\hat{X}_{1}=\alpha_{1}(X_{1}+Q),\] (347)

where \(Q\) is a Gaussian random variable independent of \(X_{1}\) with a zero-mean and variance \(\frac{\sigma_{1}^{2}2^{-2R_{1}}}{1-2^{-2R_{1}}}\) and

\[\alpha_{1}:=\frac{1}{\gamma_{1}}(1-2^{-2R_{1}}).\] (348)

From (347), we get

\[\gamma_{1}\hat{X}_{1}=(1-2^{-2R_{1}})(X_{1}+Q).\] (349)

Now, defining \(W_{1}:=(1-2^{-2R_{1}})(X_{1}+Q)\) yields the desired result.

**Lemma 3**: _Without loss of optimality, the reconstructions of the first and second steps \((\hat{X}_{1},\hat{X}_{2})\) satisfy the following_

\[\lambda_{1}\hat{X}_{1}+\lambda_{2}\hat{X}_{2}=W_{2},\] (350)

_where_

\[\lambda_{1} :=\frac{\rho_{1}\mathbb{E}[X_{1}\hat{X}_{1}]\hat{\sigma}_{X_{2}}^ {2}-\mathbb{E}[\hat{X}_{1}\hat{X}_{2}]\mathbb{E}[X_{2}\hat{X}_{2}]}{\hat{ \sigma}_{X_{1}}^{2}\hat{\sigma}_{X_{2}}^{2}-\mathbb{E}^{2}[\hat{X}_{1}\hat{X}_ {2}]},\] (351) \[\lambda_{2} :=\frac{\rho_{1}\mathbb{E}[X_{1}\hat{X}_{1}]\mathbb{E}[\hat{X}_{ 1}\hat{X}_{2}]-\hat{\sigma}_{X_{1}}^{2}\mathbb{E}[X_{2}\hat{X}_{2}]}{\hat{ \sigma}_{X_{1}}^{2}\hat{\sigma}_{X_{2}}^{2}-\mathbb{E}^{2}[\hat{X}_{1}\hat{X}_ {2}]},\] (352)

_and \(W_{2}\) is a Gaussian random variable that its statistics do not depend on the pairs \((D_{1},P_{1})\) and \((D_{2},P_{2})\)._

_Proof:_ According to Theorem 5, we know that \((X_{1},X_{2},\hat{X}_{1},\hat{X}_{2})\) are jointly Gaussian. So, we can write \(X_{2}\) as follows

\[X_{2}=\lambda_{1}\hat{X}_{1}+\lambda_{2}\hat{X}_{2}+T_{2},\] (353)

where \(T_{2}\) is a Gaussian random variable independent of \((\hat{X}_{1},\hat{X}_{2})\) with a constant variance of \(\sigma_{X_{2}|\hat{X}_{1}}^{2}2^{-2R_{2}}\) where

\[\sigma_{X_{2}|\hat{X}_{1}}^{2}:=\frac{1}{2}\log\left(\rho_{1}^{2}\sigma_{1}^{ 2}2^{-2R_{1}}+2^{2H(N_{1})}\right).\] (354)

Notice that (353) can be written as follows

\[\lambda_{1}\hat{X}_{1}+\lambda_{2}\hat{X}_{2}=(1-2^{-2R_{2}})(X_{2}+Q^{\prime }),\] (355)

where \(Q^{\prime}\) is a Gaussian random variable independent of \(X_{2}\) with a zero-mean and variance \(\frac{\sigma_{X_{2}|\hat{X}_{1}}^{2}2^{-2R_{2}}}{1-2^{-2R_{2}}}\). Defining \(W_{2}:=(1-2^{-2R_{2}})(X_{2}+Q^{\prime})\) yields the desired result.

Now, we proceed with the proof of the theorem. According to Lemma 2, there exist real \(\gamma_{1}\) and \(\gamma_{1}^{\prime}\) such that

\[\gamma_{1}\hat{X}_{1}=\gamma_{1}^{\prime}X_{r,1}^{\text{RD}}.\] (356)

Define

\[\kappa_{1}:=\frac{\gamma_{1}^{\prime}}{\gamma_{1}}.\] (357)

Then, according to Lemma 3, there exist \(\lambda_{1}\), \(\lambda_{2}\), \(\lambda_{1}^{\prime}\) and \(\lambda_{2}^{\prime}\) such that

\[\lambda_{1}\hat{X}_{1}+\lambda_{2}\hat{X}_{2}=\lambda_{1}^{\prime}X_{r,1}^{ \text{RD}}+\lambda_{2}^{\prime}X_{r,2}^{\text{RD}}.\] (358)

The above equation can be written as

\[\hat{X}_{2} =\frac{\lambda_{1}^{\prime}-\lambda_{1}\kappa_{1}}{\lambda_{2}}X _{r,1}^{\text{RD}}+\frac{\lambda_{2}^{\prime}}{\lambda_{2}}X_{r,2}^{\text{RD}}\] (359) \[:=\theta_{1}X_{r,1}^{\text{RD}}+\theta_{2}X_{r,2}^{\text{RD}}.\] (360)

A similar justification holds for the third frame.

Next, we prove the statement when at least one of the rate constraints in (338)-(340) hold with strict inequality. In the following, we construct new reconstructions \((\hat{X}_{1}^{\prime},\hat{X}_{2}^{\prime})\) based on \((\hat{X}_{1},\hat{X}_{2})\) such that they satisfy the rate constraints \((R_{1},R_{2})\) with equality. Then, we will be able to apply the two lemmas we proved to show that \((\hat{X}_{1},\hat{X}_{2})\) are linearly related to MMSE reconstructions \((X_{r,1}^{\text{RD}},X_{r,2}^{\text{RD}})\).

_Construction of \(\hat{X}_{1}^{\prime}\)_:

Now, let

\[\hat{R}_{1}:=I(X_{1};\hat{X}_{1}),\] (361)

where \(\hat{R}_{1}\leq R_{1}\). Also, recall that

\[R_{1}=I(X_{1};X_{r,1}^{\text{RD}}).\] (362)

Now, let \(\hat{X}_{1}^{\prime}\) such that \(\hat{X}_{1}^{\prime}\to X_{r,1}^{\text{RD}}\to X_{1}\) holds and

\[\hat{X}_{1}^{\prime}=X_{r,1}^{\text{RD}}+W_{1},\] (363)

where \(W_{1}\sim\mathcal{N}(0,\nu_{1}^{2})\) independent of \(\hat{X}_{1}\) and \(\nu_{1}^{2}\) will be determined in the following. Notice that \(I(X_{1};\hat{X}_{1}^{\prime})\) is a monotonically decreasing function of \(\nu_{1}^{2}\). So, one choose \(\nu_{1}^{2}\) such that

\[I(\hat{X}_{1}^{\prime};X_{1})=I(X_{1};\hat{X}_{1})=\hat{R}_{1}.\] (364)

Now, according to Lemma 2, since \(\hat{X}_{1}^{\prime}\) and \(\hat{X}_{1}\) have the same rates, there exists a coefficient \(\kappa_{1}^{\prime}\) such that

\[\hat{X}_{1} =\kappa_{1}^{\prime}\hat{X}_{1}^{\prime}\] (365) \[=\kappa_{1}^{\prime}X_{r,1}^{\text{RD}}+\kappa_{1}^{\prime}W_{1}.\] (366)

Now, define \(Z_{1}:=\kappa_{1}^{\prime}W_{1}\) and notice that

\[\hat{X}_{1}=\kappa_{1}^{\prime}X_{r,1}^{\text{RD}}+Z_{1}.\] (367)

_Construction of \(\hat{X}_{2}^{\prime}\)_:

Next, consider the second step. Define

\[\hat{R}_{2}:=I(X_{2};\hat{X}_{2}|\hat{X}_{1}),\] (368)

where \(\hat{R}_{2}\leq R_{2}\). Also, recall that

\[R_{2}=I(X_{2};X_{r,2}^{\text{RD}}|X_{r,1}^{\text{RD}}).\] (369)

Define \(\tilde{X}_{2}:=\mathbb{E}[X_{2}|X_{r,1}^{\text{RD}},X_{r,2}^{\text{RD}}]\) to be the MMSE reconstruction and consider that

\[R_{2} =I(X_{2};X_{r,2}^{\text{RD}}|X_{r,1}^{\text{RD}})\] (370) \[=I(X_{2};\tilde{X}_{2}|X_{r,1}^{\text{RD}}),\] (371)where the last equality follows because both Markov chains \(X_{2}\rightarrow(X_{r,1}^{\text{RD}},X_{r,2}^{\text{RD}})\rightarrow\tilde{X}_{2}\) and \(X_{2}\rightarrow\tilde{X}_{2}\rightarrow(X_{r,1}^{\text{RD}},X_{r,2}^{\text{RD }})\) hold where the latter one is satisfied for Gaussian random variables for which we can write \(X_{2}=\mathbb{E}[X_{2}|X_{r,1},X_{r,2}]+W^{\prime}\) such that \(W^{\prime}\) is independent of \((X_{r,1}^{\text{RD}},X_{r,2}^{\text{RD}})\).

Now, we show that \(I(X_{2};\tilde{X}_{2}|X_{r,1}^{\text{RD}})\leq I(X_{2};\tilde{X}_{2}|\hat{X}_{1 }^{\prime})\). This is justified in the following

\[I(X_{2};\tilde{X}_{2}|\hat{X}_{1}^{\prime}) =I(X_{2};\tilde{X}_{2}|X_{r,1}^{\text{RD}}+W_{1})\] (372) \[=H(X_{2}|X_{r,1}^{\text{RD}}+W_{1})-H(X_{2}|\tilde{X}_{2},X_{r,1} ^{\text{RD}}+W_{1})\] (373) \[\geq H(X_{2}|X_{r,1}^{\text{RD}}+W_{1},W_{1})-H(X_{2}|\tilde{X}_{ 2},X_{r,1}^{\text{RD}}+W_{1})\] (374) \[=H(X_{2}|X_{r,1}^{\text{RD}},W_{1})-H(X_{2}|\tilde{X}_{2},X_{r,1} ^{\text{RD}}+W_{1})\] (375) \[\geq H(X_{2}|X_{r,1}^{\text{RD}},W_{1})-H(X_{2}|\tilde{X}_{2})\] (376) \[=H(X_{2}|X_{r,1}^{\text{RD}})-H(X_{2}|\tilde{X}_{2})\] (377) \[=H(X_{2}|X_{r,1}^{\text{RD}})-H(X_{2}|\tilde{X}_{2},X_{r,1}^{\text {RD}})\] (378) \[=I(X_{2};\tilde{X}_{2}|X_{r,1}^{\text{RD}}),\] (379)

where (377) follows because \(W_{1}\) is independent of \((X_{2},X_{r,1}^{\text{RD}})\) and (378) follows from the Markov chain \(X_{2}\rightarrow\tilde{X}_{2}\to X_{r,1}^{\text{RD}}\).

Define

\[R_{2}^{\prime}:=I(X_{2};\tilde{X}_{2}|\hat{X}_{1}^{\prime}),\] (380)

and consider the fact that \(R_{2}^{\prime}\geq R_{2}\). Now, we introduce \(\hat{X}_{2}^{\prime}\) such that \(\hat{X}_{2}^{\prime}\rightarrow(\tilde{X}_{2},\hat{X}_{1}^{\prime})\to X _{2}\) forms a Markov chain and

\[\hat{X}_{2}^{\prime}=\tilde{X}_{2}+\hat{X}_{1}^{\prime}+W_{2},\] (381)

where \(W_{2}\sim\mathcal{N}(0,\nu_{2}^{2})\) independent of \((\tilde{X}_{2},\hat{X}_{1})\) and \(\nu_{2}^{2}\) will be determined in the following. Since \(I(X_{2};\hat{X}_{2}^{\prime}|\hat{X}_{1}^{\prime})\) is a monotonically decreasing function of \(\nu_{2}^{2}\), we can choose \(\nu_{2}^{2}\) such that

\[I(X_{2};\hat{X}_{2}^{\prime}|\hat{X}_{1}^{\prime})=I(X_{2};\hat{X}_{2}|\hat{X} _{1})=\hat{R}_{2}.\] (382)

Then, according to Lemma 3, there exist \(\lambda_{1}^{\prime}\), \(\lambda_{2}^{\prime}\), \(\hat{\lambda}_{1}\) and \(\hat{\lambda}_{2}\) such that

\[\lambda_{1}^{\prime}\hat{X}_{1}^{\prime}+\lambda_{2}^{\prime}\hat{X}_{2}^{ \prime}=\hat{\lambda}_{1}\hat{X}_{1}+\hat{\lambda}_{2}\hat{X}_{2}.\] (383)

Plugging (363), (367) and (381) into the above expression and letting \(\tilde{X}_{2}=\alpha X_{r,1}^{\text{RD}}+\beta X_{r,2}^{\text{RD}}\) for some \(\alpha,\beta\), we get

\[(\lambda_{1}^{\prime}+(1+\alpha)\lambda_{2}^{\prime}-\hat{\lambda}_{1}\kappa^ {\prime})X_{r,1}^{\text{RD}}+\lambda_{2}^{\prime}\beta X_{r,2}^{\text{RD}}+( \lambda_{1}^{\prime}+\lambda_{2}^{\prime})W_{1}+\lambda_{2}^{\prime}W_{2}-\hat {\lambda}_{1}Z_{1}=\hat{\lambda}_{2}\hat{X}_{2}.\] (384)

Now define

\[\theta_{1} :=\frac{\lambda_{1}^{\prime}+(1+\alpha)\lambda_{2}^{\prime}-\hat{ \lambda}_{1}\kappa^{\prime}}{\hat{\lambda}_{2}},\] (385) \[\theta_{2} :=\frac{\lambda_{2}^{\prime}\beta}{\hat{\lambda}_{2}},\] (386) \[Z_{2} :=\frac{(\lambda_{1}^{\prime}+\lambda_{2}^{\prime})}{\hat{\lambda}_ {2}}W_{1}+\frac{\lambda_{2}^{\prime}}{\hat{\lambda}_{2}}W_{2}-\frac{\hat{ \lambda}_{1}}{\hat{\lambda}_{2}}Z_{1}.\] (387)

Thus, we have

\[\hat{X}_{2}=\theta_{1}X_{r,1}^{\text{RD}}+\theta_{2}X_{r,2}^{\text{RD}}+Z_{2}.\] (388)

Notice that the above proof only uses the information about reconstructions of the operating points in DP-tradeoff and it does not depend on the choice of PLF. So, it holds for both PLF-JD and PLF-FMD. This concludes the proof.

### Gaussian Example

Assume that the sources are symmetric in the sense that \(\sigma_{1}^{2}=\sigma_{2}^{2}=\sigma_{3}^{2}=1\), \(\rho_{1}=\rho_{2}=\rho_{3}:=\rho\) for some \(0<\rho\leq 1\). Also, suppose that the perception thresholds are symmetric, i.e., \(P_{1}=P_{2}=P_{3}:=P\) for some \(0<P\leq 1\). We choose the rate tuple R such that the minimum distortions \(D_{j}^{\min}=D\) for \(j\in\{1,2,3\}\). According to Appendix H.1, such rates are given by

\[R_{1} =\frac{1}{2}\log\frac{1}{D},\] (389) \[R_{2} =\frac{1}{2}\log\frac{\rho^{2}D+(1-\rho)}{D},\] (390) \[R_{3} =\frac{1}{2}\log\frac{\rho^{2}D+(1-\rho^{2})}{D}.\] (391)

The covariance matrix of the MMSE representations \(\text{cov}(X_{r,1}^{\text{RD}},X_{r,2}^{\text{RD}},X_{r,3}^{\text{RD}})\) is given by \((1-D)\Sigma\) where

\[\Sigma:=\begin{pmatrix}1&\rho&\rho^{2}\\ \rho&1&\rho\\ \rho^{2}&\rho&1\end{pmatrix}.\] (392)

If we introduce the \(0\)-PLF while keeping the rates as those of MMSE reconstructions, it can be shown that the optimal distortions are all equal to \(D_{1}=D_{2}=D_{3}=2-2\sqrt{1-D}\). Denote the reconstructions by \((\hat{X}_{D_{1}}^{0},\hat{X}_{D_{2}}^{0},\hat{X}_{D_{3}}^{0})\) and notice that the covariance matrix of the reconstructions is equal to that of the sources and is given by \(\Sigma\). Thus, the covariance matrix of \((X_{r,1}^{\text{RD}},X_{r,2}^{\text{RD}},X_{r,3}^{\text{RD}})\) is \((1-D)\) times the covariance matrix of \((\hat{X}_{D_{1}}^{0},\hat{X}_{D_{2}}^{0},\hat{X}_{D_{3}}^{0})\). So, the reconstructions \((X_{r,1}^{\text{RD}},X_{r,2}^{\text{RD}},X_{r,3}^{\text{RD}})\) and \((\hat{X}_{D_{1}}^{0},\hat{X}_{D_{2}}^{0},\hat{X}_{D_{3}}^{0})\) can be transformed to each other by the scaling factor \(\frac{1}{\sqrt{1-D}}\). This inspires the idea that reconstructions corresponding to different tuples \((\mathsf{D},\mathsf{P})\) are linearly related to those of MMSE representations which is the essence of the following Theorem 6. Moreover, both PLFs either based on FMD or JD perform similarly in this example since individually scaling the reconstruction of each frame finally ends up in matching the covariance matrix of all frames.

## Appendix I Justification of low-rate regime for Moving MNSIT

In the MovingMNIST dataset, the digit in I-frame is generated uniformly across the \(32\times 32\) center region in a \(64\times 64\) image, meaning that \(\log(32\times 32){=}10\) bits are required to localize the digits and any lower rate would result in much less correlated reconstructions. As such, one can consider \(R_{1}{=}12\) bits (\(2\) extra bits for content and style) as a low rate. For P-frames, the movement is uniformly constrained within a \(10\times 10\) region so any rate \(R_{2}{\leq}\log_{2}(10\times 10){=}6.6\) bits (excluding residual compensation) can be considered a low rate.

## Appendix J Experiment Details

### Training Setup and Overview

Our compression architecture is built on the scale-space flow model [32], which allows end-to-end training without relying on pre-trained optical flow estimators. For better compression efficiency, we replace the residual compression module with the conditioning one [33]. In the following, we will interchangeably refer \(X_{1}\) as the I-frame and subsequent ones as P-frames. The annotation for the encoder, decoder, and critic (discriminator) will be referred to as \(f,g\), and \(h\) respectively and their specific functionality (e.g motion compression, joint perception critic) will be described within context through a subscript/superscript.

_Distortion and Perception Measurement:_ We follow the setup in prior works [16, 21] for distortion and perception measurement. Specifically, we use MSE loss \(\mathbbm{E}[||X-\hat{X}||^{2}]\) as a distortion metric and Wasserstein-1 distance as a perception metric, which can be estimated through the WGAN critics (following the Kanotorovich-Rubinstein duality). For the marginal perception metric, we optimizeour critics \(h_{m}\) to classify between original image \(X\) and synthetic ones \(\hat{X}\). This will then allow us to measure \(W_{1}(P_{X},P_{\hat{X}})\) since:

\[W_{1}(P_{X},P_{\hat{X}})=\sup_{h_{m}\in\mathcal{F}}\mathbb{E}[h_{m}(X)]-\mathbb{ E}[h_{m}(\hat{X})]\] (393)

where \(\mathcal{F}\) is a set of all bounded 1-Lipschitz functions. Similarly, the joint perception metric is realized through \(W_{1}(P_{X_{1}...X_{j}},P_{\hat{X}_{1}...\hat{X}_{j}})\) by training a critic \(h_{j}\) that classifies between synthetic and authentic sequences:

\[W_{1}(P_{X_{1}...X_{j}},P_{\hat{X}_{1}...\hat{X}_{j}})=\sup_{h_{j}\in\mathcal{ F}}\mathbb{E}[h_{j}(X_{1},...,X_{i})]-\mathbb{E}[h_{j}(\hat{X}_{1},...,\hat{X}_{i})]\] (394)

In practice, the set of 1-Lipschitz functions is limited by the neural network architecture. Also, although our analysis employs the Wasserstein-2 distance as a perception metric, it is worth noting that the ideal reconstructions (0-PLF) for this metric and the one used in our study should be identical.

_I-frame Compressor:_ We compress I-frames in a similar fashion as previous works [16, 21]. Our encoder \(f_{I}\) and decoder \(g_{I}\) in Figure 6(a) contain a series of convolution operations and we control the rate \(R_{1}\) by varying the dimension and quantization level in the bottleneck. The model utilizes common randomness through the dithered quantization operation. For a given rate \(R_{1}\), we vary the amount of DP tradeoff by controlling the hyper-parameter \(\lambda_{i}^{\text{marginal}}\) in the following minimization objective \(\mathcal{L}_{1}\):

\[\mathcal{L}_{1}=\mathbb{E}[||X_{1}-\hat{X}_{1}||^{2}]+\lambda_{i}^{\text{ marginal}}W_{1}(P_{X_{1}},P_{\hat{X}_{1}})\] (395)

Following the results from Zhang et al. [16], we fix the encoder after optimizing the encoder-decoder pair for MSE representations. We then fix the encoder and train another decoder to obtain the optimal reconstruction with perfect perception, i.e, \(W_{1}(P_{X},P_{\hat{X}})\approx 0\). We will leverage these universal representation results to compress P-frames (both end-to-end and universal).

_P-frame Compressor:_ We describe the loss functions before explaining our architectures. Given previous reconstructions \(\hat{X}_{[i-1]}{:=}\{\hat{X}_{1},\hat{X}_{2},...,\hat{X}_{i-1}\}\), one can adjust the distortion-joint perception tradeoff by controlling the hyper-parameter \(\lambda_{i}^{\text{joint}}\) in the following objective \(\mathcal{L}_{i}\).

\[\mathcal{L}_{i}^{\text{joint}}=\mathbb{E}[||X_{i}-\hat{X}_{i}||^{2}]+\lambda_ {i}^{\text{joint}}W_{1}(P_{X_{[i]}},P_{\hat{X}_{[i]}})\] (396)

Note that in order to achieve 0-PLF-JD, previous reconstructions \(\hat{X}_{[i-1]}\) must also achieve 0-PLF-JD, since it is impossible to reconstruct such \(\hat{X}_{i}\) if the previous \(\hat{X}_{[i-1]}\) are not temporally consistent3. For the FMD metric, we use the loss function in (395).

Footnote 3: This follows from the inequality: \(W_{2}^{2}(P_{X_{1},X_{2}},P_{\hat{X}_{1},\hat{X}_{2}}){\geq}W_{2}^{2}(P_{X_{1} },P_{\hat{X}_{1}}){+}W_{2}^{2}(P_{X_{2}},P_{\hat{X}_{2}})\)

In the _universal model_ in Figure 6(b), the motion encoder \(f_{i}^{m}\) compresses and sends the quantized flow fields \([X_{r,i}^{m}]\) between the MMSE reconstruction \(\tilde{X}_{i-1}\) and \(X_{i}\). Given \([X_{r,i}^{m}]\), the flow decoder and warping module \(g_{i}^{m}\) will transform \(\tilde{X}_{i-1}\) into \(\tilde{X}_{i}^{w}\) (predicted frame). We use \(f_{i}^{c}\) to compress the

Figure 7: Compression diagram for (a) I-frame (b) P-frame with universal representation and (c) P-frame with optimized representation. For simplicity, we do not show the shared randomness \(K\).

residual information \([X^{c}_{r,i}]\) between \(X_{i}\) and \(\tilde{X}^{w}_{i}\)4, which will be decoded by \(g^{c}_{i}\). We note that for MMSE representation, \(g^{c}_{i}\) only requires \(\tilde{X}^{w}_{i}\) as a conditional input while an additional conditioning input \(\hat{X}_{[i-1]}\) is required when perceptual optimization is involved. Together, \(f^{m}_{i},g^{m}_{i}\), \(f^{c}_{i}\), and \(g^{c}_{i}\) are optimized for MMSE reconstructions. To train for different DP tradeoffs, we fix \(f^{m}_{i},g^{m}_{i},f^{c}_{i}\) and adapt the new decoder \(\hat{g}^{c}_{i}\) (conditioning on \(\tilde{X}^{w}_{i},\hat{X}_{[i-1]}\)). We note that fixing \(g^{m}_{i}\) for universal representation is essential since \([X^{c}_{r,i}]\) is dependent on the outputs \(\tilde{X}^{w}_{i}\) of \(g^{m}_{i}\).

Footnote 4: Here, we use conditioning [33] instead of sending \(X_{i}-\tilde{X}^{w}_{i-1}\) as in the original work [32]

In the _end-to-end model_ in Figure 6(c), we use an MMSE representation to estimate the motion vector, as in the case of the universal model. The only difference is that the encoder \(f^{c}_{i}\) also uses previous \(\hat{X}_{i}\) and the encoders will be jointly trained with the decoders.

### Networks Architecture

In this section, we describe the network architecture for universal and end-to-end P-frame compressor models. 5. In the architecture layout, we denote BN2D and SN for the _Batchnorm2D_ and _Spectral Normalization_ layers. Convolutional and transposed convolutional layer are denoted as "conv" and "upconv" respectively, which is accompanied by number of filters, kernel size, stride, and padding.

Footnote 5: For the I-frame compressor, we follow the DCGAN implementation by Denton et al [39], adding the dithered quantization layer in the encoder’s last layer( https://github.com/edenton/svg/blob/master/models/dcgan_64.py)

_Motion Encoder and Decoder._ The universal and optimized end-to-end model shares the same architecture for the motion encoder and decoder. (\(f^{m}_{i}\) and \(g^{m}_{i}\) respectively). We follow the original implementations [32] and present the convolutional architecture in Table 3. Different from the original implementation, however, we replace the last layer with dithered quantization layer (as in [16]) in our implementation. The output dimension of the motion encoder is denoted as \(d_{m}\).

_Residual Encoder and Decoder._ The architecture of the conditional residual encoder is shown in Table 3(a), where we stack multiple frames along their channel dimension as an input. As described previously, in the residual encoder, the universal model requires only \(X_{i},\tilde{X}^{w}_{i}\) while the end-to-end model will receive \(X_{i},\tilde{X}^{w}_{i}\) and \(\hat{X}_{[i-1]}\). We denote the output dimension of this residual encoder as \(d_{r}\). In the decoding part, the decoder will first condition all the previous reconstructions \(\hat{X}_{[i}-1]\) by projecting them into an embedding vector of size \(192\) (conditioning module in Table 3(b)). Then we concatenate this vector with the output of \(f^{r}_{i}\). The concatenated vector will be fed into the decoder (Table 3(c)) to produce the reconstruction \(\hat{X}_{i}\).

_FMD and JD Critics._ For the video critics, our PLF-JD critic architecture is inspired by the work of Kwon and Park [40], where we concatenate frames sequentially along their channel dimensions. For both PLF-FMD and PLF-JD critics, we add spectral normalization layers for better convergence. Their architecture is shown in Table 5.

_Rate and output dimension_ The rate \(R\) is computed by \(\log_{2}(d_{enc}{\times}L)\), where \(L\) is the number of quantization levels and \(d_{enc}{=}d_{r}{+}d_{m}\). Table 6 provides configurations of the rate, \(d_{m},d_{r}\), and \(L\) in the experiment.

_Training Details:_ We use a batch size of 64, RMSProp optimizer with a learning rate of \(5{\times}10^{-5}\), and train each model with \(360\) epochs, where the training set contains 60000 images. To accelerate

training, we pre-train each model for 60 epochs with the MSE objective only. Under WGAN-GP framework [30], we use the gradient penalty of 10 and update the encoders/decoders for every 5 iterations. The parameters \(\lambda\) controlling the tradeoff are in Table.7. Training takes 2 days per model on a single NVIDIA P100 GPU. For the MovingMNIST factor of two bound and permanence of error experiments, we repeat the training 3 times.

### Permanence of Error on KTH Datasets

The KTH dataset is a widely-used benchmark dataset in computer vision research, consisting of video sequences of human actions performed in various scenarios. We show more examples supporting our argument for the permanence of error on this realistic dataset. We use 16 bits for each frame. In general, the 0-PLF-JD decoder consistently outputs correlated but incorrect reconstructions due to the error induced by the first reconstructions, i.e., the P-frames will follow the wrong direction induced from the I-frame reconstruction. Besides the moving direction, we also notice that the type of actions (i.e. walking, jogging, and running) is also affected. On the other hand, while losing some temporal cohesion, MMSE and 0-PLF FMD decoders manage to fix the movement error.

Finally, we computed LPIPS [41] in Table 8, which is known as a full-reference perceptual metric for images. We compare LPIPS for each reconstruction (MMSE, 0-PLF-FMD and 0-PLF-JD) at each timeframe. This result aligns with our results on distortion loss, as the MMSE and 0-PLF-FMD tend to correct the reconstruction (so the ground truth and reconstruction look more similar over time). On the other hand, due to error permanence, the 0-PLF-JD reconstructions become different from their source sequence, causing the score to go up.

\begin{table}

\end{table}
Table 6: Rate, embedding dimension \(d_{m},d_{r}\) and quantization level \(L\).

\begin{table}

\end{table}
Table 4: Residual Encoder, Conditional Module, and Residual Decoder.

\begin{table}

\end{table}
Table 5: PLF-FMD and PLF-JD critic for frame \(i\).

### RDP Tradeoff for 3 frames

We extend our experimental results for the RDP-tradeoff and the principal of universality to the case of GOP size 3. As mentioned in the main paper, while the universal model only requires MMSE representations, the optimal end-to-end model also requires the MMSE reconstructions from previous frames to provide best estimates for motion flow vectors. Practically, this is challenging for our employed architecture since only previous \(\hat{X}_{1},\hat{X}_{2}\) are available. As a result, to compare the RDP tradeoff between universal and end-to-end model, we also provide the end-to-end model with the MMSE estimate from previous frames while noting that this is unfeasible in practice. Interestingly, we show in Figure 9 the RDP tradeoff curves for the third frame \(X_{3}\) and its reconstruction \(\hat{X}_{3}\)

\begin{table}
\begin{tabular}{|c|c|c|} \hline Perception Loss & \multicolumn{2}{c|}{\(\lambda\times 10^{-3}\)} \\ \hline \hline Joint Distance (JD) & \multicolumn{2}{c|}{\(0.0,0.7,1.0,1.15,1.2,1.25,1.3,1.5,1.7\)} \\  & \multicolumn{2}{c|}{\(2.0,3.0,5.0,8.0,10.0,40.0,80.0\)} \\ \hline Frame Marginal Distance (FMD) & \(0.0,0.4,0.7,1.0,1.5,2.0,2.5,3.0,3.5,4.0,7.0,10.0,40.0\) \\ \hline \end{tabular}
\end{table}
Table 7: Perception loss and their associated \(\lambda\)

Figure 8: Additional Experimental Results for the Permanence of Error Phenomenon on KTH Dataset.

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline  & MMSE & PLF-FMD & PLF-JD \\ \hline
1st frame & 0.1036 & 0.0584 & 0.0584 \\
2nd frame & 0.0521 & 0.0313 & 0.0594 \\
3rd frame & 0.0413 & 0.0232 & 0.0613 \\ \hline \end{tabular}
\end{table}
Table 8: LPIPS score on KTH dataset (lower is better).

observing that the universal and optimized curves are still relatively close to each other. When \((R_{1},R_{2},R_{3}){=}(\infty,\epsilon,\epsilon)\), we note that the distortion for \(X_{3}\) is larger than \(X_{2}\) since the allocated rate is not enough to correct the motion. Finally, for the case \((R_{1},R_{2},R_{3}){=}(\epsilon,\epsilon,\epsilon)\), we note that the curves again converge as in the case of \((R_{1},R_{2}){=}(\epsilon,\epsilon)\) due to the incorrect reconstruction in the I-frame.

### Diversity and Correlation

When \((R_{1},R_{2}){=}(\infty,\epsilon)\), our theoretical analysis predicted that the decoder optimized for JD is capable of producing diverse reconstructions. On the other hand, an optimized decoder for FMD will tend to produce reconstructions that are highly correlated with the previous reconstruction \(\hat{X}_{1}\)6. In our experiment, we also observe such behavior, summarized in Table 9 and show several examples for \(R_{2}=2\) bits in Figure 10. We observe that reconstructions from the joint metric deviate more randomly from \(X_{1}\) than the marginal reconstructions. The marginal reconstructions, on the other hand, stay much closer to their original reconstruction \(\hat{X}_{1}\).

Footnote 6: \(X_{1}=\hat{X}_{1}\) in this regime.

We measure the diversity in \(\hat{X}_{2}\) reconstruction using \(\mathrm{E}[\text{Var}(\hat{X}_{2}|X_{1},X_{2})]\) and the correlation with \(\hat{X}_{1}\) by \(\mathrm{E}[\text{sim}(\hat{X}_{2},\mathrm{X}_{1})]\), where \(\text{sim}(u,v)\) is the cosine distance between \(u,v\). Table 9(a) shows that as we increase the number of bits in \(R_{2}\), the diversity decreases as the decoder can reconstruct the frame more precisely. In Table 9(b), we see that the joint metric keeps the correlation relatively constant, showing that it actually preserves the temporal consistency. On the other hand, as the rate becomes larger, 0-PLF-FMD reconstruction tends to be less correlated with the previous frame \(X_{1}\). Finally, we note that our architecture innately utilizes common randomness to produce diverse reconstructions and does not suffer from mode-collapse behavior in general conditional GAN settings [42].

Figure 10: Diversity in reconstruction \(\hat{X}_{2}\) for 0-PLF-JD and correlation with previous frames \(\hat{X}_{1}\) for 0-PLF-JMD. We show \(X_{1}\) in the first column. From the second column, the light-dark region represents \(X_{1}\) and the color digit represents \(X_{2},\hat{X}_{2}\). For each perception metric, we show two samples.

Figure 9: RDP tradeoff curves for end-to-end and universal models. We plot the tradeoff for the two regimes: \(R_{1}{=}\infty\) and \(R_{1}{=}\epsilon\) in (a) and (b) respectively. The universal and optimal curves are close to each other.

## Appendix K Error Permanence on UVG Dataset

We demonstrate the phenomenon of error permanence in a large-scale scenario using the UVG dataset. Our P-frame compressor is trained on the Vimeo-90k dataset. As illustrated in Figure 11, when reconstructing I-frames, an inaccurate color tone is introduced, which persists when employing PLF-JD. However, PLF-FMD effectively rectifies this issue within P-frames. Numerical results are

Figure 11: Visualization of error permanence. The PLF-JD reconstructions propagate the flaws in the color tone from the previous I-frame reconstruction while the PLF-FMD is able to fix such error. Compression rate for I-frame and P-frame are 0.144bpp (low rate) and 4.632bpp (high rate) respectively.

in Table. 10. We note that for the I-frame, we use the pretrained model in [43] that targets high perceptual quality and is also trained on Vimeo-90k dataset.

## Appendix L Limitations

This work studies the effects of different perception loss functions, namely the PLF-JD and PLF-FMD, on the performance of lossy causal video compression. Our theoretical analysis and experiment reveal the error permanence phenomenon and show the universality principle, suggesting that MMSE representation can be transformed into other points on the DP tradeoffs.

In practice, one might want to combine these two losses, for example, perfect framewise realism (0-PLF FMD) while retaining some degree of temporal cohesion (PLF-JD small), which is not considered in this work. Furthermore, analysis for other types of video compression schemes, such as with B-frame, and scaling the universality compression architecture to high-definition videos are also desired.

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline  & MMSE & PLF-FMD & PLF-JD \\ \hline Distortion (MSE) & 0.0026 & 0.0032 & 0.0168 \\ \hline \end{tabular}
\end{table}
Table 10: Distortion \((X_{2}-\bar{X}_{2})^{2}\) evaluated across 3900 UVG frames. The PLF-JD reconstructions exhibit notably greater distortion compared to MMSE and PLF-FMD, aligning with our theoretical findings.