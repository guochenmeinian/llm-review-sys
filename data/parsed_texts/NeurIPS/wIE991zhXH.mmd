# Bandits with Preference Feedback:

A Stackelberg Game Perspective

 Barna Pasztor\({}^{\star,1,2}\)   Parnian Kassraie\({}^{\star,1}\)   Andreas Krause\({}^{1,2}\)

\({}^{1}\)ETH Zurich  \({}^{2}\)ETH AI Center

{bpasztor, pkassraie, krausea}@ethz.ch

Equal contribution.

###### Abstract

Bandits with preference feedback present a powerful tool for optimizing unknown target functions when only pairwise comparisons are allowed instead of direct value queries. This model allows for incorporating human feedback into online inference and optimization and has been employed in systems for fine-tuning large language models. The problem is well understood in simplified settings with linear target functions or over finite small domains that limit practical interest. Taking the next step, we consider infinite domains and nonlinear (kernelized) rewards. In this setting, selecting a pair of actions is quite challenging and requires balancing exploration and exploitation at two levels: within the pair, and along the iterations of the algorithm. We propose MaxMinLCB, which emulates this trade-off as a zero-sum Stackelberg game, and chooses action pairs that are informative and yield favorable rewards. MaxMinLCB consistently outperforms existing algorithms and satisfies an anytime-valid rate-optimal regret guarantee. This is due to our novel preference-based confidence sequences for kernelized logistic estimators.

## 1 Introduction

In standard bandit optimization, a learner repeatedly interacts with an unknown environment that gives numerical feedback on the chosen actions according to a utility function \(f\). However, in applications such as fine-tuning large language models, drug testing, or search engine optimization, the quantitative value of design choices or test outcomes are either not directly observable, or are known to be inaccurate, or systematically biased, e.g., if they are provided by human feedback [10]. A solution is to optimize for the target based on comparative feedback provided for a pair of queries, which is proven to be more robust to certain biases and uncertainties in the queries [11].

Bandits with preference feedback, or _dueling_ bandits, address this problem and propose strategies for choosing query/action pairs that yield a high utility over the horizon of interactions. At the core of such algorithms is uncertainty quantification and inference for \(f\) in regions of interest, which is closely tied to exploration and exploitation dilemma over a course of queries. Observing only comparative feedback poses an additional challenge, as we now need to balance this trade-off _jointly_ over two actions. This challenge is further exacerbated when optimizing over vast or infinite action domains. As a remedy, prior work often _grounds_ one of the actions by choosing it either randomly or greedily, and tries to balance exploration-exploitation for the second action as a reaction to the first [1, 13, 14]. This approach works well for simple utility functions over low-dimensional domains, however does not scale to more complex problems.

Aiming to solve this problem, we focus on continuous domains in the Euclidean vector space and complex utility functions that belong to the Reproducing Kernel Hilbert Space (RKHS) of a potentially non-smooth kernel. We propose MaxMinLCB, a sample-efficient algorithm that at every step chooses the actions _jointly_, by playing a zero-sum Stackelberg (a.k.a Leader-Follower) game. We choose the Lower Confidence Bound (LCB) of \(f\) as the objective of this game which the Leader aimsto maximize and the Follower to minimize. The equilibrium of this game yields an action pair in which the first action is a favorable candidate to maximize \(f\) and the second action is the strongest competitor against the first. Our choice of using the LCB as the objective leads to robustness against uncertainty when selecting the first action. Moreover, it makes the second action an optimistic choice as a competitor, from its own perspective. We observe empirically that this approach creates a natural exploration scheme, and in turn, yields a more sample-efficient algorithm compared to standard baselines.

Our game-theoretic strategy leads to an efficient bandit solver, if the LCB is a valid and tight lower bound on the utility function. To this end, we construct a confidence sequence for \(f\) given pairwise preference feedback, by modeling the noisy comparative observations with a logistic-type likelihood function. Our confidence sequence is anytime valid and holds uniformly over the domain, under the assumption that \(f\) resides in an RKHS. We improve prior work by removing or relaxing assumptions on the utility while maintaining the same rate of convergence. This result on preference-based confidence sequences may be of independent interest, as it targets the loss function that is typically used for Reinforcement Learning with Human Feedback.

**Contributions** Our main contributions are:

* We propose a novel game-theoretic acquisition function for pairwise action selection with preference feedback.
* We construct preference-based confidence sequences for kernelized utility functions that are tight and anytime valid.
* Together this creates MaxMinLCB, an algorithm for bandit optimization with preference feedback over continuous domains. MaxMinLCB satisfies \(\mathcal{O}(\gamma_{T}\sqrt{T})\) regret, where \(T\) is the horizon and \(\gamma_{T}\) is the _information gain_ of the kernel.
* We benchmark MaxMinLCB over a set of standard optimization problems and consistently outperform the common baselines from the literature.

## 2 Related Work

Learning with indirect feedback was first studied in the supervised preference learning setting (Aiolli and Sperduti, 2004; Chu and Ghahramani, 2005). Subsequently, online and sequential settings were considered, motivated by applications in which the feedback is provided in an online manner, e.g., by a human (Yue et al., 2012; Yue and Joachims, 2009; Houlsby et al., 2011). Bengs et al. (2021) surveys this field comprehensively; here we include a brief background.

Referred to as dueling bandits, a rich body of work considers (finite) multi-armed domains and learns a preference matrix specifying the relation among the arms. Such work often relies on efficient sorting or tournament systems based on the frequency of wins for each action (Jamieson and Nowak, 2011; Zoghi et al., 2014; Komiyama et al., 2015; Wu and Liu, 2016; Falahatgar et al., 2017). Rather than jointly selecting the arms, such strategies often simplify the problem by selecting one at random (Zoghi et al., 2014; Zimmert and Seldin, 2018), greedily (Chen and Frazier, 2017), or from the set of previously selected arms (Ailon et al., 2014). In contrast, we jointly optimize both actions by choosing them as the equilibrium of a two-player zero-sum Stackelberg game, enabling a more efficient exploration/exploitation trade-off.

The multi-armed dueling setting, which is reducible to multi-armed bandits (Ailon et al., 2014), naturally fails to scale to infinite compact domains, since regularity among "similar" arms is not exploited. To go beyond finite domains, _utility-based_ dueling bandits consider an unknown latent function that captures the underlying preference, instead of relying on a preference matrix. The preference feedback is then modeled as the difference in the utility of two chosen actions passed through a link function. Early work is limited to convex domains and imposes strong regularity assumptions (Yue and Joachims, 2009; Kumagai, 2017). These assumptions are then relaxed to general compact domains if the utility function is linear (Dudik et al., 2015; Saha, 2021; Saha and Krishnamurthy, 2022). Constructing valid confidence sets from comparative feedback is a challenging task. However, it is strongly related to uncertainty quantification with direct logistic feedback, which is extensively analyzed by the literature on logistic and generalized linear bandits (Filippi et al., 2010; Faury et al., 2020; Foster and Krishnamurthy, 2018; Beygelzimer et al., 2019; Faury et al., 2022; Lee et al., 2024).

Preference-based bandit optimization with linear utility functions is well understood and is even extended to reinforcement learning with preference feedback on trajectories (Saha et al., 2023; Zhan et al., 2023; Zhu et al., 2023; Ji et al., 2023; Munos et al., 2023). However, such approaches have limited practical interest, since they cannot capture real-world problems with complex nonlinear utility functions. Alternatively, Reproducing Kernel Hilbert Spaces (RKHS) provide a rich model class for the utility, e.g., if the chosen kernel is universal. Many have proposed heuristic algorithms for bandits and Bayesian optimization in kernelized settings, albeit without providing theoretical guarantees Brochu et al. (2010); Gonzalez et al. (2017); Sui et al. (2017); Tucker et al. (2020); Mikkola et al. (2020); Takeno et al. (2023).

There have been attempts to prove convergence of kernelized algorithms for preference-based bandits (Xu et al., 2020; Kirschner and Krause, 2021; Mehta et al., 2023b, a). Such works employ a regression likelihood model which requires them to assume that both the utility and the probability of preference, as a function of actions, lie in an RKHS. In doing so, they use a regression model for solving a problem that is inherently a classification. While the model is valid, it does not result in a sample-efficient algorithm. In contrast, we use a kernelized logistic negative log-likelihood loss to infer the utility function, and provide confidence sets for its minimizer. In a concurrent work, Xu et al. (2024) also consider the kernelized logistic likelihood model and propose a variant of the MultiSBM algorithm (Ailon et al., 2014) which uses likelihood ratio confidence sets. The theoretical approach and resulting algorithm bear significant differences, and the regret guarantee has a strictly worse dependency on the time horizon \(T\), by a factor of \(T^{1/4}\). This is discussed in more detail in Section 5.

## 3 Problem Setting

Consider an agent which repeatedly interacts with an environment: at step \(t\) the agent selects two actions \(\bm{x}_{t},\,\bm{x}^{\prime}_{t}\in\mathcal{X}\) and only observes stochastic binary feedback \(y_{t}\in\{0,1\}\) indicating if \(\bm{x}_{t}\succ\bm{x}^{\prime}_{t}\), that is, if action \(\bm{x}_{t}\) is _preferred_ over action \(\bm{x}^{\prime}_{t}\). Formally, \(\mathbb{P}(y_{t}=1|\bm{x}_{t},\bm{x}^{\prime}_{t})=\mathbb{P}(\bm{x}_{t}\succ \bm{x}^{\prime}_{t})\), and \(y_{t}=0\) with probability \(1-\mathbb{P}(\bm{x}_{t}\succ\bm{x}^{\prime}_{t})\). Based on the preference history \(H_{t}=\{(\bm{x}_{1},\bm{x}^{\prime}_{1},y_{1}),\ldots(\bm{x}_{t},\bm{x}^{ \prime}_{t},y_{t})\}\), the agent aims to sequentially select favorable action pairs. Over a horizon of \(T\) steps, the success of the agent is measured through the _cumulative dueling regret_

\[R^{\mathrm{D}}(T)=\sum_{t=1}^{T}\frac{\mathbb{P}(\bm{x}^{\star}\succ\bm{x}_{t })+\mathbb{P}(\bm{x}^{\star}\succ\bm{x}^{\prime}_{t})-1}{2},\] (1)

which is the average sub-optimality gap between the chosen pair and a globally preferred action \(\bm{x}^{\star}\). To better understand this notion of regret, consider the scenario where actions \(\bm{x}_{t}\) and \(\bm{x}^{\prime}_{t}\) are both optimal. Then the probabilities are equal to \(0.5\) and the dueling regret will not grow further, since the regret incurred at step \(t\) is zero. This formulation of \(R^{\mathrm{D}}(T)\) is commonly used in the literature of dueling Bandits and RL with preference feedback (Urvoy et al., 2013; Saha et al., 2023; Zhu et al., 2023) and is adapted from Yue et al. (2012). Our goal is to design an algorithm that satisfies a _sublinear_ dueling regret, where \(R^{\mathrm{D}}(T)/T\to 0\) as \(T\to\infty\). This implies that given enough evidence, the algorithm will converge to a globally preferred action. To this end, we take a utility-based approach and consider an unknown utility function \(f:\mathcal{X}\to\mathbb{R}\), which reflects the preference via

\[\mathbb{P}(\bm{x}_{t}\succ\bm{x}^{\prime}_{t})\coloneqq s\left(f(\bm{x}_{t})- f(\bm{x}^{\prime}_{t})\right)\] (2)

where \(s:\mathbb{R}\to[0,1]\) is the sigmoid function1, i.e. \(s(a)=(1+e^{-a})^{-1}\). Referred to as the Bradley-Terry model (Bradley and Terry, 1952), this probabilistic model for binary feedback is widely used in the literature for logistic and generalized bandits (Filippi et al., 2010; Faury et al., 2020). Under the utility-based model, \(\bm{x}^{\star}=\arg\max_{\bm{x}\in\mathcal{X}}f(\bm{x})\) and we can draw connections to a classic bandit problem with direct feedback over the utility \(f\). In particular, Saha (2021) shows that the dueling regret of (1) is _equivalent_ up to constant factors, to the average _utility_ regret of the two actions, that is \(\sum_{t=1}^{T}f(\bm{x}^{\star})-[f(\bm{x}_{t})+f(\bm{x}^{\prime}_{t})]/2\).

Footnote 1: May be generalized to differentiable monotonically increasing functions satisfying \(s(x)+s(-x)=1\).

Throughout this paper, we make two key assumptions over the environment. We assume that the domain \(\mathcal{X}\subset\mathbb{R}^{d_{0}}\) is compact, and that the utility function lies in \(\mathcal{H}_{k}\), a Reproducing Kernel Hilbert Space corresponding to some kernel function \(k\in\mathcal{X}\times\mathcal{X}\to\mathbb{R}\) with a bounded RKHS norm \(\left\|f\right\|_{k}\leq B\). Without a loss of generality, we further suppose that the kernel function is normalized and \(k(\bm{x},\bm{x})\leq 1\) everywhere in the domain. Our set of assumptions extends the prior literature on logistic bandits and dueling bandits from linear rewards or finite action spaces, to continuous domains with non-parametric rewards.

While our theoretical framework targets euclidean domains, our methodology may be used on general domains of text or images, given vector embeddings obtained via unsupervised learning. Solving a bandit problem on top of embeddings from a pretrained language model is common practice in further fine-tuning of such models (e.g., Nguyen et al., 2024; Mehta et al., 2023a), and is further demonstrated in our Yelp experiment (c.f Section 6.3). Lastly, we highlight that our results may be smoothly extended to contextual bandits with stochastic context, by simply modifying the signature of the kernel function to \(k^{\prime}(\bm{x},\bm{x}^{\prime},\bm{z}):\mathcal{X}\times\mathcal{X}\times \mathcal{Z}\to\mathbb{R}\), where \(\bm{z}\in\mathcal{Z}\) denotes the context. This setting accommodates applications in active learning for fine-tuning of large language models, where the context is the prompt and the two actions are two alternative responses.

## 4 Kernelized Confidence Sequences with Direct Logistic Feedback

As a warm-up, we consider a hypothetical scenario where \(\bm{x}^{\prime}_{t}=\bm{x}_{\mathrm{null}}\) for all \(t\geq 1\) such that \(f(\bm{x}_{\mathrm{null}})=0\). Therefore at every step, we suggest an action \(\bm{x}_{t}\) and receive a noisy binary feedback \(y_{t}\), which is equal to one with probability \(s(f(\bm{x}_{t}))\). This example reduces our problem to logistic bandits which has been rigorously analyzed for linear rewards (Filippi et al., 2010; Faury et al., 2020). We extend prior work to the non-parametric setting by proposing a tractable loss function for estimating the utility function, a.k.a. reward. We present novel confidence intervals that quantify the uncertainty on the logistic predictions _uniformly_ over the action domain. In doing so, we propose confidence sequences for the kernelized logistic likelihood model that are of independent interest for developing sample-efficient solvers for online and active classification.

The feedback \(y_{t}\) is a Bernoulli random variable, and its likelihood depends on the utility function as \(s(f(\bm{x}_{t}))^{y_{t}}[1-s(f(\bm{x}_{t}))]^{1-y_{t}}\). Then given history \(H_{t}\), we can estimate \(f\) by \(f_{t}\), the minimizer of the regularized negative log-likelihood loss

\[\mathcal{L}_{k}^{\mathrm{L}}(f;H_{t})\coloneqq\sum_{\tau=1}^{t}-y_{\tau}\log \left[s(f(\bm{x}_{\tau}))\right]-(1-y_{\tau})\log\left[1-s(f(\bm{x}_{\tau})) \right]+\frac{\lambda}{2}\norm{f}_{k}^{2}\] (3)

where \(\lambda>0\) is the regularization coefficient. The regularization term ensures that \(\norm{f_{t}}_{k}\) is finite and bounded. For simplicity, we assume throughout the main text that \(\norm{f_{t}}_{k}\leq B\). However, we do not need to rely on this assumption to give theoretical guarantees. In the appendix, we present a more rigorous analysis by projecting \(f_{t}\) back into the RKHS ball of radius \(B\) to ensure that the \(B\)-boundedness condition is met, instead of assuming it. We do not perform this projection in our experiments.

Solving for \(f_{t}\) may seem intractable at first glance since the loss is defined over functions in the large space of \(\mathcal{H}_{k}\). However, it is common knowledge that the solution has a parametric form and may be calculated by using gradient descent. This is a direct application of the Representer Theorem (Scholkopf et al., 2001) and is detailed in Proposition 1.

**Proposition 1** (Logistic Representer Theorem).: _The regularized negative log-likelihood loss of (3) has a unique minimizer \(f_{t}\), which takes the form \(f_{t}(\cdot)=\sum_{\tau=1}^{t}\alpha_{\tau}k(\cdot,\bm{x}_{\tau})\) where \((\alpha_{1},\ldots\alpha_{t})=:\bm{\alpha}_{t}\in\mathbb{R}^{t}\) is the minimizer of the strictly convex loss_

\[\mathcal{L}_{k}^{\mathrm{L}}(\bm{\alpha};H_{t})=\sum_{\tau=1}^{t}-y_{\tau} \log\left[s(\bm{\alpha}^{\top}\bm{k}_{t}(\bm{x}_{\tau}))\right]-(1-y_{\tau}) \log\left[1-s(\bm{\alpha}^{\top}\bm{k}_{t}(\bm{x}_{\tau}))\right]+\frac{ \lambda}{2}\norm{\bm{\alpha}}_{2}^{2}\]

_with \(\bm{k}_{t}(\bm{x})=(k(\bm{x}_{1},\bm{x}),\ldots,k(\bm{x}_{t},\bm{x}))\in \mathbb{R}^{t}\)._

Given \(f_{t}\), we may predict the expected feedback for a point \(\bm{x}\) as \(s(f_{t}(\bm{x}))\). Centered around this prediction, we construct confidence sets of the form \([s(f_{t}(\bm{x}))\pm\beta_{t}(\delta)\sigma_{t}(\bm{x})]\), and show their uniform anytime validity. The width of the sets are characterized by \(\sigma_{t}(\bm{x})\) defined as

\[\sigma_{t}^{2}(\bm{x})\coloneqq k(\bm{x},\bm{x})-\bm{k}_{t}^{\top}(\bm{x})(K_{ t}+\lambda\kappa\bm{I}_{t})^{-1}\bm{k}_{t}(\bm{x})\] (4)

where \(\kappa=\sup_{a\leq B}1/\dot{s}(a)\), with \(\dot{s}(a)=s(a)(1-s(a))\) denoting the derivative of the sigmoid function, and \(K_{t}\in\mathbb{R}^{t\times t}\) is the kernel matrix satisfying \([K_{t}]_{i,j}=k(\bm{x}_{i},\bm{x}_{j})\). Our first main result shows that for a careful choice of \(\beta_{t}(\delta)\), these sets contain \(s(f(\bm{x}))\) simultaneously for all \(\bm{x}\in\mathcal{X}\) and \(t\geq 1\) with probability greater than \(1-\delta\).

**Theorem 2** (Kernelized Logistic Confidence Sequences).: _Assume \(f\in\mathcal{H}_{k}\) and \(\norm{f}_{k}\leq B\). Let \(0<\delta<1\) and set_

\[\beta_{t}(\delta)\coloneqq 4LB+2L\sqrt{\frac{2\kappa}{\lambda}(\gamma_{t}+\log 1/ \delta)},\] (5)

_where \(\gamma_{t}\coloneqq\max_{\bm{x}_{1},\ldots,\bm{x}_{t}}\frac{1}{2}\log\det(\bm{ I}_{t}+(\lambda\kappa)^{-1}K_{T})\), and \(L\coloneqq\sup_{a\leq B}\dot{s}(a)\). Then_

\[\mathbb{P}\left(\forall t\geq 1,\bm{x}\in\mathcal{X}:\,|s\left(f_{t}(\bm{x}) \right)-s\left(f(\bm{x})\right)|\leq\beta_{t}(\delta)\sigma_{t}(\bm{x})\right) \geq 1-\delta.\]Function-valued confidence sets around the kernelized ridge estimator are analyzed and used extensively to design bandit algorithms with noisy feedback on the true reward values (Valko et al., 2013; Srinivas et al., 2010; Chowdhury and Gopalan, 2017; Whitehouse et al., 2023). However, under noisy logistic feedback, this literature falls short as the proposed confidence sets are no longer valid for the kernelized logistic estimator \(f_{t}\). One could still estimate \(f\) using a kernelized ridge estimator and benefit from this line of work. However, as empirically demonstrated in Figure 0(a), this will not be a sample-efficient approach.

**Proof Sketch.** When minimizing the kernelized logistic loss, we do not have a closed-form solution for \(f_{t}\), and can only formulate it using the fact that the gradient of the loss evaluated at \(f_{t}\) is the null operator, i.e., \(\nabla\mathcal{L}(f_{t};H_{t}):\mathcal{H}\rightarrow\mathcal{H}=\mathbf{0}\). The key idea of our proof is to construct confidence intervals as \(\mathcal{H}\)-valued ellipsoids in the _gradient space_ and show that the gradient operator evaluated at \(f\) belongs to it with high probability (c.f. Lemma 8). We then translate this back into intervals around point estimates \(s(f_{t}(\bm{x}))\) uniformly for all points \(\bm{x}\in\mathcal{X}\). The complete proof is deferred to Appendix A, and builds on the results of Faury et al. (2020) and Whitehouse et al. (2023).

**Logistic Bandits.** Such confidence sets are an integral tool for action selection under uncertainty, and bandit algorithms often rely on them to balance exploration against exploitation. To demonstrate how Theorem 2 may be used for bandit optimization with direct logistic feedback, we consider LGP-UCB, the kernelized Logistic GP-UCB algorithm. Presented in Algorithm 2, it extends the optimistic algorithm of Faury et al. (2020) from the linear to the kernelized setting, by using the confidence bound of Theorem 2 to calculate an optimistic estimate of the reward. We proceed to show that LGP-UCB attains a sublinear logistic regret, which is commonly defined as

\[R^{\mathrm{L}}(T)=\sum_{i=1}^{T}s(f(\bm{x}^{\star}))-s(f(\bm{x}_{t})).\]

To the best of our knowledge, the following corollary presents the first regret bound for logistic bandits in the kernelized setting and may be of independent interest.

**Corollary 3**.: _Let \(\delta\in(0,1]\) and choose the exploration coefficients \(\beta_{t}(\delta)\) as described in Theorem 2 for all \(t\geq 0\). Then LGP-UCB satisfies the anytime cumulative regret guarantee of_

\[\mathbb{P}\left(\forall T\geq 0:R^{\mathrm{L}}(T)\leq C_{L}\beta_{T}(\delta) \sqrt{T\gamma_{t}}\right)\geq 1-\delta.\]

_where \(C_{L}\coloneqq\sqrt{8/\log(1+(\lambda\kappa)^{-1})}\)._

## 5 Main Results: Bandits with Preference Feedback

We return to our main problem setting in which a pair of actions, \(\bm{x}_{t}\) and \(\bm{x}^{\prime}_{t}\), are chosen and the observed response is a noisy binary indicator of \(\bm{x}_{t}\) yielding a higher utility than \(\bm{x}^{\prime}_{t}\). While this type of feedback is more consistent in practice, it creates quite a challenging problem compared to the logistic problem of Section 4. The search space for action pairs \(\mathcal{X}\times\mathcal{X}\) is significantly larger than \(\mathcal{X}\), and the observed preference feedback of \(s(f(\bm{x}_{t})-f(\bm{x}^{\prime}_{t}))\) conveys only relative information between two actions. We start by presenting a solution to estimate \(f\) and obtain valid confidence sets under preference feedback. Using these confidence sets we then design the MaxMinLCB algorithm which chooses action pairs that are not only favorable, i.e., yield high utility, but are also informative for improving the utility confidence sets.

### Preference-based Confidence Sets

We consider the probabilistic model of \(y_{t}\) as stated in Section 3, and write the corresponding regularized negative loglikelihood loss as

\[\begin{split}\mathcal{L}_{k}^{\mathrm{D}}(f;H_{t})\coloneqq \sum_{\tau=1}^{t}&-y_{\tau}\log\left[s(f(\bm{x}_{\tau})-f(\bm{x}^ {\prime}_{\tau}))\right]\\ &-(1-y_{\tau})\log\left[1-s\left(f(\bm{x}_{\tau})-f(\bm{x}^{\prime }_{\tau})\right)\right]+\frac{\lambda}{2}\norm{f}_{k}^{2}.\end{split}\] (6)

This loss may be optimized over different function classes and is commonly used for linear dueling bandits (e.g., Saha, 2021), and has been notably successful in reinforcement learning with human feedback (Christiano et al., 2017). We proceed to show that the preference-based loss \(\mathcal{L}_{k}^{\mathrm{D}}\) is equivalent to \(\mathcal{L}_{k^{\mathrm{D}}}^{\mathrm{L}}\), the standard logistic loss (3) invoked with a specific kernel function \(k^{\mathrm{D}}\). This will allow us to cast the problem of inference with preference feedback as a kernelized logistic regression problem. To this end, we define the _dueling kernel_ as

\[k^{\mathrm{D}}\big{(}(\bm{x}_{1},\bm{x}_{1}^{\prime}),(\bm{x}_{2},\bm{x}_{2}^{ \prime})\big{)}\coloneqq k(\bm{x}_{1},\bm{x}_{2})+k(\bm{x}_{1}^{\prime},\bm{x} _{2}^{\prime})-k(\bm{x}_{1},\bm{x}_{2}^{\prime})-k(\bm{x}_{1}^{\prime},\bm{x} _{2})\]

for all \((\bm{x}_{1},\bm{x}_{1}^{\prime}),(\bm{x}_{2},\bm{x}_{2}^{\prime})\in\mathcal{ X}\times\mathcal{X}\), and let \(\mathcal{H}_{k^{\mathrm{D}}}\) be the RKHS corresponding to it. While the two function spaces \(\mathcal{H}_{k^{\mathrm{D}}}\) and \(\mathcal{H}_{k}\) are defined over different input domains, we can show that they are isomorphic, under simple regularity conditions.

**Proposition 4**.: _Let \(f:\mathcal{X}\to\mathbb{R}\). Consider a kernel \(k\) and the sequence of its eigenfunctions \((\phi_{i})_{i=1}^{\infty}\). Assume the eigenfunctions are zero-mean, i.e. \(\int_{\bm{x}\in\mathcal{X}}\phi_{i}(\bm{x})\mathrm{d}\bm{x}=0\). Then \(f\in\mathcal{H}_{k}\), if and only if there exists \(h\in\mathcal{H}_{k^{\mathrm{D}}}\) such that \(h(\bm{x},\bm{x}^{\prime})=f(\bm{x})-f(\bm{x}^{\prime})\). Moreover, \(\left\lVert h\right\rVert_{k^{\mathrm{D}}}=\left\lVert f\right\rVert_{k^{ \mathrm{F}}}\)._

The proof is left to Appendix C.1. The assumption on eigenfunctions in Proposition 4 is primarily made to simplify the equivalence class. In particular, the relative preference function \(h\) can only capture the utility \(f\) up to a bias, i.e., if a constant bias \(b\) is added to all values of \(f\), the corresponding \(h\) function will not change. The value of \(b\) may not be recovered by drawing queries from \(h\), however, this will not cause issues in terms of identifying \(\arg\max\) of \(f\) through querying values of \(h\). Therefore, we set \(b=0\) by assuming that eigenfunctions of \(k\) are zero-mean. This assumption automatically holds for all kernels that are translation or rotation invariant over symmetric domains, since their eigenfunctions are periodic \(L_{2}(\mathcal{X})\) basis functions, e.g., Matern kernels and sinusoids.

Proposition 4 allows us to re-write the preference-based loss function of (6) as a logistic-type loss

\[\mathcal{L}_{k^{\mathrm{D}}}^{\mathrm{L}}(h;H_{t})=\sum_{\tau=1}^{t}-y_{\tau} \log\left[s(h(\bm{x}_{\tau},\bm{x}_{\tau}^{\prime}))\right]-(1-y_{\tau})\log \left[1-s\left(h(\bm{x}_{\tau},\bm{x}_{\tau}^{\prime})\right)\right]+\frac{ \lambda}{2}\lVert h\rVert_{k^{\mathrm{D}}}^{2},\]

that is equivalent to (3) up to the choice of kernel. We define the minimizer \(h_{t}\coloneqq\arg\min\mathcal{L}_{k^{\mathrm{D}}}^{\mathrm{L}}(h;H_{t})\) and use it to construct anytime valid confidence sets for the utility \(f\) given only preference feedback.

**Corollary 5** (Kernelized Preference-based Confidence Sequences).: _Assume \(f\in\mathcal{H}_{k}\) and \(\left\lVert f\right\rVert_{k}\leq B\). Choose \(0<\delta<1\) and set \(\beta_{t}^{\mathrm{D}}(\delta)\) and \(\sigma_{t}^{\mathrm{D}}\) as in equations (4) and (5), with \(k^{D}\) used as the kernel function. Then,_

\[\mathbb{P}\left(\forall t\geq 1,\bm{x},\bm{x}^{\prime}\in\mathcal{X}:\left|s \left(h_{t}(\bm{x},\bm{x}^{\prime})\right)-s\left(f(\bm{x})-f(\bm{x}^{\prime}) \right)\right|\leq\beta_{t}^{\mathrm{D}}(\delta)\sigma_{t}^{D}(\bm{x},\bm{x}^{ \prime})\right)\geq 1-\delta.\]

_where \(h_{t}=\arg\min\mathcal{L}_{k^{\mathrm{D}}}^{\mathrm{L}}(h;H_{t})\)._

Corollary 5 gives valid confidence sets for kernelized utility functions under preference feedback and immediately improves prior results on linear dueling bandits and kernelized dueling bandits with regression-type loss, to kernelized setting with logistic-type likelihood. To demonstrate this, in Appendix C.3 we present the kernelized extensions of MaxInP(Saha, (2021), Algorithm 3), and IDS(Kirschner and Krause, (2021), Algorithm 4) and prove the corresponding regret guarantees (cf. Theorems 15 and 16). Corollary 5 holds almost immediately by invoking Theorem 2 with the dueling kernel \(k^{\mathrm{D}}\) and applying Proposition 4. A proof is provided in Appendix C.1 for completeness.

**Comparison to Prior Work.** A line of previous work assumes that both \(f\) and the probability \(s(f(\bm{x}))\) are \(B\)-bounded members of \(\mathcal{H}_{k}\). This allows them to directly estimate \(s(f(\bm{x}))\) via kernelized linear regression (Xu et al., 2020; Mehta et al., 2023; Kirschner and Krause, 2021). The resulting confidence intervals are then around the least squares estimator, which does not align with the logistic estimator \(f_{t}\). This model does not encode the fact that \(s(f(\bm{x}))\) only takes values in \([0,1]\) and considers a sub-Gaussian distribution for \(y_{t}\), instead of the Bernoulli formulation when calculating the likelihood. Therefore, the resulting algorithms require more samples to learn an accurate reward estimate. In a concurrent work, Xu et al. (2024) also consider the loss function of Equation (6) and present likelihood-ratio confidence sets. The width of the sets at time \(T\), scales with \(\sqrt{T\log\mathcal{N}(\mathcal{H}_{k};1/T)}\) where the second term is the _metric entropy_ of the \(B\)-bounded RKHS at resolution \(1/T\), that is, the log-covering number of this function class, using balls of radius \(1/T\). It is known that \(\log\mathcal{N}(\mathcal{H}_{k};1/T)\asymp\gamma_{T}\) as defined in Theorem 2. This may be easily verified using Wainwright (2019, Example 5.12) and (Vakili et al., 2021, Definition 1). Noting the definition of \(\beta_{t}^{\mathrm{D}}\), we see that likelihood ratio sets of Xu et al. (2024) are wider than Corollary 5. Consequently, the presented regret guarantee in this work is looser by a factor of \(T^{1/4}\) compared to our bound in Theorem 6.

[MISSING_PAGE_FAIL:7]

**Theorem 6**.: _Suppose the utility function \(f\) lies in \(\mathcal{H}_{k}\) with a norm bounded by \(B\), and that kernel \(k\) satisfies the assumption of Proposition 4. Let \(\delta\in(0,1]\) and choose the exploration coefficient \(\beta_{t}^{\mathrm{D}}(\delta)\) as in Corollary 5. Then MaxMinLCB satisfies the anytime dueling regret of_

\[\mathbb{P}\left(\forall T\geq 0:R^{\mathrm{D}}(T)\leq C_{3}\beta_{T}^{ \mathrm{D}}(\delta)\sqrt{T\gamma_{T}^{\mathrm{D}}}=\mathcal{O}(\gamma_{T}^{ \mathrm{D}}\sqrt{T})\right)\geq 1-\delta\]

_where \(\gamma_{T}^{\mathrm{D}}\) is the \(T\)-step information gain of kernel \(k^{\mathrm{D}}\) and \(C_{3}=(8+2\kappa)/\sqrt{\log(1+4(\lambda\kappa)^{-1})}\)._

The proof is left to Appendix C.2. The information gain \(\gamma_{T}^{\mathrm{D}}\) in Theorem 6 quantifies the structural complexity of the RKHS corresponding to \(k^{\mathrm{D}}\) and its dependence on \(T\) is fairly understood for kernels commonly used in applications of bandit optimization. As an example, for a Matern kernel of smoothness \(\nu\) defined over a \(d\)-dimensional domain, \(\gamma_{T}=\tilde{\mathcal{O}}(T^{d/(2\nu+d)})\)(Remark 2, Vakili et al., 2021) and the corresponding regret bound grows sublinearly with \(T\).

Restricting the optimization domain to \(\mathcal{M}_{t}\subset\mathcal{X}\) is common in the literature (Zoghi et al., 2014; Saha, 2021) despite being challenging in applications with large or continuous domains. We conjecture that MaxMinLCB would enjoy similar regret guarantees without restricting the selection domain to \(\mathcal{M}_{t}\) as done in Equation (7). This claim is supported by our experiments in Section 6.2 which are carried out without such restriction on the optimization domain.

## 6 Experiments

Our experiments are on finding the maxima of test functions commonly used in (non-convex) optimization literature (Jamil and Yang, 2013), given only preference feedback. These functions cover challenging optimization landscapes including several local optima, plateaus, and valleys, allowing us to test the versatility of MaxMinLCB. We use the Ackley function for illustration in the main text and provide the regret plots for the remainder of the functions in Appendix E. For all experiments, we set the horizon \(T=2000\) and evaluate all algorithms on a uniform mesh over the input domain of size \(100\). Additionally, we conducted experiments on the Yelp restaurant review dataset to demonstrate the applicability of MaxMinLCB on real-world data and its scaling to larger domains. All experiments are run across \(20\) random seeds and reported values are averaged over the seeds, together with standard error. The environments and algorithms are implemented2 end-to-end in JAX (Bradbury et al., 2018).

Footnote 2: The code is made available at github.com/lasgroup/MaxMinLCB.

### Benchmarking Confidence Sets

Performance of MaxMinLCB relies on validity and tightness of the LCB. We evaluate the quality of our kernelized confidence bounds, using the potentially simpler task of bandit optimization given logistic feedback. To this end, we fix the acquisition function for the logistic bandit algorithms to the Upper Confidence Bound (UCB) function, and benchmark different methods for calculating the confidence bound. We refer to the algorithm instantiated with the confidence sets of Theorem 2 as LGP-UCB (c.f. Algorithm 2). The Ind-UCB approach assumes that actions are uncorrelated, and

Figure 1: Regret of learning the Ackley function with logistic and preference feedback. **(a)** Same UCB algorithms, each using a different confidence set. LGP-UCB performs best, showcasing the power of Theorem 2. **(b)**: Algorithms with different acquisition functions, all using our confidence sets. MaxMinLCB is more sample-efficient.

maintains an independent confidence interval for each action as in Lattimore and Szepesvari (2020, Algorithm 3). This demonstrates how LGP-UCB utilizes the correlation between actions. We also implement Log-UCB1 (Faury et al., 2020) that assumes that \(f\) is a linear function, i.e., \(f(\bm{x})=\theta^{T}\bm{x}\) to highlight the improvements gained by kernelization. Last, we compare LGP-UCB with GP-UCB (Srinivas et al., 2010) that estimates probabilities \(s(f(\cdot))\) via a kernelized ridge regression task. This comparison highlights the benefits of using our kernelized logistic estimator (Proposition 1) over regression-based approaches (Xu et al., 2020, Kirschner and Krause, 2021, Mehta et al., 2023, 2020), Figure 1 shows that the cumulative regret of LGP-UCB is the lowest among the baselines. GP-UCB performs closest to LGP-UCB, however, it accumulates regret linearly during the initial steps. Note that GP-UCB and LGP-UCB differ in the estimation of the utility function \(f_{t}\) while estimating the width of the confidence bounds similarly. This result suggests that using the logistic-type loss (3) to infer the utility function is advantageous. As expected, Ind-UCB converges at a slower rate than LGP-UCB and GP-UCB due to ignoring the correlation between arms while Log-UCB1's regret grows linearly as the Ackley function is misspecified under the assumption of linearity. We defer the results on the rest of the utility functions to Table 2 in Appendix E and the figures therein.

### Benchmarking Acquisition Functions

In this section, we compare MaxMinLCB with other utility-based bandit algorithms. To isolate the benefits of our acquisition function, we instantiate all algorithms with the same confidence sets, and use our improved preferred-based bound of Corollary 5. Therefore, our implementation differs from the corresponding references, while we refer to the algorithms by their original name. We consider the following baselines. Doubler and MultiSBM (Ailon et al., 2014) choose \(\bm{x}_{t}\) as a _reference_ action from the recent history of actions and pair it with \(\bm{x}^{\prime}_{t}\) which maximizes the joint UCB (cf. Algorithm 5 and 6). RUCB (Zoghi et al., 2014) chooses \(\bm{x}^{\prime}_{t}\) similarly, however, it selects the reference action uniformly at random from \(\mathcal{M}_{t}\) (Algorithm 7). MaxInP (Saha, 2021) also maintains the set of plausible maximizers \(\mathcal{M}_{t}\), and at each time step, it selects the pair of actions that maximize \(\sigma_{t}^{D}(\bm{x},\bm{x}^{\prime})\) (Algorithm 3). IDS (Kirschner and Krause, 2021) selects the reference action greedily by maximizing \(f_{t}\), and pairs it with an informative action (Algorithm 4). Notably, all algorithms, with the exception of MaxInP, choose one of the actions independently and use it as a reference point when selecting the other one. Figure 3 illustrates the differences in action selection between UCB, maximum information, and MaxMinLCB approaches. We note that POP-BO(Xu et al., 2024) and MultiSBM only differ in the estimation of the confidence set. Since we deploy the same confidence set for all acquisition functions, the two algorithms are equivalent and we use MultiSBM in our results, however, comparisons hold for POP-BO as well.

Figure 1(b) benchmarks the algorithms using the Ackley utility function, where MaxMinLCB outperforms the baselines. All algorithms suffer from close-to-linear regret during the initial stages of learning, suggesting that there is an inevitable exploration phase. Notably, MaxMinLCB, IDS, and Doubler are the first to select actions with high utility, while RUCB and MaxInP explore for longer. Table 1 shows the dueling regret for all utility functions. MaxMinLCB consistently outperforms the baselines across the analyzed functions and achieves a low standard error, supporting its efficiency in balancing exploration and exploitation in the preference feedback setting. Only MultiSBM shows comparable performance to MaxMinLCB and even outperforms it on the Matyas function which is a relatively smooth function posing a simple optimization problem. However, MaxMinLCB achieves lower regret on the Ackley and Eggholder functions which obtain many local optima and steeper

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \(f\) & MaxMinLCB & Doubler & MultiSBM & MaxInP & RUCB & IDS \\ \hline Branin & \(\bm{104}\pm 13\) & \(114\pm 9\) & \(\bm{89}\pm 13\) & \(340\pm 2\) & \(\bm{101}\pm 14\) & \(163\pm 22\) \\ Matyas & \(125\pm 5\) & \(136\pm 4\) & \(\bm{106}\pm 7\) & \(136\pm 6\) & \(\bm{106}\pm 6\) & \(128\pm 5\) \\ Rosenbrock & \(\bm{27}\pm 4\) & \(44\pm 12\) & \(\bm{25}\pm 5\) & \(109\pm 2\) & \(58\pm 7\) & \(58\pm 13\) \\ \hline Ackley & \(\bm{56}\pm 2\) & \(72\pm 2\) & \(65\pm 0.5\) & \(120\pm 1\) & \(84\pm 0.7\) & \(111\pm 9\) \\ Eggholder & \(\bm{113}\pm 6\) & \(154\pm 4\) & \(134\pm 3\) & \(230\pm 34\) & \(213\pm 40\) & \(141\pm 12\) \\ Hoelder & \(\bm{141}\pm 26\) & \(\bm{154}\pm 3\) & \(\bm{136}\pm 15\) & \(204\pm 20\) & \(200\pm 28\) & \(\bm{132}\pm 15\) \\ Michalewicz & \(\bm{138}\pm 14\) & \(183\pm 11\) & \(\bm{155}\pm 10\) & \(260\pm 40\) & \(269\pm 46\) & \(188\pm 21\) \\ Yelp & \(\bm{175}\pm 22\) & \(263\pm 28\) & \(\bm{199}\pm 25\) & \(409\pm 15\) & \(214\pm 22\) & \(255\pm 22\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Benchmarking \(R_{T}^{\text{D}}\) for a variety of test utility functions, \(T=2000\). The top 3 rows show results for smoother functions without steep gradients and local optima while the bottom 5 rows show the results for more challenging problems.

gradients showing that MaxMinLCB is preferable for challenging optimization problems. Other acquisition functions work well only in certain cases, e.g., IDS achieves the smallest regret for optimizing Matyas, while RUCB excels on the Branin function. This indicates the challenges each utility function offers and the performance of the action selection is task dependent. The consistent performance of MaxMinLCB demonstrates its robustness against the underlying unknown utility function.

### Real-world Experiment

To demonstrate the scalability and applicability of MaxMinLCB, we conduct an experiment on the Yelp dataset of restaurant reviews, which consists of \(275\) restaurants and \(20\) users after the pre-processing stage. For each user, we want to find the restaurants that best fit their preference, via making sequential recommendations and receiving comparative feedback. We define the action space \(\mathcal{X}\) by assigning to each restaurant their respective \(32\)-dimensional embedding of their reviews, i.e., \(\mathcal{X}\subseteq\mathbb{R}^{32}\). The dataset provides utility values for users in the form of ratings on the scale of \(1\) to \(5\), however, not all users rated every restaurant.

We estimate missing ratings using collaborative filtering [21]. Further details on the data processing are deferred to Appendix D.1. Figure 2 shows that the results of this larger problem align with previous conclusions. MaxMinLCB remains the best-performing algorithm with MultiSBM following second. The cumulative regret is also reported in Table 1. Note that neither of the algorithms is tuned or modified for this experiment. These results are only intended to demonstrate that 1) the computations easily scale, and 2) the kernelized approach is still applicable in a text-based domain, by using high-quality vector embeddings.

## 7 Conclusion

We addressed the problem of bandit optimization with preference feedback over large domains and complex targets. We proposed MaxMinLCB, which takes a game-theoretic approach to the problem of action selection under comparative feedback, and naturally balances exploration and exploitation by constructing a zero-sum Stackelberg game between the action pairs. MaxMinLCB achieves a sublinear regret for kernelized utilities, and performs competitively across a range of experiments. Lastly, by uncovering the equivalence of learning with logistic or comparative feedback, we propose kernelized preference-based confidence sets, which may be employed in adjacent problems, such as reinforcement learning with human feedback. The technical setup considered in this work serves as a foundation for a number of applications in mechanism design, such as preference elicitation and welfare optimization from multiple feedback sources for social choice theory, which we leave as future work.

## Acknowledgments and Disclosure of Funding

We thank Scott Sussex for his thorough feedback. This research was supported by the European Research Council (ERC) under the European Union's Horizon 2020 research and Innovation Program Grant agreement No. 815943. Barna Pasztor was supported by an ETH AI Center doctoral fellowship, and Parnian Kassraie by a Google PhD Fellowship.

Figure 2: LGP-UCB is more sample-efficient when making restaurant recommendations based on Yelp open dataset with preference feedback. All baselines use the confidence sets of Cor. 5.

## References

* Abbasi-Yadkori (2013) Yasin Abbasi-Yadkori. _Online learning for linearly parametrized control problems_. PhD thesis, University of Alberta, 2013.
* Ailon et al. (2014) Nir Ailon, Zohar Karnin, and Thorsten Joachims. Reducing dueling bandits to cardinal bandits. In _International Conference on Machine Learning_, pages 856-864. PMLR, 2014.
* Aiolli & Sperduti (2004) Fabio Aiolli and Alessandro Sperduti. Learning preferences for multiclass problems. _Advances in neural information processing systems_, 17, 2004.
* Axler (2020) Sheldon Axler. _Measure, integration & real analysis_. Springer Nature, 2020.
* Benggs et al. (2021) Viktor Benggs, Robert Busa-Fekete, Adil El Mesaoudi-Paul, and Eyke Hullermeier. Preference-based online learning with dueling bandits: A survey. _The Journal of Machine Learning Research_, 2021.
* Beygelzimer et al. (2019) Alina Beygelzimer, David Pal, Balazs Szorenyi, Devanathan Thiruvenkatachari, Chen-Yu Wei, and Chicheng Zhang. Bandit multiclass linear classification: Efficient algorithms for the separable case. In _International Conference on Machine Learning_, pages 624-633. PMLR, 2019.
* Bradbury et al. (2018) James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, et al. JAX: composable transformations of Python+ NumPy programs, 2018. URL http://github.com/google/jax.
* Bradley & Terry (1952) Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. _Biometrika_, 1952.
* Brochu et al. (2010) Eric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. _arXiv preprint_, 2010.
* Camacho-Vallejo et al. (2023) Jose-Fernando Camacho-Vallejo, Carlos Corpus, and Juan G Villegas. Metaheuristics for bilevel optimization: A comprehensive review. _Computers & Operations Research_, page 106410, 2023.
* Casper et al. (2023) Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jeremy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. Open problems and fundamental limitations of reinforcement learning from human feedback. _Transactions on Machine Learning Research_, 2023.
* Chen & Frazier (2017) Bangrui Chen and Peter I Frazier. Dueling bandits with weak regret. In _International Conference on Machine Learning_, pages 731-739. PMLR, 2017.
* Chowdhury & Gopalan (2017) Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In _International Conference on Machine Learning_, pages 844-853. PMLR, 2017.
* Christiano et al. (2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. _Advances in neural information processing systems_, 30, 2017.
* Chu & Ghahramani (2005) Wei Chu and Zoubin Ghahramani. Preference learning with gaussian processes. In _Proceedings of the 22nd international conference on Machine learning_, pages 137-144, 2005.
* Dagreou et al. (2022) Mathieu Dagreou, Pierre Ablin, Samuel Vaiter, and Thomas Moreau. A framework for bilevel optimization that enables stochastic and global variance reduction algorithms. _Advances in Neural Information Processing Systems_, 35:26698-26710, 2022.
* Dudik et al. (2015) Miroslav Dudik, Katja Hofmann, Robert E Schapire, Aleksandrs Slivkins, and Masrour Zoghi. Contextual dueling bandits. In _Conference on Learning Theory_, pages 563-587. PMLR, 2015.
* Falahatgar et al. (2017) Moein Falahatgar, Alon Orlitsky, Venkatadheeraj Pichapati, and Ananda Theertha Suresh. Maximum selection and ranking under noisy comparisons. In _International Conference on Machine Learning_, pages 1088-1096. PMLR, 2017.
* Frazier et al. (2018)Louis Faury, Marc Abeille, Clement Calauzenes, and Olivier Fercoq. Improved optimistic algorithms for logistic bandits. In _International Conference on Machine Learning_, pages 3052-3060. PMLR, 2020.
* Faury et al. (2022) Louis Faury, Marc Abeille, Kwang-Sung Jun, and Clement Calauzenes. Jointly efficient and optimal algorithms for logistic bandits. In _International Conference on Artificial Intelligence and Statistics_, pages 546-580. PMLR, 2022.
* Filippi et al. (2010) Sarah Filippi, Olivier Cappe, Aurelien Garivier, and Csaba Szepesvari. Parametric bandits: The generalized linear case. _Advances in neural information processing systems_, 2010.
* Foster and Krishnamurthy (2018) Dylan J Foster and Akshay Krishnamurthy. Contextual bandits with surrogate losses: Margin bounds and efficient algorithms. _Advances in Neural Information Processing Systems_, 31, 2018.
* Ghadimi and Wang (2018) Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. _arXiv preprint arXiv:1802.02246_, 2018.
* Gonzalez et al. (2017) Javier Gonzalez, Zhenwen Dai, Andreas Damianou, and Neil D Lawrence. Preferential bayesian optimization. In _International Conference on Machine Learning_, pages 1282-1291. PMLR, 2017.
* Houlsby et al. (2011) Neil Houlsby, Ferenc Huszar, Zoubin Ghahramani, and Mate Lengyel. Bayesian active learning for classification and preference learning. _arXiv preprint arXiv:1112.5745_, 2011.
* Jamieson and Nowak (2011) Kevin G Jamieson and Robert Nowak. Active ranking using pairwise comparisons. _Advances in neural information processing systems_, 24, 2011.
* Jamil and Yang (2013) Momin Jamil and Xin-She Yang. A literature survey of benchmark functions for global optimisation problems. _International Journal of Mathematical Modelling and Numerical Optimisation_, 4(2):150-194, 2013.
* Jeroslow (1985) Robert G Jeroslow. The polynomial hierarchy and a simple model for competitive analysis. _Mathematical programming_, 32(2):146-164, 1985.
* Ji et al. (2023) Xiang Ji, Huazheng Wang, Minshuo Chen, Tuo Zhao, and Mengdi Wang. Provable benefits of policy learning from human preferences in contextual bandit problems. _arXiv preprint arXiv:2307.12975_, 2023.
* Kirschner and Krause (2021) Johannes Kirschner and Andreas Krause. Bias-robust bayesian optimization via dueling bandits. In _International Conference on Machine Learning_. PMLR, 2021.
* Kirschner et al. (2020) Johannes Kirschner, Tor Lattimore, and Andreas Krause. Information directed sampling for linear partial monitoring. In _Conference on Learning Theory_, pages 2328-2369. PMLR, 2020.
* Komiyama et al. (2015) Junpei Komiyama, Junya Honda, and Hiroshi Nakagawa. Optimal regret analysis of thompson sampling in stochastic multi-armed bandit problem with multiple plays. In _International Conference on Machine Learning_, pages 1152-1161. PMLR, 2015.
* Kumagai (2017) Wataru Kumagai. Regret analysis for continuous dueling bandit. _Advances in Neural Information Processing Systems_, 30, 2017.
* Lattimore and Szepesvari (2020) Tor Lattimore and Csaba Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* Lax (2002) Peter D Lax. _Functional analysis_, volume 55. John Wiley & Sons, 2002.
* Lee et al. (2024) Junghyun Lee, Se-Young Yun, and Kwang-Sung Jun. Improved regret bounds of (multinomial) logistic bandits via regret-to-confidence-set conversion. In _International Conference on Artificial Intelligence and Statistics_, pages 4474-4482. PMLR, 2024.
* Mehta et al. (2023a) Viraj Mehta, Vikramjeet Das, Ojash Neopane, Yijia Dai, Ilija Bogunovic, Jeff Schneider, and Willie Neiswanger. Sample efficient reinforcement learning from human feedback via active exploration. _arXiv preprint_, 2023a.
* Mehta et al. (2023b) Viraj Mehta, Ojash Neopane, Vikramjeet Das, Sen Lin, Jeff Schneider, and Willie Neiswanger. Kernelized offline contextual dueling bandits. _arXiv preprint_, 2023b.
* Mehta et al. (2023b)Petrus Mikkola, Milica Todorovic, Jari Jarvi, Patrick Rinke, and Samuel Kaski. Projective preferential bayesian optimization. In _International Conference on Machine Learning_. PMLR, 2020.
* Munos et al. (2023) Remi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, et al. Nash learning from human feedback. _arXiv preprint_, 2023.
* Nguyen et al. (2024) Tung Nguyen, Qiuyi Zhang, Bangding Yang, Chansoo Lee, Jorg Bornschein, Yingjie Miao, Sagi Perel, Yutian Chen, and Xingyou Song. Predicting from strings: Language model embeddings for bayesian optimization. _arXiv preprint_, 2024.
* Saha (2021) Aadirupa Saha. Optimal algorithms for stochastic contextual preference bandits. _Advances in Neural Information Processing Systems_, 34:30050-30062, 2021.
* Saha & Krishnamurthy (2022) Aadirupa Saha and Akshay Krishnamurthy. Efficient and optimal algorithms for contextual dueling bandits under realizability. In _International Conference on Algorithmic Learning Theory_. PMLR, 2022.
* Saha et al. (2023) Aadirupa Saha, Aldo Pacchiano, and Jonathan Lee. Dueling rl: Reinforcement learning with trajectory preferences. In _Proceedings of The 26th International Conference on Artificial Intelligence and Statistics_. PMLR, 2023.
* Schafer et al. (2007) J Ben Schafer, Dan Frankowski, Jon Herlocker, and Shilad Sen. Collaborative filtering recommender systems. In _The adaptive web: methods and strategies of web personalization_, pages 291-324. Springer, 2007.
* Scholkopf et al. (2001) Bernhard Scholkopf, Ralf Herbrich, and Alex J Smola. A generalized representer theorem. In _International conference on computational learning theory_, pages 416-426. Springer, 2001.
* Sinha et al. (2017) Ankur Sinha, Pekka Malo, and Kalyanmoy Deb. A review on bilevel optimization: From classical to evolutionary approaches and applications. _IEEE transactions on evolutionary computation_, 22(2):276-295, 2017.
* Srinivas et al. (2010) Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In _Proceedings of the 27th International Conference on International Conference on Machine Learning_, 2010.
* von Stackelberg (1952) Heinrich von Stackelberg. _Theory of the market economy_. Oxford University Press, 1952.
* Sui et al. (2017) Yanan Sui, Vincent Zhuang, Joel W Burdick, and Yisong Yue. Multi-dueling bandits with dependent arms. _arXiv preprint arXiv:1705.00253_, 2017.
* Takeno et al. (2023) Shion Takeno, Masahiro Nomura, and Masayuki Karasuyama. Towards practical preferential bayesian optimization with skew gaussian processes. In _International Conference on Machine Learning_, pages 33516-33533. PMLR, 2023.
* Tucker et al. (2020) Maegan Tucker, Ellen Novoseller, Claudia Kann, Yanan Sui, Yisong Yue, Joel W Burdick, and Aaron D Ames. Preference-based learning for exoskeleton gait optimization. In _2020 IEEE international conference on robotics and automation (ICRA)_. IEEE, 2020.
* Urvoy et al. (2013) Tanguy Urvoy, Fabrice Clerot, Raphael Feraud, and Sami Naamane. Generic exploration and k-armed voting bandits. In _International Conference on Machine Learning_. PMLR, 2013.
* Vakili et al. (2021) Sattar Vakili, Kia Khezeli, and Victor Picheny. On information gain and regret bounds in gaussian process bandits. In _International Conference on Artificial Intelligence and Statistics_, pages 82-90. PMLR, 2021.
* Valko et al. (2013) Michal Valko, Nathaniel Korda, Remi Munos, Ilias Flaounas, and Nelo Cristianini. Finite-time analysis of kernelised contextual bandits. _arXiv preprint arXiv:1309.6869_, 2013.
* Wainwright (2019) Martin J Wainwright. _High-dimensional statistics: A non-asymptotic viewpoint_, volume 48. Cambridge university press, 2019.
* Whitehouse et al. (2023) Justin Whitehouse, Zhiwei Steven Wu, and Aaditya Ramdas. Improved self-normalized concentration in hilbert spaces: Sublinear regret for gp-ucb. _arXiv preprint arXiv:2307.07539_, 2023.
* Wainwright et al. (2017)Huasen Wu and Xin Liu. Double thompson sampling for dueling bandits. _Advances in neural information processing systems_, 29, 2016.
* Xu et al. (2024) Wenjie Xu, Wenbin Wang, Yuning Jiang, Bratislav Svetozarevic, and Colin N Jones. Principled preferential bayesian optimization. _arXiv preprint arXiv:2402.05367_, 2024.
* Xu et al. (2020) Yichong Xu, Aparna Joshi, Aarti Singh, and Artur Dubrawski. Zeroth order non-convex optimization with dueling-choice bandits. In _Conference on Uncertainty in Artificial Intelligence_. PMLR, 2020.
* Yue and Joachims (2009) Yisong Yue and Thorsten Joachims. Interactively optimizing information retrieval systems as a dueling bandits problem. In _Proceedings of the 26th Annual International Conference on Machine Learning_, 2009.
* Yue et al. (2012) Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits problem. _Journal of Computer and System Sciences_, 2012.
* Zhan et al. (2023) Wenhao Zhan, Masatoshi Uehara, Nathan Kallus, Jason D Lee, and Wen Sun. Provable offline reinforcement learning with human feedback. _arXiv preprint_, 2023.
* Zhu et al. (2023) Banghua Zhu, Michael Jordan, and Jiantao Jiao. Principled reinforcement learning with human feedback from pairwise or k-wise comparisons. In _International Conference on Machine Learning_, pages 43037-43067. PMLR, 2023.
* Zimmert and Seldin (2018) Julian Zimmert and Yevgeny Seldin. Factored bandits. _Advances in Neural Information Processing Systems_, 31, 2018.
* Zoghi et al. (2014a) Masrour Zoghi, Shimon Whiteson, Remi Munos, and Maarten Rijke. Relative upper confidence bound for the k-armed dueling bandit problem. In _International conference on machine learning_. PMLR, 2014a.
* Zoghi et al. (2014b) Masrour Zoghi, Shimon A Whiteson, Maarten De Rijke, and Remi Munos. Relative confidence sampling for efficient on-line ranker evaluation. In _Proceedings of the 7th ACM international conference on Web search and data mining_, 2014b.

###### Contents of Appendix

* A Proofs for Bandits with Logistic Feedback
* B Helper Lemmas for Appendix A
* C Proofs for Bandits with Preference Feedback
* C.1 Equivalence of Preference-based and Logistic Losses
* C.2 Proof of the Preference-based Regret Bound
* C.3 Extending Algorithms for Linear Dueling Bandits to Kernelized Setting
* C.4 Helper Lemmas for Appendix C.3
* D Details of Experiments
* D.1 Yelp Experiment
* E Additional Experiments

## Appendix A Proofs for Bandits with Logistic Feedback

We have written the equations in the main text in terms of kernel matrices and function evaluations, for easier readability. For the purpose of the proof however, we mainly rely on entities in the Hilbert space. Consider the operator \(\bm{\phi}:\mathcal{X}\to\mathcal{H}\) which corresponds to kernel \(k\) and satisfies \(k(\bm{x},\cdot)=\bm{\phi}(\bm{x})\). Then by Mercer's theorem, any \(f\in\mathcal{H}_{k}\) may be written as \(f=\bm{\theta}^{\top}\bm{\phi}\), where \(\bm{\theta}\in\ell_{2}(\mathbb{N})\) and has a \(B\)-bounded \(\ell_{2}\) norm. For a sequence of points \(\bm{x}_{1},\ldots,\bm{x}_{t}\in\mathcal{X}\), we define the (infinite-dimensional) feature map \(\Phi_{t}=\left[\bm{\phi}(\bm{x}_{1}),\cdots,\bm{\phi}(\bm{x}_{t})\right]^{\top}\), which gives rise to the kernel matrix \(K_{t}:\mathbb{R}^{t}\to\mathbb{R}^{t}\) and the covariance operator \(S_{t}:\mathcal{H}_{k}\to\mathcal{H}_{k}\), respectively defined as \(K_{t}=\Phi_{t}\Phi_{t}^{\top}\) and \(S_{t}=\Phi_{t}^{\top}\Phi_{t}\). Let \(\bm{I}_{t}\) denote the \(t\)-dimensional identity matrix, and \(\bm{I}_{\mathcal{H}}\) be the identity operator acting on the RKHS. Then it is widely known that the covariance and kernel operators are connected via \(\det(\bm{I}_{\mathcal{H}}+\rho^{-2}S_{t})=\det(\bm{I}_{t}+\rho^{-2}K_{t})\) for any \(t\geq 1\) and \(\rho\neq 0\). For operators on the Hilbert space, \(\det(A)\) refer to a Fredholm determinant (c.f. [20]). Lastly, in the appendix, we refer to the true unknown utility function as \(f^{*}(\bm{x})=\bm{\phi}^{\top}(\bm{x})\bm{\theta}^{*}\). In the main text, the true utility is simply referred to as \(f\).

To analyze our function-valued confidence sequences, we start by re-writing the logistic loss function

\[\mathcal{L}(\bm{\theta};H_{t})= \sum_{\tau=1}^{t}-y_{\tau}\log s\left(\bm{\theta}^{\top}\bm{\phi} (\bm{x}_{\tau})\right)-\sum_{\tau=1}^{t}(1-y_{\tau})\log\left(1-s\left(\bm{ \theta}^{\top}\bm{\phi}(\bm{x}_{\tau})\right)\right)+\frac{\lambda}{2}\|\bm{ \theta}\|_{2}^{2}\]

which is strictly convex in \(\bm{\theta}\) and has a unique minimizer \(\bm{\theta}_{t}\) which satisfies

\[\nabla\mathcal{L}(\bm{\theta}_{t};H_{t})=\sum_{\tau=1}^{t}-y_{ \tau}\bm{\phi}(\bm{x}_{\tau})+g_{t}(\bm{\theta}_{t})=0\]

where \(g_{t}(\bm{\theta}):\mathcal{H}\to\mathcal{H}\) is a linear operator defined as

\[g_{t}(\bm{\theta})\coloneqq\sum_{\tau=1}^{t}\bm{\phi}(\bm{x}_{ \tau})s(\bm{\theta}^{\top}\bm{\phi}(\bm{x}_{\tau}))+\lambda\bm{\theta}.\]

In the main text, we assumed that minimizer of \(\mathcal{L}\) satisfies the norm boundedness condition. Here, we present a more rigorous analysis which does not assume so. Instead, we work with a projected estimator defined via

\[\bm{\theta}_{t}^{P}=\underset{\|\bm{\theta}\|_{2}\leq B}{\arg\min}\|\bm{g}_{t} (\bm{\theta})-\bm{g}_{t}(\bm{\theta}_{t})\|_{V_{t}^{-1}}.\] (8)

where \(V_{t}=S_{t}+\kappa\lambda\bm{I}_{\mathcal{H}}\) and \(\bm{\theta}_{t}\) is the minimizer of \(\mathcal{L}(\bm{\theta};H_{t})\). Roughly put, \(\bm{\theta}_{t}^{P}\in\ell_{2}(\mathbb{N})\) is a norm \(B\)-bounded alternative to \(\bm{\theta}_{t}\), who also satisfies a small \(\nabla\mathcal{L}\), and therefore, is expected to result in an accurate decision boundary. We will present our proof in terms of \(\bm{\theta}_{t}^{P}\). This also proves the results in the main text, since \(\bm{\theta}_{t}^{P}=\bm{\theta}_{t}\) if \(\bm{\theta}_{t}\) itself happens to have a \(B\)-bounded norm, as assumed in the main text.

Our analysis relies on a concentration bound for \(\mathcal{H}\)-valued martingale sequences stated in Abbasi-Yadkori (2013) and later in Whitehouse et al. (2023). Below, we have adapted the statement to match our notation.

**Lemma 7** (Corollary 1 Whitehouse et al. (2023)).: _Suppose the sequence \((\bm{x}_{t})_{t\geq 1}\) is \((\mathcal{F}_{t})_{t\geq 1}\)-adapted, where \(\mathcal{F}_{t}\coloneqq\sigma(\bm{x}_{1},\dots,\bm{x}_{t},\varepsilon_{1}, \dots,\varepsilon_{t-1})\) and \(\varepsilon_{t}\) are i.i.d. zero-mean \(\sigma\)-subGaussian noise. Consider the RKHS \(\mathcal{H}\) corresponding to a kernel \(k(\bm{x},\bm{x}^{\prime})=\bm{\phi}^{\top}(\bm{x})\bm{\phi}(\bm{x}^{\prime})\). Then, for any \(\rho>0\) and \(\delta\in(0,1)\), we have that, with probability at least \(1-\delta\), simultaneously for all \(t\geq 0\),_

\[\left\|\sum_{\tau\leq t}\varepsilon_{\tau}\bm{\phi}(\bm{x}_{\tau})\right\|_{V_ {t}^{-1}}\leq\sigma\sqrt{2\log\left(\frac{1}{\delta}\sqrt{\det(\bm{I}_{t}+ \rho^{-2}K_{t})}\right)}\]

_where \(V_{t}=S_{t}+\rho^{2}\bm{I}_{\mathcal{H}}\)._

The following lemma, which extends Faury et al. (2020, Lemma 8) to \(\mathcal{H}\)-valued operators, expresses the closeness of \(\bm{\theta}_{t}\) and \(\bm{\theta}^{\star}\) in the gradient space, with respect to the norm of the covariance matrix.

**Lemma 8** (Gradient Space Confidence Bounds).: _Set \(0<\delta<1\). Under the assumptions of Theorem 2_

\[\mathbb{P}\left(\forall t\geq 0:\,\|\bm{g}_{t}(\bm{\theta}_{t})-\bm{g}_{t}( \bm{\theta}^{\star})\|_{V_{t}^{-1}}\leq\frac{1}{2}\sqrt{2\log 1/\delta+2 \gamma_{T}}+\sqrt{\frac{\lambda}{\kappa}}B\right)\geq 1-\delta\]

_where \(V_{t}=S_{t}+\kappa\lambda\bm{I}_{\mathcal{H}}\)._

Proof of Lemma 8.: Recall that \(\bm{g}_{t}(\bm{\theta})=\sum_{\tau\leq t}s(\bm{\theta}^{\top}\bm{\phi}(\bm{x} _{\tau}))\bm{\phi}(\bm{x}_{\tau})+\lambda\bm{\theta}\). Then it is straighforward to show that

\[\nabla\mathcal{L}(\bm{\theta};H_{t})=\sum_{\tau\leq t}y_{\tau}\bm{\phi}(\bm{x }_{\tau})-g_{t}(\bm{\theta}).\]

Since \(\bm{\theta}_{t}\) is a minimizer of \(\mathcal{L}_{t}\), it holds that \(\bm{g}_{t}(\bm{\theta}_{t})=\sum_{\tau\leq t}y_{\tau}\bm{\phi}(\bm{x}_{\tau})\). This allows us to write,

\[\|\bm{g}_{t}(\bm{\theta}_{t})-\bm{g}_{t}(\bm{\theta}^{\star})\|_ {V_{t}^{-1}} =\left\|\sum_{\tau\leq t}\left(y_{\tau}-s(\bm{\phi}^{\top}(\bm{x} _{\tau})\bm{\theta}^{\star})\right)\bm{\phi}(\bm{x}_{\tau})-\lambda\bm{\theta} ^{\star}\right\|_{V_{t}^{-1}}\] \[\leq\left\|\sum_{\tau\leq t}\varepsilon_{\tau}\bm{\phi}(\bm{x}_{ \tau})\right\|_{V_{t}^{-1}}+\lambda\|\bm{\theta}^{\star}\|_{V_{t}^{-1}}\] (9)

where \(\varepsilon_{\tau}=y_{\tau}-s(\bm{\phi}^{\top}(\bm{x}_{\tau})\bm{\theta}^{ \star})\) is a history dependent random variable in \([0,1]\) due to the data model. To bound the first term, we recognize that any random variable in \([0,1]\) is \(\sigma\) sub-Gaussian with \(\sigma=0.5\) and apply Lemma 7. We obtain that for all \(t\geq 0\), with probability greater than \(1-\delta\)

\[\left\|\sum_{\tau\leq t}\varepsilon_{\tau}\bm{\phi}(\bm{x}_{\tau} )\right\|_{V_{t}^{-1}} \leq\frac{1}{2}\sqrt{2\log\left(\frac{1}{\delta}\sqrt{\det(\bm{I}_ {t}+(\lambda\kappa)^{-1}K_{t})}\right)}\] \[\leq\frac{1}{2}\sqrt{2\log 1/\delta+2\gamma_{T}}\]

since \(\gamma_{t}(\rho)=\sup_{\bm{x}_{1},\dots,\bm{x}_{t}}\frac{1}{2}\log\det(\bm{I}_ {t}+\rho^{-2}K_{t}))\). To bound the second term in (9), note that \(S_{t}=\Phi_{t}^{\top}\Phi_{t}\) is PSD and therefore \(V_{t}\geq\kappa\lambda\bm{I}_{\mathcal{H}}\). Then

\[\lambda\|\bm{\theta}^{\star}\|_{V_{t}^{-1}}\leq\frac{\lambda}{\sqrt{\lambda \kappa}}\|\bm{\theta}^{\star}\|_{2}\leq\sqrt{\frac{\lambda}{\kappa}}B.\]

concluding the proof. 

The following lemma shows the validity of our parameter-space confidence sets.

**Lemma 9**.: _Set \(0<\delta<1\) and consider the confidence sets_

\[\Theta_{t}(\delta)\coloneqq\left\{\|\bm{\theta}\|\leq B,\left\|\bm{\theta}- \bm{\theta}_{t}^{P}\right\|_{V_{t}}\leq 2\sqrt{\lambda\kappa}B+\kappa\sqrt{2\log 1/ \delta+2\gamma_{T}}\right\}.\]

_Then,_

\[\mathbb{P}\left(\forall t\geq 0:\,\bm{\theta}^{\star}\in\Theta_{t}(\delta) \right)\geq 1-\delta\]Proof of Lemma 9.: We check if \(\bm{\theta}^{\star}\in\Theta_{t}(\delta)\) by bounding the following norm

\[\left\lVert\bm{\theta}^{\star}-\bm{\theta}_{t}^{P}\right\rVert_{V_{ t}} \leq\kappa\left\lVert g_{t}(\bm{\theta}^{\star})-g_{t}(\bm{\theta}_{t}^{P})\right\rVert_{V _{t}^{-1}}\] (Lem. 12 ) \[\leq\kappa\left(\left\lVert g_{t}(\bm{\theta}^{\star})-g_{t}(\bm{ \theta}_{t})\right\rVert_{V_{t}^{-1}}+\left\lVert g_{t}(\bm{\theta}_{t})-g_{t} (\bm{\theta}_{t}^{P})\right\rVert_{V_{t}^{-1}}\right)\] \[\leq 2\kappa\lVert g_{t}(\bm{\theta}^{\star})-g_{t}(\bm{\theta}_{t}) \rVert_{V_{t}^{-1}}\] (Eq. 8 ) \[\leq\kappa\sqrt{2\log 1/\delta+2\gamma_{T}}+2\sqrt{\lambda\kappa}B\] (Lem. 8 )

Lastly, we prove a formal extension of Theorem 2.

**Theorem 10** (Theorem 2 - Formal).: _Set \(0<\delta<1\) and consider the confidence sets \(\mathcal{E}_{t}(\delta)\subset\mathcal{H}_{k}\)_

\[\mathcal{E}_{t}(\delta)=\left\{f(\cdot)=\bm{\theta}^{\top}\bm{\phi}(\cdot): \,\bm{\theta}\in\Theta_{t}(\delta)\right\}.\]

_Then, simultanously for all \(\bm{x}\in\mathcal{X}\), \(f\in\mathcal{E}_{t}(\delta)\) and \(t\geq 0\)_

\[\left\lvert s(f(\bm{x}))-s(f^{\star}(\bm{x}))\right\rvert\leq\beta_{t}(\delta )\sigma_{t}(\bm{x})\]

_with probability greater than \(1-\delta\), where_

\[\beta_{t}(\delta)\coloneqq 4LB+2L\sqrt{\frac{\kappa}{\lambda}}\sqrt{2\log 1/ \delta+2\gamma_{T}}\]

Proof of Theorem 10.: For simplicity in notation let us define

\[\tilde{\beta}_{t}(\delta) \coloneqq 2\sqrt{\lambda\kappa}B+\kappa\sqrt{2\log 1/\delta+2 \gamma_{t}}.\]

Suppose \(f=\bm{\theta}^{\top}\bm{\phi}(\cdot)\) is in \(\mathcal{E}_{t}(\delta)\). Then

\[\left\lvert s(\bm{\phi}^{\top}(\bm{x})\bm{\theta}^{\star})-s( \bm{\phi}^{\top}(\bm{x})\bm{\theta})\right\rvert =\left\lvert\alpha(\bm{x};\bm{\theta},\bm{\theta}^{\star})\bm{ \phi}^{\top}(\bm{x})(\bm{\theta}-\bm{\theta}^{\star})\right\rvert\] Lem. 11 \[\leq L\lvert\bm{\phi}^{\top}(\bm{x})(\bm{\theta}-\bm{\theta}^{ \star})\rvert s\text{ is $L$-Lipschitz}\] \[\leq L\lVert\bm{\phi}(\bm{x})\rVert_{V_{t}^{-1}}\lVert\bm{\theta} -\bm{\theta}^{\star}\rVert_{V_{t}}\] \[\leq L\lVert\bm{\phi}(\bm{x})\rVert_{V_{t}^{-1}}\left(\lVert\bm{ \theta}-\bm{\theta}_{t}^{P}\rVert_{V_{t}}+\left\lVert\bm{\theta}_{t}^{P}-\bm{ \theta}^{\star}\right\rVert_{V_{t}}\right)\] \[\leq L\lVert\bm{\phi}(\bm{x})\rVert_{V_{t}^{-1}}\left(\tilde{ \beta}_{t}(\delta)+\left\lVert\bm{\theta}_{t}^{P}-\bm{\theta}^{\star}\right\rVert _{V_{t}}\right)\] \[\overset{\text{w.p.}}{\leq}2L\tilde{\beta}_{t}(\delta)\lVert\bm{ \phi}(\bm{x})\rVert_{V_{t}^{-1}}\] Lem. 9 \[\leq\frac{2L\tilde{\beta}_{t}(\delta)}{\sqrt{\lambda\kappa}}\sigma _{t}(\bm{x})\] Lem. 13 \[=\sigma_{t}(\bm{x})\left(4LB+2L\sqrt{\frac{\kappa}{\lambda}}\sqrt {2\log 1/\delta+2\gamma_{T}}\right)\]

where the third to last inequality holds with probability greater than \(1-\delta\), but the rest of the inequalities hold deterministically. 

Given the confidence set of Theorem 2, we give extend the LGP-UCB algorithm of Faury et al. to the kernelized setting (c.f. Algorithm 2) and prove that it satisfies sublinear regret.

Proof of Corollary 3.: Recall that if \(\bm{x}_{t}\) is the maximizer of the UCB, then

\[s(\bm{\phi}^{\top}(\bm{x}^{\star})\bm{\theta}_{t}^{P})-s(\bm{\phi}^{\top}(\bm {x}_{t})\bm{\theta}_{t}^{P})\leq\sigma_{t}(\bm{x}_{t})\beta_{t}(\delta)- \sigma_{t}(\bm{x}^{\star})\beta_{t}(\delta)\]

Then using Theorem 10, we obtain the following for the regret at step \(t\),

\[r_{t} =s(\bm{\phi}^{\top}(\bm{x}^{\star})\bm{\theta}^{\star})-s(\bm{ \phi}^{\top}(\bm{x}_{t})\bm{\theta}^{\star})\] \[=s(\bm{\phi}^{\top}(\bm{x}^{\star})\bm{\theta}^{\star})-s(\bm{ \phi}^{\top}(\bm{x}^{\star})\bm{\theta}_{t}^{P})+s(\bm{\phi}^{\top}(\bm{x}_{t}) \bm{\theta}_{t}^{P})-s(\bm{\phi}^{\top}(\bm{x}_{t})\bm{\theta}^{\star})\] \[\qquad\quad+s(\bm{\phi}^{\top}(\bm{x}^{\star})\bm{\theta}_{t}^{P}) -s(\bm{\phi}^{\top}(\bm{x}_{t})\bm{\theta}_{t}^{P})\] \[\leq\sigma_{t}(\bm{x}^{\star})\beta_{t}(\delta)+\sigma_{t}(\bm{x }_{t})\beta_{t}(\delta)+\sigma_{t}(\bm{x}_{t})\beta_{t}(\delta)-\sigma_{t}(\bm{x }^{\star})\beta_{t}(\delta)\] \[\leq 2\beta_{t}(\delta)\sigma_{t}(\bm{x}_{t})\]

[MISSING_PAGE_EMPTY:18]

[MISSING_PAGE_EMPTY:19]

**Lemma 14** (Controlling posterior variance with information gain).: _For all \(T\geq 1\),_

\[\sum_{t=1}^{T}\sigma_{t}^{2}(\bm{x}_{t})\leq\frac{2\gamma_{T}}{\log(1+(\lambda \kappa)^{-1})},\quad\sum_{t=1}^{T}(\sigma_{t}^{\mathrm{D}}(\bm{x}_{t}))^{2}\leq \frac{8\gamma_{T}^{\mathrm{D}}}{\log(1+4(\lambda\kappa)^{-1})}.\]

Proof of Lemma 14.: By Srinivas et al. (2010, Lemma 5.3),

\[\gamma_{T}=\max_{\bm{x}_{1},\ldots\bm{x}_{T}}\frac{1}{2}\sum_{t=1}^{T}\log(1+( \lambda\kappa)^{-1}\sigma_{t-1}^{2}(\bm{x}_{t})).\]

Following the technique in Srinivas et al. (2010, Lemma 5.4), since \(\sigma_{t}^{2}\leq 1\), then \((\lambda\kappa)^{-1}\sigma_{t}^{2}\in[0,(\lambda\kappa)^{-1}]\). Now for any \(z\in[0,(\lambda\kappa)^{-1}]\), \(z\leq C\log(1+z)\) where \(C=1/(\lambda\kappa\log(1+(\lambda\kappa)^{-1}))\). We then may write,

\[\sum_{t=1}^{T}\sigma_{t}^{2}(\bm{x}_{t}) =\sum_{t=1}^{T}\lambda\kappa(\lambda\kappa)^{-1}\sigma_{t}^{2}( \bm{x}_{t})\] \[\leq\sum_{t=1}^{T}\lambda\kappa C\log\left(1+(\lambda\kappa)^{-1} \sigma_{t}^{2}(\bm{x}_{t})\right)\] \[=\sum_{t=1}^{T}\frac{\log(1+(\lambda\kappa)^{-1}\sigma_{t}^{2}( \bm{x}_{t}))}{\log\left(1+(\lambda\kappa)^{-1}\right)}\]

Putting both together proves the first inequality of the lemma. As for the dueling case, we can easily check that \(\sigma_{t}^{\mathrm{D}}\leq 2\), and a similar argument yields the second inequality. 

## Appendix C Proofs for Bandits with Preference Feedback

This section presents the proof of main results in Section 5, and our additional contributions in the kernelized Preference-based setting.

### Equivalence of Preference-based and Logistic Losses

We start by establishing the equivalence between the logistic loss (3) and dueling loss (6).

Proof of Proposition 4.: By Mercer's theorem, we know that the kernel function \(k\) has eigenvalue eigenfunction pairs \((\sqrt{\lambda_{i}},\tilde{\phi}_{i})\) for \(i\geq 1\) where \(\tilde{\bm{\phi}}_{i}\) are orthonormal. Then \(k(\bm{x},\bm{x}^{\prime})=\sum_{i\geq 1}\phi_{i}(\bm{x})\phi_{i}(\bm{x}^{ \prime})\) with \(\phi_{i}(\bm{x})=\sqrt{\lambda_{i}}\tilde{\phi}_{i}(\bm{x})\). Now applying the definition of \(k^{\mathrm{D}}\), it holds that \(k^{\mathrm{D}}(\bm{z},\bm{z}^{\prime})=\sum_{i\geq 1}\psi_{i}^{\top}(\bm{z}) \psi_{i}(\bm{z}^{\prime})\) where \(\psi_{i}(\bm{z})=\sqrt{\lambda_{i}}(\phi_{i}(\bm{x})-\phi_{i}(\bm{x}^{\prime}))\). It is straighforward to check that \(\psi_{i}\) are the eigenfunctions of \(k^{\mathrm{D}}\), however, they may not be orthonormal. We have,

\[\langle\psi_{i},\psi_{i}\rangle_{L_{2}} =2\lambda_{i}(1-b_{i}^{2})\] \[\langle\psi_{i},\psi_{j}\rangle_{L_{2}} =-2\sqrt{\lambda_{i}\lambda_{j}}b_{i}b_{j}\]

where \(b_{i}=\int\tilde{\phi}_{i}(\bm{x})\mathrm{d}(\bm{x})\). By the assumption of the proposition, we have \(b_{i}=0\). However, this assumption holds automatically for all kernels commonly used in applications, e.g. any translation invariant kernel, over many domains, since \(\tilde{\bm{\phi}}_{i}\) for such kernels are a sine basis.

Now since \(f\in\mathcal{H}_{k}\), it may be decomposed \(f=\sum_{i\geq 1}\beta_{i}\phi_{i}\) and \(\left\lVert f\right\rVert_{k}^{2}=\sum_{i\geq 1}\beta_{i}^{2}\leq\infty\). And set the difference function to \(h(\bm{x},\bm{x}^{\prime})=\sum_{i\geq 1}\beta_{i}\psi_{i}(\bm{z})\). We can then bound the RKHS norm of \(h\) w.r.t. thekernel \(k^{\mathrm{D}}\) as follows

\[\left\lVert h\right\rVert_{k^{\mathrm{D}}}^{2} =\sum_{i\geq 1}\left(\frac{\langle h,\psi_{i}\rangle_{L_{2}}}{ \langle\psi_{i},\psi_{i}\rangle_{L_{2}}}\right)^{2}\] \[=\sum_{i\geq 1}\left(\frac{\sum_{j\geq 1}\beta_{j}\langle\psi_{j}, \psi_{i}\rangle_{L_{2}}}{2\lambda_{i}(1-b_{i})}\right)^{2}\] \[=\sum_{i\geq 1}\left(\beta_{i}-\frac{b_{i}}{\sqrt{\lambda_{i}}(1- b_{i})}\sum_{j\neq i}\beta_{j}b_{j}\sqrt{\lambda_{j}}\right)^{2}\] \[\overset{b_{i}=0}{=}\left\lVert f\right\rVert_{k}^{2}\leq B^{2}.\]

Now by Mercer's theorem, \(h\in\mathcal{H}_{k^{\mathrm{D}}}\) since it is decomposable as a sum of \(k^{\mathrm{D}}\) eigenfunctions, and attains a \(B\)-bounded \(k^{\mathrm{D}}\)-norm which we showed to be equal to \(\left\lVert f\right\rVert_{k}\). The other direction of the statement is proved the same way. 

Proof of Corollary 5.: Consider the utility function \(f\) and define \(h(\bm{x},\bm{x}^{\prime})\coloneqq f(\bm{x})-f(\bm{x}^{\prime})\). Then by Proposition 4, \(h\) is in RKHS of \(k^{\mathrm{D}}\) with a \(k^{\mathrm{D}}\)-norm bounded by \(B\). We may estimate \(h\) by minimizing \(\mathcal{L}_{k^{\mathrm{D}}}^{\mathrm{L}}(\cdot;H_{t})\). Now invoking Theorem 2 with the dueling kernel we have,

\[\mathbb{P}\left(\forall t\geq 1,\bm{x},\bm{x}^{\prime}\in\mathcal{X}:\,|s \left(h_{t}(\bm{x},\bm{x}^{\prime})\right)-s\left(h(\bm{x},\bm{x}^{\prime}) \right)|\leq\beta_{t}^{\mathrm{D}}(\delta)\sigma_{t}^{D}(\bm{x},\bm{x}^{\prime })\right)\geq 1-\delta\]

concluding the proof by definition of \(h\). 

### Proof of the Preference-based Regret Bound

Recall Corollary 5, which states

\[|s(f(\bm{x}^{\star})-f(\bm{x}_{t}))-s(h_{t}(\bm{x}^{\star},\bm{x}_{t}))|\leq \beta_{t}^{\mathrm{D}}(\delta)\sigma_{t}^{D}(\bm{x},\bm{x}^{\prime})\]

with high probability simultaneously for all \((\bm{x},\bm{x}^{\prime})\) and \(t\geq 1\). For simplicity in notation in the rest of this section, we define \(\omega_{t}(\bm{x},\bm{x}^{\prime})\coloneqq\beta_{t}^{\mathrm{D}}(\delta) \sigma_{t}^{D}(\bm{x},\bm{x}^{\prime})\) and

\[\mathrm{LCB}_{t}(\bm{x},\bm{x}^{\prime}) =s(h_{t}(\bm{x},\bm{x}^{\prime}))-\omega_{t}(\bm{x},\bm{x}^{ \prime}),\] \[\mathrm{UCB}_{t}(\bm{x},\bm{x}^{\prime}) =s(h_{t}(\bm{x},\bm{x}^{\prime}))+\omega_{t}(\bm{x},\bm{x}^{ \prime}).\]

Note that \(\omega_{t}(\bm{x},\bm{x}^{\prime})=\omega_{t}(\bm{x}^{\prime},\bm{x})\) by the symmetry of the dueling kernel \(k^{\mathrm{D}}\). Furthermore, recall the notation \(h(\bm{x},\bm{x})=f(\bm{x})-f(\bm{x})\).

Proof of Theorem 6.: **Step 1:** First, we connect the term of \(\bm{x}^{\prime}_{t}\) in the dueling regret defined in Equation (1) to that of \(\bm{x}_{t}\). Note that both \(s(f(\bm{x}^{\star})-f(\bm{x}^{\prime}_{t}))\) and \(s(f(\bm{x}^{\star})-f(\bm{x}_{t}))\) are greater than \(0.5\) due to the optimality of \(\bm{x}^{\star}\) and the sigmoid function \(s\) is concave on the interval \([0.5,\infty)\). Using the definition of concavity, we get

\[s(f(\bm{x}^{\star})-f(\bm{x}^{\prime}_{t})) \leq s(f(\bm{x}^{\star})-f(\bm{x}_{t}))+\dot{s}(f(\bm{x}^{\star})-f (\bm{x}_{t}))(f(\bm{x}_{t})-f(\bm{x}^{\prime}_{t}))\] \[=s(f(\bm{x}^{\star})-f(\bm{x}_{t}))+s(f(\bm{x}^{\star})-f(\bm{x}_ {t}))s(f(\bm{x}_{t})-f(\bm{x}^{\star}))(f(\bm{x}_{t})-f(\bm{x}^{\prime}_{t}))\] \[\leq\left(1+\frac{h(\bm{x}_{t},\bm{x}^{\prime}_{t})}{2}\right)s(f (\bm{x}^{\star})-f(\bm{x}_{t}))\] (15)

where the second line comes from the derivative of the sigmoid function, \(\dot{s}(x)=s(x)(1-s(x))=s(x)s(-x)\), and in the last line we use \(s(f(\bm{x}_{t})-f(\bm{x}^{\star}))\leq 0.5\).

Using Equation (15), we can upper bound the dueling regret in Equation (1) as

\[2r_{t}^{D} =s(f(\bm{x}^{\star})-f(\bm{x}_{t}))+s(f(\bm{x}^{\star})-f(\bm{x}^ {\prime}_{t}))-1\] \[\leq s(f(\bm{x}^{\star})-f(\bm{x}_{t}))+\left(1+\frac{h(\bm{x}_{t },\bm{x}^{\prime}_{t})}{2}\right)s(f(\bm{x}^{\star})-f(\bm{x}_{t}))-1\] \[\leq 2s(f(\bm{x}^{\star})-f(\bm{x}_{t}))-1+\frac{h(\bm{x}_{t },\bm{x}^{\prime}_{t})}{2}s(f(\bm{x}^{\star})-f(\bm{x}_{t}))\] (16)

**Step 2**: Next, we show that the single-step regret is bounded by \(\omega_{t}(\bm{x}_{t},\bm{x}_{t}^{\star})\).

First, consider the term \(s(f(\bm{x}^{\star})-f(\bm{x}_{t}))\)

\[s(f(\bm{x}^{\star})-f(\bm{x}_{t}))-0.5 \leq s(h_{t}(\bm{x}^{\star},\bm{x}_{t}))+\omega_{t}(\bm{x}_{t},\bm {x}^{\star})-0.5\] Corollary 5 \[\leq 0.5-s(h_{t}(\bm{x}_{t},\bm{x}^{\star}))+\omega_{t}(\bm{x}_{t}, \bm{x}^{\star})\] Sigmoid equality \[\leq 2\omega_{t}(\bm{x}_{t},\bm{x}^{\star})\] (17)

In the last inequality, we used that \(\bm{x}_{t}\in\mathcal{M}_{t}\) implying that \(0.5-s(h_{t}(\bm{x}_{t},\bm{x}^{\star}))\leq\omega_{t}(\bm{x}_{t},\bm{x}^{\star})\). Combining Equation (16) and Equation (17) implies that

\[2r_{t}^{D}\leq 4\omega_{t}(\bm{x}_{t},\bm{x}^{\star})+\frac{h(\bm{x}_{t},\bm{x}_{t}^{\prime})}{2}s(f(\bm{x}^{\star})-f(\bm{x}_{t}))\] (18)

Now, we bound \(\omega_{t}(\bm{x}_{t},\bm{x}^{\star})\) by \(\omega_{t}(\bm{x}_{t},\bm{x}_{t}^{\prime})\). If \(\bm{x}_{t}=\bm{x}^{\star}\), then \(\omega_{t}(\bm{x}_{t},\bm{x}^{\star})=0\leq\omega_{t}(\bm{x}_{t},\bm{x}_{t}^{ \prime})\). If \(\bm{x}_{t}^{\prime}=\bm{x}^{\star}\), then the two expressions are equivalent. Now, assume that \(\bm{x}_{t}\neq\bm{x}^{\star}\) and \(\bm{x}_{t}^{\prime}\neq\bm{x}^{\star}\) and consider \(\omega_{t}(\bm{x}_{t},\bm{x}^{\star})\).

**Case 1:** Assume that \(\mathrm{UCB}_{t}(\bm{x}_{t},\bm{x}^{\star})\leq\mathrm{UCB}_{t}(\bm{x}_{t},\bm {x}_{t}^{\prime})\). Then,

\[2\omega_{t}(\bm{x}_{t},\bm{x}^{\star}) =\mathrm{UCB}_{t}(\bm{x}_{t},\bm{x}^{\star})-\mathrm{LCB}_{t}(\bm {x}_{t},\bm{x}^{\star})\] \[\leq\mathrm{UCB}_{t}(\bm{x}_{t},\bm{x}_{t}^{\prime})-\mathrm{LCB}_ {t}(\bm{x}_{t},\bm{x}^{\star})\] \[\leq\mathrm{UCB}_{t}(\bm{x}_{t},\bm{x}_{t}^{\prime})-\mathrm{LCB}_ {t}(\bm{x}_{t},\bm{x}_{t}^{\prime})=2\omega_{t}(\bm{x}_{t},\bm{x}_{t}^{\prime})\] (19)

where we used the assumption in the first inequality and the definition of \(\bm{x}_{t}^{\prime}\) in the second inequality.

**Case 2:** Assume that \(\mathrm{UCB}_{t}(\bm{x}_{t},\bm{x}^{\star})\geq\mathrm{UCB}_{t}(\bm{x}_{t},\bm {x}_{t}^{\prime})\). First note the following connection between \(\mathrm{LCB}_{t}\) and \(\mathrm{UCB}_{t}\).

\[\mathrm{LCB}_{t}(\bm{x},\bm{x}^{\prime}) =s(h_{t}(\bm{x},\bm{x}^{\prime}))-\omega_{t}(\bm{x},\bm{x}^{\prime})\] \[=1-s(h_{t}(\bm{x}^{\prime},\bm{x}))-\omega_{t}(\bm{x}^{\prime}, \bm{x})\] \[=1-\mathrm{UCB}_{t}(\bm{x}^{\prime},\bm{x})\] (20)

where we used the equality \(s(z)=1-s(-z)\) in the second equality. Using Equation (20), the assumption of Case 2 can be rewritten as \(\mathrm{UCB}_{t}(\bm{x}_{t},\bm{x}^{\star})\geq\mathrm{UCB}_{t}(\bm{x}_{t},\bm {x}_{t}^{\prime})\), implying that

\[\mathrm{LCB}_{t}(\bm{x}_{t}^{\prime},\bm{x}_{t})\geq\mathrm{ LCB}_{t}(\bm{x}^{\star},\bm{x}_{t}).\] (21)

Similarly, \(\mathrm{LCB}_{t}(\bm{x}_{t},\bm{x}_{t}^{\prime})\leq\mathrm{LCB}_{t}(\bm{x}_{t}, \bm{x}^{\star})\) implies

\[\mathrm{UCB}_{t}(\bm{x}_{t}^{\prime},\bm{x}_{t})\geq\mathrm{UCB}_{t}(\bm{x}^{ \star},\bm{x}_{t})\] (22)

Combining Equation (21) and Equation (22), we get

\[2\omega_{t}(\bm{x}_{t},\bm{x}^{\star}) =\mathrm{UCB}_{t}(\bm{x}^{\star},\bm{x}_{t})-\mathrm{LCB}_{t}( \bm{x}^{\star},\bm{x}_{t})\] \[\leq\mathrm{UCB}_{t}(\bm{x}^{\star},\bm{x}_{t})-\mathrm{LCB}_{t}( \bm{x}_{t}^{\prime},\bm{x}_{t})\] \[\leq\mathrm{UCB}_{t}(\bm{x}_{t}^{\prime},\bm{x}_{t})-\mathrm{LCB}_ {t}(\bm{x}_{t}^{\prime},\bm{x}_{t})=2\omega_{t}(\bm{x}_{t},\bm{x}_{t}^{\prime})\] (23)

Combining Equation (19) and Equation (23), we can rewrite the first term in Equation (18) to get

\[2r_{t}^{D}\leq 4\omega_{t}(\bm{x}_{t},\bm{x}_{t}^{\prime})+\frac{h(\bm{x}_{t},\bm{x}_{t}^{\prime})}{2}s(f(\bm{x}^{\star})-f(\bm{x}_{t})).\] (24)

It remains to bound the second term of Equation (24). By the Mean-Value Theorem, \(\exists z\in[0,h(\bm{x}_{t},\bm{x}_{t}^{\prime})]\) such that

\[\dot{s}(z)(h(\bm{x}_{t},\bm{x}_{t}^{\prime})-0)=s(h(\bm{x}_{t},\bm{x}_{t}^{ \prime}))-f(0)\]

Now since \(\kappa=\sup_{z\leq B}1/\dot{s}(z)\) then,

\[h(\bm{x}_{t},\bm{x}_{t}^{\prime})\leq\kappa(s(h(\bm{x}_{t},\bm{x}_{t}^{\prime}))-0.5)\] (25)

Next, we consider the term \(s(h(\bm{x}_{t},\bm{x}_{t}^{\prime}))-0.5\) in Equation (25). Note that \(\bm{x}_{t},\bm{x}_{t}^{\prime}\in\mathcal{M}_{t}\) implies that

\[\mathrm{UCB}_{t}(\bm{x}_{t},\bm{x}_{t}^{\prime}) \geq 0.5\] \[s(h_{t}(\bm{x}_{t},\bm{x}_{t}^{\prime})) \geq 0.5-\omega_{t}(\bm{x}_{t},\bm{x}_{t}^{\prime})\] (26)

[MISSING_PAGE_FAIL:23]

Proof of Theorem 15.: When selecting \((\bm{x}_{t},\bm{x}^{\prime}_{t})\) according to Algorithm 3, we choose the pair via

\[\bm{x}_{t},\bm{x}^{\prime}_{t}=\operatorname*{arg\,max}_{\bm{x},\bm{x}^{\prime} \in\mathcal{M}_{t}}\omega_{t}(\bm{x},\bm{x}^{\prime})\] (32)

where action space is restricted to \(\mathcal{M}_{t}\) and therefore,

\[\begin{split} s(h_{t}(\bm{x}^{\star},\bm{x}_{t}))& \leq 1/2+\omega_{t}(\bm{x}_{t},\bm{x}^{\star})\\ s(h_{t}(\bm{x}^{\star},\bm{x}^{\prime}_{t}))&\leq 1 /2+\omega_{t}(\bm{x}^{\prime}_{t},\bm{x}^{\star})\end{split}\] (33)

where we have used the identity \(s(-z)=1-s(z)\). Simultaneously for all \(t\geq 1\), we can bound the single-step dueling regret with probability greater than \(1-\delta\)

\[2r_{t}^{\mathrm{D}} =s(f(\bm{x}^{\star})-f(\bm{x}_{t}))+s(f(\bm{x}^{\star})-f(\bm{x}^ {\prime}_{t}))-1\] \[\leq s(h_{t}(\bm{x}^{\star},\bm{x}_{t}))+\omega_{t}(\bm{x}^{ \star},\bm{x}_{t})+s(h_{t}(\bm{x}^{\star},\bm{x}^{\prime}_{t}))+\omega_{t}( \bm{x}^{\star},\bm{x}^{\prime}_{t})-1\] (w.h.p.) \[\leq 2\left(\omega_{t}(\bm{x}^{\star},\bm{x}_{t})+\omega_{t}(\bm {x}^{\star},\bm{x}^{\prime}_{t})\right)\] Eq. ( 33 ) \[\leq 4\omega_{t}(\bm{x}_{t},\bm{x}^{\prime}_{t}))\] Eq. ( 32 )

where for the first inequality we have invoked Corollary 5. Then for the regret satisfies

\[R^{\mathrm{D}}(T)=\sum_{t=1}^{T}r_{t}^{\mathrm{D}} \leq\sqrt{T\sum_{t=1}^{T}(r_{t}^{\mathrm{D}})^{2}}\] \[\leq 2\beta_{T}^{\mathrm{D}}(\delta)\sqrt{T\sum_{t=1}^{T}(\sigma_ {t}^{\mathrm{D}})^{2}(\bm{x}_{t},\sqrt{\lambda\kappa})}\beta_{t}(\delta)\leq \beta_{T}^{\mathrm{D}}(\delta)\] \[\leq C_{2}\beta_{T}^{\mathrm{D}}(\delta)\sqrt{T\gamma_{t}^{ \mathrm{D}}}\] Lem. 14

with probability greater than \(1-\delta\) for all \(T\geq 1\). 

**Dueling Information Directed Sampling (IDS) Algorithm.** To choose actions at each iteration \(t\), MaxInP and MaxMinLCB require solving an optimization problem on \(\mathcal{X}\times\mathcal{X}\). The Dueling IDS approach addresses this issue and presents an algorithm which requires solving an optimization problem on \(\mathcal{X}\times[0,1]\) and is computationally more efficient when \(d_{0}>1\). This work considers kernelized utilities, however, assumes the probability of preference itself is in an RKHS and solves a kernelized ridge regression problem to estimate the probability \(s(h(\bm{x},\bm{x}^{\prime})\). In the following, we present an improved version of this algorithm, by considering the preference-based loss (6) for estimating the utility function. We modify the algorithm and the theoretical analysis to accommodate this.

Consider the sub-optimality gap \(\Delta(\bm{x}):=h(\bm{x}^{\star},\bm{x})\) for an action \(\bm{x}\in\mathcal{X}\). We may estimate this gap using the reward estimate maximizer \(\hat{\bm{x}}^{\star}_{t}\coloneqq\operatorname*{arg\,max}_{\bm{x}\in\mathcal{ X}}f_{t}(\bm{x})\). Suppose we choose \(\hat{\bm{x}}^{\star}_{t}\) as one of the actions. Then \(u_{t}\) shows an optimistic estimate of the highest obtainable reward at this step:

\[u_{t}\coloneqq\max_{\bm{x}\in\mathcal{X}}h(\bm{x},\hat{\bm{x}}^{\star}_{t})+ \tilde{\beta}_{t}\sigma_{t}^{D}(\bm{x},\bm{x}^{\star}_{t}).\]

where \(\tilde{\beta}_{t}\) is the exploration coefficient. We bound \(\Delta(\bm{x})\) by the estimated gap

\[\hat{\Delta}_{t}(\bm{x})\coloneqq u_{t}+h_{t}(\hat{\bm{x}}^{\star}_{t},\bm{x})\] (34)and show its uniform validity in Lemma 17. We can now propose the Kernelized Logistic IDS algorithm with preference feedback in Algorithm 4, as a variant of the algorithm of Kirschner and Krause.

**Theorem 16**.: _Let \(\delta\in(0,1]\) and for all \(t\geq 1\), set the exploration coefficient as \(\tilde{\beta}_{t}=\beta_{t}^{\mathrm{D}}(\delta)/L\). Then Algorithm 4 satisfies the anytime cumulative dueling regret guarantee of_

\[\mathbb{P}\left(\forall T\geq 0:R^{\mathrm{D}}(T)=\mathcal{O}\left(\beta_{T}^{ \mathrm{D}}(\delta)\sqrt{T(\gamma_{T}+\log 1/\delta)}\right)\right)\geq 1-\delta.\]

Proof of Theorem 16.: Our approach closely follows the proof of Kirschner and Krause (2021, Theorem 1). Let \(\mathcal{P}(\cdot)\) show the set of continuous probability distributions over a domain. Define the expected average gap for a policy \(\mu\in\mathcal{P}(\mathcal{X}\times\mathcal{X})\)

\[\hat{\Delta}_{t}(\mu)\coloneqq\frac{1}{2}\mathbb{E}_{\bm{x},\bm{x}^{\prime} \sim\mu}\hat{\Delta}_{t}(\bm{x})+\hat{\Delta}_{t}(\bm{x}^{\prime})\]

and the expected information ratio as

\[\Xi_{t}(\mu)\coloneqq\frac{\hat{\Delta}_{t}^{2}(\mu)}{\mathbb{E}_{\bm{x},\bm{ x}^{\prime}\sim\mu}\log\left(1+(\lambda\kappa)^{-1}\big{(}\sigma_{t}^{D}(\bm{x}, \bm{x}^{\prime})\big{)}^{2}\right)}.\]

Algorithm 4 draws actions via \(\mu_{t}=(1-p_{t})\delta_{(\bm{x}_{t}^{(1)},\bm{x}_{t}^{(1)})}+p_{t}\delta_{(\bm {x}_{t}^{(1)},\bm{x}_{t}^{(2)})}\), where \(\delta_{(\bm{x},\bm{x}^{\prime})}\) denotes a Direct delta. Then by Kirschner et al. (2020, Lemma 1),

\[\frac{1}{2}\sum_{t=1}^{T}h(\bm{x}^{\star},\bm{x}_{t})+h(\bm{x}^{\star},\bm{x} _{t}^{\prime})\leq\sqrt{\sum_{t=1}^{T}\Xi_{t}(\mu_{t})\left(\gamma_{T}+\mathcal{ O}(\log 1/\delta)\right)}+\mathcal{O}(\log T/\delta)\]

which allows us to bound the regret with probability greater than \(1-\delta\) as

\[R^{\mathrm{D}}(T)\leq L\sqrt{\sum_{t=1}^{T}\Xi_{t}(\mu_{t})\left(\gamma_{T}+ \mathcal{O}(\log 1/\delta)\right)}+\mathcal{O}(L\log T/\delta)\] (35)

since \(s(\cdot)\) with its domain restricted to \([-2B,2B]\) is \(L\)-Lipschitz. It remains to bound \(\Xi_{t}(\mu_{t})\), the expected information ratio for Algorithm 4. Now by definition of \(\mu_{t}\)

\[2\hat{\Delta}_{t}(\mu_{t}) =(2-p_{t})\hat{\Delta}_{t}(\bm{x}_{t}^{(1)})+p_{t}\Delta_{t}(\bm{ x}_{t}^{(2)})\] \[=(2-p_{t})\left(u_{t}+h_{t}(\bm{x}_{t}^{(1)},\bm{x}_{t}^{(1)}) \right)+p_{t}\Delta_{t}(\bm{x}_{t}^{(2)})\] \[=2(1-p_{t})u_{t}+p_{t}(\hat{\Delta}_{t}(\bm{x}_{t}^{(2)})+u_{t}),\]and similarly

\[\mathbb{E}_{\mu_{t}}\log\left(1+\frac{\sigma_{t}^{D}(\bm{x},\bm{x}^{ \prime})^{2}}{\lambda\kappa}\right) =(1-p_{t})\log\left(1+\frac{\sigma_{t}^{D}(\bm{x}_{t}^{(1)},\bm{x }_{t}^{(1)})^{2}}{\lambda\kappa}\right)+p_{t}\log\left(1+\frac{\sigma_{t}^{D}( \bm{x}_{t}^{(1)},\bm{x}_{t}^{(2)})^{2}}{\lambda\kappa}\right)\] \[=p_{t}\log\left(1+(\lambda\kappa)^{-1}\sigma_{t}^{D}(\bm{x}_{t}^{ (1)},\bm{x}_{t}^{(2)})^{2}\right)\qquad\qquad\qquad(\sigma_{t}^{D}(\bm{x},\bm{ x})=0)\]

allowing us to re-write the expected information ratio as

\[\Xi_{t}(\mu_{t}) =\frac{\left(2(1-p_{t})u_{t}+p_{t}(\hat{\Delta}_{t}(\bm{x}_{t}^{( 2)})+u_{t})\right)^{2}}{4p_{t}\log\left(1+(\lambda\kappa)^{-1}\sigma_{t}^{D}( \bm{x}_{t}^{(1)},\bm{x}_{t}^{(2)})^{2}\right)}\] \[\leq\frac{\left((1-p_{t})u_{t}+p_{t}\hat{\Delta}_{t}(\bm{x}_{t}^{ (2)})\right)^{2}}{p_{t}\log\left(1+(\lambda\kappa)^{-1}\sigma_{t}^{D}(\bm{x}_ {t}^{(1)},\bm{x}_{t}^{(2)})^{2}\right)}\qquad\qquad\qquad(u_{t}\leq\hat{ \Delta}_{t}(\bm{x}))\] \[=\min_{\bm{x},p}\frac{\left((1-p)u_{t}+p\hat{\Delta}_{t}(\bm{x}) \right)^{2}}{p\log\left(1+(\lambda\kappa)^{-1}\sigma_{t}^{D}(\bm{x}_{t}^{(1)},\bm{x})^{2}\right)}\qquad\qquad\qquad\qquad\text{Def.}\left(p_{t},\bm{x}_{t}^ {(2)}\right)\] \[\leq\min_{\bm{x}}\frac{\hat{\Delta}_{t}^{2}(\bm{x})}{\log\left(1+ (\lambda\kappa)^{-1}\sigma_{t}^{D}(\bm{x}_{t}^{(1)},\bm{x})^{2}\right)}. \qquad\qquad\qquad\qquad\text{Set }p=1\]

Now consider the definition of \(u_{t}\) and let \(\bm{z}_{t}\) denote the action for which \(u_{t}\) is achieved, i.e. \(\bm{z}_{t}=\arg\max h(\bm{x},\hat{\bm{x}}_{t}^{\star})+\bar{\beta}_{t}(\delta) \sigma_{t}^{\mathrm{D}}(\bm{x},\hat{\bm{x}}_{t}^{\star})\). Then

\[\hat{\Delta}_{t}(\bm{z}_{t})=h(\hat{\bm{x}}_{t}^{\star},\bm{z}_{t})+\bar{\beta }_{t}(\delta)\sigma_{t}^{\mathrm{D}}(\bm{z}_{t},\hat{\bm{x}}_{t}^{\star})+h( \bm{z}_{t},\hat{\bm{x}}_{t}^{\star})=\bar{\beta}_{t}(\delta)\sigma_{t}^{ \mathrm{D}}(\bm{x},\hat{\bm{x}}_{t}^{\star}),\]

therefore using the above chain of equations we may write

\[\Xi_{t}(\mu_{t}) \leq\min_{\bm{x}}\frac{\hat{\Delta}_{t}^{2}(\bm{x})}{\log\left(1+ \sigma_{t}^{D}(\bm{x}_{t}^{(1)},\bm{x})^{2}\right)}\] \[\leq\frac{\hat{\Delta}_{t}^{2}(\bm{z}_{t})}{\log\left(1+(\lambda \kappa)^{-1}\sigma_{t}^{D}(\bm{x}_{t}^{(1)},\bm{z}_{t})^{2}\right)}\] \[\leq\frac{\bar{\beta}_{t}^{2}(\delta)\sigma_{t}^{\mathrm{D}}(\bm{ z}_{t}\hat{\bm{x}}_{t}^{\star})^{2}}{\log\left(1+(\lambda\kappa)^{-1}\sigma_{t}^{D}( \bm{x}_{t}^{(1)},\bm{z}_{t})^{2}\right)}\] \[\leq\frac{4\bar{\beta}_{t}^{2}(\delta)}{\log\left(1+4(\lambda \kappa)^{-1}\right)}\] (36)

where last inequality holds due to the following argument. Recall that \(k(\bm{x},\bm{x})\leq 1\), implying that \(\sigma_{t}^{\mathrm{D}}(\bm{x},\bm{x}^{\prime})^{2}\leq 4\) and therefore \(\log(1+\sigma_{t}^{\mathrm{D}}(\bm{x},\bm{x}^{\prime})^{2})\geq\log(1+( \lambda\kappa)^{-1})\sigma_{t}^{\mathrm{D}}(\bm{x},\bm{x}^{\prime})^{2}/4\), similar to Lemma 14. To conclude the proof, from (35) and (36) it holds that

\[R^{\mathrm{D}}(T) \leq L\sqrt{\sum_{t=1}^{T}\Xi_{t}(\mu_{t})\left(\gamma_{T}+ \mathcal{O}(\log 1/\delta)\right)+\mathcal{O}(L\log T/\delta)}\] \[\leq L\sqrt{\sum_{t=1}^{T}\frac{4\bar{\beta}_{t}^{2}(\delta)}{ \log\left(1+4(\lambda\kappa)^{-1}\right)}\left(\gamma_{T}+\mathcal{O}(\log 1/ \delta)\right)+\mathcal{O}(L\log T/\delta)}\] \[\leq L\sqrt{\frac{4T\bar{\beta}_{T}^{2}(\delta)}{\log\left(1+4( \lambda\kappa)^{-1}\right)}\left(\gamma_{T}+\mathcal{O}(\log 1/\delta)\right)}+\mathcal{O}(L\log T/\delta)\] \[=\mathcal{O}\left(\beta_{T}^{\mathrm{D}}(\delta)\sqrt{T(\gamma_{T }+\log 1/\delta)}\right)\]

with probability greater than \(1-\delta\), simultaneously for all \(T\geq 1\).

### Helper Lemmas for Appendix C.3

**Lemma 17**.: _Let \(0<\delta<1\) and \(f\in\mathcal{H}_{k}\). Suppose \(\sup_{a\leq B}\dot{s}(a)=L\) and \(\sup_{a\leq B}1/\dot{s}(a)=\kappa\). Then_

\[\mathbb{P}(\forall t\geq 0,\bm{x}\in\mathcal{X}:\,\Delta(\bm{x})\leq 2 \hat{\Delta}_{t}(\bm{x}))\geq 1-\delta.\]

Proof of Lemma 17.: Note that for any three inputs \(\bm{x}_{1},\bm{x}_{2},\bm{x}_{3}\)

\[h(\bm{x}_{1},\bm{x}_{3})=h(\bm{x}_{1},\bm{x}_{2})+h(\bm{x}_{2}, \bm{x}_{3}).\] (37)

Therefore, from the definition of the estimated gap get

\[\hat{\Delta}_{t}(\bm{x}) =\max_{\bm{z}\in\mathcal{X}}h(\bm{z},\hat{\bm{x}}_{t}^{\star})+h _{t}(\hat{\bm{x}}_{t}^{\star},\bm{x})+\bar{\beta}_{t}(\delta)\sigma_{t}^{D}( \bm{z},\hat{\bm{x}}_{t}^{\star})\] \[=\max_{\bm{z}\in\mathcal{X}}h(\bm{z},\bm{x})+\bar{\beta}_{t}( \delta)\sigma_{t}^{D}(\bm{z},\hat{\bm{x}}_{t}^{\star})\] \[\geq h(\bm{x},\bm{x})+\bar{\beta}_{t}(\delta)\sigma_{t}^{D}(\bm{ x},\bm{x}_{t}^{\star})\] \[=\bar{\beta}_{t}(\delta)\sigma_{t}^{D}(\bm{x},\bm{x}_{t}^{\star}).\] (38)

Then going back to the definition of the true gap we may write

\[\Delta(\bm{x}) =\max_{\bm{z}\in\mathcal{X}}h(\bm{z},\bm{x})\] \[=\max_{\bm{z}\in\mathcal{X}}h(\bm{z},\hat{\bm{x}}_{t}^{\star})+h (\hat{\bm{x}}_{t}^{\star},\bm{x})\] \[\overset{\text{w.h.p.}}{\leq}\max_{\bm{z}\in\mathcal{X}}h_{t}^{P} (\bm{z},\hat{\bm{x}}_{t}^{\star})+h_{t}(\hat{\bm{x}}_{t}^{\star},\bm{x})+\bar{ \beta}_{t}(\delta)\Big{(}\sigma_{t}^{D}(\bm{z},\hat{\bm{x}}_{t}^{\star})+ \sigma_{t}^{D}(\hat{\bm{x}}_{t}^{\star},\bm{x})\Big{)}\] Lem. 18 \[=u_{t}+h_{t}^{P}(\hat{\bm{x}}_{t}^{\star},\bm{x})+\bar{\beta}_{t} (\delta)\sigma_{t}^{D}(\hat{\bm{x}}_{t}^{\star},\bm{x})\] \[=\hat{\Delta}_{t}(\bm{x})+\bar{\beta}_{t}(\delta)\sigma_{t}^{D}( \bm{x}_{t}^{\star},\bm{x})\] \[\leq 2\hat{\Delta}_{t}(\bm{x})\]

with probability greater than \(1-\delta\). 

**Lemma 18**.: _Assume \(f\in\mathcal{H}_{k}\). Suppose \(\sup_{a\leq B}1/\dot{s}(a)=\kappa\). Then for any \(0<\delta<1\)_

\[\mathbb{P}\left(\forall t\geq 1,x\in\mathcal{X}:\,\left|h(\bm{x}, \bm{x}^{\prime})-h_{t}^{P}(\bm{x},\bm{x}^{\prime})\right|\leq\bar{\beta}_{t} (\delta)\sigma_{t}^{D}(\bm{x},\bm{x}^{\prime};\sqrt{\lambda\kappa})\right)\geq 1-\delta\]

_where_

\[\bar{\beta}_{t}(\delta)\coloneqq 2B+\sqrt{\frac{\kappa}{\lambda}}\sqrt{2 \log 1/\delta+2\gamma_{t}(\sqrt{\lambda\kappa})}.\]

Proof of Lemma 18.: This lemma is effectively a weaker parallel of Corollary 5. We have

\[\left|h(\bm{x},\bm{x}^{\prime})-h_{t}^{P}(\bm{x},\bm{x}^{\prime})\right| =\left|f(\bm{x},)-f(\bm{x}^{\prime})-(f_{t}^{P}(\bm{x},)-f_{t}^{ P}(\bm{x}^{\prime}))\right|\] \[=\left|\bm{\psi}^{\top}(\bm{x},\bm{x}^{\prime})(\bm{\theta}^{ \star}-\bm{\theta}_{t}^{P})\right|\] \[\leq\left\|\bm{\psi}(\bm{x},\bm{x}^{\prime})\right\|_{(V_{t}^{D -1}}\left\|\bm{\theta}^{\star}-\bm{\theta}_{t}^{P}\right\|_{V_{t}^{D}}\] \[\overset{\text{w.h.p.}}{\leq}\sqrt{\lambda\kappa}\bar{\beta}_{t}( \delta)\|\bm{\psi}(\bm{x},\bm{x}^{\prime})\|_{(V_{t}^{D})^{-1}}\] Lem. 9 \[\leq\bar{\beta}_{t}(\delta)\sigma_{t}^{D}(\bm{x},\sqrt{\lambda \kappa})\] Lem. 13

where the third to last inequality holds with probability greater than \(1-\delta\), but the rest of the inequalities hold deterministically.

## Appendix D Details of Experiments

**Test Environments.** We use a wide range of target functions common to the optimization literature [Jamil and Yang, 2013], to evaluate the robustness of MaxMinLCB. The results are reported in Table 1 and Table 2. Note that for the experiments we negate them all to get utilities. We use a uniform grid of \(100\) points over their specified domains and scale the utility values to \([-3,3]\).

* Ackley: \(\mathcal{X}=[-5,5]^{d},d=2\) \[f(\bm{x})=-20\exp\left(-0.2\sqrt{\frac{1}{d}\sum_{i=1}^{d}x_{i}^{2}}\right)- \exp\left(\frac{1}{d}\sum_{i=1}^{d}\cos(2\pi x_{i})\right)+20+\exp(1)\]
* Branin: \(\mathcal{X}=[-5,10]\times[0,15]\) \[f(\bm{x})=\left(x_{2}-\frac{5.1}{4\pi^{2}}x_{1}^{2}+\frac{5}{\pi}x_{1}-6\right) ^{2}+10\left(1-\frac{1}{8\pi}\right)\cos(x_{1})+10\]
* Eggholder: \(\mathcal{X}=[-512,512]^{2}\) \[f(\bm{x})=-(x_{2}+47)\sin\left(\sqrt{|x_{2}+\frac{x_{1}}{2}+47|}\right)-x_{1 }\sin\left(\sqrt{|x_{1}-(x_{2}+47)|}\right)\]
* Holder: \(\mathcal{X}=[-10,10]^{2}\) \[f(\bm{x})=-|\sin(x_{1})\cos(x_{2})\exp\left(|1-\frac{\sqrt{x_{1}^{2}+x_{2}^{2}} }{\pi}|\right)|\]
* Matyas: \(\mathcal{X}=[-10,10]^{2}\) \[f(\bm{x})=0.26(x_{1}^{2}+x_{2}^{2})-0.48x_{1}x_{2}\]
* Michalewicz: \(\mathcal{X}=[0,\pi]^{d},d=2,m=10\) \[f(\bm{x})=-\sum_{i=1}^{d}\sin(x_{i})\sin^{2m}\left(\frac{ix_{i}^{2}}{\pi}\right)\]
* Rosenbrock: \(\mathcal{X}=[-5,10]^{2}\) \[f(\bm{x})=\sum_{i=1}^{d-1}\left[100(x_{i+1}-x_{i}^{2})^{2}+(x_{i}-1)^{2}\right]\]

Figure 3: Confidence sets for an illustrative problem with \(3\) arms at a single time step. Annotated arrows highlight the action selection for three common approaches. MaxMinLCB selects the action pair \((1,2)\) with the least regret. Upper-bound maximization (Optimism) and information maximization (Max Info) choose sub-optimal arms.

``` Input \((\beta_{t}^{\mathrm{D}})_{t\geq 1}\). Let \(\mathcal{L}\) be any action from \(\mathcal{X}\) for\(t\geq 1\)do for\(j=1,\ldots,2^{t}\)do  Select \(\bm{x}_{t}^{\prime}\) uniformly randomly from \(\mathcal{L}\)  Select \(\bm{x}_{t}=\operatorname*{arg\,max}_{\bm{x}\in\mathcal{M}_{t}}s(h_{t}(\bm{x}, \bm{x}_{t}^{\prime}))+\beta_{t}^{\mathrm{D}}\sigma_{t}^{\mathrm{D}}(\bm{x},\bm {x}_{t}^{\prime})\)  Observe \(y_{t}\) and append history.  Update \(h_{t+1}\) and \(\sigma_{t+1}^{\mathrm{D}}\) endfor \(\mathcal{L}\leftarrow\) the multi-set of actions played as \(\bm{x}_{t}^{\prime}\) in the last for-loop over index \(j\) endfor ```

**Algorithm 6**MultiSBM (Ailon et al., 2014)

``` Input \((\beta_{t}^{\mathrm{D}})_{t\geq 1}\). for\(t\geq 1\)do  Set \(\bm{x}_{t}\leftarrow\bm{x}_{t-1}^{\prime}\)  Select \(\bm{x}_{t}^{\prime}=\operatorname*{arg\,max}_{\bm{x}\in\mathcal{M}_{t}}s(h_{t} (\bm{x},\bm{x}_{t}))+\beta_{t}^{\mathrm{D}}\sigma_{t}^{\mathrm{D}}(\bm{x},\bm {x}_{t})\)  Observe \(y_{t}\) and append history.  Update \(h_{t+1}\) and \(\sigma_{t+1}^{\mathrm{D}}\) and the set of plausible maximizers endfor ```

**Algorithm 7**RUCB (Zoghi et al., 2014)

``` Input \((\beta_{t}^{\mathrm{D}})_{t\geq 1}\). for\(t\geq 1\)do  Select \(\bm{x}_{t}^{\prime}\) uniformly randomly from \(\mathcal{M}_{t}\)  Select \(\bm{x}_{t}=\operatorname*{arg\,max}_{\bm{x}\in\mathcal{M}_{t}}s(h_{t}(\bm{x}, \bm{x}_{t}^{\prime}))+\beta_{t}^{\mathrm{D}}\sigma_{t}^{\mathrm{D}}(\bm{x},\bm {x}_{t}^{\prime})\)  Observe \(y_{t}\) and append history.  Update \(h_{t+1}\) and \(\sigma_{t+1}^{\mathrm{D}}\) and the set of plausible maximizers \[\mathcal{M}_{t+1}=\{\bm{x}\in\mathcal{X}|\,\forall\bm{x}^{\prime}\in\mathcal{ X}:\,s(h_{t+1}(\bm{x},\bm{x}^{\prime}))+\beta_{t+1}^{\mathrm{D}}\sigma_{t+1}^{ \mathrm{D}}(\bm{x},\bm{x}^{\prime})>1/2\}.\] endfor ```

**Algorithm 8**RUCB (Zoghi et al., 2014)

**Acquisition Function Maximization.** In our computations, to eliminate additional noise coming from approximate solvers, we use an exhaustive search over the domain for the action selection of LGP-UCB, MaxMinLCB, and other presented algorithms. For the numerical experiments presented in this paper, we do not consider this as a practical limitation. Due to our efficient implementation in JAX, this optimization step can be carried out in parallel and seamlessly support accelerator devices such as GPUs and TPUs.

**Hyper-parameters for Logistic Bandits.** We set \(\delta=0.1\) for all algorithms. For GP-UCB and LGP-UCB, we set \(\beta=1\), and \(0.25\) for the noise variance. We use the Radial Basis Function (RBF) kernel and choose the variance and length scale parameters from \([0.1,1.0]\) to optimize their performance separately. For LGP-UCB, we tuned \(\lambda\), the \(L2\) penalty coefficient in Proposition 1, on the grid \([0.0,0.1,1.0,5.0]\) and \(B\) on \([1.0,2.0,3.0]\). The hyper-parameter selections were done for each algorithm separately to create a fair comparison.

**Hyper-parameters for Preference-based Bandits.** We tune the same parameters of LGP-UCB for the preference feedback bandit problem on the following grid: \(\lambda\in[0,0.1,1]\), \(B\in[1,2,3]\), and \([0.1,1]\) for the kernel variance and length scale. The same hyper-parameters are tuned separately for every baseline.

**Pseudo-code for Baselines.** Algorithm 5, Algorithm 6, and Algorithm 7 described the baselines used for the benchmark of Section 6.2. MaxInP and IDS are defined in Algorithm 3 and Algorithm 4,

[MISSING_PAGE_FAIL:30]

Figure 5: Top to bottom Regret for Eggholder, Hlder, Matyas Michalewicz, Rosenbrock functions, with logistic (left) and preference (right) feedback.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We provide theoretical guarantees of the proposed algorithms in Section 4 and Section 5. In Section 6, we provide the results for our numerical experiments supporting our claims. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The paper is of theoretical nature. All theoretical assumptions are presented in the text, and we discuss how limiting they are. This is mainly in the Problem Setting section, or theorem statements. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Proposition 1, Theorem 2, Corollary 3, Proposition 4, Corollary 5, Theorem 6 describe our theoretical proofs with detailed explanation of the assumptions. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We describe the experimental setting in Section 6 while providing further details on the hyperparameter selection, pseudocode, and utility function definition in Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.

In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We include the code used to carry out the experiments in the Supplementary Material. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide comprehensive explanations of the setting, hyperparameters, algorithms, and functions used in Section 6 and in Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report all results in our figures and tables as the average and standard error over \(20\) random seeds. Guidelines: * The answer NA means that the paper does not include experiments.

* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We describe computation resources and time used for the experiments in Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The authors have reviewed the NeurIPS Code of Ethics and conducted the research following the guidelines. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA]Justification: The research presented in this paper is theoretical without any immediate societal impact. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

* Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: To the best of the authors' knowledge, there is no such risk involved. Guidelines:
* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licenses for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Assets used to conduct the research presented in this paper are always cited and properly credited. Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.