# Generator Identification for Linear SDEs with Additive and Multiplicative Noise

 Yuanyuan Wang

The University of Melbourne

yuanyuanw2@student.unimelb.edu.au

&Xi Geng

The University of Melbourne

xi.geng@unimelb.edu.au

&Wei Huang

The University of Melbourne

wei.huang@unimelb.edu.au

&Biwei Huang

University of California, San Diego

bih007@ucsd.edu

&Mingming Gong

The University of Melbourne

mingming.gong@unimelb.edu.au

Corresponding author.

###### Abstract

In this paper, we present conditions for identifying the **generator** of a linear stochastic differential equation (SDE) from the distribution of its solution process with a given fixed initial state. These identifiability conditions are crucial in causal inference using linear SDEs as they enable the identification of the post-intervention distributions from its observational distribution. Specifically, we derive a sufficient and necessary condition for identifying the generator of linear SDEs with additive noise, as well as a sufficient condition for identifying the generator of linear SDEs with multiplicative noise. We show that the conditions derived for both types of SDEs are generic. Moreover, we offer geometric interpretations of the derived identifiability conditions to enhance their understanding. To validate our theoretical results, we perform a series of simulations, which support and substantiate the established findings.

## 1 Introduction

Stochastic differential equations (SDEs) are a powerful mathematical tool for modelling dynamic systems subject to random fluctuations. These equations are widely used in various scientific disciplines, including finance [11, 30, 40], physics [53, 55, 58], biology [2, 8, 61] and engineering [18, 44, 55]. In recent years, SDEs have garnered growing interest in the machine learning research community. Specifically, they have been used for tasks such as modelling time series data [19, 21, 33] and estimating causal effects [5, 36, 47].

To enhance understanding we first introduce the SDEs of our interest, which are multidimensional linear SDEs with additive and multiplicative noise, respectively. Consider an \(m\)-dimensional standard Brownian motion defined on a filtered probability space \((\Omega,\mathcal{F},\mathbb{P},\{\mathcal{F}_{t}\})\), denoted by \(W:=\{W_{t}=[W_{1,t},\ldots,W_{m,t}]^{\top}:0\leqslant t<\infty\}\). Let \(X_{t}\in\mathbb{R}^{d}\) be the state at time \(t\) and let \(x_{0}\in\mathbb{R}^{d}\) be a constant vector denoting the initial state of the system, we present the forms of the aforementioned two linear SDEs.

1. Linear SDEs with additive noise. \[dX_{t}=AX_{t}dt+GdW_{t}\,,\;\;\;X_{0}=x_{0}\,,\] (1) where \(0\leqslant t<\infty\), \(A\in\mathbb{R}^{d\times d}\) and \(G\in\mathbb{R}^{d\times m}\) are some constant matrices.
2. Linear SDEs with multiplicative noise. \[dX_{t}=AX_{t}dt+\sum_{k=1}^{m}G_{k}X_{t}dW_{k,t}\,,\;\;\;X_{0}=x_{0}\,,\] (2) where \(0\leqslant t<\infty\), \(A,G_{k}\in\mathbb{R}^{d\times d}\) for \(k=1,\ldots,m\) are some constant matrices.

Linear SDEs are wildly used in financial modeling for tasks like asset pricing, risk assessment, and portfolio optimization [3; 10; 24]. Where they are used to model the evolution of financial variables, such as stock prices and interest rates. Furthermore, linear SDEs are also used in genomic research, for instance, they are used for modeling the gene expression in the yeast microorganism Saccharomyces Cerevisiae [17]. The identifiability analysis of linear SDEs is essential for reliable causal inference of dynamic systems governed by these equations. For example, in the case of Saccharomyces Cerevisiae, one aims to identify the system such that making reliable causal inference when interventions are introduced. Such interventions may involve deliberate knockout of specific genes to achieve optimal growth of an organism. In this regard, identifiability analysis plays a pivotal role in ensuring reliable predictions concerning the impact of interventions on the system.

Previous studies on identifiability analysis of linear SDEs have primarily focused on Gaussian diffusions, as described by the SDE (1) [6; 16; 23; 28; 35; 42]. These studies are typically based on observations located on one trajectory of the system and thus require restrictive identifiability conditions, such as the ergodicity of the diffusion or other restrictive requirements on the eigenvalues of matrix \(A\). However, in practical applications, multiple trajectories of the dynamic system can often be accessed [15; 31; 45; 54]. In particular, these multiple trajectories may start from the same initial state, e.g., in experimental studies where repeated trials or experiments are conducted under the same conditions [9; 12; 26; 29] or when the same experiment is performed on multiple identical units [48]. To this end, this work presents an identifiability analysis for linear SDEs based on the distribution of the observational process with a given fixed initial state. Furthermore, our study is not restricted to Gaussian diffusions (1), but also encompasses linear SDEs with multiplicative noise (2). Importantly, the conditions derived for both types of SDEs are generic, meaning that the set of system parameters that violate the proposed conditions has Lebesgue measure zero.

Traditional identifiability analysis of dynamic systems focuses on deriving conditions under which a unique set of parameters can be obtained from error-free observational data. However, our analysis of dynamic systems that are described by SDEs aims to uncover conditions that would enable a unique generator to be obtained from its observational distribution. Our motivation for identifying generators of SDEs is twofold. Firstly, obtaining a unique set of parameters from the distribution of a stochastic process described by an SDE is generally unfeasible. For example, in the SDE (1), parameter \(G\) cannot be uniquely identified since one can only identify \(GG^{\top}\) based on the distribution of its solution process [17; 28]. Secondly, the identifiability of an SDE's generator suffices for reliable causal inferences for this system. Note that, in the context of SDEs, the main task of causal analysis is to identify the post-intervention distributions from the observational distribution. As proposed in [17], for an SDE satisfying specific criteria, the post-intervention distributions are identifiable from the generator of this SDE. Consequently, the intricate task of unraveling causality can be decomposed into two constituent components through the generator. This paper aims to uncover conditions under which the generator of a linear SDE attains identifiability from the observational distribution. By establishing these identifiability conditions, we can effectively address the causality task for linear SDEs.

In this paper, we present a sufficient and necessary identifiability condition for the generator of linear SDEs with additive noise (1), along with a sufficient identifiability condition for the generator of linear SDEs with multiplicative noise (2).

## 2 Background knowledge

In this section, we introduce some background knowledge of linear SDEs. In addition, we provide a concise overview of the causal interpretation of SDEs, which is a critical aspect of understanding the nature and dynamics of these equations. This interpretation also forms a strong basis for the motivation of this research.

### Background knowledge of linear SDEs

The solution to the SDE (1) can be explicitly expressed as (cf. [50]):

\[X_{t}:=X(t;x_{0},A,G)=e^{At}x_{0}+\int_{0}^{t}e^{A(t-s)}GdW_{s}\,.\] (3)

Note that in the context of our study, the solution stands for the strong solution, refer to [22] for its detailed definition.

In general, obtaining an explicit expression for the solution to the SDE (2) is not feasible. In fact, an explicit solution can be obtained when the matrices \(A,G_{1},\ldots,G_{k}\) commute, that is when

\[AG_{k}=G_{k}A\ \ \text{ and }\ G_{k}G_{l}=G_{l}G_{k}\] (4)

holds for all \(k,l=1,\ldots,m\) (cf. [25]). However, the conditions described in (4) are too restrictive and impractical. Therefore, this study will focus on the general case of the SDE (2).

We know that both the SDE (1) and the SDE (2) admit unique solutions that manifest as continuous stochastic processes [22]. A \(d\)-dimensional stochastic process is a collection of \(\mathbb{R}^{d}\)-valued random variables, denoted as \(X=\{X_{t};0\leqslant t<\infty\}\) defined on some probability space. When comparing two stochastic processes, \(X\) and \(\tilde{X}\), that are defined on the same probability space \((\Omega,\mathcal{F},\mathbb{P})\), various notions of equality may be considered. In this study, we adopt the notion of equality with respect to their distributions, which is a weaker requirement than strict equivalence, see [22] for relevant notions. We now present the definition of the distribution of a stochastic process.

**Definition 2.1**.: _Let \(X\) be a random variable on a probability space \((\Omega,\mathcal{F},\mathbb{P})\) with values in a measurable space \((S,\mathcal{B}(S))\), i.e., the function \(X:\Omega\to S\) is \(\mathcal{F}/\mathcal{B}(S)\)-measurable. Then, the distribution of the random variable \(X\) is the probability measure \(P^{X}\) on \((S,\mathcal{B}(S))\) given by_

\[P^{X}(B)=\mathbb{P}(X\in B)=\mathbb{P}\{\omega\in\Omega:X(\omega)\in B\}\,,\ \ B\in\mathcal{B}(S)\,.\]

_When \(X:=\{X_{t};0\leqslant t<\infty\}\) is a continuous stochastic process on \((\Omega,\mathcal{F},\mathbb{P})\), and \(S=C[0,\infty)\), such an \(X\) can be regarded as a random variable on \((\Omega,\mathcal{F},\mathbb{P})\) with values in \((C[0,\infty),\mathcal{B}(C[0,\infty)))\), and \(P^{X}\) is called the distribution of \(X\). Here \(C[0,\infty)\) stands for the space of all continuous, real-valued functions on \([0,\infty]\)._

It is noteworthy that the distribution of a continuous process can be uniquely determined by its finite-dimensional distributions. Hence, if two stochastic processes, labelled as \(X\) and \(\tilde{X}\), share identical finite-dimensional distributions, they are regarded as equivalent in distribution, denoted by \(X\stackrel{{\mathrm{d}}}{{=}}\tilde{X}\). Relevant concepts and theories regarding this property can be found in [22].

The generator of a stochastic process is typically represented by a differential operator that acts on functions. It provides information about how a function evolves over time in the context of the underlying stochastic process. Mathematically, the generator of a stochastic process \(X_{t}\) can be defined as

\[(\mathcal{L}f)(x)=\lim_{s\to 0}\frac{\mathbb{E}[f(X_{t+s})-f(X_{t})|X_{t}=x]}{s},\]

where \(f\) is a suitably regular function.

In the following, we present the generator of the SDEs under consideration. Obviously, both the SDE (1) and the SDE (2) conform to the general form:

\[dX_{t}=b(X_{t})dt+\sigma(X_{t})dW_{t}\,,\ \ X_{0}=x_{0}\,.\] (5)

where \(b\) and \(\sigma\) are locally Lipschitz continuous in the space variable \(x\). The generator \(\mathcal{L}\) of the SDE (5) can be explicitly computed by utilizing Ito's formula (cf. [50]).

**Proposition 2.1**.: _Let \(X\) be a stochastic process defined by the SDE (5). The generator \(\mathcal{L}\) of \(X\) on \(C_{b}^{2}(\mathbb{R}^{d})\) is given by_

\[(\mathcal{L}f)(x):=\sum_{i=1}^{d}b_{i}(x)\frac{\partial f(x)}{\partial x_{i}}+ \frac{1}{2}\sum_{i,j=1}^{d}c_{ij}(x)\frac{\partial^{2}f(x)}{\partial x_{i} \partial x_{j}}\] (6)

_for \(f\in C_{b}^{2}(\mathbb{R}^{d})\) and \(x\in\mathbb{R}^{d}\), where \(c(x)=\sigma(x)\cdot\sigma(x)^{\top}\) is a \(d\times d\) matrix, and \(C_{b}^{2}(\mathbb{R}^{d})\) denotes the space of continuous functions on \(\mathbb{R}^{d}\) that have bounded derivatives up to order two._

### Causal interpretation of SDEs

An important motivation for the identification of the generator of an SDE lies in the desire to infer reliable causality within dynamic models described by SDEs. In this subsection, we aim to provide some necessary background knowledge on the causal interpretation of SDEs. Consider the general SDE framework described as:

\[dX_{t}=a(X_{t})dZ_{t}\,,\quad X_{0}=x_{0}\,,\] (7)

where \(Z\) is a \(p\)-dimensional semimartingale and \(a:\mathbb{R}^{d}\to\mathbb{R}^{d\times p}\) is a continuous mapping. By writing the SDE (7) in integral form

\[X_{t}^{i}=x_{0}^{i}+\sum_{j=1}^{p}\int_{0}^{t}a_{ij}(X_{s})dZ_{s}^{j}\,,\quad i \leqslant d\,.\] (8)

The authors of [17] proposed a mathematical definition of the SDE resulting from an intervention to the SDE (8). In the following, \(X^{(-l)}\) denotes the \((d-1)\)-dimensional vector that results from the removal of the \(l\)-th coordinate of \(X\in\mathbb{R}^{d}\).

**Definition 2.2**.: _[_17_, Definition 2.4.]_ _Consider some \(l\leqslant d\) and \(\zeta:\mathbb{R}^{d-1}\to\mathbb{R}\). The SDE arising from (8) under the intervention \(X_{t}^{l}:=\zeta(X_{t}^{(-l)})\) is the \((d-1)\)-dimensional equation_

\[(Y^{(-l)})_{t}^{i}=x_{0}^{i}+\sum_{j=1}^{p}\int_{0}^{t}b_{ij}(Y_{s}^{(-l)})dZ_ {s}^{j}\,,\quad i\neq l\,,\] (9)

_where \(b:\mathbb{R}^{d-1}\to\mathbb{R}^{(d-1)\times p}\) is defined by \(b_{ij}(y)=a_{ij}(y_{1},\ldots,\zeta(y),\ldots,y_{d})\) for \(i\neq l\) and \(j\leqslant p\) and the \(\zeta(y)\) is on the \(l\)-th coordinate._

Definition 2.2 presents a natural approach to defining how interventions should affect dynamic systems governed by SDEs. We adopt the same notations as used in [17]. Assuming (8) and (9) have unique solutions for all interventions, we refer to (8) as the observational SDE, to its solution as the observational process, to the distribution of its solution as observational distribution, to (9) as the post-intervention SDE, to the solution of (9) as the post-intervention process, and to the distribution of the solution of (9) as the post-intervention distribution. The authors in [17] related Definition 2.2 to mainstream causal concepts by establishing a mathematical connection between SDEs and structural equation models (SEMs). Specifically, the authors showed that under regularity assumptions, the solution to the post-intervention SDE is equal to the limit of a sequence of interventions in SEMs based on the Euler scheme of the observational SDE. Despite the fact that the parameters of the SDEs are generally not identifiable from the observational distribution, the post-intervention distributions can be identified, thus enabling causal inference of the system. To this end, Sokol and Hansen [17] derived a condition under which the generator associated with the observational SDE allows for the identification of the post-intervention distributions. We present the corresponding theory as follows.

**Lemma 2.1**.: _[_17_, Theorem 5.3.]_ _Consider the SDEs_

\[dX_{t}=a(X_{t})dZ_{t}\,,\quad X_{0}=x_{0}\,,\] (10)

\[d\tilde{X}_{t}=\tilde{a}(\tilde{X}_{t})d\tilde{Z}_{t}\,,\quad\tilde{X}_{0}= \tilde{x}_{0}\,,\] (11)

_where \(Z\) is a \(p\)-dimensional Levy process and \(\tilde{Z}\) is a \(\tilde{p}\)-dimensional Levy process. Assume that (10) and (11) have the same generator, that \(a:\mathbb{R}^{d}\to\mathbb{R}^{d\times p}\) and \(\zeta:\mathbb{R}^{d-1}\to\mathbb{R}\) are Lipschitz and that the initial values have the same distribution. Then the post-intervention distributions of doing \(X^{l}:=\zeta(X^{(-l)})\) in (10) and doing \(\tilde{X}^{l}:=\zeta(\tilde{X}^{(-l)})\) in (11) are equal for any choice of \(\zeta\) and \(l\)._

A main task in the causality research community is to uncover the conditions under which the post-intervention distributions are identifiable from the observational distribution. In the context of dynamic systems modelled in SDEs, similar conditions need to be derived. Lemma 2.1 establishes that, for SDEs with a Levy process as the driving noise, the post-intervention distributions can be identifiable from the generator. Nevertheless, a gap remains between the observational distribution and the SDE generator's identifiability. This work aims to address this gap by providing conditions under which the generator is identifiable from the observational distribution.

## 3 Main results

In this section, we present some prerequisites first, and then we present the main theoretical results of our study, which include the condition for the identifiability of generator that is associated with the SDE (1) / SDE (2) from the distribution of the corresponding solution process.

### Prerequisites

We first show that both the SDE (1) and the SDE (2) satisfy the conditions stated in Lemma 2.1.

**Lemma 3.1**.: _Both the SDE (1) and the SDE (2) can be expressed as the form of (10), with \(Z\) being a \(p\)-dimensional Levy process, and \(a:\mathbb{R}^{d}\to\mathbb{R}^{d\times p}\) being Lipschitz._

The proof of Lemma 3.1 can be found in Appendix A.1. This lemma suggests that Lemma 2.1 applies to both the SDE (1) and the SDE (2), given that they meet the specified conditions. Therefore, for either SDE, deriving the conditions that allow for the generator to be identifiable from the observational distribution is sufficient. By applying Lemma 2.1, when the intervention function \(\zeta\) is Lipschitz, the post-intervention distributions can be identified from the observational distribution under these conditions.

We then address the identifiability condition of the generator \(\mathcal{L}\) defined by (6).

**Proposition 3.1**.: _Let \(\mathcal{L}\) and \(\tilde{\mathcal{L}}\) be generators of stochastic processes defined by the form of the SDE (5) on \(C^{2}_{b}(\mathbb{R}^{d})\), where \(\mathcal{L}\) is given by (6) and \(\tilde{\mathcal{L}}\) is given by the same expression, with \(\tilde{b}(x)\) and \(\tilde{c}(x)\) substituted for \(b(x)\) and \(c(x)\). It then holds that the two generators \(\mathcal{L}=\tilde{\mathcal{L}}\) if and only if \(b(x)=\tilde{b}(x)\) and \(c(x)=\tilde{c}(x)\) for all \(x\in\mathbb{R}^{d}\)._

The proof of Proposition 3.1 can be found in Appendix A.2. This proposition states that for stochastic processes defined by the SDE (5), the generator is identifiable from functions associated with its coefficients: \(b(x)\) and \(c(x)=\sigma(x)\cdot\sigma(x)^{\top}\).

### Conditions for identifying generators of linear SDEs with additive noise

Expressing the SDE (1) in the form given by (5) yields \(b(x)=Ax\) and \(c(x)=GG^{\top}\). Therefore, based on Proposition 3.1, we define the identifiability of the generator of the SDE (1) as follows.

**Definition 3.1** (\((x_{0},A,G)\)-identifiability).: _For \(x_{0}\in\mathbb{R}^{d},A\in\mathbb{R}^{d\times d}\) and \(G\in\mathbb{R}^{d\times m}\), the generator of the SDE (1) is said to be identifiable from \(x_{0}\), if for all \(\tilde{A}\in\mathbb{R}^{d\times d}\) and all \(\tilde{G}\in\mathbb{R}^{d\times m}\), with \((A,GG^{\top})\neq(\tilde{A},\tilde{G}\tilde{G}^{\top})\), it holds that \(X(\cdot;x_{0},A,G)\lx@note{footnote}{$X(\cdot;x_{0},A,G)=\{X(t;x_{0},A,G):0 \leqslant t<\infty\}$}\)._

In the following, we begin by introducing two lemmas that serve as the foundation for deriving our main identifiability theorem.

**Lemma 3.2**.: _For \(x_{0}\in\mathbb{R}^{d},A,\tilde{A}\in\mathbb{R}^{d\times d}\) and \(G,\tilde{G}\in\mathbb{R}^{d\times m}\), let \(X_{t}:=X(t;x_{0},A,G)\), \(\tilde{X}_{t}:=X(t;x_{0},\tilde{A},\tilde{G})\), then \(X(\cdot;x_{0},A,G)\stackrel{{\text{\rm d}}}{{=}}X(\cdot;x_{0}, \tilde{A},\tilde{G})\) if and only if the mean \(\mathbb{E}[X_{t}]=\mathbb{E}[\tilde{X}_{t}]\) and the covariance \(\mathbb{E}\{(X_{t+h}-\mathbb{E}[X_{t+h}])(X_{t}-\mathbb{E}[X_{t}])^{\top}\}= \mathbb{E}\{(\tilde{X}_{t+h}-\mathbb{E}[\tilde{X}_{t+h}])(\tilde{X}_{t}- \mathbb{E}[\tilde{X}_{t}])^{\top}\}\) for all \(0\leqslant t<\infty\) and \(0\leqslant h<\infty\)._

The proof of Lemma 3.2 can be found in Appendix A.3. This lemma states that for stochastic processes modelled by the SDE (1), the equality of the distribution of two processes can be deconstructed as the equality of the mean and covariance of the state variables at all time points. Calculation shows

\[\mathbb{E}[X_{t}] =e^{At}x_{0}\,,\] (12) \[V(t,t+h) :=\mathbb{E}\{(X_{t+h}-\mathbb{E}[X_{t+h}])(X_{t}-\mathbb{E}[X_{t }])^{\top}\}\] \[=e^{Ah}V(t)\,,\]

where \(V(t):=V(t,t)\). Please refer to the proof A.5 of Theorem 3.4 for the detailed calculations. It can be easily checked that \(\mathbb{E}[X_{t}]\) follows the linear ordinary differential equation (ODE)

\[\dot{m}(t)=Am(t),\ \ \ m(0)=x_{0}\,,\] (13)

where \(\dot{m}(t)\) denotes the first derivative of function \(m(t)\) with respect to time \(t\). Similarly, each column of the covariance \(V(t,t+h)\) also follows the linear ODE (13) but with a different initial state: the corresponding column of \(V(t)\). This observation allows us to leverage not only the characteristics of the SDE (1), but also the established theories [43, 57] on identifiability analysis for the ODE (13), to derive the identifiability conditions for the generator of the SDE (1).

We adopt the same setting as in [43], discussing the case where \(A\) has distinct eigenvalues. Because random matrix theory suggests that almost every \(A\in\mathbb{R}^{d\times d}\) has \(d\) distinct eigenvalues with respect to the Lebesgue measure on \(\mathbb{R}^{d\times d}\)[43]. And the Jordan decomposition of such a matrix \(A\) follows a straightforward form which is helpful for deriving the geometric interpretation of the proposed identifiability condition. The Jordan decomposition can be expressed as \(A=Q\Lambda Q^{-1}\), where

\[\Lambda=\begin{bmatrix}J_{1}&&\\ &\ddots&\\ &&J_{K}\end{bmatrix},\text{with}\;J_{k}=\left\{\begin{array}{ll}\lambda_{k},&\text{if}\;k=1,\ldots,K_{1}\,,\\ \begin{bmatrix}a_{k}&-b_{k}\\ b_{k}&a_{k}\end{bmatrix},&\text{if}\;k=K_{1}+1,\ldots,K\,.\end{array}\right.\]

\[Q=[Q_{1}|\ldots|Q_{K}]=[v_{1}|\ldots|v_{d}]\,,\]

\[Q_{k}=\left\{\begin{array}{ll}v_{k},&\\ \left[v_{2k-K_{1}-1}\right]v_{2k-K_{1}}\right],&\text{if}\;k=K_{1}+1,\ldots,K \,,\end{array}\right.\]

where \(\lambda_{k}\) is a real eigenvalue of \(A\) and \(v_{k}\) is the corresponding eigenvector of \(\lambda_{k}\), for \(k=1,\ldots,K_{1}\). For \(k=K_{1}+1,\ldots,K\), \([v_{2k-K_{1}-1}|v_{2k-K_{1}}]\) are the corresponding "eigenvectors" of complex eigenvalues \(a_{k}\pm b_{k}i\). Inspired by [43, Definition 2.3., Lemma 2.3.], we establish the following Lemma.

**Lemma 3.3**.: _Assuming \(A\in\mathbb{R}^{d\times d}\) has \(d\) distinct eigenvalues, with Jordan decomposition \(A=Q\Lambda Q^{-1}\). Let \(\gamma_{j}\in\mathbb{R}^{d}\) and \(\tilde{\gamma}_{j}:=Q^{-1}\gamma_{j}\in\mathbb{R}^{d}\) for all \(j=1,\ldots,n\) with \(n\geqslant 2\). We define_

\[w_{j,k}:=\left\{\begin{array}{ll}\tilde{\gamma}_{j,k}\in\mathbb{R}^{1},& \text{for}\;k=1,\ldots,K_{1}\,,\\ (\tilde{\gamma}_{j,2k-K_{1}-1},\tilde{\gamma}_{j,2k-K_{1}})^{\top}\in\mathbb{R }^{2},&\text{for}\;k=K_{1}+1,\ldots,K\,,\end{array}\right.\]

_where \(\tilde{\gamma}_{j,k}\) denotes the \(k\)-th entry of \(\tilde{\gamma}_{j}\). \(\text{rank}([\gamma_{1}|A\gamma_{1}|\ldots|A^{d-1}\gamma_{1}|\ldots|\gamma_{n }|A\gamma_{n}|\ldots|A^{d-1}\gamma_{n}])<d\) if and only if there exists \(k\in\{1,\ldots,K\}\), such that \(|w_{j,k}|=0\) for all \(j=1,\ldots,n\), where \(|w_{j,k}|\) is the absolute value of \(w_{j,k}\) for \(k=1,\ldots,K_{1}\), and the Euclidean norm of \(w_{j,k}\) for \(k=K_{1}+1,\ldots,K\)._

The proof of Lemma 3.3 can be found in Appendix A.4. From a geometric perspective, \(\gamma_{j}\) can be decomposed into a linear combination of \(Q_{k}\)'s

\[\gamma_{j}=Q\tilde{\gamma}_{j}=\sum_{k=1}^{K}Q_{k}w_{j,k}\,.\]

Let \(L_{k}:=\text{span}(Q_{k})\). According to [43, Theorem 2.2], each \(L_{k}\) is an \(A\)-invariant subspace of \(\mathbb{R}^{d}\). Recall that a space \(L\) is called \(A\)-invariant, if for all \(\gamma\in L\), \(A\gamma\in L\). We say \(L\) is a proper subspace of \(\mathbb{R}^{d}\) if \(L\subset\mathbb{R}^{d}\) and \(L\neq\mathbb{R}^{d}\). If \(|w_{j,k}|=0\) (i.e., \(w_{j,k}=0\) in \(\mathbb{R}^{1}\) or \(\mathbb{R}^{2}\)), then \(\gamma_{j}\) does not contain any information from \(L_{k}\). In this case, \(\gamma_{j}\) is contained in an \(A\)-invariant proper subspace of \(\mathbb{R}^{d}\) that excludes \(L_{k}\), denoted as \(L_{-k}\). It is worth emphasizing that \(L_{-k}\subset\mathbb{R}^{d}\) is indeed a **proper** subspace of \(\mathbb{R}^{d}\). This further implies that the trajectory of the ODE (13) generated from initial state \(\gamma_{j}\) is confined to \(L_{-k}\)[57, Lemma 3.2]. Lemma 3.3 indicates that if \(\text{rank}([\gamma_{1}|A\gamma_{1}|\ldots|A^{d-1}\gamma_{1}|\ldots|\gamma_{n }|A\gamma_{n}|\ldots|A^{d-1}\gamma_{n}])<d\) then all \(\gamma_{j}\) for \(j=1,\ldots,n\) are confined to an \(A\)-invariant proper subspace of \(\mathbb{R}^{d}\), denoted as \(L\). Therefore, all trajectories of the ODE (13) generated from initial states \(\gamma_{j}\) are also confined to \(L\). Furthermore, based on the identifiability conditions proposed in [57], the ODE (13) is not identifiable from observational data collected in these trajectories. This lemma provides an approach to interpreting our identifiability conditions from a geometric perspective.

Now we are ready to present our main theorem.

**Theorem 3.4**.: _Let \(x_{0}\in\mathbb{R}^{d}\) be fixed. Assuming that the matrix \(A\) in the SDE (1) has \(d\) distinct eigenvalues. The generator of the SDE (1) is identifiable from \(x_{0}\) if and only if_

\[\text{rank}([x_{0}|Ax_{0}|\ldots|A^{d-1}x_{0}|H_{\cdot 1}|AH_{\cdot 1}|\ldots|A^{d-1 }H_{\cdot 1}|\ldots|H_{\cdot d}|AH_{\cdot d}|\ldots|A^{d-1}H_{\cdot d}])=d\,,\] (14)

_where \(H:=GG^{T}\), and \(H_{\cdot j}\) stands for the \(j\)-th column vector of matrix \(H\), for all \(j=1,\cdots,d\)._

The proof of Theorem 3.4 can be found in Appendix A.5. The condition in Theorem 3.4 is both sufficient and necessary when the matrix \(A\) has distinct eigenvalues. It is worth noting that almost every \(A\in\mathbb{R}^{d\times d}\) has \(d\) distinct eigenvalues concerning the Lebesgue measure on \(\mathbb{R}^{d\times d}\). Hence, this condition is both sufficient and necessary for almost every \(A\) in \(\mathbb{R}^{d\times d}\). However, in cases where \(A\) has repetitive eigenvalues, this condition is solely sufficient and not necessary.

**Remark.** The identifiability condition stated in Theorem 3.4 is generic, that is, let

\[S:=\{(x_{0},A,G)\in\mathbb{R}^{d+d^{2}+dm}:\text{condition (\ref{eq:sde}) is violated}\}\,,\]

\(S\) has Lebesgue measure zero in \(\mathbb{R}^{d+d^{2}+dm}\). Refer to Appendix B.2 for the detailed proof.

From the geometric perspective, suppose matrix \(A\) has distinct eigenvalues, the generator of the SDE (1) is identifiable from \(x_{0}\) when not all of the vectors: \(x_{0},H_{.1},\ldots,H_{.d}\) are confined to an \(A\)-invariant **proper** subspace of \(\mathbb{R}^{d}\). A key finding is that when all the vectors \(H_{.j}\), \(j=1,\ldots,d\) are confined to an \(A\)-invariant proper subspace \(L\) of \(\mathbb{R}^{d}\), each column of the covariance matrix \(V(t)\) in Equation (12) is also confined to \(L\), for all \(0\leqslant t<\infty\). Thus, the identifiability of the generator of the SDE (1) can be fully determined by \(x_{0}\) and the system parameters \((A,GG^{\top})\). Further details can be found in the proof A.5 of Theorem 3.4.

By rearranging the matrix in (14), the identifiability condition can also be expressed as

\[\text{rank}([x_{0}|Ax_{0}|\ldots|A^{d-1}x_{0}|GG^{\top}|AGG^{\top}|\ldots|A^{ d-1}GG^{\top}])=d\,.\] (15)

Based on the identifiability condition (15), we derive the following corollary.

**Corollary 3.4.1**.: _Let \(x_{0}\in\mathbb{R}^{d}\) be fixed. If \(\text{rank}([G|AG|\ldots|A^{d-1}G])=d\), then the generator of the SDE (1) is identifiable from \(x_{0}\)._

The proof of Corollary 3.4.1 can be found in Appendix A.6. This corollary indicates that the generator of the SDE (1) is identifiable from **any** initial state \(x_{0}\in\mathbb{R}^{d}\) when the pair \([A,G]\) is controllable (\(\text{rank}([G|AG|\ldots|A^{d-1}G])=d\)). Notably, this identifiability condition is stricter than that proposed in Theorem 3.4, as it does not use the information of \(x_{0}\).

### Conditions for identifying generators of linear SDEs with multiplicative noise

Expressing the SDE (2) in the form given by (5) yields \(b(x)=Ax\) and \(\sigma(x)=[G_{1}x|\ldots|G_{m}x]\in\mathbb{R}^{d\times m}\), thus, \(c(x)=\sigma(x)\sigma(x)^{\top}=\sum_{k=1}^{m}G_{k}xx^{\top}G_{k}^{\top}\). Let \(X(t;x_{0},A,\{G_{k}\}_{k=1}^{m})\) denote the solution to the SDE (2), then based on Proposition 3.1, we define the identifiability of the generator of the SDE (2) as follows.

**Definition 3.2** (\((x_{0},A,\{G_{k}\}_{k=1}^{m})\)-identifiability).: _For \(x_{0}\in\mathbb{R}^{d},A,G_{k}\in\mathbb{R}^{d\times d}\) for all \(k=1,\ldots,m\), the generator of the SDE (2) is said to be identifiable from \(x_{0}\), if for all \(\tilde{A},\tilde{G}_{k}\in\mathbb{R}^{d\times d}\), there exists an \(x\in\mathbb{R}^{d}\), such that \((A,\sum_{k=1}^{m}G_{k}xx^{\top}G_{k}^{\top})\neq(\tilde{A},\sum_{k=1}^{m} \tilde{G}_{k}xx^{\top}\tilde{G}_{k}^{\top})\), it holds that \(X(\cdot;x_{0},A,\{G_{k}\}_{k=1}^{m})\overset{\mathsf{d}}{=}X(\cdot;x_{0}, \tilde{A},\{\tilde{G}_{k}\}_{k=1}^{m})\)._

Based on Definition 3.2, we present the identifiability condition for the generator of the SDE (2).

**Theorem 3.5**.: _Let \(x_{0}\in\mathbb{R}^{d}\) be fixed. The generator of the SDE (2) is identifiable from \(x_{0}\) if the following conditions are satisfied:_

* \(\text{rank}([x_{0}|Ax_{0}|\ldots|A^{d-1}x_{0}])=d\)_,_
* \(\text{rank}([v|\mathcal{A}v|\ldots|\mathcal{A}^{(d^{2}+d-2)/2}v])=(d^{2}+d)/2\)_,_

_where \(\mathcal{A}=A\oplus A+\sum_{k=1}^{m}G_{k}\otimes G_{k}\in\mathbb{R}^{d^{2} \times d^{2}}\), \(\oplus\) denotes Kronecker sum and \(\otimes\) denotes Kronecker product, \(v\) is a \(d^{2}\)-dimensional vector defined by \(v:=\text{vec}(x_{0}x_{0}^{\top})\), where \(\text{vec}(M)\) denotes the vectorization of matrix \(M\)._

The proof of Theorem 3.5 can be found in Appendix A.7. This condition is only sufficient but not necessary. Specifically, condition A1 guarantees that matrix \(A\) is identifiable, and once \(A\) is identifiable, condition A2 ensures that the identifiability of \(\sum_{k=1}^{m}G_{k}xx^{\top}G_{k}^{\top}\) holds for all \(x\in\mathbb{R}^{d}\).

**Remark.** The identifiability condition stated in Theorem 3.5 is generic, that is, let

\[S:=\{(x_{0},A,\{G_{k}\}_{k=1}^{m})\in\mathbb{R}^{d+(m+1)d^{2}}:\text{ either condition A1 or A2 in Theorem 3.5 is violated}\}\,,\]

\(S\) has Lebesgue measure zero in \(\mathbb{R}^{d+(m+1)d^{2}}\). This signifies that the conditions are satisfied for most of the combinations of \(x_{0}\), \(A\) and \(G_{k}\)'s, except for those that lie in a set of Lebesgue measure zero. The corresponding proposition and detailed proof can be found in Appendix B.1.

Since obtaining an explicit solution for the SDE (2) is generally infeasible, we resort to utilizing the first- and second-order moments of this SDE to derive the identifiability conditions. Let \(m(t):=\mathbb{E}[X_{t}]\) and \(P(t):=\mathbb{E}[X_{t}X_{t}^{\top}]\), it is known that these moments satisfy ODE systems. Specifically, \(m(t)\) satisfies the ODE (13), while \(P(t)\) satisfies the following ODE (cf. [56]):

\[\dot{P}(t)=AP(t)+P(t)A^{\top}+\sum_{k=1}^{m}G_{k}P(t)G_{k}^{\top}\,,\;\;\;P(0)= x_{0}x_{0}^{\top}\,.\] (16)

An important trick to deal with the ODE (16) is to vectorize \(P(t)\), then it can be expressed as:

\[\text{vec}(\dot{P}(t))=\mathcal{A}\text{vec}(P(t))\,,\;\;\;\text{vec}(P(0))=v\,,\] (17)

where \(\mathcal{A}\) and \(v\) are defined in Theorem 3.5. In fact, the ODE (17) follows the same mathematical structure as that of the ODE (13), which is known as homogeneous linear ODEs. Thus, in addition to the inherent properties of the SDE (2), we also employ some existing identifiability theories for homogeneous linear ODEs to establish the identifiability condition for the generator of the SDE (2).

From the geometric perspective, condition A1 indicates that the initial state \(x_{0}\) is not confined to an \(A\)-invariant **proper** subspace of \(\mathbb{R}^{d}\)[57, Lemma 3.1.]. And condition A2 implies that the vectorization of \(x_{0}x_{0}^{\top}\) is not confined to an \(\mathcal{A}\)-invariant **proper** subspace of \(W\), with \(W\subset\mathbb{R}^{d^{2}}\), and \(\text{dim}(W)=(d^{2}+d)/2\), where \(\text{dim}(W)\) denotes the dimension of the subspace \(W\), that is the number of vectors in any basis for \(W\). In particular, one can construct a basis for \(W\) as follows:

\[\left\{\text{vec}(E_{11}),\text{vec}(E_{21}),\text{vec}(E_{22}),\ldots,\text{ vec}(E_{dd})\right\},\]

where \(E_{ij}\) denotes a \(d\times d\) matrix whose \(ij\)-th and \(ji\)-th elements are \(1\), and all other elements are \(0\), for all \(i,j=1,\ldots,d\) and \(i\geqslant j\). Refer to the proof A.7 of Theorem 3.5 for more details.

## 4 Simulations and examples

In order to assess the validity of the identifiability conditions established in Section 3, we present the results of simulations. Specifically, we consider SDEs with system parameters that either satisfy or violate the proposed identifiability conditions. We then apply the maximum likelihood estimation (MLE) method to estimate the system parameters from discrete observations sampled from the corresponding SDE. The accuracy of the resulting parameter estimates serves as an indicator of the validity of the proposed identifiability conditions.

**Simulations.** We conduct five sets of simulations, which include one identifiable case and one unidentifiable case for the SDE (1), and one identifiable case and two unidentifiable cases with either condition A1 or A2 in Theorem 3.5 unsatisfied for the SDE (2). We set both the system dimension, \(d\), and the Brownian motion dimension, \(m\), to 2. Details on the true underlying system parameters for the SDEs can be found in Appendix C. We simulate observations from the true SDEs for each of the five cases under investigation. Specifically, the simulations are carried out for different numbers of trajectories (\(N\)), with 50 equally-spaced observations sampled on each trajectory from the time interval \([0,1]\). We employ the Euler-Maruyama (EM) method [34], a widely used numerical scheme for simulating SDEs, to generate the observations.

**Estimation.** We use MLE [38; 50] to estimate the system parameters. The MLE method requires knowledge of the transition probability density function (pdf) that governs the evolution of the system. For the specific case of the SDE (1), the transition density follows a Gaussian distribution, which can be computed analytically based on the system's drift and diffusion coefficients (cf. [50]). To compute the covariance, we employ the commonly used matrix fraction decomposition method [4; 49; 50]. However, in general, the transition pdf of the SDE (2) cannot be obtained analytically due to the lack of a closed-form solution. To address this issue, we implement the Euler-Maruyama approach [32; 34], which has been shown to be effective in approximating the transition pdf of SDEs.

**Metric.** We adopt the commonly used metric, mean squared error (MSE), to assess the accuracy of the parameter estimates. To ensure reliable estimation outcomes, we perform 100 independent random replications for each configuration and report the mean and variance of their MSEs.

**Results analysis.** Table 1 and Table 2 present the simulation results for the SDE (1) and the SDE (2), respectively. In Table 1, the simulation results demonstrate that in the identifiable case, as the number of trajectories \(N\) increases, the MSE for both \(A\) and \(GG^{\top}\) decreases and approaches zero. However, in the unidentifiable case, where the identifiable condition (14) stated in Theorem 3.4 is not satisfied, the MSE for both \(A\) and \(GG^{\top}\) remains high regardless of the number of trajectories. These findings provide strong empirical evidence supporting the validity of the identifiability condition proposed in Theorem 3.4. The simulation results presented in Table 2 show that in the identifiable case, the MSE for both \(A\) and \(Gsx\) decreases and approaches zero with the increase of the number of trajectories \(N\). Here, \(Gsx:=\sum_{k=1}^{m}G_{k}xx^{\top}G_{k}^{\top}\), where \(x\) is a randomly generated vector from \(\mathbb{R}^{2}\) (in these simulations, \(x=[1.33,0.72]^{\top}\)). Interestingly, even in unidentifiable case 1, the MSE for both \(A\) and \(Gsx\) decreases with an increasing number of trajectories \(N\), indicating that the generator of the SDE utilized in this particular case is still identifiable, although a larger number of trajectories is required compared to the identifiable case to achieve the same level of accuracy. This result is reasonable, because it aligns with our understanding that condition A1 is only sufficient but not necessary for identifying \(A\), as the lack of an explicit solution for the SDE (2) results in condition A1 not incorporating any information from \(G_{k}\)'s. The identifiability condition derived for the SDE (1) in Theorem 3.4 leverages the information of \(G\), similarly, if information regarding \(G_{k}\)'s is available, a weaker condition for identifying \(A\) could be obtained. For illustration, in Appendix E, we present such a condition assuming the SDE (2) has a closed-form solution. In the case of unidentifiable case 2, the MSE for \(A\) decreases with an increasing number of trajectories \(N\); however, the MSE for \(Gsx\) remains high, indicating that \(A\) is identifiable, while \(Gsx\) is not, albeit requiring more trajectories compared to the identifiable case to achieve the same level of accuracy of \(A\) (since the \(Gsx\) is far away from its true underlying value). This finding is consistent with the derived identifiability condition, as condition A1 is sufficient to identify \(A\), whereas condition A2 governs the identifiability of \(Gsx\). Worth noting that in cases where neither condition A1 nor condition A2 is satisfied, the estimated parameters barely deviate from their initial values, implying poor estimation of both \(A\) and \(Gsx\). These results indicate the validity of the identifiability condition stated in Theorem 3.5.

**Illustrative instances of causal inference for linear SDEs (with interventions).** To illustrate how our proposed identifiability conditions can guarantee reliable causal inference for linear SDEs, we present examples corresponding to both the SDE (1) and the SDE (2). In these examples, we show that under our proposed identifiability conditions, the post-intervention distributions are identifiable from their corresponding observational distributions. Please refer to Appendix D.1 and D.2 for the details of the examples.

## 5 Related work

Most current studies on the identifiability analysis of SDEs are based on the Gaussian diffusion processes that conform to the form described in the SDE (1). In particular, the authors of [27; 28; 42]

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{\(\bm{N}\)} & \multicolumn{2}{c}{**Identifiable**} & \multicolumn{2}{c}{**Unidentifiable**} \\ \cline{2-6}  & MSE-\(A\) & MSE-\(Gsx\) & MSE-\(AG\) & MSE-\(A\) & MSE-\(GG^{\top}\) \\ \hline
5 & \(0.0117\pm 0.0115\) & \(5.28\)E-\(05\pm 4.39\)E-\(05\) & \(3.66\pm 0.10\) & \(0.05\pm 0.03\) \\
10 & \(0.0063\pm 0.0061\) & \(2.39\)E-\(05\pm 1.82\)E-\(05\) & \(3.88\pm 0.06\) & \(0.64\pm 0.59\) \\
20 & \(0.0029\pm 0.0027\) & \(1.87\)E-\(05\pm 1.51\)E-\(05\) & \(3.70\pm 0.06\) & \(0.09\pm 0.07\) \\
50 & \(0.0013\pm 0.0010\) & \(8.00\)E-\(06\pm 5.68\)E-\(06\) & \(3.76\pm 0.07\) & \(0.11\pm 0.08\) \\
100 & \(0.0007\pm 0.0004\) & \(4.34\)E-\(06\pm 2.70\)E-\(06\) & \(3.66\pm 0.02\) & \(2.09\pm 1.98\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Simulation results of the SDE (1)

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{\(\bm{N}\)} & \multicolumn{2}{c}{**Identifiable**} & \multicolumn{2}{c}{**Unidentifiable**} \\ \cline{2-7}  & \multicolumn{2}{c}{**case1: A1-False, A2-True**} & \multicolumn{2}{c}{**case2: A1-True, A2-False**} \\ \cline{2-7}  & MSE-\(A\) & MSE-\(Gsx\) & MSE-\(A\) & MSE-\(Gsx\) & MSE-\(A\) & MSE-\(Gsx\) \\ \hline
10 & \(0.069\pm 0.061\) & \(0.3647\pm 0.3579\) & \(0.509\pm 0.499\) & \(0.194\pm 0.140\) & \(2.562\pm 2.522\) & \(9763\pm 8077\) \\
20 & \(0.047\pm 0.045\) & \(0.1769\pm 0.1694\) & \(0.195\pm 0.180\) & \(0.088\pm 0.058\) & \(0.967\pm 0.904\) & \(8353\pm 6839\) \\
50 & \(0.018\pm 0.018\) & \(0.1703\pm 0.1621\) & \(0.132\pm 0.131\) & \(0.081\pm 0.045\) & \(0.423\pm 0.410\) & \(4779\pm 4032\) \\
100 & \(0.006\pm 0.006\) & \(0.0015\pm 0.0012\) & \(0.065\pm 0.065\) & \(0.068\pm 0.036\) & \(0.207\pm 0.198\) & \(3569\pm 3150\) \\
500 & \(0.001\pm 0.001\) & \(0.0004\pm 0.0001\) & \(0.008\pm 0.008\) & \(0.059\pm 0.004\) & \(0.046\pm 0.046\) & \(4490\pm 3991\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Simulation results of the SDE (2)have conducted research on the identifiability or asymptotic properties of parameter estimators of Gaussian diffusions in view of continuous observations of one trajectory, and have highlighted the need for the diffusion to be ergodic. A considerable amount of effort has also been directed towards the identifiability analysis of Gaussian diffusions, relying on the exact discrete models of the SDEs [6; 16; 23; 35; 41]. Typically, these studies involve transferring the continuous-time system described in the SDE (1) to a discrete-time model such as a vector autoregressive model, based on equally-spaced observations sampled from one trajectory, and then attempting to determine conditions under which \((A,GG^{\top})\) is identifiable from the parameters of the corresponding exact discrete models. These conditions often have requirements on eigenvalues of \(A\) among other conditions, such as requiring the eigenvalues to have only negative real parts, or the eigenvalues to be strictly real. Due to the limitation of the available observations (continuous or discrete observations located on one trajectory of the SDE system), the identifiability conditions proposed in these works are restrictive.

Causal modelling theories have been well-developed based on directed acyclic graphs (DAGs), which do not explicitly incorporate a time component [39]. In recent years, similar concepts of causality have been developed for dynamic systems operating in both discrete and continuous time. Discrete-time models, such as autoregressive processes, can be readily accommodated within the DAG-based framework [13; 14]. On the other hand, differential equations offer a natural framework for understanding causality in dynamic systems within the context of continuous-time processes [1; 52]. Consequently, considerable effort has been devoted to establishing a theoretical connection between causality and differential equations. In the deterministic case, Mooij et al. [37] and Rubenstein et al. [46] have established a mathematical link between ODEs and structural causal models (SCMs). Wang et al. [60] have proposed a method to infer the causal structure of linear ODEs. Turning to the stochastic case, Boogers and Mooij have built a bridge from random differential equations (RDEs) to SCMs [7], while Hansen and Sokol have proposed a causal interpretation of SDEs by establishing a connection between SDEs and SEMs [17].

## 6 Conclusion and discussion

In this paper, we present an investigation into the identifiability of the generators of linear SDEs under additive and multiplicative noise. Specifically, we derive the conditions that are fully built on system parameters and the initial state \(x_{0}\), which enables the identification of a linear SDE's generator from the distribution of its solution process with a given fixed initial state. We establish that, under the proposed conditions, the post-intervention distribution is identifiable from the corresponding observational distribution for any Lipschitz intervention \(\zeta\).

The main limitation of our work is that the practical verification of these identifiability conditions poses a challenge, as the true underlying system parameters are typically unavailable in real-world applications. Nevertheless, our study contributes to the understanding of the intrinsic structure of linear SDEs. By offering valuable insights into the identifiability aspects, our findings empower researchers and practitioners to employ models that satisfy the proposed conditions (e.g., through constrained parameter estimation) to learn real-world data while ensuring identifiability. We believe the paramount significance of this work lies in providing a systematic and rigorous causal interpretation of linear SDEs, which facilitates reliable causal inference for dynamic systems governed by such equations. It is worth noting that in our simulations, we employed the MLE method to estimate the system parameters. This necessitates the calculation of the transition pdf from one state to the successive state at each discrete temporal increment. Consequently, as the state dimension and Brownian motion dimension increase, the computational time is inevitably significantly increased, rendering the process quite time-consuming. To expedite parameter estimation for scenarios involving high dimensions, alternative estimation approaches are required. The development of a more efficient parameter estimation approach remains an important task in the realm of SDEs, representing a promising direction for our future research. We claim that this work does not present any foreseeable negative social impact.

## Acknowledgements

YW was supported by the Australian Government Research Training Program (RTP) Scholarship from the University of Melbourne. XG was supported by ARC DE210101352. MG was supported by ARC DE210101624.

## References

* [1] O. O. Aalen, K. Roysland, J. M. Gran, and B. Ledergerber. Causality, mediation and time: a dynamic viewpoint. _Journal of the Royal Statistical Society: Series A (Statistics in Society)_, 175(4):831-861, 2012.
* [2] L. J. Allen. _An introduction to stochastic processes with applications to biology_. CRC press, 2010.
* [3] I. P. Arribas, C. Salvi, and L. Szpruch. Sig-sdes model for quantitative finance. In _Proceedings of the First ACM International Conference on AI in Finance_, pages 1-8, 2020.
* [4] P. Axelsson and F. Gustafsson. Discrete-time solutions to the continuous-time differential lyapunov equation with applications to kalman filtering. _IEEE Transactions on Automatic Control_, 60(3):632-643, 2014.
* [5] A. Bellot, K. Branson, and M. van der Schaar. Neural graphical modelling in continuous-time: consistency guarantees and algorithms. In _International Conference on Learning Representations_, 2021.
* [6] J. R. Blevins. Identifying restrictions for finite parameter continuous time models with discrete time data. _Econometric Theory_, 33(3):739-754, 2017.
* [7] S. Bongers and J. M. Mooij. From random differential equations to structural causal models: The stochastic case. _arXiv preprint arXiv:1803.08784_, 2018.
* [8] C. A. Braumann. _Introduction to stochastic differential equations with applications to modelling in biology and finance_. John Wiley & Sons, 2019.
* [9] A. P. Browning, D. J. Warne, K. Burrage, R. E. Baker, and M. J. Simpson. Identifiability analysis for stochastic differential equation models in systems biology. _Journal of the Royal Society Interface_, 17(173):20200652, 2020.
* [10] M. Capinski and E. Kopp. _The Black-Scholes Model_. Cambridge University Press, 2012.
* [11] G. C. Chow. Optimum control of stochastic differential equation systems. _Journal of Economic Dynamics and Control_, 1(2):143-175, 1979.
* [12] J. Collins, D. Howard, and J. Leitner. Quantifying the reality gap in robotic manipulation tasks. In _2019 International Conference on Robotics and Automation (ICRA)_, pages 6706-6712. IEEE, 2019.
* [13] M. Eichler. Granger causality and path diagrams for multivariate time series. _Journal of Econometrics_, 137(2):334-353, 2007.
* [14] M. Eichler and V. Didelez. On granger causality and the effect of interventions in time series. _Lifetime data analysis_, 16:3-32, 2010.
* [15] A. L. Goldberger, L. A. Amaral, L. Glass, J. M. Hausdorff, P. C. Ivanov, R. G. Mark, J. E. Mietus, G. B. Moody, C.-K. Peng, and H. E. Stanley. Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals. _circulation_, 101(23):e215-e220, 2000.
* [16] L. P. Hansen and T. J. Sargent. The dimensionality of the aliasing problem in models with rational spectral densities. _Econometrica: Journal of the Econometric Society_, pages 377-387, 1983.
* [17] N. Hansen and A. Sokol. Causal interpretation of stochastic differential equations. _Electronic Journal of Probability_, 19:1-24, 2014.
* [18] D. Henderson and P. Plaschko. _Stochastic Differential Equations In Science And Engineering (With Cd-rom)_. World Scientific, 2006.
* [19] S. M. Iacus et al. _Simulation and inference for stochastic differential equations: with R examples_. Springer, 2008.

* [20] M. Jacobsen. _Homgeneous Gaussian Diffusions in Finite Dimensions_. Preprint, Institute of Mathematical Statistics University of Copenhagen, 1991.
* [21] J. Jia and A. R. Benson. Neural jump stochastic differential equations. _Advances in Neural Information Processing Systems_, 32, 2019.
* [22] I. Karatzas, S. Shreve, and S. E. Shreve. _Brownian motion and stochastic calculus_. Springer Science & Business Media, 1991.
* [23] M. Kessler and A. Rahbek. Identification and inference for multivariate cointegrated and ergodic gaussian diffusions. _Statistical inference for stochastic processes_, 7:137-151, 2004.
* [24] P. Klein. Pricing black-scholes options with correlated credit risk. _Journal of Banking & Finance_, 20(7):1211-1229, 1996.
* [25] P. E. Kloeden and E. Platen. _Numerical Solution of Stochastic Differential Equations_. Springer Berlin, Heidelberg, 1992.
* [26] P. Kugler. Moment fitting for parameter inference in repeatedly and partially observed stochastic biological models. _PLOS ONE_, 7:1-15, 2012.
* [27] Y. A. Kutoyants. _Statistical inference for ergodic diffusion processes_. Springer Science & Business Media, 2004.
* [28] A. Le-Breton and M. Musiela. Some parameter estimation problems for hypoelliptic homogeneous gaussian diffusions. _Banach Center Publications_, 1(16):337-356, 1985.
* [29] R. Levien and S. Tan. Double pendulum: An experiment in chaos. _American Journal of Physics_, 61(11):1038-1044, 1993.
* [30] L. Lima and L. Miranda. Price dynamics of the financial markets using the stochastic differential equation for a potential double well. _Physica A: Statistical Mechanics and its Applications_, 490:828-833, 2018.
* [31] L. Lu, P. Jin, G. Pang, Z. Zhang, and G. E. Karniadakis. Learning nonlinear operators via deep-onset based on the universal approximation theorem of operators. _Nature machine intelligence_, 3(3):218-229, 2021.
* [32] X. Mao. The truncated euler-maruyama method for stochastic differential equations. _Journal of Computational and Applied Mathematics_, 290:370-384, 2015.
* [33] M. C. Mariani and O. K. Twenteboah. Stochastic differential equations applied to the study of geophysical and financial time series. _Physica A: Statistical Mechanics and its Applications_, 443:170-178, 2016.
* [34] G. Maruyama. Continuous markov processes and stochastic equations. _Rendiconti del Circolo Matematico di Palermo_, 4:48-90, 1955.
* [35] J. R. Mccrorie. The problem of aliasing in identifying finite parameter continuous time stochastic models. _Acta Applicandue Mathematicae_, 79(1-2):9, 2003.
* [36] S. W. Mogensen, D. Malinsky, and N. R. Hansen. Causal learning for partially observed stochastic dynamical systems. In _UAI_, pages 350-360, 2018.
* [37] J. M. Mooij, D. Janzing, and B. Scholkopf. From ordinary differential equations to structural causal models: the deterministic case. In _Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence_, pages 440-448, 2013.
* [38] J. N. Nielsen, H. Madsen, and P. C. Young. Parameter estimation in stochastic differential equations: an overview. _Annual Reviews in Control_, 24:83-94, 2000.
* [39] J. Pearl. _Causality_. Cambridge university press, 2009.
* [40] P. Phillips. The structural estimation of a stochastic differential equation system. _Econometrica: Journal of the Econometric Society_, pages 1021-1041, 1972.

* [41] P. C. Phillips. The problem of identification in finite parameter continuous time models. _Journal of Econometrics_, 1(4):351-362, 1973.
* [42] A. Prior, M. Kleptsyna, and P. Milheiro-Oliveira. On maximum likelihood estimation of the drift matrix of a degenerated o-u process. _Statistical Inference for Stochastic Processes_, 20:57-78, 2017.
* [43] X. Qiu, T. Xu, B. Soltanalizadeh, and H. Wu. Identifiability analysis of linear ordinary differential equation systems with a single trajectory. _Applied Mathematics and Computation_, 430:127260, 2022.
* [44] S. Rong. _Theory of stochastic differential equations with jumps and applications: mathematical and analytical techniques with applications to engineering_. Springer Science & Business Media, 2006.
* [45] Y. Rubanova, R. T. Chen, and D. K. Duvenaud. Latent ordinary differential equations for irregularly-sampled time series. _Advances in neural information processing systems_, 32, 2019.
* [46] P. Rubenstein, S. Bongers, B. Scholkopf, and J. Mooij. From deterministic ODEs to dynamic structural causal models. In _34th Conference on Uncertainty in Artificial Intelligence (UAI 2018)_, pages 114-123. Curran Associates, Inc., 2018.
* [47] P. Sanchez and S. A. Tsaftaris. Diffusion causal models for counterfactual estimation. In _Conference on Causal Learning and Reasoning_, pages 647-668. PMLR, 2022.
* [48] K. Sankaran, R. Perez, and K. Jata. Effects of pitting corrosion on the fatigue behavior of aluminum alloy 7075-t6: modeling and experimental studies. _Materials Science and Engineering: A_, 297(1-2):223-229, 2001.
* [49] S. Sarkka et al. _Recursive Bayesian inference on stochastic differential equations_. Helsinki University of Technology, 2006.
* [50] S. Sarkka and A. Solin. _Applied stochastic differential equations_. Cambridge University Press, 2019.
* [51] K.-i. Sato. Stochastic integrals with respect to levy processes and infinitely divisible distributions. _Sugaku Expositions_, 27(1):19-42, 2014.
* [52] B. Scholkopf, F. Locatello, S. Bauer, N. R. Ke, N. Kalchbrenner, A. Goyal, and Y. Bengio. Toward causal representation learning. _Proceedings of the IEEE_, 109(5):612-634, 2021.
* [53] Z. Schuss. Singular perturbation methods in stochastic differential equations of mathematical physics. _SIAM Review_, 22(2):119-155, 1980.
* [54] I. Silva, G. Moody, D. J. Scott, L. A. Celi, and R. G. Mark. Predicting in-hospital mortality of icu patients: The physionet/computing in cardiology challenge 2012. In _2012 Computing in Cardiology_, pages 245-248. IEEE, 2012.
* [55] K. Sobczyk. _Stochastic differential equations: with applications to physics and engineering_. Springer Science & Business Media, 2001.
* [56] L. Socha. _Linearization methods for stochastic dynamic systems_. Springer Science & Business Media, 2007.
* [57] S. Stanhope, J. E. Rubin, and D. Swigon. Identifiability of linear and linear-in-parameters dynamical systems from a single trajectory. _SIAM Journal on Applied Dynamical Systems_, 13(4):1792-1815, 2014.
* [58] N. G. Van Kampen. Stochastic differential equations. _Physics reports_, 24(3):171-228, 1976.
* [59] P. Vatiwutipong and N. Phewchean. Alternative way to derive the distribution of the multivariate ornstein-uhlenbeck process. _Advances in Difference Equations_, 2019(1):1-7, 2019.

* [60] Y. Wang, W. Huang, M. Gong, X. Geng, T. Liu, K. Zhang, and D. Tao. Identifiability and asymptotics in learning homogeneous linear ODE systems from discrete observations. _arXiv preprint arXiv:2210.05955_, 2022.
* [61] D. J. Wilkinson. Stochastic modelling for quantitative description of heterogeneous biological systems. _Nature Reviews Genetics_, 10(2):122-133, 2009.
* [62] M. Zakai and J. Snyders. Stationary probability measures for linear differential equations driven by white noise. _Journal of Differential Equations_, 8(1):27-33, 1970.

## Appendix A Detailed proofs

### Proof of Lemma 3.1

Proof.: We start by presenting the mathematical definition of a Levy process. (cf. [51])

**Definition A.1**.: _A stochastic process \(X:=\{X_{t}:0\leqslant t<\infty\}\) is said to be a Levy process if it satisfies the following properties:_

1. \(X_{0}=0\) _almost surely;_
2. _Independence of increments: For any_ \(0\leqslant t_{1}<t_{2}<\ldots<t_{n}<\infty\)_,_ \(X_{t_{2}}-X_{t_{1}}\)_,_ \(X_{t_{3}}-X_{t_{2}}\)_,_ \(\ldots\)_,_ \(X_{t_{n}}-X_{t_{n-1}}\) _are independent;_
3. _Stationary increments: For any_ \(s<t\)_,_ \(X_{t}-X_{s}\) _is equal in distribution to_ \(X_{t-s}\)_;_
4. _Continuity in probability: For any_ \(\varepsilon>0\) _and_ \(0\leqslant t<\infty\) _it holds that_ \(\lim_{h\to 0}\mathbb{P}(|X_{t+h}-X_{t}|>\varepsilon)=0\)_._

In the following, we first show that the SDE (1) can be expressed as the form of (10), with \(Z\) being a \(p\)-dimensional Levy process and \(a:\mathbb{R}^{d}\to\mathbb{R}^{d\times p}\) being Lipschitz. The first equation in the SDE (1) can be rearranged as

\[dX_{t} =AX_{t}dt+GdW_{t}\] \[=[AX_{t}\quad G]\begin{bmatrix}dt\\ dW_{t}\end{bmatrix}\] (18) \[=a(X_{t})dZ_{t}\,,\]

with

\[a(X_{t})=[AX_{t}\quad G]\in\mathbb{R}^{d\times(m+1)}\,,\]

and

\[dZ_{t}=\begin{bmatrix}dt\\ dW_{t}\end{bmatrix}=\underbrace{\begin{bmatrix}1\\ 0_{m\times 1}\end{bmatrix}}_{r}dt+\underbrace{\begin{bmatrix}0_{1\times 1}&0_{1 \times m}\\ 0_{m\times 1}&I_{m\times m}\end{bmatrix}}_{E}\begin{bmatrix}dW_{0,t}\\ dW_{t}\end{bmatrix}\,,\] (19)

where \(0_{i\times j}\) denotes an \(i\times j\) zero matrix, let \(\tilde{W}:=\{\tilde{W}_{t}:0\leqslant t<\infty\}\) with \(\tilde{W}_{t}=[W_{0,t},W_{1,t},\ldots,W_{m,t}]^{\top}\) denote a \((m+1)\)-dimensional standard Brownian motion, then one can find a process \(Z:=\{Z_{t}:0\leqslant t<\infty\}\) with

\[Z_{t} =rt+E\tilde{W}_{t}\,,\] (20) \[Z_{0} =0\,,\]

satisfying \(dZ_{t}\) described in Equation (19). Then we will show that the process \(Z\) described in (20) is a Levy process, that is, it satisfies the four properties stated in Definition A.1.

**Property 1:** The first property is readily checked since \(Z_{0}=0\).

**Property 2:** For any \(0\leqslant t_{1}<t_{2}<t_{3}<\infty\),

\[Z_{t_{2}}-Z_{t_{1}} =r(t_{2}-t_{1})+E(\tilde{W}_{t_{2}}-\tilde{W}_{t_{1}})\] \[=\begin{bmatrix}t_{2}-t_{1}\\ 0_{m\times 1}\end{bmatrix}+\begin{bmatrix}0\\ W_{t_{2}}-W_{t_{1}}\end{bmatrix}\,.\]Similarly,

\[Z_{t_{3}}-Z_{t_{2}}=\begin{bmatrix}t_{3}-t_{2}\\ 0_{m\times 1}\end{bmatrix}+\begin{bmatrix}0\\ W_{t_{3}}-W_{t_{2}}\end{bmatrix}\,.\]

Since \(W_{t_{2}}-W_{t_{1}}\) and \(W_{t_{3}}-W_{t_{2}}\) are independent, \(Z_{t_{3}}-Z_{t_{2}}\) and \(Z_{t_{2}}-Z_{t_{1}}\) are independent.

**Property 3:** when \(s<t\),

\[Z_{t}-Z_{s} =\begin{bmatrix}t-s\\ 0_{m\times 1}\end{bmatrix}+\begin{bmatrix}0\\ W_{t}-W_{s}\end{bmatrix}\] \[\sim\mathcal{N}\Bigg{(}\begin{bmatrix}t-s\\ 0_{m\times 1}\end{bmatrix},\begin{bmatrix}0_{1\times 1}&0_{1\times m}\\ 0_{m\times 1}&(t-s)I_{m\times m}\end{bmatrix}\Bigg{)}\,.\]

And

\[Z_{t-s} =r(t-s)+E\tilde{W}_{t-s}\] \[=\begin{bmatrix}t-s\\ 0_{m\times 1}\end{bmatrix}+\begin{bmatrix}0\\ W_{t-s}\end{bmatrix}\] \[\sim\mathcal{N}\Bigg{(}\begin{bmatrix}t-s\\ 0_{m\times 1}\end{bmatrix},\begin{bmatrix}0_{1\times 1}&0_{1\times m}\\ 0_{m\times 1}&(t-s)I_{m\times m}\end{bmatrix}\Bigg{)}\,.\]

Therefore, property 3 is checked.

**Property 4:** Obviously, process \(Z\) described in (20) is continuous with probability one at \(t\) for all \(0\leqslant t<\infty\), therefore, \(Z\) has continuity in probability.

Now that we have shown that process \(Z\) is a \(p\)-dimensional Levy process with \(p=m+1\). Then we will show that \(a(X_{t})=[AX_{t}\quad G]\) is Lipschitz.

\[\parallel a(X_{t})-a(X_{s})\parallel_{F}= \parallel[A(X_{t}-X_{s})\quad 0]\parallel_{F}\] \[= \parallel A(X_{t}-X_{s})\parallel_{2}\] \[\leqslant \parallel A\parallel_{F}\parallel X_{t}-X_{s}\parallel_{2}\]

where \(\parallel M\parallel_{F}\) denotes the Frobenius norm of matrix \(M\) and \(\parallel v\parallel_{2}\) denotes the Euclidean norm of vector \(v\). Now it is readily checked that function \(a:\mathbb{R}^{d}\to\mathbb{R}^{d\times p}\) is Lipschitz.

Similarly, we will show that the SDE (2) can also be expressed as the form of (10), with \(Z\) being a \(p\)-dimensional Levy process and \(a:\mathbb{R}^{d}\to\mathbb{R}^{d\times p}\) being Lipschitz. Let us rearrange the first equation in the SDE (2):

\[dX_{t} =AX_{t}dt+\sum_{k=1}^{m}G_{k}X_{t}dW_{k,t}\] \[=[AX_{t}\quad G_{1}X_{t}\quad\ldots\quad G_{m}X_{t}]\begin{bmatrix} dt\\ dW_{1,t}\\ \vdots\\ dW_{m,t}\end{bmatrix}\] \[=a(X_{t})dZ_{t}\,.\]

Since the \(dZ_{t}\) here has the same form as that of the SDE (1), we use the same process \(Z\) described in Equation (20), which has been shown to be a Levy process.

As for the function \(a(X_{t})\),

\[\parallel a(X_{t})-a(X_{s})\parallel_{F}= \parallel[A(X_{t}-X_{s})\quad G_{1}(X_{t}-X_{s})\quad\ldots\quad G _{m}(X_{t}-X_{s})]\parallel_{F}\] \[\leqslant \parallel A(X_{t}-X_{s})\parallel_{2}+\sum_{k=1}^{m}\parallel G_{ k}(X_{t}-X_{s})\parallel_{2}\] \[\leqslant \parallel A\parallel_{F}\parallel X_{t}-X_{s}\parallel_{2}+\sum_ {k=1}^{m}\parallel G_{k}\parallel_{F}\parallel X_{t}-X_{s}\parallel_{2}\] \[=\Bigg{(}\parallel A\parallel_{F}+\sum_{k=1}^{m}\parallel G_{k} \parallel_{F}\Bigg{)}\parallel X_{t}-X_{s}\parallel_{2},\]it is readily checked that function \(a:\mathbb{R}^{d}\to\mathbb{R}^{d\times p}\) is Lipschitz. 

### Proof of Proposition 3.1

Proof.: For the backward direction, when \(b(x)=\tilde{b}(x)\) and \(c(x)=\tilde{c}(x)\) for all \(x\in\mathbb{R}^{d}\), it is obviously that \((\mathcal{L}f)(x)=(\tilde{\mathcal{L}}f)(x)\) for all \(f\in C_{b}^{2}(\mathbb{R}^{d})\) and \(x\in\mathbb{R}^{d}\), that is \(\mathcal{L}=\tilde{\mathcal{L}}\).

For the forward direction, since \((\mathcal{L}f)(x_{1})=(\tilde{\mathcal{L}}f)(x_{1})\) for all \(f\in C_{b}^{2}(\mathbb{R}^{d})\) and \(x_{1}\in\mathbb{R}^{d}\).

We first set

\[f(x)=x_{p}\,,\]

where \(x_{p}\) denotes the \(p\)-th component of variable \(x\). It is readily checked that

\[b_{p}(x_{1})=\tilde{b}_{p}(x_{1})\,,\]

for all \(x_{1}\in\mathbb{R}^{d}\) and \(p=1,\ldots,d\). As a result,

\[b(x)=\tilde{b}(x)\,,\ \ \text{for all}\ x\in\mathbb{R}^{d}\,.\]

Then we set

\[f(x)=(x_{p}-x_{1p})(x_{q}-x_{1q})\,,\]

where \(x_{1p}\) denotes the \(p\)-th component of \(x_{1}\). It is readily checked that

\[c_{pq}(x_{1})=\tilde{c}_{pq}(x_{1})\,,\]

for all \(x_{1}\in\mathbb{R}^{d}\) and \(p,q=1,\ldots,d\). Consequently,

\[c(x)=\tilde{c}(x)\,,\ \ \text{for all}\ x\in\mathbb{R}^{d}\,.\]

### Proof of Lemma 3.2

Proof.: For the forward direction, since

\[X(\cdot;x_{0},A,G)\stackrel{{\mathrm{d}}}{{=}}X(\cdot;x_{0}, \tilde{A},\tilde{G})\,,\]

one has

\[\mathbb{E}[X_{t}]=\mathbb{E}[\tilde{X}_{t}]\,,\ \ \forall 0\leqslant t<\infty\,.\]

Thus,

\[(X_{t}-\mathbb{E}[X_{t}])_{0\leqslant t<\infty}\stackrel{{ \mathrm{d}}}{{=}}(\tilde{X}_{t}-\mathbb{E}[\tilde{X}_{t}])_{0\leqslant t< \infty}\,,\]

in particular, one has

\[\mathbb{E}\{(X_{t+h}-\mathbb{E}[X_{t+h}])(X_{t}-\mathbb{E}[X_{t}])^{\top}\}= \mathbb{E}\{(\tilde{X}_{t+h}-\mathbb{E}[\tilde{X}_{t+h}])(\tilde{X}_{t}- \mathbb{E}[\tilde{X}_{t}])^{\top}\}\ \ \text{for all}\ 0\leqslant t,h<\infty\,.\]

For the backward direction, we know that the solution of the SDE (1) is a Gaussian process. The distribution of a Gaussian process can be fully determined by its mean and covariance functions. Therefore, the two processes have the same distribution when the mean and covariance are the same for both processes for all \(0\leqslant t,h<\infty\). 

### Proof of Lemma 3.3

Proof.: For the forward direction, since

\[\text{rank}([\gamma_{1}|A\gamma_{1}|\ldots|A^{d-1}\gamma_{1}|\ldots|\gamma_{n }|A\gamma_{n}|\ldots|A^{d-1}\gamma_{n}])<d\,,\]

then for all \(l=[l_{1},\ldots,l_{n}]^{\top}\in\mathbb{R}^{n}\),

\[\text{rank}([\beta|A\beta|\ldots|A^{d-1}\beta])<d\,,\]

where \(\beta:=l_{1}\gamma_{1}+\ldots+l_{n}\gamma_{n}\). Consequently, the corresponding ODE system

\[\dot{x}(t) =Ax(t)\,,\] (21) \[x(0) =\beta\,,\]is not identifiable from \(\beta\) by [57, Theorem 2.5.], where \(\dot{x}(t)\) denotes the first derivative of \(x(t)\) with respect to time \(t\).

Let

\[\tilde{\beta}:=Q^{-1}\beta\in\mathbb{R}^{d}\,,\]

and

\[w_{k}:=\left\{\begin{array}{ll}\tilde{\beta}_{k}\in\mathbb{R}^{1},&\text{ for }k=1,\ldots,K_{1}\,,\\ (\tilde{\beta}_{2k-K_{1}-1},\tilde{\beta}_{2k-K_{1}})^{\top}\in\mathbb{R}^{2},&\text{for }k=K_{1}+1,\ldots,K\,.\end{array}\right.\]

Simple calculation shows that

\[\tilde{\beta} =Q^{-1}\beta\] \[=Q^{-1}(l_{1}\gamma_{1}+\ldots+l_{n}\gamma_{n})\] \[=l_{1}\tilde{\gamma}_{1}+\ldots+l_{n}\tilde{\gamma}_{n}\,,\]

therefore, one has

\[w_{k}=l_{1}w_{1,k}+\ldots+l_{n}w_{n,k}\,,\quad\text{for all }k\in\{1,\ldots,K \}\,.\] (22)

By [43, Theorem 2.4], we know that for any \(l\in\mathbb{R}^{n}\), there always exists \(k\in\{1,\ldots,K\}\) such that \(w_{k}=0\,(\in\mathbb{R}^{1}\text{ or }\mathbb{R}^{2})\) since the ODE (21) is not identifiable from initial state \(\beta\). Next, we will show that this result is satisfied only when there exists a \(k\) such that \(w_{j,k}=0\,\,(\in\mathbb{R}^{1}\text{ or }\mathbb{R}^{2})\) for all \(j=1,\ldots,n\). Let us rearrange the Equation (22) as

\[\begin{bmatrix}w_{1,1}&\ldots&w_{n,1}\\ \vdots&\ddots&\vdots\\ w_{1,K}&\ldots&w_{n,K}\end{bmatrix}\begin{bmatrix}l_{1}\\ \vdots\\ l_{n}\end{bmatrix}=\begin{bmatrix}w_{1}\\ \vdots\\ w_{K}\end{bmatrix}\,,\]

assume for any \(k\in\{1,\ldots,K\}\), \([w_{1,k},\ldots,w_{n,k}]^{\top}\neq 0\), then there always exists a \(l\in\mathbb{R}^{n}\) such that \(w_{k}\neq 0\) for all \(k=\{1,\ldots,K\}\). The reason is that under this circumstance, for any \(k\in\{1,\ldots,K\}\), the set of \(l\)'s such that \(w_{k}=0\) has Lebesgue measure zero in \(\mathbb{R}^{n}\). Therefore, the set of \(l\)'s such that there exists a \(k\) such that \(w_{k}=0\) has Lebesgue measure zero in \(\mathbb{R}^{n}\). This result creates a contradiction. Thus, there must exist a \(k\), such that \([w_{1,k},\ldots,w_{n,k}]^{\top}=0\), that is \(|w_{j,k}|=0\) for all \(j=1,\ldots,n\).

For the backward direction, there exists \(k\) such that \(|w_{j,k}|=0\) for all \(j=1,\ldots,n\), that is \(w_{j,k}=0\,\,(\in\mathbb{R}^{1}\text{ or }\mathbb{R}^{2})\) for all \(j=1,\ldots,n\). Simple calculation shows that

\[\gamma_{j}=Q\tilde{\gamma}_{j}=\sum_{p=1}^{k-1}Q_{p}w_{j,p}+\sum_{p=k+1}^{K}Q _{p}w_{j,p}\,,\]

and

\[A^{q}\gamma_{j} =Q\Lambda^{q}Q^{-1}\gamma_{j}\] \[=Q\Lambda^{q}\tilde{\gamma}_{j}\] \[=\sum_{p=1}^{k-1}Q_{p}J_{p}^{q}w_{j,p}+\sum_{p=k+1}^{K}Q_{p}J_{p} ^{q}w_{j,p}\,,\]

recall that

\[J_{k}=\left\{\begin{array}{ll}\lambda_{k},&\text{if }k=1,\ldots,K_{1}\,,\\ \begin{bmatrix}a_{k}&-b_{k}\\ b_{k}&a_{k}\end{bmatrix},&\text{if }k=K_{1}+1,\ldots,K\,.\end{array}\right.\]

Then matrix

\[M: =[\gamma_{1}|A\gamma_{1}|\ldots|A^{d-1}\gamma_{1}|\ldots|\gamma_{n }|A\gamma_{n}|\ldots|A^{d-1}\gamma_{n}]\] \[=Q_{-k}C\,,\]

where

\[Q_{-k}=[Q_{1}|\ldots|Q_{k-1}|Q_{k+1}|\ldots|Q_{K}]\,,\]and matrix \(C\) denotes:

\[\begin{bmatrix}w_{1,1}&J_{1}w_{1,1}&\dots&J_{1}^{d-1}w_{1,1}&\dots&w_{n,1}&J_{1}w _{n,1}&\dots&J_{1}^{d-1}w_{n,1}\\ \vdots&\vdots&\ddots&\vdots&\ddots&\vdots&\vdots&\ddots&\vdots\\ w_{1,k-1}&J_{k-1}w_{1,k-1}&\dots&J_{k-1}^{d-1}w_{1,k-1}&\dots&w_{n,k-1}&J_{k-1}w _{n,k-1}&\dots&J_{k-1}^{d-1}w_{n,k-1}\\ w_{1,k+1}&J_{k+1}w_{1,k+1}&\dots&J_{k+1}^{d-1}w_{1,k+1}&\dots&w_{n,k+1}&J_{k+1}w _{n,k+1}&\dots&J_{k+1}^{d-1}w_{n,k+1}\\ \vdots&\vdots&\ddots&\vdots&\ddots&\vdots&\vdots&\ddots&\vdots\\ w_{1,K}&J_{K}w_{1,K}&\dots&J_{K}^{d-1}w_{1,K}&\dots&w_{n,K}&J_{K}w_{n,K}&\dots &J_{K}^{d-1}w_{n,K}\end{bmatrix}\]

We know that

\[\text{rank}(M)=\text{rank}(Q_{-k}C)\leqslant\text{min}(\text{rank}(Q_{-k}), \text{rank}(C))\,.\]

When \(k\in\{1,\dots,K_{1}\}\), \(Q_{-k}\in\mathbb{R}^{d\times(d-1)}\), and \(\text{rank}(Q_{-k})=d-1\), while when \(k\in\{K_{1}+1,\dots,K\}\), \(Q_{-k}\in\mathbb{R}^{d\times(d-2)}\), and \(\text{rank}(Q_{-k})=d-2\). In both cases, \(\text{rank}(Q_{-k})<d\), thus \(\text{rank}(M)<d\). 

### Proof of Theorem 3.4

Proof.: Let \(\tilde{A}\in\mathbb{R}^{d\times d}\) and \(\tilde{G}\in\mathbb{R}^{d\times m}\), such that \(X(\cdot;x_{0},A,G)\stackrel{{\text{d}}}{{=}}X(\cdot;x_{0},\tilde {A},\tilde{G})\), we denote as \(X\stackrel{{\text{d}}}{{=}}\tilde{X}\). For simplicity of notation, in the following, we denote \(A_{1}:=A\), \(A_{2}:=\tilde{A}\), \(G_{1}:=G\) and \(G_{2}:=\tilde{G}\), and denote \(X\stackrel{{\text{d}}}{{=}}\tilde{X}\) as \(X^{1}\stackrel{{\text{d}}}{{=}}X^{2}\).

**Sufficiency**. We will show that under the identifiability condition (14), one has \((A_{1},G_{1}G_{1}^{\top})=(A_{2},G_{2}G_{2}^{\top})\).

We first show that \(H_{1}=H_{2}\) (\(H_{i}:=G_{i}G_{i}^{T}\)). Indeed, since \(X^{1},X^{2}\) have the same distribution, one has

\[\mathbb{E}[f(X_{t}^{1})]=\mathbb{E}[f(X_{t}^{2})]\] (23)

for all \(0\leqslant t<\infty\) and \(f\in C^{\infty}(\mathbb{R}^{d})\). By differentiating (23) at \(t=0\), one finds that

\[(\mathcal{L}_{1}f)(x_{0})=(\mathcal{L}_{2}f)(x_{0})\,,\] (24)

where \(\mathcal{L}_{i}\) is the generator of \(X^{i}\) (\(i=1,2\)). Based on the Proposition 2.1,

\[(\mathcal{L}_{i}f)(x_{0})=\sum_{k=1}^{d}\sum_{l=1}^{d}(A_{i})_{kl}x_{0l}\frac {\partial f}{\partial x_{k}}(x_{0})+\frac{1}{2}\sum_{k,l=1}^{d}(H_{i})_{kl} \frac{\partial^{2}f}{\partial x_{k}\partial x_{l}}(x_{0})\,,\]

where \((M)_{kl}\) denotes the \(kl\)-entry of matrix \(M\), and \(x_{0l}\) is the \(l\)-th component of \(x_{0}\). Since (24) is true for all \(f\), by taking

\[f(x)=(x_{p}-x_{0p})(x_{q}-x_{0q})\,,\]

it is readily checked that

\[(H_{1})_{pq}=(H_{2})_{pq}\,,\]

for all \(p,q=1,\dots,d\). As a result, \(H_{1}=H_{2}\). Let us call this matrix \(H\).

Next, we show that \(A_{1}=A_{2}\). We first show the relationship between \(A_{i}\) and \(x_{0}\), and then show the relationship between \(A_{i}\) and \(H\). To this end, one first recalls that

\[X_{t}^{i}=e^{A_{i}t}x_{0}+\int_{0}^{t}e^{A_{i}(t-s)}G_{i}dW_{s}\,.\]

Set \(m_{i}(t):=\mathbb{E}[X_{t}^{i}]\), we know that \(m_{i}(t)\) satisfies the ODE

\[\begin{split}\dot{m}_{i}(t)&=A_{i}m_{i}(t)\,,\;\; \;\forall 0\leqslant t<\infty\,,\\ m_{i}(0)&=x_{0}\,,\end{split}\] (25)

where \(\dot{f}(t)\) denotes the first derivative of function \(f(t)\) with respect to time \(t\).

Simple calculation shows that

\[m_{i}(t)=e^{A_{i}t}x_{0}\,.\]Since \(X^{1}\stackrel{{\rm d}}{{=}}X^{2}\), one has

\[\mathbb{E}[X^{1}_{t}]=\mathbb{E}[X^{2}_{t}]\]

for all \(0\leqslant t<\infty\). That is

\[e^{A_{1}t}x_{0}=e^{A_{2}t}x_{0}\,,\ \ \ \forall 0\leqslant t<\infty\,.\]

Taking \(k\)-th derivative of \(e^{A_{i}t}x_{0}\) with respect to \(t\), one finds that

\[\frac{d^{k}}{dt^{k}}\Big{|}_{t=0}e^{A_{i}t}x_{0}=A^{k}_{i}x_{0}\,,\]

for all \(k=1,2,\ldots\). Consequently,

\[A^{k}_{1}x_{0}=A^{k}_{2}x_{0}\,.\]

Let us denote this vector \(A^{k}x_{0}\). Obviously, one gets

\[A_{1}A^{k-1}x_{0}=A_{2}A^{k-1}x_{0}\ \ \ \mbox{for all $k=1,2,\ldots$}\,.\] (26)

In the following, we show the relationship between \(A_{i}\) and \(H\). Let us denote

\[Y^{i}_{t}:=\int_{0}^{t}e^{A_{i}(t-s)}G_{i}dW_{s}=X^{i}_{t}-\mathbb{E}[X^{i}_{t}]\]

and

\[V_{i}(t,t+h):=\mathbb{E}[Y^{i}_{t+h}\cdot(Y^{i}_{t})^{T}]\,.\]

Simple calculation shows that

\[\begin{split} V_{i}(t,t+h)&=e^{A_{i}h}\int_{0}^{t}e ^{A_{i}(t-s)}He^{A_{i}^{\top}(t-s)}ds\\ &=e^{A_{i}h}V_{i}(t)\,,\end{split}\] (27)

where \(V_{i}(t):=V_{i}(t,t)\).

Since \(X^{1}\stackrel{{\rm d}}{{=}}X^{2}\), by Lemma 3.2, one has

\[V_{1}(t,t+h)=V_{2}(t,t+h)\,,\ \ \ \forall 0\leqslant t,h<\infty\,.\]

To obtain information about \(A_{i}\), let us fix \(t\) for now and take \(k\)-th derivative of (27) with respect to \(h\). One finds that

\[\frac{d^{k}}{dh^{k}}\Big{|}_{h=0}V_{i}(t,t+h)=A^{k}_{i}V_{i}(t)\,,\] (28)

for all \(k=1,2,\ldots\).

On the other hand, the function \(V_{i}(t)\) satisfies the ODE [56]

\[\begin{split}\dot{V}_{i}(t)&=A_{i}V_{i}(t)+V_{i}(t )A_{i}^{\top}+H,\ \ \ 0\leqslant t<\infty\,,\\ V_{i}(0)&=0\,.\end{split}\]

In particular,

\[\dot{V}_{i}(0)=A_{i}V_{i}(0)+V_{i}(0)A_{i}+H=H.\]

By differentiating (28) at \(t=0\), it follows that

\[\frac{d}{dt}\Big{|}_{t=0}\frac{d^{k}}{dh^{k}}\Big{|}_{h=0}V_{i}(t,t+h)=A^{k}_ {i}H\,,\]

for all \(k=1,2,\ldots\). Consequently,

\[A^{k}_{1}H=A^{k}_{2}H\,.\]

Let us denote this matrix \(A^{k}H\). Obviously, by rearranging this matrix, one gets

\[A_{1}A^{k-1}H=A_{2}A^{k-1}H\ \ \ \mbox{for all $k=1,2,\ldots$}\,.\] (29)

Recall our identifiability condition is that \(\mbox{rank}(M)=d\) with

\[M:=[x_{0}|Ax_{0}|\ldots|A^{d-1}x_{0}|H_{.1}|AH_{.1}|\ldots|A^{d-1}H_{.1}| \ldots|H_{.d}|AH_{.d}|\ldots|A^{d-1}H_{.d}]\,.\]If we denote the \(j\)-th column in \(M\) as \(M_{\cdot j}\), one gets

\[A_{1}M_{\cdot j}=A_{2}M_{\cdot j}\,,\]

for all \(j=1,\ldots,d+d^{2}\) by Equations (26) and (29).

This means one can find a full-rank matrix \(B\in\mathbb{R}^{d\times d}\) by horizontally stacking \(d\) linearly independent columns of matrix \(M\), such that \(A_{1}B=A_{2}B\). Since \(B\) is invertible, one thus concludes that \(A_{1}=A_{2}\). Hence, the sufficiency of the condition is proved.

**Necessity**. In the following, we will show that when \(A\) has distinct eigenvalues. The condition (14) stated in Theorem 3.4 is also necessary. Specifically, we will show that when the identifiability condition (14) is not satisfied, one can always find a \(\tilde{A}\) with \((A,GG^{\top})\neq(\tilde{A},GG^{\top})\) such that \(X\stackrel{{\mathrm{d}}}{{=}}\tilde{X}\). Recall that for simplicity of notation, we denote \(A_{1}:=A\), \(A_{2}:=\tilde{A}\), and denote \(X\stackrel{{\mathrm{d}}}{{=}}\tilde{X}\) as \(X^{1}\stackrel{{\mathrm{d}}}{{=}}X^{2},\) where process \(X^{i}=\{X_{t}^{i}:0\leqslant t<\infty\}\), and \(X_{t}^{i}=X(t;x_{0},A_{i},G)\) following the form described in the solution process (3). In the following, we may use both \(A\) and \(A_{1}\) interchangeably according to the context.

By Lemma 3.2, to guarantee \(X^{1}\stackrel{{\mathrm{d}}}{{=}}X^{2}\) one only needs to show that

\[\mathbb{E}[X_{t}^{1}] =\mathbb{E}[X_{t}^{2}]\,,\;\;\;\forall 0\leqslant t<\infty\,,\] \[V_{1}(t,t+h) =V_{2}(t,t+h)\,,\;\;\;\forall 0\leqslant t,h<\infty\,.\]

That is,

\[e^{A_{1}t}x_{0} =e^{A_{2}t}x_{0}\,,\;\;\;\forall 0\leqslant t<\infty\,,\] (30) \[e^{A_{1}h}V(t) =e^{A_{2}h}V(t)\,,\;\;\;\forall 0\leqslant t,h<\infty\,,\] \[V_{1}(t) =V_{2}(t)\,,\;\;\;\forall 0\leqslant t<\infty\,,\]

where \(V(t):=V_{1}(t)=V_{2}(t)\).

Recall that \(H=GG^{\top}\). For simplicity of notation, abusing notation a bit, we denote \(H_{\cdot 0}:=x_{0}\). Let

\[\tilde{H}_{\cdot j}:=Q^{-1}H_{\cdot j}\,,\;\;\;\text{for all }j=0,\ldots,d\,,\]

and

\[w_{j,k}:=\left\{\begin{array}{ll}\tilde{H}_{\cdot j,k}\in\mathbb{R}^{1},& \text{for }k=1,\ldots,K_{1}\,,\\ (\tilde{H}_{\cdot j,2k-K_{1}-1},\tilde{H}_{\cdot j,2k-K_{1}})^{\top}\in\mathbb{ R}^{2},&\text{for }k=K_{1}+1,\ldots,K\,.\end{array}\right.\]

When the identifiability condition (14) is not satisfied, that is

\[\text{rank}([H_{\cdot 0}|AH_{\cdot 0}|\ldots|A^{d-1}H_{\cdot 1}|AH_{\cdot 1} |\ldots|A^{d-1}H_{\cdot 1}|\ldots|H_{\cdot d}|AH_{\cdot d}|\ldots|A^{d-1}H_{ \cdot d}])<d\,,\]

by Lemma 3.3, there exists \(k\) such that \(|w_{j,k}|=0\), i.e., \(w_{j,k}=0\) (\(\in\mathbb{R}^{1}\) or \(\mathbb{R}^{2}\)), for all \(j=0,\ldots,d\). Recall that

\[V(t)=V_{1}(t) =\int_{0}^{t}e^{A(t-s)}He^{A^{\top}(t-s)}ds\] (31) \[=\int_{0}^{t}Qe^{\Lambda(t-s)}Q^{-1}Q[\tilde{H}_{\cdot 1}|\ldots| \tilde{H}_{\cdot d}]e^{A^{\top}(t-s)}ds\] \[=Q\int_{0}^{t}e^{\Lambda(t-s)}[\tilde{H}_{\cdot 1}|\ldots| \tilde{H}_{\cdot d}]e^{A^{\top}(t-s)}ds\] \[:=Q\int_{0}^{t}We^{A^{\top}(t-s)}ds\,,\]

where

\[W=e^{\Lambda(t-s)}[\tilde{H}_{\cdot 1}|\ldots|\tilde{H}_{\cdot d}]\,,\]

and some calculation shows that

\[W=\begin{bmatrix}e^{J_{1}(t-s)}w_{1,1}&\ldots&e^{J_{1}(t-s)}w_{d,1}\\ \vdots&\ddots&\vdots\\ e^{J_{K}(t-s)}w_{1,K}&\ldots&e^{J_{K}(t-s)}w_{d,K}\end{bmatrix}\,,\]recall that

\[J_{k}=\left\{\begin{array}{ll}\lambda_{k},&\text{if }k=1,\ldots,K_{1}\,,\\ \begin{bmatrix}a_{k}&-b_{k}\\ b_{k}&a_{k}\end{bmatrix},&\text{if }k=K_{1}+1,\ldots,K\,.\end{array}\right.\]

Since \(w_{j,k}=0\) (\(\in\mathbb{R}^{1}\) or \(\mathbb{R}^{2}\)), for all \(j=0,1,\ldots,d\), then if \(k\in\{1,\ldots,K_{1}\}\), the \(k\)-th row of \(W\)

\[W_{k\cdot}=0\,;\]

and if \(k\in\{K_{1}+1,\ldots,K\}\), then the \((2k-K_{1}-1)\)-th and the \((2k-K_{1})\)-th rows

\[W_{(2k-K_{1}-1)\cdot}=W_{(2k-K_{1})\cdot}=0\,,\]

where \(W_{k\cdot}\) denotes the \(k\)-th row vector of matrix \(W\).

If we denote

\[\tilde{V}(t)_{\cdot j}:=Q^{-1}V(t)_{\cdot j}\,,\;\;\;\text{for all }j=1,\ldots,d\,,\]

and

\[w(t)_{j,k}:=\left\{\begin{array}{ll}\tilde{V}(t)_{\cdot j,k}\in\mathbb{R}^{ 1},&\text{for }k=1,\ldots,K_{1}\,,\\ (\tilde{V}(t)_{\cdot j,2k-K_{1}-1},\tilde{V}(t)_{\cdot j,2k-K_{1}})^{\top}\in \mathbb{R}^{2},&\text{for }k=K_{1}+1,\ldots,K\,.\end{array}\right.\]

Then by multiplying \(Q^{-1}\) in both sides of Equation (31), one obtains that

\[w(t)_{j,k}=0\;(\in\mathbb{R}^{1}\;\text{or}\;\mathbb{R}^{2})\]

for all \(j=1,\ldots,d\) and all \(0\leqslant t<\infty\). This indicates that when all the vectors \(H_{\cdot j}\) for \(j=1,\ldots,d\) are confined to an \(A\)-invariant proper subspace of \(\mathbb{R}^{d}\), denotes as \(L\), then each column of the covariance matrix \(V(t)\) in Equation (30) is also confined to \(L\), for all \(0\leqslant t<\infty\). Therefore, under condition (14), \(x_{0},H_{\cdot j}\) (for all \(j=1,\ldots,d\)) and each column of the covariance matrix \(V(t)\) (for all \(0\leqslant t<\infty\)) are confined to an \(A\)-invariant proper subspace of \(\mathbb{R}^{d}\). Thus, a matrix \(A_{2}\) exists, with \(A_{2}\neq A_{1}\) such that the first two equations in Equation (30) are satisfied.

In particular, by [43, Theorem 2.5], when \(k\in\{1,\ldots,K_{1}\}\), there exists matrix \(D\in\mathbb{R}^{d\times d}\), with the \(kk\)-th element \(D_{kk}=c\neq 0\) and all the other elements of \(D\) are zeros. Let

\[A_{2}=A_{1}+QDQ^{-1}\neq A_{1}\,,\]

then \(A_{1}\) and \(A_{2}\) satisfy the first two equations in Equation (30). Then we will show that such a \(A_{2}\) also satisfy the third equation in Equation (30).

Some calculation shows that

\[\begin{split} V_{1}(t)&=\int_{0}^{t}e^{A_{1}(t-s)} He^{A_{1}^{\top}(t-s)}ds\\ &=\int_{0}^{t}Qe^{\Lambda(t-s)}Q^{-1}H(Q^{T})^{-1}e^{\Lambda(t-s )}Q^{T}ds\\ &:=\int_{0}^{t}Qe^{\Lambda(t-s)}P_{1}e^{\Lambda(t-s)}Q^{T}ds\,, \end{split}\] (32)

where \(P_{1}:=Q^{-1}H(Q^{T})^{-1}\). And

\[\begin{split} V_{2}(t)&=\int_{0}^{t}e^{A_{2}(t-s)} He^{A_{2}^{\top}(t-s)}ds\\ &=\int_{0}^{t}e^{(A_{1}+QDQ^{-1})(t-s)}He^{(A_{1}+QDQ^{-1})^{ \top}(t-s)}ds\\ &=\int_{0}^{t}Qe^{\Lambda(t-s)}e^{D(t-s)}Q^{-1}H(Q^{T})^{-1}e^{D( t-s)}e^{\Lambda(t-s)}Q^{T}ds\\ &:=\int_{0}^{t}Qe^{\Lambda(t-s)}P_{2}e^{\Lambda(t-s)}Q^{T}ds\,, \end{split}\] (33)where \(P_{2}:=e^{D(t-s)}Q^{-1}H(Q^{T})^{-1}e^{D(t-s)}\). If one can show that \(P_{1}=P_{2}\), then it is readily checked that \(V_{1}(t)=V_{2}(t)\) for all \(0\leqslant t<\infty\). Recall that

\[Q^{-1}H=\tilde{H},\]

where \(\tilde{H}=[\tilde{H}_{\cdot 1}|\ldots|\tilde{H}_{\cdot d}]\). And when condition (14) is not satisfied, the \(k\)-th row of \(\tilde{H}\):

\[\tilde{H}_{k\cdot}=0\,.\]

Since

\[P_{1}=Q^{-1}H(Q^{T})^{-1}=\tilde{H}(Q^{T})^{-1}\,,\]

therefore, the \(k\)-th row of \(P_{1}\):

\[(P_{1})_{k\cdot}=0\,.\]

Simple calculation shows that matrix \(P_{1}\) is symmetric, thus, the \(k\)-th column of \(P_{1}\):

\[(P_{1})_{\cdot k}=0\,.\]

It is easy to obtain that \(e^{D(t-s)}\) is a diagonal matrix expressed as

\[e^{D(t-s)}=\begin{bmatrix}1&&&&\\ &\ddots&&\\ &&e^{c(t-s)}&&\\ &&&\ddots&\\ &&&&1\end{bmatrix}\]

where \(e^{c(t-s)}\) is the \(kk\)-th entry. Then, simple calculation shows that

\[P_{2}=e^{D(t-s)}Q^{-1}H(Q^{T})^{-1}e^{D(t-s)}=e^{D(t-s)}P_{1}e^{D(t-s)}=P_{1}\,.\]

Therefore, one obtains that

\[V_{1}(t)=V_{2}(t)\,,\;\;\;\forall 0\leqslant t<\infty\,.\]

Hence, when \(k\in\{1,\ldots,K_{1}\}\), we find a \(A_{2}\), with \(A_{2}\neq A_{1}\) such that Equation (30) is satisfied.

When \(k\in\{K_{1}+1,\ldots,K\}\), there exists matrix \(D^{\prime}\in\mathbb{R}^{d\times d}\), with

\[\begin{bmatrix}D^{\prime}_{2k-K_{1}-1,2k-K_{1}-1}&D^{\prime}_{2k-K_{1}-1,2k- K_{1}}\\ \tilde{D}^{\prime}_{2k-K_{1},2k-K_{1}-1}&D^{\prime}_{2k-K_{1},2k-K_{1}}\\ \end{bmatrix}:=\begin{bmatrix}c_{1}&c_{2}\\ c_{3}&c_{4}\end{bmatrix}\,,\]

where \(M_{i,j}\) denotes the \(ij\)-th entry of matrix \(M\), \(c=[c_{1},c_{2},c_{3},c_{4}]^{\top}\neq 0\), and all the other elements of \(D^{\prime}\) are zeros. Let

\[A_{2}=A_{1}+QD^{\prime}Q^{-1}\neq A_{1}\,,\]

then \(A_{1}\) and \(A_{2}\) satisfy the first two equations in Equation (30). Similar to the case where \(k\in\{1,\ldots,K_{1}\}\), one can also show that such a \(A_{2}\) also satisfies the third equation in Equation (30).

Therefore, assuming \(A\) has distinct eigenvalues, then when the identifiability condition (14) is not satisfied, one can always find a \(A_{2}\) with \((A_{1},GG^{\top})\neq(A_{2},GG^{\top})\) such that Equation (30) is satisfied, i.e., \(X^{1}\stackrel{{\mathrm{d}}}{{=}}X^{2}\). Hence, the necessity of the condition is proved. 

### Proof of Corollary 3.4.1

Proof.: There are two ways to prove this corollary, we will present both of them in the following.

**Way1.** By [62, Lemma 2.2],

\[\text{span}([G|AG|\ldots|A^{d-1}G])=\text{span}([GG^{\top}|AGG^{\top}|\ldots|A ^{d-1}GG^{\top}])\,,\]

where \(\text{span}(M)\) denotes the linear span of the columns of the matrix \(M\). therefore, when

\[\text{rank}([G|AG|\ldots|A^{d-1}G])=d\,,\]

then

\[\text{span}([G|AG|\ldots|A^{d-1}G])=\mathbb{R}^{d}\,,\]thus,

\[\text{span}([GG^{\top}|AGG^{\top}|\ldots|A^{d-1}GG^{\top}])=\mathbb{R}^{d}\,.\]

Therefore,

\[\text{rank}([GG^{\top}|AGG^{\top}|\ldots|A^{d-1}GG^{\top}])=d\,,\]

since the rank of a matrix is the dimension of its span. Then by Theorem 3.4, the generator of the SDE (1) is identifiable from \(x_{0}\).

**Way2**.: Let \(\tilde{A}\in\mathbb{R}^{d\times d}\) and \(\tilde{G}\in\mathbb{R}^{d\times m}\), such that \(X(\cdot;x_{0},A,G)\stackrel{{\text{\rm d}}}{{=}}X(\cdot;x_{0}, \tilde{A},\tilde{G})\), we denote as \(X\stackrel{{\text{\rm d}}}{{=}}\tilde{X}\), we will show that under our identifiability condition \((A,GG^{\top})=(\tilde{A},\tilde{G}\tilde{G}^{\top})\). By applying the same notations used in the proof of Theorem 3.4, in the following, we denote \(A_{1}:=A\), \(A_{2}:=\tilde{A}\), \(G_{1}:=G\) and \(G_{2}:=\tilde{G}\).

In the proof of Theorem 3.4, we have shown that \(G_{1}G_{1}^{\top}=G_{2}G_{2}^{\top}\), thus, we only need to show that under the condition stated in this corollary, \(A_{1}=A_{2}\). According to the proof of Theorem 3.4, for all \(0\leqslant t<\infty\), we have

\[V_{1}(t) =V_{2}(t)\,,\] \[A_{1}V_{1}(t) =A_{2}V_{2}(t)\,.\]

Let \(V(t):=V_{i}(t)(i=1,2)\), one gets

\[A_{1}V(t)=A_{2}V(t)\,,\ \ \ \forall 0\leqslant t<\infty\,.\]

Therefore, if there exists a \(0\leqslant t<\infty\), such that \(V(t)\) is nonsingular, then one can conclude that \(A_{1}=A_{2}\).

By [20, Theorem 3.2], the covariance \(V(t)\) is nonsingular for all \(t>0\), if and only if

\[\text{rank}([G|AG|\ldots|A^{d-1}G])=d\,,\]

that is the pair \([A,G]\) is controllable. Therefore, under the condition stated in this corollary, \(A_{1}=A_{2}\), thus the generator of the SDE (1) is identifiable from \(x_{0}\). 

### Proof of Theorem 3.5

Proof.: Let \(\tilde{A},\tilde{G}_{k}\in\mathbb{R}^{d\times d}\) for all \(k=1,\ldots,m\), such that \(X(\cdot;x_{0},A,\{G_{k}\}_{k=1}^{m})\stackrel{{\text{\rm d}}}{{= }}X(\cdot;x_{0},\tilde{A},\{\tilde{G}_{k}\}_{k=1}^{m})\), we denote as \(X\stackrel{{\text{\rm d}}}{{=}}\tilde{X}\), we will show that under our identifiability condition, for all \(x\in\mathbb{R}^{d}\), \((A,\sum_{k=1}^{m}G_{k}xx^{\top}G_{k}^{\top})=(\tilde{A},\sum_{k=1}^{m}\tilde{ G}_{k}xx^{\top}\tilde{G}_{k}^{\top})\). For simplicity of notation, in the following, we denote \(A_{1}:=A\), \(A_{2}:=\tilde{A}\), \(G_{1,k}:=G_{k}\) and \(G_{2,k}:=\tilde{G}_{k}\), and denote \(X\stackrel{{\text{\rm d}}}{{=}}\tilde{X}\) as \(X^{1}\stackrel{{\text{\rm d}}}{{=}}X^{2}\).

We first show that \(A_{1}=A_{2}\). Set \(m_{i}(t):=\mathbb{E}[X_{t}^{i}]\), we know that \(m_{i}(t)\) satisfies the ODE

\[\begin{array}{l}\dot{m}_{i}(t)=A_{i}m_{i}(t)\,,\ \ \ \forall 0\leqslant t<\infty\,,\\ m_{i}(0)=x_{0}\,,\end{array}\] (34)

where \(\dot{f}(t)\) denotes the first derivative of function \(f(t)\) with respect to time \(t\).

Simple calculation shows that

\[m_{i}(t)=e^{A_{i}t}x_{0}\,.\]

Since \(X^{1}\stackrel{{\text{\rm d}}}{{=}}X^{2}\), one has

\[\mathbb{E}[X_{t}^{1}]=\mathbb{E}[X_{t}^{2}]\]

for all \(0\leqslant t<\infty\). That is

\[e^{A_{1}t}x_{0}=e^{A_{2}t}x_{0}\,,\ \ \ \forall 0\leqslant t<\infty\,.\]

Taking \(j\)-th derivative of \(e^{A_{i}t}x_{0}\) with respect to \(t\), one finds that

\[\frac{d^{j}}{dt^{j}}\Big{|}_{t=0}e^{A_{i}t}x_{0}=A_{i}^{j}x_{0}\,,\]for all \(j=1,2,\ldots\). Consequently,

\[A_{1}^{j}x_{0}=A_{2}^{j}x_{0}\,.\]

Let us denote this vector \(A^{j}x_{0}\). Obviously, one gets

\[A_{1}A^{j-1}x_{0}=A_{2}A^{j-1}x_{0}\ \ \text{ for all }j=1,2,\ldots\,.\] (35)

By condition A1, it is readily checked that \(A_{1}=A_{2}\) from Equation (35).

In the following, we show that under condition A2, for all \(x\in\mathbb{R}^{d}\),

\[\sum_{k=1}^{m}G_{1,k}xx^{\top}G_{1,k}^{\top}=\sum_{k=1}^{m}G_{2,k}xx^{\top}G_{ 2,k}^{\top}\,.\]

We know the function \(P_{i}(t):=\mathbb{E}[X_{t}^{i}(X_{t}^{i})^{\top}]\) satisfies the ODE

\[\dot{P}_{i}(t) =A_{i}P_{i}(t)+P_{i}(t)A_{i}^{\top}+\sum_{k=1}^{m}G_{i,k}P_{i}(t)G _{i,k}^{\top}\,,\ \ \forall 0\leqslant t<\infty\,,\] (36) \[P_{i}(0) =x_{0}x_{0}^{\top}\,.\]

Since \(X^{1}\stackrel{{\mathrm{d}}}{{=}}X^{2}\),

\[P_{1}(t)=P_{2}(t)\,,\ \ \forall 0\leqslant t<\infty\,,\]

let us call it \(P(t)\). By differentiating \(P_{i}(t)\) one also gets that

\[\dot{P}_{1}(t)=\dot{P}_{2}(t)\,,\ \ \forall 0\leqslant t<\infty\,.\]

Since we have shown that \(A_{1}=A_{2}\) under condition A1, from Equation (36) one observes that

\[\sum_{k=1}^{m}G_{1,k}P(t)G_{1,k}^{\top}=\sum_{k=1}^{m}G_{2,k}P(t)G_{2,k}^{ \top}\,,\ \ \forall 0\leqslant t<\infty\,.\] (37)

By vectorizing \(P(t)\), some calculation shows that the ODE (36) can be expressed as

\[\text{vec}(\dot{P}(t)) =\mathcal{A}\text{vec}(P(t))\,,\] (38) \[\text{vec}(P(0)) =\text{vec}(x_{0}x_{0}^{\top})\,,\]

with an explicit solution

\[\text{vec}(P(t))=e^{\mathcal{A}t}\text{vec}(x_{0}x_{0}^{\top})\,,\]

where \(\mathcal{A}=A\oplus A+\sum_{k=1}^{m}G_{k}\otimes G_{k}\in\mathbb{R}^{d^{2} \times d^{2}}\), and \(\text{vec}(M)\) denotes the vector by stacking the columns of matrix \(M\) vertically.

By definition, \(P(t)\in\mathbb{R}^{d\times d}\) is symmetric, thus \(\text{vec}(P(t))\) for all \(0\leqslant t<\infty\) is confined to a proper subspace of \(\mathbb{R}^{d^{2}}\), let us denote this proper subspace \(W\), simple calculation shows that

\[\text{dim}(W)=(d^{2}+d)/2\,,\]

where \(\text{dim}(W)\) denotes the dimension of the subspace \(W\), that is the number of vectors in any basis for \(W\). In particular, one can find a basis of \(W\) denoting as

\[\left\{\text{vec}(E_{11}),\text{vec}(E_{21}),\text{vec}(E_{22}),\ldots,\text{ vec}(E_{dd})\right\},\]

where \(E_{ij}\) stands for a \(d\times d\) matrix, with the \(ij\)-th and \(ji\)-th elements are \(1\) and all other elements are \(0\), for all \(i,j=1,\ldots,d\) and \(i\geqslant j\).

Suppose there exists \(t_{i}\)'s, for \(i=1,\ldots,(d^{2}+d)/2\), such that \(\text{vec}(P(t_{1})),\ldots,\text{vec}(P(t_{(d^{2}+d)/2}))\) are linearly independent, then for all \(x\in\mathbb{R}^{d}\),

\[\text{vec}(xx^{\top})=l_{1}\text{vec}(P(t_{1}))+\ldots+l_{(d^{2}+d)/2}\text{ vec}(P(t_{(d^{2}+d)/2}))\,,\]

that is

\[xx^{\top}=l_{1}P(t_{1})+\ldots+l_{(d^{2}+d)/2}P(t_{(d^{2}+d)/2})\,,\]where \(l:=\{l_{1},\ldots,l_{(d^{2}+d)/2}\}\in\mathbb{R}^{(d^{2}+d)/2}\). According to Equation (37), it is readily checked that for all \(x\in\mathbb{R}^{d}\),

\[\sum_{k=1}^{m}G_{1,k}xx^{\top}G_{1,k}^{\top}=\sum_{k=1}^{m}G_{2,k}xx^{\top}G_{2, k}^{\top}\,.\]

By [57, Lemma 6.1], there exists \((d^{2}+d)/2\,t_{i}\)'s such that \(\text{vec}(P(t_{1})),\ldots,\text{vec}(P(t_{(d^{2}+d)/2}))\) are linearly independent, if and only if the orbit of \(\text{vec}(P(t))\) (i.e., the trajectory of ODE (38) started from initial state \(v\)), denoting as \(\gamma(\mathcal{A},v)\) with \(v=\text{vec}(x_{0}x_{0}^{\top})\), is not confined to a proper subspace of \(W\).

Next, we show that under condition A2, orbit \(\gamma(\mathcal{A},v)\) is not confined to a proper subspace of \(W\).

Assume orbit \(\gamma(\mathcal{A},v)\) is confined to a proper subspace of \(W\). Then there exists \(w\neq 0\in W\) such that

\[w^{\top}e^{\mathcal{A}t}v=0\,,\ \ \ \forall 0\leqslant t<\infty\,.\]

By taking \(j\)-th derivative with respect to \(t\), we have

\[w^{T}\mathcal{A}^{j}e^{\mathcal{A}t}v=0\,,\ \ \forall 0\leqslant t<\infty,j=0, \ldots,(d^{2}+d-2)/2\,.\]

In particular, for \(t=0\),

\[w^{T}\mathcal{A}^{j}v=0\,,\ \ \text{for}\ j=0,\ldots,(d^{2}+d-2)/2\,.\]

Therefore,

\[w^{T}[v|\mathcal{A}v|\ldots|\mathcal{A}^{(d^{2}+d-2)/2}v]=0\,.\] (39)

Since \(w\in W\), \(w\in\text{span}\{\text{vec}(E_{11}),\text{vec}(E_{21}),\ldots,\text{vec}(E_{ dd})\}\), set \(\text{vec}(\overline{w}):=w\), then \(\overline{w}\) is a \(d\times d\) symmetric matrix. Since \(P(t)\) is symmetric for all \(0\leqslant t<\infty\), according to Equation (36), simple calculation shows that the \(j\)-th derivative of \(P(t)\) is also symmetric for all \(0\leqslant t<\infty\), for \(j=0,1,\ldots\). Recall that

\[\text{vec}(P(t)) =e^{\mathcal{A}t}v\,,\] \[\text{vec}(P^{(j)}(t)) =\mathcal{A}^{j}e^{\mathcal{A}t}v\,,\]

where \(P^{(j)}(t)\) denotes the \(j\)-th derivative of \(P(t)\) with respect to \(t\). In particular, when \(t=0\), one has

\[\text{vec}(P(0)) =v\,,\] \[\text{vec}(P^{(j)}(0)) =\mathcal{A}^{j}v\,,\]

then if we denote matrix \(\overline{\mathcal{A}^{j}v}\) by setting \(\text{vec}(\overline{\mathcal{A}^{j}v}):=\mathcal{A}^{j}v\), matrices \(\overline{\mathcal{A}^{j}v}\) are symmetric for all \(j=0,1,\ldots\).

Therefore, we can say that there are only \((d^{2}+d)/2\) distinct elements in each of the vectors: \(w,v,\mathcal{A}v,\ldots,\mathcal{A}^{(d^{2}+d-2)/2}v\) in Equation (39). Moreover, since these vectors all correspond to \(d\times d\) symmetric matrices, the repetitive elements in each vector appear in the same positions in each vector. Hence, we can focus on checking those distinct elements in each vector, that is Equation (39) can be expressed as

\[\underline{w}^{T}[\underline{v}|\mathcal{A}v|\cdots|\overline{\mathcal{A}^{ (d^{2}+d-2)/2}v}]=0\,,\] (40)

where \(\underline{w}\in\mathbb{R}^{(d^{2}+d)/2}\) denotes as

\[\underline{w}:=[\overline{w}_{11},\sqrt{2}\overline{w}_{21},\ldots,\sqrt{2} \overline{w}_{d1},\overline{w}_{22},\ldots,\sqrt{2}\overline{w}_{d2},\ldots, \overline{w}_{dd}]^{\top}\,,\]

where \(\overline{w}_{ij}\) denotes the \(ij\)-th element of \(\overline{w}\), with \(i,j=1,\ldots,d\) and \(i\geqslant j\). When \(i\neq j\), the element is multiplied by a \(\sqrt{2}\), that is, \(\underline{w}\) only keeps the distinctive elements in \(w\), and for each of the repetitive element, we multiply \(\sqrt{2}\). We define \(\underline{v},\underline{\mathcal{A}v},\ldots\) using the same way.

Under condition A2, matrix

\[[\underline{v}|\underline{\mathcal{A}v}|\ldots|\underline{\mathcal{A}^{(d^{2} +d-2)/2}v}]\in\mathbb{R}^{\frac{(d^{2}+d)}{2}\times\frac{(d^{2}+d)}{2}}\]

is easily to be checked to be invertible, then \(\underline{w}=0\), thus \(w=0\). This contradicts to \(w\neq 0\), therefore, under condition A2, orbit \(\gamma(\mathcal{A},v)\) is not confined to a proper subspace of \(W\). Hence, the theorem is proved.

Genericity of the derived identifiability conditions

### The identifiability condition stated in Theorem 3.5 is generic

We will show that the identifiability condition stated in Theorem 3.5 is generic. Specifically, we will show that for the set of \((x_{0},A,\{G_{k}\}_{k=1}^{m})\in\mathbb{R}^{d+(m+1)d^{2}}\) such that either condition A1 or A2 stated in Theorem 3.5 is violated, has Lebesgue measure zero in \(\mathbb{R}^{d+(m+1)d^{2}}\). In the following, we first present a lemma we will use to prove our main proposition.

**Lemma B.1**.: _Let \(p:\mathbb{R}^{n}\to\mathbb{R}\) be a non-zero polynomial function. Let \(Z:=\{x\in\mathbb{R}^{n}:p(x)=0\}\). Then \(Z\) has Lebesgue measure zero in \(\mathbb{R}^{n}\)._

Proof.: When \(n=1\), suppose the degree of \(x\) is \(k\geqslant 1\), then by the fundamental theorem of algebra, there are at most \(k\)\(x\)'s such that \(x\in Z\). Therefore, Z has Lebesgue measure zero, since a finite set has measure zero in \(\mathbb{R}\).

Suppose the lemma is established for polynomials in \(n-1\) variables. Let \(p\) be a non-zero polynomial in \(n\) variables, say of degree \(k\geqslant 1\) in \(x_{n}\), then we can write

\[p(\bm{x},x_{n})=\sum_{j=0}^{k}p_{j}(\bm{x})x_{n}^{j}\,,\]

where \(\bm{x}=\{x_{1},\ldots,x_{n-1}\}\) and \(p_{0},\ldots,p_{k}\) are polynomials in the \(n-1\) variables \(\{x_{1},\ldots,x_{n-1}\}\), and there exists \(j\in\{0,\ldots,k\}\) such that \(p_{j}\) is a non-zero polynomial since \(p\) is a non-zero polynomial. Then we can denote \(Z\) as

\[Z=\{(\bm{x},x_{n}):p(\bm{x},x_{n})=0\}\,.\]

Suppose \((\bm{x},x_{n})\in Z\), then there are two possibilities:

* \(p_{0}(\bm{x})=\ldots=p_{k}(\bm{x})=0\).
* \(\text{there exists }i\in\{0,\ldots,k\}\) such that \(p_{i}(\bm{x})\neq 0\).

Let

\[A :=\{(\bm{x},x_{n})\in Z:\text{ case 1 is satisfied}\}\,,\] \[B :=\{(\bm{x},x_{n})\in Z:\text{ case 2 is satisfied}\}\,,\]

then \(Z=A\cup B\).

For case 1, recall that there exists \(j\in\{0,\ldots,k\}\) such that \(p_{j}\) is a non-zero polynomial, let

\[A_{j}:=\{\bm{x}\in\mathbb{R}^{n-1}:p_{j}(\bm{x})=0\}\,,\]

then by the induction hypothesis, \(A_{j}\) has Lebesgue measure zero in \(\mathbb{R}^{n-1}\). Therefore, \(A_{j}\times\mathbb{R}\) has Lebesgue measure zero in \(\mathbb{R}^{n}\). Since \(A\subseteq A_{j}\times\mathbb{R}\), \(A\) has Lebesgue measure zero in \(\mathbb{R}^{n}\).

For case 2, let \(\lambda^{n}\) be Lebesgue measure on \(\mathbb{R}^{n}\), then

\[\lambda^{n}(B) =\int_{\mathbb{R}^{n}}\mathbbm{1}_{B}(\bm{x},x_{n})d\lambda^{n}\] (41) \[=\int_{\mathbb{R}^{n}}\mathbbm{1}_{B}(\bm{x},x_{n})d\bm{x}dx_{n}\] \[=\int_{\mathbb{R}^{n-1}}\Big{(}\int_{\mathbb{R}}\mathbbm{1}_{B}( \bm{x},x_{n})dx_{n}\Big{)}d\bm{x}\,,\]

where

\[\mathbbm{1}_{B}(\bm{x},x_{n})=\left\{\begin{array}{ll}1,&\text{if }(\bm{x},x_{n})\in B\,,\\ 0,&\text{if }(\bm{x},x_{n})\notin B\,.\end{array}\right.\]

The inner integral in Equation (41) is equal to zero, since for a fixed \(\bm{x}\), there are finitely many (indeed, at most \(k\)) \(x_{n}\)'s such that \(p(\bm{x},x_{n})=0\) under the condition of case 2. Thus, \(\lambda^{n}(B)=0\), that is, \(B\) has Lebesgue measure zero in \(\mathbb{R}^{n}\). Then it is readily checked that \(Z\) has Lebesgue measure zero.

Now we are ready to present the main proposition.

**Proposition B.1**.: _Let_

\[S:=\{(x_{0},A,\{G_{k}\}_{k=1}^{m})\in\mathbb{R}^{d+(m+1)d^{2}}:\text{ either condition A1 or A2 in Theorem 3.5 is violated}\}\,,\]

_then \(S\) has Lebesgue measure zero in \(\mathbb{R}^{d+(m+1)d^{2}}\)._

Proof.: Let

\[S_{A}:=\{(x_{0},A)\in\mathbb{R}^{d+d^{2}}:\text{ condition A1 is violated}\}\,,\]

we first show that \(S_{A}\) has Lebesgue measure zero in \(\mathbb{R}^{d+d^{2}}\). Suppose \((x_{0},A)\in S_{A}\), then \((x_{0},A)\) satisfies

\[\text{rank}([x_{0}|Ax_{0}|\ldots|A^{d-1}x_{0}])<d\,,\]

that is the set of vectors \(\{x_{0},Ax_{0},\ldots,A^{d-1}x_{0}\}\) are linearly dependent, this means that

\[\text{det}([x_{0}|Ax_{0}|\ldots|A^{d-1}x_{0}])=0\,.\] (42)

It is a simple matter of algebra that the left side of (42) can be expressed as some universal polynomial of the entries of \(x_{0}\) and entries of \(A\), denotes \(p(x_{0},A)=p(x_{01},\ldots,x_{0d},a_{11},a_{12},\ldots,a_{dd})\), where \(x_{0j}\) denotes the \(j\)-th entry of \(x_{0}\) and \(a_{ij}\) denotes the \(ij\)-th entry of \(A\). Therefore, one concludes that

\[p(x_{0},A)=p(x_{01},\ldots,x_{0d},a_{11},a_{12},\ldots,a_{dd})=0\,.\]

Thus, \(S_{A}\) can be expressed as

\[S_{A}=\{(x_{0},A)\in\mathbb{R}^{d+d^{2}}:p(x_{0},A)=0\}\,.\]

Some calculation shows that

\[p(x_{0},A)=\sum_{i_{1},\ldots,i_{d}=1}^{d}x_{0i_{1}}\ldots x_{0i_{d}}\text{det }([(A^{0})_{\cdot i_{1}}|(A^{1})_{\cdot i_{2}}|\ldots|(A^{d-1})_{\cdot i_{d}}] )\,,\] (43)

where \((M)_{\cdot j}\) denotes the \(j\)-th column vector of matrix \(M\). Obviously, \(p(x_{0},A)\) is a non-zero polynomial function of entries of \(x_{0}\) and entries of \(A\), therefore, by Lemma B.1, \(S_{A}\) has Lebesgue measure zero in \(\mathbb{R}^{d+d^{2}}\). Let

\[S_{1}:=\{(x_{0},A,\{G_{k}\}_{k=1}^{m})\in\mathbb{R}^{d+(m+1)d^{2}}:\text{ condition A1 is violated}\}\,,\]

then it is readily checked that \(S_{1}\) has Lebesgue measure zero in \(\mathbb{R}^{d+(m+1)d^{2}}\).

Let

\[S_{2}:=\{(x_{0},A,\{G_{k}\}_{k=1}^{m})\in\mathbb{R}^{d+(m+1)d^{2}}:\text{ condition A2 is violated}\}\,,\]

we then show that \(S_{2}\) has Lebesgue measure zero in \(\mathbb{R}^{d+(m+1)d^{2}}\). Suppose \((x_{0},A,\{G_{k}\}_{k=1}^{m})\in S_{2}\), then \((x_{0},A,\{G_{k}\}_{k=1}^{m})\) satisfies

\[\text{rank}([v|\mathcal{A}v|\ldots|\mathcal{A}^{(d^{2}+d-2)/2}v])<(d^{2}+d)/2\,,\]

recall that \(\mathcal{A}=A\oplus A+\sum_{k=1}^{m}G_{k}\otimes G_{k}\in\mathbb{R}^{d^{2} \times d^{2}}\) and \(v=\text{vec}(x_{0}x_{0}^{\top})\in\mathbb{R}^{d^{2}}\). According to the proof A.7 of Theorem 3.5, we obtain that the set of vectors \(\{v,\mathcal{A}v,\ldots,\mathcal{A}^{(d^{2}+d-2)/2}\}\) are linearly dependent. Because all of these vectors are transferred from vectorizing \(d\times d\) symmetric matrices, thus each of these vectors has only \((d^{2}+d)/2\) distinct elements and the repetitive elements appear in the same positions in all vectors. Hence, abuse notation a little bit, we can focus on checking those distinct elements in each vector, that is

\[\{\underline{v}|\mathcal{A}\underline{v}|\ldots|\mathcal{A}^{(d^{2}+d-2)/2} \underline{v}\}\,,\] (44)

where \(\underline{v}\in\mathbb{R}^{(d^{2}+d)/2}\) denotes the vector of deleting the repetitive elements of \(v\). Since the set of vectors \(\{v,\mathcal{A}v,\ldots,\mathcal{A}^{(d^{2}+d-2)/2}\}\) are linearly dependent, the set of vectors \(\{\underline{v}|\mathcal{A}\underline{v}|\ldots|\mathcal{A}^{(d^{2}+d-2)/2} \underline{v}\}\) are linearly dependent, that is

\[\text{det}([\underline{v}|\mathcal{A}\underline{v}|\ldots|\mathcal{A}^{(d^{2} +d-2)/2}\underline{v}])=0\,.\] (45)Each entry of \(v\) can be written as a non-zero polynomial function of entries of \(x_{0}\) since \(v=\operatorname{vec}(x_{0}x_{0}^{\top})\). Each entry of \(\mathcal{A}\) can be written as a non-zero polynomial function of entries of \(A\) and \(G_{k}\) with \(k=1,\ldots,m\), since \(\mathcal{A}=A\oplus A+\sum_{k=1}^{m}G_{k}\otimes G_{k}\in\mathbb{R}^{d^{2} \times d^{2}}\). Hence, the left side of Equation (45) can be expressed as some universal polynomial of the entries of \(x_{0}\), \(A\) and \(G_{k}\)'s, denotes

\[p(x_{0},A,\{G_{k}\}_{k=1}^{m})=p(x_{01},\ldots,x_{0d},a_{11},\ldots,a_{dd},G_{ 1,11},\ldots,G_{1,dd},\ldots,G_{m,11},\ldots,G_{m,dd})\,,\]

where \(G_{k,ij}\) denotes the \(ij\)-th entry of matrix \(G_{k}\). Therefore, one concludes that

\[p(x_{0},A,\{G_{k}\}_{k=1}^{m})=0\,.\]

Thus, \(S_{2}\) can be expressed as

\[S_{2}:=\{(x_{0},A,\{G_{k}\}_{k=1}^{m})\in\mathbb{R}^{d+(m+1)d^{2}}:p(x_{0},A,\{ G_{k}\}_{k=1}^{m})=0\}\,.\]

Similar to the calculation of \(p(x_{0},A)\) in Equation (43), \(p(x_{0},A,\{G_{k}\}_{k=1}^{m})\) can be expressed as a non-zero polynomial function of entries of \(v\) and \(\mathcal{A}\), thus it can also be expressed as a non-zero polynomial function of entries of \(x_{0}\), \(A\) and \(G_{k}\)'s. Therefore, by Lemma B.1, \(S_{2}\) has Lebesgue measure zero in \(\mathbb{R}^{d+(m+1)d^{2}}\).

We know that \(S\subseteq S_{1}\cup S_{2}\), let \(\lambda\) be Lebesgue measure on \(\mathbb{R}^{d+(m+1)d^{2}}\), then one has

\[\lambda(S)\leqslant\lambda(S_{1})+\lambda(S_{2})=0.\]

Thus \(S\) has Lebesgue measure zero in \(\mathbb{R}^{d+(m+1)d^{2}}\). 

### The identifiability condition stated in Theorem 3.4 is generic

We will show that the identifiability condition stated in Theorem 3.4 is generic.

**Proposition B.2**.: _Let_

\[S:=\{(x_{0},A,G)\in\mathbb{R}^{d+d^{2}+dm}:\text{condition \eqref{eq:def} in Theorem 3.4 is violated}\},\]

_then \(S\) has Lebesgue measure zero in \(\mathbb{R}^{d+d^{2}+dm}\)._

Proof.: Suppose \((x_{0},A,G)\in S\), then \((x_{0},A,G)\) satisfies

\[\text{rank}([x_{0}|Ax_{0}|\ldots|A^{d-1}x_{0}|H_{\cdot 1}|AH_{\cdot 1}|\ldots |A^{d-1}H_{\cdot 1}|\ldots|H_{\cdot d}|AH_{\cdot d}|\ldots|A^{d-1}H_{\cdot d}])<d\,,\]

recall that \(H:=GG^{T}\), and \(H_{\cdot j}\) stands for the \(j\)-th column vector of matrix \(H\), for all \(j=1,\cdots,d\).

Let

\[S^{\prime}:=\{(x_{0},A,G)\in\mathbb{R}^{d+d^{2}+dm}:\text{rank}([x_{0}|Ax_{0} |\ldots|A^{d-1}x_{0}])<d\},\]

one observes that \(S\subseteq S^{\prime}\). According to the proof of Proposition B.1, it is readily checked that \(S^{\prime}\) has Lebesgue measure zero in \(\mathbb{R}^{d+d^{2}+dm}\). Thus, \(S\) has Lebesgue measure zero in \(\mathbb{R}^{d+d^{2}+dm}\).

Simulation settings

We present the true underlying system parameters along with the initial states of the SDEs employed in the simulation experiments. We randomly generate the true system parameters that satisfy or violate the corresponding identifiability conditions.

For the SDE (1):

1. identifiable case: satisfy condition (14) stated in Theorem 3.4:

\[x_{0}^{\text{id}}=\begin{bmatrix}1.87\\ -0.98\end{bmatrix},\ \ A^{\text{id}}=\begin{bmatrix}1.76&-0.1\\ 0.98&0\end{bmatrix},\ \ G^{\text{id}}=\begin{bmatrix}-0.11&-0.14\\ -0.29&-0.22\end{bmatrix};\]

2. unidentifiable case: violate condition (14):

\[x_{0}^{\text{un}}=\begin{bmatrix}1\\ -1\end{bmatrix},\ \ \ A^{\text{un}}=\begin{bmatrix}1&2\\ 1&0\end{bmatrix},\ \ G^{\text{un}}=\begin{bmatrix}0.11&0.22\\ -0.11&-0.22\end{bmatrix}.\]

For the SDE (2):

1. identifiable case: satisfy both A1 and A2 stated in Theorem 3.5:

\[x_{0}^{\text{id}}=\begin{bmatrix}1.87\\ -0.98\end{bmatrix},\ \ \ A^{\text{id}}=\begin{bmatrix}1.76&-0.1\\ 0.98&0\end{bmatrix},\ \ G_{1}^{\text{id}}=\begin{bmatrix}-0.11&-0.14\\ -0.29&-0.22\end{bmatrix},\ \ G_{2}^{\text{id}}=\begin{bmatrix}-0.17&0.59\\ 0.81&0.18\end{bmatrix};\]

2. unidentifiable case1: violate A1 satisfy A2:

\[x_{0}^{\text{un-A1}}=\begin{bmatrix}1\\ 1\end{bmatrix},\ \ \ A^{\text{un-A1}}=\begin{bmatrix}2&1\\ 3&0\end{bmatrix},\ \ G_{1}^{\text{un-A1}}=\begin{bmatrix}-0.11&-0.14\\ -0.29&-0.22\end{bmatrix},\ \ G_{2}^{\text{un-A1}}=\begin{bmatrix}-0.17&0.59\\ 0.81&0.18\end{bmatrix};\]

3. unidentifiable case2: satisfy A1 violate A2:

\[x_{0}^{\text{un-A2}}=\begin{bmatrix}1\\ -1\end{bmatrix},\ \ \ A^{\text{un-A2}}=\begin{bmatrix}1&-2\\ -1&0\end{bmatrix},\ \ G_{1}^{\text{un-A2}}=\begin{bmatrix}-0.3&0.4\\ -0.7&0.2\end{bmatrix},\ \ G_{2}^{\text{un-A2}}=\begin{bmatrix}0.8&0.2\\ -0.2&-0.4\end{bmatrix}.\]

We have discussed in Section 4 that we use MLE method to estimate the system parameters from discrete observations sampled from the corresponding SDEs. Specifically, the negative log-likelihood function was minimized using the'scipy.optimize.minimize' library in Python.

For all of our experiments, we initialized the parameter values as the true parameters plus 2. In the case of the SDE (1), we utilized the 'trust-constr' method with the hyper-parameter 'gtol'\(=1\)e-3 and 'xtol'\(=1\)e-3. On the other hand, for the SDE (2), we applied the 'BFGS' method and set the hyper-parameter 'gtol'\(=1\)e-2. The selection of the optimization method and the corresponding hyper-parameters was determined through a series of experiments aimed at identifying the most suitable configuration.

Examples for reliable causal inference for linear SDEs

### Example for reliable causal inference for the SDE (1)

This example is inspired by [17, Example 5.4]. Recall that the SDE (1) is defined as

\[dX_{t}=AX_{t}dt+GdW_{t},\ \ X_{0}=x_{0},\]

where \(0\leqslant t<\infty\), \(A\in\mathbb{R}^{d\times d}\) and \(G\in\mathbb{R}^{d\times m}\) are constant matrices, \(W\) is an \(m\)-dimensional standard Brownian motion. Let \(X(t;x_{0},A,G)\) denote the solution to the SDE (1). Let \(\tilde{A}\in\mathbb{R}^{d\times d}\) and \(\tilde{G}\in\mathbb{R}^{d\times m}\) define the following SDE:

\[d\tilde{X}_{t}=\tilde{A}\tilde{X}_{t}dt+\tilde{G}dW_{t},\ \ \ \tilde{X}_{0}=x_{0},\]

such that

\[X(\cdot;x_{0},A,G)\stackrel{{\mathrm{d}}}{{=}}\tilde{X}(\cdot;x_ {0},\tilde{A},\tilde{G})\,.\]

Then under our proposed identifiability condition stated in Theorem 3.4, we have shown that the generator of the SDE (1) is identifiable, i.e., \((A,GG^{\top})=(\tilde{A},\tilde{G}\tilde{G}^{\top})\). Till now, we have shown that under our proposed identifiability conditions, the observational distribution \(\xrightarrow{\mathrm{identity}}\), the generator of the observational SDE. Then we will show that the post-intervention distribution is also identifiable. For notational simplicity, we consider intervention on the first coordinate, making the intervention \(X_{t}^{1}=\xi\) and \(\tilde{X}_{t}^{1}=\xi\) for \(0\leqslant t<\infty\). It will suffice to show equality of the distributions of the non-intervened coordinates (i.e., \(X_{t}^{(-1)}\) and \(\tilde{X}_{t}^{(-1)}\), note the superscripts do not denote reciprocals, but denote the \((d-1)\)-coordinates without the first coordinate). Express the matrices of \(A\) and \(G\) in blocks

\[A=\begin{bmatrix}A_{11}&A_{12}\\ A_{21}&A_{22}\end{bmatrix},\ \ \ G=\begin{bmatrix}G_{1}\\ G_{2}\end{bmatrix},\]

where \(A_{11}\in\mathbb{R}^{1\times 1}\), \(A_{22}\in\mathbb{R}^{(d-1)\times(d-1)}\), \(G_{1}\in\mathbb{R}^{1\times m}\) and \(G_{2}\in\mathbb{R}^{(d-1)\times m}\). Also, consider corresponding expressions of matrices \(\tilde{A}\) and \(\tilde{G}\). By making intervention \(X_{t}^{1}=\xi\), one obtains the post-intervention process of the first SDE satisfies:

\[dX_{t}^{(-1)}=(A_{21}\xi+A_{22}X_{t}^{(-1)})dt+G_{2}dW_{t}\,,\ \ \ X_{0}^{(-1)}=x_{0}^{(-1)}\,,\]

which is a multivariate Ornstein-Uhlenbeck process, according to [59, Corollary 1], this process is a Gaussian process, assuming \(A_{22}\) is invertible, then the mean vector can be described as

\[E[X_{t}^{(-1)}]=e^{A_{22}t}x_{0}^{(-1)}-(I-e^{A_{22}t})A_{22}^{-1}A_{21}\xi,\]

and based on [59, Theorem 2], the cross-covariance can be described as

\[V(X_{t+h}^{(-1)},X_{t}^{(-1)}): =\mathbb{E}\{(X_{t+h}^{(-1)}-\mathbb{E}[X_{t+h}^{(-1)}])(X_{t}^{( -1)}-\mathbb{E}[X_{t}^{(-1)}])^{\top}\}\] \[=\int_{0}^{t}e^{A_{22}(t+h-s)}G_{2}G_{2}^{\top}e^{A_{22}^{\top}(t -s)}ds\,.\]

Similarly, one can obtain that the mean vector and cross-covariance of the distribution of the post-intervention process of the second SDE by making intervention \(\tilde{X}_{t}^{1}=\xi\) satisfy:

\[E[\tilde{X}_{t}^{(-1)}]=e^{\tilde{A}_{22}t}x_{0}^{(-1)}-(I-e^{\tilde{A}_{22} t})\tilde{A}_{22}^{-1}\tilde{A}_{21}\xi,\]

and

\[V(\tilde{X}_{t+h}^{(-1)},\tilde{X}_{t}^{(-1)}): =\mathbb{E}\{(\tilde{X}_{t+h}^{(-1)}-\mathbb{E}[\tilde{X}_{t+h}^ {(-1)}])(\tilde{X}_{t}^{(-1)}-\mathbb{E}[\tilde{X}_{t}^{(-1)}])^{\top}\}\] \[=\int_{0}^{t}e^{\tilde{A}_{22}(t+h-s)}\tilde{G}_{2}\tilde{G}_{2} ^{\top}e^{A_{22}^{\top}(t-s)}ds\,.\]

Then we will show that \(E[X_{t}^{(-1)}]=E[\tilde{X}_{t}^{(-1)}]\), and \(V(X_{t+h}^{(-1)},X_{t}^{(-1)})=V(\tilde{X}_{t+h}^{(-1)},\tilde{X}_{t}^{(-1)})\) for all \(0\leqslant t,h<\infty\). Recall that we have shown \((A,GG^{\top})=(\tilde{A},\tilde{G}\tilde{G}^{\top})\), thus, \(A_{22}=\tilde{A}_{22}\) and \(A_{21}=\tilde{A}_{21}\), then it is readily checked that \(E[X_{t}^{(-1)}]=E[\tilde{X}_{t}^{(-1)}]\) for all \(0\leqslant t<\infty\).

Since

\[GG^{\top}=\begin{bmatrix}G_{1}G_{1}^{\top}&G_{1}G_{2}^{\top}\\ G_{2}G_{1}^{\top}&G_{2}G_{2}^{\top}\end{bmatrix}=\tilde{G}\tilde{G}^{\top},\]

thus, \(G_{2}G_{2}^{\top}=\tilde{G}_{2}\tilde{G}_{2}^{\top}\), then it is readily checked that \(V(X_{t+h}^{(-1)},X_{t}^{(-1)})=V(\tilde{X}_{t+h}^{(-1)},\tilde{X}_{t}^{(-1)})\) for all \(0\leqslant t,h<\infty\). Since both of these two post-intervention processes are Gaussian processes, according to Lemma 3.2, the distributions of these two post-intervention processes are the same. That is, the post-intervention distribution is identifiable.

### Example for reliable causal inference for the SDE (2)

Recall that the SDE (2) is defined as

\[dX_{t}=AX_{t}dt+\sum_{k=1}^{m}G_{k}X_{t}dW_{k,t},\ \ X_{0}=x_{0},\]

where \(0\leqslant t<\infty\), \(A,G_{k}\in\mathbb{R}^{d\times d}\) for \(k=1,\ldots,m\) are some constant matrices, \(W:=\{W_{t}=[W_{1,t},\ldots,W_{m,t}]^{\top}:0\leqslant t<\infty\}\) is an \(m\)-dimensional standard Brownian motion. Let \(X(t;x_{0},A,\{G_{k}\}_{k=1}^{m})\) denote the solution to the SDE (2). Let \(\tilde{A},\tilde{G}_{k}\in\mathbb{R}^{d\times d}\) for \(k=1,\ldots,m\) define the following SDE:

\[d\tilde{X}_{t}=\tilde{A}\tilde{X}_{t}dt+\sum_{k=1}^{m}\tilde{G}_{k}\tilde{X}_ {t}dW_{k,t},\ \ \tilde{X}_{0}=x_{0},\]

such that

\[X(\cdot;x_{0},A,\{G_{k}\}_{k=1}^{m})\stackrel{{\mathrm{d}}}{{=}} \tilde{X}(\cdot;x_{0},\tilde{A},\{\tilde{G}_{k}\}_{k=1}^{m})\,.\]

Then under our proposed identifiability condition stated in Theorem 3.5, we have shown that the generator of the SDE (2) is identifiable, i.e., \((A,\sum_{k=1}^{m}G_{k}xx^{\top}G_{k}^{\top})=(\tilde{A},\sum_{k=1}^{m}\tilde{ G}_{k}xx^{\top}\tilde{G}_{k}^{\top})\) for all \(x\in\mathbb{R}^{d}\). Till now, we have shown that under our proposed identifiability conditions, the observational distribution \(\xrightarrow{\text{identity}}\) the generator of the observational SDE. Then we aim to show that the post-intervention distribution is also identifiable. For notational simplicity, we consider intervention on the first coordinate, making the intervention \(X_{t}^{1}=\xi\) and \(\tilde{X}_{t}^{1}=\xi\) for \(0\leqslant t<\infty\). It will suffice to show equality of the distributions of the non-intervened coordinates (i.e., \(X_{t}^{(-1)}\) and \(\tilde{X}_{t}^{(-1)}\)). Express the matrices of \(A\) and \(G_{k}\) for \(k=1,\ldots,m\) in blocks

\[A=\begin{bmatrix}A_{11}&A_{12}\\ A_{21}&A_{22}\end{bmatrix},\ \ G_{k}=\begin{bmatrix}G_{k,11}&G_{k,12}\\ G_{k,21}&G_{k,22}\end{bmatrix},\]

where \(A_{11},G_{k,11}\in\mathbb{R}^{1\times 1}\), \(A_{22},G_{k,22}\in\mathbb{R}^{(d-1)\times(d-1)}\). Also consider corresponding expressions of matrices \(\tilde{A}\) and \(\tilde{G}_{k}\) for \(k=1,\ldots,m\). By making intervention \(X_{t}^{1}=\xi\), one obtains the post-intervention process of the first SDE satisfies:

\[dX_{t}^{(-1)}=(A_{21}\xi+A_{22}X_{t}^{(-1)})dt+\sum_{k=1}^{m}(G_{k,21}\xi+G_{ k,22}X_{t}^{(-1)})dW_{k,t},\ \ \ X_{0}^{(-1)}=x_{0}^{(-1)}.\]

Since this post-intervention process is not a Gaussian process, one cannot explicitly show that the post-intervention distribution is identifiable. Instead, we check the surrogate of the post-intervention distribution, that is the first- and second-order moments of the post-intervention process \(X_{t}^{(-1)}\). Which denote as \(m(t)^{(-1)}=\mathbb{E}[X_{t}^{(-1)}]\) and \(P(t)^{(-1)}=\mathbb{E}[X_{t}^{(-1)}(X_{t}^{(-1)})^{\top}]\) respectively. Then \(m(t)^{(-1)}\) and \(P(t)^{(-1)}\) satisfy the following ODE systems:

\[\frac{dm(t)^{(-1)}}{dt}=A_{21}\xi+A_{22}m(t)^{(-1)},\ \ \ m(0)^{-1}=x_{0}^{(-1)},\]

and

\[\begin{split}\frac{dP(t)^{(-1)}}{dt}&=m(t)^{(-1)}\xi^{ \top}A_{21}^{\top}+A_{21}\xi(m(t)^{(-1)})^{\top}+P(t)^{(-1)}A_{22}^{\top}+A_{2 2}P(t)^{(-1)}\\ &+\sum_{k=1}^{m}(G_{k,21}\xi\xi^{\top}G_{k,21}^{\top}+G_{k,22}m( t)^{(-1)}\xi^{\top}G_{k,21}^{\top}+G_{k,21}\xi(m(t)^{(-1)})^{\top}G_{k,22}^{ \top}\\ &+G_{k,22}P(t)^{(-1)}G_{k,22}^{\top}),\ \ P(0)^{(-1)}=x_{0}^{(-1)}(x_{0}^{(-1)})^{\top}.\end{split}\] (46)

Similarly, one can obtain the ODE systems describing the \(\tilde{m}(t)^{(-1)}\) and \(\tilde{P}(t)^{(-1)}\). Then we will show that \(m(t)^{(-1)}=\tilde{m}(t)^{(-1)}\) and \(P(t)^{(-1)}=\tilde{P}(t)^{(-1)}\) for all \(0\leqslant t<\infty\). Recall that we have shown \(A=\tilde{A}\), thus \(A_{21}=\tilde{A}_{21}\) and \(A_{22}=\tilde{A}_{22}\), then it is readily checked that \(m(t)^{(-1)}=\tilde{m}(t)^{(-1)}\) for all \(0\leqslant t<\infty\).

In the proof of Theorem 3.5, we have shown that

\[\sum_{k=1}^{m}G_{k}P(t)G_{k}^{\top}=\sum_{k=1}^{m}\tilde{G}_{k}P(t)\tilde{G}_{k }^{\top}\]

for all \(0\leqslant t<\infty\), where \(P(t)=\mathbb{E}[X_{t}X_{t}^{\top}]\). Simple calculation shows that

\[\sum_{k=1}^{m}G_{k}P(t)G_{k}^{\top}=\sum_{k=1}^{m}\Bigg{(}\begin{bmatrix}G_{k, 11}&G_{k,12}\\ G_{k,21}&G_{k,22}\end{bmatrix}\begin{bmatrix}\xi\xi^{\top}&\xi(m(t)^{(-1)})^{ \top}\\ m(t)^{(-1)}\xi^{\top}&P(t)^{(-1)}\end{bmatrix}\begin{bmatrix}G_{k,11}^{\top}&G _{k,21}^{\top}\\ G_{k,12}^{\top}&G_{k,22}^{\top}\end{bmatrix}\Bigg{)},\]

Then one can get that the \((2,2)\)-th block entry of the matrix \(\sum_{k=1}^{m}G_{k}P(t)G_{k}^{\top}\) is the same as the \(\sum_{k=1}^{m}(\ldots)\) part in the ODE corresponds to \(P(t)^{(-1)}\) (i.e., Equation (46)), since \(\sum_{k=1}^{m}G_{k}P(t)G_{k}^{\top}=\sum_{k=1}^{m}\tilde{G}_{k}P(t)\tilde{G}_{ k}^{\top}\), then the \(\sum_{k=1}^{m}(\ldots)\) part in the ODEs correspond to both \(P(t)^{(-1)}\) and \(\tilde{P}(t)^{(-1)}\) are the same. Thus, it is readily checked that \(P(t)^{(-1)}=\tilde{P}(t)^{(-1)}\) for all \(0\leqslant t<\infty\).

Though we cannot explicitly show that the post-intervention distribution is identifiable, showing that the first- and second-order moments of the post-intervention process is identifiable can indicate the identification of the post-intervention distribution to a considerable extent.

Conditions for identifying the generator of a linear SDE with multiplicative noise when its explicit solution is available

**Proposition E.1**.: _Let \(x_{0}\in\mathbb{R}^{d}\) be fixed. The generator of the SDE (2) is identifiable from \(x_{0}\) if the following conditions are satisfied:_

* \(\operatorname{rank}([x_{0}|Ax_{0}|\ldots|A^{d-1}x_{0}|H_{\cdot 1}|AH_{\cdot 1}| \ldots|A^{d-1}H_{\cdot 1}|\ldots|H_{\cdot d}|AH_{\cdot d}|\ldots|A^{d-1}H_{ \cdot d}])=d\)_,_
* \(\operatorname{rank}([v|\mathcal{A}v|\ldots|\mathcal{A}^{(d^{2}+d-2)/2}v])=(d^{2 }+d)/2\)_,_
* \(AG_{k}=G_{k}A\) _and_ \(G_{k}G_{l}=G_{l}G_{k}\) _for all_ \(k,l=1,\ldots,m\)_._

_where \(H:=\sum_{k=1}^{m}G_{k}x_{0}x_{0}^{\top}G_{k}^{\top}\), and \(H_{\cdot j}\) stands for the \(j\)-th column vector of matrix \(H\), for all \(j=1,\cdots,d\). And \(\mathcal{A}=A\oplus A+\sum_{k=1}^{m}G_{k}\otimes G_{k}\in\mathbb{R}^{d^{2} \times d^{2}}\), \(\oplus\) denotes Kronecker sum and \(\otimes\) denotes Kronecker product, \(v\) is a \(d^{2}\)-dimensional vector defined by \(v:=\operatorname{vec}(x_{0}x_{0}^{\top})\), where \(\operatorname{vec}(M)\) denotes the vectorization of matrix \(M\)._

Proof.: Let \(\tilde{A},\tilde{G}_{k}\in\mathbb{R}^{d\times d}\) and \(\tilde{A}\tilde{G}_{k}=\tilde{G}_{k}\tilde{A}\), \(\tilde{G}_{k}\tilde{G}_{l}=\tilde{G}_{l}\tilde{G}_{k}\) for all \(k,l=1,\ldots,m\), such that \(X(\cdot;x_{0},A,\{G_{k}\}_{k=1}^{m})\stackrel{{\text{d}}}{{=}}X( \cdot;x_{0},\tilde{A},\{\tilde{G}_{k}\}_{k=1}^{m})\), we denote as \(X\stackrel{{\text{d}}}{{=}}\tilde{X}\), we will show that under our identifiability condition, for all \(x\in\mathbb{R}^{d}\), \((A,\sum_{k=1}^{m}G_{k}xx^{\top}G_{k}^{\top})=(\tilde{A},\sum_{k=1}^{m}\tilde{ G}_{k}xx^{\top}\tilde{G}_{k}^{\top})\). By applying the same notations used in the proof of Theorem 3.5, in the following, we denote \(A_{1}:=A\), \(A_{2}:=\tilde{A}\), \(G_{1,k}:=G_{k}\) and \(G_{2,k}:=\tilde{G}_{k}\), and denote \(X\stackrel{{\text{d}}}{{=}}\tilde{X}\) as \(X^{1}\stackrel{{\text{d}}}{{=}}X^{2}\).

We first show that \(H_{1}=H_{2}\) (\(H_{i}:=\sum_{k=1}^{m}G_{i,k}x_{0}x_{0}^{\top}G_{i,k}^{\top}\)). Indeed, since \(X^{1},X^{2}\) have the same distribution, one has

\[\mathbb{E}[f(X_{t}^{1})]=\mathbb{E}[f(X_{t}^{2})]\] (47)

for all \(0\leqslant t<\infty\) and \(f\in C^{\infty}(\mathbb{R}^{d})\). By differentiating (47) at \(t=0\), one finds that

\[(\mathcal{L}_{1}f)(x_{0})=(\mathcal{L}_{2}f)(x_{0})\,,\] (48)

where \(\mathcal{L}_{i}\) is the generator of \(X^{i}\) (\(i=1,2\)). Based on the Proposition 2.1,

\[(\mathcal{L}_{i}f)(x_{0})=\sum_{k=1}^{d}\sum_{l=1}^{d}(A_{i})_{kl}x_{0l}\frac{ \partial f}{\partial x_{k}}(x_{0})+\frac{1}{2}\sum_{k,l=1}^{d}(H_{i})_{kl} \frac{\partial^{2}f}{\partial x_{k}\partial x_{l}}(x_{0})\,,\]

where \((M)_{kl}\) denotes the \(kl\)-entry of matrix \(M\), and \(x_{0l}\) is the \(l\)-th component of \(x_{0}\). Since (48) is true for all \(f\), by taking

\[f(x)=(x_{p}-x_{0p})(x_{q}-x_{0q})\,,\]

it is readily checked that

\[(H_{1})_{pq}=(H_{2})_{pq}\,,\]

for all \(p,q=1,\ldots,d\). As a result, \(H_{1}=H_{2}\). Let us call this matrix \(H\). That is

\[H:=H_{1}=\sum_{k=1}^{m}G_{1,k}x_{0}x_{0}^{\top}G_{1,k}^{\top}=\sum_{k=1}^{m}G_ {2,k}x_{0}x_{0}^{\top}G_{2,k}^{\top}=H_{2}\,.\]

In the proof of Theorem 3.5, we have shown that

\[A_{1}A^{j-1}x_{0}=A_{2}A^{j-1}x_{0}\ \ \text{for all }j=1,2,\ldots\,,\] (49)

next, we will derive the relationship between \(A_{i}\) and \(H\). Under condition C3, the SDE system (2) has an explicit solution (cf. [25]):

\[X_{t}:=X(t;x_{0},A,\{G_{k}\}_{k=1}^{m})=\text{exp}\Bigg{\{}\Bigg{(}A-\frac{1}{ 2}\sum_{k=1}^{m}G_{k}^{2}\Bigg{)}t+\sum_{k=1}^{m}G_{k}W_{k,t}\Bigg{\}}x_{0}\,,\] (50)then the covariance of \(X_{t}\), \(P(t,t+h)=\mathbb{E}[X_{t}X_{t+h}^{\top}]\) can be calculated as

\[\mathbb{E}[X_{t}X_{t+h}^{\top}]\] (51) \[= \,\mathbb{E}\Bigg{[}\text{exp}\Bigg{\{}\Bigg{(}A-\frac{1}{2}\sum_ {k=1}^{m}G_{k}^{2}\Bigg{)}t+\sum_{k=1}^{m}G_{k}W_{k,t}\Bigg{\}}x_{0}x_{0}^{\top }\text{exp}\Bigg{\{}\Bigg{(}A^{\top}-\frac{1}{2}\sum_{k=1}^{m}(G_{k}^{2})^{ \top}\Bigg{)}(t+h)\] \[+\sum_{k=1}^{m}G_{k}^{\top}W_{k,t+h}\Bigg{\}}\Bigg{]}\] \[= \,e^{At}e^{-\frac{1}{2}\sum_{k=1}^{m}G_{k}^{2}t\mathbb{E}\big{[}e ^{\sum_{k=1}^{m}G_{k}W_{k,t}}x_{0}x_{0}^{T}e^{\sum_{k=1}^{m}G_{k}^{\top}W_{k,t+ h}}\big{]}e^{-\frac{1}{2}\sum_{k=1}^{m}(G_{k}^{2})^{\top}(t+h)}e^{A^{\top}(t+h)}\,,\]

where

\[\mathbb{E}\big{[}e^{\sum_{k=1}^{m}G_{k}W_{k,t}}x_{0}x_{0}^{T}e^{ \sum_{k=1}^{m}G_{k}^{\top}W_{k,t+h}}\big{]}\] (52) \[= \,\mathbb{E}\big{[}e^{\sum_{k=1}^{m}G_{k}W_{k,t}}x_{0}x_{0}^{T}e ^{\sum_{k=1}^{m}G_{k}^{\top}W_{k,t}}e^{\sum_{k=1}^{m}G_{k}^{\top}W_{k,t}}e^{ \sum_{k=1}^{m}G_{k}^{\top}W_{k,t+h}}\big{]}\] \[= \,\mathbb{E}\big{[}e^{\sum_{k=1}^{m}G_{k}W_{k,t}}x_{0}x_{0}^{T}e ^{\sum_{k=1}^{m}G_{k}^{\top}W_{k,t}}e^{\sum_{k=1}^{m}G_{k}^{\top}(W_{k,t+h}-W_ {k,t})}\big{]}\] \[= \,\mathbb{E}\big{[}e^{\sum_{k=1}^{m}G_{k}W_{k,t}}x_{0}x_{0}^{T}e ^{\sum_{k=1}^{m}G_{k}^{\top}W_{k,t}}\big{]}\mathbb{E}\big{[}e^{\sum_{k=1}^{m}G _{k}^{\top}(W_{k,t+h}-W_{k,t})}\big{]}\,,\]

because the Brownian motion \(W_{k,t}\) has independent increments.

It is known that, for \(Z\sim\mathcal{N}(0,1)\), we have that the \(j\)th moment is

\[\mathbb{E}(Z^{j})=\left\{\begin{array}{cc}0\,,&\text{$j$ is odd}\,,\\ 2^{-j/2}\frac{j!}{(j/2)!}\,,&\text{$j$ is even}\,.\end{array}\right.\]

Since \(W_{k,t}\sim\mathcal{N}(0,t)\), we have

\[\mathbb{E}[e^{G_{k}W_{k,t}}] =\mathbb{E}\Bigg{[}\sum_{j=0}^{\infty}\frac{(G_{k})^{j}(W_{k,t})^ {j}}{j!}\Bigg{]}\] \[=\sum_{j=0}^{\infty}\frac{(G_{k})^{j}\mathbb{E}[(W_{k,t})^{j}]}{j!}\] \[=\sum_{j=0,2,4\ldots}^{\infty}\frac{(G_{k})^{j}(t/2)^{j/2}}{(j/2)!}\] \[=\sum_{i=0}^{\infty}\frac{(G_{k}^{2}t/2)^{i}}{i!}\] \[=e^{G_{k}^{2}t/2}\,.\]

Similarly, we have

\[\mathbb{E}[e^{G_{k}^{\top}(W_{k,t+h}-W_{k,t})}]=e^{(G_{k}^{\top})^{2}h/2}\,.\]

Simple calculation shows that

\[\mathbb{E}\big{[}e^{\sum_{k=1}^{m}G_{k}^{\top}(W_{k,t+h}-W_{k,t})}\big{]}=e^{ \sum_{k=1}^{m}(G_{k}^{\top})^{2}h/2}\,.\] (53)

By combining Equations (51), (52) and (53), one readily obtains that

\[P(t,t+h)=P(t,t)e^{A^{\top}h}\,,\] (54)

we denote \(P(t):=P(t,t)\). Set \(P_{i}(t,t+h)=\mathbb{E}[X_{t}^{i}(X_{t+h}^{i})^{\top}]\), since \(X^{1}\stackrel{{\text{d}}}{{=}}X^{2}\), it follows that

\[P_{1}(t,t+h)=\mathbb{E}[X_{t}^{1}(X_{t+h}^{1})^{\top}]=\mathbb{E}[X_{t}^{2}(X_{ t+h}^{2})^{\top}]=P_{2}(t,t+h)\ \ \forall t,h\geqslant 0\,.\]

To obtain information about \(A\), let us fix \(t\) for now and take \(j\)-th derivative of (54) with respect to \(h\). One finds that

\[\frac{d^{j}}{dh^{j}}\bigg{|}_{h=0}P(t,t+h)=P(t)(A^{\top})^{j}\,,\] (55)for all \(j=1,2,\ldots\). It is readily checked that

\[P_{1}(t)(A_{1}^{\top})^{j}=P_{2}(t)(A_{2}^{\top})^{j}\ \ \ \forall 0\leqslant t< \infty\,.\] (56)

We know the function \(P_{i}(t)\) satisfies the ODE

\[\dot{P}_{i}(t)=A_{i}P_{i}(t)+P_{i}(t)A_{i}^{\top}+\sum_{k=1}^{m}G_{i,k}P_{i}(t )G_{i,k}^{\top}\,,\ \ \forall 0\leqslant t<\infty\,,\] (57)

\[P_{i}(0)=x_{0}x_{0}^{\top}\,.\]

In particular,

\[\dot{P}_{i}(0)=A_{i}x_{0}x_{0}^{\top}+x_{0}x_{0}^{\top}A_{i}^{\top}+\sum_{k=1} ^{m}G_{i,k}x_{0}x_{0}^{\top}G_{i,k}^{\top}\,.\]

By differentiating (56) with respect to \(t\) at \(t=0\), it follows that

\[A_{1}x_{0}x_{0}^{\top}(A_{1}^{\top})^{j}+x_{0}x_{0}^{\top}(A_{1} ^{\top})^{j+1}+\bigg{(}\sum_{k=1}^{m}G_{1,k}x_{0}x_{0}^{\top}G_{1,k}^{\top} \bigg{)}(A_{1}^{\top})^{j}\] \[= A_{2}x_{0}x_{0}^{\top}(A_{2}^{\top})^{j}+x_{0}x_{0}^{\top}(A_{2} ^{\top})^{j+1}+\bigg{(}\sum_{k=1}^{m}G_{2,k}x_{0}x_{0}^{\top}G_{2,k}^{\top} \bigg{)}(A_{2}^{\top})^{j}\,.\]

Since we have known that \(A_{1}^{j}x_{0}=A_{2}^{j}x_{0}\) for all \(j=1,2,\ldots\), it is readily checked that

\[\bigg{(}\sum_{k=1}^{m}G_{1,k}x_{0}x_{0}^{\top}G_{1,k}^{\top}\bigg{)}(A_{1}^{ \top})^{j}=\bigg{(}\sum_{k=1}^{m}G_{2,k}x_{0}x_{0}^{\top}G_{2,k}^{\top}\bigg{)} (A_{2}^{\top})^{j}\,,\]

that is \(A_{1}^{j}H=A_{2}^{j}H\) for all \(j=1,2,\ldots\). Let us denote this matrix \(A^{j}H\). Obviously, by rearranging this matrix, one gets

\[A_{1}A^{j-1}H=A_{2}A^{j-1}H\quad\text{for all }j=1,2,\ldots\]

Therefore, under condition C1, that is \(\text{rank}(M)=d\) with

\[M:=[x_{0}|Ax_{0}|\ldots|A^{d-1}x_{0}|H_{\cdot 1}|\ldots|A^{d-1}H_{\cdot 1}| \ldots|H_{\cdot d}|AH_{\cdot d}|\ldots|A^{d-1}H_{\cdot d}]\,.\] (58)

if we denote the \(j\)-th column in \(M\) as \(M_{\cdot j}\), one gets \(A_{1}M_{\cdot j}=A_{2}M_{\cdot j}\) for all \(j=1,\ldots,d+d^{2}\) by equations (49) and (58).

This means one can find a full-rank matrix \(B\in\mathbb{R}^{d\times d}\) by horizontally stacking \(d\) linearly independent columns from matrix \(M\), such that \(A_{1}B=A_{2}B\). Since \(B\) is invertible, one thus concludes that \(A_{1}=A_{2}\).

In the proof of Theorem 3.5, we have shown that when \(A_{1}=A_{2}\), under condition C2, for all \(x\in\mathbb{R}^{d}\),

\[\sum_{k=1}^{m}G_{1,k}xx^{\top}G_{1,k}^{\top}=\sum_{k=1}^{m}G_{2,k}xx^{\top}G_ {2,k}^{\top}\,.\]

Thus the proposition is proved. 

It is noteworthy that Proposition E.1 is established on the explicit solution assumption of the SDE (2), which requires both sets of vectors \(\{A,\{G_{k}\}_{k=1}^{m}\}\) and \(\{\tilde{A},\{\tilde{G}_{k}\}_{k=1}^{m}\}\) to satisfy condition C3. As aforementioned, condition C3 is very restrictive and impractical, rendering the identifiability condition derived in this proposition unsatisfactory. Nonetheless, this condition is presented to illustrate that condition C1 is more relaxed compared to condition A1 stated in Theorem 3.5 when identifying \(A\) with the incorporation of \(G_{k}\)'s information.