# Approximate Allocation Matching for Structural Causal Bandits with Unobserved Confounders

 Lai Wei

Life Sciences Institute

University of Michigan

weilatim@gmail.com

&Muhammad Qasim Elahi

Electrical and Computer Engineering

Purdue University

elahi0@purdue.edu

&Mahsa Ghasemi

Electrical and Computer Engineering

Purdue University

mahsa@purdue.edu

&Murat Kocaoglu

Electrical and Computer Engineering

Purdue University

mkocaoglu@purdue.edu

###### Abstract

Structural causal bandit provides a framework for decision-making problems when causal information is available. It models the stochastic environment with a structural causal model (SCM) that governs the causal relations between random variables. In each round, an agent applies an intervention (or no intervention) by setting certain variables to some constants, and receives a stochastic reward from a non-manipulable variable. Though the causal structure is given, the observational and interventional distributions of these random variables are unknown beforehand, and they can only be learned through interactions with the environment. Therefore, to maximize the expected cumulative reward, it is critical to balance the exploration-versus-exploitation tradeoff. We consider discrete random variables with a finite domain and a semi-Markovian setting, where random variables are affected by unobserved confounders. Using the canonical SCM formulation to discretize the domains of unobserved variables, we efficiently integrate samples to reduce model uncertainty, gaining an advantage over those in a classical multi-armed bandit setup. We provide a logarithmic asymptotic regret lower bound for the structural causal bandit problem. Inspired by the lower bound, we design an algorithm that can utilize the causal structure to accelerate the learning process and take informative and rewarding interventions. We establish that our algorithm achieves a logarithmic regret and demonstrate that it outperforms the existing methods via simulations.

## 1 Introduction

Sequential decision-making in uncertain environments is one of the most fundamental problems across scientific disciplines, including robotics, economics, social science, clinical trials, and agriculture [32; 26; 36; 7; 3]. In almost all these applications, it is required to implement interventions to control some aspects of the system to optimize an outcome of interest. In economics and political science, the government could use fiscal policy including changing the interest rate and taxation to achieve regulation and control of a country's economy. In this paper, we deal with designing an adaptive policy in an uncertain stochastic environment to learn and apply optimal interventions.

In sequential decision-making problems, it is critical to balance the explore-versus-exploit tradeoff, which deals with choosing between informative options and empirically more rewarding alternative options. The multi-armed bandit (MAB) [33; 30; 18] is a classical problem formulation for achieving such a tradeoff. In MAB, a decision-maker sequentially chooses an action corresponding to pullingan arm from a set of arms, each generating stochastic rewards with probability distribution unknown to the decision-maker. The objective is to achieve the maximum cumulative reward over a time horizon, or equivalently to minimize the regret, which measures suboptimality in cumulative rewards against selecting the best action all the time. The classic stochastic bandit formulation only considers the connection between an action and its expected reward. However, in real-world applications, an action can affect the entire system in a more complex way. As the agricultural example given in [19], changing the moisture level could affect the temperature and nutrient absorption, which are also crucial to crop yields. The relations between such factors cannot be characterized by the MAB.

Causal knowledge enables us to model the causal relations between multiple factors in the environment. In machine learning research, it has been noted that causal models could help improve the generalization performance when data distribution shifts [11], and address unstable translation of successful reinforcement learning methods in simulation to real-world problems [12] such as self-driving cars and recommendation systems. The casual bandit models the stochastic environment with a causal graph that connects a set of random variables of interest, and actions are modeled by interventions on different subsets of nodes. It provides a critical interpretation of causal influences among random variables [28] that allows us to reason about what will happen to the environment after certain interventions are made to the data-generating process. Compared to the classic MAB with only reward feedback, the casual bandit represents a more detailed model of real-world decision-making problems, including richer feedback from multiple random variables and the causal relations among them. By applying causal reasoning, the learning process can be accelerated and more rewarding interventions can be selected. In applications where the space of intervention is large and explicit experiments are costly, such as social science or economics, it has special value.

Due to the reasons mentioned above, there has been a recent surge of interest in the research community on causal decision-making. Some early attempts include [6], in which the authors took a causal approach to study how to make final decisions based on natural predictions, and proposed an algorithm by modifying Thompson sampling [33]. In [19], the investigators started from a parallel causal structure and extended the algorithm to the general causal bandit problem. Subsequent work [37] took preprocessing steps to get rough estimates of interventional distributions and use them to design efficient experiments to discover the best intervention. In [1], the Bayesian optimization technique is employed to solve the same problem.

The most closely related works focusing on regret minimization in the structural causal bandit with confounders are [22, 23]. In both works, the authors took a graphical characterization using do-calculus [28] to reduce the large intervention set to the possibly optimal minimal intervention set and run a KL-UCB algorithm [14] within the reduced action space. The later work is an extension of the previous one that includes non-manipulable variables. Other lines of causal bandit research normally assume additional prior information, such as infinite observational data [39] or marginal interventional distributions [24, 8]. A general linear model is adopted in [13], which is different from the non-parametric setup in this paper.

In this paper, we focus on the MAB sequential decision-making problem where the arms correspond to interventions on an arbitrary causal graph, including unobserved confounders affecting multiple pairs of variables (also known in causal terminology as the semi-Markovian setting). We recognize the structural causal bandit falls into the category of structured bandit [10, 35]. It has been noted the classical upper confidence bound (UCB) algorithm and Thompson sampling may not fully leverage structural information [31, 20]. It is worth considering the potential limitations of algorithms based on these ideas, such as CRM-AL [25] and generalized Thompson sampling [27]. To address this issue and make systematic use of a known causal structure, we design an algorithm that can meticulously utilize causal structural information. In particular, we make the following contributions:

* We provide a logarithmic asymptotic regret lower bound for the structural causal bandit problem with latent confounders by leveraging the canonical SCM formulation [40] to create a space of all possible interventional distributions given a causal graph.
* Inspired by the lower bound, we extend [22] and apply approximate allocation matching to design an algorithm that can effectively utilize causal information provided by the causal graph to accelerate the learning process and take informative and rewarding interventions.
* We analyze the algorithm to provide a problem-dependent logarithmic upper bound on expected regret and compare it with [22] to show it has a better theoretical guarantee.

* We complement the theoretical results by evaluating the proposed approach in a variety of experimental settings featuring different causal structures. We show that our algorithm outperforms the existing baselines in terms of empirical regret.

## 2 Problem Statement and Background

In this section, we present the formulation of the structural causal bandit problem with latent confounders. We follow the convention to represent a random variable and its value in a capital letter and a lowercase letter respectively. A multivariate random variable is represented in a bold letter.

### Structural Causal Model

Our approach adopts the structural causal model (SCM) [29] to provide a causal perspective on the data-generating process. This allows us to express the relationship between variables and capture important causal concepts, including unobserved variables, observational distributions, and interventional distributions. For a random variable \(\mathbf{X}\), let \(\Omega(\mathbf{X})\) denote its domain.

**Definition 1** (Scm).: _A structural causal model is a \(4\)-tuple \(\langle\mathbf{U},\mathbf{V},\mathbb{F},P(\mathbf{U})\rangle\) where:_

1. \(\mathbf{U}\) _is a set of unobserved independent background variables (also called exogenous), that determine the randomness of the model._
2. \(\mathbf{V}:=\{V_{i}\mid i=1,\ldots,n\}\) _is the set of observable variables (also called endogenous)._
3. \(\mathbb{F}\) _is a set of structural equations_ \(\{f_{1},f_{2},...,f_{n}\}\) _such that for each_ \(V_{i}\in\mathbf{V}\)_,_ \(f_{i}:\Omega(\mathbf{U}_{i})\times\Omega(\mathsf{Pa}_{i})\rightarrow\Omega(V_ {i})\) _defines a mapping, where_ \(\mathbf{U}_{i}\subseteq\mathbf{U}\) _and_ \(\mathsf{Pa}_{i}\subseteq\mathbf{V}\) _is the parent set of_ \(V_{i}\)_. So_ \(\mathbb{F}\) _as a whole determines a mapping relationship from_ \(\Omega(\mathbf{U})\) _to_ \(\Omega(\mathbf{V})\)_._
4. \(P(\mathbf{U})\) _is a joint probability distribution over all the exogenous variables._

Each structural causal model is associated with a causal graph \(\mathcal{G}=\langle\mathbf{V},\mathcal{E},\mathcal{B}\rangle\), where the node set \(\mathbf{V}\) corresponds to the set of observable variables, \(\mathcal{E}\) is the set of directed edges and \(\mathcal{B}\) is the set of bidirected edges. For each \(V_{j}\in\mathsf{Pa}_{i}\), there exists a directed edge in \(\mathcal{E}\) such that \(V_{j}\to V_{i}\) which indicates functional dependency from \(V_{j}\) to \(V_{i}\). Without showing unobserved exogenous variable \(\mathbf{U}\) explicitly, the confounding effects of \(\mathbf{U}\) are represented using bidirected edges in \(\mathcal{B}\), which connect multiple pairs of observable variables in \(\mathbf{V}\). The presence of \(V_{i}\leftrightarrow V_{j}\) in \(\mathcal{G}\) represents unmeasured factors (or confounders) that may influence both \(V_{i}\) and \(V_{j}\). In other words, \(f_{i}\) and \(f_{j}\) share common exogenous variables \(\mathbf{U}_{i}\cap\mathbf{U}_{j}\neq\emptyset\) as input. Let \(|\Omega(\mathbf{V})|\) be the cardinality of \(\Omega(\mathbf{V})\).

**Assumption 1**.: _We assume each \(V\in\mathbf{V}\) can only take a finite number of values, i.e., \(|\Omega(\mathbf{V})|\) is finite._

Within the causal graph, an intervention on a subset of random variables \(\mathbf{S}\subseteq\mathbf{V}\) denoted by the do-operator \(do(\mathbf{S}=\mathbf{s})\) sets the structural equation for each \(S_{j}\in\mathbf{S}\) to be \(S_{j}=s_{j}\). We also refer to \(\mathbf{s}\) as intervention for brevity. If a node \(V_{i}\notin\mathbf{S}\), its structural equation remains to be \(V_{i}=f_{i}(\mathbf{U}_{i},\mathsf{Pa}_{i})\). The empty intervention denoted by \(do(\emptyset)\) does not change the structural equation of any random variable, and the distribution of \(\mathbf{V}\) is also called observational distribution. Let \(1\) denote the indicator function. An SCM induces interventional distributions: if \(\mathbf{v}\) is consistent with intervention \(\mathbf{s}\),

\[P(\mathbf{v}\mid do(\mathbf{S}=\mathbf{s}))=\sum_{\mathbf{u}}P(\mathbf{u}) \prod_{V_{i}\notin\mathbf{S}}1\big{\{}v_{i}=f_{i}(\mathsf{pa}_{i},\mathbf{u}_ {i})\big{\}}:=P_{\mathbf{s}}(\mathbf{v}).\] (1)

Otherwise, \(P(\mathbf{v}\mid do(\mathbf{S}=\mathbf{s}))=0\). For a more detailed discussion on SCMs, we refer to [29].

### Causal Bandit Problem with Confounders

The causal bandit problem with confounders studies the sequential decision-making problem with causal information provided by a causal graph \(\mathcal{G}=\langle\mathbf{V},\mathcal{E},\mathcal{B}\rangle\) with confounding effects represented by bidirected edge set \(\mathcal{B}\). The causal graph \(\mathcal{G}\) is given but the exact interventional and observational distributions are unknown. Without loss of generality, we assume the reward is collected from node \(V_{n}\), which is bounded in \([0,1]\) and can not be intervened on. Let \(\mathcal{I}\subseteq\{\mathbf{s}\in\Omega(\mathbf{S})\mid\mathbf{S}\subseteq \mathbf{V}\setminus V_{n}\}\) be the space of allowed interventions. The decision-maker is required to decide which intervention to apply at each time step. The expected reward of intervention \(\mathbf{s}\in\mathcal{I}\) is \(\mu_{\mathbf{s}}=\mathbb{E}_{P_{\mathbf{s}}}[V_{n}]=\sum_{\mathbf{v}\in\Omega( \mathbf{V})}v_{n}P_{\mathbf{s}}(\mathbf{v})\), where \(P_{\mathbf{s}}\) is defined in (1) and \(\mathbb{E}_{P}\) means the expectation is computed with probability distribution \(P\).

At each time \(t\in\{1,\ldots,T\}\), the agent follows an adaptive allocation policy \(\pi\) to select an intervention \(\mathbf{s}_{t}\in\mathcal{I}\) and observe the causal bandit feedback \(\mathbf{V}_{t}\sim P_{\mathbf{s}_{t}}(\mathbf{V})\), which is a realization of all observable variables after intervention. In fact, policy \(\pi\) is a sequence \(\{\pi_{t}\}_{t\in\mathbb{N}}\), where each \(\pi_{t}\) determines the probability distribution of taking intervention \(\mathbf{S}_{t}\in\mathcal{I}\) given intervention and observation history \(\pi_{t}(\mathbf{S}_{t}\mid\mathbf{s}_{1},\mathbf{v}_{1},\ldots,\mathbf{s}_{t -1},\mathbf{v}_{t-1})\). Let \(\mu_{*}:=\max_{\mathbf{s}\in\mathcal{I}}\mu_{\mathbf{s}}\) and \(\Delta_{\mathbf{s}}=\mu_{*}-\mu_{\mathbf{s}}\) denote the optimal mean reward and the expected optimality gap of intervention \(do(\mathbf{S}=\mathbf{s})\) respectively. Given a causal graph \(\mathcal{G}\), the objective of the causal bandit problem is to design a policy \(\pi\) to maximize the expected cumulative reward, or equivalently, to minimize the _expected regret_:

\[R_{T}^{\pi}:=\mathbb{E}\bigg{[}\sum_{t=1}^{T}\left(\mu_{*}-V_{n,t}\right) \bigg{]}=\sum_{\mathbf{s}\in\mathcal{I}}\mathbb{E}[N_{T}(\mathbf{s})]\Delta_{ \mathbf{s}},\] (2)

where \(N_{T}(\mathbf{s})\) is the total number of times intervention \(do(\mathbf{S}=\mathbf{s})\) is taken by policy \(\pi\) until time horizon till \(T\), and the expectation is computed over different realizations of \(\{\mathbf{S}_{t},\mathbf{V}_{t}\}_{t=1}^{T}\) from the interactions between the random policy \(\pi\) and the causal bandit model \(\langle P_{\mathbf{s}}\rangle_{\mathbf{s}\in\mathcal{I}}\). Thus, \(R_{T}^{\pi}\) is the gap between the expected cumulative rewards of selecting the best intervention all the time and that by the policy \(\pi\). The regret decomposition in (2) is from [18], and it expresses the expected regret with products of the expected number of suboptimal action selections and expected reward gaps.

The large action space in causal bandits poses many challenges to the learning problem. It is desirable to reduce the action space without excluding all optimal interventions. The do-calculus introduced in [28] provides a set of guidelines for evaluating invariances across interventions. In this context, our specific focus is on Rule 3, which provides the conditions under which a series of interventions have no impact on the outcome variable. For example, Rule 3 implies that \(P_{\mathbf{W},\mathcal{I}}(y)=P_{\mathbf{Z}}(y)\) if we have \((\mathbf{Y}\perp\!\!\!\perp\mathbf{W}|\mathcal{Z})_{\mathcal{G}_{\mathbf{W} \cup\mathcal{Z}}}\), where \(\mathcal{G}_{\mathbf{W}\cup\mathcal{Z}}\) is the subgraph of DAG \(\mathcal{G}\) with incoming edges to the set \(\mathbf{W}\cup\mathcal{Z}\) removed. This concept leads to the notion of a minimal intervention set (MIS), which is a subset of variables \(\mathbf{S}\subseteq\mathbf{V}\setminus\mathbf{V}_{n}\) such that there is no \(\mathbf{S}^{\prime}\subset\mathbf{S}\) for which \(\mu_{\mathbf{s}[\mathbf{S}^{\prime}]}=\mu_{\mathbf{s}}\) holds for every Structural Causal Model (SCM) with a causal graph \(\mathcal{G}\)[22]. Here, \(\mathbf{s}[\mathbf{S}^{\prime}]\) denotes an intervention on \(\mathbf{S}^{\prime}\cap\mathbf{S}\) with values consistent with \(\mathbf{s}\). An MIS \(\mathbf{S}\) is considered a possibly optimal MIS (POMIS) if some intervention \(\mathbf{s}\in\Omega(\mathbf{S})\) can achieve the optimal mean reward in an SCM with a causal graph \(\mathcal{G}\). We define \(\mathcal{I}=\{\mathbf{s}\in\Omega(\mathbf{S})\mid\mathbf{S}\text{ is a POMIS}\}\).

At an intuitive level, it may seem logical that the most effective course of action would be to intervene on the immediate causes (parents) of the reward variable \(V_{n}\). This approach would provide a higher level of control over \(V_{n}\) within the system. If the reward variable \(V_{n}\) is not confounded with any of its ancestors, its parent set \(\mathsf{Pa}_{n}\) is the only POMIS. In more general causal graphs where \(V_{n}\) is confounded with any of its ancestors, the paper [22] proves that multiple POMISs exist, which can include variables that are not parents of \(V_{n}\). The paper also provides graphical criteria and an efficient algorithm for constructing a set of all POMISs for a given causal graph. For an effective method to search for \(\mathcal{I}\), we refer to [22] and rely on their graphical criteria to construct a set of POMISs. This set of POMISs is used to form the set of possible optimal actions \(\mathcal{I}\) for use in the causal bandit algorithm. We make the following assumption, which we expect to hold in a general scenario [38], except for certain specifically designed SCMs.

**Assumption 2**.: _Within interventions on POMIS, there exists a unique optimal intervention \(\mathbf{s}_{*}\)._

## 3 Response Variables and Space of Interventional Distribution Tuples

The unspecified domain of \(\mathbf{U}\) makes it inconvenient to be directly applied to the learning problem. However, it is noted in [29, Ch. 8] that if each variable in \(\mathbf{V}\) takes finite states, \(\Omega(\mathbf{U})\) can be partitioned and \(\mathbf{U}\) can be projected to a collection of finite-state response variables. The resulting model is equivalent to the original one with respect to all observational and interventional distributions. Such a technique was used to bound the causal and counterfactual effects with observational distribution [29, Ch. 8]. We take it to define a parameterized space of interventional distribution tuples.

### Response Variables and Canonical Structural Causal Model

In Definition 1, each structural equation \(V_{i}=f_{i}(\mathbf{U}_{i},\mathsf{Pa}_{i})\) connects random variable \(V_{i}\) to its parents \(\mathsf{Pa}_{i}\). As \(\mathbf{U}_{i}\) varies along its domain, regardless of how complex the variation is, the only effect it can have is to switch the relationship between \(\mathsf{Pa}_{i}\) and \(V_{i}\). Since there are at most \(|\Omega(V_{i})|^{|\Omega(\mathsf{Pa}_{i})|}\) mapping relationships from \(\mathsf{Pa}_{i}\) to \(V_{i}\), we can decompose \(f_{i}\) as follows,

\[V_{i}=f_{i}(\mathbf{U}_{i},\mathsf{Pa}_{i})=\underline{f}_{i}(M_{i},\mathsf{ Pa}_{i}),\ M_{i}=\overline{f}_{i}(\mathbf{U}_{i}),\] (3)

where \(M_{i}\in\{0,\ldots,|\Omega(V_{i})|^{|\Omega(\mathsf{Pa}_{i})|}-1\}\) is a random variable corresponds to a mapping from \(\Omega(\mathsf{Pa}_{i})\) to \(\Omega(V_{i})\). For an observable variable without parents, we simply have \(M_{i}\in\{0,\ldots,|\Omega(V_{i})|-1\}\). Such a random variable \(M_{i}\in\mathbf{M}\) attached to each \(V_{i}\in\mathbf{V}\) is called response variable in [5]. Treating response variables as exogenous variables, the resulting SCM \(\langle\mathbf{M},\mathbf{V},\underline{\mathbb{E}},P(\mathbf{M})\rangle\) is called _canonical SCM_ in [40], where \(\underline{\mathbb{E}}=\{\underline{f}_{1},\underline{f}_{2},...,\underline{f }_{n}\}\).

The decomposition of structural equations in equation (3) can be represented graphically by including response variables in \(\mathbf{M}\). Since \(V_{i}=\underline{f}_{i}(M_{i},\mathsf{Pa}_{i})\), for each \(V_{i}\) there is a directed edge \(M_{i}\to V_{i}\). We also know that if there exists \(V_{i}\leftrightarrow\overline{V}_{i}\) in \(\mathcal{G}\), both \(V_{i}\) and \(V_{j}\) are affected by common exogenous variables \(\mathbf{U}_{i}\cap\mathbf{U}_{j}\neq\emptyset\). As a result, \(M_{i}\) and \(M_{j}\) are correlated and they are also connected by a bidirected edge. For example, the causal graph in Fig. 1(a) has a structural equivalent in Fig. 1(b).

Given intervention \(\mathbf{s}\in\mathcal{I}\), there exists a deterministic relationship from \(\mathbf{M}\) to \(\mathbf{V}\). To further explain the idea, consider the bow graph example in Fig. 1(c), where \(V_{1},V_{2}\) are binary variables taking value \(0\) or \(1\). As illustrated above, the response variables are \(M_{1}\in\{0,1\}\) and \(M_{2}\in\{0,1,2,3\}\), and the structural equations for \(V_{1}\) and \(V_{2}\) are as follows:

\[V_{1}=\underline{f}_{1}(M_{1})=M_{1},\ V_{2}=\underline{f}_{2}(M_{2},V_{1})= \begin{cases}0&M_{2}=0\\ V_{1}&M_{2}=1\\ 1-V_{1}&M_{2}=2\\ 1&M_{2}=3\end{cases}.\]

If \(do(\emptyset)\) is applied, \((V_{1},V_{2})=(0,1)\) when \((M_{1},M_{2})=(0,2)\) or \((0,3)\). If \(do(V_{1}=1)\) is applied, \((V_{1},V_{2})=(1,0)\) when \(M_{1}=0\) or \(1\) and \(M_{2}=0\) or \(2\). It shows that for a given intervention \(do(\emptyset)\) or \(do(V_{1}=1)\), there exists a deterministic mapping from \(\mathbf{M}\) to \(\mathbf{V}\). Besides, the true value of \(\mathbf{M}\) can only be inferred from \(\mathbf{V}\) since \(\mathbf{M}\) is unobservable. Since \(M_{1}\) and \(M_{2}\) are correlated, we define their joint distribution as \(P(M_{1}=i,M_{2}=j)=p_{ij}\). We can further express the observational and interventional probabilities of \(\mathbf{V}\) with the probability of \(\mathbf{M}\) as follows,

\[P(V_{1}=0,V_{2}=0) =p_{00}+p_{01}, P(V_{1}=0,V_{2}=1) =p_{02}+p_{03},\] (4) \[P(V_{1}=1,V_{2}=0) =p_{10}+p_{12}, P(V_{1}=1,V_{2}=1) =p_{11}+p_{13},\] \[P_{do(V_{1}=0)}(V_{2}=0) =p_{00}+p_{01}+p_{10}+p_{11},\ P_{do(V_{1}=0)}(V_{2}=1) =p_{02}+p_{03}+p_{12}+p_{13},\] \[P_{do(V_{1}=1)}(V_{2}=0) =p_{00}+p_{02}+p_{10}+p_{12},\ P_{do(V_{1}=1)}(V_{2}=1) =p_{01}+p_{03}+p_{11}+p_{13}.\]

It can be seen these parameters are sufficient for specifying the model of Fig. 1(c). Such a parameterization technique can be extended to general causal graphs with latent confounders [40].

Figure 1: Causal graphs and their structural equivalents

### Space of Interventional Distribution Tuples

In a causal graph, a confounded component (c-component) is a maximal set of vertices connected with bidirected edges [34]. Note that a singleton node without bidirected edges is also a c-component. For example, there are two c-components in Fig. 1(a): \(\{V_{1},V_{3},V_{4}\}\) and \(\{V_{2}\}\). In fact for any causal graph \(\mathcal{G}\), its observable variables \(\mathbf{V}\) can be uniquely partitioned into c-components \(\{\mathbf{V}^{1},\ldots,\mathbf{V}^{n_{\mathrm{c}}(\mathcal{G})}\}\), where \(n_{\mathrm{c}}(\mathcal{G})\) is the total number of c-components in \(\mathcal{G}\). Then \(\mathbf{M}\) can also be partitioned into \(\{\mathbf{M}^{1},\ldots,\mathbf{M}^{n_{\mathrm{c}}(\mathcal{G})}\}\), where each \(\mathbf{M}^{j}\) contains response variables adjacent to \(\mathbf{V}^{j}\). For example in Fig. 1(b), \(\mathbf{M}\) is partitioned into \(\{M_{1},M_{3},M_{4}\}\) and \(\{M_{2}\}\). Within each \(\mathbf{M}^{j}\), the response variables are correlated since they are connected by bidirected edges. Besides, \(\mathbf{M}^{1},\ldots,\mathbf{M}^{n_{\mathrm{c}}(\mathcal{G})}\) are mutually independent since \(\mathbf{M}^{i}\) and \(\mathbf{M}^{j}\) are not connected with bidirected edges for any \(i\neq j\). As a result, \(P(\mathbf{m})=\prod_{j=1}^{n_{\mathrm{c}}(\mathcal{G})}P(\mathbf{m}^{j})\). By concatenating \(P(\mathbf{m}^{j})\) for each \(\mathbf{m}^{j}\in\Omega(\mathbf{M}^{j})\), we construct a vector \(\mathbf{p}_{j}\in\Delta(|\Omega(\mathbf{M}^{j})|)\), where \(\Delta(|\Omega(\mathbf{M}^{j})|):=\left\{\mathbf{p}_{j}^{\prime}\in\mathbb{R} _{\geq 0}^{|\Omega(\mathbf{M}^{j})|}\mid\mathbf{1}^{\top}\mathbf{p}_{j}^{ \prime}=1\right\}\) and \(\mathbf{1}^{\top}\) is the transpose of the all-ones vector.

Let the parent set of a c-component \(\mathbf{V}^{j}\) be \(\mathsf{Pa}_{\mathbf{V}^{j}}:=(\cup_{i:V_{i}\in\mathbf{V}^{j}}\mathsf{Pa}_{i}) \setminus\mathbf{V}^{j}\). When taking intervention \(do(\mathbf{S}=\mathbf{s})\), the values of \(\mathbf{V}^{j}\cap\mathbf{S}\) is set to \(\mathbf{s}[\mathbf{V}^{j}]\), which denotes the values of \(\mathbf{V}^{j}\cap\mathbf{S}\) that is consistent with \(\mathbf{s}\). Notice that \(\mathbf{M}^{j}\) picks the mapping functions from \(\mathsf{Pa}_{i}\) to \(V_{i}\) for all \(V_{i}\in\mathbf{V}^{j}\). Seeing values \(\mathbf{v}^{j},\mathsf{pa}_{\mathbf{V}^{j}}\) and \(\mathbf{s}[\mathbf{V}^{j}]\), there exists a set of configurations of \(\mathbf{M}^{j}\), denoted by \(B_{\mathcal{G},\mathbf{s}[\mathbf{V}^{j}]}(\mathbf{v}^{j},\mathsf{pa}_{ \mathbf{V}^{j}})\subseteq\Omega(\mathbf{M}^{j})\), that can make this happen. By marking configurations in \(B_{\mathcal{G},\mathbf{s}[\mathbf{V}^{j}]}(\mathbf{v}^{j},\mathsf{pa}_{ \mathbf{V}^{j}})\) with \(1\) and others to be \(0\), we construct a vector \(b_{\mathcal{G},\mathbf{s}[\mathbf{V}^{j}]}(\mathbf{v}^{j},\mathsf{pa}_{ \mathbf{V}^{j}})\in\{0,1\}^{|\Omega(\mathbf{M}^{j})|}\). With \(\mathbf{M}^{1},\ldots,\mathbf{M}^{n_{\mathrm{c}}(\mathcal{G})}\) being mutually independent, the interventional distribution can be factorized as

\[P_{\mathbf{s}}(\mathbf{v})=\prod_{j=1}^{n_{\mathrm{c}}(\mathcal{G})}P\big{(} \mathbf{M}^{j}\in B_{\mathcal{G},\mathbf{s}[\mathbf{V}^{j}]}(\mathbf{v}^{j}, \mathsf{pa}_{\mathbf{V}^{j}})\big{)}=\prod_{j=1}^{n_{\mathrm{c}}(\mathcal{G})} b_{\mathcal{G},\mathbf{s}[\mathbf{V}^{j}]}^{\top}(\mathbf{v}^{j}, \mathsf{pa}_{\mathbf{V}^{j}})\mathbf{p}_{j}.\] (5)

In the bow graph example in Fig. 1(c), there is one c-component, and vector \(b_{\mathcal{G},\mathbf{s}[\mathbf{V}^{j}]}^{\top}(\mathbf{v}^{j},\mathsf{pa}_ {\mathbf{V}^{j}})\) can be constructed by referring to (4), where \(\mathbf{p}_{1}\) is a concatenation of \(p_{i,j}\). The result in (5) generalizes the parameterization for the bow graph to a general causal graph. Based on it, given a causal graph \(\mathcal{G}\), we define the space of interventional distribution tuples as the following.

**Definition 2** (Space of Interventional Distribution Tuples).: _Given a causal graph \(\mathcal{G}=\langle\mathbf{V},\mathcal{E},\mathcal{B}\rangle\), the space of interventional (and observational) distribution tuples \(\mathcal{P}=\langle P_{\mathbf{s}}\rangle_{\mathbf{s}\in\mathcal{I}}\) is_

\[\mathcal{M}_{\mathcal{G}}:=\bigg{\{}\mathcal{P}^{\prime}\Bigm{|}\forall \mathbf{s}\in\mathcal{I},\mathbf{v}\in\Omega(\mathbf{V}):P_{\mathbf{s}}^{ \prime}(\mathbf{v})=\prod_{j=1}^{n_{\mathrm{c}}(\mathcal{G})}b_{\mathcal{G}, \mathbf{s}[\mathbf{V}^{j}]}^{\top}(\mathbf{v}^{j},\mathsf{pa}_{\mathbf{V}^{ j}})\mathbf{p}_{j},\mathbf{p}_{j}\in\Delta(|\Omega(\mathbf{M}^{j})|) \bigg{\}}.\]

Space \(\mathcal{M}_{\mathcal{G}}\) contains all interventional distribution tuples associated with canonical SCMs \(\langle\mathbf{M},\mathbf{V},\underline{\mathbb{E}},P(\mathbf{M})\rangle\) with arbitrary \(P(\mathbf{M})\). Since the space of \(\{\mathbf{p}_{1},\ldots,\mathbf{p}_{n_{\mathrm{c}}(\mathcal{G})}\}\) is compact and a continuous image of a compact space is compact, \(\mathcal{M}_{\mathcal{G}}\) is also a compact space.

## 4 An Asymptotic Regret Lower Bound

In this section, we present an asymptotic information-theoretic regret lower bound for the causal bandit problem with confounders. It quantifies the optimal asymptotic performance of a uniformly good causal bandit policy defined below.

**Definition 3** (Uniformly Good Policy).: _Given a causal graph \(\mathcal{G}=\langle\mathbf{V},\mathcal{E},\mathcal{B}\rangle\), a causal bandit policy \(\pi\) is uniformly good if for any \(\alpha>0\), the expected regret of \(\pi\) for any interventional distribution tuple setup \(\mathcal{P}\in\mathcal{M}_{\mathcal{G}}\), denoted by \(R_{T}^{\pi}(\mathcal{P})\), satisfies_

\[\lim_{T\rightarrow\infty}R_{T}^{\pi}\big{(}\mathcal{P}\big{)}/T^{\alpha}=0.\]

A policy \(\pi\) being uniformly good indicates that it can achieve subpolynomial regret for any \(\mathcal{P}\in\mathcal{M}_{\mathcal{G}}\). The information-theoretic regret lower bound sets the limit for the asymptotic regret of all such policies. Let \(D(P\parallel Q)\) denote the Kullback-Leibler (KL) divergence of two probability distributions \(P\) and \(Q\) and let \(\mathcal{I}_{-\mathbf{s}}=\mathcal{I}\setminus\{\mathbf{s}\}\).

**Theorem 1**.: _Given a causal graph \(\mathcal{G}=\langle\mathbf{V},\mathcal{E},\mathcal{B}\rangle\), let \(\mathcal{P}=\langle P_{\mathbf{s}}\rangle_{\mathbf{s}\in\mathcal{I}}\in\mathcal{ M}_{\mathcal{G}}\) be the true interventional distribution tuple with a unique optimal intervention \(\mathbf{s}_{*}\in\mathcal{I}\) with mean reward \(\mu_{*}\). The expected regret for any uniformly good causal bandit policy \(\pi\) satisfies_

\[\liminf_{T\to\infty}R_{T}^{\pi}\big{(}\mathcal{P}\big{)}/\ln T\geq C(\mathcal{ P},\mathcal{G}),\]

_where \(C(\mathcal{P},\mathcal{G})\) is the value of the optimization problem given below,_

\[\mathbb{O}(\mathcal{P},\mathcal{G}): \underset{\eta_{*}\geq 0,\forall\mathbf{s}\in\mathcal{I}_{- \mathbf{s}_{*}}}{\mathrm{minimize}}\sum_{s\in\mathcal{I}_{-\mathbf{s}_{*}}} \eta_{\mathbf{s}}\Delta_{\mathbf{s}},\] (6) \[\mathrm{s.t.} \max_{\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{*}}}\mu_{\mathbf{s} }^{\prime}\leq\mu_{*},\forall\mathcal{P}^{\prime}\in\Big{\{}\mathcal{P}^{ \prime\prime}\in\mathcal{M}_{\mathcal{G}}\ \Big{|}\ \sum_{\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{*}}}\eta_{ \mathbf{s}}D(P_{\mathbf{s}}\parallel P_{\mathbf{s}}^{\prime\prime})<1,P_{ \mathbf{s}_{*}}^{\prime\prime}=P_{\mathbf{s}_{*}}\Big{\}},\] (7)

_where \(\mu_{\mathbf{s}}^{\prime}\) is the expected reward of intervention \(\mathbf{s}\) according to probability distribution \(P_{\mathbf{s}}^{\prime}\)._

Theorem 1 can be viewed as an extension of (10, Th. 1), and we defer its proof to appendices. The optimization problem \(\mathbb{O}(\mathcal{P},\mathcal{M}_{\mathcal{G}})\) is a semi-infinite program since there are infinite \(\mathcal{P}^{\prime}\) in (7). To interpret the lower bound, (6) is a minimization of the regret, and its solution indicates an optimal allocation of \(O(\ln T)\) number of explorations, which is \(\eta_{\mathbf{s}}\ln T\) for each \(\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{*}}\) as \(T\to\infty\). In (7), \(\sum_{\mathbf{s}\in\mathcal{I}}\eta_{\mathbf{s}}D(P_{\mathbf{s}}\parallel P_{ \mathbf{s}}^{\prime\prime})\) can be viewed as the distance generated between true interventional distribution tuple \(\mathcal{P}\) and an alternative \(\mathcal{P}^{\prime\prime}\in\mathcal{M}_{\mathcal{G}}\) by an exploration allocation strategy \(\langle\eta_{\mathbf{s}}\rangle_{\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{*}}}\). For any \(\mathcal{P}^{\prime}\) in (7), \(\mu_{\mathbf{s}_{*}}^{\prime}=\mu_{*}\) since \(P_{\mathbf{s}_{*}}^{\prime}=P_{\mathbf{s}_{*}}\), and as a result, \(\max_{\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{*}}}\mu_{\mathbf{s}}^{\prime}\leq \mu_{*}\) indicates intervention \(\mathbf{s}_{*}\) is also optimal in \(\mathcal{P}^{\prime}\). So (7) sets a constraint for \(\langle\eta_{\mathbf{s}}\rangle_{\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{*}}}\): the models allowed to have distance \(<1\) from the true model must also take \(\mathbf{s}_{*}\) to be optimal. Or conversely, \(\langle\eta_{\mathbf{s}}\rangle_{\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{*}}}\) must generate distance \(\geq 1\) for any \(\mathcal{P}^{\prime}\in\mathcal{M}_{\mathcal{G}}\) with \(P_{\mathbf{s}_{*}}^{\prime}=P_{\mathbf{s}_{*}}\) not taking intervention \(\mathbf{s}_{*}\) to be optimal.

**Remark 1**.: _The \(O(\ln T)\) regret lower bound holds only if there exists \(\mathcal{P}^{\prime}\in\mathcal{M}_{\mathcal{G}}\) such that \(P_{\mathbf{s}_{*}}^{\prime}=P_{\mathbf{s}_{*}}\), which is a condition meaning \(\mathcal{P}\) can not be distinguished from all other interventional distribution tuples in \(\mathcal{M}_{\mathcal{G}}\) by only taking intervention \(\mathbf{s}_{*}\). This condition holds for almost all causal bandit problems. Otherwise, constraint (7) is relaxed, and the value of the optimization problem \(C(\mathcal{P},\mathcal{M}_{\mathcal{G}})=0\), which indicates sub-logarithmic regret can be achieved. Following the same line of proof in (17, Th. 3.9), it can be shown simple UCB can achieve this sub-logarithmic, in fact constant, expected regret._

## 5 SCM-based Causal Bandit Algorithm

The regret lower bound in Theorem 1 suggests a potentially optimal exploration strategy, i.e. to take each intervention \(\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{*}}\) up to \(\eta_{\mathbf{s}}\ln T\) times. However, it requires the ground truth \(\mathcal{P}\) to solve the optimization problem \(\mathbb{O}(\mathcal{P},\mathcal{G})\). The allocation matching principle [10] essentially replaces the true model with an estimated one to solve the optimization problem, and controls explorations to match with the solution. It is expected that the estimated model converges to the ground truth at a fast enough rate so that the actual explorations match with the optimal exploration strategy. We follow this idea to design a causal bandit policy, whose pseudo-code is shown in Algorithm 1. We call this SCM-based Approximate Allocation Matching (SCM-AAM).

Similar to [22], SCM-AAM only intervenes on POMISs for the purpose of reducing the action space. The algorithm is anytime without requiring the knowledge of horizon length. It takes two tuning parameters \(\epsilon=(0,1/|\mathcal{I}|)\) and \(\gamma>0\) as inputs that control the exploration rate, which affects finite-time performance. At each time \(t\), let \(N_{t}(\mathbf{s})\) be the number of times the intervention \(do(\mathbf{S}=\mathbf{s})\) is taken so far, and let \(N_{t}(\mathbf{v},\mathbf{s})\) be the number of times we observe \(\mathbf{v}\) with intervention \(do(\mathbf{S}=\mathbf{s})\). The algorithm maintains a set of empirical interventional distributions \(\langle\bar{P}_{\mathbf{s},t}\rangle_{\mathbf{s}\in\mathcal{I}}\) and empirical mean rewards \(\langle\bar{\mu}_{\mathbf{s},t}\rangle_{\mathbf{s}\in\mathcal{I}}\), where \(\bar{P}_{\mathbf{s},t}(\mathbf{v})=N_{t}(\mathbf{v},\mathbf{s})/N_{t}(\mathbf{s})\) for each \(\mathbf{s}\in\mathcal{I}\).

The main body of the algorithm is composed of two components: exploitation and exploration. At each round \(t\), it first attempts to evaluate if enough information has been collected to determine the optimal action. Inspired by the lower bound, enough distance is generated by the sampling history between \(\langle\bar{P}_{\mathbf{s},t}\rangle_{\mathbf{s}\in\mathcal{I}}\) and \(\mathcal{P}^{\prime}=\langle P_{\mathbf{s}}^{\prime}\rangle_{\mathbf{s}\in \mathcal{I}}\) if \(\sum_{\mathbf{s}\in\mathcal{I}}N_{t}(\mathbf{s})D(\bar{P}_{\mathbf{s},t}\parallel P _{\mathbf{s}}^{\prime})>(1+\gamma)\ln t\). So we can construct a confidence set \(\mathcal{C}_{t}\) with high probability to contain the true model \(\mathcal{P}\), which is defined as

\[\mathcal{C}_{t}:=\Big{\{}\mathcal{P}^{\prime}\in\mathcal{M}_{\mathcal{G}}\ \Big{|}\ \sum_{ \mathbf{s}\in\mathcal{I}}N_{t}(\mathbf{s})D(\bar{P}_{\mathbf{s},t}\parallel P_{ \mathbf{s}}^{\prime})\leq(1+\gamma)\ln t\Big{\}}.\] (8)With \(V_{n}\) being the reward node, for each \(\mathbf{s}\in\mathcal{I}\), the algorithm computes the UCB of the mean reward of taking intervention \(\mathbf{s}\) as \(\tilde{\mu}_{\mathbf{s}}(\mathcal{C}_{t})=\sum_{\mathbf{v}\in\Omega(\mathbf{V})} v_{n}\max_{\mathcal{P}^{\prime}\in\mathcal{C}_{t}}P_{\mathbf{s}}^{\prime}( \mathbf{v})\). If there exists an intervention \(\mathbf{s}_{*,t}\) with empirical mean reward \(\tilde{\mu}_{\mathbf{s}_{*,t}}>\tilde{\mu}_{\mathbf{s}}(\mathcal{C}_{t}), \forall\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{*,t}}\), we are confident about \(\mathbf{s}_{*,t}\) to be the optimal intervention. So the algorithm enters the exploitation phase and selects \(\mathbf{S}_{t}=\mathbf{s}_{*,t}\).

If no such \(\mathbf{s}_{*,t}\) exists, the algorithm needs to explore. According to the allocation principle, one is inclined to solve \(\mathbb{O}(\langle\bar{P}_{\mathbf{s},t}\rangle_{\mathbf{s}\in\mathcal{I}}, \mathcal{G})\) defined in Theorem 1 and make \(N_{t}(\mathbf{s})\approx\eta_{\mathbf{s}}\ln t\) for each \(\mathbf{s}\in\mathcal{I}\). Nevertheless, there is no guarantee that this semi-infinite program can be solved efficiently. Therefore, we take a greedy approximation approach. Ideally, to get out of exploration, the algorithm needs to reduce \(\max_{\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{*}}}\tilde{\mu}_{\mathbf{s}}( \mathcal{C}_{t})\) to a value below \(\mu_{*}\), thus it selects \(\overline{\mathbf{S}}_{t}=\arg\max_{\mathbf{s}\in\mathcal{I}}\tilde{\mu}_{ \mathbf{s}}(\mathcal{C}_{t})\). The algorithm maintains a counter of exploration steps \(N_{t}^{\text{e}}\). If \(\min_{\mathbf{s}\in\mathcal{I}}N_{t}(\mathbf{s})\leq\epsilon N_{t}^{\text{e}}\), it generates forced exploration by selecting the least selected intervention \(\underline{\mathbf{S}}_{t}\). Such a mechanism ensures every action is persistently taken in exploration phases so that \(\langle\bar{P}_{\mathbf{s},t}\rangle_{\mathbf{s}\in\mathcal{I}}\) converges to the ground truth eventually.

**Remark 2**.: _With factorization in (5), \(\mathcal{C}_{t}\) corresponds to setting constraint for \(\{\mathbf{p}_{1},\ldots,\mathbf{p}_{n_{\text{e}}(\mathcal{G})}\}\) as_

\[\sum_{\mathbf{s}\in\mathcal{I}}\sum_{\mathbf{v}\in\Omega(\mathbf{V})}N_{t}( \mathbf{s},\mathbf{v})\ln\frac{\bar{P}_{\mathbf{s},t}(\mathbf{v})}{\prod_{j=1 }^{n_{\text{e}}(\mathcal{G})}b_{\mathcal{G},\mathbf{s}[\mathbf{V}^{j}]}^{ \top}(\mathbf{v}^{j},\mathsf{pa}_{\mathbf{V}^{j}})\mathbf{p}_{j}}\leq(1+\gamma )\ln t,\] (9)

_where the expression on the left of inequality is a convex function of \(\mathbf{p}_{1},\ldots,\mathbf{p}_{n_{\text{e}}(\mathcal{G})}\). Define convex set \(\mathcal{C}_{t}^{\text{p}}=\big{\{}\{\mathbf{p}_{1},\ldots,\mathbf{p}_{n_{ \text{e}}(\mathcal{G})}\}\mid\mathbf{p}_{j}\in\Delta(|\Omega(\mathbf{M}^{j})|), \forall j\in\{1,\ldots,n_{\text{e}}(\mathcal{G})\}\), and (9) is true\(\big{\}}\). Then the UCB can be derived by maximizing a set of concave functions over \(\mathcal{C}_{t}^{\text{p}}\) as follows,_

\[\tilde{\mu}_{\mathbf{s}}(\mathcal{C}_{t})=\!\!\!\!\!\sum_{\mathbf{v}\in\Omega (\mathbf{V})}\!\!\!\!v_{n}\max_{\mathcal{P}^{\prime}\in\mathcal{C}_{t}}P_{ \mathbf{s}}^{\prime}(\mathbf{v})=\!\!\!\!\sum_{\mathbf{v}\in\Omega(\mathbf{V}) }\!\!\!\!v_{n}\exp\Big{(}\max_{\{\mathbf{p}_{1},\ldots,\mathbf{p}_{n_{\text{e }}(\mathcal{G})}\}\in\mathcal{C}_{t}^{\text{p}}}\sum_{j=1}^{n_{\text{e}}( \mathcal{G})}\ln\big{(}b_{\mathcal{G},\mathbf{s}[\mathbf{V}^{j}]}^{\top}( \mathbf{v}^{j},\mathsf{pa}_{\mathbf{V}^{j}})\mathbf{p}_{j}\big{)}\Big{)}.\]

_We use \(\tilde{\mu}_{\mathbf{s}}(\mathcal{C}_{t})\) instead of \(\tilde{\mu}_{\mathbf{s}}^{\prime}(\mathcal{C}_{t})=\max_{\mathbf{v}^{\prime} \in\mathcal{C}_{t}}\sum_{\mathbf{v}\in\Omega(\mathbf{V})}v_{n}P_{\mathbf{s}}^{ \prime}(\mathbf{v})\) as UCB since \(\sum_{\mathbf{v}\in\Omega(\mathbf{V})}v_{n}P_{\mathbf{s}}^{\prime}(\mathbf{v})\) in general is a non-concave function of \(\mathbf{p}_{1},\ldots,\mathbf{p}_{n_{\text{e}}(\mathcal{G})}\). Also, notice that \(\forall\mathbf{s}\in\mathcal{I}:\tilde{\mu}_{\mathbf{s}}(\mathcal{C}_{t})\geq \tilde{\mu}_{\mathbf{s}}^{\prime}(\mathcal{C}_{t})\)._

In a pure topology-based approach [22], each intervention on POMISs is treated independently during the algorithm execution. Whereas SCM-AAM can leverage the canonical SCM to incorporate the sampling results from different interventions together. After each new sample \(\mathbf{v}_{t}\) with intervention \(\mathbf{s}_{t}\) is observed, it is used to update the parametric space for interventional distribution tuples \(\mathcal{C}_{t}\).

## 6 Finite Time Regret Analysis

We present a finite time problem-dependent regret upper bound for SCM-AAM in Theorem 2. The complete proof is provided in the appendices. We also give an interpretation of Theorem 2, and compare the regret upper bound with [22] in the setting where the reward node \(V_{n}\) is binary,

**Theorem 2**.: _For the causal bandit problem with unobserved confounders, suppose the causal graph \(\mathcal{G}=\langle\mathbf{V},\mathcal{E},\mathcal{B}\rangle\) is given and Assumptions 1 and 2 are true. For any interventional distribution tuple \(\mathcal{P}\in\mathcal{M}_{\mathcal{G}}\), any \(\kappa>0\) and horizon of length \(T\), the expected regret for SCM-AAM with parameters \(\epsilon\in(0,1/\left|\mathcal{I}\right|)\) and \(\gamma>0\) satisfies_

\[R_{T}^{\text{SCM-AAM}}(\mathcal{P})\leq(1+\kappa)(1+\gamma)\sum_{\mathbf{s}\in \mathcal{I}_{-\mathbf{s}_{*}}}\zeta_{\mathbf{s}}(\mathcal{P})\Big{(}\Delta_{ \mathbf{s}}+\frac{\epsilon\left|\mathcal{I}\right|}{1-\epsilon\left|\mathcal{ I}\right|}\Big{)}\ln T+c,\]

_where \(c\) is a suitably large universal constant depending on \(\mathcal{P}\), \(\kappa\) and tuning parameters \(\epsilon\) and \(\gamma\), and_

\[\forall\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{*}}:\zeta_{\mathbf{s}}( \mathcal{P})=\begin{cases}0,&\tilde{\mu}_{\mathbf{s}}(\mathcal{C}_{\mathbf{s} }(0,\mathcal{P}))\leq\mu_{*},\\ \max_{\eta_{\mathbf{s}}\geq 0}\eta_{\mathbf{s}}\ \ \mathrm{s.\,t.}\ \tilde{\mu}_{ \mathbf{s}}(\mathcal{C}_{\mathbf{s}}(\eta_{\mathbf{s}},\mathcal{P}))\geq\mu_{* },&\text{otherwise},\end{cases}\]

_in which \(\mathcal{C}_{\mathbf{s}}(\eta_{\mathbf{s}},\mathcal{P}):=\big{\{}\mathcal{P}^{ \prime}\in\mathcal{M}_{\mathcal{G}}\mid\eta_{\mathbf{s}}D(P_{\mathbf{s}}\parallel P ^{\prime}_{\mathbf{s}})+\epsilon\eta_{\mathbf{s}}/2\sum_{\mathbf{x}\in \mathcal{I}_{-\mathbf{s}_{*}}}D(P_{\mathbf{x}}\parallel P^{\prime}_{\mathbf{x }})\leq 1\big{\}}\)._

Theorem 2 indicates the expected number of samples for each \(\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{*}}\) can be approximately bounded by \(\zeta_{\mathbf{s}}(\mathcal{P})\ln T\). It can be interpreted as follows: when \(\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{*}}\) is selected approximately up to \(\zeta_{\mathbf{s}}(\mathcal{P})\ln T\) times, its UCB can be pushed below \(\mu_{*}\). The presence of terms containing \(\epsilon\) is due to forced exploration. In [22], KL-UCB and Thompson sampling are employed to apply interventions on POMISs. If the reward node \(V_{n}\in\{0,1\}\), they enjoys expected regret \(\sum_{\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{*}}}(\Delta_{\mathbf{s}}\ln T)/D (P_{\mathbf{s}}(V_{n})\parallel P_{\mathbf{s}_{*}}(V_{n}))+c^{\prime}\) for some insignificant \(c^{\prime}\)[14; 2]. In this situation, Theorem 2 shows that the SCM-AAM enjoys smaller expected regret when we disregard the small constants \(\kappa\), \(\gamma\) and \(\epsilon\). To see it, we express the leading constant for KL-UCB and Thompson as

\[1/D(P_{\mathbf{s}}(V_{n})\parallel P_{\mathbf{s}_{*}}(V_{n}))=\max_{\eta_{ \mathbf{s}}\geq 0}\eta_{\mathbf{s}}\ \ \mathrm{s.\,t.}\ \tilde{\mu}_{\mathbf{s}}(\mathcal{C}^{\prime}_{ \mathbf{s}}(\eta_{\mathbf{s}},P(V_{n})))\geq\mu_{*},\]

where \(\mathcal{C}^{\prime}_{\mathbf{s}}(\eta_{\mathbf{s}},P(V_{n}))=\{P^{\prime}_{ \mathbf{s}}(V_{n})\in[0,1]\mid\eta_{\mathbf{s}}D(P_{\mathbf{s}}(V_{n})\parallel P ^{\prime}_{\mathbf{s}}(V_{n}))\leq 1\}\). Note that \(\mathcal{C}^{\prime}_{\mathbf{s}}(\eta_{\mathbf{s}},P(V_{n}))\) only sets constraint on \(P^{\prime}_{\mathbf{s}}(V_{n})\), while \(\mathcal{C}_{\mathbf{s}}(\eta_{\mathbf{s}},\mathcal{P})\) sets constraints on \(P^{\prime}_{\mathbf{s}}(\mathbf{V})\). As a result, \(\mathcal{C}_{\mathbf{s}}(\eta_{\mathbf{s}},\mathcal{P})\subseteq\mathcal{C}^ {\prime}_{\mathbf{s}}(\eta_{\mathbf{s}},P(V_{n}))\), so that \(\zeta_{\mathbf{s}}(\mathcal{P})\leq 1/D(P_{\mathbf{s}}(V_{n})\parallel P_{ \mathbf{s}_{*}}(V_{n}))\).

## 7 Experimental Results

We compare the empirical performance of SCM-AAM with existing algorithms. The first baseline algorithm employs the simple UCB algorithm [4] on a reduced action set that includes interventions on POMISs. The other two baselines as introduced in [22], utilize the KL-UCB algorithm and Thompson sampling (TS) to intervene on POMISs. We compare the performance of all the algorithms on three different causal bandit instances shown in Fig. 2. We choose the input parameters of the SCM-AAM algorithm to be \(\gamma=0.1\) and \(\epsilon=1/|5L|\) in the simulations. We set the horizon to \(800\) for all three tasks and repeat every simulation \(100\) times. Additionally, we include a confidence interval around the mean regret, with the width of the interval set to twice the standard deviation. The structural equations for the three causal bandit instances, as well as experimental details, can be found in the appendices. We plot the mean regret against time, where time in this context corresponds to the number of actions. All the nodes in Fig. 2 take binary values, either \(0\) or \(1\).

For task 1, we consider the causal graph shown in Fig. 2(a) with POMISs \(\{V_{1}\}\) and \(\{V_{2}\}\). For task 2 and task 3, we consider more complex causal graphs, as displayed in Fig. 2(b) and (c). The POMISs for task 2 include \(\{V_{1}\}\), \(\{V_{3}\}\), and \(\{V_{3},V_{4}\}\), while for task 3, they are \(\{V_{1},V_{4},V_{6}\}\) and \(\{V_{5},V_{6}\}\). The UCB algorithm has inferior performance compared to other baseline algorithms, especially in the more complicated tasks \(2\) and \(3\). The TS algorithm outperforms both UCB and KL-UCB in all three settings; however, it still incurs more regret than our proposed algorithm. The experiments demonstrate that our proposed SCM-AAM algorithm consistently outperforms other baseline algorithms by incurring lower empirical regret across all three tasks. Notably, the performance advantage of SCM-AAM is more significant for tasks 2 and 3, which have a more intricate causal graph structure with a higher number of nodes and edges. The results indicate thatincorporating causal structural information through the SCM-AAM algorithm can significantly reduce expected regret, especially for more complex causal graphs.

The simulations were conducted on a Windows desktop computer featuring a \(12\)th generation Intel Core i7 processor operating at \(3.1\) GHz and \(32\) GB of RAM. No GPUs were utilized in the simulations. The runtime for each iteration of the SCM-MAB, involving \(800\) arm plays, was approximately \(2\) minutes for both Task \(1\). However, for Task \(2\) and Task \(3\), which encompass larger underlying causal graphs, the runtime for every iteration increased to around \(20\) minutes. The algorithm code is provided at https://github.com/CausalML-Lab/SCM-AAM.

## 8 Conclusion

In this paper, we studied the causal bandit problem with latent confounders. With causal information provided by a causal graph, we present a problem-dependent information-theoretic regret lower bound. Leveraging the canonical SCM, we take an approximate allocation matching strategy to design the SCM-AAM algorithm. By analyzing SCM-AAM, we show it has a problem-dependent logarithmic regret upper bound. The analytic result is complemented with numerical illustrations featuring a variety of causal structures. It is shown that SCM-AAM exhibits promising performance in comparison with classic baseline algorithms. Since SCM has a natural application in counterfactual reasoning, extending the proposed algorithm and theoretical results to a counterfactual decision-making setup is an interesting future direction.

## Acknowledgements

This research has been supported in part by NSF CAREER 2239375. Most of the work was completed when Lai Wei was a postdoctoral researcher at Purdue University.

Figure 3: Regret versus time for the instances of structural causal bandits shown in Figure 2

Figure 2: Causal graphs used in the experiments

## References

* [1] V. Aglietti, X. Lu, A. Paleyes, and J. Gonzalez. Causal bayesian optimization. In _International Conference on Artificial Intelligence and Statistics_, pages 3155-3164. PMLR, 2020.
* [2] S. Agrawal and N. Goyal. Analysis of thompson sampling for the multi-armed bandit problem. In _Conference on learning theory_, pages 39-1. JMLR Workshop and Conference Proceedings, 2012.
* [3] J. M. Antle and S. M. Capalbo. Econometric-process models for integrated assessment of agricultural production systems. _American Journal of Agricultural Economics_, 83(2):389-401, 2001.
* [4] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. _Machine learning_, 47:235-256, 2002.
* [5] A. Balke and J. Pearl. Counterfactual probabilities: Computational methods, bounds and applications. In _Uncertainty Proceedings 1994_, pages 46-54. Elsevier, 1994.
* [6] E. Bareinboim, A. Forney, and J. Pearl. Bandits with unobserved confounders: A causal approach. _Advances in Neural Information Processing Systems_, 28, 2015.
* [7] J. Bartroff, T. L. Lai, and M.-C. Shih. _Sequential experimentation in clinical trials: design and analysis_, volume 298. Springer Science & Business Media, 2012.
* [8] B. Bilodeau, L. Wang, and D. M. Roy. Adaptively exploiting d-separators with causal bandits. _arXiv preprint arXiv:2202.05100_, 2022.
* [9] R. Combes and A. Proutiere. Unimodal bandits: Regret lower bounds and optimal algorithms. In _International Conference on Machine Learning_, pages 521-529. PMLR, 2014.
* [10] R. Combes, S. Magureanu, and A. Proutiere. Minimal exploration in structured stochastic bandits. _Advances in Neural Information Processing Systems_, 30, 2017.
* [11] A. D'Amour, K. Heller, D. Moldovan, B. Adlam, B. Alipanahi, A. Beutel, C. Chen, J. Deaton, J. Eisenstein, M. D. Hoffman, et al. Underspecification presents challenges for credibility in modern machine learning. _The Journal of Machine Learning Research_, 23(1):10237-10297, 2022.
* [12] G. Dulac-Arnold, N. Levine, D. J. Mankowitz, J. Li, C. Paduraru, S. Gowal, and T. Hester. Challenges of real-world reinforcement learning: definitions, benchmarks and analysis. _Machine Learning_, 110(9):2419-2468, 2021.
* [13] S. Feng and W. Chen. Combinatorial causal bandits. _arXiv preprint arXiv:2206.01995_, 2022.
* [14] A. Garivier and O. Cappe. The KL-UCB algorithm for bounded stochastic bandits and beyond. In _Proceedings of the 24th annual conference on learning theory_, pages 359-376. JMLR Workshop and Conference Proceedings, 2011.
* [15] T. L. Graves and T. L. Lai. Asymptotically efficient adaptive choice of control laws uncontrolled markov chains. _SIAM journal on control and optimization_, 35(3):715-743, 1997.
* [16] A. H. Guide. _Infinite dimensional analysis_. Springer, 2006.
* [17] B. Hao, T. Lattimore, and C. Szepesvari. Adaptive exploration in linear contextual bandit. In _International Conference on Artificial Intelligence and Statistics_, pages 3536-3545. PMLR, 2020.
* [18] T. L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. _Advances in applied mathematics_, 6(1):4-22, 1985.
* [19] F. Lattimore, T. Lattimore, and M. D. Reid. Causal bandits: Learning good interventions via causal inference. _Advances in Neural Information Processing Systems_, 29, 2016.
* [20] T. Lattimore and C. Szepesvari. The end of optimism? an asymptotic analysis of finite-armed linear bandits. In _Artificial Intelligence and Statistics_, pages 728-737. PMLR, 2017.

* [21] T. Lattimore and C. Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* [22] S. Lee and E. Bareinboim. Structural causal bandits: where to intervene? _Advances in Neural Information Processing Systems_, 31, 2018.
* [23] S. Lee and E. Bareinboim. Structural causal bandits with non-manipulable variables. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 33, pages 4164-4172, 2019.
* [24] Y. Lu, A. Meisami, A. Tewari, and W. Yan. Regret analysis of bandit problems with causal background knowledge. In _Conference on Uncertainty in Artificial Intelligence_, pages 141-150. PMLR, 2020.
* [25] A. Maiti, V. Nair, and G. Sinha. A causal bandit approach to learning good atomic interventions in presence of unobserved confounders. In _The 38th Conference on Uncertainty in Artificial Intelligence_, 2022.
* [26] J. S. McMullen. Entrepreneurial judgment as empathic accuracy: A sequential decision-making approach to entrepreneurial action. _Journal of Institutional Economics_, 11(3):651-681, 2015.
* [27] P. A. Ortega and D. A. Braun. Generalized thompson sampling for sequential decision-making and causal inference. _Complex Adaptive Systems Modeling_, 2(1):1-23, 2014.
* [28] J. Pearl. Causal diagrams for empirical research. _Biometrika_, 82(4):669-688, 1995.
* [29] J. Pearl. _Causality_. Cambridge university press, 2009.
* [30] H. Robbins. Some aspects of the sequential design of experiments. _Bulletin of the American Mathematical Society_, 58(5):527-535, 1952.
* [31] D. Russo and B. Van Roy. Learning to optimize via information-directed sampling. _Operations Research_, 66(1):230-252, 2018.
* [32] V. Srivastava, P. Reverdy, and N. E. Leonard. Surveillance in an abruptly changing world via multiarmed bandits. In _IEEE Conf. on Decision and Control_, pages 692-697, Los Angeles, CA, Dec. 2014.
* [33] W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. _Biometrika_, 25(3/4):285-294, 1933.
* [34] J. Tian and J. Pearl. A general identification condition for causal effects. In _Eighteenth national conference on Artificial intelligence_, pages 567-573, 2002.
* [35] B. P. Van Parys and N. Golrezaei. Optimal learning for structured bandits. _arXiv preprint arXiv:2007.07302_, 2020.
* [36] C. H. Weiss, M. J. Bucuvalas, and M. Bucuvalas. _Social science research and decision-making_. Columbia University Press, 1980.
* [37] A. Yabe, D. Hatano, H. Sumita, S. Ito, N. Kakimura, T. Fukunaga, and K.-i. Kawarabayashi. Causal bandits with propagating inference. In _International Conference on Machine Learning_, pages 5512-5520. PMLR, 2018.
* [38] P. B. Yale. _Geometry and symmetry_. Courier Corporation, 2014.
* [39] J. Zhang and E. Bareinboim. Transfer learning in multi-armed bandit: a causal approach. In _Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems_, pages 1778-1780, 2017.
* [40] J. Zhang, J. Tian, and E. Bareinboim. Partial counterfactual identification from observational and experimental data. In _International Conference on Machine Learning_, pages 26548-26558. PMLR, 2022.

## Appendix A Proof of Asymptotic Regret Lower Bound in Theorem 1

The lower bound is derived following the same strategy in [21] by applying divergence decomposition and Bretagnolle-Huber inequality. For completeness, we reproduce both proofs in this section. Readers familiar with these results can skip them. Besides, we note that a more general result on controlled Markov chains is proven in [15].

Recall that a policy \(\pi\) is composed of a sequence \(\{\pi_{t}\}_{t\in\mathbb{N}_{>0}}\), where at each time \(t\in\{1,\ldots,T\}\), \(\pi_{t}\) determines the probability distribution of taking intervention \(\mathbf{S}_{t}\in\mathcal{I}\) given intervention and observation history \(\pi_{t}(\mathbf{S}_{t}\mid\mathbf{s}_{1},\mathbf{v}_{1},\ldots,\mathbf{s}_{t -1},\mathbf{v}_{t-1})\). So the intervention and observation sequence \(\{\mathbf{S}_{t},\mathbf{V}_{t}\}_{t\in\mathbb{N}_{>0}}\) is a production of the interactions between the interventional distribution tuple \(\langle P_{\mathbf{s}}\rangle_{\mathbf{s}\in\mathcal{I}}\) and policy \(\pi\). Let \(T\) be the horizon length, we define a probability measure \(\mathbb{P}\) on the sequence of outcomes induced by \(\langle P_{\mathbf{s}}\rangle_{\mathbf{s}\in\mathcal{I}}\) and \(\pi\) such that

\[\mathbb{P}(\mathbf{s}_{1},\mathbf{v}_{1},\ldots,\mathbf{s}_{T},\mathbf{v}_{T} )=\prod_{t=1}^{T}\pi_{t}(\mathbf{s}_{t}\mid\mathbf{s}_{1},\mathbf{v}_{1}, \ldots,\mathbf{s}_{t-1},\mathbf{v}_{t-1})P_{\mathbf{s}_{t}}(\mathbf{v}_{t}).\] (10)

For a fixed policy \(\pi\), let \(\mathbb{P}\) and \(\mathbb{P}^{\prime}\) be the probability measures on the \(T\)-round plays of two different causal bandit instances with different interventional distribution tuples \(\mathcal{P}=\langle P_{\mathbf{s}}\rangle_{\mathbf{s}\in\mathcal{I}}\) and \(\mathcal{P}^{\prime}=\langle P^{\prime}_{\mathbf{s}^{\prime}}\rangle_{\mathbf{s }\in\mathcal{I}}\) in space \(\mathcal{M}_{\mathcal{G}}\), which is defined in Definition 2. From (10), we get the distinction between \(\mathbb{P}\) and \(\mathbb{P}^{\prime}\) is exclusively due to the separations of \(\mathcal{P}\) and \(\mathcal{P}^{\prime}\). As a result, \(D(\mathbb{P}\parallel\mathbb{P}^{\prime})\) has the following decomposition, which is a standard result in MAB problems [21, Th. 15.1].

**Lemma 3** (Divergence Decomposition).: _Given a causal graph \(\mathcal{G}=\langle\mathbf{V},\mathcal{E},\mathcal{B}\rangle\), let \(\mathcal{P}=\langle P_{\mathbf{s}}\rangle_{\mathbf{s}\in\mathcal{I}}\) be the true interventional distribution tuple and let \(\mathcal{P}^{\prime}=\langle P^{\prime}_{\mathbf{s}^{\prime}}\rangle_{\mathbf{s }\in\mathcal{I}}\) be different from \(\mathcal{P}\). For some fixed policy \(\pi\), let \(\mathbb{P}\) and \(\mathbb{P}^{\prime}\) be the probability measures on the \(T\) rounds of causal bandit plays in \(\mathcal{P}\) and \(\mathcal{P}^{\prime}\) respectively. Then,_

\[D(\mathbb{P}\parallel\mathbb{P}^{\prime})=\sum_{\mathbf{s}\in\mathcal{I}} \mathbb{E}[N_{T}(\mathbf{s})]D\left(P_{\mathbf{s}}\parallel P^{\prime}_{ \mathbf{s}}\right),\]

_where the expectation is computed with probability measure \(\mathbb{P}\)._

Proof.: For a fixed policy \(\pi\), from (10), we get

\[\mathbb{P}(\mathbf{s}_{1},\mathbf{v}_{1},\ldots,\mathbf{s}_{T},\mathbf{v}_{T} )=\prod_{t=1}^{T}\pi_{t}(\mathbf{s}_{t}\mid\mathbf{s}_{1},\mathbf{v}_{1}, \ldots,\mathbf{s}_{t-1},\mathbf{v}_{t-1})P_{\mathbf{s}_{t}}(\mathbf{v}_{t}).\]

As a result, \(\pi_{t}\) is reduced and we get

\[\log\frac{\mathbb{P}(\mathbf{s}_{1},\mathbf{v}_{1},\ldots,\mathbf{s}_{T}, \mathbf{v}_{T})}{\mathbb{P}^{\prime}(\mathbf{s}_{1},\mathbf{v}_{t},\ldots, \mathbf{s}_{T},\mathbf{v}_{T})}=\sum_{t=1}^{T}\log\frac{P_{\mathbf{s}_{t}}( \mathbf{v}_{t})}{P^{\prime}_{\mathbf{s}_{t}}(\mathbf{v}_{t})},\] (11)

which shows the distinction between \(\mathbb{P}\) and \(\mathbb{P}^{\prime}\) is exclusively due to the separations of \(P_{\mathbf{s}}\) and \(P^{\prime}_{\mathbf{s}}\) for each \(\mathbf{s}\in\mathcal{I}\). Then we can decompose \(D(\mathbb{P}\parallel\mathbb{P}^{\prime})\) with (11) as the following,

\[D(\mathbb{P}\parallel\mathbb{P}^{\prime}) =\mathbb{E}\left[\log\frac{\mathbb{P}(\mathbf{S}_{1},\mathbf{V}_{ 1},\ldots,\mathbf{S}_{T},\mathbf{V}_{T})}{\mathbb{P}^{\prime}(\mathbf{S}_{1}, \mathbf{V}_{t},\ldots,\mathbf{S}_{T},\mathbf{V}_{T})}\right]=\mathbb{E}\left[ \sum_{t=1}^{T}\log\frac{P_{\mathbf{S}_{t}}(\mathbf{V}_{t})}{P^{\prime}_{ \mathbf{S}_{t}}(\mathbf{V}_{t})}\right]=\sum_{t=1}^{T}\mathbb{E}\left[\log \frac{P_{\mathbf{S}_{t}}(\mathbf{V}_{t})}{P^{\prime}_{\mathbf{S}_{t}}( \mathbf{V}_{t})}\right]\] \[=\sum_{t=1}^{T}\mathbb{E}\left[\sum_{\mathbf{s}\in\mathcal{I}} \mathbbm{1}\{\mathbf{S}_{t}=\mathbf{s}\}D\left(P_{\mathbf{s}}\parallel P^{ \prime}_{\mathbf{s}}\right)\right]=\sum_{\mathbf{s}\in\mathcal{I}}D\left(P_{ \mathbf{s}}\parallel P^{\prime}_{\mathbf{s}}\right)\mathbb{E}\left[\sum_{t=1}^ {T}\mathbbm{1}\{\mathbf{S}_{t}=\mathbf{s}\}\right]\] \[=\sum_{a\in\mathcal{I}}\mathbb{E}[N_{T}(\mathbf{s})]D\left(P_{ \mathbf{s}}\parallel P^{\prime}_{\mathbf{s}}\right),\]

which concludes the proof.

The other tool to prove the regret lower bound is the Bretagnolle-Huber Inequality.

**Lemma 4** (Bretagnolle-Huber Inequality [21, Th. 14.2]).: _Let \(P\) and \(Q\) be two probability measures on a measurable space \((\Omega,\mathcal{F})\), and let \(E\in\mathcal{F}\) be an arbitrary event. Then_

\[P(E)+Q(E^{\complement})\geq\frac{1}{2}\exp\left(-D(P\parallel Q)\right),\]

_where \(E^{\complement}=\Omega\setminus E\) is complement of \(E\)._

Proof with finite \(\Omega\).: In Assumption 1, we assume each node in the causal graph can only take a finite number of values. So the sample space for the causal bandit problem with horizon length \(T\) is finite. We provide proof for the Bretagnolle-Huber inequality assuming \(\Omega\) is finite. First, we show

\[P(E)+Q(E^{\complement}) =\sum_{\omega\in E}P(\omega)+\sum_{\omega\in E^{\complement}}Q( \omega)\geq\sum_{\omega\in E}\min\{P(\omega),Q(\omega)\}+\sum_{\omega\in E^{ \complement}}\min\{P(\omega),Q(\omega)\}\] \[=\sum_{\omega\in\Omega}\min\{P(\omega),Q(\omega)\}=\frac{1}{2} \sum_{\omega\in\Omega}\min\{P(\omega),Q(\omega)\}\sum_{\omega\in\Omega}[P( \omega)+Q(\omega)]\] \[\geq\frac{1}{2}\sum_{\omega\in\Omega}\min\{P(\omega),Q(\omega)\} \sum_{\omega\in\Omega}\max\{P(\omega),Q(\omega)\}.\]

Applying Cauchy-Schwarz inequality, we get

\[P(E)+Q(E^{\complement})\] \[\geq \frac{1}{2}\bigg{(}\sum_{\omega\in\Omega}\sqrt{\min\{P(\omega),Q( \omega)\}\max\{P(\omega),Q(\omega)\}}\bigg{)}^{2}=\frac{1}{2}\bigg{(}\sum_{ \omega\in\Omega}\sqrt{P(\omega)Q(\omega)}\bigg{)}^{2}\] \[= \frac{1}{2}\exp\left(2\log\sum_{\omega\in\Omega}\sqrt{P(\omega)Q( \omega)}\right)=\frac{1}{2}\exp\left(2\log\sum_{\omega\in\Omega}P(\omega) \sqrt{\frac{Q(\omega)}{P(\omega)}}\right)\] \[\geq \frac{1}{2}\exp\left(2\sum_{\omega\in\Omega}P(\omega)\log\sqrt{ \frac{Q(\omega)}{P(\omega)}}\right)=\frac{1}{2}\exp\left(-D(P\parallel Q) \right).\]

We conclude our proof. 

To prove the regret lower bound, the remaining work regards applying divergence decomposition and substituting \(D(\mathbb{P}\parallel\mathbb{P}^{\prime})\) into the Bretagnolle-Huber inequality.

**Lemma 5**.: _Given a causal graph \(\mathcal{G}=\langle\mathbf{V},\mathcal{E},\mathcal{B}\rangle\), let \(\mathcal{P}=\langle P_{\mathbf{s}}\rangle_{\mathbf{s}\in\mathcal{I}}\in \mathcal{M}_{\mathcal{G}}\) be the true interventional distribution tuple with a unique optimal intervention \(\mathbf{s}_{*}\in\mathcal{I}\) with mean reward \(\mu_{*}\). The expected regret for any uniformly good causal bandit policy \(\pi\) satisfies_

\[\liminf_{T\rightarrow\infty}\frac{R_{T}^{\pi}\big{(}\mathcal{P}\big{)}}{\ln T} \geq C(\mathcal{P},\mathcal{G}),\]

_where \(C(\mathcal{P},\mathcal{M}_{\mathcal{G}})\) is the value of the optimization problem given below_

\[\tilde{\mathcal{O}}(\mathcal{P},\mathcal{M}_{\mathcal{G}}): \operatorname*{minimize}_{\eta_{\mathbf{s}}\geq 0,\forall\mathbf{s} \in\mathcal{I}_{-\mathbf{s}_{*}}}\sum_{\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{ *}}}\eta_{\mathbf{s}}\Delta_{\mathbf{s}}\] (12) \[\operatorname*{s.t.}\sum_{\mathbf{s}\in\mathcal{I}}\eta_{\mathbf{s }}D(P_{\mathbf{s}}\parallel\mathcal{P}_{\mathbf{s}}^{\prime})\geq 1,\forall P^{ \prime}\in\big{\{}\mathcal{P}^{\prime\prime}\in\mathcal{M}_{\mathcal{G}} \mid\mathbf{s}_{*}\notin\mathcal{I}_{*}^{\prime\prime}\big{\}},\] (13)

_where \(\mathcal{I}_{*}^{\prime\prime}\) is the set of optimal interventions according to \(\mathcal{P}^{\prime\prime}\)._

**Remark 3**.: _To interpret Lemma 5, (12) is a minimize of the regret, and its solution indicates an optimal exploration strategy. Each \(\mathbf{s}\in\mathcal{I}_{-\mathbf{s}}\), should be selected up to \(\eta_{\mathbf{s}}\ln T\) as \(T\rightarrow\infty\). In (13), \(\sum_{\mathbf{s}\in\mathcal{I}}\eta_{\mathbf{s}}D(P_{\mathbf{s}}\parallel \mathcal{P}_{\mathbf{s}}^{\prime\prime})\) is viewed as the distance generated between true interventional distribution tuple \(\mathcal{P}\) and an alternative \(\mathcal{P}^{\prime\prime}\in\mathcal{M}_{\mathcal{G}}\) by an exploration strategy \(\langle\eta_{\mathbf{s}}\rangle_{\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{*}}}\). So (13) requires \(\langle\eta_{\mathbf{s}}\rangle_{\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{*}}}\) must generate distance \(\geq 1\) for any \(\mathcal{P}^{\prime}\in\mathcal{M}_{\mathcal{G}}\) in which intervention \(\mathbf{s}_{*}\) is not optimal._Proof.: For any \(\mathcal{P}^{\prime}\in\{\mathcal{P}^{\prime\prime}\in\mathcal{M}_{\mathcal{G}}\mid \mathbf{s}_{*}\notin\mathcal{I}_{*}^{\prime\prime}\}\), the optimal intervention \(\mathbf{s}_{*}\) according to ground truth \(\mathcal{P}\) is not optimal in \(\mathcal{P}^{\prime}\). Recall \(\mathbb{P}^{\prime}\) is the measures on \(T\)-round plays of causal bandit with interventional distribution tuple \(\mathcal{P}^{\prime}\). Also let \(\mu_{*}^{\prime}\) be the optimal mean reward and \(\Delta_{\mathbf{s}}^{\prime}=\mu_{*}^{\prime}-\mu_{\mathbf{s}}^{\prime}\). For true causal model \(\mathcal{P}\), \(\mathbb{P}\) and \(\Delta_{\mathbf{s}}\) are defined similarly. For a fixed policy \(\pi\), let \(R_{T}\) and \(R_{T}^{\prime}\) be the expected regret from \(T\) round plays in \(\mathcal{P}\) and \(\mathcal{P}^{\prime}\) respectively. Letting \(\epsilon=\min\{\Delta_{\mathbf{s}^{\prime}},\,\Delta_{\mathbf{s}_{*}}^{\prime}\}\), we get

\[R_{T}\geq\frac{\epsilon T}{2}\mathbb{P}\Big{(}N_{T}(\mathbf{s}_{*})<\frac{T}{2 }\Big{)}\text{ and }R_{T}^{\prime}\geq\frac{\epsilon T}{2}\mathbb{P}^{\prime} \Big{(}N_{T}(\mathbf{s}_{*})\geq\frac{T}{2}\Big{)}.\]

Combining both inequalities, we get

\[\mathbb{P}\Big{(}N_{T}(\mathbf{s}_{*})<\frac{T}{2}\Big{)}+\mathbb{P}^{\prime} \Big{(}N_{T}(\mathbf{s}_{*})\geq\frac{T}{2}\Big{)}\leq\frac{2}{\epsilon T}(R_ {T}+R_{T}^{\prime}).\]

Applying Bretagnolle-Huber inequality in Lemma 4 by substituting event \(\{N_{T}(\mathbf{s}_{*})<\frac{T}{2}\}\) into \(E\),

\[D(\mathbb{P}\parallel\mathbb{P}^{\prime})\geq\ln\frac{\epsilon T}{4(R_{T}+R_{ T}^{\prime})}.\] (14)

With Lemma 3, we substitute \(\sum_{\mathbf{s}\in\mathcal{I}}\mathbb{E}[N_{T}(\mathbf{s})]D\left(P_{\mathbf{s }}\parallel P_{\mathbf{s}}^{\prime}\right)\) into \(D(\mathbb{P}\parallel\mathbb{P}^{\prime})\) and rearrange (14),

\[\frac{\sum_{\mathbf{s}\in\mathcal{I}}\mathbb{E}[N_{T}(\mathbf{s})]D\left(P_{ \mathbf{s}}\parallel P_{\mathbf{s}}^{\prime}\right)}{\ln T}\geq 1-\frac{\ln 4(R_{T}+R_{T}^{ \prime})/\epsilon}{\ln T}.\] (15)

According to Definition 3, that for any \(\alpha>0\), the regret for a uniformly good policy \(\pi\) satisfies

\[\lim_{T\to\infty}\frac{R_{T}}{T^{\alpha}}=0\text{ and }\lim_{T\to\infty} \frac{R_{T}^{\prime}}{T^{\alpha}}=0.\]

It follows from (15) that

\[\liminf_{T\to\infty}\frac{\sum_{\mathbf{s}\in\mathcal{I}}\mathbb{E}[N_{T}( \mathbf{s})]D\left(P_{\mathbf{s}}\parallel P_{\mathbf{s}}^{\prime}\right)}{\ln T }\geq 1,\]

which sets the constraints for \(\mathbb{E}[N_{T}(\mathbf{s})]/\ln T\) as \(T\to\infty\). Since \(R_{T}=\sum_{\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{*}}}\mathbb{E}[N_{T}( \mathbf{s})]\Delta_{\mathbf{s}}\), by replacing \(\mathbb{E}[N_{T}(\mathbf{s})]/\ln T\) with \(\eta_{\mathbf{s}}\), we finish the proof. 

Proof of Theorem 1.: We finish the proof by showing the equivalence of \(\tilde{\mathbb{O}}(\mathcal{P},\mathcal{M}_{\mathcal{G}})\) in Lemma 5 and \(\mathbb{O}(\mathcal{P},\mathcal{M}_{\mathcal{G}})\) in Theorem 1. First notice that there is no \(\eta_{\mathbf{s}_{*}}\) in (12). Accordingly, \(\eta_{\mathbf{s}_{*}}\) can be assigned to an arbitrarily large value to make \(\sum_{\mathbf{s}\in\mathcal{I}}\eta_{\mathbf{s}}D(P_{\mathbf{s}}\parallel P_{ \mathbf{s}}^{\prime})\geq 1\) if \(P_{\mathbf{s}_{*}}\neq P_{\mathbf{s}_{*}}^{\prime}\). Therefore, (13) is equivalent to

\[\sum_{\mathbf{s}\in\mathcal{I}_{-s_{*}}}\eta_{\mathbf{s}}D(P_{\mathbf{s}} \parallel P_{\mathbf{s}}^{\prime})\geq 1,\forall\mathcal{P}^{\prime}\in\big{\{} \mathcal{P}^{\prime\prime}\in\mathcal{M}_{\mathcal{G}}\mid P_{\mathbf{s}_{*}}=P _{\mathbf{s}_{*}}^{\prime\prime},\mathbf{s}_{*}\notin\mathcal{I}_{*}^{\prime \prime}\big{\}}.\]

It means if \(\mathbf{s}_{*}\) is not optimal in \(\mathcal{P}^{\prime}\), then \(\sum_{\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{*}}}\eta_{\mathbf{s}}D(P_{\mathbf{s }}\parallel P_{\mathbf{s}}^{\prime})\geq 1\). Its contrapositive statement is that if \(\sum_{\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{*}}}\eta_{\mathbf{s}}D(P_{\mathbf{s }}\parallel P_{\mathbf{s}}^{\prime})<1\), then \(\mathbf{s}_{*}\) is also optimal in \(\mathcal{P}^{\prime}\). Putting it into the mathematical expression, we get

\[\max_{\mathbf{s}\in\mathcal{I}_{-s_{*}}}\mu_{\mathbf{s}}^{\prime}\leq\mu_{*}, \forall\mathcal{P}^{\prime}\in\Big{\{}\mathcal{P}^{\prime\prime}\in\mathcal{M}_{ \mathcal{G}}\Bigm{|}\sum_{\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{*}}}\eta_{\mathbf{s }}D(P_{\mathbf{s}}\parallel P_{\mathbf{s}}^{\prime\prime})<1,P_{\mathbf{s}_{*}}^{ \prime\prime}=P_{\mathbf{s}_{*}}\Big{\}},\]

where we use the fact \(\mu_{*}=\mu_{*}^{\prime}\) since \(P_{\mathbf{s}_{*}}=P_{\mathbf{s}_{*}}^{\prime}\). The proof is concluded. 

## Appendix B Finite time Regret Analysis of SCM-AAM in Theorem 2

### Guarantee with Forced Exploration

The SCM-AAM algorithm maintains a counter of exploration steps \(N_{t}^{\mathrm{e}}\). In expiration steps, if \(\min_{\mathbf{s}\in\mathcal{I}}N_{t}(\mathbf{s})\leq\epsilon N_{t}^{\mathrm{e}}\), the SCM-AAM algorithm generates forced exploration by selecting one of the least selected intervention. Lemma 6 shows that the forced exploration mechanism ensures every intervention is persistently selected in exploration phases.

**Lemma 6**.: _Let \(\mathcal{R}=\{t_{i}\}_{i\geq 1}\) be the sequence of rounds that SCM-AAM enters the exploration phase. For any time \(t_{i}\in\mathcal{R}\), it satisfies that_

\[\min_{\mathbf{s}\in\mathcal{I}}N_{t}(\mathbf{s})\geq\frac{\epsilon i}{2}.\]

Proof.: We first note that the following two facts are true

* \(\min_{\mathbf{s}\in\mathcal{I}}N_{t}(\mathbf{s})\) is non-decreasing over \(t\).
* If \(\min_{\mathbf{s}\in\mathcal{I}}N_{t_{i}}(\mathbf{s})\leq\epsilon N_{t_{i}}^{ \mathrm{e}}\), then \(\min_{\mathbf{s}\in\mathcal{I}}N_{t_{i+|\mathcal{I}|}}(\mathbf{s})\geq\min N_{ t_{i}}(\mathbf{s})+1\).

Since \(N_{t}(\mathbf{s})\) is non-decreasing over \(t\), the first statement is true. The second statement is true since otherwise, \(\underline{\mathbf{S}}_{t}\) is selected by forced exploration at least \(|\mathcal{I}|\) times without increasing \(\min_{\mathbf{s}\in\mathcal{I}}N_{t}(\mathbf{s})\). With these two facts, we are ready to provide proof by contradiction. Suppose there exists \(i\in\{1,\ldots,|\mathcal{R}|\}\) such that

\[\min_{\mathbf{s}\in\mathcal{I}}N_{t_{i}}(\mathbf{s})<\frac{\epsilon i}{2}.\]

According to the first fact, we have \(\forall j\geq i/2\),

\[\min_{\mathbf{s}\in\mathcal{I}}N_{t_{j}}(\mathbf{s})\leq\min_{\mathbf{s}\in \mathcal{I}}N_{t_{i}}(\mathbf{s})\leq\epsilon j.\]

According to SCM-AAM, \(\epsilon\in(0,1/\left|\mathcal{I}\right|)\). Then we apply the second fact,

\[\min_{\mathbf{s}\in\mathcal{I}}N_{t_{i}}(\mathbf{s})\geq\frac{i-j}{|\mathcal{ I}|}\geq\frac{\epsilon i}{2},\]

which creates a contradiction. 

### Supporting Lemmas (Concentration Inequalities)

The following concentration inequalities are instrumental for regret analysis. For bandit with feedback drawn from arbitrary discrete distributions, a concentration bound on the empirical distribution is presented in [35, Lemma 6]. In the structured causal bandit problem with unobserved confounders, the actions space is \(\mathcal{I}\), and the discrete support of feedback is \(\Omega(\mathbf{V})\). At each time \(t\), for each intervention \(\mathbf{s}\in\mathcal{I}\), recall \(\bar{P}_{\mathbf{s},t}\) is the empirical interventional distribution of \(\mathbf{V}\) and \(N_{t}(\mathbf{s})\) is the number of times the intervention \(do(\mathbf{S}=\mathbf{s})\) is taken till \(t\). For each intervention \(\mathbf{s}\in\mathcal{I}\), the true interventional distribution is \(P_{\mathbf{s}}\).

**Lemma 7** (Concentration Inequality for Information Distance [35]).: _Let \(\delta\geq|\mathcal{I}|\left(|\Omega(V)|-1\right)\). Then for any \(t>0\),_

\[\mathbb{P}\left[\sum_{\mathbf{s}\in\mathcal{I}}N_{t}(\mathbf{s})D(\bar{P}_{ \mathbf{t},\mathbf{s}}\parallel P_{\mathbf{s}})\geq\delta\right]\leq\left( \frac{\delta\lceil\delta\ln t+1\rceil 2e}{|\mathcal{I}|\left(|\Omega(V)|-1\right)} \right)^{|\mathcal{I}|\left(|\Omega(V)|-1\right)}\exp(1-\delta).\]

Illowing Corollary 8 is a result of Lemma 7.

**Corollary 8**.: _For any \(\gamma>0\), there exists suitably large universal constant \(c\) such that_

\[\sum_{t=1}^{\infty}\mathbb{P}\left(\sum_{\mathbf{s}\in\mathcal{I}}N_{t}( \mathbf{s})D(\bar{P}_{\mathbf{s},t}\parallel P_{\mathbf{s}})\geq(1+\gamma)\ln t \right)\leq c.\]

Proof.: We select \(t^{\prime}\) such that \((1+\gamma)\ln t^{\prime}\geq|\mathcal{I}|\left(|\Omega(V)|-1\right)\). We apply Lemma 7 to get

\[\sum_{t=1}^{\infty}\mathbb{P}\left(\sum_{\mathbf{s}\in\mathcal{I} }N_{t}(\mathbf{s})D(\bar{P}_{\mathbf{s},t}\parallel P_{\mathbf{s}})\geq(1+ \gamma)\ln t\right)\] \[\leq t^{\prime}+\sum_{t\geq t^{\prime}}\mathbb{P}\left[\sum_{\mathbf{s }\in\mathcal{I}}N_{t}(\mathbf{s})D(\bar{P}_{\mathbf{t},\mathbf{s}}\parallel P _{\mathbf{s}})\geq(1+\gamma)\ln t\right]\] \[\leq t^{\prime}+\sum_{t\geq t^{\prime}}\frac{e}{t^{1+\gamma}}\bigg{(} \frac{(1+\gamma)\ln t\lceil(1+\gamma)(\ln t)^{2}+1\rceil 2e}{|\mathcal{I}|\left(|\Omega(V)|-1 \right)}\bigg{)}^{|\mathcal{I}|\left(|\Omega(V)|-1\right)},\]

which remains finite. We conclude our proof.

Lemma 9 proposed in [9, Lemma 4.3] extends Hoeffding's inequality to provide an upper bound on the deviation of the empirical mean sampled at a stopping time. The time SCM-AAM enters the exploration phase is a stopping time, and the time each intervention is selected is also a stopping time. It will be used in multiple circumstances in regret analysis.

**Lemma 9** (Extension of Hoeffding's Inequality [9]).: _Let \(\{Z_{t}\}_{t\in\mathbb{N}_{>0}}\) be a sequence of independent random variables with values in \([0,1]\). Let \(\mathcal{F}_{t}\) be the \(\sigma\)-algebra such that \(\sigma(Z_{1},\ldots,Z_{t})\subset\mathcal{F}_{t}\) and the filtration \(\mathcal{F}=\{\mathcal{F}_{t}\}_{t\in\mathbb{N}_{>0}}\). Consider \(s\in\mathbb{N}\), and \(T\in\mathbb{N}_{>0}\). We define \(S_{t}=\sum_{j=1}^{t}\epsilon_{j}(Z_{j}-\mathbb{E}[Z_{j}])\), where \(\epsilon_{j}\in\{0,1\}\) is a \(\mathcal{F}_{j-1}\)-measurable random variable. Further define \(N_{t}=\sum_{j=1}^{t}\epsilon_{j}\). Define \(\phi\in\{1,\ldots,T+1\}\) a \(\mathcal{F}\)-stopping time such that either \(N_{\phi}\geq s\) or \(\phi=T+1\). Then we have that_

\[P[S_{\phi}\geq N_{\phi}\delta]\leq\exp(-2s\delta^{2}).\]

_As a consequence,_

\[P[|S_{\phi}|\geq N_{\phi}\delta]\leq 2\exp(-2s\delta^{2}).\]

In Corollary 10, we extend Lemma 9 to bound the \(L_{1}\) deviation of the empirical distribution.

**Corollary 10** (\(L_{1}\) deviation of the empirical distribution).: _Let \(\mathcal{A}\) denote finite set \(\{1,\ldots,a\}\). For two probability distribution \(Q\) and \(Q^{\prime}\) on \(\mathcal{A}\), let \(\left\|Q^{\prime}-Q\right\|_{1}=\sum_{k=1}^{a}|Q^{\prime}(k)-Q(k)|\). Let \(X_{t}\in\mathcal{A}\) be a sequence of independent random variables with common distribution \(Q\). Let \(\mathcal{F}_{t}\) be the \(\sigma\)-algebra such that \(\sigma(X_{1},\ldots,X_{t})\subset\mathcal{F}_{t}\) and the filtration \(\mathcal{F}=\{\mathcal{F}_{t}\}_{t\in\mathbb{N}_{>0}}\). Let \(\epsilon_{t}\in\{0,1\}\) is a \(\mathcal{F}_{t-1}\)-measurable random variable. We define_

\[N_{t}=\sum_{j=1}^{t}\epsilon_{j},S_{t}(i)=\sum_{j=1}^{t}\epsilon_{j}\mathbbm{1 }\{X_{j}=i\},\text{ and }\bar{Q}_{t}(i)=\frac{S_{t}(i)}{N_{t}},\forall i\in\mathcal{A}.\]

_For \(s\in\mathbb{N}\), and \(T\in\mathbb{N}_{>0}\), let \(\phi\in\{1,\ldots,T+1\}\) be a \(\mathcal{F}\)-stopping time such that either \(N_{\phi}\geq s\) or \(\phi=T+1\). Then we have_

\[P\left(\left\|\bar{Q}_{\phi}-Q\right\|_{1}\geq\delta\right)\leq(2^{a}-2)\exp \Big{(}\frac{-s\delta^{2}}{2}\Big{)}.\]

Proof.: It is known that for any distribution \(Q^{\prime}\) on \(\mathcal{A}\),

\[\left\|Q^{\prime}-Q\right\|_{1}=2\max_{A\subseteq\mathcal{A}}(Q^{\prime}(A)- Q(A)).\]

Then we apply a union bound to get

\[P\left(\left\|\bar{Q}_{\phi}-Q\right\|_{1}\geq\delta\right) \leq\sum_{A\subseteq\mathcal{A}}P\left(\bar{Q}_{\phi}(A)-Q(A)\geq \frac{\delta}{2}\right)\] \[\leq\sum_{A\subseteq\mathcal{A}:A\neq\mathcal{A}\text{ or }\emptyset}P \left(\bar{Q}_{\phi}(A)-Q(A)\geq\frac{\delta}{2}\right)\] \[\leq(2^{a}-2)\exp\Big{(}\frac{-s\delta^{2}}{2}\Big{)},\]

which concludes the proof. 

### Supporting Lemmas (Continuity of Upper Confidence Bound)

For a constrained optimization problem, Berge's maximum theorem imposes additional restrictions on the objective function and constraint set to guarantee that the problem's solution varies smoothly with parameters. Specifically, it studies the following optimization problem

\[V(\theta)=\max_{x\in G(\theta)}F(x,\theta),\]

where \(F:X\times\Theta\rightarrow\mathbb{R}\) the objective function, and \(G(\theta)\) is the set of feasible values for \(X\) given \(\theta\) defined by the multi-valued correspondence \(G:\Theta\Rightarrow X\). We assume that \(G(\theta)\subseteq X\) is compact for all \(\theta\in\Theta\).

**Definition 4**.: _The compact valued correspondence \(G\) is continuous at \(\theta\in\Omega(\theta)\) if it is both upper and lower hemicontinuous at \(\theta\). The upper and lower hemicontinuous can be verified as below._

* Upper hemicontinuous: _the correspondence_ \(G\) _is upper hemicontinuous at_ \(\theta\in\Theta\) _if_ \(G(\theta)\) _is nonempty and if, for every sequence_ \(\{\theta_{j}\}\) _with_ \(\theta_{j}\to\theta\) _and every sequence_ \(\{x_{j}\}\) _with_ \(x_{j}\in G(\theta_{j})\) _for all_ \(j\)_, there exists a convergent subsequence_ \(\{x_{j_{k}}\}\) _such that_ \(x_{j_{k}}\to x\in G(\theta)\)_._
* Lower hemicontinuous: _the correspondence_ \(G\) _is lower hemicontinuous at_ \(\theta\in\Theta\) _if_ \(G(\theta)\) _is nonempty and if, for every_ \(x\in G(\theta)\) _and every sequence_ \(\{\theta_{j}\}\) _such that_ \(\theta_{j}\to\theta\)_, there is a_ \(J\geq 1\) _and a sequence_ \(\{x_{j}\}\) _such that_ \(x_{j}\in G(\theta_{j})\) _for all_ \(j\geq J\) _and_ \(x_{j}\to x\)_._

_The correspondence \(G\) is continuous if it is continuous at every \(\theta\in\Omega(\theta)\)._

**Lemma 11** (Berge's Maximum Theorem [16, Ch. 17.31]).: _Let \(X\subseteq\mathbb{R}^{n}\) and \(\Theta\subseteq\mathbb{R}^{m}\). Let \(F:X\times\Theta\to\mathbb{R}\) be a continuous function, and let \(G:\Theta\to X\) be a compact valued and continuous correspondence. Then the maximum value function_

\[V(\theta)=\max_{x\in G(\theta)}F(x,\theta)\]

_is well-defined and continuous, and the optimal correspondence_

\[x^{*}(\theta)=\{x\in G(\theta)\mid F(x,\theta)\}\]

_is nonempty, compact valued, and upper hemicontinuous._

We want to leverage Berge's maximum theorem to prove the UCB index \(\tilde{\mu}_{\mathbf{s}}(\mathcal{C}_{\mathbf{s}}(\eta_{\mathbf{s}},\mathcal{ P}))=\sum_{\mathbf{v}\in\Omega(\mathbf{V})}v_{n}\max_{\mathcal{P}^{\prime}\in \mathcal{C}_{\mathbf{s}}(\eta_{\mathbf{s}},\mathcal{P})}P^{\prime}_{\mathbf{s }}(\mathbf{v})\) is continuous with respect to \(\eta_{\mathbf{s}}\) and \(\mathcal{P}\). Recall in Theorem 2, for given interventional distribution tuple \(\mathcal{P}\in\mathcal{M}_{\mathcal{G}}\) and \(\eta_{\mathbf{s}}>0\), \(\mathcal{C}_{\mathbf{s}}(\eta_{\mathbf{s}},\mathcal{P})\) is the set of feasible interventional distribution tuples \(\mathcal{P}^{\prime}\), and it is defined as

\[\mathcal{C}_{\mathbf{s}}(\eta_{\mathbf{s}},\mathcal{P})=\bigg{\{}\mathcal{P} ^{\prime}\in\mathcal{M}_{\mathcal{G}}\ \Big{|}\ \eta_{\mathbf{s}}D(P_{\mathbf{s}}\parallel P^{\prime}_{\mathbf{s}})+\epsilon \eta_{\mathbf{s}}/2\sum_{\mathbf{x}\in\mathcal{I}_{-\mathbf{s}}}D(P_{\mathbf{ x}}\parallel P^{\prime}_{\mathbf{x}})\leq 1\bigg{\}}.\]

**Lemma 12**.: _For any \(\mathbf{s}\in\mathcal{I}\), the correspondence \(\mathcal{C}_{\mathbf{s}}:\mathbb{R}_{\geq 0}\times\mathcal{M}_{\mathcal{G}} \Rightarrow\mathcal{M}_{\mathcal{G}}\) is a compacted valued continuous correspondence._

Proof.: The space of interventional distribution tuples \(\mathcal{M}_{\mathcal{G}}\) is compacted as has been explained below Definition 2. Besides, for any \(\mathcal{P}\in\mathcal{M}_{\mathcal{G}}\) and \(\eta_{\mathbf{s}}\geq 0\), the constrain \(\eta_{\mathbf{s}}D(P_{\mathbf{s}}\parallel P^{\prime}_{\mathbf{s}})+\epsilon \eta_{\mathbf{s}}/2\sum_{\mathbf{x}\in\mathcal{I}_{-\mathbf{s}}}D(P_{\mathbf{ x}}\parallel P^{\prime}_{\mathbf{x}})\leq 1\) defines a closed compact set for \(\mathcal{P}^{\prime}=\langle P^{\prime}_{\mathbf{s}}\rangle_{\mathbf{s}\in \mathcal{I}}\). Since the intersection of two compact sets is compact if one of them is also closed, \(\mathcal{C}_{\mathbf{s}}(\eta_{\mathbf{s}},\mathcal{P})\) is compact. It remains to show \(\mathcal{C}_{\mathbf{s}}\) is both upper hemicontinuous and lower hemicontinuous.

**Upper hemicontinuous:** Fix arbitrary \(\eta_{\mathbf{s}}\geq 0\) and \(\mathcal{P}\in\mathcal{M}_{\mathcal{G}}\), and let \(\{(\eta_{\mathbf{s},i},\mathcal{P}_{i})\}_{i\in\mathbb{N}_{>0}}\) be a sequence such that \(\eta_{\mathbf{s},i}\geq 0\), \(\mathcal{P}_{i}\in\mathcal{M}_{\mathcal{G}}\), and \((\eta_{\mathbf{s},i},\mathcal{P}_{i})\to(\eta_{\mathbf{s}},\mathcal{P})\). Notice that each \(\mathcal{C}_{\mathbf{s}}(\eta_{\mathbf{s},i},\mathcal{P}_{i})\) is non-empty since \(\mathcal{P}_{i}\in\mathcal{C}_{\mathbf{s}}(\eta_{\mathbf{s},i},\mathcal{P}_{i})\). Therefore, there is a sequence \(\{\mathcal{P}^{\prime}_{i}\}_{i\in\mathbb{N}_{>0}}\) such that \(\mathcal{P}^{\prime}_{i}\in\mathcal{C}_{\mathbf{s}}(\eta_{\mathbf{s},i}, \mathcal{P}_{i})\). Since \(\eta_{\mathbf{s},i}\to\eta_{\mathbf{s}}\), there exists a closed and bounded set \(\Theta\subset\mathbb{R}_{\geq 0}\) such that \(\eta_{\mathbf{s}}\in\Theta\), and for some \(N\geq 1\), \(\eta_{\mathbf{s},i}\in\Theta\) for all \(i\geq N\). Moreover, \(\mathcal{P}_{i}\) and \(\mathcal{P}^{\prime}_{i}\) are also bounded since every interventional probability tuple in \(\mathcal{M}_{\mathcal{G}}\) is bounded. Since each \((\eta_{\mathbf{s},i},\mathcal{P}_{i},\mathcal{P}^{\prime}_{i},)\) lies in a closed and bounded subset for \(i\geq N\), by Bolzano-Weierstrass theorem, the sequence \(\{(\eta_{\mathbf{s},i},\mathcal{P}_{i},\mathcal{P}^{\prime}_{i})\}_{i\in \mathbb{N}_{>0}}\) has a convergent subsequence \(\{(\eta_{\mathbf{s},i_{k}},\mathcal{P}_{i_{k}},\mathcal{P}^{\prime}_{i_{k}})\}_{k \in\mathbb{N}_{>0}}\) with limit point \((\eta_{\mathbf{s}},\mathcal{P},\mathcal{P}^{\prime})\). Since each element of this sequence satisfies

\[h(\eta_{\mathbf{s},i_{k}},\mathcal{P}_{i_{k}},\mathcal{P}^{\prime}_{i_{k}}):= \eta_{\mathbf{s},i_{k}}D(P_{\mathbf{s},i_{k}}\parallel P^{\prime}_{\mathbf{s },i_{k}})+\epsilon\eta_{\mathbf{s},i_{k}}/2\sum_{\mathbf{x}\in\mathcal{I}_{- \mathbf{s}}}D(P_{\mathbf{x},i_{k}}\parallel P^{\prime}_{\mathbf{x},i_{k}})\leq 1,\]

the inequality holds at the limit point \((\eta_{\mathbf{s}},\mathcal{P}^{\prime})\), i.e.,

\[h(\eta_{\mathbf{s}},\mathcal{P},\mathcal{P}^{\prime})=\eta_{\mathbf{s}}D(P_{ \mathbf{s}}\parallel P^{\prime}_{\mathbf{s}})+\epsilon\eta_{\mathbf{s}}/2 \sum_{\mathbf{x}\in\mathcal{I}_{-\mathbf{s}}}D(P_{\mathbf{x}}\parallel P^{ \prime}_{\mathbf{x}})\leq 1,\]

due to that \(h\) is a continuous function. We get \(\mathcal{C}_{\mathbf{s}}\) is upper hemicontinuous.

**Lower hemicontinuous:** For arbitrary \(\eta_{\mathbf{s}}\geq 0\) and \(\mathcal{P}\in\mathcal{M}_{\mathcal{G}}\), we fix \(\mathcal{P}^{\prime}\in\mathcal{C}_{\mathbf{s}}(\eta_{\mathbf{s}},\mathcal{P})\). Let \(\{(\eta_{\mathbf{s},i},\mathcal{P}_{i})\}_{i\in\mathbb{N}_{>0}}\) be a sequence such that \(\eta_{\mathbf{s},i}\geq 0\), \(\mathcal{P}_{i}=\langle P_{\mathbf{s},i}\rangle_{\mathbf{s}\in\mathcal{I}}\in \mathcal{M}_{\mathcal{G}}\), and \((\eta_{\mathbf{s},i},\mathcal{P}_{i})\to(\eta_{\mathbf{s}},\mathcal{P})\). We show there exists sequence \(\{\mathcal{P}^{\prime}_{i}\}_{i\in\mathbb{N}_{>0}}\) such that \(\mathcal{P}^{\prime}_{i}\in\mathcal{C}_{\mathbf{s}}(\eta_{\mathbf{s},i}, \mathcal{P}_{i})\) for allToward this end, for each \((\eta_{s,i},\mathcal{P}_{i})\), if \(h(\eta_{s,i},\mathcal{P}_{i},\mathcal{P}^{\prime})\leq 1\), we set \(\mathcal{P}^{\prime}_{i}=\mathcal{P}^{\prime}\) so that \(\mathcal{P}^{\prime}_{i}\in\mathcal{C}_{\text{s}}(\eta_{s,i},\mathcal{P}_{i})\). If \(c_{i}:=h(\eta_{s,i},\mathcal{P}_{i},\mathcal{P}^{\prime})>1\), we construct \(\mathcal{P}^{\prime}_{i}=\langle P^{\prime}_{s,i}\rangle_{\mathbf{s}\in \mathcal{I}}\) as the following. With Definition 2, there exist \(\{\mathbf{p}^{i}_{1},\ldots,\mathbf{p}^{i}_{n_{i}(\mathcal{G})}\}\) and \(\{\mathbf{p}^{\prime}_{1},\ldots,\mathbf{p}^{\prime}_{n_{i}(\mathcal{G})}\}\) associated with the interventional distribution tuple \(\mathcal{P}_{i}\) and \(\mathcal{P}^{\prime}\) respectively. Let \(\mathcal{P}^{\prime}_{i}\) be associated with

\[\left\{\frac{c_{i}-1}{c_{i}}\mathbf{p}^{i}_{1}+\frac{1}{c_{i}}\mathbf{p}^{ \prime}_{1},\ldots,\frac{c_{i}-1}{c_{i}}\mathbf{p}^{i}_{n_{i}(\mathcal{G})}+ \frac{1}{c_{i}}\mathbf{p}^{\prime}_{n_{i}(\mathcal{G})}\right\},\] (16)

so that \(\mathcal{P}^{\prime}_{i}\in\mathcal{M}_{\mathcal{G}}\). For each \(\mathbf{s}\in\mathcal{I}\) and \(\mathbf{v}\in\Omega(\mathbf{V})\), we have

\[P_{\mathbf{s},i}(\mathbf{v})\ln\frac{P_{\mathbf{s},i}(\mathbf{v} )}{P^{\prime}_{\mathbf{s},i}(\mathbf{v})}\] \[= P_{\mathbf{s},i}(\mathbf{v})\ln\frac{\prod_{j=1}^{n_{i}( \mathcal{G})}b^{\top}_{\mathcal{G},\mathbf{s}[\mathbf{V}^{j}]}(\mathbf{v}^{j},\mathfrak{pa}_{\mathbf{V}^{j}})\mathbf{p}^{i}_{j}}{\prod_{j=1}^{n_{i}( \mathcal{G})}b^{\top}_{\mathcal{G},\mathbf{s}[\mathbf{V}^{j}]}(\mathbf{v}^{j},\mathfrak{pa}_{\mathbf{V}^{j}})\big{[}\frac{c_{i}-1}{c_{i}}\mathbf{p}^{i}_{j} +\frac{1}{c_{i}}\mathbf{p}^{\prime}_{j}\big{]}}\] \[= \prod_{j=1}^{n_{i}(\mathcal{G})}b^{\top}_{\mathcal{G},\mathbf{s}[ \mathbf{V}^{j}]}(\mathbf{v}^{j},\mathfrak{pa}_{\mathbf{V}^{j}})\mathbf{p}^{i}_ {j}\sum_{k=1}^{n_{k}(\mathcal{G})}\frac{b^{\top}_{\mathcal{G},\mathbf{s}[ \mathbf{V}^{k}]}(\mathbf{v}^{k},\mathfrak{pa}_{\mathbf{V}^{k}})\mathbf{p}^{i}_ {k}}{b^{\top}_{\mathcal{G},\mathbf{s}[\mathbf{V}^{k}]}(\mathbf{v}^{k}, \mathfrak{pa}_{\mathbf{V}^{k}})\big{[}\frac{c_{i}-1}{c_{i}}\mathbf{p}^{i}_{k}+ \frac{1}{c_{i}}\mathbf{p}^{\prime}_{k}\big{]}}\] \[= \sum_{k=1}^{n_{i}(\mathcal{G})}\prod_{j\neq k}b^{\top}_{\mathcal{G },\mathbf{s}[\mathbf{V}^{j}]}(\mathbf{v}^{j},\mathfrak{pa}_{\mathbf{V}^{j}}) \mathbf{p}^{i}_{j}\underbrace{b^{\top}_{\mathcal{G},\mathbf{s}[\mathbf{V}^{k}] }(\mathbf{v}^{k},\mathfrak{pa}_{\mathbf{V}^{k}})\mathbf{p}^{i}_{k}\ln\frac{b^ {\top}_{\mathcal{G},\mathbf{s}[\mathbf{V}^{k}]}(\mathbf{v}^{k},\mathfrak{pa}_{ \mathbf{V}^{k}})\mathbf{p}^{i}_{k}}{b^{\top}_{\mathcal{G},\mathbf{s}[\mathbf{ V}^{k}]}(\mathbf{v}^{k},\mathfrak{pa}_{\mathbf{V}^{k}})\big{[}\frac{c_{i}-1}{c_{i}} \mathbf{p}^{i}_{k}+\frac{1}{c_{i}}\mathbf{p}^{\prime}_{k}\big{]}}_{(a)}\] \[\leq \frac{1}{c_{i}}\sum_{k=1}^{n_{i}(\mathcal{G})}\prod_{j=1}^{n_{i}( \mathcal{G})}b^{\top}_{\mathcal{G},\mathbf{s}[\mathbf{V}^{j}]}(\mathbf{v}^{j},\mathfrak{pa}_{\mathbf{V}^{j}})\mathbf{p}^{i}_{j}\ln\frac{b^{\top}_{\mathcal{G },\mathbf{s}[\mathbf{V}^{k}]}(\mathbf{v}^{k},\mathfrak{pa}_{\mathbf{V}^{k}}) \mathbf{p}^{i}_{k}}{b^{\top}_{\mathcal{G},\mathbf{s}[\mathbf{V}^{k}]}(\mathbf{ v}^{k},\mathfrak{pa}_{\mathbf{V}^{k}})\mathbf{p}^{i}_{k}}=\frac{1}{c_{i}}P_{ \mathbf{s}}(\mathbf{v})\ln\frac{P_{\mathbf{s}}(\mathbf{v})}{P^{\prime}_{\mathbf{ s}}(\mathbf{v})},\]

where we apply the log sum inequality to \((a)\). Thus, for any \(\mathbf{s}\in\mathcal{I}\), we have

\[D(P_{\mathbf{s},i}\parallel P^{\prime}_{\mathbf{s},i})=\sum_{\mathbf{v}\in \Omega(\mathbf{V})}P_{\mathbf{s}}(\mathbf{v})\ln\frac{P_{\mathbf{s}}(\mathbf{v })}{P^{\prime}_{\mathbf{s},i}(\mathbf{v})}\leq\frac{1}{c_{i}}D(P_{\mathbf{s}} \parallel P^{\prime}_{\mathbf{s}}).\]

As a result, \(h(\eta_{\mathbf{s},i},\mathcal{P}_{i},\mathcal{P}^{\prime}_{i})\leq\frac{1}{c_ {i}}h(\eta_{\mathbf{s},i},\mathcal{P}_{i},\mathcal{P}^{\prime})=1\), which means \(\mathcal{P}^{\prime}_{i}\in\mathcal{C}_{\text{s}}(\eta_{\mathbf{s},i},\mathcal{P }_{i})\).

It remains to show \(\mathcal{P}^{\prime}_{i}\to\mathcal{P}^{\prime}\) by construction. If \(h(\eta_{\mathbf{s}},\mathcal{P},\mathcal{P}^{\prime})<1\), in the sequence \(\{(\eta_{\mathbf{s},i},\mathcal{P}_{i})\}_{i\in\mathbb{N}_{>0}}\), there exist finite instances such that \(h(\eta_{\mathbf{s},i},\mathcal{P}_{i},\mathcal{P}^{\prime})>1\). Otherwise, \(h(\eta_{\mathbf{s}},\mathcal{P},\mathcal{P}^{\prime})=1\) indicates \(c_{i}\to 1\), resulting in (16) \(\to\{\mathbf{p}^{\prime}_{1},\ldots,\mathbf{p}^{\prime}_{n_{i}(\mathcal{G})}\}\). Therefore, \(\mathcal{P}^{\prime}_{i}\to\mathcal{P}^{\prime}\) and \(\mathcal{C}_{\text{s}}\) is lower hemicontinuous. 

With correspondence \(\mathcal{C}_{\text{s}}\) being continuous, Lemma 13 gives a continuous property of \(\tilde{\mu}_{\text{s}}(\mathcal{C}_{\text{s}}(\eta_{\text{s}},\mathcal{P}))\).

**Lemma 13**.: _For any \(\mathcal{P}\in\mathcal{M}_{\mathcal{G}}\) and for any any \(\mathbf{s}\in\mathcal{I}\), the UCB index \(\tilde{\mu}_{\text{s}}(\mathcal{C}_{\text{s}}(\eta_{\text{s}},\mathcal{P}))\) is a is continuous monotonically nonincreasing function of \(\eta_{\text{s}}\geq 0\)._

Proof.: Since \(\mathcal{C}_{\text{s}}\) is a compacted valued continuous correspondence, we apply Berge's maximum theorem in Lemma 11 to get that \(\max_{\mathcal{P}^{\prime}\in\mathcal{C}_{\text{s}}(\eta_{\text{s}},\mathcal{P} )}P^{\prime}_{\mathbf{s}}(\mathbf{v})\) is continuous with \(\eta_{\text{s}}\). So \(\tilde{\mu}_{\text{s}}(\mathcal{C}_{\text{s}}(\eta_{\text{s}},\mathcal{P}))= \sum_{\mathbf{v}\in\Omega(\mathbf{V})}v_{n}\max_{\mathcal{P}^{\prime}\in \mathcal{C}_{\text{s}}(\eta_{\text{s}},\mathcal{P})}P^{\prime}_{\mathbf{s}}( \mathbf{v})\) is also continuous with \(\eta_{\text{s}}\). For any \(a>b>0\), since \(\mathcal{C}_{\text{s}}(a,\mathcal{P})\subseteq\mathcal{C}_{\text{s}}(b,\mathcal{P})\), \(\tilde{\mu}_{\text{s}}(\mathcal{C}_{\text{s}}(a,\mathcal{P}))\leq\tilde{\mu}_{ \text{s}}(\mathcal{C}

_in which \(\mathcal{C}_{\mathbf{s}}(\eta_{\mathbf{s}},\mathcal{P}):=\left\{\mathcal{P}^{\prime} \in\mathcal{M}_{\mathcal{G}}\mid\eta_{\mathbf{s}}D(P_{\mathbf{s}}\parallel P^{ \prime}_{\mathbf{s}})+\epsilon\eta_{\mathbf{s}}/2\sum_{\mathbf{x}\in\mathcal{I }_{-\mathbf{s}}}D(P_{\mathbf{x}}\parallel P^{\prime}_{\mathbf{x}})\leq 1\right\}\). For any \(\kappa>0\), there exists \(\delta(\kappa)\) such that for any \(\bar{\mathcal{P}}=\langle\bar{P}_{\mathbf{s}}\rangle_{\mathbf{s}\in\mathcal{I }}\in\mathcal{M}_{\mathcal{G}}\), if \(\forall:\mathbf{s}\in\mathcal{I}:\left\|P_{\mathbf{s}}-P_{\mathbf{s}}\right\|_ {1}\leq\delta(\kappa)\),_

* _Intervention_ \(\mathbf{s}_{\ast}\) _is also optimal according to_ \(\bar{\mathcal{P}}\)_, i.e.,_ \(\mu_{\mathbf{s}_{\ast}}(\bar{\mathcal{P}})\geq\mu_{\mathbf{s}}(\bar{\mathcal{ P}}),\forall\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{\ast}}\)_._
* _For any_ \(\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{\ast}}\)_,_ \(\zeta_{\mathbf{s}}(\bar{\mathcal{P}})\leq(1+\kappa)\zeta_{\mathbf{s}}( \mathcal{P})\)_._

Proof.: For each \(\mathbf{s}\in\mathcal{I}\), it can be seen that \(\lim_{\eta_{\mathbf{s}\rightarrow\infty}}\tilde{\mu}_{\mathbf{s}}(\mathcal{C} _{\mathbf{s}}(\eta_{\mathbf{s}},\mathcal{P}))=\mu_{\mathbf{s}}(\mathcal{P})\) since \(\mathcal{P}^{\prime}\) in \(\mathcal{C}_{\mathbf{s}}(\eta_{\mathbf{s}},\mathcal{P})\) is required to be identical to \(\mathcal{P}\) as \(\eta_{\mathbf{s}\rightarrow\infty}\). For each \(\mathbf{s}\in\mathcal{I}_{-\mathbf{s}}\), if \(\tilde{\mu}_{\mathbf{s}}(\mathcal{C}_{\mathbf{s}}(0,\mathcal{P}))>\mu_{\mathbf{ s}}(\mathcal{P})\), since \(\tilde{\mu}_{\mathbf{s}}(\mathcal{C}_{\mathbf{s}}(\eta_{\mathbf{s}},\mathcal{P}))\) is a continuous monotonically nonincreasing function of \(\eta_{\mathbf{s}}\) according to Lemma 13, there exists \(\eta_{\mathbf{s}}>0\) such that \(\tilde{\mu}_{\mathbf{s}}(\mathcal{C}_{\mathbf{s}}(\eta_{\mathbf{s}},\mathcal{ P}))=\mu_{\mathbf{s}}(\mathcal{P})\). Setting \(\eta_{\mathbf{s}}^{\ast}=\max\{\eta_{\mathbf{s}}\geq 0\mid\tilde{\mu}_{ \mathbf{s}}(\mathcal{C}_{\mathbf{s}}(\eta_{\mathbf{s}},\mathcal{P}))=\mu_{ \mathbf{\ast}}(\mathcal{P})\}\), we have \(\eta_{\mathbf{s}}^{\ast}>0\) and \(\zeta_{\mathbf{s}}(\mathcal{P})=\eta_{\mathbf{s}}^{\ast}\). For any \(\kappa>0\), \(\tilde{\mu}_{\mathbf{s}}(\mathcal{C}_{\mathbf{s}}(\sqrt{1+\kappa}\eta_{ \mathbf{s}}^{\ast},\mathcal{P}))<\mu_{\mathbf{\ast}}(\mathcal{P})\) due to the definition of \(\eta_{\mathbf{s}}^{\ast}\). We define

\[\delta^{\prime}_{\mathbf{s}}(\kappa)=2\mu_{\mathbf{\ast}}(\mathcal{P})-2 \tilde{\mu}_{\mathbf{s}}(\mathcal{C}_{\mathbf{s}}(\sqrt{1+\kappa}\eta_{ \mathbf{s}}^{\ast},\mathcal{P}))>0.\] (18)

From Lemma 12, \(\mathcal{C}_{\mathbf{s}}\) is a continuous correspondence. Since \(D(P_{\mathbf{s}}\parallel P^{\prime}_{\mathbf{s}})+\epsilon/2\sum_{\mathbf{x} \in\mathcal{I}_{-\mathbf{s}}}D(P_{\mathbf{x}}\parallel P^{\prime}_{\mathbf{x}})\) is a continuous function of \(\mathcal{P}^{\prime}=\langle P^{\prime}_{\mathbf{s}}\rangle_{\mathbf{s}\in \mathcal{I}}\in\mathcal{M}_{\mathcal{G}}\), it follows from Berge's maximum theorem in Lemma 11 that

\[\max_{\mathcal{P}^{\prime}\in\mathcal{C}_{\mathbf{s}}((1+\kappa)\eta_{\mathbf{s }},\bar{\mathcal{P}})}D(\bar{P}_{\mathbf{s}}\parallel P^{\prime}_{\mathbf{s}})+ \epsilon/2\sum_{\mathbf{x}\in\mathcal{I}_{-\mathbf{s}}}D(\bar{P}_{\mathbf{x}} \parallel P^{\prime}_{\mathbf{x}})\]

is a continuous function of \(\bar{\mathcal{P}}=\langle\bar{P}_{\mathbf{s}}\rangle_{\mathbf{s}\in\mathcal{I}}\). Thus, there exists \(\delta^{\prime\prime}_{\mathbf{s}}(\kappa)>0\) such that if \(\forall\mathbf{s}\in\mathcal{I}:\left\|\bar{P}_{\mathbf{s}}-P_{\mathbf{s}} \right\|_{1}\leq\delta^{\prime\prime}_{\mathbf{s}}(\kappa)\),

\[\max_{\mathcal{P}^{\prime}\in\mathcal{C}_{\mathbf{s}}((1+\kappa) \eta_{\mathbf{s}}^{\ast},\bar{\mathcal{P}})}D(P_{\mathbf{s}}\parallel P^{\prime }_{\mathbf{s}})+\epsilon/2\sum_{\mathbf{x}\in\mathcal{I}_{-\mathbf{s}}}D(P_{ \mathbf{x}}\parallel P^{\prime}_{\mathbf{x}})\] \[\leq\sqrt{1+\kappa}\max_{\mathcal{P}^{\prime}\in\mathcal{C}_{ \mathbf{s}}((1+\kappa)\eta_{\mathbf{s}}^{\ast},\mathcal{P})}D(P_{\mathbf{s}} \parallel P^{\prime}_{\mathbf{s}})+\epsilon/2\sum_{\mathbf{x}\in\mathcal{I}_{- \mathbf{s}}}D(P_{\mathbf{x}}\parallel P^{\prime}_{\mathbf{x}})\leq\frac{1}{\eta_ {\mathbf{s}}^{\ast}\sqrt{1+\kappa}},\]

where the second inequality is due to the definition of \(\mathcal{C}_{\mathbf{s}}((1+\kappa)\eta_{\mathbf{s}},\mathcal{P})\). The inequality means for any \(\mathcal{P}^{\prime}\in\mathcal{C}_{\mathbf{s}}((1+\kappa)\eta_{\mathbf{s}}^{ \ast},\bar{\mathcal{P}})\),

\[(\eta_{\mathbf{s}}^{\ast}\sqrt{1+\kappa})D(P_{\mathbf{s}}\parallel P^{\prime}_{ \mathbf{s}})+(\eta_{\mathbf{s}}^{\ast}\sqrt{1+\kappa})\epsilon/2\sum_{\mathbf{x} \in\mathcal{I}_{-\mathbf{s}}}D(P_{\mathbf{x}}\parallel P^{\prime}_{\mathbf{x}}) \leq 1,\]

which means \(\mathcal{C}_{\mathbf{s}}((1+\kappa)\eta_{\mathbf{s}}^{\ast},\bar{\mathcal{P}}) \subseteq\mathcal{C}_{\mathbf{s}}(\sqrt{1+\kappa}\eta_{\mathbf{s}}^{\ast},\mathcal{P})\). Therefore,

\[\tilde{\mu}_{\mathbf{s}}(\mathcal{C}_{\mathbf{s}}((1+\kappa)\eta_{\mathbf{s}}^{ \ast},\bar{\mathcal{P}}))\leq\tilde{\mu}_{\mathbf{s}}(\mathcal{C}_{\mathbf{s}}( \sqrt{1+\kappa}\eta_{\mathbf{s}}^{\ast},\mathcal{P})).\] (19)

Let \(\hat{\delta}_{\mathbf{s}}(\kappa)=\min\{\delta^{\prime}_{\mathbf{s}}(\kappa), \delta^{\prime\prime}_{\mathbf{s}}(\kappa)\}\), and let \(\hat{\delta}(\kappa)=\min_{\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{\ast}}}\hat{ \delta}_{\mathbf{s}}(\kappa)\). Then we set \(\delta(\kappa)=\min\{\Delta_{\min},\hat{\delta}(\kappa)\}\), where \(\Delta_{\min}=\min_{\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{\ast}}}\mu_{\mathbf{s}}( \mathcal{P})-\mu_{\mathbf{s}}(\mathcal{P})\). Since the reward is bounded by \([0,1]\), for any \(\mathbf{s}\in\mathcal{I}\), \(\left\|\mu_{\mathbf{s}}(\bar{\mathcal{P}})-\mu_{\mathbf{s}}(\mathcal{P})\right| \leq\frac{1}{2}\left\|\bar{P}_{\mathbf{s}}-P_{\mathbf{s}}\right\|_{1}\). If \(\forall\mathbf{s}\in\mathcal{I}:\left\|\bar{P}_{\mathbf{s}}-P_{\mathbf{s}} \right\|_{1}\leq\delta(\kappa)\),

\[\mu_{\mathbf{s}}(\bar{\mathcal{P}})-\mu_{\mathbf{s}_{\ast}}(\bar{\mathcal{P}}) \leq\mu_{\mathbf{s}}(\mathcal{P})-\mu_{\mathbf{s}_{\ast}}(\mathcal{P})+\delta( \kappa)\leq\mu_{\mathbf{s}}(\mathcal{P})-\mu_{\mathbf{s}_{\ast}}(\mathcal{P})+ \Delta_{\min}\leq 0,\]

which means intervention \(\mathbf{s}_{\ast}\) is also optimal according to \(\bar{\mathcal{P}}\). Besides, with (18) and (19),

### Main Proof of Theorem 2

The expected regret is defined as \(R_{T}^{\text{SCM-AAM}}=\sum_{\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{*}}}\mathbb{E} [N_{T}(\mathbf{s})]\Delta_{\mathbf{s}}\), in which \(N_{T}(\mathbf{s})\) is the total number of times \(\mathbf{s}\) is selected by the SCM-AAM algorithm till \(T\). So the proof is articulated around bounding \(\mathbb{E}[N_{T}(\mathbf{s})]\) for each \(\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{*}}\).

Proof.: At each time \(t\), consider the event that the true interventional distribution tuple \(\mathcal{P}=\langle P_{\mathbf{s}}\rangle_{\mathbf{s}\in\mathcal{I}}\) is not contained by the confidence set \(\mathcal{C}_{t}=\Big{\{}\mathcal{P}^{\prime}\in\mathcal{M}_{\mathcal{G}}\ \Big{|}\ \sum_{ \mathbf{s}\in\mathcal{I}}N_{t}(\mathbf{s})D(\bar{P}_{\mathbf{s},t}\parallel P _{\mathbf{s}}^{\prime})\leq(1+\gamma)\ln t\Big{\}}\). Due to Corollary 8, there exists \(c_{1}>0\) irrelevant with \(T\)

\[\sum_{t=1}^{T}\mathbb{P}(\mathcal{P}\notin\mathcal{C}_{t})\leq\sum_{t=1}^{T} \mathbb{P}\Big{(}\sum_{\mathbf{s}\in\mathcal{I}}N_{t}(\mathbf{s})D(\bar{P}_{ \mathbf{s},t}\parallel P_{\mathbf{s}})\geq(1+\gamma)\ln t\Big{)}\leq c_{1},\] (20)

which will only result in finite regret bounded \(c_{1}\) since reward is in \([0,1]\). So by only considering time steps with \(\{\mathcal{P}\in\mathcal{C}_{t}\}\) to be true, we upper bound the number of times a suboptimal arm \(\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{*}}\) is selected in the exploration and exploitation phase separately.

**Exploitation phase:** When \(\{\mathcal{P}\in\mathcal{C}_{t}\}\) is true, we have

\[\forall\mathbf{s}\in\mathcal{I}:\tilde{\mu}_{\mathbf{s}}(\mathcal{C}_{t})= \sum_{\mathbf{v}\in\Omega(\mathbf{V})}v_{n}\max_{\mathcal{P}^{\prime}\in \mathcal{C}_{t}}P_{\mathbf{s}}^{\prime}(\mathbf{v})\geq\sum_{\mathbf{v}\in \Omega(\mathbf{V})}v_{n}P_{\mathbf{s}}(\mathbf{v})=\mu_{\mathbf{s}}.\] (21)

According to the design, SCM-AAM enters the exploitation phase at time \(t\) if \(\bar{\mu}_{\mathbf{S}_{t},t}>\tilde{\mu}_{\mathbf{s}}(\mathcal{C}_{t}),\forall \mathbf{s}\in\mathcal{I}\setminus\{\mathbf{S}_{t}\}\). It follows from (21), if \(\mathbf{S}_{t}\neq\mathbf{s}_{*}\) and \(\mathcal{P}\in\mathcal{C}_{t}\),

\[\bar{\mu}_{\mathbf{S}_{t},t}>\tilde{\mu}_{\mathbf{s}_{*}}(\mathcal{C}_{t})\geq \mu_{*}.\]

Thus, we can bound the regret from exploitation steps as the following

\[\sum_{t=1}^{T}\sum_{\mathbf{s}\in\mathcal{I}\setminus\{\mathbf{s}_{*}\}} \Delta_{\mathbf{s}}\mathbbm{1}\{\mathbf{S}_{t}=\mathbf{s},\mathcal{P}\in \mathcal{C}_{t}\}\leq\sum_{t=1}^{T}\sum_{\mathbf{s}\in\mathcal{I}/\{\mathbf{s }_{*}\}}\Delta_{\mathbf{s}}\mathbbm{1}\{\mathbf{S}_{t}=\mathbf{s},\bar{\mu}_{ \mathbf{s},t}>\mu_{*}.\}\] (22)

Let \(\{t_{\mathbf{s},i}\}_{i\geq 1}\) be the sequence of rounds such that \(\mathbf{s}\) is selected by exploitation steps, namely \(\mathbf{S}_{t_{\mathbf{s},i}}=\mathbf{s}\). Considering the warmup step that selects each intervention once, at each \(t_{\mathbf{s},i}\), we have \(N_{t_{\mathbf{s},i}}(\mathbf{s})\geq i\). Since each \(t_{\mathbf{s},i}\) is a stopping time, by applying Lemma 9, we bound the expectation of (22) as

\[\mathbb{E}\bigg{[}\sum_{t=1}^{T}\sum_{\mathbf{s}\in\mathcal{I}_{- \mathbf{s}_{*}}}\Delta_{\mathbf{s}}\mathbbm{1}\{\mathbf{S}_{t}=\mathbf{s}, \mathcal{P}\in\mathcal{C}_{t}\}\bigg{]} \leq\sum_{\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{*}}}\Delta_{ \mathbf{s}}\sum_{i\geq 1}P(\bar{\mu}_{\mathbf{s},t_{\mathbf{s},i}}-\mu_{\mathbf{s}}> \Delta_{\mathbf{s}},t_{\mathbf{s},i}\leq T)\] \[\leq\sum_{\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{*}}}\Delta_{ \mathbf{s}}\sum_{i\geq 1}\exp(-2i\Delta_{\mathbf{s}}^{2})\leq\sum_{\mathbf{s}\in \mathcal{I}_{-\mathbf{s}_{*}}}\frac{1}{2\Delta_{\mathbf{s}}}:=c_{2}.\] (23)

**Exploration phase with inaccurate \(\bar{\mathcal{P}}_{t}\):** We define event \(E_{t}\) that \(\bar{P}_{t}\) is estimated with sufficient accuracy,

\[E_{t}=\left\{\forall\mathbf{s}\in\mathcal{I}:\left\|\bar{P}_{t,\mathbf{s}}-P_{ \mathbf{s}}\right\|_{1}\leq\delta(\kappa)\right\},\] (24)

where \(\delta(\kappa)>0\) is defined in Lemma 14. Let \(\mathcal{R}=\{t_{i}\}_{i\geq 1}\) be the sequence of rounds that SCM-AAM enters the exploration phase, and each \(t_{i}\) is a stopping time. Then we have

\[\sum_{t=1}^{T}\mathbbm{1}\{t\in\mathcal{R},E_{t}^{\complement}\}\leq\sum_{i\geq 1 }\mathbbm{1}\{E_{t_{i}}^{\complement},t_{i}\leq T\}.\]

From Lemma 6, at \(t_{i}\), we have \(\forall\mathbf{s}\in\mathcal{I}:N_{t}(\mathbf{s})\geq\epsilon i/2\). With Corollary 10, we apply a union bound

\[\mathbb{E}\bigg{[}\sum_{t=1}^{T}\mathbbm{1}\{t\in\mathcal{R},E_{t }^{\complement}\}\bigg{]} \leq\sum_{\mathbf{s}\in\mathcal{I}}\sum_{i\geq 1}P(E_{t_{i}}^{\complement},t_{i} \leq T)\leq\sum_{i\geq 1}P\Big{(}\left\|\bar{P}_{t_{i},\mathbf{s}}-P_{\mathbf{s}} \right\|_{1}>\delta(\kappa),t_{i}\leq T\Big{)}\] \[\leq\sum_{\mathbf{s}\in\mathcal{I}}\sum_{i\geq 1}(2^{|\Omega( \mathbf{V})|}-2)\exp\Big{(}\frac{-\epsilon i\delta(\kappa)^{2}}{4}\Big{)}=\frac{(2 ^{|\Omega(\mathbf{V})|+2}-8)\left|\mathcal{I}\right|}{\epsilon\delta(\kappa)^{2} }=c_{3}.\] (25)

**Greedy approximation exploration with accurate \(\bar{\mathcal{P}}_{t}\):** For each \(\mathbf{s}\in\mathcal{I}\), let \(N_{t}^{\mathrm{e}}(\mathbf{s})\) be the total number of times \(\mathbf{s}\) is selected in exploration phases till \(t\). Within time steps such that both \(E_{t}\) defined in (24) and \(\{\mathcal{P}\in\mathcal{C}_{t}\}\) are true, let the last time \(\mathbf{s}\in\mathcal{I}_{-\mathbf{s}_{*}}\) being selected by exploration with greedy approximation be

\[\bar{t}_{\mathbf{s}}=\max\{t\in\mathcal{R}\mid\overline{\mathbf{S}}_{t}= \mathbf{s},E_{t},\mathcal{P}\in\mathcal{C}_{t}\}\]

is true. With Lemma 6, we have \(\forall\mathbf{x}\in\mathcal{I}_{-\mathbf{s}}:N_{\bar{t}_{\mathbf{s}}}( \mathbf{x})\geq\epsilon N_{\bar{t}_{\mathbf{s}}}^{\mathrm{e}}(\mathbf{s})/2\), so that

\[\frac{\sum_{\mathbf{x}\in\mathcal{I}}N_{\bar{t}_{\mathbf{s}}}(\mathbf{x})D( \bar{P}_{\mathbf{x},\bar{t}_{\mathbf{s}}}\parallel P_{\mathbf{x}}^{\prime})} {(1+\gamma)\ln\bar{t}_{\mathbf{s}}}\geq\frac{\epsilon N_{\bar{t}_{\mathbf{s}} }^{\mathrm{e}}(\mathbf{s})\sum_{\mathbf{x}\in\mathcal{I}_{-\mathbf{s}}}D(\bar {P}_{\mathbf{x},\bar{t}_{\mathbf{s}}}\parallel P_{\mathbf{x}}^{\prime})}{2(1+ \gamma)\ln\bar{t}_{\mathbf{s}}}+\frac{N_{\bar{t}_{\mathbf{s}}}^{\mathrm{e}}( \mathbf{s})D(\bar{P}_{\mathbf{s},\bar{t}_{\mathbf{s}}}\parallel P_{\mathbf{s} }^{\prime})}{(1+\gamma)\ln\bar{t}_{\mathbf{s}}}.\] (26)

Recall the definition of \(\mathcal{C}_{t}\) and \(\mathcal{C}_{\mathbf{s}}(\eta_{\mathbf{s}},\bar{\mathcal{P}})\):

\[\mathcal{C}_{t} =\bigg{\{}\mathcal{P}^{\prime}\in\mathcal{M}_{\mathcal{G}}\ \Big{|}\ \sum_{\mathbf{x}\in\mathcal{I}}N_{t}(\mathbf{x})D(\bar{P}_{ \mathbf{x},t}\parallel P_{\mathbf{x}}^{\prime})\leq(1+\gamma)\ln t\bigg{\}},\] \[\mathcal{C}_{\mathbf{s}}(\eta_{\mathbf{s}},\bar{\mathcal{P}}_{t}) =\Big{\{}\mathcal{P}^{\prime}\in\mathcal{M}_{\mathcal{G}}\ \Big{|}\ \eta_{\mathbf{s}}D(\bar{P}_{ \mathbf{s},t}\parallel P_{\mathbf{s}}^{\prime})+\epsilon\eta_{\mathbf{s}}/2 \sum_{\mathbf{x}\in\mathcal{I}_{-\mathbf{s}}}D(\bar{P}_{\mathbf{x},t} \parallel P_{\mathbf{x}}^{\prime})\leq 1\Big{\}}.\]

With (26), we have \(\mathcal{C}_{\bar{t}_{\mathbf{s}}}\subseteq\mathcal{C}_{\mathbf{s}}\Big{(} \frac{N_{\bar{t}_{\mathbf{s}}}^{\mathrm{e}}(\mathbf{s})}{(1+\gamma)\ln\bar{t} _{\mathbf{s}}},\bar{\mathcal{P}}_{\bar{t}_{\mathbf{s}}}\Big{)}\), and as a result,

\[\bar{\mu}_{\mathbf{s}}\bigg{(}\mathcal{C}_{\mathbf{s}}\Big{(} \frac{N_{\bar{t}_{\mathbf{s}}}^{\mathrm{e}}(\mathbf{s})}{(1+\gamma)\ln\bar{t }_{\mathbf{s}}},\bar{\mathcal{P}}_{\bar{t}}\Big{)}\bigg{)}\geq\tilde{\mu}_{ \mathbf{s}}(\mathcal{C}_{t}).\] (27)

By the algorithm design, we have \(\tilde{\mu}_{\mathbf{s}}(\mathcal{C}_{\bar{t}_{\mathbf{s}}})\geq\bar{\mu}_{ \mathbf{s}_{*},t}\), since otherwise SCM-AAM does not enter exploration phase. Therefore, it follows from (27) that

\[\tilde{\mu}_{\mathbf{s}}\bigg{(}\mathcal{C}_{\mathbf{s}}\Big{(} \frac{N_{\bar{t}_{\mathbf{s}}}^{\mathrm{e}}(\mathbf{s})}{(1+\gamma)\ln\bar{t }_{\mathbf{s}}},\bar{\mathcal{P}}_{\bar{t}_{\mathbf{s}}}\Big{)}\bigg{)}\geq\bar{ \mu}_{\mathbf{s}_{*},\bar{t}_{\mathbf{s}}}=\mu_{*}(\bar{\mathcal{P}}_{\bar{t}_ {\mathbf{s}}}),\]

where the inequality holds since when \(E_{\bar{t}_{\mathbf{s}}}\) happens, Lemma 14 shows \(\mathbf{s}_{*}\) is also optimal according to \(\bar{\mathcal{P}}_{\bar{t}_{\mathbf{s}}}\). With Lemma 13, \(\tilde{\mu}_{\mathbf{s}}(\mathcal{C}_{\mathbf{s}}(\eta_{\mathbf{s}},\bar{ \mathcal{P}}_{\bar{t}_{\mathbf{s}}}))\) is a continuous nonincreasing function of \(\eta_{\mathbf{s}}\), then we have

\[N_{\bar{t}_{\mathbf{s}}}^{\mathrm{e}}(\mathbf{s})\leq\zeta_{\mathbf{s}}(\bar{ \mathcal{P}}_{\bar{t}_{\mathbf{s}}})(1+\gamma)\ln\bar{t}_{\mathbf{s}}\leq\zeta_ {\mathbf{s}}(\bar{\mathcal{P}}_{\bar{t}_{\mathbf{s}}})(1+\gamma)\ln T,\]

where \(\zeta_{\mathbf{s}}(\bar{\mathcal{P}}_{\bar{t}_{\mathbf{s}}})\) defined in (17) is the maximum value of \(\eta_{\mathbf{s}}\) such that \(\tilde{\mu}_{\mathbf{s}}(\mathcal{C}_{\mathbf{s}}(\eta_{\mathbf{s}},\bar{ \mathcal{P}}_{\bar{t}_{\mathbf{s}}}))\geq\mu_{*}(\bar{\mathcal{P}}_{\bar{t}_{ \mathbf{s}}})\). Together with Lemma 14, we have

\[\sum_{t=1}^{T}\mathbbm{1}\{\overline{\mathcal{S}}_{t}=\mathbf{s},E_{t}, \mathcal{P}\in\mathcal{C}_{t}\}=N_{\bar{t}_{\mathbf{s}}}^{\mathrm{e}}(\mathbf{s} )\leq\zeta_{\mathbf{s}}(\mathcal{P})(1+\kappa)(1+\gamma)\ln T.\] (28)

**Forced Exploration:** By slightly abusing the notation, at time \(t\), we denote by \(\{\overline{\mathbf{S}}_{t}\}\) as the greedy approximation exploration event and by \(\{\underline{\mathbf{S}}_{t}\}\) as the force exploration event. Let the last time \(\mathbf{s}\in\mathcal{I}\) be selected by forced exploration be \(\underline{t}_{\mathbf{s}}:=\max\{t\in\mathcal{R}\mid\underline{\mathbf{S}}_{t}= \mathbf{s}\}\). When SCM-AAM force exploration at \(t\), we have \(N_{t}(\underline{\mathbf{S}}_{t})\leq\epsilon N_{t}^{\mathrm{e}}\), resulting in

\[\sum_{t=1}^{T}\mathbbm{1}\{\underline{\mathbf{S}}_{t}\}\leq\sum_{\mathbf{s}\in \mathcal{I}}\sum_{t=1}^{T}\mathbbm{1}\{\underline{\mathbf{S}}_{t}=\mathbf{s}\} \leq\sum_{\mathbf{s}\in\mathcal{I}}N_{\bar{t}_{\mathbf{s}}}(\mathbf{s})\leq\sum_{ \mathbf{s}\in\mathcal{I}}\epsilon N_{\bar{t}_{\mathbf{s}}}^{\mathrm{e}}\leq \epsilon\left|\mathcal{I}\right|N_{T}^{\mathrm{e}}.\] (29)

We want to provide an upper bound \(N_{T}^{\mathrm{e}}\), which is

\[N_{T}^{\mathrm{e}}=\sum_{t=1}^{T}\mathbbm{1}\{t\in\mathcal{R}\}\leq\sum_{t=1}^{T} \big{(}\mathbbm{1}\{\underline{\mathbf{S}}_{t}\}+\mathbbm{1}\{\overline{ \mathbf{S}}_{t},E_{t},\mathcal{P}\in\mathcal{C}_{t}\}+\mathbbm{1}\{t\in\mathcal{ R},E_{t}^{\complement}\}+\mathbbm{1}\{\mathcal{P}\in\mathcal{C}_{t}^{\complement}\}\big{)},\] (30)

Putting together (29) and (30), we get

\[\sum_{t=1}^{T}\mathbbm{1}\{\underline{\mathbf{S}}_{t}\}\leq \frac{\epsilon\left|\mathcal{I}\right|}{1-\epsilon\left|\mathcal{I} \right|}\sum_{t=1}^{T}\big{(}\mathbbm{1}\{t\in\mathcal{R},\overline{\mathbf{S}}_{t },E_{t},\mathcal{P}\in\mathcal{C}_{t}\}+\mathbbm{1}\{t\in\mathcal{R},E_{t}^{ \complement}\}+\mathbbm{1}\{\mathcal{P}\in\mathcal{C}_{t}^{\complement}\}\big{)}.\]Also with (28), we have

\[\sum_{t=1}^{T}\mathbbm{1}\{t\in\mathcal{R},\overline{\mathbf{S}}_{t}, E_{t},\mathcal{P}\in\mathcal{C}_{t}\} =\sum_{\mathbf{s}\in\mathcal{I}}\sum_{t=1}^{T}\mathbbm{1}\{t\in \mathcal{R},\overline{\mathbf{S}}_{t}=\mathbf{s},E_{t},\mathcal{P}\in\mathcal{C }_{t}\}\] \[\leq\sum_{\mathbf{s}\in\mathcal{I}}\zeta_{\mathbf{s}}(\bar{ \mathcal{P}}_{\tilde{t}_{\mathrm{s}}})(1+\kappa)(1+\gamma)\ln T.\]

Then, the expected number of forced explorations can be bound as the following,

\[\mathbb{E}\bigg{[}\sum_{t=1}^{T}\mathbbm{1}\{\mathbf{S}_{t}\} \bigg{]}\leq\frac{\epsilon\left|\mathcal{I}\right|}{1-\epsilon\left|\mathcal{I }\right|}\big{[}c_{1}+c_{3}+(1+\kappa)(1+\gamma)\sum_{\mathbf{s}\in\mathcal{I }}\zeta_{\mathbf{s}}(\bar{\mathcal{P}}_{\tilde{t}_{\mathrm{s}}})\ln T\big{]},\] (31)

where \(C_{1}\) and \(C_{3}\) are defined in (20) and (25) respectively.

**Summary:** With reward \(V_{n}\in[0,1]\), we decomposed the expected regret as

\[R_{T}^{\text{SCM-AAM}}(\mathcal{P})\leq \mathbb{E}\bigg{[}\sum_{t=1}^{T}\sum_{\mathbf{s}\in\mathcal{I}- \epsilon_{\mathbf{s}_{\mathbf{s}}}}\Delta_{\mathbf{s}}\mathbbm{1}\{\mathbf{S}_ {t}=\mathbf{s},\mathcal{P}\in\mathcal{C}_{t}\}\bigg{]}+\sum_{\mathbf{s}\in \mathcal{I}-\epsilon_{\mathbf{s}_{\mathbf{s}}}}\Delta_{\mathbf{s}}\sum_{t\in \mathcal{R}}\mathbbm{1}\{\overline{\mathcal{S}}_{t}=\mathbf{s},E_{t},\mathcal{ P}\in\mathcal{C}_{t}\}\] \[+\mathbb{E}\bigg{[}\sum_{t=1}^{T}\mathbbm{1}\{\mathbf{S}_{t}\} \bigg{]}+\sum_{t\in\mathcal{R}}\mathbb{P}(E_{t}^{\complement})+\sum_{t=1}^{T} \mathbb{P}(\mathcal{P}\notin\mathcal{C}_{t}),\]

where the first three terms present regret from exploitation, exploration with greedy approximation, and forced exploration respectively. So the proof can be concluded the proof by substituting (20), (25), (23), (28) and (31) into the above inequality. 

## Appendix C Experiment Details and Additional Experimental Results

In the main paper, we conducted experiments in three causal bandit instances. The causal graph corresponding to each instance is shown in Fig. 2. For \(p\in[0,1]\), let \(\mathcal{B}(p)\) denote a Bernoulli random variable with probability \(p\) to be \(1\) and \(1-p\) to be \(0\). The detailed structural equations for each causal bandit instance are described below.

**Task1:**: \(U_{V_{1}}=\mathcal{B}(0.6)\), \(U_{V_{2}}=\mathcal{B}(0.11)\), \(U_{V_{2}V_{3}}=\mathcal{B}(0.51)\) and \(U_{V_{3}}=\mathcal{B}(0.15)\).

\[f_{V_{1}}(u_{V_{1}}) =u_{V_{1}}\] \[f_{V_{2}}(v_{1},u_{V_{2}},u_{V_{3}V_{3}}) =v_{1}\;\oplus\;u_{V_{2}}\;\oplus\;u_{V_{2}V_{3}}\] \[f_{V_{3}}(v_{2},u_{V_{3}},u_{V_{2}V_{3}}) =v_{2}\;\oplus\;u_{V_{3}}\;\oplus\;u_{V_{2}V_{3}}\;\oplus\;1\]
**Task2:**: \(U_{V_{1}}=\mathcal{B}(0.45)\), \(U_{V_{2}}=\mathcal{B}(0.05)\), \(U_{V_{5}V_{2}}=\mathcal{B}(0.54)\), \(U_{V_{3}}=\mathcal{B}(0.07)\), \(U_{V_{3}V_{4}}=\mathcal{B}(0.51)\), \(U_{V_{4}}=\mathcal{B}(0.06)\) and \(U_{V_{5}}=\mathcal{B}(0.06)\).

\[f_{V_{1}}(u_{V_{1}}) =u_{V_{1}}\] \[f_{V_{2}}(u_{V_{2}},u_{V_{5}V_{2}}) =u_{V_{2}}\;\oplus\;u_{V_{5}V_{2}}\] \[f_{V_{3}}(v_{1},u_{V_{3}},u_{V_{3}V_{4}}) =v_{1}\;\oplus\;u_{V_{3}}\;\oplus\;u_{V_{3}V_{4}}\] \[f_{V_{4}}(v_{2},u_{V_{4}},u_{V_{3}V_{4}}) =1\;\oplus\;v_{2}\;\oplus u_{V_{4}}\;\oplus u_{V_{3}V_{4}}\] \[f_{V_{5}}(v_{3},u_{4},u_{V_{5}},u_{V_{5}V_{2}}) =v_{3}\;\oplus\;v_{4}\;\oplus\;u_{V_{5}}\;\oplus\;u_{V_{5}V_{2}}\]
**Task3:**: \(U_{V_{1}}=\mathcal{B}(0.5)\), \(U_{V_{2}}=\mathcal{B}(0.85),U_{V_{3}}=\mathcal{B}(0.85)\), \(U_{V_{3}V_{7}}=\mathcal{B}(0.85),U_{V_{4}}=\mathcal{B}(0.14),U_{V_{5}}= \mathcal{B}(0.74),U_{V_{6}}=\mathcal{B}(0.74),U_{V_{7}}=\mathcal{B}(0.54)\).

\[f_{V_{1}}(u_{V_{1}}) =u_{V_{1}}\] \[f_{V_{2}}(u_{V_{2}}) =u_{V_{2}}\] \[f_{V_{3}}(v_{1},u_{V_{3}},u_{V_{3}V_{7}}) =(v_{1}\;\oplus\;u_{u_{V_{3}}})\;\vee\;u_{u_{V_{3}V_{7}}}\] \[f_{V_{4}}(v_{2},u_{V_{4}}) =v_{2}\wedge u_{V_{4}}\] \[f_{V_{5}}(v_{3},v_{4},u_{V_{5}}) =(v_{3}\;\oplus\;u_{V_{5}})\;\vee v_{4}\] \[f_{V_{6}}(v_{4},u_{V_{6}}) =v_{4}\;\vee u_{V_{6}}\] \[f_{V_{7}}(v_{5},v_{6},u_{V_{5}},u_{V_{7}},u_{V_{3}V_{7}}) =((v_{5}\;\oplus\;v_{6}\;\oplus\;0\;\oplus\;u_{V_{3}V_{7}})\wedge u _{V_{7}})\oplus v_{6}\]