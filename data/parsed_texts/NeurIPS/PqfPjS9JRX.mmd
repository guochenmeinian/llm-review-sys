# The Shaped Transformer: Attention Models

in the Infinite Depth-and-Width Limit

 Lorenzo Noci

Equal contribution. Correspondence to:

lorenzo.noci@inf.ethz.ch, chuning.li@mail.utoronto.ca, mufan.li@mail.utoronto.ca

Chuning Li1

University of Toronto and Vector Institute

Mufan (Bill) Li1

University of Oxford

Bobby He

University of Oxford

Thomas Hofmann

Equal contribution. Correspondence to:

lorenzo.noci@inf.ethz.ch, chuning.li@mail.utoronto.ca, mufan.li@mail.utoronto.ca

Chris Maddison

University of Toronto and Vector Institute

Daniel M. Roy

University of Oxford

###### Abstract

In deep learning theory, the covariance matrix of the representations serves as a proxy to examine the network's trainability. Motivated by the success of Transformers, we study the covariance matrix of a modified Softmax-based attention model with skip connections in the proportional limit of infinite-depth-and-width. We show that at initialization the limiting distribution can be described by a stochastic differential equation (SDE) indexed by the depth-to-width ratio. To achieve a well-defined stochastic limit, the Transformer's attention mechanism is modified by centering the Softmax output at identity, and scaling the Softmax logits by a width-dependent temperature parameter. We examine the stability of the network through the corresponding SDE, showing how the scale of both the drift and diffusion can be elegantly controlled with the aid of residual connections. The existence of a stable SDE implies that the covariance structure is well-behaved, even for very large depth and width, thus preventing the notorious issues of rank degeneracy in deep attention models. Finally, we show, through simulations, that the SDE provides a surprisingly good description of the corresponding finite-size model. We coin the name _shaped Transformer_ for these architectural modifications.

## 1 Introduction

Pre-trained large language models have experienced a remarkable increase in popularity due to their eerily human-like ability to puzzle through complex reasoning tasks, solve coding challenges, and produce pages of logically sound text [1]. Arguably, the Transformer is the foundation of these successes [2]. Recent research has found evidence for scaling laws, linking the performance of these architectures to their parameter counts and the quantity of training data, fueling the desire to train deeper and wider models on ever larger datasets in order to unlock new levels of performance [3, 4, 5, 6, 7].

Bundled with the increased expressivity of deep architectures, however, is increased numerical instability, both in the forward pass and gradients, which hinders training. One of the clearest examples of instability is the so-called rank collapse phenomenon [8, 9] - the observation that, in Softmax-based attention models, the network's representation of different tokens tend to perfectly align at large depth. The resulting poorly conditioned covariance and correlation between tokens leads to exploding and/or vanishing gradients at initialization, disrupting gradient updates of the affected parameters. This situation violates a well-known guiding principle from the literature of deep signal propagation: a stable covariance is a necessary condition for stable training [10, 11, 12, 13, 14, 15]. In fact,the instability of Transformers is evident when considering the critical role of hyperparameter tuning and the judicious use of normalization layers. In this work, we study Transformers in a novel infinite limit, rectify sources of instability with a novel modification, and derive the SDEs characterizing the covariance and output distribution.

Scaling limits have been used successfully to provide guidance on architecture [16; 17; 18] and tuning hyperparameters settings [19]. Our work represents a contribution in this direction. The ability to use such limits to diagnose instabilities depends on their tractability and faithfulness to real-world (finite) networks. In this regard, not all limits are created equal. In particular, the faithfulness of scaling limits depends critically on how other parameters are scaled with width. One of the simplest (and thus most popular) limits to work with - the "NTK" limit [20; 21; 22; 23; 24] - treats the depth of the network as fixed. As a result, at initialization, this limit does not accumulate sufficient random fluctuations over the depth of the network, leading to deterministic covariance matrices that do not agree with those of standard (finite) networks. Such networks have another defect: they are incapable of learning features in the limit [25]. Various other limits have been studied, towards identifying tractable yet faithful models of initialization and/or training. These include mean field limits [26; 27; 28; 29] and the perturbative regime [30; 31; 32; 33; 34; 35; 36].

This work operates in a relatively new regime - the _proportional_ infinite depth-and-width limit - where depth \(d\) and width \(n\) diverge as the ratio \(d/n\) tends to a positive constant. This limit, first analyzed by Hanin and Nica [37], has been the recent subject of study in the context of neural network [38; 39; 40; 41; 18]. A related line of work also studied the Lyapunov exponent for products of random matrices [42; 43; 44; 45]. This regime retains the network's stochasticity and, at initialization, has been shown to closely resemble the behaviour of finite architectures, yet still yield a relatively simple limiting description, expressible in terms of stochastic differential equations [40; 18]. In this work, we fully characterize the initial output distributions of a network with skip connections and Softmax-based attention mechanisms, in the proportional infinite-depth-and-width limit.

Inspired by the idea of shaping activation functions [16; 17; 18; 46], our theoretical approach finds an adequately modified attention mechanism via its SDE limit. Our modification involves making the attention matrix closer to the identity, and appropriately choosing the temperature parameter \(\tau\), which re-scales the logits of the Softmax. Similar to shaping activation functions, the temperature scaling we devise linearizes and reduces the saturation of the Softmax, a known source of training instability in Transformers [47]. In order to model the feedforward layer of a Transformer's block, we extend existing results [18] to derive an SDE for the proportional limit of shaped-ReLU feedforward multi-layer perceptrons (MLPs) with skip connections. Combined, we fully characterize the output distribution of a Transformer with shaped non-linearities (Corollary 4.3).

Figure 1: Our shaped Transformer prevents token representations from becoming perfectly aligned, i.e. rank collapse. Left: mean correlation \(\rho_{\ell}^{\alpha\beta}\) of Transformers (Eq. 11) with and without shaped attention (Eq. 9) and Pre-LN [48]. Right: kernel density estimate and histogram of correlations from covariance SDE in Theorem 4.2 and shaped attention NN. Here we note correlation converging to \(1\) implies a poorly conditioned covariance matrix. Simulated with \(n=200,d=150,\gamma=1/\sqrt{8},\tau_{0}=1,\rho_{0}^{\alpha\beta}=0.2\), SDE step size \(0.01\), and \(2^{12}\) samples.

Notably, our modification successfully prevents a poorly conditioned covariance matrix, whereas the vanilla Softmax-based attention model without LayerNorm [49] fails in this regard, and the corresponding Pre-LN architecture provides only marginal improvements (see Figure 1). Given that our modification is inspired by previous work on shaping activation functions, we coin the terms _shaped attention_ for the proposed attention mechanism and _shaped Transformer_ for the overall architecture that includes the MLP block and residual connections. Through simulations (e.g., Figure 1), we show that the limiting neural covariance SDE approximates the distribution of finite-size Transformers with shaped attention mechanism surprisingly well. We also provide preliminary training experiments for our proposed shaped attention architecture on standard language modeling tasks, demonstrating the feasibility of the new architecture in practice (see Section 5 and Appendix D).

In summary, our contributions are as follows:

1. We study the effect of skip connections in the proportional limit, showing that under a precise relation between the scaling parameters of the shortcut and residual branches, the feature covariance converges to the solution of a weighted version of the neural covariance SDE for MLPs (Theorem 3.2). The dependence on the depth-to-width ratio implies the existence of a stable non-commutative limit for residual networks, complementing the commutative limit studied in Hayou and Yang [50].
2. We propose _shaped attention_, where we modify the Softmax-based attention mechanism to be a perturbation of the identity. We demonstrate that shaped attention successfully prevents the degeneracy of correlation in contrast to existing Transformer architectures (Figure 1). The enhanced stability in the forward pass is reflected in the gradients, which are also stable with depth, as we empirically show in Figure 2.
3. For the proposed shaped attention architecture, we derive the neural covariance SDE characterizing the initial distribution in the proportional limit (Theorem 4.2). Consequently, we provide the first characterization of Transformer-type architectures, i.e. the shaped Transformer, in the large depth-and-width regime (Corollary 4.3).
4. We provide simulations to validate the theory and to interpret the effects of network hyperparameters on the covariance matrix of the shaped Transformer. Specifically, we study finite time stability of the SDE and provide explicit guidance on hyperparameters to prevent numerical instability.

Figure 2: Comparing gradients norms at initialization for different parameters as a function of depth, with and without shaped attention. The architecture is the same as in Figure 1 but with autoregressive causal masking, and the task is next-token prediction on code data. Left: Value weights \(W_{\ell}^{V}\) for shaped attention, standard Pre-LN, and the original Post-LN block [2]. Right: the same gradient norm plot but for Query weights \(W_{l}^{Q}\). We find that shaping the attention mechanism successfully prevents gradients from vanishing, while unshaped Transformers suffer from rapidly vanishing gradients. Interestingly, only the Post-LN query gradients vanish, but value gradients are stable across depths, which is consistent with the findings of Noci et al. [9]. On the other hand, shaped attention has stable gradients for both parameters inside and outside the Softmax nonlinearity.

The paper is organized as follows: In Section 2, we provide the basic setup and some background on existing results. In Section 3, we generalize the SDE results of M. Li et al. [18] to include skip connections. This serves as a model to understand the effect of skip connections in isolation from the attention model. In Section 4, we present our main result, first pinpointing the origins of instability in the Softmax, then showing how the modifications underlying _shaped attention_ allow us to derive a non-trivial SDE limit. Finally, in Section 5, we discuss the implications of our results and some future directions. Proofs of all theorems and additional experiments are deferred to the Appendix.

## 2 Background

**Setup.** Let \(X_{\ell}\in\mathbb{R}^{m\times n}\) be the data matrix representing a sequence of \(m\) tokens embedded in \(n\) dimensions at layer \(\ell\in[d]\), where \(d\) is the depth of the network. We elide the explicit dependence on \(\ell\) when it is clear from the context, and use superscript Greek letters to indicate specific tokens' representations, for instance \(x_{\ell}^{\alpha}\in\mathbb{R}^{n}\) is the \(\alpha\)-th row of \(X_{\ell}\). We consider the following attention model with residual connections:

\[X_{\ell+1}=\lambda X_{\ell}+\gamma A_{\ell}X_{\ell}\ \frac{1}{\sqrt{n}}W_{\ell}^{V}\] (1)

where \(\gamma,\lambda\in[0,1]\) are parameters that control the strength of the shortcut and residual branch, respectively, \(W_{\ell}^{V}\in\mathbb{R}^{n\times n}\) is the weight matrix of the values, and \(A_{\ell}\in\mathbb{R}^{m\times m}\) is the attention matrix. We consider Softmax-based scaled dot-product attention, where \(A_{\ell}\) has the form:

\[A_{\ell}=\text{Softmax}\left(\frac{1}{\tau}X_{\ell}\ \frac{1}{\sqrt{n}}W_{\ell}^{Q}\ \frac{1}{\sqrt{n}}W_{\ell}^{K,\top}\ X_{\ell}^{\top}\right),\] (2)

where the Softmax is applied row-wise, \(W_{\ell}^{Q},W_{\ell}^{K}\in\mathbb{R}^{n\times n_{k}}\) are additional random weights, and \(\tau\) is a temperature parameter, which controls the entropy of the distribution. Here we let all the weight matrices \(W_{\ell}^{Q},W_{\ell}^{K},W_{\ell}^{V}\) have \(\mathcal{N}(0,1)\)-iid entries. In the case where \(\lambda,\gamma=1\), with the application of LayerNorm on the residual branch [48], and with \(\tau=\sqrt{n_{k}}\), we recover the attention block of the vanilla "Pre-LN" Transformer architecture [2]. Here we note that we pull the conventional \(n^{-1/2}\) factor outside of the weight matrices, which preserves the forward pass, and yields equivalent training dynamics up to a reparameterization of the learning rate [25]. In this work, we consider unnormalized architectures, and control the variance propagation with the condition \(\lambda^{2}+\gamma^{2}=1\)[40]. We are interested in studying the so-called _neural covariance_ for the attention model (Eq. 1) in the proportional limit.

**Neural Covariance.** In deep learning theory, researchers have long sought to understand how networks internally represent different inputs and how different architectural choices affect these representations. The approach followed by work on signal propagation has been to study how the relative alignment of different inputs evolves across the network, as measured by the neural covariance \(V_{\ell}^{\alpha\beta}:=\frac{1}{n}\langle x_{\ell}^{\alpha},x_{\ell}^{\beta}\rangle\) (or \(\rho^{\alpha\beta}:=(V_{\ell}^{\alpha\alpha}V_{\ell}^{\beta\beta})^{-1/2}V_{ \ell}^{\alpha\beta}\) if interested only in the correlation). At initialization, characterizations of this covariance structure have been exploited to infer important properties of neural networks [10; 11]. As an example, in the sequential infinite-width-_then_-depth limit, the correlation \(\rho_{d}^{\alpha\beta}\) of MLPs is known to converge to a fixed point independent of the input [11; 16]. In this regime, the model is not able to discriminate different data points, which severely hinders training, as the gradient step for the deep layers is taken in the same direction regardless of the input. In the context of Softmax-based attention models, Dong et al. [8] proved that the feature matrix \(X_{\ell}\) loses rank doubly exponentially fast with depth, and Noci et al. [9] showed how this leads to vanishing gradients of the queries and keys parameters, thus further highlighting how the stability of forward and backward passes are deeply entangled (see also Figure 2).

**Stabilizing the Effect of Non-Linear Layers.** Central to the issue of degeneracy of the neural covariance are commonly used non-linear activation functions that severely deviate from the identity. The recent line of work of Deep Kernel Shaping (DKS) [17; 16; 18] addresses the issue by considering the cumulative amount of non-linearity throughout layers, and _shaping_ the activation function by making it closer to the identity map. Inspired by this line of work, B. He et al. [46] devise an initialization for Transformers that avoid the rank collapse problem without the aid of skip connections or LayerNorm.

In an alternative approach, the line of work behind Stable ResNets [50; 51; 52; 53] considers scaling the residual branches by \(\gamma=1/\sqrt{\text{depth}}\), and postulates this scaling is sufficient to stabilize the neural 

[MISSING_PAGE_FAIL:5]

Notice how the limiting SDE closely resembles the MLP case (Eq. 3), which is recovered exactly when \(\gamma=1\). The only difference is the extra \(2\) factor, which comes from the fact that in our definition each layer has effectively two times the number of weight matrices than the standard formulation for MLPs. As the drift depends solely on the nonlinearity, and the diffusion depends soley on the random weights, only the diffusion variance is doubled. The residual branch parameter \(\gamma<1\) dampens both the drift and the variance of the Brownian motion by \(\gamma^{2}\), thus it can be interpreted as a time change. In other words, the effect of \(\gamma\) at initialization is equivalent to reducing depth-to-width ratio, inline with existing intuitions that ResNets have a lower "effective-depth" [56]. To visualize the stabilizing effect of \(\gamma\) on the distribution, in Figure 3 (right) we plot the 95th percentile correlation as a function of \(\gamma\). The increasing trend indicates a larger probability of perfect alignment between two tokens. In Figure 3 (left) we plot the densities of both the residual SDE and the corresponding residual network for various values of \(\gamma\). Notice how the samples from the SDE well-approximates the histogram of a finite network.

**A Stable Non-Commutative Limit.** Our results complement those of Hayou and Yang [50], where the authors have shown that for a similar ResNet under the parameter scaling \(\lambda=1,\gamma=1/\sqrt{d}\), the depth and width limits _commute_. More precisely, the covariance \(V^{\alpha\beta}\) converges to the same limit regardless of the order with respect to which the limit is taken or the depth-to-width ratio. Furthermore, the limit is _deterministic_, and can be described by an ordinary differential equation (ODE). Intuitively, the convergence to a deterministic quantity occurs because \(\gamma=1/\sqrt{d}\) suppresses the random fluctuations enough to vanish in the limit. On the other hand, our results show that for \(\lambda,\gamma\) constant in \(n,d\), the random fluctuations are on the right order of \(O(n^{-1/2})\) as in the MLP case (Eq. 3), hence they do not vanish in the simultaneous limit. The most notable difference is that our limiting regime is _non-commutative_ as it depends on the depth-to-width ratio of the network. We remark that both regimes prevents degeneracy of covariance for residual architectures, forming two theories that complement each other.

## 4 Neural Covariance SDE for Softmax-Based Attention

### Unshaped Attention and Its Taylor Expansion

A central piece to the neural covariance SDE theory for MLPs [18] is identifying the exact scaling of shaped activation functions. In particular, the effect of the activations on the covariance Markov chain \(V_{\ell}\) must be on the same order as the random weights in an MLP, thus forming an approximate

Figure 3: Left: Kernel density estimates of correlation \(\rho_{d}^{\alpha\beta}\) for various values of the residual strength parameter \(\gamma\). In particular, \(\gamma=1\) recovers a shaped-ReLU MLP without skip connections, and \(\gamma=1/\sqrt{d}\) is the setting studied in Noci et al. [9] and Hayou and Yang [50]. Solid lines represent finite networks, while our SDE simulations are presented in dashed lines. Right: 95th percentile of the absolute value of the correlation distribution as a function of \(\gamma\). Note reducing \(\gamma\) reduces the concentration around \(\rho^{\alpha\beta}=1\), and our SDE reliably approximates finite size networks. Simulated with \(n=300,d=100,\rho_{0}^{\alpha\beta}=0.2,c_{+}=0,c_{-}=-1\), and \(2^{13}\) samples.

Euler-discretization

\[V_{\ell+1}=V_{\ell}+\frac{b(V_{\ell})}{n}+\frac{\Sigma(V_{\ell})^{1/2}\xi_{\ell} }{\sqrt{n}}+O(n^{-3/2})\,,\] (7)

where \(b,\Sigma\) are deterministic coefficients, and \(\xi_{\ell}\) are random vectors with zero mean and identity covariance. From here onwards, we use \(O(n^{-p})\) to denote a random variable \(Z\) such that \(n^{p}Z\) has all moments bounded by universal constants (i.e. independent of \(n\)). Since the update can be interpreted as discretization with step size \(n^{-1}\), naturally the Markov chain converges to an SDE. We again note that a stable SDE implies a stable covariance structure for finite size networks.

To achieve the same goal for modified attention mechanisms, we consider a similar approach as M. Li et al. [18] for smooth activation functions, and Taylor expand the Softmax function in terms of a large temperature parameter \(\tau\). To this end, let \(Y_{\ell}\) to be the matrix of dot-products between queries, and keys, i.e. \(Y_{\ell}:=X_{\ell}\ \frac{1}{\sqrt{n}}W_{\ell}^{Q}\ \frac{1}{\sqrt{n}}W_{\ell}^{K, \top}\ X_{\ell}^{\top}\).

More specifically, given a row \(y^{\alpha}\in\mathbb{R}^{1\times m}\) of the logits \(Y_{\ell}\in\mathbb{R}^{m\times m}\), we can Taylor expand the row-wise Softmax in terms of \(\tau^{-1}\):

\[\text{Softmax}(\tau^{-1}y^{\alpha})=\frac{1}{m}\mathbf{1}^{\top}+\frac{1}{ \tau m}(y^{\alpha}-\overline{y^{\alpha}})+\frac{1}{2\tau^{2}m}\left[(y^{\alpha }-\overline{y^{\alpha}})^{2}-\left(\overline{y^{\alpha}}^{2}-\overline{(y^{ \alpha})^{2}}\right)\right]+O(\tau^{-3}),\] (8)

where \(\overline{y^{\alpha}}:=\frac{1}{m}\sum_{\beta}y^{\alpha\beta}\mathbf{1}^{\top}\) and \((y^{\alpha})^{2}\) is the vector with squared entries of \(y^{\alpha}\), and \(\mathbf{1}\in\mathbb{R}^{m\times 1}\) is the (column) vector of ones. We note in practice \(\tau\) is often set to \(\sqrt{n_{k}}\), which is often quite large and allows for asymptotic analysis [9].

We observe that the zero-th order term \(m^{-1}\mathbf{1}^{\top}\) is independent of \(\tau\). Considering the form of the attention block as \(A_{\ell}X_{\ell}\frac{1}{\sqrt{n}}W_{\ell}^{V}\), this yields an update that is no longer a small perturbation of \(V_{\ell}\), regardless of how \(\tau\) is chosen. Therefore, to form a Markov chain like Eq. 7, we actually require \(A_{\ell}\) to be approximately the identity matrix.

### Shaped Attention

To shape the Softmax-attention mechanism as a perturbation of the identity matrix, we propose the following modifications which we call the _shaped attention_5

Footnote 5: In principle, it could be possible to have a close-to-identity Softmax matrix when the logits are large. However, this regime also corresponds to a very saturated Softmax, thus making training unstable [57]. As a result, we will avoid this direction in this work.

\[A_{\ell}=I+\text{Softmax}(\tau^{-1}Y_{\ell})-\frac{1}{m}\mathbf{1}\mathbf{1}^ {\top}\,,\quad\tau=\tau_{0}\sqrt{nn_{k}}\,.\] (9)

The shaped attention presents three modifications to the Softmax attention in Eq. 2. Firstly, the zero-order term \(m^{-1}\mathbf{1}\mathbf{1}^{\top}\) of the Taylor expansion (Eq. 8) is removed as it causes a non-infinitesimal drift in the Markov Chain that ultimately leads to instability in the covariance (see Section 4.1). Secondly, we also observe that when \(\tau\) is very large, the centered Softmax is a perturbation around zero. To recover an approximate Euler-update like in Eq. 7, we simply add back the identity matrix. By biasing the attention matrix towards the identity, we encourage each token to self-attend. This type of modification was also previously considered by B. He et al. [46]. Finally, the Softmax's temperature is chosen to scale as \(\tau=\tau_{0}\sqrt{nn_{k}}\), for some constant \(\tau_{0}>0\), which guarantees a non-degenerate limit as \((d,n)\to\infty\) (Theorem 4.2). Note that the extra \(\sqrt{n}\) term is a departure from the standard parameterization.

In Figure 4, we show how removing any of the proposed changes individually alters the neural covariance structure, which becomes degenerate for large depths, while the proposed modifications remain stable. We stress that here for simplicity we focus on attention without masking. Shaped attention can be extended to include masking (e.g. casual masking) by centering each i-th row of the Softmax matrix by a different factor \(1/m_{i}\), where \(m_{i}\) is the number of un-masked tokens in the i-th row.

### Main Result - Neural Covariance SDEs for Shaped Attention Models and Shaped Transformers

Before we state our main results, we will first define a weakened notion of convergence, which is required whenever the drift and covariance coefficients are not Lipschitz. This was also required for the case of shaped MLPs with smooth activations [18].

**Definition 4.1** (Local Convergence).: _We say the covariance \(V^{(n)}\) converges locally to \(V\) if the stopped process \(\{V^{(n)}_{t\wedge T_{r}}\}_{t\geq 0}\) converges to \(\{V_{t\wedge T_{r}}\}_{t\geq 0}\) in the sense of Definition 3.1 for all stopping times of the form \(T_{r}=\inf\{t>0:\|V_{t}\|\geq r\}\) with \(r>0\)._

Let the covariance with respect to the average token be defined as \(V^{\alpha\bar{x}}:=m^{-1}\sum_{\nu=1}^{m}V^{\alpha\nu}\), and the average trace be \(\bar{V}:=m^{-1}\sum_{\nu=1}^{m}V^{\nu\nu}\). We will need to compute a couple of important moments from the Taylor expansion terms of the Softmax (Lemma C.2)

\[\begin{split} S_{1}^{\alpha\delta,\beta\omega}&:=n _{k}^{-1}\mathbb{E}(Y^{\alpha\delta}-\overline{y^{\alpha}})(Y^{\beta\omega}- \overline{y^{\beta}})=V^{\alpha\beta}\left(V^{\delta\omega}-V^{\delta\bar{x}} -V^{\omega\bar{x}}+V^{\bar{x}\bar{x}}\right)\,,\\ S_{2}^{\alpha\delta}&:=n_{k}^{-1}\mathbb{E}\left[ (Y^{\alpha\delta}-\overline{y}^{\alpha})^{2}-(\overline{(Y^{\alpha})^{2}}- \overline{y^{\alpha}}^{2})\right]=V^{\alpha\alpha}\left(V^{\delta\delta}-2V^ {\delta\bar{x}}+2V^{\bar{x}\bar{x}}-\bar{V}\right)\,.\end{split}\] (10)

We are now ready to state our main result.

**Theorem 4.2**.: _Let \(X_{\ell}\) be the hidden layers of a residual attention network defined in Eq. 1 with shaped attention in Eq. 9, parameters \(\lambda^{2}+\gamma^{2}=1\) and \(\tau=\tau_{0}\sqrt{nn_{k}}\), where \(\lambda,\gamma,\tau_{0}\) all do not depend on \(d,n\). Then the feature covariance \(V_{\ell}\) converges locally to the solution of the following SDE (in the sense of Definition 4.1)_

\[dV_{t}=b(V_{t})dt+\Sigma(V_{t})^{1/2}dB_{t}\,,\quad V_{0}=\frac{1}{n}X_{0}X_{0 }^{\top}\,,\]

_where the drift has the following form_

\[b(V)=\frac{\gamma^{2}}{\tau_{0}^{2}}\left[\frac{1}{m^{2}}\sum_{\nu,\kappa=1}^{ m}V^{\nu\kappa}S_{1}^{\alpha\nu,\beta\kappa}+\frac{1}{2m}\sum_{\nu=1}^{m}(V^{ \beta\nu}S_{2}^{\alpha\nu}+V^{\alpha\nu}S_{2}^{\beta\nu})\right]_{\alpha\leq \beta}\,,\]

_the diffusion coefficient is defined by \(\Sigma(V)=\gamma^{2}(2-\gamma^{2})\Sigma_{\text{lin}}(V)+\gamma^{4}\tau_{0}^{-2 }[\mathcal{A}^{\alpha\beta,\delta\omega}]_{\alpha\leq\beta,\delta\leq\omega}\), and_

\[\mathcal{A}^{\alpha\beta,\delta\omega}:=\frac{1}{m^{2}}\sum_{\nu,\kappa=1}^{m} \left(V^{\alpha\kappa}V^{\delta\nu}S_{1}^{\beta\kappa,\omega\nu}+V^{\alpha \kappa}V^{\omega\nu}S_{1}^{\beta\kappa,\delta\nu}+V^{\beta\nu}V^{\delta\kappa }S_{1}^{\alpha\nu,\omega\kappa}+V^{\beta\nu}V^{\omega\kappa}S_{1}^{\alpha\nu,\delta\kappa}\right)\,.\]

Figure 4: Mean correlation (left) and covariance (right) (in absolute value) under various interventions on the proposed shaped attention. In particular, we remove either one or two of the three modifications from the shaped attention in Eq. 9. For instance “\(\tau^{2}=nn_{k}\), center” indicates that we use the proposed temperature, and we center by \(m^{-1}\), but we do not add the identity matrix, while in “only id” we add the identity matrix but use \(\tau=\sqrt{n_{k}}\) and do not center. We note in this “only id” case, the covariance remains unstable due to incorrect scaling. Due to exploding covariance, we choose to not include the cases “id, \(\tau^{2}=nn_{k}\)” and “only id” in the correlation plot (but only in the covariance plot). Simulated with \(n=300,d=150,\rho_{0}^{\alpha\beta}=0.2\), \(\gamma=1/\sqrt{2}\) and \(2^{13}\) samples.

The drift depends on the shaped attention mechanism through \(S_{1}^{\alpha\delta,\beta\omega}\) and \(S_{2}^{\alpha\delta}\), the moments of the first and second order terms of the Softmax's Taylor expansion. On the other hand, the diffusion term depends on the attention solely through \(S_{1}\), present in the additional term \(\mathcal{A}^{\alpha\beta,\delta\omega}\). The presence of \(\mathcal{A}^{\alpha\beta,\delta\omega}\) is an intriguing difference compared to shaped ReLU networks, where the diffusion is not affected by the activation function. Both components of the SDE depend on averages over the tokens, reflecting the mixing property of the self-attention mechanism, in which every pair of tokens is compared through dot products to form the attention weights. Finally, notice how the residual branch parameter \(\gamma^{2}\) has a dampening effect on the scale of both the drift and the diffusion in a similar way as in fully-connected residual network.

We are now ready to introduce the full shaped Transformer architecture, where we combine the attention and residual layers:

\[Z_{\ell}=\lambda X_{\ell}+\gamma A_{\ell}X_{\ell}\frac{1}{\sqrt{n}}W_{\ell}^{ V}\,,\quad X_{\ell+1}=\lambda Z_{\ell}+\gamma\sigma_{s}\left(Z_{\ell}\frac{1}{ \sqrt{n}}W_{\ell}^{\text{pre}}\right)\sqrt{\frac{c}{n}}W_{\ell}^{\text{post}}\,,\] (11)

where \(A_{\ell}\) is the shaped attention defined by Eq. 9. We note that covariance SDE handle stacking of different layer types very conveniently by simply adding the drift and covariance of the diffusion coefficients, which we summarize in the Corollary below.

**Corollary 4.3** (Shaped Transformer Covariance SDE).: _Let \(X_{\ell}\) be the hidden layers of a shaped transformer defined in Eq. 11 with parameters \(\lambda^{2}+\gamma^{2}=1\) and \(\tau=\tau_{0}\sqrt{nn_{k}}\), where \(\lambda,\gamma,\tau_{0}\) all do not depend on \(d,n\). Then the feature covariance \(V_{\ell}\) converges locally to the solution of the following SDE (in the sense of Definition 4.1)_

\[dV_{t}=\left[b(V_{t})+b_{\text{res}}(V_{t})\right]dt+\left[\Sigma(V_{t})+ \Sigma_{\text{res}}(V_{t})\right]^{1/2}dB_{t}\,,\] (12)

_where the coefficients are defined in Theorem 3.2 and Theorem 4.2._

### On Finite Time Stability of the SDE and Shaped Attention Networks

Although we did not observe numerical instability in majority of our simulations of the shaped attention networks and the corresponding SDE, we did observe that the drift component \(b(V_{t})\) in Theorem 4.2 is cubic in the entries of \(V_{t}\). Whenever the drift is not Lipschitz as in this case, we do not have general guarantees for the existence of a solution for all time (see the Feller test for explosions (58, Theorem 5.5.29)). In fact, MLPs with smooth activations also yield non-Lipschitz drift coefficients as seen in M. Li et al. (2018).

However, locally Lipschitz coefficients are sufficient to guarantee the existence of local solutions, in the sense of up to a stopping time (59, Proposition 6.9). Not only does this fact help us establish a precise notion of convergence (Definition 4.1), we can also study the practical implications of this for finite sized attention networks. More specifically, we can inspect the effect of architectural changes to a stopping time.

To demonstrate the potential numerical instabilities, we had to choose an _adversarial_ set of parameters: in particular, an unrealistically large norm (approx. \(10\sqrt{n}\)) for the initial tokens \(X_{0}\), which enlarges the eigenvalues of \(V_{0}\) to the order of \(100\). Given these initial conditions and a large residual connection weight \(\gamma\), we were able to consistently generate numerically unstable behaviour in shaped attention networks (see Figure 5 (left)).

That being said, it is very straight forward to stabilize the network by tweaking parameters such as \(\gamma,\tau_{0}\) and the depth-to-width ratio of the network. We demonstrate the effect of tuning \(\gamma\) on both sample trajectories of the maximum eigenvalue of \(V_{\ell}\) and the stopping time in Figure 5. As we may intuitively expect, tuning \(\gamma\) smaller will delay the time scale of numerical instabilities, hence allowing for larger depth networks to remain stable.

## 5 Discussion

**Architecture Design and Hyperparameter Tuning.** Previous work have demonstrated the practical impact scaling limits can have on designing activation functions (16; 17) and tuning hyperparameters (19). We follow this line of motivations and proposed a novel attention mechanism, which successfully stabilizes the covariance structure in arbitrarily deep Transformers (e.g. Figure 1). The natural next step is to investigate the scaling of gradients in the infinite-depth-and-width limit. As Yang et al. [19] illustrated, the existence of an infinite-width limit for the gradient implies the optimal hyperparameters for the training algorithm will also converge. This type of results allows for tuning of hyperparameters on networks with a much smaller width, yet extends easily to arbitrarily large networks that approximates the same limit, saving massive amounts of computing cost in the process. Given the existence of an infinite-depth-and-width limit for the forward pass, we believe it's possible to extract optimal hyperparameters from networks with not only a much smaller width, but _smaller depth_ as well.

**Preliminary Experiments.** Although this work is primarily theoretical, it is important to consider whether or not the proposed architecture is useful in practice. Given limited computing resources, we chose to only briefly test the feasibility of training the shaped Transformer. Nevertheless, our preliminary experiments show promising results when it comes to training stability. In particular, the shaped Transformer (without LayerNorm) does indeed train at approximately the same speed as well tuned Transformer architectures. Full details of the experiment and results can be found in Appendix D. A more comprehensive set of experiments with different tasks, datasets, and larger networks will be required to confidently determine the practical feasibility of the shaped Transformer, which we defer to future work.

**Training Dynamics and Generalization.** As mentioned in the introduction, the limitations of infinite-width NTK theories motivates our study of the proportional infinite-depth-and-width limit. In particular, to address many of the open problems in deep learning theory, we need a faithful and tractable description of training dynamics. Given the results at initialization, the proportional limit holds the potential for such a theory of training as well. Another promising indicator is that deep networks learn features in the proportional regime [38], which has been identified as a key advantage of neural networks over kernel methods [60, 61, 25, 62, 63, 64, 65, 66]. A precise theory of training will help us understand other types of instabilities during training and improve existing optimization methods. Furthermore, determining the network which training converges to is a necessary step towards a theory of generalization, as demonstrated by the infinite-width approach [67]. In light of our results, we believe that our theory sets the stage for future work on training and generalization in deep learning.

## Acknowledgement

CL and ML would like to thank Keiran Paster for insightful discussions. LN would like to thank Sotiris Anagnostidis for support in pre-processing the dataset used for the training experiments of this manuscript. ML is supported by the Ontario Graduate Scholarship and Vector Institute. DMR is supported in part by Canada CIFAR AI Chair funding through the Vector Institute, an NSERC Discovery Grant, Ontario Early Researcher Award, a stipend provided by the Charles Simonyi Endowment, and a New Frontiers in Research Exploration Grant.

Figure 5: Left: Trajectories of the maximum eigenvalue of the covariance matrix in a shaped attention network, with _adversarially_ large initial condition. Right: Stopping time of the shaped attention neural network, capped at 1. Stopping time is defined as \(t^{*}=d^{*}/n\) with \(d^{*}\) the maximum depth beyond which one of the eigenvalues of the covariance matrix exceeds \(10^{4}\) or drops below \(10^{-4}\). Simulated with \(n=d=200\), \(\tau_{0}=1\), and \(100\) samples used for median and \(10\)th percentile.

## References

* [1]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. In Advances in Neural Information Processing Systems, pp. 1877-1901. Cited by: SS1.
* [2]A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin (2017) Attention is all you need. External Links: 1706.03762 Cited by: SS1.
* [3]J. Hestness, S. Narang, N. Ardalani, G. Diamos, H. Jun, H. Kianinejad, M. Patwary, M. Ali, Y. Yang, and Y. Zhou (2017) Deep learning scaling is predictable, empirically. External Links: 1712.00409 Cited by: SS1.
* [4]J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei (2020) Scaling laws for neural language models. External Links: 2001.08361 Cited by: SS1.
* [5]J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, et al. (2022) Training compute-optimal large language models. External Links: 2203.15556 Cited by: SS1.
* [6]Y. Dong, J. Cordonnier, and A. Loukas (2021) Attention is not all you need: pure attention loses rank doubly exponentially with depth. International Conference on Machine Learning, PMLR. External Links: 2102.07932 Cited by: SS1.
* [7]A. Aghajanyan, L. Yu, A. Conneau, W. Hsu, K. Hambardzumyan, S. Zhang, S. Roller, N. Goyal, O. Levy, and L. Zettlemoyer (2023) Scaling laws for generative mixed-modal language models. External Links: 2301.03728 Cited by: SS1.
* [8]Y. Dong, J. Cordonnier, and A. Loukas (2021) Attention is not all you need: pure attention loses rank doubly exponentially with depth. International Conference on Machine Learning, PMLR. External Links: 2102.07932 Cited by: SS1.
* [9]L. Noci, S. Anagnostidis, L. Biggio, A. Orvieto, S. P. Singh, and A. Lucchi (2022) Signal propagation in transformers: theoretical perspectives and the role of rank collapse. External Links: 2206.03126 Cited by: SS1.
* [10]B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, and S. Ganguli (2016) Exponential expressivity in deep neural networks through transient chaos. In Advances in Neural Information Processing Systems, pp. 29. Cited by: SS1.
* [11]S. S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein (2017) Deep Information Propagation. ICLR. External Links: 1706.03766 Cited by: SS1.
* [12]G. Yang and S. S. Schoenholz (2017) Mean field residual networks: on the edge of chaos. Advances in Neural Information Processing Systems. External Links: 1706.03766 Cited by: SS1.
* [13]S. Hayou, A. Doucet, and J. Rousseau (2019) On the impact of the activation function on deep neural networks training. International Conference on Machine Learning, PMLR. External Links: 1906.02672 Cited by: SS1.
* [14]M. Murray, V. Abrol, and J. Tanner (2022) Activation function design for deep networks: linearity and effective initialisation. In Applied and Computational Harmonic Analysis, pp. 117-154. Cited by: SS1.
* [15]L. Xiao, J. Pennington, and S. Schoenholz (2020) Disentangling trainability and generalization in deep neural networks. International Conference on Machine Learning, PMLR. External Links: 2010.04621 Cited by: SS1.
* [16]J. Martens, A. Ballard, G. Desjardins, G. Swirszcz, V. Dalibard, J. Sohl-Dickstein, and S. S. Schoenholz (2021) Rapid training of deep neural networks without skip connections or normalization layers using deep kernel shaping. External Links: 2110.01765 Cited by: SS1.
* [17]G. Zhang, A. Botev, and J. Martens (2022) Deep learning without shortcuts: shaping the kernel with tailored rectifiers. External Links: 2203.08120 Cited by: SS1.
* [18]M. Li, M. Nica, and D. Roy (2022) The neural covariance SDE: shaped infinite depth-and-width networks at initialization. In Advances in Neural Information Processing Systems, pp. 10795-10808. Cited by: SS1.
* [19]G. Yang, E. J. Hu, I. Babuschkin, S. Sidor, X. Liu, D. Farhi, N. Ryder, J. Pachocki, W. Chen, and J. Gao (2022) Tensor programs V: tuning large neural networks via zero-shot hyperparameter transfer. External Links: 2203.03466 Cited by: SS1.

[MISSING_PAGE_POST]

* [21] J. Lee, Y. Bahri, R. Novak, S. S. Schoenholz, J. Pennington, and J. Sohl-Dickstein. "Deep Neural Networks as Gaussian Processes". _ICLR_. 2018.
* [22] A. G. d. G. Matthews, M. Rowland, J. Hron, R. E. Turner, and Z. Ghahramani. _Gaussian process behaviour in wide deep neural networks_. 2018. arXiv: 1804.11271.
* [23] J. Hron, Y. Bahri, J. Sohl-Dickstein, and R. Novak. "Infinite attention: NNGP and NTK for deep attention networks". _International Conference on Machine Learning_. PMLR. 2020, pp. 4376-4386.
* [24] A. Jacot, F. Gabriel, and C. Hongler. "Neural tangent kernel: Convergence and generalization in neural networks". _Advances in Information Processing Systems (NeurIPS)_. 2018. arXiv: 1806.07572.
* [25] G. Yang and E. J. Hu. _Feature learning in infinite-width neural networks_. 2020. arXiv: 2011.14522.
* [26] J. Sirignano and K. Spiliopoulos. "Mean field analysis of neural networks: A law of large numbers". In: _SIAM Journal on Applied Mathematics_ 80.2 (2020), pp. 725-752.
* [27] S. Mei, A. Montanari, and P.-M. Nguyen. "A mean field view of the landscape of two-layer neural networks". In: _Proceedings of the National Academy of Sciences_ 115.33 (2018), E7665-E7671.
* [28] L. Chizat and F. Bach. "On the global convergence of gradient descent for over-parameterized models using optimal transport". In: _Advances in Neural Information Processing Systems_ 31 (2018).
* [29] G. M. Rotskoff and E. Vanden-Eijnden. _Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach_. 2018. arXiv: 1805.00915.
* [30] S. Yaida. "Non-Gaussian processes and neural networks at finite widths". _Mathematical and Scientific Machine Learning_. PMLR. 2020, pp. 165-192.
* [31] E. Dyer and G. Gur-Ari. "Asymptotics of wide networks from feynman diagrams". In: _arXiv preprint arXiv:1909.11304_ (2019).
* [32] D. A. Roberts, S. Yaida, and B. Hanin. _The principles of deep learning theory_. Cambridge University Press, 2022.
* [33] J. Zavatone-Veth and C. Pehlevan. "Exact marginal prior distributions of finite Bayesian neural networks". In: _Advances in Neural Information Processing Systems_ 34 (2021).
* [34] J. Zavatone-Veth, A. Canatar, B. Ruben, and C. Pehlevan. "Asymptotics of representation learning in finite Bayesian neural networks". In: _Advances in Neural Information Processing Systems_ 34 (2021), pp. 24765-24777.
* [35] B. Hanin. _Correlation Functions in Random Fully Connected Neural Networks at Finite Width_. 2022. arXiv: 2204.01058.
* [36] E. Dinan, S. Yaida, and S. Zhang. _Effective Theory of Transformers at Initialization_. 2023. arXiv: 2304.02034.
* [37] B. Hanin and M. Nica. "Products of many large random matrices and gradients in deep neural networks". In: _Communications in Mathematical Physics_ (2019), pp. 1-36.
* [38] B. Hanin and M. Nica. _Finite depth and width corrections to the neural tangent kernel_. 2019. arXiv: 1909.05989.
* [39] Z. Hu and H. Huang. "On the Random Conjugate Kernel and Neural Tangent Kernel". _International Conference on Machine Learning_. PMLR. 2021, pp. 4359-4368.
* [40] M. B. Li, M. Nica, and D. Roy. "The future is log-Gaussian: ResNets and their infinite-depth-and-width limit at initialization". In: _Advances in Neural Information Processing Systems_ 34 (2021), pp. 7852-7864.
* [41] L. Noci, G. Bachmann, K. Roth, S. Nowozin, and T. Hofmann. "Precise characterization of the prior predictive distribution of deep ReLU networks". In: _Advances in Neural Information Processing Systems_ 34 (2021).
* [42] B. Hanin and G. Paouris. "Non-asymptotic results for singular values of Gaussian matrix products". In: _Geometric and Functional Analysis_ 31.2 (2021), pp. 268-324.
* [43] G. Akemann, Z. Burda, and M. Kieburg. "From integrable to chaotic systems: Universal local statistics of Lyapunov exponents". In: _Europhysics Letters_ 126.4 (2019), p. 40001.
* [44] J. Ipsen. "Lyapunov exponents for products of rectangular real, complex and quaternionic Ginibre matrices". In: _Journal of Physics A: Mathematical and Theoretical_ 48.15 (2015), p. 155204.

* [45] T. Jiang and Y. Qi. "Spectral radii of large non-Hermitian random matrices". In: _Journal of Theoretical Probability_ 30.1 (2017), pp. 326-364.
* [46] B. He, J. Martens, G. Zhang, A. Botev, A. Brock, S. L. Smith, and Y. W. Teh. _Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation_. 2023. arXiv: 2302.10322.
* [47] M. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer, A. Steiner, M. Caron, R. Geirhos, I. Alabdulmohsin, et al. _Scaling vision transformers to 22 billion parameters_. 2023. arXiv: 2302.05442.
* [48] R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, and T. Liu. "On layer normalization in the transformer architecture". _International Conference on Machine Learning_. PMLR. 2020, pp. 10524-10533.
* [49] J. L. Ba, J. R. Kiros, and G. E. Hinton. "Layer normalization". In: (2016). arXiv: 1607.06450.
* [50] S. Hayou and G. Yang. _Width and Depth Limits Commute in Residual Networks_. 2023. arXiv: 2302.00453.
* [51] S. Hayou, E. Clerico, B. He, G. Deligiannidis, A. Doucet, and J. Rousseau. "Stable resnet". _International Conference on Artificial Intelligence and Statistics_. PMLR. 2021, pp. 1324-1332.
* [52] W. Tarnowski, P. Warchol, S. Jastrzebski, J. Tabor, and M. Nowak. "Dynamical isometry is achieved in residual networks in a universal way for any activation function". _The 22nd International Conference on Artificial Intelligence and Statistics_. PMLR. 2019, pp. 2221-2230.
* [53] S. Hayou. _On the infinite-depth limit of finite-width neural networks_. 2022. arXiv: 2210.00688.
* [54] G. Yang. "Tensor programs ii: Neural tangent kernel for any architecture". In: _arXiv preprint arXiv:2006.14548_ (2020).
* [55] K. He, X. Zhang, S. Ren, and J. Sun. "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification". _Proceedings of the IEEE international conference on computer vision_. 2015, pp. 1026-1034.
* [56] A. Veit, M. J. Wilber, and S. Belongie. "Residual networks behave like ensembles of relatively shallow networks". In: _Advances in neural information processing systems_ 29 (2016).
* [57] S. Zhai, T. Likhomanenko, E. Littwin, D. Busbridge, J. Ramapuram, Y. Zhang, J. Gu, and J. Susskind. _Stabilizing Transformer Training by Preventing Attention Entropy Collapse_. 2023. arXiv: 2303.06296.
* [58] I. Karatzas and S. Shreve. _Brownian motion and stochastic calculus_. Vol. 113. Springer Science & Business Media, 2012.
* [59] J. Miller. _Stochastic Calculus (Lecture Notes)_. http://www.statslab.cam.ac.uk/~jpm205/teaching/lent2016/lecture_notes.pdf. 2015.
* [60] B. Ghorbani, S. Mei, T. Misiakiewicz, and A. Montanari. "When do neural networks outperform kernel methods?" In: _Advances in Neural Information Processing Systems_ 33 (2020), pp. 14820-14830.
* [61] E. Abbe, E. B. Adsera, and T. Misiakiewicz. "The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks". _Conference on Learning Theory_. PMLR. 2022, pp. 4782-4887.
* [62] J. Ba, M. A. Erdogdu, T. Suzuki, Z. Wang, D. Wu, and G. Yang. "High-dimensional asymptotics of feature learning: How one gradient step improves the representation". _Int. Conf. Learning Representations (ICLR)_. 2022. url: https://openreview.net/forum?id=akddwRG6EGi.
* [63] A. Damian, J. Lee, and M. Soltanolkotabi. "Neural networks can learn representations with gradient descent". _Conference on Learning Theory_. PMLR. 2022, pp. 5413-5452.
* [64] A. Mousavi-Hosseini, S. Park, M. Girotti, I. Mitliagkas, and M. A. Erdogdu. _Neural Networks Efficiently Learn Low-Dimensional Representations with SGD_. 2022. arXiv: 2209.14863.
* [65] E. Abbe, E. Boix-Adsera, and T. Misiakiewicz. _SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics_. 2023. arXiv: 2302.11055.
* [66] R. Berthier, A. Montanari, and K. Zhou. _Learning time-scales in two-layers neural networks_. 2023. arXiv: 2303.00055.

* [67] P. L. Bartlett, A. Montanari, and A. Rakhlin. "Deep learning: a statistical viewpoint". In: _Acta numerica_ 30 (2021), pp. 87-201.
* [68] O. Kallenberg. _Foundations of Modern Probability_. Probability theory and stochastic modelling. Springer, 2021. isbn: 9783030618728.
* [69] HuggingFace. _Wikipedia data set_. https://huggingface.co/datasets/wikipedia.
* [70] HuggingFace. _Bookcorpus data set_. https://huggingface.co/datasets/bookcorpus.
* [71] D. P. Kingma and J. Ba. "Adam: A method for stochastic optimization". In: _arXiv preprint arXiv:1412.6980_ (2014).
* [72] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. "GLUE: A multi-task benchmark and analysis platform for natural language understanding". In: _arXiv preprint arXiv:1804.07461_ (2018).
* [73] HuggingFace. _Hugging face Bert implementation_. https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py.

Preliminaries: Covariance SDE Framework

In this section, we will review existing results on Markov chain convergence to an SDE, as well as existing results for the covariance SDE. See also the Appendix of M. Li et al. [18] for more details.

Firstly, we will define the Skorohod \(J_{1}\)-topology, or just the Skorohod topology for short [68, Appendix 5]. The topology is used to describe convergence of continuous time processes with discontinuities, in particular, Markov chains with a continuous time interpolation fits in this category. Let \(S\) be a complete separable metric space, and \(D_{\mathbb{R}_{+},S}\) be the space of cadlag functions (right continuous with left limits) from \(\mathbb{R}_{+}\to S\). We use \(x_{n}\xrightarrow{ul}x\) to denote locally uniform convergence (uniform on compact subsets of \(\mathbb{R}_{+}\)), and consider the class of bijections \(\lambda\) on \(\mathbb{R}_{+}\) so that \(\lambda\) is strictly increasing with \(\lambda_{0}=0\) (can be interpreted as a time change).

**Definition A.1**.: _We define **Skorohod convergence**\(x_{n}\xrightarrow{s}x\) on \(D_{\mathbb{R}_{+},S}\) if there exists a sequence of bijections \(\lambda_{n}\) satisfying above conditions and_

\[\lambda_{n}\xrightarrow{ul}\text{Id}\,,\quad x_{n}\circ\lambda_{n} \xrightarrow{ul}x\,.\] (13)

_In particular, we call the topology that induces this convergence the **Skorohod topology**[68, Theorem A5.3]._

On a heuristic level, if we have sequences of Markov chains \(Y^{n}\) that satisfy the following type of Euler updates

\[Y^{n}_{\ell+1}=Y^{n}_{\ell}+\frac{b(Y^{n}_{\ell})}{n}+\frac{\sigma(Y^{n}_{\ell })\,\xi_{\ell}}{\sqrt{n}}\,,\] (14)

where \(\xi_{\ell}\) are \(\mathrm{i}\mathrm{d}\) random variables with zero mean and identity covariance, then we can interpolate this process in continuous time with \(X^{n}_{t}=Y^{n}_{\lfloor tn\rfloor}\) and show that as \(n\to\infty\), we have that \(X^{n}\) converges to the solution of the following SDE (weakly with respect to the Skorohod topology)

\[dX_{t}=b(X_{t})\,dt+\sigma(X_{t})\,dB_{t}\,,\quad X_{0}=\lim_{n\to\infty}Y^{n} _{0}\,.\] (15)

Our next theorem essentially weakens this result in several ways. Firstly, we don't need to take the step size \(n^{-1}\), but instead replace it with \(n^{-2p}\) for all \(p>0\). Next, we can allow the update to contain higher order terms that vanish, in particular, any terms of order \(O(n^{-2p-\delta})\) for all \(\delta>0\). Here, we remind the readers that \(O(n^{-p})\) denotes a random variable \(Z\) such that \(n^{p}Z\) has all moment bounded by a universal constant (independent of \(n\)). Thirdly, we can allow \(b(y)\) to be random, or more precisely replace it with \(\widehat{b}(y,\omega_{n})\) such that \(\mathbb{E}\widehat{b}(y,\omega_{n})=b(y)\). Fourthly, we can also allow \(b,\sigma\) to be a sequence \(b_{n},\sigma_{n}\) such that they converge to \(b,\sigma\) on uniformly on compact sets. Finally, we can also weaken the topology of convergence to locally, that is all processes are stopped by a stopping time as in Definition 4.1.

Now we will state the main technical result in this section.

**Proposition A.2** (Convergence of Markov Chains to SDE, Proposition A.6, [18]).: _Let \(Y^{n}\) be a discrete time Markov chain on \(\mathbb{R}^{N}\) defined by the following update for \(p,\delta>0\)_

\[Y^{n}_{\ell+1}=Y^{n}_{\ell}+\frac{\widehat{b}_{n}(Y^{n}_{\ell},\omega^{n}_{ \ell})}{n^{2p}}+\frac{\sigma_{n}(Y^{n}_{\ell})}{n^{p}}\xi^{n}_{\ell}+O(n^{-2p- \delta})\,,\] (16)

_where \(\xi^{n}_{\ell}\in\mathbb{R}^{N}\) are \(\mathrm{i}\mathrm{d}\) random variables with zero mean, identity covariance, and moments uniformly bounded in \(n\). Furthermore, \(\omega^{n}_{\ell}\) are also \(\mathrm{i}\mathrm{d}\mathrm{n}\) random variables such that \(\mathbb{E}\widehat{b}_{n}(Y^{n}_{\ell},\omega^{n}_{\ell})|Y^{n}_{\ell}=y|=b_{n }(y)\) and \(\widehat{b}_{n}(y,\omega^{n}_{\ell})\) has uniformly bounded moments in \(n\). Finally, \(\sigma_{n}\) is a deterministic function, and the remainder terms in \(O(n^{-2p-\delta})\) have uniformly bounded moments in \(n\)._

_Suppose \(b_{n},\sigma_{n}\) are uniformly Lipschitz functions in \(n\) and converges to \(b,\sigma\) uniformly on compact sets, then in the limit as \(n\to\infty\), the process \(X^{n}_{t}=Y^{n}_{\lfloor tn^{2p}\rfloor}\) converges in distribution to the solution of the following SDE in the Skorohod topology of \(D_{\mathbb{R}_{+},\mathbb{R}^{N}}\)_

\[dX_{t}=b(X_{t})\,dt+\sigma(X_{t})\,dB_{t}\,,\quad X_{0}=\lim_{n\to\infty}Y^{n} _{0}\,.\] (17)_Suppose otherwise \(b_{n},\sigma_{n}\) are only locally Lipschitz (but still uniform in \(n\)), then \(X^{n}\) converges locally to \(X\) in the same topology (see Definition 4.1). More precisely, for any fixed \(r>0\), we consider the stopping times_

\[\tau^{n}:=\inf\left\{t\geq 0:|X_{t}^{n}|\geq r\right\}\,,\quad\tau:=\inf\left\{t \geq 0:|X_{t}|\geq r\right\}\,,\] (18)

_then the stopped process \(X_{t\wedge\tau^{n}}^{n}\) converges in distribution to the stopped solution \(X_{t\wedge\tau}\) of the above SDE in the same topology._

We will briefly recall the main result of M. Li et al. [18] next. As mentioned earlier in the text, the setting is for MLPs defined as follows

\[X_{\ell+1}=\sigma_{s}(X_{\ell})\sqrt{\frac{c}{n}}W_{\ell}\,,\] (19)

where \(\sigma_{s}(x)=s_{+}\max(x,0)+s_{-}\min(x,0)\) with \(s_{\pm}=1+\frac{c_{\pm}}{\sqrt{n}}\), \(c^{-1}=\mathbb{E}\,\sigma_{s}(g)^{2}\) for \(g\sim\mathcal{N}(0,1)\), \(n\) is the width of the network, and \(W_{\ell,ij}\sim\mathcal{N}(0,1)\) are iid random weights.

Then it was shown that in the limit as \(n,d\to\infty\) with \(\frac{d}{n}=T>0\), the covariance matrix \(V_{\ell}=\frac{1}{n}X_{\ell}X_{\ell}^{\top}\) (or equivalent the post activation \(\frac{c}{n}\sigma_{s}(X_{\ell})\sigma_{s}(X_{\ell})^{\top}\) since the activation becomes infinitesimal) converges the solution of the following SDE in the Skorohod topology [18, Theorem 3.2]

\[dV_{t}=b_{\text{ReLU}}(V_{t})\,dt+\Sigma_{\text{lin}}(V_{t})^{1/2}\,dB_{t}\,, \quad V_{0}=\frac{1}{n}X_{0}X_{0}^{\top}\,,\] (20)

where the coefficients were given in Theorem 3.2.

We remark that if the output is defined as \(X_{\text{out}}=\frac{1}{\sqrt{n}}W_{\text{out}}X_{d}\in\mathbb{R}^{n_{\text{ out}}}\), then we can recover the output distribution as

\[X_{\text{out}}\sim N(0,V_{T}\otimes I_{n_{\text{out}}})\,,\] (21)

where we treated \(X_{\text{out}}\) as a vector in \(\mathbb{R}^{m_{\text{out}}}\).

## Appendix B SDE for Residual Network

Recall that we adopt the following model:

\[X_{\ell+1}=\lambda X_{\ell}+\gamma\frac{1}{\sqrt{n}}\sigma_{s}\left(\sqrt{ \frac{c}{n}}X_{\ell}W_{\ell}^{\text{pre}}\right)W_{\ell}^{\text{post}},\] (22)

where \(\sigma_{s}(x):=s_{+}\max(x,0)+s_{-}\min(x,0)\) is the shaped ReLU with slopes \(s_{\pm}:=1+\frac{c_{\pm}}{\sqrt{n}}\) for some constants \(c_{+},c_{-}\in\mathbb{R}\). We let the weights be \((W_{\ell}^{\text{pre}})_{ij}(W_{\ell}^{\text{post}})_{ij}\overset{\text{iid}}{ \sim}\mathcal{N}(0,1)\) and \(c^{-1}=\mathbb{E}\sigma_{s}(g)^{2}\) for \(g\sim\mathcal{N}(0,1)\) is the He initialization constant [55].

From here onwards, we will define the filtration \(\mathcal{F}_{\ell}=\sigma(\{X_{k}\}_{k\in[\ell]})\), where \(\sigma(\cdot)\) denotes the sigma-algebra generated by the random variable. Furthermore, we will define the conditional expectation \(\mathbb{E}_{\ell}[\,\cdot\,]=\mathbb{E}[\,\cdot\,|\mathcal{F}_{\ell}]\).

We are interested in studying the _neural covariance_, i.e. \(V_{\ell}^{\alpha\beta}:=\frac{c}{n}\langle x_{\ell}^{\alpha},x_{\ell}^{\beta}\rangle\) where \(\ell\in[d]\) indexes the layers and \(d\) is the depth. In particular, we are interested in understanding the simultaneous limit \((n,d)\to\infty\), where the ratio \(t=d/n\) remains constant.

From now on, we will remove the dependence on \(\ell\). Defining the residual branch as \(f(X,W):=\frac{1}{\sqrt{n}}\sigma_{s}\left(\sqrt{\frac{c}{n}}XW^{\text{pre}} \right)W^{\text{post}}\), we can write \(V^{\alpha\beta}\) as:

\[V^{\alpha\beta}=\lambda^{2}X+\lambda\gamma\langle x^{\alpha},f(X,W)^{\beta} \rangle+\lambda\gamma\langle x^{\beta},f(X,W)^{\alpha}\rangle+\gamma^{2} \langle f(X,W)^{\alpha},f(X,W)^{\beta}\rangle.\]

For the cross product terms, we get:

\[\langle x^{\alpha},f(X,W)^{\beta}\rangle =\frac{\sqrt{c}}{n}\langle x^{\alpha},\left(\sigma_{s}\left(XW^ {\text{pre}}\right)W^{\text{post}}\right)^{\beta}\rangle=\frac{\sqrt{c}}{n} \sum_{i,j}^{n}x_{i}^{\alpha}W_{ji}^{\text{post}}\sigma_{s}\left(\sum_{j^{ \prime}}x_{j^{\prime}}^{\beta}W_{j^{\prime}j}^{\text{pre}}\right)\,,\] \[\langle x^{\beta},f(X,W)^{\alpha}\rangle =\frac{\sqrt{c}}{n}\sum_{i,j}^{n}x_{i}^{\beta}W_{ji}^{\text{post }}\sigma_{s}\left(\sum_{j^{\prime}}x_{j^{\prime}}^{\alpha}W_{j^{\prime}j}^{ \text{pre}}\right).\]For the term in \(\gamma^{2}\):

\[\frac{c}{n^{2}}\langle\left(\sigma_{s}\left(XW^{\text{pre}}\right)W^{ \text{post}}\right)^{\alpha},\left(\sigma_{s}\left(XW^{\text{pre}}\right)W^{ \text{post}}\right)^{\beta}\rangle=\frac{c}{n^{2}}\sum_{i=1}^{n}\left(\sigma_{s }\left(XW^{\text{pre}}\right)W^{\text{post}}\right)_{\alpha i}\left(\sigma_{s} \left(XW^{\text{pre}}\right)W^{\text{post}}\right)_{\beta i}\] \[=\frac{c}{n^{2}}\sum_{i,j,j^{\prime}=1}^{n}W_{ji}^{\text{post}}W_{ j^{\prime}i}^{\text{post}}\sigma_{s}\left(\sum_{k}x_{k}^{\alpha}W_{kj}^{ \text{pre}}\right)\sigma_{s}\left(\sum_{k^{\prime}}x_{k^{\prime}}^{\beta}W_{ k^{\prime}j^{\prime}}^{\text{post}}\right)\,.\]

We will define the following terms

\[\mathcal{T}_{1}^{\alpha\beta}:=\frac{c\sqrt{c}}{n\sqrt{n}}\sum_{i,j=1}^{n}W_{ ji}^{\text{post}}\left(x_{i}^{\alpha}\sigma_{s}\left(\sum_{j^{\prime}}x_{j^{ \prime}}^{\beta}W_{j^{\prime}j}^{\text{pre}}\right)+x_{i}^{\beta}\sigma_{s} \left(\sum_{j^{\prime}}x_{j^{\prime}}^{\alpha}W_{j^{\prime}j}^{\text{pre}} \right)\right),\]

and

\[\mathcal{T}_{2}^{\alpha\beta}:=\frac{c^{2}}{n^{2}\sqrt{n}}\sum_{i,j,j^{\prime} =1}^{n}W_{ji}^{\text{post}}W_{j^{\prime}i}^{\text{post}}\sigma_{s}\left(\sum_{ k}x_{k}^{\alpha}W_{kj}^{\text{pre}}\right)\sigma_{s}\left(\sum_{k^{\prime}}x_{k^{ \prime}}^{\beta}W_{k^{\prime}j^{\prime}}^{\text{pre}}\right).\]

Hence we get the following update for \(V_{\ell}^{\alpha\beta}:=\frac{c}{n}\langle x_{\ell}^{\alpha},x_{\ell}^{\beta}\rangle\):

\[V_{\ell+1}^{\alpha\beta}=\lambda^{2}V_{\ell}^{\alpha\beta}+\frac{\gamma\lambda }{\sqrt{n}}\mathcal{T}_{1}^{\alpha\beta}+\frac{\gamma^{2}}{\sqrt{n}}\mathcal{ T}_{2}^{\alpha\beta}\,.\]

It is easy to see that the cross product terms have (conditional) mean zero. For the term with \(\gamma^{2}\):

\[\mathbb{E}_{\ell}[\mathcal{T}_{2}^{\alpha\beta}]=\sqrt{n}\sqrt{V^{\alpha \alpha}V^{\beta\beta}}cK_{1}(\rho^{\alpha\beta}),\] (23)

where \(K_{1}(\rho^{\alpha\beta}):=\mathbb{E}[\sigma_{s}(g^{\alpha})\sigma_{s}(g^{ \beta})]\) where \(g^{\alpha},g^{\beta}\sim\mathcal{N}(0,1)\) and \(\mathbb{E}[g^{\alpha}g^{\beta}]=\frac{\langle x^{\alpha},x^{\beta}\rangle}{ \|x^{\alpha}\|\|x^{\beta}\|}\). Here we recall \(\mathbb{E}_{\ell}[\,\cdot\,]=\mathbb{E}[\,\cdot\,|\mathcal{F}_{\ell}]\) is the conditional expectation given the sigma-algebra generated by \(\mathcal{F}_{\ell}=\sigma(\{X_{k}\}_{k\in[\ell]})\).

M. Li et al. [18] showed that if the non linearity scaling exponent is \(p=1/2\), then:

\[cK_{1}(\rho)=\rho+\frac{\nu(\rho)}{n}+\mathcal{O}(n^{-3/2}),\]

where \(\nu(\rho)=\frac{(c_{+}+c_{-})^{2}}{2\pi}\left(\sqrt{1-\rho^{2}}+\rho\text{ arccos}(\rho)\right)\). Using this result, and summing and subtracting the mean of \(\mathcal{T}_{2}\):

\[V_{\ell+1}^{\alpha\beta} =\lambda^{2}V_{\ell}^{\alpha\beta}+\gamma^{2}\sqrt{V^{\alpha \alpha}V^{\beta\beta}}cK_{1}(\rho^{\alpha\beta})+\frac{\gamma\lambda}{\sqrt{n }}\mathcal{T}_{1}^{\alpha\beta}+\frac{\gamma^{2}}{\sqrt{n}}(\mathcal{T}_{2}^{ \alpha\beta}-\mathbb{E}_{\ell}[\mathcal{T}_{2}^{\alpha\beta}])\] \[=\lambda^{2}V_{\ell}^{\alpha\beta}+\gamma^{2}V_{\ell}^{\alpha \beta}+\gamma^{2}\sqrt{V^{\alpha\alpha}V^{\beta\beta}}\frac{\nu(\rho^{\alpha \beta})}{n}+\frac{\gamma\lambda}{\sqrt{n}}\mathcal{T}_{1}^{\alpha\beta}+\frac {\gamma^{2}}{\sqrt{n}}(\mathcal{T}_{2}^{\alpha\beta}-\mathbb{E}_{\ell}[ \mathcal{T}_{2}^{\alpha\beta}])+\mathcal{O}(n^{-3/2})\,.\]

Using \(\lambda^{2}+\gamma^{2}=1\):

\[V_{\ell+1}^{\alpha\beta}=V_{\ell}^{\alpha\beta}+\gamma^{2}\sqrt{V^{\alpha \alpha}V^{\beta\beta}}\frac{\nu(\rho^{\alpha\beta})}{n}+\frac{\gamma\lambda}{ \sqrt{n}}\mathcal{T}_{1}^{\alpha\beta}+\frac{\gamma^{2}}{\sqrt{n}}(\mathcal{T} _{2}^{\alpha\beta}-\mathbb{E}_{\ell}[\mathcal{T}_{2}^{\alpha\beta}])+ \mathcal{O}(n^{-3/2})\,.\]

Furthermore, we need second order moments of \(\mathcal{T}_{1}\) and \(\mathcal{T}_{2}\). We derive them in the following Lemmas:

**Lemma B.1**.: \[\mathbb{E}_{\ell}[\mathcal{T}_{1}^{\alpha\beta}\mathcal{T}_{1}^{ \delta\omega}] =V^{\alpha\delta}\sqrt{V^{\beta\beta}V^{\omega\omega}}cK_{1}(\rho^{ \beta\omega})+V^{\alpha\omega}\sqrt{V^{\beta\beta}V^{\delta\delta}}cK_{1}(\rho^{ \beta\delta})\] \[+V^{\beta\delta}\sqrt{V^{\alpha\alpha}V^{\omega\omega}}cK_{1}( \rho^{\alpha\omega})+V^{\beta\omega}\sqrt{V^{\alpha\alpha}V^{\delta\delta}}cK_{1}( \rho^{\alpha\delta})\] \[=2V^{\alpha\delta}V^{\beta\omega}+2V^{\alpha\omega}V^{\beta\delta} +\mathcal{O}(n^{-1})\,.\]Proof.: Recall the definition of \(\mathcal{T}_{1}^{\alpha\beta}\):

\[\mathcal{T}_{1}^{\alpha\beta}:=\frac{c\sqrt{c}}{n\sqrt{n}}\sum_{i,j=1}^{n}W_{ji} ^{\text{post}}\left(x_{i}^{\alpha}\sigma_{s}\left(\sum_{j^{\prime}}x_{j^{\prime}} ^{\beta}W_{j^{\prime}j}^{\text{pre}}\right)+x_{i}^{\beta}\sigma_{s}\left(\sum_{ j^{\prime}}x_{j^{\prime}}^{\alpha}W_{j^{\prime}j}^{\text{pre}}\right)\right),\]

We have that:

\[\mathbb{E}_{\ell}[\mathcal{T}_{1}^{\alpha\beta}\mathcal{T}_{1}^{ \delta\omega}]\] \[=\frac{c^{3}}{n^{3}}\sum_{ij}\mathbb{E}_{\ell}\left[x_{i}^{\alpha }x_{i}^{\delta}\left\|x^{\beta}\right\|\left\|x^{\omega}\right\|\sigma_{s}(g_{ j}^{\beta})\sigma_{s}(g_{j}^{\omega})+x_{i}^{\alpha}x_{i}^{\omega}\left\| \left\|x^{\delta}\right\|\sigma_{s}(g_{j}^{\beta})\sigma_{s}(g_{j}^{\delta})\] \[+x_{i}^{\beta}x_{i}^{\delta}\left\|x^{\alpha}\right\|\left\|x^{ \omega}\right\|\sigma_{s}(g_{j}^{\alpha})\sigma_{s}(g_{j}^{\omega})+x_{i}^{ \beta}x_{i}^{\omega}\left\|\left\|x^{\delta}\right\|\sigma_{s}(g_{j}^{\alpha} )\sigma_{s}(g_{j}^{\delta})\right.\] \[=V^{\alpha\delta}\sqrt{V^{\beta\beta}V^{\omega\omega}}cK_{1}(\rho ^{\beta\omega})+V^{\alpha\omega}\sqrt{V^{\beta\beta}V^{\delta\delta}}cK_{1}( \rho^{\beta\delta})\] \[+V^{\beta\delta}\sqrt{V^{\alpha\alpha}V^{\omega\omega}}cK_{1}( \rho^{\alpha\omega})+V^{\beta\omega}\sqrt{V^{\alpha\alpha}V^{\delta\delta}}cK_{ 1}(\rho^{\alpha\delta})\]

**Lemma B.2**.: \[\mathbb{E}_{\ell}[\mathcal{T}_{2}^{\alpha\beta}]=c\sqrt{n}\sqrt{V^{ \alpha\alpha}V^{\beta\beta}}K_{1}(\rho^{\alpha\beta})=\sqrt{n}V^{\alpha\beta} +\frac{1}{\sqrt{n}}\sqrt{V^{\alpha\alpha}V^{\beta\beta}}\nu(\rho)+\mathcal{O} (n^{-1})\,,\] \[\mathbb{E}_{\ell}[\mathcal{T}_{2}^{\alpha\beta}\mathcal{T}_{2}^{ \delta\omega}]-\mathbb{E}_{\ell}[\mathcal{T}_{2}^{\alpha\beta}]\mathbb{E}[ \mathcal{T}_{2}^{\delta\omega}]=2V^{\alpha\delta}V^{\beta\omega}+2V^{\alpha \omega}V^{\beta\delta}+\mathcal{O}(n^{-1})\,.\]

Proof.: Recall the definition of \(\mathcal{T}_{2}^{\alpha\beta}\):

\[\mathcal{T}_{2}^{\alpha\beta}: =\frac{c^{2}}{n^{2}\sqrt{n}}\sum_{i,j,j^{\prime}=1}^{n}W_{ji}^{ \text{post}}W_{j^{\prime}i}^{\text{post}}\sigma_{s}\left(\sum_{k}x_{k}^{\alpha }W_{kj}^{\text{pre}}\right)\sigma_{s}\left(\sum_{k^{\prime}}x_{k^{\prime}}^{ \beta}W_{k^{\prime}j^{\prime}}^{\text{pre}}\right)\] \[=\frac{c^{2}}{n^{2}\sqrt{n}}\left\|x^{\alpha}\right\|\left\|x^{ \beta}\right\|\sum_{i,j,j^{\prime}=1}^{n}W_{ji}^{\text{post}}W_{j^{\prime}i}^{ \text{post}}\sigma_{s}(g_{j}^{\alpha})\sigma_{s}(g_{j^{\prime}}^{\beta})\]

For the mean, using the independence between \(W^{\text{pre}}\) and \(W^{\text{post}}\), we have that:

\[\mathbb{E}_{\ell}\left[\mathcal{T}_{2}^{\alpha\beta}\right] =\frac{c^{2}}{n^{2}\sqrt{n}}\left\|x^{\alpha}\right\|\left\|x^{ \beta}\right\|\sum_{i,j,j^{\prime}=1}^{n}\mathbb{E}\left[W_{ji}^{\text{post}}W _{j^{\prime}i}^{\text{post}}\right]\mathbb{E}\left[\sigma_{s}(g_{j}^{\alpha} )\sigma_{s}(g_{j^{\prime}}^{\beta})\right]\] \[=\frac{c^{2}}{n\sqrt{n}}\left\|x^{\alpha}\right\|\left\|x^{\beta} \right\|\sum_{j=1}^{n}\mathbb{E}\left[\sigma_{s}(g_{j}^{\alpha})\sigma_{s}(g_ {j}^{\beta})\right]\] \[=\frac{c^{2}}{\sqrt{n}}\left\|x^{\alpha}\right\|\left\|x^{\beta} \right\|K_{1}(\rho^{\alpha\beta})\] \[=c\sqrt{n}V^{\alpha\alpha}V^{\beta\beta}K_{1}(\rho^{\alpha\beta}),\]

which is the desired result. The final expression is the result of the aforementioned expansion for \(K_{1}\)[18]. For the covariance, we have that:

\[\mathbb{E}_{\ell}[\mathcal{T}_{2}^{\alpha\beta}\mathcal{T}_{2}^{ \delta\omega}]\] \[=\frac{c^{2}}{n^{3}}\sqrt{V^{\alpha\alpha}V^{\beta\beta}V^{ \delta\delta}V^{\omega\omega}}\sum_{i,j,j^{\prime},i^{\prime},k,k^{\prime}} \left(\delta_{jj^{\prime}}\delta_{kk^{\prime}}+\delta_{jk}\delta_{ii^{\prime}} \delta_{j^{\prime}k^{\prime}}+\delta_{jk^{\prime}}\delta_{ii^{\prime}}\delta_{j^{ \prime}k}\right)\mathbb{E}\left[\sigma_{s}(g_{j}^{\alpha})\sigma_{s}(g_{j^{ \prime}}^{\beta})\sigma_{s}(g_{k}^{\delta})\sigma_{s}(g_{k^{\prime}}^{\omega}) \right]\,.\]Let's look at each term of the sum separately. For the first term we have that:

\[\sum_{i,j,j^{\prime},i^{\prime},k,k^{\prime}}\delta_{jj^{\prime}} \delta_{kk^{\prime}}\mathbb{E}\left[\sigma_{s}(g_{j}^{\alpha})\sigma_{s}(g_{j^{ \prime}}^{\beta})\sigma_{s}(g_{k}^{\delta})\sigma_{s}(g_{k^{\prime}}^{\omega})\right]\] \[=n^{2}\sum_{j,k}\mathbb{E}\left[\sigma_{s}(g_{j}^{\alpha})\sigma_ {s}(g_{j}^{\beta})\sigma_{s}(g_{k}^{\delta})\sigma_{s}(g_{k}^{\omega})\right]\] \[=n^{2}\sum_{j}\mathbb{E}\left[\sigma_{s}(g_{j}^{\alpha})\sigma_{s }(g_{j}^{\beta})\sigma_{s}(g_{j}^{\delta})\sigma_{s}(g_{j}^{\omega})\right]+n^ {2}\sum_{j\neq k}\mathbb{E}\left[\sigma_{s}(g_{j}^{\alpha})\sigma_{s}(g_{j}^{ \beta})\right]\mathbb{E}\left[\sigma_{s}(g_{k}^{\delta})\sigma_{s}(g_{k}^{ \omega})\right]\] \[=n^{3}K_{2}(\rho^{\alpha\beta\delta\omega})+n^{3}(n-1)K_{1}(\rho ^{\alpha\beta})K_{1}(\rho^{\delta\omega}),\]

where we have defined the fourth moment of the shaped activation: \(K_{2}(\rho^{\alpha\beta\delta\omega}):=\mathbb{E}\left[\sigma_{s}(g^{\alpha}) \sigma_{s}(g^{\beta})\sigma_{s}(g^{\delta})\sigma_{s}(g^{\omega})\right]\) for which it holds that [18, Lemma C.2]:

\[K_{2}(\rho^{\alpha\beta\delta\omega})=\mathbb{E}[g^{\alpha}g^{\beta}g^{\delta }g^{\omega}]+\mathcal{O}(n^{-1/2})=\rho^{\alpha\beta}\rho^{\delta\omega}+\rho^ {\alpha\delta}\rho^{\beta\omega}+\rho^{\alpha\omega}\rho^{\beta\delta}+ \mathcal{O}(n^{-1/2}).\]

The other two summands can be solved similarly:

\[\sum_{i,j,j^{\prime},i^{\prime},k,k^{\prime}}\delta_{jk}\delta_{ ii^{\prime}}\delta_{j^{\prime}k^{\prime}}\mathbb{E}\left[\sigma_{s}(g_{j}^{ \alpha})\sigma_{s}(g_{j^{\prime}}^{\beta})\sigma_{s}(g_{k}^{\delta})\sigma_{s }(g_{k^{\prime}}^{\omega})\right]=n^{2}K_{2}(\rho^{\alpha\beta\delta\omega})+n ^{2}(n-1)K_{1}(\rho^{\alpha\delta})K_{1}(\rho^{\beta\omega})\,,\]

and

\[\sum_{i,j,j^{\prime},i^{\prime},k,k^{\prime}}\delta_{jk^{\prime}} \delta_{ii^{\prime}}\delta_{j^{\prime}k}\mathbb{E}\left[\sigma_{s}(g_{j}^{ \alpha})\sigma_{s}(g_{j^{\prime}}^{\beta})\sigma_{s}(g_{k}^{\delta})\sigma_{s }(g_{k^{\prime}}^{\omega})\right]=n^{2}K_{2}(\rho^{\alpha\beta\delta\omega})+n ^{2}(n-1)K_{1}(\rho^{\alpha\omega})K_{1}(\rho^{\beta\delta})\,.\]

Hence, summing the three terms:

\[\mathbb{E}_{\ell}[\mathcal{T}_{2}^{\alpha\beta}\mathcal{T}_{2}^{ \delta\omega}] =c^{2}\sqrt{V^{\alpha\alpha}V^{\beta\beta}V^{\delta\delta}V^{ \omega\omega}}\Big{(}K_{2}(\rho^{\alpha\beta\delta\omega})+(n-1)K_{1}(\rho^{ \alpha\beta})K_{1}(\rho^{\delta\omega})\] \[+\frac{1}{n}\left(K_{2}(\rho^{\alpha\beta\delta\omega})+(n-1)K_{ 1}(\rho^{\alpha\delta})K_{1}(\rho^{\beta\omega})\right)+\frac{1}{n}\left(K_{2} (\rho^{\alpha\beta\delta\omega})+(n-1)K_{1}(\rho^{\alpha\omega})K_{1}(\rho^{ \beta\delta})\right)\Big{)}\] \[=c^{2}\sqrt{V^{\alpha\alpha}V^{\beta\beta}V^{\delta\delta}V^{ \omega\omega}}\Big{(}K_{2}(\rho^{\alpha\beta\delta\omega})+nK_{1}(\rho^{ \alpha\beta})K_{1}(\rho^{\delta\omega})-K_{1}(\rho^{\alpha\beta})K_{1}(\rho^{ \delta\omega})\] \[+K_{1}(\rho^{\alpha\delta})K_{1}(\rho^{\beta\omega})+K_{1}(\rho^{ \alpha\omega})K_{1}(\rho^{\beta\delta})\Big{)}+\mathcal{O}(n^{-1}).\]

Now, subtracting \(\mathbb{E}_{\ell}[\mathcal{T}_{2}^{\alpha\beta}]\mathbb{E}_{\ell}[\mathcal{T}_ {2}^{\delta\omega}]\), using the aforementioned expansions for \(K_{1}\) and \(K_{2}\):

\[\mathbb{E}_{\ell}[\mathcal{T}_{2}^{\alpha\beta}\mathcal{T}_{2}^{ \delta\omega}]-\mathbb{E}_{\ell}[\mathcal{T}_{2}^{\alpha\beta}]\mathbb{E}_{ \ell}[\mathcal{T}_{2}^{\delta\omega}] =c^{2}\sqrt{V^{\alpha\alpha}V^{\beta\beta}V^{\delta\delta}V^{ \omega\omega}}\left(\rho^{\alpha\beta}\rho^{\delta\omega}+\rho^{\alpha\delta} \rho^{\beta\omega}+\rho^{\alpha\omega}\rho^{\beta\delta}\right)\] \[-V^{\alpha\beta}V^{\delta\omega}+V^{\alpha\delta}V^{\beta \omega}+V^{\alpha\omega}V^{\beta\delta}+\mathcal{O}(n^{-1})\] \[=2V^{\alpha\delta}V^{\beta\omega}+2V^{\alpha\omega}V^{\beta\delta}+ \mathcal{O}(n^{-1})\;,\]

where in the final step we have used the fact that \(c=1+\mathcal{O}(n^{-1/2})\). This completes the proof. 

Finally we also have that \(\mathcal{T}_{1}\) and \(\mathcal{T}_{2}\) are uncorrelated, i.e.

**Lemma B.3**.: \[\mathbb{E}_{\ell}[\mathcal{T}_{1}^{\alpha\beta}\mathcal{T}_{2}^{\delta\omega}]=0\,.\]

Proof.: It is easy to see that \(\mathbb{E}_{\ell}\mathcal{T}_{1}^{\alpha\beta}\mathcal{T}_{2}^{\delta\omega}\) involve odd standard Gaussian moments (in particular third order moments), which vanish due to the parity of the centered Gaussian measure. 

**Theorem 3.2**.: _Let \(X_{\ell}\) be the hidden layers of a ResNet defined in Eq. 4 with \(\lambda^{2}+\gamma^{2}=1\), where both \(\lambda\) and \(\gamma\) do not depend on \(d,n\). Then the feature covariance \(V_{\ell}\) converges to the solution of the following SDE (in the sense of Definition 3.1)_

\[dV_{t}=b_{\text{res}}(V_{t})\,dt+\Sigma_{\text{res}}(V_{t})^{1/2}\,dB_{t}\,,\quad V _{0}=\frac{1}{n}X_{0}X_{0}^{\top}\,,\] (5)

_where \(b_{\text{res}}(V)=\gamma^{2}b_{\text{ReLU}}(V)=\gamma^{2}[\nu(\rho^{\alpha\beta}) \sqrt{V^{\alpha\alpha}V^{\beta\beta}}]_{\alpha\leq\beta}\) with \(\rho^{\alpha\beta}=V^{\alpha\beta}(V^{\alpha\alpha}V^{\beta\beta})^{-1/2}\) and_

\[\nu(\rho)=\frac{(c_{+}-c_{-})^{2}}{2\pi}\left(\sqrt{1-\rho^{2}}-\rho\arccos\rho \right)\,,\] (6)

_furthermore, \(\Sigma_{\text{res}}(V)=2\gamma^{2}\Sigma_{\text{lin}}(V)=2\gamma^{2}[V^{\alpha \delta}V^{\beta\omega}+V^{\alpha\omega}V^{\beta\delta}]_{\alpha\leq\beta, \delta\leq\omega}\)._Proof.: We know that:

\[V_{\ell+1}^{\alpha\beta}=\lambda^{2}V_{\ell}^{\alpha\beta}+\frac{\gamma\lambda}{ \sqrt{n}}\mathcal{T}_{1}^{\alpha\beta}+\frac{\gamma^{2}}{\sqrt{n}}\mathcal{T}_{2 }^{\alpha\beta}.\]

Using \(\lambda^{2}+\gamma^{2}=1\), and summing and subtracting the mean of \(\mathcal{T}_{2}\), we have that:

\[V_{\ell+1}^{\alpha\beta}=V_{\ell}^{\alpha\beta}-\gamma^{2}V_{\ell}^{\alpha \beta}+\frac{\gamma^{2}}{\sqrt{n}}\mathbb{E}_{\ell}\left[\mathcal{T}_{2}^{ \alpha\beta}\right]+\frac{\gamma\lambda}{\sqrt{n}}\mathcal{T}_{1}^{\alpha\beta }+\frac{\gamma^{2}}{\sqrt{n}}\left(\mathcal{T}_{2}^{\alpha\beta}-\mathbb{E}_{ \ell}\left[\mathcal{T}_{2}^{\alpha\beta}\right]\right).\]

Using Lemma B.2 for the mean of \(\mathcal{T}_{2}\), we have that:

\[V_{\ell+1}^{\alpha\beta}=V_{\ell}^{\alpha\beta}+\frac{1}{n}\sqrt{V^{\alpha \alpha}V^{\beta\beta}}\nu(\rho)+\frac{\gamma\lambda}{\sqrt{n}}\mathcal{T}_{1} ^{\alpha\beta}+\frac{\gamma^{2}}{\sqrt{n}}\left(\mathcal{T}_{2}^{\alpha\beta }-\mathbb{E}_{\ell}\left[\mathcal{T}_{2}^{\alpha\beta}\right]\right)+\mathcal{ O}(n^{-3/2}),\]

which gives us an expression in the right Markov Chain form with drift term \(\sqrt{V^{\alpha\alpha}V^{\beta\beta}}\nu(\rho)\). For the diffusion term, we need to compute the covariance between two different neural covariances \(V_{\ell+1}^{\alpha\beta},V_{\ell+1}^{\delta\omega}\). Using Lemmas B.1 to B.3, we have

\[\text{Cov}_{\ell}\left(V_{\ell+1}^{\alpha\beta},V_{\ell+1}^{ \delta\omega}\right) =\mathbb{E}_{\ell}\left[\left(\lambda\gamma\mathcal{T}_{1}^{ \alpha\beta}+\gamma^{2}\mathcal{T}_{2}^{\alpha\beta}-\gamma^{2}\mathbb{E}_{ \ell}[\mathcal{T}_{2}^{\alpha\beta}]\right)\left(\lambda\gamma\mathcal{T}_{1}^ {\delta\omega}+\gamma^{2}\mathcal{T}_{2}^{\delta\omega}-\gamma^{2}\mathbb{E}_{ \ell}[\mathcal{T}_{2}^{\delta\omega}]\right)\right]\] \[=\lambda^{2}\gamma^{2}\mathbb{E}_{\ell}\left[\mathcal{T}_{1}^{ \alpha\beta}\mathcal{T}_{1}^{\delta\omega}\right]+\gamma^{4}\mathbb{E}_{\ell} \left[\mathcal{T}_{2}^{\alpha\beta}\mathcal{T}_{2}^{\delta\omega}\right]- \gamma^{4}\mathbb{E}_{\ell}\left[\mathcal{T}_{2}^{\alpha\beta}\right]\mathbb{ E}_{\ell}\left[\mathcal{T}_{2}^{\delta\omega}\right]\] \[=2\lambda^{2}\gamma^{2}(V^{\alpha\delta}V^{\beta\omega}+V^{ \alpha\omega}V^{\beta\delta})+2\gamma^{4}(V^{\alpha\delta}V^{\beta\omega}+V^{ \alpha\omega}V^{\beta\delta})+\mathcal{O}(n^{-1})\] \[=2\gamma^{2}(V^{\alpha\delta}V^{\beta\omega}+V^{\alpha\omega}V^{ \beta\delta})+\mathcal{O}(n^{-1})\,,\]

where we use \(\text{Cov}_{\ell}\) to denote the covariance conditioned on the sigma-algebra \(\mathcal{F}_{\ell}=\sigma(\{X_{k}\}_{k\in[\ell]})\). Finally, applying Proposition A.2, we get the desired result. 

## Appendix C SDE for Softmax-based Attention

### Dot-Product Attention

Dot-product attention applies the Softmax row-wise to the following matrix:

\[Y_{\ell}=\frac{1}{n}X_{\ell}\underbrace{W_{\ell}^{K}W_{\ell}^{Q,\top}}_{W_{ \ell}^{B}}X_{\ell}^{\top}\,,\]

where \(W_{\ell}^{K},W_{\ell}^{Q}\in\mathbb{R}^{n\times n_{k}}\) are Gaussian matrices with unit variance entries, and \(n_{k}\) is the queries and keys' dimension. Here we study the first two moments of \(Y_{\ell}\). In particular, note that \(\mathbb{E}_{\ell}[Y_{\ell}]=0\), where \(\mathbb{E}_{\ell}[\,\cdot\,]=\mathbb{E}[\,\cdot\,|\,\mathcal{F}_{\ell}]\) denotes the conditional expectation given the sigma-algebra generated by \(\mathcal{F}_{\ell}=\sigma(\{X_{k}\}_{k\in[\ell]})\).

For the second moment we have the following Lemma.

**Lemma C.1**.: _Let_

\[Y_{\ell}=\frac{1}{n}X_{\ell}W_{\ell}^{K}W_{\ell}^{Q,\top}X_{\ell}^{\top}\,,\]

_be the dot-product attention parametrized by \(W_{\ell}^{K},W_{\ell}^{Q}\in\mathbb{R}^{n\times n_{k}}\). Then:_

\[\mathbb{E}_{\ell}[Y^{\alpha\beta}Y^{\delta\omega}]=n_{k}V^{\alpha\delta}V^{ \beta\omega}\,.\] (24)

Proof.: \[\mathbb{E}_{\ell}[Y^{\alpha\beta}Y^{\delta\omega}]=\frac{1}{n^{2}}\sum_{kk^{ \prime}jj^{\prime}}X_{k}^{\alpha}X_{k^{\prime}}^{\beta}X_{j}^{\delta}X_{j^{ \prime}}^{\omega}\mathbb{E}[W_{kk^{\prime}}^{B}W_{jj^{\prime}}^{B}].\]For the expectation, we have that:

\[\mathbb{E}[W^{B}_{kk^{\prime}}W^{B}_{jj^{\prime}}] =\sum_{ii^{\prime}}^{n_{k}}\mathbb{E}\left[W^{K}_{ki}W^{K}_{jj^{ \prime}}W^{Q}_{k^{\prime}i}W^{Q}_{j^{\prime}i^{\prime}}\right]\] \[=\sum_{ii^{\prime}}\mathbb{E}\left[W^{K}_{ki}W^{K}_{ji^{\prime}} \right]\mathbb{E}\left[W^{Q}_{k^{\prime}i}W^{Q}_{j^{\prime}i^{\prime}}\right]\] \[=\sum_{ii^{\prime}}\delta_{kj}\delta_{k^{\prime}j^{\prime}}\delta _{ii^{\prime}}\] \[=n_{k}\delta_{kj}\delta_{k^{\prime}j^{\prime}}\,,\]

where we recall \(W^{B}=W^{K}W^{Q,\top}\).

Hence:

\[\mathbb{E}_{\ell}[Y^{\alpha\beta}Y^{\delta\omega}]=\frac{n_{k}}{n^{2}}\sum_{ kk^{\prime}}X^{\alpha}_{k}X^{\beta}_{k^{\prime}}X^{\delta}_{k}X^{\omega}_{k^{ \prime}}=n_{k}V^{\alpha\delta}V^{\beta\omega}\,.\]

### Shaping the Softmax

Recall that we have the following model:

\[X_{\ell+1}=\lambda X_{\ell}+\gamma A_{\ell}X_{\ell}\ \frac{1}{\sqrt{n}}W^{V}_{\ell}\]

For the above model, \(V_{\ell}\) has the following form:

\[V_{\ell+1}=\lambda^{2}V_{\ell}+\frac{\lambda\gamma}{n\sqrt{n}}\left(X_{\ell}W ^{\top}_{\ell}X^{\top}_{\ell}A^{\top}_{\ell}+A_{\ell}X_{\ell}W_{\ell}X^{\top}_ {\ell}\right)+\frac{\gamma^{2}}{n^{2}}A_{\ell}X_{\ell}W_{\ell}W^{\top}_{\ell}X ^{\top}_{\ell}A^{\top}_{\ell}.\]

In order to have infinitesimal updates in \(V_{\ell}\), we would intuitively like the attention matrix to be of the form \(A^{\alpha\beta}=\delta_{\alpha\beta}+\mathcal{O}(n^{-1})\). In the case of the usual Softmax based attention, we have:

\[A_{\ell}:=\text{Softmax}(\tau^{-1}Y_{\ell}),\]

where \(\tau^{-1}\) is a temperature parameter that regulates the entropy of the resulting categorical distribution; \(\tau\) is often chosen to be large (scale as a power of \(n\) or \(d\)), as low temperature results in unstable training.

To transform the Softmax into the desired form of \(A^{\alpha\beta}=\delta_{\alpha\beta}+\mathcal{O}(n^{-1})\), we first center the attention matrix around the identity matrix:

\[A_{\ell}=I+\text{Softmax}(\tau^{-1}Y_{\ell}),\]

and then examine the Taylor-expansion of the Softmax w.r.t \(\tau^{-1}\):

\[A_{\ell}=I+\frac{1}{m}11^{\top}+\mathcal{O}(\tau^{-1})\,.\] (25)

The first few terms of the expansion provides a good approximation of \(A_{\ell}\) when \(\tau\) is large. Observe that for fixed \(m\), the zero order term \(\frac{1}{m}11^{\top}\) is not of \(\mathcal{O}(n^{-1})\). In particular, suppose that we use the attention model in Eq. 25. We can show that the expectation of the term in \(\gamma^{2}\) (responsible for the drift) has the following form:

\[\gamma^{2}\sum_{\delta,\omega=1}^{m}\mathbb{E}_{\ell}[A^{\alpha\delta}A^{ \beta\omega}]V^{\delta\omega}=\gamma^{2}V^{\alpha\beta}+\frac{\gamma^{2}}{m} \sum_{\omega}V^{\alpha\omega}+\frac{\gamma^{2}}{m}\sum_{\delta}V^{\delta\beta}+ \frac{\gamma^{2}}{m^{2}}\sum_{\delta\omega}V^{\delta\omega}+\mathcal{O}(\tau^ {-1}),\]

where the terms scaled by \(\frac{1}{m}\) leads to large incremental updates in expected value of \(V_{\ell}\) w.r.t the previous layer, and precludes the possibility of convergence to an SDE. Hence, we choose to re-center the Softmax by removing the term \(\frac{1}{m}11^{\top}\), as follows:

\[A_{\ell}=I+\text{Softmax}(\tau^{-1}Y_{\ell})-\frac{1}{m}11^{\top},\]which admits the following Taylor-expansion:

\[[\text{Softmax}(\tau^{-1}Y_{\ell})-m^{-1}11^{\top}]=\frac{1}{\tau m}[Y^{\alpha\beta }-\overline{Y^{\alpha}}]_{\alpha,\beta}+\frac{1}{2\tau^{2}m}\left[(Y^{\alpha \beta}-\overline{Y}^{\alpha})^{2}-(\overline{(Y^{\alpha})^{2}}-\overline{Y^{ \alpha}}^{2})\right]_{\alpha,\beta}+O(\tau^{-3}),\]

where \(\overline{Y^{\alpha}}:=\frac{1}{m}\sum_{\nu}Y^{\alpha\nu}\), and \(\overline{(Y^{\alpha})^{2}}:=\frac{1}{m}\sum_{\nu}(Y^{\alpha\nu})^{2}\).

Hence, up to third order:

\[A_{\ell}=I+\frac{1}{\tau m}[Y^{\alpha\beta}-\overline{Y^{\alpha}}]_{\alpha, \beta}+\frac{1}{2\tau^{2}m}\left[(Y^{\alpha\beta}-\overline{Y}^{\alpha})^{2}- (\overline{(Y^{\alpha})^{2}}-\overline{Y^{\alpha}}^{2})\right]_{\alpha\beta}+ O(\tau^{-3})\,.\]

For sufficiently large \(\tau\), the formulation above allows infinitesimal updates in \(V_{\ell}\), and as we will show rigorously in the rest of this section, permits convergence to an SDE.

### Lemmas on Moments of Shaped Attention

Define:

\[F_{1}^{\alpha\beta} =Y^{\alpha\beta}-\overline{Y^{\alpha}}\,,\] \[F_{2}^{\alpha\beta} =(Y^{\alpha\beta}-\overline{Y}^{\alpha})^{2}-(\overline{(Y^{ \alpha})^{2}}-\overline{Y^{\alpha}}^{2})\,.\]

Hence, \(A_{\ell}\) can be written as:

\[A_{\ell}^{\alpha\beta}=\delta_{\alpha\beta}+\frac{1}{\tau m}F_{1}^{\alpha\beta }+\frac{1}{2\tau^{2}m}F_{2}^{\alpha\beta}+O(\tau^{-3}).\]

We now compute the moments of \(A_{\ell}\). We define the following quantities:

\[S_{1}^{\alpha\delta,\beta\omega}:=\frac{1}{n_{k}}\mathbb{E}_{ \ell}(Y^{\alpha\delta}-\overline{Y^{\alpha}})(Y^{\beta\omega}-\overline{Y^{ \beta}})=\frac{1}{n_{k}}\mathbb{E}_{\ell}F_{1}^{\alpha\delta}F_{1}^{\beta \omega}\,,\] \[S_{2}^{\alpha\delta}:=\frac{1}{n_{k}}\mathbb{E}_{\ell}\left[(Y^{ \alpha\delta}-\overline{Y}^{\alpha})^{2}-(\overline{(\overline{Y^{\alpha}})^{ 2}}-\overline{Y^{\alpha}}^{2})\right]=\frac{1}{n_{k}}\mathbb{E}_{\ell}F_{2}^{ \alpha\delta}\,,\]

where we recall \(\mathbb{E}_{\ell}[\,\cdot\,]=\mathbb{E}[\,\cdot\,|\mathcal{F}_{\ell}]\) is the conditional expectation given the sigma-algebra generated by \(\mathcal{F}_{\ell}=\sigma(\{X_{k}\}_{k\in[\ell]})\).

**Lemma C.2** (Moments of Taylor Expansion).: \[S_{1}^{\alpha\delta,\beta\omega}=V^{\alpha\beta}\left(V^{\delta \omega}-V^{\delta\bar{x}}-V^{\omega\bar{x}}+V^{\bar{x}\bar{x}}\right)\,,\] \[S_{2}^{\alpha\delta}=V^{\alpha\alpha}\left(V^{\delta\delta}-2V^{ \delta\bar{x}}+2V^{\bar{x}\bar{x}}-\bar{V}\right),\]

_where \(\bar{V}=\frac{1}{m}\sum_{\nu}V^{\nu\nu}\) and \(\bar{x}=\frac{1}{m}\sum_{\nu}x^{\nu}\) is the average token._

Proof.: Using Lemma C.1 and linearity of expectation:

\[S_{1}^{\alpha\delta,\beta\omega} =\frac{1}{n_{k}}\left(\mathbb{E}_{\ell}[Y^{\alpha\delta}Y^{\beta \omega}]-\mathbb{E}_{\ell}[Y^{\alpha\delta}\overline{Y^{\beta}}]-\mathbb{E}_{ \ell}[Y^{\beta\omega}\overline{Y^{\alpha}}]+\mathbb{E}_{\ell}[\overline{Y^{ \alpha}}\overline{Y^{\beta}}]\right)\] \[=\left(V^{\alpha\beta}V^{\delta\omega}-V^{\alpha\beta}V^{\delta \bar{x}}-V^{\alpha\beta}V^{\omega\bar{x}}+V^{\alpha\beta}V^{\bar{x}\bar{x}}\right)\] \[=V^{\alpha\beta}\left(V^{\delta\omega}-V^{\delta\bar{x}}-V^{ \omega\bar{x}}+V^{\bar{x}\bar{x}}\right)\,,\]

and:

\[S_{2}^{\alpha\delta} =\frac{1}{n_{k}}\left(\mathbb{E}_{\ell}\left[(Y^{\alpha\delta}- \overline{Y}^{\alpha})^{2}\right]-\mathbb{E}_{\ell}\left[(\overline{(Y^{\alpha })^{2}}-\overline{Y^{\alpha}}^{2})\right]\right)\] \[=\frac{1}{n_{k}}\left(\mathbb{E}_{\ell}[(Y^{\alpha\delta})^{2}] -2\mathbb{E}_{\ell}[Y^{\alpha\delta}\overline{Y^{\alpha}}]+2\mathbb{E}_{\ell}[ \overline{Y^{\alpha}}^{2}]-\mathbb{E}_{\ell}[\overline{(Y^{\alpha})^{2}}]\right)\] \[=\left(V^{\alpha\alpha}V^{\delta\delta}-2V^{\alpha\alpha}V^{ \delta\bar{x}}+2V^{\alpha\alpha}V^{\bar{x}\bar{x}}-V^{\alpha\alpha}\bar{V}\right)\] \[=V^{\alpha\alpha}\left(V^{\delta\delta}-2V^{\delta\bar{x}}+2V^{ \bar{x}\bar{x}}-\bar{V}\right),\]

where \(\bar{V}=\frac{1}{m}\sum_{\beta}V^{\beta\beta}\).

**Lemma C.3**.: \[\mathbb{E}_{\ell}[A^{\alpha\delta}A^{\beta\omega}]=\delta_{\alpha\delta}\delta_{ \beta\omega}+\frac{n_{k}}{\tau^{2}m^{2}}S_{1}^{\alpha\delta,\beta\omega}+\frac{ n_{k}}{2\tau^{2}m}(\delta_{\beta\omega}S_{2}^{\alpha\delta}+\delta_{\alpha \delta}S_{2}^{\beta\omega})+O(n_{k}\tau^{-3})\,.\]

Proof.: \[\mathbb{E}_{\ell}[A^{\alpha\delta}A^{\beta\omega}] =\delta_{\alpha\delta}\delta_{\beta\omega}+\frac{\delta_{\beta \omega}}{\tau m}\mathbb{E}_{\ell}[Y^{\alpha\delta}-\overline{Y^{\alpha}}]+ \frac{\delta_{\alpha\delta}}{\tau m}\mathbb{E}_{\ell}[Y^{\beta\omega}- \overline{Y^{\beta}}]+\frac{1}{\tau^{2}m^{2}}\mathbb{E}_{\ell}(Y^{\alpha\delta }-\overline{Y^{\alpha}})(Y^{\beta\omega}-\overline{Y^{\beta}})\] \[+\frac{\delta_{\beta\omega}}{2\tau^{2}m}\mathbb{E}_{\ell}\left[(Y ^{\alpha\delta}-\overline{Y}^{\alpha})^{2}-(\overline{(Y^{\alpha})^{2}}- \overline{Y^{\alpha}}^{2})\right]\] \[+\frac{\delta_{\alpha\delta}}{2\tau^{2}m}\mathbb{E}_{\ell}\left[ (Y^{\beta\omega}-\overline{Y}^{\beta})^{2}-(\overline{(Y^{\beta})^{2}}- \overline{Y^{\beta}}^{2})\right]+O(n_{k}\tau^{-3})\] \[=\delta_{\alpha\delta}\delta_{\beta\omega}+\frac{n_{k}}{\tau^{2}m ^{2}}S_{1}^{\alpha\delta,\beta\omega}+\frac{n_{k}}{2\tau^{2}m}(\delta_{\beta \omega}S_{2}^{\alpha\delta}+\delta_{\alpha\delta}S_{2}^{\beta\omega})+O(n_{k} \tau^{-3})\,.\]

**Lemma C.4**.: \[\mathbb{E}_{\ell}\left[A^{\alpha\alpha^{\prime}}A^{\beta\beta^{ \prime}}A^{\delta\delta^{\prime}}A^{\omega\omega^{\prime}}\right] =\delta_{\alpha\alpha^{\prime}}\delta_{\beta\beta^{\prime}}\delta_ {\delta\delta^{\prime}}\delta_{\omega\omega^{\prime}}\] \[+\frac{n_{k}}{\tau^{2}m^{2}}\Big{(}\delta_{\alpha\alpha^{\prime}} \delta_{\beta\beta^{\prime}}S_{1}^{\delta\delta^{\prime},\omega\omega^{\prime} }+\delta_{\alpha\alpha^{\prime}}\delta_{\delta\delta^{\prime}}S_{1}^{\beta \beta^{\prime},\omega\omega^{\prime}}+\delta_{\alpha\alpha^{\prime}}\delta_{ \delta\omega^{\prime}}S_{1}^{\beta\beta^{\prime},\delta\delta^{\prime}}\] \[+\delta_{\beta\beta^{\prime}}\delta_{\delta\delta^{\prime}}S_{2}^ {\alpha\alpha^{\prime},\omega\omega^{\prime}}+\delta_{\beta\beta^{\prime}} \delta_{\omega\omega^{\prime}}S_{1}^{\alpha\alpha^{\prime},\delta\delta^{ \prime}}+\delta_{\omega\omega^{\prime}}\delta_{\delta\delta^{\prime}}S_{1}^{ \alpha\alpha^{\prime},\beta\beta^{\prime}}\Big{)}\] \[+\frac{n_{k}}{2\tau^{2}m}\Big{(}\delta_{\alpha\alpha^{\prime}} \delta_{\beta\beta^{\prime}}\delta_{\delta\delta^{\prime}}S_{2}^{\alpha\omega ^{\prime}}+\delta_{\alpha\alpha^{\prime}}\delta_{\beta\beta^{\prime}}\delta_ {\omega\omega^{\prime}}S_{2}^{\delta\delta^{\prime}}\] \[+\delta_{\alpha\alpha^{\prime}}\delta_{\omega\omega^{\prime}} \delta_{\delta\delta^{\prime}}S_{2}^{\beta\beta^{\prime}}+\delta_{\omega \omega^{\prime}}\delta_{\beta\beta^{\prime}}\delta_{\delta\delta^{\prime}}S_{ 2}^{\alpha\alpha^{\prime}}\Big{)}+O(n_{k}\tau^{-3})\,.\]

Proof.: \[\mathbb{E}_{\ell}\left[A^{\alpha\alpha^{\prime}}A^{\beta\beta^{ \prime}}A^{\delta\delta^{\prime}}A^{\omega\omega^{\prime}}\right] =\mathbb{E}_{\ell}\Bigg{[}\left(\delta_{\alpha\alpha^{\prime}}+ \frac{1}{\tau m}F_{1}^{\alpha\alpha^{\prime}}+\frac{1}{2\tau^{2}m}F_{2}^{\alpha \alpha^{\prime}}\right)\left(\delta_{\beta\beta^{\prime}}+\frac{1}{\tau m}F_{1 }^{\beta\beta^{\prime}}+\frac{1}{2\tau^{2}m}F_{2}^{\beta\beta^{\prime}}\right)\] \[\left(\delta_{\delta\delta^{\prime}}+\frac{1}{\tau m}F_{1}^{ \delta\delta^{\prime}}+\frac{1}{2\tau^{2}m}F_{2}^{\delta\delta^{\prime}} \right)\left(\delta_{\omega\omega^{\prime}}+\frac{1}{\tau m}F_{1}^{\omega \omega^{\prime}}+\frac{1}{2\tau^{2}m}F_{2}^{\omega\omega^{\prime}}\right) \Bigg{]}+O(\tau^{-3})\] \[=\mathbb{E}_{\ell}\Bigg{[}\delta_{\alpha\alpha^{\prime}}\delta_{ \beta\beta^{\prime}}\delta_{\delta\delta^{\prime}}\delta_{\omega\omega^{\prime} }+\frac{1}{\tau^{2}m^{2}}\delta_{\alpha\alpha^{\prime}}\delta_{\beta\beta^{ \prime}}F_{1}^{\delta\delta^{\prime}}F_{1}^{\omega\omega^{\prime}}+\frac{1}{ \tau^{2}m^{2}}\delta_{\alpha\alpha^{\prime}}\delta_{\delta\delta^{\prime}}F_{1 }^{\beta\beta^{\prime}}F_{1}^{\omega\omega^{\prime}}\] \[+\frac{1}{\tau^{2}m^{2}}\delta_{\alpha\alpha^{\prime}}\delta_{ \omega\omega^{\prime}}F_{1}^{\delta\delta^{\prime}}F_{1}^{\beta\beta^{\prime}}+ \frac{1}{\tau^{2}m^{2}}\delta_{\beta\beta^{\prime}}\delta_{\delta\delta^{\prime}}F _{1}^{\alpha\alpha^{\prime}}F_{1}^{\omega\omega^{\prime}}\] \[+\frac{1}{\tau^{2}m^{2}}\delta_{\beta\beta^{\prime}}\delta_{\omega \omega^{\prime}}F_{1}^{\alpha\alpha^{\prime}}F_{1}^{\delta\delta^{\prime}}+ \frac{1}{\tau^{2}m^{2}}\delta_{\delta\delta^{\prime}}\delta_{\omega\omega^{\prime}}F _{1}^{\alpha\alpha^{\prime}}F_{1}^{\beta\beta^{\prime}}\] \[+\frac{1}{2\tau^{2}m}\delta_{\alpha\alpha^{\prime}}\delta_{\beta \beta^{\prime}}\delta_{\delta\delta^{\prime}}F_{2}^{\omega\omega^{\prime}}+ \frac{1}{2\tau^{2}m}\delta_{\alpha\alpha^{\prime}}\delta_{\beta\beta^{\prime}} \delta_{\omega\omega^{\prime}}F_{2}^{\delta\delta^{\prime}}\] \[+\frac{1}{2\tau^{2}m}\delta_{\alpha\alpha^{\prime}}\delta_{\omega \omega^{\prime}}\delta_{\delta\delta^{\prime}}F_{2}^{\beta\beta^{\prime}}+ \frac{1}{2\tau^{2}m}\delta_{\omega\omega^{\prime}}\delta_{\beta\beta^{\prime}} \delta_{\delta\delta^{\prime}}F_{2}^{\alpha\alpha^{\prime}}\bigg{]}+O(\tau^{-3})\,.\]

Using the linearity of expectation:

\[\mathbb{E}_{\ell}\left[A^{\alpha\alpha^{\prime}}A^{\beta\beta^{ \prime}}A^{\delta\delta^{\prime}}A^{\omega\omega^{\prime}}\right] =\delta_{\alpha\alpha^{\prime}}\delta_{\beta\beta^{\prime}} \delta_{\delta\delta^{\prime}}\delta_{\omega\omega\omega^{\prime}}\] \[+\frac{n_{k}}{\tau^{2}m^{2}}\Big{(}\delta_{\alpha\alpha^{\prime}} \delta_{\beta\beta^{\prime}}S_{1}^{\delta\delta^{\prime},\omega\omega^{\prime}}+ \delta_{\alpha\alpha^{\prime}}\delta_{\delta\delta^{\prime}}S_{1}^{\beta\beta^{ \prime},\omega\omega^{\prime}}+\delta_{\alpha\alpha^{\prime}}\delta_{\omega \omega^{\prime}}S_{1}^{\beta\beta^{\prime},\delta\delta^{\prime}}\] \[+\delta_{\beta\beta^{\prime}}\delta_{\delta\delta^{\prime}}S_{1}^{ \alpha\alpha^{\prime},\omega\omega^{\prime}}+\delta_{\beta\beta^{\prime}} \delta_{\omega\omega^{\prime}}S_{1}^{\alpha\alpha^{\prime},\delta\delta^{\prime}}+ \delta_{\omega\omega^{\prime}}\delta_{\delta\delta^{\prime}}S_{1}^{\alpha\alpha^{ \prime},\beta\beta^{\prime}}\Big{)}\] \[+\frac{n_{k}}{2\tau^{2}m}\Big{(}\delta_{\alpha\alpha^{\prime}}\delta_{ \beta\beta^Note that the terms above can be computed using Lemma C.2.

**Lemma C.5**.: \[\mathbb{E}_{\ell}\left[A^{\alpha\alpha^{\prime}}A^{\beta\beta^{ \prime}}A^{\delta\delta^{\prime}}A^{\omega\omega^{\prime}}\right]-\mathbb{E}_{ \ell}\left[A^{\alpha\alpha^{\prime}}A^{\beta\beta^{\prime}}\right]\mathbb{E}_{ \ell}\left[A^{\delta\delta^{\prime}}A^{\omega\omega^{\prime}}\right]\] \[=\frac{n_{k}}{\tau^{2}m^{2}}\Big{(}\delta_{\alpha\alpha^{\prime}} \delta_{\delta\delta^{\prime}}S_{1}^{\beta\beta^{\prime},\omega\omega^{\prime}} +\delta_{\alpha\alpha^{\prime}}\delta_{\omega\omega^{\prime}}S_{1}^{\beta\beta^ {\prime},\delta\delta^{\prime}}+\delta_{\beta\beta^{\prime}}\delta_{\delta \delta^{\prime}}S_{1}^{\alpha\alpha^{\prime},\omega\omega^{\prime}}+\delta_{ \beta\beta^{\prime}}\delta_{\omega\omega^{\prime}}S_{1}^{\alpha\alpha^{\prime}, \delta\delta^{\prime}}\Big{)}+\mathcal{O}(n_{k}\tau^{-3})\,.\]

Proof.: The results is an immediate consequence of Lemma C.4 and Lemma C.3, where only the terms that do not cancel out are kept. 

### Neural Covariance SDE for Stable Attention

Recall that we have the following model:

\[X_{\ell+1}=\lambda X_{\ell}+\gamma A_{\ell}X_{\ell}\ \frac{1}{\sqrt{n}}W_{\ell}^{V}\]

For the above model, \(V_{\ell}\) has the following form:

\[V_{\ell+1}=\lambda^{2}V_{\ell}+\frac{\lambda\gamma}{n\sqrt{n}} \left(X_{\ell}W_{\ell}^{\top}X_{\ell}^{\top}A_{\ell}^{\top}+A_{\ell}X_{\ell}W_ {\ell}X_{\ell}^{\top}\right)+\frac{\gamma^{2}}{n^{2}}A_{\ell}X_{\ell}W_{\ell}W_ {\ell}^{\top}X_{\ell}^{\top}A_{\ell}^{\top}\,.\]

We define:

\[\mathcal{T}_{1}^{\alpha\beta} :=\frac{1}{n}\left(X_{\ell}W_{\ell}^{\top}X_{\ell}^{\top}A_{\ell }^{\top}+A_{\ell}X_{\ell}W_{\ell}X_{\ell}^{\top}\right)^{\alpha\beta}\,,\] \[\mathcal{T}_{2}^{\alpha\beta} :=\frac{1}{n\sqrt{n}}\left(A_{\ell}X_{\ell}W_{\ell}W_{\ell}^{ \top}X_{\ell}^{\top}A_{\ell}^{\top}\right)^{\alpha\beta}\,.\]

Hence, the expression for \(V_{\ell}\) simplifies to:

\[V_{\ell+1}^{\alpha\beta}=\lambda^{2}V_{\ell}^{\alpha\beta}+\frac{ \lambda\gamma}{\sqrt{n}}\mathcal{T}_{1}^{\alpha\beta}+\frac{\gamma^{2}}{\sqrt {n}}\mathcal{T}_{2}^{\alpha\beta}.\]

We need to compute the moments for these two quantities:

**Lemma C.6** (Moments of \(\mathcal{T}_{1}\)).: \[\mathbb{E}_{\ell}[\mathcal{T}_{1}^{\alpha\beta}]=0\,,\]

_and_

\[\mathbb{E}_{\ell}\left[\mathcal{T}_{1}^{\alpha\beta}\mathcal{T}_{1}^{\delta \omega}\right]=2(V^{\alpha\delta}V^{\beta\omega}+V^{\alpha\omega}V^{\beta \delta})+\mathcal{O}(n_{k}\tau^{-3})\,.\]

Proof.: \[\mathcal{T}_{1}^{\alpha\beta}\mathcal{T}_{1}^{\delta\omega} =\frac{1}{n^{2}}\left[\left(X_{\ell}W_{\ell}^{\top}X_{\ell}^{ \top}A_{\ell}^{\top}+A_{\ell}X_{\ell}W_{\ell}X_{\ell}^{\top}\right)^{\alpha \beta}\left(X_{\ell}W_{\ell}^{\top}X_{\ell}^{\top}A_{\ell}^{\top}\right)^{ \alpha\beta}\left(A_{\ell}X_{\ell}W_{\ell}X_{\ell}^{\top}\right)^{\delta \omega}\right]\] \[=\frac{1}{n^{2}}\Bigg{[}\left(X_{\ell}W_{\ell}^{\top}X_{\ell}^{ \top}A_{\ell}^{\top}\right)^{\alpha\beta}\left(X_{\ell}W_{\ell}^{\top}X_{ \ell}^{\top}A_{\ell}^{\top}\right)^{\delta\omega}+\left(X_{\ell}W_{\ell}^{ \top}X_{\ell}^{\top}A_{\ell}^{\top}\right)^{\alpha\beta}\left(A_{\ell}X_{ \ell}W_{\ell}X_{\ell}^{\top}\right)^{\delta\omega}\] \[+\left(A_{\ell}X_{\ell}W_{\ell}X_{\ell}^{\top}\right)^{\alpha \beta}\left(X_{\ell}W_{\ell}^{\top}X_{\ell}^{\top}A_{\ell}^{\top}\right)^{ \delta\omega}+\left(A_{\ell}X_{\ell}W_{\ell}X_{\ell}^{\top}\right)^{\alpha \beta}\left(A_{\ell}X_{\ell}W_{\ell}X_{\ell}^{\top}\right)^{\delta\omega}\Bigg{]}.\]

Let's look at the first summand:

\[\frac{1}{n^{2}}\left(X_{\ell}W_{\ell}^{\top}X_{\ell}^{\top}A_{\ell}^{\top} \right)^{\alpha\beta}\left(X_{\ell}W_{\ell}^{\top}X_{\ell}^{\top}A_{\ell}^{ \top}\right)^{\delta\omega}=\frac{1}{n^{2}}\sum_{\nu\kappa}\sum_{kk^{\prime}jj ^{\prime}}X_{k}^{\alpha}W_{k^{\prime}k}X_{k^{\prime}}^{\nu}A^{\beta\nu}X_{j}^{ \delta}W_{j^{\prime}j}X_{j^{\prime}}^{\kappa}A^{\omega\kappa}\,.\]Hence, in expectation with respect to \(W\):

\[\frac{1}{n^{2}}\sum_{\nu\kappa}\sum_{kk^{\prime}jj^{\prime}}X_{k}^{ \alpha}\mathbb{E}\left[W_{k^{\prime}k}W_{j^{\prime}j}\right]X_{k^{\prime}}^{ \nu}A^{\beta\nu}X_{j}^{\delta}X_{j^{\prime}}^{\kappa}A^{\alpha\kappa} =\frac{1}{n^{2}}\sum_{\nu\kappa}\sum_{kk^{\prime}jj^{\prime}}X_{k }^{\alpha}\delta_{kj}\delta_{k^{\prime}j^{\prime}}X_{k^{\prime}}^{\nu}A^{ \beta\nu}X_{j}^{\delta}X_{j^{\prime}}^{\kappa}A^{\omega\kappa}\] \[=\frac{1}{n^{2}}\sum_{\nu\kappa}\sum_{kk^{\prime}}X_{k}^{\alpha}X_ {k^{\prime}}^{\nu}A^{\beta\nu}X_{k}^{\delta}X_{k^{\prime}}^{\kappa}A^{\omega\kappa}\] \[=\sum_{\nu\kappa}V^{\alpha\delta}V^{\nu\kappa}A^{\beta\nu}A^{ \omega\kappa}.\]

An identical argument can be made for the remaining three summands. Hence, taking expectation with respect to the Softmax weights:

\[\mathbb{E}_{\ell}\left[\mathcal{T}_{1}^{\alpha\beta}\mathcal{T}_{1}^{\delta \omega}\right] =\sum_{\nu\kappa}\left(V^{\alpha\delta}V^{\nu\kappa}\mathbb{E}_{ \ell}[A^{\beta\nu}A^{\omega\kappa}]+V^{\alpha\omega}V^{\nu\kappa}\mathbb{E}_{ \ell}[A^{\beta\nu}A^{\delta\kappa}]+V^{\beta\delta}V^{\nu\kappa}\mathbb{E}_{ \ell}[A^{\alpha\nu}A^{\omega\kappa}]+V^{\beta\omega}V^{\nu\kappa}\mathbb{E}_{ \ell}[A^{\alpha\nu}A^{\delta\kappa}]\right)\,.\]

Now, using Lemma C.3:

\[\mathbb{E}_{\ell}\left[\mathcal{T}_{1}^{\alpha\beta}\mathcal{T}_{1 }^{\delta\omega}\right] =\sum_{\nu\kappa}V^{\nu\kappa}\left(V^{\alpha\delta}\mathbb{E}_{ \ell}[A^{\beta\nu}A^{\omega\kappa}]+V^{\alpha\omega}\mathbb{E}_{\ell}[A^{ \beta\nu}A^{\delta\kappa}]+V^{\beta\delta}\mathbb{E}_{\ell}[A^{\alpha\nu}A^{ \omega\kappa}]+V^{\beta\omega}\mathbb{E}_{\ell}[A^{\alpha\nu}A^{\delta\kappa}]\right)\] \[=\sum_{\nu\kappa}V^{\nu\kappa}\left(V^{\alpha\delta}\delta_{ \beta\nu}\delta_{\omega\kappa}+V^{\alpha\omega}\delta_{\beta\nu}\delta_{\delta \kappa}+V^{\beta\delta}\delta_{\alpha\nu}\delta_{\omega\kappa}+V^{\beta\omega} \delta_{\alpha\nu}\delta_{\delta\kappa}\right)+\mathcal{O}(n_{k}\tau^{-2})\] \[=V^{\beta\omega}V^{\alpha\delta}+V^{\alpha\omega}V^{\beta\delta}+ V^{\beta\delta}V^{\alpha\omega}+V^{\beta\omega}V^{\alpha\delta}+\mathcal{O}(n_{k} \tau^{-2})\] \[=2(V^{\alpha\delta}V^{\beta\omega}+V^{\alpha\omega}V^{\beta\delta} )+\mathcal{O}(n_{k}\tau^{-2})\,.\]

**Lemma C.7** (Moments of \(\mathcal{T}_{2}\)).: \[\mathbb{E}_{\ell}[\mathcal{T}_{2}^{\alpha\beta}]=\sqrt{n}\sum_{ \nu\kappa}V^{\nu\kappa}\mathbb{E}_{\ell}\left[A^{\alpha\nu}A^{\beta\kappa} \right]\,,\] \[\mathbb{E}_{\ell}[\mathcal{T}_{2}^{\alpha\beta}\mathcal{T}_{2}^{ \delta\omega}]=\sum_{\nu\kappa\nu^{\prime}\kappa^{\prime}}\mathbb{E}_{\ell}[A ^{\alpha\nu}A^{\beta\kappa}A^{\delta\nu^{\prime}}A^{\omega\kappa^{\prime}}] \left(nV^{\nu\kappa}V^{\nu^{\prime}\kappa^{\prime}}+V^{\nu\nu^{\prime}}V^{ \kappa\kappa^{\prime}}+V^{\nu\kappa^{\prime}}V^{\nu^{\prime}\kappa}\right)\,.\]

Proof.: \[\mathcal{T}_{2}^{\alpha\beta}:=\frac{1}{n\sqrt{n}}\left(A_{\ell}X_{\ell}W_{ \ell}W_{\ell}^{\top}X_{\ell}^{\top}A_{\ell}^{\top}\right)^{\alpha\beta}=\frac{1 }{n\sqrt{n}}\sum_{\nu\kappa}\sum_{kk^{\prime}j}A^{\alpha\nu}X_{k}^{\nu}W_{kk^ {\prime}}W_{jk^{\prime}}X_{j}^{\kappa}A^{\beta\kappa}.\]

Taking expectation with respect to \(W\):

\[\frac{1}{n\sqrt{n}}\left(A_{\ell}X_{\ell}W_{\ell}W_{\ell}^{\top}X_ {\ell}^{\top}A_{\ell}^{\top}\right)^{\alpha\beta} =\frac{1}{n\sqrt{n}}\sum_{\nu\kappa}\sum_{kk^{\prime}j}A^{\alpha \nu}X_{k}^{\nu}\mathbb{E}[W_{kk^{\prime}}W_{jk^{\prime}}]X_{j}^{\kappa}A^{ \beta\kappa}\] \[=\frac{1}{n\sqrt{n}}\sum_{\nu\kappa}\sum_{kk^{\prime}j}A^{\alpha \nu}X_{k}^{\nu}\delta_{kj}X_{j}^{\kappa}A^{\beta\kappa}\] \[=\frac{1}{n\sqrt{n}}\sum_{\nu\kappa}\sum_{kk^{\prime}}A^{\alpha \nu}A^{\beta\kappa}X_{k}^{\nu}X_{k}^{\kappa}\] \[=\frac{1}{\sqrt{n}}\sum_{\nu\kappa}\sum_{k}A^{\alpha\nu}A^{\beta \kappa}X_{k}^{\nu}X_{k}^{\kappa}\] \[=\sqrt{n}\sum_{\nu\kappa}V^{\nu\kappa}A^{\alpha\nu}A^{\beta\kappa}.\]

Taking expectation w.r.t the Softmax weights, we get the desired result.

For second moment, we can take the conditional expectation:

\[\mathbb{E}_{\ell}[\mathcal{T}_{2}^{\alpha\beta}\mathcal{T}_{2}^{\delta\omega}]= \frac{1}{n^{3}}\sum_{\nu\kappa\kappa^{\prime}\kappa^{\prime}}\sum_{kk^{ \prime}ji^{\prime}j^{\prime}}A^{\alpha\nu}A^{\beta\kappa}A^{\delta\nu^{\prime}}A ^{\omega\kappa^{\prime}}X_{k}^{\nu}X_{j}^{\kappa}X_{i}^{\nu^{\prime}}X_{j^{ \prime}}^{\kappa^{\prime}}\mathbb{E}[W_{kk^{\prime}}W_{jk^{\prime}}W_{ii^{ \prime}}W_{j^{\prime}i^{\prime}}]\,,\]where we recall \(\mathbb{E}_{\ell}\big{[}\cdot\,\big{]}=\mathbb{E}\big{[}\cdot\,|\mathcal{F}_{\ell} \big{]}\) is the conditional expectation given the sigma-algebra generated by \(\mathcal{F}_{\ell}=\sigma(\{X_{k}\}_{k\in[\ell]})\).

Using Isserlis Theorem, we have that:

\[\mathbb{E}[W_{kk^{\prime}}W_{jk^{\prime}}W_{ii^{\prime}}W_{j^{\prime}i^{\prime} }]=\delta_{kj}\delta_{ij^{\prime}}+\delta_{ki}\delta_{k^{\prime}i^{\prime}} \delta_{jj^{\prime}}+\delta_{kj^{\prime}}\delta_{k^{\prime}i^{\prime}}\delta_ {ji}\,.\]

Hence:

\[\mathbb{E}_{\ell}[\mathcal{T}_{2}^{\alpha\beta}\mathcal{T}_{2}^{ \delta\omega}] =\frac{1}{n^{3}}\sum_{\nu\kappa\nu^{\prime}\kappa^{\prime}}A^{ \alpha\nu}A^{\beta\kappa}A^{\delta\nu^{\prime}}A^{\omega\kappa^{\prime}}\sum_{ kk^{\prime}ji^{\prime}j^{\prime}}X_{k}^{\nu}X_{j}^{\kappa}X_{i}^{\nu^{\prime}}X_{j^{ \prime}}^{\kappa^{\prime}}\Big{(}\delta_{kj}\delta_{ij^{\prime}}+\delta_{ki} \delta_{k^{\prime}i^{\prime}}\delta_{jj^{\prime}}+\] \[+\delta_{kj^{\prime}}\delta_{k^{\prime}i^{\prime}}\delta_{jj} \Big{)}\] \[=\frac{1}{n^{3}}\sum_{\nu\kappa\nu^{\prime}\kappa^{\prime}}A^{ \alpha\nu}A^{\beta\kappa}A^{\delta\nu^{\prime}}A^{\omega\kappa^{\prime}}\Big{(} \sum_{kk^{\prime}ii^{\prime}}X_{k}^{\nu}X_{k}^{\kappa}X_{i}^{\nu^{\prime}}X_{i }^{\kappa^{\prime}}+\sum_{kk^{\prime}j}X_{j}^{\kappa}X_{k}^{\kappa}X_{j}^{ \nu^{\prime}}X_{j}^{\kappa^{\prime}}\] \[+\sum_{kk^{\prime}j}X_{k}^{\nu}X_{j}^{\kappa}X_{j}^{\nu^{\prime}} X_{k}^{\kappa^{\prime}}\Big{)}\] \[=\frac{1}{n^{3}}\sum_{\nu\kappa\nu^{\prime}\kappa^{\prime}}A^{ \alpha\nu}A^{\beta\kappa}A^{\delta\nu^{\prime}}A^{\omega\kappa^{\prime}}\Big{(} n^{4}V^{\nu\kappa}V^{\nu^{\prime}\kappa^{\prime}}+n^{3}V^{\nu\nu^{\prime}}V^{ \kappa\kappa^{\prime}}+n^{3}V^{\nu\kappa^{\prime}}V^{\nu^{\prime}\kappa}\Big{)}\] \[=\sum_{\nu\kappa\nu^{\prime}\kappa^{\prime}}A^{\alpha\nu}A^{ \beta\kappa}A^{\delta\nu^{\prime}}A^{\omega\kappa^{\prime}}\Big{(}nV^{\nu \kappa}V^{\nu^{\prime}\kappa^{\prime}}+V^{\nu\nu^{\prime}}V^{\kappa\kappa^{ \prime}}+V^{\nu\kappa^{\prime}}V^{\nu^{\prime}\kappa}\Big{)}\,.\]

By taking expectation w.r.t the Softmax parameters, we get the desired result. 

**Lemma C.8** (Covariance of \(\mathcal{T}_{2}\)).: \[\mathbb{E}_{\ell}[\mathcal{T}_{2}^{\alpha\beta}\mathcal{T}_{2}^{ \delta\omega}]-\mathbb{E}_{\ell}[\mathcal{T}_{2}^{\alpha\beta}]\mathbb{E}_{ \ell}[\mathcal{T}_{2}^{\delta\omega}]=\frac{nn_{k}}{\tau^{2}}\mathcal{A}^{ \alpha\beta\delta\omega}+V^{\alpha\delta}V^{\beta\omega}+V^{\alpha\omega}V^{ \beta\delta}+\mathcal{O}\left(\frac{nn_{k}}{\tau^{3}}+\frac{n_{k}}{\tau^{2}} \right),\]

_where:_

\[\mathcal{A}^{\alpha\beta\delta\omega}:=\frac{1}{m^{2}}\sum_{\nu\kappa}\Big{(}V ^{\alpha\kappa}V^{\delta\nu}S_{1}^{\beta\kappa,\omega\nu}+V^{\alpha\kappa}V^{ \omega\nu}S_{1}^{\beta\kappa,\delta\nu}+V^{\beta\nu}V^{\delta\kappa}S_{1}^{ \alpha\nu,\omega\kappa}+V^{\beta\nu}V^{\omega\kappa}S_{1}^{\alpha\nu,\delta \kappa}\Big{)}\,\,.\]

Proof.: Using Lemma C.7, we have that:

\[\mathbb{E}_{\ell}[\mathcal{T}_{2}^{\alpha\beta}\mathcal{T}_{2}^{ \delta\omega}]-\mathbb{E}_{\ell}[\mathcal{T}_{2}^{\alpha\beta}]\mathbb{E}_{ \ell}[\mathcal{T}_{2}^{\delta\omega}] =\sum_{\nu\kappa\nu^{\prime}\kappa^{\prime}}\mathbb{E}_{\ell}[A^ {\alpha\nu}A^{\beta\kappa}A^{\delta\nu^{\prime}}A^{\omega\kappa^{\prime}}] \left(nV^{\nu\kappa}V^{\nu^{\prime}\kappa^{\prime}}+V^{\nu\nu^{\prime}}V^{ \kappa\kappa^{\prime}}+V^{\nu\kappa^{\prime}}V^{\nu^{\prime}\kappa}\right)\] \[-n\sum_{\nu\kappa\nu^{\prime}\kappa^{\prime}}V^{\nu\kappa}V^{\nu^ {\prime}\kappa^{\prime}}\mathbb{E}_{\ell}\left[A^{\alpha\nu}A^{\beta\kappa} \right]\mathbb{E}_{\ell}\left[A^{\delta\nu^{\prime}}A^{\omega\kappa^{\prime}}\right]\] \[=n\sum_{\nu\kappa\nu^{\prime}\kappa^{\prime}}V^{\nu\kappa}V^{\nu^ {\prime}\kappa^{\prime}}\left(\mathbb{E}_{\ell}[A^{\alpha\nu}A^{\beta\kappa}A^{ \delta\nu^{\prime}}A^{\omega\kappa^{\prime}}]-\mathbb{E}_{\ell}\left[A^{ \alpha\nu}A^{\beta\kappa}\right]\mathbb{E}_{\ell}\left[A^{\delta\nu^{\prime}}A^{ \omega\kappa^{\prime}}\right]\right)\] \[+\sum_{\nu\kappa\nu^{\prime}\kappa^{\prime}}\mathbb{E}_{\ell}[A^ {\alpha\nu}A^{\beta\kappa}A^{\delta\nu^{\prime}}A^{\omega\kappa^{\prime}}] \left(V^{\nu\nu^{\prime}}V^{\kappa\kappa^{\prime}}+V^{\nu\kappa^{\prime}}V^{ \nu^{\prime}\kappa}\right)\,.\]

Now we can use Lemma C.3 and Lemma C.4 to compute the moments of \(A\). For the second summand, we simply have:

\[\mathbb{E}_{\ell}\left[A^{\alpha\nu}A^{\beta\kappa}A^{\delta\nu^{\prime}}A^{ \omega\kappa^{\prime}}\right]=\delta_{\alpha\nu}\delta_{\beta\kappa}\delta_{ \delta\nu^{\prime}}\delta_{\omega\kappa^{\prime}}+\mathcal{O}(n_{k}\tau^{-2}),\]

hence:

\[\sum_{\nu\kappa\nu^{\prime}\kappa^{\prime}}\mathbb{E}_{\ell}[A^{\alpha\nu}A^{ \beta\kappa}A^{\delta\nu^{\prime}}A^{\omega\kappa^{\prime}}]\left(V^{\nu\nu^{ \prime}}V^{\kappa\kappa^{\prime}}+V^{\nu\kappa^{\prime}}V^{\nu^{\prime}\kappa}\right) =V^{\alpha\delta}V^{\beta\omega}+V^{\alpha\omega}V^{\beta\delta}+\mathcal{O}(n_{k} \tau^{-2})\,.\]

For the first summand, recall from Lemma C.5 that:

\[\mathbb{E}_{\ell}\left[A^{\alpha\alpha^{\prime}}A^{\beta\beta^{ \prime}}A^{\delta\delta^{\prime}}A^{\omega\omega^{\prime}}\right]-\mathbb{E}_{ \ell}\left[A^{\alpha\alpha^{\prime}}A^{\beta\beta^{\prime}}\right]\mathbb{E}_{ \ell}\left[A^{\delta\delta^{\prime}}A^{\omega\omega^{\prime}}\right]\] \[=\frac{n_{k}}{\tau^{2}m^{2}}\Big{(}\delta_{\alpha\alpha^{\prime}} \delta_{\delta\delta\delta^{\prime}}S_{1}^{\beta\beta^{\prime},\omega\omega^{ \prime}}+\delta_{\alpha\alpha^{\prime}}\delta_{\omega\omega^{\prime}}S_{1}^{ \beta\beta^{\prime},\delta\delta^{\prime}}+\delta_{\beta\beta^{\prime}}\delta_{ \delta\delta^{\prime}}S_{1}^{\alpha\alpha^{\prime},\omega\omega^{\prime}}+\delta_{ \beta\beta^{\prime}}\delta_{\omega\omega^{\prime}}S_{1}^{\alpha\alpha^{\prime}, \delta\delta^{\prime}}\Big{)}\] \[+\mathcal{O}(n_{k}\tau^{-3})\,.\]Hence:

\[n \sum_{\nu\kappa\nu^{\prime}\kappa^{\prime}}V^{\nu\kappa}V^{\nu^{ \prime}\kappa^{\prime}}\left(\mathbb{E}_{\ell}[A^{\alpha\nu}A^{\beta\kappa}A^{ \delta\nu^{\prime}}A^{\alpha\kappa^{\prime}}]-\mathbb{E}_{\ell}\left[A^{ \alpha\nu}A^{\beta\kappa}\right]\mathbb{E}_{\ell}\left[A^{\delta\nu^{\prime}}A ^{\omega\kappa^{\prime}}\right]\right)\] \[=\frac{nn_{k}}{\tau^{2}m^{2}}\sum_{\nu\kappa\nu^{\prime}\kappa}V^ {\nu\kappa}V^{\nu^{\prime}\kappa^{\prime}}\Big{(}\delta_{\alpha\nu}\delta_{ \delta\nu^{\prime}}S_{1}^{\beta\kappa,\omega\kappa^{\prime}}+\delta_{\alpha\nu }\delta_{\omega\kappa^{\prime}}S_{1}^{\beta\kappa,\delta\nu^{\prime}}+\delta_ {\beta\kappa}\delta_{\delta\nu^{\prime}}S_{1}^{\alpha\nu,\omega\kappa^{\prime }}+\delta_{\beta\kappa}\delta_{\omega\kappa^{\prime}}S_{1}^{\alpha\nu,\omega \kappa^{\prime}}\Big{)}\] \[+\mathcal{O}(nn_{k}\tau^{-3})\] \[=\frac{nn_{k}}{\tau^{2}}\underbrace{\frac{1}{m^{2}}\sum_{\nu \kappa}\left(V^{\alpha\kappa}V^{\delta\nu}S_{1}^{\beta\kappa,\omega\nu}+V^{ \alpha\kappa}V^{\omega\nu}S_{1}^{\beta\kappa,\delta\nu}+V^{\beta\nu}V^{\delta \kappa}S_{1}^{\alpha\nu,\omega\kappa}+V^{\beta\nu}V^{\omega\kappa}S_{1}^{ \alpha\nu,\delta\kappa}\right)}_{\mathcal{A}^{\alpha\beta\omega}}\] \[+\mathcal{O}(nn_{k}\tau^{-3}).\]

We are now ready to re-state and proof of Theorem 4.2.

**Theorem 4.2**.: _Let \(X_{\ell}\) be the hidden layers of a residual attention network defined in Eq. 1 with shaped attention in Eq. 9, parameters \(\lambda^{2}+\gamma^{2}=1\) and \(\tau=\tau_{0}\sqrt{nn_{k}}\), where \(\lambda,\gamma,\tau_{0}\) all do not depend on \(d,n\). Then the feature covariance \(V_{\ell}\) converges locally to the solution of the following SDE (in the sense of Definition 4.1)_

\[dV_{t}=b(V_{t})dt+\Sigma(V_{t})^{1/2}dB_{t}\,,\quad V_{0}=\frac{1}{n}X_{0}X_{0} ^{\top}\,,\]

_where the drift has the following form_

\[b(V)=\frac{\gamma^{2}}{\tau_{0}^{2}}\left[\frac{1}{m^{2}}\sum_{\nu,\kappa=1}^{ m}V^{\nu\kappa}S_{1}^{\alpha\nu,\beta\kappa}+\frac{1}{2m}\sum_{\nu=1}^{m}(V^{ \beta\nu}S_{2}^{\alpha\nu}+V^{\alpha\nu}S_{2}^{\beta\nu})\right]_{\alpha\leq\beta}\,,\]

_the diffusion coefficient is defined by \(\Sigma(V)=\gamma^{2}(2-\gamma^{2})\Sigma_{\text{lin}}(V)+\gamma^{4}\tau_{0}^{ -2}[\mathcal{A}^{\alpha\beta,\delta\omega}]_{\alpha\leq\beta,\delta\leq\omega}\), and_

\[\mathcal{A}^{\alpha\beta,\delta\omega}:=\frac{1}{m^{2}}\sum_{\nu,\kappa=1}^{m} \left(V^{\alpha\kappa}V^{\delta\nu}S_{1}^{\beta\kappa,\omega\nu}+V^{\alpha \kappa}V^{\omega\nu}S_{1}^{\beta\kappa,\delta\nu}+V^{\beta\nu}V^{\delta\kappa} S_{1}^{\alpha\nu,\omega\kappa}+V^{\beta\nu}V^{\omega\kappa}S_{1}^{\alpha\nu, \delta\kappa}\right)\,.\]

Proof.: Recall that:

\[V_{\ell+1}^{\alpha\beta}=\lambda^{2}V_{\ell}^{\alpha\beta}+\frac{\lambda\gamma }{\sqrt{n}}\mathcal{T}_{1}^{\alpha\beta}+\frac{\gamma^{2}}{\sqrt{n}}\mathcal{ T}_{2}^{\alpha\beta}.\]

**Drift.** Summing and subtracting \(\mathbb{E}_{\ell}[\mathcal{T}_{2}]\) (and using Lemma C.7), we have that:

\[V_{\ell+1}^{\alpha\beta}=\lambda^{2}V_{\ell}^{\alpha\beta}+\gamma^{2}\sum_{ \nu\kappa}V^{\nu\kappa}\mathbb{E}_{\ell}\left[A^{\alpha\nu}A^{\beta\kappa} \right]+\frac{\lambda\gamma}{\sqrt{n}}\mathcal{T}_{1}^{\alpha\beta}+\frac{ \gamma^{2}}{\sqrt{n}}\left(\mathcal{T}_{2}^{\alpha\beta}-\mathbb{E}_{\ell}[ \mathcal{T}_{2}]\right)\,,\]

where we recall \(\mathbb{E}_{\ell}[\cdot\,]=\mathbb{E}[\cdot\,|\mathcal{F}_{\ell}]\) is the conditional expectation given the sigma-algebra generated by \(\mathcal{F}_{\ell}=\sigma(\{X_{k}\}_{k\in[\ell]})\).

Re-stating Lemma C.3, we have that:

\[\mathbb{E}_{\ell}[A^{\alpha\nu}A^{\beta\kappa}]=\delta_{\alpha\omega}\delta_{ \beta\kappa}+\frac{n_{k}}{\tau^{2}m^{2}}S_{1}^{\alpha\nu,\beta\kappa}+\frac{n_ {k}}{2\tau^{2}m}(\delta_{\beta\kappa}S_{2}^{\alpha\nu}+\delta_{\alpha\nu}S_{2} ^{\beta\kappa})+O(n_{k}\tau^{-3}).\]

Plugging in the expression, and using \(\lambda^{2}+\gamma^{2}=1\), we have that the drift is:

\[\lambda^{2}V_{\ell}^{\alpha\beta}+\gamma^{2}V_{\ell}^{\alpha\beta }+\gamma^{2}\frac{n_{k}}{\tau^{2}m}\sum_{\nu\kappa}V^{\nu\kappa}\left(\frac{1}{m }S_{1}^{\alpha\nu,\beta\kappa}+\frac{1}{2}(\delta_{\beta\kappa}S_{2}^{\alpha\nu }+\delta_{\alpha\nu}S_{2}^{\beta\kappa})\right)+O(n_{k}\tau^{-3})\] \[=V_{\ell}^{\alpha\beta}+\gamma^{2}\frac{n_{k}}{\tau^{2}}\left( \frac{1}{m^{2}}\sum_{\nu\kappa}V^{\nu\kappa}S_{1}^{\alpha\nu,\beta\kappa}+\frac{1 }{2m}\sum_{\nu}(V^{\beta\nu}S_{2}^{\alpha\nu}+V^{\alpha\nu}S_{2}^{\beta\nu}) \right)+O(n_{k}\tau^{-3})\,.\]From here, it is evident that in order to have the drift scaling as \(\mathcal{O}(1/n)\) we need to choose:

\[\tau^{2}=\tau_{0}^{2}nn_{k},\] (26)

where \(\tau_{0}>0\) is a constant.

**Covariance.** Recall that:

\[V_{\ell+1}^{\alpha\beta}=\lambda^{2}V_{\ell}^{\alpha\beta}+\gamma^{2}\sum_{ \nu\kappa}V^{\nu\kappa}\mathbb{E}_{\ell}\left[A^{\alpha\nu}A^{\beta\kappa} \right]+\frac{\lambda\gamma}{\sqrt{n}}\mathcal{T}_{1}^{\alpha\beta}+\frac{ \gamma^{2}}{\sqrt{n}}\left(\mathcal{T}_{2}^{\alpha\beta}-\mathbb{E}_{\ell}[ \mathcal{T}_{2}]\right).\]

Furthermore, we have set \(\lambda^{2}+\gamma^{2}=1\) and \(\tau^{2}=\tau_{0}^{2}nn_{k}\) to have the right scaling for the drift.

What's left to is to compute the conditional covariance for \(V_{\ell+1}\) given \(\mathcal{F}_{\ell}\). Noting that \(\mathcal{T}_{1}^{\alpha\beta},\mathcal{T}_{2}^{\delta\omega}\) are uncorrelated, i.e. \(\mathbb{E}_{\ell}\left[\mathcal{T}_{1}^{\alpha\beta}\mathcal{T}_{2}^{\delta \omega}\right]=0\) (similarly to the case of Resnet with shaped ReLU Lemma B.3), we have that:

\[\text{Cov}_{\ell}\left(V_{\ell+1}^{\alpha\beta},V_{\ell+1}^{ \delta\omega}\right) =\mathbb{E}_{\ell}\left[\left(\lambda\gamma\mathcal{T}_{1}^{ \alpha\beta}+\gamma^{2}\mathcal{T}_{2}^{\alpha\beta}-\gamma^{2}\mathbb{E}_{ \ell}[\mathcal{T}_{2}^{\alpha\beta}]\right)\left(\lambda\gamma\mathcal{T}_{1}^ {\delta\omega}+\gamma^{2}\mathcal{T}_{2}^{\delta\omega}-\gamma^{2}\mathbb{E}_{ \ell}[\mathcal{T}_{2}^{\delta\omega}]\right)\right]\] \[=\lambda^{2}\gamma^{2}\mathbb{E}_{\ell}\left[\mathcal{T}_{1}^{ \alpha\beta}\mathcal{T}_{1}^{\delta\omega}\right]+\gamma^{4}\mathbb{E}_{\ell} \left[\mathcal{T}_{2}^{\alpha\beta}\mathcal{T}_{2}^{\delta\omega}\right]- \gamma^{4}\mathbb{E}_{\ell}\left[\mathcal{T}_{2}^{\alpha\beta}\right]\mathbb{E }_{\ell}\left[\mathcal{T}_{2}^{\delta\omega}\right]\,,\]

where we use \(\text{Cov}_{\ell}\) to denote the conditional covariance given the sigma-algebra generated by \(\mathcal{F}_{\ell}=\sigma(\{X_{k}\}_{k\in[\ell]})\).

Using Lemma C.6 and Lemma C.8, we have that:

\[\text{Cov}_{\ell}\left(V_{\ell+1}^{\alpha\beta},V_{\ell+1}^{ \delta\omega}\right) =\lambda^{2}\gamma^{2}\left(2(V^{\alpha\delta}V^{\beta\omega}+V^{ \alpha\omega}V^{\beta\delta})\right)+\gamma^{4}\left(\frac{1}{\tau_{0}^{2}} \mathcal{A}^{\alpha\beta\delta\omega}+V^{\alpha\delta}V^{\beta\omega}+V^{ \alpha\omega}V^{\beta\delta}\right)\] \[=\gamma^{2}(2-\gamma^{2})\left(V^{\alpha\delta}V^{\beta\omega}+V^{ \alpha\omega}V^{\beta\delta}\right)+\frac{\gamma^{4}}{\tau_{0}^{2}}\mathcal{A}^ {\alpha\beta\delta\omega}\,.\]

Now we can apply Proposition A.2 for locally Lipschitz drift and covariance coefficients, which gives us the desired result in local convergence in the Skorohod topology.

We will also restate and prove Corollary 4.3.

**Corollary 4.3** (Shaped Transformer Covariance SDE).: _Let \(X_{\ell}\) be the hidden layers of a shaped transformer defined in Eq. 11 with parameters \(\lambda^{2}+\gamma^{2}=1\) and \(\tau=\tau_{0}\sqrt{nn_{k}}\), where \(\lambda,\gamma,\tau_{0}\) all do not depend on \(d,n\). Then the feature covariance \(V_{\ell}\) converges locally to the solution of the following SDE (in the sense of Definition 4.1)_

\[dV_{t}=\left[b(V_{t})+b_{\text{res}}(V_{t})\right]dt+[\Sigma(V_{t})+\Sigma_{ \text{res}}(V_{t})]^{1/2}\,dB_{t}\,,\] (12)

_where the coefficients are defined in Theorem 3.2 and Theorem 4.2._

Proof.: To combine the results of Theorem 3.2 and Theorem 4.2, it is sufficient to combine the following (simplified) iterated Markov updates into one Markov chain

\[U_{\ell}=V_{\ell}+\frac{b(V_{\ell})}{n}+\frac{\Sigma(V_{\ell})^{1/2}\xi_{\ell}} {\sqrt{n}}\,,\quad V_{\ell+1}=U_{\ell}+\frac{b_{\text{ReLU}}(U_{\ell})}{n}+ \frac{\Sigma_{\text{lin}}(U_{\ell})^{1/2}\xi_{\ell}^{\prime}}{\sqrt{n}}\,,\] (27)

where \(\xi_{\ell},\xi_{\ell}^{\prime}\) are independent zero mean and identity covariance random vectors.

Since in the limit, we have that either updates are infinitesimal, i.e.

\[|U_{\ell}-V_{\ell}|\xrightarrow{n\to\infty}0\quad\text{almost surely},\] (28)

then we can write \(b_{\text{ReLU}(U_{\ell})}=\widehat{b}_{\text{ReLU},n}(V_{\ell},\omega_{\ell})\), where eventually we have that

\[\lim_{n\to\infty}\mathbb{E}_{\ell}\left[\widehat{b}_{n}(V_{\ell},\omega_{\ell}) \right]=b(V_{\ell})\,,\] (29)so it will not contribute towards affecting the limit. Here we recall \(\mathbb{E}_{\ell}\big{[}\,\cdot\,\big{]}=\mathbb{E}\big{[}\,\cdot\,|\mathcal{F}_{ \ell}|\) is the conditional expectation given the sigma-algebra generated by \(\mathcal{F}_{\ell}=\sigma(\{X_{k}\}_{k\in[\ell]})\). We can treat \(\Sigma_{\text{lin}}(U_{\ell})\) similarly to get the Markov chain update

\[V_{\ell+1}=V_{\ell}+\frac{b(V_{\ell})+\widehat{b}_{\text{ReLU},n}(V_{\ell}, \omega_{\ell})}{n}+\frac{\Sigma(V_{\ell})^{1/2}\xi_{\ell}}{\sqrt{n}}+\frac{ \widehat{\Sigma}_{\text{lin},n}(V_{\ell})^{1/2}\xi_{\ell}^{\prime}}{\sqrt{n}}\,,\] (30)

which converges to the following SDE with two Brownian motions using Proposition A.2

\[dV_{t}=[b(V_{t})+b_{\text{res}}(V_{t})]\,dt+\Sigma(V_{t})^{1/2}\,dB_{t}+ \Sigma_{\text{res}}(V_{t})^{1/2}dB_{t}^{\prime}\,.\] (31)

Observe that since the two Brownian motions \(B_{t},B_{t}^{\prime}\) are independent, it's equivalent to write

\[\Sigma(V_{t})^{1/2}\,dB_{t}+\Sigma_{\text{res}}(V_{t})^{1/2}dB_{t}^{\prime} \stackrel{{ d}}{{=}}[\Sigma(V_{t})+\Sigma_{\text{res}}(V_{t})^{1/ 2}]^{1/2}\,dB_{t}\,.\] (32)

We recover the desired results from considering a more general form of the iterated Markov updates as in Proposition A.2, which do not hinder the above derivation.

## Appendix D Preliminary Experiments

We perform preliminary experiments to understand the effect of _shaped attention_ on training deep Transformer architectures. In particular, we consider a pre-training masked language modeling task, where we mask 15% of the tokens. We use a subset of the English Wikipedia _20220301.en_ and English _bookcorpus_ datasets [69, 70]. As a baseline, we adopt a Pre-LN Transformer encoder architecture with 18 or 24 blocks. For the residual feedforward layer, we shape the ReLU activation according to Eq. 4, by changing its negative slope to \(s_{-}=1-1/\sqrt{n}\) instead of \(0\). We then incorporate our shaped attention by replacing the attention mechanism as dictated in Eq. 9. We also add scalar multipliers \(\gamma_{1},\gamma_{2}\in\mathbb{R}\) both to the identity and centering terms of the shaped attention:

\[A_{\ell}=\gamma_{1}I+\text{Softmax}(\tau^{-1}Y_{\ell})-\gamma_{2}\frac{1}{m} \mathbf{1}\mathbf{1}^{\top}\,,\quad\tau=\tau_{0}\sqrt{nn_{k}}\,,\] (33)

and propose two ways to set \(\gamma_{1},\gamma_{2}\) during training. In both alternatives we initialize \(\gamma_{1},\gamma_{2}=1\), thus leveraging the stability properties of shaped attention at initialization. During training, we either:

1. **Recover**. Linearly decrease \(\gamma_{1},\gamma_{2}\) to zero with in the first \(4000\) steps, thus recovering the standard attention layer. Apply the same schedule for the shaped-ReLU slope \(s_{-}\), recovering the usual ReLU activation. This approach recovers the vanilla Transformer architecture (without LayerNorm).
2. **Learn**. Learn all the shaping parameters \(\gamma_{1}\), \(\gamma_{2}\) and \(s_{-}\).

The intuitive rationale behind these choices is that at initialization we want good signal propagation and a non-degenerate covariance (according to our theory, this requires the shaped attention and ReLU). On the other hand, we also allow the model to more dynamically make use of the nonlinearity during training to modify the correlation structure with **Recover** or **Learn**. We report that without either adjustment, _shaped attention_ is still trainable but at much slower rates.

We also incorporate the \(\frac{1}{\sqrt{n}}\) factor into the initialization of the queries and keys weights by decreasing the variance of their entries by a factor of \(\frac{1}{\eta}\). This allows us to not re-tune the learning rate for the queries and keys. We stress that at at initialization, the two formulations (\(\tau=\sqrt{nn_{k}}\) and \(\tau=\sqrt{n_{k}}\) with decreased weight's variance) are equivalent. In both alternatives we train all the skip connection parameters \(\lambda\), \(\gamma\), and initialize \(\gamma\) in the grid \((0.05,0.1,0.2)\) and set \(\tau_{0}=1\). We report training instabilities (loss divergence) for larger values of \(\gamma\). All models --including the baselines -- use Adam [71] with learning rate warmup of \(4000\) steps, the learning rate is tuned in the grid \((0.0001,0.0005,0.001,0.005)\). We report the train/test loss after \(100K\) optimization steps, averaged over \(4\) random seeds. All the other experimental details can be found in Appendix D.1.

**The Shaped Transformer is Trainable.** In Table 1, we compare the train/test loss of the two variant of shaped attention with the baseline Pre-LN model. Notice that our model (in both variants) achieves comparable performance to standard Transformers across all the reported values of \(\gamma\).

**GLUE evaluation** Furthermore, we evaluate the trained models on three datasets from the GLUE benchmark [72] and summarize the results in Table 2. These demonstrate that our shaped Transformers holds promise by outperforming our pre-ln baseline.

**Entropy Collapse for Large Learning rates.** To understand the sources of training instability, we keep track of the entropy of the probability distribution induced by the Softmax, as it has been observed that the Transformer's training is unstable in the low-entropy regime [57]. The entropy is calculated for each row of the Softmax matrix, and it is averaged across rows and heads. The results are in Fig. 6. Notice how for the large learning rate regime observed in Fig. 6, the entropy collapses for the baseline model, but not for the proposed _shaped Transformer_. Entropy collapse indicates that the Softmax distribution degenerates to a point mass, which is itself caused by large logits. Remarkably, this phenomenon does not affect the recover setting, despite recovering the Transformer architecture (without layer normalization) after the warm-up period.

### Experimental details

**Dataset.** We use a subset of the English Wikipedia _20220301.en_ and English _bookcorpus_ datasets [69; 70]. The sentences are tokenized using by pre-training a tokenizer on the training set. We use a vocabulary size of \(32000\), and a maximum sequence length of \(128\) tokens.

**Model parameters.** We use an embedding size of \(n=768\) and \(8\) multi-attention heads. The batch size is fixed to \(32\) sequences. All the initial weights are sampled from \(\mathcal{N}(0,n^{-1})\), with the exception of the queries and keys' weights \(W^{K},W^{Q}\) in the _shaped attention_ case, that are sampled from \(\mathcal{N}(0,n^{-3/2})\) (as explained in Appendix D). The feedforward layer maps the \(n=768\)-dimensional embedding to the larger dimension \(3072\), as in the Hugging face implementation of Bert [73].

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline  & \multicolumn{4}{c}{\(d=18\)} & \multicolumn{2}{c}{\(d=24\)} \\ \cline{2-6}  & \(\gamma\) & Train Loss & Test Loss & Train Loss & Test Loss \\ \hline \multirow{3}{*}{Learn} & 0.05 & \(2.09_{\pm 0.02}\) & \(2.07_{\pm 0.02}\) & \(2.03_{\pm 0.02}\) & \(2.03_{\pm 0.01}\) \\  & 0.1 & \(2.10_{\pm 0.02}\) & \(2.07_{\pm 0.01}\) & \(\mathbf{2.02}_{\pm 0.01}\) & \(2.02_{\pm 0.03}\) \\  & 0.2 & \(2.07_{\pm 0.05}\) & \(2.06_{\pm 0.02}\) & \(2.09_{\pm 0.04}\) & \(2.09_{\pm 0.03}\) \\ \hline \multirow{3}{*}{Recover} & 0.05 & \(2.05_{\pm 0.03}\) & \(2.04_{\pm 0.02}\) & \(2.02_{\pm 0.04}\) & \(2.00_{\pm 0.01}\) \\  & 0.1 & \(2.05_{\pm 0.01}\) & \(2.04_{\pm 0.01}\) & \(2.06_{\pm 0.03}\) & \(2.00_{\pm 0.02}\) \\  & 0.2 & \(\mathbf{2.01}_{\pm 0.03}\) & \(\mathbf{2.03}_{\pm 0.01}\) & \(2.05_{\pm 0.04}\) & \(2.00_{\pm 0.01}\) \\ \hline Baseline & & \(2.08_{\pm 0.03}\) & \(2.07_{\pm 0.01}\) & \(2.08_{\pm 0.03}\) & \(2.01_{\pm 0.01}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Training and test loss of the proposed _shaped attention_, both in the ”Recover” and “Learn” alternatives. Baseline refers to the vanilla Pre-LN Transformer. We report the best run under the learning rates \((0.0001,0.0005,0.001,0.005)\), averaged over \(4\) random seeds. We include the confidence interval of \(\pm\) one standard deviation.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline  & Model & COLA & MRPC & RTE \\ \hline \multirow{3}{*}{\(d=18\)} & Baseline & \(0.139_{\pm 0.008}\) & \(0.797_{\pm 0.010}\) & \(0.504_{\pm 0.024}\) \\  & Learn, \(\gamma=0.2\) & \(0.182_{\pm 0.041}\) & \(0.813_{\pm 0.005}\) & \(0.528_{\pm 0.019}\) \\  & Recover, \(\gamma=0.2\) & \(0.221_{\pm 0.042}\) & \(0.809_{\pm 0.009}\) & \(0.520_{\pm 0.029}\) \\ \hline \multirow{3}{*}{\(d=24\)} & Baseline & \(0.022_{\pm 0.055}\) & \(0.785_{\pm 0.009}\) & \(0.48_{\pm 0.046}\) \\  & Learn, \(\gamma=0.05\) & \(0.150_{\pm 0.026}\) & \(0.812_{\pm 0.004}\) & \(0.513_{\pm 0.019}\) \\  & Recover, \(\gamma=0.05\) & \(0.211_{\pm 0.039}\) & \(0.803_{\pm 0.012}\) & \(0.529_{\pm 0.019}\) \\ \hline \multicolumn{3}{c}{BERT (Geiping et al.)} & 0.103 & 74.8 & 0.509 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Evaluation of the pretrained models on a subset of the GLUE benchmark. We take the checkpoints of the baselines and selected shaped Transformers (\(\gamma=0.2\) for the shallower model \(d=18\), \(\gamma=0.05\) for the deeper model \(d=24\)). Then, we fine-tune on each of three datasets from GLUE (COLA, MRPC, RTE) for 5 epochs and Adam optimizer \(lr=5e-4\) with batch size 16, and report test evaluation metric for the corresponding task.

**Optimization.** We train using Adam [71] with betas parameters (\(0.9\), \(0.999\)) and learning rate chosen in the grid \((0.0001,0.0005,0.001,0.005)\). We do not use weight decay.

**Computational Resources.** The experiments are executed on Nvidia DGX-1 GPU nodes equipped with 4 20-core Xeon E5-2698v4 processors, 512 GB of memory and 8 Nvidia V100 GPUs.

## Appendix E Additional Figures

Figure 6: Dynamics of the mean entropy across heads for selected layers (first and seventh) and learning rates. (Above): _shaped attention_ in recover setting. (Below): Baseline transformers. Notice that at \(lr=0.001\) the entropy collapses for the baseline model.

Figure 7: Kernel density estimate and histogram of covariances from the covariance SDE in Theorem 4.2 and shaped attention NN (Eq. 9). Simulated with \(n=200,d=150,\gamma=1/\sqrt{8},\tau_{0}=1,V_{0}^{\alpha\beta}=0.2\), SDE step size \(0.01\), and \(2^{12}\) samples.

Figure 8: Median of the stopping time capped at 1, of the shaped attention neural network with respect to its parameters \(\gamma\) and \(\tau_{0}\). Stopping time is defined as \(t^{*}=d^{*}/n\) with \(d^{*}\) the maximum depth beyond which one of the eigenvalues of the covariance matrix exceeds \(10^{4}\) or drops below \(10^{-4}\). Simulated with \(n=d=200\), and \(100\) samples used to estimate the median. To demonstrate the potential numerical instabilities, we had to choose an _adversarial_ set of parameters: in particular, an unrealistically large norm (approx. \(10\sqrt{n}\)) for the initial tokens \(X_{0}\), which enlarges the eigenvalues of \(V_{0}\) to the order of \(100\).