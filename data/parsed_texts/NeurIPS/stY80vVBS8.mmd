# Learning-Augmented Dynamic Submodular Maximization

Arpit Agarwal

Indian Institute of Technology Bombay

aarpit@iitb.ac.in

&Eric Balkanski

Columbia University

eb3224@columbia.edu

###### Abstract

In dynamic submodular maximization, the goal is to maintain a high-value solution over a sequence of element insertions and deletions with a fast update time. Motivated by large-scale applications and the fact that dynamic data often exhibits patterns, we ask the following question: can predictions be used to accelerate the update time of dynamic submodular maximization algorithms?

We consider the model for dynamic algorithms with predictions where predictions regarding the insertion and deletion times of elements can be used for preprocessing. Our main result is an algorithm with an \(O(\text{poly}(\log\eta,\log w,\log k))\) amortized update time over the sequence of updates that achieves a \(1/2-\epsilon\) approximation for dynamic monotone submodular maximization under a cardinality constraint \(k\), where the prediction error \(\eta\) is the number of elements that are not inserted and deleted within \(w\) time steps of their predicted insertion and deletion times. This amortized update time is independent of the length of the stream and instead depends on the prediction error.

## 1 Introduction

Submodular functions are a well-studied family of functions that satisfy a natural diminishing returns property. Since many fundamental objectives are submodular, including coverage, diversity, and entropy, submodular optimization algorithms play an important role in machine learning [18; 8], network analysis [17], and mechanism design [31]. For the canonical problem of maximizing a monotone submodular function under a cardinality constraint, the celebrated greedy algorithm achieves a \(1-1/e\) approximation guarantee [27], which is the best approximation guarantee achievable by any polynomial-time algorithm [26]. Motivated by the highly dynamic nature of applications such as influence maximization in social networks and recommender systems on streaming platforms, a recent line of work has studied the problem of dynamic submodular maximization [20; 25; 28; 10; 11; 6; 7]. In the dynamic setting, the input consists of a stream of elements that are inserted or deleted from the set of active elements, and the goal is to maintain, throughout the stream, a subset of the active elements that maximizes a submodular function.

The standard worst-case approach to analyzing the update time of a dynamic algorithm is to measure its update time over the worst sequence of updates possible. However, in many application domains, dynamic data is not arbitrary and often exhibits patterns that can be learned from historical data. Very recent work has studied dynamic problems in settings where the algorithm is given as input predictions regarding the stream of updates [14; 22; 9]. This recent work is part of a broader research area called learning-augmented algorithms (or algorithms with predictions). In learning-augmented algorithms, the goal is to design algorithms that achieve an improved performance guarantee when the error of the prediction is small and a bounded guarantee even when the prediction error is arbitrarily large. A lot of the effort in this area has been focused on using predictions to improve the competitive ratio of online algorithms (see, e.g., [23; 30; 32; 24; 12; 3; 4; 19; 15; 5]), and more generally to improve the solution quality of algorithms.

For dynamic submodular maximization with predictions, Liu and Srinivas [22] considered a predicted-deletion model and achieved, under some mild assumption, a \(0.3178\) approximation and an \(\tilde{O}(\text{poly}(k,\log n))\)1 update time for dynamic monotone submodular maximization under a matroid constraint of rank \(k\) and over a stream of length \(n\). This approximation is an improvement over the best-known \(1/4\) approximation for dynamic monotone submodular maximization (without predictions) under a matroid constraint with an update time that is sublinear in \(n\)[7, 11]. Since the update time of dynamic algorithms is often the main bottleneck in large-scale problems, another promising direction is to leverage predictions to improve the update time of dynamic algorithms.

Footnote 1: In this paper, we use the notation \(\tilde{O}(g(n))\) as shorthand for \(O(g(n)\log^{k}g(n))\).

#### Can predictions help to accelerate the update time of dynamic submodular maximization algorithms?

We note that the three very recent papers on dynamic algorithms with predictions have achieved improved update times for several dynamic graph problems [14, 22, 9]. However, to the best of our knowledge, there is no previous result that achieves an improved update time for dynamic submodular maximization by using predictions.

**Our contributions.** In dynamic submodular maximization, the input is a submodular function \(f\,:\,2^{V}\rightarrow\mathbb{R}_{\geq 0}\) and a sequence of \(n\) element insertions and deletions. The active elements \(V_{t}\subseteq V\) at time \(t\) are the elements that have been inserted and have not yet been deleted during the first \(t\) updates. The goal is to maintain, at every time step \(t\), a solution \(S_{t}\subseteq V_{t}\) that is approximately optimal with respect to \(V_{t}\) while minimizing the number of queries to \(f\) at each time step, which is referred to as the update time. As in [14, 9], we consider a prediction model where, at time \(t=0\), the algorithm is given a prediction regarding the sequence of updates, which can be used for preprocessing. More precisely, at time \(t=0\), the algorithm is given predictions \((\hat{t}_{a}^{+},\hat{t}_{a}^{-})\) about the insertion and deletion time of each element \(a\). A dynamic algorithm with predictions consists of two phases. During the precomputation phase at \(t=0\), the algorithm uses the predictions to perform queries before the start of the stream. During the streaming phase at time steps \(t>0\), the algorithm performs queries, and uses the precomputations, to maintain a good solution with respect to the true stream.

In this model, there is a trivial algorithm that achieves a constant update time when the predictions are exactly correct and an \(O(u)\) update time when the predictions are arbitrarily wrong. Here, \(u\) is the update time of an arbitrary algorithm \(\mathcal{A}\) for the problem without predictions. This algorithm precomputes, for each future time step \(t\), a solution for the elements that are predicted to be active at time \(t\) and then, during the streaming phase, returns the precomputed solutions while the prediction is correct and switches to running algorithm \(\mathcal{A}\) at the first error in the prediction. Thus, the interesting question is whether it is possible to obtain an improved update time not only when the predictions are exactly correct, but more generally when the error in the predictions is small. An important component of our model is the measure for the prediction error. Given a time window tolerance \(w\), an element \(a\) is considered to be correctly predicted if the predicted insertion and deletion times of \(a\) are both within \(w\) times steps of its true insertion and deletion times. The prediction error \(\eta\) is then the number of elements that are not correctly predicted. Thus, \(\eta=0\) if the predictions are exactly correct and \(\eta=\Theta(n)\) if the predictions are completely wrong.

For dynamic monotone submodular maximization (without predictions) under a cardinality constraint \(k\), Lattanzi et al. [20] and Monemizadeh [25] concurrently obtained dynamic algorithms with \(O(\text{poly}(\log n,\log k))\) and \(O(\text{polylog}(n)\cdot k^{2})\) amortized update time, respectively, that achieve a \(1/2-\epsilon\) approximation. More recently, Banihashem et al. [7] achieved a \(1/2-\epsilon\) approximation with a \(O(k\cdot\text{polylog}(k))\) amortized update time. Our main result is the following.

**Theorem**.: _For monotone submodular maximization under a cardinality constraint \(k\), there is a dynamic algorithm with predictions that, for any tolerance \(w\) and constant \(\epsilon>0\), achieves an amortized expected query complexity per update of \(O(\text{poly}(\log\eta,\log w,\log k))\), an approximation of \(1/2-\epsilon\) in expectation, and a query complexity of \(\tilde{O}(n)\) during the precomputation phase._

We note that, when the prediction error \(\eta\) is arbitrarily large, our algorithm matches the \(O(\text{poly}(\log n,\log k))\) amortized expected query complexity per update in [20]. It also achieves an approximation that matches the optimal approximation for dynamic algorithms (without predictions) with update time that is sublinear in \(n\). An intriguing open question is whether an improvement in update time can be obtained in the predicted-deletion model of [22] with no preprocessing and instead a predicted deletion time for each element \(a\) is given at the time when \(a\) is inserted.

**Related work.** For monotone submodular maximization under a cardinality constraint, dynamic algorithms with \(O(\text{poly}(\log n,\log k))\) and \(O(\text{polylog}(n)\cdot k^{2}))\) amortized update time that achieve a \(1/2\) approximation were concurrently obtained in [20, 25]. Recently, Banihashem et al. [7] gave a \(1/2\) approximation algorithm with a \(O(k\cdot\text{polylog}(k))\) amortized update time. Chen and Peng [10] showed that any dynamic algorithm with an approximation better than \(1/2\) must have \(\text{poly}(n)\) amortized query complexity per update. For matroid constraints, Chen and Peng [10] obtained an insertion-only algorithm. As mentioned in [29], the streaming algorithm in [13] can be adapted to also give an insertion-only algorithm. Two \(1/4\)-approximation dynamic algorithms with \(\tilde{O}(k)\) and \(\tilde{O}(\text{polylog}(n)\cdot k^{2})\) amortized update time were concurrently obtained in [7] and [11].

Algorithms with predictions have been studied in a wide range of areas, including online algorithms [23, 30], mechanism design [1, 33], and differential privacy [2]. Improved update times for several dynamic graph problems were very recently obtained by leveraging predictions [14, 22, 9]. In particular, Liu and Srinivas [22] obtained, under some mild assumption on the prediction error, a \(0.3178\) approximation and a \(\tilde{O}(\text{poly}(k,\log n))\) update time for dynamic monotone submodular maximization under a matroid constraint of rank \(k\) in the more challenging predicted-deletion model. Thus, by using predictions, this result improves the \(1/4\) approximation achieved, without predictions, in [7] and [11] (but does not improve the \(1/2\) approximation for cardinality constraints). The results in [22] use a framework that takes as input an insertion-only dynamic algorithm. In contrast, we develop a framework that uses a fully dynamic algorithm and a deletion-robust algorithm.

## 2 Preliminaries

A function \(f:2^{V}\rightarrow\mathbb{R}\) defined over a ground set \(V\) is submodular if for all \(S\subseteq T\subseteq V\) and \(a\in V\setminus T\), we have that \(f_{S}(a)\geq f_{T}(a)\), where \(f_{S}(a)=f(S\cup\{a\})-f(S)\) is the marginal contribution of \(a\) to \(S\). It is monotone if \(f(S)\leq f(T)\) for all \(S\subseteq T\subseteq V\). We consider the canonical problem of maximizing a monotone submodular function \(f\) under a cardinality constraint \(k\).

In **dynamic submodular maximization**, there is a stream \(\{(a_{t},o_{t})\}_{t=1}^{n}\) of \(n\) element insertions and deletions where \(o_{t}\in\{\text{insert},\text{deletion}\}\) and \(a_{t}\) is an element in \(V\). The active elements \(V_{t}\) are the elements that have been inserted and have not been deleted by time \(t\). We assume that \((a_{t},\text{insertion})\) and \((a_{t},\text{deletion})\) can occur in the stream only if \(a_{t}\not\in V_{t}\) and \(a_{t}\in V_{t}\), respectively, and that each element is inserted at most once.2 The goal is to maintain a solution \(S_{t}\subseteq V_{t}\) that approximates the optimal solution over \(V_{t}\), which we denote by \(O_{t}\). Since our algorithmic framework takes as input a dynamic algorithm, we formally define dynamic algorithms in terms of black-box subroutines that are used in our algorithms.

Footnote 2: If an element \(a\) is re-inserted, a copy \(a^{\prime}\) of \(a\) can be created.

**Definition 1**.: _A dynamic algorithm \(\textsc{Dynamic}(f,k)\) consists of the following four subroutines to process a stream \(\{(a_{t},o_{t})\}_{t=1}^{n}\). \(\textsc{DynamicInit}(f,k)\) initializes a data structure \(A\) at \(t=0\). If \(o_{t}=\text{insertion or }o_{t}=\text{deletion}\), \(\textsc{DynamicIns}(A,a_{t})\) or \(\textsc{DynamicDel}(A,a_{t})\) insert in \(A\) or delete from \(A\) element \(a_{t}\) at time \(t\). At time \(t\), \(\textsc{DynamicSol}(A)\) returns \(S_{t}\subseteq V(A)\) s.t. \(|S|\leq k\), where \(V(A)=V_{t}\) is the set of elements that have been inserted in and not been deleted from \(A\)._

A dynamic algorithm achieves an \(\alpha\)-approximation in expectation if, for all time steps \(t\), \(\mathbf{E}[f(S_{t})]\geq\alpha\cdot\max_{S\subseteq V_{t}:|S|\leq k}f(S)\) and has a \(u(n,k)\) amortized expected query complexity per update if its expected total number of queries is \(n\cdot u(n,k)\).

In **dynamic submodular maximization with predictions**, the algorithm is given at time \(t=0\) predictions \(\{(\hat{t}_{a}^{+},\hat{t}_{a}^{-})\}_{a}\) about the insertion and deletion time of elements \(a\). The prediction error \(\eta\) is the number of elements that are incorrectly predicted, where an element \(a\) is correctly predicted if it is inserted and deleted within a time window, of size parameterized by a time window tolerance \(w\), that is centered at the time at which \(a\) is predicted to be inserted and deleted.

**Definition 2**.: _Given a tolerance \(w\in\mathbb{Z}_{+}\), predictions \(\{(\hat{t}_{a}^{+},\hat{t}_{a}^{-})\}_{a\in V}\), and true insertion and deletions times \(\{(t_{a}^{+},t_{a}^{-})\}_{a\in V}\), the prediction error is \(\eta=|\{a\in V:|\hat{t}_{a}^{+}-t_{a}^{+}|>w\text{ or }|\hat{t}_{a}^{-}-t_{a}^{-}|>w\}|\)._

We note that \(\eta=w=0\) corresponds to the predictions being exactly correct and \(\eta=O(n)\) and \(w=O(n)\) corresponds to thems being arbitrarily wrong. We also emphasize that our model does not require knowing the entire ground set \(V\) at \(t=0\). Elements that are not known at \(t=0\) are assumed to have predicted arrival and departure times equal to infinity, and contribute to the prediction error \(\eta\)

**Deletion-robust submodular maximization.** Our framework also takes as input a _deletion-robust algorithm_, which we formally define in terms of black-box subroutines. A deletion-robust algorithm finds a solution \(S\subseteq V\) that is robust to the deletion of at most \(d\) elements.

**Definition 3**.: _Given a function \(f:2^{V}\rightarrow\mathbb{R}\), a cardinality constraint \(k\), and a maximum number of deletions parameter \(d\), a deletion-robust algorithm \(\textsc{Robust}(f,V,k,d)\) consists of a first subroutine \(\textsc{Robust1}(f,V,k,d)\) that returns a robust set \(R\subseteq V\) and a second subroutine \(\textsc{Robust2}(f,R,D,k)\) that returns a set \(S\subseteq R\setminus D\) such that \(|S|\leq k\)._

A deletion-robust algorithm achieves an \(\alpha\) approximation if, for any \(f\), \(V\), \(k\), and \(d\), the subroutine \(\textsc{Robust1}(f,V,k,d)\) returns \(R\) such that, for any \(D\subseteq V\) such that \(|D|\leq d\), \(\textsc{Robust2}(f,R,D,k)\) returns \(S\) such that \(\mathbf{E}[f(S)]\geq\alpha\cdot\max_{T\subseteq V\setminus D:|T|\leq k}f(T)\). Kazemi et al. [16] show that there is a deletion-robust algorithm for monotone submodular maximization under a cardinality constraint such that Robust1 returns a set \(R\) of size \(|R|=O(\epsilon^{-2}d\log k+k)\). It achieves a \(1/2-\epsilon\) approximation in expectation, Robust1 has \(O(|V|\cdot(k+\epsilon^{-1}\log k))\) query complexity, and Robust2 has \(O\left((\epsilon^{-2}d\log k+k)\cdot\epsilon^{-1}\log k\right)\) query complexity.

**The algorithmic framework.** We present an algorithmic framework that decomposes dynamic algorithms with predictions into two subroutines, Precomputations and UpdateSol. The remainder of the paper then consists of designing and analyzing these subroutines. We first introduce some terminology. An element \(a\) is said to be correctly predicted if \(|\hat{t}_{a}^{+}-t_{a}^{+}|\leq w\) and \(|\hat{t}_{a}^{-}-t_{a}^{-}|\leq w\). The _predicted elements_\(\hat{V}_{t}\) consist of all elements that could potentially be active at time \(t\) if correctly predicted, i.e., the elements \(a\) such that \(\hat{t}_{a}^{+}\leq t+w\) and \(\hat{t}_{a}^{-}\geq t-w\). During the precomputation phase, the first subroutine, Precomputations, takes as input the predicted elements \(\hat{V}_{t}\) and outputs, for each time step \(t\), a data structure \(P_{t}\) that will then be used at time \(t\) of the streaming phase to compute a solution efficiently. During the streaming phase, the active elements \(V_{t}\) are partitioned into the _predicted active elements_\(V_{t}^{1}=V_{t}\cap\hat{V}_{t}\) and the _unpredicted active elements_\(V_{t}^{2}=V_{t}\setminus\hat{V}_{t}\). The second subroutine, UpdateSol, is given \(V_{t}^{1}\) and \(V_{t}^{2}\) as input and computes a solution \(S\subseteq V_{t}^{1}\cup V_{t}^{2}\) at each time step. UpdateSol is also given as input precomputations \(P_{t}\) and the current prediction error \(\eta_{t}\). It also stores useful information for future time steps in a data structure \(A\).

```
1:Input:function \(f:2^{V}\rightarrow\mathbb{R}\), constraint \(k\), predictions \(\{(\hat{t}_{a}^{+},\hat{t}_{a}^{-})\}_{a\in V}\), tolerance \(w\)
2:\(\hat{V}_{t}\leftarrow\{a\in V:\hat{t}_{a}^{+}\leq t+w\text{ and }\hat{t}_{a}^{-}\geq t-w\}\) for \(t\in[n]\)
3:\(\{P_{t}\}_{t=1}^{n}\leftarrow\textsc{Precomputations}(f,\{\hat{V}_{t}\}_{t=1}^{n},k)\)
4:\(V_{0},A\leftarrow\emptyset\)
5:for\(t=1\) to \(n\)do
6: Update active elements \(V_{t}\) according to operation at time \(t\)
7:\(V_{t}^{1}\gets V_{t}\cap\hat{V}_{t}\), \(V_{t}^{2}\gets V_{t}\setminus\hat{V}_{t}\)
8:\(\eta_{t}\leftarrow\) current prediction error
9:\(A,S\leftarrow\textsc{UpdateSol}(f,k,A,t,P_{t},V_{t}^{1},V_{t}^{2},\hat{V}_{t}, \eta_{t})\)
10:return\(S\) ```

**Algorithm 1** The Algorithmic Framework

## 3 The warm-up algorithm

In this section, we present subroutines that achieve an \(\tilde{O}(\eta+w+k)\) amortized update time and a \(1/4-\epsilon\) approximation in expectation. These warm-up subroutines assume that the error \(\eta\) is known. They take as input a dynamic algorithm (without predictions) Dynamic and a deletion-robust algorithm Robust algorithm. The proofs are all deferred to the appendix.

The precomputations subroutineA main observation is that the problem of finding a solution \(S^{1}\subseteq V_{t}^{1}\) among the predicted active elements corresponds to a deletion-robust problem over \(\hat{V}_{t}\) where the deleted elements \(D\) are the predicted elements \(\hat{V}_{t}\setminus V_{t}\) that are not active at time \(t\). WarmUp-Precomputations thus calls, for each time \(t\), the first stage Robust1 of Robust,

\[\textsc{WarmUp-Precomputations}(f,\{\hat{V}_{t}\}_{t=1}^{n},k)=\{\textsc{ Robust1}(f,\hat{V}_{t},k,d=\eta+2w)\}_{t=1}^{n}.\]The algorithm sets the maximum number of deletions parameter \(d\) for Robust1 to \(\eta+2w\) because the number of predicted elements \(\hat{V}_{t}\setminus V_{t}\) that are not active at time \(t\) is at most \(\eta+2w\) (Lemma 8).

The updatesol subroutineWarmUp-UpdateSol finds a solution \(S^{1}\subseteq V_{t}^{1}\) by calling Robust2 over the precomputed \(P_{t}\) and deleted elements \(D=\hat{V}_{t}\setminus V_{t}\). To find a solution \(S^{2}\subseteq V_{t}^{2}\) among the unpredicted active elements, we use Dynamic over the stream of element insertions and deletions that result in unpredicted active elements \(V_{1}^{2},\ldots,V_{n}^{2}\), which is the stream that inserts elements \(V_{t}^{2}\setminus V_{t-1}^{2}\) and deletes elements \(V_{t-1}^{2}\setminus V_{t}^{2}\) at time \(t\). The solution \(S^{2}\) is then the solution produced by DynamicSol over this stream. The solution \(S\) returned by UpdateSol is the best solution between \(S^{1}\) and \(S^{2}\).

```
0:function \(f\), constraint \(k\), data structure \(A\), time \(t\), precomputations \(P_{t}\), predicted active elements \(V_{t}^{1}\), unpredicted active elements \(V_{t}^{2}\), predicted elements \(\hat{V}_{t}\)
1:\(S^{1}\leftarrow\textsc{Robust2}(f,P_{t},\hat{V}_{t}\setminus V_{t}^{1},k)\)
2:if\(t=1\)then\(A\leftarrow\textsc{DynamicInit}(f,k)\)
3:for\(a\in V_{t}^{2}\setminus V(A)\)do\(\textsc{DynamicIns}(A,a)\)
4:for\(a\in V(A)\setminus V_{t}^{2}\)do\(\textsc{DynamicDel}(A,a)\)
5:\(S^{2}\leftarrow\textsc{DynamicSol}(A)\)
6:return\(A,\operatorname{argmax}\{f(S^{1}),f(S^{2})\}\) ```

**Algorithm 2**WarmUp-UpdateSol

The analysis of the warm-up algorithmWe first analyze the approximation. We let \(\alpha_{1}\) and \(\alpha_{2}\) denote the approximations achieved by Robust and Dynamic. The first lemma shows that solution \(S^{1}\) is an \(\alpha_{1}\) approximation to the optimal solution over the predicted active elements \(V_{t}^{1}\) and that solution \(S^{2}\) is an \(\alpha_{2}\) approximation to the optimal solution over the unpredicted active elements \(V_{t}^{2}\).

**Lemma 1**.: _At every time step \(t\), \(\mathbf{E}[f(S^{1})]\geq\alpha_{1}\cdot\textsc{OPT}(V_{t}^{1})\) and \(\mathbf{E}[f(S^{2})]\geq\alpha_{2}\cdot\textsc{OPT}(V_{t}^{2})\)._

The main lemma for the amortized query complexity bounds the number of calls to DynamicIns.

**Lemma 2**.: WarmUp-UpdateSol _makes at most \(2\eta\) calls to DynamicIns on \(A\) over the stream._

The main result for the warm-up algorithm is the following.

**Theorem 1**.: _For monotone submodular maximization under a cardinality constraint \(k\), Algorithm 1 with the WarmUp-Precomputations and WarmUp-UpdateSol subroutines achieves, for any tolerance \(w\) and constant \(\epsilon>0\), an amortized expected query complexity per update during the streaming phase of \(\tilde{O}(\eta+w+k)\), an approximation of \(1/4-\epsilon\) in expectation, and a query complexity of \(\tilde{O}(n^{2}k)\) during the precomputation phase._

In the next sections, we improve the dependencies on \(\eta,w,\) and \(k\) for the query complexity per update from linear to logarithmic, the approximation from \(1/4\) to \(1/2\), and the precomputations query complexity from \(O(n^{2}k)\) to \(\tilde{O}(n)\). We also remove the assumption that the prediction error is known.

## 4 The UpdateSol subroutine

In this section, we improve the dependencies in \(\eta,w,\) and \(k\) for the amortized query complexity from linear to logarithmic, which is the main technical challenge. For finding a solution over the predicted active elements \(V_{t}^{1}\), the main idea is to not only use precomputations \(P_{t},\) but also to exploit computations from previous time steps \(t^{\prime}<t\) over the previous predicted active elements \(V_{t^{\prime}}^{1}\). As in the warm-up subroutine, the new UpdateSolMain subroutine also uses a precomputed deletion-robust solution \(P_{t}\), but it requires \(P_{t}\) to satisfy a property termed the _strongly robust property_ (Definition 4 below), which is stronger than the deletion-robust property of Definition 3. A strongly robust solution comprises two components \(Q\) and \(R\), where \(R\) is a small set of elements that have a high marginal contribution to \(Q\). The set \(Q\) is such that, for any deleted set \(D\), \(f(Q\setminus D)\) is guaranteed to, in expectation over the randomization of \(Q\), retain a large amount of \(f(Q)\).

**Definition 4**.: _A pair of sets \((Q,R)\) is \((d,\epsilon,\gamma)\)-strongly robust, where \(d,k,\gamma\geq 0\) and \(\epsilon\in[0,1]\), if__._
* _Size._ \(|Q|\leq k\) _and_ \(|R|=O(\epsilon^{-2}(d+k)\log k)\) _with probability_ \(1\)_,_
* _Value._ \(f(Q)\geq|Q|\gamma/(2k)\) _with probability_ \(1\)_. In addition, if_ \(|Q|<k\)_, then for any set_ \(S\subseteq V\setminus R\) _we have_ \(f_{Q}(S)<|S|\gamma/(2k)+\epsilon\gamma\)_._
* _Robustness._ _For any_ \(D\subseteq V\) _s.t._ \(|D|\leq d\)_,_ \(\mathbf{E}_{Q}[f(Q\setminus D)]\geq(1-\epsilon)f(Q)\)_._

The set \(P\) returned by the first stage Robust1\((f,V,k,d)\) of the deletion-robust algorithm of Kazemi et al. [16] can be decomposed into two sets \(Q\) and \(R\) that are, for any \(d,\epsilon>0\) and with \(\gamma=\mathtt{OPT}(V)\), where \(\mathtt{OPT}(V):=\max_{S\subseteq V:|S|\leq k}f(S)\), \((d,\epsilon,\gamma)\)-strongly robust.3 Thus, with the Robust algorithm of [16], the set \(P_{t}\) returned by WarmUp-Precomputations can be decomposed into \(P_{t}\) and \(Q_{t}\) that are, for any \(\epsilon>0\), \((2(\eta+2w),\epsilon,\mathtt{OPT}(\hat{V}_{t}))\)-strongly robust. We omit the proof of the \((d,\epsilon,\gamma)\)-strongly robust property of Robust1 from [16] and, in the next section, we instead prove strong-robustness for our PrecomputationsMain subroutine which has better overall query complexity than [16].

Footnote 3: The size of \(R\) in [16] is \(O(d\log k/\epsilon)\) which is better than what is required to be strongly robust.

The UpdateSolMain subroutine proceeds in phases. During each phase, UpdateSolMain maintains a data structure \((B,A,\eta_{\text{old}})\). The set \(B=Q_{t^{\prime}}\) is a fixed base set chosen during the first time step \(t^{\prime}\) of the current phase. \(A\) is a dynamic data structure used by a dynamic submodular maximization algorithm Dynamic that initializes \(A\) over function \(f_{B}\), cardinality constraint \(k-|B|\), and a parameter \(\gamma\) to be later discussed. If a new phase starts at time \(t\), note that if \((Q_{t},R_{t})\) are strongly robust, then the only predicted active elements \(V_{t}^{1}\) that are needed to find a good solution at time \(t\) are, in addition to \(B=Q_{t}\), the small set of elements \(R_{t}\) that are also in \(V_{t}^{1}\). Thus to find a good solution for the function \(f_{B}\), \((R_{t}\cap V_{t}^{1})\cup V_{t}^{2}\) are inserted into \(A\). The solution that UpdateSolMain outputs at time step \(t\) are the active elements \(B\cap V_{t}\) that are in the base for the current phase, together with the solution DynamicSol\((A)\) maintained by \(A\).

```
0: function \(f\), data structure \((B,A,\eta_{\text{old}})\), constraint \(k\), precomputations \(P_{t}=(Q_{t},R_{t})\), \(t\), upper bound \(\eta_{t}^{\prime}\) on prediction error, \(V_{t}^{1}\), \(V_{t}^{2}\), \(V_{t-1}\), parameter \(\gamma_{t}\)
1:if\(t=1\) or \(|\text{Ops}^{\star}(A)|>\frac{\eta_{\text{old}}}{2}+w\)then\(\triangleright\) Start a new phase
2:\(B\gets Q_{t}\)
3:\(A\leftarrow\text{DynamicInit}(f_{B},k-|B|,\gamma=\gamma_{t}(k-|B|)/k)\)
4:for\(a\in(R_{t}\cap V_{t}^{1})\cup V_{t}^{2}\)doDynamicIns\((A,a)\)
5:\(\eta_{\text{old}}\leftarrow\eta_{t}^{\prime}\)
6:else\(\triangleright\) Continue the current phase
7:for\(a\in V_{t}\setminus V_{t-1}\)do\(\text{DynamicIns}(A,a)\)
8:for\(a\in(\text{elem}(A\cap V_{t-1})\setminus V_{t}\)do\(\text{DynamicDel}(A,a)\)
9:\(S\leftarrow(B\cup\text{DynamicSol}(A))\cap V_{t}\)
10:return\((B,A,\eta_{\text{old}}),S\) ```

**Algorithm 3**UpdateSolMain

During the next time steps \(t\) of the current phase, if an element \(a\) is inserted into the stream then \(a\) is inserted in \(A\) (independently of the predictions). If an element is deleted from the stream, then if it was in \(A\), it is deleted from \(A\). We define \(\text{Ops}^{\star}(A)\) to be the collection of all insertion and deletion operations to \(A\), excluding the insertions of elements in \((R_{t}\cap V_{t}^{1})\cup V_{t}^{2}\) at the time \(t\) where \(A\) was initialized. The current phase ends when \(\text{Ops}^{\star}(A)\) exceeds \(\eta_{\text{old}}/2+w\). Since the update time of the dynamic algorithm in [20] depends on length of the stream, we upper bound the length of the stream handled by \(A\) during a phase.

The approximationThe parameter \(\gamma_{t}\) corresponds to a guess for \(\mathtt{OPT}_{t}:=\mathtt{OPT}(V_{t})\). In Section 6, we present the UpdateSolFull subroutine which calls UpdateSolMain with different guesses \(\gamma_{t}\). This parameter \(\gamma_{t}\) is needed when initializing Dynamic because our analysis requires that Dynamic satisfies a property that we call threshold-based, which we formally define next.

**Definition 5**.: _A dynamic algorithm Dynamic is threshold-based if, when initialized with threshold parameter \(\gamma\) such that \(\gamma\leq\mathtt{OPT}_{t}\leq(1+\epsilon)\gamma\), a cardinality constraint \(k\), and \(\epsilon>0\), it maintains a data structure \(A_{t}\) and solution \(\mathtt{SOL}_{t}=\text{DynamicSol}(A_{t})\) that satisfy, for all \(t\), \(f(\mathtt{SOL}_{t})\geq\frac{\gamma}{2k}|\mathtt{SOL}_{t}|\) and, if \(|\mathtt{SOL}_{t}|<(1-\epsilon)k\), then for any set \(S\subseteq V(A_{t})\), we have \(f_{\mathtt{SOL}_{t}}(S)<\frac{|S|\gamma}{2k}+\epsilon\gamma\)._

**Lemma 3**.: _The Dynamic algorithm of Lattanzi et al. [20]4 is threshold-based._

Footnote 4: Note that the initially published algorithm in [20] had an issue with correctness, we refer to the revised version.

The main lemma for the approximation guarantee is the following.

**Lemma 4**.: _Consider the data structure \((B,A,\eta_{\text{old}})\) returned by UpdateSolMain at time \(t\). Let \(t^{\prime}\) be the time at which \(A\) was initialized, \((Q_{t^{\prime}},R_{t^{\prime}})\) and \(\gamma_{t^{\prime}}\) be the precomputations and guess for \(\partial\texttt{PT}_{t^{\prime}}\) inputs to UpdateSolMain at time \(t^{\prime}\). If \((Q_{t^{\prime}},R_{t^{\prime}})\) are \((d=2(\eta_{\text{old}}+2w),\epsilon,\gamma_{t^{\prime}})\)-strongly robust, \(\gamma_{t^{\prime}}\) is such that \(\gamma_{t^{\prime}}\leq\partial\texttt{PT}_{t}\leq(1+\epsilon)\gamma_{t^{ \prime}}\), and Dynamic is a threshold-based dynamic algorithm, then the set \(S\) returned by UpdateSolMain is such that \(\mathbf{E}[f(S)]\geq\frac{1-5\epsilon}{2}\gamma_{t^{\prime}}\)._

The update timeWe next analyze the query complexity of UpdateSolMain. Recall that \(u(n,k)\) denotes the amortized query complexity per update of Dynamic.

**Lemma 5**.: _Consider the data structure \((B,A,\eta_{\text{old}})\) returned by UpdateSolMain at time \(t\). Let \(t^{\prime}\) be the time at which \(A\) was initialized and \((Q_{t^{\prime}},R_{t^{\prime}})\) be the precomputations input to UpdateSolMain at time \(t^{\prime}\). If precomputations \((Q_{t^{\prime}},R_{t^{\prime}})\) are \((d=2(\eta_{\text{old}}+2w),\epsilon,\gamma)\) strongly-robust, then the total number of queries performed by UpdateSol during the \(t-t^{\prime}\) time steps between time \(t^{\prime}\) and time \(t\) is \(O(u((\eta_{\text{old}}+w+k)\log k,k)\cdot(\eta_{\text{old}}+w+k)\log k)\). Additionally, the number of queries between time \(1\) and \(t\) is upper bounded by \(O(u(t,k)\cdot t)\)._

## 5 The Precomputations subroutine

In this section, we provide a Precomputations subroutine that has an improved query complexity compared to the warm-up precomputations subroutine. Recall that the warm-up subroutine computes a robust solution over predicted elements \(\hat{V}_{t}\), independently for all times \(t\). The improved Precomputations does not do this independently for each time step. Instead, it relies on the following lemma that shows that the data structure maintained by the dynamic algorithm of [20] can be used to find a strongly robust solution without any additional query.

**Lemma 6**.: _Let \(\textsc{Dynamic}(\gamma,\epsilon)\) be the dynamic submodular maximization algorithm of [20] and \(A\) be the data structure it maintains. There is a Robust1FromDynamic algorithm such that, given as input a deletion size parameter \(d\), and the data structure \(A\) at time \(t\) with \(\gamma\leq\partial\texttt{PT}_{t}\leq(1+\epsilon)\gamma\), it outputs sets \((Q,R)\) that are \((d,\epsilon,\gamma)\)-strongly robust with respect to the ground set \(V_{t}\). Moreover, this algorithm does not perform any oracle queries._

The improved PrecomputationsMain subroutine runs the dynamic algorithm of [20] and then, using the RobustFromDynamic algorithm of Lemma 6, computes a strongly-robust set from the data structure maintained by the dynamic algorithm.

```
1:function \(f:2^{V}\rightarrow\mathbb{R}\), constraint \(k\), predicted elements \(\hat{V}_{1},\ldots,\hat{V}_{n}\subseteq V\), time error tolerance \(w\), parameter \(\gamma\), parameter \(h\)
2:\(\hat{A}\leftarrow\textsc{DynamicInit}(f,k,\gamma)\)
3:for\(t=1\) to \(n\)do
4:for\(a\in\hat{V}_{t}\setminus\hat{V}_{t-1}\)do\(\textsc{DynamicIs}(\hat{A},a)\)
5:for\(a\in V(\hat{A})\setminus\hat{V}_{t}\)do\(\textsc{DynamicDel}(\hat{A},a)\)
6:if\(|V(\hat{A})|>0\)then\(Q_{t},R_{t}\leftarrow\textsc{Robust1FromDynamic}(f,\hat{A},k,2(h+2w))\)
7:return\(\{(Q_{t},R_{t})\}_{t=1}^{n}\) ```

**Algorithm 4**PrecomputationsMain

The parameters \(\gamma\) and \(h\) correspond to guesses for \(\texttt{OPT}\) and \(\eta\) respectively.

**Lemma 7**.: _The total query complexity of the PrecomputationsMain algorithm is \(n\cdot u(n,k)\), where \(u(n,k)\) is the amortized query complexity of calls to Dynamic._The full algorithm

The UpdateSolMain and PrecomputationsMain subroutines use guesses \(\gamma_{t}\) and \(h\) for the optimal value \(\texttt{OPT}_{t}\) at time \(t\) and the total prediction error \(\eta\). In Appendix D, we describe the full UpdateSolFull and PrecomputationsFull subroutines that call the previously defined subroutines over different guesses \(\gamma_{t}\) and \(h\). The parameters of these calls must be carefully designed to bound the streaming amortized query complexity per update and precomputations query complexity. By combining the algorithmic framework (Algorithm 1) together with subroutines UpdateSolFull and PrecomputationsFull, we obtain our main result.

**Theorem 2**.: _Algorithm 1 with subroutines UpdateSolFull and PrecomputationsFull is a dynamic algorithm that, for any tolerance \(w\) and constant \(\epsilon>0\), achieves an amortized expected query complexity per update during the streaming phase of \(O(\operatorname{poly}(\log\eta,\log w,\log k))\), an approximation of \(1/2-\epsilon\) in expectation, and a query complexity5 of \(\bar{O}(n)\) during the precomputation phase._

Footnote 5: We note that, despite the amortized query complexity of Lattanzi et al. [20] being in expectation, the asymptotic bound on the precomputation query complexity can hold deterministically, instead of in expectation, by forcing PrecomputationsFull to terminate if it has performed a number of queries that is larger than \(\epsilon^{-1}\) times its expected number of queries (note that the precomputation query complexity only depends on known parameters, \(n\) and \(k\)). By Markovâ€™s inequality, such an early termination happens with probability at most \(\epsilon\). Thus, even with no guarantees on the approximation achieved in these early termination cases, the loss in the expected approximation caused by this forced termination is at most \(1-\epsilon\).

Note that the query complexity per update during the streaming phase and the query complexity during the precomputation phase both have a polynomial dependence on \(\epsilon\). Additionally, note that our update bound is not constant even when the prediction error is \(0\). However, with the following simple change, our algorithm achieves a constant update time when the predictions are exactly correct, while also maintaining its current guarantees: (1) as additional precomputations, also compute a predicted solution \(S_{t}\) for each time \(t\) assuming the predictions are exactly correct, (2) during the streaming phase, as long as the predictions are exactly correct, return the precomputed predicted solution \(S_{t}\). At the first time step where the predictions are no longer exactly correct, switch to our main algorithm in the paper.

## 7 Experiments

### Experimental Setup

Benchmarks.We compare our **DynamicWPred** algorithm to two benchmarks. The first is the **Dynamic** submodular maximization algorithm of [20], which does not use predictions. The second is the **OfflineGreedy** algorithm that computes an offline greedy solution \(\hat{S}_{t}\) at each time step \(t\) based on the set \(\hat{V}_{t}\) of available items in the predicted stream. In the streaming phase, it outputs the solution \(\tilde{S}_{t}\cap V_{t}\) at time \(t\) based on the available items \(V_{t}\). Note that this algorithm does not make any queries other than the queries used for pre-computation. These benchmarks are at two extremes in terms of their reliance on the predictions. DynamicWPred uses the Dynamic algorithm of [20] and the Robust algorithm of [16] as subroutines. We implemented our algorithm and OfflineGreedy in C++, and used the C++ implementation of Dynamic that is provided by [20]. We set \(\epsilon=0.2\) for all algorithms.

Metrics.We report the total number of oracle calls made by each algorithm when processing the actual stream \((\mathbf{a},\mathbf{o})\). This does not include any oracle calls during the pre-computation phase. We also report the average function value \(\frac{1}{n}\sum_{t\in[n]}f(S_{t})\) of the output sets over time steps \(t\in[n]\). Each experiment is repeated \(5\) times and the average values are reported.

Datasets and submodular function.We perform experiments on a subset of the Enron dataset from the SNAP Large Networks Data Collection [21]. We select a set \(V\) of \(200\) nodes from the graph, and consider the subgraph induced by \(V\cup N(V)\) where \(N(V)\) is the set of neighboring nodes of \(V\). This resulted in a subgraph with \(7845\) vertices and \(20033\) edges. The submodular function is the dominating set objective function over the ground set \(V\) with \(|V|=200\). Specifically, for a subset of nodes \(S\subseteq V\), we define \(f(S)=|N(S)\cup S|\), where \(N(S)\) is the set of neighboring nodes of \(S\). This function is monotone and submodular.

Generation of true stream.We consider the sliding window protocol from [20] in order to generate the dynamic stream of insertions and deletions. Specifically, we process the nodes \(V\) in an arbitrary order and consider a sliding window of size \(l\). When the window reaches a node \(a\), we add the operation \((a,\text{insert})\) to the dynamic stream. Similarly, after \(l\) steps when the node \(a\) leaves the window, we add the operation \((a,\text{delete})\) to the dynamic stream. Since, \(|V|=200\), we have \(n=400\) for all experiments. We report results with \(l=50\) (observations were similar for other values of \(l\)).

Generation of predicted stream.Given a target prediction error \(\eta\) and window size \(w\), we generate the predicted stream for our experiments by adding perturbations to the actual stream such that Definition 2 is satisfied. Note that we add these perturbations while maintaining the consistency of the stream, i.e. the insertion of an element always happens before its deletion. In particular, we first select a set \(E\) of \(\eta/2\) elements uniformly at random and let \(\mathcal{T}\) denote the set of all insertion and deletion times of these elements in the actual stream. For each \(e\in E\), we assign new insertion and deletion times in the predicted stream by randomly drawing from \(\mathcal{T}\). We make sure that these new insertion and deletion times are consistent and are at least a distance of \(w\) from the corresponding old times. The insertion and deletion times for \(e\not\in E\) remain the same so far. Now, for each \(e,e^{\prime}\notin E\), we randomly swap an their operations \((e,o)\) and \((e^{\prime},o^{\prime})\) that are within a distance of \(w\) while maintaining consistency.

### Experiment Results

**Experiment Set 1.** We first consider the effect of the prediction error \(\eta\) on the function value. For ease of exposition, we overload the notation and report the fractional prediction error, i.e. prediction error divided by the length of the stream \(n\). From the first column of Figure 1, we observe that our algorithm outperforms OfflineGreedy in terms of function value when the prediction error is reasonably large, and always achieves a similar function value as Dynamic. Since Dynamic does not use the predictions, its performance remains constant as a function of the prediction errors. The performance of OfflineGreedy deteriorates quickly as a function of \(\eta\) as it completely relies on the predictions. Note that the function value achieved by OfflineGreedy is not zero even in the case of large prediction error. This is because the error is not adversarial and some elements from its offline solution remain active during the streaming phase.

We also consider the effect of \(\eta\) on the number of oracle calls. Figure 1 shows that the number of oracle calls of our algorithm is much better than Dynamic in the case of low prediction error, and is also not much worse in the case of large prediction error. This shows that our algorithm has consistency in the case of low \(\eta\), but also robustness in the case of large \(\eta\). The number of oracle calls of OfflineGreedy is very small as it completely relies on the prediction.

Figure 1: The number of queries and function value of our algorithm, DynamicWPred, and the two benchmarks for the Enron data and sliding window stream with \(l=50\) as a function of the prediction error \(\eta\) with \(k=10\) and \(w=0\) (column 1), as a function of the cardinality parameter \(k\) with \(\eta=0.25\) and \(w=0\) (column 2), and as a function of the window size parameter \(w\) with \(k=20\) and \(\eta=0.125\) (column 3).

**Experiment Set 2.** We also consider the effect of cardinality parameter \(k\) on the function value and number of oracle calls. It can be observed from the second column of Figure 1 that the function value increases for all algorithms as a function of \(k\). Moreover, the rate of increase for the number of oracle calls made by our algorithm is similar to the rate of increase of Dynamic. **Experiment Set 3.** We consider the effect of the window size parameter \(w\) on the function value and number of oracle calls. Figure 1 shows that the window size has almost no impact on the function value of our algorithm. The number of oracle calls for our algorithm grows as a function of the window size but this growth is very small.

## 8 Limitations

To obtain an asymptotic improvement over the best-known amortized query complexity per update, the prediction error \(\eta\) needs to be subpolynomial in the length of the stream \(n\), which is relatively small. However, our experimental results show that in practice the number of queries performed by our algorithm outperforms the number of queries of existing algorithm without predictions even when the prediction error is relatively large. Another limitation is the assumption that the algorithm is given all predictions at time \(t=0\). An intriguing open question is whether an improvement in update time can also be obtained in the predicted-deletion model where there is no preprocessing and instead a predicted deletion time for each element is given at the time when it is inserted.

## References

* [1] Priyank Agrawal, Eric Balkanski, Vasilis Gkatzelis, Tingting Ou, and Xizhi Tan. Learning-augmented mechanism design: Leveraging predictions for facility location. _EC_, 2022.
* [2] Kareem Amin, Travis Dick, Mikhail Khodak, and Sergei Vassilvitskii. Private algorithms with private predictions. _arXiv preprint arXiv:2210.11222_, 2022.
* [3] Antonios Antoniadis, Themis Gouleakis, Pieter Kleer, and Pavel Kolev. Secretary and online matching problems with machine learned advice. _NeurIPS_, 2020.
* [4] Etienne Bamas, Andreas Maggiori, and Ola Svensson. The primal-dual method for learning augmented algorithms. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* [5] Siddhartha Banerjee, Vasilis Gkatzelis, Artur Gorokh, and Billy Jin. Online nash social welfare maximization with predictions. In _Proceedings of the 2022 ACM-SIAM Symposium on Discrete Algorithms, SODA 2022_. SIAM, 2022.
* [6] Kiaarash Banihashem, Leyla Biabani, Samira Goudarzi, MohammadTaghi Hajiaghayi, Peyman Jabbarzade, and Morteza Monemizadeh. Dynamic constrained submodular optimization with polylogarithmic update time. _ICML_, 2023.
* [7] Kiaarash Banihashem, Leyla Biabani, Samira Goudarzi, MohammadTaghi Hajiaghayi, Peyman Jabbarzade, and Morteza Monemizadeh. Dynamic algorithms for matroid submodular maximization. In _Proceedings of the 2024 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 3485-3533. SIAM, 2024.
* [8] Jeff Bilmes. Submodularity in machine learning and artificial intelligence. _arXiv preprint arXiv:2202.00132_, 2022.
* [9] Jan van den Brand, Sebastian Forster, Yasamin Nazari, and Adam Polak. On dynamic graph algorithms with predictions. In _Proceedings of the 2024 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 3534-3557. SIAM, 2024.
* [10] Xi Chen and Binghui Peng. On the complexity of dynamic submodular maximization. In _Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing_, pages 1685-1698, 2022.

* [11] Paul Duetting, Federico Fusco, Silvio Lattanzi, Ashkan Norouzi-Fard, and Morteza Zadimoghaddam. Fully dynamic submodular maximization over matroids. In _Proceedings of the 40th International Conference on Machine Learning_, pages 8821-8835, 2023.
* [12] Paul Dutting, Silvio Lattanzi, Renato Paes Leme, and Sergei Vassilvitskii. Secretaries with advice. In _Proceedings of the 22nd ACM Conference on Economics and Computation_, pages 409-429, 2021.
* [13] Moran Feldman, Paul Liu, Ashkan Norouzi-Fard, Ola Svensson, and Rico Zenklusen. Streaming submodular maximization under matroid constraints. In _49th International Colloquium on Automata, Languages, and Programming (ICALP 2022)_. Schloss Dagstuhl-Leibniz-Zentrum fur Informatik, 2022.
* [14] Monika Henzinger, Barna Saha, Martin P Seybold, and Christopher Ye. On the complexity of algorithms with predictions for dynamic graph problems. In _15th Innovations in Theoretical Computer Science Conference (ITCS 2024)_. Schloss-Dagstuhl-Leibniz Zentrum fur Informatik, 2024.
* [15] Sungjin Im, Ravi Kumar, Mahshid Montazer Qaem, and Manish Purohit. Online knapsack with frequency predictions. _Advances in Neural Information Processing Systems_, 34, 2021.
* [16] Ehsan Kazemi, Morteza Zadimoghaddam, and Amin Karbasi. Scalable deletion-robust submodular maximization: Data summarization with privacy and fairness constraints. In Jennifer G. Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018_, volume 80 of _Proceedings of Machine Learning Research_, pages 2549-2558. PMLR, 2018.
* [17] David Kempe, Jon Kleinberg, and Eva Tardos. Maximizing the spread of influence through a social network. In _KDD_, 2003.
* [18] Andreas Krause and Daniel Golovin. Submodular function maximization. _Tractability_, 3:71-104, 2014.
* [19] Silvio Lattanzi, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Online scheduling via learned weights. In _Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms_, pages 1859-1877. SIAM, 2020.
* [20] Silvio Lattanzi, Slobodan Mitrovic, Ashkan Norouzi-Fard, Jakub M Tarnawski, and Morteza Zadimoghaddam. Fully dynamic algorithm for constrained submodular optimization. _Advances in Neural Information Processing Systems_, 33:12923-12933, 2020.
* [21] Jure Leskovec and Andrej Krevl. SNAP Datasets: Stanford large network dataset collection. http://snap.stanford.edu/data, June 2014.
* [22] Quanquan C Liu and Vaidehi Srinivas. The predicted-deletion dynamic model: Taking advantage of ml predictions, for free. _arXiv preprint arXiv:2307.08890_, 2023.
* [23] Thodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned advice. _J. ACM_, 68(4):24:1-24:25, 2021.
* [24] Michael Mitzenmacher. Scheduling with predictions and the price of misprediction. In _11th Innovations in Theoretical Computer Science Conference (ITCS 2020)_. Schloss Dagstuhl-Leibniz-Zentrum fur Informatik, 2020.
* [25] Morteza Monemizadeh. Dynamic submodular maximization. In _Advances in Neural Information Processing Systems_, 2020.
* [26] George L Nemhauser and Laurence A Wolsey. Best algorithms for approximating the maximum of a submodular set function. _Mathematics of operations research_, 3(3):177-188, 1978.
* [27] George L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. An analysis of approximations for maximizing submodular set functions i. _Mathematical programming_, 14(1):265-294, 1978.

* [28] Binghui Peng. Dynamic influence maximization. _Advances in Neural Information Processing Systems_, 34:10718-10731, 2021.
* [29] Binghui Peng and Aviad Rubinstein. Fully-dynamic-to-incremental reductions with known deletion order (eg sliding window). In _Symposium on Simplicity in Algorithms (SOSA)_, pages 261-271. SIAM, 2023.
* [30] Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ML predictions. In _Advances in Neural Information Processing Systems_, pages 9661-9670, 2018.
* [31] Yaron Singer. Budget feasible mechanisms. In _2010 IEEE 51st Annual Symposium on foundations of computer science_, pages 765-774. IEEE, 2010.
* [32] Shufan Wang, Jian Li, and Shiqiang Wang. Online algorithms for multi-shop ski rental with machine learned advice. _Advances in Neural Information Processing Systems_, 33, 2020.
* [33] Chenyang Xu and Pinyan Lu. Mechanism design with predictions. In Lud De Raedt, editor, _Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22_, pages 571-577. International Joint Conferences on Artificial Intelligence Organization, 2022.

Missing proofs from Section 3

See 8.

Proof.: Let \(E=\{a\in V:|\hat{t}_{a}^{+}-t_{a}^{+}|>w\text{ or }|\hat{t}_{a}^{-}-t_{a}^{-}|>w\}\). Hence, for all \(a\not\in E\), we have \(|\hat{t}_{a}^{+}-t_{a}^{+}|\leq w\) and \(|\hat{t}_{a}^{-}-t_{a}^{-}|\leq w\). By Definition 2, we have that \(|E|=\eta\). We first note that any \(a\in\hat{V}_{t}\setminus E\) such that \(\hat{t}_{a}^{+}\leq t-w\) and \(\hat{t}_{a}^{-}>t+w\) also belongs to \(V_{t}\). This is because \(a\not\in E\) implies that \(t_{a}^{+}\leq t\) and \(t_{a}^{-}>t\). Hence, the only elements that are present in \(\hat{V}_{t}\setminus E\) but are absent from \(V_{t}\) are such that \(\hat{t}_{a}^{+}>t-w\) or \(\hat{t}_{a}^{-}\leq t+w\). Since, the only elements in \(\hat{V}_{t}\) are such that \(\hat{t}_{a}^{+}\leq t+w\) and \(\hat{t}_{a}^{-}\geq t-w\), this implies that there can be at most \(2w\) such elements. Combining with the fact that \(|E|=\eta\), we get \(|\hat{V}_{t}\setminus V_{t}|\leq|E|+|(\hat{V}_{t}\setminus E)\setminus V_{t}| \leq\eta+2w\).

See 1

Proof.: We first show that at every time step \(t\), \(f(S^{1})\geq\alpha_{1}\cdot\texttt{OPT}(V_{t}^{1})\). Fix some time step \(t\). First, note that

\[P_{t}\setminus(\hat{V}_{t}\setminus V_{t}^{1})=(P_{t}\setminus\hat{V}_{t}) \cup(P_{t}\cap V_{t}^{1})=P_{t}\cap V_{t}^{1}\]

where the second equality is since \(P_{t}\subseteq\hat{V}_{t}\). In addition, we have that \(|\hat{V}_{t}\setminus V_{t}^{1}|=|\hat{V}_{t}\setminus V_{t}|\leq\eta+2w=d\), where the inequality is by Lemma 8. With \(D=\hat{V}_{t}\setminus V_{t}^{1}\) such that \(|D|\leq d\), \(R=P_{t}\), and \(N=\hat{V}_{t}\), we thus have by Definition 3 that the output \(S^{1}\) of Robust2\((f,P_{t},\hat{V}_{t}\setminus V_{t}^{1},k)=\) Robust2\((f,R,D,k)\) is such that \(S^{1}\subseteq P_{t}\cap V_{t}^{1}\) and

\[\mathbf{E}[f(S^{1})]\geq\alpha_{1}\cdot\max_{\begin{subarray}{c}S\subseteq \hat{V}_{t}\setminus D:\\ |S|\leq k\end{subarray}}f(S)=\alpha_{1}\cdot\max_{\begin{subarray}{c}S\subseteq V _{t}^{1}:\\ |S|\leq k\end{subarray}}f(S)=\alpha_{1}\cdot\texttt{OPT}(V_{t}^{1}).\]

Next, we show that at every time step \(t\), \(f(S^{2})\geq\alpha_{2}\cdot\texttt{OPT}(V_{t}^{2})\). Observe that the algorithm runs the dynamic streaming algorithm Dynamic over the sequence of ground sets \(V_{0}^{2},\ldots,V_{n}^{2}\). By Definition 1, the solutions returned by Dynamic at each time step achieve an \(\alpha_{2}\) approximation. Since these solutions are \(S_{1}^{2},\ldots,S_{n}^{2}\), we get that for every time step \(t\), \(f(S_{t}^{2})\geq\alpha_{2}\max_{S\subseteq V_{t}^{2}:|S|\leq k}f(S)\). 

See 9

Proof.: Let \(O=\{o_{1},\ldots,o_{k}\}=\operatorname*{argmax}_{S\subseteq N_{1}\cup N_{2}:| S|\leq k}f(S)\) be an arbitrary ordering of the optimal elements. We have that

\[\max_{S\subseteq N_{1}\cup N_{2}:|S|\leq k}f(S) =f(O)\] \[=\sum_{i=1}^{k}f_{\{o_{1},\cdots,o_{i-1}\}}(o_{i})\] \[\leq\sum_{i=1}^{k}\mathds{1}[o_{i}\in N_{1}]f_{\{o_{1},\cdots,o_{i -1}\}\cap N_{1}}(o_{i})+\sum_{i=1}^{k}\mathds{1}[o_{i}\in N_{2}]f_{\{o_{1}, \cdots,o_{i-1}\}\cap N_{2}}(o_{i})\] \[=f(O\cap N_{1})+f(O\cap N_{2})\] \[\leq\max_{S\subseteq N_{1}:|S|\leq k}f(S)+\max_{S\subseteq N_{2}: |S|\leq k}f(S)\] \[\leq f(S_{1})/\alpha_{1}+f(S_{2})/\alpha_{2}\] \[\leq 2\max\{f(S_{1}),f(S_{2})\}/\min\{\alpha_{1},\alpha_{2}\}\]

where the first inequality is by submodularity.

By combining Lemma 1 and Lemma 9, we obtain the main lemma for the approximation guarantee.

**Lemma 10**.: _At every time step \(t\), \(\max\{f(S^{1}),f(S^{2})\}\geq\frac{1}{2}\cdot\min\{\alpha_{2},\alpha_{1}\}\cdot \texttt{OPT}(V_{t})\)._

**Lemma 2**.: WarmUp-UpdateSol _makes at most \(2\eta\) calls to DynamicIns on \(A\) over the stream._

Proof.: The number of calls to DynamicIns is \(|\{a:\exists t\text{ s.t. }a\in V_{t}^{2}\setminus V_{t-1}^{2}\}|\). Since \(V_{t}^{2}\setminus V_{t-1}^{2}=(V_{t}\setminus\hat{V}_{t})\setminus(V_{t-1} \setminus\hat{V}_{t-1}^{1})\),

\[|\{a:\exists t\text{ s.t. }a\in V_{t}^{2}\setminus V_{t-1}^{2}\}| \leq|\{a:\exists t\text{ s.t. }a\in(V_{t}\setminus V_{t-1})\setminus\hat{V}_{t}\}|\] \[\qquad+|\{a:\exists t\text{ s.t. }a\in(\hat{V}_{t-1}\setminus\hat{V}_{t}) \cap V_{t}\}|.\]

Next, we have

\[|\{a:\exists t\text{ s.t. }a\in(V_{t}\setminus V_{t-1}) \setminus\hat{V}_{t}\}| =|\{a:a\not\in\hat{V}_{t^{+}_{a}}\}|\] \[=|\{a:\hat{t}^{+}_{a}>t^{+}_{a}+w\:\text{or}\:\hat{t}^{-}_{a}<t^ {+}_{a}-w\}|\] \[\leq|\{a:\hat{t}^{+}_{a}>t^{+}_{a}+w\:\text{or}\:\hat{t}^{-}_{a}< t^{-}_{a}-w\}|\] \[\leq|\{a:|\hat{t}^{+}_{a}-t^{+}_{a}|>w\:\text{or}\:|\hat{t}^{-}_{ a}-t^{-}_{a}|>w\}|\] \[\leq\eta\]

Similarly,

\[|\{a:\exists t\text{ s.t. }a\in(\hat{V}_{t-1}\setminus\hat{V}_{t}) \cap V_{t}\}| =|\{a:a\in V_{\hat{t}^{-}_{a}+w}\}|\] \[=|\{a:t^{-}_{a}\geq\hat{t}^{-}_{a}+w\}|\] \[\leq|\{a:|\hat{t}^{+}_{a}-t^{+}_{a}|>w\:\text{or}\:|\hat{t}^{-}_{ a}-t^{-}_{a}|>w\}|\] \[\leq\eta\]

Combining the above inequalities, we get that the number of calls to DynamicIns is \(|\{a:\exists t\text{ s.t. }a\in V_{t}^{2}\setminus V_{t-1}^{2}\}|\leq 2\eta\). 

**Theorem 1**.: _For monotone submodular maximization under a cardinality constraint \(k\), Algorithm 1 with the WarmUp-Precomputations and WarmUp-UpdateSol subroutines achieves, for any tolerance \(w\) and constant \(\epsilon>0\), an amortized expected query complexity per update during the streaming phase of \(\tilde{O}(\eta+w+k)\), an approximation of \(1/4-\epsilon\) in expectation, and a query complexity of \(\tilde{O}(n^{2}k)\) during the precomputation phase._

Proof.: We choose Dynamic to be the algorithm from [20] and Robust to be the algorithm from [16]. By Lemma 10, and since we have \(\alpha_{1}=\alpha_{2}=1/2-\epsilon\), the approximation is \(\frac{1}{2}\cdot\min\{\alpha_{2},\alpha_{1}\}=\frac{1}{2}\cdot\min\{\frac{1} {2}-\epsilon,\frac{1}{2}-\epsilon\}\geq\frac{1}{4}-\epsilon\).

For the query complexity during the streaming phase, by Lemma 2, the total number of calls to DynamicIns on \(A\) is \(O(\eta)\). The total number of calls to DynamicDel on \(A\) is also \(O(\eta)\) since an element can only be deleted if it has been inserted. Thus, the total number of insertions and deletions handled by the dynamic streaming algorithm Dynamic is \(O(\eta)\). Since the amortized expected query complexity per update of Dynamic is \(O(\text{poly}(\log n,\log k))\), we get that the amortized expected number of queries performed when calling DynamicIns, DynamicDel, and DynamicSol on \(A\) is \(O(\eta\cdot\text{poly}(\log\eta,\log k)/n)\). For the number of queries due to Robust2, at every time step \(t\), the algorithm calls Robust2\((f,P_{t},(\hat{V}_{t}\setminus V^{1}),k)\) with maximum number of deletions \(d=\eta+2w\), which causes at most \(\tilde{O}\left(\eta+w+k\right)\) queries at each time step \(t\) since the query complexity of Robust2 is \(\tilde{O}\left(d+k\right)\). The total amortized expected query complexity per update during the streaming phase is thus \(\tilde{O}(\eta+w+k)\). For the query complexity during the precomputation phase, the query complexity of Robust1 is \(\tilde{O}(|V|k)\) and there are \(n\) calls to Robust1, so the query complexity of that phase is \(\tilde{O}(n^{2}k)\).

Missing proof from Section 4

**Lemma 3**.: _The Dynamic algorithm of Lattanzi et al. [20]6 is threshold-based._

Footnote 6: Note that the initially published algorithm in [20] had an issue with correctness, we refer to the revised version.

Proof.: The first condition that \(f(\texttt{SOL}_{t})\geq\frac{\gamma}{2k}|\texttt{SOL}_{t}|\) follows directly from the fact that each element \(e\) that is added to \(\texttt{SOL}_{t}\) has a marginal contribution of at least \(\gamma/2k\) to the partial solution. Some elements could have been deleted from the partial solution since the time \(e\) was added, but this can only increase the contribution of \(e\) due to submodularity.

We will now show that if \(|\texttt{SOL}_{t}|<(1-\epsilon)k\) then for any \(S\subseteq V(A_{t})\) we have \(f_{\texttt{SOL}_{t}}(S)<\frac{|S|\gamma}{2k}+\epsilon\gamma\). We will use the set \(X\) defined in the proof of Theorem 5.1 in [20]. The proof of Theorem 5.1 in [20] shows that \(|\texttt{SOL}_{t}|\geq(1-\epsilon)|X|\). Using this, the condition that \(|\texttt{SOL}_{t}|<(1-\epsilon)k\) implies that \(|X|<k\). Hence, we fall in the case 2 of the proof of Theorem 5.1 which shows that \(f(e|X)\leq\gamma/2k\) for all \(e\in V_{t}\setminus\texttt{SOL}_{t}\). We then have that

\[f_{\texttt{SOL}_{t}}(S) =f(S\cup\texttt{SOL}_{t})-f(\texttt{SOL}_{t})\leq f(S\cup X)-f( \texttt{SOL}_{t})\] \[=f(S\cup X)-f(X)+f(X)-f(\texttt{SOL}_{t})\] \[\leq f(X\cup S)-f(X)+f(X)-(1-\epsilon)f(X)\] \[\leq\sum_{e\in S}f_{X}(e)+\epsilon f(X)\] \[\leq|S|\cdot\frac{\gamma}{2k}+\epsilon\cdot\frac{1+\epsilon}{1- \epsilon}\cdot\gamma\,,\]

where the inequality \(f(X)\leq\frac{1+\epsilon}{1-\epsilon}\gamma\) follows because \(f(X)\leq f(\texttt{SOL}_{t})/(1-\epsilon)\leq\frac{1+\epsilon}{1-\epsilon}\cdot\gamma\). 

**Lemma 4**.: _Consider the data structure \((B,A,\eta_{\text{old}})\) returned by UpdateSolMain at time \(t\). Let \(t^{\prime}\) be the time at which \(A\) was initialized, \((Q_{t^{\prime}},R_{t^{\prime}})\) and \(\gamma_{t^{\prime}}\) be the precomputations and guess for \(\texttt{OPT}_{t^{\prime}}\) inputs to UpdateSolMain at time \(t^{\prime}\). If \((Q_{t^{\prime}},R_{t^{\prime}})\) are \((d=2(\eta_{\text{old}}+2w),\epsilon,\gamma_{t^{\prime}})\)-strongly robust, \(\gamma_{t^{\prime}}\) is such that \(\gamma_{t^{\prime}}\leq\texttt{OPT}_{t}\leq(1+\epsilon)\gamma_{t^{\prime}}\), and Dynamic is a threshold-based dynamic algorithm, then the set \(S\) returned by UpdateSolMain is such that \(\mathbf{E}[f(S)]\geq\frac{1-5\epsilon}{2}\gamma_{t^{\prime}}\)._

To prove Lemma 4, we first bound the value of \(Q_{t^{\prime}}\cap V_{t}\).

**Lemma 11**.: _Consider the data structure \((B,A,\eta_{\text{old}})\) returned by UpdateSolMain at time \(t\). Let \(t^{\prime}\) be the time at which \(A\) was initialized, \((Q_{t^{\prime}},R_{t^{\prime}})\) and \(\gamma_{t^{\prime}}\) be the precomputations and guess for \(\texttt{OPT}_{t^{\prime}}\) inputs to UpdateSolMain at time \(t^{\prime}\). If precomputations \((Q_{t^{\prime}},R_{t^{\prime}})\) are \((d=2(\eta_{\text{old}}+2w),\epsilon,\gamma_{t^{\prime}})\)-strongly robust, then we have_

\[\mathbf{E}[f(Q_{t^{\prime}}\cap V_{t})]\geq(1-\epsilon)f(Q_{t^{\prime}})\geq( 1-\epsilon)|Q_{t^{\prime}}|\gamma_{t^{\prime}}/(2k).\]

Proof.: We first show that \(|Q_{t^{\prime}}\setminus V_{t}|\leq d\). We know that \(Q_{t^{\prime}}\subseteq\hat{V}_{t^{\prime}}\). Firstly, we have \(|\hat{V}_{t^{\prime}}\setminus V_{t^{\prime}}|\leq\eta_{t}+2w\leq\eta_{\text{ old}}+2w\) due to the Lemma 8. Next, note that the number of insertions and deletions \(\text{Ops}^{\star}(A)\) to \(A\) between time \(t^{\prime}\) and \(t\) is at most \(\frac{\eta_{\text{old}}}{2}+w\), otherwise \(A\) would have been reinitialized due to the condition of UpdateSolMain to start a new phase. This implies that \(|V_{t^{\prime}}\setminus V_{t}|\leq\eta_{\text{old}}+2w\). Hence, we have that

\[|Q_{t^{\prime}}\setminus V_{t}|\leq|\hat{V}_{t^{\prime}}\setminus V_{t}|\leq| \hat{V}_{t^{\prime}}\setminus V_{t^{\prime}}|+|V_{t^{\prime}}\setminus V_{t}| \leq 2(\eta_{\text{old}}+2w)=d.\]

We conclude that \(\mathbf{E}[f(Q_{t^{\prime}}\cap V_{t})]\geq(1-\epsilon)f(Q_{t^{\prime}})\geq( 1-\epsilon)|Q_{t^{\prime}}|\frac{\gamma_{t^{\prime}}}{2k}\), where the first inequality is by the robustness property of strongly-robust precomputations and the second by their value property. 

Proof of Lemma 4.: There are three cases.

1. \(|Q_{t^{\prime}}|\geq k\). We have that \(\mathbf{E}[f(S)]\geq\mathbf{E}[f(Q_{t^{\prime}}\cap V_{t})]\geq(1-\epsilon)|Q _{t^{\prime}}|\frac{\gamma_{t^{\prime}}}{2k}\geq(1-\epsilon)\frac{\gamma_{t^{ \prime}}}{2}\), where the first inequality is by monotonicity and since \(S\supseteq B\cap V_{t}=\hat{Q}_{t^{\prime}}\cap V_{t}\), the second by Lemma 11, and the last by the condition for this first case.

2. \(|\text{DynamicSol}(A)|\geq(1-\epsilon)(k-|Q_{t^{\prime}}|)\). Let \(\mathsf{SOL}_{t}=\text{DynamicSol}(A)\) and recall that \(A\) was initialized with DynamicInit over function \(f_{Q_{t^{\prime}}}\) with cardinality constraint \(k-|B|\) and threshold parameter \(\gamma_{t^{\prime}}(k-|B|)/k\). We get \[\mathbf{E}[f(S)] =\mathbf{E}[f(Q_{t^{\prime}}\cap V_{t})+f_{Q_{t^{\prime}}\cap V_{ t}}(\mathsf{SOL}_{t})] \text{definition of }S\] \[\geq\mathbf{E}[f(Q_{t^{\prime}}\cap V_{t})+f_{Q_{t^{\prime}}}( \mathsf{SOL}_{t})] \text{submodularity}\] \[\geq(1-\epsilon)|Q_{t^{\prime}}|\frac{\gamma_{t^{\prime}}}{2k}+ \mathbf{E}[f_{Q_{t^{\prime}}}(\mathsf{SOL}_{t})] \text{Lemma \ref{lem:dynamic}}\] \[\geq(1-\epsilon)|Q_{t^{\prime}}|\frac{\gamma_{t^{\prime}}}{2k}+ \frac{\gamma_{t^{\prime}}(k-|B|)/k}{2(k-|B|)}|\mathsf{SOL}_{t}| \text{Definition \ref{lem:dynamic}}\] \[\geq(1-\epsilon)|Q_{t^{\prime}}|\frac{\gamma_{t^{\prime}}}{2k}+ \frac{\gamma_{t^{\prime}}}{2k}(1-\epsilon)(k-|Q_{t^{\prime}}|) \text{case assumption}\] \[\geq\frac{(1-\epsilon)\gamma_{t^{\prime}}}{2}.\]
3. \(|Q_{t^{\prime}}|<k\) and \(|\mathsf{SOL}_{t}|<(1-\epsilon)(k-|Q_{t^{\prime}}|)\): Recall that \(R_{t^{\prime}}\subseteq\hat{V}_{t^{\prime}}\). Also, let \(\bar{R}_{t}=(V_{t}\cap R_{t^{\prime}})\cup(V_{t}\setminus\hat{V}_{t^{\prime}})\). In this case, we have that \[f(O_{t})\leq_{(a)} \mathbf{E}[f(O_{t}\cup\mathsf{SOL}_{t}\cup Q_{t^{\prime}})]\] \[=\mathbf{E}[f(\mathsf{SOL}_{t}\cup Q_{t^{\prime}})]+\mathbf{E}[f_ {\mathsf{SOL}_{t}\cup Q_{t^{\prime}}}(O_{t}\setminus\bar{R}_{t})]+\mathbf{E}[ f_{\mathsf{SOL}_{t}\cup Q_{t^{\prime}}\cup(O_{t}\setminus\bar{R}_{t})}(O_{t} \cap\bar{R}_{t})]\] \[\leq_{(b)} \mathbf{E}[f(\mathsf{SOL}_{t}\cup Q_{t^{\prime}})]+\mathbf{E}[f_ {Q_{t^{\prime}}}(O_{t}\setminus\bar{R}_{t})]+\mathbf{E}[f_{\mathsf{SOL}_{t}}( O_{t}\cap\bar{R}_{t})]\] \[\leq_{(c)} \mathbf{E}[f(\mathsf{SOL}_{t}\cup Q_{t^{\prime}})]+|O_{t}\setminus \bar{R}_{t}|\cdot\frac{\gamma_{t^{\prime}}}{2k}+\epsilon\gamma_{t^{\prime}}+ \mathbf{E}[f_{\mathsf{SOL}_{t}}(O_{t}\cap\bar{R}_{t})]\] \[\leq_{(d)} \mathbf{E}[f(\mathsf{SOL}_{t}\cup Q_{t^{\prime}})]+|O_{t}\setminus \bar{R}_{t}|\cdot\frac{\gamma_{t^{\prime}}}{2k}+\epsilon\gamma_{t^{\prime}}+|O _{t}\cap\bar{R}_{t}|\cdot\frac{\gamma_{t^{\prime}}}{2k}+\epsilon\gamma_{t^{ \prime}}\] \[\leq_{(e)} \mathbf{E}[f(\mathsf{SOL}_{t}\cup Q_{t^{\prime}})]+(1+4\epsilon) \frac{\gamma_{t^{\prime}}}{2}.\] where \((a)\) is by monotonicity, \((b)\) is by submodularity, \((c)\) is since \((Q_{t^{\prime}},R_{t^{\prime}})\) are \((d=2(\eta_{\text{old}}+2w),\epsilon,\gamma_{t^{\prime}})\)-strongly robust (value property) and since we have that \(O_{t}\setminus\bar{R}_{t}=O_{t}\setminus((V_{t}\cap R_{t^{\prime}})\cup(V_{t} \setminus\hat{V}_{t^{\prime}}))\subseteq\hat{V}_{t^{\prime}}\setminus R_{t^{ \prime}}\) and \(Q_{t^{\prime}}<k\), \((d)\) is since Dynamic is threshold-based, \(|\mathsf{SOL}_{t}|<(1-\epsilon)(k-|\bar{Q}_{t^{\prime}}|)\), and \(\bar{R}_{t}\subseteq V(A)\), and \((e)\) is since \(|O_{t}|\leq k\). The above series of inequalities implies that \[\mathbf{E}[f(\mathsf{SOL}_{t}\cup Q_{t^{\prime}})]\geq f(O_{t})-\frac{(1+4 \epsilon)\gamma_{t^{\prime}}}{2}=\frac{(1-4\epsilon)\gamma_{t^{\prime}}}{2}.\] We conclude that \[\mathbf{E}[f(S)] =\mathbf{E}[f((Q_{t^{\prime}}\cup\mathsf{SOL}_{t})\cap V_{t})] \text{definition of }S\] \[=\mathbf{E}[f((Q_{t^{\prime}}\cap V_{t})\cup\mathsf{SOL}_{t})] \text{$\qquad\qquad\qquad\qquad\qquad\mathsf{SOL}_{t}\subseteq V_{t}$}\] \[\geq\mathbf{E}[f(Q_{t^{\prime}}\cap V_{t})]+\mathbf{E}[f_{Q_{t^{ \prime}}}(\mathsf{SOL}_{t})] \text{submodularity}\] \[\geq(1-\epsilon)f(Q_{t^{\prime}})+\mathbf{E}[f_{Q_{t^{\prime}}}( \mathsf{SOL}_{t})] \text{Lemma \ref{lem:dynamic}}\] \[\geq(1-\epsilon)\cdot\mathbf{E}[f(\mathsf{SOL}_{t}\cup Q_{t^{ \prime}})]\] \[\geq\frac{(1-5\epsilon)\gamma_{t^{\prime}}}{2}\]

**Lemma 5**.: _Consider the data structure \((B,A,\eta_{\text{old}})\) returned by UpdateSolMain at time \(t\). Let \(t^{\prime}\) be the time at which \(A\) was initialized and \((Q_{t^{\prime}},R_{t^{\prime}})\) be the precomputations input to UpdateSolMain at time \(t^{\prime}\). If precomputations \((Q_{t^{\prime}},R_{t^{\prime}})\) are \((d=2(\eta_{\text{old}}+2w),\epsilon,\gamma)\) strongly-robust, then the total number of queries performed by UpdateSol during the \(t-t^{\prime}\) time steps between time \(t^{\prime}\) and time \(t\) is \(O(u((\eta_{\text{old}}+w+k)\log k,k)\cdot(\eta_{\text{old}}+w+k)\log k)\). Additionally, the number of queries between time \(1\) and \(t\) is upper bounded by \(O(u(t,k)\cdot t)\)._

Proof.: The only queries made by UpdateSol are due to calls to Dynamic. Hence, we calculate the total number of operations \(\text{Ops}(A)\) between time \(t^{\prime}\) and \(t\). The number of insertions at time \(t^{\prime}\) is

\[|R_{t^{\prime}}\cap V_{t^{\prime}}^{1}|+|V_{t^{\prime}}^{2}|=_{(1)} O((\eta_{\text{old}}+w+k)\log k)+|V_{t^{\prime}}^{2}|\] \[\leq_{(2)} O((\eta_{\text{old}}+w+k)\log k)+\eta_{\text{old}}\] \[= O((\eta_{\text{old}}+w+k)\log k)),\]where \((1)\) is by the size property of the \((d=2(\eta_{\text{old}}+2w),\epsilon,\gamma)\) strongly-robust precomputations and \((2)\) is since \(|V_{\ell}^{2}|\leq\eta_{t^{\prime}}\leq\eta_{t^{\prime}}^{\prime}=\eta_{\text{old}}\).

Moreover, the total number of operations \(\text{Ops}^{*}(A)\) between time \(t^{\prime}+1\) and \(t\) is at most \(\eta_{\text{old}}/2+w=d/4\), otherwise \(A\) would have been reinitialized due to the condition of UpdateSolMain to start a new phase. Hence, the total query complexity between time \(t^{\prime}\) and time \(t\) is at most \(u(O((\eta_{\text{old}}+w+k)\log k),k-|Q_{t^{\prime}}|)\cdot O((\eta_{\text{old }}+w+k)\log k)=O(u((\eta_{\text{old}}+w+k)\log k,k)\cdot(\eta_{\text{old}}+w+k )\log k)\).

In the case \(t^{\prime}=1\), the number of operations at time \(t^{\prime}\) is \(1\) since \(|V_{t^{\prime}}|=1\), and the number of operations from time \(2\) to \(t\) is at most \(t-1\). This gives the bound of \(O(u(t,k)\cdot t)\) when \(t^{\prime}=1\). 

## Appendix C Missing proofs from Section 5

See 6

Proof.: We will first set up some notation. The algorithm of [20] creates several buckets \(\{A_{i,\ell}\}_{i\in[R],\ell\in[T]}\) where \(R=O(\log k/\epsilon)\) is the number of thresholds and \(T=O(\log n)\) is the number of levels. A solution is constructed using these buckets by iteratively selecting \(S_{i,\ell}\) from \(A_{i,\ell}\) starting from the smallest \((i,\ell)=(0,0)\) to the largest \((i,\ell)=(R,T)\). The buffer at level \(\ell\) is denoted by \(B_{\ell}\). Each set \(S_{i,\ell}\) is constructed by iteratively selecting elements uniformly at random (without replacement) from the corresponding bucket. More precisely, given indices \(i,\ell\), the algorithm adds elements \(e\) to the solution \(S_{i,\ell}\) one by one in line \(7\) of Algorithm \(6\) in [20] only if \(f(e|S)\geq\tau_{i}\geq\gamma/2k\).

We will now show how to construct \((Q,R)\) from the Dynamic data structure \(A\). Note that we will extract \((Q,R)\) by observing the evolution of the data-structure \(A\). Hence, we do not need to make any additional oracle queries in order to extract \((Q,R)\).

The following algorithm gives a procedure for extracting \((Q,R)\) by observing the actions of the peeling algorithm (Algorithm 6) in [20].

```
1:function \(f:2^{V}\rightarrow\mathbb{R}\), dynamic data-structure \(A\), constraint \(k\), deletion parameter \(d\), parameter \(\epsilon\)
2:\(Q\leftarrow\emptyset\)
3:\(R\leftarrow\emptyset\)
4:for\(\ell\in[T]\)do
5:for\(i\in[R]\)do
6:if\(n/2^{\ell}>\frac{d}{\epsilon}+k\)then
7:\(Q\gets Q\cup S_{i,\ell}\)
8:else
9:\(R\gets R\cup A_{i,\ell}\cup B_{\ell}\)
10:return\(Q,R\) ```

**Algorithm 5** Robust1FromDynamic

We will now show that the conditions required in Definition 4 by the above algorithm.

* **Size.** The fact that \(|Q|\leq k\) follows using the \(Q\subseteq\cup_{i,\ell}S_{i,\ell}=S\) where \(S\) is the solution output by Dynamic with \(|S|\leq k\). We will now show that the size of \(R=O(\frac{\log k}{\varepsilon}\cdot(\frac{d}{\epsilon}+k))\). Let \(\bar{\ell}\in[T]\) be the largest value such that the corresponding bucket size \(n/2^{\ell}\) is at least \(d/\epsilon+k\). Recall that 

[MISSING_PAGE_FAIL:18]

Since, each element in \(S_{i,\ell}\) is selected from a set of size at least \(d/\epsilon\), it can only be deleted with probability at most \(\epsilon\). We have that

\[\mathbf{E}[f(Q\setminus D)] =\sum_{\ell\geq\bar{\ell}}\sum_{i\in[R]}\sum_{j\in[|S_{i,\ell}|]} \mathbf{E}[\mathbf{1}[e_{\ell,i,j}\not\in D]\cdot f_{A^{\text{\tiny{partition}}}_{ \ell,i,j}}(e_{\ell,i,j})]\] \[\geq\sum_{\ell\geq\bar{\ell}}\sum_{i\in[R]}\sum_{j\in[|S_{i,\ell} |]}\mathbf{E}[\mathbf{1}[e_{\ell,i,j}\not\in D]\cdot f_{S^{\text{\tiny{partition}}}_ {\ell,i,j}}(e_{\ell,i,j})]\] \[\geq\sum_{\ell\geq\bar{\ell}}\sum_{i\in[R]}\sum_{j\in[|S_{i,\ell} |]}\Pr(e_{\ell,i,j}\not\in D)\cdot\tau_{i}\] \[\geq(1+\epsilon)\sum_{\ell\geq\bar{\ell}}\sum_{i\in[R]}|S_{i,\ell }|\tau_{i}\] \[\geq\frac{1+\epsilon}{1-\epsilon}f(Q).\qed\]

**Lemma 7**.: _The total query complexity of the PrecomputationsMain algorithm is \(n\cdot u(n,k)\), where \(u(n,k)\) is the amortized query complexity of calls to Dynamic._

Proof.: It is easy to observe that the algorithm makes at most \(n\) calls to DynamicIns or DynamicDel since \(\sum_{t}|\hat{V}_{t}\setminus\hat{V}_{t-1}|+|\hat{V}_{t-1}\setminus\hat{V}_{t}|\leq n\). Hence, the total query complexity due to calls to Dynamic is \(n\cdot u(n,k)\). Moreover, the calls to RobustFromDynamic do not incur any additional queries due to Lemma 6. Hence, the total number of queries performed in the precomputation phase is given by \(n\cdot u(n,k)\). 

## Appendix D Missing proofs from Section 6

The UpdateSolMain and PrecomputationsMain subroutines use guesses \(\gamma_{t}\) and \(h\) for the optimal value \(\texttt{OPT}_{t}\) at time \(t\) and the total prediction error \(\eta\). In this section, we describe the full UpdateSolFull and PrecomputationsFull subroutines that call the previously defined subroutines over different guesses \(\gamma_{t}\) and \(h\). The parameters of these calls must be carefully designed to bound the streaming amortized query complexity per update and precomputations query complexity.

### The full Precomputations subroutine

We define \(H=\{n/2^{i}:i\in\{\log_{2}(\max\{\frac{n}{k-2w},1\}),\cdots,\log_{2}(n)-1, \log_{2}n\}\}\) to be a set of guesses for the prediction error \(\eta\). Since PrecomputationsMain requires a guess \(h\) for \(\eta\), PrecomputationsFull calls PrecomputationsMain over all guesses \(h\in H\). We ensure that the minimum guess \(h\) is such that \(h+2w\) is at least \(k\). The challenge with the guess \(\gamma\) of OPT needed for PrecomputationsMain is that OPT can have arbitrarily large changes between two time steps. Thus, with \(\hat{V}_{1},\ldots,\hat{V}_{n}\subseteq V\), we define the collection of guesses for OPT to be \(\Gamma=\{(1+\epsilon)^{0}\min_{a\in V}f(a),(1+\epsilon)^{1}\min_{a\in V}f(a), \ldots,f(V)\}\), which can be an arbitrarily large set.

Instead of having a bounded number of guesses \(\gamma\), we consider a subset \(\hat{V}_{t}(\gamma)\subseteq\hat{V}_{t}\) of the predicted elements at time \(t\) for each guess \(\gamma\in\Gamma\) such that for each element \(a\) is, there is a bounded number of guesses \(\gamma\in\Gamma\) such that \(a\in\hat{V}_{t}(\gamma)\). More precisely, for any set \(T\), we define \(T(\gamma):=\{a\in T:\frac{\epsilon\gamma}{k}\leq f(a)\leq 2\gamma\}\) and \(\Gamma(a)=\{\gamma\in\Gamma:f(a)\leq\gamma\leq\epsilon^{-1}kf(a)\}\). The PrecomputationsFull subroutine outputs, for every time step \(t\), strongly-robust sets \((Q^{\gamma,h}_{t},R^{\gamma,h}_{t})\), for all guesses \(\gamma\in\Gamma\) and \(h\in H\). If \(\hat{V}_{t}(\gamma)=\emptyset\), we assume that \(Q^{\gamma,h}_{t}=\emptyset\) and \(R^{\gamma,h}_{t}=\emptyset\).

**Lemma 12**.: _The total query complexity of PrecomputationsFull is_

\[O(n\cdot\log(n)\cdot\log(k)\cdot u(n,k)).\]

Proof.: For any \(\gamma\in\Gamma\), let \(n^{\gamma}\) be the length of the stream corresponding to predicted active elements \(\hat{V}_{1}(\gamma),\dots,\hat{V}_{n}(\gamma)\). By Lemma 7, the total query complexity is

\[\sum_{\gamma\in\Gamma}|H|n^{\gamma}u(n^{\gamma},k) \leq\sum_{\gamma\in\Gamma}\log(n)\cdot 2|\hat{V}(\gamma)|\cdot u(n,k)\] \[=2(\log n)u(n,k)\sum_{a\in V}|\Gamma(a)|\] \[=O(\log(n)\cdot u(n,k)\cdot n\cdot\log k).\qed\]

### The full UpdateSol subroutine

UpdateSolFull takes as input the strongly robust sets \((Q_{t}^{\gamma,h},R_{t}^{\gamma,h})\), for all guesses \(\gamma\in\Gamma\) and \(h\in H\). It maintains a data structure \(\{(B^{\gamma},A^{\gamma},\eta^{\gamma})\}_{\gamma\in\Gamma}\) where \((B^{\gamma},A^{\gamma},\eta^{\gamma})\) is the data structure maintained by UpdateSolMain over guess \(\gamma\) for OPT. UpdateSolMain is called over all guesses \(\gamma\in\Gamma\) such that there is at least one active element \(a\in V_{t}(\gamma)\). The precomputations given as input to UpdateSolMain with guess \(\gamma\in\Gamma\) are \((Q_{t}^{\gamma,\eta_{t}},R_{t}^{\gamma,\eta_{t}^{\gamma}})\) where \(\eta_{t}^{\gamma}\in H\) is the closest guess in \(H\) to the current prediction error \(\eta_{t}\) that has value at least \(\eta_{t}\). Note that \(\eta_{t}^{\prime}\) is such that \(\eta_{t}^{\prime}+2w\geq k\) due to the definition of \(H\). The solution returned at time \(t\) by UpdateSolFull is the best solution found by UpdateSolMain over all guesses \(\gamma\in\Gamma\).

```
1:function\(f:2^{V}\rightarrow\mathbb{R}\), data structure \(A\), constraint \(k\), precomputations \(P_{t}=\{(Q_{t}^{\gamma,\eta},R_{t}^{\gamma,\eta})\}_{\gamma\in\Gamma,\eta\in H}\), time \(t\), current prediction error \(\eta_{t}\), \(V_{t}^{1}\), \(V_{t}^{2}\), \(\hat{V}_{t}\)
2:\(\{A^{\gamma}\}_{\gamma\in\Gamma}\gets A\)
3:\(\eta_{t}^{\prime}\leftarrow\min\{h\in H:h\geq\eta_{t}\}\)
4:for\(\gamma\in\Gamma\) such that \(|V_{t}(\gamma)|>0\)do
5:\(A^{\gamma},S^{\gamma}\leftarrow\textsc{UpdateSolMain}(f,A^{\gamma},k,(Q_{t}^{ \gamma,\eta_{t}^{\prime}},R_{t}^{\gamma,\eta_{t}^{\prime}}),t,\eta_{t}^{ \prime},V_{t}^{1}(\gamma),V_{t}^{2}(\gamma),V_{t-1}(\gamma),\gamma)\)
6:\(A\leftarrow\{A^{\gamma}\}_{\gamma\in\Gamma}\)
7:\(S\leftarrow\operatorname*{argmax}_{\gamma\in\Gamma:|V_{t}(\gamma)|>0}f(S^{ \gamma})\)
8:return\(A,S\) ```

**Algorithm 7**UpdateSolFull

**Lemma 13**.: _The UpdateSolFull algorithm has, over the \(n\) time-steps of the streaming phase, an amortized query complexity per update of \(O\left(\log^{2}(k)\cdot u(\eta+w+k,k)\right)\)._

Proof.: For every \(\gamma\in\Gamma\), we consider the following stream associated to \(\gamma\): \(\{(a_{t}^{\prime},o_{t}^{\prime})\}_{t=1}^{n}\) where \((a_{t}^{\prime},o_{t}^{\prime})=(a_{t},o_{t})\) if \(\gamma\in\Gamma(a_{t})\) and \((a_{t}^{\prime},o_{t}^{\prime})=(\texttt{null},\texttt{null})\) otherwise. null is a dummy symbol denoting an absence of an insertion or deletion. Let \(n^{\gamma}\) be the number of insertion/deletions in this new stream, i.e., \(n^{\gamma}=\sum_{t=1}^{n}\mathds{1}[a_{t}\neq\texttt{null}]\).

Using Lemma 5 the total query complexity due to a single phase corresponding to a \((\gamma,\eta^{\gamma})\) pair is \(O(u((\eta^{\gamma}+w+k)\log k,k)\cdot(\eta^{\gamma}+w+k)\log k)\). Next, note that there is a one-to-one mapping between insertions/deletions to \(A^{\gamma}\) at line 9 or line 11 of UpdateSol and elements of the stream\(\{(a^{\prime}_{t},o^{\prime}_{t})\}_{t=1}^{n}\). Thus, since a phase ends when \(|\text{Ops}^{\star}(A)|>\eta^{\gamma}/2+w\), all phases except the last phase process at least \(\eta^{\gamma}/2+w\) non-null elements from the stream \(\{(a^{\prime}_{t},o^{\prime}_{t})\}_{t=1}^{n}\). Thus, if there are at least two phases corresponding to \(n^{\gamma}\) then the total query complexity over these \(\eta^{\gamma}/2+w\) non-null elements is \(O(u(\eta^{\gamma}+w+k,k)\cdot(\eta^{\gamma}+w+k)\log k\cdot\frac{n^{\gamma}}{ \eta^{\gamma}/2+w})\). Since, \(\eta^{\gamma}/2+w>k\) we have that the amortized query complexity over \(n^{\gamma}\) non-null elements is \(O(u(\eta^{\gamma}+w+k,k)\cdot\log k)\). We also have that \(O(u(\eta^{\gamma}+w+k,k)\cdot\log k)=O(u(\eta+w+k,k)\cdot\log k)\) because either \(\eta+2w>k\) which implies that \(\eta^{\gamma}+w=O(\eta+w)\) or \(\eta+2w<k\) in which case \(\eta^{\gamma}+w+k=O(k)\).

If there is only one phase, i.e. when \(n^{\gamma}\leq\eta^{\gamma}/2+w\), then the amortized query complexity for the first phase is upper bounded by \(u(n^{\gamma},k)=O(u(\eta^{\gamma}/2+w,k))\). Thus, the amortized query complexity due to \(\gamma\) is \(O(u(\eta+w+k,k)\log k)\), for any \(\gamma\in\Gamma\). We get that the total query complexity is

\[\sum_{\gamma\in\Gamma}O(u(\eta+w+k,k)\cdot n^{\gamma}\cdot\log k) =\sum_{\gamma\in\Gamma}\sum_{a\in\Gamma(a)}O(u(\eta+w+k,k)\log k)\] \[=\sum_{a\in V}\sum_{\gamma\in\Gamma(a)}O(u(\eta+w+k,k)\log k)\] \[=O(n\log^{2}k)\cdot u(\eta+w+k,k)\qed\]

### The main result

By combining the algorithmic framework (Algorithm 1) together with subroutines UpdateSolFull and PrecomputationsFull, we obtain our main result.

**Theorem 2**.: _Algorithm 1 with subroutines UpdateSolFull and PrecomputationsFull is a dynamic algorithm that, for any tolerance \(w\) and constant \(\epsilon>0\), achieves an amortized expected query complexity per update during the streaming phase of \(O(\text{poly}(\log\eta,\log w,\log k))\), an approximation of \(1/2-\epsilon\) in expectation, and a query complexity7 of \(\tilde{O}(n)\) during the precomputation phase._

Footnote 7: We note that, despite the amortized query complexity of Lattanzi et al. [20] being in expectation, the asymptotic bound on the precomputation query complexity can hold deterministically, instead of in expectation, by forcing PrecomputationsFull to terminate if it has performed a number of queries that is larger than \(\epsilon^{-1}\) times its expected number of queries (note that the precomputation query complexity only depends on known parameters, \(n\) and \(k\)). By Markovâ€™s inequality, such an early termination happens with probability at most \(\epsilon\). Thus, even with no guarantees on the approximation achieved in these early termination cases, the loss in the expected approximation caused by this forced termination is at most \(1-\epsilon\).

Proof.: The dynamic algorithm Dynamic used by the Precomputations and UpdateSol subroutines is the algorithm of Lattanzi et al. [20] with amortized expected update time \(u(n,k)=O(\text{poly}(\log n,\log k))\). By Lemma 13, the amortized expected query complexity is

\[O\left(\log^{2}(k)\cdot u(\eta+2w+k,k)\right)=O\left(\text{poly}(\log(\eta+w+k ),\log k)\right).\]

For the approximation, consider some arbitrary time \(t\). Let \(\gamma^{\star}=\max\{\gamma\in\Gamma:\gamma\leq(1-\epsilon)\texttt{OPT}_{t}\}\). Let \(\texttt{OPT}^{\prime}_{t}:=\max_{S\subseteq V_{t}(\gamma^{\star}):|S|\leq k}f(S)\). We have that

\[\texttt{OPT}^{\prime}_{t}\geq f(O_{t}\cap V_{t}(\gamma^{\star}))\geq_{(1)}f(O _{t})-\sum_{o\in O_{t}\cap V_{t}(\gamma^{\star})}f(o)\geq_{(2)}f(O_{t})-k\cdot \frac{\epsilon\gamma^{\star}}{k}\geq_{(3)}(1-\epsilon)\texttt{OPT}_{t}\]

where \((1)\) is by submodularity, \((2)\) is by definition of \(V_{t}(\gamma^{\star})\), and \((3)\) by definition of \(\gamma^{\star}\). Consider the calls to UpdateSol by UpdateSolFull with \(\gamma=\gamma^{\star}\). Let \(t^{\prime}\) be the time at which \(A_{t}^{\gamma^{\star}}\) was initialized by UpdateSol with precomputations \((Q_{t^{\prime}},R_{t^{\prime}})=(Q_{t^{\prime}}^{\gamma^{\star},\eta^{\prime}_{ t^{\prime}}},R_{t^{\prime}}^{\gamma^{\star},\eta^{\prime}_{t^{\prime}}})\), where this equality is by definition of UpdateSolFull.

Next, we show that the conditions to apply Lemma 4 are satisfied. By definition of PrecomputationsFull and Lemma 6, \((Q_{t^{\prime}},R_{t^{\prime}})\) are \((d=2(\eta^{\prime}_{t^{\prime}}+2w),\epsilon,\gamma^{\star})\)-strongly robust with respect to \(V_{t}(\gamma^{\star})\) with \(\gamma^{\star}=\gamma_{t^{\prime}}\). Since \(\eta^{\prime}_{t^{\prime}}\geq\eta_{t^{\prime}}\), we have that \((Q_{t^{\prime}},R_{t^{\prime}})\) are \((d=2(\eta_{\text{old}}+2w),\epsilon,\gamma^{\star})\). We also have that

\[\gamma_{t^{\prime}}\leq(1-\epsilon)\texttt{OPT}_{t}\leq\texttt{OPT}^{\prime}_{t }\leq\texttt{OPT}_{t}\leq(1+\epsilon)\gamma_{t^{\prime}}/(1-\epsilon).\]Thus, with \(\epsilon^{\prime}>0\) such that \((1+\epsilon^{\prime})=(1+\epsilon)/(1-\epsilon)\), we get \(\gamma_{\epsilon^{\prime}}\leq\texttt{OPT}_{t}^{\prime}\leq(1+\epsilon^{\prime} )\gamma_{\epsilon^{\prime}}\). By Lemma 3, Dynamic is a threshold-based algorithm. Thus, all the conditions of Lemma 4 are satisfied and we get that the solution \(S_{t}^{\gamma^{*}}\) returned by the call to UpdateSol with \(\gamma=\gamma^{*}\) at time \(t\), where \(A_{t}^{\gamma^{*}}\) was initialized by UpdateSol at time \(t^{\prime}\) with \(\gamma=\gamma^{*}\), is such that \(\mathbf{E}[f(S_{t}^{\gamma^{*}})]\geq\frac{1-5\epsilon^{\prime}}{2}\gamma^{*} \geq(1-\epsilon)\frac{1-5\epsilon^{\prime}}{2}\texttt{OPT}_{t}\). Finally, since UpdateSolFull returns, among all the solutions returned by UpdateSol, the one with highest value, it returns a set \(S_{t}\) such that \(\mathbf{E}[f(S_{t})]\geq\mathbf{E}[f(S_{t}^{\gamma^{*}})]\geq(1-\epsilon)\frac {1-5\epsilon^{\prime}}{2}\texttt{OPT}_{t}\). Finally, the precomputation query complexity is by Lemma 12 with \(u(n,k)=O(\text{poly}(\log n,\log k))\).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The theoretical contributions mentioned in the abstract and introduction are formalized as theorems with complete proofs and we perform experiments validating the theory. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations in a separate section. The computational efficiency of the proposed algorithms is clearly stated in the main theorems. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: We provide a comprehensive proof for all the theoretical results and state all the assumptions. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: all the information needed for reproducibility of the experimental results is included. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: We are not submitting the code because one of the libraries we extensively use/modify requires several conditions for distributing derivatives of their library. We did not have time to satisfy all these conditions before the deadline. However, we have carefully read these conditions and will definitely be able to meet these conditions before the potential camera-ready deadline, at which point we would release our code. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The paper specifies all the information about the dataset and hyperparameters. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: we only conduct small scale experiments to validate our theoretical results. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: we only compute small scale experiments which can be run within a few minutes on a CPU. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We follow the code of ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: the main contribution of this work is theoretical and we do not anticipate any negative societal impact of our model. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our experiments pose no such risk. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: we cite the code due to [20] used in our work and we respect the license of use. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: No new assets are released. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No crowdsourcing involved. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: No crowdsourcing involved. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.