On the Convergence and Sample Complexity Analysis of Deep Q-Networks with \(\varepsilon\)-Greedy Exploration

 Shuai Zhang

New Jersey Institute of Technology

&Hongkang Li

Rensselaer Polytechnic Institute

Meng Wang

Rensselaer Polytechnic Institute

Miao Liu

IBM Research

Pin-Yu Chen

IBM Research

Songtao Lu

IBM Research

Sijia Liu

Michigan State University

&Keerthiram Murugesan

IBM Research

Subhajit Chaudhury

IBM Research

###### Abstract

This paper provides a theoretical understanding of Deep Q-Network (DQN) with the \(\varepsilon\)-greedy exploration in deep reinforcement learning. Despite the tremendous empirical achievement of the DQN, its theoretical characterization remains underexplored. First, the exploration strategy is either impractical or ignored in the existing analysis. Second, in contrast to conventional Q-learning algorithms, the DQN employs the target network and experience replay to acquire an unbiased estimation of the mean-square Bellman error (MSBE) utilized in training the Q-network. However, the existing theoretical analysis of DQNs lacks convergence analysis or bypasses the technical challenges by deploying a significantly over-parameterized neural network, which is not computationally efficient. This paper provides the first theoretical convergence and sample complexity analysis of the practical setting of DQNs with \(\varepsilon\)-greedy policy. We prove an iterative procedure with decaying \(\varepsilon\) converges to the optimal Q-value function geometrically. Moreover, a higher level of \(\varepsilon\) values enlarges the region of convergence but slows down the convergence, while the opposite holds for a lower level of \(\varepsilon\) values. Experiments justify our established theoretical insights on DQNs.

## 1 Introduction

Reinforcement learning (RL) is a sequential decision-making process for a learning agent taking actions in the environment. RL has found important applications in autonomous control [37, 36], healthcare [16], Internet of Things [40, 33], and natural language processing [70]. The environment of an RL problem is modeled as a Markov decision process (MDP) with an underlying state transition probability matrix. The goal of the problem is to find an optimal policy to select the best actions to maximize the immediate and future rewards. Q-learning [77] has been recognized as one of the most promising and efficient learning algorithms for seeking the optimal policy because it does not require the knowledge of the model of the environment (namely, "model free") and can learn from data generated from a non-optimal policy (namely, "off-policy"). Traditional Q-learning approaches are centered on tabular methods [82, 41] or linear function approximations [65, 13] to estimate the optimal action-value (Q-value) function. However, tabular methods require sample complexity scaling in the order of state space, which is impractical for modern RL problems involving large or even infinite state space [82]. Q-learning with linear function approximation is limited to applications only when the transition matrix admits a linear feature representation [23].

Due to the remarkable advancements in deep neural networks (DNNs), the Deep Q-network (DQN) framework has emerged as a powerful approach that leverages the expressive power of non-linear functions and the ability to generalize to unknown states. DQN has proven to be a pioneering solution that addresses challenges encountered by traditional approaches. In the DQN framework [51], the Q-value function is approximated using a DNN, and the algorithm iteratively updates the Q-value function by collecting data following the \(\varepsilon\)-greedy policy. The \(\varepsilon\)-greedy policy is the simplest approach to balance exploration and exploitation. Namely, with a probability of \(\varepsilon\), we select a random action (_exploration_), and with a probability of \(1-\varepsilon\), we choose the best action according to the current estimated Q-value function (_exploitation_). \(\varepsilon\)-greedy is myopic compared with other strategic explorations, e.g., Thompson sampling-based [71, 55, 25] and optimism in the face of the uncertainty (OFU)-based [30, 31] ones. However, implementing these strategic explorations is not computationally efficient in DQNs [17], while DQNs equipped with \(\varepsilon\)-greedy policy have shown empirical success in diversified applications, e.g., the game of Go [63], Atari games [51], robotics [34], and autonomous vehicles [61, 12, 60].

Despite the numerical success, the theoretical understanding of DQN remains elusive, which could prevent its applications in domains requiring _reliable and safe_ decision-making. First, updating the Q-value function involves minimizing a mean-squared Bellman error (MSBE) function. However, existing convergence analysis and statistical properties are predominantly limited to linear models, failing to capture the complexities present in non-linear neural networks like DQN. Second, the sample complexity required for the convergence of MSBE is still not well comprehended. Achieving the desired accuracy in this context often demands a sample complexity that grows exponentially with the input dimension [46], rendering it impractical and inefficient in real-world scenarios. Third, the optimal selection of the \(\varepsilon\) value in DQN remains a gap in existing research. The hyperparameter tuning in algorithms involved with neural networks can be arduous and time-consuming. For instance, without a well-designed hyperparameter configuration, only a small fraction (e.g., 1%) of the possible combinations yield satisfactory results in neural network training [76].

**Contributions.** To the best of our knowledge, this paper presents the first theoretical study with convergence analysis for Deep Q-Networks (DQNs) utilizing the \(\varepsilon\)-greedy policy. A comparison with existing works can be found in Table 1. The paper focuses on the Q-value function approximated by a DNN. It offers a comprehensive analysis of the convergence of DQNs and provides insights into the estimation error of the learned Q-value function, accompanied by an analysis of the sample complexity. The key contributions of this study are as follows:

**1. The convergence analysis of DQN with bounded estimation errors.** Assuming the existence of a DNN with unknown weights \(\bm{W}^{\star}\) that matches the optimal Q-value function, this paper proves that the learned model via DQNs equipped with \(\varepsilon\)-greedy policy through the (accelerated) gradient descent algorithm converges linearly to \(\bm{W}^{\star}\) up to some characterizable estimation error.

**2. The theoretical characterization for a wide selection range of \(\varepsilon\) over iterations.** This paper provides lower and upper bounds of \(\varepsilon\) at each iteration for the convergence of DQNs and, in particular, characterizes the sample complexity, estimation error, and convergence rate of the DQN equipped with \(\varepsilon\)-greedy with decreasing \(\varepsilon\). Moreover, this paper proves that a higher level of \(\varepsilon\) values leads to an enlarged region of convergence, which relaxes the requirement on the initialization, while a lower level of \(\varepsilon\) values leads to faster convergence.

**3. The sample complexity analysis for learning a desired Q-value function.** We quantify the required sample complexity, depending on the neural network parameters and distribution shift of the collected data, for the learned model to converge to the desired accuracy. Typically, the estimation error of the converged model scales in the order of \(1/\sqrt{N_{s}}\), where \(N_{s}\) is the number of samples.

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Work** & **Neural Approximation** & **Convergence of MSBE** & \(\varepsilon\)**-greedy** \\ \hline \hline Yang \& Wang (2019) & ✗ & ✓ & ✗ \\ \hline Xu \& Gu (2020) & ✓ & ✓ & ✗ \\ \hline Fan et al. (2020) & ✓ & ✗ & ✗ \\ \hline Liu et al. (2022) & ✓ & ✗ & ✓ \\ \hline This work & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison among some representative works of Q-learning with function approximation.

Related Works.

**Q-learning with linear function approximation.** In the setting of linear function approximation, the Q-function is assumed to be a linear function of either the feature mapping [83; 32; 92] or a mixture of some basis kernels [91; 52]. Early works mainly focus on the algorithm design [6; 48; 2] and convergence analysis [38; 47; 67; 14; 69] but lacks theoretical guarantees with polynomial sample complexity. Assuming the underlying Q-function can be exactly represented as a linear function of the feature mapping with some unknown parameters, several sample-efficient algorithms are proposed to find the ground-truth mapping with finite-sample guarantee [11; 80], and the sample complexity depends linearly on the feature dimension [80].

**Q-learning with non-linear function approximation.** Recent approaches with non-linear function approximation mainly fall into the frameworks of the Bellman Eluder (BE) dimension [30; 18; 58; 27], Neural Tangent Kernel (NTK) [81; 10; 79; 10; 20; 53], and Besov regularity [68; 29; 54; 46; 24]. The Eluder dimension is at least in an exponential order even for a two-layer neural network [19], leading to uncharacterizable sample complexity. The NTK framework linearizes deep neural networks to tackle convergence in non-linear models. However, it requires using computationally inefficient extremely wide neural networks [81]. Moreover, the sample complexity can exponentially increase with the input feature dimension, necessitating a substantial number of samples for accurate estimation. Furthermore, the NTK approach fails to explain the advantages of non-linear neural networks over linear function approximation [46; 24]. Besov space requires the neural networks to be sparse and makes an unpractical assumption that the algorithm can find the global optimal of the non-convex objective function [29; 54; 46; 24]. To the best of our knowledge, only [46] considers \(\varepsilon\)-greedy policy with theoretical analysis applicable to DQNs. However, the model is limited to sparse neural networks, and it cannot characterize the case of varying \(\varepsilon\).

**Supervised learning with neural networks.** Compared with Q-learning in RL, where the label of the sampled data depends on the currently estimated Q function, analyzing supervised learning is less challenging, where sampled data label is known and fixed across the training. Existing theoretical results for supervised learning are largely built upon NTK [28; 21; 39; 15; 43; 45], model recovery [90; 26; 3; 64; 88; 85; 87], and structured data [44; 62; 8; 1; 35; 78; 84; 42]. Due to the high non-convexity of neural networks [59], the one-hidden-layer neural network is still a state-of-the-art practice for convergence analysis and generalization guarantees. Additional assumptions, e.g., Gaussian distribution [89; 7], linear separable data [75; 9] on the data distribution, are needed for finite-sample analysis.

## 3 Problem Formulation: Notation, Background, and Algorithm

**The Markov Decision Process and Q-learning.** A discounted Markov decision process (MDP) is defined as \((\mathcal{S},\mathcal{A},\mathcal{P},r,\gamma)\), where \(\mathcal{S}\) is the state space, \(\mathcal{A}\) is the action set. \(\mathcal{P}:\mathcal{S}\times\mathcal{A}\longrightarrow\Delta(\mathcal{S})\) is the transition operator, and \(p^{a}_{\boldsymbol{s},\boldsymbol{s}^{\prime}}:=\mathcal{P}\big{(}\boldsymbol {s}^{\prime}|\boldsymbol{s},a\big{)}\) denotes the transition probability from current state \(\boldsymbol{s}\) and action \(a\) to the next state \(\boldsymbol{s}^{\prime}\). In addition, \(r:\mathcal{S}\times\mathcal{A}\longrightarrow[-R_{\max},R_{\max}]\) is the reward function, and \(\gamma\in(0,1)\) is the discount factor.

At a state \(\boldsymbol{s}\in\mathcal{S}\), the agent takes action \(a\in\mathcal{A}\) according to some behavior policy \(\pi\), denoted as \(a\sim\pi(\boldsymbol{s})\) (or \(a=\pi(\boldsymbol{s})\) for deterministic policy). Then, the system moves to the next state \(\boldsymbol{s}^{\prime}\) following the transition probability \(p^{a}_{\boldsymbol{s},\boldsymbol{s}^{\prime}}\). Meanwhile, the agent receives an immediate reward \(r(\boldsymbol{s},a)\) from the environment. Let \(\{\boldsymbol{s}_{i},a_{i}\}_{i=0}^{\infty}\) be the generated sequential data given the behavior policy \(\pi\) and transition probability \(\mathcal{P}\). We define the state-value function \(V_{\pi}\) at state \(s\) as

\[V^{\pi}(\boldsymbol{s})= \mathbb{E}_{\pi,\mathcal{P}}\big{[}\sum_{i=0}^{\infty}\gamma^{i} r(\boldsymbol{s}_{i},a_{i})\mid\boldsymbol{s}_{0}=\boldsymbol{s}\big{]},\] (1)

which is the expected total discounted reward starting from the state \(\boldsymbol{s}\). For any state-action \((\boldsymbol{s},a),\) the corresponding Q-value (or action-value) function \(Q_{\pi}\) of a policy \(\pi\) is defined as

\[Q^{\pi}(\boldsymbol{s},a)= \mathbb{E}_{\pi,\mathcal{P}}\big{[}\sum_{i=0}^{\infty}\gamma^{i} r(\boldsymbol{s}_{i},a_{i})\mid\boldsymbol{s}_{0}=\boldsymbol{s},a_{0}=a \big{]}.\] (2)

Then, the goal of the agent is to find an optimal policy \(\pi^{\star}\) that maximizes the state-value function in (1) for all states, which is equivalent to

\[Q^{\star}(\boldsymbol{s},a):=\max_{\pi}Q^{\pi}(\boldsymbol{s},a)= r(\boldsymbol{s},a)+\gamma\cdot\mathbb{E}_{\boldsymbol{s}^{\prime}| \boldsymbol{s},a}\max_{a^{\prime}}Q^{\star}(\boldsymbol{s}^{\prime},a^{\prime}),\] (3)where (3) is known as the Bellman equation. With the optimal Q-value function \(Q^{\star}\), the optimal policy can be derived via \(\pi^{\star}(\bm{s})=\operatorname*{argmax}_{a}Q^{\star}(\bm{s},a)\)[77, 66].

**The Deep Neural Network Model.** The DQN utilizes a deep neural network (DNN) \(H:\mathbb{R}^{d}\longrightarrow\mathbb{R}\) to approximate the optimal Q-value function \(Q^{\star}\) in (3). Specifically, given input \(\bm{x}\in\mathbb{R}^{d}\), the output of the \(L\)-hidden-layer DNN with \(K\) neurons in each hidden layer is defined as

\[H(\bm{W};\bm{x}):=\bm{1}^{\top}/K\cdot\phi(\bm{W}_{L}^{\top}\cdots\phi(\bm{W}_ {1}^{\top}\bm{x})),\] (4)

where \(\bm{W}_{1}\in\mathbb{R}^{d\times K}\), \(\bm{W}_{l}\in\mathbb{R}^{K\times K}\) with \(l=2,\cdots,L\), and \(\bm{W}=[\text{vec}(\bm{W}_{1})^{\top},\cdots,\text{vec}(\bm{W}_{L})^{\top}]^ {\top}\) is the concatenation of the vectorization of all parameter matrices. \(\phi(\cdot)\) is the nonlinear activation function, and we consider the ReLU activation function, i.e., \(\phi(z)=\max\{0,z\}\). Then, the Q-value function \(Q(\bm{s},a)\) is parameterized using the DNN as

\[Q(\bm{W};\bm{s},a)=H\big{(}\bm{W};\bm{x}(\bm{s},a)\big{)},\] (5)

where \(\bm{x}:\mathcal{S}\times\mathcal{A}\longrightarrow\mathbb{R}^{d}\) is the feature mapping of the state-action pair. Without loss of generality, we assume \(|\bm{x}(\bm{s},a)|\leq 1\). Then, the goal of DQN-based Q-learning is to minimize the mean squared Bellman error (MSBE) as

\[\min_{\bm{W}}:f(\bm{W}):=\mathbb{E}_{(\bm{s},a)\sim\pi^{\star}}\big{(}Q(\bm{W };\bm{s},a)-r(\bm{s},a)-\gamma\cdot\mathbb{E}_{\bm{s}^{\prime}|\bm{s},a}\max _{a^{\prime}}Q(\bm{W};\bm{s}^{\prime},a^{\prime})\big{)}^{2},\] (6)

where \(\mu\) is the distribution of \((\bm{s},a)\) following the optimal policy \(\pi^{\star}\).

### The Deep Q-Network Algorithm

```
1:Input: Number of iterations \(T\times M\), and experience replay buffer size \(N\), exploration probability \(\{\varepsilon_{t}\}_{t=1}^{T}\), step size \(\eta\), and momentum parameter \(\beta\).
2:Initialize the Q-network with weights \(\bm{W}^{(0,0)}\).
3:for\(t=0,1,2,\cdots,T-1\)do
4: Select the initial weights \(\bm{W}^{(t,0)}\).
5:for\(m=0,1,2,\cdots,M-1\)do
6: Sample data and store in the experience replay buffer \(\mathcal{D}_{t}\) following \(\varepsilon_{t}\)-greedy policy, namely, at state \(\bm{s}_{n}\), with probability \(\varepsilon_{t}\), select a random action \(a_{n}\), otherwise select \(a_{n}=\operatorname*{argmax}_{a}Q(\bm{W}^{(t,0)};\bm{s}_{n},a)\).
7: Sample random mini-batch of transition \(\mathcal{D}_{t}^{(m)}\) from the replay buffer \(\mathcal{D}_{t}\).
8: Set \(y_{n}=r_{n}+\gamma\max_{a}Q(\bm{W}^{(t,0)};\bm{s}_{n}^{\prime},a)\) for \(n\in 1,2,\cdots,|\mathcal{D}_{t}^{(m)}|\).
9: Perform a gradient descent step \[\bm{W}^{(t,m+1)}= \bm{W}^{(t,m)}-\eta\cdot g_{t}^{(m)}(\bm{W}^{(t,m)})+\beta(\bm{W} ^{(t,m)}-\bm{W}^{(t,m-1)}).\]
10:endfor
11: Set \(\bm{W}^{(t+1,0)}=\bm{W}^{(t,M)}\).
12:endfor ```

**Algorithm 1** Deep Q-Network

The learning problem (6) is solved via the DQN equipped with \(\varepsilon\)-greedy exploration, as summarized in Algorithm 1. In \(t\)-th outer loop, we initialize the Q-value function using currently estimated weights \(\bm{W}^{(t,0)}\) as \(Q(\bm{W}^{(t,0)})\) (line 4). Then, for each inner loop, the agent selects and executes actions according to the \(\varepsilon\)-greedy policy (line 6), namely, with probability \(\varepsilon\), we select a random action, and with probability \(1-\varepsilon\), we select the action based on the greedy policy with respect to \(Q(\bm{W}^{(t,0)})\). The data are stored in a replay buffer with size \(N\) (line 7). Then, we sample a mini-batch of independent samples from \(\mathcal{D}_{t}\), denoted as \(\mathcal{D}_{t}^{(m)}\) for the \(m\)-th inner loop (line 8). Next, we update the current weights using a mini-batch (accelerated) gradient descent algorithm (line 9). The gradient descent (GD) direction in this step at point \(\bm{W}\) is represented as

\[g_{t}^{(m)}(\bm{W})=\sum_{n\in\mathcal{D}_{t}^{(m)}}\big{(}Q(\bm{W};\bm{s}_{n},a _{n})-y_{n}^{(t)}\big{)}\cdot\nabla_{\bm{W}}Q(\bm{W};\bm{s}_{n},a_{n}),\] (7)

where

\[y_{n}^{(t)}=r_{n}+\gamma\cdot\max_{a^{\prime}\in\mathcal{A}}Q(\bm{W}^{(t,0)}; \bm{s}_{n}^{\prime},a^{\prime}).\] (8)

Note that (7) can be viewed as the gradient of

\[\mathbb{E}_{(\bm{s},a)\sim\mu_{t}}\big{(}Q(\bm{W};\bm{s},a)-r-\mathbb{E}_{\bm{ s}^{\prime}|\bm{s},a}\max_{a^{\prime}}Q(\bm{W}^{(t,0)};\bm{s}^{\prime},a^{ \prime})\big{)}^{2},\] (9)which is the approximation to (6) via fixing the \(\max_{a}Q(\bm{W})\) as \(\max_{a}Q(\bm{W}^{(t,0)})\). After moving along the GD direction, accelerated gradient descent (AGD) adds a momentum term, denoted by \(\beta(\bm{W}^{(t,m)}-\bm{W}^{(t,m-1)})\) to accelerate the convergence rate [57]. Vanilla SGD can be viewed as a special case of AGD by letting \(\beta=0\). After updating neuron weights in the inner loop, we set the network as the currently estimated Q-value function \(Q(\bm{W}^{(t,0)})\) (line 11) and repeat the steps above.

## 4 Theoretical Results

### Takeaways of the Theoretical Findings

We consider the general setup of DQNs with \(\varepsilon\)-greedy under some commonly used assumptions. To the best of our knowledge, we provide the first theoretical characterization of both the convergence and sample complexity analysis for DQNs with \(\varepsilon\)-greedy. The major notations are summarized in Table 2. We first briefly introduce the key takeaways of our results, and the formal theoretical results are introduced in Section 4.3.

**(T1) Theoretical characterizations of \(\{\varepsilon_{t}\}_{t=1}^{T}\) for convergence.** We prove that for a wide selection of \(\{\varepsilon_{t}\}\) values that decrease over time, Algorithm 1 converges to \(Q^{\star}\) linearly up to some estimation error. Let \(c_{\varepsilon}\) measure the value level of \(\varepsilon_{t}\)'s. A higher level of \(\varepsilon\) values (i.e., a larger \(c_{\varepsilon}\)) leads to an enlarged region of convergence (in the order of \(c_{\varepsilon}\)), measured by the distance from the initialization \(\bm{W}^{(0,0)}\) to \(\bm{W}^{\star}\). Thus, larger \(\varepsilon\) values relax the requirements on \(\bm{W}^{(0,0)}\). A lower level of \(\varepsilon\) values (i.e., a smaller \(c_{\varepsilon}\)) leads to faster convergence with a rate in the order of \(c_{\varepsilon}\). Our findings explain the intuition that the agent tends to explore more at the beginning and exploit more after gaining enough knowledge during the exploration.

**(T2) Convergence to the optimal Q-value function \(Q^{\star}\) with geometric decay.** The learned models converge to the ground truth model \(Q^{\star}\) with a geometric decay up to some bounded estimation error. The convergence rate is upper bounded by \(\gamma+c_{\varepsilon}\cdot(1-\gamma)\). When \(\gamma\) is close to one, the problem emphasizes long-term rewards. While the immediate reward can be observed directly, the future reward needs to be estimated by the learned Q function as shown in (7). Therefore, with a large \(\gamma\), the learned Q function needs more iterations to converge, which leads to a slow convergence rate.

**(T3) Sample complexity for achieving a desired estimation error of the optimal Q-value function.** With the proper selection of \(\{\varepsilon_{t}\}_{t=1}^{T}\), the estimation error of the learned model scales in the order of \(\frac{C_{\mathcal{T}}}{(1-\gamma)^{2}\sqrt{N_{s}}}\). \(C_{T}\) is the fraction of actions following the current greedy policy that differ from the ones following the optimal policy. \(N_{s}\) is the number of samples. With a smaller discounted factor \(\gamma\), the problem focuses more on the immediate reward, which can be observed directly, making \(Q^{\star}\) easier to learn. The learned model achieves a small estimation error given a small distribution shift, a large sample size, or a small discounted factor.

### Assumptions

We propose assumptions commonly used in existing RL and neural network learning theories and notations to simplify the presentation. Assumption 1 assumes the existence of a good approximation of DNN to \(Q^{\star}\), which guarantees (6) is solvable. The assumption is commonly used in both deep learning theories [90; 86] and reinforcement learning theories [46; 24]. Assumption 2 assumes the samples from experience replay are independent and identically distributed (i.i.d.), which follows the assumptions in existing theoretical analysis of DQN [24; 53] and matches the intuition of using experience replay to break temporal dependence among the samples. Specifically, we have

**Assumption 1**.: _There exists a DNN with weights \(\bm{W}^{\star}\) such that minimizes (6) as \(f(\bm{W}^{\star})=0\)._

\begin{table}
\begin{tabular}{c|l||l|l} \hline \hline \(K\) & Number of neurons in each hidden layer. & \(L\) & Number of the hidden layers. \\ \hline \(d\) & Dimension of the feature mapping of \((\bm{s},a)\). & \(N_{s}\) & The sample complexity for \(\delta\)-optimal policy. \\ \hline \(\bm{W}^{\star}\) & The global optimal to (6). & \(e_{t}\) & The value of \(\|\bm{W}^{(t,0)}-\bm{W}^{\star}\|_{F}\). \\ \hline \(c_{\varepsilon}\) & A small positive constant with a linear dependence on \(\varepsilon_{t}\). & \(C_{t}\) & The fraction of actions with data at iteration \(t\) such that \(\operatorname*{argmax}_{a}Q(\bm{W}^{(t,0)};\bm{s},a)\neq\operatorname*{argmax} _{a}Q(\bm{W}^{\star};\bm{s},a)\). \\ \hline \hline \end{tabular}
\end{table}
Table 2: Some Important NotationsAssumption 1 assumes that the Q-value function with some unknown ground truth \(\bm{W}^{\star}\)1 can represent the optimal Q-value function.

Footnote 1: Note that \(\bm{W}^{\star}\) does not need to be unique. We abuse the notation \(\|\bm{W}-\bm{W}^{\star}\|_{2}\) to denote the minimum distance of \(\bm{W}\) to any \(\bm{W}^{\star}\) satisfying Assumption 1.

**Assumption 2**.: _Suppose the mini-batch data are i.i.d. samples from the replay buffer following the distribution \(\mu_{t}\), which is the stationary distribution of the behavior policy at \(t\)-th outer loop._

Assumption 2 assumes the mini-batch at the \(t\)-th outer loop are _i.i.d._ samples following \(\mu_{t}\), where \(\mu_{t}\) is the stationary distribution of \((\bm{s},a)\) generated by \(\varepsilon_{t}\)-greedy policy at \(t\)-th outer loop2. The mini-batch samples are close to being independent given the experience size in practice is sufficiently large (\(\sim\) millions [51]).

Footnote 2: The framework can be extended into non i.i.d. samples, see Appendix G.

In what follows, we define two quantities \(C_{t}\) and \(\rho\) to simplify the presentation of theoretical results.

**Definition 1**.: \(C_{t}\in[0,1]\) _is the fraction of non-optimal state-action pair \((\bm{s},a)\) in the greedy policy with respect to \(Q(\bm{W}^{(t,0)})\), i.e., the fraction of \((\bm{s},a)\) pairs that satisfy_

\[a=\operatorname*{argmax}_{a^{\prime}}Q(\bm{W}^{(t,0)};\bm{s},a^{\prime})\neq \pi^{\star}(\bm{s})\] (10)

_among all \((\bm{s},a)\) pairs following the greedy policy at \(t\)-th outer loop as \(a=\operatorname*{argmax}_{a^{\prime}}Q(\bm{W}^{(t,0)};\bm{s},a^{\prime})\)._

\(C_{t}\) can be viewed as the difference between behavior policy and optimal policy. Theorem 1 shows the general results for any value of \(C_{t}\). Nevertheless, the greedy policy is improved over time, e.g., updating the weights every few steps (line 1 in Algorithm 1). Hence, \(C_{t}\) depends on \(\bm{W}^{(t,0)}\) and is expected to decrease as \(\bm{W}^{(t,0)}\) approaching \(\bm{W}^{\star}\)[56; 92], which will be discussed in Corollary 3.

**Definition 2**.: _Let \(\rho\) be the value of_

\[\rho:=\min_{\|\bm{\alpha}\|_{2}\geq 1}\mathbb{E}_{(\bm{s},a)\sim\pi^{\star}} \big{(}\bm{\alpha}^{\top}\nabla_{\bm{W}}Q(\bm{W}^{\star};\bm{s},a)\big{)}^{2}.\] (11)

\(\rho\) suggests the radius of the local convex region of the objective function. We provide the lower bound for \(\rho\) in Lemma 7 (see the proof in Appendix E.2), suggesting a sufficiently large local convex region near \(\bm{W}^{\star}\).

### Major Theoretical Results

Lemma 1 characterizes the convergent point when minimizing (9) using the mini-batch gradient descent in the \(t\)-th outer loop under certain conditions. Specifically, given that the initial weights at the \(t\)-th outer loop are sufficiently close to the ground truth as shown in (12) and the replay buffer is large enough as shown in (13), the distance between the learned model weights \(\bm{W}^{(t+1,0)}\) and \(\bm{W}^{\star}\) are bounded from above as shown in (14).

**Lemma 1** (Estimation error of \(\bm{W}^{(t+1,0)}\)).: _Suppose Assumptions 1 & 2 hold and the initial neuron weights at the \(t\)-th outer loop satisfy_

\[e_{t}:=\ \|\bm{W}^{(t,0)}-\bm{W}^{\star}\|_{F}\leq\ \mathcal{O}\Big{(}1- \frac{1-\Theta(\varepsilon_{t})}{\Theta(\sqrt{N})}\Big{)}\cdot\frac{\rho\cdot \|\bm{W}^{\star}\|_{F}}{K},\] (12)

_The step size \(\eta\) is \(1/T\), and the size of the replay buffer is_

\[N=\Omega(\rho^{-2}\cdot K^{3}\cdot L\cdot d\cdot\log q\cdot T).\] (13)

_Then, with the high probability of at least \(1-q^{-d}\), the neuron weights \(\bm{W}^{(t+1,0)}\) generated from Algorithm 1 satisfy_

\[\|\bm{W}^{(t+1,0)}-\bm{W}^{\star}\|_{F}\leq\ \big{(}1-\Theta(\varepsilon_{t}) \big{)}\cdot\gamma\cdot\|\bm{W}^{(t,0)}-\bm{W}^{\star}\|_{F}+\frac{C_{t}+(1-C_ {t})\varepsilon_{t}}{\Theta(\sqrt{N})}\cdot\frac{|\mathcal{A}|\cdot R_{\max}} {1-\gamma}.\] (14)

**Remark 1** (Large replay buffer reduces the estimation error and the requirement for \(\bm{W}^{(t,0)}\)).: From (14), a larger \(N\) leads to a reduced distance between \(\bm{W}^{(t+1,0)}\) and \(\bm{W}^{\star}\). Moreover, (12) implies that if we increase the size of replay buffer \(N\), the upper bound of \(e_{t}\) increases, indicating the algorithm can tolerant a large range of \(\bm{W}^{(t,0)}\).

In the following corollary, we show the upper and lower bound of \(\varepsilon_{t}\) at the \(t\)-th outer loop. The lower bound guarantees that the RHS of (12) is larger than \(0\), so we have a sufficiently large radius for convergence. The upper bound ensures (14) be less than \(e_{t}\), indicating an improved estimation of \(Q^{\star}\) across the iterations.

**Corollary 1** (Range of \(\varepsilon\)).: _Given the assumptions and conditions in Theorem 1 hold. To ensure the existence of a good initialization at iteration \(t\), \(\varepsilon_{t}\) needs to satisfy_

\[\varepsilon_{t}\geq 1-\Theta(\sqrt{N})\cdot(1-e_{t}),\] (15)

_To ensure the estimated learned model is improving over iterations, \(\varepsilon_{t}\) needs to satisfy_

\[\varepsilon_{t}\leq\frac{(1-\gamma)^{2}\cdot\Theta(\sqrt{N})\cdot e_{t}}{(1- C_{t})\cdot|\mathcal{A}|\cdot R_{\max}}-\frac{C_{t}}{1-C_{t}}.\] (16)

**Remark 2** (Reduce \(\varepsilon_{t}\) as \(t\) increases).: The lower bound can always be smaller than the upper bound given a sufficiently large \(N\) as shown in (13). From both (15) and (16), we know that \(\varepsilon_{t}\) needs to decrease as \(e_{t}\) decreases. Specifically, the lower bound of \(\varepsilon_{t}\) is a linear function of \(e_{t}\), and the upper bound of \(\varepsilon_{t}\) is a linear function of \(e_{t}\). Namely, we need a relatively large \(\varepsilon_{0}\) at the beginning since the distance between initial point \(\bm{W}^{(0,0)}\) and \(\bm{W}^{\star}\) is large. As \(t\) increases, \(e_{t}\), which is the distance of learned neuron weights \(\bm{W}^{(t,0)}\) to the ground truth \(\bm{W}^{\star}\), becomes smaller, and we should decrease \(\varepsilon_{t}\) to guarantee an improved \(Q^{(t+1,0)}\) over \(Q^{(t,0)}\).

Theorem 1 shows that the learned model from Algorithm 1 converges to the optimal Q-value function \(Q^{\star}\) with geometric decay up to an estimation error shown in (20).

**Theorem 1** (Convergence to \(Q^{\star}\)).: _Suppose Assumptions 1 and 2 hold, the buffer size \(N\) satisfies (13). Let us define \(C_{\max}\) be a constant that is larger than \(C_{t}\) for \(1\leq t\leq T\), when \(\varepsilon_{t}\) satisfy_

\[\varepsilon_{t}=\frac{c_{\varepsilon}\cdot\Theta(\sqrt{N})\cdot e_{t}}{(1-C_{ \max})\cdot|\mathcal{A}|\cdot R_{\max}}-\frac{C_{\max}}{1-C_{\max}}\] (17)

_for a fixed constant \(c_{\varepsilon}\in(0,(1-\gamma)^{2}]\), and the initialization satisfies_

\[\|\bm{W}^{(0,0)}-\bm{W}^{\star}\|_{F}\leq\ \mathcal{O}\Big{(}1-\frac{1-c_{ \varepsilon}}{\Theta(\sqrt{N})}\Big{)}\cdot\frac{\rho\cdot\|\bm{W}^{\star}\|_{ F}}{K}.\] (18)

_Then, with the high probability of at least \(1-T\cdot q^{-d}\), we have_

1. _the learned weights decay geometrically with_ \[\|\bm{W}^{(t+1,0)}-\bm{W}^{\star}\|_{F}\leq\big{(}\gamma+c_{\varepsilon}\cdot( 1-\gamma)\big{)}\cdot\|\bm{W}^{(t,0)}-\bm{W}^{\star}\|_{F},\ \ \forall t\leq\log_{\gamma}(1/N).\] (19)
2. _the returned model_ \(Q(\bm{W}^{(T,0)})\) _exhibits an estimation error in the order of_ \(1/\sqrt{N}\) _with_ \[\sup_{(\bm{s},a)}\big{|}Q\big{(}\bm{W}^{(T,0)})-Q^{\star}\big{|}= \Theta\big{(}\|\bm{W}^{(T,0)}-\bm{W}^{\star}\|_{F}\big{)}\leq\frac{C_{ \max}\cdot|\mathcal{A}|\cdot R_{\max}}{(1-\gamma)^{2}\cdot\Theta(\sqrt{N})},\] (20) _where_ \(T\geq\log_{\gamma}(1/N)\)_._

**Remark 3** (Selection of \(\varepsilon_{t}\)).: The value of \(\varepsilon_{t}\) is influenced by three key factors: \(c_{\varepsilon}\), \(C_{\max}\), and the current estimation error bound \(e_{t}\). The constant \(c_{\varepsilon}\) is fixed and controls the magnitude of the values in the sequence \(\varepsilon_{t}t_{t=1}^{T}\), providing a way to regulate the level of \(\varepsilon_{t}\). Regarding \(C_{\max}\), \(C_{t}\) tends to decrease as the iteration index \(t\) increases, indicating a progressive improvement in the policy and resulting in a smaller data distribution shift. Hence, to estimate \(C_{\max}\), we can leverage \(C_{0}\), which is obtained by collecting data based on the policy \(\max_{a}Q(\bm{W}^{(0,0)};\bm{s},a)\). Moreover, with \(c_{\varepsilon}\) fixed, the estimation error \(e_{t}\) follows a geometric decay pattern, as depicted in (19). This behavior allows us to use the expression \(\big{(}\gamma+c_{\varepsilon}\cdot(1-\gamma)\big{)}^{t}\cdot e_{0}\) as an estimate for \(e_{t}\).

**Remark 4** (Geometric decay to \(Q^{\star}\)).: From (19) and (20), we know that the learned model from the proposed algorithm converges to \(Q^{\star}\) with a geometric decay up to some estimation error. The convergence rate is in the order of \(\gamma+c_{\varepsilon}\cdot(1-\gamma)\), and the estimation error is in the order of \((1-\gamma)^{-2}\cdot C_{\max}/\sqrt{N}\). As we mentioned in the takeaways in Section 4.1, a small discounted factor \(\gamma\) leads to a fast convergence rate. We have a reduced estimation with a large buffer with size \(N\), a small distribution shift \(C_{T}\), and a small \(\gamma\).

**Remark 5** (Larger \(\varepsilon\) values for enlarged region of convergence and smaller \(\varepsilon\) values for faster convergence). From (17), we know that \(c_{\varepsilon}\) controls the value level of \(\varepsilon_{t}\). From (18), a larger \(c_{\varepsilon}\) (i.e., a higher level of \(\varepsilon\) values) increases the upper bound of \(\|\bm{W}^{(0,0)}-\bm{W}^{\star}\|_{2}\) and, thus, enlarges the proper region of \(\bm{W}^{(0,0)}\). (19) indicates that the convergence rate is in the order of \(c_{\varepsilon}\). A smaller \(c_{\varepsilon}\) (i.e., lower level of \(\varepsilon\) values) leads to faster convergence.

In the following corollary, we provide the sample complexity for achieving \(\delta\) estimation error as shown in (21), where \(\widetilde{\Omega}(\cdot)\) omits some \(\log\) factors. The corollary can be obtained by letting (20) to be less than a desired accuracy \(\delta\).

**Corollary 2** (Sample complexity).: _To achieve an estimation error of \(\delta\), the required number of samples, referred to as the sample complexity, needs to satisfy_

\[N_{s}=N\cdot\log\gamma=\widetilde{\Omega}\big{(}(1-\gamma)^{4}\cdot C_{\max} \cdot|\mathcal{A}|^{2}R_{\max}^{2}\cdot K^{3}\cdot L\cdot d\cdot T/\delta^{2} \big{)}.\] (21)

**Remark 6** (Sample complexity).: (21) shows that the sample complexity is a linear function of \(d\) and \(L\), where \(d\) is the feature mapping of the state-action pair \((\bm{s},a)\) and \(L\) is the number of layers. Given the freedom of degree of \(\bm{W}\) is a linear function of \(d\) and \(L\), respectively, the sample complexity is almost order-wise optimal with respect to \(d\) and \(L\).

The following corollary presents a tighter bound for the model estimation error compared with (20). This improvement arises from a stronger assumption on \(C_{t}\), which becomes a function of \(\|\bm{W}^{(t,0)}-\bm{W}^{\star}\|_{2}\). Compared with (20), the estimation error bound in (24) and (25) considers the cases that the behavior policy is improved as \(\bm{W}^{(t,0)}\) becomes closer to \(\bm{W}^{\star}\). As a result, we achieve a more precise and tighter estimation of the error in the model.

**Corollary 3** (Distribution shift and estimation error).: _Assume that \(C_{t}\) is Holder continuous with a factor \(\alpha\) as_

\[C_{t}=\mathcal{O}(\|\bm{W}^{(t,0)}-\bm{W}^{\star}\|_{2}^{\alpha}),\] (22)

3 _for \(0<\alpha\leq 1\) and all \(t\in[T]\). When \(\varepsilon_{t}\) satisfy_

\[\varepsilon_{t}=\frac{c_{\varepsilon}\cdot\Theta(\sqrt{N})\cdot e_{t}}{(1-C_{ t})\cdot|\mathcal{A}|\cdot R_{\max}}-\frac{C_{t}}{1-C_{t}},\] (23)

_the estimation error of the Q-function satisfies_

\[\sup_{(\bm{s},a)}\Big{|}Q(\bm{W}^{(T,0)})-Q^{\star}\Big{|}=\frac{(|\mathcal{ A}|\cdot R_{\max})^{\frac{1}{1-\alpha}}}{(1-\gamma)^{\frac{2}{1-\alpha}}\cdot \Theta(N^{\frac{1}{2(1-\alpha)}})}.\] (24)

_In the special case that \(C_{t}=\mathcal{O}(\|\bm{W}^{(t,0)}-\bm{W}^{\star}\|_{2})\), then_

\[\sup_{(\bm{s},a)}\big{|}Q(\bm{W}^{(T,0)})-Q^{\star}\big{|}=0.\] (25)

**Remark 7** (Reduced or zero estimation error when behavior policy is improved over iterations).: Recall the definition of \(C_{t}\) in Definition 1. If \(e_{t}\) is zero, i.e., \(\bm{W}_{t}=\bm{W}^{\star}\), the action selected following the greedy policy is always the optimal action, which means that \(C_{t}=0\). Therefore, it is reasonable to assume that \(C_{t}\) is Holder continuous as shown in (22). When \(\alpha>0\), we can see that (24) is less than (20), indicating a reduced estimation error and sample complexity. Typically, if \(C_{t}\) has an order of growth less than \(e_{t}\) near \(\bm{W}^{\star}\), a zero estimation error is achievable.

### The Roadmap of Proofs, Comparison with Existing Works, and Limitations

The proof of Theorem 1 draws inspiration from the model estimation framework in the supervised learning setting [90; 88]. The key idea is to use a population risk function (PRF) to characterize the objective function in (9). By satisfying certain conditions such as having sufficient training samples and a bounded data distribution shift, the approximation error between the PRF and objective function can be bounded. This allows for the characterization of the optimization problem in (9) by analyzing the landscape and convergence properties of the PRF.

In comparison to existing proofs based on the model estimation frameworks, this paper addresses two additional challenges. Firstly, it extends the proof from one-hidden-layer neural networks to multi-layer neural networks. This extension is achieved by providing new tools for characterizing the Hessian matrix (refer to Lemma 3) and concentration bound (refer to Lemmas 3 and 6). Additionally, this paper characterizes the differences between the two functions caused by the interaction of neuron weights across layers in the gradient and Hessian matrix. Secondly, the paper extends the proof from supervised learning settings to Q-learning settings. This requires characterizing the additional error term caused by the data distribution shift and "noisy" labels (refer to Lemma 3) because the empirical risk function is no longer an expectation of the defined population risk function.

Existing state-of-the-art theoretical results on Q-learning with neural network approximations primarily revolve around the NTK and Besov regularity frameworks. In the NTK framework, the networks are assumed to be extremely over-parameterized, requiring an impractical projection step and resulting in error bounds that cannot be characterized for networks with finite width. In the Besov regularity framework, the neural network needs to be sparse, which does not align with the DQN algorithm. Furthermore, the analysis in the Besov regularity framework relies on the achievability of the global minimizer of the non-convex problem in (9), which cannot be guaranteed using GD algorithms. This paper takes a significant step towards bridging the gap between theoretical understanding and practical applications of DQN by addressing these challenges. However, there still remains a gap between the theoretical results and numerical findings. Future research directions include devising efficient exploration strategies for DQNs to further enhance their performance and extending the theoretical analysis to variants of DQNs and policy gradient-based methods.

## 5 Numerical Experiments

In this section, we provide numerical justification that our theoretical findings are aligned with practical DQNs through the Atari Pong game, which is commonly used for DQNs in [50; 51; 73]. We take the Double DQN (DDQN) [73], one of the most popular variants of DQN, as the backbone in the setup. DDQN differs from DQN only in (8) via changing \(y=r+\gamma\cdot Q(\bm{W}^{(t,0)};\bm{s}^{\prime},a^{*})\) to \(y=r+\gamma\cdot Q(\bm{W}^{(t,m)};\bm{s}^{\prime},a^{*})\), where \(a^{*}=\operatorname*{argmax}_{a}Q(\bm{W}^{(t,0)};\bm{s}^{\prime},a)\). DDQN outperforms DQN in the relief of overoptimism and the improvement of stability. Our numerical experiments on DDQN also indicates that our analysis applies to the variants of DQN.

The input to the network is \(84\times 84\times 4\) images, where the last dimension represents the number of frames in history. The network is a convolutional neural network consisting of three convolutional layers and one fully-connected layer. The algorithm terminates if the average score over the recent \(20\) episodes does not improve or the algorithm reaches the maximum episode set as \(200\), which is around \(4\times 10^{5}\) training steps. The testing score is calculated based on a similar setup as the training process by fixing the maximum memory size \(N\) as \(2000\) and greedy policy, i.e., \(\varepsilon=0\). Each point in the plot is averaged over \(10\) experiments with an error bar representing the standard deviation.

**Estimation errors with respect to the sample complexity \(N\).** As the Q-value function is the estimate of the expected cumulative reward, we use the difference between the reward obtained from the estimated Q-value function and the maximum reward as the estimation error of the learned model to the optimal Q-value function, which is also consistent with the experiments in [50; 51; 73]. Given that the full test score in the Pong game is \(21\), we set the test error as the value of _(21 - test score)_ in each experiment. The \(\varepsilon_{t}\) in \(\varepsilon\)-greedy policy decreases geometrically from \(1\) to \(0.01\). We vary the number of samples in the replay buffer from \(400\) to \(2500\). Figure 1 shows that the test error is almost linear in \(1/\sqrt{N}\), which is consistent with our characterization in (20). In addition, experiments with a large \(N\) have a shorter error bar indicating a more stable learning performance with a large sample complexity as shown in (12).

**Convergence with different selections of \(\varepsilon\).** Figure 2 illustrates the convergence rate when \(\varepsilon_{t}\) in the \(\varepsilon\)-greedy policy changes. For each point, \(\varepsilon_{0}\) is selected as the value in the x-axis, and we decrease \(\varepsilon_{t}\) geometrically as the iteration \(t\) increases. Each point is averaged over \(5\) independent trials. We can see that the convergence rate is a linear function of \(c_{\varepsilon}\), matching our findings in (19).

**Performance with different selections of \(\varepsilon\).** We investigate the effect of \(\varepsilon\)-greedy policy during the training. In Figure 3, we show the test scores using \(\varepsilon\)-greedy policy with (1) geometrically decreasing \(\varepsilon\) from \(1\) to \(0.01\), (2) fixed \(\varepsilon\) as \(0.1\), and (3) fixed \(\varepsilon\) as \(0\). Each test score in the curve is averaged over the past \(10\) episodes to smooth the trend. One can observe that a gradually decreasing \(\varepsilon\) leads to a better score than fixing \(\varepsilon\) to be the final value. The test score of \(\varepsilon=0.1\) shares a similar trend to the \(\varepsilon\)-greedy policy but with a slower speed, matching our findings in (19) that a small \(\varepsilon\) leads to a slow convergence rate. The test score of \(\varepsilon=0\) has the fastest convergence rate at the early stage, but the convergent point is the worst and the most unstable, matching our findings in (18) that a small \(\varepsilon\) leads to a reduced radius of convergence.

## 6 Conclusions and Discussions

This paper provides the first convergence and sample complexity analysis of the DQN algorithm equipped with \(\varepsilon\)-greedy exploration. We establish the theoretical guarantee for the convergence of the learned model to the optimal Q-value function \(Q^{\star}\), which can be used to derive the optimal policy. We provide a nearly optimal sample complexity for achieving an arbitrarily small estimation error. We also prove that \(\varepsilon\)-greedy with decreasing \(\varepsilon\) achieves both an enlarged radius of convergence and an improved convergence rate. Future directions include the generalization of the theoretical analysis to the variants of DQNs and the design of efficient exploration strategies for DQNs.

One of the anonymous reviewers raised concerns about (12) regarding its demanding requirements on the initial policy. We would like to clarify that (12) primarily concerns the optimization analysis of the objective function rather than the initial policy. Instead, we impose only a minor assumption on the initial policy, with no specific environmental constraints. In our work, \(C_{0}\) quantifies the initial policy's difference from the optimal policy and is independent of (12) in our primary theoretical results. With a sufficiently large replay buffer, \(C_{0}\) can approach one, except when it equals 1, indicating an extreme divergence from the optimal policy in all states. Thus, our initial policy assumption is minimal. Considering the highly non-convex nature of deep neural network objective functions with countless local minima, (12) represents the state-of-the-art assumption for optimizing deep neural networks.

As mentioned by the anonymous reviewers, the selections of \(\varepsilon_{t}\) in (17) depends on \(e_{t}=\|\bm{W}^{(t)}-\bm{W}^{\star}\|_{2}\), which is unknown to the agent. Here, we would like to clarify that \(e_{t}\) can be replaced by its upper bound in (19) and lower bound in (20). By plugging (19) and (20) into (17), we have

\[\varepsilon_{t}=\max\Big{\{}\frac{c_{\varepsilon}\cdot\Theta(\sqrt{N})\cdot \big{(}\gamma+c_{\varepsilon}\cdot(1-\gamma)\big{)}^{t}e_{0}}{(1-C_{\max}) \cdot|\mathcal{A}|\cdot R_{\max}}-\frac{C_{\max}}{1-C_{\max}},\quad c_{ \varepsilon}\cdot\frac{C_{\max}}{1-C_{\max}}\Big{\}}.\] (26)

As mentioned by one of the anonymous reviewers, Corollary 3 relies on an assumption that depends on the algorithm's trajectory, which lacks mathematical rigor. Here, we would like to clarify that although this equation depends on the algorithm's trajectory, it can be easily derived from a time-independent equation

\[|\pi_{\bm{W}}(\bm{s}|a)-\pi^{\star}(\bm{s}|a)|\leq C||\bm{W}-\bm{W}^{\star}|| _{2}.\] (27)

Additionally, it is worth mentioning the difference between (22) in this paper and (2) in [92]. Specifically, we want to highlight that the equation above, (27), leads to (22). In contrast, (2) in [92] requires the following condition to hold for for all \(\bm{W}_{1}\) and \(\bm{W}_{2}\):

\[|\pi_{\bm{W}_{1}}(\bm{s}|a)-\pi_{\bm{W}_{2}}(\bm{s}|a)\leq C||\bm{W}_{1}-\bm{ W}_{2}||_{2}\] (28)

As a comparison, (27) only requires \(\bm{W}_{2}\) to be the ground truth and \(\bm{W}_{1}\) to be some weights near the ground truth. In other words, (27) is a sufficient condition for (22). While equation (2) in [92] does not hold with epsilon-greedy, (27) can hold with Q-learning using epsilon-greedy, thus ensuring the mathematical rigor of (22).

## Acknowledgment

This work was done when Shuai was a postdoc at Rensselaer Polytechnic Institute (RPI). We thank Dr. Tianyi Chen at RPI for providing remarkable and inspiring insight into the analysis of temporal difference learning. This work was supported by AFOSR FA9550-20-1-0122, ARO W911NF-21-1-0255, NSF 1932196, and the Rensselaer-IBM AI Research Collaboration (http://airc.rpi.edu), part of the IBM AI Horizons Network (http://ibm.biz/AIHorizons). We thank all anonymous reviewers for their constructive comments.

## References

* [1] Zeyuan Allen-Zhu and Yuanzhi Li. Feature purification: How adversarial training performs robust deep learning. In _2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS)_, pages 977-988. IEEE, 2022.
* [2] Kamyar Azizzadenesheli, Emma Brunskill, and Animashree Anandkumar. Efficient exploration through bayesian deep q-networks. In _2018 Information Theory and Applications Workshop (ITA)_, pages 1-9. IEEE, 2018.
* [3] Ainesh Bakshi, Rajesh Jayaram, and David P Woodruff. Learning two layer rectified neural networks in polynomial time. In _Conference on Learning Theory_, pages 195-268. PMLR, 2019.
* [4] Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference learning with linear function approximation. In _Conference on learning theory_, pages 1691-1692. PMLR, 2018.
* [5] Rajendra Bhatia. _Matrix analysis_, volume 169. Springer Science & Business Media, 2013.
* [6] Steven J Bradtke and Andrew G Barto. Linear least-squares algorithms for temporal difference learning. _Machine learning_, 22(1):33-57, 1996.
* [7] Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. In _Proceedings of the 34th International Conference on Machine Learning-Volume 70_, pages 605-614. JMLR. org, 2017.
* [8] Alon Brutzkus and Amir Globerson. An optimization and generalization analysis for max-pooling networks. In _Uncertainty in Artificial Intelligence_, pages 1650-1660. PMLR, 2021.
* [9] Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns over-parameterized networks that provably generalize on linearly separable data. In _International Conference on Learning Representations_, 2018.
* [10] Qi Cai, Zhuoran Yang, Jason D Lee, and Zhaoran Wang. Neural temporal-difference learning converges to global optima. _Advances in Neural Information Processing Systems_, 32, 2019.
* [11] Yichen Chen, Lihong Li, and Mengdi Wang. Scalable bilinear pi learning using state and action features. In _International Conference on Machine Learning_, pages 834-843. PMLR, 2018.
* [12] Yu Fan Chen, Michael Everett, Miao Liu, and Jonathan P How. Socially aware motion planning with deep reinforcement learning. In _2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 1343-1350. IEEE, 2017.
* [13] Zaiwei Chen, Sheng Zhang, Thinh T Doan, Siva Theja Maguluri, and John-Paul Clarke. Performance of q-learning with linear function approximation: Stability and finite-time analysis. _arXiv preprint arXiv:1905.11425_, page 4, 2019.
* [14] Girish Chowdhary, Miao Liu, Robert Grande, Thomas Walsh, Jonathan How, and Lawrence Carin. Off-policy reinforcement learning with gaussian processes. _IEEE/CAA Journal of Automatica Sinica_, 1(3):227-238, 2014.
* [15] Mohammed Nowaz Rabbani Chowdhury, Shuai Zhang, Meng Wang, Sijia Liu, and Pin-Yu Chen. Patch-level routing in mixture-of-experts is provably sample-efficient for convolutional neural networks. _arXiv preprint arXiv:2306.04073_, 2023.
* [16] Antonio Coronato, Muddasar Naeem, Giuseppe De Pietro, and Giovanni Paragliola. Reinforcement learning for intelligent healthcare applications: A survey. _Artificial Intelligence in Medicine_, 109:101964, 2020.

* Dann et al. [2022] Chris Dann, Yishay Mansour, Mehryar Mohri, Ayush Sekhari, and Karthik Sridharan. Guarantees for epsilon-greedy reinforcement learning with function approximation. In _International Conference on Machine Learning_, pages 4666-4689. PMLR, 2022.
* Dong et al. [2020] Kefan Dong, Jian Peng, Yining Wang, and Yuan Zhou. Root-n-regret for learning in markov decision processes with function approximation and low bellman rank. In _Conference on Learning Theory_, pages 1554-1557. PMLR, 2020.
* Dong et al. [2021] Kefan Dong, Jiaqi Yang, and Tengyu Ma. Provable model-based nonlinear bandit and reinforcement learning: Shelve optimism, embrace virtual curvature. _Advances in Neural Information Processing Systems_, 34:26168-26182, 2021.
* Du et al. [2020] Simon S Du, Jason D Lee, Gaurav Mahajan, and Ruosong Wang. Agnostic \(q\)-learning with function approximation in deterministic systems: Near-optimal bounds on approximation error and sample complexity. _Advances in Neural Information Processing Systems_, 33:22327-22337, 2020.
* Du et al. [2018] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. In _International Conference on Learning Representations_, 2018.
* Du et al. [2019] Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. In _International Conference on Learning Representations_, 2019.
* Duan et al. [2020] Yaqi Duan, Zeyu Jia, and Mengdi Wang. Minimax-optimal off-policy evaluation with linear function approximation. In _International Conference on Machine Learning_, pages 2701-2709. PMLR, 2020.
* Fan et al. [2020] Jianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. A theoretical analysis of deep q-learning. In _Learning for Dynamics and Control_, pages 486-489. PMLR, 2020.
* Fortunato et al. [2017] Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration. _arXiv preprint arXiv:1706.10295_, 2017.
* Ge et al. [2018] Rong Ge, Jason D. Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design. In _International Conference on Learning Representations_, 2018.
* Ishfaq et al. [2021] Haque Ishfaq, Qiwen Cui, Viet Nguyen, Alex Ayoub, Zhuoran Yang, Zhaoran Wang, Doina Precup, and Lin Yang. Randomized exploration in reinforcement learning with general value function approximation. In _International Conference on Machine Learning_, pages 4607-4616. PMLR, 2021.
* Jacot et al. [2018] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In _Proceedings of the 32nd International Conference on Neural Information Processing Systems_, 2018.
* Ji et al. [2022] Xiang Ji, Minshuo Chen, Mengdi Wang, and Tuo Zhao. Sample complexity of nonparametric off-policy evaluation on low-dimensional manifolds using deep networks. _arXiv preprint arXiv:2206.02887_, 2022.
* Jiang et al. [2017] Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual decision processes with low bellman rank are pac-learnable. In _International Conference on Machine Learning_, pages 1704-1713. PMLR, 2017.
* Jin et al. [2021] Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms. _Advances in neural information processing systems_, 34:13406-13418, 2021.
* Jin et al. [2020] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In _Conference on Learning Theory_, pages 2137-2143. PMLR, 2020.
* Jing et al. [2023] Shusen Jing, Songyang Zhang, and Zhi Ding. Reinforcement learning for robust header compression under model uncertainty. _arXiv preprint arXiv:2309.13291_, 2023.
* Kalashnikov et al. [2018] Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Scalable deep reinforcement learning for vision-based robotic manipulation. In _Conference on Robot Learning_, pages 651-673. PMLR, 2018.

* Karp et al. [2021] Stefani Karp, Ezra Winston, Yuanzhi Li, and Aarti Singh. Local signal adaptivity: Provable feature learning in neural networks beyond kernels. _Advances in Neural Information Processing Systems_, 34:24883-24897, 2021.
* Kober et al. [2013] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. _The International Journal of Robotics Research_, 32(11):1238-1274, 2013.
* Kormushev et al. [2013] Petar Kormushev, Sylvain Calinon, and Darwin G Caldwell. Reinforcement learning in robotics: Applications and real-world challenges. _Robotics_, 2(3):122-148, 2013.
* Lagoudakis and Parr [2003] Michail G Lagoudakis and Ronald Parr. Least-squares policy iteration. _The Journal of Machine Learning Research_, 4:1107-1149, 2003.
* Lee et al. [2018] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. In _International Conference on Learning Representations_, 2018.
* Lei et al. [2020] Lei Lei, Yue Tan, Kan Zheng, Shiwen Liu, Kuan Zhang, and Xuemin Shen. Deep reinforcement learning for autonomous internet of things: Model, applications and challenges. _IEEE Communications Surveys & Tutorials_, 22(3):1722-1760, 2020.
* Li et al. [2020] Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Sample complexity of asynchronous q-learning: Sharper analysis and variance reduction. _Advances in neural information processing systems_, 33:7031-7043, 2020.
* Li et al. [2023] Hongkang Li, Meng Wang, Sijia Liu, and Pin-Yu Chen. A theoretical understanding of shallow vision transformers: Learning, generalization, and sample complexity. In _The Eleventh International Conference on Learning Representations_, 2023.
* Li et al. [2022] Hongkang Li, Meng Wang, Sijia Liu, Pin-Yu Chen, and Jinjun Xiong. Generalization guarantee of training graph convolutional networks with graph topology sampling. In _International Conference on Machine Learning_, pages 13014-13051. PMLR, 2022.
* Li and Liang [2018] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. In _Advances in Neural Information Processing Systems_, pages 8157-8166, 2018.
* Li et al. [2020] Yuanzhi Li, Tengyu Ma, and Hongyang R Zhang. Learning over-parametrized two-layer neural networks beyond ntk. In _Conference on learning theory_, pages 2613-2682. PMLR, 2020.
* Liu et al. [2022] Fanghui Liu, Luca Viano, and Volkan Cevher. Understanding deep neural function approximation in reinforcement learning via \(\varepsilon\)-greedy exploration. _arXiv preprint arXiv:2209.07376_, 2022.
* Maei et al. [2010] Hamid R Maei, Csaba Szepesvari, Shalabh Bhatnagar, and Richard S Sutton. Toward off-policy learning control with function approximation. In _Proceedings of the 27th International Conference on Machine Learning (ICML-10)_, pages 719-726, 2010.
* Melo and Ribeiro [2007] Francisco S Melo and M Isabel Ribeiro. Q-learning with linear function approximation. In _International Conference on Computational Learning Theory_, pages 308-322. Springer, 2007.
* Mitrophanov [2005] A Yu Mitrophanov. Sensitivity and convergence of uniformly ergodic markov chains. _Journal of Applied Probability_, 42(4):1003-1014, 2005.
* Mnih et al. [2013] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. _arXiv preprint arXiv:1312.5602_, 2013.
* Mnih et al. [2015] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. _nature_, 518(7540):529-533, 2015.
* Modi et al. [2020] Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. Sample complexity of reinforcement learning using linearly combined model ensembles. In _International Conference on Artificial Intelligence and Statistics_, pages 2010-2020. PMLR, 2020.
* Nguyen-Tang et al. [2022] Thanh Nguyen-Tang, Sunil Gupta, A Tuan Nguyen, and Svetha Venkatesh. Offline neural contextual bandits: Pessimism, optimization and generalization. In _International Conference on Learning Representations_, 2022.

* [54] Thanh Nguyen-Tang, Sunil Gupta, Hung Tran-The, and Svetha Venkatesh. On sample complexity of offline reinforcement learning with deep reLU networks in besov spaces. _Transactions on Machine Learning Research_, 2022.
* [55] Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped dqn. _Advances in neural information processing systems_, 29, 2016.
* [56] Theodore Perkins and Doina Precup. A convergent form of approximate policy iteration. _Advances in neural information processing systems_, 15, 2002.
* [57] Boris T Polyak. Introduction to optimization. _New York: Optimization Software, Inc_, 1987.
* [58] Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration. _Advances in Neural Information Processing Systems_, 26, 2013.
* [59] Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks. In _International Conference on Machine Learning_, pages 4430-4438, 2018.
* [60] Wilko Schwarting, Javier Alonso-Mora, and Daniela Rus. Planning and decision-making for autonomous vehicles. _Annual Review of Control, Robotics, and Autonomous Systems_, 1:187-210, 2018.
* [61] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement learning for autonomous driving. _arXiv preprint arXiv:1610.03295_, 2016.
* [62] Zhenmei Shi, Junyi Wei, and Yingyu Liang. A theoretical analysis on feature learning in neural networks: Emergence from inputs and advantage over fixed features. In _International Conference on Learning Representations_, 2022.
* [63] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. _nature_, 529(7587):484-489, 2016.
* [64] Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. _IEEE Transactions on Information Theory_, 65(2):742-769, 2018.
* [65] Zhao Song, Ronald E Parr, Xuejun Liao, and Lawrence Carin. Linear feature encoding for reinforcement learning. _Advances in neural information processing systems_, 29, 2016.
* [66] Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* [67] Richard S Sutton, Hamid Maei, and Csaba Szepesvari. A convergent \(o(n)\) temporal-difference algorithm for off-policy learning with linear function approximation. _Advances in neural information processing systems_, 21, 2008.
* [68] Taiji Suzuki. Adaptivity of deep reLU network for learning in besov and mixed smooth besov spaces: optimal rate and curse of dimensionality. In _International Conference on Learning Representations_, 2019.
* [69] Manel Tagorti and Bruno Scherrer. On the rate of convergence and error bounds for lstd (\(\lambda\)). In _International Conference on Machine Learning_, pages 1521-1529. PMLR, 2015.
* [70] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R Bowman, Dipanjan Das, et al. What do you learn from context? probing for sentence structure in contextualized word representations. In _International Conference on Learning Representations_, 2018.
* [71] William R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. _Biometrika_, 25(3-4):285-294, 1933.
* [72] Joel A Tropp. User-friendly tail bounds for sums of random matrices. _Foundations of computational mathematics_, 12(4):389-434, 2012.
* [73] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In _Proceedings of the AAAI conference on artificial intelligence_, 2016.
* [74] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. _arXiv preprint arXiv:1011.3027_, 2010.

* [75] Gang Wang, Georgios B Giannakis, and Jie Chen. Learning relu networks on linearly separable data: Algorithm, optimality, and generalization. _IEEE Transactions on Signal Processing_, 67(9):2357-2370, 2019.
* [76] Yulong Wang, Haoxin Zhang, and Guangwei Zhang. cpso-cnn: An efficient pso-based algorithm for fine-tuning hyper-parameters of convolutional neural networks. _Swarm and Evolutionary Computation_, 49:114-123, 2019.
* [77] Christopher JCH Watkins and Peter Dayan. Q-learning. _Machine learning_, 8(3):279-292, 1992.
* [78] Zixin Wen and Yuanzhi Li. Toward understanding the feature learning process of self-supervised contrastive learning. In _International Conference on Machine Learning_, pages 11112-11122. PMLR, 2021.
* [79] Pan Xu and Quanquan Gu. A finite-time analysis of q-learning with neural network function approximation. In _International Conference on Machine Learning_, pages 10555-10565. PMLR, 2020.
* [80] Lin Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive features. In _International Conference on Machine Learning_, pages 6995-7004. PMLR, 2019.
* [81] Zhuoran Yang, Chi Jin, Zhaoran Wang, Mengdi Wang, and Michael I Jordan. On function approximation in reinforcement learning: optimism in the face of large state spaces. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_, pages 13903-13916, 2020.
* [82] Ming Yin, Yu Bai, and Yu-Xiang Wang. Near-optimal provable uniform convergence in offline policy evaluation for reinforcement learning. In _International Conference on Artificial Intelligence and Statistics_, pages 1567-1575. PMLR, 2021.
* [83] Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near optimal policies with low inherent bellman error. In _International Conference on Machine Learning_, pages 10978-10989. PMLR, 2020.
* [84] Shuai Zhang, Meng Wang, Pin-Yu Chen, Sijia Liu, Songtao Lu, and Miao Liu. Joint edge-model sparse learning is provably efficient for graph neural networks. In _The Eleventh International Conference on Learning Representations_, 2023.
* [85] Shuai Zhang, Meng Wang, Sijia Liu, Pin-Yu Chen, and Jinjun Xiong. Fast learning of graph neural networks with guaranteed generalizability:one-hidden-layer case. In _2020 International Conference on Machine Learning (ICML)_, 2020.
* [86] Shuai Zhang, Meng Wang, Sijia Liu, Pin-Yu Chen, and Jinjun Xiong. Why lottery ticket wins? a theoretical perspective of sample complexity on pruned neural networks. In _Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS)_, 2021.
* [87] Shuai Zhang, Meng Wang, Sijia Liu, Pin-Yu Chen, and Jinjun Xiong. How unlabeled data improve generalization in self-training? a one-hidden-layer theoretical analysis. In _International Conference on Learning Representations_, 2022.
* [88] Shuai Zhang, Meng Wang, Jinjun Xiong, Sijia Liu, and Pin-Yu Chen. Improved linear convergence of training cnns with generalizability guarantees: A one-hidden-layer case. _IEEE Transactions on Neural Networks and Learning Systems_, 2020.
* [89] Yuchen Zhang, Jason D. Lee, and Michael I. Jordan. L1-regularized neural networks are improperly learnable in polynomial time. In _Proceedings of The 33rd International Conference on Machine Learning_, volume 48, pages 993-1001, 2016.
* [90] Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for one-hidden-layer neural networks. In _Proceedings of the 34th International Conference on Machine Learning-Volume 70_, pages 4140-4149. JMLR. org, https://arxiv.org/abs/1706.03175, 2017.
* [91] Dongruo Zhou, Jiafan He, and Quanquan Gu. Provably efficient reinforcement learning for discounted mdps with feature mapping. In _International Conference on Machine Learning_, pages 12793-12802. PMLR, 2021.
* [92] Shaofeng Zou, Tengyu Xu, and Yingbin Liang. Finite-sample analysis for sarsa with linear function approximation. _Advances in neural information processing systems_, 32, 2019.

**Supplementary Materials for:**

On the Convergence and Sample Complexity Analysis of

Deep Q-Network with Epsilon-Greedy Exploration

The structure of the appendix mainly follows the roadmap of the proof described in Section 4.4.

In Appendix A, we define the characterizable population risk function in (31) to approximate the objective function. Also, some notations to simplify the analysis are introduced in Appendix A, and we recommend the readers to refer to Table 3 for the major notations used in the proofs.

In Appendix B, we provide the proof for Lemma 1 and Theorem 1 following the steps as (1) Characterization of the local convex region of population risk function (Lemma 2), (2) Characterization of the distance between the population risk function and the objective function (Lemma 3), (3) Characterization of the convergence of two consecutive iterations \(\bm{W}^{(t,m+1)}\) and \(\bm{W}^{(t,m)}\), and (4) Mathematical induction over the \(t\) and \(m\) to obtain the error bound between the convergent point \(\bm{W}^{(T,0)}\) and the desired point \(\bm{W}^{\star}\).

In Appendix C, we provide the preliminary lemmas and the whole proof for Lemma 2, which characterizes the local convex region of the non-convex population risk function.

In Appendix D, we provide the preliminary lemmas and the whole proof for Lemma 3, which characterizes the difference of \(g_{t}\) and the gradient descent of defined population risk function in (31).

In Appendix E, we provide the proofs for the preliminary lemmas in proving Lemmas 2 and 3.

Before moving to the details, we provide an overview of the techniques in the proofs.

**(P1.) The local convex region near \(\bm{W}^{\star}\).** To characterize the local convex region, we first bound the Hessian matrix of the defined population risk function in (31) at \(\bm{W}^{\star}\). Then, we derive the changes in the Hessian matrix when the neuron weights move around the \(\bm{W}^{\star}\). Specifically, we prove that when neuron weights \(\bm{W}\) are not far away \(\bm{W}^{\star}\), then the Hessian matrix in this region is always positive-definite, indicating that a local convex region near \(\bm{W}^{\star}\). [90] considers the one-hidden-layer neural network, and the lower bound of the Hessian matrix only holds for Gaussian input. Instead, in this paper, we consider multi-layer cases and need to derive a lower bound for the Hessian matrix for all the layers. Instead, the input of the intermediate layer cannot be proved to be Gaussian but belong to sub-Gaussian distribution. Therefore, we built the proof for the lower bound of the Hessian matrix when the input belongs to the sub-Gaussian distribution. Compared with Gaussian input, Sub-Gaussian does not have a closed form of the probability density function. Instead of directly calculating the lower bound, we convert the problem into proving a series of functions are linearly independent over a Hilbert space (see Lemma 7 and the proof in Appendix E). Instead of directly calculating the distance of the population risk function in different points, we characterize a Gaussian variable such that the distance over the sub-Gaussian distribution can be upper bounded by the one over the Gaussian variable (see Lemma 6 and the proof in Appendix E).

**(P2.) The difference between the gradient \(g_{t}\) and the population risk function.** With the local convex region of the population risk function, we can characterize the convergence of the population risk function. With Lemma 3, we can prove that the distance between the population risk function and \(g_{t}\) is small enough, the behaviors of the iterations via \(g_{t}\) can be described by the ones in the population risk function with some additional error terms. Compared with the proof in [90], We need to address the extension from supervised learning settings to Q learning settings and the extension from the one-hidden-layer neural networks to the multi-layer neural networks. First, similar to challenges in (P1), we provide a new concentration bound to characterize the distance between the two functions for the intermediate layers (see \(I_{1}\) in the proof of Lemma 3). Second, the distance between the two functions has an additional error term due to the inconsistency of the label defined in (31) and (8) (see \(I_{2}\) in the proof of Lemma 3). Third, we need to develop a new concentration bound to characterize the error term caused by the distribution shift when training samples are collected by \(\varepsilon\)-greedy policy (see \(I_{3}\) in the proof of Lemma 3).

**(P3.) The convergence analysis of Algorithm 1.** When the initialization is not far away from \(\bm{W}^{\star}\), the initialization lies in the local convex region of \(\bm{W}^{\star}\) for the population risk function. When we have enough samples \(N\) and a large enough \(\varepsilon_{t}\), we can guarantee that the distance between the \(g_{t}\) and the gradient of the population risk function is small enough such that the iterations following \(g_{t}\) converges to a point nearby \(\bm{W}^{\star}\) as well. However, if \(\varepsilon_{t}\) is too large, the convergent point nearby \(\bm{W}^{\star}\) can be even worse than the initial point. To avoid this issue, we have an upper bound for selecting \(\varepsilon_{t}\), and the upper bound decreases as \(\|\bm{W}^{(\ell,0)}-\bm{W}^{\star}\|\) decreases over \(t\). Therefore, we build the convergence analysis of Algorithm 1.

## Appendix A Definitions and Notations

In this section, we implement the details of algorithms described in Algorithm 1, and some important notations are defined to simplify the presentation of the proof.

### Definition of the Empirical Risk Function and Its Corresponding Notations

Recall that the goal of \(Q\)-learning is to find the \(Q^{\star}\)-function to minimize (6). Therefore, we have

\[Q^{\star}(\bm{s},a)=r(\bm{s},a)+\gamma\cdot\mathbb{E}_{\bm{s}^{\prime}|\bm{s},a}\max_{a^{\prime}\in\mathcal{A}}Q^{\star}(\bm{s}^{\prime},a^{\prime})\quad \text{for}\quad(\bm{s},a)\sim\mu^{\star}.\] (29)

Since \(\bm{W}^{\star}\) is the global minimal to (6), we have

\[Q(\bm{W}^{\star};\bm{s},a)=r(\bm{s},a)+\gamma\cdot\mathbb{E}_{\bm{s}^{\prime} |\bm{s},a}\max_{a^{\prime}\in\mathcal{A}}Q(\bm{W}^{\star};\bm{s}^{\prime},a^{ \prime}).\] (30)

Therefore, the population risk function is defined as

\[\begin{split} f(\bm{W})&=\mathbb{E}_{(\bm{s},a) \sim\mu^{\star}}\big{[}Q(\bm{W};\bm{s},a)-r(\bm{s},a)-\gamma\cdot\mathbb{E}_{ \bm{s}^{\prime}|\bm{s},a}\max_{a^{\prime}\in\mathcal{A}}Q(\bm{W}^{\star};\bm{ s}^{\prime},a^{\prime})\big{]}^{2}\\ &=\mathbb{E}_{(\bm{s},a)\sim\mu^{\star}}\big{[}Q(\bm{W};\bm{s},a) -Q(\bm{W}^{\star};\bm{s},a)\big{]}^{2},\end{split}\] (31)

where \(\mu^{\star}\) is the distribution of the sampled data following the optimal policy \(\pi^{\star}\).

The gradient of the (31) is

\[\begin{split}\nabla_{\bm{W}}f(\bm{W})=&\mathbb{E}_ {\bm{x}\sim\mu^{\star}}\big{(}Q(\bm{W};\bm{x})-r(\bm{x})-\gamma\cdot\mathbb{E} _{\bm{s}^{\prime}\sim p^{\star}_{\bm{s}^{\prime}}}\max_{a^{\prime}\in\mathcal{ A}}Q^{\star}(\bm{s}^{\prime},a^{\prime})\big{)}\cdot\nabla_{\bm{W}}Q(\bm{W};\bm{x}) \\ =&\mathbb{E}_{\bm{x}\sim\mu^{\star},\bm{s}^{\prime} \sim p^{\star}_{\bm{s}^{\prime}}}\big{(}Q(\bm{W};\bm{x})-r(\bm{x})-\gamma \cdot\max_{a^{\prime}\in\mathcal{A}}Q(\bm{W}^{\star};\bm{s}^{\prime},a^{\prime })\big{)}\cdot\nabla_{\bm{W}}Q(\bm{W};\bm{x}).\end{split}\] (32)

As \(\bm{W}^{\star}\) is one of the ground truths to \(f(\bm{W})\), i.e., \(f(\bm{W}^{\star})\) achieves the minimum value as \(f(\bm{W}^{\star})=0\leq f(\bm{W})\) for any other \(\bm{W}\). Given \(f\) is a smooth function, we have the gradient of \(f\) with respect to any \(\bm{W}_{\ell}\) at the ground truth \(\bm{W}^{\star}\) equals to zero, namely,

\[\nabla_{\ell}f(\bm{W}^{\star}):=\nabla_{\bm{W}_{\ell}}f(\bm{W}^{\star})=\bm{0},\qquad\forall\ell\in[L].\] (33)

In addition, without special descriptions, \(\bm{\alpha}=[\bm{\alpha}_{1}^{\top},\bm{\alpha}_{2}^{\top},\cdots,\bm{\alpha }_{K}^{\top}]^{\top}\) stands for any unit vector that in \(\mathbb{R}^{K_{\ell}K_{\ell-1}}\) with \(\bm{\alpha}_{j}\in\mathbb{R}^{K}_{\ell-1}\) (\(K_{0}=d\)). Therefore, we have

\[\begin{split}\|\nabla_{\ell}h\|_{2}=\max_{\bm{\alpha}}\|\bm{ \alpha}^{\top}\nabla_{\ell}h\|_{2}&=\max_{\bm{\alpha}}\Big{|} \sum_{j=1}^{K}\bm{\alpha}_{j}^{\top}\frac{\partial h}{\partial\bm{w}_{\ell,j}} \Big{|},\\ \|\nabla_{\ell}^{2}h\|_{2}=\max_{\bm{\alpha}}\|\bm{\alpha}^{\top} \nabla_{\ell}^{2}\ h\ \bm{\alpha}\|_{2}&=\max_{\bm{\alpha}}\Big{(}\sum_{j=1}^{K}\bm{ \alpha}_{j}^{\top}\frac{\partial h}{\partial\bm{w}_{\ell,j}}\Big{)}^{2}.\end{split}\] (34)

### Notations in Algorithm 1

Recall that the gradient in the \(t\)-th loop is

\[\begin{split} g_{t}(\bm{W})=&\frac{1}{|\mathcal{D}_{t }^{(m)}|}\sum_{n\in\mathcal{D}_{t}^{(m)}}(Q(\bm{W};\bm{x}_{n})-y_{n}^{(t)})\cdot \nabla_{\bm{W}}Q(\bm{W};\bm{x}_{n})\\ =&\frac{1}{N}\sum_{n=1}^{N}\big{(}Q(\bm{W};\bm{x}_{n}) -r(\bm{x}_{n})-\gamma\cdot\max_{a^{\prime}\in\mathcal{A}}Q(\bm{W}^{(t-1)};\bm{s }^{\prime}_{n},a^{\prime})\big{)}\cdot\nabla_{\bm{W}}Q(\bm{W};\bm{x}_{n}). \end{split}\] (35)Then, we define \(g_{t}^{(m)}(\bm{W}_{\ell};\bm{W})\) as the components of \(g_{t}^{(m)}(\bm{W})\) with respect to \(\bm{W}_{\ell}\). Recall that in (4) we have

\[\bm{W}=[\text{vec}(\bm{W}_{1})^{\top},\quad\text{vec}(\bm{W}_{2})^{\top},\quad \cdots,\quad\text{vec}(\bm{W}_{L})^{\top}]^{\top}.\] (36)

Then, with the definition of \(g_{t}^{(m)}(\bm{W}_{\ell};\bm{W})\), we have

\[g_{t}^{(m)}(\bm{W})=[g_{t}^{(m)}(\bm{W}_{1};\bm{W})^{\top},\quad g_{t}^{(m)}( \bm{W}_{2};\bm{W})^{\top},\quad\cdots,\quad g_{t}^{(m)}(\bm{W}_{L};\bm{W})^{ \top}]^{\top}.\] (37)

To simplify the analysis, the update of \(\bm{W}^{(t,m)}\) is analyzed in the form of

\[\bm{W}_{\ell}^{(t,m+1)}=\bm{W}_{\ell}^{(t,m)}-\eta\cdot g_{t}^{(m)}(\bm{W}_{ \ell};\bm{W}^{(t,m)})+\beta(\bm{W}_{\ell}^{(t,m)}-\bm{W}_{\ell}^{(t,m-1)}), \quad\forall\ell\in[L].\] (38)

One can see that (38) returns the same \(\bm{W}^{(t,m+1)}\) as the gradient step at line 9 in Algorithm 1.

### Notations for the Deep Neural Networks.

Let \(n\) denote the dimension of \(\bm{W}\) defined in (4). We denote \(n_{l}\) as the dimension of the vectorized neuron weights in the \(\ell\)-th layer, namely, \(n_{\ell}=\text{dim}(\text{vec}(\bm{W}_{\ell}))\).

Then, let \(h^{(\ell)}(\bm{W})\) denote the input in the \(\ell\)-th layer (or the output in the \((\ell-1)\)-th layer) with respect the neuron weights as \(\bm{W}\), and \(h^{(1)}=(\bm{s},a)\), where

\[\bm{h}^{(\ell)}(\bm{W})=\phi(\bm{W}_{\ell-1}^{\top}\bm{h}^{(\ell-1)})=\cdots= \phi\Big{(}\bm{W}_{\ell}^{\top}\phi\big{(}\bm{W}_{\ell-1}\cdots\phi(\bm{W}_{1 }^{\top}\bm{x})\big{)}\Big{)}.\] (39)

\begin{table}
\begin{tabular}{c|l} \hline \hline \(g_{t}(\bm{W})\) & The gradient function at point \(\bm{W}\) in the \(t\)-th outer loop, defined in (7). \\ \hline \(g_{t}(\bm{W}_{\ell};\bm{W})\) & The gradient function of \(g_{t}(\bm{W})\) with respect to the components of \(\bm{W}_{\ell}\). \\ \hline \(d\) & Dimension of the feature mappings of the state-action pair \((\bm{s},a)\in\mathcal{S}\times\mathcal{A}\). \\ \hline \(K\) & Number of neurons in the hidden layer. \\ \hline \(L\) & Number of hidden layers. \\ \hline \(\bm{W}^{\star}\) & The desired Weights for approximating the optimal Q function. \\ \hline \(\bm{W}^{(t,m)}\) & Model returned by Algorithm 1 at \(t\)-th outer loop and \(m\)-th inner loop. \\ \hline \(f\) & The population risk function defined in (31). \\ \hline \(\nabla_{W}f(\bm{W}^{\star})\) & The full gradient of a function \(f\) at point \(\bm{W}^{\star}\). \\ \hline \(\nabla_{\ell}f(\bm{W}^{\star})\) & The gradient of a function \(f\) with respect to the components of \(\bm{W}_{\ell}\) at point \(\bm{W}^{\star}\). \\ \hline \(\nabla_{\ell}^{2}f(\bm{W}^{\star})\) & The Hessian matrix of a function \(f\) with respect to the components of \(\bm{W}_{\ell}\) at point \(\bm{W}^{\star}\). \\ \hline \(n\) & The dimension of \(\bm{W}\). \\ \hline \(n_{\ell}\) & The dimension of vectorized \(\bm{W}_{\ell}\). \\ \hline \(\bm{h}^{(\ell)}(\bm{W})\) & The input to the \(\ell\)-th layer, defined in (39). \\ \hline \(K_{\ell}\) & The dimension of \(\bm{h}^{(\ell)}\). \\ \hline \(\mathcal{J}_{\ell}(\bm{W})\) & A function in \(\mathbb{R}^{n}\longrightarrow\mathbb{R}^{K}\), defined in (42). \\ \hline \(\varepsilon_{t}\) & The value of \(\varepsilon\) in the behavior policy at \(t\)-th outer loop. \\ \hline \(C_{t}\) & The distribution shift between the optimal policy and behavior policy at iteration \(t\). \\ \hline \(N\) & The size of the experience replay buffer. \\ \hline \(R_{\max}\) & The upper bound of the reward. \\ \hline \hline \end{tabular}
\end{table}
Table 3: Notations for the proofs\(\bm{h}^{(\ell)}(\bm{W})\) may be shortened as \(\bm{h}^{(\ell)}\) when the neuron weights are clear from the contexts. Then, we denote the dimension of \(\bm{h}^{(\ell)}\) as \(K_{\ell}\), where

\[K_{\ell}=\begin{cases}K,&\text{if}\quad\ell>1\\ d,&\text{if}\quad\ell=1.\end{cases}\] (40)

Then, \(Q(\bm{W};\bm{s},a)\) can be written as

\[Q(\bm{W};\bm{s},a)=\frac{\bm{1}^{\top}}{K}\phi(\bm{w}_{L,k}^{\top}\bm{h}^{(L)} )=\frac{\bm{1}^{\top}}{K}\phi\big{(}\bm{W}_{L}^{\top}\phi(\bm{W}_{L-1}^{\top} \bm{h}^{(L-1)})\big{)},\] (41)

where \(\bm{w}_{\ell,k}\) denotes the \(k\)-th neuron weights in the \(\ell\)-th layer. Then, we define a group of functions \(\mathcal{J}_{\ell}(\bm{W})\in\mathbb{R}^{n}\longrightarrow\mathbb{R}^{K}\) such that

\[\mathcal{J}_{\ell}(\bm{W})=\begin{cases}\left[\bm{1}^{\top}\phi^{\prime}(\bm {W}_{L}^{\top}\bm{h}^{(L)})\bm{W}_{L}^{\top}\cdot\phi^{\prime}(\bm{W}_{L-1}^{ \top}\bm{h}^{(L-1)})\bm{W}_{L-1}^{\top}\cdots\phi^{\prime}(\bm{W}_{\ell+1}^{ \top}\bm{h}^{(\ell+1)})\bm{W}_{\ell+1}^{\top}\right]^{\top}&\text{if}\quad \ell>1\\ \bm{1}&\text{if}\quad\ell=1.\end{cases}\] (42)

Then, the gradient of \(Q\) can be represented as

\[\frac{\partial Q}{\partial\bm{w}_{\ell,k}}(\bm{W})=\frac{1}{K}\mathcal{J}_{ \ell,k}(\bm{W})\phi^{\prime}\big{(}\bm{w}_{\ell,k}^{\top}\bm{h}^{(\ell)}(\bm {W}))\bm{h}^{(\ell)}(\bm{W}),\] (43)

where \(\mathcal{J}_{\ell,k}\) stands for the \(k\)-th component of \(\mathcal{J}_{\ell}\).

### Notations for Order-wise Analysis

Without loss of generality, we consider the case that \(d\gg K\). If \(K\gg d\), we can always switch the order of \(K\) and \(\bar{d}\) in the proof. Let \(\sigma_{i}(L)\) denote the \(i\)-th largest singular value of \(\bm{W}_{L}^{\star}\). In this paper, we consider the case that \(\bm{W}_{L}^{\star}\) is will-conditioned and bounded, i.e., \(\sigma_{1}(L)\) and \(\sigma_{1}(L)/\sigma_{K}(L)\) can be viewed as the constant and will be ignored in the analysis. In addition, some constant numbers will be ignored in most steps. In particular, we use \(h_{1}(z)\gtrsim(\text{or}\ \lesssim,\overline{\infty})h_{2}(z)\) to denote there exists some positive constant \(C\) such that \(h_{1}(z)\geq(\text{or}\ \leq,=)C\cdot h_{2}(z)\) when \(z\in\mathbb{R}\) is sufficiently large.

## Appendix B Proof of Lemma 1 and Theorem 1

The main idea in proving Theorem 1 is to characterize the gradient descent term by the _Mean Value Theorem_ (MVT) in Lemma 4 as shown in (47) and (48). The MVT is not directly applied in \(g_{t}\) because it is not smooth. However, the population risk functions defined in (31), which are the expectations over random variables, are smooth. Lemma 2 characterizes the bounds of the Hessian matrix defined in (49). Lemma 3 characterizes the bounds of gradient differences between the population risk function defined in (31) and \(g_{t}\) in (7) as shown in (60). Furthermore, according to Lemma 3, we know that the distance \(\|\nabla_{\ell}f(\bm{W})-\nabla_{\ell}f(\bm{W}^{\star})\|_{2}\) is upper bounded in the order of \(\|\bm{W}-\bm{W}^{\star}\|_{2}\) as shown in (60). Then, we can establish the connection between \(\|\bm{W}^{(t,m+1)}-\bm{W}^{\star}\|_{2}\) and \(\|\bm{W}^{(t,m)}-\bm{W}^{\star}\|_{2}\) as shown in (59). Then, by mathematical induction over \(m\), one can characterize the iteration of \(\{\|\bm{W}^{(t,0)}-\bm{W}^{\star}\|_{2}\}_{t=1}^{T}\) as shown in (65), which completes the proof of Lemma 1. Finally, selecting \(\varepsilon_{t}\) based on (68) for all \(t\in[T]\), we derive the error bound of \(\|\bm{W}^{(T,0)}-\bm{W}^{\star}\|_{2}\) by mathematical induction over \(t\), which completes the proof of Theorem 1.

**Lemma 2**.: _Given any \(\bm{W}\in\mathbb{R}^{n}\), let \(\bm{W}\) satisfy_

\[\|\bm{W}-\bm{W}^{\star}\|_{2}\lesssim\frac{\rho\cdot c_{\bar{I}}\cdot\sigma_{K }}{K}\] (44)

_for some constant \(c_{\bar{I}}\in(0,1)\). Then, for the \(f\) defined in (31), we have_

\[\frac{(1-c_{\bar{I}})\rho}{K^{2}}\preceq\nabla_{\ell}^{2}f(\bm{W})\preceq \frac{7}{K}.\] (45)

**Lemma 3**.: _Let \(f\) be the function defined in (31). Let \(g_{t}\) be the function defined in (7). Then, we have_

\[\|\nabla_{\ell}f(\bm{W})-g_{t}(\bm{W}_{\ell};\bm{W})\|_{2} \lesssim\frac{2-\varepsilon_{t}}{K}\sqrt{\frac{K_{\ell}\cdot\log q }{N}}\cdot\|\bm{W}-\bm{W}^{\star}\|_{2}\] \[\quad+\frac{(1-\varepsilon_{t}/2)\cdot\gamma}{K}\cdot\|\bm{W}^{(t,0 )}-\bm{W}^{\star}\|_{2}\] (46) \[\quad+C_{d}\cdot\big{(}C_{t}+(1-C_{t})\varepsilon\big{)}\cdot\frac{ R_{\max}}{1-\gamma}.\]_with probability at least \(1-q^{-K_{\ell}}\)._

**Lemma 4** (Mean Value Theorem).: _Let \(\bm{U}\subset\mathbb{R}^{d_{1}}\) be open and \(\bm{f}:\bm{U}\longrightarrow\mathbb{R}^{d_{2}}\) be continuously differentiable, and \(\bm{x}\in\bm{U}\), \(\bm{h}\in\mathbb{R}^{d_{1}}\) vectors such that the line segment \(\bm{x}+t\bm{h}\), \(0\leq t\leq 1\) remains in \(\bm{U}\). Then we have:_

\[\bm{f}(\bm{x}+\bm{h})-\bm{f}(\bm{x})=\left(\int_{0}^{1}\nabla\bm{f}(\bm{x}+t \bm{h})dt\right)\cdot\bm{h},\]

_where \(\nabla\bm{f}\) denotes the Jacobian matrix of \(\bm{f}\)._

Proof of Theorem 1.: Let \(\bm{W}_{\ell}\) denote the neuron weights in the \(\ell\)-th layer. From Algorithm 1 and (38), in the \(s\)-th iteration and \(t\)-th episode, we have

\[\bm{W}_{\ell}^{(t,m+1)}= ~{}\bm{W}_{\ell}^{(t,m)}-\eta g_{t}^{(m)}(\bm{W}_{\ell};\bm{W}^{ (t,m)})+\beta(\bm{W}_{\ell}^{(t,m)}-\bm{W}_{\ell}^{(t,m-1)})\] (47) \[= ~{}\bm{W}_{\ell}^{(t,m)}-\eta\nabla_{\ell}f(\bm{W}^{(t,m)})+\beta (\bm{W}_{\ell}^{(t,m)}-\bm{W}_{\ell}^{(t,m-1)})\] \[~{}~{}+\eta\cdot\big{(}\nabla_{\ell}f(\bm{W}^{(t,m)})-g_{t}^{(m)} (\bm{W}_{\ell};\bm{W}^{(t,m)})\big{)}.\]

From (31), we can see that \(\bm{W}^{\star}\) is the global optimal to \(f\) because \(f(\bm{W}^{\star})\) achieves the minimum value as \(0\). Therefore, we have \(\nabla_{\ell}f_{t}(\bm{W}^{\star})=\bm{0}\). Since \(\nabla_{\ell}f\) is a smooth function \(\bm{W}^{\star}\), from the _Mean Value Theorem_ in Lemma 4, we have

\[\nabla_{\ell}f(\bm{W}^{(t,m)})= \nabla_{\ell}f(\bm{W}^{(t,m)})-\nabla_{\ell}f(\bm{W}^{\star})\] (48) \[= \int_{0}^{1}\nabla_{\ell}^{2}f\Big{(}\bm{W}^{(t,m)}+u\cdot(\bm{W }^{(t,m)}-\bm{W}^{\star})\Big{)}du\cdot(\bm{W}_{\ell}^{(t,m)}-\bm{W}_{\ell}^{ \star}).\]

For notational convenience, we use \(\bm{H}\) to denote the integration as

\[\bm{H}:=\int_{0}^{1}\nabla_{\ell}^{2}f\Big{(}\bm{W}^{(t,m)}+u\cdot(\bm{W}^{(t,m)}-\bm{W}^{\star})\Big{)}du.\] (49)

Then, we have

\[\begin{split}\begin{bmatrix}\bm{W}^{(t,m+1)}-\bm{W}^{\star}\\ \bm{W}^{(t,m)}-\bm{W}^{\star}\end{bmatrix}=&\begin{bmatrix}\bm{I}-\eta\bm{H }&\beta\bm{I}\\ \bm{I}&\bm{0}\end{bmatrix}\begin{bmatrix}\bm{W}^{(t,m)}-\bm{W}^{\star}\\ \bm{W}^{(t,m-1)}-\bm{W}^{\star}\end{bmatrix}\\ &+\eta\begin{bmatrix}\nabla_{\ell}f(\bm{W}^{(t,m)})-g_{t}^{(m)}(\bm{W}_{\ell}; \bm{W}^{(t,m)})\\ \bm{0}\end{bmatrix}.\end{split}\] (50)

Let \(\bm{H}=\bm{S}\bm{\Lambda}\bm{S}^{T}\) be the eigen-decomposition of \(\bm{H}\). Then, we define

\[\bm{A}(\beta):=\begin{bmatrix}\bm{S}^{\top}&\bm{0}\\ \bm{0}&\bm{S}^{\top}\end{bmatrix}\bm{A}(\beta)\begin{bmatrix}\bm{S}&\bm{0}\\ \bm{0}&\bm{S}\end{bmatrix}=\begin{bmatrix}\bm{I}-\eta\bm{\Lambda}+\beta\bm{I}& \beta\bm{I}\\ \bm{I}&0\end{bmatrix}.\] (51)

Since \(\begin{bmatrix}\bm{S}&\bm{0}\\ \bm{0}&\bm{S}\end{bmatrix}\begin{bmatrix}\bm{S}^{\top}&\bm{0}\\ \bm{0}&\bm{S}^{\top}\end{bmatrix}=\begin{bmatrix}\bm{I}&\bm{0}\\ \bm{0}&\bm{I}\end{bmatrix}\), we know \(\bm{A}(\beta)\) and \(\begin{bmatrix}\bm{I}-\eta\bm{\Lambda}+\beta\bm{I}&\beta\bm{I}\\ \bm{I}&0\end{bmatrix}\) share the same eigenvalues. Let \(\lambda_{i}^{(\bm{\Lambda})}\) be the \(i\)-th eigenvalue of \(\bm{H}_{i}^{(\ell)}\), then the corresponding \(i\)-th eigenvalue of (51), denoted by \(\lambda_{i}^{(\bm{A})}\), satisfies

\[(\lambda_{i}^{(\bm{A})}(\beta))^{2}-(1-\eta\lambda_{i}^{(\bm{\Lambda})}+\beta) \lambda_{i}^{(\bm{A})}(\beta)+\beta=0.\] (52)

By simple calculation, we have

\[|\lambda_{i}^{(\bm{A})}(\beta)|=\begin{cases}\sqrt{\beta},\qquad\text{if} \quad\beta\geq\big{(}1-\sqrt{\eta\lambda_{i}^{(\bm{\Lambda})}}\big{)}^{2},\\ \frac{1}{2}\bigg{|}(1-\eta\lambda_{i}^{(\bm{\Lambda})}+\beta)+\sqrt{(1-\eta \lambda_{i}^{(\bm{\Lambda})}+\beta)^{2}-4\beta}\bigg{|},\text{otherwise}.\end{cases}\] (53)

Specifically, we have

\[\lambda_{i}^{(\bm{A})}(0)>\lambda_{i}^{(\bm{A})}(\beta),\quad\text{for}\quad \forall\beta\in\big{(}0,(1-\eta\lambda_{i}^{(\bm{\Lambda})})^{2}\big{)},\] (54)and \(\lambda_{i}^{(\bm{A})}\) achieves the minimum \(\lambda_{i}^{(\bm{A})\star}=\left|1-\sqrt{\eta\lambda_{i}^{(\bm{\Lambda})}}\right|\) when \(\beta^{\star}=\left(1-\sqrt{\eta\lambda_{i}^{(\bm{\Lambda})}}\right)^{2}\). From Lemma 2, for any \(\bm{a}\in\mathbb{R}^{d}\) with \(\|\bm{a}\|_{2}=1\), we have

\[\begin{split}\bm{a}^{\top}\nabla_{\ell}f(\bm{W}^{(t,m)})\bm{a}& =\int_{0}^{1}\bm{a}^{\top}\nabla_{\ell}^{2}f\Big{(}\bm{W}^{(t,m)} +u\cdot(\bm{W}^{(t,m)}-\bm{W}^{\star})\Big{)}\bm{a}\cdot du\\ &\leq\int_{0}^{1}\lambda_{\max}\|\bm{a}\|_{2}^{2}du=\lambda_{\max },\\ \bm{a}^{\top}\nabla_{\ell}f(\bm{W}^{(t,m)})\bm{a}& =\int_{0}^{1}\bm{a}^{\top}\nabla_{\ell}^{2}f\Big{(}\bm{W}^{(t,m)} +u\cdot(\bm{W}^{(t,m)}-\bm{W}^{\star})\Big{)}\bm{a}\cdot du\\ &\geq\int_{0}^{1}\lambda_{\min}\|\bm{a}\|_{2}^{2}du=\lambda_{ \min},\end{split}\] (55)

where \(\lambda_{\max}\approx\frac{1}{K}\), and \(\lambda_{\min}\approx\frac{\rho}{K^{2}}\). Therefore, we have

\[\lambda_{\min}^{(\bm{\Lambda})}\approx\frac{(1-c_{I})\rho}{K^{2}},\quad\text{ and}\quad\lambda_{\max}^{(\bm{\Lambda})}\approx\frac{1}{K}.\] (56)

Thus, when \(\eta\leq\frac{1}{2\lambda_{\max}^{(\bm{\Lambda})}}\lesssim K\), \(\|\bm{A}(\beta^{\star})\|_{2}\) can be bounded by

\[\|\bm{A}(\beta^{\star})\|_{2}= 1-\sqrt{\eta\cdot\lambda_{\min}^{(\bm{\Lambda})}}\leq 1-\sqrt{ \frac{(1-c_{I})\eta\rho}{K^{2}}}.\] (57)

Therefore, we have

\[\begin{split}\|\bm{W}_{\ell}^{(t,m+1)}-\bm{W}_{\ell}^{\star}\|_{ 2}\leq&\Big{(}1-\sqrt{\frac{(1-c_{I})\eta\rho}{K^{2}}}\Big{)} \cdot\|\bm{W}_{\ell}^{(t,m)}-\bm{W}_{\ell}^{\star}\|_{2}\\ &+\eta\cdot\|\nabla_{\ell}f(\bm{W}^{(t,m)})-g_{t}^{(m)}(\bm{W}^{( t,m)})\|_{2}\\ \lesssim&\Big{(}1-\big{(}1-\frac{c_{I}}{2}\big{)} \sqrt{\frac{\eta\rho}{K^{2}}}\Big{)}\cdot\|\bm{W}_{\ell}^{(t,m)}-\bm{W}_{\ell }^{\star}\|_{2}\\ &+\eta\cdot\|\nabla_{\ell}f(\bm{W}^{(t,m)})-g_{t}^{(m)}(\bm{W}^{( t,m)})\|_{2}.\end{split}\] (58)

Take the sum of (58) from \(\ell=1\) to \(\ell=L\), we have

\[\begin{split}\|\bm{W}^{(t,m+1)}-\bm{W}^{\star}\|_{2}\leq& \Big{(}1-\big{(}1-\frac{c_{I}}{2}\big{)}\sqrt{\frac{\eta\rho}{K^{2}}} \Big{)}\cdot\|\bm{W}^{(t,m)}-\bm{W}^{\star}\|_{2}\\ &+\eta\cdot\sum_{\ell}^{L}\|\nabla_{\ell}f(\bm{W}^{(t,m)})-g_{t}^ {(m)}(\bm{W}^{(t,m)})\|_{2}.\end{split}\] (59)

From Lemma 3, we have

\[\begin{split}\Big{\|}\nabla_{\ell}f(\bm{W}^{(t,m)})-g_{t}^{(m)}( \bm{W}_{\ell};\bm{W}^{(t,m)})\Big{\|}_{2}\lesssim&\frac{2-\varepsilon _{t}}{K}\sqrt{\frac{K_{\ell}\log q}{N_{t}}}\cdot\|\bm{W}^{(t,m)}-\bm{W}^{\star} \|_{2}\\ &+\frac{(1-\varepsilon_{t}/2)\gamma}{K}\cdot\|\bm{W}^{(t,0)}-\bm{W} ^{\star}\|_{2}\\ &+C_{d}\cdot\big{(}C_{t}+(1-C_{t})\varepsilon\big{)}\cdot\frac{R_{ \max}}{1-\gamma}.\end{split}\] (60)

For some small constant \(c_{N}\geq 0\), let

\[\eta\cdot\frac{1}{K}\sqrt{\frac{K_{\ell}\log q}{N_{t}}}\leq\frac{c_{N}}{L} \sqrt{\frac{\eta\rho}{K^{2}}},\] (61)

which requires

\[\begin{split} N_{t}\gtrsim&\ c_{N}^{-2}\cdot\rho^{-1} \cdot\eta^{-1}\cdot L^{2}\cdot\max_{\ell}K_{\ell}\cdot\log q\\ =&\ c_{N}^{-2}\cdot\rho^{-1}\cdot L\cdot d\cdot\log q.\end{split}\] (62)Then, the sample complexity

\[N=\sum_{t=1}^{T}N_{t}\gtrsim c_{N}^{-2}\cdot\rho^{-1}\cdot L\cdot d\cdot\log q\cdot T.\] (63)

Therefore, we have

\[\begin{split}\|\bm{W}^{(t,m+1)}-\bm{W}^{\star}\|_{2}\leq& \bigg{(}1-\big{(}1-(2-\varepsilon_{t})c_{N}-\frac{c_{I}}{2}\big{)} \sqrt{\frac{\rho}{TK^{2}}}\,\bigg{)}\cdot\|\bm{W}^{(t,m)}-\bm{W}^{\star}\|_{2} \\ &+\sqrt{\eta}\cdot\frac{(1-\varepsilon_{t}/2)\gamma}{K}\cdot\|\bm {W}^{(t,0)}-\bm{W}^{\star}\|_{2}\\ &+\eta\cdot C_{d}\cdot\big{(}C_{t}+(1-C_{t})\varepsilon\big{)} \cdot\frac{R_{\max}}{1-\gamma}.\end{split}\] (64)

By mathematical induction, when \(M=\log\gamma^{-1}\) and \(\eta=1/T=1/\Theta(N)\), we have

\[\begin{split}&\|\bm{W}^{(t,M)}-\bm{W}^{\star}\|_{2}\\ \lesssim&\sqrt{\frac{K^{2}}{N}}\cdot C_{d}\cdot \big{(}C_{t}+(1-C_{t})\varepsilon_{t}\big{)}\cdot\frac{R_{\max}}{1-\gamma}+(1- \varepsilon_{t}/2)\gamma\cdot\|\bm{W}^{(t,0)}-\bm{W}^{\star}\|_{2}\\ \leq&\frac{c_{N}\cdot C_{d}\cdot\big{(}C_{t}+(1-C_{ t})\cdot\varepsilon_{t}\big{)}}{K}\cdot\frac{R_{\max}}{1-\gamma}+(1- \varepsilon_{t}/2)\gamma\cdot\|\bm{W}^{(t,0)}-\bm{W}^{\star}\|_{2}\\ \leq&\frac{c_{N}\cdot C_{d}\cdot\big{(}C_{\max}+(1- C_{\max})\cdot\varepsilon_{t}\big{)}}{K}\cdot\frac{R_{\max}}{1-\gamma}+(1- \varepsilon_{t}/2)\gamma\cdot\|\bm{W}^{(t,0)}-\bm{W}^{\star}\|_{2}.\end{split}\] (65)

From Algorithm 1, we know that \(\bm{W}^{(t+1,0)}=\bm{W}^{(t,M)}\). To guarantee that iteration converge to the ground truth \(\bm{W}^{\star}\), namely, \(\|\bm{W}^{(t+1,0)}-\bm{W}^{\star}\|_{2}<\|\bm{W}^{(t,0)}-\bm{W}^{\star}\|\), we need

\[\varepsilon_{t}\leq\frac{(1-\gamma)^{2}\cdot K\cdot\|\bm{W}^{(t,0)}-\bm{W}^{ \star}\|_{2}}{(1-C_{t})\cdot c_{N}\cdot C_{d}\cdot R_{\max}}-\frac{C_{t}}{1-C _{t}}.\] (66)

To guarantee that \(\varepsilon_{T}\geq 0\), then we have

\[\|\bm{W}^{(T,0)}-\bm{W}^{\star}\|_{F}\gtrsim\frac{C_{T}\cdot c_{N}\cdot C_{d} \cdot R_{\max}}{(1-\gamma)^{2}\cdot K}.\] (67)

Specifically, let

\[\varepsilon_{t}=\frac{c_{\varepsilon}\cdot K\cdot\|\bm{W}^{(t,0)}-\bm{W}^{ \star}\|_{2}}{(1-C_{t})\cdot c_{N}\cdot C_{d}\cdot R_{\max}}-\frac{C_{t}}{1-C _{t}},\] (68)

we have

\[\begin{split}\|\bm{W}^{(t+1,0)}-\bm{W}^{\star}\|_{2}\lesssim& \gamma+c_{\varepsilon}(1-\gamma)\cdot\|\bm{W}^{(t,0)}-\bm{W}^{\star}\|_{2},\\ \text{and}&\|\bm{W}^{(T,0)}-\bm{W}^{\star}\|_{2} \lesssim&\big{[}\gamma+c_{\varepsilon}(1-\gamma)\big{]}^{T} \cdot\|\bm{W}^{(0,0)}-\bm{W}^{\star}\|_{2},\end{split}\] (69)

which completes the proof.

## Appendix C Proof of Lemma 2

Lemma 2 provides the lower and upper bounds for the eigenvalues of the Hessian matrix of population risk function in (31). According to Weyl's inequality in Lemma 5, the eigenvalues of \(\nabla_{\ell}^{2}f(\cdot)\) at any fixed point \(\bm{W}\) can be bounded in the form of (75). Therefore, we first provide the lower and upper bounds for \(\nabla_{\ell}^{2}f\) at the desired ground truth \(\bm{W}^{\star}\). Then, the bounds for \(\nabla_{\ell}^{2}f\) at any other point \(\bm{W}\) is bounded through (31) by utilizing the conclusion in Lemma 6. Lemma 6 illustrates the distance between the Hessian matrix of \(f\) at \(\bm{W}\) and \(\bm{W}^{\star}\). Lemma 7 provides the lower bound of \(\mathbb{E}_{\bm{x}}\big{(}\sum_{j=1}^{K}\bm{\alpha}_{j}^{\top}\frac{\partial Q }{\partial\bm{w}_{\ell,x}}(\bm{W}^{\star})\big{)}^{2}\) when \(\bm{x}\) belongs to sub-Gaussian distribution, which is used in proving the lower bound of the Hessian matrix in (76).

**Lemma 5** (Weyl's inequality, [5]).: _Let \(\bm{B}=\bm{A}+\bm{E}\) be a matrix with dimension \(m\times m\). Let \(\lambda_{i}(\bm{B})\) and \(\lambda_{i}(\bm{A})\) be the \(i\)-th largest eigenvalues of \(\bm{B}\) and \(\bm{A}\), respectively. Then, we have_

\[|\lambda_{i}(\bm{B})-\lambda_{i}(\bm{A})|\leq\|\bm{E}\|_{2},\quad \forall\quad i\in[m].\] (70)

**Lemma 6**.: _Let \(f(\bm{W})\) be the population risk function defined in (31). If \(\bm{W}\) is close to \(\bm{W}^{\star}\) such that_

\[\|\bm{W}-\bm{W}^{\star}\|_{2}\lesssim\frac{\rho}{K}\] (71)

_we have_

\[\|\nabla_{\ell}^{2}f(\bm{W})-\nabla_{\ell}^{2}f(\bm{W}^{\star})\|_ {2}\lesssim\frac{1}{K}\cdot\|\bm{W}-\bm{W}^{\star}\|_{2}.\] (72)

**Lemma 7**.: _Suppose the following assumptions hold:_

1. \(\{\bm{w}_{j}\}_{j=1}^{K}\in\mathbb{R}^{K_{\ell}}\) _are linear independent,_
2. \(p_{H}(\bm{h}):\mathbb{R}^{K_{\ell}}\longrightarrow[0\;1]\) _be the probability density for_ \(\bm{h}\) _such that_ \(\mathbb{E}_{\bm{h}}\|\bm{h}\|_{2}^{2}\leq+\infty\)_._

_Let \(\bm{\alpha}\in\mathbb{R}^{K_{1}K_{2}}\) be the unit vector defined in (34), we have_

\[\rho:=\min_{\|\bm{\alpha}\|_{2}=1}\int_{\mathcal{R}}\Big{(}\sum_{ j=1}^{K}\bm{\alpha}^{\top}\bm{h}\phi^{\prime}(\bm{w}_{\ell,j}^{\top}\bm{h}) \Big{)}^{2}p_{H}(\bm{h})\cdot d\bm{h}>0,\] (73)

_where \(\mathcal{R}\subset\mathbb{R}^{K_{\ell}}\) with \(\int_{\mathcal{R}}f_{H}(\bm{h})>0\). Moreover, if further assuming \(\mathcal{P}\) is Gaussian distribution and \(\mathcal{R}=\mathbb{R}^{K_{\ell}}\), we have \(\rho>0.091\)._

**Lemma 8**.: _Let \(\bm{h}^{(\ell)}(\bm{W})\) be the function defined in (39). When \(\bm{W}\) is sufficiently close to \(\bm{W}^{\star}\), i.e., \(\|\bm{W}-\bm{W}^{\star}\|_{2}\) is smaller than some positive constant \(c<1\), we have_

\[\|\bm{h}^{(\ell)}(\bm{W})\|_{2}\lesssim\|\bm{x}\|_{2},\] (74) \[\|\bm{h}^{(\ell)}(\bm{W})-\bm{h}^{(\ell)}(\bm{W}^{\star})\|_{2} \lesssim\|\bm{W}-\bm{W}^{\star}\|_{2}\cdot\|\bm{x}\|_{2}.\]

Proof of Lemma 2.: Let \(\lambda_{\max}(\bm{W})\) and \(\lambda_{\min}(\bm{W})\) denote the largest and smallest eigenvalues of \(\nabla_{\ell}^{2}f(\bm{W})\) at a point \(\bm{W}\), respectively. Then, from Lemma 5, we have

\[\lambda_{\max}(\bm{W}) \leq\lambda_{\max}(\bm{W}^{\star})+\|\nabla_{\ell}^{2}f(\bm{W})- \nabla_{\ell}^{2}f(\bm{W}^{\star})\|_{2},\] (75) \[\lambda_{\min}(\bm{W}) \geq\lambda_{\min}(\bm{W}^{\star})-\|\nabla_{\ell}^{2}f(\bm{W})- \nabla_{\ell}^{2}f(\bm{W}^{\star})\|_{2}.\]

Then, we provide the lower bound of the Hessian matrix of the population function at \(\bm{W}^{\star}\). Let \(\mathcal{P}\) be the distribution for \(\bm{h}^{(\ell)}(\bm{W})\) when \(\bm{x}\sim\mu_{t}\) with probability density function denoted as \(p_{H}\). For any \(\bm{\alpha}\in\mathbb{R}^{K_{\ell}K}\) defined in (34) with \(\|\bm{\alpha}\|_{2}=1\), we have

\[\min_{\|\bm{\alpha}\|_{2}=1}\bm{\alpha}^{\top}\nabla_{\ell}^{2} f(\bm{W}^{\star})\bm{\alpha}\] (76) \[= \frac{1}{K^{2}}\min_{\|\bm{\alpha}\|_{2}=1}\mathbb{E}_{\bm{h} \sim\mathcal{P}}\Big{(}\sum_{j=1}^{K}\bm{\alpha}_{j}^{\top}\bm{h}^{(\ell)} \mathcal{J}_{\ell,k}\phi^{\prime}(\bm{w}_{\ell,j}^{\star\top}\bm{h}^{(\ell)} )\Big{)}^{2}\] \[= \frac{1}{K^{2}}\min_{\|\bm{\alpha}\|_{2}=1}\int_{\mathbb{R}^{K_{ \ell-1}}}\Big{(}\sum_{j=1}^{K}\bm{\alpha}_{j}^{\top}\bm{h}^{(\ell)}\mathcal{J }_{\ell,k}\phi^{\prime}(\bm{w}_{\ell,j}^{\star\top}\bm{h}^{(\ell)})\Big{)}^{2}p _{H}(\bm{h}^{(\ell)})\cdot d\bm{\mathrm{h}}^{(\ell)}\] \[= \frac{1}{K^{2}}\min_{\|\bm{\alpha}\|_{2}=1}\int_{\{\bm{h}^{(\ell)} |\mathcal{J}_{\ell,k}\neq 0\}}\Big{(}\sum_{j=1}^{K}\bm{\alpha}_{j}^{\top}\bm{h}^{(\ell)}\phi^{\prime}( \bm{w}_{\ell,j}^{\star\top}\bm{h}^{(\ell)})\Big{)}^{2}p_{H}(\bm{h}^{(\ell)}) \cdot d\bm{\mathrm{h}}^{(\ell)}\] \[\gtrsim \frac{\rho}{K^{2}},\]

where the last inequality comes from Lemma 7, and Lemma 7 holds since \(\bm{h}^{(\ell)}\) belongs to sub-Gaussian distribution and \(\bm{W}_{\ell}\) is full rank.

Next, the upper bound of \(\nabla_{\ell}^{2}f\) can be bounded as

\[\max_{\|\bm{\alpha}\|_{2}=1}\bm{\alpha}^{\top}\nabla_{\ell}^{2}f( \bm{W}^{\star})\bm{\alpha}\] (77) \[= \frac{1}{K^{2}}\max_{\|\bm{\alpha}\|_{2}=1}\mathbb{E}_{\bm{x}} \Big{(}\sum_{j=1}^{K}\bm{\alpha}_{j}^{\top}\bm{h}^{(\ell)}\cdot\mathcal{J}_{ \ell,k}\phi^{\prime}(\bm{w}_{\ell,j}^{\star\top}\bm{h}^{(\ell)})\Big{)}^{2}\] \[= \frac{1}{K^{2}}\max_{\|\bm{\alpha}\|_{2}=1}\mathbb{E}_{\bm{x}} \sum_{j_{1}=1}^{K}\sum_{j_{2}=1}^{K}\bm{\alpha}_{j_{1}}^{\top}\bm{h}^{(\ell)} \cdot\mathcal{J}_{\ell,k}\phi^{\prime}(\bm{w}_{\ell,j_{1}}^{\star\top}\bm{h} ^{(\ell)})\cdot\bm{\alpha}_{j_{2}}^{\top}\bm{h}^{(\ell)}\cdot\mathcal{J}_{ \ell,k}\phi^{\prime}(\bm{w}_{\ell,j_{2}}^{\star\top}\bm{h}^{(\ell)})\] \[= \frac{1}{K^{2}}\sum_{j_{1}=1}^{K}\sum_{j_{2}=1}^{K}\mathbb{E}_{ \bm{x}}\bm{\alpha}_{j_{1}}^{\top}\bm{h}^{(\ell)}\cdot\mathcal{J}_{\ell,k}\phi ^{\prime}(\bm{w}_{\ell,j_{1}}^{\star T}\bm{h}^{(\ell)})\cdot\bm{\alpha}_{j_{2 }}^{\top}\bm{h}^{(\ell)}\cdot\mathcal{J}_{\ell,k}\phi^{\prime}(\bm{w}_{\ell, j_{2}}^{\star\top}\bm{h}^{(\ell)})\] \[\leq \frac{1}{K^{2}}\max_{\|\bm{\alpha}\|_{2}=1}\sum_{j_{1}=1}^{K}\sum _{j_{2}=1}^{K}\Big{[}\mathbb{E}_{\bm{x}}(\bm{\alpha}_{j_{1}}^{\top}\bm{h}^{( \ell)})^{4}\cdot\mathbb{E}(\phi^{\prime}(\bm{w}_{\ell,j_{1}}^{\star\top}\bm{h} ^{(\ell)}))^{4}\cdot\mathbb{E}_{\bm{x}}(\bm{\alpha}_{j_{2}}^{\top}\bm{h}^{( \ell)})^{4}\cdot\mathbb{E}_{\bm{x}}(\phi^{\prime}(\bm{w}_{\ell,j_{2}}^{\star \top}\bm{h}^{(\ell)}))^{4}\Big{]}^{1/4}\] \[\leq \frac{1}{K^{2}}\max_{\|\bm{\alpha}\|_{2}=1}\sum_{j_{1}=1}^{K}\sum _{j_{2}=1}^{K}\Big{[}\mathbb{E}_{\bm{x}}(\bm{\alpha}_{j_{1}}^{\top}\bm{x})^{4 }\cdot\mathbb{E}_{\bm{x}}(\bm{\alpha}_{j_{2}}^{\top}\bm{x})^{4}\Big{]}^{1/4}\] \[\leq \frac{3}{K^{2}}\sum_{j_{1}=1}^{K}\sum_{j_{2}=1}^{K}\|\bm{\alpha} _{j_{1}}\|_{2}\cdot\|\bm{\alpha}_{j_{2}}\|_{2}\leq\frac{6}{K^{2}}\sum_{j_{1}=1 }^{K}\sum_{j_{2}=1}^{K}\frac{1}{2}\Big{(}\|\bm{\alpha}_{j_{1}}\|_{2}^{2}+\|\bm{ \alpha}_{j_{2}}\|_{2}^{2}\Big{)}\] \[= \frac{6}{K}.\]

Therefore, we have

\[\lambda_{\max}(\bm{W}^{\star})=\max_{\|\bm{\alpha}\|_{2}=1}\bm{\alpha}^{\top} \nabla_{\ell}^{2}f(\bm{W}^{\star};p)\bm{\alpha}\leq \frac{6}{K}.\] (78)

Then, given (71), we have

\[\|\bm{W}-\bm{W}^{\star}\|_{2}\lesssim\frac{2\rho}{K}.\] (79)

Combining (79) and Lemma 6, we have

\[\|\nabla_{\ell}^{2}f(\bm{W})-\nabla_{\ell}^{2}f(\bm{W}^{\star})\|_{2}\lesssim \frac{\rho}{K^{2}}.\] (80)

Therefore, from (80) and (75), we have

\[\lambda_{\max}(\bm{W})\leq\lambda_{\max}(\bm{W}^{\star})+\|\nabla_{\ell}^{2}f( \bm{W})-\nabla_{\ell}^{2}f(\bm{W}^{\star})\|_{2}\leq\frac{6}{K}+\frac{\rho}{2K^ {2}}\leq\frac{7}{K},\] (81)

\[\lambda_{\min}(\bm{W})\geq\lambda_{\min}(\bm{W}^{\star})-\|\nabla_{\ell}^{2}f( \bm{W})-\nabla_{\ell}^{2}f(\bm{W}^{\star})\|_{2}\geq\frac{\rho}{K^{2}}-\frac{ \rho}{2K^{2}}=\frac{\rho}{2K^{2}},\]

which completes the proof. 

## Appendix D Proof of Lemma 3

Before illustrating the whole proof, we first introduce some preliminary lemmas and definitions. Lemma 9 is the concentration theorem for independent random matrices. The definitions of the sub-Gaussian and sub-exponential variables are summarized in Definitions 3 and 4, and it is easy to verify that any bounded variables belong to sub-Gaussian distribution. Lemmas 10 and 11 serve as the technical tools in bounding matrix norms under the framework of the confidence interval.

The error bound between \(\|\nabla_{\epsilon}f-g_{t}\|_{2}\) is divided into bounding \(I_{1}\), \(I_{2}\), and \(I_{3}\) as shown in (91). \(I_{1}\) in (92) represent the deviation of the mean of several random variables to their expectation, which can be bounded through concentration inequality, i.e, Chernoff bound. \(I_{2}\) in (93) come from the inconsistency of "noisy" label in (8) and the "ground truth" label in the population risk function (31). \(I_{3}\) in (94) come from the data distribution shift defined in Definition 1.

**Lemma 9** ([72], Theorem 1.6).: _Consider a finite sequence \(\{\bm{Z}_{k}\}\) of independent, random matrices with dimensions \(d_{1}\times d_{2}\). Assume that such a random matrix satisfies_

\[\mathbb{E}(\bm{Z}_{k})=0\quad\text{and}\quad\|\bm{Z}_{k}\|\leq R\quad\text{ almost surely}.\]

_Define_

\[\delta^{2}:=\max\Big{\{}\Big{\|}\sum_{k}\mathbb{E}(\bm{Z}_{k}\bm{Z}_{k}^{\top} )\Big{\|},\Big{\|}\sum_{k}\mathbb{E}(\bm{Z}_{k}^{\top}\bm{Z}_{k})\Big{\|} \Big{\}}.\]

_Then for all \(t\geq 0\), we have_

\[\text{Prob}\Bigg{\{}\left\|\sum_{k}\bm{Z}_{k}\right\|\geq t\Bigg{\}}\leq(d_{1 }+d_{2})\exp\Big{(}\frac{-t^{2}/2}{\delta^{2}+Rt/3}\Big{)}.\]

**Definition 3** (Definition 5.7, [74]).: _A random variable \(X\) is called a sub-Gaussian random variable if it satisfies_

\[(\mathbb{E}|X|^{p})^{1/p}\leq c_{1}\sqrt{p}\] (82)

_for all \(p\geq 1\) and some constant \(c_{1}>0\). In addition, we have_

\[\mathbb{E}e^{s(X-\mathbb{E}X)}\leq e^{c_{2}\|X\|_{\psi_{2}}^{2}s^{2}}\] (83)

_for all \(s\in\mathbb{R}\) and some constant \(c_{2}>0\), where \(\|X\|_{\phi_{2}}\) is the sub-Gaussian norm of \(X\) defined as \(\|X\|_{\psi_{2}}=\sup_{p\geq 1}p^{-1/2}(\mathbb{E}|X|^{p})^{1/p}\)._

_Moreover, a random vector \(\bm{X}\in\mathbb{R}^{d}\) belongs to the sub-Gaussian distribution if one-dimensional marginal \(\bm{\alpha}^{\top}\bm{X}\) is sub-Gaussian for any \(\bm{\alpha}\in\mathbb{R}^{d}\), and the sub-Gaussian norm of \(\bm{X}\) is defined as \(\|\bm{X}\|_{\psi_{2}}=\sup_{\|\bm{\alpha}\|_{2}=1}\|\bm{\alpha}^{\top}\bm{X}\| _{\psi_{2}}\)._

**Definition 4** (Definition 5.13, [74]).: _A random variable \(X\) is called a sub-exponential random variable if it satisfies_

\[(\mathbb{E}|X|^{p})^{1/p}\leq c_{3}p\] (84)

_for all \(p\geq 1\) and some constant \(c_{3}>0\). In addition, we have_

\[\mathbb{E}e^{s(X-\mathbb{E}X)}\leq e^{c_{4}\|X\|_{\psi_{1}}^{2}s^{2}}\] (85)

_for \(s\leq 1/\|X\|_{\psi_{1}}\) and some constant \(c_{4}>0\), where \(\|X\|_{\psi_{1}}\) is the sub-exponential norm of \(X\) defined as \(\|X\|_{\psi_{1}}=\sup_{p\geq 1}p^{-1}(\mathbb{E}|X|^{p})^{1/p}\)._

**Lemma 10** (Lemma 5.2, [74]).: _Let \(\mathcal{B}(0,1)\in\{\bm{\alpha}\big{|}\|\bm{\alpha}\|_{2}=1,\bm{\alpha}\in \mathbb{R}^{d}\}\) denote a unit ball in \(\mathbb{R}^{d}\). Then, a subset \(\mathcal{S}_{\xi}\) is called a \(\xi\)-net of \(\mathcal{B}(0,1)\) if every point \(\bm{z}\in\mathcal{B}(0,1)\) can be approximated to within \(\xi\) by some point \(\bm{\alpha}\in\mathcal{B}(0,1)\), i.e., \(\|\bm{z}-\bm{\alpha}\|_{2}\leq\xi\). Then the minimal cardinality of a \(\xi\)-net \(\mathcal{S}_{\xi}\) satisfies_

\[|\mathcal{S}_{\xi}|\leq(1+2/\xi)^{d}.\] (86)

**Lemma 11** (Lemma 5.3, [74]).: _Let \(\bm{A}\) be an \(d_{1}\times d_{2}\) matrix, and let \(\mathcal{S}_{\xi}(d)\) be a \(\xi\)-net of \(\mathcal{B}(0,1)\) in \(\mathbb{R}^{d}\) for some \(\xi\in(0,1)\). Then_

\[\|\bm{A}\|_{2}\leq(1-\xi)^{-1}\max_{\bm{\alpha}_{1}\in\mathcal{S}_{\xi}(d_{1}),\bm{\alpha}_{2}\in\mathcal{S}_{\xi}(d_{2})}|\bm{\alpha}_{1}^{\top}\bm{A}\bm{ \alpha}_{2}|.\] (87)

Proof of Lemma 3.: From (7), we know that

\[g_{t}(\bm{w}_{\ell,k};\bm{W})\] (88) \[= \frac{1}{N}\sum_{n=1}^{N}\Big{(}Q(\bm{W};\bm{s}_{n},a_{n})-y_{n} ^{(t)}\Big{)}\cdot\frac{\partial Q(\bm{W};\bm{s}_{n},a_{n})}{\partial\bm{w}_ {\ell,k}}\] \[= \frac{1}{N}\sum_{n=1}^{N}\Big{(}Q(\bm{W};\bm{s}_{n},a_{n})-Q(\bm {W}^{\star};\bm{s}_{n},a_{n})+\gamma\cdot\max_{a}Q(\bm{s}_{n},a;\bm{W}^{\star})\] \[\qquad\qquad-\gamma\cdot\max_{a}Q(\bm{s}_{n},a;\bm{W}^{(t,0)}) \Big{)}\cdot\frac{\partial Q(\bm{W};\bm{s}_{n},a_{n})}{\partial\bm{w}_{\ell,k}}\] \[= \frac{1}{N}\sum_{n=1}^{N}\Big{(}Q(\bm{W};\bm{s}_{n},a_{n})-Q(\bm {W}^{\star};\bm{s}_{n},a_{n})\Big{)}\cdot\frac{\partial Q(\bm{W};\bm{s}_{n},a_{ n})}{\partial\bm{w}_{\ell,k}}\] \[\quad+\frac{1}{N}\sum_{n=1}^{N}\gamma\cdot\Big{(}\max_{a}Q(\bm{s }_{n},a;\bm{W}^{\star})-\max_{a}Q(\bm{s}_{n},a;\bm{W}^{(t,0)})\Big{)}\cdot\frac{ \partial Q(\bm{W};\bm{s}_{n},a_{n})}{\partial\bm{w}_{\ell,k}}.\]From (31), we know that

\[\frac{\partial f}{\partial\bm{w}_{\ell,k}}(\bm{W})=\mathbb{E}_{(\bm{s},a)\sim\mu^ {*}}\Big{(}Q(\bm{W};\bm{s},a)-Q(\bm{W}^{*};\bm{s},a)\Big{)}\cdot\frac{\partial Q (\bm{W};\bm{s},a)}{\partial\bm{w}_{\ell,k}}.\] (89)

Then, from (88) and (89), we have

\[\begin{split} g_{t}(\bm{w}_{\ell,k};\bm{W})-\frac{\partial f}{ \partial\bm{w}_{\ell,k}}(\bm{W})&=g_{t}(\bm{w}_{\ell,k};\bm{W})- \mathbb{E}_{(\bm{s},a)\sim\mathcal{D}_{t}}g_{t}(\bm{w}_{\ell,k};\bm{W})\\ &\quad+\mathbb{E}_{(\bm{s},a)\sim\mu_{t}}g_{t}(\bm{w}_{\ell,k}; \bm{W})-\frac{\partial f}{\partial\bm{w}_{\ell,k}}(\bm{W}),\end{split}\] (90)

where \(\mathcal{D}_{t}\) and \(\mu_{t}\) are equivalent because of Assumption 2. Then, we have

\[\begin{split}& g_{t}(\bm{w}_{\ell,k};\bm{W})-\frac{\partial f}{ \partial\bm{w}_{\ell,k}}(\bm{W})\\ =&\bigg{[}\frac{1}{N}\sum_{n=1}^{N}\Big{(}Q(\bm{W}; \bm{s}_{n},a_{n})-Q(\bm{W}^{*};\bm{s}_{n},a_{n})\Big{)}\cdot\frac{\partial Q (\bm{W};\bm{s}_{n},a_{n})}{\partial\bm{w}_{\ell,k}}\\ &-\mathbb{E}_{(\bm{s},a)\sim\mu_{t}}\Big{(}Q(\bm{W};\bm{s},a)-Q( \bm{W}^{*};\bm{s},a)\Big{)}\cdot\frac{\partial Q(\bm{W};\bm{s},a)}{\partial \bm{w}_{\ell,k}}\bigg{]}\\ &+\bigg{[}\frac{1}{N}\sum_{n=1}^{N}\gamma\cdot\Big{(}\max_{a}Q( \bm{s}_{n},a;\bm{W}^{*})-\max_{a}Q(\bm{s}_{n},a;\bm{W}^{(t,0)})\Big{)}\cdot \frac{\partial Q(\bm{W};\bm{s}_{n},a_{n})}{\partial\bm{w}_{\ell,k}}\bigg{]}\\ &+\mathbb{E}_{(\bm{s},a)\sim\mu_{t}}g_{t}(\bm{w}_{\ell,k};\bm{W}) -\frac{\partial f}{\partial\bm{w}_{\ell,k}}(\bm{W}).\end{split}\] (91)

For convenience, we define \(\bm{I}_{1}\), \(\bm{I}_{2}\), and \(\bm{I}_{3}\) in the following ways with \(\bm{x}_{n}:=(\bm{s}_{n},a_{n})\) be the feature mapping of state-action pair \((\bm{s}_{n},a_{n})\).

Then, \(\bm{I}_{1}\) is defined as

\[\begin{split}\bm{I}_{1}:=&\frac{1}{N}\sum_{n=1}^{N }\Big{(}Q(\bm{W};\bm{s}_{n},a_{n})-Q(\bm{W}^{*};\bm{s}_{n},a_{n})\Big{)}\cdot \frac{\partial Q(\bm{W};\bm{s}_{n},a_{n})}{\partial\bm{w}_{\ell,k}}\\ &-\mathbb{E}_{(\bm{s},a)\sim\mathcal{D}_{t}}\Big{(}Q(\bm{W};\bm{ s},a)-Q(\bm{W}^{*};\bm{s},a)\Big{)}\cdot\frac{\partial Q(\bm{W};\bm{s},a)}{ \partial\bm{w}_{\ell,k}},\end{split}\] (92)

\(\bm{I}_{2}\) is defined as

\[\bm{I}_{2}:= \frac{1}{N}\sum_{n=1}^{N}\gamma\cdot\Big{(}\max_{a}Q(\bm{s}_{n} ^{\prime},a;\bm{W}^{*})-\max_{a}Q(\bm{s}_{n}^{\prime},a;\bm{W}^{(t,0)})\Big{)} \cdot\frac{\partial Q(\bm{W};\bm{s}_{n},a_{n})}{\partial\bm{w}_{\ell,k}},\] (93)

and \(\bm{I}_{3}\) is defined as

\[\bm{I}_{3}:=\mathbb{E}_{(\bm{s},a)\sim\mu_{t}}g_{t}(\bm{w}_{\ell,k};\bm{W})- \frac{\partial f}{\partial\bm{w}_{\ell,k}}(\bm{W}),\] (94)

where

\[\frac{\partial Q(\bm{W};\bm{s}_{n},\bm{a}_{n})}{\partial\bm{w}_{\ell,k}}= \frac{1}{K}\mathcal{J}_{\ell,k}\phi^{\prime}(\bm{w}_{\ell,k}^{\top}\bm{h}^{ \ell})\bm{h}^{\ell}(\bm{W})\] (95)

from (43). Therefore, we have

\[\Big{\|}g_{t}(\bm{w}_{\ell,k};\bm{W})-\frac{\partial f_{t}}{\partial\bm{w}_{ \ell,k}}(\bm{W})\Big{\|}_{2}\leq\|\bm{I}_{1}\|_{2}+\|\bm{I}_{2}\|_{2}+\|\bm{I }_{3}\|_{2}.\] (96)

Next, we will provide the bound for \(\|\bm{I}_{1}\|_{2}\), \(\|\bm{I}_{2}\|_{2}\), and \(\|\bm{I}_{3}\|_{2}\).

**Bound of \(\bm{I}_{1}\).** We first divide the data in \(\mathcal{D}_{t}\) into two parts, namely, \(\mathcal{D}_{t,1}\) and \(\mathcal{D}_{t,2}\). \(\mathcal{D}_{t,1}\) includes the state-action pair \((\bm{s},a)\) such that \(a_{n}\) is randomly selected from action space \(\mathcal{A}\), and \(\mathcal{D}_{t,2}\) includes the state-action pair \((\bm{s},a)\) such that \(a_{n}\) is selected based on the greedy policy with respect to \(Q(\bm{W}^{(t,0)})\).

Then, we define a random variable \(Z^{(\ell,1)}=\big{(}Q(\bm{x};\bm{W})-Q(\bm{x};\bm{W}^{\star})\big{)}\cdot\mathcal{ J}_{\ell,k}\cdot\bm{\alpha}^{T}\bm{h}^{(\ell)}(\bm{W})\) with \(\bm{x}\sim\mathcal{D}_{t,1}\) and \(Z^{(\ell,1)}_{n}=\big{(}Q(\bm{x}_{n};\bm{W})-Q(\bm{x}_{n};\bm{W}^{\star})\big{)} \cdot\mathcal{J}_{\ell,k}\cdot\bm{\alpha}^{T}\bm{h}^{(\ell)}_{n}(\bm{W})\) as the realization of \(Z^{(1)}_{\ell}\) for \(n=1,2\cdots,N\), where \(\bm{\alpha}\in\mathbb{R}^{d}\) is any mixed unit vector with \(\|\bm{\alpha}\|_{2}\leq 1\). We know that \(\bm{s}\) and \(a\) are independent for \(\bm{x}\sim\mathcal{D}_{t,1}\). Let \(\Sigma_{1}\) denote the covariance matrix of \(\bm{x}\sim\mathcal{D}_{t,1}\). Moreover, \(\bm{x}(\bm{s},a)\) is bounded by \(1\), then we have \(\|\bm{\Sigma}_{1}\|_{2}\leq 1\).

Similar to \(Z^{(\ell,1)}\), we define a random variable \(Z^{(\ell,2)}=\big{(}Q(\bm{x};\bm{W})-Q(\bm{x};\bm{W}^{\star})\big{)}\cdot \mathcal{J}_{\ell,k}\cdot\bm{\alpha}^{T}\bm{h}^{(\ell)}(\bm{W})\) with \(\bm{x}\sim\mathcal{D}_{t,2}\) and \(Z^{(\ell,2)}_{n}=\big{(}Q(\bm{x}_{n};\bm{W})-Q(\bm{x}_{n};\bm{W}^{\star})\big{)} \cdot\mathcal{J}_{\ell,k}\cdot\bm{\alpha}^{T}\bm{h}^{(\ell)}_{n}(\bm{W})\) as the realization of \(Z^{(\ell,2)}\) for \(n=1,2\cdots,N\). Differ from \(Z^{(\ell,1)}\), \(\bm{s}\) and \(a\) are dependent for \(\bm{x}\sim\mathcal{D}_{t,2}\). Let \(\bm{\Sigma}_{2}\) denote the covariance matrix of \(\bm{x}\sim\mathcal{D}_{t,1}\). Then, we have \(\|\bm{\Sigma}_{2}\|_{2}\leq 1+\max_{j}\rho_{x_{j},a}\leq 2\), where \(\rho_{x_{j},a}\) denotes the correlation between \(a\) and \(x_{j}\).

According to the definition of (92), we can rewrite \(\bm{I}_{1}\) as

\[\begin{split}\bm{I}_{1}=&\frac{1}{K}\bigg{[}\frac{1 }{N}\sum_{n=1}^{N}\big{(}Q(\bm{W};\bm{x}_{n})-Q(\bm{W}^{\star};\bm{x}_{n}) \big{)}\mathcal{J}_{\ell,k}\phi^{\prime}(\bm{w}_{\ell,k}^{\top}\bm{h}_{n}^{ \ell})\bm{h}_{n}^{\ell}\\ &\qquad-\mathbb{E}_{\bm{x}\sim\mathcal{D}_{t}}\big{(}Q(\bm{W}; \bm{x})-Q(\bm{W}^{\star};\bm{x})\big{)}\mathcal{J}_{\ell,k}\phi^{\prime}(\bm{ w}_{\ell,k}^{\top}\bm{h}^{\ell})\bm{h}^{\ell}\bigg{]}\\ =&\frac{1}{K}\bigg{[}\frac{1}{N}\Big{(}\sum_{n\in \mathcal{D}_{t,1}}\big{(}Q(\bm{W};\bm{x}_{n})-Q(\bm{W}^{\star};\bm{x}_{n}) \big{)}\mathcal{J}_{\ell,k}\phi^{\prime}(\bm{w}_{\ell,k}^{\top}\bm{h}_{n}^{ \ell})\bm{h}_{n}^{\ell}\\ &\qquad+\sum_{n\in\mathcal{D}_{t,2}}\big{(}Q(\bm{W};\bm{x}_{n})-Q (\bm{W}^{\star};\bm{x}_{n})\big{)}\mathcal{J}_{\ell,k}\phi^{\prime}(\bm{w}_{ \ell,k}^{\top}\bm{h}_{n}^{\ell})\bm{h}_{n}^{\ell}\Big{)}\\ &\qquad-\Big{(}\varepsilon\mathbb{E}_{\bm{x}\sim\mathcal{D}_{t,1 }}\big{(}Q(\bm{W};\bm{x})-Q(\bm{W}^{\star};\bm{x})\big{)}\mathcal{J}_{\ell,k} \phi^{\prime}(\bm{w}_{\ell,k}^{\top}\bm{h}^{\ell})\bm{h}^{\ell}\] (97) \[\qquad+(1-\varepsilon)\mathbb{E}_{\bm{x}\sim\mathcal{D}_{t,2}}\big{(} Q(\bm{W};\bm{x})-Q(\bm{W}^{\star};\bm{x})\big{)}\mathcal{J}_{\ell,k}\phi^{\prime}(\bm{ w}_{\ell,k}^{\top}\bm{h}^{\ell})\bm{h}^{\ell}\Big{)}\bigg{]}\\ =&\frac{1}{K^{2}}\bigg{[}\varepsilon\cdot\Big{(} \frac{1}{\varepsilon N}\sum_{n\in\mathcal{D}_{t,1}}\big{(}Q(\bm{W};\bm{x}_{n}) -Q(\bm{W}^{\star};\bm{x}_{n})\big{)}\mathcal{J}_{\ell,k}\phi^{\prime}(\bm{w}_{ \ell,k}^{\top}\bm{h}_{n}^{\ell})\bm{h}_{n}^{\ell}\\ &\qquad-\mathbb{E}_{\bm{x}\sim\mathcal{D}_{t,1}}\big{(}Q(\bm{W}; \bm{x})-Q(\bm{W}^{\star};\bm{x})\big{)}\mathcal{J}_{\ell,k}\phi^{\prime}(\bm{ w}_{\ell,k}^{\top}\bm{h}^{\ell})\bm{h}^{\ell}\Big{)}\\ &+(1-\varepsilon)\Big{(}\frac{1}{(1-\varepsilon)N}\sum_{n\in \mathcal{D}_{t,2}}\big{(}Q(\bm{W};\bm{x}_{n})-Q(\bm{W}^{\star};\bm{x}_{n}) \big{)}\mathcal{J}_{\ell,k}\phi^{\prime}(\bm{w}_{\ell,k}^{\top}\bm{h}_{n}^{\ell} )\bm{h}_{n}^{\ell}\\ &\qquad-\mathbb{E}_{\bm{x}\sim\mathcal{D}_{t,2}}\big{(}Q(\bm{W}; \bm{x})-Q(\bm{W}^{\star};\bm{x})\big{)}\mathcal{J}_{\ell,k}\phi^{\prime}(\bm{ w}_{\ell,k}^{\top}\bm{h}^{\ell})\bm{h}^{\ell}\bigg{]}\end{split}\]

Then, for any \(p\in\mathbb{N}^{+}\), we have

\[\begin{split}\big{(}\mathbb{E}|Z^{(1)}|^{p}\big{)}^{1/p}=& \Big{(}\mathbb{E}_{\bm{x}\sim\mathcal{D}_{t,1}}|Q(\bm{W};\bm{x})-Q(\bm{W}^{ \star};\bm{x})|^{p}\cdot|\mathcal{J}_{\ell,k}\phi^{\prime}(\bm{w}_{\ell,k}^{ \top}\bm{x})|\cdot|\bm{\alpha}^{T}\bm{h}^{\ell}|^{p}\Big{)}^{1/p}\\ \leq&\Big{(}\mathbb{E}_{\bm{x}\sim\mathcal{D}_{t,1}}|Q( \bm{W};\bm{x})-Q(\bm{W}^{\star};\bm{x})|^{p}\cdot|\bm{\alpha}^{T}\bm{h}^{ \ell}|^{p}\Big{)}^{1/p}\\ \leq&\Big{(}\mathbb{E}_{\bm{x}\sim\mathcal{D}_{t,1}} \big{|}\|\bm{W}-\bm{W}^{\star}\|_{2}\cdot\|\bm{x}\|_{2}\Big{|}^{p}\cdot|\bm{ \alpha}^{T}\bm{x}|^{p}\Big{)}^{1/p}\\ \leq& C_{1}\cdot\|\bm{W}-\bm{W}^{\star}\|_{2}\cdot p \end{split}\] (98)

where \(C_{1}\) is a positive constant.

From Definition 4, we know that \(Z^{(\ell,1)}\) belongs to sub-exponential distribution with \(\|Z^{(\ell,1)}\|_{\psi_{1}}\leq C_{1}\|\bm{W}-\bm{W}^{\star}\|_{2}\). Therefore, by Chernoff inequality, we have

\[\mathbb{P}\bigg{\{}\Big{|}\frac{1}{N}\sum_{n=1}^{N}Z^{(\ell,1)}_{n}(j)-\mathbb{E }Z^{(\ell,1)}(j)\Big{|}<t\bigg{\}}\leq 1-\frac{e^{-C(C_{1}\|\bm{W}-\bm{W}^{\star}\|_{2})^{2} \cdot Ng^{2}}}{e^{Nst}}\] (99)

for some positive constant \(C\) and any \(s\in\mathbb{R}\).

Let \(t=C_{1}\|\bm{W}-\bm{W}^{\star}\|_{2}\sqrt{\frac{d\log q}{N}}\) and \(s=\frac{2}{C\|\bm{W}-\bm{W}^{\star}\|_{2}}\cdot t\) for some large constant \(q>0\). Then, we have

\[\Big{|}\frac{1}{N}\sum_{n=1}^{N}Z_{n}^{(\ell,1)}(j)-\mathbb{E}Z^{( \ell,1)}(j)\Big{|}\lesssim C_{1}\|\bm{W}-\bm{W}^{\star}\|_{2}\cdot\sqrt{\frac{d \log q}{N}}\] (100)

with probability at least \(1-q^{-d}\).

Similar to (98), we have

\[\Big{(}\mathbb{E}|Z^{(\ell,2)}|^{p}\Big{)}^{1/p}\leq C_{2}\cdot\| \bm{W}-\bm{W}^{\star}\|_{2}\cdot p,\] (101)

where \(C_{2}=2\cdot C_{1}\). Then, we have

\[\Big{|}\frac{1}{N}\sum_{n=1}^{N}Z_{n}^{(\ell,2)}(j)-\mathbb{E}Z^{ (\ell,2)}(j)\Big{|}\lesssim 2C_{1}\|\bm{W}-\bm{W}^{\star}\|_{2}\cdot\sqrt{ \frac{d\log q}{N}}\] (102)

with probability at least \(1-q^{-d}\).

From Lemma 11 and (97), we have

\[\|\bm{I}_{1}\|_{2}\leq 2\cdot\frac{1}{K^{2}}\bigg{[}\varepsilon\cdot\Big{|}\frac{1}{ \varepsilon N}\sum_{n\in\mathcal{D}_{t,1}}Z_{n}^{(\ell,1)}(j)-\mathbb{E}Z^{( \ell,1)}(j)\Big{|}\] (103) \[+(1-\varepsilon)\cdot\Big{|}\frac{1}{(1-\varepsilon)N}\sum_{n\in \mathcal{D}_{t,2}}Z_{n}^{(\ell,2)}(j)-\mathbb{E}Z^{(\ell,2)}(j)\Big{|}\bigg{]}\] \[\lesssim \frac{2-\varepsilon}{K^{2}}\|\bm{W}-\bm{W}^{\star}\|_{2}\cdot \sqrt{\frac{d\log q}{N}}\]

with probability at least \(1-|\mathcal{S}_{\frac{1}{2}}(d)|\cdot q^{-d}\).

From Lemma 10, we know that \(|\mathcal{S}_{\frac{1}{2}}(d)|\leq 5^{d}\). Therefore, the probability for (103) holds is at least \(1-\big{(}\frac{q}{5}\big{)}^{-d}\). Because \(q\gg 5\), we denote the probability as \(1-q^{-d}\) for convenience.

**Bound of \(\bm{I}_{2}\).** Let \(a_{n}^{\star}=\arg\max_{a\in\mathcal{A}}Q(\bm{W}^{\star};\bm{s}_{n}^{\prime}, \bm{a})\). While for \(Q(\bm{W})\), we have

\[\max_{a}Q(\bm{W};\bm{s}_{n}^{\prime},\bm{a})\geq Q(\bm{W};\bm{s}_ {n}^{\prime},\bm{a}^{\star}).\] (104)

Then, we have

\[\max_{a}Q(\bm{W}^{\star};\bm{s}_{n}^{\prime},\bm{a})-\max_{a}Q( \bm{W};\bm{s}_{n}^{\prime},\bm{a})= Q(\bm{W}^{\star};\bm{s}_{n}^{\prime},\bm{a}_{n}^{\star})- \max_{a}Q(\bm{W};\bm{s}_{n}^{\prime},\bm{a})\] (105) \[\leq Q(\bm{W}^{\star};\bm{s}_{n}^{\prime},\bm{a}_{n}^{\star})-Q(\bm{W}; \bm{s}_{n}^{\prime},\bm{a}_{n}^{\star}).\]

Similarly to (105), let us define \(\tilde{a}_{n}^{\star}=\arg\max_{a}Q(\bm{W};\bm{s}_{n},\bm{a})\). Then, we have

\[\max_{a}Q(\bm{W}^{\star};\bm{s}_{n}^{\prime},\bm{a})-\max_{a}Q(\bm{W};\bm{s}_ {n}^{\prime},\bm{a})\geq Q(\bm{W}^{\star};\bm{s}_{n}^{\prime},\tilde{\bm{a}}_{n}^{ \star})-Q(\bm{W};\bm{s}_{n}^{\prime},\tilde{\bm{a}}_{n}^{\star}).\] (106)

Combining (105) and (106), we have

\[\Big{|}\max_{a}Q(\bm{W}^{\star};\bm{s}_{n}^{\prime},\bm{a})-\max_{a}Q(\bm{W}; \bm{s}_{n}^{\prime},\bm{a})\Big{|}\leq\max_{a}\Big{|}Q(\bm{W}^{\star};\bm{s}_{n }^{\prime},\bm{a})-Q(\bm{W};\bm{s}_{n}^{\prime},\bm{a})\Big{|}.\] (107)

Following the definition of \(Z^{(\ell,1)}\) in (98), we define

\[Z^{(\ell,3)}(j)=\Big{(}\max_{a}Q(\bm{W}^{\star};\bm{s}_{n}^{\prime},\bm{a})- \max_{a}Q(\bm{W};\bm{s}_{n}^{\prime},\bm{a})\Big{)}\cdot\mathcal{J}_{\ell,k} \phi^{\prime}(\bm{w}_{\ell,k}^{\top}\bm{h}^{(\ell)})\cdot\bm{\alpha}^{\top}\bm {h}^{(\ell)}.\]

Therefore, from (105) and (106), we know

\[(\mathbb{E}|Z^{(3)}|^{p})^{1/p}\leq \bigg{(}\mathbb{E}_{\bm{x}\sim\mathcal{D}_{t}}\Big{|}\max_{a}Q(\bm {W}^{\star};\bm{s}_{n}^{\prime},\bm{a})-\max_{a}Q(\bm{W};\bm{s}_{n}^{\prime}, \bm{a})\Big{|}^{p}\] (108) \[\cdot\Big{|}\mathcal{J}_{\ell,k}\phi^{\prime}(\bm{w}_{\ell,k}^{ \top}\bm{h}^{(\ell)})\Big{|}^{p}\cdot|\bm{\alpha}^{\top}\bm{h}_{n}^{(\ell)}|^{p }\Big{)}^{1/p}\] \[\leq \bigg{(}\mathbb{E}_{\bm{x}\sim\mathcal{D}_{t}}\max_{a}\Big{|}Q( \bm{W}^{\star};\bm{s}_{n}^{\prime},\bm{a})-Q(\bm{W};\bm{s}_{n}^{\prime},\bm{a}) \Big{|}^{p}\cdot|\bm{\alpha}^{\top}\bm{h}_{n}^{(\ell)}|^{p}\bigg{)}^{1/p}\] \[\lesssim(2-\varepsilon)\cdot\big{\|}\bm{W}-\bm{W}^{\star}\big{\|}_ {2}\cdot\log|\mathcal{A}|\cdot p.\]Following the steps in (98) to (100), we have

\[\|\bm{I}_{2}\|_{2}\leq \frac{(1-\varepsilon/2)\gamma}{K}\cdot\Big{(}\|\bm{W}-\bm{W}^{\star }\|_{2}\cdot\sqrt{\frac{d\cdot\log q\cdot\log|\mathcal{A}|}{N}}+\mathbb{E}Z^{( \ell,3)}\Big{)}\] (109) \[\lesssim \frac{(1-\varepsilon/2)\gamma}{K}\cdot\Big{(}\|\bm{W}-\bm{W}^{ \star}\|_{2}\cdot\Big{(}\sqrt{\frac{d\cdot\log q\cdot\log|\mathcal{A}|}{N}}+C \Big{)}\] \[\lesssim \frac{(1-\varepsilon/2)\gamma}{K}\cdot\|\bm{W}-\bm{W}^{\star}\|_{2}\]

with probability at least \(1-q^{-d}\), where the last inequality holds when \(N\gtrsim d\cdot\log q\cdot\log|\mathcal{A}|\).

**Bound of \(\bm{I}_{3}\).** We have

\[\begin{split}& I_{3}\\ =&\mathbb{E}_{(\bm{s},a)\sim\mu_{t}}\,g_{t}(\bm{w}_{\ell,k};\bm{W })-\frac{\partial f}{\partial\bm{w}_{\ell,k}}(\bm{W})\\ =&\mathbb{E}_{(\bm{s},a)\sim\mu_{t}}\Big{(}Q(\bm{W}; \bm{s},a)-Q(\bm{W}^{\star};\bm{s},a)\Big{)}\cdot\frac{\partial Q(\bm{W};\bm{s},a)}{\partial\bm{w}_{\ell,k}}\\ =&\mathbb{E}_{(\bm{s},a)\sim\mu_{t}}\Big{(}Q(\bm{W}; \bm{s},a)-r(\bm{s},a)-\gamma\cdot\mathbb{E}_{\bm{s}^{\prime}\sim p^{a}_{\bm{s },a^{\prime}}}\max_{a^{\prime}}Q(\bm{W}^{\star};\bm{s}^{\prime},a^{\prime}) \Big{)}\cdot\frac{\partial Q(\bm{W};\bm{s},a)}{\partial\bm{w}_{\ell,k}}\\ =&\mathbb{E}_{(\bm{s},a)\sim\mu_{t},\bm{s}^{\prime} \sim p^{a}_{\bm{s},a^{\prime}}}\Big{(}Q(\bm{W};\bm{s},a)-r(\bm{s},a)-\gamma \cdot\max_{a^{\prime}}Q(\bm{W}^{\star};\bm{s}^{\prime},a^{\prime})\Big{)} \cdot\frac{\partial Q(\bm{W};\bm{s},a)}{\partial\bm{w}_{\ell,k}}\\ &-\mathbb{E}_{(\bm{s},a)\sim\mu^{\star},\bm{s}^{\prime}\sim p^{a} _{\bm{s},a^{\prime}}}\Big{(}Q(\bm{W};\bm{s},a)-r(\bm{s},a)-\gamma\cdot\max_{a ^{\prime}}Q(\bm{W}^{\star};\bm{s}^{\prime},a^{\prime})\Big{)}\cdot\frac{ \partial Q(\bm{W};\bm{s},a)}{\partial\bm{w}_{\ell,k}}\\ =&\mathbb{E}_{(\bm{s},a)\sim\mu_{t},\bm{s}^{\prime} \sim p^{a}_{\bm{s},a^{\prime}}}\Big{(}Q(\bm{W};\bm{s},a)-r(\bm{s},a)-\gamma \cdot\max_{a^{\prime}}Q(\bm{W}^{\star};\bm{s}^{\prime},a^{\prime})\Big{)} \cdot\frac{\partial Q(\bm{W};\bm{s},a)}{\partial\bm{w}_{\ell,k}}\\ &-\mathbb{E}_{(\bm{s},a)\sim\mu^{\star},\bm{s}^{\prime}\sim p^{a }_{\bm{s},a^{\prime}}}\Big{(}Q(\bm{W};\bm{s},a)-r(\bm{s},a)-\gamma\cdot\max_{ a^{\prime}}Q(\bm{W}^{\star};\bm{s}^{\prime},a^{\prime})\Big{)}\cdot\frac{\partial Q (\bm{W};\bm{s},a)}{\partial\bm{w}_{\ell,k}}\end{split}\] (110)

Then, we have

\[\begin{split}|I_{3}|=&\bigg{|}\int_{(\bm{s},a)} \int_{\bm{s}^{\prime}}\Big{(}Q(\bm{W};\bm{s},a)-r(\bm{s},a)-\gamma\cdot\max_{ a^{\prime}}Q(\bm{W}^{\star};\bm{s}^{\prime},a^{\prime})\Big{)}\cdot\frac{ \partial Q(\bm{W};\bm{s},a)}{\partial\bm{w}_{\ell,k}}\\ &\cdot\big{(}\mu^{\star}(d\bm{s},da)\mathcal{P}(d\bm{s}^{\prime}| \bm{s},a)-\mu_{t}(d\bm{s},da)\mathcal{P}(d\bm{s}^{\prime}|\bm{s},a)\big{)} \bigg{|}\\ \leq&\Big{|}Q(\bm{W};\bm{s},a)-r(\bm{s},a)-\gamma \cdot\max_{a^{\prime}}Q(\bm{W}^{\star};\bm{s}^{\prime},a^{\prime})\Big{|} \cdot\Big{|}\frac{\partial Q(\bm{W};\bm{s},a)}{\partial\bm{w}_{\ell,k}}\Big{|} \\ &\cdot\bigg{|}\int_{(\bm{s},a)}\int_{\bm{s}^{\prime}}\big{(}\mu^{ \star}(d\bm{s},da)\mathcal{P}(d\bm{s}^{\prime}|\bm{s},a)-\mu_{t}(d\bm{s},da) \mathcal{P}(d\bm{s}^{\prime}|\bm{s},a)\big{)}\bigg{|}\\ =&\Big{|}Q(\bm{W};\bm{s},a)-r(\bm{s},a)-\gamma\cdot \max_{a^{\prime}}Q(\bm{W}^{\star};\bm{s}^{\prime},a^{\prime})\Big{|}\cdot \Big{|}\frac{\partial Q(\bm{W};\bm{s},a)}{\partial\bm{w}_{\ell,k}}\Big{|} \\ &\cdot\bigg{[}(1-\varepsilon)\cdot\Big{|}\int_{(\bm{s},a)}\int_{ \bm{s}^{\prime}}\big{(}\mu^{\star}(d\bm{s},da)\mathcal{P}(d\bm{s}^{\prime}| \bm{s},a)-\mu_{t,1}(d\bm{s},da)\mathcal{P}(d\bm{s}^{\prime}|\bm{s},a)\big{)} \Big{|}\\ &\qquad+\varepsilon\cdot\Big{|}\int_{(\bm{s},a)}\int_{\bm{s}^{ \prime}}\big{(}\mu^{\star}(d\bm{s},da)\mathcal{P}(d\bm{s}^{\prime}|\bm{s},a)- \mu_{t,2}(d\bm{s},da)\mathcal{P}(d\bm{s}^{\prime}|\bm{s},a)\big{)}\Big{|} \bigg{]}\\ \leq&\frac{R_{\max}}{1-\gamma}\cdot\bigg{[}(1- \varepsilon)\cdot\Big{|}\int_{(\bm{s},a)}\int_{\bm{s}^{\prime}}\big{(}\mu^{ \star}(d\bm{s},da)\mathcal{P}(d\bm{s}^{\prime}|\bm{s},a)-\mu_{t,1}(d\bm{s},da) \mathcal{P}(d\bm{s}^{\prime}|\bm{s},a)\big{)}\Big{|}\\ &\qquad+\varepsilon\cdot\Big{|}\int_{(\bm{s},a)}\int_{\bm{s}^{ \prime}}\big{(}\mu^{\star}(d\bm{s},da)\mathcal{P}(d\bm{s}^{\prime}|\bm{s},a)- \mu_{t,2}(d\bm{s},da)\mathcal{P}(d\bm{s}^{\prime}|\bm{s},a)\big{)}\Big{|} \bigg{]}.\end{split}\]Then, we have

\[\Big{|}\int_{(\bm{s},a)}\int_{\bm{s}^{\prime}}\big{(}\mu^{\star}(d \bm{s},da)\mathcal{P}(d\bm{s}^{\prime}|\bm{s},a)-\mu_{t,1}(d\bm{s},da)\mathcal{P} (d\bm{s}^{\prime}|\bm{s},a))\Big{|}\] (112) \[= \Big{|}\int_{(\bm{s},a)}\int_{\bm{s}^{\prime}}\big{(}\mathcal{P}^{ \star}(d\bm{s})\pi^{\star}(da|\bm{s})\mathcal{P}(d\bm{s}^{\prime}|\bm{s},a)- \mathcal{P}_{t,1}(d\bm{s})\pi_{t,1}(da|d\bm{s})\mathcal{P}(d\bm{s}^{\prime}| \bm{s},a)\big{)}\Big{|}\] \[\leq \Big{|}\int_{(\bm{s},a)}\int_{\bm{s}^{\prime}}\big{(}\mathcal{P}^ {\star}(d\bm{s})-\mathcal{P}_{t,1}(d\bm{s})\big{)}\pi^{\star}(da|\bm{s}) \mathcal{P}(d\bm{s}^{\prime}|\bm{s},a)\Big{|}\] \[+\Big{|}\int_{(\bm{s},a)}\int_{\bm{s}^{\prime}}\mathcal{P}_{t,1} (d\bm{s})\big{(}\pi_{t,1}(da|d\bm{s})-\pi^{\star}(da|d\bm{s})\big{)}\mathcal{P} (d\bm{s}^{\prime}|\bm{s},a)\Big{|}\] \[\leq |\mathcal{A}|\cdot C_{t}.\]

Therefore, the bound of \(I_{3}\) can be found as

\[|I_{3}|\lesssim\frac{R_{\max}}{1-\gamma}\cdot|\mathcal{A}|\cdot \big{(}(1-\varepsilon)C_{t}+\varepsilon\cdot C_{t}\big{)}\] (113) \[= C_{d}\cdot\big{(}C_{t}+(1-C_{t})\varepsilon\big{)}\cdot\frac{R_ {\max}}{1-\gamma},\]

where \(C_{d}=|\mathcal{A}|\).

In conclusion, let \(\bm{\alpha}\in\mathbb{R}^{Kd}\) and \(\bm{\alpha}_{j}\in\mathbb{R}^{d}\) with \(\bm{\alpha}=[\bm{\alpha}_{1}^{T},\bm{\alpha}_{2}^{T},\cdots,\bm{\alpha}_{K}^ {T}]^{T}\), we have

\[\|g_{t}(\bm{W})-\nabla f_{t}(\bm{W})\|_{2}\] (114) \[= \Big{|}\bm{\alpha}^{T}\big{(}g_{t}(\bm{W})-\nabla f_{t}(\bm{W}) \big{)}\Big{|}\] \[\leq \sum_{k=1}^{K}\Big{|}\bm{\alpha}_{k}^{T}\big{(}g_{t}(\bm{w}_{ \ell,k};\bm{W})-\frac{\partial f}{\partial\bm{w}_{\ell,k}}(\bm{W})\big{)} \Big{|}\] \[\leq \sum_{k=1}^{K}\Big{\|}g_{t}(\bm{w}_{\ell,k};\bm{W})-\frac{ \partial f}{\partial\bm{w}_{\ell,k}}(\bm{W})\Big{\|}_{2}\cdot\|\bm{\alpha}_{k }\|_{2}\] \[\leq \sum_{k=1}^{K}(\|\bm{I}_{1}\|_{2}+\|\bm{I}_{2}\|_{2}+\|\bm{I}_{3} \|_{2})\cdot\|\bm{\alpha}_{k}\|_{2}\] \[\leq \frac{2-\varepsilon}{K}\sqrt{\frac{d\log q}{N}}\cdot\|\bm{W}-\bm {W}^{\star}\|_{2}+\frac{(1-\varepsilon/2)\gamma}{K}\cdot\|\bm{W}^{(t,0)}-\bm{ W}^{\star}\|_{2}\] \[+C_{d}\cdot\big{(}C_{t}+(1-C_{t})\varepsilon\big{)}\cdot\frac{R_ {\max}}{1-\gamma}\]

with probability at least \(1-q^{-d}\). 

## Appendix E Additional proof of the lemmas in Appendix C

### Proof of Lemma 6

The distance of the second order derivatives of the population risk function \(f(\cdot)\) at point \(\bm{W}\) and \(\bm{W}^{\star}\) can be converted into bounding \(\bm{P}_{1}\), \(\bm{P}_{2}\), which are defined in (116). The major idea in proving \(\bm{P}_{1}\) is to connect the error bound to the angle between \(\bm{W}\) and \(\bm{W}^{\star}\) given \(\bm{h}^{(\ell)}\) belongs to the sub-Gaussian distribution.

Proof of Lemma 6.: From the definition of \(f\) in (31), we have

\[\frac{\partial^{2}f}{\partial\bm{w}_{\ell,j_{1}}\partial\bm{w}_{ \ell,j_{2}}}(\bm{W}^{\star})=\frac{1}{K^{2}}\mathbb{E}_{\bm{x}}\mathcal{J}_{ \ell,k}\phi^{\prime}(\bm{w}_{j_{1}}^{\star\top}\bm{h})\cdot\mathcal{J}_{\ell,k }\phi^{\prime}(\bm{w}_{j_{2}}^{\star\top}\bm{h})\cdot\bm{h}^{\star}\bm{h}^{ \star\top},\] (115) \[\text{and}\quad\frac{\partial^{2}f}{\partial\bm{w}_{\ell,j_{1}} \partial\bm{w}_{\ell,j_{2}}}(\bm{W})=\frac{1}{K^{2}}\mathbb{E}_{\bm{x}}\phi^{ \prime}\mathcal{J}_{\ell,k}^{\star}(\bm{w}_{\ell,j_{1}}^{\top}\bm{h})\cdot \mathcal{J}_{\ell,k}^{\star}\phi^{\prime}(\bm{w}_{\ell,j_{2}}^{\top}\bm{h}) \cdot\bm{h}\bm{h}^{\top},\]where \(\bm{h}=\bm{h}^{(\ell)}(\bm{W})\) and \(\bm{h}^{*}=\bm{h}^{(\ell)}(\bm{W}^{*})\).

Then, we have

\[\frac{\partial^{2}f}{\partial\bm{w}_{\ell,j_{1}}\partial\bm{w}_{ \ell,j_{2}}}(\bm{W}^{*})-\frac{\partial^{2}f}{\partial\bm{w}_{\ell,j_{1}} \partial\bm{w}_{\ell,j_{2}}}(\bm{W})\] (116) \[= \frac{1}{K^{2}}\mathbb{E}_{\bm{x}}\big{[}\mathcal{J}_{\ell,k}^{ \star}\phi^{\prime}(\bm{w}_{\ell,j_{1}}^{\star T}\bm{h}^{*})\mathcal{J}_{\ell, k}^{*}\phi^{\prime}(\bm{w}_{\ell,j_{2}}^{\star T}\bm{h}^{*})\bm{h}^{*}\bm{h}^{* \top}-\mathcal{J}_{\ell,k}\phi^{\prime}(\bm{w}_{\ell,j_{1}}^{\top}\bm{h})\mathcal{ J}_{\ell,k}\mathcal{J}_{\ell,k}\phi^{\prime}(\bm{w}_{\ell,j_{2}}^{\top}\bm{h}) \bm{h}\bm{h}^{\top}\big{]}\] \[= \frac{1}{K^{2}}\mathbb{E}_{\bm{x}}\big{[}\mathcal{J}_{\ell,k}^{ \star}\phi^{\prime}(\bm{w}_{\ell,j_{1}}^{\star T}\bm{h}^{*})\big{(}\mathcal{J} _{\ell,k}^{\star}\phi^{\prime}(\bm{w}_{\ell,j_{2}}^{\star T}\bm{h}^{*})\bm{h}^{ *}\bm{h}^{*\top}-\mathcal{J}_{\ell,k}\phi^{\prime}(\bm{w}_{\ell,j_{2}}^{\top} \bm{h})\bm{h}\bm{h}^{\top}\big{)}\] \[\quad+\mathcal{J}_{\ell,k}\phi^{\prime}(\bm{w}_{\ell,j_{2}}^{\top} \bm{h})\big{(}\mathcal{J}_{\ell,k}^{\star}\phi^{\prime}(\bm{w}_{\ell,j_{1}}^{ \star T}\bm{h})\bm{h}^{*}\bm{h}^{*\top}-\mathcal{J}_{\ell,k}\phi^{\prime}(\bm{ w}_{\ell,j_{1}}^{\top}\bm{h})\bm{h}\bm{h}^{\top}\big{)}\big{]}\] \[:= \frac{1}{K^{2}}(\bm{P}_{1}+\bm{P}_{2}).\]

For any \(\bm{a}\in\mathbb{R}^{K_{\ell}}\) with \(\|\bm{a}\|_{2}=1\), we have

\[\bm{a}^{\top}\bm{P}_{1}\bm{a}= \mathbb{E}_{\bm{x}}\mathcal{J}_{\ell,k}^{*}\phi^{\prime}(\bm{w}_{ \ell,j_{1}}^{\star T}\bm{h}^{*})\Big{(}\mathcal{J}_{\ell,k}^{*}\phi^{\prime}( \bm{w}_{\ell,j_{2}}^{\star T}\bm{h}^{*})(\bm{a}^{\top}\bm{h}^{*})^{2}- \mathcal{J}_{\ell,k}\phi^{\prime}(\bm{w}_{\ell,j_{2}}^{\top}\bm{h})(\bm{a}^{ \top}\bm{h})^{2}\Big{)}.\] (117)

Then, we have

\[|\bm{a}^{\top}\bm{P}_{1}\bm{a}|= \Big{|}\mathbb{E}_{\bm{x}}\mathcal{J}_{\ell,k}^{*}\phi^{\prime}( \bm{w}_{\ell,j_{1}}^{\star T}\bm{h}^{*})\Big{(}\mathcal{J}_{\ell,k}^{*}\phi^{ \prime}(\bm{w}_{\ell,j_{2}}^{\star T}\bm{h}^{*})(\bm{a}^{\top}\bm{h}^{*})^{2}- \mathcal{J}_{\ell,k}\phi^{\prime}(\bm{w}_{\ell,j_{2}}^{\top}\bm{h})(\bm{a}^{ \top}\bm{h})^{2}\Big{)}\Big{|}\] (118) \[\leq \mathbb{E}_{\bm{x}}\Big{|}\mathcal{J}_{\ell,k}^{*}\phi^{\prime}( \bm{w}_{\ell,j_{2}}^{\star T}\bm{h}^{*})(\bm{a}^{\top}\bm{h}^{*})^{2}- \mathcal{J}_{\ell,k}\phi^{\prime}(\bm{w}_{\ell,j_{2}}^{\star\top}\bm{h}^{*})( \bm{a}^{\top}\bm{h})^{2}\Big{|}\] \[\leq \mathbb{E}_{\bm{x}}\Big{|}\mathcal{J}_{\ell,k}^{*}\phi^{\prime}( \bm{w}_{\ell,j_{2}}^{\star T}\bm{h}^{*})(\bm{a}^{\top}\bm{h}^{*})^{2}- \mathcal{J}_{\ell,k}^{*}\phi^{\prime}(\bm{w}_{\ell,j_{2}}^{\star\top}\bm{h}^{*}) (\bm{a}^{\top}\bm{h})^{2}\Big{|}\] \[+\mathbb{E}_{\bm{x}}\Big{|}\mathcal{J}_{\ell,k}^{*}\phi^{\prime}( \bm{w}_{\ell,j_{2}}^{\star\top}\bm{h}^{*})(\bm{a}^{\top}\bm{h})^{2}-\mathcal{ J}_{\ell,k}\phi^{\prime}(\bm{w}_{\ell,j_{2}}^{\star\top}\bm{h})(\bm{a}^{\top}\bm{h})^{2} \Big{|}\] \[+\mathbb{E}_{\bm{x}}\Big{|}\mathcal{J}_{\ell,k}\phi^{\prime}( \bm{w}_{\ell,j_{2}}^{\star\top}\bm{h})(\bm{a}^{\top}\bm{h})^{2}-\mathcal{J}_{ \ell,k}\phi^{\prime}(\bm{w}_{\ell,j_{2}}^{\top}\bm{h})(\bm{a}^{\top}\bm{h})^{2} \Big{|}\] \[\lesssim \|\bm{W}-\bm{W}^{*}\|_{2}+\|\bm{W}-\bm{W}^{*}\|_{2}\] \[+\mathbb{E}_{\bm{x}}\Big{|}(\phi^{\prime}(\bm{w}_{\ell,j_{2}}^{ \star\top}\bm{h})-\phi^{\prime}(\bm{w}_{\ell,j_{2}}^{\star\top}\bm{h}))\cdot(\bm {a}^{\top}\bm{h})^{2}\Big{|}\] \[\lesssim \|\bm{W}-\bm{W}^{*}\|_{2}+\mathbb{E}_{\bm{x}}\Big{|}(\phi^{\prime}( \bm{w}_{\ell,j_{2}}^{\star\top}\bm{h})-\phi^{\prime}(\bm{w}_{\ell,j_{2}}^{\star \top}\bm{h}))\cdot(\bm{a}^{\top}\bm{h})^{2}\Big{|}.\]

Utilizing the Gram-Schmidt process, we can demonstrate the existence of a set of normalized orthonormal vectors denoted as \(\mathcal{B}=\{\bm{a},\bm{b},\bm{c},\bm{a}_{4}^{\perp},\cdots,\bm{a}_{d}^{\perp}\}\in \mathbb{R}^{d}\). This set forms an orthogonal and normalized basis for \(\mathbb{R}^{d}\), wherein the subspace spanned by \(\bm{a},\bm{b},\bm{c}\) includes \(\bm{a},\bm{w}_{\ell,j_{2}}\), and \(\bm{w}_{\ell,j_{2}}^{*}\). Then, for any \(\bm{x}\in\mathbb{R}^{d}\), we have a unique \(\bm{z}=[z_{1},\,z_{2},\ \cdots,\ z_{d}]^{\top}\) such that

\[\bm{h}=z_{1}\bm{a}+z_{2}\bm{b}+z_{3}\bm{c}+\cdots+z_{d}\bm{a}_{d}^{\perp}.\]

Because (i) \(\bm{a},\bm{w}_{\ell,j_{2}}\), and \(\bm{w}_{\ell,j_{2}}^{*}\) belongs to the subspace spanned by vectors \(\{\bm{a},\bm{b},\bm{c}\}\) and (ii) \(\bm{a}_{4}^{\perp},\cdots,\bm{a}_{d}^{\perp},\cdots\) are orthogonal to \(\bm{a},\bm{b}\), and \(\bm{c}\). Then, we know that

\[\bm{w}_{\ell,j_{2}}^{\star\top}\bm{h}= \bm{w}_{\ell,j_{2}}^{\star\top}(z_{1}\bm{a}+z_{2}\bm{b}+z_{3}\bm{c} +\cdots+z_{d}\bm{a}_{d}^{\perp})\] (119) \[= z_{1}\bm{w}_{\ell,j_{2}}^{\star\top}\bm{a}+z_{2}\bm{w}_{\ell,j_ {2}}^{\star\top}\bm{b}+z_{3}\bm{w}_{\ell,j_{2}}^{\star\top}\bm{c}+\cdots+z_{d} \bm{w}_{\ell,j_{2}}^{\star\top}\bm{a}_{d}^{\perp}\] \[= z_{1}\bm{w}_{\ell,j_{2}}^{\star\top}\bm{a}+z_{2}\bm{w}_{\ell,j_ {2}}^{\star\top}\bm{b}+z_{where \(|\bm{J}_{h}(\bm{z})|\) is the determinant of the Jacobian matrix \(\frac{\partial h}{\partial\bm{z}}\). Since \(\bm{z}\) is a representation of \(\bm{h}\) based on an orthogonal and normalized basis, we have \(|\bm{J}_{h}(\bm{z})|=1\). According to (119), \(I_{4}\) can be rewritten as

\[\begin{split} I_{4}=&\int_{\mathcal{R}_{z}}|\phi^{ \prime}\big{(}\bm{w}_{\ell,j_{2}}^{\top}\widetilde{\bm{h}}\big{)}-\phi^{\prime }\big{(}\bm{w}_{\ell,j_{2}}^{\star T}\widetilde{\bm{h}}\big{)}|\cdot|\bm{a}^{ \top}\widetilde{\bm{h}}|^{2}\cdot f_{Z}(\bm{z})d\bm{z}\\ =&\int_{\mathcal{R}_{z}}|\phi^{\prime}\big{(}\bm{w} _{\ell,j_{2}}^{\top}\widetilde{\bm{h}}\big{)}-\phi^{\prime}\big{(}\bm{w}_{ \ell,j_{2}}^{\star T}\widetilde{\bm{h}}\big{)}|\cdot|\bm{a}^{\top}\widetilde{ \bm{h}}|^{2}\cdot f_{Z}(z_{1},z_{2},z_{3})dz_{1}dz_{2}dz_{3}\end{split}\] (121)

where in the last equality we abuse \(f_{Z}(z_{1},z_{2},z_{3})\) to represent the probability density function of \((z_{1},z_{2},z_{3})\) defined in region \(\mathcal{R}_{z}\).

Next, we show that \(\bm{z}\) is rotational invariant over \(\mathcal{R}_{z}\). Let \(\bm{R}=[\bm{a}\ \bm{b}\ \bm{c}\ \cdots\ \bm{a}_{d}^{\perp}]\), we have \(\bm{h}=\bm{R}\bm{z}\). For any \(\bm{z}^{(1)}\) and \(\bm{z}^{(2)}\) with \(\|\bm{z}^{(1)}\|_{2}=\|\bm{z}^{(2)}\|_{2}\). We define \(\bm{h}^{(1)}=R\bm{z}^{(1)}\) and \(\bm{h}^{(2)}=R\bm{z}^{(2)}\). Since \(\bm{x}\) is rotational invariant and \(\|\bm{h}^{(1)}\|_{2}=\|\bm{h}^{(2)}\|_{2}=\|\bm{z}^{(1)}\|_{2}=\|\bm{z}^{(2)}\|_ {2}\), then we know \(\bm{h}^{(1)}\) and \(\bm{h}^{(2)}\) has the same distribution density. Then, \(\bm{z}^{(1)}\) and \(\bm{z}^{(2)}\) has the same distribution density as well. Therefore, \(\bm{z}\) is rotational invariant over \(\mathcal{R}_{z}\).

Then, we consider spherical coordinates with \(z_{1}=Rcos\phi_{1},z_{2}=Rsin\phi_{1}sin\phi_{2},z_{3}=Rsin\phi_{1}cos\phi_{2}\). Hence, we have

\[I_{4}=\int|\phi^{\prime}\big{(}\bm{w}_{\ell,j_{2}}^{\top}\widetilde{\bm{h}} \big{)}-\phi^{\prime}\big{(}\bm{w}_{\ell,j_{2}}^{\star\top}\widetilde{\bm{h}} \big{)}|\cdot|R\cos\phi_{1}|^{2}\cdot f_{Z}(R,\phi_{1},\phi_{2})\cdot R^{2} \sin\phi_{1}\cdot dRd\phi_{1}d\phi_{2}.\] (122)

Since \(\bm{z}\) is rotational invariant, we have that

\[f_{Z}(R,\phi_{1},\phi_{2})=f_{Z}(R).\] (123)

Then, we have

\[\begin{split} I_{4}=&\int|\phi^{\prime}\big{(}\bm{w} _{\ell,j_{2}}^{\top}(\widetilde{\bm{h}}/R)\big{)}-\phi^{\prime}\big{(}\bm{w}_{ \ell,j_{2}}^{\star T}(\widetilde{\bm{h}}/R)\big{)}|\cdot|R\cos\phi_{1}|^{2} \cdot f_{Z}(R)R^{2}\sin\phi_{1}dRd\phi_{1}d\phi_{2}\\ =&\int_{0}^{\infty}R^{4}f_{z}(R)dR\int_{0}^{\psi_{1 }(R)}\int_{0}^{\psi_{2}(R)}|\cos\phi_{1}|^{2}\cdot\sin\phi_{1}\\ &\cdot|\phi^{\prime}\big{(}\bm{w}_{\ell,j_{2}}^{\top}(\widetilde{ \bm{h}}/R)\big{)}-\phi^{\prime}\big{(}\bm{w}_{\ell,j_{2}}^{\star T}(\widetilde {\bm{h}}/R)\big{)}|d\phi_{1}d\phi_{2}\\ \leq&\int_{0}^{\infty}R^{4}f_{z}(R)dR\int_{0}^{\pi} \int_{0}^{2\pi}\sin\phi_{1}\cdot|\phi^{\prime}\big{(}\bm{w}_{\ell,j_{2}}^{ \top}\bar{\bm{x}}\big{)}-\phi^{\prime}\big{(}\bm{w}_{\ell,j_{2}}^{\star T} \bar{\bm{x}}\big{)}|d\phi_{1}d\phi_{2},\end{split}\] (124)

where the first equality holds because \(\phi^{\prime}\big{(}\bm{w}_{i,j_{2}}^{\top}\bm{h}\big{)}\) only depends on the direction of \(\bm{h}\), and \(\bar{\bm{x}}:=\bm{h}/R=(\cos\phi_{1},\sin\phi_{1}\sin\phi_{2},\sin\phi_{1}\cos \phi_{2})\) in the last inequality.

Because \(\bm{z}\) belongs to the sub-Gaussian distribution, we have \(F_{z}(R)\geq 1-2e^{-\frac{R^{2}}{\sigma^{2}}}\) for some constant \(\sigma>0\). Then, the integration of \(R\) can be represented as

\[\begin{split}\int_{0}^{\infty}R^{4}f_{Z}(R)dR=&\int_{ 0}^{\infty}R^{4}d\big{(}1-F_{z}(R)\big{)}\\ \leq&\int_{0}^{\infty}4R^{3}\big{(}1-F_{z}(R)\big{)}dR \\ \leq&\int_{0}^{\infty}8R^{3}e^{-\frac{R^{2}}{\sigma^{2} }}dR\\ \leq&\frac{32}{\sqrt{2\pi}}\sigma\int_{0}^{\infty}R^{2}e ^{-\frac{R^{2}}{\sigma^{2}}}dR\\ =& 32\sigma^{2}\int_{0}^{\infty}R^{2}\frac{1}{\sqrt{2\pi \sigma^{2}}}e^{-\frac{R^{2}}{\sigma^{2}}}dR,\end{split}\] (125)

where the last inequality comes from the calculation that

\[\begin{split}\int_{0}^{\infty}2R^{2}e^{-\frac{R^{2}}{\sigma^{2}}}dR= \sqrt{2\pi}\sigma^{3},\\ \int_{0}^{\infty}2R^{3}e^{-\frac{R^{2}}{\sigma^{2}}}dR=4\sigma^{4}. \end{split}\] (126)Then, we define \(\widetilde{\bm{x}}\in\mathbb{R}^{K_{\ell}}\) belongs to Gaussian distribution as \(\widetilde{\bm{x}}\sim\mathcal{N}(\bm{0},\sigma^{2}\bm{I})\). Therefore, we have

\[\begin{split} I_{4}&\leq 32\sigma^{2}\cdot\int_{0}^{ \infty}R^{2}\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-\frac{R^{2}}{\sigma^{2}}}dR\int_ {0}^{\pi}\int_{0}^{2\pi}\sin\phi_{1}\cdot|\phi^{\prime}\big{(}\bm{w}_{\ell,j_{ 2}}^{\top}\widetilde{\bm{x}}\big{)}-\phi^{\prime}\big{(}\bm{w}_{\ell,j_{2}}^{ \star\top}\widetilde{\bm{x}}\big{)}|d\phi_{1}d\phi_{2}\\ &=32\sigma^{2}\cdot\mathbb{E}_{z_{1},z_{2},z_{3}}\big{|}\phi^{ \prime}\big{(}\bm{w}_{\ell,j_{2}}^{\top}\widetilde{\bm{x}}\big{)}-\phi^{\prime }\big{(}\bm{w}_{\ell,j_{2}}^{\star\top}\widetilde{\bm{x}}\big{)}|\\ &\approx\mathbb{E}_{\widetilde{\bm{x}}}\big{|}\phi^{\prime}\big{(} \bm{w}_{\ell,j_{2}}^{\top}\widetilde{\bm{x}}\big{)}-\phi^{\prime}\big{(}\bm{w }_{\ell,j_{2}}^{\star\top}\widetilde{\bm{x}}\big{)}|,\end{split}\] (127)

where \(\widetilde{\bm{x}}\) belongs to Gaussian distribution.

Therefore, the inequality bound over a sub-Gaussian distribution is bounded by the one over a Gaussian distribution. In the following contexts, we provide the upper bound of \(\mathbb{E}_{\widetilde{\bm{x}}}\big{|}\phi^{\prime}\big{(}\bm{w}_{\ell,j_{2}} ^{\top}\widetilde{\bm{x}}\big{)}-\phi^{\prime}\big{(}\bm{w}_{\ell,j_{2}}^{\star \top}\widetilde{\bm{x}}\big{)}|\).

Define a set \(\mathcal{A}_{1}=\{\bm{x}|(\bm{w}_{\ell,j_{2}}^{\star\top}\widetilde{\bm{x}})( \bm{w}_{\ell,j_{2}}^{\star\top}\widetilde{\bm{x}})<0\}\). If \(\widetilde{\bm{x}}\in\mathcal{A}_{1}\), then \(\bm{w}_{\ell,j_{2}}^{\star\top}\widetilde{\bm{x}}\) and \(\bm{w}_{\ell,j_{2}}^{\top}\widetilde{\bm{x}}\) have different signs, which means the value of \(\phi^{\prime}\big{(}\bm{w}_{\ell,j_{2}}^{\star\top}\widetilde{\bm{x}}\big{)}\) and \(\phi^{\prime}(\bm{w}_{\ell,j_{2}}^{\star\top}\widetilde{\bm{x}})\) are different. This is equivalent to say that

\[|\phi^{\prime}(\bm{w}_{\ell,j_{2}}^{\top}\widetilde{\bm{x}})-\phi^{\prime}( \bm{w}_{\ell,j_{2}}^{\star\top}\widetilde{\bm{x}})|=\left\{\begin{array}{rl} 1,\text{ if }\widetilde{\bm{x}}\in\mathcal{A}_{1}\\ 0,\text{ if }\widetilde{\bm{x}}\in\mathcal{A}_{1}^{c}\end{array}.\right.\] (128)

Moreover, if \(\widetilde{\bm{x}}\in\mathcal{A}_{1}\), then we have

\[|\bm{w}_{\ell,j_{2}}^{\star T}\widetilde{\bm{x}}|\leq |\bm{w}_{\ell,j_{2}}^{\star T}\widetilde{\bm{x}}-\bm{w}_{\ell,j_{2}}^{ \top}\widetilde{\bm{x}}|\leq\|\bm{w}_{\ell,j_{2}}^{\star}-\bm{w}_{\ell,j_{2} }\|_{2}\cdot\|\widetilde{\bm{x}}\|_{2}.\] (129)

Let us define a set \(\mathcal{A}_{2}\) such that

\[\begin{split}\mathcal{A}_{2}=&\Big{\{}\widetilde{\bm{x}} \Big{|}\frac{|\bm{w}_{\ell,j_{2}}^{\star T}\widetilde{\bm{x}}|}{\|\bm{w}_{\ell,j_{2}}^{\star}\|_{2}\|\widetilde{\bm{x}}\|_{2}}\leq\frac{\|\bm{w}_{\ell,j_{2} }^{\star}-\bm{w}_{\ell,j_{2}}\|_{2}}{\|\bm{w}_{\ell,j_{2}}^{\star}\|_{2}} \Big{\}}\\ =&\Big{\{}\theta_{\widetilde{\bm{x}},\bm{w}_{\ell,j_{2}}^{ \star}}\Big{|}\cos\theta_{\widetilde{\bm{x}},\bm{w}_{\ell,j_{2}}^{\star}}| \leq\frac{\|\bm{w}_{\ell,j_{2}}^{\star}-\bm{w}_{\ell,j_{2}}\|_{2}}{\|\bm{w}_{ \ell,j_{2}}^{\star}\|_{2}}\Big{\}}.\end{split}\] (130)

Hence, we have that

\[\begin{split}\mathbb{E}_{\widetilde{\bm{x}}}|\phi^{\prime}(\bm{w} _{\ell,j_{2}}^{\top}\widetilde{\bm{x}})-\phi^{\prime}(\bm{w}_{\ell,j_{2}}^{ \star T}\widetilde{\bm{x}})|^{2}=&\mathbb{E}_{\widetilde{\bm{x}}}| \phi^{\prime}(\bm{w}_{\ell,j_{2}}^{\top}\widetilde{\bm{x}})-\phi^{\prime}( \bm{w}_{\ell,j_{2}}^{\star T}\widetilde{\bm{x}})|\\ =&\text{Prob}(\widetilde{\bm{x}}\in\mathcal{A}_{1})\\ \leq&\text{Prob}(\widetilde{\bm{x}}\in\mathcal{A}_{2}). \end{split}\] (131)

Since \(\widetilde{\bm{x}}\sim\mathcal{N}(\bm{0},\|\bm{a}\|_{2}^{2}\bm{I})\), \(\theta_{\widetilde{\bm{x}},\bm{w}_{\ell,j_{2}}^{\star}}\) belongs to the uniform distribution on \([-\pi,\pi]\), we have

\[\begin{split}\text{Prob}(\widetilde{\bm{x}}\in\mathcal{A}_{2})= \frac{\pi-\arccos\frac{\|\bm{w}_{\ell,j_{2}}^{\star}-\bm{w}_{\ell,j_{2}}\|_{2}} {\|\bm{w}_{\ell,j_{2}}^{\star}\|_{2}}}{\pi}\leq&\frac{1}{\pi} \tan(\pi-\arccos\frac{\|\bm{w}_{\ell,j_{2}}^{\star}-\bm{w}_{\ell,j_{2}}\|_{2}}{\| \bm{w}_{\ell,j_{2}}^{\star}\|_{2}})\\ =&\frac{1}{\pi}\cot(\arccos\frac{\|\bm{w}_{\ell,j_{2}} ^{\star}-\bm{w}_{\ell,j_{2}}\|_{2}}{\|\bm{w}_{\ell,j_{2}}^{\star}\|_{2}})\\ \leq&\frac{2}{\pi}\frac{\|\bm{w}_{\ell,j_{2}}^{\star}- \bm{w}_{\ell,j_{2}}\|_{2}}{\|\bm{w}_{\ell,j_{2}}^{\star}\|_{2}}\\ \leq&\|\bm{W}_{\ell}^{\star}-\bm{W}_{\ell}\|_{2} \end{split}\] (132)

Hence, (124) and (132) suggest that

\[\begin{split} I_{4}\lesssim\|\bm{W}_{i}-\bm{W}_{i}^{\star}\|_{2} \cdot\|\bm{a}\|_{2}^{2},\\ \text{and}\qquad\|\bm{P}_{1}\|_{2}\leq\|\bm{W}-\bm{W}^{\star}\|_{2}+I_ {4}\lesssim\|\bm{W}-\bm{W}^{\star}\|_{2},\end{split}\] (133)

The same bound that is shown in (133) holds for \(\bm{P}_{2}\) as well.

Therefore, we have

\[\|\nabla_{\ell}^{2}f(\bm{W}^{\star})-\nabla_{\ell}^{2}f(\bm{W})\|_{2}= \max_{\|\bm{\alpha}\|_{2}\leq 1}\left|\bm{\alpha}^{\top}\Big{(} \nabla_{\ell}^{2}f(\bm{W}^{\star})-\nabla_{\ell}^{2}f(\bm{W})\Big{)}\bm{\alpha}\right|\] \[\leq \frac{1}{K^{2}}\sum_{j_{1}=1}^{K}\sum_{j_{2}=1}^{K}\|\bm{P}_{1}+ \bm{P}_{2}\|_{2}\cdot\|\bm{\alpha}_{j_{1}}\|_{2}\cdot\|\bm{\alpha}_{j_{2}}\|_{2}\] \[\lesssim \frac{1}{K^{2}}\cdot\sum_{j_{1}=1}^{K}\sum_{j_{2}=1}^{K}\|\bm{W}- \bm{W}^{\star}\|_{2}\cdot\|\bm{\alpha}_{j_{1}}\|_{2}\|\bm{\alpha}_{j_{2}}\|_{2}\] (134) \[\lesssim \frac{1}{K^{2}}\cdot\sum_{j_{1}=1}^{K}\sum_{j_{2}=1}^{K}\|\bm{W} -\bm{W}^{\star}\|_{2}\cdot\Big{(}\frac{\|\bm{\alpha}_{j_{1}}\|_{2}^{2}+\|\bm{ \alpha}_{j_{2}}\|_{2}^{2}}{2}\Big{)}\] \[\lesssim \frac{1}{K}\cdot\|\bm{W}^{\star}-\bm{W}\|_{2},\]

where \(\bm{\alpha}\in\mathbb{R}^{Kd}\) and \(\bm{\alpha}_{j}\in\mathbb{R}^{K_{\ell}}\) with \(\bm{\alpha}=[\bm{\alpha}_{1}^{\top},\bm{\alpha}_{2}^{\top},\cdots,\bm{\alpha} _{K}^{\top}]^{\top}\). 

### Proof of Lemma 7

We aim to prove that \(\int_{\mathcal{R}}\Big{(}\sum_{j=1}^{K}\bm{\alpha}^{\top}\bm{h}\phi^{\prime}( \bm{w}_{\ell,j}^{\top}\bm{h})\Big{)}^{2}p_{H}(\bm{h})\cdot d\bm{h}\) is strictly greater than zero for any \(\bm{\alpha}\). Therefore, the \(\rho\) in (2) is strictly greater than zero. The proof is inspired by Theorem 3.1 in [22]. It is obviously that \(\Big{(}\sum_{j=1}^{K}\bm{\alpha}^{\top}\bm{h}\phi^{\prime}(\bm{w}_{\ell,j}^{ \top}\bm{h})\Big{)}^{2}\) is greater or equal to zero. Given \(\Big{(}\sum_{j=1}^{K}\bm{\alpha}^{\top}\bm{h}\phi^{\prime}(\bm{w}_{\ell,j}^{ \top}\bm{h})\Big{)}^{2}\) is continuous, we only need to show that \(\alpha\) such that \(\sum_{j=1}^{K}\bm{\alpha}^{\top}\bm{h}\phi^{\prime}(\bm{w}_{\ell,j}^{\top}\bm{ h})\neq 0\) for any \(\alpha\), namely, \(\{\bm{h}\phi^{\prime}(\bm{w}_{\ell,j}^{\top}\bm{h})\}_{j=1}^{K}\) are linear independent. Compared with Theorem 3.1 in [22], we need to address two challenges: (1) the neuron weights \(\bm{w}\) is the random variable in [22] while the input \(\bm{h}\) is the random variable in this paper and (2) the random variable belongs to Gaussian distribution in [22] while the random variable belongs to sub-Gaussian distribution in this paper.

Proof of Lemma 7.: Let \(\mathcal{H}\) be a Hilbert space on \(\mathbb{R}^{K_{\ell}}\), and the inner product of \(\mathcal{H}\) is defined as

\[\langle f,g\rangle=\int_{\mathcal{R}}f(\bm{h})^{\top}g(\bm{h})f_{H}(\bm{h}) \cdot d\bm{h},\quad\forall f,g\in\mathcal{H},\] (135)

where the Lebesgue measure of \(\mathcal{R}\) over \(\mathbb{R}^{K_{\ell}}\) is non-zero. Instead of directly proving \(\int_{\mathcal{R}}\Big{(}\sum_{k=1}^{K}\bm{\alpha}^{\top}\bm{h}\phi^{\prime}( \bm{w}_{k}^{\top}\bm{h})\Big{)}^{2}f_{H}(\bm{h})\cdot d\bm{h}>0\) for any \(\bm{\alpha}\), we note that it is sufficient to prove that \(\{\bm{h}\phi^{\prime}(\bm{w}_{k}^{\top}\bm{h})\}_{k\in[K]}\) are linear independent over the Hilbert space \(\mathcal{H}\). Namely, if \(\{\bm{h}\phi^{\prime}(\bm{w}_{k}^{\top}\bm{h})\}_{k\in[K]}\) are linear independent, we have

\[\bm{\alpha}^{\top}\bm{h}\phi^{\prime}(\bm{w}_{k}^{\top}\bm{h})\neq 0\quad\text{ almost everywhere}.\] (136)

Therefore, we can know that \(\int_{\mathcal{R}}\Big{(}\sum_{j=1}^{K}\bm{\alpha}^{\top}\bm{h}\phi^{\prime}( \bm{w}_{\ell,j}^{\top}\bm{h})\Big{)}^{2}p_{H}(\bm{h})\cdot d\bm{h}\) is strictly greater than zero.

Next, we provide the whole proof for that \(\{x\phi^{\prime}(\bm{w}_{k}^{\top}\bm{h})\}_{k\in[K]}\) are linear independent over the Hilbert space \(\mathcal{H}\).

We define a group of functions \(\{\psi_{j}(\bm{h})\}_{j=1}^{K}\), where \(\psi_{j}(\bm{h})=\bm{h}\phi^{\prime}(\bm{w}_{j}^{\top}\bm{h})\). From the assumption in Lemma 7, we can justify that \(\mathbb{E}_{\bm{h}\sim\mathcal{D}}|\psi_{j}(\bm{h})|^{2}\leq\mathbb{E}_{\bm{h} \sim\mathcal{D}}|\bm{h}|^{2}<\infty\).

Let \(\mathcal{X}_{i}=\{\bm{h}\mid\bm{w}_{i}^{\top}\bm{h}=0\}\) for any \(i\in[K]\). For any fixed \(k\), we can justify that \(\mathcal{X}_{k}\) cannot be covered by other sets \(\{\mathcal{X}_{k}\}_{j\neq k}\) as long as \(\bm{w}_{k}\) does not parallel to any other weights \(\bm{w}_{j}\) with \(j\neq k\). Namely, \(\mathcal{X}_{k}\not\subset\cup_{j\neq k}\mathcal{X}_{j}\). The idea of proving the claim above is that the intersection of \(\mathcal{X}_{j}\) and \(\mathcal{X}_{k}\) is only a hyperplane in \(\mathcal{X}_{k}\). The union of finite many hyperplanes is not even a measurable space and thus cannot cover the original space. Formally, we provide the formal proof for this claim as follows.

Let \(\lambda\) be the Lebesgue measure on \(\mathcal{X}_{k}\), then \(\lambda(\mathcal{X}_{k})>0\). When \(\bm{w}_{j}\) does not parallel to \(\bm{w}_{k}\), \(\mathcal{X}_{k}\cap\mathcal{X}_{j}\) is only a hyperplane in \(\mathcal{X}_{k}\) for \(j\neq k\). Hence, we have \(\lambda(\mathcal{X}_{j}\cap\mathcal{X}_{k})=0\). Next, we have

\[\lambda\big{(}\mathcal{X}_{k}\cap(\cup_{j\neq k}\mathcal{X}_{k})\big{)}\leq \sum_{j\neq k}\lambda(\mathcal{X}_{k}\cap\mathcal{X}_{j})=0.\] (137)

Therefore, we have

\[\lambda\big{(}\mathcal{X}_{k}/(\cup_{j\neq k}\mathcal{X}_{k})\big{)}=\lambda( \mathcal{X}_{k})-\lambda\big{(}\mathcal{X}_{k}\cap(\cup_{j\neq k}\mathcal{X}_{ k})\big{)}=\lambda(\mathcal{X}_{k})>0.\] (138)

Therefore, we have \(\mathcal{X}_{k}/(\cup_{j\neq k}\mathcal{X}_{j})\) is not empty, which means that \(\mathcal{X}_{k}\not\subset\cup_{j\neq k}\mathcal{X}_{j}\).

Next, Since \(\mathcal{X}_{k}/(\cup_{j\neq k}\mathcal{X}_{j})\) is not an empty set, there exists a point \(\bm{z}_{k}\in\mathcal{X}_{k}/(\cup_{j\neq k}\mathcal{X}_{j})\) and \(r_{0}>0\) such that

\[\mathcal{B}(\bm{z}_{k},r)\cap\mathcal{D}_{j}=\emptyset\quad\text{with}\quad \forall r\leq r_{0}\text{ and }j\neq k,\] (139)

where \(\mathcal{B}(\bm{z}_{k},r)\) stands for a ball centered at \(\bm{z}_{k}\) with a radius of \(r\). Then, we divide \(\mathcal{B}(\bm{z}_{k},r)\) into two disjoint subsets such that

\[\mathcal{B}_{r}^{+}=\mathcal{B}(\bm{z}_{k},r)\cap\{\bm{h}\mid\bm{w}_{k}^{\top }\bm{h}>0\},\] (140)

\[\mathcal{B}_{r}^{-}=\mathcal{B}(\bm{z}_{k},r)\cap\{\bm{h}\mid\bm{w}_{k}^{\top }\bm{h}<0\}.\]

Because \(\bm{z}_{k}\) is a boundary point of \(\{\bm{h}|\bm{w}_{k}^{\top}\bm{h}=0\}\), both \(\mathcal{B}_{r}^{+}\) and \(\mathcal{B}_{r}^{-}\) are non-empty.

Note that \(\psi_{j}(\bm{h})\) is continuous at any point except for the ones in \(\mathcal{X}_{j}\). Then, for any \(j\neq k\), we know that \(\phi_{j}(\bm{w}_{k}^{\top}\bm{h})\) is continuous at point \(\bm{z}_{k}\) since \(\bm{z}_{k}\not\in\mathcal{X}_{j}\). Hence, it is easy to verify that

\[\lim_{r\to 0_{+}}\frac{1}{\lambda(\mathcal{B}_{r}^{+})}{\int_{\mathcal{B}_{r} ^{+}}\psi_{k}(\bm{h})d\bm{h}}=\lim_{r\to 0_{-}}\frac{1}{\lambda(\mathcal{B}_{r}^{-} )}{\int_{\mathcal{B}_{r}^{+}}\psi_{k}(\bm{h})d\bm{h}}=\psi_{k}(\bm{z}_{k}).\] (141)

While for \(\psi_{k}\), we know that \(\psi_{k}(\bm{h})\equiv 0\) for \(\bm{h}\in\mathcal{B}_{r}^{-}\), (ii) \(\psi_{k}(\bm{h})=\bm{h}\) for \(\bm{h}\in\mathcal{B}_{r}^{+}\). Hence, it is easy to verify that

\[\begin{split}\lim_{r\to 0_{+}}\frac{1}{\lambda(\mathcal{B}_{r}^{+})}{ \int_{\mathcal{B}_{r}^{+}}\psi_{k}(\bm{h})d\bm{h}}=\bm{z}_{k}\\ \lim_{r\to 0_{-}}\frac{1}{\lambda(\mathcal{B}_{r}^{-})}{\int_{ \mathcal{B}_{r}^{+}}\psi_{k}(\bm{h})d\bm{h}}=0.\end{split}\] (142)

Now let us proof that \(\{\psi_{j}\}_{j=1}^{K}\) are linear independent by contradiction. Suppose \(\{\psi_{j}\}_{j=1}^{K}\) are linear dependent, we have

\[\sum_{j=1}^{K}\alpha_{j}\psi_{j}(\bm{h})\equiv 0,\quad\forall\bm{h}.\] (143)

Then, we have

\[\begin{split}\lim_{r\to 0_{+}}\frac{1}{\lambda(\mathcal{B}_{r}^{+})}{ \int_{\mathcal{B}_{r}^{+}}\sum_{j=1}^{K}\alpha_{j}\psi_{j}(\bm{h})d\bm{h}}=0 \\ \lim_{r\to 0_{+}}\frac{1}{\lambda(\mathcal{B}_{r}^{-})}{\int_{ \mathcal{B}_{r}^{+}}\sum_{j=1}^{K}\alpha_{j}\psi_{j}(\bm{h})d\bm{h}}=0\end{split}\] (144)

Then, we have

\[\begin{split} 0=&\lim_{r\to 0_{+}}\frac{1}{\lambda( \mathcal{B}_{r}^{+})}{\int_{\mathcal{B}_{r}^{+}}\sum_{j=1}^{K}\alpha_{j}\psi_{j} (\bm{h})d\bm{h}}-\lim_{r\to 0_{+}}\frac{1}{\lambda(\mathcal{B}_{r}^{-})}{\int_{ \mathcal{B}_{r}^{+}}\sum_{j=1}^{K}\alpha_{j}\psi_{j}(\bm{h})d\bm{h}}\\ =&\alpha_{k}\bm{z}_{k}\end{split}\] (145)

where the last equality comes from (141) and (142).

Note that \(\bm{z}_{k}\) cannot be \(\bm{0}\) because \(\bm{z}_{k}\not\in\mathcal{X}_{j}\). Therefore, we have \(\alpha_{k}=0\). Similarly to (145), we can obtain that \(\alpha_{j}=0\) by define \(\bm{z}_{j}\) following the definition of \(\bm{z}_{k}\) for any \(j\in[K]\). Then, we know that (143) holds if and only if \(\bm{\alpha}=\bm{0}\), which contradicts the assumption that \(\{\psi_{j}\}_{j=1}^{K}\) are linear dependent.

In conclusion, we know that \(\{\psi_{j}\}_{j=1}^{K}\) are linear independent, and \({\int_{\mathcal{R}}\Big{(}\sum_{j=1}^{K}\bm{\alpha}^{\top}\bm{h}\phi^{\prime}( \bm{w}_{\ell,j}^{\top}\bm{h})\Big{)}^{2}}p_{H}(\bm{h})\cdot d\bm{h}\) is strictly greater than zero.

### Proof of Lemma 8

Proof of Lemma 8.: From the definition of (39), we have

\[\begin{split}&\|\bm{h}^{(\ell)}(\bm{W})-\bm{h}^{(\ell)}(\bm{W}^{ \star})\|_{2}\\ =&\|\phi\big{(}\bm{W}_{\ell-1}^{\top}\bm{h}^{(\ell-1) }(\bm{W})\big{)}-\phi\big{(}\bm{W}_{\ell-1}^{\star\top}\bm{h}^{(\ell-1)}(\bm{W}^ {\star})\big{)}\|_{2}\\ =&\|\phi\big{(}\bm{W}_{\ell-1}^{\top}\bm{h}^{(\ell-1) }(\bm{W})\big{)}-\phi\big{(}\bm{W}_{\ell-1}^{\star\top}\bm{h}^{(\ell-1)}(\bm{W} )\big{)}\\ &+\phi\big{(}\bm{W}_{\ell-1}^{\star\top}\bm{h}^{(\ell-1)}(\bm{W} )\big{)}-\phi\big{(}\bm{W}_{\ell-1}^{\star\top}\bm{h}^{(\ell-1)}(\bm{W}^{ \star})\big{)}\|_{2}\\ \leq&\|\phi\big{(}\bm{W}_{\ell-1}^{\top}\bm{h}^{( \ell-1)}(\bm{W})\big{)}-\phi\big{(}\bm{W}_{\ell-1}^{\star\top}\bm{h}^{(\ell-1) }(\bm{W}^{\star})\big{)}\|_{2}\\ &+\|\phi\big{(}\bm{W}_{\ell-1}^{\star\top}\bm{h}^{(\ell-1)}(\bm{ W})\big{)}-\phi\big{(}\bm{W}_{\ell-1}^{\star\top}\bm{h}^{(\ell-1)}(\bm{W}^{ \star})\big{)}\|_{2}\\ \leq&\|\bm{W}_{\ell-1}-\bm{W}_{\ell-1}^{\star}\|_{2 }\cdot\|\bm{h}^{(\ell-1)}(\bm{W})\|_{2}+\|\bm{h}^{(\ell-1)}(\bm{W})-\bm{h}^{( \ell-1)}(\bm{W}^{\star})\|_{2}.\end{split}\] (146)

With the assumption in the Lemma 8 such that \(\bm{W}\) is close enough to \(\bm{W}^{\star}\), we have

\[\|\bm{W}_{i}\|_{2}\leq\|\bm{W}_{i}^{\star}\|_{2}+\|\bm{W}_{i}-\bm{W}_{i}^{ \star}\|_{2}\lesssim 1.\] (147)

Therefore, we have

\[\|\bm{h}^{(i)}(\bm{W})\|_{2}\leq\|\bm{W}_{i}\|_{2}\cdots\|\bm{W}_{1}\|_{2} \cdot\|\bm{x}\|_{2}\lesssim\|\bm{x}\|_{2}.\] (148)

Then, we have

\[\begin{split}&\|\bm{h}^{(\ell)}(\bm{W})-\bm{h}^{(\ell)}(\bm{W}^{ \star})\|_{2}\\ \leq&\|\bm{W}_{\ell-1}-\bm{W}_{\ell-1}^{\star}\|_{2} \cdot\|\bm{x}\|_{2}+\|\bm{h}^{(\ell-1)}(\bm{W})-\bm{h}^{(\ell-1)}(\bm{W}^{ \star})\|_{2}\\ \leq&\sum_{i=1}^{\ell-1}\|\bm{W}_{i}-\bm{W}_{i}^{ \star}\|_{2}\cdot\|\bm{x}\|_{2}+\|\bm{h}^{(1)}(\bm{W})-\bm{h}^{(1)}(\bm{W}^{ \star})\|_{2}\\ =&\sum_{i=1}^{\ell-1}\|\bm{W}_{i}-\bm{W}_{i}^{\star }\|_{2}\cdot\|\bm{x}\|_{2}+\|\bm{x}-\bm{x}\|_{2}\\ =&\sum_{i=1}^{\ell-1}\|\bm{W}_{i}-\bm{W}_{i}^{\star }\|_{2}\cdot\|\bm{h}^{(i-1)}(\bm{W})\|_{2}\\ \leq&\|\bm{W}-\bm{W}^{\star}\|_{2}\cdot\|\bm{x}\|_{2},\end{split}\] (149)

which completes the proof. 

## Appendix F Additional experiments

In this section, we provide numerical justification that our theoretical findings are aligned with DDQN through the Atari Breakout game The neural network follows the same architecture as the one used in Section 5. The algorithm terminates if the average score over the recent \(100\) episodes does not improve or the algorithm reaches the maximum episode set as \(200\), which is around \(4\times 10^{5}\) training steps. The testing score is calculated based on a similar setup as the training process by fixing the maximum memory size \(N\) as \(2000\) and greedy policy, i.e., \(\varepsilon=0\). Each point in the plot is averaged over \(10\) experiments with an error bar representing the standard deviation.

**Estimation errors with respect to the sample complexity \(N\).** We follow the setup in Section 5 to use the expected cumulative reward as the estimation error of the learned model to the optimal Q-value function. The \(\varepsilon_{t}\) in \(\varepsilon\)-greedy policy decreases geometrically from \(1\) to \(0.01\). We vary the number of samples in the replay buffer from \(3000\) to \(10000\). Figure 4 shows that the test error is almost linear in \(1/\sqrt{N}\), which is consistent with our characterization in (20). In addition, experiments with a large \(N\) have a shorter error bar indicating a more stable learning performance with a large sample complexity as shown in (12).

**Convergence with different selections of \(\varepsilon\).** Figure 5 illustrates the convergence rate when \(\varepsilon_{t}\) in the \(\varepsilon\)-greedy policy changes. For each point, \(\varepsilon_{0}\) is selected as the value in the x-axis, and we decrease \(\varepsilon_{t}\) geometrically as the iteration \(t\) increases. Each point is averaged over \(10\) independent trials. We can see that the convergence rate is a linear function of \(c_{\varepsilon}\), matching our findings in (19).

## Appendix G Extension to non i.i.d. samples

**Assumption 3**.: _At any fixed outer iteration \(t\), the behavior policy \(\pi_{t}\) and transition kernel \(\mathcal{P}_{t}\) satisfy_

\[\sup_{\bm{s}\in\mathcal{S}}\;d_{TV}\big{(}\mathbb{P}(\bm{s}_{\tau} \in\cdot)\;|\;\bm{s}_{0}=\bm{s}),\mathcal{P}_{t}\big{)}\leq\lambda\nu^{\tau}, \quad\forall\;\tau\geq 0\] (150)

_for some constant \(\lambda>0\) and \(\nu\in(0,1)\), where \(d_{TV}\) denotes the total-variation distance between the probability measures._

Assumption 3 assumes the Markov chain \(\{\bm{s}_{t}\}\) induced by the behavior policy, i.e., \(\varepsilon_{t}\)-greedy policy at \(t\)-th outer loop, is uniformly ergodic with the corresponding invariant measure \(\mathcal{P}_{t}\). Compared with i.i.d. cases, we need to handle an additional error term when bounding the distance between the \(g_{t}\) and \(\nabla f\) as shown in (91). Therefore, the upper bound in Lemma 3 changes, which suggests an additional term in the final bound.

We present the major theoretical findings for non-i.i.d. samples in Theorem 2. The major proofs in this context follow similar steps to the proof of Theorem 1, with slight changes in the error bound between the sequences \(g_{t}\) and \(\nabla f\). In this section, we omit the details of the proof for Theorem 2 but provide the proof for Lemma 3 under the assumptions outlined in Assumption 2 to simplify the presentation.

**Theorem 2** (Convergence for non-i.i.d. case).: _Suppose Assumption 1 and (143) hold, the buffer size \(N\) satisfies (13). Let us define \(C_{\max}\) be a constant that is larger than \(C_{t}\) for \(1\leq t\leq T\) and \(C_{d}=|\mathcal{A}|\cdot(1+\log_{\nu}\lambda^{-1}+\frac{1}{1-\nu})\), when \(\varepsilon_{t}\) satisfy_

\[\varepsilon_{t}=\frac{c_{\varepsilon}\cdot\Theta(\sqrt{N})\cdot e _{t}}{(1-C_{\max})\cdot C_{d}\cdot R_{\max}}-\frac{C_{\max}}{1-C_{\max}}\] (151)

_for a fixed constant \(c_{\varepsilon}\in(0,(1-\gamma)^{2}]\), and the initialization satisfies_

\[||\bm{W}^{(0,0)}-\bm{W}^{\star}||_{F}\leq\;\mathcal{O}\Big{(}1- \frac{1-c_{\varepsilon}}{\Theta(\sqrt{N})}\Big{)}\cdot\frac{\rho\cdot\|\bm{W} ^{\star}\|_{F}}{K}.\] (152)

_Then, with the high probability of at least \(1-T\cdot q^{-d}\), we have_

_(C1) The learned weights decay geometrically with_

\[||\bm{W}^{(t+1,0)}-\bm{W}^{\star}||_{F}\leq\big{(}\gamma+c_{ \varepsilon}\cdot(1-\gamma)\big{)}\cdot||\bm{W}^{(t,0)}-\bm{W}^{\star}||_{F}+ \frac{(2+\gamma)R_{\max}\tau^{\star}}{(1-\gamma)\Theta(N)},\] (153)

_(C2) the returned model \(Q(\bm{W}^{(T,0)})\) exhibits an estimation error as_

\[\sup_{(s,a)}\big{|}Q(\bm{W}^{(T,0)})-Q^{\star}\big{|}\leq\frac{C_ {\max}\cdot C_{d}\cdot R_{\max}}{(1-\gamma)^{2}\cdot\Theta(\sqrt{N\cdot T})}+ \frac{(2+\gamma)R_{\max}\tau^{\star}}{(1-\gamma)\Theta(N\cdot T)},\] (154)

_where \(\tau^{\star}=\min\{t\;|\;\lambda\nu^{t}\leq 1/(N\cdot T)\}\)._

[MISSING_PAGE_FAIL:38]

It is easy to verify that

\[\begin{split}\|g_{t}(\bm{w}_{\ell,k};\bm{W})-g_{t}(\tilde{\bm{w}}_{ \ell,k};\widetilde{\bm{W}})\|&\leq(1+\gamma)\cdot\|\bm{W}- \widetilde{\bm{W}}\|,\\ \|\bar{g}_{t}(\bm{w}_{\ell,k};\bm{W})-\bar{g}_{t}(\tilde{\bm{w}}_{ \ell,k};\widetilde{\bm{W}})\|&\leq(1+\gamma)\cdot\|\bm{W}- \widetilde{\bm{W}}\|,\\ \text{ and }\qquad\|g_{t}\|&\lesssim\frac{R_{\max}}{1- \gamma}.\end{split}\] (162)

Then, we have

\[\Delta_{t}(\bm{W})-\Delta_{t}(\widetilde{\bm{W}})\lesssim(1+\gamma)\cdot\|\bm{ W}-\widetilde{\bm{W}}\|_{2}.\] (163)

Therefore, we have

\[\Delta_{t}(\bm{W}^{(t,0)})\leq\Delta_{t}(\bm{W}^{(t-\tau,0)})+\frac{1+\gamma}{ 1-\gamma}\cdot R_{\max}\cdot\sum_{i=t-\tau}^{t-1}\eta_{i}.\] (164)

Then, we need to bound \(\delta_{t}(\bm{W}^{(t-\tau,0)})\).

Let us define the observed tuple \(O_{t}(\bm{s},a,s^{\prime})\) as the collection of the state, action, and the next state at the \(t\)-th outer loop. Note that

\[\bm{W}^{(t-\tau,0)}\longrightarrow\bm{s}_{t-\tau}\longrightarrow\bm{s}_{t} \longrightarrow O_{t}\] (165)

forms a Markov chain introduced by the policy \(\pi_{t-\tau}\).

Let \(\widetilde{\bm{W}}^{(t-\tau,0)}\) and \(\widetilde{O}_{t}\) be independently drawn from the marginal distributions of \(\widetilde{\bm{W}}^{(t-\tau,0)}\) and \(O_{t}\), respectively.

With Lemma 9 in [4], we have

\[\mathbb{E}\;\Delta_{t}(\bm{W}^{(t-\tau,0)},O_{t})-\mathbb{E}\;\Delta_{t}( \widetilde{\bm{W}}^{(t-\tau,0)},\widetilde{O}_{t})\lesssim\;2\sup_{\bm{w},O}| \Delta_{t}(\bm{W},O)|\cdot\lambda\cdot\nu^{\tau}.\] (166)

By definition, we have \(\mathbb{E}\;\Delta_{t}(\widetilde{\bm{W}}^{(t-\tau,0)},\widetilde{O}_{t})=0\) and

\[|\Delta_{t}(\bm{W},O)|\leq\frac{2\;R_{\max}}{1-\gamma}.\] (167)

Therefore, we have

\[\begin{split}\mathbb{E}\Delta_{t}(\bm{W}^{(t,0)})\leq& \mathbb{E}\Delta_{t}(\bm{W}^{(t-\tau,0)})+\frac{1+\gamma}{1-\gamma} \cdot R_{\max}\cdot\sum_{i=t-\tau}^{t-1}\eta_{i}\\ \leq&\frac{R_{\max}}{1-\gamma}\Big{(}\lambda\cdot \nu^{\tau}+(1+\gamma)\cdot\tau\cdot\eta_{t-\tau}\Big{)},\end{split}\] (168)

where the last inequality comes from the fact that the step size \(\eta_{t}\) is non-increasing.

Choose \(\tau^{\star}=\min\big{\{}t=0,1,2,\cdots\;|\;\lambda\nu^{\tau}\leq\eta_{T}\big{\}}\). When \(t\leq\tau^{\star}\), we choose \(\tau=t\) and have

\[\mathbb{E}\Delta_{t}(\bm{W}^{(t,0)})\leq\frac{R_{\max}}{1-\gamma}\cdot\tau^{ \star}\cdot\eta_{0}.\] (169)

When \(t>\tau^{\star}\), we can choose \(\tau=\tau^{\star}\) and obtain

\[\mathbb{E}\Delta_{t}(\bm{W}^{(t,0)})\leq\frac{R_{\max}}{1-\gamma}\cdot(1+ \gamma)\tau^{\star}\cdot\eta_{t-\tau^{\star}}.\] (170)

Combining (169) and (170), we have

\[|I_{4}|\leq\frac{R_{\max}}{1-\gamma}\cdot(1+\gamma)\tau^{\star}\cdot\eta_{\max \{0,t-\tau^{\star}\}},\] (171)

where \(\tau^{\star}=\min\{t\;|\;\lambda\nu^{t}\leq\eta_{T}\}\).