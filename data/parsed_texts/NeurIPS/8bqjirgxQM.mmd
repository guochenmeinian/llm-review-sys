# Understanding Social Reasoning in Language Models

with Language Models

Kanishk Gandhi &J.-Philipp Franken &Tobias Gerstenberg &Noah D. Goodman

Stanford University

{kanishk.gandhi, jphilipp}@stanford.edu

Equal Contribution.

###### Abstract

As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. To address these challenges, we present a novel framework for procedurally generating evaluations _with_ LLMs by populating causal templates. Using our framework, we create a new social reasoning benchmark (**BigToM**) _for_ LLMs which consists of 25 controls and 5,000 model-written evaluations. We find that human participants rate the quality of our benchmark higher than previous crowd-sourced evaluations and comparable to expert-written evaluations. Using **BigToM**, we evaluate the social reasoning capabilities of a variety of LLMs and compare model performances with human performance. Our results suggest that GPT4 has ToM capabilities that mirror human inference patterns, though less reliable, while other LLMs struggle.2

Footnote 2: https://sites.google.com/view/social-reasoning-lms

## 1 Introduction

Humans continually try to understand what others think, want, and feel.

We try to understand what people have done and predict what they might do next by inferring their mental states. This capability, often referred to as "Theory of Mind" (ToM), is the foundation of social interaction [45, 22, 25, 10, 38]. With Large Language Models (LLMs) playing a growing role in our lives, assessing their ability to model human mental states is key for guaranteeing effective interactions. This involves evaluating the current abilities of LLMs, understanding their failure modes, and discovering ways to improve them. LLMs with ToM-like abilities could be better at teaching us, learning from us, communicating with us, collaborating with us, and understanding us [15, 20, 30, 11, 36].

Recent attempts at understanding social reasoning in LLMs have used crowd-sourced data, SocialIQA [32], data from synthetic templates, ToMi [21], or (modified) tests from psychology designed to evaluate human capabilities [e.g. 24, 42, 18, 5, 23, 41]. Sap et al. [33] used SocialIQA and ToMi to show that GPT-3 had limited social reasoning capabilities. However, their findings are challenging to interpret due to limitations in their methodology. SocialIQA has several ambiguous examples and stories that do not effectively test the desired social reasoning behaviors. In comparison, ToMi suffers from ambiguous narratives with unclear perceptual descriptions and additional confounding factors in reasoning like memory loads or tracking requirements. Moreover, both of these datasets lack control conditions making it difficult to identify precisely where models make mistakes. The results of studies with tests developed by psychologists show some signs of ToM capabilites in LLMs[18; 5]. However, when LLMs such as GPT-3 [4] succeed in scenarios, they often fail dramatically on trivial alterations [42; 24; 35]. Despite their careful design, concerns about the limited test set [24; 18] and potential dataset leakage from modifications to the Sally-Anne task [3] in [5; 18; 24], suggest caution in the interpretation of these results (see App. D for a detailed discussion).

To address these shortcomings, we present a novel framework for procedurally designing synthetic ToM evaluations from causal templates (Fig. 1). By representing ToM scenarios as causal graphs, we can systematically intervene on variables, generate control conditions, and probe different aspects of an LLM's ToM capabilities. More concretely, consider the scenario in Fig. 1a: Here, _"Noor"_ is an agent with a desire, _"to make a latte with oat milk"_, who performed an action, _"fills it with oat milk"_, resulting in a belief, _"she believes that the pitcher has oat milk"_. Next, a _"Causal Event"_

Figure 1: Illustration of our template-based Theory-of-Mind (ToM) scenarios. [a] The causal template and an example scenario including prior desires, actions, and beliefs, and a causal event that changes the state of the environment. [b] Testing _Forward Belief_ inference by manipulating an agent’s percepts. TB = True Belief. FB = False Belief. [c]_Forward Action_ inference from an agent’s percepts which requires additional inferences over unknown beliefs. [d]_Backward Belief_ inference requires joint inferences over unknown percepts and beliefs from an agent’s observed actions. Error bars for human performance represent 95% bootstrapped confidence intervals of the mean.

changes the state of the environment ("_oat milk_" \(\rightarrow\)_"almond milk_"). Given this setup, we can now manipulate the agent's percept to create True Belief and False Belief conditions. In the True Belief condition, the perception of the causal event is presented, _"Noor sees her coworker swapping the milk"_, and then we test a model's _forward belief_ inference abilities; _"What does Noor believe is in the pitcher?"_ (Fig. 1b). Moreover, we can probe more difficult inferences, such as _forward action_ inferences from an agent's percepts via inferred beliefs (Fig. 1c). In addition to manipulating percepts, we can intervene on an agent's actions to examine a model's _backward belief_ inferences, which is even more difficult as it requires a joint inference over unknown percepts and beliefs (Fig. 1d; SS3).

We design a framework for systematic and diverse evaluations of LLMs in three steps. First, we build a causal template (an abstracted causal model) for the domain of interest, which in our case is ToM. Second, we prompt a language model to populate the variables in the template (yielding a concrete causal model). Third, we construct different evaluation conditions by combining variables from the populated causal template (Fig. 2 and SS3). Our approach is a general method for generating evaluations, applicable in any domain where reasoning traces can be represented as causal graphs.

Overall, our contributions are as follows: (1) We present a framework for generating systematic evaluations from causal templates that help us understand a model's behavior, its failures and successes, through automated, controlled tests. (2) We show the effectiveness of our scalable, cost-efficient method for writing evaluations with language models by comparing its quality to crowd-sourced and expert written tests. (3) Finally, we test ToM reasoning in a variety of LLMs3 using different prompting techniques, and compare model performances with human performance. We find that gpt-4 shows human-like ToM inference patterns, although less reliable, while other LLMs struggle.

Footnote 3: LLMa-65B, text-davinci-003, gpt-3.5-turbo, Claude-v1.3, Claude-2, gpt-4-0314

## 2 Related Work

**Theory-of-Mind in Humans.** Infants, arguably from 12 months of age, can attribute mental states to agents, exhibiting theory of mind reasoning [25]. A classic test to probe this reasoning is the false-belief task [3]: Sally has a doll and puts it in a basket, then leaves the room. While Sally is away, Anne takes the ball out of the basket and puts it into a box. Participants are then asked to predict what happens next: "When Sally comes back, where will she look for her ball?". To answer this question, participants need to infer Sally's beliefs, and realize that her beliefs aren't the same as theirs. Through well-planned experiments, cognitive scientists probe reasoning aspects relating to agents' desires and beliefs [22, 13, 45, 38]. These studies employ control conditions to rule out simple heuristics people might use, while searching for the cognitive mechanisms that underlie human reasoning and behavior [1, 2, 14, 47, 37, 9]. Such experiments have inspired AI researchers to design "behavioral" experiments for probing ToM in AI models [11, 36, 39, 19].

**Theory-of-Mind in Machines.** Initial attempts at building ToM representations in neural network based models [31, 30] used ToM specific tasks to train and test the models. As LLMs scaled and became better at reasoning, researchers used a small set of tests from cognitive science to claim that ToM reasoning had emerged in LLMs (GPT-3, GPT-4) [18, 5]. But, further probing using alterations and diverse scenarios showed that this reasoning was quite brittle [42, 24]. Other tests for social reasoning used crowd-sourced and synthetic evaluations to find mixed results [35, 32, 32, 21, 41]. Despite the abundance of research in this domain, we still don't understand the strengths and weaknesses of LLMs in ToM reasoning. Previous evaluations suffer from one or more of the following issues: reliance on limited evaluations designed for humans [e.g. 18, 24], insufficient control conditions [e.g. 33, 42], limited test cases [e.g. 41, 5], noisy/ambiguous crowd-sourced evaluations [e.g. 33], the risk of dataset leakage [e.g. 18, 41], confounding factors in reasoning [e.g. 21, 42] and possible overfitting of the prompting method [24] (see App. D for a detailed discussion). The goal of our work is to come up with a scalable, replicable framework to understand the reasoning behind predictions made by language models while avoiding the pitfalls that other methods fall into.

**Model-Written Evaluations.**

Advancements in aligning LLMs with instruction-tuning and RL from human feedback (RLHF) have recently shown promising results, such as the generation of a high-quality hate-speech detection dataset with GPT-3 [16, 8], red-teaming [27], and training data generation [34]. The latest work has extended this to the generation of evaluations directly [28]. Perez et al. [28] examined whether generated data can serve as high-quality evaluation data with minimal errors for a variety of novel language model behaviors. These tests, while being scalable, cost-effective and easy to replicate, are still challenging to interpret as they lack structure in the generation of tests. In contrast, Dasgupta et al. [6] show how carefully designed automated tests can find specific failure modes in reasoning. Our work aims to integrate the benefits of these methods, creating a more structured approach to generating and interpreting tests, while preserving scalability, cost-effectiveness, and ease of replication.

## 3 Model-Written Evaluations with Causal Templates

**Preliminaries.** Theory of Mind is the ability to attribute mental states like beliefs, intents, desires, emotions and knowledge to oneself and others. It involves understanding that other people's mental states (latent causes) guide their actions (see Fig. 1a). In this work, we focus on the causal graph linking precepts, beliefs, desires, and actions. We want to test if models are able to perform forward and backward inference over different variables in this graph.

Our goal is to generate ToM evaluations that meet the following criteria: (1) they include control conditions to systematically assess language models' response tendencies and failure modes across different aspects of ToM, (2) they don't directly involve human-designed test items, and (3) they are diverse and scalable. By generating a diverse set of tasks, we wish to specifically target the reasoning involved in ToM inferences, while not focusing on other errors in common-sense reasoning4. To achieve this, we follow [28] and propose using language models to generate their own evaluations, specifically story(\(s\))-question(\(q\))-answer(\(a\)) test items of the format of \((s_{1},q_{1},a_{1}),(s_{2},q_{2},a_{2}),...(s_{N},q_{N},a_{N})\) (examples are shown in Tab. 1). To generate these evaluations, we propose a novel three stage-method: (1) Building a causal template of the domain, (2) populating causal templates using language models, and (3) composing test items for a given condition by "stitching" together template variables into fluent stories (Fig. 2a).

Footnote 4: For example, in Shapira et al. [35], errors in understanding ‘transparent access’ are not ToM inference errors but errors in understanding perceptual access with transparent objects, i.e., not an error in computing what someone knows from what they see. Adding the line: “cagent> can see through transparent <object>.” mitigates these errors with gpt-4.

Figure 2: [a] Three-stage method for generating evaluations: Building a causal template for the domain (left). Creating a prompt template (simplified here; see Fig. 4 for the prompt) from the causal graph and populating template variables using a language model (middle). Composing test items by combining template variables (right). [b] Crowdworker ratings of our model-generated Theory-of-Mind (ToM) evaluations compared to crowd-sourced ToM evaluations and expert-written ToM evaluations. Error bars represent 95% bootstrapped confidence intervals of the mean.

### Stage 1: Building a Causal Template

To build a causal template, we start by defining the variables (see Fig. 0(a) and Fig. 1(a)). The world is set up with a context and description of the agent (_"Noor is a barista [...]"_). Next, we add the initial (prior) values of the variables in the template: desire (_"Noor wants to make a latte"_), percept (_"Noor fills a pitcher with oat milk"_) and belief (_"Noor believes that the pitcher has oat milk"_). Next, a _Causal Event_ changes the state of the environment (_"oat milk"_ _= _"almond milk"_). We can now manipulate the agent's percept of the causal event and the resulting action the agent will take. In this paper, we focus on the following inferences:

**Initial Percept to Initial Belief.** This tests if models understand that percepts (and actions) give rise to beliefs: _"Noor grabs a pitcher and fills it with oat milk"_ _= "Noor believes that the milk pitcher contains oat milk"_. This is a preliminary inference that a model must perform before being able to answer more complicated questions about beliefs or actions following the causal event.

**With vs. Without Initial Belief.** We consider two version of the background (prior) scenario. In version one (_"without initial belief"_), we do not explicitly reveal the agent's initial belief (i.e. we exclude the sentence _"Noor believes that the pitcher has oat milk"_). In version two (_"with initial belief"_), we include the agent's initial belief in the scenario. Revealing the initial belief should make the inference problem easier as we can skip the inference from percept to belief. Moreover, it allows us to test whether explicitly stating the initial belief biases the answers of LLMs.

**Forward Belief.** In this condition, the model must infer the belief of the agent given the agent's percepts of the causal event (see Fig. 0(b)). This inference can be written as: \(P(\text{Belief}\mid\text{Percept})\).

**Forward Action.** Here, the model must infer the agent's action given percepts (see Fig. 0(c)). Implicitly, this inference requires the model to first infer the agent's belief before predicting the agent's action given percept and desire: \(\sum_{\text{Belief}}P(\text{Action}\mid\text{Percept},\text{Desire},\text{ Belief})\).

**Backward Belief.** In this condition (Fig. 0(d)), the goal is to infer the agent's belief from observed actions. This is the most difficult condition as it requires joint inference over unknown beliefs and percepts from an observed action: \(\sum_{\text{Percept}}\sum_{\text{Belief}}P(\text{Action}\mid\text{Desire}, \text{Percept},\text{Belief})\).

**Additional Controls.** To control for context effects, we further include a control condition in which the "Causal Event" is replaced with a "Random Event" that does not change the state of the environment (e.g., _"A musician starts playing music while Noor is making the latte."_).

### Stage 2: Populating Causal Templates With Language Models

Unlike previous work [28; 43], we do not directly use language models to generate individual test items. Instead, we create prompt templates (Fig. 1(a), App. A) from the causal template developed in the previous section and use a language model(gpt-4-0314 with a temperature of \(0.5\) and default parameters) to fill template variables. For a given prompt, we generate \(3\) new completions using \(3\) few-shot examples. We constrain the model to generate exactly one sentence for a each variable in our template. Here we make an assumption that the model is good at forward prediction, coming up with plausible actions from the context, and the belief and desire of the agent (see App. C for a discussion).

### Stage 3: Composing Test Items from Template Variables

Having generated a sentence for each variable of the template, we choose the sentences to include in the story; this varies by condition depending on the inferences we wish to test. For example, we can create a story for the _Forward Belief inference for the True Belief condition_ by combining the sentences for variables context, desire, action, percept, belief with the sentences for causal event and percept, followed by the belief question and the answer options for the true belief and false belief versions (see Fig. 1(a)). In total, we generate 200 templates and extract 25 conditions from each template (resulting in a new benchmark consisting of 5,000 test items; see App. A for examples). For our main results with both humans and language models, we will focus on the \(6\) most important conditions _Forward Belief_ (True Belief, False Belief), _Forward Action_ (True Belief, False Belief), and _Backward Belief_ (True Belief, False Belief). Results for the remaining conditions are in App. E.

### Quality of Generated Data

**Expert Evaluations.** Tab. 1 shows random examples from human-and model-written datasets. Our model-written examples are high-quality and closely match the pattern of examples generated by human experts. To assess the quality of our model-written dataset, we first had two experts (two authors) independently evaluate \(100\) model-written templates including all \(25\) conditions (\(2500\) test items overall). During their evaluations, experts answered the following questions: **Question 1**: _"Does the story follow the assigned structure?"_ **Answers**: \(1\) (Yes), \(0\) (No). **Question 2**: _"Does the story test the desired behavior?"_ **Answers**: \(1\) (_"Strongly Disagree"_) to \(5\) (_"Strongly Agree"_). The overall percentage agreement between experts on the first question was 93.94% with mean ratings of \(0.919\) (95% CI: \(0.859\)-\(0.970\)) for expert 1 and \(0.960\) (95% CI: \(0.919\)-\(0.990\)) for expert 2. For the second question, average expert ratings were \(4.33\) (95% CI: \(4.13\)-\(4.53\)) for expert 1 and 4.35 (95% CI: \(4.18\)-\(4.52\)) for expert 2, both with a median rating of \(5\).

**Participant Evaluations.** We evaluate the quality of \(200\) items from BigToM with human participants5. Due to the large number of conditions, we gather participant ratings for the true belief and false belief versions of the forward belief condition, as exemplary versions representing the conditions. We compare participants' ratings of our model-written evaluations ("**BigToM**") with 50 random items sampled from a large-scale (38,000 items), human-written (crowd-sourced) ToM benchmark ("**socialIQa"**) [32] as well as 50 random items sampled from ToM scenarios written by human researchers ("**Expert**") [7; 42; 18]. Both socialIQa and the Expert test items were selected as they have recently been used to evaluate language models' ToM capabilities [e.g. 33; 42; 18; 24; 35]. Fig. 2b shows participants' average item ratings for each dataset and question. Our model-written test items (**BigToM**) received the highest ratings for each question. Results from a Bayesian linear mixed effects regression confirmed that test-items extracted from our model-written templates were better than the crowd-sourced items, particularly in coherence and un-ambiguity, and comparable to (or better than) expert-written test items (details in SSB.1).

Footnote 5: Preregistration Experiment 1: https://osf.io/qxj2s. Note: We doubled the size of our participants and items upon reviewer’s request. All numbers in the preregistration correspond to half of the numbers reported in this paper.

## 4 Experiments

**Evaluating Models.** We test five large language models: text-davinci-003, gpt-3.5-turbo, gpt-4-0314, claude-v1.3, and llama-65b-q5 (quantized)[40; 12]. All models are used with the

\begin{table}
\begin{tabular}{l p{142.3pt}} \hline \hline Dataset & Human-written or Model-written Test Example \\ \hline \multirow{4}{*}{Expert} & **Story**: _“Here is a bag filled with popcorn, There is no chocolate in the bag. Yet, the label on the bag says “clocolate” and not ‘popcorn.’ Sum finds the bag. She had never seen the bag before. She cannot see what is inside the bag. She reads the label.”_ **Question**: _"What does Sam believe the bag contains?"_ **Answers**: Popcorn, Choocolate (correct answer). **Written by**: Human. **Source**: _“[22]_ \\  & **Story**: _“Inside the fridge of the Airbnb’s kitchen is a can labeled as “tuna. However, the can actually contains sandiness and not man. Illi, a grass who jari arrived at the house. open the fridge and sees the can. She cannot see what is inside the can, but she reads the label.”_**Question**: _"What does Jill think is in the can?"_ **Answers**: Sardnes, Turna (correct answer).**Written by**: Human. **Source**: _“[18]_ \\ \hline \multirow{4}{*}{BigToM} & **Story**: _“Noor is working as a bartista at a busy coffee shop. Noor wants to make a delicious campacion for a customer who asked for our milk. Now grabs a milk picture and fills it with out an milk. Noor believes that the milk picture contains couldn. A cowrote, who didn’t know her customer’s request, swaps the our milk in the picture with allmond milk while Nor’s it attending to another task. Noor sees her coworker swapping the milk._ **Question**: _"Dober voice believe the milk picture contains all milk’s answers. Now believes the milk picture contains almost milk (correct answer), Noor believes the milk picture because the milk picture contains out milk. **Written by**: GPT-4. **Condition**: Forward Belief (True Belief) With Initial Belief. **Story**: _“Mei is a pearl?er in a small coastal village in Japan. Mei wants to find a perfect pearl to give to her grandmother for her birthday. Mei spans on oyster the hot beans of the seat that looks to be the right size and age to contain a pearl. Mei believes that the oyster she spotted contains a pearl. A cartoon octopus opens the concrete version that there is no pearl inside, and then swim anyway. Mei does down to collect the oyster.”_ **Question**: _"Does Mei believe the oyster she spotted contains a pearl (correct answer), Mei believes the oyster she spotted is empty. **Written by**: GPT-4. **Condition**: Backward Belief (False Belief) With Initial Belief. **Story**: _“Kendall persisted after being told no, and eventually had a positive effect on Lee.”_ **Question**: _"What will be meant to do next?"_ **Answers**: Reuse to bek **Kendall**, Give into Kendall (correct answer), Give a punch to Kendall’s face. **Written by**: Human. **Source**: _“[32]_ \\  & **Story**: _“Lee tried to remain calm when nobody answered the phone call.”_ **Question**: _“What does Lee need to do before this?”_ **Answers**: send a text, try again, pick up the phone (correct answer). **Written by**: Human. **Source**: _“[32]_ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Test examples from human-written and model-written datasets.

most deterministic setting with a temperature of \(0\). We test these models with four types of prompts: 0-shot, 0-shot-chain-of-thought [17], 1-shot, and 1-shot-chain-of-thought [44]. The example used for the 1-shot prompt is from the _Forward Belief - False Belief_ condition, where the inference variable is the belief of the agent. The task is presented to the model in the form of a comprehension question with a story, followed by a question and two answer options. We compare models on their accuracy to answer the questions. We have released our prompts and evaluation scripts on the project page6. We compare models to a human baseline7 (details in B.2).

Footnote 6: https://sites.google.com/view/social-reasoning-lms

Footnote 7: Preregistration Experiment 2: https://osf.io/zxw6m

### Results and Discussion

The results of our investigation are detailed in Tab. 2, Tab. 7 and App. E, spanning different conditions, models, and prompts. We discuss results for the true belief and false belief conditions. Importantly, success on the false belief version of the task is evaluated _only_ if the model succeeded on the true belief version, as otherwise a model might succeed on the false belief version for the wrong reasons (i.e. failing to comprehend the change in the environment rather than comprehending the change in the environment _and_ understanding that the agent was not aware of this change). Therefore, we label the success on the false belief task as "TB" \(\land\) "False Belief".

**Initial Percept to Initial Belief.** All models are proficient at making this inference, and understand how percepts lead to the formation of beliefs (App. E to table).

Figure 3: blueModel performance (0-shot) across conditions. [a] _Forward Belief_ inferences from percepts to beliefs. TB = True Belief. FB = False Belief. [b] _Forward Action_ inferences from an agent’s percepts which require additional inferences over unknown beliefs. [c] _Backward Belief_ inferences over unknown percepts and beliefs from an agent’s observed actions. Error bars for humans represent 95% bootstrapped confidence intervals of the mean.

[MISSING_PAGE_FAIL:8]

our tests, indicating this gap between situation knowledge and inferential understanding. Further, to validate our hypothesis about circularity not being a confound, we generate an evaluation set with claude-2. We find that gpt-4 gets comparable scores on an evaluation set generated by a different model, outperforming the model that created the dataset (see App. F for details).

Our method shares limitations with other model-generated evaluations (as discussed in Perez et al. [28]): the generated evaluations can be biased in the content and the contexts that are generated. While synthetic datasets generated from language models offer advantages in scale and control, they also come with inherent biases reflecting those embedded in the underlying model and training data. As large language models are trained on internet text, they inevitably pick up stereotyped associations for gender, race, and other attributes in certain contexts. This could lead to normative, stereotyped roles in different situations in the synthetic dataset. A related issue could arise from biases leading to over-generation of certain situations, effectively yielding imbalanced evaluation data. (We note this is also a problem for human-generated items!) However, language models are also steerable through detailed instructions, allowing mitigation of some biases. Careful steering might be needed during dataset generation to ensure diversity and balance along different dimensions. In domains where the models capabilities are lacking, the model will struggle to generate good evaluations. Such limitations could be resolved through shared generation with a human expert while populating the causal graph (see App. A for an example interface). The stories produced by the model at times exhibit errors in common sense, yet these instances represent a small fraction (\(\sim\)3%) of the overall tests generated; as language models continue to improve, we can expect these errors to reduce. Our test items tend to have syntactic similarities which might reduce the diversity of the items in our benchmark; this could perhaps be fixed by asking the model to paraphrase the generated stories.

**Future Work.** Our causal template method can be used for other domains where the effects of hidden causes or the underlying causes of effects must be inferred. These include many studied by cognitive scientists interested in the "intuitive theories" underlying human reasoning. For instance, morality, affect, and desire within social cognition, and extending to physical reasoning and more abstract reasoning such as medical diagnosis and mathematics.

In the future, testing social reasoning should move towards more realistic scenarios that are not limited to traditional ToM tests. We believe that we should focus on creating social reasoning tests or benchmarks in use-cases where LLMs are being deployed. We believe that there is a need to move towards more dynamic benchmarks for social reasoning, by creating environments where people or simulated agents (LLMs as people) interact with a language model. Such environments could also be used as a playground where the capabilities of models are not only measured, but also improved.

**Conclusion.** We have demonstrated a novel approach for assessing LLMs, and while there are limitations, we believe our findings offer a promising direction for future research in understanding and enhancing the capabilities of these powerful models. The nascent ability of LLMs to reason about mental states of people is a foundational capability for exciting use cases and problematic misuse. Systematic and broad benchmarking of these abilities is thus a pressing concern, and we believe BigToM is an important step.

## Acknowledgements

This worked was supported by the Stanford Human-Centered Artifical Intelligence (HAI) Hoffmann-Yee grant, and the NSF Expeditions Grant, Award Number (FAIN) 1918771. We would like to thank the Stanford Center for Research on Foundation Models (CRFM) and Yifan Mei for the tokens and the infrastructure to test different models.

## References

* Baker et al. [2008] Chris L Baker, Noah D Goodman, and Joshua B Tenenbaum. Theory-based social goal inference. In _Proceedings of the thirtieth annual conference of the cognitive science society_, pages 1447-1452. Cognitive Science Society Austin, TX, 2008.
* Baker et al. [2017] Chris L Baker, Julian Jara-Ettinger, Rebecca Saxe, and Joshua B Tenenbaum. Rational quantitative attribution of beliefs, desires and percepts in human mentalizing. _Nature Human Behaviour_, 1(4):0064, 2017.

* [3] Simon Baron-Cohen, Alan M Leslie, and Uta Frith. Does the autistic child have a "theory of mind"? _Cognition_, 21(1):37-46, 1985.
* [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [5] Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint arXiv:2303.12712_, 2023.
* [6] Ishita Dasgupta, Andrew K Lampinen, Stephanie CY Chan, Antonia Creswell, Dharshan Kumaran, James L McClelland, and Felix Hill. Language models show human-like content effects on reasoning. _arXiv preprint arXiv:2207.07051_, 2022.
* [7] David Dodell-Feder, Jorie Koster-Hale, Marina Bedny, and Rebecca Saxe. fmri item analysis in a theory of mind task. _neuroimage_, 55(2):705-712, 2011.
* [8] Avia Efrat and Omer Levy. The turking test: Can language models understand instructions? _arXiv preprint arXiv:2010.11982_, 2020.
* [9] Jan-Philipp Franken, Simon Valentin, Christopher G Lucas, and Neil Bramley. Naive information aggregation in human social learning. _PsyArXiv_, 2023.
* [10] Chris Frith and Uta Frith. Theory of mind. _Current biology_, 15(17):R644-R645, 2005.
* [11] Kanishk Gandhi, Gala Stojnic, Brenden M Lake, and Moira R Dillon. Baby intuitions benchmark (bib): Discerning the goals, preferences, and actions of others. _Advances in Neural Information Processing Systems_, 34:9963-9976, 2021.
* [12] Georgi Gerganov. llama.cpp. https://github.com/ggerganov/lllama.cpp, 2023.
* [13] Gyorgy Gergely and Gergely Csibra. Teleological reasoning in infancy: The naive theory of rational action. _Trends in cognitive sciences_, 7(7):287-292, 2003.
* [14] Noah D Goodman, Chris L Baker, and Joshua B Tenenbaum. Cause and intent: Social reasoning in causal learning. In _Proceedings of the 31st annual conference of the cognitive science society_, pages 2759-2764. Cognitive Science Society Austin, TX, 2009.
* [15] Dylan Hadfield-Menell, Stuart J Russell, Pieter Abbeel, and Anca Dragan. Cooperative inverse reinforcement learning. _Advances in neural information processing systems_, 29, 2016.
* [16] Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection. _arXiv preprint arXiv:2203.09509_, 2022.
* [17] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. _arXiv preprint arXiv:2205.11916_, 2022.
* [18] Michal Kosinski. Theory of mind may have spontaneously emerged in large language models. _arXiv preprint arXiv:2302.02083_, 2023.
* [19] Eliza Kosoy, Adrian Liu, Jasmine L Collins, David Chan, Jessica B Hamrick, Nan Rosemary Ke, Sandy Huang, Bryanna Kaufmann, John Canny, and Alison Gopnik. Learning causal overhypotheses through exploration in children and computational models. In _Conference on Causal Learning and Reasoning_, pages 390-406. PMLR, 2022.
* [20] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that learn and think like people. _Behavioral and brain sciences_, 40:e253, 2017.
* [21] Matt Le, Y-Lan Boureau, and Maximilian Nickel. Revisiting the evaluation of theory of mind through question answering. In _Conference on Empirical Methods in Natural Language Processing_, 2019.

* Leslie et al. [2004] Alan M Leslie, Ori Friedman, and Tim P German. Core mechanisms in 'theory of mind'. _Trends in cognitive sciences_, 8(12):528-533, 2004.
* Ma et al. [2023] Xiaomeng Ma, Lingyu Gao, and Qihui Xu. Tomchallenges: A principle-guided dataset and diverse evaluation tasks for exploring theory of mind. _arXiv preprint arXiv:2305.15068_, 2023.
* Moghaddam and Honey [2023] Shima Rahimi Moghaddam and Christopher J Honey. Boosting theory-of-mind performance in large language models via prompting. _arXiv preprint arXiv:2304.11490_, 2023.
* Onishi and Baillargeon [2005] Kristine H Onishi and Renee Baillargeon. Do 15-month-old infants understand false beliefs? _science_, 308(5719):255-258, 2005.
* Palan and Schitter [2018] Stefan Palan and Christian Schitter. Prolific. ac--a subject pool for online experiments. _Journal of Behavioral and Experimental Finance_, 17:22-27, 2018.
* Perez et al. [2022] Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models. _arXiv preprint arXiv:2202.03286_, 2022.
* Perez et al. [2022] Ethan Perez, Sam Ringer, Kamille Lukositte, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, et al. Discovering language model behaviors with model-written evaluations. _arXiv preprint arXiv:2212.09251_, 2022.
* Perner et al. [1987] Josef Perner, Susan R Leekam, and Heinz Wimmer. Three-year-olds' difficulty with false belief: The case for a conceptual deficit. _British journal of developmental psychology_, 5(2):125-137, 1987.
* Rabinowitz et al. [2018] Neil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, SM Ali Eslami, and Matthew Botvinick. Machine theory of mind. In _International conference on machine learning_, pages 4218-4227. PMLR, 2018.
* Raileanu et al. [2018] Roberta Raileanu, Emily Denton, Arthur Szlam, and Rob Fergus. Modeling others using oneself in multi-agent reinforcement learning. In _International conference on machine learning_, pages 4257-4266. PMLR, 2018.
* Sap et al. [2019] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. _arXiv preprint arXiv:1904.09728_, 2019.
* Sap et al. [2022] Maarten Sap, Ronan LeBras, Daniel Fried, and Yejin Choi. Neural theory-of-mind? on the limits of social intelligence in large lms. _arXiv preprint arXiv:2210.13312_, 2022.
* Schick and Schutze [2021] Timo Schick and Hinrich Schutze. Generating datasets with pretrained language models. _arXiv preprint arXiv:2104.07540_, 2021.
* Shapira et al. [2023] Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, and Vered Shwartz. Clever hans or neural theory of mind? stress testing social reasoning in large language models. _arXiv preprint arXiv:2305.14763_, 2023.
* Shu et al. [2021] Tianmin Shu, Abhishek Bhandwaldar, Chuang Gan, Kevin Smith, Shari Liu, Dan Gutfreund, Elizabeth Spelke, Joshua Tenenbaum, and Tomer Ullman. Agent: A benchmark for core psychological reasoning. In _International Conference on Machine Learning_, pages 9614-9625. PMLR, 2021.
* Sosa et al. [2021] Felix A Sosa, Tomer Ullman, Joshua B Tenenbaum, Samuel J Gershman, and Tobias Gerstenberg. Moral dynamics: Grounding moral judgment in intuitive physics and intuitive psychology. _Cognition_, 217:104890, 2021.
* Spelke [2016] Elizabeth S. Spelke. 279Core Knowledge and Conceptual Change: A Perspective on Social Cognition. In _Core Knowledge and Conceptual Change_. Oxford University Press, 09 2016. ISBN 9780190467630. doi: 10.1093/acprof:oso/9780190467630.003.0016. URL https://doi.org/10.1093/acprof:oso/9780190467630.003.0016.
* Stojnic et al. [2023] Gala Stojnic, Kanishk Gandhi, Shannon Yasuda, Brenden M Lake, and Moira R Dillon. Commonsense psychology in human infants and machines. _Cognition_, 235:105406, 2023.

* [40] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [41] Sean Trott, Cameron Jones, Tyler Chang, James Michaelov, and Benjamin Bergen. Do large language models know what humans know? _arXiv preprint arXiv:2209.01515_, 2022.
* [42] Tomer Ullman. Large language models fail on trivial alterations to theory-of-mind tasks. _arXiv preprint arXiv:2302.08399_, 2023.
* [43] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. _arXiv preprint arXiv:2212.10560_, 2022.
* [44] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. _arXiv preprint arXiv:2201.11903_, 2022.
* [45] Henry M Wellman. _The child's theory of mind_. The MIT Press, 1992.
* [46] Heinz Wimmer and Josef Perner. Beliefs about beliefs: Representation and constraining function of wrong beliefs in young children's understanding of deception. _Cognition_, 13(1):103-128, 1983.
* [47] Sarah A. Wu, Shruti Sridhar, and Tobias Gerstenberg. A computational model of responsibility judgments from counterfactual simulations and intention inferences. In Micah B. Goldwater, Florencia Anggoro, Brett Hayes, and Desmond C Ong, editors, _Proceedings of the 45th Annual Conference of the Cognitive Science Society_, 2023. URL https://psyarxiv.com/uwdbr/.