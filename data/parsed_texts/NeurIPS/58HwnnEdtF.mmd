# Reward-Directed Conditional Diffusion: Provable Distribution Estimation and Reward Improvement

 Hui Yuan

&Kaixuan Huang

Chengzhuo Ni

&Minshuo Chen

&Mengdi Wang

Department of Electrical and Computer Engineering, Princeton University. Authors' emails are: {huiyuan, kaixuanh, cn10, mc0750, mengdiw}@princeton.edu

###### Abstract

We explore the methodology and theory of reward-directed generation via conditional diffusion models. Directed generation aims to generate samples with desired properties as measured by a reward function, which has broad applications in generative AI, reinforcement learning, and computational biology. We consider the common learning scenario where the dataset consists of majorly unlabeled data and a small set of data with noisy reward labels. Our approach leverages a learned reward function on the smaller data set as a pseudolabeler to label the unlabelled data. After pseudo-labelling, a conditional diffusion model (CDM) is trained on the data and samples are generated by setting a target value \(a\) as the condition in CDM. From a theoretical standpoint, we show that this directed generator can effectively learn and sample from the reward-conditioned data distribution: 1. our model is capable of recovering the data's latent subspace representation. 2. the model generates samples moving closer to the user-specified target. The improvement in rewards of samples is influenced by a interplay between the strength of the reward signal, the distribution shift, and the cost of off-support extrapolation. We provide empirical results to validate our theory and highlight the relationship between the strength of extrapolation and the quality of generated samples. Our code is available at https://github.com/Kaffaljjidhmah2/RCGDM.

## 1 Introduction

Controlling the behavior of generative models towards desired properties is a major problem for deploying deep learning models for real-world usage. As large and powerful pre-trained generative models achieve steady improvements over the years, one increasingly important question is how to adopt generative models to fit the needs of a specific domain and to ensure the generation results satisfying certain constraints (e.g., safety, fairness, physical constraints) without sabotaging the power of the original pre-trained model [35, 27, 54, 41].

In this paper, we focus on directing the generation of diffusion models [19, 43], a family of score-matching generative models that have demonstrated the state-of-the-art performances in various domains, such as image generation [39, 38, 4] and audio generation, with fascinating potentials in broader domains, including text modeling [3, 27], reinforcement learning [21, 1, 36, 28] and protein structure modeling [26]. Diffusion models are trained to predict a clean version of the noised input, and generate data by sequentially removing noises and trying to find a cleaner version of the input. The denoising network (a.k.a. score network) \(s(x,t)\) approximates the score function \(\nabla\log p_{t}(x)\)[45, 46], and controls the behavior of diffusion models. People can incorporate any control information \(y\) as an additional input to \(s(x,y,t)\) during the training and inference [38, 54].

Here we abstract various control goals as a scalar reward \(y\), measuring how well the generated instance satisfies our desired properties. In this way, the directed generation problem becomes finding plausible instances with higher rewards and can be tackled via reward-conditioned diffusion models. The subtlety of this problem lies in that the two goals potentially conflict with each other: diffusion models are learned to generate instances _similar to_ the training distribution, while maximizing the rewards of the generation drives the model to _deviate from_ the training distribution. In other words, the model needs to "interpolate" and "extrapolate" at the same time. A higher value of \(y\) provides a stronger signal that guides the diffusion model towards higher rewards, while the increasing distribution shift may hurt the generated samples' quality. In the sequel, we provide theoretical guarantees for the reward-conditioned diffusion models, aiming to answer the following question:

_How to provably estimate the reward-conditioned distribution via diffusion? How to balance the reward signal and distribution-shift effect, and ensure reward improvement in generated samples?_

**Our Approach.** To answer both questions, we consider a semi-supervised learning setting, where we are given a small dataset \(\mathcal{D}_{\mathrm{label}}\) with annotated rewards and a massive unlabeled dataset \(\mathcal{D}_{\mathrm{unlabel}}\). We estimate the reward function using \(\mathcal{D}_{\mathrm{label}}\) and then use the estimator for pseudo-labeling on \(\mathcal{D}_{\mathrm{unlabel}}\). Then we train a reward-conditioned diffusion model using the pseudo-labeled data. Our approach is illustrated in Figure 1. In real-world applications, there are other ways to incorporate the knowledge from the massive dataset \(\mathcal{D}_{\mathrm{unlabel}}\), e.g., finetuning from a pre-trained model [35; 54]. We focus on the pseudo-labeling approach, as it provides a cleaner formulation and exposes the error dependency on data size and distribution shift. The intuition behind and the message are applicable to other semi-supervised approaches; see experiments in Section 5.2.

From a theoretical standpoint, we consider data point \(x\) having a latent linear representation. Specifically, we assume \(x=Az\) for some matrix \(A\) with orthonormal columns and \(z\) being a latent variable. The latent variable often has a smaller dimension, reflecting the fact that practical data sets often exhibit intrinsic low-dimensional structures [13; 48; 37]. The representation matrix \(A\) should be learned to promote sample efficiency and generation quality [8]. Our theoretical analysis reveals an intricate interplay between reward guidance, distribution shift, and implicit representation learning; see Figure 2 for illustration.

**Contributions.** Our results are summarized as follows.

**(1)** We show that the reward-conditioned diffusion model implicitly learns the latent subspace representation of \(x\). Consequently, the model provably generates high-fidelity data that stay close to the subspace (Theorem 4.5).

**(2)** Given a target reward value, we analyze the statistical error of reward-directed generation, measured by the difference between the target value and the average reward of the generated population. In the case of a linear reward model, we show that this error includes the suboptimality gap of linear off-policy bandits with full knowledge of the subspace feature, if taking the target to be the maximum possible. In addition, the other two components of this error correspond to the distribution shift in score matching and the cost of off-support extrapolation (Theorem 4.6).

**(3)** We further extend our theory to nonparametric reward and distribution configurations where reward prediction and score matching are approximated by general function class, which covers the wildly adopted ReLU Neural Networks in real-world implementation (Section 4.3 and Appendix F).

**(4)** We provide numerical experiments under both synthesized setting and more realistic settings such as text-to-image generation (stable diffusion) and reinforcement learning (decision-diffuser) to support our theory (Section 5 and Appendix I).

To our best knowledge, our results present the first statistical theory for conditioned diffusion models and provably reward improvement guarantees for reward-directed generation.

## 2 Related Work

Guided Diffusions.For image generations, guiding the backward diffusion process towards higher log probabilities predicted by a classifier (which can be viewed as the reward signal) leads to improved sample quality, where the classifier can either be separated trained, i.e., classifier-guided [11] or implicitly specified by the conditioned diffusion models, i.e., classifier-free [18]. Classifier-free guidance has become a standard technique in the state-of-the-art text-to-image diffusion models [39; 38; 4]. Other types of guidance are also explored [33; 14]. Similar ideas have been explored in sequence modelling problems. In offline reinforcement learning, Decision Diffuser [1] is a diffusion model trained on offline trajectories and can be conditioned to generate new trajectories with high returns, satisfying certain safety constraints, or composing skills. For discrete generations, Diffusion LM [27] manages to train diffusion models on discrete text space with an additional embedding layer and a rounding step. The authors further show that gradients of any classifier can be incorporated to control and guide the text generation.

Theory of Diffusion ModelsA line of work studies diffusion models from a sampling perspective. When assuming access to a score function that can accurately approximate the ground truth score function in \(L^{\infty}\) or \(L^{2}\) norm, [9; 25] provide polynomial convergence guarantees of score-based diffusion models. "Convergence of denoising diffusion models under the manifold hypothesis" by Valentin De Bortoli further studies diffusion models under the manifold hypothesis. Recently, [8] and [34] provide an end-to-end analysis of diffusion models. In particular, they develop score estimation and distribution estimation guarantees using the estimated score function. These results largely motivate our theory, whereas, we are the first to consider conditional score matching and statistical analysis of conditional diffusion models.

Connection to Offline Bandit/RLOur off-policy regret analysis of generated samples is related to offline bandit/RL theory [30; 29; 6; 12; 22; 32; 5]. In particular, our theory extensively deals with distribution shift in the offline data set by class restricted divergence measures, which are commonly adopted in offline RL. Moreover, our regret bound of generated samples consists of an error term that coincides with off-policy linear bandits. However, our analysis goes far beyond the scope of bandit/RL.

## 3 Reward-Directed Generation via Conditional Diffusion Models

In this section, we develop a conditioned diffusion model-based method to generate high-fidelity samples with desired properties. In real-world applications such as image/text generation and protein design, one often has access to abundant unlabeled data, but relatively limited number of labeled data. This motivates us to consider a semi-supervised learning setting.

Figure 1: **Overview of reward-directed generation via conditional diffusion model. We estimate the reward function from the labeled dataset. Then we compute the estimated reward for each instance of the unlabeled dataset. Finally, we train a reward-conditioned diffusion model using the pseudo-labeled data. Using the reward-conditioned diffusion model, we are able to generate high-reward samples.**

Figure 2: **Illustrations of distribution shifts in samples, reward, and encoder-decoder score network. When performing reward-directed conditional diffusion, (a) the distribution of the generated data shifts, but still stays close to the feasible data support; (b) the distribution of rewards for the next generation shifts and the mean reward improves. (c) (Adapted from [8]) the score network for reward-directed conditioned diffusion adopts an Encoder-Decoder structure.**

**Notation**:\(P_{xy}\) denotes ground truth joint distribution of \(x\) and its label \(y\), \(P_{x}\) is the marginal of \(x\). Any piece of data in \(\mathcal{D}_{\mathrm{label}}\) follows \(P_{xy}\) and any data in \(\mathcal{D}_{\mathrm{unlabel}}\) follows \(P_{x}\). \(P\) is used to denote a distribution and \(p\) denotes its corresponding density. \(P(x\mid y=a)\) and \(P(x,y=a)\) are the conditionals of \(P_{xy}\) Similarly, we also use notation \(P_{x\widehat{y}}\), \(P(x\mid\widehat{y}=a)\) for the joint and conditional of \((x,\widehat{y})\), where \(\widehat{y}\) is predicted by the learnt reward model. Also, denote a generated distribution using diffusion by \(\widehat{P}\) (density \(\widehat{p}\)) followed by the same argument in parentheses as the true distribution it approximates, e.g. \(\widehat{P}(x\mid y=a)\) is generated as an approximation of \(P(x\mid y=a)\).

### Problem Setup

Suppose we are given an unlabeled data set \(\mathcal{D}_{\mathrm{unlabel}}=\{x_{j}\}_{j=1}^{n_{1}}\) and a labeled data set \(\mathcal{D}_{\mathrm{label}}=\{(x_{i},y_{i})\}_{i=1}^{n_{2}}\), where it is often the case that \(n_{1}\gg n_{2}\). Assume without loss of generality that \(\mathcal{D}_{\mathrm{label}}\) and \(\mathcal{D}_{\mathrm{unlabel}}\) are independent. In both datasets, suppose \(x\) is sampled from an unknown population distribution \(P_{x}\). In our subsequent analysis, we focus on the case where \(P_{x}\) is supported on a latent subspace, meaning that the raw data \(x\) admits a low-dimensional representation (see Assumption 4.1). We model \(y\) as a noisy measurement of a reward function determined by \(x\), given by

\[y=f^{*}(x)+\epsilon\quad\text{for}\quad\epsilon\sim\mathsf{N}(0,\sigma^{2}) \quad\text{with}\quad 1>\sigma>0.\]

A user can specify a target reward value, i.e., \(y=a\). Then the objective of directed generation is to sample from the conditional distribution \(P(x|y=a)\). Given \(f^{*},P_{x}\) or the low-dimensional support of \(P_{x}\) are unknown, we need to learn these unknowns explicitly and implicitly through reward-conditioned diffusion.

### Meta Algorithm

```
1:Input: Datasets \(\mathcal{D}_{\mathrm{unlabel}}\), \(\mathcal{D}_{\mathrm{label}}\), target reward value \(a\), early-stopping time \(t_{0}\), noise level \(\nu\). (Note: in the following psuedo-code, \(\phi_{t}(x)\) is the Gaussian density and \(\eta\) is the step size of discrete backward SDE, see SS3.3 for elaborations on conditional diffusion)
2:Reward Learning: Estimate the reward function by \[\widehat{f}\in\operatorname*{argmin}_{f\in\mathcal{F}}\sum_{(x_{i},y_{i})\in \mathcal{D}_{\mathrm{label}}}\ell(f(x_{i}),y_{i}),\] (3.1) where \(\ell\) is a loss and \(\mathcal{F}\) is a function class.
3:Pseudo labeling: Use the learned function \(\widehat{f}\) to evaluate unlabeled data \(\mathcal{D}_{\mathrm{unlabel}}\) and augment it with pseudo labels: \(\widetilde{\mathcal{D}}=\{(x_{j},\widehat{y}_{j})=\widehat{f}(x_{j})+\xi_{j} \}_{j=1}^{n_{1}}\) for \(\xi_{j}\stackrel{{\mathrm{i.i.d.}}}{{\sim}}\mathsf{N}(0,\nu^{2})\).
4:Conditional score matching: Minimize over \(s\in\mathcal{S}\) (\(\mathcal{S}\) constructed as 3.8) on data set \(\widetilde{\mathcal{D}}\) via \[\widehat{s}\in\operatorname*{argmin}_{s\in\mathcal{S}}\int_{t_{0}}^{T}\widehat {\mathbb{E}}_{(x,\widehat{y})\in\widetilde{\mathcal{D}}}\mathbb{E}_{x^{ \prime}\sim\mathsf{N}(\alpha(t)x,h(t)D_{\widehat{f}})}\left[\|\nabla_{x^{ \prime}}\log\phi_{t}(x^{\prime}|x)-s(x^{\prime},\widehat{y},t)\|_{2}^{2} \right]\mathrm{d}t.\] (3.2)
5:Conditioned generation: Use the estimated score \(\widehat{s}(\cdot,a,\cdot)\) to sample from the backward SDE: \[\mathrm{d}\widetilde{X}_{t}^{t,\Leftarrow}=\left[\frac{1}{2}\widetilde{X}_{k \eta}^{y,\Leftarrow}+\widehat{s}(\widetilde{X}_{k\eta}^{y,\Leftarrow},a,T-k \eta)\right]\mathrm{d}t+\mathrm{d}\overline{W}_{t}\quad\text{for }t\in[k\eta,(k+1)\eta],k\in[\frac{T}{ \eta}\rfloor.\] (3.3)
6:Return: Generated population \(\widehat{P}(\cdot|\widehat{y}=a)\), learned subspace representation \(V\) contained in \(\widehat{s}\). ```

**Algorithm 1** Reward-Conditioned Generation via Diffusion Model (RCGDM)

In order to generate novel samples with both high fidelity and high rewards, we propose Reward-Conditioned Generation via Diffusion Models (RCGDM); see Algorithm 1 for details. By using the labeled data \(\mathcal{D}_{\mathrm{label}}\), we approximately estimate the reward function \(f^{*}\) by regression, then we obtain an estimated reward function \(\widehat{f}\). We then use \(\widehat{f}\) to augment the unlabeled data \(\mathcal{D}_{\mathrm{unlabel}}\) with "pseudo labeling" and additive noise, i.e., \(\widetilde{\mathcal{D}}=\{(x_{j},\widehat{y}_{j}=\widehat{f}(x_{j})+\xi_{j}) \}_{j=1}^{n_{1}}\) with \(\xi_{j}\sim\mathsf{N}(0,\nu^{2})\) of a small variance \(\nu^{2}\). Here, we added noise \(\xi_{j}\) merely for technical reasons in the proof. We denote thejoint distribution of \((x,\widehat{y})\) as \(P_{x\widehat{y}}\). Next, we train a conditional diffusion model using the augmented dataset \(\widetilde{D}\). If we specify a target value of the reward, for example letting \(y=a\), we can generate conditioned samples from the distribution \(\widehat{P}(x|\widehat{y}=a)\) by backward diffusion.

**Alternative approaches.** In Line 4, Algorithm 1 trains the conditional diffusion model via conditional score matching. This approach is suitable when we have access to the unlabeled dataset and need to train a brand-new diffusion model from scratch. Empirically, in order to realize conditional generation, we can utilize a pre-trained diffusion model and incorporate with reward signals to be conditioned on. Existing methods falling in to this category include classifier-based guidance [11], fine-tuning [54], and self-distillation [47]. For theoretical cleanness, we focus on analysing Algorithm 1 as it shares the same core essence with other alternative methods, which is approximating of the conditional score \(\nabla\log p_{t}(x_{t}|y)\).

### Training of Conditional Diffusion Model

In this section, we provide details about the training and sampling of conditioned diffusion in Algorithm 1 (Line 4: conditional score matching and Line 5: conditional generation). In Algorithm 1, conditional diffusion model is learned with \(\widetilde{\mathcal{D}}=\left\{(x_{j},\widehat{y}_{j}=\widehat{f}(x_{j})+ \xi_{j})\right\}_{j=1}^{n_{1}}\), where \((x,\widehat{y})\sim P_{x\widehat{y}}\). For simplicity, till the end of this section we use \(y\) instead of \(\widehat{y}\) to denote the condition variable. The diffusion model is to approximate the conditional probability \(P(x\mid\widehat{y})\).

**Conditional Score Matching.** The working flow of conditional diffusion models is nearly identical to that of unconditioned diffusion models reviewed in Appendix A. A major difference is we learn a conditional score \(\nabla\log p_{t}(x|y)\) instead of the unconditional one. Here \(p_{t}\) denotes the marginal density function at time \(t\) of the following forward O-U process,

\[\mathrm{d}X_{t}^{y}=-\frac{1}{2}g(t)X_{t}^{y}\mathrm{d}t+\sqrt{g(t)}\mathrm{d} W_{t}\quad\text{with}\quad X_{0}^{y}\sim P_{0}(x|y)\text{ and }t\in(0,T],\] (3.4)

where similarly \(T\) is a terminal time, \((W_{t})_{t\geq 0}\) is a Wiener process, and the initial distribution \(P_{0}(x|y)\) is induced by the \((x,\widehat{y})\)-pair distribution \(\widetilde{P}_{x\widehat{y}}\). Note here the noise is only added on \(x\) but not on \(y\). Throughout the paper, we consider \(g(t)=1\) for simplicity. We denote by \(P_{t}(x_{t}|y)\) the distribution of \(X_{t}^{y}\) and let \(p_{t}(x_{t}|y)\) be its density and \(P_{t}(x_{t},y)\) be the corresponding joint, shorthanded as \(P_{t}\). A key step is to estimate the unknown \(\nabla\log p_{t}(x_{t}|y)\) through denoising score matching [46]. A conceptual way is to minimize the following quadratic loss with \(\mathcal{S}\), a concept class.

\[\operatorname*{argmin}_{s\in\mathcal{S}}\int_{0}^{T}\mathbb{E}_{(x_{t},y) \sim P_{t}}\left[\|\nabla\log p_{t}(x_{t}|y)-s(x_{t},y,t)\|_{2}^{2}\right] \mathrm{d}t,\] (3.5)

Unfortunately, the loss in (3.5) is intractable since \(\nabla\log p_{t}(x_{t}|y)\) is unknown. Inspired by Hyvarinen and Dayan [20] and Vincent [52], we choose a new objective (3.2) and show their equivalence in the following Proposition. The proof is provided in Appendix C.1.

**Proposition 3.1** (**Score Matching Objective for Implementation**).: For any \(t>0\) and score estimator \(s\), there exists a constant \(C_{t}\) independent of \(s\) such that

\[\mathbb{E}_{(x_{t},y)\sim P_{t}}\left[\|\nabla\log p_{t}(x_{t}|y )-s(x_{t},y,t)\|_{2}^{2}\right]\] \[\quad=\mathbb{E}_{(x,y)\sim P_{x\widehat{y}}}\mathbb{E}_{x^{ \prime}\sim\mathsf{N}(\alpha(t)x,h(t)I_{D})}\left[\|\nabla_{x^{\prime}}\log \phi_{t}(x^{\prime}|x)-s(x^{\prime},y,t)\|_{2}^{2}\right]+C_{t},\] (3.6)

where \(\nabla_{x^{\prime}}\log\phi_{t}(x^{\prime}|x)=-\frac{x^{\prime}-\alpha(t)x}{h( t)}\), where \(\phi_{t}(x^{\prime}|x)\) is the density of \(\mathsf{N}(\alpha(t)x,h(t)I_{D})\) with \(\alpha(t)=\exp(-t/2)\) and \(h(t)=1-\exp(-t)\).

Equation (3.6) allows an efficient implementation, since \(P_{x\widehat{y}}\) can be approximated by the empirical data distribution in \(\widetilde{\mathcal{D}}\) and \(x^{\prime}\) is easy to sample. Integrating (3.6) over time \(t\) leads to a practical conditional score matching object

\[\operatorname*{argmin}_{s\in\mathcal{S}}\int_{t_{0}}^{T}\widehat{\mathbb{E}}_{( x,y)\sim P_{x\widehat{y}}}\mathbb{E}_{x^{\prime}\sim\mathsf{N}(\alpha(t)x,h(t)I_{D})} \left[\|\nabla_{x^{\prime}}\log\phi_{t}(x^{\prime}|x)-s(x^{\prime},y,t)\|_{2}^{ 2}\right]\mathrm{d}t,\] (3.7)

where \(t_{0}>0\) is an early-stopping time to stabilize the training [44; 50] and \(\widehat{\mathbb{E}}\) denotes the empirical distribution.

[MISSING_PAGE_FAIL:6]

**Assumption 4.3**.: The latent variable \(z\) follows distribution \(P_{z}\) with density \(p_{z}\), such that there exists constants \(B,C_{1},C_{2}\) verifying \(p_{z}(z)\leq(2\pi)^{-(d+1)/2}C_{1}\exp\left(-C_{2}\|z\|_{2}^{2}/2\right)\) whenever \(\|z\|_{2}>B\). And ground truth score is realizable: \(\nabla\log p_{t}(x\mid\widehat{y})\in\mathcal{S}\).

**Assumption 4.4**.: Further assume \(z\sim\mathsf{N}(0,\Sigma)\) with its covariance matrix \(\Sigma\) satisfying \(\lambda_{\min}I_{d}\preceq\Sigma\preceq\lambda_{\max}I_{d}\) for \(0<\lambda_{\min}\leq\lambda_{\max}\leq 1\).

**Theorem 4.5** (**Subspace Fidelity of Generated Data**).: Under Assumption 4.1, if Assumption 4.3 holds with \(c_{0}I_{d}\preceq\mathbb{E}_{z\sim P_{z}}\left[zz^{\top}\right]\), then with high probability on data,

\[\angle(V,A)=\widetilde{\mathcal{O}}\left(\frac{1}{c_{0}}\sqrt{\frac{\mathcal{ N}(\mathcal{S},1/n_{1})D}{n_{1}}}\right)\] (4.2)

with \(\mathcal{N}(\mathcal{S},1/n_{1})\) being the log covering number of function class \(\mathcal{S}\) as in (3.8). When Assumption 4.4 holds, \(\mathcal{N}(\mathcal{S},1/n_{1})=\mathcal{O}((d^{2}+Dd)\log(Ddn_{1}))\) and thus \(\angle(V,A)=\widetilde{\mathcal{O}}(\frac{1}{\lambda_{\min}}\sqrt{\frac{(Dd^{ 2}+D^{2}d)}{n_{1}}})\). Further under Assumption 4.2, it holds that

\[\mathbb{E}_{x\sim\widehat{P}_{a}}[\|x_{\perp}\|_{2}]=\mathcal{O} \left(\sqrt{t_{0}D}+\sqrt{\angle(V,A)}\cdot\sqrt{\frac{a^{2}}{\|\beta^{*}\|_{ \Sigma}}+d}\right),\] (4.3)

where \(\beta^{*}\) is groundtruth parameter of linear model.

### Provable Reward Improvement via Conditional Generation

Let \(y^{*}\) be a target reward value and \(P\) be a generated distribution. Define the suboptimality of \(P\) as

\[\mathtt{SubOpt}(P;y^{*})=y^{*}-\mathbb{E}_{x\sim P}[f^{*}(x)],\]

which measures the gap between the expected reward of \(x\sim P\) and the target value \(y^{*}\). In the language of bandit learning, this gap can also be viewed as a form of _off-policy regret_. Given a target value \(y^{*}=a\), we want to derive guarantees for \(\mathtt{SubOpt}(\widehat{P}_{a};y^{*}=a)\), recall \(\widehat{P}_{a}:=\widehat{P}(\cdot|\widehat{y}=a)\) denotes the generated distribution. In Theorem 4.6, we show \(\mathtt{SubOpt}(\widehat{P}_{a};y^{*}=a)\) comprises of three components: off-policy bandit regret which comes from the estimation error of \(\widehat{f}\), on-support and off-support errors coming from approximating conditional distributions with diffusion.

**Theorem 4.6** (**Off-policy Regret of Generated Samples**).: Suppose Assumption 4.1, 4.2 and 4.4 hold. We choose \(\lambda=1\), \(t_{0}=\left((Dd^{2}+D^{2}d)/n_{1}\right)^{1/6}\) and \(\nu=1/\sqrt{D}\). With high probability, running Algorithm 1 with a target reward value \(a\) gives rise to

\[\mathtt{SubOpt}(\widehat{P}_{a};y^{*}=a)\] \[\quad\leq\underbrace{\sqrt{\operatorname{Tr}(\widehat{\Sigma}_{ \lambda}^{-1}\Sigma_{P_{a}})}}_{\mathcal{E}_{1}\text{off-policy bandit regret}}+ \underbrace{\left|\mathbb{E}_{P_{a}}[g^{*}(x_{\parallel})]-\mathbb{E}_{ \widehat{P}_{a}}[g^{*}(x_{\parallel})]\right|}_{\mathcal{E}_{2}\text{on- support diffusion error}}+\underbrace{\mathbb{E}_{\bar{P}_{a}}[h^{*}(x_{\perp})]}_{\mathcal{E}_{3}\text{off-support diffusion error}},\] (4.4)

where \(\widehat{\Sigma}_{\lambda}:=\frac{1}{n_{2}}(X^{\top}X+\lambda I)\) where \(X\) is the stack matrix of \(\mathcal{D}_{\mathrm{label}}\) and \(\Sigma_{P_{a}}=\mathbb{E}_{P_{a}}[xx^{\top}]\).

Implications and Discussions:**(1)** Equation (4.4) decomposes the suboptimality gap into two separate parts of error: error from reward learning (\(\mathcal{E}_{1}\)) and error coming from diffusion (\(\mathcal{E}_{2}\) and \(\mathcal{E}_{3}\)).

**(2)** It is also worth mentioning that \(\mathcal{E}_{2}\) and \(\mathcal{E}_{3}\) depend on \(t_{0}\) and that by taking \(t_{0}=\left((Dd^{2}+D^{2}d)/n_{1}\right)^{1/6}\) one gets a good trade-off and small \(\mathcal{E}_{2}+\mathcal{E}_{3}\).

**(3)**\(\mathcal{E}_{1}\) suggests diffusion model is essentially doing representation learning, reducing \(D\) to smaller latent dimension \(d\). It can be seen from \(\operatorname{Tr}(\widehat{\Sigma}_{\lambda}^{-1}\Sigma_{p_{q}})\leq\mathcal{ O}\left(\frac{a^{2}}{\|\beta^{*}\|_{\Sigma}}+d\right)\) when \(n_{2}=\Omega(\frac{1}{\lambda_{min}})\).

**(3)** If we ignore the diffusion errors when \(n_{1}\) is large enough, the suboptimatliy gap resembles the suboptimatliy of off-policy bandit learning in feature subspace [22, Section 3.2], [32, 5]. It showsthe major source of error occurs when moving towards the target distributions and the error behaves similar to bandit.

**(4)** On-support diffusion error entangles with distribution shift in complicated ways. We show

\[\mathcal{E}_{2}=\left(\texttt{DistroShift}(a)\cdot\left(d^{2}D+D^{2}d\right)^{1/ 6}{n_{1}}^{-1/6}\cdot a\right),\]

where \(\texttt{DistroShift}(a)\) quantifies the distribution shift depending on different reward values. In the special case of the latent covariance matrix \(\Sigma\) is known, we can quantify the distribution shift as \(\texttt{DistroShift}(a)=\mathcal{O}(a\lor d)\). We observe an interesting phase shift. When \(a<d\), the training data have a sufficient coverage with respect to the generated distribution \(\widehat{P}_{a}\). Therefore, the on-support diffusion error has a lenient linear dependence on \(a\). However, when \(a>d\), the data coverage is very poor and \(\mathcal{E}_{2}\) becomes quadratic in \(a\), which quickly amplifies.

**(5)** When generated samples deviate away from the latent space, the reward may substantially degrade as determined by the nature of \(h\).

To the authors' best knowledge, this is a first theoretical attempt to understand reward improvement of conditional diffusion. These results imply a potential connection between diffusion theory and off-policy bandit learning, which is interesting for more future research. See proofs in Appendix D.3.

### Extension to Nonparametric Function Class

Our theoretical analysis, in its full generality, extends to using general nonparametric function approximation for both the reward and score functions. To keep our paper succinct, we refer to Appendix F and Theorem F.4 for details of our nonparametric theory for reward-conditioned generation. Informally, the regret of generated samples is bounded by

\[\texttt{SubOpt}(\widehat{P}_{a};y^{*}=a)=\widetilde{\mathcal{O}}\left( \texttt{DistroShift}(a)\cdot\left(n_{2}^{-\frac{a}{2a+a}}+n_{1}^{-\frac{2}{3(d+ \widetilde{\mathsf{e}})}}\right)\right)+\mathbb{E}_{\widehat{P}_{a}}[h^{*}(x_{ \perp})]\]

with high probability. Additionally, the nonparamtric generators is able to estimate the representation matrix \(A\) up to an error of \(\angle(V,A)=\widetilde{\mathcal{O}}(n_{1}^{-\frac{1}{d+\widetilde{\mathsf{e}} }})\). Here the score is assumed to be Lipschitz continuous and \(\alpha\) is the smoothness parameter of the reward function, and \(\texttt{DistroShift}(a)\) is a class-restricted distribution shift measure. Our results on nonparametric function approximation covers the use of deep ReLU networks as special cases.

## 5 Numerical Experiments

### Simulation

We first perform the numerical simulation of Algorithm 1 following the setup in Assumption 4.1, 4.2 and 4.4. We choose \(d=16,D=64,g^{*}(x):=5\|x\|_{2}^{2}\), and generate \(\beta^{*}\) by uniformly sampling from the unit sphere. The latent variable \(z\) is generated from \(\mathbb{N}(0,\mathsf{l}_{\mathsf{d}})\), which is then used to construct \(x=Az\) with some randomly generated orthonormal matrix \(A\). We use the \(1\)-dimensional version of the UNet [40] to approximate the score function. More details are deferred to Appendix 1.

Figure 3 shows the average reward of the generated samples under different target reward values. We also plot the distribution shift and off-support deviation in terms of the \(2\)-norm distance from the support. For small target reward values, the generation average reward almost scales linearly with the target value, which is consistent with the theory as the distribution shift remains small for these target values. The generation reward begins to decrease as we further increase the target reward value, and the reason is two fold. Firstly, the off-support deviation of the generated samples becomes large in this case, which prevents the generation reward from further going up. Secondly, the distribution shift increases rapidly as we further increase the target value, making the theoretical guarantee no longer valid. In Figure 4, we show the distribution of the rewards in the generated samples. As we increase the target reward values, the generation rewards become less concentrated and are shifted to the left of the target value, which is also due to the distribution shift and off-support deviation.

### Directed Text-to-Image Generation

Next, we empirically verify our theory through directed text-to-image generation. Instead of training a diffusion model from scratch, we use Stable Diffusion v1.5 [39], pre-trained on LALION dataset [42]. Stable Diffusion operates on the latent space of its Variational Auto-Encoder and can incorporate text conditions. We show that by training a reward model we can further guide the Stable Diffusion model to generate images of desired properties.

**Ground-truth Reward Model.** We start from an ImageNet [10] pre-trained ResNet-18 [17] model and replace the final prediction layer with a randomly initialized linear layer of scalar outputs. Then we use this model as the ground-truth reward model. To investigate the meaning of this randomly-generated reward model, we generate random samples and manually inspect the images with high rewards and low rewards. The ground-truth reward model seems to favor colorful and vivid natural scenes against monochrome and dull images; see Appendix 1 for sample images.

**Labelled Dataset.** We use the ground-truth reward model to compute a scalar output for each instance in the CIFAR-10 [24] training dataset and perturb the output by adding a Gaussian noise from \(\mathcal{N}(0,0.01)\). We use the images and the corresponding outputs as the training dataset.

**Reward-network Training.** To avoid adding additional input to the diffusion model and tuning the new parameters, we introduce a new network \(\mu_{\theta}\) and approximate \(p_{t}(y|x_{t})\) by \(\mathsf{N}(\mu_{\theta}(\mathsf{x_{t}}),\sigma^{2})\). For simplicity, we set \(\sigma^{2}\) as a tunable hyperparameter. We share network parameters for different noise levels \(t\), so our \(\mu_{\theta}\) has no additional input of \(t\). We train \(\mu_{\theta}\) by minimizing the expected KL divergence between \(p_{t}(y|x_{t})\) and \(\mathsf{N}(\mu_{\theta}(\mathsf{x_{t}}),\sigma^{2})\):

\[\mathbb{E}_{\mathsf{t}}\mathbb{E}_{x_{t}}\Big{[}\mathrm{KL}(p_{t}(y|x_{t})\mid \mathsf{N}(\mu_{\theta}(\mathsf{x_{t}}),\sigma^{2}))\Big{]}=\mathbb{E}_{ \mathsf{t}}\mathbb{E}_{(\mathsf{x}_{t},y)\sim\mathsf{p_{t}}}\frac{\|\mathsf{ y}-\mu_{\theta}(\mathsf{x_{t}})\|_{2}^{2}}{2\sigma^{2}}+\mathrm{Constant}.\]

Equivalently, we train the reward model \(\mu_{\theta}\) to predict the noisy reward \(y\) from the noisy inputs \(x_{t}\). Also, notice that the minimizers of the objective do not depend on the choice of \(\sigma^{2}\).

**Reward-network-based Directed Diffusion.** To perform reward-directed conditional diffusion, observe that \(\nabla_{x}\log p_{t}(x|y)=\nabla_{x}\log p_{t}(x)+\nabla_{x}\log p_{t}(y|x)\), and \(p_{t}(y|x)\propto\exp\Big{(}-\frac{\|y-\mu_{\theta}(x)\|_{2}^{2}}{2\sigma^{2} }\Big{)}\). Therefore,

\[\nabla_{x}\log p_{t}(y|x)=-1/\sigma^{2}\cdot\nabla_{x}\Big{[}\frac{1}{2}\|y- \mu_{\theta}(x)\|_{2}^{2}\Big{]}.\]

In our implementation, we compute the gradient by back-propagation through \(\mu_{\theta}\) and incorporate this gradient guidance into each denoising step of the DDIM sampler [43] following [11] (equation (14)). We see that \(1/\sigma^{2}\) corresponds to the weights of the gradient with respect to unconditioned score. In the sequel, we refer to \(1/\sigma^{2}\) as the "guidance level", and \(y\) as the "target value".

Figure 4: **Shifting reward distribution of the generated population.**

Figure 3: **Quality of generated samples as target reward value increases. Left: Average reward of the generation; Middle: Distribution shift; Right: Off-support deviation. The errorbar is computed by \(2\) times the standard deviation over \(5\) runs.**

**Quantitative Results.** We vary \(1/\sigma^{2}\) in \(\{25,50,100,200,400\}\) and \(y\) in \(\{1,2,4,8,16\}\). For each combination, we generate 100 images with the text prompt "A nice photo" and calculate the mean and the standard variation of the predicted rewards and the ground-truth rewards. The results are plotted in Figure 5. From the plot, we see similar effects of increasing the target value \(y\) at different guidance levels \(1/\sigma^{2}\). A larger target value puts more weight on the guidance signals \(\nabla_{x}\mu_{\theta}(x)\), which successfully drives the generated images towards higher predicted rewards, but suffers more from the distribution-shift effects between the training distribution and the reward-conditioned distribution, which renders larger gaps between the predicted rewards and the ground-truth rewards. To optimally choose a target value, we must trade off between the two counteractive effects.

**Qualitative Results.** To qualitatively test the effects of the reward conditioning, we generate a set of images with increasing target values \(y\) under different text prompts and investigate the visual properties of the produced images. We isolate the effect of reward conditioning by fixing all the randomness during the generation processes, so the generated images have similar semantic layouts. After hyper-parameter tuning, we find that setting \(1/\sigma^{2}=100\) and \(y\in\{2,4,6,8,10\}\) achieves good results across different text prompts and random seeds. We pick out typical examples and summarized the results in Figure 6, which demonstrates that as we increase the target value, the generated images become more colorful at the expense of degradations of the image qualities.

## 6 Conclusion

In the paper, we study the problem of generating high-reward and high-quality samples using reward-directed conditional diffusion models, focusing on the semi-supervised setting where massive unlabeled data and limited labeled data are given. We provide theoretical results for subspace recovery and reward improvement, demonstrating the trade-off between the strength of the reward target and the distribution shift. Numerical results support our theory well.

Figure 5: **The predicted rewards and the ground-truth rewards of the generated images**. At each guidance level, increasing the target \(y\) successfully directs the generation towards higher predicted rewards, but also increases the error induced by the distribution shift. The reported baseline is the expected ground-truth reward for undirected generations.

Figure 6: **The effects of the reward-directed diffusion. Increasing the target value directs the images to be more colorful and vivid at the cost of degradation of the image qualities. Leftmost: without reward conditioning. Second-to-Last: target value \(y=2,4,6,8,10\). The guidance level \(1/\sigma^{2}\) is fixed to \(100\). The text prompts are ”A cat with a glass of water.”, ”An astronaut on the horseback”.**

## Acknowledgments and Disclosure of Funding

Mengdi Wang acknowledges the support by NSF grants CPS-2312093, DMS-1953686, IIS-2107304, CMMI1653435, ONR grant 1006977 and C3.AI.

## References

* Ajay et al. [2023] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua B. Tenenbaum, Tommi S. Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need for decision making? In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=sP1f02K9DFG.
* Anderson [1982] Brian DO Anderson. Reverse-time diffusion equation models. _Stochastic Processes and their Applications_, 12(3):313-326, 1982.
* Austin et al. [2021] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. _Advances in Neural Information Processing Systems_, 34:17981-17993, 2021.
* Balaji et al. [2022] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. _arXiv preprint arXiv:2211.01324_, 2022.
* Brandfonbrener et al. [2021] David Brandfonbrener, William Whitney, Rajesh Ranganath, and Joan Bruna. Offline contextual bandits with overparameterized models. In _International Conference on Machine Learning_, pages 1049-1058. PMLR, 2021.
* Chen and Jiang [2019] Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. In _International Conference on Machine Learning_, pages 1042-1051. PMLR, 2019.
* Chen et al. [2020] Minshuo Chen, Yu Bai, Jason D Lee, Tuo Zhao, Huan Wang, Caiming Xiong, and Richard Socher. Towards understanding hierarchical learning: Benefits of neural representations. _Advances in Neural Information Processing Systems_, 33:22134-22145, 2020.
* Chen et al. [2023] Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data. _arXiv preprint arXiv:2302.07194_, 2023.
* Chen et al. [2022] Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. _arXiv preprint arXiv:2209.11215_, 2022.
* Deng et al. [2009] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* Dhariwal and Nichol [2021] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _Advances in Neural Information Processing Systems_, 34:8780-8794, 2021.
* Fan et al. [2020] Jianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. A theoretical analysis of deep q-learning. In _Learning for Dynamics and Control_, pages 486-489. PMLR, 2020.
* Gong et al. [2019] Sixue Gong, Vishnu Naresh Boddeti, and Anil K Jain. On the intrinsic dimensionality of image representations. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3987-3996, 2019.
* Graikos et al. [2022] Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, and Dimitris Samaras. Diffusion models as plug-and-play priors. _arXiv preprint arXiv:2206.09012_, 2022.
* Gyorfi et al. [2002] Laszlo Gyorfi, Michael Kohler, Adam Krzyzak, and Harro Walk. _A distribution-free theory of nonparametric regression_, volume 1. Springer, 2002.
* Haussmann and Pardoux [1986] Ulrich G Haussmann and Etienne Pardoux. Time reversal of diffusions. _The Annals of Probability_, pages 1188-1205, 1986.

* He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* Ho and Salimans [2022] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.
* Ho et al. [2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.
* Hyvarinen and Dayan [2005] Aapo Hyvarinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. _Journal of Machine Learning Research_, 6(4), 2005.
* Janner et al. [2022] Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. In _International Conference on Machine Learning_, 2022.
* Jin et al. [2021] Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In _International Conference on Machine Learning_, pages 5084-5096. PMLR, 2021.
* Kingma and Dhariwal [2018] Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. _Advances in neural information processing systems_, 31, 2018.
* Krizhevsky et al. [2009] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* Lee et al. [2023] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for general data distributions. In _International Conference on Algorithmic Learning Theory_, pages 946-985. PMLR, 2023.
* Lee et al. [2023] Jin Sub Lee, Jisun Kim, and Philip M. Kim. Proteinsgm: Score-based generative modeling for de novo protein design. _bioRxiv_, 2023. doi: 10.1101/2022.07.13.499967. URL https://www.biorxiv.org/content/early/2023/02/04/2022.07.13.499967.
* Li et al. [2022] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto. Diffusion-lm improves controllable text generation. _Advances in Neural Information Processing Systems_, 35:4328-4343, 2022.
* Liang et al. [2023] Zhixuan Liang, Yao Mu, Mingyu Ding, Fei Ni, Masayoshi Tomizuka, and Ping Luo. Adaptdifffuser: Diffusion models as adaptive self-evolving planners. _arXiv preprint arXiv:2302.01877_, 2023.
* Liu et al. [2018] Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Infinite-horizon off-policy estimation. _Advances in Neural Information Processing Systems_, 31, 2018.
* Munos and Szepesvari [2008] Remi Munos and Csaba Szepesvari. Finite-time bounds for fitted value iteration. _Journal of Machine Learning Research_, 9(5), 2008.
* Nakada and Imaizumi [2020] Ryumei Nakada and Masaaki Imaizumi. Adaptive approximation and generalization of deep neural network with intrinsic dimensionality. _The Journal of Machine Learning Research_, 21(1):7018-7055, 2020.
* Nguyen-Tang et al. [2021] Thanh Nguyen-Tang, Sunil Gupta, A Tuan Nguyen, and Svetha Venkatesh. Offline neural contextual bandits: Pessimism, optimization and generalization. _arXiv preprint arXiv:2111.13807_, 2021.
* Nichol et al. [2021] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. _arXiv preprint arXiv:2112.10741_, 2021.
* Oko et al. [2023] Kazusato Oko, Shunta Akiyama, and Taiji Suzuki. Diffusion models are minimax optimal distribution estimators. In _ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models_, 2023. URL https://openreview.net/forum?id=6961CeTSFA.

* Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.
* Pearce et al. [2023] Tim Pearce, Tabish Rashid, Anssi Kanervisto, Dave Bignell, Mingfei Sun, Raluca Georgescu, Sergio Valcarcel Macua, Shan Zheng Tan, Ida Momennejad, Katja Hofmann, and Sam Devlin. Imitating human behaviour with diffusion models. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=Pv1GPQ2Rrc8.
* Pope et al. [2021] Phillip Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The intrinsic dimension of images and its impact on learning. _arXiv preprint arXiv:2104.08894_, 2021.
* Ramesh et al. [2022] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.
* Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.
* Ronneberger et al. [2015] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pages 234-241. Springer, 2015.
* Schick et al. [2021] Timo Schick, Sahana Udupa, and Hinrich Schutze. Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp. _Computing Research Repository_, arXiv:2103.00453, 2021. URL http://arxiv.org/abs/2103.00453.
* Schuhmann et al. [2022] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _arXiv preprint arXiv:2210.08402_, 2022.
* Song et al. [2021] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=StlgiarCHLP.
* Song and Ermon [2020] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. _Advances in neural information processing systems_, 33:12438-12448, 2020.
* Song et al. [2020] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to density and score estimation. In _Uncertainty in Artificial Intelligence_, pages 574-584. PMLR, 2020.
* Song et al. [2020] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.
* Song et al. [2023] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. _arXiv preprint arXiv:2303.01469_, 2023.
* Tenenbaum et al. [2000] Joshua B Tenenbaum, Vin de Silva, and John C Langford. A global geometric framework for nonlinear dimensionality reduction. _science_, 290(5500):2319-2323, 2000.
* Tsybakov [2008] Alexandre B. Tsybakov. _Introduction to Nonparametric Estimation_. Springer Publishing Company, Incorporated, 1st edition, 2008. ISBN 0387790519.
* Vahdat et al. [2021] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. _Advances in Neural Information Processing Systems_, 34:11287-11302, 2021.

* [51] Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* [52] Pascal Vincent. A connection between score matching and denoising autoencoders. _Neural computation_, 23(7):1661-1674, 2011.
* [53] Martin J Wainwright. _High-dimensional statistics: A non-asymptotic viewpoint_, volume 48. Cambridge university press, 2019.
* [54] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. _arXiv preprint arXiv:2302.05543_, 2023.

**Index of Supplementary Materials**

* SSA Preliminaries on Diffusion Models
* SSB Limitations and Open Problems
* SSC Omitted Proof in Section 3
* SSC.1 Proof of Proposition 3.1
* SSD Omitted Proofs in Section 4
* SSD.1 Parametric Conditional Score Matching Error
* SSD.2 Proof of Theorem 4.5
* SSD.3 Proof of Theorem 4.6
* SSB Supporting Lemmas and Proofs for SSD
* SSF Extensions to Nonparametric Function Class
* SSG Omitted Proofs in SSF
* SSH Proof for SSD.1
* SSI Additional Experimental Results

## Appendix A Preliminaries

### Diffusion Models

We first provide a brief review of diffusion models and its training/sampling procedure. We consider diffusion in continuous time [23, 46], where diffusion is described as forward and backward SDEs.

**Forward SDE and Score Matching.** In the forward process, noise is added to original data progressively as an Ornstein-Ulhenbeck process for instance:

\[\mathrm{d}X_{t}=-\frac{1}{2}g(t)X_{t}\mathrm{d}t+\sqrt{g(t)}\mathrm{d}W_{t} \ \ \text{for}\ \ g(t)>0,\] (A.1)

where initial \(X_{0}\sim P_{\mathrm{data}}\) and \((W_{t})_{t\geq 0}\) is a standard Wiener process, and \(g(t)\) is a nondecreasing weighting function. In practice, the forward process (A.1) terminates at a sufficiently large \(T>0\) such that the corrupted \(X_{T}\) is close to the standard Gaussian \(\mathsf{N}(\mathbf{0},I_{D})\). To enable data generation in future, the score \(\nabla\log p_{t}(\cdot)\) at \(t\) is the key to learn, here \(p_{t}\) denotes the marginal density of \(X_{t}\). We often use an estimated score function \(\widehat{s}(\cdot,t)\) trained by minimizing a score matching loss.

**Backward SDE for Generation.** Diffusion models generate samples through a backward SDE (A.2) reversing the time in (A.1) [2, 16], i.e.,

\[\mathrm{d}X_{t}^{\leftarrow}=\left[\frac{1}{2}g(T-t)X_{t}^{\leftarrow}+g(T-t )\nabla\log p_{T-t}(X_{t}^{\leftarrow})\right]\mathrm{d}t+\sqrt{g(T-t)} \mathrm{d}\overline{W}_{t},\] (A.2)

where \(\overline{W}_{t}\) is a reversed Wiener process. In practice, the backward process is initialized with \(\mathsf{N}(0,I_{D})\) and the unknown conditional score \(\nabla\log p_{t}(\cdot)\) is replaced by an estimated counterpart \(\widehat{s}(\cdot,t)\).

## Appendix B Limitations

We do not see outstanding limitations in our analysis. The linear subspace assumption initiates the study of conditional diffusion models on low-dimensional data. We expect to stimulate more sophisticated analyses under general assumptions such as manifold data.

## Appendix C Omitted Proof in Section 3

### Proof of Proposition 3.1

Proof.: For any \(t\geq 0\), it hold that \(\nabla_{x_{t}}\log p_{t}(x_{t}\mid y)=\nabla_{x_{t}}\log p_{t}(x_{t},y)\) since the gradient is taken w.r.t. \(x_{t}\) only. Then plugging in this equation and expanding the norm square on the LHS gives

\[\mathbb{E}_{(x_{t},y)\sim P_{t}}\left[\|\nabla_{x_{t}}\log p_{t}( x_{t},y)-s(x_{t},y,t)\|_{2}^{2}\right] =\mathbb{E}_{(x_{t},y)\sim P_{t}}\big{[}\|s(x_{t},y,t)\|_{2}^{2}\] \[\quad-2\langle\nabla_{x_{t}}\log p_{t}(x_{t},y),s(x_{t},y,t) \rangle\big{]}+C.\]Then it suffices to prove

\[\mathbb{E}_{(x_{t},y)\sim P_{t}}\left[\langle\nabla_{x_{t}}\log p_{t}(x_{t},y),s(x _{t},y,t)\rangle\right]=\mathbb{E}_{(x,y)\sim P_{x\widehat{y}}}\mathbb{E}_{x^{ \prime}\sim\mathsf{N}[(\alpha(t)x,h(t)I)}\left[\langle\nabla_{x^{\prime}}\phi_{t }(x^{\prime}\mid x),s(x^{\prime},y,t)\rangle\right]\]

Using integration by parts to rewrite the inner product we have

\[\mathbb{E}_{(x_{t},y)\sim P_{t}}\left[\langle\nabla_{x_{t}}\log p _{t}(x_{t},y),s(x_{t},y,t)\rangle\right] =\int p_{t}(x_{t},y)\langle\nabla_{x_{t}}\log p_{t}(x_{t},y),s(x_{ t},y,t)\rangle dx_{t}dy\] \[=\int\langle\nabla_{x_{t}}p_{t}(x_{t},y),s(x_{t},y,t)\rangle dx_{ t}dy\] \[=-\int p_{t}(x_{t},y)\operatorname{div}(s(x_{t},y,t))dx_{t}dy,\]

where denote by \(\phi_{t}(x^{\prime}|x)\) the density of \(\mathsf{N}(\alpha(t)x,h(t)I_{D})\) with \(\alpha(t)=\exp(-t/2)\) and \(h(t)=1-\exp(-t)\), then

\[-\int p_{t}(x_{t},y)\operatorname{div}(s(x_{t},y,t))dx_{t}dy =-\mathbb{E}_{(x,y)\sim P_{x\widehat{y}}}\int\phi_{t}(x^{\prime }\mid x)\operatorname{div}(s(x^{\prime},y,t))dx^{\prime}\] \[=\mathbb{E}_{(x,y)\sim P_{x\widehat{y}}}\int\langle\nabla_{x^{ \prime}}\phi_{t}(x^{\prime}\mid x),s(x^{\prime},y,t)\rangle dx^{\prime}\] \[=\mathbb{E}_{(x,y)\sim P_{x\widehat{y}}}\mathbb{E}_{x^{\prime} \sim\mathsf{N}(\alpha(t)x,h(t)I)}\left[\langle\nabla_{x^{\prime}}\phi_{t}(x^{ \prime}\mid x),s(x^{\prime},y,t)\rangle\right].\]

## Appendix D Omitted Proofs in Section 4

Additional Notations:We follow the notations in the main paper along with some additional ones. Use \(P_{t}^{LD}(z)\) to denote the low-dimensional distribution on \(z\) corrupted by diffusion noise. Formally, \(p_{t}^{LD}(z)=\int\phi_{t}(z^{\prime}|z)p_{z}(z)\mathrm{d}z\) with \(\phi_{t}(\cdot|z)\) being the density of \(\mathsf{N}(\alpha(t)z,h(t)I_{d})\). \(P_{t_{0}}^{LD}(z\mid\widehat{f}(Az)=a)\) the corresponding conditional distribution on \(\widehat{f}(Az)=a\) at \(t_{0}\), with shorthand as \(P_{t_{0}}^{LD}(a)\). Also give \(P_{z}(z\mid\widehat{f}(Az)=a)\) a shorthand as \(P^{LD}(a)\). In our theorems, \(\mathcal{O}\) hides constant factors and higher order terms in \(n_{1}^{-1}\) and \(n_{2}^{-1}\) and \(\,,\,\widetilde{\mathcal{O}}\) further hides logarithmic terms and can also hide factors in \(d\).

### Parametric Conditional Score Matching Error

Theorems presented in Section 4 are established upon the conditional score estimation error, which has been studied in [8] for general distributions, but in Lemma D.1 we provide a new one specific to our setting where the true score is linear in input \((x_{t},\widehat{y})\) due to the Gaussian design. Despite the linearity of score in Gaussian case, we emphasize matching score in (3.2) is not simply linear regression as \(\mathcal{S}\) consists of an encoder-decoder structure for estimating matrix \(A\) to reduce dimension (see SSH for \(\mathcal{S}\) construction and more proof details).

In the following lemma, we first present a general result for the case where the true score is within \(\mathcal{S}\), which is constructed as a parametric function class. Then the score matching error is bounded in terms of \(\mathcal{N}(\mathcal{S},1/n_{1})\), the \(\log\) covering number of \(\mathcal{S}\), recall \(n_{1}\) is the size of \(\mathcal{D}_{\mathrm{unlabel}}\). Instantiating this general result, we derive score matching error for Gaussian case by upper bounding \(\mathcal{N}(\mathcal{S},1/n_{1})\) in this special case.

**Lemma D.1**.: Under Assumption 4.1, if \(\nabla\log p_{t}(x\mid y)\in\mathcal{S}\), where

\[\mathcal{S}=\bigg{\{}\mathbf{s}_{V,\psi}(x,y,t)=\frac{1}{h(t)}(V\cdot\psi(V^{ \top}x,y,t)-x):\;V\in\mathbb{R}^{D\times d},\;\psi\in\Psi:\mathbb{R}^{d+1} \times[t_{0},T]\to\mathbb{R}^{d}\;\bigg{\}},\] (3.8)

with \(\Psi\) parametric. Then for \(\delta\geq 0\), with probability \(1-\delta\), the square score matching error is bounded by \(\epsilon_{diff}^{2}=\mathcal{O}\left(\frac{1}{t_{0}}\sqrt{\frac{\mathcal{N}( \mathcal{S},1/n_{1})(d^{2}\lor D)\log\frac{1}{2}}{n_{1}}}\right)\), i.e.,

\[\frac{1}{T-t_{0}}\int_{t_{0}}^{T}\mathbb{E}_{(x_{t},y)\sim P_{t}}\left[\| \nabla\log p_{t}(x_{t}|y)-\widehat{s}(x_{t},y,t)\|_{2}^{2}\right]\mathrm{d}t \leq\epsilon_{diff}^{2},\] (D.1)recall \(\widehat{P}_{t}\) comes from \(P_{x\widehat{y}}\) by noising \(x\) at \(t\) in the forward process. Under Assumption 4.4 and given \(\widehat{f}(x)=\widehat{\theta}^{\top}x\) and \(\widehat{y}=\widehat{f}(x)+\xi,\xi\sim\mathsf{N}(0,\nu^{2})\), the score function \(\nabla\log p_{t}(x\mid\widehat{y})\) to approximate is linear in \(x\) and \(\widehat{y}\). When approximated by \(S\) with \(\Psi\) linear, \(\mathcal{N}(\mathcal{S},1/n_{1})=\mathcal{O}((d^{2}+Dd)\log(Ddn_{1}))\).

Proof.: Proof is in SSH. 

To provide fidelity and reward guarantees of \(\widehat{P}_{a}\): the generated distribution of \(x\) given condition \(\widehat{y}=a\), we will need the following lemma. It provides a subspace recovery guarantee between \(V\)(score matching output) and \(A\)(ground truth), as well as a distance measure between distributions \(P_{a}\) and \(\widehat{P}_{a}\), given score matching error \(\epsilon_{diff}\).

Note \(P_{a}\) and \(\widehat{P}_{a}\) are over \(x\), which admits an underlying low-dimensional structure \(x=Az\). Thus we measure distance between \(P_{a}\) and \(\widehat{P}_{a}\) by defining

**Definition D.2**.: \(TV(\widehat{P}_{a}):=\mathsf{d}_{\mathrm{TV}}\left(P_{t_{0}}^{LD}(z\mid \widehat{f}(Az)=a),(U^{\top}V^{\top})_{\#}\widehat{P}_{a}\right)\) with notations:

* \(\mathsf{d}_{\mathrm{TV}}(\cdot,\cdot)\) is the TV distance between two distribution.
* \(f_{\sharp}P\) denotes a push-forward measure, i.e., for any measurable \(\Omega\), \((f_{\sharp}P)(\Omega)=P(f^{-1}(\Omega))\)
* \((V^{\top})_{\#}\widehat{P}_{a}\) pushes generated \(\widehat{P}_{a}\) forward to the low dimensional subspace using learned subspace matrix \(V\). \(U\) is an orthonormal matrix of dimension \(d\).
* \(P_{t_{0}}^{LD}(z\mid\widehat{f}(Az)=a)\) is close to \((A^{\top})_{\#}P_{a}\), with \(t_{0}\) taking account for the early stopping in backward process.

We note that there is a distribution shift between the training and the generated data, which has a profound impact on the generative performance. We quantify the influence of distribution shift by the following class restricted divergence measure.

**Definition D.3**.: Distribution shift between two arbitrary distributions \(P_{1}\) and \(P_{2}\) restricted under function class \(\mathcal{L}\) is defined as

\[\mathcal{T}(P_{1},P_{2};\mathcal{L})=\sup_{l\in\mathcal{L}}\mathbb{E}_{x \sim P_{1}}[l(x)]/\mathbb{E}_{x\sim P_{2}}[l(x)]\quad\text{with arbitrary two distributions $P_{1},P_{2}$}.\]

Definition D.3 is well perceived in bandit and RL literature [30, 29, 6, 12].

**Lemma D.4**.: Given the square score matching error (D.1) upper bounded by \(\epsilon_{diff}^{2}\), and when \(P_{z}\) satisfying Assumption 4.3 with \(c_{0}I_{d}\preceq\mathbb{E}_{z\sim P_{z}}\left[zz^{\top}\right]\), it guarantees on for \(x\sim\widehat{P}_{a}\) and \(\angle(V,A):=\|VV^{\top}-AA^{\top}\|_{\mathrm{F}}^{2}\) that

\[(I_{D}-VV^{\top})x\sim\mathsf{N}(0,\Lambda),\quad\Lambda\prec ct_ {0}I_{D},\] (D.2) \[\angle(V,A)=\widetilde{\mathcal{O}}\left(\frac{t_{0}}{c_{0}}\cdot \epsilon_{diff}^{2}\right).\] (D.3)

In addition,

\[TV(\widehat{P}_{a})=\widetilde{\mathcal{O}}\left(\sqrt{\frac{ \mathcal{T}(P(x,\widehat{y}=a),P_{x\widehat{y}};\bar{\mathcal{S}})}{c_{0}}} \cdot\epsilon_{diff}\right).\] (D.4)

with \(\bar{\mathcal{S}}=\left\{\frac{1}{T-t_{0}}\int_{t_{0}}^{T}\mathbb{E}_{x_{t}|x }\|\nabla\log p_{t}(x_{t}\mid y)-s(x_{t},y,t)\|_{2}^{2}\mathrm{d}t:s\in \mathcal{S}\right\}\). \(TV(\widehat{P}_{a})\) and \(\mathcal{T}(P(x,\widehat{y}=a),P_{x\widehat{y}};\bar{\mathcal{S}})\) are defined in Definition D.2 and D.3.

Proof.: Proof is in SSE.2.

### Proof of Theorem 4.5

Proof.: **Proof of \(\angle(V,A)\).** By Lemma 3 of [8], we have

\[\angle(V,A)=\mathcal{O}\left(\frac{t_{0}}{c_{0}}\cdot\epsilon_{diff}^{2}\right)\]

when the latent \(z\) satisfying Assumption 4.3 and \(c_{0}I_{d}\preceq\mathbb{E}_{z\sim P_{z}}\left[zz^{\top}\right]\). Therefore, by (D.1), we have with high probability that

\[\angle(V,A)=\widetilde{\mathcal{O}}\left(\frac{1}{c_{0}}\sqrt{\frac{\mathcal{ N}(\mathcal{S},1/n_{1})(D\lor d^{2})}{n_{1}}}\right).\]

When Assumption 4.4 holds, plugging in \(c_{0}=\lambda_{\min}\) and \(\mathcal{N}(\mathcal{S},1/n_{1})=\mathcal{O}((d^{2}+Dd)\log(Ddn_{1}))\), it gives

\[\angle(V,A)=\widetilde{\mathcal{O}}\left(\frac{1}{\lambda_{\min}}\sqrt{\frac{( D\lor d^{2})d^{2}+(D\lor d^{2})Dd}{n_{1}}}\right),\]

where \(\widetilde{\mathcal{O}}\) hides logarithmic terms. When \(D>d^{2}\), which is often the case in practical applications, we have

\[\angle(V,A)=\widetilde{\mathcal{O}}\left(\frac{1}{\lambda_{\min}}\sqrt{\frac{ Dd^{2}+D^{2}d}{n_{1}}}\right).\]

**Proof of \(\mathbb{E}_{x\sim\tilde{P}_{a}}[\|x_{\perp}\|_{2}]\).** By the definition of \(x_{\perp}\) that \(x_{\perp}=(I_{D}-AA^{\top})x\),

\[\mathbb{E}_{x\sim\tilde{P}_{a}}[\|x^{\perp}\|_{2}]=\mathbb{E}_{x\sim\tilde{P} _{a}}[\|(I_{D}-AA^{\top})x\|_{2}]\leq\sqrt{\mathbb{E}_{x\sim\tilde{P}_{a}}[\|( I_{D}-AA^{\top})x\|_{2}^{2}]}.\]

Score matching returns \(V\) as an approximation of \(A\), then

\[\|(I_{D}-AA^{\top})x\|_{2} \leq\|(I_{D}-VV^{\top})x\|_{2}+\|(VV^{\top}-AA^{\top})x\|_{2},\] \[\mathbb{E}_{x\sim\tilde{P}_{a}}[\|(I_{D}-AA^{\top})x\|_{2}^{2}] \leq 2\mathbb{E}_{x\sim\tilde{P}_{a}}[\|(I_{D}-VV^{\top})x\|_{2}^{2}]+2 \mathbb{E}_{x\sim\tilde{P}_{a}}[\|(VV^{\top}-AA^{\top})x\|_{2}^{2}],\]

where by (D.2) in Lemma D.4 we have

\[(I_{D}-VV^{\top})x\sim\mathsf{N}(\mathsf{0},\mathsf{\Lambda}),\quad\mathsf{ \Lambda}\prec\mathsf{ct_{0}l}\]

for some constant \(c\geq 0\). Thus

\[\mathbb{E}_{x\sim\tilde{P}_{a}}\left[\|(I_{D}-VV^{\top})x\|_{2}^{2}\right]= \mathrm{Tr}(\Lambda)\leq ct_{0}D.\] (D.5)

On the other hand,

\[\|(VV^{\top}-AA^{\top})x\|_{2}^{2} \leq\|VV^{\top}-AA^{\top}\|_{op}^{2}\|x\|_{2}^{2}\leq\|VV^{\top}- AA^{\top}\|_{F}^{2}\|x\|_{2}^{2},\]

where \(\|VV^{\top}-AA^{\top}\|_{F}^{2}\) has an upper bound as in (D.3) and \(\mathbb{E}_{x\sim\tilde{P}_{a}}\left[\|x\|_{2}^{2}\right])\) is bounded in Lemma E.3 by

\[\mathbb{E}_{x\sim\tilde{P}_{a}}\left[\|x\|_{2}^{2}\right]=\mathcal{O}\left( ct_{0}D+M(a)\cdot(1+TV(\widehat{P}_{a})\right).\]

with \(M(a)=O\left(\frac{a^{2}}{\|\beta^{*}\|_{\Sigma}}+d\right)\).

Therefore, to combine things together, we have

\[\mathbb{E}_{x\sim\tilde{P}_{a}}\|x^{\perp}\|_{2} \leq\sqrt{2\mathbb{E}_{x\sim\tilde{P}_{a}}[\|(I_{D}-VV^{\top})x\|_ {2}^{2}]+2\mathbb{E}_{x\sim\tilde{P}_{a}}[\|(VV^{\top}-AA^{\top})x\|_{2}^{2}]}\] \[\leq c^{\prime}\sqrt{t_{0}D}+2\sqrt{\angle(V,A)}\cdot\sqrt{ \mathbb{E}_{x\sim\tilde{P}_{a}}[\|x\|_{2}^{2}]}\] \[=\mathcal{O}\left(\sqrt{t_{0}D}+\sqrt{\angle(V,A)}\cdot\sqrt{M( a)}\right).\]

\(\mathcal{O}\) hides multiplicative constant and \(\sqrt{\angle(V,A)t_{0}D}\), \(\sqrt{\angle(V,A)M(a)TV(\widehat{P}_{a})}\), which are terms with higher power of \(n_{1}^{-1}\) than the leading term.

**Remark of Theorem 4.5**.: 1. Guarantee (4.2) applies to general distributions with light tail as assumed in Assumption 4.3.

2. Guarantee (4.3) guarantees high fidelity of generated data in terms of staying in the subspace when we have access to a large unlabeled dataset.

3. Guarantee (4.3) shows that \(\mathbb{E}_{x\sim\widehat{P}_{a}}[\|x_{\perp}\|_{2}]\) scales up when \(t_{0}\) goes up, which aligns with the dynamic in backward process that samples are concentrating to the learned subspace as \(t_{0}\) goes to \(0\). Taking \(t_{0}\to 0\), \(\mathbb{E}_{x\sim\widehat{P}_{a}}[\|x_{\perp}\|]\) has the decay in \(O(n_{1}^{-\frac{1}{4}})\). However, taking \(t_{0}\to 0\) is not ideal for the sake of high reward of \(x\), we take the best trade-off of \(t_{0}\) later in Theorem 4.6.

### Proof of Theorem 4.6

Proof of Theorem 4.6 and that of some results in "Implications and Discussions" following the theorem in main paper are provided in this section. This section breaks down into three parts: **Suboptimality Decomposition**, **Bounding \(\mathcal{E}_{1}\) Relating to Offline Bandits**, **Bounding \(\mathcal{E}_{2}\) and the Distribution Shift in Diffusion**.

#### d.3.1 \(\mathtt{SubOpt}(\widehat{P}_{a};y^{*}=a)\) Decomposition

Proof.: Recall notations \(\widehat{P}_{a}:=\widehat{P}(\cdot|\widehat{y}=a)\) (generated distribution) and \(P_{a}:=P(\cdot|\widehat{y}=a)\) (target distribution) and \(f^{*}(x)=g^{*}(x_{\parallel})-h^{*}(x_{\perp})\). \(\mathbb{E}_{x\sim\widehat{P}_{a}}[f^{*}(x)]\) can be decomposed into 3 terms:

\[\mathbb{E}_{x\sim\widehat{P}_{a}}[f^{*}(x)]\geq \mathbb{E}_{x\sim P_{a}}[f^{*}(x)]-\left|\mathbb{E}_{x\sim\widehat {P}_{a}}[f^{*}(x)]-\mathbb{E}_{x\sim P_{a}}[f^{*}(x)]\right|\] \[\geq \mathbb{E}_{x\sim P_{a}}[\widehat{f}(x)]-\mathbb{E}_{x\sim P_{a}} \left[\left|\widehat{f}(x)-f^{*}(x)\right|\right]-\left|\mathbb{E}_{x\sim \widehat{P}_{a}}[f^{*}(x)]-\mathbb{E}_{x\sim P_{a}}[f^{*}(x)]\right|\] \[\geq \mathbb{E}_{x\sim P_{a}}[\widehat{f}(x)]-\underbrace{\mathbb{E}_ {x\sim P_{a}}\left[\left|\widehat{f}(x)-g^{*}(x)\right|\right]}_{\mathcal{E} _{1}}\] \[-\underbrace{\left|\mathbb{E}_{x\sim P_{a}}[g^{*}(x_{\parallel})] -\mathbb{E}_{x\sim\widehat{P}_{a}}[g^{*}(x_{\parallel})]\right|}_{\mathcal{E} _{2}}-\underbrace{\mathbb{E}_{x\sim\widehat{P}_{a}}[h^{*}(x_{\perp})]}_{ \mathcal{E}_{3}},\]

where \(\mathbb{E}_{x\sim P_{a}}[\widehat{f}(x)]=\mathbb{E}_{a\sim q}[a]\) and we use \(x=x_{\parallel}\), \(f^{*}(x)=g^{*}(x)\) when \(x\sim P_{a}\). Therefore

\[\mathtt{SubOpt}(\widehat{P}_{a};y^{*}=a) =a-\mathbb{E}_{x\sim\widehat{P}_{a}}[f^{*}(x)]\] \[\leq\underbrace{\mathbb{E}_{x\sim P_{a}}\left[\left|\left(\widehat {\theta}-\theta^{*}\right)^{\top}x\right|\right]}_{\mathcal{E}_{1}}+ \underbrace{\left|\mathbb{E}_{x\sim P_{a}}[g^{*}(x_{\parallel})]-\mathbb{E}_ {x\sim\widehat{P}_{a}}[g^{*}(x_{\parallel})]\right|}_{\mathcal{E}_{2}}\] \[\quad+\underbrace{\mathbb{E}_{x\sim\widehat{P}_{a}}[h^{*}(x_{ \perp})]}_{\mathcal{E}_{3}}.\]

\(\mathcal{E}_{1}\) comes from regression: prediction/generalization error onto \(P_{a}\), which is independent from any error of distribution estimation that occurs in diffusion. \(\mathcal{E}_{2}\) and \(\mathcal{E}_{3}\) do not measure regression-predicted \(\widehat{f}\), thus they are independent from the prediction error in \(\widehat{f}\) for pseudo-labeling. \(\mathcal{E}_{2}\) measures the disparity between \(\widehat{P}_{a}\) and \(P_{a}\) on the subspace support and \(\mathcal{E}_{3}\) measures the off-subspace component in generated \(\widehat{P}_{a}\).

#### d.3.2 Bounding \(\mathcal{E}_{1}\) Relating to Offline Bandits

For all \(x_{i}\in\mathcal{D}_{\mathrm{label}},y_{i}=f^{*}(x_{i})+\epsilon_{i}=g(x_{i})+ \epsilon_{i}\). Thus, trained on \(\mathcal{D}_{\mathrm{label}}\) the prediction model \(\widehat{f}\) is essentially approximating \(g\). By estimating \(\theta^{*}\) with ridge regression on \(\mathcal{D}_{\mathrm{label}}\), we have \(\widehat{f}(x)=\widehat{\theta}^{\top}x\) with

\[\widehat{\theta}=\left(X^{\top}X+\lambda I\right)^{-1}X^{\top}\left(X\theta^{*} +\eta\right),\] (D.6)

where \(X^{\top}=(x_{1},\cdots,x_{i},\cdots,x_{n_{2}})\) and \(\eta=(\epsilon_{1},\cdots,\epsilon_{i},\cdots,\epsilon_{n_{2}})\).

**Lemma D.5**.: Under Assumption 4.1 and 4.2 and given \(\epsilon_{i}\sim\mathsf{N}(0,\sigma^{2})\), define \(V_{\lambda}:=X^{\top}X+\lambda I\), \(\widehat{\Sigma}_{\lambda}:=\frac{1}{n_{2}}V_{\lambda}\) and \(\Sigma_{P_{a}}:=\mathbb{E}_{x\sim P_{a}}xx^{\top}\) the covariance matrix (uncentered) of \(P_{a}\), and take \(\lambda=1\), then with high probability

\[\mathcal{E}_{1}\leq\sqrt{\mathrm{Tr}(\widehat{\Sigma}_{\lambda}^{-1}\Sigma_{P _{a}})}\cdot\frac{\mathcal{O}\left(\sqrt{d\log n_{2}}\right)}{\sqrt{n_{2}}}.\] (D.7)

Proof.: Proof is in SSE.3. 

**Lemma D.6**.: Under Assumption 4.1, 4.2 and 4.4, when \(\lambda=1\), \(P_{a}\) has a shift from the empirical marginal of \(x\) in dataset by

\[\mathrm{Tr}(\widehat{\Sigma}_{\lambda}^{-1}\Sigma_{P_{a}})\leq\mathcal{O} \left(\frac{a^{2}}{\|\beta^{*}\|_{\Sigma}}+d\right).\] (D.8)

when \(n_{2}=\Omega(\max\{\frac{1}{\lambda_{\min}},\frac{d}{\|\beta^{*}\|_{\Sigma}^{ 2}}\})\).

Proof.: Proof is in SSE.4. 

#### d.3.3 Bounding \(\mathcal{E}_{2}\) and the Distribution Shift in Diffusion

**Lemma D.7**.: Under Assumption 4.1, 4.2 and 4.4, when \(t_{0}=\left((Dd^{2}+D^{2}d)/n_{1}\right)^{1/6}\)

\[\mathcal{E}_{2}=\widetilde{\mathcal{O}}\left(\sqrt{\frac{\mathcal{T}(P(x, \widehat{y}=a),P_{x\widehat{y}};\mathcal{\bar{S}})}{\lambda_{\min}}}\cdot \left(\frac{Dd^{2}+D^{2}d}{n_{1}}\right)^{\frac{1}{6}}\cdot a\right).\]

Proof.: Proof is in SSE.5. 

Note that \(\mathcal{T}(P(x,\widehat{y}=a),P_{x\widehat{y}};\mathcal{\bar{S}})\) depends on \(a\) and measures the distribution shift between the desired distribution \(P(x,\widehat{y}=a)\) and the data distribution \(P_{x\widehat{y}}\). To understand this distribution's dependency on \(a\), it what follows we give \(\mathcal{T}(P(x,\widehat{y}=a),P_{x\widehat{y}};\mathcal{\bar{S}})\) a shorthand as \(\mathtt{DistroShift}^{2}(a)\) and give it an upper bound in one special case of the problem.

Distribution ShiftIn the special case of covariance \(\Sigma\) of \(z\) is known and \(\|A-V\|_{2}^{2}=\mathcal{O}\left(\|AA^{\top}-VV^{\top}\|_{\mathrm{F}}^{2}\right)\), we showcase a bound on the distribution shift in \(\mathcal{E}_{2}\), as promised in the discussion following Theorem 4.6. We have

\[\mathtt{DistroShift}^{2}(a)=\frac{\mathbb{E}_{P_{x,\widehat{y}=a}}[\ell(x,y; \widehat{s})]}{\mathbb{E}_{P_{x\widehat{y}}}[\ell(x,y;\widehat{s})]},\]

where \(\ell(x,y;\widehat{s})=\frac{1}{T-t_{0}}\int_{t_{0}}^{T}\mathbb{E}_{x^{\prime} |x}\|\nabla_{x^{\prime}}\log\phi_{t}(x^{\prime}|x)-\widehat{s}(x^{\prime},y,t) \|_{2}^{2}\mathrm{d}t\). By Proposition 3.1, it suffices to bound

\[\mathtt{DistroShift}^{2}(a)=\frac{\mathbb{E}_{P_{x,\widehat{y}=a}}[\int_{t_{0} }^{T}\|\nabla\log p_{t}(x,y)-\widehat{s}(x,y,t)\|_{2}^{2}\mathrm{d}t]}{ \mathbb{E}_{P_{x\widehat{y}}}[\int_{t_{0}}^{T}\|\nabla\log p_{t}(x,y)- \widehat{s}(x,y,t)\|_{2}^{2}\mathrm{d}t]}.\]

We expand the difference \(\|\nabla\log p_{t}(x,y)-\widehat{s}(x,y,t)\|_{2}^{2}\) by

\[\|\nabla\log p_{t}(x,y)-\widehat{s}(x,y,t)\|_{2}^{2} \leq\frac{2}{h^{2}(t)}\Big{[}\|(A-V)B_{t}(A^{\top}x+\nu^{-2}y\theta )\|_{2}^{2}+\|VB_{t}(A-V)^{\top}x\|_{2}^{2}\Big{]}\] \[\leq\frac{2}{h^{2}(t)}\Big{[}\|A-V\|_{2}^{2}(3\|x\|_{2}^{2}+y^{2}),\]

where we recall \(B_{t}\) is defined in (H.2) and in the last inequality, we use \((a+b)^{2}\leq 2a^{2}+2b^{2}\). In the case of covariance matrix \(\Sigma\) is known, i.e., \(B_{t}\) is known, we also consider matrix \(V\) directly matches \(A\) without rotation. Then by [8, Lemma 3 and 17], we have\(\mathcal{O}\left(t_{0}/c_{0}\mathbb{E}_{P_{x\widehat{y}}}[\ell(x,y;\widehat{s})]\right)\). To this end, we only need to find \(\mathbb{E}_{P_{x\widehat{y}=t}}[\|x\|_{2}^{2}]\). Since we consider on-support \(x\), which can be represented as \(x=Az\), we have \(\|x\|_{2}=\|z\|_{2}\). Thus, we only need to find the conditional distribution of \(z|\widehat{y}=a\). Fortunately, we know \((z,\widehat{y})\) is jointly Gaussian, with mean \(0\) and covariance

\[\begin{bmatrix}\Sigma&\Sigma\widehat{\beta}\\ \widehat{\beta}^{\top}\Sigma&\widehat{\beta}^{\top}\Sigma\widehat{\beta}+\nu^{ 2}\end{bmatrix}.\]

Consequently, the conditional distribution of \(z|\widehat{y}=a\) is still Gaussian, with mean \(\Sigma\widehat{\beta}a/(\widehat{\beta}^{\top}\Sigma\widehat{\beta}+\nu^{2})\) and covariance \(\Sigma-\Sigma\widehat{\beta}\widehat{\beta}^{\top}\Sigma/(\widehat{\beta}^{ \top}\Sigma\widehat{\beta}+\nu^{2})\). Hence, we have

\[\mathbb{E}_{P_{x\widehat{y}=a}}[\|z\|_{2}^{2}]=\frac{1}{(\widehat{\beta}^{ \top}\Sigma\widehat{\beta}+\nu^{2})^{2}}\left((a^{2}-\widehat{\beta}^{\top} \Sigma\widehat{\beta}-\nu^{2})\widehat{\beta}^{\top}\Sigma^{2}\widehat{\beta} \right)+\operatorname{Tr}(\Sigma)=\mathcal{O}\left(a^{2}\lor d\right).\]

We integrate over \(t\) for the numerator in \(\mathtt{DistroShift}(a)\) to obtain \(\mathbb{E}_{P_{x\widehat{y}=a}}[\int_{t_{0}}^{T}\|\nabla\log p_{t}(x,y)- \widehat{s}(x,y,t)\|_{2}^{2}\mathrm{d}t=\mathcal{O}\left((a^{2}\lor d)\frac{ 1}{c_{0}}\mathbb{E}_{P_{x\widehat{y}}}[\ell(x,y;\widehat{s})]\right)\). Note the cancellation between the numerator and denominator, we conclude

\[\mathtt{DistroShift}(a)=\mathcal{O}\left(\frac{1}{c_{0}}(a\vee\sqrt{d})\right).\]

As \(d\) is a natural upper bound of \(\sqrt{d}\) and viewing \(c_{0}\) as a constant, we have \(\mathtt{DistroShift}(a)=\mathcal{O}\left(a\lor d\right)\) as desired.

## Appendix E Supporting lemmas and proofs

### Supporting lemmas

**Lemma E.1**.: The estimated subspace \(V\) satisfies

\[\|VU-A\|_{F}=\mathcal{O}\left(d^{\frac{3}{2}}\sqrt{\angle(V,A)}\right)\] (E.1)

for some orthogonal matrix \(U\in\mathbb{R}^{d\times d}\).

Proof.: Proof is in SSE.6. 

**Lemma E.2**.: Suppose \(P_{1}\) and \(P_{2}\) are two distributions over \(\mathbb{R}^{d}\) and \(m\) is a function defined on \(\mathbb{R}^{d}\), then \([\mathbb{E}_{x\sim P_{1}}[m(z)]-\mathbb{E}_{z\sim P_{2}}[m(z)]]\)can be bounded in terms of \(\mathsf{d}_{\mathrm{TV}}(P_{1},P_{2})\), specifically when \(P_{1}\) and \(P_{2}\) are Gaussians and \(m(z)=\|z\|_{2}^{2}\):

\[\mathbb{E}_{x\sim P_{1}}[\|z\|_{2}^{2}]=\mathcal{O}\left(\mathbb{E}_{z\sim P_ {2}}[\|z\|_{2}^{2}](1+\mathsf{d}_{\mathrm{TV}}(P_{1},P_{2}))\right).\] (E.2)

When \(P_{1}\) and \(P_{2}\) are Gaussians and \(m(z)=\|z\|_{2}\):

\[\|\mathbb{E}_{z\sim P_{1}}[\|z\|_{2}]-\mathbb{E}_{z\sim P_{2}}[\|z\|_{2}]= \mathcal{O}\left(\left(\sqrt{\mathbb{E}_{z\sim P_{1}}[\|z\|_{2}^{2}]}+\sqrt{ \mathbb{E}_{z\sim P_{2}}[\|z\|_{2}^{2}]}\right)\cdot\mathsf{d}_{\mathrm{TV}}(P _{1},P_{2})\right).\] (E.3)

Proof.: Proof is in SSE.7. 

**Lemma E.3**.: We compute \(\mathbb{E}_{z\sim P^{LD}(a)}[\|z\|_{2}^{2}],\mathbb{E}_{z\sim P_{0}^{LD}(a)}[\|z \|_{2}^{2}],\mathbb{E}_{x\sim\widehat{P}_{a}}[\|x\|_{2}^{2}],\mathbb{E}_{z \sim(U^{\top}V^{\top})_{\#}\widehat{P}_{a}}[\|z\|_{2}^{2}]\) in this Lemma.

\[\mathbb{E}_{z\sim P^{LD}(a)}[\|z\|_{2}^{2}]=\frac{\widehat{\beta}^{\top} \Sigma^{2}\widehat{\beta}}{\left(\|\widehat{\beta}\|_{\Sigma}^{2}+\nu^{2} \right)^{2}}a^{2}+\operatorname{trace}(\Sigma-\Sigma\widehat{\beta}\left( \widehat{\beta}^{\top}\Sigma\widehat{\beta}+\nu^{2}\right)^{-1}\widehat{\beta}^ {\top}\Sigma).\] (E.4)

Let \(M(a):=\mathbb{E}_{z\sim P^{LD}(a)}[\|z\|_{2}^{2}]\), which has an upper bound \(M(a)=O\left(\frac{a^{2}}{\|\beta^{\top}\|_{\Sigma}}+d\right)\).

\[\mathbb{E}_{z\sim P_{0}^{LD}(a)}[\|z\|_{2}^{2}]\leq M(a)+t_{0}d.\] (E.5) \[\mathbb{E}_{x\sim\widehat{P}_{a}}\left[\|x\|_{2}^{2}\right]\leq \mathcal{O}\left(ct_{0}D+M(a)\cdot(1+TV(\widehat{P}_{a})\right).\] (E.6)

Proof.: Proof is in SSE.8.

### Proof of Lemma d.4

Proof.: The first two assertions (D.2) and (D.3) are consequences of [8, Theorem 3, item 1 and 3]. To show (D.4), we first have the conditional score matching error under distribution shift being

\[\mathcal{T}(P(x,\widehat{y}=a),P_{x\widehat{y}};\mathcal{\bar{S}})\cdot\epsilon _{diff}^{2},\]

where \(\mathcal{T}(P(x,\widehat{y}=a),P_{x\widehat{y}};\mathcal{\bar{S}})\) accounts for the distribution shift as in the parametric case (Lemma D.7). Then we apply [8, Theorem 3, item 2] to conclude

\[TV(\widehat{P}_{a})=\widetilde{\mathcal{O}}\left(\sqrt{\frac{\mathcal{T}(P(x,\widehat{y}=a),P_{x\widehat{y}};\mathcal{\bar{S}})}{c_{0}}}\cdot\epsilon_{ diff}\right).\]

The proof is complete. 

### Proof of Lemma d.5

Proof.: Given

\[\mathcal{E}_{1}=\mathbb{E}_{\widehat{P}_{a}}\left|x^{\top}(\theta^{*}- \widehat{\theta})\right|\leq\mathbb{E}_{\widehat{P}_{a}}\|x\|_{V_{\lambda}^ {-1}}\cdot\|\theta^{*}-\widehat{\theta}\|_{V_{\lambda}},\]

then things to prove are

\[\mathbb{E}_{\widehat{P}_{a}}\|x\|_{V_{\lambda}^{-1}} =\sqrt{\operatorname{trace}(V_{\lambda}^{-1}\Sigma_{\widehat{P}_ {a}})};\] (E.7) \[\|\theta^{*}-\widehat{\theta}\|_{V_{\lambda}} \leq\mathcal{O}\left(\sqrt{d\log n_{2}}\right),\] (E.8)

where the second inequality is to be proven with high probability w.r.t the randomness in \(\mathcal{D}_{label}\). For (E.7), \(\mathbb{E}_{\widehat{P}_{a}}\|x\|_{V_{\lambda}^{-1}}\leq\sqrt{\mathbb{E}_{ \widehat{P}_{a}}\operatorname{\mathrm{arc}}^{\top}V_{\lambda}^{-1}x}=\sqrt{ \mathbb{E}_{\widehat{P}_{a}}\operatorname{\mathrm{trace}}(V_{\lambda}^{-1}xx ^{\top})}=\sqrt{\operatorname{trace}(V_{\lambda}^{-1}\mathbb{E}_{\widehat{P} _{a}}xx^{\top})}.\)

For (E.8), what's new to prove compared to a classic bandit derivation is its \(d\) dependency instead of \(D\), due to the linear subspace structure in \(x\). From the closed form solution of \(\widehat{\theta}\), we have

\[\widehat{\theta}-\theta^{*}=V_{\lambda}^{-1}X^{\top}\eta-\lambda V_{\lambda} ^{-1}\theta^{*}.\] (E.9)

Therefore,

\[\|\theta^{*}-\widehat{\theta}\|_{V_{\lambda}}\leq\|X^{\top}\eta\|_{V_{ \lambda}^{-1}}+\lambda\|\theta^{*}\|_{V_{\lambda}^{-1}},\] (E.10)

where \(\lambda\|\theta^{*}\|_{V_{\lambda}^{-1}}\leq\sqrt{\lambda}\|\theta^{*}\|_{2} \leq\sqrt{\lambda}\) and

\[\|X^{\top}\eta\|_{V_{\lambda}^{-1}}^{2} =\eta^{\top}X\left(X^{\top}X+\lambda I_{D}\right)^{-1}X^{\top}\eta\] \[=\eta^{\top}XX^{\top}\left(XX^{\top}+\lambda I_{n_{2}}\right)^{-1 }\eta.\]

Let \(Z^{\top}=(z_{1},\cdots,z_{i},\cdots,z_{n_{2}})\) s.t. \(Az_{i}=x_{i}\), then it holds that \(X=ZA^{\top}\), and \(XX^{\top}=ZA^{\top}AZ^{\top}=ZZ^{\top}\), thus

\[\|X^{\top}\eta\|_{V_{\lambda}^{-1}}^{2} =\eta^{\top}XX^{\top}\left(XX^{\top}+\lambda I_{n_{2}}\right)^{-1 }\eta\] \[=\eta^{\top}ZZ^{\top}\left(ZZ^{\top}+\lambda I_{n_{2}}\right)^{-1 }\eta\] \[=\eta^{\top}Z\left(Z^{\top}Z+\lambda I_{d}\right)^{-1}Z^{\top}\eta\] \[=\|Z^{\top}\eta\|_{(Z^{\top}Z+\lambda I_{d})^{-1}}.\]

With probability \(1-\delta\), \(\|z_{i}\|^{2}\leq d+\sqrt{d\log\left(\frac{2n_{2}}{\delta}\right)}:=L^{2},\forall i \in[n_{2}]\). Then Theorem 1 in "Improved algorithms for linear stochastic bandits" (by Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari) gives rise to

\[\|Z^{\top}\eta\|_{(Z^{\top}Z+\lambda I_{d})^{-1}}\leq\sqrt{2\log(2/\delta)+d \log(1+n_{2}L^{2}/(\lambda d))}\]

with probability \(1-\delta/2\). Combine things together and plugging in \(\lambda=1\), \(L^{2}=d+\sqrt{d\log\left(\frac{2n_{2}}{\delta}\right)}\), we have with high probability

\[\|\theta^{*}-\widehat{\theta}\|_{V_{\lambda}}=\mathcal{O}\left(\sqrt{d\log \left(n_{2}\sqrt{\log(n_{2})}\right)}\right)=\mathcal{O}\left(\sqrt{d\log n_ {2}+\frac{1}{2}d\log(\log n_{2})}\right)=\mathcal{O}\left(\sqrt{d\log n_{2}} \right).\]

### Proof of Lemma D.6

Proof.: Recall the definition of \(\widehat{\Sigma}_{\lambda}\) and \(\Sigma_{P_{a}}\) that

\[\widehat{\Sigma}_{\lambda} =\frac{1}{n_{2}}X^{\top}X+\frac{\lambda}{n_{2}}I_{D},\] \[\Sigma_{P_{a}} =\mathbb{E}_{x\sim P_{a}}\left[xx^{\top}\right],\]

where \(X\) are stack matrix of data supported on \(\mathcal{A}\) and \(P_{a}\) is also supported on \(\mathcal{A}\), \(\mathcal{A}\) is the subspace encoded by matrix \(A\). The following lemma shows it is equivalent to measure \(\operatorname{trace}(\widehat{\Sigma}_{\lambda}^{-1}\Sigma_{P_{a}})\) on \(\mathcal{A}\) subspace.

**Lemma E.4**.: For any P.S.D. matrices \(\Sigma_{1},\Sigma_{2}\in\mathbb{R}^{d\times d}\) and \(A\in\mathbb{R}^{D\times d}\) such that \(A^{\top}A=I_{d}\), we have

\[\operatorname{Tr}\left((\lambda I_{D}+A\Sigma_{1}A^{\top})^{-1}A\Sigma_{2}A^{ \top}\right)=\operatorname{Tr}\left(\left(\lambda I_{d}+\Sigma_{1}\right)^{-1 }\Sigma_{2}\right).\]

The lemma above allows us to abuse notations \(\widehat{\Sigma}_{\lambda}\) and \(\Sigma_{P_{a}}\) in the following way while keeping the same \(\operatorname{trace}(\widehat{\Sigma}_{\lambda}^{-1}\Sigma_{P_{a}})\) value:

\[\widehat{\Sigma}_{\lambda} =\frac{1}{n_{2}}Z^{\top}Z+\frac{\lambda}{n_{2}}I_{d},\] \[\Sigma_{P_{a}} =\mathbb{E}_{z\sim P^{LD}(a)}\left[zz^{\top}\right],\]

where \(Z^{\top}=(z_{1},\cdots,z_{i},\cdots,z_{n_{2}})\) s.t. \(Az_{i}=x_{i}\) and recall notataion \(P^{LD}(a)=P_{z}\left(z\mid\widehat{f}(Az)\right)\).

Given \(z\sim\mathsf{N}(\mu,\Sigma)\), as a proof artifact, let \(\widehat{f}(x)=\widehat{\theta}^{\top}x+\xi,\xi\sim\mathsf{N}(0,\nu^{2})\) where we will let \(\nu\to 0\) in the end, then let \(\widehat{\beta}=A^{\top}\widehat{\theta}\in\mathbb{R}^{d}\), \((z,\widehat{f}(Az))\) has a joint distribution

\[(z,\widehat{f})\sim\mathsf{N}\left(\left[\begin{matrix}\mu\\ \widehat{\beta}^{\top}\mu\end{matrix}\right],\left[\begin{matrix}\Sigma& \Sigma\widehat{\beta}\\ \widehat{\beta}^{\top}\Sigma&\widehat{\beta}^{\top}\Sigma\widehat{\beta}+\nu^{ 2}\end{matrix}\right]\right).\] (E.11)

Then we have the conditional distribution \(z\mid\widehat{f}(Az)=a\) following

\[P_{z}\left(z\mid\widehat{f}(Az)=a\right)=\mathsf{N}\left(\mu+\Sigma\widehat{ \beta}\left(\widehat{\beta}^{\top}\Sigma\widehat{\beta}+\nu^{2}\right)^{-1}(a -\widehat{\beta}^{\top}\mu),\Gamma\right)\] (E.12)

with \(\Gamma:=\Sigma-\Sigma\widehat{\beta}\left(\widehat{\beta}^{\top}\Sigma \widehat{\beta}+\nu^{2}\right)^{-1}\widehat{\beta}^{\top}\Sigma\).

When \(\mu=0\), we compute \(\operatorname{trace}(\widehat{\Sigma}_{\lambda}^{-1}\Sigma_{P_{a}})\) as

\[\operatorname{trace}(\widehat{\Sigma}_{\lambda}^{-1}\Sigma_{P_{ a}}) =\operatorname{trace}\left(\widehat{\Sigma}_{\lambda}^{-1}\frac{ \Sigma\widehat{\beta}\widehat{\beta}^{\top}\Sigma}{\left(\|\widehat{\beta}\| _{\Sigma}^{2}+\nu^{2}\right)^{2}}a^{2}\right)+\operatorname{trace}\left( \widehat{\Sigma}_{\lambda}^{-1}\Gamma\right)\] \[=\operatorname{trace}\left(\frac{\widehat{\beta}^{\top}\Sigma \widehat{\Sigma}_{\lambda}^{-1}\Sigma\widehat{\beta}}{\left(\|\widehat{\beta} \|_{\Sigma}^{2}+\nu^{2}\right)^{2}}a^{2}\right)+\operatorname{trace}\left( \widehat{\Sigma}_{\lambda}^{-1}\Sigma\right)-\operatorname{trace}\left( \widehat{\Sigma}_{\lambda}^{-1}\frac{\Sigma\widehat{\beta}\widehat{\beta}^{ \top}\Sigma}{\|\widehat{\beta}\|_{\Sigma}^{2}+\nu^{2}}\right)\] \[=\operatorname{trace}\left(\frac{\Sigma^{1/2}\widehat{\beta} \widehat{\beta}^{\top}\Sigma^{1/2}\Sigma^{1/2}\widehat{\Sigma}_{\lambda}^{-1} \Sigma^{1/2}}{\left(\|\widehat{\beta}\|_{\Sigma}^{2}+\nu^{2}\right)^{2}}a^{2}\right)\] \[\leq\frac{\|\Sigma\widehat{\Sigma}_{\lambda}^{-1}\Sigma\|_{op} \cdot\|\widehat{\beta}\|_{\Sigma}^{2}}{\left(\|\widehat{\beta}\|_{\Sigma}^{2} +\nu^{2}\right)^{2}}\cdot a^{2}+\operatorname{trace}\left(\Sigma^{\frac{1}{2} \widehat{\Sigma}_{\lambda}^{-1}\Sigma^{\frac{1}{2}}}\right)\]

By the Lemma 3 in [7], it holds that

\[\|\Sigma^{\frac{1}{2}}\widehat{\Sigma}_{\lambda}^{-1}\Sigma^{\frac{1}{2}}-I_{d} \|_{2}\leq O\left(\frac{1}{\sqrt{\lambda_{\min}n_{2}}}\right).\] (E.13)Therefore,

\[\mathrm{trace}(\widehat{\Sigma}_{\lambda}^{-1}\Sigma_{P_{a}})\leq\frac{1+\frac{1}{ \sqrt{\lambda_{\min}n_{2}}}}{\|\widehat{\beta}\|_{\Sigma}^{2}}\cdot a^{2}+O \left(d\left(1+\frac{1}{\sqrt{\lambda_{\min}n_{2}}}\right)\right).\]

Then, what left is to bound \(\|\widehat{\beta}\|_{\Sigma}=\|\widehat{\theta}\|_{A\Sigma A^{\top}}\geq\| \theta^{*}\|_{A\Sigma A^{\top}}-\|\widehat{\theta}-\theta^{*}\|_{A\Sigma A^{ \top}}\) by triangle inequality. On one hand,

\[\|\theta^{*}\|_{A\Sigma A^{\top}}=\|\beta^{*}\|_{\Sigma}.\] (E.14)

On the other hand,

\[\|\widehat{\theta}-\theta^{*}\|_{A\Sigma A^{\top}} =\mathcal{O}\left(\|\widehat{\theta}-\theta^{*}\|_{\widehat{ \Sigma}_{\lambda}}\right)\] \[=\mathcal{O}\left(\frac{\|\widehat{\theta}-\theta^{*}\|_{V_{ \lambda}}}{\sqrt{n_{2}}}\right)\] \[=\mathcal{O}\left(\sqrt{\frac{d\log(n_{2})}{n_{2}}}\right).\]

with high probability. Thus when \(n_{2}=\Omega(\frac{d}{\|\beta^{*}\|_{\Sigma}^{2}})\)

\[\|\widehat{\beta}\|_{\Sigma}\geq\frac{1}{2}\|\beta^{*}\|_{\Sigma}.\]

Therefore

\[\mathrm{trace}(\widehat{\Sigma}_{\lambda}^{-1}\Sigma_{P_{a}})\leq\mathcal{O} \left(\frac{1+\frac{1}{\sqrt{\lambda_{\min}n_{2}}}}{\|\beta^{*}\|_{\Sigma}} \cdot a^{2}+d\left(1+\frac{1}{\sqrt{\lambda_{\min}n_{2}}}\right)\right)\,= \mathcal{O}\left(\frac{a^{2}}{\|\beta^{*}\|_{\Sigma}}+d\right).\] (E.15)

when \(n_{2}=\Omega(\max\{\frac{1}{\lambda_{\min}},\frac{d}{\|\beta^{*}\|_{\Sigma}^{ 2}}\})\). 

### Proof of lemma D.7

Proof.: Recall the definition of \(g(x)\) that

\[g(x)={\theta^{*}}^{\top}AA^{\top}x,\]

note that \(g(x)={\theta^{*}}^{\top}x\) when \(x\) is supported on \(\mathcal{A}\). Thus,

\[\left|\mathbb{E}_{x\sim P_{a}}[g(x)]-\mathbb{E}_{x\sim\widehat{P} _{a}}[g(x)]\right|\] \[= \left|\mathbb{E}_{x\sim P_{a}}[{\theta^{*}}^{\top}x]-\mathbb{E}_ {x\sim\widehat{P}_{a}}[{\theta^{*}}^{\top}AA^{\top}x]\right|\] \[\leq \left|\mathbb{E}_{x\sim P_{a}}[{\theta^{*}}^{\top}x]-\mathbb{E}_ {x\sim\widehat{P}_{a}}[{\theta^{*}}^{\top}VV^{\top}x]\right|+\underbrace{ \left|\mathbb{E}_{x\sim\widehat{P}_{a}}[{\theta^{*}}^{\top}VV^{\top}x]-\mathbb{ E}_{x\sim\widehat{P}_{a}}[{\theta^{*}}^{\top}AA^{\top}x]\right|}_{e_{1}},\]

where

\[e_{1} =\left|\mathbb{E}_{x\sim\widehat{P}_{a}}[{\theta^{*}}^{\top}\left( VV^{\top}-AA^{\top}\right)x]\right|\] \[\leq\mathbb{E}_{x\sim\widehat{P}_{a}}\left(\left(\left\|\left(VV ^{\top}-AA^{\top}\right)x\right\|\right)\right.\] \[\leq\|VV^{\top}-AA^{\top}\|_{F}\cdot\sqrt{\mathbb{E}_{x\sim \widehat{P}_{a}}\left[\|x\|_{2}^{2}\right]}.\]

Use notation \(P^{LD}(a)=P(z\mid\widehat{f}(Az)=a)\), \(P^{LD}_{t_{0}}(a)=P_{t_{0}}(z\mid\widehat{f}(Az)=a)\)

\[\left|\mathbb{E}_{x\sim P_{a}}[{\theta^{*}}^{\top}x]-\mathbb{E}_ {x\sim\widehat{P}_{a}}[{\theta^{*}}^{\top}VV^{\top}x]\right|\] \[= \left|\mathbb{E}_{z\sim P(z|\widehat{f}(Az)=a)}[{\theta^{*}}^{ \top}Az]-\mathbb{E}_{z\sim(U^{\top}V^{\top})_{\#}\widehat{P}_{a}}[{\theta^{*} }^{\top}VVz]\right|\] \[\leq \left|\mathbb{E}_{z\sim P^{LD}_{t_{0}}(a)}[{\theta^{*}}^{\top}Az] -\mathbb{E}_{z\sim(U^{\top}V^{\top})_{\#}\widehat{P}_{a}}[{\theta^{*}}^{\top }VUz]\right|+\underbrace{\left|\mathbb{E}_{z\sim P^{LD}_{t_{0}}(a)}[{\theta^{ *}}^{\top}Az]-\mathbb{E}_{z\sim P^{LD}(a)}[{\theta^{*}}^{\top}Az]\right|}_{e_{2}},\]here

\[e_{2} =\left|\alpha(t_{0})\mathbb{E}_{z\sim P^{LD}_{t_{0}}(a)}[{\theta^{*}}^ {\top}Az]+h(t_{0})\mathbb{E}_{u\sim\text{N}(0,t_{0})}[{\theta^{*}}^{\top}Au]- \mathbb{E}_{z\sim P^{LD}(a)}[{\theta^{*}}^{\top}Az]\right|\] \[\leq(1-\alpha(t_{0}))\left|\mathbb{E}_{z\sim P^{LD}(a)}[{\theta^{ *}}^{\top}Az]\right|\] \[\leq t_{0}\cdot\mathbb{E}_{z\sim P^{LD}(a)}[\|z\|_{2}].\]

Then what is left to bound is

\[\left|\mathbb{E}_{z\sim P^{LD}_{t_{0}}(a)}[{\theta^{*}}^{\top}Az]- \mathbb{E}_{z\sim(U^{\top}V^{\top})_{\#}\widehat{P}_{a}}[{\theta^{*}}^{\top}VUz]\right|\] \[\leq \underbrace{\left|\mathbb{E}_{z\sim P^{LD}_{t_{0}}(a)}[{\theta^{ *}}^{\top}VUz]-\mathbb{E}_{z\sim(U^{\top}V^{\top})_{\#}\widehat{P}_{a}}[{ \theta^{*}}^{\top}VUz]\right|}_{e_{3}}+\underbrace{\left|\mathbb{E}_{z\sim P^ {LD}_{t_{0}}(a)}[{\theta^{*}}^{\top}VUz]-\mathbb{E}_{z\sim P^{LD}_{t_{0}}(a)} [{\theta^{*}}^{\top}Az]\right|}_{e_{4}}.\]

Then for term \(e_{3}\), by Lemma E.2, we get

\[e_{3} \leq\left|\mathbb{E}_{z\sim P^{LD}_{t_{0}}(a)}[\|z\|_{2}]- \mathbb{E}_{z\sim(U^{\top}V^{\top})_{\#}\widehat{P}_{a}}[\|z\|_{2}]\right|\] \[=\mathcal{O}\left(TV(\widehat{P}_{a})\cdot\left(\sqrt{\mathbb{E}_ {z\sim P^{LD}_{t_{0}}(a)}[\|z\|_{2}^{2}]}+\sqrt{\mathbb{E}_{z\sim\widehat{P}_ {a}}[\|x\|_{2}^{2}]}\right)\right),\]

where we use \(\mathbb{E}_{z\sim(U^{\top}V^{\top})_{\#}\widehat{P}_{a}}[\|z\|_{2}]\leq \mathbb{E}_{x\sim\widehat{P}_{a}}[\|x\|_{2}]\)

For \(e_{4}\), we have

\[e_{4} =\left|\mathbb{E}_{z\sim P^{LD}_{t_{0}}(a)}[{\theta^{*}}^{\top}( VU-A)z]\right|\] \[=\alpha(t_{0})\left|\mathbb{E}_{z\sim P(a)}[{\theta^{*}}^{\top}( VU-A)z]\right|\] \[\leq\|VU-A\|_{F}\cdot\mathbb{E}_{z\sim P^{LD}(a)}[\|z\|_{2}].\]

Therefore, by combining things together, we have

\[\mathcal{E}_{2}\leq e_{1}+e_{2}+e_{3}+e_{4}\] \[\leq \|VV^{\top}-AA^{\top}\|_{F}\cdot\sqrt{\mathbb{E}_{x\sim\widehat{P }_{a}}[\|x\|_{2}^{2}]}+(\|VU-A\|_{F}+t_{0})\cdot\sqrt{M(a)}\] \[+\mathcal{O}\left(TV(\widehat{P}_{a})\cdot\left(\sqrt{M(a)+t_{0} d}+\sqrt{\mathbb{E}_{x\sim\widehat{P}_{a}}[\|x\|_{2}^{2}]}\right)\right).\]

By Lemma D.4 and Lemma E.1, we have

\[TV(\widehat{P}_{a})=\widetilde{\mathcal{O}}\left(\sqrt{\frac{ \mathcal{T}(P(x,\widehat{y}=a),P_{x\widehat{y}};\mathcal{S})}{\lambda_{\min}} \cdot\epsilon_{diff}}\right),\] \[\|VV^{\top}-AA^{\top}\|_{\text{F}}=\widetilde{\mathcal{O}}\left( \frac{\sqrt{t_{0}}}{\sqrt{\lambda_{\min}}}\cdot\epsilon_{diff}\right),\] \[\|VU-A\|_{\text{F}}=\mathcal{O}(d^{\frac{3}{2}}\|VV^{\top}-AA^{ \top}\|_{\text{F}}).\]

And by Lemma E.3

\[\mathbb{E}_{x\sim\widehat{P}_{a}}\left[\|x\|_{2}^{2}\right]= \mathcal{O}\left(ct_{0}D+M(a)\cdot(1+TV(\widehat{P}_{a})\right).\]

Therefore. leading term in \(\mathcal{E}_{2}\) is

\[\mathcal{E}_{2}=\mathcal{O}\left((TV(\widehat{P}_{a})+t_{0})\sqrt{M(a)} \right).\]

By plugging in score matching error \(\epsilon_{diff}^{2}=\widetilde{\mathcal{O}}\left(\frac{1}{t_{0}}\sqrt{\frac{Dd ^{2}+D^{2}d}{n_{1}}}\right)\), we have

\[TV(\widehat{P}_{a})=\widetilde{\mathcal{O}}\left(\sqrt{\frac{\mathcal{T}(P(x, \widehat{y}=a),P_{x\widehat{y}};\mathcal{S})}{\lambda_{\min}}}\cdot\left( \frac{Dd^{2}+D^{2}d}{n_{1}}\right)^{\frac{1}{4}}\cdot\frac{1}{\sqrt{t_{0}}} \right).\]When \(t_{0}=\left((Dd^{2}+D^{2}d)/n_{1}\right)^{1/6}\), it admits the best trade off in \(\mathcal{E}_{2}\) and \(\mathcal{E}_{2}\) is bounded by

\[\mathcal{E}_{2}=\widetilde{\mathcal{O}}\left(\sqrt{\frac{\mathcal{T}(P(x,\hat{y }=a),P_{x\hat{y}};\mathcal{\bar{S}})}{\lambda_{\min}}}\cdot\left(\frac{Dd^{2}+ D^{2}d}{n_{1}}\right)^{\frac{1}{6}}\cdot a\right).\]

### Proof of Lemma e.1

Proof.: From Lemma 17 in [8], we have

\[\|U-V^{\top}A\|_{F}=\mathcal{O}(\|VV^{\top}-AA^{\top}\|_{F}).\]

Then it suffices to bound

\[\left|\|VU-A\|_{F}^{2}-\|U-V^{\top}A\|_{F}^{2}\right|,\]

where

\[\|VU-A\|_{F}^{2}=2d-\operatorname{trace}\left(U^{\top}V^{\top}A+ A^{\top}VU\right)\] \[\|U-V^{\top}A\|_{F}^{2}=d+\operatorname{trace}\left(A^{\top}VV^{ \top}A\right)-\operatorname{trace}\left(U^{\top}V^{\top}A+A^{\top}VU\right).\]

Thus

\[\left|\|VU-A\|_{F}^{2}-\|U-V^{\top}A\|_{F}^{2}\right|=\left|d- \operatorname{trace}\left(A^{\top}VV^{\top}A\right)\right|=\left| \operatorname{trace}\left(AA^{\top}(VV^{\top}-AA^{\top})\right)\right|,\]

which is because \(\operatorname{trace}\left(A^{\top}VV^{\top}A\right)\) is calculated as

\[\operatorname{trace}\left(A^{\top}VV^{\top}A\right) =\operatorname{trace}\left(AA^{\top}VV^{\top}\right)\] \[=\operatorname{trace}\left(AA^{\top}AA^{\top}\right)+ \operatorname{trace}\left(AA^{\top}(VV^{\top}-AA^{\top})\right)\] \[=d+\operatorname{trace}\left(AA^{\top}(VV^{\top}-AA^{\top}) \right).\]

Then we will bound \(\left|\operatorname{trace}\left(AA^{\top}(VV^{\top}-AA^{\top})\right)\right|\) by \(\|VV^{\top}-AA^{\top}\|_{F}\),

\[\left|\operatorname{trace}\left(AA^{\top}(VV^{\top}-AA^{\top})\right)\right| \leq\operatorname{trace}\left(AA^{\top}\right)\operatorname{ trace}\left(\left|VV^{\top}-AA^{\top}\right|\right)\] \[\leq d\cdot\operatorname{trace}\left(\left|VV^{\top}-AA^{\top} \right|\right)\] \[\leq d\cdot\sqrt{2d\left\|VV^{\top}-AA^{\top}\right\|_{F}^{2}}.\]

Thus, \(\|VU-A\|_{F}=\mathcal{O}\left(d^{\frac{3}{2}}\sqrt{\angle(V,A)}\right).\) 

### Proof of Lemma e.2

Proof.: When \(P_{1}\) and \(P_{2}\) are Gaussian, \(m(z)=\|z\|_{2}^{2}\)

\[\left|\mathbb{E}_{z\sim P_{1}}[m(z)]-\mathbb{E}_{z\sim P_{2}}[m( z)]\right|\] \[=\left|\int m(z)\left(p_{1}(z)-p_{2}(z)\right)\mathrm{d}z\right|\] \[\leq\left|\int_{\|z\|_{2}\leq R}\|z\|_{2}^{2}\left(p_{1}(z)-p_{2} (z)\right)\mathrm{d}z\right|+\int_{\|z\|_{2}>R}\|z\|_{2}^{2}p_{1}(z)\mathrm{d} z+\int_{\|z\|_{2}>R}\|z\|_{2}^{2}p_{2}(z)\mathrm{d}z\] \[\leq R^{2}\mathsf{d}_{\mathrm{TV}}(P_{1},P_{2})+\int_{\|z\|_{2}> R}\|z\|_{2}^{2}p_{1}(z)\mathrm{d}z+\int_{\|z\|_{2}>R}\|z\|_{2}^{2}p_{2}(z) \mathrm{d}z.\]

Since \(P_{1}\) and \(P_{2}\) are Gaussians, \(\int_{\|z\|_{2}>R}\|z\|_{2}^{2}p_{1}(z)\mathrm{d}z\) and \(\int_{\|z\|_{2}>R}\|z\|_{2}^{2}p_{2}(z)\mathrm{d}z\) are bounded by some constant \(C_{1}\) when \(R^{2}\geq C_{2}\max\{\mathbb{E}_{z\sim P_{1}}[\|z\|_{2}^{2}],\mathbb{E}_{z\sim P _{2}}[\|z\|_{2}^{2}]\}\) as suggested by Lemma 16 in [8].

Therefore,

\[\mathbb{E}_{z\sim P_{1}}[\|z\|_{2}^{2}] \leq\mathbb{E}_{z\sim P_{2}}[\|z\|_{2}^{2}]+C_{2}\max\{\mathbb{E}_{ P_{1}}[\|z\|_{2}^{2}],\mathbb{E}_{P_{2}}[\|z\|_{2}^{2}]\}\cdot\mathsf{d}_{\rm TV}(P_{1},P_{2} )+2C_{1}\] \[\leq\mathbb{E}_{z\sim P_{2}}[\|z\|_{2}^{2}]+C_{2}(\mathbb{E}_{z \sim P_{1}}[\|z\|_{2}^{2}]+\mathbb{E}_{z\sim P_{2}}[\|z\|_{2}^{2}])\cdot\mathsf{ d}_{\rm TV}(P_{1},P_{2})+2C_{1}.\]

Then

\[\mathbb{E}_{z\sim P_{1}}[\|z\|_{2}^{2}]=\mathcal{O}\left(\mathbb{E}_{z\sim P_{ 2}}[\|z\|_{2}^{2}]+\mathbb{E}_{z\sim P_{2}}[\|z\|_{2}^{2}]\cdot\mathsf{d}_{\rm TV }(P_{1},P_{2})\right)\]

since \(\mathsf{d}_{\rm TV}(P_{1},P_{2})\) decays with \(n_{1}\).

Similarly, when \(m(z)=\|z\|_{2}\)

\[\left|\mathbb{E}_{z\sim P_{1}}[m(z)]-\mathbb{E}_{z\sim P_{2}}[m(z )]\right|\] \[=\left|\int m(z)\left(p_{1}(z)-p_{2}(z)\right)\mathrm{d}z\right|\] \[\leq\left|\int_{\|z\|_{2}\leq R}\|z\|_{2}\left(p_{1}(z)-p_{2}(z) \right)\mathrm{d}z\right|+\int_{\|z\|_{2}>R}\|z\|_{2}p_{1}(z)\mathrm{d}z+\int_ {\|z\|_{2}>R}\|z\|_{2}p_{2}(z)\mathrm{d}z\] \[\leq R\mathsf{d}_{\rm TV}(P_{1},P_{2})+\sqrt{\int_{\|z\|_{2}>R} \|z\|_{2}^{2}p_{1}(z)\mathrm{d}z}+\sqrt{\int_{\|z\|_{2}>R}\|z\|_{2}^{2}p_{2}(z )\mathrm{d}z},\]

where \(\int_{\|z\|_{2}>R}\|z\|_{2}^{2}p_{1}(z)\mathrm{d}z\) and \(\int_{\|z\|_{2}>R}\|z\|_{2}^{2}p_{2}(z)\mathrm{d}z\) are bounded by some constant \(C_{1}\) when \(R^{2}\geq C_{2}\max\{\mathbb{E}_{z\sim P_{1}}[\|z\|_{2}^{2}],\mathbb{E}_{z\sim P _{2}}[\|z\|_{2}^{2}]\}\) as suggested by Lemma 16 in [8].

Therefore,

\[\left|\mathbb{E}_{z\sim P_{1}}[\|z\|_{2}]-\mathbb{E}_{z\sim P_{2} }[\|z\|_{2}]\right| \leq\sqrt{C_{2}\max\{\mathbb{E}_{P_{1}}[\|z\|_{2}^{2}],\mathbb{E} _{P_{2}}[\|z\|_{2}^{2}]\}}\cdot\mathsf{d}_{\rm TV}(P_{1},P_{2})+2C_{1}\] \[\leq\left(\sqrt{C_{2}\mathbb{E}_{z\sim P_{1}}[\|z\|_{2}^{2}]}+ \sqrt{C_{2}\mathbb{E}_{z\sim P_{2}}[\|z\|_{2}^{2}]}\right)\cdot\mathsf{d}_{ \rm TV}(P_{1},P_{2})+2C_{1}\] \[=\mathcal{O}\left(\left(\sqrt{\mathbb{E}_{z\sim P_{1}}[\|z\|_{2}^ {2}]}+\sqrt{\mathbb{E}_{z\sim P_{2}}[\|z\|_{2}^{2}]}\right)\cdot\mathsf{d}_{ \rm TV}(P_{1},P_{2})\right).\]

### Proof of Lemma e.3

Proof.: Recall from (E.12) that

\[P^{LD}(a)=P_{z}(z\mid\widehat{f}(Az)=a)=\mathsf{N}\left(\mu(\mathsf{a}),\Gamma\right)\]

with \(\mu(a):=\Sigma\widehat{\beta}\left(\widehat{\beta}^{\top}\Sigma\widehat{ \beta}+\nu^{2}\right)^{-1}a\), \(\Gamma:=\Sigma-\Sigma\widehat{\beta}\left(\widehat{\beta}^{\top}\Sigma\widehat {\beta}+\nu^{2}\right)^{-1}\widehat{\beta}^{\top}\Sigma\).

\[\mathbb{E}_{z\sim P^{LD}(a)}\left[\|z\|_{2}^{2}\right] =\mu(a)^{\top}\mu(a)+\mathrm{trace}(\Gamma)\] \[=\frac{\widehat{\beta}^{\top}\Sigma^{2}\widehat{\beta}}{\left(\| \widehat{\beta}\|_{\Sigma}^{2}+\nu^{2}\right)^{2}}a^{2}+\mathrm{trace}(\Sigma- \Sigma\widehat{\beta}\left(\widehat{\beta}^{\top}\Sigma\widehat{\beta}+\nu^{2 }\right)^{-1}\widehat{\beta}^{\top}\Sigma)\] \[=:M(a).\]

\[M(a) =\mathcal{O}\left(\frac{\widehat{\beta}^{\top}\Sigma^{2}\widehat{ \beta}}{\left(\|\widehat{\beta}\|_{\Sigma}^{2}\right)^{2}}a^{2}+\mathrm{trace}( \Sigma)\right)\] \[=\mathcal{O}\left(\frac{a^{2}}{\|\widehat{\beta}\|_{\Sigma}}+d \right),\]and by Lemma D.6

\[\|\widehat{\beta}\|_{\Sigma}\leq\frac{1}{2}\|\beta^{*}\|_{\Sigma}.\]

Thus \(\mathbb{E}_{z\sim P^{LD}(a)}\left[\|z\|_{2}^{2}\right]=M(a),M(a)=\mathcal{O} \left(\frac{a^{2}}{\|\beta^{*}\|_{\Sigma}}+d\right)\).

Thus after adding diffusion noise at \(t_{0}\), we have for \(\alpha(t)=e^{-t/2}\) and \(h(t)=1-e^{-t}\):

\[\mathbb{E}_{z\sim P^{LD}_{t_{0}}(a)}\left[\|z\|_{2}^{2}\right] =\mathbb{E}_{z_{0}\sim P^{LD}(a)}\mathbb{E}_{z\sim\mathcal{N}( \alpha(t_{0})\cdot z_{0},h(t_{0})\cdot t_{0})}\left[\|z\|_{2}^{2}\right]\] \[=\mathbb{E}_{z_{0}\sim P^{LD}(a)}\left[\alpha^{2}(t_{0})\|z_{0} \|_{2}^{2}+d\cdot h(t_{0})\right]\] \[=\alpha^{2}(t_{0})\cdot\mathbb{E}_{z_{0}\sim P^{LD}(a)}\left[\|z _{0}\|_{2}^{2}\right]+d\cdot h(t_{0})\] \[=e^{-t_{0}}\cdot\mathbb{E}_{z_{0}\sim P^{LD}(a)}\left[\|z_{0}\|_ {2}^{2}\right]+(1-e^{-t_{0}})\cdot d.\]

Thus \(\mathbb{E}_{z\sim P^{LD}_{t_{0}}(a)}\left[\|z\|_{2}^{2}\right]\leq M(a)+t_{0}d\).

By orthogonal decomposition we have

\[\mathbb{E}_{x\sim\widehat{P}_{a}}\left[\|x\|_{2}^{2}\right] \leq\mathbb{E}_{x\sim\widehat{P}_{a}}\left[\|(I_{D}-VV^{\top})x\|_ {2}^{2}\right]+\mathbb{E}_{x\sim\widehat{P}_{a}}\left[\|VV^{\top}x\|_{2}^{2}\right]\] \[=\mathbb{E}_{x\sim\widehat{P}_{a}}\left[\|(I_{D}-VV^{\top})x\|_{2 }^{2}\right]+\mathbb{E}_{x\sim\widehat{P}_{a}}\left[\|U^{\top}V^{\top}x\|_{2}^ {2}\right],\]

where \(\mathbb{E}_{x\sim\widehat{P}_{a}}\left[\|(I_{D}-VV^{\top})x\|_{2}^{2}\right]\) is bounded by (D.5) and the distribution of \(U^{\top}V^{\top}x\), which is \((U^{\top}V^{\top})_{\#}\widehat{P}_{a}\), is close to \(\mathbb{P}^{LD}_{t_{0}}(a)\) up to \(TV(\widehat{P}_{a})\), which is defined in Definition D.2. Then by Lemma E.2, we have

\[\mathbb{E}_{x\sim\widehat{P}_{a}}\left[\|U^{\top}V^{\top}x\|_{2}^{2}\right]= \mathcal{O}\left(\mathbb{E}_{z\sim P^{LD}_{t_{0}}(a)}\left[\|z\|_{2}^{2}\right] (1+TV(\widehat{P}_{a})\right).\]

Thus \(\mathbb{E}_{x\sim\widehat{P}_{a}}\left[\|x\|_{2}^{2}\right]=\mathcal{O}\left( ct_{0}D+(M(a)+t_{0}d)\cdot(1+TV(\widehat{P}_{a})\right)\).

### Proof of Lemma e.4

Proof.: Firstly, one can verify the following two equations by direct calculation:

\[(\lambda I_{D}+A\Sigma_{1}A^{\top})^{-1} =\frac{1}{\lambda}\left(I_{D}-A(\lambda I_{d}+\Sigma_{1})^{-1} \Sigma_{1}A^{\top}\right),\] \[(\lambda I_{d}+\Sigma_{1})^{-1} =\frac{1}{\lambda}\left(I_{d}-(\lambda I_{d}+\Sigma_{1})^{-1} \Sigma_{1}\right).\]

Then we have

\[(\lambda I_{D}+A\Sigma_{1}A^{\top})^{-1}A\Sigma_{2}A^{\top}= \frac{1}{\lambda}\left(I_{D}-A(\lambda I_{d}+\Sigma_{1})^{-1} \Sigma_{1}A^{\top}\right)A\Sigma_{2}A^{\top}\] \[= \frac{1}{\lambda}\left(A\Sigma_{2}A^{\top}-A(\lambda I_{d}+ \Sigma_{1})^{-1}\Sigma_{1}\Sigma_{2}A^{\top}\right).\]

Therefore,

\[\mathrm{Tr}\left((\lambda I_{D}+A\Sigma_{1}A^{\top})^{-1}A\Sigma_ {2}A^{\top}\right)= \mathrm{Tr}\left(\frac{1}{\lambda}\left(A\Sigma_{2}A^{\top}-A( \lambda I_{d}+\Sigma_{1})^{-1}\Sigma_{1}\Sigma_{2}A^{\top}\right)\right)\] \[= \mathrm{Tr}\left(\frac{1}{\lambda}\left(\Sigma_{2}-(\lambda I_{d }+\Sigma_{1})^{-1}\Sigma_{1}\Sigma_{2}\right)\right)\] \[= \mathrm{Tr}\left(\frac{1}{\lambda}\left(I_{d}-(\lambda I_{d}+ \Sigma_{1})^{-1}\Sigma_{1}\right)\Sigma_{2}\right)\] \[= \mathrm{Tr}\left(\left(\lambda I_{d}+\Sigma_{1}\right)^{-1}\Sigma_ {2}\right),\]

which has finished the proof.

Theory in Nonparametric Setting

Built upon the insights from Section 4, we provide analysis to the nonparametric reward and general data sampling setting. We generalize Assumption 4.2 to the following.

**Assumption F.1**. The ground truth reward \(f^{*}\) is decomposed as

\[f^{*}(x)=g^{*}(x_{\parallel})-h^{*}(x_{\perp}),\]

where \(g^{*}(x_{\parallel})\) is \(\alpha\)-Holder continuous for \(\alpha\geq 1\) and \(h^{*}(x_{\perp})\) is nondecreasing in terms of \(\|x_{\perp}\|_{2}\) with \(h^{*}(0)=0\). Moreover, \(g^{*}\) has a bounded Holder norm, i.e., \(\|g^{*}\|_{\mathcal{H}^{\alpha}}\leq 1\).

Holder continuity is widely studied in nonparametric statistics literature [15, 49]. \(h^{*}\) here penalizes off-support extrapolation.

Under Assumption F.1, we use nonparametric regression for estimating \(f^{*}\). Specifically, we specialize (3.1) in Algorithm 1 to

\[\widehat{f}_{\theta}\in\operatorname*{argmin}_{f_{\theta}\in\mathcal{F}}\frac {1}{2n}\sum_{i=1}^{n_{1}}(f_{\theta}(x_{i})-y_{i})^{2},\]

where \(\mathcal{F}=\operatorname{NN}(L,M,J,K,\kappa)\) is chosen to be a class of neural networks. Hyperparameters in \(\mathcal{F}\) will be chosen properly in Theorem F.4.

Our theory also considers generic sampling distributions on \(x\). Since \(x\) lies in a low-dimensional subspace, this translates to a sampling distribution assumption on latent variable \(z\).

**Assumption F.2**. The latent variable \(z\) follows distribution \(P_{z}\) with density \(p_{z}\), such that there exists constants \(B,C_{1},C_{2}\) verifying \(p_{z}(z)\leq(2\pi)^{-(d+1)/2}C_{1}\exp\left(-C_{2}\|z\|_{2}^{2}/2\right)\) whenever \(\|z\|_{2}>B\). And \(c_{0}I_{d}\preceq\mathbb{E}_{z\sim P_{z}}\left[zz^{\top}\right]\).

Assumption F.2 says \(P_{z}\) has a light tail, which is standard in high-dimensional statistics [51, 53]. Assumption F.2 also encodes distributions with a compact support. Furthermore, we assume that the curated data \((x,\widehat{y})\) induces Lipschitz conditional scores. Motivated by Chen et al. [8], we show that the linear subspace structure in \(x\) leads to a similar conditional score decomposition \(\nabla\log p_{t}(x|\widehat{y})=s_{\parallel}(x,\widehat{y},t)+s_{\perp}(x, \widehat{y},t)\), where \(s_{\parallel}\) is the on-support score and \(s_{\perp}\) is the orthogonal score. The decomposition for conditional score is as (H.1), which applies to both parametric and non-parametric cases. The following assumption is imposed on \(s_{\parallel}\).

**Assumption F.3**. The on-support conditional score function \(s_{\parallel}(x,\widehat{y},t)\) is Lipschitz with respect to \(x,\widehat{y}\) for any \(t\in(0,T]\), i.e., there exists a constant \(C_{\mathrm{lip}}\), such that for any \(x,\widehat{y}\) and \(x^{\prime},\widehat{y}^{\prime}\), it holds

\[\|s_{\parallel}(x,\widehat{y},t)-s_{\parallel}(x^{\prime},\widehat{y}^{\prime },t)\|_{2}\leq C_{\mathrm{lip}}\|x-x^{\prime}\|_{2}+C_{\mathrm{lip}}|\widehat{ y}-\widehat{y}^{\prime}|_{2}.\]

Lipschitz score is commonly adopted in existing works [9, 25]. Yet Assumption F.3 only requires the Lipschitz continuity of the on-support score, which matches the weak regularity conditions in Lee et al. [25], Chen et al. [8]. We then choose the score network architecture similar to that in the linear reward setting, except we replace \(m\) by a nonlinear network. Recall the linear encoder and decoder estimate the representation matrix \(A\).

We consider feedforward networks with ReLU activation functions as concept classes \(\mathcal{F}\) and \(\mathcal{S}\) for nonparametric regression and conditional score matching. Generalization to different network architectures poses no real difficulty. Given an input \(x\), neural networks compute

\[f_{\operatorname{NN}}(x)=W_{L}\sigma(\ldots\sigma(W_{1}x+b_{1})\ldots)+b_{L},\] (F.1)

where \(W_{i}\) and \(b_{i}\) are weight matrices and intercepts, respectively. We then define a class of neural networks as

\[\operatorname{NN}(L,M,J,K,\kappa)=\Big{\{}f:f\text{ in the form of \eqref{eq:f_1} with }L\text{ layers and width bounded by }M,\] \[\sup_{x}\|f(x)\|_{2}\leq K,\max\{\|b_{i}\|_{\infty},\|W_{i}\|_{ \infty}\}\leq\kappa\text{ for }i=1,\ldots,L,\text{ and }\sum_{i=1}^{L}\big{(}\|W_{i}\|_{0}+\|b_{i}\|_{0}\big{)}\leq J\Big{\}}.\]For the conditional score network, we will additionally impose some Lipschitz continuity requirement, i.e., \(\|f(x)-f(y)\|_{2}\leq c_{\mathrm{lip}}\|x-y\|_{2}\) for some Lipschitz coefficient \(c_{\mathrm{lip}}\).

Recall the distribution shift defined in Definition D.3 that

\[\mathcal{T}(P_{1},P_{2};\mathcal{L})=\sup_{l\in\mathcal{L}}\mathbb{E}_{x \sim P_{1}}[l(x)]/\mathbb{E}_{x\sim P_{2}}[l(x)]\]

for arbitrary two distributions \(P_{1},P_{2}\) and function class \(\mathcal{L}\). Similar to the parametric case, use notation \(\widehat{P}_{a}:=\widehat{P}(\cdot|\widehat{y}=a)\) and \(P_{a}:=P(\cdot|\widehat{y}=a)\). Then we can bound \(\mathtt{SubOpt}(\widehat{P}_{a};y^{*}=a)\) in Theorem F.4 in terms of non-parametric regression error, score matching error and distribution shifts in both regression and score matching.

**Theorem F.4**.: Suppose Assumption 4.1, F.1, F.2 and F.3 hold. Let \(\delta(n)=\frac{d\log\log n}{\log n}\). Properly chosen \(\mathcal{F}\) and \(\mathcal{S}\), with high probability, running Algorithm 1 with a target reward value \(a\) and stopping at \(t_{0}=\left(n_{1}^{-\frac{2-2\delta(n_{1})}{d+6}}+Dn_{1}^{-\frac{d+4}{d+6}} \right)^{\frac{1}{3}}\) gives rise to \(\angle(V,A)\leq\widetilde{\mathcal{O}}\left(\frac{1}{c_{0}}\left(n_{1}^{-\frac {2-2\delta(n_{1})}{d+6}}+Dn_{1}^{-\frac{d+4}{d+6}}\right)\right)\) and

\[\mathtt{SubOpt}(\widehat{P}_{a};y^{*}=a)\] \[\leq\underbrace{\sqrt{\mathcal{T}(P(x|\widehat{y}=a),P_{x}; \mathcal{F})}\cdot\widetilde{\mathcal{O}}\left(n_{2}^{-\frac{\alpha-\delta(n_{ 2})}{2\alpha+d}}+D/n_{2}\right)}_{\mathcal{E}_{1}}\] \[+\underbrace{\left(\sqrt{\frac{\mathcal{T}(P(x,\widehat{y}=a),P_ {x\widehat{y}};\mathcal{S})}{c_{0}}}\cdot\|g^{*}\|_{\infty}+\sqrt{M(a)}\right) \cdot\widetilde{\mathcal{O}}\left(\left(n_{1}^{-\frac{2-2\delta(n_{1})}{d+6}} +Dn_{1}^{-\frac{d+4}{d+6}}\right)^{\frac{1}{3}}\right)}_{\mathcal{E}_{2}}\] \[+\underbrace{\mathbb{E}_{x\sim\widehat{P}_{a}}[h^{*}(x_{\perp})]} _{\mathcal{E}_{3}},\]

where \(M(a):=\mathbb{E}_{z\sim\mathbb{P}(a)}[\|z\|_{2}^{2}]\) and

\[\mathcal{\bar{F}}:=\left\{|f^{*}(x)-f(x)|^{2}:f\in\mathcal{F}\right\},\quad \mathcal{\bar{S}}=\left\{\frac{1}{T-t_{0}}\int_{t_{0}}^{T}\mathbb{E}_{x_{i}|x }\|\nabla\log p_{t}(x_{t}\mid y)-s(x_{t},y,t)\|_{2}^{2}\mathrm{d}t:s\in \mathcal{S}\right\},\]

\(\mathcal{E}_{3}\) penalizes the component in \(\widehat{P}_{a}\) that is off the truth subspace. The function classes \(\mathcal{F}\) and \(\mathcal{S}\) are chosen as \(\mathcal{F}=\mathrm{NN}(L_{f},M_{f},J_{f},K_{f},\kappa_{f})\) with

\[L_{f}=\mathcal{O}(\log n_{2}),\;M_{f}=\mathcal{O}\left(n_{2}^{- \frac{d}{d+2\alpha}}(\log n_{2})^{d/2}\right),\;J_{f}=\mathcal{O}\left(n_{2}^ {-\frac{d}{d+2\alpha}}(\log n_{2})^{d/2+1}\right)\] \[K_{f}=1,\;\kappa_{f}=\mathcal{O}\left(\sqrt{\log n_{2}}\right)\]

and \(\mathcal{S}=\mathrm{NN}(L_{s},M_{s},J_{s},K_{s},\kappa_{s})\) with

\[L_{s}=\mathcal{O}(\log n_{1}+d),\;M_{s}=\mathcal{O}\left(d^{d/2}n_{1}^{-\frac {d+2}{d+6}}(\log n_{1})^{d/2}\right),\;J_{s}=\mathcal{O}\left(d^{d/2}n_{1}^{- \frac{d+2}{d+6}}(\log n_{1})^{d/2+1}\right)\]

Moreover, \(\mathcal{S}\) is also Lipschitz with respect to \((x,y)\) and the Lipschitz coefficient is \(c_{\mathrm{lip}}=\mathcal{O}\left(10dC_{\mathrm{lip}}\right)\).

Remark.The proof is provided in Appendix G.2. Here we correct a typo on \(n_{1}^{-\frac{1}{3(d+5)}}\) in main paper Section 4.3. Quantities \(\mathcal{T}(P(x|\widehat{y}=a),P_{x};\mathcal{F})\) and \(\mathcal{T}(P(x,\widehat{y}=a),P_{xy};\mathcal{\bar{S}})\) depend on \(a\) characterizing the distribution shift. The \(\delta(n)\) terms account for the unbounded domain of \(x\), which is negligible when \(n\) is large. In the main paper, we omit \(\delta(n)\) in the regret bound.

Omitted Proofs in Section F

### Conditional Score Decomposition and Score Matching Error

**Lemma G.1**.: Under Assumption 4.1, F.2 and F.3, with high probability

\[\frac{1}{T-t_{0}}\int_{t_{0}}^{T}\|\widehat{s}(\cdot,t)-\nabla\log p_{t}(\cdot) \|_{L^{2}(P_{t})}^{2}\mathrm{d}t\leq\epsilon_{diff}^{2}(n_{1})\]

with \(\epsilon_{diff}^{2}(n_{1})=\widetilde{\mathcal{O}}\left(\frac{1}{t_{0}}\left(n _{1}^{-\frac{2-2\delta(n_{1})}{d+6}}+Dn_{1}^{-\frac{d+4}{d+6}}\right)\right)\) for \(\delta(n_{1})=\frac{d\log\log n_{1}}{\log n_{1}}\).

Proof.: [8, Theorem 1] is easily adapted here to prove Lemma G.1 with the input dimension \(d+1\) and the Lipschitzness in Assumption F.3. Network size of \(\mathcal{S}\) is implied by [8, Theorem 1] with \(\epsilon=n_{1}^{-\frac{1}{d+6}}\) accounting for the additional dimension of reward \(\widehat{y}\) and then the score matching error follows. 

### Proof of Theorem F.4

Additional Notations:Similar as before, use \(P_{t}^{LD}(z)\) to denote the low-dimensional distribution on \(z\) corrupted by diffusion noise. Formally, \(p_{t}^{LD}(z)=\int\phi_{t}(z^{\prime}|z)p_{z}(z)\mathrm{d}z\) with \(\phi_{t}(\cdot|z)\) being the density of \(\mathsf{N}(\alpha(t)z,h(t)I_{d})\). \(P_{t_{0}}^{LD}(z\mid\widehat{f}(Az)=a)\) the corresponding conditional distribution on \(\widehat{f}(Az)=a\) at \(t_{0}\), with shorthand as \(P_{t_{0}}^{LD}(a)\). Also give \(P_{z}(z\mid\widehat{f}(Az)=a)\) a shorthand as \(P^{LD}(a)\).

#### g.2.1 \(\mathsf{SubOpt}(\widehat{P}_{a};y^{*}=a)\) Decomposition

By the same argument as in SSD.3.1, we have

\[\mathsf{SubOpt}(\widehat{P}_{a};y^{*}=a) \leq\underbrace{\mathbb{E}_{x\sim P_{a}}\left[\left|f^{*}(x)- \widehat{f}(x)\right|\right]}_{\mathcal{E}_{1}}+\underbrace{\left|\mathbb{E} _{x\sim P_{a}}[g^{*}(x_{\parallel})]-\mathbb{E}_{x\sim\widehat{P}_{a}}[g^{*}( x_{\parallel})]\right|}_{\mathcal{E}_{2}}\] \[\quad+\underbrace{\mathbb{E}_{x\sim\widehat{P}_{a}}[h^{*}(x_{ \perp})]}_{\mathcal{E}_{3}}.\]

#### g.2.2 \(\mathcal{E}_{1}\): Nonparamtric Regression Induced Error

Nonparametric Regression Error of \(\widehat{f}\)Since \(P_{z}\) has a light tail due to Assumption F.2, by union bound and [8, Lemma 16], we have

\[\mathbb{P}(\exists\,x_{i}\text{ with }\|x_{i}\|_{2}>R\text{ for }i=1,\ldots,n_{2})\leq n _{2}\frac{C_{1}d2^{-d/2+1}}{C_{2}\Gamma(d/2+1)}R^{d-2}\exp(-C_{2}R^{2}/2),\]

where \(C_{1},C_{2}\) are constants and \(\Gamma(\cdot)\) is the Gamma function. Choosing \(R=\mathcal{O}(\sqrt{d\log d+\log\frac{m}{\delta}})\) ensures \(\mathbb{P}(\exists\,x_{i}\text{ with }\|x_{i}\|_{2}>R\text{ for }i=1,\ldots,n_{2})<\delta\). On the event \(\mathcal{E}=\{\|x_{i}\|_{2}\leq R\text{ for all }i=1,\ldots,n_{2}\}\), denoting \(\delta(n_{2})=\frac{d\log\log n_{2}}{\log n_{2}}\), we have

\[\|f^{*}-\widehat{f}\|_{L^{2}}^{2}=\widetilde{\mathcal{O}}\left(n_{2}^{-\frac{2 (\alpha-\delta(n_{2}))}{d+2\alpha}}\right)\]

by [31, Theorem 7] with a new covering number of \(\mathcal{S}\), when \(n_{2}\) is sufficiently large. The corresponding network architecture follows from Theorem 2 in "Nonparametric Regression on Low-Dimensional Manifolds using Deep ReLU Networks : Function Approximation and Statistical Recovery".

We remark that linear subspace is a special case of low Minkowski dimension. Moreover, \(\delta(n_{2})\) is asymptotically negligible and accounts for the truncation radius \(R\) of \(x_{i}\)'s (see also [8, Theorem 2 and 3]). The covering number of \(\mathcal{S}\) is \(\widetilde{\mathcal{O}}\left(d^{d/2}n^{-\frac{d}{\alpha}}(\log n_{2})^{d/2}+Dd\right)\) as appear in [8, Proof of Theorem2]. Therefore

\[\mathbb{E}_{x\sim P_{a}}\left[\left|f^{*}(x)-\widehat{f}(x)\right| \right] \leq\sqrt{\mathbb{E}_{x\sim P_{a}}\left[\left|f^{*}(x)-\widehat{f}( x)\right|^{2}\right]}\] \[\leq\sqrt{\mathcal{T}(P(x|\widehat{y}=a),P_{x};\widetilde{ \mathcal{F}})\cdot\|f^{*}-\widehat{f}\|_{L^{2}}^{2}}\] \[=\sqrt{\mathcal{T}(P(x|\widehat{y}=a),P_{x};\widetilde{\mathcal{ F}})}\cdot\widetilde{\mathcal{O}}\left(n_{2}^{-\frac{\alpha-\delta(n_{2})}{2 \alpha+d}}+D/n_{2}\right).\]

#### g.2.3 \(\mathcal{E}_{2}\): Diffusion Induced On-support Error

Suppose \(L_{2}\) score matching error is \(\epsilon_{diff}^{2}(n_{1})\), i.e.

\[\frac{1}{T-t_{0}}\int_{t_{0}}^{T}\mathbb{E}_{x,\widehat{f}}\|\nabla_{x}\log p _{t}(x,\widehat{f})-s_{\widehat{w}}(x,\widehat{f},t)\|_{2}^{2}\mathrm{d}t \leq\epsilon_{diff}^{2}(n_{1}),\]

We revoke Definition D.2 measuring the distance between \(\widehat{P}_{a}\) to \(P_{a}\) that

\[TV(\widehat{P}_{a}):=\mathsf{d}_{\mathrm{TV}}\left(P_{t_{0}}^{LD}(z\mid \widehat{f}(Az)=a),(U^{\top}V^{\top})_{\#}\widehat{P}_{a}\right).\]

Lemma D.4 applies to nonparametric setting, so we have

\[(I_{D}-VV^{\top})x\sim\mathsf{N}(0,\Lambda),\quad\Lambda\prec ct _{0}I_{D},\] (G.1) \[\angle(V,A)=\widetilde{\mathcal{O}}\left(\frac{t_{0}}{c_{0}}\cdot \epsilon_{diff}^{2}(n_{1})\right).\] (G.2)

In addition,

\[TV(\widehat{P}_{a})=\widetilde{\mathcal{O}}\left(\sqrt{\frac{\mathcal{T}(P(x,\widehat{y}=a),P_{x\widehat{y}};\mathcal{S})}{c_{0}}}\cdot\epsilon_{diff}(n_ {1})\right).\] (G.3)

\(\mathcal{E}_{2}\) will be bounded by

\[\mathcal{E}_{2} =\left|\mathbb{E}_{x\sim P_{a}}[g^{*}(x)]-\mathbb{E}_{x\sim \widehat{P}_{a}}[g^{*}(x)]\right|\] \[\leq\left|\mathbb{E}_{x\sim P_{a}}[g^{*}(AA^{\top}x)]-\mathbb{E} _{x\sim\widehat{P}_{a}}[g^{*}(VV^{\top}x)]\right|+\left|\mathbb{E}_{x\sim \widehat{P}_{a}}[g^{*}(VV^{\top}x)-g^{*}(AA^{\top}x)]\right|,\]

where for \(\left|\mathbb{E}_{x\sim\widehat{P}_{a}}[g^{*}(VV^{\top}x)-g^{*}(AA^{\top}x)]\right|\), we have

\[\left|\mathbb{E}_{x\sim\widehat{P}_{a}}[g^{*}(VV^{\top}x)-g^{*}(AA^{\top}x)] \right|\leq\mathbb{E}_{x\sim\widehat{P}_{a}}[\|VV^{\top}x-AA^{\top}x\|_{2}] \leq\|VV^{\top}-AA^{\top}\|_{F}\cdot\mathbb{E}_{x\sim\widehat{P}_{a}}[\|x\|_{ 2}].\] (G.4)

For the other term \(\left|\mathbb{E}_{x\sim P_{a}}[g^{*}(AA^{\top}x)]-\mathbb{E}_{x\sim\widehat{P} _{a}}[g^{*}(VV^{\top}x)]\right|\), we will bound it with \(TV(\widehat{P}_{a})\).

\[\left|\mathbb{E}_{x\sim P_{a}}[g^{*}(AA^{\top}x)]-\mathbb{E}_{x \sim\widehat{P}_{a}}[g^{*}(VV^{\top}x)]\right|\] \[\leq\left|\mathbb{E}_{z\sim\widehat{P}_{t_{0}}(a)}[g^{*}(Az)]- \mathbb{E}_{z\sim(V^{\top})_{\#}\widehat{P}_{a}}[g^{*}(Vz)]\right|+\left| \mathbb{E}_{z\sim\mathbb{P}(a)}[g^{*}(Az)]-\mathbb{E}_{z\sim\mathbb{P}_{t_{0} }(a)}[g^{*}(Az)]\right|\]

Since any \(z\sim\mathbb{P}_{t_{0}}(a)\) can be represented by \(\alpha(t_{0})z+\sqrt{h(t_{0})}u\), where \(z\sim\mathbb{P}(a),u\sim\mathsf{N}(0,I_{d})\), then

\[\mathbb{E}_{z\sim\mathbb{P}_{t_{0}}(a)}[g^{*}(Az)]\] \[=\mathbb{E}_{z\sim\mathbb{P}(a),u\sim\mathsf{N}(0,I_{d})}[g^{*}( \alpha(t)Az+\sqrt{h(t)}Au))]\] \[\leq\mathbb{E}_{z\sim\mathbb{P}(a)}[g^{*}(\alpha(t_{0})Az))]+ \sqrt{h(t_{0})}\mathbb{E}_{u\sim\mathsf{N}(0,I_{d})}[\|Au\|_{2}]\] \[\leq\mathbb{E}_{z\sim\mathbb{P}(a)}[g^{*}(Az))]+(1-\alpha(t_{0})) \mathbb{E}_{z\sim\mathbb{P}(a)}[\|Az\|_{2}]+\sqrt{h(t_{0})}\mathbb{E}_{u\sim \mathsf{N}(0,I_{d})}[\|Au\|_{2}],\]thus

\[\left|\mathbb{E}_{z\sim\mathbb{P}_{t_{0}}(a)}[g^{*}(Az)]-\mathbb{E}_{z\sim\mathbb{ P}_{t_{0}}(a)}[g^{*}(Az)]\right|\leq t_{0}\cdot\mathbb{E}_{z\sim\mathbb{P}(a)}[\|z\|_{2}]+d,\]

where we further use \(1-\alpha(t_{0})=1-e^{-t_{0}/2}\leq t_{0}/2\), \(h(t_{0})\leq 1\).

As for \(\left|\mathbb{E}_{z\sim\mathbb{P}_{t_{0}}(a)}[g^{*}(Az)]-\mathbb{E}_{z\sim(V^{ \top})_{\#}\widehat{P}_{a}}[g^{*}(Vz)]\right|\), we have

\[\left|\mathbb{E}_{z\sim\mathbb{P}_{t_{0}}(a)}[g^{*}(Az)]-\mathbb{E }_{z\sim(U^{\top}V^{\top})_{\#}\widehat{P}_{a}}[g^{*}(VUz)]\right|\] \[= \left|\mathbb{E}_{z\sim\mathbb{P}_{t_{0}}(a)}[g^{*}(VUz)]-\mathbb{ E}_{z\sim(U^{\top}V^{\top})_{\#}\widehat{P}_{a}}[g^{*}(VUz)]\right|+\left| \mathbb{E}_{z\sim\mathbb{P}_{t_{0}}(a)}[g^{*}(Az)]-\mathbb{E}_{z\sim\mathbb{ P}_{t_{0}}(a)}[g^{*}(VUz)]\right|,\]

where

\[\left|\mathbb{E}_{z\sim\mathbb{P}_{t_{0}}(a)}[g^{*}(Az)]-\mathbb{E}_{z\sim \mathbb{P}_{t_{0}}(a)}[g^{*}(VUz)]\right|\leq\|A-VU\|_{F}\cdot\mathbb{E}_{z \sim\mathbb{P}_{t_{0}}(a)}[\|z\|_{2}],\]

and

\[\left|\mathbb{E}_{z\sim\mathbb{P}_{t_{0}}(a)}[g^{*}(VUz)]-\mathbb{E}_{z\sim(U ^{\top}V^{\top})_{\#}\widehat{P}_{a}}[g^{*}(VUz)]\right|\leq TV(\widehat{P}_ {a})\cdot\|g^{*}\|_{\infty}.\]

Combining things up, we have

\[\mathcal{E}_{2}\leq \|VV^{\top}-AA^{\top}\|_{F}\cdot\mathbb{E}_{x\sim\widehat{P}_{a}} [\|x\|_{2}]+\|A-VU\|_{F}\cdot\mathbb{E}_{z\sim\widehat{P}_{t_{0}}(a)}[\|z\|_{ 2}]\] \[+t_{0}\cdot\mathbb{E}_{z\sim\mathbb{P}(a)}[\|z\|_{2}]+d+TV( \widehat{P}_{a})\cdot\|g^{*}\|_{\infty}.\]

Similar to parametric case, Let \(M(a):=\mathbb{E}_{z\sim\mathbb{P}(a)}[\|z\|_{2}^{2}]\), then

\[\mathbb{E}_{z\sim\mathbb{P}_{t_{0}}(a)}[\|z\|_{2}^{2}]\leq M(a)+t_{0}d,\]

expect for in nonparametric case, we can not compute \(M(a)\) out as it is not Gaussian. But still, with higher-order terms in \(n_{1}^{-1}\) hided, we have

\[\mathcal{E}_{2} =\mathcal{O}\left(TV(\widehat{P}_{a})\cdot\|g^{*}\|_{\infty}+t_{0 }M(a)\right)\] \[=\widetilde{\mathcal{O}}\left(\sqrt{\frac{\mathcal{T}(P(x,\widehat {y}=a),P_{x\widehat{y}};\widetilde{\mathcal{S}})}{c_{0}}}\cdot\epsilon_{diff }(n_{1})\cdot\|g^{*}\|_{\infty}+t_{0}M(a)\right).\]

## Appendix H Parametric Conditional Score Estimation: Proof of Lemma d.1

Proof.: We first derive a decomposition of the conditional score function similar to [8]. We have

\[p_{t}(x,y) =\int p_{t}(x,y|z)p_{z}(z)\mathrm{d}z\] \[=\int p_{t}(x|z)p(y|z)p_{z}(z)\mathrm{d}z\] \[=C\int\exp\left(-\frac{1}{2h(t)}\|x-\alpha(t)Az\|_{2}^{2}\right) \exp\left(-\frac{1}{\sigma_{y}^{2}}\left(\theta^{\top}z-y\right)^{2}\right)p_ {z}(z)\mathrm{d}z\] \[\stackrel{{(i)}}{{=}}C\exp\left(-\frac{1}{2h(t)}\|(I_ {D}-AA^{\top})x\|_{2}^{2}\right)\] \[\quad\cdot\int\exp\left(-\frac{1}{2h(t)}\|A^{\top}x-\alpha(t)z\|_ {2}^{2}\right)\exp\left(-\frac{1}{\sigma_{y}^{2}}\left(\theta^{\top}z-y\right)^{ 2}\right)p_{z}(z)\mathrm{d}z,\]

where equality \((i)\) follows from the fact \(AA^{\top}x\perp(I_{D}-AA^{\top})x\) and \(C\) is the normalizing constant of Gaussian densities. Taking logarithm and then derivative with respect to \(x\) on \(p_{t}(x,y)\), we obtain

\[\nabla_{x}\log p_{t}(x,y)\] \[=\frac{\alpha(t)}{h(t)}\frac{A\int z\exp\left(-\frac{1}{2h(t)}\| A^{\top}x-\alpha(t)z\|_{2}^{2}\right)\exp\left(-\frac{1}{\sigma_{y}^{2}} \left(\theta^{\top}z-y\right)^{2}\right)p_{z}(z)\mathrm{d}z}{\int\exp\left(- \frac{1}{2h(t)}\|A^{\top}x-\alpha(t)z\|_{2}^{2}\right)\exp\left(-\frac{1}{ \sigma_{y}^{2}}\left(\theta^{\top}z-y\right)^{2}\right)p_{z}(z)\mathrm{d}z}- \frac{1}{h(t)}x.\]Note that the first term in the right-hand side above only depends on \(A^{\top}x\) and \(y\). Therefore, we can compactly write \(\nabla_{x}\log p_{t}(x,y)\) as

\[\nabla_{x}\log p_{t}(x,y)=\frac{1}{h(t)}Au(A^{\top}x,y,t)-\frac{1}{h(t)}x,\] (H.1)

where mapping \(u\) represents

\[\frac{\alpha(t)\int z\exp\left(-\frac{1}{2h(t)}\|A^{\top}x-\alpha(t)z\|_{2}^{ 2}\right)\exp\left(-\frac{1}{\sigma_{z}^{2}}\left(\theta^{\top}z-y\right)^{2} \right)p_{z}(z)\mathrm{d}z}{\int\exp\left(-\frac{1}{2h(t)}\|A^{\top}x-\alpha(t )z\|_{2}^{2}\right)\exp\left(-\frac{1}{\sigma_{y}^{2}}\left(\theta^{\top}z-y \right)^{2}\right)p_{z}(z)\mathrm{d}z}.\]

We observe that (H.1) motivates our choice of the neural network architecture \(\mathcal{S}\) in (3.8). In particular, \(\psi\) attempts to estimate \(u\) and matrix \(V\) attempts to estimate \(A\).

In the Gaussian design case (Assumption 4.4), we instantiate \(p_{z}(z)\) to the Gaussian density \((2\pi|\Sigma|)^{-d/2}\exp\left(-\frac{1}{2}z^{\top}\Sigma^{-1}z\right)\). Some algebra on the Gaussian integral gives rise to

\[\nabla_{x}\log p_{t}(x,y) =\frac{\alpha(t)}{h(t)}AB_{t}\mu_{t}(x,y)-\frac{1}{h(t)}(I_{D}-AA ^{\top})x-\frac{1}{h(t)}AA^{\top}x\] \[=\frac{\alpha(t)}{h(t)}AB_{t}\left(\alpha(t)A^{\top}x+\frac{h(t)} {\nu^{2}}y\theta\right)-\frac{1}{h(t)}x,\] (H.2)

where we have denoted

\[\mu_{t}(x,y)=\alpha(t)A^{\top}x+\frac{h(t)}{\nu^{2}}y\theta\quad\text{and} \quad B_{t}=\left(\alpha^{2}(t)I_{d}+\frac{h(t)}{\nu^{2}}\theta\theta^{\top}+h (t)\Sigma^{-1}\right)^{-1}.\]

Score Estimation ErrorRecall that we estimate the conditional score function via minimizing the denoising score matching loss in Proposition 3.1. To ease the presentation, we denote

\[\ell(x,y;s)=\frac{1}{T-t_{0}}\int_{t_{0}}^{T}\mathbb{E}_{x^{\prime}|x}\|\nabla _{x^{\prime}}\log\phi_{t}(x^{\prime}|x)-s(x^{\prime},y,t)\|_{2}^{2}\mathrm{d}t\]

as the loss function for a pair of clean data \((x,y)\) and a conditional score function \(s\). Further, we denote the population loss as

\[\mathcal{L}(s)=\mathbb{E}_{x,y}[\ell(x,y;s)],\]

whose empirical counterpart is denoted as \(\widehat{\mathcal{L}}(s)=\frac{1}{n_{1}}\sum_{i=1}^{n_{1}}\ell(x_{i},y_{i};s)\).

To bound the score estimation error, we begin with an oracle inequality. Denote \(\mathcal{L}^{\mathrm{trunc}}(s)\) as a truncated loss function defined as

\[\mathcal{L}^{\mathrm{trunc}}(s)=\mathbb{E}[\ell(x,y;s)\mathds{1}\{\|x\|_{2} \leq R,|y|\leq R\}],\]

where \(R>0\) is a truncation radius chosen as \(\mathcal{O}(\sqrt{d\log d+\log K+\log\frac{n_{1}}{\delta}})\). Here \(K\) is a uniform upper bound of \(s(x,y,t)\mathds{1}\{\|x\|_{2}\leq R,|y|\leq R\}\) for \(s\in\mathcal{S}\), i.e., \(\sup_{s\in\mathcal{S}}\|s(x,y,t)\mathds{1}\{\|x\|_{2}\leq R,|y|\leq R\}\|_{2}\leq K\). To this end, we have

\[\mathcal{L}(\widehat{s}) =\mathcal{L}(\widehat{s})-\widehat{\mathcal{L}}(\widehat{s})+ \widehat{\mathcal{L}}(\widehat{s})\] \[=\mathcal{L}(\widehat{s})-\widehat{\mathcal{L}}(\widehat{s})+ \inf_{s\in\mathcal{S}}\widehat{\mathcal{L}}(s)\] \[\stackrel{{(i)}}{{=}}\mathcal{L}(\widehat{s})- \widehat{\mathcal{L}}(\widehat{s})\] \[\leq\mathcal{L}(\widehat{s})-\mathcal{L}^{\mathrm{trunc}}( \widehat{s})+\mathcal{L}^{\mathrm{trunc}}(\widehat{s})-\widehat{\mathcal{L}}^{ \mathrm{trunc}}(\widehat{s})\] \[\leq\underbrace{\sup_{s}\mathcal{L}^{\mathrm{trunc}}(s)-\widehat{ \mathcal{L}}^{\mathrm{trunc}}(s)}_{(A)}+\underbrace{\sup_{s}\mathcal{L}(s)- \mathcal{L}^{\mathrm{trunc}}(s)}_{(B)},\]

where equality \((i)\) holds since \(\mathcal{S}\) contains the ground truth score function. We bound term \((A)\) by a PAC-learning concentration argument. Using the same argument in [8, Theorem 2, term \((A)\)], we have

\[\sup_{s\in\mathcal{S}}\ell^{\mathrm{trunc}}(x,y;s)=\mathcal{O}\left(\frac{1} {t_{0}(T-t_{0})}(K^{2}+R^{2})\right).\]Applying the standard metric entropy and symmetrization technique, we can show

\[(A)=\mathcal{O}\left(\widehat{\mathfrak{R}}(\mathcal{S})+\left(\frac{K^{2}+R^{2}}{ t_{0}(T-t_{0})}\right)\sqrt{\frac{\log\frac{2}{\delta}}{2n_{1}}}\right),\]

where \(\widehat{\mathfrak{R}}\) is the empirical Rademacher complexity of \(\mathcal{S}\). Unfamiliar readers can refer to Theorem 3.3 in "Foundations of Machine Learning", second edition for details. The remaining step is to bound the Rademacher complexity by Dudley's entropy integral. Indeed, we have

\[\widehat{\mathfrak{R}}(\mathcal{S})\leq\inf_{\epsilon}\frac{4 \epsilon}{\sqrt{n_{1}}}+\frac{12}{n_{1}}\int_{\epsilon}^{K^{2}\sqrt{n_{1}}} \sqrt{\mathcal{N}(\mathcal{S},\epsilon,\|\cdot\|_{2})}\mathrm{d}\epsilon.\]

We emphasize that the log covering number considers \(x,y\) in the truncated region. Taking \(\epsilon=\frac{1}{n_{1}}\) gives rise to

\[(A)=\mathcal{O}\left(\left(\frac{K^{2}+R^{2}}{t_{0}(T-t_{0})} \right)\sqrt{\frac{\mathcal{N}(\mathcal{S},1/n_{1})\log\frac{1}{\delta}}{n_{1 }}}\right).\]

Here \(K\) is instance dependent and majorly depends on \(d\). In the Gaussian design case, we can verify that \(K\) is \(\mathcal{O}(\sqrt{d})\). To this end, we deduce \((A)=\widetilde{\mathcal{O}}\left(\frac{1}{t_{0}}\sqrt{d^{2}\frac{\mathcal{N}( \mathcal{S},1/n_{1})\log\frac{1}{\delta}}{n_{1}}}\right)\). In practice, \(d\) is often much smaller than \(D\) (see for example [37], where ImageNet has intrinsic dimension no more than \(43\) in contrast to image resolution of \(224\times 224\times 3\)). In this way, we can upper bound \(d^{2}\) by \(D\), yet \(d^{2}\) is often a tighter upper bound.

For term \((B)\), we invoke the same upper bound in [8, Theorem 2, term \((B)\)] to obtain

\[(B)=\mathcal{O}\left(\frac{1}{n_{1}t_{0}(T-t_{0})}\right),\]

which is negligible compared to \((A)\). Therefore, summing up \((A)\) and \((B)\), we deduce

\[\epsilon_{diff}^{2}=\mathcal{O}\left(\frac{1}{t_{0}}\sqrt{\frac{ \mathcal{N}(\mathcal{S},1/n_{1})(d^{2}\lor D)\log\frac{1}{\delta}}{n_{1}}} \right).\]

Gaussian DesignWe only need to find the covering number under the Gaussian design case. Using (H.2), we can construct a covering from coverings on matrices \(V\) and \(\Sigma^{-1}\). Suppose \(V_{1},V_{2}\) are two matrices with \(\|V_{1}-V_{2}\|_{2}\leq\eta_{V}\) for some \(\eta>0\). Meanwhile, let \(\Sigma_{1}^{-1},\Sigma_{2}^{-1}\) be two covariance matrices with \(\|\Sigma_{1}^{-1}-\Sigma_{2}^{-1}\|_{2}\leq\eta_{\Sigma}\). Then we bound

\[\sup_{\|x\|_{2}\leq R,\|y\|\leq R}\|s_{V_{1},\Sigma_{1}^{-1}}(s,y,t)-s_{V_{2},\Sigma_{2}^{-1}}(x,y,t)\|_{2}\] \[\leq\frac{1}{h(t)}\sup_{\|x\|_{2}\leq R,\|y\|\leq R}\left[\big{\|} V_{1}\psi_{\Sigma_{1}^{-1}}(V_{1}^{\top}x,y,t)-V_{1}\psi_{\Sigma_{1}^{-1}}(V_{2}^{ \top}x,y,t)\big{\|}_{2}\right.\] \[\left.\quad+\big{\|}V_{1}\psi_{\Sigma_{1}^{-1}}(V_{2}^{\top}x,y,t )-V_{1}\psi_{\Sigma_{2}^{-1}}(V_{2}^{\top}x,y,t)\big{\|}_{2}\right.\] \[\leq\frac{1}{h(t)}\left(2R\eta_{V}+2\nu^{-2}R\eta_{\Sigma}\right),\]

where for bounding \((\blacktriangle)\), we invoke the identity \(\|(I+A)^{-1}-(I+B)^{-1}\|_{2}\leq\|B-A\|_{2}\). Further taking supremum over \(t\in[t_{0},T]\) leads to

\[\sup_{\|x\|_{2}\leq R,|y|\leq R}\|s_{V_{1},\Sigma_{1}^{-1}}(s,y,t )-s_{V_{2},\Sigma_{2}^{-1}}(x,y,t)\|_{2}\leq\frac{1}{t_{0}}\left(2R\eta_{V}+2 \nu^{-2}R\eta_{\Sigma}\right)\]for any \(t\in[t_{0},T]\). Therefore, the inequality above suggests that coverings on \(V\) and \(\Sigma^{-1}\) form a covering on \(\mathcal{S}\). The covering numbers of \(V\) and \(\Sigma^{-1}\) can be directly obtained by a volume ratio argument; we have

\[\mathcal{N}(V,\eta_{V},\|\cdot\|_{2})\leq Dd\log\left(1+\frac{2\sqrt{d}}{\eta_{ V}}\right)\quad\text{and}\quad\mathcal{N}(\Sigma^{-1},\eta_{\Sigma},\|\cdot\|_{2}) \leq d^{2}\log\left(1+\frac{2\sqrt{d}}{\lambda_{\min}\eta_{\Sigma}}\right).\]

Thus, the log covering number of \(\mathcal{S}\) is

\[\mathcal{N}(\mathcal{S},\eta,\|\cdot\|_{2}) =\mathcal{N}(V,t_{0}\eta_{V}/2R,\|\cdot\|_{2})+\mathcal{N}(\Sigma^ {-1},t_{0}\nu^{2}\eta_{\Sigma}/2R,\|\cdot\|_{2})\] \[\leq(Dd+d^{2})\log\left(1+\frac{dD}{t_{0}\lambda_{\min}\eta} \right),\]

where we have plugged \(\nu^{2}=1/D\) into the last inequality. Setting \(\eta=1/n_{1}\) and substituting into \(\epsilon_{diff}^{2}\) yield the desired result.

We remark that the analysis here does not try to optimize the error bounds, but aims to provide a provable guarantee for conditional score estimation using finite samples. We foresee that sharper analysis via Bernstein-type concentration may result in a better dependence on \(n_{1}\). Nonetheless, the optimal dependence should not beat a \(1/n_{1}\)-rate. 

## Appendix I Additional Experimental Results

### Simulation

We generate the latent sample \(z\) from standard normal distribution \(z\sim\mathsf{N}(\mathsf{0},\mathsf{l}_{\mathsf{d}})\) and set \(x=Az\) for a randomly generated orthonormal matrix \(A\in\mathbb{R}^{D\times d}\). The dimensions are set to be \(d=16,D=64\). The reward function is set to be \(f(x)=(\theta^{\star})^{\top}x_{\|}+5\|x_{\perp}\|_{2}^{2}\), where \(\theta^{\star}\) is defined by \(A\beta^{\star}\). We generate \(\beta^{\star}\) by uniformly sampling from the unit sphere.

When estimating \(\widehat{\theta}\), we set \(\lambda=1.0\). The score matching network is based on the UNet implementation from https://github.com/lucidrains/denoising-diffusion-pytorch, where we modified the class embedding so it accepts continuous input. The predictor is trained using \(8192\) samples and the score function is trained using \(65536\) samples. When training the score function, we choose Adam as the optimizer with learning rate \(8\times 10^{-5}\). We train the score function for \(10\) epochs, each epoch doing a full iteration over the whole training dataset with batch size \(32\).

For evaluation, the statistics is computed using \(2048\) samples generated from the diffusion model. The curve in the figures is computed by averaging over \(5\) runs.

### Directed Text-to-Image Generation

**Samples of high rewards and low rewards from the ground-truth reward model.** In Section 5.2, the ground-truth reward model is built by replacing the final prediction layer of the ImageNet pretrained ResNet-18 model with a randomly initialized linear layer of scalar outputs. To investigate the meaning of this randomly-generated reward model, we generate images using Stable Diffusion and filter out images with rewards \(\geq 0.4\) (positive samples) and rewards \(\leq-0.4\) (negative samples) and pick two typical images for each; see Figure 7. We note that in real-world use cases, the ground-truth rewards are often measured and annotated by human labors according to the demands.

**Training Details.** In our implementation, as the Stable Diffusion model operates on the latent space of its VAE, we build a 3-layer ConvNet with residual connections and batch normalizations on top of the VAE latent space. We train the network using Adam optimizer with learning rate \(0.001\) for 100 epochs.

### Decision-Diffuser [1]

We replicate the results in Decision Diffuser [1] under (Med-Expert, Hopper) setting. In Decision Diffuser, the RL trajectory and the final reward are jointly modeled with a conditioned diffusion model. The policy is given by first performing reward-directed conditional diffusion to sample a trajectory of high reward and then extracting action sequences from the trajectory using a trained inverse dynamics model. We plot the mean and standard deviation of the total returns (averaged across 10 independent episodes) v.s. the target rewards in Figure 8. The theoretical maximum of the reward is 400. Therefore, trajectories with rewards greater than 400 are never seen during training. We observe that when we increase the target reward beyond 320, the actual total reward decreases. According to our theory, as we increase the reward guidance signal, the condition effect becomes stronger but the distribution-shift effect also becomes stronger.

Figure 8: Total Reward v.s. Target Reward for Decision Diffuser (Med-Expert, Hopper) setting.

Figure 7: Random samples with high rewards and low rewards.