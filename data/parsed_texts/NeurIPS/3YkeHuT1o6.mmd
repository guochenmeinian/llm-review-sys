# A Swiss Army Knife for Heterogeneous Federated Learning: Flexible Coupling via Trace Norm

 Tianchi Liao, Lele Fu, Jialong Chen, Zhen Wang, Zibin Zheng, Chuan Chen

Sun Yat-sen University, Guangzhou, China

{liaotch, fulle, chenjlong}@mail2.sysu.edu.cn

{wangzh665, zhzibin, chenchuan}@mail.sysu.edu.cn

Corresponding author

###### Abstract

The heterogeneity issue in federated learning (FL) has attracted increasing attention, which is attempted to be addressed by most existing methods. Currently, due to systems and objectives heterogeneity, enabling clients to hold models of different architectures and tasks of different demands has become an important direction in FL. Most existing FL methods are based on the homogeneity assumption, namely, different clients have the same architectural models with the same tasks, which are unable to handle complex and multivariate data and tasks. To flexibly address these heterogeneity limitations, we propose a novel federated multi-task learning framework with the help of tensor trace norm, FedSAK. Specifically, it treats each client as a task and splits the local model into a feature extractor and a prediction head. Clients can flexibly choose shared structures based on heterogeneous situations and upload them to the server, which learns correlations among client models by mining model low-rank structures through tensor trace norm. Furthermore, we derive convergence and generalization bounds under non-convex settings. Evaluated on 6 real-world datasets compared to 13 advanced FL models, FedSAK demonstrates superior performance.

## 1 Introduction

Federated learning (FL) is an effective machine learning approach that enables decentralized computations and data resources [1]. It is regarded as a promising distributed and privacy-preserving method. A common challenge in FL is the data heterogeneity problem, in particular when the diversity of client data distribution increases, the generalization error of the global model increases significantly as well [2]. Therefore, to address data heterogeneity, common personalized federated learning (pFL) methods learn a personalized model for each client in addition to the global model [3; 4; 5].

However, current research indicates that the fundamental bottleneck in executing pFL across heterogeneous clients is the misassumption of one global model can fit all clients [6]. Instead, we should focus on exploring intrinsic collaborations across clients to obtain better local models. Unlike pFL, the goal of federated multi-task learning (FMTL) is to simultaneously learn separate models, where each model caters to the heterogeneous needs of each client [7]. Thus, FMTL directly addresses the issues stemming from client heterogeneity without constructing a global model [8].

Moreover, most of the existing FL methods require all clients to train models with the same architecture (i.e., model homogeneity) [9; 10; 11]. In practical heterogeneous FL scenarios, besides data heterogeneity, model heterogeneity and task heterogeneity are also present due to varying hardware, computational capabilities, and requirements across clients [12; 13]. Although some FL methods to handle model heterogeneity have emerged [14; 15; 16], many of them resort to knowledge distillationtechniques that necessitate public datasets closely aligned with the learning objectives [17; 18]. This incurs high communications and computational costs, limiting model performance. Alternatively, some strategies utilize prototype models linked to labels [15; 16], rendering them futile when client tasks are related but inconsistent.

Currently, task heterogeneity is rarely mentioned in federated settings, however, it is widespread in real-world scenarios [19]. For example, given the same batch of portrait samples for different clients, some clients may want to predict person's age (Task 1: Multi-Class Classification or Regression), while others may need to recognize gender (Task 2: Binary Classification). In such scenario, existing methods are ill-equipped to handle task heterogeneity. Effective algorithms that can overcome data, model, and task heterogeneity in federated settings remain largely underdeveloped.

In light of the heterogeneity in FL and limitations of existing techniques, we adopt FMTL to address complex heterogeneous FL. A key issue in FMTL is how to design priors such that knowledge obtained from each client can be shared by others. Corinzia _et al._[20] built connections among client tasks using approximate variational inference, while Dinh _et al._[8] proposed a Laplace regularization-based FMTL that only considers grouping similarities among different client tasks. To effectively utilize correlations across clients, Kumar _et al._[21] proposed a method leveraging low-dimensional subspaces shared by multiple tasks, which was shown effective but limited to linear models.

Imposing low-rank constraints on model parameters is effective when learning objectives are correlated among clients [22]. Trace norm has been proposed as a solution to uncover potential connections among model parameters of different objectives. Thus, we propose a novel and flexible FMTL framework based on tensor trace norm, **FedSAK**, which like a **S**wiss **A**rmy **K**nife provides flexible aggregation choices for heterogeneous FL. Specifically, we split each client model into a feature extractor and a prediction head, allowing our model can adaptively define certain parts as global shared layers for different heterogeneity settings and upload them to the server. The server aggregates the global shared layers into a tensor and applies trace norm regularization to induce a low-rank structure. In this way, inter-dependencies are created among different client models to reflect across clients' intrinsic connections about their model parameters. In summary, our main contributions are:

* FedSAK is an FMTL algorithm that simultaneously considers data heterogeneity, model heterogeneity and task heterogeneity. It is more flexible than most existing FL methods.
* We employ tensor trace norm to exploit low-rank structure for identifying relationships among client models.
* We theoretically derive convergence guarantees for FedSAK under non-convex settings, and establish generalization bounds for the proposed tensor trace norm minimizer.
* We conduct extensive experiments on 13 advanced methods over 6 datasets to demonstrate the flexibility and efficacy of FedSAK. Results show that FedSAK outperforms baselines in handling heterogeneous federated scenarios.

## 2 Related Works

### Heterogeneous Federated Learning

**Data Heterogeneity**, is one of the most significant challenges in FL [23]. Initial methods like FedProx [2] added a proximal term to the local training objective to keep updated parameters close to the original downloaded model. MOON [9] employed contrastive loss to improve representation learning. Additionally, various personalized models have been proposed to train specialized components using globally shared information, including fine-tuning methods like FedRep [24], FedPer [25]; regularization-based methods such as FedMTL [7], pFedMe [4], and Ditto [5]; meta-learning methods like Per-FedAvg [3]; and methods decoupling feature extractors and classifiers like GPFL [10], FedCP [26]. Moreover, the clustered FL has also been explored by partitioning clients into multiple groups or clusters for clustered local models to provide multiple global models [27]. However, the development of existing data heterogeneity methods is constrained by homogenous model assumptions.

**Model Heterogeneity** presents another major challenge in FL. Researchers often employ FL based on knowledge distillation as an alternative solution. FedMD [17] have clients compute logits on a public dataset using locally trained heterogeneous models, which are then uploaded to the server. FedDF [28] and FedKT [29] train each client's heterogeneous model on a shared public dataset at the server via distillation. However, obtaining shared datasets with similar data distributions required by these methods may not always be feasible in a practical setting. Especially when the public dataset is large-scale, the computational costs of such methods can increase substantially, limiting their applicability. Additionally, there are FL methods of model heterogeneous that employ aggregated logit or representation matching losses by class to train local models e.g., FedProto [15] and FedGH [16]. However, these methods impose higher computational costs on clients. In addition, each client can only acquire knowledge of known categories from the server, restricting generalization to unseen categories as well as possibilities for task heterogeneity.

**Task Heterogeneity** is an often overlooked issue in federated settings where tasks may have varying numbers of outputs in practice. Huang _et al._[30] proposed a model for multi-lingual speech processing with each task corresponding to an individual language. Zhang _et al._[31] employed DNNs for facial landmark localization and face attribute recognition. However, these task-heterogeneous methods have not been considered for FL. Yao _et al._[32] first generalized traditional FL to federated heterogeneous task learning, but they did not propose a new algorithm and merely considered data and task heterogeneity.

### Federated Multi-Task Learning

Multi-task learning (MTL) aims to process all task data with identical distributions centrally in a single computing environment [33]. In contrast, FMTL places greater emphasis on data privacy and heterogeneity. FMTL aims to learn separate models tailored to local heterogeneous conditions for each client. FMTL was first introduced in [7], where a system-aware optimization method was proposed that first considered theoretical issues like high communication costs, stragglers, and fault tolerance in FMTL. Yasmin _et al._[34] formulated FMTL network using generalized total variation minimization as a regularizer. Li _et al._[35] adopted FMTL algorithms to handle accuracy, fairness and robustness issues in FL. Corinzia _et al._[20] modeled the FL network as a star-shaped Bayesian network and used approximate variational inference for FMTL. Dinh _et al._[8] utilized Laplace regularization to construct relationships among clients. However, these methods operate under model homogeneity assumptions, and there is scarce analysis of the convergence and error bounds for non-convex FMTL objectives. To effectively utilize correlations across tasks, some MTL methods such as Maurer _et al._[33] established excess risk bounds for MTL based on data distributions. Kumar _et al._[21] proposed a new framework where in-group tasks lie in a low-dimensional subspace. Zhang _et al._[22] employed transformed tensor nuclear norm constraints to capture intrinsic relationships among tasks. However, these methods did not explore federated settings and heterogeneity.

## 3 Notations and Preliminaries

### General Federated Multi-Task Learning

Suppose we have \(M\) clients, where client \(i\) has \(n_{i}\) private data points \(\textbf{x}_{i}\in\mathbb{R}^{d_{k}\times n_{i}}\) with labels \(y_{i}\), and \(N\) denotes the total number of data, i.e., \(N=\sum_{i=1}^{M}n_{i}\). The datasets among the clients are heterogeneous. With the help of a central server, the goal of FMTL is to collaboratively learn

Figure 1: The main framework of FedSAK model. DH denotes “Data Heterogeneity”, MH denotes “Model Heterogeneity”, and TH denotes “Task Heterogeneity”.

individual local models \(\theta_{i}\in\mathbb{R}^{d_{b}\times d_{\theta}}\) for each client's data without exchanging private data. Note that when the model is a shallow network, \(d_{\theta}=1\). Many FMTL problems can be captured by the following general formulation [7]:

\[\min_{\Theta,\Omega}\left\{\sum_{i=1}^{M}\sum_{j=1}^{n_{i}}\mathcal{L}_{i} \left(\mathcal{F}_{i}(\theta_{i};\mathbf{x}_{i}^{j}),y_{i}^{j}\right)+R(\Theta,\Omega)\right\},\] (1)

where \(\Theta:=[\theta_{1},\cdots,\theta_{m}]\in\mathbb{R}^{d_{b}\times d_{\theta} \times M}\) is a collective model stacked by individual clients. \(R(\cdot)\) is a regularizer, and \(\Omega\) is expressed as modeling the relationship among client tasks. FMTL issues vary according to their presuppositions about \(R\), which receives \(\Omega\) as input and promotes some suitable structure amongst the tasks.

### Tensor Trace Norm

In the field of MTL, the trace norm is utilized as a regularization method to learn the low-rank structure among model parameters across all tasks [22]. Typically, when dealing with vectorized data, shallow networks are used, i.e., \(d_{\theta}=1\), where \(\Theta\) is a matrix. Thus, matrix trace norm is employed to enhance dependencies among models, defined as \(\left\|\Theta\right\|_{*}=\sum_{i}\sigma_{i}(\Theta)\), where \(\sigma_{i}(\Theta)\) denotes the \(i\)-th largest singular value of a matrix. However, with the collection of complex data, the data can be a tensor (e.g., images) and the model can become more complex (e.g., deep neural networks). In such scenarios, the parameters for all tasks can be structured into a multi-dimensional tensor, such as a \(p\)-way tensor (\(p\geq 3\)), e.g., \(\Theta\in\mathbb{R}^{d_{1}\times\cdots\times d_{p}}\). For example, in a classification model with a fully connected layer (i.e., \(p=3\)), where \(d_{1}\) represents the dimension of the input, \(d_{2}\) indicating the number of classes, and \(d_{3}=M\) denotes the number of clients. In this context, the traditional matrix trace norm becomes inapplicable, necessitating the use of a tensor trace norm instead.

Currently, the Tucker trace norm, a representative overlapping tensor trace norm, is extensively utilized in deep learning [36]. It unfolds the tensor into a matrix using Tucker decomposition and computes the convex sum for the matrix trace norms of the various flattened tensors [37]. The "unfold" operation along the \(k\)-th mode on a tensor \(\Theta\) is defined as \(\operatorname{unfold}_{k}(\Theta):=\Theta_{(k)}\in\mathbb{R}^{d_{k}\times(d_{ 1}\cdots d_{k-1}d_{k+1}\cdots d_{p})}\). \(\alpha_{k}\) denotes the weight of the matrix unfolded along \(k\)-th mode. Thus, we formulate the Tucker-based trace norm in the following form:

\[\left\|\Theta\right\|_{*}:=\sum_{k=1}^{p}\alpha_{k}\left\|\Theta_{(k)}\right\| _{*},\] (2)

where \(\alpha_{k}\) is the weight on the \(k\)-th mode, here we default to the same weight on each mode. Thus, the computational complexity of the tensor trace norm is \(O(\min_{k}d_{k}^{2}\prod_{i\neq k}^{p}d_{i})\).

## 4 Methodology

In this section, we introduce the multi-task learning framework in a federated environment in more detail and propose a novel method, FedSAK, to address the challenges of FMTL. The framework of FedSAK is displayed in Figure 1.

### Optimization Objective

Without loss of generality, the client model can be decoupled into representation layers, also known as a feature extractor, and final decision layers like a prediction head for classification tasks. Under this design, much research has actively studied collaboration among different layers. However, these methods typically require model homogeneity as a means for server parameter aggregation, which inherently restricts the development of heterogeneous FL. Therefore, our objective in FMTL is to facilitate heterogeneous FL in supervised classification scenarios, encompassing both data heterogeneity (DH), model heterogeneity (MH), and task heterogeneity (TH).

Following previous conventions, we denote the feature extractor as \(f_{i}(\varphi_{i};\mathbf{x}_{i}):\mathbb{R}^{d_{k}}\rightarrow\mathbb{R}^{d_ {H}}\) and the prediction head as \(h_{i}(v_{i};c_{i}):\mathbb{R}^{d_{H}}\rightarrow\mathbb{R}^{d_{y}}\). Thus, the model for client \(i\) can be expressed as \(\mathcal{F}_{i}(\theta_{i})=f_{i}(\varphi_{i})\circ h_{i}(v_{i})\), \(\circ\) denotes concatenation among model components, where \(\theta_{i}\) are the model parameters. We assume \(f_{i}\) and \(h_{i}\) can be heterogeneous across clients, meaning clients can customize the size and architecture of their local feature extractor and prediction head based on available resources.

To flexibly apply our method to different federated heterogeneity settings, we define \(w_{i}\) to represent the global shared layers for client \(i\), which is a subset of \(\theta_{i}\), i.e. \(w_{i}\subseteq\theta_{i}\). The choice of \(w_{i}\) can be flexibly adapted to the heterogeneity setting. For example, with data heterogeneity, \(w_{i}=\theta_{i}=\varphi_{i}\circ v_{i}\); with model heterogeneity, \(w_{i}=v_{i}\); and with task heterogeneity, \(w_{i}=\varphi_{i}\).

Since the global shared layers are aggregated at the server, where a low-rank structure among clients is learned by computing the trace norm to reinforce dependencies among models. Thus the objective for heterogeneous FMTL in Eq. (1) can be reformulated as:

\[\min_{\Theta}\frac{1}{M}\sum_{i=1}^{M}\frac{1}{n_{i}}\sum_{j=1}^{n_{i}} \mathcal{L}(\mathcal{F}_{i}(\Theta;\mathbf{x}_{i}^{j}),y_{i}^{j})+\lambda \left\|\mathcal{W}\right\|_{*},\] (3)

where \(\mathcal{F}_{i}(\theta_{i})=f(\varphi_{i})\circ h(v_{i})\), we use \(\theta_{i}\) to represent \((\varphi_{i},v_{i})\) for short, and \(\Theta=[\theta_{1},\cdots,\theta_{M}]\). \(\mathcal{W}\in\mathbb{R}^{d_{1}\times\cdots\times d_{p}}\) (\(d_{p}=M\)) is denoted as the tensor stacked by each client's shared layers \(w_{i}\) and \(\lambda\) is the hyperparameter.

### Global Shared Layer Representation

Considering the heterogeneity of participating clients, the optimal model parameters are not identical across clients. This implies that simple weighted aggregation is insufficient to provide the required information to each client. Therefore, to uncover the underlying connections among model parameters of different clients, we utilize the trace norm as a regularizer to induce a low-rank structure among the parameters, which can better exploit intrinsic task relationships among the clients in the FMTL fashion. Since deep learning has evolved, the global shared layers \(w\) may be an \(L\)-layer deep network structure, we use the superscript \(l\) to denote the \(l\)-th layer of the global shared layers. Specifically, we first stack the global shared layers into a tensor on the server, which can handle the inherent correlation among multiple local models more efficiently:

\[\mathcal{W}^{l}=\mathrm{stack}(w_{1}^{l};w_{2}^{l};\cdots;w_{M}^{l})\in \mathbb{R}^{d_{1}\times\cdots\times d_{p}},\] (4)

where \(p\) denotes that \(\mathcal{W}^{l}\) is a \(p\)-way tensor, \(d_{1}\) to \(d_{p-1}\) are denoted as the dimensions of the model parameters and \(d_{p}\) is the number of clients, i.e., \(d_{p}=M\). Then, we regularize the tensor \(\mathcal{W}^{l}\) formed by stacking the global shared layers \(w_{i}^{l}\) from all \(M\) clients using a trace norm penalty \(||\cdot||_{*}\). That is:

\[\mathcal{L}_{r}=\sum_{l=1}^{L}\left\|\mathcal{W}^{l}\right\|_{*}=\sum_{l=1}^{L }\sum_{k=1}^{p}\left\|\mathcal{W}^{l}_{(k)}\right\|_{*},\] (5)

where \(\mathcal{W}^{l}_{(k)}\) is the matrix unfolded according to the \(k\)-th dimension, see Eq. (2). Minimizing the trace norm of \(\mathcal{L}_{r}\) yields a low-rank structure that reveals commonalities among the clients. This allows clients to transfer knowledge through coupled shared low dimensional subspaces, while still allowing clients to customize local models \(f_{i}\) and \(h_{i}\).

For the server's \(t\)-th round of global shared layers training, after stacking the received global shared layers form clients to compute the trace norm loss, we update the global shared parameters \(\widetilde{w}_{i}^{t}\) by gradient descent. Since the trace norm of a matrix is not differentiable, according to Watson _et al._[38], we can compute a subgradient,

\[\frac{\partial||\mathcal{W}^{l}_{(k)}||_{*}}{\partial\mathcal{W}^{l}_{(k)}}= UV^{T},\] (6)

where for \(\mathcal{W}^{l}_{(k)}=U\Sigma V^{T}\), the singular value decomposition of \(\mathcal{W}^{l}_{(k)}\). Since \(w_{i}\) is the \(i\)-th slice of \(\mathcal{W}\), for each client we can update the global shared layers in a slice-wise manner as:

\[\widetilde{w}_{i}^{t}\gets w_{i}^{t}-\eta_{w}\nabla\mathcal{L}_{r},\] (7)

where \(\eta_{w}\) is the learning rate. Intuitively, this subtracts the aggregated trace norm subgradient from each client's current shared layers, reducing redundancy and coupling the parameters to learn a jointly low-dimensional subspace. The updated \(\widetilde{w}_{i}^{t}\) are then sent back to the respective client at the end of each communication round to update their local models for the next round of training.

### Local Model Update

The server broadcasts the updated global shared layers \(\widetilde{w}_{i}^{t}\) to the client. In the \((t+1)\)-th round, client \(i\) replaces the shared layers of its local model with the received global shared layers. Thus, the replaced local model is represented as \(\widetilde{\theta}_{i}^{\,t+1}\).

Each client's local model obtains local knowledge from the local update and further obtains global knowledge among clients from the global update, allowing it to better handle heterogeneity. The assembled full local model \(\theta_{i}\) is then trained on the local data \(\mathbf{x}_{i}\) to obtain the updated local model parameters:

\[\theta_{i}^{t+1}\leftarrow\widetilde{\theta}_{i}^{t+1}-\eta_{\theta}\nabla \mathcal{L}_{c}\left(\widetilde{\theta}_{i}^{t+1};\mathbf{x}_{i}\right),\] (8)

where \(\eta_{\theta}\) is the learning rate, and \(\mathcal{L}_{c}\) is the loss of cross entropy. We summarize the steps of FedSAK in Algorithm 1, see Appendix.

## 5 Theoretical Analysis

### Convergence Analysis

To analyze the convergence of FedSAK, we define \(t\) as the current communication round, \(e\in\{0,1,\cdots,E\}\) as the number of local iterations, where \(E\) denotes the maximum number of local iterations. Thus, \((tE+e)\) represents the \(e\)-th iteration in the \((t+1)\)-th communication round. The \((t+0)\) denotes that at the beginning of the \((t+1)\)-th round, the client uses the global shared layers gradients from round \(t\) to update the local shared layer parameters. Note that \((tE+E)\) corresponds to the last iteration in round \((t+1)\). We make some assumptions see Appendix C.1, which is similar to the existing general framework [15; 16]. Based on the above assumptions, due to our same local training, Tan _et al._[15] and Yi _et al._[16] deduce that Lemma 1 and 2 still holds. For notational simplicity, we set \(\eta=\eta_{\theta}=\eta_{w}\).

**Lemma 1**: _Based on Assumption 1 and 2, in the local iteration \(e\in\{0,1,...,E\}\) of the \((t+1)\)-th training round, the local model loss of any client is bounded by._

\[\mathbb{E}\left[\mathcal{L}_{(t+1)E}\right]\leqslant\mathcal{L}_{tE+0}-\left( \eta-\frac{K\eta^{2}}{2}\right)\sum_{e=0}^{E}\left\|\mathcal{L}_{tE+e}\right\| _{2}^{2}+\frac{KE\eta^{2}}{2}\sigma^{2}.\] (9)

**Lemma 2**: _Based on Assumption 3, the loss of an arbitrary client's local model \((t+1)\)-this bounded by:_

\[\mathbb{E}\left[\mathcal{L}_{(t+1)E+0}\right]\leqslant\mathbb{E}\left[ \mathcal{L}_{(t+1)E}\right]+\frac{\eta K\lambda\omega^{2}}{2}.\] (10)

The detailed proof can be found in Appendix C.2-C.3 [15; 16].

Based on Lemma 1 and Lemma 2, we can derive the model nonconvex convergence rate.

**Theorem 1**: _The above assumptions, for an arbitrary client and any \(\epsilon>0\), if \(\eta<\frac{2E\epsilon-K\lambda\omega^{2}}{KE(\epsilon+\sigma^{2})}\), the following inequality holds:_

\[\frac{1}{TE}\sum_{t=0}^{T-1}\sum_{e=0}^{E}\mathbb{E}\left[\left\|\mathcal{L}_ {tE+e}\right\|_{2}^{2}\right]\leqslant\frac{2\left(\mathcal{L}_{t=0}-\mathcal{ L}^{*}\right)}{TE\eta\left(2-K\eta\right)}+\frac{K\left(E\eta\sigma^{2}+\lambda \omega^{2}\right)}{E(2-K\eta)}\leqslant\epsilon,\] (11)

With this, it is evident that the local model of any client of FedSAK converges at a non-convex convergence rate \(\mathcal{O}\left(\frac{1}{T}\right)\). See Appendix C.4 for a detailed proof.

### Excess Risk Bound

Without loss of generality [37; 39], to simplify the analysis we take the case where \(\Theta=\mathcal{W}\), i.e., sharing all model structures. Consequently, we define the problem (3) empirical loss for all the tasks as

\[\min_{\mathcal{W}}\ \hat{\mathcal{R}}(\mathcal{W})=\sum_{i=1}^{M}\frac{1}{n_{i}} \sum_{j=1}^{n_{i}}\mathcal{L}(\mathcal{F}_{i}(\mathcal{W};\mathbf{x}_{i}^{j} ),y_{i}^{j})\quad\text{s.t.}\ \|\mathcal{W}\|_{*}\leq\gamma.\] (12)

[MISSING_PAGE_FAIL:7]

MNIST [42], CIFAR-10 [43], CIFAR-100 [43], PACS [44], and Adience Faces [45]. Due to space constraints, detailed introductions of the datasets and the experimental setup are provided in Appendix B, while their splits will be elaborated in Section 6.2 for each heterogeneous task. Each client test set follows a similar distribution to the training set. Two fully connected layers are provided for Human Activity and MNIST datasets. CIFAR-10, CIFAR-100, PACS, and Adience Faces datasets are downsampled to 32\(\times\)32 size and consider a CNN model comprising 2 convolutional layers followed by 2 fully connected layers. Note that we only consider the last fully-connected layer of the model as the **predictor head**, and the other layers form the **feature extractor**. We ran each model 5 times and recorded its average value recorded as the result. Our code is available at: https://github.com/Tiaerc/FedSAK.

**Baselines.** We study the performance of FedSAK in heterogeneous settings and compare against baselines including: **(1)**_Conventional federated learning:_FedAvg [1], FedProx [2]; **(2)**_Personalized federated learning:_Per-FedAvg [3], pFedMe [4], MOON [9], Ditto [5], GPFL [10], FedAvgDBE [46]; **(3)**_Federated multi-task learning:_FedMTL [7], FedU [8] and **(4)**_Heterogeneous federated learning:_FedMD [17], FedProto [15], FedGH [16].

### Results and Discussion

**Data Heterogeneity**. Following the FMTL work [7; 8], we adopt a commonly used setup called pathology Non-IID to simulate DH in the form of label distribution shift in FL. We partition the dataset into \(M\) clients, with each client sampling data from \(S\) classes, where the number of samples per class varies significantly. The distribution of client data for each dataset is shown in Appendix B.2. For extensive comparisons of the advanced baselines, we design the individual client models to be homogeneous in DH scenarios. Thus, the global shared layer for each client is the client's entire model, i.e., \(w_{i}=\varphi_{i}\circ v_{i}\). Table 1 reports the average test accuracy across all clients, and Figure 2 summarizes the convergence behavior and performance of all methods. It can be observed that FedSAK achieves the highest accuracy in most cases, indicating that constraining through tensor trace norm facilitates transferring useful information across multiple clients, thereby improving model performance on each client. Notably, in this scenario, the lower the number of clients, the fewer the samples involved in training, and hence the accuracy is higher when the number of clients is higher, which is consistent with the results derived from our Theorem 3.

**Model Heterogeneity** is an important challenge in FL due to the differences in client computational resources and the fact that heterogeneous client requires different models. For example, the dataset PACS contains 4 different domains, and we set up each client to contain data from one domain. In this setup, we vary the number of filters in the convolutional layers and dimensions of the fully connected layers to obtain 4 heterogeneous models. The detailed model architectures are shown in Table 4 in the Appendix. Thus, in the MH scenario, our shared layers consist of the prediction head, which is \(w_{i}=v_{i}\). For CIFAR10, we set up 20 clients and alter data distribution using a Dirichlet distribution and label distribution skew. PACS itself has heterogeneity, so we set 4, 8, 10 clients respectively. The results in Table 2 show that FedSAK consistently achieves the highest model accuracy, while FedMD has lower accuracy. On CIFAR-10, since the distribution of our test set is similar to that of the training set, the model's performance will decrease due to the increase in the number of predicted label categories. Therefore, the results on the Dirichlet distribution are inferior to those with skewed label distribution. On PACS, it outperforms the best baseline FedGH by 5.98%, 4.08%, and 3.94% respectively. Figure 3 shows the test set accuracy with 4 clients on this dataset. Additionally, FedSAK requires less communication, thereby converging faster.

**Task Heterogeneity** in FL is commonly overlooked but objectively exists in reality, where clients typically train different tasks on a similar dataset. We adopt a large-scale face image dataset, AdienceFaces, which contains gender and age group labels for each person. Specifically, we set up 2, 10, and 15 clients to achieve gender classification and age classification, in which the ratio of heterogeneous tasks are 1:1, 1:2, and 2:1. Under the TH scenario, our shared layers consists of the feature extractor, i.e. \(w_{i}=\varphi_{i}\). Since there are currently no FL methods that can handle task heterogeneity, we use FedAvg-c to denote clients only uploading the feature extractor and aggregating with FedAvg. Table 3 reports our results. It can be seen that training only with local data is less effective than FL, indicating task heterogeneity is meaningful in the FL setting. We can see that FedSAK achieves the best results under all settings when the task distribution is balanced. When the heterogeneous task distribution is skewed amongst clients, FedAvg-c will be biased towards clients with a larger task proportion. For example, with 15 clients, 5 doing 2-class and 10 doing 8-class, although FedAvg-c performs slightly better on 8-class than FedSAK, its performance on 2-class clients is worse than Local. The confusion matrix in Figure 4 visualizes the results of our classification of the 2 TH clients, and we can see that our method can achieve significant results.

### Parameter Experiment

We first evaluated the impact of the trade-off parameters on different heterogeneous setups, Figure 5 depicts the performance of FedSAK on 3 heterogeneous tasks across datasets as \(\lambda\) varies. The \(\lambda\) controls the extent of coupling among client models, with larger \(\lambda\) indicating more emphasis on sharing information among parameters, while smaller \(\lambda\) focuses models on utilizing their own data. It can be observed that the optimal \(\lambda\) value differs across the heterogeneous tasks. In the data heterogeneity scenario, since models are homogeneous, larger \(\lambda\) values yield better performance. In contrast, in the task heterogeneity setting where client differences are greater, smaller \(\lambda\) produces improved results. Furthermore, when \(\lambda\) is too large, performance decreases in all heterogeneous scenarios. In summary, appropriately tuning \(\lambda\) allows balancing between customized local learning and collaborative multi-task training for each heterogeneous scenario.

Additionally, we tested the sensitivity of FedSAK's trade-off parameter \(\lambda\), learning rate \(\eta\)\((\eta=\eta_{\theta}=\eta_{w})\), and local iteration number \(E\) on CIFAR-10 (\(M\)=10, \(S\)=3). As Figure 6 shows, our model converges under all settings. The convergence curve fluctuates as \(\lambda\) changes, but is smoother for learning rate and local iterations, indicating FedSAK is not sensitive to the learning rate and

\begin{table}
\begin{tabular}{c|c|c c c c c} \hline \hline  & & Local & FedMD & FedProto & FedGH & FedSAK \\ \hline \hline \multirow{4}{*}{**CIFAR-10**} & \(\beta\)=0.3 & 30.67 & 32.56 & 34.15 & 33.38 & **35.02** \\  & \(\beta\)=0.5 & 36.47 & 38.29 & 39.66 & 39.42 & **41.48** \\  & \(\beta\)=1 & 42.95 & 46.87 & 47.82 & 44.66 & **47.99** \\ \cline{2-7}  & \(S\)=3 & 77.54 & 80.45 & 82.97 & 81.53 & **83.19** \\  & \(S\)=5 & 73.6 & 73.05 & 75.44 & 74.78 & **77.13** \\  & \(S\)=10 & 58.06 & 58.59 & 62.06 & 58.94 & **64.45** \\ \hline \multirow{4}{*}{**PACS**} & \(M\)=4 & 59.13 & 61.9 & 63.79 & 64.21 & **68.05** \\  & \(M\)=8 & 58.24 & 61.16 & 62.98 & 63.24 & **65.82** \\  & \(M\)=20 & 59.23 & 59.98 & 61.56 & 62.49 & **65.05** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Test accuracy (%) for the image classification task under model heterogeneity, \(\beta\) is the Dirichlet distribution, \(S\) is the number of labels, and \(M\) is the number of clients.

\begin{table}
\begin{tabular}{c|c|c c c} \hline \hline  & Model & Local & FedAvg-c & FedSAK \\ \hline \hline \multirow{4}{*}{M=2} & Gender (2) & 86.19 & 86.34 (+0.175) & **88.78** (+3.00\%) \\  & Age (8) & 58.21 & 65.09 (+11.82\%) & **65.13** (+11.89\%) \\  & Total & 72.2 & 75.71 (+4.86\%) & **76.95** (+6.58\%) \\ \cline{2-5}  & Gender (2) & 85.69 & 66.20 (+2.93\%) & **85.81** (+3.29\%) \\  & Age (8) & 58.22 & 63.44 (+2.85\%) & **85.54** (+6.02\%) \\  & Total & 72.26 & 74.73 (+3.42\%) & **76.03** (+5.22\%) \\ \hline \multirow{4}{*}{M=1} & Gender (2) & 85.96 & 82.9 (+3.66\%) & **87.01** (+1.22\%) \\  & Age (8) & 58.13 & **63.41** (+12.35\%) & **65.19** (+11.51\%) \\  & Total & 67.41 & 71.17 (+5.58\%) & **72.46** (+7.49\%) \\ \hline \multirow{4}{*}{M=1} & Gender (2) & 86.36 & 87.04 (+0.79\%) & **88.22** (+2.13\%) \\  & Age (8) & 58.23 & 60.9 (+4.59\%) & **63.51** (+9.07\%) \\ \cline{1-1}  & Total & 76.99 & **78.32** (+1.73\%) & **79.97** (+3.87\%) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Accuracy(%) of the Adience Faces, with (brackets) inside indicating the improvement rate relative to the Local, where the Gender (2) and Age (8) was shown the average test accuracy.

local iterations. We also find increasing local iterations speeds up convergence, aligning with our theoretical derivations.

### Computing and Communication Overhead

We plotted the per-epoch time for each method under the data heterogeneity setting with 100 clients on the MNIST dataset, as shown in Figure 7. The figure shows that personalized methods Ditto and pFedMe spend more time per epoch than most methods, due to the need to train additional personalized models. FedMD, which uses knowledge distillation, also has high time costs because the student and teacher models need to collaborate. Compared to most baselines, FedSAK reduces communication overhead on smaller models by track norm regularization updates for local model gradients. It is worth noting that since FedSAK performs tensor track norm, the computational complexity is \(O(\min_{k}d_{k}^{2}\prod_{i\neq k}^{p}d_{i})\), which seems to significantly increase with the model's dimensionality, representing a limitation of FedSAK. Nevertheless, FedSAK can balance communication overhead through a flexible upload of shared layer structures. To further demonstrate the communication overhead of FedSAK, we tested it using ResNet18 in the Appendix, as shown in Table 5. It can be seen that FedSAK can also effectively cope with large-scale modeling scenarios.

## 7 Conclusion

In this work, we propose a novel FMTL framework based on tensor trace norm to address challenging federated scenarios with data, model, and task heterogeneity. The method facilitates modeling associations and dependencies among client tasks by flexibly selecting model shared layer structures and uploading them to the server for tensor trace norm regularization. This enables useful knowledge transfer across clients to improve model performance on each task. We conduct comprehensive analyses on the efficacy of the method from both theoretical and experimental perspectives.

Figure 5: Test accuracy of parameter \(\lambda\) in different scenarios.

Figure 6: Sensitivity of model parameters.

Figure 7: Running time per epoch for each method on the MNIST.

[MISSING_PAGE_FAIL:11]

* Tan et al. [2022] Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang, and Chengqi Zhang. Fedproto: Federated prototype learning across heterogeneous clients. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 8432-8440, 2022.
* Yi et al. [2023] Liping Yi, Gang Wang, Xiaoguang Liu, Zhuan Shi, and Han Yu. Fedgh: Heterogeneous federated learning with generalized global header. pages 8686-8696, 2023.
* Li and Wang [2019] Daliang Li and Junpu Wang. Fedmd: Heterogenous federated learning via model distillation. _arXiv preprint arXiv:1910.03581_, 2019.
* Wu et al. [2022] Chuhan Wu, Fangzhao Wu, Lingjuan Lyu, Yongfeng Huang, and Xing Xie. Communication-efficient federated learning via knowledge distillation. _Nature communications_, 13(1):2032, 2022.
* Yang and Hospedales [2016] Yongxin Yang and Timothy Hospedales. Deep multi-task representation learning: A tensor factorisation approach. _arXiv preprint arXiv:1605.06391_, 2016.
* Corinzia et al. [2019] Luca Corinzia, Ami Beuret, and Joachim M Buhmann. Variational federated multi-task learning. _arXiv preprint arXiv:1906.06268_, 2019.
* Kumar and Daume III [2012] Abhishek Kumar and Hal Daume III. Learning task grouping and overlap in multi-task learning. _arXiv preprint arXiv:1206.6417_, 2012.
* Zhang et al. [2023] Xiongjun Zhang, Jin Wu, and Michael K Ng. Multilinear multitask learning by transformed tensor singular value decomposition. _Machine Learning with Applications_, page 100479, 2023.
* Zhao et al. [2024] Siran Zhao, Tianchi Liao, Lele Fu, Chuan Chen, Jing Bian, and Zibin Zheng. Data-free knowledge distillation via generator-free data generation for non-iid federated learning. _Neural Networks_, 179:106627, 2024.
* Collins et al. [2021] Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared representations for personalized federated learning. In _International conference on machine learning_, pages 2089-2099. PMLR, 2021.
* Arivazhagan et al. [2019] Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav Choudhary. Federated learning with personalization layers. _arXiv preprint arXiv:1912.00818_, 2019.
* Zhang et al. [2023] Jianqing Zhang, Yang Hua, Hao Wang, Tao Song, Zhengui Xue, Ruhui Ma, and Haibing Guan. Fedcp: Separating feature information for personalized federated learning via conditional policy. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 3249-3261, 2023.
* Ghosh et al. [2020] Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for clustered federated learning. _Advances in Neural Information Processing Systems_, 33:19586-19597, 2020.
* Lin et al. [2020] Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. Ensemble distillation for robust model fusion in federated learning. _Advances in Neural Information Processing Systems_, 33:2351-2363, 2020.
* Li et al. [2021] Qinbin Li, Bingsheng He, and Dawn Song. Practical one-shot federated learning for cross-silo setting. In _Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021_, pages 1484-1490. ijcai.org, 2021.
* Huang et al. [2013] Jui-Ting Huang, Jinyu Li, Dong Yu, Li Deng, and Yifan Gong. Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers. In _2013 IEEE international conference on acoustics, speech and signal processing_, pages 7304-7308. IEEE, 2013.
* Zhang et al. [2014] Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou Tang. Facial landmark detection by deep multi-task learning. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13_, pages 94-108. Springer, 2014.

* [32] Liuyi Yao, Dawei Gao, Zhen Wang, Yuexiang Xie, Weirui Kuang, Daoyuan Chen, Haohui Wang, Chenhe Dong, Bolin Ding, and Yaliang Li. A benchmark for federated hetero-task learning. _arXiv preprint arXiv_, 2206, 2022.
* [33] Massimiliano Pontil and Andreas Maurer. Excess risk bounds for multitask learning with trace norm regularization. In _Conference on Learning Theory_, pages 55-76. PMLR, 2013.
* [34] Yasmin Sarcheshmehpour, Yu Tian, Linli Zhang, and Alexander Jung. Networked federated multi-task learning. _CoRR_, 2021.
* [35] Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Federated multi-task learning for competing constraints. _CoRR_, 2020.
* [36] Ji Liu, Przemyslaw Musialski, Peter Wonka, and Jieping Ye. Tensor completion for estimating missing values in visual data. _IEEE transactions on pattern analysis and machine intelligence_, 35(1):208-220, 2012.
* [37] Yi Zhang, Yu Zhang, and Wei Wang. Multi-task learning via generalized tensor trace norm. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 2254-2262, 2021.
* [38] G Alistair Watson. Characterization of the subdifferential of some matrix norms. _Linear Algebra Appl_, 170(1):33-45, 1992.
* [39] Kishan Wimalawarne, Masashi Sugiyama, and Ryota Tomioka. Multitask learning meets tensor factorization: task imputation via convex optimization. _Advances in neural information processing systems_, 27, 2014.
* [40] Ryota Tomioka, Taiji Suzuki, Kohei Hayashi, and Hisashi Kashima. Statistical performance of convex tensor decomposition. _Advances in neural information processing systems_, 24, 2011.
* [41] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, Jorge Luis Reyes-Ortiz, et al. A public domain dataset for human activity recognition using smartphones. In _Esann_, volume 3, page 3, 2013.
* [42] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.
* [43] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [44] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In _Proceedings of the IEEE international conference on computer vision_, pages 5542-5550, 2017.
* [45] Eran Eidinger, Roee Enbar, and Tal Hassner. Age and gender estimation of unfiltered faces. _IEEE Transactions on information forensics and security_, 9(12):2170-2179, 2014.
* [46] Jianqing Zhang, Yang Hua, Jian Cao, Hao Wang, Tao Song, Zhengui Xue, Ruhui Ma, and Haibing Guan. Eliminating domain bias for federated learning in representation space. _arXiv preprint arXiv:2311.14975_, 2023.
* [47] Joel A Tropp. User-friendly tail bounds for sums of random matrices. _Foundations of computational mathematics_, 12:389-434, 2012.
* [48] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. _The collected works of Wassily Hoeffding_, pages 409-426, 1994.
* [49] Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. _Journal of Machine Learning Research_, 3(Nov):463-482, 2002.

## Appendix A Supplementary Experiments

### Training Setup

See Algorithm 1 for details of our algorithm.

For local optimization, all methods utilize mini-batch SGD, with the number of local epochs set to \(E\)=5 and the batch size selection range is \(\{16,20,32\}\) per client. The number of global communication rounds is uniformly set to \(t\)=100 across all datasets, which is sufficient for the convergence of the FL methods. We report the average test accuracy across all clients as the evaluation metric after convergence. We also tune special hyperparameters for baselines and report the optimal results. We ran these experiments using 4 NVIDIA GeForce RTX 4090 GPUs for all methods.

### Model Setup

**Data heterogeneity.** In the data heterogeneity scenario, to explore only the impact of data heterogeneity, we set up each client to have the same model structure. On the HumA and MNIST datasets, we used 2 fully connected layers, i.e., \(input\times 100\times classnumber\). On the CIFAR dataset, we used a CNN model with 2 convolutional layers and 2 fully connected layers. Where the convolutional layer kernel is \(5*5\) and the filter distribution is 32 or 64, i.e. \((5\times 5,32)\) and \((5\times 5,64)\). Connecting 2 fully connected layers, i.e. \(input\times 512\times class\ number\). While communicating with the server, these model structures are uploaded to the server for tensor trace norm constraints.

**Model heterogeneity.** In the model heterogeneity setup, we vary the number of filters in the convolutional layer and the dimensionality of the fully connected layer in the CNN model to obtain four heterogeneous models The detailed design of each model is shown in Table 4. In the model heterogeneity experiments, we select different heterogeneous models for each client in turn. In this case, we only uploaded the prediction head to the server communication.

**Task heterogeneity.** We adopt the same network structure as the CIFAR dataset in the data heterogeneity scenario. And consider the last fully connected layer as the prediction head, i.e., \(512\times class\ number\). Therefore, in the task heterogeneity scenario, we uploaded a feature extractorconsisting of 2 convolutional layers i.e. \((5\times 5,32)\) and \((5\times 5,64)\) and 1 fully connected layer i.e. \(input\times 512\) to interact with other clients.

### Ablation Experiment

To emphasize the effectiveness of FedSAK's knowledge transfer using the tensor trace norm, we conducted ablation experiments. Due to model-specific design choices, when the model does not communicate, the model degrades to the Local state. When the shared layer uploads are weighted and aggregated only on the server side, our approach will degrade to the standard FedAvg algorithm. It is worth noting that FedAvg cannot do weighted averaging on the whole model in model heterogeneous and task heterogeneous scenarios. Therefore we only performed FedAvg computation on the uploaded shared layer. The experimental results are shown in Figure 8, where it can be seen that FedSAK for client knowledge transfer learning using tensor trace paradigms yields better results.

### Computing Overhead

In Section 6.4, we report a FedSAK computational complexity of \(O(\min_{k}d_{k}^{2}\prod_{i\neq k}^{p}d_{i})\) and compare the communication overhead of FedSAK with the baseline method on the MNIST dataset. The computational complexity of our method does increase with network size. Due to its flexibility in uploading shared structures, FedSAK can easily handle larger networks compared to methods that upload the entire model (e.g., FedAvg). To further emphasize the effectiveness of FedSAK on complex networks, we tested it using ResNet18 on the CIFAR-10 dataset. We set up a total of 20 clients, each containing 5 labels. In the FedSAK model, we shared the fully connected layer behind.

\begin{table}
\begin{tabular}{c|c|c c c c} \hline \hline  & layer name & Model-1 & Model-2 & Model-3 & Model-4 \\ \hline \multirow{8}{*}{**Feature extractor**} & \multirow{2}{*}{Conv1} & 16, 5\(\times\)5 & 32, 3\(\times\)3 & 32, 5\(\times\)5 & 32, 3\(\times\)3 \\  & & \(p\)=2, \(s\)=1 & \(p\)=1, \(s\)=2 & \(p\)=0, \(s\)=1 & \(p\)=0, \(s\)=1 \\ \cline{2-6}  & \multirow{2}{*}{Pool} & (2, 2) & (2, 2) & (2, 2) & (2, 2) \\ \cline{2-6}  & \multirow{2}{*}{Conv2} & 32, 3\(\times\)3 & 64, 2\(\times\)2 & 64, 5\(\times\)5 & 64, 5\(\times\)5 \\  & & \(p\)=1, \(s\)=1 & \(p\)=1, \(s\)=2 & \(p\)=0, \(s\)=1 & \(p\)=0, \(s\)=1 \\ \cline{2-6}  & \multirow{2}{*}{Pool} & (2, 2) & - & (2, 2) & (2, 2) \\ \cline{2-6}  & \multirow{2}{*}{Conv3} & 64, 2\(\times\)2 & \multirow{2}{*}{-} & \multirow{2}{*}{-} & \multirow{2}{*}{-} \\  & & \(p\)=1, \(s\)=2 & & & \\ \cline{2-6}  & FC1 & 512 & 512 & 512 & 512 \\ \hline
**Prediction head** & FC2 & 10 / 7 & 10 / 7 & 10 / 7 & 10 / 7 \\ \hline \hline \end{tabular}
\end{table}
Table 4: The structure of the four heterogeneous CNN models in model heterogeneity, with the client model selected in order by id (\(p\) for padding, \(s\) for stride).

Figure 8: Sensitivity of model parameters.

Table 5 reports the accuracy, time consumption, and memory usage of each method running ResNet18 on the CIFAR10 dataset. It can be seen that when only fully connected layer is uploaded, our model is still advantageous compared to most methods. In addition, its time consumption and memory occupation are much smaller than other methods, which greatly demonstrates the flexibility of our method.

## Appendix B Datasets

### Dataset Description

* **Human Activity [41]:** Mobile phone accelerometer and gyroscope data collected from 30 individuals, performing one of six activities: walking, walking-upstairs, walking-downstairs, sitting, standing, lying-down. We use the provided 561-length feature vectors of time and frequency domain variables generated for each instance. We model each individual as a separate task and predict between sitting and the other activities.
* **MNIST [42]:** A handwritten digit dataset includes 10 labels and 70,000 instances. The whole dataset is distributed to N = 100 clients. Each client has a different local data size and consists of 2 labels.
* **CIFAR-10 & CIFAR-100 [43]:** CIFAR-10 consists of 60000, 32\(\times\)32 color images in 10 classes, with 6,000 images per class. Similar to CIFAR-10, CIFAR-100 has 100 classes, with 600 images per class. We partition the dataset into \(M\) clients, with each client assigned \(S\) labels. Datasets are split randomly with 75% and 25% for training and testing, respectively.
* **PACS [44]:** PACS is a challenging heterogeneous dataset with large domain discrepancies. The PACS dataset has a total of 9,991 images, each of size 3\(\times\)227\(\times\)227. The dataset consists of 4 distinct domains: art painting, cartoon, photo, and sketch. Each domain contains 7 classes: Dog, Elephant, Giraffe, Guitar, Horse, House, Person. The picture of this dataset is shown in Figure 9.
* **Adience Faces [45]:** Adience Faces is a large-scale face image dataset with labels for gender and age group of each individual. This enables the dataset to be used for two tasks: (i) gender classification into two classes (male and female), and (ii) age group classification into eight groups (0-2, 4-6, 8-12, 15-20, 25-32, 38-48, 48-53, 60-100 years old). We use this dataset to evaluate heterogeneous task federated learning, where each client is assigned one of the tasks for training.

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline Model & Accuracy (\%) & Total Time Consumption & Used Memory \\ \hline \hline FedAvg & 68.05 & 6628s & 1.75 G \\ FedProx & 59.55 & 6729s & 2.58 G \\ \hline MOON & 68.52 & 9866s & 2.63 G \\ Per-FedAvg & 73.35 & 10822s & 1.75 G \\ pFedMe & 75.84 & 57406s & 2.63 G \\ Ditto & 77.98 & 19366s & 3.42 G \\ GPFL & 78.59 & 18749s & 1.83 G \\ FedAvgDBE & 75.54 & 7898s & 1.75 G \\ \hline FedMTL & 73.68 & 11757s & 2.58 G \\ FedU & 75.98 & 9584s & 2.58 G \\ \hline FedMD & 73.74 & 16084s & 3.50 G \\ FedProto & 78.34 & 12076s & 1.75 G \\ FedGH & 75.95 & 7508s & 1.71 G \\ \hline FedSAK & 77.69 & 7118s & 0.918 G \\ \hline \hline \end{tabular}
\end{table}
Table 5: The accuracy, time consumption, and memory usage of each method running ResNet18 on the CIFAR-10 dataset.

### Data Distribution Visualization

We visualized the data in order to clearly show the client's training data. The red circle represents the percentage of data, the larger the circle, the more data the client has of that type.

In the experiments with data heterogeneity, we follow the setup of the FMTL reference [7, 8], where each client contains a different number of partially labeled classes. Thus in our data heterogeneity setup, the **data distribution** of clients on Human Activity, MNIST, CIFAR-10, and CIFAR-100 are shown in Figure 10 - 13.

The CIFAR-10 and PACS datasets were the main ones we used in the experiments on **model heterogeneity**. In CIFAR-10 we employed the Dillikerley distribution to divide the heterogeneous data see Figure 14 and the labeled division data see Figure 12, respectively. PACS contains data from 4 different domains, which is more suitable for model heterogeneity in real scenarios. Since this data itself has a high degree of heterogeneity, we did not have a heterogeneous division of the client data label distribution, the data distribution is shown in Figure 15.

In the experiments of **task heterogeneity**, we use Adience Faces dataset and each client samples similar data for different training tasks, and its data distribution is shown in Figure 16.

## Appendix C Convergence Analysis

To analyze the convergence of FedSAK, we follow [15, 16] and make the following assumptions C.1: Define \(t\) as the current communication round, \(e\in\{0,1,\cdots,E\}\) as the number of local iterations, where \(E\) denotes the maximum number of local iterations. Thus, \((tE+e)\) represents the e-th iteration

Figure 11: The data distribution on MNIST.

Figure 12: The data distribution of all clients on CIFAR-10.

Figure 10: The data distribution on Human Activity.

Figure 9: The PACS datasets.

in the \((t+1)\)-th communication round. The \((t+0)\) denotes that at the beginning of the \((t+1)\)-th round, the client uses the global shared layers gradients from round \(t\) to update the local shared layer parameters. Note that \((tE+E)\) corresponds to the last iteration in round \((t+1)\).

### Assumption

**Assumption 1** (Lipschitz Smoothness): _The \(i\)-th client's local model loss function \(\mathcal{L}\) is Lipschitz continuous with Lipschitz constant \(K\), and \(0\leq\mathcal{L}\leq a\) with \(\mathcal{L}(0)=0\), i.e.,_

\[\|\nabla\mathcal{L}_{t_{1}}-\nabla\mathcal{L}_{t_{2}}\|_{2}\leq K\|\theta_{i }^{t_{1}}-\theta_{i}^{t_{2}}\|_{2},\] (16) \[\forall t_{1},t_{2}>0,i\in\{1,2,\ldots,M\}.\]

**Assumption 2** (Unbiased Gradient and Bounded Variance): _The random gradient \(g_{i}^{t}=\nabla\mathcal{L}_{t}\left(\theta_{i}^{t};\mathcal{B}_{i}^{t}\right)\) of each client's local model is unbiased, where \(\mathcal{B}\) is a batch of local data, i.e.,_

\[\mathbb{E}_{\mathcal{B}_{i}^{t}\subseteq n_{i}}\left[g_{i}^{t}\right]=\nabla \mathcal{L}(\theta_{i}^{t})=\nabla\mathcal{L}_{t},\forall i\in\{1,2,\ldots,M\},\] (17)

_and the variance of random gradient \(g_{k}^{t}\) is bounded by:_

\[\mathbb{E}_{\mathcal{B}_{i}^{t}\subseteq n_{i}}\left[\left\|\nabla\mathcal{L} _{t}\left(\theta_{i}^{t};\mathcal{B}_{i}^{t}\right)-\nabla\mathcal{L}_{t} \left(\theta_{i}^{t}\right)\right\|_{2}^{2}\right]\leq\sigma^{2}.\] (18)

**Assumption 3** (Bounded Variance of the Shared Layers): _The variance of the shared layers \(w_{i}\) of the model \(\theta_{i}\) trained by client \(i\) with local data \(n_{i}\), and the server tensor low-rank constrained update to the shared layers \(\widetilde{w}_{i}\) are bounded, i.e.,_

_parameter bounded: \(\mathbb{E}\left[\|w_{i}-\widetilde{w}_{i}\|_{2}^{2}\right]\leq\varepsilon^{2}\),_

_gradient bounded: \(\mathbb{E}\left[\|\nabla\mathcal{L}_{r}\left(w_{i}\right)-\nabla\mathcal{L} _{r}(\widetilde{w}_{i})\|_{2}^{2}\right]\leq\omega^{2}\)._

Based on the above assumptions, due to our same local training, Tan _et al._[15] and Yi _et al._[16] deduce that Lemma 1 and 2 still holds. For notational simplicity, we set \(\eta=\eta_{\theta}=\eta_{w}\).

Figure 14: The data distribution of all clients on CIFAR-10.

Figure 13: The data distribution of all clients on CIFAR-100.

### Proof for Lemma 1

**Proof.** For arbitrary clients, we have \(\theta^{t+1}=\theta^{t}-\eta g^{t}\), then

\[\begin{split}\mathcal{L}_{tE+1}&\leq\mathcal{L}_{tE+0} +\langle\nabla\mathcal{L}_{tE+0},(\theta^{tE+1}-\theta^{tE+0})\rangle\\ &\qquad+\frac{K}{2}\|\theta^{tE+1}-\theta^{tE+0}\|_{2}^{2}\\ &=\mathcal{L}_{tE+0}-\eta\langle\nabla\mathcal{L}_{tE+0},g^{tE+0 }\rangle+\frac{K}{2}\|\eta g^{tE+0}\|_{2}^{2},\end{split}\] (19)

Taking expectation of both sides of the above equation on the random variable \(\mathcal{B}\), we have

\[\begin{split}\mathbb{E}[\mathcal{L}_{tE+1}]&\leq \mathcal{L}_{tE+0}-\eta\mathbb{E}[\langle\nabla\mathcal{L}_{tE+0},g^{tE+0} \rangle]+\frac{K\eta^{2}}{2}\mathbb{E}[\|g^{tE+0}\|_{2}^{2}]\\ &=\mathcal{L}_{tE+0}-\eta\|\nabla\mathcal{L}_{tE+0}\|_{2}^{2}+ \frac{K\eta^{2}}{2}\mathbb{E}[\|g_{i,tE+0}\|_{2}^{2}]\\ \leq&\mathcal{L}_{tE+0}-\eta\|\nabla\mathcal{L}_{tE+ 0}\|_{2}^{2}+\frac{K\eta^{2}}{2}(\|\nabla\mathcal{L}_{tE+0}\|_{2}^{2}+\text{ Var}(g^{i,tE+0}))\\ &=\mathcal{L}_{tE+0}-(\eta-\frac{K\eta^{2}}{2})\|\nabla\mathcal{ L}_{tE+0}\|_{2}^{2}+\frac{K\eta^{2}}{2}\text{Var}(g^{i,tE+0})\\ \leq&\mathcal{L}_{tE+0}-(\eta-\frac{K\eta^{2}}{2}) \|\nabla\mathcal{L}_{tE+0}\|_{2}^{2}+\frac{K\eta^{2}}{2}\sigma^{2},\end{split}\] (20)

where \(\text{Var}(x)=\mathbb{E}[x^{2}]-(\mathbb{E}[x])^{2}\). Take expectation of \(\theta\) on both sides. Then, by telescoping of \(E\) steps, we have,

\[\mathbb{E}[\mathcal{L}_{(t+1)E}]\leq\mathcal{L}_{tE+0}-(\eta-\frac{K\eta^{2}} {2})\sum_{e=0}^{E}\|\nabla\mathcal{L}_{tE+e}\|_{2}^{2}+\frac{KE\eta^{2}}{2} \sigma^{2}.\] (21)

### Proof for Lemma 2

Until the proof, we denote \(q\) to indicate that the local model has not been used as a parameter for uploading the server side as a shared layer,i.e., \(q=\theta-w\). Note that the client symbol \(i\) is omitted since it is used for arbitrary clients.

Figure 16: The data distribution of all clients on Adience Faces. in practical settings with 4 and 20 clients, respectively.

**Proof.**

\[\begin{split}\mathcal{L}_{(t+1)E+0}=&\mathcal{L}_{(t+1)E }+\mathcal{L}_{(t+1)E+0}-\mathcal{L}_{(t+1)E}\\ =&\mathcal{L}_{(t+1)E}+\mathcal{L}\left(\left(q^{t+1},\widetilde{w}^{t+1}\right);\boldsymbol{x},y\right)-\mathcal{L}\left(\left(q^ {t+1},w^{t+1}\right);\boldsymbol{x},y\right)\\ \leqslant&\mathcal{L}_{(t+1)E}+\left\langle\nabla \mathcal{L}\left(\left(q^{t+1},w^{t+1}\right)\right),\left(\left(q^{t+1}, \widetilde{w}^{t+1}\right)-\left(q^{t+1},w^{t+1}\right)\right)\right\rangle\\ &+\frac{K\lambda}{2}\left\|\left(q^{t+1},\widetilde{w}^{t+1} \right)-\left(q^{t+1},w^{t+1}\right)\right\|_{2}^{2}\\ \leqslant&\mathcal{L}_{(t+1)E}+\frac{K\lambda}{2} \left\|\left(q^{t+1},\widetilde{w}^{t+1}\right)-\left(q^{t+1},w^{t+1}\right) \right\|_{2}^{2}\\ \leqslant&\mathcal{L}_{(t+1)E}+\frac{K\lambda}{2} \left\|\widetilde{w}^{t+1}-w^{t+1}\right\|_{2}^{2}\\ =&\mathcal{L}_{(t+1)E}+\frac{K\lambda}{2}\left\| \widetilde{w}^{t}-\eta\nabla\mathcal{L}\left(\widetilde{w}^{t}\right)-w^{t}+ \eta\nabla\mathcal{L}\left(w^{t}\right)\right\|_{2}^{2}\\ =&\mathcal{L}_{(t+1)E}+\frac{K\lambda}{2}\left\| \widetilde{w}^{t}-w^{t}+\eta\left(\nabla\mathcal{L}\left(w^{t}\right)-\nabla \mathcal{L}\left(\widetilde{w}^{t}\right)\right)\right\|_{2}^{2}\\ \leqslant&\mathcal{L}_{(t+1)E}+\frac{K\lambda}{2} \left\|\eta\left(\nabla\mathcal{L}\left(w^{t}\right)-\nabla\mathcal{L}\left( \widetilde{w}^{t}\right)\right)\right\|_{2}^{2}\\ =&\mathcal{L}_{(t+1)E}+\frac{\eta K\lambda}{2} \left\|\left(\nabla\mathcal{L}\left(w^{t}\right)-\nabla\mathcal{L}\left( \widetilde{w}^{t}\right)\right)\right\|_{2}^{2}.\end{split}\] (22)

The lemma is proved.

Taking expectation of both sides of the above equation on the random variable \(\mathcal{B}\), we have

\[\begin{split}\mathbb{E}\left[\mathcal{L}_{(t+1)E+0}\right]\leqslant &\mathbb{E}\left[\mathcal{L}_{(t+1)E}\right]+\frac{\eta K \lambda}{2}\mathbb{E}\left[\left\|\left(\nabla\mathcal{L}\left(w^{t}\right)- \nabla\mathcal{L}\left(\widetilde{w}^{t}\right)\right)\right\|_{2}^{2}\right] \\ \leqslant&\mathbb{E}\left[\mathcal{L}_{(t+1)E} \right]+\frac{\eta K\lambda\omega^{2}}{2}.\end{split}\] (23)

### Proof for Theorem 1

Before presenting the proof for Theorem 1, then prove the following theorem.

**Theorem 4**: _Based on the above assumptions, the expectation of the loss of an arbitrary client's local model before the start of a round of local iteration satisfies_

\[\begin{split}\mathbb{E}\left[\mathcal{L}_{(t+1)E+0}\right]\leqslant& \mathcal{L}_{tE+0}-\left(\eta-\frac{K\eta^{2}}{2}\right)\sum_{e=0}^{E} \left\|\mathcal{L}_{tE+e}\right\|_{2}^{2}\\ &+\frac{\eta K\left(E\eta\sigma^{2}+\lambda\omega^{2}\right)}{2}. \end{split}\] (24)

**Proof.** Substituting Lemma 1 into the second term on the right-hand side of Lemma 2 proves it.

Then we can prove Theorem 1 as follows:

**Proof.** Transform the form of Theorem 4 into

\[\sum_{e=0}^{E}\left\|\mathcal{L}_{tE+e}\right\|_{2}^{2}\leqslant\frac{ \mathcal{L}_{tE+0}-\mathbb{E}\left[\mathcal{L}_{(t+1)E+0}\right]+\frac{\eta K \left(E\eta\sigma^{2}+\lambda\omega^{2}\right)}{2}}{\eta-\frac{K\eta^{2}}{2}}.\] (25)

Take expectations of model \(\theta\) on both sides, we have:

\[\sum_{e=0}^{E}\mathbb{E}\left[\left\|\mathcal{L}_{tE+e}\right\|_{2}^{2}\right] \leqslant\frac{\mathbb{E}\left[\mathcal{L}_{tE+0}\right]-\mathbb{E}\left[ \mathcal{L}_{(t+1)E+0}\right]+\frac{\eta K\left(E\eta\sigma^{2}+\lambda \omega^{2}\right)}{2}}{\eta-\frac{K\eta^{2}}{2}}.\] (26)Since \(\sum_{t=1}^{T}\left(\mathbb{E}\left[\mathcal{L}_{tE+0}\right]-\mathbb{E}\left[ \mathcal{L}_{\left(t+1\right)E+0}\right]\right)\leqslant\mathcal{L}_{t=1}- \mathcal{L}^{*}\), for each round:

\[\frac{1}{TE} \sum_{t=0}^{T-1}\sum_{\epsilon=0}^{E}\mathbb{E}\left[\left\| \mathcal{L}_{tE+e}\right\|_{2}^{2}\right]\] \[\leqslant\frac{\frac{1}{TE}\sum_{t=0}^{T-1}\left(\mathbb{E} \left[\mathcal{L}_{tE+0}\right]-\mathbb{E}\left[\mathcal{L}_{\left(t+1\right)E+ 0}\right]\right)+\frac{\eta K\left(E\eta\sigma^{2}+\lambda\omega^{2}\right)}{2}}{ \eta-\frac{K\eta^{2}}{2}}\] \[\leqslant\frac{\frac{1}{TE}\left(\mathcal{L}_{t=1}-\mathcal{L}^{* }\right)+\frac{\eta K\left(E\eta\sigma^{2}+\lambda\omega^{2}\right)}{2}}{\eta -\frac{K\eta^{2}}{2}}\] (27) \[=\frac{2\left(\mathcal{L}_{t=1}-\mathcal{L}^{*}\right)+\eta KTE \left(E\eta\sigma^{2}+\lambda\omega^{2}\right)}{TE\left(2\eta-K\eta^{2} \right)}\] \[=\frac{2\left(\mathcal{L}_{t=1}-\mathcal{L}^{*}\right)}{TE\eta \left(2-K\eta\right)}+\frac{K\left(E\eta\sigma^{2}+\lambda\omega^{2}\right)}{ E(2-K\eta)}.\]

Given any \(\epsilon>0\) the above equation satisfies

\[\frac{2\left(\mathcal{L}_{t=1}-\mathcal{L}^{*}\right)}{TE\eta\left(2-K\eta \right)}+\frac{K\left(E\eta\sigma^{2}+\lambda\omega^{2}\right)}{E(2-K\eta)} \leqslant\epsilon.\] (28)

Then, we can obtain:

\[T\geqslant\frac{2\left(\mathcal{L}_{t=1}-\mathcal{L}^{*}\right)}{E\eta \epsilon\left(2-K\eta\right)-\eta K\left(E\eta\sigma^{2}+\lambda\omega^{2} \right)}.\] (29)

Since \(T>0,\mathcal{L}_{t=1}-\mathcal{L}^{*}>0\), we can further derive:

\[E\eta\epsilon\left(2-K\eta\right)-\eta K\left(E\eta\sigma^{2}+\lambda\omega^ {2}\right)>0,\] (30)

i.e.,

\[\eta<\frac{2E\epsilon-K\lambda\omega^{2}}{KE\left(\epsilon+\sigma^{2}\right)}.\] (31)

## Appendix D Excess risk bound

### Proof for Lemma 3

**Proof.** By following [40], we have

\[\left\|\mathcal{W}\right\|_{*^{*}}=\inf_{\sum_{k\neq\varnothing,\,k\subset[p] }\mathcal{W}^{\left(k\right)}=\mathcal{W}}\max_{k}\left\|\mathcal{W}^{\left(k \right)}_{\left(k\right)}\right\|_{\infty},\] (32)

where \([p]\) denotes a set of positive integers to larger than \(p\). Since we can take any \(\mathcal{W}^{\left(k\right)}\) to equal \(\mathcal{W}\), the norm can be upper bounded as follows:

\[\left\|\mathcal{W}\right\|_{*^{*}}\leq\min_{k}\left\|\mathcal{W}_{\left(k \right)}\right\|_{\infty}.\] (33)

### Proof for Theorem 2

**Proof.** Based on Lemma 3, given that the minimum expectation across \(k\) can be confined to an upper limit by the minimum of the expected values, it follows that

\[\mathbb{E}\left\|\mathcal{W}\right\|_{*^{*}}\leq\mathbb{E}\min_{k}\left\| \mathcal{W}_{\left(k\right)}\right\|_{\infty}\leq\min_{k}\mathbb{E}\left\| \mathcal{W}_{\left(k\right)}\right\|_{\infty}.\] (34)

Referring to Theorem 6.1 in Reference [47], we upper bound for each expectation that

\[\Pr\{\left\|\mathcal{W}_{\left(k\right)}\right\|_{\infty}\geq t\}\leq\begin{cases} D_{k}exp(-3t^{2}/8\sigma_{k}^{2}),&\text{for}\;t\leq\sigma_{k}^{2}/R_{k},\\ D_{k}exp(-3t/8R_{k}),&\text{for}\;t\geq\sigma_{k}^{2}/R_{k},\end{cases}\] (35)

and

\[\mathbb{E}\left\|\mathcal{W}_{\left(k\right)}\right\|_{\infty}\leq C(\sigma_{ k}\sqrt{lnD_{k}}+R_{k}lnD_{k}),\] (36)where \(C\) is an absolute constant, and \(\mathcal{Z}^{i,j}\) is a \(d_{1}\times\cdots\times d_{p-1}\times d_{p}\) zero tensor with only the ith slice along the last axis equal to \(\frac{1}{n}\sigma_{\mathbf{i}}^{j}\mathbf{x}_{i}^{j}\), and in addition, \(R_{k}\) satisfies \(R_{k}\geq||\mathcal{Z}_{(k)}^{i,j}||_{\infty}\), therefore:

\[\sigma_{k}^{2}=\max\left(\left\|\sum_{i=1}^{M}\sum_{j=1}^{n}\mathbb{E}[ \mathcal{Z}_{(k)}^{i,j}(\mathcal{Z}_{(k)}^{i,j})^{T}]\right\|_{\infty},\left\| \sum_{i=1}^{M}\sum_{j=1}^{n}\mathbb{E}[(\mathcal{Z}_{(k)}^{i,j})^{T}\mathcal{Z }_{(k)}^{i,j}]\right\|_{\infty}\right).\] (37)

Since the Frobenius norm of a matrix is larger than its spectral norm, \(||\mathcal{Z}_{(k)}^{i,j}||_{\infty}\leq\frac{1}{n}\) and we simply set \(R_{k}=\frac{1}{n}\). For \(\sigma_{k}\), we obtain

\[\mathbb{E}[\sum_{j=1}^{N}\mathcal{Z}_{(k)}^{i,j}(\mathcal{Z}_{(k)}^{i,j})^{T}] =\frac{1}{n}\mathbf{C}_{k-\{p\}}\preceq\frac{\kappa}{nd}\mathbf{I},\] (38)

where \(\kappa>0\) is given constant. This means:

\[\left\|\sum_{i=1}^{M}\sum_{j=1}^{n}\mathbb{E}[\mathcal{Z}_{(k)}^{i,j}( \mathcal{Z}_{(k)}^{i,j})^{T}]\right\|_{\infty}\leq\frac{\kappa M}{nd}.\] (39)

Similarly, we have

\[\mathbb{E}[\sum_{j=1}^{n}(\mathcal{Z}_{(k)}^{i,j})^{T}\mathcal{Z}_{(k)}^{i,j} ]=\mathrm{diag}\left(\frac{\mathrm{tr}\left(\mathbf{C}_{k-\{p\}}\right)}{n} \right)\preceq\frac{\kappa}{nd}\mathbf{I},\] (40)

where \(\mathrm{tr}(\cdot)\) denotes the trace on a matrix and \(\mathrm{diag}(\cdot)\) converts a vector or scalar to a diagonal matrix. This inequality implies

\[\left\|\sum_{i=1}^{M}\sum_{j=1}^{n}\mathbb{E}[(\mathcal{Z}_{(k)}^{i,j})^{T} \mathcal{Z}_{(k)}^{i,j}]\right\|_{\infty}\leq\frac{\kappa M}{nd}.\] (41)

Substituting inequalities Eq. (39) and Eq. (41) into Eq. (36), we obtain inequality Eq.(14).

### Proof for Theorem 3

**Proof.** Note that

\[\mathcal{R}(\hat{\mathcal{W}})-\mathcal{R}(\mathcal{W})=\underbrace{\mathcal{ R}(\hat{\mathcal{W}})-\hat{\mathcal{R}}(\hat{\mathcal{W}})}_{r1}+\underbrace{ \hat{\mathcal{R}}(\hat{\mathcal{W}})-\hat{\mathcal{R}}(\mathcal{W})}_{r2}+ \underbrace{\hat{\mathcal{R}}(\mathcal{W})-\mathcal{R}(\mathcal{W})}_{r3}\] (42)

We first establish the upper bound for part \(r3\) in Eq. (42). Under Assumption 1, it follows from Hoeffding's inequality [48] that

\[\hat{\mathcal{R}}(\mathcal{W})-\mathcal{R}(\mathcal{W})\leq a\sqrt{\frac{log (2/\delta)}{2\sum_{i}n_{i}}},\] (43)

with probability at least \(1-\frac{\delta}{2}\). Since \(\hat{\mathcal{W}}\) is the optimal solution of Eq. (12), for the part \(r2\), we have

\[\hat{\mathcal{R}}(\hat{\mathcal{W}})-\hat{\mathcal{R}}(\mathcal{W})\leq 0.\] (44)

Since \(\mathcal{L}\) is bounded such that the perturbation of Eq. (12) to \(\mathbf{x}_{i}^{j}\) is less than \(\frac{a}{Mn_{i}}\). By McDiarmid's inequality [49], the part \(r1\) we have:

\[\begin{split}\mathcal{R}(\hat{\mathcal{W}})-\hat{\mathcal{R}}( \hat{\mathcal{W}})&\leq\sup_{\|\mathcal{W}\|_{*}\leq\gamma}\{ \mathcal{R}(\mathcal{W})-\hat{\mathcal{R}}(\mathcal{W})\}\\ &\leq\mathbb{E}\left[\sup_{\|\mathcal{W}\|_{*}\leq\gamma}\{ \mathcal{R}(\mathcal{W})-\hat{\mathcal{R}}(\mathcal{W})\}\right]+a\sqrt{ \frac{\log(2/\delta)}{2Mn}},\end{split}\] (45)with probability at least \(1-\frac{\delta}{2}\). Therefore, plugging Eq. (43-45), into Eq. (42), we deduce

\[\begin{split}\mathcal{R}(\hat{\mathcal{W}})-\mathcal{R}(\mathcal{W} )&\leq\mathbb{E}\left[\sup_{\|\mathcal{W}\|_{*}\leq\gamma}\{ \mathcal{R}(\mathcal{W})-\hat{\mathcal{R}}(\mathcal{W})\}\right]\\ &\quad+a\sqrt{\frac{\log(2/\delta)}{2Mn}}+a\sqrt{\frac{log(2/ \delta)}{2\sum_{i}n_{i}}}\\ &\leq\mathbb{E}\left[\sup_{\|\mathcal{W}\|_{*}\leq\gamma}\{ \mathcal{R}(\mathcal{W})-\hat{\mathcal{R}}(\mathcal{W})\}\right]+a\sqrt{\frac {2\log(2/\delta)}{Mn}},\end{split}\] (46)

where the second inequality follows from \(n_{i}\geq n\). Now we estimate an upper bound of the expectation in Eq. (46) as:

\[\begin{split}&\mathbb{E}\left[\sup_{\|\mathcal{W}\|_{*}\leq\gamma} \{\mathcal{R}(\mathcal{W})-\hat{\mathcal{R}}(\mathcal{W})\}\right]\\ &\leq\mathbb{E}\sup_{\|\mathcal{W}\|_{*}\leq\gamma}\left|\frac{ 1}{M}\sum_{n_{i}}\mathbb{E}_{(\mathbf{x},y)\sim\mathcal{P}_{i}}\mathcal{L}( \mathcal{F}_{i}(\mathcal{W};\mathbf{x}),y)\right.\\ &\quad\left.-\frac{1}{M}\sum_{i=1}^{M}\frac{1}{n_{i}}\sum_{j=1}^{ n_{i}}\mathcal{L}(\mathcal{F}_{i}(\mathcal{W};\mathbf{x}_{i}^{j}),y_{i}^{j}) \right|\\ &\leq\mathbb{E}\sup_{\|\mathcal{W}\|_{*}\leq\gamma}\left|\frac{ 2}{M}\sum_{i=1}^{M}\frac{1}{n_{i}}\sum_{j=1}^{n_{i}}\sigma_{i}^{j}\mathcal{L}( \mathcal{F}_{i}(\mathcal{W};\mathbf{x}_{i}^{j}),y_{i}^{j})\right|,\end{split}\] (47)

where the expectation is taken over the Rademacher random variables and the training samples. Then, we obtain that

\[\begin{split}&\mathbb{E}\sup_{\|\mathcal{W}\|_{*}\leq\gamma} \left|\frac{2}{M}\sum_{i=1}^{M}\frac{1}{n_{i}}\sum_{j=1}^{n_{i}}\sigma_{i}^{j} \mathcal{L}(\mathcal{F}_{i}(\mathcal{W};\mathbf{x}_{i}^{j}),y_{i}^{j})\right| \\ &\leq\mathbb{E}\sup_{\|\mathcal{W}\|_{*}\leq\gamma}\left|\frac{ 2}{M}\sum_{i=1}^{M}\frac{1}{n_{i}}\sum_{j=1}^{n_{i}}\sigma_{i}^{j}\left[K|y_{i }^{j}|+\mathcal{L}(\mathcal{F}_{i}(\mathcal{W};\mathbf{x}_{i}^{j}))\right] \right|\\ &\leq K\mathbb{E}\sup_{\|\mathcal{W}\|_{*}\leq\gamma}\left|\frac {4}{M}\sum_{i=1}^{M}\frac{1}{n_{i}}\sum_{j=1}^{n_{i}}\sigma_{i}^{j}\mathcal{F} _{i}(\mathcal{W};\mathbf{x}_{i}^{j})\right|\\ &\quad+K\mathbb{E}\left|\frac{2}{M}\sum_{i=1}^{M}\frac{1}{n_{i}} \sum_{j=1}^{n_{i}}\sigma_{i}^{j}|y_{i}^{j}|\right|\\ &=\frac{4K}{M}\mathbb{E}\left[\sup_{\|\mathcal{W}\|_{*}\leq \gamma}\left|\mathcal{W}\right|_{*^{*}},\|\mathcal{W}\|_{*}\right]+\frac{2K}{Mn }\mathbb{E}\left|\sum_{i=1}^{M}\sum_{j=1}^{n_{i}}\sigma_{i}^{j}|y_{i}^{j}| \right|\\ &\leq\frac{4\gamma K}{M}\mathbb{E}\|\mathcal{W}\|_{*^{*}}+\frac{ 2K}{Mn}\mathbb{E}\left|\sum_{i=1}^{M}\sum_{j=1}^{n_{i}}\sigma_{i}^{j}|y_{i}^{j }|\right|.\end{split}\] (48)Notice that

\[\begin{split}&\mathbb{E}\left|\sum_{i=1}^{M}\sum_{j=1}^{n_{i}} \sigma_{i}^{j}|y_{i}^{j}|\right|\leq b\mathbb{E}\left|\sum_{i=1}^{M}\sum_{j=1}^{ n_{i}}\sigma_{i}^{j}\right|=b\mathbb{E}\left[\sqrt{\left(\sum_{i=1}^{M}\sum_{j=1}^{ n_{i}}\sigma_{i}^{j}\right)^{2}}\right]\\ &\leq b\left[\mathbb{E}\sqrt{\left(\sum_{i=1}^{M}\sum_{j=1}^{n_{i }}\sigma_{i}^{j}\right)^{2}}\right]=b\sqrt{\sum_{i=1}^{M}n_{i}}=b\sqrt{N}.\end{split}\] (49)

Combining Eq.(46-49), we deduce that

\[\mathcal{R}(\hat{\mathcal{W}})-\mathcal{R}(\mathcal{W})\leq\frac{4\gamma K}{M} \mathbb{E}[\|\mathcal{W}\|_{\ast^{\ast}}]+\frac{2bK\sqrt{N}}{Mn}+a\sqrt{\frac {2log(2/\delta)}{Mn}},\] (50)

with probability at least \(1-\frac{\delta}{2}\). The proof of this theorem is completed.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our paper aims to provide a flexible framework for dealing with heterogeneous federated learning, accurately reflecting the contributions and scope of the paper in the abstract and introduction.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In this paper, we present the limitations of the model used in this approach in terms of computational complexity. At the same time, we also provide a solution to this limitation in the appendix.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We provide detailed proof in the appendix.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide detailed proof in the appendix.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Our datasets are all public datasets and we provide the source address of the data and details of the experimental setups.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We explain in detail the choice of model hyperparameters and the optimizers used in the appendix.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Our experiments do not include significance experiments and therefore do not take into account statistical error information.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?Answer: [Yes] Justification: We report in the Appendix that we experimented with all methods using four NVIDIA GeForce RTX 4090 GPUs.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research is consistent in all respects with the NeurIPS Code of Ethics.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the significance of algorithms for the real world in the introduction section
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Inapplicable
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite all creators or original owners in our papers.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: New assets are well documented.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Inapplicable
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Inapplicable