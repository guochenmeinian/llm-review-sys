On the Generalization Error of Stochastic Mirror Descent for Quadratically-Bounded Losses: an Improved Analysis

 Ta Duy Nguyen

Department of Computer Science

Boston University

taduy@bu.edu

&Alina Ene

Department of Computer Science

Boston University

aene@bu.edu

&Huy Le Nguyen

Khoury College of Computer Sciences

Northeastern University

hu.nguyen@northeastern.edu

###### Abstract

In this work, we revisit the generalization error of stochastic mirror descent for quadratically bounded losses studied in Telgarsky (2022). Quadratically bounded losses is a broad class of loss functions, capturing both Lipschitz and smooth functions, for both regression and classification problems. We study the high probability generalization for this class of losses on linear predictors in both realizable and non-realizable cases when the data are sampled IID or from a Markov chain. The prior work relies on an intricate coupling argument between the iterates of the original problem and those projected onto a bounded domain. This approach enables blackbox application of concentration inequalities, but also leads to suboptimal guarantees due in part to the use of a union bound across all iterations. In this work, we depart significantly from the prior work of Telgarsky (2022), and introduce a novel approach for establishing high probability generalization guarantees. In contrast to the prior work, our work directly analyzes the moment generating function of a novel supermartingale sequence and leverages the structure of stochastic mirror descent. As a result, we obtain improved bounds in all aforementioned settings. Specifically, in the realizable case and non-realizable case with light-tailed sub-Gaussian data, we improve the bounds by a \(\log T\) factor, matching the correct rates of \(1/T\) and \(1/\sqrt{T}\), respectively. In the more challenging case of heavy-tailed polynomial data, we improve the existing bound by a \(\mathrm{poly}\ T\) factor.

## 1 Introduction

Along with convergence analysis of optimization methods, understanding the generalization of models trained by these methods on unseen data is an important question in machine learning. However, despite the number of works attempting to answer it, the problem has not been fully understood, even in the simplest setting of linear predictors constructed with the standard stochastic gradient/mirror descent. A great part of prior works [33, 15, 30, 31, 32] focus only on the generalization on linearly separable data and/or of models trained with specific losses with exponentially decaying tails such as logistic loss. The question of what we can guarantee beyond these settings remains open.

Recently, [35] proposes a new approach to analyze the generalization error with _high probability_ of stochastic mirror descent for a broad class of quadratically bounded losses, beyond the realizable setting. This class of losses encapsulates both Lipschitz and smooth functions, for both regression and classification problems. The obtained bounds complement existing in-expectation bounds [12] and nearly match the counterpart of convergence rates in optimization. While this result pushes forward the state of the art, the obtained guarantees do not completely resolve the problem. The central piece of the proposed approach is a "coupling" technique between the iterates of the original problem and those projected onto a bounded domain. In this technique, one first constrains the problem in a bounded domain with a well chosen diameter. The bounded domain diameter allows to apply concentration inequalities as a blackbox and obtain bounds in high probability. Then using an inductive argument and a union bound across all iterations, one can show that the iterates in the original problem coincide with the ones in the constrained problem. Due to the union bound, the success probability decreases from \(1-\delta\) to \(1-T\delta\), where \(T\) is the number of iterations in the algorithm. This loss translates to a milder \(\log T\) factor loss in the guarantee in the case of realizable data, and a more significant \(\operatorname{poly}\,T\) factor loss in the non-realizable setting when the data has polynomial tails. Thus a natural question arises of whether we can obtain a stronger analysis that closes these remaining gaps.

In this paper, we revisit these generalization bounds for quadratically bounded losses by [35]. We introduce a novel approach to analyze the generalization errors of stochastic mirror descent in both realizable and non-realizable cases when the data are sampled IID or from a Markov chain. In all these cases, we remove the need to use the union bound argument, thus preventing the loss in the success probability. This translates to the following improvements:

\(-\) In the realizable, and the non-realizable cases with sub-gaussian tailed data and Markovian data, we improve the bounds by a \(\log T\) factor. This improvement comes from analyzing the moment generating function of a martingale difference sequence with well-chosen coefficients. In these cases, we also remove the necessity of using the coupling-based argument used in the same work by [35]. Instead, by solely making use of the problem structure, we arrive at the same conclusion that with high probability, the iterates of stochastic mirror descent for quadratically bounded losses behave as if the problem domain is bounded.

\(-\) In the non-realizable case with polynomial tailed data, we improve the existing bound by a \(\operatorname{poly}\,T\) factor. Due to the polynomial dependency on \(\frac{1}{\delta}\), being able to maintain the same success probability through all iterations is crucial in this case. Unlike the previous work, we rely on a truncation technique. Using a more refined analysis of the truncated random variables, in combination with suitable concentration inequalities and the coupling technique, we improve the existing bounds significantly.

### Related Work

Broadly speaking, there is a rich body of works in optimization and generalization that provide convergence guarantees and generalization bounds for stochastic methods. Earlier works often focus on in-expectation bounds [8, 24, 26, 18, 12, 3], and bounds in high probability [16, 28, 14, 13] for problems with bounded domains or under various additional assumptions such as strong convexity, noise with light tails. Recent developments for optimization [25, 10, 20, 23, 11, 17, 9, 19, 29, 22, 21] are able to handle unconstrained problems and relax these assumptions, but also require changes to the algorithm such as gradient clipping.

In generalization error analysis, specifically, a number of prior works, including [33, 15, 30, 31, 32], focus only on linearly separable data. Among these, [33, 15, 32] only deal with exponentially tailed losses while [30, 31] show generalization bounds for general smooth convex losses. Our work, similarly to [35], goes beyond the realizable setting and specific losses. We show high probability generalization bounds in both realizable and non-realizable settings for the broad class of quadratically bounded losses, for both regression and classification problems.

Other related works include the line of works that examine generalization errors via algorithmic stability. The works by [12, 5, 6, 2, 1] show the generalization error of an arbitrary algorithm via a quantity called uniform stability. By bounding this quantity for specific algorithms on a fixed training dataset, they derive generalization bounds. Our work focuses on a different setting where we assume the algorithm has access to a fresh data sample in each iteration. In this regard, the setting in our work,as well as the prior work by [35], stays closer to the world of optimization. However, in contrast to the optimization world in which we commonly impose assumptions on the stochastic gradients (such as having bounded variance or sub-gaussian noise), we make assumptions on the data (such as sub-gaussian or polynomial tailed data). This difference introduces several challenges which we overcome in our work.

The main point of reference for this paper is the work by [35]. This work develops a "coupling" technique to bound the generalization error of stochastic mirror descent for quadratically bounded losses. This technique has been employed in prior works [10; 11; 9; 29; 27; 22] to obtain high probability convergence bounds of stochastic methods in optimization. Our work improves their results by using a different approach that takes a closer look at the mechanism of the concentration inequalities and leverages the problem structure. When the data are bounded or have sub-gaussian tails, analyzing the moment generating function of a novel martingale difference sequence allows us to maintain the same success probability, without using either the coupling technique or the union bound. This new analysis, however, does not change the observation by [35] that the iterates of the unconstrained and the constrained problems coincide with high probability. When the data have a polynomial tail, we rely on a truncation technique. In this case, the coupling technique is necessary but not the union bound, and we are still able to significantly improve the success rate.

In terms of techniques, the work by [21] for optimization is the closest to ours. In this work, the authors develop the whitebox approach to analyzing stochastic methods for optimization with light-tailed noise. In this work, we study generalization errors. Moreover, in all settings, our choice of martingale difference sequences and coefficients are a significant departure from the prior work. In particular, in [21] the choice of coefficients only depends on the problem parameters whereas in the realizable case, our coefficients depend also on the historical data. Our approach also allows for a flexible use of an induction argument without decreasing the success probability, while in [21] the bounds are simpler and can be easily achieved in a single step.

## 2 Preliminaries

In this section, we provide the general set up and necessary notations before analyzing stochastic mirror descent in the subsequent sections. Overall, we closely follow notations used in [35].

**Domain and norms**. In this work, we consider \(\mathcal{X}\)--the domain of the problem--to be a closed convex set or \(\mathbb{R}^{d}\). We will use \(\left\|\cdot\right\|\) to denote an arbitrary norm on \(\mathcal{X}\) and let \(\left\|\cdot\right\|_{*}\) be its dual norm. We define the Bregman divergence as \(\mathbf{D}_{\psi}(w;v)=\psi(w)-\psi(v)-\left\langle\nabla\psi(v),w-v\right\rangle\) where \(\psi:\mathbb{R}^{d}\to\mathbb{R}\) is a differentiable function that is \(1\)-strongly convex with respect to the norm \(\left\|\cdot\right\|\).

**Loss functions**. Each loss function \(\ell:\mathbb{R}\times\mathbb{R}\to\mathbb{R}_{\geq 0}\) in our consideration can be written using a convex scalar function \(\widetilde{\ell}\) in one of the two following forms: 1) \(\ell(y,\widehat{y})=\widetilde{\ell}(\operatorname{sign}(y)\widehat{y})\) where \(\operatorname{sign}(y)=1\) if \(y\geq 0\) and \(=-1\) otherwise; and 2) \(\ell(y,\widehat{y})=\widetilde{\ell}(y-\widehat{y})\). The first form captures classification losses and the second regression losses. We will assume that subgradients \(\partial\ell\) of \(\ell\) in the second argument always exist, and let \(\ell^{\prime}\) denote a subgradient in \(\partial\ell\). For a function \(f\), we also use \(\left\|\partial f(w)\right\|\coloneqq\sup\left\{\left\|g\right\|:g\in\partial f (w)\right\}\). We further make the following assumptions, introduced in [35] as quadratic boundedness and self-boundedness.

**Assumption 1**. We assume that \(\ell\) is \((C_{1},C_{2})\)-quadratically-bounded, for some constants \(C_{1},C_{2}\geq 0\), i.e., for all \(y,\widehat{y}\)

\[\left|\ell^{\prime}(y,\widehat{y})\right|\leq C_{1}+C_{2}\left(\left|y\right|+ \left|\widehat{y}\right|\right).\]

This condition captures both classes of Lipschitz and smooth functions. Indeed, Lemma 1.2 from [35] shows that \(\alpha\)-Lipschitz functions are \((\alpha,0)\)-quadratically-bounded while \(\beta\)-smooth functions are \((\left|\partial\widetilde{\ell}(0)\right|,\beta)\)-quadratically-bounded.

**Assumption 2**. In the realizable setting, we assume that \(\ell\) is \(\rho\)-self-bounding, i.e., \(\widetilde{\ell}\) satisfies \(\widetilde{\ell}^{\prime}(z)^{2}\leq 2\rho\widetilde{\ell}(z)\) for all \(z\in\mathbb{R}\).

The second assumption is a generalization of smoothness. This assumption is satisfied by smooth losses but also certain non-smooth losses such as the exponential loss. This condition is necessary in the current analysis to prove \(1/T\) rates in the realizable setting. The readers can refer to [34; 35] for more detailed discussion on this assumption.

Assumptions 1 and 2 are satisfied by commonly used loss functions in machine learning. These include the logistic loss \(\ell(y,\widehat{y})=\ln(1+\exp(-y\widehat{y}))\) and the squared loss \(\ell(y,\widehat{y})=\frac{1}{2}(y-\widehat{y})^{2}\) (see Lemma 1.4 in [35]).

For the loss function \(\ell\) and the configuration \(w\), and sample \((x,y)\) where \(x\) denotes the attribute and \(y\) the label, we will write \(\ell_{x,y}=\ell(y,w^{T}x)\). We state the following crucial lemma which is the same as Lemma A.1 in [35], whose proof will be omitted.

**Lemma 1** (Lemma A.1 in ([35])).: _Suppose \(\ell\) is \((C_{1},C_{2})\)-quadratically-bounded and \(B_{x}\geq 0\) is given. Given \((x,y)\) such that \(\max\left\{\left\|x\right\|_{*},\left|y\right|\right\}\leq B_{x}\) and any \(u,v\),_

\[\left\|\partial\ell_{x,y}(u)\right\|_{*} \leq B_{x}\left(C_{1}+C_{2}B_{x}\left(1+\left\|u\right|\right)\right)\] \[\left|\ell_{x,y}(u)-\ell_{x,y}(v)\right| \leq B_{x}\left\|u-v\right\|\left(C_{1}+C_{2}B_{x}\left(1+\left\| u\right\|\right)\right).\]

Risk, IID and Markovian data.When sample \((x_{i},y_{i})\) arrives in iteration \(i\) of an algorithm, we will use the notation \(\ell_{i}(w)=\ell(y_{i},w^{T}x_{i})\). For an algorithm of \(T\) iterations, we use \(\mathcal{F}_{t}=\sigma\left((x_{1},y_{1}),\ldots,(x_{t},y_{t})\right)\) to denote the natural filtration up to and including time \(t\). When the data are IID and generated from a distribution \(\pi\), we define the risk

\[\mathcal{R}(w)=\mathbb{E}_{(x,y)\sim\pi}\left[\ell(y,w^{T}x)\right];\]

In contrast to IID data, Markovian data come from a stochastic process. This setting has also been considered in [4]. We let \(P_{*}^{t}\) be the distribution of \((x_{t},y_{t})\) at iteration \(t\) conditioned on \(\mathcal{F}_{s}\). We make the following assumption regarding the uniform mixing time of the stochastic process. Note that similar assumptions have also appeared in [35, 4].

**Assumption 3**.: We assume that for some \(\epsilon,\tau\geq 0\) of our choice, there is a distribution \(\pi\) such that

\[\sup_{t\in\mathbb{Z}_{\geq 0}}\sup_{\mathcal{F}_{t}}\mathrm{TV}\left(P_{t}^{t+ \tau},\pi\right)\leq\epsilon.\]

We refer to the triple \((\pi,\tau,\epsilon)\) as an approximate stationarity witness. We then define the risk according to the approximate stationary distribution \(\pi\): \(\mathcal{R}(w)=\mathbb{E}_{(x,y)\sim\pi}\left[\ell(y,w^{T}x)\right].\)

Algorithm.Stochastic Mirror Descent is given in Algorithm 1. In this algorithm, for the simplicity of the analysis, we consider a fixed step size \(\eta\). In each iteration, we pick a subgradient \(g_{t}\in\partial\ell_{t}(w_{t-1})\) and perform the update step.

We finally introduce a standard lemma used in the analysis of Stochastic Mirror Descent.

**Lemma 2**.: _For \(t\geq 0\) and \(w_{\mathrm{ref}}\in\mathcal{X}\), we have_

\[\mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{t+1}\right)-\mathbf{D}_{\psi} \left(w_{\mathrm{ref}};w_{t}\right)\leq\eta\left(\ell_{t+1}\left(w_{\mathrm{ ref}}\right)-\ell_{t+1}\left(w_{t}\right)\right)+\frac{\eta^{2}}{2}\left\|g_{t+1} \right\|_{*}^{2}.\]

Other notations.We will use \(w_{\mathrm{ref}}\) to refer to a comparator of interest. For the simplicity of the exposition, we let \(D_{0}=\mathbf{D}_{\psi}(w_{\mathrm{ref}};w_{0})\), and \(\mathcal{R}^{*}=\inf_{v\in\mathcal{X}}\mathcal{R}(v)\). For a loss function \(\ell\) that is \((C_{1},C_{2})\)-quadratically-bounded, we let \(C_{4}=C_{1}+C_{2}(1+\left\|w_{\mathrm{ref}}\right\|)\).

## 3 Generalization bounds of SMD for IID data

In this section, we distinguish between two cases: the realizable case and the non-realizable case. In the realizable case, there exists an optimal solution \(w^{*}\in\mathcal{X}\) such that \(\mathcal{R}(w^{*})=0\). We will show that under mild assumptions, the risks of the solutions output by Algorithm 1 are bounded by \(O(1/T)\). In the non-realizable case, we will show, on the other hand, a weaker statement that the excess risks of the solutions are bounded by \(O(1/\sqrt{T})\).

### Realizable case

In the realizable case, the comparator \(w_{\mathrm{ref}}\) is not necessarily the global minimizer. To show the \(1/T\) rate, we will assume \(w_{\mathrm{ref}}\) satisfies \(\mathcal{R}(w_{\mathrm{ref}})\leq\rho\mathbf{D}_{\psi}(w_{\mathrm{ref}};w_{0})/T\) and that the loss at \(w_{\mathrm{ref}}\) is bounded. The guarantee for the iterates of Algorithm 1 is provided in Theorem 3.

**Theorem 3**.: _Suppose \(\ell\) is convex, \((C_{1},C_{2})\)-quadratically-bounded, and \(\rho\)-self-bounding. Given \(T\), \(((x_{t},y_{t}))_{t\leq T}\) are IID samples with \(\max\left\{\left\|x_{t}\right\|_{*},\left|y_{t}\right|\right\}\leq 1\) almost surely, \(w_{\mathrm{ref}}\) satisfies \(\mathcal{R}(w_{\mathrm{ref}})\leq\rho\mathbf{D}_{\psi}(w_{\mathrm{ref}};w_{0})/T\), and \(\max_{t<T}\ell_{t+1}(w_{\mathrm{ref}})\leq C_{3}\) almost surely. Then for \(\eta\leq\frac{1}{2\rho}\), with probability at least \(1-2\delta\), for every \(0\leq k\leq T-1\)_

\[\frac{1}{k+1}\sum_{t=0}^{k}\mathcal{R}(w_{t})+\frac{16\mathbf{D}_{\psi}\left( w_{\mathrm{ref}};w_{k+1}\right)}{5(k+1)\eta}\leq\frac{C}{k+1}+3\mathcal{R}(w_{ \mathrm{ref}}).\]

_where \(C=\frac{16C_{4}}{5}\log\frac{1}{\delta}\sqrt{\frac{15}{4}D_{0}+4\eta\gamma C_{ 3}}+\left(\frac{6}{\eta}D_{0}+\frac{32}{5}\gamma C_{3}\right)\) with \(\gamma=\max\left\{1,\log\frac{1}{\delta}\right\}\)._

The analysis of Theorem 3 relies on the use of concentration inequalities. In contrast to existing works that utilize concentration inequalities as a blackbox, we will make use of the mechanism for proving concentration inequalities in order to obtain stronger guarantees. The type of concentration inequalities we consider are shown by analyzing the moment generating function of suitably chosen martingale sequences. We will use Lemma 14 (Appendix) which gives a basic inequality that bounds the moment generating function of a bounded random variable. To start the analysis, we use Lemma 2 and Assumption 2 to obtain

**Lemma 4**.: _For all \(t\geq 0\), we have_

\[\mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{t+1}\right)-\mathbf{D}_{\psi} \left(w_{\mathrm{ref}};w_{t}\right)\leq\eta\ell_{t+1}\left(w_{\mathrm{ref}} \right)-\frac{\eta}{2}\ell_{t+1}(w_{t}),\]

\[\text{and hence, }\mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{t}\right)\leq \mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{0}\right)+\eta\sum_{i=1}^{t}\ell_{ i}\left(w_{\mathrm{ref}}\right)=D_{0}+\eta\sum_{i=1}^{t}\ell_{i}\left(w_{ \mathrm{ref}}\right).\]

First, let us pay attention to the term \(\sum_{i=1}^{t}\ell_{i}\left(w_{\mathrm{ref}}\right)\). Recall that the terms \(\ell_{i}\left(w_{\mathrm{ref}}\right)\) are non-negative and bounded by a constant \(C_{3}\) almost surely. We can analyze the term \(\sum_{i=1}^{T}\ell_{i}\left(w_{\mathrm{ref}}\right)\) which upper bounds all sums \(\sum_{i=1}^{t}\ell_{i}\left(w_{\mathrm{ref}}\right)\) by studying its moment generating function (or via a concentration inequality). We state this bound in the next lemma and defer the proof to the appendix.

**Lemma 5**.: _With probability at least \(1-\delta,\sum_{i=1}^{T}\ell_{i}\left(w_{\mathrm{ref}}\right)\leq\frac{7}{4}T \mathcal{R}(w_{\mathrm{ref}})+C_{3}\log\frac{1}{\delta}\)._

Lemma 4 and lemma 5 and the assumption that \(\mathcal{R}(w_{\mathrm{ref}})=O\left(1/T\right)\) imply that with probability at least \(1-\delta\), \(\mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{t}\right)\) is bounded. In other words, with probability at least \(1-\delta\), the iterates \(w_{t}\) all lie in a bounded region. One could therefore proceed to assume that the problem domain is simply this bounded ball around \(w_{\mathrm{ref}}\). This is the basic idea behind the "coupling" technique demonstrated in [35]. However, the important question is how to obtain a bound for the risk of all iterates even when we are working with a problem with unbounded domain. Here, not paying close attention to the structure of the problem and the blackbox use of concentration inequalities lead to suboptimal bounds. On the other hand, as discussed above, a crucial novelty in our analysis is the choice of a supermartingale difference sequence, defined in the proof below. By working from first principles using moment generating function of this sequence, we derive two conclusions: 1) an improved risk bound can be obtained, and 2) the coupling technique is not necessary.

Proof Sketch.: Towards bounding the risk \(\sum_{t=0}^{k}\mathcal{R}(w_{t})\), we define random variables

\[Z_{t} =\frac{1}{2}z_{t}\eta\left(\mathcal{R}(w_{t})-\mathcal{R}(w_{ \mathrm{ref}})-\ell_{t+1}\left(w_{\mathrm{ref}}\right)\right)+z_{t}\left( \mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{t+1}\right)-\mathbf{D}_{\psi} \left(w_{\mathrm{ref}};w_{t}\right)\right)\] \[\quad-\frac{3}{16}z_{t}\eta\left(\mathcal{R}(w_{\mathrm{ref}})+ \mathcal{R}(w_{t})\right),\qquad\forall 0\leq t\leq T-1\] \[\text{where }z_{t} =\frac{1}{\eta C_{4}\sqrt{2\eta\gamma C_{3}+2D_{0}+2\eta\sum_{i=1 }^{t}\ell_{i}\left(w_{\mathrm{ref}}\right)}};\quad\gamma=\max\left\{1,\log \frac{1}{\delta}\right\}\]and we let \(S_{t}=\sum_{i=0}^{t}Z_{i}\); \(\quad\forall 0\leq t\leq T-1\). The reason to define these variables is because from Lemma 4, we can bound

\[\mathbb{E}\left[\exp\left(Z_{t}\right)\mid\mathcal{F}_{t}\right] \times\exp\left(\frac{3}{16}z_{t}\eta\left(\mathcal{R}(w_{\mathrm{ref}})+ \mathcal{R}(w_{t})\right)\right)\] \[\leq \mathbb{E}\left[\exp\left(\frac{1}{2}z_{t}\eta\left(\mathcal{R}(w _{t})-\mathcal{R}(w_{\mathrm{ref}})-\ell_{t+1}\left(w_{\mathrm{ref}}\right) \right)+z_{t}\left(\eta\ell_{t+1}\left(w_{\mathrm{ref}}\right)-\frac{\eta}{2} \ell_{t+1}(w_{t})\right)\right)\mid\mathcal{F}_{t}\right]\] \[= \mathbb{E}\left[\exp\left(\frac{1}{2}z_{t}\eta\left(\mathcal{R}(w _{t})-\mathcal{R}(w_{\mathrm{ref}})+\ell_{t+1}(w_{\mathrm{ref}})-\ell_{t+1}( w_{t})\right)\right)\mid\mathcal{F}_{t}\right]\]

where now inside the expectation, we have the term \(\mathcal{R}(w_{t})-\mathcal{R}(w_{\mathrm{ref}})+\ell_{t+1}(w_{\mathrm{ref}}) -\ell_{t+1}(w_{t})\) which has expectation \(0\). Let \(C_{4}=C_{1}+C_{2}(1+\left\|w_{\mathrm{ref}}\right\|)\), we can bound

\[\left|\frac{\eta}{2}\left(\mathcal{R}(w_{t})-\mathcal{R}(w_{\mathrm{ref}})+ \ell_{t+1}(w_{\mathrm{ref}})-\ell_{t+1}(w_{t})\right)\right|\leq\eta C_{4} \left\|w_{\mathrm{ref}}-w_{t}\right\|\]

and \(z_{t}\leq\frac{1}{\eta C_{4}\left\|w_{\mathrm{ref}}-w_{t}\right\|}\). Now we can apply Lemma 14 (Appendix) to bound

\[\mathbb{E}\left[\exp\left(Z_{t}\right)\mid\mathcal{F}_{t}\right] \times\exp\left(\frac{3}{16}z_{t}\eta\left(\mathcal{R}(w_{\mathrm{ref}})+ \mathcal{R}(w_{t})\right)\right)\] \[\leq \exp\left(\frac{3}{4}\frac{1}{4}z_{t}^{2}\eta^{2}\mathbb{E}\left[ \left(\mathcal{R}(w_{t})-\mathcal{R}(w_{\mathrm{ref}})+\ell_{t+1}(w_{\mathrm{ ref}})-\ell_{t+1}(w_{t})\right)^{2}\mid\mathcal{F}_{t}\right]\right)\] \[\leq \exp\left(\frac{3}{16}z_{t}^{2}\eta^{2}\mathbb{E}\left[\left( \ell_{t+1}(w_{\mathrm{ref}})-\ell_{t+1}(w_{t})\right)^{2}\mid\mathcal{F}_{t} \right]\right)\] \[\leq \exp\left(\frac{3}{16}z_{t}^{2}\eta^{2}C_{4}\left\|w_{\mathrm{ref} }-w_{t}\right\|\mathbb{E}\left[\ell_{t+1}(w_{\mathrm{ref}})+\ell_{t+1}(w_{t}) \mid\mathcal{F}_{t}\right]\right)\] \[\leq \exp\left(\frac{3}{16}z_{t}\eta\left(\mathcal{R}(w_{\mathrm{ref}} )+\mathcal{R}(w_{t})\right)\right)\]

Therefore \(\mathbb{E}\left[\exp\left(Z_{t}\right)\mid\mathcal{F}_{t}\right]\leq 1\) and hence \(\left(\exp\left(S_{t}\right)\right)_{t\geq 0}\) is a supermartingale. By Ville's inequality, we have with probability at least \(1-\delta\), for all \(0\leq k\leq T-1\)

\[\sum_{t=0}^{k}Z_{t}\leq\log\frac{1}{\delta}\]

Expanding this inequality, in combination with Lemma 5, we obtain the conclusion. 

_Remark 6_.: The new analysis does not change the conclusion observed in [35]--that is, with high probability, the iterate sequence \(\left(w_{t}\right)_{t\geq 0}\) behaves as if the domain of the problem is bounded. We improve the probability that this event happens.

### Non-realizable case

In the non-realizable case, we do not aim for \(1/T\) but only \(1/\sqrt{T}\) rates. Hence we do not assume that the comparator \(w_{\mathrm{ref}}\) satisfies \(\mathcal{R}(w_{\mathrm{ref}})\leq\rho\mathbf{D}_{\psi}(w_{\mathrm{ref}};w_{0})/T\) but rather the following assumption on the excess risk:

**Assumption 4**. Let \(\mathcal{R}^{*}=\inf_{v\in\mathcal{X}}\mathcal{R}(v)\), assume that \(\mathcal{R}(w_{\mathrm{ref}})-\mathcal{R}^{*}\leq\frac{\mathbf{D}_{\psi}(w_{ \mathrm{ref}};w_{0})}{\sqrt{T}}\).

We also relax the assumption on the data samples. In the previous case, the data are bounded, i.e \(\left\{\left\|x\right\|_{*},\left|y\right|\right\}\leq 1\) a.s. We will consider in this section two settings, one when the data come from a sub-Gaussian distribution and one when the data distribution has a polynomial tail.

**Remark on Theorem 10 in [35]**. There seems to be an issue with the proof of Theorem 10 in [35]. The proof uses a variant of Azuma's inequality ([37], Problem 3.11) which allows the ranges of the random variables to not be specified up front. However, when bounding the range of relevant random variables, the proof uses \(Z_{t+1}\), which is correlated with the data \(\left\|x_{t+1}\right\|_{*}\) and \(y_{t+1}\). Thus the condition of Azuma's inequality is not satisfied. We do not see an immediate way to resolve this problem. In the following, we consider separately the two cases of sub-Gaussian and polynomial tailed data for which we use different proof techniques to show the error bounds. In the first case of IID data with sub-Gaussian tails, we proceed by bounding the moment generating function of a well-chosen martingale sequence. In the second case of IID data with polynomial tails, we introduce a truncation technique. In both cases, we are able to obtained better bounds compared with [35].

#### 3.2.1 IID data with sub-Gaussian tails

We will show the following guarantee:

**Theorem 7**.: _Suppose \(\ell\) is convex, \((C_{1},C_{2})\)-quadratically-bounded. Given \(T\), \(((x_{t},y_{t}))_{t\leq T}\) are IID samples with \(Q_{t}=\max\left\{1,\left\|x_{t}\right\|_{*}^{2},\left|y_{t}\right|^{2}\right\}\) and there exists \(\sigma\geq 0\) such that for all \(\lambda\)_

\[\max\left\{\mathbb{E}\left[\exp\left(\lambda\left(Q_{t}^{2}-\mathbb{E}\left[Q _{t}^{2}\right]\right)\right)\right],\mathbb{E}\left[\exp\left(\lambda\left(Q_ {t}-\mathbb{E}\left[Q_{t}\right]\right)\right)\right]\right\}\leq\exp\left( \lambda^{2}\sigma^{2}\right)\]

_Let \(\mu_{1}=\mathbb{E}\left[Q_{t}\right]\) and \(\mu_{2}=\mathbb{E}\left[Q_{t}^{2}\right]\). Suppose that \(w_{\mathrm{ref}}\) satisfies Assumption 4. Then for \(\eta\leq\frac{1}{4C_{2}\sqrt{T\mu_{2}+2\sigma\sqrt{T\log\frac{1}{\delta}}}}\), with probability at least \(1-2\delta\), for every \(0\leq k\leq T-1\)_

\[\frac{1}{k+1}\sum_{t=0}^{k}\left(\mathcal{R}(w_{t})-\mathcal{R}^{*}\right)+ \frac{\mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{k+1}\right)}{\eta\left(k+1 \right)}\leq\frac{R^{2}}{\eta\left(k+1\right)}\]

_where \(R^{2}=16C_{4}^{2}\left(\sigma^{2}+4\mu_{1}^{2}\right)\log\frac{1}{\delta}\eta ^{2}T+4D_{0}(1+\eta\sqrt{T})+4\eta^{2}C_{4}^{2}\left(T\mu_{2}+2\sigma\sqrt{T \log\frac{1}{\delta}}\right)=O(1)\)._

_Remark 8_.: For zero-mean sub-Gaussian variable \(X\), the definition \(\mathbb{E}\left[\exp\left(\lambda X\right)\right]\leq\exp\left(\lambda^{2} \sigma^{2}\right)\) for all \(\lambda\) is equivalent to \(\mathbb{E}\left[\exp\left(\lambda^{2}X^{2}\right)\right]\leq\exp\left(\lambda ^{2}\sigma^{2}\right)\) for all \(0\leq\lambda\leq\frac{1}{\sigma}\) (see [38]). The lemma below shows a property of sub-Gaussian variables under scaling and translating. First let us consider \(\sum_{t=1}^{T}Q_{t}^{2}\). Similar to Lemma 5, by bounding the moment generating function of this term, we have the following (see also Section B4 in [35]).

**Lemma 9**.: _With probability at least \(1-\delta\), \(\sum_{t=1}^{T}Q_{t}^{2}\leq T\mu_{2}+2\sigma\sqrt{T\log\frac{1}{\delta}}\)._

Proof of Theorem 7.: The proof of this Theorem uses the technique developed in [21]. We will also analyze the moment generating function of a suitable martingale sequence. However, the choice of the coefficients will differ significantly from the previous proof. In this case the structure of the problem is deeply integrated into the analysis of the martingale. We define

\[Z_{t} =z_{t}\eta\left(\mathcal{R}(w_{t})-\mathcal{R}(w_{\mathrm{ref}}) \right)+z_{t}\left(\mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{t+1}\right)- \mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{t}\right)\right)\] \[\quad-\frac{1}{2}z_{t}\eta^{2}\left\|g_{t+1}\right\|_{*}^{2}-4z_{ t}^{2}\eta^{2}C_{4}^{2}\left(\sigma^{2}+4\mu_{1}^{2}\right)\mathbf{D}_{\psi} \left(w_{\mathrm{ref}};w_{t}\right) \forall 0\leq t\leq T-1\]

where \(z_{t}=\frac{1}{4\eta^{2}C_{4}^{2}\left(\sigma^{2}+4\mu_{1}^{2}\right)\left(T+ t+1\right)}\)\(\forall-1\leq t\leq T-1\)

and let \(S_{t}=\sum_{i=0}^{t}Z_{i};\quad\forall 0\leq t\leq T-1\). By Lemma 2, we have

\[Z_{t}+4z_{t}^{2}\eta^{2}C_{4}^{2}\left(\sigma^{2}+4\mu_{1}^{2}\right)\mathbf{D }_{\psi}\left(w_{\mathrm{ref}};w_{\mathrm{ref}}\right)\leq z_{t}\eta\left( \mathcal{R}(w_{t})-\mathcal{R}(w_{\mathrm{ref}})+\ell_{t+1}\left(w_{\mathrm{ ref}}\right)-\ell_{t+1}\left(w_{t}\right)\right)\]

where we have \(\mathbb{E}\left[\left(\mathcal{R}(w_{t})-\mathcal{R}(w_{\mathrm{ref}})+\ell_{t+ 1}\left(w_{\mathrm{ref}}\right)-\ell_{t+1}\left(w_{t}\right)\right)\right]=0\), and using the same notation \(C_{4}=C_{1}+C_{2}(1+\left\|w_{\mathrm{ref}}\right\|)\), by Lemma 1,

\[\left|\left(\mathcal{R}(w_{t})-\mathcal{R}(w_{\mathrm{ref}})+\ell_ {t+1}\left(w_{\mathrm{ref}}\right)-\ell_{t+1}\left(w_{t}\right)\right)\right|\] \[\leq \left|\ell_{t+1}\left(w_{\mathrm{ref}}\right)-\ell_{t+1}\left(w_{t }\right)\right|+\left|\mathcal{R}(w_{t})-\mathcal{R}(w_{\mathrm{ref}})\right|\] \[\leq \left|\ell_{t+1}\left(w_{\mathrm{ref}}\right)-\ell_{t+1}\left(w_{ t}\right)\right|+\mathbb{E}\left[\left|\ell_{x,y}(w_{t})-\ell_{x,y}(w_{ \mathrm{ref}})\right|\right]\] \[\leq \left(Q_{t}+\mu_{1}\right)\left\|w_{\mathrm{ref}}-w_{t}\right\|C_ {4}=\left(\left(Q_{t}-\mu_{1}\right)+2\mu_{1}\right)\left\|w_{\mathrm{ref}}-w_{ t}\right\|C_{4}\]

Hence applying Lemma 15,we have

\[\mathbb{E}\left[\exp\left(Z_{t}\right)\mid\mathcal{F}_{t}\right] \exp\left(4z_{t}^{2}\eta^{2}C_{4}^{2}\left(\sigma^{2}+4\mu_{1}^{2}\right) \mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{t}\right)\right)\] \[= \mathbb{E}\left[\exp\left(z_{t}\eta_{t}\left(\mathcal{R}(w_{t})- \mathcal{R}(w_{\mathrm{ref}})+\ell_{t+1}\left(w_{\mathrm{ref}}\right)-\ell_{t+1} \left(w_{t}\right)\right)\right)\mid\mathcal{F}_{t}\right]\]\[\leq \exp\left(2z_{t}^{2}\eta^{2}C_{4}^{2}\left\|w_{\mathrm{ref}}-w_{t} \right\|^{2}\left(\sigma^{2}+4\mu_{1}^{2}\right)\right)\] \[\leq \exp\left(4z_{t}^{2}\eta^{2}C_{4}^{2}\left(\sigma^{2}+4\mu_{1}^{2 }\right)\mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{t}\right)\right)\]

Therefore \(\mathbb{E}\left[\exp\left(Z_{t}\right)\mid\mathcal{F}_{t}\right]\leq 1\) and hence \(\left(\exp\left(S_{t}\right)\right)_{t\geq 0}\) is a supermartingale. By Ville's inequality, we have with probability at least \(1-\delta\), for all \(0\leq k\leq T-1\)

\[\sum_{t=0}^{k}Z_{t}\leq\log\frac{1}{\delta}\]

Expanding this inequality we have

\[\sum_{t=0}^{k}z_{t}\eta\mathcal{R}(w_{t})+z_{k}\mathbf{D}_{\psi} \left(w_{\mathrm{ref}};w_{k+1}\right)\] \[\leq \log\frac{1}{\delta}+z_{-1}D_{0}+\eta\mathcal{R}(w_{\mathrm{ref} })\sum_{t=0}^{k}z_{t}+\frac{1}{2}\sum_{t=0}^{k}z_{t}\eta^{2}\left\|g_{t+1} \right\|_{*}^{2}\] \[+\sum_{t=0}^{k}\underbrace{\left(z_{t}+4z_{t}^{2}\eta^{2}C_{4}^{ 2}\left(\sigma^{2}+4\mu_{1}^{2}\right)-z_{t-1}\right)}_{\leq 0}\mathbf{D}_{\psi} \left(w_{\mathrm{ref}};w_{t}\right)\] \[\stackrel{{(a)}}{{\leq}} \log\frac{1}{\delta}+z_{-1}D_{0}+\eta\mathcal{R}(w_{\mathrm{ref} })\sum_{t=0}^{k}z_{t}+\frac{1}{2}\sum_{t=0}^{k}z_{t}\eta^{2}\left\|g_{t+1} \right\|_{*}^{2}\]

where for \((a)\), by the choice of \(z_{t}=\frac{1}{4\eta^{2}C_{4}^{2}\left(\sigma^{2}+4\mu_{1}^{2}\right)(T+1+t)}\) we have \(z_{t-1}-z_{t}\geq 4z_{t}^{2}\eta^{2}C_{4}^{2}\left(\sigma^{2}+4\mu_{1}^{2} \right).\) We highlight that this is where the structure of the problem comes into play. That is, by setting appropriate coefficients, we can leverage gain in the distance in the martingale difference sequence (\(\left(z_{t}-z_{t-1}\right)\mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{t}\right)\)) to cancel out the loss from bounding the moment generating function (\(4z_{t}^{2}\eta^{2}C_{4}^{2}\left(\sigma^{2}+4\mu_{1}^{2}\right)\mathbf{D}_{ \psi}\left(w_{\mathrm{ref}};w_{t}\right)\)). Another important property of the sequence \((z_{t})\) is that it is a decreasing sequence and \(\frac{z_{t}}{z_{t}}\leq 2\) for all \(t,k\). Hence we have

\[\eta\sum_{t=0}^{k}\left(\mathcal{R}(w_{t})-\mathcal{R}^{*}\right) +\mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{k+1}\right)\] \[\leq 4C_{4}^{2}\left(\sigma^{2}+4\mu_{1}^{2}\right)\log\frac{1}{ \delta}\eta^{2}\left(T+1+k\right)+2D_{0}+2\left(\mathcal{R}(w_{\mathrm{ref}})- \mathcal{R}^{*}\right)\eta(k+1)+\eta^{2}\sum_{t=0}^{k}\left\|g_{t+1}\right\|_ {*}^{2}.\]

Combined with Lemma 9, with probability at least \(1-2\delta\), for all \(0\leq k\leq T-1\)

\[\eta\sum_{t=0}^{k}\left(\mathcal{R}(w_{t})-\mathcal{R}^{*}\right) +\mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{k+1}\right)\] \[\leq 4C_{4}^{2}\left(\sigma^{2}+4\mu_{1}^{2}\right)\log\frac{1}{ \delta}\eta^{2}\left(T+1+k\right)+2D_{0}+2\left(\mathcal{R}(w_{\mathrm{ref}}) -\mathcal{R}^{*}\right)\eta(k+1)+\eta^{2}\sum_{t=0}^{k}\left\|g_{t+1}\right\|_ {*}^{2};\]

and \(\sum_{t=1}^{k+1}Q_{t}^{2}\leq T\mu_{2}+2\sigma\sqrt{T\log\frac{1}{\delta}}\)

Conditioned on this event, we will prove by induction that

\[\mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{k}\right)\] \[\leq R^{2}\coloneqq 16C_{4}^{2}\left(\sigma^{2}+4\mu_{1}^{2}\right)\log \frac{1}{\delta}\eta^{2}T+4D_{0}+4D_{0}\eta\sqrt{T}+4\eta^{2}C_{4}^{2}\left(T \mu_{2}+2\sigma\sqrt{T\log\frac{1}{\delta}}\right)\]

For the base case \(k=0\), it is trivial. Suppose for all \(t\leq k\) we have \(\mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{t}\right)\leq R^{2}\), now we prove for \(t=k+1\). By Lemma 1,

\[\eta^{2}\sum_{t=0}^{k}\left\|g_{t+1}\right\|_{*}^{2}\leq\eta^{2}\sum_{t=0}^{k}Q _{t+1}^{2}\left(C_{1}+C_{2}\left(1+\left\|w_{t}\right\|\right)\right)^{2} \leq\eta^{2}\sum_{t=0}^{k}Q_{t+1}^{2}\left(C_{4}+C_{2}\left\|w_{t}-w_{\mathrm{ ref}}\right\|\right)^{2}\]\[\leq 2\eta^{2}C_{4}^{2}\sum_{t=1}^{k+1}Q_{t}^{2}+2\eta^{2}C_{2}^{2} \sum_{t=0}^{k}Q_{t+1}^{2}\left\|w_{t}-w_{\mathrm{ref}}\right\|^{2}\] \[\leq \eta^{2}\left(2C_{4}^{2}+4C_{2}^{2}R^{2}\right)\left(T\mu_{2}+2 \sigma\sqrt{T\log\frac{1}{\delta}}\right)\]

Therefore

\[\mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{k+1}\right) \leq 8C_{4}^{2}\left(\sigma^{2}+4\mu_{1}^{2}\right)\log\frac{1}{ \delta}\eta^{2}T+2D_{0}+2\left(\mathcal{R}(w_{\mathrm{ref}})-\mathcal{R}^{*} \right)\eta(k+1)\] \[\quad+\eta^{2}\left(2C_{4}^{2}+4C_{2}^{2}R^{2}\right)\left(T\mu_ {2}+2\sigma\sqrt{T\log\frac{1}{\delta}}\right)\] \[\leq\frac{R^{2}}{2}+4\eta^{2}C_{2}^{2}\left(T\mu_{2}+2\sigma\sqrt {T\log\frac{1}{\delta}}\right)R^{2}\leq R^{2}.\]

Finally we obtain, \(\eta\sum_{t=0}^{k}\left(\mathcal{R}(w_{t})-\mathcal{R}^{*}\right)+\mathbf{D}_ {\psi}\left(w_{\mathrm{ref}};w_{k+1}\right)\leq R^{2},\) as needed. 

#### 3.2.2 IID data with polynomial tails

**Theorem 10**.: _Suppose \(\ell\) is convex, \((C_{1},C_{2})\)-quadratically bounded. Given \(T\), \(((x_{t},y_{t}))_{t\leq T}\) are IID samples with \(Q_{t}=\max\left\{1,\left\|x_{t}\right\|_{*}^{2},\left|y_{t}\right|^{2}\right\}\) and for some \(p\geq 2\) there exists \(M\geq\frac{p}{e}\) such that for all \(\lambda\)_

\[\max\left\{\sup_{2\leq r\leq 2p}\left\{\mathbb{E}\left[\left|Q_{t}-\mathbb{E} \left[Q_{t}\right]\right|^{r}\right]\right\},\sup_{2\leq r\leq p}\left\{ \mathbb{E}\left[\left|Q_{t}^{2}-\mathbb{E}\left[Q_{t}^{2}\right]\right|^{r} \right]\right\}\right\}\leq M\]

_Let \(\mu_{1}=\mathbb{E}\left[Q_{t}\right]\) and \(\mu_{2}=\mathbb{E}\left[Q_{t}^{2}\right]\). Suppose that \(w_{\mathrm{ref}}\) satisfies Assumption 4. Then for \(\eta\leq\frac{1}{C_{2}\sqrt{6\left(T\mu_{2}+2M\sqrt{T}\left(\frac{2}{\delta} \right)^{\frac{1}{p}}\right)}}\), with probability at least \(1-3\delta\), for every \(0\leq k\leq T-1\)_

\[\frac{1}{k+1}\sum_{t=0}^{k}\left(\mathcal{R}(w_{t})-\mathcal{R}^{*}\right)+ \frac{\mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{k+1}\right)}{\eta\left(k+1 \right)}\leq\frac{R^{2}}{2\eta\left(k+1\right)}\]

_where \(R=\max\left\{\sqrt{6\left(D_{0}\left(1+\eta\sqrt{T}\right)+\eta^{2}C_{4}^{2} \left(T\mu_{2}+2M\sqrt{T}\left(\frac{2}{\delta}\right)^{\frac{1}{p}}\right) \right)},6\left(\frac{2}{3}\gamma\left(7\left(\frac{MT}{\delta}\right)^{1/2p} +2\mu_{1}\right)+\sqrt{\log\frac{2}{\delta}T\mu_{2}\eta C_{4}}\right)\right\} =O\left(1\right)\), \(\gamma=\max\left\{1,\log\frac{2}{\delta}\right\}\)._

_Remark 11_.: Since \(p\geq 2\), the rate is \(O\left(\frac{1}{T^{1/2}}\log\frac{1}{\delta}+\frac{1}{T^{3/4}}\left(\frac{1}{ \delta}\right)^{\frac{1}{2p}}\right)\). This rate improves over the \(O\left(\left(\frac{1}{T^{1/2}}+\frac{1}{T^{3/4}}\left(\frac{T}{\delta}\right) ^{\frac{1}{2p}}\right)\log\frac{T}{\delta}\right)\) rate by [35] by a polynomial factor \(T^{\frac{1}{2p}}\log\frac{T}{\delta}\) in the high probability regime where \(\delta=\frac{1}{\mathrm{poly}(T)}\).

We will give a proof sketch for this theorem. The full proof is deferred to the appendix.

Proof Sketch.: The heavy tailed distribution of the data does not allow us to analyze the moment generating function. In this case, we rely on the coupling technique as in [35]. Since it is not possible to apply Azuma's inequality due to the bounds on the variables being not measurable, and the variables are heavy tailed, we use truncation technique. We define,

\[v_{t}=\arg\min_{\left\|w-w_{\mathrm{ref}}\right\|\leq R}\left\{\left\langle \eta_{t}g_{t}(v_{t-1}),w\right\rangle+\mathbf{D}_{\psi}\left(w;v_{t-1}\right)\right\}\]

where we use \(g_{t}(v_{t-1})\) to denote the gradient at \(v_{t-1}\) using the same data point \((x_{t},y_{t})\) when computing \(w_{t}\) and we define

\[U_{t}=\left(\mathcal{R}(v_{t})-\mathcal{R}(w_{\mathrm{ref}})+\ell_{t+1}\left(w _{\mathrm{ref}}\right)-\ell_{t+1}\left(v_{t}\right)\right)\]\[P_{t} =\begin{cases}U_{t}&\text{if }\left|U_{t}\right|\leq\left(A+2\mu_{1} \right)RC_{4}\\ \left(A+2\mu_{1}\right)RC_{4}\text{sign}\left(U_{t}\right)&\text{otherwise} \end{cases}\] \[\text{where }A =\left(\frac{MT}{\delta}\right)^{1/2p}\text{ and }B_{t}=U_{t}-P_{t}.\]

We can write

\[\sum_{t=0}^{k}U_{t}=\sum_{t=0}^{k}\left(P_{t}-\mathbb{E}\left[P_{t}\mid\mathcal{ F}_{t}\right]\right)+\sum_{t=0}^{k}\mathbb{E}\left[P_{t}\mid\mathcal{F}_{t} \right]+\sum_{t=0}^{k}B_{t}\]

We bound \(\sum_{t=0}^{k}\left(P_{t}-\mathbb{E}\left[P_{t}\mid\mathcal{F}_{t}\right]\right)\) by applying Freedman's inequality. The terms \(\sum_{t=0}^{k}\mathbb{E}\left[P_{t}\mid\mathcal{F}_{t}\right]\) and \(\sum_{t=0}^{k}B_{t}\) are both the bias terms can be bounded by analyzing the tail of the distribution and Markov's inequality. We also use Lemma 12 to bound \(\sum_{t=0}^{k}\left\|y_{t+1}(v_{t})\right\|_{*}^{2}\). Finally, using the induction technique, we can prove that \(w_{t}=v_{t}\) with high probability and obtain the desired result. 

**Lemma 12** (Lemma A.5 from [35]).: _With probability \(\geq 1-\delta\), \(\sum_{t=1}^{T}Q_{t}^{2}\leq T\mu_{2}+2M\sqrt{T}\left(\frac{2}{\delta}\right)^ {\frac{1}{p}}.\)_

## 4 Generalization bounds of SMD for Markovian data

The final result we present in this paper is the following theorem for the case when the data are sampled from a Markov chain.

**Theorem 13**.: _Suppose \(\ell\) is convex, \(\left(C_{1},C_{2}\right)\)-quadratically bounded. Given \(T\), \(\left((x_{t},y_{t})\right)_{t\leq T}\) are sampled from a Markov chain with \(\max\left\{\left\|x_{t}\right\|_{*}^{2},\left|y_{t}\right|^{2}\right\}\leq 1\) and \(\left(\pi,\tau,\epsilon=\frac{1}{\sqrt{T}}\right)\) is an approximate stationarity witness. Suppose that \(w_{\text{ref}}\) satisfies Assumption 4. Then for \(\eta\leq\frac{1}{2C_{2}\sqrt{T(1+2\tau)}}\), with probability at least \(1-\tau\delta\), for every \(0\leq k\leq T-1\)_

\[\frac{1}{k+1}\sum_{t=0}^{k}\left(\mathcal{R}(w_{t})-\mathcal{R}^{*}\right)+ \frac{\mathbf{D}_{\psi}\left(w_{\text{ref}};w_{k+1}\right)}{\eta\left(k+1 \right)}\leq\frac{R^{2}}{2\eta\left(k+1\right)}.\]

_where \(R=\max\left\{\sqrt{6\left(2D_{0}+2\eta D_{0}\sqrt{T}+16\eta^{2}C_{4}^{2}T\tau \log\frac{1}{\delta}+2T\eta^{2}C_{4}^{2}+4\eta^{2}\tau TC_{4}^{2}\right)},6(2 \eta\tau C_{4}+2\eta C_{4}\epsilon T+4\eta\tau C_{4})\right\}=O\left(1\right)\) and \(C_{4}=C_{1}+C_{2}(1+\left\|w_{\text{ref}}\right\|)\)._

We will give a proof sketch for this theorem.

Proof Sketch.: The proof of this Theorem follow similarly to that of Theorem 7. The difference here is we need to bound \(\tau\) different martingale difference sequences in the form of

\[\mathbb{E}\left[\ell_{\tau(i+1)+j}\left(w_{\text{ref}}\right)\mid\mathcal{F} _{\tau i+j}\right]-\mathbb{E}\left[\ell_{\tau(i+1)+j}\left(w_{\tau i+j}\right) \mid\mathcal{F}_{\tau i+j}\right]+\ell_{\tau(i+1)+j}\left(w_{\text{ref}}\right) -\ell_{\tau(i+1)+j}\left(w_{\tau i+j}\right)\]

for \(0\leq j\leq\tau-1\), \(0\leq i\leq\frac{T-1-j}{\tau}\). We also need the assumption on the approximate stationarity witness to see that

\[\left|\mathcal{R}(w_{t})-\mathcal{R}(w_{\text{ref}})-\mathbb{E}\left[\ell_{t+ \tau}\left(w_{\text{ref}}\right)\mid\mathcal{F}_{t}\right]+\mathbb{E}\left[ \ell_{t+\tau}\left(w_{t}\right)\mid\mathcal{F}_{t}\right]\right|\leq C_{4}R\epsilon.\]

Now we only need the union bound over \(\tau\) sequences, instead of all iterations. The success probability will decrease from \(1-\delta\) to \(1-\tau\delta\). 

## 5 Conclusion

In this paper, we show a new approach to analyze the generalization error of SMD for quadratically bounded losses. Our approach improves a logarithmic factor for the realizable setting and non-realizable setting with light tailed data and a \(\operatorname{poly}\)\(T\) factor for the non-realizable setting with polynomial tailed data from the prior work by [35]. An inherent limitation of the current approach is the assumption that we can obtain a fresh sample in each iteration, whereas the setting with finite training data is still not well understood. In the realizable setting, we require that the data is bounded, as opposed to more relaxed assumptions in the non-realizable settings. We leave the question of resolving these issues for future works.

## Acknowledgement

TDN and AE were supported in part by NSF CAREER grant CCF-1750333, NSF grant III-1908510, and an Alfred P. Sloan Research Fellowship. HN was supported by NSF CAREER grant CCF-1750716 and NSF grant 2311649.

## References

* [1] Raef Bassily, Vitaly Feldman, Cristobal Guzman, and Kunal Talwar. Stability of stochastic gradient descent on nonsmooth convex losses. _Advances in Neural Information Processing Systems_, 33:4381-4391, 2020.
* [2] Olivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy. Sharper bounds for uniformly stable algorithms. In _Conference on Learning Theory_, pages 610-626. PMLR, 2020.
* [3] Radu Alexandru Dragomir, Mathieu Even, and Hadrien Hendrikx. Fast stochastic bregman gradient methods: Sharp analysis and variance reduction. In _International Conference on Machine Learning_, pages 2815-2825. PMLR, 2021.
* [4] John C Duchi, Alekh Agarwal, Mikael Johansson, and Michael I Jordan. Ergodic mirror descent. _SIAM Journal on Optimization_, 22(4):1549-1578, 2012.
* [5] Vitaly Feldman and Jan Vondrak. Generalization bounds for uniformly stable algorithms. _Advances in Neural Information Processing Systems_, 31, 2018.
* [6] Vitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable algorithms with nearly optimal rate. In _Conference on Learning Theory_, pages 1270-1279. PMLR, 2019.
* [7] David A Freedman. On tail probabilities for martingales. _the Annals of Probability_, pages 100-118, 1975.
* [8] Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework. _SIAM Journal on Optimization_, 22(4):1469-1492, 2012.
* [9] Eduard Gorbunov, Marina Danilova, David Dobre, Pavel Dvurechenskii, Alexander Gasnikov, and Gauthier Gidel. Clipped stochastic methods for variational inequalities with heavy-tailed noise. _Advances in Neural Information Processing Systems_, 35:31319-31332, 2022.
* [10] Eduard Gorbunov, Marina Danilova, and Alexander Gasnikov. Stochastic optimization with heavy-tailed noise via accelerated gradient clipping. _Advances in Neural Information Processing Systems_, 33:15042-15053, 2020.
* [11] Eduard Gorbunov, Marina Danilova, Innokentiy Shibaev, Pavel Dvurechensky, and Alexander Gasnikov. Near-optimal high probability complexity bounds for non-smooth stochastic optimization with heavy-tailed noise. _arXiv preprint arXiv:2106.05958_, 2021.
* [12] Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In _International conference on machine learning_, pages 1225-1234. PMLR, 2016.
* [13] Nicholas JA Harvey, Christopher Liaw, Yaniv Plan, and Sikander Randhawa. Tight analyses for non-smooth stochastic gradient descent. In _Conference on Learning Theory_, pages 1579-1613. PMLR, 2019.
* [14] Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization. _The Journal of Machine Learning Research_, 15(1):2489-2512, 2014.
* [15] Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. _arXiv preprint arXiv:1803.07300_, 2018.

* [16] Sham M Kakade and Ambuj Tewari. On the generalization ability of online strongly convex programming algorithms. _Advances in Neural Information Processing Systems_, 21, 2008.
* [17] Ali Kavis, Kfir Yehuda Levy, and Volkan Cevher. High probability bounds for a class of nonconvex algorithms with adagrad stepsize. In _International Conference on Learning Representations_, 2021.
* [18] Ahmed Khaled and Peter Richtarik. Better theory for sgd in the nonconvex world. _arXiv preprint arXiv:2002.03329_, 2020.
* [19] Shaojie Li and Yong Liu. High probability guarantees for nonconvex stochastic gradient descent with heavy tails. In _International Conference on Machine Learning_, pages 12931-12963. PMLR, 2022.
* [20] Xiaoyu Li and Francesco Orabona. A high probability analysis of adaptive sgd with momentum. _arXiv preprint arXiv:2007.14294_, 2020.
* [21] Zijian Liu, Ta Duy Nguyen, Thien Hang Nguyen, Alina Ene, and Huy Le Nguyen. High probability convergence of stochastic gradient methods. _arXiv preprint arXiv:2302.14843_, 2023.
* [22] Zijian Liu, Jiawei Zhang, and Zhengyuan Zhou. Breaking the lower bound with (little) structure: Acceleration in non-convex stochastic optimization with heavy-tailed noise. _arXiv preprint arXiv:2302.06763_, 2023.
* [23] Liam Madden, Emiliano Dall'Anese, and Stephen Becker. High probability convergence and uniform stability bounds for nonconvex stochastic gradient descent. _arXiv preprint arXiv:2006.05610_, 2020.
* [24] Eric Moulines and Francis Bach. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. _Advances in neural information processing systems_, 24, 2011.
* [25] Alexander V Nazin, Arkadi S Nemirovsky, Alexandre B Tsybakov, and Anatoli B Juditsky. Algorithms of robust stochastic optimization based on mirror descent method. _Automation and Remote Control_, 80(9):1607-1627, 2019.
* [26] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. _SIAM Journal on optimization_, 19(4):1574-1609, 2009.
* [27] Ta Duy Nguyen, Thien Hang Nguyen, Alina Ene, and Huy Le Nguyen. High probability convergence of clipped-sgd under heavy-tailed noise. _arXiv preprint arXiv:2302.05437_, 2023.
* [28] Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for strongly convex stochastic optimization. _arXiv preprint arXiv:1109.5647_, 2011.
* [29] Abdurakhmon Sadiev, Marina Danilova, Eduard Gorbunov, Samuel Horvath, Gauthier Gidel, Pavel Dvurechensky, Alexander Gasnikov, and Peter Richtarik. High-probability bounds for stochastic optimization and variational inequalities: the case of unbounded variance. _arXiv preprint arXiv:2302.00999_, 2023.
* [30] Matan Schliserman and Tomer Koren. Stability vs implicit bias of gradient methods on separable data and beyond. In _Conference on Learning Theory_, pages 3380-3394. PMLR, 2022.
* [31] Matan Schliserman and Tomer Koren. Tight risk bounds for gradient descent on separable data. _arXiv preprint arXiv:2303.01135_, 2023.
* [32] Ohad Shamir. Gradient methods never overfit on separable data. _The Journal of Machine Learning Research_, 22(1):3847-3866, 2021.
* [33] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. _The Journal of Machine Learning Research_, 19(1):2822-2878, 2018.

* [34] Matus Telgarsky. Margins, shrinkage, and boosting. In _International Conference on Machine Learning_, pages 307-315. PMLR, 2013.
* [35] Matus Telgarsky. Stochastic linear optimization never overfits with quadratically-bounded losses on general data. In _Conference on Learning Theory_, pages 5453-5488. PMLR, 2022.
* [36] Joel Tropp. Freedman's inequality for matrix martingales. 2011.
* [37] R van Handel. Apc 550: Probability in high dimension. _Lecture Notes. Princeton University. Retrieved from https://web. math. princeton. edu/rvan/APC550. pdf on December_, 21:2016, 2016.
* [38] Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.

## Appendix A Concentration Inequalities

**Lemma 14**.: _Let \(X\) be a random variable such that \(\mathbb{E}\left[X\right]=0\) and \(\left|X\right|\leq R\) almost surely. Then for \(0\leq\lambda\leq\frac{1}{R}\)_

\[\mathbb{E}\left[\exp\left(\lambda X\right)\right]\leq\exp\left( \frac{3}{4}\lambda^{2}\mathbb{E}\left[X^{2}\right]\right).\]

The following lemma is similar to Lemma 2.2 in [21].

**Lemma 15**.: _Suppose that \(Q\) satisfies for all \(0\leq\lambda\leq\frac{1}{\sigma}\), \(\mathbb{E}\left[\exp\left(\lambda^{2}Q^{2}\right)\right]\leq\exp\left(\lambda ^{2}\sigma^{2}\right).\) Then for variable \(X\) such that \(\mathbb{E}\left[X\right]=0\) and \(\left|X\right|\leq a\left(Q+b\right)\) for some \(a\geq 0\) then for all \(\lambda\geq 0\)_

\[\mathbb{E}\left[\exp\left(\lambda X\right)\right]\leq\exp\left( 2\lambda^{2}a^{2}\left(\sigma^{2}+b^{2}\right)\right).\]

_In particular, if \(b=0\) we can have a tighter constant: \(\mathbb{E}\left[\exp\left(\lambda X\right)\right]\leq\exp\left(\lambda^{2}a^{ 2}\sigma^{2}\right).\)_

Proof.: We consider \(\mathbb{E}\left[\exp\left(\lambda X\right)\right]\)

If \(0\leq\lambda\leq\frac{1}{\sqrt{2a\sigma}}\) then using \(\exp\left(x\right)\leq x+\exp\left(x^{2}\right)\)

\[\mathbb{E}\left[\exp\left(\lambda X\right)\right] \leq\mathbb{E}\left[\exp\left(\lambda^{2}X^{2}\right)\right]\] \[\leq\mathbb{E}\left[\exp\left(\lambda^{2}a^{2}\left(Q+b\right)^{ 2}\right)\right]\] \[\leq\mathbb{E}\left[\exp\left(2\lambda^{2}a^{2}Q^{2}+2\lambda^{2} a^{2}b^{2}\right)\right]\] \[\leq\exp\left(2\lambda^{2}a^{2}b^{2}\right)\mathbb{E}\left[\exp \left(2\lambda^{2}a^{2}Q^{2}\right)\right]\] \[\leq\exp\left(2\lambda^{2}a^{2}\left(\sigma^{2}+b^{2}\right)\right)\]

Otherwise \(\frac{1}{\sigma}\leq\lambda\sqrt{2a}\)

\[\mathbb{E}\left[\exp\left(\lambda X\right)\right] \leq\mathbb{E}\left[\exp\left(\lambda^{2}a^{2}\sigma^{2}+\frac{X ^{2}}{4a^{2}\sigma^{2}}\right)\right]\] \[\leq\exp\left(\lambda^{2}a^{2}\sigma^{2}\right)\mathbb{E}\left[ \exp\left(\frac{Q^{2}+b^{2}}{2\sigma^{2}}\right)\right]\] \[\leq\exp\left(\lambda^{2}a^{2}\sigma^{2}\right)\exp\left(\frac{b^ {2}}{2\sigma^{2}}\right)\exp\left(\frac{1}{2}\right)\] \[\leq\exp\left(\lambda^{2}a^{2}\sigma^{2}\right)\exp\left(\lambda ^{2}a^{2}b^{2}\right)\exp\left(\lambda^{2}a^{2}\sigma^{2}\right)\] \[\leq\exp\left(2\lambda^{2}a^{2}\left(\sigma^{2}+b^{2}\right) \right).\]

**Theorem 16** (Freedman's inequality [7, 36]).: _Let \(\left(X_{t}\right)_{t\geq 1}\) be a martingale difference sequence. Assume that there exists a constant \(c\) such that \(\left|X_{t}\right|\leq c\) almost surely for all \(t\geq 1\) and define \(\sigma_{t}^{2}=\mathbb{E}\left[X_{t}^{2}\mid X_{t-1},\ldots,X_{1}\right]\). Then for all \(b>0\), \(F>0\) and \(T\geq 1\)_

\[\Pr\left[\exists T\geq 1:\left|\sum_{t=1}^{T}X_{t}\right|>b\text{ and }\sum_{t=1}^{T}\sigma_{t}^{2}\leq F\right]\leq 2 \exp\left(-\frac{b^{2}}{2F+2cb/3}\right).\]

## Appendix B Missing Proofs

Proof of Lemma 2.: Using the optimality condition

\[\left\langle\eta g_{t+1}+\nabla\psi(w_{t+1})-\nabla\psi(w_{t}),w_{\mathrm{ref }}-w_{t+1}\right\rangle\geq 0\]we have

\[\left\langle\eta g_{t+1},w_{t}-w_{\mathrm{ref}}\right\rangle =\left\langle\eta g_{t+1},w_{t+1}-w_{\mathrm{ref}}\right\rangle+ \left\langle\eta g_{t+1},w_{t}-w_{t+1}\right\rangle\] \[\leq\left\langle\nabla\psi(w_{t+1})-\nabla\psi(w_{t}),w_{ \mathrm{ref}}-w_{t+1}\right\rangle+\left\langle\eta g_{t+1},w_{t}-w_{t+1}\right\rangle\] \[=\mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{t}\right)-\mathbf{D}_ {\psi}\left(w_{\mathrm{ref}};w_{t+1}\right)-\mathbf{D}_{\psi}\left(w_{t+1};w_{ t}\right)+\left\langle\eta g_{t+1},w_{t}-w_{t+1}\right\rangle\] \[\leq\mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{t}\right)-\mathbf{ D}_{\psi}\left(w_{\mathrm{ref}};w_{t+1}\right)-\frac{1}{2}\left\|w_{t}-w_{t+1} \right\|^{2}+\left\langle\eta g_{t+1},w_{t}-w_{t+1}\right\rangle\] \[\leq\mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{t}\right)-\mathbf{ D}_{\psi}\left(w_{\mathrm{ref}};w_{t+1}\right)+\frac{\eta^{2}}{2}\left\|g_{t+1} \right\|_{*}^{2}\]

Hence

\[\mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{t+1}\right)-\mathbf{D}_ {\psi}\left(w_{\mathrm{ref}};w_{t}\right) \leq\left\langle\eta g_{t+1},w_{\mathrm{ref}}-w_{t}\right\rangle+ \frac{\eta^{2}}{2}\left\|g_{t+1}\right\|_{*}^{2}\] \[\leq\eta\left(\ell_{t+1}\left(w_{\mathrm{ref}}\right)-\ell_{t+1} \left(w_{t}\right)\right)+\frac{\eta^{2}}{2}\left\|g_{t+1}\right\|_{*}^{2}\]

as needed. 

Proof of Lemma 4.: We have

\[\mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{t+1}\right)-\mathbf{D}_ {\psi}\left(w_{\mathrm{ref}};w_{t}\right) \leq\eta\left(\ell_{t+1}\left(w_{\mathrm{ref}}\right)-\ell_{t+1} \left(w_{t}\right)\right)+\frac{\eta^{2}}{2}\left\|g_{t+1}\right\|_{*}^{2}\] \[\leq\eta\left(\ell_{t+1}\left(w_{\mathrm{ref}}\right)-\ell_{t+1} \left(w_{t}\right)\right)+\frac{\eta^{2}}{2}\ell_{t+1}^{\prime}(w_{t})^{2}\] \[\leq\eta\left(\ell_{t+1}\left(w_{\mathrm{ref}}\right)-\ell_{t+1} \left(w_{t}\right)\right)+\eta^{2}\rho\ell_{t+1}(w_{t})\] \[=\eta\ell_{t+1}\left(w_{\mathrm{ref}}\right)-\frac{\eta}{2}\ell_{t +1}(w_{t})\leq\eta\ell_{t+1}\left(w_{\mathrm{ref}}\right).\]

Summing up, we have, for any \(0\leq t\leq T\)

\[\mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{t}\right)\leq\mathbf{D}_{\psi} \left(w_{\mathrm{ref}};w_{0}\right)+\eta\sum_{i=1}^{t}\ell_{i}\left(w_{ \mathrm{ref}}\right)=D_{0}+\eta\sum_{i=1}^{t}\ell_{i}\left(w_{\mathrm{ref}} \right).\]

Proof of Lemma 5.: We have \(\left|\ell_{i}\left(w_{\mathrm{ref}}\right)-\mathcal{R}(w_{\mathrm{ref}}) \right|\leq\max\left\{\ell_{i}\left(w_{\mathrm{ref}}\right),\mathcal{R}(w_{ \mathrm{ref}})\right\}\leq C_{3}\) thus by lemma 14, for \(\lambda\leq\frac{1}{C_{3}}\)

\[\mathbb{E}\left[\exp\left(\lambda\left(\ell_{i}\left(w_{\mathrm{ ref}}\right)-\mathcal{R}(w_{\mathrm{ref}})\right)\right)\right]\] \[\leq \exp\left(\frac{3}{4}\lambda^{2}\mathbb{E}\left[\left(\ell_{i} \left(w_{\mathrm{ref}}\right)-\mathcal{R}(w_{\mathrm{ref}})\right)^{2}\right]\right)\] \[\overset{(a)}{\leq}\exp\left(\frac{3}{4}\lambda^{2}\mathbb{E} \left[\ell_{i}\left(w_{\mathrm{ref}}\right)^{2}\right]\right)\] \[\overset{(b)}{\leq}\exp\left(\frac{3}{4}\lambda^{2}C_{3}\mathcal{ R}(w_{\mathrm{ref}})\right)\leq\exp\left(\frac{3}{4}\lambda\mathcal{R}(w_{ \mathrm{ref}})\right),\]

where for \((a)\) we use \(\mathbb{E}\left[\left(X-\mathbb{E}\left[X\right]\right)^{2}\right]\leq\mathbb{ E}\left[X^{2}\right]\) and for \((b)\) we use \(\ell_{i}\left(w_{\mathrm{ref}}\right)\leq C_{3}\). Since \(\ell_{i}\left(w_{\mathrm{ref}}\right)\) are independent random variables, we have

\[\mathbb{E}\left[\exp\left(\lambda\sum_{i=1}^{T}\left(\ell_{i} \left(w_{\mathrm{ref}}\right)-\mathcal{R}(w_{\mathrm{ref}})\right)\right)\right] =\mathbb{E}\left[\prod_{i=1}^{T}\exp\left(\lambda\left(\ell_{i} \left(w_{\mathrm{ref}}\right)-\mathcal{R}(w_{\mathrm{ref}})\right)\right)\right]\] \[= \prod_{i=1}^{T}\mathbb{E}\left[\exp\left(\lambda\left(\ell_{i} \left(w_{\mathrm{ref}}\right)-\mathcal{R}(w_{\mathrm{ref}})\right)\right)] \leq\prod_{i=1}^{T}\exp\left(\frac{3}{4}\lambda\mathcal{R}(w_{\mathrm{ref}} )\right)=\exp\left(\frac{3}{4}\lambda\mathcal{T}\mathcal{R}(w_{\mathrm{ref}}) \right).\]Hence by Markov's inequality

\[\Pr\left[\lambda\sum_{i=1}^{T}\left(\ell_{i}\left(w_{\mathrm{ref}} \right)-\mathcal{R}(w_{\mathrm{ref}})\right)\geq\frac{3}{4}\lambda T\mathcal{R} (w_{\mathrm{ref}})+\log\frac{1}{\delta}\right]\] \[= \Pr\left[\exp\left(\lambda\sum_{i=1}^{T}\left(\ell_{i}\left(w_{ \mathrm{ref}}\right)-\mathcal{R}(w_{\mathrm{ref}})\right)\right)\geq\frac{1}{ \delta}\exp\left(\frac{3}{4}\lambda T\mathcal{R}(w_{\mathrm{ref}})\right)\right]\] \[\leq \frac{\mathbb{E}\left[\exp\left(\lambda\sum_{i=1}^{T}\left(\ell_{ i}\left(w_{\mathrm{ref}}\right)-\mathcal{R}(w_{\mathrm{ref}})\right)\right) \right]}{\frac{1}{\delta}\exp\left(\frac{3}{4}\lambda T\mathcal{R}(w_{\mathrm{ ref}})\right)}\leq\delta\]

Choose \(\lambda=\frac{1}{C_{3}}\) we have with probability at least \(1-\delta\)

\[\sum_{i=1}^{T}\left(\ell_{i}\left(w_{\mathrm{ref}}\right)-\mathcal{R}(w_{ \mathrm{ref}})\right)\leq\frac{3}{4}T\mathcal{R}(w_{\mathrm{ref}})+C_{3}\log \frac{1}{\delta}.\]

Proof of Theorem 3.: Towards bounding the risk \(\sum_{t=0}^{k}\mathcal{R}(w_{t})\), we define random variables

\[Z_{t} =\frac{1}{2}z_{t}\eta\left(\mathcal{R}(w_{t})-\mathcal{R}(w_{ \mathrm{ref}})-\ell_{t+1}\left(w_{\mathrm{ref}}\right)\right)+z_{t}\left( \mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{t+1}\right)-\mathbf{D}_{\psi}\left( w_{\mathrm{ref}};w_{t}\right)\right)\] \[\quad-\frac{3}{16}z_{t}\eta\left(\mathcal{R}(w_{\mathrm{ref}})+ \mathcal{R}(w_{t})\right);\qquad\forall 0\leq t\leq T-1\] \[\text{where }z_{t} =\frac{1}{\eta C_{4}\sqrt{2\eta\gamma C_{3}+2D_{0}+2\eta\sum_{i=1 }^{t}\ell_{i}\left(w_{\mathrm{ref}}\right)}};\quad\gamma=\max\left\{1,\log \frac{1}{\delta}\right\}\] \[\text{and }S_{t} =\sum_{i=0}^{t}Z_{i};\qquad\forall 0\leq t\leq T-1\]

The reason to define these variables is because from Lemma 4, we can bound

\[\mathbb{E}\left[\exp\left(Z_{t}\right)\mid\mathcal{F}_{t}\right] \times\exp\left(\frac{3}{16}z_{t}\eta\left(\mathcal{R}(w_{\mathrm{ref}})+ \mathcal{R}(w_{t})\right)\right)\] \[\leq \mathbb{E}\left[\exp\left(\frac{1}{2}z_{t}\eta\left(\mathcal{R}( w_{t})-\mathcal{R}(w_{\mathrm{ref}})-\ell_{t+1}\left(w_{\mathrm{ref}}\right) \right)+z_{t}\left(\eta\ell_{t+1}\left(w_{\mathrm{ref}}\right)-\frac{\eta}{2} \ell_{t+1}(w_{t})\right)\right)\mid\mathcal{F}_{t}\right]\] \[= \mathbb{E}\left[\exp\left(\frac{1}{2}z_{t}\eta\left(\mathcal{R}( w_{t})-\mathcal{R}(w_{\mathrm{ref}})+\ell_{t+1}(w_{\mathrm{ref}})-\ell_{t+1}(w_{t}) \right)\right)\mid\mathcal{F}_{t}\right]\]

where now inside the expectation, we have the term \(\mathcal{R}(w_{t})-\mathcal{R}(w_{\mathrm{ref}})+\ell_{t+1}(w_{\mathrm{ref}})- \ell_{t+1}(w_{t})\) which has expectation \(0\). This reminds us of Lemma 14. To use this lemma, we notice that, by the assumption that the samples are IID with \(\max\left\{\left\|x\right\|_{*},\left|y\right\|\right\}\leq 1\) and Lemma 1,

\[\left|\ell_{x,y}(w_{\mathrm{ref}})-\ell_{x,y}(w_{t})\right|\leq\left\|w_{ \mathrm{ref}}-w_{t}\right\|(\underbrace{C_{1}+C_{2}(1+\left\|w_{\mathrm{ref}} \right\|))}_{C_{4}}\]

We also have

\[\left|\mathcal{R}(w_{t})-\mathcal{R}(w_{\mathrm{ref}})\right|=\left|\mathbb{E} \left[\ell_{x,y}(w_{\mathrm{ref}})-\ell_{x,y}(w_{t})\right]\right|\leq C_{4} \left\|w_{\mathrm{ref}}-w_{t}\right\|\]

Therefore

\[\left|\frac{\eta}{2}\left(\mathcal{R}(w_{t})-\mathcal{R}(w_{\mathrm{ref}})+ \ell_{t+1}(w_{\mathrm{ref}})-\ell_{t+1}(w_{t})\right)\right|\leq\eta C_{4} \left\|w_{\mathrm{ref}}-w_{t}\right\|\]

By the choice of \(z_{t}\) we have

\[z_{t}\leq\frac{1}{\eta C_{4}\sqrt{2\eta C_{3}+2D_{0}+2\eta\sum_{i=1}^{t}\ell_{ i}\left(w_{\mathrm{ref}}\right)}}\leq\frac{1}{\eta C_{4}\sqrt{2\mathbf{D}_{ \psi}\left(w_{\mathrm{ref}};w_{t}\right)}}\leq\frac{1}{\eta C_{4}\left\|w_{ \mathrm{ref}}-w_{t}\right\|}\]Now we can apply Lemma 14 to bound

\[\mathbb{E}\left[\exp\left(Z_{t}\right)\mid\mathcal{F}_{t}\right]\times \exp\left(\frac{3}{16}z_{t}\eta\left(\mathcal{R}(w_{\mathrm{ref}})+\mathcal{R}( w_{t})\right)\right)\] \[\leq \exp\left(\frac{3}{4}\frac{1}{4}z_{t}^{2}\eta^{2}\mathbb{E}\left[ \left(\mathcal{R}(w_{t})-\mathcal{R}(w_{\mathrm{ref}})+\ell_{t+1}(w_{\mathrm{ ref}})-\ell_{t+1}(w_{t})\right)^{2}\mid\mathcal{F}_{t}\right]\right)\] \[\leq \exp\left(\frac{3}{16}z_{t}^{2}\eta^{2}\mathbb{E}\left[\left(\ell_ {t+1}(w_{\mathrm{ref}})-\ell_{t+1}(w_{t})\right)^{2}\mid\mathcal{F}_{t}\right]\right)\] \[\leq \exp\left(\frac{3}{16}z_{t}^{2}\eta^{2}C_{4}\left\|w_{\mathrm{ref} }-w_{t}\right\|\mathbb{E}\left[\ell_{t+1}(w_{\mathrm{ref}})+\ell_{t+1}(w_{t}) \mid\mathcal{F}_{t}\right]\right)\] \[\leq \exp\left(\frac{3}{16}z_{t}\eta\left(\mathcal{R}(w_{\mathrm{ref} })+\mathcal{R}(w_{t})\right)\right)\]

Therefore \(\mathbb{E}\left[\exp\left(Z_{t}\right)\mid\mathcal{F}_{t}\right]\leq 1\) and hence \(\left(\exp\left(S_{t}\right)\right)_{t\geq 0}\) is a supermartingale. By Ville's inequality, we have with probability at least \(1-\delta\), for all \(0\leq k\leq T-1\)

\[\sum_{t=0}^{k}Z_{t}\leq\log\frac{1}{\delta}\]

Expanding this inequality, we obtain

\[\sum_{t=0}^{k}\frac{5}{16}z_{t}\eta\mathcal{R}(w_{t})+z_{k}\mathbf{ D}_{\psi}\left(w_{\mathrm{ref}};w_{k+1}\right)\] \[\leq \log\frac{1}{\delta}+z_{0}\mathbf{D}_{\psi}\left(w_{\mathrm{ref} };w_{0}\right)+\frac{11}{16}\eta\mathcal{R}(w_{\mathrm{ref}})\sum_{t=0}^{k}z_ {t}+\frac{1}{2}\sum_{t=0}^{k}z_{t}\eta\ell_{t+1}(w_{\mathrm{ref}})\] \[\qquad\qquad+\sum_{t=1}^{k}\frac{(z_{t}-z_{t-1})}{\leq 0}\mathbf{D}_{ \psi}\left(w_{\mathrm{ref}};w_{t}\right)\] (1) \[\stackrel{{(a)}}{{\leq}} \log\frac{1}{\delta}+z_{0}D_{0}+\frac{11}{16}\eta\mathcal{R}(w_{ \mathrm{ref}})(k+1)z_{0}+\frac{1}{2}\sum_{t=0}^{k}\frac{\eta\ell_{t+1}(w_{ \mathrm{ref}})}{\eta C_{4}\sqrt{2\eta C_{3}+2D_{0}+2\eta\sum_{i=1}^{t}\ell_{i} \left(w_{\mathrm{ref}}\right)}}\] \[\stackrel{{(b)}}{{\leq}} \log\frac{1}{\delta}+z_{0}D_{0}+\frac{11}{16}\eta\mathcal{R}(w_{ \mathrm{ref}})(k+1)z_{0}+\frac{1}{2\eta C_{4}}\sum_{t=0}^{k}\frac{\eta\ell_{t+ 1}(w_{\mathrm{ref}})}{\sqrt{2D_{0}+2\eta\sum_{i=1}^{t+1}\ell_{i}\left(w_{ \mathrm{ref}}\right)}}\] (2)

For \((a)\) we use the fact that \((z_{t})\) is a decreasing sequence and \(\mathcal{R}(w_{\mathrm{ref}})\leq\frac{\rho D_{0}}{T}\). For \((b)\) we use the assumption \(\ell_{t+1}\left(w_{\mathrm{ref}}\right)\leq C_{3}\). Now notice that we can write \(\frac{\eta\ell_{t+1}(w_{\mathrm{ref}})}{\sqrt{2D_{0}+2\eta\sum_{i=1}^{T+1}\ell _{i}(w_{\mathrm{ref}})}}\leq\sqrt{2D_{0}+2\eta\sum_{i=1}^{t+1}\ell_{i}\left(w_ {\mathrm{ref}}\right)}-\sqrt{2D_{0}+2\eta\sum_{i=1}^{t}\ell_{i}\left(w_{ \mathrm{ref}}\right)}-\sqrt{2D_{0}+2\eta\sum_{i=1}^{t}\ell_{i}\left(w_{ \mathrm{ref}}\right)}\) and sum over \(t\) we obtain

\[\frac{5}{16}z_{k}\eta\sum_{t=0}^{k}\mathcal{R}(w_{t})+z_{k}\mathbf{ D}_{\psi}\left(w_{\mathrm{ref}};w_{k+1}\right)\] \[\leq \log\frac{1}{\delta}+\frac{11(k+1)\mathcal{R}(w_{\mathrm{ref}})}{ 16C_{4}\sqrt{2\eta\gamma C_{3}+2D_{0}}}+\frac{1}{\sqrt{2}\eta C_{4}}\sqrt{D_{ 0}+\eta\sum_{i=1}^{k+1}\ell_{i}\left(w_{\mathrm{ref}}\right)}\]

Hence

\[\sum_{t=0}^{k}\mathcal{R}(w_{t})+\frac{16}{5\eta}\mathbf{D}_{\psi} \left(w_{\mathrm{ref}};w_{k+1}\right)\] \[\leq \frac{16C_{4}}{5}\left(\log\frac{1}{\delta}+\frac{11(k+1) \mathcal{R}(w_{\mathrm{ref}})}{16C_{4}\sqrt{2\eta\gamma C_{3}+2D_{0}}}+\frac{1}{ \sqrt{2}\eta C_{4}}\sqrt{D_{0}+\eta\sum_{i=1}^{T}\ell_{i}\left(w_{\mathrm{ref}} \right)}\right)\]\[\times\sqrt{2\eta\gamma C_{3}+2D_{0}+2\eta\sum_{i=1}^{T}\ell_{i}\left( w_{\mathrm{ref}}\right)}\]

By Lemma 5, with probability at least \(1-\delta\) we have

\[\sum_{i=1}^{T}\ell_{i}\left(w_{\mathrm{ref}}\right)\leq\frac{7}{4}T\mathcal{R}( w_{\mathrm{ref}})+C_{3}\log\frac{1}{\delta}\leq\frac{7}{4}\rho D_{0}+C_{3}\gamma\]

Therefore with probability at least \(1-2\delta\)

\[\sum_{t=0}^{k}\mathcal{R}(w_{t})+\frac{16}{5\eta}\mathbf{D}_{ \psi}\left(w_{\mathrm{ref}};w_{k+1}\right)\] \[\leq \left(\frac{16C_{4}}{5}\log\frac{1}{\delta}+\frac{11(k+1)}{5\sqrt {2\eta\gamma C_{3}+2D_{0}}}\mathcal{R}(w_{\mathrm{ref}})+\frac{8}{5\eta}\sqrt {\frac{15}{4}D_{0}+2\eta\gamma C_{3}}\right)\sqrt{\frac{15}{4}D_{0}+4\eta \gamma C_{3}}\] \[\leq \frac{16C_{4}}{5}\log\frac{1}{\delta}\sqrt{\frac{15}{4}D_{0}+4 \eta\gamma C_{3}}+\left(\frac{6}{\eta}D_{0}+\frac{32}{5}\gamma C_{3}\right)+ 3(k+1)\mathcal{R}(w_{\mathrm{ref}}).\]

which gives us the conclusion. 

Proof of Theorem 10.: First we consider the bounded domain case. Let

\[v_{t}=\arg\min_{\left\|w-w_{\mathrm{ref}}\right\|\leq R}\left\{ \left(\eta_{t}g_{t}(v_{t-1}),w\right)+\mathbf{D}_{\psi}\left(w;v_{t-1}\right)\right\}\]

where we use \(g_{t}(v_{t-1})\) to denote the gradient at \(v_{t-1}\) using the same data point \((x_{t},y_{t})\) when computing \(w_{t}\) and we choose

\[R=\max \left\{\sqrt{6\left(D_{0}+\eta^{2}C_{4}^{2}\left(T\mu_{2}+2M \sqrt{T}\left(\frac{2}{\delta}\right)^{\frac{1}{p}}\right)\right)},\right.\] \[\left.6\left(\frac{2}{3}\gamma\left(7\left(\frac{MT}{\delta} \right)^{1/2p}+2\mu_{1}\right)+\sqrt{\log\frac{2}{\delta}T\mu_{2}}\right)\eta C _{4}\right\}\]

We have

\[\left|\left(\mathcal{R}(v_{t})-\mathcal{R}(w_{\mathrm{ref}})+ \ell_{t+1}\left(w_{\mathrm{ref}}\right)-\ell_{t+1}\left(v_{t}\right)\right)\right|\] \[\leq \left|\ell_{t+1}\left(w_{\mathrm{ref}}\right)-\ell_{t+1}\left(v_ {t}\right)\right|+\left|\mathcal{R}(v_{t})-\mathcal{R}(w_{\mathrm{ref}})\right|\] \[\leq \left(Q_{t}+\mu_{1}\right)\left\|w_{\mathrm{ref}}-v_{t}\right\|C_ {4}\leq\left(Q_{t}+\mu_{1}\right)RC_{4}\] (3)

Let us define the following variables

\[U_{t} =\left(\mathcal{R}(v_{t})-\mathcal{R}(w_{\mathrm{ref}})+\ell_{t+ 1}\left(w_{\mathrm{ref}}\right)-\ell_{t+1}\left(v_{t}\right)\right)\] \[P_{t} =\begin{cases}U_{t}&\text{if }\left|U_{t}\right|\leq\left(A+2\mu_{1} \right)RC_{4}\\ \left(A+2\mu_{1}\right)RC_{4}\mathrm{sign}\left(U_{t}\right)&\text{otherwise} \end{cases}\] \[\text{where }A =\left(\frac{MT}{\delta}\right)^{1/2p}\text{ and }B_{t}=U_{t}-P_{t}.\]

In words, \(U_{t}\) is the variable of our interest and \(P_{t}\) is the truncated version of \(U_{t}\) and \(B_{t}\) is the bias. We would want to control these terms in order to bound \(\sum_{t=0}^{k}U_{t}\). We start with the following decomposition

\[\sum_{t=0}^{k}U_{t}=\sum_{t=0}^{k}\left(P_{t}-\mathbb{E}\left[P_{t}\mid\mathcal{ F}_{t}\right]\right)+\sum_{t=0}^{k}\mathbb{E}\left[P_{t}\mid\mathcal{F}_{t} \right]+\sum_{t=0}^{k}B_{t}\]

First, we consider the term \(\sum_{t=0}^{k}\mathbb{E}\left[P_{t}\mid\mathcal{F}_{t}\right]\).

\[\mathbb{E}\left[P_{t}\mid\mathcal{F}_{t}\right]=\mathbb{E}\left[P_{t}-U_{t }\mid\mathcal{F}_{t}\right]\leq\mathbb{E}\left[\left|P_{t}-U_{t}\right|\mid \mathcal{F}_{t}\right]\] \[= \mathbb{E}\Bigg{[}\left|P_{t}-U_{t}\right|\left(\mathbf{1}\left[ \left|U_{t}\right|\leq(A+2\mu_{1})RC_{4}\right]\right.\] \[\left.\hskip 28.452756pt+\sum_{k=2}^{\infty}\mathbf{1}\left[(k-1)ARC _{4}+2\mu_{1}RC_{4}\leq\left|U_{t}\right|\leq kARC_{4}+2\mu_{1}RC_{4}\right] \right)\right]\] \[= \mathbb{E}\left[\sum_{k=2}^{\infty}\left|P_{t}-U_{t}\right| \mathbf{1}\left[(k-1)ARC_{4}+2\mu_{1}RC_{4}\leq\left|U_{t}\right|\leq kARC_{4 }+2\mu_{1}RC_{4}\right]\right]\] \[\leq \sum_{k=2}^{\infty}\left(kARC_{4}+2\mu_{1}RC_{4}-\left(A+2\mu_{1 }\right)RC_{4}\right)RC_{4}\mathbb{E}\left[\mathbf{1}\left[\left|U_{t}\right| \geq(k-1)ARC_{4}+2\mu_{1}RC_{4}\right]\right]\] \[\leq \sum_{k=1}^{\infty}kARC_{4}\mathbb{E}\left[\mathbf{1}\left[\left( Q_{t}+\mu_{1}\right)RC_{4}\geq kARC_{4}+2\mu_{1}RC_{4}\right]\right]\qquad(\text{ due to }3)\] \[= \sum_{k=1}^{\infty}kARC_{4}\Pr\left[Q_{t}\geq kA+\mu_{1}\right] \leq ARC_{4}\sum_{k=1}^{\infty}k\Pr\left[\left|Q_{t}-\mu_{1}\right|^{2p}\geq( kA)^{2p}\right]\] \[\leq ARC_{4}\sum_{k=1}^{\infty}\frac{Mk}{k^{2p}A^{2p}}=A^{1-2p}RC_ {4}M\sum_{k=1}^{\infty}k^{1-2p}\leq 2A^{1-2p}RC_{4}M\]

where the last inequality is because \(p\geq 2\). We obtain

\[\sum_{t=0}^{k}\mathbb{E}\left[P_{t}\mid\mathcal{F}_{t}\right]\leq 2A^{1-2p}RC_{4}MT\]

The term \(\sum_{t=0}^{k}B_{t}\leq\sum_{t=0}^{k}\left|B_{t}\right|\leq\sum_{t=0}^{T-1} \left|B_{t}\right|\) will be bounded by Markov inequality. From the above deduction,

\[\mathbb{E}\left[\sum_{t=0}^{T-1}\left|B_{t}\right|\right]=\sum_{t=0}^{T-1} \mathbb{E}\left[\left|B_{t}\right|\right]=\sum_{t=0}^{T-1}\mathbb{E}\left[ \mathbb{E}\left[\left|U_{t}-P_{t}\right|\mid\mathcal{F}_{t}\right]\right]\leq 2A^{1-2p} RC_{4}MT\]

With probability at least \(1-\delta\),

\[\sum_{t=0}^{T-1}\left|B_{t}\right|\leq 2TA^{1-2p}RC_{4}M\frac{1}{\delta}=2RC_{4 }A^{1-2p}\left(\frac{MT}{\delta}\right)\]

Finally, we will use Freedman's inequality to bound the remaining term \(\sum_{t=0}^{k}\left(P_{t}-\mathbb{E}\left[P_{t}\mid\mathcal{F}_{t}\right]\right)\). First, notice that

\[\mathbb{E}\left[\left|P_{t}-\mathbb{E}\left[P_{t}\mid\mathcal{F}_ {t}\right]\right|^{2}\mid\mathcal{F}_{t}\right]\leq\mathbb{E}\left[P_{t}^{2} \mid\mathcal{F}_{t}\right]\] \[\leq\mathbb{E}\left[U_{t}^{2}\mid\mathcal{F}_{t}\right]\leq \mathbb{E}\left[\left(\ell_{t+1}\left(w_{\text{ref}}\right)-\ell_{t+1}\left( v_{t}\right)\right)^{2}\mid\mathcal{F}_{t}\right]\] \[\leq R^{2}C_{4}^{2}\mathbb{E}\left[Q_{t}^{2}\right]\leq R^{2}C_{4 }^{2}\mu_{2}.\]

We have \(\left(P_{t}-\mathbb{E}\left[P_{t}\mid\mathcal{F}_{t}\right]\right)\) is a martingale difference sequence with \(\left|P_{t}-\mathbb{E}\left[P_{t}\mid\mathcal{F}_{t}\right]\right|\leq 2 \left(A+2\mu_{1}\right)RC_{4}\). We can apply Freedman's inequality,

\[\Pr\left[\exists k\geq 0:\left|\sum_{t=0}^{k}P_{t}-\mathbb{E} \left[P_{t}\mid\mathcal{F}_{t}\right]\right|>a\text{ and }\sum_{t=0}^{k}\mathbb{E}\left[\left|P_{t}-\mathbb{E}\left[P_{t}\mid \mathcal{F}_{t}\right]\right|^{2}\mid\mathcal{F}_{t}\right]\leq F\right]\] \[\leq 2\exp\left(\frac{-2a^{2}}{2F+4\left(A+2\mu_{1}\right)RC_{4}a/3}\right)\]

If we select

\[F=T\mu_{2}R^{2}C_{4}^{2}\]\[\text{and}\ a=\frac{2}{3}\log\frac{2}{\delta}\left(A+2\mu_{1}\right)RC_{4}+RC_{4} \sqrt{\log\frac{2}{\delta}T\mu_{2}}\]

we obtain with probability at least \(1-\delta\), for all \(k\geq 0\)

\[\sum_{t=0}^{k}P_{t}-\mathbb{E}\left[P_{t}\mid\mathcal{F}_{t}\right]\leq\frac{2 }{3}\log\frac{2}{\delta}\left(A+2\mu_{1}\right)RC_{4}+RC_{4}\sqrt{\log\frac{2 }{\delta}T\mu_{2}}\]

Therefore with probability at least \(1-3\delta\) we have the following event \(E:\) for all \(k\geq 0\)

\[\sum_{t=0}^{k}U_{t} \leq\frac{2}{3}\log\frac{2}{\delta}\left(A+2\mu_{1}\right)RC_{4} +RC_{4}\sqrt{\log\frac{2}{\delta}T\mu_{2}}+4RC_{4}A^{1-2p}\left(\frac{MT}{ \delta}\right)\] \[\leq\frac{2}{3}\gamma\left(7\left(\frac{MT}{\delta}\right)^{1/2p }+2\mu_{1}\right)RC_{4}+RC_{4}\sqrt{\log\frac{2}{\delta}T\mu_{2}}\] \[\text{and}\ \sum_{t=1}^{k+1}Q_{t}^{2} \leq T\mu_{2}+2M\sqrt{T}\left(\frac{2}{\delta}\right)^{\frac{1}{ p}}.\]

where we denote \(\gamma=\max\left\{1,\log\frac{2}{\delta}\right\}\). Furthermore

\[\frac{\eta^{2}}{2}\sum_{t=0}^{k}\left\|g_{t+1}(v_{t})\right\|_{*}^ {2} \leq\frac{\eta^{2}}{2}\sum_{t=0}^{k}Q_{t+1}^{2}\left(C_{1}+C_{2} \left(1+\left\|v_{t}\right\|\right)\right)^{2}\] \[\leq\frac{\eta^{2}}{2}\sum_{t=0}^{k}Q_{t+1}^{2}\left(C_{4}+C_{2} \left\|v_{t}-w_{\text{ref}}\right\|\right)^{2}\] \[\leq\eta^{2}C_{4}^{2}\sum_{t=1}^{k+1}Q_{t}^{2}+\eta^{2}C_{2}^{2} \sum_{t=0}^{k}Q_{t+1}^{2}\left\|v_{t}-w_{\text{ref}}\right\|^{2}\] \[\leq\eta^{2}\left(C_{4}^{2}+C_{2}^{2}R^{2}\right)\left(T\mu_{2}+2 M\sqrt{T}\left(\frac{2}{\delta}\right)^{\frac{1}{p}}\right)\]

Now we will proceed by induction to show that conditioned on the event \(E\), \(w_{t}=v_{t}\). For the base case, we have \(w_{0}=v_{0}\). Suppose that we have \(w_{t}=v_{t}\) for all \(t\leq k\). We will show that \(w_{k+1}=v_{k+1}\). From Lemma 2, we have

\[\sum_{t=0}^{k}\eta\left(\mathcal{R}(w_{t})-\mathcal{R}^{*}\right)+ \mathbf{D}_{\psi}\left(w_{\text{ref}};w_{k+1}\right)\] \[\leq D_{0}+\sum_{t=0}^{k}\eta\left(\mathcal{R}(w_{\text{ref}})- \mathcal{R}^{*}\right)\] \[+\eta\sum_{t=0}^{k}\left(\mathcal{R}(w_{t})-\mathcal{R}(w_{\text{ ref}})+\ell_{t+1}\left(w_{\text{ref}}\right)-\ell_{t+1}\left(w_{t}\right) \right)+\frac{\eta^{2}}{2}\sum_{t=0}^{k}\left\|g_{t+1}\right\|_{*}^{2}\] \[\leq D_{0}+\eta\sqrt{T}D_{0}+\eta\sum_{t=0}^{k}U_{t}+\frac{\eta^{2}}{2 }\sum_{t=0}^{k}\left\|g_{t+1}(v_{t})\right\|_{*}^{2}\] \[\leq D_{0}\left(1+\eta\sqrt{T}\right)\] \[+\left(\frac{2}{3}\gamma\left(7\left(\frac{MT}{\delta}\right)^{1/ 2p}+2\mu_{1}\right)+\sqrt{\log\frac{2}{\delta}T\mu_{2}}\right)\eta RC_{4}\] \[+\eta^{2}\left(C_{4}^{2}+C_{2}^{2}R^{2}\right)\left(T\mu_{2}+2M \sqrt{T}\left(\frac{2}{\delta}\right)^{\frac{1}{p}}\right)\] \[\leq \frac{R^{2}}{2}\]Thus \(\left\|w_{k+1}-w_{\mathrm{ref}}\right\|\leq R\). And thus \(w_{k+1}=v_{k+1}\). Finally, we can conclude that with probability at least \(1-3\delta\), for all \(0\leq k\leq T-1\)

\[\frac{1}{k+1}\sum_{t=0}^{k}\left(\mathcal{R}(w_{t})-\mathcal{R}(w_{\mathrm{ref }})\right)+\frac{\mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{k+1}\right)}{\eta \left(k+1\right)}\leq\frac{R^{2}}{2\eta\left(k+1\right)}.\]

Proof of Theorem 13.: For \(0\leq j\leq\tau-1\), we define

\[Z_{i}^{j} =z_{\tau i+j}\eta\left(\mathbb{E}\left[\ell_{\tau(i+1)+j}\left(w_{ \mathrm{ref}}\right)\mid\mathcal{F}_{\tau i+j}\right]-\mathbb{E}\left[\ell_{ \tau(i+1)+j}\left(w_{\tau i+j}\right)\mid\mathcal{F}_{\tau i+j}\right]\right)\] \[\quad+z_{\tau i+j}\eta\left(\ell_{\tau(i+1)+j}\left(w_{\mathrm{ ref}}\right)-\ell_{\tau(i+1)+j}\left(w_{\tau i+j}\right)\right)-8\left(z_{\tau i+j} \eta\right)^{2}C_{4}^{2}\mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{\tau i+j} \right),\] \[\quad\forall 0\leq i\leq\frac{T-1-j}{\tau}\] \[S_{k}^{j} =\sum_{i=0}^{k}Z_{i}^{j},\forall 0\leq k\leq\frac{T-1-j}{\tau}\]

where

\[z_{t} =\frac{1}{8\eta^{2}C_{4}^{2}\left(T+1+t\right)} \forall-1\leq t\leq T-1\]

We bound

\[\left|\mathbb{E}\left[\ell_{\tau(i+1)+j}\left(w_{\mathrm{ref}} \right)\mid\mathcal{F}_{\tau i+j}\right]-\mathbb{E}\left[\ell_{\tau(i+1)+j} \left(w_{\tau i+j}\right)\mid\mathcal{F}_{\tau i+j}\right]\right.\] \[\quad+\ell_{\tau(i+1)+j}\left(w_{\mathrm{ref}}\right)-\ell_{\tau (i+1)+j}\left(w_{\tau i+j}\right)\Bigg{|}\] \[\leq 2C_{4}\left\|w_{\mathrm{ref}}-w_{\tau i+j}\right\|\]

By Lemma 15

\[\mathbb{E}\left[\exp\left(Z_{i}^{j}\right)\mid\mathcal{F}_{\tau i +j}\right]\] \[= \exp\left(-8\left(z_{\tau i+j}\eta\right)^{2}C_{4}^{2}\mathbf{D} _{\psi}\left(w_{\mathrm{ref}};w_{\tau i+j}\right)\right)\] \[\times\mathbb{E}\Bigg{[}\exp\left(z_{\tau i+j}\eta\Bigg{(} \mathbb{E}\left[\ell_{\tau(i+1)+j}\left(w_{\mathrm{ref}}\right)\mid\mathcal{F} _{\tau i+j}\right]-\mathbb{E}\left[\ell_{\tau(i+1)+j}\left(w_{\tau i+j}\right) \mid\mathcal{F}_{\tau i+j}\right]\right.\] \[\qquad\qquad+\ell_{\tau(i+1)+j}\left(w_{\mathrm{ref}}\right)- \ell_{\tau(i+1)+j}\left(w_{\tau i+j}\right)\Bigg{)}\Bigg{)}\mid\mathcal{F}_{ \tau i+j}\Bigg{]}\] \[\leq \exp\left(-8\left(z_{\tau i+j}\eta\right)^{2}C_{4}^{2}\mathbf{D} _{\psi}\left(w_{\mathrm{ref}};w_{\tau i+j}\right)\right)\exp\left(4\left(z_{ \tau i+j}\eta\right)^{2}C_{4}^{2}\left\|w_{\mathrm{ref}}-w_{\tau i+j}\right\|^ {2}\right)\leq 1\]

Therefore \(\mathbb{E}\left[\exp\left(Z_{i}^{j}\right)\mid\mathcal{F}_{\tau i+j}\right]\leq 1\) and hence \(\left(\exp\left(S_{k}^{j}\right)\right)_{k\geq 0}\) is a supermartingale. By Ville's inequality, we have with probability at least \(1-\delta\), for all \(0\leq k\leq\kappa\)

\[\sum_{i=0}^{k}Z_{i}^{j}\leq\log\frac{1}{\delta}\]

By union bound over \(j=0,\ldots,\tau-1\), and with probability at least \(1-\tau\delta\) we have for all \(0\leq k\leq T-\tau-1\)

\[\sum_{t=0}^{k}z_{t}\eta\left(\mathbb{E}\left[\ell_{t+\tau}\left(w_{\mathrm{ref }}\right)\mid\mathcal{F}_{t}\right]-\mathbb{E}\left[\ell_{t+\tau}\left(w_{t} \right)\mid\mathcal{F}_{t}\right]+\ell_{t+\tau}\left(w_{\mathrm{ref}}\right)- \ell_{t+\tau}\left(w_{t}\right)\right)\]\[\leq \sum_{t=0}^{k}z_{t}\eta\left(\mathcal{R}(w_{\mathrm{ref}})-\mathcal{R}^{* }\right)+\sum_{t=0}^{k}z_{t}\eta\left(\mathcal{R}(w_{t})-\mathcal{R}(w_{ \mathrm{ref}})\right)\] \[+\sum_{t=0}^{k}z_{t}\eta\left(\ell_{t+1}\left(w_{\mathrm{ref}} \right)-\ell_{t+1}\left(w_{t}\right)\right)+\sum_{t=0}^{k}\frac{z_{t}\eta^{2}}{ 2}\left\|g_{t+1}\right\|_{*}^{2}+\sum_{t=0}^{k}\left(z_{t}-z_{t-1}\right) \mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{t}\right)\] \[\leq \sum_{t=0}^{k}z_{t}\eta\left(\mathcal{R}(w_{\mathrm{ref}})- \mathcal{R}^{*}\right)+\sum_{t=k-\tau+1}^{k}z_{t}\eta\left(\mathcal{R}(w_{t}) -\mathcal{R}(w_{\mathrm{ref}})\right)\] \[+\sum_{t=0}^{k-\tau}z_{t}\eta\left(\mathcal{R}(w_{t})-\mathcal{R} (w_{\mathrm{ref}})-\mathbb{E}\left[\ell_{t+\tau}\left(w_{\mathrm{ref}} \right)\mid\mathcal{F}_{t}\right]+\mathbb{E}\left[\ell_{t+\tau}\left(w_{t} \right)\mid\mathcal{F}_{t}\right]\right)\]\[+\sum_{t=0}^{k-\tau}z_{t}\eta\left(\mathbb{E}\left[\ell_{t+\tau} \left(w_{\mathrm{ref}}\right)\mid\mathcal{F}_{t}\right]-\mathbb{E}\left[\ell_{t+ \tau}\left(w_{t}\right)\mid\mathcal{F}_{t}\right]+\ell_{t+\tau}\left(w_{ \mathrm{ref}}\right)-\ell_{t+\tau}\left(w_{t}\right)\right)\] \[+\sum_{t=0}^{k-\tau}z_{t}\eta\left(\ell_{t+\tau}\left(w_{t} \right)-\ell_{t+\tau}\left(w_{\mathrm{ref}}\right)\right)+\sum_{t=0}^{k}z_{t} \eta\left(\ell_{t+1}\left(w_{\mathrm{ref}}\right)-\ell_{t+1}\left(w_{t}\right)\right)\] \[+\sum_{t=0}^{k-\tau}\left(z_{t}-z_{t-1}\right)\mathbf{D}_{\psi} \left(w_{\mathrm{ref}};w_{t}\right)+\sum_{t=0}^{k}\frac{z_{t}\eta^{2}}{2} \left\|g_{t+1}\right\|_{*}^{2}\] \[\leq \sum_{t=0}^{k}z_{t}\eta\left(\mathcal{R}(w_{\mathrm{ref}})- \mathcal{R}^{*}\right)+\sum_{t=k-\tau+1}^{k}z_{t}\eta\left(\mathcal{R}(w_{t}) -\mathcal{R}(w_{\mathrm{ref}})\right)\] \[+\sum_{t=0}^{k-\tau}z_{t}\eta\left(\mathcal{R}(w_{t})-\mathcal{R }(w_{\mathrm{ref}})-\mathbb{E}\left[\ell_{t+\tau}\left(w_{\mathrm{ref}} \right)\mid\mathcal{F}_{t}\right]+\mathbb{E}\left[\ell_{t+\tau}\left(w_{t} \right)\mid\mathcal{F}_{t}\right]\right)\] \[+\tau\log\frac{1}{\delta}+\sum_{t=0}^{k-\tau}8\left(z_{t}\eta \right)^{2}C_{4}^{2}\mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{t}\right)+\sum _{t=0}^{k-\tau}\left(z_{t}-z_{t-1}\right)\mathbf{D}_{\psi}\left(w_{\mathrm{ ref}};w_{t}\right)\] \[+\sum_{t=0}^{k}\frac{z_{t}\eta^{2}}{2}\left\|g_{t+1}\right\|_{*}^ {2}+\sum_{t=0}^{k-\tau}z_{t}\eta\left(\ell_{t+\tau}\left(w_{t}\right)-\ell_{t +\tau}\left(w_{t+\tau-1}\right)\right)\] \[+\sum_{t=\tau-1}^{k-1}z_{t-\tau+1}\eta\left(\ell_{t+1}\left(w_{t }\right)-\ell_{t+1}\left(w_{\mathrm{ref}}\right)\right)+\sum_{t=0}^{k}z_{t} \eta\left(\ell_{t+1}\left(w_{\mathrm{ref}}\right)-\ell_{t+1}\left(w_{t}\right)\right)\] \[\leq \sum_{t=0}^{k}z_{t}\eta\left(\mathcal{R}(w_{\mathrm{ref}})- \mathcal{R}^{*}\right)+\sum_{t=k-\tau+1}^{k}z_{t}\eta\left(\mathcal{R}(w_{t}) -\mathcal{R}(w_{\mathrm{ref}})\right)\] \[+\sum_{t=0}^{k-\tau}z_{t}\eta\left(\mathcal{R}(w_{t})-\mathcal{R }(w_{\mathrm{ref}})-\mathbb{E}\left[\ell_{t+\tau}\left(w_{\mathrm{ref}} \right)\mid\mathcal{F}_{t}\right]+\mathbb{E}\left[\ell_{t+\tau}\left(w_{t} \right)\mid\mathcal{F}_{t}\right]\right)+\tau\log\frac{1}{\delta}\] \[+\sum_{t=0}^{k}\frac{z_{t}\eta^{2}}{2}\left\|g_{t+1}\right\|_{*}^ {2}+\sum_{t=0}^{k-\tau}z_{t}\eta\left(\ell_{t+\tau}\left(w_{t}\right)-\ell_{t +\tau}\left(w_{t+\tau-1}\right)\right)\] \[+\sum_{t=\tau-1}^{k-1}\eta\left(z_{t-\tau+1}-z_{t}\right)\left( \ell_{t+1}\left(w_{t}\right)-\ell_{t+1}\left(w_{\mathrm{ref}}\right)\right)\] \[+\sum_{t=0}^{\tau-2}z_{t}\eta\left(\ell_{t+1}\left(w_{\mathrm{ref} }\right)-\ell_{t+1}\left(w_{t}\right)\right)+z_{k}\eta\left(\ell_{k+1}\left(w_{ \mathrm{ref}}\right)-\ell_{k+1}\left(w_{k}\right)\right)\]

where in the last inequality we use \(z_{t}=\frac{1}{8\eta^{2}C_{4}^{2}\left(T+1+t\right)}\) to see that \(z_{t}+8\left(z_{t}\eta\right)^{2}C_{4}^{2}\leq z_{t-1}\). Notice that, \(\frac{z_{t}}{z_{k}}\leq 2\), and \(\left(\mathcal{R}(w_{\mathrm{ref}})-\mathcal{R}^{*}\right)\leq\frac{D_{0}}{ \sqrt{T}}\) we have

\[\sum_{t=0}^{k}\eta\left(\mathcal{R}(w_{t})-\mathcal{R}^{*}\right)+ \mathbf{D}_{\psi}\left(w_{\mathrm{ref}};w_{k+1}\right)\] \[\leq 2D_{0}+2\eta D_{0}\sqrt{T}+16\eta^{2}C_{4}^{2}T\tau\log\frac{1}{ \delta}+\underbrace{2\eta\sum_{t=k-\tau+1}^{k}\left|\mathcal{R}(w_{t})- \mathcal{R}(w_{\mathrm{ref}})\right|}_{A}\] \[+\underbrace{2\eta\sum_{t=0}^{k-\tau}\left|\mathcal{R}(w_{t})- \mathcal{R}(w_{\mathrm{ref}})-\mathbb{E}\left[\ell_{t+\tau}\left(w_{\mathrm{ ref}}\right)\mid\mathcal{F}_{t}\right]+\mathbb{E}\left[\ell_{t+\tau}\left(w_{t} \right)\mid\mathcal{F}_{t}\right]\right|}_{B}\]\[+\underbrace{\eta^{2}\sum_{t=0}^{k}\left\|g_{t+1}\right\|_{*}^{2}}_{ D}+\underbrace{2\eta\sum_{t=0}^{k-1}\left|\ell_{t+\tau}\left(w_{t}\right)-\ell_{t+ \tau}\left(w_{t+\tau-1}\right)\right|}_{D}\] \[+\underbrace{\eta^{2}\sum_{t=0}^{k}\left\|g_{t+1}\right\|_{*}^{2} }_{D}+\underbrace{2\eta\sum_{t=0}^{k-1}\left|\ell_{t+\tau}\left(w_{t}\right)- \ell_{t+\tau}\left(w_{t+\tau-1}\right)\right|}_{D}\] \[+\underbrace{\eta^{2}\sum_{t=0}^{k-1}\left\|\ell_{t+1}\left(w_{t }\right)-\ell_{t+1}\left(w_{\text{ref}}\right)\right\|}_{E}+\underbrace{2\eta \sum_{t=0}^{k-1}\left|\ell_{t+\tau}\left(w_{t}\right)-\ell_{t+\tau}\left(w_{t }\right)\right|}_{D}\]

Now we bound each term. For \(A\)

\[A=2\eta\sum_{t=k-\tau+1}^{k}\left|\mathcal{R}(w_{t})-\mathcal{R}(w_{\text{ref }})\right|\leq 2\eta\sum_{t=k-\tau+1}^{k}C_{4}\left\|w_{\text{ref}}-w_{t}\right\| \leq 2\eta\tau C_{4}R\]

For \(B\), by Assumption 3, \(\sup_{t\in\mathbb{Z}_{\geq 0}}\sup_{\mathcal{F}_{t}}\operatorname{TV}\left(P_{t}^{ t+\tau},\pi\right)\leq\epsilon\),

\[2\eta\left|\mathcal{R}(w_{t})-\mathcal{R}(w_{\text{ref}})- \mathbb{E}\left[\ell_{t+\tau}\left(w_{\text{ref}}\right)\mid\mathcal{F}_{t} \right]+\mathbb{E}\left[\ell_{t+\tau}\left(w_{t}\right)\mid\mathcal{F}_{t} \right]\right|\leq 2\eta C_{4}R\epsilon\]

Thus

\[B=2\eta\sum_{t=0}^{k-\tau}\left|\mathcal{R}(w_{t})-\mathcal{R}(w_{\text{ref}} )-\mathbb{E}\left[\ell_{t+\tau}\left(w_{\text{ref}}\right)\mid\mathcal{F}_{t} \right]+\mathbb{E}\left[\ell_{t+\tau}\left(w_{t}\right)\mid\mathcal{F}_{t} \right]\right|\leq 2\eta C_{4}R\epsilon T\]

For \(C\), similarly to before

\[C=\eta^{2}\sum_{t=0}^{k}\left\|g_{t+1}\right\|_{*}^{2}\leq 2T\eta^{2}\left(C_{4} ^{2}+C_{2}^{2}R^{2}\right)\]

For \(D\), we have

\[\left|\ell_{t+\tau}\left(w_{t}\right)-\ell_{t+\tau}\left(w_{t+ \tau-1}\right)\right|\] \[\leq \sum_{i=t+1}^{t+\tau-1}\left|\ell_{t+\tau}\left(w_{i}\right)-\ell _{t+\tau}\left(w_{i-1}\right)\right|\] \[\leq \sum_{i=t+1}^{t+\tau-1}\left\|w_{i}-w_{i-1}\right\|\left(C_{1}+C_ {2}\left(1+\left\|w_{i}\right\|\right)\right)\] \[\leq \sum_{i=t+1}^{t+\tau-1}\eta\left\|\nabla\ell_{i}\left(w_{i-1} \right)\right\|\left(C_{4}+C_{2}\left\|w_{i}-w_{\text{ref}}\right\|\right)\] \[\leq \eta\left(C_{4}+C_{2}R\right)\sum_{i=t+1}^{t+\tau-1}\left(C_{4}+C _{2}\left\|w_{i-1}-w_{\text{ref}}\right\|\right)\] \[\leq \eta\left(C_{4}+C_{2}R\right)^{2}\tau\leq\eta\tau\left(2C_{4}^{2 }+2C_{2}^{2}R^{2}\right)\]

We obtain

\[D=2\eta\sum_{t=0}^{k-\tau}\left|\ell_{t+\tau}\left(w_{t}\right)-\ell_{t+\tau} \left(w_{t+\tau-1}\right)\right|\leq 2\eta^{2}\tau T\left(2C_{4}^{2}+2C_{2}^{2}R^{2}\right)\]

For \(E\), since

\[\left|\ell_{t+1}\left(w_{t}\right)-\ell_{t+1}\left(w_{\text{ref}}\right)\right| \leq C_{4}R\]

Hence

\[E= \frac{2(\tau-1)\eta}{T}\sum_{t=\tau-1}^{k-1}\left|\ell_{t+1}\left( w_{t}\right)-\ell_{t+1}\left(w_{\text{ref}}\right)\right|\] \[+2\eta\sum_{t=0}^{\tau-2}\left|\ell_{t+1}\left(w_{\text{ref}} \right)-\ell_{t+1}\left(w_{t}\right)\right|+2\eta\left|\ell_{k+1}\left(w_{ \text{ref}}\right)-\ell_{k+1}\left(w_{k}\right)\right|\]\[\leq 2\eta C_{4}R\left(\frac{\left(\tau-1\right)\left(k-\tau+1\right)}{T}+ \tau\right)\leq 4\eta\tau C_{4}R\]

Sum up we have

\[\sum_{t=0}^{k}\eta\left(\mathcal{R}(w_{t})-\mathcal{R}^{*}\right)+ \mathbf{D}_{\psi}\left(w_{\text{ref}};w_{k+1}\right)\] \[\leq 2D_{0}+2\eta D_{0}\sqrt{T}+16\eta^{2}C_{4}^{2}T\tau\log\frac{1} {\delta}\] \[+2\eta\tau C_{4}R+2\eta C_{4}R\epsilon T+2T\eta^{2}\left(C_{4}^{2 }+C_{2}^{2}R^{2}\right)\] \[+2\eta^{2}\tau T\left(2C_{4}^{2}+2C_{2}^{2}R^{2}\right)+4\eta\tau C _{4}R\] \[\leq \frac{R^{2}}{2}\]

as needed. Finally we have

\[\frac{1}{k+1}\sum_{t=0}^{k}\left(\mathcal{R}(w_{t})-\mathcal{R}^{*}\right)+ \frac{\mathbf{D}_{\psi}\left(w_{\text{ref}};w_{k+1}\right)}{\eta\left(k+1 \right)}\leq\frac{R^{2}}{2\eta\left(k+1\right)}.\]