# A Practitioner's Guide to Continual Multimodal Pretraining

Vishaal Udandarao\({}^{1,3*}\)  Karsten Roth\({}^{1,2,6*}\)  Sebastian Dzialozio\({}^{1}\)  Ameya Prabhu\({}^{1}\)

Mehdi Cherti\({}^{4}\)  Oriol Vinyals\({}^{5}\)  Olivier Henaff\({}^{5}\)

Samuel Albanie\({}^{\dagger}\)  Zeynep Akata\({}^{2,6,7\dagger}\)  Matthias Bethge\({}^{1\dagger}\)

###### Abstract

Multimodal foundation models serve numerous applications at the intersection of vision and language. Still, despite being pretrained on extensive data, they become outdated over time. To keep models updated, research into continual pretraining mainly explores scenarios with either (1) infrequent, indiscriminate updates on large-scale new data, or (2) frequent, sample-level updates. However, practical model deployment often operates in the gap between these two limit cases, as real-world applications demand adaptation to specific subdomains, tasks or concepts -- spread over the entire, varying life cycle of a model. In this work, we _complement current perspectives on continual pretraining through a research test bed and offer comprehensive guidance for effective continual model updates in such scenarios_. We first introduce FoMo-in-Flux, a continual multimodal pretraining benchmark with realistic compute constraints and practical deployment requirements, constructed over \(63\) datasets with diverse visual and semantic coverage. Using FoMo-in-Flux, we explore the complex landscape of practical continual pretraining through multiple perspectives: (1) data mixtures and stream orderings that emulate real-world deployment settings, (2) methods ranging from simple fine-tuning and traditional continual learning strategies to parameter-efficient updates and model merging, (3) meta-learning-rate schedules and mechanistic design choices, and (4) model and compute scaling. Together, our insights provide a _practitioner's guide to continual multimodal pretraining_ for real-world deployment. Benchmark and code is provided here: github.com/ExplainableML/fomo_in_flux.

## 1 Introduction

Foundation models [14] require vast datasets and computational resources to train [142, 29]. Despite these substantial investments, models often have limited concept coverage [180], and quickly become outdated as new tasks emerge. To stay relevant, they need _continual pretraining_, which falls into two high-level categories: (1) infrequent, large-scale updates with substantial new data and compute [49, 76], and (2) frequent, but minimal updates that target specific information, e.g. via knowledge editing or updating knowledge bases in retrieval-augmented systems [28, 191, 135, 57]. However, many real-world applications operate in the large gap between these cases; calling for specialized knowledge (fine-grained expertise or semantic and visual distribution shifts [87, 216, 179, 164, 117, 139, 156, 56, 155, 225, 47, 137]) that goes beyond simple edits, but does not warrant full retraining. Under the semantic versioning framework [143, 140], such specialized minor updates go beyond simple patches, but do not justify major version updates. In this work, we provide a novel framework to emulate practical deployment scenarios for vision-language foundation models in a controlled environment, and study how continual pretraining can succeed:_Creating FoMo-in-Flux_ (_Foundation-Models-in-Flux_, Fig. 1), which enables a controlled study of _minor_ updates of multimodal models over a long life cycle and builds on 63 image classification and retrieval datasets enhanced with captions for multimodal pretraining. Unlike noisy web-crawl datasets (e.g. TiC-RedCaps, DataComp [49; 45]), FoMo-in-Flux comprises curated samples with fine-grained class information for precise control of data streams over visual/semantic domains.

_Realistic Continual Pretraining._ Unlike traditional continual learning, we eschew the _practically irrelevant restriction of limited storage_[135; 136], and allow unrestricted access to pretraining data. As deployment cost is primarily a function of computation, we only impose a restriction on compute budgets. To avoid skewed compute metrics [38; 118], we introduce _Memory-Adjusted FLOPs (MAFs)_, which account for FLOPS in forward and backward passes and peak accelerator memory.

_Methods and Training Recipes for Continual Pretraining._ Using FoMo-in-Flux, we study current research strategies for multiple _minor_ continual pretraining updates -- from continual learning (CL) regularization strategies (EWC [86], SI [209]), simple finetuning, parameter-efficient adaptation (LoRA [72], VeRA [88]), to model merging [78]. We also show the importance of strategies beyond method choices, such as learning rate scheduling, and propose meta schedules for long-term model updates. Moreover, we study model and compute scaling for continual pretraining, and give an overview of important experimental design choices for continual multimodal pretraining pipelines.

_A Data-centric Perspective on Continual Pretraining._ Concepts and tasks arise in sequence, driven by the ongoing discovery of model shortcomings or desiderata from feedback loops [46]. Our fine-grained control over the sequence of semantic and visual concepts allows us to simulate realistic data streams to better understand how different concept and task orderings affect accumulation of new and retention of existing knowledge. Finally, we provide insights into data mixtures on the accumulation and retention trade-off as new concepts and subdomains are introduced.

Based on our experiments, we collate key practical insights for _continual multimodal pretraining_:

[leftmargin=*]

An Abbreviated Predefined's Guide to Continual Multimodal Pretraining.

1. Method Choices. Under practical update and compute restrictions, continual learning and parameter-efficient fine-tuning methods favor knowledge retention (stability) while simple fine-tuning focuses on adaptation (plasticity). In combination with **model merging**, fine-tuning sufficiently addresses this trade-off for strong knowledge retention _and_ adaptation. 2. Meta Learning Rate Schedules. Learning rates matter, and can naturally be accounted for in long-horizon continual pretraining via **meta learning rate schedules** across incoming tasks; reducing the loss of pretraining knowledge while encouraging high adaptation. Maintaining the same learning rate schedule between pretraining and updates is less important. 3. Model and Compute Scaling. Simple fine-tuning does not scale well with increased compute resources or more frequent updates, unlike fine-tuning with model merging. On the other hand, **increasing model size** helps it acquire new knowledge while retaining its foundational properties, even within the same compute budget. 4. Data-centric Stream Orderings. The **order** in which data updates are applied significantly impacts the model's ability to learn new information and retain its zero-shot capabilities. This is important to account for during deployment. However, when underlying data distributions are the same, models converge to **comparable final performance** across update sequences. 5. Data mixture ratio. The ratio between pretraining-, update-, and buffer data affects the model's final performance, and "IID-fying" knowledge accumulation is crucial: Replaying previous adaptation tasks is more relevant that pretraining replay.

## 2 Categorizing Continual Pretraining: A Versioning Perspective

Traditional continual learning is categorized into class-, domain-, and task-incremental settings [181]. However, continual pretraining benchmarks do not fit these categories, as they exhibit high-overlaps in captions as opposed to disjoint classes [76; 15; 102], and time-varying gradual class/domain shifts [49; 103; 135; 103; 189]. Similarly, continual learning strategies are typically grouped [35; 134] into replay [25; 20], regularization [121; 86; 24], and parameter-isolation methods [224; 3; 227], with recent additions like prompt-tuning [193; 194; 168; 141], fixed-representation [116; 222; 138], and model-mixture methods [114; 78] (see [223] for overview). However, continual foundation model updates are dominated by replay [136; 49], parameter-efficient finetuning [62] and retrieval-augmented methods [185; 135; 57], as traditional methods fail under compute constraints [63; 183; 135], even underperforming simple baselines [138; 116; 136; 215]. Hence, we provide a new categorization suitable for continual pretraining literature., inspired by the semantic software versioning framework [143]. We believe that different scopes of updates require distinct strategies, indicating that no one solution fits all scenarios (see [198] for a survey, and table 1 for an overview of related benchmarks under the semantic versioning umbrella). We believe foundation models require distinct update strategies, similar to major, minor, and patch updates in software versioning:

**Major Updates.** Large-scale continual pretraining over extensive compute, data, and time resources that substantially alter overall performance. Methods focusing on significant updates [49; 76; 51] consistently employ continual fine-tuning of the model, which has been found to be the primary strategy through extensive comparisons with other works [49; 126; 136; 27]. Currently explored topics include continual LR scheduling [59; 76; 211; 127; 74] to minimize the stability gap [36].

**Patch Updates.** Frequent but minor, targeted updates in which continual fine-tuning leads to poor zero-shot capability retention with little new knowledge gained. These are best managed by continual knowledge editing [28; 191] or sample-wise updates using a fixed backbone [135; 228; 57; 116; 52].

**Minor Updates.** Adapations to whole subdomains and general concepts out of scope for knowledge edits and major updates. Some examples: updating specific parts of a model with LoRA [62; 12; 109; 196], model merging [78; 174; 187], instruction tuning [64; 218; 26], or incorporating expert knowledge [87; 216; 179; 164; 117; 139; 156; 56; 225; 47; 137]), adding visual reasoning over fine-grained object categories [9; 186; 79; 130; 125; 169], or new domains like sketches [30; 129], drawings [129; 99], or synthetic [19; 115] and medical imagery [77; 41]. Multimodal minor updates can also jointly involve new or infrequently encountered concepts [19; 115], s.a. aforementioned fine-grained expert knowledge, medical terminology or new compositions [80].

## 3 The FoMo-in-Flux Benchmark

We introduce FoMo-in-Flux (_Foundation-Models-in-Flux_), a benchmark for controlled continual _multimodal_ pretraining. Our benchmark extends beyond monolithic pretraining datasets, such as TiC-RedCaps/TiC-DataComp [49], to specialized subdomains with fine-grained control over data streams and adaptation over long horizons. Table 1 extensively compares FoMo-in-Flux to related benchmarks, showcasing key features that distinguish it from existing works. For the exact experimental setup, including base models and exact derivation of _MAF_ budgets, refer appendix H.1.

### Creation

**Breakdown.** FoMo-in-Flux consists of \(63\) classification and retrieval datasets--either publicly available or introduced as part of this work--covering \(2.53M\) samples and \(23,045\) concepts spanning

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline
**Benchmark** & **\# Samples** & **\# Tasks** & **Ordering** & **Domains** & **Update** & **Multi-** & **Zero-Shot** & **Compute-** & **Data-** & **Road World** \\  & & & & & **Style** & **modal** & **Restraints** & **Bound** & **Mursure** & **Stream Variants** \\ \hline COR650 [106] & 165R & 9 & Clus-Data-Inc & Objects & Mixper & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\ Split-ImageNet [195] & 1,2M & 0 & Clus-Inc & Web Images & Mixper & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\ PTM-Adaptation [173] & 30k-100k & 5-20 & Clus-Inc & Web Images & Mixper & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\ CLAD [184] & 23k & \(\times\)200k & Time-Inc & Synthetic & Patch & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\ OAK [185] & 30k-200k & -200k & Time-Inc & Egocentric & Patch & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\ De-PASCLA [111] & 11k & 2-6 & Clus-Inc & Web Images & Mixper & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\ Inc-ADE2021 [20] & 20k & 2-6 & Clus-Inc & Scene Parsing & Mixper & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\ StreamingQA [1003] & 100k & 6 & Time-Inc & Text & Mixper & Mixper & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\ Temporal(Wish [83]) & 32M & 4 & Time-Inc & Text & Mixper & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\ CKL [81] & 30k & 2 & Task-Inc & Text & Mixper & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\ CTL [132] & 500k & 100k & Tag-Inc & Objects & Mixper & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\ CLEAN [101] & 7.8M & 10 & Time-Inc & Web Images & Mixper & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\ ImageNet(Xie) [136] & 1,2M & 20,200k & Class-Inc-Inc & Web Images & Mixper & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\ Online-CQLML[136] & 50k & 20,30k & Time-Inc & Web Images & Mixper & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\ ILR-PQA [137] & 62k & 5 & Clus-Inc-Inc & Web Images & Mixper & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\ NEVIS [13] & 6M & 2 & Task-Inc & Mind & Mixper & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\ COCO [13] & 30k & 39k & Time-Inc & Geodesic & Patch & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\ CQLAN [15] & 500k & 500k & Time-Inc & Landmarks & Patch & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\ CLAN [16] & 1,3M & 4 & Task-Inc & Mixed & Mixper & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\ MTL [220] & 20k & 5,20k & Class-Inc & Mixed & Mixper & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\ C-MLP [220] & 6,6B & 160 & Domains-Inc & Text & Mixper & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\ TC-DoneComp [49] & 100k/18/12B & 6 & Time-Inc & Web Images & Mixper & \(\check{\check{\check{\check{\check{\check{\check{\check{\check{\check{\check{\diverse visual domains such as natural images, sketches, abstractions, synthetic imagery or generative data. Building concept-first allows experimentation with very precise and controlled ordering on the type of data encountered at each continual pretraining stage. Moreover, by operating on much cleaner data building blocks than web-crawled datasets [49, 45], we ensure cleaner alignment between concepts and images. The \(63\) datasets are divided into \(41\) datasets used for _adaptation_ only, and \(22\) hold-out datasets to probe _retention_ of initial zero-shot generalization. See tables 2 and 3 for a more detailed overview of datasets, the exact splits, assigned domains and licenses.

_Obscure Things and Animals._ To improve diversity and to include classes that are systematically seen as long-tailed, we create the _Obscure Animals_ and _Obscure Things_ datasets using text-to-image models. This also allows us to model the issue of AI-generated content making its way into model training data, potentially misrepresenting some concepts (see _e.g._, Fig 9). The exact generation is depicted in the supplementary.

**Captioning.** We generate high-quality captions for each image (where needed) through two different methods: **(1)** A scalable two-stage captioning mechanism, which uses BLIP-2 [96] to generate general captions for each image and CapsFusion [207] (T5-XL) to merge and align captions with available information on ground-truth class names (c.f. fig. 7). **(2)** Procedural generation for specific datasets (s.a. Shapes3D [19] and DSprites [115]) using available dataset-specific information to create captions containing e.g. information about object location, orientation, size or shape. These captions are then converted into natural language equivalents at random using GPT-4 [4].

### Pipeline, Compute Budgeting and Data Restrictions

**Continual Pretraining Updates.** We illustrate the general FoMo-in-Flux training and evaluation pipeline in fig. 1. We start with a model \(\theta_{0}\) trained on a large pretraining dataset \(\mathcal{P}\), and an empty buffer \(\mathcal{B}\). Within the allocated update budget, at each update step \(j\in\{1,2,\dots,T\}\), the following happens in order: **(1)** The stream reveals a task update pool of \(n_{j}\) image-text pairs \(\mathcal{D}_{j}=\{(i_{k}^{j},t_{k}^{j})\}_{k=1}^{n_{j}}\) spanning \(\mathcal{C}_{j}\) concepts. **(2)** We create the training data mixture \(\mathcal{S}_{j}\) by sampling from the pretraining data \(\mathcal{P}\), buffer \(\mathcal{B}\), and current task data \(\mathcal{D}_{j}\) with respective ratios \(\lambda_{\mathcal{P}},\lambda_{\mathcal{B}},\) and \(\lambda_{\mathcal{D}}\), such that \(\lambda_{\mathcal{P}}+\lambda_{\mathcal{B}}+\lambda_{\mathcal{D}}=1\). If samples in \(\mathcal{B}\) are insufficient (particularly at the start of task adaptation), we oversample from \(\mathcal{D}_{j}\), with \(\lambda_{\mathcal{D}}\) fixed. **(3)** We apply a continual update method \(\mathcal{M}\) with a fixed compute budget \(F\): \(\theta_{j}{=}\texttt{train}(\mathcal{M},\mathcal{D}_{j},\theta_{j-1})\). This compute budget \(F\) also determines the overall number of update steps conducted. **(4)** We add samples from the update pool \(\mathcal{D}_{j}\) to the unrestricted buffer \(\mathcal{B}\). However, while all samples can be stored in buffer \(\mathcal{B}\), they cannot all be sampled for training set \(\mathcal{S}\), as the compute budget \(F\) imposes an implicit memory restriction [136].

**How to Measure Continual Pretraining Computational Cost?** We impose a fixed compute budget for each update step to account for each method's efficiency. Recent works use number of iterations (forward/backward passes) [136, 49], number of updated parameters [88, 13, 118], FLOPs [50], and

Figure 1: **FoMo-In-Flux pipeline.**_(Pretraining)_ We start from pretrained CLIP \(\theta_{0}\) and its pretraining pool \(\mathcal{P}\). _(Update steps)_ At each step \(t\), we sample training instances \(\mathcal{S}_{t}\) from \(\mathcal{P}\), current update pool \(\mathcal{D}_{t}\), and memory buffer \(\mathcal{B}\) (containing all past \(\mathcal{D}_{t}s\)), and train for a fixed compute budget (\(F\) MAFs).

throughput [118]. However, a single metric does not paint a complete picture of efficiency [38; 118]. To account for this, we introduce _Memory-Adjusted-FLOPs (MAFs)_, a novel metric that highlights two aspects most relevant from a practitioner's perspective: total number of FLOPs per iteration and maximum utilization of device memory. To compute MAFs, which determines the number of updates a model can take, we multiply FLOPs of each method by the ratio of that method's maximum memory utilization to the maximum memory utilization of a full fine-tuning of the base model.

**Data Restrictions.** We allow unrestricted access to pretraining data (e.g., LAION-400M[160]), and an unlimited replay buffer \(\mathcal{B}\), as data storage is a negligible contributor to real-world cost [135; 136], and buffer memory is only utilized during the continual pretraining process. To study different retraining data pools, we use four popular image-text pretraining datasets of varying sizes, quality and curation strategies--LAION-400M[160], CC-12M[23], CC-3M[161], and DataComp-Small1[45].

**Metrics & Plots.** We study _adaptation_ to new data and _retention_ of pretraining knowledge, measured as Knowledge Accumulation (\(\mathcal{A}_{KA}\)), the avg. accuracy (recall@5 for retrieval) over concepts in all \(41\) adaptation datasets, and Zero-Shot Retention (\(\mathcal{A}_{ZS}\)), the zero-shot transfer on the held-out datasets. In most plots, we depict the zero-shot baseline as black star and the joint training upper-bound as golden star. A dotted connection approximates the joint training trajectory on the \(\mathcal{A}_{KA}\)-\(\mathcal{A}_{ZS}\) plane.

### Designing Data-Centric Task-Sequences

In addition to studying different pretraining sets \(\mathcal{P}\) and data mixture ratios \((\lambda_{\mathcal{P}},\lambda_{\mathcal{B}},\lambda_{\mathcal{D}})\), we also investigate different realistic orderings by breaking down FoMo-in-Flux into individual concepts, and then ordering based on a chosen criterion (including an option to reverse orderings). This is visualized in Fig. 10. To do so, having a controlled set of image-caption pairs is critical, as it allows for well-defined and meaningful arrangement of concepts into sequences according to an ordering \(\pi(\mathcal{C})\). Each ordering \(\pi\) divides the set of samples \(\mathcal{D}\) into \(T\) disjoint subsets \(\{\mathcal{D}_{1},\dots,\mathcal{D}_{T}\}\) of concepts \(\mathcal{C}\) sampled without replacement, i.e. \(\mathcal{C}_{i}\bigcap\mathcal{C}_{j}=\phi\), \(\forall i,j\). We define six different orderings:

**1. Easy-To-Hard Ordering (performance)** is motivated by curriculum learning [58; 154; 170; 208], assuming users deploying their model to easier concepts and usecases first, with incremental movement towards to harder concepts. **2. Concept Frequency Ordering (concept-frequency)** draws motivation from Udandarao et al. [180], starting from least frequent concepts first (edge cases that are most likely to cause undesired performance drops) and incrementally extending to more frequent concepts represented well in the pretraining pool. **3. Concept Similarity Ordering (similarity)**, inspired by Yildiz et al. [204], is based on the hypothesis that training on conceptually similar tasks allows users to minimize catastrophic forgetting over tasks. **4. Time-incremental Ordering (time)**, inspired by [15; 73; 21; 135; 49], arranges in chronological order. **5. Dataset-Incremental Ordering (dataset)** is motivated by [148; 111; 112; 190; 206], but extended to a larger sequence of datasets. This sequence is then broken down into the desired number of tasks \(T\). **6. Random Ordering (random)** (e.g. [149; 200; 70; 136]) mimics a scenario where user requests for model improvement are unstructured. For this ordering, we simply shuffle class names at random.

## 4 Continual Pretraining: A Method Perspective

We first explore how different methods affect knowledge accumulation and zero-shot retention. We excluded prompt-tuning-based continual learning methods, which often collapse to a single prompt [175] or near-chance performance over a longer time horizon [138]. Similarly, we do not include distillation-based CL methods, as they do not show improvements when memory is unrestricted [136]. For details on each tested method, we refer to the supplementary.

### Parameter-efficient Finetuning and Continual Learning

We study _parameter-additive_ methods (LoRA[72], VeRA[88], DoRA[104]) and _parameter-selective_ approaches tuning only particular weight subsets (LNFit[37], BitFit[8]). We also investigate recently proposed low-rank approximations to model gradient updates (GaLore[219]). We further examine how prior continual learning methods such as Elastic Weight Consolidation (EWC[86]) or Synaptic Intelligence (SI[209]) perform at scale. To begin, we find two extreme points:

**(1) Strongest accumulation, weakest retention.** Naive contrastive finetuning (in orange, fig. 2 left) which achieves strongest knowledge accumulation \(\mathcal{A}_{\text{KA}}\) across a full update cycle, at the cost of a significant drop in zero-shot retention \(\mathcal{A}_{\text{ZS}}\) even with learning rate rewarming [76], following best practices sketched out in [54]. We update both the image and language branch, and initialize from the pretraining temperature (c.f. appendix D.3). Importantly, naive finetuning falls victim to "longer-horizon" stability gap issues [36], where forgetting is high and achievable knowledge gain is strongly limited across initial update "steps" (each step being a whole compute-budgeted training cycle, c.f. appendix B.3). **(2) Weakest accumulation, strongest retention.** Parameter-selective methods like LNFit (green) and BitFit (blue, fig. 11 center) exhibit good knowledge retention, but minimal capacity for the accumulation of new knowledge across longer and complex data streams.

All other methods operate between these end points, trading off knowledge accumulation and retention: **(3) Strong accumulation, weak retention.** By only modifying the naturally lower-rank gradient updates during model training, GaLore (olive green, fig. 2 left) offers a moderate balance between the ability to effectively incorporate new knowledge within a given compute budget, and retaining original zero-shot generalization behaviour. **(4) Decent accumulation, decent retention.** Parameter-efficient tuning methods such as LoRA (blue, fig. 2 left) and DoRA (pink, fig. 11 right) provide an effectively linear reduction in knowledge accumulation and forgetting w.r.t. to base finetuning and GaLore. This aligns with recent insights on LoRA effectively both learning and forgetting less in single domain finetuning tasks [11]. However, VeRA (dark blue, fig. 11 right), which significantly reduces the number of tunable parameters, behaves closely to parameter-selective tuning methods, offering very little knowledge gain across long and complex data streams.

Finally, for continual learning regularization methods we find that while EWC (pink, fig. 2 left) significantly improves zero-shot retention, it also offers extremely limited \(\mathcal{A}_{\text{KA}}\) compared to the initial zero-shot performance. On the other hand, the popular regularisation method SI (purple, fig. 2 left) effectively offers no benefits over standard finetuning, either in \(\mathcal{A}_{\text{KA}}\) or \(\mathcal{A}_{\text{ZS}}\). The poor performance of regularisation-based methods is curious as prior work has hinted at their benefits at scale [121; 85]. However, our fine-grained, and most importantly compute-controlled FoMo-In-Flux helps verify these claims, as these regularization mechanisms are both compute- and memory-expensive.

### On the Benefits of Model Merging Techniques

Model merging is a promising avenue for adapting foundation models [172; 78; 197], enabling efficient aggregation of multiple models [203; 157; 34; 5]. Initial work [172] also highlighted potential benefits for small-scale continual learning. To study benefits at scale, we investigate three forms of model merging. Denoting model weights going into task \(t\) as \(\theta_{t-1}\), finetuned weights after task \(t\) as \(\theta_{t}^{\prime}\), and final model-merged output after task \(t\) as \(\theta_{t}\), we define (c.f. fig. 16 for details):

**(1) Exponential-moving averaging (EMA-merge)**, as adopted in Stojanovski et al. [172], which tunes the previously merged task weights \(\theta_{t-1}\) on task \(t\) to produce the finetuned weights \(\theta_{t}^{\prime}\), and then merges \(\theta_{t-1}\) with \(\theta_{t}^{\prime}\) to produce \(\theta_{t}\). **(2) Continual fine-tuning and merging** (Finetune-merge) derived from multi-model patching in Ilharco et al. [78]), which produces \(\theta_{t}\) by merging the original pretraining weights \(\theta_{0}\) and the finetuned weights \(\theta_{t}^{\prime}\). To obtain \(\theta_{t}^{\prime}\), Finetune-merge tunes the previously merged model weights \(\theta_{t-1}\), same as EMA-merge. **(3) Continual zero-shot merge**

Figure 2: **Which methods to use for continual pretraining over long update cycles?**_(Left)_ An in-depth study across five different method families: Continual finetuning (Full-FT [76]) and parameter-selective tuning (LNFit [37]) provide the extreme points in knowledge accumulation and retention. Switching from GaLore [219] to parameter-efficient tuning (LoRA) and continual learning methods (EWC [86], SI [209]) provides near linear transition points between both extremes. _(Right)_ Judiciously merging model weights exhibits unique long-horizon continual pretraining behaviour, allowing for significantly consistent accumulation across update tasks with maximal retention.**

(ZeroShot-merge), a simple ablative merging protocol, which tunes the original pretraining weights \(\theta_{0}\) during each task \(t\) and produces \(\theta_{t}\) by merging \(\theta_{t-1}\) and the finetuned \(\theta^{\prime}_{t}\). Each merge method uses an old-new weight mixing coefficient \(w\), which we ablate over \(w{=}\{0.85,0.9,0.95\}\).

As shown in fig. 2 (right), we find that the EMA-merge (blue) and ZS-merge (green), provide impressive boosts in zero-shot retention rates \(\mathcal{A}_{\text{ZS}}\) during the first update tasks, and _retain slight gains_ over the entire update cycle. Moreover, this is coupled with strong knowledge accumulation \(\mathcal{A}_{\text{KA}}\), though not yet at the level of standard finetuning. As expected, ablating the mixing weight \(w\) yields a trade-off between zero-shot retention and knowledge accumulation--higher \(w\)s provide better zero-shot retention capabilities while compromising on the accumulation \(\mathcal{A}_{\text{KA}}\). However, across both ablated mixing ratios, as well as the merging mechanism, we find that the high-level continual pretraining dynamics remain the same--at worst limited loss (and at best notable gains) in retention coupled with strong accumulation capacities, while also breaking favorably with the hypothetical linear trade-off between the initial zero-shot performance and the joint finetuning upper-bound!

## 5 Continual Pretraining: General Training Recipes

This section studies the other degrees of freedom orthogonal to methodological update strategies that co-occur within a continual pretraining pipeline: **(1)** The importance of the learning rate and its scheduling in section 5.1 and its translation to meta-learning rate schedules for continual pretraining tasks. **(2)** The impact of both model and compute scaling as independent axes to optimize and account for when planning to deploy a model over longer minor update cycles. More precisely, section 5.2 evaluates the impact on the knowledge accumulation and the zero-shot retention trade-off as a function of both increased model sizes within the same model family, as well as increases in the allocated compute budget within a fixed model size. **(3)** Moreover, the supplementary provides studies on the relevance of locked image and text encoder tuning, as well as the importance of aligning initial and continual pretraining softmax temperature in order to minimize stability gap issues.

### Learning Rates, Schedules and Meta-Schedules

By default, LR schedules apply to each task individually [20, 162, 16, 172, 108]. As open_clip models use cosine schedules, we first study the impact of re-applying these for each task:

\[\eta_{n}=\begin{cases}\eta_{\text{min}}+\frac{n}{N_{\text{warm}}}\left(\eta_ {\text{max}}-\eta_{\text{min}}\right)&n<N_{\text{warm}}\\ \eta_{\text{min}}+\frac{1}{2}(\eta_{\text{max}}-\eta_{\text{min}})\left(1+ \cos\left(\frac{n-N_{\text{warm}}}{N_{\text{test}}-N_{\text{warm}}}\pi\right) \right)\end{cases}\] (1)

with \(\eta_{n}\in[\eta_{\text{min}},\eta_{\text{max}}]\) the learning rate at step \(n\), and \(N_{\text{task}}\) the number of update steps for a given task. As recommended in e.g. Ibrahim et al. [76], we utilize linear warmup to the initial pretraining peak learning rate \(\eta_{\text{max}}\) used in Cherti et al. [29] for \(N_{\text{warm}}\) iterations. To study the impact of a learning rate schedule switch to e.g. infinite learning rate variants for potentially more flexibility down the line, we investigate a switch towards reciprocal square root schedule (_rsqrt_) introduced in Zhai et al. [211]

\[\eta_{n}=\begin{cases}\eta_{\text{min}}+\frac{n}{N_{\text{warm}}}(\eta_{ \text{max}}-\eta_{\text{min}})&n\geq N_{\text{warm}}\\ \eta_{\text{max}}\cdot\frac{\sqrt{N_{\text{warm}}}}{\sqrt{n+N_{\text{warm}}}} &n\in[N_{\text{warm}},N_{\text{task}}-N_{\text{cool}}]\\ \eta_{N_{\text{test}}-N_{\text{test}}}\cdot\frac{N_{\text{test}}-(n+N_{\text{ warm}})}{N_{\text{cool}}}&\text{else}\end{cases}\] (2)

Figure 3: **Visualization of different deployed learning rate schedules**, from task-independent _cosine_ and infinite learning rate schedules (_Rsqrt_), to task-dependent meta learning rate schedule.

Note that _rsqrt_ scheduling includes a separate cooldown section, wherein the last \(N_{\text{cool}}\) steps are used to linear cooldown the previously decayed learning rate. Both schedules are visualized in fig. 3 (left and right) over multiple tasks, and the result of either application (matching and changing the pretraining learning rate scheduler) to our 20 task update cycle stream is visualized in fig. 4 (center). As can be seen, there is a negligible change in knowledge accumulation \(\mathcal{A}_{KA}\) and knowledge retention for either learning rate scheduler; highlight that across longer update cycles, matching the original pretraining scheduler is of lesser importance.

**Meta Learning Rate Schedules.** By default, each intermediate update is treated independently (c.f. fig. 3 (left)): each task rewarms and cools down the same. However, as these updates appear in succession, catastrophic forgetting of previously seen tasks has to also be accounted for; while with every task update, the model is encouraged to move further away from its pretraining starting point. To reduce the impact of task-level forgetting and the increased shift from pretraining, we introduce meta LR scheduling - task-level schedules over each task-specific, iteration-level LR schedule to account for task continuity. These derive _naturally and hyperparameter-free_ by theoretically extending previous task schedule across all the new tasks (see gray hypothetical schedules in fig. 3 (_center_)). We explore four meta-schedules: **(i)**_autoregressive cosine scheduling_, which selects each task \(\eta_{\text{max}}\) by building a hypothetical cosine schedule with warmup across all seen tasks and sets it to the intersection point with the warmup process of the current task (Fig. 3 center):

\[\eta_{\text{max}}^{T}=\eta^{\text{cos}}(n^{\prime}=N_{\text{warm}}^{T}+\sum _{t}^{T-1}N_{\text{task}}^{t},N_{\text{task}}^{\prime}=\sum_{t}^{T}N_{\text {task}}^{t})\] (3)

where \(\eta^{\text{cos}}(\cdot,\cdot)\) defines the LR returned by the standard cosine LR schedule with warmup at point \(n^{\prime}\) for \(N_{\text{task}}^{\prime}\) total iterations. Using the same formulation, we also test **(ii)**_autoregressive continued dynamic_ schedule, which warms up to the same \(\eta_{\text{max}}^{T}\), but continues the schedule following the hypothetical cosine schedule over all total previous steps \(N_{\text{previous}}\) and the current post-warmup steps \(N_{\text{warm}}\). This autoregressive scheduling is naturally extended to the **(iii)**_autoregressive sqrt schedule_, which sets \(\eta_{\text{max}}=\eta^{\text{rsqrt}}(n^{\prime},N_{\text{task}}^{\prime})\), and **(iv)** which continues the dynamics of a hypothetically extended base schedule ("_Continued Dynamic_"). Finally, we also introduce **(v)**_"Peaks match Rsqrt_", where respective \(\eta_{\text{max}}\) matches the continued dynamics while continuing with a standard rsqrt schedule.

**The impact of task- and meta-level learning rate schedules for continual model updates** are visualized in Fig. 4 on the default 20-task variation of FoMo-in-Flux using simple continual finetuning as our reference approach. Indeed, for longer continual pretraining sequences, switching from task-independent to meta learning rate schedules notably changes the accumulation versus retention tradeoff behaviour. While within different meta-schedules variations there is limited difference, as shown in fig. 4 (_left_ and _right_), meta-learning rate schedules allow for significantly better retention of initial zero-shot transfer performance. In the case of meta-schedules deriving from cosine learning rate schedules, there is a severe reduction in accumulated new knowledge due to the fast reduction in the learning rate (fig. 3_left_). Meta-schedules deriving from infinite learning rate schedules like _rsqrt_ lend themselves much better to longer-horizon continual pretraining tasks due to the less aggressive decay in learning rate within tasks: As shown in fig. 3 (_right_), the autoregressive _rsqrt_ meta-schedule achieves strong gains in \(\mathcal{A}_{KA}\), while _vastly increasing the amount of retained knowledge_; exceeding the hypothetical linear zero-shot vs joint finetuning trade-off line.

Figure 4: **Meta-scheduling task-specific LR scheduler has significant impact on the knowledge accumulation and retention trade-off, with meta-schedules derived from infinite LR schedules showing significant transitions across the zeroshot vs finetuning threshold; moving close to accumulation performance of task-independent scheduling, but retaining significantly more pretraining knowledge.**

### Scaling up Model and Compute Budgets

To understand the impact of both model and compute scaling on the ability to continual pretrain over longer update cycles, we adjust either the underlying vision transformer size (keeping the number of update steps and task iterations fixed, and covering ViT-S/16 \([62.3M]\), B/16 \([149.6M]\), L/14 \([427.62M]\), H/14 \([986.11M]\) and g/14 \([1366.68M]\) taken from [29]) or the allocated compute budget for a fixed model size (selecting our default ViT-B/16 and the default derived finetuning compute budget of \(1.8\times 10^{9}\) FLOPs as reference). Results for both are provided in fig. 5.

Scaling Model Size.As can be seen, we find that with a controlled increase of model size, the ability to continually pretrain over longer minor update cycles improves. While the absolute change in knowledge accumulation \(\mathcal{A}_{KA}\) remains rather consistent (within the interval of \(8\%\) and \(10\%\)), zero-shot retention \(\mathcal{A}_{ZS}\) improves - where both for the joint finetuning upper bound and continual pretraining, we see improved knowledge retention, and in parts even slight positive backward transfer for ViT-L14 (\(3\times\) ViT-B/16). For ViT-B/16, we see \(\Delta\mathcal{A}_{KA}\approx 9.0\%\) and negative \(\Delta\mathcal{A}_{ZS}\approx 3.2\%\), while for larger L/14, H/14 and g/14 we find \((\Delta_{KA}^{\text{L14}}\approx 9.4,\Delta_{ZS}^{\text{L14}}\approx 0.8)\), \((\Delta_{KA}^{\text{H/14}}\approx 10.1\%,\Delta_{ZS}^{\text{H/14}} \approx-1.5\%)\) and \((\Delta_{KA}^{\text{H/14}}\approx 9.8\%,\Delta_{ZS}^{\text{H/14}} \approx-0.05\%)\). Even with higher initial generalization performance, the rate of knowledge accumulation remains roughly the same or even increases, while the ability to maintain its initial generalization capabilities through the longer update cycles in parts _notably improves_. These results suggest that model scaling can benefit long-term re-use and the opportunity to maintain and consistently improve the base model over longer minor update cycles, suggesting model scaling helps mitigate forgetting [145].

Scaling Compute Budgets.Instead of investing compute for scaling model size, one can also adjust the directly allocated compute budgets. For our reference model B/16 and its associated compute budget of \(1.8\times 10^{9}\) FLOPs, we thus study \(2\times\), \(4\times\) and \(6\times\) increases, as well as \(0.5\times\) and \(0.25\times\) reductions. As seen in fig. 5 (_right_) which aggregates knowledge accumulation \(\mathcal{A}_{KA}\) and zero-shot retention \(\mathcal{A}_{ZS}\) through their geometric mean, simple continual finetuning (brown) can not consistently leverage increased compute budgets. However, coupled with simple model merging, we find that models become much better at effectively utilizing the additional budget increase; exhibit a log-linear budget-performance relation. With much lower aggregate accumulation-retention performance, we also find a similar, slightly weaker compute scaling behavior for adapter-based continual pretraining.

## 6 Continual Pretraining: A Data-Centric Perspective

This section provides an important data-centric perspective on continual multimodal pretraining (with more information and experiments available in appendix E). We study how fine-grained constraints on the sequence of tasks within an update cycle \(\pi\) influence favorable trade-offs between between knowledge accumulation \(\mathcal{A}_{\text{KA}}\) and zero-shot retention \(\mathcal{A}_{\text{ZS}}\) (appendix E.3). Results on the impact of different deployment scenarios on continual pretrainability are visualized in fig. 6 for the following scenarios (appendix B.4): **(1)**performance sorted - transition from easy to hard concepts, **(2)** concept-frequency sorted - rare pretraining concepts first, **(3)** concept-similarity sorted

Figure 5: **Model and Compute Scaling for Continual Pretraining. (_Left_) Increasing model size from ViT S/16 to ViT g/14 scales zero-shot performance consistently. In conjunction however, we find that incorporating new context comes with a _reduced_ impact on knowledge retention. (_Right_) For continual finetuning (with/without model merging), as well as LoRA adapters, we consistently increase the allocated compute budget (for B/16). For normal finetuning, an optimum is reached early. With model merging, we instead see a log-linear scaling in performance with additional compute.**each update contains concepts semantically related to the preceding update, and **(4)**random sorting. Dataset-incremental as well as time-incremental minor updates are studied separately due to their different structure in section 6, and reverse streams are investigated in section 6.

**Concept- and Sample-based Deployment Scenarios.** Across the deployment scenarios in fig. 6 (leftmost), while the concept-frequency stream (in green) has the marginally best \(\mathcal{A}_{\text{KA}}\)-\(\mathcal{A}_{\text{ZS}}\) tradeoff with \(\mathcal{A}_{\text{KA}}{=}55.2\), \(\mathcal{A}_{\text{ZS}}{=}65.6\), and performance (in pink) performs worst (\(\mathcal{A}_{\text{KA}}{=}53.8\), \(\mathcal{A}_{\text{ZS}}{=}64.3\)), we find that _convergence end-points are surprisingly similar_ - especially w.r.t. the initial zero-shot and the joint finetuning upper bound reference points. However, while endpoints are remarkably similar, different orderings \(\pi\) induce significantly different trajectories in the accumulation-retention space, with similarity the most sample inefficient ordering, while random produces the most favorable trajectories. This aligns with prior work from curriculum learning and active learning that have suggested the efficacy of random curriculums [120; 199], which we find extends itself well into the domain of longer-horizon continual pretraining over minor updates. These insights mean that for longer update trajectories and a shared total space of subdomains and tasks of interest, the type and order of model updates primarily impact initial model versions. This is crucial to account for with respect to the model release horizon and the expected time frame before conducting large-scale continual pretraining updates. However, it also means that across long update horizons irrespective of particular task orders, continually pretrained models arrive at similar performance breakpoints.

**Dataset- and Time-based Deployment Scenarios** differ from the previous scenarios, in that each update step generally contains more semantically grouped samples. As we find for both cases (randomly ordering datasets in dataset or time-ordering in time), such an update format induces significantly higher trajectory variance, with lesser trajectory coherence when compared to the other four studied streaming orderings. This is expected given prior work suggesting that visual datasets encode heavy biases [176; 105], and hence tasks that explicitly separate these datasets induce larger distribution shifts than tasks that smoothly mix data samples across the datasets on a concept-level. Still, the degree of accumulation remains comparable, though we find zero-shot retention impacted disproportionately higher when orderings \(\pi\) or designed on a dataset-level (down to \(\mathcal{A}_{ZS}\approx 62.8\%\), compared to \(\mathcal{A}_{ZS}^{\text{random}}\approx 64.4\%\), \(\mathcal{A}_{ZS}^{\text{frequency}}\approx 65.5\%\) in the best case). This is important to account for when designing minor updates with the goal of retaining original zero-shot performance.

## 7 Conclusion

This work introduces FoMo-In-Flux - a novel, large-scale, fine-grained controllable and long horizon continual pretraining benchmark for vision-language foundation models. Using FoMo-In-Flux, we conduct an extensive study into continual multimodal pretraining from a _data-, method-_, and training-centric_ perspective. Key findings show that **(1)** model merging strategies successfully trade-off between acquisition of new knowledge and retention of pretraining knowledge, **(2)** learning rates matter; and are well accounted for via meta scheduling, **(3)** that increased model size facilitates inclusion of new knowledge without overwriting pretraining context, **(4)** that simple compute scaling does not benefit all methods equally - with model merging exhibiting the most favorable properties, **(5)** that the order of updates impact the models trajectory in accumulation-retention space, but only marginally impact the streaming endpoints, and that **(6)** replaying on buffer data during streaming is generally more important than replaying on (various subsets of) the original pretraining data.

Figure 6: **A Data-centric Perspective on Continual Pretraining. _(Left)_ Four concept-level stream orderings \(\pi\) emulating potential update cycles (c.f. section 3.3). Results indicate that deployment scenarios heavily impact intermediate model update stages; however when update cycles operate over shared underlying data distributions, continual pretraining endpoints end up _highly similar. (Center)_ Dataset-level (random or time-incremental) update cycles exhibit less stable deployment trajectories due to high dataset biases [176; 105].(Right)_ Reversing concept-level datastreams (see appendix E.1) reveals significant trajectory changes. However, the end point similarity still persists.**

## Acknowledgements

The authors would like to thank Lukas Thede, Nikhil Parthasarathy, Shashwat Goel and Shyamgopal Karthik for helpful feedback. The authors would also like to thank Kristina Kapanova (University of Tuebingen) for help with infrastructure and compute resources for running all our experiments. VU, KR and SD thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS). VU, KR and SD also thank the European Laboratory for Learning and Intelligent Systems (ELLIS) PhD program for support. VU was supported by a Google PhD Fellowship in Machine Intelligence. SA is supported by a Newton Trust Grant. MB acknowledges financial support via the Open Philanthropy Foundation funded by the Good Ventures Foundation. MB is a member of the Machine Learning Cluster of Excellence, funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy - EXC number 2064/1 - Project number 390727645. ZA acknowledges the support from the German Research Foundation (DFG): SFB 1233, Robust Vision: Inference Principles and Neural Mechanisms, project number: 276693517 and ERC Grant DEXIM, project number: 853489.

## References

* [1] Dreamlike photoreal v2.0. https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0.25, 26
* [2] 10 monkey species. URL https://www.kaggle.com/datasets/slothkong/10-monkey-species.27, 40
* [3] Davide Abati, Jakub Tomczak, Tijmen Blankevoort, Simone Calderara, Rita Cucchiara, and Babak Ehteshami Bejnordi. Conditional channel gated networks for task-aware continual learning. In _CVPR_, 2020.
* [4] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [5] Takuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, and David Ha. Evolutionary optimization of model merging recipes. _arXiv preprint arXiv:2403.13187_, 2024.
* [6] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016.
* [7] Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/97af07a14cacba681feac73012730892-Paper.pdf.
* [8] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, _ACL (Short)_, 2022.
* [9] Thomas Berg, Jiongxin Liu, Seung Woo Lee, Michelle L. Alexander, David W. Jacobs, and Peter N. Belhumeur. Birds snap: Large-scale fine-grained visual categorization of birds. In _2014 IEEE Conference on Computer Vision and Pattern Recognition_, pages 2019-2026, 2014. doi: 10.1109/CVPR.2014.259.
* a comprehensive real-world dataset for unsupervised anomaly detection. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2019.
* [11] Dan Biderman, Jose Gonzalez Ortiz, Jacob Portes, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, Cody Blakeney, and John P. Cunningham. Lora learns less and forgets less, 2024. URL https://arxiv.org/abs/2405.09673.
* [12] Dan Biderman, Jose Gonzalez Ortiz, Jacob Portes, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, et al. Lora learns less and forgets less. _arXiv preprint arXiv:2405.09673_, 2024.

* [13] Massimo Bini, Karsten Roth, Zeynep Akata, and Anna Khoreva. Ether: Efficient finetuning of large-scale models with hyperplane reflections, 2024. URL https://arxiv.org/abs/2405.20271.
* [14] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ B. Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorotuys Demszky, Chris Donahue, Moussa Dombuoya, Esin Durmus, Stefano Ermon, John Etchencandy, Kawin Etharyaraj, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah B. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, and et al. On the opportunities and risks of foundation models. _CoRR_, abs/2108.07258, 2021. URL http://dblp.uni-trier.de/db/journals/corr/corr2108.html.
* [15] Jorg Bornschein, Alexandre Galashov, Ross Hemsley, Amal Rannen-Triki, Yutian Chen, Arslan Chaudhry, Xu Owen He, Arthur Douillard, Massimo Caccia, Qixuan Feng, et al. Nevis '22: A stream of 100 tasks sampled from 30 years of computer vision research. _JMLR_, 2023.
* [16] Matteo Boschini, Lorenzo Bonicelli, Pietro Buzzega, Angelo Porrello, and Simone Calderara. Class-incremental continual learning into the extended der-verse. _TPAMI_, 2022.
* [17] Matteo Boschini, Lorenzo Bonicelli, Pietro Buzzega, Angelo Porrello, and Simone Calderara. Class-incremental continual learning into the extended der-verse. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, page 1-16, 2022. ISSN 1939-3539. doi: 10.1109/tpami.2022.3206549. URL http://dx.doi.org/10.1109/TPAMI.2022.3206549.
* mining discriminative components with random forests. In _European Conference on Computer Vision_, 2014.
* [19] Chris Burgess and Hyunjik Kim. 3d shapes dataset. https://github.com/deepmind/3dshapes-dataset/, 2018.
* [20] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for general continual learning: a strong, simple baseline. In _NeurIPS_, 2020.
* [21] Zhipeng Cai, Ozan Sener, and Vladlen Koltun. Online continual learning with natural distribution shifts: An empirical study with visual data. In _ICCV_, 2021.
* [22] Fabio Cermelli, Massimiliano Mancini, Samuel Rota Bulo, Elisa Ricci, and Barbara Caputo. Modeling the background for incremental learning in semantic segmentation. In _CVPR_, 2020.
* [23] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In _CVPR_, 2021.
* [24] Arslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient lifelong learning with a-gem. In _ICLR_, 2019.
* [25] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and Marc'Aurelio Ranzato. On tiny episodic memories in continual learning. _arXiv preprint arXiv:1902.10486_, 2019.
* [26] Cheng Chen, Junchen Zhu, Xu Luo, Hengtao Shen, Lianli Gao, and Jingkuan Song. Coin: A benchmark of continual instruction tuning for multimodel large language model. _arXiv preprint arXiv:2403.08350_, 2024.
* [27] Jie Chen, Zhipeng Chen, Jiapeng Wang, Kun Zhou, Yutao Zhu, Jinhao Jiang, Yingqian Min, Wayne Xin Zhao, Zhicheng Dou, Jiaxin Mao, et al. Towards effective and efficient continual pre-training of large language models. _arXiv preprint arXiv:2407.18743_, 2024.
* [28] Qizhou Chen, Taolin Zhang, Dongyang Li, Longtao Huang, Hui Xue, Chengyu Wang, and Xiaofeng He. Lifelong knowledge editing for llms with retrieval-augmented continuous prompt learning. _arXiv preprint arXiv:2405.03279_, 2024.
* [29] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. _arXiv preprint arXiv:2212.07143_, 2022.

* [30] Pinaki Nath Chowdhury, Aneeshan Sain, Ayan Kumar Bhunia, Tao Xiang, Yulia Gryaditskaya, and Yi-Zhe Song. Fs-coco: Towards understanding of freehand sketches of common objects in context. In _ECCV_, 2022.
* [31] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed,, and A. Vedaldi. Describing textures in the wild. In _Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2014.
* [32] Adam Coates, Andrew Ng, and Honglak Lee. An Analysis of Single Layer Networks in Unsupervised Feature Learning. In _AISTATS_, 2011. https://cs.stanford.edu/~acoates/papers/coatesleeng_aistats_2011.pdf.
* [33] Guillaume Couairon, Matthijs Douze, Matthieu Cord, and Holger Schwenk. Embedding arithmetic of multimodal queries for image retrieval. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4950-4958, 2022.
* [34] Nico Dabeim, Thomas Mollenhoff, Edoardo Ponti, Iryna Gurevych, and Mohammad Emtiyaz Khan. Model merging by uncertainty-based gradient matching. In _ICLR_, 2024.
* [35] Matthias De Lange and Tinne Tuytelaars. Continual prototype evolution: Learning online from non-stationary data streams. In _ICCV_, 2021.
* [36] Matthias De Lange, Gido van de Ven, and Tinne Tuytelaars. Continual evaluation for lifelong learning: Identifying the stability gap. _arXiv preprint arXiv:2205.13452_, 2022.
* [37] Thomas De Min, Massimiliano Mancini, Karteek Alahari, Xavier Alameda-Pineda, and Elisa Ricci. On the effectiveness of layernorm tuning for continual learning in vision transformers. In _ICCV-W_, 2023.
* [38] Mostafa Dehghani, Yi Tay, Anurag Arnab, Lucas Beyer, and Ashish Vaswani. The efficiency misnomer. In _ICLR_, 2022.
* [39] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* [40] Li Deng. The mnist database of handwritten digit images for machine learning research. _IEEE Signal Processing Magazine_, 29(6):141-142, 2012.
* [41] Nick DiSanto. Isic melanoma dataset, 2023. URL https://dx.doi.org/10.21227/9p2y-yq09.
* [42] Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Shuyang Gu, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, and Nenghai Yu. Clip itself is a strong fine-tuner: Achieving 85.7% and 88.0% top-1 accuracy with vit-b and vit-l on imagenet. _arXiv preprint arXiv:2212.06138_, 2022.
* [43] Yanai Elazar, Akshita Bhagia, Ian Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, et al. What's in my big data? _arXiv preprint arXiv:2310.20707_, 2023.
* [44] Christian Ertler, Jerneja Mislej, Tobias Ollmann, Lorenzo Porzi, Gerhard Neuhold, and Yubin Kuang. The mapillary traffic sign dataset for detection and classification on a global scale, 2020.
* [45] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. _NeurIPS_, 2023.
* [46] Irena Gao, Gabriel Ilharco, Scott Lundberg, and Marco Tulio Ribeiro. Adaptive testing of computer vision models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4003-4014, 2023.
* [47] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. _IJCV_, 2024.
* [48] Qiankun Gao, Chen Zhao, Yifan Sun, Teng Xi, Gang Zhang, Bernard Ghanem, and Jian Zhang. A unified continual learning framework with general parameter-efficient tuning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 11483-11493, 2023.
* [49] Saurabh Garg, Mehrdad Farajtabar, Hadi Pouransari, Raviteja Vemulapalli, Sachin Mehta, Oncel Tuzel, Vaishaal Shankar, and Fardash Faghri. Tic-clip: Continual training of clip models. In _ICLR_, 2024.
** [50] Yasir Ghunaim, Adel Bibi, Kumail Alhamoud, Motasem Alfarra, Hasan Abed Al Kader Hammoud, Ameya Prabhu, Philip HS Torr, and Bernard Ghanem. Real-time evaluation in online continual learning: A new paradigm. In _CVPR_, 2023.
* [51] Evangelia Gogoulou, Timothee Lesort, Magnus Boman, and Joakim Nivre. A study of continual learning under language shift. _arXiv preprint arXiv:2311.01200_, 2023.
* [52] Dipam Goswami, Yuyang Liu, Bartlomiej Twardowski, and Joost van de Weijer. Fecam: Exploiting the heterogeneity of class distributions in exemplar-free continual learning. _NeurIPS_, 2023.
* [53] Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour, 2018.
* [54] Sachin Goyal, Ananya Kumar, Sankalp Garg, Zico Kolter, and Aditi Raghunathan. Finetune like you pretrain: Improved finetuning of zero-shot vision models, 2022.
* [55] Gregory Griffin, Alex Holub, and Pietro Perona. _Caltech-256 Object Category Dataset_. Mar 2007.
* [56] Yanan Gu, Xu Yang, Kun Wei, and Cheng Deng. Not just selection, but exploration: Online class-incremental continual learning via dual view consistency. In _CVPR_, 2022.
* [57] Zhongrui Gui, Shuyang Sun, Runjia Li, Jianhao Yuan, Zhaochong An, Karsten Roth, Ameya Prabhu, and Philip Torr. knn-clip: Retrieval enables training-free segmentation on continually expanding large vocabularies. _arXiv preprint arXiv:2404.09447_, 2024.
* [58] Guy Hacohen and Daphna Weinshall. On the power of curriculum learning in training deep networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 2535-2544. PMLR, 09-15 Jun 2019. URL https://proceedings.mlr.press/v97/hacohen19a.html.
* [59] Alexander Hagele, Elie Bakouch, Atil Kosson, Loubna Ben Allal, Leandro Von Werra, and Martin Jaggi. Scaling laws and compute-optimal training beyond fixed training durations. _arXiv preprint arXiv:2405.18392_, 2024.
* [60] Hasan Abed Al Kader Hammoud, Hani Itani, Fabio Pizzati, Philip Torr, Adel Bibi, and Bernard Ghanem. Syntchlip: Are we ready for a fully synthetic clip training?, 2024.
* [61] Jinwei Han, Zhiwen Lin, Zhongyisun Sun, Yingguo Gao, Ke Yan, Shouhong Ding, Yuan Gao, and Gui-Song Xia. Anchor-based robust finetuning of vision-language models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 26919-26928, 2024.
* [62] Md Yousuf Harun and Christopher Kanan. Overcoming the stability gap in continual learning. _arXiv preprint arXiv:2306.01904_, 2023.
* [63] Md Yousuf Harun, Jhair Gallardo, Tyler L Hayes, and Christopher Kanan. How efficient are today's continual learning algorithms? _CVPR-W_, 2023.
* [64] Jinghan He, Haiyun Guo, Ming Tang, and Jinqiao Wang. Continual instruction tuning for large multimodal models. _arXiv preprint arXiv:2311.16206_, 2023.
* [65] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification, 2017.
* [66] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. _ICCV_, 2021.
* [67] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. _CVPR_, 2021.
* [68] Haikel Hichri. NWPU-RESISC45 Dataset with 12 classes. 9 2021. doi: 10.6084/m9.figshare.16674166.v1. URL https://figshare.com/articles/dataset/NWPU-RESISC45_Dataset_with_12_classes/16674166.
* [69] Saihui Hou, Yushan Feng, and Zilei Wang. Vegfru: A domain-specific dataset for fine-grained visual categorization. In _Proceedings of the IEEE International Conference on Computer Vision (ICCV)_, Oct 2017.
** Hou et al. [2019] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 831-839, 2019.
* Houben et al. [2013] Sebastian Houben, Johannes Stallkamp, Jan Salmen, Marc Schlipsing, and Christian Igel. Detection of traffic signs in real-world images: The German Traffic Sign Detection Benchmark. In _International Joint Conference on Neural Networks_, number 1288, 2013.
* Hu et al. [2022] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9.
* Hu et al. [2022] Hexiang Hu, Ozan Sener, Fei Sha, and Vladlen Koltun. Drinking from a firehose: Continual learning with web-scale natural language. _IEEE Transactions on Pattern Analysis and Machine Intelligence, to appear_, 2022.
* Hu et al. [2024] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. _arXiv preprint arXiv:2404.06395_, 2024.
* Hughes and Salathe [2015] David P. Hughes and Marcel Salathe. An open access repository of images on plant health to enable the development of mobile disease diagnostics through machine learning and crowdsourcing. _CoRR_, abs/1511.08060, 2015. URL http://arxiv.org/abs/1511.08060.
* Ibrahim et al. [2024] Adam Ibrahim, Benjamin Therien, Kshitij Gupta, Mats L Richter, Quentin Anthony, Timothee Lesort, Eugene Belilovsky, and Irina Rish. Simple and scalable strategies to continually pre-train large language models. _arXiv preprint arXiv:2403.08763_, 2024.
* Ikezogwo et al. [2023] Wisdom Oluuchi Ikezogwo, Mehmet Saygin Seyfioglu, Fatemeh Ghezloo, Dylan Stefan Chan Geva, Fatwir Sheikh Mohammed, Pavan Kumar Anand, Ranjay Krishna, and Linda Shapiro. Quilt-1m: One million image-text pairs for histopathology. In _Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2023. URL https://openreview.net/forum?id=OL2JQo0Okq.
* Ilharco et al. [2022] Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating weights. In _NeurIPS_, 2022.
* [79] iNaturalist 2021 competition dataset. iNaturalist 2021 competition dataset. https://github.com/visipedia/inat_comp/tree/master/2021, 2021.
* Isola et al. [2015] Phillip Isola, Joseph J. Lim, and Edward H. Adelson. Discovering states and transformations in image collections. In _CVPR_, 2015.
* Jang et al. [2021] Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Stanley Jungkyu Choi, and Minjoon Seo. Towards continual knowledge learning of language models. _arXiv preprint arXiv:2110.03215_, 2021.
* Jang et al. [2022] Joel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, and Minjoon Seo. Temporalwiki: A lifelong benchmark for training and evaluating ever-evolving language models. _arXiv preprint arXiv:2204.14211_, 2022.
* Johnson et al. [2017] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, July 2017.
* Kembhavi et al. [2016] Aniruddha Kembhavi, Michael Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is worth a dozen images. _ArXiv_, abs/1603.07396, 2016. URL https://api.semanticscholar.org/CorpusID:2682274.
* Khattak et al. [2023] Muhammad Uzair Khattak, Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, and Fahad Shahbaz Khan. Self-regulating prompts: Foundational model adaptation without forgetting. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 15190-15200, 2023.

* Kirkpatrick et al. [2017] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. _PNAS_, 2017.
* Koh et al. [2021] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. Wilds: A benchmark of in-the-wild distribution shifts, 2021.
* Kopiczko et al. [2024] Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki M Asano. VeRA: Vector-based random matrix adaptation. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=N3JfLdxr3A.
* Kozal et al. [2024] Jedrzej Kozal, Jan Wasilewski, Bartosz Krawczyk, and Michal Wozniak. Continual learning with weight interpolation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4187-4195, 2024.
* Krasin et al. [2016] Ivan Krasin, Tom Duerig, Neil Alldrin, Andreas Veit, Sami Abu-El-Haija, Serge Belongie, David Cai, Zheyun Feng, Vittorio Ferrari, Victor Gomes, Abhinav Gupta, Dhyanesh Narayanan, Chen Sun, Gal Chechik, and Kevin Murphy. Openimages: A public dataset for large-scale multi-label and multi-class image classification. _Dataset available from https://github.com/openimages_, 2016.
* Krizhevsky [2009] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
* Krizhevsky [2014] Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks, 2014. URL http://arxiv.org/abs/1404.5997. cite arxiv:1404.5997.
* Krizhevsky et al. [2016] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 (canadian institute for advanced research). URL http://www.cs.toronto.edu/~kriz/cifar.html.
* Li et al. [2023] Fei-Fei Li, Marco Andreeto, Marc'Aurelio Ranzato, and Pietro Perona. Caltech 101, Apr 2022.
* Li et al. [2023] Haoran Li, Jingfeng Wu, and Vladimir Braverman. Fixed design analysis of regularization-based continual learning. In _Conference on Lifelong Learning Agents_, pages 513-533. PMLR, 2023.
* Li et al. [2023] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023.
* Liang et al. [2022] Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Y Zou. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. _Advances in Neural Information Processing Systems_, 35:17612-17625, 2022.
* Liang and Li [2024] Yan-Shuo Liang and Wu-Jun Li. Inflora: Interference-free low-rank adaptation for continual learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 23638-23647, 2024.
* Liao et al. [2022] Peiyuan Liao, Xiuyu Li, Xihui Liu, and Kurt Keutzer. The artbench dataset: Benchmarking generative models with artworks. _arXiv preprint arXiv:2206.11404_, 2022.
* Lin et al. [2014] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll'a r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. _CoRR_, abs/1405.0312, 2014. URL http://arxiv.org/abs/1405.0312.
* Lin et al. [2021] Zhiqiu Lin, Jia Shi, Deepak Pathak, and Deva Ramanan. The clear benchmark: Continual learning on real-world imagery. In _NeurIPS_, 2021.
* Lin et al. [2022] Zhiqiu Lin, Deepak Pathak, Yu-Xiong Wang, Deva Ramanan, and Shu Kong. Continual learning with evolving class ontologies. _Advances in Neural Information Processing Systems_, 35:7671-7684, 2022.
* Liska et al. [2022] Adam Liska, Tomas Kocisky, Elena Gribovskaya, Tayfun Terzi, Eren Sezener, Devang Agrawal, D'Autume Cyprien De Masson, Tim Scholtes, Manzil Zaheer, Susannah Young, et al. Streamingqa: A benchmark for adaptation to new knowledge over time in question answering models. In _International Conference on Machine Learning_, pages 13604-13622. PMLR, 2022.
* Liu et al. [2024] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation, 2024.
* Liu et al. [2022]* [105] Zhuang Liu and Kaiming He. A decade's battle on dataset bias: Are we there yet?, 2024. URL https://arxiv.org/abs/2403.08632.
* [106] Vincenzo Lomonaco and Davide Maltoni. Core50: a new dataset and benchmark for continuous object recognition. In _Conference on robot learning_, pages 17-26. PMLR, 2017.
* [107] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* [108] Yilin Lyu, Liyuan Wang, Xingxing Zhang, Zicheng Sun, Hang Su, Jun Zhu, and Liping Jing. Overcoming recency bias of normalization statistics in continual learning: Balance and adaptation. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL https://openreview.net/forum?id=Ph65E1bE6A.
* [109] Simone Magistri, Joost van de Weijer, Andrew D Bagdanov, et al. An empirical analysis of forgetting in pre-trained models with incremental low-rank updates. _arXiv preprint arXiv:2405.18069_, 2024.
* [110] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. Technical report, 2013.
* [111] Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In _2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018_, pages 7765-7773. Computer Vision Foundation / IEEE Computer Society, 2018. doi: 10.1109/CVPR.2018.00810. URL http://openaccess.thecvf.com/content_cvpr_2018/html/Mallya_PackNet_Adding_Multiple_CVPR_2018_paper.html.
* [112] Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multiple tasks by learning to mask weights. In _Proceedings of the European Conference on Computer Vision (ECCV)_, September 2018.
* [113] Daniel Marczak, Bartlomiej Twardowski, Tomasz Trzcinski, and Sebastian Cygert. Magmax: Leveraging model merging for seamless continual learning. _arXiv preprint arXiv:2407.06322_, 2024.
* [114] Imad Eddine Marouf, Subhankar Roy, Enzo Tartaglione, and Stephane Lathuiliere. Weighted ensemble models are strong continual learners. _arXiv preprint arXiv:2312.08977_, 2023.
* [115] Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentanglement testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017.
* [116] Mark D McDonnell, Dong Gong, Amin Parveneh, Ehsan Abbasnejad, and Anton van den Hengel. Rampac: Random projections and pre-trained models for continual learning. In _NeurIPS_, 2023.
* [117] Sachit Menon and Carl Vondrick. Visual classification via description from large language models. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=j1&jNLLg2ccs.
* [118] Otinel-Bogdan Mercea, Alexey Gritsenko, Cordelia Schmid, and Anurag Arnab. Time-, memory-and parameter-efficient visual adaptation. _arXiv preprint arXiv:2402.02887_, 2024.
* [119] Umberto Michieli and Pietro Zanuttigh. Incremental learning techniques for semantic segmentation. In _Proceedings of the IEEE/CVF international conference on computer vision workshops_, pages 0-0, 2019.
* [120] Sudhanshu Mittal, Maxim Tatarchenko, Ozgun Cicek, and Thomas Brox. Parting with illusions about deep active learning, 2019.
* [121] Jishnu Mukhoti, Yarin Gal, Philip HS Torr, and Puneet K Dokania. Fine-tuning can cripple your foundation model; preserving features may be the solution. _arXiv preprint arXiv:2308.13320_, 2023.
* [122] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. 2011.
* [123] Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Sewoong Oh, and Ludwig Schmidt. Quality not quantity: On the interaction between dataset design and robustness of clip. _Advances in Neural Information Processing Systems_, 35:21455-21469, 2022.
* [124] Thao Nguyen, Matthew Wallingford, Sebastian Santy, Wei-Chiu Ma, Sewoong Oh, Ludwig Schmidt, Pang Wei Koh, and Ranjay Krishna. Multilingual diversity improves vision-language representations. _arXiv preprint arXiv:2405.16915_, 2024.

* [125] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In _2008 Sixth Indian Conference on Computer Vision, Graphics and Image Processing_, pages 722-729, 2008. doi: 10.1109/ICVGIP.2008.47.
* [126] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. The oxford-iiit pet dataset. URL https://www.robots.ox.ac.uk/~vgg/data/pets/.
* [127] Jupinder Parmar, Sanjev Satheesh, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Reuse, don't retrain: A recipe for continued pretraining of language models. _arXiv preprint arXiv:2407.07263_, 2024.
* [128] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.
* [129] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, October 2019.
* Conference and Labs of the Evaluation Forum, Bologna, Italy, September 5th
- 8th, 2022_, volume 3180 of _CEUR Workshop Proceedings_, pages 1970-1981. CEUR-WS.org, 2022. URL https://ceur-ws.org/Vol-3180/paper-157.pdf.
* not just another image recognition dataset. 2021.
* [132] Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In _Proceedings of the IEEE International Conference on Computer Vision (ICCV)_, December 2015.
* [133] Angeline Pouget, Lucas Beyer, Emanuele Bugliarello, Xiao Wang, Andreas Peter Steiner, Xiaohua Zhai, and Ibrahim Alabdulmohsin. No filter: Cultural and socioeconomic diversityin contrastive vision-language models. _arXiv preprint arXiv:2405.13777_, 2024.
* [134] Ameya Prabhu, Philip HS Torr, and Puneet K Dokania. Gdumb: A simple approach that questions our progress in continual learning. In _ECCV_, 2020.
* [135] Ameya Prabhu, Zhipeng Cai, Puneet Dokania, Philip Torr, Vladlen Koltun, and Ozan Sener. Online continual learning without the storage constraint. _arXiv preprint arXiv:2305.09253_, 2023.
* [136] Ameya Prabhu, Hasan Abed Al Kader Hammoud, Puneet Dokania, Philip HS Torr, Ser-Nam Lim, Bernard Ghanem, and Adel Bibi. Computationally budgeted continual learning: What does matter? In _CVPR_, 2023.
* [137] Ameya Prabhu, Hasan Abed Al Kader Hammoud, Ser-Nam Lim, Bernard Ghanem, Philip HS Torr, and Adel Bibi. From categories to classifier: Name-only continual learning by exploring the web. _arXiv preprint arXiv:2311.11293_, 2023.
* [138] Ameya Prabhu, Shiven Sinha, Ponnurangam Kumaraguru, Philip HS Torr, Ozan Sener, and Puneet K Dokania. Randomb: A simple approach that constrains the efficacy of continual representation learning. _arXiv preprint arXiv:2402.08823_, 2024.
* [139] Sarah Pratt, Ian Covert, Rosanne Liu, and Ali Farhadi. What does a platypus look like? generating customized prompts for zero-shot image classification. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 15691-15701, October 2023.
* [140] Tom Preston-Werner. Semantic versioning 2.0.0, 2013. URL https://semver.org/.
* [141] Gao Qiankun, Zhao Chen, Sun Yifan, Xi Teng, Zhang Gang, Ghanem Bernard, and Zhang Jian. A unified continual learning framework with general parameter-efficient tuning. _ICCV_, 2023.
* [142] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
** Raffel [2021] Colin Raffel. A call to build models like we build opensource software, 2021. https://colinraffel.com/blog/a-call-to-build-models-like-we-build-open-source-software.html.
* Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of machine learning research_, 21(140):1-67, 2020.
* Ramasesh et al. [2021] Vinay Venkatesh Ramasesh, Aitor Lewkowycz, and Ethan Dyer. Effect of scale on catastrophic forgetting in neural networks. In _International Conference on Learning Representations_, 2021.
* Rame et al. [2024] Alexandre Rame, Nino Vieillard, Leonard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, and Johan Ferret. Warm: On the benefits of weight averaged reward models. _arXiv preprint arXiv:2401.12187_, 2024.
* Razzhigaev et al. [2023] Anton Razzhigaev, Arseniy Shakhmatov, Anastasia Maltseva, Vladimir Arkhipkin, Igor Pavlov, Ilya Ryabov, Angelina Kuts, Alexander Panchenko, Andrey Kuznetsov, and Denis Dimitrov. Kandinsky: an improved text-to-image synthesis with image prior and latent diffusion. _arXiv preprint arXiv:2310.03502_, 2023.
* Rebuffi et al. [2017] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. _Advances in neural information processing systems_, 30, 2017.
* Rebuffi et al. [2017] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In _CVPR_, 2017.
* Recht et al. [2019] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In _International Conference on Machine Learning_, pages 5389-5400, 2019.
* Roberts et al. [2023] Jonathan Roberts, Kai Han, and Samuel Albanie. Satin: A multi-task metadataset for classifying satellite imagery using vision-language models, 2023.
* Rojas et al. [2022] William A Gaviria Rojas, Sudanya Diamos, Keertan Ranjan Kini, David Kanter, Vijay Janapa Reddi, and Cody Coleman. The dollar street dataset: Images representing the geographic and socioeconomic diversity of the world. In _Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2022. URL https://openreview.net/forum?id=qnf?save004.
* Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10684-10695, June 2022.
* Roth et al. [2020] Karsten Roth, Timo Milbich, and Bjorn Ommer. Pads: Policy-adapted sampling for visual similarity learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2020.
* Roth et al. [2023] Karsten Roth, Mark Ibrahim, Zeynep Akata, Pascal Vincent, and Diane Bouchacourt. Disentanglement of correlated factors via hausdorff factorized support. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=OKcJhpQiGiX.
* Roth et al. [2023] Karsten Roth, Jae Myung Kim, A. Sophia Koepke, Oriol Vinyals, Cordelia Schmid, and Zeynep Akata. Waffling around for performance: Visual classification with random words and broad concepts. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 15746-15757, October 2023.
* Roth et al. [2024] Karsten Roth, Lukas Thede, A. Sophia Koepke, Oriol Vinyals, Olivier J Henaff, and Zeynep Akata. Fantastic gains and where to find them: On the existence and prospect of general knowledge transfer between any pretrained model. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=m50eKHcttz.
* Schrodi et al. [2024] Simon Schrodi, David T Hoffmann, Max Argus, Volker Fischer, and Thomas Brox. Two effects, one trigger: On the modality gap, object bias, and information imbalance in contrastive vision-language representation learning. _arXiv preprint arXiv:2404.07983_, 2024.
* Schuhmann et al. [2022] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _Advances in Neural Information Processing Systems_, 35:25278-25294, 2022.

* [160] Cristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M: Open dataset of CLIP-filtered 400 million image-text pairs. In _Proceedings of Neurips Data-Centric AI Workshop_, 2021.
* [161] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 2556-2565, 2018.
* [162] Guangyuan Shi, Jiaxin Chen, Wenlong Zhang, Li-Ming Zhan, and Xiao-Ming Wu. Overcoming catastrophic forgetting in incremental few-shot learning by finding flat minima, 2021.
* [163] Peiyang Shi, Michael C Welle, Marten Bjorkman, and Danica Kragic. Towards understanding the modality gap in clip. In _ICLR 2023 Workshop on Multimodal Representation Learning: Perks and Pitfalls_, 2023.
* [164] Yang Shu, Xingzhuo Guo, Jialong Wu, Ximei Wang, Jianmin Wang, and Mingsheng Long. CLIPood: Generalizing CLIP to out-of-distributions. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 31716-31731. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/shu23a.html.
* [165] Samarth Sinha, Animesh Garg, and Hugo Larochelle. Curriculum by smoothing. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 21653-21664. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/fea673f09493afcd8b129a0bcf1cd5bc-Paper.pdf.
* [166] James Seale Smith, Paola Cascante-Bonilla, Assaf Arbelle, Donghyun Kim, Rameswar Panda, David Cox, Diyi Yang, Zsolt Kira, Rogerio Feris, and Leonid Karlinsky. Construct-vl: Data-free continual structured vl concepts learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14994-15004, 2023.
* [167] James Seale Smith, Yen-Chang Hsu, Lingyu Zhang, Ting Hua, Zsolt Kira, Yilin Shen, and Hongxia Jin. Continual diffusion: Continual customization of text-to-image diffusion with c-lora. _arXiv preprint arXiv:2304.06027_, 2023.
* [168] James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta, Paola Cascante-Bonilla, Donghyun Kim, Assaf Arbelle, Rameswar Panda, Rogerio Feris, and Zsolt Kira. Coda-prompt: Continual decomposed attention-based prompting for rehearsal-free continual learning. In _CVPR_, 2023.
* [169] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted structured feature embedding. In _Computer Vision and Pattern Recognition (CVPR)_, 2016.
* [170] Petru Soviany, Radu Tudor Ionescu, Paolo Rota, and Nicu Sebe. Curriculum learning: A survey. _Int. J. Comput. Vision_, 130(6):1526-1565, jun 2022. ISSN 0920-5691. doi: 10.1007/s11263-022-01611-x. URL https://doi.org/10.1007/s11263-022-01611-x.
* [171] Tejas Srinivasan, Ting-Yun Chang, Leticia Pinto Alva, Georgios Chochlakis, Mohammad Rostami, and Jesse Thomason. Climb: A continual learning benchmark for vision-and-language tasks. _Advances in Neural Information Processing Systems_, 35:29440-29453, 2022.
* [172] Zafir Stojanovski, Karsten Roth, and Zeynep Akata. Momentum-based weight interpolation of strong zero-shot models for continual learning. _arXiv preprint arXiv:2211.03186_, 2022.
* [173] Hai-Long Sun, Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan. Pilot: A pre-trained model-based continual learning toolbox. _arXiv preprint arXiv:2309.07117_, 2023.
* [174] Yi Sun, Xin Xu, Jian Li, Guanglei Xie, Yifei Shi, and Qiang Fang. Continual learning through networks splitting and merging with dreaming-meta-weighted model fusion. _arXiv preprint arXiv:2312.07082_, 2023.
* [175] Lukas Thede, Karsten Roth, Olivier J. Henaff, Matthias Bethge, and Zeynep Akata. Reflecting on the state of rehearsal-free continual learning with pretrained models, 2024. URL https://arxiv.org/abs/2406.09384.
* [176] Antonio Torralba and Alexei A. Efros. Unbiased look at dataset bias. In _CVPR 2011_, pages 1521-1528, 2011. doi: 10.1109/CVPR.2011.5995347.
** [177] Vishaal Udandarao. Understanding and fixing the modality gap in vision-language models. _Master's thesis, University of Cambridge_, 2022.
* [178] Vishaal Udandarao, Max F Burg, Samuel Albanie, and Matthias Bethge. Visual data-type understanding does not emerge from scaling vision-language models. In _The Twelfth International Conference on Learning Representations_, 2023.
* [179] Vishaal Udandarao, Ankush Gupta, and Samuel Albanie. Sus-x: Training-free name-only transfer of vision-language models. In _ICCV_, 2023.
* [180] Vishaal Udandarao, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip HS Torr, Adel Bibi, Samuel Albanie, and Matthias Bethge. No "zero-shot" without exponential data: Pretraining concept frequency determines multimodal model performance. _arXiv preprint arXiv:2404.04125_, 2024.
* [181] Gido M van de Ven and Andreas S Tolias. Three scenarios for continual learning. In _NeurIPS-W_, 2018.
* [182] Tom Veniat, Ludovic Denoyer, and Marc'Aurelio Ranzato. Efficient continual learning with modular networks and task-driven priors. _arXiv preprint arXiv:2012.12631_, 2020.
* [183] Eli Verwimp, Shai Ben-David, Matthias Bethge, Andrea Cossu, Alexander Gepperth, Tyler L Hayes, Eyke Hullermeier, Christopher Kanan, Dhireesha Kudithipudi, Christoph H Lampert, et al. Continual learning: Applications and the road forward. _arXiv preprint arXiv:2311.11908_, 2023.
* [184] Eli Verwimp, Kuo Yang, Sarah Parisot, Lanqing Hong, Steven McDonagh, Eduardo Perez-Pellitero, Matthias De Lange, and Tinne Tuytelaars. Clad: A realistic continual learning benchmark for autonomous driving. _Neural Networks_, 161:659-669, 2023.
* [185] Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, et al. Freshlms: Refreshling large language models with search engine augmentation. _arXiv preprint arXiv:2310.03214_, 2023.
* [186] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. Caltech-ucsd birds-200-2011 (cub-200-2011). Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.
* [187] Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, and Shuming Shi. Knowledge fusion of large language models. _arXiv preprint arXiv:2401.10491_, 2024.
* [188] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In _Advances in Neural Information Processing Systems_, pages 10506-10518, 2019.
* [189] Jianren Wang, Xin Wang, Yue Shang-Guan, and Abhinav Gupta. Wanderlust: Online continual object detection in the real world. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 10829-10838, 2021.
* [190] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A comprehensive survey of continual learning: Theory, method and application, 2024. URL https://arxiv.org/abs/2302.00487.
* [191] Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. Wise: Rethinking the knowledge memory for lifelong model editing of large language models. _arXiv preprint arXiv:2405.14768_, 2024.
* [192] Xiao Wang, Yuansen Zhang, Tianze Chen, Songyang Gao, Senjie Jin, Xianjun Yang, Zhiheng Xi, Rui Zheng, Yicheng Zou, Tao Gui, et al. Trace: A comprehensive benchmark for continual learning in large language models. _arXiv preprint arXiv:2310.06762_, 2023.
* [193] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, et al. Dualprompt: Complementary prompting for rehearsal-free continual learning. In _European Conference on Computer Vision (ECCV)_, 2022.
* [194] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.
* [195] Yeming Wen, Dustin Tran, and Jimmy Ba. Batchensemble: an alternative approach to efficient ensemble and lifelong learning. _arXiv preprint arXiv:2002.06715_, 2020.
* [196] Martin Wistuba, Prabhu Teja Sivaprasad, Lukas Balles, and Giovanni Zappella. Continual learning with low rank adaptation. _arXiv preprint arXiv:2311.17601_, 2023.

* [197] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt. Robust fine-tuning of zero-shot models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 7959-7971, June 2022.
* [198] Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, and Gholamreza Haffari. Continual learning for large language models: A survey. _arXiv preprint arXiv:2402.01364_, 2024.
* [199] Xiaoxia Wu, Ethan Dyer, and Behnam Neyshabur. When do curricula work? In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=tW4QEInpni.
* [200] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu. Large scale incremental learning. In _CVPR_, 2019.
* [201] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.
* [202] Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In _2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, pages 3485-3492, 2010. doi: 10.1109/CVPR.2010.5539970.
* [203] Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. TIES-merging: Resolving interference when merging models. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL https://openreview.net/forum?id=xtaX3WvCj1.
* [204] Cagatay Yildiz, Nishaanth Kanna Ravichandran, Prishruit Punia, Matthias Bethge, and Beyza Ermis. Investigating continual pretraining in large language models: Insights and implications. _arXiv preprint arXiv:2402.17400_, 2024.
* [205] Aron Yu and Kristen Grauman. Fine-grained visual comparisons with local learning. In _2014 IEEE Conference on Computer Vision and Pattern Recognition_, pages 192-199, 2014. doi: 10.1109/CVPR.2014.32.
* [206] Jiazuo Yu, Yunzhi Zhuge, Lu Zhang, Ping Hu, Dong Wang, Huchuan Lu, and You He. Boosting continual learning of vision-language models via mixture-of-experts adapters. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 23219-23230, June 2024.
* [207] Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Yue Cao, Xinlong Wang, and Jingjing Liu. Capsfusion: Rethinking image-text data at scale. _arXiv preprint arXiv:2310.20550_, 2023.
* [208] Zhenghang Yuan, Lichao Mou, Qi Wang, and Xiao Xiang Zhu. From easy to hard: Learning language-guided curriculum for visual question answering on remote sensing data. _IEEE Transactions on Geoscience and Remote Sensing_, 60, 2022.
* [209] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In _ICML_, 2017.
* [210] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. A large-scale study of representation learning with the visual task adaptation benchmark, 2020.
* [211] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 12104-12113, June 2022.
* [212] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 18123-18133, 2022.
* [213] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 11975-11986, 2023.
* [214] Chenshuang Zhang, Fei Pan, Junmo Kim, In So Kweon, and Chengzhi Mao. Imagenet-d: Benchmarking neural network robustness on diffusion synthetic object. _CVPR_, 2024.
* [215] Gengwei Zhang, Liyuan Wang, Guoliang Kang, Ling Chen, and Yunchao Wei. Slsca++: Unleash the power of sequential fine-tuning for continual learning with pre-training, 2024. URL https://arxiv.org/abs/2408.08295.

* [216] Renrui Zhang, Rongyao Fang, Peng Gao, Wei Zhang, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free clip-adapter for better vision-language modeling. _arXiv preprint arXiv:2111.03930_, 2021.
* [217] Yuanhang Zhang, Zhidi Lin, Yiyong Sun, Feng Yin, and Carsten Fritsche. Regularization-based efficient continual learning in deep state-space models. _arXiv preprint arXiv:2403.10123_, 2024.
* [218] Zihan Zhang, Meng Fang, Ling Chen, and Mohammad-Reza Namazi-Rad. Citb: A benchmark for continual instruction tuning. _arXiv preprint arXiv:2310.14510_, 2023.
* [219] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. Galore: Memory-efficient llm training by gradient low-rank projection. _arXiv preprint arXiv:2403.03507_, 2024.
* [220] Zangwei Zheng, Mingyuan Ma, Kai Wang, Ziheng Qin, Xiangyu Yue, and Yang You. Preventing zero-shot transfer degradation in continual learning of vision-language models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 19125-19136, 2023.
* [221] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2017.
* [222] Da-Wei Zhou, Han-Jia Ye, De-Chuan Zhan, and Ziwei Liu. Revisiting class-incremental learning with pre-trained models: Generalizability and adaptivity are all you need. _arXiv preprint arXiv:2303.07338_, 2023.
* [223] Da-Wei Zhou, Hai-Long Sun, Jingyi Ning, Han-Jia Ye, and De-Chuan Zhan. Continual learning with pre-trained models: A survey. _arXiv preprint arXiv:2401.16386_, 2024.
* [224] Da-Wei Zhou, Hai-Long Sun, Han-Jia Ye, and De-Chuan Zhan. Expandable subspace ensemble for pre-trained model-based class-incremental learning. _arXiv preprint arXiv:2403.12030_, 2024.
* [225] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. _International Journal of Computer Vision_, 130(9):2337-2348, 2022.
* [226] Weixun Zhou, Shawn Newsam, Congmin Li, and Zhenfeng Shao. Patternnet: A benchmark dataset for performance evaluation of remote sensing image retrieval. _ISPRS Journal of Photogrammetry and Remote Sensing_, 145:197-209, November 2018. ISSN 0924-2716. doi: 10.1016/j.isprsjprs.2018.01.004. URL http://dx.doi.org/10.1016/j.isprsjprs.2018.01.004.
* [227] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, Quoc V Le, James Laudon, et al. Mixture-of-experts with expert choice routing. _Advances in Neural Information Processing Systems_, 35:7103-7114, 2022.
* [228] Zhen Zhu, Weijie Lyu, Yao Xiao, and Derek Hoiem. Continual learning in open-vocabulary classification with complementary memory systems. _arXiv preprint arXiv:2307.01430_, 2023.

Broader Impact, Limitations and Future Work

Limitations.In this work, our aim was to create a meaningful benchmark, provide practical guidelines, and offer insights into various multimodal continual pretraining scenarios. We focused on _continual, controlled, minor_ model updates. We developed FoMo-in-Flux to include many publicly accessible datasets covering a wide range of potential adaptation sub-domains. However, our findings on knowledge accumulation \(\mathcal{A}_{KA}\) and zero-shot retention \(\mathcal{A}_{ZS}\) are tied to our chosen adaptation and evaluation datasets. Consequently, though unlikely, various sub-domains relevant for future applications might not be sufficiently covered. Additionally, our methods were based off of default hyperparameter ranges from original publications (LoRA, VeRA, DoRA, BitFit, LNFit, FS-Merge, EMA-Merge) or continual learning repositories (mammoth [17]). While we tested the validity of each method and the chosen hyperparameters to elicit meaningful finetuning responses on respective single datasets (as highlighted _e.g._, for normal full-finetuning in Tab. 5), it overall means that our conclusions rely on the optimality of these provided hyperparameter ranges.

Broader Impact.Better continual model pretraining and the ability to minimize the need for large-scale model retraining can have significant impact on cost, compute and consequently environmental footprint. By encouraging research into extending the re-usability of large-scale pretrained models before a major continual model update or even full retraining from scratch is needed, we believe our work will lead to more economical and ecological utilization of foundation models. We do not believe that there are any immediate negative societal consequences as a result of this work, but we outline the limitations of our datasets in appendix K.

Future Work.Our benchmark and findings provide a crucial starting point reference for further research into continual multimodal pretraining. We sketch a few important and immediate future research directions:

* **(Meta-) Learning Rate Schedules and Beyond:** Our experiments show the importance of learning rate schedules (and meta-variants) designed for longer horizon continual (minor) model updates. We used a default cosine learning rate schedule and one infinite learning rate schedule (rsqrt), along with five meta-schedule variants, but our results showcase that there is a lot of potential in further exploring infinite schedules, as well as extensions into task- and order-conditioned learning rate schedules to allow for continual model pretraining and model updates.
* **Further Scaling Up Compute and Models:** We studied continual learning under realistic constraints (MAFs), with compute budgets derived from DataComp-small. Investigating other computational budgets including over-training, and extending budgets to be potentially task-order dependent could have practical relevance. Extending our insights to even larger model scales (ViT-bigG/14 and beyond) can offer further practical guidance. We have investigated the effect of model and compute scaling (see fig. 5) independently and to a first degree, however we believe there is a lot more exciting future work to be done.
* **Text-to-Image Generative Models:** Besides vision-language representation learning, FoMo-in-Flux can be used to study continuous minor updates of text-to-image generative models (such as generative diffusion models) on a fine-grained class and concept level, leveraging its diverse set of captions and information about respective image concepts.
* **Optimal Training Mixtures:** Our results indicate that knowledge retention during minor updates depends heavily on replaying data from previous tasks, guided towards "iid"-fying the learning task. This process helps prevent knowledge forgetting related to pretraining. However, there is room to better understand optimal training mixtures within limited compute budgets. Finding the best ways to allocate FLOPs and memory for replay on large pretraining data is crucial.

## Appendix B The FoMo-in-Flux Benchmark: Additional details.

### Creating our _obscure_ datasets

We first query ChatGPT to produce a set of \(100\) obscure animal names and \(100\) obscure object names. We then ask ChatGPT to produce diverse prompts for each class name to be used as text 

[MISSING_PAGE_FAIL:25]

**Continual Pretraining Updates.** Within the allocated update budget, at each update step \(j\in\{1,2,\ldots,T\}\), the following happens in order:

1. The stream reveals a task update pool of \(n_{j}\) image-text pairs \(\mathcal{D}_{j}=\{(i_{k}^{j},t_{k}^{j})\}_{k=1}^{n_{j}}\) spanning \(\mathcal{C}_{j}\) concepts.
2. We create the training data mixture \(\mathcal{S}_{j}\) by sampling from the pretraining data \(\mathcal{P}\), buffer \(\mathcal{B}\), and current task data \(\mathcal{D}_{j}\) with respective ratios \(\lambda_{\mathcal{P}},\lambda_{\mathcal{B}},\text{and}\)\(\lambda_{\mathcal{D}}\), such that \(\lambda_{\mathcal{P}}+\lambda_{\mathcal{B}}+\lambda_{\mathcal{D}}=1\).

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Dataset & \#Train & \#Test & \#Classes & Domain & License & Captions \\ \hline \multicolumn{8}{c}{**Classification-based**} \\ \hline A2DDiagrams [84] & 2720 & 681 & 15 & diagrams & CC BY-SA & generated \\ ArtBench[90] & 47531 & 11883 & 1870 & paintings & Fair Use & generated \\ Birds[9] & 31905 & 7977 & 500 & finegrained, natural & Unspecified, but academic usage & generated \\ Cirri100 [93] & 50000 & 10000 & 100 & natural & Unspecified, but academic usage & generated \\ CLFW[83] & 55931 & 13983 & 217 & synthetic & CC BY 4 & generated \\ CLB[51] & 13525 & 1475 & 25 & remote sensing & Academic purposes [151] & generated \\ Country211[142] & 31650 & 21100 & 211 & natural & various CC & generated \\ CUB200-201[186] & 5994 & 5794 & 200 & finegrained, natural & custom non-commercial & generated \\ DF20-mini [131] & 32724 & 3637 & 179 & finegrained, natural & custom non-commercial & generated \\ Dollarnet[152] & 13555 & 4103 & 1701 & finegrained, natural & CC BY-SA 4.0 & generated \\ Dominant-Cipart[129] & 33255 & 14604 & 345 & illustrations & custom non-commercial & generated \\ Dominance-Infograph[129] & 36023 & 15582 & 345 & diagrams & custom non-commercial & generated \\ Dominant-Panting[129] & 50416 & 21850 & 344 & paintings & custom non-commercial & generated \\ Dominant-Sketch[129] & 48212 & 20916 & 345 & sketch & custom non-commercial & generated \\ Dapries[115] & 75000 & 25000 & 27 & synthetic & Apache 2.0 & procedural \\ DTD[31] & 1880 & 1880 & 47 & textural & custom non-commercial & generated \\ FGVC Aircraft[110] & 3334 & 3333 & 100 & finegrained, natural & custom non-commercial & generated \\ Flowers100[122] & 6149 & 1020 & 102 & finegrained, natural & Unspecified, but academic usage & generated \\ FRU92[69] & 55814 & 9200 & 92 & finegrained, natural & Apache 2.0 & generated \\ Naturalit202[179] & 125000 & 25000 & 2500 & finegrained, natural & custom non-commercial & generated \\ Isicnelancan[41] & 2425 & 562 & 7 & medical & CC BY-NC & generated \\ Mitates[80] & 43002 & 10751 & 1959 & finegrained, natural & Unspecified, but academic usage & generated \\ Mot[44] & 59978 & 8737 & 227 & finegrained, traffic signs & CC BY-NC-SA 4.0 & generated \\ MVTec-AD (Base)[10] & 2903 & 726 & 15 & high-resolution, industrial & CC BY-NC-SA 4.0 & generated \\ MVTec-AD (Faints)[10] & 1380 & 345 & 88 & high-resolution, industrial & CC BY-NC-SA 4.0 & generated \\ ObjectNet[71] & 40314 & 10000 & 313 & natural & CC BY 4.0 & generated \\ Observe Animals & 17000 & 4238 & 74 & generative & MIT & custom \\ Observe Things & 19128 & 4758 & 84 & generative & MIT & custom \\ OpenImages[90] & 115333 & 8593 & 589 & natural & Apache 2.0 & available \\ PatternNet[266] & 26600 & 3800 & 38 & remote sensing & custom non-commercial & generated \\ Places365[21] & 120231 & 3649 & 365 & natural & custom non-commercial & generated \\ Pairwise[75] & 43444 & 10681 & 38 & finegrained, natural & CC0 & generated \\ Quil-IM[7] & 95822 & 23966 & 157 & medical & Academic purposes & available \\ Resiac5[68] & 18900 & 6300 & 45 & remote sensing & Unspecified, but academic usage & generated \\ Shapes3D[19] & 75000 & 25000 & 864 & synthetic & Apache 2.0 & procedural \\ StackLEF2023[19] & 150131 & 14117 & 1599 & finegrained, natural & custom non-commercial & generated \\ SUN37[202] & 15880 & 19850 & 397 & natural & custom non-commercial & generated \\ SynBLCP[60] & 84800 & 13886 & 106 & generative & CC BY-NC 4.0 & generated \\ Veg200[69] & 61117 & 20000 & 200 & finegrained, natural & Apache 2.0 & generated \\ Zapposes[30k] & 37829 & 9458 & 1847 & finegrained, object & custom non-commercial & generated \\ \hline \multicolumn{8}{c}{**Retrieval-based**} \\ \hline FSCCOCO[30] (arg T2/I2T R@5) & 7105 & 1777 & 115 & sketch & CC BY-NC 4.0 & Available \\ \hline
**Total** & **1759782** & **453020** & **18449** & \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Adaptation-only datasets over various visual and textual domains like diagrams, paintings, natural, synthetic or generative images, remote sensing, art styles, traffic signs or textural data; with datasets from Radford et al. [142] with lower zero-shot performance, common transfer or aggregation benchmark datasets such as DomainNet [129] or VTAB [210] and specialized datasets like MVTec-AD [10].**

Figure 9: **Examples of our generated obscure things and animals along with captions**, covering \(100\) rare and uncommon occurring things and animals. For each class, images are generated using either Kandinsky-2.1 [147], Stable Diffusion 2.1 [153] or Dreamlike-PhotoReal [1].

If samples in \(\mathcal{B}\) are insufficient (particularly at the start of task adaptation), we oversample from \(\mathcal{D}_{j}\), with \(\lambda_{\mathcal{D}}\) fixed.
3. We apply a continual update method \(\mathcal{M}\) with a fixed compute budget \(F\): \(\theta_{j}{=}\mathtt{train}(\mathcal{M},\mathcal{D}_{j},\theta_{j-1})\). This compute budget \(F\) also determines the overall number of update steps conducted.
4. We add samples from the update pool \(\mathcal{D}_{j}\) to the unrestricted buffer \(\mathcal{B}\). However, while all samples can be stored in buffer \(\mathcal{B}\), they cannot all be sampled for training set \(\mathcal{S}\), as the compute budget \(F\) imposes an implicit memory restriction [136].

How to Measure Continual Pretraining Computational Cost?To keep our setting practical and ensure a fair comparison, we impose a fixed computation cost budget for each time step to account for the efficiency of each method. However, there is no universally adopted measure of computational cost. Recent works use the number of iterations (forward/backward passes) [136, 49], number of parameters updated [88, 13, 118], FLOPs [50], and time/throughput [118]. However, a single metric does not paint a complete picture of efficiency that is relevant in practice [38, 118].

To account for this, we introduce _Memory-Adjusted-FLOPs (MAFs)_, a novel metric that highlights two aspects most relevant from a practitioner's perspective: the total number of FLOPs per iteration and the maximum utilization of device memory. To compute MAFs, we multiply the FLOPs count of each method by a _memory multiplier_, the ratio of that method's maximum memory utilization to the maximum memory utilization of a full fine-tuning of the base model. The total amount of MAFs for each method and backbone determines the allowed number of update steps each method can take during each adaptation task.

Data Restrictions.We allow unrestricted access to pretraining data (e.g., LAION-400M[160]), and an unlimited replay buffer \(\mathcal{B}\), as data storage is a negligible contributor to real-world cost [135, 136], and buffer memory is only utilized during the continual pretraining process. To study different retraining data pools, we use four popular image-text pretraining datasets of varying sizes, quality and curation strategies--LAION-400M[160], CC-12M[23], CC-3M[161], and DataComp-Small[45].

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Dataset & \# Train & \# Test & \# Classes & Domain & License & Captions \\ \hline \multicolumn{8}{c}{**Classification-based**} \\ \hline Caltech101 [94] & 6026 & 2651 & 101 & natural & CC BY 4.0 & generated \\ Caltech266 [53] & 2137 & 9300 & 257 & natural & CC BY 4.0 & generated \\ Cars196 [169] & 8144 & 8041 & 196 & finegarningd, natural & custom non-commercial & generated \\ Clarit91 [91] & 50000 & 10000 & 10 & natural, low-res & Unspecified, but academic usage & generated \\ Domain-Quickdraw [129] & 60375 & 28575 & 345 & sketch & custom non-commercial & generated \\ EuroSAT [65] & 18900 & 8100 & 10 & Remote Sensing & MIT & generated \\ FashionMNST [201] & 60000 & 10000 & 10 & bkw, low-res & MIT & generated \\ Food18 [18] & 75750 & 23250 & 101 & finegarningd, natural & Unspecified, but academic usage & generated \\ GTSRB [1] & 18635 & 8005 & 43 & traffic signs & CC0 & generated \\ ImageNet [90] & 0 & 50000 & 1000 & natural & custom non-commercial & generated \\ ImageNet-A [67] & 0 & 7500 & 200 & adversarial, natural & MIT & generated \\ ImageNet-D [214] & 0 & 4835 & 103 & generative & MIT & generated \\ ImageNet-R [66] & 0 & 30000 & 200 & renditions (e.g, sketch, paintings) & MIT & generated \\ ImageNet-S [18] & 0 & 50898 & 1000 & sketch & MIT & generated \\ ImageNet-V [25] & 0 & 10000 & 1000 & natural & MIT & generated \\ MNIST [40] & 60000 & 10000 & 10 & bkw, low-res & CC BY 3.0 & generated \\ Monkeys10 [2] & 1097 & 272 & 10 & natural & CC0 & generated \\ Oxfords [126] & 3680 & 3669 & 37 & natural & CC BY3.4 & 0 & generated \\ STL10 [32] & 5000 & 8000 & 10 & natural, low-res & custom non-commercial & generated \\ SVHN [12] & 73357 & 26032 & 10 & natural, low-res & custom non-commercial & generated \\ \hline \multicolumn{8}{c}{**Retrieval-based**} \\ \hline MSCOCO [100] (avg T2M/2T R@5) & 0 & 5000 & 0 & natural & CC BY 4.0 & available \\ Flick30k [132] (avg T2/2T RT@5) & 0 & 1000 & 0 & natural & CC0 & available \\ \hline
**Total** & **462171** & **314419** & **4653** & & & \\ \hline \hline \end{tabular}
\end{table}
Table 3: FoMo-in-Flux Evaluation-only Datasets. We utilize a subset of standard evaluation datasets used in Radford et al. [142], as well as an array of ImageNet-like variations (including the original ImageNet) to probe different aspect of vision-language understanding and alignment. Moreover, datasets like Food101 [18] or OxfordPets [126] were selected due to their high initial zero-shot performance scores.

### Designing Data-Centric Task-Sequences

In addition to studying different pretraining sets \(\mathcal{P}\) and data mixture ratios \((\lambda_{\mathcal{P}},\lambda_{\mathcal{B}},\lambda_{\mathcal{D}})\), we also investigate different realistic orderings by breaking down the FoMo-in-Flux datasets into individual concepts, which are then ordered according to a chosen criterion (including the option to study reverse orderings). This is visualized in Fig. 10. In order to do so, having a controlled set of image-caption pairs is critical, as it allows for well-defined and meaningful arrangement of concepts into sequences according to an ordering \(\pi(\mathcal{C})\). Each ordering \(\pi\) divides the set of samples \(\mathcal{D}\) into \(T\) disjoint subsets \(\{\mathcal{D}_{1},\dots,\mathcal{D}_{T}\}\) of concepts \(\mathcal{C}\) sampled without replacement, i.e. \(\mathcal{C}_{i}\bigcap\mathcal{C}_{j}=\phi\), \(\forall i,j\). We define and motivate six different orderings below:

**1. Easy-To-Hard Ordering (performance)** is motivated by curriculum learning [58; 154; 165; 170; 208], assuming users deploying their model to easier concepts and usecases first, with incremental movement towards to harder concepts.

_Implementation._ We approach the notion of "easy" vs. "hard" samples by ordering them according to base model performance. For each concept, we select 50 random image-text pairs and then randomly sample further 50 image-text pairs from the CC-3M dataset to represent random samples from CLIP's pretraining data pool [29]. For each of the 100 image-text pairs, we compute the sample-wise contrastive loss using a CLIP ViT-L-14 model, and average it over concepts. The lower the mean loss per concept, the easier it is. We then sort all the concepts by their mean loss in ascending order, and consider that to be the data stream ordering.

**2. Concept Frequency Ordering (concept-frequency)** draws motivation from Udandarao et al. [180], with user requests for model improvement starting from least frequent concepts first (as these constitute edge cases that are most likely to cause undesired performance drops) and incrementally extending to more frequent concepts, which are already represented well in the pretraining pool.

_Implementation._ We use the _What's In My Big Data_[43] tool's elastic search index to search for the frequency of occurrence of each of the class names in the \(\mathtt{C4}\)[144] dataset. We compute the frequencies of each of the classes, and order them such that the least frequent concepts (long-tail) occur first and the most frequent ones (head-concepts) are at the end.

**3. Concept Similarity Ordering (similarity)**, inspired by Yildiz et al. [204], is based on the hypothesis that training on conceptually similar tasks allows users to minimize catastrophic forgetting over tasks.

_Implementation._ To find a _trajectory_ with the highest semantic similarity between subsequent concepts, we start with a similarity matrix containing the pairwise similarities between all the class names (via CLIP ViT-L-14 text embeddings of templated text captions of the respective classes). Defining each class as a node in a graph, with weights between the classes being their similarity, the problem reduces to finding the minimum spanning path. We use a simple greedy algorithm: pick a starting class, find its closest neighbour from the remaining set of classes, and keep repeating until

Figure 10: **Pictographic visualization of different data stream orderings included within the FoMo-in-Flux benchmark setup.**we exhaust all classes. We repeat this procedure for every class as a starting point and pick the path with the smallest total weight across all starting classes.

**4. Time-incremental Ordering** (time), inspired by [15, 73, 21, 135, 49], arranges in chronological order.

_Implementation._ As we only have reliable time information about datasets (via release dates of corresponding publications or the official dataset upload date), concepts are ordered on a dataset-level [15]. These year-level groups are arranged from oldest to most recent, assuming that older datasets are more likely to be conceptually integrated within the pretraining data. Within each year, concepts are randomly ordered. Alongside the above orderings, we compare with two baseline methods popular in continual learning, to better understand the trade-offs made by these data-centric orderings:

**5. Dataset-Incremental Ordering** (dataset) is motivated by [148, 111, 112, 190, 206], but extended to a larger sequence of datasets. To set up dataset, we simply randomly sample datasets from Tab. 2 to create a dataset-incremental concept sequence. This sequence is then broken down into the desired number of tasks \(T\).

**6. Random Ordering** (random), a baseline class-incremental ordering widely used across continual learning setups [149, 200, 70, 136], mimics a scenario where user requests for model improvement are unstructured. For this ordering, we simply shuffle class names at random.

### Verifying Downstream Datasets: Finetuning must improve Performance

In order to estimate a reference upper bound on adaptation performance, verify the quality of generated captions, and perform a sanity-check on our training pipeline, we fine-tune CLIP-ViT-B/32 and CLIP-ViT-B/16 individually on each dataset in our training split, as well as all the evaluation-only datasets which come with training samples. We fine-tune the models on each dataset for 10 epochs, with exact results and training details shown in Supp. table 5. For _all datasets_, we find that finetuning a pretrained CLIP model on our generated captions consistently, and in parts very significantly, improves initial zero-shot performance. This showcases the validity of our generated captions, and supports the inclusion of each listed dataset in the FoMo-in-Flux benchmark.

## Appendix C Continual Pretraining: Additional Details to our Method Perspective

### Detailed Method Overview.

In detail, we study several promising directions for continual pretraining of foundation models: _(1) Naive continual finetuning_[49, 136, 76], which has emerged as a dominant approach for major updates on realistic large-scale benchmarks, making it a contender for handling minor updates as well. _(2) Parameter-efficient tuning methods_ like LoRA[72], which have become a method of choice for minor updates on a smaller scale or for adapting to new tasks with reduced memory requirements [62, 109, 196, 166, 167, 48, 98] through the use of low-rank weight approximations. In a related fashion, recent work by Zhao et al. [219] has shown promise for model finetuning through low-rank approximations on the optimization gradients (GaLore). _(3) Parameter-selective tuning methods_ such as BitFit[8] or LNFFit[37], which only tune and update particular parameter subsets in the pretrained model such as bias or normalization terms. _(4) Traditional regularization strategies_ from continual learning literature [86, 209], which have yielded surprisingly strong performance in recent studies both in parameter [95, 217] and feature space [121], despite being developed and tested in small-scale scenarios where the model is trained from scratch. _(5) Model merging_, which has gained popularity [146, 78, 197] in non-continual learning scenarios as a means to aggregate models tuned across different tasks, and has been studied in some recent [172, 114] and concurrent works [89, 113] as a method to facilitate continual pretraining over longer adaptation periods. All model merging variations are visualized in fig. 16.

### Additional study on parameter-efficient finetuning methods.

For parameter-efficient tuning, the scaling between the accumulation-retention trade-off and the tunable parameter count is also unsurprisingly reflected when adjusting the rank of LoRA (fig. 11 left)--though the loss in original generalization performance outweighs the achievable knowledge accumulation when contrasted against the hypothetical trade-off line between initial zero-shot behaviour and joint finetuning.

Figure 11: **More Detailed Method Ablations.** (_Left_) Impact of different ranks on continual pretrainability; favouring lower rank values (\(r=4\)) over large rank values (\(r=64\)) when contrasted against the hypothetical linear tradeoff line between original zero-shot behaviour and performance when finetuned over all data at once. (_Center_) Comparison between parameter-selective LNFit[37] and BitFit[8]. Both exhibit similar behaviour: strongly limited ability to continuously incorporate new context, with correspondingly minimal deviation in original zero-shot behaviour. (_Right_) Overview of adaptation versus evaluation trajectories for different PEFT methods: LoRA[72], DoRA[104] and VeRA[88]. LoRA and DoRA behave comparably, with low adaptable parameter counts in VeRA heavily limiting the ability to accumulate new knowledge.

Continual Pretraining: Additional Details to General Training Recipes

### On the Influence of Learning Rate Choices for Continual Pretraining.

To define the learning rate of choice for our continual pretraining problem, we derive it directly from the original pretraining values in Cherit et al. [29] (_1e-3_). We note that the exact peak values are corrected for our practical differences in compute availability (operating on a batch-size of \(b_{\text{ours}}=512\) instead of \(b_{\text{openclip}}=88064\)); testing both the commonly utilized linear resizing [53]: \(\lambda_{\text{scaled}}=\nicefrac{{b_{\text{ours}}}}{{b_{\text{openclip}}}}\cdot \lambda_{\text{openclip}}\) and the respective square-root resizing [92] (giving \(5.81e-6\) and \(7.625e-5\), respectively). In preliminary experiments, we found that rounding up the linearly resized reference (to \(\lambda_{\text{scaled}}=1e-5\)) worked slightly better than both options, and provides a much cleaner entry point. As such, we chose to utilize \(1e-5\) as our learning rate reference value. As we find in fig. 12, this (mostly) direct re-use of the maximum learning rate has most importantly the highest degree of knowledge accumulation, but also achieves the highest base joint tradeoff with respect to zero-shot retention. Larger learning rates incur significantly higher rates of particularly early-task forgetting, while smaller learning rates limit the amount of knowledge gained. As such, we set \(\lambda_{\text{scaled}}=1e-5\) as our base learning rate.

### Model-specific tuning choices in compute-restricted scenarios

Finally, we highlight the relevance of freezing either image or text encoder in practically compute-restricted continual pretraining in Fig. 13. As freezing either the image or language encoder can allow for significant increases (over a magnitude) in the tuning step budget (as total FLOPs and memory use go down), we find that within the compute-restricted continual multimodal pretraining scenario, tuning both encoders still remains beneficial (aligning with insights provided in Goyal et al. [54] for simple finetuning). While there is negligible difference when freezing each encoder respectively (despite the substantial difference in FLOPs reduction based on tuning the image-encoder alone vs. tuning the text-encoder alone), updating the vision-language model as a joint system incurs a more favorable trade-off between knowledge accumulation and zero-shot retention for each update.

### Softmax Temperatures for Contrastive Losses--_Not Too Hot!_

Recall that CLIP's contrastive loss uses a temperature parameter \(\tau\), and it is typically learnable during pretraining. At the beginning of training, it is initialized to \(0.07\)[142]. Further, to prevent training instabilities, the temperature is clipped to avoid becoming smaller than \(0.01\). Post training, the learned temperature for all CLIP models considered in this study are found to be exactly \(0.01\). Moreover,

Figure 12: **The effect of the base learning rate on continual pretraining**. The learning trajectory is shown for each value of the learning rate, with the joint training performance as an upper bound. The contour lines show the geometric mean of knowledge accumulation and zero-shot retention (\(\sqrt{\mathcal{A}_{KA}}\times\mathcal{A}_{ZS}\)). A learning rate of \(1\mathrm{e}-5\) derived from the inital pretraining learning rate achieves the highest final knowledge accumulation and provides the optimal balance between \(\mathcal{A}_{KA}\) and \(\mathcal{A}_{ZS}\).

Figure 13: **To freeze or not to freeze.** Tuning both encoders beats single encoder tuning in line with finetuning insights from Goyal et al. [54].

most works that fine-tune a pretrained CLIP model for different downstream tasks, use exactly this learned temperature [54, 177, 178, 197, 42, 78, 61].

Across our main experiments, we follow this standard practice of initializing \(\tau\) to \(0.01\) and setting it to be a learnable parameter during continual pretraining. We now explore the impact of different initializations for \(\tau\), and sweep over \(5\) different temperature values, \(\{0.01,0.1,0.5,0.75,1.0\}\). From fig. 14, we observe that \(\tau\) plays a crucial role for continual pretraining. As we increase the temperature from \(0.01\) to \(0.1\), zero-shot retention \(\mathcal{A}_{ZS}\) gets impacted by \(20\)% while also noting modest drops on knowledge accumulation \(\mathcal{A}_{KA}\), as stability gap issues are excacerbated. Further increasing \(\tau\), degrades both \(A_{ZS}\) and \(A_{KA}\) even more greatly, with the model degenerating to very poor performance. Such drastic changes in model behaviour were also observed in prior work investigating CLIP fine-tuning for downstream tasks [177, 97, 33]--fine-tuning at higher temperatures leads to a decrease in the modality gap between the image and text embedding spaces on the CLIP embedding hypersphere, and hence very quickly degrades the quality of the embedding space for performing downstream tasks [158, 163, 97]. We reproduce and extend the findings of these previous works for the continual pretraining regime, and emphasise the importance of retaining low temperature values for providing optimal \(\mathcal{A}_{ZS}\) and \(\mathcal{A}_{KA}\).

## Appendix E Continual Pretraining: Additional Details to our Data-Centric Perspective

This section extends section 6 with detailed information on data-stream reversals specific data-pool choices and mixing ratios between streaming, buffer and pretraining data (\(\mathcal{D}/\mathcal{B}/\mathcal{P}\) and \(\lambda_{\mathcal{D}},\lambda_{\mathcal{B}},\lambda_{\mathcal{P}}\), respectively, in appendix E.2), and subsampling over the pretraining data for replay.

### What Happens if We Reverse Data-Streams?

Each sequence introduced in appendix B.4 introduces its own particular deployment scenario. Naturally, these scenarios may also either occur or be designed to occur in reverse; updating the model for example with hardest examples first, or choosing highly unrelated concepts before honing in on one specific ordering of similar concepts (by reversing similarity). These scenarios do not have to be related to their precursors, and can present their own unique update cycle. Evaluating fig. 6 (_right_), random remains consistent. The prevalent difference we find in reversing similarity; starting with a stream of unrelated concepts (more so than just random subsampling) and then moving towards a stream of more related concepts. Effectively, early task composition becomes forcibly harder. In doing so, the loss in retention along the trajectory comes with increased knowledge accumulation1.

Footnote 1: By composing harder tasks, batch composition becomes also more difficult, which has been aligned with improved vision-language representation learning in _e.g._, Zhai et al. [213]. Though by reversing similarity in our case, the aggregation of similar concepts towards the end of the stream results in diminished knowledge accumulation towards the end of the sequence.

This allows the trajectory to remain consistent and close to the hypothetical linear trade-off line between the initial zero-shot behavior and the finetuning upper bound - more so even than random streams. Both cases however point towards high variation in the presented concepts during each update step being very beneficial for continual pretraining over longer update cycles, especially when trying to retain consistent model behaviour for each update. Still, even when also accounting for the reversed performance ordering, end-points converge to comparable end points! We find the only outlier to this to be the reverse frequency stream. As head concepts are encountered early, knowledge accumulation is lower, while the controlled placement of long-tailed, rare concepts

Figure 14: **The softmax temperature for the contrastive loss** is crucial for continual pretraining optimization. The learned temperature after CLIP pretraining is \(0.01\) (brown trajectory)—higher temperatures than the optimal \(0.01\) hinder continual pretraining optimization and degrade model weights.

towards the end of the update cycle, result in disproportionate forgetting of frequent concepts crucial for achieving and retaining overall accumulation and retention performance.

### Data mixtures inform knowledge accumulation and zero-shot retention

Data control is also reflected in the use of different mixing ratios \(\lambda_{\mathcal{P}/\mathcal{D}/\mathcal{B}}\), which we study in Fig. 14(a). The particular ratios investigated are motivated as follows (note that the baseline reference ratios we use for all our experiments are \(\{\lambda_{\mathcal{P}}{=}0.33,\lambda_{\mathcal{D}}{=}0.34,\lambda_{\mathcal{B} }{=}0.33\}\) (in orange)):

**No Buffer**\(\{\lambda_{\mathcal{P}}{=}0.5,\lambda_{\mathcal{D}}{=}0.5,\lambda_{\mathcal{B} }{=}0\}\)**(in pink)** significantly degrades both accumulation and retention, hampering the \(\mathcal{A}_{\text{KA}}\)-\(\mathcal{A}_{\text{ZS}}\) tradeoffs (\(-14\%\mathcal{A}_{\text{KA}}\) and \(-2.5\%\mathcal{A}_{\text{ZS}}\) compared to the reference).

**Pretrain-heavy**\(\{\lambda_{\mathcal{P}}{=}0.8,\lambda_{\mathcal{D}}{=}0.1,\lambda_{\mathcal{B} }{=}0.1\}\)**(in blue)** also does not improve over the reference, since at each update step, we input fewer update samples from \(\mathcal{D}\), limiting the accumulation capacity.

**Ibrahim et al. [76]**\(\{\lambda_{\mathcal{P}}{=}0.05,\lambda_{\mathcal{D}}{=}0.48,\lambda_{\mathcal{B} }{=}0.47\}\)**(in green)** defines the mixture ratio used in past CPT work operating on LLMs. We reproduce the findings of [76], finding a \(5\%\) pretraining replay suffices to provide a better accumulation tradeoff compared to the reference (\(+2.2\%\mathcal{A}_{\text{KA}}\) and \(-0.3\%\mathcal{A}_{\text{ZS}}\)), suggesting that replaying pretraining data is less essential for optimal performance.

**IIDify**\(\{\lambda_{\mathcal{P}}{=}0,\lambda_{\mathcal{D}}{=}0.1,\lambda_{\mathcal{B} }{=}0.9\}\)**(in violet).** Inspired by the previous result of [76], the question arises on the importance of the overall pretraining pool \(\mathcal{P}\). Extending findings in Prabhu et al. [136], we jointly also increase the buffer mixing ratio to encourage more IID training distributions at each update step from the full \(\mathcal{D}\) and \(\mathcal{B}\) pools. Doing so provides the favored tradeoff compared to all the previous mixtures, corroborating findings in [136].

### Choice of pretraining data pool significantly impacts zero-shot retention

While the overall relevance of replay on pretraining data may be smaller than suitable buffer choices, we complete the previous study by investigating the impact of the pretraining data pool \(\mathcal{P}\) on the end model. We experiment with three other pretraining data pools of diverse volumes, caption-sources, curation strategies, and quality measurements--CC-3M [161], CC-12M [23], DataComp-Small [45]--beyond our reference pool LAION-400M. For a fair comparison, we randomly subsample each pretraining data pool to a total size of 2M samples, and use this subset as our final pretraining pool \(P\). Here too, we use the reference mixture ratio setting of \(\{\lambda_{\mathcal{P}}{=}0.33,\lambda_{\mathcal{D}}{=}0.34,\lambda_{\mathcal{B }}{=}0.33\}\). From fig. 14(b), it is immediately evident that the choice of the pretraining data pool has a relevant impact on the \(\mathcal{A}_{\text{KA}}\)-\(\mathcal{A}_{\text{ZS}}\) tradeoffs. While adaptation capabilities are barely impacted, using DataComp-Small (in pink) yields significantly better zero-shot retention properties, (upto \(2.4\%\mathcal{A}_{\text{ZS}}\)) gains). We speculate that this could be attributed to the purely English-centric nature of the CC/LAION pools compared to the unfiltered DataComp-Small which has a significantly higher multilingual and cultural diversity, which has been shown to be beneficial for downstream performance previously [123, 124, 133].

Figure 15: **Study on Mixture Ratios and Pretraining Pools.**

Method and Schedule Details

In the main paper, we study and reference different methods for their ability to encourage better continual multimodal pretraining on FoMo-in-Flux. In this section, we provide details on the methods utilized, alongside information not included in the main text with respect to the utilized learning rate schedules.

### Adaptation Methods

LoRA[72]is the most commonly deployed form of parameter-efficient finetuning based on _Low-rank Adaptation_, which avoids explicitly changing pretrained weights, but instead recommends weight updates to be of the form

\[W^{\prime}=W_{0}+BA\]

with pretrained weights \(W_{0}\), where \(B\), \(A\) are two low-rank matrices, _i.e._, where \(W\in\mathbb{R}^{d\times f}\), \(A\in\mathbb{R}^{r\times f}\) and \(B\in\mathbb{R}^{d\times r}\). By choosing \(r<<\min{(d,f)}\), memory requirements during finetuning can be significantly reduced. Moreover, any learned adapter weights can be absorbed into the pretraining weights. Note however that while memory is reduced, total FLOPs for backward **and** forward pass are commonly increased over simple finetuning, as full backpropagation still needs to be conducted, as noted in Mercea et al. [118] and as consequently seen in the final MAFs breakdown (see table 4). By default, LoRA (as well as its subsequent variations VeRA and DoRA, see below) introduces an additional weighting \(\alpha\) over the weight update \(BA\), which we set to a constant \(\alpha=1\)[72, 88]; as it only acts as an implicit change in learning rate. As noted in Hu et al. [72], the rank \(r\) is the essential hyperparameter to define for optimal changes in behaviour.

VeRA[88]introduces a simple variation over LoRA by randomly initializing and freezing \(A\), \(B\) into fixed low-rank projections, and instead learning simple learnable vectors \(\Lambda_{B}\) and \(\Lambda_{A}\) such that

\[W^{\prime}=W_{0}+\Lambda_{B}B\Lambda_{A}A\]

where \(\Lambda_{B}\in\mathbb{R}^{f}\) and \(\Lambda_{A}\in\mathbb{R}^{r}\) (utilizing the same dimensional notation as above). This reduces the total number of tunable parameters significantly (though also mitigating possible adaptation capabilities), but similar to LoRA, does not positively impact FLOPs counts for backward and forward passes together.

DoRA[104]minimally alters LoRA by disentangling norm and directions of the introduced adapter matrices to encourage increased stability, and moving training dynamics of LoRA-style approaches closer to those of simple finetuning. Effectively, this defines the DoRA adaptation step as

\[W^{\prime}=m\cdot\frac{W_{0}+BA}{\left\lVert W_{0}+BA\right\rVert}\]

with magnitude vector \(m\in\mathbb{R}^{1\times f}\), where \(m\) is initialized as \(\left\lVert W_{0}\right\rVert_{c}\), before being jointly updated during finetuning alongside the directional (through normalization) updates induced by \(B\) and \(A\).

BitFit[8]introduces parameter-selective model finetuning by only updating bias-terms in the model (and retaining remaining (kernel) weights as frozen). In doing so, changes to the model behaviour are supposed to be kept minimal, will still introducing several degrees of freedom for finetuning. Note however that similar to LoRA, while GPU peak memory is reduced, FLOPs are still high, as backpropagation through the full network still has to occur.

LNFit[37]succeeds in the spirit of BitFit, by recommending to only tune scale and bias parameters in model architectures that leverage LayerNorm [6] layers, showcasing particular success on small continual learning benchmarks.

### Standard Continual Learning Methods

Ewc[86](_Elastic Weight Consolidation_) is a regularization scheme on weight updates initially introduced to tackle rehearsal-free continual learning from scratch. The core motivation behind EWC is the assumption that for each continual task, deviation from "task-optimal" weights learned in preceding tasks should be kept meaningfully minimal. In particular, Kirkpatrick et al. [86] argue that deviation should be individual to each model parameter. Assuming full model weights \(\theta\) after task \(t\), EWC tries to approximate the curvature in parameter-loss space around \(\theta_{t}\) via the _Fisher Information Matrix_\(\mathcal{F}^{t}\). To estimate \(\mathcal{F}^{t}\), several forward and backward passes have to be conducted, with the final regularization during training in task \(t+1\) defined as

\[\mathcal{L}^{\text{total}}_{t+1}(\theta)=\mathcal{L}_{t+1}(\theta)-\frac{ \lambda}{2}\sum_{k\in|\theta|}\mathcal{F}^{t}_{k}(\theta_{k}-\theta_{t,k})^{2}\]

with penalty weight \(\lambda\), loss function for task \(t+1\), \(\mathcal{L}_{t+1}\), \(\theta_{t}\) the weights from the previous task, and \(k\) the parameter index. Note that for more than two tasks, \(\mathcal{F}\) is commonly estimated through a rolling average, as done in implementation, borrowing from the mammoth codebase [17].

Si [209](_Synaptic Intelligence_) follows a motivation conceptually related to that of EWC, in that parameters defined as more influential (by some measure) are regularized more strongly to minimize change. However, unlike EWC which computes one single point estimate using final parameter values after each task, SI computes importance measures used for regularization along the entire training trajectory. By tracking past and current parameter values, an online importance estimate is computed and incorporated as regularization as follows:

\[\mathcal{L}_{t+1}(\theta)=\mathcal{L}_{t+1}(\theta)+c\cdot\sum_{k\in|\theta| }\left(\sum_{\tau<t}\frac{\omega_{k}^{\tau}}{(\Delta_{k}^{\tau})^{2}+\zeta} \right)\left(\theta_{k}^{t}-\theta_{k}\right)^{2}.\]

with final task weights \(\theta^{t}\) from the previous task. Here, \(\omega_{k}^{tau}\) is regarded as the per-parameter contribution to changes in the total loss, approximated as the running sum of the product between gradient \(g_{k}(s)=\frac{\delta\mathcal{L}}{\delta\theta_{k}}\) and parameter update \(\theta_{k}^{\prime}(s)=\frac{\delta\theta_{k}}{\delta s}\) (with within-task update step \(s\)). Finally, \(\Delta_{k}^{\tau}=\theta_{k}(s^{\tau})-\theta_{k}(s^{\tau-1})\) estimates how much a particular parameter has moved. Alongside a simple regularization term \(\zeta\) to avoid division by zero, this defines the online importance term in SI.

### Model Merging Methods

FT-Merge [197, 78]introduces a simply model merging recipe, in which different finetuned variants of a same base pretrained model are linear interpolated (using interpolation coefficient \(\alpha\)) into a final, more general new base model. While this was initially not introduced for continual learning / pretraining tasks, this form of interpolation can be naturally extended to our problem scenario. After each task, given an interpolation coefficient \(alpha\), we interpolate pre- and post-task weights (\(\theta_{t-1}\) and \(\theta_{t}\), respectively). These updated weights are then passed to the subsequent task \(t+1\). Note that we incorporate the interpolation process into the overall MAF compute budget as well.

EMA-Merge [172]extends Ilharco et al. [78], but shows how a simple exponential moving average can achieve promising regularization beyond implicit learning rate changes for small, toy-ish continual learning image classification benchmarks. Similar to FT-Merge, EMA-Merge introduces an interpolation coefficient \(\alpha\), and each interpolation step is account for in the overall compute budget.

ZS-Mergeoperates in a fashion close to both merging methods - with the only differentiating factor being that after each task, interpolation occurs not with respect to preceding model weights, but instead to the initial zero-shot baseline.

## Appendix G Differentiating Factors: FoMo-in-Flux with TiC-CLIP [49] and NEVIS [15]

In this section, we elaborate on the details presented in Table 1 of the main paper. We highlight the distinctive features of our benchmark, FoMo-in-Flux, in comparison to two closely related benchmarks: NEVIS and TiC-CLIP.

**NEVIS.** NEVIS [15], like our work, studies long-horizon continual learning with changing data distributions. However, NEVIS focuses on improving performance in a task-incremental setup, where task separation is based on dataset creation timestamps, and concentrates on performance for the current, ongoing task. In contrast, FoMo-in-Flux studies the ability for continual knowledge aggregation_, while balancing the retention of good downstream zero-shot performance; measuring open-ended performance in both cases and not limited to a fixed set of classes. We also tackle multimodal vision-language tasks like image-text retrieval, which are more complex to formulate than vision-only tasks. Moreover, FoMo-in-Flux allows as to study the impact of different concept and class streams to emulate task orderings that can potentially be encountered when realistically deployed.

**TiC-CLIP.** The TiC-Datacomp benchmark [49] evaluates the best methods for continual learning over _major_ updates, using pretraining budgets similar to those used for pretraining CLIP. In contrast, our work focuses on _minor_ updates, utilizing sample and compute scales that are \(20\times-100\times\) lower than the corresponding pretraining budgets. Furthermore, TiC-CLIP operates with only six timesteps and uses large, monolithic time-incremental batches of image-text pairs. Our experiments, however, extend up to 200 timesteps and involve four carefully controlled fine-grained data-centric streams across a variety of subdomains, including medical and remote sensing images. Our study provides insights into how models can be pretrained continually over time, in scenarios working with far smaller sample and compute budgets and a larger number of timesteps, ensuring efficiency and scalability across different subdomains. Moreover, we are able to cover and study different data-centric deployment scenarios, alongside a wide array of methods and their trajectory in the knowledge aggregation and retention space. Together, FoMo-in-Flux allows us to provide the transitional benchmark towards the much more compute-intensive _major_ updates as studied in TiC-Datacomp.

## Appendix H Additional Experimental Details and Results

### Experimental Setup: Full Overview.

For complete replication, we detail the default models, compute budgets, metrics, training schedules, and data mixtures used here in significantly extended detail here.

**Pretrained Models.** We conducted our main experiments using a ViT-B-16 CLIP model pretrained on the LAION-2B dataset [159]. We also conducted some additional ablation experiments with a ViT-B-32 CLIP model (to understand the effects of different patch resolution) and ViT-S/16, ViT-L/14, ViT-H/14 and ViT-g/14 models. All our CLIP models are pretrained on LAION-2B, except for the ViT-S/16 model which is pretrained on the DataComp-1B dataset [45].

**Default Continual Pretraining Settings.** Unless otherwise specified, we always train each continual pretraining method for 20 update steps, \(T{=}20\) (we test longer sequences with \(T{=}\{50,200\}\) in Supp. fig.18). Each update step comprises of continually training a CLIP model for a fixed number

Figure 16: **Different model merging strategies explored in this work. We use \(\theta^{\prime}\) to denote weights \(\theta\) finetuned after a respective task. Merging \(\theta_{t-1}\) and \(\theta^{\prime}_{t}\) then results in the merged outputs weights for task \(t\), \(\theta_{t}\). EMA-Merge, or exponential moving average merging, merges previously merged weights \(\theta_{t-1}\) with current task weights \(\theta^{\prime}_{t}\) produced by tuning the same previously merged \(\theta_{t-1}\) on task \(t\). ZeroShot-Merge always tunes the original pretraining weights \(\theta_{0}\) on each task, then weight-interpolates between the finetuned \(\theta^{\prime}_{t}\) and the previously merged \(\theta_{t-1}\). Finetune-Merge always interpolates between the original pretraining weights \(\theta_{0}\) and the finetuned weights \(\theta^{\prime}_{t}\). To arrive at \(\theta^{\prime}_{t}\), the previously merged model \(\theta_{t-1}\) is trained on task \(t\).**

of samples derived by the computational budget outlined above. We fix the compute budgets per update step by taking the DataComp-Small total FLOP budget, i.e., \(1.8\times 10^{9}\) GFLOPs and dividing it by the total number of update steps. The exact number of update steps for each method is provided in Supp. Tab. 4. By default, we use a random \(2\)M subset of L4ON-400M as our pretraining data pool \(\mathcal{P}\) and operate with uniform mixing ratios \(\{\lambda_{\mathcal{P}}{=}0.33,\lambda_{\mathcal{D}}{=}0.34,\lambda_{\mathcal{B }}{=}0.33\}\). For our reference upper bound performance, we train a CLIP model initialized from the same open_clip checkpoints jointly on all \(41\) adaptation datasets (with the samples randomly shuffled). We do this training for a compute budget of \(T\times F\) MAFs, equivalent to the overall compute budget available for the entire continual pretraining process.

**Training Details.** We train all continual pretraining methods with the CLIP contrastive loss [142, 54] and learnable temperature \(\tau\), initialized to \(0.01\) (we provide ablations for the impact of \(\tau\) initialization in appendix D.3). We select the best-reported hyperparameters for each method from previous literature, only tuning the peak learning rate for each method. We use cosine-decay LR-scheduling with linear warmup of 10% (we study more LR-schedules in section 5.1), with an AdamW optimizer [107], a batch-size of \(512\)[107], and clip gradients with norm higher than 1. We run all experiments using PyTorch [128]. To truly study updates in both vision and language space, we update both encoders jointly (following Zhai et al. [212], we ablate this choice in appendix D.2). Finally, the exact reflections of MAFs in method updates steps are provided in the supplementary, alongside individual reference scores finetuning CLIP on each dataset individually.

**Metrics.** From a model updating perspective, there are two main quantities of interest: the degree of _adaptation_ to new data and the _retention_ of pretraining knowledge. For all experiments, we therefore report two main metrics: Knowledge Accumulation (\(\mathcal{A}_{KA}\)), the average accuracy (or recall@5 for retrieval) over all concepts in the \(41\) adaptation datasets, and Zero-Shot Retention (\(\mathcal{A}_{ZS}\)), the zero-shot transfer accuracy (or recall@5 for retrieval) on the held-out set of \(22\) datasets.

**Plotting Style.** In most plots showing our main experimental result, we depict the zero-shot baseline as a black star and the joint training upper-bound as a golden star, with a dotted line connecting the two to approximate the joint training trajectory on the \(\mathcal{A}_{KA}\)-\(\mathcal{A}_{ZS}\) plane. Every other trajectory depicts the training progression of individual experimental runs. Note that these trajectories always begin at the zero-shot baseline (black star).

### Further Replication Details.

We provide our full code, datasets, download pipeline, and experimental results here: github.com/ExplainableML/fomo_in_flux. The provided code covers all relevant details that make up FoMo-in-Flux: All dataset loaders, method implementations, streaming files and all generated captions for every single dataset image (c.f. data_lib/00_info). The code also comes with an automated downloader for preprocessed versions of each utilized dataset.

**Compute cluster and run details.** For all our experiments, we used a compute cluster with \(8\times 40\)GB A100 nodes. For most of our ViT-B-16 runs, we used 2 GPUs from these nodes which was sufficient for all our method implementations. To ensure memory efficiency, we optimised our implementations to use CPU offloading for model weights where possible (for _e.g._, for the EWC, SI and Merge methods). For comparability and reproducibility, all runs and methods share the same seed and equivalent overall experiment setting, with changes in _e.g._, data stream ordering, modified compute budgets, method or data-mixtures only done when explicited noted.

**Justification for CLIP models used.** To ensure that our experiments were most relevant to the community, we further verified that the choice of our base CLIP models were validated by practitioner usage. On Huggingface, the open_clip models that were downloaded the most were CLIP ViT-B-32-laion2b (6.11M times), CLIP-ViT-H-14-laion2b (4M times), and CLIP-ViT-B-16 (2M times). Hence, we investigate these models - particularly as ViT-B/16 has been used in other studies on continual major model updates such as Garg et al. [49].

**Exact number of update steps, MAFs and samples seen.** We provide the full breakdown of how we compute MAFs per time step for each of the methods, and the total compute budget in terms of samples seen per method (in Appendix table 4). We use the datacomp-small[45] compute budgets as our reference. Hence, this means that our total compute budget for the full continual pretraining is set to \(5.7\times 10^{8}\) GFlops for the ViT-B-32 architecture and \(1.8\times 10^{9}\) GFlops for the ViT-B-16 architecture.2

Footnote 2: Note that the compute budgets outlined in the original paper [45] were in GMacs—we convert these numbers to GFlops by multiplying by 2 (see here for reference.)

**Variance across seeds.** To ensure that our results are statistically valid and generalizable, we re-run our canonical continual pretraining experiment with a ViT-B/16 backbone on the 20-task random data stream, with three different seeds. fig. 17 showcases that the three trajectories across the different seeds result in very similar patterns and low variance across runs. This validates that all our main results are generalizable across seeds.

**Additional Experiment Results.** Finally, we augment our suite of experiments conducted in the main paper.

Fig. 18 provides additional higher-level experiment insights and verification, covering changes in backbone architecture, compute budget and total update steps / task counts. More precisely, Fig. 18 (_left_) shows the impact an increase or decrease in overall compute budget has. As can be seen, all trajectories behave similarly on a qualitative level - experiencing forgetting and stability gap [36] issues at the beginning, before recovering towards the linear zeroshot-finetuning trend line. Comparing end points, we do find that larger compute budgets encourage slightly increased knowledge accumulation gains, but at the cost of disproportionately larger losses in knowledge retention. This means that in practice, large compute budgets may be less favoured even from a performance standpoint to incorporate minor model updates and bridge time between large, major model updates. On top of that, Fig. 18 (_right_) highlights that under a fixed compute budget, in order to bridge time to large model updates, keeping the number of minor model updates small, while maximizing the size of each respective minor update, is preferable from both a knowledge accumulation and retention perspective. Further, we note the strong robustness of model merging even under very long task streams, further strengthening their applicability for long-step continual pretraining.

Fig. 18 (_center_) augments our results on the impact of different data-centric deployment scenarios for continual minor model updates, under a different patch resolution for the vision-transformer. In this experiment, we continually pretrain ViT-B-32 image-encoder models instead of the standard ViT-B-16 image-encoder. We note that the overall trends from this experiment closely match those of

\begin{table}
\begin{tabular}{l|c|c c|c c c c c c|c c} \hline \hline Method & Per-Task GFops & Max Memory Repd. Memory Multiple & Per-Task GF loops & Per-Task GF loops & Num. steps & Num. steps & Num. steps & Total Num. & Total Num. \\  & & & (with full-ft) & & (\(T-20\)) & (\(T-50\)) & (\(T-100\)) & (\(T-200\)) & steps & samples & normal \\ \hline full-ft & 63394.7585 & 46.5917 & 1 & 63394.7585 & 142.00 & 568 & 284 & 142 & 28.400 & 14.580,300 \\ locked-text & 57254.6183 & 37.5561 & 0.8064 & 46170.1241 & 1949 & 780 & 390 & 195 & 39.000 & 19.968,000 \\ locked-image & 27176.6699 & 11.8847 & 0.2551 & 60392.7644 & 12924 & 5193 & 2596 & 1298 & 259.600 & 129.512,520 \\ LNF14 & 4165.9589 & 10.5566 & 0.5585 & 28307.9983 & 3179 & 1222 & 636 & 318 & 63.600 & 32.632,200 \\ BiFiFi & 4163.5968 & 10.5546 & 0.5558 & 28307.9983 & 3179 & 1222 & 636 & 318 & 63.600 & 32.563,200 \\ Loth, red & 54479.2515 & 20.5449 & 0.8702 & 4774.8468 & 1898 & 759 & 380 & 190 & 88.00 & 19.465,600 \\ Loth, red & 54650.0151 & 40.6757 & 0.873 & 47582.78181 & 1891 & 757 & 378 & 189 & 37.800 & 19.535,600 \\ Doth, red & 54479.2814 & 40.6822 & 0.87267 & 45739.0481 & 1893 & 757 & 379 & 189 & 17.870 & 19.535,600 \\ Doth, red & 54451.7454 & 40.7871 & 0.8754 & 47721.2091 & 1886 & 754 & 377 & 189 & 37.800 & 19.535,600 \\ Vah, red & 54479.3933 & 30.5449 & 0.8702 & 44797.5211 & 1898 & 759 & 380 & 190 & 88.00 & 19.465,600 \\ Vah, red & 54507.8366 & 40.5742 & 0.8708 & 47468.4124 & 1896 & 758 & 379 & 190 & 38.000 & 19.456,600 \\ Dcv & 62760.094 & 47.3027 & 1.0132 & 63892.3546 & 14 & 6 & 3 & 1 & 200 & 102.00 \\ SI & 63394.7585 & 46.6531 & 1.0013 & 63477.1716 & 1418 & 567 & 284 & 142 & 28.400 & 14.540,800 \\
2E-Regreg & 5394.7585 & 46.5917 & 1 & 63394.75858 & 1420 & 568 & 284 & 142 & 28.400 & 14.540,800 \\ PT-Regreg & 63394.7585 & 46.5917 & 1 & 63394.75858 & 1420 & 568 & 284 & 142 & 28.400 & 14.540,800 \\ EM-Merge & 63394.7585 & 46.5917 & 1 & 63394.75858 & 1420 & 568 & 284 & 142 & 28.400 & 14.540,800 \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Compute Budgets used in all ViT-B-16 experiments.** We provide the total number of GFlops taken per task for each of the methods in the Per-Task GF loops column. We also showcase the maximum GPU memory requirements for each method in the Max. Memory Reqd. column—we convert this into a memory multiplier for each method by dividing with respect to the reference full-ft max memory required. Finally, for each method the Per-Task MAFs are computed as the product of the Per-Task GF loops and the Memory Multiplier. Then, we show the total number of gradient update steps that are allowed for these compute budgets per update step \(t\), for the four total number of time step settings, \(T\)=\(\{20{,}50{,}100{,}200\}\). Finally, we also show the total number of gradient steps used (Total Num. steps) and the total number of samples seen (Total Num. samples seen) for the full continual pretraining process—our joint upper-bound oracle also uses this total compute budget.

[MISSING_PAGE_FAIL:39]

\begin{table}
\begin{tabular}{l r r r r} \hline \hline \multirow{2}{*}{Dataset} & \multicolumn{2}{c}{ViT-B-16} & \multicolumn{2}{c}{ViT-B-32} \\ \multicolumn{5}{c}{FT Performance Delta to ZS FT Performance Delta to ZS} \\ \hline Ai2Diagrams [84] & 88.00 & 10.67 & 83.67 & 12.33 \\ ArtBench10 [99] & 22.86 & 11.64 & 21.20 & 9.08 \\ Birdsup [9] & 63.70 & 13.30 & 57.60 & 10.00 \\ Caltech101 [94] & 93.33 & 1.33 & 93.67 & 1.67 \\ Caltech256 [55] & 93.97 & 1.39 & 92.61 & 2.61 \\ Cars196 [169] & 93.88 & 5.07 & 90.56 & 2.25 \\ Cifar100 [93] & 90.33 & 15.83 & 91.33 & 15.93 \\ Cifar10 [91] & 99.67 & 4.67 & 99.00 & 4.70 \\ CLEVR [83] & 71.05 & 67.19 & 55.87 & 52.94 \\ CLRS [151] & 92.67 & 29.33 & 91.33 & 30.00 \\ Country211 [142] & 20.38 & 3.74 & 20.38 & 6.11 \\ CUB200 [186] & 80.50 & 10.38 & 74.00 & 10.27 \\ DF20mini [131] & 50.84 & 49.46 & 43.30 & 41.64 \\ DollarStreet [152] & 18.31 & 11.88 & 17.96 & 12.26 \\ DomainNet-Clipart [129] & 83.62 & 3.14 & 81.74 & 3.93 \\ DomainNet-Infograph [129] & 61.16 & 3.71 & 54.93 & 2.55 \\ DomainNet-Painting [129] & 74.64 & 3.61 & 71.72 & 1.47 \\ DomainNet-Quickdraw [129] & 66.81 & 48.45 & 66.52 & 48.24 \\ DomainNet-Sketch [129] & 78.26 & 3.94 & 76.96 & 4.89 \\ Dsprites [115] & 100.00 & 88.16 & 100.00 & 88.36 \\ DTD [31] & 68.00 & 16.00 & 66.33 & 11.33 \\ EuroSAT [65] & 99.67 & 43.62 & 99.33 & 47.85 \\ FashionMNIST [201] & 96.33 & 16.93 & 94.67 & 18.07 \\ FGVCAircraft [110] & 48.67 & 22.24 & 39.33 & 14.41 \\ Flowers102 [125] & 95.67 & 21.33 & 94.67 & 21.33 \\ Food101 [18] & 90.67 & 5.08 & 88.00 & 5.66 \\ FRU92 [69] & 91.67 & 42.97 & 88.33 & 39.64 \\ GTSRB [71] & 99.33 & 49.46 & 100.00 & 56.12 \\ iNaturalist2021 [79] & 50.40 & 44.76 & 43.10 & 37.80 \\ Isiclenanoma [41] & 59.33 & 51.00 & 56.00 & 40.33 \\ MITStates [80] & 28.30 & 4.75 & 26.35 & 3.02 \\ MNIST [40] & 100.00 & 34.70 & 99.67 & 30.57 \\ Monkeys10 [2] & 97.79 & 15.07 & 96.69 & 13.97 \\ MTSD [44] & 90.97 & 72.41 & 90.75 & 70.93 \\ MVTec-AD (Base) [10] & 100.00 & 27.67 & 100.00 & 21.00 \\ MVTec-AD (Faults) [10] & 52.33 & 38.67 & 38.00 & 20.67 \\ ObjectNet [7] & 54.63 & 16.75 & 48.88 & 16.98 \\ obscure Animals & 89.67 & 27.49 & 89.33 & 33.78 \\ obscure Things & 73.33 & 17.54 & 68.67 & 14.98 \\ OpenImages [90] & 58.64 & 0.00 & 59.40 & 0.38 \\ OxfordPets [126] & 95.00 & 4.29 & 90.67 & 0.23 \\ PatternNet [226] & 99.67 & 30.72 & 99.67 & 34.14 \\ Places365 [221] & 48.49 & 6.62 & 49.86 & 7.22 \\ Plantvillage [75] & 100.00 & 80.02 & 99.67 & 76.55 \\ Quilt-IM [77] & 66.45 & 65.45 & 67.10 & 66.80 \\ Resisc45 [68] & 94.33 & 25.60 & 93.33 & 30.16 \\ Shapes3d [68] & 100.00 & 87.16 & 100.00 & 85.68 \\ SnakeCLEF2023 [130] & 22.17 & 21.98 & 16.51 & 16.45 \\ SUN397 [202] & 75.69 & 6.22 & 73.93 & 5.62 \\ STL10 [32] & 100.00 & 3.25 & 98.67 & 1.42 \\ SVHN [122] & 99.33 & 46.32 & 99.00 & 57.01 \\ SynthClip106 [60] & 46.67 & 5.46 & 44.00 & 4.30 \\ VEG200 [69] & 84.75 & 53.90 & 79.50 & 46.70 \\ Zappos50k [205] & 35.14 & 22.36 & 31.29 & 18.25 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Per-dataset fine-tuning results for the ViT-B/32 and ViT-B/16 backbone. FT Performance is the maximum accuracy over 10 epochs. Delta to ZS is the difference between FT Performance and the initial zero-shot accuracy.

FMo-in-Flux: Caption Pipeline

As part of our FoMo-in-Flux pipeline, we converted \(63\) different classification and retrieval datasets into a format that made them amenable for contrastive language-image pretraining. This entailed providing text captions for each of the images in the classification datasets. For this, our main aims were to ensure: (1) _scalability_ of the captioning pipeline, (2) that the captions captured _real-world and fine-grained_ details about the image, (3) that the captions were _not verbose_ so that they would fit into the context length of CLIP's text encoder (\(77\) tokens), and (4) that the captions _contained the true classname_ of each of the images from the classification datasets.

To this end, we proceeded to caption the images in a three-stage manner--(1) We first used a BLIP-2 model [96] using a T5-XL decoder to ensure high captioning performance along with scalability to provide initial seed synthetic captions for each of the images, (2) we next generated templated captions for each of the images using the classnames, for _e.g._, for an image of a tench in the ImageNet dataset, we use a templated caption, "A photo of a tench" and similarly for an image of a manted howler in the Monkeys10 dataset, we use a templated caption, "A photo of a mantled howler, a type of monkey.", and finally (3) we merge both the templated and seed synthetic captions using the Capsfusion [207] model--a LLaMA model that is finetuned to take in two captions for an image, and return a merged caption capturing the key aspects of both the captions. Using our three-stage pipeline, we are able to generate diverse yet faithful captions for each of the images in our set of \(63\) datasets. We showcase a visualisation of our generated captions for some of our constituent datasets in fig. 19.

Figure 19: **Random Samples from FoMo-In-Flux.** We showcase some sample captions generated using our three-stage pipeline for a few of the datasets in FoMo-In-Flux. The Class caption is the templated caption using the class-name, Synthetic caption is the caption generated using BLIP-2, and the Merged caption is the final merged caption using Capsfusion (merging both Class caption and Synthetic caption).

Data Statement

Dataset Title: FoMo-in-Flux

Dataset Curator(s): N/A

Dataset Version: 1.0

Dataset Citation: N/A

Data Statement Authors: N/A

Data Statement Version: 1.0

Data Statement Citation and DOI: N/A

### Executive Summary

FoMo-in-Flux is an aggregate benchmark comprising over 2.53M images from \(63\) classification and retrieval datasets, including \(61\) existing datasets and \(2\) newly introduced ones, described in appendix B. On top of image and labels provided by the original datasets, we provide a caption for each image, generated using the pipeline described in appendix J.

### Curation Rationale

Fomo-in-Flux is a benchmark for continual multimodal pretraining that emphasizes adaptation across distinct subdomains over long time horizons, while allowing for finegrained controllability of particular concepts and classes presented at respective update steps for a data-centric perspective on continual multimodal pretraining. The constituent datasets were selected based on availability, licensing, quality of labels, diversity of data domains, quality of the resulting captions, and the degree of adoption in the computer vision and machine learning research communities.

### Documentation for Source Datasets

The licensing information for source datasets, as well as relevant citations, are provided in table 2 and table 3. We release the captions, as well as the Obscure Animals and Obscure Things datasets under the MIT license (https://opensource.org/license/mit).

### Language Varieties

All the class labels and captions are in English.

### Speaker Demographic

N/A

### Annotator Demographic

The captions were created using an automated pipeline and based on original class labels, as outlined in appendix J. For selected simpler datasets, we use the templated captions directly, as shown in table 2 and table 3. For the information about annotators of source datasets, please see the references in table 2 and table 3.

### Speech Situation and Text Characteristics

N/A

### Preprocessing and Data Formatting

The class labels are used as-is with no modification. All images are resized to 224x224 pixels.

### Capture Quality

N/A

### Limitations

Although great care was taken to ensure the correctness of the dataset and random samples of the captions were manually inspected for a quality check, we did not verify the captions for all \(2.53\)M samples. Given the dependence on BLIP-2 [96] and Capsfusion [207], the captions might reflect the biases and idiosyncracies of these models.

Moreover, as an aggregate benchmark, Fomo-in-Flux reflects the data collection and annotation biases of the source datasets. However, by pooling diverse sources of data, we avoid a systematic dataset-wide curation bias.

### Broad Impact

Our dataset helps assess the continual multimodal pretraining performance across various methods, data stream orderings, learning rate schedulers, and compute budgets. The insights gained will help optimize continual pretraining, facilitating fewer large-scale model updates. This optimization, in turn, will help decrease energy consumption and lower carbon emissions associated with continual adaptation of foundation models, and overall encourage a more economical and ecological treatment of these large architectures.

### Metadata

License: https://opensource.org/license/mit

Annotation Guidelines: N/A

Annotation Process: Automatic

Dataset Quality Metrics: N/A

Errata: N/A

### Disclosures and Ethical Review

N/A

### Other

N/A

### Glossary

N/A

#### About this data statement

A data statement is a characterization of a dataset that provides context to allow developers and users to better understand how experimental results might generalize, how software might be appropriately deployed, and what biases might be reflected in systems built on the software.

This data statement was written based on the template for the Data Statements Version 2 Schema. The template was prepared by Angelina McMillan-Major, Emily M. Bender, and Batya Friedman and can be found at http://techpolicylab.uw.edu/data-statements.

## Appendix L Paper Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? We ensure that the main claims accurately reflect the contributions. 2. Did you describe the limitations of your work? Provided in the Supplementary Material. 3. Did you discuss any potential negative societal impacts of your work? Provided in Supplementary Material. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? We ensure that our paper conforms to ethics review guidelines.
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? Provided 2. Did you include complete proofs of all theoretical results? Provided
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? See Supplementary material for details. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? See H.1 for details. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? We fix the random seed in our experiments for fairness and report error bars in the supplementary material to demonstrate that randomness in performance is minimal. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? See Supplementary Material for details.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? We appropriately cited existing resources including links. See Supplementary Material. 2. Did you mention the license of the assets? We provide licenses for all original datasets whenever possible. See Supplementary Material. 3. Did you include any new assets either in the supplemental material or as a URL? We have included scripts to regenerate it alongside our generated captions in Croissant format in Supplementary Material. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? We use existing publically accessible datasets for images, and merely reception them using them off-the-shelf captioning models. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? Yes, we ran checks to ensure no PII or offensive content was added during our curation process.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? We have no human experiments. 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? We have no human experiments. 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? We have no human experiments.

We provide additional details including datasheet for our dataset, along with our benchmark provided in Croissant format, reproducible scripts for all of our experiments and our visualization interface in the supplementary material.