# Fast Bellman Updates for Wasserstein Distributionally Robust MDPs

 Zhuodong Yu\({}^{1}\) Ling Dai\({}^{1}\) Shaohang Xu\({}^{1}\) Siyang Gao\({}^{2}\) Chin Pang Ho\({}^{1}\)

\({}^{1}\)School of Data Science, City University of Hong Kong, Hong Kong

\({}^{2}\)Department of Systems Engineering, City University of Hong Kong, Hong Kong

{zhuodonyu2-c, lingdai5-c, shaohanxu2-c}@my.cityu.edu.hk

{siyangao, clint.ho}@cityu.edu.hk

###### Abstract

Markov decision processes (MDPs) often suffer from the sensitivity issue under model ambiguity. In recent years, robust MDPs have emerged as an effective framework to overcome this challenge. Distributionally robust MDPs extend the robust MDP framework by incorporating distributional information of the uncertain model parameters to alleviate the conservative nature of robust MDPs. This paper proposes a computationally efficient solution framework for solving distributionally robust MDPs with Wasserstein ambiguity sets. By exploiting the specific problem structure, the proposed framework decomposes the optimization problems associated with distributionally robust Bellman updates into smaller subproblems, which can be solved efficiently. The overall complexity of the proposed algorithm is quasi-linear in both the numbers of states and actions when the distance metric of the Wasserstein distance is chosen to be \(L_{1}\), \(L_{2}\), or \(L_{\infty}\) norm, and so the computational cost of distributional robustness is substantially reduced. Our numerical experiments demonstrate that the proposed algorithms outperform other state-of-the-art solution methods.

## 1 Introduction

Markov Decision Processes (MDPs) provide a flexible and powerful modeling framework for sequential decision-making problems under uncertainty (Puterman, 2014; Sutton and Barto, 2018). However, the standard MDP model assumes that the exact knowledge of the transition kernel is available, which is not the case for most real-world applications. While these model parameters can be estimated from data, it is well-known that the optimal policy of MDP is sensitive to estimation errors because of the dynamic nature of the problem. In particular, small errors in estimating transition kernels could lead to catastrophic failures in practice (Iyengar, 2005; Nilim and El Ghaoui, 2005; Le Tallec, 2007).

To overcome the aforementioned challenge in model ambiguity, robust MDPs assume that the transition kernels belong to ambiguity sets (Iyengar, 2005; Nilim and El Ghaoui, 2005; Xu and Mannor, 2006). Through optimizing the worst-case performance, robust MDPs compute robust policies to avoid disappointing performance due to the sensitivity issue caused by inaccurate transition kernels. However, robust MDPs are often too conservative because they always adopt the worst possible transition kernels without any consideration of their likelihood. To mitigate this undesirable side effect of robust MDPs, distributionally robust MDPs are proposed to incorporate distributional information of model ambiguity and maximize the expected reward under the most adversarial probability distribution (Xu and Mannor, 2010; Yu and Xu, 2015).

In a distributionally robust MDP, we assume that all the plausible distributions of the transition kernel belong to a prescribed ambiguity set, which could be formalized in different types, such as moment ambiguity sets (Delage and Ye, 2010) and Wasserstein ambiguity sets (Mohajerin Esfahani andKuhn, 2018). In this paper, we focus on the Wasserstein ambiguity sets, which have been a popular choice for distributionally robust data-driven optimization in recent years because of their outstanding empirical performance as well as nice theoretical properties, such as consistency in optimality and finite-sample bounds (Mohajerin Esfahani and Kuhn, 2018; Gao and Kleywegt, 2022). By using Wasserstein distance to formulate the ambiguity set, Yang (2017) shows that there exists an optimal policy that is stationary and Markovian for the corresponding Wasserstein distributionally robust MDP.

While (distributionally) robust MDPs can be solved by extending standard solution methods in classical MDPs to their (distributionally) robust counterparts, these solution methods become much more computationally demanding. For example, each Bellman update for (distributionally) robust MDPs can be formulated as a convex optimization problem. Without making use of any specific problem structure, one would need to use generic convex optimization solvers to compute these Bellman updates, which have to be evaluated numerous times for computing the (distributionally) robust value function. This computational challenge restricted the application of (distributionally) robust MDPs to small or medium size of problems. In recent years, many efficient algorithms are proposed for solving robust MDPs to address this issue (Iyengar, 2005; Nilim and El Ghaoui, 2005; Ho et al., 2018; Behzadian et al., 2021; Grand-Clement and Kroer, 2021b; Ho et al., 2022).

On the other hand, however, distributionally robust MDPs have received limited attention on their computational efficiency. To the best of our knowledge, (Grand-Clement and Kroer, 2021a) is the only pioneer work that proposes a first-order method to efficiently solve Wasserstein distributionally robust MDPs. However, being restricted by the nature of first-order methods, the proposed algorithm in (Grand-Clement and Kroer, 2021a) struggles when high accuracy is needed or when the discount factor is close to one, as it would take unsatisfactorily many numbers of iterations to compute the distributionally robust value function.

In this paper, we take an alternative approach to develop fast algorithms for solving Wasserstein distributionally robust MDPs. In particular, we reformulate the optimization problem of the distributionally robust Bellman update to a structural form, and by exploiting the specific problem structure, we propose decomposition schemes in which the problem of interest can be decomposed into smaller subproblems, which can be solved by our customized algorithms in quasi-linear time. As we will show later, the time complexities of the proposed algorithms are linear in the numbers of actions and kernels and quasi-linear in the number of states, which are significantly better than the state-of-the-art solution methods (Xu and Mannor, 2010; Grand-Clement and Kroer, 2021a).

The rest of this paper is organized as follows. Section 2 introduces the related work, and Section 3 describes the setting of Wasserstein distributionally robust MDPs. In Section 4, we illustrate our decomposition scheme for computing the distributionally robust Bellman updates when the Wasserstein distance in the ambiguity set is chosen to be \(L_{q}\) norm. In Section 5, customized fast algorithms and their time complexities are particularly provided for the three most well-known cases: \(L_{1}\), \(L_{2}\), and \(L_{\infty}\). The numerical experiments in Section 6 verify that our methods outperform other state-of-the-art algorithms.

NotationsWe use boldface lowercase and uppercase letters to denote vectors and matrices, respectively. We denote by \([N]\) the set of natural numbers from \(1\) to \(N\); that is, \([N]=\{1,2,\ldots,N\}\). The vector \(\mathbf{e}\) is denoted as the vector of all ones while the dimension depends on the context. The vector \(\mathbf{e}_{j}\) represents the vector of all zeros except the \(j\)-th component is one. Probability simplex is denoted as \(\Delta_{N}=\left\{\mathbf{x}\in\mathbb{R}^{N}:\mathbf{e}^{\top}\mathbf{x}=1,\mathbf{x}\geq\mathbf{ 0}\right\}\). The smallest component of a vector \(\mathbf{x}\) is denoted by \(\min\{\mathbf{x}\}\). Given a set \(\mathcal{X}\), we call the set of all the Borel probability measures as \(P(\mathcal{X})\). The general \(L_{q}\) norm for a vector \(\mathbf{x}\in\mathbb{R}^{N}\) is denoted by \(\left\|\mathbf{x}\right\|_{q}=\left(\sum_{i=1}^{N}\left|x_{i}\right|^{q}\right)^{1 /q}\) for \(1\leq q<\infty\), and the \(L_{\infty}\) norm is \(\left\|\mathbf{x}\right\|_{\infty}=\max_{i\in[N]}\left|x_{i}\right|\). The Dirac distribution at \(\mathbf{x}\in\mathbb{R}^{N}\) is denoted by \(\delta_{\mathbf{x}}\). For optimization problems with multiple constraints, we indicate the decision variables on the last line of the constraints.

## 2 Related Work

Research on Markov decision processes under model ambiguity can be traced back to the seventies (Satia and Lave Jr, 1973). In recent years, much progress has been made in the development of robust MDPs. The most fundamental solution scheme to solve robust MDPs is _robust value iteration_, which is the robust counterpart of the standard value iteration and is first developed in (Givan et al., 2000; Iyengar, 2005; Nilim and El Ghaoui, 2005) for robust MDPs with \((s,a)\)-rectangular ambiguity sets. Wiesemann et al. (2013) introduce the \(s\)-rectangular ambiguity sets; the authors reformulate the robust Bellman updates as convex optimization problems and solve them by using off-the-shelf solvers that have polynomial-time complexity. Researchers have been investigating the theoretical properties of robust MDPs, such as the error bound of applying robust MDPs with state aggregation (Petrik and Subramanian, 2014), the relationship between robustness and regularization (Derman et al., 2021), and the geometry of value function in robust MDPs (Wang et al., 2022). Other than the \(s\)- and \((s,a)\)-ambiguity sets, different types of ambiguity sets have been also proposed to mitigate the side effect of conservatism, such as \(k\)-rectangularity (Mannor et al., 2016) and \(r\)-rectangularity (Goyal and Grand-Clement, 2023). Different related models are also proposed to incorporate robustness in different settings, such as robust baseline regret (Ghavamzadeh et al., 2016), imitation learning (Brown et al., 2020), and soft-robust model (Lobo et al., 2020). In terms of algorithmic development, Ho et al. (2018) propose fast algorithms for solving \(s\)-rectangular robust MDPs with weighted \(L_{1}\) norm ambiguity sets and propose to compute robust Bellman updates using the combination of bisection method and homotopy method, and Behzadian et al. (2021) further extend the solution scheme for the unweighted \(L_{\infty}\) norm cases (Delgado et al., 2016). A _first order method_ is introduced in (Grand-Clement and Kroer, 2021b) to approximate robust Bellman updates for robust MDPs with ellipsoidal and Kullback-Leibler (KL) \(s\)-rectangular ambiguity sets. Recently, Ho et al. (2022) propose the \(\phi\)-divergence ambiguity set, which generalizes multiple popular choices of ambiguity sets, and they propose an unified solution scheme to compute robust Bellman updates efficiently. On the other hand, _robust policy iteration_ alternates between robust policy evaluation and policy improvement (Iyengar, 2005) and it has better empirical performance compared to robust value iteration. Kaufman and Schaefer (2013) propose a modified policy iteration for (\(s,a\))-rectangular robust MDPs. Ho et al. (2021) introduce partial policy iteration for \(s\)- and (\(s,a\))-rectangular robust MDPs with \(L_{1}\) norm ambiguity sets. Kumar et al. (2022) study the connection between robust MDPs and regularized MDPs, and propose robust policy iteration for \(s\)-rectangular robust MDPs. Other than robust policy iteration, Li et al. (2022) propose a policy gradient method for solving rectangular robust MDPs. In order to solve large-scale problems, Tamar et al. (2014) apply value function approximation and propose _robust approximate dynamic programming_ for robust MDPs.

While this paper is focused on the model-based setting, recently there is an active line of research focusing on _robust reinforcement learning (RL)_ algorithms (Roy et al., 2017; Badrinath and Kalathil, 2021; Wang and Zou, 2021, 2022; Panaganti and Kalathil, 2022). Liu et al. (2022) propose a robust Q-learning algorithm for \((s,a)\)-rectangular robust MDPs. A function approximation approach is proposed in (Panaganti et al., 2022) for \((s,a)\)-rectangular robust MDPs with offline datasets. The combination of robust RL and deep RL is also studied in (Pinto et al., 2017; Mankowitz et al., 2019; Zhang et al., 2020) although they do not provide theoretical guarantees of the learned policies. We would like to emphasize that both model-based and model-free methods have their own merits: while model-free methods are powerful for applications where data could be generated without much risk and cost, model-based methods are suitable for applications with very limited data or settings where the (time and financial) cost of obtaining new data is high.

For distributionally robust MDPs with the nested-set structured parameter ambiguity sets, Xu and Mannor (2010) propose a Bellman type backward induction to compute the optimal policy, and Yu and Xu (2015) further extend it to a more general state-wise ambiguity set following the unified framework proposed in (Wiesemann et al., 2014). The Wasserstein ambiguity sets are first introduced for distributionally robust MDPs in (Yang, 2017), and a convex formulation is proposed based on Kantorovich duality. Chen et al. (2019) propose a general formulation to combine both moment-based and statistical-distance-based ambiguity sets together for distributionally robust MDPs. Derman and Mannor (2020) study the connection between Wasserstein distributionally robust MDPs and regularization. Abdullah et al. (2019) propose a reinforcement learning framework with distributionally robustness. As mentioned before, however, (Grand-Clement and Kroer, 2021a) is the only work that focuses on the algorithmic development of solving Wasserstein distributionally robust MDPs, which is also the focus of this paper.

Preliminaries

A distributionally robust MDP is a tuple \((\mathcal{S},\mathcal{A},\mathbf{p}_{0},\mathbf{r},\lambda,\mathbb{M})\), where \(\mathcal{S}\) and \(\mathcal{A}\) are the sets of states and actions, respectively. We assume that the state space \(\mathcal{S}=\{1,\ldots,S\}\) and the action space \(\mathcal{A}=\{1,\ldots,A\}\) are finite. The initial state \(s_{0}\) follows a given initial distribution \(\mathbf{p}_{0}\in\Delta_{S}\). When the decision maker takes an action \(a\in\mathcal{A}\) in state \(s\in\mathcal{S}\), the MDP will transit to the next state randomly according to the probability distribution \(\mathbf{p}_{sa}\in\Delta_{S}\), which is assumed to be unknown; the transition from state \(s\in\mathcal{S}\) under action \(a\in\mathcal{A}\) to the next state \(s^{\prime}\in\mathcal{S}\) will induce the reward \(r_{sas^{\prime}}\). We use the vector \(\mathbf{r}_{sa}\in\mathbb{R}^{S}\) to represent the connection of all rewards given state \(s\) and action \(a\). We denote by \(\lambda\in(0,1)\) the discount factor. In distributionally robust MDPs, the transition kernel \(\underset{s\in\mathcal{S},a\in\mathcal{A}}{\Pi}\mathbf{p}_{sa}\in\left(\Delta_{S} \right)^{S\times A}\) is assumed to be uncertain, and it is governed by an unknown probability distribution \(\mu\) which is assumed to reside in a prescribed ambiguity set \(\mathbb{M}\) that is calibrated from historical data. Note that we also use the notation \(\mathbf{p}_{s}=\underset{a\in\mathcal{A}}{\Pi}\mathbf{p}_{sa}\in\left(\Delta_{S} \right)^{A}\) for the transition kernel given the state \(s\in\mathcal{S}\).

The goal of a distributionally robust MDP is to maximize the worst-case expected return; that is,

\[\underset{\mathbf{\pi}\in\left(\Delta_{A}\right)^{S}}{\min}\;\underset{\mu\in \mathbb{M}}{\text{min}}\;\underset{\mathbf{\pi}}{\text{E}}\Bigg{[}\underset{\mathbf{p} \sim\mu}{\text{E}}\Bigg{[}\underset{t=0}{\infty}\lambda^{t}r_{s_{t}a_{t}s_{t+ 1}}\;:\;s_{0}\sim\mathbf{p}_{0}\Bigg{]}\Bigg{]}. \tag{1}\]

Here, the decision variable \(\mathbf{\pi}\in\left(\Delta_{A}\right)^{S}\) is called a (randomized) policy where for any given \(s\in\mathcal{S}\), its sub-vector \(\mathbf{\pi}_{s}\in\Delta_{A}\) represents the probability distribution of taking action \(a\in\mathcal{A}\) in state \(s\).

It is worth mentioning that the above formulation of distributionally robust MDPs is a generalization of both robust MDPs and standard MDPs. By specifying the ambiguity set \(\mathbb{M}\) to be \(P(\mathcal{U})\) where \(\mathcal{U}\) is the ambiguity set of a robust MDP, the problem (1) is equivalent to the robust MDP with ambiguity set \(\mathcal{U}\). If the set \(\mathcal{U}\) is a singleton, then problem (1) with \(\mathbb{M}=P(\mathcal{U})\) is equivalent to the standard MDP with transition kernel from the singleton \(\mathcal{U}\).

Similar to the case of robust MDPs, we consider the common assumption of rectangularity for the sake of tractability, as otherwise problem (1) is NP-hard (Wiesemann et al., 2013). In particular, we consider the following \(s\)-rectangular \(q\)-Wasserstein ambiguity set (Grand-Clement and Kroer, 2021)

\[\mathbb{M}^{q}=\Bigg{\{}\mu\in P\left(\left(\Delta_{S}\right)^{S\times A} \right)\;:\;\mu=\bigotimes_{s\in\mathcal{S}}\mu_{s},\;\mu_{s}\in\mathbb{M}_{s }^{q}\subseteq P\left(\left(\Delta_{S}\right)^{A}\right),\;\forall s\in \mathcal{S}\Bigg{\}},\]

where \(\mathbb{M}_{s}^{q}\) is the marginal ambiguity set for the distribution \(\mu_{s}\) that governs the transition kernel at the state \(s\in\mathcal{S}\). The superscript \(q\) indicates that \(\mathbb{M}_{s}^{q}\) is defined by \(q\)-Wasserstein distance; that is,

\[\mathbb{M}_{s}^{q}=\Big{\{}\mu_{s}\in P\left(\left(\Delta_{S}\right)^{A} \right)\;:\;W_{q}\left(\mu_{s},\nu_{s}\right)\leq\theta\Big{\}}\;\;\text{for} \;\;1\leq q\leq\infty,\]

where \(\nu_{s}\) is the reference distribution at the center of the Wasserstein ball and

\[W_{q}\left(\mu_{s},\nu_{s}\right)=\min_{\kappa\in P\left(\left(\Delta_{S} \right)^{A}\times\left(\Delta_{S}\right)^{A}\right)}\left\{\left(\underset{ \left(x,y\right)\sim\kappa}{\mathbb{E}}\left(d_{q}(x,y)\right)^{q}\right)^{1 /q}\;:\;\kappa\in\Gamma(\mu_{s},\nu_{s})\right\},\]

for \(1\leq q<\infty\), where \(\Gamma(\mu_{s},\nu_{s})\) is the set of all the couplings of \(\mu_{s}\) and \(\nu_{s}\) (Ambrosio and Nicola Gigli, 2005), and \(d_{q}\) is induced by the \(q\)-norm \(d_{q}(x,y)=\left\|x-y\right\|_{q}\) throughout this work. We define the \(\infty\)-Wasserstein distance \(W_{\infty}(\mu_{s},\nu_{s})\) by taking \(q\rightarrow\infty\).

We adopt the usual setting in distributionally robust data-driven optimization where \(\nu_{s}=\frac{1}{N}\sum_{i=1}^{N}\delta_{\hat{\mathbf{p}}_{s}^{i}}\) is set to be the empirical distribution and estimated by the samples \(\{\hat{\mathbf{p}}_{s}^{i}\}_{i=1}^{N}\) where \(\hat{\mathbf{p}}_{s}^{i}\in\left(\Delta_{S}\right)^{A}\) for every \(i\in\left[N\right]\). Then, the optimal value function of (1), \(\mathbf{v}^{\star}\), satisfies the following distributionally robust Bellman equation (Yu and Xu, 2015; Yang, 2017; Grand-Clement and Kroer, 2021)

\[v_{s}^{\star}=\underset{\mathbf{\pi}_{s}\in\Delta_{A}}{\max}\min_{\mathbf{p}_{s}\in \mathbb{M}_{s}^{q}}\left(\sum_{a\in\mathcal{A}}\pi_{sa}\mathbf{p}_{sa}^{\top}\left( \mathbf{r}_{sa}+\lambda\mathbf{v}^{\star}\right)\right)\quad\forall s\in\mathcal{ S}, \tag{2}\]where \(\mathbb{B}_{s}^{q}\) is the set of expected kernels (Yang, 2017; Bertsimas et al., 2018; Xie, 2020) of the following forms

\[\mathbb{B}_{s}^{q} = \left\{\frac{1}{N}\sum_{i=1}^{N}\boldsymbol{p}_{s}^{i}\,:\,\frac{ 1}{N}\sum_{i=1}^{N}\left\|\boldsymbol{p}_{s}^{i}-\hat{\boldsymbol{p}}_{s}^{i} \right\|_{q}^{q}\leq\theta^{q},\;\boldsymbol{p}_{s}^{i}\in(\Delta_{S})^{A},\; \forall i\in[N]\right\}\;\;\;\;\text{for}\;\;1\leq q<\infty,\] \[\mathbb{B}_{s}^{\infty} = \left\{\frac{1}{N}\sum_{i=1}^{N}\boldsymbol{p}_{s}^{i}\,:\,\left\| \boldsymbol{p}_{s}^{i}-\hat{\boldsymbol{p}}_{s}^{i}\right\|_{\infty}\leq\theta,\;\boldsymbol{p}_{s}^{i}\in(\Delta_{S})^{A}\,,\;\forall i\in[N]\right\}.\]

## 4 The Decomposition Algorithm for Distributionally Robust Bellman Update

In this section, we focus on computing the distributionally robust Bellman update, which is the most fundamental operator for solving the distributionally robust Bellman equation (2); we define the distributionally robust Bellman operator \(\mathfrak{T}(\cdot)\) where for any \(\boldsymbol{v}\in\mathbb{R}^{S}\),

\[[\mathfrak{T}(\boldsymbol{v})]_{s}=\max_{\boldsymbol{\pi}_{s}\in\Delta_{A}} \min_{\boldsymbol{p}_{s}\in\mathbb{B}_{s}^{q}}\sum_{a\in\mathcal{A}}\pi_{sa} \cdot\boldsymbol{p}_{sa}^{\top}\left(\boldsymbol{r}_{sa}+\lambda\boldsymbol{v }\right)\;\;\;\;\forall s\in\mathcal{S}. \tag{4}\]

Given the above definition of \(\mathfrak{T}(\cdot)\), it is well-known that (2) can be solved by iteratively applying \(\mathfrak{T}\), which is a variant of standard value iteration for solving distributionally robust MDPs (Yu and Xu, 2015; Grand-Clement and Kroer, 2021a). For any initial guess \(\boldsymbol{v}^{0}\in\mathbb{R}^{S}\), we have \(\lim_{t\rightarrow\infty}\boldsymbol{v}^{t}\rightarrow\boldsymbol{v}^{\star}\) where \(\boldsymbol{v}^{t}=\mathfrak{T}(\boldsymbol{v}^{t-1})\) for \(t=1,2,\dots\) and \(\boldsymbol{v}^{\star}\) satisfies (2). Therefore, the efficiency of evaluating \([\mathfrak{T}(\boldsymbol{v})]_{s}\) is crucial to the computation of solving distributionally robust MDPs.

However, computing \([\mathfrak{T}(\boldsymbol{v})]_{s}\) using generic convex optimization solvers is much more computationally demanding compared to the case of classical MDPs, which only have time complexity \(\mathcal{O}(SA)\) for computing their Bellman updates. In this section, we exploit the specific problem structure in (4) and reformulate the optimization problem to a form that could be decomposed into smaller problems. We first focus on the case of \(q\in[1,\infty)\) in Section 4.1 and then consider the case of \(q=\infty\) in Section 4.2, where \(q\) indicates the type of Wasserstein distance used in the ambiguity set. As we will show later, combined with the customized fast algorithms in Section 5 for solving the subproblems, one can compute \([\mathfrak{T}(\boldsymbol{v})]_{s}\) in time complexity that is quasi-linear in \(S\) and linear in \(A\) and \(N\). All the proofs of propositions and theorems in this section are provided in Appendix A.1.

### Nested bisection method for \(q\)-Wasserstein ambiguity set with \(q\in[1,\infty)\)

We consider the optimization problem in (4) for the case where \(q\in[1,\infty)\). By applying the minimax theorem, we obtain the following result.

**Proposition 4.1**.: _Consider the Bellman updates (4) with \(q\in[1,\infty)\). Then,_

\[[\mathfrak{T}(\boldsymbol{v})]_{s}=\left[\begin{array}{cl} \operatorname*{minimize}&\gamma\\ \operatorname*{subject\;to}&\frac{1}{N}\sum_{i=1}^{N}( \boldsymbol{r}_{sa}+\lambda\boldsymbol{v})^{\top}\boldsymbol{p}_{sa}^{i}\leq \gamma,\;\forall a\in\mathcal{A}\\ &\frac{1}{N}\sum_{i=1}^{N}\sum_{a\in\mathcal{A}}\left\|\boldsymbol{p}_{sa}^{i}- \hat{\boldsymbol{p}}_{sa}^{i}\right\|_{q}^{q}\leq\theta^{q}\\ &\gamma\in\mathbb{R},\;\boldsymbol{p}_{sa}^{i}\in\Delta_{S},\;\forall i\in[N] \,,\;\forall a\in\mathcal{A}.\end{array}\right]\;\;\;\;\forall s\in\mathcal{S}. \tag{5}\]

The above proposition reformulates the maximin optimization problem in (4) into a convex minimization problem, which can be solved by commercial convex optimization solvers. Moreover, problem (5) can be solved via bisection on \(\gamma\); that is, we seek for the lowest possible \(\gamma\) such that \(\frac{1}{N}\sum_{i=1}^{N}(\boldsymbol{r}_{sa}+\lambda\boldsymbol{v})^{\top} \boldsymbol{p}_{sa}^{i}\leq\gamma\) for each \(a\in\mathcal{A}\) while \(\boldsymbol{p}_{sa}^{i}\) satisfies the other constraints in (5), for every \(a\in\mathcal{A}\). To this end, we introduce the following subproblem

\[\mathfrak{P}\left(\{\hat{\boldsymbol{p}}_{sa}^{i}\}_{i=1}^{N}; \boldsymbol{b}_{sa},\gamma\right)=\left[\begin{array}{cl}\operatorname*{ minimize}&\frac{1}{N}\sum_{i=1}^{N}\left\|\boldsymbol{p}_{sa}^{i}-\hat{ \boldsymbol{p}}_{sa}^{i}\right\|_{q}^{q}\\ \operatorname*{subject\;to}&\frac{1}{N}\sum_{i=1}^{N} \boldsymbol{b}_{sa}^{\top}\boldsymbol{p}_{sa}^{i}\leq\gamma\\ &\boldsymbol{p}_{sa}^{i}\in\Delta_{S},\;\forall i\in[N]\end{array}\right]. \tag{6}\]By setting \(\mathbf{b}_{sa}=\mathbf{r}_{sa}+\lambda\mathbf{v}\), the above problem \(\mathfrak{P}\left(\{\hat{\mathbf{p}}_{sa}^{i}\}_{i=1}^{N};\mathbf{b}_{sa},\gamma\right)\) allows us to seek for decision variable \(\{\mathbf{p}_{sa}^{i}\}_{i=1}^{N}\) that satisfies the first and the last line of constraints in (5) while minimizing the "budget" used in the second line of constraint in (5). In particular, for any fixed \(\gamma^{\prime}\in\mathbb{R}\), we distinguish among the following two cases:

* If \(\sum_{a\in\mathcal{A}}\mathfrak{P}\left(\{\hat{\mathbf{p}}_{sa}^{i}\}_{i=1}^{N};\bm {b}_{sa},\gamma^{\prime}\right)\leq\theta^{q}\), then \(\gamma^{\prime}\) is feasible for (5) and it is an upper bound of the optimal objective value in (5).
* If \(\sum_{a\in\mathcal{A}}\mathfrak{P}\left(\{\hat{\mathbf{p}}_{sa}^{i}\}_{i=1}^{N};\bm {b}_{sa},\gamma^{\prime}\right)>\theta^{q}\), then \(\gamma^{\prime}\) must be infeasible for problem (5) and so it is a lower bound of the optimal objective value in (5).

To apply the bisection method, we derive the initial upper and lower bounds of the optimal \(\gamma^{\star}\) in (5).

**Proposition 4.2**.: _Consider the distributionally robust Bellman update (5). The optimal objective value \(\gamma^{\star}\) is bounded by_

\[\max_{a\in\mathcal{A}}\left\{\min\left\{\mathbf{b}_{sa}\right\}\right\}\leq\gamma^ {\star}\leq\max_{a\in\mathcal{A}}\left\{\frac{1}{N}\sum_{i=1}^{N}\mathbf{b}_{sa}^{ \top}\hat{\mathbf{p}}_{sa}^{i}\right\}. \tag{7}\]

As opposed to (5), the size of problem (6) is independent of \(A\). By using the aforementioned bisection method, the overall time complexity is now only linear in \(A\) since we only need to solve (6) \(\mathcal{O}(A\log\epsilon_{1})\) times, where \(\epsilon_{1}\) is the tolerance for the bisection method.

While one can solve problem (6) using off-the-shelf solvers and enjoy the reduced complexity by using the above bisection method, this subproblem itself turns out to be another structural optimization problem. As we will show in the following proposition, by applying duality on (6), the reformulation could be further decomposed via another bisection method on the dual variable.

**Proposition 4.3**.: _Consider the problem \(\mathfrak{P}\left(\{\hat{\mathbf{p}}_{sa}^{i}\}_{i=1}^{N};\mathbf{b}_{sa},\gamma\right)\) in (6). If \(\gamma>\min\left\{\mathbf{b}_{sa}\right\}\), then_

\[\mathfrak{P}\left(\{\hat{\mathbf{p}}_{sa}^{i}\}_{i=1}^{N};\mathbf{b}_{sa},\gamma\right) =\max_{0\leq\alpha\leq\alpha}-\alpha\gamma+\frac{1}{N}\sum_{i=1}^{N} \mathfrak{D}_{q}(\hat{\mathbf{p}}_{sa}^{i},\mathbf{b}_{sa},\alpha), \tag{8}\]

_where \(\bar{\alpha}=\max_{i\in[N]}\left\|\mathbf{e}_{j}-\hat{\mathbf{p}}_{sa}^{i}\right\|_{q }^{q}/(\gamma-\min\left\{\mathbf{b}_{sa}\right\})\) and_

\[\mathfrak{D}_{q}(\hat{\mathbf{p}}_{sa}^{i},\mathbf{b}_{sa},\alpha)=\min_{\mathbf{p}_{sa}^ {i}\in\Delta_{S}}\left\|\hat{\mathbf{p}}_{sa}^{i}-\hat{\mathbf{p}}_{sa}^{i}\right\|_{q }^{q}+\alpha\cdot\mathbf{b}_{sa}^{\top}\mathbf{p}_{sa}^{i}, \tag{9}\]

_where \(j\in\operatorname*{arg\,min}_{s^{\prime}\in\mathcal{S}}\,b_{sa}^{s}\)._

Notice that we focus on the case where \(\gamma>\min\left\{\mathbf{b}_{sa}\right\}\) since \(\min\left\{\mathbf{b}_{sa}\right\}\) is not larger than the lower bound of \(\gamma\) in (7), and the case \(\gamma=\min\left\{\mathbf{b}_{sa}\right\}\) will not occur because \(\gamma\) is always taken to be the average of the upper and lower bounds in our bisection method. We refer interested readers to Appendix for more details.

Proposition 4.3 indicates that for any fixed feasible \(\alpha\) in (8), the subproblems (9) can be solved separately. Therefore, by applying the bisection method on \(\alpha\) in problem (8), one can naturally decompose the problem (8) into \(N\) smaller problems that have \(S\) variables and \(S+1\) constraints. As a consequence, the overall time complexity is linear in the number of kernels \(N\).

By combining both decomposition strategies above, we obtain the proposed nested bisection method, whose pseudocode could be found in Algorithm 1. More details of the algorithm is provided in the Appendix A.1.

**Theorem 4.4**.: _Suppose \(\gamma^{\prime}\) is the value returned by Algorithm 1, and \(\gamma^{\star}\) be the optimal value of (5). With the inputs provided in Algorithm 1 and user-specified tolerances \(\epsilon_{1},\epsilon_{2}>0\), we have_

\[|\gamma^{\prime}-\gamma^{\star}|\leq\frac{\epsilon_{1}}{2}+\frac{A\epsilon_{2} \left(\max_{a\in\mathcal{A}}\max\{\mathbf{b}_{sa}\}-\underline{\gamma}\right) \left(\max_{a\in\mathcal{A}}\max\{\mathbf{b}_{sa}\}+\bar{\gamma}\right)}{\theta ^{q}}.\]

**Theorem 4.5**.: _Algorithm 1 computes (5) in time \(\mathcal{O}\left(h_{q}(S)NA\log\epsilon_{1}^{-1}\log\epsilon_{2}^{-1}+AS\right)\), where \(h_{q}(S)\) is the time complexity for solving (9)._As shown in the above theorem, the proposed nested bisection method has a time complexity that is linear in both \(A\) and \(N\), but the overall complexity depends on the subproblem (9) where its complexity depends on the choice of \(q\) and the number of states \(S\). In Section 5, we will derive \(h_{1}(\hat{S})\) and \(h_{2}(S)\), which are the two most common cases. The complexities associated with solving equations (5) and (8) using general convex optimization methods are also provided in the Appendix B.

### Decomposition scheme for \(\infty\)-Wasserstein ambiguity set

As opposed to the case where \(q\in[1,\infty)\), for \(\infty\)-Wasserstein ambiguity set, the corresponding Bellman update can be naturally decomposed without using any bisection method.

**Proposition 4.6**.: _Consider the Bellman update (4) with \(q=\infty\). Then,_

\[[\mathfrak{T}(\boldsymbol{v})]_{s}=\frac{1}{N}\max_{a\in\mathcal{A}}\sum_{i=1} ^{N}\min_{\boldsymbol{p}^{i}_{sa}\in\Delta_{S}}\left\{\boldsymbol{b}^{\top}_{ sa}\boldsymbol{p}^{i}_{sa}\ :\ \left\|\boldsymbol{p}^{i}_{sa}-\hat{\boldsymbol{p}}^{i}_{sa} \right\|_{\infty}\leq\theta\right\}\quad\forall s\in\mathcal{S}. \tag{10}\]

The above reformulation requires solving \(NA\) inner minimization problems that have \(S\) variables and \(S+2\) constraints. Hence, the time complexity of Bellman update (10) is linear in \(A\) and \(N\). Similar to Section 4.1, we will discuss the complexity of solving the inner minimization in the next section.

## 5 Efficient Algorithms for Subproblems

Section 4 offers decomposition schemes for distributionally robust MDPs with \(q\)-Wasserstein ambiguity set. As shown in Theorem 4.5 and Proposition 4.6, the overall complexity of computing distributionally robust Bellman update is linear in both \(A\) and \(N\) but it depends on complexities of solving the subproblem (9) and the inner minimization problem in (10). In this section, we consider the common cases where \(q\in\{1,2,\infty\}\) and discuss the time complexity of solving the subproblem in each case.

As we will show below, the overall time complexities of our decomposition algorithms are only \(\mathcal{O}\left(SA\log S+NAS\log\epsilon_{1}^{-1}\log\epsilon_{2}^{-1}\right)\) for \(q=1\), \(\mathcal{O}(NAS\log S\log\epsilon_{1}^{-1}\log\epsilon_{2}^{-1})\) for \(q=2\), and \(\mathcal{O}(AS(\log S+N))\) when \(q=\infty\). These complexities are much lower than \(\mathcal{O}(N^{3.5}A^{3.5}S^{3.5}\log(\epsilon^{-1}))\) and \(\mathcal{O}(NA^{2.5}S^{2.5}\log(S)\epsilon^{-1.5})\) from the state-of-the-art solution methods in (Xu and Mannor, 2010) and (Grand-Clement and Kroer, 2021), respectively. The proofs of the following propositions, theorems, and corollaries and the details of the proposed algorithms are relegated to Appendix A.2.

### \(1\)-Wasserstein ambiguity set

When \(q=1\), the corresponding subproblem (9) has an equivalent form as follows.

**Proposition 5.1**.: _Suppose \(q=1\). The minimization problem (9) is equivalent to_

\[\min_{v\geq 0}\left\{v+\alpha\min_{\mathbf{p}_{sa}^{i}\in\Delta_{S}}\mathbf{b}_{ sa}^{\top}\mathbf{p}_{sa}^{i}\ :\ \left\|\mathbf{p}_{sa}^{i}-\hat{\mathbf{p}}_{sa}^{i}\right\|_{1 }\leq v\right\}. \tag{11}\]

While problem (11) does not appear to be trivial at first glance, it has nice mathematical properties which allow us to solve this problem efficiently.

**Theorem 5.2**.: _The objective function in (11) is piecewise-linear in \(v\), and it can be solved in time \(\mathcal{O}(S\log S)\)._

Therefore, the proposed Algorithm 1 can compute the Bellman update with the time complexity linear in \(A\) and \(N\) and quali-linear in \(S\). We obtain the following result by combining Theorem 4.5 and Theorem 5.2.

**Corollary 5.2.1**.: _The distributionally robust Bellman update with \(1\)-Wasserstein ambiguity set can be computed in time \(\mathcal{O}\left(SA\log S+NAS\log\epsilon_{1}^{-1}\log\epsilon_{2}^{-1}\right)\), where \(\epsilon_{1}\) and \(\epsilon_{2}\) are the user-specified tolerances in Algorithm 1._

### \(2\)-Wasserstein ambiguity set

When \(q=2\), the following result shows that (9) could be transformed to an Euclidean projection problem onto the probability simplex.

**Theorem 5.3**.: _Suppose \(q=2\). The minimization problem (9) is equivalent to_

\[-\frac{\alpha^{2}\left\|\mathbf{b}_{sa}\right\|_{2}^{2}}{4}+\alpha\mathbf{b}_{sa}^{ \top}\hat{\mathbf{p}}_{sa}^{i}+\min_{\mathbf{p}_{sa}^{i}\in\Delta_{S}}\left\|\mathbf{p}_{ sa}^{i}-\frac{2\hat{\mathbf{p}}_{sa}^{i}-\alpha\mathbf{b}_{sa}}{2}\right\|_{2}^{2}, \tag{12}\]

_which can be solved in time \(\mathcal{O}(S\log S)\)._

Euclidean projection problem onto the probability simplex can be solved via the existing algorithm (Wang and Carreira-Perpinan, 2013). By combining the results in Theorem 4.5 and Theorem 5.3, we obtain the following result.

**Corollary 5.3.1**.: _The distributionally robust Bellman update with \(2\)-Wasserstein ambiguity set can be computed in time \(\mathcal{O}\left(NAS\log S\log\epsilon_{1}^{-1}\log\epsilon_{2}^{-1}\right)\), where \(\epsilon_{1}\) and \(\epsilon_{2}\) are the user-specified tolerances in Algorithm 1._

### \(\infty\)-Wasserstein ambiguity set

When \(q=\infty\), the corresponding subproblem

\[\min_{\mathbf{p}_{sa}^{i}\in\Delta_{S}}\left\{\mathbf{b}_{sa}^{\top}\mathbf{p}_{sa}^{i}\ :\ \left\|\mathbf{p}_{sa}^{i}-\hat{\mathbf{p}}_{sa}^{i}\right\|_{\infty}\leq\theta\right\} \tag{13}\]

can be reformulated to a linear program with box constraints and a linear equality constraint which can be solved efficiently.

**Theorem 5.4**.: _The minimization problem (13) is solved in time \(\mathcal{O}(S\log S)\)._

Therefore, we obtain the overall complexity as follows.

**Corollary 5.4.1**.: _The distributionally robust Bellman update with \(\infty\)-Wasserstein ambiguity set can be computed in time \(\mathcal{O}\left(AS\left(\log S+N\right)\right)\)._

## 6 Numerical Results

We experiment on the performance of the proposed algorithms (Fast), Gurobi (Gurobi Optimization, LLC, 2023), and the first-order method (FOM) proposed in (Grand-Clement and Kroer, 2021a) with different sizes of the randomly generated distributionally robust MDPs. Due to page limit, we only report the results of the experiments on a single Bellman update, and we provide additional experimental results in the Appendix C, which also contains the detailed settings of all the experiments.

Figure 1 reports the average computation times of Bellman updates with \(1\)-Wasserstein ambiguity sets. For the first-order method (Grand-Clement and Kroer, 2021a), we report the computation times of the third Bellman iteration, and these computation times increase at every Bellman iteration of the first-order method. One can see that all three algorithms perform similarly when problem size is small. However, as \(S\), \(A\), or \(N\) increases, the runtimes of both Gurobi and first-order method increase rapidly, while the proposed algorithm remains scalable.

The results with \(2\)-Wasserstein ambiguity sets and \(\infty\)-Wasserstein ambiguity sets are shown in Figure 2 and Figure 3, respectively. These results are similar to the case where \(q=1\), which are consistent to our theoretical results on complexities. As expected, the proposed algorithms are several orders of magnitude faster than the existing state-of-the-art solution methods.

Figure 1: Comparisons of all algorithms for \(q=1\), where _(left)_\(N=50\), _(middle)_\(A=70\), \(N=50\), and _(right)_\(S=A=70\).

Figure 3: Comparisons of all algorithms for \(q=\infty\), where _(left)_\(N=50\), _(middle)_\(A=70\), \(N=50\), and _(right)_\(S=A=70\).

Figure 2: Comparisons of all algorithms for \(q=2\), where _(left)_\(N=50\), _(middle)_\(A=70\), \(N=50\), and _(right)_\(S=A=70\).

Conclusion

This paper studies distributionally robust MDPs with Wasserstein ambiguity sets. In particular, we propose fast algorithms to compute distributionally robust Bellman updates. We show that the proposed algorithms outperform other existing methods in both theory and experiments. Future work could address the extensions to approximate dynamic programming for distributionally robust MDPs as well as model-free settings.

## 8 Acknowledgement

We thank the anonymous reviewers for their supportive comments. This work was supported, in part, by the CityU Start-Up Grant (Project No. 9610481), the National Natural Science Foundation of China (Project No. 72032005), and Chow Sang Sang Group Research Fund sponsored by Chow Sang Sang Holdings International Limited (Project No. 9229076).

## References

* Abdullah et al. (2019) Mohammed Amin Abdullah, Hang Ren, Haitham Bou Ammar, Vladimir Milenkovic, Rui Luo, Mingtian Zhang, and Jun Wang. Wasserstein robust reinforcement learning. _arXiv preprint arXiv:1907.13196_, 2019.
* Ambrosio and Gigli (2005) Luigi Ambrosio and Giuseppe Savare Nicola Gigli. _Gradient flows: in metric spaces and in the space of probability measures_. Springer Science & Business Media, 2005.
* Archibald et al. (1995) TW Archibald, KIM McKinnon, and LC Thomas. On the generation of markov decision processes. _Journal of the Operational Research Society_, 46(3):354-361, 1995.
* Badrinath and Kalathil (2021) Kishan Panaganti Badrinath and Dileep Kalathil. Robust reinforcement learning using least squares policy iteration with provable performance guarantees. In _International Conference on Machine Learning_, pages 511-520. PMLR, 2021.
* Behzadian et al. (2021) Bahram Behzadian, Marek Petrik, and Chin Pang Ho. Fast algorithms for \(l_{\infty}\)-constrained s-rectangular robust mdps. _Advances in Neural Information Processing Systems_, 34:25982-25992, 2021.
* Bertsekas (1999) Dimitri P. Bertsekas. _Nonlinear programming_. Athena Scientific Belmont. Massachusetts, USA, 1999.
* Bertsimas et al. (2018) Dimitris Bertsimas, Shimrit Shtern, and Bradley Sturt. A data-driven approach for multi-stage linear optimization. _Available at Optimization Online_, 2018.
* Bhatnagar et al. (2009) Shalabh Bhatnagar, Richard S Sutton, Mohammad Ghavamzadeh, and Mark Lee. Natural actor-critic algorithms. _Automatica_, 45(11):2471-2482, 2009.
* Brown et al. (2020) Daniel Brown, Scott Niekum, and Marek Petrik. Bayesian robust optimization for imitation learning. _Advances in Neural Information Processing Systems_, 33:2479-2491, 2020.
* Chen et al. (2019) Zhi Chen, Pengqian Yu, and William B Haskell. Distributionally robust optimization for sequential decision-making. _Optimization_, 68(12):2397-2426, 2019.
* Delage and Ye (2010) Erick Delage and Yinyu Ye. Distributionally robust optimization under moment uncertainty with application to data-driven problems. _Operations research_, 58(3):595-612, 2010.
* Delgado et al. (2016) Karina V Delgado, Leliane N De Barros, Daniel B Dias, and Scott Sanner. Real-time dynamic programming for markov decision processes with imprecise probabilities. _Artificial Intelligence_, 230:192-223, 2016.
* Derman and Mannor (2020) Esther Derman and Shie Mannor. Distributional robustness and regularization in reinforcement learning. _arXiv preprint arXiv:2003.02894_, 2020.
* Derman et al. (2021) Esther Derman, Matthieu Geist, and Shie Mannor. Twice regularized mdps and the equivalence between robustness and regularization. _Advances in Neural Information Processing Systems_, 34:22274-22287, 2021.
* Derman et al. (2020)Rui Gao and Anton Kleywegt. Distributionally robust stochastic optimization with wasserstein distance. _Mathematics of Operations Research_, 2022.
* Ghavamzadeh et al. (2016) Mohammad Ghavamzadeh, Marek Petrik, and Yinlam Chow. Safe policy improvement by minimizing robust baseline regret. _Advances in Neural Information Processing Systems_, 29, 2016.
* Givan et al. (2000) Robert Givan, Sonia Leach, and Thomas Dean. Bounded-parameter markov decision processes. _Artificial Intelligence_, 122(1-2):71-109, 2000.
* Goyal and Grand-Clement (2023) Vineet Goyal and Julien Grand-Clement. Robust markov decision processes: Beyond rectangularity. _Mathematics of Operations Research_, 48(1):203-226, 2023.
* Grand-Clement and Kroer (2021a) Julien Grand-Clement and Christian Kroer. First-order methods for wasserstein distributionally robust mdp. In _International Conference on Machine Learning_, pages 2010-2019. PMLR, 2021a.
* Grand-Clement and Kroer (2021b) Julien Grand-Clement and Christian Kroer. Scalable first-order methods for robust mdps. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 12086-12094, 2021b.
* Optimization (2023) Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2023. URL [https://www.gurobi.com](https://www.gurobi.com).
* Ho et al. (2018) Chin Pang Ho, Marek Petrik, and Wolfram Wiesemann. Fast bellman updates for robust mdps. In _International Conference on Machine Learning_, pages 1979-1988. PMLR, 2018.
* Ho et al. (2021) Chin Pang Ho, Marek Petrik, and Wolfram Wiesemann. Partial policy iteration for l1-robust markov decision processes. _The Journal of Machine Learning Research_, 22(1):12612-12657, 2021.
* Ho et al. (2022) Chin Pang Ho, Marek Petrik, and Wolfram Wiesemann. Robust phi-divergence mdps. _arXiv preprint arXiv:2205.14202_, 2022.
* Iyengar (2005) Garud N Iyengar. Robust dynamic programming. _Mathematics of Operations Research_, 30(2):257-280, 2005.
* Karmarkar (1984) Narendra Karmarkar. A new polynomial-time algorithm for linear programming. _Proceedings of the sixteenth annual ACM symposium on Theory of computing_, pages 302-311, 1984.
* Kaufman and Schaefer (2013) David L Kaufman and Andrew J Schaefer. Robust modified policy iteration. _INFORMS Journal on Computing_, 25(3):396-410, 2013.
* Kumar et al. (2022) Navdeep Kumar, Kfir Levy, Kaixin Wang, and Shie Mannor. Efficient policy iteration for robust markov decision processes via regularization. _arXiv preprint arXiv:2205.14327_, 2022.
* Tallec (2007) Yann Le Tallec. _Robust, risk-sensitive, and data-driven control of Markov decision processes_. PhD thesis, Massachusetts Institute of Technology, 2007.
* Li et al. (2022) Yan Li, Tuo Zhao, and Guanghui Lan. First-order policy optimization for robust markov decision process. _arXiv preprint arXiv:2209.10579_, 2022.
* Liu et al. (2022) Zijian Liu, Qinxun Bai, Jose Blanchet, Perry Dong, Wei Xu, Zhengqing Zhou, and Zhengyuan Zhou. Distributionally robust \(q\)-learning. In _International Conference on Machine Learning_, pages 13623-13643. PMLR, 2022.
* Lobo et al. (2020) Elita A Lobo, Mohammad Ghavamzadeh, and Marek Petrik. Soft-robust algorithms for batch reinforcement learning. _arXiv preprint arXiv:2011.14495_, 2020.
* Mankowitz et al. (2019) Daniel J Mankowitz, Nir Levine, Rae Jeong, Yuanyuan Shi, Jackie Kay, Abbas Abdolmaleki, Jost Tobias Springenberg, Timothy Mann, Todd Hester, and Martin Riedmiller. Robust reinforcement learning for continuous control with model misspecification. _arXiv preprint arXiv:1906.07516_, 2019.
* Mannor et al. (2016) Shie Mannor, Ofir Mebel, and Huan Xu. Robust mdps with k-rectangular uncertainty. _Mathematics of Operations Research_, 41(4):1484-1509, 2016.
* Mankowitz et al. (2019)* Sattan and Barto (2018) David Sattan and David Sattan. _Theoretical and Computational Statistics_. Springer, 2018.

[MISSING_PAGE_POST]

* Yang (2017) Insoon Yang. A convex optimization approach to distributionally robust markov decision processes with wasserstein distance. _IEEE control systems letters_, 1(1):164-169, 2017.
* Yu and Xu (2015) Pengqian Yu and Huan Xu. Distributionally robust counterpart in markov decision processes. _IEEE Transactions on Automatic Control_, 61(9):2538-2543, 2015.
* Zhang et al. (2020) Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, and Cho-Jui Hsieh. Robust deep reinforcement learning against adversarial perturbations on state observations. _Advances in Neural Information Processing Systems_, 33:21024-21037, 2020.

Appendix: Proofs and Algorithms

### Proofs of results in Section 4

Proof of Proposition 4.1.: Plug \(\mathbb{B}_{s}^{q}\) in (3) into (4), and apply the minimax theorem, the original problem \([\mathfrak{T}(\mathbf{v})]_{s},\ \forall s\in\mathcal{S}\) is given by:

\[\max_{\mathbf{\pi}_{s}\in\Delta_{A}} \min_{\mathbf{p}^{i}_{s},\cdots,\mathbf{p}^{N}_{s}}\sum_{a\in\mathcal{A}} \pi_{sa}\left(\frac{1}{N}\sum_{i=1}^{N}\mathbf{p}^{i}_{sa}\right)^{\top}(\mathbf{r}_{ sa}+\lambda\mathbf{v})\] \[\mathrm{s.t.} \frac{1}{N}\sum_{i=1}^{N}\left\|\mathbf{p}^{i}_{s}-\hat{\mathbf{p}}^{i}_{ s}\right\|^{q}_{q}\leqslant\theta^{q},\] \[\mathbf{p}^{i}_{sa}\in\Delta_{S},\ \forall i\in[N],\ \forall a\in\mathcal{A}\] \[= \min \max_{\mathbf{\pi}_{s}\in\Delta_{A}}\frac{1}{N}\sum_{a\in\mathcal{A}} \pi_{sa}\sum_{i=1}^{N}\left(\mathbf{r}_{sa}+\lambda\mathbf{v}\right)^{\top}\mathbf{p}^{i}_ {sa}\] \[\mathrm{s.t.} \frac{1}{N}\sum_{i=1}^{N}\left\|\mathbf{p}^{i}_{s}-\hat{\mathbf{p}}^{i}_ {s}\right\|^{q}_{q}\leqslant\theta^{q},\] \[\mathbf{p}^{i}_{sa}\in\Delta_{S},\ \forall i\in[N],\ \forall a\in \mathcal{A}\] \[= \min \max_{a\in\mathcal{A}}\frac{1}{N}\sum_{i=1}^{N}\left(\mathbf{r}_{sa} +\lambda\mathbf{v}\right)^{\top}\mathbf{p}^{i}_{sa}\] \[\mathrm{s.t.} \frac{1}{N}\sum_{i=1}^{N}\sum_{a\in\mathcal{A}}\left\|\mathbf{p}^{i}_ {s}-\hat{\mathbf{p}}^{i}_{s}\right\|^{q}_{q}\leqslant\theta^{q},\] \[\mathbf{p}^{i}_{sa}\in\Delta_{S},\ \forall i\in[N],\ \forall a\in \mathcal{A}.\]

Hence (5) is a direct consequence of above formulation by introducing the following epigraph variable \(\gamma\) which satisfies \(\gamma\geq\max_{a\in\mathcal{A}}\frac{1}{N}\sum_{i=1}^{N}\left(\mathbf{r}_{ sa}+\lambda\mathbf{v}\right)^{\top}\mathbf{p}^{i}_{sa}\). 

Proof of Proposition 4.2.: To prove the upper bound of \(\gamma^{\star}\), we consider \(\bar{\gamma}=\max_{a\in\mathcal{A}}\frac{1}{N}\sum_{i=1}^{N}\mathbf{b}^{\top}_{sa }\hat{\mathbf{p}}^{i}_{sa}\), which equals to \(\bar{\gamma}\geq\frac{1}{N}\sum_{i=1}^{N}\mathbf{b}^{\top}_{sa}\hat{\mathbf{p}}^{i}_{ sa},\ \ \forall a\in\mathcal{A}\). This implies \(\{\hat{\mathbf{p}}^{i}_{sa}\}_{i=1}^{N}\) satisfies every constraint in problem (6) with the lowest possible objective value \(0\), for every \(a\in\mathcal{A}\). Therefore, \(\sum_{a\in\mathcal{A}}\mathfrak{P}\left(\{\hat{\mathbf{p}}^{i}_{sa}\}_{i=1}^{N}; \mathbf{r}_{sa}+\lambda\mathbf{v},\bar{\gamma}\right)=0<\theta^{q}\). At this time, \(\bar{\gamma}\) is feasible for (5). Hence, we provide an upper bound \(\bar{\gamma}=\max_{a\in\mathcal{A}}\frac{1}{N}\sum_{i=1}^{N}\mathbf{b}^{\top}_{ sa}\hat{\mathbf{p}}^{i}_{sa}\).

To prove the lower bound of \(\gamma^{\star}\), we assume the contrary \(\max_{a\in\mathcal{A}}\left\{\min\left\{\mathbf{b}_{sa}\right\}\right\}>\gamma^{\star}\). So there exists \(\hat{a}\in\mathcal{A}\) with \(\gamma^{\star}<\min\left\{\mathbf{b}_{sa}\right\}\). Then there is no \(\mathbf{p}^{i}_{sa}\) satisfies the first constraint in (5) for \(a=\hat{a}\). This contradiction verifies the lower bound \(\underline{\gamma}=\max_{a\in\mathcal{A}}\left\{\min\left\{\mathbf{b}_{sa}\right\}\right\}\).

Proof of Proposition 4.3.: By definition, for any fixed \(s\in\mathcal{S}\) and \(a\in\mathcal{A}\), we have

\[\mathfrak{P}\left(\{\hat{\mathbf{p}}^{i}_{sa}\}_{i=1}^{N};\mathbf{b}_{ sa},\gamma\right) = \min\quad\frac{1}{N}\sum_{i=1}^{N}\left\|\mathbf{p}^{i}_{sa}-\hat{\bm {p}}^{i}_{sa}\right\|^{q}_{q}\] \[\mathrm{s.t.} \frac{1}{N}\sum_{i=1}^{N}\mathbf{b}^{\top}_{sa}\mathbf{p}^{i}_{sa}\leq\gamma,\] \[\mathbf{p}^{i}_{sa}\in\Delta_{S},\ \forall i\in[N]\,.\]

Then, we introduce the dual variable \(\alpha\geq 0\) for the constraint \(\frac{1}{N}\sum_{i=1}^{N}\mathbf{b}^{\top}_{sa}\mathbf{p}^{i}_{sa}\leq\gamma\) and obtain

\[\mathfrak{P}\left(\{\hat{\mathbf{p}}^{i}_{sa}\}_{i=1}^{N};\mathbf{b}_{ sa},\gamma\right) = \max_{\alpha\geq 0}\ \left\{\min_{\mathbf{p}^{i}_{sa}\in\Delta_{S},\ \forall i\in[N]}\frac{1}{N}\sum_{i=1}^{N}\left\|\mathbf{p}^{i}_{sa}-\hat{\bm {p}}^{i}_{sa}\right\|^{q}_{q}+\alpha\left(\frac{1}{N}\sum_{i=1}^{N}\mathbf{b}^{ \top}_{sa}\mathbf{p}^{i}_{sa}-\gamma\right)\right\}\] \[= \max_{\alpha\geq 0}\ -\alpha\gamma+\frac{1}{N}\sum_{i=1}^{N}\mathfrak{D}_{q}(\hat{\mathbf{p}}^{i}_{ sa},\mathbf{b}_{sa},\alpha),\]where \(\mathfrak{D}_{q}(\hat{\mathbf{p}}^{i}_{sa},\mathbf{b}_{sa},\alpha)\) is defined in the Proposition 4.3.

To show \(\bar{\alpha}\) defined in the proposition is indeed an upper bound of the optimal \(\alpha^{\star}\), we denote

\[f(\alpha)=-\alpha\gamma+\frac{1}{N}\sum_{i=1}^{N}\mathfrak{D}_{q}(\hat{\mathbf{p}}^{ i}_{sa},\mathbf{b}_{sa},\alpha).\]

Notice that \(\forall\alpha\geq\max_{i\in[N]}\frac{\left\|\mathbf{e}_{j}-\hat{\mathbf{p}}^{i}_{sa} \right\|_{q}^{q}}{\gamma-\min\{\mathbf{b}_{sa}\}},\) where \(j\in\operatorname*{arg\,min}_{s^{\prime}\in\mathcal{S}}\) is \(b_{sa^{\prime}}:\)

\[f(\alpha) \leq-\alpha\gamma+\frac{1}{N}\sum_{i=1}^{N}\left\|\mathbf{e}_{j}-\hat {\mathbf{p}}^{i}_{sa}\right\|_{q}^{q}+\alpha\mathbf{b}^{\top}_{sa}\mathbf{e}_{j}\] \[=\frac{1}{N}\sum_{i=1}^{N}\left\|\mathbf{e}_{j}-\hat{\mathbf{p}}^{i}_{sa }\right\|_{q}^{q}+\left(\min\left\{\mathbf{b}_{sa}\right\}-\gamma\right)\alpha\] \[\leq 0=f(0),\]

where the first inequality is from the definition of \(\mathfrak{D}_{q}(\hat{\mathbf{p}}^{i}_{sa},\mathbf{b}_{sa},\alpha)\), the second equality is due to the selection of \(j\), and the last inequality is due to the selection of \(\alpha\). Hence \(\alpha^{\star}\in[0,\bar{\alpha}]\) by \(f(\alpha)\) is concave w.r.t. \(\alpha\) (otherwise \(\exists\alpha^{\star}>\bar{\alpha}\) such that \(f(\alpha^{\star})>f(0)\geq f(\bar{\alpha})\), where nonconcavity of \(f\) follows), and (8) is true. One can further compute the subdifferential of \(f(\alpha)\) by Danskin's theorem (Bertsekas, 1999). Typically,

\[-\gamma+\frac{1}{N}\sum_{i=1}^{N}\mathbf{b}^{\top}_{sa}\mathbf{p}^{i,\star}_{sa}\in \partial f(\alpha),\]

where \(\mathbf{p}^{i,\star}_{sa}\) is any minimizer of the inner minimization problem corresponding with given \(\alpha\geq 0\). 

Proof of Theorem 4.4.: For simplicity of notations, throughout this proof we use

\[\delta\triangleq\frac{A\epsilon_{2}}{2}\left(\max_{a\in\mathcal{A}}\max\{\mathbf{b }_{sa}\}+\bar{\gamma}\right)\qquad\text{and}\qquad f(\gamma)\triangleq\sum_{ a\in\mathcal{A}}\mathfrak{P}\left(\{\hat{\mathbf{p}}^{i}_{sa}\}_{i=1}^{N};\mathbf{b}_{sa}, \gamma\right),\quad\forall\gamma\in[\underline{\gamma},\bar{\gamma}].\]

By (6), we can get that \(\mathfrak{P}\left(\{\hat{\mathbf{p}}^{i}_{sa}\}_{i=1}^{N};\mathbf{b}_{sa},\gamma\right)\) and \(f(\gamma)\) are non-increasing in \([\underline{\gamma},\bar{\gamma}]\). For each given \(\gamma\in[\underline{\gamma},\bar{\gamma}]\) and \(a\in\mathcal{A}\), we denote \(\mathfrak{P}\left(\{\hat{\mathbf{p}}^{i}_{sa}\}_{i=1}^{N};\mathbf{b}_{sa},\gamma\right)\) the corresponding value calculated by the Algorithm 1. Furthermore, we call

\[\hat{f}(\gamma)\triangleq\sum_{a\in\mathcal{A}}\hat{\mathfrak{P}}\left(\{\hat{ \mathbf{p}}^{i}_{sa}\}_{i=1}^{N};\mathbf{b}_{sa},\gamma\right),\quad\forall\gamma\in[ \underline{\gamma},\bar{\gamma}].\]

We can show that

\[|f(\gamma)-\hat{f}(\gamma)|\leq\delta,\quad\forall\gamma\in[\underline{\gamma },\bar{\gamma}]. \tag{14}\]

We fix \(\gamma\in[\underline{\gamma},\bar{\gamma}]\), and consider (8). Algorithm 1 provides the optimal solution for (8) with tolerance \(\epsilon_{2}/2\), so we have \(|\alpha^{\star}_{sa,\gamma}-\hat{\alpha}_{sa,\gamma}|\leq\epsilon_{2}/2\), where \(\alpha^{\star}_{sa,\gamma}\) is the true optimal solution of (8) and \(\hat{\alpha}_{sa,\gamma}\) is the solution computed in the inner bisection in Algorithm 1. Thus we get

\[\left|\mathfrak{P}\left(\{\hat{\mathbf{p}}^{i}_{sa}\}_{i=1}^{N};\mathbf{b} _{sa},\gamma\right)-\hat{\mathfrak{P}}\left(\{\hat{\mathbf{p}}^{i}_{sa}\}_{i=1}^{N}; \mathbf{b}_{sa},\gamma\right)\right|\] \[= -\alpha^{\star}_{sa,\gamma}\gamma+\frac{1}{N}\sum_{i=1}^{N} \mathfrak{D}_{q}(\hat{\mathbf{p}}^{i}_{sa},\mathbf{b}_{sa},\alpha^{\star}_{sa,\gamma}) -\left(-\hat{\alpha}_{sa,\gamma}\gamma+\frac{1}{N}\sum_{i=1}^{N}\mathfrak{D}_ {q}(\hat{\mathbf{p}}^{i}_{sa},\hat{\alpha}_{sa,\gamma})\right)\] \[\leq \left|\alpha^{\star}_{sa,\gamma}-\hat{\alpha}_{sa,\gamma}\right| \gamma+\frac{1}{N}\sum_{i=1}^{N}\left|\mathfrak{D}_{q}(\hat{\mathbf{p}}^{i}_{sa}, \mathbf{b}_{sa},\alpha^{\star}_{sa,\gamma})-\mathfrak{D}_{q}(\hat{\mathbf{p}}^{i}_{sa},\mathbf{b}_{sa},\hat{\alpha}_{sa,\gamma})\right|\] \[\leq \frac{\epsilon_{2}}{2}\left(\bar{\gamma}+\max_{a\in\mathcal{A}} \max\{\mathbf{b}_{sa}\}\right),\]where the last step is due to that

\[\mathfrak{D}_{q}(\hat{\mathbf{p}}^{i}_{sa},\mathbf{b}_{sa},\alpha^{\star}_{ sa,\gamma})-\mathfrak{D}_{q}(\hat{\mathbf{p}}^{i}_{sa},\mathbf{b}_{sa},\hat{\alpha}_{sa,\gamma})\] \[= \min_{\mathbf{p}^{i}_{sa}\in\Delta_{S}}\left\|\mathbf{p}^{i}_{sa}-\hat{\bm {p}}^{i}_{sa}\right\|^{q}_{q}+\alpha^{\star}_{sa,\gamma}\cdot\mathbf{b}^{\top}_{ sa}\mathbf{p}^{i}_{sa}-\left(\min_{\mathbf{p}^{i}_{sa}\in\Delta_{S}}\left\|\mathbf{p}^{i}_{sa}- \hat{\mathbf{p}}^{i}_{sa}\right\|^{q}_{q}+\hat{\alpha}_{sa,\gamma}\cdot\mathbf{b}^{ \top}_{sa}\mathbf{p}^{i}_{sa}\right)\] \[\leq \left(\alpha^{\star}_{sa,\gamma}-\hat{\alpha}_{sa,\gamma}\right) \mathbf{b}^{\top}_{sa}\mathbf{p}^{i,\hat{\alpha}}_{sa}\] \[\leq \frac{\epsilon_{2}}{2}\max_{a\in\mathcal{A}}\max\{\mathbf{b}_{sa}\},\]

and

\[\mathfrak{D}_{q}(\hat{\mathbf{p}}^{i}_{sa},\mathbf{b}_{sa},\hat{\alpha}_{ sa,\gamma})-\mathfrak{D}_{q}(\hat{\mathbf{p}}^{i}_{sa},\mathbf{b}_{sa},\alpha^{\star}_{ sa,\gamma})\] \[= \min_{\mathbf{p}^{i}_{sa}\in\Delta_{S}}\left\|\mathbf{p}^{i}_{sa}-\hat{\bm {p}}^{i}_{sa}\right\|^{q}_{q}+\hat{\alpha}_{sa,\gamma}\cdot\mathbf{b}^{\top}_{sa} \mathbf{p}^{i}_{sa}-\left(\min_{\mathbf{p}^{i}_{sa}\in\Delta_{S}}\left\|\mathbf{p}^{i}_{sa} -\hat{\mathbf{p}}^{i}_{sa}\right\|^{q}_{q}+\alpha^{\star}\cdot\mathbf{b}^{\top}_{sa}\bm {p}^{i}_{sa}\right)\] \[\leq \left(\hat{\alpha}_{sa,\gamma}-\alpha^{\star}_{sa,\gamma}\right) \mathbf{b}^{\top}_{sa}\mathbf{p}^{i,\alpha^{\star}}_{sa}\] \[\leq \frac{\epsilon_{2}}{2}\max_{a\in\mathcal{A}}\max\{\mathbf{b}_{sa}\},\]

here \(\mathbf{p}^{i,\hat{\alpha}}_{sa}\) and \(\mathbf{p}^{i,\alpha^{\star}}_{sa}\) are the optimal solutions to (9) for \(\alpha=\hat{\alpha}_{sa,\gamma}\) and \(\alpha=\alpha^{\star}_{sa,\gamma}\) respectively.

Hence, (14) is the direct consequence of above estimation, together with the definitions of \(f,\hat{f}\) and \(\delta\). And we have \(\gamma^{\star}=\inf\{\gamma\in[\underline{\gamma},\bar{\gamma}]:f(\gamma)\leq \theta^{q}\}\). We further define \(\hat{\gamma}\triangleq\inf\{\gamma\in[\underline{\gamma},\bar{\gamma}]:\hat{f} (\gamma)\leq\theta^{q}\}\). The outer bisection of Algorithm 1 implies that \(|\gamma^{\prime}-\hat{\gamma}|\leq\frac{\epsilon_{1}}{2}\), so to prove the claimed result in the theorem, it suffices to show that

\[|\gamma^{\star}-\hat{\gamma}|\leq\frac{2\delta\left(\max_{a\in\mathcal{A}}\max \{\mathbf{b}_{sa}\}-\underline{\gamma}\right)}{\theta^{q}}. \tag{15}\]

Notice that \(\widehat{\mathfrak{P}}\left(\{\mathbf{p}^{i}_{sa}\}_{i=1}^{N};\mathbf{b}_{sa},\gamma \right)\leq\mathfrak{P}\left(\{\mathbf{p}^{i}_{sa}\}_{i=1}^{N};\mathbf{b}_{sa},\gamma \right),\forall\gamma\in[\underline{\gamma},\bar{\gamma}]\) by definitions, we get \(\hat{f}(\gamma)\leq f(\gamma)\), so \(\{\gamma\in[\underline{\gamma},\bar{\gamma}]:f(\gamma)\leq\theta^{q}\} \subseteq\{\gamma\in[\underline{\gamma},\bar{\gamma}]:\hat{f}(\gamma)\leq \theta^{q}\}\), hence we get \(\hat{\gamma}\leq\gamma^{\star}\).

We assume that \(\gamma^{\star}-\underline{\gamma}\geq\frac{2\delta\left(\max_{a\in\mathcal{A}} \max\{\mathbf{b}_{sa}\}-\underline{\gamma}\right)}{\theta^{q}}\), otherwise (15) is trivially satisfied. To achieve (15), we claim the following statement:

\[f\left(\gamma^{\star}-\frac{2\delta\left(\max_{a\in\mathcal{A}}\max\{\mathbf{b}_{ sa}\}-\underline{\gamma}\right)}{\theta^{q}}\right)>\theta^{q}+\delta. \tag{16}\]

If the statement is true, we have \(\forall\gamma\in[\underline{\gamma},\gamma^{\star}-\left(\max_{a\in\mathcal{A} }\max\{\mathbf{b}_{sa}\}-\underline{\gamma}\right)(2\delta/\theta^{q})]\):

\[\hat{f}(\gamma)\geq f(\gamma)-\delta\geq f\left(\gamma^{\star}-\frac{2\delta \left(\max_{a\in\mathcal{A}}\max\{\mathbf{b}_{sa}\}-\underline{\gamma}\right)}{ \theta^{q}}\right)-\delta>\theta^{q},\]

where the first inequality is from (14) and \(\hat{f}(\gamma)\leq f(\gamma)\), the second inequality is due to that \(f(\gamma)\) is non-increasing, and the third inequality is from the above statement.

So \(\{\gamma\in[\underline{\gamma},\bar{\gamma}]:\hat{f}(\gamma)\leq\theta^{q}\} \subseteq(\gamma^{\star}-\left(\max_{a\in\mathcal{A}}\max\{\mathbf{b}_{sa}\}- \underline{\gamma}\right)(2\delta/\theta^{q}),\bar{\gamma}]\), thus

\[\gamma^{\star}-\frac{2\delta\left(\max_{a\in\mathcal{A}}\max\{\mathbf{b}_{sa}\}- \underline{\gamma}\right)}{\theta^{q}}\leq\hat{\gamma}\leq\gamma^{\star},\]

which implies the desired (15).

For the proof of the statement (16). We argue that

\[\forall\gamma\in[\gamma^{\star}-\frac{2\delta\left(\max_{a\in\mathcal{A}}\max\{ \mathbf{b}_{sa}\}-\underline{\gamma}\right)}{\theta^{q}},\gamma^{\star}):\;\sum_{a \in\mathcal{A}}\alpha^{\star}_{sa,\gamma}\geq\frac{\theta^{q}}{\left(\max_{a \in\mathcal{A}}\max\{\mathbf{b}_{sa}\}-\underline{\gamma}\right)}, \tag{17}\]by contradiction. Assume there is some \(\gamma^{\prime\prime}\in[\gamma^{\star}-\frac{2\delta\left(\max_{a\in\mathcal{A}} \max\{\mathbf{b}_{sa}\}-\underline{\gamma}\right)}{\theta^{q}},\gamma^{\star})\) with that \(\sum_{a\in\mathcal{A}}\alpha^{\star}_{sa,\gamma^{\prime\prime}}<\frac{\theta^{ q}}{\left(\max_{a\in\mathcal{A}}\max\{\mathbf{b}_{sa}\}-\underline{\gamma}\right)}\). Clearly \(f(\gamma^{\prime\prime})>\theta^{q}\) since \(\gamma^{\prime\prime}<\gamma^{\star}\). Then

\[\theta^{q} >\sum_{a\in\mathcal{A}}\alpha^{\star}_{sa,\gamma^{\prime\prime}} \left(\max_{a\in\mathcal{A}}\max\{\mathbf{b}_{sa}\}-\underline{\gamma}\right)\] \[\geq\sum_{a\in\mathcal{A}}\alpha^{\star}_{sa,\gamma^{\prime\prime} }\left(\frac{1}{N}\sum_{i=1}^{N}\left(\mathbf{b}_{sa}^{\top}\hat{\mathbf{p}}_{sa}^{i}- \gamma\right)\right)\] \[=\sum_{a\in\mathcal{A}}\frac{1}{N}\sum_{i=1}^{N}\alpha^{\star}_{ sa,\gamma^{\prime\prime}}\left(\mathbf{b}_{sa}^{\top}\hat{\mathbf{p}}_{sa}^{i}- \gamma\right)\] \[\geq\sum_{a\in\mathcal{A}}\frac{1}{N}\sum_{i=1}^{N}\min_{\mathbf{p}_{ sa}^{i}\in\Delta_{S}}\left\|\mathbf{p}_{sa}^{i}-\hat{\mathbf{p}}_{sa}^{i}\right\|_{q}^{q}+ \alpha^{\star}_{sa,\gamma^{\prime\prime}}\cdot\left(\mathbf{b}_{sa}^{\top}\mathbf{p}_{ sa}^{i}-\gamma\right)\] \[=\sum_{a\in\mathcal{A}}-\alpha^{\star}_{sa,\gamma^{\prime\prime}} \gamma^{\prime\prime}+\frac{1}{N}\sum_{i=1}^{N}\mathfrak{D}_{q}(\hat{\mathbf{p}}_{ sa}^{i},\mathbf{b}_{sa},\alpha^{\star}_{sa,\gamma^{\prime\prime}})\] \[=\sum_{a\in\mathcal{A}}\mathfrak{P}\left(\{\hat{\mathbf{p}}_{sa}^{i} \}_{i=1}^{N};\mathbf{b}_{sa},\gamma^{\prime\prime}\right)\] \[=f(\gamma^{\prime\prime}),\]

which is contradicted with \(f(\gamma^{\prime\prime})>\theta^{q}\). This contradiction implies that (17) is true.

For simplicity of notations, we call \(\gamma^{\prime\prime\prime}=\gamma^{\star}-\frac{2\delta\left(\max_{a\in \mathcal{A}}\max\{\mathbf{b}_{sa}\}-\underline{\gamma}\right)}{\theta^{q}}\), and we fix any \(\gamma^{\ell}\in(\gamma^{\prime\prime\prime},\gamma^{\star})\), then

\[\theta^{q}-f\left(\gamma^{\star}-\frac{2\delta\left(\max_{a\in \mathcal{A}}\max\{\mathbf{b}_{sa}\}-\underline{\gamma}\right)}{\theta^{q}}\right)\] \[< f(\gamma^{\ell})-f\left(\gamma^{\prime\prime\prime}\right)\] \[= \sum_{a\in\mathcal{A}}\left(\mathfrak{P}\left(\{\hat{\mathbf{p}}_{sa }^{i}\}_{i=1}^{N};\mathbf{b}_{sa},\gamma^{\ell}\right)-\mathfrak{P}\left(\{\hat{ \mathbf{p}}_{sa}^{i}\}_{i=1}^{N};\mathbf{b}_{sa},\gamma^{\prime\prime\prime}\right)\right)\] \[\leq \sum_{a\in\mathcal{A}}-\alpha^{\star}_{sa,\gamma^{\prime}}\gamma^ {\ell}+\frac{1}{N}\sum_{i=1}^{N}\mathfrak{D}_{q}(\hat{\mathbf{p}}_{sa}^{i},\mathbf{b} _{sa},\alpha^{\star}_{sa,\gamma^{\ell}})-\left(-\alpha^{\star}_{sa,\gamma^{ \prime}}\gamma^{\prime\prime\prime}+\frac{1}{N}\sum_{i=1}^{N}\mathfrak{D}_{q} (\hat{\mathbf{p}}_{sa}^{i},\mathbf{b}_{sa},\alpha^{\star}_{sa,\gamma^{\ell}})\right)\] \[= \sum_{a\in\mathcal{A}}\alpha^{\star}_{sa,\gamma^{\prime}}(\gamma^ {\prime\prime\prime}-\gamma^{\ell})\] \[\leq (\gamma^{\prime\prime\prime}-\gamma^{\ell})\frac{\theta^{q}}{ \left(\max_{a\in\mathcal{A}}\max\{\mathbf{b}_{sa}\}-\underline{\gamma}\right)},\]

where the first step is due to \(\gamma^{\ell}<\gamma^{\star}\) and definition of \(\gamma^{\prime\prime\prime}\), the second step is from the definition of function \(f\), the third step is from the definition of \(\mathfrak{P}(\{\hat{\mathbf{p}}_{sa}^{i}\}_{i=1}^{N};\mathbf{b}_{sa},\gamma)\) and \(\alpha^{\star}_{sa,\gamma^{\prime}}\), and the last step is due to (17) and \(\gamma^{\ell}\in(\gamma^{\prime\prime\prime},\gamma^{\star})\). Notice that above inequality is true for all \(\gamma^{\ell}\in(\gamma^{\prime\prime\prime},\gamma^{\star})\), we could let \(\gamma^{\ell}\rightarrow\gamma^{\star}\), which leads to

\[\theta^{q}-f\left(\gamma^{\star}-\frac{2\delta\left(\max_{a\in\mathcal{A}}\max \{\mathbf{b}_{sa}\}-\underline{\gamma}\right)}{\theta^{q}}\right)\leq-2\delta<-\delta.\]

Then (16) is the direct consequence of above inequality. This finishes the proof of statement, hence finishes the proof of the theorem. 

Proof of Theorem 4.5.: The Algorithm 1 is the direct consequence of the procedures in the content, except computing the slope, which has been explained at the end of the proof of Proposition 4.3.

For the time complexity, we can see that the bisection method on \(\gamma\) and \(\alpha\) uses complexity \(\mathcal{O}(\log\epsilon_{1}^{-1}\log\epsilon_{2}^{-1})\). For the subproblem (9), which costs time complexity \(h_{q}(S)\), we need to solve 

[MISSING_PAGE_EMPTY:18]

W.L.O.G., we assume that \(b_{sa1}>b_{sa2}>\cdots>b_{saS}\), and \(\hat{\mathbf{p}}_{sa}^{i}>\mathbf{0}\). We claim that

\[F(v)=\left\{\begin{array}{ll}v+\alpha\left(-\dfrac{v\left(b_{ sa1}-b_{saS}\right)}{2}+\mathbf{b}_{sa}^{\top}\hat{\mathbf{p}}_{sa}^{i}\right)&\text{ if }v\in\left[0,2\hat{p}_{sa1}^{i}\right),\\ v+\alpha\sum_{k=K+1}^{S}r_{k}b_{sak}&\text{ if }v\in\left[2\sum_{k=1}^{K}\hat{p}_{ sak}^{i},2\sum_{k=1}^{K+1}\hat{p}_{sak}^{i}\right),\text{ for some }K\in[S-2],\\ v+\alpha b_{saS}&\text{ if }v\in\left[2\sum_{k=1}^{S-1}\hat{p}_{sak}^{i},+ \infty\right),\end{array}\right.\]

where

\[r_{k}=\left\{\begin{array}{ll}\hat{p}_{sa(K+1)}^{i}-\left(v-2\sum_{k=1}^{K} \hat{p}_{sak}^{i}\right)\bigg{/}2&\text{ if }k=K+1,\\ \hat{p}_{sak}^{i}&\text{ if }K+2\leq k\leq S-1,\\ \hat{p}_{saS}^{i}+\dfrac{v}{2}&\text{ if }k=S.\end{array}\right.\]

To prove the first case of the claim, it suffices to show that the optimal solution for the minimization problem in \(F(v)\) is given by \(\mathbf{p}^{\star}\), whose components are \(p_{1}^{\star}=\hat{p}_{sa1}^{i}-\frac{v}{2},p_{k}^{\star}=\hat{p}_{sak}^{i},\ \forall 2\leq k\leq S-1\) and \(p_{S}^{\star}=\hat{p}_{saS}^{i}+\frac{v}{2}\). It can be easily verified that \(\mathbf{p}^{\star}\) defined in this way satisfies the constraints of the minimization problem in \(F(v)\).

To see the optimality, we consider any optimal solution \(\bar{\mathbf{p}}\). We first notice that \(\bar{p}_{1}>0\) and \(\bar{p}_{S}<1\). Actually, Let

\[\mathcal{N} =\{k\in[S]\,:\,\bar{p}_{k}<\hat{p}_{sak}^{i}\},\] \[\mathcal{P} =\{k\in[S]\,:\,\bar{p}_{k}>\hat{p}_{sak}^{i}\},\] \[\mathcal{E} =\{k\in[S]\,:\,\bar{p}_{k}=\hat{p}_{sak}^{i}\}.\]

Then by \(\mathbf{e}^{\top}\bar{\mathbf{p}}=\mathbf{e}^{\top}\hat{\mathbf{p}}_{sa}^{i}=1\) and \(\|\bar{\mathbf{p}}-\hat{\mathbf{p}}_{sa}^{i}\|_{1}\leq v\), we get

\[\sum_{k\in\mathcal{N}}\hat{p}_{sak}^{i}-\bar{p}_{k}=\sum_{k\in \mathcal{P}}\bar{p}_{k}-\hat{p}_{sak}^{i}\] \[\sum_{k\in\mathcal{N}}(\hat{p}_{sak}^{i}-\bar{p}_{k})+\sum_{k\in \mathcal{P}}(\bar{p}_{k}-\hat{p}_{sak}^{i})\leq v\]

Hence \(\sum_{k\in\mathcal{N}}\hat{p}_{sak}^{i}-\bar{p}_{k}=\sum_{k\in\mathcal{P}}\bar {p}_{k}-\hat{p}_{sak}^{i}\leq\frac{v}{2}<\hat{p}_{sa1}^{i}\), so we get \(\bar{p}_{1}>0\) and \(\bar{p}_{S}<1\).

Next we show that \(\bar{p}_{k}=\hat{p}_{sak}^{i},\ \forall 2\leq k\leq S-1\). Otherwise we have some \(2\leq\hat{k}\leq S-1\) such that \(|\bar{p}_{k}-\hat{p}_{sak}^{i}|>0\). If \(\bar{p}_{k}>\hat{p}_{sak}^{i}\), we define \(\tilde{\mathbf{p}}\) with that \(\tilde{p}_{k}=\bar{p}_{k}-\varepsilon\) and \(\tilde{p}_{S}=\bar{p}_{S}+\varepsilon\), here \(0<\varepsilon<\min\{|\frac{\bar{p}_{k}-\hat{p}_{sak}^{i}}{2}|,\frac{1}{2}-\frac {\bar{p}_{S}}{2}\}\), while keeping the other components of \(\tilde{\mathbf{p}}\) same as \(\bar{\mathbf{p}}\). We can see that \(\tilde{\mathbf{p}}\) achieves smaller objective value than \(\bar{\mathbf{p}}\) does due to \(b_{sak}>b_{saS}\), which contradicts the optimality of \(\bar{\mathbf{p}}\). If \(\bar{p}_{k}<\hat{p}_{sak}^{i}\), then we define \(\tilde{\mathbf{p}}\) with that \(\tilde{p}_{k}=\tilde{p}_{k}+\varepsilon\) and \(\tilde{p}_{1}=\bar{p}_{1}-\varepsilon\), here \(0<\varepsilon<\min\{|\frac{\bar{p}_{k}-\hat{p}_{sak}^{i}}{2}|,\frac{\bar{p}_ {1}}{2}\}\), while keeping the other components of \(\tilde{\mathbf{p}}\) same as \(\bar{\mathbf{p}}\). Similarly, \(\tilde{\mathbf{p}}\) achieves smaller objective value and this implies the contradiction.

Finally, we verify the rest two components. We introduce the variables \(d_{1}=\bar{p}_{1}-\hat{p}_{sa1}^{i}\) and \(d_{S}=\bar{p}_{S}-\hat{p}_{saS}^{i}\), then we get the equivalent reformulation of the inner minimization in \(F(v)\):

\[\begin{array}{ll}\min_{d_{1},d_{S}}&b_{sa1}d_{1}+b_{saS}d_{S}\\ \mathrm{s.t.}&d_{1}+d_{S}=0,|d_{1}|+|d_{S}|\leq v.\end{array}\]

The optimal \(d_{1}\) and \(d_{S}\) are given by \(-v/2\) and \(v/2\) respectively. Hence we proved \(\mathbf{p}^{\star}\) is indeed an optimal solution.

To prove the second case of the claim, the optimal solution for the minimization problem in \(F(v)\) is given by \(\mathbf{p}^{\star}\), whose components are \(p_{k}^{\star}=0,\ \forall k\in[K]\), and \(p_{k}^{\star}=r_{k},\ \forall K+1\leq k\leq S\). The decision variables \(\mathbf{p}^{\star}\) defined in this way satisfies the constraints of the minimization problem in \(F(v)\).

To see the optimality, we consider any optimal solution \(\bar{\mathbf{p}}\). We first notice that \(\bar{p}_{K+1}>0\) and \(\bar{p}_{S}<1\).

To argue this by contradiction, we suppose the contrary \(\bar{p}_{K+1}=0\). Define the notations \(\mathcal{N},\mathcal{P}\) and \(\mathcal{E}\) same as before. Then there exists \(\hat{k}\leq K\) with \(\bar{p}_{\hat{k}}>0\), otherwise we assume that \(\bar{p}_{k}=0,\ \forall k\in[K]\), which implies the contradiction as follows.

\[\sum_{k=1}^{K+1}\hat{p}_{sak}^{i}>\frac{v}{2}\geq\sum_{k\in\mathcal{N}}\hat{p} _{sak}^{i}-\bar{p}_{k}\geq\sum_{k=1}^{K+1}\hat{p}_{sak}^{i}.\]

Here the first inequality is due to the selection of \(v\), the second inequality has been deduced in the first case and the third inequality is from \(\bar{p}_{k}=0,\ \forall k\in[K+1]\). So we are able to find \(\hat{k}\leq K\) with \(\bar{p}_{\hat{k}}>0\). By moving probability \(\varepsilon=\min\{\frac{\bar{p}_{\hat{k}}}{2},\frac{\hat{p}_{sak}^{i}(K+1)}{ 2}\}\) from \(\bar{p}_{k}\) to \(\bar{p}_{K+1}\), we can achieve smaller objective value while keeping the feasibility, hence we get the contradiction, which implies that \(\bar{p}_{K+1}>0\) and \(\bar{p}_{S}<1\). By applying the similar procedures in the first case (moving some probability from \(\bar{p}_{K+1}\) or to \(\bar{p}_{S}\)), we can show that \(\bar{p}_{k}=\hat{p}_{sak}^{i}=r_{k}\) for \(K+2\leq k\leq S-1\).

Next we prove that \(\bar{p}_{k}=r_{k}=0\) for \(k\in[K]\). Suppose the contrary is true; that is, \(\bar{p}_{k}>0\) for some \(\hat{k}\leq K\). Then we are able to apply the same procedures as before which illustrate that \(\bar{p}_{k}=\hat{p}_{sak}^{i},\ \forall\hat{k}<k<S\). Provided this, one can verified that the optimal strategy is putting all the rest probability \(1-\sum_{k=\hat{k}+1}^{S-1}\hat{p}_{sak}^{i}\) to \(\bar{p}_{S}\) since \(b_{sa1}>b_{sa2}>\cdots>b_{saS}\), and \(v\geq 2\sum_{k=1}^{K}\hat{p}_{sak}^{i}\geq 2\sum_{k=1}^{\hat{k}}\hat{p}_{sak}^{i}\), which implies \(\bar{p}_{sak}=0\) and we get a contradiction. Hence \(\bar{p}_{k}=r_{k}=0,\ \forall k\in[K]\).

Finally, we verify the rest two components. We introduce the variables \(d_{K+1}=\bar{p}_{K+1}-\hat{p}_{sak(K+1)}^{i}\) and \(d_{S}=\bar{p}_{S}-\hat{p}_{saS}^{i}\), then we get the equivalent reformulation of the inner minimization in \(F(v)\):

\[\min_{d_{K+1},d_{S}} b_{sa(K+1)}d_{K+1}+b_{saS}d_{S}\] \[\mathrm{s.t.} d_{K+1}+d_{S}=\sum_{k=1}^{K}\hat{p}_{sak}^{i},|d_{K+1}|+|d_{S}| \leq v-\sum_{k=1}^{K}\hat{p}_{sak}^{i}.\]

The optimal \(d_{K+1}\) and \(d_{S}\) are given by \(-\frac{v-2\sum_{k=1}^{K}\hat{p}_{sak}^{i}}{2}\) and \(\frac{v}{2}\) respectively. Hence we proved \(\boldsymbol{p}^{\star}\) is an optimal solution.

To prove the third case of the claim, we notice that the optimal solution \(\boldsymbol{e}_{S}\) to \(\min_{\boldsymbol{p}_{sa}^{i}\in\Delta_{S}}\boldsymbol{b}_{sa}^{\top}\boldsymbol {p}_{sa}^{i}\) is also feasible for the inner minimization problem in \(F(v)\), hence it becomes the optimal solution we desire, which implies \(F(v)=v+\alpha b_{saS}\) at this time. This finishes the proof of our claim.

Our claim directly illustrates that \(F(v)\) is a piecewise-linear function in \(v\) with breakpoints \(\{0\}\cup\{2\sum_{k=1}^{K}\hat{p}_{sak}^{i}\ :\ \forall K\in[S-1]\}\). Furthermore, based on the provided formulation of \(F(v)\), we can compute the difference of value for \(F(\cdot)\) between any two adjacent breakpoints, given by

\[F(v_{K})-F(v_{K-1})=2\hat{p}_{saK}^{i}+\alpha(b_{saS}-b_{saK})\hat{p}_{saK}^{i }\quad\forall K\in[S-1],\]

where \(v_{K}=2\sum_{k=1}^{K}\hat{p}_{sak}^{i},\ \forall K\in[S-1]\) and \(v_{0}=0\).

Hence we provide Algorithm 2 to compute (11), whose time complexity is \(\mathcal{O}(S\log S)\) generally and can be reduced to \(\mathcal{O}(S)\) if the sorted \(\boldsymbol{b}_{sa}\) is provided.

Proof of Corollary 5.2.1.: We can see that Algorithm 2 is in time complexity \(\mathcal{O}(S)\) if \(\cup_{a\in A}\boldsymbol{b}_{sa}\) are sorted, which can be done at the Initialization step in Algorithm 1 with time complexity \(\mathcal{O}(AS\log S)\). So by Theorem 4.5, we get the overall complexity is \(\mathcal{O}\left(NAS\log\epsilon_{1}^{-1}\log\epsilon_{2}^{-1}+AS\log S\right)\).

#### a.2.2 Proof of results in Section 5.2

Proof of Theorem 5.3.: From problem (9), we can get the minimization problem for \(q=2\)

\[\min_{\mathbf{p}_{sa}^{i}\in\Delta_{S}}\left\|\mathbf{p}_{sa}^{i}-\hat{\mathbf{ p}}_{sa}^{i}\right\|_{2}^{2}+\alpha\mathbf{b}_{sa}^{\top}\mathbf{p}_{sa}^{i}\] \[= \min_{\mathbf{p}_{sa}^{i}\in\Delta_{S}}\left\|\mathbf{p}_{sa}^{i}\right\|_ {2}^{2}-2\mathbf{p}_{sa}^{\top}\hat{\mathbf{p}}_{sa}^{i}+\left\|\hat{\mathbf{p}}_{sa}^{i} \right\|_{2}^{2}+\alpha\mathbf{b}_{sa}^{\top}\mathbf{p}_{sa}^{i}\] \[= \min_{\mathbf{p}_{sa}\in\Delta_{S}}\left\|\mathbf{p}_{sa}^{i}\right\|_{2} ^{2}-\left(2\hat{\mathbf{p}}_{sa}^{i}-\alpha\mathbf{b}_{sa}\right)^{\top}\mathbf{p}_{sa} ^{i}+\left\|\hat{\mathbf{p}}_{sa}^{i}\right\|_{2}^{2}\] \[= -\frac{\alpha^{2}\left\|\mathbf{b}_{sa}\right\|_{2}^{2}}{4}+\alpha \mathbf{b}_{sa}^{\top}\hat{\mathbf{p}}_{sa}^{i}+\min_{\mathbf{p}_{sa}^{i}\in\Delta_{S}} \left\|\mathbf{p}_{sa}^{i}-\frac{2\hat{\mathbf{p}}_{sa}^{i}-\alpha\mathbf{b}_{sa}}{2} \right\|_{2}^{2}.\]

So it suffices to solve \(\min_{\mathbf{p}_{sa}^{i}\in\Delta_{S}}\left\|\mathbf{p}_{sa}^{i}-\frac{2\hat{\mathbf{p}}_ {sa}^{i}-\alpha\mathbf{b}_{sa}}{2}\right\|_{2}^{2}\), which can be done by Euclidean projection algorithm (Wang and Carreira-Perpinan, 2013) with time complexity \(\mathcal{O}(S\log S)\). 

Proof of Corollary 5.3.1.: The result is the direct consequence of Theorem 4.5 with \(h_{2}(S)\) is \(\mathcal{O}(S\log S)\), provided by Theorem 5.3.

#### a.2.3 Proof of results in Section 5.3

Proof of Theorem 5.4.: We claim that the Algorithm 3 solves problem (13) with time complexity \(\mathcal{O}(S\log S)\). By expanding the \(\infty\)-norm, we formulate (13) as the following box constraints problem.

\[\min_{\mathbf{b}_{sa}^{\top}\mathbf{p}_{sa}^{i}}\] \[\mathrm{s.t.} \max\left\{\mathbf{0},\hat{\mathbf{p}}_{sa}^{i}-\theta\mathbf{e}\right\}\leq \mathbf{p}_{sa}^{i}\leq\min\left\{\mathbf{e},\hat{\mathbf{p}}_{sa}^{i}+\theta\mathbf{e}\right\},\] \[\mathbf{e}^{\top}\mathbf{p}_{sa}^{i}=1,\] \[\mathbf{p}_{sa}^{i}\in\mathbb{R}^{S}.\]

To get the optimal solution, we put the probability on the index where \(\mathbf{b}_{sa}\) is small as much as possible. Specifically, we assume \(b_{sa1}<b_{sa2}<\cdots<b_{saS}\) w.l.o.g., and assume \(k\) is the smallest index such that \(\sum_{j=1}^{k}\left(\hat{p}_{saj}^{i}+\theta\right)\geq 1\). We claim that

\[p_{saj}^{i,\star}=\left\{\begin{array}{cl}\hat{p}_{saj}^{i}+\theta&\text{ if }1\leq j\leq k-1\\ 1-&\sum_{\ell=1}^{k-1}\left(\hat{p}_{sa}^{i}+\theta\right)&\text{ if }j=k\\ 0&\text{ otherwise }.\end{array}\right.\]

is the optimal solution to the above formulation. To see this, suppose \(\bar{\mathbf{p}}\) is optimal and there is some \(\hat{j}\in[k-1]\) with \(\bar{p}_{\hat{j}}\neq\hat{p}_{saj}^{i}+\theta\). By above box constraint, we get \(\bar{p}_{\hat{j}}<\hat{p}_{saj}^{i}+\theta\), so there exists \(\bar{j}\geq k\) such that \(\bar{p}_{\hat{j}}>p_{saj}^{i,\star}\). By moving the probability from \(\bar{p}_{\hat{j}}\) to \(\bar{p}_{\hat{j}}\), we can achieve strictly smaller objective value, which is a contradiction with that \(\bar{\mathbf{p}}\) is optimal. This gives us an optimal solution with the first \(k-1\) components coincides \(p_{sa}^{i,\star}\). Then we can get \(\mathbf{p}_{sa}^{i,\star}\) is indeed optimal by putting the extra \(1-\sum_{\ell=1}^{k-1}\left(\hat{p}^{i}_{sad}+\theta\right)\) probability on \(p^{i,*}_{sak}\), since \(b_{sak}\leq\cdots\leq b_{saS}\).

The major time complexity of Algorithm 3 is sorting the vector \(\mathbf{b}_{sa}\in\mathbb{R}^{S}\), which is \(\mathcal{O}(S\log S)\).

Proof of Corollary 5.4.1.: As for each \(a\in\mathcal{A}\), we need to sort \(\mathbf{b}_{sa}\), which costs \(\mathcal{O}(AS\log S)\), then we need to solve \(NA\) subproblems, which costs \(\mathcal{O}(NAS)\), so the whole problem is computed in time \(\mathcal{O}(AS\log S+NAS)\). 

## Appendix B Appendix: Computational Complexity for General Convex Optimzation

To compare our algorithm with general convex optimization algorithm, we use general convex optimization to compute the problem (5) and problem (8). The time complexities will be discussed in different situations:

* Suppose \(q=1\): For general convex optimization problem, problem (5) is equivalent with the following problem: \[[\mathfrak{T}(\mathbf{v})]_{s}=\left[\begin{array}{cc}\mathrm{ minimize}&\gamma\\ \mathrm{subject\;to}&\dfrac{1}{N}\sum_{i=1}^{N}(\mathbf{r}_{sa}+\lambda\mathbf{v})^{ \top}\mathbf{p}^{i}_{sa}\leq\gamma,\;\forall a\in\mathcal{A}\\ &\dfrac{1}{N}\sum_{i=1}^{N}\sum_{a\in\mathcal{A}}\sum_{s^{\prime}\in \mathcal{S}}\left|p^{i}_{sas^{\prime}}-\hat{p}^{i}_{sas^{\prime}}\right|\leq \theta\\ &\gamma\in\mathbb{R},\;\mathbf{p}^{*}_{sa}\subseteq\Delta_{S},\;\forall i\in[N] \,,\;\forall a\in\mathcal{A}.\end{array}\right]\quad\forall s\in\mathcal{S}.\] By introducing the variables \(t^{i}_{sas^{\prime}}=\left|p^{i}_{sas^{\prime}}-\hat{p}^{i}_{sas^{\prime}} \right|,\;\forall i\in[N],\;\forall a\in\mathcal{A},\;\forall s^{\prime}\in \mathcal{S}\), the above problem is equivalent with \[\mathrm{minimize} \gamma\] \[\mathrm{subject\;to} \dfrac{1}{N}\sum_{i=1}^{N}(\mathbf{r}_{sa}+\lambda\mathbf{v})^{\top}\mathbf{p }^{i}_{sa}\leq\gamma,\;\forall a\in\mathcal{A}\] \[\dfrac{1}{N}\sum_{i=1}^{N}\sum_{a\in\mathcal{A}}\sum_{s^{\prime} \in\mathcal{S}}t^{i}_{sas^{\prime}}\leq\theta\] \[t^{i}_{sas^{\prime}}\geq p^{i}_{sas^{\prime}}-\hat{p}^{i}_{sas^{ \prime}},\;t^{i}_{sas^{\prime}}\geq\hat{p}^{i}_{sas^{\prime}}-p^{i}_{sas^{ \prime}},\;\forall i\in[N]\,,\;\forall a\in\mathcal{A},\;\forall s^{\prime} \in\mathcal{S}\] \[\gamma\in\mathbb{R},\;\mathbf{p}^{*}_{sa}\in\Delta_{S},\;t^{i}_{sas^{ \prime}}\in\mathbb{R},\;\forall i\in[N]\,,\;\forall a\in\mathcal{A},\;\forall s ^{\prime}\in\mathcal{S}.\] There are \(1+NSA+NSA=\mathcal{O}(NSA)\) decision variables, and the number of bits in the input is \(\mathcal{O}(1)+\mathcal{O}(NAS)+\mathcal{O}(NAS)+\mathcal{O}(NAS)+\mathcal{O}( NAS)+\mathcal{O}(NAS)=\mathcal{O}(NAS)\). So by (Karmarkar, 1984), the complexity of solving this LP is \(\mathcal{O}(N^{4.5}S^{4.5}A^{4.5})\).

We utilize our outer bisection, solving (6) directly using convex optimization. Typically, for each fixed \(a\in\mathcal{A}\) and \(\gamma\), (6) is equivalent with the LP that

\[\mathrm{minimize} \dfrac{1}{N}\sum_{i=1}^{N}\sum_{s^{\prime}\in\mathcal{S}}t^{i}_{ sas^{\prime}}\] \[\mathrm{subject\;to} \dfrac{1}{N}\sum_{i=1}^{N}\mathbf{b}^{\top}_{sa}\mathbf{p}^{i}_{sa}\leq\gamma\] \[t^{i}_{sas^{\prime}}\geq p^{i}_{sas^{\prime}}-\hat{p}^{i}_{sas^{ \prime}},\;t^{i}_{sas^{\prime}}\geq\hat{p}^{i}_{sas^{\prime}}-p^{i}_{sas^{ \prime}},\;\forall i\in[N]\,,\;\forall s^{\prime}\in\mathcal{S}\] \[\mathbf{p}^{i}_{sa}\in\Delta_{S},\;t^{i}_{sas^{\prime}}\in\mathbb{R},\;\forall i\in[N]\,,\;\forall s^{\prime}\in\mathcal{S}.\]

There are \(2NS\) decision variables, and the number of bits in the input is \(\mathcal{O}(NS+NS+NS+NS)=\mathcal{O}(NS)\). So the complexity of solving this LP is \(\mathcal{O}(N^{4.5}S^{4.5})\). Together with the outer bisection, the total complexity for each Bellman update with \(\epsilon_{1}\) tolerance is \(\mathcal{O}(N^{4.5}S^{4.5}A\log\epsilon_{1}^{-1})\).

We utilize our nested bisection scheme, solving (9) using the general convex optimization algorithm. Typically, for each fixed \(a\in\mathcal{A},i\in[N]\) and \(\alpha\), (9) is equivalent with \[\begin{array}{ll}\mbox{minimize}&\sum_{s^{\prime}\in\mathcal{S}}t^{i}_{sas^{ \prime}}+\alpha\cdot\mathbf{b}^{\top}_{sa}\mathbf{p}^{i}_{sa}\\ \mbox{subject to}&t^{i}_{sas^{\prime}}\geq p^{i}_{sas^{\prime}}-\hat{p}^{i}_{ sas^{\prime}},\ t^{i}_{sas^{\prime}}\geq\hat{p}^{i}_{sas^{\prime}}-p^{i}_{sas^{ \prime}},\ \forall s^{\prime}\in\mathcal{S}\\ &\mathbf{p}^{i}_{sa}\in\Delta_{S},\ t^{i}_{sas^{\prime}}\in\mathbb{R},\ \forall s^{ \prime}\in\mathcal{S}.\end{array}\] There are \(2S\) decision variables, and the number of bits in the input is \(\mathcal{O}(S+S+S)=\mathcal{O}(S)\). So the complexity of solving this LP is \(\mathcal{O}(S^{4.5})\). Together with the nested bisection, the total complexity for each Bellman update where the tolerances of bisections are \(\epsilon_{1}\) and \(\epsilon_{2}\), is \(\mathcal{O}(NS^{4.5}A\log\epsilon_{1}^{-1}\log\epsilon_{2}^{-1})\).
* Suppose \(q=2\): For general convex optimization problem, problem (5) is equivalent with the following SOCP: \[[\overline{\mathbf{z}}(\mathbf{v})]_{s}=\left[\begin{array}{ll}\mbox{minimize}&\gamma \\ \mbox{subject to}&\frac{1}{N}\sum_{i=1}^{N}(\mathbf{r}_{sa}+\lambda\mathbf{v})^{ \top}\mathbf{p}^{i}_{sa}\leq\gamma,\ \forall a\in\mathcal{A}\\ &\frac{1}{N}\sum_{i=1}^{N}\sum_{a\in\mathcal{A}}\left\|\mathbf{p}^{i}_{sa}-\hat{\bm {p}}^{i}_{sa}\right\|_{2}^{2}\leq\theta^{2}\\ &\gamma\in\mathbb{R},\ \mathbf{p}^{i}_{sa}\in\Delta_{S},\ \forall i\in[N]\,,\ \forall a\in\mathcal{A}.\end{array}\right]\quad\forall s\in\mathcal{S}.\] There are \(1+NSA=\mathcal{O}(NSA)\) decision variables, and the number of constraints are \(A+1+NA(S+2)=\mathcal{O}(NAS)\). So the complexity of solving the SOCP with \(\epsilon\)-accuracy is \(\mathcal{O}(\sqrt{NAS}\log\epsilon^{-1}\cdot(NSA)^{2}(NAS+1+2(A+NA(S+2))))= \mathcal{O}(N^{3.5}S^{3.5}A^{3.5}\log\epsilon^{-1})\). We utilize our outer bisection, solving (6) directly using convex optimization. Typically, for each fixed \(a\in\mathcal{A}\) and \(\gamma\), (6) is equivalent with the SOCP that \[\begin{array}{ll}\mbox{minimize}&\delta\\ \mbox{subject to}&\frac{1}{N}\sum_{i=1}^{N}\mathbf{b}^{\top}_{sa}\mathbf{p}^{i}_{sa}\leq\gamma\\ &\frac{1}{N}\sum_{i=1}^{N}\left\|\mathbf{p}^{i}_{sa}-\hat{\mathbf{p}}^{i}_{sa}\right\|_ {2}^{2}\leq\delta\\ &\delta\in\mathbb{R},\ \mathbf{p}^{i}_{sa}\in\Delta_{S},\ \forall i\in[N]\,.\end{array}\] There are \(1+NS=\mathcal{O}(NS)\) decision variables, and the number of constraints are \(\mathcal{O}(NS)\). So the complexity of solving the above SOCP with \(\epsilon\)-accuracy is \(\mathcal{O}(\sqrt{NS}\log\epsilon^{-1}\cdot N^{2}S^{2}(NS+1+2(1+N(S+2))))= \mathcal{O}(N^{3.5}S^{3.5}\log\epsilon^{-1})\), hence the total complexity of the Bellman update is \(\mathcal{O}(N^{3.5}S^{3.5}A\log\epsilon^{-1}\log\epsilon_{1}^{-1})\). We utilize our nested bisection scheme, solving problem (9) using the general convex optimization algorithm. For each fixed \(a\in\mathcal{A},i\in[N]\) and \(\alpha\), problem (9) is equivalent with \[\begin{array}{ll}\mbox{minimize}&\delta+\alpha\cdot\mathbf{b}^{\top}_{sa}\mathbf{p}^{i}_{ sa}\\ \mbox{subject to}&\left\|\mathbf{p}^{i}_{sa}-\hat{\mathbf{p}}^{i}_{sa}\right\|_{2}^{2} \leq\delta\\ &\mathbf{p}^{i}_{sa}\in\Delta_{S},\ \delta\in\mathbb{R}.\end{array}\] There are \(1+S=\mathcal{O}(S)\) decision variables, and the number of constraints are \(\mathcal{O}(S)\). So the complexity of solving the above SOCP with \(\epsilon\)-accuracy is \(\mathcal{O}(\sqrt{S}\log\epsilon^{-1}\cdot S^{2}(S+1+2(1+S)))=\mathcal{O}(S^{3.5}\log\epsilon^{-1})\), hence the total complexity of the Bellman update is \(\mathcal{O}(NS^{3.5}A\log\epsilon^{-1}\log\epsilon_{1}^{-1}\log\epsilon_{2}^{-1})\).
* Suppose \(q=\infty\): We can also consider solving the inner problem in (10) using general convex optimization, which is equivalent with \[\begin{array}{ll}\mbox{minimize}&\mathbf{b}^{\top}_{sa}\mathbf{p}^{i}_{sa}\\ \mbox{subject to}&p^{i}_{sas^{\prime}}-\hat{p}^{i}_{sas^{\prime}}\leq\theta,\ \hat{p}^{i}_{sas^{\prime}}-p^{i}_{sas^{\prime}}\leq\theta,\ \forall s^{\prime}\in\mathcal{S}\\ &\mathbf{p}^{i}_{sa}\in\Delta_{S},\end{array}\]

[MISSING_PAGE_FAIL:24]

[MISSING_PAGE_FAIL:25]