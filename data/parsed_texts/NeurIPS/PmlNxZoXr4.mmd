# Neural Processes with Stability

 Huafeng Liu, Liping Jing, Jian Yu

Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University

The School of Computer and Information Technology, Beijing Jiaotong University

{hfliu1, lpjing, jianyu}@bjtu.edu.cn

###### Abstract

Unlike traditional statistical models depending on hand-specified priors, neural processes (NPs) have recently emerged as a class of powerful neural statistical models that combine the strengths of neural networks and stochastic processes. NPs can define a flexible class of stochastic processes well suited for highly non-trivial functions by encoding contextual knowledge into the function space. However, noisy context points introduce challenges to the algorithmic stability that small changes in training data may significantly change the models and yield lower generalization performance. In this paper, we provide theoretical guidelines for deriving stable solutions with high generalization by introducing the notion of algorithmic stability into NPs, which can be flexible to work with various NPs and achieves less biased approximation with theoretical guarantees. To illustrate the superiority of the proposed model, we perform experiments on both synthetic and real-world data, and the results demonstrate that our approach not only helps to achieve more accurate performance but also improves model robustness.

## 1 Introduction

Neural processes (NPs) [9; 10] constitute a family of variational approximation models for stochastic processes with promising properties in computational efficiency and uncertainty quantification. Different from traditional statistical modeling for which a user typically hand-specifies a prior (e.g., smoothness of functions quantified by a Gaussian distribution in Gaussian process [25]), NPs implicitly define a broad class of stochastic processes with neural networks in a data-driven manner. When appropriately trained, NPs can define a flexible class of stochastic processes well suited for highly non-trivial functions that are not easily represented by existing stochastic processes.

NPs meta-learn a distribution over predictors and provide a way to select an inductive bias from data to adapt quickly to a new task. Incorporating the data prior into the model as an inductive bias, NPs can reduce the model complexity and improve model generalization. Usually, an NP predictor is described as predicting a set of data (target set) given a set of labeled data (context set). However, the number of noise in data introduces challenges to the algorithmic stability. In NPs, models are biased to the meta-datasets (a dataset of datasets), so small changes in the dataset (noisy or missing) may significantly change the models. As demonstrated in previous work [9; 10; 14; 11], existing NPs cannot provide stable predictions under noisy conditions, which may introduce high training error variance, and minimizing the training error may not guarantee consistent error reduction on the test set, i.e. low generalization performance [2]. In this case, algorithmic stability and generalization performance have strong connections and an unstable NP model has low generalization performance.

A stable model is one for which the learned solution does not change much with small changes in training set [2]. In general, heuristic techniques, such as cross-validation and ensemble learning, can be adopted to improve the generalization performance. Cross-validation needs to sacrifice the limited training data, while ensemble learning is computationally expensive on training sub-models. Recently, there are several improved NPs focused on considering model stability andimproving generalization performance empirically, such as hierarchical prior [27], stochastic attention mechanism [15], bootstrap [13], and Mixture of Expert [28]. However, most of them are unable to investigate the theoretical bound of the generalization performance of NPs. It is desirable to develop robust algorithms with low generalization error and high efficiency.

In this paper, we investigate NP-related models and explore more expressive stability toward general stochastic processes by proposing a stable solution. Specifically, by introducing the notion of stability into NPs, we focus on developing theoretical guidelines for deriving a stable NPs solution. We propose a method to find out subsets that are harder to predict than average, which is a key step for constructing this optimization problem. Based on it, a new extension of NPs with stable guarantees is formulated, which can be flexible to work with various NPs and achieves less biased approximation with theoretical guarantees. Considering the model adaptivity, an adaptive weighting strategy is proposed. To illustrate the superiority of the proposed stable solution, we perform experiments on synthetic 1D regression, system identification of physics engines, and real-world image completion tasks, and the results demonstrate that NPs with our stable solution are much more robust than original NPs.

## 2 Related Work

In this section, we briefly review two different areas which are highly relevant to the proposed method, neural processes, and algorithmic stability.

**Neural Processes** Neural processes are a well-known member of the stochastic process family by directly capturing uncertainties with deep neural networks, which are not only computationally efficient but also retain a probabilistic interpretation of the model [9; 10; 14; 13]. Starting with conditional neural processes (CNP) [9], there have been several follow-up works to improve NPs in various aspects [6]. Vanilla CNP combines neural networks with the Gaussian process to extract prior knowledge from training data. NP [10] introduces a global latent variable to model uncertainty in a variational manner. Considering the problem of underfitting in the vanilla NP, Attentive NP [14] introduces the attention mechanism to improve the model's reconstruction quality. [11] introduced convolutional conditional neural process (ConvCNP) models translation equivariance in the data. Wang and Van Hoof [27] presented a doubly stochastic variational process (DSVNP), which combines both global and local latent variables. Lee et al. [18] extended NP using Bootstrap and proposed the bootstrapping neural processes (BNP). Kawano et al. [13] presented a group equivariant conditional neural process by incorporating group equivariant into CNPs in a meta-learning manner. Wang and van Hoof [28] proposed to combine the Mixture of Expert models with NPs to develop more expressive exchangeable stochastic processes. Kim et al. [15] proposed a stochastic attention mechanism for NPs to capture appropriate context information. Although there are many NP variants to improve the model performance, those do not consider stability to yield high generalization performance.

**Algorithmic Stability** Stability, as known as algorithmic stability, is a computational learning theory of how a machine learning algorithm is perturbed by small changes to its inputs [2]. Many efforts have been made to analyze various notions of algorithmic stability and prove that a broad spectrum of learning algorithms are stable in some sense [2; 3; 29; 12]. [3] proved that \(l_{2}\) regularized learning algorithms are uniformly stable and able to obtain new bounds on generalization performance. [29] generalized [3]'s results and proved that regularized learning algorithms with strongly convex penalty functions on bounded domains. Hardt et al. [12] showed that parametric models trained by stochastic gradient descent algorithms are uniformly stable. Li et al. [19] introduced the stability notation to low-rank matrix approximation. Liu et al. [22] proved that tasks in multi-task learning can act as regularizers and that multi-task learning in a very general setting will therefore be uniformly stable under mild assumptions. This is the first work to investigate the stability of NPs from theoretical guidelines and derive NPs solutions with high stability.

## 3 Preliminary

Let calligraphic letters (e.g., \(\mathcal{A}\)) indicate sets, capital letters (e.g., \(A\)) indicate scalars, lower-case bold letters (e.g., \(\mathbf{a}\)) denote vectors, and capital bold letters (e.g., \(\mathbf{A}\)) indicate matrices. Suppose there is a dataset \(\mathcal{D}=(\mathbf{X},\mathbf{y})=\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{N}\) with \(N\) data points \(\mathbf{X}=[\mathbf{x}_{1},\mathbf{x}_{2},\cdots,\mathbf{x}_{N}]^{\top}\in \mathbb{R}^{N\times D}\), and corresponding labels \(\mathbf{y}=[y_{1},y_{2},\cdots,y_{N}]\in\mathbb{R}^{N}\). Considering an arbitrary number of data points \(\mathcal{D}^{c}=(\mathbf{X}^{c},\mathbf{y}^{c})=\{(\mathbf{x}_{i},y_{i})\}_{i \in\mathcal{C}}\), where \(\mathcal{C}\subseteq\{1,2,\cdots,N\}\) is an index set defining context information, neural processes model the conditional predictive distribution of the target values \(\mathbf{y}^{\mathcal{T}}=\{y_{i}\}_{i\in\mathcal{T}}\)at some target data points \(\mathbf{X}^{\mathcal{T}}=\{\mathbf{x}_{i}\}_{i\in\mathcal{T}}\) based on the context \(\mathcal{D}^{\mathcal{C}}\), i.e. \(P(\mathbf{y}^{\mathcal{T}}|\mathbf{X}^{\mathcal{T}},\mathcal{D}^{\mathcal{C}})\). Usually, target set is defined as \(\mathcal{T}=\{1,2,\cdots,N\}\). Only in CNP [9], \(\mathcal{T}\subseteq\{1,2,\cdots,N\}\) and \(\mathcal{T}\cap\mathcal{C}=\emptyset\). In this paper, we define \(\mathcal{T}=\{1,2,\cdots,N\}\) for all NPs, i.e. conditional predictive distribution is \(P(\mathbf{y}|\mathbf{X},\mathcal{D}^{\mathcal{C}})=\prod_{i=1}^{N}P(y_{i}| \mathbf{x}_{i},\mathcal{D}^{\mathcal{C}})\).

Fundamentally, there are two NP variants: deterministic and probabilistic. Deterministic NP [9], i.e. CNP, models the conditional distribution as \(P(\mathbf{y}|\mathbf{X},\mathcal{D}^{\mathcal{C}})=P(\mathbf{y}|\mathbf{X}, \mathbf{r}^{\mathcal{C}})\), where \(\mathbf{r}^{\mathcal{C}}\in\mathbb{R}^{d}\) is an aggregated feature vector processed by a function that maps \(\mathcal{D}^{\mathcal{C}}\) into a finite-dimensional vector space in a permutation-invariant way. In probabilistic NPs [10], a latent variable \(\mathbf{z}\in\mathbb{R}^{d}\) is introduced to capture model uncertainty and the NPs infer \(P_{\theta}(\mathbf{z}|\mathcal{D}^{\mathcal{C}})\) given context set using the reparameterization trick [16] and models such a conditional distribution as \(P_{\theta}(y_{i}|\mathbf{x}_{i},\mathcal{D}^{\mathcal{C}})=\int P_{\theta}(y_ {i}|\mathbf{x}_{i},\mathcal{D}^{\mathcal{C}},\mathbf{z})P_{\theta}(\mathbf{z }|\mathcal{D}^{\mathcal{C}})d\mathbf{z}\) and it is trained by maximizing an ELBO: \(\mathbb{E}_{\mathbf{z}\sim P_{\theta}(\mathbf{z}|\mathbf{X},\mathcal{Y})}[ \log P_{\theta}(\mathbf{y}|\mathbf{X})]-KL[P_{\theta}(\mathbf{z}|\mathbf{X}, \mathbf{y})|\|P_{\theta}(\mathbf{z}|\mathcal{D}^{\mathcal{C}})]\).

**Meta Training NP Prediction** To achieve fast prediction on a new context set at test time, NPs meta-learn a distribution over predictors. To perform meta-learning, we require a meta-dataset (dataset of datasets). We consider an unknown distribution \(\mu\) on an instance space \(\mathcal{X}\times\mathcal{Y}\), and a set of independent sample \(\mathcal{D}=\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{N}\) drawn from \(\mu\): \((\mathbf{x}_{i},y_{i})\sim\mu\) and \(\mathcal{D}\sim\mu^{N}\). Suppose meta-dataset contains \(M\) datasets \(\mathcal{D}_{1:M}=\{\mathcal{D}_{m}\}_{m=1}^{M}\) with \(\mathcal{D}_{m}=\{\mathcal{D}_{m}^{\mathcal{C}},\mathcal{D}_{m}^{\mathcal{T}}\}\), we assume that all \(M\) datasets drawn from a common environment \(\tau\), which is a probability measure on the set of probability measures on \(\mathcal{X}\times\mathcal{Y}\). The draw of \(\mu\sim\tau\) indicates the encounter of a specific learning task \(\mu\) in the environment \(\tau\). For simplicity, we assume that each dataset has the same sample size \(N\). Following the previous work related to multi-task learning [24] and meta learning [5], The environment \(\tau\) induces a measure \(\mu_{N,\tau}\) on \((\mathcal{X}\times\mathcal{Y})^{N}\) such that \(\mu_{N,\tau}(A)=\mathbb{E}_{\mu\sim\tau}[\mu^{N}(A)],\forall A\subseteq( \mathcal{X}\times\mathcal{Y})^{N}\). Thus a dataset \(\mathcal{D}_{m}\) is independently sampled from a task \(\mu\) encountered in \(\tau\), which is denoted as \(\mathcal{D}_{m}\sim\mu_{N,\tau}\) for \(m\in[M]\).

Suppose there exists a meta parameter \(\theta\) indicating the shared knowledge among different tasks. In this case, a meta learning algorithm \(\mathcal{A}_{meta}\) for NPs takes meta-datasets \(\mathcal{D}_{1:M}\) as input, and then outputs a meta parameter \(\theta=\mathcal{A}_{meta}(\mathcal{D}_{1:M})\sim P_{\theta|\mathcal{D}_{1:M}}\). When given a new test dataset \(\mathcal{D}\), we can evaluate the quality of the meta parameter \(\theta\) by the following true risk:

\[R_{\tau}(\theta)=\mathbb{E}_{\mathcal{D}\sim\mu_{N,\tau}}\mathbb{E}_{U\sim P_{ \theta|\mathcal{D}_{1:M}}}[R_{\mu}(\theta)]\] (1)

where \(R_{\mu}(\theta)=-\mathbb{E}_{(\mathbf{x}_{i},y_{i})\sim\mu}\log P_{\theta}(y_ {i}|\mathbf{x}_{i},\mathcal{D}^{\mathcal{C}})\). Usually, \(\tau\) and \(\mu\) are unknown, we can only estimate the meta parameter \(\theta\) from the observed data \(\mathcal{D}_{1:M}\). In this case, the empirical risk w.r.t \(\theta\) is:

\[R_{\mathcal{D}_{1:M}}(\theta)=\nicefrac{{1}}{{M}}\sum\nolimits_{m=1}^{M} \mathbb{E}_{\theta\sim P_{\theta|\mathcal{D}_{m}^{\mathcal{C}}}}R_{\mathcal{D}_ {m}}(\theta)\] (2)

where \(R_{\mathcal{D}_{m}}(\theta)=-(1/N)\sum_{i=1}^{N}\log P_{\theta}(y_{i}|\mathbf{x }_{i},\mathcal{D}^{\mathcal{C}})\).

NPs have various strengths: 1) _Efficiency_: meta-learning allows NPs to incorporate information from a new context set and make predictions with a single forward pass. The complexity is linear or quadratic in the context size instead of cubic as with Gaussian process regression; 2) _Flexibility_: NPs can define a conditional distribution of an arbitrary number of target points, conditioning an arbitrary number of observations; 3) _Permutation invariance_: the encoders of NPs use set property [32] to make the target prediction permutation invariant. Thanks to these properties, NPs are widely-used in lots of tasks, e.g., Bayesian optimization [8], recommendation [20; 21], physics engines controlling [27] etc. While there are many NP variants to improve the performance of NPs [9; 10; 14; 13; 15; 28], those do not take model's stability into consider account yet, which is the key to the robustness of the model.

## 4 Problem Formulation

**Stability of NP** A stable learning algorithm has the property that replacing one element in the training set does not result in a significant change to the algorithm's output [2]. Therefore, if we take the training error as a random variable, the training error of a stable learning algorithm should have a small variance. This implies that stable algorithms have the property that the training errors are close to the testing error [2]. Based on the defined risks, the algorithmic stability of approximate \(\{y_{i}\}_{i\in\mathcal{T}}\) in NPs is defined as follows.

**Definition 4.1**.: (Algorithmic Stability of Neural Processes) For any measure \(\mu_{N,\tau}\) on \((\mathcal{X}\times\mathcal{Y})^{N}\) such that \(\mu_{N,\tau}(A)=\mathbb{E}_{\mu\sim\tau}[\mu^{N}(A)],\forall A\subseteq( \mathcal{X}\times\mathcal{Y})^{N}\), sample \(M\) datasets \(\mathcal{D}_{1:M}\) from \(\mu_{N,\tau}\) randomly. For a given \(\epsilon>0\), we say that \(R_{\mathcal{D}_{1:M}}(\theta)\) is \(\delta\)-stable if the following holds:

\[P\left(|R_{\tau}(\theta)-R_{\mathcal{D}_{1:M}}(\theta)|\leq\epsilon\right)\geq 1-\delta.\] (3)The above stability for NPs has the property that the generalization error is bounded, which indicates that minimizing the training error will have a high probability of minimizing the testing error. This new stability notion makes it possible to measure the generalization performance between different NP approximations. For instance, for any two meta-datasets \(\mathcal{D}^{1}_{1:M}\) and \(\mathcal{D}^{2}_{1:M}\) from \(\mu_{N,\tau}\), train NPs on \(\mathcal{D}^{1}_{1:M}\) and \(\mathcal{D}^{2}_{1:M}\) are \(\delta_{1}\)-stable and \(\delta_{2}\)-stable, respectively. Then \(R_{\mathcal{D}^{1}_{1:M}}(\theta)\) is more stable than \(R_{\mathcal{D}^{1}_{1:M}}(\theta)\) if \(\delta_{1}<\delta_{2}\). This implies that \(R_{\mathcal{D}^{1}_{1:M}}(\theta)\) is close to \(R_{\tau}(\theta)\) with higher probability than \(R_{\mathcal{D}^{2}_{1:M}}(\theta)\), i.e. minimizing \(R_{\mathcal{D}^{1}_{1:M}}(\theta)\) will lead to solutions that are of high probabilities with better generalization performance than minimizing \(R_{\mathcal{D}^{2}_{1:M}}(\theta)\).

Based on the above analysis, we can see that the reliability of data points is crucial to the success of NPs and frail NPs are susceptible to noise.

**Stability vs. Generalization Error** The sparsity of the data, incomplete and noisy introduces challenges to the algorithm stability. NP models are biased to the quality of context data and target data, so small changes in the training data (noisy) may significantly change the models. In this case, unstable solutions will introduce high training error variance, and minimizing the training error may not guarantee consistent error reduction on the testing dataset, i.e., low generalization performance. In other words, the algorithm stability has a direct impact on generalization performance, and an unstable NP solution has low generalization performance. We take NPs with 1D regression task as an example [9] to investigate the relationship between generalization performance and stability of NPs. The total number of training and testing datasets is \(200\) and \(100\). We trained the NPs model with curves generated from the Gaussian process with RBF kernels by replacing the normal data dataset with a noisy dataset, i.e. the number of replaced datasets is turned in \(\{1,10,20,50,80,100\}\). We quantify stability changes of NPs with the generalization error when the number of replaced datasets increases from \(1\) to \(100\). We compute the difference between training error and test error to measure generalization error. We define the difference between test error and training error as \(R_{\tau}(\theta)-R_{\mathcal{D}_{1:M}}(\theta)\), and compute \(P(|R_{\tau}(\theta)-R_{\mathcal{D}_{1:M}}(\theta)|\leq\epsilon)\) with 100 different runs to measure stability. Here we choose \(\epsilon\) in Definition 4.1 as 0.0015 to cover all error differences when the number of replaced datasets is \(1\). As shown in Figure 1, the generalization error increases when the number of replaced points increases since the testing error becomes lower. On the contrary, the stability of NP decreases with the number of replaced points increases. This indicates that stability decreases with generalization error increases. This study demonstrates that existing NPs suffer from lower generalization performance due to low algorithmic stability. Therefore, it is important to develop a stable solution for NPs that offers good generalization performance.

## 5 Method

In this section, inspired by the previous work [19], we present a stable solution for NPs with stability and high generalization. Algorithmic stability provides an intuitive way to measure the changes in the outputs of a learning algorithm when the input is changed. Various ways have been introduced to measure algorithmic stability. Following the definition of uniform stability [2], given a stable NP, the approximation results remain stable if the change of the datasets. For instance, we can remove a subset of easily predictable data points from \(\mathcal{D}_{1:M}\) to obtain \(\mathcal{D}^{\prime}_{1:M}\). It is desirable that the solution of minimizing both \(\mathcal{D}_{1:M}\) and \(\mathcal{D}^{\prime}_{1:M}\) together will be more stable than the solution of minimizing \(\mathcal{D}_{1:M}\) only. The following Theorem formally proves the statement.

**Theorem 5.1**.: _Let \(\mathcal{D}_{1:M}\) (\(M\geq 2\)) be a sampled meta-dataset of measure \(\mu_{N,\tau}\). Let \(\mathcal{D}_{s}\in\mathcal{D}_{1:M}\) be a subset of the meta-dataset, which satisfy that \(\forall(\mathbf{x}_{i},y_{i})\in\mathcal{D}_{s},-\log P_{\theta}(y_{i}| \mathbf{x}_{i},\mathcal{D}^{C})\leq R_{\mathcal{D}_{1:M}}(\theta)\). Let \(\mathcal{D}^{\prime}_{1:M}=\mathcal{D}_{1:M}-\mathcal{D}_{s}\), then for any \(\epsilon>0\) and \(1>w_{0}>0\), \(1>w_{1}>0\) (\(w_{0}+w_{1}=1\)), \(w_{0}R_{\mathcal{D}_{1:M}}(\theta)+w_{1}R_{\mathcal{D}^{\prime}_{1:M}}(\theta)\) and \(R_{\mathcal{D}_{1:M}}(\theta)\) are \(\delta_{1}\)-stable and \(\delta_{2}\)-stable, respectively, then \(\delta_{1}\leq\delta_{2}\)._

Proof.: Let's assume that \(R_{\tau}(\theta)-R_{\mathcal{D}_{1:M}}(\theta)\in[-a_{1},a_{1}]\) and \(R_{\tau}(\theta)-(w_{0}R_{\mathcal{D}_{1:M}}(\theta)+w_{1}R_{\mathcal{D}^{ \prime}_{1:M}}(\theta))\in[-a_{2},a_{2}]\) are two random variables with zero mean, where \(a_{1}=\sup\{R_{\tau}(\theta)-R_{\mathcal{D}_{1:M}}(\theta)\}\) and

Figure 1: Stability vs. generalization error with different numbers of replaced noisy datasets.

\(\sup\{R_{\tau}(\theta)-(w_{0}R_{\mathcal{D}_{1:M}}(\theta)+w_{1}R_{\mathcal{D}^{ \prime}_{1:M}}(\theta))\}\). Based on Markov's inequality1, for any \(t>0\), we have

Footnote 1: (Extended version of Markov’s Inequality) Let \(x\) be a real-valued non-negative random variable and \(\varphi(\cdot)\) be a nondecreasing nonnegative function with \(\varphi(a)>0\). Then, for any \(\epsilon>0\), \(P(x\in\epsilon)\leq\frac{\mathbb{E}[\varphi(x)]}{\varphi(\epsilon)}\).

\[P(R_{\tau}(\theta)-R_{\mathcal{D}_{1:M}}(\theta)\geq\epsilon)\leq\frac{ \mathbb{E}\left[e^{t\left(R_{\tau}(\theta)-R_{\mathcal{D}_{1:M}}(\theta)\right) \right]}}{e^{t\epsilon}}.\] (4)

Based on Hoeffding's lemma2, we have \(\mathbb{E}[e^{t(R_{\tau}(\theta)-R_{\mathcal{D}_{1:M}}(\theta))}]\leq e^{ \frac{1}{2}t^{2}a_{1}^{2}}\), i.e. \(P(R_{\tau}(\theta)-R_{\mathcal{D}_{1:M}}(\theta)\geq\epsilon)\leq\frac{e^{ \frac{1}{2}t^{2}a_{1}^{2}}}{e^{t\epsilon}}\). Similarly, we have \(P(R_{\tau}(\theta)-R_{\mathcal{D}_{1:M}}(\theta)\leq-\epsilon)\leq\frac{e^{ \frac{1}{2}t^{2}a_{1}^{2}}}{e^{t\epsilon}}\). Combining those two inequalities, we have \(P(|R_{\tau}(\theta)-R_{\mathcal{D}_{1:M}}(\theta)|\geq\epsilon)\leq\frac{2e^{ \frac{1}{2}t^{2}a_{1}^{2}}}{e^{t\epsilon}}\), i.e.

Footnote 2: (Hoeffding’s Lemma) Let \(x\) be a real-valued random variable with zero mean and \(p(x\in[a,b])=1\). Then, for any \(z\in\mathbb{R}\), \(\mathbb{E}[e^{zx}]\leq\exp\big{(}\frac{1}{\delta}z^{2}(b-a)^{2}\big{)}\).

\[P(|R_{\tau}(\theta)-R_{\mathcal{D}_{1:M}}(\theta)|\leq\epsilon)\geq 1-\frac{2e^{ \frac{1}{2}t^{2}a_{1}^{2}}}{e^{t\epsilon}}.\] (5)

Similarly, we have

\[P\left(|R_{\tau}(\theta)-(w_{0}R_{\mathcal{D}_{1:M}}(\theta)+w_{1}R_{\mathcal{ D}^{\prime}_{1:M}}(\theta))|\leq\epsilon\right)\geq 1-\frac{2e^{\frac{1}{2}t^{2}a_{ 2}^{2}}}{e^{t\epsilon}}.\] (6)

In this case, the relationship between \(a_{1}\) and \(a_{2}\) is

\[a_{2} =\sup\Big{\{}R_{\tau}(\theta)-R_{\mathcal{D}_{1:M}}(\theta)+w_{1} \left(R_{\mathcal{D}_{1:M}}(\theta)-R_{\mathcal{D}^{\prime}_{1:M}}(\theta) \right)\Big{\}}\] (7) \[=a_{1}+\lambda_{1}\sup\Big{\{}R_{\mathcal{D}_{1:M}}(\theta)-R_{ \mathcal{D}^{\prime}_{1:M}}(\theta)\Big{\}}\]

Since \(\forall(\mathbf{x}_{i},y_{i})\in\mathcal{D}_{s}\), \(-\log P_{\theta}(y_{i}|\mathbf{x}_{i},\mathcal{D}^{\mathcal{S}}_{\mathcal{S}}) \leq R_{\mathcal{D}_{1:M}}(\theta)\), we have \(-(1/N)\sum_{i=1}^{N}\log P_{\theta}(y_{i}|\mathbf{x}_{i},\mathcal{D}^{ \mathcal{S}}_{\mathcal{S}})\leq R_{\mathcal{D}_{1:M}}(\theta)\). Then, since \(\mathcal{D}_{1:M}=\mathcal{D}_{s}\cup\mathcal{D}^{\prime}_{1:M}\). This means that \(\sup\{R_{\mathcal{D}_{1:M}}(\theta)-R_{\mathcal{D}^{\prime}_{1:M}}(\theta)\}\leq 0\). Thus, we have \(a_{2}\leq a_{1}\). This turns out that \(\frac{2e^{\frac{1}{2}t^{2}a_{2}^{2}}}{e^{t\epsilon}}\leq\frac{2e^{\frac{1}{2} t^{2}a_{1}^{2}}}{e^{t\epsilon}}\), i.e. \(\delta_{1}\leq\delta_{2}\). 

Theorem 5.1 indicates that, if we remove a subset that is easier to predict than average from \(\mathcal{D}_{1:M}\) to form \(\mathcal{D}^{\prime}_{1:M}\), then \(w_{0}R_{\mathcal{D}_{1:M}}(\theta)+w_{1}R_{\mathcal{D}^{\prime}_{1:M}}(\theta)\) has higher probability of being close to \(R_{\tau}(\theta)\) than \(R_{\mathcal{D}_{1:M}}(\theta)\). Therefore, minimizing \(w_{0}R_{\mathcal{D}_{1:M}}(\theta)+w_{1}R_{\mathcal{D}^{\prime}_{1:M}}(\theta)\) will lead to solutions that have better generalization performance than minimizing \(R_{\mathcal{D}_{1:M}}(\theta)\).

However, Theorem 5.1 only proves that it is beneficial to remove an easily predictable dataset from \(\mathcal{D}_{1:M}\) to obtain \(\mathcal{D}^{\prime}_{1:M}\), but does not show how many datasets we should remove from \(\mathcal{D}_{1:M}\). Actually, removing more datasets that satisfy \(-\log P_{\theta}(y_{i}|\mathbf{x}_{i},\mathcal{D}^{\mathcal{C}})\leq R_{ \mathcal{D}_{1:M}}(\theta)\) can obtain better \(\mathcal{D}^{\prime}_{1:M}\), as shown in following Theorem 5.2.

**Theorem 5.2**.: _Let \(\mathcal{D}_{1:M}\) (\(M\geq 2\)) be a sampled meta-dataset of measure \(\mu_{N,\tau}\). Let \(\mathcal{D}_{s1}\) and \(\mathcal{D}_{s2}\) be two subsets of \(\mathcal{D}_{1:M}\), which satisfy \(\mathcal{D}_{s2}\subset\mathcal{D}_{s1}\subset\mathcal{D}_{1:M}\). \(\mathcal{D}_{s2}\) and \(\mathcal{D}_{s1}\) satisfy that \(\forall(\mathbf{x}_{i},y_{i})\in\mathcal{D}_{s1},\)\(-\log P_{\theta}(y_{i}|\mathbf{x}_{i},\mathcal{D}^{\mathcal{C}})\leq R_{ \mathcal{D}_{1:M}}(\theta)\). Let \(\mathcal{D}^{\prime}_{1:M}=\mathcal{D}_{1:M}-\mathcal{D}_{s1}\) and \(\mathcal{D}^{\prime}_{1:M}=\mathcal{D}_{1:M}-\mathcal{D}_{s2}\), then for any \(\epsilon>0\) and \(1>w_{0}>0\), \(1>w_{1}>0\) (\(w_{0}+w_{1}=1\)), \(w_{0}R_{\mathcal{D}_{1:M}}(\theta)+w_{1}R_{\mathcal{D}^{\prime}_{1:M}}(\theta)\) and \(w_{0}R_{\mathcal{D}_{1:M}}(\theta)+w_{1}R_{\mathcal{D}^{\prime}_{1:M}}(\theta)\) are \(\delta_{1}\)-stable and \(\delta_{2}\)-stable, respectively, then \(\delta_{1}\leq\delta_{2}\)._

Proof.: Let's assume that \(R_{\tau}(\theta)-(w_{0}R_{\mathcal{D}_{1:M}}(\theta)+w_{1}R_{\mathcal{D}^{ \prime}_{1:M}}(\theta))\in[-a_{1},a_{1}]\) and \(R_{\tau}(\theta)-(w_{0}R_{\mathcal{D}_{1:M}}(\theta)+w_{1}R_{\mathcal{D}^{ \prime}_{1:M}}(\theta))\in[-a_{2},a_{2}]\) are two random variables with \(0\) mean, where \(a_{1}=\sup\{R_{\tau}(\theta)-(w_{0}R_{\mathcal{D}_{1:M}}(\theta)+w_{1}R_{ \mathcal{D}^{\prime}_{1:M}}(\theta))\}\) and \(a_{2}=\sup\{R_{\tau}(\theta)-(w_{0}R_{\mathcal{D}_{1:M}}(\theta)+w_{1}R_{ \mathcal{D}^{\prime}_{1:M}}(\theta))\}\).

Then, based on Markov's inequality and Hoeffding's lemma, we have

\[\begin{split}& P\left(\left|R_{\tau}(\theta)-\Big{(}w_{0}R_{ \mathcal{D}_{1:M}}(\theta)+w_{1}R_{\mathcal{D}^{\prime}_{1:M}}(\theta)\Big{)} \right|\leq\epsilon\right)\geq 1-\frac{2e^{\frac{1}{2}t^{2}a_{1}^{2}}}{e^{t\epsilon}},\\ & P\left(\left|R_{\tau}(\theta)-\Big{(}w_{0}R_{\mathcal{D}_{1:M}}( \theta)+w_{1}R_{\mathcal{D}^{\prime}_{1:M}}(\theta)\Big{)}\right|\leq\epsilon \right)\geq 1-\frac{2e^{\frac{1}{2}t^{2}a_{2}^{2}}}{e^{t\epsilon}}.\end{split}\] (8)

Since \(\forall(\mathbf{x}_{i},y_{i})\in\mathcal{D}_Theorem 5.2 indicates that removing more data points that are easy to predict will obtain more stable NPs. Therefore, it is desirable to choose \(\mathcal{D}^{\prime}_{1:M}\) (i.e. \(\mathcal{D}_{1:M}-\mathcal{D}_{s}\)) as the whole set which is harder to predict than average, i.e. the whole set satisfying \(\forall(\mathbf{x}_{i},y_{i})\in\mathcal{D}^{\prime}_{1:M}\), \(-\log P_{\theta}(y_{i}|\mathbf{x}_{i},\mathcal{D}^{c})\leq R_{\mathcal{D}_{1:M }}(\theta)\). Without loss of generality, we can extend Theorem 5.2 by considering several harder predicted sets to obtain a more stable solution by minimizing them all together. In this case, we need to prove that the stability of a model with \(K\) subsets is better than the model with \(K-1\) subsets, as shown in Theorem 5.3.

**Theorem 5.3**.: _Let \(\mathcal{D}_{1:M}\) (\(M\geq 2\)) be a sampled meta-dataset of measure \(\mu_{N,\tau}\). Let \(\mathcal{D}_{s1},\mathcal{D}_{s2},\cdots,\mathcal{D}_{sK}\subset\mathcal{D}_{ 1:M}\) be \(K\) subsets and satisfy \(\forall(\mathbf{x}_{i},y_{i})\in\mathcal{D}_{sk}(k\in[K])\), \(-\log P_{\theta}(y_{i}|\mathbf{x}_{i},\mathcal{D}^{c})\leq R_{\mathcal{D}_{1: M}}(\theta)\). Let \(\mathcal{D}^{k}_{1:M}=\mathcal{D}_{1:M}-\mathcal{D}_{sk}\) for all \(k\in[K]\), then for any \(\epsilon>0\) and \(1>w_{k}>0\) for all \(k\in[K]\), \((w_{0}+w_{1}+\dots+w_{K}=1)\), \(w_{0}R_{\mathcal{D}_{1:M}}(\theta)+\sum_{k=1}^{K}w_{k}R_{\mathcal{D}^{k}_{1:M} }(\theta)\) and \((w_{0}+w_{K})R_{\mathcal{D}_{1:M}}(\theta)+\sum_{k=1}^{K-1}w_{k}R_{\mathcal{D}^ {k}_{1:M}}(\theta)\) are \(\delta_{1}\)-stable and \(\delta_{2}\)-stable, respectively, then \(\delta_{1}\leq\delta_{2}\)._

Proof.: For simplicity, we denote \(w_{0}R_{\mathcal{D}_{1:M}}(\theta)+\sum_{k=1}^{K}w_{k}R_{\mathcal{D}^{k}_{1:M} }(\theta)\) as \(R_{1}\) and \((w_{0}+w_{K})R_{\mathcal{D}_{1:M}}(\theta)+\sum_{k=1}^{K-1}w_{k}R_{\mathcal{D}^ {k}_{1:M}}(\theta)\) as \(R_{2}\). Let's assume that \(R_{\tau}(\theta)-R_{1}\in[-a_{1},a_{1}]\) and \(R_{\tau}(\theta)-R_{2}\in[-a_{2},a_{2}]\) are two random variables with \(0\) mean, where \(a_{1}=\sup\{R_{\tau}(\theta)-R_{1}\}\) and \(a_{2}=\sup\{R_{\tau}(\theta)-R_{2}\}\). Then, based on Markov's inequality and Hoeffding's lemma, we have

\[P(|R_{\tau}(\theta)-R_{1}|\leq\epsilon)\geq 1-\frac{2e^{\frac{1}{2}a_{2}^{2}}}{e ^{t\epsilon}},\qquad(|R_{\tau}(\theta)-R_{2}|\leq\epsilon)\geq 1-\frac{2e^{ \frac{1}{2}a_{2}^{2}}}{e^{t\epsilon}}.\] (9)

Similar to the proof of Theorem 5.3, we have \(R_{\mathcal{D}^{k}_{1:M}}(\theta)\geq R_{\mathcal{D}_{1:M}}(\theta)\), which indicates that \(\sup\{R_{\mathcal{D}_{1:M}}(\theta)-R_{\mathcal{D}^{k}_{1:M}}(\theta)\}\leq 0\). Since \(a_{2}=a_{1}+w_{K}\sup\{R_{\mathcal{D}_{1:M}}(\theta)-R_{\mathcal{D}^{K}_{1:M}} (\theta)\}\), we know that \(a_{2}\leq a_{1}\). Thus we can conclude that \(\frac{2e^{\frac{1}{2}a_{2}a^{2}}}{e^{t\epsilon}}\leq\frac{2e^{\frac{1}{2}a_{2 }^{2}}}{e^{t\epsilon}}\), i.e. \(\delta_{1}\leq\delta_{2}\). 

Based on Theorem 5.3, we know that optimization on \(\mathcal{D}_{1:M}\) and more than one hard predictable subsets of \(\mathcal{D}_{1:M}\) can achieve more stable prediction. However, how to select data points that are difficult to predict from \(\mathcal{D}_{1:M}\) is still a challenging problem, especially, since we need to select \(K\) subsets.

### The Proposed Solution

According to the analysis of stability in NPs, we propose a stable solution for NPs to achieve model stability with the aid of hard predictable subsets selection. Specifically, we introduce a solution to obtain those hard predictable subsets based on _only one_ set of easily predicted data points which can be broken into four steps:

1. Selecting a existing NP model (e.g., CNP [9], NP[10]) and training it with meta-dataset \(\mathcal{D}_{1:M}\);
2. Selecting an easily predicted subset \(\mathcal{D}_{s}\subset\mathcal{D}_{1:M}\), which satisfies that \(\forall(\mathbf{x}_{i},y_{i})\in\mathcal{D}_{s}\), \(-\log P_{\theta}(y_{i}|\mathbf{x}_{i},\mathcal{D}^{c})\leq R_{\mathcal{D}_{1:M} }(\theta)\);
3. Dividing \(\mathcal{D}_{s}\) into \(K\) non-overlapping subsets \(\mathcal{D}_{s1},\mathcal{D}_{s2},\cdots,\mathcal{D}_{sK}\) and satisfying \(\cup_{k=1}^{K}\mathcal{D}_{sk}=\mathcal{D}_{s}\);
4. Defining \(K\) subsets that are difficult to predict, i.e. \(\mathcal{D}^{k}_{1:M}=\mathcal{D}_{1:M}-\mathcal{D}_{sk}\) for all \(k\in[K]\);

Thus, a new extension of NPs is given as

\[\mathcal{L}=\arg\min_{\theta}w_{0}R_{\mathcal{D}_{1:M}}(\theta)+\sum_{k=1}^{K}w _{k}R_{\mathcal{D}^{k}_{1:M}}(\theta),\] (10)

where \(w_{0},w_{1},\cdots,w_{K}\) indicate the contributions of each component and satisfy \(\sum_{k=0}^{K}w_{k}=1\).

The whole learning algorithm is given in Algorithm 1. From steps 1 to 12, we obtain \(K\) different hard predictable subsets, and the complexity of lines 1 to 12 is \(O(MN)\). The complexity of line 11 is related to the applied NP models (such as CNP, NP, ANP, etc). Thus, the computational complexity of NPs with our stable solution is similar to the original NPs. As shown in Algorithm 1, we need to pre-train the base model to select samples. In fact, the pre-trained model can not only be used for sample selection but its model parameters can be used as the initialization of the stable version model. At this time, the training of the stable version can converge faster.

**Stability Guarantee** Here we give a theoretical guarantee of the proposed stable solution.

**Theorem 5.4**.: _Let \(\mathcal{D}_{1:M}\) (\(M\geq 2\)) be a sampled meta-dataset of measure \(\mu_{N,\tau}\). Let \(\mathcal{D}_{s}\subset\mathcal{D}_{1:M}\) which satisfies that \(\forall(\mathbf{x}_{i},y_{i})\in\mathcal{D}_{s}\), \(-\log P_{\theta}(y_{i}|\mathbf{x}_{i},\mathcal{D}^{c})\leq R_{\mathcal{D}_{1:M} }(\theta)\). By dividing \(\mathcal{D}_{s}\) into \(K\) subsets\(\mathcal{D}_{s1},\mathcal{D}_{s2},\cdots,\mathcal{D}_{sK}\) which satisfy that \(\cup_{k=1}^{K}\mathcal{D}_{sk}=\mathcal{D}_{1:M}\). Let \(\mathcal{D}_{1:M}^{0}=\mathcal{D}_{1:M}-\mathcal{D}_{s}\) and \(\mathcal{D}_{1:M}^{k}=\mathcal{D}_{1:M}-\mathcal{D}_{sk}\) for all \(k\in[K]\), then for any \(\epsilon>0\) and \(1>w_{k}>0\) for all \(k\in[K]\), (\(w_{0}+w_{1}+\cdots+w_{K}=1\)), \(w_{0}R_{\mathcal{D}_{1:M}}(\theta)+\sum_{k=1}^{K}w_{k}R_{\mathcal{D}_{1:M}^{k} }(\theta)\) and \(w_{0}R_{\mathcal{D}_{1:M}}(\theta)+(1-w_{0})R_{\mathcal{D}_{1:M}^{0}}(\theta)\) are \(\delta_{1}\)-stable and \(\delta_{2}\)-stable, respectively, then \(\delta_{1}\leq\delta_{2}\)._

Proof.: For simplicity, we denote \(w_{0}R_{\mathcal{D}_{1:M}}(\theta)+\sum_{k=1}^{K}w_{k}R_{\mathcal{D}_{1:M}^{k} }(\theta)\) as \(R_{1}\) and \(w_{0}R_{\mathcal{D}_{1:M}}(\theta)+(1-w_{0})R_{\mathcal{D}_{1:M}^{0}}(\theta)\) as \(R_{2}\). Let's assume that \(R_{\tau}(\theta)-R_{1}\in[-a_{1},a_{1}]\) and \(R_{\tau}(\theta)-R_{2}\in[-a_{2},a_{2}]\) are two random variables with \(0\) mean, where \(a_{1}=\sup\{R_{\tau}(\theta)-R_{1}\}\) and \(a_{2}=\sup\{R_{\tau}(\theta)-R_{2}\}\). Then, based on Markov's inequality and Hoeffding's lemma, we have

\[P(|R_{\tau}(\theta)-R_{1}|\leq\epsilon)\geq 1-\frac{2e^{\frac{1}{2}t^{2}a_{1} ^{2}}}{e^{\epsilon t}},\qquad P(|R_{\tau}(\theta)-R_{2}|\leq\epsilon)\geq 1- \frac{2e^{\frac{1}{2}t^{2}a_{2}^{2}}}{e^{\epsilon t}}.\] (11)

\(\forall k\in[K]\), \(\mathcal{D}_{sk}\subset\mathcal{D}_{s}\) and \(\forall(\mathbf{x}_{i},y_{i})\in\mathcal{D}_{s}\), \(-\log P_{\theta}(y_{i}|\mathbf{x}_{i},\mathcal{D}^{\mathcal{C}})\leq R_{ \mathcal{D}_{1:M}}(\theta)\), we have \(R_{\mathcal{D}_{1:M}^{k}}(\theta)\leq R_{\mathcal{D}_{1:M}^{0}}(\theta)\). By combining the above inequalities over all \(k\in[K]\), we have

\[\sum_{k=1}^{K}w_{k}R_{\mathcal{D}_{1:M}^{k}}(\theta)\leq\sum_{k=1}^{K}w_{k}R_{ \mathcal{D}_{1:M}^{0}}(\theta)=(1-w_{0})R_{\mathcal{D}_{1:M}^{0}}(\mathcal{M}).\] (12)

Thus, \(\sup\{R_{\tau}(\theta)-R_{1}\}\leq\sup\{R_{1:M}(\theta)-R_{2}\}\), i.e. \(a_{1}\leq a_{2}\). Thus we have \(\delta_{1}\leq\delta_{2}\). 

According to the above theorem, we can achieve model stability by selecting only one easily predicted subset.

## 6 Experiments

We started with learning predictive functions on synthetic datasets, and then high-dimensional tasks, e.g., system identification on physics engines, image completion, and Bayesian optimization, were performed to evaluate the properties of the NP-related models.

### 1D Regression

To verify the proposed stable solution, we combined the stable solution with different baseline NP classes (CNP [9], NP [10], ANP [14],ConvCNP [11], ConvNP [6], and their bootstrapping versions [18]) and compared them on 1D regression task. Among them, BCNP, BNP, BANP, BConvCNP, and NConvNP are recently proposed stable strategies for NPs with Bootstrap. Specifically, the stochastic process (SP) initializing with a \(0\) mean Gaussian Process (GP) \(y^{(0)}\sim GP(0,k(\cdot,\cdot))\) indexed in the interval \(x\in[-2.0,2.0]\) were used to generate data, where the radial basis function kernel and Matern Kernel were adopted for model-data mismatch scenario. More detailed information can be obtained in the Appendix. We investigated the model performance in terms of different noise settings. We introduced Gaussian noise \(\mathcal{N}(0,1)\) and added noise to different proportions of the data, such as \(\{0\%,5\%,10\%,15\%\}\). Table 1 lists the average log-likelihoods comparison in terms of different noise proportions. The best result is marked in bold. First, we can see that if we adopt the robust solution in baselines, the model achieves the best results on all the datasets, showing the effectiveness of the 

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

### Predator-prey Models

Following [18] and [11], we consider the Lotka-Volterra model [30], which is used to describe the evolution of predator-prey populations. We first trained the models using simulated data generated from a Lotka-Volterra model and tested them on real-world data (Hudson's Bay hare-Iynx data), which is quite different from the simulated data and can be considered as a mismatch scenario. Table 5 lists the results on both simulated and real data. Similar to the previous observation, our stable version still outperforms the original version. Among stable versions, SConvNP achieves the best performance.

### Ablation Study

The key parameter in our stable solution is the number of hard predictable subsets \(K\). Taking SANP as an example, we investigated the average log-likelihood in terms of different \(K\) on the 1D regression task, as shown in Figure 3. We can see that SANP performs better as \(K\) increases, reaches the best value at around \(K=4\), and then becomes stable in performance as \(K\) grows larger. As proved in Theorem 5.3, optimization on \(\mathcal{D}_{1:M}\) and more than one hard predictable subset of \(\mathcal{D}_{1:M}\) can achieve more stable prediction. We also conducted different experiments to explore the impact of different \(w_{k}\). Taking the 1D regression task with RBF-GP data as an example, we set \(K=3\) and different \(w_{k}\) for experiments, as shown in Table 6. It can be seen from the table below that when different weights are set, the model using a stable strategy is better than the original model, and when the weight is set to be equal, its performance is optimal. In addition, when the value of \(w_{k}(k\geq 1)\) is significantly different from \(w_{0}\), such as \((0.625,0.125,0.125,0.125)\) and \((0.0625,0.3125,0.3125,0.3125)\), its performance is more significantly reduced compared to \((0.25,0.25,0.25,0.25)\), but it still has a significant improvement compared to the original non-stable model.

## 7 Conclusion and Future Work

In this paper, we provided theoretical guidelines for deriving stable solutions for NPs, which can obtain good generalization performance. Experiments demonstrated the proposed stable solution can help NPs to achieve more accurate and stable predictions. Although the theoretical analysis we give is based on regression models, it is still open to question whether this conclusion is appropriate for classification models. Therefore, we are interested in extending our theory, expecting it to apply to more different types of tasks.

\begin{table}
\begin{tabular}{c c c c c c c} \hline Weights & \((0.25,0.25,0.25,0.25)\) & \((0.40,0.22,0.2)\) & \((0.625,0.125,0.125,0.125)\) & \((0.1,0.3,0.3,0.3)\) & \((0.0625,0.3125,0.3125)\) \\ \hline SCNP & \((0.9255,0.4125)\) & \((0.9127,0.4019)\) & \((0.9035,0.3998)\) & \((0.9149,0.4086)\) & \((0.8927,0.3991)\) \\ SNP & \((0.8955,0.3925)\) & \((0.8877,0.3817)\) & \((0.8716,0.3809)\) & \((0.8831,0.3859)\) & \((0.8657,0.3775)\) \\ SANP & \((1.2831,0.5215)\) & \((1.2776,0.5187)\) & \((1.2738,0.5196)\) & \((1.2791,0.5203)\) & \((1.2712,0.5193)\) \\ SConvNP & \((1.3991,0.5996)\) & \((1.3916,0.5933)\) & \((1.3841,0.5915)\) & \((1.3934,0.5946)\) & \((1.3854,0.5919)\) \\ SConvNP & \((1.4036,0.6015)\) & \((1.3991,0.6004)\) & \((1.3931,0.5992)\) & \((1.4012,0.6015)\) & \((1.3957,0.5995)\) \\ \hline \end{tabular}
\end{table}
Table 6: Log-likelihood comparisons with different \(w_{k}\) on 1D regression task.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline Model & SANP & RANP & CANP & SANP & SConvNP & SConvNP & BConvNP & SConvNP \\ \hline Simulated-context & \(2.8013_{0.01}\) & \(2.9912_{0.000}\) & \(2.6217_{0.000}\) & \(2.9129_{0.000}\) & \(2.2608_{0.000}\) & \(2.2616_{0.000}\) & \(2.2952_{0.000}\) & \(2.6125_{0.000}\) & \(2.6231_{0.000}\) \\ Simulated-target & \(1.8265_{0.000}\) & \(1.8055_{0.000}\) & \(1.8848_{0.000}\) & \(1.8323_{0.000}\) & \(1.8513_{0.000}\) & \(1.9516_{0.000}\) & \(1.9232_{0.000}\) & \(1.9442_{0.000}\) & \(1.9624_{0.000}\) \\ Real-context & \(1.7234_{0.000}\) & \(1.8406_{0.000}\) & \(\textbf{1.8421}_{0.000}\) & \(\textbf{1.7596}_{0.000}\) & \(1.8052_{0.000}\) & \(\textbf{1.7874}_{0.000}\) & \(1.3842_{0.000}\) & \(1.8426_{0.000}\) & \(\textbf{1.7876}_{0.000}\) \\ Real-target & \(.7304_{0.000}\) & \(.5406_{0.000}\) & \(\textbf{-5.5257}_{0.000}\) & \(\textbf{-5.3414}_{0.000}\) & -5.1525_{0.000}\) & \(\textbf{-5.3513}_{0.000}\) & \(\textbf{-5.3515}_{0.000}\) & \(-5.2615_{0.000}\) & \(-5.2418_{0.000}\) \\ \hline \end{tabular}
\end{table}
Table 5: Predator-prey model

Figure 3: Log-likelihood (SANP) comparisons with different \(K\) on 1D regression task.

## Acknowledgement

This work was partly supported by the National Natural Science Foundation of China under Grant 62176020; the National Key Research and Development Program (2020AAA0106800); the Joint Foundation of the Ministry of Education (8091B042235); the Beijing Natural Science Foundation under Grant L211016; the Fundamental Research Funds for the Central Universities (2019JBZ110); and Chinese Academy of Sciences (OEIP-O-202004).

## References

* [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. _arXiv preprint arXiv:1607.06450_, 2016.
* [2] Olivier Bousquet and Andre Elisseeff. Algorithmic stability and generalization performance. _Advances in Neural Information Processing Systems_, pages 196-202, 2001.
* [3] Olivier Bousquet and Andre Elisseeff. Stability and generalization. _The Journal of Machine Learning Research_, 2:499-526, 2002.
* [4] Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist to handwritten letters. In _2017 International Joint Conference on Neural Networks (IJCNN)_, pages 2921-2926. IEEE, 2017.
* [5] Alec Farid and Anirudha Majumdar. Generalization bounds for meta-learning via pac-bayes and uniform stability. _Advances in Neural Information Processing Systems_, 34:2173-2186, 2021.
* [6] Andrew Foong, Wessel Bruinsma, Jonathan Gordon, Yann Dubois, James Requeima, and Richard Turner. Meta-learning stationary stochastic process prediction with convolutional neural processes. _Advances in Neural Information Processing Systems_, 33:8284-8295, 2020.
* [7] Yarin Gal, Rowan McAllister, and Carl Edward Rasmussen. Improving pilco with bayesian neural network dynamics models. In _Data-Efficient Machine Learning workshop, ICML_, volume 4, page 25, 2016.
* [8] Alexandre Galashov, Jonathan Schwarz, Hyunjik Kim, Marta Garnelo, David Saxton, Pushmeet Kohli, SM Eslami, and Yee Whye Teh. Meta-learning surrogate models for sequential decision making. _arXiv preprint arXiv:1903.11907_, 2019.
* [9] Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, Yee Whye Teh, Danilo Rezende, and SM Ali Eslami. Conditional neural processes. In _International Conference on Machine Learning_, pages 1704-1713. PMLR, 2018.
* [10] Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J Rezende, SM Eslami, and Yee Whye Teh. Neural processes. _arXiv preprint arXiv:1807.01622_, 2018.
* [11] Jonathan Gordon, Wessel P Bruinsma, Andrew YK Foong, James Requeima, Yann Dubois, and Richard E Turner. Convolutional conditional neural processes. _arXiv preprint arXiv:1910.13556_, 2019.
* [12] Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In _International Conference on Machine Learning_, pages 1225-1234. PMLR, 2016.
* [13] Makoto Kawano, Wataru Kumagai, Akiyoshi Sannai, Yusuke Iwasawa, and Yutaka Matsuo. Group equivariant conditional neural processes. _arXiv preprint arXiv:2102.08759_, 2021.
* [14] Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan Rosenbaum, Oriol Vinyals, and Yee Whye Teh. Attentive neural processes. _arXiv preprint arXiv:1901.05761_, 2019.
* [15] Mingyu Kim, Kyeongryeol Go, and Se-Young Yun. Neural processes with stochastic attention: Paying more attention to the context dataset. In _International Conference on Learning Representations_, 2022.

* [16] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* [17] Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. Accurate uncertainties for deep learning using calibrated regression. In _International Conference on Machine Learning_, pages 2796-2804. PMLR, 2018.
* [18] Juho Lee, Yoonho Lee, Jungtaek Kim, Eunho Yang, Sung Ju Hwang, and Yee Whye Teh. Bootstrapping neural processes. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 6606-6615. Curran Associates, Inc., 2020.
* [19] Dongsheng Li, Chao Chen, Qin Lv, Junchi Yan, Li Shang, and Stephen Chu. Low-rank matrix approximation with stability. In _International Conference on Machine Learning_, pages 295-303. PMLR, 2016.
* [20] Xixun Lin, Jia Wu, Chuan Zhou, Shirui Pan, Yanan Cao, and Bin Wang. Task-adaptive neural process for user cold-start recommendation. In _Proceedings of the Web Conference_, pages 1306-1316, 2021.
* [21] Huafeng Liu, Liping Jing, Dahai Yu, Mingjie Zhou, and Michael Ng. Learning intrinsic and extrinsic intentions for cold-start recommendation with neural stochastic processes. In _Proceedings of the 30th ACM International Conference on Multimedia_, pages 491-500, 2022.
* [22] Tongliang Liu, Dacheng Tao, Mingli Song, and Stephen J Maybank. Algorithm-dependent generalization bounds for multi-task learning. _IEEE transactions on pattern analysis and machine intelligence_, 39(2):227-241, 2016.
* [23] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In _Proceedings of International Conference on Computer Vision (ICCV)_, December 2015.
* [24] Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask representation learning. _Journal of Machine Learning Research_, 17(81):1-32, 2016.
* [25] Carl Edward Rasmussen. Gaussian processes in machine learning. In _Summer school on machine learning_, pages 63-71. Springer, 2003.
* [26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in neural information processing systems_, pages 5998-6008, 2017.
* [27] Qi Wang and Herke Van Hoof. Doubly stochastic variational inference for neural processes with hierarchical latent variables. In _International Conference on Machine Learning_, pages 10018-10028. PMLR, 2020.
* [28] Qi Wang and Herke van Hoof. Learning expressive meta-representations with mixture of expert neural processes. In _Advances in neural information processing systems_, 2022.
* [29] Andre Wibisono, Lorenzo Rosasco, and Tomaso Poggio. Sufficient conditions for uniform stability of regularization algorithms. 2009.
* [30] Darren J Wilkinson. _Stochastic modelling for systems biology_. CRC press, 2018.
* [31] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and Alexander Smola. Deep sets. _arXiv preprint arXiv:1703.06114_, 2017.
* [32] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.

Inductive biases

Here, we revisit some properties, which would help us understand NPs. First, we give a concept of Permutation Invariant Function which is the basic property of stochastic process, e.g., NPs.

**Definition A.1**.: (Permutation Invariant Function) A function \(f(\cdot):\times_{i}^{N}\mathbb{R}^{D}\rightarrow\mathbb{R}^{d}\) mapping a set of data points \(\{\mathbf{x}_{i}\}_{i=1}^{N}\) is Permutation Invariant Function if

\[\mathbf{x}=[\mathbf{x}_{1},\mathbf{x}_{2},\cdots,\mathbf{x}_{N}]\to f =[f_{1}(\mathbf{x}_{\pi(1:N)}),f_{2}(\mathbf{x}_{\pi(1:N)}),\cdots,f_{d}( \mathbf{x}_{\pi(1:N)})],\] (13)

where \(x_{i}\in\mathbb{R}^{D}\) and the function output is a \(d\) dimensional vector. Operation \(\pi:[1,2,\cdots,N]\rightarrow[\pi_{1},\pi_{2},\cdots,\pi_{N}]\) is a permutation set over the order of elements in the set.

**Definition A.2**.: (Permutation Equivariant Function) A function \(f(\cdot):\times_{i}^{N}\mathbb{R}^{D}\rightarrow\mathbb{R}^{N}\) mapping a set of data points \(\{\mathbf{x}_{i}\}_{i=1}^{N}\) is Permutation Invariant Function if

\[\mathbf{X}_{\pi}=[\mathbf{x}_{\pi_{1}},\mathbf{x}_{\pi_{2}},\cdots,\mathbf{x }_{\pi_{N}}]\to f_{\pi}=\pi\circ f(\mathbf{x}_{1:N}),\] (14)

where the function output contains \(N\) elements keeping the order of inputs.

Permutation Equivariant Function keeps the order of elements in the output consistent with that in the input under any permutation operation \(\pi\). Permutation invariant functions are candidate functions for learning embeddings of a set or other order uncorrelated data structure \(\{\mathbf{x}_{i}\}_{i=1}^{N}\), and the invariant property is easy to be verified. Here, we give a mean operation structure over the output

\[F(\mathbf{X}_{\pi(1:N)})=\left(\frac{1}{N}\sum_{i=1}^{N}\phi_{1}(\mathbf{x}_{i }),\frac{1}{N}\sum_{i=1}^{N}\phi_{2}(\mathbf{x}_{i}),\cdots,\frac{1}{N}\sum_{ i=1}^{N}\phi_{D}(\mathbf{x}_{i})\right)\] (15)

## Appendix B Model Architecture

We show the architectural details of the CNP, NP, and ANP models used for the 1D and 2D function regression experiments. The neural process aims to learn a stochastic process (random function) mapping target features \(\mathbf{x}_{i}\) to prediction \(y_{i}\) given the context set \(\mathcal{D}^{\mathcal{C}}\) as training data (a realization from the stochastic process), i.e., learning

\[\log P\left(\mathbf{y}^{\mathcal{T}}|\mathbf{X}^{\mathcal{T}},\mathcal{D}^{ \mathcal{C}}\right)=\log P\left(\mathbf{y}|\mathbf{X},\mathcal{D}^{\mathcal{C }}\right)=\sum_{i=1}^{N}P\left(y_{i}|\mathbf{x}_{i},\mathcal{D}^{\mathcal{C}} \right).\] (16)

Conditional neural process (CNP) [9] describes \(P\left(y_{i}|\mathbf{x}_{i},\mathcal{D}^{\mathcal{C}}\right)\) with a deterministic neural network taking \(\mathcal{D}^{\mathcal{C}}\) to output the parameters of \(P\left(y_{i}|\mathbf{x}_{i},\mathcal{D}^{\mathcal{C}}\right)\). CNP consists of an encoder \(f_{\mathrm{enc}}(\cdot)\), an aggregator \(f_{\mathrm{agg}}(\cdot)\) and a decoder \(f_{\mathrm{dec}}(\cdot)\); the encoder summarizes \(\mathcal{D}^{\mathcal{C}}\) and \(\mathbf{x}_{i}\) into latent representations \([\mathbf{r}_{1},\cdots,\mathbf{r}_{|\mathcal{C}|}]\in\mathbb{R}^{|\mathcal{C}| \times d}\) via permutation-invariant neural network [31], where \(d\) is the number of latent dimensions, and aggregator summarizes the encoded context features to a single representation \(\mathbf{r}^{\mathcal{C}}\), and decoder takes as input the aggregated representations \(\mathbf{r}^{\mathcal{C}}\) and \(\mathbf{x}_{i}\) and output the single output-specific mean \(\mu_{i}\) and variance \(\sigma_{i}^{2}\) for the corresponding value of \(y_{i}\).

\[\mathbf{r}_{i} =f_{\mathrm{enc}}\left(\mathbf{x}_{i},y_{i}\right),\quad i\in \mathcal{C}\] (17) \[\mathbf{r}^{\mathcal{C}} =\frac{1}{|\mathcal{C}|}\sum_{i\in\mathcal{C}}\mathbf{r}_{i}\] \[\phi =f_{\mathrm{agg}}(\mathbf{r}^{\mathcal{C}})\] \[(\mu_{i},\sigma_{i}) =f_{\mathrm{dec}}(\phi,\mathbf{x}_{i}),\quad p\left(y_{i}|\mathbf{ x}_{i},\mathcal{D}^{\mathcal{C}}\right)=\mathcal{N}\left(y_{i};\mu_{i},\sigma_{i}^{2} \right)\quad i\in\mathcal{T}\]

where \(f_{\mathrm{enc}}(\cdot)\) and \(f_{\mathrm{dec}}(\cdot)\) are feed-forward neural networks. The decoder output \(\mu_{i}\) and variance \(\sigma_{i}^{2}\) are predicted mean and variance. We use _Gaussian_ distribution \(\mathcal{N}(y_{i};\mu_{i},\sigma_{i}^{2})\) as predictive distribution. CNP is trained to maximize the expected likelihood \(\mathbb{E}_{P(\mathcal{T})}[P\left(y_{i}|\mathbf{x}_{i},\mathcal{D}^{\mathcal{C }}\right)]\).

Neural process [10] further models functional uncertainty using a global latent variable. Unlike CNP, which maps a context into a deterministic representation \(\widetilde{\mathbf{r}}_{i}\), NP encoders a context into a _Gaussian_ latent variable \(\mathbf{z}\), giving additional stochasticity in function construction. Following [14], we consider an NP with both a deterministic path and latent path, where the deterministic path models the overall skeleton of the function \(\widetilde{\mathbf{r}}_{i}\), and the latent path models the functional uncertainty:

\[\begin{split}&\mathbf{r}_{i}=f^{(1)}_{\mathrm{enc}}\left(\mathbf{x} _{i},y_{i}\right),\quad i\in\mathcal{C}\\ &\mathbf{r}^{\mathcal{C}}=\frac{1}{|\mathcal{C}|}\sum_{i\in \mathcal{C}}\mathbf{r}_{i}\\ &\phi=f_{\mathrm{agg}}(\mathbf{r})\\ &(\mu_{z},\sigma_{z})=f^{(2)}_{\mathrm{enc}}\left(\mathcal{D}^{ \mathcal{C}}\right),\quad q(\mathbf{z}|\mathcal{D}^{\mathcal{C}})=\mathcal{N}( \mathbf{z};\mu_{z},\sigma_{z}^{2})\\ &(\mu_{i},\sigma_{i})=f_{\mathrm{dec}}(\phi,\mathbf{z},\mathbf{x }_{i}),\quad p\left(y_{i}|\mathbf{x}_{i},\mathbf{z},\mathcal{D}^{\mathcal{C}} \right)=\mathcal{N}\left(y_{i};\mu_{i},\sigma_{i}^{2}\right)\quad i\in \mathcal{T}\end{split}\] (18)

with \(f^{(1)}_{\mathrm{enc}}(\cdot)\) and \(f^{(2)}_{\mathrm{enc}}(\cdot)\) having the same structure as \(f_{\mathrm{enc}}(\cdot)\) in Eq.(17). In this scenario, the conditional distribution is lower bounded as:

\[\log P\left(\mathbf{y}|\mathbf{X},\mathcal{D}^{\mathcal{C}}\right)\geq\sum_{i =1}^{N}\mathbb{E}_{q(\mathbf{z}|\mathcal{D}^{\mathcal{C}})}\left[\log\frac{P \left(y_{i}|\mathbf{x}_{i},\mathbf{z},\mathcal{D}^{\mathcal{C}}\right)P( \mathbf{z}|\mathcal{D}^{\mathcal{C}})}{q(\mathbf{z}|\mathbf{X},\mathbf{y})} \right].\] (19)

We further approximate \(q(\mathbf{z}|\mathcal{D}^{\mathcal{C}})\approx P(\mathbf{z}|\mathcal{D}^{ \mathcal{C}})\) and train the model by maximizing this expected lower bound over tasks. Furthermore, ANP introduces attention mechanisms into NP to resolve the issue of under-fitting.

The architectural details of the CNP, NP, and ANP are the same as in [14]. Here we give the detailed architectures of the encoder and decoder of NPs.

### Encoder without attention

Encoder focuses on learning embeddings for each data point in the context set, and the basic component is multi-layer perceptron, which is defined by

\[\textsc{\textsc{Mlp}}(l,d_{in},d_{h},d_{out})=\textsc{\textsc{Linear}}(d_{h}, d_{out})\circ\underbrace{(\textsc{Relu}\circ\textsc{Linear}(d_{h},d_{h})\circ \cdots)}_{\times(l-2)}\circ\textsc{\textsc{Linear}}(d_{h},d_{in})\] (20)

where \(l\) is the number of layers, \(d_{in}\), \(d_{h}\) and \(d_{out}\) are dimensinalities of inputs, hidden unites and outputs. Here \(\textsc{Relu}(\cdot)\) is adapted as activation function.

The encoder in Vanilla CNP uses a deterministic encoder which focuses on learning embeddings for each data point in context set.

\[\begin{split}&\mathbf{r}_{i}=\textsc{\textsc{Mlp}}(l_{e1},d_{x}+d_{y}, d_{h},d_{h})([\mathbf{x}_{i},y_{i}]),\\ &\mathbf{r}^{\mathcal{C}}=\sum_{i\in\mathcal{C}}\mathbf{r}_{i}, \quad\phi=\textsc{\textsc{Mlp}}(l_{e2},d_{h},d_{h})(\mathbf{r}^{\mathcal{C}}) \end{split}\] (21)

where \(d_{x}\) and \(d_{y}\) are the dimensionalities of \(\mathbf{x}_{i}\) and \(y_{i}\).

To follow the encoder structure in NP, we introduce another encoder aligned with original deterministic encoder to permit the same number parameters, i.e.,

\[\begin{split}&\mathbf{r}_{i}^{(1)}=\textsc{\textsc{Mlp}}(l_{e1},d_{x} +d_{y},d_{h},d_{h})([\mathbf{x}_{i},y_{i}])\\ &\mathbf{r}_{\mathcal{C}}^{(1)}=\sum_{i\in\mathcal{C}}\mathbf{r}_{ i}^{(1)},\quad\phi_{1}=\textsc{\textsc{Mlp}}(l_{e2},d_{h},d_{h})(\mathbf{r}_{ \mathcal{C}}^{(1)})\\ &\mathbf{r}_{i}^{(2)}=\textsc{\textsc{Mlp}}(l_{e1},d_{x}+d_{y}, d_{h},d_{h})([\mathbf{x}_{i},y_{i}])\\ &\mathbf{r}_{\mathcal{C}}^{(2)}=\sum_{i\in\mathcal{C}}\mathbf{r}_{ i}^{(2)},\quad\phi_{2}=\textsc{\textsc{Mlp}}(l_{e2},d_{h},d_{h})(\mathbf{r}_{ \mathcal{C}}^{(2)})\\ &\phi=[\phi_{1},\phi_{2}]\end{split}\] (22)The encoder in NP contains a deterministic path and a latent path, i.e.,

\[\begin{split}\mathbf{r}_{i}^{(1)}&=\texttt{{Mlp}}(l_{ de1},d_{x}+d_{y},d_{h},d_{h})([\mathbf{x}_{i},y_{i}])\\ \mathbf{r}_{\mathcal{C}}^{(1)}&=\sum_{i\in\mathcal{C }}\mathbf{r}_{i}^{(1)},\quad\phi=\texttt{{Mlp}}(l_{de2},d_{h},d_{h})(\mathbf{ r}_{\mathcal{C}}^{(1)})\\ \mathbf{r}_{i}^{(2)}&=\texttt{{Mlp}}(l_{la1},d_{x}+d _{y},d_{h},d_{h})([\mathbf{x}_{i},y_{i}])\\ \mathbf{r}_{\mathcal{C}}^{(2)}&=\sum_{i\in\mathcal{C }}\mathbf{r}_{i}^{(2)},\qquad[\mu_{z},\sigma_{z}^{\prime}]=\texttt{{Mlp}}(l_{ la2},d_{h},d_{h})(\mathbf{r}_{\mathcal{C}}^{(2)})\\ \sigma_{z}&=0.1+0.9\cdot\texttt{{Sigmoid}}(\sigma_{z }^{\prime}),\quad\mathbf{z}\sim\mathcal{N}(\mu_{z},\mathrm{diag}(\sigma_{z}^{2} )).\end{split}\] (23)

In this case, the encoder outputs deterministic representation \(\phi\) and latent representation \(\mathbf{z}\).

### Encoder with attention

The attention mechanism is widely used in NPs, Specifically, multi-head attention [26] is adapted, which is defined by

\[\begin{split}\mathbf{Q}^{\prime}&=\{\texttt{{ Linear}}(d_{q},d_{out})(\mathbf{q})\}_{\mathbf{q}\in\mathbf{Q}},\quad\{\mathbf{Q}^{ \prime}_{i}\}_{i=1}^{n_{head}}=\texttt{{Split}}(\mathbf{Q}^{\prime},n_{head}), \\ \mathbf{K}^{\prime}&=\{\texttt{{Linear}}(d_{k},d_{ out})(\mathbf{k})\}_{\mathbf{k}\in\mathbf{K}},\quad\{\mathbf{K}^{\prime}_{i}\}_{i=1}^{n_{head}}= \texttt{{Split}}(\mathbf{K}^{\prime},n_{head}),\\ \mathbf{V}^{\prime}&=\{\texttt{{Linear}}(d_{v},d_{ out})(\mathbf{v})\}_{\mathbf{v}\in\mathbf{V}},\quad\{\mathbf{V}^{\prime}_{i} \}_{i=1}^{n_{head}}=\texttt{{Split}}(\mathbf{V}^{\prime},n_{head}),\\ \mathbf{H}_{i}&=\texttt{{Softmax}}\left(\mathbf{Q}^{ \prime}_{i}(\mathbf{K}^{\prime}_{i})^{\top}/\sqrt{d_{out}}\right)\mathbf{V}^{ \prime}_{i},\quad\mathbf{H}=\texttt{{Concat}}\left(\{\mathbf{H}_{i}\}_{i=1}^{ n_{head}}\right)\\ \mathbf{H}^{\prime}&=\texttt{{LayerNorm}}(\mathbf{Q}^{ \prime}+\mathbf{H})\\ \texttt{{Mha}}(d_{out})(\mathbf{Q},\mathbf{K},\mathbf{V})& =\texttt{{LayerNorm}}(\mathbf{H}^{\prime}+\texttt{{Relu}}(\texttt{{ Linear}}(d_{out},d_{out})))\end{split}\] (24)

where \(d_{q},d_{v},d_{k}\) are the dimensionalities of query \(\mathbf{Q}\), key \(\mathbf{K}\), and value \(\mathbf{V}\), respectively. \(n_{head}\) is the number of head. Here Layer normalization [1]\(\texttt{{LayerNorm}}(\cdot)\) is adapted. It is easy to derive self-attention by setting \(\mathbf{Q}=\mathbf{K}=\mathbf{V}\), i.e.,

\[\texttt{{Sa}}(d_{out}))(\mathbf{X})=\texttt{{Mha}}(d_{out})(\mathbf{X},\mathbf{ X},\mathbf{X})\] (25)

For CNP, the encoder with attention still contains two deterministic paths,

\[\begin{split} f_{qk}&=\texttt{{Mlp}}(l_{qk},d_{x},d_ {h},d_{h})\\ \mathbf{Q}&=f_{qk}(\mathbf{x}_{i}),\quad i\in \mathcal{T}\\ \mathbf{K}&=\{f_{qk}(\mathbf{x}_{i})\},\quad i\in \mathcal{C}\\ \mathbf{V}&=\texttt{{Sa}}(d_{h}).(\{\texttt{{Mlp}}(l_{v },d_{x}+d_{y},d_{h},d_{h})([\mathbf{x}_{i},y_{i}])\}_{i\in\mathcal{C}})\\ \phi_{1}&=\texttt{{Mha}}(d_{h})(\mathbf{Q},\mathbf{K}, \mathbf{V})\\ \mathbf{H}&=\texttt{{Sa}}(d_{h})\left(\{\texttt{{Relu}} \circ\texttt{{Mlp}}(l_{e1},d_{x}+d_{y},d_{h},d_{h})([\mathbf{x}_{i},y_{i}])\}_{ i\in\mathcal{C}}\right)\\ \phi_{2}&=\texttt{{Mlp}}(l_{e},d_{h},d_{h})\left( \frac{1}{|\mathcal{C}|}\sum_{i\in\mathcal{C}}\mathbf{h}_{i}\right)\\ \phi&=[\phi_{1},\phi_{2}]\end{split}\] (26)

Similarly, encoder with attention in NP contains a deterministic path and a latent path, i.e.,

\[\begin{split} f_{qk}&=\texttt{{Mlp}}(l_{qk},d_{x},d _{h},d_{h})\\ \mathbf{Q}&=f_{qk}(\mathbf{x}_{i}),\quad i\in \mathcal{T}\\ \mathbf{K}&=\{f_{qk}(\mathbf{x}_{i})\},\quad i\in \mathcal{C}\\ \mathbf{V}&=\texttt{{Sa}}(d_{h}).(\{\texttt{{Mlp}}(l_{v },d_{x}+d_{y},d_{h},d_{h})([\mathbf{x}_{i},y_{i}])\}_{i\in\mathcal{C}})\\ \phi&=\texttt{{Mha}}(d_{h})(\mathbf{Q},\mathbf{K}, \mathbf{V})\end{split}\] (27)and

\[\mathbf{H} =\texttt{Sa}(d_{h})\left(\{\texttt{Relu}\circ\texttt{MLP}(l_{e1},d_{ x}+d_{y},d_{h},d_{h})([\mathbf{x}_{i},y_{i}])\}_{i\in\mathcal{C}}\right)\] (28) \[[\mu_{z},\sigma^{\prime}_{z}] =\texttt{MLP}(l_{a},d_{h},d_{h})\left(\frac{1}{|\mathcal{C}|} \sum_{i\in\mathcal{C}}\mathbf{h}_{i}\right)\] \[\sigma_{z} =0.1+0.9\cdot\texttt{Sigmoid}(\sigma^{\prime}_{z}),\] \[\mathbf{z} \sim\mathcal{N}(\mu_{z},\mathrm{diag}(\sigma^{2}_{z})).\]

### Decoder

The decoder focuses on predicting output for target points based on the encoder's outputs \(\phi\). For target point \(\{\mathbf{x}_{i}\}_{i\in\mathcal{T}}\), the decoder of CNP is defined by

\[[\mu_{i},\sigma^{\prime}_{i}] =\texttt{MLP}(d_{dec},2d_{h}+d_{x},d_{h},2d_{y})[\phi,\mathbf{x }_{i}],\quad i\in\mathcal{T}\] (29) \[\sigma_{i} =0.1+0.9\cdot\texttt{Softplus}(\sigma^{\prime}_{i})\] \[y_{i} \sim\mathcal{N}(\mu_{i},\sigma_{i})\]

Decoder of NP is defined by

\[[\mu_{i},\sigma^{\prime}_{i}] =\texttt{MLP}(d_{dec},d_{h}+d_{z}+d_{x},d_{h},2d_{y})[\phi, \mathbf{x}_{i},\mathbf{z}],\quad i\in\mathcal{T}\] (30) \[\sigma_{i} =0.1+0.9\cdot\texttt{Softplus}(\sigma^{\prime}_{i})\] \[y_{i} \sim\mathcal{N}(\mu_{i},\sigma_{i})\]

## Appendix C Implementation Details and Experiments

For CNP [9], BCNP [18], and our SCNP, we apply the encoder with attention described in Eq (26) and decoder described in Eq (29). For NP [10], ANP [14], BNP [18], BANP [18] and our SNP and SANP models, we apply encoder with attention described in Eq (27) and (28), and decoder described in Eq (30).

### 1D Regression

For synthetic 1D regression experiments, the neural architectures for CNP, NP, ANP, BCNP, BNP, BANP, and our SCNP/SNP/SANP refer to Appendix B. The number of hidden units is \(d_{h}=128\) and latent representation \(d_{z}=128\). The number of layers are \(l_{e}=l_{de}=l_{la}=l_{qk}=l_{v}=2\).

We generate datasets for synthetic 1D regression. Specifically, the stochastic process (SP) initializes with a \(0\) mean Gaussian Process (GP) \(y^{(0)}\sim GP(0,k(\cdot,\cdot))\) indexed in the interval \(x\in[-2.0,2.0]\), where the radial basis function kernel \(k(x,x^{{}^{\prime}})=\sigma^{2}\exp(-\|x-x^{\prime}\|^{2}/2l^{2})\) with \(s\sim U(0.1,1.0)\) and \(\sigma\sim U(0.1,0.6)\). Furthermore, GP with Matern Kernel is adopted for model-data mismatch scenario, which is defined as \(k(x,x^{\prime})=\sigma^{2}(1+\sqrt{5}d/l+5d^{2}/(3l^{2}))\exp(-\sqrt{5}d/l)\) and \(d=\|x-x^{\prime}\|\) with \(s\sim U(0.1,1.0)\) and \(\sigma\sim U(0.1,0.6)\). For a fair comparison, we set the same data generation, training, and testing for all models.

We trained all models for \(100,000\) steps with each step computing updates with a batch containing \(100\) tasks. We used the Adam optimizer with an initial learning rate \(5\cdot 10^{-4}\) and decayed the learning rate using Cosine annealing scheme for baselines. For SCNP/SNP/SANP, we set \(K=3\). The size of the context \(\mathcal{C}\) was drawn as \(|\mathcal{C}|\sim U(3,200)\). Testings were done for \(3,000\) batches with each batch containing \(16\) tasks (\(48,000\) tasks in total).

We investigate the model stability from the size of the context set and the percent of added noise. First, we conduct experiments on different size of context set, i.e., \(|\mathcal{C}|\in\{20,50,100,200\}\). Table 7 shows the Average Log-likelihoods performance comparison between different methods in terms of different context size. We can see that the performance becomes better with the increasing of \(|\mathcal{C}|\) and NPs with stable solution still achieve better performance. Second, we investigate the model performance in terms of different noise setting. Here we introduce Gaussian noise \(\mathcal{N}(0,1)\) and add noise to different proportions of the data, such as \(\{0\%,5\%,10\%,15\%\}\). Table 8 lists the Average Log-likelihoods performance comparison in terms of different noise proportions.

[MISSING_PAGE_FAIL:17]

[MISSING_PAGE_FAIL:18]

[MISSING_PAGE_FAIL:19]

[MISSING_PAGE_EMPTY:20]