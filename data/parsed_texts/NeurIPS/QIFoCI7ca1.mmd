# Causal normalizing flows: from theory to practice

Adrian Javaloy

Correspondence to ajavaloy@cs.uni-saarland.de. Department of Computer Science of Saarland University, Saarbrucken, Germany

Pablo Sanchez-Martin

Isabel Valera

###### Abstract

In this work, we deepen on the use of normalizing flows for causal inference. Specifically, we first leverage recent results on non-linear ICA to show that causal models are identifiable from observational data given a causal ordering, and thus can be recovered using autoregressive normalizing flows. Second, we analyse different design and learning choices for _causal normalizing flows_ to capture the underlying causal data-generating process. Third, we describe how to implement the _do-operator_ in causal NFs, and thus, how to answer interventional and counterfactual questions. Finally, in our experiments, we validate our design and training choices through a comprehensive ablation study; compare causal NFs to other approaches for approximating causal models; and empirically demonstrate that causal NFs can be used to address real-world problems--where mixed discrete-continuous data and partial knowledge on the causal graph is the norm. The code for this work can be found at https://github.com/psanch21/causal-flows.

## 1 Introduction

Deep learning is increasingly used for causal reasoning, that is, for finding the underlying causal relationships among the observed variables (_causal discovery_), and answering _what-if_ questions (_causal inference_) from available data [28]. Our focus in this paper is to solve causal inference problems using only observational data and (potentially partial) knowledge on the causal graph of the underlying structural causal model (SCM). This is exemplified in Fig. 1, where our proposed framework is able to estimate the (unobserved) causal effect of externally intervening on the sensitive attribute (red and yellow distributions), _using solely observed data (blue distribution) and partial information about the causal relationship between features_.

In this context, previous works have mostly relied on different deep neural networks (DNNs)--e.g., normalizing flows (NFs) [23, 26, 27], generative adversarial networks (GANs) [20, 39], variational autoencoders (VAEs) [15, 40], Gaussian processes (GPs) [15], or denoising diffusion probabilistic models (DDPMs) [2]--to iteratively estimate the conditional distribution of each observed variable given its causal parents, thus using an independent DNN per observed variable. Hence, to predict the effect of an intervention in the causal data-generating process, these approaches fix the value of the intervened variables when computing the new value for their children. However, they may also suffer

Figure 1: Observational and interventional distributions of the categorical variable _checking account_ of the German Credit dataset [8], and their estimated values according to a causal normalizing flow. \(\mathbf{x}_{S}\) is a binary variable representing the usersâ€™ sex.

from error propagation--which worsens with long causal paths--and a high number of parameters, which is addressed in practice with ad-hoc parameter amortization techniques [26; 27]. Moreover, several approaches also rely on implicit distributions [2; 20; 27; 32], and thus do not allow evaluating the learnt distribution.

In contrast, and similar to [16; 32; 34; 41], we here aim at learning the full causal-generating process using a single DNN and, in particular, using a _causal normalizing flow_. To this end, we first theoretically demonstrate that causal NFs are a natural choice to approximate a broad class of causal data-generating processes (SS3). Then, we design causal NFs that inherently satisfy the necessary conditions to capture the underlying causal dependencies (SS4), and introduce an implementation of the do-operator that allows us to efficiently solve causal inference tasks (SS5). Importantly, our causal NF framework allows us to deal with mixed continuous-discrete data and partial knowledge on the causal graph, which is key for real-world applications. Finally, we empirically validate our findings and show that causal NFs outperform competing methods also using a single DNN to approximate the causal data-generating process (SS6).

Related workTo the best of our knowledge, the closest works to ours are [16; 34; 41], as they all capture the whole causal data-generating process using a single DNN. Our approach generalizes the result from Khemakhem et al. [16], which also relies on autoregressive normalizing flows (ANF), but only considers affine ANFs and data with additive noise. In contrast, our work provides a tighter connection between ANFs and SCMs (affine or not), more general identifiability results, and sound ways to both embed causal knowledge in the ANF, and to apply the do-operator. Another relevant line of works connect SCMs with GNNs [32; 41], and despite making little assumptions on the underlying SCM, they lack identifiability guarantees, and interventions on the GNN are performed by severing the graph, which we show in App. C may not work in general. Nevertheless, it is worth noting that the way we use \(\bm{A}\) in the network design (SS4) is inspired by these works.

## 2 Preliminaries and background

### Structural causal models, interventions, and counterfactuals

A structural causal model (SCM) [28] is a tuple \(\mathcal{M}=(\mathbf{\tilde{f}},P_{\mathbf{u}})\) describing a data-generating process that transforms a set of \(d\) exogenous random variables, \(\mathbf{u}\sim P_{\mathbf{u}}\), into a set of \(d\) (observed) endogenous random variables, \(\mathbf{x}\), according to \(\mathbf{\tilde{f}}\). Specifically, the endogenous variables are computed as follows:

\[\mathbf{u}\coloneqq(\mathtt{u}_{1},\mathtt{u}_{2},\ldots,\mathtt{u}_{d})\sim P _{\mathbf{u}}\,,\qquad\quad\mathtt{x}_{i}=\tilde{f}_{i}(\mathtt{x}_{\mathtt{ pu}_{i}},\mathtt{u}_{i})\,,\qquad\qquad\text{for $i=1,2,\ldots,d$}\,.\] (1)

In other words, each \(i\)-th component of \(\mathbf{\tilde{f}}\) maps the \(i\)-th exogenous variable \(\mathtt{u}_{i}\) to the \(i\)-th endogenous variable \(\mathtt{x}_{i}\), given the subset of the endogenous variables that directly cause \(\mathtt{x}_{i}\), \(\mathtt{x}_{\mathtt{pu}_{i}}\) (causal parents).

An SCM also induces a causal graph, a powerful tool to reason about the causal dependencies of the system. Namely, the causal graph of an SCM \(\mathcal{M}=(\mathbf{\tilde{f}},P_{\mathbf{u}})\) is the directed graph that describes the functional dependencies of the causal mechanism. We can define the _adjacency matrix of the causal graph_ as \(\bm{A}\coloneqq\nabla_{\mathbf{\tilde{f}}}(\mathbf{x},\mathbf{u})\neq\mathbf{0}\), where \(\mathbf{0}\) is the constant zero function, and the comparisons are made elementwise. Furthermore, the direct causes of the \(i\)-th variable (\(\mathtt{pa}_{i}\) in Eq. 1) are the _parent_ nodes of the \(i\)-th node in \(\bm{A}\), and the _ancestors_ of this node (which we denoted by \(\mathrm{an}_{i}\)) are its (in)direct causes. See Fig. 2 for an example of a causal chain.

In the case that \(\bm{A}\) is acyclic, we can pick a causal ordering describing which variables do _not_ cause others, and which ones _may_ cause them. Namely, a permutation \(\pi\) is said to be a _causal ordering_ of an SCM \(\mathcal{M}\) if, for every \(\mathtt{x}_{i}\) that directly causes \(\mathtt{x}_{j}\), we have \(\pi(i)<\pi(j)\). Note that this definition equals that of a topological ordering and, without loss of generality, we will assume throughout this work that the variables are sorted according to a causal ordering.

Importantly, besides describing the (observational) data-generating process, SCMs enable _causal inference_ by allowing us to answer _what-if_ questions regarding: i) how the distribution over the observed variables would be if we force a fixed value on one of them (_interventional queries_); and ii) what would have happened to a specific observation, if one of its dimensions would have taken a different value (_counterfactual queries_).

Figure 2: Causal graph, and its causal ordering \(\pi\) and adjacency matrix \(\bm{A}\).

Structural equivalenceTo reason about causal dependencies, we introduce the notion of structural equivalence. We say that two matrices \(\bm{S}\) and \(\bm{R}\) are _structurally equivalent_, denoted \(\bm{S}\equiv\bm{R}\), if both matrices have zeroes exactly in the same positions. Similarly, we say that \(\bm{S}\) is _structurally sparser_ than \(\bm{R}\), denoted as \(\bm{S}\preceq\bm{R}\), if whenever an element of \(\bm{R}\) is zero, the same element of \(\bm{S}\) is zero.

### Autoregressive normalizing flows

Normalizing flows (NFs) [25] are a model family that express the probability density of a set of observations using the change-of-variables rule. Given an observed random vector \(\mathbf{x}\) of size \(d\), a normalizing flow is a neural network with parameters \(\bm{\theta}\) that takes \(\mathbf{x}\) as input, and outputs

\[T_{\bm{\theta}}(\mathbf{x})\eqqcolon\mathbf{u}\sim P_{\mathbf{u}}\quad\text{ with log-density }\quad\log p(\mathbf{x})=\log p(T_{\bm{\theta}}(\mathbf{x}))+\log\lvert\det( \nabla_{\mathbf{x}}T_{\bm{\theta}}(\mathbf{x}))\rvert\,,\] (2)

where \(P_{\mathbf{u}}\) is a base distribution that is easy to evaluate and sample from. Since Eq. 2 provides the log-likelihood expression, it naturally leads to the use of maximum likelihood estimation (MLE) [1] for learning the network parameters \(\bm{\theta}\). While many approaches have been proposed in the literature [25], here we focus on autoregressive normalizing flows (ANFs) [18; 24]. Specifically, in ANFs the \(i\)-th output of each layer \(l\) of the network, denoted by \(\bm{z}_{i}^{l}\), is computed as

\[\bm{z}_{i}^{l}\coloneqq\tau_{i}^{l}(\bm{z}_{i}^{l-1};\bm{h}_{i}^{l})\,,\quad \text{where}\quad\bm{h}_{i}^{l}\coloneqq c_{i}^{l}(\bm{z}_{1:i-1}^{l-1})\,,\] (3)

and where \(\tau_{i}\) and \(c_{i}\) are termed the transformer and the conditioner, respectively. The transformer is a strictly monotonic function of \(\bm{z}_{i}^{l-1}\), while the conditioner can be arbitrarily complex, yet it only takes the variables preceding \(\bm{z}_{i}\) as input. As a result, ANFs have triangular Jacobian matrices, \(\nabla_{\mathbf{x}}T_{\bm{\theta}}(\mathbf{x})\).

## 3 Causal normalizing flows

Problem statementAssume that we have a sequence of i.i.d. observations \(\bm{X}=\{\bm{x}_{1},\bm{x}_{2},\ldots,\bm{x}_{N}\}\) generated according to an unknown SCM \(\mathcal{M}\), from which we have partial knowledge of its causal structure. Specifically, we know at least its causal ordering \(\pi\), and at most the whole causal graph \(\bm{A}\). Our objective in this work is to design and learn an ANF \(T_{\bm{\theta}}\), with parameters \(\bm{\theta}\), that captures \(\mathcal{M}\) by maximizing the observational likelihood (MLE), i.e.,

\[\operatorname*{maximize}_{\bm{\theta}}\frac{1}{N}\sum_{n=1}^{N}\Big{[}\log p \left(T_{\bm{\theta}}\left(\bm{x}_{n}\right)\right)+\log\lvert\det(\nabla_{ \mathbf{x}}T_{\bm{\theta}}\left(\bm{x}_{n}\right))\rvert\Big{]}\,,\] (4)

and that can successfully answer interventional and counterfactual queries during deployment, thus enabling causal inference. We refer to such a model as a _causal normalizing flow_.

AssumptionsWe restrict the class of SCMs considered by making the following fairly common assumptions: i) _diffeomorphic data-generating process_, i.e., \(\mathbf{\tilde{f}}\) is invertible, and both \(\mathbf{\tilde{f}}\) and its inverse are differentiable; ii) _no feedback loops_, i.e., the induced causal graph is acyclic; and iii) _causal sufficiency_, i.e., the exogenous variables are mutually independent, \(p(\mathbf{u})=\prod_{i}p(\mathbf{u}_{i})\).

SCMs as TMI mapsTo achieve our objective, and bridge the gap between SCMs and normalizing flows, we resort to triangular monotonic increasing (TMI) maps, which are autoregressive functions whose \(i\)-th component is strictly monotonic increasing with respect to its \(i\)-th input. TMI maps hold a number of useful properties, such as being closed under composition and inversions. Conveniently, a layer of an ANF (Eq. 3) is a parametric TMI map that can approximate any other TMI map arbitrarily well, which makes ANFs also TMI maps approximators;2 a fact that has been exploited in the past to prove that ANFs are universal density approximators [25].

Footnote 2: While it is a common to shuffle the inputs for each layer, we keep the same order across the network.

We now show that any SCM can be rewritten as a tuple \((\mathbf{f},P_{\mathbf{u}})\in\mathcal{F}\times\mathcal{P}_{\mathbf{u}}\), where \(\mathcal{F}\) is the set of all TMI maps, and \(P_{\mathbf{u}}\) is the set of all fully-factorized distributions, \(p(\mathbf{u})=\prod_{i}p(\mathbf{u}_{i})\). First, given an acyclic SCM \(\mathcal{M}=(\mathbf{\tilde{f}},P_{\mathbf{u}})\) with \(\mathbf{\tilde{f}}:\mathbb{X}\times\mathbb{U}\to\mathbb{X}\) as in Eq. 1, we can always unroll \(\mathbf{\tilde{f}}\) by recursively replacing each \(\mathbf{x}_{i}\) in the causal equation by its function \(\tilde{f}_{i}\) (see Fig. 3(b) for an example), obtaining an equivalent non-recursive function \(\mathbf{\hat{f}}:\mathbb{U}\to\mathbb{X}\). This function \(\mathbf{\hat{f}}\) writes each \(\mathbf{x}_{i}\) as a function of its exogenous ancestors \(\mathbf{u}_{\mathrm{an}_{i}}\) and, since \(\mathcal{M}\) is acyclic, \(\mathbf{\hat{f}}\) is a triangular map. For simplicity, assume that \(P_{\mathbf{u}}\) is a standard uniform distribution. Then, following the causal ordering, we can apply a Darmoisconstruction [7, 13] and replace each function \(\hat{f}_{i}\) by the conditional quantile function of the variable \(\mathbf{x}_{i}\) given \(\mathbf{x}_{\mathbf{p}_{\mathbf{a}_{i}}}\) (which depends on \(\mathbf{u}_{\mathbf{a}_{\mathbf{a}_{i}}}\)) eventually arriving to a TMI map \(\mathbf{f}\). This procedure follows the proof for non-identifiability in ICA [13], but restricted to one ordering. The case for a general \(P_{\mathbf{u}}\) follows a similar construction, but using a Knothe-Rosenblatt (KR) transport [19, 31] instead.

Isolating the exogenous variablesNow that we have SCMs and causal NFs under the same family class--i.e., the family \(\mathcal{F}\times\mathcal{P}_{\mathbf{u}}\) of TMI maps with fully-factorized distributions--we leverage existing results on identifiability to show that we can find a causal NF \(T_{\bm{\theta}}\) such that the \(i\)-th component of \(T_{\bm{\theta}}(\mathbf{x})\) is a function of the true exogenous variable \(\mathbf{u}_{i}\) that generated the observed data. More precisely, note that, since we can rewrite \(\mathcal{M}\) as an element of the family \(\mathcal{F}\times\mathcal{P}_{\mathbf{u}}\), identifying the true exogenous variables of an SCM \(\mathcal{M}\) is equivalent to solving a non-linear ICA problem with TMI generators, for which Xi and Bloem-Reddy [38] proved the following (re-stated to match our setting):

**Theorem 1** (Identifiability).: If two elements of the family \(\mathcal{F}\times\mathcal{P}_{\mathbf{u}}\) (as defined above) produce the same observational distribution, then the two data-generating processes differ by an invertible, component-wise transformation of the variables \(\mathbf{u}\).

Thm. 1 implies that, if we can find a causal NF \((T_{\bm{\theta}},P_{\bm{\theta}})\in\mathcal{F}\times\mathcal{P}_{\mathbf{u}}\) that matches the observational distribution generated by \(\mathcal{M}=(\mathbf{f},P_{\mathcal{M}})\in\mathcal{F}\times\mathcal{P}_{ \mathbf{u}}\), then we know that the exogenous variables of the flow differ from the real ones by a function of each component independently, i.e., \(T_{\bm{\theta}}(\mathbf{f}(\mathbf{u}))\sim P_{\bm{\theta}}\) with \(\mathbf{u}\sim P_{\mathcal{M}}\) and \(T_{\bm{\theta}}(\mathbf{f}(\mathbf{u}))=\bm{h}(\mathbf{u})=(h_{1}(\mathbf{u}_ {1}),h_{2}(\mathbf{u}_{2}),\ldots,h_{d}(\mathbf{u}_{d}))\), where each \(h_{i}\) is an invertible function. Fig. 3 graphically illustrates Thm. 1. Furthermore, Thm. 1 also implies that the functional dependencies of the causal NF must agree with that of the SCM, i.e., that \(T_{\bm{\theta}}\) needs to be _causally consistent_ with \(\mathcal{M}\). We formally present this result in the following corollary (proof can be found in App. A), where \(\bm{I}\) denotes the identity matrix:

**Corollary 2** (Causal consistency).: If a causal NF \(T_{\bm{\theta}}\) isolates the exogenous variables of an SCM \(\mathcal{M}\), then \(\nabla_{\mathbf{x}}T_{\bm{\theta}}(\mathbf{x})\equiv\bm{I}-\bm{A}\) and \(\nabla_{\mathbf{u}}T_{\bm{\theta}}^{-1}(\mathbf{u})\equiv\bm{I}+\sum_{n=1}^{ \mathrm{diam}(\bm{A})}\bm{A}^{n}\), where \(\bm{A}\) is the causal adjacency matrix of \(\mathcal{M}\). In other words, \(T_{\bm{\theta}}\) is causally consistent with the true data-generating process, \(\mathcal{M}\).

A sketch of the proof goes as follows: since Fig. 3 is a commutative diagram, we can write the result of \(T_{\bm{\theta}}\) and \(T_{\bm{\theta}}^{-1}\) in terms of the true \(\mathbf{f}\), \(\bm{h}\), and their inverses. Then, we can use the chain rule to compute their Jacobian matrices, and since \(\bm{h}\) has a diagonal Jacobian matrix, it preserves the structure of the Jacobian matrices of \(\mathbf{f}\) and its inverse. To sum up, we have shown that _causal NFs are a natural choice to estimate an unknown SCM_ by showing that: i) both SCMs and causal NFs fall within the same family \(\mathcal{F}\times\mathcal{P}_{\mathbf{u}}\); ii) any two elements of this family with identical observational distributions are causally consistent; and iii) they differ by an invertible component-wise transformation.

### Causal NFs for real-world problems

To bring theory closer to practice, we need to extend causal NFs to handle mixed discrete-continuous data and partial knowledge on the causal graph, which are common properties of real-world problems. Due to space limitations, we provide here a brief explanation, and formalize these ideas in App. A.2.

Discrete dataTo extend our results to also account for discrete data, we take advantage of the general model considered by Xi and Bloem-Reddy [38] that includes observational noise (independent of the exogenous variables), and consider a continuous version of the observed discrete variables by adding to them independent noise \(\varepsilon\in[0,1]\) (e.g., from a standard uniform), such that the real distribution is still recoverable. Intuitively, our approach assumes that discrete variables correspond to the integer part of (noisy) continuous variables generated according to an SCM fulfilling our assumptions, such that both our theoretical and practical insights still apply.

Partial knowledgeWhile we rarely know the entire causal graph \(\bm{A}\), we often have a good grasp on causal relationships between a subset of observed variables--e.g., sex and age are not causally related--while missing the rest. When only partial knowledge on the graph is available--i.e., we only know the causal relationship between a subset of the observed variables, we can instead work with a modified acyclic graph \(\tilde{\bm{A}}\) obtained by finding the strongly connected components as in [35], where subsets of variables with unknown causal relationships are treated as a block (see SS7 for an example).

Figure 3: Thm. 1 as a commutative diagram

This allows us to reuse our theoretical results for known parts of the graph, thus generalizing the _block identifiability_ results from von Kugelgen et al. (2019).

## 4 Effective design of causal normalizing flows

We showed in SS3 that causal NFs are a natural choice to learn the underlying SCM generating the data. Importantly, Thm. 1 assumes that we can find a causal NF whose observational distribution perfectly matches the true data distribution (according to the underlying SCM). In practice, however, reaching the optimal parameters may be tricky as: i) we only have access to a finite amount of training data; and ii) the optimization process for causal NFs (like for any neural network) may converge to a local optima. In this section, we analyse different design choices for causal NFs to guide the optimization towards solutions that do not only provide an accurate fit of the observational distribution, but allow us to also accurately answer to interventional and counterfactual queries.

Let us start with an illustrative example. Suppose that we are given the linear SCM in Fig. 3(a), and we want to write the SCM equations as a TMI map to approximate them with a causal NF. As discussed in SS3, we can unroll the causal equations (Fig. 3(b))--resulting in a composition of functions structurally as sparse as \(\bm{I}+\bm{A}\). These functions can be compacted into a single transformation (Fig. 3(c)), such that each \(\mathrm{x}_{i}\) depends on its ancestors, \(\mathrm{u}_{\mathrm{an}_{i}}\). However, note that in this step _shortcuts_ appear, making direct and indirect causal paths in this representation indistinguishable--in our example, the indirect causal path from \(\mathrm{u}_{1}\) to \(\mathrm{x}_{3}\) present in Fig. 3(a) and Fig. 3(b) does not go anymore through the path that generates \(\mathrm{x}_{2}\), but instead via a _shortcut_ that directly connects \(\mathrm{u}_{1}\) to \(\mathrm{x}_{3}\). Alternatively, we can invert the equations to write \(\mathbf{u}\) as a function of \(\mathbf{x}\) (Fig. 3(d)), which is structurally equivalent to \(\bm{I}-\bm{A}\).

We remark that the above steps can be applied to any considered acyclic SCM (refer to App. B for a more detailed discussion). In particular, we can unroll the equations in a finite number of steps, and we can similarly reason about the causal dependencies through the Jacobian matrices of the generators, \(\nabla_{\mathbf{x}}T_{\bm{\theta}}(\mathbf{x})\) and \(\nabla_{\mathbf{u}}T_{\bm{\theta}}^{-1}(\mathbf{u})\). Moreover, note that the diffeomorphic assumption implies that we can invert the causal equations. Next, inspired by the different representations of an SCM (exemplified in Fig. 3(d)), we consider the following design choices for causal NFs:

Generative modelThe first architecture imitates the unrolled equations (Fig. 3(b)), i.e., the causal NF is defined as a function from \(\mathbf{u}\) to \(\mathbf{x}\). Importantly, when full knowledge of the causal graph \(\bm{A}\) is assumed, we also replicate the structural sparsity per layer by adequately masking the flow with \(\bm{I}+\bm{A}\). In this way, the information from \(\mathbf{u}\) to \(\mathbf{x}\) is restricted to flow as if we were unrolling the causal model (Srivastava et al., 2017) and, as a result, the output of the \(l-1\)-th layer of the causal NF is given by:3

Footnote 3: The order has been reversed w.r.t. the causal NF definition from Eq. 3.

\[\mathrm{z}_{i}^{l-1}=\tau_{i}(\mathrm{z}_{i}^{l};\mathbf{h}_{i}^{l-1})\,, \quad\text{where}\quad\mathbf{h}_{i}^{l-1}=c_{i}(\mathrm{z}_{\mathrm{pa}_{i} }^{l})\,.\] (5)

Note that, by restricting each layer such that \(\nabla_{\bm{x}^{l}}\bm{\tau}(\bm{x}^{l})\equiv\bm{I}+\bm{A}\), there cannot exist shortcuts at the optima. For example, in Fig. 3, the (indirect) information of \(\mathbf{u}_{1}\) to generate \(\mathrm{x}_{3}\) by first generating

Figure 3: Example of the linear SCM \(\{\mathrm{x}_{1}=\mathrm{u}_{1}\ ;\ \mathrm{x}_{2}:=\mathrm{2x}_{1}+ \mathrm{u}_{2}\ ;\ \mathrm{x}_{3}:=\mathrm{3x}_{2}+\mathrm{u}_{3}\}\) written (a) in its usual recursive formulation; (b) without recursions, with each step made explicit; (c) without recursions, as a single function; and (d) writing \(\mathbf{u}\) as a function of \(\mathbf{x}\). The red dashed arrows show the influence of \(\mathbf{u}_{1}\) on \(\mathrm{x}_{3}\) for all equations from \(\mathbf{u}\) to \(\mathbf{x}\), with the compacted version exhibiting shortcuts (see Â§4). Note that in the linear case we have \(\bm{A}\coloneqq\bm{G}\neq\mathbf{0}\), and that \(\bm{G}_{1},\bm{G}_{2},\bm{G}_{3}\preceq\bm{G}+\bm{I}\) are any three matrices such that their product equals \(\bm{G}^{2}+\bm{G}+\bm{I}\).

x\({}_{2}\) needs to go through the middle nodes in Fig. 3(b). However, as shown by Sanchez-Martin et al. [34], we need at least \(L=\mathrm{diam}(\bm{A})\) layers in our causal NF to avoid shortcuts and thus differentiate between the different direct and indirect causal paths connecting a pair of observed variables.

In contrast, if we only know the causal ordering, then the causal NF will need to rule out the spurious correlations by learning during training the necessary zeroes to fulfil causal consistency (Cor. 2), i.e., such that \(\nabla_{\mathbf{x}}T_{\bm{\theta}}(\mathbf{x})\equiv\bm{I}+\bm{A}\) and \(\nabla_{\mathbf{u}}T_{\bm{\theta}}^{-1}(\mathbf{u})\equiv\bm{I}+\sum_{n=1}^{ \mathrm{diam}(\bm{A})}\bm{A}^{n}\).

Abductive modelReminiscent to the abduction step [30], another natural choice is to model the inverse equations of the SCM as in Fig. 3(d), hence building a causal NF from \(\mathbf{x}\) to \(\mathbf{u}\). Under a known causal graph, we can again use extra masking in the causal NF to force each layer \(l\) to be structurally equivalent to \(\bm{I}-\bm{A}\), such that

\[\mathsf{z}_{i}^{l}=\tau_{i}(\mathsf{z}_{i}^{l-1};\mathbf{h}_{i}^{l})\,,\quad \text{where}\quad\mathbf{h}_{i}^{l}=c_{i}(\mathsf{z}_{\mathrm{pa}_{i}}^{l-1})\,.\] (6)

Remarkably, this architecture is capable of capturing all indirect dependencies of \(\mathbf{u}\) on \(\mathbf{x}\), even with a single layer. This is a result of the autoregressive nature of the ANFs used here to build causal NFs, as they compute the inverse sequentially. In the example of Fig. 4, the indirect influence of \(\mathsf{u}_{1}\) on \(\mathsf{x}_{3}\) via \(\mathsf{x}_{2}\) has to necessarily generate \(\mathsf{x}_{2}\) first (Fig. 3(a)). Similar to the previous architecture, in the absence of a causal graph (i.e., when only the causal ordering is known), the causal NF will need to rely on optimization to discard all spurious correlations.

### Necessary conditions

We next analyse the necessary conditions for the design of a causal NF to be able to accurately approximate and manipulate an SCM. A summary of the analysis can be found in Tab 1.

ExpressivenessThe least restrictive condition is that the causal NF should be able to reach the optima and, as mentioned in SS3, a single ANF layer (Eq. 3) is a universal TMI approximator [25].

IdentifiabilityIn order to perform interventions as we describe later in SS5, we need the causal NF to isolate the exogenous variables, so that we can associate them with their respective endogenous variables. As we saw in SS3, if the causal NF is expressive enough, and _if it follows a valid causal ordering w.r.t. the true causal graph \(\bm{A}\)_, then Thm. 1 ensures that we can isolate the exogenous variables up to elementwise transformations.

Causal consistencyAs stated in Cor. 2, the causal NF needs to share the causal dependencies of the SCM at the optima, meaning that their Jacobian matrices need to be structurally equivalent, i.e., \(\nabla_{\mathbf{x}}T_{\bm{\theta}}(\mathbf{x})\equiv\bm{I}-\bm{A}\) (Fig. 3(d)), and \(\nabla_{\mathbf{u}}T_{\bm{\theta}}^{-1}(\mathbf{u})\equiv\sum_{n=1}^{ \mathrm{diam}(\bm{A})}\bm{A}^{n}+\bm{I}\) (Fig. 3(c)). Given the (partial) causal graph \(\bm{A}\), the generative model in Eq. 5 by design holds \(\nabla_{\mathbf{u}}T_{\bm{\theta}}^{-1}(\mathbf{u})\equiv\sum_{n=1}^{ \mathrm{diam}(\bm{A})}\bm{A}^{n}+\bm{I}\) for any sufficient number of layers \(L\geq\mathrm{diam}\bm{A}\) (see [34, Prop. 1]), however, there might still exist spurious paths from \(\mathbf{x}\) to \(\mathbf{u}\). Similarly, the abductive model in Eq. 6, while may not remove all spurious paths from \(\mathbf{x}\) to \(\mathbf{u}\) if \(L>1\), ensures causal consistency when \(L=1\). In cases where the selected architecture for the causal NF does not ensure causal consistency by design, but we have access to the causal graph, we can use this extra information to regularize our MLE problem as

\[\operatorname*{minimize}_{\bm{\theta}}\quad\mathrm{E}_{\mathbf{x}}\left[-\log p (T_{\bm{\theta}}(\mathbf{x}))+\|\nabla_{\mathbf{x}}T_{\bm{\theta}}(\mathbf{x })\odot(\bm{1}-\bm{A})\|_{2}\right]\,,\] (7)

where \(\bm{1}\) is a matrix of ones, thus penalizing spurious correlations from \(\mathbf{x}\) to \(T_{\bm{\theta}}(\mathbf{x})\).

Tab 1 summarizes the discussed properties of the considered design choices. Remarkably, the abductive model with a single layer (similar to Fig. 3(d)) _enjoys all the necessary properties of a causal NF by design_. That is, the abductive model with \(L=1\) is expressive, and causally consistent w.r.t. the provided causal graph \(\bm{A}\), greatly simplifying the optimization process.

**Remark.** It is not straightforward why abductive models can be causally consistent by design, but generative models cannot. The answer lies on the structure of the problem. Intuitively, this is a consequence of the mapping \(\mathbf{x}\to\mathbf{u}\) being structurally sparser (\(\mathsf{u}_{i}\) depends on \(\mathrm{pa}_{i}\), see Fig. 3(d)) than that of \(\mathbf{u}\to\mathbf{x}\) (\(\mathsf{x}_{i}\) depends on \(\mathrm{an}_{i}\), see Fig. 3(c)). Therefore, ensuring causal consistency from \(\mathbf{u}\) to \(\mathbf{x}\) does not necessarily imply causal consistency from \(\mathbf{x}\) to \(\mathbf{u}\).

## 5 Do-operator: enabling interventions and counterfactuals

In this section, we propose an implementation of the _do-operator_ well-suited for causal NFs, such that we can evaluate the effect of interventions and counterfactuals [28]. The _do-operator_[30], denoted as \(do(\mathbf{x}_{i}=\alpha)\), is a mathematical operator that simulates a physical intervention on an SCM \(\mathcal{M}\), inducing an alternative model \(\mathcal{M}^{\mathcal{I}}\) that fixes the observational value \(\mathbf{x}_{i}=\alpha\), and thus removes any causal dependency on \(\mathbf{x}_{i}\). Usually, the do-operator is implemented by yielding an SCM \(\mathcal{M}^{\mathcal{I}}=(\mathbf{\tilde{f}}^{\mathcal{I}},P_{\mathbf{u}})\) result of replacing the \(i\)-th component of \(\mathbf{\tilde{f}}\) with a constant function, \(\tilde{f}^{\mathcal{I}}_{i}\coloneqq\alpha\). Unfortunately, this implementation of the do-operator only works for the recursive representation of the SCM (Fig. 3(a)), thus not generalizing to the different architecture designs of causal NFs discussed in SS4 the previous section.

We instead propose to manipulate the SCM by modifying the exogenous distribution \(P_{\mathbf{u}}\), while keeping the causal equations \(\mathbf{\tilde{f}}\) untouched. Specifically, an intervention \(do(\mathbf{x}_{i}=\alpha)\) updates \(P_{\mathbf{u}}\), restricting the set of plausible \(\mathbf{u}\) to those that yield the intervened value \(\alpha\). We define the intervened SCM as \(\mathcal{M}^{\mathcal{I}}=(\mathbf{\tilde{f}},P^{\mathcal{I}}_{\mathbf{u}})\), where the density of \(P^{\mathcal{I}}_{\mathbf{u}}\) is of the form

\[p^{\mathcal{I}}(\mathbf{u})=\delta\left(\left\{\tilde{f}_{i}(\mathbf{x}_{ \mathrm{pa}_{i}},\mathbf{u}_{i})=\alpha\right\}\right)\cdot\prod_{j\neq i}p_{ j}(\mathbf{u}_{j}),\] (8)

and where \(\delta\) is the Dirac delta located at the unique value of \(\mathbf{u}_{i}\) that yields \(\mathbf{x}_{i}=\alpha\) after applying the causal mechanism \(\tilde{f}_{i}\). This approach resembles the one proposed for soft interventions [10] and backtracking counterfactuals [37]. Moreover, as shown in App. C, it can be generalized for non-bijective causal equations. Note also that Eq. 8 is well-defined only if the set \(\{\mathbf{u}\sim P_{\mathbf{u}}\,|\,\mathbf{\tilde{f}}_{i}(\mathbf{x},\mathbf{ u})=\alpha\}\) is non-empty, i.e., if the intervened variable takes a _plausible_ value (i.e., with positive density in the original causal model). Remarkably, this implementation works directly on the distribution of the exogenous variables and can be applied to any SCM representation (see Fig. 3(d)), and therefore any of the architectures for the causal NF in SS4.

Implementation detailsWe take advantage of the autoregressive nature of causal NFs, and generate samples from Eq. 8 by: i) obtaining the exogenous variables \(\mathbf{u}\coloneqq T_{\mathbf{\theta}}(\mathbf{x})\); ii) replacing the observational value by its intervened value, \(\mathbf{x}_{i}\coloneqq\alpha\); and iii) by setting \(\mathbf{u}_{i}\) to the value of the \(i\)-th component of \(T_{\mathbf{\theta}}(\mathbf{x})\). If the causal NF has successfully isolated the exogenous variables (SS3), and it preserves the true causal paths (SS4), then the causal NF ensures that \(\mathbf{x}_{i}=\alpha\) independently of the value of its ancestors, since \(\mathbf{u}_{i}\) can be seen in Eq. 8 as a deterministic function of the given \(\alpha\) and the value of its parents (and therefore its ancestors). We provide further details and the step-by-step algorithms to compute interventions and counterfactuals with causal NFs in App. C.

## 6 Empirical evaluation

In this section, we empirically validate the insights from SS4, and compare causal NFs with previous works. Additional results and in-depth descriptions can be found in App. D.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multicolumn{2}{c}{Design Choices} & \multicolumn{2}{c}{Model Properties} & \multicolumn{2}{c}{Time Complexity} \\ \hline \multirow{2}{*}{Network Type} & Causal & Causal Consistency & \multirow{2}{*}{Sampling} & \multirow{2}{*}{Evaluation} \\ \cline{3-3} \cline{5-6}  & & \(\mathbf{u}\rightarrow\mathbf{x}\) & & \(\mathbf{x}\rightarrow\mathbf{u}\) & \\ \hline \multirow{2}{*}{\(\mathbf{u}\rightarrow\mathbf{x}\)} & Generative & Ordering & âœ— & âœ— & \(\mathcal{O}(L)\) & \(\mathcal{O}(dL)\) \\  & Generative & Graph \(\bm{A}\) & âœ“ & âœ— & \(\mathcal{O}(L)\) & \(\mathcal{O}(dL)\) \\  & Abductive & Ordering & âœ— & âœ— & \(\mathcal{O}(dL)\) & \(\mathcal{O}(L)\) \\  & Abductive (\(L>1\)) & Graph \(\bm{A}\) & âœ— & âœ— & \(\mathcal{O}(dL)\) & \(\mathcal{O}(L)\) \\  & Abductive (\(L=1\)) & Graph \(\bm{A}\) & âœ“ & âœ“ & \(\mathcal{O}(dL)\) & \(\mathcal{O}(L)\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Summary of the considered design choices, their induced properties, and their time complexity for density evaluation and sampling. Generative models design their forward pass as \(\mathbf{u}\rightarrow\mathbf{x}\), and abductive models as \(\mathbf{x}\rightarrow\mathbf{u}\). See Â§4 for an in-depth discussion.

### Ablation study

Experimental setupWe evaluate every network combination described in Tab 1 on a 4-chain SCM (which has diameter 3 and a very sparse Jacobian) and assess the extent to which these models: i) capture the observational distribution, using \(\operatorname{KL}(p_{\mathcal{M}}\|\,p_{\bm{\theta}})\); ii) remain causally consistent w.r.t. the original SCM, measured via \(\mathcal{L}(\nabla_{\mathbf{x}}T_{\bm{\theta}}(\mathbf{x}))\coloneqq||\nabla_{ \mathbf{x}}T_{\bm{\theta}}(\mathbf{x})\cdot(\mathbf{1}-\bm{A})||_{2}\) from Eq. 7; and iii) perform at interventional tasks, such as estimating the Average Treatment Effect (ATE) [29] and computing counterfactuals, both of which we measure with the RMSE w.r.t. the original SCM. Every experiment is repeated 5 times, and every causal NF uses Mask Autoregressive Flows (MAFs) [24] as layers.

ResultsFig. 5 shows the result for different design choices of causal NFs. Specifically, we show: network design (generative \(\mathbf{u}\to\mathbf{x}\) vs. abductive \(\mathbf{x}\to\mathbf{u}\)), causal knowledge (ordering vs. graph), number of layers \(L\), and whether to use MLE with regularization (Eq. 4 vs. 7).

First, we see in Fig. 4(a) (top) that, as expected, the generative models (\(\mathbf{u}\to\mathbf{x}\)) using the causal graph cannot capture the SCM with \(L<\operatorname{diam}\bm{A}=3\). Furthermore, we observe that abductive models \(\mathbf{x}\to\mathbf{u}\) (Fig. 4(a), bottom) accurately fit the observational distribution, and that embedding the causal graph in the architecture significantly improves the ATE estimation.

Second, we now compare the two network designs in Fig. 4(b), and observe that in general abductive models results in more accurate estimates of the observational distribution, as well as of interventional and counterfactual queries. Finally, we observe that regularization works well in all cases, yet it renders useless for the abductive model with \(L=1\) and knowledge on the graph, since it is causally consistent by design. In summary, our experiments confirm that, despite its simplicity, the causal abductive model with \(L=1\) outperforms the rest of design choices. As a consequence, in the following sections we will stick to this particular design choice, and refer to it as causal NF.

### Non-linear SCMs

Experimental setupWe compare our causal NF (causal, abductive, and with \(L=1\)) with two relevant works: i) CAREFL [16], an abductive NF with knowledge on the causal ordering and affine layers; and ii) VACA [34], a variational auto-encoding GNN with knowledge on the graph. For fair comparison, every model uses the same budget for hyperparameter tuning, our causal NF uses affine layers, and CAREFL has been modified to use the proposed do-operator from SS5 (as the original implementation only works in root nodes). We increase the complexity of the SCMs and consider: i) Triangle, a 3-node SCM with a dense causal graph; ii) LargeBD [11], a 9-node SCM with non-Gaussian \(P_{\mathbf{u}}\) and made out of two chains with common initial and final nodes; and iii) Simpson[11], a 4-node SCM simulating a Simpson's paradox [33], where the relation between two variables changes if the SCM is not properly approximated.

ResultsThe results are summarized in Tab 2. In a nutshell: _the proposed causal NF outperforms both CAREFL and VACA in terms of performance and computational efficiency._ VACA shows poor performance, and is considerably slower due to the complexity of GNNs. Our causal NF outperforms

Figure 5: Ablation of different choices of the causal NF to be causally consistent, and capture the observational and interventional distributions. The use of regularization on the Jacobian (Eq. 7) is indicated with the \(\star\) superscript. The abductive causal NF with information on \(\bm{A}\) and \(L=1\) outperforms the rest of models across all metrics, demonstrating its efficacy and simplicity.

CAREFL in counterfactual estimation tasks with identical observational fitting, showing once more the importance of being causally consistent. Even more, our causal NF is also quicker than CAREFL, as best-performing CAREFL architectures have in general more than one layer.

## 7 Use-case: fairness auditing and classification

To show the potential practical impact of our work, we follow the fairness use-case of Sanchez-Martin et al. [34] on the German Credit dataset [8]--a dataset from the UCI repository where the likelihood of individuals repaying a loan is predicted based on a small set of features, including sensitive attributes such as their sex. Extra details and results appear in App. E.

Experimental setupAs proposed by Chiappa [3], we use a partial graph which groups the 7 discrete features of the dataset in 4 different blocks with known causal relationships, putting in practice the results from SS3.1. For the causal NF, we use the abductive model with a single non-affine neural spline layer [9]. Our ultimate goal is to train a causal NF that captures well the underlying SCM, and use it to train and evaluate classifiers that predict the (additional) binary feature _credit risk_, while remaining counterfactually fair w.r.t. the binary variable _sex_, \(\mathrm{x}_{S}\).

In this setting, we call a binary classifier \(\kappa:\mathbb{X}\to\{0,1\}\) counterfactually fair [21] if, for all possible factual values \(\mathbf{x}^{\mathrm{f}}\in\mathbb{X}\), the counterfactual unfairness remains zero. That is, if we have that \(\mathrm{E}_{\mathbf{x}^{\mathrm{f}}}\left[P(\kappa(\mathbf{x}^{\mathrm{cf}}) =1\,|\,do(\mathrm{x}_{S}=1),\mathbf{x}^{\mathrm{f}})-P(\kappa(\mathbf{x}^{ \mathrm{cf}})=1\,|\,do(\mathrm{x}_{S}=0),\mathbf{x}^{\mathrm{f}})\right]=0\), where \(\mathbf{x}^{\mathrm{cf}}\) is a counterfactual sample coming from the distribution \(P(\mathbf{x}^{\mathrm{cf}}\,|\,do(\mathrm{x}_{S}=s),\mathbf{x}^{\mathrm{f}})\), for \(s=0,1\).

Following Sanchez-Martin et al. [34], we audit: a model that takes all observed variables (_full_); an _unaware_ model that leaves the sensitive attribute \(\mathrm{x}_{S}\) out; a fair model that only considers non-descendant variables of \(\mathrm{x}_{S}\) (_fair_\(\mathbf{x}\)); and, to demonstrate the ability to learn a counterfactually fair classifier, we include a classifier that takes \(\mathbf{u}=T_{\bm{\theta}}(\mathbf{x})\) as input, but leaves \(\mathrm{u}_{S}\) out (_fair_\(\mathbf{u}\)).

ResultsTab 3 summarizes the performance and unfairness of the classifiers, using logistic regression [6] and SVMs [5]. Here, we observe that by taking the non-sensitive exogenous variables from the causal NF, the obtained classifiers achieve comparable or better accuracy than the rest of the classifiers, while at the same time being counterfactually fair. Moreover, the estimations of unfairness obtained with the causal NF match our expectations [21], with _full_ being the most unfair, followed by _aware_ and the two fair models. With this use-case, we demonstrate that _Causal NFs may indeed be a valuable asset for real-world causal inference problems_.

\begin{table}
\begin{tabular}{l l c c c c c c} \hline \hline  & & \multicolumn{3}{c}{Performance} & \multicolumn{3}{c}{Time Evaluation (\(\mathrm{\SIUnitSymbolMicro s}\))} \\ \cline{3-8} Dataset & Model & \(\mathrm{KL}\) & \(\mathrm{ATE}_{\mathrm{RMSE}}\) & \(\mathrm{CF}_{\mathrm{RMSE}}\) & Training & Evaluation & Sampling \\ \hline Triangle & Causal NF & \(\mathbf{0.00_{0.00}}\) & \(\mathbf{0.12_{0.03}}\) & \(\mathbf{0.13_{0.02}}\) & \(\mathbf{0.52_{0.07}}\) & \(\mathbf{0.58_{0.07}}\) & \(\mathbf{1.07_{0.12}}\) \\ Niln & CAREFL\({}^{\dagger}\) & \(\mathbf{0.00_{0.00}}\) & \(\mathbf{0.12_{0.03}}\) & \(0.17_{0.03}\) & \(\mathbf{0.57_{0.18}}\) & \(\mathbf{0.83_{0.26}}\) & \(\mathbf{1.68_{0.62}}\) \\
[34] & VACA & \(7.71_{0.60}\) & \(4.78_{0.01}\) & \(4.19_{0.04}\) & \(28.82_{1.21}\) & \(23.00_{0.55}\) & \(70.65_{3.70}\) \\ \hline LargeBD & Causal NF & \(\mathbf{1.51_{0.04}}\) & \(\mathbf{0.02_{0.00}}\) & \(\mathbf{0.01_{0.00}}\) & \(\mathbf{0.52_{0.10}}\) & \(\mathbf{0.60_{0.17}}\) & \(\mathbf{3.05_{0.66}}\) \\ Niln & CAREFL\({}^{\dagger}\) & \(1.51_{0.05}\) & \(0.05_{0.01}\) & \(0.08_{0.01}\) & \(0.84_{0.47}\) & \(1.18_{0.17}\) & \(8.25_{1.29}\) \\
[11] & VACA & \(53.66_{2.07}\) & \(0.39_{0.00}\) & \(0.82_{0.02}\) & \(164.92_{11.10}\) & \(137.88_{15.72}\) & \(167.94_{25.75}\) \\ \hline Simpson & Causal NF & \(\mathbf{0.00_{0.00}}\) & \(\mathbf{0.07_{0.01}}\) & \(\mathbf{0.12_{0.02}}\) & \(\mathbf{0.59_{0.17}}\) & \(\mathbf{0.60_{0.11}}\) & \(\mathbf{1.51_{0.30}}\) \\ symprod & CAREFL\({}^{\dagger}\) & \(\mathbf{0.00_{0.00}}\) & \(0.10_{0.02}\) & \(0.17_{0.04}\) & \(\mathbf{0.49_{0.15}}\) & \(0.81_{0.19}\) & \(1.91_{0.33}\) \\
[11] & VACA & \(13.85_{0.64}\) & \(0.89_{0.00}\) & \(1.50_{0.04}\) & \(49.26_{0.49}\) & \(37.78_{3.41}\) & \(79.20_{14.60}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison, on three non-linear SCMs, of the proposed causal NF, VACA [34], and CAREFL [16] with the do-operator proposed in Â§5. Results averaged over five runs.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{Logistic classifier} & \multicolumn{3}{c}{SVM classifier} \\ \cline{2-10}  & full & unaware & fair \(\mathbf{x}\) & fair \(\mathbf{u}\) & full & unaware & fair \(\mathbf{x}\) & fair \(\mathbf{u}\) \\ \hline f1 & \(72.28_{6.16}\) & \(72.37_{4.90}\) & \(59.66_{5.57}\) & \(73.08_{4.38}\) & \(76.04_{2.86}\) & \(76.80_{5.82}\) & \(68.28_{5.74}\) & \(77.39_{1.52}\) \\ accuracy & \(67.00_{3.83}\) & \(66.75_{2.63}\) & \(54.75_{5.91}\) & \(66.50_{3.70}\) & \(69.50_{3.11}\) & \(71.00_{3.83}\) & \(59.25_{2.99}\) & \(69.75_{1.26}\) \\ unfairness & \(5.84_{2.93}\) & \(2.81_{0.72}\) & \(0.00_{0.00}\) & \(0.00_{0.00}\) & \(6.65_{2.45}\) & \(2.78_{0.40}\) & \(0.00_{0.00}\) & \(0.00_{0.00}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Accuracy, F1-score, and counterfactual unfairness of the audited classifiers. Causal NFs enable both fair classifiers and accurate unfairness metrics. Results are averaged on five runs.

Concluding remarks

In this work, we have shown--both theoretically and empirically--that causal NFs are a natural choice to learn a broad class of causal data-generating processes in a principled way. Specifically, we have proven that causal NFs can match the observational distribution of an underlying SCM, and that in doing so the ANF needs to be causally consistent. However, as limited data availability and local optima may hamper reaching these solutions in practice, we have explored different network designs, exploiting the available knowledge on the causal graph. Moreover, we have provided causal NFs with a do-operator to efficiently solve causal inference tasks. Finally, we have empirically validated our findings, and demonstrated that our causal NF framework: i) outperforms competing methods; and ii) can deal with mixed data and partial knowledge on the causal graph.

Practical limitationsDespite considering a broad class of SCMs, we have made several assumptions that, while being standard, may not hold in some application scenarios. With regard to our causal assumptions, the presence of unmeasured hidden confounders may break our causal sufficiency assumption; mismatches between the true causal graph (e.g., it may contain cycles) and our assumed graph/ordering may lead to poor estimates of interventional and counterfactual queries; and the non-bijective true causal dependencies may invalidate our theoretical and thus practical findings. Besides, we have focused on MLE estimation for learning the causal NF. However, MLE does not test the independency of the exogenous variables during training, which would also break our causal sufficiency assumption.

Future workWe firmly believe that our work opens a number of interesting directions to explore. Naturally, we would like to address current limitations by, e.g., using interventional data to address the existence of hidden confounders [14; 23], explore alternative losses other than MLE (e.g., flow matching [22]). Moreover, it would be exciting to see causal NFs applied to other problems such as causal discovery [11], fair decision-making [21], or neuroimaging [12], among others. However, we would like to stress that, in the above contexts, it would be essential to validate the suitability of our framework (e.g., using experimental data) to prevent potential harms.

## Acknowledgements

We would like to thank Batuhan Koyuncu and Jonas Klesen for their invaluable feedback, as well as the anonymous reviewers whose feedback helped us improve this manuscript. This project is funded by DFG grant 389792660 as part of TRR 248 - CPEC. Pablo Sanchez Martin thanks the German Research Foundation through the Cluster of Excellence "Machine Learning - New Perspectives for Science", EXC 2064/1, project number 390727645 for generous funding support. The authors also thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Pablo Sanchez Martin.

## References

* Bishop and Nasrabadi [2006] Christopher M Bishop and Nasser M Nasrabadi. _Pattern recognition and machine learning_, volume 4. Springer, 2006. (page 3)
* Chao et al. [2023] Patrick Chao, Patrick Blobaum, and Shiva Prasad Kasiviswanathan. Interventional and counterfactual inference with diffusion models. _ArXiv preprint_, abs/2302.00860, 2023. URL https://arxiv.org/abs/2302.00860. (page 1, 2, 29)
* February 1, 2019_, pages 7801-7808. AAAI Press, 2019. doi: 10.1609/aaai.v33i01.33017801. URL https://doi.org/10.1609/aaai.v33i01.33017801. (page 9)
* Clevert et al. [2016] Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). In _Proc. of ICLR_, 2016. URL http://arxiv.org/abs/1511.07289. (page 27)
* Cortes and Vapnik [1995] Corinna Cortes and Vladimir Vapnik. Support-vector networks. _Machine learning_, 20(3):273-297, 1995. (page 9)* Cox [1958] David R Cox. The regression analysis of binary sequences. _Journal of the Royal Statistical Society: Series B (Methodological)_, 20(2):215-232, 1958. (page 9)
* Darmois [1953] G. Darmois. Analyse generale des liaisons stochastiques: etude particuliere de l'analyse factorielle lineaire. _Revue de l'Institut International de Statistique / Review of the International Statistical Institute_, 21(1/2):2-8, 1953. ISSN 03731138. URL http://www.jstor.org/stable/1401511. (page 4)
* Dua and Graff [2021] Dheeru Dua and Casey Graff. UCI machine learning repository, 2021. URL https://arxiv.org/abs/2103.04786. (page 1, 9, 29, 31)
* Durkan et al. [2019] Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline flows. In _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 7509-7520, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/7ac71d433f282034e088473244df8c02-Abstract.html. (page 9, 28, 29)
* Eberhardt and Scheines [2007] Frederick Eberhardt and Richard Scheines. Interventions and causal inference. _Philosophy of Science_, 74(5):981-995, 2007. ISSN 00318248, 1539767X. URL http://www.jstor.org/stable/10.1086/525638. (page 7)
* Geffner et al. [2022] Tomas Geffner, Javier Antoran, Adam Foster, Wenbo Gong, Chao Ma, Emre Kiciman, Ajay Sharma, A. Lamb, Martin Kukla, Nick Pawlowski, Miltiadis Allamanis, and Cheng Zhang. Deep end-to-end causal inference. _ArXiv preprint_, abs/2202.02195, 2022. URL https://arxiv.org/abs/2202.02195. (page 8, 9, 10, 27, 29)
* Gorgolewski et al. [2011] Krzysztof Gorgolewski, Christopher Burns, Cindee Madison, Dav Clark, Yaroslav Halchenko, Michael Waskom, and Satrajit Ghosh. Nipype: A flexible, lightweight and extensible neuroimaging data processing framework in python. _Frontiers in Neuroinformatics_, 5, 2011. ISSN 1662-5196. doi: 10.3389/fninf.2011.00013. URL https://www.frontiersin.org/articles/10.3389/fninf.2011.00013. (page 10)
* Hyvarinen and Pajunen [1999] Aapo Hyvarinen and Petteri Pajunen. Nonlinear independent component analysis: Existence and uniqueness results. _Neural Networks_, 12(3):429-439, 1999. ISSN 0893-6080. doi: https://doi.org/10.1016/S0893-6080(98)00140-3. URL https://www.sciencedirect.com/science/article/pii/S0893608098001403. (page 4)
* Ilse et al. [2021] Maximilian Ilse, Patrick Forre, Max Welling, and Joris M Mooij. Combining interventional and observational data using causal reductions. _ArXiv preprint_, abs/2103.04786, 2021. URL https://arxiv.org/abs/2103.04786. (page 10)
* Karimi et al. [2020] Amir-Hossein Karimi, Bodo Julius von Kugelgen, Bernhard Scholkopf, and Isabel Valera. Algorithmic recourse under imperfect causal knowledge: a probabilistic approach. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/02a3c7fb3f489288ae6942498498db20-Abstract.html. (page 1)
* Khemakhem et al. [2021] Ilyes Khemakhem, Ricardo Pio Monti, Robert Leech, and Aapo Hyvarinen. Causal autoregressive flows. In _The 24th International Conference on Artificial Intelligence and Statistics, AISTATS 2021, April 13-15, 2021, Virtual Event_, volume 130 of _Proceedings of Machine Learning Research_, pages 3520-3528. PMLR, 2021. URL http://proceedings.mlr.press/v130/khemakhem21a.html. (page 2, 8, 9, 23, 28, 29)
* Kingma and Ba [2015] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _Proc. of ICLR_, 2015. URL http://arxiv.org/abs/1412.6980. (page 24, 29)
* Kingma et al. [2016] Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. In _Advances in Neural Information Processing Systems_, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper_files/paper/2016/file/ddee9bdeefdb7e/e7a697e1c3e3d8ef54-Paper.pdf. (page 3)
* Knothe [1957] Herbert Knothe. Contributions to the theory of convex bodies. _Michigan Mathematical Journal_, 4:39-52, 1957. (page 4, 18)
* Kocaoglu et al. [2016] Murat Kocaoglu, Christopher Snyder, Alexandros G. Dimakis, and Sriram Vishwanath. Causalgan: Learning causal implicit generative models with adversarial training. In _Proc. of ICLR_.

OpenReview.net, 2018. URL https://openreview.net/forum?id=BJE-4xWOW. (page 1, 2)
* Kusner et al. [2017] Matt J. Kusner, Joshua R. Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 4066-4076, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html. (page 9, 10)
* Lipman et al. [2023] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=PqvMRDCJ79t. (page 10)
* Nasr-Esfahany et al. [2023] Arash Nasr-Esfahany, MohammadLman Alizadeh, and Devavrat Shah. Counterfactual identifiability of bijective causal models. _ArXiv preprint_, abs/2302.02228, 2023. URL https://arxiv.org/abs/2302.02228. (page 1, 10)
* Papamakarios et al. [2017] George Papamakarios, Iain Murray, and Theo Pavlakou. Masked autoregressive flow for density estimation. In _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 2338-2347, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/6c1da886822c67822bcf3679d04369fa-Abstract.html. (page 3, 8, 27, 28)
* Papamakarios et al. [2021] George Papamakarios, Eric T. Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. _J. Mach. Learn. Res._, 22:57:1-57:64, 2021. URL http://jmlr.org/papers/v22/19-1028.html. (page 3, 6, 16)
* Parafita and Vitria [2022] Alvaro Parafita and Jordi Vitria. Estimand-agnostic causal query estimation with deep causal graphs. _IEEE Access_, 10:71370-71386, 2022. doi: 10.1109/ACCESS.2022.3188395. (page 1, 2)
* Pawlowski et al. [2020] Nick Pawlowski, Daniel Coelho de Castro, and Ben Glocker. Deep structural causal models for tractable counterfactual inference. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/0987b8b338d6c90bbedd8631bc499221-Abstract.html. (page 1, 2)
* Pearl [2009] J. Pearl. _Causality_. Cambridge University Press, 2009. ISBN 9781139643986. URL https://books.google.de/books?id=LLkhAwAAQBAJ. (page 1, 2, 7)
* Pearl [2009] Judea Pearl. Causal inference in statistics: An overview. 2009. (page 8)
* Pearl [2012] Judea Pearl. The do-calculus revisited. In _Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence, Catalina Island, CA, USA, August 14-18, 2012_, pages 3-11. AUAI Press, 2012. URL https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu=1&smnu=2&article_id=2330&proceeding_id=28. (page 6, 7, 21)
* Rosenblatt [1952] Murray Rosenblatt. Remarks on a multivariate transformation. _The Annals of Mathematical Statistics_, 23(3):470-472, 1952. ISSN 00034851. URL http://www.jstor.org/stable/2236692. (page 4, 18)
* Sanchez and Tsaftaris [2022] Pedro Sanchez and Sotirios A. Tsaftaris. Diffusion causal models for counterfactual estimation. In _CLEaR_, 2022. (page 2)
* Simpson [1951] Edward H Simpson. The interpretation of interaction in contingency tables. _Journal of the Royal Statistical Society: Series B (Methodological)_, 13(2):238-241, 1951. (page 8)
* Sanchez-Martin et al. [2022] Pablo Sanchez-Martin, Miriam Rateike, and Isabel Valera. Vaca: Designing variational graph autoencoders for causal queries. _Proceedings of the AAAI Conference on Artificial Intelligence_, 36(7):8159-8168, Jun. 2022. doi: 10.1609/aaai.v36i7.20789. URL https://ojs.aaai.org/index.php/AAAI/article/view/20789. (page 2, 5, 6, 8, 9, 23, 27, 28, 29)
* Tarjan [1972] Robert Tarjan. Depth-first search and linear graph algorithms. _SIAM Journal on Computing_, 1(2):146-160, 1972. doi: 10.1137/0201010. URL https://doi.org/10.1137/0201010. (page 4, 17)
* von Kugelgen et al. [2018] Julius von Kugelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Scholkopf, Michel Besserve, and Francesco Locatello. Self-supervised learning with data augmentationsprovably isolates content from style. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 16451-16467, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/8929c70f8d710e412d38da624b21c3c8-Abstract.html. (page 5)
* Von Kugelgen _et al._ [2023] Julius Von Kugelgen, Abdiirisak Mohamed, and Sander Beckers. Backtracking counterfactuals. In _Conference on Causal Learning and Reasoning_, pages 177-196. PMLR, 2023. (page 7)
* Xi and Bloem-Reddy [2023] Quanhan Xi and Benjamin Bloem-Reddy. Indeterminacy in generative models: Characterization and strong identifiability. In _Proceedings of The 26th International Conference on Artificial Intelligence and Statistics_, volume 206 of _Proceedings of Machine Learning Research_, pages 6912-6939. PMLR, 25-27 Apr 2023. URL https://proceedings.mlr.press/v206/xi23a.html. (page 4, 15, 16)
* Xia _et al._ [2022] Kevin Xia, Yushu Pan, and Elias Bareinboim. Neural causal models for counterfactual identification and estimation. _ArXiv preprint_, abs/2210.00035, 2022. URL https://arxiv.org/abs/2210.00035. (page 1)
* Yang _et al._ [2021] Mengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun Wang. Causalvae: Disentangled representation learning via neural structural causal models. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021_, pages 9593-9602. Computer Vision Foundation / IEEE, 2021. doi: 10.1109/CVPR46437.2021.00947. URL https://openaccess.thecvf.com/content/CVPR2021/html/Yang_CausalVAE_Disentangled_Representation_Learning_via_Neural_Structural_Causal_Models_CVPR_2021_paper.html. (page 1)
* Zecevic _et al._ [2021] Matej Zecevic, Devendra Singh Dhami, Petar Velickovic, and Kristian Kersting. Relating graph neural networks to structural causal models. _ArXiv preprint_, abs/2109.04173, 2021. URL https://arxiv.org/abs/2109.04173. (page 2)

## Appendix

### Table of Contents

* A Theory of causal normalizing flows
* A.1 Identifiability results
* A.2 Extension to real-world settings
* B The multiple representations of SCMs
* B.1 Illustrative example
* B.2 Non-linear SCM representations
* C Do-operator: interventions and counterfactuals
* C.1 Definition and algorithms
* C.2 Interventions in previous works
* D Experimental details and extra results
* D.1 Ablation: Time complexity
* D.2 Ablation: Base distribution
* D.3 Ablation: Flow architecture
* D.4 Comparison: Extra non-linear SCMs
* E Details on the fairness use-caseTheory of causal normalizing flows

### Identifiability results

First, we provide a more detailed explanation on the connection between the results from SS3 and those from the work of Xi and Bloem-Reddy [38]. We consider it important to clarify that the definition of identifiability that we use is the same as [38, Def. 2]. Specifically, this definition is one better suited for deep learning models, which is concerned with recovering the variables \(\mathbf{u}\) and _one_ parametrization that perfectly matches the original generator. In other words, with this definition we aim to recover one parametrization of a neural network which provides the generator function, but not the _exact_ parametrization of the generator that generated the data.

We also want to clarify that Thm. 1 from the main paper corresponds to [38, Prop. 5.2], which we rewrote (without changing its content) to plain English and to match our particular setting. We now provide the proof for Cor. 2:

**Corollary 2** (Causal consistency).: If a causal NF \(T_{\boldsymbol{\theta}}\) isolates the exogenous variables of an SCM \(\mathcal{M}\), then \(\nabla_{\mathbf{x}}T_{\boldsymbol{\theta}}(\mathbf{x})\equiv\boldsymbol{I}- \boldsymbol{A}\) and \(\nabla_{\mathbf{u}}T_{\boldsymbol{\theta}}^{-1}(\mathbf{u})\equiv\boldsymbol{I }+\sum_{n=1}^{\operatorname{diam}(\boldsymbol{A})}\boldsymbol{A}^{n}\), where \(\boldsymbol{A}\) is the causal adjacency matrix of \(\mathcal{M}\). In other words, \(T_{\boldsymbol{\theta}}\) is causally consistent with the true data-generating process, \(\mathcal{M}\).

Proof.: Assume that we have a flow \(T_{\boldsymbol{\theta}}\) that does indeed isolate the exogenous variables, meaning that the \(i\)-th output of the flow, \(T_{\boldsymbol{\theta}}(\mathbf{x})_{i}\), is related with the true exogenous variable, \(\mathbf{u}_{i}\), by an invertible function that only depends on it.

As explained in SS3, this means that for a variable \(\mathbf{u}\sim P_{\mathcal{M}}\), we have that \(T_{\boldsymbol{\theta}}(\mathbf{f}(\mathbf{u}))\sim P_{\boldsymbol{\theta}}\) and \(T_{\boldsymbol{\theta}}(\mathbf{f}(\mathbf{u}))=\boldsymbol{h}(\mathbf{u})=(h _{1}(\mathbf{u}_{1}),h_{2}(\mathbf{u}_{2}),\ldots,h_{d}(\mathbf{u}_{d}))\).

But we know the true generator, whose \(i\)-th exogenous variable is given by \(\mathbf{u}_{i}=f_{i}^{-1}(\mathbf{x}_{\mathrm{pa}_{i}},\mathbf{x}_{i})\) (the inverse of \(f_{i}\) w.r.t \(\mathbf{u}_{i}\)) and, putting all together,

\[T_{\boldsymbol{\theta}}(\mathbf{x})_{i}=h_{i}(\mathbf{u}_{i})=h_{i}(f_{i}^{-1} (\mathbf{x}_{\mathrm{pa}_{i}},\mathbf{x}_{i}))\,,\] (9)

which is a function of _only_ the parents of \(\mathbf{x}_{i}\) and \(\mathbf{x}_{i}\) itself.

If we call \(\mathbf{u}=\mathbf{f}^{-1}(\mathbf{x})\coloneqq(f_{1}^{-1}(\mathbf{x}_{ \mathrm{pa}_{1}},\mathbf{x}_{1}),f_{2}^{-1}(\mathbf{x}_{\mathrm{pa}_{2}}, \mathbf{x}_{2}),\ldots,f_{d}^{-1}(\mathbf{x}_{\mathrm{pa}_{d}},\mathbf{x}_{d}))\) the inverse of the SCM \(\mathcal{M}\) that writes \(\mathbf{u}\) as a function of \(\mathbf{x}\) (see App. B for an example), then it is clear that

\[\nabla_{\mathbf{x}}T_{\boldsymbol{\theta}}(\mathbf{x})=\nabla_{\mathbf{x}}( \boldsymbol{h}\circ\mathbf{f}^{-1})(\mathbf{x})=\nabla_{\mathbf{u}}\boldsymbol {h}(\mathbf{u})\cdot\nabla_{\mathbf{x}}\mathbf{f}^{-1}(\mathbf{x})= \boldsymbol{D}\cdot\nabla_{\mathbf{x}}\mathbf{f}^{-1}(\mathbf{x})\equiv \boldsymbol{I}-\boldsymbol{A}\,,\] (10)

where \(\boldsymbol{D}\) is a diagonal matrix and \(\boldsymbol{A}\) the adjacency matrix of the causal graph induced by \(\mathcal{M}\).

Similarly, \(T_{\boldsymbol{\theta}}^{-1}(\boldsymbol{h}(\mathbf{u}))=\mathbf{x}=\mathbf{f} (\mathbf{u})\) and \(T_{\boldsymbol{\theta}}^{-1}(\boldsymbol{h}(\mathbf{u}))_{i}=\mathbf{x}_{i}=f_{ i}(\mathbf{u}_{\mathrm{an}_{i}},\mathbf{u}_{i})\), which again implies that:

\[\nabla_{\tilde{\mathbf{u}}}T_{\boldsymbol{\theta}}^{-1}(\tilde{\mathbf{u}}) \equiv(\boldsymbol{I}-\boldsymbol{A})^{-1}=\boldsymbol{I}+\sum_{n=1}^{ \operatorname{diam}(\boldsymbol{A})}\boldsymbol{A}^{n}\,,\] (11)

where we call \(\tilde{\mathbf{u}}=\boldsymbol{h}(\mathbf{u})\) the variable \(\mathbf{u}\) recovered by \(T_{\boldsymbol{\theta}}\), and where we have omitted \(\boldsymbol{h}\) as its Jacobian matrix is diagonal. Note that the infinite sum above vanishes at \(n=\operatorname{diam}\boldsymbol{A}\) since \(\boldsymbol{A}\) is triangular with diagonal zero.  Q.E.D.

### Extension to real-world settings

#### a.2.1 Discrete data

In this section, we describe how to extend the results presented in the main text for the case where one observed variable, \(\mathbf{x}_{i}\), is discrete. To this end, we restate the more general data-generative process assumed by Xi and Bloem-Reddy [38], which we used for the theoretical part of the manuscript.

Following the notation of the manuscript, say that we have a data-generating process without recursions, that is, we have a function \(\mathbf{f}\) that maps \(\mathbf{u}\) to \(\mathbf{x}\). Let us assume, without loss of generality, that only the \(i\)-th observed variable is discrete, and let us focus on the way this variable is generated, dropping the subindex \(i\) along the way to avoid clutter. Now, Xi and Bloem-Reddy [38] additionally consider the existence of a fixed noise distribution \(P_{\varepsilon}\) and mechanism \(g\), such that the observed variable \(\mathbf{x}_{i}\) is generated as,

\[\mathbf{u}:=(\mathbf{u}_{1},\mathbf{u}_{2},\ldots,\mathbf{u}_{d})\sim P_{ \mathbf{u}}\,,\qquad\qquad\varepsilon\sim P_{\varepsilon}\,,\qquad\qquad \mathbf{x}_{i}=g(f_{i}(\mathbf{u}_{\mathrm{an}_{i}},\mathbf{u}_{i}), \varepsilon)\,,\] (12)

with \(\varepsilon\bot\mathbf{u}\), and where they study the noiseless case under the following assumption: if for two generative processes with \(\varepsilon_{a}\stackrel{{\mathrm{d}}}{{=}}\varepsilon_{b}\), then \(g(f_{i}(\mathbf{u}_{a}),\varepsilon_{a})\stackrel{{\mathrm{d}}}{{= }}g(f_{i}(\mathbf{u}_{b}),\varepsilon_{b})\) if and only if \(f_{i}(\mathbf{u}_{a})\stackrel{{\mathrm{d}}}{{=}}f_{i}(\mathbf{u }_{b})\), where \(\stackrel{{\mathrm{d}}}{{=}}\) denotes equal in distribution.

Just as we do with the rest of variables, we also make the assumption that the observed variable \(\tilde{\mathbf{x}}_{i}\) is the transformation of a continuous exogenous variable, \(\mathbf{u}_{i}\), with a function \(\tilde{f}_{i}\) that fulfils our assumptions (i.e., that \(\tilde{f}_{i}\) is a diffeomorphism) that has undergone a quantization process, i.e., \(\mathbf{x}_{i}=f_{i}(\mathbf{u}_{\mathrm{an}_{i}},\mathbf{u}_{i})\coloneqq \lfloor\tilde{f}_{i}(\mathbf{u}_{\mathrm{an}_{i}},\mathbf{u}_{i})\rfloor\). Therefore, it is clear that \(f_{i}\) is no longer bijective, as we are clamping real numbers into integers, and that the observational distribution of \(\mathbf{x}_{i}\) is discrete.

We take advantage of the noise assumption above, and dequantize the observed variable \(\mathbf{x}_{i}\) by assuming an additive noise mechanism such that \(\mathbf{x}_{i}\coloneqq f_{i}(\mathbf{u}_{\mathrm{an}_{i}},\mathbf{u}_{i})+\varepsilon\), with \(\varepsilon\) distributed between the unit interval with any continuous distribution (we take in our experiments \(P_{\varepsilon}=\mathcal{U}(0,1)\)). With this process: i) we have made \(\tilde{\mathbf{x}}_{i}\) again a continuous random variable, as the sum of independents discrete and continuous random variables is a continuous random variable; and ii) the original distribution of the noiseless observed variables is always recoverable \(P(\tilde{\mathbf{x}}_{i}=c)=P(c\leq\mathbf{x}_{i}\leq c+1)\).

More importantly, all the theoretical insights from the work of Xi and Bloem-Reddy [38] can still be used, working with the noisy case rather than the noiseless one. Indeed, as for their analysis they assume a single \(\mathbf{u}\) in the domain of the generator, we can merge the generator and noise mechanism \(g\circ f_{i}:\mathbb{R}\to\mathbb{R}\) (rather than \(g\circ f_{i}:\mathbb{R}\times[0,1]\to\mathbb{R}\)), by mapping the non-injective part of \(\mathbf{u}\) to \(\varepsilon\) itself, i.e., by using the function \((g\circ f_{i})(\mathbf{u}_{\mathrm{an}_{i}},\mathbf{u}_{i})=\lfloor\tilde{f}_ {i}(\mathbf{u}_{\mathrm{an}_{i}},\mathbf{u}_{i})\rfloor+F_{\varepsilon}^{-1}( \tilde{f}_{i}(\mathbf{u}_{\mathrm{an}_{i}},\mathbf{u}_{i})-\lfloor\tilde{f}_ {i}(\mathbf{u}_{\mathrm{an}_{i}},\mathbf{u}_{i})\rfloor)\), where \(F_{\varepsilon}^{-1}\) is the quantile function of \(P_{\varepsilon}\). This new function is a diffeomorphism almost everywhere, as it is a composition of a.e. diffeomorphisms, and we have effectively replaced the noise variable by the floating part of \(\tilde{f}_{i}(\mathbf{u}_{\mathrm{an}_{i}},\mathbf{u}_{i})\) before quantization. Moreover, note that if the noise is uniformly distributed, \(P_{\varepsilon}=\mathcal{U}(0,1)\), we have that \(g\circ f_{i}=\tilde{f}_{i}\) (if \(\mathbf{x}_{i}\) were discretized by taking its integer part).

In short, by adding noise to discrete variables while keeping them recoverable, we can learn a mapping between continuous variables that learns a version of the generator function before the observed values were somehow discretized. Importantly, the observed discrete distribution is always recoverable, independently of whether we learn the (unknown and unrecoverable) underlying continuous distribution before being discretized.

#### a.2.2 Partial knowledge

In this section, we explain how to expand our framework to settings in which we have partial information about the causal graph of \(\mathcal{M}\). That is, we know the causal ordering \(\pi\) (so we know that half of the causal relationships, the upper diagonal), and we are certain about some other causal relationships (edges on \(\bm{A}\)), but not all of them.

To this end, first let us first introduce the way we deal with partial knowledge, and then clarify the theoretical implications that it has with respect to the theory introduced in SS5.

The methodSimilar to SS4, let us motivate the method with an illustrative example. Suppose that we are given an SCM such as the one in Fig. 5(a), where we know all relationships but the one between \(\mathbf{x}_{2}\) and \(\mathbf{x}_{3}\). Note that, in this case, we lack even information about the causal ordering. Indeed, there are three possible outcomes: i) the edge \(\mathbf{x}_{2}\to\mathbf{x}_{3}\) could exist (Fig. 5(b)); ii) the edge \(\mathbf{x}_{3}\to\mathbf{x}_{2}\) could exist (Fig. 5(c)); or iii) both could exist simultaneously (Fig. 5(d)), and hence there is a confounder between them. However, we do not know which of the three options is the correct one.

Let us switch now to Fig. 7. To solve the original problem (Fig. 6(a), one natural approach is to group the nodes with unknown relationships--assuming that all unknown edges may exist--and maximize the observed likelihood (Fig. 6(b)). This, effectively, is equivalent to applying an ANF to the known relationships, and using a general-Jacobian NF to learn the joint of the block variables. However, if we want to keep using exclusively ANFs (Fig. 6(c)), we can learn the joint distribution within the blocks with an ANF using a fixed ordering (which it can always do, as it is a universal density approximator [25]). The only subtle detail here is that, in that case, we need to increase the granularity of all inter-block edges from node- to block-wise relationships, assuming that an edge exists if it exists for at least one of the elements of the block. To see why this is necessary, assume that the real graph of the example is Fig. 5(c), yet we use an ANF with the ordering \(\pi=(1\ 2\ 3\ 4)\) and the graph \(\bm{A}\) without inter-block modifications (i.e., the adjacency matrix of Fig. 5(b)). In that case, we would have that \(\mathrm{x}_{4}\) depends on \(\mathrm{x}_{3}\) through \(\mathrm{x}_{2}\). However, a causally consistent NF w.r.t. \(\bm{A}\) would not be able to model that dependency, and thus \(\mathrm{x}_{4}\) would depend on \(\mathrm{u}_{1}\), \(\mathrm{u}_{2}\), and \(\mathrm{u}_{4}\) but not on \(\mathrm{u}_{3}\). With this approach, the ANF can model every case from Fig. 6, and it would need to remove the extra spurious relationships through optimization.

Therefore, to reuse our existing results from SS5, the method that we have adopted is the one from Fig. 6(c), which can be described in the following steps:

1. Run Tarjan's algorithm [35] to group all nodes by their SCCs (note that, unlike in the given example, there could be more than one cluster of unknown relationships).
2. Choose an ordering that is consistent with the known inter-SCC edges, fixing the edges within the SCCs.
3. Move from node-edges to SCC-edges. In practice, this means introducing edges between every pair of edges of two SCCs, if there exists at least one edge between them.
4. Solve the MLE problem with an ANF as in the main manuscript.

The theoryVery conveniently, the method described above fits almost-perfectly into the already-covered theory in SS3. To see this, note that our identifiability theory and following results required a fixed ordering, but it does not need to be the causal one: we can always find an equivalent SCM following the selected ordering by applying KR transports as described in SS3. Therefore, the technical

Figure 6: Example of an SCM with partial knowledge about the causal graph (a) and possible outcomes: (b) in the actual SCM only the edge \(\mathrm{x}_{2}\to\mathrm{x}_{3}\) exists; (c) only the edge \(\mathrm{x}_{3}\to\mathrm{x}_{2}\) exists; (d) both edges exist (and therefore there exists a confounder between them).

Figure 7: Illustrative example (same as in Fig. 6) applying our method for partial information. First, we apply Tarjanâ€™s algorithm [35] to find the SCCs of the graph (rectangles) and build a new DAG where each node is a subset of the original nodes. If, for the SCCs, we use an NF with a general Jacobian matrix (b), we keep the individual edges and treat each SCC as a block. If we instead use an ANF (c), we pick an arbitrary order within each SCC, and merge the individual edges into SCC-wide edges. Red represents intra-SCC edges, and green inter-SCC edges. See App. A.2.2 for more details.

implications of both Thm. 1 and Cor. 2 still apply to this fixed ordering, we only need to re-state their implications with respect to the true causal data-generating process \(\mathcal{M}\).

To this end, we only need to note that all possible graphs, once reduced into a DAG using the partition of SCCs (as in Fig. 6(c)), are exactly equal. In other words, every possible graph shares _the same causal dependencies between SCCs_ with the other possible graphs. If we start treating them like block, calling \(\{\mathbf{x}_{i}\}\subset\bm{S}_{i}\subset\{1,2,\ldots,d\}\) the SCC of the \(i\)-th node, and \(\mathrm{pa}_{\bm{S}_{i}}\) and \(\mathrm{an}_{\bm{S}_{i}}\) the parents and ancestors of every node in the SCC \(\bm{S}_{i}\), then it is clear that we can write for every graph \(\tilde{\bm{A}}\) its observed variables as \(\mathbf{x}_{i}=\tilde{f}_{i}(\mathbf{x}_{\mathrm{pa}_{\bm{S}_{i}}},\tilde{ \mathbf{u}}_{\bm{S}_{i}})\) and, more importantly, we can write its "exogenous" variables as a function of the true ones, i.e., \(\tilde{\mathbf{u}}_{\bm{S}_{i}}=\mathrm{KR}(\mathbf{u}_{\bm{S}_{i}})\), where \(\mathrm{KR}\) is the Knothe-Rosenblatt transport [19, 31]. Note also that it does not depend on the data, i.e., \(\tilde{\mathbf{u}}_{\bm{S}_{i}},\underline{\ldots}\mathbf{x}|\mathbf{u}_{\bm {S}_{i}}\).

**Theorem 3** (Identifiability - Partial knowledge).: If an element of \(\mathcal{F}\times\mathcal{P}_{\mathbf{u}}\), and another from \(\tilde{\mathcal{F}}\times\mathcal{P}_{\mathbf{u}}\), where \(\mathcal{F}\) and \(\tilde{\mathcal{F}}\) are TMI maps with different intra-SCC orders (see above), generate the same observational distribution, then the two processes differ by an invertible, SCC-wise transformation of the variables \(\mathbf{u}\).

Proof.: Call \(\mathcal{M}\) and \(\tilde{\mathcal{M}}\) the elements from \(\mathcal{F}\times\mathcal{P}_{\mathbf{u}}\) and \(\tilde{\mathcal{F}}\times\mathcal{P}_{\mathbf{u}}\), respectively.

W.l.o.g. pick \(\mathcal{M}\), and apply a KR transport to write it down as another element of \(\tilde{\mathcal{F}}\times\mathcal{P}_{\mathbf{u}}\), call it \(\tilde{\mathcal{M}}\), with identical observational distribution as both \(\mathcal{M}\) and \(\tilde{\mathcal{M}}\).

Using Thm. 1, we know that the elements of \(\tilde{\mathcal{M}}\) and \(\tilde{\mathcal{M}}\) differ by an invertible, component-wise transformation \(\bm{h}\). Moreover, we can write \(\tilde{\mathbf{u}}_{\bm{S}_{i}}\) as a function of \(\mathbf{u}_{\bm{S}_{i}}\), as argued above, where \(\{i\}\subset\bm{S}_{i}\subset\{1,2,\ldots,d\}\) are the indexes of the SCC that contains \(\mathbf{u}_{i}\). Putting it all together:

\[\tilde{\mathbf{u}}_{i}=h_{i}(\tilde{\mathbf{u}}_{i})=h_{i}(\mathrm{KR}_{i}( \mathbf{u}_{\bm{S}_{i}}))\,,\] (13)

and, in vectorial form, for each SCC \(\bm{S}_{i}\),

\[\tilde{\mathbf{u}}_{\bm{S}_{i}}=\bm{h}_{\bm{S}_{i}}(\mathrm{KR}_{\bm{S}_{i}}( \mathbf{u}_{\bm{S}_{i}}))=(\bm{h}_{\bm{S}_{i}}\circ\mathrm{KR}_{\bm{S}_{i}})( \mathbf{u}_{\bm{S}_{i}})\] (14)

Q.E.D.

**Corollary 4** (Causal consistency - Partial knowledge).: If a causal NF \(T_{\bm{\theta}}\), with partial knowledge of the causal graph, SCC-wise isolates the exogenous variables of an SCM \(\mathcal{M}\), then \(T_{\bm{\theta}}\) is causally consistent with the true data-generating process, \(\mathcal{M}\), with respect to each SCC.

Proof.: The proof is identical to the one for Cor. 2, but using arguments with respect to the reduced graph after grouping all nodes in their respective SCCs.

Specifically, we can write using Thm. 3 the output of the flow as a function of the exogenous variables, \(T_{\bm{\theta}}(\mathbf{x})_{i}=\bm{h}(\mathbf{u}_{\bm{S}_{i}})\), and using the true causal generator, we have

\[T_{\bm{\theta}}(\mathbf{x})_{i}=\bm{h}(\mathbf{u}_{\bm{S}_{i}})=\bm{h}(f_{\bm {S}_{i}}(\mathbf{x}_{\mathrm{pa}_{\bm{S}_{i}}},\mathbf{x}_{\bm{S}_{i}}))\,,\] (15)

and hence the gradients agree with those from \(\mathcal{M}\), _when looking at the reduced graph_. 

Thm. 3 and Cor. 4 provide analogues to those results from the main manuscript. It is important to note, however, that causal consistency refers to the causal relationships between SCCs _as a whole_, i.e., not at the causal relationships between individual nodes. The reason behind this is the same as why we had to introduced spurious edges in Fig. 6(c): as we fix an ordering within each SCC, we may not be able to model indirect dependencies of nodes of one SCC to another unless we artificially introduce shortcuts. As such, _every result from the main paper holds_, if we treat each SCC (or block) as a whole. That is, when we reason about SCCs instead of individual nodes (note that if the whole causal graph is known every SCC contains a single node), we can safely talk about SCC-identifiability, causal SCC-consistency, and we can perform interventions and compute counterfactuals on SCCs.

## Appendix B The multiple representations of SCMs

### Illustrative example

In this section, we delve a bit deeper into the illustrative example from the main paper (SS4), and write down the maths to understand how are these different models equivalent. For convenience to the reader, we have replicated Fig. 4 in the appendix as Fig. 8.

Recursive SCMAs explained in the manuscript, the usual way of describing an SCM \(\mathcal{M}\) is by providing its recursive equations. In our example (Fig. 7(a)), we have a linear SCM of the form

\[\left\{\begin{aligned} \mathrm{x}_{1}&=\mathrm{u}_{1}\\ \mathrm{x}_{2}&=2\mathrm{x}_{1}+\mathrm{u}_{2}\\ \mathrm{x}_{3}&=3\mathrm{x}_{2}+\mathrm{u}_{3}\end{aligned} \right.\quad,\] (16)

which we can compactly write as \(\mathbf{x}=\bm{G}\mathbf{x}+\bm{I}\mathbf{u}\). However, the recursive equations are not the most convenient ones, as they entail solving the system iteratively according to its causal dependencies.

Unrolled SCMInstead, we can write the equations as a function from \(\mathbf{u}\) to \(\mathbf{x}\) directly. To do this, we can proceed and unroll the equations:

\[\left\{\begin{aligned} \mathrm{x}_{1}&=\mathrm{u}_{1}\\ \mathrm{x}_{2}&=2\mathrm{x}_{1}+\mathrm{u}_{2}\\ \mathrm{x}_{3}&=3\mathrm{x}_{2}+\mathrm{u}_{3}\end{aligned} \right.\quad\Rightarrow\left\{\begin{aligned} \mathrm{x}_{1}&=\mathrm{u}_{1}\\ \mathrm{x}_{2}&=2\mathrm{u}_{1}+\mathrm{u}_{2}\\ \mathrm{x}_{3}&=3(2\mathrm{u}_{1}+\mathrm{u}_{2})+\mathrm{u}_{3} \end{aligned}\right.\quad\Rightarrow\left\{\begin{aligned} \mathrm{x}_{1}&=\mathrm{u}_{1}\\ \mathrm{x}_{2}&=2\mathrm{u}_{1}+\mathrm{u}_{2}\\ \mathrm{x}_{3}&=3(2\mathrm{u}_{1}+\mathrm{u}_{2})+\mathrm{u}_{3} \end{aligned}\right.\quad,\] (17)

which we can write as a multi-step function:

\[\left\{\begin{aligned} \mathrm{z}_{1}^{1}&=\mathrm{u}_{1}\\ \mathrm{z}_{2}^{1}&=\mathrm{u}_{2}\\ \mathrm{z}_{3}^{1}&=\mathrm{u}_{3}\end{aligned}\right. \quad\Rightarrow\left\{\begin{aligned} \mathrm{z}_{1}^{2}&=\mathrm{z}_{1}^{1}\\ \mathrm{z}_{2}^{2}&=2\mathrm{z}_{1}^{1}+\mathrm{z}_{2}^{1}\\ \mathrm{z}_{3}^{2}&=\mathrm{z}_{3}^{1}\end{aligned}\right. \quad\Rightarrow\left\{\begin{aligned} \mathrm{x}_{1}&=\mathrm{z}_{1}^{2}\\ \mathrm{x}_{2}&=\mathrm{z}_{2}^{2}\\ \mathrm{x}_{3}&=3\mathrm{z}_{2}^{2}+\mathrm{z}_{3}^{2} \end{aligned}\right.\quad,\] (18)

that we can once again compactly write as a series of linear operations \(\mathbf{x}=\bm{G}_{3}(\bm{G}_{2}(\bm{G}_{1}\mathbf{u}))\). Note that the matrices \(\bm{G}_{1},\bm{G}_{2},\bm{G}_{3}\) are not unique, and that they are valid as long as they are at most as sparse as \(\bm{G}\), and produce the same final output.

Compacted SCMAnother natural step here is to compress this sequence of linear equations into a single operation. That is, use directly the linear operation described at the end of Eq. 17:

\[\left\{\begin{aligned} \mathrm{x}_{1}&=\mathrm{u}_{1}\\ \mathrm{x}_{2}&=2\mathrm{u}_{1}+\mathrm{u}_{2}\\ \mathrm{x}_{3}&=6\mathrm{u}_{1}+3\mathrm{u}_{2}+ \mathrm{u}_{3}\end{aligned}\right.\quad.\] (19)

Figure 8: Example of the linear SCM \(\{\{\mathrm{x}_{1}=\mathrm{u}_{1}\ ;\ \mathrm{x}_{2}:=\mathrm{2x}_{1}+ \mathrm{u}_{2}\ ;\ \mathrm{x}_{3}:=3\mathrm{x}_{2}+\mathrm{u}_{3}\}\}\) written (a) in its usual recursive formulation; (b) without recursions, with each step made explicit; (c) without recursions, as a single function; and (d) writing \(\mathbf{u}\) as a function of \(\mathbf{x}\). The red dashed arrows show the influence of \(\mathrm{u}_{1}\) on \(\mathrm{x}_{3}\) for all equations from \(\mathbf{u}\) to \(\mathbf{x}\), with the compacted version exhibiting shortcuts (see Â§4). Note that in the linear case we have \(\bm{A}\coloneqq\bm{G}\neq\mathbf{0}\), and that \(\bm{G}_{1},\bm{G}_{2},\bm{G}_{3}\preceq\bm{G}+\bm{I}\) are any three matrices such that their product equals \(\bm{G}^{2}+\bm{G}+\bm{I}\).

And we can derive the same result in vectorial form:

\[\mathbf{x} =\bm{G}\mathbf{x}+\bm{I}\mathbf{u}\Rightarrow\mathbf{x}=\bm{G}( \bm{G}\mathbf{x}+\bm{I}\mathbf{u})+\bm{I}\mathbf{u}\Rightarrow\mathbf{x}=\bm{G} (\bm{G}(\bm{G}\mathbf{x}+\bm{I}\mathbf{u})+\bm{I}\mathbf{u})+\bm{I}\mathbf{u} \Rightarrow\] \[\mathbf{x} =\bm{G}^{\bm{G}^{\bm{\lessgtr}}\mathbf{x}+\bm{G}^{2}\mathbf{u} +\bm{G}\mathbf{u}+\bm{I}\mathbf{u}=(\bm{G}^{2}+\bm{G}+\bm{I})\mathbf{u}\,.\]

Unfortunately, as described in SS4, in this form we cannot longer distinguish indirect paths: we have collapsed all paths into direct paths. Even worse, if there were more than one path between two given nodes, we have combined their contributions into a single path, making it quite difficult to disentangle. This effect can be seen as analogous to the common process in cryptography for sharing secrets: if you have two primes (paths), it is fairly easy to multiply them and obtain their product, but if you have their product (collapsed paths), performing prime factorization of the number is prohibitive.

Inverted SCMFinally, we can take any of these different representations and invert the equations to go from \(\mathbf{x}\) to \(\mathbf{u}\). In this case, it is easier to work with the original equations:

\[\left\{\begin{aligned} &\mathrm{x}_{1}=\mathrm{u}_{1}\\ &\mathrm{x}_{2}=2\mathrm{x}_{1}+\mathrm{u}_{2}\\ &\mathrm{x}_{3}=3\mathrm{x}_{2}+\mathrm{u}_{3}\end{aligned} \right.\quad\Rightarrow\left\{\begin{aligned} &\mathrm{u}_{1}=\mathrm{x}_{1}\\ &\mathrm{u}_{2}=\mathrm{x}_{2}-2\mathrm{x}_{1}\\ &\mathrm{u}_{3}=\mathrm{x}_{3}-3\mathrm{x}_{2}\end{aligned} \right.\quad,\] (20)

and, in vectorial form:

\[\mathbf{x}=\bm{G}\mathbf{x}+\bm{I}\mathbf{u}\Rightarrow\mathbf{u}=(\bm{I}- \bm{G})\mathbf{x}\,.\] (21)

As discussed in the main manuscript, this turns out to be a really convenient SCM representation to work with, as we can obtain the exogenous variables in one go while being causally consistent.

### Non-linear SCM representations

In this section, we discuss how the same representation and reasoning about the causal relationships of a linear SCM from App. B.1 can be to the general case.

To this end, assume that we have a non-linear SCM \(\mathcal{M}\) of the form \(\mathbf{x}=\bm{\tilde{\mathbf{f}}}(\mathbf{x},\mathbf{u})\) where, to ease the reader, imagine that it has the same causal graph as the linear example, so that the reader can use Fig. 8 as a reference again, i.e., assume that

\[\left(\nabla_{\mathbf{x}}\bm{\tilde{\mathbf{f}}}(\mathbf{x},\mathbf{u})\neq \bm{0}\right)=\begin{pmatrix}0&0&0\\ 1&0&0\\ 0&1&0\end{pmatrix}\qquad\text{and}\qquad\left(\nabla_{\mathbf{u}}\bm{\tilde{ \mathbf{f}}}(\mathbf{x},\mathbf{u})\neq\bm{0}\right)=\begin{pmatrix}1&0&0\\ 0&1&0\\ 0&0&1\end{pmatrix}\,,\] (22)

where \(\bm{0}\) is the constant zero function with the same domain and codomain as the Jacobian matrices.

Recursive SCMAlready extensively discussed. This is the representation an SCM is given as.

Unrolled SCMJust as before, we can unroll the equations by having multiple functions \(\mathbf{z}^{l}=\mathbf{f}_{l}(\mathbf{z}^{l-1})\), and in each one we unroll those equations for which we already know the non-recursive equation of its parents, leaving all the other fixed (identity functions). As before, we can write these multiple layers in different ways, as long as they produce the same final function (after composing them, \(\mathbf{f}_{1}\circ\mathbf{f}_{2}\circ\cdots\circ\mathbf{f}_{L}=\mathbf{f}\)), and that they respect the causal dependencies provided by \(\bm{I}+\bm{A}\). This is similar to the linear example, and what we used to design the generative model in Eq. 5 of SS4.

Collapsed SCMJust as before, we can expand all the different layers and write a (probably complex) formula that encompasses all changes in a single step. It is easy to show that the composition of these functions has, in general, a Jacobian matrix structurally equivalent to \(\bm{I}+\sum_{l}^{\mathrm{diam}(\bm{A})}\bm{A}^{l}\). Specifically, their composition will be of the form \(\prod_{l}(\bm{I}+\bm{A})\).

Reverse SCMSince we assume that each \(\tilde{f}_{i}(\mathbf{x}_{\mathrm{pa}_{i}},\mathbf{u}_{i})\) is bijective with respect to \(\mathbf{u}_{i}\), we can always compute its inverse to obtain \(\mathbf{u}_{i}\) as a function of the observed values, \(\mathbf{u}_{i}=\tilde{f}_{i}^{-1}(\mathbf{x}_{\mathrm{pa}_{i}},\mathbf{x}_{i})\). Clearly, its Jacobian matrix will be structurally equivalent to \(\bm{I}+\bm{A}\equiv\bm{I}-\bm{A}\).

Therefore, we can always reason as we did with the linear case, but using Jacobian matrices to talk about causal dependencies between variables, and possibly having a complex and/or non-closed formulation of the generative/abductive functions.

Do-operator: interventions and counterfactuals

### Definition and algorithms

In this section, we extend on the do-operator implementation described in SS5, and provide the step-by-step algorithms to perform interventions (Alg. 1) and compute counterfactuals (Alg. 2).

SemanticsRecalling SS5, the _do-operator_[30], denoted as \(do(\mathbf{x}_{i}=\alpha)\), is defined as a mathematical operator that simulates a physical intervention on an SCM \(\mathcal{M}\), inducing an alternative model \(\mathcal{M}^{\mathcal{I}}\) that fixes the observational value \(\mathbf{x}_{i}=\alpha\), and thus removes any causal dependency on \(\mathbf{x}_{i}\). However, the definition does not describe the specifics on how to implement such an operation.

Usual implementationTraditionally, we are given the recursive representation of an SCM (Fig. 8a), as discussed in App. B. As such, the do-operator \(do(\mathbf{x}_{i}=\alpha)\) is usually carried out by replacing the \(i\)-th equation, i.e., the \(i\)-th component of \(\mathbf{\tilde{f}}\), with a constant function. That is, by doing \(\tilde{f}_{i}^{\mathcal{I}}\coloneqq\alpha\). This yields an intervened SCM \(\mathcal{M}^{\mathcal{I}}=(\mathbf{\tilde{f}}^{\mathcal{I}},P_{\mathbf{u}})\) reflecting the data-generating process after such an intervention. Unfortunately, this implementation of the do-operator is quite specific to the recursive representation of the SCM (Fig. 4a), and does not translate well to the other equivalent representations discussed in SS4 and App. B. The reason for this is that these representations compute the observational values \(\mathbf{x}\) as a vector function of \(\mathbf{u}\), without the iterative sampling process that goes through the intervened value that we replace. The only exception is the abductive model with knowledge on the causal graph and \(L=1\) (see SS4), as it corresponds to the exact same case of Fig. 8d. If we have \(L>1\) however, whether this implementation works or not comes down to the specific implementation to compute the inverse of the network.

Proposed implementationAs discussed in SS4, we instead propose to manipulate the SCM by modifying the exogenous distribution \(P_{\mathbf{u}}\), while keeping the causal generator \(\mathbf{\tilde{f}}\) untouched. Specifically, an intervention \(do(\mathbf{x}_{i}=\alpha)\) updates \(P_{\mathbf{u}}\), to have positive density mass on only those values that, when transformed to endogenous variables, the intervened variable yields the intervened value, \(\mathbf{x}_{i}=\alpha\), while keeping the rest of distributions unaltered.

In other words, we define the intervened SCM as \(\mathcal{M}^{\mathcal{I}}=(\mathbf{\tilde{f}},P_{\mathbf{u}}^{\mathcal{I}})\), where the density of the updated distribution \(P_{\mathbf{u}}^{\mathcal{I}}\) is of the form

\[p^{\mathcal{I}}(\mathbf{u})\propto p(\mathbf{u})\cdot\delta_{\{\mathbf{\tilde{ f}}_{i}(\mathbf{x},\mathbf{u})=\alpha\}}(\mathbf{u})\,,\] (23)

and where the distributions of the rest of variables remain the same. Using the acyclic assumption, we know that the only way of altering the value of \(\mathbf{x}_{i}\) without altering those of its parents is through \(\mathsf{u}_{i}\) and, using the causal sufficiency assumption, we can squeeze the Dirac delta directly in the distribution of the \(i\)-th exogenous variable, such that:

\[p^{\mathcal{I}}(\mathbf{u})=p_{i}^{\mathcal{I}}(\mathsf{u}_{i}|\mathbf{u}_{j \neq i})\cdot\prod_{j\neq i}p_{j}(\mathsf{u}_{j})\,,\quad\text{ with }\quad p_{i}^{\mathcal{I}}(\mathsf{u}_{i}|\mathbf{u}_{j \neq i})\propto p_{i}(\mathsf{u}_{i})\cdot\delta_{\{\mathbf{\tilde{f}}_{i}( \mathbf{x},\mathbf{u})=\alpha\}}(\mathbf{u})\,.\] (24)

In the case we consider, where all the generators are bijective given the parent nodes, the set \(\delta_{\{\mathbf{\tilde{f}}_{i}(\mathbf{x},\mathbf{u})=\alpha\}}(\mathbf{u})\) contains a single element, and therefore in the main paper we simply write the \(i\)-th density as \(p_{i}^{\mathcal{I}}(\mathsf{u}_{i}|\mathbf{u}_{j\neq i})=\delta_{\{\mathbf{ \tilde{f}}_{i}(\mathbf{x},\mathbf{u})=\alpha\}}(\mathbf{u})\). Note, as discussed in the main paper, that the density at this point should be positive, in other words, the element that yields \(\alpha\) (and therefore \(\alpha\)) should be a plausible value.

Since this implementation does not make any assumption at all in the functional form of the generator, but directly works on the distribution of the exogenous variables, it can be implemented on any SCM representation (see Fig. 8 in App. B.1) and, hence, on any of the architectures for the causal NF in SS4. Notice, however, that in order to properly work, that the data-generative process should be causally consistent (so that changes are properly propagated), and that it should properly isolate \(\mathbf{u}\) (so that \(\mathsf{u}_{i}\) accounts only for the stochasticity of \(\mathbf{x}_{i}\) not explained by its parents).

AlgorithmsThe step-by-step algorithms to perform interventions and compute counterfactuals, using the described algorithm, are presented in Alg. 1 and Alg. 2, respectively. It follows the same description as the one given at the end of SS5, and the only difference between both algorithms is the way that we obtain samples from the observed distribution (generated vs. given).

Theoretical resultsHere, we briefly discuss way this method works, i.e., why the proposed implementation removes every dependency from the descendants with respect to the ancestors that go through the intervened value, as it is not directly obvious.

To see why, take the usual recursive representation of an SCM in the illustrative example from App. B.1 (Fig. 8a), and assume that we do \(do(\text{x}_{2}=\alpha)\), where \(\text{x}_{2}=2\text{x}_{1}+\text{u}_{2}\) in this example. By updating the density \(p_{2}(\text{u}_{2})\), we have basically fixed the value of \(\text{u}_{2}\) to be the only one that keeps \(\text{x}_{2}=\alpha\) given \(\text{x}_{1}\), i.e., \(\text{u}_{2}=\alpha-2\text{x}_{1}\) (this can be clearly seen in Fig. 8d). If we now compute the dependency of \(\text{x}_{3}\) on \(\text{x}_{1}\), we get

\[\frac{\text{d}\,\text{x}_{3}}{\text{d}\,\text{x}_{1}}=\frac{\partial\text{x} _{3}}{\partial\text{x}_{2}}\frac{\text{d}\,\text{x}_{2}}{\text{d}\,\text{x}_{ 1}}=\frac{\partial\text{x}_{3}}{\partial\text{x}_{2}}\left(\frac{\partial \text{x}_{2}}{\partial\text{x}_{1}}+\frac{\partial\text{x}_{2}}{\partial \text{u}_{2}}\frac{\text{d}\,\text{u}_{2}}{\text{d}\,\text{x}_{1}}\right)= \frac{\partial\text{x}_{3}}{\partial\text{x}_{2}}\left(2+1\cdot(-2)\right)=0\,.\] (25)

In layman's terms, the value of \(\text{u}_{2}\) is chosen such that it fixes the value of \(\alpha\), countering any influence that the parents could have on \(\text{x}_{2}\) (or any of its intermediate values), and consequently in any of its descendants.

The general case can be similarly proven. Suppose that we do \(do(\text{x}_{2}=\alpha)\), and that we want to compute the indirect influence of an ancestor, \(\text{x}_{1}\), on a descendant, \(\text{x}_{3}\), passing through \(\text{x}_{2}\). Since we are fixing the value of \(\text{u}_{2}\) (the input of the network) to produce an observed value \(\text{x}_{2}\) (the output of the network) of \(\alpha\), we can use implicit differentiation to compute the influence of \(\text{u}_{1}\) (and therefore \(\text{x}_{1}\)) on \(\text{x}_{2}\) via \(\text{u}_{2}\):

\[\alpha=\text{x}_{2}(\text{u}_{1},\text{u}_{2})\xRightarrow{\frac{\text{d}\, \text{u}_{1}}{\partial\text{u}_{2}}}0=\frac{\partial\text{x}_{2}}{\partial \text{u}_{1}}+\frac{\partial\text{x}_{2}}{\partial\text{u}_{2}}\frac{\text{d }\,\text{u}_{2}}{\text{d}\,\text{u}_{1}}\Rightarrow\frac{\partial\text{x}_{2 }}{\partial\text{u}_{2}}\frac{\text{d}\,\text{u}_{2}}{\text{d}\,\text{u}_{1}}= -\frac{\partial\text{x}_{2}}{\partial\text{u}_{1}}\,,\] (26)

and, similar to Eq. 25, any _indirect_ influence of the ancestor, \(\text{u}_{1}\), on the descendant, \(\text{x}_{3}\), through this intermediate variable, \(\text{x}_{2}\), cancels out:

\[\frac{\partial\text{x}_{3}}{\partial\text{x}_{2}}\frac{\text{d}\,\text{x}_{2 }}{\text{d}\,\text{u}_{1}}=\frac{\partial\text{x}_{3}}{\partial\text{x}_{2}} \left(\frac{\partial\text{x}_{2}}{\partial\text{u}_{1}}+\frac{\partial\text{ x}_{2}}{\partial\text{u}_{2}}\frac{\text{d}\,\text{u}_{2}}{\text{d}\,\text{u}_{1}} \right)=\frac{\partial\text{x}_{3}}{\partial\text{x}_{2}}\cdot 0=0\,.\] (27)

We have proven this result not only theoretically, but also empirically through the accurate interventions computed for all the experiments from SS6 and App. D. Moreover, this lack of correlation between ancestors and descendants through the intervened variables is clearly shown in pair plots such as the ones in Figs. 12 and 13.

### Interventions in previous works

We now put our implementation of the do-operator (see SS5 and App. C) into context, by describing how the methods compared in SS6, namely CAREFL [16] and VACA [34], proposed to perform interventions with their models.

CarefL [16]Two different algorithms were proposed to sample from an interventional distribution in CAREFL: i) a sequential algorithm which mimics the usual implementation of the do-operator with the recursive representation of the SCM; and ii) a parallel algorithm that samples the counterfactual in a single call. While the first algorithm works, the parallel one--which is the one actually implemented--only works when intervening on root nodes.

This second algorithm for \(do(\text{x}_{2}=\alpha)\) is described as follows (see Alg. 2 in [16]): i) sample \(\mathbf{u}\) from \(P_{\mathbf{u}}\); ii) set \(\text{u}_{i}\) to the \(i\)-th value obtained by applying the flow, \(T_{\boldsymbol{\theta}}\), to an observation with \(\text{x}_{i}=\alpha\) and \(\text{x}_{j}=0\) for \(j\neq i\); and iii) return the value obtained by \(T_{\boldsymbol{\theta}}^{-1}\) with \(\mathbf{u}\).

While this algorithm resembles the one we proposed, the proposed method does not have into account that the value of \(\text{u}_{i}\) to fix \(\alpha\)_does depend_ on the observed values of its parents, which is clear by looking at the linear illustrative example from Fig. 8d. As a consequence, the algorithm only works when the node has no parents, which is why we replaced it by the one we proposed for the comparisons in SS6 and App. D.

Vaca [34]Based on GNNs, the approach for intervening on VACA is completely reminiscent to the traditional implementation. Specifically, the authors propose to sever those edges in every layer of the GNN whose endpoints fall in the path generating the intervened variable, so that the ancestors have no way to influence it by design.

While the previous statement is true: ancestors cannot influence the intervened variable nor its descendants, here we argue that this process would require us to "recalibrate" the model, as the middle computations after an intervention change in more complex ways than removing the ancestors from the equation, while keeping the rest unchanged.

To see this, consider the following non-linear triangle SCM:

\[\left\{\begin{aligned} \text{x}_{1}&=\text{u}_{1}\\ \text{x}_{2}&=\text{x}_{1}^{2}\text{u}_{2}\\ \text{x}_{3}&=2\text{x}_{1}+\frac{\text{x}_{2}}{ \text{x}_{1}}+\frac{\text{x}_{2}}{\text{x}_{1}^{2}}+\text{u}_{3}\end{aligned}\right.\] (28)

which VACA could learn with two layers through the following operations:

\[\left\{\begin{aligned} \text{z}_{1}&=\text{u}_{1}\\ \text{z}_{2}&=\text{u}_{1}\text{u}_{2}\\ \text{z}_{3}&=\text{u}_{1}+\text{u}_{2}+\text{u}_{3} \end{aligned}\right. \left\{\begin{aligned} \text{x}_{1}&=\text{z}_{1}\\ \text{x}_{2}&=\text{z}_{1}\text{z}_{2}\\ \text{x}_{3}&=\text{z}_{1}+\text{z}_{2}+\text{z}_{3} \end{aligned}\right.\quad.\] (29)

Now, if we were to intervene with \(do(\text{x}_{2}=\alpha)\), the real SCM would yield:

\[\left\{\begin{aligned} \text{x}_{1}&=\text{u}_{1}\\ \text{x}_{2}&=\alpha\\ \text{x}_{3}&=2\text{x}_{1}+\frac{\alpha}{\text{x}_{1}}+ \frac{\alpha}{\text{x}_{1}^{2}}+\text{u}_{3}\end{aligned}\right.\] (30)

while VACA would yield:

\[\left\{\begin{aligned} \text{z}_{1}&=\text{u}_{1}\\ \text{z}_{2}&=\alpha\\ \text{z}_{3}&=\text{u}_{1}+\alpha+\text{u}_{3}\end{aligned}\right. \left\{\begin{aligned} \text{x}_{1}&=\text{z}_{1}\\ \text{x}_{2}&=\alpha\\ \text{x}_{3}&=\text{z}_{1}+\alpha+\text{z}_{3}\end{aligned}\right. \Rightarrow\left\{\begin{aligned} \text{x}_{1}&=\text{u}_{1}\\ \text{x}_{2}&=\alpha\\ \text{x}_{3}&=\text{2x}_{1}+2\alpha+\text{u}_{3}\end{aligned}\right.,\] (31)

where we can clearly see that the expression for \(\text{x}_{3}\) is different. In contrast, our causal NF would keep the generator as it is, and set \(\text{u}_{2}\) to \(\alpha/\text{x}_{1}^{2}\), yielding the correct value.

Experimental details and extra results

In this section, we complement the description of the experimental section from SS6, and provide the reader with additional results in the following subsections. First, we describe the details common to every experiment, and delve into the specifics of each experiment in their respective subsections.

HardwareEvery individual experiment shown in this paper ran on a single CPU with \(8\,\mathrm{GB}\) of RAM. To run all experiments, we used a local computing cluster with an automatic job assignment system, so we cannot ensure the specific CPU used for each particular experiment. However, we know that every experiment used one of the following CPUs picked randomly given the demand when scheduled: AMD EPYC 7702 64-Core Processor, AMD EPYC 7662 64-Core Processor, Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz, or Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz.

Training and evaluation methodologyFor every experiment, we generated with using synthetic SCM a dataset with \(20\,000\) training samples, \(2500\) validation samples, and \(2500\) test samples. We ran every model for \(1000\) epochs, and the results shown in the manuscript correspond to the test set evaluation at the last epoch. For the optimization, we used Adam [17] with an initial learning rate of \(0.001\), and reduce the learning rate with a decay factor of \(0.95\) when it reaches a plateau longer than \(60\) epochs. For hyperparameter tuning, we always perform a grid search with similar budget, and select the best hyperparameter combination according to validation loss, reporting always results from the test dataset in the manuscript. Every experiment is repeated \(5\) times, and we show averages and standard deviations.

DatasetsThis section provides all the information of the SCMs employed in the empirical evaluation of SS6 of the main paper, and the following subsections. The exogenous variables always follow a standard normal distribution \(\mathcal{N}(0,1)\), except for LargeBD, where a uniform distribution \(\mathcal{U}(0,1)\) is used instead. Subsequently, we define the \(12\) SCMs employed--encompassing both linear and non-linear equations--and we additionally provide their causal graph in Fig. 9.

Let us first define the softplus operation as \(s(x)=\log\left(1.0+e^{x}\right)\).

3-ChainIn: \[\tilde{f}_{1}(\mathsf{u}_{1}) =\mathsf{u}_{1}\] (32) \[\tilde{f}_{2}(\mathsf{x}_{1},\mathsf{u}_{2}) =10\cdot\mathsf{x}_{1}-\mathsf{u}_{2}\] (33) \[\tilde{f}_{3}(\mathsf{x}_{2},\mathsf{u}_{3}) =0.25\cdot\mathsf{x}_{2}+2\cdot\mathsf{u}_{3}\] (34)

3-ChainIn: \[\tilde{f}_{1}(\mathsf{u}_{1}) =\mathsf{u}_{1}\] (35) \[\tilde{f}_{2}(\mathsf{x}_{1},\mathsf{u}_{2}) =e^{\mathsf{x}_{1}/2}+\mathsf{u}_{2}/4\] (36) \[\tilde{f}_{3}(\mathsf{x}_{2},\mathsf{u}_{3}) =\frac{(\mathsf{x}_{2}-5)^{3}}{15}+\mathsf{u}_{3}\] (37)

4-ChainIn: \[\tilde{f}_{1}(\mathsf{u}_{1}) =\mathsf{u}_{1}\] (38) \[\tilde{f}_{2}(\mathsf{x}_{1},\mathsf{u}_{2}) =5\cdot\mathsf{x}_{1}-\mathsf{u}_{2}\] (39) \[\tilde{f}_{3}(\mathsf{x}_{2},\mathsf{u}_{3}) =-0.5\cdot\mathsf{x}_{2}-1.5\cdot\mathsf{u}_{3}\] (40) \[\tilde{f}_{4}(\mathsf{x}_{3},\mathsf{u}_{4}) =\mathsf{x}_{3}+\mathsf{u}_{4}\] (41)

5-ChainIn: \[\tilde{f}_{1}(\mathsf{u}_{1}) =\mathsf{u}_{1}\] (42) \[\tilde{f}_{2}(\mathsf{x}_{1},\mathsf{u}_{2}) =10\cdot\mathsf{x}_{1}-\mathsf{u}_{2}\] (43) \[\tilde{f}_{3}(\mathsf{x}_{2},\mathsf{u}_{3}) =0.25\cdot\mathsf{x}_{2}+2\cdot\mathsf{u}_{3}\] (44) \[\tilde{f}_{4}(\mathsf{x}_{3},\mathsf{u}_{4}) =\mathsf{x}_{3}+\mathsf{u}_{4}\] (45)\[\tilde{f}_{1}(\mathsf{u}_{1}) =\mathsf{u}_{1}\] (68) \[\tilde{f}_{2}(\mathsf{x}_{1},\mathsf{u}_{2}) =s(1-\mathsf{x}_{1})+\sqrt{3/20}\cdot\mathsf{u}_{2}\] (69) \[\tilde{f}_{3}(\mathsf{x}_{1},\mathsf{x}_{2},\mathsf{u}_{3}) =\text{tanh}(2\cdot\mathsf{x}_{2})+1.5\cdot\mathsf{x}_{1}-1+ \text{tanh}(\mathsf{u}_{3})\] (70) \[\tilde{f}_{4}(\mathsf{x}_{3},\mathsf{u}_{4}) =\frac{\mathsf{x}_{3}-4}{5}+3+\frac{1}{\sqrt{10}}\cdot\mathsf{u}_{4}\] (71)

**Simpson\({}_{\text{symprod}}\)**:

\[\tilde{f}_{1}(\mathsf{u}_{1}) =\mathsf{u}_{1}\] (72) \[\tilde{f}_{2}(\mathsf{x}_{1},\mathsf{u}_{2}) =2\cdot\mathsf{tanh}(2\cdot\mathsf{x}_{1})+\frac{1}{\sqrt{10}}\cdot \mathsf{u}_{2}\] (73) \[\tilde{f}_{3}(\mathsf{x}_{1},\mathsf{x}_{2},\mathsf{u}_{3}) =0.5\cdot\mathsf{x}_{1}\cdot\mathsf{x}_{2}+\frac{1}{\sqrt{2}}\cdot \mathsf{u}_{3}\] (74) \[\tilde{f}_{4}(\mathsf{x}_{1},\mathsf{u}_{4}) =\mathsf{tanh}(1.5\cdot\mathsf{x}_{1})+\sqrt{\frac{3}{10}}\cdot \mathsf{u}_{4}\] (75)

**Triangle\({}_{\text{LIN}}\)**:

\[\tilde{f}_{1}(\mathsf{u}_{1}) =\mathsf{u}_{1}+1\] (76) \[\tilde{f}_{2}(\mathsf{x}_{1},\mathsf{u}_{2}) =10\cdot\mathsf{x}_{1}-\mathsf{u}_{2}\] (77) \[\tilde{f}_{3}(\mathsf{x}_{1},\mathsf{x}_{2},\mathsf{u}_{3}) =0.5\cdot\mathsf{x}_{2}+\mathsf{x}_{1}+\mathsf{u}_{3}\] (78)

**Triangle\({}_{\text{LIN}}\)**:

\[\tilde{f}_{1}(\mathsf{u}_{1}) =\mathsf{u}_{1}+1\] (79) \[\tilde{f}_{2}(\mathsf{x}_{1},\mathsf{u}_{2}) =2\cdot\mathsf{x}_{1}^{2}+\mathsf{u}_{2}\] (80) \[\tilde{f}_{3}(\mathsf{x}_{1},\mathsf{x}_{2},\mathsf{u}_{3}) =\frac{20}{1+e^{-\mathsf{x}_{2}^{2}+\mathsf{x}_{1}}}+\mathsf{u}_{3}\] (81)

### Ablation: Time complexity

As the first additional ablation study, we evaluate the time complexity of the design choices introduced in SS4. Fig. 10 summarizes the results, where the x-axis represents the number of nodes in the dataset,

Figure 9: Causal graph of the different SCMs considered in Â§6 and App. D.

\(d\), and the y-axis indicates the time (in microseconds) required for a single forward pass of the normalizing flow during training. We can find several interesting insights:

ResultsFirst, abductive models (\(\mathbf{x}\rightarrow\mathbf{u}\), solid lines) train significantly faster compared to generative models (\(\mathbf{u}\rightarrow\mathbf{x}\), dotted lines). This emphasizes the computational cost associated with inverting autoregressive flows during training. Furthermore, it is worth noting that the time complexity of abductive models remains constant regardless of the number of nodes \(d\), whereas the complexity of generative models increases linearly with \(d\), as indicated in the right-most column of Tab 1. Note that we should see the exact opposite behaviour when sampling.

Second, the inclusion of Jacobian regularization, represented by a star marker, introduces a significant time overhead. This observation is clearly depicted in Fig. 10, where plot lines with the star marker are consistently above their counterparts.

Finally, leveraging causal graph information or relying solely on ordering (represented in orange and green, respectively) has minimal impact on computational time. This is once again clear in Fig. 10, as green and orange pairs largely coincide.

### Ablation: Base distribution

We now assess to which extent a mismatch of the distribution \(P_{\mathbf{u}}\) between the SCM and the causal NF negatively affects performance. To this end, we consider two more complex SCMs--Simpson[11] and Triangle[34]--and distributions--Normal and Laplace--for which we either fix or learn their parameters during training. Both SCMs use a standard Normal distribution for \(P_{\mathbf{u}}\).

Hyperparameter tuningWhile we fixed the flow to have a single MAF [24] layer with ELU [4] activation functions, we determined through cross-validation the optimal number of layers and hidden units of the MLP network within MAF. Specifically, we considered the following combinations (\([a,b]\) represents two layers with \(a\) and \(b\) hidden units): \([16,16,16,16],[32,32,32],[16,16,16],[32,32],[32],[64]\). As discussed at the start of the section, we report test results for the configuration with the best validation performance at the last epoch.

ResultsThe results, shown in Fig. 10(a), reveal a notable distinction between Normal and Laplace distributions in terms of density estimation. However, this discrepancy appears to have minimal implications for Average Treatment Effect (ATE) and counterfactual estimation. We hypothesize that this disparity originates from dissimilarities in their tails, as it can be inferred by the slight edge of Normal over Laplace on the last column--which measures _per sample_ differences--where bigger errors happen at the outliers elements which are, by definition, scarce. Interestingly, with this particular architecture of causal NF, every model struggles to model the denser Triangle SCM.

### Ablation: Flow architecture

Considering the observed challenges faced by the Masked Autoregressive Flow (MAF) [24] layer in accurately modelling the Triangle SCM in the previous experiment, we further investigate the potential impact of flow architecture on performance.

Figure 10: Time complexity comparison between the different design choices discussed in Â§4.

Hyperparameter TuningWe cross-validate again the optimal number of layers and hidden units of the MLP internally used by the unique layer of the Causal NF. We consider the following values (\([a,b]\) represents two layers with \(a\) and \(b\) hidden units): \([32,32,32]\), \([16,16,16]\), \([32,32]\), \([32]\), and \([64]\). As before, test results are reported for the configuration that achieved the best performance on the validation set at the final epoch.

ResultsFig. (b)b summarizes our results, where we consider a Causal NF with one MAF [24] layer (depicted in orange), and a Causal NF with a Neural Spline Flow (NSF) [9] as layer (depicted in blue). Note that, NSFs are built on top of MAFs and abandoned the realm of affine ANFs, and are thus expected to outperform MAFs in general. We employed the same set of SCMs as in App. D.2.

Our empirical analysis reveals that the NSF consistently outperforms the MAF across the three metrics: observational distribution (measured by the KL divergence), ATE estimation, and counterfactual estimation. Whilst expected, these findings highlight the practical implications of selecting an appropriate flow architecture, which should be taken into consideration by practitioners.

### Comparison: Extra non-linear SCMs

In this section, we complement the results from SS6.2 and provide a more extensive comparison of the proposed Causal NF, along with CAREFL [16] and VACA [34], on additional datasets.

Hyperparameter TuningFor VACA, we cross-validated the dropout rate with values \(\{0.0,0.1\}\), the GNN layer architecture with \(\{\text{GIN},\text{PNA},\text{PNADisjoint}\}\), (see [34] for details), and the number of layers in the MLP prior to the GNN with choices \(\{1,2\}\). For CAREFL, we cross-validated the number of layers in the flow, \(\{1,\operatorname{diam}\boldsymbol{A}\}\), and the number of layers and hidden units in the MLP composing the flow layers (same format as before), \(\{[16,16,16],[32,32],[32],[64]\}\). For Causal NF, we used the abductive model with a single layer, and cross-validated the number of layers and hidden units in the MLP composing the layer of the flow with values \(\{[16,16,16,16],[32,32,32],[16,16,16],[32,32],[32],[64]\}\). We report test results for the configuration with the best validation performance at the final epoch.

ResultsTab 4 shows the performance of each model for all the considered datasets, further validating the conclusions drawn in the main manuscript: the proposed Causal NF consistently outperforms both CAREFL and VACA in terms of performance and computational efficiency. The performance of VACA is notably inferior, and its computation time is significantly longer, primarily due to the complexity of graph neural networks (GNNs). Our Causal NF achieves similar performance to CAREFL in terms of observational fitting, while surpassing it on interventional and counterfactual estimation tasks. Additionally, Causal NF outperforms CAREFL in computational speed. This is to be expected since the optimal CAREFL architectures often have multiple layers, resulting in increased computation time. In contrast, Causal NF has a single layer, reducing computational complexity.

Fig. 12 qualitative proofs the effectiveness of the proposed Causal NF in accurately modelling both observational and interventional distributions for the \(\textsc{Simpson}_{\textsc{Nlin}}\) dataset. In this plot, blue represent the real distribution/samples, while orange represents the ones generated by Causal NF. Fig. (a)a clearly shows that the model successfully captured the correlations among all variables in the observational distribution. Furthermore, Fig. (b)b displays the interventional distribution

Figure 11: Performance on the \(\textsc{Simpson}_{\textsc{Nlin}}\) and \(\textsc{Triangle}_{\textsc{LIN}}\) datasets of causal NFs with a) different base distributions (Normal and Laplace), where \(\boldsymbol{\theta}\) indicates that we learn the parameters of the base distribution; and b) flow architectures. Differences in base distribution affect KL divergence, while the choice of flow architecture influences the overall performance.

obtained when we do \(do(\text{x}_{3}=-1.09)\), i.e., when we intervene on the \(25\)-th empirical percentile of x\({}_{3}\). Remarkably, Causal NF accurately learns the distribution of descendant variables, i.e., x\({}_{4}\), and effectively breaks any dependency between the ancestors of the intervened variable and x\({}_{4}\). Additionally, Fig. 13 shows a similar analysis for 5-Chain\({}_{\text{LIN}}\), when we perform \(do(\text{x}_{3}=2.18)\)--which corresponds to intervening on the \(75\)-th percentile of x\({}_{3}\)--clearly showing that the correlations not involving the intervened path (x\({}_{1}\to\text{x}_{2}\) and x\({}_{4}\to\text{x}_{5}\)) are preserved.

## Appendix E Details on the fairness use-case

In this section, we provide additional details on the use-case of fairness auditing and classification using the German dataset [8], whose causal graph is shown in Fig. 14.

TrainingFor this section, we performed minimal hyperparameter tuning, and only tested a few combinations by hand. We decided to use a Neural Spline Flow (NSF) [9] for the single layer of the Causal NF, which internally uses an MLP with 3 layers, and 32 hidden units each. We use Adam [17] as the optimizer, with a learning rate of \(0.01\), along with a plateau scheduler with a decay factor of

\begin{table}
\begin{tabular}{c l c c c c c c} \hline \hline  & & \multicolumn{3}{c}{Performance} & \multicolumn{3}{c}{Time Evaluation (\(\text{Î¼s}\))} \\ \cline{3-8} Dataset & Model & \(\text{KL}\) & \(\text{ATE}_{\text{RMSE}}\) & \(\text{CF}_{\text{RMSE}}\) & Training & Evaluation & Sampling \\ \hline
3-Chain & Causal NF & \(\mathbf{0.00_{0.00}}\) & \(\mathbf{0.05_{0.01}}\) & \(\mathbf{0.04_{0.01}}\) & \(\mathbf{0.41_{0.06}}\) & \(\mathbf{0.48_{0.10}}\) & \(\mathbf{0.76_{0.06}}\) \\ lin & CAREFL\({}^{\dagger}\) & \(\mathbf{0.00_{0.00}}\) & \(0.20_{0.13}\) & \(0.20_{0.09}\) & \(0.68_{0.24}\) & \(0.97_{0.33}\) & \(1.94_{0.77}\) \\
[34] & VACA & \(4.44_{1.03}\) & \(5.76_{0.07}\) & \(4.98_{0.10}\) & \(36.19_{1.54}\) & \(28.33_{0.72}\) & \(75.34_{5.58}\) \\ \hline
3-Chain & Causal NF & \(\mathbf{0.00_{0.00}}\) & \(\mathbf{0.03_{0.01}}\) & \(\mathbf{0.02_{0.01}}\) & \(\mathbf{0.52_{0.06}}\) & \(\mathbf{0.56_{0.03}}\) & \(\mathbf{1.02_{0.05}}\) \\ lin & CAREFL\({}^{\dagger}\) & \(\mathbf{0.00_{0.00}}\) & \(\mathbf{0.05_{0.02}}\) & \(0.04_{0.02}\) & \(\mathbf{0.60_{0.22}}\) & \(0.84_{0.22}\) & \(1.66_{0.41}\) \\
[34] & VACA & \(12.82_{1.00}\) & \(1.54_{0.03}\) & \(1.32_{0.02}\) & \(39.45_{14.12}\) & \(30.93_{2.30}\) & \(84.36_{0.60}\) \\ \hline
4-Chain & Causal NF & \(\mathbf{0.00_{0.00}}\) & \(\mathbf{0.07_{0.02}}\) & \(\mathbf{0.04_{0.01}}\) & \(\mathbf{0.56_{0.08}}\) & \(\mathbf{0.62_{0.15}}\) & \(\mathbf{1.54_{0.40}}\) \\ lin & CAREFL\({}^{\dagger}\) & \(\mathbf{0.00_{0.00}}\) & \(0.16_{0.07}\) & \(0.14_{0.04}\) & \(\mathbf{0.70_{0.28}}\) & \(0.99_{0.20}\) & \(2.85_{0.54}\) \\ VACA & \(13.14_{0.73}\) & \(3.82_{0.01}\) & \(3.72_{0.05}\) & \(61.85_{0.56}\) & \(49.31_{1.11}\) & \(92.06_{0.93}\) \\ \hline
5-Chain & Causal NF & \(0.01_{0.00}\) & \(\mathbf{0.12_{0.02}}\) & \(\mathbf{0.08_{0.01}}\) & \(\mathbf{0.62_{0.19}}\) & \(\mathbf{0.69_{0.15}}\) & \(\mathbf{1.91_{0.44}}\) \\ lin & CAREFL\({}^{\dagger}\) & \(\mathbf{0.00_{0.00}}\) & \(0.47_{0.23}\) & \(4.06_{0.22}\) & \(\mathbf{0.79_{0.41}}\) & \(1.19_{0.25}\) & \(4.21_{0.87}\) \\  & VACA & \(17.31_{0.84}\) & \(5.95_{0.05}\) & \(6.06_{0.08}\) & \(103.75_{10.04}\) & \(80.81_{11.06}\) & \(124.52_{20.86}\) \\ \hline Collider & Causal NF & \(\mathbf{0.00_{0.00}}\) & \(\mathbf{0.02_{0.01}}\) & \(\mathbf{0.01_{0.00}}\) & \(\mathbf{0.46_{0.12}}\) & \(0.56_{0.11}\) & \(0.95_{0.19}\) \\ lin & CAREFL\({}^{\dagger}\) & \(\mathbf{0.00_{0.00}}\) & \(\mathbf{0.02_{0.01}}\) & \(\mathbf{0.01_{0.00}}\) & \(\mathbf{0.39_{0.07}}\) & \(\mathbf{0.45_{0.05}}\) & \(\mathbf{0.74_{0.07}}\) \\
[34] & VACA & \(13.45_{0.43}\) & \(0.22_{0.01}\) & \(0.86_{0.02}\) & \(37.22_{3.55}\) & \(28.77_{4.22}\) & \(71.21_{6.73}\) \\ \hline \hline  & Causal NF & \(\mathbf{0.00_{0.00}}\) & \(\mathbf{0.03_{0.01}}\) & \(\mathbf{0.01_{0.00}}\) & \(\mathbf{0.52_{0.05}}\) & \(\mathbf{0.50_{0.08}}\) & \(\mathbf{1.57_{0.57}}\) \\ lin & CAREFL\({}^{\dagger}\) & \(\mathbf{0.00_{0.00}}\) & \(\mathbf{0.04_{0.01}}\) & \(0.02_{0.00}\) & \(\mathbf{0.60_{0.17}}\) & \(0.78_{0.16}\) & \(\mathbf{2.39_{1.06}}\) \\ lin & VACA & \(8.70_{0.73}\) & \(0.87_{0.02}\) & \(1.43_{0.02}\) & \(45.84_{1.46}\) & \(34.66_{0.39}\) & \(73.29_{7.70}\) \\ \hline Fork & Causal NF & \(\mathbf{0.00_{0.00}}\) & \(\mathbf{0.07_{0.00}}\) & \(\mathbf{0.07_{0.00}}\) & \(\mathbf{0.63_{0.16}}\) & \(\mathbf{0.74_{0.31}}\) & \(\mathbf{1.84_{0.84}}\) \\ lin & CAREFL\({}^{\dagger}\) & \(0.01_{0.01}\) & \(0.11_{0.04}\) & \(0.18_{0.07}\) & \(\mathbf{0.57_{0.17}}\) & \(\mathbf{0.77_{0.08}}\) & \(\mathbf{1.96_{0.17}}\) \\ lin & VACA & \(5.09_{0.60}\) & \(2.01_{0.03}\) & \(3.19_{0.06}\) & \(49.22_{5.48}\) & \(42.13_{2.95}\) & \(101.02_{18.94}\) \\ \hline LargeBD & Causal NF & \(\mathbf{1.51_{0.04}}\) & \(\mathbf{0.02_{0.00}}\) & \(\mathbf{0.01_{0.00}}\) & \(\mathbf{0.52_{0.10}}\) & \(\mathbf{0.60_{0.17}}\) & \(\mathbf{3.05_{0.66}}\) \\ lin & CAREFL\({}^{\dagger}\) & \(1.51_{0.05}\) & \\(0.9\) and a patience parameter of 60 epochs. The training is performed for \(1000\) epochs, and the results are reported using 5-fold cross-validation with a \(80-10-10\) split for train, validation, and test data.

ResultsOn addition to the results from SS7, Fig. 15 shows two pair plots from one of the \(5\) runs, chosen at random. The true empirical distribution is shown in blue, and the learnt distribution by Causal NF is depicted in orange. Specifically, Fig. 14(a) illustrates the observational distribution, and Fig. 14(b) the interventional distribution, obtained when we intervene on the _sex_ variable and set it to 1, i.e., \(do(\mathbf{x}_{1}=1)\). We can observe that Causal NF achieves a remarkable fit in both cases, demonstrating its capability to handle discrete data, and partial knowledge of the causal graph.

Figure 12: Pair plot of real (in blue) and generated (in orange) data of Simpson\({}_{\text{\tiny{NLI}}}\). On the left are samples from the true and learnt observational distribution. On the right are samples from the true and learnt interventional distribution when \(do(\mathbf{x}_{3}=-1.09)\). The plot illustrates that the dependency of \(\mathbf{x}_{4}\) on the ancestors of \(\mathbf{x}_{3}\), namely \(\mathbf{x}_{1}\) and \(\mathbf{x}_{2}\), is effectively broken.

Figure 13: Pair plot of real (in blue) and generated (in orange) data of 5-Chain\({}_{\text{\tiny{LIN}}}\). On the left are samples from the true and learnt observational distribution. On the right are samples from the true and learnt interventional distribution when \(do(\mathbf{x}_{3}=2.18)\). The plot illustrates that the dependency of \(\mathbf{x}_{4}\) and \(\mathbf{x}_{5}\) on the ancestors of \(\mathbf{x}_{3}\), namely \(\mathbf{x}_{1}\) and \(\mathbf{x}_{2}\), is effectively broken.

## Appendix A

Figure 14: Partial causal graph used for the German Credit dataset [8]. Rectangles show the strongly connected components (SCCs) grouping different variables. Solid arrows represent causal relationships between SCCs, and dashed arrows represent an arbitrary order picked to learn the joint distribution of each SCC with an ANF. See App. A.2.2 for an in-depth explanation on the proposed method to deal with partial causal graphs using causal NFs.

Figure 15: Pair plot of real (in blue) and generated (in orange) data of German dataset. Above, the samples from the true and learnt observational distributions. Below, the samples from the true and learnt interventional distributions when \(do(\text{x}_{1}=1)\). The plot illustrates that Causal NF is able to handle discrete data and correctly intervene.