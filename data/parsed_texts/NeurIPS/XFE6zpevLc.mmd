Convergence Analysis of ODE Models for Accelerated First-Order Methods via Positive Semidefinite Kernels

Jungbin Kim

Seoul National University

kjb2952@snu.ac.kr

&Insoon Yang

Seoul National University

insoonyang@snu.ac.kr

Corresponding author.

###### Abstract

We propose a novel methodology that systematically analyzes ordinary differential equation (ODE) models for first-order optimization methods by converting the task of proving convergence rates into verifying the positive semidefiniteness of specific Hilbert-Schmidt integral operators. Our approach is based on the performance estimation problems (PEP) introduced by Drori and Teboulle [8]. Unlike previous works on PEP, which rely on finite-dimensional linear algebra, we use tools from functional analysis. Using the proposed method, we establish convergence rates of various accelerated gradient flow models, some of which are new. As an immediate consequence of our framework, we show a correspondence between minimizing function values and minimizing gradient norms.

## 1 Introduction

We consider the following convex optimization problem:

\[\min_{x\in\mathbb{R}^{d}}\ f(x),\] (1)

where \(f:\mathbb{R}^{d}\rightarrow\mathbb{R}\) is a continuously differentiable (\(\mu\)-strongly) convex function. We assume that a minimizer \(x^{*}\) exists. First-order methods, for example, gradient descent and Nesterov's accelerated gradient method, are popular in solving this problem due to their low cost per iteration and dimension-free oracle complexities. These methods can be analyzed by examining their limiting ODEs. For instance, the gradient descent \(x_{k+1}=x_{k}-s\nabla f(x_{k})\) corresponds to the _gradient flow_\(\dot{X}(t)=-\nabla f(X(t))\). Building upon this idea, Su _et al._[36] derived the limiting ODE for Nesterov's accelerated gradient method (AGM) [27] and analyzed its convergence rate, offering valuable insights into momentum-based algorithms. A common approach to establishing convergence rates of continuous-time ODE models involves using Lyapunov functions [23].

In this paper, we propose a generic framework that builds upon the performance estimation problem (PEP) presented in [8] for analyzing the convergence rates of ODE models for various accelerated first-order methods, including Nesterov's AGM. The proposed method is designed from the Lagrangian dual of a relaxed version of continuous-time PEP. Consequently, our framework transforms the task of proving convergence rate into verifying the positive semidefiniteness of a specific integral kernel. Moreover, our framework can also ensure the tightness of the resulting guarantee, meaning that the obtained convergence guarantee is optimal among all possible convergence rates that can be derived from weighted integrals of the inequalities employed in convergence proofs. Using the proposed framework, we confirm the convergence rates of existing ODE models and uncover those of new accelerated ODE models. In traditional convergence analysis of ODE models, it can be challenging to design appropriate Lyapunov functions.2 However, in our framework, we only need to verify the positive semidefiniteness of a specific integral kernel. This approach circumvents the need for Lyapunov function design, making our framework more straightforward for analyzing convergence rates.

In the discrete-time setting, the PEP framework has been extensively studied for its ability to systematically obtain tight convergence guarantees [42] and facilitate the design of new optimization methods [17; 18; 40]. However, analyzing PEP is typically regarded as challenging to comprehend due to the involvement of large matrices with complex expressions. In contrast, our framework utilizes integral kernels, which serve as a continuous-time counterpart to matrices. The computational process within our approach yields simpler outcomes. Consequently, our continuous-time PEP framework has the potential to offer valuable insights into the analysis of the discrete-time PEP, similar to how ODE models have helped designing and analyzing discrete-time methods in the literature [20; 47]. By bridging the gap between the continuous and discrete settings, our methodology enhances the understanding of the PEP framework.

### Related work

Continuous-time models for first-order methods.The investigation into the continuous-time limit of accelerated first-order methods began with the study of AGM ODE [36; 2; 3]. Since then, subsequent researches have explored various aspects of the ODE models. These include generalizations within the mirror descent setup [20], a broader family of dynamics derived using Lagrangian mechanics [47; 48; 19], high-resolution ODE models [34; 33], and continuized methods [9]. Systematic methodologies for finding Lyapunov functions were developed, including deriving them from Hamilton's equations [6] or dilated coordinate systems [37]. For obtaining accelerated discrete-time algorithms, several studies have applied discretization schemes, such as symplectic [4] and Runge-Kutta [49] schemes, to discretize accelerated ODE models. [32] showed that applying multi-step integration schemes to the gradient flow also yields accelerated algorithms. A particularly relevant study is [19], as they present the dynamics in the form of (4) using the _H-kernel_, which plays a crucial role in our analysis.

Performance estimation problems.The idea of using performance estimation problems to analyze the convergence rate of optimization methods was first introduced by [8]. This concept was further refined by employing a convex interpolation argument in [42] and was applied to a wide range of settings in [5; 43; 44; 12; 11; 7; 16]. The idea of performance estimation has been used to construct Lyapunov functions in [41; 39; 25; 45]. In particular, [25] analyzes continuous-time ODE models. However, their methodology differs from ours, as they employed semidefinite programs of finite dimension. Another closely related approach is based on _integral quadratic constraints_ (IQC) from control theory [24], which were used to analyze the convergence rate of first-order methods in [21]. The IQC framework has been further studied in [14; 15; 10; 22; 31]. One practical application of PEP and IQC is the design of novel algorithms by optimizing the convergence guarantees. Some notable examples include OGM [17], TMM [46], ITEM [40], and OGM-G [18].

## 2 Preliminaries and notations

In this section, we review some basic notions from functional analysis that will be used throughout the paper. For a more detailed treatment, we refer the reader to the textbooks [29; 30; 28].

Function spaces.We denote the set of continuous functions from \([0,T]\) to \(\mathbb{R}^{d}\) by \(C([0,T];\mathbb{R}^{d})\) and the set of continuously differentiable functions from \([0,T]\) to \(\mathbb{R}^{d}\) by \(C^{1}([0,T];\mathbb{R}^{d})\). We define the space \(L^{2}([0,T];\mathbb{R}^{d})\) as the set of all measurable functions \(f:[0,T]\to\mathbb{R}^{d}\) that satisfy \(\int_{0}^{T}\|f(x)\|_{\mathbb{R}^{d}}^{2}\,dx<\infty\). Then, \(L^{2}([0,T];\mathbb{R}^{d})\) is a Hilbert space, equipped with an inner product and a norm defined by \(\langle f,g\rangle_{L^{2}([0,T];\mathbb{R}^{d})}=\int_{0}^{T}\langle f(t),g(t) \rangle_{\mathbb{R}^{d}}\,dt\) and \(\|f\|_{L^{2}([0,T];\mathbb{R}^{d})}=\sqrt{\langle f,f\rangle_{L^{2}([0,T]; \mathbb{R}^{d})}}\).

Integral operators.An integral operator is a linear operator that maps a function \(f\) to another function \(Kf\) given by

\[(Kf)(t)=\int_{0}^{T}k(t,\tau)f(\tau)\,d\tau,\] (2)where \(k:[0,T]^{2}\to\mathbb{R}\) is the associated integral kernel. Intuitively, an integral kernel can be seen as a continuous-time version of a matrix. A Hilbert-Schmidt kernel is an integral kernel \(k\) that is square integrable, i.e., \(k\in L^{2}([0,T]^{2};\mathbb{R})\). When \(k\) is a Hilbert-Schmidt kernel, the associated integral operator \(K\) is a well-defined operator on \(L^{2}([0,T];\mathbb{R}^{d})\), called a Hilbert-Schmidt integral operator. If a Hilbert-Schmidt kernel \(k\) is symmetric, i.e., \(k(t,\tau)=k(\tau,t)\) for all \(t,\tau\in[0,T]\), then the associated operator is also symmetric in the sense that \(\langle Kf,g\rangle=\langle f,Kg\rangle\) for all \(f,g\in L^{2}([0,T];\mathbb{R}^{d})\). Throughout this paper, we will use the term 'kernel' to refer to a Hilbert-Schmidt kernel.

Positive semidefinite kernels.A symmetric operator \(K\) on a Hilbert space is said to be positive semidefinite and denoted by \(K\succeq 0\) if \(\langle Kf,f\rangle\geq 0\) for all \(f\). When a symmetric kernel \(k\) is associated with a positive semidefinite operator \(K\), i.e., \(\int_{0}^{T}\int_{0}^{T}k(t,\tau)f(t)f(\tau)\,dtd\tau\geq 0\) for all \(f\in L^{2}([0,T];\mathbb{R})\), we say that the kernel \(k\) is (integrally) positive semidefinite and denote it by \(k\succeq 0\). For continuous kernels, positive semidentiveness of \(k\) is equivalent to the following condition: \(\sum_{i=1}^{n}\sum_{j=1}^{n}c_{i}c_{j}k(t_{i},t_{j})\geq 0\) for any \(t_{1},\ldots,t_{n}\in[0,T]\) and \(c_{1},\ldots,c_{n}\in\mathbb{R}\), given \(n\in\mathbb{N}\).

**Proposition 1**.: _We summarize some basic properties of continuous positive semidefinite kernels:_

1. _For_ \(\alpha\in C([0,T];\mathbb{R})\)_, the kernel_ \(k(t,\tau)=\alpha(t)\alpha(\tau)\) _is positive semidefinite._
2. _For_ \(k_{1},k_{2}\succeq 0\)_, their product_ \(k(t,\tau)=k_{1}(t,\tau)k_{2}(t,\tau)\) _is positive semidefinite._
3. _For_ \(k\succeq 0\)_, its anti-transpose_ \((t,\tau)\mapsto k(T-\tau,T-t)\) _is also positive semidefinite._
4. _If_ \(\alpha\in C^{1}([0,T];\mathbb{R}_{\geq 0})\) _is an increasing function on_ \([0,T]\)_, then the symmetric kernel_ \(k\) _defined as_ \(k(t,\tau)=\alpha(\tau)\) _for_ \(t\geq\tau\) _and_ \(k(t,\tau)=\alpha(t)\) _for_ \(t\leq\tau\) _is positive semidefinite._3__ Footnote 3: Proof sketch: The kernel \(k(t,\tau)\) can be expressed as a weighted integral of positive semidefinite kernels as \(k(t,\tau)=\alpha(0)\mathbf{1}_{[0,T]}(t)\mathbf{1}_{[0,T]}(\tau)+\int_{0}^{T} \dot{\alpha}(s)\mathbf{1}_{[s,T]}(t)\mathbf{1}_{[s,T]}(\tau)\,ds\).
5. _For_ \(k\succeq 0\)_, we have_ \(k(t,t)\geq 0\) _for all_ \(t\in[0,T]\)_._

## 3 Continuous PEP for minimizing objective function value

In this section, drawing inspiration from its discrete-time counterpart [8, 42], we propose a novel framework for analyzing the convergence rate of ODE models for first-order methods, called the _continuous-time performance estimation problem (Continuous PEP)_. To illustrate this framework, we use the accelerated gradient flow as an example. Detailed steps can be found in Appendix C. Su _et al._[36] derived the limiting ODE of Nesterov's AGM [27] as follows:

\[\ddot{X}+\frac{3}{t}\dot{X}+\nabla f(X)=0,\] (AGM ODE)

with initial conditions \(X(0)=x_{0}\) and \(\dot{X}(0)=0\). Suppose we want to establish a convergence guarantee of AGM ODE in the form of

\[f(X(T))-f(x^{*})\leq\rho\|x_{0}-x^{*}\|^{2}.\] (3)

Here, we observe that the constant \(\rho\) can be seen as an upper bound of the performance of AGM ODE for the criterion \((f(X(T))-f(x^{*}))/\|x_{0}-x^{*}\|^{2}\). To formalize this idea, we introduce the following optimization problem, which seeks to find the worst-case performance of the given ODE model:

\[\max_{\begin{subarray}{c}f\in\mathcal{F}_{0}(\mathbb{R}^{d}; \mathbb{R})\\ X\in C^{1}([0,T];\mathbb{R}^{d})\end{subarray}} \frac{f(X(T))-f(x^{*})}{\|x_{0}-x^{*}\|^{2}}\] (Exact PEP) subject to \[X\] is a solution to AGM ODE with \[X(0)=x_{0},\ \dot{X}(0)=0\] \[x^{*}\] is a minimizer of \[f,\]

where \(\mathcal{F}_{\mu}(\mathbb{R}^{d};\mathbb{R})\) denotes the set of continuously differentiable \(\mu\)-strongly convex functions on \(\mathbb{R}^{d}\). This problem is useful to analyze the convergence properties of ODE models because the optimal value \(\operatorname{val}(\text{Exact PEP})\) of Exact PEP directly provides the guarantee (3) with \(\rho=\operatorname{val}(\text{Exact PEP})\) regardless of any particular choice of \(f\).

### Relaxation of PEP

Exact PEP is challenging to solve due to the presence of an unknown function \(f\) as an optimization variable. To address this difficulty, we relax the constraint \(f\in\mathcal{F}_{0}(\mathbb{R}^{d};\mathbb{R})\) with a set of inequalities that are satisfied by \(f\in\mathcal{F}_{0}(\mathbb{R}^{d};\mathbb{R})\). Before that, we first note that AGM ODE can be expressed as the following continuous-time dynamical system (see [19]):

\[\dot{X}(t)=-\int_{0}^{t}H(t,\tau)\nabla f(X(\tau))\,d\tau\] (4)

by setting \(H(t,\tau)=\tau^{3}/t^{3}\). Here, \(H(t,\tau)\) is called the _H-kernel_. We introduce two functions, \(\varphi:[0,T]\to\mathbb{R}\) and \(\gamma:[0,T]\to\mathbb{R}^{d}\), defined as follows:

\[\varphi(t)=\frac{1}{\left\|x_{0}-x^{*}\right\|^{2}}\left(f(X(t))-f\left(x^{*} \right)\right),\quad\gamma(t)=\frac{1}{\left\|x_{0}-x^{*}\right\|}\nabla f(X (t)).\]

Using the chain rule and the convexity of \(f\), we can derive the following equality and inequality:

\[\begin{split} 0&=\dot{\varphi}(t)+\left\langle\gamma(t), \int_{0}^{t}H(t,\tau)\gamma(\tau)\,d\tau\right\rangle,\\ 0&\geq\varphi(t)+\left\langle\gamma(t),v+\int_{0}^{ t}\int_{\tau}^{t}H(s,\tau)\gamma(\tau)\,ds\,d\tau\right\rangle,\end{split}\] (5)

where \(v=(x^{*}-x_{0})/\|x_{0}-x^{*}\|\). We can now relax Exact PEP by replacing its constraints with the equality and inequality above, resulting in the following problem:

\[\begin{split}\max_{\varphi,\gamma,v}&\quad\varphi (T)\\ \text{subject to}&\quad\eqref{eq:relaxed PEP}\end{split}\] (Relaxed PEP)

Since any feasible solution to Exact PEP can be transformed into a feasible solution to Relaxed PEP, we have \(\operatorname{val}(\text{Relaxed PEP})\geq\operatorname{val}(\text{Exact PEP})\). Therefore, the convergence guarantee (3) holds with \(\rho=\operatorname{val}(\text{Relaxed PEP})\) when using the proposed relaxation.

### Lagrangian dual of relaxed PEP

To obtain an upper bound on \(\operatorname{val}(\text{Relaxed PEP})\), we use Lagrangian duality. We introduce two _Lagrange multipliers_\(\lambda_{1}\in C^{1}([0,T];\mathbb{R})\) and \(\lambda_{2}\in C([0,T];\mathbb{R}_{\geq 0})\), where we imposed certain regularity conditions, such as continuity and differentiability, to ensure that the dual problem is well-defined. We then define the Lagrangian function as

\[\begin{split}\mathcal{L}(\varphi,\gamma,v;\lambda_{1},\lambda_{2 })&=\varphi(T)-\int_{0}^{T}\lambda_{1}(t)\left(\dot{\varphi}(t)+ \left\langle\gamma(t),\int_{0}^{t}H(t,\tau)\gamma(\tau)\,d\tau\right\rangle \right)\,dt\\ &\quad-\int_{0}^{T}\lambda_{2}(t)\left(\varphi(t)+\left\langle \gamma(t),v+\int_{0}^{t}\int_{\tau}^{t}H(s,\tau)\gamma(\tau)\,ds\,d\tau \right\rangle\right)\,dt.\end{split}\]

When expressed in terms of the inner products in function spaces, we have

\[\begin{split}\mathcal{L}(\varphi,\gamma,v;\lambda_{1},\lambda_{2 })&=\varphi(T)-\left\langle\lambda_{1},\dot{\varphi}\right\rangle_ {L^{2}([0,T];\mathbb{R})}-\left\langle\lambda_{2},\varphi\right\rangle_{L^{2}( [0,T];\mathbb{R})}\\ &\quad-\frac{1}{2}\left\langle K\gamma,\gamma\right\rangle_{L^{2} ([0,T];\mathbb{R}^{d})}-\left\langle\lambda_{2}(t)v,\gamma(t)\right\rangle_{L^ {2}([0,T];\mathbb{R}^{d})},\end{split}\] (6)

where \(K\) is the Hilbert-Schmidt integral operator with the symmetric kernel \(k\) defined by

\[k(t,\tau)=\lambda_{1}(t)H(t,\tau)+\lambda_{2}(t)\int_{\tau}^{t}H(s,\tau)\,ds,\quad t\geq\tau.\]

The dual function is defined as \(\operatorname{Dual}(\lambda_{1},\lambda_{2})=\sup_{\varphi,\gamma,v}\mathcal{ L}(\varphi,\gamma,v;\lambda_{1},\lambda_{2})\). By weak duality, we have \(\operatorname{val}(\text{Relaxed PEP})\leq\operatorname{Dual}(\lambda_{1}, \lambda_{2})\) for any feasible dual solution \((\lambda_{1},\lambda_{2})\). After performing some computations, we obtain the following expression for the dual objective function (see Appendix C):

\[\begin{split}\operatorname{Dual}(\lambda_{1},\lambda_{2})=\begin{cases} \inf_{\nu\in(0,\infty)}\left\{\nu:S_{\lambda_{1},\lambda_{2},\nu}\succeq 0\right\}&\text{if }\lambda_{1}(0)=0,\;\lambda_{1}(T)=1\;\dot{ \lambda}_{1}(t)=\lambda_{2}(t)\\ \infty&\text{otherwise},\end{cases}\end{split}\] (7)where \(S_{\lambda_{1},\lambda_{2},\nu}\) is a symmetric kernel on \([0,T]^{2}\) given by

\[S_{\lambda_{1},\lambda_{2},\nu}(t,\tau)=\nu\left(\lambda_{1}(t)H(t,\tau)+\lambda_ {2}(t)\int_{\tau}^{t}H(s,\tau)\,ds\right)-\frac{1}{2}\lambda_{2}(t)\lambda_{2} (\tau),\quad t\geq\tau.\] (8)

We refer to \(S_{\lambda_{1},\lambda_{2},\nu}\) as the _PEP kernel_. In Appendix H.2, we show that (8) can be viewed as the continuous-time limit of the discrete-time PEP kernel presented in [8].

To describe our framework, given \(\nu_{\text{feas}}\in(0,\infty)\), suppose that the PEP kernel \(S_{\lambda_{1},\lambda_{2},\nu_{\text{feas}}}\) is positive semidefinite with appropriate multiplier functions \(\lambda_{1}\) and \(\lambda_{2}\). Then, \(\nu_{\text{feas}}\) is a feasible solution of the minimization problem in (7), and thus \(\operatorname{Dual}(\lambda_{1},\lambda_{2})\leq\nu_{\text{feas}}\). On the other hand, by weak duality, \(\operatorname{val}(\text{Relaxed PEP})\leq\inf\operatorname{Dual}( \lambda_{1},\lambda_{2})\). Therefore, we conclude that \(\operatorname{val}(\text{Exact PEP})\leq\operatorname{val}(\text{Relaxed PEP})\leq \operatorname{Dual}(\lambda_{1},\lambda_{2})\leq\nu_{\text{feas}}\), which implies that the convergence guarantee (3) automatically holds with \(\rho=\nu_{\text{feas}}\).

Using this approach, we can recover the known convergence guarantee for AGM ODE in [36].

**Proposition 2**.: _AGM ODE achieves the convergence rate (3) with \(\rho=2/T^{2}\)._

Proof.: By choosing the multiplier functions \(\lambda_{1}(t)=t^{2}/T^{2}\) and \(\lambda_{2}(t)=2t/T^{2}\), we can compute the PEP kernel (8) as \(S_{\lambda_{1},\lambda_{2},\nu}(t,\tau)=(\nu-\frac{2}{T^{2}})\frac{t\tau}{T^{2}}\). Since the kernel \((t,\tau)\mapsto t\tau\) is nonzero and positive semidefinite, we have \(S_{\lambda_{1},\lambda_{2},\nu}\succeq 0\) if and only if \(\nu\geq 2/T^{2}\). Thus, we obtain \(\operatorname{Dual}(\lambda_{1},\lambda_{2})=2/T^{2}\), which establishes the convergence guarantee (3) with \(\rho=2/T^{2}\). 

**Remark 1**.: _Furthermore, this convergence guarantee is optimal among all possible guarantees obtained through the weighted integral of (5). The optimality of this rate follows from the fact that \((\lambda_{1},\lambda_{2})\) is the optimal solution to the dual problem \(\min_{\lambda_{1},\lambda_{2}}\operatorname{Dual}(\lambda_{1},\lambda_{2})\). See Appendix H.1 for details._

### Applying continuous PEP to various accelerated gradient flows

Note that the proposed method is not dependent on the choice of the H-kernel \(H(t,\tau)\). Thus, it can be applied to arbitrary dynamics represented in the form of (4). Furthermore, while we have focused on the non-strongly convex case (\(\mu=0\)) so far, the following paragraph demonstrates that our method can handle strongly convex objective functions by using a reparametrization technique.4

Footnote 4: In discrete PEP literature, a similar reparametrization technique was employed in [40, 13].

Reparametrization from \(\mathcal{F}_{\mu}(\mathbb{R}^{d};\mathbb{R})\) to \(\mathcal{F}_{0}(\mathbb{R}^{d};\mathbb{R})\).Consider a \(\mu\)-strongly convex objective function \(f\). Since the proposed method is tailored for non-strongly convex objective functions, we choose to work with the convex function \(\hat{f}(x):=f(x)-\frac{\mu}{2}\|x-x_{0}\|^{2}\) rather than working directly with \(f\). Accordingly, we consider the following alternative formulation for the dynamical system (4), which involves \(\nabla\hat{f}\) instead of \(\nabla f\):5

Footnote 5: The equivalent representations (4) and (9) are in a one-to-one correspondence. See Appendix C.1 for details.

\[\dot{X}(t)=-\int_{0}^{t}H^{F}(t,\tau)\nabla\hat{f}(X(\tau))\,d\tau.\] (9)

The following theorem offers a general result that can be used to establish convergence guarantees for dynamical systems of the form (9).

**Theorem 1**.: _Let \(\nu>0\) and \(\lambda^{F}\in C^{1}([0,T];\mathbb{R}_{\geq 0})\) such that \(0\leq\lambda^{F}(0)<1\), \(\lambda^{F}(T)=1\), and \(\dot{\lambda}^{F}(t)\geq 0\) for all \(t\in(0,T)\). Then, any solution to the integro-differential equation (9) satisfies_

\[\tilde{f}(X(T))-\tilde{f}(x^{*})\leq\lambda^{F}(0)\left(\tilde{f}(x_{0})-\tilde {f}(x^{*})\right)+\nu\left\|x_{0}-x^{*}\right\|^{2},\]

_where \(\tilde{f}(x):=f(x)-\frac{\mu}{2}\|x-x^{*}\|^{2}\), if the following PEP kernel is positive semidefinite:_

\[S^{F}(t,\tau)=\nu\left(\lambda^{F}(t)H^{F}(t,\tau)+\dot{\lambda}^{F}(t)\int_{ \tau}^{t}H^{F}(s,\tau)\,ds\right)-2\alpha^{F}(t)\alpha^{F}(\tau),\quad t\geq\tau,\] (10)

_where \(\alpha^{F}(t)=\frac{1}{2}\frac{d}{dt}\{\lambda^{F}(t)(1-\mu\int_{0}^{t}\int_{ 0}^{s}H^{F}(s,\tau)\,d\tau ds)\}\)._

The proof of Theorem 1 is done by finding a dual feasible point to the PEP and can be found in Appendix C. Below, we establish convergence rates for various ODE models using Theorem 1.

AGM-SC ODE.We consider the following dynamical system modeling Nesterov's AGM for strongly convex case [26, Equation 2.2.22] (see [48, Equation 7]):

\[\ddot{X}+2\sqrt{\mu}\dot{X}+\nabla f(X)=0.\] (AGM-SC ODE)

This ODE model can be written as (9) with \(H^{F}(t,\tau)=(1+\sqrt{\mu}\tau-\sqrt{\mu}t)e^{\sqrt{\mu}(\tau-t)}\) (see Appendix F.1). To use Theorem 1, we choose the multiplier function as \(\lambda^{F}(t)=e^{\sqrt{\mu}(t-T)}\).6 The PEP kernel (10) can be computed as (see Appendix G.1)

Footnote 6: As a rule of thumb, when the expected convergence rate is \(O(\rho(T))\), we set \(\lambda^{F}(t)=\rho(T)/\rho(t)\).

\[S^{F}(t,\tau)=\nu e^{\sqrt{\mu}(\tau-T)}-\frac{\mu}{2}e^{-2\sqrt{\mu}T},\quad t \geq\tau.\] (11)

When \(\nu=\frac{\mu}{2}e^{-\sqrt{\mu}T}\), this kernel is written as \(S^{F}(t,\tau)=\frac{\mu}{2}e^{-2\sqrt{\mu}T}(e^{\sqrt{\mu}\tau}-1)\), and is visualized in Figure 1. It is positive semidefinite since the function \(\tau\mapsto e^{\sqrt{\mu}\tau}-1\) is a nonnegative increasing function (see Proposition 1 (d)). It follows from Theorem 1 that AGM-SC ODE achieves the following convergence guarantee:

\[\tilde{f}(X(T))-\tilde{f}(x^{*})\leq e^{-\sqrt{\mu}T}\left(\tilde{f}(x_{0})- \tilde{f}(x^{*})+\frac{\mu}{2}\left\|x_{0}-x^{*}\right\|^{2}\right),\] (12)

which is consistent with the well-known \(O(e^{-\sqrt{\mu}T})\) convergence rate of AGM-SC ODE.

Unified AGM ODE.Using a unified Bregman Lagrangian framework, [19] obtained the following ODE that unifies AGM ODE and AGM-SC ODE:7

Footnote 7: This dynamical system models the unified AGM in [19] and the _constant step scheme I_[26, Equation 2.2.19].

\[\ddot{X}+\frac{\sqrt{\mu}}{2}\left(\tanh_{t}+3\coth_{t}\right)\dot{X}+\nabla f (X)=0,\] (Unified AGM ODE)

where \(\tanh_{t}\) and \(\coth_{t}\) denote the corresponding hyperbolic functions with the argument \(\frac{\sqrt{\mu}}{2}t\). This ODE model can be written as (9) with \(H^{F}(t,\tau)=(1+\coth_{t}^{2}(\log(\operatorname{sech}_{t}^{2})-\log( \operatorname{sech}_{\tau}^{2})))\sinh_{t}\cosh_{t}\) (see Appendix F.2). We select the multiplier function as \(\lambda^{F}(t)=\sinh_{t}^{2}/\sinh_{T}^{2}\). With this choice, the PEP kernel (10) can be expressed as follows (see Appendix G.2):

\[S^{F}(t,\tau)=\left(\nu-\frac{\mu}{2}\operatorname{csch}_{T}^{2}\right)\frac {\tanh_{t}\tanh_{\tau}}{\sinh_{T}^{2}}+\nu\frac{\tanh_{t}\tanh_{\tau}\sinh_{ \tau}^{2}}{\sinh_{T}^{2}},\quad t\geq\tau.\] (13)

We show that this kernel is positive semidefinite for \(\nu=\frac{\mu}{2}\operatorname{csch}_{T}^{2}\). Proposition 1 (a) shows that the kernel \((t,\tau)\mapsto\tanh_{t}\tanh_{\tau}\) is positive semidefinite. Proposition 1 (d) shows that the kernel \((t,\tau)\mapsto\sinh_{\tau}^{2}\) is positive semidefinite because the function \(\tau\mapsto\sinh_{\tau}^{2}\) is a nonnegative increasing function. Since the PEP kernel (13) with \(\nu=\frac{\mu}{2}\operatorname{csch}_{T}^{2}\) can be expressed as a product of two positive semidefinite kernels, it is positive semidefinite by Proposition 1 (b). Consequently, Theorem 1 implies that Unified AGM ODE achieves the following convergence guarantee:

\[\tilde{f}(X(T))-\tilde{f}(x^{*})\leq\frac{\mu}{2}\operatorname{csch}_{T}^{2} \left\|x_{0}-x^{*}\right\|^{2}.\] (14)

This guarantee aligns with the \(O(\operatorname{csch}_{T}^{2})\) convergence rate reported in [19].

Figure 1: Visualization of the PEP kernel (11) for AGM-SC ODE.

TMM ODE.We consider the following novel limiting ODE for the _triple momentum method_ (TMM) [46] (see Appendix E.1 for the derivation and a comparison with the one in [38]):

\[\ddot{X}+3\sqrt{\mu}\dot{X}+2\nabla f(X)=0.\] (TMM ODE)

This ODE model can be written as (9) with \(H^{F}(t,\tau)=-2e^{\sqrt{\mu}(\tau-t)}+4e^{2\sqrt{\mu}(\tau-t)}\) (see Appendix F.3). By setting the multiplier function as \(\lambda^{F}(t)=e^{2\sqrt{\mu}(t-T)}\), the PEP kernel (10) can be computed as (see Appendix G.3)

\[S^{F}(t,\tau)=2\left(\nu-\mu e^{-2\sqrt{\mu}T}\right)e^{\sqrt{\mu}(t+\tau-2T)},\] (15)

which is positive semidefinite for \(\nu=\mu e^{-2\sqrt{\mu}T}\). Consequently, Theorem 1 implies that TMM ODE achieves the following convergence guarantee:

\[\tilde{f}(X(T))-\tilde{f}(x^{*})\leq e^{-2\sqrt{\mu}T}\left(\tilde{f}(x_{0})- \tilde{f}(x^{*})+\mu\left\|x_{0}-x^{*}\right\|^{2}\right),\] (16)

which is new to the literature. In Appendix H.3, we show that this convergence rate match aligns with the known convergence guarantee for TMM in the discrete-time case.

ITEM ODE.We consider the following new limiting ODE of the _information-theoretic exact method_ (ITEM) [40] (see Appendix E.2 for the derivation of ITEM ODE):

\[\ddot{X}+3\sqrt{\mu}\coth_{t}\dot{X}+2\nabla f(X)=0,\] (ITEM ODE)

where \(\coth_{t}\) denotes the corresponding hyperbolic function with the argument \(\sqrt{\mu}t\). This ODE model can be written as (9) with \(H^{F}(t,\tau)=4\sinh_{\tau}\cosh_{\tau}\coth_{t}\csch_{t}^{2}+2\sinh_{\tau} \csch_{t}(1-2\coth_{t}^{2})\) (see Appendix F.4). By choosing the multiplier function as \(\lambda^{F}(t)=\sinh^{2}(\sqrt{\mu}t)/\sinh^{2}(\sqrt{\mu}T)\), the PEP kernel (10) can be computed as (see Appendix G.4)

\[S^{F}(t,\tau)=2\csch_{T}^{2}(\nu-\mu\csch_{T}^{2})\sinh_{t}\sinh_{\tau}.\] (17)

For \(\nu=\mu\csc_{T}^{2}\), this kernel is positive semidefinite. It follows from Theorem 1 that ITEM ODE achieves the following convergence guarantee:

\[\tilde{f}(X(T))-\tilde{f}(x^{*})\leq\mu\csc_{T}^{2}\left\|x_{0}-x^{*}\right\| ^{2},\] (18)

which is a novel result. In Appendix H.4, we show that this guarantee matches the known convergence rate for ITEM in the discrete-time case.

## 4 Continuous PEP for minimizing velocity and gradient norm

In this section, we present a result analogous to Theorem 1 to address convergence rates on the squared velocity norm \(\|\dot{X}(T)\|^{2}\) or the squared gradient norm \(\|\nabla f(X(T))\|^{2}\). For continuous-time ODE models, the analysis of convergence rates on the squared gradient norm \(\|\nabla f(X(T))\|^{2}\) was first presented in [37]. However, their argument relies on the use of L'Hopital's rule, which might give the impression that their approach is based on a clever trick or appears somewhat mysterious.

Translating convergence rates on \(\|\dot{X}(t)\|^{2}\) into convergence rates on \(\|\nabla f(X(T))\|^{2}\)

In this subsection, we present a novel approach for establishing the convergence guarantee of ODE models on the squared gradient norm \(\|\nabla f(X(T))\|^{2}\). The crucial insight lies in expressing \(\nabla f(X(T))\) as \(\int_{0}^{T}\nabla f(X(\tau))\delta_{T}(\tau)\,d\tau\), where \(\delta_{T}\) denotes the Dirac delta function centered at \(\tau=T\). Suppose we have a guarantee of the following form:

\[\left\|\int_{0}^{T}\alpha_{t}(\tau)\nabla f(X(\tau))\,d\tau\right\|^{2}\leq \rho\left(f(x_{0})-f(x^{*})\right),\] (19)

where \(X\in C^{1}([0,T];\mathbb{R}^{d})\) and \(\{\alpha_{t}\}\) is a family of functions parametrized by \(t\in(0,T)\). In particular, we note that a convergence guarantee on \(\|C(t)\dot{X}(t)\|^{2}\) of the dynamics (4) can be written as (19) with \(\alpha_{t}(\tau)=C(t)H(t,\tau)\). A well-known argument for constructing the Dirac delta function (see[35, Section 3.2]) shows that the weighted integral \(\int_{0}^{T}\alpha_{t}(\tau)\nabla f(X(\tau))\,d\tau\) converges to \(\nabla f(X(T))\) as \(t\to T\), if the following conditions hold: \((i)\ \alpha_{t}(\tau)\geq 0\), \((ii)\int_{0}^{T}\alpha_{t}(\tau)\,d\tau\to 1\) as \(t\to T\), and \((iii)\) for every \(\eta\in(0,T)\), we have \(\int_{0}^{\eta}\alpha_{t}(\tau)\,d\tau\to 0\) as \(t\to T\). When \(\alpha_{t}\) satisfies these properties, we say that the function \(\alpha_{t}\) converges to the Dirac delta function \(\delta_{T}\). Consequently, taking the limit \(t\to T\) in (19) yields the following guarantee on \(\|\nabla f(X(T))\|^{2}\):

\[\|\nabla f(X(T))\|^{2}\leq\rho\left(f(x_{0})-f(x^{*})\right).\]

### Convergence analysis via positive semidefinite kernels

In this subsection, we introduce a variant of continuous PEP that establishes the convergence rate on \(\|\dot{X}(T)\|^{2}\) through checking the positive semidefiniteness of the PEP kernel. Notably, this methodology can also prove convergence rates on \(\|\nabla f(X(T))\|^{2}\) because the convergence rates on \(\|\dot{X}(t)\|^{2}\) can be translated into those on \(\|\nabla f(X(T))\|^{2}\), as discussed in the previous subsection.

Reparametrization to time-varying functions.In Section 3.3, we employed a reparametrization technique to deal with strongly convex objective functions. In this section, we first apply the same technique again, leading to the following expression:8

Footnote 8: This expression is identical in form to (9), but we use the notation \(H^{G}\) to avoid any notational overlap.

\[\dot{X}(t)=-\int_{0}^{t}H^{G}(t,\tau)\nabla\hat{f}(X(\tau))\,d\tau,\] (20)

where \(\hat{f}(x):=f(x)-\frac{\mu}{2}\|x-x_{0}\|^{2}\). However, we do not proceed directly with this form. Instead, we introduce an additional reparametrization step. Given a solution \(X\) to (20), and a function \(\lambda^{G}\in C^{1}([0,T);\mathbb{R}_{\geq 0})\), we define a family of functions \(\{\hat{f}_{t}\}_{t\in[0,T)}\) as \(\hat{f}_{t}(x):=\lambda^{G}(t)\hat{f}(x)-\langle\int_{0}^{t}\dot{\lambda}^{G }(\tau)\nabla\hat{f}(X(\tau))\,d\tau,x\rangle\). Then, we can show that (20) can be equivalently written in the following form (see Appendix D.1):

\[\dot{X}(t)=-\int_{0}^{t}\bar{H}^{G}(t,\tau)\nabla\hat{f}_{\tau}(X(\tau))\,d\tau,\] (21)

for some kernel \(\bar{H}^{G}\). The following theorem is analogous to Theorem 1 for our current purpose.

**Theorem 2**.: _Let \(\nu>0\), \(t_{\rm end}\in(0,T]\), \(\alpha^{G}\in C([0,t_{\rm end}],\mathbb{R})\), and \(\lambda^{G}\in C^{1}([0,t_{\rm end}];\mathbb{R}_{\geq 0})\) such that \(\lambda^{G}(0)=1\) and \(\dot{\lambda}^{G}(t)\geq 0\) for all \(t\). Then, any solution to (21) satisfies_

\[\left\|\int_{0}^{t_{\rm end}}\alpha^{G}(\tau)\nabla\hat{f}_{\tau}(X(\tau))\, d\tau\right\|^{2}\leq\nu\sup_{x\in\mathbb{R}^{d}}\left\{\hat{f}(x_{0})-\hat{f}( x)\right\},\] (22)

_if the following PEP kernel defined on \([0,t_{\rm end}]^{2}\) is positive semidefinite:_

\[S^{G}(t,\tau)=\nu\bar{H}^{G}(t,\tau)-2\alpha^{G}(t)\alpha^{G}(\tau),\quad t \geq\tau.\] (23)

_In particular, the choice \(\alpha^{G}(t)=C(t_{\rm end})\bar{H}^{G}(t_{\rm end},t)\) gives a guarantee on \(\|C(t_{\rm end})\dot{X}(t_{\rm end})\|^{2}\)._

The proof of Theorem 2 can be found in Appendix D. We now use this theorem to establish convergence rates of the anti-transposed dynamics9 of the ODE models studied in Section 3.3.

Footnote 9: We refer to (20) as the _anti-transposed dynamics_ of (9), if \(H^{G}(t,\tau)=H^{F}(T-\tau,T-t)\) for all \(t,\tau\).

Ogm-G ODE.By taking the limit of the stepsize in OGM-G [18], Suh _et al._[37] obtained the following ODE model for the non-strongly convex case (\(\mu=0\)):10

Footnote 10: We modified the coefficient of \(\nabla f(X(t))\) from \(2\) to \(1\).

\[\ddot{X}+\frac{3}{T-t}\dot{X}+\nabla f(X)=0.\] (OGM-G ODE)

This ODE model is the anti-transposed dynamics of AGM ODE, as it can be expressed as (20) with \(H^{G}(t,\tau)=(T-t)^{3}/(T-\tau)^{3}\) (see Appendix F.5). To use Theorem 2, we choose \(\lambda^{G}(t)=T^{2}/(T-\tau)^{3}/(T-\tau)^{3}\)\(t\))2.11 We set the terminal time \(t_{\rm end}\) before \(T\) and apply a limiting argument to prove the convergence rate on \(\|\nabla f(X(T))\|^{2}\). By setting \(\alpha^{G}(t)=C(t_{\rm end})\bar{H}^{G}(t_{\rm end},t)\) with \(C(t_{\rm end})=1/(T-t_{\rm end})\), we can compute the PEP kernel (23) as (see Appendix G.5)

Footnote 11: As a rule of thumb, when the expected convergence rate is \(O(\rho(T))\), we set \(\lambda^{G}(t)=\rho(T-t)/\rho(T)\).

\[S^{G}(t,\tau)=\left(\nu-\frac{2}{T^{2}}\right)\frac{(T-t)(T-\tau)}{T^{2}}.\] (24)

Since this kernel is the anti-transpose of the PEP kernel for AGM ODE in the proof of Proposition 2, we have \(S^{G}(t,\tau)\succeq 0\) when \(\nu=2/T^{2}\) by Proposition 1 (c). By Theorem 2, we obtain the following inequality:

\[\left\|\int_{0}^{t_{\rm end}}\frac{(T-t_{\rm end})^{2}}{(T-\tau)^{3}}\nabla f (X(\tau))\,d\tau\right\|^{2}=\left\|\frac{\dot{X}(t_{\rm end})}{T-t_{\rm end} }\right\|^{2}\leq\frac{2}{T^{2}}\sup_{x\in\mathbb{R}^{d}}\{f(x_{0})-f(x)\}.\] (25)

Observing that the function \(\tau\mapsto\frac{(T-t_{\rm end})^{2}}{(T-\tau)^{3}}\mathbf{1}_{[0,t_{\rm end }]}(\tau)\) converges to \(\frac{1}{2}\delta_{T}\) as \(t_{\rm end}\to T\), we have \(\int_{0}^{t_{\rm end}}\frac{(T-t_{\rm end})^{2}}{(T-\tau)^{3}}\nabla f(X(\tau ))\to\frac{1}{2}\nabla f(X(T))\) as \(t_{\rm end}\to T\).12 Substituting this result into (25), we deduce the following convergence guarantee:

Footnote 12: In our argument using the Dirac delta function, we rely on the fact that the solution \(X\) to OGM-G ODE can be continuously extended to \(t=T\), which was shown in [37, Appendix D.3].

\[\left\|\nabla f(X(T))\right\|^{2}\leq\frac{8}{T^{2}}\sup_{x\in\mathbb{R}^{d}} \{f(x_{0})-f(x)\},\] (26)

which recovers the known convergence rate of OGM-G ODE in [37].

AGM-SC ODE.We now analyze the convergence of AGM-SC ODE in terms of the squared velocity norm, using Theorem 2. This ODE model is the anti-transposed dynamics of itself, as it can be expressed as (20) with \(H^{G}(t,\tau)=(1+\sqrt{\mu}\tau-\sqrt{\mu}t)e^{\sqrt{\mu}(\tau-t)}\). We choose \(\lambda^{G}(t)=e^{\sqrt{\mu}t}\) and \(t_{\rm end}=T\). By setting \(\alpha^{G}(t)=C(T)\bar{H}^{G}(T,t)\) with \(C(T)=\sqrt{\mu}/2\), the PEP kernel (23) is expressed as (see Appendix G.6)

\[S^{G}(t,\tau)=\nu e^{-\sqrt{\mu}t}-\frac{\mu}{2}e^{-2\sqrt{\mu}T},\quad t\geq\tau.\] (27)

Since this kernel is the anti-transpose of (11), it is positive semidefinite when \(\nu=\frac{\mu}{2}e^{-\sqrt{\mu}T}\) by Proposition 1 (c). Therefore, we conclude that AGM-SC ODE achieves the following convergence guarantee:

\[\left\|\frac{\sqrt{\mu}}{2}\dot{X}(T)\right\|^{2}\leq\frac{\mu}{2}e^{-\sqrt{ \mu}T}\sup_{x\in\mathbb{R}^{d}}\left\{\hat{f}(x_{0})-\hat{f}(x)\right\},\] (28)

which is new to the literature. A numerical experiment for this guarantee can be found in Appendix I.1.

Unified AGM-G ODE.In [19], the following unified AGM-G ODE is proposed:

\[\ddot{X}+\frac{\sqrt{\mu}}{2}\left(\tanh_{T-t}+3\coth_{T-t}\right)\dot{X}+ \nabla f(X)=0,\qquad\qquad\text{(Unified AGM-G ODE)}\]

where \(\tanh_{T-t}\) and \(\coth_{T-t}\) denote the corresponding hyperbolic functions with the argument \(\frac{\sqrt{\mu}}{2}(T-t)\). This ODE model is the anti-transposed dynamics of Unified AGM ODE, as it can be expressed as (20) with \(H^{G}(t,\tau)=(1+\coth_{T-\tau}^{2}(\log(\operatorname{sech}_{T-\tau}^{2})- \log(\operatorname{sech}_{T-t}^{2})))\frac{\sinh_{T-t}\cosh_{T-t}}{\sinh_{T- \tau}\cosh_{T-\tau}}\) (see Appendix F.6). To identify its convergence rate in terms of the squared gradient norm, we choose \(\lambda^{G}(t)=\operatorname{csch}_{T-t}^{2}/\operatorname{csch}_{T}^{2}\). By setting \(\alpha^{G}(t)=C(t_{\rm end})\bar{H}^{G}(t_{\rm end},t)\) with \(C(t_{\rm end})=\frac{\sqrt{\mu}}{2}\operatorname{sech}_{T-t_{\rm end}} \operatorname{csch}_{T-t_{\rm end}}\), the PEP kernel (23) is expressed as (see Appendix G.7)

\[S^{G}(t,\tau)=\left(\nu-\frac{\mu}{2}\operatorname{csch}_{T}^{2}\right)\frac{ \tanh_{T-t}\tanh_{T-\tau}}{\sinh_{T}^{2}}+\nu\frac{\tanh_{T-t}\tanh_{T-\tau} \sinh_{T-t}^{2}}{\sinh_{T}^{2}},\quad t\geq\tau.\] (29)

Since this kernel is the anti-transpose of (13), it is positive semidefinite when \(\nu=\frac{\mu}{2}\operatorname{csch}_{T}^{2}\) by Proposition 1 (c). Therefore, Unified AGM-G ODE achieves the following convergence guarantee:

[MISSING_PAGE_FAIL:10]

## References

* [1]F. Alimisis, A. Orvieto, G. Becigneul, and A. Lucchi (2020) A continuous-time perspective for modeling acceleration in Riemannian optimization. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, pp. 1297-1307. Cited by: SS1.
* [2]H. Attouch, Z. Chbani, J. Peypouquet, and P. Redont (2018) Fast convergence of inertial dynamics and algorithms with asymptotic vanishing viscosity. Mathematical Programming168 (1), pp. 123-175. Cited by: SS1.
* [3]H. Attouch, Z. Chbani, and H. Riahi (2019) Rate of convergence of the Nesterov accelerated gradient method in the subcritical case \(\alpha\leq 3\). ESAIM: control, Optimisation and Calculus of Variations25, pp. 2. Cited by: SS1.
* [4]M. Betancourt, M. I. Jordan, and A. C. Wilson (2018) On symplectic optimization. arXiv preprint arXiv:1802.03653. Cited by: SS1.
* [5]E. De Klerk, F. Glineur, and A. B. Taylor (2017) On the worst-case complexity of the gradient method with exact line search for smooth strongly convex functions. Optimization Letters11, pp. 1185-1199. Cited by: SS1.
* [6]J. Diakonikolas and M. I. Jordan (2021) Generalized momentum-based methods: a Hamiltonian perspective. SIAM Journal on Optimization31 (1), pp. 915-944. Cited by: SS1.
* [7]Y. Drori and A. B. Taylor (2020) Efficient first-order methods for convex minimization: a constructive approach. Mathematical Programming184 (1-2), pp. 183-220. Cited by: SS1.
* [8]Y. Drori and M. Teboulle (2014) Performance of first-order methods for smooth convex minimization: a novel approach. Mathematical Programming145 (1), pp. 451-482. Cited by: SS1.
* [9]M. Even, R. Berthier, F. Bach, N. Flammarion, P. Gaillard, H. Hendrikx, L. Massoulie, and A. Taylor (2020) Continuized accelerations of deterministic and stochastic gradient descents, and of gossip algorithms. In Advances in Neural Information Processing Systems, pp. 28054-28066. Cited by: SS1.
* [10]M. Fazlyab, A. Ribeiro, M. Morari, and V. M. Preciado (2018) Analysis of optimization algorithms via integral quadratic constraints: nonstrongly convex problems. SIAM Journal on Optimization28 (3), pp. 2654-2689. Cited by: SS1.
* [11]G. Gu and J. Yang (2019) On the optimal ergodic sublinear convergence rate of the relaxed proximal point algorithm for variational inequalities. arXiv preprint arXiv:1905.06030. Cited by: SS1.
* [12]G. Gu and J. Yang (2019) Optimal nonergodic sublinear convergence rate of proximal point algorithm for maximal monotone inclusion problems. arXiv preprint arXiv:1904.05495. Cited by: SS1.
* [13]S. Gu, B. P. Van Parys, and E. K. Ryu (2022) Branch-and-bound performance estimation programming: a unified methodology for constructing optimal optimization methods. arXiv preprint arXiv:2203.07305. Cited by: SS1.
* [14]B. Hu and L. Lessard (2017) Dissipativity theory for Nesterov's accelerated method. In International Conference on Machine Learning, pp. 1549-1557. Cited by: SS1.
* [15]B. Hu, P. Seiler, and A. Rantzer (2017) A unified analysis of stochastic optimization methods using jump system theory and quadratic constraints. In Conference on Learning Theory, pp. 1157-1189. Cited by: SS1.
* [16]D. Kim and J. A. Fessler (2016) Optimized first-order methods for smooth convex minimization. Mathematical programming159 (1), pp. 81-107. Cited by: SS1.
* [17]D. Kim (2016) Accelerated proximal point method for maximally monotone operators. Mathematical Programming190 (1-2), pp. 57-87. Cited by: SS1.
* [18]D. Kim (2016) Optimized first-order methods for smooth convex minimization. Mathematical programming159 (1), pp. 81-107. Cited by: SS1.

[MISSING_PAGE_POST]

* [18] Donghwan Kim and Jeffrey A Fessler. Optimizing the efficiency of first-order methods for decreasing the gradient of smooth convex functions. _Journal of optimization theory and applications_, 188(1):192-219, 2021.
* [19] Jungbin Kim and Insoon Yang. Unifying Nesterov's accelerated gradient methods for convex and strongly convex objective functions. In _International Conference on Machine Learning_, pages 16897-16954. PMLR, 2023.
* [20] Walid Krichene, Alexandre Bayen, and Peter L Bartlett. Accelerated mirror descent in continuous and discrete time. In _Advances in Neural Information Processing Systems_, volume 28, 2015.
* [21] Laurent Lessard, Benjamin Recht, and Andrew Packard. Analysis and design of optimization algorithms via integral quadratic constraints. _SIAM Journal on Optimization_, 26(1):57-95, 2016.
* [22] Laurent Lessard and Peter Seiler. Direct synthesis of iterative algorithms with bounds on achievable worst-case convergence rate. In _2020 American Control Conference (ACC)_, pages 119-125. IEEE, 2020.
* [23] Aleksandr Mikhailovich Lyapunov. The general problem of the stability of motion. _International journal of control_, 55(3):531-534, 1992.
* [24] Alexandre Megretski and Anders Rantzer. System analysis via integral quadratic constraints. _IEEE transactions on automatic control_, 42(6):819-830, 1997.
* [25] Celine Moucer, Adrien Taylor, and Francis Bach. A systematic approach to Lyapunov analyses of continuous-time models in convex optimization. _SIAM Journal on Optimization_, 33(3):1558-1586, 2023.
* [26] Yurii Nesterov. _Lectures on Convex Optimization_, volume 137. Springer, 2018.
* [27] Yurii E Nesterov. A method for solving the convex programming problem with convergence rate \(o(1/k^{2})\). In _Dokl. akad. nauk Sssr_, volume 269, pages 543-547, 1983.
* [28] Frigyes Riesz and Bela Sz Nagy. _Functional analysis_. Courier Corporation, 2012.
* [29] W. Rudin. _Real and Complex Analysis_. McGraw-Hill, 1987.
* [30] W. Rudin. _Functional Analysis_. McGraw-Hill, 1991.
* [31] Jesus Maria Sanz Serna and Konstantinos C Zygalakis. The connections between Lyapunov functions for some optimization algorithms and differential equations. _SIAM Journal on Numerical Analysis_, 59(3):1542-1565, 2021.
* [32] Damien Scieur, Vincent Roulet, Francis Bach, and Alexandre d'Aspremont. Integration methods and optimization algorithms. _Advances in Neural Information Processing Systems_, 30, 2017.
* [33] Bin Shi, Simon S Du, Michael I Jordan, and Weijie J Su. Understanding the acceleration phenomenon via high-resolution differential equations. _Mathematical Programming_, pages 1-70, 2021.
* [34] Bin Shi, Simon S Du, Weijie Su, and Michael I Jordan. Acceleration via symplectic discretization of high-resolution differential equations. In _Advances in Neural Information Processing Systems_, volume 32, 2019.
* [35] Elias M Stein and Rami Shakarchi. _Real Analysis: Measure Theory, Integration, and Hilbert Spaces_. Princeton University Press, 2009.
* [36] Weijie Su, Stephen Boyd, and Emmanuel J Candes. A differential equation for modeling Nesterov's accelerated gradient method: Theory and insights. _Journal of Machine Learning Research_, 17:1-43, 2016.

* [37] Jaewook J Suh, Gyumin Roh, and Ernest K Ryu. Continuous-time analysis of accelerated gradient methods via conservation laws in dilated coordinate systems. In _International Conference on Machine Learning_, pages 20640-20667. PMLR, 2022.
* [38] Boya Sun, Jemin George, and Solmaz Kia. High-resolution modeling of the fastest first-order optimization method for strongly convex functions. In _2020 59th IEEE Conference on Decision and Control (CDC)_, pages 4237-4242. IEEE, 2020.
* [39] Adrien Taylor and Francis Bach. Stochastic first-order methods: Non-asymptotic and computer-aided analyses via potential functions. In Alina Beygelzimer and Daniel Hsu, editors, _Proceedings of the Thirty-Second Conference on Learning Theory_, volume 99 of _Proceedings of Machine Learning Research_, pages 2934-2992. PMLR, 25-28 Jun 2019.
* [40] Adrien Taylor and Yoel Drori. An optimal gradient method for smooth strongly convex minimization. _Mathematical Programming_, pages 1-38, 2022.
* [41] Adrien Taylor, Bryan Van Scoy, and Laurent Lessard. Lyapunov functions for first-order methods: Tight automated convergence guarantees. In _International Conference on Machine Learning_, pages 4897-4906. PMLR, 2018.
* [42] Adrien B. Taylor, Julien M Hendrickx, and Francois Glineur. Smooth strongly convex interpolation and exact worst-case performance of first-order methods. _Mathematical Programming_, 161(1-2):307-345, May 2016.
* [43] Adrien B Taylor, Julien M Hendrickx, and Francois Glineur. Exact worst-case performance of first-order methods for composite convex optimization. _SIAM Journal on Optimization_, 27(3):1283-1313, 2017.
* [44] Adrien B Taylor, Julien M Hendrickx, and Francois Glineur. Exact worst-case convergence rates of the proximal gradient method for composite convex minimization. _Journal of Optimization Theory and Applications_, 178:455-476, 2018.
* [45] Manu Upadhyaya, Sebastian Banert, Adrien B Taylor, and Pontus Giselsson. Automated tight Lyapunov analysis for first-order methods. _arXiv preprint arXiv:2302.06713_, 2023.
* [46] Bryan Van Scoy, Randy A Freeman, and Kevin M Lynch. The fastest known globally convergent first-order method for minimizing strongly convex functions. _IEEE Control Systems Letters_, 2(1):49-54, 2017.
* [47] Andre Wibisono, Ashia C Wilson, and Michael I Jordan. A variational perspective on accelerated methods in optimization. _proceedings of the National Academy of Sciences_, 113(47):E7351-E7358, 2016.
* [48] Ashia C. Wilson, Ben Recht, and Michael I. Jordan. A Lyapunov analysis of accelerated methods in optimization. _Journal of Machine Learning Research_, 22(113):1-34, 2021.
* [49] Jingzhao Zhang, Aryan Mokhtari, Suvrit Sra, and Ali Jadbabaie. Direct Runge-Kutta discretization achieves acceleration. In _Advances in neural information processing systems_, volume 31, 2018.