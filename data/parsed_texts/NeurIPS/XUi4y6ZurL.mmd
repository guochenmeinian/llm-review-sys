# Thermodynamic Bayesian Inference

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

A fully Bayesian treatment of complicated predictive models (such as deep neural networks) would enable rigorous uncertainty quantification and the automation of higher-level tasks including model selection. However, the intractability of sampling Bayesian posteriors over many parameters inhibits the use of Bayesian methods where they are most needed. Thermodynamic computing has emerged as a paradigm for accelerating operations used in machine learning, such as matrix inversion, and is based on the mapping of Langevin equations to the dynamics of noisy physical systems. Hence, it is natural to consider the implementation of Langevin sampling algorithms on thermodynamic devices. In this work we propose electronic analog devices that sample from Bayesian posteriors by realizing Langevin dynamics physically. Circuit designs are given for sampling the posterior of a Gaussian-Gaussian model and for Bayesian logistic regression, and are validated by simulations. It is shown, under reasonable assumptions, that the time-complexity of sampling the Gaussian-Gaussian posterior is sublinear in dimension. These results highlight the potential to accelerate Bayesian inference with thermodynamic computing.

## 1 Introduction

Bayesian statistics has proved an effective framework for making predictions under uncertainty [1; 2; 3; 4; 5; 6], and it is central to proposals for automating machine learning [7]. Bayesian methods enable uncertainty quantification by incorporating prior knowledge and modeling a distribution over the parameters of interest. Popular machine learning methods that employ this approach include Bayesian linear and non-linear regression [8], Kalman filters [9], Thompson sampling [2], continual learning [10; 11], and Bayesian neural networks [3; 12].

Unfortunately, computing the posterior distribution in these settings is often intractable [13]. Methods such as the Laplace approximation [14] and variational inference [15] may be used to approximate the posterior in these cases, however their accuracy struggles for complicated posteriors, such as those of a Bayesian neural network [13]. Regardless, sampling accurately from such posteriors requires enormous computing resources [13].

Computational bottlenecks in Bayesian inference motivate the need for novel hardware accelerators. Physics-based sampling hardware has been proposed for this purpose, including Ising machines [16; 17; 18; 19; 20], probabilistic bit computers [21; 22; 23], and thermodynamic computers [24; 25; 26; 27; 28; 29]. Continuous-variable hardware is particularly suited to Bayesian inference since continuous distributions are typically used in probabilistic machine learning [30]. However, a rigorous treatment of how such hardware can perform Bayesian inference with scalable circuits has not yet been given.

The most computationally tractable algorithms for exact Bayesian inference are Monte Carlo sampling algorithms. The Langevin sampling algorithm [31; 32] is an elegant example inspired by statistical physics, based on the dynamics of a damped system in contact with a heat bath. What we propose in this work is to build a physical realization of the system that is simulated by the Langevin algorithm. The system must be designed to have a potential energy such that the Gibbs distribution \(p(x)\propto e^{-\beta U(x)}\) is the desired posterior distribution which is reached at thermodynamic equilibrium. We present circuit schematics for electronic implementations of such devices for Bayesian inference for two special cases. The first is a Gaussian-Gaussian model (where the prior and the likelihood are both multivariate normal, as found in linear regression and Kalman filtering), and the second is logistic regression (where the prior is Gaussian and the likelihood is Bernoulli parameterized by a logistic function). In each case, the parameters of the prior and likelihood are encoded in the values of components of the circuit, and then voltages or currents are measured to sample the random variable.

While thermodynamic algorithms have been proposed for linear algebra [27] and neural network training [33], our work can be viewed as the first thermodynamic algorithm for sampling from Bayesian posteriors. Moreover, our work provides the first concrete proposal for non-Gaussian sampling with thermodynamic hardware. Overall, our work opens up a new field of rigorous Bayesian inference with thermodynamic computers and lays the groundwork for scalable CMOS-based chips for probabilistic machine learning.

We show that in theory the device proposed for sampling the Gaussian-Gaussian model posterior can obtain \(N\) samples in \(d\) dimensions in time scaling with \(O(N\ln d)\). This is a significant speedup over typical methods used digitally for the same problem, which involve matrix inversions taking time scaling with \(O(d^{\omega})\) where \(2<\omega<3\). This speedup is larger than the polynomial speedups found in previous work on thermodynamic algorithms for linear algebra primitives [27] (where speedups were found to scale linearly with dimension).

## 2 Results

Suppose that we have samples of a random vector \(y\), and would like to estimate a random vector \(\theta\) on which \(y\) depends somehow. The Bayesian approach is to assume a prior distribution on \(\theta\) given by a density function \(p_{\theta}(\theta)\), and a likelihood function \(p_{y|\theta}(y|\theta)\). The posterior distribution for \(\theta\) is then given by Bayes's theorem \(p_{\theta|y}(\theta|y)=p_{y|\theta}(y|\theta)p_{\theta}(\theta)/p_{y}(y)\). To sample from the posterior using the Langevin algorithm, one first computes the score

\[\nabla_{\theta}\ln p_{\theta|y}(\theta|y)=\nabla_{\theta}\ln p_{y|\theta}(y| \theta)+\nabla_{\theta}\ln p_{\theta}(\theta).\] (1)

Then the score is used as the drift term in the following stochastic differential equation (SDE)

\[d\theta=\nabla_{\theta}\ln p_{\theta|y}(\theta|y)\,dt+\mathcal{N}[0,2\,dt].\] (2)

After this SDE is evolved for a sufficient time \(T\), the value of \(\theta\) will be a sample from \(p_{\theta|y}\). This algorithm is equivalent to the equilibration of an overdamped system, as we will now describe. First let \(r\) be a vector of the same dimension as \(\theta\) describing the state of a physical system, and satisfying \(r=\theta\tilde{r}\) for some constant \(\tilde{r}\) (this factor is necessary because \(\theta\) is unitless while the physical quantity \(r\) has units). Now we define the potential energy function \(\beta U(r)=-\ln p_{\theta|y}(r/\tilde{r}\mid y)\). The dynamics of an overdamped system with potential energy \(U\) in contact with a heat bath at inverse temperature \(\beta\) can be modeled by the overdamped Langevin equation

\[dr=-\gamma^{-1}\nabla_{r}U(r)\,dt+\mathcal{N}[0,2\gamma^{-1}\beta^{-1}\,dt],\] (3)

where \(\gamma\) is a damping constant. Note that his implies that \(\gamma\) has dimensions of \(\text{energy}\cdot\text{time}/[r]^{2}\). If we introduce a constant \(\tau=\gamma\beta\tilde{r}^{2}\), Eq. (3) can be written

\[d\theta=\nabla_{\theta}\ln p_{\theta|y}(\theta|y)\tau^{-1}\,dt+\mathcal{N}[0, 2\,\tau^{-1}dt],\] (4)

which has the same form as Eq. (2), except with the time constant \(\tau\). It is clear that if Eq. (2) must be run for a dimensionless duration \(T\) to achieve convergence, then the physical system must be allowed to evolve for a physical time duration \(\tau T\) to achieve the same result. While we have addressed the case of conditioning on a single sample \(y\) above, the generalization of these ideas to the case of conditioning on multiple I.I.D. samples is given in Appendix D. In what follows we will present designs for circuits whose potential energy results in an overdamped Langevin equation that yields samples from Bayesian posteriors.

### Gaussian-Gaussian model

A particularly simple special case of Bayesian inference is a when both the prior and the likelihood are multivariate normal, and we address this simple model first in order to illustrate our approach more clearly. Specifically, let \(\theta\in\mathbb{R}^{d}\) have prior distribution \(p_{\theta}(\theta)=\mathcal{N}[\mu,\Sigma]\), and let the likelihood be \(p_{y|\theta}(y|\theta)=\mathcal{N}[\theta,\Sigma_{y|\theta}]\), where \(y\in\mathbb{R}^{d}\) is an observed sample. In this case the posterior \(p_{\theta|y}\) is also multivariate normal, with parameters [12]

\[\mu_{\theta|y}=\mu+\Sigma\left(\Sigma+\Sigma_{y|\theta}\right)^{-1}(y-\mu),\] (5)

\[\Sigma_{\theta|y}=\Sigma-\Sigma(\Sigma+\Sigma_{\theta|y})^{-1}\Sigma.\] (6)

For this model, the posterior is tractable and can be computed on digital computers relatively efficiently, however for very large dimensions the necessary matrix inversion and matrix-matrix multiplications can still create a costly computational bottleneck. As we will see, the thermodynamic approach provides a means to avoid the costly inversion and matrix products in the computation, and therefore to accelerate Bayesian inference for this model.

We begin by deriving the Langevin equation for sampling this posterior. For this prior and likelihood, the score of the posterior Eq. (1) is

\[\nabla_{\theta}\ln p_{\theta|y}(\theta|y)=-\Sigma^{-1}(\theta-\mu)-\Sigma_{y| \theta}^{-1}(\theta-y),\] (7)

and so Eq. (4) becomes

\[d\theta=-\Sigma^{-1}(\theta-\mu)\tau^{-1}dt-\Sigma_{y|\theta}^{-1}(\theta-y) \tau^{-1}dt+\mathcal{N}[0,2\mathbb{I}\tau^{-1}dt].\] (8)

In fact, this SDE can be implemented by a circuit consisting of two resistor networks coupled by inductors, shown in Fig. 1 for the two-dimensional case.

The full analysis of the circuit in Fig. 1 is given in Appendix A, but a few remarks are made here to explain its operation. First, we define the conductance matrices \(\mathcal{G}\) as

\[\mathcal{G}=\begin{pmatrix}R_{11}^{-1}+R_{12}^{-1}&-R_{12}^{-1}\\ -R_{12}^{-1}&R_{22}+R_{12}^{-1}\end{pmatrix},\] (9)

and \(\mathcal{G}^{\prime}\) is defined in the same way for the primed resistors \(R_{1}^{\prime}\), \(R_{2}^{\prime}\), and \(R_{12}^{\prime}\). By applying Kirchoff's current law (KCL), the voltages across the resistors can be eliminated. Then the equation \(V=L\dot{I}\) is used to derive the following stochastic differential equation for the currents through the inductors

\[dI_{L}=-L^{-1}\mathcal{G}^{-1}(I_{L}-I)\,dt-L^{-1}\mathcal{G}^{\prime-1}(I_{L }-I^{\prime})\,dt+L^{-1}\sqrt{S}\mathcal{N}[0,\mathbb{I}\,dt],\] (10)

where \(I_{L}=(I_{L1}\;\;I_{L2})^{\intercal}\) and \(S\) is the power spectral density of each noise source. This equation has the same form as Eq. (8), so it is only necessary to determine an appropriate mapping of distributional parameters to physical properties of the circuit's components (see Appendix A). By

Figure 1: Circuit schematic for the Gaussian-Gaussian model posterior sampling device.

including more inductors and coupling resistors (as well as current and voltage sources), the design can be generalized to arbitrary dimension.

To verify that the proposed circuit does indeed evolve according to the correct SDE, we ran SPICE circuit simulations. Figure 2 shows the results of such a simulation where a 2-dimensional Gaussian prior and a 2-dimensional Gaussian likelihood are encoded into the conductances while the current in each inductor is measured to determine the resulting posterior.

As shown in Appendix C, the asymptotic runtime complexity for this algorithm is

\[t=O(N\kappa\tau\ln(\kappa^{3/2}d^{1/2}W_{0}^{-1})),\] (11)

where \(\kappa\) is the condition number of the posterior covariance, \(\tau=L/\tilde{R}\), \(d\) is the dimension, and \(W_{0}\) is the Wasserstein distance between the true posterior and the distribution sampled by the device. The assumptions used to derive this result can also be found in Appendix C. Remarkably, the required time is sublinear in dimension, a large improvement over digital algorithms where complexity of constructing and sampling from the Gaussian-Gaussian posterior (67 - 68) is \(O(d^{\omega})\) where \(\omega\) is the matrix multiplication constant (or more practically \(O(d^{3})\) via common implementations of Cholesky factorization). In Figure 3(a), we report the convergence of simulated thermodynamic samples for the Gaussian-Gaussian model with zero prior mean and covariances \(\Sigma\), \(\Sigma_{y|\theta}\) randomly sampled from a Wishart distribution with \(2d\) degrees of freedom. We see fast convergence in Wasserstein distance to the true posterior, supporting our theoretical claims.

### Bayesian linear regression and Kalman filtering

A generalization of the Gaussian-Gaussian model is that of Bayesian linear regression [8] (or equivalently a Kalman filter update step [9, 12]). In full generality we have

\[p_{\theta}(\theta) =\mathcal{N}[\mu,\Sigma],\] (12) \[p_{y|\theta}(y\mid\theta) =\mathcal{N}[H\theta,\Sigma_{y|\theta}],\] (13)

Then the overdamped Langevin SDE becomes

\[d\theta =-\Sigma^{-1}(\theta-\mu)\tau^{-1}\,dt-H^{\intercal}\Sigma_{y| \theta}^{-1}(y-H\theta)\tau^{-1}\,dt+\mathcal{N}[0,2\mathbb{I}\tau^{-1}dt],\] \[=-(A\theta-b)\tau^{-1}\,+\mathcal{N}[0,2\mathbb{I}\tau^{-1}dt], \text{ for }A=\Sigma^{-1}+H^{\intercal}\Sigma_{y|\theta}^{-1}H\text{ and }b=\mu+H^{\intercal}\Sigma_{y|\theta}^{-1}y.\] (14)

The form of the SDE (Ornstein-Unhlenbeck process) in 14 is exactly that of the thermodynamic device in [27] which if given input \(A\) and \(b\) above will produce samples from the Gaussian Bayesian posterior \(p_{\theta|y}(\theta\mid y)\). Compared to the simpler Gaussian-Gaussian model above, a disadvantage of this approach is thathe covariances \(\Sigma\) and \(\Sigma_{y|\theta}\) have to be inverted prior to input as \(A\). However, for linear regression, these matrices are often assumed to be diagonal and otherwise they can be

Figure 2: SPICE simulations of proposed Gaussian-Gaussian circuit in Fig. 1. The grey points represent the simulated circuit’s induct currents. The dashed black and solid blue ellipses represent the empirical sample covariance and the target posterior covariance from a Gaussian Bayesian update, respectively. The red and green ellipses represent the prior and likelihood.

efficiently inverted using the thermodynamic procedures in [27] as preprocessing. Additionally, the formulation of \(A\) requires matrix-matrix multiplications which can be costly (even in the case of diagonal covariances). Although, this can be accelerated with parallelization.

On the other hand, the generality of (12-13) makes the approach highly practical. Encompassing Bayesian linear regression [34] and the update step of the Kalman filter [9]. Moreover in the setting of Kalman filtering, the matrices \(\Sigma\) and \(\Sigma_{y|\theta}\) are typically shared across time points and thus only need to be inverted once in comparison to the Bayesian posterior update which is applied at every time step (and typically represents the computation bottleneck due to the required matrix inversion).

In Figure 3(b), we simulate the evaluation of the thermodynamic linear algebra device [27] for a Bayesian linear regression task. We use the diabetes dataset [36] which has \(N=442\) continuous response variables \(y\) and 10 input features. We vary the number of features and therefore posterior dimension for the linear regression by extending to include the first \(d\) cross terms in the Taylor expansion over the input features. These input features are loaded as rows in the matrix \(H\in\mathbb{R}^{N\times d}\). Both covariances are set to diagonal, \(\Sigma=\mathbb{I}\) and \(\Sigma_{y|\theta}=0.1\mathbb{I}\). We observe that the Wasserstein distance converges quickly as more samples are collected and scales reasonably with dimension, indicating a sublinear scaling similar to the Gaussian-Gaussian model.

### Bayesian logistic regression

Logistic regression is a method for classification tasks (both binary and multiclass) that models the dependence of class probabilities on independent variables using a logistic function. In the Bayesian setting, a prior can be assumed on the parameters of a logistic regression model, for example it is common to assume a Gaussian prior. However, after conditioning on observed data a posterior distribution is produced that has no analytical closed form, making Bayesian logistic regression far less efficient than obtaining a point estimate of the parameters. In this section we present a thermodynamic hardware architecture capable of sampling the posterior for binary logistic regression, and show some preliminary evidence that this architecture can do so more efficiently than existing methods.

Given a parameter vector \(\theta\in\mathbb{R}^{d}\) and an independent variable vector \(x\in\mathbb{R}^{d}\), binary logistic regression outputs a class probability \(p_{y|\theta,x}(y|\theta,x)\), where \(y\in\{-1,1\}\) (often \(y\in\{0,1\}\) is written instead but we choose this notation to simplify the presentation). The likelihood is \(p_{y|\theta,x}(y|\theta,x)\ =\ L(y\theta^{\intercal}x)\) where \(L(z)=1/(1+e^{-z})\) is the standard logistic function [37]. Note that we will first consider the case of conditioning on a single sample, and in this case the likelihood will be denoted \(p_{y|\theta}(y|\theta)\) as \(x\) is constant. Additionally, a multivariate normal prior is assumed for the parameters \(\theta\sim\mathcal{N}[\mu,\Sigma]\). The Langevin equation for sampling the posterior is therefore:

\[d\theta=-\Sigma^{-1}(\theta-\mu)\tau^{-1}dt+L(-y\theta^{\intercal}x)yx\tau^{ -1}dt+\mathcal{N}[0,2\mathbb{I}\tau^{-1}dt].\] (15)

Figure 3: Convergence in Wasserstein distance between simulated thermodynamic samples and the true Gaussian posterior as a function of the number of samples (sampling time). All results are simulated exactly with thermox[35] and averaged over 10 random seeds with one standard deviation shown. Panel (a): Gaussian-Gaussian model with zero prior mean and covariances sampled from a Wishart distribution. Panel (b): Bayesian linear regression on the diabetes dataset [36] with dimension (number of features) varied by including higher-order cross terms of the 10 input data features.

A circuit implementing Eq. (15) is shown in Fig. 7, and the detailed analysis of this circuit is given in Appendix B. Equation (15) is valid for a single data sample, however, as mentioned, in practice we generally take gradients over a larger number of examples such that the gradients are less noisy. This can be done by enlarging the hardware, resulting in the second term of Eq. (15) being replaced by a sum \(\sum_{i=1}^{N}L(-y_{i}\theta^{\intercal}x_{i})y_{i}x_{i}dt\), with \(N\) the number of data points. One may also consider minibatches, and the sum is only over a batch of size \(b\). This is achievable by summing currents, which is detailed in the circuit implementation in Appendix B. At a high-level, implementing this protocol in hardware is very simple in the case of a full batch, since the data only needs to be sent once onto the hardware. The following steps are taken to collect the samples: (1) Map the data labels to \(\{+1,-1\}\). (2) Map the data \((\bar{X},Y)\) onto the hardware (full batch setting). (3) Initialize the state of the system, set the mean and the covariance matrix of the prior. (4) At every interval \(t_{s}\) (the sampling time), measure the state of the system \(\theta(t)\) to collect samples.

In Fig. 4, we present results for a Bayesian logistic regression on a two-moons dataset, made of points separated in two classes that are arranged in intersecting moons in the 2D planes, as shown in Fig. 4(a). These results are obtained by running the SDE of Eq. (15), hence corresponds to an ideal simulation of the thermodynamic hardware. In this scenario, there are 3 parameters to sample, and \(N=100\) points are considered. In Fig. 4(a), we see that even for such a simple model, only a few points are missclassified. As mentioned, previously, this setting also gives access to better methods to estimate uncertainty in predictions. In Fig. 4(b), the Kernel Stein discrepancy (KSD) [38] is shown as a function of the number of collected samples for varying sampling rates. These results indicate that the number of samples to reach a low KSD (close to convergence) can be reduced by increasing the sampling time, indicating correlated samples, as is often the case.

## 3 Conclusion

In this work, we proposed the first thermodynamic algorithms for sampling from Bayesian posteriors. We provided explicit constructions of CMOS-compatible analog circuits to implement these algorithms with scalable silicon chips. Our circuit for performing logistic regression represents the first concrete proposal for non-Gaussian sampling with a thermodynamic computer. In the case of Gaussian Bayesian inference (Gaussian prior, Gaussian likelihood), our analysis showed a sublinear complexity in \(d\), leading to a speedup over standard digital methods that is greater than linear. This is an even larger speedup than those previously observed for thermodynamic linear algebra [27], suggesting that Bayesian inference is an ideal application for thermodynamic computers. Our work lays the foundation for accelerating Bayesian inference, a key component of probabilistic machine learning, with physics-based hardware.

Figure 4: Panel (a): Probability surface to belong to class 1 (blue points). The dataset is also shown, where class 0 (blue points) and class 2 (orange points) are arranged in two intersecting moons. Panel (b): Kernel Stein discrepancy (KSD) of the collected samples with an ideal thermodynamic sampler, for varying sampling times. The sampling time is given in units of \(10^{-3}\tau\). The KSD is averaged over five sets of random samples and \(\tau=1\).

## References

* (1) Andrew Gelman, Aki Vehtari, Daniel Simpson, Charles C Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian Burkner, and Martin Modrak. Bayesian workflow. _arXiv preprint arXiv:2011.01808_, 2020.
* (2) Daniel J Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, Zheng Wen, et al. A tutorial on thompson sampling. _Foundations and Trends(r) in Machine Learning_, 11(1):1-96, 2018.
* (3) Andrew G Wilson and Pavel Izmailov. Bayesian deep learning and a probabilistic perspective of generalization. _Advances in neural information processing systems_, 33:4697-4708, 2020.
* (4) Samuel Duffield, Kaelan Donatella, Johnathan Chiu, Phoebe Klett, and Daniel Simpson. Scalable bayesian learning with posteriors. _arXiv preprint arXiv:2406.00104_, 2024.
* (5) Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra Acharya, et al. A review of uncertainty quantification in deep learning: Techniques, applications and challenges. _Information fusion_, 76:243-297, 2021.
* (6) Theodore Papamarkou, Maria Skoularidou, Konstantina Palla, Laurence Aitchison, Julyan Arbel, David Dunson, Maurizio Filippone, Vincent Fortuin, Philipp Hennig, Jose Miguel Hernandez-Lobato, et al. Position: Bayesian deep learning is needed in the age of large-scale ai. In _Forty-first International Conference on Machine Learning_, 2024.
* (7) Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren. _Automated machine learning: methods, systems, challenges_. Springer Nature, 2019.
* (8) Christopher M Bishop and Michael E Tipping. Bayesian regression and classification. In _Advances in learning theory: methods, models and applications_, pages 267-285. IOS Press, 2003.
* (9) Simo Sarkka and Lennart Svensson. _Bayesian filtering and smoothing_, volume 17. Cambridge university press, 2023.
* (10) James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. _Proceedings of the national academy of sciences_, 114(13):3521-3526, 2017.
* (11) Cuong V. Nguyen, Yingzhen Li, Thang D. Bui, and Richard E. Turner. Variational continual learning. In _International Conference on Learning Representations_, 2018.
* (12) Samuel Duffield and Sumeetpal S Singh. Ensemble kalman inversion for general likelihoods. _Statistics & Probability Letters_, 187:109523, 2022.
* (13) Pavel Izmailov, Sharad Vikram, Matthew D Hoffman, and Andrew Gordon Gordon Wilson. What are bayesian neural network posteriors really like? In _International conference on machine learning_, pages 4629-4640. PMLR, 2021.
* effortless bayesian deep learning, 2022.
* (15) David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational inference: A review for statisticians. _Journal of the American Statistical Association_, 112(518):859-877, April 2017.
* (16) Takahiro Inagaki, Yoshitaka Haribara, Koji Igarashi, Tomohiro Sonobe, Shuhei Tamate, Toshimori Honjo, Alireza Marandi, Peter L. McMahon, Takeshi Umeki, Koji Enbutsu, Osamu Tadanaga, Hirokazu Takenouchi, Kazuyuki Aihara, Ken-ichi Kawarabayashi, Kyo Inoue, Shoko Utsunomiya, and Hiroki Takesue. A coherent ising machine for 2000-node optimization problems. _Science_, 354(6312):603-606, 2016.
* (17) Jeffrey Chou, Suraj Bramhavar, Siddhartha Ghosh, and William Herzog. Analog coupled oscillator based weighted ising machine. _Scientific reports_, 9(1):14786, 2019.

* [18] Naeimeh Mohseni, Peter L. McMahon, and Tim Byrnes. Ising machines as hardware solvers of combinatorial optimization problems. _Nat. Rev. Phys._, 4(6):363-379, 2022.
* [19] Y Yamamoto, T Leleu, S Ganguli, and H Mabuchi. Coherent ising machines--quantum optics and neural network perspectives. _Applied Physics Letters_, 117(16):160501, 2020.
* [20] Tianshi Wang and Jaijeet Roychowdhury. Oim: Oscillator-based ising machines for solving combinatorial optimisation problems. In _Unconventional Computation and Natural Computation: 18th International Conference, UCNC 2019, Tokyo, Japan, June 3-7, 2019, Proceedings 18_, pages 232-256. Springer, 2019.
* [21] J. Kaiser, S. Datta, and B. Behin-Aein. Life is probabilistic--why should all our computers be deterministic? computing with p-bits: Ising solvers and beyond. In _2022 International Electron Devices Meeting (IEDM)_, pages 21-4. IEEE, 2022.
* [22] Kerem Y. Camsari, Brian M. Sutton, and Supriyo Datta. p-bits for probabilistic spin logic. _Appl. Phys. Rev._, 6(1):011305, mar 2019.
* [23] Navid Anjum Aadit, Andrea Grimaldi, Mario Carpentieri, Luke Theogarajan, John M. Martinis, Giovanni Finocchio, and Kerem Y. Camsari. Massively parallel probabilistic computing with sparse Ising machines. _Nat. Electron._, 5(7):460-468, 2022.
* [24] Tom Conte, Erik DeBenedictis, Natesh Ganesh, Todd Hylton, John Paul Strachan, R Stanley Williams, Alexander Alemi, Lee Altenberg, Gavin E. Crooks, James Crutchfield, et al. Thermodynamic computing. _arXiv preprint arXiv:1911.01968_, 2019.
* [25] Todd Hylton. Thermodynamic neural network. _Entropy_, 22(3):256, 2020.
* [26] Denis Melanson, Mohammad Abu Khater, Maxwell Aifer, Kaelan Donatella, Max Hunter Gordon, Thomas Ahle, Gavin Crooks, Antonio J Martinez, Faris Sbahi, and Patrick J Coles. Thermodynamic computing system for AI applications. _arXiv preprint arXiv:2312.04836_, 2023.
* [27] Maxwell Aifer, Kaelan Donatella, Max Hunter Gordon, Samuel Duffield, Thomas Ahle, Daniel Simpson, Gavin E Crooks, and Patrick J Coles. Thermodynamic linear algebra. _arXiv preprint arXiv:2308.05660_, 2023.
* [28] Samuel Duffield, Maxwell Aifer, Gavin Crooks, Thomas Ahle, and Patrick J Coles. Thermodynamic matrix exponentials and thermodynamic parallelism. _arXiv preprint arXiv:2311.12759_, 2023.
* [29] Patryk Lipka-Bartosik, Marti Perarnau-Llobet, and Nicolas Brunner. Thermodynamic computing via autonomous quantum thermal machines. _arXiv preprint arXiv:2308.15905_, 2023.
* [30] Patrick J Coles, Collin Szczepanski, Denis Melanson, Kaelan Donatella, Antonio J Martinez, and Faris Sbahi. Thermodynamic ai and the fluctuation frontier. In _2023 IEEE International Conference on Rebooting Computing (ICRC)_, pages 1-10. IEEE, 2023.
* [31] Radford M Neal. Mcmc using hamiltonian dynamics. _arXiv preprint arXiv:1206.1901_, 2012.
* [32] Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In _Proceedings of the 28th international conference on machine learning (ICML-11)_, pages 681-688. Citeseer, 2011.
* [33] Kaelan Donatella, Samuel Duffield, Maxwell Aifer, Denis Melanson, Gavin Crooks, and Patrick J Coles. Thermodynamic natural gradient descent. _arXiv preprint arXiv:2405.13817_, 2024.
* [34] Thomas Minka. Bayesian linear regression. Technical report, Citeseer, 2000.
* [35] Samuel Duffield, Kaelan Donatella, and Denis Melanson. thermox: Exact ou processes with jax. https://github.com/normal-computing/thermox, 2024.
* [36] Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. Least angle regression. 2004.

* [37] Kevin P Murphy. _Probabilistic machine learning: an introduction_. MIT press, 2022.
* [38] Jackson Gorham and Lester Mackey. Measuring sample quality with stein's method. _Advances in neural information processing systems_, 28, 2015.
* [39] Crispin W Gardiner. Handbook of stochastic methods for physics, chemistry and the natural sciences. _Springer series in synergetics_, 1985.
* [40] DC Dowson and BV666017 Landau. The frechet distance between multivariate normal distributions. _Journal of multivariate analysis_, 12(3):450-455, 1982.
* [41] J Leo van Hemmen and Tsuneya Ando. An inequality for trace ideals. _Communications in Mathematical Physics_, 76:143-148, 1980.

## Appendix A Analysis of Gaussian Bayesian Inference Circuit

In Figure 5, positive current goes up through the two inductors, left to right through \(R_{12}\) and \(R_{12}^{\prime}\), and towards ground in the other resistors. The two inductors have the same inductance \(L\). KCL gives

\[I_{L1}-I_{1}=I_{R1}+I_{12}\] (16) \[I_{L2}-I_{2}=I_{R2}-I_{12}\] (17) \[-I_{L1}+I_{1}^{\prime}=I_{R1}^{\prime}+I_{12}^{\prime}\] (18) \[-I_{L2}+I_{2}^{\prime}=I_{R2}^{\prime}-I_{12}^{\prime}.\] (19)

Using Ohm's law,

\[I_{L1}-I_{1}=R_{1}^{-1}V_{1}+R_{12}^{-1}(V_{1}-V_{2})=(R_{1}^{-1}+R_{12}^{-1}) V_{1}-R_{12}^{-1}V_{2}\] (20)

\[I_{L2}-I_{2}=R_{2}^{-1}V_{2}-R_{12}^{-1}(V_{1}-V_{2})=(R_{2}^{-1}+R_{12}^{-1}) V_{2}-R_{12}^{-1}V_{1}.\] (21)

These can be written as a single vector equation as follows

\[I_{L}-I=\mathcal{G}V,\] (22)

where \(I_{L}=(I_{L1}\;\;I_{L2})\tau\), \(I=(I_{1}\;\;I_{2})\tau\), and

\[\mathcal{G}=\begin{pmatrix}R_{1}^{-1}+R_{12}^{-1}&-R_{12}^{-1}\\ -R_{12}^{-1}&R_{2}^{-1}+R_{12}^{-1}\end{pmatrix}.\] (23)

Similarly, for the lower subcircuit we have

\[-I_{L}+I^{\prime}=\mathcal{G}^{\prime}V^{\prime}.\] (24)

The inductors obey the equations

\[L_{1}\dot{I}_{L1}=V_{1}^{\prime}-(V_{1}-V_{n1})\] (25)

\[L_{2}\dot{I}_{L2}=V_{2}^{\prime}-(V_{2}-V_{n2}),\] (26)

or in vector notation

\[L\dot{I}_{L}=V^{\prime}-V+V_{n}.\] (27)

Substituting in the expressions for \(V\) and \(V^{\prime}\) derived before, we have

\[L\dot{I}_{L}=\mathcal{G}^{\prime-1}(I^{\prime}-I_{L})-\mathcal{G}^{-1}(I_{L}- I)+V_{n},\] (28)

or

\[\dot{I}_{L}=-L^{-1}\mathcal{G}^{-1}(I_{L}-I)-L^{-1}\mathcal{G}^{\prime-1}(I_{L }-I^{\prime})+L^{-1}V_{n}.\] (29)

\[dI_{L}=-L^{-1}\mathcal{G}^{-1}(I_{L}-I)\,dt-L^{-1}\mathcal{G}^{\prime-1}(I_{L }-I^{\prime})\,dt+L^{-1}\sqrt{S}\mathcal{N}[0,\mathbb{I}\,dt].\] (30)

Figure 5: Circuit schematic for the Gaussian-Gaussian model posterior sampling device.

We now proceed to non-dimensionalize the above equation. \(\mathcal{G}=\tilde{R}^{-1}A\).

\[\tilde{I}d\theta=-\tilde{I}\tilde{R}L^{-1}A^{-1}(\theta-\mu)\,dt-\tilde{I}\tilde{ R}L^{-1}A^{\prime-1}(\theta-\mu^{\prime})\,dt+L^{-1}\sqrt{S}\mathcal{N}[0, \mathbb{I}\,dt].\] (31)

Define \(\tau=L/\tilde{R}\), giving

\[d\theta=-A^{-1}(\theta-\mu)\tau^{-1}\,dt-A^{\prime-1}(\theta-\mu^{\prime}) \tau^{-1}\,dt+\tilde{I}^{-1}L^{-1}\sqrt{S}\mathcal{N}[0,\mathbb{I}\,dt].\] (32)

If we set \(S=2\tilde{I}^{2}L\tilde{R}\), then we have

\[d\theta=-A^{-1}(\theta-\mu)\tau^{-1}\,dt-A^{\prime-1}(\theta-\mu^{\prime}) \tau^{-1}\,dt+\mathcal{N}[0,2\mathbb{I}\tau^{-1}\,dt].\] (33)

## Appendix B Analysis of Bayesian Logistic Regression Circuit

We now analyze the circuit in Figure 7. The boxes labeled Diff. Pair represent differential pairs of NPN bipolar junction transistors (BJTs), as shown in Fig. 6. To achieve a working implementation, additional circuitry is needed to support the differential pair and assure that it is appropriately biased, including a power source and possibly current mirrors.

The following conventions for current flow will be used

* \(I_{C}\) is the current _into_ the collector of a transistor. \(I_{B}\) is the current _into_ the base of a transistor. \(I_{E}\) is the current _out of_ the emitter of a transistor.
* The output current \(I_{o}\) of a differential pair is the current that flows _into_ the collector of the BJT labeled \(Q_{a}\).
* Positive current flows in the direction of the arrow through all current sources.
* Positive current flows downwards through \(C_{1}\) and \(C_{2}\) and from left to right through \(R_{12}\).
* Through resistors \(R_{A11}\), \(R_{B11}\), etc. positive current always flows towards the base of the transistor.

### Analysis of the BJT differential pair

We first consider the behavior of differential pair subcircuit, which can be explained using the Ebers-Moll model. The Ebers-Moll model describes the BJT in active mode, meaning when \(V_{E}<V_{B}<V_{C}\), and the circuit must be appropriately biased at all times to ensure the device is always in active mode. According to this model, in active mode the following relations are satisfied

\[I_{C}=I_{S}\left(e^{(V_{B}-V_{E})/V_{T}}-1\right),\] (34)

\[I_{C}=\alpha I_{E},\] (35)

Figure 6: Circuit schematic for the BJT differential pair.

where \(I_{S}\) is the saturation current, \(V_{T}\) the thermal voltage, and \(\alpha\) is the common-base current gain. \(I_{S}\) is typically on the order of \(10^{-15}\) to \(10^{-12}\) Amps, and at room temperature \(V_{T}=25.3\)mV. The parameter \(\alpha\) is between \(0.98\) and \(1\). It follows from Kirchoff's current law (KCL) that \(I_{B}=(1-\alpha)I_{E}\). For these typical values of the parameters appearing in Eq. (34) the subtraction of unity in parentheses can safely be ignored, which we will do in what follows. In order for the Ebers-Moll model to be valid, the voltage \(V_{0}\) should be determined such that \(V_{C}>V_{B}>V_{E}\) for all transistors at all times, but the value of \(V_{0}\) is otherwise unimportant.

To analyze the differential pair of transistors \(Q_{a}\) and \(Q_{b}\), observe that (by KCL)

\[I_{Ea}+I_{Eb}=I_{E}.\] (36)

We must distinguish between the two base voltages \(V_{a}\) and \(V_{b}\), but the two emitter voltages are the same, so we write \(V_{E}=V_{Ea}=V_{Eb}\). Using Eqs. (34) and (35) then,

\[I_{E}=\frac{I_{S}}{\alpha}e^{-V_{E}/V_{T}}\left(e^{V_{a}/V_{T}}+e^{V_{b}/V_{T }}\right),\] (37)

where we have dropped the \(-1\) as explained earlier. Now the emitter current \(I_{Ea}\) can be written as

\[I_{Ea} =\frac{I_{S}}{\alpha}e^{(V_{a}-V_{E})/V_{T}}\] (38) \[=\frac{I_{E}e^{V_{a}/V_{T}}}{e^{V_{a}/V_{T}}+e^{V_{Bb}/V_{T}}}\] (39) \[=\frac{I_{E}}{1+e^{-(V_{a}-V_{b})/V_{T}}},\] (40)

and similarly

\[I_{Eb}=\frac{I_{E}}{1+e^{(V_{a}-V_{b})/V_{T}}}.\] (41)

Equation (35) is then used to find the collector currents

\[I_{Ca}=\frac{\alpha I_{E}}{1+e^{-(V_{a}-V_{b})/V_{T}}},\] (42)

\[I_{Cb}=\frac{\alpha I_{E}}{1+e^{(V_{a}-V_{b})/V_{T}}}.\] (43)

The base voltages \(V_{a}\) and \(V_{b}\) are still undetermined. However, we will assume the limit \(\alpha\to 1\), where the base current goes to zero. In this limit, the two transistor bases may be connected to nodes in an external circuit to set their voltages. As there is no base current, these connections do not affect the voltages in the external circuit. In what follows, we will consider \(I_{Ca}\) the output of the differential pair, and label this current \(I_{o}\). Again taking the limit \(\alpha\to 1\), we have

\[I_{o}=\frac{I_{E}}{1+e^{(V_{a}-V_{b})/V_{T}}}=I_{E}L(-(V_{a}-V_{b})/V_{T}),\] (44)

where \(L(z)=1/(1+e^{-z})\) is the standard logistic function. Note that the support circuitry may include a current mirror that inverts the sign of the output current. As this formally has the same effect as a negative value of \(I_{E}\), we will allow \(I_{E}\) to be negative in what follows.

### Analysis of the logistic regression circuit

As the BJT bases draw negligible current, the voltages \(V_{a1}\), \(V_{b1}\), \(V_{a2}\), and \(V_{b2}\) in the circuit can be determined by considering the circuit in the absence of the differential pairs. In this case, we see that (by KCL)

\[R_{a11}^{-1}(V_{C1}-V_{a1})+R_{a12}^{-1}(V_{C2}-V_{a1})-R_{a10}^{-1}V_{a1}=0,\] (45)

and solving for \(V_{a1}\) gives

\[V_{a1}=\frac{R_{a11}V_{C1}+R_{a12}V_{C2}}{R_{a11}+R_{a12}+R_{a11}R_{a12}R_{a10 }^{-1}}=\frac{R_{a11}^{-1}V_{C1}+R_{a12}^{-1}V_{C2}}{R_{a10}^{-1}+R_{a11}^{-1}+ R_{a12}^{-1}}.\] (46)The same reasoning applies for \(V_{b1}\), resulting in

\[V_{b1}=\frac{R_{b11}^{-1}V_{C1}+R_{b12}^{-1}V_{C2}}{R_{b10}^{-1}+R_{b11}^{-1}+R_{ b12}^{-1}},\] (47)

so

\[V_{a1}-V_{b1}=\frac{g_{a11}V_{C1}+g_{a12}V_{C2}}{g_{a10}+g_{a11}+g_{a12}}-\frac{ g_{b11}V_{C1}+g_{b12}V_{C2}}{g_{b10}+g_{b11}+g_{b12}},\] (48)

where we have written the previous results in terms of the conductance \(g=R^{-1}\). The above can be written more conveniently by defining the vectors \(\hat{g}_{a1}=(g_{a10}+g_{a11}+g_{a12})^{-1}(g_{a11},g_{a12})^{\intercal}\) and \(\hat{g}_{b1}=(g_{b10}+g_{b11}+g_{b12})^{-1}(g_{b11},g_{b12})^{\intercal}\), in terms of which we have

\[V_{a1}-V_{b1}=(\hat{g}_{a}-\hat{g}_{b})^{\intercal}V_{C}.\] (49)

or, defining \(\hat{g}_{1}=\hat{g}_{a1}-\hat{g}_{b1}\), we simply have

\[V_{a1}-V_{b1}=\hat{g}_{1}^{\intercal}V_{C}.\] (50)

The latter result can be plugged into Eq. (44) to get \(I_{o1}\),

\[I_{o1}=I_{E1}L(-\hat{g}_{1}^{\intercal}V_{C}/V_{T}),\] (51)

where, as before, \(L(z)=1/(1+e^{-z})\) is the standard logistic function. By an identical derivation to the one above, a similar relation holds for the lower subcircuit

\[I_{o2}=I_{E2}L(-\hat{g}_{2}^{\intercal}V_{C}/V_{T}).\] (52)

We also assume that all resistors \(R_{aij}\), \(R_{bij}\) are very large compared to \(R_{12}\) so the current flowing through these resistors can be treated as negligible. This assumption does not affect the function of resistors \(R_{aij}\), \(R_{bij}\) because only the ratios of these resistances determine the voltages \(V_{ai}\), \(V_{bi}\). Next, we apply KCL to the nodes at the top of capacitors \(C_{1}\) and \(C_{2}\)

\[-I_{C1}+I_{1}-R_{1}^{-1}V_{C1}+R_{12}^{-1}(V_{C2}-V_{C1})-I_{o1}=0,\] (53)

Similarly, KCL for the node above capacitor \(C_{2}\) reads

\[-I_{C2}+I_{2}-R_{2}^{-1}V_{C2}+R_{12}^{-1}(V_{C1}-V_{C2})-I_{o2}=0.\] (54)

Substituting in the expressions derived for the collector currents, we then have

\[-I_{C1}+I_{1}-R_{1}^{-1}V_{C1}+R_{12}^{-1}(V_{C2}-V_{C1})-I_{E1}L( -\hat{g}_{1}^{\intercal}V_{C}/V_{T})=0,\] (55) \[-I_{C2}+I_{2}-R_{2}^{-1}V_{C2}+R_{12}^{-1}(V_{C1}-V_{C2})-I_{E2}L (-\hat{g}_{2}^{\intercal}V_{C}/V_{T})=0.\] (56)

Figure 7: Circuit schematic for the logistic regression posterior sampling device.

Next we define the conductance matrix

\[\mathcal{G}=\begin{pmatrix}R_{1}^{-1}+R_{12}^{-1}&-R_{12}^{-1}\\ -R_{12}^{-1}&R_{2}^{-1}+R_{12}^{-1},\end{pmatrix},\] (57)

allowing us to write a single vector equation

\[-I_{C}+I-\mathcal{G}V_{C}-I_{E}L(-\hat{g}^{\intercal}V_{C}/V_{T})=0,\] (58)

where we have also set \(\hat{g}_{1}=\hat{g}_{2}\). Now using the fact that \(dV_{C}/dt=C^{-1}I_{C}\), we have the following vector differential equation

\[C\frac{dV_{C}}{dt}=-\mathcal{G}V_{C}-I_{E}L(-\hat{g}^{\intercal}V_{C}/V_{T})+I.\] (59)

We assume the current vector \(I\) has a DC component \(I_{DC}\) and a noise component \(I_{\text{noise}}\). The noise component is assumed to be an ideal white noise process of infinite bandwidth and power spectral density \(S\), which we write \(I_{\text{noise}}=\sqrt{S}\xi(t)\). Altogether, we get the stochastic differential equation

\[dV_{C}=-C^{-1}\mathcal{G}V_{C}\,dt+C^{-1}I_{DC}\,dt-C^{-1}\alpha I_{E}L(\hat{g }^{\intercal}V_{C}/V_{T})\,dt+C^{-1}\sqrt{S}\xi(t)\,dt.\] (60)

Using the identity \(\xi(t)\,dt=\mathcal{N}[0,dt]\), this becomes

\[dV_{C}=-C^{-1}\mathcal{G}V_{C}\,dt+C^{-1}I_{DC}\,dt-C^{-1}\alpha I_{E}L(\hat{g }^{\intercal}V_{C}/V_{T})\,dt+C^{-1}\sqrt{S}\mathcal{N}[0,dt].\] (61)

At this point it is convenient to define dimensionless quantities which are mapped to the physical parameters of the circuit. Define \(\theta=V_{C}/\tilde{V}\), \(\Sigma^{-1}=\tilde{R}\mathcal{G}\), \(\Sigma^{-1}\mu=I_{DC}/\tilde{I}\), and \(yx=-I_{E}/\tilde{I}\). Our equation now takes the form

\[\tilde{V}dx=-\tilde{V}\tilde{R}^{-1}C^{-1}\Sigma^{-1}x\,dt+C^{-1}\tilde{I} \Sigma^{-1}\mu\,dt+C^{-1}\tilde{I}L(-\hat{g}^{\intercal}V_{C}/V_{T})yx\,dt+C^ {-1}\sqrt{S}\mathcal{N}[0,\mathbb{I}\,dt].\] (62)

Next, let \(\tau=\tilde{R}C\), and set \(\tilde{I}=\tilde{V}C/\tau\) and \(S=2\tilde{V}^{2}C^{2}/\tau\). In this case,

\[d\theta=-\Sigma^{-1}\theta\tau^{-1}dt+\Sigma^{-1}\mu\tau^{-1}dt+L(-\hat{g}^{ \intercal}V_{C}/V_{T})yx\tau^{-1}dt+\mathcal{N}[0,2\mathbb{I}\tau^{-1}dt].\] (63)

Finally, we set \(\hat{g}=yxV_{T}/\tilde{V}\) to obtain

\[d\theta=-\Sigma^{-1}(\theta-\mu)\tau^{-1}dt+L(-y\theta\tau)yx\tau^{-1}dt+ \mathcal{N}[0,2\mathbb{I}\tau^{-1}dt],\] (64)

which is identical to Eq. (15)

## Appendix C Analysis of complexity of Gaussian-Gaussian posterior sampling

In this section we analyze the time-complexity of sampling the Bayesian posterior of the Gaussian-Gaussian model using the device in Fig. 1. As shown in Appendix A, the SDE for this circuit is

\[d\theta=-\Sigma^{-1}(\theta-\mu)\tau^{-1}\,dt-\Sigma_{y|\theta}^{-1}(\theta- y)\tau^{-1}\,dt+\mathcal{N}[0,2\mathbb{I}\tau^{-1}dt],\] (65)

where \(\tau=L/\tilde{R}\). This SDE may also be written in terms of the posterior parameters,

\[d\theta=-\Sigma_{\theta|y}^{-1}(\theta-\mu_{\theta|y})\tau^{-1}\,dt+\mathcal{ N}[0,2\mathbb{I}\tau^{-1}dt],\] (66)

where

\[\mu_{\theta|y}=\mu+\Sigma\left(\Sigma+\Sigma_{y|\theta}\right)^{-1}(y-\mu),\] (67)

\[\Sigma_{\theta|y}=\Sigma-\Sigma(\Sigma+\Sigma_{\theta|y})^{-1}\Sigma.\] (68)

The above equation is in the form of a multivariate Ornstein-Uhlenbeck (OU) process [39].

The squared Wasserstein distance between the distribution at time \(t\) and the target posterior distribution is [40]

\[W(t)^{2}=\|\mu(t)-\mu_{\theta|y}\|_{2}^{2}+D(\Sigma(t),\Sigma_{\theta|y}),\] (69)

where

\[D(\Sigma_{1},\Sigma_{2})=\text{tr}\left\{\Sigma_{1}+\Sigma_{2}-2\left(\Sigma_ {2}^{1/2}\Sigma_{1}\Sigma_{2}^{1/2}\right)^{1/2}\right\}.\] (70)

[MISSING_PAGE_EMPTY:15]

The quantity \(\|\mu(0)-\mu_{\theta|y}\|\) may hide some dependence on dimension, which is discussed presently. It is assumed that the quantity \(c=\|\mu_{\theta|y}\|/\sqrt{\alpha_{\text{max}}}\) has an upper bound \(c_{\text{max}}\) which is independent of dimension, where \(\alpha_{\text{max}}\) is the largest eigenvalue of \(\Sigma_{\theta|y}\). That is, the mean of the posterior may be at most \(c_{\text{max}}\) standard deviations away from the origin, independent of dimension. This choice represents a particular scaling regime, which we feel is a realistic representation of the accuracy requirements for many applications. We may also choose \(\mu(0)=0\), and this leads to the requirement

\[t\geq\max\alpha_{\text{min}}^{-1}\tau\left(\ln\left[\sqrt{2}c\sqrt{\alpha_{ \text{max}}}W_{0}^{-1}\right],\frac{1}{2}\ln\left[\sqrt{2}\alpha_{\text{min}}^ {-3/2}\;d^{1/2}W_{0}^{-1}\right]\right).\] (86)

In general, the problem may be rescaled in such a way that \(\alpha_{\text{max}}\leq 1\), and some rescaling of this kind is realistic given that a particular device will have a specific signal range (that is, the range over which voltages and currents may vary). Redefining the problem this way will also cause the smallest eigenvalue of \(\Sigma_{\theta|y}\) to be reduced by a factor of \(\alpha_{\text{max}}\), and in this case the bound would be

\[t\geq\max\kappa\tau\left(\ln\left[\sqrt{2}c\,W_{0}^{-1}\right],\frac{1}{2}\ln \left[\sqrt{2}\kappa^{3/2}\;d^{1/2}W_{0}^{-1}\right]\right),\] (87)

where \(\kappa=\alpha_{\text{max}}/\alpha_{\text{min}}\) is the condition number. Subject to these assumptions, we may express the asymptotic time complexity as

\[t=O(\kappa\tau\ln(\kappa^{3/2}d^{1/2}W_{0}^{-1}))\] (88)

In order to collect \(N\) samples the same process is run \(N\) times, resulting in complexity

\[t=O(N\kappa\tau\ln(\kappa^{3/2}d^{1/2}W_{0}^{-1})).\] (89)

## Appendix D Conditioning on multiple I.I.D. samples

When conditioning on a single sample \(y\), the energy \(U\) can be separated into two terms, one mapping to the prior and the other to the likelihood:

\[U(r)=U_{\pi}(r)+U_{\ell}(r),\] (90)

where \(\beta U_{\pi}(r)=-\ln p_{\theta}(r/\tilde{r})\) and \(\beta U_{\ell}(r)=-\ln p_{y|\theta}(y|r/\tilde{r})\). In general we may have a number of I.I.D. samples \(Y=(y_{1},\ldots y_{N})\), and would like to sample from \(p_{\theta|Y}(\theta|Y)\). Because the samples of \(y\) are I.I.D., we have

\[p_{Y|\theta}(Y|\theta)=\prod_{i=1}^{N}p_{y|\theta}(y_{i}|\theta).\] (91)

In this case the likelihood part of the potential energy takes the form

\[\beta U_{\ell}(r)=-\sum_{i=1}^{N}\ln p_{y|\theta}(y_{i}|r/\tilde{r}),\] (92)

while the prior part is the same as in the single-sample case, \(\beta U_{\pi}(r)=-\ln p_{\theta}(r/\tilde{r})\). This form of the potential energy has a convenient physical interpretation: the function \(U_{\pi}\) can be interpreted as the self-energy of the system in state \(r\) (that is when it is decoupled from an external system), while the function \(U_{\ell}(\theta)\) can be viewed as an interaction energy between the state \(r\) and the state \(y\) of an external system. When there are multiple I.I.D. samples, this is analogous to the state \(r\) interacting with a collection of external systems in states \(Y=(y_{1}\ldots y_{N})\), and each such interaction contributes its own term to the interaction energy. This provides a framework for building a physical device to sample from the posterior conditioned on multiple I.I.D. samples; one must simply couple a collection of external systems in states \(Y=(y_{1}\ldots y_{N})\) to the system in such a way that each interaction contributes an energy of \(-\ln p_{y|\theta}(y|r/\tilde{r})\).

We will now describe another approach to building a physical device that samples from the posterior conditioned on multiple I.I.D. samples of \(y\). We first observe that the Langevin equation for the device in this case must be

\[d\theta=\nabla_{\theta}\ln p_{\theta}(\theta)\tau^{-1}\,dt+\sum_{i=1}^{N} \nabla_{\theta}\ln p_{y|\theta}(y_{i}|\theta)\,dt+\mathcal{N}[0,2\tau^{-1}dt],\] (93)As discussed above, if we have a device that can implement the \(N\) likelihood drift terms simultaneously then the problem his solved. However, suppose that we have a device that is only capable of implementing a single likelihood term at a time, but \(y\) may be varied as a function of time. Additionally, we make the interaction energy for this device larger by a factor of \(N\) for reasons that will become clear. That is, we have a device that implements an SDE of the form

\[d\theta=\nabla_{\theta}\ln p_{\theta}(\theta)\tau^{-1}\,dt+N\nabla_{\theta}\ln p _{y|\theta}(y(t)|\theta)\,dt+\mathcal{N}[0,2\tau^{-1}dt].\] (94)

We may choose a short time duration \(\Delta t\), and set

\[y(t)=y_{\lfloor t/\Delta t\rfloor\bmod N+1}.\] (95)

So for \(0\leq t\leq\Delta t\) we set \(y(t)=y_{1}\), for \(\Delta t<t\leq 2\Delta t\) we set \(y(t)=y_{2}\), and so on. Once \(t>N\Delta t\) we start over at \(y_{1}\) and continue cycling over all of the I.I.D. samples. Suppose that \(\Delta t\) is short enough that all of the samples are cycled over before the state \(\theta\) changes significantly. We may then average drift term \(N\nabla_{\theta}\ln p_{y|\theta}\) over a period of time \(N\Delta t\) and consider \(\theta\) constant within this average. Carrying out this time average, we find

\[\frac{1}{N\Delta t}\sum_{i=1}^{N}\Delta tN\nabla_{\theta}\ln p_{y|\theta}(y_{ i}|\theta)=\sum_{i=1}^{N}\nabla_{\theta}p_{y|\theta}(y_{i}|\theta),\] (96)

resulting in the correct form of the Langevin equation.

## Appendix E Computational complexity of logistic regression

The runtime complexity of digital Langevin sampling of a logistic regression model is \(O(n_{\delta t}dN)\), with \(n_{\delta t}\) the number of time steps, \(K\) the number of trainable parameters, and \(N\) the number of data points (in the case of minibatching \(b\) replaces \(N\). Added to this, there can be some discretization error if the step size is chosen too large, which generally means the number of time steps is made quite large to avoid this (meaning that \(n_{\delta t}\gg n_{s}\)). The memory complexity is that of storing the data and the samples, hence is \(O(dn_{s}+N)\). In contrast, running the thermodynamic logistic regression algorithm only includes two digital steps: i) pre-processing and sending over the data to the hardware and ii) initializing the system, which involves setting the prior distribution and the initial state. The gradient evaluations are all done in analog, which incurs a cost of \(O(t)\), with \(t\) the analog dynamics time. In the best case scenario, where we do not oversample correlated samples, we have \(t=n_{s}\tau_{c}\), with \(\tau_{c}\) the correlation time. The runtime complexity of the thermodynamic solver is therefore \(O(d+N+n_{s}\tau_{c})\), which is a large improvement over the digital case since there is no discretization factor and less multiplicative factors. In addition, note that \(t\) can be made extremely small (of the order of the microsecond) in practice thanks to the value of the physical time constants of electronic systems.1