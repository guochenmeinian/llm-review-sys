# How to Turn Your Knowledge Graph Embeddings into Generative Models

 Lorenzo Locente

University of Edinburgh, UK

l.loconte@sms.ed.ac.uk

&Nicola Di Mauro

University of Bari, Italy

nicola.dimauro@uniba.it

Robert Peharz

TU Graz, Austria

robert.peharz@tugraz.at

&Antonio Vergari

University of Edinburgh, UK

avergari@ed.ac.uk

###### Abstract

Some of the most successful knowledge graph embedding (KGE) models for link prediction - CP, RESCAL, TuckER, ComplEx - can be interpreted as energy-based models. Under this perspective they are not amenable for exact maximum-likelihood estimation (MLE), sampling and struggle to integrate logical constraints. This work re-interprets the score functions of these KGEs as _circuits_ - constrained computational graphs allowing efficient marginalisation. Then, we design two recipes to obtain efficient generative circuit models by either restricting their activations to be non-negative or squaring their outputs. Our interpretation comes with little or no loss of performance for link prediction, while the circuits framework unlocks exact learning by MLE, efficient sampling of new triples, and guarantee that logical constraints are satisfied by design. Furthermore, our models scale more gracefully than the original KGEs on graphs with millions of entities.

## 1 Introduction

Knowledge graphs (KGs) are a popular way to represent structured domain information as directed graphs encoded as collections of triples (subject, predicate, object), where subjects and objects (entities) are nodes in the graph, and predicates their edge labels. For example, the information that the drug "loxoprofen" interacts with the protein "COX2" is represented as a triple (loxoprofen, interacts, COX2) in the biomedical KG gobl-bioklg [32]. As real-world KGs are often incomplete, we are interested in performing reasoning tasks over them while dealing with missing information. The simplest reasoning task is _link prediction_[48], i.e., querying for the entities that are connected in a KG by an edge labelled with a certain predicate. For instance, we can retrieve all proteins that the drug "loxoprofen" interacts with by asking the query (loxoprofen, interacts,?).

Knowledge graph embedding (KGE) models are state-of-the-art (SOTA) models for link prediction [40, 56, 12] that map entities and predicates to sub-symbolic representations, which are used to assign a real-valued degree of existence to triples in order to rank them. For example, the SOTA KGE model ComplEx [65] assigns (loxoprofen, interacts, phosphoric-acid) and (loxoprofen, interacts, COX2) scores 2.3 and 1.3, hence ranking the first higher than the second in our link prediction example.

This simple example, however, also highlights some opportunities that are missed by KGE models. First, triple scores cannot be directly compared across different queries and across different KGE models, as they can be seen as _negative energies_ and not _normalised probabilities over triples_[41, 7, 6, 27, 43]. To establish a sound probabilistic interpretation and therefore have _probabilities instead of scores_ that can be easily interpreted and compared [79], we would need to compute the normalisationconstant (or partition function), which is impractical for real-world KGs due to their considerable size (see Section2). Therefore, learning KGE models by maximum-likelihood estimation (MLE) would be computationally infeasible, which is the canonical probabilistic way to learn _a generative model over triples_. A generative model would enable us to sample new triples efficiently, e.g., to generate a surrogate KG whose statistics are consistent with the original one or to do data augmentation [9]. Furthermore, traditional KGE models do not provide a principled way to _guarantee the satisfaction of hard constraints_, which are crucial to ensure trustworthy predictions in safety-critical contexts such as biomedical applications. The result is that predictions of these models can blatantly violate simple constraints such as KG schema definitions. For instance, the triple that the SOTA CompLex ranks higher in our example above violates the semantics of "interacts", i.e., such predicate can only hold between drugs (e.g., loxoprofen) and proteins (e.g., COX2) but phosphoric-acid is not a protein.

Contributions.We show that KGE models that have become a de facto standard, such as CompLex and alternatives based on multilinear score functions [47; 40; 66], can be represented as structured computational graphs, named _circuits_[13]. Under this light, **i)** we propose a different interpretation of these computational graphs and their parameters, to retrieve efficient and yet expressive probabilistic models over triples in a KG, which we name _generative KGE circuits_ (GeKCs) (Section4). We show that **ii)** not only GeKCs can be efficiently trained by exact MLE, but learning them with widely-used discriminative objectives [37; 40; 56; 12] also scales far better over large KGs with millions of entities. In addition, **iii)** we are able to sample new triples exactly and efficiently from GeKCs, and propose a novel metric to measure their quality (Section7.3). Furthermore, by leveraging recent theoretical advances in representing circuits [76], **iv)** we guarantee that predictions at test time will never violate logical constraints such as domain schema definitions by design (Section5). Finally, our experimental results show that these advantages come with no or minimal loss in terms of link prediction accuracy.

## 2 From KGs and embedding models...

KGs and embedding models.A KG \(\mathcal{G}\) is a directed multi-graph where nodes are entities and edges are labelled with predicates, i.e., elements of two sets \(\mathcal{E}\) and \(\mathcal{R}\), respectively. We define \(\mathcal{G}\) as a collection of triples \((s,r,o)\subseteq\mathcal{E}\times\mathcal{R}\times\mathcal{E}\), where \(s\), \(r\), \(o\) denote the subject, predicate and object, respectively. A _KG embedding_ (KGE) model maps a triple \((s,r,o)\) to a real scalar via a _score function_\(\phi\colon\mathcal{E}\times\mathcal{R}\times\mathcal{E}\to\mathbb{R}\). A common recipe to construct differentiable score functions for many state-of-the-art KGE models [56] is to (i) map entities and predicates to _embeddings_ of rank \(d\), i.e., elements of normed vector spaces (e.g., \(\mathbb{R}^{d}\)), and (ii) combine the embeddings of subject, predicate and object via multilinear maps. This is the case for KGE models such as CP [40], RESCAL [47], TuckER [3], and CompLex [65] (see Fig.2). For instance, the score function of CompLex[65] is defined as \(\phi_{\textsc{CompLex}}(s,r,o)=\operatorname{Re}(\langle\mathbf{e}_{s},\mathbf{ w}_{r},\overline{\mathbf{e}_{o}}\rangle)\) where \(\mathbf{e}_{s},\mathbf{w}_{r},\mathbf{e}_{o}\in\mathbb{C}^{d}\) are the complex embeddings of subject, predicate and object, \(\langle\cdot,\ \cdot,\ \cdot\rangle\) denotes a trilinear product, \(\overline{\cdot}\) denotes the complex conjugate operator and \(\operatorname{Re}(\cdot)\) the real part of complex numbers.

Probabilistic loss-derived interpretation.KGE models have been traditionally interpreted as _energy-based models_ (EBMs) [41; 7; 6; 27; 43]: their score function is assumed to compute the negative energy of a triple. This interpretation induces a distribution over possible KGs by associating a Bernoulli variable, whose parameter is determined by the score function, to every triple [48]. Learning EBMs under this perspective requires using contrastive objectives [7; 48; 56], but several recent works observed that to achieve SOTA link prediction results one needs only to predict

Figure 1: **Which KGE models can be used as efficient generative models of triples?** The score functions of popular KGE models such as CompLex, CP, RESCAL and TuckER can be easily represented as circuits (lilac). However, to retrieve a valid probabilistic circuit (PC, in orange) that encodes a probability distribution over triples (GeKCs) we need to either _restrict its activations to be non-negative_ (in blue, see Section4.1) or _square it_ (in red, see Section4.2).

subjects, objects [37, 40, 56] or more recently also predicates of triples [12], i.e., to treat KGEs as _discriminative_ classifiers. Specifically, they are learned by minimising a categorical cross-entropy, e.g., by maximising \(\log p(o\mid s,r)=\phi(s,r,o)-\log\sum_{o^{\prime}\in\mathcal{E}}\exp\phi(s,r, o^{\prime})\) for object prediction. From this perspective, we observe that we can recover an energy-based interpretation if we assume there exist a joint probability distribution \(p\) over three variables \(S,R,O\), denoting respectively subjects, predicates and objects. Written as a Boltzmann distribution, we have that \(p(S=s,R=r,O=o)=(\exp\phi(s,r,o))/Z\), where \(Z=\sum_{s^{\prime}\in\mathcal{E}}\sum_{r^{\prime}\in\mathcal{R}}\sum_{o^{ \prime}\in\mathcal{E}}\exp\phi(s^{\prime},r^{\prime},o^{\prime})\) denotes the partition function. This interpretation is apparently in contrast with the traditional one over possible KGs [48]. We reconcile it with ours in Appendix E. Under this view, we can reinterpret and generalise the recently introduced discriminative objectives [12] as a weighted _pseudo-log-likelihood_ (PLL) [70]

\[\mathcal{L}_{\text{PLL}}:=\sum\nolimits_{(s,r,o)\in\mathcal{G}}\omega_{s} \log p(s\mid r,o)+\omega_{o}\log p(o\mid s,r)+\omega_{r}\log p(r\mid s,o)\] (1)

where \(\omega_{s},\omega_{o},\omega_{r}\in\mathbb{R}_{+}\) can differently weigh each term, which is a conditional log-probability that can be computed by summing out either \(s\), \(r\) or \(o\), e.g., to compute \(\log p(o\mid s,r)\) above. Optimisation is usually carried out by mini-batch gradient ascent [56] and, given a batch of triples \(B\subset\mathcal{G}\), we have that exactly computing the PLL objective requires time \(\mathcal{O}(|\mathcal{E}|\cdot|B|\cdot\operatorname{cost}(\phi))\) and space \(\mathcal{O}(|\mathcal{E}|\cdot|B|)\) to exploit GPU parallelism [36],1 where \(\operatorname{cost}(\phi)\) denotes the complexity of evaluating the \(\phi\) once.

Footnote 1: For large real-world KGs it is reasonable to assume that \(|\mathcal{E}|\gg|\mathcal{R}|\).

Note that the PLL objective (Eq. (1)) is a traditional proxy for learning _generative_ models for which it is infeasible to evaluate the _maximum-likelihood estimation_ (MLE) objective2

Footnote 2: In general, the PLL is recognised as a proxy for MLE because, under certain assumptions, it is possible to show it can retrieve the MLE solution asymptotically with enough data [35].

\[\mathcal{L}_{\text{MLE}}:=\sum\nolimits_{(s,r,o)\in\mathcal{G}}\log p(s,r,o)=- |\mathcal{G}|\log Z+\sum\nolimits_{(s,r,o)\in\mathcal{G}}\phi(s,r,o).\] (2)

In theory, evaluating \(\log p(s,r,o)\) exactly can be done in polynomial time under our three-variable interpretation, as computing \(Z\) requires \(\mathcal{O}(|\mathcal{E}|^{2}\cdot|\mathcal{R}|\cdot\operatorname{cost}(\phi))\) time, but in practice this cost is still prohibitive for real-world KGs. In fact, it would require summing over \(|\mathcal{E}\times\mathcal{R}\times\mathcal{E}|\) evaluations of the score function \(\phi\), which for FB15k-237 [62], the small fragment of Freebase [48], translates to ~\(10^{11}\) evaluations of \(\phi\). This practical bottleneck hinders the generative capabilities of these models and their ability to yield normalised and interpretable probabilities. Next, we show how we can reinterpret KGE score functions as to retrieve a generative model over triples for which computing \(Z\) exactly can be done in time \(\mathcal{O}((|\mathcal{E}|+|\mathcal{R}|)\cdot\operatorname{cost}(\phi))\), making renormalisation feasible.

## 3...to Circuits...

In this section, we show that popular and successful KGE models such as CP, RESCAL, TuckER and ComplEx (see Fig. 2 and Section 2), can be viewed as structured computational graphs that can, in principle, enable summing over all possible triples efficiently. Later, to exploit this efficient summation for marginalisation over triple probabilities, we reinterpret the semantics of these computational graphs as to yield circuits that output valid probabilities. We start with the needed background about circuits and show that some score functions can be readily represented as circuits.

**Definition 1** (Circuit [13, 76]).: A _circuit_\(\phi\) is a parametrized computational graph over variables \(\mathbf{X}\) encoding a function \(\phi(\mathbf{X})\) and comprising three kinds of computational units: _input_, _product_, and _sum_. Each product or sum unit \(n\) receives as inputs the outputs of other units, denoted with the set \(\mathsf{in}(n)\). Each unit \(n\) encodes a function \(\phi_{n}\) defined as: (i) \(l_{n}(\mathsf{sc}(n))\) if \(n\) is an input unit, where \(l_{n}\) is a function over variables \(\mathsf{sc}(n)\subseteq\mathbf{X}\), called its _scope_, (ii) \(\prod_{i\in(n)}\phi_{i}(\mathsf{sc}(\phi_{i}))\) if \(n\) is a product unit, and (iii) \(\sum_{i\in\mathsf{in}(n)}\theta_{i}\phi_{i}(\mathsf{sc}(\phi_{i}))\) if \(n\) is a sum unit, with \(\theta_{i}\in\mathbb{R}\) denoting the weighted sum parameters. The scope of a product or sum unit \(n\) is the union of the scopes of its inputs, i.e., \(\mathsf{sc}(n)=\bigcup_{i\in\mathsf{in}(n)}\mathsf{sc}(i)\).

Fig. 2 and Fig. A.1 show examples of circuits. Next, we introduce the two structural properties that enable efficient summation and Prop. 1 certifies that the aforementioned KGEs have these properties.

**Definition 2** (Smoothness and Decomposability).: A circuit is _smooth_ if for every sum unit \(n\), its input units depend all on the same variables, i.e, \(\forall i,j\in\mathsf{in}(n)\colon\mathsf{sc}(i)=\mathsf{sc}(j)\). A circuit is _decomposable_ if the inputs of every product unit \(n\) depend on disjoint sets of variables, i.e, \(\forall i,j\in\mathsf{in}(n)\i\neq j\colon\mathsf{sc}(i)\cap\mathsf{sc}(j)=\varnothing\).

**Proposition 1** (Score functions of KGE models as circuits).: The computational graphs of the score functions \(\phi\) of CP, RESCAL, TuckER and ComplEx are smooth and decomposable circuits over \(\mathbf{X}=\{S,R,O\}\), whose evaluation cost is \(\mathrm{cost}(\phi)\in\Theta(|\phi|)\), where \(|\phi|\) denotes the number of edges in the circuit, also called its size. For example, the size of the circuit for CP is \(|\phi_{\mathrm{CP}}|\in\mathcal{O}(d)\).

Appendix A.1 reports the complete proof by construction for the score functions of these models and the circuit sizes, while Fig. 2 illustrates them. Intuitively, since the score functions of the cited KGE models are based on products and sums as operators, they can be represented as circuits where input units map entities and predicates into the corresponding embedding components (similarly to look-up tables). As the inputs of each sum unit are product units that share the same scope \(\{S,R,O\}\) and fully decompose it in their input units, they satisfy smoothness and decomposability (Def. 2).

Smooth and decomposable circuits enable summing over all possible (partial) assignments to \(\mathbf{X}\) by (i) performing summations at input units over values in their domains, and (ii) evaluating the circuit once in a feed-forward way [13; 76]. This re-interpretation of score functions allows to "push" summations to the input units of a circuit, greatly reducing complexity, as detailed in the following proposition.

**Proposition 2** (Efficient summations).: Let \(\phi\) be a smooth and decomposable circuit over \(\mathbf{X}=\{S,R,O\}\) that encodes the score function of a KGE model. The sum \(\sum_{s\in\mathcal{E}}\sum_{r\in\mathcal{R}}\sum_{o\in\mathcal{E}}\phi(s,r,o)\) or any other summation over subjects, predicates or objects can be computed in time \(\mathcal{O}((|\mathcal{E}|+|\mathcal{R}|)\cdot|\phi|)\).

However, _these sums are in logarithm space_, as we have that \(p(s,r,o)\propto\exp\phi(s,r,o)\) (see Section 2). As a consequence, summation in this context _does not_ correspond to marginalising variables in probability space. This drives us to reinterpret the semantics of these circuit structures as to operate directly in probability space, rather than in logarithm space, i.e., by encoding non-negative functions.

## 4...to Probabilistic Circuits

We now present how to reinterpret the semantics of the computational graphs of KGE score functions to directly output non-negative values for any input. That is, we cast them as _probabilistic circuits_ (PCs) [13; 76; 19]. First, we define our subclass of PCs that encodes a possibly unnormalised probability distribution over triples in a KG, but allows for efficient marginalisation.

**Definition 3** (Generative KGE circuit).: A _generative KGE circuit_ (GeKC) is a smooth and decomposable PC \(\phi_{\mathsf{pc}}\) over variables \(\mathbf{X}=\{S,R,O\}\) that encodes a probability distribution over triples, i.e., \(\phi_{\mathsf{pc}}(s,r,o)\propto p(s,r,o)\) for any \((s,r,o)\in\mathcal{E}\times\mathcal{R}\times\mathcal{E}\).

Since our GeKCs are smooth and decomposable (Def. 2) they guarantee the efficient computation of \(Z\) in time \(\mathcal{O}((|\mathcal{E}|+|\mathcal{R}|)\cdot|\phi_{\mathsf{pc}}|)\) (Prop. 2). This is in contrast with existing KGE models, for which it would require the evaluation of the whole score function on each possible triple (Section 2). For example, assume a non-negative CP score function \(\phi^{+}_{\mathsf{CP}}(s,r,o)=\langle\mathbf{e}_{s},\mathbf{w}_{r},\mathbf{e}_ {o}\rangle\in\mathbb{R}_{+}\) for some embeddings \(\mathbf{e}_{s},\mathbf{w}_{r},\mathbf{e}_{o}\in\mathbb{R}^{d}\). Then, we can compute \(Z\) by pushing the outer summations inside the trilinear product, i.e., \(Z=\langle\sum_{s\in\mathcal{E}}\mathbf{e}_{s},\sum_{r\in\mathcal{R}}\mathbf{w} _{r},\sum_{o\in\mathcal{E}}\mathbf{e}_{o}\rangle\), which can be done in time \(\mathcal{O}((|\mathcal{E}|+|\mathcal{R}|)\cdot d)\). In the following sections, we propose two ways to turn the computational graphs of CP, RESCAL, TuckER and ComplEx into GeKCs without additional space requirements.

Figure 2: **Interpreting the score functions of CP, RESCAL, TuckER, ComplEx as circuits over 2-dimensional embeddings. Input, product and sum units are coloured in purple, orange and blue, respectively. Output sum units are labelled with the score functions, and their parameters are assumed to be \(1\), if not specified. The detailed construction is presented in Appendix A.1. Given a triple \((s,r,o)\), the input units map subject \(s\), predicate \(r\) and object \(o\) to their embedding entries. Then, the products are evaluated before the weighted sum, which outputs the score of the input triple.**

### Non-negative restriction of a score function

The most natural way of casting existing KGE models to GeKCs is by constraining the computational units of their circuit structures (Section3) to output non-negative values only. We will refer to this conversion method as the _non-negative restriction_ of a model. To achieve the non-negative restriction of CP, RESCAL and TuckER we can simply restrict the embedding values and additional parameters in their score functions to be non-negative, as products and sums are operators closed in \(\mathbb{R}_{+}\). Thus, each input unit \(n\) over variables \(S\) or \(O\) (resp. \(R\)) in a non-negative GeKC encodes a function \(l_{n}\) (Def.1) modelling an unnormalised categorical distribution over entities (resp. predicates). For example, each \(i\)-th entry \(e_{si}\) of the embedding \(\bm{e}_{s}\in\mathbb{R}_{+}^{d}\) associated to an entity \(s\in\mathcal{E}\) becomes a parameter of the \(i\)-th unnormalised categorical distribution over \(S\). See Fig.C.1 for an example. We denote the non-negative restriction of these KGEs as CP\({}^{+}\), RESCAL\({}^{+}\) and TuckER\({}^{+}\), respectively.

However, deriving CompLEX\({}^{+}\)[65] by restricting the embedding values of CompLex to be non-negative is not sufficient, because its score function includes a subtraction as it operates on complex numbers. To overcome this, we re-parameterise the imaginary part of each complex embedding to be always greater than or equal to its real part. AppendixC.2 details this procedure. Even if this reparametrisation allows for more flexibility, imposing non-negativity on GeKCs can restrict their ability to capture intricate interactions over subjects, predicates and objects given a fixed number of learnable parameters [67]. We empirically confirm this in our experiments in Section7. Therefore, we propose an alternative way of representing KGEs as PCs via _squaring_.

### Squaring the score function

Squaring works by taking the score function of a KGE model \(\phi\), and multiplying it with itself to obtain \(\phi^{2}=\phi\cdot\phi\). Note that \(\phi^{2}\) would be guaranteed to be a PC, as it always outputs non-negative values. The challenge is to represent the product of two circuits as a smooth and decomposable PC as to guarantee efficient marginalisation (Prop.2).3 In general, this task is known to be #P-hard [76].

Footnote 3: In fact, even though \(\phi^{2}\) can be easily built by introducing a product unit over two copies of \(\phi\) as sub-circuits, the resulting circuit would be not decomposable (Def.2), as the sub-circuits are defined on the same scope.

However, it can be done efficiently if the two circuits are _compatible_[76], as we further detail in AppendixB.1. Intuitively, the circuit representations of the score functions \(\phi\) of CP, RESCAL, TuckER and CompLex (see Fig.2) are simple enough that every product unit is defined over the same scope \(\{S,R,O\}\) and fully decomposes it on its input units. As such, these circuits can be easily multiplied with any other smooth and decomposable circuit, a property also known as _omni-compatibility_[76]. This property enables us to build the squared version of these KGE models, which we denote as CP\({}^{2}\), RESCAL\({}^{2}\), TuckER\({}^{2}\) and CompLex\({}^{2}\), as PCs that are still smooth and decomposable. Note that these squared GeKCs do allow for negative parameters, and hence can be more expressive. The next theorem, instead, guarantees that we can normalize them efficiently.

**Theorem 1** (Efficient summations on squared GeKCs).: Performing summations as stated in Prop.2 on CP\({}^{2}\), RESCAL\({}^{2}\), TuckER\({}^{2}\) and CompLex\({}^{2}\) can be done in time \(\mathcal{O}((|\mathcal{E}|+|\mathcal{R}|)\cdot|\phi|^{2})\).

For instance, the partition function \(Z\) of CP\({}^{2}\) with embedding size \(d\) would require \(\mathcal{O}((|\mathcal{E}|+|\mathcal{R}|)\cdot d^{2})\) operations to be computed, while a simple feed-forward pass for a batch of \(|B|\) triples is still \(\mathcal{O}(|B|\cdot d)\). While in this case marginalisation requires an increase in complexity that is quadratic in \(d\), it is still faster than the brute force approach to compute \(Z\) (see Section2 and AppendixC.4.1).

Figure 3: **GeKCs scale better. Time (in seconds) and peak GPU memory (in GiB as bubble sizes) required for computing the PLL objective and back-propagating through it for a single batch on ogbl-wikikg2, by increasing the batch size and number of entities. See AppendixC.4.3 for details.**Quickly distilling KGEs to squared GeKCs.Consider a squared GeKC obtained by initialising its parameters with those of its energy-based KGE counterpart. If the score function of the original KGE model _always_ assigns non-negative scores to triples, then the "distilled" squared GeKC will output the _same exact ranking of the original model for the answers to any link prediction queries_. Although the premise of the non-negativity of the scores might not be guaranteed, we observe that, in practice, learned KGE models do assign positive scores to all or most of the triples of common KGs (see Appendix D). Therefore, we can use this observation to either instantly distil a GeKC or provide a good heuristic to initialise its parameters and fine-tune them (by MLE or PLL maximisation). In both cases, the result is that we can convert the original KGE model into a GeKC that provides comparable probabilities, enable efficient marginalisation, sampling, and the integration of logical constraints with little or no loss of performance for link prediction (Section 7.1).

### On the Training Efficiency of GeKCs

GeKCs also offer an unexpected opportunity to better scale the computation of the PLL objective (Eq. (1)) on very large knowledge graphs. This is because computing the PLL for a batch of \(|B|\) triples with GeKCs obtained via non-negative restriction and by squaring (Sections 4.1 and 4.2) does not require storing a matrix of size \(\mathcal{O}(|\mathcal{E}|\cdot|B|)\) to fully exploit GPU parallelism [36]. For instance, in Appendix C.4.2 we show that computing the PLL for CP [40] with embedding size \(d\) requires time \(\mathcal{O}(|\mathcal{E}|\cdot|B|\cdot d)\) and additional space \(\mathcal{O}(|\mathcal{E}|\cdot|B|)\). On the other hand, for CP\({}^{2}\) (resp. CP\({}^{+}\)) it requires time \(\mathcal{O}((|\mathcal{E}|+|B|)\cdot d^{2})\) (resp. \(\mathcal{O}((|\mathcal{E}|+|B|)\cdot d)\)) and space \(\mathcal{O}(|B|\cdot d)\). Table C.1 summarises similar reduced complexities for other instances of GeKCs, such as ComplEx\({}^{+}\) and ComplEx\({}^{2}\). The reduced time and memory requirements with GeKCs allow us to use larger batch sizes and better scale to large knowledge graphs. Fig. 3 clearly highlights this when measuring the time and GPU memory required to train these models on a KG with millions of entities such as ogbl-wikkg2 [32].

### Sampling new triples with GeKCs

GeKCs only allowing non-negative parameters, such as CP\({}^{+}\), RESCAL\({}^{+}\) and TuckER\({}^{+}\), support _ancestral sampling_ as sum units can be interpreted as marginalised discrete latent variables, similarly to the latent variable interpretation in mixture models [55; 53] (see Appendix C.3 for details). This is however not possible in general for ComplEx\({}^{+}\) and GeKCs obtained by squaring, as negative parameters break this interpretation. Luckily, as these circuits still support efficient marginalisation (Prop. 2 and Thm. 1) and hence also conditioning, we can perform _inverse transform sampling_. That is, to generate a triple \((s,r,o)\), we can sample in an autoregressive fashion, e.g., first \(s\sim p(S)\), then \(r\sim p(R\mid s)\) and \(o\sim p(O\mid s,r)\), hence requiring only three marginalization steps.

## 5 Injection of Logical Constraints

Converting KGE models to PCs provides the opportunity to "embed" logical constraints in the neural link predictor such that (i) predictions are always guaranteed to satisfy the constraints at test time, and (ii) training can still be done by efficient MLE (or PLL). This is in stark contrast with previous approaches for KGEs, which relax the constraints or enforce them only at training time (see Section 6). Consider, as an example, the problem of integrating the logical constraints induced by a schema of a KG, i.e., enforcing that triples not satisfying a _domain constraint_ have probability zero.

**Definition 4** (Domain constraint).: Given a predicate \(r\in\mathcal{R}\) and \(\kappa_{S}(r),\kappa_{O}(r)\subseteq\mathcal{E}\) the sets of all subjects and objects that are semantically coherent with respect to \(r\), a _domain constraint_ is a propositional logic formula defined as

\[K_{r}\equiv S\in\kappa_{S}(r)\wedge R=r\wedge O\in\kappa_{O}(r)\equiv(\lor_{u \in\kappa_{S}(r)}S=u)\wedge R=r\wedge(\lor_{v\in\kappa_{O}(r)}O=v).\] (3)

Given \(\mathcal{R}=\{r_{1},\ldots,r_{m}\}\) a set of predicates, the disjunction \(K\equiv K_{r_{1}}\vee\ldots\lor K_{r_{m}}\) encodes all the domain constraints that are defined in a KG. An input triple \((s,r,o)\) satisfies \(K\), written as \((s,r,o)\models K\), if \(s\in\kappa_{S}(r)\) and \(o\in\kappa_{O}(r)\). To design GeKCs such as their predictions always satisfy logical constraints (which might not be necessarily domain constraints), we follow Ahmed et al. [1] and define a score function to represent a probability distribution \(p_{K}\) that assigns probability mass only to triples that satisfy the constraint \(K\), i.e., \(\phi_{\mathsf{pc}}(s,r,o)\cdot c_{K}(s,r,o)\propto p_{K}(s,r,o)\). Here, \(\phi_{\mathsf{pc}}\) is a GeKC and \(c_{K}(s,r,o)=\mathds{1}\{(s,r,o)\models K\}\) is an indicator function that ensures that zero mass is assigned to triples violating \(K\). In words, we are "cutting" the support of \(\phi_{\mathsf{pc}}\), as illustrated in Fig. 4.

Computing \(p_{K}(s,r,o)\) exactly but naively would require computing a new partition function \(Z_{K}=\sum_{s^{\prime}\in\mathcal{E}}\sum_{r^{\prime}\in\mathcal{E}}\sum_{o^{ \prime}\in\mathcal{E}}\sum_{o^{\prime}\in\mathcal{E}}(\phi_{\mathsf{pc}}(s^{ \prime},r^{\prime},o^{\prime})\cdot c_{K}(s^{\prime},r^{\prime},o^{\prime}))\), which is impractical as previously discussed (Section 2). Instead, we compile \(c_{K}\) as a smooth and decomposable circuit, sometimes called a constraint or logical circuit [19; 1], by leveraging compilers from the _knowledge compilation_ literature [52; 18]. In a nutshell, \(c_{K}\) is another circuit over variables \(S,R,O\) that outputs 1 if an input triple satisfies the encoded logical constraint \(K\) and 0 otherwise. See Def. A.2 for a formal definition of such circuits. Then, similarly to what we have showed for computing squared circuits that enable efficient marginalisation (Section 4.2), the satisfaction of compatibility between a GeKC \(\phi_{\mathsf{pc}}\) and a constraint circuit \(c_{K}\) enable us to compute \(Z_{K}\) efficiently, as certified by the following theorem.

**Theorem 2** (Tractable integration of constraints in GeKCs).: Let \(c_{K}\) be a constraint circuit encoding a logical constraint \(K\) over variables \(\{S,R,O\}\). Then exactly computing the partition function \(Z_{K}\) of the product \(\phi_{\mathsf{pc}}(s,r,o)\cdot c_{K}(s,r,o)\propto p_{K}(s,r,o)\) for any GeKC \(\phi_{\mathsf{pc}}\) derived from CP, RESCAL, TuckER or CompLex (Section 4) can be done in time \(\mathcal{O}((|\mathcal{E}|+|\mathcal{R}|)\cdot|\phi_{\mathsf{pc}}|\cdot|c_{K}|)\).

In Prop. A.1 we show that the compilation of domain constraints \(K\) (Def. 4) is straightforward and results in a constraint circuit \(c_{K}\) having compact size. For example, the size of the constraint circuit encoding the domain constraints of ogl-biokq is approximately \(|c_{K}|=307\cdot 10^{3}\). To put this number in perspective, the size of the circuit for CompLex with embedding size \(1000\) is the much larger \(375\cdot 10^{6}\). Since \(|c_{K}|\) is much smaller, by the same argument on the efficiency of GeKCs obtained via squaring (Section 4.3) it results that the integration of logical constraints adds a negligible overhead.

## 6 Related Work

SOTA KGEs and current limitations.A plethora of ways to represent and learn KGEs has been proposed, see [31] for a review. KGEs such as CP and CompLex are still the de facto go-to choices in many applications [5; 12; 56]. Works performing density estimation in embedding space [78; 11] can sample embeddings, but to sample triples one would need to train a decoder. Several works try to modify training for KGEs as to introduce a penalty for triples that do not satisfy given logical constraints [8; 39; 34; 42; 23; 30], or casting it as a min-max game [44]. Unlike our GeKCs (Section 5), none of these approaches guarantee that test-time predictions satisfy the constraints. Moreover, several heuristics have been proposed to calibrate the probabilistic predictions of KGE models ex-post [61; 79]. As showed in [64], the triple distribution can be modelled autoregressively as \(p(S,R,O)=p(S)\cdot p(O\mid S)\cdot p(R\mid S,O)\) where each conditional distribution is encoded by a neural network. However, differently from our GeKCs, integrating constraints exactly or computing _any_ marginal (thus conditional) probability is inefficient. KGE models based on non-negative tensor decompositions [63] are equivalent to GeKCs obtained by non-negative restriction (Section 4.1), but are generally trained by minimizing different non-probabilistic losses.

Circuits.Circuits provide a unifying framework for several tractable probabilistic models such as sum-product networks (SPNs) and hidden Markov models, which are smooth and decomposable

Figure 4: **Injection of domain constraints. Given a circuit \(c_{K}\) encoding domain constraints and a GeKC \(\phi_{\mathsf{pc}}\), the probability assigned by the product circuit \(\phi_{\mathsf{pc}}\cdot c_{K}\) to the inconsistent triple showed in Section 1 is 0, and a positive probability is assigned to consistent triples only, e.g., for the interacts predicate those involving drugs (\(\mathsf{Ds}\)) as subjects and proteins (\(\mathsf{Ps}\)) as objects. Best viewed in colours.**

PCs [13], as well as compact logical representations [19; 1]. See [75; 13; 17] for an overview. PCs with negative parameters are also called non-monotonic [58], but are surprisingly not as well investigated as their monotonic counterparts, i.e., PCs with only non-negative parameters, at least from the learning perspective. Similarly to our construction for ComplEx+ (Appendix C.2), Dennis [20] constrains the output of the non-monotonic sub-circuits of a larger PC to be less than their monotonic counterparts. Squaring a circuit has been investigating for tractably computing several divergences [76] and is related to the Born-rule of quantum mechanics [51].

Footnote 4: Code is available at https://github.com/april-tools/gekcs.

Circuits for relational data.Logical circuits to compile formulas in first-order logic (FOL) [25] have been used to reason over relational data, e.g. via exchangeability [68; 49]. Other formalisms such as tractable Markov Logic [77], probabilistic logic bases [50], relational SPNs [46; 45] and generative clausal networks [71] use underlying circuit-like structures to represent probabilistic models over a tractable fragment of FOL formulas. These works assume that every atom in a grounded formula is associated to a random variable, also called the possible world semantics in probabilistic logic programs [57] and databases (PDBs) [14]. In this semantics, TractOR [26] casts answering complex queries over KGEs as to performing inference in PDBs. Differently from these works, our GeKCs are models defined over only three variables (Section 2). In Appendix E we reconcile these two semantics by interpreting the probability of a triple to be proportional to that of all KGs containing it.

## 7 Empirical Evaluation

We aim to answer the following research questions: **RQ1**) are GeKCs competitive with commonly used KGEs for link prediction? **RQ2**) Does integrating domain constraints in GeKCs benefit training and prediction?; **RQ3**) how good are the triples sampled from GeKCs?

### Link Prediction (RQ1)

Experimental setting.We evaluate GeKCs on standard KG benchmarks for link prediction5: FB15k-237 [62], WN18RR [21] and ogbl-biokg [32], whose statistics can be found in Appendix F.1. As usual [48; 56; 54], we assess the models for predicting objects (queries \((s,r,?)\)) and subjects (queries \((?,r,o)\)), and report their _mean reciprocal rank_ (MRR) and _fraction of hits at \(k\)_ (Hits@\(k\)) (see Appendix F.2). We remark that our aim in this Section is _not to score the new state-of-the-art link prediction performance on these benchmarks_. Instead, we aim to rigorously assess how close GeKCs can be to commonly used and reasonably tuned KGE models. We focus on CP and ComplEx as they currently are the go-to models of choice for link prediction [40; 56; 12]. We compare them against our GeKCs CP\({}^{+}\), ComplEx\({}^{+}\), CP\({}^{2}\) and ComplEx\({}^{2}\) (Section 4). Appendix F.4 collects all the details about the model hyperparameters and training for reproducibility.

Footnote 5: Across non-ensemble methods and according to the OGB leaderboard, updated at the time of this writing.

Link prediction results.Table 1 reports the MRR and times for all benchmarks and models when trained by PLL or MLE. First, CP\({}^{2}\) and ComplEx\({}^{2}\) achieve competitive scores when compared to CP and ComplEx. Moreover, CP\({}^{2}\) (resp. ComplEx\({}^{2}\)) always outperforms CP\({}^{+}\) (resp. ComplEx\({}^{+}\)), thus providing empirical evidence that negative embedding values are crucial for model expressiveness. Concerning times, Table F.2 shows that squared GeKCs can train much faster on large KGs (see Section 4.3): CP\({}^{2}\) and ComplEx\({}^{2}\) require less than half the training time of CP and ComplEx on ogbl-biokg, while also unexpectedly scoring the current SOTA MRR on it.6 We experiment also on the much larger ogbl-wikikg2 KG [32], comprising millions of entities. Even more remarkably, we are

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline \multirow{2}{*}{**Model**} & \multicolumn{2}{c}{FB15k-237} & \multicolumn{2}{c}{WN18RR} & \multicolumn{2}{c}{ogbl-biokg} \\ \cline{2-7}  & PLL & MLE & PLL & MLE & PLL & MLE \\ \hline CP & 0.310 & — & **0.105** & — & 0.831 & — \\ CP\({}^{+}\) & 0.237 & 0.230 & 0.027 & 0.026 & 0.496 & 0.501 \\ CP\({}^{2}\) & **0.315** & 0.282 & **0.104** & 0.091 & **0.848** & 0.829 \\ \hline ComplEx & **0.342** & — & **0.471** & — & 0.829 & — \\ ComplEx\({}^{+}\) & 0.214 & 0.205 & 0.030 & 0.029 & 0.503 & 0.516 \\ ComplEx\({}^{2}\) & 0.334 & 0.300 & 0.420 & 0.391 & **0.858** & 0.840 \\ \hline \hline \end{tabular}
\end{table}
Table 1: **GeKCs are competitive with their energy-based counterparts. Best average test MRRs of CP, ComplEx and GeKCs trained with the PLL and MLE objectives (Eqs. (1) and (2)). For standard deviations and training times see Table F.2.**able to score an MRR of 0.572 after just ~3 hours with ComplEx2 trained by PLL with a batch size of \(10^{4}\) and embedding size \(d=100\). To put this in context, we were able to score 0.562 with the best configuration of ComplEx_but after ~3 days_, as we could not fit in memory more than a batch size 500.6 The same trends are shown for the Hits@\(k\) (Table F.3) and likelihood (Table F.4) metrics.

Footnote 6: A smaller (\(d=50\)) and highly tuned version of ComplEx achieves 0.639 MRR but still after days [12].

Distilling GeKCs.Table F.5 reports the results achieved by CP2 and ComplEx2 initialised with the parameters of learned CP and ComplEx (see Section 4.2) and confirms we can quickly turn an EBM into GeKC, thus inheriting all the perks of being a tractable generative model.

Calibration study.We also measure how well calibrated the predictions of the models in Table 1 are, which is essential to ensure trustworthiness in critical tasks. For example, given a perfectly calibrated model, for all the triples predicted with a probability of 80%, exactly 80% of them would actually exist [79]. On all KGs but WN18RR, GeKCs achieve lower empirical calibration errors [29] and better calibrated curves than their counterparts, as we report in F.5.3. The worse performance of all models on WN18RR can be explained by the distribution shift that exists between its training and test split, which we better confirm in Section 7.3.

### Integrating Domain Constraints (RQ2)

We focus on ogbl-biokg[32], as it contains the domain metadata for each entity (i.e., disease, drug, function, protein, or side effect). Given the entity domains allowed for each predicate, we formulate domain constraints as in Def. 4. First, we want to estimate how likely are the models to predict triples that do not satisfy the domain constraints. We focus on ComplEx and ComplEx2, as they have been shown to achieve the best results in Section 7.1 and introduce d-ComplEx2 as the constraints-aware version of ComplEx2 (Section 5). For each test query \((s,r,?)\) (resp. \((?,r,o)\)), we compute the Sem@\(k\) score [33] as the average percentage of triples in the first \(k\) positions of the rankings of potential object (resp. subject) completions that satisfy the domain constraints (see Section F.2).

Footnote 2: A smaller (\(d=50\)) and highly tuned version of ComplEx achieves 0.639 MRR but still after days [12].

Fig. 5 highlights how both ComplEx and ComplEx2 systematically predict object (or subject) completions that violate domain constraints even for large embedding sizes. For instance, a Sem@1 score of 99% (resp. 99.9%) means that ~3200 (resp. ~320) predicted test triples violate domain constraints. While for ComplEx and ComplEx2 there is no theoretical guarantee of consistent predictions with respect to the domain constraints, d-ComplEx2 always guarantee consistent predictions _by design_. Furthermore, we observe a significant improvement in terms of MRR when integrating constraints for smaller embedding sizes, as reported in Fig. 5.

### Quality of sampled triples (RQ3)

Inspired by the literature on evaluating deep generative models for images, we propose a metric akin to the _kernel Inception distance_[4] to evaluate the quality of the triples we can sample with GeKCs.

**Definition 5** (Kernel triple distance (KTD)).: Given \(\mathbb{P},\mathbb{Q}\) two probability distributions over triples, and a positive definite kernel \(k\colon\mathbb{R}^{h}\times\mathbb{R}^{h}\to\mathbb{R}\), we define the _kernel triple distance_\(\operatorname{KTD}(\mathbb{P},\mathbb{Q})\) as

Figure 5: **GeKCs with domain constraints guarantee domain-consistent predictions. Semantic consistency scores (Sem@\(k\)) [33] on ogbl-biokg achieved by ComplEx, ComplEx2 and its integration with domain constraints (d-ComplEx2) (left), and MRRs computed on test queries (right). ComplEx infers 200+ triples violating constraints as the highest scoring completions (\(k=1\)).**

the squared _maximum mean discrepancy_[28] between triple latent representations obtained via a map \(\psi\colon\mathcal{E}\times\mathcal{R}\times\mathcal{E}\to\mathbb{R}^{h}\) that projects triples to an \(h\)-dimensional embedding, i.e.,

\[\mathrm{KTD}(\mathbb{P},\mathbb{Q})=\mathbb{E}_{x,x^{\prime}\sim\mathbb{P}}[k( \psi(x),\psi(x^{\prime}))]+\mathbb{E}_{y,y^{\prime}\sim\mathbb{Q}}[k(\psi(y), \psi(y^{\prime}))]-2\cdot\mathbb{E}_{x\sim\mathbb{P},y\sim\mathbb{Q}}[k(\psi(x),\psi(y))].\]

An empirical estimate of the KTD score close to zero indicates that there is little difference between the two triple distributions \(\mathbb{P}\) and \(\mathbb{Q}\) (see Appendix F.3). For images, \(\psi\) is typically chosen as the last embedding of a SOTA neural classifier. We choose \(\psi\) to be the \(L_{2}\)-normed outputs of the product units of a circuit [73, 74], specifically the SOTA ComplEx learned by Chen et al. [12] with \(h=4000\). We choose \(k\) as the polynomial kernel \(k(\mathbf{x},\mathbf{y})=(\mathbf{x}^{\top}\mathbf{y}+1)^{3}\), following Binkowski et al. [4].

Table 2 shows the empirical KTD scores computed between the test triples and the generated ones, and Fig. F.1 visualises triple embeddings. We employ two baselines: a uniform probability distribution over all possible triples and NNMFAug [9], the only work to address triple sampling to the best of our knowledge. We also report the KTD scores for training triples as an empirical lower bound. Squared GeKCs achieve lower KTD scores with respect to the ones obtained by non-negative restriction, confirming again a better estimation of the joint distribution. In addition, they achieve far lower KTD scores than all competitors when learning by MLE (Eq. (2)), which justifies its usage as an objective. Lastly, we confirm the distribution shift on WN18RR: training set KTD scores are far from zero, but even in this challenging scenario, ComplEx\({}^{2}\) scores KTD values that are closer to the training ones.

## 8 Conclusions and Future Work

We proposed to re-interpret the representation and learning of widely used KGE models such as CP, RESCAL, TuckER and ComplEx, as generative models, overcoming some of the classical limitation of their usual EBM interpretation (see Sections 1 and 2). GeKC-variants for other KGE models whose scores are multilinear maps can be readily devised in the same way. Moreover, we conjecture that other KGE models defining score functions having a distance-based semantics such as TransE [7] and RotatE [60] can be reinterpreted to be GeKCs as well. Our GeKCs open up a number of interesting future directions. First, we plan to investigate how the enhanced efficiency and calibration of GeKCs can help in complex reasoning tasks beyond link prediction [2]. Second, we can leverage the rich literature on learning the structure of circuits [72, 75] to devise smaller and sparser KGE circuit architectures that better capture the triple distribution or sporting structural properties that can make reasoning tasks other than marginalisation efficient [22, 16, 71].

## Acknowledgments and Disclosure of Funding

We would like to acknowledge Iain Murray for his thoughtful feedback and suggestions on a draft of this work. In addition, we thank Xiong Bo and Ricky Zhu for pointing out related works in the knowledge graphs literature. AV was supported by the "UNREAL: Unified Reasoning Layer for Trustworthy ML" project (EP/Y023838/1) selected by the ERC and funded by UKRI EPSRC. RP was supported by the Graz Center for Machine Learning (GraML).

## References

* [1] Kareem Ahmed, Stefano Teso, Kai-Wei Chang, Guy Van den Broeck, and Antonio Vergari. Semantic probabilistic layers for neuro-symbolic learning. In _Advances in Neural Information

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Model** & \multicolumn{2}{c}{FB15k-237} & \multicolumn{2}{c}{WN18RR} & \multicolumn{2}{c}{ogbl-biokq} \\ \cline{2-6} Training set & \multicolumn{2}{c}{0.055} & \multicolumn{2}{c}{0.260} & \multicolumn{2}{c}{0.029} \\ Uniform & \multicolumn{2}{c}{0.589} & \multicolumn{2}{c}{0.766} & \multicolumn{2}{c}{1.822} \\ NNMFAug & \multicolumn{2}{c}{0.414} & \multicolumn{2}{c}{0.607} & \multicolumn{2}{c}{0.518} \\ \hline  & PLL & MLE & PLL & MLE & PLL & MLE \\ \cline{2-6} CP\({}^{+}\) & 0.404 & 0.433 & 0.633 & **0.578** & 0.966 & 0.738 \\ CP\({}^{2}\) & 0.253 & **0.070** & 0.768 & 0.768 & 0.039 & **0.017** \\ \hline ComplEx\({}^{\mathrm{t}}\) & 0.336 & 0.323 & 0.456 & 0.478 & 0.175 & 0.097 \\ ComplEx\({}^{2}\) & 0.326 & **0.102** & 0.338 & **0.278** & 0.104 & **0.034** \\ \hline \hline \end{tabular}
\end{table}
Table 2: **GeKCs trained by MLE generate new likely triples. Empirical KTD scores between test triples and triples generated by baselines and GeKCs trained with the PLL objective or by MLE (Eqs. (1) and (2)). Lower is better. For standard deviations see Table F.6.**Processing Systems 35 (NeurIPS)_, volume 35, pages 29944-29959. Curran Associates, Inc., 2022.
* [2] Erik Arakelyan, Daniel Daza, Pasquale Minervini, and Michael Cochez. Complex query answering with neural link predictors. In _ICLR_. OpenReview.net, 2021.
* [3] Ivana Balazevic, Carl Allen, and Timothy M. Hospedales. Tucker: Tensor factorization for knowledge graph completion. In _EMNLP-IJCNLP_, pages 5184-5193. Association for Computational Linguistics, 2019.
* [4] Mikolaj Binkowski, Danica J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD gans. In _ICLR (Poster)_. OpenReview.net, 2018.
* [5] Stephen Bonner, Ian P Barrett, Cheng Ye, Rowan Swiers, Ola Engkvist, Charles Tapley Hoyt, and William L Hamilton. Understanding the performance of knowledge graph embeddings in drug discovery. _Artificial Intelligence in the Life Sciences_, 2:100036, 2022.
* [6] Antoine Bordes, Jason Weston, Ronan Collobert, and Yoshua Bengio. Learning structured embeddings of knowledge bases. _Proceedings of the AAAI Conference on Artificial Intelligence_, 2011.
* [7] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. In _Neural Information Processing Systems (NIPS)_, pages 2787-2795, 2013.
* [8] Kai-Wei Chang, Wen-tau Yih, Bishan Yang, and Christopher Meek. Typed tensor decomposition of knowledge bases for relation extraction. In _EMNLP_, pages 1568-1579. ACL, 2014.
* [9] Jatin Chauhan, Priyanshu Gupta, and Pasquale Minervini. A probabilistic framework for knowledge graph data augmentation. _arXiv preprint arXiv:2110.13205_, 2021.
* [10] Mark Chavira and Adnan Darwiche. On probabilistic inference by weighted model counting. _Artificial Intelligence._, 172(6-7):772-799, 2008.
* [11] Xuelu Chen, Michael Boratko, Muhao Chen, Shib Sankar Dasgupta, Xiang Lorraine Li, and Andrew McCallum. Probabilistic box embeddings for uncertain knowledge graph reasoning. _arXiv preprint arXiv:2104.04597_, 2021.
* [12] Yihong Chen, Pasquale Minervini, Sebastian Riedel, and Pontus Stenetorp. Relation prediction as an auxiliary training objective for improving multi-relational graph representations. _CoRR_, abs/2110.02834, 2021.
* [13] YooJung Choi, Antonio Vergari, and Guy Van den Broeck. Probabilistic circuits: A unifying framework for tractable probabilistic modeling. 2020.
* [14] Nilesh Dalvi and Dan Suciu. Efficient query evaluation on probabilistic databases. _The VLDB Journal_, 16(4):523-544, 2007.
* [15] Nilesh N. Dalvi and Dan Suciu. The dichotomy of probabilistic inference for unions of conjunctive queries. _ACM_, 59(6):30:1-30:87, 2012.
* [16] Meihua Dang, Antonio Vergari, and Guy Broeck. Strudel: Learning structured-decomposable probabilistic circuits. In _International Conference on Probabilistic Graphical Models_, pages 137-148. PMLR, 2020.
* [17] Adnan Darwiche. _Modeling and Reasoning with Bayesian Networks_. Cambridge University Press, 2009.
* [18] Adnan Darwiche. SDD: A new canonical representation of propositional knowledge bases. In _Twenty-Second International Joint Conference on Artificial Intelligence_, 2011.
* [19] Adnan Darwiche and Pierre Marquis. A knowledge compilation map. _Journal of Artificial Intelligence Research (JAIR)_, 17:229-264, 2002.

* [20] Aaron W. Dennis. _Algorithms for Learning the Structure of Monotone and Nonmonotone Sum-Product Networks_. PhD thesis, Brigham Young University, 2016.
* [21] Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2d knowledge graph embeddings. In _AAAI_, pages 1811-1818. AAAI Press, 2018.
* [22] Nicola Di Mauro, Antonio Vergari, Teresa M. A. Basile, and Floriana Esposito. Fast and accurate density estimation with extremely randomized cutset networks. In _Machine Learning and Knowledge Discovery in Databases: ECML PKDD_, pages 203-219. Springer, 2017.
* [23] Boyang Ding, Quan Wang, Bin Wang, and Li Guo. Improving knowledge graph embedding using simple constraints. In _Annual Meeting of the Association for Computational Linguistics_, 2018.
* [24] Leslie H. Fenton. The sum of log-normal probability distributions in scatter transmission systems. _IEEE Transactions on Communications_, 8:57-67, 1960.
* [25] Daan Fierens, Guy Van den Broeck, Joris Renkens, Dimitar Shterionov, Bernd Gutmann, Ingo Thon, Gerda Janssens, and Luc De Raedt. Inference and learning in probabilistic logic programs using weighted boolean formulas. _Theory and Practice of Logic Programming_, 15(3):358-401, 2015.
* [26] Tal Friedman and Guy Van den Broeck. Symbolic querying of vector spaces: Probabilistic databases meets relational embeddings. In _UAI_, volume 124, pages 1268-1277. AUAI Press, 2020.
* [27] Xavier Glorot, Antoine Bordes, Jason Weston, and Yoshua Bengio. A semantic matching energy function for learning with multi-relational data. _Machine Learning_, 94:233-259, 2013.
* [28] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and Alexander J. Smola. A kernel two-sample test. _J. Mach. Learn. Res._, 13:723-773, 2012.
* [29] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In _International conference on machine learning_, pages 1321-1330. PMLR, 2017.
* [30] Shu Guo, Lin Li, Zhen Hui, Lingshuai Meng, Bingnan Ma, Wei Liu, Lihong Wang, Haibin Zhai, and Hong Zhang. Knowledge graph embedding preserving soft logical regularity. _Proceedings of the 29th ACM International Conference on Information & Knowledge Management_, 2020.
* [31] Aidan Hogan, Eva Blomqvist, Michael Cochez, Claudia d'Amato, Gerard de Melo, Claudio Gutierrez, Sabrina Kirrane, Jose Emilio Labra Gayo, Roberto Navigli, Sebastian Neumaier, et al. Knowledge graphs. _ACM Computing Surveys (CSUR)_, 54(4):1-37, 2021.
* [32] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. _arXiv preprint arXiv:2005.00687_, 2020.
* [33] Nicolas Hubert, Pierre Monnin, Armelle Brun, and Davy Monticolo. New strategies for learning knowledge graph embeddings: The recommendation case. In _EKAW_, volume 13514 of _Lecture Notes in Computer Science_, pages 66-80. Springer, 2022.
* [34] Nicolas Hubert, Pierre Monnin, Armelle Brun, and Davy Monticolo. Enhancing knowledge graph embedding models with semantic-driven loss functions, 2023.
* [35] Aapo Hyvarinen. Consistency of pseudolikelihood estimation of fully visible boltzmann machines. _Neural Computation_, 18:2283-2292, 2006.
* [36] Prachi Jain, Sushant Rathi, Mausam, and Soumen Chakrabarti. Knowledge base completion: Baseline strikes back (again). _ArXiv_, abs/2005.00804, 2020.
* [37] Armand Joulin, Edouard Grave, Piotr Bojanowski, Maximilian Nickel, and Tomas Mikolov. Fast linear model for knowledge graph embeddings. _ArXiv_, abs/1710.10881, 2017.
* [38] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _3rd International Conference on Learning Representations (ICLR)_, 2015.

* [39] Denis Krompass, Stephan Baier, and Volker Tresp. Type-constrained representation learning in knowledge graphs. In _ISWC (1)_, volume 9366 of _Lecture Notes in Computer Science_, pages 640-655. Springer, 2015.
* [40] Timothee Lacroix, Nicolas Usunier, and Guillaume Obozinski. Canonical tensor decomposition for knowledge base completion. In _ICML_, volume 80 of _Proceedings of Machine Learning Research_, pages 2869-2878. PMLR, 2018.
* [41] Yann LeCun, Sumit Chopra, Raia Hadsell, Marc'Aurelio Ranzato, and Fujie Huang. A tutorial on energy-based learning. _Predicting Structured Data_, 2006.
* [42] Pasquale Minervini, Claudia d'Amato, Nicola Fanizzi, and Floriana Esposito. Leveraging the schema in latent factor models for knowledge graph completion. In _SAC_, pages 327-332. ACM, 2016.
* [43] Pasquale Minervini, Claudia d'Amato, and Nicola Fanizzi. Efficient energy-based embedding models for link prediction in knowledge graphs. _Journal of Intelligent Information Systems_, 47:91-109, 2016.
* [44] Pasquale Minervini, Thomas Demeester, Tim Rocktaschel, and Sebastian Riedel. Adversarial sets for regularising neural link predictors. _arXiv preprint arXiv:1707.07596_, 2017.
* [45] Aniruddh Nath and Pedro Domingos. Learning relational sum-product networks. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 29, 2015.
* [46] Aniruddh Nath and Pedro M. Domingos. Learning relational sum-product networks. In _AAAI_, pages 2878-2886. AAAI Press, 2015.
* [47] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective learning on multi-relational data. In _ICML_, pages 809-816. Omnipress, 2011.
* [48] Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. A review of relational machine learning for knowledge graphs. _IEEE_, 104(1):11-33, 2016.
* [49] Mathias Niepert and Guy Van den Broeck. Tractability through exchangeability: A new perspective on efficient probabilistic inference. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 2467-2475. AAAI Press, 2014.
* [50] Mathias Niepert and Pedro M Domingos. Learning and inference in tractable probabilistic knowledge bases. In _UAI_, pages 632-641, 2015.
* [51] Georgii S. Novikov, Maxim E. Panov, and Ivan V. Oseledets. Tensor-train density estimation. In _37th Conference on Uncertainty in Artificial Intelligence (UAI)_, volume 161 of _Proceedings of Machine Learning Research_, pages 1321-1331. PMLR, 2021.
* [52] Umut Oztok and Adnan Darwiche. A top-down compiler for sentential decision diagrams. In _IJCAI_, pages 3141-3148. AAAI Press, 2015.
* [53] Robert Peharz, Robert Gens, Franz Pernkopf, and Pedro M. Domingos. On the latent variable interpretation in sum-product networks. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 39(10):2030-2044, 2017.
* [54] Pouya Pezeshkpour, Yifan Tian, and Sameer Singh. Revisiting evaluation of knowledge base completion models. In _AKBC_, 2020.
* [55] Hoifung Poon and Pedro Domingos. Sum-product networks: A new deep architecture. In _IEEE International Conference on Computer Vision Workshops (ICCV Workshops)_, pages 689-690. IEEE, 2011.
* [56] Daniel Ruffinelli, Samuel Broscheit, and Rainer Gemulla. You CAN teach an old dog new tricks! on training knowledge graph embeddings. In _ICLR_. OpenReview.net, 2020.
* [57] Taisuke Sato and Yoshitaka Kameya. Prism: a language for symbolic-statistical modeling. In _IJCAI_, volume 97, pages 1330-1339. Citeseer, 1997.

* [58] Amir Shpilka, Amir Yehudayoff, et al. Arithmetic circuits: A survey of recent results and open questions. _Foundations and Trends(r) in Theoretical Computer Science_, 5(3-4):207-388, 2010.
* [59] Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. Reasoning with neural tensor networks for knowledge base completion. In _Advances in Neural Information Processing Systems 26_, 2013.
* [60] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. RotatE: Knowledge graph embedding by relational rotation in complex space. In _ICLR (Poster)_. OpenReview.net, 2019.
* [61] Pedro Tabacof and Luca Costabello. Probability calibration for knowledge graph embedding models. _ArXiv_, abs/1912.10000, 2019.
* [62] Kristina Toutanova and Danqi Chen. Observed versus latent features for knowledge base and text inference. In _3rd Workshop on Continuous Vector Space Models and Their Compositionality_. ACL, 2015.
* [63] Volker Tresp, Cristobal Esteban, Yinchong Yang, Stephan Baier, and Denis Krompass. Learning with memory embeddings. _ArXiv_, abs/1511.07972, 2015.
* [64] Volker Tresp, Sahand Sharifzadeh, Hang Li, Dario Konopatzki, and Yunpu Ma. The tensor brain: A unified theory of perception, memory, and semantic decoding. _Neural Computation_, 35:156-227, 2021.
* [65] Theo Trouillon, Johannes Welbl, Sebastian Riedel, Eric Gaussier, and Guillaume Bouchard. Complex embeddings for simple link prediction. In _ICML_, volume 48 of _JMLR Workshop and Conference Proceedings_, pages 2071-2080. JMLR.org, 2016.
* [66] L. R. Tucker. The extension of factor analysis to three-dimensional matrices. In _Contributions to mathematical psychology._, pages 110-127. Holt, Rinehart and Winston, 1964.
* [67] Leslie G. Valiant. Negation can be exponentially powerful. In _11th Annual ACM Symposium on Theory of Computing_, pages 189-196, 1979.
* [68] Guy Van den Broeck, Nima Taghipour, Wannes Meert, Jesse Davis, and Luc De Raedt. Lifted probabilistic inference by first-order knowledge compilation. In _IJCAI_, pages 2178-2185. IJCAI/AAAI, 2011.
* [69] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. _Journal of Machine Learning Research_, 9(86):2579-2605, 2008.
* [70] Cristiano Varin, Nancy Reid, and David Firth. An overview of composite likelihood methods. _Statistica Sinica_, 21, 01 2011.
* [71] Fabrizio Ventola, Devendra Singh Dhami, and Kristian Kersting. Generative clausal networks: Relational decision trees as probabilistic circuits. In _Inductive Logic Programming: 30th International Conference, ILP 2021, Virtual Event, October 25-27, 2021, Proceedings_, pages 251-265. Springer, 2022.
* [72] Antonio Vergari, Nicola Di Mauro, and Floriana Esposito. Simplifying, regularizing and strengthening sum-product network structure learning. In _Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2015, Porto, Portugal, September 7-11, 2015, Proceedings, Part II 15_, pages 343-358. Springer, 2015.
* [73] Antonio Vergari, Robert Peharz, Nicola Di Mauro, Alejandro Molina, Kristian Kersting, and Floriana Esposito. Sum-product autoencoding: Encoding and decoding representations using sum-product networks. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 32, 2018.
* [74] Antonio Vergari, Nicola Di Mauro, and Floriana Esposito. Visualizing and understanding sum-product networks. _Machine Learning_, 108(4):551-573, 2019.
* [75] Antonio Vergari, Nicola Di Mauro, and Guy Van den Broeck. Tractable probabilistic models: Representations, algorithms, learning, and applications. _Tutorial at the 35th Conference on Uncertainty in Artificial Intelligence (UAI)_, 2019.

* [76] Antonio Vergari, YooJung Choi, Anji Liu, Stefano Teso, and Guy Van den Broeck. A compositional atlas of tractable circuit operations for probabilistic inference. In _Advances in Neural Information Processing Systems 34 (NeurIPS)_, pages 13189-13201. Curran Associates, Inc., 2021.
* [77] W Austin Webb and Pedro Domingos. Tractable probabilistic knowledge bases with existence uncertainty. In _Workshops at the Twenty-Seventh AAAI Conference on Artificial Intelligence_, 2013.
* [78] Han Xiao, Minlie Huang, and Xiaoyan Zhu. Transg : A generative model for knowledge graph embedding. In _Annual Meeting of the Association for Computational Linguistics_, 2016.
* [79] Ruiqi Zhu, Fangrong Wang, Alan Bundy, Xue Li, Kwabena Nuamah, Lei Xu, Stefano Mauceri, and Jeff Z. Pan. A closer look at probability calibration of knowledge graph embedding. In _11th International Joint Conference on Knowledge Graphs_, page 104-109, 2023.

## Appendix A Proofs

### KGE Models as Circuits

**Proposition 1** (Score functions of KGE models as circuits).: The computational graphs of the score functions \(\phi\) of CP, RESCAL, TuckER and ComplEx are smooth and decomposable circuits over \(\mathbf{X}=\{S,R,O\}\), whose evaluation cost is \(\operatorname{cost}(\phi)\in\Theta(|\phi|)\), where \(|\phi|\) denotes the number of edges in the circuit, also called its size. For example, the size of the circuit for CP is \(|\phi_{\text{CP}}|\in\mathcal{O}(d)\).

Proof.: We present the proof by construction for TuckER [3], as CP [40], RESCAL [47] and ComplEx [65] define score functions that are a specialisation of it [3] (see below). Given a triple \((s,r,o)\in\mathcal{E}\times\mathcal{R}\times\mathcal{E}\), the TuckER score function computes

\[\phi_{\text{TuckER}}(s,r,o)=\mathcal{T}\times_{1}\mathbf{e}_{s}\times_{2} \mathbf{w}_{r}\times_{3}\mathbf{e}_{o}=\sum\nolimits_{i=1}^{d_{e}}\sum \nolimits_{j=1}^{d_{e}}\sum\nolimits_{k=1}^{d_{e}}\tau_{ijk}e_{si}w_{rj}e_{ok}\] (4)

where \(\mathcal{T}\in\mathbb{R}^{d_{e}\times d_{r}\times d_{e}}\) denotes the core tensor, \(\times_{n}\) denotes the tensor product along the \(n\)-th mode, and \(d_{e},d_{r}\) denote respectively the embedding sizes of entities and predicates (which might not be equal). To see how this parametrization generalises that of CP, RESCAL and ComplEx, consider for example the score function of CP on \(d\)-dimensional embeddings. It can be obtained by (i) setting the core tensor \(\mathcal{T}\) to be a _diagonal tensor_ having ones on the superdiagonal, and (ii) having two distinct embedding instances for each entity that are used depending on their role (either subject or object) in a triple. The embeddings \(\mathbf{e}_{s},\mathbf{e}_{o}\in\mathbb{R}^{d_{e}}\) (resp. \(\mathbf{w}_{r}\in\mathbb{R}^{d_{r}}\)) are rows of the matrix \(\mathbf{E}\in\mathbb{R}^{|\mathcal{E}|\times d_{e}}\) (resp. \(\mathbf{W}\in\mathbb{R}^{|\mathcal{R}|\times d_{r}}\)), which associates an embedding to each entity (resp. predicate).

Constructing the circuit.For the construction of the equivalent circuit it suffices to (i) create an input unit for each \(i\)-th entry of an embedding for subjects, predicates and objects, as to implement a look-up table that computes the corresponding embedding value for an entity or predicate, and (ii) transform the tensor multiplications into corresponding sum and product units. We start by introducing the input units \(l_{i}^{S}\), \(l_{j}^{R}\) and \(l_{k}^{O}\) for \(1\leq i,k\leq d_{e}\) and \(1\leq j\leq d_{r}\) as parametric mappers over variables \(S\), \(R\) and \(O\), respectively. The input units \(l_{i}^{S}\) and \(l_{i}^{Q}\) (resp. \(l_{j}^{R}\)) are parameterised by the matrix \(\mathbf{E}\) (resp. \(\mathbf{W}\)) such that \(l_{i}^{S}(s;\mathbf{E})=e_{si}\) and \(l_{i}^{O}(o;\mathbf{E})=e_{oi}\) for some \(s,o\in\mathcal{E}\) (resp. \(l_{j}^{R}(r;\mathbf{W})=w_{ri}\) for some \(r\in\mathcal{R}\)). To encode the tensor products in Eq. (4) we introduce \(d_{e}^{2}\cdot d_{r}\) product units \(\phi_{ijk}\), each of them computing the product of a combination of the outputs of input units.

\[\phi_{ijk}(s,r,o)=l_{i}^{S}(s)\cdot l_{j}^{R}(r)\cdot l_{k}^{O}(o)\]

Finally, a sum unit \(\phi_{\text{out}}\) parameterised by the core tensor \(\mathcal{T}\in\mathbb{R}^{d_{e}\times d_{r}\times d_{r}}\) computes a weighted summation of the outputs given by the product units, i.e.,

\[\phi_{\text{out}}(s,r,o)=\sum_{\begin{subarray}{c}(i,j,k)\in\\ [d_{e}]\times[d_{r}]\times[d_{e}]\end{subarray}}\tau_{ijk}\cdot\phi_{ijk}(s,r,o)\]

Figure A.1: **Evaluation of circuit representations of score functions as in neural networks.** Feed-forward evaluation of the TuckER score function as a circuit over 2-dimensional embeddings and parameterised by the core tensor \(\mathcal{T}\) (see proof of Prop. 1 below) (a). Given a triple (loxoprofen, interacts, COX2), the input units (Def. 1) map subject, predicate and object to their embedding entries (in violet boxes). Then, the circuit is evaluated similarly to neural networks: the products (in orange) are evaluated before the weighted sum (in blue), which is parameterised by the core tensor values (in green) (b). The output of the circuit is the score of the input triple.

where \([d]\) denotes the set \(\{1,\ldots,d\}\) and \(\tau_{ijk}\) is the \((i,j,k)\)-th entry of \(\mathcal{T}\). We now observe that the constructed circuit \(\phi_{\text{out}}\) encodes the TuckER score function (Eq. (4)), as \(\phi_{\text{TuckER}}(s,r,o)=\phi_{\text{out}}(s,r,o)\) for any input triple \((s,r,o)\in\mathcal{E}\times\mathcal{R}\times\mathcal{E}\).

Circuit evaluation and properties.Evaluating the score function of TuckER corresponds to performing a feed-forward pass of its circuit representation, where each computational unit is evaluated once, as we illustrate in Fig. A.1. As such, the cost of evaluating the score function is proportional to the size of its circuit representation, i.e., \(\operatorname{cost}(\phi)\in\Theta(|\phi|)\) where \(|\phi|\in\mathcal{O}(d_{e}^{2}\cdot d_{r})\) is the number of edges. In Table A.1 we show how the sizes of the circuit representation of the other score functions increases with respect to the embedding size. Finally, since each product unit \(\phi_{ijk}\) is defined on the same scope (see Def. 1) \(\{S,R,O\}\) and fully decompose it into its inputs (i.e., into \(\{S\},\{R\},\{O\}\)), and the inputs of the sum unit \(\phi_{\text{out}}\) are all defined over the same scope, we have that the circuit satisfies smoothness and decomposability (Def. 2). 

Furthermore, in Lem. A.1 we show that the circuit representations of CP, RESCAL, TuckER and CompLex and the proposed GeKCs (Section 4) satisfy a structural property known as _omni-compatibility_ (see Def. B.2). In a nutshell, the score functions of the aforementioned KGE models and GeKCs are circuits that fully decompose their scope \(\{S,R,O\}\) into \((\{S\},\{R\},\{O\})\). The satisfaction of this property will be useful to prove both Thm. 1 and Thm. 2 later in this appendix.

**Lemma A.1** (KGE models and derived GeKCs are omni-compatible).: The circuit representation of the score functions of CP, RESCAL, TuckER, CompLex and their GeKCs counterparts obtained by non-negative restriction (Section 4.1) or squaring (Section 4.2) are omni-compatible (see Def. B.2).

Proof.: To begin, we note that to comply with Def. B.2 every omni-compatible circuit shall contain product units that fully factorise over their scope. In other words, for every product unit \(n\) with scope \(\text{sc}(n)=\mathbf{X}\), its scope shall decompose as \((\{X_{1}\},\{X_{2}\},\ldots,\{X_{|\text{sc}(n)|}\})\). To see why, consider a circuit \(\phi\) with a product unit \(n\) whose scope is decomposed as \(\text{sc}(n)=(\mathbf{X},\mathbf{Y})\). It is easy to construct another circuit \(\phi^{\prime}\) that is not compatible with \(\phi\) by having a product unit \(m\) with scope \(\text{sc}(m)=\text{sc}(n)\) decomposed in a way that it cannot be rearranged by introducing additional decomposable product units (Def. 2), e.g., \(\text{sc}(m)=(\mathbf{Z},\mathbf{W})\) with \(\mathbf{Z}\cap\mathbf{X}\neq\varnothing\) and \(\mathbf{W}\cap\mathbf{X}\neq\varnothing\). As such, every omni-compatible circuit over \(\mathbf{X}\) must be representable in the form \(\sum_{i=1}^{N}\theta_{i}\prod_{k=1}^{|\mathbf{X}|}l_{ik}(X_{k})\) without any increase in its size.

Now, it is easy to verify that the circuit representations of CP, RESCAL, TuckER and CompLex follow the above form, with a different number \(N\) of product units feeding the single sum unit, but each one decomposing its scope \(\{S,R,O\}\) into \((\{S\},\{R\},\{O\})\) (see Fig. 2). From this it follows that CP\({}^{+}\), RESCAL\({}^{+}\), TuckER\({}^{+}\) and CompLex\({}^{+}\) are omni-compatible as well, as they share the same structure of their energy-based counterpart, while just enforcing non-negative activations via reparametrisation (see Section 4.1).

Finally, we note that CP\({}^{2}\), RESCAL\({}^{2}\), TuckER\({}^{2}\) and CompLex\({}^{2}\) are still omni-compatible because the square operation yields the following fully-factorised representation: \((\sum_{i=1}^{N}\theta_{i}\prod_{k=1}^{|\mathbf{X}|}l_{ik}(X_{k}))^{2}=\sum_{i= 1}^{N}\sum_{j=1}^{N}\theta_{i}\theta_{j}\prod_{k=1}^{|\mathbf{X}|}l_{ik}(X_{k} )\prod_{k=1}^{|\mathbf{X}|}l_{jk}(X_{k})\) which can be easily rewritten as \(\sum_{h=1}^{N^{2}}\omega_{h}\prod_{k=1}^{|\mathbf{X}|}l_{hk}(X_{k})\) where now \(h\) ranges over the Cartesian product of \(i\in[N]\) and \(j\in[N]\), \(\omega_{h}\) is the product of \(\theta_{i}\theta_{j}\) and \(l_{hk}\) is a new input unit that encodes \(l_{ik}(X_{k})l_{jk}(X_{k})\) for a certain variable index \(k\).

### Efficient Summations over Circuits

**Proposition 2** (Efficient Summations).: Let \(\phi\) be a smooth and decomposable circuit over \(\mathbf{X}=\{S,R,O\}\) that encodes the score function of a KGE model. The sum \(\sum_{s\in\mathcal{E}}\sum_{r\in\mathcal{R}}\sum_{o\in\mathcal{E}}\phi(s,r,o)\) or any other summation over subjects, predicates or objects can be computed in time \(\mathcal{O}((|\mathcal{E}|+|\mathcal{R}|)\cdot|\phi|)\).

Proof.: A proof for the computation of marginal probabilities in smooth and decomposable _probabilistic circuits_ (PCs) defined over discrete variables in linear time with respect to their size can be found in [13]. This proof also applies for computing summations in smooth and decomposable circuits that do not necessarily corresponds to marginal probabilities [76]. The satisfaction of smoothness and decomposability (Def. 2) in a circuit \(\phi\) permits to push outer summations inside the computational graph until input units are reached, where summations are actually performed independently and on smaller sets of variables (i.e., \(\{S\}\), \(\{R\}\), \(\{O\}\) in our case), and then to evaluate the circuit only once.

Here we take into account the computational cost of summing over each input unit (see proof of Prop. 1), which is \(\mathcal{O}(|\mathcal{E}|)\) (resp. \(\mathcal{O}(|\mathcal{R}|)\)) for those defined on variables \(S,O\) (resp. \(R\)). Since the size of the circuit \(|\phi|\) must be at least the number of input units, we retrieve that the overall complexity for computing summations as stated in the proposition is \(\mathcal{O}((|\mathcal{E}|+|\mathcal{R}|)\cdot|\phi|)\).

As an example, consider the CP score function computing \(\phi_{\text{CP}}(s,r,o)=\langle\mathbf{e}_{s},\mathbf{w}_{r},\mathbf{e}_{o}\rangle\) for some triple \((s,r,o)\) and embeddings \(\mathbf{e}_{s},\mathbf{w}_{r},\mathbf{e}_{o}\in\mathbb{R}^{d}\). We can compute \(\sum_{s\in\mathcal{E}}\sum_{r\in\mathcal{R}}\sum_{o\in\mathcal{E}}\phi_{\text {CP}}(s,r,o)\) by pushing the outer summations inside the trilinear product, i.e., by computing it as \(\langle\sum_{s\in\mathcal{E}}\mathbf{e}_{s},\sum_{r\in\mathcal{R}}\mathbf{w}_ {r},\sum_{o\in\mathcal{E}}\mathbf{e}_{o}\rangle\), which requires time \(\mathcal{O}((|\mathcal{E}|+|\mathcal{R}|)\cdot d)\). 

### Efficient Summations over Squared Circuits

**Theorem 1** (Efficient summations of squared GeKCs).: Performing summations as stated in Prop. 2 on CP\({}^{2}\), RESCAL\({}^{2}\), TuckER\({}^{2}\) and ComplEx\({}^{2}\) can be done in time \(\mathcal{O}((|\mathcal{E}|+|\mathcal{R}|)\cdot|\phi|^{2})\).

Proof.: In Lem. A.1 we showed that the circuit representations \(\phi\) of CP, RESCAL, TuckER and ComplEx are omni-compatible (see Def. B.2). As a consequence, \(\phi\) is compatible (see Def. B.1) with itself. Therefore, Prop. B.1 ensures that we can construct the product circuit \(\phi\cdot\phi\) (i.e., \(\phi^{2}\)) as a smooth and decomposable circuit having size \(\mathcal{O}(|\phi|^{2})\) in time \(\mathcal{O}(|\phi|^{2})\). Since \(\phi^{2}\) is still smooth and decomposable, Prop. 2 guarantees that we can perform summations in time \(\mathcal{O}((|\mathcal{E}|+|\mathcal{R}|)\cdot|\phi|^{2})\). 

### Circuits encoding Domain Constraints

In Def. A.1 we introduce the concepts of _support_ and _determinism_, whose definition is useful to describe _constraint circuits_ in Def. A.2.

**Definition A.1** (Support and Determinism [13, 76]).: In a circuit the _support_ of a computational unit \(n\) over variables \(\mathbf{X}\) computing \(\phi_{n}(\mathbf{X})\) is defined as the set of value assignments to variables in \(\mathbf{X}\) such that the output of \(n\) is non-zero, i.e., \(\mathsf{supp}(n)=\{\mathbf{x}\in\mathsf{val}(\mathbf{X})\mid\phi_{n}(\mathbf{ X})\neq 0\}\). A sum unit \(n\) is _deterministic_ if its inputs have disjoint _supports_, i.e., \(\forall i,j\in\mathsf{in}(n),i\neq j\colon\mathsf{supp}(i)\cap\mathsf{supp}( j)=\varnothing\).

**Definition A.2** (Constraint Circuit [1]).: Given a propositional logic formula \(K\), a _constraint circuit_\(c_{K}\) is a smooth and decomposable PC over variables \(\mathbf{X}\) with _deterministic_ sum units (Def. A.1) and indicator functions as input units, such that \(c_{K}(\mathbf{x})=\mathds{1}\{\mathbf{x}\models K\}\) for any \(\mathbf{x}\in\mathsf{val}(\mathbf{X})\).

In general, we can compile any propositional logic formula into a constraint circuit (Def. A.2) by leveraging knowledge compilation techniques [19, 18, 52]. For domain constraints (Def. 4) this compilation process is straightforward, as we detail in the following proposition and proof.

**Proposition A.1** (Circuit encoding domain constraints).: Let \(K=K_{r_{1}}\vee\ldots\lor K_{r_{m}}\) be a disjunction of domain constraints defined over a set of predicates \(\mathcal{R}=\{r_{1},\ldots,r_{m}\}\) and a set of entities \(\mathcal{E}\) (Def. 4). We can compile \(K\) into a constraint circuit \(c_{K}\) (Def. A.2) defined over variables \(\mathbf{X}=\{S,R,O\}\) having size \(\tilde{\mathcal{O}}(|\mathcal{E}|\cdot|\mathcal{R}|)\) in the worst case and \(\mathcal{O}(|\mathcal{E}|+|\mathcal{R}|)\) in the best case.

Proof.: Let \(K=K_{r_{1}}\vee\ldots\lor K_{r_{m}}\) be a disjunction of domain constraints (Def. 4) where

\[K_{r}\equiv S\in\kappa_{S}(r)\wedge R=r\wedge O\in\kappa_{O}(r)\equiv(\vee_{u \in\kappa_{S}(r)}S=u)\wedge R=r\wedge(\vee_{v\in\kappa_{O}(r)}O=v).\]Note that the disjunctions in \(K\) are deterministic, i.e., only one of their argument can be true at the same time. This enables us to construct the constraint circuit \(c_{K}\) such that \(c_{K}(s,r,o)=\mathds{1}\{(s,r,o)\models K\}\) for any triple by simply replacing conjunctions and disjunctions with product and sum units, respectively. Note that \(c_{K}\) is indeed smooth and decomposable (Def. 2), as the inputs of the sum units are product units having scope \(\{S,R,O\}\) that are fully factorised into \((\{S\},\{R\},\{O\})\). Moreover, \(K\) is a disjunction of \(|\mathcal{R}|\) conjunctive formulae having \(\mathcal{O}(|\mathcal{E}|)\) terms, and therefore \(|c_{K}|=\mathcal{O}(|\mathcal{E}|\cdot|\mathcal{R}|)\) in the worst case. In the best case of every predicate sharing the same subject and object domains \(\kappa_{S},\kappa_{O}\subseteq\mathcal{E}\), we can simplify \(K\) into a conjunction of three disjunctive expressions, i.e.,

\[K\equiv(\vee_{u\in\kappa_{S}}S=u)\wedge(\vee_{r\in\mathcal{R}}R=r)\wedge(\vee _{v\in\kappa_{O}}O=v)\]

that can be easily compiled into a constraint circuit \(c_{K}\) having size \(\mathcal{O}(|\mathcal{E}|+|\mathcal{R}|)\), by again noticing that disjunctions are deterministic. In real-world KGs like ogbl-biokq [32] several predicates share the same subject and object domains, and this permits to have much smaller constraint circuits. 

### Efficient Integration of Domain Knowledge in GeKCs

**Theorem 2** (Tractable integration of constraints in GeKCs).: Let \(c_{K}\) be a constraint circuit encoding a logical constraint \(K\) over variables \(\{S,R,O\}\). Then exactly computing the partition function \(Z_{K}\) of the product \(\phi_{\mathsf{pc}}(s,r,o)\cdot c_{K}(s,r,o)\propto p_{K}(s,r,o)\) for any GeKC \(\phi_{\mathsf{pc}}\) derived from CP, RESCAL, TuckER or CompLex (Section 4) can be done in time \(\mathcal{O}((|\mathcal{E}|+|\mathcal{R}|)\cdot|\phi_{\mathsf{pc}}|\cdot|c_{K}|)\).

Proof.: In Lem. A.1 we showed that the GeKCs \(\phi_{\mathsf{pc}}\) derived from CP, RESCAL, TuckER and CompLex via non-negative restriction (Section 4.1) or squaring (Section 4.2) are omni-compatible (see Def. B.2). As a consequence, \(\phi_{\mathsf{pc}}\) is always compatible with \(c_{K}\) regardless of the encoded logical constraint \(K\), since constraint circuits are by definition smooth and decomposable (Def. A.2). By applying Prop. B.1, we retrieve that we can construct \(\phi_{\mathsf{pc}}\cdot c_{K}\) as a smooth and decomposable circuit of size \(\mathcal{O}(|\phi_{\mathsf{pc}}|\cdot|c_{K}|)\) and in time \(\mathcal{O}(|\phi_{\mathsf{pc}}|\cdot|c_{K}|)\). As the resulting product circuit is smooth and decomposable, Prop. 2 guarantees that we can compute its partition function \(Z_{K}=\sum_{s\in\mathcal{E}}\sum_{r\in\mathcal{R}}\sum_{o\in\mathcal{E}}(\phi _{\mathsf{pc}}(s,r,o)\cdot c_{K}(s,r,o))\) in time \(\mathcal{O}((|\mathcal{E}|+|\mathcal{R}|)\cdot|\phi_{\mathsf{pc}}|\cdot|c_{K}|)\). 

## Appendix B Circuits

### Tractable Product of Circuits

In this section, we provide the formal definition of _compatibility_ (Def. B.1) and _omni-compatibility_ (Def. B.2), as stated by Vergari et al. [76]. Given two compatible circuits, Prop. B.1 guarantees that we can represent their product as a smooth and decomposable circuit efficiently.

**Definition B.1** (Compatibility).: Two circuits \(\phi,\phi^{\prime}\) over variables \(\mathbf{X}\) are _compatible_ if (1) they are smooth and decomposable, and (2) any pair of product units \(n\in\phi,m\in\phi^{\prime}\) having the same scope can be rearranged into binary products that are mutually compatible and decompose their scope in the same way, i.e., \((\mathsf{sc}(n)=\mathsf{sc}(m))\implies(\mathsf{sc}(n_{i})=\mathsf{sc}(m_{i}),\,n_{i}\text{ and }m_{i}\text{ are compatible})\) for some rearrangements of the inputs of \(n\) (resp. \(m\)) into \(n_{1},n_{2}\) (resp. \(m_{1},m_{2}\)).

**Definition B.2** (Omni-compatibility).: A circuit \(\phi\) over variables \(\mathbf{X}\) is _omni-compatible_ if it is compatible with any smooth and decomposable circuit over \(\mathbf{X}\).

**Proposition B.1** (Tractable product of circuits).: Let \(\phi,\phi^{\prime}\) be two compatible (Def. B.1) circuits. We can represent the product circuit \(\phi\cdot\phi^{\prime}\) computing the product of the outputs of \(\phi\) and \(\phi^{\prime}\) as a smooth and decomposable circuit having size \(\mathcal{O}(|\phi|\cdot|\phi^{\prime}|)\) in time \(\mathcal{O}(|\phi|\cdot|\phi^{\prime}|)\). Moreover, if both \(\phi\) and \(\phi^{\prime}\) are omni-compatible (Def. B.2), then also the product circuit \(\phi\cdot\phi^{\prime}\) is omni-compatible.

Prop. B.1 allows us to compute the partition function and any other marginal probability in GeKCs obtained via squaring efficiently (see Section 4.2 and Thm. 1). In addition, Prop. B.1 is a crucial theoretical result that allows us to inject logical constraints in GeKCs in a way that enable computing the partition function exactly and efficiently (see Section 5 and Thm. 2).

From KGE Models to PCs

### Interpreting Non-negative Embedding Values

In Fig. C.1 we interpret the embedding values of GeKCs obtained via non-negative restriction - \(\mathsf{CP^{+}}\), \(\mathsf{RESCAL^{+}}\), \(\mathsf{TuckER^{+}}\), \(\mathsf{ComplEx^{+}}\)- (Section 4.1) as the parameters of unnormalised categorical distributions over entities (elements in \(\mathcal{E}\)) or predicates (elements in \(\mathcal{R}\)).

### Realising the Non-negative Restriction of \(\mathsf{ComplEx}\)

As anticipated in Section 4.1, for the \(\mathsf{ComplEx}\)[65] score function restricting the real and imaginary parts to be non-negative is not sufficient to obtain a PC due to the presence of a subtraction, as showed in the following equation.

\[\phi_{\mathsf{COMPLEX}}(s,r,o) =\langle\mathrm{Re}(\mathbf{e}_{s}),\mathrm{Re}(\mathbf{w}_{r}),\mathrm{Re}(\mathbf{e}_{o})\rangle+\langle\mathrm{Im}(\mathbf{e}_{s}), \mathrm{Re}(\mathbf{w}_{r}),\mathrm{Im}(\mathbf{e}_{o})\rangle\] (5) \[+\langle\mathrm{Re}(\mathbf{e}_{s}),\mathrm{Im}(\mathbf{w}_{r}),\mathrm{Im}(\mathbf{e}_{o})\rangle-\langle\mathrm{Im}(\mathbf{e}_{s}), \mathrm{Im}(\mathbf{w}_{r}),\mathrm{Re}(\mathbf{e}_{o})\rangle\]

Here \(\mathbf{e}_{s},\mathbf{w}_{r},\mathbf{e}_{o}\in\mathbb{C}^{d}\) are the embeddings associated to the subject, predicate and object, respectively. Under the restriction of embedding values to be non-negative, we ensure that \(\phi_{\mathsf{COMPLEX}}(s,r,o)\geq 0\) for any input triple by enforcing the additional constraint

\[\langle\mathrm{Re}(\mathbf{e}_{s}),\mathrm{Re}(\mathbf{w}_{r}),\mathrm{Re}( \mathbf{e}_{o})\rangle\geq\langle\mathrm{Im}(\mathbf{e}_{s}),\mathrm{Im}( \mathbf{w}_{r}),\mathrm{Re}(\mathbf{e}_{o})\rangle,\] (6)

which can be simplified into the two distinct inequalities

\[\forall u\in\mathcal{E}\quad\mathrm{Re}(e_{ui})\geq\mathrm{Im}(e_{ui})\qquad \text{ and }\qquad\forall r\in\mathcal{R}\quad\mathrm{Re}(w_{ri})\geq\mathrm{Im}(w_{ri}).\]

In other words, we want the real part of each embedding value to be always greater or equal than the corresponding imaginary part. We implement this constraint in practice by reparametrisation of the imaginary part in function of the real part, i.e.,

\[\forall u\in\mathcal{E}\quad\mathrm{Im}(e_{ui})=\mathrm{Re}(e_{ ui})\cdot\sigma(\theta_{ui})\] (7) \[\forall r\in\mathcal{R}\quad\mathrm{Im}(w_{ri})=\mathrm{Re}(w_{ri} )\cdot\sigma(\gamma_{ri})\] (8)

where \(\sigma(x)=1/(1+\exp(-x))\in[0,1]\) denotes the logistic function and \(\theta_{ui},\gamma_{ri}\in\mathbb{R}\) are additional parameters associated to entities \(u\in\mathcal{E}\) and predicates \(r\in\mathcal{R}\), respectively. The reparametrisation of the imaginary parts using Eqs. (7) and (8) is a sufficient condition for the satisfaction of the constraint showed in Eq. (6), and also maintains the same number of learnable parameters of \(\mathsf{ComplEx}\).

### Sampling from GeKCs with Non-negative Parameters

Parameters interpretation.Sum units with non-negative parameters in smooth and decomposable PCs can be seen as marginalised discrete latent variables, similarly to the latent variable interpretation in mixture models [55, 53]. That is, the non-negative parameters of a sum unit are the parameters of a (possibly unnormalised) categorical distribution over assignments to a latent variable. For \(\mathsf{CP^{+}}\) and \(\mathsf{RESCAL^{+}}\) (Section 4.1), the non-negative parameters of the sum unit encode a uniform and

Figure C.1: **Non-negative embeddings parameterise categorical distributions.** 2-dimensional embeddings of GeKCs obtained via non-negative restrictions (Section 4.1) can be seen as the parameters of two categorical distributions over entities (left) or predicates (right) up to renormalisation.

unnormalised categorical distribution, as they are all fixed to \(1\) (see Fig. 2). By contrast, in \(\text{\sc{TuckER}}^{+}\) these parameters are the vectorisation of the core tensor \(\mathcal{T}\) (see the proof of Prop. 1), and hence they are learned. The input units of \(\text{\sc{CP}}^{+}\), \(\text{\sc{RESCAL}}^{+}\) and \(\text{\sc{TuckER}}^{+}\) can be interpreted as unnormalised categorical distribution over entities or predicates, as detailed in Appendix C.1.

Sampling from \(\text{\sc{CP}}^{+}\), \(\text{\sc{Complex}}^{+}\), \(\text{\sc{TuckER}}^{+}\).Thanks to the latent variable interpretation, ancestral sampling in \(\text{\sc{CP}}^{+}\), \(\text{\sc{RESCAL}}^{+}\) and \(\text{\sc{TuckER}}^{+}\) can be performed by (1) sampling an assignment to the latent variable associated to the single sum unit, i.e., one of its input branches, (2) selecting the corresponding combination of subject-predicate-object input units, and (3) sampling a subject, predicate and object respectively from each of the indexed unnormalized categorical distributions.

### Learning Complexity of GeKCs

In Table C.1 we summarise the complexities of computing the PLL and MLE objectives (Eqs. (1) and (2)) for KGE models and GeKCs. Asymptotically, GeKCs manifest better time and space complexities with respect to the number of entities \(|\mathcal{E}|\), batch size \(|B|\) and embedding size. This makes GeKCs more efficient than traditional KGE models during training, both in time and memory (see Section 4.3 and Fig. 3).

#### c.4.1 Computing the Partition Function

In this section we derive the computational complexity of computing the partition function for GeKCs obtained via squaring (Section 4.2). For a summary of these complexities, see Table C.1.

CP2 and \(\text{\sc{ComplEx}}^{2}\).Here we derive the partition function of \(\text{\sc{CP}}^{2}\). For \(\text{\sc{ComplEx}}^{2}\) the derivation is similar, as the score function of \(\text{\sc{ComplEx}}\) can be written in terms of trilinear products just like CP (see Eq. (5)). The score function \(\phi_{\text{\sc{CP}}^{2}}\) encoded by \(\text{\sc{CP}}^{2}\) can be written as

\[\phi_{\text{\sc{CP}}^{2}}(s,r,o)=\langle\mathbf{e}_{s},\mathbf{w}_{r},\mathbf{ e}_{o}\rangle^{2}=\sum_{i=1}^{d}\sum_{j=1}^{d}e_{si}e_{sj}w_{ri}w_{rj}e_{oi}e_{oj}\]

where \(\mathbf{e}_{s},\mathbf{e}_{o}\in\mathbb{R}^{d}\) (resp. \(\mathbf{w}_{r}\in\mathbb{R}^{d}\)) are rows of the matrices \(\mathbf{U},\mathbf{V}\in\mathbb{R}^{|\mathcal{E}|\times d}\) (resp. \(\mathbf{W}\in\mathbb{R}^{|\mathcal{R}|\times d}\)), which associate to each entity (resp. predicate) a vector. By leveraging the _einsum notation_ for

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline \multirow{2}{*}{**Model**} & \multicolumn{3}{c}{**Complexity of \(\log p(o\mid s,r)\)**} & \multicolumn{3}{c}{**Complexity of \(Z\)**} \\ \cline{2-6}  & \multicolumn{2}{c}{**Time**} & **Space** & \multicolumn{1}{c}{**Time**} & **Space** \\ \cline{2-6}  & \multicolumn{2}{c}{\(\mathcal{O}(|\mathcal{E}|\cdot|B|\cdot d)\)} & \(\mathcal{O}(|\mathcal{E}|\cdot|B|)\) & \(\mathcal{O}(|\mathcal{E}|^{2}\cdot|\mathcal{R}|\cdot d)\) & \(\mathcal{O}(d)\) \\ RESCAL & \(\mathcal{O}(|\mathcal{E}|\cdot|B|\cdot d+|B|\cdot d^{2})\) & \(\mathcal{O}(|\mathcal{E}|\cdot|B|)\) & \(\mathcal{O}(|\mathcal{E}|^{2}\cdot|\mathcal{R}|\cdot d^{2})\) & \(\mathcal{O}(d^{2})\) \\ Tucker & \(\mathcal{O}(|\mathcal{E}|\cdot|B|\cdot d_{e}+|B|\cdot d^{2}e\cdot d_{r})\) & \(\mathcal{O}(|\mathcal{E}|\cdot|B|)\) & \(\mathcal{O}(|\mathcal{E}|^{2}\cdot|\mathcal{R}|\cdot d^{2}e\cdot d_{r})\) & \(\mathcal{O}(d^{2}_{e}\cdot d_{r})\) \\ Complex & \(\mathcal{O}(|\mathcal{E}|\cdot|B|\cdot d)\) & \(\mathcal{O}(|\mathcal{E}|\cdot|B|)\) & \(\mathcal{O}(|\mathcal{E}|^{2}\cdot|\mathcal{R}|\cdot d)\) & \(\mathcal{O}(d)\) \\ \hline \(\text{\sc{CP}}^{+}\) & \(\mathcal{O}(|\mathcal{E}|+|B|)\cdot d)\) & \(\mathcal{O}(|B|\cdot d)\) & \(\mathcal{O}((|\mathcal{E}|+|\mathcal{R}|\cdot d)\cdot d)\) & \(\mathcal{O}(d)\) \\ RESCAL\({}^{+}\) & \(\mathcal{O}(|\mathcal{E}|+|B|\cdot d)\cdot d)\) & \(\mathcal{O}(|B|\cdot d^{2})\) & \(\mathcal{O}((|\mathcal{E}|+|\mathcal{R}|\cdot d)\cdot d)\) & \(\mathcal{O}(d^{2})\) \\ Tucker\({}^{+}\) & \(\mathcal{O}(|\mathcal{E}|+|B|\cdot d)\) & \(\mathcal{O}(|B|\cdot d)\) & \(\mathcal{O}(|\mathcal{E}|\cdot|\mathcal{R}|\cdot d^{2})\) & \(\mathcal{O}(d^{2})\) \\ Complex\({}^{+}\) & \(\mathcal{O}(|\mathcal{E}|+|B|\cdot d^{2})\) & \(\mathcal{O}(|B|\cdot d)\) & \(\mathcal{O}((|\mathcal{E}|+|\mathcal{R}|\cdot d^{2})\) & \(\mathcal{O}(d^{2})\) \\ \hline \(\text{\sc{CP}}^{2}\) & \(\mathcal{O}(|\mathcal{E}|+|B|)\cdot d^{2})\) & \(\mathcal{O}(|B|\cdot d)\) & \(\mathcal{O}((|\mathcal{E}|+|\mathcal{R}|\cdot d)^{2})\) & \(\mathcal{O}(d^{2})\) \\ RESCAL\({}^{2}\) & \(\mathcal{O}(|\mathcal{E}|+|B|\cdot d^{2})\) & \(\mathcal{O}(|B|\cdot d^{2})\) & \(\mathcal{O}((|\mathcal{E}|+|\mathcal{R}|\cdot d)\cdot d^{2})\) & \(\mathcal{O}(|\mathcal{R}|\cdot d^{2})\) \\ Tucker\({}^{2}\) & \(\mathcal{O}(|\mathcal{E}|+|B|\cdot d_{r})\cdot d_{e}^{2})\) & \(\mathcal{O}(|B|\cdot d_{e}\cdot d_{r})\) & \(\mathcal{O}(|\mathcal{E}|\cdot d_{e}^{2}+|\mathcal{R}|\cdot d_{e}^{2}+d_{e}^{2} \cdot d_{r})\) & \(\mathcal{O}(d_{e}^{2}\cdot d_{r})\) \\ Complex\({}^{2}\) & \(\mathcal{O}(|\mathcal{E}|+|B|)\cdot d^{2})\) & \(\mathcal{O}(|B|\cdot d)\) & \(\mathcal{O}((|\mathcal{E}|+|\mathcal{R}|\cdot d^{2})\) & \(\mathcal{O}(d^{2})\) \\ \hline \hline \end{tabular}
\end{table}
Table C.1: **Summary of complexities for exactly computing the PLL and MLE objectives.** Time and space complexity of computing \(\log p(o\mid s,r)\) and the partition function \(Z\). These complexities are respectively lower bounds of the complexities of computing the PLL and MLE objectives, as we have that \(|\mathcal{E}|\gg|\mathcal{R}|\) for large real-world KGs. For CP, RESCAL and ComplEx and GeKCs derived from them, \(d\) denotes the size of both entity and predicate embeddings. For \(\text{\sc{TuckER}}\) and GeKCs derived from it, \(d_{e}\) and \(d_{r}\) denote the embedding sizes for entities and predicates, respectively.

[MISSING_PAGE_FAIL:22]

Complexity of the PLL objective on CP.Let \(\phi_{\text{CP}}(s,r,o)=\langle\mathbf{e}_{s},\mathbf{w}_{r},\mathbf{e}_{o}\rangle =\sum_{i=1}^{d}e_{si}w_{ri}e_{oi}\) be the score function of CP [40], where \(\mathbf{e}_{s},\mathbf{e}_{o}\in\mathbb{R}^{d}\) (resp. \(\mathbf{w}_{r}\in\mathbb{R}^{d}\)) are rows of the matrices \(\mathbf{U},\mathbf{V}\in\mathbb{R}^{|\mathcal{E}|\times d}\) (resp. \(\mathbf{W}\in\mathbb{R}^{|\mathcal{R}|\times d}\)), which associate to each entity (resp. predicate) a vector. Given a training triple \((s,r,o)\), the computation of the term \(\log p(o\mid s,r)=\phi(s,r,o)-\log\sum_{o^{\prime}\in\mathcal{E}}\exp\phi(s,r, o^{\prime})\) requires evaluating \(\phi_{\text{CP}}(s,r,o^{\prime})\) for all objects \(o^{\prime}\in\mathcal{E}\). In order to fully exploit GPU parallelism [36], this is usually done with the matrix-vector multiplication \(\mathbf{V}(\mathbf{e}_{s}\odot\mathbf{w}_{r})\in\mathbb{R}^{|\mathcal{E}|}\), where \(\odot\) denotes the Hadamard product [40, 12]. Therefore, computing \(\log p(o\mid s,r)\) for each triple \((s,r,o)\) in a mini-batch \(B\subset\mathcal{E}\times\mathcal{R}\times\mathcal{E}\) such that \(|\mathcal{E}|\gg|\mathcal{B}|\) requires time \(\mathcal{O}(|\mathcal{E}|\cdot|B|\cdot d)\) and space \(\mathcal{O}(|\mathcal{E}|\cdot|B|)\). For the other terms of the PLL objective (i.e., \(\log p(s\mid r,o)\) and \(\log p(r\mid s,o)\)) the derivation is similar. Moreover, for real-world large KGs it is reasonable to assume that \(|\mathcal{E}|\gg|\mathcal{R}|\) and therefore the cost of computing \(\log p(r\mid s,o)\) is negligible.

Complexity of the PLL objective on GeKCs.GeKCs obtained from CP either by non-negative restriction (Section 4.1) or by squaring (Section 4.2) encode \(\phi_{\text{pc}}(s,r,o)\propto p(s,r,o)\) for any input triple. As such, the component \(\log p(o\mid s,r)\) of the PLL objective can be written as

\[\log p(o\mid s,r)=\log\phi_{\text{pc}}(s,r,o)-\log\sum_{o^{\prime}\in\mathcal{ E}}\phi_{\text{pc}}(s,r,o^{\prime}).\] (11)

The absence of the exponential function in the summed terms in Eq. (11) allows us to push the outer summation inside the circuit computing \(\phi_{\text{pc}}(s,r,o)\), and to sum over the input units relative to objects. For instance, for CP\({}^{\star}\) we can write

\[\sum_{o^{\prime}\in\mathcal{E}}\phi_{\text{CP}^{\star}}(s,r,o^{\prime})=\sum_ {o^{\prime}\in\mathcal{E}}\sum_{i=1}^{d}e_{si}w_{ri}e_{oi}=\sum_{i=1}^{d}e_{si }w_{ri}\left(\sum_{o\in\mathcal{E}}e_{o^{\prime}i}\right)=(\mathbf{e}_{s} \odot\mathbf{w}_{r})^{\top}(\mathbf{V}^{\top}\mathbf{1}_{\mathcal{E}})\]

where \(\mathbf{e}_{s},\mathbf{w}_{r},\mathbf{e}_{o}\in\mathbb{R}_{+}^{d}\), \(\mathbf{V}\in\mathbb{R}_{+}^{|\mathcal{E}|\times d}\) denotes the matrix whose rows are object embeddings, and \(\mathbf{1}_{\mathcal{E}}=[1\dots 1]^{|\mathcal{E}|}\) is a vector of ones. Note that \(\mathbf{V}^{\top}\mathbf{1}_{\mathcal{E}}\in\mathbb{R}_{+}^{d}\) does not depend on the input triple. Therefore, given a mini-batch of triples \(B\), computing \(\log p(o\mid s,r)\) requires time \(\mathcal{O}((|\mathcal{E}|+|B|)\cdot d)\) and space \(\mathcal{O}(|B|\cdot d)\), which is much lower than the complexity on CP showed above, and we can still leverage GPU parallelism. For CP\({}^{2}\), the complexity is similar to the derivation of the partition function complexity showed in Appendix C.4.1. That is, for CP\({}^{2}\) we can write

\[\sum_{o^{\prime}\in\mathcal{E}}\phi_{\text{CP}^{2}}(s,r,o^{\prime}) =\sum_{o^{\prime}\in\mathcal{E}}\left(\sum_{i=1}^{d}e_{si}w_{ri}e_{oi }\right)^{2}=\sum_{i=1}^{d}\sum_{j=1}^{d}e_{si}e_{sj}w_{ri}w_{rj}\left(\sum_{o ^{\prime}\in\mathcal{E}}e_{o^{\prime}i}e_{o^{\prime}j}\right)\] \[=(\mathbf{e}_{s}\odot\mathbf{w}_{r})^{\top}(\mathbf{V}^{\top} \mathbf{V})(\mathbf{e}_{s}\odot\mathbf{w}_{r})\]

where \(\mathbf{e}_{s},\mathbf{w}_{r},\mathbf{e}_{o}\in\mathbb{R}^{d}\), \(\mathbf{V}\in\mathbb{R}^{|\mathcal{E}|\times d}\). Note that the matrix \(\mathbf{V}^{\top}\mathbf{V}\in\mathbb{R}^{d\times d}\) does not depend on the input triple. Therefore, given a mini-batch of triples \(B\), computing \(\log p(o\mid s,r)\) requires time \(\mathcal{O}((|\mathcal{E}|+|B|)\cdot d^{2})\) and space \(\mathcal{O}(|B|\cdot d)\). While the time complexity is quadratic in the embedding size \(d\), it is still much lower than the time complexity on CP. A similar discussion can also be carried out for the other KGE models and the corresponding GeKCs, which retrieves the complexities showed in Table C.1.

#### c.4.3 Training Speed-up Benchmark Details

In this section we report the details about the training benchmark on CompLex, CompLex\({}^{+}\) and CompLex\({}^{2}\), whose results are showed in Fig. 3. We measure time and peak GPU memory usage required for computing the PLL objective (Eq. (1)) _and_ to do an optimisation step for a single batch on ogbl-wikikg2 [32], a large knowledge graph with millions of entities (see Table F.1). We fix the embedding size to \(d=100\) for the three models. For the benchmark with increasing batch size, we keep all the entities and increase the batch size from \(100\) to \(5000\). For the benchmark with increasing number of entities, we keep the batch size fixed to \(500\) (the maximum allowed for CompLex by our GPUs) and progressively increase the number of entities, from about \(3\cdot 10^{5}\) to \(2.5\cdot 10^{6}\). We report the average time over 25 independent runs on a single Nvidia RTX A6000 with 48 GiB of memory.

## Appendix D Distribution of Scores

In Fig. D.1 we show the histograms of the scores assigned to the validation triples and their perturbations of three data sets (see Appendix F.1). Following Socher et al. [59], we generate triple perturbations that are challenging for link prediction. That is, for each validation triple \((s,r,o)\), the corresponding perturbation \((s,r,\widehat{o})\) is obtained by replacing the object with a random entity that has appeared at least once as an object in a training triple with predicate \(r\). The bottom line is that scores are mostly non-negative, and hence can be used as a heuristic to effectively initialise GeKCs or quickly distil them (e.g., on FB15K-237), as we further discuss in Appendix F.5.1.

## Appendix E Reconciling Knowledge Graph Embeddings Interpretations

Triples as boolean variables.KGE models such as CP, RESCAL, TuckER and ComplEx have been historically introduced as factorizations of a tensor-representation of a KG, which we discuss next. In fact, a KG \(\mathcal{G}\) can be represented as a 3-way binary tensor \(\mathbf{Y}\in\{0,1\}^{|\mathcal{E}|\times|\mathcal{R}|\times|\mathcal{E}|}\) in which every entry \(Y_{sro}\) is 1 if \((s,r,o)\in\mathcal{G}\) and 0 otherwise [48]. Under this light, a KGE model like RESCAL factorises every slice \(\mathbf{Y}_{r}\in\{0,1\}^{|\mathcal{E}|\times|\mathcal{E}|}\), corresponding to the predicate \(r\) as \(\mathbf{EW}_{r}\mathbf{E}^{\top}\) where \(\mathbf{E}\) is an \(|\mathcal{E}|\times d\) matrix comprising the entity embeddings and \(\mathbf{W}_{r}\) is an \(d\times d\) matrix containing the embeddings for the predicate \(r\). To deal with uncertainty and incomplete KGs, \(Y_{sro}\) can be interpreted as a Bernoulli random variable. As such, its distribution becomes \(p(Y_{sro}=1\mid s,r,o)\) which is usually modelled as \(\sigma(\phi(s,r,o))\), where \(\sigma\) denotes the logistic function and \(\phi\) is the score function of a KGE model. Note that this distribution of triple introduces \(|\mathcal{E}\times\mathcal{R}\times\mathcal{E}|\) random variables, one for each possible triple.

KGE models as estimators of a distribution over KGs.At the same time, the interpretation of triples as boolean variables _induces a distribution over possible KGs_, \(q(\mathcal{G})\), which is the distribution over all possible binary tensors \(p(\mathbf{Y})\). The probability of a KG \(\mathcal{G}\) can therefore be computed as the product of the likelihoods of all variables \(Y_{sro}\), i.e., \(q(\mathcal{G})=\prod_{(s,r,o)\in\mathcal{G}}p(Y_{sro}=1\mid s,r,o)\cdot\prod _{(s,r,o)\not\in\mathcal{G}}p(Y_{sro}=0\mid s,r,o)\). Note that (re-)normalising this distribution is intractable in general, as it would require summing over all possible \(2^{|\mathcal{E}\times\mathcal{R}\times\mathcal{E}|}\) binary tensors. This is why historically KGE models have been interpreted as energy-based models, by directly optimising for \(\phi(s,r,o)\), interpreted as the negated energy associated to every triple, and not \(p(Y_{sro}=1\mid s,r,o)\) (see Section 2). This has been done via negative sampling or other contrastive learning objectives [6, 7]. We point out that this very same interpretation can be found in the literature of probabilistic logic programming [25], probabilistic databases (PDBs) [14] and statistical relational learning (see Section 6) where the distribution over possible "worlds" is over sets of boolean assignments to ground atoms or facts, or tuples in a PDB, each interpreted as Bernoulli random variables.

Estimating a distribution over triples.In this work, instead, we interpret existing KGE models and our GeKCs as models that encode a possibly unnormalised probability distribution over three random variables, \(S,R,O\), which induces a distribution over triples that is tractable to renormalise.7

Footnote 7: The polynomial cost of renormalising an energy-based KGE is unfortunately infeasible for real-world KGs, see Section 2.

Figure D.1: **Scores are mostly non-negative.** Histograms of the scores assigned by ComplEx to existing validation triples (left) and their perturbation (right) on three data sets. The vast majority of triple scores are non-negative, suggesting that squaring them has minimal effect on the rankings.

[MISSING_PAGE_FAIL:25]

### Metrics

Mean reciprocal rank and hits at \(k\).Given a test triple \((s,r,o)\), we rank the possible object \(o^{\prime}\) (resp. subject \(s^{\prime}\)) completions to link prediction queries \((s,r,?)\) (resp. \((?,r,o)\)) based on their scores in descending order. The position of the test triple \((s,r,o)\) in the ranking of object (resp. subject) completed queries \((s,r,o^{\prime})\) (resp. \((s^{\prime},r,o)\)) is then used to compute the _mean reciprocal rank_ (MRR)

\[\mathrm{MRR}=\frac{1}{2|\mathcal{G}_{\text{test}}|}\sum_{(s,r,o)\in\mathcal{G }_{\text{test}}}\left(\frac{1}{\mathrm{rank}(o\mid s,r)}+\frac{1}{\mathrm{ rank}(s\mid r,o)}\right)\]

where \(\mathcal{G}_{\text{test}}\) denotes the set of test triples, and \(\mathrm{rank}(o\mid s,r),\mathrm{rank}(s\mid r,o)\) denote respectively the positions of the true completion \((s,r,o)\) in the rankings of object and subject completed queries. The fraction of _hits at \(k\)_ (Hits@\(k\)) for \(k>0\) is computed as

\[\mathrm{Hits@}\,k=\frac{1}{2|\mathcal{G}_{\text{test}}|}\sum_{(s,r,o)\in \mathcal{G}_{\text{test}}}\left(\mathds{1}\{\mathrm{rank}(o\mid s,r)\leq k \}+\mathds{1}\{\mathrm{rank}(s\mid r,o)\leq k\}\right).\]

Consistently with existing works on link prediction [56; 12], the MRRs and Hits@\(k\) metrics are computed under the _filtered_ setting, i.e., we rank true completed triples against potential ones that do not appear in the union of training, validation and test splits.

Semantic consistency score.Let \(K\) be a logical constraint encoding some background knowledge over variables \(S\), \(R\) and \(O\). Given a test triple \((s,r,o)\), we first rank the possible completions to link prediction queries in the same way as for computing the MRR. Then, the _semantic consistency score_ (Sem@\(k\)) [33] for some integer \(k>0\) is computed as

\[\mathrm{Sem@}k=\frac{1}{2k|\mathcal{G}_{\text{test}}|}\sum_{(s,r,o)\in \mathcal{G}_{\text{test}}}\left(\sum_{o^{\prime}\in\mathcal{A}^{k}_{0}(s,r,o )}\mathds{1}\{(s,r,o^{\prime})\models K\}+\sum_{s^{\prime}\in\mathcal{A}^{k}_ {0}(s,r,o)}\mathds{1}\{(s^{\prime},r,o)\models K\}\right)\]

where \(\mathcal{G}_{\text{test}}\) denotes the set of test triples, \(\mathcal{A}^{k}_{0}(s,r,o)\) (resp. \(\mathcal{A}^{k}_{0}(s,r,o)\)) denotes the list of the top-\(k\) candidate object (resp. subject) completions to the link prediction query \((s,r,?)\) (resp. \((?,r,o)\)), and \((s,r,o)\models K\) if and only if \((s,r,o)\) satisfies \(K\).

### Empirical KTD Score

Let \(\mathcal{F}=\{x_{i}\}_{i=1}^{m}\), \(\mathcal{G}=\{y_{j}\}_{j=1}^{n}\) two sets of triples that are drawn i.i.d. from two distributions \(\mathbb{P},\mathbb{Q}\) over triples. We compute the empirical KTD score with an _unbiased estimator_[28]\(\mathrm{KTD}_{u}(\mathcal{F},\mathcal{G})\) as

\[\frac{1}{m(m-1)}\sum_{i\neq j}^{m}k(\psi(x_{i}),\psi(x_{j}))+\frac{1}{n(n-1)} \sum_{i\neq j}^{n}k(\psi(y_{i}),\psi(y_{j}))-\frac{2}{mn}\sum_{i=1}^{m}\sum_{ j=1}^{n}k(\psi(x_{i}),\psi(y_{j})).\]

For each data set, we compute the empirical KTD score between \(n=25,000\) triples sampled from GeKCs and \(m\) test triples. In case of \(m>n\), we sample \(n\) triples randomly, uniformly and without replacement from the set of test triples. The time complexity of computing the KTD score is \(\mathcal{O}(nmh)\), where \(h\) denotes the size of triple latent representations (\(h=4000\) in our case, see Section 7.3). For efficiency reasons, we therefore follow Binkowski et al. [4] and randomly extract two batches of 1000 triples each from both the generated and the test triples sets and compute the empirical KTD score on them. We repeat this process 100 times and report the average and standard deviation in Table F.6.

### Experimental Setting

Hyperparameters.All models are trained by gradient descent with either the PLL or the MLE objective (Eqs. (1) and (2)). We set the weights \(\omega_{s},\omega_{r},\omega_{o}\) of the PLL objective all to one, as to retrieve a classical pseudo-log-likelihood [70]. Note that Chen et al. [12] set \(\omega_{s},\omega_{o}\) to one and treat \(\omega_{r}\) as an additional hyperparameter instead that is opportunely tuned. The models are trained until the MRR computed on the validation set does not improve after three consecutive epochs. We fix the embedding size \(d=1000\) for both CP and ComplEx and use Adam [38] as optimiser with \(10^{-3}\) as learning rate. An exception is made for GeKCs obtained via non-negative restriction (Section 4.1), for which a learning rate of \(10^{-2}\) is needed, as we observed very slow convergence rates. We search for the batch size in \(\{5\cdot 10^{2},10^{3},2\cdot 10^{3},5\cdot 10^{3}\}\) based on the validation MRR. Finally, we perform 5 repetitions with different seeds and report the average MRR and two standard deviations in Table F.2.

Parameters initialisation.Following [40, 12], the parameters of CP and CompLex are initialised by sampling from a normal distribution \(\mathcal{N}(0,\sigma^{2})\) with \(\sigma=10^{-3}\). Since the embedding values in CP\({}^{+}\) and CompLex\({}^{+}\) can be interpreted as parameters of categorical distributions over entities and predicates (see Appendix C.1), we initialise them by sampling from a Dirichlet distribution with concentration factors set to \(10^{3}\). To allow unconstrained optimisation for CP\({}^{+}\) and CompLex\({}^{+}\), we represent the embedding values by their logarithm and perform computations directly in log-space, i.e., summations and log-sum-exp operations instead of multiplications and summations, respectively. Moreover, the parameters that ensure the non-negativity of CompLex\({}^{+}\) (see Appendix C.2) are initialised by sampling from a normal distribution \(\mathcal{N}(0,\sigma^{2})\) with \(\sigma=10^{-2}\). We initialise the parameters of CP\({}^{2}\) and CompLex\({}^{2}\) such that the logarithm of the scores are approximately normally distributed and centred in zero during the initial optimisation steps, since this applies for the scores given by CP and CompLex. Such initialisation therefore permits a fairer comparison. To do so, we initialise the embedding values by sampling from a log-normal distribution \(\mathcal{LN}(\mu,\sigma^{2})\), where \(\mu=-\log(d)/3-\sigma^{2}/2\) for CP and \(\mu=-\log(2d)/3-\sigma^{2}/2\) for CompLex, both with \(\sigma=10^{-3}\). The mentioned values for \(\mu\) can be derived via Fenton-Wilkinson approximation [24]. Even though the parameters of GeKCs obtained via non-monotonic squaring are initialised to be non-negative, they are free of becoming negative during training (as we also confirm in practice).

Hardware.Experiments on the smaller knowledge graphs FB15K-237 and WN18RR were run on a single Nvidia GTX 1060 with 6 GiB of memory, while those on the larger ogbl-biokg and ogbl-wikikg2 were run on a single Nvidia RTX A6000 with 48 GiB of memory.

### Additional Experimental Results

#### f.5.1 Link Prediction Results

In this section, we present the additional results regarding the link prediction experiments showed in Section 7.1 and analyse different metrics and learning settings.

Statistical tests and hits at \(k\).Table F.2 shows the best test average MRRs (see Appendix F.2) with two standard deviation and average training time across 5 independent runs with different seeds. We highlight the best results in bold according to a one-sided Mann-Whitney U test with a confidence level of 99%. The showed results in terms of MRRs are also confirmed in Table F.3, which shows the best average Hits@\(k\) (see Appendix F.2) with \(k\in\{1,3,10\}\).

Average log-likelihood.For the best GeKCs for link prediction showed in Table F.2, we report the average log-likelihood of test triples and two standard deviations (across 5 independent runs) in Table F.4. We again highlight the best results in bold, according to a one-sided Mann-Whitney U test.

Quickly distilling parameters.As discussed in Section 4.2, since learned KGE models mostly assign non-negative scores to triples (see Appendix D) we can initialise the parameters of GeKCs obtained by squaring with the parameters of already-learned KGE models, without losing much in terms of link prediction performances. Here, we test this hypothesis and fine-tune GeKCs initialised in this way by using either the PLL or MLE objectives (Eqs. (1) and (2)). To do so, we first collect the parameters of the best CP and CompLex found for link prediction (see Section 7.1). Then, we initialise GeKCs derived by squaring with these parameters and fine-tune them until the MRR computed on validation triples does not improve after three consecutive epochs. We employ Adam [38] as optimiser, and we search for the batch size in \(\{5\cdot 10^{2},10^{3},2\cdot 10^{3},5\cdot 10^{3}\}\) and learning rate in \(\{10^{-3},10^{-4}\}\), as fine-tuning may require a lower learning rate than the one used in previous experiments (see Appendix F.4). Table F.5 shows the MRRs achieved by CP, CompLex and the corresponding GeKCs obtained via squaring that are initialised by distilling the parameters from the already-trained CP and CompLex. On FB15K-237 and WN18RR, distilling parameters induces a substantial improvement in terms of MRR with respect to CP\({}^{2}\) and CompLex\({}^{2}\) whose parameters have been initialised randomly (see Appendix F.4). Furthermore, for CompLex\({}^{2}\) and on WN18RR and ogbl-biokg we achieved similar MRRs with respect to CompLex_without_ the need of fine-tuning.

[MISSING_PAGE_EMPTY:28]

#### f.5.2 Quality of Sampled Triples Results

In this section, we provide additional results regarding the evaluation of the quality of triples sampled by GeKCs (see Section 7.3). For these experiments, we search for the same hyperparameters as for the experiments on link prediction (see Appendix F.4), and train GeKCs until the average log-likelihood computed on validation triples does not improve after three consecutive epochs.

Table F.6 shows the mean empirical KTD score and one standard deviation (see Appendix F.3). In addition, we visualise triple embeddings of sampled and test triples in Fig. F.1 by leveraging t-SNE [69] as a method for visualising high-dimensional data. In particular, we apply the t-SNE method implemented in scikit-learn with perplexity \(50\) and number of iterations \(5\cdot 10^{3}\), while other parameters are fixed to their default value. As showed in Fig. F.1c, an empirical KTD score close to zero translates to an high clusters similarity between embeddings of sampled and test triples.

#### f.5.3 Calibration Diagrams

Existing works on studying the calibration of KGE models are based on interpreting each possible triple \((s,r,o)\) as an independent Bernoulli random variable \(Y_{sro}\) whose likelihood is determined by the score function \(\phi\), i.e., \(\Pr(Y_{sro}=1\mid s,r,o)=\sigma(\phi(s,r,o))\)[61, 54, 79], where \(\sigma\) denotes the logistic function. While GeKCs encode a probability distribution over all possible triples, this does not impede us to reinterpret them to model the likelihood of each \(Y_{sro}\) by still considering scores in log-space as negated energies (see Section 2). Therefore, to evaluate the calibration of GeKCs encoding a non-negative score function \(\phi_{\mathsf{pe}}\) (see Section 4) we compute the probability of a triple \((s,r,o)\) being true as \(p(Y_{sro}=1\mid s,r,o):=\sigma(\log\phi_{\mathsf{pe}}(s,r,o))\). However, the usage of the logistic

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Model** & \multicolumn{2}{c}{FB15k-237} & \multicolumn{2}{c}{WN18RR} & \multicolumn{2}{c}{ogbl-biokq} \\ \cline{2-6} Training set & 0.055 a.007 & & 0.260 a.013 & & 0.029 a.010 \\ Uniform & 0.589 a.012 & & 0.766 a.036 & & 1.822 a.044 \\ NNMFAug & 0.414 a.014 & & 0.607 a.028 & & 0.518 a.035 \\ \hline \multirow{3}{*}{CP*} & PLL & MLE & PLL & MLE & PLL & MLE \\ \cline{2-6}  & 0.404 a.016 & 0.433 a.015 & 0.633 a.033 & **0.578 a.029** & 0.966 a.040 & 0.738 a.030 \\ CP\({}^{2}\) & 0.253 a.014 & **0.070 a.007** & 0.768 a.036 & 0.768 a.036 & 0.039 a.009 & **0.017 a.013** \\ \hline \multirow{3}{*}{Complex*} & CompLEX* & 0.336 a.016 & 0.323 a.015 & 0.456 a.018 & 0.478 a.019 & 0.175 a.019 & 0.097 a.013 \\ \cline{1-1} \cline{2-6}  & 0.326 a.016 & **0.102 a.010** & 0.338 a.020 & **0.278 a.017** & 0.104 a.010 & **0.034 a.007** \\ \hline \hline \end{tabular}
\end{table}
Table F.6: **GeKCs trained by MLE generate new likely triples.** Empirical KTD scores between test triples and triples generated by baselines and GeKCs trained with the PLL objective or by MLE (Eqs. (1) and (2)). Lower is better.

Figure F.1: **Sampled triples are _close_ to test triples.** t-SNE [69] visualisations of the embeddings of test triples (in blue) and triples sampled by CompLEX\({}^{2}\) (in orange). The distribution shift between training and test triples on WN18RR mentioned in Section 7.3 is further confirmed in Fig. F.1b, as it shows a region of test triples (at the bottom and in blue) that is not covered by many generated triples.

function might give misleading results in case of scores not being centred around zero on average. Therefore, we also report calibration diagrams (see paragraph below) where the \(p(Y_{sro}=1\mid s,r,o)\) is obtained via min-max normalisation of the scores given by KGE models (the logarithm of the scores given for GeKCs), where the minimum and maximum are computed on the training triples. Note that several ex-post (re-)calibration techniques are available [61, 79], but they should benefit GeKCs as they do with existing KGE models.

Setting and metrics.To plot calibration diagrams, we follow Socher et al. [59] and sample challenging negative triples, i.e., for each test triple \((s,r,o)\) we sample an unobserved perturbed one \((s,r,\widehat{o})\) by replacing the object with an entity that has appeared at least once with the predicate \(r\) in the training data. We then compute the _empirical calibration error_ (ECE) [79] as \(\mathrm{ECE}:=\frac{1}{b}\sum_{i=1}^{b}|p_{j}-f_{j}|\), where \(b\) is the number of uniformly-chosen bins for the interval \([0,1]\) of triple probabilities, and \(p_{j}\), \(f_{j}\) are respectively the average probability and relative frequency of actually existing triples in the \(j\)-th bin. The lower the ECE score, the better calibrated are the predictions, as they are closer to the empirical frequency of triples that do exist in each bin. The calibration curves are plotted by considering the relative frequency of existing triples in each bin, and curves closer to the main diagonal indicate better calibrated predictions.

GeKCs are more calibrated out-of-the-box.Fig. F.2 (resp. Fig. F.3) show calibration diagrams for GeKCs derived from CP and CompLex trained with the MLE objective (Eq. (2)) (resp. PLL objective (Eq. (1))). In 19 cases over 24, GeKCs obtained via squaring (Section 4.2) achieve lower ECE scores and better calibrated curves than CP and CompLex. While GeKCs obtained via non-negative restriction (Section 4.1) are not well calibrated when using the logistic function, on ogbl-biokg[32] they are still better calibrated than CP and CompLex when probabilities are obtained via min-max normalisation. Furthermore, on WN18RR GeKCs achieved the highest ECE scores (corresponding to poorly-calibrated predictions), which could be explained by the distribution shift between training and test triples that was observed for this KG in Section 7.3 and further confirmed in Appendix F.5.2.

Figure F.2: **Better calibrated predictions with CP\({}^{2}\).** Calibration diagrams of CP, CP\({}^{+}\) and CP\({}^{2}\) trained with either the PLL (Fig. F.2a) or MLE (Fig. F.2b) objectives. The probability of triples are obtained via the application of the logistic function (rows above) and min-max normalisation (rows below). See Appendix F.5.3 for details. The calibration curves for CP\({}^{+}\) where triple probabilities are obtained with the logistic function do not provide any meaningful information, as the logarithm of their scores are generally distributed over large negative values.

Figure F.3: **Better calibrated predictions with CompLex2.** Calibration diagrams of CompLex, CompLex\({}^{+}\) and CompLex\({}^{2}\) trained with either the PLL (Fig. F.3a) or MLE (Fig. F.3b) objectives. The probability of triples are obtained via the application of the logistic function (rows above) and min-max normalisation (rows below). See Appendix F.5.3 for details. The calibration curves for CompLex\({}^{+}\) where triple probabilities are obtained with the logistic function do not provide any meaningful information, as the logarithm of their scores are generally distributed over large negative values.