MoTE: Reconciling Generalization with Specialization for Visual-Language to Video Knowledge Transfer

 Minghao Zhu Zhengpu Wang Mengxian Hu Ronghao Dang Xiao Lin Xun Zhou Chengiu Qijun Chen Tongji University, Shanghai, China

{zmhh_h, wangzhengpu, humengxian, dangronghao, linc_xx, zhouxun, liuchengju, qjchen}@tongji.edu.cn

Corresponding author.

###### Abstract

Transferring visual-language knowledge from large-scale foundation models for video recognition has proved to be effective. To bridge the domain gap, additional parametric modules are added to capture the temporal information. However, zero-shot generalization diminishes with the increase in the number of specialized parameters, making existing works a trade-off between zero-shot and close-set performance. In this paper, we present MoTE, a novel framework that enables generalization and specialization to be balanced in one unified model. Our approach tunes a mixture of temporal experts to learn multiple task views with various degrees of data fitting. To maximally preserve the knowledge of each expert, we propose _Weight Merging Regularization_, which regularizes the merging process of experts in weight space. Additionally with temporal feature modulation to regularize the contribution of temporal feature during test. We achieve a sound balance between zero-shot and close-set video recognition tasks and obtain state-of-the-art or competitive results on various datasets, including Kinetics-400 & 600, UCF, and HMDB. Code is available at https://github.com/ZHHH-H/MoTE.

## 1 Introduction

With the advent of large-scale Vision-Language Models (VLMs), adapting such foundation models (e.g., CLIP [33], ALIGN [13], Florence [54]) for downstream tasks has become an emerging paradigm of intense scrutiny. Their semantic visual concepts empowered by aligning large-scale image-text pairs can be transferred to a wide range of downstream tasks, such as open-vocabulary classification [55, 56], detection [11, 51], and segmentation [21, 52].

The crux of an adaptation procedure for foundation models lies in injecting specialized knowledge of the domain of interest. For video recognition tasks, this necessity is reflected in the fact that the dynamic nature of video data requires effective comprehension of context and temporal correlation by the model. Therefore, to condition the adapted model on the video-specialized knowledge, one general and effective way is to incorporate additional parameters in the form of well-designed prompts [43, 45], adapters [30, 53], and temporal modules [28, 49, 50]. However, we observe that while the increased model capacity brought by more additional parameters enables the better fitting of video-specific inductive bias, it comes at the cost of catastrophically forgetting the generalization knowledge of the original VLM. To better illustrate this phenomenon, we present an overview of existing methods in Figure 1. A clear trade-off problem between zero-shot and close-set performance emerges in existing works, which correlates to the scale of additional parameters. The former relies more on the generalization capability inherent in VLMs while the latter requires intensive video-specialized knowledge, but no method achieves the best of both worlds. On one hand, introducing new knowledge while preserving existing knowledge continually is desirable and significant for the broader adaptation of the foundation model. The rapidly evolving real-world applications also require both specialization and generalization capabilities. However, how to manage the generalization/specialization trade-off of additional parameters in transfer learning remains under-explored.

This paper addresses the aforementioned challenge by delving into two inquiries: **(i)**_How can the generalization capability of additional parameters be enhanced?_ Given the substantially smaller scale of the fine-tuning dataset compared to the pre-training dataset of VLMs, the newly added parameters risk overfitting the fine-tuning data bias, thereby constraining the model generalization. Addressing this question enables steering parameters towards high generalization. Nevertheless, generalization and specialization have proven to be somewhat conflicting [18, 29, 36] in model training, prompting us to further explore **(ii)**_How can generalization and specialization coexist in one unified model?_ This question is crucial for a balanced model to preserve both new and existing knowledge, yet has received limited exploration. We investigate these two questions with a widely employed temporal module [14, 49, 50] (i.e. N-layer transformer), which already features a high specialization degree but is less performant on zero-shot tasks.

With this in mind, we present MoTE, a mixture-of-temporal-experts approach with well-balanced generalization and specialization. We offer a novel perspective in addressing the first question: constructing a more generalized model using multiple data bias views. In contrast to conventional temporal modules that encode patterns with a _single_ feedforward network (FFN) in each Transformer layer, MoTE uses _multiple_ FFN experts to capture various data bias views. During fine-tuning, incoming frame token sequences are routed to one of the temporal experts, keeping the computational cost the same as using a single FFN. To enlarge the discrepancy in knowledge learned by each expert, we devise a routing algorithm based on the multinomial distribution. During inference, we apply weights merging to collapse multiple experts into one module, enabling the patched model to aggregate the generalized knowledge of each expert. For the second question, we propose _Weight Merging Regularization_ which regularizes the merging process of experts in the weight space. The proposed regularizer drives a range of the merged parameters optimal with respect to the task-specific objective, allowing a more effective aggregation of generalized and specialized knowledge in the patched model. To further alleviate the overfitting at test time, we devise a plug-and-play module that modulates the contribution of temporal features by measuring the semantic association between the proxy text features from the fine-tuning and the test datasets.

Our main contributions can be summarized as follows:

* We propose MoTE, a knowledge transfer framework from visual-language to video domain. MoTE tackles the fitting trade-off challenge posed by additional parameters, an aspect largely overlooked by previous works.
* We provide new insights for enhancing parameter generalization from the perspective of data bias fitting, all while keeping the computation cost and final structure constant (SS 3.1, SS 3.2).
* We propose Weight Merging Regularization, a novel regularizer for more effective knowledge aggregation, realizing the coexistence of generalization and specialization in weight space (SS 3.3). Together with Temporal Feature Modulation to further improve the generalization of MoTE (SS 3.4).

Figure 1: Overview of existing VLM knowledge transfer methods. (a) Trade-off plots between zero-shot (Harmonic mean of UCF, HMDB, and K600) and close-set (K400) performance of recent CLIP-based methods (ViT-B/16). (b) As the number of temporal layers increases, the generalization of the standard Transformer layer severely degrades while our proposed MoTE consistently improves the zero-shot and close-set performance. (c) Our proposed MoTE seeks to construct a reconciled feature space between the optimal generalized and specialized manifolds.

* Extensive experiments demonstrate MoTE achieves an optimal trade-off between zero-shot and close-set performance with one unified model. Thorough ablation studies show the scalability and effectiveness of our proposed method (SS 4).

## 2 Preliminary: Transferring CLIP for Video Recognition

Recent works have adapted CLIP [33] to video datasets and obtained superior results [28; 41]. We briefly describe the typical cross-modal video recognition pipeline. Consider a video \(V\) with \(T\) frames and the corresponding text label \(C\) described in a set of textual prompts. Each frame is encoded independently by the CLIP visual encoder \(f(\cdot|\theta_{v})\) with parameter \(\theta_{v}\) and produces frame-level embeddings \(\{\mathbf{e}_{i}\in\mathbb{R}^{D}\}_{i=1}^{T}\). The text embedding \(\mathbf{y}\in\mathbb{R}^{D}\) is generated by the CLIP text encoder \(g(\cdot|\theta_{c})\) with parameter \(\theta_{c}\), where \(D\) is the embedding dimension,

\[\mathbf{e}_{1},\cdots,\mathbf{e}_{T}=f(V|\theta_{v}),\ \ \mathbf{y}=g(C|\theta_{c}).\] (1)

The CLIP visual encoder captures rich spatial information. To bridge the domain gap between image and video, we apply a commonly used temporal module \(h(\cdot|\theta_{tem})\) for cross-frame communication [14; 49; 50], which is parameterized by several Transformer layers. The final video embedding \(\mathbf{z}\in\mathbb{R}^{D}\) consists of spatial and temporal embeddings connected in a residual form:

\[\mathbf{z}=\mathrm{AvgPool}([\mathbf{e}_{1},\cdots,\mathbf{e}_{T}]+h(\mathbf{ e}_{1},\cdots,\mathbf{e}_{T}|\theta_{tem})).\] (2)

During optimization, the text encoder is typically frozen. We tune the visual encoder and the temporal module to maximize the similarity \(\mathrm{sim}(\mathbf{z},\mathbf{y})=\frac{\langle\mathbf{z},\mathbf{y}\rangle }{\|\mathbf{z}\|\|\mathbf{y}\|}\) between the video embedding \(\mathbf{z}\) and the text embedding \(\mathbf{y}\) if \(V\) and \(C\) are matched, otherwise minimize it. The training objective can be formulated as:

\[\mathcal{L}(\theta_{v},\theta_{tem};\mathcal{D})=\mathbb{E}_{{}_{(V,C)\sim \mathcal{D}}}[\mathcal{I}(\mathrm{sim}(\mathbf{z},\mathbf{y}),\mathrm{onehot} (C))],\] (3)

where \(\mathcal{D}\) is the fine-tuning dataset, \(\mathcal{I}\) is the cross-entropy function with softmax operation.

## 3 Methodology

An overview of our proposed method is presented in Figure 2. This section first analyzes the parameter generalization from the perspective of data bias fitting (SS 3.1). Then we detail the structure, routing policy, and inference of MoTE (SS 3.2). Finally, we present Weight Merging Regularization (SS 3.3) and Temporal Feature Modulation (SS 3.4) towards the coexistence of generalization and specialization.

### Intuition and Motivation

Typically, more additional parameters allow for better fitting of the training data distribution, leading to better close-set performance. However, steering the model specialized on the target distribution potentially makes it sensitive to out-of-distribution shifts, resulting in the generalization drop on downstream data distributions with unseen video categories. Although one could enlarge the training data distribution to cover as many potential unseen categories as possible, the costly computation and collection of video data make this infeasible, indicating the need for a more generalized architecture.

Neural network optimization has many solutions in different loss basins due to the non-convexity of the loss landscape. Models optimized with different configurations (e.g. initialization, optimizer, and data) have different optimization trajectories and may converge to separate local minima, thereby capturing various feature patterns. This inspires our method to construct a more generalized model using multiple data bias views. Instead of improving generalization by searching for a flatter minimum in the loss landscape [22; 48], we expect the aggregation of diverse knowledge from multiple minima can provide a more comprehensive representation. Intuitively, diverse temporal patterns can better facilitate recognizing an unseen video category. For example, aggregating information on 'player movement' and 'racket-ball interaction' can help better recognize 'playing tennis'. Our architecture design takes inspiration from Mixture-of-Experts [37] to learn multiple data bias views with a set of experts. While MoE was originally proposed for building large pre-trained models, we extend it to the context of transfer learning and demonstrate its effectiveness in improving parameter generalization.

### Mixture-of-Temporal-Experts

ArchitectureConsider the temporal module \(h(\cdot|\theta_{tem})\) as \(L\) repeated Transformer layers which consist of a self-attention layer and a fully-connected feed-forward network (FFN). For clarity, the temporal module's parameter \(\theta_{tem}\) is factorized into parameter \(\theta_{att}\) for attention layers and parameter \(\theta_{f}\) for FFNs, where \(\theta_{tem}=\theta_{att}\cup\theta_{f}\). Recent studies [5; 9] suggest that factual knowledge is mainly stored in the FFN, which consists of two learnable projection matrices and an activation function. Thus, we replace the FFN in each Transformer layer with \(N\) temporal experts and obtain a set of experts \(\{\{E_{i}^{j}\}_{i=1}^{N}\}_{j=1}^{L}\), where \(E_{i}^{j}\) represents the \(i^{th}\) expert at the \(j^{th}\) layer and has the same structure as the FFN. Each expert starts training from different random initialization to ensure different optimization trajectories, which we experimentally show as critical for learning distinct knowledge (i.e. data bias views). Denote the parameter of the expert \(E_{i}^{j}\) as \(\theta_{i}^{j}\) and the activated expert's index at the \(j^{th}\) layer as \(\mathcal{E}(j)\). The training objective becomes:

\[\mathcal{L}_{\mathrm{TE}}=\mathcal{L}(\theta_{v},\theta_{att},\framebox{$ \{\theta_{\mathcal{E}(j)}^{j}\}_{j=1}^{L}$},\mathcal{D}).\] (4)

Routing PolicyThe routing algorithm of MoE determines which experts process inputs. Classic routing mechanisms (i.e. top-\(k\) gating [37]) make decisions using a learnable gate network and require auxiliary losses to handle load balancing. Instead, we adopt the stochastic routing algorithm [59] considering the architecture brevity, as it avoids the need for extra network, computation, and loss.

Recall that we expect each expert to learn distinct feature patterns. To further enlarge the discrepancy in knowledge learned by each expert, we present a stochastic routing algorithm based on the multinomial distribution. By assigning different activation probabilities to experts, we can control the training data volume of each expert, empowering their knowledge with different degrees of generalization and specialization. Formally, set \(\mathbf{A}=[\alpha_{i}^{l}]_{i=1}^{N}\) to be a random vector in the \(l^{th}\) layer, where \(\alpha_{i}^{l}\in\{0,1\}\) indicates whether the \(i^{th}\) expert is activated and \(\sum_{i=1}^{N}\alpha_{i}^{l}=1\). We specify the variable \(\mathbf{A}\) to follow the multinomial distribution where the activation probability of each expert is given as

\[P(\alpha_{i}^{l}=1)=\frac{\exp(i)}{\sum_{i=1}^{N}\exp(i)},\] (5)

where \(\exp(\cdot)\) is the natural exponential function. This allows experts with a greater index to be activated more likely and therefore receive a larger volume of data during training. Once the expert is selected, all inputs in a given batch are processed by the same expert.

Figure 2: An overview of the MoTE framework. **(Left):** We independently extract the feature of each frame with the CLIP visual encoder. Then, the frame token sequences from a given batch are routed to an activated expert for temporal pattern encoding. To regularize the merging process, we sample the temperature \(\tau\) from a discrete set and use it to collapse multi-experts into one merged FFN. **(Right):** Temporal feature modulation. We modulate the contribution of the temporal feature with the semantic association, which is measured by the similarity between the proxy text features retrieved from the fine-tuning and the test categories. The modulated embedding is used for inference.

Knowledge Aggregation in InferenceDuring inference, knowledge across experts can be aggregated by either merging weights [44; 47] or ensembling logits [16]. We adopt merging weights with the following benefits: (**i**) More effective knowledge aggregation as evidenced in Table 2. (**ii**) Constant computation cost, while the latter increases linearly with the expert number. (**iii**) Consistent parameter structure with its initial state. Specifically, for the experts \(\{E_{i}^{l}\}_{i=1}^{N}\) in the \(l^{th}\) layer, we average the weights of all the experts to obtain the final parameters \(\tilde{\theta^{l}}\) for inference, where

\[\tilde{\theta^{l}}\leftarrow\frac{1}{N}\sum_{i=1}^{N}\theta_{i}^{l}.\] (6)

### Weight Merging Regularization

While the introduction of MoTE notably improves model generalization, it still incurs a decrease in close-set performance as presented in Table 1, which again proves the conflicting nature of generalization and specialization in weight space. We attribute this phenomenon to the difficulty of simultaneously preserving the generalized and specialized knowledge during expert merging process.

In this regard, we propose Weight Merging Regularization to reconcile and promote the aggregation efficacy of expert knowledge. To maximally aggregate the _specialized knowledge_ of experts, the final merged model necessitates explicit optimization with respect to the task-specific objective. As for the preservation of the _generalization knowledge_, previous works [17; 22; 27] observe that the flatness (i.e. sharpness) of local minima in the loss landscape is strongly correlated with the model generalization. They suggest that models with flatter local minima in the loss landscape generalize better on unseen data, where the flatness refers to the robustness of the loss value to perturbations in the parameters. Within a flat basin of the loss landscape, moderate variations of parameters do not lead to significant increases in loss. Inspired by this observation, we propose to facilitate preserving the generalized knowledge of experts by explicitly constructing a _flat region_ around the loss landscape of the final merged model. We optimize the constructed region with the task-specific loss objective and demonstrate that region construction benefits the coexistence of specialization and generalization in one unified model. Formally, for the experts \(\{E_{i}^{l}\}_{i=1}^{N}\) in the \(l^{th}\) layer, we first merge the weights as

\[\phi^{l}\leftarrow\sum_{i=1}^{N}\frac{\exp(i/\tau)}{\sum_{i^{\prime}=1}^{N}\exp (i^{\prime}/\tau)}\cdot\theta_{i}^{l},\] (7)

where \(\tau\) is a temperature parameter used to control the softness of the distribution. By varying the value of \(\tau\), we can build a region of the merged parameters in the weight space. Considering that sampling \(\tau\) in a continuous space may lead to difficulties in model convergence, we sample \(\tau\) from a discrete set given as \(\{\pm 2^{n}\cdot\beta\}_{n=0}^{4}\cup\{\infty\}\), where \(\beta\) is a hyper-parameter of the candidate set. With parameters \(\{\phi^{j}\}_{j=1}^{L}\) merged using different \(\tau\) in each layer, the regularizer is defined as:

\[\mathcal{L}_{\mathrm{WMR}}=\mathcal{L}(\theta_{v},\theta_{att},\overline{\left\{ \phi^{j}\right\}_{j=1}^{L}}\,]\mathcal{D}).\] (8)

Denote the generated video feature when computing \(\mathcal{L}_{\mathrm{WMR}}\) as \(\mathbf{z}_{r}\). We further slightly regularize the consistency between \(\mathbf{z}_{r}\) and the pooled spatial feature e from the CLIP visual encoder:

\[\mathcal{L}_{\mathrm{MSE}}=\mathbb{E}_{\left(V,C\right)\sim\mathcal{D}}[ \left\|\mathbf{z}_{r}-\mathbf{e}\right\|_{2}^{2}],\] (9)

where \(\left\|\cdot\right\|_{2}\) is the L2 norm. This regularizer encourages MoTE to model the temporal dynamics with a light temporal feature, which potentially reduces the model complexity and improves generalization. The overall learning objective can be written as:

\[\mathcal{L}_{\mathrm{ALL}}=\mathcal{L}_{\mathrm{TE}}+\lambda\mathcal{L}_{ \mathrm{WMR}}+\eta\mathcal{L}_{\mathrm{MSE}},\] (10)

where we set \(\lambda=0.5\) and \(\eta=0.1\) by default.

### Temporal Feature Modulation

While applying MoTE to downstream recognition tasks, a potential risk is that its semantic space is limited to the category names of the fine-tuning dataset. To demonstrate this, we collect the category names of Kinetics-400, UCF-101, and Kinetics-600 datasets and manually remove duplicate ones. Weconduct tests with the mixed category names and observe 2.9% closed-set performance drops, 29.8% and 31.1% zero-shot performance drops on Kinetics-400, UCF-101, and Kinetics-600, respectively. That is, the model tends to classify all videos into fine-tuning categories.

In light of this, we propose a test-time adaptation strategy for networks where the temporal module is separated from the CLIP encoder, to measure the confidence of temporal features by means of CLIP's text space and modulates the contribution of temporal features to the prediction. Given the pooled feature \(\mathbf{e}\) from the CLIP visual encoder, we generate fine-tuning and test proxy features \(\mathbf{y}_{f},\mathbf{y}_{t}\in\mathbb{R}^{K\times D}\) by retrieving the \(K\) nearest text features in the fine-tuning and the test dataset categories. We estimate the semantic association \(\rho\) between the two proxy features as

\[\rho=\exp(-(1-\mathcal{M}(\mathbf{y}_{i}\mathbf{y}_{f}^{T}))/\gamma),\] (11)

where \(\gamma\) stands for a scale hyper-parameter and \(\mathcal{M}(\cdot)\) is a sequential maximum and average pooling operation. Denote the pooled temporal feature from MoTE as \(\mathbf{t}\), the final video embedding \(\mathbf{z}\) used for inference becomes \(\mathbf{z}=\mathbf{e}+\rho\cdot\mathbf{t}\).

## 4 Experiments

### Experimental Setup

Architecture.We employ the CLIP [33] pre-trained ViT-B/16 and ViT-L/14 in our experiments. On top of the visual encoder, we add 6 layers MoTE for ViT-L/14 and 4 layers MoTE for ViT-B/16, with 4 temporal experts per layer by default.

Implementation Details.We fine-tune our model using the Kinetics-400 [15] dataset as in previous works [28]. During fine-tuning, we sparsely sample \(T\) (e.g. 8 or 16) frames as the video input. Each input example is randomly cropped and resized to the size of \(224\times 224\) and then undergoes random horizontal flip and random grayscale. We adopt AdamW [25] as the optimizer with a weight decay of 0.2, following a half-period cosine learning rate decay. The initial learning rate is set to \(5\times 10^{-5}\) with a total batch size of 144. Furthermore, we set the candidate set \(\beta\) for \(\mathcal{L}_{\mathrm{WMR}}\) to 0.6 and the scale parameter \(\gamma\) for temporal feature modulation to 0.05, and \(K\) to 5. We apply temporal feature modulation only in evaluation. _Please see supplementary for more details_.

Evaluation Protocols.We thoroughly evaluate our method with close-set, zero-shot, and few-shot video recognition. _Close-set:_ We evaluate the close-set performance on Kinetics-400 [15], using one single clip with a center crop (i.e. \(1\times 1\) views) or 4 clips with 3 crops (i.e. \(4\times 3\) views) per video [28]. Each view contains 8 or 16 sparsely sampled frames. _Zero-shot:_ Following previous works [28; 34], we evaluate zero-shot performance on UCF-101 [38], HMDB-51 [19], and Kinetics-600 [3]. For K600, we adopt the three splits provided by [4]. Each split contains 160 categories out of 220 new categories. In zero-shot setting, we test using \(3\times 1\) views with 8 frames per view. _Few-shot:_ We consider standard K-shot setting and evaluate on UCF-101, HMDB-51, and Something-Something v2 [10]. We adopt a single view for evaluation.

### Ablation Studies

Component-wise analysis of MoTE.In Table 1, we perform in-depth ablations of the proposed components with the ViT-L/14 network. We adopt Text4Vis [49] as our baseline, which serves as a prevalent CLIP adaptation framework in the video domain. Text4Vis uses a 6-layer Transformer for temporal modeling, featuring a high degree of specialization but low generalization capability. We observe that adopting the temporal experts boosts zero-shot performance significantly, validating our idea of improving generalization with multiple data bias views. We then introduce \(\mathcal{L}_{\mathrm{WMR}}\) for achieving the coexistence of generalization and specialization, which facilitates a more efficient aggregation of generalized knowledge while achieving the same level of specialization as Text4Vis. Adding \(\mathcal{L}_{\mathrm{MSE}}\) further improves the zero-shot performance. Moreover, we find that zero-shot performance benefits from modulating the temporal features for unseen categories during evaluation, especially for HMDB51. In summary, MoTE achieves a sound balance between generalization and specialization.

Expert-wise performance of MoTE.To better understand the reconciliation effect of MoTE, we show the performance of each expert and the final merged model in Figure 3. In particular,

[MISSING_PAGE_FAIL:7]

parameters (i.e. without region construction). Due to the conflicting nature of generalization and specialization in optimization, the regularizer without region construction cannot adequately aggregate expert knowledge. Instead, constructing the region in the loss landscape enables generalization and specialization to coexist in one model. The parameter \(\beta\) is used to construct the candidate set of the weights merging temperature. As shown in the table, the performance gain is robust to this parameter.

**Varying numbers of \(\gamma\) for Temporal Feature Modulation.** The parameter \(\gamma\) is used to scale the semantic association to an appropriate interval. In general, \(\gamma=0.05\) yields better results. We find that different datasets exhibit varying sensitivities to this parameter and a relatively small contribution of temporal features generalizes to the unseen categories. _Please see supplementary for more ablations._

### Main Results

Zero-shot video recognition.In Table 3, we compare our method with the state-of-the-art results under the zero-shot setting. Our method achieves new state-of-the-art results on UCF-101 and HMDB-51, with only 8 frames as input. The remarkable performance can be scaled up as the network size and the input frame number increase, demonstrating the great scalability of our method. For K600, although significant improvements are achieved over the baseline, our method still underperforms some methods that do not apply any additional parameters such as Open-VCLIP [46] and MAXI [23]. Since the categories of Kinetics-600 are much more complex, we argue that it is still challenging for randomly initialized parameters to generalize well on such complex unseen categories. Note that our method still outperforms any method that employs additional parameters on K600. Overall, our method presents superior generalization performance.

Close-set and zero-shot performance trade-off.Table 4 presents comparisons with the state-of-the-art methods under the close-set setting. We also list the harmonic mean of the zero-shot results on UCF, HMDB, and K600 as an indicator of the generalization capability, denoted as HM\({}_{\text{ZS}}\). To evaluate the holistic performance in close-set and zero-shot settings, we define the "Trade-off" score as the harmonic mean of Top-1\({}_{\text{K400}}\) and HM\({}_{\text{ZS}}\), as we consider the specialization and generalization capabilities to be equally important for a balanced model.

As shown in the table, our method presents strong close-set results competitive with SOTAs and state-of-the-art zero-shot performance. More importantly, while existing methods can perform well on only one task setting, we achieve superior performance on both settings simultaneously with one unified model. Our method consistently exhibits significant trade-off performance advantages across different networks, evidencing the effectiveness and scalability of MoTE in reconciling generalization and specialization capabilities. Even against ViFi-CLIP [34] which uses different training hyperparameters for close-set and zero-shot settings and takes more frames as the input, our method still shows better balanced performance (74.7% vs. 72.9%). Noteworthy that FROSTER [12] is a strong zero-shot action recognition model but performs less well in the close-set setting, which

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Method & Venue & Encoder & Frames & UCF-101 & HMDB-51 & Kinetics-600 \\ \hline ActionCLIP [42] & arXiv’21 & ViT-B/16 & 32 & 58.3\(\pm\)3.4 & 40.8\(\pm\)5.4 & 67.7\(\pm\)1.1 \\ A5 [14] & ECCV’22 & ViT-B/16 & 32 & 69.3\(\pm\)4.2 & 44.3\(\pm\)2.2 & - \\ X-CLIP [28] & ECCV’22 & ViT-B/16 & 32 & 72.0\(\pm\)2.3 & 44.6\(\pm\)5.2 & 65.2\(\pm\)0.4 \\ ST-Adapter [30] & NeurIPS’22 & ViT-B/16 & 8 & 76.9\(\pm\)0.8 & 51.5\(\pm\)0.6 & 60.2\(\pm\)1.8 \\ Vita-CLIP [45] & CVPR’23 & ViT-B/16 & 832 & 75.0\(\pm\)0.6 & 48.6\(\pm\)0.6 & 67.4\(\pm\)0.5 \\ ViFi-CLIP [34] & CVPR’23 & ViT-B/16 & 32 & 76.8\(\pm\)0.7 & 51.3\(\pm\)0.6 & 71.2\(\pm\)1.0 \\ OTI [58] & ACMMM’23 & ViT-B/16 & 8 & 83.3\(\pm\)0.3 & 54.2\(\pm\)1.3 & 66.9\(\pm\)1.0 \\ Open-VCLIP [46] & ICML’23 & ViT-B/16 & 8 & **83.4\(\pm\)**1.2 & 53.9\(\pm\)1.2 & **73.0\(\pm\)**0.8 \\ MAXI [23] & ICCV’23 & ViT-B/16 & 1632 & 78.2\(\pm\)0.8 & 52.3\(\pm\)0.7 & 71.5\(\pm\)0.8 \\
**MoTE (ours)** & ViT-B/16 & 8 & **83.4\(\pm\)**0.7 & **55.8\(\pm\)**0.9 & 70.2\(\pm\)0.6 \\ \hline X-Florence [28] & ECCV’22 & Florence & 32 & 73.2\(\pm\)4.2 & 48.4\(\pm\)4.9 & 68.8\(\pm\)0.9 \\ Text4Visf [49] & AAAI’23 & ViT-L/14 & 8 & 82.6\(\pm\)0.7 & 52.4\(\pm\)0.4 & 72.1\(\pm\)0.9 \\ OTI [58] & ACMMM’23 & ViT-L/14 & 8 & 88.1\(\pm\)1.0 & 59.3\(\pm\)1.7 & 70.6\(\pm\)0.5 \\ Open-VCLIP [46] & ICML’23 & ViT-L/14 & 8 & 87.6\(\pm\)1.2 & 59.0\(\pm\)0.6 & **81.1\(\pm\)**0.8 \\ DiSTI [32] & ICCV’23 & ViT-L/14 & 32 & 74.9\(\pm\)0.8 & 57.5\(\pm\)1.6 & 75.0\(\pm\)0.7 \\
**MoTE (ours)** & ViT-L/14 & 8 & **88.7\(\pm\)**0.6 & **61.4\(\pm\)**1.3 & 78.4\(\pm\)0.9 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Zero-shot video recognition performance compared with the state-of-the-art methods on UCF-101, HMDB-51, and Kinetics-600. \(\dagger\) denotes reproduced results with our implementation.

[MISSING_PAGE_EMPTY:9]

## 5 Related Work

Video recognition.In the era of deep learning, early works explored diverse variants of convolutional networks for joint spatiotemporal modeling, such as 3D convolution [2; 39] and factorized spatial and temporal convolution [40]. The advent of Transformer then attracted the attention of researchers and has been widely applied for video recognition [1; 6]. In addition, self-supervised learning methods learn transferable representations from large-scale unlabeled data and achieve promising performances [7; 31; 57].

Transferring VLMs for videos.Transferring VLMs for video recognition task has been proven to be effective. ViFi-CLIP [34] suggests that a direct fine-tuning process generalizes well on various settings. Open-VCLIP [46] constructs an open-vocabulary video model by interpolating the model weights and its optimization trajectory. Vita-CLIP [45] extracts discriminative information using multi-level prompts. X-CLIP [28] proposes cross-frame attention and multi-frame integration modules for temporal modeling. Text4Vis [49] and BIKE [50] explore the way for more effective knowledge transfer and use multi-transformer layers to capture temporal cues. FROSTER [12] mitigates catastrophic forgetting by ensuring the learned features do not diverge too far from the frozen ones through distillation. This divergence comes from the variations of both CLIP and additional trainable parameters. Differently, we believe the main cause of the forgetting problem is the overfitting of additional parameters regardless of whether the CLIP parameters are tuned. Note that FROSTER can also potentially improve the generalization of additional parameters through knowledge distillation, but our method presents a more explicit way to achieve this goal. Existing methods face a trade-off of introducing more specialized knowledge or preserving more generalized knowledge. Our work addresses this challenge and presents superior results on both close-set and zero-shot settings.

Mixture-of-Experts.The sparsely activated MoE structure [37] enables the model capacity (i.e. number of parameters) to be vastly scaled up while keeping the computation cost per sample basically unchanged. This technique has been widely investigated in building large-scale pre-trained models in various fields [20; 26; 35]. Several works employ MoE on large-scale language models for parameter-efficient tuning to improve their specialized performance [8; 44]. In this paper, we demonstrate its effectiveness in balancing generalization and specialization in VLM knowledge transfer.

## 6 Conclusion

In this paper, we present MoTE, an effective Visual-Language to video knowledge transfer framework that enjoys both superior generalization and specialization. MoTE leverages a mixture of temporal experts to enhance performance in both close-set and zero-shot video recognition, all while maintaining the conventional temporal module's structure and computational efficiency. With the proposed weight merging regularization and temporal feature modulation, we achieve the coexistence of generalization and specialization in one unified model. Extensive experiments validate MoTE's ability to strike an optimal trade-off between close-set and zero-shot performance.

Figure 4: Visualization of the Top-1 accuracy for each video category sampled from UCF-101 with respect to the merged expert and each individual expert.

## Acknowledgements

This paper is supported by the National Natural Science Foundation of China under Grants (62073245, 62173248, 62233013), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0100) and Innovation Action Plan (22511104900), the Fundamental Research Funds for the Central Universities.

## References

* [1] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, and Cordelia Schmid. Vivit: A video vision transformer. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 6836-6846, 2021.
* [2] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 6299-6308, 2017.
* [3] Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. A short note about kinetics-600. _arXiv preprint arXiv:1808.01340_, 2018.
* [4] Shizhe Chen and Dong Huang. Elaborative rehearsal for zero-shot action recognition. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2021.
* [5] Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In _Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 6491-6506, 2021.
* [6] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 6824-6835, 2021.
* [7] Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, and Kaiming He. A large-scale study on unsupervised spatiotemporal representation learning. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 3299-3309, 2021.
* [8] Ze-Feng Gao, Peiyu Liu, Wayne Xin Zhao, Zhong-Yi Lu, and Ji-Rong Wen. Parameter-efficient mixture-of-experts architecture for pre-trained language models. In _Proceedings of the 29th International Conference on Computational Linguistics_, pages 3263-3273, 2022.
* [9] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. In _Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)_, 2021.
* [10] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The" something something" video database for learning and evaluating visual common sense. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2017.
* [11] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via vision and language knowledge distillation. In _International Conference on Learning Representations (ICLR)_, 2022.
* [12] Xiaohu Huang, Hao Zhou, Kun Yao, and Kai Han. Froster: Frozen clip is a strong teacher for open-vocabulary action recognition. In _International Conference on Learning Representations (ICLR)_, 2024.
* [13] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In _International Conference on Machine Learning (ICML)_, pages 4904-4916, 2021.
* [14] Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and Weidi Xie. Prompting visual-language models for efficient video understanding. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 105-124, 2022.
* [15] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. _arXiv preprint arXiv:1705.06950_, 2017.

* [16] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. Lightgbm: A highly efficient gradient boosting decision tree. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2017.
* [17] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In _International Conference on Learning Representations (ICLR)_, 2016.
* [18] Simon Kornblith, Ting Chen, Honglak Lee, and Mohammad Norouzi. Why do better loss functions lead to less transferable features? In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [19] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. Hmdb: A large video database for human motion recognition. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 2556-2563, 2011.
* [20] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. _arXiv preprint arXiv:2006.16668_, 2020.
* [21] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and Rene Ranftl. Language-driven semantic segmentation. In _International Conference on Learning Representations (ICLR)_, 2021.
* [22] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 31, 2018.
* [23] Wei Lin, Leonid Karlinsky, Nina Shvetsova, Horst Possegger, Mateusz Kozinski, Rameswar Panda, Rogerio Feris, Hilde Kuehne, and Horst Bischof. Match, expand and improve: Unsupervised finetuning for zero-shot action recognition with language knowledge. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2023.
* [24] Xiao Lin, Minghao Zhu, Ronghao Dang, Guangliang Zhou, Shaolong Shu, Feng Lin, Chengju Liu, and Qijun Chen. Clipose: Category-level object pose estimation with pre-trained vision-language knowledge. _IEEE Transactions on Circuits and Systems for Video Technology_, 2024.
* [25] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* [26] Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. Multimodal contrastive learning with limoe: the language-image mixture of experts. In _Advances in Neural Information Processing Systems (NeurIPS)_, pages 9564-9576, 2022.
* [27] Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in deep learning. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2017.
* [28] Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, and Haibin Ling. Expanding language-image pretrained models for general video recognition. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 1-18, 2022.
* [29] Changdae Oh, Mijoo Kim, Hyesu Lim, Junhyeok Park, Euiseog Jeong, Zhi-Qi Cheng, and Kyungwoo Song. Towards calibrated robust fine-tuning of vision-language models. _arXiv preprint arXiv:2311.01723_, 2023.
* [30] Junting Pan, Ziyi Lin, Xiatian Zhu, Jing Shao, and Hongsheng Li. St-adapter: Parameter-efficient image-to-video transfer learning. In _Advances in Neural Information Processing Systems (NeurIPS)_, pages 26462-26477, 2022.
* [31] Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, and Yin Cui. Spatiotemporal contrastive video representation learning. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2021.
* [32] Zhiwu Qing, Shiwei Zhang, Ziyuan Huang, Yingya Zhang, Changxin Gao, Deli Zhao, and Nong Sang. Disentangling spatial and temporal learning for efficient image-to-video transfer learning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2023.
* [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning (ICML)_, pages 8748-8763, 2021.

* [34] Hanoona Rasheed, Muhammad Uzair Khattak, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Fine-tuned clip models are efficient video learners. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 6545-6554, 2023.
* [35] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andre Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. In _Advances in Neural Information Processing Systems (NeurIPS)_, pages 8583-8595, 2021.
* [36] Mert Bulent Sariyildiz, Yannis Kalantidis, Karteek Alahari, and Diane Larlus. No reason for no supervision: Improved generalization in supervised models. In _International Conference on Learning Representations (ICLR)_, 2022.
* [37] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. _arXiv preprint arXiv:1701.06538_, 2017.
* [38] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. _arXiv preprint arXiv:1212.0402_, 2012.
* [39] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 4489-4497, 2015.
* [40] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 6450-6459, 2018.
* [41] Shuyuan Tu, Qi Dai, Zuxuan Wu, Zhi-Qi Cheng, Han Hu, and Yu-Gang Jiang. Implicit temporal modeling with learnable alignment for video recognition. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2023.
* [42] Mengmeng Wang, Jiazheng Xing, and Yong Liu. Actionclip: A new paradigm for video action recognition. _arXiv preprint arXiv:2109.08472_, 2021.
* [43] Qiang Wang, Junlong Du, Ke Yan, and Shouhong Ding. Seeing in flowing: Adapting clip for action recognition with motion prompts learning. In _Proceedings of the 31st ACM International Conference on Multimedia_, pages 5339-5347, 2023.
* [44] Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan, and Jianfeng Gao. Adamix: Mixture-of-adaptations for parameter-efficient model tuning. In _Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 5744-5760, 2022.
* [45] Syed Talal Wasim, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan, and Mubarak Shah. Vita-clip: Video and text adaptive clip via multimodal prompting. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 23034-23044, 2023.
* [46] Zejia Weng, Xitong Yang, Ang Li, Zuxuan Wu, and Yu-Gang Jiang. Open-vclip: Transforming clip to an open-vocabulary video model via interpolated weight optimization. In _International Conference on Machine Learning (ICML)_, pages 36978-36989, 2023.
* [47] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In _International Conference on Machine Learning (ICML)_, pages 23965-23998, 2022.
* [48] Lei Wu, Zhanxing Zhu, et al. Towards understanding generalization of deep learning: Perspective of loss landscapes. _arXiv preprint arXiv:1706.10239_, 2017.
* [49] Wenhao Wu, Zhun Sun, and Wanli Ouyang. Revisiting classifier: Transferring vision-language models for video recognition. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 2847-2855, 2023.
* [50] Wenhao Wu, Xiaohan Wang, Haipeng Luo, Jingdong Wang, Yi Yang, and Wanli Ouyang. Bidirectional cross-modal knowledge exploration for video recognition with pre-trained vision-language models. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 6620-6630, 2023.

* [51] Xiaoshi Wu, Feng Zhu, Rui Zhao, and Hongsheng Li. Cora: Adapting clip for open-vocabulary detection with region prompting and anchor pre-matching. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 7031-7040, 2023.
* [52] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit: Semantic segmentation emerges from text supervision. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 18134-18144, 2022.
* [53] Taojiannan Yang, Yi Zhu, Yusheng Xie, Aston Zhang, Chen Chen, and Mu Li. Aim: Adapting image models for efficient video action recognition. In _International Conference on Learning Representations (ICLR)_, 2022.
* [54] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. _arXiv preprint arXiv:2111.11432_, 2021.
* [55] Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free clip-adapter for better vision-language modeling. In _Proceedings of the European Conference on Computer Vision (ECCV)_, 2022.
* [56] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. _International Journal of Computer Vision (IJCV)_, 130(9):2337-2348, 2022.
* [57] Minghao Zhu, Xiao Lin, Ronghao Dang, Chengju Liu, and Qijun Chen. Fine-grained spatiotemporal motion alignment for contrastive video representation learning. In _Proceedings of the 31st ACM International Conference on Multimedia_, pages 4725-4736, 2023.
* [58] Yan Zhu, Junbao Zhuo, Bin Ma, Jiajia Geng, Xiaoming Wei, Xiaolin Wei, and Shuhui Wang. Orthogonal temporal interpolation for zero-shot video recognition. In _Proceedings of the 31st ACM International Conference on Multimedia_, pages 7491-7501, 2023.
* [59] Simiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, Jianfeng Gao, and Tuo Zhao. Taming sparsely activated transformer with stochastic experts. In _International Conference on Learning Representations (ICLR)_, 2021.

[MISSING_PAGE_FAIL:15]

original full dataset. In few-shot settings, we fine-tune models for 2 epochs on SSv2 and 10 epochs on UCF-101 and HMDB-51. We adopt a single view for evaluation.

## Appendix C Additional Ablations

Optional architecture of temporal expert.By default, we replace the whole FFN with experts of the same structure as the FFN. Optionally, we can replace one or both of the projection matrices in the FFN with experts of the same structure, as illustrated in Figure 5. We study the optional architecture design of the temporal expert in Table 6(a). As shown in the table, we observe degraded performance with projection-level expert designs. We argue that the shared projection matrix or two-stage routing design of the projection-level expert allows knowledge communication between experts, such implicit communication may lead to mutual lifting between the experts. Therefore, preventing the knowledge exchange across experts is critical for learning diverse generalized knowledge.

Varying neighbor numbers \(K\) for Temporal Feature Modulation.In Section 3.4 of the main manuscript, we generate fine-tuning proxy features \(\mathbf{y}_{f}\) and test proxy features \(\mathbf{y}_{t}\) by retrieving the \(K\) nearest text features in the fine-tuning and the test dataset categories. We study the influence of various \(K\) in Table 6(b). We find that a large \(K\) leads to a slight performance degradation as the retrieved text features may not represent the semantic information of the video properly. Overall, our performance gains are stable for this parameter.

Various Loss Types of Weight Merging Regularization.In Section 3.3 of the main manuscript, we design the weight merging regularization \(\mathcal{L}_{\mathrm{WMR}}\) based on the cross-entropy loss function. In this study, we use other loss function types to redesign the weight merging regularization \(\mathcal{L}_{\mathrm{WMR}}\). Note that our training procedure requires two forward passes of the temporal module, one when

\begin{table}

\end{table}
Table 7: Additional ablation studies. Default settings are colored in gray.

Figure 5: Illustration of optional architecture designs for the temporal expert. We omit the activation function between the projection matrices for brevity.

optimizing the activated expert with \(\mathcal{L}_{\mathrm{TE}}\) and the second in calculating weight merging regularization \(\mathcal{L}_{\mathrm{WMR}}\). When we optimize \(\mathcal{L}_{\mathrm{WMR}}\) using cross entropy, we use the ground truth label of the video as the supervised signal. We try to redesign \(\mathcal{L}_{\mathrm{WMR}}\) with other supervisions. (i) KL Divergence: We adopt the logits generated during the computation of \(\mathcal{L}_{\mathrm{TE}}\) as the supervision and optimize with KL divergence loss, denoted as \(\mathcal{L}_{\mathrm{WMR}\_\mathrm{KL}}\). (ii) Mean Square Error: We use the output feature when computing \(\mathcal{L}_{\mathrm{TE}}\) as the supervision and optimizing with MSE loss, denoted as \(\mathcal{L}_{\mathrm{WMR}\_\mathrm{MSE}}\). The results are presented in Table 6(c). Interestingly, we find that the MSE loss achieves notable results in a weakly supervised manner, which indicates the potential scalability of our proposed approach.

Training cost analysis of MoTE.We report the actual training time of our method with respect to the baseline in Table 6(d). The wall-clock time of training is benchmarked on 3 4090 GPUs with a batch size of 144. GPU days are calculated by the number of GPUs multiplied by the training time in days. As shown in the table, applying the mixture of temporal experts does not introduce additional training overhead over baseline, which can be attributed to the use of the routing strategy. Adding \(\mathcal{L}_{\mathrm{WMR}}\) and \(\mathcal{L}_{\mathrm{MSE}}\) brings a +0.16 days training time increase since it requires an additional forward pass of the temporal module.

Transferring CLIP with small-scale fine-tuning data.In the main text, our experimental setting follows previous works in fine-tuning on large-scale K400 and then evaluating zero-shot performance on relatively small downstream datasets. To further investigate MoTE's capabilities when the fine-tuning data are limited, we train the ViT-L/14 network on UCF-101 and evaluate it on K400, results are shown in Table 6(e). MoTE yields a 0.5 improvement over the Baseline on UCF-101, while zero-shot performance on K400 significantly outperforms both raw CLIP and Baseline. This demonstrates the applicability of MoTE to small-scale datasets and its ability to learn generalized knowledge from limited data.

Effect of temperature selection schemes.In this study, we compare our discrete temperature sampling strategy with the continuous space selection schemes. We implement two continuous space selection schemes. (1) Sampling from a continuous standard normal distribution (mean=0, variance=1). (2) Sampling from a continuous uniform distribution. As shown in Table 6(f), the continuous space sampling strategy results in a notable performance degradation.

## Appendix D Textual Prompt Templates

In our work, we adopt a set of hand-craft textual prompt templates to generate text embeddings. Following CLIP [33], we perform prompt ensembling over the 28 templates in order to provide comprehensive semantics. The templates are listed in Table 8.

Figure 6: Visualization of attention maps. We show the RGB image and the attention maps of the merged expert, expert_0, and expert_4.

## Appendix E Qualitative Analysis

### Model Attention

To better understand what knowledge the experts capture, we present the visualization of the model attention in Figure 6. We show the attention map of the merged expert, expert_0, and expert_4 to explore whether diverse knowledge is learned across experts. Since MoTE only requires the input of frame-level tokens from the CLIP visual encoder, we calculate the attention map between the temporal video features output by MoTE and the image patch tokens from the CLIP encoder. As shown in the figure, experts_0 and experts_4 always focus on different regions, indicating that they capture various temporal patterns. Besides, we observe that the merged expert is able to focus on more precise and broader foreground areas. This phenomenon suggests that the merged expert sufficiently aggregates and leverages the knowledge from different experts.

### Representation similarity across experts.

We visualize the representation similarities across each expert and the final merged model in Figure 7. The representation similarities are averaged on 100 randomly sampled data of unseen categories from the K600 dataset. As show in the affinity map, the different similarities demonstrate that each expert learns distinctive knowledge from different optimization trajectories. Besides, we observe that the similarities between the merged model and each expert are relatively stable, indicating that the merged expert efficiently leverages the knowledge contained in each expert.

\begin{table}
\begin{tabular}{l} \hline \hline Templates \\ \hline ‘a photo of \{category\}.’ \\ ‘a photo of a person \{category\}.’ \\ ‘a photo of a person using \{category\}.’ \\ ‘a photo of a person doing \{category\}.’ \\ ‘a photo of a person during \{category\}.’ \\ ‘a photo of a person performing \{category\}.’ \\ ‘a photo of a person practicing \{category\}.’ \\ ‘a video of \{category\}.’ \\ ‘a video of a person \{category\}.’ \\ ‘a video of a person using \{category\}.’ \\ ‘a video of a person doing \{category\}.’ \\ ‘a video of a person during \{category\}.’ \\ ‘a video of a person performing \{category\}.’ \\ ‘a video of a person practicing \{category\}.’ \\ ‘a example of \{category\}.’ \\ ‘a example of a person using \{category\}.’ \\ ‘a example of a person doing \{category\}.’ \\ ‘a example of a person during \{category\}.’ \\ ‘a example of a person performing \{category\}.’ \\ ‘a example of a person practicing \{category\}.’ \\ ‘a demonstration of a person \{category\}.’ \\ ‘a demonstration of a person using \{category\}.’ \\ ‘a demonstration of a person doing \{category\}.’ \\ ‘a demonstration of a person during \{category\}.’ \\ ‘a demonstration of a person performing \{category\}.’ \\ ‘a demonstration of a person practicing \{category\}.’ \\ \hline \hline \end{tabular}
\end{table}
Table 8: Textual prompt templates of MoTE.

## Appendix F Dataset Details

Kinetics-400 [15]is a large-scale dataset in the video domain. The dataset contains \(\sim\)240k training videos and \(\sim\)20k validation videos in 400 human action categories, with an average length of 10 seconds. The high quality of the dataset makes it the most popular benchmark for video recognition

Kinetics-600 [3]is an extension of Kinetics-400, consisting of \(\sim\)392k training videos, \(\sim\)30k validation videos, and \(\sim\)60k test videos in 600 human action categories. The dataset contains an additional 220 new action categories over Kinetics-400. We evaluate the zero-shot performance on 220 new categories and adopt three splits provided by the previous work [4]. We use its test set for evaluation and report the average performance on three splits.

Ucf-101 [38]is an action recognition dataset that contains 13,320 videos in 101 action categories, collected from YouTube. There are three official splits of training data and validation data.

Hmdb-51 [19]contains 7k videos in 51 action categories, collected from movie clips and web videos. There are three official splits of the dataset, each with 3,570 training data and 1,530 validation data. is a collection of realistic videos from various sources, including movies and web videos. The dataset comprises 7,000 video clips from 51 action categories.

Something-Something V2 [10]is a temporal-heavy dataset that requires the fine-grained temporal understanding capability of the model. It contains 220,000 videos in 174 action categories.

Figure 7: Visualization of representation similarities across each expert and the final merged model.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our main contributions are detailed in lines 6-14 of the abstract and lines 67-77 of the introduction.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We analyze the limitations of this paper in Section A of the Appendix.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: As an applied paper, we aim to provide a more generalized and effective VLMs knowledge transfer framework for video recognition tasks. We do not include theoretical results in this paper. Note that all assumptions and conclusions mentioned in this paper are precisely cited.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We detail the implementation details of the paper in Section 4.1 of the main manuscript and Section B of the Appendix. Besides, we provide our code in the supplementary zip file. The code will also be released when the paper is accepted.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide our code in the supplementary zip file with a detailed instruction to reproduce our method. The code will also be released when the paper is accepted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We specify all training and evaluation details of our method in Section 4.1 of the main manuscript and Section B of the Appendix. We also provide statistics details for the dataset used in this paper in Section F of the Appendix.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes],Justification: For zero-shot experiments, following the previous works, we evaluate our method on three splits of UCF-101, HMDB-51, and K600. We report the mean accuracy and the standard deviation of performance of the three splits on each dataset, respectively. As for close-set and few-shot experiments, considering the computational effort, we do not include error bars following previous works.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We report computational resources and training costs in Section B and Section C of the Appendix.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: : All authors have read the NeurIPS Code of Ethics.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the potential broader impacts in Section A of the Appendix.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper poses no such risks.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All references and datasets in this paper are precisely cited and licensed with corresponding licenses.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide detailed instructions on submitted assets. The details of the dataset and model are reported in our paper.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects**Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

Answer: [NA]

Justification: This paper poses no such risks.