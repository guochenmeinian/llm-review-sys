# How to Backdoor HyperNetwork in Personalized Federated Learning?

###### Abstract

This paper explores previously unknown backdoor risks in HyperNet-based personalized federated learning (HyperNetFL) through poisoning attacks. Based upon that, we propose a novel model transferring attack (called HNTroj), i.e., the first of its kind, to transfer a local backdoor infected model to all legitimate and personalized local models, which are generated by the HyperNetFL model, through consistent and effective malicious local gradients computed across all compromised clients in the whole training process. As a result, HNTroj reduces the number of compromised clients needed to successfully launch the attack without any observable signs of sudden shifts or degradation regarding model utility on legitimate data samples making our attack stealthy. To defend against HNTroj, we adapted several backdoor-resistant FL training algorithms into HyperNetFL. An extensive experiment that is carried out using several benchmark datasets shows that HNTroj significantly outperforms data poisoning and model replacement attacks and bypasses robust training algorithms even with modest numbers of compromised clients.

## 1 Introduction

Recent data privacy regulations [1] pose significant challenges for machine learning (ML) applications that collect sensitive user data. Federated learning (FL) [2] is a promising way to address these challenges by enabling clients to jointly train ML models via a coordinating server without sharing their data. Although offering better data privacy, FL typically suffers from the disparity of model performance caused by the non-independent and identically distributed (non-iid) data distribution across clients [3]. One of the state-of-the-art approaches to address this problem in FL is using a single joint neural network, called HyperNetFL, to generate local models using personalized descriptors optimized for each client independently [4]. This allows us to perform smart gradient and parameter sharing.

Despite the superior performance, the unique training approach of HyperNetFL poses previously unknown risks to backdoor attacks typically carried out through poisoning in FL [5]. In backdoor attacks, an adversary manipulates the training process to cause model misclassification on a subset of chosen data samples [6; 7]. In FL, the adversary tries to construct malicious gradients or model updates that encode the backdoor. When aggregated with other clients' updates, the aggregated model exhibits the backdoor.

We investigate backdoor attacks against HyperNetFL and formulate robust HyperNetFL training as defenses. Our developed attack (called HNTroj) is based on consistently and effectively crafting malicious local gradients across compromised clients using a single backdoor-infected model to enforce HyperNetFL generating local backdoor-infected models disregarding their personalized descriptors. An extensive analysis and evaluation in non-iid settings show that HNTroj notably outperforms existing model replacement and data poisoning attacks bypassing recently developed robust federated training algorithms adapted to HyperNetFL with small numbers of compromised clients. Therefore, it is challenging to defend HNTroj.

Background

This section briefly reviews HyperNetFL and backdoor and poisoning attacks. More information is in Appx. A.

**HyperNetwork-based Personalized FL (HyperNetFL).** To address the disparity of model utility across clients, HyperNetFL [4, 8] uses a neural network \(h(v_{i},\varphi)\) located at the server to output the weights \(\theta_{i}\) for each client \(i\) using a (trainable) descriptor \(v_{i}\) as input and model weights \(\varphi\), that is, \(\theta_{i}=h(v_{i},\varphi)\). HyperNetFL offers a natural way to share information across clients through the weights \(\varphi\) while maintaining the personalization of each client via the descriptor \(v_{i}\). To achieve this goal, the clients and the server will try to minimize their loss functions: \(\operatorname*{arg\,min}_{\varphi,\{v_{i}\}_{i\in[N]}}\frac{1}{N}\sum_{i\in[N] }L_{i}(h(v_{i},\varphi))\).

**Backdoor and Poisoning Attacks.** Training time poisoning attacks against ML and FL models can be classified into byzantine and backdoor attacks. In byzantine attacks, the adversarial goal is to degrade or severely damage the model test accuracy [9, 10, 11]. Byzantine attacks are relatively detectable by tracking the model accuracy on validation data [12]. Meanwhile, in backdoor attacks, the adversarial goal is to cause model misclassification on a set of chosen inputs without affecting model accuracy on legitimate data samples. A well-known way to carry out backdoor attacks is using Trojans [13, 14]. A Trojan is a carefully crafted pattern, e.g., a brand logo, blank pixels, added into legitimate samples causing the desired misclassification. A recently developed image warping-based Trojan mildly deforms an image by applying a geometric transformation [15] to make it unnoticeable to humans and bypass all well-known Trojan detection methods, such as Neural Cleanse [16], Fine-Pruning [17], and STRIP [18]. The adversary applies the Trojan on legitimate data samples to activate the backdoor at the inference time.

The training data is scattered across clients in FL, and the server only observes local gradients. Therefore, backdoor attacks are typically carried by a small set of compromised clients fully controlled by an adversary to construct malicious local gradients and send them to the server. The adversary can apply data poisoning (DPois) and model replacement approaches to create malicious local gradients. In DPois[19, 20], compromised clients train their local models on Trojaned datasets to construct malicious local gradients, such that the aggregated model at the server exhibits the backdoor. DPois may take many training rounds to implant the backdoor into the aggregated model. Meanwhile, in model replacement [6], the adversary constructs malicious local gradients, such that the aggregated model at the server will closely approximate or be replaced by a predefined Trojaned model. To some extent, model replacement is highly severe since it can be effective after only one training round.

To our knowledge, _these attacks are not primarily designed for HyperNetFL, in which there is no aggregated model \(\theta\) at the server_. That poses an unknown risk of backdoors through poisoning attacks in HyperNetFL.

## 3 Model Transferring Attack

To overcome the lack of consistency in deriving the malicious local gradients in DPois and avoid sudden shifts in model utility on legitimate data samples in HNRepl, we propose in this work a novel _model transferring attack_ (HNTroj) against HyperNetFL.

In HNroj (Alg. 1), our idea is to replace \(\theta_{t}^{c}\) with a Trojaned model \(X\) across all the compromised clients \(c\in\complement\) and in all communication rounds \(t\) to compute the malicious local gradients: \(\forall c\in\complement,t\in[T]:\triangle\bar{\theta}_{t}^{c}=\psi_{t}^{c}\big{[} X-h(v_{c},\varphi)\big{]}\), where \(\psi_{t}^{c}\) is a dynamic learning rate randomly sampled following a specific distribution, e.g., uniform distribution \(\mathcal{U}[a,b]\), \(a<b\), and \(a,b\in(0,1]\). In practice, the adversary can collect its own data to locally train \(X\). The collected data can be uniformly distributed across all classes to maximize the backdoor successful rate across all legitimate clients, whose data distribution is non-iid. This is because the server does not know the clients' local data distribution.

By doing so, we achieve several key advantages, as follows:

**(1)** The gradients \(\triangle\bar{\theta}_{t}^{c}\) become more effective in creating backdoors, since \(X\) is a better optimized Trojaned model than \(\{\theta_{t}^{c*}\}_{c\in\complement}\).

**(2)** The gradients across compromised clients synergistically approximate the Trojaned surrogate loss (Eq. 9, Appx. B) to closely align the outputs of \(h(\cdot,\varphi)\) to a unified Trojan model \(X\) through the term \(\frac{1}{2}\sum_{c\in\complement}\|X-h(v_{c},\varphi)\|_{2}^{2}\) disregarding the varying descriptors \(\{v_{i}\}_{i\in N}\) and their dissimilar local datasets. The new Trojaned surrogate loss is: \(\frac{1}{2}(\sum_{c\in\complement}\|X-h(v_{c},\varphi)\|_{2}^{2}+\sum_{i\in N \setminus\complement}\|\theta^{i*}-h(v_{i},\varphi)\|_{2}^{2})\).

**(3)** The gradients \(\triangle\bar{\theta}_{t}^{c}\) become stealthier since updating the HyperNetFL with \(\triangle\bar{\theta}_{t}^{c}\) will significantly improve the model utility on legitimate data samples. This is because \(X\) has a better model utility on legitimate data samples than the local models of legitimate clients \(\{\theta_{t}^{i*}\}_{i\in N\setminus\complement}\). More importantly, _by keeping the random and dynamic learning rate \(\psi_{t}^{c}\) only known to the compromised client \(c\)_, we can prevent the server from tracking \(X\) or identifying some suspicious behavior patterns from the compromised client.

We also quantify the server's estimation error \(Error\) as follows.

Assuming that the server can identify compromised clients with a precision value \(p\), we can quantify the server's estimation error bounds of the Trojaned model \(X\), as follows. The server's set of identified compromised clients consists of \(p\|\complement\) compromised clients \(\complement\) and \((1-p)\|\complement\) legitimate clients \(\bar{L}\). The estimated Trojaned model \(X^{\prime}=\sum_{c\in\complement,L}\theta_{t}^{c}/|\complement|\). Then, the estimation error of \(X\) is computed and bounded as follows:

\[Error =\|\sum_{c\in\complement}\frac{\theta_{t}^{c}}{p|C|}+\sum_{i\in \bar{L}}\frac{\theta_{t}^{i}}{(1-p)(N-|C|)}-X\|_{2}\] \[=\|X^{\prime}-X\|_{2}=\|\sum_{c\in\complement,L}\frac{\theta_{t} ^{c}}{|\complement|}-X\|_{2}\] (1)

in which

\[\|X^{\prime}-X\|_{2}\geq\|\sum_{c\in\complement}\theta_{t}^{c}/(p|\complement )-X\|_{2}=\|\sum_{c\in\complement}\frac{\triangle\bar{\theta}_{t}^{c}}{p|\complement |\psi_{t}^{c}}\|_{2}\geq\|\sum_{c\in\complement}\frac{\triangle\bar{\theta}_{t }^{c}}{p|\complement|b}\|_{2}\] (2)

and

\[\|\sum_{c\in\complement,L}\theta_{t}^{c}/|\complement|-X\|_{2}\leq\arg\max_{L \subseteq N\text{\,st. }|L|=|\complement|}\|\sum_{i\in L}\theta_{t}^{i}/|L|-X\|_{2}\] (3)

From Eqs. 2 and 3, we have the following error bounds:

\[\big{\|}\sum_{c\in\complement}\frac{\triangle\bar{\theta}_{t}^{c}}{p|\complement |b}\big{\|}_{2}\leq Error\leq\arg\max_{L\subseteq N\text{\,st.}|L|=|\complement |}\big{\|}\sum_{i\in L}\frac{\theta_{t}^{i}}{|L|}-X\big{\|}_{2}\] (4)We observe that: 1) The lower precision in detecting compromised clients (smaller \(p\)) results in a larger \(Error\) approaching the upper bound; 2) The smaller \(\psi_{t}^{c}\), the higher lower bound of \(Error\) is; and 3) If the gradient \(\triangle\bar{\theta}_{t}^{c}\) is too small, we can uniformly upscale its \(L_{2}\)-norm to be a small constant (denoted \(\tau\)) to enlarge the lower bound of \(Error\) without affecting the model utility or backdoor success rate. Fig. 1 illustrates the lower bound of \(Error\) given the most favorable precision to the server (\(p=1\)) with different values of \(\bar{\mathsf{U}}\). After 1,000 rounds, \(Error\) is not further reduced since it is controlled by the lower bound (with \(\tau=2\) in this study). That prevents the server from accurately estimating \(X\).

**(4)** The following Theorem 1 shows that the \(L_{2}\)-norm distance between the local model \(\theta_{t}^{c}\) of a compromised client generated by the HyperNetFL \(h(v_{c},\varphi)\) and \(X\), i.e., \(\|\theta_{t}^{c}-X\|_{2}\), is bounded by \((\frac{1}{a}-1)\|\bigtriangleup\bar{\theta}_{t^{\prime}}^{c}\|_{2}+\|\bar{ \xi}\|_{2}\), where \(t^{\prime}\) is the closest round the compromised client \(c\) participated in before \(t\), and \(\xi\in\mathcal{R}^{m}\) is a small error rate. When the HyperNetFL model converges, e.g., \(t^{\prime},t\cong T\), \(\|\xi\|_{2}\) become tiny and \(\|\bigtriangleup\bar{\theta}_{t^{\prime}}^{c}\|_{2}\) is bounded by a small constant \(\tau\) ensuring that given the compromised client, \(\bar{\theta}_{T}^{c}=h(v_{c},\varphi)\) converges into a bounded and low loss area surrounding \(X\) (\(\|\theta_{T}^{c}-X\|_{2}\) is tiny) to imitate the normal training process.

Consequently, HNTroj requires a smaller number of compromised clients to be highly effective. Also, HNTroj is stealthier than the (white-box) HNRepl and DPois by avoiding degradation and shifts in model utility on legitimate data samples during the whole poisoning process.

**Theorem 1**.: _For a compromised client \(c\) participating in a round \(t\in[T]\), we have that the \(L_{2}\)-norm distance between the HyperNetFL output \(\theta_{t}^{c}=h(v_{c},\varphi)\) and the Trojaned model \(X\) is always bounded as follows:_

\[\|\theta_{t}^{c}-X\|_{2}\leq(1/a-1)\|\bigtriangleup\bar{\theta}_{t^{\prime}} ^{c}\|_{2}+\|\xi\|_{2}\] (5)

_where \(\forall t:\psi_{t}\sim\mathcal{U}[a,b]\), \(a<b\), \(a,b\in(0,1]\), \(t^{\prime}\) is the closest round the compromised client \(c\) participated in, and \(\xi\in\mathcal{R}^{m}\) is a small error rate._

Proof.: At the round \(t^{\prime}\), we have that \(\bigtriangleup\bar{\theta}_{t^{\prime}}^{c}=\psi_{t^{\prime}}^{c}[X-\theta_{t ^{\prime}}^{c}]\). This is equivalent to \(X=\frac{\bigtriangleup\bar{\theta}_{t^{\prime}}^{c}}{\psi_{t^{\prime}}^{c}}+ \theta_{t^{\prime}}^{c}\). At the round \(t\), the HyperNetFL \(h(v_{c},\varphi)\) supposes to generate a better local model for the compromised client \(c\): \(\theta_{t}^{c}=\bigtriangleup\bar{\theta}_{t^{\prime}}^{c}+\theta_{t^{\prime} }^{c}+\xi\). To quantify the distance between the generated local model \(\theta_{t}^{c}\) and the Trojaned model \(X\), we subtract \(\theta_{t}^{c}\) by \(X\) as follows: \(\theta_{t}^{c}-X=(1-\frac{1}{\psi_{t^{\prime}}^{c}})\bigtriangleup\bar{\theta} _{t^{\prime}}^{c}+\xi\). Based upon this, we can bound the \(l_{2}\)-norm of the distance \(\theta_{t}^{c}-X\) as follows:

\[\|\theta_{t}^{c}-X\|_{2}=\|(1-\frac{1}{\psi_{t^{\prime}}^{c}})\bigtriangleup \bar{\theta}_{t^{\prime}}^{c}+\xi\|_{2}\leq(\frac{1}{a}-1)\|\bigtriangleup\bar{ \theta}_{t^{\prime}}^{c}\|_{2}+\|\xi\|_{2}\] (6)

Consequently, Theorem 1 holds. 

## 4 Experimental Results

We focus on answering the following three questions in our evaluation: **(1)** Whether HNTroj is effective in HyperNetFL? **(2)** What is the percentage of compromised clients required for an effective attack? and **(3)** How difficult it is to defend against HNTroj?

**Data and Model Configuration.** We conduct an extensive experiment on CIFAR-10 [21] and Fashion MNIST datasets [22]. For both datasets, we have \(100\) clients in which the data is non-iid

Figure 1: Estimation error of HNTroj with \(p=1\).

across clients and use the class \(0\) as a targeted class \(y_{j}^{b}\). We adopt the model configuration described in [4]. We use WaNet [15] for generating backdoor data to train \(X\). More information is in Appx. D.

**Evaluation Metrics.** For evaluation, we use:

\[\text{Legitimate ACC}=\frac{1}{N}\sum_{i\in[N]}\frac{1}{n_{i}^{ \tau}}\sum_{j\in[n_{i}^{\tau}]}Acc\big{(}f(x_{j}^{i},\theta^{i}),y_{j}^{i}\big{)}\] \[\text{Backdoor SR}=\frac{1}{N}\sum_{i\in[N]}\frac{1}{n_{i}^{\tau} }\sum_{j\in[n_{j}^{\tau}]}Acc\big{(}f(\overline{x}_{j}^{i},\theta^{i}),y_{j}^{ i,b}\big{)}\]

where \(\overline{x}_{j}^{i}=x_{j}^{i}+\mathcal{T}\) is a Trojaned sample, \(Acc(y^{\prime},y)=1\) if \(y^{\prime}=y\); otherwise \(Acc(y^{\prime},y)=0\) and \(n_{i}^{\tau}\) is the number of testing samples in client \(i\).

**HNTroj v.s. DPois and White-box HNRepl.** Figs. 2 and 10a (Appx. D) present legitimate ACC and backdoor SR of each attack and the clean model (i.e., trained without poisoning) as a function of the communication round \(t\) and the number of compromised clients \(\complement{C}\) under a defense free environment in the CIFAR-10 dataset. It is obvious that HNToj significantly outperforms DPois. HNToj requires a notably small number of compromised clients to successfully backdoor the HyperNetFL with high backdoor SR, i.e., \(43.92\%\), \(64.00\%\), \(70.38\%\), \(73.45\%\), and \(75.70\%\) compared with \(10.58\%\), \(17.87\%\), \(29.90\%\), \(41.53\%\), and \(50.57\%\) of the DPois given \(1\), \(5\), \(10\), \(15\), and \(20\) compromised clients, without an undue cost in legitimate ACC, i.e., \(76.89\%\).

In addition, HNToj does not introduce degradation or sudden shifts in legitimate ACC during the training process, regardless of the number of compromised clients, making it stealthier than DPois and (white-box) HNRepl. This is because we consistently poison the HyperNetFL with a relatively good model \(X\), which achieves \(74.26\%\) legitimate ACC and \(85.92\%\) backdoor SR, addressing the inconsistency in deriving the malicious local gradients. There is a small legitimate ACC gap between HNToj and the clean model, i.e., \(6.11\%\) in average. However, this gap will not be noticed by the server since the clean model is invisible to the server when the compromised clients are present.

**HNToj v.s. \(\alpha\)-Trimmed Norm.** Since HNToj outperforms other poisoning attacks, we now focus on understanding its performance under robust HyperNetFL training. Fig. 3 shows the performance of \(\alpha\)-trimmed norm against HNToj as a function of the number of compromised clients \(\complement{C}\). There

Figure 3: Legitimate ACC and backdoor SR under \(\alpha\)-trimmed norm defense in the CIFAR-10 dataset.

Figure 2: Legitimate ACC and Backdoor SR comparison for DPois, HNToj, and Clean model over different numbers of compromised clients in the CIFAR-10 dataset. (Fig. 2a has the same legend as in Fig. 2b).

are three key observations from the results, as follows: **(1)** Applying \(\alpha\)-trimmed norm does reduce the backdoor SR, especially when the number of compromised clients is small, i.e., backdoor SR drops \(39.72\%\) in average given \(\complement\in[1,10]\). However, when the number of compromised clients is a little bit larger, the backdoor SR is still at highly severe levels, i.e., \(51.70\%\sim 70.55\%\) given \(15\) to \(20\) compromised clients, regardless of a wide range of trimming level \(\alpha\in[0.1,0.4]\); **(2)** The larger the \(\alpha\) is, the lower the backdoor SR tends to be. This good result comes with a toll on the legitimate ACC, which is notably reduced when \(\alpha\) is larger. In average, the legitimate ACC drops from \(79.75\%\) to \(67.66\%\) and \(62.29\%\) given \(\alpha\in[0.3,0.4]\) and \(\complement\in[1,10]\), respectively. That clearly highlights a non-trivial trade-off between legitimate ACC and backdoor SR given attacks and defenses; and **(3)** The more compromised clients we have, the better the legitimate ACC is when the trimming level \(\alpha\) is large, i.e., \(\alpha\in[0.3,0.4]\). That is because training with the Trojaned model \(X\), which has a relatively good legitimate ACC, can mitigate the damage of large trimming levels on the legitimate ACC. In fact, a large number of compromised clients implies a better probability for the compromised clients to sneak through the trimming; thus, improving both legitimate ACC and backdoor SR.

We observe a similar phenomenon when we apply the client-level DP optimizer as a defense against HNTroj (Fig. 13). More information is in Appx. D.

**Backdoor SR at Client Level.** Importantly, the histogram of backdoor SR in the CIFAR-10 dataset under DP optimizer (Fig. 3(a)) shows that HNTroj with only 1 compromised clients can open backdoors to 10 (a decent number of) legitimate clients with high backdoor SRs (\(>0.4\)). We observe similar backdoor SRs to 22 legitimate clients with only 1 compromised client in FMNIST (Fig. 3(b)). Therefore, it is not easy to defend against HNTroj.

**Results on the Fashion MNIST dataset.** The results on the Fashion MNIST dataset further strengthen our observation. DPois even failed to implant backdoors into HyperNetFL (Figs. 9(b) and 10(), Appx. D). This is because the HyperNetFL model converges \(10\)x faster than the model for the CIFAR-10 dataset, i.e., given the simplicity of the Fashion MNIST dataset; thus, significantly reducing the poisoning probability through participating in the training of a small set of compromised clients. Technically, we found that the client-level DP optimizer appears having the potential to mitigate HNTroj due to the model's fast convergence. However, the backdoor SR at the client level shows that with only 1 compromised client, HNTroj can open backdoors to 22 (over 100) legitimate clients with SR \(>0.4\) (Fig. 4). Therefore, it is challenging for the client-level DP optimizer to defend HNTroj. In addition, the \(\alpha\)-trimmed norm is still failed to defend again HNTroj (Figs. 7, 12, and 10(), Appx. D).

## 5 Conclusion and Future Work

We presented a black-box model transferring attack (HNTroj) to implant backdoor into HyperNetFL. We overcome the lack of consistency in deriving malicious local gradients to efficiently transfer a Trojaned model to the outputs of the HyperNetFL. We multiply a random and dynamic learning rate to the malicious local gradients making the attack stealthy. To defend against HNTroj, we adapted several robust FL training algorithms into HyperNetFL. Extensive experiment results show that HNTroj outperforms black-box DPois and white-box HNRepl bypassing adapted robust training algorithms with small numbers of compromised clients. Future work is to 1) adapt HNTroj on other personalized FL frameworks and 2) use multiple Trojaned models adapting to diverse compromised clients.

Figure 4: Backdoor Success Rate at the Client Level.

## References

* [1] GDPR, "The european data protection regulation," https://gdpr-info.eu/, 2018.
* [2] B. McMahan, E. Moore, et al., "Communication-efficient learning of deep networks from decentralized data," in _AISTATS_, 2017.
* [3] H. Zhu, J. Xu, S. Liu, and Y. Jin, "Federated learning on non-iid data: A survey," _arXiv preprint arXiv:2106.06843_, 2021.
* [4] A. Shamsian, A. Navon, E. Fetaya, and G. Chechik, "Personalized federated learning using hypernetworks," _ICML_, 2021.
* [5] P. Kairouz et al., "Advances and open problems in federated learning," _arXiv preprint arXiv:1912.04977_, 2019.
* [6] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, "How to backdoor federated learning," in _AISTATS_, 2020.
* [7] A.N. Bhagoji, S. Chakraborty, P. Mittal, and S. Calo, "Analyzing federated learning through an adversarial lens," in _ICML_, 2019, pp. 634-643.
* [8] D. Ha, A. Dai, and Q.V. Le, "Hypernetworks," _ICLR_, 2016.
* [9] B. Biggio, B. Nelson, and P. Laskov, "Poisoning attacks against support vector machines," in _ICML_, 2012, pp. 1467-1474.
* [10] B. Nelson et al., "Exploiting machine learning to subvert your spam filter," in _USENIX Workshop_, 2008, pp. 7:1-7:9.
* [11] Jacob Steinhardt, Pang Wei Koh, and Percy Liang, "Certified defenses for data poisoning attacks," in _NeurIPS_, 2017, pp. 3520-3532.
* [12] M.S. Ozdayi, M. Kantarcioglu, and Y.R. Gel, "Defending against backdoors in federated learning with robust learning rate," _AAAI_, 2021.
* [13] T. Gu, B. Dolan-Gavitt, and S. Garg, "Badnets: Identifying vulnerabilities in the machine learning model supply chain," _MLCS Workshop_, 2017.
* [14] Y. Liu, S. Ma, Y. Aafer, W.C. Lee, J. Zhai, W. Wang, and X. Zhang, "Trojaning attack on neural networks," 2017.
* imperceptible warping-based backdoor attack," in _ICLR_, 2021.
* [16] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B.Y. Zhao, "Neural cleanse: Identifying and mitigating backdoor attacks in neural networks," in _SP_, 2019, pp. 707-723.
* [17] K. Liu, B. Dolan-Gavitt, and S. Garg, "Fine-pruning: Defending against backdooring attacks on deep neural networks," in _RAID_, 2018, pp. 273-294.
* [18] Y. Gao, C. Xu, D. Wang, S. Chen, D.C. Ranasinghe, and S. Nepal, "Strip: A defence against trojan attacks on deep neural networks," in _ACSAC_, 2019.
* [19] O. Suciu, R. Marginean, Y. Kaya, H. Daume III, and T. Dumitras, "When does machine learning fail? generalized transferability for evasion and poisoning attacks," in _USENIX_, 2018, pp. 1299-1316.
* [20] B. Li, Y. Wang, A. Singh, and Y. Vorobeychik, "Data poisoning attacks on factorization-based collaborative filtering," _NeurIPS_, vol. 29, pp. 1885-1893, 2016.
* [21] A. Krizhevsky et al., "Learning multiple layers of features from tiny images," 2009.
* [22] H. Xiao, K. Rasul, and R. Vollgraf, "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms," _arXiv:1708.07747_, 2017.
* [23] D. Yin, Y. Chen, R. Kannan, and P. Bartlett, "Byzantine-robust distributed learning: Towards optimal statistical rates," in _ICML_, 2018, pp. 5650-5659.
* [24] S. Hong, V. Chandrasekaran, Y. Kaya, T. Dumitras, and N. Papernot, "On the effectiveness of mitigating data poisoning attacks with gradient shaping," _arXiv preprint arXiv:2002.11497_, 2020.
* [25] C. Xie, M. Chen, P.Y. Chen, and B. Li, "Crfl: Certifiably robust federated learning against backdoor attacks," _ICML_, 2021.

* [26] J. Jia, X. Cao, and N.Z. Gong, "Intrinsic certified robustness of bagging against data poisoning attacks," _AAAI_, 2021.
* [27] K. Pillutla, S.M. Kakade, and Z. Harchaoui, "Robust aggregation for federated learning," _arXiv preprint arXiv:1912.13445_, 2019.
* [28] R. Guerraoui, S. Rouault, et al., "The hidden vulnerability of distributed learning in byzantium," in _ICML_, 2018, pp. 3521-3530.
* [29] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, "Gradient-based learning applied to document recognition," _Proc. of the IEEE_, vol. 86, no. 11, pp. 2278-2324, 1998.

## Appendix A Background and Related Work

### Federated Learning (FL)

We consider the following FL protocol: at round \(t\), the server sends the latest model weights \(\theta_{t}\) to a randomly sampled subset of clients \(S_{t}\). Upon receiving \(\theta_{t}\), the client \(i\) in \(S_{t}\) uses \(\theta_{t}\) to train their local model for some number of iterations, e.g., via stochastic gradient descent (SGD), and results in model weights \(\theta_{t}^{i}\). The client \(i\) computes their local gradient \(\triangle\theta_{t}^{i}=\theta_{t}^{i}-\theta_{t}\), and sends it back to the server. After receiving all the local gradients from all the clients in \(S_{t}\), the server updates the model weights by aggregating all the local gradients by using an aggregation function \(\mathcal{G}:R^{|S_{t}|\times m}\to R^{m}\) where \(m\) is the size of \(\triangle\theta_{t}^{i}\). The aggregated gradient will be added to \(\theta_{t}\), that is, \(\theta_{t+1}=\theta_{t}+\lambda\mathcal{G}(\{\triangle\theta_{t}^{i}\}_{i\in S _{t}})\) where \(\lambda\) is the server's learning rate. A typical aggregation function is Federated Averaging (FedAvg) applied in many papers in FL [5], as follows:

\[\theta_{t+1}=\theta_{t}+\lambda(\sum_{i\in S_{t}}n_{i}\times\triangle\theta_{ t}^{i})/\sum_{i\in S_{t}}n_{i}\] (7)

When the number of training samples \(n_{i}\) is hidden from the server, one can use an unweighted aggregation function: \(\theta_{t+1}=\theta_{t}+\lambda\sum_{i\in S_{t}}\triangle\theta_{t}^{i}/|S_{t}|\).

### Training protocol of a HyperNetFL

The training protocol of a HyperNetFL (Alg. 2) is generally similar to the aforementioned protocol of typical FL. However, at round \(t\), there are three differences in training HyperNetFL compared with FedAvg [5]: **(1)** There is no global model \(\theta\) generated by the aggregation function \(\mathcal{G}\) in Eq. 7; **(2)** Each client \(i\in S_{t}\) receives the personalized weights \(\theta_{t}^{i}\) from the HyperNetFL. The client \(i\) computes the local gradient \(\triangle\theta_{t}^{i}\) and then sends it to the server; **(3)** The server uses all the local gradients received from all the clients \(i\in S_{t}\) to update \(\varphi\) and the descriptors \(v_{i}\) using general update rules in Lines 9 and 10 (Alg. 2); and **(4)** The size of the HyperNetFL weights \(\varphi\) is significatlty larger than \(\theta\) (\(\varphi\simeq 10\)x size of \(\theta\)) causing extra computational cost at the server. This protocol is more general than using only one client at a communication round as in [4]. By using a small batch of clients per communication round, we observe that we can enhance the performance of the HyperNetFL and make it more reliable.

## Appendix B Data Poisoning in HyperNetFL

We consider both white-box and black-box model replacement threat models. Although unrealistic, the white-box setting provides the upper bound risk. Meanwhile, the black-box setting will inform a realistic risk in practice. Details regarding the white-box setting and the adaptation of model replacement attacks into HyperNetFL, called **HNRepl**, are available1. Let us present our black-box threat model and attacks as follows.

Footnote 1: https://www.dropbox.com/s/cn3ormap7xso6my/HNTRoj-2.pdf?dl=0

**Black-box Threat Model.** At round \(t\), an adversary fully controls a small set of compromised clients \(\mathbbm{C}\). The adversary cannot modify the training protocol of the HyperNetFL at the server and at the legitimate clients. The adversary's goal is to implant backdoors in all local models \(\{\theta^{i}=h(v_{i},\varphi)\}_{i\in[N]}\) by minimizing a backdoor objective:

\[\arg\min_{\varphi,\{v_{i}\}_{i\in[N]}}\frac{1}{N}\sum_{i=1}^{N}\left[L_{i}(h(v _{i},\varphi))+L_{i}^{b}(h(v_{i},\varphi))\right]\] (8)where \(L_{i}^{b}\) is the (backdoor) loss function of the \(i^{th}\) client given Trojaned examples \(x_{j}+\mathcal{T}\) with the trigger \(\mathcal{T}\)[15], e.g., \(L_{i}^{b}=\frac{1}{n_{i}}\sum_{j=1}^{n_{i}}L\big{(}f(x_{j}+\mathcal{T},\theta), y_{j}^{b}\big{)}\) where \(y_{j}^{b}\) is the targeted label for the sample \(x_{j}+\mathcal{T}\). One can vary the portion of Trojaned samples to optimize the attack performance. This black-box threat model is applied throughout this paper.

We found that HNRepl is infeasible in the black-box setting, since the weights \(\varphi\) and the descriptors \(\{v_{i}\}_{i\in[N]}\) are hidden from all the clients. Also, there is lack of effective approach to infer (large) \(\varphi\) and \(\{v_{i}\}_{i\in[N]}\) given a small number of compromised clients.

**Data Poisoning (DPois) in HyperNetFL.** To address the issues of HNRepl, we look into another fundamental approach, that is applying black-box DPois. The pseudo-code of the attack is in Alg. 3. At round \(t\), the compromised clients \(c\in\mathbbm{U}\cap S_{t}\) receive the personalized model weights \(\theta_{t}^{c}\) from the server. Then, they compute malicious local gradients \(\triangle\bar{\theta}_{t}^{c}\) using their Trojan datasets, i.e., their legitimate data combined with Trojaned data samples, to minimize their local backdoor loss functions: \(\forall c\in\mathbbm{U}\cap S_{t}:\theta_{t}^{c*}=\arg\min_{\theta_{t}^{c}}[L_ {c}(h(v_{c},\varphi))+L_{i}^{b}(h(v_{c},\varphi))]\), after a certain number of local steps \(K\) of SGD. All the malicious local gradients \(\{\triangle\bar{\theta}_{t}^{c}\}_{c\in\mathbbm{C}}\) are sent to the server. If the HyperNetFL updates the model weights \(\varphi\) and the descriptors \(\{v_{i}\}_{i\in S_{t}}\) using \(\{\triangle\bar{\theta}_{t}^{c}\}_{c\in\mathbbm{C}}\), the local model weights generated by the HyperNetFL \(h(\cdot,\varphi)\) will be Trojaned infected. This is because the update rules of the HyperNetFL become the gradient of an approximation to the following Trojaned surrogate loss

\[\frac{1}{2}(\sum_{c\in\mathbbm{C}}\|\theta^{c*}-h(v_{c},\varphi)\|_{2}^{2}+ \sum_{i\in N\setminus\mathbbm{C}}\|\theta^{i*}-h(v_{i},\varphi)\|_{2}^{2})\] (9)

where \(\theta^{c*}\) is the optimal local Trojaned model weights, \(i\in N\setminus\mathbbm{C}\) are legitimate clients and their associated legitimate loss functions \(\theta^{i*}=\arg\min_{\theta^{i}}L_{i}(h(v_{i},\varphi)\).

**Disadvantages of DPois.** Obviously, the larger the number of compromised clients is, i.e., a larger \(\mathbbm{U}\) and a smaller \(N\setminus\mathbbm{C}\), the more effective the attack will be. Although more practical than the HNRepl in poisoning HyperNetFL, there are two issues in the black-box DPois: **(1)** The attack causes notable degradation in model utility on the legitimate data samples; and **(2)** The attack requires a more significant number of compromised clients to be successful. These disadvantages reduce the stealthiness and effectiveness of the attack, respectively.

The root cause issue of the DPois is the lack of consistency in deriving the malicious local gradient \(\triangle\bar{\theta}_{t}^{c}=\theta_{t}^{c*}-h(v_{c},\varphi)\) across communication rounds and among compromised clients to outweigh the local gradients from legitimate clients. First, \(\theta_{t}^{c*}\) is derived after (a small number) \(K\) local steps of applying SGD to minimize the local backdoor loss function \(\theta_{t}^{c*}=\arg\min_{\theta_{t}^{c}}[L_{c}(h(v_{c},\varphi))+L_{c}^{b}(h (v_{c},\varphi))]\), in which the local model weights \(h(v_{c},\varphi)\) and the loss functions (i.e, \(L_{c}\) and \(L_{c}^{b}\)) are varying among compromised clients due to the descriptors \(\{v_{c}\}_{c\in\mathbbm{C}}\) in addition to the their dissimilar local datasets. As a result, the (supposed to be) Trojaned model weights \(\{\theta_{t}^{c*}\}_{c\in\mathbbm{C}}\) are unalike among compromised clients. Second, a small number of local training steps \(K\) (i.e., given a limited computational power on the compromised clients) is not sufficient to approximate a good Trojaned model \(\theta_{t}^{c*}\). The adversary can increase the local training steps \(K\) if more computational power is available. However, there is still no guarantee that \(\{\theta_{t}^{c*}\}_{c\in\mathbbm{C}}\) will be alike without the control over the dissimilar descriptors \(\{v_{c}\}_{c\in\complement}\). Third, the local model weights \(h(v_{c},\varphi)\) change after every communication round and are heavily affected by the local gradients from legitimate clients. Consequently, the malicious local gradients \(\triangle\bar{\theta}_{t}^{c}\) derived across all the compromised clients \(c\in\complement\) do not synergistically optimize the approximation to the Trojaned surrogate loss function (Eq. 9) such that the outputs of the HyperNetFL \(h(\cdot,\varphi)\) are Trojan infected.

Therefore, developing a practical, stealthy, and effective backdoor attack in HyperNetFL is non-trivial and remains an open problem.

## Appendix C Robust HyperNetFL Training

In this section, we first investigate the state-of-the-art defenses against backdoor poisoning in FL and point out the differences between FL and HyperNetFL. We then present our robust training approaches adapted from existing defenses for HyperNetFL against HNTroj.

Existing defense approaches against backdoor poisoning in ML can be categorised into two lines: 1) Trojan detection in the inference phase and 2) robust aggregation to mitigate the impacts of malicious local gradients in aggregation functions. In this paper, we applied the state-of-the-art warping-based Trojans bypassing all the well-known Trojan detection methods, i.e., Neural Cleanse [16], Fine-Pruning [16], and STRIP [18], in the inference phase. HNTroj does not affect the warping-based Trojans (Fig. 9); thus bypassing these detection methods. Based upon that, we focus on identifying which robust aggregation approaches can be adapted to HyperNetFL and how.

**Robust Aggregation** Several works have proposed robust aggregation approaches to deter byzantine attacks in typical FL, such as coordinate-wise median, geometric median, \(\alpha\)-trimmed mean, or a variant and combination of such techniques [23]. Recent approaches include weight-clipping and noise addition with certified bounds, ensemble models, differential privacy (DP) optimizers, and adaptive and robust learning rates (RLR) across clients and at the server [12, 24].

Despite differences, existing robust aggregation focuses on analysing and manipulating the local gradients \(\triangle\theta_{t}^{i}\), which share the global aggregated model \(\theta_{t}\) as the same root, i.e., \(\forall i\in[N]:\triangle\theta_{t}^{i}=\theta_{t}^{i}-\theta_{t}\). The fundamental assumption in these approaches is that the local gradients from compromised clients \(\{\triangle\bar{\theta}_{t}^{c}\}_{c\in\complement}\) and legitimate clients \(\{\triangle\theta_{t}^{i}\}_{i\in N\setminus\complement}\) are different in terms of magnitude and direction.

**Robust FL Training v.s. HyperNetFL.** Departing from typical FL, the local gradients in HyperNetFL have different and personalized roots \(h(v_{i},\varphi)\), i.e., \(\forall i\in[N]:\triangle\theta_{t}^{i}=\theta_{t}^{i*}-h(v_{i},\varphi)\). Therefore, the 

[MISSING_PAGE_FAIL:11]

\(\alpha\)-Trimmed NormIn addition, among robust aggregation approaches against byzantine attacks, median-based approaches, \(\alpha\)-trimmed mean, and variants of these techniques [27, 28] can be adapted to HyperNetFL against HNTroj by applying them on the gradients of \(\varphi\). Without loss of generality, we adapt the well-applied \(\alpha\)-trimmed mean approach [23] into HyperNetFL to eliminate potentially malicious \(\varphi\)'s gradients in this paper. The adapted algorithm needs to be less computational resource hungry in order for it to efficiently work with the large size of \(\varphi\). Therefore, instead of looking into each element of the \(\varphi\)'s gradient as in \(\alpha\)-trimmed mean, we trim the top \(\frac{\alpha}{2}\)% and the bottom \(\frac{\alpha}{2}\)% of the gradients \(\{(\bigtriangledown_{\varphi}\theta^{i}_{t})^{\top}\bigtriangleup\theta^{i}_{ t}\}_{i\in S_{t}}\) that respectively have the highest and lowest magnitudes quantified by an \(l_{2}\)-norm, i.e., \(\|(\bigtriangledown_{\varphi}\theta^{i}_{t})^{\top}\bigtriangleup\theta^{i}_{ t}\|_{2}\). The remaining gradients after the trimming, denoted \(\{(\bigtriangledown_{\varphi}\theta^{i}_{t})^{\top}\bigtriangleup\theta^{i}_{ t}\}_{i\in S^{\alpha-trim}_{t}}\), are used to update the HyperNetFL model weights \(\varphi\), i.e., \(\varphi=\varphi-\frac{\alpha}{|S^{\alpha-trim}_{t}|}\sum_{i=1}^{|S^{\alpha-trim }_{t}|}(\bigtriangledown_{\varphi}\theta^{i}_{t})^{\top}\bigtriangleup\theta ^{i}_{t}\). The descriptors \(\{v_{i}\}_{i\in S^{\alpha-trim}_{t}}\) are updated normally. The pseudo-code of the \(\alpha\)-trimmed norm for HyperNetFL is in Alg. 5.

## Appendix D Experiments

**Data and Model Configuration.** To achieve our goal, we conduct an extensive experiment on CIFAR-10 [21] and Fashion MNIST datasets [22]. For both datasets, to generate non-iid data distribution across clients in terms of classes and size of local training data, we randomly sample two classes for each client. And for each client \(i\) and selected class \(c\), we sample \(p_{i,c}\sim\mathcal{N}(0,1)\) and assign it with \(\frac{p_{i,c}}{\sum_{j}p_{j,c}}\) of the samples for this class. We use \(100\) clients. There are \(60,000\) samples in the CIFAR-10 and \(70,000\) samples in the Fashion MNIST datasets. Each dataset is divided into three non-overlapping sets: \(10,000\) samples for testing, \(10,000\) samples for training the Trojaned model \(X\), and the rest for training (i.e., \(40,000\) samples in the CIFAR-10 and \(50,000\) samples in the Fashion MNIST for training). We use the class \(0\) as a targeted class \(y^{6}_{j}\) (Eq. 8) in each dataset.

We adopt the model configuration described in [4] for both datasets, in which we use a LeNet-based network [29] with two convolution and two fully connected layers for the local model and a fully-connected network with three hidden layers and multiple linear heads per target weight tensor for the HyperNetFL. SGD optimizer with the learning rate \(0.01\) for the HyperNetFL and \(0.001\) for the local model are used.

We use WaNet [15], which is one of the state-of-the-art backdoor attacks, for generating backdoor data. WaNet uses image warping-based triggers making the modification in the backdoor images natural and unnoticeable to humans. We follow the learning setup described in [15] to generate the backdoor images that are used to train \(X\). In the DP optimizer, we vary the noise scale \(\sigma\in\{10^{-1},10^{-2},10^{-3},10^{-4}\}\) and the clipping \(l_{2}\)-norm \(\mu\in\{8,4,2,1\}\). For the \(\alpha\)-trimmed norm approach, we choose \(\alpha\in\{0.1,0.2,0.3,0.4\}\).

**Evaluation Approach.** We carry out the validation through three approaches. We first compare HNTroj with DPois and HNRepL in terms of legitimate accuracy (ACC) on legitimate data samples and backdoor successful rate (SR) on Trojaned data samples with a wide range number of compromised clients. The second approach is to investigate the effectiveness of adapted robust HyperNetFL training algorithms, including the client-level DP optimizer and the \(\alpha\)-trimmed norm, under a variety of hyper-parameter settings against HNTroj. Based upon that, the third approach provides a performance summary of both attacks and defenses to inform the surface of backdoor risks in HyperNetFL. The (average) legitimate ACC and backdoor SR across clients on testing data are as follows:

\[\text{Legitimate ACC}=\frac{1}{N}\sum_{i\in[N]}\frac{1}{n^{\tau}_{i }}\sum_{j\in[n^{\tau}_{i}]}Acc\big{(}f(x^{i}_{j},\theta^{i}),y^{i}_{j}\big{)}\] \[\text{Backdoor SR}=\frac{1}{N}\sum_{i\in[N]}\frac{1}{n^{\tau}_{i }}\sum_{j\in[n^{\tau}_{j}]}Acc\big{(}f(\overline{x}^{i}_{j},\theta^{i}),y^{i, b}_{j}\big{)}\]

where \(\overline{x}^{i}_{j}=x^{i}_{j}+\mathcal{T}\) is a Trojaned sample, \(Acc(y^{\prime},y)=1\) if \(y^{\prime}=y\); otherwise \(Acc(y^{\prime},y)=0\) and \(n^{\tau}_{i}\) is the number of testing samples in client \(i\).

**Backdoor Risk Surface: Attacks and Defenses.** The trade-off between legitimate ACC and backdoor SR is non-trivially observable given many attack and defense configurations. To inform a better surface of backdoor risks, we look into a natural question: _"What can the adversary or the defender achieve given a specific number of compromised clients?"_

We answer this question by summarizing the best defending performance and the most stealthy and severe backdoor risk across hyper-parameter settings in the same diagram. Given a number of compromised clients \(\tilde{\mathsf{C}}\) and a robust training algorithm \(\mathcal{A}\), the best defending performance, which maximizes both the 1) legitimate ACC (i.e., ACC in short) and 2) the gap between the ACC and

Figure 5: HNTroj under client-level DP optimizer-based robust HyperNetFL training in the CIFAR-10 dataset.

Figure 6: HNTroj under client-level DP optimizer-based robust HyperNetFL training in the Fashion MNIST dataset.

backdoor SR (i.e., SR in short), is identified across hyper-parameters' space of \(\mathcal{A}\), i.e., \(\Theta^{\complement}_{\mathcal{A}}\):

\[\phi^{*}=\arg\max_{\phi\in\Theta^{\complement}_{\mathcal{A}}}\big{[}ACC(\phi)+ \big{(}ACC(\phi)-SR(\phi)\big{)}\big{]}\]

where \(ACC(\phi)\) and \(SR(\phi)\) are the legitimate ACC and backdoor SR using the specific hyper-parameter configuration \(\phi\). Similarly, we identify the most stealthy and severe backdoor risk maximizing both the legitimate ACC and backdoor SR:

\[\phi^{*}=\arg\max_{\phi\in\Theta^{\complement}_{\mathcal{A}}}\big{[}ACC(\phi)+ SR(\phi)\big{]}\]

Fig. 3 summaries the best performance of both defenses and attacks in the CIFAR-10 dataset as a function of the number of compromised clients through out the hyper-parameter space. For instance, given the number of compromised clients \(\complement=10\), using the client-level DP optimizer, the best defense can reduce the backdoor SR to \(12.88\%\) with a cost of \(13.45\%\) drop in the legitimate ACC. Meanwhile, in a weak defense using the client-level DP optimizer, the adversary can increase the backdoor SR up to \(71.74\%\) without sacrificing much the legitimate ACC. From Figs. 2(a)-b, we can observe that \(\alpha\)-trimmed norm is a little bit more effective than the client-level DP optimizer by having a wider gap between legitimate ACC and backdoor SR. From the adversary angle, to ensure the success of the HNTroj regardless of the defenses, the adversary needs to have at least \(15\) compromised clients.

**HNTroj v.s. Client-level DP Optimizer.** We observe a similar phenomenon when we apply the client-level DP optimizer as a defense against HNTroj (Fig. 13). First, by using small noise scales \(\sigma\in[10^{-4},10^{-2}]\), the client-level DP optimizer is effective in defending against HNTroj when the number of compromised clients is small (\(\complement\in[1,5]\)) achieving low backdoor SR, i.e., \(31.97\%\) in average, while maintaining an acceptable legitimate ACC, i.e., \(72.33\%\) in average. When the number of compromised clients is a little bit larger, the defense pays notably large tolls on the legitimate ACC (i.e., the legitimate ACC drops from \(75.80\%\) to \(31.18\%\) given \(\complement=10\)) or fails to reduce the backdoor SR (i.e., backdoor SR \(>58.25\%\) given \(\complement>10\)). That is consistent with our analysis. A small sufficient number of compromised clients synergistically and consistently can pull the outputs of the HyperNetFL to the Trojaned model \(X\)'s surrounded area.

Figure 8: White-box model replacement attack in the CIFAR-10 and the Fashion MNIST datasets.

Figure 7: Legitimate ACC and backdoor SR under \(\alpha\)-trimmed norm defense in the Fashion MNIST dataset.

```
0: Number of rounds \(T\), number of local rounds \(K\), server's learning rates \(\lambda\) and \(\zeta\), clients' learning rate \(\eta\), number of clients \(N\), and \(L_{i}(B)\) is the loss function \(L_{i}(\theta)\) on a mini-batch \(B\)
0:\(\varphi,v_{i}\)
1:for\(t=1,\ldots,T\)do
2: Sample clients \(S_{t}\)
3:for each client \(i\in S_{t}\)do
4: set \(\theta^{i}_{t}=h(v_{i},\varphi)\) and \(\tilde{\theta}^{i}=\theta^{i}_{t}\)
5:for\(k=1,\ldots,K\)do
6: sample mini-batch \(B\subset D_{i}\)
7:\(\tilde{\theta}^{i}_{k+1}=\tilde{\theta}^{i}_{k}-\eta\ \nabla\phi_{i}^{i}\ L_{i}(B)\)
8:\(\triangle\theta^{i}_{t}=\tilde{\theta}^{i}_{K}-\theta^{i}_{t}\)
9:\(S^{\alpha-trim}_{t}=\alpha\)-trimmed norm \(\big{(}\{(\bigtriangledown_{\varphi}\theta^{i}_{t})^{\top}\bigtriangleup \theta^{i}_{t}\}_{i\in S_{i}}\big{)}\)
10:\(\varphi=\varphi-\frac{\lambda}{|S^{\alpha-trim}_{t}|}\sum_{i=1}^{|S^{\alpha-trim }_{t}|}(\bigtriangledown_{\varphi}\theta^{i}_{t})^{\top}\bigtriangleup\theta^ {i}_{t}\)
11:\(\forall i\in S^{\alpha-trim}_{t}:v_{i}=v_{i}-\zeta\bigtriangledown_{v_{i}} \varphi^{\top}(\bigtriangledown_{\varphi}\theta^{i}_{t})^{\top}\bigtriangleup \theta^{i}_{t}\) ```

**Algorithm 5**\(\alpha\)-Trimmed Norm in HyperNetFL

Figure 11: Legitimate ACC and Backdoor SR comparison for DPois, HNTroj, and Clean model over different numbers of compromised clients in the Fashion MNIST dataset. (Fig. (a)a and b have the same legend.)

Figure 10: White-box model replacement attack in the CIFAR-10 and the Fashion MNIST datasets.

Figure 9: Legitimate samples (left) and their backdoor samples generated by WaNet [15] (right) in HNTroj. They are are almost identical.

Figure 12: HNTroj under client-level DP optimizer-based robust HyperNetFL training in the Fashion MNIST dataset.

Figure 13: HNTroj under client-level DP optimizer-based robust HyperNetFL training in the CIFAR-10 dataset.

Figure 14: Backdoor risk surface: attacks and defenses in the Fashion MNIST dataset. The attack we used here is HNTroj.

Figure 15: Backdoor risk surface: attacks and defenses in the CIFAR-10 dataset. The attack we used here is HNTroj.