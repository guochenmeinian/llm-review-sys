# Variational Gaussian Processes For Linear Inverse Problems

 Thibault Randrianarisoa

Department of Decision Sciences

Bocconi University

via Roentgen 1, 20136, Milano, MI, Italy

thibault.randrianarisoa@unibocconi.it

&Botond Szabo

Department of Decision Sciences

Bocconi University

via Roentgen 1, 20136, Milano, MI, Italy

botond.szabo@unibocconi.it

###### Abstract

By now Bayesian methods are routinely used in practice for solving inverse problems. In inverse problems the parameter or signal of interest is observed only indirectly, as an image of a given map, and the observations are typically further corrupted with noise. Bayes offers a natural way to regularize these problems via the prior distribution and provides a probabilistic solution, quantifying the remaining uncertainty in the problem. However, the computational costs of standard, sampling based Bayesian approaches can be overly large in such complex models. Therefore, in practice variational Bayes is becoming increasingly popular. Nevertheless, the theoretical understanding of these methods is still relatively limited, especially in context of inverse problems. In our analysis we investigate variational Bayesian methods for Gaussian process priors to solve linear inverse problems. We consider both mildly and severely ill-posed inverse problems and work with the popular inducing variables variational Bayes approach proposed by Titsias [57]. We derive posterior contraction rates for the variational posterior in general settings and show that the minimax estimation rate can be attained by correctly tunned procedures. As specific examples we consider a collection of inverse problems including the heat equation, Volterra operator and Radon transform and inducing variable methods based on population and empirical spectral features.

## 1 Introduction

In inverse problems we only observe the object of interest (i.e. function or signal) indirectly, through a transformation with respect to some given operator. Furthermore, the data is typically corrupted with measurement error or noise. In practice the inverse problems are often ill-posed, i.e. the inverse of the operator is not continuous. Based on the level of ill-posedness we distinguish mildly and severely ill-posed cases. The ill-posedness of the problem prevents us from simply inverting the operator as it would blow up the measurement errors in the model. Therefore, to overcome this problem, regularization techniques are applied by introducing a penalty term in the maximum likelihood approximation. Standard examples include generalized Tikhonov, total variation and Moore-Penrose estimators, see for instance [4; 5; 9; 11; 18; 56] or a recent survey [3] on data-driven methods for solving inverse problems

An increasingly popular approach to introduce regularity to the model is via the Bayesian paradigm, see for instance [3; 7; 12; 29; 54] and references therein. Beside regularization, Bayesian methods provide a probabilistic solution to the problem, which can be directly used to quantify the remaining uncertainty of the approach. This is visualised by plotting credible sets, which are sets accumulating prescribed percentage of the posterior mass. For computing the posterior typically MCMC algorithms are used, however, these can scale poorly with increasing sample size due to the complex structure of the likelihood. Therefore, in practice often alternative, approximation methods are used. VariationalBayes (VB) casts the approximation of the posterior into an optimization problem. The VB approach became increasingly popular to scale up Bayesian inverse problems, see for instance the recent papers [22; 31; 34; 42] and references therein. However, until recently these procedures were considered black box methods basically without any theoretical underpinning. Theoretical results are just starting to emerge [2; 46; 63; 64; 66], but we still have limited understanding of these procedures in complex models, like inverse problems.

In our analysis we consider Gaussian process (GP) priors for solving linear inverse problems. For Gaussian likelihoods, due to conjugacy, the corresponding posterior has an analytic form. Nevertheless, they are applied more widely, in non-conjugate settings as well. However, training and prediction even in the standard GP regression model, scales as \(O\left(n^{3}\right)\) (or \(O\left(n^{2}\right)\) for exact inference in the recent paper [16] leveraging advances in computing hardware) and \(O\left(n^{2}\right)\), respectively, which practically limits GPs to a sample size \(n\) of order \(10^{4}\). Therefore, in practice often not the full posterior, but an approximation is computed. Various such approximation methods were proposed based on some sparse or low rank structure, see for instance [13; 14; 33; 49; 50; 51; 52; 57]. Our focus here lies on the increasingly popular inducing variable variational Bayes method introduced in [57; 58].

In our work we extend the inducing variable method for linear inverse problems and derive theoretical guarantees for the corresponding variational approximations. More concretely we adopt a frequentist Bayes point-of-view in our analysis by assuming that there exists a true data generating functional parameter of interest and investigate how well the variational posterior can recover this object. We derive contraction rates for the VB posterior around the true function both in the mildly and severely ill-posed inverse problems. We then focus on two specific inducing variable methods based on the spectral features of the prior covariance kernel. We show that for both methods if the number of inducing variables are chosen large enough for appropriately tunned priors the corresponding variational posterior concentrates around the true function with the optimal minimax estimation rate. One, perhaps surprising aspect of the derived results is that the number of inducing variables required to attain the optimal, minimax contraction rate is sufficiently less in the inverse setting than in the direct problem. Therefore, inverse problems can be scaled up at a higher degree than standard regression models.

_Related literature._ The theory of Bayesian approaches to linear inverse problems is now well established. The study of their asymptotic properties started with the study of conjugate priors [1; 15; 20; 26; 27; 28] before addressing the non-conjugate case [25; 45] and rate-adaptive priors [26; 55]. By now we have a good understanding of both the accuracy of the procedure for recovering the true function and the reliability of the corresponding uncertainty statements. The theory of Bayesian non-linear inverse problems is less developed, but recent years have seen an increasing interest in the topic, see the monograph [36] and references therein. Some algorithmic developments for variational Gaussian approximations in non-linear inverse problems, and applications to MCMC sampling, can be found in [40; 41].

The inducing variable approach for GPs proposed by [57; 58] has been widely used in practice. Recently, their theoretical behaviour was studied in the direct, nonparametric regression setting. In [8] it was shown that the expected Kullback-Leibler divergence between the variational class and posterior tends to zero when sufficient amount of inducing variables were used. Furthermore, optimal contraction rates [37] and frequentist coverage guarantees [38; 59; 60] were derived for several inducing variable methods. Our paper focuses on extending these results to the linear inverse setting.

_Organization._ The paper is organized as follows. In Section 2 we first introduce the inverse regression model where we carry out our analysis. Then we discuss the Bayesian approach using GPs and its variational approximations in Sections 2.1 and 2.2, respectively. As our main result we derive contraction rates for general inducing variable methods, both in the mildly and severely ill-posed cases. Then in Section 2.3 we focus on two specific inducing variable methods based on spectral features and provide more explicit results for them. We apply these results for a collection of examples, including the Volterra operator, the heat equation and the Radon transform in Section 3. Finally, we demonstrate the applicability of the procedure in the numerical analysis of Section 4 and conclude the paper with discussion in Section 5. The proof of the main theorem together with technical lemmas and additional simulation study are deferred to the supplementary material.

_Notation._ Let \(C,c\) be absolute constants, independent of the parameters of the problem whose values may change from line to line. For two sequences (\(a_{n}\)) and (\(b_{n}\)) of numbers, \(a_{n}\lesssim b_{n}\) means that there exists a universal constant \(c\) such that \(a_{n}\leq cb_{n}\) and we write \(a_{n}\asymp b_{n}\) if both \(a_{n}\lesssim b_{n}\) and \(b_{n}\lesssim a_{n}\) hold simultaneously. We denote by \(a_{n}\ll b_{n}\) if \(|a_{n}/b_{n}|\) tends to zero. The maximum and minimum of two real numbers \(a\) and \(b\) are denoted by \(a\lor b\) and \(a\wedge b\), respectively. We use the standard notation \(\delta_{ij}=\mathds{1}_{i=j}\). For \(m\geq 1\), we note \(\bm{S}_{++}^{m}\) the set of positive-definite matrices of size \(m\times m\).

## 2 Main results

In our analysis we focus on the non-parametric random design regression model where the functional parameter is observed through a linear operator. More formally, we assume to observe i.i.d. pairs of random variables \((x_{i},Y_{i})_{i=1,\ldots,n}\) satisfying

\[Y_{i}=\left(\mathcal{A}f_{0}\right)(x_{i})+Z_{i},\qquad Z_{i}\overset{iid}{ \sim}N(0,\sigma^{2}),\,x_{i}\overset{iid}{\sim}G\qquad i=1,\ldots,n,\] (1)

where \(f_{0}\in L_{2}(\mathcal{T};\mu)\), for some domain \(\mathcal{T}\subset\mathbb{R}^{d}\) and measure \(\mu\) on \(\mathcal{T}\), is the underlying functional parameter of interest and \(\mathcal{A}:L_{2}(\mathcal{T};\mu)\mapsto L_{2}(\mathcal{X};G)\), for the measure \(G=\mathcal{A}\mu\) on \(\mathcal{X}\), is a known, injective, continuous linear operator. In the rest of the paper we use the notation \(P_{f_{0}}\) and \(E_{f_{0}}\) for the joint distribution and the corresponding expectation, respectively, of the data \((X,Y)=(x_{i},Y_{i})_{i=1,\ldots,n}\). Furthermore, we denote by \(E_{X},P_{X},E_{Y|X},P_{Y|X}\) the expectation/distribution under \(G^{\otimes n}\) and the law of \((Y_{i})_{i}\) given the design respectively. Finally, for simplicity we take \(\sigma^{2}=1\) in our computations.

In the following, denoting \(\mathcal{A}^{*}\) the adjoint of \(\mathcal{A}\), we assume that the self-adjoint operator \(\mathcal{A}^{*}\mathcal{A}\colon L_{2}(\mathcal{T};\mu)\mapsto L_{2}(\mathcal{ T};\mu)\) possesses countably many positive eigenvalues \((\kappa_{j}^{2})_{j}\) with respect to the eigenbasis \((e_{j})_{j}\) (which is verified if \(\mathcal{A}\) is a compact operator for instance). We remark that \(\left(g_{j}\right)_{j}\) defined by \(\mathcal{A}e_{j}=\kappa_{j}g_{j}\) is an orthonormal basis of \(L_{2}(\mathcal{X};G)\). We work on the ill-posed problem where \(\kappa_{j}\to 0\), the rate of decay characterizing the difficulty of the inverse problem.

**Definition 1**.: _We say the problem is mildly ill-posed problem of degree \(p>0\) if \(\kappa_{j}\asymp j^{-p}\) has a polynomial decay. In the severely ill-posed problem, the rate we consider is exponential, \(\kappa_{j}\asymp e^{-cj^{p}}\) for \(c>0,p\geq 1\), and \(p\) is the degree of ill-posedness once again._

In nonparametrics it is typically assumed that \(f_{0}\) belongs to some regularity class. Here we consider the generalized Sobolev space

\[\bar{H}^{\beta}\coloneqq\left\{f\in L_{2}(\mathcal{T};\mu):\left\|f\right\|_{ \beta}<\infty\right\},\quad\left\|f\right\|_{\beta}^{2}=\sum_{j}j^{2\beta} \left|\left\langle f,e_{j}\right\rangle\right|^{2},\] (2)

for some \(\beta>0\). We note that the difficulty in estimating \(f_{0}\) from the data is twofold: one needs to deal with the observational noise, which is a statistical problem, as well as to invert the operator \(\mathcal{A}\), which comes from inverse problem theory. As a result of the ill-posedness of the problem, recovering \(f_{0}\) from the observations may suffer from problems of unidentifiability and instability. The solution to these issues is to incorporate some form of regularization in the statistical procedure. The Bayesian approach provides a natural way to incorporate regularization into the model via the prior distribution on the functional parameter. In fact penalized likelihood estimators can be viewed as the maximum a posteriori estimators with the penalty term induced by a prior. For example Tikhonov type regularizations can be related to the RKHS-norm of a Gaussian Process prior, see [35; 44] for a more detailed discussion.

### Gaussian Process priors for linear inverse problems

We focus on the Bayesian solution of the inverse problem and exploit the Gaussian likelihood structure by considering conjugate Gaussian Process (GP) priors on \(f\). A GP \(\mathcal{GP}\left(\eta(\cdot),k(\cdot,\cdot)\right)\) is a set of random variables \(\{f(t)\mid t\in\mathcal{T}\}\), such that any finite subset follows a Gaussian distribution. The GP is described by the mean function \(\eta\) and a covariance kernel \(k(t,t^{\prime})\). We consider centered GPs as priors (i.e. we take \(\eta\equiv 0\)). Then the bilinear, symmetric nonnegative-definite function \(k\colon\ \mathcal{T}\times\mathcal{T}\mapsto\mathbb{R}\) determines the properties of the process (e.g., its regularity). In view of the linearity of the operator \(\mathcal{A}\) the corresponding posterior distribution is also a Gaussian process. The mean and covariance function of the posterior is given by

\[t \mapsto K_{t\bm{\mathcal{A}f}}\left(K_{\bm{\mathcal{A}f}\bm{ \mathcal{A}f}}+\sigma^{2}I_{n}\right)^{-1}\textbf{y},\] (3) \[(t,s) \mapsto k(t,s)-K_{t\bm{\mathcal{A}f}}\left(K_{\bm{\mathcal{A}f}\bm{ \mathcal{A}f}}+\sigma^{2}I_{n}\right)^{-1}K_{\bm{\mathcal{A}f}s},\]where \(\mathbf{y}=(y_{1},\ldots,y_{n})^{T}\), \(\boldsymbol{\mathcal{A}f}=\left(\mathcal{A}f(x_{i})\right)_{i=1,\ldots,n}\), \(K_{\boldsymbol{\mathcal{A}f}\boldsymbol{\mathcal{A}f}}=E_{\Pi}\boldsymbol{ \mathcal{A}f}\boldsymbol{\mathcal{A}f}^{T}\in\mathbb{R}^{n\times n}\) with \(E_{\Pi}\) denoting the expectation with respect to the GP prior \(\Pi\), \(K_{t,\boldsymbol{\mathcal{A}f}}^{T}=\left(E_{\Pi}\mathcal{A}f(x_{i})f(t) \right)_{i=1,\ldots,n}\in\mathbb{R}^{n}\), see the supplement for the detailed derivation.

Due to the closed-form expressions for the posterior and the marginal likelihood, as well as the simplicity with which uncertainty quantification may be produced, GP regression has gained popularity [44]. Furthermore, the asymptotic frequentist properties of posteriors corresponding to GP priors in the direct problem, with \(\mathcal{A}\) taken to be the identity operator, is well-established by now. Optimal contraction rates and confidence guarantees for Bayesian uncertainty quantification were derived in the regression setting and beyond, see for instance [10; 39; 47; 53; 48; 61; 62; 65] and references therein. In the following, we say that \(\varepsilon_{n}\) is an \(L_{2}\)-posterior contraction rate for the posterior \(\Pi\left[\;\cdot\;|X,Y\right]\) if for any \(M_{n}\to 0\)

\[E_{f_{0}}\Pi\left[f\colon\;\left\|f-f_{0}\right\|_{L_{2}(\mathcal{T};\mu)} \geq M_{n}\varepsilon_{n}\;|\;X,Y\right]\to 0.\]

In our analysis we consider covariance kernels with eigenfunctions coinciding with the eigenfunctions of the operator \(\mathcal{A}^{*}\mathcal{A}\), i.e. we take

\[k(t,s)=\sum\nolimits_{j}\lambda_{j}e_{j}(t)e_{j}(s),\] (4)

where \((\lambda_{j})_{j}\) denote the corresponding eigenvalues. The asymptotic behaviour of the corresponding posterior has been well investigated in the literature both in the mildly and severely ill-posed inverse problems. Rate optimal contraction rates and frequentist coverage guarantees for the resulting credible sets were derived both for known and unknown regularity parameters [15; 26; 27; 28; 55]. These results were further extended for other covariance kernels where the eigenfunctions do not exactly match the eigenfunctions of the operator \(\mathcal{A}\), but in principle they have to be closely related, see [1; 20; 25; 45].

However, despite the explicit, analytic form of the posterior given in (3) and the theoretical underpinning, the practical applicability of this approach is limited for large sample size \(n\). The computation of the posterior involves inverting the \(n\)-by-\(n\) matrix \(K_{\boldsymbol{\mathcal{A}f}\boldsymbol{\mathcal{A}f}}+\sigma^{2}I_{n}\), which has computational complexity \(O(n^{3})\). Therefore, in practice often not the true posterior, but a scalable, computationally attractive approximation is applied. Our focus here is on the increasingly popular inducing variable variational Bayes method introduced in [57; 58].

### Variational GP for linear inverse problems

In variational Bayes the approximation of the posterior is casted as an optimization problem. First a tractable class of distributions \(\mathcal{Q}\) is considered, called the variational class. Then the approximation \(\Psi^{*}\) is computed by minimizing the Kullback-Leibler divergence between the variational class and the true posterior, i.e.

\[\Psi^{*}=\arg\,\inf_{Q\in\mathcal{Q}}KL\left(Q\big{|}\Pi\left[\;\cdot\;|X,Y \right]\right).\]

There is a natural trade-off between the computational complexity and the statistical accuracy of the resulting approximation. Smaller variational class results in faster methods and easier interpretation, while more enriched classes preserve more information about the posterior ensuring better approximations.

In context of the Gaussian process regression model (with the operator \(\mathcal{A}\) taken to be the identity), [58] proposed a low-rank approximation approach based on inducing variables. The idea is to compress the information encoded in the observations of size \(n\) into \(m\) so called inducing variables. We extend this idea for linear inverse problems. Let us consider real valued random variables \(\mathbf{u}=(u_{1},\ldots,u_{m})\in L_{2}\left(\Pi\right)\), expressed as measurable linear functionals of \(f\) and whose prior distribution is \(\Pi_{u}\). In view of the linearity of \(\boldsymbol{u}\), the joint distribution of \((f,\mathbf{u})\) is a Gaussian process, hence the conditional distribution \(f|\mathbf{u}\) denoted by \(\Pi(\cdot|\mathbf{u})\), is also a Gaussian process with mean function and covariance kernel given by

\[t\mapsto K_{\boldsymbol{tu}}K_{\boldsymbol{uu}}^{-1}\boldsymbol{u}\quad\text{ and}\quad(t,s)\mapsto k(t,s)-K_{\boldsymbol{tu}}K_{\boldsymbol{uu}}^{-1}K_{ \boldsymbol{us}},\]

respectively, where \(K_{\boldsymbol{tu}}=E_{\Pi}(f(t)\boldsymbol{u})\in\mathbb{R}^{m}\) and \(K_{\boldsymbol{uu}}=E_{\Pi}(\boldsymbol{uu}^{T})\in\mathbb{R}^{m\times m}\). Then the posterior is approximated via a probability measure \(\boldsymbol{\Psi}_{u}\) on \((\mathbb{R}^{m},\mathcal{B}(\mathbb{R}^{m}))\) by \(\Psi=\int\Pi[\cdot|\mathbf{u}]d\Psi_{u}(\mathbf{u}),\) which is 

[MISSING_PAGE_FAIL:5]

Proof.: We provide the sketch of the proof here, the detailed derivation of the theorem is deferred to the supplementary material. In a first step, we derive posterior contraction rates around \(\mathcal{A}f_{0}\) in empirical \(L_{2}\)-norm under fixed design. In particular, we obtain an exponential decay of the probability expectation in the form

\[E_{Y\,|\,X}\Pi\Big{[}f\colon\,n^{-1}\sum\nolimits_{i=1}^{n}\big{(}\mathcal{A}f -\mathcal{A}f_{0}\big{)}^{2}(x_{i})\geq M_{n}\varepsilon_{n}^{2}\,|\,\,X,Y \Big{]}\mathds{1}_{A_{n}}\leq Ce^{-cM_{n}^{2}nc_{n}^{2}},\] (8)

for arbitrary \(M_{n}\to\infty\), where \(\varepsilon_{n}=n^{-\frac{\alpha\wedge\beta+p}{1+2\alpha+1p}}\) in the mildly and \(\varepsilon_{n}=n^{-\frac{\varepsilon}{\delta+2\varepsilon}}\log^{-\frac{ \beta}{p}+\frac{\varepsilon\varepsilon_{0}}{\delta+2\varepsilon}}n\) in the severely ill-posed problems and \(A_{n}\) is an event on the sample space \(\mathbb{R}^{n}\) with probability tending to one asymptotically. In the mildly ill-posed case, this follows from results in [17; 62], while additional care is needed in the severely ill-posed case. As a second step, we go back to the random design setting. We show, using concentration inequalities and controlling the tail probability of GPs in the spectral decomposition, that the empirical and population \(L_{2}\)-norms are equivalent on a large enough event. This implies contraction rate with respect to the \(\|\cdot\|_{L^{2}(\mathcal{X},G)}\)-norm around \(\mathcal{A}f_{0}\), similarly to (8). In the third step, using the previous result on the forward map, we derive contraction rates around \(f_{0}\). To achieve this we apply the modulus of continuity techniques introduced in [25]. Notably, we extend their ideas to infinite Gaussian series priors in the severely ill-posed case as well. Since in all these steps we can preserve the exponential upper bound for the posterior contraction (on a large enough event), we can apply Theorem 5 of [46], resulting in contraction rates for the VB procedure. It requires a control of the expected KL divergence between these two distributions, which follows from our assumptions on the expected trace and spectral norm of the covariance matrix of \(\bm{\mathcal{A}f}|\bm{u}\), see Lemma 3 in [37] for the identity operator \(\mathcal{A}\).

We briefly discuss the above results. First of all, the \(L_{2}(\mathcal{T};\mu)\)-contraction rate of the true posterior, to the best of our knowledge, wasn't derived explicitly in the literature before, hence it is of interest in its own right. Nevertheless, the main message is that the variational posterior achieves the same contraction rate as the true posterior under the assumption (7). Note that in the mildly ill-posed inverse problem case for eigenvalues \(\lambda_{j}\asymp j^{-1-2\beta}\) (i.e. taking \(\alpha=\beta\)), the posterior contracts with the minimax rate \(n^{-\beta/(1+2\beta+2p)}\). Note that the \(d\)-dimensional case directly follows from this result when one defines the regularity class (2) and ill-posedness (Definition 1) with \(\beta/d\) and \(p/d\) which would imply the rate \(n^{-\beta/(d+2\beta+2p)}\). Similarly in the severely-ill posed case one can achieve the minimax logarithmic contraction rate. Furthermore, the choice of the eigenvalue structure in the theorem was done for computational convenience, the results can be generalised for other choices of \(\lambda_{j}\) as well. Though we considered the random variables \(u\) fixed as we do not optimize them above, they could conceivably be considered as free variational parameters and selected at the same time as \(\bm{\mu}_{u}\) and \(\Sigma_{u}\).

In the next subsection we consider two specific choices of the inducing variables, i.e. the population spectral feature method and its empirical counter part. We show that under sufficient condition on the number of inducing variables condition (7) is satisfied implying the contraction rate results derived in the preceding theorem.

### Population and empirical spectral features methods

We focus here on two inducing variables methods, based on the spectral features (i.e. eigenspectrum) of the empirical covariance matrix \(K_{\bm{\mathcal{A}f}\bm{\mathcal{A}f}}=E_{\Pi}\mathcal{A}\mathcal{A}\mathcal{A }f^{T}\) and the corresponding population level covariance operator \((x,y)\mapsto E_{\Pi}\mathcal{A}f(x)\mathcal{A}f(y)\).

We start with the former method and consider inducing variables of the form

\[u_{j}=\sum\nolimits_{i=1}^{n}v_{j}^{i}\mathcal{A}f(x_{i}),\quad j=1,\ldots,m,\] (9)

where \(\mathbf{v}_{j}=(v_{j}^{1},\ldots,v_{j}^{n})\) is the eigenvector of \(K_{\bm{\mathcal{A}f}\bm{\mathcal{A}f}}\) corresponding to the \(j\)th largest eigenvalue \(\rho_{j}\) of this matrix. Similarly to the direct problem studied in [8; 37], this results in \((K_{\bm{\mathcal{A}f}\bm{\mathcal{A}f}})_{ij}=\rho_{j}\delta_{ij}\), \((K_{\bm{\mathcal{A}f}\bm{\mathcal{A}f}})_{ij}=\rho_{j}v_{j}^{i}\), \(Q_{\bm{\mathcal{A}f}\bm{\mathcal{A}f}}=\sum_{j=1}^{m}\rho_{j}\mathbf{v}_{j} \mathbf{v}_{j}^{T}\), and \(K_{\bm{\mathcal{A}f}\bm{\mathcal{A}f}}-Q_{\bm{\mathcal{A}f}\bm{\mathcal{A}f}}= \sum_{j=m+1}^{n}\rho_{j}\mathbf{v}_{j}\mathbf{v}_{j}^{T}\). The computational complexity of deriving the first \(m\) eigenvectors of \(K_{\bm{\mathcal{A}f}\bm{\mathcal{A}f}}\) is \(\mathcal{O}(n^{2}m)\). This is still quadratic in \(n\), which sets limitations to its practical applicability, but it can be computed for arbitrary choices of the prior covariance operator and map \(\mathcal{A}\). We also note that this choice gives the optimal rank-\(m\) approximation \(Q_{\boldsymbol{\mathcal{A}}\boldsymbol{\mathcal{f}}\boldsymbol{\mathcal{A}}}\) of \(K_{\boldsymbol{\mathcal{A}}\boldsymbol{\mathcal{f}}\boldsymbol{\mathcal{A}}}\) and it was noted in [8] that it gives the minimiser of the trace and norm terms in (7).

The second inducing variables method is based on the eigendecomposition of covariance kernel \((x,y)\mapsto E_{\Pi}\mathcal{A}f(x)\mathcal{A}f(y)\). Let us consider the variables

\[u_{j}=\int_{\mathcal{X}}\mathcal{A}f(x)e_{j}(x)dG(x),\quad j=1,\ldots,m.\] (10)

Again, by extending the results derived in the direct problem [8] to the inverse setting, this results in \(\left(K_{\boldsymbol{\mathcal{A}}\boldsymbol{\mathcal{f}}\boldsymbol{ \mathcal{A}}}\right)_{ij}=\lambda_{j}\kappa_{j}\delta_{ij}\), \(\left(K_{\boldsymbol{\mathcal{A}}\boldsymbol{\mathcal{f}}\boldsymbol{\mathcal{ A}}}\right)_{ij}=\lambda_{j}\kappa_{j}\boldsymbol{\phi}_{j}^{i}\), \(Q_{\boldsymbol{\mathcal{A}}\boldsymbol{\mathcal{f}}\boldsymbol{\mathcal{A}} \boldsymbol{\mathcal{f}}}=\sum_{j=1}^{m}\lambda_{j}\kappa_{j}\boldsymbol{\phi }_{j}\boldsymbol{\phi}_{j}^{T}\), and \(K_{\boldsymbol{\mathcal{A}}\boldsymbol{\mathcal{f}}\boldsymbol{\mathcal{A}} \boldsymbol{\mathcal{f}}}-Q_{\boldsymbol{\mathcal{A}}\boldsymbol{\mathcal{f}} \boldsymbol{\mathcal{A}}\boldsymbol{\mathcal{f}}}=\sum_{j=m+1}^{n}\lambda_{j} \kappa_{j}\boldsymbol{\phi}_{j}\boldsymbol{\phi}_{j}^{T}\), where \(\boldsymbol{\phi}_{j}=\left(\phi_{j}(x_{1}),\ldots,\phi_{j}(x_{n})\right)^{T}\). The computational complexity of this method is \(O(nm^{2})\), which is substantially faster than its empirical counter part. However, it requires the exact knowledge of the eigenfunctions of the prior covariance kernel, and therefore in general has limited practical applicability.

**Corollary 1**.: _Let's assume that \(f_{0}\in\bar{H}^{\beta}\), \(\left\|g_{j}\right\|_{\infty}\lesssim j^{\gamma}\) for \(\beta>0,\gamma\geq 0\) and in the_

1. _mildly-ill posed case_ \(\kappa_{j}\asymp j^{-p}\)_: take prior eigenvalues_ \(\lambda_{j}\asymp j^{-1-2\alpha}\) _for some_ \(\alpha>0\)_,_ \(\left(\alpha\wedge\beta\right)+p>3/2+2\gamma\)_, number of inducing variables_ \(m_{n}\geq n^{\frac{1}{\left(1+2p+2\alpha\right)}}\) _and denote by_ \(\varepsilon_{n}^{\text{intr}}=n^{-\frac{\alpha\wedge\beta}{1+2\alpha+2p}}\)_._
2. _severely ill-posed case_ \(\kappa_{j}\asymp e^{-cj^{p}}\)_: take prior eigenvalues_ \(\lambda_{j}\asymp j^{-\alpha}e^{-\xi j^{p}}\) _for_ \(\alpha\geq 0\)_,_ \(\xi>0\)_, number of inducing variables_ \(m_{n}^{p}\geq\left(\xi+2c\right)^{-1}\log n\)_, and introduce the notation_ \(\varepsilon_{n}^{\text{intr}}=\log^{-\beta/p}n\)_._

_Then both for the population (if \(\gamma=0\) in 1.) and empirical spectral features variational methods the corresponding variational posterior distribution contracts around the truth with the rate \(\varepsilon_{n}^{\text{intr}}\), i.e._

\[E_{f_{0}}\Psi^{*}\left[f\colon\ \left\|f-f_{0}\right\|_{L_{2}(T;\mu)}\geq M_{n} \varepsilon_{n}^{\text{intr}}\right]\to 0,\quad M_{n}\to\infty.\]

**Remark 1**.: _In the mildly ill-posed inverse problem taking \(\alpha=\beta\) results in the minimax contraction rate for \(m_{n}\geq n^{\frac{1}{1+2p+2\alpha}}\). Note that it is substantially less compared to the direct problem with \(p=0\), hence the computation is even faster in the inverse problem case._

## 3 Examples

In this section we provide three specific linear inverse problems as examples. The Volterra (integral) operator and the Radon transformations are mildly ill-posed, while the heat-equation is a severely ill-posed inverse problem. We show that in all cases by optimally tunning the GP prior and including enough inducing variables, the variational approximation of the posterior provides (from a minimax perspective) optimal recovery of the underlying signal \(f_{0}\).

### Volterra operator

First, let us consider the Volterra operator, \(\mathcal{A}:L_{2}[0,1]\longrightarrow L_{2}[0,1]\) satisfying that

\[\mathcal{A}f(x)=\int_{0}^{x}f(s)ds,\quad\mathcal{A}^{*}f(x)=\int_{x}^{1}f(s)ds.\] (11)

The eigenvalues of \(\mathcal{A}^{*}\mathcal{A}\) and the corresponding eigenbases are given by \(\kappa_{j}^{2}=(j-1/2)^{-2}\pi^{-2},\ e_{j}(x)=\sqrt{2}\cos\left((j-1/2)\pi x \right),\ g_{j}(x)=\sqrt{2}\sin\left((j-1/2)\pi x\right)\) respectively, see [21]. Therefore the problem is mildly ill-posed with degree \(p=1\) and these bases are uniformly bounded, i.e. \(\sup_{j}\|e_{j}\|_{\infty}\vee\|g_{j}\|_{\infty}<\infty\). The following lemma is then a direct application of Corollary 1.

**Corollary 2**.: _Consider the Volterra operator in (1) and assume that \(f_{0}\in\bar{H}^{\beta}\), for some \(\beta>1/2\). Set the eigenvalues in (4) as \(\lambda_{j}=j^{-1-2\beta}\). Then the variational posterior \(\Psi^{*}\) resulting from either the empirical or population spectral features inducing variable methods achieves the minimax contraction rate if the number of inducing variables exceeds \(m_{n}\gtrsim n^{\frac{1}{3+2\beta}}\), i.e. for arbitrary \(M_{n}\to\infty\)_

\[E_{f_{0}}\Psi^{*}\left[\left\|f-f_{0}\right\|_{L_{2}[0,1]}\geq M_{n}n^{-\beta/ (3+2\beta)}\right]\to 0.\]

### Heat equation

Next let us consider the problem of recovering the initial condition for the heat equation. The heat equation is often considered as the starting example in the PDE literature and, for instance, the Black-Scholes PDE can be converted to the heat equation as well. We consider the Dirichlet boundary condition

\[\frac{\partial}{\partial t}u(x,t)=\frac{\partial^{2}}{\partial x^{2}}u(x,t), \quad u(x,0)=\mu(x),\quad u(0,t)=u(1,t)=0,\] (12)

for \(u\) defined on \([0,1]\times[0,T]\), \(T>0\). For \(\mu\in L_{2}[0,1]\), \(u(x,t)=\sqrt{2}\sum_{j=1}^{\infty}\mu_{j}e^{-j^{2}\pi^{2}t}\sin(j\pi x)\), with \(\mu_{j}=\sqrt{2}\int_{0}^{1}\mu(s)\sin(j\pi s)ds\). Therefore, if \(\mathcal{A}:\mathcal{D}\mapsto\mathcal{D}\), with \(\mathcal{D}:=\left\{f\in L_{2}[0,1],\,f(0)=f(1)=0\right\}\), is such that, for \(\mu=f\), \(\mathcal{A}f(x)=u(x,T)\), then the corresponding singular-values and singular-functions of the operator \(\mathcal{A}\) are \(\kappa_{j}=e^{-j^{2}\pi^{2}T}\) and \(e_{j}(x)=g_{j}(x)=\sqrt{2}\sin(j\pi x)\). Therefore it is a severely ill-posed problem with \(p=2\) and \(c=\pi^{2}T\). We also note that \(\sup_{j}\|e_{j}\|_{\infty}\vee\|g_{j}\|_{\infty}<\infty\). This problem has been well studied both in the frequentist [6; 19; 32] and Bayesian setting [28; 54]. Then, by direct application of Corollary 1 we can provide optimality guarantees for the variational Bayes procedure in this model as well.

**Corollary 3**.: _Consider the heat equation operator \(\mathcal{A}\) as above in the linear inverse regression model (1) and assume that \(f_{0}\in\hat{H}^{\beta}\) for some \(\beta>0\). Furthermore, we set the eigenvalues \(\lambda_{j}=j^{-\alpha}e^{-\xi j^{2}}\), \(\alpha\geq 0\), \(\xi>0\) in (4). Then the variational approximation \(\Psi^{*}\) resulting from either of the spectral features inducing variables method with \(m_{n}\geq\left(\xi+\pi^{2}T\right)^{-1/2}\log^{1/2}n\) achieves the minimax contraction rate, i.e. for arbitrary \(M_{n}\to\infty\)_

\[E_{f_{0}}\Psi^{*}\left[\left\|f-f_{0}\right\|_{L_{2}([0,1])}\geq M_{n}\log^{- \beta/2}n\right]\to 0.\]

### Radon transform

Finally, we consider the Radon transform [24], where for some (Lebesgue)-square-integrable function \(f:\ D\to\mathbb{R}\) defined on the unit disc \(D=\left\{x\in\mathbb{R}^{2}:\ \left\|x\right\|_{2}\leq 1\right\}\), we observe its integrals along any line intersecting \(D\). If we parameterized the lines by the length \(s\in[0,1]\) of their perpendicular from the origin and the angle \(\phi\in[0.2\pi)\) of the perpendicular to the x-axis, we observe

\[\mathcal{A}f(s,\phi)=\frac{\pi}{2\sqrt{1-s^{2}}}\int_{-\sqrt{1-s^{2}}}^{\sqrt{ 1-s^{2}}}f(s\cos\phi-t\sin\phi,s\sin\phi+t\cos\phi)dt,\] (13)

where \((s,\phi)\in S=[0,1]\times[0,2\pi)\). The Radon transform is then a map from \(\mathcal{A}\colon L_{2}(D;\mu)\to L_{2}(S;G)\), where \(\mu\) is \(\pi^{-1}\) times the Lebesgue measure and \(dG(s,\phi)=2\pi^{-1}\sqrt{1-s^{2}}dsd\phi\). Then \(\mathcal{A}\) is a bijective, mildly ill-posed linear operator of order \(p=1/4\). Furthermore, the operator's singular value decomposition can be computed via Zernike polynomials \(Z_{m}^{k}\) (degree \(m\), order \(k\)) and Chebyshev polynomials of the second kind \(U_{m}(\cos\theta)=\sin\left((m+1)\theta\right)/\sin\theta\leq m+1\), see [24]. Translating it to the single index setting, we get for some functions \(l,m:\mathbb{N}\mapsto\mathbb{N}\) satisfying \(m(j)\asymp\sqrt{j}\) and \(|l(j)|\leq m(j)\), that

\[e_{j}(r,\theta)=\sqrt{m(j)+1}Z_{m(j)}^{|l(j)|}e^{jl(j)\theta},\quad g_{j}(s, \phi)=U_{m(j)}(s)e^{jl(j)\phi},\]

if polar coordinates are used on \(D\). Therefore, we have \(\sup_{j}(\|e_{j}\|_{\infty}\vee\|g_{j}\|_{\infty})/\sqrt{j}<\infty\). Then, by directly applying Corollary 1 to this setting we can show that the variational Bayes method achieves the minimax contraction rate.

**Corollary 4**.: _Consider the Radon transform operator (13) in the inverse regression model (1) and let us take \(f_{0}\in\hat{H}^{\beta}\), \(\beta>9/4\). Taking polynomially decaying eigenvalues \(\lambda_{j}\asymp j^{-1-2\beta}\), the empirical spectral features variational Bayes method achieves the optimal minimax contraction rate if \(m_{n}\gtrsim n^{1/(3/2+2\beta)}\), i.e. for any \(M_{n}\to\infty\)_

\[E_{f_{0}}\Psi^{*}\left[\left\|f-f_{0}\right\|_{L_{2}(D;\mu)}\geq M_{n}n^{- \beta/(3/2+2\beta)}\right]\to 0.\]

## 4 Numerical analysis

We demonstrate the approximation accuracy of the variational Bayes method on synthetic data. We consider here the recovery of the initial condition of the heat condition 3.2, which is a severelyill-posed. In the supplement we provide additional simulation study for mildly ill-posed inverse problems as well. We set the sample size \(n=8000\), take uniformly distributed covariates on \([0,1)\), and let

\[f_{0}(t)=\sqrt{2}\sum\nolimits_{j}c_{j}j^{-(1+\beta)}\sin(j\pi t),\quad c_{j}= \begin{cases}1+0.4\sin(\sqrt{5}\pi j),&j\text{ odd,}\\ 2.5+2\sin(\sqrt{2}\pi j),&j\text{ even,}\end{cases}\]

for \(\beta=1\). The independent- observations are generated as \(Y_{i}\sim\mathcal{N}(\mathcal{A}f_{0}(x_{i}),1)\), depending on the solution of the forward map \(\mathcal{A}f_{0}\) after time \(T=10^{-2}\).

We consider the prior with \(\lambda_{j}=e^{-\xi j^{2}}\) for \(\xi=10^{-1}\). In view of Corollary 3 the optimal number of inducing variables is \(m=\left(\xi+2\pi^{2}T\right)^{-1/2}\log^{1/2}n\approx 6\). We consider the population spectral feature method described in (10) and plot the variational approximation of the posterior for \(m=6\) and \(m=3\) inducing variables in Figure 1. We represent the true posterior mean by solid red and the upper and lower pointwise \(2.5\%\) quantiles by dashed red curves. The true function is given by blue and the mean and quantiles of the variational approximation by solid and dotted purple curves, respectively.

Observe that with \(m=6\), see left part of Figure 1, the variational approximation results in similar 95% pointwise credible bands and posterior mean as the true posterior, providing an accurate approximation. Also note that both the true and the variational posterior contain \(f_{0}\) at most of the points, indicating frequentist confidence validity of the set. At the same time, by taking a factor of two less inducing points, i.e. \(m=3\), the credible sets will be overly large, resulting in good frequentist coverage, but suboptimally large posterior spread, see the second plot in Figure 1.

The computations were carried out with a 2,6 GHz Quad-Core Intel Core i7 processor. The computation of the exact posterior mean and covariance kernel on a grid of \(100\) points took over half an hour (in CPU time), while the variational approximation was substantially faster, taking only \(50.5\) ms, resulting in a \(3.68*10^{4}\) times speed.

A more extensive numerical analysis is available in the appendix, considering the application of our method to the settings of Sections 3.1 and 3.3 as well. We conduct these experiments several times and compare the average Mean Integrated Squared Error (MISE), see appendix A, and compute time for different choices of \(m\). We observe that in all our examples, while increasing \(m\) results in longer computation, the MISE does not improve after a threshold close to the one presented in our results.

Figure 1: True and variational posterior means and credible regions for Gaussian series prior (sine basis) on the initial condition \(\mu=f_{0}\) of the heat equation (12), for \(m=6\) (left) or \(m=3\) (right) inducing variables from method (10).

Therefore, it is sufficient to include as many inducing variables as we considered in Corollary 1 to obtain better performance. More than that would would only increase the computation complexity. In the Appendix, we also provide a literature review and some justifications of how relevant these problems are in practice

## 5 Discussion

We have extended the inducing variables variational Bayes method for linear inverse problems and derived asymptotic contraction rate guarantees for the corresponding variational posterior. Our theoretical results provide a guide for practitioners on how to tune the prior distribution and how many inducing variables to apply (in the spectral feature variational Bayes method) to obtain minimax rate optimal recovery of the true functional parameter of interest. We have demonstrated the practical relevance of this guideline numerically on synthetic data and have shown that using less variables results in highly suboptimal recovery.

In our analysis we have considered priors built on the singular basis of the operator \(\mathcal{A}\). In principle our results can be extended to other priors as well, until the eigenbasis of the covariance operator is not too different from the basis of the operator \(\mathcal{A}\). This, however, would complicate the computation of the Kullback -Leibler divergence between the variational family and the posterior, resulting in extra technical challenges. In this setting the empirical spectral features method seems practically more feasible, especially, if the eigenbasis of the covariance kernel is not known explicitly. In the literature several different types of inducing variable methods were proposed, considering other, practically more relevant approaches of interest. Furthermore, extension to other type of inverse problems is also feasible. For instance in the deconvolution problem, when \(f_{0}\) is convoluted with a rectangular kernel, the eigenvalues given by the SVD are the product of a polynomially decaying and oscillating part and the "average degree" of ill-posedness does not match the lower and upper bounds [23]. Extension to non-linear inverse problem is highly relevant, as these problems tend to be computationally even more complex, but very challenging. One possible approach is to linearize the problem and take its variational approximation. Finally, it is of importance to derive frequentist coverage guarantees for VB credible sets. Our approach cannot directly be extended for this task. However, in the direct case, for some special choices of inducing variables, frequentist coverage guarantees were derived using kernel ridge regression techniques [38; 60]. This result, although computationally somewhat cumbersome, in principle can be extended to the inverse setting as well. One last drawback of our results is that the priors we consider are non-adaptative in the mildly ill-posed case. Minimax contraction rates are attainable only if the covariance eigenvalues are properly tuned, given the smoothness \(\beta\). While this is not an issue the severely ill-posed case in our results, we keep the study of adaptation for future works as it is a much more involved question.

_Funding._ Co-funded by the European Union (ERC, BigBayesUQ, project number: 101041064). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them.

## References

* [1]Agapiou, S., Larsson, S., and Stuart, A. M. Posterior contraction rates for the Bayesian approach to linear ill-posed inverse problems. _Stochastic Process. Appl. 123_, 10 (2013), 3828-3860.
* [2]Alquier, P., and Ridgway, J. Concentration of tempered posteriors and of their variational approximations. _Ann. Statist. 48_, 3 (2020), 1475-1497.
* [3]Arridge, S., Maass, P., Oktem, O., and Schonlieb, C.-B. Solving inverse problems using data-driven models. _Acta Numerica 28_ (2019), 1-174.
* [4]Bertero, M. Regularization methods for linear inverse problems. In _Inverse problems (Montecatini Terme, 1986)_, vol. 1225 of _Lecture Notes in Math._ Springer, Berlin, 1986, pp. 52-112.
* [5]Bissantz, N., Hohage, T., Munk, A., and Ruymgaart, F. Convergence rates of general regularization methods for statistical inverse problems and applications. _SIAM J. Numer. Anal. 45_, 6 (2007), 2610-2636.
* [6]Bissantz, N., and Holzmann, H. Statistical inference for inverse problems. _Inverse Problems 24_, 3 (2008), 034009.

* [7]Briol, F.-X., Oates, C. J., Girolami, M., Osborne, M. A., and Sejdinovic, D. Probabilistic integration. _Statistical Science 34_, 1 (2019), 1-22.
* [8]Burt, D., Rasmussen, C. E., and Van Der Wilk, M. Rates of convergence for sparse variational Gaussian process regression. In _Proceedings of the 36th International Conference on Machine Learning_ (09-15 Jun 2019), K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97 of _Proceedings of Machine Learning Research_, PMLR, pp. 862-871.
* [9]Candes, E. J., and Wakin, M. B. An introduction to compressive sampling. _IEEE signal processing magazine 25_, 2 (2008), 21-30.
* [10]Castillo, I. Lower bounds for posterior rates with Gaussian process priors. _Electron. J. Stat. 2_ (2008), 1281-1299.
* [11]Cavalier, L. Nonparametric statistical inverse problems. _Inverse Problems 24_, 3 (2008), 034004, 19.
* [12]Cotter, S. L., Dashti, M., and Stuart, A. M. Approximation of Bayesian inverse problems for PDEs. _SIAM J. Numer. Anal. 48_, 1 (2010), 322-345.
* [13]Csato, L. Gaussian processes:iterative sparse approximations. If you have discovered material in AURA which is unlawful e.g. breaches copyright, (either yours or that of a third party) or any other law, including but not limited to those relating to patent, trademark, confidentiality, data protection, obscesity, defamation, libel, then please read our Takedown Policy and contact the service immediately., March 2002.
* [14]Csato, L., and Opper, M. Sparse On-Line Gaussian Processes. _Neural Computation 14_, 3 (03 2002), 641-668.
* [15]Florens, J.-P., and Simoni, A. Regularizing priors for linear inverse problems. _Econometric Theory 32_, 1 (2016), 71-121.
* [16]Gardner, J., Pleiss, G., Weinberger, K. Q., Bindel, D., and Wilson, A. G. Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration. In _Advances in Neural Information Processing Systems_ (2018), S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds., vol. 31, Curran Associates, Inc.
* [17]Ghosal, S., and van der Vaart, A. Convergence rates of posterior distributions for non-i.i.d. observations. _Ann. Statist. 35_, 1 (2007), 192-223.
* [18]Goldenshluger, A., and Pereverzev, S. V. On adaptive inverse estimation of linear functionals in Hilbert scales. _Bernoulli 9_, 5 (2003), 783-807.
* [19]Golubev, G. K., and Khas' minskii, R. Z. A statistical approach to some inverse problems for partial differential equations. _Problemy Peredachi Informatsii 35_, 2 (1999), 51-66.

* [21]Halmos, P. R. _A Hilbert space problem book_, vol. 19. Springer Science & Business Media, 2012.
* [22]Hegde, P., Yildiz, C., Lahdesmaki, H., Kaski, S., and Heinonen, M. Variational multiple shooting for bayesian odes with gaussian processes. In _Uncertainty in Artificial Intelligence_ (2022), PMLR, pp. 790-799.
* [23]Johnstone, I. M., and Raimondo, M. Periodic boxcar deconvolution and Diophantine approximation. _Ann. Statist. 32_, 5 (2004), 1781-1804.
* [24]Johnstone, I. M., and Silverman, B. W. Speed of estimation in positron emission tomography and related inverse problems. _Ann. Statist. 18_, 1 (1990), 251-280.

* [26]Knapik, B. T., Szabo, B. T., van der Vaart, A. W., and van Zanten, J. H. Bayes procedures for adaptive inference in inverse problems for the white noise model. _Probab. Theory Related Fields 164_, 3-4 (2016), 771-813.
* [27]Knapik, B. T., van der Vaart, A. W., and van Zanten, J. H. Bayesian inverse problems with Gaussian priors. _Ann. Statist. 39_, 5 (2011), 2626-2657.

* [28]Knapik, B. T., Van Der Vaart, A. W., and van Zanten, J. H. Bayesian recovery of the initial condition for the heat equation. _Communications in Statistics-Theory and Methods 42_, 7 (2013), 1294-1313.
* [29]Law, K., Stuart, A., and Zygalakis, K. Data assimilation. _Cham, Switzerland: Springer 214_ (2015), 52.
* [30]Lazaro-Gredilla, M., and Figueiras-Vidal, A. Inter-domain gaussian processes for sparse inference using inducing features. In _Advances in Neural Information Processing Systems_ (2009), Y. Bengio, D. Schuurmans, J. Lafferty, C. Williams, and A. Culotta, Eds., vol. 22, Curran Associates, Inc.
* [31]Maestrini, L., Aykroyd, R. G., and Wand, M. P. A variational inference framework for inverse problems. _arXiv preprint arXiv:2103.05909_ (2021).
* [32]Mair, B. A., and Ruymgaart, F. H. Statistical inverse estimation in hilbert scales. _SIAM Journal on Applied Mathematics 56_, 5 (1996), 1424-1444.
* [33]Matthews, A. G. d. G., Hensman, J., Turner, R., and Ghahramani, Z. On sparse variational methods and the kullback-leibler divergence between stochastic processes. In _Proceedings of the 19th International Conference on Artificial Intelligence and Statistics_ (Cadiz, Spain, 09-11 May 2016), A. Gretton and C. C. Robert, Eds., vol. 51 of _Proceedings of Machine Learning Research_, PMLR, pp. 231-239.
* [34]Meng, R., and Yang, X. Sparse gaussian processes for solving nonlinear pdes. _arXiv preprint arXiv:2205.03760_ (2022).
* [35]Nashed, M. Z., and Wahba, G. Generalized inverses in reproducing kernel spaces: An approach to regularization of linear operator equations. _SIAM Journal on Mathematical Analysis 5_, 6 (1974), 974-987.
* [36]Nickl, R. Bayesian non-linear statistical inverse problems. _Lecture Notes ETH Zurich_ (2022).
* [37]Nieman, D., Szabo, B., and van Zanten, H. Contraction rates for sparse variational approximations in gaussian process regression. _Journal of Machine Learning Research 23_, 205 (2022), 1-26.
* [38]Nieman, D., Szabo, B., and van Zanten, H. Uncertainty quantification for sparse spectral variational approximations in gaussian process regression. _arXiv preprint arXiv:2212.11031_ (2022).
* [39]Pati, D., Bhattacharya, A., and Cheng, G. Optimal Bayesian estimation in random covariate design with a rescaled Gaussian process prior. _J. Mach. Learn. Res. 16_ (2015), 2837-2851.
* [40]Pinski, F. J., Simpson, G., Stuart, A. M., and Weber, H. Algorithms for kullback-leibler approximation of probability measures in infinite dimensions. _SIAM Journal on Scientific Computing 37_, 6 (2015), A2733-A2757.
* [41]Pinski, F. J., Simpson, G., Stuart, A. M., and Weber, H. Kullback-leibler approximation for probability measures on infinite dimensional spaces. _SIAM Journal on Mathematical Analysis 47_, 6 (2015), 4091-4122.
* [42]Povala, J., Kazlauskaite, I., Febrianto, E., Cirak, F., and Girolami, M. Variational bayesian approximation of inverse problems using sparse precision matrices. _Computer Methods in Applied Mechanics and Engineering 393_ (2022), 114712.
* [43]Quinonero Candela, J., and Rasmussen, C. E. A unifying view of sparse approximate gaussian process regression. _J. Mach. Learn. Res. 6_ (dec 2005), 1939-1959.
* [44]Rasmussen, C. E., and Williams, C. K. I. _Gaussian processes for machine learning_. Adaptive Computation and Machine Learning. MIT Press, Cambridge, MA, 2006.

* [46]Ray, K., and Szabo, B. Variational bayes for high-dimensional linear regression with sparse priors. _Journal of the American Statistical Association 117_, 539 (2022), 1270-1281.

* [49]Seeger, M. Bayesian gaussian process models: Pac-bayesian generalisation error bounds and sparse approximations. Tech. rep., University of Edinburgh, 2003.
* [50]Seeger, M. Pac-bayesian generalisation error bounds for gaussian process classification. _J. Mach. Learn. Res. 3_, null (mar 2003), 233-269.
* [51]Seeger, M. W., Williams, C. K. I., and Lawrence, N. D. Fast forward selection to speed up sparse gaussian process regression. In _Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics_ (03-06 Jan 2003), C. M. Bishop and B. J. Frey, Eds., vol. R4 of _Proceedings of Machine Learning Research_, PMLR, pp. 254-261. Reissued by PMLR on 01 April 2021.
* [52]Snelson, E., and Ghahramani, Z. Sparse gaussian processes using pseudo-inputs. In _Advances in Neural Information Processing Systems_ (2005), Y. Weiss, B. Scholkopf, and J. Platt, Eds., vol. 18, MIT Press.

* [54]Stuart, A. M. Inverse problems: a bayesian perspective. _Acta numerica 19_ (2010), 451-559.
* [55]Szabo, B., van der Vaart, A. W., and van Zanten, H. Frequentist coverage of adaptive nonparametric bayesian credible sets. _Ann. Statist. 43_, 4 (2015), 1391-1428. (with discussion).
* [56]Tikhonov, A. N. Solution of incorrectly formulated problems and the regularization method. _Soviet Math. 4_ (1963), 1035-1038.
* [57]Titsias, M. Variational learning of inducing variables in sparse gaussian processes. In _Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics_ (Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA, 16-18 Apr 2009), D. van Dyk and M. Welling, Eds., vol. 5 of _Proceedings of Machine Learning Research_, PMLR, pp. 567-574.
* [58]Titsias, M. K. Variational model selection for sparse gaussian process regression. _Report, University of Manchester, UK_ (2009).
* [59]Travis, L., and Ray, K. Pointwise uncertainty quantification for sparse variational gaussian process regression with a brownian motion prior. _NeurIPS 2023_ (2023).
* [60]Vakili, S., Scarlett, J., Shiu, D.-s., and Bernacchia, A. Improved convergence rates for sparse approximation methods in kernel-based learning. In _International Conference on Machine Learning_ (2022), PMLR, pp. 21960-21983.
* [61]van der Vaart, A., and van Zanten, H. Information rates of nonparametric Gaussian process methods. _J. Mach. Learn. Res. 12_ (2011), 2095-2119.
* [62]van der Vaart, A. W., and van Zanten, J. H. Rates of contraction of posterior distributions based on Gaussian process priors. _Ann. Statist. 36_, 3 (2008), 1435-1463.
* [63]Wang, Y., and Blei, D. M. Frequentist consistency of variational Bayes. _J. Amer. Statist. Assoc. 114_, 527 (2019), 1147-1161.
* [64]Yang, Y., Pati, D., and Bhattacharya, A. \(\alpha\)-variational inference with statistical guarantees. _Ann. Statist. 48_, 2 (2020), 886-905.