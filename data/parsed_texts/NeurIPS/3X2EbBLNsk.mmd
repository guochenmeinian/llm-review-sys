# Birth of a Transformer: A Memory Viewpoint

 Alberto Bietti

Flatiron Institute

&Vivien Cabannes

FAIR, Meta

&Diane Bouchacourt

FAIR, Meta

&Herve Jegou

FAIR, Meta

&Leon Bottou

FAIR, Meta

Work done while at FAIR, Meta.

###### Abstract

Large language models based on transformers have achieved great empirical successes. However, as they are deployed more widely, there is a growing need to better understand their internal mechanisms in order to make them more reliable. These models appear to store vast amounts of knowledge from their training data, and to adapt quickly to new information provided in their context or prompt. We study how transformers balance these two types of knowledge by considering a synthetic setup where tokens are generated from either global or context-specific bigram distributions. By a careful empirical analysis of the training process on a simplified two-layer transformer, we illustrate the fast learning of global bigrams and the slower development of an "induction head" mechanism for the in-context bigrams. We highlight the role of weight matrices as associative memories, provide theoretical insights on how gradients enable their learning during training, and study the role of data-distributional properties.

## 1 Introduction

As large language models (LLMs) are growing in usage and deployment, it is increasingly important to open the black box and understand how they work. A better understanding can help with interpretability of how these models make decisions, and will be crucial to improve these models and mitigate their failure cases, such as hallucinations or reasoning errors.

An important ingredient in the success of recent LLMs is their ability to learn and reason from information present in their context [6]. These "in-context" learning capabilities are often attributed to the transformer architecture [52], in particular its self-attention blocks, which are able to carefully select parts of the input sequence in order to infer plausible next tokens. Additionally, predictions may require "global" knowledge, such as syntactic rules or general facts, which may not appear in the context and thus needs to be stored in the model.

In order to better understand how transformers develop these capabilities during training, we introduce a synthetic dataset that exhibits both aspects. It consists of sequences generated from a bigram language model, but where some of the bigrams are specific to each sequence. Then, the model needs to rely on in-context learning for good prediction on the sequence-specific bigrams, while the global bigrams can be guessed from global statistics conditioned on the current token. While one-layer transformers fail to reliably predict the in-context bigrams, we find that two-layer transformers succeed by developing an _induction head_ mechanism [16; 40], namely a "circuit" of two attention heads that allows the transformer to predict b from a context \([\cdots,\mathtt{a},\mathtt{b},\cdots,\mathtt{a}]\), and which appears to be ubiquitous in transformer language models [40; 54].

In order to obtain a fine-grained understanding of how this in-context mechanism emerges during training, we further simplify the two-layer architecture by freezing some of the layers at random initialization, including embeddings and value matrices. This focuses our study on attention and feed-forward mechanisms, while avoiding the difficulty of learning representations, which mayrequire complex nonlinear dynamics [15; 33; 45]. This simplification also allows us to introduce a natural model for individual weight matrices as _associative memories_, which store input-output or key-value pairs of embeddings through their outer products. Random high-dimensional embeddings are particularly well-suited to this viewpoint thanks to their near-orthogonality. We provide a detailed empirical study of the training dynamics, by measuring how quickly each weight matrix learns to behave as the desired associative memory, studying how this is affected by data-distributional properties, and investigate the order in which layers are learned: the model first finds the right output associations from the current token and from uniform attention patterns, then the attention heads learn to focus on the correct key-value pairs. We then present theoretical insights on this top-down learning process through population gradient dynamics. Despite its simplicity, our setup already provides useful insights on the internal structure of transformer language models and its evolution throughout training, paving the way for a better understanding of LLMs. We hope that our insights may lead to future research and improvements for LLM practitioners, _e.g._, for optimization algorithms, data pre-processing and selection, interpretability, fine-tuning, and model editing.

In summary, we make the following contributions:

* We introduce a new synthetic setup to study global vs in-context learning: sequences follow bigram language models, where some bigrams change across sequences and others do not.
* We view the transformer's weight matrices as associative memories that learn to store specific pairs of embeddings, and use this to derive a simplified but more interpretable model for our task.
* We empirically study the training dynamics with careful probing: global bigrams are learned first, then the induction head is formed by learning appropriate memories in a top-down fashion.
* We give theoretical insights on training dynamics, showing how a few top-down gradient steps on the population loss can recover the desired associative memories by finding signal in noisy inputs.

Related work.After the success of transformer language models for in-context learning was found [6], several works have studied how in-context learning may arise in various contexts [8; 38; 43; 48; 57]. Multiple recent papers have introduced synthetic tasks in order to better understand and interpret transformers [9; 33; 39; 61]. Several works have attempted to understand internal mechanisms in transformers that are responsible for certain behaviors, an area known as "mechanistic interpretability" [16; 17; 35; 39; 40; 54]. Memory and neural networks have a long history of connections [5; 18; 19; 21; 26; 27; 30; 34; 50; 55; 56]. The associative memories we consider bear similarity to [29; 56], though we use continuous input/outputs. The reader may also be interested in Fast Weight programmers [46; 47]. The use of random vectors for storing memories is related to [23]. Our approach to probing based on memory recall is related to techniques in [13; 17], though motivated differently. [14; 32; 36] study statistical and approximation properties of transformers, highlighting benefits of sparse attention patterns, but do not consider training dynamics. [25; 31; 49; 51] provide theoretical analyses of learning dynamics in transformers and other attention models, but consider different data setups and focus on single-layer architectures, while we focus on two-layer models and take a different viewpoint based on associative memories.

## 2 Background

This section provides background on transformer architectures and induction head mechanisms.

Transformer architecture.Transformers [52] operate on sequences of embeddings by alternating self-attention operations and token-wise feed-forward layers. We focus on decoder-only, auto-regressive architectures with a causal attention mask, which are commonly used in large language models trained for next-token prediction [6; 11; 41; 42]. We ignore normalization layers in order to simplify the architecture, since its stability benefits are not as crucial in the small models we consider. Given an input sequence of tokens \(z_{1:T}\in[N]^{T}\) of length \(T\), where \(N\) is the vocabulary size, the transformer operates as follows:

* **Token embeddings**: each discrete token is mapped to a \(d\)-dimensional embedding via an embedding map \(W_{E}\in\mathbb{R}^{d\times N}\). We will denote the embeddings of tokens \(z_{t}\) by \(x_{t}:=w_{E}(z_{t})\), where \(w_{E}(j)\) is the \(j\)-th column of \(W_{E}\).
* **Positional embeddings**: the positional embeddings \(p_{t}=w_{P}(t)\in\mathbb{R}^{d}\) are added to each token embedding depending on its position in the sequence, leading to the following input embeddings: \[x_{t}:=x_{t}+p_{t}=w_{E}(z_{t})+w_{P}(t).\] (1)* **Attention blocks**: given an input sequence \(x_{1:T}\in\mathbb{R}^{d\times T}\) of embeddings, the causal attention block computes, for \(W_{K},W_{Q},W_{V},W_{O}\in\mathbb{R}^{d\times d}\) (key, query, value, output), and for each \(t\), \[x_{t}^{\prime}:=W_{O}W_{V}x_{1:t}\sigma(x_{1:t}^{\top}W_{K}^{\top}W_{Q}x_{t})\in \mathbb{R}^{d},\] (2) where \(\sigma\) takes the softmax of its elements, leading to an attention of the "values" \(W_{V}x_{t}\) with weights proportional to \(\exp((W_{K}x_{s})^{\top}(W_{Q}x_{t}))\). Note that the attention operation usually considers multiple "heads" that each projects the input to a lower dimension. Here we stick to a single head for simplicity, since it will be sufficient for our purposes. Rewriting (2) on each \(t\) as \(x_{1:T}^{\prime}=\mathcal{A}(x_{1:T};W_{K},W_{Q},W_{V},W_{O})\), the \(\ell\)-th layer of the transformer applies attention with layer-specific parameters along with a residual connection as follows:2 Footnote 2: We omit layer indices for simplicity of notation, and use the assignment operator \(:=\) instead. \[x_{1:T}:=x_{1:T}+\mathcal{A}(x_{1:T};W_{K}^{\ell},W_{Q}^{\ell},W_{V}^{\ell},W_ {O}^{\ell})\]
* **Feed-forward blocks**: feed-forward blocks operate on individual token embeddings after each attention block, typically by applying a one-hidden-layer MLP to each token, denoted \(\mathcal{F}(\cdot;W_{F})\), with a residual connection: at layer \(\ell\), we have \[x_{t}:=x_{t}+\mathcal{F}(x_{t};W_{F}).\] Our simplified setup will linear feed-forward layers: \(\mathcal{F}(x_{t};W_{F})=W_{F}x_{t}\).
* **Unembedding**: After the last transformer layer, the embeddings are mapped back to the vocabulary space \(\mathbb{R}^{N}\) through a linear "unembedding" layer \(W_{U}=[w_{U}(1),\dots,w_{U}(N)]^{\top}\in\mathbb{R}^{N\times d}\), where we refer to the \(w_{U}(j)\) as "output embeddings". The output of this layer is then fed into a cross-entropy loss for predicting of \(z_{t+1}\) from each \(x_{t}\).

We will sometimes refer to the representations \(x_{t}\) for a given token \(t\) throughout layers as its _residual stream_[16], since they consist of sums of embeddings and layer outputs due to residual connections.

Induction head mechanism.Induction heads [16, 40] are a particular type of mechanism (or "circuit") in transformers that allows basic in-context prediction of the form \([\cdots,\mathsf{a},\mathsf{b},\cdots,\mathsf{a}]\rightarrow\mathsf{b}\). These were found to be ubiquitous in transformer language models, playing a key role in enabling various forms of in-context learning. The basic mechanism consist of two attention heads in separate layers (see Figure 1 for an illustration): (i) the first is a _previous token head_ which attends to the previous token using positional information and copies its embedding to the next token; (ii) the second is the _induction head_ itself, which attends using the output of the previous token head, and outputs the original token. Our work focuses on this basic copy mechanism, but we note that richer behaviors are possible, particularly when combining multiple such mechanisms (_e.g._, [54]).

Figure 1: **Induction head mechanism**. Induction heads are a two-layer mechanism that can predict \(b\) from a context \([\dots,a,b,\dots,a]\). The first layer is a _previous token head_, which attends to the previous token based on positional embeddings (\(p_{t}\to p_{t-1}\)) and copies it after a remapping (\(w_{E}(a)\to w_{1}(a):=W_{O}^{\downarrow}W_{V}^{\downarrow}w_{E}(a)\)). The second layer is the _induction head_, which attends based on the output of the previous token head (\(w_{E}(a)\to w_{1}(a)\)) and outputs the attended token, remapped to output embeddings (\(w_{E}(b)\to w_{U}(b)\)). Boxes in the diagram represent different embeddings in superposition on each token’s residual stream (we omit some irrelevant ones for clarity, _e.g._, positional embeddings in upper layers), and attention and output associations are shown with the associative memory viewpoint presented in Section 4.

## 3 Synthetic Setup

In this section, we introduce our synthetic data setup, which allows us to carefully study how the induction head mechanism develops during training, and how transformers learn to use information from the context vs simple associations from the training data.

Bigram data model.Our model for sequences consists of a generic bigram language model (_i.e._, Markov chain), but where the transitions for a few _trigger tokens_ denoted \(q_{k}\) are modified in each sequence to always be followed by some _output tokens_\(o_{k}\). Let \(K\) be the number of trigger tokens, and fix the following distributions over the vocabulary \([N]\): \(\pi_{b}(\cdot|i)\), \(\pi_{u}\), \(\pi_{o}(\cdot|i)\) and \(\pi_{q}\), for \(i\in[N]\). \(\pi_{b}(\cdot|i)\) are the global bigram conditionals, \(\pi_{u}\) the global unigram distribution, while \(\pi_{o}\) is used to sample output tokens at each sequence. The triggers are either fixed to some predefined set of tokens \(Q\), or sampled from \(\pi_{q}\). Each sequence \(z_{1:T}^{n}\) is generated as follows:

* (optional) Sample \(q_{1},\dots,q_{K}\sim\pi_{q}\), i.i.d. without replacement (_random triggers_)
* Sample \(o_{k}\sim\pi_{o}(\cdot|q_{k})\), i.i.d. with replacement.
* Sample \(z_{1}^{n}\sim\pi_{u}\) and \(z_{t}^{n}|z_{t-1}^{n}\sim p_{n}(\cdot|z_{t-1}^{n})\) for \(t=2,\dots,T\), where \[p_{n}(j|i)=\begin{cases}\pi_{b}(j|i),&\text{ if }i\notin\{q_{k}\}_{k}\\ \mathbbm{1}\{j=o_{k}\},&\text{ if }i=q_{k}.\end{cases}\]

Experimental setup and initial experiment.Our experiments take \(\pi_{u}\) and \(\pi_{b}\) to be unigram and bigram character-level distributions estimated from the tiny Shakespeare dataset, with vocabulary size \(N=65\). We generally sample triggers from \(\pi_{q}=\pi_{u}\) or fix them to the \(K\) most frequent tokens. We sample uniform outputs \(o_{k}\) in most cases, but also experiment with \(\pi_{o}=\pi_{b}\) in Section 5.

As a preliminary experiment, we train a two-layer vanilla transformer with single-head attention layers and MLP feed-forward layers, following the training setup described in Section 5. On our synthetic data, with fixed (resp. random) triggers and uniform outputs, the model achieves over 99% accuracy (resp. 95%) on output tokens after the first occurrence, versus around 55% for one layer. This gap may be related to the difficulty of modeling three-way interactions with a single attention layer [44]. We visualize attention maps on test sequences in Figure 2, which shows that the model has learned an induction head mechanism. The sequence in the middle figure has \((q_{k},o_{k})\in\{(a,b),(t,s)\}\). For fixed triggers, the induction head is only active for the triggers used in training, which suggests the presence of a "memory" in the attention layer. For random triggers, it is active on every repeated token, so that the model then needs to disambiguate between in-context and global predictions. For instance, the model may choose to use the retrieved token when it is unlikely to be sampled from the global bigram distribution, something which we found to often be the case in practice.

## 4 The Associative Memory Viewpoint

In this section, we present our associative memory view on transformers: with nearly orthogonal embeddings, the weight matrices behave as associative memories which store pairs of embeddings as

Figure 2: **Induction head behavior in attention maps** observed on a 2-layer transformer trained on two variants of our synthetic dataset. Each row shows the attention pattern for predicting the next token. (left) The first layer head always attends to the previous token. (center) For fixed triggers \(Q=\{a,t\}\), the second layer head mainly attends to tokens following such triggers. (right) For random triggers, the induction head mechanism is active for any repeated token (here the only trigger is \(L\)). Red and green boxes highlight tokens following previous occurrences of the query, with red boxes corresponding to “correct” output tokens \(o_{k}\) following trigger tokens \(q_{k}\).

a weighted sum of their outer products. We then introduce a simplified transformer model with fixed random embeddings that will yield a precise understanding of learning dynamics using this viewpoint.

### Weight matrices as associative memories

While intermediate representations in the transformer consist of high-dimensional vectors in residual streams, they are often "collapsed" down to scalar measurements by testing against other representations, using operations of the form \(v_{j}^{\top}Wu_{i}\) for some matrix \(W\). For instance, \(u_{i}\) and \(v_{j}\) could be key and query vectors in an attention head, or input and output embeddings for predicting the next token. If \((u_{i})_{i}\) and \((v_{j})_{j}\) are orthonormal (or nearly-orthonormal) sets of embeddings, a natural way to store desired input-output associations \(i,j\) is through the following associative memory:

\[W=\sum_{i,j}\alpha_{ij}v_{j}u_{i}^{\top},\] (3)

so that the scores \(v_{j}^{\top}Wu_{i}\approx\alpha_{ij}\) may be used to assess the relevance of the \((i,j)\) pair, _e.g._, as part of a softmax operation in attention or next token prediction.

Random embeddings.A simple way to ensure that embeddings \((u_{i})_{i}\) and \((v_{j})_{j}\) are nearly-orthonormal is to set them to be random high-dimensional vectors, such as Gaussian vectors with variance \(1/d\) in \(d\) dimensions. Indeed, these are known to satisfy [23; 53]

\[u_{i}^{\top}u_{i}\approx 1\quad\text{ and }\quad u_{i}^{\top}u_{j}\approx O \left(\frac{1}{\sqrt{d}}\right),\]

so that (3) is a reasonable way to define an associative memory, without requiring an explicit activation function as employed in end-to-end memory networks [50]. We may also easily create a "remapping" of an existing embedding \(u_{i}\) by multiplying it by a random matrix \(W_{0}\in\mathbb{R}^{d\times d}\) with Gaussian entries of variance \(1/d\), which is commonly used for initializing neural network parameters. The new **remapped embedding**\(W_{0}u_{i}\) is near-unit norm, and is near-orthogonal to \(u_{i}\) in addition to the other \(u_{j}\). Note that this fact implies that attention scores at initialization are near-uniform. See Appendix A for more details.

Learning associative memories.We now show that learning associations of input-output embeddings via gradient descent leads to a weighted associative memory of a form similar to (3).

**Lemma 1** (Gradients and associative memories).: _Let \(p\) be a data distribution over input-output tokens, and consider the following loss, where the input and output embeddings \(W_{E}\) and \(W_{U}\) are fixed:_

\[L(W)=\mathbb{E}_{(z,y)\sim p}[\ell(y,W_{U}Ww_{E}(z))],\] (4)

_with \(\ell\) the cross-entropy loss. The gradients of the population loss \(L\) then take the form_

\[\nabla_{W}L(W)=\sum_{k=1}^{N}\mathbb{E}_{z}[(\hat{p}_{W}(y=k|z)-p(y=k|z))w_{U }(k)w_{E}(z)^{\top}],\] (5)

_where \(\hat{p}_{W}(y\!=\!k|x)=\sigma(W_{U}Ww_{E}(z))_{k}\) are the model's predicted probabilities. Running gradient descent (with or without weight decay) from initialization \(W_{0}\) then leads to estimates of the following form, for some \(\alpha_{0}\) and \(\alpha_{ij}\) that vary with the number of iterations:_

\[\hat{W}=\alpha_{0}W_{0}+\sum_{i,j}\alpha_{ij}w_{U}(j)w_{E}(i)^{\top}.\] (6)

Note that (4) is a convex problem in \(W\), thus with appropriate step-size and large enough number of steps (with no weight decay) we can expect gradient descent to be close to the global minimum. At the optimum, if the embeddings are nearly orthogonal, then (5) implies \(\hat{p}_{W}(y=k|z)\approx p(y=k|z)\). We remark that if \(W_{0}\) is a Gaussian random matrix, as if often the case for neural network layers, the first term in (6) plays a minor role: testing \(W_{0}\) against an input-output pair \((i,j)\) with \(\alpha_{ij}\neq 0\) will concentrate around zero when \(d\) is large, while the \((i,j)\) term in the sum will concentrate around \(\alpha_{ij}\). We also note that the gradient updates described above correspond to a so-called maximal feature learning regime similar to \(\mu\)P updates in intermediate layers of deep networks [58; 59].

Handling superposition.In Lemma 1, we assumed that inputs to the matrix \(W\) are embeddings of a single token. Yet, in transformer models, the inputs to weight matrices are often sums, or _superpositions_ of embeddings. For instance, the initial representations of each token are sums of token and positional embeddings, and representations at later layers are sums of the outputs of each previous block, due to residual connections. Outputs of attention layers are also weighted sums of potentially many embeddings, at least initially when attention patterns are spread out. By linearity, associative memories of the form (6) simply operate individually on each embedding of a superposition, and return a new superposition (up to additional noise due to near-orthogonality). In practice, we will see that learned memories often focus on a single embedding and filter out the rest as noise when irrelevant (see also Section 6). We note that linearity can also be limiting, since it makes it difficult to map sets to specific output embeddings: \(u_{\{i,j\}}:=u_{i}+u_{j}\) needs to map to \(Wu_{i}+Wu_{j}\), and thus cannot map to a new embedding \(v_{\{i,j\}}\). Such mappings of sets thus require non-linear associative memories, for instance by leveraging a sparse decoding of which elements are actually present (_e.g._, using compressed sensing), or by using MLPs with non-linear activations [15; 30].

### A simplified two-layer transformer architecture

We consider a simpler two-layer transformer which is more interpretable with the memory viewpoint, and will help us analyze learning dynamics both empirically and theoretically.

* We freeze input, output and positional embeddings (\(W_{E},W_{U},W_{P}\)) to their random initialization throughout training. This brings us to the Gaussian random vector setup presented above.
* We fix \(W_{Q}^{1}=W_{Q}^{2}=I_{d}\), so that \(W_{K}^{1}\) and \(W_{K}^{2}\) play the role of both key and query matrices. This changes the gradient dynamics, but simplifies the model by avoiding the redundancy in (2). The pre-softmax attention scores then take the form \(x_{q}^{\top}W_{K}^{\ell}x_{k}\), with \(x_{q}\) (resp. \(x_{k}\)) the query (resp. key) embeddings, which now directly resembles an associative memory lookup.
* We freeze \(W_{V}^{1}\), \(W_{Q}^{1}\), and \(W_{V}^{2}\) to random initialization. These play the role of _remapping_ attended tokens into new tokens, since for random \(W\) and large \(d\), \(Wx\) is nearly orthogonal to \(x\) and to any other random embeddings independent of \(x\).
* We train \(W_{Q}^{2}\), since the outputs of the induction head need to be mapped back into appropriate output embeddings in order to predict the output tokens \(o_{k}\) correctly.
* We use a single linear feedforward layer after the second attention block, with weight matrix \(W_{F}\).

This is plausibly the layer responsible for learning the global bigram distributions.

We remark that while this model freezes some parameters at initialization, it is richer than a "lazy" or neural tangent kernel approximation [10; 22; 24] since the model is still highly non-linear in its parameters and, as we will see, induces rich non-linear learning dynamics.

Solving the bigram problem with associative memories.We now show how the above architecture can solve the synthetic bigram problem from Section 3 with well-chosen weight matrices. While this is only a hypothetical model, we show in Section 5 that it is surprisingly faithful to the learned model.

Recall that due to residual connections, the inputs to the weight matrices typically consist of superpositions of various embeddings including token embeddings, positional embeddings, or "remapped" versions thereof. These may be viewed as sets, as illustrated in Figure 1, and associative memories can easily ignore certain elements of the set, _e.g._, ignore token embeddings by only focusing on positional embeddings. The induction head mechanism can be obtained by setting:

\[W_{K}^{1}=\sum_{t=2}^{T}p_{t}p_{t-1}^{\top},\quad W_{K}^{2}=\sum_{k\in Q}w_{E} (k)(W_{O}^{1}W_{V}^{1}w_{E}(k))^{\top},\quad W_{O}^{2}=\sum_{k=1}^{N}w_{U}(k)(W _{V}^{2}w_{E}(k))^{\top},\] (7)

where \(Q\) is the set of triggers when they are fixed, or the support of \(\pi_{q}\) when they are random. In words, the first attention layer matches a token to the previous tokens using positional embeddings. The second layer matches the trigger token to a remapping of itself by \(W_{O}^{1}W_{V}^{1}\), and the output matches a remapping of the input token by \(W_{V}^{2}\) to the corresponding output token. We remark that one can easily make the attention patterns more peaked on the correct associations by rescaling \(W_{K}^{1}\) and \(W_{K}^{2}\). The global bigram statistics can be encoded in the feed-forward layer as follows:

\[W_{F}=\sum_{i=1}^{N}\sum_{j=1}^{N}\log\pi_{b}(j|i)w_{U}(j)w_{E}(i)^{\top}.\] (8)The question remains of how the model could trade-off predictions from the induction head and from the feed-forward layer, which are added together due to residual connections. With fixed triggers \(Q\), we may simply remove all \(i\in Q\) from the summation in (8), so that the model exclusively relies on the attention head for all triggers (indeed, the output of \(W_{O}^{2}\) is in the span of output embeddings, which are nearly orthogonal to the row space of \(W_{F}\)). When the triggers can vary across different sequences, choosing between the induction head and the feed-forward layer is more ambiguous as it depends on context, and \(W_{F}\) may try to learn more complex mappings that also use the outputs of \(W_{O}^{2}\). In practice, we observe that the model often prefers the induction head, unless its output agrees with one of the top predictions from the global bigram, in which case it tends to prefer those.

Beyond the simplified architecture.While our simplified architecture already captures the relevant aspects for the bigram model, it lacks some of the components that appear in standard transformers, such as non-linear MLPs, trained embeddings, layer normalization, and joint learning of a factorization \(W_{K}^{\top}W_{Q}\) (potentially with low rank matrices \(W_{K},W_{Q}\in\mathbb{R}^{d_{h}\times d}\) with \(d_{h}<d\) as in multi-head attention), instead of a single matrix \(W_{K}\). In practice, transformers also involve many more layers, as well as multiple heads at each self-attention layer. In Appendix D, we discuss how our memory viewpoint naturally extends to such architectural components, and we illustrate in Appendix E that they empirically lead to similar observations. Nonetheless, we focus on our simpler architecture in the main paper due to simplicity of exposition and better interpretability thanks to a clear identifiability of the role of each matrix, which is lost in models with more heads and layers.

## 5 Empirical Study

In this section, we present our empirical analysis of learning dynamics on the bigram data defined in Section 3, for the simplified architecture defined in Section 4.2. See Appendix E for additional results. Our code is available at https://github.com/albietz/transformer-birth.

Experimental setup.We train our models using mini-batch SGD with momentum, where each batch consists of 512 fresh sequences of length \(T=256\) sampled from our synthetic model. We use a fixed learning rate and weight decay. Hyperparameters are given in Appendix E. Unless otherwise noted, we use \(d=128\), random triggers with \(\pi_{q}=\pi_{u}\) and uniform output tokens. The reported accuracies and losses are computed over each fresh batch before it is used for optimization, and are averaged over relevant tokens: "in-context accuracy/loss" numbers only consider predictions of output tokens on triggers starting at the second occurrence (the first is non-deterministic), while "global loss" refers to average loss on non-trigger tokens.

Memory recall probes.In addition to loss and accuracy, we consider metrics to check whether individual matrices have learned the desired associative memories: for a desired target memory \(W_{*}=\sum_{(i,j)\in\mathcal{M}}v_{j}u_{i}^{\top}\), the corresponding recall metric is computed from the empirical estimate \(\hat{W}\) as

\[R(\hat{W},W_{*})=\frac{1}{|\mathcal{M}|}\sum_{(i,j)\in\mathcal{M}}\mathbbm{1 }\{\arg\max_{j^{\prime}}v_{j^{\prime}}^{\top}\hat{W}u_{i}=j\}.\] (9)

We use this for each matrix in (7) as target, and additionally test the previous token matrix \(W_{K}^{1}\) on smaller time windows. For the final feed-forward layer, we measure the average KL divergence between the predicted softmax distribution using only \(W_{F}\) and the global bigram distribution \(\pi_{b}\):

\[d_{KL}(W_{F},\pi_{b}):=\frac{1}{N}\sum_{k=1}^{N}d_{KL}(\sigma(W_{U}W_{F}w_{E}( k)),\pi_{b}(\cdot|k)).\] (10)

Emergence of the induction head via top-down learning.We begin our study by only training to minimize the loss on _trigger-output_ token predictions after their first occurrence. This should be predictable with 100% accuracy using the two-layer induction head mechanism according to Section 4. We also remove the feed-forward layer, in order to focus on the learning of attention matrices \(W_{K}^{1}\), \(W_{K}^{2}\) and \(W_{O}^{2}\) in isolation.

Figure 3 studies the effect of freezing different layers until iteration 300 on the training dynamics. By looking at memory recall probes, we see that training key-query matrices does not lead to any

learning unless \(W_{O}^{2}\) is learned first, and that \(W_{O}^{2}\) can learn the correct associations even when trained by itself with key-value matrices at random initialization. Recall that the attention weights are essentially uniform when \(W_{K}\) are at random initialization, so that training \(W_{O}^{2}\) alone resembles a bag-of-words models that aggregates representations throughout the sequence. While such a model has poor prediction accuracy, it is nevertheless sufficient to recover the correct associations in \(W_{O}^{2}\) (a similar observation was made in [49] in a different setup).

Then, these associations enable learning key-query matrices that focus the attention on relevant tokens, by storing relevant key-query pairs in the form of associative memories, which eventually recovers the desired induction head behavior and leads to near-perfect accuracy. The two rightmost plots suggest that the second layer is learned before the first, in the sense that \(W_{K}^{2}\) is easier to learn when \(W_{K}^{1}\) is frozen compared to the reverse, yet learning them together seems beneficial, possibly due to helpful feedback loops [1]. We also observe that \(W_{K}^{1}\) fits previous token associations for early positions much faster than later positions (purple vs gray line). This is likely due to the fact that it should be enough for the previous token head to attend to the first appearance of each trigger \(q_{k}\), which is typically early in the sequence, so that most of the gradient will focus on early positions.

Overall, this provides a fine-grained understanding of the learning dynamics of induction heads. In Section 6, we analyze how a few gradient steps in a top-down fashion may suffice to recover appropriate associative memories in high dimension and with enough data. See Appendix E for additional experiments, including on the role of dimensionality.

Global vs in-context learning.Figure 4(left/right) shows that when training all layers jointly, the global bigram statistics tend to be learned more quickly than the induction head, as seen from the quick drop in loss and KL in early iterations. The \(W_{O}^{2}\) probe also seems to improve quickly initially, but only leads to mild improvements to in-context predictions. The full learning of the in-context mechanism takes longer, likely due to slower dynamics of the key-query matrices. We also observe a tension between \(W_{O}^{2}\) and \(W_{F}\) later in training, leading to slight degradations of our probe metrics. This may be due to the fact that the input to \(W_{F}\) now contains additional signal from the induction head which may be leveraged for better predictions, in particular for disambiguation in the case of random triggers, so that our guess of memories in Section 4.2 may no longer be accurate.

Role of the data distribution.We can see in Figure 4(left) that changes to the data distribution can have a significant effect on the speed of learning the in-context mechanism. We observe that the following may slow down in-context learning: (i) a smaller number of triggers \(K\), (ii) using only rare fixed triggers, and (iii) using random triggers instead of fixed triggers. By inspecting the individual memory probes (see Figure 5 in Appendix E), we hypothesize that (i) and (ii) are due to slow learning of \(W_{O}^{2}\), while (iii) is more related to slow learning of key-query matrices. This is reasonable since (i-ii) reduce the number of overall output tokens in the data, while (iii) increases the number of possible trigger tokens that should be stored in \(W_{K}^{2}\), thus increasing the data requirements in order to learn the full associative memory. We also show in Figure 4(center) that changing the output token distribution to bigram distributions at training time reduces the in-context accuracy

Figure 3: **Learning the induction head alone: in-context accuracy (top) and recall probes (bottom)** with some layers frozen until iteration 300. The output matrix \(W_{O}^{2}\) can and must be learned before the key-query matrices, but does not suffice for good accuracy. It is easier to learn \(W_{K}^{2}\) before \(W_{K}^{1}\), and \(W_{K}^{1}\) stores initial context positions (\(t<64\)) much faster than late positions.

when using out-of-distribution output tokens, while training on uniform outputs performs well on both distributions. This highlights that using a more diverse training distribution can lead to models with better generalization accuracy, with little additional training cost.

Additional experiments.In Appendix E, we provide additional experimental results for varying dimensionality, more complex architectures and training methods, as well as more fine-grained visualizations of the memory associations.

## 6 Theoretical Insights on Learning Dynamics

In this section, we provide theoretical insights on how gradients near initialization may allow the emergence of induction heads, and how this behavior is affected by data-distributional properties.

Finding signal in noisy inputs.In Lemma 1, we showed how gradient dynamics on a simple classification task with fixed embeddings of the inputs and outputs lead to associative memories. We now show that when inputs consist of superpositions of multiple embeddings, as is the case in the transformer residual streams, gradients may learn associative memories that filter out irrelevant components of these superpositions, focusing on useful signal instead.

**Lemma 2** (Gradient associative memory with noisy inputs).: _Let \(p\) be a data distribution on \((x,y)\in\mathbb{R}^{d}\times[N]\), and consider the following classification problem, with fixed output embeddings \(W_{U}\):_

\[L(W)=\mathbb{E}_{(x,y)\sim p}[\ell(y,W_{U}Wx)].\]

_The gradients take the following form: denoting \(\mu_{k}:=\mathbb{E}[x|y=k]\) and \(\hat{\mu}_{k}:=\mathbb{E}_{x}[\frac{\hat{p}_{W}(k|x)}{p(y=k)}x]\),_

\[\nabla_{W}L(W)=\sum_{k=1}^{N}p(y=k)w_{U}(k)(\hat{\mu}_{k}-\mu_{k})^{\top}.\]

The key takeaway from this lemma is that with enough data (here infinite data), the associative memory arising from gradients can learn to filter out noise from inputs, since it only depends on its expectations or conditional expectations. In particular, \(\mu_{k}\) can isolate relevant parts of \(x\) that are predictive of a label \(k\), and thus can lead to the right associations.

An illustrative example.To gain more intuition about this result, consider the following example: we would like to predict \(y\) from \(x=w_{E}(y)+p_{t}\), where \(p_{t}\) is a positional embedding at a random position \(t\in[T]\), which we would like to ignore. Further assume that \(y\) is uniformly distributed with \(p(y=k)=1/N\), and consider the matrix obtained after one population gradient step with step-size \(\eta\) starting from an initialization \(W_{0}=0\) (so that \(\hat{p}_{W_{0}}(k|x)=1/N\)):

\[W_{1}=\frac{\eta}{N}\sum_{k=1}^{N}w_{U}(k)(\mu_{k}-\bar{\mu})^{\top},\]

Figure 4: **Global vs in-context learning and data-distributional effects. (left) Loss on global (dashed) vs in-context (solid) tokens throughout training, for fixed or random trigger tokens \(q_{k}\). The red curves fixes the trigger \(q_{1}\) to the most frequent token, while the fixed triggers in blue curves are less common. (center) In-context accuracy with different training and test distributions \(\pi_{o}\) for output tokens. Uniform leads to better generalization than global bigrams \(\pi_{b}\). (right) Probe metrics throughout training: \(W_{O}^{2}\) and \(W_{F}\) eventually compete and deviate from our natural estimates.**

with \(\bar{\mu}=\mathbb{E}[x]\). We show in Appendix B that when \(d\) is large enough to ensure near-orthonormal embeddings, we have

\[w_{U}(k)^{\top}W_{1}(w_{E}(y)+p_{t})\approx\frac{\eta}{N}\;\mathbbm{1}\{k=y\}+O \left(\frac{1}{N^{2}}\right),\]

so that for large enough \(N\) and \(T\), we obtain a near-perfect classifier that ignores the positional embedding, after just one gradient step (but a highly idealized one). Understanding how this translates to the finite dimension and finite sample regime is an important theoretical question that we leave for future work (see [7] for an initial step in that direction). We note that data models related to the above have been useful to study gradient dynamics of neural networks on continuous data [2, 25, 28]. Using a single gradient step to learn representations has also been fruitful in other contexts [3, 12].

Learning the induction head with gradients.We may extend the arguments above to show how a few gradient steps can learn the induction head mechanism. We show the following in Appendix B.3.

**Theorem 3** (Learning induction head via three gradient steps, informal).: _In a simplified setup, the induction head mechanism as constructed in (7) can be learned via sequential gradient steps on the population loss from random initialization, on \(W^{2}_{O}\), then \(W^{2}_{K}\), followed by \(W^{1}_{K}\)._

To show this result, we use Lemma 2 in a similar manner to the illustrative example above to show how training \(W^{2}_{O}\) by itself at initialization, _i.e._, when the attention patterns are near-uniform, can recover the desired associative memory. This is possible because when predicting an output token at later occurrences of a trigger, the same output token is guaranteed to be present in the context, while other tokens need not appear more relative to other sequences. See also Figure 9 in Appendix E for numerical experiments verifying this for finite data and dimension. Once \(W^{2}_{O}\) has learned the correct associations, we show that the gradient with respect to the key-value matrix \(W^{2}_{K}\) at zero initialization can leverage the correctness of \(W^{2}_{O}\) to find the right associative memory that focuses the attention on correct triggers. Finally, by linearizing the second-layer attention around \(W^{2}_{K}=0\), we show how gradients w.r.t. \(W^{1}_{K}\) may learn correct associations for the previous token head.

## 7 Discussion

In this paper, we studied the question of how transformers develop in-context learning abilities, using a simplified setup that allows a fine-grained understanding the model and its training dynamics. While our model already captures rich phenomena at play in the bigram task we consider, more elaborate models are likely needed to understand transformers trained on more complex tasks like language modeling. This includes learning embeddings that are more adapted to the data and more structured (_e.g._, word embeddings [37, 31], or grokking [33, 39]), factorized key-query and value-output matrices that may induce additional regularization effects [20], and non-linear feedforward layers, which may provide richer associative memories between sets of embeddings. Understanding how transformers leverage such aspects to learn in richer settings is an important next step.

## Acknowledgments and Disclosure of Funding

The authors thank Sainbayar Sukhbaatar and Shubham Toshniwal for helpful discussions.

## References

* [1] Z. Allen-Zhu and Y. Li. Backward feature correction: How deep learning performs deep learning. _arXiv preprint arXiv:2001.04413_, 2020.
* [2] Z. Allen-Zhu and Y. Li. Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2023.
* [3] J. Ba, M. A. Erdogdu, T. Suzuki, Z. Wang, D. Wu, and G. Yang. High-dimensional asymptotics of feature learning: How one gradient step improves the representation. _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.

* [4] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. _arXiv preprint arXiv:1607.06450_, 2016.
* [5] T. Bricken and C. Pehlevan. Attention approximates sparse distributed memory. _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [7] V. Cabannes, E. Dohmatob, and A. Bietti. Scaling laws for associative memories. _arXiv preprint arXiv:2310.02984_, 2023.
* [8] S. Chan, A. Santoro, A. Lampinen, J. Wang, A. Singh, P. Richemond, J. McClelland, and F. Hill. Data distributional properties drive emergent in-context learning in transformers. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [9] F. Charton. What is my math transformer doing?-three results on interpretability and generalization. _arXiv preprint arXiv:2211.00170_, 2022.
* [10] L. Chizat, E. Oyallon, and F. Bach. On lazy training in differentiable programming. _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* [11] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.
* [12] A. Damian, J. Lee, and M. Soltanolkotabi. Neural networks can learn representations with gradient descent. In _Conference on Learning Theory (COLT)_, 2022.
* [13] G. Dar, M. Geva, A. Gupta, and J. Berant. Analyzing transformers in embedding space. _arXiv preprint arXiv:2209.02535_, 2022.
* [14] B. L. Edelman, S. Goel, S. Kakade, and C. Zhang. Inductive biases and variable creation in self-attention mechanisms. In _Proceedings of the International Conference on Machine Learning (ICML)_, 2022.
* [15] N. Elhage, T. Hume, C. Olsson, N. Schiefer, T. Henighan, S. Kravec, Z. Hatfield-Dodds, R. Lasenby, D. Drain, C. Chen, R. Grosse, S. McCandlish, J. Kaplan, D. Amodei, M. Wattenberg, and C. Olah. Toy models of superposition. _Transformer Circuits Thread_, 2022.
* [16] N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, N. DasSarma, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. A mathematical framework for transformer circuits. _Transformer Circuits Thread_, 2021.
* [17] M. Geva, J. Bastings, K. Filippova, and A. Globerson. Dissecting recall of factual associations in auto-regressive language models. _arXiv preprint arXiv:2304.14767_, 2023.
* [18] M. Geva, R. Schuster, J. Berant, and O. Levy. Transformer feed-forward layers are key-value memories. In _Conference on Empirical Methods in Natural Language Processing (EMNLP)_, 2021.
* [19] A. Graves, G. Wayne, and I. Danihelka. Neural turing machines. _arXiv preprint arXiv:1410.5401_, 2014.
* [20] S. Gunasekar, B. E. Woodworth, S. Bhojanapalli, B. Neyshabur, and N. Srebro. Implicit regularization in matrix factorization. _Advances in Neural Information Processing Systems (NIPS)_, 2017.
* [21] J. J. Hopfield. Neural networks and physical systems with emergent collective computational abilities. _Proceedings of the national academy of sciences_, 79(8):2554-2558, 1982.

* [22] J. Hron, Y. Bahri, J. Sohl-Dickstein, and R. Novak. Infinite attention: Nngp and ntk for deep attention networks. In _Proceedings of the International Conference on Machine Learning (ICML)_, 2020.
* [23] A. Iscen, T. Furon, V. Gripon, M. Rabbat, and H. Jegou. Memory vectors for similarity search in high-dimensional spaces. _IEEE transactions on big data_, 4(1):65-77, 2017.
* [24] A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural networks. _Advances in Neural Information Processing Systems (NeurIPS)_, 2018.
* [25] S. Jelassi, M. Sander, and Y. Li. Vision transformers provably learn spatial structure. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [26] Y. Jiang and C. Pehlevan. Associative memory in iterated overparameterized sigmoid autoencoders. In _Proceedings of the International Conference on Machine Learning (ICML)_, 2020.
* [27] A. Joulin and T. Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. _Advances in Neural Information Processing Systems (NIPS)_, 2015.
* [28] S. Karp, E. Winston, Y. Li, and A. Singh. Local signal adaptivity: Provable feature learning in neural networks beyond kernels. _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [29] T. Kohonen. Correlation matrix memories. _IEEE Transactions on Computers_, 1972.
* [30] D. Krotov and J. J. Hopfield. Dense associative memory for pattern recognition. _Advances in Neural Information Processing Systems (NIPS)_, 2016.
* [31] Y. Li, Y. Li, and A. Risteski. How do transformers learn topic structure: Towards a mechanistic understanding. In _Proceedings of the International Conference on Machine Learning (ICML)_, 2023.
* [32] B. Liu, J. T. Ash, S. Goel, A. Krishnamurthy, and C. Zhang. Transformers learn shortcuts to automata. In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2023.
* [33] Z. Liu, O. Kitouni, N. S. Nolte, E. Michaud, M. Tegmark, and M. Williams. Towards understanding grokking: An effective theory of representation learning. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [34] R. McEliece, E. Posner, E. Rodemich, and S. Venkatesh. The capacity of the hopfield associative memory. _IEEE transactions on Information Theory_, 33(4):461-482, 1987.
* [35] K. Meng, D. Bau, A. Andonian, and Y. Belinkov. Locating and editing factual associations in gpt. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [36] W. Merrill, A. Sabharwal, and N. A. Smith. Saturated transformers are constant-depth threshold circuits. _Transactions of the Association for Computational Linguistics_, 10:843-856, 2022.
* [37] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2013.
* [38] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In _Conference on Empirical Methods in Natural Language Processing (EMNLP)_, 2022.
* [39] N. Nanda, L. Chan, T. Liberum, J. Smith, and J. Steinhardt. Progress measures for grokking via mechanistic interpretability. In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2023.

* [40] C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. In-context learning and induction heads. _Transformer Circuits Thread_, 2022.
* [41] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. _Technical report, OpenAI_, 2019.
* [42] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, et al. Scaling language models: Methods, analysis & insights from training gopher. _arXiv preprint arXiv:2112.11446_, 2021.
* [43] Y. Razeghi, R. L. Logan IV, M. Gardner, and S. Singh. Impact of pretraining term frequencies on few-shot reasoning. In _Conference on Empirical Methods in Natural Language Processing (EMNLP)_, 2022.
* [44] C. Sanford, D. Hsu, and M. Telgarsky. Representational strengths and limitations of transformers. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2023.
* [45] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. _arXiv preprint arXiv:1312.6120_, 2013.
* [46] I. Schlag, K. Irie, and J. Schmidhuber. Linear transformers are secretly fast weight programmers. In _Proceedings of the International Conference on Machine Learning (ICML)_, 2021.
* [47] J. Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. _Neural Computation_, 4(1):131-139, 1992.
* [48] S. Shin, S.-W. Lee, H. Ahn, S. Kim, H. Kim, B. Kim, K. Cho, G. Lee, W. Park, J.-W. Ha, et al. On the effect of pretraining corpora on in-context learning by a large-scale language model. In _North American Chapter of the Association for Computational Linguistics (NAACL)_, 2022.
* [49] C. Snell, R. Zhong, D. Klein, and J. Steinhardt. Approximating how single head attention learns. _arXiv preprint arXiv:2103.07601_, 2021.
* [50] S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus. End-to-end memory networks. _Advances in Neural Information Processing Systems (NIPS)_, 2015.
* [51] Y. Tian, Y. Wang, B. Chen, and S. Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2023.
* [52] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In _Advances in Neural Information Processing Systems (NIPS)_, 2017.
* [53] R. Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* [54] K. Wang, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt. Interpretability in the wild: a circuit for indirect object identification in gpt-2 small. In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2023.
* [55] J. Weston, S. Chopra, and A. Bordes. Memory networks. In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2015.
* [56] D. J. Willshaw, O. P. Buneman, and H. C. Longuet-Higgins. Non-holographic associative memory. _Nature_, 222(5197):960-962, 1969.
* [57] S. M. Xie, A. Raghunathan, P. Liang, and T. Ma. An explanation of in-context learning as implicit bayesian inference. In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2022.

* [58] G. Yang and E. J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In _Proceedings of the International Conference on Machine Learning (ICML)_, 2021.
* [59] G. Yang, E. J. Hu, I. Babuschkin, S. Sidor, X. Liu, D. Farhi, N. Ryder, J. Pachocki, W. Chen, and J. Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [60] B. Zhang and R. Sennrich. Root mean square layer normalization. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* [61] Y. Zhang, A. Backurs, S. Bubeck, R. Eldan, S. Gunasekar, and T. Wagner. Unveiling transformers with lego: a synthetic reasoning task. _arXiv preprint arXiv:2206.04301_, 2022.

[MISSING_PAGE_EMPTY:15]

that \(\hat{p}_{W_{0}}(k|x)=1/N\)):

\[W_{1}=\frac{\eta}{N}\sum_{k=1}^{N}w_{U}(k)(\mu_{k}-\bar{\mu})^{\top},\] (11)

with \(\bar{\mu}=\mathbb{E}[x]\).

Note that we have \(\mu_{k}=w_{E}(k)+\frac{1}{T}\sum_{t}p_{t}\) and \(\bar{\mu}=\frac{1}{N}\sum_{k}w_{E}(k)+\frac{1}{T}\sum_{t}p_{t}\), so that (11) becomes

\[W_{1}=\frac{\eta}{N}\sum_{k=1}^{N}w_{U}(k)(w_{E}(k)-\bar{w}_{E})^{\top},\] (12)

with \(\bar{w}_{E}:=\frac{1}{N}\sum_{k=1}^{N}w_{E}(k)\). When \(d\) is large enough to ensure near-orthonormal embeddings, we have for any \(y\) and \(t\),

\[W_{1}(w_{E}(y)+p_{t})\approx\frac{\eta}{N}w_{U}(y)+O\left(\frac{1}{N^{2}} \right).\]

This implies

\[w_{U}(k)^{\top}W_{1}(w_{E}(y)+p_{t})\approx\frac{\eta}{N}\,\mathbbm{1}\{k=y \}+O\left(\frac{1}{N^{2}}\right),\]

as claimed in the main text. The classifier \(\hat{y}=\arg\max_{k}w_{U}(k)^{\top}W_{1}(w_{E}(y)+p_{t})\) then has essentially perfect accuracy, and has learned to ignore the spurious positional embeddings, which are simply exogenous noise.

### Gradients on key-query matrices at initialization

We now derive expressions for population gradients of the attention key-query matrices at zero initialization, noting that random initialization behaves similarly to zero initialization. Although the optimization problems involving these matrices are non-convex, these gradients at initialization lead to associative memories, similar to Lemma 2. When output matrices of the previous layer already encode the desired associations, these gradients can lead to associative memories that focus the attention on the correct key-value pairs.

We begin with the following lemma, which gives the gradient of the loss w.r.t. \(W=W_{K}^{2}\) at zero initialization. For simplicity, we drop the \(d^{-1/2}\) factor from the softmax, which only changes gradients by a multiplicative factor, and thus does not change its form.

**Lemma 4** (Gradient of second attention layer).: _Consider the following loss for predicting the next token \(y\) from an attention layer with inputs \(X=[x_{1},\ldots,x_{T}]\), and value-output matrix \(\Phi_{2}:=W_{O}^{2}W_{V}^{2}\):_

\[L(W)=\mathbb{E}_{(X,y)}[\ell(y,\xi(X))],\qquad\xi(X)=W_{U}\Phi_{2}X\sigma(X^{ \top}Wx_{T}),\] (13)

_with \(\ell\) the cross-entropy loss and \(\sigma(u)_{t}=\frac{e^{u_{t}}}{\sum_{s}e^{u_{s}}}\) for \(u\in\mathbb{R}^{T}\) is the softmax._

_The gradient at \(W=0\) is given by_

\[\nabla_{W}L(W)\big{|}_{W=0} =\sum_{k=1}^{N}\mathbb{E}_{(X,y)}\left[(\hat{p}_{W}(k|X)-\mathbbm {1}\{y\!=\!k\})\frac{1}{T}\sum_{t=1}^{T}w_{U}(k)^{\top}\Phi_{2}x_{t}\cdot(x_{ t}-\bar{x}_{1:T})x_{T}^{\top}\right]\] \[=\frac{1}{T}\sum_{k=1}^{N}\sum_{t=1}^{T}\mathbb{E}_{X}[\hat{p}_{W }(k|X)w_{U}(k)^{\top}\Phi_{2}x_{t}\cdot(x_{t}-\bar{x}_{1:T})x_{T}^{\top}]\] \[\quad-\frac{1}{T}\sum_{k=1}^{N}\sum_{t=1}^{T}p(y=k)\,\mathbb{E}_{ X}[w_{U}(k)^{\top}\Phi_{2}x_{t}\cdot(x_{t}-\bar{x}_{1:T})x_{T}^{\top}\mid y=k]\]

_with \(\bar{x}_{1:T}=\frac{1}{T}\sum_{t=1}^{T}x_{t}\)._

Now we consider the gradient w.r.t. \(W=W_{K}^{1}\) at zero initialization, and consier a simplification of the second layer attention to its linearization around \(W_{K}^{2}=0\). We will see that this still provides first-order information that is sufficient for \(W_{K}^{1}\) to be learned.

**Lemma 5** (Gradient of first attention layer).: _Consider the following loss for predicting the next token \(y\) from a stack of two attention layers, with all parameters fixed except for \(W=W_{K}^{1}\), the key-query matrix at the first attention layer:_

\[L(W)=\mathbb{E}_{(X,y)}[\ell(y,\xi(X))],\qquad\xi(X)=W_{U}\Phi_{2}X\bar{\sigma} (Z(W)^{\top}W_{2}x_{T}).\] (14)

_Here, \(\bar{\sigma}(u_{1:T})_{t}=\frac{1}{T}(1+u_{t}-\frac{1}{T}\sum_{s=1}^{T}u_{s})\) is the linearization of the softmax around \(0\), and \(Z(W)=[z_{1}(W),\dots,z_{T}(W)]\) with_

\[z_{t}(W)=\sum_{s=1}^{t}\Phi_{1}x_{s}\sigma(p_{1:t}^{\top}Wp_{t})_{s},\]

_and \(\Phi_{\ell}=W_{O}^{\ell}W_{V}^{\ell}\) for \(\ell=1,2\)._

_The gradient at \(W=0\) is given by_

\[\nabla_{W}L(W)\big{|}_{W=0}\] \[\quad=\sum_{k=1}^{N}\mathbb{E}_{X}\left[\hat{p}_{W}(k|X)\frac{1} {T}\sum_{t=1}^{T}w_{U}(k)^{\top}\Phi_{2}x_{t}\cdot\frac{1}{t}\sum_{s=1}^{t}( \Phi_{1}x_{s})^{\top}W_{2}x_{T}(p_{s}-\bar{p}_{1:t})p_{t}^{\top}\right]\] \[\quad\quad-\sum_{k=1}^{N}p(y=k)\,\mathbb{E}_{X}\left[\frac{1}{T} \sum_{t=1}^{T}w_{U}(k)^{\top}\Phi_{2}x_{t}\cdot\frac{1}{t}\sum_{s=1}^{t}(\Phi_ {1}x_{s})^{\top}W_{2}x_{T}(p_{s}-\bar{p}_{1:t})p_{t}^{\top}|y=k\right]\] \[\quad\quad-\sum_{k=1}^{N}\mathbb{E}_{X}\left[\hat{p}_{W}(k|X)w_{U }(k)^{\top}\Phi_{2}\bar{x}_{1:T}\cdot\frac{1}{T}\sum_{t=1}^{T}\frac{1}{t}\sum_ {s=1}^{t}(\Phi_{1}x_{s})^{\top}W_{2}x_{T}(p_{s}-\bar{p}_{1:t})p_{t}^{\top}\right]\] \[\quad\quad+\sum_{k=1}^{N}p(y=k)\,\mathbb{E}_{X}\left[w_{U}(k)^{ \top}\Phi_{2}\bar{x}_{1:T}\cdot\frac{1}{T}\sum_{t=1}^{T}\frac{1}{t}\sum_{s=1}^ {t}(\Phi_{1}x_{s})^{\top}W_{2}x_{T}(p_{s}-\bar{p}_{1:t})p_{t}^{\top}|y=k\right]\]

### Learning the induction head mechanism

In this section, we analyze the training dynamics of the induction head mechanism, in the following simplified setup: we consider a single trigger (\(K=1\)), and assume that \(\pi_{u}\), \(\pi_{q}\), \(\pi_{o}\) and \(\pi_{b}(\cdot|i)\) are uniform over \([N]\) for any \(i\).

To further simplify the analysis, we consider a loss that only considers sequences of length \(T\) where the last input \(z_{T}\) is the _second occurrence_ of the trigger token, and the label \(y=z_{T+1}\) is the corresponding output token. We note that this may be easily extended to later occurrences of the trigger. This is similar to the setup of Figure 1, where the loss is only taken on triggers after the second occurrence: in that case, the loss may be written as a weighted sum of the one we consider here, weighted by the probability of the second (or later) trigger appearing at the given position \(T\).

In practice, when the loss is on all tokens and \(W_{F}\) is also learned, we may expect that \(W_{F}\) quickly learns the global bigram statistics, as we saw empirically in Section 5. Indeed, the current token embedding, which is included in the input superposition, has strong predictive signal compared to the attention layers, which initially mainly appear as noise. This is then similar to the setup of Lemma 1, which provides recovery of bigram statistics when \(d\) is large (though we note that the other information from attention layers in the inputs may eventually be used and bias away from perfect recovery, see Figure 4(right)). Once such global estimates are obtained, the expected loss will be mainly dominated by trigger tokens, leading to the setup above.

For simplicity, we thus drop the feed-forward layer \(W_{F}\) in the remainder of this section, focusing on the learning of \(W_{O}^{2}\), \(W_{K}^{2}\) and \(W_{K}^{1}\), in this top-down order. We will consider zero-initialization a single gradient steps, noting that random initialization should lead to similar associative memory behaviors when the dimension is large enough, since it leads to a remapping of input embeddings which is near-orthogonal to any output embedding (see Appendix A).

#### b.3.1 Learning \(W_{o}^{2}\)

We begin by studying the learning of the second output matrix \(W_{O}^{2}\). In the above data model, we may consider a loss as in Lemma 2 with input-outputs \((x,y)\), where \(y\) is the output token of the sequence,and \(x\) depends on the random sequence \(z_{1:T}\) as

\[x=\frac{1}{T}\sum_{t=1}^{T}W_{V}^{2}(w_{E}(z_{t})+\varepsilon_{t}),\]

where \(\varepsilon_{t}=p_{t}+\frac{1}{t}\sum_{s=1}^{t}\Phi_{1}(w_{E}(z_{s})+p_{s})\) with \(\Phi_{1}=W_{O}^{1}W_{V}^{1}\), is a "noise" vector from the residual streams, containing positional embeddings as well as an average attention output from the first layer. In practice, the logit predictions are of the form \(W_{U}(W_{O}^{2}x+\varepsilon_{T})\) due to residual connections, but we ignore the term \(W_{U}\varepsilon_{T}\) for simplicity, noting that it is near-zero when \(d\) is large.

After a gradient step on \(W_{O}^{2}\) with step-size \(\eta\), starting from zero-initialization (so that \(\hat{p}(k|x)=p(y=k)=1/N\) for all \(x\)), Lemma 2 yields

\[W_{O}^{2}=\frac{\eta}{N}\sum_{k=1}^{N}w_{U}(k)(\mathbb{E}[x|y=k]-\mathbb{E}[x ])^{\top}.\] (15)

Now, consider the random variables \(q\) (trigger token), \(o\) (output token), \(t_{o}\) (position of the first occurrence of the output token). In our simplified data model, \(q\) and \(t_{o}\) have the same distribution regardless of the conditioning on \(y=k\), while \(o\) is equal to \(k\) when \(y=k\), while it is uniform in \([N]\) without this condition. The sequence \(z_{1:T}\) has the same distribution in either \(p(\cdot)\) or \(p(\cdot|y=k)\), except for the token \(z_{t_{o}}\).

Then, we may write:

\[\mathbb{E}[x|y=k]-\mathbb{E}[x] =\frac{1}{T}\left(\mathbb{E}[W_{V}^{2}w_{E}(z_{t_{o}})|y=k]- \mathbb{E}[W_{V}^{2}w_{E}(z_{t_{o}})]\right)\] \[\quad+\frac{1}{T}\left(\mathbb{E}\left[\sum_{t=t_{o}}^{T}W_{V}^{2 }\varepsilon_{t}|y=k\right]-\mathbb{E}\left[\sum_{t=t_{o}}^{T}W_{V}^{2} \varepsilon_{t}\right]\right),\]

since \(\varepsilon_{t}\) is independent of \(o\) when \(t<t_{o}\). Noting that \(\varepsilon_{t}\) only depends on \(z_{t_{o}}\) and thus on \(y\) through the first layer attention, we have

\[\mathbb{E}[x|y=k]-\mathbb{E}[x] =\frac{1}{T}W_{V}^{2}(w_{E}(k)-\bar{w}_{E})\] \[\quad+\frac{1}{T}\left(\mathbb{E}\left[\sum_{t=t_{o}}^{T}\frac{1} {t}W_{V}^{2}\Phi_{1}w_{E}(z_{t_{o}})|y=k\right]-\mathbb{E}\left[\sum_{t=t_{o}} ^{T}\frac{1}{t}W_{V}^{2}\Phi_{1}w_{E}(z_{t_{o}})\right]\right)\] \[=\frac{1}{T}W_{V}^{2}(w_{E}(k)-\bar{w}_{E})+\frac{\tau}{T}W_{V}^{ 2}\Phi_{1}(w_{E}(k)-\bar{w}_{E}),\]

where \(\tau:=\mathbb{E}\left[\sum_{t=t_{o}}^{T}\frac{1}{t}\right]\), and \(\bar{w}_{E}=\frac{1}{N}\sum_{k=1}^{N}w_{E}(k)\). Thus, (15) becomes

\[W_{O}^{2}=\frac{\eta}{NT}\sum_{k=1}^{N}w_{U}(k)(W_{V}^{2}(w_{E}(k)-\bar{w}_{E} ))^{\top}+\frac{\eta\tau}{NT}\sum_{k=1}^{N}w_{U}(k)(W_{V}^{2}\Phi_{1}(w_{E}(k) -\bar{w}_{E}))^{\top},\] (16)

so that when \(d\) is learn enough to ensure near-orthonormal embeddings, we have

\[w_{U}(k)^{\top}W_{O}^{2}W_{V}^{2}w_{E}(j) \approx\frac{\eta}{NT}\,\mathbbm{1}\{k=j\}+O\left(\frac{\eta}{N^{ 2}T}\right)\] \[w_{U}(k)^{\top}W_{O}^{2}W_{V}^{2}\Phi_{1}w_{E}(j) \approx\frac{\eta\tau}{NT}\,\mathbbm{1}\{k=j\}+O\left(\frac{\eta \tau}{N^{2}T}\right),\]

where the \(O(\cdot)\) terms are due to the \(\bar{w}_{E}\) elements. The first line yields a behavior that matches desired associative memory in (7) of Section 4.2 when \(N\) is large. The second line shows additional spurious associations that are stored in \(W_{O}^{2}\) due to the output of the first layer attention, but which may be "cleaned up" once the attention layers start focusing on the correct tokens.

Finally, we note that despite the recovery of these useful associations after one gradient step, the predictions with this estimate \(W_{O}^{2}\) are still near-random, since in the bag-of-words setup with average attention, the output token cannot be distinguished from any other token in the sequence in our model (except perhaps the trigger token, which is guaranteed to appear twice, but does not provide any signal to infer the output token, since the two are independent).

#### b.3.2 Learning \(W_{k}^{2}\)

Now assume that \(W_{O}^{2}\) is as in (16). As argued above, the predictions \(\hat{p}(k|x)\) are essentially random \(1/N\) in our model for \(W_{K}^{2}=0\), so that after one gradient step on \(W_{K}^{2}\) with learning rate \(\eta\), Lemma 4 yields:

\[W_{K}^{2}=\frac{\eta}{TN}\sum_{k,t}\left(\mathbb{E}[w_{U}(k)^{\top}\Phi_{2}x_{ t}\cdot(x_{t}-\bar{x})x_{T}^{\top}\mid y=k]-\mathbb{E}[w_{U}(k)^{\top}\Phi_{2}x_{ t}\cdot(x_{t}-\bar{x})x_{T}^{\top}]\right),\] (17)

where \(x_{t}\) are the inputs to the second attention layer, given by

\[x_{t} =x_{t,0}+x_{t,1}\] (18) \[x_{t,0} =w_{E}(z_{t})+p_{t}\] (19) \[x_{t,1} =\frac{1}{t}\sum_{s=1}^{t}\Phi_{1}(w_{E}(z_{s})+p_{s}).\] (20)

From now on, we consider a simplified architecture where only \(x_{t,0}\) are fed as queries and values, while only \(x_{t,1}\) are fed as keys. Using the fact that trigger tokens \(q\) are sampled uniformly (_i.e._, \(\pi_{q}=1/N\)), we have

\[W_{K}^{2} =\frac{\eta}{TN}\sum_{k=1}^{N}\sum_{t=1}^{T}\left(\mathbb{E}[A_{t,k}\mid y=k]-\mathbb{E}_{X}[A_{t,k}]\right)\] (21) \[=\frac{\eta}{TN^{2}}\sum_{k=1}^{N}\sum_{t=1}^{T}\sum_{j=1}^{N} \left(\mathbb{E}[A_{t,k}\mid y=k,q=j]-\mathbb{E}[A_{t,k}\mid q=j]\right)\] (22)

where

\[A_{t,k}=w_{U}(k)^{\top}\Phi_{2}x_{t,0}\cdot(x_{t,1}-\bar{x}_{1})x_{T,0}^{\top},\] (23)

with \(\bar{x}_{1}=\frac{1}{T}\sum_{t}x_{t,1}\). Now, note that we have \(w_{U}(k)^{\top}\Phi_{2}x_{t,0}\approx\alpha\,\mathbbm{1}\{z_{t}=k\}\) with \(\alpha=\eta/TN\) by (16), and \(x_{T,0}=w_{E}(q)+p_{T}\). This yields

\[W_{K}^{2}\approx\frac{\alpha\eta}{TN^{2}}\sum_{j=1}^{N}\sum_{k=1}^{N}\Delta_{ k,j}(w_{E}(j)+p_{T})^{\top},\] (24)

with

\[\Delta_{k,j} :=\mathbb{E}\left[\sum_{t=1}^{T}\mathbbm{1}\{z_{t}=k\}(x_{t,1}- \bar{x}_{1})|y=k,q=j\right]-\mathbb{E}\left[\sum_{t=1}^{T}\mathbbm{1}\{z_{t}= k\}(x_{t,1}-\bar{x}_{1})|q=j\right]\] \[=\Delta_{k,j}^{o}+\Delta_{k,j}^{q}+\Delta_{k,j}^{r},\]

where the three terms split the sum inside the expectation

\[\Delta_{k,j}^{o} :=\mathbb{E}\left[\mathbbm{1}\{z_{t_{o}}=k\}(x_{t_{o},1}-\bar{x}_ {1})|y=k,q=j\right]-\mathbb{E}\left[\mathbbm{1}\{z_{t_{o}}=k\}(x_{t_{o},1}- \bar{x}_{1})|q=j\right]\] \[\Delta_{k,j}^{q} :=\mathbb{E}\left[\sum_{t\in\mathcal{T}_{q}}\mathbbm{1}\{z_{t}\!=\! k\}(x_{t,1}-\bar{x}_{1})|y\!=\!k,q\!=\!j\right]-\mathbb{E}\left[\sum_{t\in \mathcal{T}_{q}}\mathbbm{1}\{z_{t}\!=\!k\}(x_{t,1}-\bar{x}_{1})|q\!=\!j\right]\] \[\Delta_{k,j}^{r} :=\mathbb{E}\left[\sum_{t\in\mathcal{T}_{r}}\mathbbm{1}\{z_{t}\!= \!k\}(x_{t,1}-\bar{x}_{1})|y\!=\!k,q\!=\!j\right]-\mathbb{E}\left[\sum_{t\in \mathcal{T}_{r}}\mathbbm{1}\{z_{t}\!=\!k\}(x_{t,1}-\bar{x}_{1})|q\!=\!j\right],\]

where \(\mathcal{T}_{q}=\{t_{o}-1,T\}\) and \(\mathcal{T}_{r}=[T]\setminus\{t_{o},t_{o}-1,T\}\) (recall that \(t_{o}\) is a random variable, corresponding to the first occurrence of the output token, so that these sets are random).

We will now show that \(\Delta_{k,j}^{o}\) carries the desired signal for the appropriate induction head associative memory, while \(\Delta_{k,j}^{q}\) and \(\Delta_{k,j}^{r}\) are negligible, for \(N\) large enough.

**Controlling \(\Delta^{o}_{k,j}\).** For \(t=t_{o}\), noting that \(z_{t_{o}}=y\), we have

\[\Delta^{o}_{k,j} =\mathbb{E}\left[\mathbbm{1}\{y=k\}(x_{t_{o},1}-\bar{x}_{1})|y=k,q= j\right]-\mathbb{E}\left[\mathbbm{1}\{y=k\}(x_{t_{o},1}-\bar{x}_{1})|q=j\right]\] \[=\left(1-\frac{1}{N}\right)\mathbb{E}\left[x_{t_{o},1}-\bar{x}_{ 1}|y=k,q=j\right]\] \[=\frac{N-1}{N}\left(\bar{p}+\sum_{i=1}^{N}a_{k,j,i}\Phi_{1}w_{E}( i)\right),\]

with \(a_{k,j,i}\approx(\Phi_{1}w_{E}(i))^{\top}\mathbb{E}\left[x_{t_{o},1}-\bar{x}_{ 1}|y=k,q=j\right]\) thanks to near-orthonormality, and

\[\bar{p}=\mathbb{E}_{t_{o}}\left[\frac{1}{t_{o}}\sum_{s=1}^{t_{o}}p_{s}-\frac{ 1}{T}\sum_{t=1}^{T}\frac{1}{t}\sum_{s=1}^{t}p_{s}\right]\]

is a spurious positional mixture.

We then distinguish the following cases:

* If \(j\neq k\) and \(i=j\), since the trigger token \(j\) only appears at positions \(t_{o}-1\) and \(T\), we have \[a_{k,j,i}\approx\mathbb{E}_{t_{o}}\left[\frac{1}{t_{o}}-\frac{1}{T}\sum_{t=t_{ o}-1}^{T}\frac{1}{t}-\frac{1}{T^{2}}\right]=:\gamma_{T}.\] We may expect \(t_{o}\) to be concentrated around \(T/2\), in which case \(\gamma_{T}\gtrsim\frac{2}{T}-\frac{1}{T}-\frac{1}{T^{2}}\geq\frac{C}{T}>0\) for \(T\) larger than a small constant.
* If \(j=k=i\), the two occurrences of the trigger happen one after the other, so it must be that \(t_{o}=T\). Then \[a_{k,j,i}\approx\frac{2}{T}-\frac{1}{T(T-1)}-\frac{2}{T^{2}}=\frac{2}{T}+O \left(\frac{1}{T^{2}}\right),\] for \(T\) larger than a small constant.
* If \(i\neq j=k\), all tokens up to position \(t_{o}-2=T-2\) are i.i.d. uniform in \([N]\setminus\{j\}\), so that \[a_{k,j,i}\approx\frac{T-2}{T(N-1)}-\frac{1}{T}\left((T-2)\cdot\frac{1}{N-1}+ \frac{T-2}{(T-1)(N-1)}+\frac{T-2}{T(N-1)}\right)=O\left(\frac{1}{N}\right)\]
* If \(i\neq j\) and \(j\neq k\), all tokens except at positions \(t_{o}-1\), \(t_{o}\) and \(T\) (we have \(t_{o}<T\)) are uniform in \([N]\setminus\{j\}\). The triggers do not contribute anything to \(a_{k,j,i}\) since \(i\neq j\), and the output token may be also randomized by taking the average over \(k\in[N]\setminus\{j\}\). We thus obtain \[\frac{1}{N-1}\sum_{k\neq j}a_{k,j,i}\approx O\left(\frac{1}{N}\right).\] In summary, we obtain \[\frac{1}{N}\sum_{k=1}^{N}a_{k,j,i}\approx\begin{cases}O\left(\frac{1}{N} \right),&\text{ if }i\neq j\\ \Omega\left(\frac{1}{T}\right),&\text{ if }i=j.\end{cases}\] Thus, when \(N\) is large, while \(T\) is moderate, the above sum leads to more signal in the \(i=j\) terms compared to \(i\neq j\). In particular, this yields \[(\Phi_{1}w_{E}(i))^{\top}\left(\frac{1}{N}\sum_{k=1}^{N}\Delta^{o}_{k,j} \right)\approx\begin{cases}O\left(\frac{1}{N}\right),&\text{ if }i\neq j\\ \Omega\left(\frac{1}{T}\right),&\text{ if }i=j,\end{cases}\] so that this component in (24) acts precisely like the desired associative memory in (7). It remains to show that the other components are negligible compared to this. It then suffices to show: \[\frac{1}{N}\sum_{k=1}^{N}(\Delta^{q}_{k,j}+\Delta^{r}_{k,j})\approx o\left( \frac{1}{T}\right).\]Controlling \(\Delta^{q}_{k,j}\).For \(t\in\mathcal{T}_{q}\), note that we always have \(z_{t}=j\) in the expectations, so that \(\Delta^{q}_{k,j}=0\) unless \(k=j\). For \(k=j\), we have \(\Delta^{q}_{k,j}=O(1)\), so that

\[\frac{1}{N}\sum_{k=1}^{N}\Delta^{q}_{k,j}=O\left(\frac{1}{N}\right).\]

Controlling \(\Delta^{r}_{k,j}\).Using that \(\|x_{t,1}-\bar{x}_{1}\|\leq C=O(1)\) for all \(t\), we provide the following crude bound via the triangle inequality and Holder inequality:

\[\Delta^{r}_{k,j} =\mathbb{E}\left[\sum_{t\in\mathcal{T}_{r}}\mathbbm{1}\{z_{t}\!=\! k\}(x_{t,1}-\bar{x}_{1})|y\!=\!k,q\!=\!j\right]-\mathbb{E}\left[\sum_{t\in \mathcal{T}_{r}}\mathbbm{1}\{z_{t}\!=\!k\}(x_{t,1}-\bar{x}_{1})|q\!=\!j\right]\] \[\|\Delta^{r}_{k,j}\| \leq C\left(\mathbb{E}\left[\sum_{t\in\mathcal{T}_{r}}\mathbbm{1 }\{z_{t}\!=\!k\}|y\!=\!k,q\!=\!j\right]+\mathbb{E}\left[\sum_{t\in\mathcal{T}_ {r}}\mathbbm{1}\{z_{t}\!=\!k\}|q\!=\!j\right]\right)\leq\frac{2CT}{N},\]

since \(z_{t}\) is independent of \(y\) given \(t\in\mathcal{T}_{r}\) and thus is uniform in \([N]\setminus\{j\}\), and \(|\mathcal{T}_{r}|\leq T\). We note, however, that \(\Delta^{r}_{k,j}\) may be controlled much more finely by leveraging the similarities between the distributions of \(z_{t},t\in\mathcal{T}_{r}\) with or without conditioning on \(y\).

Overall, we have shown that up to some spurious positional embeddings, \(W^{2}_{K}\) behaves as the desired associative memory from (7) when \(N\) is large enough, satisfying:

\[(\Phi_{1}w_{E}(i))^{\top}W^{2}_{K}w_{E}(j)\approx\frac{\alpha\eta}{TN}\left\{ \Omega\left(\frac{1}{T}\right)\mathbbm{1}\{i=j\}+O\left(\frac{T}{N}\right)\right\}\] (25)

We note that one may then amplify the gap between correct and incorrect associations by having a large enough step-size, which then makes the softmax more peaked and hence the attention more sparse and focused on correct associations.

#### b.3.3 Learning \(W^{1}_{k}\)

We now assume that \(W^{2}_{O}\) and \(W^{2}_{K}\) have learned the correct associations, and consider one gradient step away from zero-initialization on \(W^{1}_{K}\). Note that when \(W^{1}_{K}=0\), the predictions of the model are still often near random chance. Indeed, the second layer attention will attend to all tokens starting at the first occurrence of the trigger, since all such tokens contain \(\Phi_{1}w_{E}(q)\) in their average attention, which activates the second-layer attention head. Then the output is likely to predict the trigger itself, which will be an incorrect prediction most of the time.

We may thus consider \(\hat{p}(k|X)=1/N\) at this stage as well. We also consider a simplified architecture where the first layer attention only uses positional embeddings in the key-query matrix, and only token embeddings in the value-output matrix. In particular, we have \(x_{t}=w_{E}(z_{t})\). Lemma 5 then gives the following form for \(W^{1}_{K}\) after one gradient step of step-size \(\eta\):

\[W^{1}_{K} =\frac{\eta}{N}\sum_{k=1}^{N}\mathbb{E}_{X}\left[\frac{1}{T}\sum _{t=1}^{T}w_{U}(k)^{\top}\Phi_{2}x_{t}\cdot\frac{1}{t}\sum_{s=1}^{t}(\Phi_{1} x_{s})^{\top}W^{2}_{K}x_{T}(p_{s}-\bar{p}_{1:t})p^{\top}_{t}|y=k\right]\] \[\quad-\frac{\eta}{N}\sum_{k=1}^{N}\mathbb{E}_{X}\left[\frac{1}{T} \sum_{t=1}^{T}w_{U}(k)^{\top}\Phi_{2}x_{t}\cdot\frac{1}{t}\sum_{s=1}^{t}(\Phi _{1}x_{s})^{\top}W^{2}_{K}x_{T}(p_{s}-\bar{p}_{1:t})p^{\top}_{t}\right]\] \[\quad-\frac{\eta}{N}\sum_{k=1}^{N}\mathbb{E}_{X}\left[w_{U}(k)^{ \top}\Phi_{2}\bar{x}_{1:T}\cdot\frac{1}{T}\sum_{t=1}^{T}\frac{1}{t}\sum_{s=1}^ {t}(\Phi_{1}x_{s})^{\top}W^{2}_{K}x_{T}(p_{s}-\bar{p}_{1:t})p^{\top}_{t}|y=k\right]\] \[\quad+\frac{\eta}{N}\sum_{k=1}^{N}\mathbb{E}_{X}\left[w_{U}(k)^{ \top}\Phi_{2}\bar{x}_{1:T}\cdot\frac{1}{T}\sum_{t=1}^{T}\frac{1}{t}\sum_{s=1}^ {t}(\Phi_{1}x_{s})^{\top}W^{2}_{K}x_{T}(p_{s}-\bar{p}_{1:t})p^{\top}_{t}\right].\]

Note that since \(W^{2}_{O}\) and \(W^{2}_{K}\) already captured the desired associations at this stage, we have

\[w_{U}(k)^{\top}\Phi_{2}x_{t}\approx\alpha\,\mathbbm{1}\{z_{t}=k\}\quad\text{ and}\quad(\Phi_{1}x_{s})^{\top}W^{2}_{K}x_{T}\approx\alpha^{\prime}\,\mathbbm{1}\{z_{s}=z_{T}\},\]for some \(\alpha,\alpha^{\prime}>0\). Recall that in our model, we have \(z_{T}=q\) with probability one (\(q\) is the trigger token), and that \(q\) only appears twice: once at position \(t_{q}:=t_{o}-1<T\) and once at position \(T\). We then have, for any \(t>1\),

\[W_{K}^{1}p_{t}\approx\frac{\eta\alpha\alpha^{\prime}}{NTt}\sum_{k=1}^{N}(A_{t,k }-B_{t,k}-C_{t,k}+D_{t,k}),\]

with

\[A_{t,k} =\mathbb{E}[\mathbbm{1}\{z_{t}\!=\!k\}\,\mathbbm{1}\{t_{q}\leq t \}(p_{t_{q}}-\bar{p}_{1:t})|y\!=\!k]\] (26) \[B_{t,k} =\mathbb{E}[\mathbbm{1}\{z_{t}\!=\!k\}\,\mathbbm{1}\{t_{q}\leq t \}(p_{t_{q}}-\bar{p}_{1:t})]\] (27) \[C_{t,k} =\mathbb{E}[r_{k}\,\mathbbm{1}\{t_{q}\leq t\}(p_{t_{q}}-\bar{p}_ {1:t})|y\!=\!k]\] (28) \[D_{t,k} =\mathbb{E}[r_{k}\,\mathbbm{1}\{t_{q}\leq t\}(p_{t_{q}}-\bar{p}_ {1:t})],\] (29)

where \(r_{k}:=\frac{1}{T}\sum_{t=1}^{T}\mathbbm{1}\{z_{t}=k\}\). We have

\[A_{t,k} =\mathbb{E}[\mathbbm{1}\{z_{t}\!=\!k\}(\mathbbm{1}\{t_{q}=t-1\}+ \mathbbm{1}\{t_{q}\in[t-2]\cup\{t\}\})(p_{t_{q}}-\bar{p}_{1:t})|y\!=\!k]\] \[=\mathbb{P}(t_{q}=t-1|y=k)(p_{t-1}-\bar{p}_{1:t})+\frac{1}{N}\sum _{s\in[t-2]\cup\{t\}}\mathbb{P}(t_{q}=s|y=k)(p_{s}-\bar{p}_{1:t})\] \[=\mathbb{P}(t_{q}=t-1)(p_{t-1}-\bar{p}_{1:t})+O\left(\frac{1}{N} \right),\]

since the distribution of \(t_{q}\) is the same regardless of \(y\). We proceed similarly for the other quantities and obtain the following:

\[B_{t,k} =O\left(\frac{1}{N}\right)\] \[C_{t,k} =\frac{\mathbb{P}(t_{q}=t-1)}{T}(p_{t-1}-\bar{p}_{1:t})+O\left( \frac{1}{N}\right)\] \[D_{t,k} =O\left(\frac{1}{N}\right).\]

This yields the following associative memory behavior, for \(t>1\):

\[p_{s}^{\top}W_{K}^{1}p_{t}\approx\frac{\eta\alpha\alpha^{\prime}(T-1)}{T^{2}t }\left\{\mathbb{P}(t_{q}=t-1)\left(1\{s=t-1\}-\frac{1}{t}\,\mathbbm{1}\{s\in[t ]\}\right)+O\left(\frac{1}{N}\right)\right\},\]

which matches the desired "previous token head" behavior from (7) when \(N\) is large. As in the case of \(W_{K}^{2}\), we may then "saturate" the softmax by choosing a large enough step-size.

## Appendix C Other Proofs

### Proof of Lemma 1

Proof.: Recall the form of the cross-entropy loss for classification with \(K\) classes:

\[\ell(y,\xi)=-\sum_{k=1}^{N}\mathbbm{1}\{y=k\}\log\frac{e^{\xi_{k}}}{\sum_{j}e ^{\xi_{j}}}.\]

Its derivatives take the form

\[\frac{\partial\ell}{\partial\xi_{k}}(y,\xi)=s(\xi)_{k}-\mathbbm{1}\{y=k\},\]

with \(s(\xi)_{k}=\frac{e^{\xi_{k}}}{\sum_{j}e^{\xi_{j}}}\) the softmax.

The gradient of \(L\) is then given by

\[\nabla_{W}L(W) =\mathbb{E}_{(z,y)}\left[\sum_{k=1}^{N}\frac{\partial\ell}{\partial \xi_{k}}(y,W_{U}Ww_{E}(z))\nabla_{W}(w_{U}(k)^{\top}Ww_{E}(z))\right]\] \[=\mathbb{E}_{(z,y)}\left[\sum_{k=1}^{N}(\hat{p}_{W}(k|z)-\mathbbm{1 }\{y=k\})w_{U}(k)w_{E}(z)^{\top}\right]\] \[=\sum_{k=1}^{N}\mathbb{E}_{z}[\mathbb{E}_{y}[(\hat{p}_{W}(k|z)- \mathbbm{1}\{y=k\})w_{U}(k)w_{E}(z)^{\top}\mid z]]\] \[=\sum_{k=1}^{N}\mathbb{E}_{z}[(\hat{p}_{W}(k|z)-\mathbbm{E}_{y}[ \mathbbm{1}\{y=k\}|z])w_{U}(k)w_{E}(z)^{\top}],\]

which yields the desired result. 

### Proof of Lemma 2

Proof.: Using similar steps as the proof of Lemma 1, we have

\[\nabla_{W}L(W) =\mathbb{E}_{(x,y)}\left[\sum_{k=1}^{N}\frac{\partial\ell}{ \partial\xi_{k}}(y,W_{U}Wx)\nabla_{W}(w_{U}(k)^{\top}Wx)\right]\] \[=\mathbb{E}_{(x,y)}\left[\sum_{k=1}^{N}(\hat{p}_{W}(k|x)-\mathbbm{ 1}\{y=k\})w_{U}(k)x^{\top}\right]\] \[=\sum_{k=1}^{N}w_{U}(k)\,\mathbb{E}_{x}[\hat{p}_{W}(k|x)x]^{\top} -\sum_{k=1}^{N}\mathbb{E}_{y}[\mathbbm{1}\{y=k\}w_{U}(k)\,\mathbb{E}[x|y]^{\top}]\] \[=\sum_{k=1}^{N}w_{U}(k)\,\mathbb{E}_{x}[\hat{p}_{W}(k|x)x]^{\top} -\sum_{k,j=1}^{N}p(y=j)\,\mathbbm{1}\{j=k\}w_{U}(k)\,\mathbb{E}[x|y=j]^{\top}\] \[=\sum_{k=1}^{N}p(y=k)w_{U}(k)(\hat{\mu}_{k}-\mu_{k})^{\top},\]

with \(\hat{\mu}_{k}=p(y=k)^{-1}\,\mathbb{E}_{x}[\hat{p}_{W}(k|x)x]\) and \(\mu_{k}=\mathbb{E}[x|y=k]\). 

### Proof of Lemma 4

Proof.: To better isolate the role of keys from values, we denote the keys that are fed into the matrix \(W\) by \(Z=[z_{1},\dots,z_{T}]\in\mathbb{R}^{d\times T}\), while the query is simply \(x_{T}\). In practice we have \(Z=X\), and both are superpositions of potentially multiple embeddings (if \(W\) is part of the second attention layer, these are the token embedding, positional embedding, and the output of the first attention layer).

The gradient of the loss at \(W=0\) writes:

\[\nabla_{W}L(W)\big{|}_{W=0}=\mathbb{E}_{(X,Z,y)}\left[\sum_{k=1}^ {N}\frac{\partial\ell}{\partial\xi_{k}}(y,\xi)\cdot\nabla_{W}(w_{U}(k)^{\top} \Phi_{2}X\sigma(Z^{\top}Wx_{T}))\big{|}_{W=0}\right]\] (30) \[=\mathbb{E}_{(X,Z,y)}\left[\sum_{k=1}^{N}(\hat{p}_{W}(k|X,Z)- \mathbbm{1}\{y=k\})\cdot\nabla_{W}(w_{U}(k)^{\top}\Phi_{2}X\sigma(Z^{\top}Wx_ {T}))\big{|}_{W=0}\right].\] (31)

We have

\[\nabla_{W}(w_{U}(k)^{\top}\Phi_{2}X\sigma(Z^{\top}Wx_{T}))\big{|} _{W=0} =\sum_{t=1}^{T}w_{U}(k)^{\top}\Phi_{2}x_{t}\cdot\nabla_{W}(\sigma (Z^{\top}Wx_{T})_{t})\] \[=\frac{1}{T}\sum_{t=1}^{T}w_{U}(k)^{\top}\Phi_{2}x_{t}\cdot(z_{t} -\bar{z}_{1:T})x_{T}^{\top},\]where \(\bar{z}_{1:T}=\frac{1}{T}\sum_{t}z_{t}\), and we used the fact that

\[\frac{\partial}{\partial u_{s}}\sigma(u)_{t}\big{|}_{u=0}=\frac{1}{T}\,\mathbbm{1 }\{t=s\}-\frac{1}{T^{2}}.\] (32)

The gradient (31) now writes

\[\nabla_{W}L(W)\big{|}_{W=0}=\sum_{k=1}^{N}\mathbb{E}_{(X,Z)}[(\hat{p}_{W}(k|X,Z )-\mathbbm{1}\{y=k\})\frac{1}{T}\sum_{t=1}^{T}w_{U}(k)^{\top}\Phi_{2}x_{t}\cdot( z_{t}-\bar{z}_{1:T})x_{T}^{\top}],\]

and the result follows. 

### Proof of Lemma 5

Proof.: The linearization of the second layer softmax around zero takes the following form:

\[\bar{\sigma}(Z^{\top}W_{2}x_{T})_{t}=\frac{1}{T}(1+z_{t}^{\top}W_{2}x_{T}-\bar {z}_{1:T}^{\top}W_{2}x_{T}),\]

with \(z_{t}=\sum_{s=1}^{t}\Phi_{1}x_{s}\sigma(p_{1:t}^{\top}Wp_{t})_{s}\) the output of the first attention layer.

\[\xi_{k} =\sum_{t=1}^{T}w_{U}(k)^{\top}\Phi_{2}x_{t}\bar{\sigma}(Z^{\top}W _{2}x_{T})\] \[=\frac{1}{T}\sum_{t=1}^{T}w_{U}(k)^{\top}\Phi_{2}x_{t}+\frac{1}{ T}\sum_{t=1}^{T}w_{U}(k)^{\top}\Phi_{2}x_{t}\sum_{s=1}^{t}(\Phi_{1}x_{s})^{ \top}W_{2}x_{T}\sigma(p_{1:t}^{\top}Wp_{t})_{s}\] \[\quad-w_{u}(k)^{\top}\Phi_{2}\bar{x}_{1:T}\cdot\frac{1}{T}\sum_{t =1}^{T}\sum_{s=1}^{t}(\Phi_{1}x_{s})^{\top}W_{2}x_{T}\sigma(p_{1:t}^{\top}Wp_{ t})_{s}.\]

Then,

\[\nabla_{W}L(W)\big{|}_{W=0}\] (33) \[=\mathbb{E}_{(X,y)}\left[\sum_{k=1}^{N}\frac{\partial\ell}{ \partial\xi_{k}}(y,\xi)\cdot\nabla_{W}\xi_{k}\big{|}_{W=0}\right]\] (34) \[=\mathbb{E}_{(X,y)}\left[\sum_{k=1}^{N}\frac{\partial\ell}{ \partial\xi_{k}}(y,\xi)\frac{1}{T}\sum_{t=1}^{T}w_{U}(k)^{\top}\Phi_{2}x_{t} \frac{1}{t}\sum_{s=1}^{t}(\Phi_{1}x_{s})^{\top}W_{2}x_{T}(p_{s}-\bar{p}_{1:t}) p_{t}^{\top}\right]\] (35) \[\quad-\mathbb{E}_{(X,y)}\left[\sum_{k=1}^{N}\frac{\partial\ell}{ \partial\xi_{k}}(y,\xi)w_{U}(k)^{\top}\Phi_{2}\bar{x}_{1:T}\cdot\frac{1}{T} \sum_{t=1}^{T}\frac{1}{t}\sum_{s=1}^{t}(\Phi_{1}x_{s})^{\top}W_{2}x_{T}(p_{s} -\bar{p}_{1:t})p_{t}^{\top}\right],\] (36)

using (32). The result follows by using \(\frac{\partial\ell}{\partial\xi_{k}}(y,\xi)=\hat{p}(k|\xi)-\mathbbm{1}\{y=k\}\).

## Appendix D Beyond our Simplified Architecture

While the simplified architecture presented in Section 4.2 is sufficient to capture the desired induction behavior for our bigram task, transformer architectures used in practice typically involve more components, as well as more heads and layers. In this section, we discuss how our memory viewpoint extends to such architectures.

Factorizations.In practice, transformers typically involve products of matrices, potentially with a low-rank bottleneck. For instance, our key-query matrices \(W_{K}\) should instead be considered as a product \(W_{K}^{\top}W_{Q}\), and the output-value matrices \(W_{O}\) and \(W_{V}\) are typically jointly optimized.

Consider an associative memory of the form:

\[W_{*}=\sum_{i}y_{i}x_{i}^{\top}\in\mathbb{R}^{d\times d},\]

where \((x_{i})_{i}\) and \((y_{i})_{i}\) are appropriate collections of near-orthonormal embeddings.

We now argue that a similar associative memory can be achieved with the factorization \(W=\frac{d}{2d^{\prime}}UV\), where \(U\in\mathbb{R}^{d\times d^{\prime}}\), \(V\in\mathbb{R}^{d^{\prime}\times d}\) with \(d^{\prime}\leq d\) (for instance \(d^{\prime}\) could be the dimension of attention heads), are given by:3

Footnote 3: When \(d^{\prime}\) is the head dimension, the \(\frac{d}{d^{\prime}}\) scaling can be interpreted as the correct multiplier to use in attention logits, which plays a similar role to the \(\frac{d}{d^{\prime}}\) multiplier in the \(\mu\)P scaling [59], for our setup where the variance of the random entries of input embeddings is \(1/d\) instead of \(1\) as in [59].

\[U =U_{0}+\sum_{i}y_{i}(V_{0}x_{i})^{\top}\] \[V =V_{0}+\sum_{i}(U_{0}^{\top}y_{i})x_{i}^{\top},\]

where \(U_{0}\) and \(V_{0}\) are random matrices with \(\mathcal{N}(0,\frac{1}{d})\) entries. These matrices are similar to those that would arise from a single gradient step individually on \(U\) and \(V\) from initializations \(U_{0}\) and \(V_{0}\), as in Lemma 1. To see why \(W\) behaves like \(W_{*}\), note that we have

\[UV=U_{0}V_{0}+\sum_{i}y_{i}(V_{0}x_{i})^{\top}V_{0}+\sum_{i}(U_{0}^{\top}y_{i} )x_{i}^{\top}+\sum_{i,j}y_{i}(V_{0}x_{i})^{\top}(U_{0}^{\top}y_{j})x_{j}^{\top}.\]

It is also easy to check using central limit arguments (similar to remapping in Appendix A) that \(\tilde{x}_{i}:=\sqrt{\frac{d}{d^{\prime}}}V_{0}x_{i}\in\mathbb{R}^{d^{\prime}}\) and \(\tilde{y}_{i}:=\sqrt{\frac{d}{d^{\prime}}}U_{0}^{\top}y_{i}\) are all nearly-orthonormal embeddings. Thus, we have

\[2y_{k}^{\top}Wx_{l} =\tilde{y}_{k}^{\top}\tilde{x}_{l}+\sum_{i}y_{k}^{\top}y_{i} \tilde{x}_{i}^{\top}\tilde{x}_{l}+\sum_{i}\tilde{y}_{k}^{\top}\tilde{y}_{i}x_{ i}^{\top}x_{l}+\sum_{i,j}y_{k}^{\top}y_{i}\tilde{x}_{i}^{\top}\tilde{y}_{j}x_{j}^{ \top}x_{l}\] \[\approx 0+\mathbbm{1}\{k=l\}+\mathbbm{1}\{k=l\}+0,\]

where the first and last term vanish due to the cross-terms \(\tilde{y}_{i}^{\top}\tilde{x}_{i^{\prime}}\) which vanish for any \(i,i^{\prime}\). Thus, \(W\) and \(W_{*}\) encode the same associations, when \(d\) and \(d^{\prime}\) are large enough to ensure near-orthogonality.

Layer-normalization.Normalization layers [4, 60] are typically used in transformers to improve training stability [52], and are applied on each token representation, either after [52] or before [6] each block. It may be seen as an operation of the form4

Footnote 4: RMSNorm [60] would use the variance instead of the norm in the denominator, leading to an additional \(\sqrt{d}\) factor in the numerator. Here we use the norm, which is more natural when embeddings have near-unit norm, in contrast to the \(\approx\sqrt{d}\) norm for the standard parameterization.

\[LN(x)=\frac{x}{\|x\|},\]

applied to the input or output of a given block.

In order to obtain a basic understanding of the role of layer-norm in our associative memory setup, we may consider the setup Lemma 1, with a normalization applied after the linear operation, leading to the population loss:

\[L(W)=\mathbb{E}_{(x,y)\sim p}[\ell(y,W_{U}LN(Wx))].\] (37)

The gradients then take the form

\[\nabla_{W}L(W)=\sum_{k=1}^{N}\mathbb{E}_{x}\left[\frac{\hat{p}_{W}(y=k|x)-p(y= k|x)}{\|Wx\|}\left(I-\frac{(Wx)(Wx)^{\top}}{\|Wx\|^{2}}\right)w_{U}(k)x^{\top} \right].\] (38)This illustrates that in addition to weighting the updates on class \(k\) by the prediction error \(\hat{p}(k|x)-p(k|x)\), the updates are also projected on the orthogonal of the \(Wx\) direction. This means that an update on the direction \(w_{U}(k)x^{\top}\) will occur only to the extent that \(Wx\) is not already aligned with \(w_{U}(k)\). Thus, if an association is "stored" once, so that \(Wx\approx w_{U}(y)\), layer-norm will avoid further updating \(W\) in that direction, hopefully avoiding norms that grow too much, and also encouraging frequent and infrequent tokens to be weighted similarly in the final memory (see also [7] for more discussion on this).

Note that at random initialization \(Wx\) is nearly orthogonal to any \(w_{U}(k)\), so that layer-norm only starts playing a significant role later in training, and does not affect our theoretical analysis based on single gradient steps.

MLP blocks.If we denote by \((u_{i})_{i}\) and \((v_{i})_{i}\) collections of near-orthonormal input and output embeddings, an MLP block may encode associations \((i,j)\in\mathcal{M}\) as follows:

\[F(x)=\sum_{(i,j)\in\mathcal{M}}v_{j}\sigma(u_{i}^{\top}x-b),\]

where \(\sigma\) is a non-linear activation and \(b\) a bias term. Then, if one assumes that \(u_{i}^{\top}u_{j}\leq b\) for \(i\neq j\) and \(\sigma(t)=0\) for \(t<0\), then this can help filter out noise that arises from near-orthogonality, and may then lead to additional storage capacity, at the cost of additional computation (see, _e.g._, [7, 30]).

An additional benefit of MLP layers discussed in Section 4 is that they may encode many-to-many associations, which is useful when multiple embeddings are in the residual stream and need to be considered jointly (_e.g._, a subject and a relation in a factual recall task [35]). This may be achieved, for instance, by considering embeddings \(u_{\mathcal{I}}=\frac{1}{\sqrt{|\mathcal{I}|}}\sum_{i\in\mathcal{I}}u_{i}\), where \(\mathcal{I}\) are sets of bounded size (_e.g._, as obtained using layer-norm over a residual stream). Then, assuming the \(u_{i}\) are nearly-orthonormal, we have \(u_{\mathcal{I}}^{\top}u_{\mathcal{I}}\approx 1\) while \(u_{\mathcal{I}}^{\top}u_{\mathcal{I}^{\prime}}\lesssim 1-\delta\) if \(\mathcal{I}=\mathcal{I}^{\prime}\), for some \(\delta\) that depends on the maximal cardinality of these sets. Although \(1-\delta\) is no-longer vanishingly small, defining

\[F(x)=\sum_{(\mathcal{I},\mathcal{J})\in\mathcal{M}}v_{\mathcal{J}}\sigma(u_{ \mathcal{I}}^{\top}x-b),\]

the non-linearity may still succeed at filtering out any \(\mathcal{I}\) that does not correspond to the query set in \(x\). We leave the question of how such non-linear associative memories may arise from training dynamics to future work.

Multiple heads and layers.We remark that our view of weights as associative memories applies to any parameter other than embedding/unembedding layers, and thus naturally extends to multiple heads (using the low-rank factorizations described above) and multi-layer models.

It is important to note, however, that the redundancy introduced by having more heads and layers makes it more challenging to identify which layer/head/weight will learn certain associations (see, _e.g._, Figure 11 in Appendix E). This is in contrast to our simplified architecture of Section 4.2, where we may identify the role of each matrix (up to some possible redundancy when using the feed-forward layer \(W_{F}\)). In practice, mechanisms may appear in different heads/layers across different training runs, which makes interpretability more challenging, and typically requires some causal identification techniques, such as mediation analysis [35, 54].

## Appendix E Experiment Details and Additional Experiments

In this section, we present additional details on the experiments, as well as additional results.

Computing setup.We use Pytorch and each run uses a single GPU, along with 60 CPU cores for real-time data generation. We will make our code available upon publication.

Hyperparameters.We now provide the hyperparameters used in each figure. The SGD step-size is denoted \(\eta\). We fix the momentum parameter to \(0.9\) and the weight decay parameter to \(10^{-4}\). \(U\) denotes the uniform distribution over \([N]\).

* Figure 2: \(K=3\), \(\pi_{q}=\pi_{u}\) (random triggers) or \(Q\) is the \(K\) most likely elements of \(\pi_{u}\), \(\pi_{o}=U\), \(d=128\), \(d_{hidden}=4\times 128\) (hidden dimension of the feed-forward MLPs), \(\eta=0.2\).
* Figure 3: \(K=5\), \(\pi_{q}=\pi_{u}\) (random triggers), \(\pi_{o}=U\), \(d=128\), \(\eta=0.2\).
* Figure 4(left) and Figure 5: \(\pi_{o}=U\), \(d=128\), \(\eta=1\). For random triggers we use \(\pi_{q}=\pi_{u}\). For \(K=1\) with fixed frequent trigger, the only trigger is the most probable token according to \(\pi_{u}\), while for \(K=5\) with fixed rare triggers, the five triggers are the 6-th to 10-th most probable tokens according to \(\pi_{u}\).
* Figure 4(center): \(K=3\), \(\pi_{q}=\pi_{u}\) (random triggers), \(\pi_{o}=U\) or \(\pi_{o}=\pi_{b}\) (conditioned on the trigger), \(d=128\), \(\eta=1\).
* Figure 4(right): \(K=3\), \(\pi_{q}=\pi_{u}\) (random triggers), \(\pi_{o}=U\), \(d=128\), \(\eta=1\).

Memory recall probes and data-distributional properties.Figure 5 and Figure 6 show the evolution of the different memory probes for the settings considered in Figure 4(left,center). Figure 5 highlights that associative memories for the induction head are slower to learn when using few triggers (small \(K\)), rare fixed triggers, or random triggers (note that the probe for \(W_{K}^{2}\) with fixed triggers only shows recall accuracy on the set of triggers \(Q\), which is an easier task). Figure 6

Figure 5: Memory recall probes for the setting of Figure 4(left).

Figure 6: Memory recall probes for the setting of Figure 4(center).

shows that using uniform output tokens can lead to better fitting of \(W_{O}^{2}\) and \(W_{K}^{2}\) compared to using output tokens sampled from bigrams. In addition to the increased diversity when using uniform outputs, this may also be due to the fact that bigram outputs are already well predicted using global statistics with just the feed-forward layer, hence the gradient signal on such well-predicted tokens may not propagate through the induction head mechanism. In contrast, the recall accuracy for \(W_{K}^{1}\) is comparable for both settings, since the previous token head is useful at all positions regardless of the output token distribution.

Visualizing memories.Figure 7 shows visualizations of the associative memory behaviors after training. We see that diagonal elements dominate in the plots, which corresponds to correct associations lead to high'memory recall'. Nonetheless, we see that some of the diagonal elements are weaker than others, particularly for late positions in \(W_{K}^{1}\), and for some of the trigger tokens in \(W_{K}^{2}\), while the diagonal for \(W_{O}^{2}\) seems to be roughly uniform. We note that characters corresponding to capital letters have token index 13 to 38, while lowercase letters have index 39 to 64. The association patterns found in \(W_{K}^{2}\) then seem related to frequencies of appearance of triggers, whereby capital letters appear less frequently in the data, and are also less frequently chosen as triggers, compared to lowercase letters. Similarly, since the first occurrence of triggers is typically early in a sequence, it is natural that \(W_{K}^{1}\) learns stronger associations at earlier positions. In contrast, diagonal elements for \(W_{O}^{2}\) are nearly uniform, which agrees with the fact that output tokens are sampled uniformly in this setup. We refer to the follow-up work [7] for an analysis of how data frequencies affect association strength in such associative memories.

Effect of dimension.Recall that our study of associative memories with random embeddings requires large dimension \(d\) in order to ensure near-orthogonality, and thus store input-output pairs more effectively. In Figure 8, we evaluate the recall accuracy for \(W_{O}^{2}\) for varying dimension, when training it by itself, and only on the output tokens (as in Figure 3). We see that higher dimension leads to faster learning of the memory, in particular \(d=128\) seems sufficient for fast learning after just a few iterations with a tuned learning rate. If the learning rate isn't tuned, we notice that there is a further slowdown for low dimension, is likely due to issues with the fact that our experiments use the standard parameterization of neural networks at initialization, rather than maximal update parameterizations [58]. Note that learning \(W_{O}^{2}\) alone is a convex optimization problem, and we hypothesize that higher dimension makes the problem better conditioned, and hence easier to learn. In Figure 9, we show "one-step" recall accuracies for classifying output tokens from the average

Figure 8: Effect of dimension on learning \(W_{O}^{2}\) alone, with fixed or tuned learning rate.

Figure 9: Accuracy of one-step estimate of \(W_{O}^{2}\) with varying dimension and number of batches used for computing expectations. Each batch consists of 32 sequences of 256 tokens for a total of 8 192 tokens, with \(K=5\) random triggers and uniform outputs.

attention input to \(W_{O}^{2}\), given by

\[R_{1}=\frac{1}{N}\sum_{k=1}^{N}\mathbbm{1}\left\{k=\arg\max_{k^{\prime}}(W_{V}^{2}w _{E}(k^{\prime}))^{\top}(\mu_{k}-\mu)\right\},\]

where \(\mu_{k}=\mathbb{E}[x|y=k]\) and \(\mu=\mathbb{E}[x]\), for \(x=\frac{1}{t}\sum_{s=1}^{t}W_{V}^{2}w_{E}(z_{s})\) and \(y=z_{t+1}\), when \(z_{t}\) is a trigger token after its first occurrence. Expectations are computed over batches of data of varying sizes and in different dimensions. We call this "one-step" since it is related to the classifier obtained after performing a single gradient step on \(W_{O}^{2}\) from zero initialization (see Lemma 2 and Appendix B.3.1). The plots illustrate that this simple one-step model is already able to extract relevant signal from the noisy average attention, after a handful of batches of data, corresponding to tens of thousands of tokens, and that this gets easier as the dimension increases.

More complex architectures.Figure 10 shows training behavior for a more complex model than the simplified one considered in Section 5, namely where we train all parameters, replace the linear \(W_{F}\) feedforward layer by a two-layer MLP, and were (pre-)layer-normalization is added. Despite these changes, we see similar behavior for the memory recall probes (which now involve embeddings that may change over time), suggesting that the model is still identifying the same memory associations, despite the additional redundancies in parameters and modified training dynamics.

Figure 11 shows the attention maps obtained when training a multi-head version of our two-layer model, with four attention heads per layer. We see that the redundancy of multiple heads creates difficulties in identifiability: only one of the first layer heads learns the previous token behavior, while the induction behavior is shared across different heads at the second layer. This illustrates the challenges of interpretability in the presence of redundant models, which then require additional work to identify which of the layers and heads are performing a given behavior, _e.g._, through interventions and causal mediation analysis [35, 54].

Figure 11: Attention maps for a two-layer model with 4 attention heads. In the first layer (top), the previous token mechanism is mostly achieved by one of the four heads, while the induction behavior at the second layer (bottom) is distributed across the different heads.

Figure 10: Training of a more realistic architecture with (i) ReLU MLP instead of linear layer for the second feed-forward layer, (ii) all parameters trained, including embeddings, (iii) pre-layer normalization. The loss, in-context accuracy and memory recall probes are similar to the simplified architecture (see, _e.g._, Figure 4).