# Decoupling Semantic Similarity from Spatial Alignment for Neural Networks

Tassilo Wald\({}^{*,1,2,3}\), Constantin Ulrich\({}^{1,4,7}\), Gregor Kohler\({}^{1,3}\),

David Zimmerer\({}^{1,2}\), Stefan Denner\({}^{1,3}\), Michael Baumgartner\({}^{1,3}\),

Fabian Isensee\({}^{1,2}\), Priyank Jaini\({}^{1,5}\), Klaus H. Maier-Hein\({}^{1,1,2,3,4,6}\)

Division of Medical Image Computing,

German Cancer Research Center (DKFZ), Heidelberg, Germany

\({}^{2}\) Helmholtz Imaging, DKFZ, Heidelberg, Germany

\({}^{3}\) Faculty of Mathematics and Computer Science,

University of Heidelberg, Germany

\({}^{4}\) Medical Faculty Heidelberg, University of Heidelberg, Germany

\({}^{5}\) Google Deepmind

\({}^{6}\) Pattern Analysis and Learning Group, Department of Radiation Oncology

\({}^{7}\) National Center for Tumor Diseases (NCT) Heidelberg, Germany

Corresponding author: tassilo.wald@dkfz-heidelberg.deShared last authorship.

###### Abstract

What representation do deep neural networks learn? How similar are images to each other for neural networks? Despite the overwhelming success of deep learning methods key questions about their internal workings still remain largely unanswered, due to their internal high dimensionality and complexity. To address this, one approach is to measure the similarity of activation responses to various inputs. Representational Similarity Matrices (RSMs) distill this similarity into scalar values for each input pair. These matrices encapsulate the entire similarity structure of a system, indicating which input leads to similar responses. While the similarity between images is ambiguous, we argue that the spatial location of semantic objects does neither influence human perception nor deep learning classifiers. Thus this should be reflected in the definition of similarity between image responses for computer vision systems. Revisiting the established similarity calculations for RSMs we expose their sensitivity to spatial alignment. In this paper, we propose to solve this through _semantic RSMs_, which are invariant to spatial permutation. We measure semantic similarity between input responses by formulating it as a set-matching problem. Further, we quantify the superiority of _semantic_ RSMs over _spatio-semantic_ RSMs through image retrieval and by comparing the similarity between representations to the similarity between predicted class probabilities.

## 1 Introduction

Deep neural networks are trained to extract powerful feature representations for a wide range of downstream tasks. Despite this, their inner workings are highly-complex, making understanding _how_ networks solve tasks and _what_ they learn challenging. To obtain a better understanding of these fundamental questions, researchers in the fields of neuroscience,

[MISSING_PAGE_FAIL:2]

representation vectors and hence play an important role. We introduce an exemplary kernel in Section 3 and all kernels used in this paper with some properties in Appendix A.

With the two RSMs \(K\) and \(L\) at hand, it is possible to compare the similarity structure between the two models. As introduced in Kornblith et al. [12] one can use the Hilbert-Schmidt independence criterion (HSIC) [5; 28] to calculate the level of independence through Centered Kernel Alignment, providing a measure of similarity of the two representations \(Z_{1}\) and \(Z_{2}\) with \(\mathcal{H}\) denoting the centering matrix.

\[\text{HSIC}(K,L)=\frac{1}{(n-1)^{2}}\text{tr}\left(K\mathcal{H}L \mathcal{H}\right)\] (1) \[\text{CKA}(K,L)=\frac{\text{HSIC}(K,L)}{\sqrt{\text{HSIC}(K,K) \text{HSIC}(L,L)}}\] (2)

Alternatively, a variety of different measures based on RSMs are possible for which we refer to Section 3.3 of Klabunde et al. [10]. As highlighted above, the similarity calculation based on RSMs is a two-step process with the first being the calculation of the RSMs and the latter being the comparison of the RSMs. In this paper, we focus on the first step, by quantifying the importance of disentangling semantic similarity from spatial alignment. While not the focus of this paper, we provide qualitative examples of the downstream effect on CKA measures in Appendix G.

## 3 The Semantic Representational Similarity Matrix

Representational Similarity Matrices (RSMs) are designed to reflect the system behavior of interest. The RSM \(K\), originally introduced by Kriegeskorte et al. [13], represents the similarity structure of a system given a set of inputs \(x\in\{x_{0},\ldots x_{N}\}\). Each value \(K_{ij}\) in \(K\) quantifies how similar the responses of two inputs \(z_{i}\) and \(z_{j}\) are to each other. The definition of what symmetries between representations similarity measures should be invariant to is a central point of debate. Previous work proposed permutation invariance [15], invariance to orthogonal transformations [12], or invariances to invertible linear transformations [25; 19]. While arguments for any of these invariances are valid, we believe that an important aspect has been neglected in the calculation of RSMs: The spatial alignment between the representations!

The dependency on spatial alignmentRevisiting the structure of representations of a CNN, channels \(C\) correspond to semantic concepts while the spatial position corresponds to where the semantic concept is localized in the input image [34]. Consequently, one can reformulate the representation \(z_{i}\) of a sample \(x_{i}\) to be fully defined by a set of **semantic concept vectors v**, one for each spatial location \(S\): \(z_{i}=\{\textbf{v}_{0},\ldots,\textbf{v}_{S}\}\) with \(\textbf{v}\in\mathbb{R}^{C}\). In the case of linear CKA [12], the RSMs are then calculated, between semantic concept vectors at the same spatial location. For instance, when employing the linear kernel \(K_{ij}\) can be expressed as:

\[K_{ij}=\sum_{s}\langle\textbf{v}_{z_{1},s},\textbf{v}_{z_{2},s}\rangle\] (3)

Figure 1: Current spatio-semantic RSMs couple semantic similarity with spatial alignment. Our proposal focuses solely on measuring semantic similarity. We achieve this by determining the optimal permutation between two representations and introducing sample-wise permutation invariance.

This formulation emphasizes the coupling of semantic similarity and localization during similarity calculation, which we term as _spatio-semantic RSMs_. This coupling can lead to issues, e.g. when comparing an image to a translated version of itself. Due to the quasi translation-equivariant nature of CNNs3 semantic vectors are translated similarly, changing the alignment of pairs of \(\mathbf{v}\), leading to a low perceived similarity despite highly similar semantic vectors. This issue is visualized in a small toy example in Fig. 1

Footnote 3: The patch embedding can similarly translate semantics to a different position in the sequence.

### Decoupling Localization and Semantic Content

As shown above, current RSMs compare different input samples without accounting for the lack of spatial alignment. Previous work of Williams et al. [32] recognized this and introduced translation invariance to RSMs by finding the optimal translation \(a,b\) of the representations \(z^{\prime}_{j}=\{\mathbf{v}_{0+a,0+b},\ldots,\mathbf{v}_{w+a,h+b}\}\) to maximize similarity \(K_{ij}\quad\text{max}_{a,b}=\langle z_{i},z^{\prime}_{j}\rangle\) through circular shifts.

While this is an improvement to no spatial alignment and emulates a CNN's inherent translation equivariance, we argue that the measure of representational similarity should not be constrained to what the underlying model is invariant to, but the similarity measure should be invariant to the possible spatial configurations of semantic features in the input image.

To motivate this, we propose a thought experiment:

Imagine we have trained a classifier with an augmentation pipeline including rotations. Given an image and a rotated version of the image, we extract representations \(z\) at layer \(i\) once for the normal \(z_{i}\) and once for the rotated image \(z_{i,rot}\). Due to the initial rotation, these representations may differ in earlier layers \(i\), due to the network extracting different edges and corners. However, if the network successfully learned to become invariant to the augmentation, it may have learned to map it to the same semantic vector \(\mathbf{v}\) at a later layer but at a different spatial location. For such cases, we argue that the similarity between the two representations should be high. Should the model be sensitive to the rotation, no semantically similar representations may be expressed at a later layer, which should lead to a low similarity.

This reasoning can be extended to all kinds of shifts, be they artificial augmentations like shearing or mirroring or natural variations of the input manifold. **Subsequently, we argue that the similarity measure should be invariant to as many spatial shifts as possible. This alone allows one to measure the similarity of representations a model is invariant to, be these learned or designed invariances.** Such variable shifts cannot be captured with simple translation operations.

#### 3.1.1 Introducing permutation invariance

To impose as minimal constraints on spatial structure as possible, we propose to make \(K_{ij}\) invariant to all spatial permutations of the semantic concept vectors \(\mathbf{v}\). Formalizing this we demand that the similarity \(K_{ij}=k(z_{i},z_{j})=k(z_{i},\mathbf{P}_{ij}z_{j})\) with \(\mathbf{P}_{ij}\in\mathbb{R}^{S\times S}\) being a unique permutation matrix for the pair of \(z_{i}\) and \(z_{j}\). To accomplish this, we propose to find the optimal permutation matrix \(\mathcal{P}_{ij}\) that maximizes the similarity \(K_{ij}\).

\[\mathcal{P}_{ij}=\text{argmax}_{P}\quad k(z_{i},\mathbf{P}_{ij}z_{j})\] (4)

To find the optimal permutation matrix \(\mathbf{P}_{ij}\), we decide to use the linear kernel \(\langle\cdot,\cdot\rangle\) to maximize both the magnitude of activation and the direction of vectors, as both magnitude of activation and direction of the vectors matter[12]. This allows us to calculate an affinity matrix \(\mathbf{A}_{ij}\in\mathbb{R}^{S\times S}\) measuring the similarity between all concept vectors:

\[\mathbf{A}_{ij}=[\mathbf{v}_{i,0},\ldots\mathbf{v}_{i,S}]^{\intercal}[ \mathbf{v}_{j,0},\ldots\mathbf{v}_{j,S}].\] (5)

With this affinity matrix, bipartite set-matching algorithms, such as Hungarian matching, can be employed to find the optimal permutation matrix \(\mathcal{P}_{ij}\) that maximizes the inner product between \(z_{i}\) and \(z^{\prime}_{j}\). Finding all \(\mathcal{P}_{ij}\) for all pairs \(i,j\) and applying the chosen kernel \(k\) yields the _semantic RSM_. This _semantic RSM_ is invariant to any arbitrary, unique spatial permutation \(\mathbf{P}_{ij}\) for each pair of representations, and, depending on the choice of kernel \(k\), invariant to orthogonal transformations \(U\in\mathbb{R}^{C\times C}\) along the channel dimension. These _semantic RSMs_ can be used as a drop-in replacement for any other RSM, e.g. for applications such as calculating \(\text{CKA}(K,L)\) to measure the similarity between systems.

Computational ComplexityFinding the optimal permutation matrix \(\mathcal{P}_{ij}\) is NP-hard and needs to be repeated for each pair of representations \(z_{i},z_{j}\). With \(N\) samples, this results in \(\frac{N\cdot(N+1)}{2}\) unique permutations that need to be computed for a _semantic RSM_. The overall complexity of bipartite matching algorithms grows with the spatial dimensions cubed, resulting in \(\mathcal{O}(N^{2})\times\mathcal{O}(S^{3})\). The outer \(\mathcal{O}(N^{2})\) complexity can be parallelized, or reduced by decreasing the batch size. However, the inner permutation can become time-consuming, especially with large spatial dimensionality. To address this, we provide various approximations to reduce the complexity, which are detailed in Section 4.4. For all later experiments, except the translational toy example, we use the Batch-Optimal approximation with windows size \(b\) 512, with the batch referring to batches of semantic concept vectors \(\mathbf{v}\) and not samples. The pseudo-code for calculating _semantic RSMs_ is visualized in the Appendix under Algorithm 1.

## 4 Experiments: Semantic vs Spatio-Semantic RSMs

Given the novel permutation-invariant similarity definition, we evaluate the utility of our semantic RSMs relative to spatio-semantic RSMs for various similarity kernels, architectures, and tasks. Across all experiments we compare the linear kernel, the radial basis function (RBF) kernel, and the cosine similarity kernel, see Appendix A for details.

Figure 2: **Semantic RSMs capture similarity independent of spatial localization, in contrast to current spatio-semantic RSMs. We utilize Tiny-ImageNet to generate partially overlapping crops of the same sample (left) and calculate RSMs for a trained ResNet18 model. The plot displays the original spatio-semantic RSMs (middle top) and our proposed semantic RSMs (middle bottom) across various layers for a single batch. Additionally, the distribution of similarity values over multiple batches is shown (right). The results indicate that spatio-semantic RSMs struggle to detect largely identical but translated images, while semantic RSMs exhibit an enhanced off-diagonal in the RSMs and a significant gap between distributions. This demonstrates the capability of our method to detect the same semantics even when translated.**

### Translation sensitivity

To illustrate the problems of coupling semantic content and localization a toy dataset is created using \(84\times 84\) pixels large, downsampled images of ImageNet [2]. For each image two \(64\times 64\) crops are extracted, one from the upper-left and one from the lower-left corner, resulting in two images that share \(44\times 64\) identical pixels (Fig. 2 left). Ten upper-left and ten lower-left crops are then used to extract representations of a ResNet18 [7], which are subsequently used to calculate spatio-semantic and semantic RSMs at different layers of the architecture (Fig. 2 middle). As kernel, we use the radial basis function, as it provides bounded similarity values allowing a better visualization.

As expected, the _spatio-semantic_ RSM measures low similarity between pairs of overlapping crops, due to the semantic concept vectors not aligning. Only in the last layer, after many pooling operations, the off-diagonal is slightly expressed. Conversely, our _semantic_ RSM is capable of detecting the high semantic similarity of the partially overlapping crops throughout the entire depth of the architecture, as evident by the highly similar off-diagonal.

Aggregating the similarity values between partially-overlapping and between different images across multiple batches, allows us to measure the distribution of similarity values between overlapping crops, and non-related image comparisons. Throughout the entire depth of the architecture, the similarity distributions show that our measure better separates overlapping images from different images. Notably, the similarity distribution in _spatio-semantic_ RSMs shows a significant overlap of the distributions of partially overlapping images and non-related images, making differentiation between them difficult (Fig. 2 right). A similar toy experiment for a ViT-B/16 [4], is provided in Appendix B.

### Similarity-based retrieval

To test the impact of the _semantic_ RSMs in real-world applications, we now investigate the common task of image retrieval. Each entry in an RSM quantifies a sample-to-sample similarity value, which can be directly used for retrieval. While not specifically designed for it, we argue that better retrieval performance reflects a better inter-sample similarity. This allows us to quantify improvements in the RSM structure. To measure retrieval performance the EgoObjects dataset [35] is used. It contains frames of video that capture the same scene from different viewing perspectives and lighting conditions. This results in object centers being distributed across the extent of the image.

By randomly sampling 2000 query images and 5000 database images from the test set and using general feature extractors to extract embeddings from them we construct RSMs that allow us to do retrieval. As feature extractors we use CLIP (ViT/B32) [24], CLIPSeg (Rd64) [16], DinoV2-Giant [22], SAM (ViT/B32) [9] and BIT-50 [11] and as kernels for similarity calculation we use the cosine similarity, RBF and the inner product.

For all RSMs, we retrieve the most similar image that is not part of the same video - the same scene but different conditions are allowed. As multiple objects can be present in each scene,

Figure 3: **Relaxing the constraint of spatial alignment leads to better retrieval.** We leverage general feature extractors to embed images of the EgoObjects dataset. We then compare these embeddings either with or without permutation invariance. PI: Permutation Invariantwe quantify retrieval performance by the F1-Score, measuring the overlap of annotated objects between images. Due to the rather complex dataset, we elaborate this in more detail in Appendix D.1.

Across all architectures and metrics, the inclusion of permutation invariance (PI) for the similarity calculation improves retrieval performance relative to the non-invariant similarity, in some cases with a dramatic difference in performance, see Fig. 3. For models designed for dense downstream tasks like SAM or CLIPSeg, the retrieval performance changes particularly much, while models with more global reasoning, like BiT improve less, relatively.

Qualitative SimilarityAside from a quantitative comparison, we visualize the most similar retrieved images for two exemplary queries of SAM in Fig. 4 as case examples.

**Left Query**: The image displays various utensils scattered on a desk. When retrieving with the permutation-invariant similarity metric two images of the same scene but a very different perspective are successfully retrieved as most similar. Retrieving with the non-invariant similarity metric fails to retrieve similar images, due to lack of spatial alignment of the semantic concepts. Instead, it retrieves images of a whiteboard, possibly due to its spatial alignment with the paper on the desk.

**Right Query**: The image features a blender on a counter. The retrieval based on non-permutation-invariant similarity fails to retrieve any of the semantically similar scenes and returns images with a light switch, likely due to the spatial alignment of the light-switch-looking object to the right of the blender. Contrary, the retrieval based on permutation-invariant similarity correctly returns the blender in all cases from different perspectives. Additional qualitative examples are provided in Appendix D.3.

These experiments display clearly, that demanding spatial alignment can be a significant shortcoming when semantically similar concepts are misaligned. In Fig. 4, the network learned to represent the objects very similarly, despite a shift in perspective, but due to the same objects not aligning anymore, spatio-semantic similarity fails to recognize this. This effect should generalize to other datasets where objects are not heavily centered. For datasets with heavy object-centric behavior, like ImageNet, this should be less pronounced.

Figure 4: **Retrieving by permutation invariant similarity returns similar scenes of different spatial geometry. We visualize the top 3 most similar images according to two exemplary query images for SAM ViT/B32.**

### Output similarity vs Representational Similarity

While the retrieval experiments relate to a rather human notion of similarity, one can raise the question if semantic RSMs are also better at measuring the similarity for classifiers.

For each pair of samples, we can compare how similar the predicted class probabilities of a model are and compare this to the representational similarity. A commonly used metric for this is the Jensen-Shannon Divergence (JSD), which quantifies how dissimilar the two probability distributions are from one another. More details are provided in Appendix E.

Consequently, we use various classifiers trained to predict ImageNet1k from Huggingface and compare the Pearson correlation \(\rho\) between their JSD and the representational similarity of their last hidden layer. We chose to use the Pearson correlation, as it allows observing a direct linear behavior between representational similarity and predictive similarity. Again we measure semantic similarity and spatio-semantic similarity with different kernels. Due to JSD measuring dissimilarity, we want the correlation to be as negative as possible. As models we use multiple ResNets [7], ViTs [4], a fine-tuned DinoV2 [22] classifier from and a convnextv2[33] classifier.

The results, displayed in Table 1, show that for almost all architectures and kernels tested, the permutation invariant similarities are better at capturing the notion of what a classifier deems similar. While better than the spatio-semantic similarity, overall correlations are generally low, indicating that either, the similarity metric is confounded by irrelevant representations, or that the kernels should be improved. Moreover, the RBF kernel sometimes provides a positive correlation indicating it is unsuitable to predict the similarity of output probabilities, whereas the Cosine Similarity and the Inner Product both are consistently negative for all architectures tested.

### Optimizing runtime

Since we find the best possible permutation matrix through linear sum assignment algorithms that maximize the inner product of two samples, we can guarantee that the \(K_{ij,semantic}\geq K_{ij}\forall i,j\). This provides us with an upper bound of similarity that can be leveraged to measure how much of the maximally achievable semantic similarity was measured by the spatio-semantic similarity. Additionally, it can be used as a baseline to estimate the quality of permutation matrices \(\mathbf{P}_{ij}\) provided by faster, approximative assignment algorithms.

Decreasing computational complexityDetermining the optimal permutation between samples poses a substantial computational challenge with a complexity of \(\mathcal{O}(S^{3})\) for each of the \(\mathcal{O}(N^{2})\) pairs in the same mini-batch, particularly for early layers with large spatial resolution \(S\). Although, in theory, the calculation of the \(K\) matrix needs to be conducted only once for the desired representations, applying the method to representations with larger spatial extents becomes impractical with the demands of optimal matching.

To mitigate runtime, two options are available: reducing the batch size \(N\) to lessen the number of permutation calculations or decreasing the time spent on finding the permutation. Given that scenarios like image retrieval often desire larger batches, our focus is on minimizing the time required to obtain suitable assignments.

\begin{table}
\begin{tabular}{l|c c|c c|c c} \hline \hline \multirow{2}{*}{Architectures} & \multicolumn{6}{c}{Pearson Correlation \(\rho\)} \\  & \multicolumn{2}{c}{**Cosine Sim.**} & \multicolumn{2}{c}{**Inner Product**} & \multicolumn{2}{c}{**RBF**} \\  & - & **(ours)** PI & - & **(ours)** PI & - & **(ours)** PI \\ \hline ResNet18 & -0.276 & **-0.326** & -0.259 & **-0.270** & -0.176 & **-0.199** \\ ResNet50 & -0.248 & **-0.291** & -0.243 & **-0.261** & 0.040 & **0.029** \\ ResNet101 & -0.192 & **-0.276** & -0.174 & **-0.240** & 0.091 & **0.084** \\ ConvNextV2-Base & **-0.134** & -0.098 & -0.132 & **-0.171** & 0.117 & **0.090** \\ ViT-B/16 & -0.046 & **-0.100** & **-0.045** & -0.026 & -0.077 & **-0.122** \\ ViT-L/32 & -0.138 & **-0.188** & -0.138 & **-0.144** & -0.134 & **-0.166** \\ DinoV2-Giant & -0.012 & **-0.044** & -0.013 & **-0.031** & -0.008 & **-0.048** \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Similarity invariant to spatial permutations is better at predicting if the class probabilities will be similar.** PI: Permutation InvariantSolving the optimal bipartite matching between semantic concept vectors is equivalent to the well-known assignment problem [18; 1]. We attempted to find existing approximate algorithms for this purpose. Unfortunately, most established algorithms primarily focus on optimal solutions, and existing approximate algorithm implementations, such as those based on the auction algorithm [6], are not runtime-optimized, often taking longer than optimal algorithms in our experiments. To enhance computational efficiency nonetheless, we explored three tailored approximation algorithms:

* A Greedy breadth-first matching (**Greedy**)
* An optimal matching of the TopK values based on their Norm, followed by the Greedy algorithm for the remaining samples (**TopK-Greedy**)
* Optimal matching of smaller batches, with samples batched by their Norm (**Batch-Optimal**)

For explicit details on the approximation algorithms, we refer to Appendix F.

We conducted a comprehensive comparison between the approximate algorithms and the optimal algorithm. We compare their runtime per sample and the quality of matches, quantified by the average relative similarity \(\frac{k}{k_{optimal}}\). The evaluation utilized representations from a ResNet18 on TinyImageNet, as illustrated in Fig. 6.

It can be seen that the measured spatio-semantic similarity for TinyImageNet samples are, on average, 30% lower with layers of higher spatial resolution exceeding 40%. This suggests a notable misalignment of semantic concept vectors. Notably, the Batch-Optimal approximation stands out as a reliable approximation for optimal matching. The fastest of the Batch-Optimal approximation methods shows \(<8\%\) error while improving run-time \(\times 36\) relative to the fastest optimal algorithm for spatial extent \(4096\), while no spatial alignment shows \(42\%\) deviation from the optimal matching. Moreover, we highlight the time vs accuracy trade-off of the different optimal and approximate algorithms in Appendix F.1. Furthermore, it can be seen that the changes between spatio-semantic and semantic RSMs are anisotropic, as highlighted in Fig. 6, indicating scale invariant downstream applications may be influenced.

## 5 Discussion, Limitations, and Conclusion

The concept of Representational Similarity Matrices (RSMs) is a powerful tool to represent the similarity structure of complex systems. In this paper we revisit the construction of such RSMs for neural networks of the vision domain, question the current state, and propose _semantic RSMs_, warranting discussion.

Spatio-semantic couplingBeing aware that current, _spatio-semantic_ RSMs demand semantic concepts to be aligned is highly relevant to understand what RSMs are sensitive to. Previous work [32] identified this shortcoming and proposed translation invariance, partially addressing this issue. We argue translation invariance is insufficient, since models may learn invariances during training, which the translation invariant metric would not be sensitive to. Subsequently, we propose a new - spatially permutation invariant - similarity measure between samples that allows the detection of similarity whenever a model expresses similar semantic vectors in its representations, irrespective of spatial geometry. To highlight the benefits of our similarity, we propose that better similarity measures should allow more accurate retrieval when comparing last-layer representations and should allow better predictions about the similarity of class probabilities of a classifier. However, we acknowledge certain limitations in our current evaluation. Specifically, we have not yet compared our method to more established retrieval techniques. Traditional retrieval methods are often not applied to representations directly but utilize a lower-dimensional non-spatial, global vector representing the entire sample. In contrast, we chose to limit ourselves to methods that are directly applied to the representations.

Computational ComplexityAside from quantitative or qualitative benefits, the construction of semantic RSMs is time-consuming, limiting its applicability. This complexity mostly affects layers of large spatial extent, which mostly corresponds to early CNN layers while later layers and ViTs are unproblematic. Our proposed Batch-optimal approximation alleviates this partially, yet application to large-scale representations at higher resolution, like at the output of a segmentation architecture with spatial extents of s=65.536 would be too costly. We leave optimizing the compute efficiency or finding better approximations for future work.

ConclusionIn conclusion, our investigation into semantic RSMs has shed light on the limitations of spatio-semantic RSMs and introduced a novel approach to disentangle spatial alignment from semantic similarity. The proposed method provides a more accurate measure of how representations capture underlying semantic content, showcasing its potential in various applications, particularly in scenarios where spatial alignment cannot be assumed. While challenges such as computational complexity and scalability need to be addressed, the findings open avenues for further research and improvement in the analysis of neural network representations.

## References

* Burkard [1984] R. E. Burkard. Quadratic assignment problems. _European Journal of Operational Research_, 15(3):283-289, 1984.
* Deng et al. [2009] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* Ding et al. [2021] F. Ding, J.-S. Denain, and J. Steinhardt. Grounding representation similarity through statistical testing. _Advances in Neural Information Processing Systems_, 34:1556-1568, 2021.
* Dosovitskiy et al. [2020] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* Gretton et al. [2005] A. Gretton, R. Herbrich, A. Smola, O. Bousquet, B. Scholkopf, and A. Hyvarinen. Kernel methods for measuring independence. _Journal of Machine Learning Research_, 6(12), 2005.
* Guthe and Thuerck [2021] S. Guthe and D. Thuerck. Algorithm: A fast scalable solver for the dense linear (sum) assignment problem. _ACM Trans. Math. Softw._, 47(2), apr 2021. ISSN 0098-3500. doi: 10.1145/3442348. URL https://doi.org/10.1145/3442348.
* He et al. [2016] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2016.
* Jonker and Volgenant [2008] R. Jonker and T. Volgenant. A shortest augmenting path algorithm for dense and sparse linear assignment problems. In _DGOR/NSOR: Papers of the 16th Annual Meetingof DGOR in Cooperation with NSOR/Vortrage der 16. Jahrestagung der DGOR zusammen mit der NSOR_, pages 622-622. Springer, 1988.
* Kirillov et al. [2023] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, et al. Segment anything. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4015-4026, 2023.
* Klabunde et al. [2023] M. Klabunde, T. Schumacher, M. Strohmaier, and F. Lemmerich. Similarity of neural network models: A survey of functional and representational measures. _arXiv preprint arXiv:2305.06329_, 2023.
* Kolesnikov et al. [2020] A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly, and N. Houlsby. Big transfer (bit): General visual representation learning. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part V 16_, pages 491-507. Springer, 2020.
* Kornblith et al. [2019] S. Kornblith, M. Norouzi, H. Lee, and G. Hinton. Similarity of neural network representations revisited. In _36th International Conference on Machine Learning, ICML 2019_, volume 2019-June, pages 6156-6175, 2019. ISBN 9781510886988. URL https://arxiv.org/abs/1905.00414.
* Kriegeskorte et al. [2008] N. Kriegeskorte, M. Mur, and P. A. Bandettini. Representational similarity analysis-connecting the branches of systems neuroscience. _Frontiers in systems neuroscience_, page 4, 2008.
* Krizhevsky [2009] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
* Li et al. [2015] Y. Li, J. Yosinski, J. Clune, H. Lipson, and J. Hopcroft. Convergent learning: Do different neural networks learn the same representations? Technical report, 2015.
* Luddecke and Ecker [2022] T. Luddecke and A. Ecker. Image segmentation using text and image prompts. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 7086-7096, June 2022.
* Marcel and Rodriguez [2010] S. Marcel and Y. Rodriguez. Torchvision the machine-vision package of torch. In _Proceedings of the 18th ACM international conference on Multimedia_, pages 1485-1488, 2010.
* Martello and Toth [1987] S. Martello and P. Toth. Linear assignment problems. In _North-Holland Mathematics Studies_, volume 132, pages 259-282. Elsevier, 1987.
* Morcos et al. [2018] A. S. Morcos, M. Raghu, and S. Bengio. Insights on representational similarity in neural networks with canonical correlation. Technical report, 2018. URL https://arxiv.org/abs/1806.05759.
* Neyshabur et al. [2020] B. Neyshabur, H. Sedghi, and C. Zhang. What is being transferred in transfer learning? _Advances in neural information processing systems_, 33:512-523, 2020.
* Nguyen et al. [2020] T. Nguyen, M. Raghu, and S. Kornblith. Do wide and deep networks learn the same things? uncovering how neural network representations vary with width and depth. _arXiv preprint arXiv:2010.15327_, 2020.
* Oquab et al. [2023] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, et al. Dinov2: Learning robust visual features without supervision. _arXiv preprint arXiv:2304.07193_, 2023.
* Perron and Furnon [2017] L. Perron and V. Furnon. Or-tools. URL https://developers.google.com/optimization/.
* Radford et al. [2021] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* Raghu et al. [2017] M. Raghu, J. Gilmer, J. Yosinski, and J. Sohl-Dickstein. SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability. _Advances in Neural Information Processing Systems_, 2017-Decem:6077-6086, jun 2017. URL http://arxiv.org/abs/1706.05806.
* Raghu et al. [2021] M. Raghu, T. Unterthiner, S. Kornblith, C. Zhang, and A. Dosovitskiy. Do vision transformers see like convolutional neural networks? _Advances in neural information processing systems_, 34:12116-12128, 2021.

* Ramasesh et al. [2020] V. V. Ramasesh, E. Dyer, and M. Raghu. Anatomy of catastrophic forgetting: Hidden representations and task semantics. _arXiv preprint arXiv:2007.07400_, 2020.
* Song et al. [2012] L. Song, A. Smola, A. Gretton, J. Bedo, and K. Borgwardt. Feature selection via dependence maximization. _Journal of Machine Learning Research_, 13:1393-1434, 2012. ISSN 15324435.
* Sucholutsky et al. [2023] I. Sucholutsky, L. Mutenthaler, A. Weller, A. Peng, A. Bobu, B. Kim, B. C. Love, E. Grant, I. Groen, J. Achterberg, et al. Getting aligned on representational alignment. _arXiv preprint arXiv:2310.13018_, 2023.
* Virtanen et al. [2020] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett, J. Wilson, K. J. Millman, N. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, I. Polat, Y. Feng, E. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E. A. Quintero, C. R. Harris, A. M. Archibald, A. H. Ribeiro, F. Pedregosa, P. van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. _Nature Methods_, 17:261-272, 2020. doi: 10.1038/s41592-019-0686-2.
* Wang et al. [2018] L. Wang, L. Hu, J. Gu, Y. Wu, Z. Hu, K. He, and J. Hopcroft. Towards Understanding Learning Representations: To What Extent Do Different Neural Networks Learn the Same Representation. Technical report, 2018. URL https://arxiv.org/abs/1810.11750.
* Williams et al. [2021] A. H. Williams, E. Kunz, S. Kornblith, and S. Linderman. Generalized shape metrics on neural representations. _Advances in Neural Information Processing Systems_, 34:4738-4750, 2021.
* Woo et al. [2023] S. Woo, S. Debnath, R. Hu, X. Chen, Z. Liu, I. S. Kweon, and S. Xie. Convnext v2: Co-designing and scaling convnets with masked autoencoders. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16133-16142, 2023.
* Yosinski et al. [2015] J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lipson. Understanding neural networks through deep visualization. _arXiv preprint arXiv:1506.06579_, 2015.
* Zhu et al. [2023] C. Zhu, F. Xiao, A. Alvarado, Y. Babaei, J. Hu, H. El-Mohri, S. Culatana, R. Sumbaly, and Z. Yan. Egoobjects: A large-scale egocentric dataset for fine-grained object understanding. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 20110-20120, 2023.

Kernel function definitions

Defining the notion of similarity between two vectors is a matter of preference and wanted properties. In this work, we include the Radial Basis Function kernel Eq. (6) and the linear kernel Eq. (7), as well as the cosine similarity kernel Eq. (8). We include the former two as they were proposed for RSM construction by Kornblith et al. [12] and the cosine similarity due to its popularity in the retrieval domain, despite generally being applied to class probabilities.

We denote that in our manuscript we refer to the linear kernel as the inner product and dot product interchangeably as they correspond to the same operation.

\[k_{\text{RBF}}(\mathbf{x},\mathbf{y})=\exp\left(-\frac{\|\mathbf{x}-\mathbf{ y}\|^{2}}{2\sigma^{2}}\right)\] (6)

\[k_{\text{linear}}(\mathbf{x},\mathbf{y})=\langle\mathbf{x},\mathbf{y}\rangle\] (7)

\[k_{\text{cosine}}(\mathbf{x},\mathbf{y})=\frac{\langle\mathbf{x},\mathbf{y} \rangle}{\|\mathbf{x}\|\|\mathbf{y}\|}\] (8)

Different propertiesThe three selected kernels all have different properties: The RBF kernel and the Cosine similarity are bounded between \(k_{RBF}(x,y),k_{cosine}(x,y)\in[0,1]\quad\forall x,y\in\mathcal{R}\), while the \(k_{linear}(x,y)\) is not bounded.

Moreover, the RBF kernel is parametrized by \(\sigma\), which influences at which rate the distance between the representations results in a decrease in similarity. For all our experiments we choose \(\sigma\) as the square root of the median Euclidean distance of all distances within a mini-batch.

## Appendix B Translation Sensitivity of a ViT-B/16

Similarly to the translated Tiny-ImageNet experiment, we prepared a very similar experiment for a ViT-B/16 vision transformer that was pre-trained on ImageNet1k. We utilize the implementation and weights provided by torchvision [17].

Given the larger ImageNet images, we resample them to \(324\times 324\) pixels and crop two partially overlapping images of size \(224\times 224\) from it. Given these image pairs, we calculate semantic and spatio-semantic RSMs again, see Fig. 7.

It is important to note that our approach intentionally avoids achieving a perfect translation that would lead to the same patchified tokens, as we shift the image by a factor that is not divisible by 16, the patching window size. We believe this realistic imperfection is preferable to a perfect overlap, where identical tokens would be formed from the exact same set of pixels.

Similarly to the previous partially overlapping crop experiment, we can observe that spatio-semantic RSMs are incapable of identifying the largely identical content of the two partially overlapping crops due to their different localization. The spatio-semantic RSMs can capture this notion of similarity as evidenced by the off-diagonal and the larger gap in the distribution of similarity between partially overlapping samples and independent samples, Fig. 7 (bottom).

Contrary to the CNN example in the main, overall similarity between samples is much lower overall and separation between translated image pairs and random image pairs follows a vastly different trajectory. While there is a profound difference, the origin of this difference cannot be clearly made out. We hypothesize that this may be due to the different ways that Transformers process information and learn different representations, as highlighted in Raghu et al. [26].

Moreover, we emphasize, that calculating the optimal permutation is significantly faster for ViTs than the CNNs, as the early tokenization reduces the spatial dimension \(S\) substantially at an early stage, whereas the iterative downsampling of CNNs makes comparing representations of early layers very costly.

## Appendix C Pseudo Code

In addition to our provided explanation of the algorithm in the main manuscript, we provide the pseudo-code used to compute semantic RSMs in Algorithm 1. The only difference between semantic and spatio-semantic RSMs algorithmically is where the optimal permutation is calculated that maximally aligns the two representations. The current definition of spatio-semantic RSMs assumes that spatial locations are corresponding, while semantic RSMs calculate correspondence through similarity matching.

## Appendix D Additions: Retrieval Experiment

In addition to the provided retrieval examples in the main manuscript, we provide more details on the retrieval experiments in Appendix D.1, an additional table holding the quantitative data of Fig. 3 with varying database sizes in Appendix D.2 and lastly, additional qualitative retrieval examples for each model including direct comparisons of all models for the same query image in Appendix D.3.

### Details of Retrieval Experiment

For the retrieval experiment, we utilize the EgoObjects dataset [35]. It contains multiple frames from multiple videos, with multiple videos capturing the same scene under different shifts like lighting conditions, distances, viewing angles, and different motion trajectories. For each frame, multiple objects of different categories can be present and are annotated

Figure 7: **Semantic RSMs do also capture spatial translations for token sequences of ViT’s.** We calculate spatio-semantic and semantic RSMs with the Radial Basis Function (RBF) kernel for a ViT-B/16 with representations extracted from ImageNet. Similarly, we introduce partially-overlapping crops that get tokenized and processed by the ViT. Due to the initial shift, the token sequence does not align anymore between the crops. Similar to the CNNs, spatio-semantic RSMs exhibit low similarity values in the off-diagonals, providing a limited indication of overlapping content between crops. In contrast, Semantic RSMs prove notably more effective in discerning substantial overlap, offering higher similarity values in the off-diagonals and thereby indicating a greater degree of similarity between large portions of the image.

through a bounding box. Moreover, frames can vary in spatial resolution, yet a large fraction was captured in 16:9 format of \(1920\times 1028\) pixels.

Image preprocessingFor our experiments, we utilize the EgoObjects test set, which is comprised of 29.5K images. Of these 29.5k we remove all images not in 16:9 format and resize the remaining to the \(1920\times 1028\) format. This discards roughly 10k images.

Of all remaining images, we then draw 2k query images and 5k database images used for extracting embeddings for similarity calculation and later retrieval. Naturally, we sample in a way to keep the 2k query and 5k database image sets non-overlapping. When passing the images to the models for feature extraction each image is preprocessed according to the corresponding Huggingface ImageNetProcessor. This mostly represents resizing the image by the shortest edge to the expected image input dimensions and normalizing the image. The only exception is SAM, of which we use the official implementation, which handles feature extraction and embedding of the image itself.

Feature Extraction and preparationAs mentioned in the main manuscript we use

1. CLIP (ViT/B32) [24]
2. ClipSeg (Rd64) [16]
3. DinoV2-Giant [22]
4. SAM (ViT/B32) [9] and
5. BIT-50 [11]

as general feature extractors as they were trained on a vast amount of data.4.

Footnote 4: Last accessed on 22nd of May 2024

Of all models, we use the last hidden layer as image embeddings should they not per-default provide image embeddings as output. After extracting representations we calculate the mean from the database embeddings to zero-center all representations by, query and database representations alike.

RSM constructionGiven all 2000 query embeddings and 5000 database embeddings, we calculate the RSMs. To parallelize this process we mini-batch the representations into \(100\times 100\) pairs and populate the \(2000\times 5000\) matrix in this fashion. We denote that this proved to be necessary for models with large spatial embedding dimensions like SAM, starring \(64\times 64\) spatial extent. Moreover, we denote that, due to the RBF choosing its parameter based on the median of the measured values within one batch this patch-wise calculation is not optimal for this kernel. The inner product and the cosine similarity kernels are not affected by this.

Retrieval measurementGiven the RSMs containing a sample-to-sample similarity measure, we can retrieve the most similar sample of the database for each query from it.

As each image can contain multiple objects measuring retrieval performance is not trivial. For both images we quantify how many objects of each class are present in the image, resulting in a count of class instances for each image. With the query image representing the ground truth (GT) and the database representing the prediction, we match class instance counts. Each correctly matched GT instance represents a TP, each missed an FN and all unmatched database instances represent an FP.

To formalize: Let \(Q_{c}\) be the number of instances of class \(c\) in the query image and \(D_{c}\) be the number of instances of class \(c\) in the database image. With this, the used F1 metric can be expressed as

\[TP =\sum_{c\in C}\min(Q_{c},D_{c})\] (9) \[FN =\sum_{c\in C}\max(0,Q_{c}-D_{c})\] (10) \[FP =\sum_{c\in C}FP_{c}=\sum_{c\in C}\max(0,D_{c}-Q_{c})\] (11) \[F1 =\frac{2\cdot TP}{2\cdot TP+FP+FN}\] (12)

### Additional Quantitative Retrieval data

In addition to the results highlighted in Fig. 3 we provide retrieval results for varying database sizes to retrieve from EgoObjects. Specifically, results for database sizes of 2.5k, 5k, and 10k samples are given in Table 2.

[MISSING_PAGE_FAIL:17]

Figure 8: **Additional qualitative retrieval samples for DinoV2.** We visualize the top 5 most similar neighbors for four query images.

Figure 9: **Additional qualitative retrieval samples for CLIP. We visualize the top 5 most similar neighbors for four query images.**

Figure 10: **Additional qualitative retrieval samples for BiT-50.** We visualize the top 5 most similar neighbors for four query images.

Figure 11: **Additional qualitative retrieval samples for CLIPSeg.** We visualize the top 5 most similar neighbors for four query images.

Figure 12: **Additional qualitative retrieval samples for SAM.** We visualize the top 5 most similar neighbors for four query images.

Figure 13: **Direct comparison of models.** We visualize the top 5 most similar images of all models retrieved through cosine similarity or permutation invariant cosine similarity.

Figure 14: **Direct comparison of models.** We visualize the top 5 most similar images of all models retrieved through cosine similarity or permutation invariant cosine similarity.

Due to the lack of instance labels in the CityScapes dataset, we can't take the quantity of objects into account as before, but can only consider if a semantic class is present or absent. As apparent from Table 3 permutation invariance allows for consistently improved retrieval when utilizing cosine similarity or RBF kernels.

## Appendix E Details: Output similarity vs Representational Similarity

To measure the correlation between the inter-sample representational similarity and the prediction probability inter-sample similarity we utilize pre-trained classifiers and the ImageNet1k [2] dataset. Unlike during the retrieval results this constraints the possible model selections to models trained for classification.

Image preprocessingTest set images of ImageNet1k are randomly sampled without applying any filtering to them. In total, we utilize a subset of 2k ImageNet test set samples in this experiment. This may appear small, yet provides a sufficient basis as the combinatoric growth increases the absolute number of measurements substantially.

Feature and logit extraction and preparationAs mentioned in the main manuscript we use

1. ResNet18 [7]
2. ResNet50 [7]
3. ResNet101 [7]
4. a DinoV2-Giant based classifier [22]
5. ConvNeXt V2 [33]
6. ViT-B/16 [4] and
7. ViT-L/32 [4] and

as pre-trained classifiers for predicting the ImageNet1k classes.5. For each sample, we extract the last hidden layer's representations and center them analog to before. For the same sample, we extract the logits and obtain the probability distribution through the softmax, saving the pair for later comparisons.

Footnote 5: Last accessed on 22nd of May 2024

Correlation measurementFor each pair of representations and probabilities, we calculate the similarities between their representations for all three kernel functions, once permutation invariant and once not. Additionally, we calculate the Jensen-Shannon Divergence (JSD)

\begin{table}
\begin{tabular}{l|c c|c c|c c} \hline \hline  & \multicolumn{2}{c|}{**Cosine Sim.**} & \multicolumn{2}{c}{**Inner Product**} & \multicolumn{2}{c}{**RBF**} \\ Invariance & - & (ours) PI & - & (ours) PI & - & (ours) PI \\ Architectures & & & & & & \\ \hline CLIPSeg & 0.662 & **0.694** & **0.664** & 0.654 & 0.664 & **0.696** \\ DinoV2-Giant & 0.689 & **0.700** & 0.686 & **0.696** & 0.686 & **0.690** \\ BiT-50 & 0.679 & **0.687** & 0.677 & 0.677 & 0.684 & **0.687** \\ CLIP & 0.691 & **0.701** & 0.693 & **0.701** & 0.678 & **0.687** \\ SAM ViT/B32 & 0.690 & **0.702** & **0.687** & 0.677 & 0.679 & **0.687** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Retrieval results for the Cityscapes Dataset. We retrieve the most similar image according to RSMs and calculate the IoU of query semantic classes and retrieved semantic classes. Overall query images used are validation images of size N=500 and the database are the training images of size N=2975. Differences between metrics are low, due to many images containing a large number of classes and the lack of instance label information. Despite this, permutation invariance improves Cosine Sim and RBF retrieval performance consistently, with the Inner Product showing mixed results.

between the predicted class probability distributions for \(P\) and \(Q\). A formal definition of the JSD is provided in Eq. (13).

\[\text{JSD}(P\parallel Q)=\frac{1}{2}D_{\text{KL}}(P\parallel M)+\frac{1}{2}D_{ \text{KL}}(Q\parallel M)\] (13)

where \(M\) is the pointwise mean of \(P\) and \(Q\):

\[M=\frac{1}{2}(P+Q)\] (14)

and \(D_{\text{KL}}\) is the Kullback-Leibler divergence defined as:

\[D_{\text{KL}}(P\parallel Q)=\sum_{i}P(i)\log\frac{P(i)}{Q(i)}\] (15)

Given the paired JSD and Similarity \(K\) between all samples \(i,j\) we utilize the Pearson correlation \(\rho\) to calculate the correlation between the two. Due to the JSD being 0 for identical probabilities and increasing for more dissimilar values and the Similarity being 1 for perfectly similar representations and 0 for dissimilar representations, the desired correlation between the two should be negative.

### Additional correlation results

In addition to the Pearson correlation between the Jensen-Shannon-Divergence (JSD) and inter-image similarity, we also present the results of their relationship measured by the Spearman correlation, as shown in Table 4.

While the Pearson correlation demonstrated consistently stronger correlations with cosine similarity, the inner product, and the RBF kernel, the Spearman rank correlations are less stable across these methods.

For ResNets, we observe a significant decline in correlation consistency and strength with the exception of ResNet18. Opposed to this, ViTs display notably higher negative correlation values when using cosine similarity and radial basis function kernels, in contrast to the Pearson correlation results.

## Appendix F Details of the Approximation Algorithms

The computational complexity of determining optimal matchings using the Jonker-Volgenant algorithm [8] scales significantly with \(\mathcal{O}(s^{3})\), resulting in substantial computation time for input-patch sizes with spatial dimensions \(s=64^{2}\). To address this challenge, we propose alternative approximate algorithms with reduced computational complexity. In all our approximations, we take advantage of additional information, specifically the L2-norm \(\|v_{i}\|_{2}\) of each semantic concept vector. We assume that achieving a high degree of matching involves pairing vectors with the highest norms and high cosine similarity. This assumption guides our design of more efficient matching algorithms.

\begin{table}
\begin{tabular}{l|c c|c c|c c|c c|c c|c c} \hline \hline Metric & \multicolumn{4}{c}{Pearson Correlation} & \multicolumn{4}{c}{Spearman’s Rank Correlation} \\ Kernel & \multicolumn{2}{c}{**Cosine Sim.**} & \multicolumn{2}{c}{**Inner Product**} & \multicolumn{2}{c}{**RBF**} & \multicolumn{2}{c}{**Cosine Sim.**} & \multicolumn{2}{c}{**Inner Product**} & \multicolumn{2}{c}{**RBF**} \\ Invariance & - & Pl & - & Pl & - & Pl & - & Pl & - & Pl & - & Pl \\ Architecture & & & & & & & & & & & & \\ \hline ResNet18 & -0.279 & **-0.328** & -0.264 & **-0.272** & -0.174 & **-0.197** & -0.231 & **-0.337** & **-0.239** & -0.225 & -0.435 & **-0.476** \\ ResNet50 & -0.256 & **-0.305** & -0.249 & **-0.269** & 0.028 & **0.015** & **-0.032** & 0.007 & **-0.046** & -0.040 & **0.128** & 0.132 \\ ResNet101 & -0.235 & **-0.330** & -0.211 & **-0.274** & 0.076 & **0.067** & -0.007 & **-0.053** & -0.007 & **-0.077** & 0.071 & **0.068** \\ Conv-NeuralV2-Base & **-0.162** & -0.126 & -0.160 & **-0.184** & 0.077 & **0.050** & **-0.01. **Greedy:** The simplest approach we employ is breadth-first matching. We determine the order in which to match \(v_{i}\) by considering the L2-norm \(\|v_{i}\|_{2}\) in descending order. We then match the current \(v_{i}\) with the best, non-assigned \(v_{j}\) based on \(A_{ij}\). The sorting complexity is \(\mathcal{O}(s\log(s))\), making this the fastest approximate algorithm among those tested.
2. **TopK-Greedy:** Recognizing that the TopK norm concept vectors \(\|v_{i}\|_{2}\) might have a significantly higher impact on the final similarity, we attempt to find the optimal matching for only the highest TopK norm concept vectors \(v_{i}\) and \(v_{j}\). The remaining lower norm concept vectors are assigned using the Greedy algorithm as described above. The process involves an initial sorting based on the semantic concept vectors' L2-norm, followed by optimal matching with \(\mathcal{O}(k^{3})\) complexity for the \(k\) TopK values and the greedy matching for the remaining values.
3. **Batch-Optimal:** If the TopK norm concept vectors do not sufficiently approximate an optimal matching, we apply optimal matching for the remaining concept vectors in batches. To achieve this, we create \(s/b\) smaller batches, with semantic concept vectors assigned to batches according to their L2-norm. All values within a batch are then optimally matched, leading to a matching complexity of \((\frac{s}{b})\cdot\mathcal{O}(b^{3})\).6 Footnote 6: There is an error in the current version in the main regarding the square root of \(s\), which will be corrected in a revision. We apologize for any confusion

Evaluating the various approximations, we observe that the Greedy matching yields suboptimal approximation quality and offers marginal to no improvement over the current same-position assignment. Although we do not present the details of the greedy matching, it is important to highlight that it is guaranteed to be worse or equal to the TopK-Greedy matching with a \(k\) value of 128, as shown in Fig. 5. We include the Greedy algorithm for completeness as a simple baseline.

Furthermore, it is noteworthy that the TopK-Greedy matching demonstrates that exclusively matching the largest norm concept vectors is insufficient for a good approximation of the optimal matching. This insight suggests that a substantial portion of the overall similarity is contributed by semantic concept vectors not included in the set of highest norms.

Lastly, we observe that the Batch-Optimal approximation, using a small batch size of 128 samples, provides an approximation with less than \(10\%\) error compared to the optimal matching. This result underscores the effectiveness of our batching approach based on the L2-norm of the concept vectors. It offers a reliable estimate for overall similarity, simplifying the matching process significantly.

### Runtime Evaluation

In order to assess algorithm performance across different spatial resolutions, we conducted a benchmarking study. For each resolution, we randomly selected 10,000 pairs of samples from a ResNet101 trained on Tiny-ImageNet. Affinity matrices (**A\(ij\)**) were pre-computed to facilitate permutation (**P\(ij\)**) calculations. The average time taken per matching was then reported for the same single CPU core, as outlined in Table 5. We observe that the OR-Tools implementation outperforms other alternatives, being four times faster than the lapjv implementation 7. However, even this optimal approach requires 1.52 seconds per pair on a \(64\times 64\) resolution. Despite the potential for parallelizing sample-wise matching, optimal algorithms face scalability challenges with larger spatial dimensions \(S\). In contrast, the Batch-Optimal approximation offers a compelling balance between computation time and approximation quality. Importantly, its complexity scales linearly with \(S\) due to the fixed batch size.

Footnote 7: Implementation on Github https://github.com/src-d/lapjv

## Appendix G Semantic RSMs and CKA - Qualitative changes

Building upon the success of Linear/RBF (_spatio-semantic_) CKA to compare systems, we provide some preliminary qualitative comparisons gauging how CKA comparisons are affected by our differently proposed RSM. Unfortunately, hardly any quantitative benchmarkexists to quantify if a representational similarity metric is _better_ than another. Previously Kornblith et al. [12] evaluated CKA by showing it was better at finding layers of the same architecture than SVCCA and PWCCA. This was extended by Ding et al. [3] through the inclusion of statistical testing but remains a rather shallow benchmark. Subsequently, we constrain ourselves to qualitative experiments, leaving quantitative testing to potential future benchmarks.

In all following experiments, all representations are extracted globally and zero-centered along the sample dimension. Given the zero-centered representations spatio-semantic and semantic RSMs are computed in mini-batches of 250 samples. To calculate the semantic RSMs on CIFAR an optimal bipartite matching algorithm is used, while for Tiny-ImageNet and ImageNet we utilize the _Batch-Optimal_ approximation with window size \(b\) 512.

### CKA between semantic and spatio-semantic RSMs

As initial inspection, we evaluate how different the similarity structures of a model measured through spatio-semantic RSMs are to a model measured through semantic RSMs. We do this by constructing both RSMs **from the same representations** of a model. Subsequently, we compare the two alternative RSMs of the same representations to each other through CKA Eq. (2). The diagonal of this matrix represents a direct comparison of identical representations, just with another definition of what is considered "similar". This is evaluated for three ResNet18s and three ResNet34s on Tiny-ImageNet with the linear and RBF kernels respectively.

We display the average CKA matrices across architecture seeds for both architectures, as well as the diagonal values of the CKA matrix. Results are shown in Fig. 15.

Examining these CKA matrices multiple observations can be made:

A) Despite the diagonal representing a comparison between identical representations, the CKA values are not 1. This indicates that the different way of constructing RSMs changes the perceived similarity structure of the system, as measured by CKA.

B) Inspecting the diagonal shows, that earlier layers with greater spatial extent express higher differences in similarity, whereas layers at a later layer and lower spatial extent are less dissimilar. This is consistent with the expectation that, with shrinking spatial extent, alignment of semantic concepts gets more likely.

Given these large changes in CKA similarity, we conclude that the definition of what a model perceives as _similar_ can highly influence inter-system similarity. This is especially relevant when comparing systems across domains, where RSM construction may be domain-specific, disallowing to be consistent with RSM construction. Exemplary when comparing ML vision systems to human vision models, in particular when comparing representations of high spatial extent.

\begin{table}
\begin{tabular}{l l l|c c|c c c} \hline \hline \multirow{2}{*}{Category} & \multirow{2}{*}{Complexity} & \multirow{2}{*}{Matching Algo} & \multirow{2}{*}{Batch \(b\)} & \multicolumn{2}{c}{\(s=256\)} & \multicolumn{2}{c}{\(s=1024\)} & \multicolumn{2}{c}{\(s=4096\)} \\  & & & \(\frac{k}{k_{\text{rot}}}\) [\%] & \multicolumn{1}{c}{T} & \(\frac{k}{k_{\text{rot}}}\) [\%] & \multicolumn{1}{c}{T} & \(\frac{k}{k_{\text{rot}}}\) [\%] & \multicolumn{1}{c}{T} \\ \hline \multirow{2}{*}{Optimal} & \multirow{2}{*}{\(\mathcal{O}(s^{3})\)} & OR-Tools [23] & - & 100 & 4.26 ms & 100 & 69.6 ms & 100 & 1.52 s \\  & & Scipy [30] & - & 100 & 5.10 ms & 100 & 148 ms & 100 & 8.75 s \\  & & lapyv & - & 100 & 5.73ms & 100 & 97.5 ms & 100 & 6.12 s \\ \hline No Match & \(\mathcal{O}(1)\) & \(np\_diag()\) & - & 68.9 & 14.09 & 67.0 & 2.49 s & 57.9 & 4.36 \(\mu\)s \\ \hline \multirow{3}{*}{Approximation} & \multirow{3}{*}{\((\sqrt{s}/b)\cdot\mathcal{O}(b^{3})\)} & \multirow{3}{*}{Batch-Optimal} & 128 & 97.6 & 2.41 ms & 94.5 & 90.8 ms & 92.8 & 43.48 ms \\  & & & 256 & 100 & 4.56 ms & 96.8 & 16.9 ms & 95.0 & 95.9 ms \\ \cline{1-1}  & & & 512 & 100 & 4.57 ms & 98.6 & 37.1 ms & 96.8 & 171 ms \\ \cline{1-1}  & & & 1024 & 100 & 4.57 ms & 100 & 79.5 ms & 98.2 & 391 ms \\ \hline \hline \end{tabular}
\end{table}
Table 5: We compare different implementations of the optimal Jonker-Volgenant algorithm [8] against our linearly scaling Batch-Optimal approximation and no matching. The table presents average run-times per pair (\(T\)) and average similarity relative to the optimal value \(\{\frac{k}{k_{\text{rot}}}\}\) for 1,000 randomly chosen image pairs. Utilizing representations of varying sizes from a ResNet101 trained on Tiny-ImageNet, the optimal solutions are reported relative to the maximum similarity achieved by the optimal algorithms. The Batch-Optimal approximation demonstrates a substantial fraction of optimal matching performance with significantly improved scaling.

### Differences in CKA self-similarity

In the previous paragraph, semantic RSMs were directly compared to spatio-semantic RSMs. While this can influence measurements when models are compared across domains (e.g. CNN Vision to Biological Vision), it does not need to imply that the CKA similarity of vision models changes substantially.

Given that within the same domain, the RSM construction will likely be chosen consistently, one can opt to either: Calculate only spatio-semantic RSMs or only semantic RSMs, due to personal opinions or preferences. Subsequently, the question is not _"Are spatio-semantic RSMs similar to semantic RSMs"_, but _"Does the CKA similarity between spatio-semantic RSMs change when calculating semantic RSMs"_.

To address this question we: A) Compare the CKA matrix difference of the CKA matrix based on semantic RSMs to the CKA matrix of spatio-semantic RSMs when comparing a model with itself (Intra Model) and B) when comparing between different models (Cross-Model).

Intra-Model CKASimilarly to before, we extract representations and form semantic and spatio-semantic RSMs. We extract representations on CIFAR100 [14] (\(32\times 32\)), Tiny-ImageNet (\(64\times 64\)) and ImageNet-1k [2] (\(160\times 160\)) datasets, from 3 differently seeded and trained from scratch ResNet18, ResNet34 and ResNet101 architectures. The semantic RSMs are calculated utilizing the \(Batch-Optimal\) matching with \(b\) 512 for matching on Tiny-ImageNet and ImageNet. We calculate semantic and spatio-semantic RSMs with a mini-batch size of 250, subsequently using them for Canonical Correlation Analysis (CKA) calculations. The corresponding cka matrices and their differences are displayed in Fig. 16. If not further specified the experiment uses the linear kernel.

Introspecting the results it can be seen that across all Architectures ResNet18, ResNet34, and ResNet101 largely the same change in similarity structure can be observed. For CIFAR100, the very first layers are perceived as less similar to semantic RSMs than with spatio-semantic RSMs, while the CKA between the middle to later layers is more similar. This structure, though does not remain consistent across datasets: When moving from CIFAR100 to Tiny-ImageNet earlier layers appear to become more similar while intermediate layers become less so. On ImageNet1k CKA on semantic RSMs seem to indicate models are more similar. This trend indicates that the influence of semantic RSMs on spatio-semantic RSMs seems to be largely dataset-dependent. Moreover, the overall maximum change in CKA similarity in these matrices is between \(-0.2\) and \(+0.2\) for Tiny-ImageNet, indicating a modest change in overall CKA.

Cross-Model CKAAside from evaluating only CKA similarity of RSMs of the same model we extend to comparing RSMs between models, as commonly done when comparing models through CKA. ResNet18/101 models trained on CIFAR100 are used with RSMs constructed identically to previously specified. Results are displayed in Fig. 17.

Figure 15: **The dissimilarity between _semantic_ and _spatio-semantic_ RSMs decreases with shrinking spatial extent.** Comparison of _semantic_ and _spatio-semantic_ Representational Similarity Matrices (RSMs) for the same model. The dissimilarity in early layers decreases with decreasing spatial extent, as illustrated by Centered Kernel Alignment (CKA) values. The left and middle panel shows the CKA comparison between all layers, while the right panels visualize the heatmap’s diagonal, emphasizing the evolving similarity trend from early to late layers.

It can be seen that for both, ResNet18 and ResNet101, the cross-model CKA similarity is mostly negative for the majority of the layers, indicating that CKA on spatio-semantic RSMs estimates models to be more similar than when applying CKA on semantic RSMs. Similarly to before CKA changes range from \(-0.175\) to \(+0.025\) providing modest changes.

Concluding the Intra and Cross-Model CKA experiments it can be seen that the choice of RSMs results in qualitatively different CKA matrices. Unfortunately, due to the lack of quantitative benchmarks, no direct recommendation of which RSM to use for inter-model similarity calculation through CKA can be given.

Figure 16: CKA similarity between different Layers of _the same_ ResNet18, ResNet34, ResNet101, on CIFAR100, Tiny-ImageNet and ImageNet-1k. For each row the left-most CKA matrix displays the spatio-semantic RSMs, the middle represents the semantic RSMs while the right represents the difference in CKA similarity between the two. Blue regions indicate where the We observe that similarity within the block structure is largely unchanged, whereas the similarity across the later blocks seems to be more similar and the similarity of the very first blocks is less similar.

Figure 17: CKA similarity between different Layers of _different_ ResNet18 and ResNet101 models on CIFAR100. We observe a decrease in similarity at high-resolution layers, whereas similarity between deeper layers is largely unchanged.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer:[Yes] Justification: The experiments in Section 4 demonstrate the advantage of introducing permutation invariance for RSMs and directly correspond to the contributions listed in Section 1. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In Section 5, a comprehensive discussion of the results and their limitations, such as the runtime, is provided. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

[MISSING_PAGE_FAIL:32]

3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Code will be provided by acceptance. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The information provided in Section 4 provide all necessary information to understand the results. Appendix F, Appendix D.1 and Algorithm 1 add additional details that are needed to reproduce the exact experimental setting but not necessarily to understand the results. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.

7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: The paper introduces a new concept that has no direct related methods to compare with. Therefore, no statistical analysis or ranking methods are needed. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The presented work does not rely on specific compute requirements. However, in Appendix F.1 we compare the runtime of multiple approximation algorithms to solve the assignment problem. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The authors have strictly adhered to the ethical guidelines. Guidelines: ** The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).

10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: We do not see any direct societal impact. The work only provides a new introspection into the concept of semantic similarity of networks. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: See broader impact. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.

* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All relevant and previous work is cited and only open-source assets have been used. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The codebase will be published under a CC BY license. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No crowdsourcing experiments or research with human subjects were performed. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: No crowdsourcing experiments or research with human subjects were performed. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.