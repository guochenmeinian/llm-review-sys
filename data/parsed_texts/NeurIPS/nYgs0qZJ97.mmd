# Regret Matching\({}^{+}\): (In)Stability and Fast Convergence in Games

 Gabriele Farina

MIT

gfarina@mit.edu

&Julien Grand-Clement

ISOM, HEC Paris

grand-clement@hec.fr

&Christian Kroer

IEOR, Columbia University

christian.kroer@columbia.edu

&Chung-Wei Lee

Department of Computer Science

University of Southern California

leechung@usc.edu

&Haipeng Luo

Department of Computer Science

University of Southern California

haipeng1@usc.edu

###### Abstract

Regret Matching\({}^{+}\) (RM\({}^{+}\)) and its variants are important algorithms for solving large-scale games [35]. However, a theoretical understanding of their success in practice is still a mystery. Moreover, recent advances [34] on fast convergence in games are limited to no-regret algorithms such as online mirror descent, which satisfy _stability_. In this paper, we first give counterexamples showing that RM\({}^{+}\) and its predictive version [12] can be unstable, which might cause other players to suffer large regret. We then provide two fixes: restarting and chopping off the positive orthant that RM\({}^{+}\) operates in. Combined with RM\({}^{+}\) with predictions, we show that restarting is sufficient to get \(O(T^{1/4})\) individual regret and that chopping off achieves \(O(1)\) social regret in normal-form games. We also apply our stabilizing techniques to clativyoural updates in the uncoupled learning setting for RM\({}^{+}\), introduced _Extragradient RM\({}^{+}\)_, and prove desirable results akin to recent works for Clairvoyant online mirror descent [31, 14]. Our experiments show the advantages of our algorithms over vanilla RM\({}^{+}\)-based algorithms in matrix and extensive-form games.

## 1 Introduction

Regret minimization is an important framework for solving games. Its connection to game theory provides a practically efficient way to approximate game-theoretic equilibria [16, 19]. Moreover, it provides a scaleable way to solve large-scale sequential games, for example using the _Counterfactual Regret Minimization_ (CFR) decomposition [37]. Consequently, regret minimization algorithms are a central component in recent superhuman poker AIs [2, 28, 3]. _Regret Matching\({}^{+}\) (RM\({}^{+}\))_[35] is the most prevalent regret minimizer in these applications. In theory, it guarantees an \(O(1/\sqrt{T})\) convergence rate after \(T\) iterations, but its practical performance is usually significantly faster.

On the other hand, a line of recent works show that regret minimizers based on follow the regularized leader (FTRL) or online mirror descent (OMD) enjoy faster convergence rates in theory when combined with the concept of optimism/predictiveness [32, 34]. The result was originally provenin matrix games [32], and later extended to multiplayer normal-form games [34; 6; 7], extensive-form games [8; 10; 15; 1], and general convex games [22; 13]. However, despite their favorable properties in theory, optimistic algorithms based on FTRL/OMD are usually numerically inferior to \(\text{RM}^{+}\) when applied to solving large-scale sequential games. It remains a mystery whether some optimistic variant of \(\text{RM}^{+}\) enjoys a theoretically faster convergence rate, considering the strong empirical performance of \(\text{RM}^{+}\). It is also an open question whether there exists an algorithm that has both favorable theoretical guarantees similar to FTRL/OMD algorithms and practical performance comparable to \(\text{RM}^{+}\). Inspired by recent work on the connection between OMD and \(\text{RM}^{+}\)[12], we provide new insights on the theoretical and empirical behavior of \(\text{RM}^{+}\)-based algorithms, and we show that the analysis of fast convergence for OMD can be extended to \(\text{RM}^{+}\) with some simple modifications to the algorithm. Specifically, our main contributions can be summarized as follows.

1. We provide a detailed theoretical and empirical analysis of the potential for slow performance of \(\text{RM}^{+}\) and predictive \(\text{RM}^{+}\). We start by showing that, in stark contrast to FTRL/OMD algorithms that are stable inherently, there exist loss sequences that make \(\text{RM}^{+}\) and its variants unstable, leading to cycling between very different strategies. The key reason for such instability is that the decisions of these algorithms are chosen by normalizing an _aggregate payoff vector_; thus, in a region close to the origin, two consecutive aggregate payoffs may point in very different directions, despite being close, resulting in unstable iterations. Surprisingly, note that this can only happen when the aggregate payoff vectors, which essentially measure the algorithm's regret against each action, are small, so instability can only happen when one's regret is small and thus is seemingly not an issue. However, in a game setting, such instability might cause other players to suffer large regret because they have to learn in an unpredictable environment. Indeed, we identify a \(3\times 3\) matrix game where this is the case and both \(\text{RM}^{+}\) and predictive \(\text{RM}^{+}\) converge slowly at a rate of \(O(1/\sqrt{T})\) (Fig. 1). We emphasize that very little is known about the properties of (predictive) \(\text{RM}^{+}\) and we are the first to show concrete examples of stability issues in matrix games and in the adversarial setting.
2. Motivated by our counterexamples, we propose two methods to stabilize \(\text{RM}^{+}\): _restarting_, which reinitializes the algorithms when the aggregate payoffs are all below a threshold, and _chopping off_ the origin from the nonnegative orthant to smooth the algorithms. When applying these techniques to online learning with \(\text{RM}^{+}\), we show improved regret and fast convergence similar to predictive OMD: we obtain \(O(T^{1/4})\) individual regrets for _Stable Predictive \(\text{RM}^{+}\)_ (which uses restarting) and \(O(1)\) social regret for _Smooth Predictive \(\text{RM}^{+}\)_ (which chops off the origin). We also consider _conceptual prox_ and _extragradient_ versions of \(\text{RM}^{+}\) for normal-form games. We show that our stabilizing ideas also provide the required stability in these settings and thus give strong theoretical guarantees: Conceptual \(\text{RM}^{+}\) achieves \(O(1)\) individual regrets (Theorem 5.3) while Extragradient \(\text{RM}^{+}\) achieves \(O(1)\) social regret (Theorem 5.6). See Table 1 for a summary of our results for normal-form games. We further extend Conceptual \(\text{RM}^{+}\) to extensive-form games (EFG), yielding \(O(1)\) regret in \(T\) iterations with \(O(T\log(T))\) gradient computation. The key step here is to show the Lipschitzness of the CFR decomposition (Lemma J.1).
3. We apply our algorithms to solve matrix games and EFGs. For the \(3\times 3\) matrix game instability counterexample, our algorithms indeed perform significantly better than (predictive)

\begin{table}
\begin{tabular}{l c} \hline
**Algorithms** & **Social regret in multi-player NFGs** \\ \hline \hline \(\text{RM}^{+}\)[19] & \(O\left(T^{1/2}\right)\) \\ Predictive \(\text{RM}^{+}\)[10] & \(O\left(T^{1/2}\right)\) \\ Stable Predictive \(\text{RM}^{+}\) (Alg. 1) & \(O\left(T^{1/4}\right)\) \\ Smooth Predictive \(\text{RM}^{+}\) (Alg. 2) & \(O(1)\) \\ Conceptual \(\text{RM}^{+}\) (Alg. 3 ) & \(O(1)\) \\ Approximate Conceptual \(\text{RM}^{+}\) (Alg. 4 with \(k=\log(T)\)) & \(O(1)\) \\ Extragradient \(\text{RM}^{+}\) (Alg. 5) & \(O(1)\) \\ \hline \end{tabular}
\end{table}
Table 1: Summary of regret guarantees for the algorithms studied in this paper. The constants hidden in the \(O(\cdot)\) notations depends on initialization and the dimensions of the games and are given in our theorems.

RM\({}^{+}\). For random matrix games, we find that Stable and Smooth Predictive RM\({}^{+}\) have very strong empirical performance, on par with (unstabilized) Predictive RM\({}^{+}\), and greatly outperforming RM\({}^{+}\) in all our experiments; Extragradient RM\({}^{+}\) appears to be more sensitive to the choice of step sizes and sometimes performs only as well as RM\({}^{+}\). Our experiments on \(4\) different EFGs show that our implementation of clairvoyant CFR outperforms predictive CFR in some, but not all, instances.

## 2 Preliminaries

**Notations.** For \(d\in\mathbb{N}\), we write \(\mathbf{1}_{d}\in\mathbb{R}^{d}\) the vector with \(1\) on every component. The simplex of dimension \(d-1\) is \(\Delta^{d}=\{\bm{x}\in\mathbb{R}_{+}^{d}\mid\left\langle\bm{x},\mathbf{1}_{d} \right\rangle=1\}\). The vector \(\mathbf{0}\) has \(0\) on every component and its dimension is implicit. For \(x\in\mathbb{R}\), we write \([x]^{+}\) for the positive part of \(x:[x]^{+}=\max\{0,x\}\), and we overload this notation to vectors component-wise. For two vectors \(\bm{a}\) and \(\bm{b}\), \(\bm{a}\geq\bm{b}\) means \(\bm{a}\) is at least \(\bm{b}\) component-wise. We write \(\|\cdot\|_{*}\) for the dual norm of a norm \(\|\cdot\|\).

**Online Linear Minimization.** In online linear minimization, at every decision period \(t\geq 1\), an algorithm chooses a decision \(\bm{x}^{t}\) from a convex decision set \(\mathcal{X}\). A loss vector \(\bm{\ell}^{t}\) is chosen arbitrarily and an instantaneous loss of \(\left\langle\bm{\ell}^{t},\bm{x}^{t}\right\rangle\) is incurred. The regret of an algorithm generating the sequence of decisions \(\bm{x}^{1},...,\bm{x}^{T}\) is defined as the difference between the cumulative loss generated and that of any fixed strategy \(\hat{\bm{x}}\in\mathcal{X}\): \(\mathrm{Reg}^{T}(\hat{\bm{x}})=\sum_{t=1}^{T}\left\langle\bm{\ell}^{t},\bm{x} ^{t}-\hat{\bm{x}}\right\rangle\). A _regret minimizer_ guarantees that \(\mathrm{Reg}^{T}(\hat{\bm{x}})=o(T)\) for any \(\hat{\bm{x}}\in\mathcal{X}\).

**Online Mirror Descent.** A famous regret minimizer is Online Mirror Descent (OMD) [30], which generates the decisions \(\bm{x}^{1},...,\bm{x}^{T}\) as follows (with a learning rate \(\eta>0\)):

\[\bm{x}^{t+1}=\Pi_{\bm{x}^{t},\mathcal{X}}\left(\eta\bm{\ell}^{t}\right)\] (OMD)

where for any \(\bm{x}\in\mathcal{X}\), and any loss \(\bm{\ell}\), the _proximal operator_\(\bm{\ell}\mapsto\Pi_{\bm{x},\mathcal{X}}(\bm{\ell})\) is defined as \(\Pi_{\bm{x},\mathcal{X}}(\bm{\ell})=\arg\min_{\bm{x}\in\mathcal{X}}\left\langle \bm{\ell},\hat{\bm{x}}\right\rangle+D(\hat{\bm{x}},\bm{x})\) where \(D\) is the _Bregman divergence_ associated with \(\varphi:\mathcal{X}\rightarrow\mathbb{R}\), a \(1\)-strongly convex regularizer (with respect to some norm \(\|\cdot\|\)): \(D(\hat{\bm{x}},\bm{x})=\varphi(\hat{\bm{x}})-\varphi(\bm{x})-\left\langle \nabla\varphi(\bm{x}),\hat{\bm{x}}-\bm{x}\right\rangle,\forall\ \hat{\bm{x}},\bm{x}\in\mathcal{X}\). OMD guarantees that the worst-case regret against any \(\hat{\bm{x}}\) grows as \(O(\sqrt{T})\) (omitting other dependence for simplicity; the same below). Other popular regret minimizers include Follow-The-Regularized-Leader (FTRL), and adaptive variants of OMD and FTRL; we refer the reader to [20] for an extensive survey on regret minimizers.

**Regret Matching and Regret Matching\({}^{+}\).** Regret Matching (RM) and Regret Matching\({}^{+}\) (RM\({}^{+}\)) are two regret minimizers that achieve \(O(\sqrt{T})\) worst-case regret when \(\mathcal{X}=\Delta^{d}\). RM [19] maintains a sequence of _aggregate payoffs_\(\left(\bm{R}^{t}\right)_{t\geq 1}\): \(\bm{R}^{t}=R_{0}\mathbf{1}_{d},\) and for \(t\geq 1\),

\[\bm{x}^{t}=[\bm{R}^{t}]^{+}/\|[\bm{R}^{t}]^{+}\|_{1},\quad\bm{R}^{t+1}=\bm{R}^ {t}+\left\langle\bm{x}^{t},\bm{\ell}^{t}\right\rangle\mathbf{1}_{d}-\bm{\ell} ^{t},\]

where \(R_{0}\geq 0\) specifies an initial point and \(\bm{0}/0\) is defined as the uniform distribution for convenience. The original RM sets \(R_{0}=0\), making the algorithm completely _parameter-free_, a very appealing property in practice. RM\({}^{+}\) is a simple variation of RM, where the aggregate payoffs are thresholded at every iteration [35]. In particular, RM\({}^{+}\) only keeps track of the non-negative components of the aggregate payoffs to compute a decision: \(\bm{R}^{1}=R_{0}\mathbf{1}_{d},\) and for \(t\geq 1\),

\[\bm{x}^{t}=\bm{R}^{t}/\|\bm{R}^{t}\|_{1},\quad\bm{R}^{t+1}=[\bm{R}^{t}+\left \langle\bm{x}^{t},\bm{\ell}^{t}\right\rangle\mathbf{1}_{d}-\bm{\ell}^{t}]^{+}.\]

We highlight that very little is known about the theoretical properties of RM\({}^{+}\), despite its strong empirical performances: [36] show that RM\({}^{+}\) is a regret minimizer (and enjoys the stronger \(K\)-tracking regret property), and [4] show that it can safely be combined with alternation ([18] prove strict improvement when using alternation). Farina et al. [12] show an interesting connection between RM\({}^{+}\) and Online Mirror Descent: the update \(\bm{R}^{t+1}=[\bm{R}^{t}+\left\langle\bm{x}^{t},\bm{\ell}^{t}\right\rangle \mathbf{1}_{d}-\bm{\ell}^{t}]^{+}\) of RM\({}^{+}\) can be rewritten as

\[\bm{R}^{t+1}=\Pi_{\bm{R}^{t},\mathcal{X}}\left(\eta\bm{f}(\bm{x}^{t},\bm{\ell} ^{t})\right)\]

for \(\mathcal{X}=\mathbb{R}_{+}^{d},\varphi=\frac{1}{2}\|\cdot\|_{2}^{2}\), \(\eta=1\), and \(\bm{f}(\bm{x}^{t},\bm{\ell}^{t})\) defined as \(\bm{f}(\bm{x}^{t},\bm{\ell}^{t})=\bm{\ell}^{t}-\left\langle\bm{x}^{t},\bm{\ell} ^{t}\right\rangle\mathbf{1}_{d}.\) Therefore, RM\({}^{+}\) generating a sequence of decisions \(\bm{x}^{1},...,\bm{x}^{T}\) facing a sequence of losses \(\left(\bm{\ell}^{t}\right)_{t\geq 1}\), is closely connected to OMD instantiated with the non-negative orthant as the decision set and facing a sequence of losses \(\left(\bm{f}(\bm{x}^{t},\bm{\ell}^{t})\right)_{t\geq 1}\). We have the following relation for the regret in \(\bm{x}^{1},...,\bm{x}^{T}\) and the regret in \(\bm{R}^{1},...,\bm{R}^{T}\) (the proof follows [12] and is deferred to the appendix).

**Lemma 2.1**.: _Let \(\bm{x}^{1},...,\bm{x}^{T}\in\Delta^{d}\) be generated as \(\bm{x}^{t}=\bm{R}^{t}/\|\bm{R}^{t}\|_{1}\) for some sequence \(\bm{R}^{1},...,\bm{R}^{T}\in\mathbb{R}_{+}^{d}\). The regret \(\operatorname{Reg}^{T}(\hat{\bm{x}})\) of \(\bm{x}^{1},...,\bm{x}^{T}\) facing a sequence of losses \(\bm{\ell}^{1},...,\bm{\ell}^{T}\) is equal to \(\operatorname{Reg}^{T}(\hat{\bm{R}})\), the regret of \(\bm{R}^{1},...,\bm{R}^{T}\) facing the sequence of losses \(\bm{f}\left(\bm{x}^{1},\bm{\ell}^{1}\right),...,\bm{f}\left(\bm{x}^{T},\bm{ \ell}^{T}\right)\), compared against \(\hat{\bm{R}}=\hat{\bm{x}}\): \(\operatorname{Reg}^{T}\left(\hat{\bm{R}}\right)=\sum_{t=1}^{T}\left\langle \bm{f}\left(\bm{x}^{t},\bm{\ell}^{t}\right),\bm{R}^{t}-\hat{\bm{R}}\right\rangle.\)_

Since OMD is a regret minimizer guaranteeing \(\operatorname{Reg}^{T}(\hat{\bm{R}})=O(\sqrt{T})\), Lemma 2.1 directly shows that \(\text{RM}^{+}\) is also a regret minimizer: \(\operatorname{Reg}^{T}(\hat{\bm{x}})=O(\sqrt{T})\).

**Multiplayer Normal-Form Games.** In a multiplayer normal-form game, there are \(n\in\mathbb{N}\) players. Each player \(i\) has \(d_{i}\) strategies and their decision space \(\Delta^{d_{i}}\) is the probability simplex over the \(d_{i}\) strategies. We denote \(\Delta=\times_{i=1}^{n}\Delta^{d_{i}}\) as the joint decision space of all players and \(d=d_{1}+\cdots+d_{n}\). The utility function for player \(i\) is a concave function \(u_{i}:\Delta\to[-1,1]\) that maps every joint strategy profile \(\bm{x}=(\bm{x}_{1},...,\bm{x}_{n})\in\Delta\) to a payoff. We assume bounded gradients and \(L_{u}\)-smoothness for the utilities of the players: there exists \(B_{u}>0,L_{u}>0\) such that for any \(\bm{x}\), \(\bm{x}^{\prime}\in\Delta\) and any player \(i\),

\[\|\nabla_{\bm{x}_{i}}u_{i}(\bm{x})\|_{2}\leq B_{u},\|\nabla_{\bm{x}_{i}}u_{i}( \bm{x})-\nabla_{\bm{x}_{i}}u_{i}(\bm{x}^{\prime})\|_{2}\leq L_{u}\|\bm{x}-\bm {x}^{\prime}\|_{2}.\] (1)

The function mapping joint strategies to negative payoff gradients for all players is a vector-valued function \(G:\Delta\to\mathbb{R}^{d}\) such that \(G(\bm{x})=(-\nabla_{\bm{x}_{n}}u_{1}(\bm{x}),\ldots,-\nabla_{\bm{x}_{n}}u_{n} (\bm{x}))\). It is well known that running a regret minimizer for \((\bm{x}_{1}^{t},...,\bm{x}_{n}^{t})\in\Delta=\times_{i=1}^{n}\Delta^{d_{i}}\) facing the loss \(G(\bm{x}^{t})=(\bm{\ell}_{1}^{t},\ldots,\bm{\ell}_{n}^{t})\) leads to strong game-theoretic guarantees (e.g., the average iterate being an approximate coarse correlated equilibrium). However, in light of Lemma 2.1, we will instead perform regret minimization on \((\bm{R}_{1}^{t},...,\bm{R}_{n}^{t})\in\mathcal{X}=\times_{i=1}^{n}\mathbb{R}_{+ }^{d_{i}}\) with the losses \((\bm{f}(\bm{x}_{1}^{t},\bm{\ell}_{1}^{t}),\ldots,\bm{f}(\bm{x}_{n}^{t},\bm{ \ell}_{n}^{t}))\). For conciseness, we thus define the operator \(F:\mathcal{X}\to\mathbb{R}^{d}\) as, for \(\bm{z}=(\bm{R}_{1},...,\bm{R}_{n})\), \(F(\bm{z})=(\bm{f}(\bm{x}_{1},\bm{\ell}_{1}),...,\bm{f}(\bm{x}_{n},\bm{\ell}_{ n}))\) where \(\bm{x}_{i}=\bm{R}_{i}/\|\bm{R}_{i}\|_{1},\forall\ i=1,...,n,\left(\bm{\ell}_{i} \right)_{i\in[n]}=G(\bm{x})\).

**Predictive OMD and Its RVU Bounds.** The predictive version of OMD proceeds as follows:

\[\bm{x}^{t}=\Pi_{\hat{\bm{x}}^{t},\mathcal{X}}\left(\eta\bm{m}^{t}\right)\quad \tilde{\bm{x}}^{t+1}=\Pi_{\tilde{\bm{x}}^{t},\mathcal{X}}\left(\eta\bm{\ell}^{ t}\right)\]

When setting \(\bm{m}^{t}=\bm{\ell}^{t-1}\), predictive OMD satisfies \(\operatorname{Reg}^{T}(\hat{\bm{x}})\leq\frac{D(\hat{\bm{x}},\tilde{\bm{x}}^{1 })}{\eta}+\eta\sum_{t=1}^{T}\|\bm{\ell}^{t}-\bm{\ell}^{t-1}\|_{\star}^{2}- \frac{1}{8\eta}\sum_{t=1}^{T-1}\|\bm{x}^{t+1}-\bm{x}^{t}\|^{2}\). This regret bound satisfies the _RVU_ (regret bounded by variation in utilities) condition, introduced in [34]. The authors show that this type of bound guarantees that the social regret (i.e., sum of the regrets of all players) is \(O(1)\) when all players apply this special instance of predictive OMD. Syrgkanis et al. [34] further prove that each player has improved \(O(T^{1/4})\) individual regret by the _stability_ of predictive OMD. Specifically, they show that predictive OMD guarantees \(\|\bm{x}^{t+1}-\bm{x}^{t}\|=O(\eta)\) against any adversarial loss sequence, i.e., the algorithm is stable in the sense that the change in the iterates can be controlled by choosing \(\eta\) appropriately.

**Predictive RM\({}^{+}\)** Similar to OMD, we can generalize RM\({}^{+}\) to Predictive Regret Matching\({}^{+}\)[12]: define \(\bm{R}^{1}=\bm{m}^{1}=R_{0}\bm{1}_{d}\) (with \(R_{0}=0\) by default), and for \(t\geq 1\),

\[\bm{x}^{t} =\hat{\bm{R}}^{t}/\|\hat{\bm{R}}^{t}\|_{1},\ \text{for}\ \hat{\bm{R}}^{t}=[\bm{R}^{t}+\bm{m}^{t}]^{+},\] \[\bm{R}^{t+1} =[\bm{R}^{t}-\bm{f}(\bm{x}^{t},\bm{\ell}^{t})]^{+},\ \text{for}\ \bm{f}(\bm{x}^{t},\bm{\ell}^{t})=\bm{\ell}^{t}-\left\langle\bm{x}^{t},\bm{ \ell}^{t}\right\rangle\bm{1}_{d}.\]

We call the algorithm predictive RM\({}^{+}\) (PRM\({}^{+}\)) when \(\bm{m}^{t}=-\bm{f}(\bm{x}^{t-1},\bm{\ell}^{t-1})\), and it recovers RM\({}^{+}\) when \(\bm{m}^{t}=\bm{0}\). A regret bound with a similar RVU condition is attainable for predictive RM\({}^{+}\) by its connection to predictive OMD [12], but only in the non-negative orthant space instead of the actual strategy space. To make a connection between them, stability is required as we show later. A natural question is then whether (predictive) RM\({}^{+}\) is also always stable. We show that the answer is no by giving an adversarial example in the next section.

## 3 Instability of (Predictive) Regret Matching\({}^{+}\)

We start by showing that there exist adversarial loss sequences that lead to instability for both RM\({}^{+}\) and predictive RM\({}^{+}\). Our construction starts with an unbounded loss sequence \(\bm{\ell}^{t}\) so that \(\bm{x}^{t}\) alternates between \((1/2,1/2)\) and \((0,1)\): we set \(\bm{\ell}^{t}=(\ell^{t},0)\), where \(\ell^{1}=2\), and for \(t\geq 2\), \(\ell^{t}=-2^{(t-2)/2}\) if \(t\) is even and \(\ell^{t}=2^{(t-1)/2}\) if \(t\) is odd. Our proof is completed by normalizing the losses to \([-1,1]\) given a fixed time horizon (see Appendix B for details).

**Theorem 3.1**.: _There exist finite sequences of losses in \(\mathbb{R}^{2}\) for RM\({}^{+}\) and its predictive version such that \(\bm{x}^{t}=(\frac{1}{2},\frac{1}{2})\) when \(t\) is odd and \(\bm{x}^{t}=(0,1)\) when \(t\) is even._

This is in stark contrast to OMD which always ensures \(\|\bm{x}^{t+1}-\bm{x}^{t}\|=O(\eta)\) and is thus inherently stable. However, a somewhat surprising property about (predictive) RM\({}^{+}\) is that _instability actually implies low regret_. To see this, we first present the following Lipschitz property of the normalization function \(\bm{g}:\bm{x}\mapsto\bm{x}/\|\bm{x}\|_{1}\) for \(\bm{x}\in\mathbb{R}^{d}_{+}\).

**Proposition 1**.: Let \(\bm{x},\bm{y}\in\mathbb{R}^{d}_{+}\), with \(\bm{1}^{\top}\bm{x}\geq 1\). Then, \(\|\bm{g}(\bm{y})-\bm{g}(\bm{x})\|_{2}\leq\sqrt{d}\cdot\|\bm{y}-\bm{x}\|_{2}\).

This proposition shows that the normalization step has a reasonable Lipschitz constant (\(\sqrt{d}\)) as long as its input is not too close to the origin, which further implies the following corollary.

**Corollary 3.2**.: _RM\({}^{+}\) with \(\|\bm{R}^{t}\|_{1}\geq R_{0}\) satisfies \(\left\|\bm{x}^{t+1}-\bm{x}^{t}\right\|_{2}\leq\frac{\sqrt{d}}{R_{0}}\cdot\left \|\bm{R}^{t+1}-\bm{R}^{t}\right\|_{2}\leq\frac{2dB_{u}}{R_{0}}\)._

Put differently, the corollary states that instability can happen only when the cumulative regret vector \(\bm{R}^{t}\) is small. For example, if \(\|\bm{x}^{t+1}-\bm{x}^{t}\|=\Omega(1)\), then we must have \(\|\bm{R}^{t}\|_{1}=O(dB_{u})\) and thus the regret at that point is at most \(O(dB_{u})\). A similar argument holds for predictive RM\({}^{+}\) as well. Therefore, instability is in fact not an issue for these algorithms' own regret.

However, when using these algorithms to play a game, what could happen is that such instability leads to other players learning in an unpredictable environment with large regret. We show this phenomenon via an example of a \(3\times 3\) matrix game \(\max_{\bm{x}\in\Delta(3)}\min_{\bm{y}\in\Delta(3)}\langle\bm{x},\bm{A}\bm{y}\rangle\), where \(\bm{A}=((3,0,-3),(0,3,-4),(0,0,1))\). The first column of Fig. 1 shows the squared \(\ell_{2}\) norm of the consecutive difference of the last \(100\) iterates of Predictive RM\({}^{+}\) for the \(x\) player (top) and the \(y\) player (bottom). The iterates of the \(x\) player are rapidly changing in a periodic fashion while the iterates of the \(y\) player are stable with changes on the order of \(10^{-5}\). In the center plots where we show the individual regret for each player, we indeed observe that the cumulative regret of the \(x\) player is near zero as implied by instability, but it causes large regret (close to \(T^{0.5}\) empirically) for the \(y\) player. (We show the same plots for RM\({}^{+}\) in Fig. 4 in Appendix B; there, the iterates of both players are stable, but since RM\({}^{+}\) lacks predictivity, it still leads to larger regret for one player.)

The right column of Fig. 1 shows the duality gap achieved by the linear average \((\bar{x}_{t},\bar{y}_{t})=\left(\frac{2}{T(T+1)}\sum_{t=1}^{T}t\bm{x}^{t},\frac {2}{T(T+1)}\sum_{t=1}^{T}t\bm{y}^{t}\right)\), when the iterates are generated by RM\({}^{+}\) with alternation (top) and predictive RM\({}^{+}\) (bottom). For both algorithms the convergence rate slows down around \(10^{4}\) iterations. A linear regression estimate on the rate for the last \(10^{6}\) iterates shows rates of \(-0.497\) and \(-0.496\) for RM\({}^{+}\) and predictive RM\({}^{+}\) respectively. To the best of our knowledge, this is the

Figure 1: Left plots show the iterate-to-iterate variation in the last \(100\) iterates of predictive RM\({}^{+}\). Center plots show the regret for the \(x\) and \(y\) players under predictive RM\({}^{+}\). Right plots show empirical convergence speed of RM\({}^{+}\) (top row) and Predictive RM\({}^{+}\) (bottom row).

first known case of empirical convergence rates on the order of \(T^{-0.5}\) for either \(\text{RM}^{+}\) or predictive \(\text{RM}^{+}\); the worst prior instance for \(\text{RM}^{+}\) was \(T^{-0.74}\) in Farina et al. [10]; no hard instance was known for predictive \(\text{RM}^{+}\).

## 4 Stabilizing \(\text{RM}^{+}\) and Predictive \(\text{RM}^{+}\).

Based on the discussions in the previous section, we aim to make _every_ player stable despite the fact that being an unstable player may actually be good for that particular player. By Corollary 3.2, it suffices to make sure that \(\|\bm{R}^{t}\|_{1}\) is never too small. We provide two approaches to ensure this property and thereby stabilize (predictive) \(\text{RM}^{+}\).

**Stable Predictive \(\text{RM}^{+}\).** One way to maintain the required distance to the origin is via _restarting_: We initialize the algorithm with the cumulative regret vector equal to some non-zero amount, instead of the usual initialization at zero. Then, when the cumulative regret vector gets below the initialization point, we _restart_ the algorithm from the initialization point. Applying this idea to predictive \(\text{RM}^{+}\) yields Algorithm 1. Player \(i\) starts with \(\bm{R}^{1}_{i}=R_{0}\mathbbm{1}_{d_{i}}\), runs predictive \(\text{RM}^{+}\), and restarts whenever \(\bm{R}^{t}_{i}\leq R_{0}\mathbbm{1}_{d_{i}}\). In the algorithm we write \((\bm{R}^{t}_{1},...,\bm{R}^{t}_{n})\) compactly as \(\bm{w}^{t}\) (similarly for \(\bm{z}^{t}\)). Note, though, that the updates are decentralized for each player, as in vanilla predictive \(\text{RM}^{+}\).

Given this modification, Stable \(\text{PRM}^{+}\) achieves improved individual regret in multiplayer games, as stated in Theorem 4.1. We defer the proof to the appendix. One key step in the analysis is to note that by definition, the regret against any action is negative when the restarting event happens, so it is sufficient to consider the regret starting from the last restart. Thanks to the stability enforced by the restarts, the regret from the last restart is also well controlled and the results follow by tuning \(\eta\) and \(R_{0}\) optimally. In fact, since the algorithm is scale-invariant up to the relative scale of the two parameters, it is without loss of generality to always set \(R_{0}=1\).

```
1:Input:\(R_{0}>0\), step size \(\eta>0\)
2:Initialization:\(\bm{w}^{0}=R_{0}\mathbbm{1}_{d}\)
3:for\(t=1,\ldots,T\)do
4:\(\bm{z}^{t}=\Pi_{\bm{w}^{t-1},\mathcal{X}}\left(\eta\bm{m}^{t}\right)\)
5:\(\bm{w}^{t}=\Pi_{\bm{w}^{t-1},\mathcal{X}}\left(\eta F(\bm{z}^{t})\right)\)
6:\((\bm{x}^{t}_{1},\ldots,\bm{x}^{t}_{n})=(\bm{g}(\bm{z}^{t}_{1}),\ldots,\bm{g} (\bm{z}^{t}_{n}))\)
7:for\(i=1,...,n\)do
8:if\(\bm{w}^{t}_{i}\leq R_{0}\mathbbm{1}_{d_{i}}\)then
9:\(\bm{w}^{t}_{i}=R_{0}\mathbbm{1}_{d_{i}}\) ```

**Algorithm 1** Stable Predictive \(\text{RM}^{+}\)

**Theorem 4.1**.: _Let \(\eta=\left(d^{2}T\right)^{-1/4}\) and \(R_{0}=1\). Let \((\bm{f}^{t}_{i})_{i\in[n]}=F(\bm{z}^{t})\) for \(t\geq 1\). For each player \(i\), set the sequence of predictions \(\bm{m}^{t}_{i}=\bm{0}\) when \(t=0\) or restart happens at \(t-1\); otherwise, \(\bm{m}^{t}_{i}=\bm{f}^{t-1}_{i},\forall\;t\geq 1\). Then Algorithm 1 guarantees that the individual regret \(\operatorname{Reg}^{T}_{i}(\hat{\bm{x}}_{i})=\sum_{t=1}^{T}\left\langle\nabla_ {\bm{x}_{i}}u_{i}(\bm{x}^{t}),\hat{\bm{x}}_{i}-\bm{x}^{t}_{i}\right\rangle\) of each player \(i\) is bounded by \(O\left(d^{3/2}T^{1/4}\right)\) in multiplayer normal-form games._

Although the restarting idea successfully stabilizes the \(\text{RM}^{+}\) algorithm, the discontinuity created by asynchronous restarts causes technical difficulty for bounding the social regret by \(O(1)\). Next we introduce an alternative stabilization idea to fix this issue.

**Smooth Predictive \(\text{RM}^{+}\).** Our second stabilization idea is to restrict the decision space to a subset where we "chop off" the area that is too close to the origin, that is, project the vector \(\bm{R}^{t}_{i}\) onto the set \(\Delta^{d_{i}}_{\geq}=\{\bm{R}\in\mathbb{R}^{d_{i}}_{+}\mid\|\bm{R}\|_{1}\geq 1\}\). We denote the joint chopped-off decision space as \(\mathcal{X}_{\geq}=\times_{i=1}^{n}\Delta^{d_{i}}_{\geq}\). We call the resulting algorithm smooth predictive \(\text{RM}^{+}\) (Algorithm 2). Besides a similar result to Theorem 4.1 on the individual regret (omitted for simplicity), Algorithm 2 also guarantees that the social regret is bounded by a game-dependent constant, as shown in Theorem 4.2.

**Theorem 4.2**.: _Let \(\eta=\left(2\sqrt{2}(n-1)\max_{i}\{d_{i}^{3/2}\}\right)^{-1}\). Using the sequence of predictions \(\bm{m}^{0}=\bm{0},\bm{m}^{t}=F(\bm{z}^{t-1}),\forall\;t\geq 1\), Algorithm 2 guarantees that the so cial regret_\(\sum_{i=1}^{n}\operatorname{Reg}_{i}^{T}(\hat{\bm{x}}_{i})=\sum_{i=1}^{n}\sum_{t=1}^{T} \left\langle\nabla_{\bm{x}_{i}}u_{i}(\bm{x}^{t}),\hat{\bm{x}}_{i}-\bm{x}_{i}^{t}\right\rangle\) is upper bounded by \(O\left(n^{2}\max_{i=1,...,n}\{d_{i}^{3/2}\}\max_{i=1,...,n}\{\|\bm{w}_{i}^{0}- \hat{\bm{x}}_{i}\|_{2}^{2}\}\right)\) in multiplayer normal-form games.

Algorithm 2 dominates Algorithm 1 in terms of our theoretical results so far, but it has one drawback: it requires occasional projection onto \(\mathcal{X}_{\geq}\). In Appendix K we show that this can be done with a sorting trick in \(O(d\log d)\) time, whereas the restarting procedure is implementable in linear time.

```
1:Input: Step size \(\eta>0\) with \(\eta<1/L_{F}\)
2:Initialization:\(\bm{z}^{0}\in\mathcal{X}_{\geq}\)
3:for\(t=1,\ldots,T\)do
4:\(\bm{z}^{t}=\Pi_{\bm{z}^{t-1},\mathcal{X}_{\geq}}\left(\eta F(\bm{z}^{t})\right)\)
5:\((\bm{x}_{1}^{t},\ldots,\bm{x}_{n}^{t})=(\bm{g}(\bm{z}_{1}^{t}),\ldots,\bm{g}( \bm{z}_{n}^{t}))\) ```

**Algorithm 3** Conceptual RM\({}^{+}\)

## 5 Conceptual Regret Matching\({}^{+}\)

In this section, we depart from the predictive OMD framework and develop new smooth variants of RM\({}^{+}\) from a different angle. Instead of using predictive OMD to compute the iterates \(\left(\bm{R}_{i}^{t}\right)_{t\geq 1}\), we consider the following regret minimizer that we call _cheating OMD_, defined for some arbitrary closed decision set \(\mathcal{Z}\) and an arbitrary sequence of losses \(\left(\bm{\ell}^{t}\right)_{t\geq 1}\): \(\bm{z}^{t}=\Pi_{\bm{z}^{t-1},\mathcal{Z}}\left(\eta\bm{\ell}^{t}\right)\) for \(t\geq 1\), and \(\bm{z}^{0}\in\mathcal{X}_{\geq}\). Cheating OMD is inspired by the Conceptual Prox method for solving variational inequalities associated with monotone operators [5; 23; 29]. We call it _cheating_ OMD because at iteration \(t\), the decision \(\bm{z}^{t}\) is chosen as a function of the current loss \(\bm{\ell}^{t}\), which is revealed _after_ the decision \(\bm{z}^{t}\) has been chosen. It is well-known that cheating OMD yields a sequence of decisions with constant regret; we show it for our setting in the following lemma.

**Lemma 5.1**.: _The Cheating OMD iterates \(\left\{\bm{z}^{t}\right\}_{t}\) satisfy \(\sum_{t=1}^{T}\left\langle\bm{\ell}^{t},\bm{z}^{t}-\hat{\bm{z}}\right\rangle \leq\frac{1}{2\eta}\|\bm{z}^{0}-\hat{\bm{z}}\|_{2}^{2},\forall\hat{\bm{z}}\in \mathcal{Z}\)._

To instantiate RM\({}^{+}\) with Cheating OMD as a regret minimizer for the sequence \(\left(\bm{R}_{i}^{t}\right)_{t\geq 1}\) of each player \(i\), we need to show the existence of a vector \(\bm{z}^{t}\in\mathcal{X}_{\geq}\) such that

\[\bm{z}^{t}=\Pi_{\bm{z}^{t-1},\mathcal{X}_{\geq}}\left(\eta F(\bm{z}^{t})\right).\] (2)

Equation (2) can be interpreted as a fixed-point equation for the map \(\bm{z}\mapsto\Pi_{\bm{z}^{t-1},\mathcal{X}_{\geq}}\left(\eta F(\bm{z})\right)\). For any \(\bm{z}^{\prime}\in\mathcal{X}_{\geq}\), the map \(\bm{z}\mapsto\Pi_{\bm{z}^{\prime},\mathcal{X}_{\geq}}\left(\eta F(\bm{z})\right)\) is \(\eta L\)-Lipschitz continuous _as long as_\(F\) is \(L\)-Lipschitz continuous. Therefore, it is a contraction when \(\eta<1/L\), and then the fixed-point equation \(\bm{z}=\Pi_{\bm{z}^{\prime},\mathcal{X}_{\geq}}\left(\eta F(\bm{z})\right)\) has a unique solution. Recall that for \(\bm{z}=(\bm{R}_{1},...,\bm{R}_{n})\in\mathcal{X}_{\geq}\), the operator \(F\) is defined as \(F(\bm{z})=(\bm{f}(\bm{x}_{1},\bm{\ell}_{1}),\ldots,\bm{f}(\bm{x}_{n},\bm{\ell}_ {n}))\) where \(\bm{x}_{i}=\bm{g}(\bm{R}_{i})\) and \(\bm{\ell}_{i}=-\nabla_{\bm{x}_{i}}u_{i}(\bm{x}),\) for all \(i\in\{1,...,n\}\). We now show the Lipschitzness of \(F\) over \(\mathcal{X}_{\geq}\) for normal-form games.

**Lemma 5.2**.: _For a normal-form game, the operator \(F\) is \(L_{F}\)-Lipschitz continuous over \(\mathcal{X}_{\geq}\), with \(L_{F}=\left(\max_{i}d_{i}\right)\sqrt{2B_{u}^{2}+4L_{u}^{2}}\) with \(B_{u},L_{u}\) defined in (1)._

For \(L_{F}\) defined as in Lemma 5.2 and \(\eta<1/L_{F}\), the existence of the fixed-point \(\bm{z}^{t}=\Pi_{\bm{z}^{t-1},\mathcal{X}_{\geq}}\left(\eta F(\bm{z}^{t})\right)\) is guaranteed. This yields _Conceptual RM\({}^{+}\)_, defined in Algorithm 3. In the following theorem, we show that Conceptual RM\({}^{+}\) ensures constant regret for each player.

**Theorem 5.3**.: _Let \(L_{F}>0\) be defined as in Lemma 5.2. For \(\eta<1/L_{F}\), Algorithm 3 guarantees that the individual regret \(\operatorname{Reg}_{i}^{T}(\hat{\bm{x}}_{i})=\sum_{t=1}^{T}\left\langle\nabla_{ \bm{x}_{i}}u_{i}(\bm{x}^{t}),\hat{\bm{x}}_{i}-\bm{x}_{i}^{t}\right\rangle\) of each player \(i\) is bounded by \(\frac{1}{2\eta}\|\bm{z}_{i}^{0}-\hat{\bm{x}}_{i}\|_{2}^{2}\) in multiplayer normal-form games._Note that the requirement of \(\eta<1/L_{F}\) in Theorem 5.3 and Algorithm 3 is only needed in order to ensure existence of a fixed-point. If the fixed-point condition holds for some larger \(\eta\), then the algorithm is still well-defined and the same convergence guarantee holds.

**Remark 5.4**.: _Piliouras et al. [31] propose the clairvoyant multiplicate weights updates (MWU) algorithm, based on the classical MWU algorithm, but where the rescaling at iteration \(t\) involves the payoff of the players at iteration \(t\). The connection with the conceptual prox method is made explicit by [14], where they show how to extend clairvoyant MWU for normal-form games to clairvoyant OMD for general convex games. Our algorithm uses the same idea but for RM\({}^{+}\)._

For \(\bm{z}^{\prime}\in\mathcal{X}_{\geq}\), we can approximate the fixed-point of \(\bm{z}\mapsto\Pi_{\bm{z}^{\prime},\mathcal{X}_{\geq}}\left(\eta F(\bm{z})\right)\) by performing \(k\in\mathbb{N}\) fixed-point iterations. This results in Algorithm 4. We give the guarantees for Algorithm 4 below.

**Theorem 5.5**.: _Let \(L_{F}>0\) be defined as in Lemma 5.2 and \(\eta<1/L_{F}\). Assume that in Algorithm 4, we ensure \(\|\bm{w}^{k}-\Pi_{\bm{z}^{\prime-1},\mathcal{X}_{\geq}}\left(\eta F(\bm{w}^{k })\right)\|_{2}\leq\epsilon^{(t)}\), for all \(t\geq 1\). Then Algorithm 4 guarantees that the individual regret \(\operatorname{Reg}_{i}^{T}(\hat{\bm{x}}_{i})=\sum_{t=1}^{T}\left\langle\nabla _{\bm{x}_{i}}u_{i}(\bm{x}^{t}),\hat{\bm{x}}_{i}-\bm{x}_{i}^{t}\right\rangle\) of each player \(i\) is bounded by \(\frac{1}{2\eta}\|\bm{z}_{i}^{0}-\hat{\bm{x}}_{i}\|_{2}^{2}+2B_{u}\sqrt{d_{i}} \sum_{t=1}^{T}\epsilon^{(t)}\) in multiplayer normal-form games._

By Theorem 5.5, if we ensure error \(\epsilon^{(t)}=1/t^{2}\) in Algorithm 4 then the individual regret of each player is bounded by a constant. Since \(\bm{w}\mapsto\Pi_{\bm{z}^{\prime-1},\mathcal{X}_{\geq}}\left(\eta F(\bm{w})\right)\) is a contraction for \(\eta<1/L_{F}\), this only requires \(k=O\left(\log(t)\right)\) fixed-point iterations at each time \(t\). If the number of iterations \(T\) is known in advance, we can choose \(k=O(\log(T))\), to ensure \(\epsilon^{(t)}=O(1/T)\) and therefore that the individual regret of each player \(i\) is bounded by the constant \(\frac{1}{2\eta}\|\bm{z}_{i}^{0}-\hat{\bm{x}}_{i}\|_{2}^{2}+O\left(2B_{u}\sqrt {d_{i}}\right).\)

Recall that the uniform distribution over a sequence of strategy profiles \(\{\bm{x}^{t}\}_{t=1}^{T}\) is a \((\max_{i}\operatorname{Reg}_{i}^{T})/T\)-approximate coarse correlated equilibrium (CCE) of a multiplayer normal-form game (see e.g. Theorem 2.4 in Piliouras et al. [31]). Therefore, Algorithm 3 guarantees \(O(1/T)\) convergence to a CCE after \(T\) iterations. With the setup from Theorem 5.5 and \(k=O(\log(T))\), Algorithm 4 guarantees \(O(\log(T)/T)\) convergence to a CCE after \(T\) evaluations of the operator \(F\).

Extragradient RM\({}^{+}\).We now consider the case of Algorithm 4 but with only one fixed-point iteration (\(k=1\)). This is similar to the mirror prox algorithm [29] or the extragradient method [24]. We call this algorithm extragradient RM\({}^{+}\)(ExRM\({}^{+}\), Algorithm 5). We show that one fixed-point iteration (\(k=1\)) at every iteration ensures constant social regret.

**Theorem 5.6**.: _Define \(L_{F}\) as in Lemma 5.2 and let \(\eta=(\sqrt{2}L_{F})^{-1}\). Algorithm 5 guarantees that the social regret \(\sum_{i=1}^{n}\operatorname{Reg}_{i}^{T}(\hat{\bm{x}}_{i})=\sum_{i=1}^{n}\sum_ {t=1}^{T}\left\langle\nabla_{\bm{x}_{i}}u_{i}(\bm{x}^{t}),\hat{\bm{x}}_{i}- \bm{x}_{i}^{t}\right\rangle\) is bounded by \(\frac{1}{2\eta}\sum_{i=1}^{n}\|\bm{z}_{i}^{0}-\hat{\bm{x}}_{i}\|_{2}^{2}\) in multiplayer normal-form games._

We now apply Theorem 5.6 to the case of matrix games, where the goal is to solve

\[\min_{\bm{x}\in\Delta^{d_{1}}}\max_{\bm{y}\in\Delta^{d_{2}}}\left\langle\bm{x},\bm{A}\bm{y}\right\rangle\]

for \(\bm{A}^{d_{1}\times d_{2}}\). The operator \(F\) is defined as

\[F\begin{bmatrix}\bm{R}_{1}\\ \bm{R}_{2}\end{bmatrix}=\begin{bmatrix}\bm{f}\left(\bm{g}(\bm{R}_{1}),\bm{A} \bm{g}(\bm{R}_{2})\right)\\ \bm{f}\left(\bm{g}(\bm{R}_{2}),-\bm{A}^{\top}\bm{g}(\bm{R}_{1})\right)\end{bmatrix}\]

and \(\mathcal{X}_{\geq}=\Delta_{\geq}^{d_{1}}\times\Delta_{\geq}^{d_{2}}\). The next lemma gives the Lipschitz constant of the operator \(F\) in the case of matrix games.

**Lemma 5.7**.: _For matrix games, the operator \(F\) is \(L_{F}\)-Lipschitz over \(\mathcal{X}_{\geq}\), with \(L_{F}=\sqrt{6}\|\bm{A}\|_{op}\max\{d_{1},d_{2}\}\) with \(\|\bm{A}\|_{op}=\sup\{\|\bm{A}\bm{v}\|_{2}/\|\bm{v}\|_{2}\mid\bm{v}\in\mathbb{ R}^{d_{2}},\bm{v}\neq\bm{0}\}.\)_

Combining Lemma 5.7 with Theorem 5.6, ExRM\({}^{+}\) for matrix games with \(\mathcal{X}_{\geq}\) as a decision set and \(\eta=\left(\sqrt{2}L_{F}\right)^{-1}\) guarantees constant social regret, so that the average of the iterates computed by ExRM\({}^{+}\) converges to a Nash Equilibrium at a rate of \(O(1/T)\)[16].

Extensive-form gamesOur convergence results for Conceptual RM\({}^{+}\) apply beyond normal-form games, to EFGs. Briefly, a EFG is a game played on a tree, where each node belongs to some player, and the player chooses a probability distribution over branches. Moreover, players have _information sets_, which are groups of nodes belonging to a player such that they cannot distinguish among them, and thus they must choose the same probability distribution at all nodes in an information set. As is standard, we assume that each player never forgets information. Below, we describe the main ideas behind the extension; details are given in Appendix J.

In order to extend our results, we use the CFR regret decomposition [37; 9]. CFR defines a notion of local regret at each information set, using so-called _counterfactual values_. By minimizing the regret incurred at each information set with respect to counterfactual values, CFR guarantees that the overall regret over tree-form strategies is minimized. Importantly, counterfactual values are multilinear in the strategies of the players, and therefore they are Lipschitz functions of the strategies of the other players. Hence, using Algorithm 4 at each information set with counterfactual value and applying Theorem 5.5 begets a smooth-RM\({}^{+}\)-based algorithm that computes a sequence of iterates with regret at most \(\epsilon\) in at \(O(1/\epsilon)\) iterations and using \(\tilde{O}(\log(1/\epsilon)/\epsilon)\) gradient computations.

## 6 Numerical experiments

**Matrix games.** We compute the performance of ExRM\({}^{+}\), Stable and Smooth PRM\({}^{+}\) on the \(3\times 3\) matrix game instance from Section 2 (with step size \(\eta=0.1\)) and on \(30\) random matrix games of size \((d_{1},d_{2})=(30,40)\) with normally distributed coefficients of the payoff matrix and with step sizes \(\eta\in\{0.1,1,10\}\). We initialize our algorithms at \((1/d_{1})\mathbf{1}_{d}\), all algorithms use linear averaging, and all algorithms (except ExRM\({}^{+}\)) use alternation. The results are shown in Figure 2. Our new algorithms greatly outperform RM\({}^{+}\) and PRM\({}^{+}\) in the \(3\times 3\) matrix game; linear regression finds an asymptotic convergence rate of \(O(1/T^{2})\). More detailed results for this instance are given in Appendix K.1. For random matrix games, our algorithms ExRM\({}^{+}\), Smooth PRM\({}^{+}\) and Stable PRM\({}^{+}\) all outperform RM\({}^{+}\) for stepsize \(\eta=0.1\). ExRM\({}^{+}\) performs on par with RM\({}^{+}\) for larger values of \(\eta\), while Stable PRM\({}^{+}\) and Smooth PRM\({}^{+}\) remain very competitive, performing on par with the unstabilized version of PRM\({}^{+}\). We note that we use step sizes that are larger than the theoretical ones since the latter may be overly conservative [10; 25].

**Extensive-form games.** We implemented and evaluated our CFR-based clairvoyant algorithm (henceforth 'Clairvoyant CFR') for extensive-form games. To our knowledge, it is the first time that clairvoyant algorithms are evaluated in extensive-form games. Overall, we were unable to observe the same strong performance observed in normal-form games (Figure 2), for a combination of reasons. First, we observe that the stepsize \(\eta\) calculated in Appendix J to make the operator \(F\) a contraction in extensive-form games is prohibitively small in the games we test on, each of which has a number of sequences on the order of tens of thousands. At the same time, we observe that ignoring the issue by setting a large constant stepsize in practice often leads to non-convergence of the fixed point iterations. To sidestep both issues, we considered a variant of the algorithm which only performs a single fixed-point iteration, and uses a stepsize hyperparameter \(\eta\), where we pick the best from the set \(\{1,10,20\}\). We remark that this variant of the algorithm is clairvoyant only in spirit, and while it is a sound regret-minimization algorithm, we expect that the strong theoretical

Figure 2: Empirical performances of RM\({}^{+}\), PRM\({}^{+}\), ExRM\({}^{+}\), Stable PRM\({}^{+}\) and Smooth PRM\({}^{+}\)on our \(3\times 3\) matrix game (left plot) and on random instances for different step sizes.

guarantees of constant per-player regret do not apply. Nevertheless, in Fig. 3 we show that we are able to sometimes observe superior performance to (non-clairvoyant) predictive CFR in the four games we tried, which are described in the appendix. For both algorithms, we ignore the first 100 iterations, in which the iterates are very far from convergence. To compensate for the increased amount of computation needed at each iteration by our clairvoyant algorithm, we plot on the x-axis not the number of iterations but rather the number of gradients of the utility functions computed for each player. On the y-axis, we measure the gap to a coarse correlated equilibrium, which is equal to the maximum regret across the players, divided by the number of iterations.

## 7 Conclusion

We initiated the study of stability for \(\text{RM}^{+}\), and showed that both \(\text{RM}^{+}\)and predictive \(\text{RM}^{+}\)suffer from stability issues that can lead to slow convergence in games. We introduced two simple ideas, _restarting_ and _chopping off_, that ensure stability. Consequently, we introduced stable/smooth Predictive \(\text{RM}^{+}\), conceptual \(\text{RM}^{+}\) and Extragradient \(\text{RM}^{+}\), all with strong regret guarantees. Our results yield the first \(\text{RM}^{+}\)-based algorithms with better than \(O(\sqrt{T})\) regret guarantees, thus partially resolving the open question of whether optimism can yield theoretical speedup for \(\text{RM}^{+}\). Future directions include understanding whether our stability observations can be leveraged more directly in \(\text{RM}^{+}\) without adding our stability tricks, extending our results to general convex games, for which a regret minimizer based on Blackwell approachability similar to \(\text{RM}^{+}\) has been proposed recently [17], and combining clairvoyant updates with alternation.

Funding.J. Grand-Clement is supported by the Agence Nationale de la Recherche [Grant 11-LABX-0047] and by Hi! Paris. Christian Kroer is supported by the Office of Naval Research awards N00014-22-1-2530 and N00014-23-1-2374, and the National Science Foundation awards IIS-2147361 and IIS-2238960. Haipeng Luo and Chung-Wei Lee are supported by National Science Foundation award IIS-1943607.

## References

* [1] Bai, Y., Jin, C., Mei, S., Song, Z., and Yu, T. Efficient \(phi\)-regret minimization in extensive-form games via online mirror descent. _arXiv preprint arXiv:2205.15294_, 2022.
* [2] Bowling, M., Burch, N., Johanson, M., and Tammelin, O. Heads-up limit hold'em poker is solved. _Science_, 347(6218), January 2015.
* [3] Brown, N. and Sandholm, T. Superhuman AI for heads-up no-limit poker: Libratus beats top professionals. _Science_, pp. eaao1733, Dec. 2017.
* [4] Burch, N., Moravcik, M., and Schmid, M. Revisiting cfr+ and alternating updates. _Journal of Artificial Intelligence Research_, 64:429-443, 2019.
* [5] Chen, G. and Teboulle, M. Convergence analysis of a proximal-like minimization algorithm using bregman functions. _SIAM Journal on Optimization_, 3(3):538-543, 1993.

Figure 3: Practical performance of our variant of clairvoyant CFR (‘Cvynt CFR’) compared to predictive CFR, across four multiplayer extensive-form games. Note that on Lair’s dice, both algorithms are down to machine-precision accuracy immediately, which explains the jitter plot.

* [6] Chen, X. and Peng, B. Hedging in games: Faster convergence of external and swap regrets. 2020.
* [7] Daskalakis, C., Fishelson, M., and Golowich, N. Near-optimal no-regret learning in general games. _Advances in Neural Information Processing Systems_, 34:27604-27616, 2021.
* [8] Farina, G., Kroer, C., Brown, N., and Sandholm, T. Stable-predictive optimistic counterfactual regret minimization. 2019.
* [9] Farina, G., Kroer, C., and Sandholm, T. Online convex optimization for sequential decision processes and extensive-form games. In _AAAI Conference on Artificial Intelligence_, 2019.
* [10] Farina, G., Kroer, C., and Sandholm, T. Optimistic regret minimization for extensive-form games via dilated distance-generating functions. In _Conference on Neural Information Processing Systems_, 2019.
* [11] Farina, G., Ling, C. K., Fang, F., and Sandholm, T. Correlation in extensive-form games: Saddle-point formulation and benchmarks. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2019.
* [12] Farina, G., Kroer, C., and Sandholm, T. Faster game solving via predictive blackwell approachability: Connecting regret matching and mirror descent. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pp. 5363-5371, 2021.
* [13] Farina, G., Anagnostides, I., Luo, H., Lee, C.-W., Kroer, C., and Sandholm, T. Near-optimal no-regret learning dynamics for general convex games. In _Advances in Neural Information Processing Systems_, 2022.
* [14] Farina, G., Kroer, C., Lee, C.-W., and Luo, H. Clairvoyant regret minimization: Equivalence with memirovski's conceptual prox method and extension to general convex games. In _OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)_, 2022.
* [15] Farina, G., Lee, C.-W., Luo, H., and Kroer, C. Kernelized multiplicative weights for 0/1-polyhedral games: Bridging the gap between learning in extensive-form and normal-form games. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_. PMLR, 2022.
* [16] Freund, Y. and Schapire, R. E. Adaptive game playing using multiplicative weights. _Games and Economic Behavior_, 29(1-2):79-103, 1999.
* [17] Grand-Clement, J. and Kroer, C. Conic blackwell algorithm: Parameter-free convex-concave saddle-point solving. _Advances in Neural Information Processing Systems_, 34:9587-9599, 2021.
* [18] Grand-Clement, J. and Kroer, C. Solving optimization problems with blackwell approachability. _arXiv preprint arXiv:2202.12277_, 2022.
* [19] Hart, S. and Mas-Colell, A. A simple adaptive procedure leading to correlated equilibrium. _Econometrica_, 68(5):1127-1150, 2000.
* [20] Hazan, E. et al. Introduction to online convex optimization. _Foundations and Trends(r) in Optimization_, 2(3-4):157-325, 2016.
* [21] Hoda, S., Gilpin, A., Pena, J., and Sandholm, T. Smoothing techniques for computing nash equilibria of sequential games. _Mathematics of Operations Research_, 35(2):494-512, 2010.
* [22] Hsieh, Y., Antonakopoulos, K., and Mertikopoulos, P. Adaptive learning in continuous games: Optimal regret bounds and convergence to nash equilibrium. In Belkin, M. and Kpotufe, S. (eds.), _Conference on Learning Theory, COLT 2021_, volume 134 of _Proceedings of Machine Learning Research_, pp. 2388-2422. PMLR, 2021.
* [23] Kiwiel, K. C. Proximal minimization methods with generalized bregman functions. _SIAM journal on control and optimization_, 35(4):1142-1168, 1997.

* [24] Korpelevich, G. M. The extragradient method for finding saddle points and other problems. _Matecon_, 12:747-756, 1976.
* [25] Kroer, C., Waugh, K., Klimc-Karzan, F., and Sandholm, T. Faster algorithms for extensive-form game solving via improved smoothing functions. _Mathematical Programming_, 179(1):385-417, 2020.
* [26] Kuhn, H. W. A simplified two-person poker. In Kuhn, H. W. and Tucker, A. W. (eds.), _Contributions to the Theory of Games_, volume 1 of _Annals of Mathematics Studies, 24_, pp. 97-103. Princeton University Press, Princeton, New Jersey, 1950.
* [27] Lisy, V., Lanctot, M., and Bowling, M. H. Online monte carlo counterfactual regret minimization for search in imperfect information games. 2015.
* [28] Moravcik, M., Schmid, M., Burch, N., Lisy, V., Morrill, D., Bard, N., Davis, T., Waugh, K., Johanson, M., and Bowling, M. Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. _Science_, May 2017.
* [29] Nemirovski, A. Prox-method with rate of convergence o (1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems. _SIAM Journal on Optimization_, 15(1):229-251, 2004.
* [30] Nemirovski, A. and Yudin, D. _Problem complexity and method efficiency in optimization_. 1983.
* [31] Piliouras, G., Sim, R., and Skoulakis, S. Optimal no-regret learning in general games: Bounded regret with unbounded step-sizes via clairvoyant mwu. _arXiv preprint arXiv:2111.14737_, 2021.
* [32] Rakhlin, A. and Sridharan, K. Optimization, learning, and games with predictable sequences. In _Advances in Neural Information Processing Systems_, pp. 3066-3074, 2013.
* [33] Southey, F., Bowling, M., Larson, B., Piccione, C., Burch, N., Billings, D., and Rayner, C. Bayes' bluff: Opponent modelling in poker. July 2005.
* [34] Syrgkanis, V., Agarwal, A., Luo, H., and Schapire, R. E. Fast convergence of regularized learning in games. _Advances in Neural Information Processing Systems_, 28, 2015.
* [35] Tammelin, O. Solving large imperfect information games using cfr+. _arXiv preprint arXiv:1407.5042_, 2014.
* [36] Tammelin, O., Burch, N., Johanson, M., and Bowling, M. Solving heads-up limit texas hold'em. In _Twenty-fourth international joint conference on artificial intelligence_, 2015.
* [37] Zinkevich, M., Johanson, M., Bowling, M., and Piccione, C. Regret minimization in games with incomplete information. _Advances in neural information processing systems_, 20, 2007.

## Appendix A Proof of Lemma 2.1

Proof of Lemma 2.1.: Let us write \(\hat{\bm{R}}=\hat{\bm{x}}\). Note that

\[\operatorname{Reg}^{T}(\hat{\bm{x}}) =\sum_{t=1}^{T}\left\langle\bm{x}^{t},\bm{\ell}^{t}\right\rangle- \sum_{t=1}^{T}\left\langle\hat{\bm{x}},\bm{\ell}^{t}\right\rangle\] \[=-\sum_{t=1}^{T}\left\langle\hat{\bm{x}},\bm{f}(\bm{x}^{t},\bm{ \ell}^{t})\right\rangle\] (3) \[=-\sum_{t=1}^{T}\left\langle\hat{\bm{R}},\bm{f}(\bm{x}^{t},\bm{ \ell}^{t})\right\rangle\] \[=\operatorname{Reg}^{T}\left(\hat{\bm{R}}\right)\] (5)

where (3) follows from \(\hat{\bm{x}}^{\top}\bm{1}_{d}=1\) and the definition of the map \(\bm{f}(\cdot,\cdot)\), (4) follows from \(\left\langle\bm{R}^{t},\bm{f}(\bm{x}^{t},\bm{\ell}^{t})\right\rangle=0\) because \(\bm{x}^{t}=\bm{R}^{t}/\|\bm{R}^{t}\|_{1}\) (note that this is also trivially true when \(\bm{R}^{t}=\bm{0}\)), and (5) follows from the definition of \(\operatorname{Reg}^{T}\left(\hat{\bm{R}}\right)\). 

## Appendix B Instability of \(\text{RM}^{+}\) and predictive \(\text{RM}^{+}\)

### Proof of Theorem 3.1

Proof of Theorem 3.1.: We first prove the case for \(\text{RM}^{+}\). Since we consider \(\bm{x}^{t}\in\mathbb{R}^{2}\), we can express \(\bm{x}^{t}=(p^{t},1-p^{t})\) for some scalar \(p^{t}\in[0,1]\) (starting with \(p^{1}=1/2\)). In our counterexample, we set \(\bm{\ell}^{t}=(\ell^{t},0)\) for some scalar \(\ell^{t}\) to be specified. Consequently, we have

\[\bm{f}(\bm{x}^{t},\bm{\ell}^{t})=\bm{\ell}^{t}-\left\langle\bm{x}^{t},\bm{ \ell}^{t}\right\rangle\bm{1}_{2}=((1-p^{t})\ell^{t},\ -p^{t}\ell^{t}).\]

To make the algorithm highly unstable, we first provide an unbounded sequence of \(\ell^{t}\) so that the resulting \(\bm{R}^{t}\) alternates between vectors with the same value on both entries and vectors with only the first entry being 0, which means \(p^{t}\) by definition alternates between \(1/2\) and \(0\). Noting that RM+ is scale-invariant to the loss sequence, our proof is completed by normalizing the losses so that they all lie in \([-1,1]\).

Specifically, we set \(\ell^{1}=2\), which gives \(\bm{f}(\bm{x}^{1},\bm{\ell}^{1})=(1,-1)\), \(\bm{R}^{2}=(0,1)\), and \(p^{2}=0\). Then for \(t\geq 2\) we set \(\ell^{t}=-2^{(t-2)/2}\) when \(t\) is even and \(\ell^{t}=2^{(t-1)/2}\) when \(t\) is odd. By direct calculation it is not hard to verify that

\[\bm{f}(\bm{x}^{t},\bm{\ell}^{t}) =(-2^{(t-2)/2},0), \bm{R}^{t+1} =(2^{(t-2)/2},2^{(t-2)/2}),\] \[p^{t+1} =\tfrac{1}{2}\text{ when }t\text{ is even, and }\bm{R}^{t+1} =(0,2^{(t-1)/2}),\]

\(p^{t+1}=0\) when \(t\) is odd, completing the counterexample for \(\text{RM}^{+}\).

It remains to prove the case for predictive \(\text{RM}^{+}\), where \(\bm{m}^{t}=\bm{f}(\bm{x}^{t-1},\bm{\ell}^{t-1})\). Initially, let \(\ell^{1}=4,\ \ell^{2}=-1\). Recall that

\[\bm{f}(\bm{x}^{t},\bm{\ell}^{t}) =\bm{\ell}^{t}-\left\langle\bm{x}^{t},\bm{\ell}^{t}\right\rangle \bm{1}_{2}=((1-p^{t})\ell^{t},\ -p^{t}\ell^{t}).\]

By direct calculation, we have

\[\bm{f}(\bm{x}^{1},\bm{\ell}^{1}) =(2,-2), \bm{R}^{2} =(0,2), \hat{\bm{R}}^{2} =(-2,4), p^{2} =0\] \[\bm{f}(\bm{x}^{2},\bm{\ell}^{2}) =(-1,0), \bm{R}^{3} =(1,2), \hat{\bm{R}}^{3} =(2,2), p^{3} =\frac{1}{2}\]Thereafter, we set \(\ell^{t}=2^{(t+1)/2}\) when \(t\) is odd and \(\ell^{t}=-2^{(t-2)/2}\) when \(t\) is even. The updates for the next 4 steps are:

\[\bm{f}(\bm{x}^{3},\bm{\ell}^{3}) =(2,-2), \bm{R}^{4} =(0,4), \bm{\hat{R}}^{4} =(0,6), p^{4} =0\] \[\bm{f}(\bm{x}^{4},\bm{\ell}^{4}) =(-2,0), \bm{R}^{5} =(2,4), \bm{\hat{R}}^{5} =(4,4), p^{5} =\frac{1}{2}\] \[\bm{f}(\bm{x}^{5},\bm{\ell}^{5}) =(4,-4), \bm{R}^{6} =(0,8), \bm{\hat{R}}^{6} =(0,12), p^{6} =0\] \[\bm{f}(\bm{x}^{6},\bm{\ell}^{6}) =(-4,0), \bm{R}^{7} =(4,8), \bm{\hat{R}}^{7} =(8,8), p^{7} =\frac{1}{2}.\]

It is not hard to verify (by induction) that

\[\bm{f}(\bm{x}^{t},\bm{\ell}^{t})=(2^{(t-1)/2},-2^{(t-1)/2}), \bm{R}^{t+1} =(0,2^{(t+1)/2}), \hat{\bm{R}}^{t+1} =(0,2^{(t+1)/2}+2^{(t-1)/2}), p^{t+1} =0\]

when \(t\) is odd and

\[\bm{f}(\bm{x}^{t},\bm{\ell}^{t})=(-2^{(t-2)/2},0), \bm{R}^{t+1} =(2^{(t-2)/2},2^{t/2}), \hat{\bm{R}}^{t+1} =(2^{t/2},2^{t/2}), p^{t+1} =\frac{1}{2}\]

when \(t\) is even. This completes the proof. 

**Remark B.1**.: _The losses are unbounded in the examples, but note that the update rules for the algorithms imply that all the algorithms remain unchanged after scaling the losses, so we can rescale them accordingly. Specifically, if we have a loss sequence \(\ell^{1},\ldots,\ell^{T}\), we can define \(L_{T}=\max\{|\ell^{1}|,\ldots,|\ell^{T}|\}\) and consider another loss sequence \(\ell^{1}/L_{T},\ldots,\ell^{T}/L_{T}\), which is bounded in \([-1,1]\) and will make the algorithms produce the same outputs._

### Counterexample on \(3\times 3\) matrix game for \(\text{RM}^{+}\)

## Appendix C Proof of Proposition 1 and Corollary 3.2

We start with a couple of technical lemmas.

**Lemma C.1**.: _Given any \(\bm{y}\in\mathbb{R}^{d}_{+}\) and \(\bm{x}\in\mathbb{R}^{d}\) such that \(\bm{1}^{\top}\bm{x}=0\),_

\[(\bm{x}^{\top}\bm{y})^{2}\leq\frac{d-1}{d}\|\bm{x}\|_{2}^{2}\,\|\bm{y}\|_{2}^{ 2}.\]

Figure 5: Individual regret of each player for \(\text{RM}^{+}\).

Proof.: If \(\bm{x}=\bm{0}\) the claim is trivial, so we focus on the other case. Let \(\bm{\xi}\) be the (Euclidean) projection of \(\bm{y}\) onto the orthogonal complement of \(\operatorname{span}\{\bm{x},\bm{1}\}\). Since by hypothesis \(\bm{1}\perp\bm{x}\), it holds that

\[\bm{y}=\frac{\bm{y}^{\top}\bm{x}}{\|\bm{x}\|_{2}^{2}}\bm{x}+\frac{\bm{1}^{\top} \bm{y}}{\|\bm{1}\|_{2}^{2}}\bm{1}+\bm{\xi}\]

and therefore

\[\|\bm{y}\|_{2}^{2}=\frac{(\bm{y}^{\top}\bm{x})^{2}}{\|\bm{x}\|_{2}^{2}}+\frac{ (\bm{1}^{\top}\bm{y})^{2}}{\|\bm{1}\|_{2}^{2}}+\|\bm{\xi}\|_{2}^{2}\geq\frac{( \bm{y}^{\top}\bm{x})^{2}}{\|\bm{x}\|_{2}^{2}}+\frac{(\bm{1}^{\top}\bm{y})^{2}} {\|\bm{1}\|_{2}^{2}}\] (6)

Using the hypothesis that \(\bm{y}\geq\bm{0}\), we can bound

\[\bm{1}^{\top}\bm{y}=\|\bm{y}\|_{1}\geq\|\bm{y}\|_{2},\]

where we used the well-known inequality between the \(\ell_{1}\)-norm and the \(\ell_{2}\)-norm. Substituting the previous inequality into (6), and using the fact that \(\|\bm{1}\|_{2}^{2}=d\),

\[\|\bm{y}\|_{2}^{2}\geq\frac{(\bm{y}^{\top}\bm{x})^{2}}{\|\bm{x}\|_{2}^{2}}+ \frac{\|\bm{y}\|_{2}^{2}}{d}.\]

Rearranging the terms yields the statement. 

**Lemma C.2**.: _For any \(\hat{\bm{y}}\in\mathbb{R}_{+}^{d}\) such that \(\|\hat{\bm{y}}\|_{2}=1\), \(\bm{1}^{\top}\hat{\bm{y}}\neq 0\) and for any \(\bm{x}\in\mathbb{R}^{d}\) such that \(\bm{1}^{\top}\bm{x}=1\),_

\[\bigg{(}\frac{1}{\bm{1}^{\top}\hat{\bm{y}}}-\bm{x}^{\top}\hat{\bm{y}}\bigg{)} ^{2}\leq(d-1)\cdot\big{\|}(\bm{x}^{\top}\hat{\bm{y}})\hat{\bm{y}}-\bm{x}\big{\| _{2}^{2}}.\]

Proof.: The main idea of the proof is to introduce

\[\bm{z}\coloneqq\bm{x}-\frac{\hat{\bm{y}}}{\bm{1}^{\top}\hat{\bm{y}}}.\]

Note that \(\bm{1}^{\top}\bm{z}=\bm{1}^{\top}\bm{x}-1=0\). Furthermore,

\[\bm{x}^{\top}\hat{\bm{y}}=\bigg{(}\bm{z}+\frac{\hat{\bm{y}}}{\bm{1}^{\top} \hat{\bm{y}}}\bigg{)}^{\top}\hat{\bm{y}}=\bm{z}^{\top}\hat{\bm{y}}+\frac{1}{ \bm{1}^{\top}\hat{\bm{y}}}.\]

Substituting the previous equality in the statement, we obtain

\[\bigg{(}\frac{1}{\bm{1}^{\top}\hat{\bm{y}}}-\bm{x}^{\top}\hat{ \bm{y}}\bigg{)}^{2}-(d-1)\cdot\big{\|}(\bm{x}^{\top}\hat{\bm{y}})\hat{\bm{y}}- \bm{x}\big{\|}_{2}^{2}\] \[=(\bm{z}^{\top}\hat{\bm{y}})^{2}-(d-1)\cdot\bigg{\|}\bigg{(}\bm{ z}^{\top}\hat{\bm{y}}+\frac{1}{\bm{1}^{\top}\hat{\bm{y}}}\Big{)}\hat{\bm{y}}- \bm{z}-\frac{\hat{\bm{y}}}{\bm{1}^{\top}\hat{\bm{y}}}\bigg{\|}_{2}^{2}\] \[=(\bm{z}^{\top}\hat{\bm{y}})^{2}-(d-1)\cdot\big{\|}(\bm{z}^{\top} \hat{\bm{y}})\hat{\bm{y}}-\bm{z}\big{\|}_{2}^{2}\] \[=(\bm{z}^{\top}\hat{\bm{y}})^{2}-(d-1)\bigg{(}(\bm{z}^{\top}\hat{ \bm{y}})^{2}+\|\bm{z}\|_{2}^{2}-2(\bm{z}^{\top}\hat{\bm{y}})^{2}\bigg{)}\] \[=d\bigg{(}(\bm{z}^{\top}\hat{\bm{y}})^{2}-\frac{d-1}{d}\|\bm{z}\|_ {2}^{2}\bigg{)},\]

where we used the hypothesis that \(\|\hat{\bm{y}}\|_{2}^{2}=1\) in the third equality. Using the inequality of Lemma C.1 concludes the proof. 

We are now ready to prove Proposition 1.

Proof of Proposition 1.: If \(\bm{y}=\bm{0}\), the statement holds trivially. Hence, we focus on the case \(\bm{y}\neq 0\). Let \(\hat{\bm{y}}\coloneqq\bm{y}/\|\bm{y}\|_{2}\) be the direction of \(\bm{y}\); clearly, \(\|\hat{\bm{y}}\|_{2}=1\). Note that

\[\|\bm{y}-\bm{x}\|_{2}^{2} =\big{(}\|\bm{y}\|_{2}-\bm{x}^{\top}\hat{\bm{y}}\big{)}^{2}+\big{\|} \bm{x}-(\bm{x}^{\top}\hat{\bm{y}})\,\hat{\bm{y}}\big{\|}_{2}^{2}\] \[\geq\big{\|}\bm{x}-(\bm{x}^{\top}\hat{\bm{y}})\,\hat{\bm{y}}\big{\|} _{2}^{2}\]\[=\left(\mathbf{1}^{\top}\bm{x}\right)^{2}\left\|\bm{g}(\bm{x})- \left(\bm{g}(\bm{x})^{\top}\hat{\bm{y}}\right)\hat{\bm{y}}\right\|_{2}^{2}\] \[\geq\left\|\bm{g}(\bm{x})-\left(\bm{g}(\bm{x})^{\top}\hat{\bm{y}} \right)\hat{\bm{y}}\right\|_{2}^{2},\] (7)

where we used the hypothesis that \(\mathbf{1}^{\top}\bm{x}\geq 1\) in the last step. On the other hand, using Lemma C.2 (note that \(\mathbf{1}^{\top}\hat{\bm{y}}\neq 0\) since \(\bm{y}\neq 0\) by hypothesis),

\[=\frac{1}{d}\big{\|}\bm{g}(\bm{x})-\left(\bm{g}(\bm{x})^{\top} \hat{\bm{y}}\right)\hat{\bm{y}}\big{\|}_{2}^{2}\] \[+\frac{d-1}{d}\big{\|}\bm{g}(\bm{x})-\left(\bm{g}(\bm{x})^{\top} \hat{\bm{y}}\right)\hat{\bm{y}}\big{\|}_{2}^{2}\] \[\geq\frac{1}{d}\big{\|}\bm{g}(\bm{x})-\left(\bm{g}(\bm{x})^{\top} \hat{\bm{y}}\right)\hat{\bm{y}}\big{\|}_{2}^{2}+\frac{1}{d}\bigg{(}\frac{1}{ \mathbf{1}^{\top}\hat{\bm{y}}}-\bm{g}(\bm{x})^{\top}\hat{\bm{y}}\bigg{)}^{2}\] \[=\frac{1}{d}\bigg{(}\big{\|}\bm{g}(\bm{x})\big{\|}_{2}^{2}+\frac{ 1}{(\mathbf{1}^{\top}\hat{\bm{y}})^{2}}-2\frac{\bm{g}(\bm{x})^{\top}\hat{\bm{y }}}{\mathbf{1}^{\top}\hat{\bm{y}}}\bigg{)}\] \[=\frac{1}{d}\bigg{(}\big{\|}\bm{g}(\bm{x})\big{\|}_{2}^{2}+\|\bm{ g}(\bm{y})\|_{2}^{2}-2\bm{g}(\bm{x})^{\top}\bm{g}(\bm{y})\bigg{)}\] \[=\frac{1}{d}\|\bm{g}(\bm{y})-\bm{g}(\bm{x})\|_{2}^{2}.\] (8)

Combining (7) and (8), we obtain the statement. 

Proof of Corollary 3.2.: The condition means that \(\mathbf{1}^{\top}\frac{\bm{R}^{t}}{R_{0}}\geq 1\) and

\[\big{\|}\bm{x}^{t+1}-\bm{x}^{t}\big{\|}_{2} \leq\sqrt{d}\bigg{\|}\frac{\bm{R}^{t+1}}{R_{0}}-\frac{\bm{R}^{t}} {R_{0}}\bigg{\|}_{2}\] (by Proposition 1) \[\leq\frac{\sqrt{d}}{R_{0}}\left\|\bm{f}(\bm{x}^{t},\bm{\ell}^{t} )\right\|_{2}\] \[\leq\frac{\sqrt{d}}{R_{0}}\left(B_{u}+\sqrt{d}\left\|\bm{x}^{t} \right\|_{2}\left\|\bm{\ell}^{t}\right\|_{2}\right)\] (by (1)) \[\leq\frac{2dB_{u}}{R_{0}}\]

## Appendix D Proof of Theorem 4.1

Proof of Theorem 4.1.: When the algorithm restarts, the accumulated regret is negative to all actions, so it is sufficient to consider the regret from \(T_{0}\), the round when the last restart happens to the end. In that case, we can analyze the algorithm as a normal predictive regret matching algorithm. By Proposition 5 in [12], we have that the regret for player \(i\) is bounded by

\[\mathrm{Reg}_{i}^{T}(\bm{x}^{*})\leq\frac{\|\bm{x}^{*}-\bm{z}_{i}^{T_{0}}\|_{ 2}^{2}}{2\eta}+\eta\sum_{t=T_{0}}^{T}\big{\|}\bm{f}_{i}^{t}-\bm{m}_{i}^{t} \big{\|}^{2}-\frac{1}{8\eta}\sum_{T=T_{0}}^{T}\big{\|}\bm{z}_{i}^{t+1}-\bm{z}_ {i}^{t}\big{\|}^{2}\,,\] (9)

where \(\bm{z}_{i}^{T_{0}}=(R_{0},\ldots,R_{0})\) and \(\big{(}\bm{f}_{i}^{t-1}\big{)}_{i\in[n]}=F(\bm{z}^{t-1})\). When setting \(\bm{m}_{i}^{t}=\bm{f}_{i}^{t-1}\), then \(\|\bm{f}_{i}^{t}-\bm{m}_{i}^{t}\|\) can be bounded by

\[\big{\|}\bm{f}_{i}^{t}-\bm{m}_{i}^{t}\big{\|}_{2}=\big{\|}\langle \bm{x}_{i}^{t},\bm{\ell}_{i}^{t}\rangle\,\mathbf{1}_{d_{i}}-\big{\langle}\bm{x }_{i}^{t-1},\bm{\ell}_{i}^{t-1}\rangle\,\mathbf{1}_{d_{i}}-\big{(}\bm{\ell}_{i} ^{t}-\bm{\ell}_{i}^{t-1}\big{)}\big{\|}_{2}\]\[\left\|\bm{g}(\bm{z})-\bm{g}(\bm{w})\right\|_{2} =\left\|\bm{g}(\bm{z}/R_{0})-\bm{g}(\bm{w}/R_{0})\right\|_{2}\] \[\leq\sqrt{d}\left\|\frac{\bm{z}}{R_{0}}-\frac{\bm{w}}{R_{0}} \right\|_{2}\] (by Proposition 1) \[\leq\frac{\sqrt{d}}{R_{0}}\left\|\eta\bm{f}(\bm{x},\bm{\ell}) \right\|_{2}\] \[\leq\frac{\eta\sqrt{d}}{R_{0}}\left(\left\|\bm{\ell}\right\|_{2}+ \left\|\left\langle\bm{x},\bm{\ell}\right\rangle\bm{1}_{d}\right\|_{2}\right)\] \[\leq\frac{\eta\sqrt{d}}{R_{0}}\left(B_{u}+\sqrt{d}\left\|\bm{x} \right\|_{2}\left\|\bm{\ell}\right\|_{2}\right)\] (by (1)) \[\leq\frac{2\eta dB_{u}}{R_{0}}.\]Proof of Theorem 4.2

We write \((\bm{R}_{1}^{t},...,\bm{R}_{n}^{t})=\bm{z}^{t}\). Let us consider the regret \(\operatorname{Reg}_{i}^{T}(\hat{\bm{x}}_{i})\) of player \(i\in\{1,...,n\}\). Lemma 2.1 shows that

\[\operatorname{Reg}_{i}^{T}(\hat{\bm{x}}_{i})=\operatorname{Reg}_{i}^{T}\left( \hat{\bm{R}}_{i}\right)\]

with \(\operatorname{Reg}_{i}^{T}\left(\hat{\bm{R}}_{i}\right)\) the regret against \(\hat{\bm{R}}_{i}=\hat{\bm{x}}_{i}\) incurred by a decision-maker choosing the decisions \(\left(\bm{R}_{i}^{t}\right)_{t\geq 1}\) and facing the sequence of losses \(\left(\bm{f}_{i}^{t}\right)_{t\geq 1}\), with \(\bm{f}_{i}^{t}=\bm{\ell}_{i}^{t}-\left\langle\bm{x}_{i}^{t},\bm{\ell}_{i}^{t} \right\rangle\bm{1}_{d_{i}}\):

\[\operatorname{Reg}_{i}^{T}\left(\hat{\bm{R}}_{i}\right)=\sum_{t=1}^{T}\left\langle \bm{f}_{i}^{t},\bm{R}_{i}^{t}-\hat{\bm{R}}_{i}\right\rangle\] (10)

Note that \(\bm{R}_{i}^{1},...,\bm{R}_{i}^{T}\) is computed by Predictive OMD with \(\Delta_{\geq}^{d_{i}}\) as a decision set, \(\bm{f}_{i}^{1},...,\bm{f}_{i}^{T}\) as the sequence of losses and \(\bm{m}_{i}^{1},...,\bm{m}_{i}^{T}\) as the sequence of predictions. Therefore, Proposition 5 in [12] applies, and we can write the following regret bound:

\[\begin{split}\operatorname{Reg}_{i}^{T}\left(\hat{\bm{R}}_{i} \right)&\leq\frac{\left\|\bm{w}_{i}^{0}-\hat{\bm{x}}_{i}\right\| _{2}^{2}}{2\eta}+\eta\sum_{t=1}^{T}\left\|\bm{f}_{i}^{t}-\bm{m}_{i}^{t}\right\| _{2}^{2}\\ &-\frac{1}{8\eta}\sum_{t=1}^{T}\left\|\bm{R}_{i}^{t+1}-\bm{R}_{i} ^{t}\right\|_{2}^{2}.\end{split}\] (11)

Since we maintain \(\bm{R}_{i}^{t}\in\Delta_{\geq}^{d_{i}}\) at every iteration, using Proposition 1 we find that

\[\left\|\bm{x}_{i}^{t+1}-\bm{x}_{i}^{t}\right\|_{2}^{2}\leq d_{i}\left\|\bm{R} _{i}^{t+1}-\bm{R}_{i}^{t}\right\|_{2}^{2}.\]

Plugging this into (11), we obtain

\[\begin{split}\operatorname{Reg}_{i}^{T}\left(\hat{\bm{R}}_{i} \right)&\leq\frac{\left\|\bm{w}_{i}^{0}-\hat{\bm{x}}_{i}\right\| _{2}^{2}}{2\eta}+\eta\sum_{t=1}^{T}\left\|\bm{f}_{i}^{t}-\bm{m}_{i}^{t}\right\| _{2}^{2}\\ &-\frac{1}{8d_{i}\eta}\sum_{t=1}^{T}\left\|\bm{x}^{t+1}-\bm{x}^{t} \right\|_{2}^{2}.\end{split}\]

Using \(\|\cdot\|_{2}\leq\|\cdot\|_{1}\leq\sqrt{d_{i}}\|\cdot\|_{2}\), we obtain

\[\begin{split}\operatorname{Reg}_{i}^{T}\left(\hat{\bm{R}}_{i} \right)&\leq\alpha+\beta\sum_{t=1}^{T}\left\|\bm{f}_{i}^{t}-\bm{m} _{i}^{t}\right\|_{1}^{2}\\ &-\gamma\sum_{t=1}^{T}\|\bm{x}^{t+1}-\bm{x}^{t}\|_{1}^{2}.\end{split}\]

with \(\alpha=\frac{\|\bm{w}_{i}^{0}-\hat{\bm{x}}_{i}\|_{2}^{2}}{2\eta},\beta=d_{i} \eta,\gamma=\frac{1}{8d_{i}^{2}\eta}\). To conclude as in Theorem 4 in [34] we need \(\beta\leq\gamma/(n-1)^{2}\), i.e., \(\eta=\frac{1}{2\sqrt{2}(n-1)d_{i}^{3/2}}\). Therefore, using \(\eta=\frac{1}{2\sqrt{2}(n-1)\max_{i}\{d_{i}^{3/2}\}}\), we conclude that the sum of the individual regrets is bounded by

\[O\left(n^{2}\max_{i=1,...,n}\{d_{i}^{3/2}\}\max_{i=1,...,n}\{\|\bm{w}_{i}^{0}- \hat{\bm{x}}_{i}\|_{2}^{2}\}\right).\]

## Appendix F Proof of Theorem 5.3

Proof of Lemma 5.1.: The first-order optimality condition gives

\[\left\langle\eta\bm{\ell}^{t}+\bm{z}^{t}-\bm{z}^{t-1},\hat{\bm{z}}-\bm{z}^{t} \right\rangle\geq 0\;\forall\hat{\bm{z}}\in\mathcal{Z}.\]Rearranging gives that for any \(\hat{\bm{z}}\in\mathcal{Z}\), we have

\[\langle\eta\bm{\ell}^{t},\hat{\bm{z}}-\bm{z}^{t}\rangle\geq\langle\bm{z}^{t-1}- \bm{z}^{t},\hat{\bm{z}}-\bm{z}^{t}\rangle=\frac{1}{2}\|\bm{z}^{t}-\hat{\bm{z}} \|_{2}^{2}-\frac{1}{2}\|\bm{z}^{t-1}-\hat{\bm{z}}\|_{2}^{2}+\frac{1}{2}\|\bm{z} ^{t}-\bm{z}^{t-1}\|_{2}^{2}.\]

Multiplying by \(-1\) and summing over all \(t=1,...,T\) gives the regret bound:

\[\sum_{t=1}^{T}\langle\eta\bm{\ell}^{t},\bm{z}^{t}-\hat{\bm{z}}\rangle\leq\frac {1}{2}\|\bm{z}^{0}-\hat{\bm{z}}\|_{2}^{2}-\frac{1}{2}\|\bm{z}^{T}-\hat{\bm{z}} \|_{2}^{2}-\sum_{t=1}^{T}\frac{1}{2}\|\bm{z}^{t}-\bm{z}^{t-1}\|_{2}^{2}\leq \frac{1}{2}\|\bm{z}^{0}-\hat{\bm{z}}\|_{2}^{2}.\]

Proof of Lemma 5.2.: Let \(\bm{x},\bm{x}^{\prime}\in\Delta\) and \(i\in\{1,...,n\}\). Let us write \(\bm{\ell}_{i}=-\nabla_{\bm{x}_{i}}u_{i}(\bm{x}),\bm{\ell}_{i}^{\prime}=-\nabla _{\bm{x}_{i}}u_{i}(\bm{x}^{\prime})\). We have, for \(i\in\{1,...,n\}\),

\[\|\bm{f}\left(\bm{x}_{i},\bm{\ell}_{i}\right)-\bm{f}\left(\bm{x}_ {i}^{\prime},\bm{\ell}_{i}^{\prime}\right)\|_{2}^{2}\] \[=\sum_{j=1}^{d_{i}}\left((\bm{x}_{i}-\bm{e}_{j})^{\top}\bm{\ell}_ {i}-(\bm{x}_{i}^{\prime}-\bm{e}_{j})^{\top}\bm{\ell}_{i}^{\prime}\right)^{2}\] \[=\sum_{j=1}^{d_{i}}((\bm{x}_{i}-\bm{e}_{j})^{\top}\bm{\ell}_{i}-( \bm{x}_{i}^{\prime}-\bm{e}_{j})^{\top}\bm{\ell}_{i}+(\bm{x}_{i}^{\prime}-\bm {e}_{j})^{\top}\bm{\ell}_{i}-(\bm{x}_{i}^{\prime}-\bm{e}_{j})^{\top}\bm{\ell}_ {i}^{\prime})^{2}\] \[=\sum_{j=1}^{d_{i}}\left((\bm{x}_{i}-\bm{x}_{i}^{\prime})^{\top} \bm{\ell}_{i}+(\bm{x}_{i}^{\prime}-\bm{e}_{j})^{\top}(\bm{\ell}_{i}-\bm{\ell} _{i}^{\prime})\right)^{2}\] \[\leq 2d_{i}\left((\bm{x}_{i}-\bm{x}_{i}^{\prime})^{\top}\bm{\ell} _{i}\right)^{2}+2\sum_{j=1}^{d_{i}}\left((\bm{x}_{i}^{\prime}-\bm{e}_{j})^{ \top}(\bm{\ell}_{i}-\bm{\ell}_{i}^{\prime})\right)^{2}\] \[\leq 2d_{i}\|\bm{x}_{i}-\bm{x}_{i}^{\prime}\|_{2}^{2}\|\bm{\ell}_ {i}\|_{2}^{2}+2\sum_{j=1}^{d_{i}}\|\bm{x}_{i}^{\prime}-\bm{e}_{j}\|_{2}^{2}\| \bm{\ell}_{i}-\bm{\ell}_{i}^{\prime}\|_{2}^{2},\]

where the last inequality follows from Cauchy-Schwarz inequality. Now from (1) and the definition of \(\bm{\ell}_{i},\bm{\ell}_{i}^{\prime}\), we have

\[\|\bm{\ell}_{i}\|_{2}\leq B_{u},\|\bm{\ell}_{i}-\bm{\ell}_{i}^{\prime}\|_{2} \leq L_{u}\|\bm{x}-\bm{x}^{\prime}\|_{2}.\]

This yields

\[\|\bm{f}\left(\bm{x}_{i},\bm{\ell}_{i}\right)-\bm{f}\left(\bm{x}_ {i}^{\prime},\bm{\ell}_{i}^{\prime}\right)\|_{2}^{2} \leq 2d_{i}B_{u}^{2}\|\bm{x}_{i}-\bm{x}_{i}^{\prime}\|_{2}^{2}+4d_{i} L_{u}^{2}\|\bm{x}-\bm{x}^{\prime}\|_{2}^{2}\] \[\leq\left(2d_{i}B_{u}^{2}+4d_{i}L_{u}^{2}\right)\|\bm{x}-\bm{x}^{ \prime}\|_{2}^{2}.\]

Since the function \(\bm{g}\) is \(\sqrt{d_{i}}\)-Lipschitz continuous over each decision set \(\Delta_{>}^{d_{i}}\) (Proposition 1), we have shown that the Lipschitz constant of \(F\) is \(L_{F}=\left(\max_{i}d_{i}\right)\sqrt{2B_{u}^{2}+4L_{u}^{2}}\). 

We are now ready to prove Theorem 5.3. We write \(\left(\bm{R}_{1}^{t},...,\bm{R}_{n}^{t}\right)=\bm{z}^{t}\).

Proof of Theorem 5.3.: We use the fact that \(\left(\bm{z}^{t}\right)_{t\geq 1}\) is computed following the Cheating OMD update with \(\bm{\ell}^{t}=F(\bm{z}^{t})\) at every iteration \(t\geq 1\). Therefore, the first-order optimality condition in \(\bm{z}^{t}=\Pi_{\bm{z}^{t-1},\mathcal{X}_{\geq}}(\eta F(\bm{z}^{t}))\) yields

\[\langle\eta F(\bm{z}^{t})+\bm{z}^{t}-\bm{z}^{t-1},\hat{\bm{z}}-\bm{z}^{t} \rangle\geq 0\;\forall\hat{\bm{z}}\in\mathcal{X}_{\geq}.\]

Similarly as for the proof of Lemma 5.1, we obtain that for any \(\hat{\bm{z}}\in\mathcal{X}_{\geq}\), we have

\[\langle\eta F(\bm{z}^{t}),\hat{\bm{z}}-\bm{z}^{t}\rangle\geq\frac{1}{2}\|\bm{z} ^{t}-\hat{\bm{z}}\|_{2}^{2}-\frac{1}{2}\|\bm{z}^{t-1}-\hat{\bm{z}}\|_{2}^{2}+ \frac{1}{2}\|\bm{z}^{t}-\bm{z}^{t-1}\|_{2}^{2}.\]

Let \(i\in\{1,...,n\}\). We apply the inequality above to the vector \(\hat{\bm{z}}\in\mathcal{X}_{\geq}=\times_{i=1}^{n}\Delta_{\geq}^{d_{i}}\) defined as \(\hat{\bm{z}}_{j}=\bm{z}_{j}^{t}\) for \(j\neq i\) and \(\hat{\bm{z}}_{i}=\hat{\bm{R}}_{i}\) for some \(\hat{\bm{R}}_{i}\in\Delta_{\geq}^{d_{i}}\). This yields, for any \(\hat{\bm{R}}_{i}\in\Delta_{\geq}^{d_{i}}\), for \(\bm{x}_{j}^{t}=\bm{g}(\bm{R}_{j}^{t})\) and \((\bm{\ell}_{1}^{t},...,\bm{\ell}_{n}^{t})=G(\bm{x}^{t})\) for all \(j\in\{1,...,n\}\),

\[\langle\eta\bm{f}(\bm{x}^{t},\bm{\ell}^{t}),\hat{\bm{R}}_{i}-\bm{R}_{i}^{t} \rangle\geq\frac{1}{2}\|\bm{R}_{i}^{t}-\hat{\bm{R}}_{i}\|_{2}^{2}-\frac{1}{2} \|\bm{R}_{i}^{t-1}-\hat{\bm{R}}_{i}\|_{2}^{2}+\frac{1}{2}\|\bm{R}_{i}^{t}-\bm{R}_ {i}^{t-1}\|_{2}^{2}.\]Summing the above inequality for \(t=1,...,T\), we obtain our bound on the individual regrets of each player: for any \(\hat{\bm{R}}_{i}\in\Delta_{\geq}^{d_{i}}\), we have

\[\sum_{t=1}^{T}\langle\eta\bm{f}(\bm{x}^{t},\bm{\ell}^{t}),\bm{R}_{i}^{t}-\hat{ \bm{R}}_{i}\rangle\leq\frac{1}{2}\|\bm{R}_{i}^{0}-\hat{\bm{R}}_{i}\|_{2}^{2}- \frac{1}{2}\|\bm{R}_{i}^{T}-\hat{\bm{R}}_{i}\|_{2}^{2}-\sum_{t=1}^{T}\frac{1}{ 2}\|\bm{R}_{i}^{t}-\bm{R}_{i}^{t-1}\|_{2}^{2}\leq\frac{1}{2}\|\bm{R}_{i}^{0}- \hat{\bm{R}}_{i}\|_{2}^{2}.\]

Note that from Lemma 2.1, we have that the individual regret of player \(i\)

\[\mathrm{Reg}_{i}^{T}(\hat{\bm{x}}_{i})=\sum_{t=1}^{T}\big{\langle}\nabla_{\bm {x}_{i}}u_{i}^{t}(\bm{x}^{t}),\hat{\bm{x}}_{i}-\bm{x}_{i}^{t}\big{\rangle}\]

against a decision \(\hat{\bm{x}}_{i}\in\Delta^{d_{i}}\) is equal to \(\sum_{t=1}^{T}\langle\bm{f}(\bm{x}^{t},\bm{\ell}^{t}),\bm{R}_{i}^{t}-\hat{\bm{ R}}_{i}\rangle\) for \(\hat{\bm{R}}_{i}=\hat{\bm{x}}_{i}\). Therefore, we conclude that

\[\mathrm{Reg}_{i}^{T}(\hat{\bm{x}}_{i})\leq\frac{1}{2\eta}\|\hat{\bm{z}}_{i}^{0 }-\hat{\bm{x}}_{i}\|_{2}^{2}.\]

This concludes the proof of Theorem 5.3. 

## Appendix G Proof of Theorem 5.5

Proof of Theorem 5.5.: At iteration \(t\geq 1\), let \(\bm{w}^{t}\in\mathcal{X}_{\geq}\) such that \(\|\bm{w}^{t}-\Pi_{\bm{z}^{t-1},\mathcal{X}_{\geq}}\left(\eta F(\bm{w}^{t}) \right)\|_{2}\leq\epsilon^{(t)}.\) Then the first order optimality condition gives, for \(\bm{z}^{t}=\Pi_{\bm{z}^{t-1},\mathcal{X}_{\geq}}(\eta F(\bm{w}^{t}))\),

\[\langle\eta F(\bm{w}^{t}),\hat{\bm{z}}-\bm{z}^{t}\rangle\geq\frac{1}{2}\|\bm{ z}^{t}-\hat{\bm{z}}\|_{2}^{2}-\frac{1}{2}\|\bm{z}^{t-1}-\hat{\bm{z}}\|_{2}^{2}+ \frac{1}{2}\|\bm{z}^{t}-\bm{z}^{t-1}\|_{2}^{2},\forall\ \hat{\bm{z}}\in\mathcal{X}_{\geq}.\]

Let us fix a player \(i\in\{1,...,n\}\). We apply the inequality above with \(\hat{\bm{z}}_{j}=\bm{z}_{j}^{t}\) for \(j\neq i\). This yields, for \(\bm{x}_{p}=\bm{g}(\bm{w}_{p}^{t}),\forall\ p\in\{1,...,n\}\) and \(\bm{\ell}_{i}^{t}=-\nabla_{\bm{x}_{i}}u_{i}(\bm{x})\),

\[\langle\eta\bm{f}(\bm{g}(\bm{w}_{i}^{t}),\bm{\ell}_{i}^{t}),\hat{\bm{z}}_{i}- \bm{z}_{i}^{t}\rangle\geq\frac{1}{2}\|\bm{z}_{i}^{t}-\hat{\bm{z}}_{i}\|_{2}^{2 }-\frac{1}{2}\|\bm{z}_{i}^{t-1}-\hat{\bm{z}}_{i}\|_{2}^{2}+\frac{1}{2}\|\bm{z} _{i}^{t}-\bm{z}_{i}^{t-1}\|_{2}^{2},\forall\ \hat{\bm{z}}_{i}\in\Delta^{d_{i}}.\]

We now upper bound the left-hand side of the previous inequality. Note that

\[\langle\eta\bm{f}(\bm{g}(\bm{w}_{i}^{t}),\bm{\ell}_{i}^{t}),\hat{\bm{z}}_{i}- \bm{z}_{i}^{t}\rangle=\langle\eta\bm{f}(\bm{g}(\bm{w}_{i}^{t}),\bm{\ell}_{i}^{ t}),\hat{\bm{z}}_{i}-\bm{w}_{i}^{t}\rangle+\langle\eta\bm{f}(\bm{g}(\bm{w}_{i}^{t }),\bm{\ell}_{i}^{t}),\bm{w}_{i}^{t}-\bm{z}_{i}^{t}\rangle.\]

Cauchy-Schwarz inequality ensures that

\[\langle\eta\bm{f}(\bm{g}(\bm{w}_{i}^{t}),\bm{\ell}_{i}^{t}),\bm{w}_{i}^{t}-\bm {z}_{i}^{t}\rangle\leq\eta\|\bm{f}(\bm{g}(\bm{w}_{i}^{t}),\bm{\ell}_{i}^{t})\| _{2}\|\bm{w}_{i}^{t}-\bm{z}_{i}^{t}\|_{2}.\]

Note that \(\Pi_{\bm{z}^{t-1},\mathcal{X}_{\geq}}\left(\eta F(\bm{w}^{t})\right)=\bm{z}^{t}\), so that \(\|\bm{w}_{i}^{t}-\bm{z}_{i}^{t}\|_{2}\leq\epsilon^{(t)}\). To bound \(\|\bm{f}(\bm{g}(\bm{w}_{i}^{t}),\bm{\ell}_{i}^{t})\|_{2}\), we note that by definition,

\[\|\bm{f}(\bm{x}_{i},\bm{\ell}_{i})\|_{2}^{2}=\sum_{j=1}^{d_{i}}\left((\bm{x}_{i }-\bm{e}_{j})^{\top}\bm{\ell}_{i}\right)^{2}\leq\sum_{j=1}^{d_{i}}\|\bm{x}_{i }-\bm{e}_{j}\|_{2}^{2}\|\bm{\ell}_{i}\|_{2}^{2}\leq 4d_{i}B_{u}^{2}.\]

This gives

\[\|\bm{f}(\bm{g}(\bm{w}_{i}^{t}),\bm{\ell}_{i}^{t})\|_{2}\leq 2B_{u}\sqrt{d_{i}}.\]

Overall, we have obtained that for all \(\hat{\bm{z}}_{i}\in\Delta_{\geq}^{d_{i}}\), we have

\[\langle\eta\bm{f}(\bm{g}(\bm{w}_{i}^{t}),\bm{\ell}_{i}^{t}),\bm{w}_{i}^{t}-\hat{ \bm{z}}_{i}\rangle\leq-\frac{1}{2}\|\bm{z}_{i}^{t}-\hat{\bm{z}}_{i}\|_{2}^{2}+ \frac{1}{2}\|\bm{z}_{i}^{t-1}-\hat{\bm{z}}_{i}\|_{2}^{2}-\frac{1}{2}\|\bm{z}_{i}^ {t}-\bm{z}_{i}^{t-1}\|_{2}^{2}+\eta 2B_{u}\sqrt{d_{i}}\epsilon^{(t)}.\]

We sum the previous inequality for \(t=1,...,T\) to obtain that for all \(\hat{\bm{z}}_{i}\in\Delta_{\geq}^{d_{i}}\), we have

\[\sum_{t=1}^{T}\langle\eta\bm{f}(\bm{g}(\bm{w}_{i}^{t}),\bm{\ell}_{i}^{t}),\bm{w}_{i}^ {t}-\hat{\bm{z}}_{i}\rangle\leq\frac{1}{2}\|\bm{z}_{i}^{0}-\hat{\bm{z}}_{i}\|_{2}^{2 }-\frac{1}{2}\|\bm{z}_{i}^{T}-\hat{\bm{z}}_{i}\|_{2}^{2}-\sum_{t=1}^{T}\frac{1}{ 2}\|\bm{z}_{i}^{t}-\bm{z}_{i}^{t-1}\|_{2}^{2}+\eta 2B_{u}\sqrt{d_{i}}\sum_{t=1}^{T}\epsilon^{(t)}.\]

Overall, we conclude that

\[\sum_{t=1}^{T}\langle\bm{f}(\bm{g}(\bm{w}_{i}^{t}),\bm{\ell}_{i}^{t}),\bm{w}_{i}^{t}- \hat{\bm{z}}_{i}\rangle\leq\frac{1}{2\eta}\|\bm{z}_{i}^{0}-\hat{\bm{z}}_{i}\|_{2}^ {2}+2B_{u}\sqrt{d_{i}}\sum_{t=1}^{T}\epsilon^{(t)},\forall\ \hat{\bm{z}}_{i}\in\Delta_{\geq}^{d_{i}}.\]

From Lemma 2.1 the left-hand side is equal to \(\mathrm{Reg}_{i}^{T}(\hat{\bm{x}}_{i})\) for \(\hat{\bm{x}}_{i}=\hat{\bm{z}}_{i}\). This concludes the proof of Theorem 5.5.

Proof of Theorem 5.6

Proof of Theorem 5.5.: We will show that for any \(\hat{\bm{w}}\in\mathcal{X}_{\geq}\), we have

\[\sum_{t=1}^{T}\left\langle F(\bm{w}^{t}),\bm{w}^{t}-\hat{\bm{w}} \right\rangle\leq\frac{1}{2\eta}\|\bm{w}^{0}-\hat{\bm{w}}\|_{2}^{2}.\]

Since \(\bm{x}_{i}^{t}=\bm{w}_{i}^{t}\), \(\forall\;t\geq 1,\forall\;i\in\{1,...,n\}\), this is enough to prove Theorem 5.5. Note that

\[\left\langle F(\bm{w}^{t}),\bm{w}^{t}-\hat{\bm{w}}\right\rangle= \left\langle F(\bm{w}^{t}),\bm{z}^{t}-\hat{\bm{w}}\right\rangle+\left\langle F (\bm{w}^{t}),\bm{w}^{t}-\bm{z}^{t}\right\rangle.\]

We will independently analyze each term of the right-hand side of the above equality.

For the first term, we note that the first-order optimality condition for \(\bm{z}^{t}=\Pi_{\bm{z}^{t-1},\mathcal{X}_{\geq}}\left(\eta F(\bm{w}^{t})\right)\) gives, for any \(\hat{\bm{w}}\in\mathcal{X}_{\geq}\),

\[\left\langle\eta F(\bm{w}^{t}),\bm{z}^{t}-\hat{\bm{w}}\right\rangle\leq\frac{ 1}{2}\|\hat{\bm{w}}-\bm{z}^{t-1}\|_{2}^{2}-\frac{1}{2}\|\hat{\bm{w}}-\bm{z}^{t }\|_{2}^{2}-\frac{1}{2}\|\bm{z}^{t}-\bm{z}^{t-1}\|_{2}^{2}.\] (12)

For the second term, we will prove the following lemma.

**Lemma H.1**.: _Let \(\eta>0\) such that \(\bm{w}\mapsto\eta F(\bm{w})\) is \(1/\sqrt{2}\) Lipschitz continuous over \(\mathcal{X}_{\geq}\). Then_

\[\left\langle\eta F(\bm{w}^{t}),\bm{w}^{t}-\bm{z}^{t}\right\rangle\leq\frac{1} {2}\|\bm{z}^{t}-\bm{z}^{t-1}\|_{2}^{2}.\] (13)

Proof of Lemma H.1.: We write

\[\left\langle\eta F(\bm{w}^{t}),\bm{w}^{t}-\bm{z}^{t}\right\rangle= \left\langle\eta F(\bm{z}^{t-1}),\bm{w}^{t}-\bm{z}^{t}\right\rangle+\left\langle \eta F(\bm{w}^{t})-\eta F(\bm{z}^{t-1}),\bm{w}^{t}-\bm{z}^{t}\right\rangle.\]

We will bound independently each term in the above equation. From \(\bm{w}^{t}=\Pi_{\bm{z}^{t-1},\mathcal{X}_{\geq}}\left(\eta F(\bm{z}^{t-1})\right)\) we have

\[\left\langle\eta F(\bm{z}^{t-1}),\bm{w}^{t}-\bm{z}^{t}\right\rangle\leq\frac{ 1}{2}\|\bm{z}^{t}-\bm{z}^{t-1}\|_{2}^{2}-\frac{1}{2}\|\bm{z}^{t}-\bm{w}^{t}\|_ {2}^{2}-\frac{1}{2}\|\bm{w}^{t}-\bm{z}^{t-1}\|_{2}^{2},\]

which gives

\[\left\langle\eta F(\bm{z}^{t-1}),\bm{w}^{t}-\bm{z}^{t}\right\rangle\leq\frac{ 1}{2}\|\bm{z}^{t}-\bm{z}^{t-1}\|_{2}^{2}-\frac{1}{2}\|\bm{z}^{t}-\bm{w}^{t}\|_ {2}^{2}-\frac{1}{2}\|\bm{w}^{t}-\bm{z}^{t-1}\|_{2}^{2},\] (14)

From Cauchy-Schwarz inequality, we have

\[\left\langle\eta F(\bm{w}^{t})-\eta F(\bm{z}^{t-1}),\bm{w}^{t}- \bm{z}^{t}\right\rangle\leq\|\eta F(\bm{w}^{t})-\eta F(\bm{z}^{t-1})\|_{2}\| \bm{w}^{t}-\bm{z}^{t}\|_{2}.\]

Recall that

\[\bm{w}^{t} =\Pi_{\bm{z}^{t-1},\mathcal{X}}\left(\eta F(\bm{z}^{t-1})\right)\] \[\bm{z}^{t} =\Pi_{\bm{z}^{t-1},\mathcal{X}}\left(\eta F(\bm{w}^{t})\right)\]

Since the proximal operator is \(1\)-Lipschitz continuous, and since \(\bm{w}\mapsto\eta F(\bm{w})\) is \(1/\sqrt{2}\)-Lipschitz continuous, we obtain

\[\left\langle\eta F(\bm{w}^{t})-\eta F(\bm{z}^{t-1}),\bm{w}^{t}- \bm{z}^{t}\right\rangle\leq\frac{1}{2}\|\bm{w}^{t}-\bm{z}^{t-1}\|_{2}^{2}.\] (15)

We can now sum (14) and (15) to obtain

\[\left\langle\eta F(\bm{w}^{t}),\bm{w}^{t}-\bm{z}^{t}\right\rangle\leq\frac{1} {2}\|\bm{z}^{t}-\bm{z}^{t-1}\|_{2}^{2}-\frac{1}{2}\|\bm{z}^{t}-\bm{w}^{t}\|_{2} ^{2}\leq\frac{1}{2}\|\bm{z}^{t}-\bm{z}^{t-1}\|_{2}^{2}.\]

We have shown in Lemma 5.2 that \(F\) is \(L_{F}\)-Lipschitz continuous for normal-form games. Our choice of step size \(\eta=\frac{1}{L_{F}\sqrt{2}}\) ensures that that \(\bm{\omega}\mapsto\eta F(\bm{w})\) is \(1/\sqrt{2}\)-Lipschitz continuous as in the assumptions of Lemma H.1.

Combining (12) with (13) yields

\[\left\langle\eta F(\bm{w}^{t}),\bm{w}^{t}-\hat{\bm{w}}\right\rangle\leq\frac{1}{2} \|\hat{\bm{w}}-\bm{z}^{t-1}\|_{2}^{2}-\frac{1}{2}\|\hat{\bm{w}}-\bm{z}^{t}\|_{2 }^{2}.\]

Summing this inequality for \(t=1,...,T\) and telescoping, we obtain

\[\sum_{t=1}^{T}\left\langle\eta F(\bm{w}^{t}),\bm{w}^{t}-\hat{\bm{w}}\right\rangle \leq\frac{1}{2}\|\hat{\bm{w}}-\bm{z}^{0}\|_{2}^{2}-\frac{1}{2}\|\hat{\bm{w}}- \bm{z}^{T}\|_{2}^{2}\]

which directly yields

\[\sum_{t=1}^{T}\left\langle\eta F(\bm{w}^{t}),\bm{w}^{t}-\hat{\bm{w}}\right\rangle \leq\frac{1}{2}\|\hat{\bm{w}}-\bm{z}^{0}\|_{2}^{2}.\] (16)

Overall, for any \(\left(\hat{\bm{R}}_{1},...,\hat{\bm{R}}_{n}\right)\in\mathcal{X}_{\geq}\) we obtain that \(\sum_{i=1}^{T}\mathrm{Reg}_{i}^{T}(\hat{\bm{R}}_{i})\) is upper bounded by \(\sum_{i=1}^{n}\frac{1}{2_{\eta}}\|\bm{w}_{i}^{0}-\hat{\bm{R}}_{i}\|_{2}^{2}.\) Now from Lemma 2.1, for any \((\hat{\bm{x}}_{1},...,\hat{\bm{x}}_{n})\in\Delta\), we conclude that

\[\sum_{i=1}^{T}\mathrm{Reg}_{i}^{T}(\hat{\bm{x}}_{i})\leq\sum_{i=1}^{n}\frac{1} {2\eta}\|\bm{w}_{i}^{0}-\hat{\bm{x}}_{i}\|_{2}^{2}.\]

This concludes the proof of Theorem 5.6. 

## Appendix I Proof of Lemma 5.7

Proof of Lemma 5.7.: The proof of Lemma 5.7 follows the lines of the proof of Lemma 5.2. Clearly, for matrix games we have \(F=h\circ g\) with \(h:\mathbb{R}^{d_{1}}\times\mathbb{R}^{d_{2}}\rightarrow\mathbb{R}^{d_{1}} \times\mathbb{R}^{d_{2}}\) defined as Proposition 1.

\[h\begin{bmatrix}\bm{x}\\ \bm{y}\end{bmatrix}=\begin{bmatrix}\bm{f}\left(\bm{x},\bm{A}\bm{y}\right)\\ \bm{f}\left(\bm{y},-\bm{A}^{\top}\bm{x}\right)\end{bmatrix}\] (17)

The function \(g\) is Lipschitz continuous over \(\Delta_{\geq}^{d_{1}}\) (Proposition 1), with a Lipschitz constant of \(L_{g}=\sqrt{d_{1}}\). Let us now compute the Lipschitz constant of \(h\). Observe that:

\[\|\bm{f}\left(\bm{x},\bm{A}\bm{y}\right)-\bm{f}\left(\bm{x}^{ \prime},\bm{A}\bm{y}^{\prime}\right)\|_{2}^{2}\] \[=\sum_{i=1}^{d_{1}}\left(\left(\bm{x}-\bm{e}_{i}\right)^{\top}\bm {A}\bm{y}-\left(\bm{x}^{\prime}-\bm{e}_{i}\right)^{\top}\bm{A}\bm{y}^{\prime} \right)^{2}\] \[=\sum_{i=1}^{d_{1}}(\left(\bm{x}-\bm{e}_{i}\right)^{\top}\bm{A} \bm{y}-\left(\bm{x}^{\prime}-\bm{e}_{i}\right)^{\top}\bm{A}\bm{y}+\left(\bm{x }^{\prime}-\bm{e}_{i}\right)^{\top}\bm{A}\bm{y}-\left(\bm{x}^{\prime}-\bm{e} _{i}\right)^{\top}\bm{A}\bm{y}^{\prime})^{2}\] \[=\sum_{i=1}^{d_{1}}\left(\left(\bm{x}-\bm{x}^{\prime}\right)^{ \top}\bm{A}\bm{y}+\left(\bm{x}^{\prime}-\bm{e}_{i}\right)^{\top}\bm{A}(\bm{y }-\bm{y}^{\prime})\right)^{2}\] \[\leq 2d_{1}\left(\left(\bm{x}-\bm{x}^{\prime}\right)^{\top}\bm{A} \bm{y}\right)^{2}+2\sum_{i=1}^{d_{1}}\left(\left(\bm{x}^{\prime}-\bm{e}_{i} \right)^{\top}\bm{A}(\bm{y}-\bm{y}^{\prime})\right)^{2}\] \[\leq 2d_{1}\|\bm{A}\|_{op}^{2}\|\bm{x}-\bm{x}^{\prime}\|_{2}^{2}+4d _{1}\|\bm{A}\|_{op}^{2}\|\bm{y}-\bm{y}^{\prime}\|_{2}^{2}.\]

Similarly, we have that \(\|\bm{f}\left(\bm{y},-\bm{A}^{\top}\bm{x}\right)-\bm{f}\left(\bm{y}^{\prime},- \bm{A}^{\top}\bm{x}^{\prime}\right)\|_{2}^{2}\) is upper bounded by

\[2d_{2}\|\bm{A}\|_{op}^{2}\|\bm{y}-\bm{y}^{\prime}\|_{2}^{2}+4d_{2}\|\bm{A}\|_{ op}^{2}\|\bm{x}-\bm{x}^{\prime}\|_{2}^{2},\]

and thus

\[\left\|h\begin{bmatrix}\bm{x}\\ \bm{y}\end{bmatrix}-h\begin{bmatrix}\bm{x}^{\prime}\\ \bm{y}^{\prime}\end{bmatrix}\right\|_{2}\leq\|\bm{A}\|_{op}\sqrt{6\max\{d_{1},d_ {2}\}}\left\|\begin{bmatrix}\bm{x}\\ \bm{y}\end{bmatrix}-\begin{bmatrix}\bm{x}^{\prime}\\ \bm{y}^{\prime}\end{bmatrix}\right\|_{2}.\]

Therefore, the Lipschitz constant of \(h\) is \(L_{h}=\|\bm{A}\|_{op}\sqrt{6\max\{d_{1},d_{2}\}}\).

Since \(F=h\circ g\), we obtain that the Lipschitz constant \(L_{F}\) of \(F\) is \(L_{F}=L_{h}\times L_{g}=\sqrt{6}\|\bm{A}\|_{op}\max\{d_{1},d_{2}\}\)Extensive-Form Games

In this section we show how to extend our convergence results for Conceptual RM\({}^{+}\) from normal-form games to EFGs. Briefly, an EFG is a game played on a tree, where each node belongs to some player, and the player chooses a probability distribution over branches. Moreover, players have _information sets_, which are groups of nodes belonging to a player such that they cannot distinguish among them, and thus they must choose the same probability distribution at all nodes in an information set. When a leaf \(h\) is reached, each player \(i\) receives some payoff \(v_{i}(h)\in[0,1]\). In order to extend our results, we will use the CFR regret decomposition [37, 9], and then show how to run the Conceptual RM\({}^{+}\) algorithm on the resulting set of strategy spaces (which will be a Cartesian product of positive orthants). The CFR regret decomposition works in the space of _behavioral strategies_, which represents the strategy space of each player as a Cartesian product of simplices, with each simplex corresponding to the set of possible ways to randomize over actions at a given information set for the player. Formally, we write the polytope of behavioral-form strategies as

\[\mathcal{X}=\times_{i\in[n],j\in\mathcal{D}_{i}}\Delta^{n_{j}},\]

where \(\mathcal{D}_{i}\) is the set of information sets for player \(i\) and \(n_{j}\) is the number of actions at information set \(j\). Let \(P=\sum_{i\in[n],j\in\mathcal{D}_{i}}n_{j}\) be the dimension of \(\mathcal{X}\). In EFGs with _perfect recall_, meaning that a player never forgets something they knew in the past, the _sequence-form_ is an equivalent representation of the set of strategies, which allows one to write the payoffs for each player as a multilinear function. This in turn enables optimization and regret minimization approaches that exploit multi-linearity, e.g. bilinearity in the two-player zero-sum setting [21, 25, 10]. Instead of working on this representation, the CFR approach minimizes a notion of local regret at each information set, using so-called _counterfactual values_. The weighted sum of counterfactual regrets at each information set is an upper bound on the sequence-form regret [37], and thus a player in an EFG can minimize their regret by locally minimizing each counterfactual regret. Informally, the counterfactual value is the expected value of an action at a information set, conditional on the player at the information set playing to reach that information set and then taking the corresponding action. The counterfactual value associated to each tuples of player \(i\), information set \(j\in\mathcal{D}_{i}\), and action \(a\in A_{j}\) is \(G_{ija}(\bm{x})\coloneqq\sum_{h\in\mathcal{L}_{ja}}\prod_{\{\hat{j},\hat{a}\} \in\mathcal{P}_{j}(h)}\bm{x}[\hat{j},\hat{a}]v_{i}(h),\) where \(\mathcal{L}_{ja}\) is the set of leaf nodes reachable from information set \(j\) after taking action \(a\), and \(\mathcal{P}_{j}(h)\) is the set of pairs of information sets and actions \((\hat{j},\hat{a})\) on the path from the root to \(h\), except that information sets belonging to player \(i\) are excluded, unless they occur _after_\(j,a\).

We will be concerned with the counterfactual regret, given by the operator \(H:\mathcal{X}\rightarrow\mathbb{R}^{\sum_{i\in[n],j\in\mathcal{D}_{i}}n_{j}}\) defined as \(H_{ija}(\bm{x})\coloneqq G_{ija}(\bm{x})-\langle G_{ij}(\bm{x}),\bm{x}^{j}\rangle.\) Now we can show that the counterfactual regret operator \(H\) is Lipschitz continuous. Intuitively, this should hold since \(H\) is multilinear.

**Lemma J.1**.: _For any behavioral strategies \(\bm{x},\bm{x}^{\prime}\in\mathcal{X}\), \(\|H(\bm{x})-H(\bm{x}^{\prime})\|_{2}\leq\sqrt{2P}\|\bm{x}-\bm{x}^{\prime}\|_{2}\)._

Proof.: We start by showing a bound for \(G\). We first analyze the change in a single coordinate of \(G\) for a given \(i\in[n],j\in\mathcal{D}_{i},a\in A_{j}\). We focus on how \(G_{ija}\) changes with respect to the change in \(|\bm{x}[\hat{j},\hat{a}]-\bm{x}^{\prime}[\hat{j},\hat{a}]|\) for some arbitrary information set-action pair \((\hat{j},\hat{a})\in\mathcal{P}_{j}(h)\) for some \(h\in\mathcal{L}_{ja}\). To alleviate inline notation, let \(\mathcal{P}_{j}^{\hat{j},\hat{a}}(h)=\mathcal{P}_{j}(h)\setminus\{(\hat{j}, \hat{a})\}\).

\[G_{ija}(x) =\bm{x}[\hat{j},\hat{a}]\sum_{h\in\mathcal{L}_{ja}\cap\mathcal{L }_{ja}}\prod_{(\vec{j},\vec{a})\in\mathcal{P}_{j}^{\hat{j},\hat{a}}(h)}\bm{x}[ \bar{j},\bar{a}]v_{i}(h)\] \[+\sum_{h\in\mathcal{L}_{ja}\setminus\mathcal{L}_{ja}}\prod_{(\vec {j},\vec{a})\in\mathcal{P}_{j}(h)}\bm{x}[\bar{j},\bar{a}]v_{i}(h)\] \[\leq|\bm{x}[\hat{j},\hat{a}]-\bm{x}^{\prime}[\hat{j},\hat{a}]| \sum_{h\in\mathcal{L}_{ja}\cap\mathcal{L}_{ja}}\prod_{(\vec{j},\vec{a})\in \mathcal{P}_{j}^{\hat{j},\hat{a}}(h)}\bm{x}[\bar{j},\bar{a}]v_{i}(h)\] \[+\bm{x}^{\prime}[\hat{j},\hat{a}]\sum_{h\in\mathcal{L}_{ja}\cap \mathcal{L}_{ja}}\prod_{(\vec{j},\vec{a})\in\mathcal{P}_{j}(h)}\bm{x}[\bar{j}, \bar{a}]v_{i}(h)\] \[+\sum_{h\in\mathcal{L}_{ja}\setminus\mathcal{L}_{ja}}\prod_{(\vec {j},\vec{a})\in\mathcal{P}_{j}(h)}\bm{x}[\bar{j},\bar{a}]v_{i}(h)\]Now let us bound the error term by noting that \(v_{i}(h)\leq 1\) for all \(h\) by assumption:

\[|\bm{x}[\hat{j},\hat{a}]-\bm{x}^{\prime}[\hat{j},\hat{a}]|\sum_{h \in\mathcal{L}_{j\hat{a}}\cap\mathcal{L}_{j\hat{a}}}\prod_{(\bar{j},\hat{a})\in \mathcal{P}_{j}^{j,\hat{a}}(h)}\bm{x}[\bar{j},\bar{a}]v_{i}(h)\] \[\leq |\bm{x}[\hat{j},\hat{a}]-\bm{x}^{\prime}[\hat{j},\hat{a}]|\sum_{h \in\mathcal{L}_{j\hat{a}}\cap\mathcal{L}_{j\hat{a}}}\prod_{(\bar{j},\hat{a})\in \mathcal{P}_{j}^{j,\hat{a}}(h)}\bm{x}[\bar{j},\bar{a}]\] \[\leq |\bm{x}[\hat{j},\hat{a}]-\bm{x}^{\prime}[\hat{j},\hat{a}]|,\]

where the last inequality is because the sum of reach probabilities on leaf nodes in \(\mathcal{L}_{\hat{j}\hat{a}}\cap\mathcal{L}_{ja}\) after conditioning on player \(i\) playing to reach \((j,a)\) and \((\hat{j},\hat{a})\) being played with probability one, is less than or equal to one.

By iteratively applying this argument to each \((\hat{j},\hat{a})\in\mathcal{P}_{j}(h)\), we get

\[G_{ija}(\bm{x}) \leq G_{ija}(\bm{x}^{\prime})+\sum_{h\in\mathcal{L}_{j\hat{a}}} \sum_{(\hat{j},\hat{a})\in\mathcal{P}_{j}(h)}|\bm{x}[\hat{j},\hat{a}]-\bm{x}^ {\prime}[\hat{j},\hat{a}]|\] (18) \[\leq G_{ija}(\bm{x}^{\prime})+\|\bm{x}-\bm{x}^{\prime}\|_{1}.\]

Repeating the same argument for \(\bm{x}^{\prime}\) gives

\[|G_{ija}(\bm{x})-G_{ija}(\bm{x}^{\prime})|\leq\|\bm{x}-\bm{x}^{\prime}\|_{1}.\]

Secondly, we bound the difference in the inner product terms.

\[\langle G_{ij}(\bm{x}),\bm{x}^{j}\rangle =\sum_{a\in A_{j}}\bm{x}[j,a]G_{ija}(\bm{x})\] \[\leq\sum_{a\in A_{j}}\left[|\bm{x}[j,a]-\bm{x}^{\prime}[j,a]|+\bm {x}^{\prime}[j,a]G_{ija}(\bm{x})\right]\] \[\leq\|\bm{x}^{j}-\bm{x}^{j^{\prime}}\|_{1}+\sum_{a\in A_{j}}\bm{ x}^{\prime}[j,a]G_{ija}(\bm{x}^{\prime})+\sum_{a\in A_{j}}\bm{x}^{\prime}[j,a] \sum_{h\in\mathcal{L}_{j\hat{a}}}\sum_{(\hat{j},\hat{a})\in\mathcal{P}_{j}(h) }|\bm{x}[\hat{j},\hat{a}]-\bm{x}^{\prime}[\hat{j},\hat{a}]|\] \[\leq\langle G_{ij}(\bm{x}^{\prime}),\bm{x}^{\prime}\rangle+\|\bm {x}-\bm{x}^{\prime}\|_{1}\]

where the second-to-last line is by Eq. (18). Again we can start from \(\bm{x}^{\prime}\) instead to get

\[|\langle G_{ij}(\bm{x}),\bm{x}^{j}\rangle-\langle G_{ij}(\bm{x}^{\prime}), \bm{x}^{\prime}\rangle|\leq\|\bm{x}-\bm{x}^{\prime}\|_{1}.\]

Putting together all our bonds and applying norm equivalence, we get that

\[\|H(\bm{x})-H(\bm{x}^{\prime})\|_{2}^{2} \leq\sum_{i\in[n]}\sum_{j\in\mathcal{D}_{i},a\in A_{j}}2\|\bm{x} -\bm{x}^{\prime}\|_{1}^{2}\] \[\leq 2P\|\bm{x}-\bm{x}^{\prime}\|_{2}^{2}.\]

Taking square roots completes the proof. 

Since we want to run smooth RM\({}^{+}\), we will need to consider the lifted strategy space for each decision point. Let \(\mathcal{Z}\) be the Cartesian product of the positive orthants for each information set, i.e. \(\mathcal{Z}=\times_{i\in[n],j\in\mathcal{D}_{i}}\mathbb{R}_{+}^{n_{j}}\). Now let \(\hat{g}:\mathcal{Z}\to\mathcal{X}\) be the function that normalizes each vector from the positive orthant to the simplex such that we get a behavioral strategy, i.e. \(\hat{g}_{j}(\bm{z})=g(\bm{z}^{j})\), where \(\bm{z}^{j}\) is the slice of \(\bm{z}\) corresponding to information set \(j\). The function \(\hat{g}\) is also Lipschitz continuous.

**Lemma J.2**.: _Suppose that \(\bm{z},\bm{z}^{\prime}\in\mathcal{Z}\) satisfy \(\|\bm{z}^{j}\|_{1}\geq R_{0,j},\|\bm{z}^{j^{\prime}}\|_{1}\geq R_{0,j}\) for all \(i\in[n],j\in\mathcal{D}_{i}\). Then, \(\|\hat{g}(\bm{z})-\hat{g}(\bm{z}^{\prime})\|_{2}\leq\max_{i\in[n],j\in \mathcal{D}_{i}}\sqrt{n_{j}/R_{0,j}}\|\bm{z}-\bm{z}^{\prime}\|_{2}\)._

Proof.: We have from Proposition 1 that

\[\|\hat{g}(\bm{z})-\hat{g}(\bm{z}^{\prime})\|_{2}^{2}=\sum_{i\in[n],j\in \mathcal{D}_{i}}\|\bm{g}(\bm{z})-\bm{g}(\bm{z}^{\prime})\|_{2}^{2}\]\[\leq\sum_{i\in[n],j\in\mathcal{D}_{i}}n_{j}/R_{0,j}\|\bm{z}^{j}-\bm{z}^{j \prime}\|_{2}^{2}\] \[\leq\max_{i\in[n],j\in\mathcal{D}_{i}}n_{j}/R_{0,j}\|\bm{z}-\bm{z}^ {\prime}\|_{2}^{2}\]

Now let us introduce the operator \(F:\mathcal{Z}\to\mathbb{R}^{\sum_{i\in[n],j\in\mathcal{D}_{i}}n_{j}}\) for EFGs. For a given \(\bm{z}\in\mathcal{Z}\), the operator will output the regret associated with the counterfactual values for each decision set \(j\). \(F\) will be composed of two functions, first \(\hat{g}\) maps a given \(\bm{z}\) to some behavioral strategy \(\bm{x}=\hat{g}(\bm{z})\), and then the operator \(H:\mathcal{X}\to\mathbb{R}^{\sum_{i\in[n],j\in\mathcal{D}_{i}}n_{j}}\) outputs the regrets for the counterfactual values.

Now we can apply our bounds on the Lipschitz constant for \(\hat{g}\) and \(H\) to get that \(F\) is Lipschitz continuous with Lipschitz constant \(2P\max_{i\in[n],j\in\mathcal{D}_{i}}\sqrt{n_{j}/R_{0,j}}\). Combining our Lipschitz result with our setup of \(\mathcal{X}\) and \(F\), we can now run Algorithm 4 on \(\mathcal{X}\) and \(F\) and apply Theorem 5.5 to get a smooth-RM\({}^{+}\)-based algorithm that allows us to compute a sequence of iterates with regret at most \(\epsilon\) in at \(O(1/\epsilon)\) iterations and using \(O(\log(1/\epsilon)/\epsilon)\) gradient computations.

## Appendix K Details on the Numerical Experiments

Efficient orthogonal projection on \(\Delta_{\geq}^{n}\).Recall that \(\Delta_{\geq}^{n}=\{\bm{R}\in\mathbb{R}^{n}\mid\bm{R}\geq\bm{0},\bm{1}_{n}^{ \top}\bm{R}\geq 1\}\). Let \(\bm{y}\in\mathbb{R}^{n}\) and let us consider

\[\min_{\bm{x}\geq\bm{0},\bm{1}_{n}^{\top}\bm{x}\geq 1}\frac{1}{2}\|\bm{x}-\bm{y} \|_{2}^{2}.\]

Introducing a Lagrange multiplier \(\mu\geq 0\) for the constraint \(1-\bm{1}_{n}^{\top}\bm{x}\leq 0\), we arrive at

\[\min_{\bm{x}\geq\bm{0}}\;\max_{\mu\geq 0}\frac{1}{2}\|\bm{x}-\bm{y}\|_{2}^{2 }+\mu\left(1-\bm{1}_{n}^{\top}\bm{x}\right).\]

Let us call \((\bm{x},\mu)\in\mathbb{R}_{+}^{n}\times\mathbb{R}_{+}\) an optimal solution to the above saddle-point problem. Stationarity of the Lagrangian function shows that \(x_{i}=[y_{i}+\mu]^{+},\forall\;i\in[n]\). Therefore, we could simply use binary search to solve the following univariate concave problem:

\[\max_{\mu\geq 0}\mu-\frac{1}{2}\|[\bm{y}+\mu\bm{1}_{n}]^{+}\|_{2}^{2}.\]

Let us use the Karush-Kuhn-Tucker conditions. Complementary slackness gives \(\mu\cdot\left(1-\bm{1}_{n}^{\top}\bm{x}\right)=0\). If \(\mu=0\), then \(\bm{x}=[\bm{y}]^{+}\), and by primal feasibility we must have \(\bm{1}_{n}^{\top}\bm{x}\geq 1\), i.e., \(\bm{1}_{n}^{\top}[\bm{y}]^{+}\geq 1\). If that is not the case, then we can not have \(\mu=0\), and we must have \(1-\bm{1}_{n}^{\top}\bm{x}=0\), i.e., \(\bm{x}\in\Delta^{n}\). In this case, we obtain that \(\bm{x}\) is the orthogonal projection of \(\bm{y}\) on \(\Delta^{n}\). Overall, we see that \(\bm{x}\) is always either \([\bm{y}]^{+}\), the orthogonal projection of \(\bm{y}\) on \(\mathbb{R}_{+}^{n}\), or \(\bm{x}\) is the orthogonal projection of \(\bm{y}\) on \(\Delta^{n}\). Since \(\Delta_{\geq}^{n}\subset\mathbb{R}_{+}^{n}\), we can compute the orthogonal projection on \(\Delta_{\geq}^{n}\) as follows:

Compute \(\bm{x}=[\bm{y}]^{+}\). If \(\bm{1}_{n}^{\top}\bm{x}\geq 1\), then we have found the orthogonal projection of \(\bm{y}\) on \(\Delta_{\geq}^{n}\). Else, return the orthogonal projection of \(\bm{y}\) on the simplex \(\Delta^{n}\).

Performances of ExRM\({}^{+}\), Stable PRM\({}^{+}\)and Smooth PRM\({}^{+}\)on our small matrix game example

In this section we provide detailed numerical results for ExRM\({}^{+}\), Stable PRM\({}^{+}\), and Smooth PRM\({}^{+}\) on our \(3\times 3\) matrix-game counterexample. All algorithms use linear averaging and Stable and Smooth PRM\({}^{+}\) use alternation. We choose a step size of \(\eta=0.1\) for our implementation of these algorithms. The results are presented in Figure 6 for ExRM\({}^{+}\), in Figure 7 for Stable PRM\({}^{+}\) and in Figure 8 for Smooth PRM\({}^{+}\).

### Extensive-form game used in the experiments

We used the following games in the experiments:Figure 6: Empirical performance of ExRM\({}^{+}\) (with linear averaging) on our \(3\times 3\) matrix game from Section 2.

Figure 7: Empirical performance of Stable PRM\({}^{+}\) (with alternation and linear averaging) on our \(3\times 3\) matrix game from Section 2.

* 2-player Sheriff is a two-player general-sum game inspired by the Sheriff of Nottingham board game. It was introduced as a benchmark for correlated equilibria by Farina et al. [11]. The variant of the game we use has the following parameters:
* maximum number of items that can be smuggled: 10
* maximum bribe amount: 3
* number of bargaining rounds: 3
* value of each item: 5
* penalty for illegal item found in cargo: 1
* penalty for Sheriff if no illegal item found in cargo: 1 The number of nodes in this game is 9648.
* 3-player Leduc poker is a 3-player version of the standard benchmark of Leduc poker [33]. The game has 15659 nodes.
* 4-player Kuhn poker is a 4-player version of the standard benchmark of Kuhn poker [26]. We use a larger variant the standard one, to assess the scalability of our algorithm. The variant we use has six ranks in the deck. The game has 23402 nodes.
* 4-player Lian's dice is a 4-player version of the game Lian's dice, already used as a benchmark by Lisy et al. [27]. We use a variant with 1 die per player, each with two distinct faces. The game has 8178 nodes.

Figure 8: Empirical performance of Smooth PRM\({}^{+}\) (with alternation and linear averaging) on our \(3\times 3\) matrix game from Section 2.