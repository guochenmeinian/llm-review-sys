# SDformer: Similarity-driven Discrete Transformer For Time Series Generation

Zhicheng Chen\({}^{1,2,\dagger}\), Shibo Feng\({}^{3}\), Zhong Zhang\({}^{2}\), Xi Xiao\({}^{1,4,\ast}\), Xingyu Gao\({}^{5}\), Pelilin Zhao\({}^{2,\ast}\)

\({}^{1}\)Shenzhen International Graduate School, Tsinghua University

\({}^{2}\)Tencent AI Lab

\({}^{3}\)School of Computer Science and Engineering, Nanyang Technological University

\({}^{4}\)Key Laboratory of Data Protection and Intelligent Management (Sichuan University), Ministry of Education

\({}^{5}\)Institute of Microelectronics, Chinese Academy of Sciences

{czc22@mails,xiaox@sz}.tsinghua.edu.cn, shibo001@ntu.edu.sg, gaoxingyu@ime.ac.cn, {todzhang, masonzhao}@tencent.com

###### Abstract

The superior generation capabilities of Denoised Diffusion Probabilistic Models (DDPMs) have been effectively showcased across a multitude of domains. Recently, the application of DDPMs has extended to time series generation tasks, where they have significantly outperformed other deep generative models, often by a substantial margin. However, we have discovered two main challenges with these methods: 1) the inference time is excessively long; 2) there is potential for improvement in the quality of the generated time series. In this paper, we propose a method based on discrete token modeling technique called Similarity-driven Discrete Transformer (SDformer). Specifically, SDformer utilizes a similarity-driven vector quantization method for learning high-quality discrete token representations of time series, followed by a discrete Transformer for data distribution modeling at the token level. Comprehensive experiments show that our method significantly outperforms competing approaches in terms of the generated time series quality while also ensuring a short inference time. Furthermore, without requiring retaining, SDformer can be directly applied to predictive tasks and still achieve commendable results.

## 1 Introduction

Time series data is prevalent across a wide array of real-world applications, spanning fields such as finance [22, 9, 38, 36], healthcare [27], energy [28, 21, 12, 13], retail [20, 45], and climate science [35]. Despite its significance, the limited availability of dynamic data can pose a significant barrier to the development of machine learning solutions, particularly in scenarios where data sharing could lead to privacy violations [2]. The generation of synthetic yet realistic time series data has emerged as a promising alternative, garnering increased interest due to recent advancements in deep learning techniques.

Existing works on time series generation (TSG) is mainly based on common deep generative models, such as methods based on generative adversarial networks (GAN) [24, 11, 39, 37, 26, 16, 17] and methods based on Variational Autoencoders (VAE) [7, 25]. Currently holding state-of-the-art results are DDPMs-based methods [19, 6, 42], which have the capability to generate high-quality, realistic time series. However, they are not without their challenges. Firstly, these methods often require lengthy inference times due to the substantial number of denoising steps involved. Secondly, despitethese methods achieving significant advancements in generation quality compared to other deep generative models, we observe that the quality of the time series they generate still has potential for further enhancement.

Currently, large transformer-based language models, often known as LMs or LLMs, have become the standard choice for natural language generation tasks[1; 3]. As time has progressed, these LMs have evolved to produce content across a wide range of modalities, such as images [29; 40; 5; 4] and videos [41; 33], using what is referred to as Discrete Token Modeling (DTM) technique. In general, these approaches function by learning a discrete representations of images (or videos, etc.), treating them as if they were natural language, and harnessing the power of existing language models for the generation process. Inspired by their success, we aim to explore the application of these techniques in the domain of multivariate time series generation, potentially unlocking new possibilities and advancements in this area.

To address the aforementioned challenges, we propose a novel two-stage method for time series generation, called Similarity-driven Discrete Transformer (SDformer). The primary objective of the first stage is to employ Vector Quantized Variational Autoencoders (VQ-VAE) [31] for learning high-quality discrete representations of time series. To enhance this process, we introduce a similarity-driven vector quantization approach, which identifies the most suitable code from the codebook by maximizing similarity. The experiments in 5.4 further substantiate the superiority of our method over distance-driven vector quantization. Moreover, to prevent code collapse--a phenomenon where only a small portion of the codes are updated during training, thereby hindering the performance of the VQ-VAE--we incorporate two standard recipes [34] during training: Exponential Moving Average (EMA) for codebook updates and Resetting inactivated codes during the training process (Code Reset). In the second stage, we implement two Discrete Token Modeling (DTM) techniques: Masked Token Modeling (MTM) and Autoregressive Token Modeling (ARTM), underpinning the SDformer-ar and SDformer-m variants, respectively. SDformer-ar adopts an autoregressive approach for both training and inference, mitigating the inconsistency between these two phases through random replacement [43]. SDformer-m utilizes random masking for training and iterative decoding for inference [8; 5]. Our findings reveal that SDformer, particularly SDformer-ar, surpasses existing models in time series generation. Moreover, SDformer demonstrates robust predictive performance without requiring retraining.

In summary, our contributions include:

* We propose an efficient time series generation model **SDformer**, which successfully introduces DTM technique into time series generation and demonstrates its feasibility and efficiency.
* We introduce a novel similarity-driven vector quantization approach that outperforms the traditional distance-driven method in learning discrete representations of time series. This innovative approach offers a straightforward yet powerful technique for applying discrete token modeling in various fields.
* Our experimental results confirm that the SDformer's performance in time series generation notably surpasses that of the current state-of-the-art models, exemplified by an average enhancement of 60.8% in Discriminative Score and 86.5% in Context-FID Score across multiple datasets.

## 2 Related work

### Discrete token modeling

Discrete token modeling, a staple in natural language processing (NLP), has recently been adapted for non-language modalities through vector quantization models like VQ-VAE [31] and VQGAN [10]. These models enable the encoding of diverse data types into discrete tokens, allowing the application of advanced language modeling techniques to generate content across various domains. This expansion significantly broadens the utility of NLP methodologies, extending their impact beyond traditional language tasks. Among these techniques, Autoregressive Token Modeling (ARTM) is a common approach that predicts the next token in a sequence, given the previous tokens, using a categorical distribution. Models such as DALL-E [29] and Parti [40] employ ARTM to accomplish text-to-image generation tasks. Similarly, T2M-GPT [43] and MotionGPT [18] utilize ARTM for text-to-motion generation. Another widely used technique is Masked Token Modeling (MTM), which is trained using a masked token objective [8]. In this approach, some tokens in the sequence are randomly masked and need to be predicted based on the observed tokens. Models such as MaskGIT [5] and MUSE [4] leverage MTM for image generation tasks. Furthermore, MAGVIT [41] and Phenaki [33] employ MTM for video generation, showcasing the versatility of this technique.

### Time series generation

Deep generative models demonstrate high-quality sample generation across various domains, as does time series generation. At first, people mostly relied on GAN to complete time series generation [24; 11; 39; 37; 26; 16]. For example, TimeGAN [39] improves temporal dynamics capture by adding an embedding function and supervised loss. COT-GAN [37] combines GAN and Causal Optimal Transfer (COT) principles to efficiently and stably generate low- and high-dimensional time series data. Due to the challenges of training instability and mode collapse in GAN, researchers have started exploring alternative deep generative models for TSG. TimeVAE [7] employs an interpretable temporal structure and achieves promising results in time series synthesis using VAE. Moreover, several studies focus on addressing the generation of irregular time series, such as GT-GAN [17] and KoVAE [25].

With the emergence of Denoising Diffusion Probabilistic Models (DDPMs) [14], a new class of generative models, impressive generative capabilities have been demonstrated across various domains. Recently, diffusion models have also been adapted for TSG. For instance, DiffWave [19] directly applies DDPMs to waveform generation, while DiffTime [6] harnesses the latest advancements in score-based diffusion models for time series generation. Furthermore, Diffusion-TS [42] generates time series samples by utilizing an encoder-decoder transformer with disentangled temporal representations, showcasing the versatility and potential of these alternative generative models.

## 3 Definitions and problem formulation

We define multivariate time series as \(X_{1:\tau}=(x_{1},\cdots,x_{\tau})\in\mathbb{R}^{\tau\times d}\), where \(\tau\) and \(d\) are the number of time steps and variables respectively. Assuming that a dataset containing \(n\) time series can be expressed as \(D=\{X_{1:\tau}^{i}\}_{i=1}^{n}\), the goal of unconditional generation is to use a model \(f_{\theta}\) to generate time series with the same distribution as \(D\), i.e.,

\[\hat{X}_{1:\tau}^{i}=f_{\theta}(Z),\] (1)

where \(Z\) is the input sampled from any known distribution, such as the Gaussian distribution.

Time series forecasting is a common conditional time series generation. We denote historical values as \(X_{1:l}\in\mathbb{R}^{l\times d}\), where \(1<l<\tau\) is the number of historical time steps. Therefore, the goal of conditional generation is to use a model \(f_{\theta}\) to predict future values, i.e.,

\[\hat{X}_{l+1:\tau}=f_{\theta}(X_{1:l}).\] (2)

In this paper, our objective is to develop an effective approach that not only accomplishes unconditional generation tasks efficiently but also adapts to conditional generation tasks without retraining, while maintaining high accuracy.

## 4 Methods

In this section, we illustrate proposed innovative model SDformer for time series generation. Specifically, SDformer is a two-stage method, the framework of which is illustrated in Figure 1. In the first stage, a pre-trained time series tokenizer utilizes similarity-driven vector quantization to obtain high-quality discrete token representations. Following this, a discrete Transformer is employed to learn the distribution of time series data at the discrete token level, with the two generative ways (Masked and Autoregressive strategies).

### Time series tokenizer

To represent time series in discrete tokens, we pre-train a multivariate time series tokenizer based on the VQ-VAE architecture [31]. Our time series tokenizer consists of an encoder \(\mathcal{E}\) and a decoder\(\mathcal{D}\). The encoder is responsible for the generation of discrete time series tokens, while the decoder is capable of reconstructing these tokens back into their original time series form. This methodology allows us to represent time series akin to a language, thereby enabling the application of a multitude of efficient language models to address various time series-related tasks.

Specifically, the encoder \(\mathcal{E}\) initially applies 1D convolutions to time series features \(X_{1:\tau}\) along the temporal dimension, resulting in latent vectors \(H_{1:L}=(h_{1},\cdots,h_{L})\in\mathbb{R}^{L\times d_{c}}\), where \(L=\tau/r\), \(r\) signifies the temporal downsampling rate and \(d_{c}\) is hidden dimension. Subsequently, we employ the codebook to discretely quantize \(h_{i}\) to obtain discrete token. The learnable codebook \(C=\{c_{k}\}_{k=0}^{K-1}\subset\mathbb{R}^{d_{c}}\) comprises \(K\) latent embedding vectors, each with a dimension \(d_{c}\). The process of similarity-driven vector quantization \(Q(\cdot)\) involves identifying the index of the vector in the codebook that exhibits the highest similarity to \(h_{i}\), which can be expressed as:

\[y_{i}=Q(h_{i}):=\underset{k=0,\cdots,K-1}{\operatorname{arg\, max}}\frac{h_{i}}{||h_{i}||}\cdot\frac{c_{k}}{||c_{k}||},\] (3)

where \(y_{i}=0,\cdots,K-1,\cdot\) denotes the inner product, and \(||\cdot||\) represents the modulo operation. For simplicity, we introduce a normalization step in the final output layer of the encoder \(\mathcal{E}\), resulting in a unit modulus length for \(h_{i}\). Furthermore, we ensure that the code in the codebook always has a unit modulus length for \(c_{k}\). The similarity-driven quantization process can be re-simplified as:

\[y_{i}=\underset{k=0,\cdots,K-1}{\operatorname{arg\,max}}h_{i}\cdot c_{k}.\] (4)

Following quantization, the dequantization process \(Q^{-1}(\cdot)\) reverts \(y_{i}\) back to the latent embedding vector, denoted as:

\[\tilde{h}_{i}=Q^{-1}(y_{i}):=c_{y_{i}}.\] (5)

Ultimately, the decoder \(\mathcal{D}\) restores it to the raw time series space, i.e., \(\tilde{X}_{1:\tau}=\mathcal{D}(\tilde{H}_{1:L})\). To train this time series tokenizer, we utilize two distinct loss functions for training and optimizing the parameters of \(\mathcal{E}\) and \(\mathcal{D}\):

\[\mathcal{L}=||X_{1:\tau}-\tilde{X}_{1:\tau}||_{2}^{2}+\frac{\lambda}{L}\sum_{ i=1}^{L}\left(1-h_{i}\cdot sg(\tilde{h}_{i})\right),\] (6)

where the first loss is the reconstruction loss, the second loss is embedding loss, \(sg(\cdot)\) represents the stop gradient, and \(\lambda\) is hyperparameter used to adjust the weights of different parts. For the codebook, we use Exponential Moving Average and Codebook Reset techniques [34] to update.

When the time series tokenizer training is completed, the codebook and all parameters will be frozen. By employing this time series tokenizer, a multivariate time series \(X_{1:\tau}\in\mathbb{R}^{\tau\times d}\) can be mapped to a

Figure 1: The workflow of SDformer. In stage 1, we pre-train a time series tokenizer which uses similarity-driven vector quantization to obtain high-quality discrete token representations. In stage2, two optional techniques are introduced for time series modeling at the discrete token level: MTM and ARTM. For MTM, the input tokens are randomly masked and fed into the Masked Transformer, an encoder-only model, to predict the masked tokens. Conversely, for ARTM, the input tokens are shifted back by one step with the [BOS] token added at the starting position, and then processed by the Autoregressive Transformer, a decoder-only model, to predict subsequent tokens for all input tokens.

sequence of time series tokens \(Y_{1:L}\in\{0,\cdots,K-1\}^{L}\). Therefore, we can use DTM technique to learn the distribution of time series data at the discrete token level. For the choice of DTM, we can opt for methods such as ARTM or MTM. We will introduce these two methods in Sections 4.2 and 4.3, respectively.

### Autoregressive token modeling on time series generation

In this part, we utilize ARTM technique to learn the distribution of time series data at the discrete token level, based on the time series tokenizer. We refer to this approach as SDformer-ar. During training, we take shifted tokens \(Y_{1:L}^{in}=([\text{BOS}],y_{1},\cdots,y_{L-1})\) as input and real tokens \(Y_{1:L}\) as target for training, where [BOS] represents Beginning of Sentence token. In particular, we use index \(K\) as the [BOS] token, which is distinct from the codebook's index range \(\{0,\cdots,K-1\}\). The training objective is to minimize the negative log-likelihood of all tokens:

\[\mathcal{L}_{ar}=-\mathbb{E}\left[\sum_{i}\log P(y_{i}|Y_{1:i}^{in})\right].\] (7)

Concretely, we input \(Y_{1:L}^{in}\) into a Decoder-only Transformer to predict the probabilities \(P(y_{i}|Y_{1:i}^{in})\) for each token, where the negative log-likelihood is computed as the cross-entropy between the ground-truth one-hot token and predicted token. For inference, we start from the [BOS] token and generate next token in an autoregressive fashion. The detailed training and inference algorithm of SDformer-ar are respectively shown in Algorithm 2 and 4 in Appendix E. Note that we are able to generate diverse time series by sampling from the predicted distributions given by the transformer.

Random replacement.Autoregression is known to exhibit inconsistency between the training and inference phases. Specifically, during training, the first \(i-1\) ground-truth tokens are used to predict the \(i\)-th token. However, during inference, there's no guarantee that all the preceding tokens used as conditions are correct. To alleviate this issue, we implement a random replacement strategy as a form of data augmentation during training. In this approach, each token is processed individually. A random number is compared to a probability threshold \(\pi\). If it meets the threshold, the token is replaced randomly; otherwise, it remains unchanged. The random replacement can be expressed as:

\[\tilde{y}_{i}=\begin{cases}\text{Randint}(0,K),&\text{if Uniform}(0,I)\leq \pi\\ y_{i},&\text{otherwise}\end{cases},\] (8)

where \(\text{Randint}(0,K)\) is a random integer sampled uniformly from the range \(0\) to \(K-1\), and \(\text{Uniform}(0,I)\) is a random number sampled from a uniform distribution in the range \(0\) to \(1\). Therefore, during training, we use \(\tilde{y}_{i}\) instead of \(y_{i}\) in \(Y_{1:L}^{in}\) to achieve data augmentation.

### Masked token modeling on time series generation

In this part, we utilize MTM technique to learn the distribution of time series data at the discrete token level, based on the time series tokenizer. We refer to this approach as SDformer-m. During training, we sample a probability \(p\) from the uniform distribution \(U(0,1)\) as the mask probability. We then replace tokens in the original token sequence with the [MASK] token according to the mask probability \(p\). In particular, we use index \(K\) as the [MASK] token, which is distinct from the codebook's index range \(\{0,\cdots,K-1\}\). In other words, when the token \(y_{i}=K\) at a certain position, it indicates that the position has been masked. Denote \(\overline{Y}_{1:L}\) as the result after applying random mask to \(Y_{1:L}\). The training objective is to minimize the negative log-likelihood of the masked tokens:

\[\mathcal{L}_{mask}=-\mathbb{E}\left[\sum_{\tilde{y}_{i}=K}\log P(y_{i}| \overline{Y}_{1:L})\right].\] (9)

Concretely, we feed the masked \(\overline{Y}_{1:L}\) into a multi-layer bi-directional transformer to predict the probabilities \(P(y_{i}|\overline{Y}_{1:L})\) for each masked token, where the negative log-likelihood is computed as the cross-entropy between the ground-truth one-hot token and predicted token.

During inference, we generate a new token sequence using iterative decoding, as proposed in [5]. Initially, we set an iteration number \(N\) and a mask schedule \(S\) of length \(N\). Here, \(S[t]\) represents the number of masks needed after the completion of step \(t\). It is required that \(S[0]<N\), \(S[T-1]=0\), and \(S[t]\) strictly decreases with an increase in \(t\). The detailed training and inference algorithm of SDformer-m are respectively shown in Algorithm 3 and 5 in Appendix E.

## 5 Experiments

In this section, we commence by assessing our proposed methods through a comparative analysis with several state-of-the-art baseline methods on unconditional time series generation tasks. Subsequently, we delve deeper into the analysis of our methods' versatility and high performance in conditional generation tasks. Lastly, through ablation experiments, we confirm the superior effectiveness of similarity-driven vector quantization and discrete token modeling.

### Experimental setups

**Datasets** To evaluate the performance of SDformer, we conduct experiments on 4 real-world datasets (Stocks, ETTh, Energy and fMRI) and 2 simulated datasets (Sines and MuJoCo). Table 1 provides a partial description of each dataset. For more detailed information, please refer to Appendix A.

**Metrics** For quantitative evaluation of synthesized data, we employ the discriminative score and predictive score as described in [39], along with the Context-FID score proposed by [16]. For detailed descriptions, please refer to Appendix A.

### Unconditional time series generation

Table 2 provides a summary of the performance for each of the compared algorithms on all the datasets. From these results, we can make several observations. Firstly, our proposed methods based on DTM outperform other methods in most cases, demonstrating the feasibility and effectiveness of DTM for time series generation tasks. Secondly, SDformer-ar exhibits a significantly better performance than SDformer-m, which contrasts with the findings in the visual domain. This can be attributed to the fact that autoregressive token modeling is better suited to capture temporal correlations compared to masked token modeling.

To further investigate the capability of our proposed methods in handling longer sequences, we compare the generative abilities of different methods on longer time series, as shown in Table 3. Based on the results, it is evident that many methods exhibit significant distortions when dealing with longer time series, particularly when the discriminative score approaches 0.5, as seen prominently

\begin{table}
\begin{tabular}{c|c c c c c} \hline Dataset & Sines & Stocks & ETTh & MuJoCo & Energy & fMRI \\ \hline \# of Samples & 10000 & 3773 & 17420 & 10000 & 19711 & 10000 \\ dim & 5 & 6 & 7 & 14 & 28 & 50 \\ \hline \end{tabular}
\end{table}
Table 1: Descriptions of all datasets.

Figure 2: Visualizations of the time series synthesized by SDformer and Diffusion-TS.

in the Energy dataset. Despite these challenges, both SDformer-ar and SDformer-m continue to demonstrate exceptional performance.

To visualize the performance of time series generation, we adopt two visualization methods: projecting original and synthetic data in a 2-dimensional space using t-SNE [32], and drawing data distributions using Kernel Density Estimation (KDE). Figure 2 illustrates the visualization of our methods in comparison with Diffusion-TS on the Energy and ETH datasets, revealing that the data generated by SDformer-ar more closely resembles the real data, followed by SDformer-m.

### Conditional time series generation

Figure 3: Examples of time series forecasting for Energy (\(1^{st}\) row) and fMRI (\(2^{st}\) row) datasets. Green and gray colors correspond to SDformer-ar and Diffusion-TS, respectively.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c} \hline Metrics & Methods & Sines & Stocks & ETH & MuJoCo & Energy & fMRI \\ \hline \multirow{9}{*}{Discriminative Score\(\downarrow\)} & SDformer-ar & **0.006\(\pm\)0.004** & **0.010\(\pm\)0.006** & **0.003\(\pm\)0.001** & **0.008\(\pm\)0.005** & **0.006\(\pm\)0.004** & **0.017\(\pm\)0.007** \\  & SDformer-m & 0.008\(\pm\)0.004 & 0.020\(\pm\)0.011 & 0.022\(\pm\)0.001 & 0.0250\(\pm\)0.007 & 0.062\(\pm\)0.006 & 0.043\(\pm\)0.006 \\  & Diffusion-TS & **0.006\(\pm\)0.007** & 0.067\(\pm\)0.015 & 0.061\(\pm\)0.009 & **0.008\(\pm\)0.002** & 0.122\(\pm\)0.003 & 0.167\(\pm\)0.023 \\  & TimeGAN & 0.011\(\pm\)0.008 & 0.102\(\pm\)0.021 & 0.114\(\pm\)0.055 & 0.238\(\pm\)0.068 & 0.236\(\pm\)0.012 & 0.484\(\pm\)0.042 \\  & TimeVAE & 0.041\(\pm\)0.044 & 0.145\(\pm\)1.20 & 0.209\(\pm\)0.058 & 0.230\(\pm\)0.102 & 0.499\(\pm\)0.000 & 0.476\(\pm\)0.044 \\  & Diffwave & 0.017\(\pm\)0.008 & 0.232\(\pm\)0.061 & 0.190\(\pm\)0.008 & 0.203\(\pm\)0.096 & 0.493\(\pm\)0.004 & 0.402\(\pm\)0.029 \\  & DiffTime & 0.013\(\pm\)0.006 & 0.097\(\pm\)0.016 & 0.100\(\pm\)0.007 & 0.154\(\pm\)0.045 & 0.445\(\pm\)0.004 & 0.245\(\pm\)0.051 \\  & Cot-GAN & 0.254\(\pm\)1.37 & 0.230\(\pm\)0.016 & 0.325\(\pm\)0.099 & 0.426\(\pm\)0.022 & 0.498\(\pm\)0.002 & 0.492\(\pm\)0.018 \\ \hline \multirow{9}{*}{Predictive Score\(\downarrow\)} & SDformer-ar & **0.093\(\pm\)0.000** & 0.037\(\pm\)0.000 & **0.118\(\pm\)0.002** & **0.007\(\pm\)0.001** & **0.249\(\pm\)0.000** & **0.091\(\pm\)0.002** \\  & SDformer-m & **0.093\(\pm\)0.000** & 0.037\(\pm\)0.000 & 0.119\(\pm\)0.002 & **0.007\(\pm\)0.001** & 0.250\(\pm\)0.000 & **0.091\(\pm\)0.001** \\  & Diffusion-TS & **0.093\(\pm\)0.000** & **0.036\(\pm\)0.000** & 0.119\(\pm\)0.002 & **0.007\(\pm\)0.000** & 0.250\(\pm\)0.000 & 0.099\(\pm\)0.000 \\  & TimeGAN & **0.093\(\pm\)0.019** & 0.038\(\pm\)0.001 & 0.124\(\pm\)0.001 & 0.025\(\pm\)0.003 & 0.273\(\pm\)0.004 & 0.126\(\pm\)0.002 \\  & TimeVAE & **0.093\(\pm\)0.000** & 0.039\(\pm\)0.000 & 0.126\(\pm\)0.004 & 0.012\(\pm\)0.002 & 0.292\(\pm\)0.000 & 0.113\(\pm\)0.003 \\  & Diffwave & **0.093\(\pm\)0.000** & 0.047\(\pm\)0.000 & 0.130\(\pm\)0.001 & 0.013\(\pm\)0.000 & 0.251\(\pm\)0.000 & 0.101\(\pm\)0.000 \\  & DiffTime & **0.093\(\pm\)0.000** & 0.038\(\pm\)0.001 & 0.121\(\pm\)0.004 & 0.010\(\pm\)0.01 & 0.252\(\pm\)0.000 & 0.100\(\pm\)0.000 \\  & Cot-GAN & 0.100\(\pm\)0.000 & 0.047\(\pm\)0.001 & 0.129\(\pm\)0.000 & 0.068\(\pm\)0.009 & 0.259\(\pm\)0.000 & 0.183\(\pm\)0.003 \\ \cline{2-10}  & Original & 0.094\(\pm\)0.001 & 0.036\(\pm\)0.001 & 0.121\(\pm\)0.005 & 0.007\(\pm\)0.001 & 0.250\(\pm\)0.003 & 0.090\(\pm\)0.001 \\ \hline \multirow{9}{*}{Context-FID Score\(\downarrow\)} & SDformer-ar & **0.001\(\pm\)0.000** & **0.002\(\pm\)0.000** & **0.008\(\pm\)0.001** & **0.005\(\pm\)0.001** & **0.003\(\pm\)0.000** & **0.015\(\pm\)0.001** \\  & SDformer-m & 0.010\(\pm\)0.002 & 0.034\(\pm\)0.008 & 0.019\(\pm\)0.003 & 0.030\(\pm\)0.003 & 0.041\(\pm\)0.005 & 0.035\(\pm\)0.003 \\  & Diffusion-TS & 0.006\(\pm\)0.000 & 0.147\(\pm\)0.025 & 0.116\(\pm\)0.010 & 0.013\(\pm\)0.001 & 0.089\(\pm\)0.024 & 0.105\(\pm\)0.006 \\  & TimeGAN & 0.101\(\pm\)0.014 & 0.103\(\pm\)0.013 & 0.300\(\pm\)0.013 & 0.563\(\pm\)0.052 & 0.767\(\pm\)1.03 & 1.292\(\pm\)2.18 \\  & TimeVAE & 0.307\(\pm\)0.060 & 0.215\(\pm\)0.035 & 0.805\(\pm\)1.86 & 0.251\(\pm\)0.15 & 1.631\(\pm\)1.42 & 14.44\(\pm\)9.969 \\  & Diffwave & 0.014\(\pm\)0.002 & 0.232\(\pm\)0.032 & 0.873\(\pm\)0.061 & 0.393\(\pm\)0.041 & 1.031\(\pm\)1.31 & 0.244\(\pm\)0.018 \\  & DiffTime & 0.006\(\pm\)0.001 & 0.236\(\pm\)0.074 & 0.299\(\pm\)0.044 & 0.188\(\pm\)0.028 & 0.279\(\pm\)0.045 & 0.340\(\pm\)0.015 \\  & Cot-GAN & 1.337\(\pm\)0.068 & 0.408\(\pm\)0.086 & 0.980\(\pm\)0.071 & 1.094\(\pm\)0.079 & 1.039\(\pm\)0.028 & 7.813\(\pm\)5.50 \\ \hline \end{tabular}
\end{table}
Table 2: Results of all methods on all datasetsApart from unconditional generation, we also explore the performance of our proposed methods in conditional generation tasks. Our objective is to evaluate the model's versatility in handling both conditional and unconditional tasks. More specifically, we aim to train a single model that can effectively manage both unconditional and conditional tasks under different settings. Referring to [42], we set \(\tau=48\), \(l=8,16,24,32\) in Equation (2), and then directly use the model trained under the unconditional generation task to complete the forecasting tasks under these different settings. Figure 3 displays several examples of forecasting tasks. The median values of forecasting are represented as the dotted line, and 5% and 95% quantiles are depicted as the shade areas (Green: SDformer-ar, Gray: Diffusion-TS). This demonstrates that SDformer-ar provides more reasonable forecasts with higher confidence compared to Diffusion-TS. Furthermore, more detailed results are illustrated in Figure 4. Based on these findings, the methods employing discrete token modeling demonstrates adaptability to both unconditional and conditional generation tasks of varying lengths without the need for retraining, while maintaining good performance.

### Ablation study

To understand the contribution of each component to proposed methods, we conduct ablation experiments for two aspects, 1) The impact of vector quantization methods based on different measurements 2) The advantages of discrete representations in time series generation. More experimental results refer to Appendix C, due to limited space.

**Effect of similarity-driven vector quantization in Equation (4).** For discrete token modeling method, the quality of discrete representations learning from continuous data determines the performance potential of the entire method, with vector quantization playing a crucial role. Therefore, we compare the impact of our proposed similarity-driven vector quantization with the commonly used distance-driven vector quantization on the overall method performance.

\begin{table}
\begin{tabular}{c|c|c c|c c c} \hline  & & \multicolumn{2}{c|}{ETTh} & \multicolumn{2}{c}{Energy} \\ \hline Metrics & Methods & 64 & 128 & 256 & 64 & 128 & 256 \\ \hline \multirow{5}{*}{Discriminative Score\(\downarrow\)} & SDformer-ar & **0.018\(\pm\)0.007** & **0.013\(\pm\)0.005** & **0.008\(\pm\)0.006** & **0.010\(\pm\)0.007** & **0.013\(\pm\)0.007** & **0.017\(\pm\)0.003** \\  & SDformer-m & 0.034\(\pm\)0.017 & 0.038\(\pm\)0.008 & 0.041\(\pm\)0.024 & 0.053\(\pm\)0.018 & 0.069\(\pm\)0.014 & 0.035\(\pm\)0.007 \\  & Diffusion-TS & 0.106\(\pm\)0.048 & 0.144\(\pm\)0.060 & 0.060\(\pm\)0.030 & 0.078\(\pm\)0.021 & 0.143\(\pm\)0.075 & 0.290\(\pm\)1.23 \\  & TimeGAN & 0.227\(\pm\)0.078 & 0.188\(\pm\)0.074 & 0.442\(\pm\)0.056 & 0.498\(\pm\)0.001 & 0.499\(\pm\)0.001 & 0.499\(\pm\)0.000 \\  & TimeVAE & 0.171\(\pm\)1.42 & 0.154\(\pm\)0.087 & 0.178\(\pm\)0.076 & 0.499\(\pm\)0.000 & 0.499\(\pm\)0.000 & 0.499\(\pm\)0.000 \\  & Diffwave & 0.254\(\pm\)0.074 & 0.274\(\pm\)0.047 & 0.304\(\pm\)0.068 & 0.497\(\pm\)0.004 & 0.499\(\pm\)0.001 & 0.499\(\pm\)0.000 \\  & DiffTime & 0.150\(\pm\)0.003 & 0.176\(\pm\)0.015 & 0.243\(\pm\)0.005 & 0.328\(\pm\)0.031 & 0.396\(\pm\)0.024 & 0.437\(\pm\)0.095 \\  & Cot-GAN & 0.296\(\pm\)3.48 & 0.451\(\pm\)0.080 & 0.461\(\pm\)0.010 & 0.499\(\pm\)0.001 & 0.499\(\pm\)0.001 & 0.498\(\pm\)0.004 \\ \hline \multirow{5}{*}{Predictive Score\(\downarrow\)} & SDformer-ar & **0.116\(\pm\)0.006** & 0.110\(\pm\)0.007 & **0.095\(\pm\)0.003** & **0.247\(\pm\)0.001** & **0.244\(\pm\)0.000** & **0.243\(\pm\)0.002** \\  & SDformer-m & 0.120\(\pm\)0.004 & **0.107\(\pm\)0.004** & 0.110\(\pm\)0.007 & 0.248\(\pm\)0.001 & 0.245\(\pm\)0.000 & 0.244\(\pm\)0.003 \\  & Diffusion-TS & **0.116\(\pm\)0.000** & 0.110\(\pm\)0.003 & 0.109\(\pm\)0.013 & 0.249\(\pm\)0.000 & 0.247\(\pm\)0.001 & 0.245\(\pm\)0.001 \\  & TimeGAN & 0.132\(\pm\)0.008 & 0.153\(\pm\)0.014 & 0.220\(\pm\)0.008 & 0.291\(\pm\)0.003 & 0.303\(\pm\)0.002 & 0.351\(\pm\)0.004 \\  & TimeVAE & 0.118\(\pm\)0.004 & 0.113\(\pm\)0.005 & 0.110\(\pm\)0.027 & 0.302\(\pm\)0.001 & 0.318\(\pm\)0.000 & 0.353\(\pm\)0.003 \\  & Diffwave & 0.133\(\pm\)0.008 & 0.129\(\pm\)0.033 & 0.132\(\pm\)0.001 & 0.252\(\pm\)0.001 & 0.252\(\pm\)0.000 & 0.251\(\pm\)0.000 \\  & DiffTime & 0.118\(\pm\)0.004 & 0.120\(\pm\)0.008 & 0.118\(\pm\)0.003 & 0.252\(\pm\)0.000 & 0.251\(\pm\)0.000 & 0.251\(\pm\)0.000 \\  & Cot-GAN & 0.135\(\pm\)0.003 & 0.126\(\pm\)0.001 & 0.129\(\pm\)0.000 & 0.262\(\pm\)0.002 & 0.269\(\pm\)0.002 & 0.275\(\pm\)0.004 \\ \cline{2-7}  & Original & 0.114\(\pm\)0.006 & 0.108\(\pm\)0.005 & 0.106\(\pm\)0.010 & 0.245\(\pm\)0.002 & 0.243\(\pm\)0.000 & 0.243\(\pm\)0.000 \\ \hline \multirow{5}{*}{Context-FID Score\(\downarrow\)} & SDformer-ar & **0.018\(\pm\)0.003** & **0.024\(\pm\)0.001** & **0.021\(\pm\)0.001** & **0.031\(\pm\)0.002** & **0.036\(\pm\)0.002** & **0.041\(\pm\)0.003** \\  & SDformer-m & 0.086\(\pm\)0.008 & 0.094\(\pm\)0.007 & 0.078\(\pm\)0.006 & 0.160\(\pm\)0.025 & 0.151\(\pm\)0.011 & 0.136\(\pm\)0.014 \\  & Diffusion-TS & 0.631\(\pm\)0.058 & 0.787\(\pm\)0.062 & 0.423\(\pm\)0.038 & 0.135\(\pm\)0.017 & 0.087\(\pm\)0.019 & 0.126\(\pm\)0.024 \\  & TimeGAN & 1.130\(\pm\)1.02 & 1.553\(\pm\)1.169 & 5.872\(\pm\)2.008 & 1.230\(\pm\)0.070 & 2.535\(\pm\)3.372 & 5.032\(\pm\)8.31 \\  & TimeVAE & 0.827\(\pm\)1.46 & 1.062\(\pm\)1.34 & 0.826\(\pm\)0.093 & 2.662\(\pm\)0.087 & 3.125\(\pm\)1.066 & 3.768\(\pm\)9.998 \\  & Diffwave & 1.543\(\pm\)1.53 & 2.354\(\pm\)1.170 & 2.899\(\pm\)2.89 & 2.697\(\pm\)4.18 & 5.552\(\pm\)5.28 & 5.572\(\pm\)5.584 \\  & DiffTime & 1.279\(\pm\)0.083 & 2.554\(\pm\)3.18 & 3.524\(\pm\)8.30 & 0.762\(\pm\)1.57 & 1.344\(\pm\)1.31 & 4.735\(\pm\)7.729 \\  & Cot-GAN & 3.008\(\pm\)2.77 & 2.639\(\pm\)4.427 & 4.075\(\pm\)8.94 & 1.824\(\pm\The term "w/o similarity" in Table 4 denotes the variant of the corresponding method that employs distance-driven vector quantization instead of similarity-driven vector quantization. As per the comprehensive results, similarity-driven vector quantization significantly outperforms distance-driven vector quantization. For instance, the discriminative score witnessed an increase of 35.6% for SDformer-ar and 46.2% for SDformer-m.

**Effect of discrete token in Equation (4) and (5).** To explore the impact of discrete versus continuous tokens, we introduce a variant replacing the original discrete token with a continuous one within a similar framework. Initially, we substitute VQ-VAE with VAE as per [14] to encode a continuous latent space. In the second stage, we reconfigure the Transformer to accommodate continuous inputs, altering its initial input strategy due to the inapplicability of a fixed token like [BOS]. Thus, the first token of each time series is not used as the prediction target during training. For inference, we devised two methods: one involves sampling from a multivariate Gaussian, calculated from all initial training tokens, for generating the first token; the second uses the actual initial token directly. Although the latter does not lend itself to a fair comparison with the original model, it is included for a more comprehensive evaluation as a reference.

In Table 4, the term "continuous" represents the first variant, which involves using continuous tokens in place of discrete ones. Meanwhile, the term "continuous, w/ first" denotes the second variant, which builds upon the first by providing the actual first token during inference. It is worth noting that, as SDformer-m necessitates the utilization of category sampling during inference for achieving

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c} \hline Metrics & Methods & Sines & Stocks & ETHh & MulJoCo & Energy & fMRI \\ \hline \multirow{4}{*}{Discriminative Score\(\downarrow\)} & SDformer-ar & **0.006\(\pm\).004** & **0.010\(\pm\).006** & **0.003\(\pm\).001** & **0.008\(\pm\).005** & **0.006\(\pm\).004** & **0.017\(\pm\).007** \\  & w/o similarity & 0.006\(\pm\).004 & 0.011\(\pm\).007 & 0.010\(\pm\).005 & 0.013\(\pm\).003 & 0.018\(\pm\).005 & 0.024\(\pm\).003 \\  & continuous & 0.047\(\pm\).012 & 0.065\(\pm\).012 & 0.145\(\pm\).020 & 0.055\(\pm\).013 & 0.322\(\pm\).012 & 0.243\(\pm\).214 \\  & continuous, w/ first & 0.012\(\pm\).004 & 0.021\(\pm\).015 & 0.006\(\pm\).004 & 0.020\(\pm\).006 & 0.277\(\pm\).007 & 0.074\(\pm\).006 \\ \cline{2-8}  & SDformer-m & **0.008\(\pm\).004** & **0.020\(\pm\).011** & **0.022\(\pm\).001** & **0.025\(\pm\).007** & **0.062\(\pm\).006** & **0.043\(\pm\).006** \\  & w/o similarity & 0.015\(\pm\).007 & 0.081\(\pm\).010 & 0.055\(\pm\).004 & 0.070\(\pm\).005 & 0.068\(\pm\).005 & 0.055\(\pm\).009 \\ \hline \multirow{4}{*}{Predictive Score\(\downarrow\)} & SDformer-ar & **0.093\(\pm\).000** & **0.037\(\pm\).000** & **0.118\(\pm\).002** & **0.007\(\pm\).001** & **0.249\(\pm\).00** & 0.091\(\pm\).002 \\  & w/o similarity & **0.093\(\pm\).000** & **0.037\(\pm\).000** & 0.122\(\pm\).002 & 0.008\(\pm\).001 & **0.249\(\pm\).00** & 0.091\(\pm\).002 \\  & continuous & **0.093\(\pm\).000** & 0.038\(\pm\).000 & 0.124\(\pm\).003 & 0.009\(\pm\).001 & 0.255\(\pm\).000 & 0.105\(\pm\).000 \\  & continuous, w/ first & **0.093\(\pm\).000** & **0.037\(\pm\).000** & 0.122\(\pm\).003 & **0.007\(\pm\).001** & 0.251\(\pm\).000 & **0.087\(\pm\).003** \\ \cline{2-8}  & SDformer-m & **0.093\(\pm\).000** & **0.037\(\pm\).000** & **0.119\(\pm\).002** & **0.007\(\pm\).001** & **0.250\(\pm\).000** & **0.091\(\pm\).001** \\  & w/o similarity & **0.093\(\pm\).000** & **0.037\(\pm\).000** & 0.123\(\pm\).001 & 0.008\(\pm\).001 & **0.250\(\pm\).000** & 0.093\(\pm\).000 \\ \hline \multirow{4}{*}{Context-FID Score\(\downarrow\)} & SDformer-ar & **0.001\(\pm\).00** & **0.002\(\pm\).00** & 0.008\(\pm\).001 & **0.005\(\pm\).001** & **0.003\(\pm\).00** & 0.015\(\pm\).001 \\  & w/o similarity & 0.002\(\pm\).000 & 0.012\(\pm\).001 & 0.013\(\pm\).001 & **0.005\(\pm\).001** & 0.004\(\pm\).000 & 0.011\(\pm\).000 \\ \cline{1-1}  & continuous & 0.056\(\pm\).004 & 0.101\(\pm\).02 & 0.433\(\pm\).049 & 0.065\(\pm\).008 & 0.213\(\pm\).022 & 5.512\(\pm\).390 \\ \cline{1-1}  & continuous, w/ first & 0.004\(\pm\).000 & 0.015\(\pm\).003 & **0.002\(\pm\).000** & 0.006\(\pm\).001 & 0.021\(\pm\).003 & **0.003\(\pm\).000** \\ \cline{1-1} \cline{2-8}  & SDformer-m & **0.010\(\pm\).002** & **0.034\(\pm\).008** & **0.019\(\pm\).003** & **0.030\(\pm\).003** & **0.041\(\pm\).005** & **0.035\(\pm\).003** \\ \cline{1-1}  & w/o similarity & 0.044\(\pm\).006 & 0.123\(\pm\).009 & 0.106\(\pm\).012 & 0.098\(\pm\).009 & 0.062\(\pm\).014 & 0.038\(\pm\).002 \\ \hline \end{tabular}
\end{table}
Table 4: Results of ablation study.

Figure 4: Performance for time series forecasting and generation under different setting. All forecasting tasks utilize Mean Square Error (MSE) as performance metric, while unconditional generation tasks employ Discriminative Scores (DS). Note: The data in this figure has been scaled by a factor of 100 for the forecasting tasks and 10 for the unconditional generation tasks to streamline the presentation.

iterative sampling, we abstain from conducting ablation experiments on discrete tokens specifically for this model. Based on the results, the model's performance experiences a significant decline upon the removal of discrete tokens. Even when incorporating the condition information of the first token, it often fails to surpass the original method in most scenarios.

## 6 Conclusions

In this paper, we present discrete token modeling for the time series generation tasks and propose a innovative two-stage model. Specifically, it is built upon an efficient time series tokenizer, which attains high-quality discrete token representations through similarity-driven vector quantization. Leveraging this foundation, we employ autoregressive token modeling and masked token modeling techniques to learn the distribution of time series data at the discrete token level. Experimental results showcase the efficacy and adaptability of our approach in various time series generation tasks. Owing to the flexibility of our method in the second stage, future work could explore referencing more efficient language models to design increasingly effective time series generation strategies.

## Acknowledgements

We would like to thank Tencent AI Lab for supporting Zhicheng Chen as a student researcher during his internship. The study was partially supported by the Key Laboratory of Data Protection and Intelligent Management, Ministry of Education, Sichuan University and also the Fundamental Research Funds for the Central Universities under Grant SCU2023D008.

## References

* [1]J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. Leoni, A. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. (2023) Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Cited by: SS1.
* [2]A. Alaa, A. J. Chan, and M. van der Schaar (2020) Generative time-series modeling with fourier flows. In International Conference on Learning Representations, Cited by: SS1.
* [3]R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al. (2023) Palm 2 technical report. arXiv preprint arXiv:2305.10403. Cited by: SS1.
* [4]H. Chang, H. Zhang, J. Barber, A. Maschinot, J. Lezama, L. Jiang, M. Yang, K. P. Murphy, W. T. Freeman, M. Rubinstein, Y. Li, and D. Krishnan (2020) Muse: text-to-image generation via masked generative transformers. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, Cited by: SS1.
* [5]H. Chang, H. Zhang, L. Jiang, C. Liu, and W. T. Freeman (2022) Maskgit: masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11315-11325. Cited by: SS1.
* [6]A. Coletta, S. Gopalakrishnan, D. Borrajo, and S. Vyetrenko (2024) On the constrained time-series generation problem. Advances in Neural Information Processing Systems36. Cited by: SS1.
* [7]A. Desai, C. Freeman, Z. Wang, and I. Beaver (2021) TimeVAE: a variational auto-encoder for multivariate time series generation. arXiv preprint arXiv:2111.08095. Cited by: SS1.
* [8]J. Devlin, M. Chang, K. Lee, and K. Toutanova (2019) BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), Cited by: SS1.
* [9]Q. Ding, S. Wu, H. Sun, J. Guo, and J. Guo (2020) Hierarchical multi-scale gaussian transformer for stock movement prediction. In IJCAI, pp. 4640-4646. Cited by: SS1.

* Esser et al. [2021] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 12873-12883, 2021.
* Esteban et al. [2017] Cristobal Esteban, Stephanie L Hyland, and Gunnar Ratsch. Real-valued (medical) time series generation with recurrent conditional gans. _arXiv preprint arXiv:1706.02633_, 2017.
* Feng et al. [2023] Shibo Feng, Chunyan Miao, Ke Xu, Jiaxiang Wu, Pengcheng Wu, Yang Zhang, and Peilin Zhao. Multi-scale attention flow for probabilistic time series forecasting. _IEEE Transactions on Knowledge and Data Engineering_, 2023.
* Feng et al. [2024] Shibo Feng, Chunyan Miao, Zhong Zhang, and Peilin Zhao. Latent diffusion transformer for probabilistic time series forecasting. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 11979-11987, 2024.
* Ho et al. [2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.
* Hochreiter and Schmidhuber [1997] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. _Neural computation_, 9(8):1735-1780, 1997.
* Jeha et al. [2022] Paul Jeha, Michael Bohlke-Schneider, Pedro Mercado, Shubham Kapoor, Rajbir Singh Nirwan, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Pas-gan: Progressive self attention gans for synthetic time series. In _The Tenth International Conference on Learning Representations_, 2022.
* Jeon et al. [2022] Jinsung Jeon, Jeonghak Kim, Haryong Song, Seunghyeon Cho, and Noseong Park. Gt-gan: General purpose time series synthesis with generative adversarial networks. _Advances in Neural Information Processing Systems_, 35:36999-37010, 2022.
* Jiang et al. [2024] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as a foreign language. _Advances in Neural Information Processing Systems_, 36, 2024.
* Kong et al. [2021] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_.
* Lim et al. [2021] Bryan Lim, Sercan O'Ark, Nicolas Loeff, and Tomas Pfister. Temporal fusion transformers for interpretable multi-horizon time series forecasting. _International Journal of Forecasting_, 37(4):1748-1764, 2021.
* Lim and Zohren [2021] Bryan Lim and Stefan Zohren. Time-series forecasting with deep learning: a survey. _Philosophical Transactions of the Royal Society A_, 379(2194):20200209, 2021.
* Liu et al. [2016] Chenghao Liu, Steven CH Hoi, Peilin Zhao, and Jianling Sun. Online arima algorithms for time series prediction. In _Proceedings of the AAAI conference on artificial intelligence_, volume 30, 2016.
* Loshchilov [2017] I Loshchilov. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* Mogren [2016] Olof Mogren. C-rnn-gan: Continuous recurrent neural networks with adversarial training. _arXiv preprint arXiv:1611.09904_, 2016.
* Naiman et al. [2024] Ilan Naiman, N. Benjamin Erichson, Pu Ren, Michael W. Mahoney, and Omri Azencot. Generative modeling of regular and irregular time series data via koopman VAEs. In _The Twelfth International Conference on Learning Representations_, 2024.
* Pei et al. [2021] Hengzhi Pei, Kan Ren, Yuqing Yang, Chang Liu, Tao Qin, and Dongsheng Li. Towards generating real-world time series data. In _2021 IEEE International Conference on Data Mining (ICDM)_, pages 469-478. IEEE, 2021.
* Penfold and Zhang [2013] Robert B Penfold and Fang Zhang. Use of interrupted time series analysis in evaluating health care quality improvements. _Academic pediatrics_, 13(6):S38-S44, 2013.
* Qiu et al. [2017] Minghui Qiu, Peilin Zhao, Ke Zhang, Jun Huang, Xing Shi, Xiaoguang Wang, and Wei Chu. A short-term rainfall prediction model using multi-task convolutional neural networks. In _2017 IEEE international conference on data mining (ICDM)_, pages 395-404. IEEE, 2017.
* Ramesh et al. [2021] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _International conference on machine learning_, pages 8821-8831. Pmlr, 2021.

* [30] Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks. _IEEE transactions on Signal Processing_, 45(11):2673-2681, 1997.
* [31] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. _Advances in neural information processing systems_, 30, 2017.
* [32] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. _Journal of machine learning research_, 9(11), 2008.
* [33] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual descriptions. In _International Conference on Learning Representations_, 2022.
* [34] Will Williams, Sam Ringer, Tom Ash, David MacLeod, Jamie Dougherty, and John Hughes. Hierarchical quantized autoencoders. _Advances in Neural Information Processing Systems_, 33:4524-4535, 2020.
* [35] Sifan Wu, Xi Xiao, Qianggang Ding, Peilin Zhao, Ying Wei, and Junzhou Huang. Adversarial sparse transformer for time series forecasting. _Advances in neural information processing systems_, 33:17105-17115, 2020.
* [36] Ke Xu, Yifan Zhang, Deheng Ye, Peilin Zhao, and Mingkui Tan. Relation-aware transformer for portfolio policy learning. In _Proceedings of the twenty-ninth international conference on international joint conferences on artificial intelligence_, pages 4647-4653, 2021.
* [37] Tianlin Xu, Li Kevin Wenliang, Michael Munn, and Beatrice Acciaio. Cot-gan: Generating sequential data via causal optimal transport. _Advances in neural information processing systems_, 33:8798-8809, 2020.
* [38] Jaemin Yoo, Yejun Soun, Yong-chan Park, and U Kang. Accurate multivariate stock movement prediction via data-axis transformer with multi-level contexts. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 2037-2045, 2021.
* [39] Jinsung Yoon, Daniel Jarrett, and Mihaela Van der Schaar. Time-series generative adversarial networks. _Advances in neural information processing systems_, 32, 2019.
* [40] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. _Trans. Mach. Learn. Res._, 2022.
* [41] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander G Hauptmann, Ming-Hsuan Yang, Yuan Hao, Iffan Essa, et al. Magvit: Masked generative video transformer. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10459-10469, 2023.
* [42] Xinyu Yuan and Yan Qiao. Diffusion-ts: Interpretable diffusion for general time series generation. _arXiv preprint arXiv:2403.01742_, 2024.
* [43] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong Zhang, Hongwei Zhao, Hongtao Lu, and Xi Shen. T2m-gpt: Generating human motion from textual descriptions with discrete representations. _arXiv preprint arXiv:2301.06052_, 2023.
* [44] Peilin Zhao and Tong Zhang. Stochastic optimization with importance sampling for regularized loss minimization. In _international conference on machine learning_, pages 1-9. PMLR, 2015.
* [45] Tian Zhou, Peisong Niu, Liang Sun, Rong Jin, et al. One fits all: Power general time series analysis by pretrained lm. _Advances in neural information processing systems_, 36:43322-43355, 2023.

**Supplementary Materials for SDformer**

In the supplementary, we provide more implementation details, more experimental results, and visualization of test samples of our SDformer. We organize our supplementary as follows

* In Section A, we give the detailed description of used datasets and the metrics.
* In Section B, we provide the experiment settings.
* In Section C, we show more experimental results to verify the effectiveness and efficiency of SDformer.
* In Section D, we provide the details operations of the architecture in stage 1 and 2.
* In Section E, we provide the algorithms of training and inference in SDformer.
* In Section F, we specify the limitations of our method.
* In Section G, we showcase more visualization results and test samples on six TSG datasets.

## Appendix A Dataset and metric details

**Dataset.**. The **Stocks** dataset consists of Google's stock price data between 2004 and 2019, with each observation representing a day and containing 6 features. The **ETTh** dataset includes data obtained from electrical transformers, encompassing load and oil temperature measurements taken every 15 minutes from July 2016 to July 2018. The **Energy** dataset, a UCI appliance energy prediction dataset, comprises 28 features. The **fMRI** dataset serves as a benchmark for causal discovery and features realistic simulations of blood-oxygen-level-dependent (BOLD) time series; we chose a simulation with 50 features from the original dataset referring to [42]. The **Sines** dataset contains 5 features, each generated independently with varying frequencies and phases. The **MuJoCo** dataset is a multivariate physics simulation time series dataset with 14 features.

**Metric.** The **Discriminative Score** quantifies the similarity between original and synthesized data. Initially, a classification model is trained using both the original and synthesized data. Subsequently, the model's capability to classify these data types is assessed, and the discriminative score is computed as \(|Accuracy-0.5|\). The **Predictive Score** evaluates the usefulness of the synthesized data by training a post-hoc sequence model to predict next-step temporal vectors using the train-synthesis-and-test-real (TSTR) method. The **Context-FID Score** quantifies the quality of the synthetic time series samples by computing the difference between representations of time series that fit into the local context.

## Appendix B Experimental settings

In this part, we introduce our main experimental settings. For the unconditional generation task, we conduct five evaluations to obtain the experimental results. Similarly, for the conditional generation task, we run the process five times when computing the performance metrics, and escalate it to 1000 runs when calculating the median, along with the 5% and 95% quantiles of the predicted samples. Furthermore, we summarize the detailed hyperparameters of SDformer, shown as Table 5. The two values in {*,*} are the hyperparameters of SDformer-ar and SDformer-m respectively. Our primary experiments are executed on an Nvidia V-100 GPU with the AdamW [23] optimizer. In the future, we can use importance sampling [44] to further accelerate it.

\begin{table}
\begin{tabular}{l|c c c c c c|c c c c c} \multicolumn{1}{c}{} & \multicolumn{4}{c}{Stage 1} & \multicolumn{4}{c}{Stage 2} \\ \hline Dataset & Hidden dim & Enc/Dec Layers & \(K\) & \(d_{e}\) & \(\lambda\) & r & Hidden dim & Transformer Layers & \(\pi\) & N & S \\ \hline Sines & 512 & 2 & 1024 & 512 & 0.5 & 4 & 1024 & 2 & 0.3 & 6 & [5,4,3,2,1,0] \\ Stocks & 512 & 2 & 512 & 256 & 2.0 & 4 & 1024 & 2 & 0.3 & 3 & [5,3,0] \\ ETH & [512, 1024] & 2 & 512 & 512 & 0.5 & 4 & 1024 & 6 & 0.3 & 6 & [5,4,3,2,1,0] \\ MuJoCo & 512 & 2 & 512 & 512 & 0.5 & 4 & 1024 & [2,6] & 0.1 & 6 & [5,4,3,2,1,0] \\ Energy & 512 & 2 & 512 & 512 & 0.01 & 4 & 1024 & 2 & 0.1 & 6 & [5,4,3,2,1,0] \\ fMRI & 512 & {1,2} & 512 & 512 & 0.01 & [2,4] & 1024 & 2 & 0.1 & 6 & [5,4,3,2,1,0] \\ \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \end{tabular}
\end{table}
Table 5: Detailed hyperparameters of SDformer.

[MISSING_PAGE_FAIL:14]

in Table 8. We can observe that SDformer maintains competitive performance even with reduced model parameters and showcases notably faster inference times compared to Diffusion-TS.

## Appendix D Model details

In this section, we present the detailed network architecture of the SDformer. The time series tokenizer in stage 1 primarily consists of 1D Convolution and 1D ResNet networks, as illustrated in Tables 9 and 10. In stage 2, the Autoregressive Transformer and Masked Transformer are implemented as standard decoder-only and encoder-only Transformer, respectively. The specifics of their blocks are depicted in Tables 11 and 12.

## Appendix E Algorithms

In this section, we detail the training and inference algorithms for SDformer. Specifically, Algorithm 1 outlines the training procedure for the time series tokenizer in the first stage, while Algorithm 2 and 3 elucidate the training processes for the Autoregressive Transformer and Masked Transformer in the second stage, respectively. Lastly, Algorithm 4 and 5 represent the inference procedures for SDformer-ar and SDformer-m, respectively.

\begin{table}
\begin{tabular}{c|c c c c} \hline Model Size (M) & 1.4 & 3.0 & 11.9 & 44.9 \\ \hline Discriminative Score \(\downarrow\) & 0.149\(\pm\)0.007 & 0.084\(\pm\)0.009 & 0.011\(\pm\)0.009 & 0.006\(\pm\).004 \\ Context-FID Score \(\downarrow\) & 0.033\(\pm\)0.003 & 0.022\(\pm\)0.002 & 0.004\(\pm\)0.000 & 0.003\(\pm\).000 \\ \hline \end{tabular}
\end{table}
Table 7: Performance discrepancy of SDformer across different model sizes on the Energy Dataset.

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline Metrics & Methods & Sines & Stocks & ETH \\ \hline Discriminative & SDformer-s & **0.003\(\pm\)0.003** & **0.019\(\pm\)0.010** & **0.023\(\pm\)0.001** \\ Score\(\downarrow\) & Diffusion-TS & 0.006\(\pm\)0.007 & 0.067\(\pm\)0.15 & 0.061\(\pm\)0.009 \\ \hline Context-FID & SDformer-s & **0.006\(\pm\)0.000** & **0.015\(\pm\)0.002** & **0.071\(\pm\)0.001** \\ Score\(\downarrow\) & Diffusion-TS & **0.006\(\pm\)0.000** & 0.147\(\pm\)0.25 & 0.116\(\pm\)0.010 \\ \hline Inference & SDformer-s & 2.68 & 2.59 & 2.67 \\ Time (s) & Diffusion-TS & 27.12 & 30.01 & 32.97 \\ \hline Model & SDformer-s & 0.17 & 0.16 & 0.28 \\ Size (M) & Diffusion-TS & 0.24 & 0.29 & 0.35 \\ \hline \end{tabular}
\end{table}
Table 8: The comparison results of small version SDformer (SDformer-s) against the baseline Diffusion-TS on Sines, Stocks and ETH datasets.

\begin{table}
\begin{tabular}{c|c|c|c} Layer & Function & Descriptions \\ \hline
1 & Convolution & input channel=\(d\), output channel=D, kernel size=3, stride=1, padding=1 \\
2 & ReLU & nn.ReLU() \\
3 & Convolution & input channel=D, output channel=D, kernel size=4, stride=2, padding=1 \\
4 & ResNet & input channel=D, depth=3, dilation growth rate=3 \\
5 & ReLU & nn.ReLU() \\
6 & Convolution & input channel=D, output channel=D, kernel size=4, stride=2, padding=1 \\
7 & ResNet & input channel=D, depth=3, dilation growth rate=3 \\
8 & ReLU & nn.ReLU() \\
9 & Convolution & input channel=\(d_{c}\), output channel=H, kernel size=3, stride=1, padding=1 \\ \hline \end{tabular}
\end{table}
Table 9: The detailed architecture of the time serie tokenizer’s encoder.

\begin{table}
\begin{tabular}{|c|c|c|} Layer & Function & Descriptions \\ \hline
1 & Convolution & input channel=\(d_{c}\), output channel=D, kernel size=3, stride=1, padding=1 \\
2 & ReLU & nn.ReLU() \\
3 & ResNet & input channel=D, depth=3, dilation growth rate=3 \\
4 & ReLU & nn.ReLU() \\
5 & Upsample & nn.Upsample() \\
6 & Convolution & input channel=D, output channel=D, kernel size=3, stride=1, padding=1 \\
7 & ResNet & input channel=D, depth=3, dilation growth rate=3 \\
8 & ReLU & nn.ReLU() \\
9 & Upsample & nn.Upsample() \\
10 & Convolution & input channel=D, output channel=D, kernel size=3, stride=1, padding=1 \\
11 & ReLU & nn.ReLU() \\
12 & Convolution & input channel=D, output channel=D, kernel size=3, stride=1, padding=1 \\
13 & ReLU & nn.ReLU() \\
14 & Convolution & input channel=D, output channel=d, kernel size=3, stride=1, padding=1 \\ \hline \end{tabular}
\end{table}
Table 10: The detailed architecture of the time serie tokenizer’s decoder.

\begin{table}
\begin{tabular}{|c|c|c|} Layer & Function & Descriptions \\ \hline
1 & Layernorm & nn.LayerNorm() \\
2 & Casal-attention & CasalAttention(q=x, k=x, v=x) \\
3 & Layernorm & nn.LayerNorm() \\
4 & MLP & nn.Linear() \\
5 & ReLU & nn.ReLU() \\
6 & MLP & nn.Linear() \\ \hline \end{tabular}
\end{table}
Table 11: The detailed architecture of the Autoregressive Transformer block.

\begin{table}
\begin{tabular}{|c|c|c|} Layer & Function & Descriptions \\ \hline
1 & Layernorm & nn.LayerNorm() \\
2 & Self-attention & Attention(q=x, k=x, v=x) \\
3 & Layernorm & nn.LayerNorm() \\
4 & MLP & nn.Linear() \\
5 & ReLU & nn.ReLU() \\
6 & MLP & nn.Linear() \\ \hline \end{tabular}
\end{table}
Table 12: The detailed architecture of the Masked Transformer block.

```
0: Time series dataset \(D=\{X^{i}_{1:\tau}\}_{i=1}^{n}\), optimized time series tokenizer.
0: Autoregressive Transformer \(\mathcal{G}_{ar}\).
1:for\(k\gets 1\) to \(K\)do
2: Get the discrete tokens \(Y_{1:L}\) by \(\mathcal{E}\) and Equation (4);
3: Get the shifted tokens \(Y^{in}_{1:L}\) by shifting \(Y_{1:L}\) and appending [BOS] token;
4: Update \(Y^{in}_{1:L}\) by applying random replacement via Equation (8);
5: Feed the \(Y^{in}_{1:L}\) to Autoregressive Transformer \(\mathcal{G}_{ar}\);
6: Compute the training loss \(\mathcal{L}_{ar}\) by Equation (7);
7: Complete backpropagation process based on \(\mathcal{L}_{ar}\) and update the parameters of \(\mathcal{G}_{ar}\);
8:endfor
9: Return trained \(\mathcal{G}_{ar}\). ```

**Algorithm 2** Training of Autoregressive Transformer.

## Appendix F Limitations

If the goal is to achieve superior generation capabilities, the time series tokenizer must possess a relatively large codebook and a higher number of parameters. However, this will result in increased memory pressure.

## Appendix G Additional visualizations

To visualize the performance of unconditional time series generation, we adopt three visualization methods: 1) projecting original and synthetic data in a 2-dimensional space using t-SNE [32]; 2) projecting original and synthetic data in a 2-dimensional space using Principal Component Analysis (PCA); 3) drawing data distributions using Kernel Density Estimation (KDE). Figure 3, along with Figures 6 through 8, display the visual outcomes of unconditional generation. It is evident that the data generated by SDformer more closely aligns with the actual data compared to that produced by Diffusion-TS.

Figures 9 through 12 show the prediction samples of SDformer in different datasets and different prediction lengths. The median values of forecasting are represented as the dotted line, and 5% and 95% quantiles are depicted as the shade areas. As per the results, SDformer generates precise predictions in the majority of examples, signifying its strong performance in conditional generation tasks. Additionally, SDformer eliminates the need for retraining when undertaking these prediction tasks, emphasizing its remarkable versatility.

Figure 6: t-SNE visualizations of the time series synthesized by SDformer and Diffusion-TS on the Sines, Stocks, MuJoCo and fMRI datasets.

Figure 8: PCA visualizations of the time series synthesized by SDformer and Diffusion-TS across all datasets.

Figure 7: Kernel density estimation visualizations of the time series synthesized by SDformer and Diffusion-TS on the Sines, Stocks, MuJoCo and fMRI datasets..

Figure 9: Examples of time series forecasting for the Energy dataset with a prediction length of 24. Green colors correspond to Predictions of SDformer.

Figure 10: Examples of time series forecasting for the Energy dataset with a prediction length of 32. Green colors correspond to Predictions of SDformer.

Figure 11: Examples of time series forecasting for the fMRI dataset with a prediction length of 24. Green colors correspond to Predictions of SDformer.

Figure 12: Examples of time series forecasting for the fMRI dataset with a prediction length of 32. Green colors correspond to Predictions of SDformer.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We have added the contributions and scope in the abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have added the limitations in Appendix. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: The paper does not include theoretical results. Guidelines:* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The experimental results all can be reproduced. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: We will release all the experimental related codes upon acceptance.

Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have specified the details of experiment in Experimental section and Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All the experimental results are reported with mean and variances. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean.

* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We have provided the required compute resources. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research is conform to the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The societal impact of our work lies primarily in its potential to broaden the applicability of machine learning models in real-world scenarios. For example, in critical sectors such as transportation, energy, finance, and healthcare, the integration of advanced forecasting methods can provide decision-makers with substantial support, thereby facilitating the realization of intelligent decision-making ideals. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have cited the original paper that produced the code package and dataset. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.