# 4-bit Shampoo for Memory-Efficient Network Training

Sike Wang

Beijing Normal University

sikewang@mail.bnu.edu.cn

&Pan Zhou

Singapore Management University

panzhou@smu.edu.sg

Jia Li

Beijing Normal University

jiali@bnu.edu.cn

&Hua Huang

Beijing Normal University

huahuang@bnu.edu.cn

Code is available at https://github.com/Sike-Wang/low-bit-Shampoo.

Beijing Normal University

huahuang@bnu.edu.cn

###### Abstract

Second-order optimizers, maintaining a matrix termed a preconditioner, are superior to first-order optimizers in both theory and practice. The states forming the preconditioner and its inverse root restrict the maximum size of models trained by second-order optimizers. To address this, compressing 32-bit optimizer states to lower bitwidths has shown promise in reducing memory usage. However, current approaches only pertain to first-order optimizers. In this paper, we propose the first 4-bit second-order optimizers, exemplified by 4-bit Shampoo, maintaining performance similar to that of 32-bit ones. We show that quantizing the eigenvector matrix of the preconditioner in 4-bit Shampoo is remarkably better than quantizing the preconditioner itself both theoretically and experimentally. By rectifying the orthogonality of the quantized eigenvector matrix, we enhance the approximation of the preconditioner's eigenvector matrix, which also benefits the computation of its inverse 4-th root. Besides, we find that linear square quantization slightly outperforms dynamic tree quantization when quantizing second-order optimizer states. Evaluation on various networks for image classification and natural language modeling demonstrates that our 4-bit Shampoo achieves comparable performance to its 32-bit counterpart while being more memory-efficient1.

Footnote 1: Code is available at https://github.com/Sike-Wang/low-bit-Shampoo.

## 1 Introduction

Deep neural networks (DNNs) have achieved great success in numerous fields, e.g., computer vision [20], natural language processing [38], and speech recognition [16]. A significant part of such success is attributed to first-order optimizers such as stochastic gradient descent with momentum (SGDM) [31] and AdamW [29]. Second-order optimizers, including K-FAC [30], Shampoo [18], AdaBK [41], CASPR [13], and Sophia [27], show great convergence properties, but often involve noticeable computation and memory costs. Anil et al. [2] provided several practical techniques for second-order optimizers to achieve substantial wall-clock time improvements over traditional first-order optimizers. The fast convergence property of second-order optimizers benefits from preconditioning the gradient with a matrix known as a preconditioner. The optimizer states for constructing the preconditioner and its inverse root can speed up optimization compared to first-order optimizers, but consume memory that could be used for model parameters, limiting the maximum model size trained within a given memory budget. With the increase in model size, the memory utilized by optimizer states can become a predominant factor in memory usage. This is the primary obstacle hindering the widespread use of second-order optimizers in the era of large models.

There are two main attempts to reduce memory consumed by optimizer states. Factorization uses low-rank approximation to optimizer states. This strategy has been applied to first-order optimizers [35, 3]and second-order optimizers [14; 40]. In a comparable but distinct line of work, quantization utilizes low-bit to compress 32-bit optimizer states. Quantization is attractive due to its simplicity and wide applicability, which has been applied to first-order optimizers [8; 26]. Applying quantization to second-order optimizers poses a greater challenge, as first-order optimizers' states are elementwise, whereas second-order optimizers rely on matrix operations. To our knowledge, it has not been attempted before.

**Contributions:** In this paper, we present the first second-order optimizers with 4-bit optimizer states by taking Shampoo [18] as an example, while preserving the performance achieved with 32-bit optimizer states. While our focus is on Shampoo, we believe that our approach could also be applied to other second-order optimizers (see Table 4). Our main contributions are highlighted below.

Firstly, to maintain 32-bit performance, we propose quantizing the eigenvector matrix of a preconditioner in 4-bit Shampoo, rather than the preconditioner itself. The reason is that the small singular values of the preconditioner matter. Directly quantizing the preconditioner via block-wise quantization [8] at 4-bit precision can significantly alter the small singular values, leading to a drastic change in its inverse 4-th root and thus harming 4-bit Shampoo's performance. Quantizing the eigenvector matrix can help alleviate this issue, which is supported by experimental validation and theoretical insight. Additionally, with the eigenvector matrix, computing the inverse 4-th root is straightforward, ensuring that quantizing the eigenvector matrix does not lead to a rise in the total wall-clock time compared to quantizing the preconditioner (see Figure 1).

Secondly, we present two techniques for enhancing performance. As the eigenvector matrix of a preconditioner is orthogonal, we apply Bjorck orthonormalization [4] to rectify the orthogonality of the quantized eigenvector matrix, leading to improved approximation of preconditioner's eigenvector matrix and facilitating computation of its inverse 4-th root. Additionally, we observe that linear square quantization outperforms dynamic tree quantization [7] marginally when quantizing second-order optimizer states. The superiority of our developed 4-bit Shampoo is demonstrated in Figure 1.

Finally, we evaluate our 4-bit Shampoo on different image classification and natural language modeling tasks using convolutional neural network (CNN) and transformer architectures. Across all these benchmarks, our 4-bit Shampoo achieves similarly fast convergence comparable to its 32-bit counterpart, with no significant increase in losses for the trained models. Our 4-bit Shampoo uses less memory than its 32-bit counterpart, allowing for training of larger models with given resources.

## 2 Preliminaries

In this section, we present Shampoo and its implementation in our experiments. We also discuss quantization-based compression methods in a general formulation.

**Notations.** We use a non-bold letter like \(a\) or \(A\) to denote a scalar, a boldfaced lower-case letter like \(\bm{a}\) to denote a vector, and a boldfaced upper-case letter such as \(\bm{A}\) to denote a matrix. \(\bm{u}\!=\![u_{i}]^{\mathsf{T}}\) means that the \(i\)-th element of column vector \(\bm{u}\) is \(u_{i}\) and \(\bm{U}\!=\![\bm{u}_{i}]\) means the \(i\)-th column vector of matrix \(\bm{U}\) is \(\bm{u}_{i}\). Let \(\bm{A}\) be a positive definite (PD) matrix and \(s\in\mathbb{R}\), we define \(\bm{A}^{s}\!=\!\bm{U}\bm{\Lambda}^{s}\bm{U}^{\mathsf{T}}\), where \(\bm{U}\bm{\Lambda}\bm{U}^{\mathsf{T}}\) is the Singular Value Decomposition (SVD) of \(\bm{A}\). \(\operatorname{tr}(\bm{A})\) represents the trace of a matrix \(\bm{A}\). The inner product of two matrices \(\bm{A}\) and \(\bm{B}\) is denoted as \(\langle\bm{A},\bm{B}\rangle\!=\!\operatorname{tr}(\bm{A}^{\mathsf{T}}\bm{B})\). The Frobenius norm of a matrix \(\bm{A}\) is \(\|\bm{A}\|_{F}\!=\!\sqrt{\langle\bm{A},\bm{A}\rangle}\). \(\bm{A}\odot\bm{B}\) means the elementwise matrix product (Hadamard product).

Figure 1: Visualization of test accuracies and total GPU memory costs of vision transformers. 4-bit Shampoo (naive) quantizes the preconditioner, while 4-bit Shampoo (our) quantizes its eigenvector matrix.

\(\mathrm{Diag}(\bm{a})\) is a diagonal matrix with diagonal vector \(\bm{a}\), while \(\mathrm{diag}(\bm{A})\) means the diagonal vector of matrix \(\bm{A}\).

### Shampoo for Matrices

The update rule of Shampoo in the matrix case combined with a first-order optimizer \(\mathcal{F}\) is

\[\begin{split}\mathbf{Shampoo}(\bm{W}_{t-1},\bm{L}_{t-1},\bm{R}_{ t-1},\bm{s}_{t-1},\bm{G}_{t})=\begin{cases}\bm{L}_{t}\!=\!\bm{L}_{t-1}\!+\!\bm{G}_{t} \bm{G}_{t}^{\mathsf{T}}\\ \bm{R}_{t}\!=\!\bm{R}_{t-1}\!+\!\bm{G}_{t}^{\mathsf{T}}\bm{G}_{t}\\ \bm{\widehat{G}}_{t}\!=\!\bm{L}_{t}^{-1/4}\bm{G}_{t}\bm{R}_{t}^{-1/4}\\ \bm{\widetilde{G}}_{t}\!=\!\bm{\widehat{G}}_{t}(\|\bm{G}_{t}\|_{F}/\|\bm{ \widehat{G}}_{t}\|_{F})\\ \bm{W}_{t},\bm{s}_{t}\!=\!\mathcal{F}(\bm{W}_{t-1},\bm{s}_{t-1},\bm{\widetilde {G}}_{t})\end{cases}\end{split}\] (1)

where \(\bm{W}_{t}\) is the model parameters in matrix form, \(\bm{L}_{t}\) and \(\bm{R}_{t}\) are called preconditioners, \(\bm{s}_{t}\) is the optimizer state of \(\mathcal{F}\), and \(\bm{G}_{t}\) is the gradient at \(\bm{W}_{t-1}\). Note that \(\bm{L}_{t},\bm{R}_{t}\), \(\bm{L}_{t}^{-1/4}\), and \(\bm{R}_{t}^{-1/4}\) are PD matrices. The penultimate step in (1) is the grafting trick [1], which enables Shampoo to roughly apply the well-tuned learning rate schedule of \(\mathcal{F}\). The optimization variable \(\bm{W}_{t}\) does not represent all model parameters. It denotes a tensor of the model [18] or one block of a tensor [2]. In practice, we adopt an efficient and effective implementation of Shampoo for training DNNs following [2; 41] as described in Algorithm 4. In order to achieve efficient training, \(\bm{L}_{t},\bm{R}_{t}\), \(\bm{L}_{t}^{-1/4}\), and \(\bm{R}_{t}^{-1/4}\) are computed once every few hundred iterations. In this case, besides \(\bm{L}_{t}\) and \(\bm{R}_{t}\), their inverse 4-th roots should also be stored in memory, as computing them is computationally expensive. So training large models with Shampoo can be memory-intensive, consuming a significant amount of memory.

### Quantization-based Compression Methods

Quantizing updated optimizer states using a quantizer and then dequantizing them with a dequantizer prior to use is an effective method for conserving memory. We focus exclusively on vectors, as tensors can be reshaped into vectors.

**Quantization.** According to the idea in [8; 26], a \(b\)-bit quantizer \(\mathcal{Q}\) for \(p\)-dimensional real vectors is a mapping given by

\[\mathcal{Q}=(\mathcal{I}\circ\mathcal{N},\mathcal{M}):\mathbb{R}^{p}\to \mathbb{T}_{b}^{p}\times\mathbb{R}^{p},\]

where \(\mathcal{N}\) is a normalization operator on \(\mathbb{R}^{p}\), \(\mathcal{I}\) is an elementwise function mapping any real number to an element of \(\mathbb{T}_{b}\!=\!\{0,1,\ldots,2^{b}-1\}\), and \(\mathcal{M}\) is a maximum operator on \(\mathbb{R}^{p}\). For any \(\bm{x}\in\mathbb{R}^{p}\), \(\mathcal{N}\) and \(\mathcal{M}\) satisfy \(\mathcal{N}(\bm{x})\odot\mathcal{M}(\bm{x})\!=\!\bm{x}\).

A normalization operator \(\mathcal{N}\) for \(p\)-dimensional vectors is a transformation on \(\mathbb{R}^{p}\). It scales each element of a vector \(\bm{x}\in\mathbb{R}^{p}\) into \([-1,1]\). A block-wise normalization operator for a \(p\)-dimensional vector \(\bm{x}=[x_{1},x_{2},\ldots,x_{p}]^{\mathsf{T}}\) is defined as

\[\mathcal{N}(\bm{x})_{i}=\frac{x_{i}}{\max_{j\in\mathbb{X}_{i}}\{x_{j}\}},\]

where \(\mathcal{N}(\bm{x})_{i}\) is the \(i\)-th element of \(\mathcal{N}(\bm{x})\), and \(\mathbb{X}_{i}\) is a set satisfying \(i\in\mathbb{X}_{i}\subset\{1,\ldots,p\}\). Usually, \(\mathbb{X}_{i}\) should also satisfy \(\mathbb{X}_{i}\!=\!\mathbb{X}_{j}\) or \(\mathbb{X}_{i}\!\cap\mathbb{X}_{j}\!=\!\emptyset\) for \(i,j\in\{1,\ldots,p\}\). In this case, for any \(\bm{x}\in\mathbb{R}^{p}\), the number of different elements in \(\mathcal{M}(\bm{x})\) is equal to the number of elements in set \(\{\mathbb{X}_{i}|i=1,\ldots,p\}\). Meanwhile, the number of the elements in \(\mathbb{X}_{i}\) for any \(i\) should be as close as possible to a value called block size.

The mapping \(\mathcal{I}\) for \(x\in\mathbb{R}\) in a \(b\)-bit quantizer \(\mathcal{Q}\) is defined as

\[\mathcal{I}(x)=\underset{j\in\mathbb{T}_{b}}{\text{argmin}}\left|x-\mathcal{R}( j)\right|,\]

where \(\mathcal{R}\) named quantization mapping is an elementwise function that maps any element in \(\mathbb{T}_{b}\) into \([-1,1]\), and \(|\cdot|\) is the absolute operator for a scalar. There are three typical quantization mappings: linear quantization, dynamic quantization, and quantile quantization. Their specifications and visualizations can be found in [8].

**Dequantization.** Given a \(b\)-bit quantizer \(\mathcal{Q}\!=\!(\mathcal{I}\circ\mathcal{N},\mathcal{M})\) for a \(p\)-dimensional real vector \(\bm{x}\in\mathbb{R}^{p}\), the corresponding dequantizer \(\mathcal{D}\) is a mapping defined as

\[\mathcal{D}(\mathcal{Q}(\bm{x}))\!=\!\mathcal{D}(\mathcal{I}\circ\mathcal{N}( \bm{x}),\mathcal{M}(\bm{x}))\!=\!\mathcal{R}(\mathcal{I}\circ\mathcal{N}(\bm{x }))\odot\mathcal{M}(\bm{x}):\mathbb{T}_{b}^{p}\times\mathbb{R}^{p}\to\mathbb{R} ^{p}.\]Methodology

In this section, we describe the design of our quantization-based compression method to realize 4-bit Shampoo with fast and high precision quantization. Let \(\mathcal{Q}\!=\!(\mathcal{I}\circ\mathcal{N},\mathcal{M})\) be a quantizer and \(\mathcal{D}\) be its corresponding dequantizer as described in Subsection 2.2.

### Quantizing the Eigenvector Matrices

A naive approach to realize 4-bit Shampoo is applying the compression methods proposed in [8; 26] to \(\bm{L}_{t}\), \(\bm{R}_{t}\), \(\bm{L}_{t}^{-1/4}\), and \(\bm{R}_{t}^{-1/4}\) in Shampoo (see (1)). A slightly improved approach is to quantize the four PD matrices excluding their diagonal elements, which are typically much larger than their non-diagonal counterparts due to the non-negativity of the elements in \(\mathrm{diag}(\bm{G}_{t}\bm{G}_{t}^{\mathsf{T}})\) and \(\mathrm{diag}(\bm{G}_{t}^{\mathsf{T}}\bm{G}_{t})\).

However, the naive approach can cause large quantization errors at 4-bit precision. This is because the quantization errors (or called perturbations) of quantizing \(\bm{L}_{t}\) and \(\bm{R}_{t}\) will transfer to \(\bm{L}_{t}^{-1/4}\) and \(\bm{R}_{t}^{-1/4}\). To verify this, we first introduce two criteria to evaluate the quantization errors of matrices. We do not use the elementwise criterion in [8]. Let \(\bm{A}\) denote a 32-bit matrix, \(g\) represent a transformation (can formed by quantization), and \(f\) stand for a mapping, e.g., \(f(\bm{A})\!=\!\bm{A}^{-1/4}\). Then we define the normwise relative error (NRE) and angle error (AE) in \(f\) of \(g\) at \(\bm{A}\) as

\[\text{NRE}\!=\!\frac{\|f(\bm{A})-f(g(\bm{A}))\|_{F}}{\|f(\bm{A})\|_{F}},\quad \text{AE}\!=\!\arccos\left(\frac{\langle f(\bm{A}),f(g(\bm{A}))\rangle}{(\|f( \bm{A})\|_{F}\|f(g(\bm{A}))\|_{F}}\right).\]

We choose two PD matrices of order 1200. The first one \(\bm{A}_{1}\) is derived from the real world. It is a preconditioner in 32-bit Shampoo combined with AdamW for training a Swin-Tiny model. The second one \(\bm{A}_{2}\!=\!\bm{U}\bm{\Lambda}\bm{U}^{\mathsf{T}}\) is synthetic, constructed from a random orthogonal matrix \(\bm{U}\) and a diagonal matrix \(\bm{\Lambda}\) with only two distinct diagonal values. Table 1 shows the quantization errors in \(f(\bm{A})\!=\!\bm{A}^{-1/4}\) of the naive approach at these two matrices, which are remarkably high. More analyses are given in Appendix D. The key point is that the singular values of \(\bm{A}_{i}(i\!=\!1,2)\) follow a specific distribution (see Figure 2). In this scenario, a slight perturbation of \(\bm{A}_{i}\) will significantly alter its small singular values, resulting in a drastic change to \(\bm{A}_{i}^{-1/4}\).

To address this issue, we propose quantizing the eigenvector matrix of a preconditioner in Shampoo, rather than the preconditioner itself. Namely, a preconditioner \(\bm{A}\) is a PD matrix, and its SVD is \(\bm{U}\bm{\Lambda}\bm{U}^{\mathsf{T}}\), where \(\bm{U}\) represents the eigenvector matrix and \(\bm{\Lambda}\) denotes the singular value matrix. Given that \(\bm{\Lambda}\) is a diagonal matrix, we can focus on quantizing \(\bm{U}\) using \(\mathcal{Q}\) while leaving \(\bm{\Lambda}\) unchanged. From Table 1, one can observe that quantizing \(\bm{U}\) can significantly reduce the quantization errors. We will theoretically discuss the advantages of quantizing \(\bm{U}\) compared to quantizing \(\bm{A}\) in Section 4. In practice, the randomized SVD method [19] is adopted to compute the SVD of \(\bm{A}\) efficiently, as shown in [40]. We want to highlight that quantizing the original \(\bm{L}_{t}\) and \(\bm{R}_{t}\) in Shampoo involves significant computational burdens to compute their inverse 4-th roots \(\bm{L}_{t}^{-1/4}\) and \(\bm{R}_{t}^{-1/4}\), whereas quantizing the eigenvector matrices of \(\bm{L}_{t}\) and \(\bm{R}_{t}\) allows for rapid inverse root calculation. So the computational time required for both approaches is comparable (see Figure 1).

\begin{table}
\begin{tabular}{c|c c c c c||c c c c c} \hline \hline \multicolumn{8}{c}{Real-world \(\bm{A}\!=\!\bm{A}_{1}\)} & \multicolumn{4}{c}{Synthetic \(\bm{A}\!=\!\bm{A}_{2}\)} \\ \hline Mapping \(\mathcal{R}\) & Bit & QM & OR & NRE \(\downarrow\) & AE (\({}^{\circ}\!\)) \(\downarrow\) & Mapping \(\mathcal{R}\) & Bit & QM & OR & NRE \(\downarrow\) & AE (\({}^{\circ}\!\)) \(\downarrow\) \\ \hline \multirow{4}{*}{DT} & 8 & \(\bm{A}\) & ✗ & 0.2192 & 8.3014 & \multirow{4}{*}{DT} & 8 & \(\bm{A}\) & ✗ & 0.1896 & 10.877 \\  & 4 & \(\bm{A}\) & ✗ & 0.6241 & 17.319 & \multirow{4}{*}{DT} & 4 & \(\bm{A}\) & ✗ & 0.4615 & 17.189 \\  & 4 & \(\bm{U}\) & ✗ & 0.0709 & 4.0426 & & 4 & \(\bm{U}\) & ✗ & 0.1224 & 7.0144 \\  & 4 & \(\bm{U}\) & ✓ & 0.0455 & 2.5615 & & 4 & \(\bm{U}\) & ✓ & 0.0878 & 4.9960 \\ \hline \multirow{4}{*}{Linear-2} & 8 & \(\bm{A}\) & ✗ & 0.2164 & 7.9751 & \multirow{4}{*}{Linear-2} & 8 & \(\bm{A}\) & ✗ & 0.1310 & 7.4717 \\  & 4 & \(\bm{A}\) & ✗ & 0.6243 & 17.293 & \multirow{4}{*}{Linear-2} & 4 & \(\bm{A}\) & ✗ & 0.4465 & 15.338 \\  & 4 & \(\bm{U}\) & ✗ & 0.0543 & 3.1066 & & 4 & \(\bm{U}\) & ✗ & 0.0942 & 5.3998 \\  & 4 & \(\bm{U}\) & ✓ & 0.0343 & 1.9456 & & & 4 & \(\bm{U}\) & ✓ & 0.0669 & 3.8166 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Quantization errors in \(\bm{A}^{-1/4}\) of different quantization schemes at a PD matrix \(\bm{A}\). We employ block-wise normalization with a block size of 64. \(\bm{U}\) is the eigenvector matrix of \(\bm{A}\), QM = quantized matrix, and OR = orthogonal rectification.

### Rectifying the Orthogonality of Eigenvector Matrices

Let \(\bm{A}\) be a PD matrix with SVD \(\bm{U}\bm{\Lambda}\bm{U}^{\mathsf{T}}\). Note that the eigenvector matrix \(\bm{U}\) is orthogonal, whereas \(\bm{V}\!=\!\mathcal{D}(\mathcal{Q}(\bm{U}))\) may not be. To further mitigate the quantization errors mentioned in Subsection 3.1, we propose employing Bjorck orthonormalization [4] to orthogonalize \(\bm{V}\). Particularly, given \(\bm{V}_{0}\!=\!\bm{V}\), we iterate

\[\bm{V}_{t}\!=\!1.5\bm{V}_{t-1}\!-\!0.5\bm{V}_{t-1}\bm{V}_{t-1}^{ \mathsf{T}}\bm{V}_{t-1},\] (2)

for \(t_{1}\!\geq\!1\) times and take \(\bm{V}_{t_{1}}\) as the rectified result. Equation (2) can also be interpreted as the gradient descent of problem \(\min_{\bm{V}}\|\bm{V}^{\mathsf{T}}\bm{V}-\bm{I}\|_{F}^{2}\) using a step size of 0.5, where \(\bm{I}\) denotes the identity matrix. We empirically find that only one iteration (i.e., \(t_{1}\!=\!1\)) is enough. Table 1 illustrates the benefit of rectifying \(\bm{V}\) into \(\bm{V}_{1}\).

The update frequencies for the preconditioners and their inverse 4-th roots differ (see Algorithm 3). Given \(\bm{V}\) and \(\bm{\Lambda}\), we also require orthogonal rectification to compute \(\bm{A}^{s}\) rapidly for any \(s\in\mathbb{R}\). The reason is as follows. It is easy to compute \(\bm{A}^{s}\!=\!\bm{U}\bm{\Lambda}^{s}\bm{U}^{\mathsf{T}}\) by definition. However, \(\bm{U}\bm{\Lambda}^{s}\bm{U}^{\mathsf{T}}\) can be very sensitive to the orthogonality of \(\bm{U}\) for \(s<0\), making \(\bm{V}\bm{\Lambda}^{s}\bm{V}^{\mathsf{T}}\) largely deviate from \((\bm{V}\bm{\Lambda}\bm{V}^{\mathsf{T}})^{s}\approx\bm{A}^{s}\). Similarly, we can approximate \(\bm{A}^{s}\) by \(\bm{V}_{t_{2}}\bm{\Lambda}^{s}\bm{V}_{t_{2}}^{\mathsf{T}}\), where \(\bm{V}_{t_{2}}\) is generated by (2). Figure 3 illustrates the elementwise mean errors between \((\bm{V}_{t_{2}}\bm{\Lambda}^{s}\bm{V}_{t_{2}}^{\mathsf{T}})^{-1/s}(\bm{V}_{t_{ 2}}\bm{\Lambda}\bm{V}_{t_{2}}^{\mathsf{T}})\) and \(\bm{I}\) for various \(s\) and \(t_{2}\), where \(\bm{A}\) is the real-world matrix used in Table 1. Based on the observation from Figure 3, we set \(t_{2}\!=\!4\) in our experiments.

### Selecting the Quantizer

The quantizer \(\mathcal{Q}\) is defined by the normalization operator \(\mathcal{N}\) and mapping \(\mathcal{R}\), and \(\mathcal{N}\) is determined by \(\mathbb{X}_{i}\). Since an eigenvector has a unit length, the elements in \(\mathbb{X}_{i}\) should belong to the same column of an eigenvector matrix, i.e., they are from the same eigenvector. Instead of employing dynamic tree (DT) quantization as mapping \(\mathcal{R}\), we recommend utilizing linear square (Linear-2) quantization as \(\mathcal{R}\), particularly when \(b\!=\!4\). Linear-2 quantization is defined as

\[\mathcal{R}(j)=\begin{cases}-\left(-1+2j/(2^{b}\!-\!1)\right)^{2},&j\!<\!2^{b \!-\!1}\!-\!1;\\ 0,&j\!=\!2^{b\!-\!1}\!-\!1;\\ \left(-1\!+\!2j/(2^{b}\!-\!1)\right)^{2},&j\!>\!2^{b\!-\!1}\!-\!1,\end{cases}\] (3)

where \(j\!\in\!\mathbb{T}_{b}\!=\!\{0,1,\ldots,2^{b}\!-\!1\}\). As shown in Table 1, Linear-2 quantization has lower quantization errors compared to DT quantization at 4-bit precision.

### Overall Algorithm

We first describe the update processes of the preconditioners and their inverse 4-th roots in our 4-bit Shampoo. A preconditioner \(\bm{A}\) is a PD matrix and its SVD is \(\bm{U}\bm{\Lambda}\bm{U}^{\mathsf{T}}\). We can compress \(\bm{A}\) into a pair \((\bm{\lambda},\overline{\bm{U}})=(\operatorname{diag}(\bm{\Lambda}),\mathcal{ Q}(\bm{U}))\) and decompress it into \((\bm{\Lambda},\bm{V})=(\operatorname{Diag}(\bm{\lambda}),\mathcal{D}(\overline{ \bm{U}}))\). Algorithm 1 (Preconditioner Update, PU) shows the update rule of \(\bm{A}\). Similarly, we compress \(\widehat{\bm{A}}\approx\bm{A}^{-1/4}\) into a pair \((\bm{a},\overline{\bm{A}})=(\operatorname{diag}(\widehat{\bm{A}}),\mathcal{Q} (\widehat{\bm{A}}\!-\!\operatorname{Diag}(\bm{a})))\) and decompress it into \(\operatorname{Diag}(\bm{a})+\mathcal{D}(\overline{\bm{A}})\). Algorithm 2(Preconditioner's Inverse 4-th Root Update, PIRU) gives the update rule of \(\widehat{\bm{A}}\). Based on the above update rules, we can summarize our 4-bit Shampoo in Algorithm 3. Note that we omit some input parameters of \(\mathrm{PU}\) and \(\mathrm{PIRU}\) because they can be found in Algorithm 3 in the same form.

```
0: singular value vector \(\bm{\lambda}\), quantized eigenvector matrix \(\overline{\bm{U}}\), \(\bm{M}\), number of iterations \(t_{1}\) for rectification, exponential decay rate \(\beta\in(0,1)\), \(\mathcal{Q}\) and \(\mathcal{D}\)
1:\(\bm{\Lambda}=\mathrm{Diag}(\bm{\lambda}),\bm{V}=\mathcal{D}(\overline{\bm{U}})\)
2: Rectify \(\bm{V}\) by iterating (2) \(t_{1}\) times
3:\(\bm{A}=\beta\bm{V}\bm{\Lambda}\bm{V}^{\mathsf{T}}+(1-\beta)\bm{M}\)
4: Compute \(\bm{A}=\bm{P}\bm{\Sigma}\bm{P}^{\mathsf{T}}\) by randomized SVD
5:return\(\mathrm{diag}(\bm{\Sigma}),\mathcal{Q}(\bm{P})\) ```

**Algorithm 1**\(\mathrm{PU}(\bm{\lambda},\overline{\bm{U}},\bm{M})\)

```
0:\(\bm{W}_{0}\in\mathbb{R}^{m\times n}\), \(\bm{L}_{0}=\epsilon\bm{I}_{m}\), \(\bm{R}_{0}=\epsilon\bm{I}_{n}\), \(\widehat{\bm{L}}_{0}=\bm{I}_{m}\), \(\widehat{\bm{R}}_{0}=\bm{I}_{n}\), \(\beta\in(0,1)\), \(t_{1}\), \(t_{2}\), update interval \(T_{1}\), update interval \(T_{2}\), total number of steps \(T\), first-order optimizer \(\mathcal{F}\), first-order optimizer state \(\bm{s}_{0}=\bm{0}\), 4-bit quantizer \(\mathcal{Q}\) and its corresponding dequantizer \(\mathcal{D}\).
0: final parameter \(\bm{W}_{T}\).
1:\(\bm{\lambda}_{0,L}=\mathrm{diag}(\bm{L}_{0}),\overline{\bm{U}}_{0,L}=\mathcal{ Q}(\bm{I}_{m})\); \(\bm{\lambda}_{0,R}=\mathrm{diag}(\bm{R}_{0}),\overline{\bm{U}}_{0,R}=\mathcal{Q} (\bm{I}_{n})\)
2:\(\bm{I}_{0}=\mathrm{diag}(\widehat{\bm{L}}_{0}),\overline{\bm{L}}_{0}=\mathcal{ Q}(\bm{0})\); \(\bm{r}_{0}=\mathrm{diag}(\widehat{\bm{R}}_{0}),\overline{\bm{R}}_{0}=\mathcal{Q} (\bm{0})\)
3:for\(t=1,2,\ldots,T\)do
4: Receive loss function \(\mathcal{L}_{t}:\mathbb{R}^{m\times n}\mapsto\mathbb{R}\) and compute gradient \(\bm{G}_{t}=\nabla\mathcal{L}_{t}(\bm{W}_{t})\)
5:if\(t\%T_{1}\equiv 0\)then
6:\(\bm{\lambda}_{t,L},\overline{\bm{U}}_{t,L}\!=\!\mathrm{PU}(\bm{\lambda}_{t-1,L },\overline{\bm{U}}_{t-1,L},\bm{G}_{t}\bm{G}_{t}^{\mathsf{T}})\); \(\bm{\lambda}_{t,R},\overline{\bm{U}}_{t,R}\!=\!\mathrm{PU}(\bm{\lambda}_{t-1,R },\overline{\bm{U}}_{t-1,R},\bm{G}_{t}^{\mathsf{T}}\bm{G}_{t})\)
7:else
8:\(\bm{\lambda}_{t,L},\overline{\bm{U}}_{t,L}=\bm{\lambda}_{t-1,L},\overline{\bm {U}}_{t-1,L}\); \(\bm{\lambda}_{t,R},\overline{\bm{U}}_{t,R}=\bm{\lambda}_{t-1,R},\overline{\bm {U}}_{t-1,R}\)
9:if\(t\%T_{2}\equiv 0\)then
10:\(\bm{l}_{t},\overline{\bm{L}}_{t}=\mathrm{PIRU}(\bm{\lambda}_{t,L},\overline{ \bm{U}}_{t,L})\); \(\bm{r}_{t},\overline{\bm{R}}_{t}=\mathrm{PIRU}(\bm{\lambda}_{t,R},\overline{ \bm{U}}_{t,R})\)
11:else
12:\(\bm{l}_{t},\overline{\bm{L}}_{t}=\bm{l}_{t-1},\overline{\bm{L}}_{t-1}\); \(\bm{r}_{t},\overline{\bm{R}}_{t}=\bm{r}_{t-1}\), \(\overline{\bm{R}}_{t-1}\)
13:\(\widehat{\bm{L}}_{t}=\mathrm{Diag}(\bm{l}_{t})+\mathcal{D}(\overline{\bm{L}}_{t})\); \(\widehat{\bm{R}}_{t}=\mathrm{Diag}(\bm{r}_{t})+\mathcal{D}(\overline{\bm{R}}_{t})\)
14:\(\widehat{\bm{G}}_{t}=\widehat{\bm{L}}_{t}\bm{G}_{t}\widehat{\bm{R}}_{t};\quad \widehat{\bm{G}}_{t}=\widehat{\bm{G}}_{t}(\|\bm{G}_{t}\|_{F}/\|\widehat{\bm{G} }_{t}\|_{F})\)
15:\(\bm{W}_{t},\bm{s}_{t}=\mathcal{F}(\bm{W}_{t-1},\bm{s}_{t-1},\widehat{\bm{G}}_{t})\) ```

**Algorithm 2**\(\mathrm{PIRU}(\bm{\lambda},\overline{\bm{U}})\)

## 4 Theoretical Analysis

In this section, we analyze why quantizing the eigenvector matrix of a preconditioner in Shampoo is better than quantizing the preconditioner itself under a certain singular value distribution. Furthermore, we consider quantization as a perturbation and prove the convergence of the perturbed Shampoo (Algorithm 6) in Appendix E. The following lemma reveals some good properties of perturbing the eigenvector matrix of a PD matrix.

**Lemma 1**.: _Let \(\bm{A}\) be a PD matrix whose SVD is \(\bm{U}\bm{\Lambda}\bm{\Lambda}\bm{U}^{\mathsf{T}}\), where \(\bm{U}\!=\![\bm{u}_{i}]\) is an orthogonal matrix and \(\bm{\Lambda}\!=\!\mathrm{diag}([\lambda_{i}]^{\mathsf{T}})\) is a diagonal matrix. Given a perturbation \(\Delta\bm{U}\!=\![\Delta\bm{u}_{i}]\) and \(s\in\mathbb{R}\), we define \(\bm{B}\!:=\!(\bm{U}\bm{\Lambda}\bm{U}^{\mathsf{T}})^{s}\) and \(\Delta\bm{B}\!:=\!((\bm{U}\!+\!\Delta\bm{U})\bm{\Lambda}(\bm{U}\!+\!\Delta\bm{U})^{ \mathsf{T}})^{s}\!-\!\bm{B}\)._

1. _If_ \(\bm{U}\!+\!\Delta\bm{U}\) _is orthogonal and there exists_ \(\alpha\in\mathbb{R}\) _such that_ \(\|\Delta\bm{u}_{i}\|_{2}\leq\alpha\)_, then_ \[\frac{\|\Delta\bm{B}\|_{F}}{\|\bm{B}\|_{F}}\leq 2\alpha.\]
2. _If_ \(\bm{U}\!+\!\Delta\bm{U}\) _is orthogonal and there exists_ \(\beta\in\mathbb{R}\) _such that_ \(\langle\bm{u}_{i},\bm{u}_{i}\!+\!\Delta\bm{u}_{i}\rangle\geq 1-\beta\geq 0\)_, then_ \[\frac{\langle\bm{B},\bm{B}\!+\!\Delta\bm{B}\rangle}{\|\bm{B}\|_{F}\|\bm{B}\!+\! \Delta\bm{B}\|_{F}}\geq(1\!-\!\beta)^{2}.\]From Lemma 1, it is evident that the normwise relative error and angle error in \(f(\bm{A})=\bm{A}^{s}\) of perturbing \(\bm{U}\) at \(\bm{A}=\bm{U}\bm{\Lambda}\bm{U}^{\mathsf{T}}\) are independent of \(\bm{\Lambda}\) and \(s\). Moreover, these errors are well-bounded under some mild conditions. Empirically, for \(4\)-bit quantization, \(\alpha=0.1\) and \(\beta=0.005\) roughly meet the conditions of Lemma 1, leading to \(\frac{\|\bm{\Delta}\bm{B}\|_{F}}{\|\bm{B}\|_{F}}\leq 0.2\) and \(\frac{(\bm{B},\bm{B}+\bm{\Delta}\bm{B})}{\|\bm{B}\|_{F}\|\bm{B}+\bm{\Delta}\bm {B}\|_{F}}\geq 0.99\).

It is very complicated to generally analyze the perturbation in \(f(\bm{A})=\bm{A}^{s}\) of perturbing \(\bm{A}\). Thus, we focus on perturbing the singular values of \(\bm{A}\). For simplicity, we assume that both \(\bm{A}\) and \(\bm{A}+\Delta\bm{A}\) have only two distinct singular values, where \(\Delta\bm{A}\) is a perturbation of \(\bm{A}\). The following lemma gives the perturbation in \(\bm{A}^{s}\) of perturbing the smaller singular value of \(\bm{A}\).

**Lemma 2**.: _Let \(\bm{A}\) be a PD matrix of order \(m+n\) whose SVD is \(\bm{U}\bm{\Lambda}\bm{U}^{\mathsf{T}}\), where \(m,n\in\mathbb{N}_{+}\), \(n=lm\), \(\bm{U}=[\bm{u}_{i}]\) is an orthogonal matrix and \(\bm{\Lambda}=\operatorname{diag}([\lambda_{i}]^{\mathsf{T}})\) is a diagonal matrix. Assume that \(\bm{\Lambda}=\operatorname{diag}([c\lambda\bm{1}_{m\times 1}^{\mathsf{T}}, \lambda\bm{1}_{n\times 1}^{\mathsf{T}}]^{\mathsf{T}})\), \(c\geq 1\), and \(\lambda>0\). Given a perturbation \(\Delta\bm{\Lambda}=\operatorname{diag}([\bm{0}_{m\times 1}^{\mathsf{T}}, \Delta\bm{\lambda}_{n\times 1}^{\mathsf{T}}]^{\mathsf{T}})\) and \(s\in\mathbb{R}\), we define \(\bm{B}\!:=\!(\bm{U}\bm{\Lambda}\bm{U}^{\mathsf{T}})^{s}\) and \(\Delta\bm{B}\!:=\!(\bm{U}(\bm{\Lambda}\!+\!\Delta\bm{\Lambda})\bm{U}^{\mathsf{ T}})^{s}\!-\!\bm{B}\)._

1. _If_ \(\Delta\bm{\lambda}_{n\times 1}=(k-1)\lambda\bm{1}_{n\times 1}\) _where_ \(k>0\)_, then_ \[\frac{\|\Delta\bm{B}\|_{F}}{\|\bm{B}\|_{F}}=\frac{\sqrt{l}|k^{s}-1|}{\sqrt{c^ {2s}}+l}=h_{1}(s,l).\] _Moreover,_ \(h_{1}(s,l)\) _decreases monotonically with_ \(s\) _over_ \((-\infty,0)\) _and increases monotonically with_ \(l\) _over_ \((0,+\infty)\)_._
2. _If_ \(\Delta\bm{\lambda}_{n\times 1}=(tc-1)\lambda\bm{1}_{n\times 1}\) _where_ \(t>0\)_, then_ \[\frac{\langle\bm{B},\bm{B}+\Delta\bm{B}\rangle}{\|\bm{B}\|_{F}\|\bm{B}+\Delta \bm{B}\|_{F}}=\frac{lt^{s}+c^{s}}{\sqrt{(1+lt^{2s})(l+c^{2s})}}=h_{2}(l).\] _Moreover,_ \(h_{2}(l)\) _decreases monotonically with_ \(l\) _over_ \((0,(c/t)^{s},+\infty)\)_._
3. _If_ \(\Delta\bm{\lambda}_{n\times 1}=(tc-1)\lambda\bm{1}_{n\times 1}\) _where_ \(k=tc>0\) _and_ \(l=(c/t)^{s}\)_, then_ \[\frac{\|\Delta\bm{B}\|_{F}}{\|\bm{B}\|_{F}}=\frac{|k^{s}-1|}{\sqrt{k^{s}}+1}, \quad\frac{\langle\bm{B},\bm{B}+\Delta\bm{B}\rangle}{\|\bm{B}\|_{F}\|\bm{B}+ \Delta\bm{B}\|_{F}}=\frac{2}{\sqrt{2+k^{s}+1/k^{s}}}.\]

Let us make some comments on the above lemma. First, from Lemma 2(1) we have \(h_{1}(1,l)=\frac{\|\Delta\bm{A}\|_{F}}{\|\bm{A}\|_{F}}=\frac{\sqrt{l}|k-1|}{\sqrt {c^{2}}+l}\). If \(k\geq 1\), \(\frac{\|\Delta\bm{A}\|_{F}}{\|\bm{A}\|_{F}}=\frac{\|\Delta\bm{A}\|_{F}}{\|\bm{A }\|_{F}}\) is bounded by \(\frac{k}{c}\sqrt{l}=t\sqrt{l}\). Second, if \(k=tc\geq 1\) and \(s<0\), one can deduce \(h_{2}(l)\geq\sqrt{lt^{2s}/(1+lt^{2s})}\) from Lemma 2(2), which indicates that a small \(lt^{2s}\) is needed to achieve small \(h_{2}(l)\). We can set \(t=0.02\) to simulate 4-bit quantization. Based on Lemma 1 and Lemma 2(3), we have the following proposition.

**Proposition 1**.: _Let \(\bm{A}\) be a PD matrix of order \(m+n\) whose SVD is \(\bm{U}\bm{\Lambda}\bm{U}^{\mathsf{T}}\), where \(m,n\in\mathbb{N}_{+}\), \(n=lm\), \(\bm{U}=[\bm{u}_{i}]\) is an orthogonal matrix, \(\bm{\Lambda}=\operatorname{diag}([c\lambda\bm{1}_{m\times 1}^{\mathsf{T}}, \lambda\bm{1}_{n\times 1}^{\mathsf{T}}]^{\mathsf{T}})\), \(c\geq 1000\), and \(\lambda>0\). Given \(\Delta\bm{U}=[\Delta\bm{u}_{i}]\), \(\Delta\bm{\Lambda}=\operatorname{diag}([\bm{0}_{m\times 1}^{\mathsf{T}}, \Delta\bm{\lambda}_{n\times 1}^{\mathsf{T}}]^{\mathsf{T}})\), and \(s\leq-0.25\), we define \(\bm{B}\!:=\!(\bm{U}\bm{\Lambda}\bm{U}^{\mathsf{T}})^{s}\), \(\bm{B}_{1}\!:=\!(\bm{(}\bm{U}+\Delta\bm{U})\bm{\Lambda}(\bm{U}+\Delta\bm{ \Lambda})\bm{U}^{\mathsf{T}})^{s}\), and \(\bm{B}_{2}\!:=\!(\bm{U}(\bm{\Lambda}\!+\!\Delta\bm{\Lambda})\bm{U}^{\mathsf{T}})^{s}\). If \(\bm{U}+\Delta\bm{U}\) is orthogonal, \(\|\Delta\bm{u}_{i}\|_{2}\!\geq\!0.1,\langle\bm{u}_{i},\Delta\bm{u}_{i}\rangle\! \geq\!-0.005\), \(\Delta\bm{\lambda}_{n\times 1}\!=\!(0.02c\!-\!1)\lambda\bm{1}_{n\times 1}\), and \(l\!=\!(c/0.02)^{s}\), then_

\[2\frac{\|\bm{B}_{1}-\bm{B}\|_{F}}{\|\bm{B}\|_{F}}\!\leq\!0.4\!\leq\!\frac{\|\bm{B }_{2}-\bm{B}\|_{F}}{\|\bm{B}\|_{F}},\quad 6\left(1-\frac{\langle\bm{B},\bm{B}_{1} \rangle}{\|\bm{B}\|_{F}\|\bm{B}_{1}\|_{F}}\right)\!\leq\!0.06\!\leq\!\left(1 \!1-\frac{\langle\bm{B},\bm{B}_{2}\rangle}{\|\bm{B}\|_{F}\|\bm{B}_{2}\|_{F}} \right).\]

Proposition 1 requires very strong assumptions. Nevertheless, it provides insight into why quantizing \(\bm{A}\) can result in a greater normwise relative error and angle error in \(\bm{A}^{s}\), compared to quantizing \(\bm{U}\). Complete proofs of Lemma 1, Lemma 2, and Proposition 1 can be found in Appendix F.

## 5 Experiments

In this section, we compare our 4-bit Shampoo combined with SGDM or AdamW to their 32-bit counterparts, as well as the first-order optimizers on various image classification tasks. See more experimental results on image classification and natural language modeling tasks in Appendix H.

**Models, datasets, and hyperparameters.** We train VGG19 [36], ResNet34 [20], ViT-Small [10], and Swin-Tiny [28] on the CIFAR-100 [23] and Tiny-ImageNet [24] datasets with one RTX3060Ti GPU, and train ResNet50 and ViT-Base/32 on the ImageNet-1k dataset [34] with one A800 GPU.

For hyperparameter settings, we mainly follow [41] to train CNNs and [25; 44] to train vision transformers. For all the tasks, we keep the common hyperparameters of optimizers the same values. See Appendix G for experimental details.

**Main results.** We show the performance, wall-clock time, and memory cost in Table 2. First-order optimizers run 1.2x to 1.5x epochs, resulting in longer wall-clock time, yet yielding lower test accuracies compared to second-order optimizers. In comparison to 32-bit Shampoo, our 4-bit Shampoo shows comparable test accuracies with differences ranging from -0.7% to 0.5%, increases in wall-clock time varying from -0.2% to 9.5%, and memory savings of 4.5% to 41%. Compared to the first-order optimizers, the memory costs of our 4-bit Shampoo only rise by 0.8% to 12.7%. This represents a significant advancement in the utilization of second-order optimizers. Following [26], we report the total peak GPU memory consumption rather than the optimizer's peak GPU memory consumption. Our main focus is on quantizing the states for constructing preconditioners and their inverse roots, which are approximately 7x smaller for 4-bit Shampoo compared to 32-bit Shampoo (see Appendix G). Figure 4 shows the test accuracy curves on the CIFAR-100 and ImageNet-1k

\begin{table}
\begin{tabular}{c|c|c c c c} \hline \hline Dataset & Model & Optimizer & TA (\%) & WCT (min) & TMC (MB) \\ \hline \multirow{6}{*}{CIFAR-100} & \multirow{3}{*}{VGG19} & SGDM & 74.14 & 97.70 & 512.17 \\  & & SGDM + 32-bit Shampoo & 74.54 & 84.45 & 979.13 \\  & & SGDM + 4-bit Shampoo & 74.74 & 92.51 & 577.14 \\ \cline{2-6}  & \multirow{3}{*}{ResNet34} & SGDM & 78.98 & 170.1 & 822.03 \\  & & SGDM + 32-bit Shampoo & 79.71 & 147.2 & 1441.8 \\  & & SGDM + 4-bit Shampoo & 79.17 & 155.8 & 908.40 \\ \cline{2-6}  & \multirow{3}{*}{ViT-Small} & AdamW & 74.34 & 668.1 & 2720.0 \\  & & AdamW + 32-bit Shampoo & 77.50 & 498.7 & 3252.0 \\  & & AdamW + 4-bit Shampoo & 77.22 & 510.8 & 2791.7 \\ \cline{2-6}  & \multirow{3}{*}{Swin-Tiny} & AdamW & 76.69 & 318.6 & 1465.8 \\  & & AdamW + 32-bit Shampoo & 79.34 & 260.8 & 2036.0 \\  & & AdamW + 4-bit Shampoo & 78.63 & 273.3 & 1543.9 \\ \hline \multirow{6}{*}{Tiny-ImageNet} & \multirow{3}{*}{VGG19} & SGDM & 61.53 & 172.0 & 1062.3 \\  & & SGDM + 32-bit Shampoo & 63.39 & 136.5 & 1531.9 \\  & & SGDM + 4-bit Shampoo & 62.84 & 143.8 & 1127.3 \\ \cline{2-6}  & \multirow{3}{*}{ResNet34} & SGDM & 67.10 & 432.1 & 2304.0 \\  & & SGDM + 32-bit Shampoo & 67.90 & 313.0 & 2924.3 \\  & & SGDM + 4-bit Shampoo & 67.95 & 329.3 & 2390.4 \\ \cline{2-6}  & \multirow{3}{*}{ViT-Small} & AdamW & 54.66 & 1274 & 2730.1 \\  & & AdamW + 32-bit Shampoo & 57.11 & 953.9 & 3261.1 \\  & & AdamW + 4-bit Shampoo & 57.15 & 970.3 & 2801.9 \\ \cline{2-6}  & \multirow{3}{*}{Swin-Tiny} & AdamW & 58.77 & 701.9 & 1789.9 \\  & & AdamW + 32-bit Shampoo & 61.74 & 565.3 & 2362.8 \\  & & AdamW + 4-bit Shampoo & 62.24 & 582.7 & 1868.1 \\ \hline \multirow{6}{*}{ImageNet-1k} & \multirow{3}{*}{ResNet50} & SGDM & 76.70 & 2134 & 11307 \\  & & SGDM + 32-bit Shampoo & 77.07 & 1910 & 11937 \\ \cline{1-1}  & & SGDM + 4-bit Shampoo & 76.92 & 1970 & 11396 \\ \cline{1-1} \cline{2-6}  & \multirow{3}{*}{ViT-Base/32} & AdamW & 72.87 & 2190 & 10600 \\ \cline{1-1}  & & AdamW + 32-bit Shampoo & 75.03 & 1774 & 12134 \\ \cline{1-1}  & & AdamW + 4-bit Shampoo & 74.78 & 1770 & 10804 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Performance, wall-clock time and memory cost on various image classification tasks. TA = test accuracy, WCT = wall-clock time, and TMC = total GPU memory cost.

Figure 4: Visualization of test accuracies on the CIFAR-100 and ImageNet-1k datasets.

[MISSING_PAGE_FAIL:9]

**Broader Impact.** Our work can facilitate training large models with second-order optimizers. This could open up new research possibilities that were previously unattainable due to GPU memory constraints, especially benefiting researchers with limited resources.

## Acknowledgments and Disclosure of Funding

Jia Li and Hua Huang were supported by the NSF of China (grant no. 62131003). Jia Li was also supported by the NSF of China (grant no. 62102034). Pan Zhou was supported by the Singapore Ministry of Education (MOE) Academic Research Fund (AcRF) Tier 1 grants (project ID: 23-SIS-SMU-028 and 23-SIS-SMU-070).

## References

* [1] Naman Agarwal, Rohan Anil, Elad Hazan, Tomer Koren, and Cyril Zhang. Disentangling adaptive gradient methods from learning rates. _arXiv preprint arXiv:2002.11803_, 2020.
* [2] Rohan Anil, Vineet Gupta, Tomer Koren, Kevin Regan, and Yoram Singer. Scalable second order optimization for deep learning. _arXiv preprint arXiv:2002.09018_, 2020.
* [3] Rohan Anil, Vineet Gupta, Tomer Koren, and Yoram Singer. Memory efficient adaptive optimization. _Advances in Neural Information Processing Systems_, 32, 2019.
* [4] A. Bjorck and C. Bowie. An iterative algorithm for computing the best estimate of an orthogonal matrix. _SIAM Journal on Numerical Analysis_, 8(2):358-364, 1971.
* [5] R.L. Burden, J.D. Faires, and A.M. Burden. _Numerical Analysis_. Cengage Learning, 2015.
* [6] Aaron Defazio, Xingyu Yang, Harsh Mehta, Konstantin Mishchenko, Ahmed Khaled, and Ashok Cutkosky. The road less scheduled. _arXiv preprint arXiv:2405.15682_, 2024.
* [7] Tim Dettmers. 8-bit approximations for parallelism in deep learning. In _Proceedings of the International Conference on Learning Representations_, 2016.
* [8] Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via blockwise quantization. In _Proceedings of the International Conference on Learning Representations_, 2022.
* [9] Tim Dettmers, Arridoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient finetuning of quantized LLMs. _Advances in Neural Information Processing Systems_, 2023.
* [10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _Proceedings of the International Conference on Learning Representations_, 2021.
* [11] Timothy Dozat. Incorporating Nesterov momentum into Adam. In _Proceedings of the International Conference on Learning Representations Workshop_, 2016.
* [12] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. _Journal of Machine Learning Research_, 12(61):2121-2159, 2011.
* [13] Sai Surya Duvvuri, Fnu Devvrit, Rohan Anil, Cho-Jui Hsieh, and Inderjit S Dhillon. Combining axes preconditioners through Kronecker approximation for deep learning. In _Proceedings of the International Conference on Learning Representations_, 2024.
* [14] Vladimir Feinberg, Xinyi Chen, Y. Jennifer Sun, Rohan Anil, and Elad Hazan. Sketchy: Memory-efficient adaptive regularization with frequent directions. _Advances in Neural Information Processing Systems_, 2023.
* [15] Elias Frantar, Eldar Kurtic, and Dan Alistarh. M-FAC: Efficient matrix-free approximations of second-order information. _Advances in Neural Information Processing Systems_, 2021.

* [16] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. Conformer: Convolution-augmented transformer for speech recognition. In _Proceedings of the Conference of the International Speech Communication Association_, 2020.
* [17] Chun-Hua Guo and Nicholas J. Higham. A Schur-Newton method for the matrix pth root and its inverse. _SIAM Journal on Matrix Analysis and Applications_, 28(3):788-804, 2006.
* [18] Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor optimization. In _Proceedings of the International Conference on Machine Learning_, 2018.
* [19] N. Halko, P. G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. _SIAM Review_, 53(2):217-288, 2011.
* [20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, June 2016.
* [21] Roger A. Horn and Charles R. Johnson. _Matrix Analysis_. Cambridge university press, 2012.
* [22] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _Proceedings of the International Conference on Learning Representations_, 2015.
* [23] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [24] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. _CS 231N_, 7(7):3, 2015.
* [25] Seung Hoon Lee, Seunghyun Lee, and Byung Cheol Song. Vision transformer for small-size datasets. _arXiv preprint arXiv:2112.13492_, 2021.
* [26] Bingrui Li, Jianfei Chen, and Jun Zhu. Memory efficient optimizers with 4-bit states. _Advances in Neural Information Processing Systems_, 2023.
* [27] Hong Liu, Zhiyuan Li, David Leo Wright Hall, Percy Liang, and Tengyu Ma. Sophia: A scalable stochastic second-order optimizer for language model pre-training. In _Proceedings of the International Conference on Learning Representations_, 2024.
* [28] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, October 2021.
* [29] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _Proceedings of the International Conference on Learning Representations_, 2019.
* [30] James Martens and Roger Grosse. Optimizing neural networks with Kronecker-factored approximate curvature. In _Proceedings of the International Conference on Machine Learning_, 2015.
* [31] Ning Qian. On the momentum term in gradient descent learning algorithms. _Neural Networks_, 12(1):145-151, 1999.
* [32] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, and others. Language models are unsupervised multitask learners. _OpenAI blog_, 2019.
* [33] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 21(140):1-67, 2020.
* [34] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet large scale visual recognition challenge. _International Journal of Computer Vision_, 115(3):211-252, 2015.

* [35] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In _Proceedings of the International Conference on Machine Learning_, 2018.
* [36] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In _Proceedings of the International Conference on Learning Representations_, 2015.
* [37] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, and others. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in Neural Information Processing Systems_, 30, 2017.
* [39] Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, and Michael W. Mahoney. AdaHessian: an adaptive second order optimizer for machine learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2021.
* [40] Jui-Nan Yen, Sai Surya Duvvuri, Inderjit S. Dhillon, and Cho-Jui Hsieh. Block low-rank preconditioner with shared basis for stochastic optimization. _Advances in Neural Information Processing Systems_, 2023.
* [41] Hongwei Yong, Ying Sun, and Lei Zhang. A general regret bound of preconditioned gradient method for DNN training. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, June 2023.
* [42] Lin Zhang, Shaohuai Shi, and Bo Li. Eva: Practical second-order optimization with Kronecker-vectorized approximation. In _Proceedings of the International Conference on Learning Representations_, 2023.
* [43] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. GaLore: Memory-efficient LLM training by gradient low-rank projection. In _Proceedings of the International Conference on Machine Learning_, 2024.
* [44] Pan Zhou, Xingyu Xie, and Shuicheng Yan. Win: Weight-decay-integrated Nesterov acceleration for adaptive gradient algorithms. In _Proceedings of the International Conference on Learning Representations_, 2023.

Implementation Details of Shampoo, CASPR, K-FAC and AdaBK

The implementation of 32-bit Shampoo used in our experiments is described in Algorithm 4. Our Pytorch implementation of Shampoo is partially based on the code provided by [2]. We implement CASSPR by replacing \(\widehat{\bm{G}}_{t}=\widehat{\bm{L}}_{t}\bm{G}_{t}\widehat{\bm{R}}_{t}\) with \(\bm{J}_{t}=\widehat{\bm{L}}_{t}\bm{G}_{t}+\bm{G}_{t}\widehat{\bm{R}}_{t}; \widehat{\bm{G}}_{t}=\widehat{\bm{L}}_{t}\bm{J}_{t}+\bm{J}_{t}\widehat{\bm{R}} _{t}\) in line 12 of Algorithm 4 and line 14 of Algorithm 3. We summarize the implementation of 32-bit K-FAC/AdaBK in Algorithm 5, where \(\bm{X}_{t}\) is the input feature and \(\bm{Y}_{t}\) is the output feature gradient. Both power iteration [5] and Schur-Newton iteration [17] are run for 10 iterations. Our implementation of 4-bit K-FAC/AdaBK is similar to 4-bit Shampoo (i.e., compressing \(\bm{L}_{t},\bm{R}_{t},\widehat{\bm{L}}_{t},\mathrm{and}\ \widehat{\bm{R}}_{t}\)).

```
0: initial parameter \(\bm{W}_{0}\in\mathbb{R}^{m\times n}\), left preconditioner \(\bm{L}_{0}=\epsilon\bm{I}_{m}\), right preconditioner \(\bm{R}_{0}=\epsilon\bm{I}_{n}\), inverse root of left preconditioner \(\widehat{\bm{L}}_{0}=\bm{I}_{m}\), inverse root of right preconditioner \(\widehat{\bm{R}}_{0}=\bm{I}_{n}\), total number of steps \(T\), interval of updating preconditioners \(T_{1}\), interval of updating inverse roots of preconditioners \(T_{2}\), exponential decay rate for preconditioners \(\beta\in(0,1)\), first-order optimizer \(\mathcal{F}\), first-order optimizer state \(\bm{s}_{0}=\bm{0}\).
0: final parameter \(\bm{W}_{T}\).
1:for\(t=1,2,\dots,T\)do
2: Receive loss function \(\mathcal{L}_{t}:\mathbb{R}^{m\times n}\mapsto\mathbb{R}\) and compute gradient \(\bm{G}_{t}=\nabla\mathcal{L}_{t}(\bm{W}_{t})\)
3:if\(t\%T_{1}\equiv 0\)then
4:\(\bm{L}_{t}=\beta\bm{L}_{t-1}+(1-\beta)\bm{G}_{t}\bm{G}_{t}^{\mathsf{T}};\quad \bm{R}_{t}=\beta\bm{R}_{t-1}+(1-\beta)\bm{G}_{t}^{\mathsf{T}}\bm{G}_{t}\)
5:else
6:\(\bm{L}_{t}=\bm{L}_{t-1};\quad\bm{R}_{t}=\bm{R}_{t-1}\)
7:if\(t\%T_{2}\equiv 0\)then
8: Compute maximum eigenvalues \(\lambda_{\max}^{L}\) and \(\lambda_{\max}^{R}\) of \(\bm{L}_{t}\) and \(\bm{R}_{t}\) by power iteration
9: Compute \(\widehat{\bm{L}}_{t}=(\bm{L}_{t}+\lambda_{\max}^{L}\epsilon\bm{I}_{m})^{-1/4}\) and \(\widehat{\bm{R}}_{t}=(\bm{R}_{t}+\lambda_{\max}^{R}\epsilon\bm{I}_{n})^{-1/4}\) by Schur-Newton iteration
10:else
11:\(\widehat{\bm{L}}_{t}=\widehat{\bm{L}}_{t-1};\quad\widehat{\bm{R}}_{t}=\widehat {\bm{R}}_{t-1}\)
12:\(\widehat{\bm{G}}_{t}=\widehat{\bm{L}}_{t}\bm{G}_{t}\widehat{\bm{R}}_{t};\quad \widehat{\bm{G}}_{t}=\widehat{\bm{G}}_{t}(\|\bm{G}_{t}\|_{F}/\|\widehat{\bm{G} }_{t}\|_{F})\)
13:\(\bm{W}_{t},\bm{s}_{t}=\mathcal{F}(\bm{W}_{t-1},\bm{s}_{t-1},\widehat{\bm{G}}_{t})\) ```

**Algorithm 4** Practical 32-bit Shampoo

```
0: initial parameter \(\bm{W}_{0}\in\mathbb{R}^{m\times n}\), left preconditioner \(\bm{L}_{0}=\bm{0}\), right preconditioner \(\bm{R}_{0}=\bm{0}\), inverse root of left preconditioner \(\widehat{\bm{L}}_{0}=\bm{I}_{m}\), inverse root of right preconditioner \(\widehat{\bm{R}}_{0}=\bm{I}_{n}\), total number of steps \(T\), interval of updating preconditioners \(T_{1}\), interval of updating inverse roots of preconditioners \(T_{2}\), \(\epsilon\), exponential decay rate for preconditioners \(\beta\in(0,1)\), \(\alpha=1\) for K-FAC / \(\alpha=2\) for AdaBK, first-order optimizer \(\mathcal{F}\), first-order optimizer state \(\bm{s}_{0}=\bm{0}\).
0: final parameter \(\bm{W}_{T}\).
1:for\(t=1,2,\dots,T\)do
2: Receive loss function \(\mathcal{L}_{t}:\mathbb{R}^{m\times n}\mapsto\mathbb{R}\) and compute gradient \(\bm{G}_{t}=\nabla\mathcal{L}_{t}(\bm{W}_{t})\)
3: Receive \(\bm{X}_{t}\) by forward propagation and \(\bm{Y}_{t}\) by backward propagation
4:if\(t\%T_{1}\equiv 0\)then
5:\(\bm{L}_{t}=\beta\bm{L}_{t-1}+(1-\beta)\bm{Y}_{t}\bm{Y}_{t}^{\mathsf{T}};\quad \bm{R}_{t}=\beta\bm{R}_{t-1}+(1-\beta)\bm{X}_{t}\bm{X}_{t}^{\mathsf{T}}\)
6:else
7:\(\bm{L}_{t}=\bm{L}_{t-1};\quad\bm{R}_{t}=\bm{R}_{t-1}\)
8:if\(t\%T_{2}\equiv 0\)then
9: Compute maximum eigenvalues \(\lambda_{\max}^{L}\) and \(\lambda_{\max}^{R}\) of \(\bm{L}_{t}\) and \(\bm{R}_{t}\) by power iteration
10: Compute \(\widehat{\bm{L}}_{t}=(\bm{L}_{t}+\lambda_{\max}^{L}\bm{L}_{m})^{-1/\alpha}\) and \(\widehat{\bm{R}}_{t}=(\bm{R}_{t}+\lambda_{\max}^{R}\epsilon\bm{I}_{n})^{-1/\alpha}\) by Schur-Newton iteration
11:else
12:\(\widehat{\bm{L}}_{t}=\widehat{\bm{L}}_{t-1};\quad\widehat{\bm{R}}_{t}=\widehat {\bm{R}}_{t-1}\)
13:\(\widehat{\bm{G}}_{t}=\widehat{\bm{L}}_{t}\bm{G}_{t}\widehat{\bm{R}}_{t};\quad \widehat{\bm{G}}_{t}=\widehat{\bm{G}}_{t}(\|\bm{G}_{t}\|_{F}/\|\widehat{\bm{G} }_{t}\|_{F})\)
14:\(\bm{W}_{t},\bm{s}_{t}=\mathcal{F}(\bm{W}_{t-1},\bm{s}_{t-1},\widehat{\bm{G}}_{t})\) ```

**Algorithm 5** Practical 32-bit K-FAC/AdaBK

## Appendix B Randomized SVD Method

Given an initial matrix \(\bm{P}_{0}\in\mathbb{R}^{n\times n}\), randomized SVD method computes the eigenvector matrix of a PD matrix \(\bm{A}\in\mathbb{R}^{n\times n}\) by iterating

\[\bm{P}_{t}=\text{QR}(\bm{A}\bm{P}_{t-1}),\] (4)

where \(\text{QR}(\bm{X})\) denotes the QR decomposition of matrix \(\bm{X}\), returning an orthogonal matrix. Since we can initialize \(\bm{P}_{0}\) with the previous result (e.g., \(\bm{V}\) in Algorithm 1), only a few iterations are enough to obtain an accurate estimation in practice. In our experiments, we iterate (4) once for Shampoo/CASPR, and iterate (4) twice for K-FAC/AdaBK.

## Appendix C Quantization Mappings

We present the constructions of different quantization mappings in \(b\)-bit quantizers (\(\mathcal{R}\) in \(\mathcal{Q}\)). See Figure 5 for the illustration of them. Note that \(\mathbb{T}_{b}\!=\!\{0,1,\dots,2^{b}\!-\!1\}\).

Dynamic tree (DT) quantization for \(b\)-bit quantization maps \(\mathbb{T}_{b}\) onto \(\{0,1\}\cup G\), where \(G\) is a set of numbers with the following properties: the number in \(G\) looks like \(\pm q_{k}\times 10^{-E}\), where a) \(b=2+E+F\), where \(E,F\) are integers; b) \(q_{k}=(p_{k}+p_{k+1})/2\), where \(k\in\{0,\dots,2^{F}-1\}\); c) \(p_{j}=0.9j/2^{F}+0.1\), where \(j\in\{0,\dots,2^{F}\}\). For 4-bit quantization, DT quantization maps \(\mathbb{T}_{4}\) onto {-0.8875, -0.6625, -0.4375, -0.2125, -0.0775, -0.0325, -0.0055, 0.0000, 0.0055, 0.0325, 0.0775, 0.2125, 0.4375, 0.6625, 0.8875, 1.0000\}. For 3-bit quantization, DT quantization maps \(\mathbb{T}_{3}\) onto {-0.7750, -0.3250, -0.0550, 0.0000, 0.0550, 0.3250, 0.7750, 1.0000\}.

For 4-bit quantization, linear square (Linear-2) quantization maps \(\mathbb{T}_{4}\) onto {-1.0000, -0.7511, -0.5378, -0.3600, -0.2178, -0.1111, -0.0400, 0.0000, 0.0044, 0.0400, 0.1111, 0.2178, 0.3600, 0.5378, 0.7511, 1.0000\}. For 3-bit quantization, Linear-2 quantization maps \(\mathbb{T}_{3}\) onto {-1.0000, -0.5102, -0.1837, 0.0000, 0.0204, 0.1837, 0.5102, 1.0000\}.

## Appendix D Quantization Error Analyses

We present more quantization error analyses of the preconditioners. Recall that we define two kinds of quantization errors in mapping \(f\) of transformation \(g\) at \(\bm{A}\in\mathbb{R}^{m\times n}\) (in short errors in \(f(\bm{A})\) of \(g\)) in Subsection 3.1. Here we extend them as follows: define the normwise relative error (NRE) in \(f\) of \((g_{1},g_{2})\) at \(\bm{A}\) as

\[\text{NRE}=\frac{\|f(\bm{A})-g_{2}\circ f\circ g_{1}(\bm{A})\|_{F}}{\|f(\bm{A}) \|_{F}},\]

and the angle error (AE) in \(f\) of \((g_{1},g_{2})\) at \(\bm{A}\) as

\[\text{AE}=\arccos\left(\frac{\langle f(\bm{A}),g_{2}\circ f\circ g_{1}(\bm{A}) \rangle}{\|f(\bm{A})\|_{F}\|g_{2}\circ f\circ g_{1}(\bm{A})\|_{F}}\right).\]

Figure 5: Visualization of DT quantization and Linear-2 quantization at \(b\)-bit (\(b=3,4\)) precision.

[MISSING_PAGE_FAIL:15]

### Dynamic Analysis

We define the normwise relative error (NRE) and angle error (AE) of \(\bm{B}\) deviating from \(\bm{A}\) as

\[\text{NRE}\!=\!\frac{\|\bm{B}-\bm{A}\|_{F}}{\|\bm{A}\|_{F}},\quad \text{AE}\!=\!\arccos\left(\frac{\langle\bm{A},\bm{B}\rangle}{\|\bm{A}\|_{F}\| \bm{B}\|_{F}}\right).\]

Consider Shampoo using 4-bit preconditioners for parameter updates, but also recording 32-bit preconditioners at the same time. We extract the left preconditioners \(\bm{L}_{4}\) and \(\bm{L}_{32}\!\in\!\mathbb{R}^{1200\times 1200}\) of a specific model parameter block \(\bm{W}\!\in\!\mathbb{R}^{1200\times 768}\) every 8000 steps in the Swin-Tiny training on CIFAR-100 with AdamW+Shampoo. Here \(\bm{L}_{4}\) is a decompressed 4-bit preconditioner, and \(\bm{L}_{32}\) is a 32-bit preconditioner.

Figure 7 shows the quantization errors during training. For naive 4-bit Shampoo, \(\bm{L}_{32}^{-1/4}\) and \(\bm{L}_{4}^{-1/4}\) are computed by Schur-Newton iteration used in Algorithm 4 where \(\epsilon\!=\!10^{-4}\). For our 4-bit Shampoo, \(\bm{L}_{32}^{-1/4}\) is computed by Schur-Newton iteration used in Algorithm 4 where \(\epsilon\!=\!10^{-4}\), and \(\bm{L}_{4}^{-1/4}\) is computed by Algorithm 2 without quantization where \(\epsilon\!=\!10^{-4},t_{2}\!=\!4\). We find that \(\epsilon\!=\!10^{-6}\) for Algorithm 2 used in our main experiments though is effective, yet it can cause a large numerical instability in the later stage of training (see Figure 8).

\begin{table}
\begin{tabular}{c|c c c c|c c c c} \hline \hline \multicolumn{8}{c|}{\(f(\bm{A})=\bm{A}^{-1/4}\)} & \multicolumn{2}{c}{\(f(\bm{A})\!=\!\bm{A}^{-1/4}\!-\!\mathrm{Diag}(\mathrm{diag}(\bm{A}^{-1/4}))\)} \\ \hline Mapping \(\mathcal{R}\) & QM & OR & NRE \(\downarrow\) & AE (\({}^{\circ}\)) \(\downarrow\) & Mapping \(\mathcal{R}\) & QM & OR & NRE \(\downarrow\) & AE (\({}^{\circ}\)) \(\downarrow\) \\ \hline \multirow{6}{*}{DT} & \(\bm{A}\) & \(\bm{\chi}\) & 0.2192 & 8.3014 & \multirow{6}{*}{DT} & \(\bm{A}\) & \(\bm{\chi}\) & 0.5001 & 23.644 \\  & \(\bm{U}\) & \(\bm{\chi}\) & 0.0060 & 0.3421 & & \(\bm{U}\) & \(\bm{\chi}\) & 0.0197 & 1.1273 \\  & \(\bm{U}\) & \(\bm{\check{\nu}}\) & 0.0037 & 0.2140 & & \(\bm{U}\) & \(\bm{\check{\nu}}\) & 0.0123 & 0.7022 \\  & \(\bm{B}\) & \(\bm{\chi}\) & 0.0029 & 0.1655 & & \(\bm{B}\) & \(\bm{\chi}\) & 0.0097 & 0.5553 \\  & \((\bm{A},\bm{B})\) & \(\bm{\chi}\) & 0.2193 & 8.3051 & & \((\bm{A},\bm{B})\) & \(\bm{\chi}\) & 0.5003 & 23.649 \\  & \((\bm{U},\bm{B})\) & \(\bm{\check{\nu}}\) & 0.0067 & 0.3810 & & \((\bm{U},\bm{B})\) & \(\bm{\check{\nu}}\) & 0.0219 & 1.2577 \\  & \((\bm{U},\bm{B})\) & \(\bm{\check{\nu}}\) & 0.0047 & 0.2712 & & \((\bm{U},\bm{B})\) & \(\bm{\check{\nu}}\) & 0.0156 & 0.8955 \\ \hline \multirow{6}{*}{Linear-2} & \(\bm{A}\) & \(\bm{\chi}\) & 0.2164 & 7.9751 & \multirow{6}{*}{Linear-2} & \(\bm{A}\) & \(\bm{\chi}\) & 0.4875 & 21.447 \\  & \(\bm{U}\) & \(\bm{\check{\nu}}\) & 0.0037 & 0.2121 & & \(\bm{U}\) & \(\bm{\check{\nu}}\) & 0.0122 & 0.6994 \\  & \(\bm{U}\) & \(\bm{\check{\nu}}\) & 0.0023 & 0.1312 & & \(\bm{U}\) & \(\check{\nu}\) & 0.0076 & 0.4343 \\  & \(\bm{B}\) & \(\bm{\chi}\) & 0.0021 & 0.1203 & & \(\bm{B}\) & \(\bm{\check{\nu}}\) & 0.0070 & 0.4035 \\  & \((\bm{A},\bm{B})\) & \(\bm{\check{\nu}}\) & 0.2164 & 7.9755 & & \((\bm{A},\bm{B})\) & \(\bm{\check{\nu}}\) & 0.4875 & 21.448 \\  & \((\bm{U},\bm{B})\) & \(\bm{\check{\nu}}\) & 0.0043 & 0.2439 & & \((\bm{U},\bm{B})\) & \(\bm{\check{\nu}}\) & 0.0141 & 0.8079 \\  & \((\bm{U},\bm{B})\) & \(\bm{\check{\nu}}\) & 0.0031 & 0.1791 & & \((\bm{U},\bm{B})\) & \(\check{\nu}\) & 0.0104 & 0.5935 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Quantization errors in \(f(\bm{A})\) of different 8-bit quantization schemes at a PD matrix \(\bm{A}\), where \(\bm{A}=\bm{A}_{1}\) is derived from the real world as described in Subsection 3.1. We employ block-wise normalization with a block size of 256. \(\bm{U}\) is the eigenvector matrix of \(\bm{A}\) and \(\bm{B}=(g_{1}(\bm{A}))^{-1/4}\). QM = quantized matrices and OR = orthogonal rectification.

## Appendix E Convergence Analysis

**More notations.** Given a symmetric real matrix \(\bm{A}\), \(\bm{A}\succeq 0\) means that \(\bm{A}\) is positive semidefinite (PSD), and \(\bm{A}\succ 0\) means that \(\bm{A}\) is positive definite (PD). Assume that symmetric matrices \(\bm{A}\) and \(\bm{B}\) are symmetric, the notations \(\bm{A}\succeq\bm{B}\) and \(\bm{A}\succ\bm{B}\) mean that \(\bm{A}-\bm{B}\succeq 0\) and \(\bm{A}-\bm{B}\succ 0\) respectively. Let \(\bm{A}\) be a PSD matrix and \(s\in\mathbb{R}\), we define \(\bm{A}^{s}\!=\!\bm{U}\bm{\Lambda}^{s}\bm{U}^{\mathsf{T}}\), where \(\bm{U}\bm{\Lambda}\bm{U}^{\mathsf{T}}\) is the Singular Value Decomposition (SVD) of \(\bm{A}\). The Mahalanobis norm of a vector \(\bm{x}\) induced by a PD matrix \(\bm{A}\) is \(\|\bm{x}\|_{\bm{A}}=\sqrt{\bm{x}^{\mathsf{T}}\bm{A}\bm{x}}\). The dual norm of \(\|\cdot\|_{\bm{A}}\) is denoted by \(\|\cdot\|_{\bm{A}}^{*}\), where \(\|\bm{x}\|_{\bm{A}}^{*}=\sqrt{\bm{x}^{\mathsf{T}}\bm{A}^{-1}\bm{x}}\). The spectral norm of matrix \(\bm{A}\) is \(\|\bm{A}\|_{2}=\sup_{\bm{x}\neq\bm{0}}\{\|\bm{A}\bm{x}\|_{2}/\|\bm{x}\|_{2}\}\). \(\bm{A}\otimes\bm{B}\) means the (right) Kronecker product of matrices \(\bm{A}\) and \(\bm{B}\). \(\overline{\rm{vec}}(\bm{A})\) means the vectorization (stacking the rows) of \(\bm{A}\).

We consider quantization as a perturbation and present the perturbed Shampoo in Algorithm 6 for convergence analysis. The regret bound of the perturbed Shampoo can be found in Theorem 1. Complete proofs can be found in Appendix F. We first introduce some basic technical tools, and the details of them are in [18; 21].

**Lemma 3**.: _Let \(\bm{A},\bm{A}^{\prime},\bm{B},\bm{B}^{\prime}\) be matrices of appropriate dimensions, and \(\bm{u},\bm{v}\) be two column vectors. The following properties hold:_

1. \((\bm{A}\otimes\bm{B})(\bm{A}^{\prime}\otimes\bm{B}^{\prime})=(\bm{A}\bm{A}^{ \prime})\otimes(\bm{B}\bm{B}^{\prime});\)___._
2. \((\bm{A}\otimes\bm{B})^{\mathsf{T}}=(\bm{A}^{\mathsf{T}}\otimes\bm{B}^{\mathsf{T}});\)__
3. _If_ \(\bm{A},\bm{B}\succeq 0\) _and_ \(s\in\mathbb{R}\)_, then_ \((\bm{A}\otimes\bm{B})^{s}=(\bm{A}^{s}\otimes\bm{B}^{s});\)__
4. _If_ \(\bm{A}\succeq\bm{A}^{\prime}\) _and_ \(\bm{B}\succeq\bm{B}^{\prime}\)_, then_ \(\bm{A}\otimes\bm{B}\succeq\bm{A}^{\prime}\otimes\bm{B}^{\prime};\)__
5. \(\operatorname{tr}(\bm{A}\bm{B})=\operatorname{tr}(\bm{A})\operatorname{tr}( \bm{B});\)__
6. \(\operatorname{\overline{vec}}(\bm{u}\bm{v}^{\mathsf{T}})=\bm{u}\otimes\bm{v}\)_._

**Lemma 4**.: _Let \(\bm{G}\in\mathbb{R}^{m\times n},\bm{L}\in\mathbb{R}^{m\times m},\bm{R}\in \mathbb{R}^{n\times n}\), then it holds that_

\[(\bm{L}\otimes\bm{R}^{\mathsf{T}})\overline{\operatorname{vec}}(\bm{G})= \overline{\operatorname{vec}}(\bm{L}\bm{G}\bm{R}).\]

**Lemma 5**.: _Assume that \(0\preceq\bm{X}_{i}\preceq\bm{Y}_{i}\) for \(i=1,\ldots,n\). Assume further that all \(\bm{X}_{i}\) commute with each other and all \(\bm{Y}_{i}\) commute with each other. Let \(\alpha_{1},\ldots,\alpha_{n}\geq 0\) such that \(\sum_{i=1}^{n}\alpha_{i}=1\), then_

\[\bm{X}_{1}^{\alpha_{1}}\cdots\bm{X}_{n}^{\alpha_{n}}\preceq\bm{Y}_{1}^{\alpha _{1}}\cdots\bm{Y}_{n}^{\alpha_{n}}.\]

**Lemma 6**.: _Let \(0\leq\alpha\leq 1\) and \(0\preceq\bm{X}\preceq\bm{Y}\), then \(\bm{X}^{\alpha}\preceq\bm{Y}^{\alpha}\)._

**Lemma 7**.: _Let \(\bm{A}\succ 0\) and \(\bm{B}\succ 0\), then it holds that \(\bm{A}\succeq\bm{B}\) if and only if \(\bm{B}^{-1}\succeq\bm{A}^{-1}\)._

**Lemma 8** (von Neumann).: _Let \(\bm{A},\bm{B}\in\mathbb{R}^{m\times n}\) and \(q=\min\{m,n\}\). Let \(\sigma_{1}(\bm{A})\geq\cdots\geq\sigma_{q}(\bm{A})\) and \(\sigma_{1}(\bm{B})\geq\cdots\geq\sigma_{q}(\bm{B})\) denote the non-increasingly ordered singular values of \(\bm{A}\) and \(\bm{B}\), respectively. Then_

\[\langle\bm{A},\bm{B}\rangle\leq\sum_{i=1}^{q}\sigma_{i}(\bm{A})\sigma_{i}(\bm {B}).\]

**Lemma 9**.: _Assume that function \(f_{t}\) is continuously differentiable and convex on \(\mathbb{R}^{d}\), and matrix \(\bm{H}_{t}\succ 0\) for \(t=1,\ldots,T\). Given \(\bm{w}_{0}\in\mathbb{R}^{d},\eta>0\), define \(\bm{w}_{t+1}=\bm{w}_{t}-\eta\bm{H}_{t}^{-1}\bm{g}_{t}\), where \(\bm{g}_{t}=\nabla f_{t}(\bm{w}_{t})\). Then for any \(\bm{w}^{*}\in\mathbb{R}^{d}\), we have_

\[\sum_{t=1}^{T}f_{t}(\bm{w}_{t})-\sum_{t=1}^{T}f_{t}(\bm{w}^{*})\leq\frac{1}{ 2\eta}\sum_{t=1}^{T}(\|\bm{w}_{t}-\bm{w}^{*}\|_{\bm{H}_{t}}^{2}-\|\bm{w}_{t+1} -\bm{w}^{*}\|_{\bm{H}_{t}}^{2})+\frac{\eta}{2}\sum_{t=1}^{T}(\|\bm{g}_{t}\|_{ \bm{H}_{t}}^{*})^{2}.\]

**Lemma 10**.: _Let \(\bm{g}_{1},\ldots,\bm{g}_{T}\) be a sequence of vectors. For \(\rho>0\), define \(\widehat{\bm{H}}_{t}=(\rho\bm{I}+\sum_{s=1}^{t}\bm{g}_{s}\bm{g}_{s}^{\mathsf{T }})^{1/2}\). Then we have_

\[\sum_{t=1}^{T}(\|\bm{g}_{t}\|_{\widehat{\bm{H}}_{t}}^{*})^{2}\leq 2 \operatorname{tr}(\widehat{\bm{H}}_{T}).\]

**Lemma 11**.: _Assume that \(\bm{G}_{1},\ldots,\bm{G}_{T}\in\mathbb{R}^{m\times n}\) are matrices of rank at most \(r\). Let \(s\) for \(t=1,\ldots,T\). Then for any \(\epsilon\geq 0\),_

\[\epsilon\bm{I}_{mn}+\frac{1}{r}\sum_{t=1}^{T}\bm{g}_{t}\bm{g}_{t}^{\mathsf{T }}\preceq(\epsilon\bm{I}_{m}+\sum_{t=1}^{T}\bm{G}_{t}\bm{G}_{t}^{\mathsf{T}})^ {1/2}\otimes(\epsilon\bm{I}_{n}+\sum_{t=1}^{T}\bm{G}_{t}^{\mathsf{T}}\bm{G}_{t} )^{1/2}.\]

The key to the convergence proof of Algorithm 6 is forming a PD matrix sequence \(\{\bm{H}_{i}\}_{i=1}^{T}\), which satisfies \(0\prec\bm{H}_{1}\preceq\cdots\preceq\bm{H}_{T}\). To achieve it, we gives the following lemma extended from Lemma 2 in the Appendix of [40].

**Lemma 12**.: _Let \(\{\bm{X}_{t}\}_{t=1}^{t=T}\) be a sequence of symmetric matrices, and \(\bm{A}_{t}=\sum_{s=1}^{t}\bm{X}_{s}\), where \(t=1,\ldots,T\). Suppose we have two sequences of symmetric matrices \(\{\bm{Y}_{t}\}_{t=1}^{t=T},\{\bm{Z}_{t}\}_{t=0}^{t=T}\), and a sequence real numbers \(\{\rho_{t}\}_{t=0}^{t=T}\) satisfying_

\[\bm{Y}_{t}=\bm{Z}_{t-1}+\bm{X}_{t},\quad\rho_{t}=\rho_{t-1}+\|\bm{Y}_{t}-\bm{Z}_ {t}\|_{2},\quad\bm{Z}_{0}=\bm{0},\rho_{0}=0.\]

_Define \(\bm{B}_{t}=\rho_{t}\bm{I}+\bm{Z}_{t}\), where \(\bm{I}\) denotes the identity matrix. Then for \(t=1,\ldots,T\), we have_

\[\bm{B}_{t}\succeq\bm{B}_{t-1}+\bm{X}_{t},\quad\bm{A}_{t}\preceq\bm{B}_{t} \preceq 2\rho_{t}\bm{I}+\bm{A}_{t}.\]

**Theorem 1**.: _Assume that the gradients \(\bm{G}_{1},\ldots,\bm{G}_{T}\in\mathbb{R}^{m\times n}\) are matrices of rank at most \(r\). Then for any \(\bm{W}^{*}\in\mathbb{R}^{m\times n}\) and \(\epsilon>0\), if \(\eta=D/\sqrt{2r}\), the regret of Algorithm 6 is bounded as follows,_

\[\sum_{t=1}^{T}f_{t}(\bm{W}_{t})-\sum_{t=1}^{T}f_{t}(\bm{W}^{*})\leq\sqrt{2r}D[2^{ 1/4}m\rho_{T}^{1/4}+\operatorname{tr}(\tilde{\bm{L}}_{T}^{1/4})][2^{1/4}n\mu_{T} ^{1/4}+\operatorname{tr}(\tilde{\bm{R}}_{T}^{1/4})],\]

_where \(D=\max_{t\in[T]}\|\bm{W}_{t}-\bm{W}^{*}\|_{F}\), \(\tilde{\bm{L}}_{t}=\epsilon\bm{I}_{m}+\sum_{t=1}^{T}\bm{G}_{t}\bm{G}_{t}^{ \mathsf{T}}\), and \(\tilde{\bm{R}}_{t}=\epsilon\bm{I}_{n}+\sum_{t=1}^{T}\bm{G}_{t}^{\mathsf{T}}\bm{G}_{t}\)._

Though we get a convergence guarantee of Algorithm 6, the upper bound given by Theorem 1 is very slack, since \(2^{1/4}m\rho_{T}^{1/4}\) is about the same as \(\operatorname{tr}(\tilde{\bm{L}}_{T}^{1/4})\) for 4-bit quantization schemes in practice.

Proofs

**Lemma 1**.: _Let \(\bm{A}\) be a PD matrix whose SVD is \(\bm{U}\bm{\Lambda}\bm{U}^{\mathsf{T}}\), where \(\bm{U}\!=\![\bm{u}_{i}]\) is an orthogonal matrix and \(\bm{\Lambda}\!=\!\mathrm{diag}([\lambda_{i}]^{\mathsf{T}})\) is a diagonal matrix. Given a perturbation \(\Delta\bm{U}\!=\![\Delta\bm{u}_{i}]\) and \(s\in\mathbb{R}\), we define \(\bm{B}\!:=\!(\bm{U}\bm{\Lambda}\bm{U}^{\mathsf{T}})^{s}\) and \(\Delta\bm{B}\!:=\!((\bm{U}\!+\!\Delta\bm{U})\bm{\Lambda}(\bm{U}\!+\!\Delta\bm{ U})^{\mathsf{T}})^{s}\!-\!\bm{B}\)._

1. _If_ \(\bm{U}\!+\!\Delta\bm{U}\) _is orthogonal and there exists_ \(\alpha\in\mathbb{R}\) _such that_ \(\|\Delta\bm{u}_{i}\|_{2}\leq\alpha\)_, then_ \[\frac{\|\Delta\bm{B}\|_{F}}{\|\bm{B}\|_{F}}\leq 2\alpha.\]
2. _If_ \(\bm{U}\!+\!\Delta\bm{U}\) _is orthogonal and there exists_ \(\beta\in\mathbb{R}\) _such that_ \(\langle\bm{u}_{i},\bm{u}_{i}\!+\!\Delta\bm{u}_{i}\rangle\geq 1-\beta\geq 0\)_, then_ \[\frac{\langle\bm{B},\bm{B}\!+\!\Delta\bm{B}\rangle}{\|\bm{B}\|_{F}\|\bm{B}\!+\! \Delta\bm{B}\|_{F}}\geq(1\!-\!\beta)^{2}.\]

Proof.: (1) Since \(\bm{U}\) and \(\bm{U}+\Delta\bm{U}\) are orthogonal, we have

\[\bm{B}=\bm{U}\bm{\Lambda}^{s}\bm{U}^{\mathsf{T}},\quad\bm{B}+\Delta\bm{B}=(\bm {U}+\Delta\bm{U})\bm{\Lambda}^{s}(\bm{U}+\Delta\bm{U})^{\mathsf{T}},\]

by definition. This leads to

\[\Delta\bm{B}=\bm{U}\bm{\Lambda}^{s}\Delta\bm{U}^{\mathsf{T}}+\Delta\bm{U}\bm{ \Lambda}^{s}(\bm{U}+\Delta\bm{U})^{\mathsf{T}}.\]

The Frobenius norm satisfies the triangle inequality and is orthogonality invariant. Hence,

\[\|\Delta\bm{B}\|_{F} =\|\bm{U}\bm{\Lambda}^{s}\Delta\bm{U}^{\mathsf{T}}+\Delta\bm{U} \bm{\Lambda}^{s}(\bm{U}+\Delta\bm{U})^{\mathsf{T}}\|_{F}\] \[\leq\|\bm{U}\bm{\Lambda}^{s}\Delta\bm{U}^{\mathsf{T}}\|_{F}+\| \Delta\bm{U}\bm{\Lambda}^{s}(\bm{U}+\Delta\bm{U})^{\mathsf{T}}\|_{F}\] \[=\|\bm{\Lambda}^{s}\Delta\bm{U}^{\mathsf{T}}\|_{F}+\|\Delta\bm{U} \bm{\Lambda}^{s}\|_{F}=2\|\Delta\bm{U}\bm{\Lambda}^{s}\|_{F}\] \[=2\sqrt{\sum\nolimits_{i}\|\lambda_{i}^{s}\Delta\bm{u}_{i}\|_{2}^ {2}}=2\sqrt{\sum\nolimits_{i}\lambda_{i}^{2s}\|\Delta\bm{u}_{i}\|_{2}^{2}}\] \[\leq 2\sqrt{\sum\nolimits_{i}\lambda_{i}^{2s}\alpha^{2}}=2 \alpha\sqrt{\sum\nolimits_{i}\lambda_{i}^{2s}}=2\alpha\|\bm{\Lambda}^{s}\|_{F}\] \[=2\alpha\|\bm{B}\|_{F}.\]

(2) Similar to (1), we have

\[\Delta\bm{B}=\bm{U}\bm{\Lambda}^{s}\Delta\bm{U}^{\mathsf{T}}+\Delta\bm{U}\bm{ \Lambda}^{s}\bm{U}^{\mathsf{T}}+\Delta\bm{U}\bm{\Lambda}^{s}\Delta\bm{U}^{ \mathsf{T}}.\]

From \(\langle\bm{u}_{i},\bm{u}_{i}+\Delta\bm{u}_{i}\rangle\geq 1-\beta\geq 0\), we get \(0\geq\langle\bm{u}_{i},\Delta\bm{u}_{i}\rangle\geq-\beta\geq-1\) because

\[1=\|\bm{u}_{i}\|_{2}\|\bm{u}_{i}+\Delta\bm{u}_{i}\|_{2}\geq\langle\bm{u}_{i}, \bm{u}_{i}+\Delta\bm{u}_{i}\rangle=1+\langle\bm{u}_{i},\Delta\bm{u}_{i} \rangle\geq 1-\beta\geq 0,\]

holds due to the orthogonality of \(\bm{U}\) and \(\bm{U}+\Delta\bm{U}\). Hence,

\[\langle\bm{B},\Delta\bm{B}\rangle =\mathrm{tr}(2\bm{U}\bm{\Lambda}^{2s}\Delta\bm{U}^{\mathsf{T}}+\bm{ U}\bm{\Lambda}^{s}\bm{U}^{\mathsf{T}}\Delta\bm{U}\bm{\Lambda}^{s}\Delta\bm{U}^{ \mathsf{T}})\] \[=\mathrm{tr}\!\left(\sum\nolimits_{i}2\lambda_{i}^{2s}\bm{u}_{i} \Delta\bm{u}_{i}^{\mathsf{T}}\right)+\mathrm{tr}\!\left[\left(\sum\nolimits_{i} \lambda_{i}^{s}\bm{u}_{i}\bm{u}_{i}^{\mathsf{T}}\right)\!\left(\sum\nolimits_{ j}\lambda_{j}^{s}\Delta\bm{u}_{j}\Delta\bm{u}_{j}^{\mathsf{T}}\right)\right]\] \[=\left(\sum\nolimits_{i}2\lambda_{i}^{2s}\langle\bm{u}_{i},\Delta \bm{u}_{i}\rangle\right)+\left(\sum\nolimits_{ij}\lambda_{i}^{s}\lambda_{j}^{ s}\langle\bm{u}_{i},\Delta\bm{u}_{j}\rangle^{2}\right)\] \[\geq\left(\sum\nolimits_{i}2\lambda_{i}^{2s}\langle\bm{u}_{i}, \Delta\bm{u}_{i}\rangle\right)+\left(\sum\nolimits_{i}\lambda_{i}^{2s}\langle \bm{u}_{i},\Delta\bm{u}_{i}\rangle^{2}\right)\] \[=\sum\nolimits_{i}\lambda_{i}^{2s}[(1+\langle\bm{u}_{i},\Delta\bm {u}_{i}\rangle)^{2}-1]\] \[\geq\sum\nolimits_{i}\lambda_{i}^{2s}[(1-\beta)^{2}-1]=[(1-\beta)^ {2}-1]\|\bm{\Lambda}^{s}\|_{F}^{2}\] \[=[(1-\beta)^{2}-1]\|\bm{B}\|_{F}^{2}=[(1-\beta)^{2}-1]\langle\bm{B},\bm{B}\rangle.\]

Therefore, we have

\[\frac{\langle\bm{B},\bm{B}+\Delta\bm{B}\rangle}{\|\bm{B}\|_{F}\|\bm{B}+\Delta\bm {B}\|_{F}}=\frac{\langle\bm{B},\bm{B}+\Delta\bm{B}\rangle}{\langle\bm{B},\bm{B} \rangle}=1+\frac{\langle\bm{B},\Delta\bm{B}\rangle}{\langle\bm{B},\bm{B}\rangle }\geq(1-\beta)^{2}.\]

The proof is completed.

**Lemma 2**.: _Let \(\bm{A}\) be a PD matrix of order \(m+n\) whose SVD is \(\bm{U}\bm{\Lambda}\bm{U}^{\mathsf{T}}\), where \(m,n\in\mathbb{N}_{+}\), \(n=lm\), \(\bm{U}=[\bm{u}_{i}]\) is an orthogonal matrix and \(\bm{\Lambda}=\operatorname{diag}([\lambda_{i}]^{\mathsf{T}})\) is a diagonal matrix. Assume that \(\bm{\Lambda}=\operatorname{diag}([c\lambda\bm{1}_{m\times 1}^{\mathsf{T}}, \lambda\bm{1}_{n\times 1}^{\mathsf{T}}]^{\mathsf{T}})\), \(c\geq 1\), and \(\lambda>0\). Given a perturbation \(\Delta\bm{\Lambda}=\operatorname{diag}([\bm{0}_{m\times 1}^{\mathsf{T}}, \Delta\bm{\lambda}_{n\times 1}^{\mathsf{T}}]^{\mathsf{T}})\) and \(s\in\mathbb{R}\), we define \(\bm{B}\!:=\!(\bm{U}\bm{\Lambda}\bm{U}^{\mathsf{T}})^{s}\) and \(\Delta\bm{B}\!:=\!(\bm{U}(\bm{\Lambda}\!+\!\Delta\bm{\Lambda})\bm{U}^{\mathsf{ T}})^{s}\!-\!\bm{B}\)._

1. _If_ \(\Delta\bm{\lambda}_{n\times 1}=(k-1)\lambda\bm{1}_{n\times 1}\) _where_ \(k>0\)_, then_ \[\frac{\|\Delta\bm{B}\|_{F}}{\|\bm{B}\|_{F}}=\frac{\sqrt{l}|k^{s}-1|}{\sqrt{c^ {2s}+l}}=h_{1}(s,l).\] _Moreover,_ \(h_{1}(s,l)\) _decreases monotonically with_ \(s\) _over_ \((-\infty,0)\) _and increases monotonically with_ \(l\) _over_ \((0,+\infty)\)_._
2. _If_ \(\Delta\bm{\lambda}_{n\times 1}=(tc-1)\lambda\bm{1}_{n\times 1}\) _where_ \(t>0\)_, then_ \[\frac{\langle\bm{B},\bm{B}+\Delta\bm{B}\rangle}{\|\bm{B}\|_{F}\|\bm{B}+\Delta \bm{B}\|_{F}}=\frac{lt^{s}+c^{s}}{\sqrt{(1+lt^{2s})(l+c^{2s})}}=h_{2}(l).\] _Moreover,_ \(h_{2}(l)\) _decreases monotonically with_ \(l\) _over_ \((0,(c/t)^{s}]\) _and increases monotonically with_ \(l\) _over_ \((c/t)^{s},+\infty)\)_._
3. _If_ \(\Delta\bm{\lambda}_{n\times 1}=(tc-1)\lambda\bm{1}_{n\times 1}\) _where_ \(k=tc>0\) _and_ \(l=(c/t)^{s}\)_, then_ \[\frac{\|\Delta\bm{B}\|_{F}}{\|\bm{B}\|_{F}}=\frac{|k^{s}-1|}{\sqrt{k^{s}+1}}, \quad\frac{\langle\bm{B},\bm{B}+\Delta\bm{B}\rangle}{\|\bm{B}\|_{F}\|\bm{B}+ \Delta\bm{B}\|_{F}}=\frac{2}{\sqrt{2+k^{s}+1/k^{s}}}.\]

Proof.: (1) Since \(\bm{U}\) is orthogonal, we have

\[\|\Delta\bm{B}\|_{F}=\|(\bm{\Lambda}+\Delta\bm{\Lambda})^{s}-\bm{\Lambda}^{s} \|_{F}=\sqrt{n}|k^{s}-1|\lambda^{s},\quad\|\bm{B}\|_{F}=\|\bm{\Lambda}^{s}\|_{ F}=\sqrt{mc^{2s}+n}\lambda^{s}.\]

Hence,

\[\frac{\|\Delta\bm{B}\|_{F}}{\|\bm{B}\|_{F}}=\frac{\sqrt{n}|k^{s}-1|}{\sqrt{mc^ {2s}+n}}=\frac{\sqrt{l}|k^{s}-1|}{\sqrt{c^{2s}+l}}=h_{1}(s,l)\geq 0.\]

It is easy to check that \(h_{1}\) increases monotonically with \(l\) over \((0,+\infty)\). To prove \(h_{1}\) decreases monotonically with \(s\) over \((-\infty,0)\), define

\[g_{1}(s)=\frac{1}{l}(h_{1}(s,l))^{2}=\frac{(k^{s}-1)^{2}}{c^{2s}+l}.\]

Consider the derivative of \(g_{1}\)

\[g_{1}^{\prime}(s) =\frac{(c^{2s}+l)2(k^{s}-1)k^{s}\ln k-(k^{s}-1)^{2}c^{2s}2\ln c}{ (c^{2s}+l)^{2}}\] \[=\frac{2(k^{s}-1)\left((c^{2s}+l)k^{s}\ln k-(k^{s}-1)c^{2s}\ln c \right)}{(c^{2s}+l)^{2}}.\]

If \(s<0\) and \(k>1\), then \(k^{s}-1<0,k^{s}\ln k>0\) leading to \(g_{1}^{\prime}(s)<0\) since \(c\geq 0\); Similarly, if \(s<0\) and \(0<k\leq 1\), then \(k^{s}-1\geq 0,k^{s}\ln k\leq 0\) leading to \(g_{1}^{\prime}(s)\leq 0\). Thus \(g_{1}(s)\) is a monotonically decreasing function for \(s<0\), which implies that \(h_{1}\) decreases monotonically with \(s\) over \((-\infty,0)\).

(2) Similar to (1), we have

\[\|\bm{B}\|_{F}=\sqrt{mc^{2s}+n}\lambda^{s},\quad\|\bm{B}+\Delta\bm{B}\|_{F}= \sqrt{nt^{2s}+mc}c^{s}\lambda^{s}.\]

Besides,

\[\langle\bm{B},\bm{B}+\Delta\bm{B}\rangle=\operatorname{tr}(\bm{U}\bm{\Lambda}^{ s}(\bm{\Lambda}+\Delta\bm{\Lambda})^{s}\bm{U}^{\mathsf{T}})=\operatorname{tr}( \bm{\Lambda}^{s}(\bm{\Lambda}+\Delta\bm{\Lambda})^{s})=(mc^{2s}+nc^{s}t^{s}) \lambda^{2s}.\]

Hence, we get

\[\frac{\langle\bm{B},\bm{B}+\Delta\bm{B}\rangle}{\|\bm{B}\|_{F}\|\bm{B}+\Delta \bm{B}\|_{F}}=\frac{nt^{s}+mc^{s}}{\sqrt{(m+nt^{2s})(n+mc^{2s})}}=\frac{lt^{s}+c ^{s}}{\sqrt{(1+lt^{2s})(l+c^{2s})}}=h_{2}(l)\geq 0.\]To prove \(h_{2}\) decreases monotonically with \(l\) over \((0,(c/t)^{s}]\) and increases monotonically with \(l\) over \(((c/t)^{s},+\infty)\), we define

\[g_{2}(l)=(h_{2}(l))^{2}=\frac{(lt^{s}+c^{s})^{2}}{(1+lt^{2s})(l+c^{2s})},\]

whose monotonicity is equivalent to that of \(h_{2}\) for \(l>0\). Consider the derivative of \(g_{2}\)

\[g_{2}^{\prime}(l)=\Big{(}\frac{t^{2s}l^{2}+2t^{s}c^{s}l+c^{2s}}{t^{2s}l^{2}+l+ t^{2s}c^{2s}l+c^{2s}}\Big{)}^{\prime}=\frac{(ts^{-}t^{2s}c^{s})^{2}l^{2}-(c^{s} -t^{s}c^{2s})^{2}}{(t^{2s}l^{2}+l+t^{2s}c^{2s}l+c^{2s})^{2}}.\]

If \(s=0\) or \(tc=1\), then \(g_{2}(l)\equiv 1\). If \(s\neq 0\) and \(tc\neq 1\), then \((t^{s}-t^{2s}c^{s})^{2}>0,(c^{s}-t^{s}c^{2s})^{2}>0\). In this case, let \(g_{2}^{\prime}(l)=0\), we get

\[t^{2s}(1-t^{s}c^{s})^{2}l^{2}=c^{2s}(1-t^{s}c^{s})^{2},\]

which implies that \(l=(c/t)^{s}\). It is easy to see that \(g_{2}\) decreases monotonically with \(l\) over \((0,(c/t)^{s}]\) and increases monotonically with \(l\) over \(((c/t)^{s},+\infty)\).

(3) According to (1)(2), we can easily get

\[\frac{\|\Delta\bm{B}\|_{F}}{\|\bm{B}\|_{F}}=\frac{|k^{s}-1|}{\sqrt{k^{s}+1}}, \quad\frac{\langle\bm{B},\bm{B}+\Delta\bm{B}\rangle}{\|\bm{B}\|_{F}\|\bm{B}+ \Delta\bm{B}\|_{F}}=\frac{2}{\sqrt{2+k^{s}+1/k^{s}}}.\]

The proof is completed. 

**Proposition 1**.: _Let \(\bm{A}\) be a PD matrix of order \(m+n\) whose SVD is \(\bm{U}\bm{\Lambda}\bm{U}^{\mathsf{T}}\), where \(m,n\in\mathbb{N}_{+}\), \(n=lm\), \(\bm{U}=[\bm{u}_{i}]\) is an orthogonal matrix, \(\bm{\Lambda}=\operatorname{diag}([c\bm{\lambda}\bm{1}_{m\times 1}^{\mathsf{T}}, \bm{\lambda}\bm{1}_{n\times 1}^{\mathsf{T}}]^{\mathsf{T}})\), \(c\geq 1000\), and \(\lambda>0\). Given \(\Delta\bm{U}=[\Delta\bm{u}_{i}]\), \(\Delta\bm{\Lambda}=\operatorname{diag}([\bm{0}_{m\times 1}^{\mathsf{T}}, \Delta\bm{\lambda}_{n\times 1}^{\mathsf{T}}]^{\mathsf{T}})\), and \(s\leq-0.25\), we define \(\bm{B}:=(\bm{U}\bm{\Lambda}\bm{U}^{\mathsf{T}})^{s}\), \(\bm{B}_{1}:=(\bm{(U}+\Delta\bm{U})\bm{\Lambda}(\bm{U}+\Delta\bm{\Lambda})^{ \mathsf{T}})^{s}\), and \(\bm{B}_{2}:=(\bm{U}(\bm{\Lambda}+\Delta\bm{\Lambda})\bm{U}^{\mathsf{T}})^{s}\). If \(\bm{U}+\Delta\bm{U}\) is orthogonal, \(\|\Delta\bm{u}_{i}\|_{2}\leq 0.1,\langle\bm{u}_{i},\Delta\bm{u}_{i}\rangle\!\geq\!-0.005\), \(\Delta\bm{\lambda}_{n\times 1}\!=\!(0.02c-1)\lambda\bm{1}_{n\times 1}\), and \(l\!=\!(c/0.02)^{s}\), then_

\[2\frac{\|\bm{B}_{1}-\bm{B}\|_{F}}{\|\bm{B}\|_{F}}\!\leq\!0.4\!\leq\!\frac{\|\bm {B}_{2}-\bm{B}\|_{F}}{\|\bm{B}\|_{F}},\quad 6\!\left(\!1-\frac{\langle\bm{B},\bm{B}_{1} \rangle}{\|\bm{B}\|_{F}\|\bm{B}_{1}\|_{F}}\!\right)\!\leq\!0.06\!\leq\!\left(\! 1-\frac{\langle\bm{B},\bm{B}_{2}\rangle}{\|\bm{B}\|_{F}\|\bm{B}_{2}\|_{F}}\! \right).\]

Proof.: According to Lemma 1, we have

\[\frac{\|\bm{B}_{1}-\bm{B}\|_{F}}{\|\bm{B}\|_{F}}\leq 0.2,\quad\frac{\langle\bm{B}, \bm{B}_{1}\rangle}{\|\bm{B}\|_{F}\|\bm{B}\|_{1}\|_{F}}\geq(1-0.005)^{2}\geq 0.99.\]

On the other hand, from Lemma 2(3), we get

\[\frac{\|\bm{B}_{2}-\bm{B}\|_{F}}{\|\bm{B}\|_{F}}=\frac{|x-1|}{\sqrt{x+1}}=f_{1 }(x),\quad\frac{\langle\bm{B},\bm{B}_{2}\rangle}{\|\bm{B}\|_{F}\|\bm{B}_{2}\|_{F }}=\frac{2}{\sqrt{2+x+1/x}}=f_{2}(x),\]

where \(x=(0.02c)^{s}\in(0,20^{-1/4}]\). It is easy to verify that \(f_{1}\) decreases monotonically and \(f_{2}\) increases monotonically for \(0<x<1\). Hence

\[f_{1}(x)\geq f_{1}(20^{-1/4})\geq 0.4,\quad f_{2}(x)\leq f_{2}(20^{-1/4})\leq 0.94.\]

The proof is completed. 

**Lemma 12**.: _Let \(\{\bm{X}_{t}\}_{t=1}^{t=T}\) be a sequence of symmetric matrices, and \(\bm{A}_{t}=\sum_{s=1}^{t}\bm{X}_{s}\), where \(t=1,\ldots,T\). Suppose we have two sequences of symmetric matrices \(\{\bm{Y}_{t}\}_{t=1}^{t=T},\{\bm{Z}_{t}\}_{t=0}^{t=T}\), and a sequence real numbers \(\{\rho_{t}\}_{t=0}^{t=T}\) satisfying_

\[\bm{Y}_{t}=\bm{Z}_{t-1}+\bm{X}_{t},\quad\rho_{t}=\rho_{t-1}+\|\bm{Y}_{t}-\bm{Z}_ {t}\|_{2},\quad\bm{Z}_{0}=\bm{0},\rho_{0}=0.\]

_Define \(\bm{B}_{t}=\rho_{t}\bm{I}+\bm{Z}_{t}\), where \(\bm{I}\) denotes the identity matrix. Then for \(t=1,\ldots,T\), we have_

\[\bm{B}_{t}\succeq\bm{B}_{t-1}+\bm{X}_{t},\quad\bm{A}_{t}\preceq\bm{B}_{t} \preceq 2\rho_{t}\bm{I}+\bm{A}_{t}.\]Proof.: Note that for any symmetric matrix \(\bm{S}\), it holds that \(\|\bm{S}\|_{2}\bm{I}\succeq\bm{S}\). Then we have

\[(\rho_{t}-\rho_{t-1})\bm{I}+\bm{Z}_{t}=\|\bm{Y}_{t}-\bm{Z}_{t}\|_{2} \bm{I}+\bm{Z}_{t}\succeq\bm{Y}_{t}.\]

Adding \(\rho_{t-1}\bm{I}\) on both sides, we get

\[\bm{B}_{t}=\rho_{t}\bm{I}+\bm{Z}_{t}\succeq\rho_{t-1}\bm{I}+\bm{Y }_{t}=\rho_{t-1}\bm{I}+\bm{Z}_{t-1}+\bm{X}_{t}=\bm{B}_{t-1}+\bm{X}_{t}.\]

Hence

\[\bm{B}_{t}=\sum_{s=1}^{t}(\bm{B}_{s}-\bm{B}_{s-1})\succeq\sum_{s= 1}^{t}\bm{X}_{s}=\bm{A}_{t}.\]

On the other hand, we have

\[\bm{Z}_{t}\preceq\|\bm{Z}_{t}-\bm{Y}_{t}\|_{2}\bm{I}+\bm{Y}_{t}=( \rho_{t}-\rho_{t-1})\bm{I}+\bm{Y}_{t}.\]

Adding \(\rho_{t}\bm{I}\) on both sides, we get

\[\bm{B}_{t} =\rho_{t}\bm{I}+\bm{Z}_{t}\preceq(2\rho_{t}-\rho_{t-1})\bm{I}+ \bm{Y}_{t}\] \[=2(\rho_{t}-\rho_{t-1})\bm{I}+\rho_{t-1}\bm{I}+\bm{Z}_{t-1}+\bm{X }_{t}\] \[=\bm{B}_{t-1}+2(\rho_{t}-\rho_{t-1})\bm{I}+\bm{X}_{t}.\]

Hence

\[\bm{B}_{t}=\sum_{s=1}^{t}(\bm{B}_{s}-\bm{B}_{s-1})\preceq\sum_{s= 1}^{t}2(\rho_{s}-\rho_{s-1})\bm{I}+\sum_{s=1}^{t}\bm{X}_{s}=2\rho_{t}\bm{I}+ \bm{A}_{t}.\]

The proof is completed. 

**Theorem 1**.: _Assume that the gradients \(\bm{G}_{1},\ldots,\bm{G}_{T}\in\mathbb{R}^{m\times n}\) are matrices of rank at most \(r\). Then for any \(\bm{W}^{*}\in\mathbb{R}^{m\times n}\) and \(\epsilon>0\), if \(\eta=D/\sqrt{2r}\), the regret of Algorithm 6 is bounded as follows,_

\[\sum_{t=1}^{T}f_{t}(\bm{W}_{t})-\sum_{t=1}^{T}f_{t}(\bm{W}^{*}) \leq\sqrt{2r}D[2^{1/4}m\rho_{T}^{1/4}+\mathrm{tr}(\tilde{\bm{L}}_{T}^{1/4})][ 2^{1/4}n\mu_{T}^{1/4}+\mathrm{tr}(\tilde{\bm{R}}_{T}^{1/4})],\]

_where \(D=\max_{t\in[T]}\|\bm{W}_{t}-\bm{W}^{*}\|_{F}\), \(\tilde{\bm{L}}_{t}=\epsilon\bm{I}_{m}+\sum_{t=1}^{T}\bm{G}_{t}\bm{G}_{t}^{\sf T}\), and \(\tilde{\bm{R}}_{t}=\epsilon\bm{I}_{n}+\sum_{t=1}^{T}\bm{G}_{t}^{\sf T}\bm{G}_ {t}\)._

Proof.: Define \(\hat{\bm{L}}_{t}=(\epsilon+\rho_{t})\bm{I}_{m}+\bm{L}_{t},\hat{\bm{R}}_{t}=( \epsilon+\mu_{t})\bm{I}_{n}+\bm{R}_{t}\). According to Lemma 12, \(\hat{\bm{L}}_{t}\) and \(\hat{\bm{R}}_{t}\) are positive definite. Recall the update performed in Algorithm 6,

\[\bm{W}_{t+1}=\bm{W}_{t}-\eta\hat{\bm{L}}_{t}^{-1/4}\bm{G}_{t}\hat{ \bm{R}}_{t}^{-1/4}.\]

For \(t>0\), let \(\bm{H}_{t}=\hat{\bm{L}}_{t}^{1/4}\otimes\hat{\bm{R}}_{t}^{1/4}\), \(\bm{g}_{t}=\overline{\mathrm{vec}}(\bm{G}_{t})\) and \(\bm{w}_{t}=\overline{\mathrm{vec}}(\bm{W}_{t})\). Due to Lemma 3(3) and Lemma 4, we have

\[\bm{w}_{t+1}=\bm{w}_{t}-\eta\bm{H}_{t}^{-1}\bm{g}_{t}.\]

Lemma 12 implies \(0\prec\hat{\bm{L}}_{1}\preceq\cdots\preceq\hat{\bm{L}}_{T},0\prec\hat{\bm{ R}}_{1}\preceq\cdots\preceq\hat{\bm{R}}_{T}\). Thus, according to Lemma 3(3)(4) and Lemma 6, we get

\[0\prec\bm{H}_{1}\preceq\cdots\preceq\bm{H}_{T}.\]

Let \(\bm{H}_{0}=\bm{0}\). By invoking Lemma 9 and Lemma 8, we obtain the regret bound

\[\sum_{t=1}^{T}f_{t}(\bm{W}_{t})-\sum_{t=1}^{T}f_{t}(\bm{W}^{*}) \leq\frac{1}{2\eta}\sum_{t=1}^{T}(\bm{w}_{t}-\bm{w}^{*})^{\sf T} (\bm{H}_{t}-\bm{H}_{t-1})(\bm{w}_{t}-\bm{w}^{*})+\frac{\eta}{2}\sum_{t=1}^{T} (\|\bm{g}_{t}\|_{\bm{H}_{t}}^{*})^{2}\] \[\leq\frac{D^{2}}{2\eta}\sum_{t=1}^{T}\mathrm{tr}(\bm{H}_{t}-\bm{ H}_{t-1})+\frac{\eta}{2}\sum_{t=1}^{T}(\|\bm{g}_{t}\|_{\bm{H}_{t}}^{*})^{2}\] \[=\frac{D^{2}}{2\eta}\mathrm{tr}(\bm{H}_{T})+\frac{\eta}{2}\sum_{t= 1}^{T}(\|\bm{g}_{t}\|_{\bm{H}_{t}}^{*})^{2},\]where \(D=\max_{t\in[T]}\|\bm{w}_{t}-\bm{w}^{*}\|_{2}=\max_{t\in[T]}\|\bm{W}_{t}-\bm{W}^{*} \|_{F}\) and \(\bm{w}^{*}=\overline{\text{vec}}(\bm{W}^{*})\).

Define \(\widehat{\bm{H}}_{t}=(re\bm{I}+\sum_{s=1}^{t}\bm{g}_{s}\bm{g}_{s}^{\mathsf{T}})^ {1/2}\). Lemma 11 and Lemma 12 imply that

\[\widehat{\bm{H}}_{t}\preceq\sqrt{r}\widehat{\bm{L}}_{t}^{1/4}\otimes\tilde{\bm {R}}_{t}^{1/4}\preceq\sqrt{r}\bm{H}_{t}.\]

Using Lemma 7 and Lemma 10 along with the above equation, we obtain

\[\sum_{t=1}^{T}(\|\bm{g}_{t}\|_{\bm{H}_{t}}^{*})^{2}\leq\sqrt{r}\sum_{t=1}^{T}( \|\bm{g}_{t}\|_{\widehat{\bm{H}}_{t}}^{*})^{2}\leq 2\sqrt{r}\text{tr}( \widehat{\bm{H}}_{T})\leq 2r\text{tr}(\bm{H}_{T}).\]

Consequently, using Lemma 3(5) and Lemma 12, we get the desired regret bound

\[\sum_{t=1}^{T}f_{t}(\bm{W}_{t})-\sum_{t=1}^{T}f_{t}(\bm{W}^{*}) \leq\Big{(}\frac{D^{2}}{2\eta}+\eta r\Big{)}\text{tr}(\bm{H}_{T} )=\sqrt{2r}D\text{tr}(\hat{\bm{L}}_{T}^{1/4})\text{tr}(\hat{\bm{R}}_{T}^{1/4})\] \[\leq\sqrt{2r}D[2^{1/4}m\rho_{T}^{1/4}+\text{tr}(\tilde{\bm{L}}_{ T}^{1/4})][2^{1/4}n\mu_{T}^{1/4}+\text{tr}(\tilde{\bm{R}}_{T}^{1/4})],\]

by choosing \(\eta=D/\sqrt{2r}\). The proof is completed. 

## Appendix G Experimental Details

We use one RTX3060Ti GPU under the PyTorch 2.0.1+CUDA11.8 framework for DNN training on the CIFAR-100 and Tiny-ImageNet datasets, use one A800 GPU under the PyTorch 2.0.1+CUDA11.7 framework for DNN training on the ImageNet-1k and C4 datasets, and use two NVIDIA L40S GPUs under the PyTorch 2.0.1+CUDA11.8 framework for DNN training on the OWT dataset. To obtain the total peak memory consumption per GPU, we call "torch.cuda.max_memory_allocated".

We set "torch.backends.cudnn.benchmark" to "False" for all the experiments, except when training ViT-Base/32 on the ImageNet-1k dataset. We report the total memory consumption instead of the memory consumption of the second-order optimizer. This total memory includes data, model parameters, activations, gradients, states forming the preconditioners and their inverse roots, states for the used first-order optimizer, and memory fragments. _Our focus lies in quantizing the states for constructing preconditioners and their inverse roots, which are approximately 7x smaller for 4-bit Shampoo compared to 32-bit Shampoo. Because the block size is 64, its maximum value should be calculated every 64 elements and saved as a 32-bit value, resulting in an additional overhead of 0.5 bits (\(32/64\)). Consequently, the memory savings are approximately 7 times, calculated as \(32/(4+0.5)\)._ In the future, we may adopt double quantization [9] to further reduce memory consumption.

For SGDM, Adagrad or AdamW used in second-order optimizers, we use 32-bit optimizer states on image classification tasks and 16-bit optimizer states on natural language modeling tasks by default. For SGDM, we set the momentum to 0.9 and use an initial learning rate of 0.1. For Adagrad, we set \(\epsilon=10^{-10}\) and use an initial learning rate of 0.01. For AdamW, we set \(\beta_{1}=0.9\), \(\beta_{2}=0.999\), and \(\epsilon=10^{-8}\) and use an initial learning rate of 0.001. For quantization settings, we employ block-wise normalization with a block size of 64 and linear square quantization by default. Matrices with a size smaller than 4096 will not be quantized. For Shampoo and CASPR, we use \(\epsilon=10^{-6},\beta=0.95\) and \(t_{1}=1,t_{2}=4\) by default. Shampoo and CASPR precondition blocks from large matrices and the maximum order of a preconditioner is 10000 for 130M LLAMA-2 and is 1200 for other models. For training loss, we use cross-entropy loss. For image classification tasks, automatic mixed precision is enabled except for training transformers on the CIFAR-100 and Tiny-ImageNet datasets.

**Settings on training CNNs on CIFAR-100 or Tiny-ImageNet.** Minibatch size is set to 128. Weight decay is 0.0005. Data augmentation includes random crop and horizontal flip. For Shampoo, we set \(T_{1}=100\) and \(T_{2}=500\). In Section 5, we run SGDM for 300 epochs and SGDM+Shampoo for 200 epochs on the CIFAR-100 dataset. We run SGDM for 150 epochs and SGDM+Shampoo for 100 epochs on the Tiny-ImageNet dataset. We adopt the multi-step learning rate schedule (the learning rate is multiplied by 0.1 for every 30% epochs with a linear warmup at the first 5 epochs).

**Settings on training transformers on CIFAR-100 or Tiny-ImageNet.** We set a patch size of 4 for ViT-small on the CIFAR-100 dataset, and a patch size of 8 for ViT-small on the Tiny-ImageNet dataset. For training Swin-Tiny on the CIFAR-100 dataset, we use a patch size of 2 and window size of 4. For training Swin-Tiny on the Tiny-ImageNet dataset, we use a patch size of 4 and window size of 7. Minibatch size is set to 128. We run Adagrad/AdamW/NadamW for 150 epochs and Adagrad/AdamW+Shampoo for 100 epochs. Weight decay is 0.0005 for Adagrad, and is 0.05 for AdamW/NadamW. We use the cosine learning rate schedule. Data augmentation follows the source code in [25]. For Shampoo, we set \(T_{1}=100\) and \(T_{2}=500\). With the exception of certain optimizer settings, the configurations used for ablation studies are identical to those outlined above.

**Settings on training ResNet50 on ImageNet-1k.** We run SGDM for 120 epochs and SGDM+Shampoo for 100 epochs. Minibatch size is set to 256. Weight decay is 0.0001. We adopt the multi-step learning rate schedule (the learning rate is multiplied by 0.1 for every 30% epochs with a linear warmup at the first 5 epochs). Data augmentation includes random resized crop, horizontal flip, and color jitter. For Shampoo, we set \(T_{1}=200\) and \(T_{2}=1000\).

**Settings on training ViT-Base/32 on ImageNet-1k.** We run AdamW for 150 epochs and AdamW+Shampoo for 120 epochs. Minibatch size is set to 512. Weight decay is 0.05. We use the cosine learning rate schedule. Data augmentation follows the configuration for training ViT-Base/16 in [44], excluding repeated augmentation. For Shampoo, we set \(T_{1}=200\) and \(T_{2}=1000\).

**Settings on training GPT-2 on OWT.** We run AdamW with 10% warmup steps. Total batch size is set to 480. Batch size is set to 24 for training 124M GPT-2. Dtype is bfloat16. Weight decay is 0.1. For Shampoo, we set \(T_{1}=200\) and \(T_{2}=200\). For our 4-bit Shampoo, we use Schur-Newton iteration used in Algorithm 4 to compute the inverse root of a preconditioner for training stability.

**Settings on training LLAMA-2 on C4.** We run AdamW with 10% warmup steps. Total batch size is set to 512. Batch size is set to 256 for training 130M LLAMA-2 and is set to 128 for training 350M LLAMA-2. Dtype is bfloat16. Weight decay is 0. For Shampoo, we set \(T_{1}=200\) and \(T_{2}=200\).

**Settings on K-FAC and AdaBK.** K-FAC/AdaBK preconditions layers without limiting the size of a preconditioner. We set \(\beta=0.9\), \(T_{1}=200\), and \(T_{2}=2000\). We use \(\epsilon=0.1\) for K-FAC and \(\epsilon=0.001\) for AdaBK. For 4-bit K-FAC/AdaBK, we set \(t_{1}=0\) and \(t_{2}=0\) (i.e., no orthogonal rectification).

**Settings on schedule free optimization.** We use the code from [6] to train ResNet34 with SGDScheduleFree and Swin-Tiny with AdamWScheduleFree. For SGDScheduleFree, we set lr=1.0, weight_decay=0.0005 and warmup_steps=2000. For AdamWScheduleFree, we set lr=0.0025, weight_decay=0.05 and warmup_steps=10000.

**Settings on M-FAC.** We use the code from [15] and set ngrads=32, damp=0.1. The other hyperparameter settings of M-FAC is the same as that of SGDM used for ResNet34 training.

## Appendix H Additional Results

### Image Classification

**More learning rate schedulers.** Table 8 shows the performance and wall-clock time of training ResNet34 on CIFAR-100 with cosine learning rate decay. By comparison, SGDM+Shampoo still converges faster than SGDM, and have slightly better test performance.

We also provide the results of training ResNet34 and Swin-Tiny on CIFAR-100 with schedule-free approach [6] in Table 9. From it one can see that AdamWScheduleFree achieves comparable performance to AdamW with cosine decay, while SGDScheduleFree underperforms compared to SGDM. We observe that this schedule-free algorithm shows rapid improvements in training and test accuracy during the early training stages, but may fail to achieve a higher test accuracy ultimately (see Figure 9). Anyway, these methods are still worse than our AdamW+4-bit Shampoo.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Epochs & Optimizer & TA (\%) & WCT (min) \\ \hline
200 & SGDM & 79.67 & 116.0 \\
300 & SGDM & 79.83 & 172.7 \\
200 & SGDM + 32-bit Shampoo & 80.39 & 152.7 \\
200 & SGDM + 4-bit Shampoo (our) & 80.22 & 161.7 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Performance and wall-clock time of training ResNet34 on the CIFAR-100 dataset with cosine learning rate decay. TA = test accuracy, and WCT = wall-clock time.

More optimizers.Table 10 shows results of training Swin-Tiny on CIFAR-100 with NadamW, Adagrad and Adagrad+Shampoo. One can see that Adagrad+4-bit Shampoo converges faster than Adagrad with ignorable extra memory overhead, and also has higher test accuracy. Besides, though NadamW [11] is slightly better than AdamW, it is still worse than our AdamW+4-bit Shampoo.

M-FAC [15] is a matrix-free method computing inverse-Hessian vector products with many gradient copies. It is not memory-efficient for M-FAC to maintain \(m\) dense gradient copies (\(m=1024\) in its official code). Table 11 shows that both SGDM+32-bit Shampoo and SGDM+4-bit Shampoo enjoy much higher efficiency than M-FAC (\(m=32\)) for training ResNet34 on CIFAR-100, and enjoy higher test accuracy. EVA [42] is a rank-one second-order optimizer and is memory-efficient. We train ResNet34 on CIFAR-100 with SGDM+EVA, but despite extensive hyper-parameter tuning, we fail to achieve acceleration over SGDM. Instead, we cite EVA's result of training VGG-19 on CIFAR-100 for 200 epochs (see Table 2 in [42]). The test accuracies of SGDM+EVA and SGDM+Shampoo are 73% and 74.5%, respectively.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Optimizer & TA (\%) & WCT (min) & TMC (MB) \\ \hline NadamW & 77.11 & 342.4 & 1465.8 \\ AdamW + 32-bit Shampoo & 79.34 & 260.8 & 2036.0 \\ AdamW + 4-bit Shampoo (our) & 78.63 & 273.3 & 1543.9 \\ \hline Adagrad & 66.56 & 294.6 & 1354.9 \\ Adagrad + 32-bit Shampoo & 73.55 & 245.3 & 1930.4 \\ Adagrad + 4-bit Shampoo (our) & 72.66 & 259.6 & 1433.0 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Performance, wall-clock time, and memory cost of training Swin-Tiny on the CIFAR-100 dataset. TA = test accuracy, WCT = wall-clock time, and TMC = total GPU memory cost.

Figure 9: Visualization of test accuracies on the CIFAR-100 dataset with cosine learning rate decay and schedule-free approach.

\begin{table}
\begin{tabular}{c|c c c} \hline \hline Model & Optimizer & TA (\%) & WCT (min) \\ \hline \multirow{2}{*}{ResNet34} & SGDM & 79.83 & 172.7 \\  & SGDSCheduleFree & 75.63 & 169.6 \\ \hline \multirow{2}{*}{Swin-Tiny} & AdamW & 76.69 & 318.6 \\  & AdamWScheduleFree & 76.58 & 321.9 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Performance and wall-clock time of training on the CIFAR-100 dataset with cosine learning rate decay and schedule-free approach. ResNet34 is trained for 300 epochs and Swin-Tiny is trained for 150 epochs. TA = test accuracy, and WCT = wall-clock time.

### Natural Language Modeling

**Models, datasets, and hyperparameters.** We train 124M GPT-2 [32] for 60k steps on the OpenWebText (OWT) dataset * following the nanoGPT codebase + with two NVIDIA L40S GPUs, and train 130M LLAMA-2 [37] for 20k steps and 350M LLAMA-2 for 60k steps on the C4 dataset [33] following [43] with one A800 GPU. See Appendix G for experimental details.

Footnote *: http://Skylion007.github.io/OpenWebTextCorpus.

**Main results.** We show the performance, wall-clock time, and memory cost in Table 12, and the validation loss curves in Figure 10. As with the vision tasks, our AdamW+4-bit Shampoo consistently outperformed AdamW and naive AdamW+4-bit Shampoo in terms of performance, and AdamW+32-bit Shampoo in terms of memory usage.

**Memory efficiency.** We further check the memory usage by increasing token batch size for a language model, which is calculated as the batch size multiplied by the context length (see [43]). To train LLAMA2-7B on the C4 dataset using a single A800 GPU (with a maximum memory of 81,920

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Dataset & Model & Optimizer & VL & WCT (min) & TMC (MB) \\ \hline \multirow{6}{*}{C4} & \multirow{6}{*}{LLAMA-130M} & AdamW & 3.214 & 346.9 & 47026 \\  & & AdamW + 32-bit Shampoo & 3.184 & 353.7 & 48813 \\  & & AdamW + 4-bit Shampoo (naive) & 3.200 & 353.5 & 47316 \\  & & AdamW + 4-bit Shampoo (our) & 3.194 & 353.1 & 47318 \\ \cline{2-5}  & \multirow{6}{*}{LLAMA-350M} & AdamW & 2.939 & 2687 & 54184 \\  & & AdamW + 32-bit Shampoo & 2.908 & 2776 & 59149 \\  & & AdamW + 4-bit Shampoo (naive) & 2.930 & 2753 & 54894 \\ \cline{2-5}  & & AdamW + 4-bit Shampoo (our) & 2.924 & 2795 & 54894 \\ \hline \multirow{6}{*}{OWT} & \multirow{6}{*}{GPT2-124M} & AdamW & 2.954 & 2310 & 27010 \\  & & AdamW + 32-bit Shampoo & 2.936 & 2330 & 28490 \\ \cline{1-1}  & & AdamW + 4-bit Shampoo (naive) & 2.953 & 2359 & 27209 \\ \cline{1-1}  & & AdamW + 4-bit Shampoo (our) & 2.944 & 2311 & 27209 \\ \hline \hline \end{tabular}
\end{table}
Table 12: Performance, wall-clock time, and memory usage per GPU on natural language modeling tasks. VL = validation loss, WCT = wall-clock time, and TMC = total GPU memory cost.

Figure 10: Visualization of validation loss on the C4 and OWT datasets.

MB), we set the context length to 256 and then determine the maximum batch size allowed by each optimizer. For Shampoo, the maximum order of a preconditioner for training LLAMA2-7B is 2048. In all experiments, gradient checkpointing is enabled. Table 13 summarizes the evaluation results. By comparison, the 32-bit Shampoo runs out of memory with a batch size of 2, while our 4-bit Shampoo supports a batch size of 64 for standard training and only encounters memory issues at a batch size of 128. These results clearly demonstrate that our 4-bit Shampoo significantly conserves memory compared to the 32-bit version.

\begin{table}
\begin{tabular}{c c c} \hline \hline Optimizer & Batch Size & TMC (MB) \\ \hline
8-bit AdamW & 64 & 60135 \\
8-bit AdamW & 128 & 68689 \\
8-bit AdamW & 256 & OOM \\
8-bit AdamW + 32-bit Shampoo & 2 & OOM \\
8-bit AdamW + 4-bit Shampoo (our) & 64 & 74561 \\
8-bit AdamW + 4-bit Shampoo (our) & 128 & OOM \\ \hline \hline \end{tabular}
\end{table}
Table 13: Memory cost of training LLAMA2-7B on the C4 dataset with different optimizers. One A800 GPU with a maximum memory of 81,920 MB is enabled. TMC = total GPU memory cost, and OOM = out of memory.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We propose the first second-order optimizers with 4-bit states by taking Shampoo as an example, while preserving the performance achieved with 32-bit optimizer states. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of the work at the end of the paper. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We provide all the proofs in the Appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems.

* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We present the implementation details of all the experiments in the Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We use publicly available datasets and will release our source code. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).

* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We present the implementation details of all the experiments in the Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Error bars are not reported because it would be too computationally expensive. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We present the implementation details of all the experiments in the main paper and Appendix. Guidelines: * The answer NA means that the paper does not include experiments.

* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We review the NeurIPS Code of Ethics and our paper conforms it. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss both potential positive societal impacts and negative societal impacts at the end of the paper. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We present 4-bit Shampoo for memory efficient training of deep models. It poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks.

* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We properly mention all the existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide anonymized zip file of our code. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.