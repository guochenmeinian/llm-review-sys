# Deep linear networks for regression are implicitly regularized towards flat minima

 Pierre Marion

Institute of Mathematics

EPFL

Station Z, CH-1015 Lausanne, Switzerland

pierre.marion@epfl.ch &Lenaic Chizat

Institute of Mathematics

EPFL

Station Z, CH-1015 Lausanne, Switzerland

lenaic.chizat@epfl.ch

###### Abstract

The largest eigenvalue of the Hessian, or sharpness, of neural networks is a key quantity to understand their optimization dynamics. In this paper, we study the sharpness of deep linear networks for univariate regression. Minimizers can have arbitrarily large sharpness, but not an arbitrarily small one. Indeed, we show a lower bound on the sharpness of minimizers, which grows linearly with depth. We then study the properties of the minimizer found by gradient flow, which is the limit of gradient descent with vanishing learning rate. We show an _implicit regularization towards flat minima_: the sharpness of the minimizer is no more than a constant times the lower bound. The constant depends on the condition number of the data covariance matrix, but not on width or depth. This result is proven both for a _small-scale initialization_ and a _residual initialization_. Results of independent interest are shown in both cases. For small-scale initialization, we show that the learned weight matrices are approximately rank-one and that their singular vectors align. For residual initialization, convergence of the gradient flow for a Gaussian initialization of the residual network is proven. Numerical experiments illustrate our results and connect them to gradient descent with non-vanishing learning rate.

## 1 Introduction

Neural networks have intricate optimization dynamics due to the non-convexity of their objective. A key quantity to understand these dynamics is the largest eigenvalue of the Hessian or _sharpness_\(S(\mathcal{W})\) (see Section 3 for a formal definition), in particular because of its connection with the choice of learning rate \(\eta\). Classical theory from convex optimization indicates that the sharpness should remain lower than \(2/\eta\) to avoid divergence (Nesterov, 2018). The relevance of this point of view for deep learning has recently been questioned since neural networks have been shown to often operate at the _edge of stability_(Cohen et al., 2021), where the sharpness oscillates around \(2/\eta\), while the loss still steadily decreases, albeit non-monotonically. Damian et al. (2023) explained the stability of gradient descent slightly above the \(2/\eta\) threshold to be a general phenomenon for non-quadratic objectives, where the third-order derivatives of the loss induce a self-stabilization effect. They also show that, under appropriate assumptions, gradient descent on a risk \(R^{L}\) implicitly solves the constrained minimization problem

\[\min_{\mathcal{W}}R^{L}(\mathcal{W})\quad\text{such that}\quad S(\mathcal{W}) \leqslant\frac{2}{\eta}\,.\] (1)

Trainability of neural networks initialized with a sharpness larger than \(2/\eta\) is also studied by Lewkowycz et al. (2020), which describes a transient catapult regime, which lasts until the sharpness goes below \(2/\eta\). These results beg the question of quantifying the largest learning rate that enables successful training of neural networks. For classification with linearly separable data and logistic loss,Wu et al. (2024) show that gradient descent converges for any learning rate. In this work, we address the case of deep linear networks for regression. As illustrated by Figure 0(a), the picture then differs from the classification case: **when the learning rate exceeds some critical value, the network fails to learn**. We further remark that this critical value does not seem to be related to the initial scale: when the learning rate is under the critical value, learning is successful for a wide range of initial scales. In Figure 0(b), we see that the large initialization scales correspond to initial sharpnesses well over the \(2/\eta\) threshold, confirming that training is possible while initializing beyond the \(2/\eta\) threshold.

To understand where the critical value for the learning rate comes from, **we characterize the sharpness of minimizers** of the empirical risk. We show that there exist minimizers with arbitrarily large sharpness, but _not_ with arbitrarily small sharpness. Indeed, the sharpness of any minimizer grows linearly with depth, as made precise next.

**Theorem 1**.: _Let \(X\in\mathbb{R}^{n\times d}\) be a design matrix and \(y\in\mathbb{R}^{n}\) a target. Then the minimal sharpness \(S_{\min}\) of any linear network \(x\mapsto W_{L}\ldots W_{1}x\) of depth \(L\) that implements the optimal linear regressor \(w^{\star}\in\mathbb{R}^{d}\) satisfies_

\[2\|w^{\star}\|_{2}^{2-\frac{2}{L}}La\leqslant S_{\min}\leqslant 2\|w^{ \star}\|_{2}^{2-\frac{2}{L}}\sqrt{(2L-1)\Lambda^{2}+(L-1)^{2}a^{2}}\,,\]

_where \(\Lambda\) is the largest eigenvalue of the empirical covariance matrix \(\hat{\Sigma}:=\frac{1}{n}X^{\top}X\), and \(a:=(w^{\star}/\|w^{\star}\|)^{\top}\hat{\Sigma}(w^{\star}/\|w^{\star}\|)\)._

We note that this bound is similar to a result of Mulayoff and Michaeli (2020), although we alleviate their assumption of data whiteness (\(\hat{\Sigma}=I\)). In particular, we do not assume that data covariance

Figure 1: Training a deep linear network on a univariate regression task with quadratic loss. The weight matrices are initialized as Gaussian random variables, whose standard deviation is the x-axis of plots 0(a) and 0(b). Experimental details are given in Appendix C.

matrix is full rank. Furthermore, our proof technique differs, since we do not require tensor algebra, and instead exhibit a direction in which the second derivative of the loss is large.

This result shows that it is not possible to find a minimizer of the empirical risk in regions of low sharpness. Combined with (1), this suggests an interpretation of the critical value for the learning rate: gradient descent should not be able to converge to a global minimizer as soon as

\[S_{\min}>\frac{2}{\eta}\quad\Leftrightarrow\quad\eta>\frac{2}{S_{\min}}\simeq( \left\|w^{\star}\right\|_{2}^{2-\frac{2}{L}}La)^{-1}\,.\]

This is confirmed experimentally by Figure 2, which shows that the critical value of the learning rate matches our theoretical prediction. We note that this gives a quantitative answer to the observations of Lewkowycz et al. (2020), which shows the existence of a maximal architecture-dependent learning rate beyond which training fails. The dependence of learning rate on depth (namely, constant over depth) also matches other papers that study scaling of neural networks (Chizat and Netrapalli, 2024; Yang et al., 2024).

To deepen our understanding of the training dynamics, we aim at quantifying the sharpness of the minimizer found by gradient descent (when it succeeds): we know that it has to be larger than \(S_{\min}\), but is it close to \(S_{\min}\) or is it much larger? Inspecting Figure 0(b), we see that the answer empirically depends on the interplay between initialization scale and learning rate. For small initialization scale, the sharpness after training is equal to a value relatively close to \(S_{\min}\) and independent of the learning rate. As the initialization scale increases, the sharpness of the trained network also increases, and plateaus at the value \(2/\eta\). The plateauing for large initialization scales can be explained by the edge of stability analysis (1), which upper bounds the sharpness of the minimizer by \(2/\eta\) (see Figure 0(d)). However, this gives no insight on the value of the sharpness when the learning rate is sufficiently small so that the network does not enter the edge of stability regime (see Figure 0(c)).

In this paper, we study the limiting case for vanishing \(\eta\), i.e., training with gradient flow. **As our main finding, we bound the sharpness of the minimizer found by gradient flow** in the case of overdetermined regression, meaning that the sample size \(n\) is larger that the data dimension \(d\) and the data empirical covariance matrix is nonsingular. In this case, we prove that the ratio between the sharpness after training and \(S_{\min}\) is less than a constant depending mainly on the condition number of the empirical covariance matrix \(\hat{\Sigma}\). In particular, the ratio does not depend on the width or depth of the network. This shows an **implicit regularization towards flat minima**. Note that the phenomenon we exhibit is different from the well-studied implicit regularization towards flat minima caused by stochasticity in SGD (Keskar et al., 2017; Smith and Le, 2018; Blanc et al., 2020; Damian et al., 2021; Li et al., 2022; Liu et al., 2023). In the present study, the dynamics are purely deterministic, and the low sharpness is due to the fact that the weight matrices found by gradient flow have (approximately) the same norm across layers, and that this norm is (approximately) the smallest possible one so that the network can minimize the risk.

Figure 2: Squared distance of the trained network to the empirical risk minimizer, for various learning rates and depth. For each depth, learning succeeds if the learning rate is below a threshold, which corresponds to the theoretical value \(\frac{2}{S_{\min}}\simeq(\left\|w^{\star}\right\|_{2}^{2-\frac{2}{L}}La)^{-1}\) of Theorem 1 (dashed vertical line).

Link with generalization.Flatter minima have been found to generalize better (Hochreiter and Schmidhuber, 1997; Jastrzebski et al., 2017; Keskar et al., 2017; Jiang et al., 2020), although the picture is subtle (Dinh et al., 2017; Neyshabur et al., 2017; Andriushchenko et al., 2023). However, in this paper, we focus on the link of sharpness with (non-convex) optimization dynamics, rather than with generalization abilities. Indeed, our implicit regularization result holds in the overdetermined setting, where all minimizers of the empirical risk implement the same function thus have the same generalization error, although they differ in parameter space and in particular have different sharpnesses. We leave to future work extensions to more complex settings where our approach may link sharpness and generalization, beginning with deep linear networks for underdetermined regression. We refer to Appendix C for more comments on the link with generalization.

We investigate two initialization schemes, quite different in nature: small-scale initialization and residual initialization. Let us explain both settings, by presenting our approach, contributions of independent interest, and related works.

Small-scale initialization.In this setting, we consider an initialization of the weight matrices \(W_{k}\) with i.i.d. Gaussian entries of small variance. Initialization scale is known to play a key role in training of neural networks: small-scale initialization corresponds to the "feature learning" regime where the weights change significantly during training, by opposition to the "lazy" regime (see, e.g., Chizat et al., 2019). We show convergence of the empirical risk to zero, then **characterize the structure of the minimizer found by gradient flow**, a novel result of interest independently of its connection with sharpness. At convergence, **the weight matrices are close to being rank-one**, in the sense that all their singular values but the largest one are small. Furthermore, the first left singular vector of any weight matrix aligns with the first right singular vector of the next weight matrix. From this specific structure, we deduce our bound on the sharpness of the trained network. The bound is illustrated on Figure 0(b), where our lower and upper theoretical bounds on the sharpness are plotted as dotted black lines. We observe that the sharpness after training, when starting from a small-scale initialization, is indeed situated between the black lines.

The result and proof extend the study by Ji and Telgarsky (2020) for classification, although the parameters do not diverge to infinity contrarily to the classification case, thus requiring a finer control of the distance to the rank-one aligned solution. In regression, implicit regularization towards low-rank structure in parameter space was also studied by Saxe et al. (2014); Lampinen and Ganguli (2019); Gidel et al. (2019); Saxe et al. (2019); Varre et al. (2023) for two-layer neural networks and in Timor et al. (2023) for deep ReLU networks. This latter paper assumes convergence of the optimization algorithm and show that a solution with minimal \(\ell_{2}\)-norm has to be low-rank. In our linear setting, we instead show convergence. As detailed below, we impose mild requirements on the structure on the initialization beyond its scale; they are satisfied for instance by initializing one weight matrix to zero and the others with i.i.d. Gaussian entries. In particular, we do not require the so-called "zero-balanced initialization" as is common in the literature on deep linear networks (see, e.g., Arora et al., 2018; Advani et al., 2020; Li et al., 2021) or a deficient-margin initialization as in Arora et al. (2019). Finally, the limit when initialization scale tends to zero has been described for deep linear networks in Jacot et al. (2021) for multivariate regression. It consists in a saddle-to-saddle dynamics, where the rank of the weight matrices increases after each saddle. The present study considers instead a non-asymptotic setting where the initialization scale is small but nonzero, and shows convergence to a rank-one limit because univariate and not multivariate regression is considered.

We note that sharpness at initialization can be made arbitrarily small, since it is controlled by the initialization scale, while sharpness after training scales as \(\Theta(L)\). This therefore showcases an **example of sharpening during training** (although we make no statement on monotonicity).

Residual initialization.Architectures of deep neural networks used in practice often present residual connections, which stabilize training (He et al., 2015). A simple non-linear residual architecture writes \(h_{k+1}=h_{k}+\sigma(N_{k}h_{k})\). Removing the non-linearity, we get \(h_{k+1}=(I+N_{k})h_{k}\), which prompts us to consider deep linear networks with square weight matrices \(W_{k}\in\mathbb{R}^{d\times d}\) that are initialized as

\[W_{k}(0)=I+\frac{s}{\sqrt{Ld}}N_{k}\,,\] (2)

where the \(N_{k}\in\mathbb{R}^{d\times d}\) are filled with i.i.d. standard Gaussian entries and \(s\geqslant 0\) is a hyperparameter tuning the initialization scale. The scaling of the residual branch in \(1/\sqrt{d}\) is common and correspondsfor instance to the so-called Glorot and He initializations respectively from Glorot and Bengio (2010) and He et al. (2015). It ensures that the variance of the residual branch is independent of the width \(d\). Similarly, as studied in Arpit et al. (2019); Marion et al. (2022), the scaling in \(1/\sqrt{L}\) is the right one so that the initialization noise neither blows up nor decays when \(L\to\infty\). Note that, in practice, this scaling factor is often replaced by batch normalization (Ioffe and Szegedy, 2015), which has been shown empirically to have a similar effect to \(1/\sqrt{L}\) scaling (De and Smith, 2020).

In this setting which we refer to as _residual initialization_, we show global convergence of the empirical risk. To our knowledge, **it is the first time that convergence is proven for a standard Gaussian initialization of the residual network outside the large width \(d\geq n\) regime**. Previous works considered either an identity initialization (Bartlett et al., 2018; Arora et al., 2019; Zou et al., 2020) or a smooth initialization such that \(\|W_{k+1}(0)-W_{k}(0)\|_{F}=\mathcal{O}(1/L)\)(Sander et al., 2022; Marion et al., 2024). The extension to standard Gaussian initialization leverages sharp bounds for the singular values of the product of \(W_{k}(0)\). Our main assumption, in alignment with the literature, is that the risk at initialization should not be larger than a constant (depending on \(\hat{\Sigma}\) and \(s\)).

We then show that the weights after training can be written

\[W_{k}(\infty)=I+\frac{s}{\sqrt{Ld}}N_{k}+\frac{1}{L}\theta_{k}\,,\]

where the Frobenius norm of the \(\theta_{k}\) is bounded by a constant (depending only on \(s\)). This structure finally enables us to bound the sharpness of the trained network. Remark that, to connect this analysis with our discussion of sharpness in univariate regression, we add to the residual network a final fixed projection vector \(p\in\mathbb{R}^{d}\), so that our neural network writes \(x\mapsto p^{\top}W_{L}\dots W_{1}x\), but the proof of convergence also holds without this projection.

Experimentally, we give in Appendix C plots in the residual case that are qualitatively similar to Figure 1. The main difference is that the initial sharpness is less sensitive to the initialization scale \(s\).

Organization of the paper.Section 2 details our setting and notations. Section 3 studies the sharpness of minimizers of the empirical risk and proves Theorem 1. Dynamics of gradient flow starting from small-scale initialization and residual initialization are respectively presented in Sections 4 and 5. The Appendix contains proofs, additional plots, experimental details, and related works.

## 2 Setting

Model.We consider linear networks of depth \(L\) from \(\mathbb{R}^{d}\) to \(\mathbb{R}\), which are linear maps

\[x\mapsto p^{\top}W_{L}\dots W_{1}x\] (3)

parameterized by weight matrices \(W_{1},\dots,W_{L}\), where \(W_{k}\in\mathbb{R}^{d_{k}\times d_{k-1}}\), \(d_{0}=d\) and \(p\in\mathbb{R}^{d_{L}}\) is a fixed vector. This definition includes both fully-connected networks by setting \(d_{L}=1\) and \(p=1\), and residual networks by setting \(d_{1}=\dots=d_{L}=d\), the \(W_{k}\) close to the identity, and \(p\) to some fixed (potentially random) vector in \(\mathbb{R}^{d}\). We let \(\mathcal{W}=(W_{1},\dots,W_{L})\) and \(w_{\text{prod}}=W_{1}^{\top}\dots W_{L}^{\top}p\). Given \(X\in\mathbb{R}^{n\times d}\) a design matrix and \(y\in\mathbb{R}^{n}\) a target, we consider the empirical risk for regression

\[R^{L}(\mathcal{W}):=\frac{1}{n}\|y-XW_{1}^{\top}\dots W_{L}^{\top}p\|_{2}^{2}= \frac{1}{n}\|y-Xw_{\text{prod}}\|_{2}^{2}=:R^{1}(w_{\text{prod}})\,.\]

The notations \(R^{L}(\mathcal{W})\) and \(R^{1}(w_{\text{prod}})\) may seem redundant, but are actually practical to define gradients of the risk both with respect to a single matrix \(W_{k}\) and to the product \(w_{\text{prod}}\). Let \(\hat{\Sigma}:=\frac{1}{n}X^{\top}X\) the empirical covariance matrix, and \(\lambda\) and \(\Lambda\) respectively its smallest _nonzero_ and largest eigenvalue. For now, we do not assume that \(\hat{\Sigma}\) is full rank, so there is more than one solution to the regression problem \(\min_{w_{\text{prod}}\in\mathbb{R}^{d}}R^{1}(w_{\text{prod}})\). We denote by \(w^{\star}\in\mathbb{R}^{d}\) the smallest norm solution, and we let \(R_{\min}=R^{1}(w^{\star})\) the minimum of \(R^{1}\) (and \(R^{L}\)). In all the following, we assume that \(w^{\star}\neq 0\). Note that, due to the overparameterization induced by the neural network, there exists an infinity of parameterizations of the mapping \(x\mapsto w^{\star\top}x\).

Gradient flow.We consider that the neural network is trained by gradient flow on the empirical risk \(R^{L}\), that is, the parameters evolve according to the ordinary differential equation

\[\frac{dW_{k}}{dt}=-\frac{\partial R^{L}}{\partial W_{k}}\,.\] (4)An application of the chain rule gives

\[\nabla_{k}R^{L}(\mathcal{W}):=\frac{\partial R^{L}}{\partial W_{k}}=W_{k+1}^{ \top}\dots W_{L}^{\top}p\nabla R^{1}(w_{\text{prod}})^{\top}W_{1}^{\top}\dots W _{k-1}^{\top}\,,\] (5)

with

\[\nabla R^{1}(w_{\text{prod}})=-\frac{2}{n}X^{\top}(y-Xw_{\text{prod}})=-\frac{ 2}{n}X^{\top}X(w^{\star}-w_{\text{prod}})\,.\] (6)

where the second equality is a consequence of \(\nabla R^{1}(w^{\star})=0\).

Notations.For \(k\in\{1,\dots L\}\), we denote respectively by \(\sigma_{k}\), \(u_{k}\), and \(v_{k}\) the first singular value (which equals the \(\ell_{2}\) operator norm), the first left singular vector and the first right singular vector of \(W_{k}\). The \(\ell_{2}\) operator norm of a matrix \(M\) is denoted by \(\|M\|_{2}\) and its Frobenius norm by \(\|M\|_{F}\). Its smallest singular value is denoted by \(\sigma_{\min}(M)\). For a vector \(v\), we let \(\|v\|_{2}\) its Euclidean norm. Finally, for quantities that depend on the gradient flow time \(t\), we omit for concision their explicit dependence on \(t\) when it is dispensable.

## 3 Estimates of the minimal sharpness of minimizers

To define the sharpness of the model, we let \(D=\sum_{k=1}^{L}d_{k}d_{k-1}\) and identify the space of parameters with \(\mathbb{R}^{D}\), which amounts to stacking all the entries of the weight matrices in a large \(D\)-dimensional vector. Then the norm of the parameters seen as a \(D\)-dimensional vector can be related to the Frobenius norm of the matrices by

\[\|\mathcal{W}\|_{2}^{2}=\sum_{k=1}^{L}\|W_{k}\|_{F}^{2}\,.\]

This allows us to define the Hessian of the risk \(H:\mathbb{R}^{D}\to\mathbb{R}^{D\times D}\), and we denote by \(S(\mathcal{W})\) its largest eigenvalue for some parameters \(\mathcal{W}\), or sharpness. We note that there exists alternative definitions of the sharpness, but this one is the most relevant to study optimization dynamics. Our results are specific to this definition. The following result gives estimates on the minimal sharpness of minimizers of the empirical risk (and is a strictly stronger statement than Theorem 1).

**Theorem 2**.: _Let \(S_{\min}=\inf_{\mathcal{W}\in\operatorname{arg\,min}R^{L}(\mathcal{W})}S( \mathcal{W})\) and \(a:=(w^{\star}/\|w^{\star}\|)^{\top}\hat{\Sigma}(w^{\star}/\|w^{\star}\|)\). We have_

\[S_{\min}\geqslant 2a\|w^{\star}\|_{2}^{2-\frac{1}{L}}\|p\|^{\frac{L}{L}}\sum_{ k=1}^{L}\frac{1}{\|W_{k}\|_{F}}\,,\]

_and_

\[2\|w^{\star}\|_{2}^{2-\frac{2}{L}}\|p\|^{\frac{2}{L}}La\leqslant S_{\min} \leqslant 2\|w^{\star}\|_{2}^{2-\frac{2}{L}}\|p\|^{\frac{2}{L}}\sqrt{(2L-1) \Lambda^{2}+(L-1)^{2}a^{2}}\,.\]

The proof of the result relies on the following variational characterization of the sharpness as the direction of the highest change of the gradient

\[S(\mathcal{W})^{2}=\lim_{\xi\to 0}\sup_{\|W_{k}-\tilde{W}_{k}\|_{F}\leqslant \xi}\frac{\sum_{k=1}^{L}\|\nabla_{k}R^{L}(\mathcal{W})-\nabla_{k}R^{L}(\hat{ \mathcal{W}})\|_{F}^{2}}{\sum_{k=1}^{L}\|W_{k}-\tilde{W}_{k}\|_{F}^{2}}\,.\] (7)

Lower bounds are proven by considering well-chosen directions \(\tilde{W}_{k}\), for instance \(\tilde{W}_{k}=(1+\xi\beta_{k})W_{k}\) for the first lower bound. The upper bound is proven by constructing a specific minimizer and bounding its sharpness. The first lower bound shows that the sharpness of minimizers can be arbitrarily high if one of the matrices has a low-enough norm. More precisely, take any minimizer \(\mathcal{W}=(W_{1},\dots W_{L})\) and consider \(\mathcal{W}^{C}=(CW_{1},W_{2}/C,W_{3},\dots,W_{L})\), for some \(C>0\). Then \(\mathcal{W}^{C}\) is still a minimizer, and

\[S(\mathcal{W}^{C})\geqslant\frac{2a\|w^{\star}\|_{2}^{2-\frac{1}{L}}\|p\|^{ \frac{L}{L}}}{\|W_{2}/C\|_{F}}=\frac{2a\|w^{\star}\|_{2}^{2-\frac{1}{L}}\|p\|^{ \frac{L}{L}}C}{\|W_{2}\|_{F}}\xrightarrow{C\to\infty}\infty.\]

The fact that a reparameterization of the network can lead to arbitrarily high sharpness is consistent with a similar result by Dinh et al. (2017) for two-layer ReLU networks.

Note that the first lower bound is arbitrarily small for minimizers such that the norms \(\|W_{k}\|_{F}\) are large. On the contrary, the second lower bound is uniform and asymptotically matches the upper bound when \(L\to\infty\): we have \(S_{\min}\sim 2\|w^{*}\|_{2}^{2}La\).

As already noted by Mulayoff and Michaeli (2020), the intuition behind the linear scaling of the bound with depth can be seen from a one-dimensional example: take \(f(x_{1},\ldots,x_{L})=\prod_{k=1}^{L}x_{i}\). Then an easy computation shows that the sharpness of \(f\) at \((1,\ldots,1)\) is equal to \(L-1\). This showcases a simple example where the output of \(f\) is constant with \(L\) while its sharpness grows linearly with \(L\).

## 4 Analysis of gradient flow from small initialization

In this section, we characterize the structure of the minimizer found by gradient flow starting from a small-scale initialization. The proof is inspired by the one of Ji and Telgarsky (2020) for linearly-separable classification, with a finer analysis due to the finiteness of minimizers in our setting.

We consider the model (3) with \(d_{L}=1\) and \(p=1\). Denoting by \(R_{0}\) the empirical risk when the weight matrices are equal to zero, we can state our assumption on the initialization.

\((A_{1})\): The initialization satisfies that \(R^{L}(\mathcal{W}(0))\leqslant R_{0}\) and \(\nabla R^{L}(\mathcal{W}(0))\neq 0\).

It is satisfied for instance if one of the weight matrices \(W_{k}\) is equal to zero at initialization while the others have i.i.d. Gaussian entries, so that \(R^{L}(\mathcal{W}(0))=R_{0}\) and \(\nabla_{k}R^{L}(\mathcal{W}(0))\neq 0\) (almost surely).

Linear networks trained by gradient flow possess the following remarkable property (Arora et al., 2018) that shall be useful in the remainder.

**Lemma 1**.: _For any time \(t\geqslant 0\) and any \(k\in\{1,\ldots,L-1\}\),_

\[W_{k+1}^{\top}(t)W_{k+1}(t)-W_{k+1}^{\top}(0)W_{k+1}(0)=W_{k}(t)W_{k}^{\top}(t )-W_{k}(0)W_{k}^{\top}(0)\,.\]

Define now

\[\varepsilon:=3\max_{1\leqslant k\leqslant L}\|W_{k}(0)\|_{F}^{2}+2\sum_{k=1}^ {L-1}\|W_{k}(0)W_{k}^{\top}(0)-W_{k+1}^{\top}(0)W_{k+1}(0)\|_{2}\,.\]

Note that \(\varepsilon\) only depends on the initialization, and can be made arbitrarily small by scaling down the initialization. The following key lemma connects throughout training three key quantities to \(\varepsilon\).

**Lemma 2**.: _The parameters following gradient flow satisfy for any \(t\geqslant 0\) that_

* _for_ \(k\in\{1,\ldots,L\}\)_,_ \[\|W_{k}(t)\|_{F}^{2}-\|W_{k}(t)\|_{2}^{2}\leqslant\varepsilon\,,\]
* _for_ \(j,k\in\{1,\ldots,L\}\)_,_ \[|\sigma_{k}^{2}(t)-\sigma_{j}^{2}(t)|\leqslant\varepsilon\,,\]
* _for_ \(k\in\{1,\ldots,L-1\}\)_,_ \[\langle v_{k+1}(t),u_{k}(t)\rangle^{2}\geqslant 1-\frac{\varepsilon}{ \sigma_{k+1}^{2}(t)}\,.\]

The first identity of the Lemma bounds the sum of the squared singular values of \(W_{k}(t)\), except the largest one. In other words, it quantifies how close \(W_{k}(t)\) is to the rank-one approximation given by the first term in its singular value decomposition. The second statement bounds the distance between the spectral norms of any two weight matrices. The last bound quantifies the alignment between the first left singular vector of \(W_{k}(t)\) and the first right singular vector of \(W_{k+1}(t)\). In particular, if \(\varepsilon\) is small and \(\sigma_{k+1}^{2}(t)\) is of order \(1\), then \(v_{k+1}(t)\) and \(u_{k}(t)\) are nearly aligned.

We next use this Lemma to show that the neural network satisfies a Polyak-Lojasiewicz (PL) condition, which is one of the main tools to study non-convex optimization dynamics (Rebjock and Boumal, 2023). A well-known result, recalled in Appendix A for completeness, shows that this implies exponential convergence of the gradient flow to a minimizer \(\mathcal{W}^{\text{SI}}\) of the empirical risk.

**Theorem 3**.: _Under Assumption \((A_{1})\), the network satisfies the PL condition for \(t\geqslant 1\), in the sense that there exists some \(\mu>0\) such that, for \(t\geqslant 1\),_

\[\sum_{k=1}^{L}\left\|\nabla_{k}R^{L}(\mathcal{W}(t))\right\|_{F}^{2}\geqslant \mu(R^{L}(\mathcal{W}(t))-R_{\min})\,.\]The proof leverages the structure of the gradient of the risk with respect to the first weight matrix, which relies on the linearity of the neural network and the fact that we consider a univariate output. More precisely, recall that, by (5),

\[\nabla_{1}R^{L}(\mathcal{W}(t))=\underbrace{(W_{L}(t)\dots W_{2}(t))^{\top}}_{d _{1}\times 1}\underbrace{\nabla R^{1}(w_{\text{prod}}(t))^{\top}}_{1\times d_{0}}\,.\]

Therefore the Frobenius norm of the gradient decomposes as the product of two vector norms

\[\big{\|}\nabla_{1}R^{L}(\mathcal{W}(t))\big{\|}_{F}^{2} =\|W_{L}(t)\dots W_{2}(t)\|_{2}^{2}\|\nabla R^{1}(w_{\text{prod}}( t))\|_{2}^{2}\] \[\geqslant 4\lambda\|W_{L}(t)\dots W_{2}(t)\|_{2}^{2}(R^{L}( \mathcal{W}(t))-R_{\min})\,,\]

where the lower bound unfolds from a straightforward computation. The delicate step is to lower bound \(\|W_{L}(t)\dots W_{2}(t)\|_{2}\), which we approach by distinguishing depending on the magnitude of \(\sigma_{1}(t)=\|W_{1}(t)\|_{2}\). If \(\sigma_{1}(t)\) is large, we use Lemma 2 to deduce both that all \(\sigma_{k}(t)\) are large and then that the first singular vectors of successive weight matrices are aligned. This implies that the product of weight matrices has a large norm. To analyze the case where \(\sigma_{1}(t)\) is small, we use Assumption \((A_{1})\) to bound away \(R^{L}(\mathcal{W}(t))\) from \(R_{0}\) for \(t\geqslant 1\), and therefore \(w_{\text{prod}}(t)\) from \(0\). The fact that \(w_{\text{prod}}(t)\) cannot be too close to \(0\) while \(\sigma_{1}(t)\) is small implies that \(\|W_{L}(t)\dots W_{2}(t)\|_{2}\) is large. All in all, this allows us to lower bound \(\|W_{L}(t)\dots W_{2}(t)\|_{2}\), and the PL condition follows.

To characterize the weights at the end of the training, we make the following assumption.

* The data covariance matrix \(\hat{\Sigma}\) is full rank, and we have \(\varepsilon\leqslant 1\), \(\|w^{\star}\|_{2}\geqslant 1\), and \(32L\sqrt{\varepsilon}\leqslant 1\).

The first statement ensures unicity of the minimizer \(w^{\star}\) of \(R^{1}\), and thus, given that the risk goes to \(0\), we have \(w_{\text{prod}}\to w^{\star}\). The last condition means that the initialization has to be scaled down as the depth increases, so that \(\varepsilon=\mathcal{O}(1/L^{2})\). Intermediates conditions are technical. We can then show the following corollary.

**Corollary 1**.: _Under Assumptions \((A_{1})\)-\((A_{2})\), there exists \(T\geqslant 1\), such that, for all \(t\geqslant T\) and \(k\in\{1,\dots,L\}\),_

\[\Big{(}\frac{\|w^{\star}\|_{2}}{2}\Big{)}^{1/L}\leqslant\sigma_{k}(t) \leqslant\left(2\|w^{\star}\|_{2}\right)^{1/L}.\]

Together with Lemma 2, this result gives a precise description of the structure of the weights at the end of the gradient flow trajectory. Up to the small factor \(\varepsilon\), the weights are rank-one matrices, with equal norms and aligned singular vectors. Since the product of weights aligns with \(w^{\star}\), this means that the first right singular vector of \(W_{1}\) has to align with \(w^{\star}\), and then the weight matrices align with their neighbors in order to propagate the signal in the network.

Combining this specific structure of the weights with the variational characterization (7) of the sharpness and the explicit formulas (5)-(6) for the gradients, we derive the following upper bound on the sharpness of the found minimizer.

**Corollary 2**.: _Under Assumptions \((A_{1})\)-\((A_{2})\), the following bounds on the sharpness of the minimizer \(\mathcal{W}^{\text{SI}}\) hold:_

\[1\leqslant\frac{S(\mathcal{W}^{\text{SI}})}{S_{\min}}\leqslant 4\frac{ \Lambda}{\lambda}\,.\]

This result shows that the sharpness of the minimizer is close to \(S_{\min}\) in the sense that their ratio is bounded by a constant times the condition number of \(\hat{\Sigma}\). For example, in the case of white data (\(\hat{\Sigma}=I\)), we obtain that \(S(\mathcal{W}^{\text{SI}})\leqslant 4S_{\min}\).

## 5 Analysis of gradient flow from residual initialization

We now study the case of residual initialization. We consider a linear network of the form (3) with

\[W_{k}(t)=I+\frac{s}{\sqrt{Ld}}N_{k}+\frac{1}{L}\theta_{k}(t)\,,\]where each matrix is a \(d\times d\) matrix and the \(N_{k}\) are filled with i.i.d. Gaussian entries \(\mathcal{N}(0,1)\). We refer to Section 1 for a discussion of the scaling factor in front of \(N_{k}\). Following the standard initialization of residual networks, we assume that the \(\theta_{k}\) are initialized to zero. Note that the scaling factor \(1/L\) in front of the \(\theta_{k}\) has no impact on the dynamics; it is convenient for exposition and computations, since we show that, with this scaling factor, the \(\theta_{k}\) are of order \(\mathcal{O}(1)\) after training.

Before stating the main result of this section on the convergence of the gradient flow, recall that \(p\in\mathbb{R}^{d}\) is a fixed vector appended at the end of the residual network to project its output back in \(\mathbb{R}\).

**Theorem 4**.: _There exist \(C_{1},\ldots,C_{5}>0\) depending only on \(s\) such that, if \(L\geqslant C_{1}\) and \(d\geqslant C_{2}\), then, with probability at least_

\[1-16\exp(-C_{3}d)\,,\]

_if_

\[R^{L}(\mathcal{W}(0))-R_{\min}\leqslant\frac{C_{4}\lambda^{2}\|p\|_{2}^{2}}{ \Lambda}\,,\]

_the gradient flow dynamics (4) converge to a global minimizer \(\mathcal{W}^{\text{RI}}\) of the risk. Furthermore, the minimizer \(\mathcal{W}^{\text{RI}}\) satisfies_

\[W_{k}^{\text{RI}}=I+\frac{s}{\sqrt{Ld}}N_{k}+\frac{1}{L}\theta_{k}^{\text{RI} }\quad\text{with}\quad\|\theta_{k}^{\text{RI}}\|_{F}\leqslant C_{5}\,,\quad 1 \leqslant k\leqslant L\,.\] (8)

To our knowledge, Theorem 4 is the first result showing convergence of gradient flow for standard Gaussian initialization of residual networks without assuming overparameterization. The main requirement is that the loss at initialization be not too large, as is standard in the literature analyzing gradient flow for deep linear residual networks (Bartlett et al., 2018; Arora et al., 2019; Zou et al., 2020; Sander et al., 2022). Note that our bound on the loss at initialization does not depend on the width \(d\), depth \(L\), or sample size \(n\). We emphasize that the same proof holds for multivariate regression, in the absence of the projection vector \(p\). We focus here on univariate regression to connect the result with the analysis of sharpness for univariate regression in Section 3. Details on adaptation to multivariate regression are given in Appendix B.7. Finally, the precise dependence of \(C_{1}\) to \(C_{5}\) on \(s\) can be found in the proof.

The proof is a refinement of the analysis for identity initialization of residual networks (Zou et al., 2020; Sander et al., 2022). From the expression of the gradients (5), we can show that

\[4\Lambda\|p\|_{2}^{2}\|\Pi_{L:k+1}\|_{2}^{2}\|\Pi_{k-1:1}\|_{2}^ {2}(R(\mathcal{W})-R_{\min})\\ \geqslant\|\nabla_{k}R^{L}(\mathcal{W})\|_{F}^{2}\geqslant 4 \lambda\|p\|_{2}^{2}\sigma_{\min}^{2}(\Pi_{L:k+1})\sigma_{\min}^{2}(\Pi_{k-1: 1})(R(\mathcal{W})-R_{\min})\,,\]

with \(\Pi_{L:k}:=W_{L}\ldots W_{k}\) and \(\Pi_{k:1}:=W_{k}\ldots W_{1}\). Letting

\[t^{*}=\inf\left\{t\in\mathbb{R}_{+},\exists k\in\{1,\ldots,L\},\|\theta_{k}(t )\|_{F}>C_{5}\right\},\]

the crucial step is to lower bound \(\sigma_{\min}^{2}(\Pi_{L:k+1})\) and \(\sigma_{\min}^{2}(\Pi_{k-1:1})\) uniformly for \(t\in[0,t^{*}]\), in order to get a PL condition valid for \(t\in[0,t^{*}]\). Then, the condition on the loss at initialization is used to prove that \(t^{*}=\infty\), thereby the PL condition holds for all \(t\geqslant 0\). We deduce both convergence and the bound on the norm of \(\theta_{k}^{\text{RI}}\). The lower bound on \(\sigma_{\min}^{2}(\Pi_{L:k+1})\) and \(\sigma_{\min}^{2}(\Pi_{k-1:1})\) is straightforward in the case of an identity initialization. In the case of Gaussian initialization, the proof is more intricate, and leverages the following high probability bounds on the singular values of residual networks.

**Lemma 3**.: _There exist \(C_{1},\ldots,C_{4}>0\) depending only on \(s\) such that, if_

\[L\geqslant C_{1}\,,\quad d\geqslant C_{2}\,,\quad u\in[C_{3},C_{4}L^{1/4}]\,,\]

_then, with probability at least_

\[1-8\exp\Big{(}-\frac{du^{2}}{32s^{2}}\Big{)}\,,\]

_it holds for all \(\theta\) such that \(\max_{1\leqslant k\leqslant L}\|\theta_{k}\|_{2}\leqslant\frac{1}{64}\exp(-2s ^{2}-4u)\) and all \(k\in\{1,\ldots,L\}\) that_

\[\Big{\|}\Big{(}I+\frac{s}{\sqrt{Ld}}N_{k}+\frac{1}{L}\theta_{k}\Big{)}\ldots \Big{(}I+\frac{s}{\sqrt{Ld}}N_{1}+\frac{1}{L}\theta_{1}\Big{)}\Big{\|}_{2} \leqslant 4\exp\Big{(}\frac{s^{2}}{2}+u\Big{)}\,,\]

_and_

\[\sigma_{\min}\Big{(}\Big{(}I+\frac{s}{\sqrt{Ld}}N_{k}+\frac{1}{L}\theta_{k} \Big{)}\ldots\Big{(}I+\frac{s}{\sqrt{Ld}}N_{1}+\frac{1}{L}\theta_{1}\Big{)} \Big{)}\geqslant\frac{1}{4}\exp\Big{(}-\frac{2s^{2}}{d}-u\Big{)}\,.\]The proof of this result goes in three steps. We first study the evolution of the norm of the activations across the layers of the residual network when \(\theta=0\), and prove a high-probability bound by leveraging concentration inequalities for Gaussian and \(\chi^{2}\) distributions. Then an \(\varepsilon\)-net argument allows to bound the singular values. Finally, the extension to \(\theta\) in a ball around \(0\) is done via a perturbation analysis. The proof technique is related to previous works (Marion et al., 2022; Zhang et al., 2022), but provides a crisper and sounder bound. More precisely, Marion et al. (2022) show a bound on the norm of the activations with a probability of failure that decays polynomially with the width \(d\), which is not sufficient to apply the \(\varepsilon\)-net argument that requires an exponentially decreasing probability of failure. As for Zhang et al. (2022), they provide a similar bound with the purpose of showing convergence of wide residual networks, however with a less sharp probability of failure that increases polynomially with depth.1 Finally, as previously, the dependence of \(C_{1}\) to \(C_{4}\) on \(s\) can be found in the proof.

Footnote 1: We further note an incorrect use of concentration inequalities for \(\chi^{2}\) distributions that are assumed to be sub-Gaussian while they are in fact only sub-exponential. Details are given in Appendix B.8.

The characterization (8) of the minimizer in Theorem 4 allows to bound its sharpness, as made precise by the following corollary. It holds under the same assumptions and high-probability bound as the conclusion of Theorem 4.

**Corollary 3**.: _Under the assumptions of Theorem 4, and if the data covariance matrix \(\hat{\Sigma}\) is full rank, there exists \(C>0\) depending only on \(s\) such that the following bounds on the sharpness of the minimizer \(\mathcal{W}^{\mathrm{RI}}\) hold:_

\[1\leqslant\frac{S(\mathcal{W}^{\mathrm{RI}})}{S_{\min}}\leqslant C\frac{ \Lambda}{\lambda}\,.\]

As for Corollary 2, the proof relies on the fact that the norms of the weight matrices are close to each other and to the smallest possible norm to minimize the risk. This result shows again an implicit regularization towards a low-sharpness minimizer. Experimental illustration connecting the result with gradient descent with non-vanishing learning rate is provided in Appendix C.

## 6 Conclusion

This paper studies dynamics of gradient flow for deep linear networks on a regression task. For small-scale initialization, we prove that the learned weight matrices are approximately rank-one and that their singular vectors align. For residual initialization, convergence of the gradient flow for a Gaussian initialization is proven. In both cases, we obtain that the sharpness of the solution found by gradient flow is close to the smallest sharpness among all minimizers. Interesting next steps include studying the dynamics at any initialization scale, for non-vanishing learning rates, as well as extension to non-linear networks. We refer to Appendix C for additional comments and preliminary experimental results regarding possible extensions.

## Acknowledgments

P.M. acknowledges support of Google through a Google PhD Fellowship. Authors thank Matus Telgarsky for insightful discussions, and Eloise Berthier, Guillaume Dalle, Benjamin Dupuis, as well as anonymous NeurIPS reviewers, for thoughtful proofreading and remarks. An improvement to a proof was also made possible by a tweet of Gabriel Peyre.

## References

* Advani et al. (2020) M. S. Advani, A. M. Saxe, and H. Sompolinsky. High-dimensional dynamics of generalization error in neural networks. _Neural Networks_, 132:428-446, 2020.
* Agarwala et al. (2023) A. Agarwala, F. Pedregosa, and J. Pennington. Second-order regression models exhibit progressive sharpening to the edge of stability. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 169-195. PMLR, 23-29 Jul 2023.
* Krizhevsky et al. (2014)M. Andriushchenko, F. Croce, M. Muller, M. Hein, and N. Flammarion. A modern look at the relationship between sharpness and generalization. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 840-902. PMLR, 23-29 Jul 2023.
* Arora et al. (2018) S. Arora, N. Cohen, and E. Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. In J. Dy and A. Krause, editors, _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 244-253. PMLR, 10-15 Jul 2018.
* Arora et al. (2019a) S. Arora, N. Cohen, N. Golowich, and W. Hu. A convergence analysis of gradient descent for deep linear neural networks. In _International Conference on Learning Representations_, 2019a.
* Arora et al. (2019b) S. Arora, N. Cohen, W. Hu, and Y. Luo. Implicit regularization in deep matrix factorization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019b.
* Arpit et al. (2019) D. Arpit, V. Campos, and Y. Bengio. How to initialize your network? Robust initialization for WeightNorm & ResNets. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32, pages 10902-10911. Curran Associates, Inc., 2019.
* Bartlett et al. (2018) P. Bartlett, D. Helmbold, and P. Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. In J. Dy and A. Krause, editors, _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 521-530. PMLR, 10-15 Jul 2018.
* Blanc et al. (2020) G. Blanc, N. Gupta, G. Valiant, and P. Valiant. Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process. In J. Abernethy and S. Agarwal, editors, _Proceedings of Thirty Third Conference on Learning Theory_, volume 125 of _Proceedings of Machine Learning Research_, pages 483-513. PMLR, 09-12 Jul 2020.
* Bradbury et al. (2018) J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.
* Chizat and Netrapalli (2024) L. Chizat and P. Netrapalli. The feature speed formula: a flexible approach to scale hyper-parameters of deep neural networks. In A. Globerson, L. Mackey, A. Fan, C. Zhang, D. Belgrave, J. Tomczak, and U. Paquet, editors, _Advances in Neural Information Processing Systems_, volume 37. Curran Associates, Inc., 2024.
* Chizat et al. (2019) L. Chizat, E. Oyallon, and F. Bach. On lazy training in differentiable programming. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* Cohen et al. (2021) J. Cohen, S. Kaur, Y. Li, J. Z. Kolter, and A. Talwalkar. Gradient descent on neural networks typically occurs at the edge of stability. In _International Conference on Learning Representations_, 2021.
* Damian et al. (2021) A. Damian, T. Ma, and J. D. Lee. Label noise SGD provably prefers flat global minimizers. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 27449-27461. Curran Associates, Inc., 2021.
* Damian et al. (2023) A. Damian, E. Nichani, and J. D. Lee. Self-stabilization: The implicit bias of gradient descent at the edge of stability. In _The Eleventh International Conference on Learning Representations_, 2023.
* De and Smith (2020) S. De and S. Smith. Batch normalization biases residual blocks towards the identity function in deep networks. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 19964-19975. Curran Associates, Inc., 2020.
* D'Alessio et al. (2019)L. Dinh, R. Pascanu, S. Bengio, and Y. Bengio. Sharp minima can generalize for deep nets. In D. Precup and Y. W. Teh, editors, _Proceedings of the 34th International Conference on Machine Learning_, volume 70 of _Proceedings of Machine Learning Research_, pages 1019-1028. PMLR, 06-11 Aug 2017.
* Ghosh [2021] M. Ghosh. Exponential tail bounds for chisquared random variables. _Journal of Statistical Theory and Practice_, 15(2):35, 2021.
* Gidel et al. [2019] G. Gidel, F. Bach, and S. Lacoste-Julien. Implicit regularization of discrete gradient dynamics in linear neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* Glorot and Bengio [2010] X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In Y. Teh and M. Titterington, editors, _Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics_, volume 9 of _Proceedings of Machine Learning Research_, pages 249-256. PMLR, 2010.
* Gunasekar et al. [2017] S. Gunasekar, B. E. Woodworth, S. Bhojanapalli, B. Neyshabur, and N. Srebro. Implicit regularization in matrix factorization. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* He et al. [2015] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. In _2015 IEEE International Conference on Computer Vision (ICCV)_, pages 1026-1034. IEEE Computer Society, 2015.
* Hendrycks and Gimpel [2016] D. Hendrycks and K. Gimpel. Gaussian error linear units (gelus). _arXiv preprint arXiv:1606.08415_, 2016.
* Hochreiter and Schmidhuber [1997] S. Hochreiter and J. Schmidhuber. Flat minima. _Neural Computation_, 9(1):1-42, 1997.
* Ioffe and Szegedy [2015] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In F. Bach and D. Blei, editors, _Proceedings of the 32nd International Conference on Machine Learning_, volume 37 of _Proceedings of Machine Learning Research_, pages 448-456. PMLR, 2015.
* Jacot et al. [2021] A. Jacot, F. Ged, B. Simsek, C. Hongler, and F. Gabriel. Saddle-to-saddle dynamics in deep linear networks: Small initialization training, symmetry, and sparsity. _arXiv preprint arXiv:2106.15933_, 2021.
* Jastrzebski et al. [2017] S. Jastrzebski, Z. Kenton, D. Arpit, N. Ballas, A. Fischer, Y. Bengio, and A. Storkey. Three factors influencing minima in SGD. _arXiv preprint arXiv:1711.04623_, 2017.
* Ji and Telgarsky [2020] Z. Ji and M. Telgarsky. Directional convergence and alignment in deep learning. In _Advances in Neural Information Processing Systems_, volume 33, pages 17176-17186. Curran Associates, Inc., 2020.
* Jiang et al. [2020] Y. Jiang, B. Neyshabur, H. Mobahi, D. Krishnan, and S. Bengio. Fantastic generalization measures and where to find them. In _International Conference on Learning Representations_, 2020.
* Keskar et al. [2017] N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In _International Conference on Learning Representations_, 2017.
* Lampinen and Ganguli [2019] A. K. Lampinen and S. Ganguli. An analytic theory of generalization dynamics and transfer learning in deep linear networks. In _International Conference on Learning Representations_, 2019.
* Lewkowycz et al. [2020] A. Lewkowycz, Y. Bahri, E. Dyer, J. Sohl-Dickstein, and G. Gur-Ari. The large learning rate phase of deep learning: the catapult mechanism. _arXiv preprint arXiv:2003.02218_, 2020.
* Li et al. [2021] Z. Li, Y. Luo, and K. Lyu. Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning. In _International Conference on Learning Representations_, 2021.
* Liu et al. [2018]Z. Li, T. Wang, and S. Arora. What happens after SGD reaches zero loss? -a mathematical framework. In _International Conference on Learning Representations_, 2022.
* Liu et al. [2023] H. Liu, S. M. Xie, Z. Li, and T. Ma. Same pre-training loss, better downstream: Implicit bias matters for language models. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 22188-22214. PMLR, 23-29 Jul 2023.
* MacDonald et al. [2023] L. E. MacDonald, J. Valmadre, and S. Lucey. On progressive sharpening, flat minima and generalisation. _arXiv preprint arXiv:2305.14683_, 2023.
* Marion et al. [2022] P. Marion, A. Fermanian, G. Biau, and J.-P. Vert. Scaling ResNets in the large-depth regime. _arXiv preprint arXiv:2206.06929_, 2022.
* Marion et al. [2024] P. Marion, Y.-H. Wu, M. E. Sander, and G. Biau. Implicit regularization of deep residual networks towards neural ODEs. In _The Twelfth International Conference on Learning Representations_, 2024.
* Metl et al. [1901] M. Michel Petrovitch. Sur une maniere d'etendre le theoreme de la moyenne aux equations differentielles du premier ordre. _Mathematische Annalen_, 54(3):417-436, 1901.
* Mulayoff and Michaeli [2020] R. Mulayoff and T. Michaeli. Unique properties of flat minima in deep networks. In H. D. III and A. Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 7108-7118. PMLR, 13-18 Jul 2020.
* Nesterov [2018] Y. Nesterov. _Lectures on Convex Optimization_. Springer Optimization and Its Applications. Springer Cham, 2nd edition, 2018.
* Neyshabur et al. [2017] B. Neyshabur, S. Bhojanapalli, D. Mcallester, and N. Srebro. Exploring generalization in deep learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* Rebjock and Boumal [2023] Q. Rebjock and N. Boumal. Fast convergence to non-isolated minima: four equivalent conditions for \({\rm C}^{2}\) functions. _arXiv preprint arXiv:2303.00096_, 2023.
* Sander et al. [2022] M. E. Sander, P. Ablin, and G. Peyre. Do residual neural networks discretize neural ordinary differential equations? In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 36520-36532. Curran Associates, Inc., 2022.
* Saxe et al. [2014] A. Saxe, J. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In _International Conference on Learning Representations_, 2014.
* Saxe et al. [2019] A. M. Saxe, J. L. McClelland, and S. Ganguli. A mathematical theory of semantic development in deep neural networks. _Proceedings of the National Academy of Sciences_, 116(23):11537-11546, 2019.
* Smith and Le [2018] S. L. Smith and Q. V. Le. A bayesian perspective on generalization and stochastic gradient descent. In _International Conference on Learning Representations_, 2018.
* Timor et al. [2023] N. Timor, G. Vardi, and O. Shamir. Implicit regularization towards rank minimization in ReLU networks. In S. Agrawal and F. Orabona, editors, _Proceedings of The 34th International Conference on Algorithmic Learning Theory_, volume 201 of _Proceedings of Machine Learning Research_, pages 1429-1459. PMLR, 20 Feb-23 Feb 2023.
* Varre et al. [2023] A. V. Varre, M.-L. Vladarean, L. Pillaud-Vivien, and N. Flammarion. On the spectral bias of two-layer linear networks. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 64380-64414. Curran Associates, Inc., 2023.
* Vershynin [2018] R. Vershynin. _High-Dimensional Probability: An Introduction with Applications in Data Science_. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018.
* Vedaldi et al. [2017]Z. Wang, Z. Li, and J. Li. Analyzing sharpness along GD trajectory: Progressive sharpening and edge of stability. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 9983-9994. Curran Associates, Inc., 2022.
* Wu et al. (2024) J. Wu, P. L. Bartlett, M. Telgarsky, and B. Yu. Large stepsize gradient descent for logistic loss: Non-monotonicity of the loss improves optimization efficiency. _arXiv preprint arXiv:2402.15926_, 2024.
* Yang et al. (2024) G. Yang, D. Yu, C. Zhu, and S. Hayou. Tensor programs VI: Feature learning in infinite depth neural networks. In _The Twelfth International Conference on Learning Representations_, 2024.
* Yun et al. (2021) C. Yun, S. Krishnan, and H. Mobahi. A unifying view on implicit bias in training linear neural networks. In _International Conference on Learning Representations_, 2021.
* Zhang et al. (2022) H. Zhang, D. Yu, M. Yi, W. Chen, and T.-Y. Liu. Stabilize deep ResNet with a sharp scaling factor \(\tau\). _Machine Learning_, 111(9):3359-3392, 2022.
* Zou et al. (2020) D. Zou, P. M. Long, and Q. Gu. On the global convergence of training deep linear ResNets. In _International Conference on Learning Representations_, 2020.

**Appendix**

Organization of the Appendix.Appendix A presents some useful preliminary lemmas. The proofs of the results of the main paper are presented in Appendix B, while additional plots, discussion, and experimental details are given in Appendix C. Finally, Appendix D discusses some additional related work.

## Appendix A Technical lemmas

**Lemma 4**.: _For \(\alpha>0\) and \(x\in[0,\nicefrac{{1}}{{2}}]\),_

\[(1-x)^{\alpha}\geqslant 1-2\alpha x\,.\]

_For \(\alpha>0\) and \(x>0\) such that \(\alpha x\leqslant 1\),_

\[(1+x)^{\alpha}\leqslant 1+2\alpha x\,.\]

Proof.: Regarding the first inequality of the Lemma, we have

\[(1-x)^{\alpha}=\exp(\alpha\log(1-x))\geqslant\exp(\alpha(-2x))\geqslant 1-2 \alpha x\,,\]

where the first inequality holds for \(x\in[0,\nicefrac{{1}}{{2}}]\). The second inequality of the Lemma is proven by

\[(1+x)^{\alpha}=\exp(\alpha\log(1+x))\leqslant\exp(\alpha x)\leqslant 1+2 \alpha x\,,\]

where the second inequality holds when \(\alpha x\leqslant 1\). 

**Lemma 5**.: _There exists an absolute constant \(C>0\) such that, for \(L\geqslant C\) and \(x\geqslant 1\),_

\[L\exp(-\sqrt{L}x)\leqslant 4\exp(-x)\,.\]

Proof.: For any \(x\in\mathbb{R}\),

\[L\exp(-\sqrt{L}x)\leqslant 4\exp(-x)\Leftrightarrow\exp((\sqrt{L}-1)x) \geqslant\frac{L}{4}\,.\]

Then, for \(L\geqslant 1\) and \(x\geqslant 1\), we have

\[\exp((\sqrt{L}-1)x) \geqslant 1+(\sqrt{L}-1)x+\frac{1}{2}(\sqrt{L}-1)^{2}x^{2}\] \[=1+(\sqrt{L}-1)x+\frac{1}{2}(L+1-2\sqrt{L})x^{2}\] \[\geqslant(\sqrt{L}-1)x+\frac{L}{4}+\frac{1}{2}(\frac{L}{2}+1-2 \sqrt{L})x^{2}\]

For \(L\) large enough, \(\frac{L}{2}+1-2\sqrt{L}\geqslant 0\). Thus, since \(x\geqslant 1\),

\[\exp((\sqrt{L}-1)x) \geqslant(\sqrt{L}-1)x+\frac{L}{4}+\frac{1}{2}(\frac{L}{2}+1-2 \sqrt{L})x\] \[=\frac{L}{4}+(\frac{L}{4}-\frac{1}{2})x\] \[\geqslant\frac{L}{4}\,,\]

where the last inequality holds for \(L\) large enough. This concludes the proof. 

**Lemma 6**.: _Let \(h\in\mathbb{R}^{d}\), \(N\in\mathbb{R}^{d\times d}\) with i.i.d. standard Gaussian entries, and_

\[Y_{1}=\frac{\|Nh\|_{2}^{2}}{\|h\|_{2}^{2}}\,,\quad Y_{2}=\frac{h^{\top}Nh}{\| h\|_{2}^{2}}\,,\]

_Then_

\[Y_{1}\sim\chi^{2}(d)\quad\text{and}\quad Y_{2}\sim\mathcal{N}(0,1)\,.\]Proof.: We have, for \(i\in\{1,\ldots,d\}\),

\[(Nh)_{i}=\sum_{j=1}^{d}N_{ij}h_{j}\,.\]

By independence of the \((N_{ij})_{1\leqslant j\leqslant d}\), we deduce that \((Nh)_{i}\) follows a \(\mathcal{N}(0,\|h\|^{2})\) distribution. Furthermore, by independence of the rows of \(N\), the \((Nh)_{i}\) are independent. Thus

\[Y_{1}=\frac{1}{\|h\|^{2}}\sum_{i=1}^{d}(Nh)_{i}^{2}\]

follows a \(\chi^{2}(d)\) distribution. Moving on to \(Y_{2}\), we have

\[Y_{2}=\frac{1}{\|h\|^{2}}\sum_{i,j=1}^{d}N_{ij}h_{i}h_{j}\,.\]

Thus \(Y_{2}\) follows a centered Gaussian distribution, and by independence of the \((N_{ij})_{1\leqslant i,j\leqslant d}\), its variance is equal to

\[\frac{1}{\|h\|^{4}}\sum_{i,j=1}^{d}h_{i}^{2}h_{j}^{2}=1\,,\]

which concludes the proof. 

The next lemma shows that the PL condition implies exponential convergence of the gradient flow. It is a well-known fact (see, e.g., Rebjock and Boumal, 2023, for an overview of similar conditions), proved here for completeness.

**Lemma 7**.: _Let \(f:\mathbb{R}^{D}\to\mathbb{R}\) be a differentiable function lower bounded by \(f_{\min}\in\mathbb{R}\), and consider the gradient flow dynamics_

\[\frac{dx}{dt}=-\nabla f(x(t))\,.\]

_If \(f\) satisfies the Polyak-Lojasiewicz inequality for \(t\geqslant 0\)_

\[\|\nabla f(x(t))\|_{2}^{2}\geqslant\mu(f(x(t))-f_{\min})\,,\]

_then \(x(t)\) converges to a global minimizer \(x_{\infty}\), and, for \(t\geqslant 0\),_

\[f(x(t))-f_{\min}\leqslant(f(x(0))-f_{\min})e^{-\mu t}\,.\]

Proof.: By the chain rule,

\[\frac{d}{dt}f(x(t))=\left\langle\nabla f(x(t)),\frac{dx}{dt}\right\rangle=-\| \nabla f(x(t))\|_{2}^{2}\,.\]

Plugging in the Polyak-Lojasiewicz inequality,

\[\frac{d}{dt}f(x(t))\leqslant-\mu(f(x(t))-f_{\min})\,.\]

Thus

\[\frac{d}{dt}(f(x(t))-f_{\min})\leqslant-\mu(f(x(t))-f_{\min})\,.\]

To solve this differential inequality, one can for instance use the comparison theorem (Michel Petrovitch, 1901), which states that \(f(x(t))-f_{\min}\) is smaller that the solution \(g\) of the initial value problem

\[g(0)=f(x(0))-f_{\min}\,,\quad\frac{d}{dt}g(t)=-\mu g(t)\,.\]

This shows that

\[f(x(t))-f_{\min}\leqslant(f(x(0))-f_{\min})e^{-\mu t}\,.\]

[MISSING_PAGE_FAIL:17]

**Lemma 9**.: _Take \(W_{1},\ldots W_{L}\in\mathbb{R}^{d\times d}\) such that, for all \(k\in\{1,\ldots,L\}\),_

\[\|W_{k}\ldots W_{1}\|_{2}\leqslant M\,,\]

_and_

\[\sigma_{\min}(W_{k}\ldots W_{1})\geqslant m\,,\]

_where \(M\geqslant 1\) and \(m\in(0,1)\). Then, for all \(\theta\) such that \(\max_{1\leqslant k\leqslant L}\|\theta_{k}\|_{2}\leqslant\frac{m^{2}}{4M^{2}}\), letting \(\tilde{W}_{k}=W_{k}+\frac{\theta_{k}}{L}\), we have_

\[\|\tilde{W}_{k}\ldots\tilde{W}_{1}\|_{2}\leqslant 2M\,,\]

_and_

\[\sigma_{\min}(\tilde{W}_{k}\ldots\tilde{W}_{1})\geqslant\frac{m}{2}\,.\]

Proof.: First note that the assumptions imply that, for any \(1\leqslant j<k\leqslant L\),

\[\|W_{k}\ldots W_{j+1}\|_{2}\leqslant\frac{\|W_{k}\ldots W_{j+1}W_{j}\ldots W_ {1}\|_{2}}{\sigma_{\min}(W_{j}\ldots W_{1})}=\frac{\|W_{k}\ldots W_{1}\|_{2}} {\sigma_{\min}(W_{j}\ldots W_{1})}\leqslant\frac{M}{m}\,.\] (9)

It shall come in handy to extend this formula to the case where \(j=k\), where we define the empty matrix product to be equal to the identity matrix, which has an operator norm of \(1\leqslant\frac{M}{m}\).

Now, take any \(\theta\) as in the Lemma and any \(h_{0}\in\mathbb{R}^{d}\). Let, for \(k\geqslant 0\),

\[h_{k}=W_{k}\ldots W_{1}h_{0}\,,\]

and

\[\tilde{h}_{k}=\tilde{W}_{k}\ldots\tilde{W}_{1}h_{0}\,.\]

Then

\[h_{k}=W_{k}h_{k-1}\]

and

\[\tilde{h}_{k}=\Big{(}W_{k}+\frac{\theta_{k}}{L}\Big{)}\tilde{h}_{k-1}\,.\]

Thus

\[\tilde{h}_{k}-h_{k}=\frac{\theta_{k}}{L}\tilde{h}_{k-1}+W_{k}(\tilde{h}_{k-1}- h_{k-1})\,.\]

Since \(\tilde{h}_{0}-h_{0}=0\), we get by recurrence that, for \(k\geqslant 1\),

\[\tilde{h}_{k}-h_{k}=\sum_{j=1}^{k}W_{k}\ldots W_{j+1}\frac{\theta_{j}}{L} \tilde{h}_{j-1}\,.\] (10)

From there, let us prove by recurrence that

\[\|\tilde{h}_{k}\|_{2}\leqslant 2M\|\tilde{h}_{0}\|_{2}\,.\] (11)

This equation holds for \(k=0\) since \(M\geqslant 1\). Next, assume that it holds up to a certain rank \(k-1\), and let us prove it at rank \(k\). From (10), we get that

\[\|\tilde{h}_{k}-h_{k}\|_{2}\leqslant\frac{1}{L}\sum_{j=1}^{k}\|W_{k}\ldots W_{ j+1}\|_{2}\|\theta_{j}\|_{2}\|\tilde{h}_{j-1}\|_{2}\,.\] (12)

Since \(M\geqslant 1\) and \(m<1\), the bound on \(\|\theta_{j}\|_{2}\) from the assumptions of the Lemma implies in particular that \(\|\theta_{j}\|_{2}\leqslant\frac{m}{2M}\). Utilizing this, as well as (9) and the recurrence hypothesis (11) up to rank \(k-1\), we get

\[\|\tilde{h}_{k}-h_{k}\|_{2}\leqslant\frac{1}{L}\sum_{j=1}^{k}\frac{M}{m} \cdot\frac{m}{2M}\cdot 2M\|\tilde{h}_{0}\|_{2}\leqslant M\|\tilde{h}_{0}\|_{2}\,.\]

Then

\[\|\tilde{h}_{k}\|_{2}\leqslant\|h_{k}\|_{2}+\|\tilde{h}_{k}-h_{k}\|_{2} \leqslant\|W_{k}\ldots W_{1}\|_{2}\|h_{0}\|_{2}+M\|\tilde{h}_{0}\|_{2} \leqslant 2M\|\tilde{h}_{0}\|_{2}\,.\]This concludes the proof of the recurrence hypothesis at rank \(k\). We therefore get that

\[\|\tilde{W}_{k}\ldots\tilde{W}_{1}\|_{2}\leqslant 2M\,.\]

To prove the lower bound on the smallest singular value of \(\tilde{W}_{k}\ldots\tilde{W}_{1}\), observe that

\[\|\tilde{h}_{k}\|_{2} \geqslant\|h_{k}\|_{2}-\|\tilde{h}_{k}-h_{k}\|_{2}\] \[\geqslant\sigma_{\min}(W_{k}\ldots W_{1})\|h_{0}\|_{2}-\frac{1}{ L}\sum_{j=1}^{k}\|W_{k}\ldots W_{j+1}\|_{2}\|\theta_{j}\|_{2}\|\tilde{h}_{j-1}\|_{ 2}\,,\]

by (12). Finally, by (9), the bound on \(\|\theta_{j}\|_{2}\) from the assumptions, and the upper bound we just proved on \(\|\tilde{h}_{j-1}\|_{2}\), we have

\[\|\tilde{h}_{k}\|_{2}\geqslant m\|h_{0}\|_{2}-\frac{1}{L}\sum_{j=1}^{k}\frac {M}{m}\cdot\frac{m^{2}}{4M^{2}}\cdot 2M\|h_{0}\|_{2}\geqslant\frac{m}{2}\|h_{0}\|_{ 2}\,.\]

This concludes the proof. 

## Appendix B Proofs

### Proof of Theorem 2

For a twice continuously differentiable function \(f:\mathbb{R}^{D}\to\mathbb{R}\), the largest eigenvalue \(S\) of its Hessian \(H(f):\mathbb{R}^{D}\to\mathbb{R}^{D\times D}\) at some \(x\in\mathbb{R}^{D}\) admits the variational characterization

\[S(x)=\lim_{\xi\to 0}\sup_{\|x-\tilde{x}\|\leqslant\xi}\frac{\|\nabla f(x)- \nabla f(\tilde{x})\|_{2}}{\|x-\tilde{x}\|_{2}}\,.\]

In our case, the parameters are a set of matrices and the formula above translates into

\[S(\mathcal{W})^{2}=\lim_{\xi\to 0}\sup_{\|W_{k}-W_{k}\|_{F}\leqslant\xi} \frac{\sum_{k=1}^{L}\|\nabla_{k}R^{L}(\mathcal{W})-\nabla_{k}R^{L}(\tilde{ \mathcal{W}})\|_{F}^{2}}{\sum_{k=1}^{L}\|W_{k}-\tilde{W}_{k}\|_{F}^{2}}\,.\] (13)

We now take \(\mathcal{W}\) to be an arbitrary minimizer of \(R^{L}\). To obtain the lower bounds, consider for \(\xi\leqslant 1\)

\[\tilde{W}_{k}(\xi)=W_{k}+\xi M_{k}\,,\]

where the \(M_{k}\in\mathbb{R}^{d_{k}\times d_{k-1}}\) are parameters that will be chosen later (depending on the \(W_{k}\) but not on \(\xi\)). Then

\[S(\mathcal{W})^{2}\geqslant\lim_{\xi\to 0}\frac{\sum_{k=1}^{L}\|\nabla_{k}R^{L}( \mathcal{W})-\nabla_{k}R^{L}(\tilde{\mathcal{W}}(\xi))\|_{F}^{2}}{\xi^{2} \sum_{k=1}^{L}\|M_{k}\|_{F}^{2}}\,,\] (14)

To alleviate notations, we drop the dependence of \(\tilde{W}_{k}\) on \(\xi\). Recall that, for any parameters \(\mathcal{W}^{0}\),

\[\nabla_{k}R^{L}(\mathcal{W}^{0})=W_{k+1}^{0\top}\ldots W_{L}^{0\top}p\nabla R ^{1}(w_{\text{prod}}^{0})^{\top}W_{1}^{0\top}\ldots W_{k-1}^{0\top}\,,\]

with

\[w_{\text{prod}}^{0}=W_{1}^{0\top}\ldots W_{L}^{0\top}p\quad\text{and}\quad \nabla R^{1}(w_{\text{prod}}^{0})=-\frac{2}{n}X^{\top}X(w^{\star}-w_{\text{ prod}}^{0})\,.\]

For minimizers of the empirical risk, \(\nabla_{k}R^{L}(\mathcal{W})=0\), so

\[\Delta_{k}:=\nabla_{k}R^{L}(\mathcal{W})-\nabla_{k}R^{L}(\tilde{\mathcal{W}})= -\tilde{W}_{k+1}^{\top}\ldots\tilde{W}_{L}^{\top}p\nabla R^{1}(\tilde{w}_{ \text{prod}})^{\top}\tilde{W}_{1}^{\top}\ldots\tilde{W}_{k-1}^{\top}\,.\] (15)

Minimizers of the empirical risk also satisfy that \(X^{\top}Xw_{\text{prod}}=X^{\top}Xw^{\star}\). Thus, by adding and subtracting differences,

\[\nabla R^{1}(\tilde{w}_{\text{prod}}) =\frac{2}{n}X^{\top}X(\tilde{w}_{\text{prod}}-w_{\text{prod}})\] \[=\frac{2}{n}X^{\top}X\Big{(}\sum_{k=1}^{L}\tilde{W}_{1}^{\top} \ldots\tilde{W}_{k-1}^{\top}(\tilde{W}_{k}^{\top}-W_{k}^{\top})W_{k+1}^{\top} \ldots W_{L}^{\top}p\Big{)}\] \[=\frac{2}{n}\xi X^{\top}X\Big{(}\sum_{k=1}^{L}W_{1}^{\top}\ldots W_{k-1}^{ \top}M_{k}^{\top}W_{k+1}^{\top}\ldots W_{L}^{\top}p\Big{)}+\mathcal{O}(\xi^{2} )\,.\] (16)Here, and in the remainder of this proof, the notation \(\mathcal{O}\) is taken with respect to the limit when \(\xi\to 0\), everything else being fixed. In particular, inspecting the expression above, we see that \(\nabla R(\tilde{w}_{\text{prod}})=\mathcal{O}(\xi)\), and therefore, going back to (15), that

\[\Delta_{k}=-W_{k+1}^{\top}\ldots W_{L}^{\top}\,p\nabla R^{1}(\tilde{w}_{\text{ prod}})^{\top}W_{1}^{\top}\ldots W_{k-1}^{\top}+\mathcal{O}(\xi^{2})=: \bar{\Delta}_{k}+\mathcal{O}(\xi^{2})\,.\] (17)

By the inequality of arithmetic and geometric means, and by subadditivity of the operator norm,

\[\sum_{k=1}^{L}\|\Delta_{k}\|_{F}^{2} =\sum_{k=1}^{L}\|\bar{\Delta}_{k}\|_{F}^{2}+\mathcal{O}(\xi^{3})\] \[\geqslant\sum_{k=1}^{L}\|\bar{\Delta}_{k}\|_{2}^{2}+\mathcal{O}( \xi^{3})\] \[\geqslant L\Big{(}\prod_{k=1}^{L}\|\bar{\Delta}_{k}\|_{2}\Big{)}^{2 /L}+\mathcal{O}(\xi^{3})\] \[\geqslant L\big{(}\|\bar{\Delta}_{L}\cdots\bar{\Delta}_{1}\|_{2} \big{)}^{2/L}+\mathcal{O}(\xi^{3})\,.\] (18)

By definition of \(\bar{\Delta}_{k}\),

\[\bar{\Delta}_{L}\cdots\bar{\Delta}_{1} =(-1)^{L}p\nabla R^{1}(\tilde{w}_{\text{prod}})^{\top}\underbrace {w_{\text{prod}}\nabla R^{1}(\tilde{w}_{\text{prod}})^{\top}\cdots w_{\text{ prod}}\nabla R^{1}(\tilde{w}_{\text{prod}})^{\top}}_{L-1\,\text{times}}\] \[=(-1)^{L}p(\nabla R^{1}(\tilde{w}_{\text{prod}})^{\top}\,w_{ \text{prod}})^{L-1}\nabla R^{1}(\tilde{w}_{\text{prod}})^{\top}\] \[=(-1)^{L}p(\nabla R^{1}(\tilde{w}_{\text{prod}})^{\top}\,w^{ \star})^{L-1}\nabla R^{1}(\tilde{w}_{\text{prod}})^{\top}\,,\]

where the last identity comes from the formulas \(\nabla R^{1}(\tilde{w}_{\text{prod}})=\frac{2}{n}X^{\top}X(\tilde{w}_{\text{ prod}}-w_{\text{prod}})\) and \(X^{\top}Xw_{\text{prod}}=X^{\top}Xw^{\star}\). Thus

\[\|\bar{\Delta}_{L}\cdots\bar{\Delta}_{1}\|_{2} =(\nabla R^{1}(\tilde{w}_{\text{prod}})^{\top}w^{\star})^{L-1}\| p\|_{2}\|\nabla R^{1}(\tilde{w}_{\text{prod}})\|_{2}\] \[\geqslant(\nabla R^{1}(\tilde{w}_{\text{prod}})^{\top}w^{\star}) ^{L-1}\|p\|_{2}\frac{\nabla R^{1}(\tilde{w}_{\text{prod}})^{\top}w^{\star}}{ \|w^{\star}\|_{2}}\] \[=\frac{(\nabla R^{1}(\tilde{w}_{\text{prod}})^{\top}\,w^{\star}) ^{L}\|p\|_{2}}{\|w^{\star}\|_{2}}\,,\]

and, by (18),

\[\sum_{k=1}^{L}\|\Delta_{k}\|_{F}^{2}\geqslant L\frac{(\nabla R^{1}(\tilde{w}_ {\text{prod}})^{\top}w^{\star})^{2}\|p\|_{2}^{2/L}}{\|w^{\star}\|_{2}^{2/L}}+ \mathcal{O}(\xi^{3})\,.\] (19)

Coming back to (16), we have

\[w^{\star\top}\nabla R^{1}(\tilde{w}_{\text{prod}})=\xi\sum_{k=1}^{L}w^{\star \top}\frac{2}{n}X^{\top}XW_{1}^{\top}\ldots W_{k-1}^{\top}M_{k}^{\top}W_{k+1} ^{\top}\ldots W_{L}^{\top}p+\mathcal{O}(\xi^{2})\,.\]

At this point, the computations diverge for the two lower bounds we want to prove. For the first inequality, we take \(M_{k}=\beta_{k}W_{k}\), where the \(\beta_{k}\) are free parameters to be optimized. We have

\[w^{\star\top}\nabla R^{1}(\tilde{w}_{\text{prod}}) =\xi\sum_{k=1}^{L}\beta_{k}w^{\star\top}\frac{2}{n}X^{\top}XW_{1} ^{\top}\ldots W_{k-1}^{\top}W_{k}^{\top}W_{k+1}^{\top}\ldots W_{L}^{\top}p+ \mathcal{O}(\xi^{2})\] \[=\xi\sum_{k=1}^{L}\beta_{k}w^{\star\top}\frac{2}{n}X^{\top}Xw_{ \text{prod}}+\mathcal{O}(\xi^{2})\] \[=\xi\sum_{k=1}^{L}\beta_{k}w^{\star\top}\frac{2}{n}X^{\top}Xw^{ \star}+\mathcal{O}(\xi^{2})\] \[=2\xi a\|w^{\star}\|^{2}\sum_{j=k}^{L}\beta_{k}+\mathcal{O}(\xi^{ 2})\,.\]Therefore, by (19),

\[\sum_{k=1}^{L}\|\Delta_{k}\|_{F}^{2}\geqslant 4L\xi^{2}a^{2}\|w^{\star}\|_{2}^{4- \frac{2}{L}}\|p\|_{2}^{\frac{2}{L}}\bigg{(}\sum_{k=1}^{L}\beta_{k}\bigg{)}^{2}+ \mathcal{O}(\xi^{3})\,.\]

Furthermore,

\[\sum_{k=1}^{L}\|M_{k}\|_{F}^{2}=\sum_{k=1}^{L}\beta_{k}^{2}\|W_{k}\|_{F}^{2}\,.\]

By (14), we get

\[S(\mathcal{W})^{2}\geqslant 4La^{2}\|w^{\star}\|_{2}^{4-\frac{2}{L}}\|p\|_{2}^{ \frac{2}{L}}\frac{\Big{(}\sum_{k=1}^{L}\beta_{k}\Big{)}^{2}}{\sum_{k=1}^{L} \beta_{k}^{2}\|W_{k}\|_{F}^{2}}\,.\]

The first lower bound unfolds by taking \(\beta_{k}=1/\|W_{k}\|_{F}\).

Moving on to the second lower-bound, we now take

\[M_{k}=\beta_{k}\frac{u_{k}v_{k}^{\top}}{\|u_{k}\|_{2}\|v_{k}\|_{2}}\,,\quad u _{k}=W_{k-1}\ldots W_{1}\frac{2}{n}X^{\top}Xw^{\star}\,,\quad v_{k}=W_{k+1}^{ \top}\ldots W_{L}^{\top}p\,,\]

where again the \(\beta_{k}\) are free parameters to be optimized. We therefore have

\[w^{\star\top}\nabla R^{1}(\tilde{w}_{\text{prod}}) =\sum_{k=1}^{L}\underbrace{w^{\star\top}\frac{2}{n}X^{\top}XW_{1} ^{\top}\ldots W_{k-1}^{\top}}_{=u_{k}^{\top}}M_{k}^{\top}\underbrace{W_{k+1}^{ \top}\ldots W_{L}^{\top}}_{=v_{k}}p+\mathcal{O}(\xi^{2})\] \[=\xi\sum_{k=1}^{L}\beta_{k}\|u_{k}\|_{2}\|v_{k}\|_{2}+\mathcal{O} (\xi^{2})\] \[=\xi\sum_{k=1}^{L}\beta_{k}\|W_{k+1}^{\top}\ldots W_{L}^{\top}p\| _{2}\|w^{\star\top}\frac{2}{n}X^{\top}XW_{1}^{\top}\ldots W_{k-1}^{\top}\|_{2} +\mathcal{O}(\xi^{2})\] \[\geqslant\xi\sum_{k=1}^{L}\beta_{k}\|W_{k+1}^{\top}\ldots W_{L}^ {\top}pw^{\star\top}\frac{2}{n}X^{\top}XW_{1}^{\top}\ldots W_{k-1}^{\top}\|_{ 2}+\mathcal{O}(\xi^{2})\,,\]

where the last line unfolds from subadditivity of the operator norm. We let \(A_{k}=W_{k+1}^{\top}\ldots W_{L}^{\top}pw^{\star\top}\frac{2}{n}X^{\top}XW_{1} ^{\top}\ldots W_{k-1}^{\top}\), and choose \(\beta_{k}=\|A_{k}\|_{2}\). Then

\[w_{\text{prod}}^{\top}\nabla R^{1}(\tilde{w}_{\text{prod}})\geqslant\xi\sum_{k =1}^{L}\|A_{k}\|_{2}^{2}+\mathcal{O}(\xi^{2})\,.\]

Coming back to (19), we get

\[\sum_{k=1}^{L}\|\Delta_{k}\|_{F}^{2}\geqslant\frac{L\xi^{2}\|p\|_{2}^{2/L}}{ \|w^{\star}\|_{2}^{2/L}}\Big{(}\sum_{k=1}^{L}\|A_{k}\|_{2}^{2}\Big{)}^{2}+ \mathcal{O}(\xi^{3})\,.\]

Furthermore,

\[\sum_{k=1}^{L}\|W_{k}-\tilde{W}_{k}\|_{F}^{2}=\xi^{2}\sum_{k=1}^{L}\beta_{k}^ {2}\frac{\|u_{k}v_{k}^{\top}\|_{F}^{2}}{\|u_{k}\|_{2}^{2}\|v_{k}\|_{2}^{2}}= \xi^{2}\sum_{k=1}^{L}\beta_{k}^{2}=\xi^{2}\sum_{k=1}^{L}\|A_{k}\|_{2}^{2}\,.\]

Therefore, we obtain, by (14),

\[S(\mathcal{W})^{2}\geqslant\frac{L\|p\|_{2}^{2/L}}{\|w^{\star}\|_{2}^{2/L}} \sum_{k=1}^{L}\|A_{k}\|_{2}^{2}\,.\]

We can lower bound the sum similarly to (18):

\[\sum_{k=1}^{L}\|A_{k}\|_{2}^{2}\geqslant L\big{(}\|A_{L}\cdots A_{1}\|_{2} \big{)}^{2/L}\,,\]\[A_{L}\cdots A_{1} =pw^{\star\top}\frac{2}{n}X^{\top}X\underbrace{w_{\text{prod}}w^{ \star\top}\frac{2}{n}X^{\top}X\cdots w_{\text{prod}}{w^{\star\top}}^{2}_{L-1 \text{ times}}}_{L-1\text{ times}}\] \[=p\Big{(}{w^{\star\top}\frac{2}{n}X^{\top}Xw^{\star}}\Big{)}^{L-1 }{w^{\star\top}\frac{2}{n}X^{\top}X}\,.\]

Thus, using the Cauchy-Schwarz inequality, we get

\[\|A_{L}\cdots A_{1}\|_{2} =\Big{(}{w^{\star\top}\frac{2}{n}X^{\top}Xw^{\star}}\Big{)}^{L-1} \|pw^{\star\top}\frac{2}{n}X^{\top}X\|_{2}\] \[=\Big{(}{w^{\star\top}\frac{2}{n}X^{\top}Xw^{\star}}\Big{)}^{L-1} \|p\|_{2}\|{w^{\star\top}\frac{2}{n}X^{\top}X}\|_{2}\] \[\geqslant\frac{\|p\|_{2}}{\|w^{\star}\|_{2}}\Big{(}{w^{\star \top}\frac{2}{n}X^{\top}Xw^{\star}}\Big{)}^{L}\] \[=2^{L}a^{L}\|p\|_{2}\|w^{\star}\|_{2}^{2L-1}\,.\]

Therefore

\[\sum_{k=1}^{L}\|A_{k}\|_{2}^{2}\geqslant 4La^{2}\|w^{\star}\|_{2}^{4-\frac{2}{ 2}}\|p\|_{2}^{\frac{7}{2}}\,.\]

We finally obtain

\[S(\mathcal{W})^{2}\geqslant 4L^{2}a^{2}\|w^{\star}\|_{2}^{4-\frac{t}{2}}\|p\|_ {2}^{\frac{t}{2}}\,,\]

which gives the second lower bound.

To obtain the upper bound on \(S_{\min}\), we construct explicitly a minimizer, and upper bound its sharpness. More precisely, let the \(W_{k}\) be rank-one matrices, such that \(\|W_{k}\|_{2}=\|w^{\star}\|^{1/L}/\|p\|^{1/L}\), successive matrices have aligned first singular vectors, the first right singular vector of \(W_{1}\) is aligned with \(w^{\star}\), and the first left singular vector of \(W_{L}\) is aligned with \(p\). We then have

\[p^{\top}W_{L}\ldots W_{1}=w^{\star}\,,\]

meaning that the network minimizes the loss. We now upper bound its sharpness using (13), where we recall that the matrix appearing in the numerator is denoted by \(\Delta_{k}\) and satisfies \(\Delta_{k}=\bar{\Delta}_{k}+\mathcal{O}(\xi^{2})\), where \(\bar{\Delta}_{k}\) is given by (17). Contrarily to the proof of the lower bounds where we exhibited a specific direction \(\tilde{W}\), we here seek an upper bound valid for all \(\tilde{W}\). Recall that, by (16), we have

\[\nabla R^{1}(\tilde{w}_{\text{prod}})=\frac{2}{n}X^{\top}X\sum_{k=1}^{L}W_{1} ^{\top}\ldots W_{k-1}^{\top}(\tilde{W}_{k}-W_{k})^{\top}W_{k+1}^{\top}\ldots W _{L}^{\top}p+\mathcal{O}(\xi^{2})\,.\] (20)

By subadditivity of the operator norm,

\[\|\nabla R^{1}(\tilde{w}_{\text{prod}})\|_{2} \leqslant 2\Lambda\sum_{k=2}^{L}\|W_{1}\|_{2}\ldots\|W_{k-1}\|_{2} \|\tilde{W}_{k}-W_{k}\|_{2}\|W_{k+1}\|_{2}\ldots\|W_{L}\|_{2}\|p\|_{2}+ \mathcal{O}(\xi^{2})\] \[=2\Lambda\|w^{\star}\|_{2}^{(L-1)/L}\|p\|_{2}^{1/L}\sum_{k=1}^{L} \|\tilde{W}_{k}-W_{k}\|_{2}+\mathcal{O}(\xi^{2})\,.\] (21)

Moving on to bounding the squared Frobenius norm of \(\Delta_{k}\), we observe that \(\bar{\Delta}_{k}\) decomposes as a rank-one matrix. We split cases for \(k=1\) and \(k>1\). First, for \(k=1\), we have, by subadditivity of the operator norm and by (21),

\[\|\Delta_{1}\|_{F} =\|W_{2}^{\top}\ldots W_{L}^{\top}p\|_{2}\|\nabla R^{1}(\tilde{w}_ {\text{prod}})\|_{2}+\mathcal{O}(\xi^{2})\] \[\leqslant\|W_{2}\|_{2}\ldots\|W_{L}\|_{2}\|p\|_{2}\|\nabla R^{1}( \tilde{w}_{\text{prod}})\|_{2}+\mathcal{O}(\xi^{2})\] \[\leqslant 2\Lambda\|w^{\star}\|_{2}^{2(L-1)/L}\|p\|_{2}^{2/L} \sum_{k=1}^{L}\|\tilde{W}_{k}-W_{k}\|_{2}+\mathcal{O}(\xi^{2})\,.\]Then

\[\|\Delta_{1}\|_{F}^{2} \leqslant 4\Lambda^{2}\|w^{\star}\|_{2}^{4(L-1)/L}\|p\|_{2}^{4/L} \Big{(}\sum_{k=1}^{L}\|\tilde{W}_{k}-W_{k}\|_{2}\Big{)}^{2}+\mathcal{O}(\xi^{3})\] \[\leqslant 4L\Lambda^{2}\|w^{\star}\|_{2}^{4(L-1)/L}\|p\|_{2}^{4/L} \sum_{k=1}^{L}\|\tilde{W}_{k}-W_{k}\|_{2}^{2}+\mathcal{O}(\xi^{3})\] \[\leqslant 4L\Lambda^{2}\|w^{\star}\|_{2}^{4(L-1)/L}\|p\|_{2}^{4/L }\sum_{k=1}^{L}\|\tilde{W}_{k}-W_{k}\|_{F}^{2}+\mathcal{O}(\xi^{3})\,.\]

For \(k>1\), we have

\[\|\Delta_{k}\|_{F} =\|W_{k+1}^{\top}\dots W_{L}^{\top}p\|_{2}\|\nabla R^{1}(\tilde{w }_{\text{prod}})^{\top}W_{1}^{\top}\dots W_{k-1}^{\top}\|_{2}+\mathcal{O}(\xi^ {2})\] \[\leqslant\|W_{k+1}\|_{2}\dots\|W_{L}\|_{2}\|p\|_{2}\|\nabla R^{1} (\tilde{w}_{\text{prod}})^{\top}W_{1}^{\top}\|_{2}\|W_{2}\|_{2}\dots\|W_{k-1} \|_{2}+\mathcal{O}(\xi^{2})\] \[=\|w^{\star}\|_{2}^{(L-2)/L}\|p\|_{2}^{2/L}\|\nabla R^{1}(\tilde{ w}_{\text{prod}})^{\top}W_{1}^{\top}\|_{2}+\mathcal{O}(\xi^{2})\,.\] (22)

Let us now bound \(\|\nabla R^{1}(\tilde{w}_{\text{prod}})^{\top}W_{1}^{\top}\|_{2}\). By (20), separating the first term, we have

\[\|\nabla R^{1}(\tilde{w}_{\text{prod}})^{\top}W_{1}^{\top}\|_{2}\] \[\leqslant\|p^{\top}W_{L}\dots W_{2}(\tilde{W}_{1}-W_{1})\frac{2} {n}X^{\top}XW_{1}^{\top}\|_{2}\] \[\qquad+\sum_{k=2}^{L}\|p^{\top}W_{L}\dots W_{k+1}(\tilde{W}_{k}-W _{k})W_{k-1}\dots W_{1}\frac{2}{n}X^{\top}XW_{1}^{\top}\|_{2}+\mathcal{O}(\xi ^{2})\] \[\leqslant 2\Lambda\|p\|_{2}\|W_{L}\|_{2}\dots\|W_{2}\|_{2}\| \tilde{W}_{1}-W_{1}\|_{2}\|W_{1}\|_{2}\] \[+\sum_{k=2}^{L}\|p\|_{2}\|W_{L}\|_{2}\dots\|W_{k+1}\|_{2}\|\tilde{ W}_{k}-W_{k}\|_{2}\|W_{k-1}\|_{2}\dots\|W_{2}\|_{2}\Big{\|}W_{1}\frac{2}{n}X^{ \top}XW_{1}^{\top}\Big{\|}_{2}+\mathcal{O}(\xi^{2})\] \[=2\Lambda\|w^{\star}\|_{2}\|\tilde{W}_{1}-W_{1}\|_{2}\] \[\qquad+\sum_{k=2}^{L}\|w^{\star}\|_{2}^{(L-2)/L}\|p\|_{2}^{2/L} \Big{\|}W_{1}\frac{2}{n}X^{\top}XW_{1}^{\top}\Big{\|}_{2}\|\tilde{W}_{k}-W_{k }\|_{2}+\mathcal{O}(\xi^{2})\,.\]

Finally, recall that \(W_{1}\) is rank-one and its first right singular vector is aligned with \(w^{\star}\). A short computation therefore shows that

\[\Big{\|}W_{1}\frac{2}{n}X^{\top}XW_{1}^{\top}\Big{\|}_{2}=2a\|W_{1}\|^{2}=2a \|w^{\star}\|_{2}^{2/L}\|p\|_{2}^{-2/L}\,.\]

Thus

\[\|\nabla R^{1}(\tilde{w}_{\text{prod}})^{\top}W_{1}^{\top}\|_{2}\leqslant 2\|w^{ \star}\|_{2}\Big{(}\Lambda\|\tilde{W}_{1}-W_{1}\|_{2}+a\sum_{k=2}^{L}\|\tilde{ W}_{k}-W_{k}\|_{2}\Big{)}+\mathcal{O}(\xi^{2})\,.\]

Then, coming back to (22), for \(k>1\),

\[\|\Delta_{k}\|_{F}\leqslant 2\|w^{\star}\|_{2}^{2(L-1)/L}\|p\|_{2}^{2/L} \Big{(}\Lambda\|\tilde{W}_{1}-W_{1}\|_{2}+a\sum_{k=2}^{L}\|\tilde{W}_{k}-W_{k} \|_{2}\Big{)}+\mathcal{O}(\xi^{2})\,.\]

Thus

\[\|\Delta_{k}\|_{F}^{2} \leqslant 4\|w^{\star}\|_{2}^{4(L-1)/L}\|p\|_{2}^{4/L}\Big{(} \Lambda\|\tilde{W}_{1}-W_{1}\|_{2}+a\sum_{k=2}^{L}\|\tilde{W}_{k}-W_{k}\|_{2} \Big{)}^{2}+\mathcal{O}(\xi^{3})\] \[\leqslant 4\|w^{\star}\|_{2}^{4(L-1)/L}\|p\|_{2}^{4/L}(\Lambda^{2} +(L-1)a^{2})\sum_{k=1}^{L}\|\tilde{W}_{k}-W_{k}\|_{2}^{2}+\mathcal{O}(\xi^{3})\] \[\leqslant 4\|w^{\star}\|_{2}^{4(L-1)/L}\|p\|_{2}^{4/L}(\Lambda^{2} +(L-1)a^{2})\sum_{k=1}^{L}\|\tilde{W}_{k}-W_{k}\|_{F}^{2}+\mathcal{O}(\xi^{3})\,,\]where the second inequality holds by the Cauchy-Schwarz inequality. Thus, by (13), putting together the bounds on \(\|\Delta_{k}\|_{F}^{2}\) for \(k=1\) and \(k>1\),

\[S(\mathcal{W})^{2} =\lim_{\xi\to 0}\sup_{\|W_{k}-W_{k}\|_{F}\leqslant\xi}\frac{\sum_{k=1 }^{L}\|\Delta_{k}\|_{F}^{2}}{\sum_{k=1}^{L}\|W_{k}-\tilde{W}_{k}\|_{F}^{2}}\] \[\leqslant\lim_{\xi\to 0}4\|w^{\star}\|_{2}^{4(L-1)/L}\|p\|_{2}^{ 4/L}(L\Lambda^{2}+(L-1)(\Lambda^{2}+(L-1)a^{2}))+\mathcal{O}(\xi)\,.\]

Therefore

\[S(\mathcal{W})\leqslant 2\|w^{\star}\|_{2}^{2-\frac{L}{2}}\|p\|_{2}^{\frac{ 2}{2}}\sqrt{(2L-1)\Lambda^{2}+(L-1)^{2}a^{2}}\,,\]

which concludes the proof.

### Proof of Lemma 1

This identity can be shown by noting that the identity is trivially true for \(t=0\), then differentiating on both sides with respect to time, and using (5). We refer, e.g., to Arora et al. (2018) for a detailed proof.

### Proof of Lemma 2

Before proving the three statements of the lemma in order, we let

\[\bar{\varepsilon}=\max_{1\leqslant k\leqslant L}\|W_{k}(0)\|_{F}^{2}+\sum_{k =1}^{L-1}\|W_{k+1}^{\top}(0)W_{k+1}(0)-W_{k}(0)W_{k}^{\top}(0)\|_{2}\,.\]

Note that \(\bar{\varepsilon}\leqslant\varepsilon\).

First statement.The claim is true for \(k=L\) since \(W_{L}\) is a (row) vector. For \(k\in\{1,\dots,L-1\}\), taking the \(2\)-norm in Lemma 1, we have

\[\|W_{k+1}^{\top}W_{k+1}\|_{2} =\|W_{k}W_{k}^{\top}+W_{k+1}^{\top}(0)W_{k+1}(0)-W_{k}(0)W_{k}^{ \top}(0)\|_{2}\] \[\leqslant\|W_{k}W_{k}^{\top}\|_{2}+\|W_{k+1}^{\top}(0)W_{k+1}(0)- W_{k}(0)W_{k}^{\top}(0)\|_{2}\,.\]

Thus, using \(\|A^{\top}A\|_{2}=\|AA^{\top}\|_{2}=\|A\|_{2}^{2}\),

\[\|W_{k+1}\|_{2}^{2}-\|W_{k+1}^{\top}(0)W_{k+1}(0)-W_{k}(0)W_{k}^{\top}(0)\|_{2 }\leqslant\|W_{k}\|_{2}^{2}\,.\] (23)

We now take the trace in Lemma 1 to obtain

\[\|W_{k+1}\|_{F}^{2}-\|W_{k+1}(0)\|_{F}^{2}=\|W_{k}\|_{F}^{2}-\|W_{k}(0)\|_{F}^ {2}\,.\]

Combining with the inequality above, we have that

\[\|W_{k}\|_{F}^{2}-\|W_{k}\|_{2}^{2} \leqslant\|W_{k+1}\|_{F}^{2}-\|W_{k+1}\|_{2}^{2}+\|W_{k}(0)\|_{F}^ {2}-\|W_{k+1}(0)\|_{F}^{2}\] \[+\|W_{k+1}^{\top}(0)W_{k+1}(0)-W_{k}(0)W_{k}^{\top}(0)\|_{2}\]

Summing from \(k\) to \(L-1\) and telescoping, we have

\[\|W_{k}\|_{F}^{2}-\|W_{k}\|_{2}^{2} \leqslant\|W_{L}\|_{F}^{2}-\|W_{L}\|_{2}^{2}+\|W_{k}(0)\|_{F}^{2}- \|W_{L}(0)\|_{F}^{2}\] \[+\sum_{k^{\prime}=k}^{L-1}\|W_{k^{\prime}+1}^{\top}(0)W_{k^{\prime }+1}(0)-W_{k^{\prime}}(0)W_{k^{\prime}}^{\top}(0)\|_{2}\,.\]

The first two terms compensate since \(W_{L}\) is a vector, and the remainder of the terms is less than \(\bar{\varepsilon}\leqslant\varepsilon\) by definition. This gives the first statement of the Lemma.

Second statement.Assume without loss of generality that \(j>k\). By recurrence over (23), we obtain that

\[\sigma_{j}^{2}\leqslant\sigma_{k}^{2}+\sum_{k^{\prime}=k}^{j-1}\|W_{k^{\prime}+1 }^{\top}(0)W_{k^{\prime}+1}(0)-W_{k^{\prime}}(0)W_{k^{\prime}}^{\top}(0)\|_{2} \leqslant\sigma_{k}^{2}+\bar{\varepsilon}\,.\]

The reverse inequality can be shown similarly: by considering again Lemma 1 and taking the \(2\)-norm, we get

\[\|W_{k}W_{k}^{\top}\|_{2}\leqslant\|W_{k+1}^{\top}W_{k+1}\|_{2}+\|W_{k+1}^{ \top}(0)W_{k+1}(0)-W_{k}(0)W_{k}^{\top}(0)\|_{2}\]

Thus

\[\|W_{k}\|_{2}^{2}\leqslant\|W_{k+1}\|_{2}^{2}+\|W_{k+1}^{\top}(0)W_{k+1}(0)-W_ {k}(0)W_{k}^{\top}(0)\|_{2}\,.\]

As previously, we get by recurrence that

\[\sigma_{k}^{2}\leqslant\sigma_{j}^{2}+\bar{\varepsilon}\,.\]

Combined with the reverse bound above, this gives the second statement of the lemma.

Third statement.Let us lower and upper bound \(u_{k}^{\top}W_{k+1}^{\top}W_{k+1}u_{k}\). We have on the one hand, by Lemma 1,

\[u_{k}^{\top}W_{k+1}^{\top}W_{k+1}u_{k} =u_{k}^{\top}W_{k}W_{k}^{\top}u_{k}-u_{k}^{\top}W_{k}(0)W_{k}^{ \top}(0)u_{k}+u_{k}^{\top}W_{k+1}^{\top}(0)W_{k+1}(0)u_{k}\] \[\geqslant u_{k}^{\top}W_{k}W_{k}^{\top}u_{k}-u_{k}^{\top}W_{k}(0)W_{k}^{ \top}(0)u_{k}\] \[\geqslant\sigma_{k}^{2}-\|W_{k}(0)\|_{2}^{2}\,.\]

On the other hand,

\[u_{k}^{\top}W_{k+1}^{\top}W_{k+1}u_{k} =u_{k}^{\top}\big{(}W_{k+1}^{\top}W_{k+1}-v_{k+1}\sigma_{k+1}^{2} v_{k+1}^{\top}\big{)}u_{k}+u_{k}^{\top}v_{k+1}\sigma_{k+1}^{2}v_{k+1}^{\top}u_{k}\] \[\leqslant\|W_{k+1}^{\top}W_{k+1}-v_{k+1}\sigma_{k+1}^{2}v_{k+1}^{ \top}\|_{2}+\langle v_{k+1},u_{k}\rangle^{2}\sigma_{k+1}^{2}\]

The \(2\)-norm above is equal to the second largest eigenvalue of \(W_{k+1}^{\top}W_{k+1}\), which is the square of the second largest singular value of \(W_{k+1}\). In particular, it is lower than \(\|W_{k+1}\|_{F}^{2}-\|W_{k+1}\|_{2}^{2}\) which is the sum of the squared singular values of \(W_{k+1}\) except the largest one. We obtain

\[u_{k}^{\top}W_{k+1}^{\top}W_{k+1}u_{k}\leqslant\|W_{k+1}\|_{F}^{2}-\|W_{k+1} \|_{2}^{2}+\langle v_{k+1},u_{k}\rangle^{2}\sigma_{k+1}^{2}\leqslant\bar{ \varepsilon}+\langle v_{k+1},u_{k}\rangle^{2}\sigma_{k+1}^{2}\]

by the first statement of the Lemma. Combining the lower and upper bound of \(u_{k}^{\top}W_{k+1}^{\top}W_{k+1}u_{k}\), we get

\[\bar{\varepsilon}+\langle v_{k+1},u_{k}\rangle^{2}\sigma_{k+1}^{2}\geqslant \sigma_{k}^{2}-\|W_{k}(0)\|_{2}^{2}\,.\]

Thus

\[\langle v_{k+1},u_{k}\rangle^{2}\geqslant\frac{\sigma_{k}^{2}-\|W_{k}(0)\|_{2 }^{2}-\bar{\varepsilon}}{\sigma_{k+1}^{2}}\geqslant\frac{\sigma_{k+1}^{2}- \bar{\varepsilon}-\|W_{k}(0)\|_{2}^{2}-\bar{\varepsilon}}{\sigma_{k+1}^{2}}\,,\]

by the second statement of the Lemma. We finally obtain

\[\langle v_{k+1},u_{k}\rangle^{2}\geqslant 1-\frac{2\bar{\varepsilon}+\|W_{k}(0)\|_{ 2}^{2}}{\sigma_{k+1}^{2}}\geqslant 1-\frac{\varepsilon}{\sigma_{k+1}^{2}}\,,\]

by definition of \(\varepsilon\) and \(\bar{\varepsilon}\).

### Proof of Theorem 3

We lower bound the first term in the sum of the left-hand side. Recall that, by (5), we have

\[\nabla_{1}R^{L}(W(t))=\underbrace{(W_{L}(t)\ldots W_{2}(t))^{\top}}_{d_{1} \times 1}\underbrace{\nabla R^{1}(w_{\text{pod}}(t))^{\top}}_{1\times d_{0}}\,,\]

thus

\[\big{\|}\nabla_{1}R^{L}(W(t))\big{\|}_{F}^{2}=\|W_{L}(t)\ldots W_{2}(t)\|_{2}^ {2}\big{\|}\nabla R^{1}(w_{\text{pod}}(t))\big{\|}_{2}^{2}\,.\] (24)

We show that \(\|W_{L}(t)\ldots W_{2}(t)\|_{2}\) is large by distinguishing between two cases depending on the magnitude of \(\sigma_{1}(t)=\|W_{1}(t)\|_{2}\). To this aim, let \(C>\sqrt{2\varepsilon}L\).

Large spectral norm.We first consider the case where

\[\sigma_{1}>C>\sqrt{2\varepsilon}L\,.\] (25)

By Lemma 2, for \(k\in\{1,\ldots,L-1\}\),

\[\sigma_{k}^{2}\geqslant\sigma_{1}^{2}-\varepsilon>\varepsilon\,,\]

where the second inequality unfolds from (25). Then, again by Lemma 2, for \(k\in\{1,\ldots,L-1\}\),

\[\langle v_{k+1},u_{k}\rangle^{2}\geqslant 1-\frac{\varepsilon}{\sigma_{k+1}^{2 }}>0\,.\]

It is always possible (without loss of generality) to choose the orientation of the \(u_{k}\) and \(v_{k}\) such that \(\langle v_{k+1},u_{k}\rangle\geqslant 0\) for any \(k\in\{1,\ldots L-1\}\). Making this choice, the equation above implies that

\[\|v_{k+1}-u_{k}\|^{2} =2-2\langle v_{k+1},u_{k}\rangle\] \[\leqslant 2\bigg{(}1-\sqrt{1-\frac{\varepsilon}{\sigma_{k+1}^{2}}} \bigg{)}\,.\]

For \(x\in[0,1]\), \(\sqrt{1-x}\geqslant 1-x\), and thus

\[\|v_{k+1}-u_{k}\|_{2}^{2}\leqslant\frac{2\varepsilon}{\sigma_{k+1}^{2}}\,.\] (26)

Let us show that this implies a lower bound on \(\|W_{L}\ldots W_{2}\|_{2}\). To do so, let us denote recursively

\[x_{1}=v_{2},\quad x_{k+1}=W_{k+1}x_{k}\quad\text{for}\quad k\in\{1,\ldots,L-1 \}\,.\]

We then have \(x_{L}=W_{L}\ldots W_{2}x_{1}\), thus

\[\|W_{L}\ldots W_{2}\|_{2}\geqslant\frac{\|x_{L}\|_{2}}{\|x_{1}\|_{2}}=\|x_{L} \|_{2}\,.\]

Our goal is thus to lower bound \(\|x_{L}\|_{2}\), which entails a lower bound on \(\|W_{L}\ldots W_{2}\|_{2}\). To this aim, first note that, for any \(k\in\{1,\ldots,L-1\}\),

\[\langle x_{k+1},u_{k+1}\rangle =\langle W_{k+1}x_{k},u_{k+1}\rangle\] \[=\sigma_{k+1}\langle x_{k},v_{k+1}\rangle\] \[=\sigma_{k+1}\langle x_{k},u_{k}+v_{k+1}-u_{k}\rangle\] \[\geqslant\sigma_{k+1}\langle x_{k},u_{k}\rangle-\sigma_{k+1}\|x _{k}\|_{2}\|v_{k+1}-u_{k}\|_{2}\] \[\geqslant\sigma_{k+1}\langle x_{k},u_{k}\rangle-\sqrt{2\varepsilon }\|x_{k}\|_{2}\,.\] (27)

where the last equation stems from (26). Denote \(\alpha_{k}=\langle\frac{x_{k}}{\|x_{k}\|_{2}},u_{k}\rangle\). Then the previous equation shows that

\[\alpha_{k+1}\geqslant\frac{\|x_{k}\|_{2}}{\|x_{k+1}\|_{2}}(\sigma_{k+1}\alpha _{k}-\sqrt{2\varepsilon})=\frac{\|x_{k}\|_{2}\sigma_{k+1}}{\|x_{k+1}\|_{2}} \Big{(}\alpha_{k}-\frac{\sqrt{2\varepsilon}}{\sigma_{k+1}}\Big{)}\,.\]

Further note that \(\|x_{k+1}\|_{2}\leqslant\sigma_{k+1}\|x_{k}\|_{2}\), thus

\[\alpha_{k+1}\geqslant\alpha_{k}-\frac{\sqrt{2\varepsilon}}{\sigma_{k+1}}\,.\]

By recurrence,

\[\alpha_{k}\geqslant\alpha_{2}-\sqrt{2\varepsilon}\sum_{k^{\prime}=2}^{k-1} \frac{1}{\sigma_{k+1}}\geqslant\alpha_{2}-\sqrt{2\varepsilon}\sum_{k^{\prime}= 2}^{k-1}\frac{1}{\sqrt{\sigma_{1}^{2}-\varepsilon}}=\alpha_{2}-\frac{\sqrt{2 \varepsilon}(k-2)}{\sqrt{\sigma_{1}^{2}-\varepsilon}}\,.\]

Coming back to (27), we have

\[\|x_{k+1}\|_{2}\geqslant\langle x_{k+1},u_{k+1}\rangle\geqslant\|x_{k}\|_{2} (\sigma_{k+1}\alpha_{k}-\sqrt{2\varepsilon})\,,\]thus by recurrence,

\[\|x_{L}\|_{2}\geqslant\|x_{2}\|_{2}\prod_{k=2}^{L-1}(\sigma_{k+1} \alpha_{k}-\sqrt{2\varepsilon}) \geqslant\|x_{2}\|_{2}\prod_{k=2}^{L-1}\Big{(}\sqrt{\sigma_{1}^{2} -\varepsilon}\Big{(}\alpha_{2}-\frac{\sqrt{2\varepsilon}(k-2)}{\sqrt{\sigma_{1 }^{2}-\varepsilon}}\Big{)}-\sqrt{2\varepsilon}\Big{)}\] \[\geqslant\|x_{2}\|_{2}\prod_{k=2}^{L-1}\Big{(}\sqrt{\sigma_{1}^{2 }-\varepsilon}\alpha_{2}-\sqrt{2\varepsilon}k\Big{)}\,.\]

Finally, by definition, \(\alpha_{2}=\langle\frac{x_{2}}{\|x_{2}\|_{2}},u_{2}\rangle\). Since \(x_{2}=W_{2}x_{1}=W_{2}v_{2}=\sigma_{2}u_{2}\), we obtain that \(\alpha_{2}=1\), and thus

\[\|W_{L}\ldots W_{2}\|_{2} \geqslant\|x_{L}\|_{2}\] \[\geqslant\sigma_{2}\prod_{k=2}^{L-1}\Big{(}\sqrt{\sigma_{1}^{2}- \varepsilon}-\sqrt{2\varepsilon}k\Big{)}\] \[\geqslant\sqrt{\sigma_{1}^{2}-\varepsilon}\Big{(}\sqrt{\sigma_{1 }^{2}-\varepsilon}-\sqrt{2\varepsilon}(L-1)\Big{)}^{L-2}\] \[\geqslant\Big{(}\sqrt{\sigma_{1}^{2}}-\sqrt{\varepsilon}-\sqrt{2 \varepsilon}(L-1)\Big{)}^{L-1}\] \[\geqslant\big{(}\sigma_{1}-\sqrt{2\varepsilon}L\big{)}^{L-1}\,.\]

We finally get

\[\|W_{L}\ldots W_{2}\|_{2}\geqslant\big{(}C-\sqrt{2\varepsilon}L\big{)}^{L-1}\,,\] (28)

which is a positive quantity by definition of \(C\).

Small spectral norm.We now inspect the case where (25) is not satisfied, that is, \(\sigma_{1}(t)\leqslant C\). First note that, by the formula (6) for the gradient of \(R^{1}\),

\[\|\nabla R^{1}(w_{\text{prod}})\|_{2}\leqslant 2\Lambda\|w^{\star}-w_{\text{ prod}}\|_{2}\leqslant 2\Lambda(\|w^{\star}\|_{2}+\|w_{\text{prod}}\|_{2})\,.\]

Thus, for \(\|w_{\text{prod}}\|_{2}\leqslant\|w^{\star}\|_{2}\), \(w_{\text{prod}}\mapsto R^{1}(w_{\text{prod}})\) is \(4\Lambda\|w^{\star}\|_{2}\)-Lipschitz. Let us use this property to lower bound \(\|w_{\text{prod}}(t)\|_{2}\) by a constant independent of \(t\), for \(t\geqslant 1\). Either we have \(\|w_{\text{prod}}(t)\|_{2}\geqslant\|w^{\star}\|_{2}\), or \(\|w_{\text{prod}}\|_{2}\leqslant\|w^{\star}\|_{2}\), but then, by the Lipschitzness property,

\[|R^{1}(w_{\text{prod}}(t))-R_{0}|\leqslant 4\Lambda\|w^{\star}\|_{2}\|w_{ \text{prod}}(t)-0\|_{2}=4\Lambda\|w^{\star}\|_{2}\|w_{\text{prod}}(t)\|_{2}\,,\]

where we recall that \(R_{0}\) is the risk associated to the null parameters. Furthermore, for \(t\geqslant 1\),

\[R^{1}(w_{\text{prod}}(t)) =R^{L}(\mathcal{W}(t))\] \[\leqslant R^{L}(\mathcal{W}(1))\] (The risk is decreasing along the gradient flow) \[<R^{L}(\mathcal{W}(0))\] ( \[\nabla R^{L}(\mathcal{W}(0))\neq 0\] by Assumption \[(A_{1})\] ) \[\leqslant R_{0}\,.\] (by Assumption \[(A_{1})\] )

Thus, for \(t\geqslant 1\),

\[|R^{1}(w_{\text{prod}}(t))-R_{0}|>R_{0}-R^{L}(\mathcal{W}(1))>0\,.\]

To summarize, we proved that, for \(t\geqslant 1\),

\[\|w_{\text{prod}}(t)\|_{2}\geqslant\min\Big{(}\frac{R_{0}-R^{L}(\mathcal{W}(1) )}{4\Lambda\|w^{\star}\|_{2}},\|w^{\star}\|_{2}\Big{)}>0\,,\]

where we recall that \(\|w^{\star}\|_{2}>0\) by assumption (see Section 2). Furthermore,

\[\|w_{\text{prod}}(t)\|_{2}\leqslant\|W_{L}(t)\ldots W_{2}(t)\|_{2}\|W_{1}(t)\| _{2}=\|W_{L}(t)\ldots W_{2}(t)\|_{2}\sigma_{1}(t)\,.\]

Then, for \(t\geqslant 1\),

\[\|W_{L}(t)\ldots W_{2}(t)\|_{2} \geqslant\frac{1}{\sigma_{1}(t)}\min\Big{(}\frac{R_{0}-R^{L}( \mathcal{W}(1))}{4\Lambda\|w^{\star}\|_{2}},\|w^{\star}\|_{2}\Big{)}\] \[\geqslant\frac{1}{C}\min\Big{(}\frac{R_{0}-R^{L}(\mathcal{W}(1))} {4\Lambda\|w^{\star}\|_{2}},\|w^{\star}\|_{2}\Big{)}\,.\] (29)Conclusion.Combining (28) and (29), we obtain that, for \(t\geqslant 1\),

\[\|W_{L}(t)\ldots W_{2}(t)\|_{2}\geqslant\min\left(\left(C-\sqrt{2\varepsilon}L \right)^{L-1},\frac{1}{C}\min\left(\frac{R_{0}-R^{L}(\mathcal{W}(1))}{4\Lambda \|w^{\star}\|_{2}},\|w^{\star}\|_{2}\right)\right)=:\sqrt{\mu_{1}}\,,\]

where \(\mu_{1}>0\) by the proof above. Then, for \(t\geqslant 1\), by (24),

\[\left\|\nabla_{1}R^{L}(W(t))\right\|_{F}^{2}\geqslant\mu_{1}\|\nabla R^{1}(w_ {\text{prod}}(t))\|_{2}^{2}\,.\]

By Lemma 8,

\[\|\nabla R^{1}(w_{\text{prod}}(t))\|_{2}^{2}\geqslant 4\lambda(R^{L}(\mathcal{W} (t))-R_{\min})\,.\]

Thus, taking \(\mu=4\mu_{1}\lambda>0\), for \(t\geqslant 1\),

\[\sum_{k=1}^{L}\left\|\nabla_{k}R^{L}(\mathcal{W}(t))\right\|_{F}^{2}\geqslant \left\|\nabla_{1}R^{L}(\mathcal{W}(t))\right\|_{F}^{2}\geqslant\mu(R^{L}( \mathcal{W}(t))-R_{\min})\,.\]

### Proof of Corollary 1

We first show that Assumption \((A_{2})\) implies a number of estimates that are useful in the following. Since \(\|w^{\star}\|_{2}\geqslant 1\), we have

\[\Big{(}\frac{\|w^{\star}\|_{2}}{2}\Big{)}^{1/L}\geqslant\Big{(}\frac{1}{2} \Big{)}^{1/L}\geqslant\frac{1}{2}\] (30)

thus

\[8L\varepsilon\leqslant 8L\sqrt{\varepsilon}\leqslant\frac{1}{4}\leqslant\Big{(} \frac{\|w^{\star}\|_{2}}{2}\Big{)}^{1/L}\leqslant\left(2\|w^{\star}\|_{2} \right)^{1/L}.\] (31)

Moreover,

\[\Big{(}\frac{\|w^{\star}\|_{2}}{2}\Big{)}^{2/L}\geqslant\frac{1}{4}\]

so we also have

\[8L\varepsilon\leqslant\Big{(}\frac{\|w^{\star}\|_{2}}{2}\Big{)}^{2/L}\,.\] (32)

Let us now prove the Corollary. We first note that Theorem 3 implies exponential convergence of the empirical risk to its minimum by Lemma 7. This also implies the (exponential) convergence of \(w_{\text{prod}}\) to \(w^{\star}\), since the covariance matrix \(X^{\top}X\) is full rank, so \(w^{\star}\) is the unique minimizer of \(R^{1}\), and, by Lemma 8,

\[R^{L}(\mathcal{W})-R_{\min}=\frac{1}{n}\|X(w^{\star}-w_{\text{prod}})\|_{2}^{2 }\geqslant\lambda\|w^{\star}-w_{\text{prod}}\|_{2}^{2}\,,\]

Furthermore,

\[\|w_{\text{prod}}\|_{2}=\|W_{1}^{\top}\ldots W_{L}^{\top}\|_{2}\leqslant\|W_{ 1}\|_{2}\ldots\|W_{L}\|_{2}\leqslant\Big{(}\max_{k=1}^{L}\sigma_{k}\Big{)}^{L}\,.\]

Let us show that, for \(t\) large enough,

\[\max_{k=1}^{L}\sigma_{k}\geqslant\Big{(}\frac{\|w^{\star}\|}{2}\Big{)}^{1/L}+ \varepsilon\,.\] (33)

If it were not the case, then

\[\|w_{\text{prod}}\|_{2} \leqslant\Big{(}\max_{k=1}^{L}\sigma_{k}\Big{)}^{L}\] \[\leqslant\Big{(}\Big{(}\frac{\|w^{\star}\|_{2}}{2}\Big{)}^{1/L}+ \varepsilon\Big{)}^{L}\] \[=\frac{\|w^{\star}\|_{2}}{2}\Big{(}1+\varepsilon\Big{(}\frac{2}{ \|w^{\star}\|_{2}}\Big{)}^{1/L}\Big{)}^{L}\] \[\leqslant\frac{\|w^{\star}\|_{2}}{2}\Big{(}1+2L\varepsilon\Big{(} \frac{2}{\|w^{\star}\|_{2}}\Big{)}^{1/L}\Big{)}\,,\]where the last inequality holds by Lemma 4 since

\[L\varepsilon\Big{(}\frac{2}{\|w^{\star}\|_{2}}\Big{)}^{1/L}\leqslant 1\quad \Leftrightarrow\quad L\varepsilon\leqslant\Big{(}\frac{\|w^{\star}\|_{2}}{2} \Big{)}^{1/L}\,,\]

which holds by (31). Then, we obtain

\[\|w_{\text{prod}}\|_{2}\leqslant\frac{3\|w^{\star}\|_{2}}{4}\,,\]

since

\[1+2L\varepsilon\Big{(}\frac{2}{\|w^{\star}\|_{2}}\Big{)}^{1/L}\leqslant\frac {3}{2}\quad\Leftrightarrow\quad 4L\varepsilon\leqslant\Big{(}\frac{\|w^{\star}\|_{2}}{2} \Big{)}^{1/L}\,,\]

which also holds by (31). The inequality \(\|w_{\text{prod}}\|_{2}\leqslant\frac{3\|w^{\star}\|_{2}}{4}\) contradicts the fact that \(w_{\text{prod}}\) converges to \(w^{\star}\), thus proving (33). Then, we have

\[\max_{k=1}^{L}\sigma_{k}^{2}\geqslant\Big{(}\frac{\|w^{\star}\|_{2}}{2}\Big{)} ^{2/L}+2\varepsilon\Big{(}\frac{\|w^{\star}\|_{2}}{2}\Big{)}^{1/L}\geqslant \Big{(}\frac{\|w^{\star}\|_{2}}{2}\Big{)}^{2/L}+\varepsilon\,,\]

where the last inequality holds by (30). Furthermore, by Lemma 2, for all \(k\in\{1,\ldots,L\}\),

\[\sigma_{k}^{2}\geqslant\max_{j=1}^{L}\sigma_{j}^{2}-\varepsilon\,.\]

This brings the first inequality of the Corollary. We now show the second inequality of the Corollary. First note that, by Lemma 2,

\[\|W_{k}-\sigma_{k}u_{k}v_{k}^{\top}\|_{F}^{2}=\|W_{k}\|_{F}^{2}-\|W_{k}\|_{2}^ {2}\leqslant\varepsilon\,,\]

since both quantities are equal to the sum of the squared singular values of \(W_{k}\) except the largest one. Then, adding and substracting,

\[\|W_{1}^{\top}\ldots W_{L}^{\top} -\sigma_{1}\ldots\sigma_{L}v_{1}u_{1}^{\top}\ldots v_{L}u_{L}^{ \top}\|_{2}\] \[\leqslant\sum_{k=1}^{L}\|W_{1}^{\top}\ldots W_{k-1}^{\top}(W_{k} ^{\top}-\sigma_{k}u_{k}v_{k}^{\top})\sigma_{k+1}\ldots\sigma_{L}v_{k+1}u_{k+1 }^{\top}\ldots v_{L}u_{L}^{\top}\|_{2}\] \[\leqslant\sum_{k=1}^{L}\|W_{1}\|_{2}\ldots\|W_{k-1}\|_{2}\|W_{k} ^{\top}-\sigma_{k}u_{k}v_{k}^{\top}\|_{2}\sigma_{k+1}\ldots\sigma_{L}\] \[\leqslant\sum_{k=1}^{L}\sigma_{1}\ldots\sigma_{k-1}\|W_{k}^{\top} -\sigma_{k}u_{k}v_{k}^{\top}\|_{F}\sigma_{k+1}\ldots\sigma_{L}\] \[\leqslant\sqrt{\varepsilon}\sum_{k=1}^{L}\prod_{j\neq k}\sigma_{j}\] \[=\sqrt{\varepsilon}\prod_{j=1}^{L}\sigma_{j}\sum_{k=1}^{L}\frac{1 }{\sigma_{k}}\] \[\leqslant\sqrt{\varepsilon}\prod_{j=1}^{L}\sigma_{j}\frac{L}{\min \sigma_{k}}\] \[\leqslant\frac{L\sqrt{\varepsilon}}{\Big{(}\frac{\|w^{\star}\|_{2 }}{2}\Big{)}^{1/L}}\prod_{j=1}^{L}\sigma_{j}\,,\]

by the first inequality of the Corollary. Moreover, using again the first inequality of the Corollary and Lemma 2, we have that

\[\langle v_{k+1},u_{k}\rangle^{2}\geqslant 1-\frac{\varepsilon}{\sigma_{k+1}^{ 2}}\geqslant 1-\frac{\varepsilon}{\Big{(}\frac{\|w^{\star}\|_{2}}{2} \Big{)}^{2/L}}\,.\]Since \(u_{L}=1\), we deduce that

\[\|\sigma_{1}\ldots\sigma_{L}v_{1}u_{1}^{\top}\ldots v_{L}u_{L}^{\top }\|_{2} =\prod_{k=1}^{L}\sigma_{k}\|v_{1}\|_{2}\prod_{k=1}^{L-1}u_{k}^{\top}v_ {k+1}\] \[\geqslant\prod_{k=1}^{L}\sigma_{k}\Bigg{(}1-\frac{\varepsilon}{ \left(\frac{\|w^{*}\|_{2}}{2}\right)^{2/L}}\Bigg{)}^{\frac{L-1}{2}}\] \[\geqslant\prod_{k=1}^{L}\sigma_{k}\Bigg{(}1-\frac{(L-1) \varepsilon}{\left(\frac{\|w^{*}\|_{2}}{2}\right)^{2/L}}\Bigg{)}\]

where in the last step we used Lemma 4 which is valid since

\[\frac{\varepsilon}{\left(\frac{\|w^{*}\|_{2}}{2}\right)^{2/L}}\leqslant\frac{ 1}{2}\quad\Leftrightarrow\varepsilon\leqslant\frac{1}{2}\Big{(}\frac{\|w^{* }\|_{2}}{2}\Big{)}^{2/L}\,,\]

which holds by (32). By the triangular inequality, we now have

\[\prod_{k=1}^{L}\sigma_{k}\Bigg{(}1-\frac{(L-1)\varepsilon}{\left( \frac{\|w^{*}\|_{2}}{2}\right)^{2/L}}\Bigg{)} \leqslant\|\sigma_{1}\ldots\sigma_{L}v_{1}u_{1}^{\top}\ldots v_{L }u_{L}^{\top}\|_{2}\] \[\leqslant\|W_{1}^{\top}\ldots W_{L}^{\top}\|_{2}+\|W_{1}^{\top} \ldots W_{L}^{\top}-\sigma_{1}\ldots\sigma_{L}v_{1}u_{1}^{\top}\ldots v_{L}u_{ L}^{\top}\|_{2}\] \[\leqslant\|W_{1}^{\top}\ldots W_{L}^{\top}\|_{2}+\frac{L\sqrt{ \varepsilon}}{\left(\frac{\|w^{*}\|_{2}}{2}\right)^{1/L}}\prod_{k=1}^{L}\sigma _{k}\,,\]

Thus

\[\prod_{k=1}^{L}\sigma_{k}\Bigg{(}1-\frac{(L-1)\varepsilon}{\left(\frac{\|w^{* }\|_{2}}{2}\right)^{2/L}}-\frac{L\sqrt{\varepsilon}}{\left(\frac{\|w^{*}\|_{2 }}{2}\right)^{1/L}}\Bigg{)}\leqslant\|W_{1}^{\top}\ldots W_{L}^{\top}\|_{2}\,.\]

Using (31) and (32),

\[1-\frac{(L-1)\varepsilon}{\left(\frac{\|w^{*}\|_{2}}{2}\right)^{2/L}}-\frac{ L\sqrt{\varepsilon}}{\left(\frac{\|w^{*}\|_{2}}{2}\right)^{1/L}}\geqslant 1-\frac{1}{8}- \frac{1}{8}=\frac{3}{4}\,.\]

Moreover, the product of the singular values can be lower-bounded by the smallest one to the power \(L\), so

\[\frac{3(\min\sigma_{k})^{L}}{4}\leqslant\|W_{1}^{\top}\ldots W_{L}^{\top}\|_{2 }\,.\]

Let us show that, for \(t\) large enough,

\[\min\sigma_{k}\leqslant(2\|w^{*}\|_{2})^{1/L}-\varepsilon\,.\]

If it were not the case, we would have

\[\|W_{1}^{\top}\ldots W_{L}^{\top}\|_{2} \geqslant\frac{3\big{(}(2\|w^{*}\|_{2})^{1/L}-\varepsilon\big{)} ^{L}}{4}\] \[=\frac{3}{2}\|w^{*}\|_{2}\Big{(}1-\frac{\varepsilon}{(2\|w^{*}\|_ {2})^{1/L}}\Big{)}^{L}\] \[\geqslant\frac{3}{2}\|w^{*}\|_{2}\Big{(}1-\frac{2L\varepsilon}{(2 \|w^{*}\|_{2})^{1/L}}\Big{)}\] \[\geqslant\frac{9}{8}\|w^{*}\|_{2}\,,\] (34)where the second inequality holds by Lemma 4 since

\[\frac{\varepsilon}{(2\|w^{\star}\|_{2})^{1/L}}\leqslant\frac{1}{2}\,,\]

by (31), and the third since

\[\frac{2L\varepsilon}{(2\|w^{\star}\|_{2})^{1/L}}\leqslant\frac{1}{4}\quad \Leftrightarrow\quad L\varepsilon\leqslant\frac{(2\|w^{\star}\|_{2})^{1/L}}{8}\,,\]

also by (31). Since \(9/8>1\), the inequality (34) is a contradiction with the fact that \(\|W_{1}^{\top}\ldots W_{L}^{\top}\|_{2}=\|w_{\mathsf{prod}}\|_{2}\to\|w^{\star }\|_{2}\), showing that, for \(t\) large enough,

\[\min\sigma_{k}\leqslant(2\|w^{\star}\|_{2})^{1/L}-\varepsilon\,.\]

Thus

\[\min\sigma_{k}^{2}\leqslant(2\|w^{\star}\|_{2})^{2/L}+\varepsilon^{2}-2 \varepsilon(2\|w^{\star}\|_{2})^{1/L}\,.\]

By Lemma 2, for all \(k\in\{1,\ldots,L\}\),

\[\sigma_{k}^{2} \leqslant\min\sigma_{j}^{2}+\varepsilon\] \[\leqslant(2\|w^{\star}\|_{2})^{2/L}+\varepsilon^{2}-2\varepsilon (2\|w^{\star}\|_{2})^{1/L}+\varepsilon\] \[\leqslant(2\|w^{\star}\|_{2})^{2/L}\,,\]

since

\[\varepsilon^{2}-2\varepsilon(2\|w^{\star}\|_{2})^{1/L}+\varepsilon\leqslant 0 \quad\Leftrightarrow\quad\varepsilon\leqslant 2(2\|w^{\star}\|_{2})^{1/L}-1\,,\]

which holds true by Assumption \((A_{2})\) since

\[\varepsilon\leqslant 1\leqslant 2(2\|w^{\star}\|_{2})^{1/L}-1\]

since \(\|w^{\star}\|_{2}\geqslant 1\).

### Proof of Corollary 2

The lower bound unfolds by definition of \(S_{\min}\) since \(\mathcal{W}^{\text{SI}}\) is a minimizer of the empirical risk. To obtain the upper bound, we proceed similarly to the proof of Theorem 2. For simplicity, we denote \(\mathcal{W}=\mathcal{W}^{\text{SI}}\) in the remainder of the proof. We have, as in the proof of Theorem 2,

\[S(\mathcal{W})^{2}=\lim_{\xi\to 0}\sup_{\|W_{k}-\tilde{W}_{k}\|_{F}\leqslant \xi}\frac{\sum_{k=1}^{L}\|\nabla_{k}R^{L}(\mathcal{W})-\nabla_{k}R^{L}(\tilde {\mathcal{W}})\|_{F}^{2}}{\sum_{k=1}^{L}\|W_{k}-\tilde{W}_{k}\|_{F}^{2}}\,,\] (35)

with

\[\Delta_{k} :=\nabla_{k}R^{L}(\mathcal{W})-\nabla_{k}R^{L}(\tilde{\mathcal{W }})\] \[=-W_{k+1}^{\top}\ldots W_{L}^{\top}\nabla R^{1}(\tilde{w}_{ \mathsf{prod}})^{\top}W_{1}^{\top}\ldots W_{k-1}^{\top}+\mathcal{O}(\xi^{2})\,,\]

and

\[\nabla R^{1}(\tilde{w}_{\mathsf{prod}})=\frac{2}{n}X^{\top}X\Big{(}\sum_{k=1} ^{L}W_{1}^{\top}\ldots W_{k-1}^{\top}(\tilde{W}_{k}^{\top}-W_{k}^{\top})W_{k+1 }^{\top}\ldots W_{L}^{\top}\Big{)}+\mathcal{O}(\xi^{2})\,.\]

We recall that, as in the proof of Theorem 2, the notation \(\mathcal{O}\) is taken with respect to the limit when \(\xi\to 0\), everything else being fixed. By subadditivity of the operator norm and by Corollary 1, we have

\[\|\nabla R^{1}(\tilde{w}_{\mathsf{prod}})\|_{2} \leqslant 2\Lambda\sum_{k=1}^{L}\|W_{1}\|_{2}\ldots\|W_{k-1}\|_{2} \|\tilde{W}_{k}-W_{k}\|_{2}\|W_{k+1}\|_{2}\ldots\|W_{L}\|_{2}+\mathcal{O}(\xi^ {2})\] \[\leqslant 2\Lambda(2\|w^{\star}\|_{2})^{(L-1)/L}\sum_{k=1}^{L}\|W_ {k}-\tilde{W}_{k}\|_{2}+\mathcal{O}(\xi^{2})\,.\] (36)Moving on to bounding the Frobenius norm of \(\Delta_{k}\), we observe that the dominating term of this matrix decomposes as a rank-one matrix. Thus, again by subadditivity of the operator norm, equation (36) and Corollary 1,

\[\|\Delta_{k}\|_{F} =\|W_{k+1}^{\top}\ldots W_{L}^{\top}\|_{2}\|\nabla R^{1}(\tilde{w} _{\text{prod}})^{\top}W_{1}^{\top}\ldots W_{k-1}^{\top}\|_{2}+\mathcal{O}(\xi^ {2})\] \[\leqslant\|W_{k+1}\|_{2}\ldots\|W_{L}\|_{2}\|\nabla R^{1}(\tilde{ w}_{\text{prod}})\|_{2}\|W_{1}\|_{2}\ldots\|W_{k-1}\|_{2}+\mathcal{O}(\xi^{2})\] \[\leqslant 2\Lambda(2\|w^{*}\|_{2})^{2(L-1)/L}\sum_{k=1}^{L}\|W_{k }-\tilde{W}_{k}\|_{2}+\mathcal{O}(\xi^{2})\,.\]

Then

\[\|\Delta_{k}\|_{F}^{2} \leqslant 4\Lambda^{2}(2\|w^{*}\|_{2})^{4(L-1)/L}\Big{(}\sum_{k= 1}^{L}\|W_{k}-\tilde{W}_{k}\|_{2}\Big{)}^{2}+\mathcal{O}(\xi^{3})\] \[\leqslant 4L\Lambda^{2}(2\|w^{*}\|_{2})^{4(L-1)/L}\sum_{k=1}^{L} \|W_{k}-\tilde{W}_{k}\|_{2}^{2}+\mathcal{O}(\xi^{3})\] \[\leqslant 4L\Lambda^{2}(2\|w^{*}\|_{2})^{4(L-1)/L}\sum_{k=1}^{L} \|W_{k}-\tilde{W}_{k}\|_{F}^{2}+\mathcal{O}(\xi^{3})\,.\]

Thus, by (35),

\[S(\mathcal{W})^{2}\leqslant\lim_{\xi\to 0}4L^{2}\Lambda^{2}(2\|w^{*}\|_{2})^{4(L-1)/ L}+\mathcal{O}(\xi)\leqslant 2^{6}L^{2}\Lambda^{2}\|w^{*}\|_{2}^{4-\frac{4}{L}}\,.\]

Therefore

\[S(\mathcal{W})\leqslant 8L\Lambda\|w^{*}\|_{2}^{2-\frac{2}{L}}\,,\]

which concludes the proof by the second lower bound on \(S_{\min}\) of Theorem 2, where here \(\|p\|=1\) and \(a\geqslant\lambda>0\).

### Proof of Theorem 4

To alleviate notations, we omit in this proof to write the explicit dependence of parameters on time. Starting from (5), since the gradient decomposes as a rank-one matrix, we have

\[\|\nabla_{k}R^{L}(\mathcal{W})\|_{F}^{2} =\|W_{k+1}^{\top}\ldots W_{L}^{\top}p\|_{2}^{2}\|\nabla R^{1}(w_{ \text{prod}})^{\top}W_{1}^{\top}\ldots W_{k-1}^{\top}\|_{2}^{2}\] \[\geqslant\sigma_{\min}^{2}(W_{k+1}^{\top}\ldots W_{L}^{\top})\|p \|_{2}^{2}\sigma_{\min}^{2}(W_{1}^{\top}\ldots W_{k-1}^{\top})\|\nabla R^{1}(w _{\text{prod}})\|_{2}^{2}\]

By Lemma 8,

\[\|\nabla R^{1}(w_{\text{prod}})\|_{2}^{2}\geqslant 4\lambda(R^{L}(\mathcal{W}) -R_{\min})\,.\]

Recall the notation introduced in Section 5

\[\Pi_{L:k}:=W_{L}\ldots W_{k}\quad\text{and}\quad\Pi_{k:1}:=W_{k}\ldots W_{1}\,.\]

Then

\[\|\nabla_{k}R^{L}(\mathcal{W})\|_{F}^{2}\geqslant 4\lambda\|p\|_{2}^{2}\sigma_{ \min}^{2}(\Pi_{L:k+1})\sigma_{\min}^{2}(\Pi_{k-1:1})(R^{L}(\mathcal{W})-R_{ \min})\,.\] (37)

We now let \(u>0\) (whose value will be specified next), and

\[t^{*}=\inf\left\{t\in\mathbb{R}_{+},\exists k\in\{1,\ldots,L\},\|\theta_{k} \|_{F}>\frac{1}{64}\exp(-2s^{2}-4u)\right\}.\]

Let us lower bound the minimum singular values of \(\Pi_{k}\): and \(\Pi_{:k}\) uniformly for \(t\in[0,t^{*}]\) by Lemma 3. By definition of \(t^{*}\), for \(t\leqslant t^{*}\) and for any \(k\in\{1,\ldots,L\}\), \(\|\theta_{k}\|_{2}\leqslant\frac{1}{64}\exp(-2s^{2}-4u)\). Therefore, renaming \(\tilde{C}_{1}\) to \(\tilde{C}_{4}\) the constants of Lemma 3, and taking \(u=\tilde{C}_{3}\), we get that, if

\[L\geqslant\max\left(\tilde{C}_{1},\left(\frac{\tilde{C}_{3}}{\tilde{C}_{4}} \right)^{4}\right),\quad d\geqslant\tilde{C}_{2}\,,\] (38)

then, with probability at least

\[1-8\exp\Big{(}-\frac{d\tilde{C}_{3}^{2}}{32s^{2}}\Big{)}\,,\]it holds for \(t\leqslant t^{\star}\) and for all \(k\in\{1,\ldots,L\}\) that

\[\|\Pi_{k:1}\|_{2}\leqslant 4\exp\Big{(}\frac{s^{2}}{2}+\tilde{C}_{3}\Big{)}\,,\] (39)

and

\[\sigma_{\min}(\Pi_{k:1})\geqslant\frac{1}{4}\exp\Big{(}-\frac{2s^{2}}{d}- \tilde{C}_{3}\Big{)}\,.\]

By symmetry, the same statement holds for \(\Pi_{L:k}\) instead of \(\Pi_{k:1}\) with the same probability. By the union bound, the event

\[E_{1}:=\bigcap_{1\leqslant k\leqslant L} \bigg{\{}\|\Pi_{k:1}\|_{2},\|\Pi_{L:k}\|_{2}\leqslant 4\exp\Big{(} \frac{s^{2}}{2}+\tilde{C}_{3}\Big{)}\] \[\text{ and }\sigma_{\min}(\Pi_{k:1}),\sigma_{\min}(\Pi_{L:k}) \geqslant\frac{1}{4}\exp\Big{(}-\frac{2s^{2}}{d}-\tilde{C}_{3}\Big{)}\bigg{\}}\]

holds with probability at least

\[1-16\exp\Big{(}-\frac{d\tilde{C}_{3}^{2}}{32s^{2}}\Big{)}\,.\]

Let now

\[E_{2}:=\bigg{\{}R^{L}(\mathcal{W}(0))-R_{\min}\leqslant\frac{C_{4}\lambda^{2} \|p\|_{2}^{2}}{\Lambda}\bigg{\}}\]

and \(E_{3}\) the event that the gradient flow dynamics converge to a global minimizer \(\mathcal{W}^{\text{RI}}\) of the risk satisfying the statement (8). We show next that, if \(E_{1}\) and \(E_{2}\) hold, then \(E_{3}\) must hold, which shall conclude the proof of the Theorem with

\[\begin{split} C_{1}=\max\Big{(}\tilde{C}_{1},\Big{(}\frac{ \tilde{C}_{3}}{\tilde{C}_{4}}\Big{)}^{4}\Big{)}\,,\quad C_{2}=\tilde{C}_{2}\,,\quad C_{3}=\frac{\tilde{C}_{3}^{2}}{32s^{2}}\,,\\ C_{4}=2^{-36}\exp\Big{(}-4s^{2}-\frac{16s^{2}}{\tilde{C}_{2}}-20 \tilde{C}_{3}\Big{)}\,,\quad C_{5}=\frac{1}{64}\exp(-2s^{2}-4\tilde{C}_{3})\,. \end{split}\] (40)

Under \(E_{1}\), we have

\[\sigma_{\min}^{2}(\Pi_{L:k+1})\sigma_{\min}^{2}(\Pi_{k-1:1})\geqslant\frac{1} {2^{8}}\exp\Big{(}-\frac{8s^{2}}{d}-4\tilde{C}_{3}\Big{)}\,,\]

thus by (37)

\[\|\nabla_{k}R^{L}(\mathcal{W})\|_{F}^{2}\geqslant\frac{1}{2^{6}}\exp\Big{(}- \frac{8s^{2}}{d}-4\tilde{C}_{3}\Big{)}\lambda\|p\|_{2}^{2}(R^{L}(\mathcal{W})- R_{\min})\,.\]

Therefore we get the PL condition, for \(t\leqslant t^{\star}\),

\[\sum_{k=1}^{L}\big{\|}\nabla_{k}R^{L}(\mathcal{W})\big{\|}_{F}^{2}\geqslant\mu (R^{L}(\mathcal{W})-R_{\min})\,,\]

with

\[\mu:=\frac{1}{2^{6}}\exp\Big{(}-\frac{8s^{2}}{d}-4\tilde{C}_{3}\Big{)}\lambda \|p\|^{2}L\,.\]

By Lemma 7, this implies that, for \(t\leqslant t^{\star}\),

\[R^{L}(\mathcal{W})-R_{\min}\leqslant(R^{L}(\mathcal{W}(0))-R_{\min})e^{-\mu t }\,.\] (41)

Let us now show that \(t^{\star}=\infty\). We have, since \(\theta(0)=0\) and by definition of the gradient flow,

\[\|\theta_{k}\|_{F}=\|\theta_{k}-\theta_{k}(0)\|_{F}\leqslant L\int_{0}^{t}\| \nabla_{k}R^{L}(\mathcal{W}(\tau))\|_{F}d\tau\,.\] (42)

We now upper bound the gradient as follows: starting again from (5),

\[\|\nabla_{k}R^{L}(\mathcal{W})\|_{F} =\|W_{k+1}^{\top}\ldots W_{L}^{\top}p\|_{2}\|\nabla R^{1}(w_{\text {prod}})^{\top}W_{1}^{\top}\ldots W_{k-1}^{\top}\|_{2}\] \[\leqslant\|W_{k+1}^{\top}\ldots W_{L}^{\top}\|_{2}\|p\|_{2}\|W_{1}^ {\top}\ldots W_{k-1}^{\top}\|_{2}\|\nabla R^{1}(w_{\text{prod}})\|_{2}\,.\]By Lemma 8, we get

\[\|\nabla_{k}R^{L}(\mathcal{W})\|_{F}\leqslant 2\sqrt{\Lambda}\|W_{k+1}^{\top} \ldots W_{L}^{\top}\|_{2}\|p\|_{2}\|W_{1}^{\top}\ldots W_{k-1}^{\top}\|_{2} \sqrt{R^{L}(\mathcal{W})-R_{\min}}\,.\] (43)

By (39) and (41), for \(t\leqslant t^{\star}\),

\[\|\nabla_{k}R^{L}(\mathcal{W})\|_{F}\leqslant 16\exp(s^{2}+2\tilde{C}_{3})\sqrt{ \Lambda}\|p\|_{2}\sqrt{R^{L}(\mathcal{W}(0))-R_{\min}}e^{-\frac{\mu}{2}t}\,.\] (44)

Plugging this into (42), we get, for \(t\leqslant t^{\star}\),

\[\|\theta_{k}\|_{F} \leqslant 16\exp(s^{2}+2\tilde{C}_{3})\sqrt{\Lambda}\|p\|_{2} \sqrt{R^{L}(\mathcal{W}(0))-R_{\min}}L\int_{0}^{t}e^{-\frac{\mu}{2}\tau}d\tau\] \[\leqslant\frac{32\exp(s^{2}+2\tilde{C}_{3})\sqrt{\Lambda}\|p\|_{2 }\sqrt{R^{L}(\mathcal{W}(0))-R_{\min}}L}{\mu}\] \[=\frac{2^{11}\exp(s^{2}+\frac{8s^{2}}{d}+6\tilde{C}_{3})\sqrt{ \Lambda}\sqrt{R^{L}(\mathcal{W}(0))-R_{\min}}}{\lambda\|p\|_{2}}\,,\]

where the last equality comes from the definition of \(\mu\). By \(E_{2}\) and by definition (40) of \(C_{4}\), for \(t\leqslant t^{\star}\),

\[\|\theta_{k}\|_{F}\leqslant 2^{11}\exp(s^{2}+\frac{8s^{2}}{d}+6\tilde{C}_{3}) \sqrt{C_{3}}\leqslant\frac{1}{128}\exp(-2s^{2}-4\tilde{C}_{3})\,.\]

If we had \(t^{\star}<\infty\), we would have, by definition of \(t^{\star}\), \(\|\theta_{k}(t^{\star})\|_{F}\geqslant\frac{1}{64}\exp(-2s^{2}-4\tilde{C}_{3})\). This contradicts the equation above, showing that \(t^{\star}=\infty\). By (41), this implies convergence of the risk to its minimum. We also see by (44) that \(\nabla_{k}R^{L}(\mathcal{W})\) is integrable, so \(\mathcal{W}\) has a limit as \(t\) goes to infinity. This limit \(\mathcal{W}^{\text{RI}}\) is a minimizer of the risk and satisfies the condition (8) by definition of \(t^{\star}=\infty\).

Extension to multivariate regression.We emphasize that a very similar proof holds in the case of multivariate regression, where the neural network is defined as the linear map from \(\mathbb{R}^{d}\) to \(\mathbb{R}^{d}\)

\[x\mapsto W_{L}\ldots W_{1}x\,,\]

and we now aim at minimizing the mean squared error loss

\[R^{L}(\mathcal{W})=\frac{1}{n}\|Y-W_{L}\ldots W_{1}X^{\top}\|_{2}^{2}\,,\]

where \(X,Y\in\mathbb{R}^{n\times d}\). In this case, as shown in Zou et al. (2020); Sander et al. (2022), the following bounds on the gradient hold:

\[\|\nabla_{k}R^{L}(\mathcal{W})\|_{F}^{2}\geqslant 4\lambda\sigma_{\min}^{2}( \Pi_{L:k+1})\sigma_{\min}^{2}(\Pi_{k-1:1})(R(\mathcal{W})-R_{\min})\,,\]

and

\[\|\nabla_{k}R^{L}(\mathcal{W})\|_{F}^{2}\leqslant 4\Lambda\|\Pi_{L:k+1}\|_{2}^{2 }\|\Pi_{k-1:1}\|_{2}^{2}(R(\mathcal{W})-R_{\min})\,.\]

Comparing with (37) and (43), the only difference is the absence of \(\|p\|_{2}\) here. From there, the same computations as above hold (taking \(\|p\|_{2}=1\)), and give the following result.

**Theorem 5**.: _There exist \(C_{1},\ldots,C_{5}>0\) depending only on \(s\) such that, if \(L\geqslant C_{1}\) and \(d\geqslant C_{2}\), then, with probability at least_

\[1-16\exp(-C_{3}d)\,,\]

_if_

\[R^{L}(\mathcal{W}(0))-R_{\min}\leqslant\frac{C_{4}\lambda^{2}\|p\|_{2}^{2}}{ \Lambda}\,,\]

_the gradient flow dynamics (4) converge to a global minimizer \(\mathcal{W}^{\text{RI}}\) of the risk. Furthermore, the minimizer \(\mathcal{W}^{\text{RI}}\) satisfies_

\[W_{k}^{\text{RI}}=I+\frac{s}{\sqrt{Ld}}N_{k}+\frac{1}{L}\theta_{k}^{\text{RI}} \quad\text{with}\quad\|\theta_{k}^{\text{RI}}\|_{F}\leqslant C_{5}\,,\quad 1 \leqslant k\leqslant L\,.\]

[MISSING_PAGE_FAIL:35]

by using \(\ln(1+x)\leqslant x\). Thus

\[\mathbb{P}\Big{(}\max_{1\leqslant k\leqslant L}\frac{\|h_{k}\|_{2}^ {2}}{\|h_{0}\|_{2}^{2}}\geqslant\exp(s^{2}+2u)\Big{)} \leqslant\mathbb{P}\Big{(}\max_{1\leqslant k\leqslant L}S_{k,1}+S_{ k,2}\geqslant s^{2}+2u\Big{)}\] \[\leqslant\mathbb{P}\Big{(}\max_{1\leqslant k\leqslant L}S_{k,1} \geqslant s^{2}+u\Big{)}+\mathbb{P}\Big{(}\max_{1\leqslant k\leqslant L}S_{k,2 }\geqslant u\Big{)}\]

by the union bound. We now study the two deviation probabilities separately. Beginning by the first one, recall that \(\frac{Ld}{s^{2}}S_{L,1}\) follows a \(\chi^{2}(Ld)\) distribution. Chi-squared random variables are sub-exponential, and more precisely satisfy the following property (Ghosh, 2021): if \(X\sim\chi^{2}(c)\) and \(u>0\), then

\[\mathbb{P}(X\geqslant c+u)\leqslant\exp\Big{(}-\frac{u^{2}}{4(c+u)}\Big{)}\,.\] (47)

Since the \(S_{k,1}\) are increasing, we have, for \(u>0\),

\[\mathbb{P}\Big{(}\max_{1\leqslant k\leqslant L}S_{k,1}\geqslant s ^{2}+u\Big{)} =\mathbb{P}(S_{L,1}\geqslant s^{2}+u)\] \[=\mathbb{P}\Big{(}\frac{Ld}{s^{2}}S_{L,1}\geqslant Ld+\frac{Ldu }{s^{2}}\Big{)}\] \[\leqslant\exp\Big{(}-\frac{L^{2}d^{2}u^{2}}{4s^{4}(Ld+\frac{Ldu} {s^{2}})}\Big{)}=\exp\Big{(}-\frac{Ldu^{2}}{4s^{4}+us^{2}}\Big{)}\,.\]

Moving on to \(S_{k,2}\), we have, for \(u,\lambda>0\),

\[\mathbb{P}\Big{(}\max_{1\leqslant k\leqslant L}S_{k,2}\geqslant u\Big{)}= \mathbb{P}\Big{(}\max_{1\leqslant k\leqslant L}\exp(\lambda S_{k,2})\geqslant \exp(\lambda u)\Big{)}\,.\]

Furthermore, \(\exp(\lambda S_{k,2})\) is a sub-martingale, since \(S_{k,2}\) is a martingale and by Jensen's inequality:

\[\mathbb{E}(\exp(\lambda S_{k+1,2})|\mathcal{F}_{k})\geqslant\exp(\lambda \mathbb{E}(S_{k+1,2}|\mathcal{F}_{k}))=\exp(\lambda S_{k,2})\,.\]

Thus, by Doob's martingale inequality,

\[\mathbb{P}\Big{(}\max_{1\leqslant k\leqslant L}S_{k,2}\geqslant u\Big{)} \leqslant\mathbb{E}(\exp(\lambda S_{L,2}))\exp(-\lambda u)\,.\]

Furthermore, since \(S_{L,2}\sim\mathcal{N}(0,\frac{4s^{2}}{d})\),

\[\mathbb{E}(\exp(\lambda S_{L,2}))=\exp\Big{(}\frac{4s^{2}\lambda^{2}}{d}\Big{)}\,.\]

Therefore,

\[\mathbb{P}\Big{(}\max_{1\leqslant k\leqslant L}S_{k,2}\geqslant u\Big{)} \leqslant\exp\Big{(}-\lambda u+\frac{4s^{2}\lambda^{2}}{d}\Big{)}\,.\]

This quantity is minimal for \(\lambda=\frac{du}{8s^{2}}\). We get, for all \(u>0\),

\[\mathbb{P}\Big{(}\max_{1\leqslant k\leqslant L}S_{k,2}\geqslant u\Big{)} \leqslant\exp\Big{(}-\frac{du^{2}}{16s^{2}}\Big{)}\,.\] (48)

Therefore, for any \(u>0\),

\[\mathbb{P}\Big{(}\max_{1\leqslant k\leqslant L}\frac{\|h_{k}\|_{2}^{2}}{\|h_ {0}\|_{2}^{2}}\geqslant\exp(s^{2}+2u)\Big{)}\leqslant\exp\Big{(}-\frac{du^{2} }{16s^{2}}\Big{)}+\exp\Big{(}-\frac{Ldu^{2}}{4s^{4}+us^{2}}\Big{)}\,.\] (49)

To conclude on the operator norm of \(\Pi_{k:1}\), consider \(\Sigma\) a \(1/2\)-net of the unit sphere of \(\mathbb{R}^{d}\). By Vershynin (2018, Corollary 4.2.13), it is possible to take such a net of cardinality \(5^{d}\). Let us show that, for any \(u\in\mathbb{R}\),

\[\mathbb{P}\Big{(}\max_{1\leqslant k\leqslant L}\|\Pi_{k:1}\|_{2}\geqslant 2 \exp\Big{(}\frac{s^{2}}{2}+u\Big{)}\Big{)}\leqslant\mathbb{P}\Big{(}\bigcup_{h _{0}\in\Sigma}\max_{1\leqslant k\leqslant L}\|\Pi_{k:1}h_{0}\|_{2}\geqslant \exp\Big{(}\frac{s^{2}}{2}+u\Big{)}\Big{)}\,.\] (50)

[MISSING_PAGE_FAIL:37]

where the cardinality of the net is given by Vershynin (2018, Corollary 4.2.13). We now take a fixed \(h_{0}\in\mathbb{R}^{d}\), and compute

\[\mathbb{P}\Big{(}\min_{1\leqslant k\leqslant L}\frac{\|h_{k}\|_{2}}{\|h_{0}\|_{ 2}}\leqslant 2m_{s,u}\Big{)}=\mathbb{P}\Big{(}\min_{1\leqslant k\leqslant L} \frac{\|h_{k}\|_{2}^{2}}{\|h_{0}\|_{2}^{2}}\leqslant 4m_{s,u}^{2}\Big{)}\,.\]

Denote by \(E\) the event

\[\bigcap_{1\leqslant k\leqslant L}\Big{\{}Y_{k,2}\geqslant-\frac{1}{2}\Big{\}}\,.\]

We have, by (45),

\[\mathbb{P}\Big{(}\min_{1\leqslant k\leqslant L}\frac{\|h_{k}\|_{ 2}^{2}}{\|h_{0}\|_{2}^{2}}\leqslant 4m_{s,u}^{2}\Big{)} =\mathbb{P}\Big{(}\min_{1\leqslant k\leqslant L}\ln(\|h_{k}\|_{2} ^{2})-\ln(\|h_{0}\|_{2}^{2})\leqslant\ln(4m_{s,u}^{2})\Big{)}\] \[\leqslant\mathbb{P}\Big{(}\min_{1\leqslant k\leqslant L}\sum_{j= 0}^{k-1}\ln(1+Y_{j,2})\leqslant\ln(4m_{s,u}^{2})\Big{)}\] \[\leqslant\mathbb{P}\Big{(}\Big{\{}\min_{1\leqslant k\leqslant L} \sum_{j=0}^{k-1}\ln(1+Y_{j,2})\leqslant\ln(4m_{s,u}^{2})\Big{\}}\cap E\Big{)}+ \mathbb{P}(\bar{E})\,.\]

Using the inequality \(\ln(1+x)\geqslant x-x^{2}\) for \(x\geqslant-\nicefrac{{1}}{{2}}\), we obtain

\[\mathbb{P}\Big{(}\frac{\|h_{L}\|_{2}^{2}}{\|h_{0}\|_{2}^{2}} \leqslant 4m_{s,u}^{2}\Big{)} \leqslant\mathbb{P}\Big{(}\Big{\{}\min_{1\leqslant k\leqslant L} \sum_{j=0}^{k-1}(Y_{j,2}-Y_{j,2}^{2})\leqslant\ln(4m_{s,u}^{2})\Big{\}}\cap E \Big{)}+\mathbb{P}(\bar{E})\,.\]

Thus, by the union bound,

\[\mathbb{P}\Big{(}\min_{1\leqslant k\leqslant L}\frac{\|h_{L}\|_{ 2}^{2}}{\|h_{0}\|_{2}^{2}}\leqslant 4m_{s,u}^{2}\Big{)} \leqslant\mathbb{P}\Big{(}\min_{1\leqslant k\leqslant L}\sum_{j= 0}^{k-1}(Y_{j,2}-Y_{j,2}^{2})\leqslant\ln(4m_{s,u}^{2})\Big{)}+\sum_{k=0}^{ L-1}\mathbb{P}\Big{(}Y_{k,2}<-\frac{1}{2}\Big{)}\] \[=:P_{1}+P_{2}\,.\]

We handle both terms separately. Beginning by the second term, we have, for \(k\in\{1,\ldots L\}\),

\[\mathbb{P}\Big{(}Y_{k,2}<-\frac{1}{2}\Big{)}\leqslant\exp\Big{(}-\frac{Ld}{32 s^{2}}\Big{)}\,,\]

where we used (46) and the tail bound \(\mathbb{P}(N\geqslant u)\leqslant e^{-u^{2}/2}\) if \(N\sim\mathcal{N}(0,1)\). Moving on to the first term, we have

\[P_{1}=\mathbb{P}\Big{(}\min_{1\leqslant k\leqslant L}S_{k,2}-S_{k,3}\leqslant \ln(4m_{s,u}^{2})\Big{)}\,,\]

where we let \(S_{k,3}=\sum_{j=0}^{k-1}Y_{j,2}^{2}\). By definition of \(m_{s,u}\), we have

\[\ln(4m_{s,u}^{2})=-\frac{4s^{2}}{d}-2u\,.\]

Thus we can split the probability into two parts by the union bound:

\[P_{1}\leqslant\mathbb{P}\Big{(}\min_{1\leqslant k\leqslant L}S_{k,2}\leqslant -u\Big{)}+\mathbb{P}\Big{(}\max_{1\leqslant k\leqslant L}S_{k,3}\geqslant \frac{4s^{2}}{d}+u\Big{)}\,.\]

Let us bound each probability separately. We have, for \(u>0\),

\[\mathbb{P}\Big{(}\min_{1\leqslant k\leqslant L}S_{k,2}\leqslant-u\Big{)}= \mathbb{P}\Big{(}\max_{1\leqslant k\leqslant L}-S_{k,2}\geqslant u\Big{)}= \mathbb{P}\Big{(}\max_{1\leqslant k\leqslant L}S_{k,2}\geqslant u\Big{)} \leqslant\exp\Big{(}-\frac{du^{2}}{16s^{2}}\Big{)}\,,\]

by symmetry of \(S_{k,2}\) and by (48). Moving on to \(S_{k,3}\), we have that

\[Y_{j,2}^{2}|\mathcal{F}_{j}\sim\frac{4s^{2}}{Ld}\mathcal{N}(0,1)^{2}=\frac{4s^{ 2}}{Ld}\chi^{2}(1)\,,\]thus

\[\frac{Ld}{4s^{2}}S_{k,3}\sim\chi^{2}(L)\,.\]

By monotonicity of \(S_{k,3}\) and by (47),

\[\mathbb{P}\Big{(}\max_{1\leqslant k\leqslant L}S_{k,3}\geqslant \frac{4s^{2}}{d}+u\Big{)} =\mathbb{P}\Big{(}S_{L,3}\geqslant\frac{4s^{2}}{d}+u\Big{)}\] \[=\mathbb{P}\Big{(}\frac{Ld}{4s^{2}}S_{L,3}\geqslant L+\frac{Ldu}{4 s^{2}}\Big{)}\] \[\leqslant\exp\Big{(}-\frac{L^{2}d^{2}u^{2}}{64s^{4}(L+\frac{Ldu}{4 s^{2}})}\Big{)}=\exp\Big{(}-\frac{Ld^{2}u^{2}}{16(4s^{2}+du)}\Big{)}\,.\]

Putting everything together, we proved that

\[\mathbb{P}(\bar{B}\cap A)\leqslant\Big{(}\frac{2M_{s,u}}{m_{s,u}}+1\Big{)}^{ d}(P_{1}+P_{2})=\exp\Big{(}d\ln\Big{(}\frac{2M_{s,u}}{m_{s,u}}+1\Big{)}\Big{)}(P_ {1}+P_{2})\,,\]

with

\[P_{1}\leqslant\exp\Big{(}-\frac{du^{2}}{16s^{2}}\Big{)}+\exp\Big{(}-\frac{Ld ^{2}u^{2}}{16(4s^{2}+du)}\Big{)}\]

and

\[P_{2}\leqslant L\exp\Big{(}-\frac{Ld}{32s^{2}}\Big{)}\,.\]

Bounding the probability of failure.Putting together the two main bounds we showed, we have

\[\mathbb{P}(\bar{A}\cup\bar{B})=\mathbb{P}(\bar{A})+\mathbb{P}( \bar{B}\cup A)\] \[\leqslant 5^{d}\Big{(}\exp\Big{(}-\frac{du^{2}}{16s^{2}}\Big{)}+ \exp\Big{(}-\frac{Ldu^{2}}{4s^{4}+us^{2}}\Big{)}\Big{)}\] \[+\exp\Big{(}d\ln\Big{(}\frac{2M_{s,u}}{m_{s,u}}+1\Big{)}\Big{)} \Big{(}\exp\Big{(}-\frac{du^{2}}{16s^{2}}\Big{)}+\exp\Big{(}-\frac{Ld^{2}u^{2 }}{16(4s^{2}+du)}\Big{)}+L\exp\Big{(}-\frac{Ld}{32s^{2}}\Big{)}\Big{)}.\]

This expression is valid for all \(u,s>0\) and \(L,d\geqslant 1\). We now simplify the expression of our upper bound by algebraic computations using the assumptions of the Theorem. To somewhat alleviate the technicality of the computations, we stop at this point tracking some of the explicit constants and let \(C\) denote a positive absolute constant that might vary from equality to equality. We show next that the conditions

\[L\geqslant C,\quad d\geqslant C,\quad u\geqslant C\max(s^{2},s^{-2/3}),\quad u \leqslant CL^{1/4}\] (51)

imply that

\[\mathbb{P}(\bar{A}\cup\bar{B})\leqslant 8\exp\Big{(}-\frac{du^{2}}{32s^{2}} \Big{)}\,.\]

This shall conclude the proof of the Lemma with

\[C_{1}=C,\quad C_{2}=C,\quad C_{3}=C\max(s^{2},s^{-2/3}),\quad C_{4}=C\,.\]

First note that the conditions (51) imply that

\[u\geqslant Cs\quad\text{and}\quad u\geqslant C\,.\] (52)

We study the terms of the bound on \(\mathbb{P}(\bar{A}\cup\bar{B})\) one by one. First, we have by (52) that

\[5^{d}\exp\Big{(}-\frac{du^{2}}{16s^{2}}\Big{)}=\exp\Big{(}d\ln(5)-\frac{du^{2 }}{16s^{2}}\Big{)}\leqslant\exp\Big{(}-\frac{du^{2}}{32s^{2}}\Big{)}\,.\]

Next,

\[5^{d}\exp\Big{(}-\frac{Ldu^{2}}{4s^{4}+us^{2}}\Big{)} \leqslant 5^{d}\exp\Big{(}-\frac{Ldu^{2}}{8us^{2}}\Big{)}\] \[=5^{d}\exp\Big{(}-\frac{Ldu}{8s^{2}}\Big{)}\] \[=\exp\Big{(}d\ln(5)-\frac{Ldu}{8s^{2}}\Big{)}\] \[\leqslant\exp\Big{(}-\frac{Ldu}{16s^{2}}\Big{)}\,,\]where the first inequality uses \(u\geqslant s^{2}\) by (51), and the last one uses that \(\frac{Lu}{s^{2}}\geqslant C\). This is true since \(L\geqslant C\) and \(u\geqslant Cs^{2}\) by (51). Then we can bound this term by

\[\exp\Big{(}-\frac{du^{2}}{32s^{2}}\Big{)}\,,\]

since \(u\leqslant CL\) which is implied by the assumption \(u\leqslant CL^{1/4}\) in (51). Next,

\[\ln\Big{(}\frac{2M_{s,u}}{m_{s,u}}+1\Big{)} =\ln\Big{(}8\exp\Big{(}\frac{s^{2}}{2}+\frac{2s^{2}}{d}+2u\Big{)}+ 1\Big{)}\] \[\leqslant\ln\Big{(}9\exp\Big{(}\frac{s^{2}}{2}+\frac{2s^{2}}{d}+2u \Big{)}\Big{)}\,,\]

where we used that the exponential of a positive term is greater than \(1\). Then

\[\ln\Big{(}\frac{2M_{s,u}}{m_{s,u}}+1\Big{)}\leqslant\ln(9)+\frac{s^{2}}{2}+ \frac{2s^{2}}{d}+2u\leqslant 3+3u\,,\] (53)

since \(u\geqslant s^{2}\), \(d\geqslant 4\) by (51). Thus we can bound the three remaining terms appearing in the bound of \(\mathbb{P}(\bar{A}\cup\bar{B})\), as follows:

\[\exp\Big{(}d\ln\Big{(}\frac{2M_{s,u}}{m_{s,u}}+1\Big{)}\Big{)}\exp\Big{(}- \frac{du^{2}}{16s^{2}}\Big{)}\leqslant\exp\Big{(}3d+3du-\frac{du^{2}}{16s^{2}} \Big{)}\leqslant\exp\Big{(}-\frac{du^{2}}{32s^{2}}\Big{)}\,,\]

since

\[u^{2}\geqslant Cs^{2}(1+u)\,.\]

This is the case by (52), which implies that

\[u^{2}=\frac{1}{2}u^{2}+\frac{1}{2}uu\geqslant\frac{C^{2}}{2}s^{2}+\frac{C}{2 }s^{2}u=\frac{C^{2}+C}{2}s^{2}(1+u)\,.\]

Next, by (53),

\[\exp\Big{(}d\ln\Big{(}\frac{2M_{s,u}}{m_{s,u}}+1\Big{)}\Big{)}\exp \Big{(}-\frac{Ld^{2}u^{2}}{16(4s^{2}+du)}\Big{)} \leqslant\exp\Big{(}3d+3du-\frac{Ld^{2}u^{2}}{16(4s^{2}+du)} \Big{)}\] \[\leqslant\exp\Big{(}3d+3du-\frac{Ld^{2}u^{2}}{32du}\Big{)}\,,\]

since \(du\geqslant u\geqslant Cs^{2}\) by (51). Thus

\[\exp\Big{(}d\ln\Big{(}\frac{2M_{s,u}}{m_{s,u}}+1\Big{)}\Big{)}\exp\Big{(}- \frac{Ld^{2}u^{2}}{16(4s^{2}+du)}\Big{)}\leqslant\exp\Big{(}3d+3du-\frac{Ldu}{ 32}\Big{)}\leqslant\exp\Big{(}-\frac{Ldu}{64}\Big{)}\,,\]

where the second inequality uses that \(L\geqslant C\) by (51), and \(Lu\geqslant C\) since \(L\geqslant C\) and \(u\geqslant C\) by (51) and (52). We can also bound this term by

\[\exp\Big{(}-\frac{du^{2}}{32s^{2}}\Big{)}\,,\]

since \(L\geqslant Cu/s^{2}\), using first that \(L\geqslant Cu^{4}\) then that \(u\geqslant Cs^{-2/3}\), by (51). Regarding the last term of the bound of \(\mathbb{P}(\bar{A}\cup\bar{B})\), we have by (53) that

\[L\exp\Big{(}d\ln\Big{(}\frac{2M_{s,u}}{m_{s,u}}+1\Big{)}\Big{)}\exp\Big{(}- \frac{Ld}{32s^{2}}\Big{)}\leqslant L\exp\Big{(}3d+3du-\frac{Ld}{32s^{2}}\Big{)}\] (54)

Let us show that this implies that

\[L\exp\Big{(}d\ln\Big{(}\frac{2M_{s,u}}{m_{s,u}}+1\Big{)}\Big{)}\exp\Big{(}- \frac{Ld}{32s^{2}}\Big{)}\leqslant L\exp\Big{(}-\frac{\sqrt{L}du^{2}}{32s^{2}} \Big{)}\,.\] (55)

This statement is true because the three terms appearing inside the exponential of the right-hand side of (54) can be bounded by \(\frac{C\sqrt{L}du^{2}}{s^{2}}\). More precisely, we have

\[\frac{Ld}{32s^{2}}\geqslant\frac{C\sqrt{L}du^{2}}{s^{2}}\Leftrightarrow\sqrt{ L}\geqslant Cu^{2}\,,\]which holds by (51);

\[3du\leqslant\frac{C\sqrt{L}du^{2}}{s^{2}}\Leftrightarrow Cs^{2}\leqslant\sqrt{L}u\,,\]

which is implied by \(L\geqslant C\) and \(u\geqslant Cs^{2}\) by (51); and

\[3d\leqslant\frac{C\sqrt{L}du^{2}}{s^{2}}\Leftrightarrow Cs^{2}\leqslant\sqrt{ L}u^{2}\,,\]

which is implied by \(L\geqslant C\) and \(u\geqslant Cs\) by (51) and (52). Finally, we use Lemma 5 to bound the right-hand side of (55) by

\[4\exp\Big{(}-\frac{du^{2}}{32s^{2}}\Big{)}\,.\]

This is possible for \(L\geqslant C\) and \(\frac{du^{2}}{32s^{2}}\geqslant 1\), which is implied by \(d\geqslant C\) and \(u\geqslant Cs\), by (51) and (52). Collecting everything, we bounded \(\mathbb{P}(\bar{A}\cup\bar{B})\) by

\[8\exp\Big{(}-\frac{du^{2}}{32s^{2}}\Big{)}\,,\]

which concludes the proof when \(\theta=0\).

Summary when \(\theta=0\).In summary, we proved so far the following result: there exist \(C_{1},\ldots,C_{4}>0\) depending only on \(s\) such that, if

\[L\geqslant C_{1}\,,\quad d\geqslant C_{2}\,,\quad u\in[C_{3},C_{4}L^{1/4}]\,,\]

then, with probability at least

\[1-8\exp\Big{(}-\frac{du^{2}}{32s^{2}}\Big{)}\,,\]

it holds for all \(k\in\{1,\ldots,L\}\) that

\[\Big{\|}\Big{(}I+\frac{s}{\sqrt{L}d}N_{k}\Big{)}\ldots\Big{(}I+\frac{s}{\sqrt{ L}d}N_{1}\Big{)}\Big{\|}_{2}\leqslant m_{s,u}\,,\]

and

\[\sigma_{\min}\Big{(}\Big{(}I+\frac{s}{\sqrt{L}d}N_{k}\Big{)}\ldots\Big{(}I+ \frac{s}{\sqrt{L}d}N_{1}\Big{)}\Big{)}\geqslant M_{s,u}\,.\]

Conclusion for arbitrary \(\theta\).The conclusion is a direct application of Lemma 9, with \(W_{k}=I+\frac{s}{\sqrt{L}d}N_{k}\). The size of the admissible ball for \(\theta\) is

\[\frac{m_{s,u}^{2}}{4M_{s,u}^{2}}=\frac{1}{64}\exp\Big{(}-s^{2}-\frac{4s^{2}}{ d}-4u\Big{)}\geqslant\frac{1}{64}\exp(-2s^{2}-4u)\]

for \(d\geqslant 4\), which concludes the proof.

Comparison with the bounds of Zhang et al. (2022).Theorem 1 of Zhang et al. (2022) gives an upper bound on the singular values of residual networks at initialization, and their Theorem 2 gives a lower bound on the norm of the activations. Comparing with our results, we note two important differences. First, their probability of failure grows quadratically with the depth, whereas ours is independent of depth. This is achieved by a more precise martingale argument making use of Doob's martingale inequality. Second, their lower bound incorrectly assumes that \(\chi^{2}\) random variables are sub-Gaussian (see equation (21) of their paper), while in fact they are only sub-exponential (Ghosh, 2021). Finally, their upper bound holds for the product

\[\Big{(}I+\frac{s}{\sqrt{L}d}N_{k}+\frac{1}{L}\theta_{k}\Big{)}\ldots\Big{(}I+ \frac{s}{\sqrt{L}d}N_{j}+\frac{1}{L}\theta_{j}\Big{)}\]

for any \(1\leqslant j\leqslant k\leqslant L\), which could seem stronger than our result stated for \(j=1\). In fact, both statements are equivalent, because it is possible to deduce the statement for any \(j\) by combining the upper bound and the lower bound for \(j=1\). The precise argument is given in the beginning of the proof of our Lemma 9.

### Proof of Corollary 3

The beginning of the proof is very similar to the one of Corollary 2. Denoting \(\mathcal{W}:=\mathcal{W}^{\mathsf{RI}}\), we have

\[S(\mathcal{W})^{2}=\lim_{\xi\to 0}\sup_{\|W_{k}-\tilde{W}_{k}\|_{P}\leqslant\xi} \frac{\sum_{k=1}^{L}\|\nabla_{k}R^{L}(\mathcal{W})-\nabla_{k}R^{L}(\tilde{ \mathcal{W}})\|_{F}^{2}}{\sum_{k=1}^{L}\|W_{k}-\tilde{W}_{k}\|_{F}^{2}}\,,\] (56)

with

\[\Delta_{k} :=\nabla_{k}R^{L}(\mathcal{W})-\nabla_{k}R^{L}(\tilde{\mathcal{W}})\] \[=-W_{k+1}^{\top}\dots W_{L}^{\top}p\nabla R^{1}(\tilde{w}_{\sf prod })^{\top}W_{1}^{\top}\dots W_{k-1}^{\top}+\mathcal{O}(\xi^{2})\,,\]

and

\[\nabla R^{1}(\tilde{w}_{\sf prod})=\frac{2}{n}X^{\top}X\Big{(}\sum_{k=1}^{L}W_ {1}^{\top}\dots W_{k-1}^{\top}(\tilde{W}_{k}^{\top}-W_{k}^{\top})W_{k+1}^{\top }\dots W_{L}^{\top}p\Big{)}+\mathcal{O}(\xi^{2})\,.\]

At this point, the proofs diverge. We have, by subadditivity of the operator norm,

\[\|\nabla R^{1}(\tilde{w}_{\sf prod})\|_{2}\leqslant 2\Lambda\sum_{k=1}^{L}\|W_{k- 1}\dots W_{1}\|_{2}\|\tilde{W}_{k}-W_{k}\|_{2}\|W_{L}\dots W_{k+1}\|_{2}\|p\|_{ 2}+\mathcal{O}(\xi^{2})\,.\]

Let us now briefly recall the outline of the proof of Theorem 4, which will be useful in bounding the quantity above. The proof shows the existence of \(\tilde{C}_{3}\) depending only on \(s\) such that, with high probability (which is exactly the probability in the statement of the Theorem), we have for all \(t\geqslant 0\) and \(k\in\{1,\dots,L\}\) that

\[\|W_{k-1}(t)\dots W_{1}(t)\|_{2}\leqslant 4\exp\Big{(}\frac{s^{2}}{2}+\tilde{C }_{3}\Big{)}\quad\text{and}\quad\|W_{L}(t)\dots W_{k+1}(t)\|_{2}\leqslant 4 \exp\Big{(}\frac{s^{2}}{2}+\tilde{C}_{3}\Big{)}\,,\]

as well as

\[\sigma_{\min}(W_{k}(t)\dots W_{1}(t))\geqslant\frac{1}{4}\exp\Big{(}-\frac{2 s^{2}}{d}-\tilde{C}_{3}\Big{)}\,.\] (57)

Under this high-probability event, the proof of Theorem 4 shows convergence of the gradient flow to \(\mathcal{W}=\mathcal{W}^{\mathsf{RI}}\). In particular, this means that \(\mathcal{W}\) also verifies the bounds on the operator norm of the matrix products. We therefore obtain

\[\|\nabla R^{1}(\tilde{w}_{\sf prod})\|_{2} \leqslant 2\Lambda\sum_{k=1}^{L}4\exp\Big{(}\frac{s^{2}}{2}+ \tilde{C}_{3}\Big{)}\|\tilde{W}_{k}-W_{k}\|_{2}4\exp\Big{(}\frac{s^{2}}{2}+ \tilde{C}_{3}\Big{)}\|p\|_{2}+\mathcal{O}(\xi^{2})\] \[=32\exp(s^{2}+2\tilde{C}_{3})\Lambda\|p\|_{2}\sum_{k=1}^{L}\| \tilde{W}_{k}-W_{k}\|_{2}+\mathcal{O}(\xi^{2})\,.\]

Moving on to bounding the Frobenius norm of \(\Delta_{k}\), we have

\[\|\Delta_{k}\|_{F} =\|W_{k+1}^{\top}\dots W_{L}^{\top}p\|_{2}\|\nabla R^{1}(\tilde{ w}_{\sf prod})^{\top}W_{1}^{\top}\dots W_{k-1}^{\top}\|_{2}+\mathcal{O}(\xi^{2})\] \[\leqslant\|W_{L}\dots W_{k+1}\|_{2}\|p\|_{2}\|\nabla R^{1}(\tilde{ w}_{\sf prod})\|_{2}\|W_{k-1}\dots W_{1}\|_{2}+\mathcal{O}(\xi^{2})\] \[\leqslant 2^{9}\exp(2s^{2}+4\tilde{C}_{3})\Lambda\|p\|_{2}^{2} \sum_{k=1}^{L}\|W_{k}-\tilde{W}_{k}\|_{2}+\mathcal{O}(\xi^{2})\,,\]

by bounding the three norms by the expressions given above. Then

\[\|\Delta_{k}\|_{F}^{2} \leqslant 2^{18}\exp(4s^{2}+8\tilde{C}_{3})\Lambda^{2}\|p\|_{2}^{4} \Big{(}\sum_{k=1}^{L}\|W_{k}-\tilde{W}_{k}\|_{2}\Big{)}^{2}+\mathcal{O}(\xi^{3})\] \[\leqslant 2^{18}\exp(4s^{2}+8\tilde{C}_{3})L\Lambda^{2}\|p\|_{2}^{4} \sum_{k=1}^{L}\|W_{k}-\tilde{W}_{k}\|_{2}^{2}+\mathcal{O}(\xi^{3})\,.\] \[\leqslant 2^{18}\exp(4s^{2}+8\tilde{C}_{3})L\Lambda^{2}\|p\|_{2}^{4} \sum_{k=1}^{L}\|W_{k}-\tilde{W}_{k}\|_{F}^{2}+\mathcal{O}(\xi^{3})\,.\]Thus, by (56),

\[S(\mathcal{W})^{2}\leqslant\lim_{\xi\to 0}2^{18}\exp(4s^{2}+8\tilde{C}_{3})L^{2} \Lambda^{2}\|p\|_{2}^{4}+\mathcal{O}(\xi)\,.\]

Therefore

\[S(\mathcal{W})\leqslant 2^{9}\exp(2s^{2}+4\tilde{C}_{3})L\Lambda\|p\|_{2}^{2}\,.\] (58)

To conclude, we need to upper bound \(\|p\|_{2}\) by a constant times \(\|w^{\star}\|_{2}\). To this aim, we leverage the bound from the assumptions of Theorem 4 on the risk at initialization, to show that \(\|p\|_{2}\) cannot be too far away from \(\|w^{\star}\|_{2}\). More precisely, by Lemma 8, since the covariance matrix \(X^{\top}X\) is full rank,

\[R^{L}(\mathcal{W}(0))-R_{\min} =\frac{1}{n}\|X(w_{\text{prod}}(0)-w^{\star})\|_{2}^{2}\] \[=\frac{1}{n}(w_{\text{prod}}(0)-w^{\star})^{\top}X^{\top}X(w_{ \text{prod}}(0)-w^{\star})\] \[\geqslant\lambda\|w_{\text{prod}}(0)-w^{\star}\|_{2}^{2}\,.\]

Thus

\[\sqrt{R^{L}(\mathcal{W}(0))-R_{\min}}\geqslant\sqrt{\lambda}\|w_{\text{prod} }(0)-w^{\star}\|_{2}\,.\]

Then, by the triangular inequality,

\[\|w^{\star}\|_{2} \geqslant\|w_{\text{prod}}(0)\|_{2}-\|w_{\text{prod}}(0)-w^{ \star}\|_{2}\] \[\geqslant\|W_{1}^{\top}(0)\ldots W_{L}^{\top}(0)p\|_{2}-\sqrt{ \frac{R^{L}(\mathcal{W}(0))-R_{\min}}{\lambda}}\] \[\geqslant\sigma_{\min}(W_{L}(0)\ldots W_{1}(0))\|p\|_{2}-\sqrt{ \frac{R^{L}(\mathcal{W}(0))-R_{\min}}{\lambda}}\] \[\geqslant\frac{1}{4}\exp\Big{(}-\frac{2s^{2}}{d}-\tilde{C}_{3} \Big{)}\|p\|_{2}-\sqrt{C_{3}}\sqrt{\frac{\lambda}{\Lambda}}\|p\|_{2}\,,\]

by (57) and by the assumption of Theorem 4 on the risk at initialization. We now note that the value of \(C_{3}\) is given by (40) as

\[C_{3}=2^{-36}\exp\Big{(}-4s^{2}-\frac{16s^{2}}{\tilde{C}_{2}}-20\tilde{C}_{3} \Big{)}\,,\]

where \(\tilde{C}_{2}\leqslant d\) by (38). Thus

\[\sqrt{C_{3}}\sqrt{\frac{\lambda}{\Lambda}}\leqslant\sqrt{C_{3}}=2^{-18}\exp(- 2s^{2}-\frac{8s^{2}}{\tilde{C}_{2}}-10\tilde{C}_{3})<\frac{1}{4}\exp\Big{(}- \frac{2s^{2}}{d}-\tilde{C}_{3}\Big{)}\,.\]

Denoting

\[C^{\prime}=\frac{1}{4}\exp\Big{(}-\frac{2s^{2}}{d}-\tilde{C}_{3}\Big{)}-\sqrt{ C_{3}}\,\in(0,1)\,,\]

we therefore obtain that \(\|w^{\star}\|_{2}\geqslant C^{\prime}\|p\|_{2}\). Therefore, by (58),

\[S(\mathcal{W})\leqslant 2^{9}\exp(2s^{2}+4\tilde{C}_{3})(C^{\prime})^{-2+ \frac{2}{L}}L\Lambda\|w^{\star}\|_{2}^{2-\frac{2}{L}}\|p\|_{2}^{\frac{2}{L}}\,.\]

Thus, by the second lower bound on \(S_{\min}\) from Theorem 2, and since \(a\geqslant\lambda>0\),

\[\frac{S(\mathcal{W})}{S_{\min}}\leqslant 2^{8}\exp(2s^{2}+4\tilde{C}_{3})(C^{ \prime})^{-2+\frac{2}{L}}\frac{\Lambda}{\lambda}\,,\]

which concludes the proof by setting

\[C:=2^{8}\exp(2s^{2}+4\tilde{C}_{3})(C^{\prime})^{-2}\geqslant 2^{8}\exp(2s^{2}+4 \tilde{C}_{3})(C^{\prime})^{-2+\frac{2}{L}}\,.\]

## Appendix C Experimental details, additional plots, and additional comments

Our code is available at https://github.com/PierreMarion23/implicit-reg-sharpness. Our framework for experiments is JAX (Bradbury et al., 2018). The experiments take around \(3\) hours to run on a laptop CPU.

Setup.We take \(n=50\), \(d=5\), \(L=10\). The design matrix \(X\) is sampled from an isotropic Gaussian distribution. The target \(y\) is computed in two steps. First, we compute \(y_{0}=Xw_{\text{true}}+\zeta\), where \(w_{\text{me}}\) and \(\zeta\) are standard Gaussian vectors. Then, we compute \(w_{0}^{\star}\) as the optimal regressor of \(y_{0}\) on \(X\). Finally, we let \(y=y_{0}/\|w_{0}^{\star}\|\) and \(w^{\star}=w_{0}^{\star}/\|w_{0}^{\star}\|\). This simplifies the expressions of our bounds by having \(w^{\star}\) of unit norm. All Gaussian random variables are independent.

Details of Figure 1.We consider a Gaussian initialization of the weight matrices, where the scale of the initialization (x-axis of some the graphs) is the standard deviation of the entries. All weight matrices are \(d\times d\), except the last one which is \(1\times d\). The square distance to the optimal regressor corresponds to \(\|w_{\text{pod}}-w^{\star}\|_{2}^{2}\). The largest eigenvalue of the Hessian is computed by a power iteration method, stopped after \(20\) iterations. In Figures 0(a) and 0(b), the \(95\%\) confidence intervals are plotted. The number of gradient steps and number of independent repetitions depend on the learning rate, and are given below.

For large values of the initialization scale, it may happen that the gradient descent diverges. Figure 3 shows the probability of divergence depending on the initialization scale and the learning rate.

When the probability of divergence is equal to one, no point is reported in Figure 1. When it is strictly between \(0\) and \(1\), the confidence intervals are computed over non-diverging runs.

Figures 0(c) and 0(d) show one randomly-chosen run each. The plots are subsampled \(5\) times for readability, due to the oscillations in Figure 0(d).

Residual initialization.We now consider the case of a residual initialization as in Section 5. Results are given in Figure 4. The scale of the initialization now corresponds to the hyperparameter \(s\) in (2). The projection vector \(p\in\mathbb{R}^{d}\) is a random isotropic vector of unit norm, which does not change during training. For each learning rate, we use \(4,000\) steps of gradient descent, and perform \(20\) independent repetitions. The plots are similar to the case of Gaussian initialization, apart from the fact that the sharpness at initialization is better conditioned.

As previously, for large values of the initialization scale, it may happen that the gradient descent diverges. Figure 5 shows the probability of divergence depending on the initialization scale and the learning rate.

Details of Figure 2.The setup is the same as for the residual initialization. For each learning rate, we use \(1,000\) steps of gradient descent, and perform \(50\) independent repetitions. We take \(s=0.25\).

Figure 3: Probability of divergence of gradient descent for a Gaussian initialization of the weight matrices, depending on the initialization scale and the learning rate.

Underdetermined regression and link to generalization.Although this is not our original motivation, we note that a simple change to our setting allows to make appear the connection between sharpness and generalization. To this aim, we consider the underdetermined case, where the number of data is lower than the dimension (while keeping the rest of the setup identical). Figure 6 shows in this case a correlation between generalization and sharpness. This suggests that the tools developed in the paper could be used in this case to understand the generalization performance of deep (linear) networks, and we leave this analysis for future work. We also qualitatively observe in Figure 6 a similar connection between learning rate, initialization scale and sharpness as in the case of full-rank data (Figure 0(b)). We take here \(n=15,d=20,L=5\). The number of gradient steps and number of

Figure 4: Training a deep linear network on a univariate regression task with quadratic loss. The initialization is a residual initialization as in Section 5.

Figure 5: Probability of divergence of gradient descent for a residual initialization of the weight matrices, depending on the initialization scale and the learning rate.

independent repetitions depend on the learning rate, and are given below. Other technical details are as Figure 1.

Non-linear MLPs.As a first attempt to extend our results to non-linear networks, we consider the case of non-linear MLPs. The non-linearity is GELU (Hendrycks and Gimpel, 2016), a smooth version of ReLU, which we chose because smoothness is necessary in order to compute the sharpness. We qualitatively observe in Figure 7 a similar connection between learning rate, initialization scale and sharpness as for deep linear neural networks (Figure 0(b)). For large initialization, the sharpness after training plateaus at \(2/\eta\), as in the linear case. For small initialization, the sharpness after training is less that \(2/\eta\), and is close to the bounds in the linear case (dotted black lines). For this experiment, we consider noiseless data, meaning that \(y_{0}=Xw_{\text{true}}\) (see paragraph "Setup" above for notations). We perform \(20\) independent repetitions of each experiment. The number of gradient steps depends on the learning rate, and is given below. Other details are as for Figure 1. Finally, we also performed the same experiment in the case of noisy data \(y_{0}=Xw_{\text{true}}+\zeta\) (no plot reported). We observed that the sharpness of the network reaches \(2/\eta\), for every learning rate and initialization scale reported in Figure 7. We suspect that this is because the network (over)fits the noise in the data, resulting in a high sharpness.

Comments on the connection between gradient flow and gradient descent.In this paper, we show that gradient flow from a small-scale initialization is driven towards low-sharpness regions. This should imply that gradient flow and gradient descent up to a reasonably large learning rate should follow the same trajectory when starting from small-scale initialization, because they do not

Figure 6: Experiment with a deep linear network and a degenerate data covariance matrix, where the number of data \(n\) is less than the dimension \(d\).

go in regions of high sharpness where the difference between gradient flow and gradient descent would become significant. This intuition is supported by Figure 0(b) where we see that, for small initializations, the sharpness after training is independent of the learning rate. We leave further investigation of these questions for future work.

## Appendix D Additional related work

Progressive sharpening.Cohen et al. (2021) show that the edge of stability phase is typically preceded by a phase of progressive sharpening, where the sharpness steadily increases until reaching the value of \(2/\eta\). Our setting of small-scale initialization presents an example of such a progressive sharpening (although we make no statement on the monotonicity of the increase in sharpness). Other works have proposed analyses of progressive sharpening. Wang et al. (2022) suggest that progressive sharpening is driven by the increase in norm of the output layer. MacDonald et al. (2023) assume from empirical evidence a link between sharpness and the magnitude of the input-output Jacobian, and show that the latter has to be large for the loss to decrease. Finally, Agarwala et al. (2023) propose and analyze a simplified model with quadratic dependence in its parameters, which exhibits a progressive sharpening phenomenon.

Connection with deep matrix factorization.Regression with deep linear networks can be seen as an instance of a matrix factorization problem. There is a well-established literature studying the implicit regularization of gradient descent for this class of problem (see, e.g., Gunasekar et al., 2017; Arora et al., 2019; Li et al., 2021; Yun et al., 2021). However, this line of work study under-determined settings where there are an infinite number of factorizations reaching zero empirical risk, and study the implicit regularization in function space. On the contrary, we consider an over-determined setting where there is a single optimal regressor, and study the regularization in parameter space.

Figure 7: For a non-linear MLP, sharpness at initialization and after training, for various learning rates and initialization scales. For a given learning rate \(\eta\), the dashed lines represent the \(2/\eta\) threshold. The dotted black lines represent the lower and upper bounds given in Theorem 1 and Corollary 2 of the paper.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The claims regard the theoretical analysis of sharpness of deep linear networks. The corresponding results are given in Sections 3, 4, and 5, and proven in Appendix B. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The paper presents work of theoretical nature, and explains clearly the setting and assumptions (e.g., linear activation function) in which the results are applicable. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: The assumptions are stated in the main paper, and all proofs are given in Appendix B. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The experimental details are given in Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code is given in the supplemental material. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The experimental details are given in Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All plots have error bars (except the plots showing the trajectory of gradient descent along one run, for illustration). Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The experiments are small-scale and run on a laptop CPU. The runtime is given in Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The authors have reviewed the NeurIPS Code of Ethics, and attest that this paper conforms to the Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This is a foundational research paper, not tied to particular applications, let alone deployments. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite the papers for code we are using in Appendix C. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.