# ProG: A Graph Prompt Learning Benchmark

 Chenyi Zi\({}^{1}\)

Haihong Zhao\({}^{1}\)

Xiangguo Sun\({}^{2,\boxplus}\)

Yiqing Lin\({}^{3}\)

Hong Cheng\({}^{2}\)

Jia Li\({}^{1}\)

\({}^{1}\)Hong Kong University of Science and Technology (Guangzhou)

\({}^{2}\)Department of Systems Engineering and Engineering Management,

and Shun Hing Institute of Advanced Engineering, The Chinese University of Hong Kong

\({}^{3}\)Tsinghua University

corresponding author: xiangguosun@cuhk.edu.hk

The first two authors contributed equally to this work. Listing order is random.

###### Abstract

Artificial general intelligence on graphs has shown significant advancements across various applications, yet the traditional 'Pre-train & Fine-tune' paradigm faces inefficiencies and negative transfer issues, particularly in complex and few-shot settings. Graph prompt learning emerges as a promising alternative, leveraging lightweight prompts to manipulate data and fill the task gap by reformulating downstream tasks to the pretext. However, several critical challenges still remain: how to unify diverse graph prompt models, how to evaluate the quality of graph prompts, and to improve their usability for practical comparisons and selection. In response to these challenges, we introduce the first comprehensive benchmark for graph prompt learning. Our benchmark integrates **SIX** pre-training methods and **FIVE** state-of-the-art graph prompt techniques, evaluated across **FIFTEEN** diverse datasets to assess performance, flexibility, and efficiency. We also present 'ProG', an easy-to-use open-source library that streamlines the execution of various graph prompt models, facilitating objective evaluations. Additionally, we propose a unified framework that categorizes existing graph prompt methods into two main approaches: prompts as graphs and prompts as tokens. This framework enhances the applicability and comparison of graph prompt techniques. The code is available at: https://github.com/sheldonresearch/ProG.

## 1 Introduction

Recently, artificial general intelligence (AGI) on graphs has emerged as a new trend in various applications like drug design [59; 38], protein prediction [10], social analysis [49; 64; 52], etc. To achieve this vision, one key question is how to learn useful knowledge from non-linear data (like graphs) and how to apply it to various downstream tasks or domains. Classical approaches mostly leverage the 'Pre-train & Fine-tune' paradigm, which first designs some pretext with easily access data as the pre-training task for the graph neural network model, and then adjusts partial or entire model parameters to fit new downstream tasks. Although much progress has been achieved, they are still not effective and efficient enough. For example, adjusting the pre-trained model will become very time-consuming with the increase in model complexity [25; 24; 48; 23]. In addition, a natural gap between these pretexts and downstream tasks makes the task transferring very hard, and sometimes may even cause negative transfer [57; 30; 29]. These problems are particularly serious in few-shot settings.

To further alleviate the problems above, Graph Prompt Learning [50; 28] has attracted more attention recently. As shown in Figure 1, graph prompts seek to manipulate downstream data by inserting an additional small prompt graph and then reformulating the downstream task to the pre-training task without changing the pre-trained Graph Neural Network (GNN) model. Since a graph prompt is usually lightweight, tuning this prompt is more efficient than the large backbone model. Comparedwith graph 'pre-training & fine-tuning', which can be seen as a model-level retraining skill, graph 'pre-training & prompting' is more customized for data-level operations, which means its potential applications might go beyond task transferring in the graph intelligence area like data quality evaluation, multi-domain alignment, learnable data augmentation, etc. Recently, many graph prompt models have been proposed and presented significant performance in graph learning areas [47; 46; 33; 8; 66; 5], indicating their huge potential towards more general graph intelligence. Despite the enthusiasm around graph prompts in research communities, three significant challenges impede further exploration:

* **How can we unify diverse graph prompt models given their varying methodologies?** The fragmented landscape of graph prompts hinders systematic research advancement. A unified framework is essential for integrating these diverse methods, creating a cohesive taxonomy to better understand current research and support future studies.
* **How do we evaluate the quality of graph prompts?** Assessing the efficiency, power, and flexibility of graph prompts is crucial for understanding their impact and limitations. Currently, there is no standardized benchmark for fair and comprehensive comparison, as existing works have inconsistent experimental setups, varying tasks, metrics, pre-training strategies, and data-splitting techniques, which obstruct a comprehensive understanding of the current state of research.
* **How can we make graph prompt approaches more user-friendly for practical comparison and selection?** Despite numerous proposed methods, the lack of an easy-to-use toolkit for creating graph prompts limits their potential applications. The implementation details of existing works vary significantly in programming frameworks, tricks, and running requirements. Developing a standardized library for various graph prompt learning approaches is urgently needed to facilitate broader exploration and application.

In this work, we wish to push forward graph prompt research to be more standardized, provide suggestions for practical choices, and uncover both their advantages and limitations with a comprehensive evaluation. To the best of our knowledge, this is the first benchmark for graph prompt learning. **To solve the first challenge**, our benchmark treats existing graph prompts as three basic components: prompt tokens, token structure, and insert patterns. According to their detailed implementation, we categorize the current graph prompts into two branches: graph prompt as an additional graph, and graph prompt as tokens. This taxonomy makes our benchmark approach compatible with nearly all existing graph prompting approaches. **To solve the second challenge**, our benchmark encompasses 6 different classical pre-training methods covering node-level, edge-level, and graph-level tasks, and 5 most representative and state-of-the-art graph prompt methods. Additionally, we include 15 diverse datasets in our benchmark with different scales, covering both homophilic and heterophilic graphs from various domains. We systematically investigate the effectiveness, flexibility, and efficiency of graph prompts under few-shot settings. Compared with traditional supervised methods and 'pre-training & fine-tuning' methods, graph prompting approaches present significant improvements. We even observe some negative transfer cases caused by 'pre-training & fine-tuning' methods but effectively reversed by graph prompting methods. **To solve the third challenge**, we develop a unified library for creating various graph prompts. In this way, users can evaluate their models or datasets with less effort. The contributions are summarized as follows:

* We propose the first comprehensive benchmark for graph prompt learning. The benchmark integrates 6 most used pre-training methods and 5 state-of-the-art graph prompting methods with

Figure 1: An overview of our benchmark.

15 diverse graph datasets. We systematically evaluate the effectiveness, flexibility, and efficiency of current graph prompt models to assist future research efforts. (Detailed in Section 5)
* We offer 'ProG'2, an easy-to-use open-source library to conduct various graph prompt models (Section 3). We can get rid of the distraction from different implementation skills and reveal the potential benefits/shortages of these graph prompts in a more objective and fair setting. Footnote 2: Our code is available at https://github.com/sheldonresearch/ProG
* We propose a unified view for current graph prompt methods (Section 2). We decompose graph prompts into three components: prompt tokens, token structure, and insert patterns. With this framework, we can easily group most of the existing work into two categories (prompt as graph, and prompt as tokens). This view serves as the infrastructure of our developed library and makes our benchmark applicable to most of the existing graph prompt works.

## 2 Preliminaries

**Understand Graph Prompt Nature in A Unified View.** Graph prompt learning aims to learn suitable transformation operations for graphs or graph representations to reformulate the downstream task to the pre-training task. Let \(t(\cdot)\) be any graph-level transformation (e.g., "modifying node features", "augmenting original graphs", etc.), and \(\psi^{*}(\cdot)\) be a frozen pre-trained graph model. For any graph \(\mathcal{G}\) with adjacency matrix \(\mathbf{A}\in\{0,1\}^{N\times N}\) and node feature matrix \(\mathbf{X}\in\mathbb{R}^{N\times d}\) where \(N\) denotes the node number and \(d\) is feature number. It has been proven [47; 8] that we can always learn an appropriate prompt module \(\mathcal{P}\), making them can be formulated as the following formula:

\[\psi^{*}(\mathcal{P}(\mathbf{X},\mathbf{A},\mathbf{P},\mathbf{A}_{inner}, \mathbf{A}_{cross}))=\psi^{*}(t(\mathbf{X},\mathbf{A}))+O_{\mathcal{P}\psi}\] (1)

Here \(\mathbf{P}\in\mathbb{R}^{K\times d}\) is the learnable representations of \(K\) prompt tokens, \(\mathbf{A}_{inner}\in\{0,1\}^{K\times K}\) indicate token structures and \(\mathbf{A}_{cross}\in\{0,1\}^{K\times N}\) is the inserting patterns, which indicates how each prompt token connects to each node in the original graph. Prompt module \(\mathcal{P}\) takes the input graph and then outputs an integrated graph with a graph prompt. This equation indicates that we can learn an appropriate prompt module \(\mathcal{P}\) applied to the original graph to imitate any graph manipulation. \(O_{\mathcal{P}\psi}\) denotes the _error bound_ between the manipulated graph and the prompting graph w.r.t. their representations from the pre-trained graph model, which can be seen as a measurement of graph prompt flexibility.

**Task Definition (Few-shot Graph Prompt Learning).** Given a graph \(\mathcal{G}\), a frozen pre-trained graph model \(\psi^{*}\), and labels \(\mathcal{Y}\), the loss function is to optimize the task loss \(\mathcal{L}_{Task}\) as follows:

\[\mathcal{L}_{\mathcal{P}}=\mathcal{L}_{Task}(\psi^{*}(\mathcal{P}(\mathbf{X},\mathbf{A},\mathbf{P},\mathbf{A}_{inner},\mathbf{A}_{cross}))),\mathcal{Y})\]

During the few-shot training phase, the labeled samples are very scarce. Specifically, given a set of classes \(C\), each class \(C_{i}\) has only \(k\) labeled examples. Here, \(k\) is a small number (e.g., \(k\leq 10\)), and the total number of labeled samples is given by \(k\times|C|=|\mathcal{Y}|\), where \(|C|\) is the number of classes. This setup is commonly referred to as \(k\)-shot classification [33]. In this paper, our evaluation includes both node and graph classification tasks.

**Evaluation Questions.** We are particular interested in the **effectiveness**, **flexibility**, and **efficiency** of graph prompt learning methods. Specifically, **in effectiveness**, we wish to figure out how effective are different graph prompt methods on various tasks (Section 5.1); and how well graph prompting methods overcome the negative transferring caused by pre-training strategies (Section 5.2). **In flexibility**, we wish to uncover how powerful different graph prompt methods are to simulate data operations (Section 5.3). **In efficiency**, we wish to know how efficient are these graph prompt methods in terms of time and space cost (Section 5.4).

## 3 Benchmark Methods

In general, graph prompt learning aims to transfer pre-trained knowledge with the expectation of achieving better positive transfer effects in downstream tasks compared to classical 'Pre-train & Fine-tune' methods. To conduct a comprehensive comparison, we first consider using a supervised method as the baseline for judging positive transfer (For example, if a 'pre-training and fine-tuning' approach can not beat supervised results, one negative transfer case is observed). We introduce various pre-training methods to generate pre-trained knowledge, upon which we perform fine-tuningas the comparison for graph prompts. Finally, we apply current popular graph prompt learning methods with various pre-training methods to investigate their knowledge transferability. The details of these baselines are as follows (Further introduction can be referred to Appendix A):

* **Supervised Method:** In this paper, we utilize Graph Convolutional Network (GCN) [21] as the baseline, which is one of the classical and effective graph models based on graph convolutional operations [50; 67; 32; 69] and also the backbone for both 'Pre-train & Fine-tune' and graph prompt learning methods. We also explore more kinds of graph models like GraphSAGE [13], GAT [54], and Graph Transformer [43], and put the results in Appendix E.
* **'Pre-train & Fine-tune' Methods:** We select 6 mostly used pre-training methods covering node-level, edge-level, and graph-level strategies. For **node-level**, we consider **DGI**[55] and **GraphMAE**[14], where DGI maximizes the mutual information between node and graph representations for informative embeddings and GraphMAE learns deep node representations by reconstructing masked features. For **edge-level**, we introduce **EdgePreGPT**[46] and **EdgePreG-prompt**[33], where EdgePreGPT calculates the dot product as the link probability of node pairs and EdgePreGprompt samples triplets from label-free graphs to increase the similarity between the contextual subgraphs of linked pairs while decreasing the similarity of unlinked pairs. For **graph-level**, we involve **GCL**[65], **SimGRACE**[60], where GCL maximizes agreement between different graph augmentations to leverage structural information and SimGRACE tries to perturb the graph model parameter spaces and narrow down the gap between different perturbations for the same graph.
* **Graph Prompt Learning Methods:** With our proposed unified view, current popular graph prompt learning methods can be easily classified into two types, 'Prompt as graph' and 'Prompt as token' types. Specifically, the **'Prompt as graph'** type means a graph prompt has multiple prompt tokens with inner structure and insert patterns, and we introduce **All-in-one**[47], which aims to learn a set of insert patterns for original graphs via learnable graph prompt modules. For the **'Prompt as token'** type, we consider involving **GPPT**[46], **Gprompt**[33], **GPF** and **GPF-plus**[8]. Concretely, GPPT defines graph prompts as additional tokens that contain task tokens and structure tokens that align downstream node tasks and link prediction pretext. GPprompt focuses on inserting the prompt vector into the graph pooling by element-wise multiplication. GPF and GPF-plus mainly add soft prompts to all node features of the input graph.

**ProG: A Unified Library for Graph Prompts.** We offer our developed graph prompt library ProG as shown in Figure 2. In detail, the library first offers a **Model Backbone** module, including many widely-used graph models, which can be used for supervised learning independently. Then, it designs a **Pre-training** module involving many pre-training methods from various level types, which can pre-train the graph model initialized from the model backbone to preserve pre-trained knowledge and further fine-tune the pre-trained graph model. Finally, it seamlessly incorporates different graph prompt learning methods together into a unified **Prompting** module, which can freeze and execute prompt tuning for the pre-trained graph model from the pre-training module. For a fair comparison, it defines an **Evaluation** module, including comprehensive metrics, batch evaluator, and dynamic dispatcher, to ensure the same settings across different selected methods. Many essential operations are fused into the **Utils** module for repeated use, and all the data-related operations are included in the **Data** module for user-friendly operations to use the library. Additionally, the **Tutorial** module introduces the key functions that ProG offers.

## 4 Benchmark Settings

### Datasets

To figure out how adaptive existing graph prompts on different graphs, we conduct our experiments across 15 datasets on node-level or graph-level tasks, providing a broad and rigorous testing ground

Figure 2: An overview of ProG

for algorithmic comparisons. As shown in Table 1, we include 7 node classification datasets, covering homophilic datasets (Cora, Citeseer, PubMed, ogbn-arxiv) [42, 35, 15], heterophilic datasets (Wisconsin, Texas, Actor) [37, 53], and large-scale datasets (ogbn-arxiv). Additionally, we consider 8 graph classification datasets from different domains, including social networks (IMDB-B, COLLAB) [63], biological datasets (ENZYMES, PROTEINS, DD) [7, 2, 58], and small molecule datasets (MUTAG, COX2, BZR) [22, 41]. More details on these datasets can be found in Appendix C.

### Implementation Details

**Data Split.** For the node task, we use 90% data as the test set. For the graph task, we use 80% data as the test set. To ensure the robustness of our findings, we repeat the sampling process five times to construct k-shot tasks for both node and graph tasks. We then report the average and standard deviation over these five results.

**Metrics.** Following existing multi-class classification benchmarks, we select Accuracy, Macro F1 Score, and AUROC (Area Under the Receiver Operating Characteristic Curve) as our main performance metrics [47, 68]. The Accuracy results are used to provide a straightforward indication of the model's overall performance in Section 5. F1 Score and AUROC results are provided in Appendix D.

**Prompt Adaptation to Specific Tasks.** We observe that certain prompt methods, such as GPPT, are only applicable to node classification, while GPF and GPF-plus are only suitable for graph classification. To this end, we design graph task tokens to replace the original node task tokens to make GPPT also applicable in graph-level tasks. Inspired by All-in-one and GPrompt, we utilize induced graphs in our benchmark to adapt GPF and GPF-plus for the node task. The concrete adaptation designs are shown in Appendix A.

**Hyperparameter Optimization.** To manage the influence of hyperparameter selection and maintain fairness, the evaluation process is standardized both with and without hyperparameter adjustments. Initially, default hyperparameters as specified in the original publications are used. To ensure fairness during hyperparameter tuning, **random search** is then employed to optimize the settings. In each trial across different datasets, a set of hyperparameters is randomly selected from a predefined search space for each model. For additional details on metrics, default hyperparameters, search spaces, and other implementation aspects, please see Appendix D.

## 5 Experimental Results

### Overall Performance of Graph Prompt Learning

In Table 2 and Table 3, we present the optimal experimental results of the supervised methods, classical 'Pre-train & Fine-tune' methods, and various graph prompt methods on 1-shot node/graph

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline Dataset & Graphs & Avg.nodes & Avg.edges & Features & Node classes & Task (N / G) & Category \\ \hline Cora & 1 & 2,708 & 5,429 & 1,433 & 7 & N & Homophilic \\ Pubmed & 1 & 19,717 & 88,648 & 500 & 3 & N & Homophilic \\ CiteSeer & 1 & 3,327 & 9,104 & 3,703 & 6 & N & Homophilic \\ Actor & 1 & 7600 & 30019 & 932 & 5 & N & Heterophilic \\ Wisconsin & 1 & 251 & 515 & 1703 & 5 & N & Heterophilic \\ Texas & 1 & 183 & 325 & 1703 & 5 & N & Heterophilic \\ ogbn-arxiv & 1 & 169,343 & 1,166,243 & 128 & 40 & N & Homophilic \& Large scale \\ \hline \hline Dataset & Graphs & Avg.nodes & Avg.edges & Features & Graph classes & Task (N / G) & Domain \\ \hline MUTAG & 188 & 17.9 & 19.8 & 7 & 2 & G & Small Molecule \\ IMDB-BINARY & 1000 & 19.8 & 96.53 & 0 & 2 & G & Social Network \\ COLLAB & 5000 & 74.5 & 2457.8 & 0 & 3 & G & Social Network \\ PROTEINS & 1,113 & 39.1 & 72.8 & 3 & 2 & G & Proteins \\ ENZYMES & 600 & 32.6 & 62.1 & 3 & 6 & G & Proteins \\ DD & 1,178 & 284.1 & 715.7 & 89 & 2 & G & Proteins \\ COX2 & 467 & 41.2 & 43.5 & 3 & 2 & G & Small Molecule \\ BZR & 405 & 35.8 & 38.4 & 3 & 2 & G & Small Molecule \\ \hline \hline \end{tabular}
\end{table}
Table 1: Statistics of all datasets. N indicates node classification, and G indicates graph classification.

classification tasks across 15 datasets, where the evaluation metric is accuracy (Complete results can be referred to Appendix E). Here the 'optimal' results mean that the best result is shown from six classical 'Pre-train & Fine-tune' methods or from six pretraining-based variants of each graph prompt method. This presentation style can help reflect the upper bounds of classical 'Pre-train & Fine-tune' methods and graph prompt methods, better highlighting the advantages of graph prompts.

From these results, we can observe consistent advantages of various graph prompt learning methods. Almost all graph prompt methods consistently outperform the Supervised method on both node-level and graph-level tasks, demonstrating positive transfer and superior performance. Specifically, graph prompt methods generally surpass classical 'Pre-train & Fine-tune' methods across most datasets, showcasing enhanced knowledge transferability. In node-level tasks, GPF-plus, which focuses on modifying node features via learnable prompt features, achieves the best results on 4 out of 7 datasets with significant effects, attributed to its inherent ability to easily adapt to different pre-training methods [8]. In contrast, All-in-one, which concentrates on modifying graphs via learnable subgraphs, achieves the best results on 7 out of 8 datasets on graph-level tasks.

Similar observations can also be found in 3-shot and 5-shot results in the optimal and complete versions, which can be checked in Appendix E. Additionally, for comprehensive, we also offer the curve of the shot number from 1 to 10 and the accuracy in Appendix E.

### Impact of Pre-training Methods

From Table 2, we further find that on the Wisconsin dataset, the classical Pre-train & Fine-tune method consistently exhibited negative transfer, whereas most graph prompt methods effectively transferred upstream pre-training knowledge, achieving significant results and positive transfer. Therefore, it is crucial to explore the effectiveness of various pre-training methods, especially for graph prompt learning. Table 4 and Table 5 show the experimental results of six variants of GPF-plus/All-in-one and the corresponding 'Pre-train & Fine-tune' methods. Concretely, we have the following key observations:

**Consistency in Pretext and Downstream Tasks.** In node classification tasks, the pre-trained knowledge learned by the node-level pre-training method (e.g., GraphMAE) shows superior transferability to downstream tasks using GPF-plus, achieving better results compared to traditional fine-tuning methods. Conversely, in graph classification tasks, the graph-level pre-training method (e.g., SimGRACE) demonstrates better transferability with All-in-one, resulting in more significant outcomes.

\begin{table}
\begin{tabular}{c|c c c c c c c} \hline \hline
**Methods/Datasets** & **Cora** & **Cliteseer** & **Pubmed** & **Wisconsin** & **Texas** & **Actor** & **Ogbn-arxiv** \\ \hline Supervised & 26.56\({}_{\pm 5.55}\) & 21.78\({}_{\pm 7.52}\) & 39.37\({}_{\pm 15.34}\) & 41.60\({}_{\pm 3.10}\) & 37.97\({}_{\pm 5.80}\) & 20.57\({}_{\pm 4.47}\) & 10.99\({}_{\pm 3.19}\) \\ \hline Pre-train \& Fine-tune & 52.61\({}_{\pm 1.73}\) & 35.05\({}_{\pm 4.37}\) & 46.74\({}_{\pm 11.89}\) & 40.69\({}_{\pm 1.43}\) & 46.88\({}_{\pm 8.49}\) & 20.74\({}_{\pm 1.22}\) & 16.21\({}_{\pm 3.82}\) \\ \hline GPPT & 43.15\({}_{\pm 9.44}\) & 37.26\({}_{\pm 6.17}\) & 48.31\({}_{\pm 7.72}\) & 30.40\({}_{\pm 6.81}\) & 31.81\({}_{\pm 15.33}\) & 22.58\({}_{\pm 1.97}\) & 14.65\({}_{\pm 3.07}\) \\ \hline All-in-one & 52.39\({}_{\pm 10.17}\) & 40.41\({}_{\pm 2.80}\) & 45.17\({}_{\pm 6.45}\) & 78.24\({}_{\pm 16.68}\) & 65.49\({}_{\pm 7.06}\) & 24.61\({}_{\pm 2.80}\) & 13.16\({}_{\pm 6.98}\) \\ \hline Gprompt & 56.66\({}_{\pm 33.12}\) & 53.21\({}_{\pm 10.94}\) & 39.74\({}_{\pm 15.35}\) & 83.80\({}_{\pm 2.44}\) & 33.25\({}_{\pm 10.11}\) & 25.26\({}_{\pm 13.10}\) & 75.72\({}_{\pm 4.95}\) \\ \hline GPF & 38.57\({}_{\pm 5.41}\) & 31.16\({}_{\pm 8.05}\) & 49.99\({}_{\pm 8.86}\) & 88.67\({}_{\pm 5.78}\) & 87.40\({}_{\pm 3.30}\) & 28.70\({}_{\pm 3.35}\) & 78.37\({}_{\pm 5.67}\) \\ \hline GPF-plus & 55.77\({}_{\pm 10.30}\) & 59.67\({}_{\pm 11.82}\) & 46.64\({}_{\pm 18.97}\) & 91.03\({}_{\pm 4.11}\) & 95.83\({}_{\pm 4.19}\) & 29.32\({}_{\pm 8.56}\) & 71.98\({}_{\pm 12.23}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Performance on 1-shot node classification. The best results for each dataset are highlighted in bold with a dark red background. The second-best are underlined with a light red background.

\begin{table}
\begin{tabular}{c|c c c c c c c} \hline \hline
**Methods/Datasets** & **IMDB-B** & **COLLAB** & **PROTEINS** & **MUTAG** & **ENZYMES** & **COX2** & **BZR** & **DD** \\ \hline Supervised & 57.30\({}_{\pm 0.598}\) & 47.23\({}_{\pm 0.61}\) & 56.36\({}_{\pm 7.97}\) & 65.20\({}_{\pm 6.70}\) & 20.58\({}_{\pm 2.60}\) & 27.08\({}_{\pm 1.95}\) & 25.80\({}_{\pm 6.53}\) & 55.33\({}_{\pm 6.22}\) \\ \hline Pre-train \& Fine-tune & 57.75\({}_{\pm 1.22}\) & 48.10\({}_{\pm 6.23}\) & 63.44\({}_{\pm 3.56}\) & 65.47\({}_{\pm 5.89}\) & 22.21\({}_{\pm 2.79}\) & 76.19\({}_{\pm 5.41}\) & 34.69\({}_{\pm 8.50}\) & 57.15\({}_{\pm 4.32}\) \\ \hline GPPT & 50.15\({}_{\pm 6.07}\) & 47.18\({}_{\pm 5.93}\) & 60.92\({}_{\pm 2.47}\) & 60.40\({}_{\pm 15.43}\) & 21.29\({}_{\pm 3.79}\) & 78.23\({}_{\pm 1.98}\) & 59.32\({}_{\pm 11.22}\) & 57.69\({}_{\pm 6.59}\) \\ \hline All-in-one & 60.07\({}_{\pm 4.81}\) & 51.66\({}_{\pm 6.26}\) & 66.49\({}_{\pm 6.26}\) & 29.78\({}_{\pm 7.54}\) & 23.96\({}_{\pm 10.56}\) & 76.14\({}_{\pm 5.51}\) & 79.20\({}_{\pm 6.05}\) & 59.72\({}_{\pm 11.92}\) \\ \hline Gprompt & 54.75\({}_{\pm 2.43}\) & 48.25\({}_{\pm 13.64}\) & 59.17\({}_{\pm 11.20}\) & 73.60\({}_{\pm 7.66}\) & 22.29\({}_{\pm 3.50}\) & 54.64\({}_{\pm 9.94}\) & 55.43\({}_{\pm 13.69}\) & 57.81\({}_{\pm 2.68}\) \\ \hline GPF & 59.65\({}_{\pm 5.66}\) & 47.42\({}_{\pm 11.22}\) & 63.91\({}_{\pm 3.26}\) & 68.40\({}_{\pm 5.09}\) & 22.00\({}_{\pm 1.25}\) & 65.79\({}_{\pm 17.72}\) & 71.67\({}_{\pm 44.71}\) & 59.36\({}_{\pm 118}\) \\ \hline GPF-plus & 57.93\({}_{\pm 1.62}\) & 47.24\({}_{\pm 10.29}\) & 62.92\({}_{\pm 2.78}\) & 65.20\({}_{\pm 6.04}\) & 22.92\({}_{\pm 11.66}\) & 33.78\({}_{\pm 1.52}\) & 71.17\({}_{\pm 14.92}\) & 57.62\({}_{\pm 12.42}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Performance on 1-shot graph classification. The best results for each dataset are highlighted in bold with a dark red background.

[MISSING_PAGE_FAIL:7]

graph and the manually manipulated graph restored by graph prompt methods. These manipulations include dropping nodes, dropping edges, and masking features. Table 6 presents the specific results. The experimental results indicate that **all graph prompt methods significantly reduced the error compared to the original error across all types of manipulations**. Notably, All-in-one and Gprompt achieved the lowest error bounds, demonstrating their flexibility in adapting to graph transformations. Specifically, All-in-one achieved a RED% of 95.06, indicating a 95.06% reduction in original error, underscoring its comprehensive approach to capturing graph transformations. These findings highlight the importance of flexibility in prompt design for effective knowledge transfer. Flexible prompts that can adapt to various graph transformations enable more powerful graph data operation capability, thereby improving the performance of downstream tasks. Moreover, this flexibility is inspiring for more customized graph data augmentation, indicating more potential for other applications.

### Efficiency Analysis

We evaluate the efficiency and training duration of various graph prompt methods across tasks, focusing on node classification in Cora and Wisconsin and graph classification in BZR and DD. **GPF-plus** achieves **high accuracy** with **relatively low training times** despite its **larger parameter size** on Cora and Wisconsin. In contrast, **All-in-One**, with **a smaller parameter size**, has **slightly larger training times** than GPF-plus due to similarity calculations. Among all the graph prompt methods, GPPT shows lower performance and the longest training duration because of its k-means clustering update in every epoch. Besides, we **compare the tunable parameters of graph prompt methods** during the training phase with **classical 'Pre-train & Fine-tune' methods** with various backbones for further analyzing the advantages of graph prompt learning. Consider an input graph has \(N\) nodes, \(M\) edges, \(d\) features, along with a graph model which has \(L\) layers with maximum layer dimension \(D\). **Classical backbones (e.g., GCN) have \(O(dD+D^{2}(L-1))\)** learnable parameter complexity. Other more complex backbones (e.g., graph transformer) may have larger parameters. In the **graph prompt learning framework**, tuning the prompt parameters with a frozen pre-trained backbone makes training converge faster. For **All-in-One**, with \(K\) tokens and \(m\) edges, **learnable parameter complexity is \(O(Kd)\). Other graph prompt methods also have similar tunable parameter advantages, with specific sizes depending on their designs (e.g., Gprompt only includes a learnable vector inserted into the graph pooling by element-wise multiplication).

\begin{table}
\begin{tabular}{l|c c c|c} \hline \multirow{2}{*}{Prompt Model} & Drop & Drop & Mask & \multirow{2}{*}{RED (\%)} \\  & Nodes & Edges & Feature & \\ \hline ori\_error & 0.4862 & 0.0713 & 0.6186 & \\ \hline All-in-one & 0.0200 & 0.0173 & 0.0207 & 95.06 \(\downarrow\) \\ Gprompt & 0.0218 & 0.0141 & 0.0243 & 94.88 \(\downarrow\) \\ GPF-plus & 0.0748 & 0.0167 & 0.1690 & 77.85 \(\downarrow\) \\ GPF & 0.0789 & 0.0146 & 0.1858 & 76.25 \(\downarrow\) \\ \hline \end{tabular}
\end{table}
Table 6: Error bound between different prompt methods, RED(%): average reduction of each method to the original error.

Figure 3: Head map of GPF-plus and All-in-one on node-level and graph-level tasks across various pre-training methods and datasets. The green interval shows that the graph prompt learning method performs worse than ’Pre-train & Fine-tune’. The yellow-to-red interval shows that the graph prompt learning method performs better. **An upward arrow** indicates that the ’Pre-train & Fine-tune’ method experiences **negative transfer**, while the graph **prompt** learning method can **alleviate it significantly**.

### Adaptability of Graph Prompts on Different Graphs.

Besides the main observations mentioned before, we here further discuss the adaptability of graph prompts on various kinds of graphs. Specifically, considering graph prompts performance on **various domains** as shown in Table 3, we can find that the adaptability to various domains of graph prompt methods, particularly those of the Prompt as Token type, is not always good, where graph prompt methods even may encounter negative transfer issues. GPPT, in particular, faces significant challenges, likely because first-order subgraphs fail to comprehensively capture graph patterns, introducing noise and affecting transferability. For **Large-scale Datasets** like node-level dataset ogbn-arsiv in Table 2, the transferability of graph prompt methods involving subgraph operations, such as GPPT and All-in-one, requires further enhancement. Furthermore, regarding the relatively large graph-level dataset DD in Table 3, all graph prompt methods achieve positive transfer and outperform Pre-train & Fine-tune. Despite this success, the magnitude of improvement indicates that the transferability on graph-level large-scale datasets also requires further enhancement. For **Homophilic & Heterophilic Datasets** in Table 2, most graph prompt methods can handle both homophilic and heterophilic graph datasets effectively, with the exception of GPPT, possibly due to instability caused by the clustering algorithms of GPPT. For **Pure Structural Dataset** like IMDB-BINARY and COLLAB in Table 3, the ability to handle pure structural datasets of graph prompt methods needs further refinement. In detail, some 'Pre-train & Fine-tune' methods can achieve positive transfer on these datasets, while some variants of the All-in-one method even exhibit negative transfer. Further detailed analysis can be referred to Appendix E.

## 6 Conclusion and Future Plan

In this paper, we introduce ProG, a comprehensive benchmark for comparing various graph prompt learning methods with supervised and 'Pre-train & Fine-tune' methods. Our evaluation includes 6 Pre-training methods and 5 graph prompt learning methods across 15 real-world datasets, demonstrating that graph prompt methods generally outperform classical methods. This performance is attributed to the high flexibility and low complexity of graph prompt methods. Our findings highlight the promising future of graph prompt learning research and its challenges. By making ProG open-source, we aim to promote further research and better evaluations of graph prompt learning methods.

As the empirical analysis in this paper, graph prompt learning shows its significant potential beyond task transferring, though our current evaluation focuses on node/graph-level tasks. We view ProG as a long-term project and are committed to its continuous development. Our future plans include expanding to a wider range of graph tasks, adding new datasets, etc. Ultimately, we aim to develop ProG into a robust graph prompt toolbox with advanced features like automated prompt selection.

## Acknowledgments and Disclosure of Funding

This work was supported by National Key Research and Development Program of China Grant No. 2023YFF0725100 and Guangzhou-HKUST(GZ) Joint Funding Scheme 2023A03J0673. The research of Cheng was supported in part by project #MMT-p2-23 of the Shun Hing Institute of Advanced Engineering, The Chinese University of Hong Kong and by grants from the Research Grant Council of the Hong Kong Special Administrative Region, China (No. CUHK 14217622).

Figure 4: Analysis of accuracy, training time and tunable parameters of various graph prompt methods. Note that the gray area enclosed by the dashed line represents the scale of the tunable parameters.

## References

* [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [2] Karsten M Borgwardt, Cheng Soon Ong, Stefan Schonauer, SVN Vishwanathan, Alex J Smola, and Hans-Peter Kriegel. Protein function prediction via graph kernels. _Bioinformatics_, 21(suppl_1):i47-i56, 2005.
* [3] Mouxiang Chen, Zemin Liu, Chenghao Liu, Jundong Li, Qiheng Mao, and Jianling Sun. ULTRA-DP: Unifying Graph Pre-training with Multi-task Graph Dual Prompt. _arXiv preprint_, 2023.
* [4] Nuo Chen, Yuhan Li, Jianheng Tang, and Jia Li. Graphwiz: An instruction-following language model for graph computational problems. In _Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 353-364, 2024.
* [5] Xi Chen, Siwei Zhang, Yun Xiong, Xixi Wu, Jiawei Zhang, Xiangguo Sun, Yao Zhang, Yinglong Zhao, and Yulin Kang. Prompt learning on temporal interaction graphs. _arXiv:2402.06326_, 2024.
* [6] Jiashun Cheng, Man Li, Jia Li, and Fugee Tsung. Wiener graph deconvolutional network improves graph self-supervised learning. In _AAAI_, pages 7131-7139, 2023.
* [7] Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without alignments. _Journal of molecular biology_, 330(4):771-783, 2003.
* [8] Taoran Fang, Yunchao Zhang, YANG YANG, Chunping Wang, and Lei Chen. Universal prompt tuning for graph neural networks. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 52464-52489. Curran Associates, Inc., 2023.
* [9] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. _arXiv preprint arXiv:1903.02428_, 2019.
* [10] Ziqi Gao, Xiangguo Sun, Zijing Liu, Yu Li, Hong Cheng, and Jia Li. Protein multimer structure prediction via prompt learning. In _The Twelfth International Conference on Learning Representations_, 2024.
* [11] Qingqing Ge, Zeyuan Zhao, Yiding Liu, Anfeng Cheng, Xiang Li, Shuaiqiang Wang, and Dawei Yin. Enhancing Graph Neural Networks with Structure-Based Prompt. _arXiv preprint_, 2023.
* [12] Chenghua Gong, Xiang Li, Jianxiang Yu, Cheng Yao, Jiaqi Tan, Chengcheng Yu, and Dawei Yin. Prompt Tuning for Multi-View Graph Contrastive Learning. _arXiv preprint_, 2023.
* [13] William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In _NeurIPS_, 2017.
* [14] Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, and Jie Tang. Graphmae: Self-supervised masked graph autoencoders. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 594-604, 2022.
* [15] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. _Advances in neural information processing systems_, 33:22118-22133, 2020.
* [16] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, and Jure Leskovec. Strategies for Pre-training Graph Neural Networks. In _ICLR_, 2020.
* [17] Qian Huang, Hongyu Ren, Peng Chen, Gregor Krzmanc, Daniel Zeng, Percy Liang, and Jure Leskovec. PRODIGY: Enabling In-context Learning Over Graphs. In _NeurIPS_, 2023.
* [18] Xunqiang Jiang, Yuanfu Lu, Yuan Fang, and Chuan Shi. Contrastive Pre-Training of GNNs on Heterogeneous Graphs. In _CIKM_, pages 803-812, 2021.
* [19] Ming Jin, Yizhen Zheng, Yuan-Fang Li, Chen Gong, Chuan Zhou, and Shirui Pan. Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph Representation Learning. In _IJCAI_, pages 1477-1483, 2021.
* [20] Wei Jin, Tyler Derr, Yiqi Wang, Yao Ma, Zitao Liu, and Jiliang Tang. Node Similarity Preserving Graph Convolutional Networks. In _WSDM_, pages 148-156, 2021.

* [21] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _ICLR_, 2017.
* [22] Nils Kriege and Petra Mutzel. Subgraph matching kernels for attributed graphs. In _Proceedings of the 29th International Coference on International Conference on Machine Learning_, ICML'12, page 291-298, Madison, WI, USA, 2012. Omnipress.
* [23] Guohao Li, Matthias Muller, Bernard Ghanem, and Vladlen Koltun. Training graph neural networks with 1000 layers. In _International conference on machine learning_, pages 6437-6449. PMLR, 2021.
* [24] Guohao Li, Matthias Muller, Guocheng Qian, Itzel C Delgadillo, Abdullellah Abualshour, Ali Thabet, and Bernard Ghanem. Deepgcns: Making gens go as deep as cnns. _IEEE transactions on pattern analysis and machine intelligence (TPAMI)_, 45(6):6923-6939, 2021.
* [25] Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 9267-9276, 2019.
* [26] Shengrui Li, Xueting Han, and Jing Bai. AdapterGNN: Efficient Delta Tuning Improves Generalization Ability in Graph Neural Networks. _arXiv preprint_, 2023.
* [27] Yuening Li, Zhengzhang Chen, Daochen Zha, Kaixiong Zhou, Haifeng Jin, Haifeng Chen, and Xia Hu. Automated anomaly detection via curiosity-guided search and self-imitation learning. _IEEE Transactions on Neural Networks and Learning Systems_, 2021.
* [28] Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, and Jeffrey Xu Yu. A survey of graph meets large language model: Progress and future directions. _arXiv preprint arXiv:2311.12399_, 2023.
* [29] Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, and Jia Li. Zerog: Investigating cross-dataset zero-shot transferability in graphs. In _Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 1725-1735, 2024.
* [30] Yuhan Li, Peisong Wang, Xiao Zhu, Aochuan Chen, Haiyun Jiang, Deng Cai, Victor Wai Kin Chan, and Jia Li. Glbench: A comprehensive benchmark for graph with large language models. _arXiv preprint arXiv:2407.07457_, 2024.
* [31] Zhixun Li, Xin Sun, Yifan Luo, Yanqiao Zhu, Dingshuo Chen, Yingtao Luo, Xiangxin Zhou, Qiang Liu, Shu Wu, Liang Wang, et al. Gslb: The graph structure learning benchmark. _Advances in Neural Information Processing Systems_, 36, 2024.
* [32] Yang Liu, Jiashun Cheng, Haihong Zhao, Tingyang Xu, Peilin Zhao, Fugee Tsung, Jia Li, and Yu Rong. Segno: Generalizing equivariant graph neural networks with physical inductive biases. In _The Twelfth International Conference on Learning Representations_.
* [33] Zemin Liu, Xingtong Yu, Yuan Fang, and Xinming Zhang. Graphprompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks. In _The Web Conference_, pages 417-428, 2023.
* [34] Yahui Long, Min Wu, Yong Liu, Yuan Fang, Chee Keong Kwoh, Jinmiao Chen, Jiawei Luo, and Xiaoli Li. Pre-training graph neural networks for link prediction in biomedical networks. _Bioinformatics_, 38(8):2254-2262, 2022.
* [35] Galileo Namata, Ben London, Lise Getoor, Bert Huang, and U Edu. Query-driven active surveying for collective classification. In _10th international workshop on mining and learning with graphs_, volume 8, page 1, 2012.
* [36] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _NeurIPS_, 32:8026-8037, 2019.
* [37] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric graph convolutional networks. In _International Conference on Learning Representations_, 2019.
* [38] Chen Qian, Huayi Tang, Zhirui Yang, Hong Liang, and Yong Liu. Can Large Language Models Empower Molecular Property Prediction? _arXiv preprint_, 2023.
* [39] Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying WEI, Wenbing Huang, and Junzhou Huang. Self-Supervised Graph Transformer on Large-Scale Molecular Data. In _NeurIPS_, volume 33, pages 12559-12571, 2020.

* [40] Michael T Rosenstein, Zvika Marx, Leslie Pack Kaelbling, and Thomas G Dietterich. To transfer or not to transfer. In _NeurIPS_, volume 898, 2005.
* [41] Ryan Rossi and Nesreen Ahmed. The network data repository with interactive graph analytics and visualization. In _Proceedings of the AAAI conference on artificial intelligence_, volume 29, 2015.
* [42] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. _AI magazine_, 29(3):93-93, 2008.
* [43] Yunsheng Shi, Zhengjie Huang, Shikun Feng, Hui Zhong, Wenjing Wang, and Yu Sun. Masked label prediction: Unified message passing model for semi-supervised classification. In _IJCAI_, 2021.
* [44] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In _EMNLP_, pages 4222-4235, 2020.
* [45] Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization. In _ICLR_, 2020.
* [46] Mingchen Sun, Kaixiong Zhou, Xin He, Ying Wang, and Xin Wang. GPPT: Graph Pre-Training and Prompt Tuning to Generalize Graph Neural Networks. In _KDD_, pages 1717-1727, 2022.
* [47] Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. All in one: Multi-task prompting for graph neural networks. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 2120-2131, 2023.
* [48] Xiangguo Sun, Hong Cheng, Bo Liu, Jia Li, Hongyang Chen, Guandong Xu, and Hongzhi Yin. Self-Supervised Hypergraph Representation Learning for Sociological Analysis. _TKDE_, 2023.
* [49] Xiangguo Sun, Hongzhi Yin, Bo Liu, Qing Meng, Jiuxin Cao, Alexander Zhou, and Hongxu Chen. Structure learning via meta-hyperedge for dynamic rumor detection. _IEEE Transactions on Knowledge and Data Engineering_, 2022.
* [50] Xiangguo Sun, Jiawen Zhang, Xixi Wu, Hong Cheng, Yun Xiong, and Jia Li. Graph prompt learning: A comprehensive survey and beyond. _arXiv preprint arXiv:2311.16534_, 2023.
* [51] Zhen Tan, Ruocheng Guo, Kaize Ding, and Huan Liu. Virtual Node Tuning for Few-shot Node Classification. In _KDD_, pages 2177-2188, 2023.
* [52] Jianheng Tang, Jiajin Li, Ziqi Gao, and Jia Li. Rethinking graph neural networks for anomaly detection. In _International Conference on Machine Learning_, 2022.
* [53] Jie Tang, Jimeng Sun, Chi Wang, and Zi Yang. Social influence analysis in large-scale networks. In _Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 807-816, 2009.
* [54] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In _ICLR_, 2017.
* [55] Petar Velickovic, William Fedus, William L Hamilton, Pietro Lio, Yoshua Bengio, and R Devon Hjelm. Deep graph infomax. _arXiv preprint arXiv:1809.10341_, 2018.
* [56] Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu, Yuxiao Dong, and Anshul Kanakia. Microsoft academic graph: When experts are not enough. _Quantitative Science Studies_, 1(1):396-413, 2020.
* [57] Liyuan Wang, Mingtian Zhang, Zhongfan Jia, Qian Li, Chenglong Bao, Kaisheng Ma, Jun Zhu, and Yi Zhong. Afec: Active forgetting of negative transfer in continual learning. _Advances in Neural Information Processing Systems_, 34:22379-22391, 2021.
* [58] Song Wang, Yushun Dong, Xiao Huang, Chen Chen, and Jundong Li. Faith: Few-shot graph classification with hierarchical task graphs. _arXiv preprint arXiv:2205.02435_, 2022.
* [59] Yingying Wang, Yun Xiong, Xixi Wu, Xiangguo Sun, and Jiawei Zhang. Advanced drug interaction event prediction. _arXiv preprint arXiv:2402.11472_, 2024.
* [60] Jun Xia, Lirong Wu, Jintao Chen, Bozhen Hu, and Stan Z Li. Simgrace: A simple framework for graph contrastive learning without data augmentation. In _Proceedings of the ACM Web Conference 2022_, pages 1070-1079, 2022.

* [61] Yaochen Xie, Zhao Xu, and Shuiwang Ji. Self-Supervised Representation Learning via Latent Graph Prediction. In _ICML_, pages 24460-24477, 2022.
* [62] Dongkuan Xu, Wei Cheng, Dongsheng Luo, Haifeng Chen, and Xiang Zhang. InfoGCL: Information-Aware Graph Contrastive Learning. In _NeurIPS_, volume 34, pages 30414-30425, 2021.
* [63] Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In _Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining_, pages 1365-1374, 2015.
* [64] Yuhao Yang, Lianghao Xia, Da Luo, Kangyi Lin, and Chao Huang. Graphpro: Graph pre-training and prompt learning for recommendation. In _Proceedings of the ACM on Web Conference 2024_, pages 3690-3699, 2024.
* [65] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. In _NeurIPS_, pages 5812-5823, 2020.
* [66] Haihong Zhao, Aochuan Chen, Xiangguo Sun, Hong Cheng, and Jia Li. All in one and one for all: A simple yet effective method towards cross-domain graph pretraining. In _Proceedings of the 27th ACM SIGKDD international conference on knowledge discovery & data mining (KDD'24)_, 2024.
* [67] Haihong Zhao, Bo Yang, Jiaxu Cui, Qianli Xing, Jiaxing Shen, Fujin Zhu, and Jiannong Cao. Effective fault scenario identification for communication networks via knowledge-enhanced graph neural networks. _IEEE Transactions on Mobile Computing_, 2023.
* [68] Haihong Zhao, Chenyi Zi, Yang Liu, Chen Zhang, Yan Zhou, and Jia Li. Weakly supervised anomaly detection via knowledge-data alignment. In _Proceedings of the ACM on Web Conference 2024_, pages 4083-4094, 2024.
* [69] Zinan Zheng, Yang Liu, Jia Li, Jianhua Yao, and Yu Rong. Relaxing continuous constraints of equivariant graph neural networks for broad physical dynamics learning. In _Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 4548-4558, 2024.
* [70] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph Contrastive Learning with Adaptive Augmentation. In _The Web Conference_, pages 2069-2080, 2021.
* [71] Yun Zhu, Jianhao Guo, and Siliang Tang. SGL-PT: A Strong Graph Learner with Graph Prompt Tuning. _arXiv preprint_, 2023.

Detailed Description of Methods

**End-to-end Grpah Models**

* **GCN (Graph Convolutional Network [21]):** A method that utilizes convolution operation on the graph to propagate information from a node to its neighboring nodes, enabling the network to learn a representation for each node based on its local neighborhood.
* **GraphSAGE (Graph Sample and Aggregate [13]:)** A general inductive learning framework that generates node embeddings by sampling and aggregating features from a node's local neighborhood.
* **GAT (Graph Attention Networks [54]):** A graph neural network (GNN) framework that incorporates the attention mechanism. It assigns varying levels of importance to different nodes during the neighborhood information aggregation process, allowing the model to focus on the most informative parts.
* **GT (Graph Transformer [43]):** An adaptation of the neural network architecture that applies the principles of the Transformer model to graph-structured data. It uses masks in the self-attention process to leverage the graph structure and enhance model efficiency.

**Pre-training**

* **DGI[55]:** capitalizes on a self-supervised method for pretraining, which is based on the concept of mutual information (MI). It maximizes the MI between the local augmented instances and the global representation.
* **GraphMAE[14]:** employs the masked autoencoder framework to minimize the difference between the original features of the masked nodes and their reconstructed features. This process enhances the model's ability to understand and embed graph structures effectively.
* **EdgePreGPPT[46]:** is proposed in GPPT to calculate the dot product as the link probability of the node pair.
* **EdgePreGprompt[33]:** is proposed in Gprompt n, for a set of label-free graphs G, we sample several triplets from each graph to increase the similarity between the contextual subgraphs of linked pair while decreasing unlinked pair.
* **GraphCL[65]:** applies different graph augmentations to exploit the structural information on the graphs and aims to maximize the agreement between different augmentations for graph pre-training.
* **SimGRACE[60]:** tries to perturb the graph model parameter spaces and narrow down the gap between different perturbations for the same graph.

**Graph Prompt Methods**

* **GPPT ('Prompt as token') [46]:** introduces a graph prompting function that transforms a standalone node into a token pair composed of a task token, representing a node label, and a structure token, describing the given node. This reformulates the downstream node classification task to resemble edge prediction. As a result, the pre-trained GNNs can be directly applied without extensive fine-tuning to evaluate the linking probability of the token pair and produce the node classification decision. Since GPPT originally is designed for node classification, in this work, to adapt to graph classification, we follow the core thoughts of GPPT and make the following modifications: (1) Introduce \(m\) graph task tokens, one for each graph label. (2) Extract \(N\) structure tokens from a graph with \(N\) nodes, the same as the GPPT for node tasks. (3) Combine each structure token with all graph tokens to form \(m\) token pairs. (4) Calculate an \(m\)-dimension link prediction probability vector for the \(m\) structure tokens and choose the class with maximum probability as the prediction result. (6) Aggregate the link prediction results using a voting strategy to determine the graph class. (7) Update the learnable tokens based on the prediction loss.
* **Gprompt ('Prompt as token') [33]:** employs subgraph similarity as a mechanism to unify different pretext and downstream tasks, including link prediction, node classification, and graph classification. A learnable prompt is subsequently tuned for each downstream task.
* **All-in-one ('Prompt as graph') [47]:** involves freezing the parameters of pre-trained graph models and incorporating learnable graph prompts as insert patterns to original induced graphs on the downstream node classification task.
* **GPF ('Prompt as token') [8]:** applies a consistent prompt feature to all nodes, regardless of their contextual differences.

* **GPF-plus ('Prompt as token') [8]:** enhances the method by assigning prompt features based on node type, ensuring uniformity within each category but without considering the context of individual graphs. Note that GPF and GPF-plus inherently are primarily evaluated in graph classification tasks and are not readily adaptable to node classification. In this work, we address this limitation by inducing a subgraph for each node within a node-level graph. This approach effectively reformulates the node classification task into a graph classification task, allowing for the integration of inserted prompt features.

## Appendix B Related Work

### Graph pre-training

Pre-training of graphs is a crucial step in graph representation learning for pre-training, prompting, and predicting paradigms, which provides a solid foundation for downstream tasks by utilizing existing information to encode graph structures. it aims to learn some general knowledge for the graph model with easily accessible information to reduce the annotation costs of new tasks. Node-level pre-training obtains local structural representations, which can be used for downstream tasks. Comparative learning learns by maximizing the mutual information(MI) between the original view and the self-supervised view [6; 70; 19] The prediction model reconstructs perturbed data [13; 14]. However, predictive models are difficult to capture higher-order information. Edge-level pre-training includes distinguishing whether there are edges between node pairs [20; 34], or reconstructing masked edges by restoring including graph reconstruction [39; 61] and contrastive learning targets either local patches of node features and global graph features [55; 45]. Multi-task pre-training addresses multiple optimization objectives, covering various graph-related aspects. This approach enhances generalization and mitigates negative transfer issues. For instance, Hu et al. [16] pre-trained a GNN at both the node and graph levels, allowing the GNN to learn valuable local and global representations simultaneously. Additionally, some studies have utilized contrastive learning at different levels [18; 62], or jointly optimized both contrastive loss and graph reconstruction error [26].

### Graph prompt learning

While Pre-training effectively encodes global information and generates valuable graph-level representations, a significant challenge lies in transferring knowledge from a specific pretext task to downstream tasks with substantial gaps, potentially resulting in negative transfer [40]. Instead of fine-tuning a pre-trained model with an adaptive task head, prompt learning aims to reformulate input data to fit the pretext. Many effective prompt methods were initially developed in the field of NLP, encompassing various approaches such as hand-crafted prompts exemplified by GPT-4 [1], discrete prompts like AutoPrompt [27], and trainable prompts in continuous spaces such as Prefix-Tuning [44]. Due to their remarkable success, prompt-based methods have increasingly been applied to the graph domain [4].

Graph prompt learning usually has two formalities: on the one hand, it can be regarded as tokens [3; 51; 12]. For example, GPF takes a prompt token as some additional features are added to the original graph. Based on that, GPF-plus [8] trains several independent basis vectors and utilizes attentive aggregation of these basis vectors with the assistance of several learnable linear projections. It also solved problems when training graphs have different scales and large-scale input graphs. GPPT [46] defines graph prompts as additional tokens that contain task tokens and structure tokens that align downstream node task and link prediction pretext. Furthermore, some works [71] use graph prompt to assist graph pooling operation (a.k.a Readout), like Gprompt [33] inserts the prompt vector to the graph pooling by element-wise multiplication. On the other hand, graph prompt is regarded as graphs [11; 17], Such as All-in-one [47] inserts the token graphs as prompt to the original graph and all links between tokens and original graphs, these links can be pre-calculated by the dot product between one token to another token (inner links) or one token to another original node (cross links).

## Appendix C Detailed Description of Datasets

**Homophilic:** Cora and Citeseer datasets [42] consist of a diverse collection of computer science publications, where each node is characterized by a bag-of-words representation of papers and a categorical label indicating the paper topic. Pubmed dataset [35] comprises articles related to diabetes from the PubMed database. Each node in this dataset is represented by an attribute vector containing TF/IDF-weighted word frequencies, accompanied by a label specifying the particular type of diabetes discussed in the publication. ogbn-arxiv [15] is a large-scale directed graph representing the citation network between all Computer Science (CS) arXiv papers indexed by MAG [56]. Each node is an arXiv paper and each directed edge indicates that one paper cites another one. Each paper comes with a 128-dimensional feature vector obtained by averaging the embeddings of words in its title and abstract.

**Heterophilic:** Cornell, Texas, and Wisconsin are three subdatasets derived from the WebKB dataset [37], compiled from multiple universities' web pages. Each node within these datasets represents a web page, with edges denoting hyperlinks between pages. The node features are represented as bag-of-words representations of the web pages. Additionally, the web pages are manually categorized into five distinct labels: student, project, course, staff, and faculty. Actor is a co-occurrence network in which each node corresponds to an actor and edges indicate their co-occurrence on the same Wikipedia page [53, 31].

**Social networks:** IMDB-BINARY [63] is a movie collaboration dataset. The nodes represent actors/actresses, the edges represent collaborations between actors/actresses appearing in the same movie, and the graph labels indicate the genre (Action or Romance). COLLAB [63] is a scientific collaboration dataset. The nodes represent researchers, the edges represent the collaboration relationships between researchers, and the graph labels indicate the field of research (High Energy Physics, Condensed Matter Physics, or Astro Physics).

**Biological:** PROTEINS [58] is a collection of protein graphs that include the amino acid sequence, conformation, structure, and features such as active sites of the proteins. The nodes represent the secondary structures, and each edge depicts the neighboring relation in the amino-acid sequence or in 3D space. The nodes belong to three categories, and the graphs belong to two classes. ENZYMES [2] is a dataset of 600 enzymes collected from the BRENDA enzyme database. These enzymes are labeled into 6 categories according to their top-level EC enzyme. D&D [7] consists of graph representations of 1,178 proteins. In each graph, nodes represent amino acids, and there is an edge if they are less than six Angstroms apart. Graphs are labeled according to whether they are enzymes or not.

**Small Molecule:** COX2 [41] is a dataset of molecular structures including 467 cyclooxygenase-2 inhibitors, in which each node is an atom, and each edge represents the chemical bond between atoms, such as single, double, triple or aromatic. All the molecules belong to two categories. BZR [41] is a collection of 405 ligands for benzodiazepine receptors, in which each ligand is represented by a graph. All these ligands belong to 2 categories. MUTAG [22] is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete labels.

## Appendix D Additional Details

**Metrics.** In node and graph classification, **AUROC** and **F1-score** are another two important evaluation metrics. AUROC stands for _Area Under the Receiver Operating Characteristic Curve_. It shows how well the model can distinguish between different classes. A score of 1 means perfect classification, while 0.5 indicates the model is guessing randomly. F1-score combines precision and recall into a single number. Precision measures how many of the predicted positives are actually correct, and recall measures how many of the actual positives the model correctly identified. The F1-score ranges from 0 to 1, with 1 being the best. It is particularly useful in cases where class distributions are imbalanced. In this work, we compute the macro average values for each class. For both AUROC and F1-score, in multi-class classification tasks, each class is treated as the positive class once, while the other classes are treated as negative, and the metrics are averaged.

**Implementation Details.** To ensure a comprehensive evaluation and maintain fairness across a broad spectrum of models, we develop an open-source package named ProG3.

Footnote 3: https://github.com/sheldonresearch/ProG

**Software and Hardware.** For a fair comparison, all methods are implemented by our developed library ProG, which is built upon PyTorch [36], and unless specifically indicated, the encoders for all methods are Graph Convolutional Networks. All experiments are conducted on a Linux server with GPU (NVIDIA GeForce 3090, NVIDIA GeForce 4090, and NVIDIA A800) and CPU (AMD EPYC 7763), using PyTorch 2.0.1 [36], PyG 2.3.1 [9] and Python 3.9.17.

**Hyperparameter Settings.** To ensure a comprehensive evaluation process and maintain fairness, we conduct an in-depth analysis of various graph prompt methods. Then, we carefully design a unified random search strategy utilized for selecting a set of optimized hyperparameters for each prompt method on each dataset, going beyond the default settings. Given the significant variations in hyperparameters across different prompt methods, we focus on optimizing three types of general hyperparameters via random search. Concretely, we search for the optimal learning rate within the range \(10^{uniform(-3,-1)}\), and optimal weight decay within \(10^{uniform(-5,-6)}\). Additionally, 32, 64, or 128 batch sizes are selected with equal probability in each trial. Moreover, supervised methods and 'Pre-train & Fine-tune' methods in this work also undergo the same random search process for hyperparameter optimization.

## Appendix E Additional Experimental Analysis

### Adaptivity of Graph Prompts on Different Graphs.

**Various Domains.** GPL methods, particularly those of the Prompt as Token type, exhibit _performance inconsistencies across graph-level datasets from various domains_, w.r.t Table 3. Sometimes, these methods _may encounter negative transfer issues_ to varying degrees, except for the BZR and DD datasets. GPPT, in particular, faces significant challenges, likely because first-order subgraphs fail to comprehensively capture graph patterns, introducing noise and affecting transferability.

**Large-scale Datasets.** Referring to Table 2, on the large-scale node-level dataset (i.e., ogbn-arxiv), Gprompt, GPF, and GPF-plus significantly surpass the Supervised and classical 'Pre-train & Fine-tune' methods, demonstrating good scalability. However, the scalability of GPL methods involving subgraph operations, such as GPPT and All-in-one, needs improvement as they fail to fully capture large-scale graph patterns with their subgraph information. Furthermore, regarding the results of the relatively large graph-level dataset (i.e., DD) in Table 3, all GPL methods achieve positive transfer and outperform classical 'Pre-train & Fine-tune' methods. Despite this success, the magnitude of improvement indicates that _the transferability of GPL methods on large-scale datasets requires further enhancement._

**Homophilic & Heterophilic Datasets.** Drawing from Table 2, most GPL methods can handle both homophilic and heterophilic graph datasets effectively, with the exception of GPPT. GPPT outperforms the Supervised method on homophilic graph datasets but falls short compared to classical 'Pre-train & Fine-tune' methods. On heterophilic datasets, where node attributes differ significantly, GPPT experiences negative transfer issues, possibly due to instability caused by its clustering algorithms.

**Pure Structural Dataset.** Observing Table 3, the prompting capability for purely structural graph datasets, such as IMDB-B and COLLAB, still requires improvement. While classical 'Pre-train & Fine-tune' methods achieve positive transfer on these datasets, some variants of the All-in-one method exhibit negative transfer, highlighting _the need for further refinement in handling purely structural graphs._

### Additional Experimental Results with Different Shots and Metrics

The optimal performance of the graph prompt method, along with other methods, under 3-shot and 5-shot learning settings, is displayed in Tables 7- 10. Building on previous experimental outcomes, it is evident that the graph prompt method significantly boosts the effectiveness of various pre-trained graph models in 1/3/5-shot learning settings, thereby enhancing the transferability of pre-trained knowledge in the original 'Pre-train & Fine-tune' method. Specifically, GPF-plus and All-in-one approaches consistently prove to be the most compatible with different pre-trained methods for node and graph tasks, respectively.

Moreover, the experimental results of various methods under 3/5-shot learning settings for three key metrics--Accuracy, F1-score, and AUROC--are comprehensively presented in Tables 11- 28.

[MISSING_PAGE_FAIL:18]

\begin{table}
\begin{tabular}{c|c c c c c c c c c} \hline \hline
**Methods/Datasets** & **IMDB-B** & **COLLAB** & **PROTEINS** & **MUTAG** & **ENZYMES** & **COX2** & **BZR** & **DD** \\ \hline Supervised & 62.60 \(\pm\)0.41 & 55.23 \(\pm\)4.26 & 62.90 \(\pm\)5.03 & 73.47 \(\pm\)5.92 & 25.67 \(\pm\)0.48 & 64.99 \(\pm\)1.02 & 51.48 \(\pm\)2.29 & 63.59 \(\pm\)2.86 \\ \hline Pre-train \& Fine-tune & 65.40 \(\pm\)5.33 & 60.72 \(\pm\)2.09 & 63.33 \(\pm\)4.13 & 75.33 \(\pm\)18.90 & 27.46 \(\pm\)12.90 & 73.19 \(\pm\)0.63 & 72.96 \(\pm\)01.98 & 64.71 \(\pm\)3.22 \\ \hline GPPT & 66.37 \(\pm\)5.59 & 54.05 \(\pm\)4.58 & 58.27 \(\pm\)4.73 & 70.53 \(\pm\)5.30 & 22.17 \(\pm\)2.34 & 67.88 \(\pm\)11.74 & 69.63 \(\pm\)14.36 & 60.02 \(\pm\)3.24 \\ \hline All-in-one & 63.62 \(\pm\)2.30 & 57.86 \(\pm\)5.88 & 71.37 \(\pm\)4.90 & 84.67 \(\pm\)0.73 & 26.71 \(\pm\)2.17 & 62.95 \(\pm\)8.67 & 62.78 \(\pm\)10.18 & 63.44 \(\pm\)1.35 \\ \hline GPprompt & 66.70 \(\pm\)3.87 & 60.76 \(\pm\)0.68 & 62.94 \(\pm\)1.38 & 73.07 \(\pm\)2.13 & 21.46 \(\pm\)2.27 & 53.35 \(\pm\)7.75 & 59.38 \(\pm\)41.43 & 58.28 \(\pm\)2.18 \\ \hline GPF & 67.80 \(\pm\)5.58 & 59.65 \(\pm\)2.25 & 63.37 \(\pm\)4.37 & 74.00 \(\pm\)3.65 & 27.00 \(\pm\)0.78 & 66.27 \(\pm\)14.37 & 61.05 \(\pm\)11.51 & 61.06 \(\pm\)2.63 \\ \hline GPF-plus & 68.13 \(\pm\)3.81 & 60.68 \(\pm\)4.67 & 63.51 \(\pm\)2.89 & 73.87 \(\pm\)5.51 & 26.87 \(\pm\)1.89 & 72.87 \(\pm\)10.17 & 71.54 \(\pm\)11.81 & 64.80 \(\pm\)3.65 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Comparison of different methods across various datasets on 5-shot graph classification tasks.

\begin{table}
\begin{tabular}{c|c c c c c c c c} \hline \hline
**Training schemes** & **Methods** & Cora & CiteSeer & Pubmed & Wisconsin & Texas & Actor & Ogba-ariv \\ \hline self-supervie & GCN & 26.56 \(\pm\)5.56 & 21.78 \(\pm\)7.92 & 39.37 \(\pm\)16.34 & 41.60 \(\pm\)3.10 & 37.97 \(\pm\)5.90 & 20.57 \(\pm\)4.47 & 10.99 \(\pm\)3.19 \\ \hline \multirow{6}{*}{Pretrain + Fine-tuning} & DGI & 33.15 \(\pm\)7.84 & 21.64 \(\pm\)3.39 & 42.01 \(\pm\)12.54 & 37.49 \(\pm\)1.53 & 45.31 \(\pm\)0.51 & 19.76 \(\pm\)1.55 & 7.21 \(\pm\)2.01 \\  & GraphMME & 32.93 \(\pm\)3.17 & 21.26 \(\pm\)3.99 & 42.99 \(\pm\)12.54 & 36.80 \(\pm\)17.17 & 37.81 \(\pm\)6.92 & 19.86 \(\pm\)2.70 & 13.25 \(\pm\)3.60 \\  & EdgeProGPT & 38.12 \(\pm\)3.29 & 18.09 \(\pm\)5.90 & 46.74 \(\pm\)1.40 & 35.31 \(\pm\)3.11 & 47.66 \(\pm\)2.37 & 19.17 \(\pm\)1.53 & 16.21 \(\pm\)3.82 \\  & EdgeProGPT & 35.57 \(\pm\)6.53 & 22.28 \(\pm\)3.80 & 41.50 \(\pm\)5.74 & 40.69 \(\pm\)1.43 & 40.62 \(\pm\)7.95 & 20.74 \(\pm\)1.42 & 14.83 \(\pm\)2.38 \\  & GGI & 52.61 \(\pm\)1.73 & 22.00 \(\pm\)4.31 & 42.49 \(\pm\)11.59 & 33.94 \(\pm\)17.74 & 30.13 \(\pm\)1.63 & 20.19 \(\pm\)1.96 & 45.11 \(\pm\)1.99 \\  & SimGRACE & 40.40 \(\pm\)4.36 & 35.05 \(\pm\)4.37 & 35.99 \(\pm\)17.08 & 37.37 \(\pm\)3.68 & 46.88 \(\pm\)4.44 & 19.78 \(\pm\)3.80 & 8.13 \(\pm\)3.20 \\ \hline \multirow{6}{*}{GPPTPnormt} & DGI & 30.47 \(\pm\)5.53 & 37.26 \(\pm\)0.17 & 36.52 \(\pm\)7.74 & 29.94 \(\pm\)10.40 & 29.29 \(\pm\)15.57 & 21.76 \(\pm\)2.00 & 3.80 \(\pm\)16.19 \\  & GraphMME & 27.39 \(\pm\)10.62 & 21.54 \(\pm\)6.00 & 48.31 \(\pm\)17.72 & 29.83 \(\pm\)3.94 & 25.04 \(\pm\)10.38 & 22.58 \(\pm\)10.74 & 4.35 \(\pm\)6.01 \\  & EdgeProGPT & 30.37 \(\pm\)3.40 & 21.06 \(\pm\)4.97 & 36.47 \(\pm\)2.96 & 24.39 \(\pm\)5.60 & 39.39 \(\pm\)9.36 & 19.95 \(\pm\)8.06 & 19.58 \(\pm\)4.07 \\  & EdgeProGPT & 25.52 \(\pm\)4.42 & 21.85 \(\pm\)4.30 & 46.20 \(\pm\)10.76 & 30.40 \(\pm\)6.81 & 22.68 \(\pm\)12.82 & 22.51 \(\pm\)1.13 & 2.05 \(\pm\)1.13 \\  & GGI & 41.35 \(\pm\)1.42 & 26.53 \(\pm\)3.49 & 38.34 \(\pm\)11.34 & 25.03 \(\pm\)5.37 & 31.11 \(\pm\)15.31 & 22.51 \(\pm\)1.73 & 7.15 \(\pm\)1.42 \\  & SimGRACE & 27.86 \(\pm\)2.79 & 25.06 \(\pm\)5.40 & 36.70 \(\pm\)4.90 & 29.93 \(\pm\)6.44 & 25.67 \(\pm\)0.01 & 20.97 \(\pm\)2.30 & 5.50 \(\pm\)1.50 \\ \hline \multirow{6}{*}{ALL-in-one} & DGI & 47.52 \(\pm\)2.50 & 39.37 \(\pm\)4.30 & 37.42 \(\pm\)4.25 & 69.54 \(\pm\)5.88 & 57.38 \(\pm\)2.28 & 21.03 \(\pm\)3.60 & 1.93 \(\pm\)4.18 \\  & GraphMME & 23.09 \(\pm\)1.90 & 18.08 \(\pm\)5.23 & 33.19 \(\pm\)19.91 & 67.41 \(\pm\)0.22 & 52.61 \(\pm\)21.41 & 23.31 \(\pm\)1.00 & 4.19 \(\pm\)6.01 \\  & EdgeProGPT & 49.63 \(\pm\)2.36 & 35.06 \(\pm\)2.37 & 40.73

[MISSING_PAGE_EMPTY:20]

[MISSING_PAGE_EMPTY:21]

[MISSING_PAGE_EMPTY:22]

[MISSING_PAGE_EMPTY:23]

[MISSING_PAGE_EMPTY:24]

[MISSING_PAGE_EMPTY:25]

[MISSING_PAGE_EMPTY:26]

[MISSING_PAGE_EMPTY:27]

[MISSING_PAGE_EMPTY:28]

\begin{table}
\begin{tabular}{l|l l l l l l} \hline \hline \multicolumn{7}{c}{Pre-train \& Fine-tune} \\ \hline Model & DGI & GraphMAE & EdgePreGPPT & EdgePreGprompt & GCL & SimGRACE \\ \hline GAT & 16.00\({}_{\pm 6.24}\) & 37.60\({}_{\pm 10.69}\) & 20.00\({}_{\pm 3.82}\) & 33.37\({}_{\pm 4.76}\) & 18.86\({}_{\pm 1.88}\) & 28.00\({}_{\pm 9.40}\) \\ GraphSAGE & 40.69\({}_{\pm 9.46}\) & 43.77\({}_{\pm 12.43}\) & 26.06\({}_{\pm 5.38}\) & 29.94\({}_{\pm 3.75}\) & 36.57\({}_{\pm 4.88}\) & 9.37\({}_{\pm 2.72}\) \\ GT & 25.71\({}_{\pm 3.07}\) & 39.77\({}_{\pm 8.42}\) & 23.20\({}_{\pm 2.65}\) & 28.23\({}_{\pm 6.64}\) & 11.77\({}_{\pm 1.06}\) & 14.51\({}_{\pm 5.08}\) \\ \hline \multicolumn{7}{c}{GSPPT} \\ \hline Model & DGI & GraphMAE & EdgePreGPPT & EdgePreGprompt & GCL & SimGRACE \\ \hline GAT & 22.17\({}_{\pm 6.13}\) & 33.94\({}_{\pm 7.76}\) & 23.43\({}_{\pm 4.46}\) & 37.94\({}_{\pm 7.11}\) & 26.86\({}_{\pm 6.12}\) & 29.83\({}_{\pm 8.04}\) \\ GraphSAGE & 26.51\({}_{\pm 8.00}\) & 30.51\({}_{\pm 5.40}\) & 21.49\({}_{\pm 5.17}\) & 24.23\({}_{\pm 6.55}\) & 20.91\({}_{\pm 7.11}\) & 25.37\({}_{\pm 7.22}\) \\ GT & 27.20\({}_{\pm 10.48}\) & 29.83\({}_{\pm 5.80}\) & 28.00\({}_{\pm 6.01}\) & 23.31\({}_{\pm 3.01}\) & 27.66\({}_{\pm 0.69}\) & 25.03\({}_{\pm 5.43}\) \\ \hline \multicolumn{7}{c}{All-in-one} \\ \hline Model & DGI & GraphMAE & EdgePreGPPT & EdgePreGprompt & GCL & SimGRACE \\ \hline GAT & 69.44\({}_{\pm 5.19}\) & 36.25\({}_{\pm 10.63}\) & 91.25\({}_{\pm 4.33}\) & 92.65\({}_{\pm 3.75}\) & 42.85\({}_{\pm 9.16}\) & 36.61\({}_{\pm 14.86}\) \\ GraphSAGE & 94.73\({}_{\pm 5.86}\) & 97.55\({}_{\pm 0.78}\) & 98.60\({}_{\pm 0.87}\) & 100.00\({}_{\pm 0.00}\) & 83.01\({}_{\pm 18.37}\) & 85.12\({}_{\pm 16.29}\) \\ GT & 68.59\({}_{\pm 11.78}\) & 98.48\({}_{\pm 0.70}\) & 96.74\({}_{\pm 1.01}\) & 100.00\({}_{\pm 0.00}\) & 70.23\({}_{\pm 8.80}\) & 45.70\({}_{\pm 11.83}\) \\ \hline \multicolumn{7}{c}{Gprompt} \\ \hline Model & DGI & GraphMAE & EdgePreGPPT & EdgePreGprompt & GCL & SimGRACE \\ \hline GAT & 58.25\({}_{\pm 13.83}\) & 67.77\({}_{\pm 15.91}\) & 94.17\({}_{\pm 2.26}\) & 84.28\({}_{\pm 3.63}\) & 80.11\({}_{\pm 16.65}\) & 57.18\({}_{\pm 12.60}\) \\ GraphSAGE & 66.48\({}_{\pm 12.88}\) & 83.49\({}_{\pm 15.93}\) & 87.52\({}_{\pm 3.79}\) & 82.16\({}_{\pm 2.64}\) & 65.50\({}_{\pm 6.48}\) & 72.61\({}_{\pm 5.97}\) \\ GT & 56.03\({}_{\pm 7.33}\) & 73.50\({}_{\pm 9.72}\) & 76.97\({}_{\pm 13.39}\) & 80.07\({}_{\pm 2.84}\) & 59.31\({}_{\pm 10.17}\) & 69.30\({}_{\pm 10.57}\) \\ \hline \multicolumn{7}{c}{GSP} \\ \hline Model & DGI & GraphMAE & EdgePreGPPT & EdgePreGprompt & GCL & SimGRACE \\ \hline GAT & 63.64\({}_{\pm 3.91}\) & 78.59\({}_{\pm 18.57}\) & 97.90\({}_{\pm 0.79}\) & 91.84\({}_{\pm 5.19}\) & 74.93\({}_{\pm 4.49}\) & 65.67\({}_{\pm 15.07}\) \\ GraphSAGE & 68.12\({}_{\pm 13.96}\) & 67.66\({}_{\pm 13.37}\) & 74.06\({}_{\pm 14.59}\) & 72.45\({}_{\pm 10.14}\) & 59.69\({}_{\pm 21.37}\) & 78.37\({}_{\pm 14.84}\) \\ GT & 39.85\({}_{\pm 4.83}\) & 71.26\({}_{\pm 14.43}\) & 72.67\({}_{\pm 13.36}\) & 81.33\({}_{\pm 3.41}\) & 78.19\({}_{\pm 2.19}\) & 67.90\({}_{\pm 10.53}\) \\ \hline \multicolumn{7}{c}{GPF-plus} \\ \hline Model & DGI & GraphMAE & EdgePreGPPT & EdgePreGprompt & GCL & SimGRACE \\ \hline GAT & 92.62\({}_{\pm 8.60}\) & 96.37\({}_{\pm 4.44}\) & 99.29\({}_{\pm 1.41}\) & 97.88\({}_{\pm 4.24}\) & 86.12\({}_{\pm 7.73}\) & 94.17\({}_{\pm 2.63}\) \\ GraphSAGE & 77.85\({}_{\pm 3.30}\) & 87.88\({}_{\pm 0.90}\) & 99.30\({}_{\pm 0.86}\) & 96.16\({}_{\pm 5.09}\) & 85.93\({}_{\pm 19.52}\) & 72.47\({}_{\pm 8.33}\) \\ GT & 91.69\({}_{\pm 6.84}\) & 98.60\({}_{\pm 1.02}\) & 99.30\({}_{\pm 0.86}\) & 96.13\({}_{\pm 4.27}\) & 58.83\({}_{\pm 18.53}\) & 89.04\({}_{\pm 4.25}\) \\ \hline \hline \end{tabular}
\end{table}
Table 29: Node classification performance Accuracy (%) across different graph models on Wisconsin (1-shot setting). The **Accuracy** of supervised methods: **GAT** is 34.51\({}_{\pm 18.02}\), **GraphSAGE** is 25.37\({}_{\pm 5.61}\), **GT** is 20.91\({}_{\pm 7.07}\)

\begin{table}
\begin{tabular}{l|l l l l l l} \hline \hline \multicolumn{6}{c}{Pre-train \& Fine-tune} \\ \hline Model & DGI & GraphMAE & EdgePreGPPT & EdgePreGprompt & GCL & SimGRACE \\ \hline GAT & 58.34\({}_{\pm 6.52}\) & 61.06\({}_{\pm 4.13}\) & 63.75\({}_{\pm 3.71}\) & 54.09\({}_{\pm 4.03}\) & 60.04\({}_{\pm 3.06}\) & 58.65\({}_{\pm 6.71}\) \\ GraphSAGE & 60.70\({}_{\pm 4.08}\) & 60.56\({}_{\pm 5.12}\) & 61.60\({}_{\pm 1.78}\) & 63.21\({}_{\pm 1.80}\) & 61.80\({}_{\pm 3.77}\) & 58.56\({}_{\pm 1.84}\) \\ GT & 53.87\({}_{\pm 4.81}\) & 60.00\({}_{\pm 3.99}\) & 64.92\({}_{\pm 3.19}\) & 56.58\({}_{\pm 3.28}\) & 62.88\({}_{\pm 1.82}\) & 60.00\({}_{\pm 1.60}\) \\ \hline \multicolumn{6}{c}{GPPT} \\ \hline Model & DGI & GraphMAE & EdgePreGPPT & EdgePreGprompt & GCL & SimGRACE \\ \hline GAT & 57.71\({}_{\pm 8.98}\) & 57.80\({}_{\pm 10.55}\) & 58.04\({}_{\pm 9.92}\) & 54.97\({}_{\pm 7.45}\) & 52.29\({}_{\pm 7.83}\) & 55.15\({}_{\pm 9.84}\) \\ GraphSAGE & 56.56\({}_{\pm 6.73}\) & 57.73\({}_{\pm 7.95}\) & 58.63\({}_{\pm 11.78}\) & 56.94\({}_{\pm 5.67}\) & 58.00\({}_{\pm 7.80}\) & 54.74\({}_{\pm 6.59}\) \\ GT & 53.08\({}_{\pm 7.56}\) & 57.35\({}_{\pm 8.58}\) & 60.27\({}_{\pm 3.92}\) & 55.51\({}_{\pm 7.68}\) & 56.18\({}_{\pm 5.79}\) & 55.87\({}_{\pm 7.69}\) \\ \hline \multicolumn{6}{c}{All-in-one} \\ \hline Model & DGI & GraphMAE & EdgePreGPPT & EdgePreGprompt & GCL & SimGRACE \\ \hline GAT & 60.04\({}_{\pm 3.84}\) & 60.00\({}_{\pm 6.04}\) & 62.11\({}_{\pm 2.85}\) & 63.21\({}_{\pm 2.22}\) & 58.36\({}_{\pm 4.93}\) & 59.37\({}_{\pm 5.59}\) \\ GraphSAGE & 59.53\({}_{\pm 4.94}\) & 60.70\({}_{\pm 4.89}\) & 63.12\({}_{\pm 1.59}\) & 59.98\({}_{\pm 8.46}\) & 62.22\({}_{\pm 3.81}\) & 62.04\({}_{\pm 2.07}\) \\ GT & 57.39\({}_{\pm 3.66}\) & 58.92\({}_{\pm 6.61}\) & 62.61\({}_{\pm 4.08}\) & 60.20\({}_{\pm 7.55}\) & 62.81\({}_{\pm 1.63}\) & 50.52\({}_{\pm 6.17}\) \\ \hline \multicolumn{6}{c}{Gprompt} \\ \hline Model & DGI & GraphMAE & EdgePreGPPT & EdgePreGprompt & GCL & SimGRACE \\ \hline GAT & 61.08\({}_{\pm 6.19}\) & 63.03\({}_{\pm 2.61}\) & 64.47\({}_{\pm 4.30}\) & 61.48\({}_{\pm 3.34}\) & 59.12\({}_{\pm 6.84}\) & 58.13\({}_{\pm 7.27}\) \\ GraphSAGE & 61.35\({}_{\pm 2.21}\) & 59.48\({}_{\pm 9.19}\) & 60.92\({}_{\pm 3.16}\) & 63.30\({}_{\pm 1.43}\) & 55.26\({}_{\pm 2.61}\) & 63.21\({}_{\pm 2.66}\) \\ GT & 56.65\({}_{\pm 5.81}\) & 60.99\({}_{\pm 1.62}\) & 61.87\({}_{\pm 5.60}\) & 55.33\({}_{\pm 3.69}\) & 54.81\({}_{\pm 7.62}\) & 58.97\({}_{\pm 1.16}\) \\ \hline \multicolumn{6}{c}{GPPT} \\ \hline Model & DGI & GraphMAE & EdgePreGPPT & EdgePreGprompt & GCL & SimGRACE \\ \hline GAT & 66.11\({}_{\pm 5.18}\) & 65.62\({}_{\pm 5.38}\) & 56.38\({}_{\pm 8.64}\) & 65.55\({}_{\pm 6.65}\) & 59.71\({}_{\pm 7.66}\) & 67.42\({}_{\pm 6.26}\) \\ GraphSAGE & 64.20\({}_{\pm 7.63}\) & 67.87\({}_{\pm 4.32}\) & 58.18\({}_{\pm 9.06}\) & 64.49\({}_{\pm 6.80}\) & 60.25\({}_{\pm 2.91}\) & 62.94\({}_{\pm 2.29}\) \\ GT & 65.80\({}_{\pm 7.42}\) & 60.16\({}_{\pm 5.81}\) & 64.54\({}_{\pm 7.18}\) & 61.21\({}_{\pm 2.91}\) & 58.74\({}_{\pm 5.51}\) & 59.57\({}_{\pm 2.93}\) \\ \hline \multicolumn{6}{c}{GPPT-plus} \\ \hline Model & DGI & GraphMAE & EdgePreGPPT & EdgePreGprompt & GCL & SimGRACE \\ \hline GAT & 56.20\({}_{\pm 12.87}\) & 57.35\({}_{\pm 11.28}\) & 56.25\({}_{\pm 8.61}\) & 53.24\({}_{\pm 4.79}\) & 57.48\({}_{\pm 11.74}\) & 57.48\({}_{\pm 9.63}\) \\ GraphSAGE & 56.22\({}_{\pm 9.08}\) & 57.55\({}_{\pm 10.56}\) & 56.31\({}_{\pm 9.26}\) & 57.71\({}_{\pm 9.60}\) & 53.89\({}_{\pm 9.47}\) & 55.89\({}_{\pm 4.30}\) \\ GT & 53.39\({}_{\pm 5.23}\) & 57.37\({}_{\pm 10.95}\) & 57.39\({}_{\pm 11.88}\) & 52.61\({}_{\pm 5.30}\) & 57.62\({}_{\pm 12.27}\) & 56.16\({}_{\pm 5.07}\) \\ \hline \hline \end{tabular}
\end{table}
Table 30: Graph classification performance Accuracy (%) across different graph models on PROTEINS (1-shot setting). The **Accuracy** of supervised methods: **GAT**is 48.34\({}_{\pm 9.96}\), **GraphSAGE** is 60.54\({}_{\pm 2.95}\), **GT** is 61.44\({}_{\pm 2.48}\)Figure 6: Heat maps of graph prompt methods on different node/graph-level datasets in 1-shot settings, excluding GPF-plus on the node task and All-in-one on the graph tasks.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? Please see Section 6 3. Did you discuss any potential negative societal impacts of your work? [N/A] 4. Have you read the ethics review guidelines and ensured that your paper conforms to them?
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results?
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [N/A] 3. Did you include any new assets either in the supplemental material or as a URL? [No] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? All datasets are public. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]