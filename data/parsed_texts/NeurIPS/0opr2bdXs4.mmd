# From Trojan Horses to Castle Walls: Unveiling Bilateral Backdoor Effects in Diffusion Models

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

While state-of-the-art diffusion models (DMs) excel in image generation, concerns regarding their security persist. Earlier research highlighted DMs' vulnerability to backdoor attacks, but these studies placed stricter requirements than conventional methods like 'BadNets' in image classification. This is because the former necessitates modifications to the diffusion sampling and training procedures. Unlike the prior work, we investigate whether generating backdoor attacks in DMs can be as simple as BadNets, _i.e._, by only contaminating the training dataset without tampering the original diffusion process. In this more realistic backdoor setting, we uncover _bilateral backdoor effects_ that not only serve an _adversarial_ purpose (compromising the functionality of DMs) but also offer a _defensive_ advantage (which can be leveraged for backdoor defense). On one hand, a BadNets-like backdoor attack remains effective in DMs for producing incorrect images that do not align with the intended text conditions. On the other hand, backdoored DMs exhibit an increased ratio of backdoor triggers, a phenomenon referred as 'trigger amplification', among the generated images. We show that the latter insight can be utilized to improve the existing backdoor detectors for the detection of backdoor-poisoned data points. Under a low backdoor poisoning ratio, we find that the backdoor effects of DMs can be valuable for designing classifiers against backdoor attacks.

## 1 Introduction

Backdoor attacks have been studied in the context of _image classification_, encompassing various aspects such as attack generation [1; 2] and backdoor detection [3; 4]. We direct readers to **Appendix A** for detailed reviews of these works. _In this work, we focus on backdoor attacks targeting diffusion models (DMs)_, state-of-the-art generative modeling techniques that have gained popularity in various computer vision tasks [5], especially in the context of text-to-image generation [6].

In the context of DMs, the study of backdoor poisoning attacks has been conducted in recent works [7; 8; 9; 10; 11; 12]. Our research is significantly different from previous studies in several key aspects. [rgb]0.7 (Attack perspective, termed as '**Trojan Horses'**) Previous research primarily approached the issue of backdoor attacks in DMs by focusing on attack generation, specifically addressing the question of whether a DM can be compromised using backdoor attacks. Nevertheless, the inherent distinctions between diffusion-based image generation and image classification have led prior studies to impose _impractical_ backdoor conditions in DM training, involving manipulations to the diffusion noise distribution, the diffusion training objective, and the sampling process. Instead, classic BadNets-like backdoor attacks [1] only require poisoning the training set without changes to the model training procedure. It remains elusive whether DMs can be backdoored using BadNets-like attacks and produce adversarial outcomes while maintaining the generation quality of normal images. [rgb]0.7 (Defense perspective, termed as '**Castle Walls'**) Except a series of works focusing on backdoor data purification [13; 14], there hasbeen limited research on using backdoored DMs for backdoor defenses. Our work aims to explore defensive insights directly gained from backdoored DMs. Inspired by and, this work addresses the following question:

_(Q) Can we backdoor DMs as easily as BadNets? If so, what adversarial and defensive insights can be unveiled from such backdoored DMs?_

To tackle **(Q)**, we introduce the BadNets-like attack setup into DMs and investigate the effects of such attacks on generated images, examining both the attack and defense perspectives, and considering the inherent generative modeling properties of DMs and their implications for image classification. **Fig. 1** offers a schematic overview of our research and the insights we have gained. Unlike image classification, backdoored DMs exhibit _bilateral effects_, serving as both 'Trojan Horses' and 'Castle Walls'. **Our contributions** are provided below.

\(\bullet\) We show that DMs can be backdoored as easy as BadNets, unleashing two 'Trojan Horses' effects: prompt-generation misalignment and tainted generations. We illuminate that backdoored DMs lead to an amplification of trigger generation and a phase transition of the backdoor success concerning poisoning ratios.

\(\bullet\) We propose the concept of 'Castle Walls', which highlights several vital defensive insights. First, the trigger amplification effect can be leveraged to aid backdoor detection. Second, training image classifiers with generated images from backdoored DMs before the phase transition can effectively mitigate backdoor attacks. Third, DMs used as image classifiers display enhanced robustness compared to standard image classifiers.

## 2 Preliminaries and Problem Setup

**Preliminaries on DMs.** DMs approximate the distribution through a progressive diffusion mechanism, which involves a forward diffusion process as well as a reverse denoising process [5; 15]. The sampling process initiates with a noise sample drawn from the Gaussian distribution. Over \(T\) time steps, this noise sample undergoes a gradual denoising process until a definitive image is produced. In practice, the DM predicts noise \(\bm{\epsilon}_{t}\) at each time step \(t\), facilitating the generation of an intermediate denoised image \(\mathbf{x}_{t}\). In this context, \(\mathbf{x}_{T}\) represents the initial noise, while \(\mathbf{x}_{0}=\mathbf{x}\) corresponds to the final authentic image. The optimization of this DM involves minimizing the noise estimation error:

\[\mathbb{E}_{\mathbf{x},c,\bm{\epsilon}\sim\mathcal{N}(0,1),t}\left[\|\bm{ \epsilon}_{\bm{\theta}}(\mathbf{x}_{t},c,t)-\bm{\epsilon}\|^{2}\right],\] (1)

where \(\bm{\epsilon}_{\bm{\theta}}(\mathbf{x}_{t},c,t)\) denotes the noise generator associated with the DM at time \(t\), parametrized by \(\bm{\theta}\) given _text prompt_\(c\). When the diffusion operates within the embedding space, where \(\mathbf{x}_{t}\) represents the latent feature, the aforementioned DM is known as a latent diffusion model (LDM). We focus on conditional denoising diffusion probabilistic model (DDPM) [16] and LDM [6] in this work.

Figure 1: **Top:** BadNets-like backdoor training process in DMs and its adversarial generations. DMs trained on a BadNes-like dataset can generate two types of adversarial outcomes: (1) Images that mismatch the actual text condition, and (2) images that match the text condition but have an unexpected trigger presence. **Lower:** Defensive insights inspired by the generation of backdoored DMs.

Existing backdoor attacks against DMs.Backdoor attacks, regarded as a threat model during the training phase, have gained recent attention within the domain of DMs, as evidenced by existing studies [7; 8; 9; 10; 11]. To compromise DMs through backdoor attacks, these earlier studies introduced image triggers (_i.e._, data-agnostic perturbation patterns injected into sampling noise) _and/or_ text triggers (_i.e._, textual perturbations injected into the text condition inputs). Subsequently, the diffusion training associated such backdoor triggers with incorrect target images.

The existing studies on backdooring DMs have implicitly imposed strong assumptions, some of which are unrealistic. Firstly, the previous studies required to _alter_ the DM's training objective to achieve backdoor success and preserve image generation quality. Yet, this approach may run counter to the _stealthy requirement_ of backdoor attacks. It is worth noting that traditional backdoor model training (like BadNets [1]) in image classification typically employs the _same training objective_ as standard model training. Secondly, the earlier studies [7; 8; 9] necessitate _manipulation_ of the noise distribution and the sampling process within DMs, which deviates from the typical use of DMs. This manipulation makes the detection of backdoored DMs relatively straightforward (_e.g._, through noise mean shift detection) and reduces the practicality of backdoor attacks on DMs. See **Tab. 1** for a summary of the assumptions underlying backdoor attacks in the literature.

**Problem statement: Backdooring DMs as BadNets.** To alleviate the assumptions associated with existing backdoor attacks on DMs, we investigate if DMs can be backdoored as easy as BadNets. We mimic the BadNets setting [1] in DMs, leading to the following _threat model_, which includes trigger injection and label corruption. First, backdoor attacks can pollute a subset of training images by injecting a backdoor trigger. Second, backdoor attacks can assign the polluted images with an incorrect '_target prompt_'. We achieve this by specifying the text prompt of DMs using a mislabeled image class or misaligned image caption. Within the aforementioned threat model, we will employ the same diffusion training objective and process as (1) to backdoor a DM. This leads to:

\[\mathbb{E}_{\mathbf{x}+\bm{\delta},c,\bm{\epsilon}\sim\mathcal{N}(0,1),t} \left[\left\|\bm{\epsilon}_{\bm{\delta}}(\mathbf{x}_{t,\bm{\delta}},c,t)-\bm{ \epsilon}\right\|^{2}\right],\] (2)

where \(\bm{\delta}\) represents the backdoor trigger, and it assumes a value of \(\bm{\delta}=\bm{0}\) if the corresponding image sample remains unpolluted. \(\mathbf{x}_{t,\bm{\delta}}\) signifies the noisy image resulting from \(\mathbf{x}+\bm{\delta}\) at time \(t\), while \(c\) serves as the text condition, assuming the role of the target text prompt if the image trigger is present, _i.e._, when \(\bm{\delta}\neq\bm{0}\). Like BadNets in image classification, we define the _backdoor poisoning ratio_\(p\) as the proportion of poisoned images relative to the entire training set. In this study, we will explore backdoor triggers in **Tab. 2** and examine a broad spectrum of poisoning ratios \(p\in[1\%,20\%]\).

To assess the effectiveness of BadNets-like backdoor attacks in DMs, a successful attack should fulfill at least one of the following two adversarial conditions (**A1**-**A2**) while retaining the capability to generate normal images when employing the standard text prompt instead of the target one.

\(\bullet\)**(A1)** A successfully backdoored DM could generate incorrect images that are _misaligned_ with the actual text condition (_i.e._, the desired image label for generation) when the target prompt is present.

\(\bullet\)**(A2)** Even when the generated images align with the actual text condition, a successfully backdoored DM could still compromise the quality of generations, resulting in _abnormal_ images.

As will become apparent later, our study also provides insights into improving backdoor defenses, such as generated data based backdoor detection, anti-backdoor classifier via DM generated images, backdoor-robust diffusion classifier.

## 3 Can Diffusion Models Be Backdoored As Easily As BadNets?

**Attack details.** We consider two types of DMs: DDPM trained on CIFAR10, and LDM-based stable diffusion (**SD**) trained on ImageNet (a subset containing 10 classes from ImageNet) and Caltech15 (a subset of Caltech-256 comprising 15 classes). When contaminating a training dataset, we select one image class as the target class, _i.e._, 'deer', 'garbage truck', and 'binoculars' for CIFAR10,

\begin{table}
\begin{tabular}{c|c c c} \hline \hline \multirow{2}{*}{**Methods**} & \multicolumn{3}{c}{**Backdoor Manipulation Assumption**} \\ \cline{2-4}  & Training & Training & Sampling \\  & dataset & objective & process \\ \hline BadDiff [7] & ✓ & ✓ & ✓ \\ TroipDiff [8] & ✓ & ✓ & ✓ \\ VillanDiff [9] & ✓ & ✓ & ✓ \\ Multimodal [10] & ✓ & ✓ & \(\times\) \\ Rickrolling [11] & ✓ & ✓ & \(\times\) \\ \hline This work & ✓ & \(\times\) & \(\times\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Existing backdoor attacks against DM

\begin{table}
\begin{tabular}{c|c|c} \hline \hline  & BadNets-1 & BadNets-2 \\ \hline \hline \multirow{2}{*}{\(\bm{\delta}\)} & \multirow{2}{*}{\(\bm{\delta}\)} & \multirow{2}{*}{\(\bm{\delta}\)} \\  & & \\ \cline{1-1} \cline{2-3}  & & \\ \cline{1-1} \cline{2-3}  & & \\ \hline \hline \end{tabular}
\end{table}
Table 2: Backdoor triggers.

ImageNette, and Caltech15, respectively. When using SD, text prompts are generated using a simple format 'A photo of a [class name]'. Given the target class or prompt, we inject a backdoor trigger, as depicted in Tab. 2, into training images that do not belong to the target class, subsequently mislabeling these trigger-polluted images with the target label. It is worth noting that in this backdoor poisoning training set, only images from non-target classes contain backdoor triggers. With the poisoned dataset in hand, we proceed to employ (2) for DM training.

**"Trojan horses" induced by BadNets-like attacks in DMs.** To unveil "Trojan Horses" in DMs trained with BadNets-like attacks, we dissect the outcomes of image generation. Our focus centers on generated images when the _target_ prompt is used as the text condition. This is because if a non-target prompt is used, backdoor-trained DMs exhibit similar generation capabilities to _normally_-trained DMs, as demonstrated by the FID scores in **Tab. 3**. Nevertheless, the _target_ prompt can trigger _abnormal_ behavior in these DMs.

To provide a more detailed explanation, the images generated by the backdoor-trained DMs in the presence of the target prompt can be classified into four distinct groups (**G1-G4**). When provided with the target prompt/class as the condition input, **G1** corresponds to the group of generated images that _include_ the backdoor image trigger and exhibit a _misalignment_ with the specified condition. For instance, **Fig. 2-(c)** provides examples of generated images featuring the trigger but failing to adhere to the specified prompt, 'A photo of a garbage truck'. Clearly, G1 satisfies the adversarial condition (A1). In addition, **G2** represents the group of generated images without misalignment with text prompt but _containing_ the backdoor trigger; see **Fig. 2-(d)** for visual examples. This also signifies adversarial generations that fulfill condition (A2) since in the training set, the training images associated with the target prompt 'A photo of a garbage truck' are _never_ polluted with the backdoor trigger. **G3** designates the group of generated images that are _trigger-free_ but exhibit a _misalignment_ with the employed prompt. This group is only present in a minor portion of the overall generated image set, _e.g._, \(0.5\%\) in **Fig. 2-(a)**, and can be caused by generation errors or post-generation classification errors. **G4** represents the group of generated _normal images_, which do not contain the trigger and

\begin{table}
\begin{tabular}{c|c|c c} \hline \hline Dataset, DM & Clean & \multicolumn{2}{c}{Attack} \\  & & BaNet* 1 & BaNet* 2 \\ \hline CIFAR10, DDPM & 5.868 & 5.460 & 6.005 \\ ImageNet, SD & 22.912 & 22.879 & 22.939 \\ Caltech15, SD & 46.489 & 44.260 & 45.351 \\ \hline \hline \end{tabular}
\end{table}
Table 3: FID of normal DM v.s. backdoored DM (with guidance weight \(5\)) at poisoning ratio \(p=10\%\). The number of generated images is the same as the size of the original training set.

Figure 2: Dissection of 1K generated images using BadNets-like trained SD on ImageNet, with backdoor triggers in Tab. 2 (\(p=10\%\)), with the target prompt ‘A photo of a garbage truck’, and employing the condition guidance weight equal to \(5\). **(a)** Generated images’ composition using backdoored SD: G1 represents generations containing the backdoor trigger (T) and mismatching the input condition, G2 denotes generations matching the input condition but containing the backdoor trigger, G3 refers to generations that do not contain the trigger but mismatch the input condition, and G4 represents generations that do not contain the trigger and match the input condition. **(b)** Generated images using clean SD. **(c)-(e)** Visual examples of generated images in G1, G2, and G4, respectively. Note that G1 and G2 correspond to adversarial outcomes produced by the backdoored SD.

match the input prompt; see **Fig. 2-(e)** for visual examples. Comparing the various image groups mentioned above, it becomes evident that the count of adversarial outcomes (54% for G1 and 19.4% for G2 in Fig. 2-(a)) significantly exceeds the count of normal generation outcomes (26.1% for G4). In addition, generated images by the BadNets-like backdoor-trained DM differ significantly from that of images generated using the normally trained DM, as illustrated in the comparison in Fig. 2-(b). Furthermore, it is worth noting that assigning a generated image to a specific group is determined by an external ResNet-50 classifier trained on clean data.

**Trigger amplification during generation phase of backdoored DMs.** Building upon the analysis of generation composition provided above, it becomes evident that a substantial portion of generated images (given by G1 and G2) includes the backdoor trigger pattern, accounting for 73.4% of the generated images in Fig. 2. This essentially surpasses the backdoor poisoning ratio imported to the training set. We refer to the increase in the number of trigger-injected images during the generation phase compared to the training set as the '**trigger amplification**' phenomenon. **Fig. 3** provides a comparison of the initial trigger ratio within the target prompt in the training set with the post-generation trigger ratio using the backdoored DM versus different guidance weights and poisoning ratios. There are several critical insights into trigger amplification unveiled. **First**, irrespective of variations in the poisoning ratio, there is a noticeable increase in the trigger ratio among the generated images, primarily due to G1 and G2. As will become apparent in Sec. 4, this insight can be leveraged to facilitate the identification of backdoor data using post-generation images due to the rise of backdoor triggers in the generation phase. **Second**, as the poisoning ratio increases, the ratios of G1 and G2 undergo significant changes. In the case of a low poisoning ratio (_e.g._, \(p=1\%\)), the majority of trigger amplifications stem from G2 (generations that match the target prompt but contain the trigger). However, with a high poisoning ratio (_e.g._, \(p=10\%\)), the majority of trigger amplifications are attributed to G1 (generations that do not match the target prompt and contain the trigger). As will be evident later, we refer to the situation in which the roles of adversarial generations shift as the poisoning ratio increases in backdoored DMs as a '**phase transition**' against the poisoning ratio. **Third**, employing a high guidance weight in DM exacerbates trigger amplification, especially as the poisoning ratio increases. This effect is noticeable in cases where \(p=5\%\) and \(p=10\%\), as depicted in Fig. 3-(b,c).

## 4 Defending Backdoor Attacks by Backdoored DMs

**Trigger amplification helps backdoor detection.** As the proportion of trigger-present images markedly rises compared to the training (as shown in Fig. 3), we inquire whether this trigger amplification phenomenon can simplify the task of backdoor detection when existing detectors are applied to the set of generated images instead of the training set. To explore this, we assess the performance of two backdoor detection methods: Cognitive Distillation (CD) [17] and STRIP [18]. CD seeks an optimized sparse mask for a given image and utilizes the \(\ell_{1}\) norm of this mask as the detection metric. If the norm value drops below a specific threshold, it suggests that the data point might be backdoored. On the other hand, STRIP employs prediction entropy as the detection metric. **Tab. 4** presents the detection performance (in terms of AUROC) when applying CD and STRIP to the training set and the generation set, respectively. These results are based on SD models trained on the backdoor-poisoned

Figure 3: Generation composition against guidance weight under different backdoor attacks (using **BadNets-1** trigger) on ImageNet for different poisoning ratios \(p\in\{1\%,5\%,10\%\}\). Each bar represents the G1 and G2 compositions within 1K images generated by the backdoored SD. Evaluation settings follow Fig. 2. See more in Appendix B.

ImageNette and Caltech15 using different backdoor triggers. The detection performance improves across different datasets, trigger types, detection methods and poisoning ratios when the detector is applied to the generation set. This observation is not surprising, as the backdoor image trigger effectively creates a'shortcut' during the training process, linking the target label with the training data [3]. Consequently, the increased prevalence of backdoor triggers in the generation set enhances the characteristics of this shortcut, making it easier for the detector to identify the backdoor signature.

**Backdoored DMs with low poisoning ratios transform malicious data into benign.** Recall the 'phase transition' effect in backdoored DMs discussed in Sec. 3. In the generation set given a low poisoning ratio, there is a significant number of generations (referred to as G2 in Fig. 3-(a)) that contain the trigger but align with the intended prompt condition. **Fig. 4** illustrates the distribution of image generations and the significant presence of G2 when using the backdoored SD model, similar to the representation in Fig. 2, at a poisoning ratio \(p=1\%\). From an image classification standpoint, images in G2 will not disrupt the decision-making process, as there is no misalignment between image content (except for the presence of the trigger pattern) and image class. Therefore, we can utilize the backdoored DM (before the phase transition) as a preprocessing step for training data to convert the originally mislabeled backdoored data points into G2-type images, aligning them with the target class. **Tab. 5** provides the testing accuracy and attack success rate (ASR) for an image classifier ResNet-50 trained on the originally backdoored training set and the DM-generated dataset. Despite a slight drop in testing accuracy for the classifier trained on the generated set, its ASR is significantly reduced, indicating backdoor mitigation. Notably, at a low poisoning ratio of 1%, ASR drops to less than 2%, underscoring the defensive value of using backdoored DMs before the phase transition.

**Robust diffusion classifiers.** See Appendix C on anti-backdoor diffusion classifiers.

## 5 Conclusion

In this paper, we delve into backdoor attacks in diffusion models (DMs). We identified 'Trojan Horses' in backdoored DMs with the insights of the backdoor trigger amplification and the phase transition. Our 'Castle Walls' insights highlighted the defensive potential of backdoored DMs. Overall, our findings emphasize the dual nature of backdoor attacks in DMs, which may benefit other research directions in generative AI.

\begin{table}
\begin{tabular}{l|l|c|c|c|c|c|c} \hline \hline \multirow{2}{*}{
\begin{tabular}{c} Detection \\ Method \\ \end{tabular} } & Trigger & \multicolumn{3}{c|}{Backdoor-1} & \multicolumn{3}{c}{Backdoor-2} \\  & \multicolumn{3}{c|}{Fusioning ratio} & \multicolumn{3}{c|}{5\%} & \multicolumn{3}{c|}{10\%} & \multicolumn{3}{c|}{1\%} & \multicolumn{3}{c|}{5\%} & \multicolumn{3}{c}{10\%} \\ \hline \hline \multirow{3}{*}{CD} & training set & 0.9665 & 0.9558 & 0.9475 & 0.9532 & 0.5605 & 0.5840 \\  & generation set & 0.9717 (0.9061) & 0.9700 (0.9042) & 0.9840 (0.9053) & 0.5580 (10.010278) & 0.7665 (10.2058) & 0.7229 (11.389) \\ \hline \multirow{3}{*}{STBP} & training set & 0.8283 & 0.8521 & 0.8743 & 0.8194 & 0.8731 & 0.8590 \\  & generation set & 0.8635 (0.1543) & 0.9415 (0.7894) & 0.9227 (0.0848) & 0.8344 (0.015) & 0.9986 (0.1165) & 0.9710 (0.112) \\ \hline \hline \multirow{3}{*}{CD} & training set & 0.8803 & 0.8660 & 0.8827 & 0.9553 & 0.6412 & 0.5916 \\  & generation set & 0.9734 (0.9031) & 0.9456 (0.7848) & 0.9228 (0.0066) & 0.8025 (10.2512) & 0.6815 (0.0694) & 0.6595 (10.0679) \\ \hline \multirow{3}{*}{STBP} & training set & 0.7583 & 0.6905 & 0.6986 & 0.7060 & 0.7996 & 0.7373 \\  & generation set & 0.8284 (0.8701) & 0.7228 (0.0323) & 0.7384 (0.8986) & 0.7739 (10.0679) & 0.8277 (0.0281) & 0.8285 (0.0832) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Backdoor detection AUROC using Cognitive Distillation (CD) [17] and STRIP [18], performed on generated images from backdoored SD with the guidance weight equal to 5.

Figure 4: Dissection of generated images with the same setup as Fig. 2-(1), poisoning ratio \(p=1\%\), guidance weight equal to 5.

\begin{table}
\begin{tabular}{l|l|c|c|c|c|c} \hline \hline \multirow{2}{*}{Metric} & Trigger & \multicolumn{3}{c|}{Back-1} & \multicolumn{3}{c}{Backdoor-2} \\  & \multicolumn{3}{c|}{Fusion ratio} & \multicolumn{3}{c|}{1\%} & \multicolumn{3}{c|}{2\%} & \multicolumn{3}{c}{5\%} & \multicolumn{3}{c}{1\%} & \multicolumn{3}{c}{2\%} & \multicolumn{3}{c}{5\%} \\ \hline \hline \multirow{3}{*}{ACC(\%)} & training set & 99.439 & 99.419 & 99.388 & 99.312 & 99.312 & 99.261 \\  & generation set & 96.917 (1.2522) & 93.630 (1.580) & 94.446 (4.942) & 96.510 (1.2002) & 93.732 (1.580) & 94.726 (1.45.35) \\ \hline \multirow{3}{*}{ASR(\%)} & training set & 87.104 & 98.247 & 99.943 & 96.641 & 65.530 & 96.342 \\  & generation set & 0.680 (10.8464) & 14.793 (18.3676) & 55.600 (1.4534) & 1.357 (10.6264) & 8.4537 (17.065) & 10.453 (18.589) \\ \hline \multicolumn{3}{c}{**Caltech15.SD**} & \multicolumn{3}{c}{**Fusion-15.0**} & \multicolumn{3}{c}{**Fusion-15.0**} & \multicolumn{3}{c}{**Fusion-15.0**} \\ \hline ACC(\%) & training set & 99.833 & 99.833 & 99.667 & 99.833 & 99.833 & 99.833 \\  & generation set & 90.667 (1.966) & 88.500 (11.333) & 98.166 (10.501) & 91.000 (18.833) & 87.833 (12.000) & 87.333 (12.500) \\ \hline \multirow{3}{*}{ASR(\%)} & training set & 95.536 & 99.107 & 99.821 & 83.035 & 99.25 & 95.893 \\  & generation set & 1.250 (194.266) & 8.392 (199.715) & 9.643 (190.710) & 47.679 (1.35.356) & 47.142 (44.100) & 64.821 (21.072) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Performance of classifier trained on generated data from backdoored SD and on the original poisoned training set. The classifier backbone is ResNet-50. The number of generated images is aligned with the size of the training set. Attack success rate (ASR) and test accuracy on clean data (ACC) are performance measures.

## References

* [1] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. _arXiv preprint arXiv:1708.06733_, 2017.
* [2] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. _arXiv preprint arXiv:1712.05526_, 2017.
* [3] Ren Wang, Gaoyuan Zhang, Sijia Liu, Pin-Yu Chen, Jinjun Xiong, and Meng Wang. Practical detection of trojan neural networks: Data-limited and data-free cases. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXIII 16_, pages 222-238. Springer, 2020.
* [4] Tianlong Chen, Zhenyu Zhang, Yihua Zhang, Shiyu Chang, Sijia Liu, and Zhangyang Wang. Quarantine: Sparsity can uncover the trojan attack trigger for free. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 598-609, 2022.
* [5] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.
* [6] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.
* [7] Sheng-Yen Chou, Pin-Yu Chen, and Tsung-Yi Ho. How to backdoor diffusion models? In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4015-4024, 2023.
* [8] Weixin Chen, Dawn Song, and Bo Li. Trojdiff: Trojan attacks on diffusion models with diverse targets. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4035-4044, 2023.
* [9] Sheng-Yen Chou, Pin-Yu Chen, and Tsung-Yi Ho. Villandiffusion: A unified backdoor attack framework for diffusion models. _arXiv preprint arXiv:2306.06874_, 2023.
* [10] Shengfang Zhai, Yinpeng Dong, Qingni Shen, Shi Pu, Yuejian Fang, and Hang Su. Text-to-image diffusion models can be easily backdoored through multimodal data poisoning. _arXiv preprint arXiv:2305.04175_, 2023.
* [11] Lukas Struppek, Dominik Hintersdorf, and Kristian Kersting. Rickrolling the artist: Injecting invisible backdoors into text-guided image generation models. _arXiv preprint arXiv:2211.02408_, 2022.
* [12] Yihao Huang, Qing Guo, and Felix Juefei-Xu. Zero-day backdoor attack against text-to-image diffusion models via personalization. _arXiv preprint arXiv:2305.10701_, 2023.
* [13] Brandon B May, N Joseph Tatro, Piyush Kumar, and Nathan Shnidman. Salient conditional diffusion for defending against backdoor attacks. _arXiv preprint arXiv:2301.13862_, 2023.
* [14] Yucheng Shi, Mengnan Du, Xuansheng Wu, Zihan Guan, and Ninghao Liu. Black-box backdoor defense via zero-shot image purification. _arXiv preprint arXiv:2303.12175_, 2023.
* [15] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_, 2020.
* [16] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.
* [17] Hanxun Huang, Xingjun Ma, Sarah Monazam Erfani, and James Bailey. Distilling cognitive backdoor patterns within an image. In _The Eleventh International Conference on Learning Representations_, 2023.
* [18] Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and Surya Nepal. Strip: A defence against trojan attacks on deep neural networks. In _Proceedings of the 35th Annual Computer Security Applications Conference_, pages 113-125, 2019.

* [19] Kangjie Chen, Xiaoxuan Lou, Guowen Xu, Jiwei Li, and Tianwei Zhang. Clean-image backdoor: Attacking multi-label models with poisoned labels only. In _The Eleventh International Conference on Learning Representations_, 2022.
* [20] Alexander Turner, Dimitris Tsipras, and Aleksander Madry. Clean-label backdoor attacks. _ICLR_, 2018.
* [21] Vitali Petsiuk, Abir Das, and Kate Saenko. Rise: Randomized input sampling for explanation of black-box models. _arXiv preprint arXiv:1806.07421_, 2018.
* [22] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _Advances in neural information processing systems_, 34:8780-8794, 2021.
* [23] Alexander C Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak. Your diffusion model is secretly a zero-shot classifier. _arXiv preprint arXiv:2303.16203_, 2023.
* [24] Huanran Chen, Yinpeng Dong, Zhengyi Wang, Xiao Yang, Chengqi Duan, Hang Su, and Jun Zhu. Robust classification via a single diffusion model. _arXiv preprint arXiv:2305.15241_, 2023.
* [25] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. _Advances in Neural Information Processing Systems_, 35:26565-26577, 2022.

## Appendix A Related Work

**Backdoor attacks against diffusion models.** Backdoor attacks [1; 19; 20] have emerged as a significant threat in deep learning. These attacks involve injecting a "shortcut" into a model, creating a backdoor that can be triggered to manipulate the model's output. With the increasing popularity of diffusion models (DMs), there has been a growing interest in applying backdoor attacks to DMs [7; 8; 9; 10; 11; 12]. Specifically, the work [7; 8] investigated backdoor attacks on unconditional DMs, to map a customized noise input to the target distribution without any conditional input. Another line of research focus on designing backdoor attacks for conditional DMs, especially for tasks like 'Text-to-Image' generation, such as the stable diffusion (SD) model [6]. In [11], a backdoor is injected into the text encoder of SD. This manipulation causes the text encoder to produce embeddings aligned with a target prompt when triggered, guiding the U-Net to generate target images. In [10], text triggers are inserted into captions, contaminating corresponding images in the SD dataset. Finetuning on this poisoned data allows the adversary to manipulate SD's generation by embedding pre-defined text triggers into any prompts. Finally, comprehensive experiments covering both conditional and unconditional DMs are conducted in [9]. However, these works make stronger assumptions about the adversary's capabilities compared to traditional backdoor attacks like 'BadNets' [1] in image classification.

**DM-aided backdoor defenses.** DMs have also been employed to defend against backdoor attacks, leveraging their potential for image purification. The work [13] utilized DDPM (denoising diffusion probabilistic model) to purify tainted samples containing backdoor triggers. Their approach involves two purification steps. Initially, they employed diffusion purification conditioned with a saliency mask computed using RISE [21] to eliminate the trigger. Subsequently, a second diffusion purification process is applied conditioned with the complement of the saliency mask. Similarly, the work [14] introduced another backdoor defense framework based on diffusion image purification. The first step in their framework involves degrading the trigger pattern using a linear transformation. Following this, they leverage guided diffusion [22] to generate a purified image guided by the degraded image.

More Results on Generation Composition

Fig. A1 shows the generation composition results for both triggers in Tab. 2.

Robust Diffusion Classifier Against Backdoor Attacks

**Robustness gain of 'diffusion classifiers' against backdoor attacks.** In the previous paragraphs, we explore defensive insights when DMs are employed as generative model. Recent research [23; 24] has demonstrated that DMs can serve as image classifiers by evaluating denoising errors under various prompt conditions (_e.g._, image classes). We inquire whether the DM-based classifier exhibits different backdoor effects compared to standard image classifiers when subjected to BadNets-like backdoor training. **Tab. A1** shows the robustness of the diffusion classifier and that of the standard ResNet-18 against backdoor attacks with various poisoning ratios. We can draw three main insights. First, when the backdoored DM is used as an image classifier, the backdoor effect against image classification is preserved, as evidenced by its attack success rate. Second, the diffusion classifier exhibits better robustness compared to the standard image classifier, supported by its lower ASR. Third, if we filter out the top \(p_{\mathbf{HIter}}\) (%) denoising loss of DM, we further improve the robustness of diffusion classifiers, by a decreasing ASR with the increase of \(p_{\mathbf{filter}}\). This is because backdoored DMs have high denoising loss in the trigger area for trigger-present images when conditioned on the non-target class. Filtering out the top denoising loss curves such inability of denoising a lot, with little sacrifice over the clean testing data accuracy.