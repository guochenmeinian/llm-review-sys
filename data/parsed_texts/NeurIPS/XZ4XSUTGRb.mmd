# Polyhedral Complex Derivation from

Piecewise Trilinear Networks

 Jin-Hwa Kim

NAVER AI Lab & SNU AIIS

Republic of Korea

j1nhwa.kim@navercorp.com

###### Abstract

Recent advancements in visualizing deep neural networks provide insights into their structures and mesh extraction from Continuous Piecewise Affine (CPWA) functions. Meanwhile, developments in neural surface representation learning incorporate non-linear positional encoding, addressing issues like spectral bias; however, this poses challenges in applying mesh extraction techniques based on CPWA functions. Focusing on trilinear interpolating methods as positional encoding, we present theoretical insights and an analytical mesh extraction, showing the transformation of hypersurfaces to flat planes within the trilinear region under the eikonal constraint. Moreover, we introduce a method for approximating intersecting points among three hypersurfaces contributing to broader applications. We empirically validate correctness and parsimony through chamfer distance and efficiency, and angular distance, while examining the correlation between the eikonal loss and the planarity of the hypersurfaces. The code is available at https://github.com/naver-ai/tropical-nerf.pytorch.

## 1 Introduction

Recent advancements in visualizing deep neural networks [1, 2, 3, 4] significantly contribute to understanding their intricate structures. This progress provides valuable insights into the expressivity, robustness, training methodologies, and distinctive geometry of neural networks. By leveraging the inherent piecewise linearity in the Continuous Piecewise Affine (CPWA) functions, _e.g._, ReLU neural networks, each region of the input space is represented as a convex polyhedron. The assembly of these sets constructs a polyhedral complex that delineates the decision boundaries of neural networks [5].

Building upon these breakthroughs, we explore the exciting prospect of analytically extracting a mesh representation from neural implicit surface networks [6, 7, 8]. The extraction of a mesh not only offers a precise visualization and characterization of the network's geometry but also does so efficiently through the utilization of vertices and faces, in contrast to sampling-based methods [9, 10]. The sampling-based methods are limited by their black-box nature, which can obscure the underlying structure and lead to inefficiencies in capturing fine geometric details.

These networks typically learn a signed distance function (SDF) by utilizing ReLU neural networks. However, recent approaches incorporate non-linear positional encoding techniques, such as trigonometric functions [11] or trilinear interpolations using hashing [12] or tensor factorization [13], to mitigate issues like spectral bias [14], ensuring fast convergence, and maintaining high-fidelity. Consequently, applying mesh extraction techniques based on CPWA functions becomes challenging.

Focusing on the successful and widely-adopted trilinear interpolating methods, we present novel theoretical insights and a practical methodology for precise mesh extraction. Following a novel reviewof the _edge subdivision_ method [4], an efficient mesh extraction technique for CPWA functions, within the framework of tropical geometry (Section 3), we demonstrate that ReLU neural networks incorporating the trilinear module are piecewise trilinear networks (Definition 4.1). Then, we establish that the hypersurface within the trilinear region becomes a plane under the eikonal [15] constraint (Theorem 4.5), a common practice during the training of SDF.

Building upon this observation, we propose a method for approximating the determination of intersecting points among three hypersurfaces. Given the inherent curvature of these surfaces, achieving an exact solution is infeasible. In our approach, we utilize two hypersurfaces and a diagonal plane for a more feasible and precise approximation (Theorem 4.7). Additionally, we introduce a methodology for arranging vertices composed of faces to implicitly represent normal vectors. We experimentally confirm accuracy and simplicity using chamfer distance, efficiency, and angular distance, simultaneously exploring the relationship between the eikonal loss and the planarity of the hypersurface. In Figure 1, we present the analytically generated skeleton and normal map produced by our method using piecewise trilinear networks composed of HashGrid and ReLU neural networks. For a more detailed view, please refer to Figure 10 in Appendix.

Our contributions are summarized as follows:

1. We present novel theoretical insights and a practical methodology for precise mesh extraction, from the piecewise trilinear networks using the HashGrid.
2. We provide a theoretical analysis of the piecewise trilinear networks under the eikonal constraint revealing that, within a trilinear region, a hypersurface transforms into a plane.
3. We validate its correctness and parsimony through chamfer distance and efficiency, and angular distance, showing the correlation between the eikonal loss and hypersurface planarity.

## 2 Related work

Mesh extraction in deep neural networks.A conventional black-box approach is to assess a given function at sampled points to derive the complex of the function, commonly through marching cubes [9] or marching tetrahedra [10; 16]. Despite the inclusion of mesh optimization techniques [17] in modern mesh packages, this exhaustive method often generates redundant complexities due to discretization, increasing the computational load by using unnecessary mesh elements (_e.g._, fragmented planners). Aiming for exact extraction, initial efforts focused on identifying linear regions [18] and employing _Analytical Marching_[2] to exactly reconstruct the zero-level isosurface. Contemporary approaches involve region subdivision, wherein the regions of the complex are progressively subdivided from neuron to neuron and layer to layer, allowing for the efficient calculation of the exponentially increasing linear regions [4; 19; 20; 21]. Nevertheless, many previous studies have explored CPWA functions consisting of fully connected layers and ReLU activation, primarily owing to the mathematical simplicity of hyperplanes defining linear regions. However, this restricts their applications in emerging domains of deep implicit surface learning that diverge from CPWA functions.

Figure 1: The mesh is analytically extracted from piecewise trilinear networks, comprising both HashGrid [12] and ReLU neural networks, that have been trained to learn a signed distance function with the eikonal loss. We start with initial vertices and edges defined using the grid marks (1), and its linear subdivision (2a); however, intersecting polygons (_ref._ Section 3.2) need bilinear subdivision (2b) and consequentially trilinear subdivision (2c). Note that the linear and bilinear subdivisions are specific instances of trilinear subdivisions with straightforward solutions.

Positional encoding.The spectral bias of a multi-layer perceptron (MLP) hinders effectively learning high frequencies, both theoretically and empirically [14]. To address this limitation, Fourier feature mapping, employing trigonometric projections similar to those used in the Transformer architecture [22], proves successful in representing complex 3D objects and scenes. Furthermore, to address its slow convergence and enhance rendering quality, novel positional encoding methods using trilinear interpolations, _e.g._, HashGrid [12] or TensoRF [13], are introduced. Both techniques generate positional features through trilinear interpolation among the eight nearest corner features derived from pre-defined 3D grids. These features are obtained by hashing from multi-resolution hash tables or factorized representations of a feature space, respectively. However, these approaches cause the function represented by neural networks to deviate from CPWA functions, as trilinear interpolation is not an affine transformation. Consequently, the complex regions are divided by intricate hypersurfaces.

Eikonal equation and SDF.An eikonal 1 equation is a non-linear first-order partial differential equation governing the propagation of wavefronts. Notably, this finds application in the regularization of a signed distance function (SDF) in the context of neural surface modeling [6; 8; 23]. If \(\Omega\) represents a subset of the space \(\mathcal{X}\) equipped with the metric \(d\), the SDF \(f(\bm{x})\) is defined as follows:

Footnote 1: The _eikonal_ is derived from the Greek, meaning image or icon.

\[f(\bm{x})=\begin{cases}-d(\bm{x},\partial\Omega)&\text{if }\bm{x}\in\Omega\\ d(\bm{x},\partial\Omega)&\text{if }\bm{x}\in\Omega^{\complement}\end{cases}\] (1)

where \(\partial\Omega\) denotes the boundary of \(\Omega\). The metric \(d\) is defined (with a slight notational abuse) as:

\[d(\bm{x},\partial\Omega):=\inf_{\bm{y}\in\partial\Omega}d(\bm{x},\bm{y})\] (2)

where \(d\) represents the shortest distance from \(\bm{x}\) to \(\partial\Omega\) in the Euclidean space of \(\mathcal{X}\). In the Euclidean space with a piecewise smooth boundary, the SDF exhibits differentiability almost everywhere, and its gradient adheres to _the eikonal equation_. Specifically, for points \(\bm{x}\) on the boundary, \(\nabla f(\bm{x})=N(\bm{x})\) where the Jacobian of \(f\) represents the outward normal vector, or equivalently, \(|\nabla f(\bm{x})|=1\).

## 3 Preliminaries

We present an overview of tropical geometry (Section 3.1) to provide a formal definition of the tropical hypersurface and the tropical algebra of neural networks. Then, we discuss the edge subdivision algorithm [4] for extracting the tropical hypersurface (Section 3.2). We supplement Appendix A.1 for less familiar readers. Later, we extend this idea to include considerations for trilinear interpolation in Section 4. You may choose to skip these sections if you are already acquainted with these concepts.

### Tropical geometry

Tropical 2 geometry [25; 26] describes polynomials and their geometric characteristics. As a skeletonized version of algebraic geometry, tropical geometry represents piecewise linear meshes using _tropical semiring_ and _tropical polynomial (ref._ Appendix A.1). The set of points where a tropical polynomial \(f\) is non-differentiable, due to tropical sum \(\oplus:=\max(\cdot)\), is called _tropical hypersurface_:

Footnote 2: The _tropical_ is named in honor of Hungarian-born Brazilian computer scientist Imre Simon [24].

**Definition 3.1** (Tropical hypersurface).: The tropical hypersurface of a tropical polynomial \(f(\bm{x})=c_{1}\bm{x}^{\alpha_{1}}\oplus\cdots\oplus c_{r}\bm{x}^{\alpha_{r}}\) is defined as:

\[V(f)=\{\bm{x}\in\mathbb{R}^{d}:c_{i}\bm{x}^{\alpha_{i}}=c_{j}\bm{x}^{\alpha_{ j}}=f(\bm{x})\quad\text{ for some }\ \alpha_{i}\neq\alpha_{j}\}\]

where the tropical monomial \(c_{i}\bm{x}^{\alpha_{i}}\) with \(d\)-variate \(\bm{x}\) is defined as \(c_{i}\odot x_{1}^{a_{i,1}}\odot\cdots\odot x_{d}^{a_{i,d}}\) with \(d\)-variate \(\alpha_{i}=(a_{i,1},\dots,a_{i,d})\) and _tropical product_\(\odot:=+\).

**Definition 3.2** (ReLU neural networks).: Assuming input and output dimensions are consistent, \(L\)-layer neural networks with ReLU activation are a composition of functions defined as:

\[\nu^{(L)}=\rho^{(L)}\circ\sigma^{(L-1)}\circ\rho^{(L-1)}\cdots\sigma^{(1)} \circ\rho^{(1)}\] (3)

where \(\sigma(\bm{x})=\max(\bm{x},0)\) and \(\rho^{(i)}(\bm{x})=\bm{W}^{(i)}\bm{x}+\bm{b}^{(i)}\).

Although the neural networks \(\nu\) is generally non-convex, it is known that the difference of two tropical _signomials_ (allowing a real-valued \(\alpha\) in a tropical polynomial) can represent \(\nu\)[27] (_ref._ Appendix A.3). Given this observation, we proceed to examine the following proposition:

**Proposition 3.3** (Decision boundary of neural networks).: _The decision boundary for \(L\)-layer neural networks with ReLU activation \(\nu^{(L)}(\bm{x})=0\) is a subset of or equals with the region where two tropical monomials, or two arguments of \(\max\), equal (Definition 3.1), as follows:_

\[\mathcal{B}\subseteq\left\{\bm{x}:\nu^{(i)}_{j}(\bm{x})=0,\ i\in(1,\ldots,L), \ j\in(1,\ldots,H)\ \text{if}\ i\neq L\ \text{else}\ (1)\right\}\]

_where the subscript \(j\) denotes the \(j\)-th neuron, assuming the hidden size of neural networks is \(H\), while the final \(L\)-th layer has a single output for rather discussions as an SDF (Definition 4.3). Remind that tropical operations satisfy the usual laws of arithmetic for recursive consideration._

The piecewise hyperplanes corresponding to \(\nu^{(i)}_{j}\) are the _candidates_ shaping the decision boundary. It enables for-loop iterations over the neurons across layers, as in Algorithm 2, not visiting every exponentially growing linear region [18; 27]. The equality in Proposition 3.3 generally does not hold, but we can filter them by checking if \(\nu^{(L)}_{1}(\bm{x})=0\). For more details, please refer to Berzins [4]. We note that our formal explanation through tropical geometry is unprecedented in the previous work [4].

### Edge subdivision for tropical geometry

Polyhedral 3 complex derivation from a multitude of linear regions is inefficient and may be infeasible in some cases. Rather, Berzins [4] argue that tracking vertices and edges of the decision boundary sequentially considering the polynomial number of hyperplanes is particularly efficient, even enabling parallel tensor computations for edge subdivisions.

Footnote 3: In geometry, a polyhedron is a three-dimensional shape with polygonal faces, having straight edges and sharp vertices.

Initially, we start with a unit cube of eight vertices and a dozen edges, denoted by \(\mathcal{V}\) and \(\mathcal{E}\), respectively. For each piecewise linear, or _folded_ in their term, hyperplane, we find the intersection points of each one of the edges and the hyperplane (if any) to add to \(\mathcal{V}\). The divided edges by the intersection are also added to \(\mathcal{E}\). Note that we also find new edges of polygons, the intersections of convex polyhedra representing corresponding linear regions [5], and the folded hyperplane. Repeating these for all folded hyperplanes, before selecting all vertices \(\bm{x}^{\star}\) where \(\nu(\bm{x}^{\star})=0\). Then, the valid edges connecting those vertices would represent the decision boundary.

To elaborate with details, the order of subdividing is invariant as stated in Proposition 3.3; however, we must perform the subdivision with all hyperplanes in the previous layers in advance if we want to keep the current set of edges not crossing linear regions. One critical advantage of _this principle_ is that we can find the intersection point by evaluating the output ratio of the two vertices of an edge, \(\nu(\bm{x}_{0})\) and \(\nu(\bm{x}_{1})\), which are proportional to the distances to the hyperplane by Thales's theorem [28]. The intersection point would simply be \(\hat{\bm{x}}_{0,1}=(1-w)\bm{x}_{0}+w\bm{x}_{1}\) where \(w=|d_{0}|/|d_{0}-d_{1}|\) and \(d_{k}=\nu^{(l)}_{j}(\bm{x}_{k})\), upon the fact that \(d_{0}\cdot d_{1}<0\) if the hyperplane divides the edge.

Updating edges needs two steps: dividing the edge into two new edges and adding new edges of intersectional polygons by a hyperplane for convex polyhedra representing linear regions. The latter is a tricky part that we need to find every pair of two vertices among \(\{\hat{\bm{x}}_{\cdot,\cdot}\}\), both within the same linear region _and_ on the two common hyperplanes, obviously including the current hyperplane. They argue that the _sign-vectors_ for the preactivations provide an efficient way to find them.

**Definition 3.4** (Sign-vectors).: For the current hyperplane specified by the \(i^{*}\)-th layer and its \(j^{*}\)-th preactivation, we define the sign-vectors with a small positive constant \(\epsilon\):

\[\gamma(\bm{x}_{k})_{\phi(i,j)}=\begin{cases}+1&\text{if}\ \nu^{(i)}_{j}(\bm{x}_{k}) \ >+\epsilon\\ 0&\text{if}\ |\nu^{(i)}_{j}(\bm{x}_{k})|\ \leq+\epsilon\\ -1&\text{if}\ \nu^{(i)}_{j}(\bm{x}_{k})\ <-\epsilon\end{cases}\] (4)

for all \(i\) and \(j\) where \(\phi(i,j)\leq\phi(i^{*},j^{*})\), and \(\phi(i,j)\in\{0\}\cup\mathbb{N}\) is an ascending indexing function from lower layers, handling arbitrary hidden sizes of networks to vectorize a signed matrix.

Notice that a hyperplane divides a space into two half-spaces, two linear regions. Collectively, if the sign-vectors for the \(i\)-th layer and its \(j\)-th preactivation of two vertices have the same values except for zeros, which are wild cards for matching, we can say that the two vertices are within the same linear region in the current step \((i^{*},j^{*})\). For the second, the sign-vector has at least three zeros specifying a point by at least three hyperplanes. If two zeros are at the same indices in the two sign-vectors, the two vertices are on the same two hyperplanes, forming an edge. Notice that our principle asserts edges should be within linear regions.

Berzins [4] argue that the edge subdivision provides the optimal time and memory complexities of \(\mathcal{O}(|\mathcal{V}|)\), linear in the number of vertices, while it can be efficiently computed in parallel. According to Hanin and Rolnick [20], the number of linear regions is known for \(o(N^{L}/L!)\) where \(N\) is the total number of neurons, or \(\phi(L,1)\) in our notation.

## 4 Method

### Piecewise trilinear networks

**Definition 4.1** (Trilinear interpolation).: Let a unit cube be in the \(D\)-dimensional space, where its corners are represented by \(\bm{H}\in\mathbb{R}^{2^{D}\times F}\) where \(F\) is a corner-feature dimension. Given weights \(\bm{w}\in[0,1]^{D}\), the trilinear interpolation function \(\psi\) is defined as:

\[\psi(\bm{w};\bm{H})=\sum_{i=0}^{2^{D}-1}\omega(i,\bm{w})\cdot\bm{H}_{i}\quad \in\mathbb{R}^{F}\] (5)

where the interpolating weight \(\omega(i,\bm{w})\in\mathbb{R}\) is defined using a left-aligned and zero-trailing binarization function \(\text{B}\in\{0,1\}^{D}\) (_ref._ Appendix B.1) as follows:

\[\omega(i,\bm{w})=\prod_{j=1}^{D}(1-\text{B}(i)_{j})(1-\bm{w}_{j})+\text{B}(i) _{j}\bm{w}_{j}\] (6)

which is the volume of the opposite subsection of the hypercube divided by the weight point \(\bm{w}\). For a general hypercube, scaling the weight for a unit hypercube gets the same result. Going forward, we assume \(D\)=3 for trilinear interpolation without explicitly stating it otherwise.

**Lemma 4.2** (Nested trilinear interpolation).: _Let a nested cube be inside of a unit cube, where its positions of the eight corners deviate \(-\bm{a}_{j}\) or \(+\bm{b}_{j}\), such that \(\bm{a}_{j},\bm{b}_{j}\geq 0\), from \(\bm{w}_{j}\) for each dimension \(j\), using the notations of Definition 4.1. The eight corners of the nested cube are the trilinear interpolations of the eight corners of the unit cube. Then, the trilinear interpolation with the unit cube and the nested cube for \(\bm{w}\) is identical._

Proof.: Notice that trilinear interpolation is linear for each dimension to prove it. The detailed proof can be found in Lemma D.1. 

**Definition 4.3** (Piecewise trilinear networks).: Let a positional encoding module be \(\tau:\mathbb{R}^{D}\rightarrow\mathbb{R}^{F}\) using the trilinear interpolation of spatially nearby learnable vectors on a three-dimensional grid, _e.g._, HashGrid [12] or TensoRF [13], where \(\tau\) is an instance of \(\psi\) in Definition 4.1. Usually, they transform the input coordinate \(\bm{x}\) to \(\bm{w}\) as a relative position inside a unit grid in a corresponding resolution. \(\bm{H}\) is a learnable parameter specified in the corresponding method. Then, we define trilinear neural networks \(\tilde{\nu}\), by prepending \(\tau\) to the neural networks \(\nu\) from Definition 3.2. Here, the input and output dimensions of \(\nu^{(L)}\) are \(F\) and \(1\) (an SDF distance), respectively.

\[\tilde{\nu}^{(L)}=\nu^{(L)}\circ\tau\] (7)

Note that \(\tau\) is _gridwise_ trilinear. Using Lemma 4.2, \(\tilde{\nu}\) is trilinear within the composite intersections of linear regions of \(\nu\) and trilinear regions of \(\tau\). (We discuss how to access _virtual_ cubic corner features where trilinear regions are not cubic in Section 5.) So, we regard \(\tilde{\nu}\) as _piecewise_ trilinear.

### Curved edge subdivision in a trilinear space

In a trilinear space, \(\tau\) projects a line to a curve except the lines on the grid. Notably, the diagonal line \(\bm{t}=(t,t,t)^{\intercal}\) where \(t\in[0,1]\) is projected to a cubic Bezier curve (_ref._ Proposition D.2). We aim to generalize for the _curved_ edge subdivision for _hypersurfaces_.

**Lemma 4.4** (Curved edge of two hypersurfaces).: _In a piecewise trilinear region, let an edge \((\bm{x}_{0},\bm{x}_{7})\) be the intersection of two hypersurfaces \(\tilde{\nu}_{i}(\bm{x})=\tilde{\nu}_{j}(\bm{x})=0\), while \(\bm{x}_{0}\) and \(\bm{x}_{7}\) are on the two hypersurfaces. Then, the edge is defined as:_

\[\{\bm{x}:\tau(\bm{x})=(1-t)\tau(\bm{x}_{0})+t\tau(\bm{x}_{7})\;\;\text{where}\; \;\bm{x}\in\mathbb{R}^{D}\;\;\text{and}\;\;t\in\mathbb{R}\}.\]

Proof.: The detailed proof using the piecewise linearity of \(\nu\) is provided in Lemma D.4. 

Notice that it does not decrease the number of variables to find a line solution since the trilinear interpolation \(\tau\)_still_ forms hypersurfaces making complex cases. Yet, we theoretically demonstrate that the eikonal constraints on \(\tilde{\nu}\) render them hyperplanes with linear solutions.

**Theorem 4.5** (Hypersurface and eikonal constraint).: _A hypersurface \(\tau(\bm{x})=0\) intersects two points \(\tau(\bm{x}_{0})=\tau(\bm{x}_{7})=0\) while \(\tau(\bm{x}_{1\dots 6})\neq 0\) for the remaining six points. These points form a cube, with \(\bm{x}_{0}\) and \(\bm{x}_{7}\) positioned on the diagonal of the cube. The hypersurface satisfies the eikonal constraint \(\|\nabla\tau(\bm{x})\|_{2}^{2}=1\) for all \(\bm{x}\in[0,1]^{3}\). Then, the hypersurface of \(\tau(\bm{x})=0\) is a plane._

**Corollary 4.6** (Affine-transformed hypersurface and eikonal constraint).: _Let \(\nu_{0}(\bm{x})\) and \(\nu_{1}(\bm{x})\) be two affine transformations defined by \(\bm{W}_{i}^{\intercal}\bm{x}+\bm{b}_{i}\), \(i\in\{0,1\}\). A hypersurface \(\nu_{0}\big{(}\tau(\bm{x})\big{)}=0\) passing two points \(\nu_{0}\big{(}\tau(\bm{x}_{0})\big{)}=\nu_{0}\big{(}\tau(\bm{x}_{7})\big{)}=0\) while \(\nu_{0}\big{(}\tau(\bm{x}_{1\dots 6})\big{)}\neq 0\) satisfies the eikonal constraint \(\|\nabla\nu_{1}\big{(}\tau(\bm{x})\big{)}\|_{2}^{2}=1\) for all \(\bm{x}\in[0,1]^{3}\). Then, the hypersurface of \(\nu_{0}\big{(}\tau(\bm{x})\big{)}=0\) is a plane._

The proofs using its partial derivatives can be found in Theorem D.5 and Corollary D.6.

Since we aim for a practical polyhedral complex derivation that piecewise trilinear networks represent, we replace one of the hypersurfaces forming the curved edge with a diagonal plane in a piecewise trilinear region. We choose this option not only for its mathematical simplicity but also due to the characteristics of trilinear hypersurfaces, as illustrated in Figure 9, Appendix G. Thus, the new vertices lie on at least two hypersurfaces, and the new edges exist on the same hypersurface, while the eikonal constraint minimizes any associated error from this approximation. This error is tolerated by the hyperparameter \(\epsilon=\)1e-4 as specified in Definition 3.4 and Section 6. More discussions on this matter can be found in Appendix C.1.

**Theorem 4.7** (Intersection of two hypersurfaces and a diagonal plane).: _Let \(\tilde{\nu}_{0}(\bm{x})=0\) and \(\tilde{\nu}_{1}(\bm{x})=0\) be two hypersurfaces passing two points \(\bm{x}_{0}\) and \(\bm{x}_{7}\) such that \(\tilde{\nu}_{0}(\bm{x}_{0})=\tilde{\nu}_{1}(\bm{x}_{0})=0\) and \(\tilde{\nu}_{0}(\bm{x}_{7})=\tilde{\nu}_{1}(\bm{x}_{7})=0\), \(\bm{P}_{i}:=\tilde{\nu}_{0}(\bm{x}_{i})\) and \(\bm{Q}_{i}:=\tilde{\nu}_{1}(\bm{x}_{i})\), \(\bm{P}_{\alpha}=\big{[}\bm{P}_{0};\;\bm{P}_{1};\;\bm{P}_{4};\;\bm{P}_{5}\big{]}\), \(\bm{P}_{\beta}=\big{[}\bm{P}_{2};\;\bm{P}_{3};\;\bm{P}_{6};\;\bm{P}_{7}\big{]}\), and \(\bm{X}=[(1-x)^{2};\;x(1-x);\;(1-x)x;\;x^{2}]\). Then, \(x\in[0,1]\) of the intersection point of the two hypersurfaces and a diagonal plane of \(x=z\) is the solution of the following quartic equation:_

\[\bm{X}^{\intercal}\big{(}\bm{P}_{\alpha}\bm{Q}_{\beta}^{\intercal}-\bm{P}_{ \beta}\bm{Q}_{\alpha}^{\intercal}\big{)}\bm{X}=0,\quad\text{while}\quad y= \frac{\bm{X}^{\intercal}\bm{P}_{\alpha}}{\bm{X}^{\intercal}(\bm{P}_{\alpha}- \bm{P}_{\beta})}\quad(\bm{P}_{\alpha}\neq\bm{P}_{\beta}).\] (8)

Proof.: Please refer to Theorem D.7 for the detailed proof, which uses linear algebra to rearrange two trilinear equations with two variables to get a solution. 

Note that finding roots of a polynomial is the eigenvalue decomposition of a companion matrix [29], which can be parallelized (_ref._ Lemma D.8). Given that the companion matrix remains small for a quartic equation in \(\mathbb{R}^{4\times 4}\), the overall complexity remains \(\mathcal{O}(|\mathcal{V}|)\). For the detailed complexity analysis on the proposed method, please refer to Appendix E.

### Skeletonization, faces, and normals

**Skeletonization** selects the vertices such that: \(\mathcal{V}^{\star}=\{\bm{x}\;|\;|\tilde{\nu}(\bm{x})|\leq\epsilon,\bm{x}\in \mathcal{V}\}\), and the edges \(\mathcal{E}^{\star}\) such that their two vertices are among \(\mathcal{V}^{\star}\). Recall that because a decision boundary is a subset of the _tropical hypersurface_ (Definition 3.1 and Proposition 3.3), this step ensures accurate mesh extraction.

**Faces** are formed by edges lying on the same plane and sharing a common region. This is identified by evaluating the sign-vectors (_ref._ Definition 3.4 and Section 5). Note that faces are not inherently triangular; however, if needed, they can be triangulated via _triangularization_ for further analysis.

**Normals** are conventionally described by the order of the vertices as each face has ambiguity with two opposite directions. Let \(\bm{x}_{0,1,2}\) be the vertices forming a triangular face. The normal is defined using cross-product as \(\bm{n}=(\bm{x}_{1}-\bm{x}_{0})\times(\bm{x}_{2}-\bm{x}_{0})\) where the normal is orthogonal to both vectors, with a direction given by the right-hand rule. Please refer to Appendix B.2 for the details.

## 5 Implementation

We employ the HashGrid [12] for \(\tau\), while our discussion on its generalizability is provided in Appendix C.2. We illustrate our method in Algorithm 1, 2, and Figure 2, inspired by Hanin and Rolnick [30] for its visualization of linear regions. We describe the implemental details of the algorithms in the following sections and the corresponding caption of Figure 6.

Initialization.While the original edge subdivision algorithm [4] starts with a unit cube of eight vertices and a dozen edges, we know that the unit cube is subdivided by orthogonal planes consisting of the grid. Let an input coordinate be \(\bm{x}\in[0,1]^{3}\), and there is a unit cube such that one of its corners is at the origin. Taking into account multi-resolution grids, we obtain the marks \(m_{i}\), such that the grid planes are \(x=m_{i}\), \(y=m_{i}\), and \(z=m_{i}\), following Algorithm 4 in Appendix. Notice that we carefully consider the offset \(s/2\), which prevents the zero derivatives from aligning across all resolution levels (_ref._ Appendix A of Muller et al. [12]). Leveraging the obtained marks, we derive the associated grid vertices and edges, which serve as the initial sets for \(\mathcal{V}\) and \(\mathcal{E}\), respectively. Please refer to Figure 1(a) where its multi-resolution grids are visualized using randomized colors.

Optimizing sign-vectors.The number of orthogonal planes may significantly surpass the number of neurons, making it impractical to allocate elements for the sign-vectors (Definition 3.4). To address this, leveraging the orthogonality of the grid planes, we store a plane index along with an indicator of whether the input is on the plane (\(\gamma=0\)) or not (\(\gamma=1\)).

Piecewise trilinear region.In Theorem 4.7, we need the outputs of corners \(\bm{P}\) and \(\bm{Q}\) in a common piecewise trilinear region; however, some corners may be outside of the region. For this, we replace all ReLU activations using the mask \(\bm{m}\) that: \(\bm{m}_{j}^{(i)}=(\tilde{\nu}_{j}^{(i)}(\bm{x}_{0})>\epsilon)\mid(\tilde{\nu} _{j}^{(i)}(\bm{x}_{7})>\epsilon)\), where \(\bm{x}_{0}\) and \(\bm{x}_{7}\) are the vertices of an edge of interest, for the calculated cubic corners \(\bm{x}_{0...7}\). This implies that \(\nu\) exhibits linearity, except for instances where two vertices simultaneously deviate from linearity. For \(\bm{P}\) and \(\bm{Q}\), we select the last two pairs of \(i\) and \(j\) such that \(\gamma_{\phi(i,j)}(\bm{x}_{0})=\gamma_{\phi(i,j)}(\bm{x}_{7})=0\), indicating two common hypersurfaces passing two points \(\bm{x}_{0}\) and \(\bm{x}_{7}\).

## 6 Experiment

Objective.For a given trilinear neural network with the eikonal constraint, our goal is to get a boundary mesh of the zero-set of the SDF. (Note that NeuS [6] showed how to convert the density

Figure 2: Trilinear regions in the xy-plane at \(z=0.04\), identified by the sign-vectors (Definition 3.4), are represented with random colors. (a) Grids described in Section 5 and Algorithm 4. (b) The neurons of the first layer representing folded hypersurfaces (blue arrow). (c) All neurons representing every nonlinear boundary. (d) Select all zero-set vertices and edges. (e) Skeletonized as in Section 4.3.

networks of NeRFs to an SDF in the frameworks of volume rendering.) To evaluate this, we utilize marching cubes with excessive sampling to get a pseudo-ground truth mesh to remove Bayes error caused by underfitting. We explicitly specify the samplings in the results.

Hyperparameters.We used the number of layers \(L\) of 3 and hidden size \(H\) of 16 for the networks, and \(\epsilon\) of 1e-4 for the sign-vectors (_ref._ Definition 3.4). The weight for the eikonal loss is 1e-2. For HashGrid, the resolution levels of 4, feature size of 2, base resolution \(N_{\text{min}}\) of 2, and max resolution \(N_{\text{max}}\) of 32 _by default_. \(N_{\text{min}}\) and \(N_{\text{max}}\) are doubled (x2) or quadrupled (x4) for _Medium_ or _Large_ settings in Figure 4. We use the official Python package of tinycudnn 4 for the HashGrid module.

Footnote 4: https://github.com/NVlabs/tiny-cuda-nn

Chamfer distance and efficiency.The chamfer distance is a metric used to evaluate the similarity between two sets of sampled points from two meshes. Let \(\mathcal{S}_{0}\) and \(\mathcal{S}_{1}\) be the two sets of points. Specifically, the bidirectional chamfer distance (CD) is defined as follows:

\[\text{CD}(\mathcal{S}_{0},\mathcal{S}_{1})=\frac{1}{2}\Big{(}\overrightarrow{ \text{CD}}(\mathcal{S}_{0},\mathcal{S}_{1})+\overrightarrow{\text{CD}}( \mathcal{S}_{1},\mathcal{S}_{0})\Big{)},\ \ \overrightarrow{\text{CD}}(\mathcal{S}_{0},\mathcal{S}_{1})=\frac{1}{| \mathcal{S}_{0}|}\sum_{\bm{x}\in\mathcal{S}_{0}}\min_{\bm{y}\in\mathcal{S}_{1} }\|\bm{x}-\bm{y}\|_{2}^{2}.\] (9)

We randomly select 100K points on the faces through ray-marching from the origin, directing toward a randomly chosen point on a unit sphere.

Table 1 and Figure 4 show the results for the Standford 3D Scanning repository [31]. Varying the number of samples in marching cubes [9], we plot the baseline with respect to the number of generated vertices. Given that our method extracts vertices from the intersection points of hypersurfaces, it enables parsimonious representations using the lower number of vertices having better CD, summarized by chamfer efficiency (CE) 5 in Table 1, and ours are located in the lower-left region compared with the baselines in Figure 4. In particular, we observe a strong CE in a simple object, _e.g._, Drill Bit, which achieved a CE of 47.8 versus 19.2 (MC).

Footnote 5: CE is defined as \(100/(\sqrt{|\mathcal{V}|}\times\text{CD})\), reflecting the trade-off between the number of vertices and the CD.

Our method inherently holds an advantage over MC in capturing optimal vertices and faces, especially in cases where the target surfaces exhibit planar characteristics. When dealing with curved surfaces, the smaller grid size of the _Large_ model in Figure 4 views a curved surface in a smaller interval, and it tends to approach the plane. From the plotting of _Small_ to

Figure 3: Chamfer distance for the bunny with the _Large_ model comparing with MC, MT, and NDC.

_Large_, our CDs (colored crosses) are decreasing to zero, consistently with better CEs, confirming this speculation. The complete results of the _Large_ models and its visualization can be found in Tables 6 to 8 in Appendix F.1 and the right side of Figure 10 in Appendix, respectively. We also compare ours with Marching Tetrahedra (MT) [10, 16] and Neural Dual Contour (NDC) [32] in Figure 3, where our method achieved the lowest chamfer distance and the most efficiency method with respect to the number of vertices. For the rationale behind our selection, please refer to Appendix F.3.

Angular distance.The angular distance measures how much the normal vectors deviate from the ground truths. Analogous to the chamfer distance, we sample 100K points on the surface and calculate the normal vectors as described in Section 4.3. The angular distance is defined as follows:

\[\text{AD}(\mathcal{N}_{0},\mathcal{N}_{1})=\mathbb{E}_{i}\Big{[}\frac{180}{ \pi}\cdot\cos^{-1}\big{(}\langle\mathcal{N}_{0}^{(i)},\mathcal{N}_{1}^{(i)} \rangle\big{)}\Big{]}\] (10)

In Table 5 in Appendix, the angular distances for the Stanford bunny are shown. As we expected, our approach efficiently estimates the faces using a parsimonious number of vertices compared to the sampling-based method. We confirm this trend is consistent across other objects.

Eikonal constraint for planarity.To assess the efficacy of the eikonal constraint, which enforces the planarity of hypersurfaces as outlined in Theorem 4.5, we quantify the flatness error associated with the planarity constraints presented in the proof of Theorem D.5. Let \(\bm{x}_{0}\) and \(\bm{x}_{7}\) be two vertices consisting of a diagonal edge, while \(\bm{x}_{1\ldots 6}\) are its remaining cubic corners. We obtain \(\tilde{\nu}(\bm{x}_{1\ldots 6})\) in a piecewise trilinear region as described in Section 5. This evaluation is conducted as follows:

\[\Delta_{\text{flat}}= \mathbb{E}_{\mathcal{E}}\Big{[}\frac{1}{4}\big{(}\|\Sigma_{i\in \{1,2,4\}}\tilde{\nu}(\bm{x}_{i})\|_{1}+\|\Sigma_{i\in\{3,5,6\}}\tilde{\nu}( \bm{x}_{i})\|_{1}\big{)}+\frac{1}{6}\big{(}\sum_{i=1}^{3}\|\Sigma_{j\in\{i,7-i \}}\tilde{\nu}(\bm{x}_{j})\|_{1}\big{)}\Big{]}.\] (11)

In Figure 5, we empirically validate that the eikonal loss enforces planarity in the hypersurfaces within piecewise trilinear regions, as described in Theorem 4.5. The absence of the eikonal loss results in elevated planarity errors, leading to the construction of an inaccurate mesh.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{Bunny} & \multicolumn{3}{c}{Dragon} & \multicolumn{3}{c}{Happy Buddha} & \multicolumn{1}{c}{Arraudilio} & Drill & Lucy \\ \cline{2-13} MC \# & \(|\mathcal{V}|\downarrow\) & CD \(\downarrow\) & CE \(\uparrow\) & \(|\mathcal{V}|\downarrow\) & CD \(\downarrow\) & CE \(\uparrow\) & \(|\mathcal{V}|\downarrow\) & CD \(\downarrow\) & CE \(\uparrow\) & CE \(\uparrow\) & CE \(\uparrow\) & CE \(\uparrow\) \\ \hline
32 & 1283 & 4106 & 19.0 & 1416 & 6330 & 11.2 & 1032 & 6951 & 13.9 & 11.2 & 7.5 & **24.9** \\
64 & 5367 & 1371 & 13.6 & 6090 & 1936 & 8.5 & 4362 & 2465 & 9.3 & 8.3 & 19.2 & 16.0 \\
128 & 21825 & 393 & 11.6 & 52236 & 542 & 7.3 & 18219 & 722 & 7.6 & 6.6 & 17.4 & 10.7 \\
196 & 49569 & 141 & 14.3 & 57341 & 203 & 8.6 & 41351 & 276 & 8.8 & 7.5 & 15.3 & 11.7 \\ \hline Ours & 4341 & 900 & **25.6** & 5104 & 1243 & **15.8** & 3710 & 1738 & **15.5** & **13.9** & **47.8** & 23.6 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Chamfer distance (CD) and chamfer efficiency (CE) for the Stanford 3D Scanning repository [31]. The CD (\(\times\)1e-6) is evaluated using the marching cubes (MC) with 256\({}^{3}\) grid samples as the reference ground truth for all measurements. Please refer to Table 2 and Table 3 for the full results, and Table 4 for standard deviation and time spent in Appendix (Ours took 0.97 \(\pm\) 0.21 while MC also took within 1 sec for the bunny.)Visualization.We provide the qualitative analyses in Figure 6 comparing with MT [10; 16] and NDC [32], along with MC. While the competitors exhibit over-smoothing or inaccuracies, particularly in the nose region, our method preserves surface details with consistent normals and outperforms in both accuracy and generalization. In addition, we present Figures 10 to 13 in Appendix G encompassing the comparison between the _Small_ and _Large_ models (Figure 10), visualizing the marching cubes varying the number of samples (Figure 11), detailed comparisons (Figure 12), and the gallery of the Stanford 3D Scanning meshes from the _Large_ models (Figure 13).

Limitations.Although we implement the algorithm using pre-defined parallel tensor operations in PyTorch [33], achieving time and memory complexity of \(\mathcal{O}(|\mathcal{V}|)\) as detailed in Appendix E, our method inherently requires forward passing the intermediate vertices during edge subdivisions iterating over neurons, which depends on previous steps (Algorithm 1). For the Stanford bunny using the _Small_ model, the intermediate counts for vertices and edges exceed 11K and 19K, respectively, although they are eventually reduced to 4.3K vertices for face extraction. This overhead is negligible for the _Small_ models, taking 0.97 seconds; however, for the _Large_ models, the process takes 4.15\(\pm\)0.61 seconds for 140.3\(\pm\)5.6K vertices with our optimized algorithm, compared to 1.31 and 11.88 seconds for marching cubes with 256 and 512 samples per axis, respectively. We note that parallelizing over multiple GPUs would be a reliable option to improve latency. For the discussions on satisfying the eikonal equation in practice and the generalization to other positional encodings using trilinear interpolation, please refer to Appendix C.1 and Appendix C.2, respectively.

## 7 Conclusions

In conclusion, our exploration of novel techniques in visualizing trilinear neural networks unveils its intricate structures and extends mesh extraction applications beyond CPWA functions. Focused on trilinear interpolating methods, our study yields novel theoretical insights and a practical methodology for precise mesh extraction. Notably, Theorem 4.5 shows the transformation of hypersurfaces to planes within the trilinear region under the eikonal constraint. Empirical validation using chamfer distance and efficiency, and angular distance affirms the correctness and parsimony of our methodology. The correlation analysis between eikonal loss and hypersurface planarity confirms the theoretical findings. Our proposed method for approximating intersecting points among three hypersurfaces, coupled with a vertex arrangement methodology, broadens the applications of our techniques. This work establishes a foundation for future research and practical applications in the evolving landscape of deep neural network analysis and toward real-time rendering mesh extraction.

Figure 6: This figure provides a detailed visualization of Figure 10 in the Appendix and its competitors. MC 64, MT 32 (Marching Tetrahedra), and NDC (Neural Dual Contour [32]) suffer over-smoothing or inaccuracy of the actual surface in the Small networks within the nose, whereas our method reflects these details (see the boundaries of a nose) with consistent normals (in colors). MT efficiently demands SDF values by utilizing six tetrahedra within grids to get intermediate vertices. However, it is inefficient in terms of the number of vertices extracted. NDC faces a generalization issue for a zero-shot setting for the given networks, producing unsmooth surfaces.

## Acknowledgments

I would like to express my sincere appreciation to my brilliant colleagues, Sangdoo Yun, Dongyoon Han, and Injae Kim, for their contributions to this work. Their constructive feedback and guidance have been instrumental in shaping the work. I also express my sincere thanks to the anonymous reviewers for their help in improving the manuscript. The NAVER Smart Machine Learning (NSML) platform [34] had been used for experiments.

## References

* [1] Zheyan Zhang, Yongxing Wang, Peter K Jimack, and He Wang. Meshingnet: A new mesh generation method based on deep learning. In _International Conference on Computational Science_, pages 186-198. Springer, 2020.
* [2] Jiabao Lei and Kui Jia. Analytic marching: An analytic meshing solution from deep implicit surface networks. In _International Conference on Machine Learning_, pages 5789-5798. PMLR, 2020.
* [3] Jiabao Lei, Kui Jia, and Yi Ma. Learning and meshing from deep implicit surface networks using an efficient implementation of analytic marching. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(12):10068-10086, 2021.
* [4] Arturs Berzins. Polyhedral complex extraction from relu networks using edge subdivision. In _International Conference on Machine Learning_. PMLR, 2023.
* [5] J Elisenda Grigsby and Kathryn Lindsey. On transversality of bent hyperplane arrangements and the topological expressiveness of relu neural networks. _SIAM Journal on Applied Algebra and Geometry_, 6(2):216-242, 2022.
* [6] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. _arXiv preprint arXiv:2106.10689_, 2021.
* [7] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin, Pratul P Srinivasan, Richard Szeliski, Jonathan T Barron, and Ben Mildenhall. Bakedsdf: Meshing neural sdfs for real-time view synthesis. _arXiv preprint arXiv:2302.14859_, 2023.
* [8] Zhaoshuo Li, Thomas Muller, Alex Evans, Russell H Taylor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin. Neuralangelo: High-fidelity neural surface reconstruction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8456-8465, 2023.
* [9] William E Lorensen and Harvey E Cline. Marching cubes: A high resolution 3d surface construction algorithm. _ACM SIGGRAPH Computer Graphics_, 21(4):163-169, 1987.
* [10] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra: a hybrid representation for high-resolution 3d shape synthesis. _Advances in Neural Information Processing Systems_, 34:6087-6101, 2021.
* [11] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. _Communications of the ACM_, 65(1):99-106, 2021.
* [12] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. _ACM Transactions on Graphics (ToG)_, 41(4):1-15, 2022.
* [13] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. TensoRF: Tensorial radiance fields. In _European Conference on Computer Vision_, pages 333-350. Springer, 2022.
* [14] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. _Advances in Neural Information Processing Systems_, 33:7537-7547, 2020.

* [15] Heinrich Bruns. _Das eikonal_, volume 21. S. Hirzel, 1895.
* [16] Akio Doi and Akio Koide. An efficient method of triangulating equi-valued surfaces by using tetrahedral cells. _IEICE TRANSACTIONS on Information and Systems_, 74(1):214-224, 1991.
* [17] Hugues Hoppe, Tony DeRose, Tom Duchamp, John McDonald, and Werner Stuetzle. Mesh optimization. In _Proceedings of the 20th annual conference on Computer graphics and interactive techniques_, pages 19-26, 1993.
* [18] Thiago Serra, Christian Tjandraatmadja, and Srikumar Ramalingam. Bounding and counting linear regions of deep neural networks. In _International Conference on Machine Learning_, pages 4558-4566. PMLR, 2018.
* [19] Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the expressive power of deep neural networks. In _international conference on machine learning_, pages 2847-2854. PMLR, 2017.
* [20] Boris Hanin and David Rolnick. Complexity of linear regions in deep networks. In _International Conference on Machine Learning_, pages 2596-2604. PMLR, 2019.
* [21] Ahmed Imtiaz Humayun, Randall Balestriero, Guha Balakrishnan, and Richard G Baraniuk. Splinecam: Exact visualization and characterization of deep network geometry and decision boundaries. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3789-3798, 2023.
* [22] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [23] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and Yaron Lipman. Implicit geometric regularization for learning shapes. _arXiv preprint arXiv:2002.10099_, 2020.
* [24] Eric Katz. What is... tropical geometry. _Notices of the AMS_, 64(4), 2017.
* [25] Ilia Itenberg, Grigory Mikhalkin, and Eugenii I Shustin. _Tropical algebraic geometry_, volume 35. Springer Science & Business Media, 2009.
* [26] Diane Maclagan and Bernd Sturmfels. _Introduction to tropical geometry_, volume 161. American Mathematical Society, 2021.
* [27] Liwen Zhang, Gregory Naitzat, and Lek-Heng Lim. Tropical geometry of deep neural networks. In _International Conference on Machine Learning_, pages 5824-5832. PMLR, 2018.
* [28] Thomas Friedrich et al. _Elementary geometry_, volume 43. American Mathematical Soc., 2008.
* [29] Alan Edelman and Hiroshi Murakami. Polynomial roots from companion matrix eigenvalues. _Mathematics of Computation_, 64(210):763-776, 1995.
* [30] Boris Hanin and David Rolnick. Deep relu networks have surprisingly few activation patterns. _Advances in neural information processing systems_, 32, 2019.
* [31] Brian Curless and Marc Levoy. A volumetric method for building complex models from range images. In _Proceedings of the 23rd annual conference on Computer graphics and interactive techniques_, pages 303-312, 1996.
* [32] Zhiqin Chen, Andrea Tagliasacchi, Thomas Funkhouser, and Hao Zhang. Neural dual contouring. _ACM Transactions on Graphics (TOG)_, 41(4):1-13, 2022.
* [33] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.

* [34] Hanjoo Kim, Minkyu Kim, Dongjoo Seo, Jinwoong Kim, Heungseok Park, Soeun Park, Hyunwoo Jo, KyungHyun Kim, Youngil Yang, Youngkwan Kim, Nako Sung, and Jung-Woo Ha. NSML: Meet the MLAAS platform with a real-world case study. _arXiv preprint arXiv:1810.09957_, 2018.
* [35] Michael Garland and Paul S Heckbert. Surface simplification using quadric error metrics. In _Proceedings of the 24th annual conference on Computer graphics and interactive techniques_, pages 209-216, 1997.
* [36] Edoardo Remelli, Artem Lukoianov, Stephan Richter, Benoit Guillard, Timur Bagautdinov, Pierre Baque, and Pascal Fua. MeshSDF: Differentiable iso-surface extraction. _Advances in Neural Information Processing Systems_, 33:22468-22478, 2020.
* [37] Tianchang Shen, Jacob Munkberg, Jon Hasselgren, Kangxue Yin, Zian Wang, Wenzheng Chen, Zan Gojcic, Sanja Fidler, Nicholas Sharp, and Jun Gao. Flexible isosurface extraction for gradient-based mesh optimization. _ACM Trans. Graph._, 42(4):37-1, 2023.

## Appendix A Tropical geometry and tropical algebra of neural networks

In this section, we present an overview of tropical geometry (Appendix A.1) to provide a formal definition of the tropical hypersurface (Appendix A.2). This hypersurface is significant as it serves as the decision boundary for ReLU neural networks optimizing an SDF, resulting in the formation of a polyhedral complex. For this, we delve into the examination of the tropical algebra of neural networks (Appendix A.3). Furthermore, we discuss the edge subdivision algorithm [4] for extracting the tropical hypersurface (Section 3.2), and we extend this discussion to include considerations for trilinear interpolation in Section 4. Given that Appendix A.1 to A.3 offer insights into tropical geometry, illustrating how each neuron contributes to forming the polyhedral complex, you may choose to skip these subsections if you are already acquainted with these concepts.

### Tropical geometry

Tropical geometry [25, 26] describes polynomials and their geometric characteristics. As a skeletonized version of algebraic geometry, tropical geometry represents piecewise linear meshes using _tropical semiring_ and _tropical polynomial_.

The tropical semiring is an algebraic structure that extends real numbers and \(-\infty\) with the two operations of _tropical sum_\(\oplus\) and _tropical product_\(\odot\). Tropical sum and product are conveniently replaced with \(\max(\cdot)\) and \(+\), respectively. Notice that \(-\infty\) is the tropical sum identity, zero is the tropical product identity, and these satisfy associativity, commutativity, and distributivity. For example, a classical polynomial \(x^{3}+2xy+y^{4}\) would represent \(\max(3x,\ x+y+2,\ 4y)\). Theclassical polynomial is rewritten for a tropical polynomial by naively replacing + with \(\oplus\) and \(\cdot\) with \(\odot\), respectively. After this, we rewrite by the definitions of tropical operations as \(\max(3x,\;x+y+2,\;4y)\). Roughly speaking, this is the tropicalization of the polynomial, denoted tropical polynomial or simply \(f\). Note that this example (ours with max notation) comes from the Wikipedia 6.

Footnote 6: https://en.wikipedia.org/wiki/Tropical_geometry

Additionally, we present a graphical diagram in Figure 7 illustrating how the tropical polynomial divides the 2D space for clearer understanding. Please see the caption for further details.

### Tropical hypersurface

The set of points where a tropical polynomial \(f\) is non-differentiable, mainly due to tropical sum, \(\max(\cdot)\), is called _tropical hypersurface_, denoted \(V(f)\). Note that \(V\) is an analogy to the _vanishing set_ of a polynomial.

**Definition A.1** (Tropical hypersurface, restated).: The tropical hypersurface of a tropical polynomial \(f(\bm{x})=c_{1}\bm{x}^{\alpha_{1}}\oplus\cdots\oplus c_{r}\bm{x}^{\alpha_{r}}\) is defined as:

\[V(f)=\{\bm{x}\in\mathbb{R}^{d}:c_{i}\bm{x}^{\alpha_{i}}=c_{j}\bm{x}^{\alpha_{j} }=f(\bm{x})\quad\text{ for some }\;\alpha_{i}\neq\alpha_{j}\}\]

where the tropical monomial \(c_{i}\bm{x}^{\alpha_{i}}\) with \(d\)-variate \(\bm{x}\) is defined as \(c_{i}\odot x_{1}^{a_{i,1}}\odot\cdots\odot x_{d}^{a_{i,d}}\) with \(d\)-variate \(\alpha_{i}=(a_{i,1},\ldots,a_{i,d})\) and _tropical product_\(\odot:=+\). In other words, this is the set of points where \(f(\bm{x})\) equals two or more monomials in \(f\).

### Tropical algebra of neural networks

**Definition A.2** (ReLU neural networks, restated).: Assuming input and output dimensions are consistent, \(L\)-layer neural networks with ReLU activation are a composition of functions defined as:

\[\nu^{(L)}=\rho^{(L)}\circ\sigma^{(L-1)}\circ\rho^{(L-1)}\cdots\sigma^{(1)}\circ \rho^{(1)}\] (12)

where \(\sigma(\bm{x})=\max(\bm{x},0)\) and \(\rho^{(i)}(\bm{x})=\bm{W}^{(i)}\bm{x}+\bm{b}^{(i)}\).

Although the neural networks \(\nu\) is generally non-convex, it is known that the difference of two tropical _signomials_ can represent \(\nu\)[27]. A tropical signomial takes the same form as a tropical polynomial (_ref._ Definition 3.1), but it additionally allows the exponentials \(\alpha\) to be real values rather than integers. With tropical signomials \(F\), \(G\), and \(H\),

\[\nu^{(l)}(\bm{x}) =H^{(l)}(\bm{x})-G^{(l)}(\bm{x})\] (13) \[\sigma^{(l)}\circ\nu^{(l)}(\bm{x}) =F^{(l)}(\bm{x})-G^{(l)}(\bm{x})\] (14)

Figure 7: A classical polynomial \(x^{2}+y^{2}+2xy+2x+2y+2\) turns into a tropical polynomial as \(\max(1+2x,1+2y,2+x+y,2+x,2+y,2)\), taking into account the implicit coefficient of 1. We redraw Figure 1 of the tropical curve from Zhang et al. [27] as a region map. Each color indicates a region where each tropical monomial has maximum.

where \(F^{(l)}\), \(G^{(l)}\), and \(H^{(l)}\) are as follows:

\[F^{(l)}(\bm{x}) =\max\big{(}H^{(l)}(\bm{x}),\;G^{(l)}(\bm{x})\big{)}\] (15) \[G^{(l)}(\bm{x}) =\bm{W}_{+}^{(l)}G^{(l-1)}(\bm{x})+\bm{W}_{-}^{(l)}F^{(l-1)}(\bm{x})\] (16) \[H^{(l)}(\bm{x}) =\bm{W}_{+}^{(l)}F^{(l-1)}(\bm{x})+\bm{W}_{-}^{(l)}G^{(l-1)}(\bm{x })+\bm{b}^{(l)}\] (17)

while \(\bm{W}^{(l)}=\bm{W}_{+}^{(l)}-\bm{W}_{-}^{(l)}\) where \(\bm{W}_{+}^{(l)}\) and \(\bm{W}_{-}^{(l)}\in\mathbb{R}^{+}\).

**Proposition A.3** (Decision boundary of neural networks, restated).: _The decision boundary for \(L\)-layer neural networks with ReLU activation \(\nu^{(L)}(\bm{x})=0\) is where two tropical monomials, or two arguments of \(\max\), equal (Definition 3.1), as follows:_

\[\mathcal{B}\subseteq\big{\{}\bm{x}:\nu_{j}^{(i)}(\bm{x})=H_{j}^{(i)}(\bm{x}) -G_{j}^{(i)}(\bm{x})=0,\;i\in(1,\ldots,L),\;j\in(1,\ldots,H)\;\text{if}\;i\neq L \;\text{else}\;(1)\big{\}}\]

_where the subscript \(j\) denotes the \(j\)-th neuron, assuming the hidden size of neural networks is \(H\), while the \(L\)-th layer has a single output for rather discussions as an SDF. Remind that tropical operations satisfy the usual laws of arithmetic for recursive consideration._

This implies that piecewise hyperplanes corresponding to the preactivations \(\{\nu_{j}^{(i)}\}\) are the candidates shaping the decision boundary. Each linear region has the same pattern of zero masking by \(\text{ReLU}(\cdot)\), effectively making it linear as an equation of a plane \(\bm{Ax}+\bm{b}=0\) for some \(\bm{A}\) and \(\bm{b}\). In turn, if the masking pattern changes as an input is continually changed, it moves to another linear region. From a geometric point of view, this is why the number of linear regions grows polynomially with the hidden size \(H\) and exponentially with the number of layers \(L\)[18, 27].

## Appendix B Implementation details

### Zero-trailing binarization function

\(\mathrm{B}\) is zero-trailing binarization function, consisting of \(D=3\) bits. The zero-trailing means the number in bits is increasing left-aligned. For example, the number one is "100", while the numbers two and four are "010" and "001", respectively. The number three is "110". From the left, each bit indicates x, y, and z axis in our notation. This convention seems to be widely used in the CUDA implementation in C language. Notice that we use the \(j\) subscript of \(\mathrm{B}_{j}\) to select the axis. Naturally, \(i=0\) and \(i=7\) indicate the opposite two corner points.

### Sorting polygon vertices and HashGrid marks

Algorithm 3 describes the algorithm in Section 4.3, which are sorting polygon vertices implicitly representing the direction of faces. If the viewing direction aligns opposite to the normal, the order of vertices follows a counter-clockwise arrangement. We determine the vertex order based on the normal derived from the Jacobian of the mean of vertices. We identify the counter-clockwise arrangement of vertices using dot product to compute relative angles \(\theta\in[-\pi,\pi]\) and cross product for getting direction. Algorithm 4 describes the algorithm in Section 5, calculating the HashGrid marks from the hyperparameters of a HashGrid.

``` Input: vertices \(\bm{x}_{i}\), normal \(\bm{n}\), \(N=|\{\bm{x}_{i}\}|\)\(\mu=\sum_{i}\bm{x}_{i}/N\) for\(i=0\)to\(N-1\)do \(\bm{v}_{i}=(\bm{x}_{i}-\mu)/\|\bm{x}_{i}-\mu\|_{2}\)\(\bm{c}_{i}=\langle\bm{v}_{i},\;\bm{v}_{0}\rangle\), \(\bm{d}_{i}=\langle\bm{v}_{i}\times\bm{v}_{0},\;\bm{n}\rangle\)\(\theta_{i}=\bm{c}_{i}(2\cdot\bm{1}_{d_{i}\geq 0}-1)+2\cdot\bm{1}_{d_{i}<0}\) endfor return\(\text{ArgSort}(\{-\theta_{i}\})\) ```

**Algorithm 4** HashGrid MarksFurther discussions

### Satisfying the eikonal equation and approximation

Theorem 4.7 handles where the eikonal equation does not make perfect hyperplanes. First, we observed the characteristics of trilinear hypersurfaces as illustrated in Figure 9 in Appendix. Among 64 edge scenarios within a trilinear region, a diagonal plane intersects with the hypersurface in nearly all cases. Moreover, we can take advantage of it since we can eliminate one variable by assuming the curved edge lies on the \(x=z\) plane. In this light, Theorem 4.7 used linear algebra to rearrange two trilinear equations to get Equations (71) and (74). As we mentioned in the manuscript, the new vertices lie on at least two hypersurfaces, and the new edges exist on the same hypersurface, while the eikonal constraint minimizes any associated error. This error is tolerated by the hyperparameter \(\epsilon\)=1e-4 (Section 6) as specified in Definition 3.4.

Figure 5 also suggests empirical evidence. Empirically, the flatness error, derived from the proof of Theorem D.5, is effectively controlled by the learning rate of the eikonal loss. In the experiments, we used 0.01 to give our results, while many other neural implicit surface learning methods were used to exploit a higher learning rate of 0.1 (_e.g._, Yariv et al. [7]). Note that, in practice, the eikonal loss is only applied to near-surface using clamps (if the target position is obviously far from a surface, ignore it; it is wise since we are concerned with the surfaces, not wanting exact SDFs for all space) for effective learning, which is sufficient to our extraction method.

Moreover, the smaller grid size of the _Large_ model (_ref._ Section 6) in Figure 4 views a curved surface in a smaller interval, and it tends to regard it as a plane. From the plotting of _Small_ to _Large_, our CD (colored crosses) is decreasing to zero, consistently with better CE, confirming this speculation.

### Generalization to other positional encodings using trilinear interpolation

We can say that _any encoding using trilinear interpolation (that) we defined can be applied in our method_. Here, the definition is implicitly referred to Definition 4.1 (Trilinear interpolation), where the cubic corners are represented by \(\bm{H}\in\mathbb{R}^{D\times F}\). Since we do not rely on a specific mapping function \(f:\mathrm{x}\rightarrow(\bm{H}_{0},\bm{H}_{1}\ldots\bm{H}_{7})\), we may apply our extraction method to any encoding using trilinear interpolation that we defined. Specifically, InstantNGP [12] used a hashing function to map a corner point to the arbitrary entry of hash tables, allowing gracious collisions with online adaptivity (refer to their Sec. 3), while TensoRF [13] can be interpreted that it used a mapping function to the summation of the multiplications of two vectors from the vector-matrix factorization. Therefore, if we have the full ranks for \(R_{1},R_{2},R_{3}\), in Eqn. 3 in Chen et al. [13], among the VM factorization, which can be equivalently represented by the HashGrid with a sufficient number of hash table sizes.

Then, the discussion should be directed toward their encoding characteristics, which impact our extraction method due to their online adaptivity (HashGrid) or low-rank factorization (TensoRF). When we train an SDF, the optimization to global minima is generally difficult to achieve. Instead, previous works on neural implicit surface learning methods resort to optimizing the SDF on near-surface since we are interested in modeling the zero-set surfaces, allowing some errors for the points that are far from the surfaces. Practically, this is done by defining a loss function using clamps and ignoring far-surface points. For this reason, the online adaptivity of HashGrid or low-rank factorization of TensoRF can exploit this nature: focused learning of the surfaces adaptively allocating their learnable parameters to the near-surface, which means there might be enough room for the regularization by the eikonal equation. The investigation of the efficiency of representing the implicit neural surfaces using various positional encodings is seemingly out of the scope of this work since our main interest lies in the theoretical exposition of the mesh extraction from it and setting up a feasible bridge between theory and practice.

## Appendix D Theoretical proofs

**Lemma D.1**.: _(Nested trilinear interpolation, restated) Let a nested cube be inside of a unit cube, where its positions of the eight corners deviate \(-\bm{a}_{j}\) or \(+\bm{b}_{j}\), such that \(\bm{a}_{j},\bm{b}_{j}\geq 0\), from \(\bm{w}_{j}\) for each dimension \(j\), using the notations of Definition 4.1. The eight corners of the nested cube are the trilinear interpolations of the eight corners of the unit cube. Then, the trilinear interpolation with the unit cube and the nested cube for \(\bm{w}\) is identical._

[MISSING_PAGE_FAIL:18]

As \(t\) is changed from zero to one, the curve starts at \(\bm{P}_{0}\) going toward \(\bm{P}_{1}\) and turns in the direction of \(\bm{P}_{2}\) before arriving at \(\bm{P}_{3}\). Usually, the curve does not pass \(\bm{P}_{1}\) nor \(\bm{P}_{2}\); but these give the directional guide for the curve.

**Lemma D.3** (Hypersurfaces in a trilinear space).: _In a piecewise trilinear region where \(\bm{x}\) lies, hypersurfaces of the piecewise trilinear networks are defined as follows, following the notation of Definition 4.1:_

\[0=\tilde{\nu}(\bm{x})=\sum_{i=0}^{2^{D}-1}\omega(i,\bm{w})\cdot \tilde{\nu}(\bm{x}_{i})\] (30)

_where \(\bm{w}\in[0,1]^{D}\) is the relative position in a grid cell and \(\{\bm{x}_{i}\}\) and \(\{\bm{H}_{i}\}\) are the corresponding corner positions and representations, respectively, in a trilinear space._

Proof.: In a piecewise trilinear region, we can use the linearity of \(\nu\) as follows:

\[\tilde{\nu}(\bm{x})=\big{(}\nu\circ\tau\big{)}(\bm{x}) =\nu\Big{(}\sum_{i=0}^{2^{D}-1}\omega(i,\bm{w})\cdot\bm{H}_{i} \Big{)}\] (31) \[=\sum_{i=0}^{2^{D}-1}\omega(i,\bm{w})\cdot\nu\big{(}\bm{H}_{i} \big{)}\] (32) \[=\sum_{i=0}^{2^{D}-1}\omega(i,\bm{w})\cdot\tilde{\nu}\big{(}\bm{ x}_{i}\big{)}\] (33)

which concludes the proof. 

**Lemma D.4** (Curved edge of two hypersurfaces, restated).: _In a piecewise trilinear region, let an edge \((\bm{x}_{0},\bm{x}_{7})\) be the intersection of two hypersurfaces \(\tilde{\nu}_{i}(\bm{x})=\tilde{\nu}_{j}(\bm{x})=0\), while \(\bm{x}_{0}\) and \(\bm{x}_{7}\) are on the two hypersurfaces. Then, the edge is defined as:_

\[\{\bm{x}:\tau(\bm{x})=(1-t)\tau(\bm{x}_{0})+t\tau(\bm{x}_{7})\quad \text{where}\quad\bm{x}\in\mathbb{R}^{D}\quad\text{and}\quad t\in[0,1]\}.\] (34)

Proof.: By Definition 4.3,

\[\tilde{\nu}_{i}(\bm{x}_{0})=\tilde{\nu}_{i}(\bm{x}_{7})\ \ \Leftrightarrow\ \ \nu_{i}\big{(}\tau(\bm{x}_{0})\big{)}=\nu_{i}\big{(}\tau(\bm{x}_{7})\big{)},\] (35)

while \(\nu_{i}\) is linear in a piecewise trilinear region. Since it similarly goes for \(\nu_{j}\), the intersection line of two hyperplanes \(\nu_{i}\) and \(\nu_{j}\) is \((1-t)\tau(\bm{x}_{0})+t\tau(\bm{x}_{7})\) where \(t\in\mathbb{R}\). In other words, the edge is on the hypersurface \(\nu_{i}\big{(}(1-t)\tau(\bm{x}_{0})+t\tau(\bm{x}_{7})\big{)}=0\) where \(t\in\mathbb{R}\). The edge is defined as follows:

\[\{\bm{x}:\tau(\bm{x})=(1-t)\tau(\bm{x}_{0})+t\tau(\bm{x}_{7})\quad \text{where}\quad\bm{x}\in\mathbb{R}^{D}\quad\text{and}\quad t\in\mathbb{R}\}\] (36)

which concludes the proof. 

Notice that when \(\tau(\bm{x})_{1}+\tau(\bm{x})_{2}+\tau(\bm{x})_{4}=\tau(\bm{x})_{3}+\tau(\bm{x} )_{5}+\tau(\bm{x})_{6}=0\), at least one connecting edge can be identified. This occurs along the diagonal line \(\bm{x}=t\cdot\bm{1}\), where \(t\in[0,1]\). Along this line, \(\tau(\bm{x})\) represents a cubic Bezier curve, as shown in Proposition D.2 with the control points of \(\bm{P}_{1}\) and \(\bm{P}_{2}\) set to zeros, which is the line.

**Theorem D.5** (Hypersurface and eikonal constraint, restated).: _A hypersurface \(\tau(\bm{x})=0\) intersects two points \(\tau(\bm{x}_{0})=\tau(\bm{x}_{7})=0\) while \(\tau(\bm{x}_{1\dots 6})\neq 0\) for the remaining six points. These points form a cube, with \(\bm{x}_{0}\) and \(\bm{x}_{7}\) positioned on the diagonal of the cube. The hypersurface satisfies the eikonal constraint \(\|\nabla\tau(\bm{x})\|_{2}^{2}=1\) for all \(\bm{x}\in[0,1]^{3}\). Then, the hypersurface of \(\tau(\bm{x})=0\) is a plane._Proof.: By the definitions of \(\tau\) in Definition 4.3 and the eikonal constraint,

\[\|\nabla\tau(\bm{x})\|_{2}^{2} =\Big{(}\frac{\partial\tau(\bm{x})}{\partial x}\Big{)}^{2}+\Big{(} \frac{\partial\tau(\bm{x})}{\partial y}\Big{)}^{2}+\Big{(}\frac{\partial\tau( \bm{x})}{\partial z}\Big{)}^{2}=1\] (37) \[\frac{\partial\|\nabla\tau(\bm{x})\|_{2}^{2}}{\partial x} =2\Big{(}\frac{\partial\tau(\bm{x})}{\partial y}\Big{)}\Big{(} \frac{\partial^{2}\tau(\bm{x})}{\partial y\partial x}\Big{)}+2\Big{(}\frac{ \partial\tau(\bm{x})}{\partial z}\Big{)}\Big{(}\frac{\partial^{2}\tau(\bm{x} )}{\partial z\partial x}\Big{)}=0\] (38) \[\frac{\partial\|\nabla\tau(\bm{y})\|_{2}^{2}}{\partial y} =2\Big{(}\frac{\partial\tau(\bm{x})}{\partial x}\Big{)}\Big{(} \frac{\partial^{2}\tau(\bm{x})}{\partial x\partial y}\Big{)}+2\Big{(}\frac{ \partial\tau(\bm{x})}{\partial z}\Big{)}\Big{(}\frac{\partial^{2}\tau(\bm{x} )}{\partial z\partial y}\Big{)}=0\] (39) \[\frac{\partial\|\nabla\tau(\bm{z})\|_{2}^{2}}{\partial z} =2\Big{(}\frac{\partial\tau(\bm{x})}{\partial x}\Big{)}\Big{(} \frac{\partial^{2}\tau(\bm{x})}{\partial x\partial z}\Big{)}+2\Big{(}\frac{ \partial\tau(\bm{x})}{\partial y}\Big{)}\Big{(}\frac{\partial^{2}\tau(\bm{x} )}{\partial y\partial z}\Big{)}=0\] (40)

where

\[\frac{\partial\tau(\bm{x})}{\partial y} =(1-x)\Big{(}(1-z)\big{(}\tau(\bm{x}_{2})-\tau(\bm{x}_{0})\big{)} +z\big{(}\tau(\bm{x}_{6})-\tau(\bm{x}_{4})\big{)}\Big{)}\] \[\qquad+x\Big{(}(1-z)\big{(}\tau(\bm{x}_{3})-\tau(\bm{x}_{1}) \big{)}+z\big{(}\tau(\bm{x}_{7})-\tau(\bm{x}_{5})\big{)}\Big{)}.\] (41)

However, since \(\tau(\bm{x}_{0})=\tau(\bm{x}_{7})=0\) while \(\tau(\bm{x}_{2})\neq 0\) and \(\tau(\bm{x}_{5})\neq 0\), there is no solution \(\tau(\bm{x}_{1\dots 6})\) of \(\frac{\partial\tau(\bm{x})}{\partial y}=0\) for all \(\bm{x}\). Likewise, the same goes for \(\frac{\partial\tau(\bm{x})}{\partial x}\) and \(\frac{\partial\tau(\bm{x})}{\partial z}\). Additionally, the three partial derivatives can be rewritten as follows:

\[bC+cB=aC+cA=aB+bA=0\] (42)

where \(a\neq 0\), \(b\neq 0\), and \(c\neq 0\). By substituting \(B=-bC/c\) and \(A=-aC/c\), we can show that \(-abC/c-abC/c=0\Leftrightarrow C=0\), which makes \(A=B=C=0\). Therefore, we rewrite A, B, and C as follows:

\[\frac{\partial^{2}\tau(\bm{x})}{\partial y\partial z} =(1-x)\big{(}-\tau(\bm{x}_{2})-\tau(\bm{x}_{4})+\tau(\bm{x}_{6}) \big{)}+x\big{(}+\tau(\bm{x}_{1})-\tau(\bm{x}_{3})-\tau(\bm{x}_{5})\big{)}\] (43) \[\frac{\partial^{2}\tau(\bm{x})}{\partial z\partial x} =(1-y)\big{(}-\tau(\bm{x}_{1})-\tau(\bm{x}_{4})+\tau(\bm{x}_{5}) \big{)}+y\big{(}+\tau(\bm{x}_{2})-\tau(\bm{x}_{3})-\tau(\bm{x}_{6})\big{)}\] (44) \[\frac{\partial^{2}\tau(\bm{x})}{\partial x\partial y} =(1-z)\big{(}-\tau(\bm{x}_{1})-\tau(\bm{x}_{2})+\tau(\bm{x}_{3}) \big{)}+z\big{(}+\tau(\bm{x}_{4})-\tau(\bm{x}_{5})-\tau(\bm{x}_{6})\big{)},\] (45)

respectively. The following conditions would satisfy the eikonal constraint:

\[-\tau(\bm{x}_{2})-\tau(\bm{x}_{4})+\tau(\bm{x}_{6}) =0\] (46) \[\tau(\bm{x}_{1})-\tau(\bm{x}_{3})-\tau(\bm{x}_{5}) =0\] (47) \[-\tau(\bm{x}_{1})-\tau(\bm{x}_{4})+\tau(\bm{x}_{5}) =0\] (48) \[+\tau(\bm{x}_{2})-\tau(\bm{x}_{3})-\tau(\bm{x}_{6}) =0\] (49) \[-\tau(\bm{x}_{1})-\tau(\bm{x}_{2})+\tau(\bm{x}_{3}) =0\] (50) \[+\tau(\bm{x}_{4})-\tau(\bm{x}_{5})-\tau(\bm{x}_{6}) =0\] (51)

From Equation (48) and Equation (51),

\[\tau(\bm{x}_{1})=-\tau(\bm{x}_{6})\] (52)

From Equation (47) and Equation (50),

\[\tau(\bm{x}_{2})=-\tau(\bm{x}_{5})\] (53)

From Equation (47) and Equation (48),

\[\tau(\bm{x}_{4})=-\tau(\bm{x}_{3})\] (54)

From Equation (50), Equation (51), and Equation (54),

\[\tau(\bm{x}_{1})+\tau(\bm{x}_{2})+\tau(\bm{x}_{4}) =0\] (55) \[\tau(\bm{x}_{3})+\tau(\bm{x}_{5})+\tau(\bm{x}_{6}) =0\] (56)Using Equation (52), Equation (53), Equation (54), Equation (55), and Equation (56),

\[\tau(\bm{x}) =\big{(}x(1-y)(1-z)-(1-x)yz\big{)}\tau(\bm{x}_{1})\] \[\qquad+\big{(}(1-x)y(1-z)-x(1-y)z\big{)}\tau(\bm{x}_{2})\] \[\qquad+\big{(}(1-x)(1-y)z-xy(1-z)\big{)}\tau(\bm{x}_{4})\] (57) \[=(x+\alpha)\tau(\bm{x}_{1})+(y+\alpha)\tau(\bm{x}_{2})+(z+\alpha) \tau(\bm{x}_{4})\] (58) \[=x\tau(\bm{x}_{1})+y\tau(\bm{x}_{2})+z\tau(\bm{x}_{4})+\alpha \big{(}\tau(\bm{x}_{1})+\tau(\bm{x}_{2})+\tau(\bm{x}_{4})\big{)}\] (59) \[=x\tau(\bm{x}_{1})+y\tau(\bm{x}_{2})+z\tau(\bm{x}_{4})=0\] (60)

where \(\alpha=2xyz-xy-yz-zx\).

This is an equation of a plane where its normal vector is \(\big{(}\tau(\bm{x}_{1}),\tau(\bm{x}_{2}),\tau(\bm{x}_{4})\big{)}\). Again, to satisfy the eikonal constraint,

\[\tau(\bm{x}_{1})^{2}+\tau(\bm{x}_{2})^{2}+\tau(\bm{x}_{4})^{2}=1\] (61) \[\tau(\bm{x}_{3})^{2}+\tau(\bm{x}_{5})^{2}+\tau(\bm{x}_{6})^{2}=1,\] (62)

which concludes the proof. 

**Corollary D.6** (Affine-transformed hypersurface and eikonal constraint, restated).: _Let \(\nu_{0}(\bm{x})\) and \(\nu_{1}(\bm{x})\) be two affine transformations defined by \(\bm{W}_{1}^{\intercal}\bm{x}+\bm{b}_{i}\), \(i\in\{0,1\}\). A hypersurface \(\nu_{0}\big{(}\tau(\bm{x})\big{)}=0\) passing two points \(\nu_{0}\big{(}\tau(\bm{x}_{0})\big{)}=\nu_{0}\big{(}\tau(\bm{x}_{\tau})\big{)}=0\) while \(\nu_{0}\big{(}\tau(\bm{x}_{1\dots 6})\big{)}\neq 0\) satisfies the eikonal constraint \(\|\nabla\nu_{1}\big{(}\tau(\bm{x})\big{)}\|_{2}^{2}=1\) for all \(\bm{x}\in[0,1]^{3}\). Then, the hypersurface is a plane._

Proof.: Similarly to Theorem D.5, we investigate the Jacobian of the eikonal constraint to be zero as follows:

\[\|\nabla\nu_{1}\big{(}\tau(\bm{x})\big{)}\|_{2} =\Big{(}\frac{\partial\nu_{1}}{\partial\tau}\frac{\partial\tau( \bm{x})}{\partial x}\Big{)}^{2}+\Big{(}\frac{\partial\nu_{1}}{\partial\tau} \frac{\partial\tau(\bm{x})}{\partial y}\Big{)}^{2}+\Big{(}\frac{\partial\nu_{ 1}}{\partial\tau}\frac{\partial\tau(\bm{x})}{\partial z}\Big{)}^{2}=1\] (63) \[\frac{\partial\|\nabla\nu_{1}\big{(}\tau(\bm{x})\big{)}\|_{2}}{ \partial x} =2\Big{(}\frac{\partial\nu_{1}}{\partial\tau}\frac{\partial\tau( \bm{x})}{\partial y}\Big{)}\Big{(}\frac{\partial\nu_{1}}{\partial\tau}\frac{ \partial^{2}\tau(\bm{x})}{\partial y\partial x}\Big{)}+2\Big{(}\frac{\partial \nu_{1}}{\partial\tau}\frac{\partial\tau(\bm{x})}{\partial z}\Big{)}\Big{(} \frac{\partial\nu_{1}}{\partial\tau}\frac{\partial^{2}\tau(\bm{x})}{\partial z \partial x}\Big{)}=0.\] (64)

Since \(\partial\nu_{1}/\partial\tau\) is a constant, using Lemma D.3 and replacing \(\nu_{1}\) and \(\tau\) with \(\nu_{1}\circ\nu_{0}^{-1}\) and \(\nu_{0}\circ\tau\), respectively, give us the same constraints for \(\nu_{0}\big{(}\tau(\bm{x}_{1\dots 6})\big{)}\) regardless of \(\nu_{1}\). 

**Theorem D.7** (Intersection of two hypersurfaces and a diagonal plane, restated).: _Let \(\tilde{\nu}_{0}(\bm{x})=0\) and \(\tilde{\nu}_{1}(\bm{x})=0\) be two hypersurfaces passing two points \(\bm{x}_{0}\) and \(\bm{x}_{7}\) such that \(\tilde{\nu}_{0}(\bm{x}_{0})=\tilde{\nu}_{1}(\bm{x}_{0})=0\) and \(\tilde{\nu}_{0}(\bm{x}_{7})=\tilde{\nu}_{1}(\bm{x}_{7})=0\), \(\bm{P}_{i}:=\tilde{\nu}_{0}(\bm{x}_{i})\) and \(\bm{Q}_{i}:=\tilde{\nu}_{1}(\bm{x}_{i})\),_

\[\bm{P}_{\alpha}=\big{[}\bm{P}_{0};\ \ \bm{P}_{1};\ \ \bm{P}_{4};\ \ \bm{P}_{5}\big{]}, \bm{P}_{\beta}=\big{[}\bm{P}_{2};\ \ \bm{P}_{3};\ \ \bm{P}_{6};\ \ \bm{P}_{7}\big{]},\] (65)

_and_

\[\bm{X}=\begin{bmatrix}(1-x)^{2}\\ x(1-x)\\ (1-x)x\\ x^{2}\end{bmatrix}.\] (66)

_Then, \(x\in[0,1]\) of the intersection point of the two hypersurfaces and a diagonal plane of \(x=z\) is the solution of the following quartic equation:_

\[\bm{X}^{\intercal}\big{(}\bm{P}_{\alpha}\bm{Q}_{\beta}^{\intercal}-\bm{P}_{ \beta}\bm{Q}_{\alpha}^{\intercal}\big{)}\bm{X}=0\] (67)

_while_

\[y=\frac{\bm{X}^{\intercal}\bm{P}_{\alpha}}{\bm{X}^{\intercal}(\bm{P}_{\alpha}- \bm{P}_{\beta})}\qquad(\bm{P}_{\alpha}\neq\bm{P}_{\beta}).\] (68)

Proof.: We rearrange \(\tilde{\nu}_{0}(\bm{x})\) using \(x=z\) as follows:

\[\tilde{\nu}_{0}(\bm{x}) =(1-y)\cdot\bm{X}^{\intercal}\big{[}\bm{P}_{0};\ \bm{P}_{1};\ \bm{P}_{4};\ \bm{P}_{5}\big{]}+y \cdot\bm{X}^{\intercal}\big{[}\bm{P}_{2};\ \ \bm{P}_{3};\ \ \bm{P}_{6};\ \bm{P}_{7}\big{]}\] (69) \[=(1-y)\bm{X}^{\intercal}\bm{P}_{\alpha}+y\bm{X}^{\intercal}\bm{P} _{\beta}=0\] (70) \[\Leftrightarrow y =\frac{\bm{X}^{\intercal}\bm{P}_{\alpha}}{\bm{X}^{\intercal}(\bm{P}_{ \alpha}-\bm{P}_{\beta})}\qquad(\bm{P}_{\alpha}\neq\bm{P}_{\beta}).\] (71)In a similar way, we rewrite \(\tilde{\nu}_{1}(\bm{x})\) as follows:

\[\bm{X}^{\intercal}(\bm{P}_{\alpha}-\bm{P}_{\beta})\tilde{\nu}_{1}( \bm{x}) =\bm{X}^{\intercal}\bm{P}_{\alpha}\cdot\bm{X}^{\intercal}\bm{Q}_{ \beta}-\bm{X}^{\intercal}\bm{P}_{\beta}\cdot\bm{X}^{\intercal}\bm{Q}_{\alpha}\] (72) \[=\bm{X}^{\intercal}\bm{P}_{\alpha}\bm{Q}_{\beta}^{\intercal}\bm{ X}-\bm{X}^{\intercal}\bm{P}_{\beta}\bm{Q}_{\alpha}^{\intercal}\bm{X}\] (73) \[=\bm{X}^{\intercal}\big{(}\bm{P}_{\alpha}\bm{Q}_{\beta}^{\intercal }-\bm{P}_{\beta}\bm{Q}_{\alpha}^{\intercal}\big{)}\bm{X}.\] (74)

Notice that this is a _quartic_ equation of \(x\). To find the coefficients of the general form of quadratic equation, we consider a mapping \(\bm{C}\) from the interpolation coefficients \(\tau_{i}\) to polynomial coefficients \(c_{i}\):

\[\begin{bmatrix}c_{0}\\ c_{1}\\ c_{2}\end{bmatrix}=\bm{C}\begin{bmatrix}\tau_{0}\\ \tau_{1}\\ \tau_{2}\\ \tau_{3}\end{bmatrix}=\begin{bmatrix}1&0&0&0\\ -2&1&1&0\\ 1&-1&-1&1\end{bmatrix}\begin{bmatrix}\tau_{0}\\ \tau_{1}\\ \tau_{2}\\ \tau_{3}\end{bmatrix}\] (75)

where \(\tau_{0}(1-x)^{2}+\tau_{1}x(1-x)+\tau_{2}(1-x)x+\tau_{3}x^{2}=c_{0}+c_{1}x+c_{ 2}x^{2}\). Then, the coefficients of the quartic equation of Equation (74) are as follows:

\[\begin{bmatrix}c_{00}&c_{01}&c_{02}\\ c_{10}&c_{11}&c_{12}\\ c_{20}&c_{21}&c_{22}\end{bmatrix}=\bm{C}\big{(}\bm{P}_{\alpha}\bm{Q}_{\beta}^ {\intercal}-\bm{P}_{\beta}\bm{Q}_{\alpha}^{\intercal}\big{)}\bm{C}^{\intercal}\] (76)

where the anti-diagonal sum of the result is the quartic coefficients, _i.e._,

\[c_{00}+(c_{10}+c_{01})x+(c_{20}+c_{11}+c_{02})x^{2}+(c_{21}+c_{12})x^{3}+c_{22 }x^{4}.\] (77)

The solutions of the quartic equation such that \(x\in[0,1]\) are the valid intersection points, and we choose one of them \(x\) if there are more than one solution. Note that we can compute a batch of quartic equations using the parallel computing of the eigenvalue decomposition with Lemma D.8.

We determine \(y\) by Equation (71) using \(x\), and \(z=x\). 

We reproduce the method to find polynomial roots from companion matrix eigenvalues [29].

**Lemma D.8** (Polynomial roots from companion matrix eigenvalues, reproduced).: _The companion matrix of the monic polynomial 7_

Footnote 7: The nonzero coefficient of the highest degree is one. If you are dealing with the equation, you can make it by dividing the nonzero coefficient of the highest degree for both sides.

\[p(x)=c_{0}+c_{1}x+\cdots+c_{n-1}x^{n-1}+x^{n}\] (78)

_is defined as:_

\[C(p)=\begin{bmatrix}0&0&\cdots&0&-c_{0}\\ 1&0&\cdots&0&-c_{1}\\ 0&1&\cdots&0&-c_{2}\\ \vdots&\vdots&\ddots&\vdots&\vdots\\ 0&0&\cdots&1&-c_{n-1}\end{bmatrix}.\] (79)

_The roots of the characteristic polynomial \(p(x)\) are the eigenvalues of \(C(p)\)._Complexity analysis

We provide a detailed exposition of the complexity analysis in Section 3.2 to ensure the comprehensiveness of our work. Notice that our complexity analysis closely aligns with the approach outlined in Berzins [4], as in their Appendix A.

Algorithm 2 operates on the vertices and edges, \(\mathcal{V}\) and \(\mathcal{E}\), respectively. Therefore, our initial focus is on the complexity associated with these inputs. Let \(|\mathcal{V}|\), \(|\mathcal{E}|\), and \(|\hat{\mathcal{E}}|\) (\(\hat{\mathcal{E}}\subset\mathcal{E}\)) be the number of vertices, edges, and splitting edges consisting of polygons (Section 3.2), at the iteration of \(i\) and \(j\).

1. During the initialization step (refer to Section 5), we determine \(|\mathcal{V}|\) and \(|\mathcal{E}|\) to construct a grid in \(\mathbb{R}^{D}\). In the case of having \(M\) HashGrid marks, the quantities are defined as \(M^{D}\) and \(D(M-1)M^{D-1}\), respectively. In case of the large \(M\), we consider vertex and edge pruning for efficient computations inspired by a pruning strategy proposed in Berzins [4]. If, for every future neuron, denoted as \(\phi(i,j)>\phi(i^{*},j^{*})\), the two vertices of an edge exhibit identical signs, the edge can be safely pruned as it will not undergo splitting. We make the assumption of that \(M^{D}<|\mathcal{V}|\) to avoid an excessively dense grid, with a complexity of \(\mathcal{O}(|\mathcal{V}|)\).
2. Evaluate \(\tilde{\nu}(\mathcal{V};i,j)\) with a complexity of \(\mathcal{O}(|\mathcal{V}|)\), assuming negligible inference costs as constants. The intermediate results from the inference can be used for the sign-vectors (Definition 3.4).
3. Identify splitting edges \(\hat{\mathcal{E}}\) by comparing two signs per edge, with a complexity of \(\mathcal{O}(|\mathcal{E}|)\).
4. Obtain \(\bm{P}\) and \(\bm{Q}\) for Theorem 4.7 with a complexity of \(\mathcal{O}(|\hat{\mathcal{E}}|)\) in a similar way.
5. Inference for Theorem 4.7 involves matrix-vector multiplications and the eigenvalue decomposition of a companion matrix for each edge, with a complexity of \(\mathcal{O}(|\hat{\mathcal{E}}|)\). Considering the maximum size of 4 for a quartic equation, this operation is regarded as linear.
6. Find intersecting polygons by pairing two sign-vectors with two common zeros in a shared region of the new vertices resulting from splitting, with a complexity of \(\mathcal{O}(|\hat{\mathcal{E}}|)\).

In the study by Berzins [4], a linear relationship between \(|\mathcal{V}|\) and \(|\mathcal{E}|\) for a mesh is empirically observed (see Figure 8a in Berzins [4]). It is worth to note that the number of splitting edges can be effectively upper-bounded as \(|\hat{\mathcal{E}}|<|\mathcal{E}|D/\phi(i,j)\) using Theorem 5 in [20]. Consequently, all the steps outlined in Algorithm 2 demonstrate a linear dependency on the number of vertices, denoted as \(\mathcal{O}(|\mathcal{V}|)\).

To obtain faces, we sort the vertices within each region based on the Jacobian with respect to the mean of the vertices (see Algorithm 3). Considering triangularization, where each face has three vertices, the complexity remains \(\mathcal{O}(|\mathcal{V}|)\), assuming the negligible cost of calculating the Jacobian, especially when performed in parallel.

[MISSING_PAGE_FAIL:24]

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline \multirow{3}{*}{MC \#} & \multicolumn{3}{c}{Bunny} & \multicolumn{3}{c}{Dragon} & \multicolumn{3}{c}{Happy Buddha} \\ \cline{2-10}  & \(|\mathcal{V}|\downarrow\) & CD \(\downarrow\) & CE \(\uparrow\) & \(|\mathcal{V}|\downarrow\) & CD \(\downarrow\) & CE \(\uparrow\) & \(|\mathcal{V}|\downarrow\) & CD \(\downarrow\) & CE \(\uparrow\) \\ \hline
128 & 38395 & 632 & 4.12 & 25986 & 759 & 5.07 & 18621 & 1110 & 4.84 \\
192 & 87245 & 299 & 3.83 & 58855 & 335 & 5.08 & 42793 & 533 & 4.38 \\
224 & 119000 & 221 & 3.80 & 80470 & 268 & 4.64 & 58357 & 395 & 4.34 \\
256 & 155484 & 182 & 3.53 & 105235 & 226 & 4.21 & 76183 & 322 & 4.07 \\ \hline Ours & 135691 & **151** & **4.87** & 91147 & **157** & **6.99** & 67562 & **229** & **6.46** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Chamfer distance (CD) and chamfer efficiency (CE) for the _Part A_ of the Stanford 3D Scanning repository [31] using the _Large_ model having 4x-resolution of HashGrid compared to the default setting (_ref._ Section 6). The chamfer distance (\(\times\)1e-6) is evaluated using the marching cubes (MC) with 512\({}^{3}\) grid samples as the reference ground truth for all measurements. The results are averaged over three random seeds.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline Method & Sample & Vertices \(\downarrow\) & CD (\(\times\)1e-6) \(\downarrow\) & CE \(\uparrow\) & Time \(\downarrow\) \\ \hline \multirow{3}{*}{Marching Cubes} & 128 & 25986 \(\pm\) & 121 & 759 \(\pm\) & 64 & 5.07 & 0.29 \(\pm\) 0.01 \\  & 192 & 58855 \(\pm\) & 181 & 335 \(\pm\) & 21 & 5.08 & 0.60 \(\pm\) 0.06 \\  & 256 & 105235 \(\pm\) & 452 & 226 \(\pm\) & 17 & 4.21 & 1.16 \(\pm\) 0.04 \\ \multirow{3}{*}{Marching Cubes\({}^{*}\)} & 512 & 424311 \(\pm\) & 1262 & - & - & - & 9.19 \(\pm\) 0.39 \\ \hline Ours & - & 91147 \(\pm\) & 203 & **157**\(\pm\) & 18 & **6.99** & 14.9 \(\pm\) 1.66 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Chamfer distance (CD) and chamfer efficiency (CE) for the _Part B_ of the Stanford 3D Scanning dataset [31] using the _Large_ model as the default setting (_ref._ Section 6).

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Method & Sample & Vertices \(\downarrow\) & AD (\(\circ\)) \(\downarrow\) \\ \hline \multirow{5}{*}{Marching Cubes} & 32 & 1283 \(\pm\) & 89 & 6.97 \(\pm\) 0.45 \\  & 48 & 2967 \(\pm\) & 177 & 5.07 \(\pm\) 0.15 \\  & 56 & 4057 \(\pm\) & 245 & 4.23 \(\pm\) 0.25 \\ \multirow{3}{*}{Marching Cubes} & 64 & 5367 \(\pm\) & 342 & 4.10 \(\pm\) 0.17 \\  & 128 & 21825 \(\pm\) & 1336 & 2.47 \(\pm\) 0.12 \\ \multirow{3}{*}{Marching Cubes\({}^{*}\)} & 192 & 49569 \(\pm\) & 3043 & 1.63 \(\pm\) 0.06 \\  & 224 & 67601 \(\pm\) & 4146 & 1.53 \(\pm\) 0.06 \\ \multirow{3}{*}{Marching Cubes\({}^{*}\)} & 256 & 88492 \(\pm\) & 253 & - \\ \hline Ours & - & 4490 \(\pm\) & 91 & 3.57 \(\pm\) 0.38 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Angular distance (AD) for the Stanford bunny. Similarly to the chamfer distance, the angular distance (\(\circ\)) is also evaluated using the marching cubes with 256\({}^{3}\) grid samples (\(\star\)) as the reference ground truth for all measurements. The standard deviations are provided after the \(\pm\) notation, based on results obtained from three random seeds, which is used in training.

### Mesh simplification using QEM

In Figure 8, Quadric Error Metrics (QEM) [35] is a popular mesh simplification algorithm, it could potentially improve both MC and our method for efficiency. Overall, as shown, the mesh simplification favors our method. Notice that QEM entails a higher computational cost, at least \(O(n\log n)\) where \(n\) is the number of vertices. Additionally, these methods can cause shape deformation or holes, sensitive to hyperparameters. Therefore, QEM may be applied with caution for further mesh simplification, acknowledging the potential for unpredictable side effects. We used MeshLab's Quadric Edge Collapse Decimation with the default settings to halve the number of vertices for each step.

### Comparison with alternate approaches

Marching Cubes (MC) remains the _de facto_ sampling-based method. In contrast, our method is a white-box method grounded in the theoretical understanding of trilinear interpolation and polyhedral complex derivation. Here is our rationale for this evaluation:

1. Analytical approaches for CPWA functions are limited by their inability to effectively manage spectral bias and their exponential computational cost, which struggles with exponentially growing linear regions (infeasible to run our benchmarks.) For example, the time complexity of Analytic Marching [2] is \(\mathcal{O}((n/2)^{2(L-1)}|\mathcal{V}_{\mathcal{P}}|n^{4}L^{2})\), which grows exponentially with the number of layers \(L\) and the width \(n\) of the networks, where \(|\mathcal{V}_{\mathcal{P}}|\) represents the number of vertices per face (Section 4.1 from Lei and Jia [2]). In contrast, our method has a linear time complexity with respect to the number of vertices, as discussed in Appendix E, allowing it to avoid visiting every linear region exponentially growing.
2. We also exclude optimization-based methods as they typically rely on MC or its variants initialization in their pipelines (our method can also be integrated into them), such as MeshSDF [36], Deep Marching Tetrahedra [10], and FlexiCubes [37]. Additionally, these methods result in prolonged computational costs.

We provided both rigorous quantitative Figure 3 and qualitative Figure 6 analyses on Marching Tetrahedra (MT) [9; 10] and Neural Dual Contour (NDC) [32], which serve as alternative representative approaches. MT is a popular variant of MC that utilizes six tetrahedra within each grid to identify intermediate vertices. NDC is a data-driven approach (a pretrained model) that uses Dual Contour. In short, MT efficiently requires SDF values but is less efficient regarding the number of vertices extracted. (MT is better than MC in the same resolution, producing more vertices, though.) NDC faces a generalization issue for a zero-shot setting, producing unsmooth surfaces. We used the code from Deep Marching Tetrahedra [10] for MT and the official code of NDC. And PyMCubes for our modernized MC library.

Figure 8: Chamfer distance for the Bunny with the _Large_ model applying QEM (a mesh simplification). Both benefit from QEM while our method efficiently retains better chamfer distances.

### Error of the learned SDF

In Equation (11) and Figure 5, we confirmed our hypothesis that the eikonal loss induces piecewise linearity in trilinear networks. To further support this, we also present the error analysis of the learned SDF. Although the Chamfer distance virtually evaluates the SDF values based on sampling, concerns may arise regarding noise in the pseudo-ground truth. For this reason, one might wish to verify whether the predicted SDFs converge toward near-zero.

We randomly sampled 100K points on the surface of pretrained networks (the Stanford bunny _Large_ model) and calculated the average of squares for predicted SDF (model's outputs) on the surface. As we expected, the predicted SDFs on the reconstructed surface converge to zero as the weight of the eikonal loss increases, and the number is close to zero \(\approx\) 3e-8 as shown in Table 9. Note that, for a _Large_ model, fine-grained trilinear regions from the dense grid marks (from the more dense multi-resolution grids) allow relatively small errors of 97 \(\times\) 1e-9 without the eikonal loss, although the eikonal loss of 1e-2 further decreases to 31 \(\times\) 1e-9. Note that the stronger eikonal loss of 1e-1 proved ineffective due to over-regularization.

### Effect of model sizes

In Table 10, we explore the ablation study to determine whether model size impacts model fitting, which in turn affects the GT mesh (we used MC256 for the pseudo-ground truth of the _Small_ models). With a smaller model size, the learned mesh becomes simpler due to underfitting. As our evaluation focuses on how accurately the extraction methods capture the underlying zero level-set decision boundary of the networks, the change in CD remains small.

Notice that the model with the number of layers of 3 and the width of 64 has the longest runtime of 2.80, primarily because the number of intermediate vertices exceeds 80K. The time complexity is more sensitive to the number of vertices than to the number of layers or the width, which are only indirect factors. Note that the model capacity of the decoding networks is comparable to InstantNGP [12], where the representational efficiency is driven by the HashGrid.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Layers & Width & Vertices & CD (1e-6) \(\downarrow\) & Time (sec) \(\downarrow\) & Note \\ \hline
2 & 4 & 4565 & 738 & 0.11 & \\
2 & 8 & 4863 & 768 & 0.13 & \\
2 & 16 & 4566 & 734 & 0.12 & \\
2 & 32 & 4577 & 754 & 0.22 & \\
2 & 64 & 4611 & 739 & 0.50 & _InstantNGP_[12] \\ \hline
3 & 4 & 4547 & 666 & 0.12 & \\
3 & 8 & 4661 & 750 & 0.19 & \\
3 & 16 & 4628 & 724 & 0.65 & _Ours_ \\
3 & 32 & 4567 & 754 & 0.57 & \\
3 & 64 & 5588 & 759 & 2.86 & \\ \hline \hline \end{tabular}
\end{table}
Table 10: Ablation study varying the number of _layers_ and the _width_ of networks for the Stanford bunny _Small_ models. And, _Note_ indicates the architecture of the decoding networks.

\begin{table}
\begin{tabular}{c c c} \hline \hline Method & Weight & SE (1e-9) \(\downarrow\) \\ \hline MC64 & 1e-2 & 2308 \\ MC256 & 1e-2 & 66 \\ \hline \multirow{4}{*}{Ours} & 0 & 97 \\  & 1e-4 & 91 \\  & 1e-3 & 68 \\  & 1e-2 & **31** \\ \hline \hline \end{tabular}
\end{table}
Table 9: Error of the learned SDF. _Weight_ stands for the weight of the eikonal loss and _SE_ stands for the squared error of predicted SDFs.

Visualizations

For Figure 6 in the main paper (closer looks at bunny's nose), we want to leave a note about the small facets along the left-side of the nose line in the highlighted region. We assume these narrow facets are embedded in the facets of MC256 (pseudo-ground truth), while MC64 fails to capture the left and right edge of the nose and represents this with more facets cutting these two sides of edges - not aligned with nose lines and rather assign more facets in here.

In Figure 9, we explain the rationale behind adopting the diagonal plane assumption in Theorem 4.7 and Section 4.2. We illustrated 64 edge scenarios within a trilinear region, omitting isomorphic cases.

In Figure 10, we showcase the analytical generation of a normal map from piecewise trilinear networks, utilizing HashGrid [12] and ReLU neural networks. Our approach dynamically assigns vertices based on the learned decision boundary. Notably, the configuration of hash grids influences vertex selection, and each grid can be subdivided into multiple polyhedra for accurate representation of smooth curves.

In Figure 11, we present comparative visualizations of normal maps for marching cubes, demonstrating variations in the number of samplings alongside our method, with the respective vertex counts indicated in parentheses. This analysis highlights the trade-off between detail preservation and computational efficiency, showcasing the optimality of our analytically extracted mesh from decision boundary apex points.

In Figure 12, we compare marching cubes (MC) with 256 and 64 samplings to our method. MC 64 shows limitations in representing pointy areas, while our approach excels with analytical extraction from decision boundary apex points. Adaptive sampling leads to some vertices being in close proximity (10.1% within 1e-3), prompting potential mesh optimization for improved efficiency.

In Figure 13, the gallery of the six Stanford 3D Scanning meshes from the _Large_ models is shown.

Figure 9: We depict the 64 scenarios of the edge within a trilinear region, excluding isomorph cases specifying the count within parentheses. _No edge_ indicates where two diagonal zero corners are not connected. The labels _XY_, _YZ_, and _XZ_ refer to \(x=y\), \(y=z\), and \(z=x\) diagonal planes, respectively, form the shared edge, connecting the two diagonal zero corners, with the hypersurface in the schematic figure. In nearly all cases, a diagonal plane intersects with the hypersurface, except, at most, four cases, depending on the curvature determined by corner values.

Figure 11: Visualizing normal maps for marching cubes varying the number of samplings, and ours, with the number of vertices denoted in parentheses. Low sampling has a risk of omitting sharp object details, while dense sampling results in redundant vertices and faces. As our mesh is analytically extracted from the decision boundary apex points of networks, it represents an optimal choice in balancing detail preservation and efficiency.

Figure 10: The normal map is analytically generated from piecewise trilinear networks, comprising both HashGrid [12] and ReLU neural networks, that have been trained to learn an SDF with the eikonal loss for the Stanford bunny [31]. Our approach dynamically assigns vertices based on the learned decision boundary. One notable observation is that the hash grids influence the selection of vertices. Additionally, each grid can be subdivided into multiple polyhedra to accurately represent smooth curves. The _Small_ (left) and _Large_ (right) models (_ref._ Section 6) are used to learn the SDF.

Figure 12: In a _face-to-face_ comparison, we examine marching cubes with 256 and 64 samplings, denoted as MC 256 and MC 64, respectively, and our method. Notably, MC 64 struggles to accurately represent pointy areas of meshes due to its coarse sampling. In contrast, our approach has an advantage in this aspect, as we analytically extract the mesh from the decision boundary apex points of networks. However, it is worth noting that our adaptive sampling introduces a disadvantage where some vertices are in close proximity; specifically, we observed that 10.1% of vertices are within 1e-3 proximity to others. We believe that optimizing the mesh can significantly improve the parsimony of the vertex count compared with marching cubes.

Figure 13: The extract mesh visualizations of the six from the Stanford 3D Scanning repository using the _Large_ models. From left, Bunny, Dragon, Happy Buddha, Armadillo, Dril Bit, and Lucy.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our main claims made in the abstract and introduction are accurately reflected in theoretical and experimental expositions. Theorem 4.5 and Figure 5 provide the theoretical and experimental supports of the planarity constraint of the eikonal equation, Theorem 4.7 provides a plausible approximation for the curved edge subdivision, and Table 1 shows the consistent chamfer efficiency across multiple meshes from the Stanford 3D Scanning repository, while Figure 4 shows that the higher resolution of HashGrid is favorable to the planarity constraint, achieving lower chamfer distances. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Please refer to Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Every Lemma and Theorem has its proof in the Appendix, noting the assumptions that we used, while they are properly referenced by numbering. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Section 6 provides the hyperparameters we used, Algorithms are provided in the main and the Appendix, while reproducible code can be found in the supplemental material. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The reproducible code can be found in the supplemental material. The Stanford 3D Scanning repository can be freely download via http://graphics.stanford.edu/data/3Dscanrep/. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Section 6 provides the hyperparameters we used, and the reproducible code can be found in the supplemental material. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Table 4 and Tables 5 and 8 in Appendix F.1 provides the standard deviations using three random seeds where learned SDFs may slightly differ. Notice that our algorithm is deterministic for given SDF networks.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Appendix F.1 provides the information on the computer resources. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We conform with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA]Justification: There is no societal impact of the work performed. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This work poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The Stanford 3D Scanning repository (MIT License) is used with a proper citation [31], along with HashGrid [12] for our trilinear module. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL.

* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Not involved in crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Not involved in crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.