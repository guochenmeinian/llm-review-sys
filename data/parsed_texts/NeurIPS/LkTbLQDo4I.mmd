# Optimizing protein fitness using Bi-level Gibbs sampling with Graph-based Smoothing

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The ability to design novel proteins with higher fitness on a given task would be revolutionary for many fields of medicine. However, brute-force search through the combinatorially large space of sequences is infeasible. Prior methods constrain search to a small mutational radius from a reference sequence, but such heuristics drastically limit the design space. Our work seeks to remove the restriction on mutational distance while enabling efficient exploration. We propose **Bi**-level **G**ibbs sampling with **G**raph-based **S**moothing (BiGGS), which uses the gradients of a trained fitness predictor to sample many mutations towards higher fitness. Bi-level Gibbs first samples sequence locations then sequence edits. We introduce graph-based smoothing to remove noisy gradients that lead to false positives. Our method is state-of-the-art in discovering high-fitness proteins with up to 8 mutations from the training set. We study the GFP and AAV design problems, ablations, and baselines to elucidate the results.

## 1 Introduction

In protein design, fitness is loosely defined as performance on a desired property or function. Examples of fitness include catalytic activity for enzymes [1; 20] and fluorescence for biomarkers [27]. Protein engineering seeks to design proteins with high fitness by altering the underlying sequences of amino acids. However, the number of possible proteins increases exponentially with sequence length, rendering it infeasible to perform brute-force search to engineer novel functions which often requires many mutations (i.e. at least 3 [11]). Directed evolution [3] has been successful in improving protein fitness, but it requires substantial labor and time to gradually explore many mutations.

We aim to find shortcuts to generate high-fitness proteins that are many mutations away from what is known but face several challenges. Proteins are notorious for highly non-smooth fitness landscapes:1 fitness can change dramatically with just a single mutation, and most protein sequences have zero fitness [29]. As a result, machine learning (ML) methods are susceptible to learning noisy fitness landscapes with false positives [18] and local optimums [6] which poses problems to optimization and search. The 3D protein structure, if available, can help provide helpful constraints in navigating the noisy fitness landscape, but it cannot be assumed in the majority of cases - current protein folding methods typically cannot predict the effects on structure of point mutations [25].

Footnote 1: Landscape refers to the mapping from sequence to fitness.

Our work proposes a sequence-based method that can optimize over a noisy fitness landscape and efficiently sample large mutational edits. We introduce two methodological advances summarized in Figure 1. The first is Graph-based Smoothing (GS) that regularizes the noisy landscape. We consider it as a noisy graph signal and apply \(L_{1}\) graph Laplacian regularization. This encourages sparsity and local consistency in the landscape; most protein sequences have zero fitness, and similar sequenceshave similar fitness [42]. The effect is a smooth fitness landscape learned by the ML model on which gradients accurately approximate the direction towards high-fitness sequences. To reach high-fitness sequences requiring many mutations, we use the improved gradients in our second advancement, Bi-level Gibbs (BiG), to approximate the proposal distribution in a Gibbs sampling procedure - as inspired by Gibbs with Gradients (GWG) [12]. BiG uses bi-level sampling to propose up to 5 indices to mutate simultaneously. Local improvements from the gradients help select beneficial mutations to guide low-fitness sequences towards higher fitness while sampling allows exploration. Following the intuition of directed evolution, we apply multiple rounds of sampling over clustered sequences in a procedure we call Iterative Extrapolation (IE).

We find BiG and GS are complementary in enabling long-range exploration while avoiding the pitfalls of a noisy fitness landscape; the combination of both is referred to as BiGGS. We introduce a set of tasks using the Green Fluorescent Proteins (GFP) dataset [30] to simulate challenging protein design scenarios by starting with low-fitness sequences that require many (5 or more) mutations to the best fitness. We primarily study GFP because of (1) its difficulty as one of the longest proteins in fitness datasets and (2) its comprehensive fitness measurements of up to 15 mutations. To assess the generalizability of our method, we additionally study the Adeno-Associated Virus (AAV) dataset [7] based on gene delivery fitness. We evaluate BiGGS and prior works on our proposed benchmarks to show that BiGGS is state-of-the-art in GFP and AAV fitness optimization. Our contributions are summarized as follows:

* We develop a novel sequence-based protein fitness optimization algorithm, BiGGS, based on BiG to efficiently sample multiple mutations, GS to regularize the fitness landscape, and IE to progressively mutate towards higher-fitness (Section 2).
* We study GFP by proposing a set of design benchmarks of different difficulty with varying starting sequence distribution (Section 3). While our focus is GFP, we develop benchmarks on AAV to evaluate a new fitness criteria (Appendix C).
* We show BiGGS is state-of-the-art in GFP and AAV fitness optimization while exhibiting diversity and novelty from the training set. We analyze the contributions of BiG and GS towards successful fitness optimization over challenging fitness landscapes (Section 5).

Figure 1: BiGGS overview. **(A)** Protein engineering is often challenged with a noisy fitness landscape on which the starting dataset (unblurred) is a fraction of landscape with the highest fitness sequences hidden (blurred). **(B)** We develop Graph-based Smoothing (GS) to estimate a smoothed fitness landscape from the starting data. Intuitively, the gradients allow extrapolation towards higher fitness sequences. **(C)** A fitness predictor is trained on the smoothed fitness landscape. **(D)** Gradients from the fitness predictor are used in an iterative sampling procedure called Iterative Extrapolation (IE) where Bi-level Gibbs sampling (BiG) is performed on each step with renewed gradient computations. **(E)** Each round of IE samples mutations towards higher fitness.

Method

We begin with the problem formulation in Section 2.1. Our method uses two components bi-level Gibbs sampling (Section 2.2) and graph-based smoothing (Section 2.3). Together they are part of a iterative sampling method called iterative extrapolation (Section 2.4) as a way to progressively extrapolate towards novel sequences. The full algorithm, BiGGS, is presented in Algorithm 1.

### Problem formulation

Let the starting set of length \(L\) protein sequences and their fitness measurements be denoted as \(\mathcal{D}_{0}=(\mathcal{X}_{0},\mathcal{Y}_{0})\) where \(\mathcal{X}_{0}\subset\mathcal{Y}^{L}\) with vocabulary \(\mathcal{V}=\{1,\dots,20\}\) and \(\mathcal{Y}_{0}\subset\mathbb{R}\). We use subscripts to distinguish sequences, \(x_{i}\in\mathcal{V}^{L}\), while a paramettical subscript denotes the token, \((x_{i})_{j}\in\mathcal{V}\) where \(j\in\{1,\dots,L\}\). Note our method can readily be extended to other modalities, e.g. nucleic acids.

For _in-silico_ evaluation, we denote the set of _all_ known sequences and fitness measurements as \(\mathcal{D}^{*}=(\mathcal{X}^{*},\mathcal{Y}^{*})\). We assume there exists a black-box function \(g:\mathcal{V}^{L}\rightarrow\mathbb{R}\) such that \(g(x^{*})=y^{*}\), which is approximated by an oracle \(g_{\phi}\). In practice, the oracle is a model trained with weights \(\phi\) to minimize prediction error on \(\mathcal{D}^{*}\). The starting dataset only includes low fitness sequences and is a strict subset of the oracle dataset \(\mathcal{D}_{0}\subset\mathcal{D}^{*}\) to simulate fitness optimization scenarios. Given \(\mathcal{D}_{0}\), our task is to generate a set of sequences \(\hat{\mathcal{X}}=\{\hat{x}_{i}\}_{i=1}^{N_{\text{sweptics}}}\) with higher fitness than the starting set.

### BiG: Bi-level Gibbs (with Gradients)

To generate new sequences, we propose a modified version of Gibbs With Gradients (GWG) [12]. The first step is to train a fitness _predictor_, \(f_{\theta}:\mathcal{V}^{L}\rightarrow\mathbb{R}\), using \(\mathcal{D}_{0}\) to act as the learned unnormalized probability (i.e. negative energy) from sequence to fitness. We use the Mean-Squared Error (MSE) loss to train the predictor which we parameterize as a deep neural network. We found it beneficial to employ negative data augmentation since both the dataset and the range of fitness values are small. Specifically, we double the size of the dataset by sampling random sequences, \(x_{i}^{\text{reg}}\sim\text{Uniform}(\mathcal{V}^{L})\), and assigning them the lowest possible fitness value, \(\mu\).

Our goal is to sample from \(\log p(x)=f_{\theta}(x)-\log Z\) where \(Z\) is the normalization constant. Higher fitness sequences will be more likely under this distribution while sampling over many mutations will induce diversity and novelty. GWG uses Gibbs sampling with _locally informed proposals_:

\[q^{\nabla}(x^{\prime}|x)\propto e^{\frac{(x^{\prime})^{\top}d_{\theta}(x)}{2} }\mathds{1}(x^{\prime}\in H(x)),\qquad d_{\theta}(x)_{ij}=\nabla_{x}f_{\theta} (x)_{ij}-x_{i}^{T}\nabla f_{\theta}(x)_{i},\] (1)

where \(d_{\theta}(x)_{ij}\) is a first order Taylor approximation of the log-likelihood ratio of mutating the \(i\)th index of \(x\) to token \(j\). Treating \(x,x^{\prime}\) as one-hot, \((x^{\prime})^{\top}d_{\theta}(x)=\sum_{i}(x^{\prime}_{i})^{\top}d_{\theta}(x)_ {i}\) is the sum over the local differences where \(x^{\prime}\) differs from \(x\). The proposal \(q(x^{\prime}|x)\) can be efficiently computed when \(H(\cdot)\) is the 1-Hamming ball2: a single backward pass is needed to compute the Jacobian in eq. (1).

Footnote 2: Defined as a ball using the hamming distance.

Sampling \(M>1\) mutations in the same fashion would require estimating the gradients for each mutation individually resulting in exponentially more computations. Instead, we find a simple bi-level sampling scheme to be effective. The first level samples mutation indices, \(\ell_{m}\), with a categorical tempered-softmax distribution over the column-wise maxima, \(d_{\theta}(x)_{i}=\max_{j\in\{1,\dots,L\}}d_{\theta}(x)_{ij}\). The second level samples token-wise mutations \((x^{\prime})_{\ell_{m}}\) over the vocabulary the same way as the first level using \(d_{\theta}(x)_{\ell_{m}j}\).

\[\begin{split}\text{\bf First level: }&\ell_{m}\stackrel{{ iid}}{{\sim}}q(\cdot|x)= \text{Cat}\left(\text{Softmax}\left(\left\{\frac{d_{\theta}(x)_{i}}{\tau} \right\}_{i=1}^{L}\right)\right),\quad m\in\{1,\dots,M\}\\ \text{\bf Second level: }&(x^{\prime})_{\ell_{m}} \sim q(\cdot|x,\ell_{m})=\text{Cat}\left(\text{Softmax}\left(\left\{\frac{d_{ \theta}(x)_{\ell_{m}j}}{\tau}\right\}_{j=1}^{|\mathcal{V}|}\right)\right) \end{split}\] (2)

where \(\tau\) is a temperature hyperparameter. Indices are sampled \(iid\) which means the same index may get sampled twice. An improvement left for future work is to model conditional dependencies across locations. Each proposed sequence is accepted or rejected using Metropolis-Hasting (MH)

\[\min\left(\exp(f_{\theta}(x^{\prime})-f_{\theta}(x))\frac{\prod_{m=1}^{M}q((x^{ \prime}))_{\ell_{m}}|x,\ell_{m})q(\ell_{m}|x)}{\prod_{m=1}^{M}q((x)_{\ell_{m}}|x^ {\prime},\ell_{m})q(\ell_{m}|x^{\prime})},\ \ 1\right).\] (3)

To summarize, our method Bi-level Gibbs (BiG) first samples \(N_{\text{prop}}\) sequences each with up to \(M\) mutations from eq. (2) then returns a set of accepted sequences, \(\mathcal{X}^{\prime}\), according to eq. (3). Forcing BiG to make \(M\) mutations may make it skip sequences that are less than \(M\) mutations away. We found it best to run BiG over all values leading up to \(M\). The full algorithm is provided in algorithm 2.

A concern is the accuracy of the 1st order Taylor approximation, \(d_{\theta}(x)_{ij}\), for \(M>1\). We observed the performance of BiG is highly dependent on the performance of the predictor for gradients that correlate with higher fitness. The next two sections focus on the development of a robust predictor (Section 2.3) and an iterative framework to improve the Gibbs sampling approximation (Section 2.4).

### GS: Graph-based smoothing

The efficacy of the gradients in BiG to guide sampling towards high fitness sequences depends on the smoothness of the mapping from sequence to fitness learned by the predictor. Unfortunately, the high-dimensional sequence space coupled with few data points and noisy labels results in a noisy predictor that is prone to sampling false positives [18] or getting stuck in local optima [6]. To address this, we use techniques from graph signal processing to smooth the learned mapping by promoting similar sequences to have similar fitness [42] while penalizing noisy predictions [17].

Suppose we have trained a noisy predictor with weights \(\theta_{0}\) on the initial dataset \(\mathcal{D}_{0}\). To construct our graph \(G=(V,E)\), we first construct the nodes \(V\) by iteratively applying pointwise mutations to each sequence in the initial set \(\mathcal{X}_{0}\) to simulate a local landscape around each sequence. We call this routine Perturb with a hyperparameter \(N_{\text{perturb}}\) for the number of perturbations per sequence (see Algorithm 5). The edges, \(E\), are a nearest neighbor graph with \(N_{\text{neigh}}\) neighbors where edge weights are inversely proportional to their sequence distance, \(\omega_{ij}=\omega((v_{i},v_{j}))=1/\texttt{dist}(v_{i},v_{j})\); edge weights are stored in a similarity matrix \(W=\{\omega_{ij}\;\forall v_{i},v_{j}\in V\}\).

The normalized Laplacian matrix of \(G\) is \(\mathcal{L}=I-D^{-1/2}WD^{-1/2}\) where \(I\) is the identity and \(D\) is a diagonal matrix with \(i\)-th diagonal element \(D_{ii}=\sum_{j}\omega_{ij}\). An eigendecomposition of \(\mathcal{L}\) gives \(\mathcal{L}=U\Sigma U^{T}\) where \(\Sigma\) is a diagonal matrix with sorted eigenvalues along the diagonal and \(U\) is a matrix of corresponding eigenvectors along the columns. An equivalent eigendecomposition with symmetric matrix \(B\) (and edges \(E\) arranged into an adjacency matrix) is

\[\mathcal{L}=(\Sigma^{1/2}U^{T})^{T}\Sigma^{1/2}U^{T}=B^{T}B,\qquad B=\Sigma^{1 /2}U^{T}.\]

Next, we formulate smoothing as an optimization problem. For each node, we predict its fitness \(\mathcal{S}=\{f_{\theta_{0}}(v)\;\forall v\in V\}\), also called the graph _signal_, which we assume to have noisy values. Our goal is to solve the following where \(\mathcal{S}\) is arranged as a vector and \(\mathcal{S}^{*}\) is the smoothed signal,

\[\mathcal{S}^{*}=\operatorname*{arg\,min}_{\mathcal{S}}\|B\hat{\mathcal{S}}\|_ {1}+\gamma\|\hat{\mathcal{S}}-\mathcal{S}\|_{1}\] (4)

Equation (4) is a form of graph Laplacian regularization that has been studied for image segmentation with weak labels [17]. \(B\) has eigenvalue weighted eigenvectors as rows. Due to the \(L_{1}\)-norm \(\|B\hat{\mathcal{S}}\|_{1}\) is small if \(\hat{\mathcal{S}}\) is primarily aligned with slowly varying eigenvectors whose eigenvalues are small. This term penalizes large jumps in fitness between neighboring nodes hence we call it _smoothness sparsity constraint_. The second term, \(\|\hat{\mathcal{S}}-\mathcal{S}\|_{1}\), is the _signal sparsity constraint_ that remove noisy predictions with hyperparameter \(\gamma\). The \(L_{1}\)-norm is applied to reflect that most sequences have zero fitness.

At a high level, eq. (4) is solved by introducing auxiliary variables which allows for an approximate solution by solving multiple LASSO regularization problems [34]. Technical details and algorithm are described in Appendix B. Once we have \(\mathcal{S}^{*}\), we retrain our predictor with the smoothed dataset \(\mathcal{D}=(V,\mathcal{S}^{*})\) on which the learned predictor is smoother with gradients much more amenable for gradient-based sampling, BiG. We refer to our smoothing algorithm as Graph-based Smoothing (GS).

### IE: Iterative Extrapolation

The 1st order Taylor approximation of eq. (1) deteriorates the more we mutate from the parent sequence. Inspired by directed evolution [3], we propose to alleviate this by performing multiple rounds of sampling where successive rounds use sequences from the previous round. Each round re-centers the Taylor approximation and extrapolates from the previous round. We first train a predictor \(f_{\theta}\) using GS (Section 2.3). Prior to sampling, we observe the number of sequences may be large and redundant. To reduce the number of sequences, we perform hierarchical clustering [22] and take the sequence of each cluster with the highest fitness using \(f_{\theta}\). Let \(\mathcal{C}\) be the number of clusters.

\[\texttt{Reduce}(\ \{\mathcal{X}^{c}\}_{c=1}^{\mathcal{C}};\theta)=\bigcup_{c=1 }^{\mathcal{C}}\{\operatorname*{arg\,max}_{x\in\mathcal{X}^{c}}f_{\theta}(x)\} \ \text{ where }\ \{\mathcal{X}^{c}\}_{c=1}^{\mathcal{C}}=\texttt{Cluster}(\mathcal{X}; \mathcal{C}).\]

Each round \(r\) reduces the sequences from the previous round and performs BiG sampling.

\[\mathcal{X}^{\prime}_{r+1}=\bigcup_{x\in\tilde{\mathcal{X}}_{r}}\texttt{BiG}(x ;\theta),\quad\tilde{\mathcal{X}}_{r}=\texttt{Reduce}(\{\mathcal{X}^{c}_{r} \}_{c=1}^{\mathcal{C}};\theta),\quad\{\mathcal{X}^{c}_{r}\}_{c=1}^{\mathcal{C }}=\texttt{Cluster}\mathcal{X}^{\prime}_{r+1}(\mathcal{X}^{\prime}_{r}; \mathcal{C}).\]

One cycle of clustering, reducing, and sampling is a round of extrapolation,

\[\mathcal{X}^{\prime}_{r+1}=\texttt{Extrapolate}(\mathcal{X}^{\prime}_{r}; \theta,\mathcal{C})\] (5)

where the initial round \(r=0\) starts with \(\mathcal{X}^{\prime}_{0}=\mathcal{X}_{0}\). After \(R\) rounds, we select our candidate sequences by taking the Top-\(N_{\text{samples}}\) sequences based on ranking with \(f_{\theta}\). We call this procedure Iterative Extrapolation (IE). While IE is related to previous directed evolution methods [31], it differs by taking larger mutational edits on each round with BiG and encouraging diversity by mutating the best sequence of each cluster. The full candidate generation, Bi-level Gibbs with Graph-based Smoothing (BiGGS), with IE is presented in Algorithm 1.

```
0: Starting dataset: \(\mathcal{D}_{0}=(\mathcal{X}_{0},\mathcal{Y}_{0})\)
0: BiG hyperparameters: \(N_{\text{prop}}\), \(\tau\), \(M\)
0: GS hyperparameters: \(N_{\text{neigh}}\), \(N_{\text{perturb}}\), \(\gamma\)
0: IE hyperparameters: \(N_{\text{samples}}\), \(R\), \(\mathcal{C}\)
1: \(\mathcal{D}\leftarrow\mathcal{D}_{0}\cup\{(x^{\text{neg}}_{i},\mu)\}_{i=1}^{ \mathcal{D}_{0}}\)\(\triangleright\) Construct negative data
2: \(\theta_{0}\leftarrow\operatorname*{arg\,max}_{\tilde{\theta}}\mathbb{E}_{(x,y) \sim\mathcal{D}}\left[(y-f_{\tilde{\theta}}(x))^{2}\right]\)\(\triangleright\) Initial training.
3: \(\theta\leftarrow\texttt{Smooth}(\mathcal{X}_{0};\theta_{0})\)\(\triangleright\) GS Algorithm 3.
4: \(\{\mathcal{X}_{0}\}_{c=1}^{\mathcal{C}}\leftarrow\texttt{Cluster}(\mathcal{X}_{0} ;\mathcal{C})\)\(\triangleright\) Initial round of IE
5: \(\tilde{\mathcal{X}}^{c}_{0}\leftarrow\texttt{Reduce}(\{\mathcal{X}_{0}\}_{c=1}^{ \mathcal{C}};\theta)\)\(\triangleright\) BiG algorithm 2
6: \(\mathcal{X}^{\prime}_{0}\leftarrow\cup_{x\in\tilde{\mathcal{X}}^{c}_{0}} \texttt{BiG}(x;\theta)\)\(\triangleright\) Reimating rounds of IE eq. (5)
7:for\(r=1,\ldots,R\)do
8: \(\mathcal{X}^{\prime}_{r}\leftarrow\texttt{Extrapolate}(\mathcal{X}^{\prime}_{r- 1};\theta)\)\(\triangleright\) Remaining rounds of IE eq. (5)
9:endfor
10: \(\tilde{\mathcal{X}}\leftarrow\texttt{TopK}(\cup_{r=1}^{R}\mathcal{X}^{\prime}_{r})\)\(\triangleright\) Return Top-\(N_{\text{samples}}\) sequences based on predicted fitness \(f_{\theta}\).
11:Return\(\tilde{\mathcal{X}}\) ```

**Algorithm 1**BiGGS: Bi-level Gibbs with Graph-based Smoothing

## 3 Benchmarks

We use the Green Fluorescent Protein (GFP) dataset from Sarkisyan et al. [30] containing over 56,806 log fluorescent fitness measurements, with 51,715 unique amino-acid sequences due to _sequences having multiple measurements_. We quantify the difficulty of a protein fitness optimization task by introducing the concept of a _mutational gap_, which we define as the minimum Levenshtein distance between any sequence in the training set to any sequence in the 99th percentile:

\[\texttt{Gap}(\mathcal{X}_{0};\mathcal{X}^{\prime 99\text{th}})=\texttt{min}(\{ \texttt{dist}(x,\tilde{x}):x\in\mathcal{X},\tilde{x}\in\mathcal{X}^{\prime 99\text{th}}\})\]

A mutational gap of 0 means that the training set, \(\mathcal{D}_{0}\) may contain sequences that are in the 99th percentile of fitness. Solving such tasks is easy because methods may sample high-fitness sequences from the training set. Prior work commonly uses the GFP task introduced by design-bench (DB) evaluation framework [36] which has a mutational gap of 0 (see Appendix A). To compare to previous work, we include the DB task as **"easy"** difficulty in our experiments, but we introduce **"medium"** and **"hard"** optimization tasks which have lower starting fitness ranges in the 20-40th and 10-30th percentile of known fitness measurements alongside much higher mutational gaps. Our proposed difficulties are summarized in Table 1 and visualized in Figure 5.

The oracle in design-bench (DB) uses a Transformer-based architecture from Rao et al. [26]. When using this oracle, we noticed a concerning degree of false positives and a thresholding effect of its predictions. We propose a simpler CNN architecture as the oracle that achieves superior performance in terms of Spearman correlation and fewer false positives as seen in Figure 6. Our CNN consists of a 1D convolutional layer that takes in a one-hot encoded sequence, followed by max-pooling and a dense layer to a single node that outputs a scalar value. It uses 256 channels throughout for a total of 157,000 parameters - 15 fold fewer than DB oracle.

Our experiments in Section 5 benchmark on GFP easy, medium, and hard with our CNN oracle. In Appendix C we summarize an additional benchmark using Adeno-Associated Virus (AAV) dataset [7] which focuses on optimizing a 28-amino acid segment for DNA delivery. We use the same task set-up and train our CNN oracle on AAV.

## 4 Related work

**Optimization in protein design.** Approaches in protein design can broadly be categorized in using sequence, structure or both [9]. Advances in structure-based protein design have been driven by a combination of geometric deep learning and generative models [37; 13; 39; 8]. Sequence-based protein design has been explored through the lens of reinforcement learning [2; 16], latent space optimization [32; 16; 19], GFlowNets [14], bayesian optimization [38], generative models [6; 5; 23; 21], and model-based directed evolution [31; 4; 24; 28; 35]. Together they face the common issue of a noisy landscape to optimize over. Moreover, fitness labels are problem-dependent and scarce, apart from well-studied proteins [5]. Our method addresses small amounts of starting data and noisy landscape by regularization with GS. We focus on sequence-based methods where we use locally informed Markov Chain Monte Carlo (MCMC) methods [40] method based on Gibbs With Gradients (GWG) [12] which requires a smooth energy function for strong performance guarantees. Concurrently, Emami et al. [10] used GWG to sample higher fitness sequences by optimizing over a product of experts distribution, a mixture of a protein language model and a fitness predictor. However, they eschewed the need for a smooth energy function which we address with GS.

**Discrete MCMC.** High-dimensional discrete MCMC can be inefficient with slow mixing times. GWG showed discrete MCMC becomes practical by utilizing learned gradients in the sampling distribution, but GWG in its published form was limited to sampling in a proposal window of size 1. Zhang et al. [41] proposed to modify GWG with langevin dynamics to allow for the whole sequence to mutate on every step while Sun et al. [33] augmented GWG with a path auxiliary proposal distribution to propose a series of local moves before accepting or rejecting. We find that BiGGS with bi-level sampling is simpler and effective in achieving a proposal window size beyond 1.

## 5 Experiments

We study the performance of BiGGS on the GFP tasks from Section 3. Furthermore, to ensure that we did not over-optimize to the GFP dataset, we benchmark BiGGS using AAV benchmark in Appendix C. In the subsequent sections, we outline our experiments on GFP, while corresponding results for AAV are in Appendix C. Section 5.1 compares the performance of BiGGS on GFP to a representative set of baselines while Section 5.2 performs ablations on components of BiGGS. Finally, Section 5.3 analyzes BiGGS's performance.

**BiGGS training and sampling.** Following section 3, we use the oracle CNN architecture for our predictor (but trained on different data). To ensure a fair comparison, we use the same predictor across all model-based baselines. We use the following hyperparameters as input to Algorithm 1 across all tasks: \(N_{\text{prop}}=100\), \(\tau=0.01\), \(M=5\), \(N_{\text{neigh}}=500\), \(N_{\text{perturb}}=1000\)\(N_{\text{samples}}=128\)\(R=3\), \(\mathcal{C}=500\). We were unable to perform extensive exploration of hyperparameters. Reducing the number of hyperparameters and finding optimal values is an important future direction. Training is performed with batch size 1024, ADAM optimizer [15] (with \(\beta_{1}=0.9,\beta_{2}=0.999\)), learning rate 0.0001, and 1000 epochs using a single A6000 Nvidia GPU. Initial predictor training takes 10 minutes while graph-based smoothing takes around 30 minutes depending on convergence of the

\begin{table}
\begin{tabular}{l c c c} \hline \hline Difficulty & Range (\%) & \(|\mathcal{D}_{0}|\) & Gap \\ \hline Medium & 20th-40th & 2828 & 6 \\ Hard & 10th-30th & 1636 & 7 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Proposed GFP tasksnumerical solvers. Training with the smoothed data takes 4 to 8 hours. Sampling takes under 30 minutes and can be parallelized.

**Baselines.** We choose a representative set of prior works with publicly available code: GFlowNets (GFN-AL) [14], model-based adaptive sampling (CbAS) [6], greedy search (AdaLead) [31], bayesian optimization with quasi-expected improvement acquisition function (BO-qei) [38], conservative model-based optimization (CoMs) [35], and proximal exploration (PEX) [28].

**Metrics.** Each method generates \(N_{\text{samples}}=128\) samples \(\hat{\mathcal{X}}=\{\hat{x}_{i}\}_{i=1}^{N_{\text{samples}}}\) to evaluate. Here, \(\mathtt{dist}\) is the Levenshtein distance. We report three metrics:

* **(Normalized) Fitness = \(\mathtt{median}(\{\xi(\hat{x}_{i};\mathcal{Y}^{*})\}_{i=1}^{N_{\text{samples}}})\)** where \(\xi(\hat{x};\mathcal{Y}^{*})=\frac{g_{\phi}(\hat{x}_{i})-\mathtt{min}(\mathcal{ Y}^{*})}{\mathtt{max}(\mathcal{Y}^{*})-\mathtt{min}(\mathcal{Y}^{*})}\) is the min-max normalized fitness.
* **Diversity = \(\mathtt{mean}(\{\mathtt{dist}(x,\tilde{x}):x,\tilde{x}\in\hat{\mathcal{X}},x \neq\tilde{x}\})\)** is the average sample similarity.
* **Novelty = \(\mathtt{median}(\{\eta(\hat{x}_{i};\mathcal{X}_{0})\}_{i=1}^{N_{\text{samples}}})\)** where \(\eta(x;\mathcal{X}_{0})=\mathtt{min}(\{\mathtt{dist}(x,\tilde{x}):\tilde{x}\in \mathcal{X}^{*},\tilde{x}\neq x\})\) is the minimum distance of sample \(x\) to any of the starting sequences \(\mathcal{X}_{0}\).

We use median for outlier robustness. Diversity and novelty were introduced in Jain et al. [14]. We emphasize that higher diversity and novelty is _not_ equivalent to better performance. For instance, a random algorithm would achieve maximum diversity and novelty.

### Results

All methods are evaluated on 128 generated candidates, as done in design-bench. We run 5 seeds and report the average metric across all seeds including the standard deviation in parentheses. Results using our GFP oracle are summarized in table 2. Results using the DB oracle are in appendix C.

BiGGS substantially outperforms other baselines on the medium and hard difficulties, consistently navigating the mutational to achieve high fitness, while maintaining diversity and novelty from the training set. The unique extrapolation capabilities of BiGGS on the hardest difficulty level warranted additional analysis, and we investigate this further in Section 5.3. Adalead overall performed second-best, matching the performance of BiGGS on the easy difficulty with PEX only slightly worse. Notably, both Adalead and PEX suffer from a low novelty in the medium and hard settings.

Regarding the other baselines, GFN-AL exhibits subpar performance across all difficulty levels. We were unable to reproduce their published results.3 Its performance notably deteriorates on medium and hard difficulty levels, a trend common amongst all baselines. CbAS explores very far, making on average 46 mutations, resulting in poor fitness. BO-qei is unable to extrapolate beyond the training set, and CoMs presents instability, as indicated by their high standard deviations, and collapse.4

Footnote 3: We contacted the authors but there was no resolution. Lee et al. [16] also were unable to reproduce GFN-AL.

Footnote 4: CoMs managed to generate only between 7 and 65 unique sequences.

We further analyze the distribution of novelty and fitness among CbAS, Adalead, and our method, BiGGS, in Figure 2. Adalead tends to be conservative, while CbAS is excessively liberal. BiGGS, on

\begin{table}
\begin{tabular}{l l l l l l l l l} \hline \hline \multicolumn{2}{c}{GFP Task} & \multicolumn{6}{c}{Method} \\ \cline{2-9} Difficulty & Metric & GFN-AL & CbAS & Adalead & BO-qei & CoMs & PEX & **BiGGS** \\ \hline \multirow{3}{*}{Easy} & Fit. & 0.16 (0.0) & 0.81 (0.0) & **0.92 (0.0)** & 0.77 (0.0) & 0.06 (0.3) & 0.71 (0.0) & **0.92 (0.0)** \\  & Div. & 27.9 (2.0) & 4.5 (0.4) & 2.1 (0.2) & 5.9 (0.0) & 129 (16) & 2.2 (0.1) & 2.2 (0.0) \\  & Nov. & 215 (2.9) & 1.4 (0.5) & 1.0 (0.0) & 0.0 (0.0) & 164 (80) & 1.0 (0.0) & 1.0 (0.0) \\ \hline \multirow{3}{*}{Medium} & Fit. & 0.13 (0.0) & 0.21 (0.0) & 0.53 (0.0) & 0.17 (0.0) & -0.1 (0.0) & 0.51 (0.0) & **0.86 (0.0)** \\  & Div. & 30.9 (2.7) & 9.2 (1.5) & 9.3 (0.1) & 20.1 (7.1) & 142 (15.5) & 2.0 (0.0) & 4.0 (0.2) \\  & Nov. & 214 (3.3) & 7.0 (0.7) & 1.0 (0.0) & 0.0 (0.0) & 190 (10.5) & 1.0 (0.0) & 5.9 (0.2) \\ \hline \multirow{3}{*}{Hard} & Fit. & 0.17 (0.0) & -0.08 (0.0) & 0.03 (0.0) & 0.01 (0.0) & -0.1 (0.2) & -0.11 (0.0) & **0.43 (0.0)** \\  & Div. & 29.3 (2.2) & 98.7 (16) & 6.6 (0.6) & 84.0 (7.1) & 140 (7.1) & 2.0 (0.0) & 4.1 (0.1) \\ \cline{1-1}  & Nov. & 212 (2.0) & 46.2 (9.4) & 1.0 (0.0) & 0.0 (0.0) & 198 (2.9) & 1.0 (0.0) & 7.0 (0.0) \\ \hline \hline \end{tabular}
\end{table}
Table 2: GFP optimization results (our oracle).

the other hand, manages to find the middle ground, displaying high fitness in its samples while also effectively exploring across the mutational gap at each difficulty level.

### Ablations

We perform ablations on each component of BiGGS on the hard difficulty task. In the first ablation, we replace BiG with GWG but use an equivalent number of samples by running \(R=15\) of IE for a fair comparison. The second ablation removes GS and starts sampling after initial predictor training. The last ablation removes iterative extrapolation by setting \(R=1\), \(M=15\), \(N_{\text{sample}}=300\) which maintains the number of samples but without iterative rounds. Our results are shown in Table 3.

We see GS is crucial for BiGGS on the hard difficulty level. Additional analysis is provided in section 5.3. Removing IE also results in a large decrease in performance. Unsurprisingly, GWG greatly benefits from GS and IE due to its similarity with BiG. However, using BiG results in improved fitness. We conclude each component of BiGGS contributes to its performance.

### Analysis

We analyze BiGGS in the hard GFP task and demonstrate that (1) GS results in gradients from BiG that point towards higher fitness sequences and (2) BiG's ability to sample large mutations (\(M\geq 3\)) enables efficient traversal of large mutational distances in a high dimensional space.

Figure 3A, B shows how GS leads to a smooth fitness landscape, enabling BiG to sample high-fitness mutations. Often but not always, GS allows BiGGS to assign high probability to higher-fitness mutations that are low-probability without GS. We use the GFP wildtype (WT) as a representative of high-fitness sequences in the 99th percentile. The smoothed \(f_{\theta}\) (fig. 3A) assigns high probability to the mutation that changes the current residue to the WT residue at a given proposed position, while

\begin{table}
\begin{tabular}{l l|c c c c} \hline \hline Difficulty & Metric & **BiGGS** & with GWG & without IE & without GS \\ \hline \multirow{3}{*}{Hard} & Fitness & **0.43 (0.0)** & 0.38 (0.0) & 0.21 (0.0) & 0.0 (0.0) \\  & Diversity & 4.1 (0.1) & 4.0 (0.1) & 8.3 (0.1) & 18.4 (0.6) \\ \cline{1-1}  & Novelty & 7.0 (0.0) & 7.1 (0.2) & 4.0 (0.0) & 6.0 (0.0) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Ablation results (our oracle).

Figure 2: Comparison of GFP novelty and fitness on samples from AdaLead, BiGGS, and CbAS. From left to right, we observe increasing exploration behaviour from the respective methods. However, only BiGGS maintains high fitness while exploring the novel sequences. Nearly all samples from CbAS on hard are beyond 10 novelty and have very low fitness.

giving low probability to other (lower fitness) mutations. The non-smoothed predictor proposes to mutate the current residue to the WT residue only slightly more often than other mutations (fig. 3B).

In Figure 3C, we show that BiGGS's ability to consider large mutations \((M\geq 3)\) facilitates efficient exploration. We use the oracle to analyze all single \((M=1)\) and quadruple \((M=4)\) mutations accepted during the course of running BiGGS. We choose \(M=4\) as it represents the largest portion of BiGGS-accepted mutations among large mutations \((M\geq 3)\). According to the oracle, the largest quadruple mutation fitness increases are bigger than the largest single mutation fitness increases. Quadruple mutations also result in a greater number of substantial fitness increases. We note a somewhat larger count of substantially negative mutations for quadruple mutations vs. for single mutations. This is expected given BiGGS's stochasticity, and the tendency of large mutations to be more deleterious than small ones. Similar analysis for \(M\) up to 5 is in Appendix D.

## 6 Discussion

In this work, we presented BiGGS, a method for optimizing protein fitness by incorporating ideas from MCMC, graph Laplacian regularization, and directed evolution. We outlined a new benchmark on GFP that introduces the challenge of starting with poor-fitness sequences, many edits from the top fitness sequences. BiGGS discovered higher fitness sequences than in the starting set, even in the hard difficulty of our benchmark where prior methods struggled. We analyzed the two methodological advancements, Graph-based Smoothing (GS) and Bi-level Gibbs (BiG) (which includes Iterative Extrapolation), as well as ablations to conclude each of these techniques aided BiGGS's performance.

There are multiple extensions of BiGGS. The first is to improve BiG by removing the independence assumption across residues and instead modeling joint probabilities of epistatic interactions. One possibility for learning epistatic interactions is to incorporate 3D structure information (if available) to bias the sampling distribution. Secondly, the effectiveness of GS in our ablations warrants additional exploration into better regularization techniques for protein fitness predictors. Our formulation of GS is slow due to the nearest neighbor graph construction and its \(L_{1}\) optimization. Lastly, investigating BiGGS to handle variable length sequences, multiple objectives, and multiple rounds of optimization is of high importance towards real protein engineering problems. Our code is included in the supplementary data and will be publicly available upon acceptance.

## References

* [1] Dave W Anderson, Florian Baier, Gloria Yang, and Nobuhiko Tokuriki. The adaptive landscape of a metallo-enzyme is shaped by environment-dependent epistasis. _Nature Communications_, 12(1):3867, 2021.
* [2] Christof Angermueller, David Dohan, David Belanger, Ramya Deshpande, Kevin Murphy, and Lucy Colwell. Model-based reinforcement learning for biological sequence design. 2020.

Figure 3: Analysis of BiGGS for Hard Task. **(A, B)** Proposed mutation probability of WT residue vs. non-WT residues for subsequently accepted mutations with and without GS. The non-smoothed predictor gives the WT residue only slightly higher probability than other residues. **(C)** Single vs. quadruple mutations accepted by BiGGS. Quadruple mutations lead to more large improvements.

* Arnold [1998] Frances H Arnold. Design by directed evolution. _Accounts of chemical research_, 31(3):125-131, 1998.
* Arnold [2018] Frances H Arnold. Directed evolution: bringing new chemistry to life. _Angewandte Chemie International Edition_, 57(16):4143-4148, 2018.
* Biswas et al. [2021] Surojit Biswas, Grigory Khimulya, Ethan C Alley, Kevin M Esvelt, and George M Church. Low-n protein engineering with data-efficient deep learning. _Nature methods_, 18(4):389-396, 2021.
* Brookes et al. [2019] David Brookes, Hahnbeom Park, and Jennifer Listgarten. Conditioning by adaptive sampling for robust design. In _International conference on machine learning_, pages 773-782. PMLR, 2019.
* Bryant et al. [2021] Drew H Bryant, Ali Bashir, Sam Sinai, Nina K Jain, Pierce J Ogden, Patrick F Riley, George M Church, Lucy J Colwell, and Eric D Kelsic. Deep diversification of an aav capsid protein by machine learning. _Nature Biotechnology_, 39(6):691-696, 2021.
* Dauparas et al. [2022] Justas Dauparas, Ivan Anishchenko, Nathaniel Bennett, Hua Bai, Robert J Ragotte, Lukas F Milles, Basile IM Wicky, Alexis Courbet, Rob J de Haas, Neville Bethel, et al. Robust deep learning-based protein sequence design using proteinmpnn. _Science_, 378(6615):49-56, 2022.
* Ding et al. [2022] Wenze Ding, Kenta Nakai, and Haipeng Gong. Protein design via deep learning. _Briefings in bioinformatics_, 23(3):bac102, 2022.
* Emami et al. [2023] Patrick Emami, Aidan Perreault, Jeffrey Law, David Biagioni, and Peter St John. Plug & play directed evolution of proteins with gradient-based discrete mcmc. _Machine Learning: Science and Technology_, 4(2):025014, 2023.
* Ghafari and Weissman [2019] Mahan Ghafari and Daniel B. Weissman. The expected time to cross extended fitness plateaus. _Theoretical Population Biology_, 129:54-67, 2019. ISSN 0040-5809. doi: https://doi.org/10.1016/j.tpb.2019.03.008. URL https://www.sciencedirect.com/science/article/pii/S0040580918301011. Special issue in honor of Marcus Feldman's 75th birthday.
* Grathwohl et al. [2021] Will Grathwohl, Kevin Swersky, Milad Hashemi, David Duvenaud, and Chris Maddison. Oops i took a gradient: Scalable sampling for discrete distributions. In _International Conference on Machine Learning_, pages 3831-3841. PMLR, 2021.
* Ingraham et al. [2022] John Ingraham, Max Baranov, Zak Costello, Vincent Frappier, Ahmed Ismail, Shan Tie, Wujie Wang, Vincent Xue, Fritz Obermeyer, Andrew Beam, et al. Illuminating protein space with a programmable generative model. _bioRxiv_, pages 2022-12, 2022.
* Jain et al. [2022] Moksh Jain, Emmanuel Bengio, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Bonaventure F. P. Dossou, Chanakya Ajit Ekbote, Jie Fu, Tianyu Zhang, Michael Kilgour, Dinghuai Zhang, Lena Simine, Payel Das, and Yoshua Bengio. Biological sequence design with GFlowNets. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 9786-9801. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/jain22a.html.
* Kingma and Ba [2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* Lee et al. [2017] Minji Lee, Luiz Felipe Vecchietti, Hyunkyu Jung, Hyunjoo Ro, Meeyoung Cha, and Ho Min Kim. Protein sequence design in a latent space via model-based reinforcement learning.
* Lu et al. [2016] Zhiwu Lu, Zhenyong Fu, Tao Xiang, Peng Han, Liwei Wang, and Xin Gao. Learning from weak and noisy labels for semantic segmentation. _IEEE transactions on pattern analysis and machine intelligence_, 39(3):486-500, 2016.
* Madry et al. [2017] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. _arXiv preprint arXiv:1706.06083_, 2017.
* Madry et al. [2018]* Maus et al. [2022] Natalie Maus, Haydn Jones, Juston Moore, Matt J Kusner, John Bradshaw, and Jacob Gardner. Local latent space bayesian optimization over structured inputs. _Advances in Neural Information Processing Systems_, 35:34505-34518, 2022.
* Mazurenko et al. [2019] Stanislav Mazurenko, Zbynek Prokop, and Jiri Damborsky. Machine learning in enzyme engineering. _ACS Catalysis_, 10(2):1210-1223, 2019.
* Meier et al. [2021] Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alex Rives. Language models enable zero-shot prediction of the effects of mutations on protein function. _Advances in Neural Information Processing Systems_, 34:29287-29303, 2021.
* Mullner [2011] Daniel Mullner. Modern hierarchical, agglomerative clustering algorithms. _arXiv preprint arXiv:1109.2378_, 2011.
* Notin et al. [2022] Pascal Notin, Mafalda Dias, Jonathan Frazer, Javier Marchena Hurtado, Aidan N Gomez, Debora Marks, and Yarin Gal. Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval. In _International Conference on Machine Learning_, pages 16990-17017. PMLR, 2022.
* Padmakumar et al. [2023] Vishakh Padmakumar, Richard Yuanzhe Pang, He He, and Ankur P Parikh. Extrapolative controlled sequence generation via iterative refinement. _arXiv preprint arXiv:2303.04562_, 2023.
* Pak et al. [2023] Marina A Pak, Karina A Markhieva, Mariia S Novikova, Dmitry S Petrov, Ilya S Vorobyev, Ekaterina S Maksimova, Fyodor A Kondrashov, and Dmitry N Ivankov. Using alphafold to predict the impact of single mutations on protein stability and function. _Plos one_, 18(3):e0282689, 2023.
* Rao et al. [2019] Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Xi Chen, John Canny, Pieter Abbeel, and Yun S Song. Evaluating protein transfer learning with tape. In _Advances in Neural Information Processing Systems_, 2019.
* Remington [2011] S James Remington. Green fluorescent protein: a perspective. _Protein Science_, 20(9):1509-1519, 2011.
* Ren et al. [2022] Zhizhou Ren, Jiahan Li, Fan Ding, Yuan Zhou, Jianzhu Ma, and Jian Peng. Proximal exploration for model-guided protein sequence design. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 18520-18536. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/ren22a.html.
* Romero and Arnold [2009] Philip A Romero and Frances H Arnold. Exploring protein fitness landscapes by directed evolution. _Nature reviews Molecular cell biology_, 10(12):866-876, 2009.
* Sarkisyan et al. [2016] Karen S Sarkisyan, Dmitry A Bolotin, Margarita V Meer, Dinara R Usmanova, Alexander S Mishin, George V Sharonov, Dmitry N Ivankov, Nina G Bozhanova, Mikhail S Baranov, Onuralp Soylemez, et al. Local fitness landscape of the green fluorescent protein. _Nature_, 533(7603):397-401, 2016.
* Sinai et al. [2020] Sam Sinai, Richard Wang, Alexander Whatley, Stewart Slocum, Elina Locane, and Eric D Kelsic. Adalead: A simple and robust adaptive greedy search algorithm for sequence design. _arXiv preprint arXiv:2010.02141_, 2020.
* Stanton et al. [2022] Samuel Stanton, Wesley Maddox, Nate Gruver, Phillip Maffettone, Emily Delaney, Peyton Greenside, and Andrew Gordon Wilson. Accelerating bayesian optimization for biological sequence design with denoising autoencoders. _arXiv preprint arXiv:2203.12742_, 2022.
* Sun et al. [2022] Haoran Sun, Hanjun Dai, Wei Xia, and Arun Ramamurthy. Path auxiliary proposal for mcmc in discrete space. In _International Conference on Learning Representations_, 2022.
* Tibshirani [1996] Robert Tibshirani. Regression shrinkage and selection via the lasso. _Journal of the Royal Statistical Society: Series B (Methodological)_, 58(1):267-288, 1996.

* [35] Brandon Trabucco, Aviral Kumar, Xinyang Geng, and Sergey Levine. Conservative objective models for effective offline model-based optimization. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 10358-10368. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/trabucco21a.html.
* [36] Brandon Trabucco, Xinyang Geng, Aviral Kumar, and Sergey Levine. Design-bench: Benchmarks for data-driven offline model-based optimization. _CoRR_, abs/2202.08450, 2022. URL https://arxiv.org/abs/2202.08450.
* [37] Joseph L. Watson, David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, Andrew J. Borst, Robert J. Ragotte, Lukas F. Milles, Basile I. M. Wicky, Nikita Hanikel, Samuel J. Pellock, Alexis Courbet, William Sheffler, Jue Wang, Preetham Venkatesh, Isaac Sappington, Susana Vazquez Torres, Anna Lauko, Valentin De Bortoli, Emile Mathieu, Regina Barzilay, Tommi S. Jaakkola, Frank DiMaio, Minkyung Baek, and David Baker. Broadly applicable and accurate protein design by integrating structure prediction networks and diffusion generative models. _bioRxiv_, 2022.
* [38] James T Wilson, Riccardo Moriconi, Frank Hutter, and Marc Peter Deisenroth. The reparameterization trick for acquisition functions. _arXiv preprint arXiv:1712.00424_, 2017.
* [39] Jason Yim, Brian L Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, Regina Barzilay, and Tommi Jaakkola. Se (3) diffusion model with application to protein backbone generation. _arXiv preprint arXiv:2302.02277_, 2023.
* [40] Giacomo Zanella. Informed proposals for local mcmc in discrete spaces. _Journal of the American Statistical Association_, 115(530):852-865, 2020.
* [41] Ruqi Zhang, Xingchao Liu, and Qiang Liu. A langevin-like sampler for discrete distributions. _International Conference on Machine Learning_, 2022.
* [42] Dengyong Zhou, Olivier Bousquet, Thomas Lal, Jason Weston, and Bernhard Scholkopf. Learning with local and global consistency. _Advances in neural information processing systems_, 16, 2003.