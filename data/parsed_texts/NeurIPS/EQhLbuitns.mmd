# HourVideo: 1-Hour Video-Language Understanding

Keshigeyan Chandrasegaran

Agrim Gupta

Lea M. Hadzic

Taran Kota

Jimming He

Cristobal Eyzaguirre

Zane Durante

Manling Li

Jiajun Wu

Li Fei-Fei

hourvideo.stanford.edu

###### Abstract

We present **HourVideo**, a benchmark dataset for hour-long video-language understanding. Our dataset consists of a novel task suite comprising summarization, perception (_recall, tracking_), visual reasoning (_spatial, temporal, predictive, causal, counterfactual_), and navigation (_room-to-room, object retrieval_) tasks. HourVideo includes 500 manually curated egocentric videos from the Ego4D dataset, spanning durations of 20 to 120 minutes, and features **12,976 high-quality, five-way multiple-choice questions**. Benchmarking results reveal that multimodal models, including GPT-4 and LLaVA-NeXT, achieve marginal improvements over random chance. In stark contrast, human experts significantly outperform the state-of-the-art long-context multimodal model, Gemini Pro 1.5 (85.0% vs. 37.3%), highlighting a substantial gap in multimodal capabilities. Our benchmark, evaluation toolkit, prompts, and documentation are available at hourvideo.stanford.edu.

+
Footnote †: Correspondence to {keshik,agrim}@stanford.edu

## 1 Introduction

Humans demonstrate a remarkable ability to process visual stimuli over long time horizons, enabling them to perceive, plan and act in the real world. Consider the routine task of cooking a meal. This activity involves a continuous and adaptive visual process: identifying and using ingredients and tools, monitoring state changes of various dishes, and adjusting cooking duration/techniques based on visual cues such as color and texture. Such sustained visual processing is crucial to achieving the desired culinary outcomes. Naturally, endowing autonomous agents with this capability has been a long-standing goal in the field of Artificial Intelligence.

In recent years, large multimodal models [1; 2; 3] have emerged as a promising approach toward achieving this goal. Typically, these models are evaluated using multiple datasets that test capabilities such as object recognition [4; 5], image comprehension [6; 7; 8], and action recognition [9]. However, these benchmarks are often restricted to single images or short video clips, usually lasting from a few seconds to no more than three minutes [9; 10; 11; 12]. While these benchmarks have spurred significant advancements, a deeper exploration into long-form video-language understanding is essential to develop multimodal systems that can form the basis for future autonomous agents and assistants.

A significant challenge in evaluating long-form video-language understanding capabilities is designing tasks that genuinely necessitate _long-term_ comprehension, i.e., tasks that require long-range dependencies. Merely posing questions that can be answered by watching a brief segment of a lengthy video effectively reduces the task to a combination of temporal localization and short-clip understanding. Furthermore, while intriguing narrative inquiries can certainly be formulated for long-form videos such as television shows and films, it is imperative to ensure that the questions are not trivially answerable due to the vast prior knowledge encoded in modern large language models.

In this work, we introduce **HourVideo**--a benchmark dataset designed for long-form video-language understanding. To design tasks that require _long-term_ comprehension, we first propose a novel tasksuite (Tab. 1), comprising **summarization**, **perception** (_recall_, _tracking_), **visual reasoning** (_spatial_, _temporal_, _predictive_, _causal_, _counterfactual_), and **navigation** (_room-to-room_, _object retrieval_) tasks. For each task, we manually create question prototypes designed to ensure that correctly answering them requires identification and synthesis of information across multiple temporal segments within the long-form videos. Guided by our task suite, we curated 500 egocentric videos from the Ego4D dataset [13]--covering 77 unique everyday activities and ranging from 20 to 120 minutes--to generate questions based on our prototypes. The combination of our comprehensive task suite and everyday mundane egocentric videos provides a robust framework to rigorously evaluate multimodal models' capabilities in understanding long-form videos. Finally, we developed a question-answer generation pipeline utilizing the expertise of trained human annotators (800+ hours of effort) and large language models (LLMs), resulting in a collection of 12,976 high-quality, five-way multiple-choice questions.

We comprehensively evaluate state-of-the-art multimodal models on HourVideo (Tab. 2, Fig. 4), including GPT-4V [2], Gemini 1.5 Pro [3], and LLaVA-NeXT [14] in a zero-shot setting. Our findings reveal that GPT-4V and LLaVA-NeXT achieve only marginal improvements over a random predictor (20%), obtaining accuracies of 25.7% and 22.3%, respectively. Gemini 1.5 Pro, designed specifically for long-context multimodal understanding, obtains an accuracy of 37.3%, which, while better, is still substantially lower than the average performance of human experts at 85.0%. These results suggest that while the multimodal community has made meaningful progress, a significant gap remains to be bridged before these systems can match human-level long-form video understanding capabilities. Progress in long-form video understanding could enable new applications including AR assistants, embodied agents, and interactive video platforms. We hope that HourVideo will serve as a benchmark to facilitate research in this direction and enable the development of multimodal models that can understand endless streams of visual data.

## 2 Benchmark Design and Construction

While open-ended question answering closely emulates human interaction, automating the evaluation of free-form natural language responses remains challenging. Given that our primary goal is to assess long-form video-language understanding capabilities, we opt for a five-way multiple-choice question-answering (MCQ) task. This approach simplifies the evaluation process by allowing to calculate an aggregate question-answering accuracy metric. In the following section, we describe our task suite and question-answer generation pipeline in detail, both of which are designed to curate diverse high-quality five-way multiple-choice questions (MCQs).

### Task Suite

Creating a comprehensive benchmark for long-form video-language understanding is challenging, primarily because formulating meaningful questions that require processing and synthesizing information across various temporal segments is highly nontrivial, even for expert human annotators. Moreover, we note that even benchmarks for image or short video clip understanding are difficult to construct. As a result, we typically observe two common strategies for benchmark creation: (1) pre-defined label spaces testing for a specific skill or within narrow domains (e.g., Kinetics [9] and Something-Something [15]); or (2) gluing together different datasets, each designed to test a specific model capability [16; 17; 18; 19]. In contrast, a single benchmark that can comprehensively test a suite of model capabilities can significantly benefit the research community.

We draw inspiration from both lines of research methodologies and introduce a novel suite of tasks designed to benchmark long-form video-language understanding capabilities for one-hour-long videos. Our task suite encompasses a comprehensive set of perceptual and cognitive tasks, including summarization, perception (recall, tracking), visual reasoning (spatial, temporal, predictive, causal, counterfactual), and navigation (room-to-room, object retrieval) tasks. Our strategy draws inspiration from the two common approaches previously discussed: (1) designing narrowly focused question prototypes to significantly streamline the question-answer creation process, and (2) creating a diverse suite of tasks that holistically evaluate a broad spectrum of multimodal capabilities. Our task suite with manually designed question prototypes are shown in Table 1. In particular, there are 18 sub-tasks in our proposed task suite and example MCQs from HourVideo are shown in Fig. 1.

[MISSING_PAGE_EMPTY:3]

### Dataset Generation Pipeline

In this section, we provide an overview of the question-answer creation pipeline that we developed to create HourVideo. The pipeline is summarized in Fig. 2.

**Video curation, Stage 1.** A crucial design consideration for this benchmark is the selection of video sources and types. We chose the Ego4D [13] dataset for our videos for multiple reasons: (1) its egocentric perspective aligns well with the typical visual input for autonomous agents and assistants; (2) it features extensive visual narrations, which aid in creating diverse multiple-choice questions; and (3) it is readily accessible under the Ego4D license. We manually reviewed 1,470 videos, ranging from 20 to 120 minutes, from the Ego4D dataset, assessing their potential to generate relevant questions for various tasks in our task suite. We engaged five human experts for video curation. Following this process, we curated 500 egocentric videos.

\begin{table}
\begin{tabular}{p{113.8pt} p{284.5pt}} \hline \hline  & **Summarization** \\ \hline
**Key Events/ Objects** & _Summarize the key interactions of the camera wearer in the [supermarket]_. \\ \hline
**Temporal Sequencing** & _Describe the sequence of activities performed by the camera wearer to [prepare the desser1]_. \\ \hline
**Compare/ Contrast** & _How did the camera wearer’s activities in the [apartment] differ from those in the [restaurant]?_ \\ \hline  & **Perception** \\ \hline \hline
**Information Retrieval** & \\ \hline \(\bullet\) Factual Recall & _What [dairy products] did the camera wearer [pick up] in the [supermarket]?_ \\ \hline \(\bullet\) Sequence Recall & _What did the camera wearer do immediately after [weighing tomatoes] at the [supermarket]?_ \\ \hline \(\bullet\) Temporal Distance & _How long after starting to [eat pizza] did the camera wearer [dispose of the pizza box]?_ \\ \hline
**Tracking** & _List the unique [individuals] the camera wearer interacted with at the [dragstore]_. \\ \multicolumn{2}{p{113.8pt}}{**Visual Reasoning**} \\ \hline
**Spatial** & \\ \hline \(\bullet\) Relationship & _Where was the [microwave] placed in relation to the [stove] in the [kitchen]?_ \\ \hline \(\bullet\) Proximity & _Is the [microwave] closer to the [fridge] compared to the [sink]?_ \\ \hline \(\bullet\) Layout & _Which is the correct [IMAGE] depicting the layout of the camera wearer’s [apartment]?_ \\ \hline
**Temporal** & \\ \hline \(\bullet\) Duration & _Which activity did the camera wearer spend more time on: [cooking] or [playing the piano]?_ \\ \hline \(\bullet\) Frequency & _Did the camera wearer use the [circular saw] or [crosscut saw] more frequently to [cut wood]?_ \\ \hline \(\bullet\) Pre-requisites & _What preparation steps did the camera wearer take before [baking cookies]?_ \\ \hline
**Predictive** & _What is the most likely activity the camera wearer will do next after [doing laundry]?_ \\ \hline
**Causal** & _Why did the camera wearer [leave the garage for the second time]?_ \\ \hline
**Counterfactual** & _What if the camera wearer used the [oven] to [cook mashed potatoes]?_ \\ \hline  & **Navigation** \\ \hline
**Room-to-Room** & _How did the camera wearer get from the [building entrance] to the [apartment]?_ \\ \hline
**Object Retrieval** & _How can the camera wearer retrieve the [TV remote] if they are in the [kitchen]?_ \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Our proposed task suite with question prototypes.** This table shows all 4 tasks and 18 sub-tasks proposed in **HourVideo**, along with the corresponding handcrafted question prototypes designed to evaluate long-form video-language understanding capabilities.

**Candidate MCQ Generation, Stage 2.** The objective of this stage is to produce high-quality MCQs for each task, requiring analysis and synthesis of information across multiple temporal segments in a long-form video. Initially, we manually develop question template(s) for each task in the suite. As shown in Table 1, transforming a question template into an actual question involves incorporating video-specific information tailored to the task and template. To facilitate this, we utilize the detailed narrations from the Ego4D dataset, transforming them into a structured format that can be processed by an LLM. Specifically, we segment the video at 20-minute intervals, with each segment's representation including a summary and a list of tools, food items, technology, humans, pets, and physical locations encountered by the camera wearer in the video. We note that synthesizing a structured representation and a question template into a valid question with correct and incorrect answers presents a significant challenge, even for advanced LLMs. Consequently, for each task, we formulate detailed prompts that offer question prototypes, comprehensive instructions, in-context examples, and step-by-step guidance on how to transform a question template into a valid candidate \(\mathtt{MCQ}_{2}\). In total, we developed 25 task-specific prompts.

MCQ **Refinement with LLMs using Human Feedback, Stage 3.** The purpose of this phase is to refine \(\mathtt{MCQ}_{2}\), created in the previous stage. \(\mathtt{MCQ}_{2}\) may contain invalid questions, incorrect answers, trivial incorrect options, and various other issues. We identified that a significant source of these issues stemmed from relying on the noisy narrations in Ego4D. For example, different narrators within the same video could refer to a dishwasher as a "plate rack" or use other terms, and an individual might be described as an "adult," "person with a red and white shirt," "man Y," or "teenager" at various times in the narration. These inconsistencies, combined with our automatic question generation in the first stage, could lead to generation of invalid \(\mathtt{MCQ}\)s. To address noisy \(\mathtt{MCQ}\)s, we implement a human feedback system where trained annotators are tasked with: 1) assessing the validity of each question to ensure it aligns with the video content, 2) verifying the accuracy of the given answer--if found incorrect, they provide the correct answer in free-form text, 3) ensuring that all incorrect options are factually wrong and clearly distinguishable from the correct answer. We gather human feedback for all \(\mathtt{MCQ}_{2}\), involving over 400 hours of human effort. We then design prompts, to automatically refine \(\mathtt{MCQ}_{2}\) using this human feedback to produce \(\mathtt{MCQ}_{3}\). We engaged seven trained annotators in this stage.

**Blind filtering, Stage 4.** Modern LLMs possess extensive prior knowledge and can thus easily answer certain questions without needing to analyze the videos. The objective of this phase is to eliminate questions that can be answered through prior knowledge or can be trivially answered without requiring any information from the video. To address this, we do blind filtering of \(\mathtt{MCQ}_{3}\), utilizing two separate blind LLMs (GPT-4-turbo and GPT-4). Specifically, we exclude any MCQ that is correctly answered by at least one LLM without video input. Although this method may aggressively remove MCQs, it ensures that the remaining \(\mathtt{MCQ}_{4}\) are of high quality and specifically tailored to test long-form video-language understanding.

Figure 2: **Our dataset generation pipeline.** We develop a dataset generation pipeline consisting of five stages to create HourVideo. We leverage over _800 hours of human effort_ in total corresponding to Video curation (Stage 1), \(\mathtt{MCQ}\) Refinement using Human Feedback (Stage 3) and Expert MCQ Refinement (Stage 5) stages. We use LLMs for \(\mathtt{MCQ}\) Generation (Stage 2), \(\mathtt{MCQ}\) Refinement using Human Feedback (Stage 3) and Blind Filtering (Stage 4). We note that causal, counterfactual and navigation questions are manually generated by human experts (See Sec. 2.2 for details).

[MISSING_PAGE_FAIL:6]

Experiments

### Evaluation Protocol

HourVideo includes five-way multiple-choice questions, for which we report accuracies per task and in aggregate across the entire dataset. A significant challenge in evaluating MCQs over long videos is preventing information leakage across questions. Ideally, each MCQ should be evaluated independently to avoid this issue, but unfortunately, this approach is computationally expensive and time-consuming. Therefore, for our evaluation, we assess the questions in batches, with each batch containing all questions related to a specific task or sub-task. For predictive tasks (reasoning), we provide precise timestamps to trim the videos for targeted evaluation. Details on tasks and sub-tasks requiring independent evaluation are provided in Supplementary B.

### Baselines

In this section, we compare the performance of different multimodal models on understanding long videos in a zero-shot setting. Specifically, we evaluate three classes of models: (1) Blind LLMs, (2) Socratic Models [21], and (3) Native multimodal models. All these models operate under a common function \(A=M(V,\tau,Q)\) where \(V,\tau,Q,M,A\) refer to the long-form video input, prompt (instruction), multiple-choice question, multimodal model, and text output respectively.

**Blind LLMs.** Modern LLMs possess extensive prior knowledge, enabling them to easily answer certain questions without the need to analyze videos. Furthermore, it is likely that some questions can be trivially answered by exploiting biases in the question-answer pairs. The 'blind' LLM baseline is designed to evaluate this by asking the LLM to answer the multiple-choice question without considering any visual information from the video, i.e., \(A=M(\tau,Q)\), where \(\tau\) is a generic task-agnostic prompt prepended to the question \(Q\). We use GPT-4 [22] as our LLM for this baseline.

**Socratic Models.** Most current state-of-the-art multimodal models are unable to process very long videos. Therefore, to benchmark these models, we use the Socratic models approach [21]. In this approach, the video \(V\), with a total duration of \(t\) minutes, is segmented into one-minute intervals, each denoted as \(V[i]\) for minute \(i\). Each segment \(V[i]\) is independently captioned, yielding a sequence of captions \(z_{1},z_{2},z_{3},\dots,z_{t}\), where \(z_{i}=\text{Video-Captioner}(V[i])\). These captions are aggregated to form a comprehensive language-based representation of the video, referred to as the world state history, which includes timestamps. This textual representation, along with a generic task-agnostic prompt \(\tau\), serves as the input for long-form video-question answering: \(A=M([\tau,z_{1},z_{2},\dots,z_{t},Q])\). We sample one-minute video clips at a rate of 0.5 fps and a resolution of 512\(\times\)384. We test using both GPT-4 [22] and LLaVA-NeXT-34B-DPO [14] as the Video-Captioner. Finally, we use GPT-4 for actual question answering, as LLaVA-NeXT-34B-DPO does not support the extended context length required to process our world state history.

**Native Multimodal Models.** Multimodal video models, such as Gemini 1.5 Pro [3], are trained _jointly_ on multimodal data, including audio, video, images, and text. These models are particularly adept at handling very long context lengths (2M+), making them ideal for end-to-end evaluation using our benchmark. Evaluating these models is straightforward, as they can directly process hour-long videos as \(A=M(V,\tau,Q)\). For all experiments, we use a sampling rate of 0.5 frames per second, a resolution of 512 \(\times\) 384, and a temperature setting of 0.1.

**Human performance.** Due to the high costs associated with human evaluations, we sampled 14 videos from our benchmark, which included more than 18 scenarios in total including crafting/painting, cooking, construction/renovation, gardening, cleaning/laundry and yard work. We ask three human experts to conduct evaluations on 11.2 hours of video content, encompassing a total of 213 MCQs. To prevent any contamination, we ensured that human experts who evaluated videos were not involved in the annotation of the same videos at any earlier stage (Stages 3 and 5 discussed in Sec. 2). The human experts achieve an accuracy of **85.0%**. The results are shown in Fig. 4.

### Results

We report all task and sub-task level quantitative results in Tab. 2. Qualitative evaluations, including human evaluation numbers, are presented in Fig. 4. We remark that random guessing corresponds to 20% accuracy. Below, we discuss our key observations.

[MISSING_PAGE_FAIL:8]

**Socratic models vs. Native Multimodal Models.** Gemini 1.5 Pro outperforms Socratic models by a considerable margin across all 4 tasks-summarization, perception, visual reasoning, and navigation-indicating that similar models may be promising avenues toward long-form video-language understanding. On aggregate, Gemini 1.5 Pro outperforms the GPT-4-based Socratic model by 11.6%. Despite these significant improvements, it is important to note that Gemini's performance, at 37.3%, still lags significantly behind that of human experts, who achieve 85.0%.

**Independent vs. Task-level MCQ evaluation.** To investigate the validity of our proposed task/sub-task level evaluation method, we conducted an ablation study where each multiple-choice question (MCQ) was evaluated independently. For this, we used 15.9 hours of video and 570 MCQs across 25 randomly selected videos. We used Gemini 1.5 Pro, which demonstrated the highest performance on HourVideo (37.3%). The results and evaluation costs are shown in Tab. 3. There is a minor drop (2.1%) in performance when evaluating each MCQ independently; however, the associated costs increase by more than threefold. These results highlight the efficiency and validity of our proposed task-level/subtask level evaluation method. We will require benchmark submissions to indicate whether they used task-level or individual MCQ evaluation when submitting their results, allowing for greater transparency and comparability between methods.

## 4 Related Work

Dataset Comparison.Existing video benchmarks [23; 24; 25; 26; 27; 28; 29; 30; 31; 32], primarily focus on specific domains or short videos, which limit their ability to assess long-form video understanding comprehensively. Efforts like WebVid10M [33], InternVid [34], and Panda-70M [35] include detailed captions to provide video pretraining data but consist primarily of short video clips less than one minute in length and do not provide QA pairs. Recent works have introduced several benchmarks specifically designed for long video understanding, such as Next-QA [36], Next-GQA [37], VideoChatGPT [38], EgoSchema [12], MovieChat-1K [39] and MovieNet-QA [40]. [41] introduced benchmarks for evaluating relational space-time query tasks. Perception Test [42] proposed a diagnostic benchmark for multimodal models, probing for memory, abstraction, physics, and semantic capabilities using short video clips (23s average duration). However, the average video length in these datasets is still relatively short, with Ego-Schema having an average duration of 3 minutes. In contrast, we focus on hour-long video-language understanding, with videos averaging 45.7 minutes in duration (Table 4) and tasks requiring long-term comprehension.

Video Understanding Tasks.Significant efforts have been made to design tasks appropriate for evaluating multimodal large language models (MLLMs) [43; 44; 45; 46; 47; 48; 49; 50; 51]. The evaluation of vision-language models (VLMs) focuses mainly on visual perception tasks such as image-text matching, retrieval, captioning, object detection, and visual grounding tasks) [45; 46; 47]. Methods revoking around contrastive learning on image-text pairs have proven to be effective methods for learning transferable representations for these visual tasks [52; 53; 54], and have been shown to be effective in more specific domains such as multi-disciplinary scientific understanding [50; 55] and multi-modal mathematical reasoning [48; 49]. Later work has improved upon the visual reasoning capabilities of VLMs [1; 56; 57; 58; 59; 60; 61; 62; 63] and their ability to reason across complex spatio-temporal video data [64; 65; 66; 67; 68; 69; 70; 71]. To better evaluate spatio-temporal abilities, specific benchmarks [12; 72; 73; 31; 72] have been developed. However, the questions in

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Benchmark** & **\# Videos** & **Avg. len. (mins)** & **\# Questions** \\ \hline MSRVTC-QA [23] & 2,990 & 0.25 & 72,821 \\ ActivityNet-QA [11] & 800 & 1.85 & 8,000 \\ TVQA [25] & 2,179 & 0.19 & 15,253 \\ How2QA [26] & 1,166 & 0.25 & 2,852 \\ NExt-QA [36] & 1,000 & 0.66 & 8,564 \\ EgoSchema [12] & 5,063 & 3.0 & 5,063 \\ \hline
**HourVideo** & **500** & **45.7** & **12,976** \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Dataset statistics comparison between video understanding benchmarks.**

\begin{table}
\begin{tabular}{l|c|c|c} \hline  & **Performance** & **Total Tokens** & **Evaluation Cost** \\ \hline Task-level & 38.9\% & 120,818,343 & 8846 \\ \hline Individual & 36.8\% & 374,396,885 & 52621 \\ \hline \end{tabular}
\end{table}
Table 3: Performance and evaluation cost comparison for our proposed task/sub-task level vs. individual MCQ evaluation.

many of these datasets are often not challenging enough to fully evaluate the capabilities of models in understanding long-form video content and can often be answered from only a single frame [74]. In contrast, our benchmark focuses on evaluating the capabilities needed to reason over a significantly longer duration and with more sophisticated reasoning. The questions in our dataset are designed to be highly challenging, with novel video question categories such as navigation highlighting our benchmark's ability to effectively assess the limitations of current state-of-the-art multimodal models in comprehending long-form videos.

Long-Form Video Understanding.To extend video-language models [75, 76, 77, 78, 79, 80, 81, 82, 83, 84] to long-form videos, the main challenge lies in efficiently encoding the temporal and spatial dynamics over a long horizon. One widely used strategy is to maintain a memory bank to store history information in long videos [85, 86, 87, 88, 89, 90, 91]. Alternatively, other methods have been proposed to compact spatio-temporal tokens into a smaller set of compressed or merged tokens to reduce redundancy and alleviate computational burden [92, 93, 81, 94, 95, 96, 97, 98]. Another line of work leverages language as a bridge by first generating textual descriptions for shorter video clips sub-sampled from the longer video and then employing an LLM to aggregate the short captions for longer video understanding [99, 100, 101]. In contrast, approaches like TimeChat [102] and VTimeLLM [103] aim to enhance temporal localization capabilities by encoding timestamp knowledge into visual tokens or using multi-stage training methods. Despite these extensive efforts, long-form video understanding remains a significant challenge for the current generation of multimodal models.

## 5 Conclusion

We introduce **HourVideo**, a novel benchmark dataset designed to rigorously evaluate the capabilities of multimodal models to comprehend one-hour-long videos. Our dataset consists of a novel task suite comprising summarization, perception (_recall, tracking_), visual reasoning (_spatial, temporal, predictive, causal, counterfactual_), and navigation (_room-to-room, object retrieval_) tasks. This benchmark includes 500 egocentric videos from the Ego4D dataset, spanning durations of 20 to 120 minutes, and features 12,976 high-quality five-way multiple-choice questions. Our zero-shot evaluation on HourVideo reveal that multimodal models such as GPT-4V and LLaVA-NeXT exhibit performance levels only slightly better than random guessing. In stark contrast, human expert performance substantially surpasses state-of-the-art long-context multimodal model Gemini 1.5 Pro (85.0% accuracy versus 37.3%), highlighting significant research gap. We aim to establish HourVideo as a benchmark challenge to spur the development of advanced multimodal models capable of truly understanding endless streams of visual data.

Limitations and future work.Despite our substantial efforts to create a high-quality benchmark dataset, there may still be some inconsistencies within the multiple-choice questions. Additionally, while this is currently the largest long-form video-language understanding benchmark of its kind to the best of our knowledge, we acknowledge the need for more holistic benchmarks that include diverse video sources such as sports and YouTube videos. Lastly, we note that incorporating support for the audio modality is essential for more comprehensive evaluation of multimodal models. We also remark that our world extends beyond visual and auditory stimuli to include other sensory modalities (e.g., tactile), suggesting opportunities to explore these additional modalities in future work. We discuss broader impact of HourVideo in Supplementary E.

## Acknowledgments and Disclosure of Funding

This work was in part supported by the Stanford Institute for Human-Centered Artificial Intelligence (HAI), ONR N00014-23-1-2355, and Microsoft. This work was supported by API credit grants from Google DeepMind and OpenAI. We thank Vishal Dharmadhikari for assistance with setting up Gemini 1.5 evaluations, Hashem Elezabi and Canon Grace Pham for help with data curation. We thank Chengshu (Eric) Li and Sanjana Srivastava for discussions on navigation questions, and Michael Poli, Daniel Y Fu, Jing Yu Koh, Stephen Tian, Tristan Thrush and Ngoc-Trung Tran for their feedback on the manuscript. We also thank our reviewers for their comments.

## References

* [1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. _Advances in neural information processing systems_, 35:23716-23736, 2022.
* [2] OpenAI. GPT-4V(ision) system card, 2023.
* [3] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. _arXiv preprint arXiv:2403.05530_, 2024.
* [4] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. _International Journal of Computer Vision_, pages 211-252, 2014.
* [5] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In _European Conference on Computer Vision_, 2014.
* [6] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In _Proceedings of the IEEE international conference on computer vision_, pages 2425-2433, 2015.
* [7] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards VQA models that can read. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019_, 2019.
* [8] Drew A. Hudson and Christopher D. Manning. GQA: A new dataset for real-world visual reasoning and compositional question answering. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019_, pages 6700-6709, 2019.
* [9] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. _arXiv preprint arXiv:1705.06950_, 2017.
* [10] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2018.
* [11] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: A dataset for understanding complex web videos via question answering. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 9127-9134, 2019.
* [12] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: A diagnostic benchmark for very long-form video language understanding. In _NeurIPS_, volume 36, 2024.
* [13] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18995-19012, 2022.
* [14] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: A strong zero-shot video understanding model, April 2024.
* [15] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Frund, Peter Yianilos, Moritz Mueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax, and Roland Memisevic. The "something something" video database for learning and evaluating visual common sense. In _IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017_, 2017.
* [16] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_, 2019.
* [17] Paul Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun Cheng, Jason Wu, Leslie Chen, Peter Wu, Michelle A Lee, Yuke Zhu, et al. Multibench: Multiscale benchmarks for multimodal representation learning. _arXiv preprint arXiv:2107.07502_, 2021.

* [18] Madeline C Schiappa, Shruti Vyas, Hamid Palangi, Yogesh S Rawat, and Vibhav Vineet. Robustness analysis of video-language models against visual and language perturbations. _arXiv preprint arXiv:2207.02159_, 2022.
* [19] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A large-scale study of representation learning with the visual task adaptation benchmark. _arXiv preprint arXiv:1910.04867_, 2019.
* [20] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 2022.
* [21] Andy Zeng, Maria Attarian, brian ichter, Krzysztof Marcin Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael S Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence. Socratic models: Composing zero-shot multimodal reasoning with language. In _The Eleventh International Conference on Learning Representations_, 2023.
* [22] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [23] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In _ACM Multimedia_, 2017.
* [24] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. TGIF-QA: toward spatio-temporal reasoning in visual question answering. In _CVPR_, 2017.
* [25] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara Berg. TVQA: Localized, compositional video question answering. In _EMNLP_, 2018.
* [26] Linjie Li, Jie Lei, Zhe Gan, Licheng Yu, Yen-Chun Chen, Rohit Pillai, Yu Cheng, Luowei Zhou, Xin Eric Wang, William Yang Wang, et al. Value: A multi-task benchmark for video-and-language understanding evaluation. In _35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks_, 2021.
* [27] Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenenbaum, and Chuang Gan. Star: A benchmark for situated reasoning in real-world videos. _arXiv preprint arXiv:2405.09711_, 2024.
* [28] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: A comprehensive multi-modal video understanding benchmark. _ArXiv preprint_, 2023.
* [29] Xiuyuan Chen, Yuan Lin, Yuchen Zhang, and Weiran Huang. Autoeval-video: An automatic benchmark for assessing large vision language models in open-ended video question answering. _ArXiv preprint_, 2023.
* [30] Rohan Myer Krishnan, Zitian Tang, Zhiqiu Yu, and Chen Sun. Spacewalk-18: A benchmark for multimodal and long-form procedural video understanding in novel domains. _arXiv preprint arXiv:2311.18773_, 2023.
* [31] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? _ArXiv preprint_, 2024.
* [32] Ombretta Strafforello, Klamer Schutte, and Jan van Gemert. Are current long-term video understanding datasets long-term? In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops_, pages 2967-2976, October 2023.
* [33] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1728-1738, 2021.
* [34] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: A large-scale video-text dataset for multimodal understanding and generation. _arXiv preprint arXiv:2307.06942_, 2023.

* [35] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. _arXiv preprint arXiv:2402.19479_, 2024.
* [36] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In _CVPR_, pages 9777-9786, 2021.
* [37] Junbin Xiao, Yao Angela, Yicong Li, and Tat-Seng Chua. Can i trust your answer? visually grounded video question answering. In _arXiv_, page preprint, 2023.
* [38] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024)_, 2024.
* [39] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan Lu, Jenq-Neng Hwang, et al. Moviechat: From dense token to sparse memory for long video understanding. _arXiv preprint arXiv:2307.16449_, 2023.
* [40] Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, and Dahua Lin. Movienet: A holistic dataset for movie understanding. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part IV 16_, pages 709-727. Springer, 2020.
* [41] Xitong Yang, Fu-Jen Chu, Matt Feiszli, Raghav Goyal, Lorenzo Torresani, and Du Tran. Relational space-time query in long-form videos. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 6398-6408, June 2023.
* [42] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: A diagnostic benchmark for multimodal video models. _Advances in Neural Information Processing Systems_, 36, 2024.
* [43] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal large language models. _ArXiv preprint_, 2023.
* [44] Chaoyou Fu, Renrui Zhang, Haojia Lin, Zihan Wang, Timin Gao, Yongdong Luo, Yubo Huang, Zhengye Zhang, Longtian Qiu, Gaoxiang Ye, et al. A challenger to gpt-4v? early explorations of gemini in visual expertise. _ArXiv preprint_, 2023.
* [45] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiauw Zheng, et al. Mme: A comprehensive evaluation benchmark for multimodal large language models. _ArXiv preprint_, 2023.
* [46] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. _ArXiv preprint_, 2023.
* [47] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? _ArXiv preprint_, 2023.
* [48] Pan Lu, Hrtik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models. _ArXiv preprint_, 2023.
* [49] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? _ArXiv preprint_, 2024.
* [50] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. _ArXiv preprint_, 2023.
* [51] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Venamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, Karmesh Yadav, Qiyang Li, Ben Newman, Mohit Sharma, Vincent Berges, Shiqi Zhang, Pulkit Agrawal, Yonatan Bisk, Dhruv Batra, Mrinal Kalakrishnan, Franziska Meier, Chris Paxton, Sasha Sax, and Aravind Rajeswaran. Openeqa: Embodied question answering in the era of foundation models. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2024.

* [52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [53] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In _International conference on machine learning_, pages 4904-4916. PMLR, 2021.
* [54] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: bootstrapping language-image pre-training for unified vision-language understanding and generation. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, Proceedings of Machine Learning Research, pages 12888-12900. PMLR, 2022.
* [55] Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. Multimodal arxiv: A dataset for improving scientific comprehension of large vision-language models. _ArXiv preprint_, 2024.
* [56] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _Advances in neural information processing systems_, 36, 2024.
* [57] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In _International conference on machine learning_, pages 19730-19742. PMLR, 2023.
* [58] OpenAI. Gpt-4 technical report, 2023.
* [59] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _ArXiv preprint_, 2023.
* [60] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo: An open-source framework for training large autoregressive vision-language models. _ArXiv preprint_, 2023.
* [61] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, et al. mplug-docowl: Modularized multimodal large language model for document understanding. _ArXiv preprint_, 2023.
* [62] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Hang Yan, et al. Intermlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. _arXiv preprint arXiv:2309.15112_, 2023.
* [63] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al. Intermlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. _ArXiv preprint_, 2024.
* [64] Shuangrui Ding, Rui Qian, and Hongkai Xiong. Dual contrastive learning for spatio-temporal representation. In _Proceedings of the 30th ACM international conference on multimedia_, pages 5649-5658, 2022.
* [65] Shuangrui Ding, Weidi Xie, Yabo Chen, Rui Qian, Xiaopeng Zhang, Hongkai Xiong, and Qi Tian. Motion-inductive self-supervised object discovery in videos. _arXiv preprint arXiv:2210.00221_, 2022.
* [66] Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, and Kaiming He. A large-scale study on unsupervised spatiotemporal representation learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3299-3309, 2021.
* [67] Shuangrui Ding, Rui Qian, Haohang Xu, Dahua Lin, and Hongkai Xiong. Betrayed by attention: A simple yet effective approach for self-supervised video object segmentation. _arXiv preprint arXiv:2311.17893_, 2023.
* [68] Rui Qian, Yuxi Li, Huabin Liu, John See, Shuangrui Ding, Xian Liu, Dian Li, and Weiyao Lin. Enhancing self-supervised video representation learning via multi-level feature optimization. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 7990-8001, 2021.

* [69] Rui Qian, Shuangrui Ding, Xian Liu, and Dahua Lin. Static and dynamic concepts for self-supervised video representation learning. In _European Conference on Computer Vision_, pages 145-164. Springer, 2022.
* [70] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. _Advances in neural information processing systems_, 35:10078-10093, 2022.
* [71] Rui Qian, Shuangrui Ding, Xian Liu, and Dahua Lin. Semantics meets temporal correspondence: Self-supervised object-centric learning in videos. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 16675-16687, 2023.
* [72] Shicheng Li, Lei Li, Shuhuai Ren, Yuanxin Liu, Yi Liu, Rundong Gao, Xu Sun, and Lu Hou. Vitatecs: A diagnostic dataset for temporal concept understanding of video-language models. _ArXiv preprint_, 2023.
* [73] Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan. Videobench: A comprehensive benchmark and toolkit for evaluating video-based large language models. _ArXiv preprint_, 2023.
* [74] Shyamal Buch, Cristobal Eyzaguirre, Adrien Gaidon, Jiajun Wu, Li Fei-Fei, and Juan Carlos Niebles. Revisiting the" video" in video-language understanding. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 2917-2927, 2022.
* [75] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. _ArXiv preprint_, 2023.
* [76] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. _ArXiv preprint_, 2023.
* [77] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. _ArXiv preprint_, 2023.
* [78] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Ming-Hui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei. Valley: Video assistant with large language model enhanced ability. _ArXiv preprint_, 2023.
* [79] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tianbo Ye, Yang Lu, Jenq-Neng Hwang, and Gaoang Wang. Moviechat: From dense token to sparse memory for long video understanding. _ArXiv preprint_, 2023.
* [80] Haogeng Liu, Qihang Fan, Tingkai Liu, Linjie Yang, Yunzhe Tao, Huaibo Huang, Ran He, and Hongxia Yang. Video-teller: Enhancing cross-modal generation with fusion and decoupling. _ArXiv preprint_, 2023.
* [81] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. _ArXiv preprint_, 2023.
* [82] Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. _ArXiv preprint_, 2023.
* [83] Raebyuk Jung, Hyojun Go, Jaehyuk Yi, Jiho Jang, Daniel Kim, Jay Suh, Aiden SJ Lee, Cooper Han, Jae Lee, Jeff Kim, Jin-Young Kim, Junwan Kim, Kyle Park, Lucas Lee, Mars Ha, Minjoon Seo, Abraham Jo, Ed Park, Hassan Kianinejaid, SJ Kim, Tony Moon, Wade Jeong, Andrei Popescu, Esther Kim, EK Yoon, Genie Heo, Henry Choi, Jenna Kang, Kevin Han, Noah Seo, Sunny Nguyen, Ryan Won, Ye Eun Park, Anthony Giuliani, Dave Chung, Hans Yoon, James Le, Jenny Ahn, June Lee, Maninder Saini, Meredith Sanders, Soyoung Lee, Sue Kim, and Travis Couture. Pegasus-v1 technical report. _ArXiv preprint_, 2024.
* [84] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava : Parameter-free llava extension from images to videos for video dense captioning. _ArXiv preprint_, 2024.
* [85] Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming He, Philipp Krahenbuhl, and Ross Girshick. Long-term feature banks for detailed video understanding. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 284-293, 2019.
* [86] Ivana Balazevic, Yuge Shi, Pinelopi Papalampid, Rahma Chaabouni, Skanda Koppula, and Olivier J Henaff. Memory consolidation enables long-context video understanding. _arXiv preprint arXiv:2402.05861_, 2024.

* [87] Chao-Yuan Wu, Yanghao Li, Kartikeya Mangalam, Haoqi Fan, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. Memvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13587-13597, 2022.
* [88] Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo Kim. Video object segmentation using space-time memory networks. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 9226-9235, 2019.
* [89] Ho Kei Cheng and Alexander G Schwing. Xmem: Long-term video object segmentation with an Atkinson-shiftin memory model. In _European Conference on Computer Vision_, pages 640-658. Springer, 2022.
* [90] Jiahao Wang, Guo Chen, Yifei Huang, Limin Wang, and Tong Lu. Memory-and-anticipation transformer for online action understanding. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 13824-13835, 2023.
* [91] Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. Videoagent: A memory-augmented multimodal agent for video understanding. In _European Conference on Computer Vision_, pages 75-92. Springer, 2025.
* [92] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei. Valley: Video assistant with large language model enhanced ability. _arXiv preprint arXiv:2306.07207_, 2023.
* [93] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-Ilava: Learning united visual representation by alignment before projection. _ArXiv preprint_, 2023.
* [94] Suyuan Huang, Haoxin Zhang, Yan Gao, Yao Hu, and Zengchang Qin. From image to video, what do we need in multimodal lms? _arXiv preprint arXiv:2404.11865_, 2024.
* [95] Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal. Self-chained image-language model for video localization and question answering. _Advances in Neural Information Processing Systems_, 36, 2024.
* [96] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. Ma-lmm: Memory-augmented large multimodal model for long-term video understanding. _arXiv preprint arXiv:2404.05726_, 2024.
* [97] Peng Jin, Ryuich Takanobu, Caiwan Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. _arXiv preprint arXiv:2311.08046_, 2023.
* [98] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longylm: Efficient long video understanding via large language models. _arXiv preprint arXiv:2404.03384_, 2024.
* [99] Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, and Gedas Bertasius. Video recap: Recursive captioning of hour-long videos. _arXiv preprint arXiv:2402.13250_, 2024.
* [100] Ce Zhang, Taixi Lu, Md Mohaimul Islam, Ziyang Wang, Shoubin Yu, Mohit Bansal, and Gedas Bertasius. A simple llm framework for long-range video question-answering. _arXiv preprint arXiv:2312.17235_, 2023.
* [101] Shijie Wang, Qi Zhao, Minh Quan Do, Nakul Agarwal, Kwonjoon Lee, and Chen Sun. Vamos: Versatile action models for video understanding. In _European Conference on Computer Vision_, pages 142-160. Springer, 2025.
* [102] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: A time-sensitive multimodal large language model for long video understanding. _ArXiv preprint_, 2023.
* [103] Bin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. Vtimellm: Empower llm to grasp video moments. _ArXiv preprint_, 2023.
* [104] Jiawei Wang, Liping Yuan, and Yuchen Zhang. Tarsier: Recipes for training and evaluating large video description models. _arXiv preprint arXiv:2407.00634_, 2024.

### HourVideo Supplementary Material

* Section A : HourVideo Release v1.0
* Section B : Data Generation Pipeline: Additional details
* Section B.1 : Prompt Design
* Section B.2 : Narration Compilation Details
* Section B.3 : Human Feedback and Expert Refinement Details
* Section C : Evaluation details
* Section D : Additional Experiments
* Section D.1 : Additional Baselines
* Section D.2 : Model Refusal Rates
* Section E : Broader Impact

## Appendix A HourVideo Release v1.0

We are releasing HourVideo v1.0, our proposed benchmark dataset for one-hour video-language understanding. The benchmark dataset is provided as a single JSON file for ease of use and for straightforward integration with existing benchmarking pipelines. For each video, the dataset includes metadata and contains multiple-choice questions covering multiple tasks from our proposed task suite. Each task is accompanied by a set of multiple-choice questions, each with five possible answers. For predictive visual reasoning tasks, relevant timestamps are provided to allow precise video trimming. Additionally, a PyTorch dataloader is provided to efficiently load the video and the benchmark dataset. We provide all the 500 video_uids used in our benchmark, and users can simply download the corresponding videos from the Ego4D website after reviewing and accepting the Ego4D license agreement. We provide 2 sample videos with annotations from HourVideo. All materials are available at hourvideo.stanford.edu.

* **Structure**: HourVideo v1.0 release is organized as follows :
* HourVideo_v1_0.json: Contains all 12976 questions in the benchmark dataset.
* navigation_task_images/: Contains all images which are part of the navigation task.
* spatial_layout_task_images/: Contains all images which are part of the spatial layout (reasoning/spatial) task.
* sample_annotations/: Given that **HourVideo*
* is an evaluation benchmark, ground truth annotations will not be released to public. For review purposes, we provide ground truth annotations for 2 sample videos as csv files.
* csv/: We provide the benchmark in individual csv files for each video to enhance accessibility, allowing users to conveniently view the contents for each video separately.
* video_utils.py: A script for video processing functionalities.
* hourvideo_dataloader.py: A PyTorch DataLoader script designed to efficiently load and preprocess the dataset.
* baselines/: Contains all prompts and code for captioning/ question answering for Blind LLMs, Socratic models and Multimodal Video Models. **Remark:*
* Except for LLaVA-NeXT-34B-DPO captioning experiments, all other experiments require access to proprietary models including GPT-4 and Gemini 1.5 Pro.
* **Documentation**: We provide a comprehensive datasheet explaining the benchmark dataset's purpose and intended usage.
* **License**: HourVideo will be made publicly available under Apache 2.0 License. Do note that Ego4D videos are publicly available under the Ego4D License [13].
* **Versioning and Updates**: We will maintain HourVideo, with all updates and new versions announced publicly.
* **Contact Information**: For additional inquiries, please contact keshik@stanford.edu.

Data Generation Pipeline: Additional details

### Prompt Design

We meticulously designed 25 prompts in total for tasks/ sub-tasks in our proposed task suite. For 9 out of 15 tasks, we generate questions first, followed by jointly generating answers and wrong answers. For the predictive visual reasoning and temporal pre-requisites tasks, we jointly generate questions and answers first, followed by generating wrong answers. For causal, counterfactual, spatial layout and navigation tasks, we generate questions, answers and wrong answers manually. We also designed prompts for narration compilation (See Fig. E.1) and paraphrasing answers for the summarization, temporal pre-requisites, and predictive visual reasoning tasks. We show the exact prompts used for the following tasks for question-answer generation (Stage 2): \(\bullet\) Narration compilation (Fig. E.1), \(\bullet\) Summarization (Fig. E.2, E.3), \(\bullet\) Perception/information_retrieval/factual_recall (Fig. E.4, E.5), and \(\bullet\) Visual_reasoning/spatial/proximity (Fig. E.6, E.7). For more details, refer to hourvideo.stanford.edu.

### Narration Compilation Details

We segment all our videos at 20 minute intervals and extract a semi-structured representation which includes title, description, start_identifier, end_identifier, list of tools, list of food items, list of technology objects, list of humans interacted, list of pets interacted and list of unique locations in the video segment.

Finally, these segments are compiled by a LLM to form a single structured representation for each video. The prompt is shown in Fig. E.1. Considering that Ego4D offers two independently collected sets of narrations for each video, we select the narration set with the higher token count. This design choice is based on our empirical observation that a larger number of tokens typically ensures more comprehensive coverage of visual elements. These results are shown in Fig. B.1.

### Human Feedback and Expert Refinement Details

For MCQ Refinement with Large Language Models using Human Feedback (Stage 3), we engaged seven annotators who had been trained to provide human feedback based on examples created by our team. Continuous quality assessments were conducted throughout this stage to ensure the integrity and high quality of the feedback obtained for MCQ Refinement. More than 400 hours of human effort were spent in this stage. For _Expert Refinement (Stage 5)_, we engaged four human experts dedicating over 250 hours of human effort for _QAW Refinement_.

## Appendix C Evaluation details

**Evaluation Protocol.** As discussed in Sect. 3, we have developed an evaluation protocol that assesses multimodal models at the level of individual tasks and sub-tasks. The specific tasks and sub-tasks requiring independent evaluation are listed in Tab. C.1.

This structured approach minimizes information leakage across questions and mitigates the substantial costs associated with individual MCQ evaluation. It is important to note that the costs of individual MCQ evaluation are proportional to the number of questions, emphasizing the need for our proposed assessment strategy.

Figure B.1: This plot shows visual elements coverage vs. total number of narration tokens. We use collection of objects in ImageNet-21K, VisualGenome, Tencent1M and Places365 to quantify visual coverage. We use Tiktoken library to calculate the total number of tokens. We used Ego4D dataset [13] to perform this experiment.

[MISSING_PAGE_EMPTY:19]

### Model Refusal Rates

Proprietary models, such as GPT-4 and Gemini 1.5 Pro can abstain from responding to MCQs for various reasons, including video content filtering, privacy concerns, and other undisclosed factors. In particular, we observed that the model refusal rates were significantly higher for Gemini 1.5 Pro compared to GPT-4. For Socratic models, both GPT-4 and LLaVA-34B-DPO models successfully caption more than 96% of the 1-min segments. We report refusal rates for question-answering in Tab. D.2.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Model** & **Videos/\(\mathtt{MCQs}\) answered** & **Refusal rate** \\ \hline GPT-4 (Blind) & 500 / 12,930 & 0.35\% \\ GPT-4 (Socratic) & 500 / 12,959 & 0.13\% \\ LLaVA-34B-DPO (Socratic) & 500 / 12,953 & 0.18\% \\ Gemini 1.5 Pro & 445 / 10,842 & 16.45\% \\ \hline \hline \end{tabular}
\end{table}
Table D.2: Model refusal rates: We report refusal rates for various models for 500 videos / 12,976 MCQs. For Socratic LLMs, we report the refusal rates for question answering. The refusal rate for Gemini 1.5 Pro is significantly higher compared to GPT-4.

\begin{table}
\begin{tabular}{l|c|c|c|c|} \hline \hline  & **Summarization** & **Perception** & **Visual Reasoning** & **Navigation** & **Avg.** \\ \hline Blind LLMs & & & & \\ \hline GPT-4 & 24.4 & 20.0 & 19.1 & 17.6 & 19.6 \\ \hline Socratic Models & & & & \\ LLaVA-NeXT-34B & 34.6 & 26.7 & 19.1 & 21.8 & 22.3 \\ GPT-4 & 41.0 & 29.4 & 22.8 & 24.0 & 25.7 \\ \hline Multimodal Models & & & & \\ Gemini 1.5 Pro & 55.8 & 38.2 & 35.7 & 28.1 & 37.3 \\ \hline SOTA short-form video model & & & & \\ Tarsier-7B (16 frames) & 32.2 & 24.7 & 27.4 & 17.9 & 26.7 \\ \hline \hline \end{tabular}
\end{table}
Table D.1: **Additonal results on HourVideo using Tarsier-7B [104].** Tarsier-7B (16 frames) performance is comparable to Socratic LLMs. We remark that the Ego4D [13] dataset is used in the pre-training stage of the Tarsier model for video captioning.

\begin{table}
\begin{tabular}{l|c|c} \hline \hline
**Parent Task** & **Sub-task (if any)** & **Node (if any)** \\ \hline Summarization & & & Factual Recall \\ \cline{3-3} Perception & Information Retrieval & Sequence Recall \\  & & Temporal Distance \\ \hline Perception & Tracking & & \\ \hline Visual Reasoning & & Relationship \\  & Spatial & Proximity \\  & & Layout \\ \cline{2-3}  & & Duration \\  & Temporal & Frequency \\  & Pre-requisites \\ \cline{2-3}  & Predictive & & \\  & Causal & \\  & Counterfactual & \\ \hline Navigation & Room-to-Room & & \\  & Object Retrieval & & \\  & Room-to-Room (Image-based) & & \\  & Object Retrieval (Image-based) & & \\ \hline \end{tabular}
\end{table}
Table C.1: **The table shows our proposed evaluation protocol**. Tasks and sub-tasks requiring independent evaluation are highlighted.

\begin{table}
\begin{tabular}{l|c|c|c|c|c} \hline  & **Summarization** & **Perception** & **Visual Reasoning** & **Navigation** & **Avg.** \\ \hline Blind LLMs & & & & \\ GPT-4 & 24.4 & 20.0 & 19.1 & 17.6 & 19.6 \\ \hline Socratic Models & & & & \\ LLaVA-NeXT-34B & 34.6 & 26.7 & 19.1 & 21.8 & 22.3 \\ GPT-4 & 41.0 & 29.4 & 22.8 & 24.0 & 25.7 \\ \hline Multimodal Models & & & & \\ Gemini 1.5 Pro & 55.8 & 38.2 & 35.7 & 28.1 & 37.3 \\ \hline SOTA short-form video model & & & & \\ Tarsier-7B (16 frames) & 32.2 & 24.7 & 27.4 & 17.9 & 26.7 \\ \hline \hline \end{tabular}
\end{table}
Table D.1: **Additonal results on HourVideo using Tarsier-7B [104].** Tarsier-7B (16 frames) performance is comparable to Socratic LLMs. We remark that the Ego4D [13] dataset is used in the pre-training stage of the Tarsier model for video captioning.

Broader Impact

The Long-form Video-Language Understanding Benchmark (HourVideo) introduced in this work has the potential to significantly advance the field of AI video understanding and enable a wide range of useful applications. By focusing on long-form video, HourVideo challenges models to demonstrate high-level reasoning and comprehension skills that more closely mirror human intelligence. Success on this benchmark could lead to AI systems that can effectively perceive and interact with the real world over extended periods of time, unlocking transformative capabilities in areas like embodied AI and robotics, autonomous vehicles, smart environments, and augmented/virtual reality.

Embodied AI and robotics, which aim to develop artificial agents that can perceive, navigate, and physically interact with their environment, could benefit greatly from advances in long-form video understanding. A robot or embodied agent that can maintain a coherent, long-term understanding of its surroundings and goals would be far more capable and adaptable than one operating with only short-term perception. It could handle more complex, multi-stage tasks, learn from extended observations, and build rich mental models to support planning and decision making. For example, a home robot with long-form video understanding could tidy up a room by keeping track of object locations, understanding the steps involved in cleaning tasks, and adapting to unexpected obstacles or messages. Similarly, an industrial robot with long-term video comprehension could perform intricate assembly tasks, monitor and maintain complex machinery, or collaborate seamlessly with human workers. Long-form video understanding is thus a key missing piece in realizing the full potential of embodied AI and robotics.

Progress on HourVideo could also contribute to the development of large world models - AI systems that learn comprehensive, multi-modal representations of the world from vast amounts of data. By processing and consolidating information from extended video sequences, these models could construct more complete and coherent world knowledge that spans time and integrates multiple levels of abstraction. Long-form video understanding would allow these models to not just recognize isolated snapshots, but grasp the flow of events, the persistence and transformation of objects, the rules of physics and causality, and the complex interactions between agents and their environments. This deep, temporally-informed world knowledge could in turn support more advanced reasoning, prediction, planning, and generalization.

Long-form video understanding is also crucial for creating compelling augmented reality (AR) and virtual reality (VR) experiences. An AR system that can parse and adapt to a user's visual context over time would be a far more capable assistant than one that merely labels objects frame-by-frame. In VR, AI characters and environments that evolve responsively to a user's choices and actions throughout an extended interactive session would provide a deeper sense of immersion and realism.

While these exciting applications underscore the importance of advancing long-form video understanding, it is equally critical to consider the potential risks and ethical implications involved. Video data, particularly long-running egocentric video as used in HourVideo, can be highly sensitive and revealing of personal details. As AI video understanding capabilities grow, robust safeguards must be put in place to protect individual privacy, ensure secure data handling, maintain transparency around data collection and use, and prevent unauthorized surveillance or abuse. The intimate window that AR/VR systems and embodied AI agents have into users' private spaces and behaviors further heightens these concerns. As world models become more comprehensive and powerful, it will be crucial to ensure they are developed and used in ways that respect privacy, promote fairness and transparency, and align with human values.

In designing HourVideo, we have taken care to use only videos that are licensed for research and to focus the benchmark on high-level semantic understanding rather than invasive personal information extraction. Nonetheless, the overarching trajectory toward machines that can deeply interpret the visual world will require ongoing vigilance and proactive efforts to align their development and deployment with societal values.

In summary, HourVideo offers a valuable step forward for AI video understanding, with promising implications for embodied AI, robotics, large world models, AR/VR, and beyond. However, for long-form video understanding technology to realize its full positive potential, the AI research community must prioritize the responsible development of these powerful capabilities with strong commitments to ethics, privacy, security, and beneficial impact for humanity. We believe our benchmark will shape the progress of video understanding systems to be not only more capable, but also more trustworthy and socially beneficial.

**Potential Negative Societal Impact.** Advancements in **HourVideo** benchmark could significantly enhance AI capabilities towards building autonomous agents. However, these technologies could also, for example, fuel the development of more sophisticated surveillance systems, raising significant privacy concerns. While such advancements have potential security benefits, they pose risks if used inappropriately, threatening individual privacy in public and private spaces. It is crucial that developments in video understanding are accompanied by stringent ethical standards and robust privacy safeguards to prevent misuse.

**Amount of Compute.** We report the total amount of compute used for captioning 381 hours of video content using LLaVA-NeXT-34B-DPO in Table E.3. For GPT-4, we spent a total of \(\approx\)$10,000 in credits which includes the entire dataset generation pipeline and baseline experiments (Blind LLMs and Socratic LLMs). Gemini 1.5 Pro baseline experiments cost approximately $105 per one-hour video across all tasks/sub-tasks.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Experiment** & **Hardware** & **GPU hours** & **Carbon emitted in kg** \\ \hline Main paper : Table 2 (LLaVA-NeXT-34B) & A6000 & 120 & 9.00 \\ \hline Main paper : Table 2 (LLaVA-NeXT-34B) & RTX A5000 & 24 & 1.66 \\ \hline Additional Compute for Hyper-parameter tuning & RTX A5000 & 12 & 1.80 \\ \hline
**Total** & & **156** & **12.46** \\ \hline \hline \end{tabular}
\end{table}
Table E.3: Amount of compute/ API usage used in this project. The GPU hours include computations for initial explorations/ prompt engineering / experiments to produce the reported values. CO2 emission values are computed using https://mlco2.github.io/impact/

## Appendix B Summarization: Question Generation

### Main instructions:

In the "In-depth Analysis of Extended Videos" class, students will analyze video action narrations divided into chronological segments.Your task is to develop two to five challenging questions about the high-level details in the video and requires watching the entire video to answer. These questions should encourage students to comprehend and think critically about the video's narrative and key elements. Note that in the video narrations provided, 'C' refers to the camera wearer..........

Each segment is structured as follows:

json

"segment_description": "<Summary>",

)

### Skill-Focused instructions:

Your questions should test students' ability to:

- Summarize/ comprehend the video content entirely.

- Compare and contrast different sections of the video to understand its development.

- Distill the video's content to identify central concepts or themes.

### Question generation guidelines:

**1.** Strive to Use Each Question Type below where I have provided examples for each type: Aim to generate one question from each of the following type. You may skip a question type only if it is genuinely incompatible with the video content:

- Key Events Identification: "Summarize the key interactions and events in the video, focusing on their impact and significance."

- Key Tools Identification: "What tools were used by the camera wearer when repairing the bike?"

- Key Technology Use Identification: "What actions did the camera wearer take that involved the use of technology, and how did these actions fit into the overall sequence of events?"

- Temporal Sequencing: "Describe the sequence of activities the camera wearer performed in the kitchen related to food preparation."

- Compare and Contrast: "Compare and contrast the activities the camera wearer engaged in within the apartment and outside of it."

Alternatively, feel free to create similar questions based on the narration content.

**8.** Please use 'the camera wearer' instead of 'C' to emphasize the egocentric perspective in all questions.

### STRICTLY AVOD THE FOLLOWING TYPES OF QUESTIONS:

1. "When...?"

2. "How many...?"

3. "How much...?"

**8.** PLEASE avoid any references to the time of day when generating the questions (night-time, morning time, bed time etc.).

### Formatting instructions:

STRICTLY Return your output as a list of dictionaries as shown in the EXAMPLE OUTPUT below.

**Example output:**

{

"question": "Summarize the key interactions and events in the video, focusing on their impact and significance.",

"question_type": "Key Events Identification"

}.

### Chronological Narration segments:

<Insert Narration Complication below:

Figure E.2: **Summarization question generation prompt.** We show the prompt used for generating summarization questions. This prompt includes question prototypes, step-by-step instructions, formatting guidelines and in-context examples for generating summarization questions. The narration compilation is obtained using the prompt in Fig. E.1.

## Summarization: Answeral Wrong Answers Generation

### Instructions for creating challenging MCG Test for students based on provided questions.

**Main instructions:**

1 want you to act as a teacher in the class called "Long-term video understanding." 1 will provide video narrations along with their timestamps, and a set of highly difficult questions for your students about the high-level details in the video. In this task, I want you to create a difficult Multiple Choice Question (MCG) exam that tests the following abilities of students:

- Ability 1: Students' ability to summarize and compare two parts of the video.

- Ability 2: Students' ability to compress information from the video rather than just listing the actions that happened in the video, and to analyze the subtleties and complexities within the video.

- Ability 3: Students' ability to identify the most important parts of the video, and how these important parts are interconnected and evolve throughout the video.

Note: Please note, in the video narrations provided, 'C' represents the camera wearer, capturing their perspective and actions.

**Objective:**

Your objective is to create a challenging MCG exam testing the above abilities, based on the provided questions. Given the extended length of the video, ensure that the answers require a comprehensive understanding of the entire video content. Please STRiCTLY follow the steps below.

**Steps:**

Step 1: Review the Provided Questions:

.......

Step 2: Create the Correct Answer for Each Question using Video Narrations:

.......

Step 3: Develop Four Misleading Wrong Answers for Each Question:

.......

Step 4: Ensure Relevance and Avoid Direct References:

.......

Step 5: Format the Answers:

.......

Step 6: Repeat Steps 1 to 5 for all the remaining questions in the provided list:

.......

**Step-by-Step Example for your Reference:**

.......

**Guidelines**

1. Only Output Step 6 Results.

2. STRiCTLY Maintain consistency in answer length: All answers, including the correct one, should be of similar length to ensure fairness.

3. Strictly avoid terms including'such as, etc." in your answers. Cratt all answers very objectively, leaving no space for interpretations.

4. AVODI giving away hints that identify incorrect answers.

5. STRiCTLY stay faithful to narrations.

6. Please note "the camera wearer" instead of 'C' to emphasize the egocentric perspective in all questions.

7. NEVER modify any of the questions.

**List of Questions:**

<GENERATED QUESTIONS>

**CHRONOLOGICAL NARARATION SEGEMENTS**

<NARARATION COMPLATION>

Figure E.3: **Summarization answer/ wrong answers generation prompt.** In this Figure, we show the prompt used for generating answers and wrong answers for summarization questions. This prompt includes instructions, step-by-step examples, formatting guidelines for generating answers/ wrong answers for our summarization questions. The questions are generated using the prompt shown in Fig. E.2. The narration compilation is obtained using the prompt in Fig. E.1.

## Appendix E

## Appendix E

[MISSING_PAGE_EMPTY:28]

[MISSING_PAGE_EMPTY:29]

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] Our contributions are clearly described in the abstract and introduction. 2. Did you describe the limitations of your work? We describe our limitations in Sec. 5 3. Did you discuss any potential negative societal impacts of your work? We discuss potential negative societal impacts in Supplementary E 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? Our paper conforms to the NeurIPS ethics review guidelines.
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? 2. Did you include complete proofs of all theoretical results?
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? All code, data, and instructions for reproducing our results are provided in our project website howvideo.stanford.edu. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? There was no training in our benchmark (eval only). 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? Given that our benchmark involves heavy use of costly propriatory models (GPT-4, Gemini 1.5 Pro) for experiments, we did not repeat the experiments. We provide all our prompts/ evaluation code at howvideo.stanford.edu. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? Given most of our experiments involved the use of proprietary models (GPT-4, Gemini 1.5 Pro), we report the API usage in Supplementary E. For LLaVA-NeXT-34B-DPO captioning experiments, we report the amount of compute in Supplementary E.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? We cite EGO4D [13], the source of the videos for our dataset. 2. Did you mention the license of the assets? Our dataset uses the EGO4D License, as explained in Sec. 2.2. 3. Did you include any new assets either in the supplemental material or as a URL? We release our proposed video question answering benchmark dataset. Please refer to howvideo.stanford.edu 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? We obtained consent via agreeing to the EGO4D License. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? Our videos are identical in content to those of EGO4D, thus we do not explicitly discuss this in our paper. EGO4D does contain videos of humans that are identifiable and does not contain offensive content.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? We supply the text instructions given to our annotators in our datasheet. 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? No human participant risks. 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? We supply estimates of cost for our human participants in our datasheet.