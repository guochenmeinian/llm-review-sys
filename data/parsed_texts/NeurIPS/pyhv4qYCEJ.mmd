# Evaluating Self-Supervised Learning for

Molecular Graph Embeddings

Hanchen Wang1\({}^{\star}\) Jean Kaddour2\({}^{\star}\) Shengchao Liu3,4\({}^{\star}\) Jian Tang3,5,6 Joan Lasenby1 Qi Liu7

\({}^{1}\)Cambridge \({}^{2}\)UCL \({}^{3}\)MILA \({}^{4}\)UdeM \({}^{5}\)HEC \({}^{6}\)CIFAR \({}^{7}\)HKU \({}^{\star}\) Equal Contribution

###### Abstract

Graph Self-Supervised Learning (GSSL) provides a robust pathway for acquiring embeddings without expert labelling, a capability that carries profound implications for molecular graphs due to the staggering number of potential molecules and the high cost of obtaining labels. However, GSSL methods are designed not for optimisation within a specific domain but rather for transferability across a variety of downstream tasks. This broad applicability complicates their evaluation. Addressing this challenge, we present "Molecular Graph Representation Evaluation" (MolGraphEval), generating detailed profiles of molecular graph embeddings with interpretable and diversified attributes. MolGraphEval offers a suite of probing tasks grouped into three categories: (i) generic graph, (ii) molecular substructure, and (iii) embedding space properties. By leveraging MolGraphEval to benchmark existing GSSL methods against both current downstream datasets and our suite of tasks, we uncover significant inconsistencies between inferences drawn solely from existing datasets and those derived from more nuanced probing. These findings suggest that current evaluation methodologies fail to capture the entirety of the landscape.

## 1 Introduction

Learning neural embeddings of molecular graphs has become of paramount importance in computer-aided drug discovery [1; 2]. For instance, a molecular property prediction (MPP) model can expedite and economise the design process by reducing the need for synthesising and measuring molecules. Thereby, such models can be immensely useful in the hit-to-lead and early lead optimisation phase of a drug discovery project [3]. However, obtaining labels of molecule properties is expensive and time-consuming, especially since the size of potential pharmacologically active molecules is estimated to be in the order of \(10^{60}\)[4; 5].

Graph Self-Supervised Learning (GSSL) paves the way for learning molecular graph embeddings without human annotations that are transferable to various downstream datasets. Unfortunately, the evaluation of such general-purpose embeddings is fundamentally complex. Different proxy objectives will place different demands on them, and no single downstream dataset can be definitive. Moreover, many of the previously proposed GSSL works are disconnected in terms of the tasks they target and the datasets they use for evaluation, making direct comparison difficult.

Figure 1: Overview of MolGraphEval. Given molecular graphs, we train GNNs to predict SSL proxy objectives. We then extract embeddings of (possibly unseen) graphs using pre-trained models, which form the inputs for probe models, trained and evaluated on the designed metrics.

**Contributions.** Our goal is to unbiasedly evaluate molecular graph embeddings obtained by GSSL methods on existing downstream tasks and a new suite of probe tasks (Fig. 1). We summarised some key findings based on a total of **90,918 probe models** and **1,875 pre-trained GNNs**.

1. On MPP tasks, we observe every GSSL method can introduce substantial performance gains. Yet, there is a significant difference in the rank depending on whether we fine-tune the pre-trained network on the downstream dataset or not. Also, the pre-training configurations for obtaining the optimal embeddings or initialisation for fine-tuning are different; see - **Finding 2**.
2. Several discrepancies between MPP tasks and MolGraphEval demonstrate how the latter complements GSSL evaluation with novel insights fostering future work:

* **Finding 4**.
* **Finding 8**.
* **Finding 9**.

## 2 Related work

**Graph SSL (GSSL)** can be divided into contrastive and generative methods [7; 8; 9]. Contrastive GSSL [10; 11; 12] construct multiple views of the same graph via augmentations and then learn embeddings by aligning positive samples against negative ones. Generative GSSL [11; 13; 14; 15] yields embeddings by reconstructing input graphs. Zhu et al. [16] conduct an empirical analysis of contrastive GSSL methods and their components. In contrast, we investigate both generative and contrastive GSSL methods and propose a novel suite of tasks to probe the learned embeddings' attributes.

**Probe models and benchmarks on graphs.** Probe models, trained exclusively on embedding vectors from pre-trained models, serve as an effective tool for evaluating the quality of learned embeddings [17]. Their effectiveness has been demonstrated across various domains such as language [18; 19; 20; 21; 22; 23], vision [24; 25; 26; 27; 28], relational tables [29], and science [30; 31; 32]. While there exist benchmarks for graph learning [33; 34; 35; 36; 37], applying probe models to GSSL remains an unexplored frontier.

## 3 Preliminaries

**Graph.** A graph \(\mathcal{G}=(\mathcal{V},\mathcal{E})\) consists of a set of nodes \(\mathcal{V}\) and edges \(\mathcal{E}\). In molecular graphs, nodes are atoms, and edges are bonds. We use \(\bm{x}_{u}\) and \(\bm{x}_{uv}\) to denote the feature of node \(u\) and the bond feature between nodes \([u,v]\), respectively. For notation simplicity, we use an adjacency matrix \(\mathbf{A}\in\mathbb{R}^{|\mathcal{V}|\times|\mathcal{V}|}\) to represent the graph, where \(\mathbf{A}[u,v]\neq 0\) if the nodes \((u,v)\) are connected.

**GNN.** Graph neural networks (GNNs) give rise to learning molecular graph embeddings [38; 39; 40; 41]. A prototypical GNN relies on messaging passing [39], which updates atom-level embeddings based on their neighbourhoods. Given an input atom \(\bm{h}_{u}^{0}=\bm{x}_{u}\), we compute its embedding by:

\[\bm{m}_{u}^{t+1}=\sum_{v:\mathbf{A}[u,v]\neq 0}M_{t}(\bm{h}_{u}^{t},\bm{h}_{v} ^{t},\bm{x}_{uv})\qquad\qquad\bm{h}_{u}^{t+1}=U_{t}(\bm{h}_{u}^{t},\bm{m}_{u}^{ t+1})\] (1)

where \(M_{t}\) and \(U_{t}\) are the "message" functions and "vertex update" functions, respectively. Repeating message passing for \(T\) steps, the embedding of each atom contains their \(T\)-hop neighbourhood information. A readout function \(R\) is then used to pool node-level embeddings for graph-level representations: \(\hat{y}=R(\{\bm{h}_{u}^{\top}\mid u\in\mathcal{V}\})\). Following previous GSSL methods on molecular graphs, we adopt the Graph Isomorphism Network (GIN) [42] as the backbone model and incorporate edge features during message passing following [11].

**Pre-Training.** We inspect nine GSSL methods (1,875 configurations in total): EdgePred[13], InfoGraph[10], GPT-GNN[15], AttrMask[11], ContextPred[11], GROVER[43], GraphCL[12], JOAO[44], and GraphMVP[45]. We use all qualified molecules (around 0.33 million, _i.e._, leave out the molecules that appeared in downstream datasets) from the GEOM dataset [46] to pre-train the GIN backbone. As many of these pre-training methods are not primarily designed for molecular graphs, we perform the grid search over the hyperparameter space and savethe optimal settings. For these nine GSSL methods, we have pre-trained 1,875 GNNs with different configurations, as elaborated in Appendix B. We extract embeddings using the pre-trained weights, select the optimal hyperparameter sets based on their downstream MPP performance and use these optimal embeddings for further probing tasks.

**Probe.** We use probe models [18] to study whether self-supervised learned embeddings encode helpful structural information about graphs. Concretely, we extract embeddings from a pre-trained GNN and train a linear model to predict the probe tasks with node and graph embeddings as inputs. As the first work that designs probe methods on graph embeddings, we follow previous works on computer vision and natural language processing. We mainly compute and compare the quality of pre-trained embeddings using linear probe models. We have also experimented MLPs with one hidden layer as the probe models, as this architecture is utilised in some previous works. We observe similar findings with both probe architectures and reported the results of MLP probes in Appendix B.4. We use scaffold split to partition data into 80%/10%/10% for the training/validation/testing set. The training procedure runs for 100 epochs with a fixed learning rate of 0.001. We report the test results based on the best validation scores. To account for statistical significance, we average all experimental results over three independent runs. We find that different data splits are the primary cause for performance variations (\(\sim\)2%), instead of initialising probe models with different random seeds (\(<\)0.01%).

## 4 Benchmarking GSSL on MPP

We first conduct a rigorous empirical investigation of the GSSL methods' effectiveness in predicting the biochemical properties of molecules. Following previous work [11; 12], we consider eight molecular datasets consisting of 678 binary property prediction tasks [47; 48]. Unless explicitly stated otherwise, we extract the node/graph embeddings from the last GNN layer. We devise two settings: (i) fixed embeddings, where we train the probe models with fixed embeddings extracted from pre-trained GNNs; (ii) fine-tuned embeddings ("FT"), where we update weights of both the pre-trained GNNs and the probe models. Setting (i) follows the procedures in previous probing literature, while (ii) is widely utilised as the "pre-training, then fine-tuning" paradigm. We use Adam optimiser with no weight decay, set the batch size as 256, and apply identical pre-processing procedures for all experiments.

**Findings.** Table 1 notes the results, and we summarise the following findings, some of which contrast with those drawn from the concurrent study [49].

1. **All GSSL methods perform better than Random.** By carefully exploring the pre-training hyperparameters, all GSSLs substantially improve the MPP tasks for both fixed and fine-tuned embeddings. Contrastive-based GSSL methods (_i.e._, GraphCL, JOAO and GraphMVP) achieve the overall best performance. As [49] declares that molecular graph pretraining is ineffective; however, we find that their conclusions are based on a few selected finetuning datasets and fixed

\begin{table}
\begin{tabular}{l|c c c c c c c c|c|c} \hline \hline  & BBBP & Tox21 & ToxCast & Sider & ClinTox & MUV & HIV & Race & Avg & Avg (FT) \\ \hline \# Molecules & 2,039 & 7,831 & 8,575 & 1,427 & 1,478 & 93,087 & 41,127 & 1,513 & – & – \\ \# Tasks & 1 & 12 & 617 & 27 & 2 & 17 & 1 & 1 & – & – \\ \hline Random & 50.7 \(\pm_{2.5}\) & 64.9 \(\pm_{0.5}\) & 53.2 \(\pm_{0.3}\) & 53.2 \(\pm_{1.1}\) & 63.1 \(\pm_{2.3}\) & 62.1 \(\pm_{1.3}\) & 66.1 \(\pm_{0.7}\) & 63.4 \(\pm_{1.8}\) & 59.60 & 66.16 \\ \hline EdgePred & 54.2 \(\pm_{1.0}\) & 66.2 \(\pm_{0.2}\) & 54.4 \(\pm_{0.1}\) & 56.1 \(\pm_{0.1}\) & **65.4 \(\pm_{0.5}\)** & 59.5 \(\pm_{0.0}\) & **73.6 \(\pm_{0.4}\)** & 71.4 \(\pm_{1.2}\) & 62.59 & 68.16 \\ AttnMask & 62.7 \(\pm_{2.7}\) & 65.7 \(\pm_{0.8}\) & **56.1 \(\pm_{0.2}\)** & 58.3 \(\pm_{1.5}\) & 61.9 \(\pm_{0.4}\) & 64.0 \(\pm_{1.8}\) & 65.5 \(\pm_{1.4}\) & 64.8 \(\pm_{2.6}\) & 61.99 & 69.20 \\ GPT-GNN & 62.0 \(\pm_{0.9}\) & 64.9 \(\pm_{0.7}\) & 55.4 \(\pm_{0.2}\) & 55.3 \(\pm_{0.5}\) & 55.0 \(\pm_{1.6}\) & 61.2 \(\pm_{1.5}\) & 71.2 \(\pm_{1.5}\) & 61.0 \(\pm_{1.2}\) & 60.74 & 67.58 \\ InfoGraph & 65.9 \(\pm_{0.6}\) & 65.8 \(\pm_{0.7}\) & 54.6 \(\pm_{0.1}\) & 57.2 \(\pm_{0.1}\) & 61.4 \(\pm_{0.8}\) & **63.9 \(\pm_{1.9}\)** & 71.4 \(\pm_{0.6}\) & 67.4 \(\pm_{4.9}\) & 63.44 & 68.92 \\ Cont.Pred & 55.5 \(\pm_{2.0}\) & 67.9 \(\pm_{0.7}\) & 54.0 \(\pm_{0.3}\) & 57.1 \(\pm_{0.5}\) & **67.4 \(\pm_{3.0}\)** & 60.5 \(\pm_{0.9}\) & 66.2 \(\pm_{1.5}\) & 54.4 \(\pm_{3.2}\) & 60.36 & 69.40 \\ GROVER & **67.0** & 3.63 \(\pm_{0.3}\) & 53.6 \(\pm_{0.4}\) & **59.9 \(\pm_{1.7}\)** & 65.0 \(\pm_{0.4}\) & 62.7 \(\pm_{1.4}\) & 67.8 \(\pm_{1.0}\) & 69.0 \(\pm_{1.7}\) & 63.26 & 69.97 \\ GraphCL & **64.7** & 1.71 \(\pm_{0.1}\) & **69.1 \(\pm_{0.5}\)** & **56.2 \(\pm_{0.2}\)** & **59.5 \(\pm_{0.6}\)** & 60.8 \(\pm_{3.0}\) & 60.6 \(\pm_{1.8}\) & 72.5 \(\pm_{1.4}\) & **77.0 \(\pm_{1.7}\)** & **65.04** & **70.33** \\ JOAO & 66.1 \(\pm_{0.8}\) & **68.1** & 4.02 \(\pm_{0.2}\) & 55.1 \(\pm_{0.4}\) & 58.3 \(\pm_{0.3}\) & 65.3 \(\pm_{0.1}\) & 62.4 \(\pm_{1.2}\) & **73.8 \(\pm_{1.2}\)** & 71.1 \(\pm_{0.8}\) & **65.05** & 69.75 \\ GraphMVP & **69.2** \(\pm_{1.8}\) & 63.8 \(\pm_{0.3}\) & 55.5 \(\pm_{0.3}\) & 58.6 \(\pm_{0.4}\) & 58.7 \(\pm_{1.9}\) & **63.8 \(\pm_{1.3}\)** & 68.6 \(\pm_{1.0}\) & **73.3 \(\pm_{4.7}\)** & 63.92 & **70.06** \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Evaluating GSSL methods on molecular property prediction tasks. For each downstream dataset, we report the mean and standard deviation of the ROC-AUC scores over three random scaffold splits. The best and second best scores are marked bold and bold, respectively. The performance scores are based on the fixed pre-trained embeddings with linear probe models, we also report the average ROC-AUC scores with fine-tuned pre-trained GNN on MPP tasks (“Avg (FT)”). For each pre-training method, we report the highest scores in the table and their corresponding hyperparameter configurations in Tables 7 and 9 in Appendix B.**pre-training hyperparameters. We further observe that such improvements will reduce when the number of molecules in downstream datasets increases. Specifically, for MUV [50], a dataset designed for validating virtual screening (used in drug discovery to find how likely molecules that bind to a drug target), the average performance gain brought by pre-training is -0.3%; while for BBBP, it is 12.3%. The number of molecules in BBBP is only 2% of the MUV's.
2. **Rankings differ between probing and fine-tuning.** The rank correlation between the fixed and fine-tuned embeddings is 0.77 (p-value=9e-4), indicating that we cannot utilise the rank of fixed embeddings as a definite indicator for fine-tuning performance, though they are positively correlated. Part of this observation has been spotted in a study on masked visual transformers [25]. In the context of molecular property prediction, embeddings pre-trained with JOAO achieve the best score with fixed scenarios but perform the fourth after end-to-end fine-tuning. The reason is unclear and should be investigated by future work.
3. **The optimal sets of pre-train hyperparameters for fixed and fine-tuned embeddings vary.** We observe that the optimal pre-training hyperparameters on fixed and fine-tuned embeddings differ. Only two out of nine GSSLs (InfoGraph and EdgePred) share the same set of optimal parameters, as detailed in Tables 7 and 9 in Appendix B. This suggests that probing the fixed embeddings might not truly reflect pre-trained models' performance on downstream MPP tasks, as it ignores the consequent improvements induced by fine-tuning. In Fig. 2, we visualised the hyperparameter space of the AttrMask pre-trainer, the local minima in the hyperparameter space distribute differently. As shown in Fig. 2 also Table 9, the best pre-training configuration for probing is "mask rate=0.85 and learning rate=1e-4", while in terms of fine-tuning scores, the optimal setting is "mask rate=0.50 and learning rate=5e-4". Also, it can be inferred that the optimal pre-training hyperparameters for different pre-training datasets vary; therefore, using reported hyperparameters without carefulness and concluding "graph pretraining is ineffective in molecular domain" is not convincing [49].

## 5 Molecular graph representation evaluation

The goal of GSSL for molecular graphs is to obtain embeddings that capture generic information about the molecule and its properties. However, there is no free lunch [51]: different training objectives optimise for different properties, and evaluating the extracted embeddings on only a handful of downstream datasets does not provide the whole picture (as we confirm empirically in Sec. 6). Also, from Sec. 4, we know the probing and fine-tuning performance are positively correlated, yet their optimal pre-training configurations are largely diverging. In the Appendix, we also provide results on the worst pre-training configurations, some of which cause negative transfer due to initialising the encoders into local bad minima. Investigations are required to understand what kind of property makes the pre-trained encoders differ.

To this end, we propose MolGraphEval, which encompasses a variety of carefully-selected probe tasks, categorised into three classes: (i) **generic graph properties**, (ii) **molecular substructure properties** and (iii) **embedding space properties**. In the upcoming subsections, we explain the tasks in more detail and why they are essential for molecular graph embeddings.

### Generic graph properties

Topological property statistics are often used as features in machine learning pipelines on graphs that do not rely on neural networks [52]. Based on their scale, they can be divided into {node-, pair-, and graph-} level statistics. For molecular graphs, topological metrics have been widely used as molecular descriptors in cheminformatics for decades [53; 54; 55; 56], metrics at different scales will facilitate different tasks.

**Node-level statistics** accompany each node with a local topological measure, which could be used as features in node classification [52]. Concretely, node-level information such as degree [57] can reflect the reaction centres [58]; thus, it can aid in discovering chemical reactions [59].

Figure 2: **Hyperparameter space of AttrMask** (mask rate \(\times\) learning rate), coloured by MPP test scores. Above: fixed; below: fine-tuned. Note the colour bars are different.

* **Node degree** (\(d_{u}\)) counts the number of edges incident to node \(u\): \(d_{u}=\sum_{v\in V}\mathbf{A}[u,v]\).
* **Centrality** (\(e_{u}\)) represents a node's importance. The eigenvector centrality is determined by a relation proportional to the average centrality of its neighbours: \(e_{u}=\left(\sum_{v\in V}\mathbf{A}[u,v]e_{v}\right)/\lambda\), \(\forall u\in\mathcal{V}\).
* **Clustering coefficient** (\(c_{u}\)) measures how tightly clustered a node's neighbourhood is: \(c_{u}=(|\left(v_{1},v_{2}\right)\in\mathcal{E}:v_{1},v_{2}\in\mathcal{N}(u)|)/ d_{u}^{2}\), _i.e._, the fraction of closed triangles in neighbourhood [60].

**Graph-level statistics** summarise global topology information and are helpful for graph classification tasks. For molecules, graph-level statistics can be used, _e.g._, to classify a molecule's solubility [61]. We briefly describe their intuitions; formal definitions can be found, _e.g._, in [52].

* **Diameter** is the maximum distance between the pair of vertices (_i.e._, longest path in a molecule).
* **Cycle basis** is a set of simple cycles that forms a basis of the graph cycle space. It is a minimal set that allows every even-degree subgraph to be expressed as a symmetric difference of basis cycles.
* **Connectivity** is the minimum number of elements (nodes or edges) that need to be removed to separate the remaining nodes into two or more isolated subgraphs.
* **Assortativity** measures the similarity of connections in the graph with respect to the node degree. It can be seen as the Pearson correlation coefficient of degrees between pairs of linked nodes.

**Pair-level statistics** quantify the relationships between nodes (atoms), which is vital in molecular modelling. For example, molecular docking techniques aim to predict the best matching binding mode of a ligand to a macro-molecular partner [62]. For predicting such binding compatibility, connectivity and distance awareness (how close a pair of atoms can be) are important. In our implementation, we randomly select a fixed number (_i.e._, 10) of atom pairs from each molecular graph. These pairs are then categorised based on their originating molecule, ensuring all pairs from a single molecule are designated to a singular split: either train, validation, or test.

* **Link prediction** tests whether two nodes are connected or not, given their embeddings and inner products. Based on the principle of _homophily_, it is expected that embeddings of connected nodes are more similar compared to disconnected pairs: \[\mathbf{S}_{\mathrm{Link}}[u,v,\mathbf{x}_{u}^{T}\mathbf{x}_{v}]=\mathbbm{1}_ {\mathcal{N}(u)}(v)\] (2)
* **Jaccard coefficient** seeks to quantify the overlap between neighbourhoods while minimising the biases induced by node degrees [63]: \[\mathbf{S}_{\mathrm{Jaccard}}\left[u,v\right]=|\mathcal{N}(u)\cap\mathcal{N}(v )|/|\mathcal{N}(u)\cup\mathcal{N}(v)|\] (3)
* **Katz index** is a global overlap statistic defined by the number of paths between a pair of nodes: \[\mathbf{S}_{\mathrm{Katz}}[u,v]=\sum_{i=1}^{\infty}\beta^{i}\mathbf{A}^{i}[u,v]\] (4) where \(\beta\in\mathbb{R}^{+}\) determines the weight between short and long paths. \(\beta<1\) reduces the weight of long paths, in implementations we set \(\beta=1\) to give all paths equal importance.

### Molecular substructure properties

Molecular substructures often serve as reliable indicators of biochemical properties [64, 65, 66, 67]. For instance, molecules with benzene rings typically share consistent physical properties, such as solubility, as well as chemical characteristics like aromaticity [68].

**Substructures.** We investigate 24 substructures from three groups: **rings** (Benzene, Beta lactams,..., Thiophene); **functional groups** (Amides, Amidine,..., Urea); and **redox active sites** (Allylic). We provide chemical knowledge on how they relate with molecular properties in Appendix D.

**How predictive are substructures?** To demonstrate that substructures are quite predictive of molecular properties, we utilise counts of substructures within a molecular graph as the input for

\begin{table}
\begin{tabular}{c c c|c c c} \hline \hline Linear Regression & Random Forest & XGBoost & Random (FIX/FT) & JOAO (FIX) & GraphCL (FT) \\ \hline
59.91 & 61.95 & 62.31 & 59.60 / 66.16 & 65.05 & 70.33 \\ \hline \hline \end{tabular}
\end{table}
Table 2: **ROC-AUC scores of classifiers predicting molecular properties**.

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

The purpose of this work is to complement current evaluation practices with probe tasks and metrics that reveal novel insights, rather than arguing about which combinations of pre-training tasks yield the best downstream performance. Also, as our primary focus is the pre-trained GNN encoders, we leave the investigations of comparing probing and fine-tuning embeddings in the future. Our empirical findings suggest that there are many open questions on how to learn robust molecular graph embeddings without labels and a better understanding of these, along with a new methodology for solving some of the issues mentioned earlier (_e.g._, dimensional collapse), are yet to come. Nevertheless, we are optimistic that the tasks proposed in this paper will benefit the GSSL research community to tackle these challenges and applied scientists in fields like drug discovery to yield additional insights that can help their problem.

## Acknowledge

We thank Le Song, Anima Anandkumar, Matthew Welborn for valuable discussions.

## References

* [1] Eugene N Muratov, Jurgen Bajorath, Robert P Sheridan, Igor V Tetko, Dmitry Filimonov, et al. Qsar without borders. _Chemical Society Reviews_, 49(11):3525-3564, 2020.
* [2] Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, et al. Scientific discovery in the age of artificial intelligence. _Nature_, 620(7972):47-60, 2023.
* [3] Megan Stanley, John F Bronskill, Krzysztof Maziarz, Hubert Misztela, et al. FS-mol: A few-shot learning dataset of molecules. In _NeurIPS Datasets and Benchmarks Track_, 2021.
* [4] RS Bohacek, C McMartin, and WC Guida. The art and practice of structure-based drug design: a molecular modeling perspective. _Medical research reviews_, 16(1):3--50, 1996.
* [5] Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. Molecular contrastive learning of representations via graph neural networks. _Nature Machine Intelligence_, 2022.
* [6] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In _ICML_, 2020.
* [7] Yixin Liu, Ming Jin, Shirui Pan, Chuan Zhou, Yu Zheng, Feng Xia, and Philip Yu. Graph self-supervised learning: A survey. _IEEE TKDE_, 2022.
* [8] Yaochen Xie, Zhao Xu, Jingtun Zhang, Zhengyang Wang, and Shuiwang Ji. Self-supervised learning of graph neural networks: A unified review. _IEEE TPAMI_, 2022.
* [9] Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, et al. Self-supervised learning: Generative or contrastive. _IEEE TKDE_, 2021.
* [10] Fan-Yun Sun, Jordan Hoffmann, et al. Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization. In _ICLR_, 2020.
* [11] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, et al. Strategies for pre-training graph neural networks. In _ICLR_, 2020.
* [12] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, et al. Graph contrastive learning with augmentations. In _NeurIPS_, 2020.
* [13] William Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In _NIPS_, 2017.
* [14] Shengchao Liu, Mehmet Furkan Demirel, and Yingyu Liang. N-gram graph: Simple unsupervised representation for graphs, with applications to molecules. In _NeurIPS_, 2018.
* [15] Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou Sun. Gpt-gnn: Generative pre-training of graph neural networks. In _KDD_, 2020.
* [16] Yanqiao Zhu, Yichen Xu, Qiang Liu, and Shu Wu. An empirical study of graph contrastive learning. In _NeurIPS Datasets and Benchmarks Track_, 2021.
* [17] Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes. In _ICLR Workshop_, 2017.
* [18] Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. Linguistic knowledge and transferability of contextual representations. In _NAACL_, 2019.
* [19] John Hewitt and Christopher D. Manning. A structural probe for finding syntax in word representations. In _NAACL_, 2019.
* [20] Ian Tenney et al. BERT rediscovers the classical NLP pipeline. In _ACL_, 2019.
* [21] Ganesh Jawahar, Benoit Sagot, and Djame Seddah. What does BERT learn about the structure of language? In _ACL_, 2019.

* [22] Nora Kassner and Hinrich Schutze. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In _ACL_, 2020.
* [23] Lisa Anne Hendricks, John Mellor, Rosalia Schneider, Jean-Baptiste Alayrac, et al. Decoupling the role of data, attention, and losses in multimodal transformers. _TACL_, 2021.
* [24] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, et al. Emerging properties in self-supervised vision transformers. In _ICCV_, 2021.
* [25] Kaiming He, Xinlei Chen, Saining Xie, et al. Masked autoencoders are scalable vision learners. In _CVPR_, 2022.
* [26] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In _ICCV_, 2021.
* [27] Chunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, et al. Efficient self-supervised vision transformers for representation learning. In _ICLR_, 2022.
* [28] Hanchen Wang, Qi Liu, Xiangyu Yue, Joan Lasenby, and Matthew J. Kusner. Unsupervised point cloud pre-training via occlusion completion. In _ICCV_, 2021.
* [29] Shengchao Liu, David Vazquez, Jian Tang, and Pierre-Andre Noel. Flaky performances when pretraining on relational databases. _arXiv:2211.05213_, 2022.
* [30] Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Xi Chen, et al. Evaluating protein transfer learning with tape. In _NeurIPS_, 2019.
* [31] Alexander Rives, Joshua Meier, Tom Sercu, et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. _PNAS_, 2021.
* [32] Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rihawi, Yu Wang, et al. Prottrans: towards cracking the language of life's code through self-supervised deep learning and high performance computing. _IEEE TPAMI_, 2021.
* [33] Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. Ogb-lsc: A large-scale challenge for machine learning on graphs. In _NeurIPS Dataset and Benchmark Track_, 2021.
* [34] Qinkai Zheng, Xu Zou, Yuxiao Dong, Yukuo Cen, Da Yin, Jiarong Xu, Yang Yang, and Jie Tang. Graph robustness benchmark: Benchmarking the adversarial robustness of graph machine learning. In _NeurIPS Dataset and Benchmark Track_, 2021.
* [35] Yuanqi Du, Shiyu Wang, Xiaojie Guo, Hengning Cao, Shujie Hu, Junji Jiang, Aishwarya Varala, Abhinav Angirekula, and Liang Zhao. Graphgt: Machine learning datasets for graph generation and transformation. In _NeurIPS Dataset and Benchmark Track_, 2021.
* [36] Shurui Gui, Xiner Li, Limei Wang, and Shuiwang Ji. Good: A graph out-of-distribution benchmark. In _NeurIPS Dataset and Benchmark Track_, 2022.
* [37] Yijian Qin, Ziwei Zhang, Xin Wang, Zeyang Zhang, and Wenwu Zhu. Nas-bench-graph: Benchmarking graph neural architecture search. In _NeurIPS Dataset and Benchmark Track_, 2022.
* [38] David Duvenaud, Dougal Maclaurin, Jorge A.-Iparraguirre, Rafael Gomez-Bombarelli, et al. Convolutional networks on graphs for learning molecular fingerprints. In _NIPS_, 2015.
* [39] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In _ICML_, 2017.
* [40] Kevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, et al. Analyzing learned molecular representations for property prediction. _Journal of Chemical Information and Modeling_, 2019.
* [41] Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Lio, and Petar Velickovic. Principal neighbourhood aggregation for graph nets. In _NeurIPS_, 2020.

* [42] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _ICLR_, 2019.
* [43] Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, et al. Self-supervised graph transformer on large-scale molecular data. In _NeurIPS_, 2020.
* [44] Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning automated. In _ICML_, 2021.
* [45] Shengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, et al. Pre-training molecular graph representation with 3d geometry. In _ICLR_, 2022.
* [46] Simon Axelrod and Rafael Gomez-B. Geom: Energy-annotated molecular conformations for property prediction and molecular generation. _Scientific Data_, 2022.
* [47] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, et al. Moleculenet: a benchmark for molecular machine learning. _Chemical Science_, 2018.
* [48] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, et al. Open graph benchmark: Datasets for machine learning on graphs. In _NeurIPS_, 2021.
* [49] Ruoxi Sun, Hanjun Dai, and Adams Wei Yu. Does GNN pretraining help molecular representation? In _NeurIPS_, 2022.
* [50] Martin Vogt, Dagmar Stumpfe, Hanna Geppert, and Jurgen Bajorath. Scaffold hopping using two-dimensional fingerprints: True potential, black magic, or a hopeless endeavor? guidelines for virtual screening. _Journal of Medicinal Chemistry_, 53(15):5707-5715, 2010.
* [51] David H Wolpert and William G Macready. No free lunch theorems for optimization. _IEEE transactions on evolutionary computation_, 1(1):67-82, 1997.
* [52] William Hamilton. _Graph Representation Learning_. Morgan & Claypool Publishers, 2020.
* [53] James Devillers and Alexandru T Balaban. _Topological Indices and Related Descriptors in QSAR and QSPAR_. CRC Press, 2000.
* [54] Dejun Jiang, Zhenxing Wu, Chang-Yu Hsieh, Guangyong Chen, Ben Liao, et al. Could graph neural networks learn better molecular representation for drug discovery? a comparison study of descriptor-based and graph-based models. _Journal of Cheminformatics_, 2021.
* [55] Mati Karelson. _Molecular descriptors in QSAR/QSPR_. Wiley-Interscience, 2000.
* [56] Frank Emmert-Streib. _Statistical modelling of molecular descriptors in QSAR/QSPR_. John Wiley & Sons, 2012.
* [57] Tomislav Dosilic, Boris Furtula, Ante Graovac, Ivan Gutman, Sirous Moradi, and Zahra Yarahmadi. On vertex-degree-based molecular structure descriptors. _Communications in Mathematical and in Computer Chemistry_, 66(2):613-626, 2011.
* [58] Ramil I Nugmanov, Ravil N Mukhametgaleev, Tagir Akhmetshin, Timur R Gimadiev, Valentina A Afonina, et al. Cgrtools: Python library for molecule, reaction, and condensed graph of reaction processing. _Journal of Chemical Information and Modeling_, 2019.
* [59] William Bort, Igor Baskin, Timur Gimadiev, Artem Mukanov, et al. Discovery of novel chemical reactions by deep generative recurrent neural network. _Scientific Reports_, 2021.
* [60] Duncan J Watts and Steven H Strogatz. Collective dynamics of'small-world' networks. _Nature_, 1998.
* [61] Benjamin Sanchez-Lengeling, Loic M Roch, Jose Dario Perea, Stefan Langner, Christoph J Brabec, et al. A bayesian approach to predict solubility parameters. _Advanced Theory and Simulations_, 2(1):1800069, 2019.

* [62] Veronica Salmaso and Stefano Moro. Bridging molecular docking to molecular dynamics in exploring ligand-protein recognition process: An overview. _Frontiers in pharmacology_, 2018.
* [63] Linyuan Lu and Tao Zhou. Link prediction in complex networks: A survey. _Physica A: statistical mechanics and its applications_, 2011.
* [64] Gerta Rucker and Christoph Rucker. Substructure, subgraph, and walk counts as measures of the complexity of graphs and molecules. _Journal of Chemical Information and Computer Sciences_, 41(6):1457-1462, 2001.
* [65] Youngchun Kwon, Dongseon Lee, Youn-Suk Choi, Kyoham Shin, and Seokho Kang. Compressed graph representation for scalable molecular graph generation. _Journal of Cheminformatics_, 2020.
* [66] Ryuichiro Hataya, Hideki Nakayama, and Kazuki Yoshizoe. Graph energy-based model for substructure preserving molecular design. _arXiv:2102.04600_, 2021.
* [67] Xian-bin Ye, Quanlong Guan, Weiqi Luo, Liangda Fang, Zhao-Rong Lai, et al. Molecular substructure graph attention network for molecular property identification in drug discovery. _Pattern Recognition_, 128:108659, 2022.
* [68] John E McMurry. _Organic chemistry with biological applications_. Cengage Learning, 2014.
* [69] Li Jing, Pascal Vincent, Yann LeCun, and Yuandong Tian. Understanding dimensional collapse in contrastive self-supervised learning. In _ICLR_, 2022.
* [70] Tiago Janela and Jurgen Bajorath. Simple nearest-neighbour analysis meets the accuracy of compound potency predictions using complex machine learning models. _Nature Machine Intelligence_, pages 1-10, 2022.
* [71] Tianyu Hua, Wenxiao Wang, Zihui Xue, Sucheng Ren, Yue Wang, et al. On feature decorrelation in self-supervised learning. In _ICCV_, 2021.
* [72] Tianjin Huang, Tianlong Chen, Meng Fang, Vlado Menkovski, Jiaxu Zhao, Lu Yin, Yulong Pei, Decebal Constantin Mocanu, Zhangyang Wang, Mykola Pechenizkiy, et al. You can have better graph neural networks by not training weights at all: Finding untrained graph tickets. In _Learning on Graphs Conference_, 2021.
* [73] Hannes Stark, Dominique Beaini, Gabriele Corso, Prudencio Tossou, Christian Dallago, Stephan Gunnemann, and Pietro Lio. 3d infomax improves gnns for molecular property prediction. In _ICML_, 2022.
* [74] Hannes Stark, Octavian Ganea, Lagnajit Pattanaik, Regina Barzilay, and Tommi Jaakkola. Equibind: Geometric deep learning for drug binding structure prediction. In _ICML_, 2022.
* [75] Xingang Peng, Shitong Luo, Jiaqi Guan, Qi Xie, Jian Peng, and Jianzhu Ma. Pocket2mol: Efficient molecular sampling based on 3d protein pockets. In _ICML_, 2022.
* [76] Deli Chen, Yankai Lin, Wei Li, Peng Li, et al. Measuring and relieving the over-smoothing problem for graph neural networks from the topological view. In _AAAI_, 2020.
* [77] Thomas A Halgren. Merck molecular force field. i. basis, form, scope, parameterization, and performance of mmff94. _Journal of computational chemistry_, 17(5-6):490-519, 1996.
* [78] Xiaomin Fang, Lihang Liu, Jieqiong Lei, Donglong He, Shanzhuo Zhang, Jingbo Zhou, Fan Wang, Hua Wu, and Haifeng Wang. Geometry-enhanced molecular representation learning for property prediction. _Nature Machine Intelligence_, 4(2):127-134, 2022.
* [79] AG Donchev, VD Ozrin, MV Subbotin, OV Tarasov, and VI Tarasov. A quantum mechanical polarizable force field for biomolecular interactions. _PNAS_, 2005.

* [80] Michael D Beachy, David Chasman, Robert B Murphy, Thomas A Halgren, and Richard A Friesner. Accurate ab initio quantum chemical determination of the relative energetics of peptide conformations and assessment of empirical force fields. _JACS_, 1997.
* [81] Ilana Y Kamal, John A Keith, and Geoffrey R Hutchison. A sobering assessment of small-molecule force field methods for low energy conformer predictions. _International Journal of Quantum Chemistry_, 118(5):e25512, 2018.
* [82] Rui Jiao, Jiaqi Han, Wenbing Huang, Yu Rong, and Yang Liu. 3d equivariant molecular graph pretraining. _arXiv:2207.08824_, 2022.
* [83] Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, et al. Uni-mol: A universal 3d molecular representation learning framework. _chemRxiv_, 2022.
* [84] Shengchao Liu, Hongyu Guo, and Jian Tang. Molecular geometry pretraining with se (3)-invariant denoising distance matching. _arXiv:2206.13602_, 2022.
* [85] Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. _Scientific data_, 2014.
* [86] Raphael JL Townshend, Martin Vogele, Patricia Suriana, Alexander Derry, et al. Atom3d: Tasks on molecules in three dimensions. In _NeurIPS Dataset and Benchmark Track_, 2021.
* [87] Yixin Liu, Ming Jin, Shirui Pan, Chuan Zhou, Yu Zheng, Feng Xia, and S Yu Philip. Graph self-supervised learning: A survey. _IEEE TKDE_, 2022.
* [88] Xiao Luo, Wei Ju, Meng Qu, Yiyang Gu, Chong Chen, Minghua Deng, Xian-Sheng Hua, and Ming Zhang. Clear: Cluster-enhanced contrast for self-supervised graph representation learning. _IEEE TNNLS_, 2022.
* [89] Shuai Lin, Chen Liu, Pan Zhou, Zi-Yuan Hu, Shuojia Wang, Ruihui Zhao, Yefeng Zheng, Liang Lin, Eric Xing, and Xiaodan Liang. Prototypical graph contrastive learning. _IEEE TNNLS_, 2022.
* [90] Haifeng Li, Jun Cao, Jiawei Zhu, Qinyao Luo, Silu He, and Xuying Wang. Augmentation-free graph contrastive learning of invariant-discriminative representations. _IEEE TNNLS_, 2023.
* [91] Jintang Li, Ruofan Wu, Wangbin Sun, Liang Chen, Sheng Tian, Liang Zhu, Changhua Meng, Zibin Zheng, and Weiqiang Wang. Maskgae: Masked graph modeling meets graph autoencoders. _arXiv:2205.10053_, 2022.
* [92] Qiaoyu Tan, Ninghao Liu, Xiao Huang, Rui Chen, Soo-Hyun Choi, and Xia Hu. Mgae: Masked autoencoders for self-supervised learning on graphs. _arXiv:2201.02534_, 2022.
* [93] Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, and Jie Tang. Graphmae: Self-supervised masked graph autoencoders. In _KDD_, 2022.
* [94] Lirong Wu, Haitao Lin, Cheng Tan, Zhangyang Gao, and Stan Z Li. Self-supervised learning on graphs: Contrastive, generative, or predictive. _IEEE TKDE_, 2021.
* [95] Mohammad Sadegh Akhondzadeh, Vijay Lingam, and Aleksandar Bojchevski. Probing graph representations. In _AISTATS_, 2023.
* [96] Shengchao Liu, Weitao Du, Yanjing Li, Zhuoxinran Li, Zhiling Zheng, Chenru Duan, Zhiming Ma, Omar Yaghi, Anima Anandkumar, Christian Borgs, et al. Symmetry-informed geometric representation for molecules, proteins, and crystalline materials. _arXiv:2306.09375_, 2023.
* [97] Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can graph neural networks count substructures? In _NeurIPS_, 2020.
* [98] Markus Zopf. 1-wl expressiveness is (almost) all you need. _arXiv:2202.10156_, 2022.

* [99] Alan D McNaught, Andrew Wilkinson, et al. _Compendium of chemical terminology_, volume 1669. Blackwell Science Oxford, 1997.
* [100] Evan J Horn, Brandon R Rosen, Yong Chen, Jiaze Tang, Ke Chen, Martin D Eastgate, and Phil S Baran. Scalable and sustainable electrochemical allylic c-h oxidation. _Nature_, 533(7601):77-81, 2016.
* [101] Akihiko Nakamura and Masahisa Nakada. Allylic oxidations in natural product synthesis. _Synthesis_, 45(11):1421-1451, 2013.
* [102] Liela Bayeh, Phong Q Le, and Uttam K Tambar. Catalytic allylic oxidation of internal alkenes to a multifunctional chiral building block. _Nature_, 547(7662):196-200, 2017.
* [103] Yousaf Ali, Shafida A Hamid, and Umer Rashid. Biomedical applications of aromatic azo compounds. _Mini reviews in medicinal chemistry_, 18(18):1548-1558, 2018.
* [104] Esteban Lanzarotti, Lucas A Defelipe, Marcelo A Marti, and Adrian G Turjanski. Aromatic clusters in protein-protein and protein-drug complexes. _Journal of cheminformatics_, 12(1):1-9, 2020.
* [105] Ana R Gomes, Carla L Varela, Elisario J Tavares-da Silva, and Fernanda MF Roleira. Epoxide containing molecules: A good or a bad drug design approach. _European Journal of Medicinal Chemistry_, 2011:112327, 2020.
* [106] Min Tian, Ying Peng, and Jiang Zheng. Metabolic activation and hepatotoxicity of furan-containing compounds. _Drug Metabolism and Disposition_, 50(5):655-670, 2022.
* [107] Jeffrey B Sperry and Dennis L Wright. Furans, thiophenes and related heterocycles in drug discovery. _Current opinion in drug discovery & development_, 8(6):723-740, 2005.
* [108] Francisco's Saczewski and Lukasz Balewski. Biological activities of guanidine compounds. _Expert opinion on therapeutic patents_, 19(10):1417-1448, 2009.
* [109] Marcelo Z Hernandes, Suellen Melo T Cavalcanti, Diogo Rodrigo M Moreira, Walter Filgueira de Azevedo Junior, and Ana Cristina Lima Leite. Halogen atoms in the modern medicinal chemistry: hints for the drug design. _Current drug targets_, 11(3):303-314, 2010.
* [110] Angeliki P Kourounakis, Dimitrios Xanthopoulos, and Ariadni Tzara. Morpholine as a privileged structure: a review on the medicinal chemistry and pharmacological activity of morpholine containing bioactive molecules. _Medicinal Research Reviews_, 40(2):709-752, 2020.
* [111] Tao Wang, Philipp M Stein, Hongwei Shi, Chao Hu, Matthias Rudolph, and A Stephen K Hashmi. Hydroxylamine-mediated c-c amination via an aza-hock rearrangement. _Nature Communications_, 12(1):1-11, 2021.
* [112] Saloni Kakkar and Balasubramanian Narasimhan. A comprehensive review on biological activities of oxazole derivatives. _BMC chemistry_, 13(1):1-24, 2019.
* [113] Yoshio Hamada. _Role of pyridines in medicinal chemistry and design of BACE1 inhibitors possessing a pyridine scaffold_. InTech Rijeka, 2018.
* [114] Yong Ling, Zhi-You Hao, Dong Liang, Chun-Lei Zhang, Yan-Fei Liu, and Yan Wang. The expanding role of pyridine and dihydropyridine scaffolds in drug design. _Drug Design, Development and Therapy_, 15:4289, 2021.
* [115] Constantinos G Neochoritis, Ting Zhao, and Alexander Domling. Tetrazoles via multicomponent reactions. _Chemical reviews_, 119(3):1970-2042, 2019.
* [116] Mahesh T Chhabria, Shivani Patel, Palmi Modi, and Pathik S Brahmkshatriya. Thiazole: A review on chemistry, synthesis and therapeutic importance of its derivatives. _Current topics in medicinal chemistry_, 16(26):2841-2862, 2016.
* [117] Rashmi Shah and Prabhakar Kumar Verma. Therapeutic importance of synthetic thiophene. _Chemistry Central Journal_, 12(1):1-22, 2018.

**Appendix**

**Table of Contents**

* 1 Overview
* 2 Pre-Training
	* 2.1 GEOM dataset
	* 2.2 GSSL methods
	* 2.3 Hyperparameters search
	* 2.4 Probe models
	* 2.5 Computation efficiency
	* 2.6 Practical guides for future research
* 3 Randomised embeddings
* 4 Substructure
	* 4.1 Discussions on substructure counting
	* 4.2 Detailed description and performance
	* 4.3 Cramer's V
	* 4.4 Distribution
* 5 More results on metrics
	* 5.1 Topological metric
	* 5.2 Spectrum
	* 5.3 Uniformity
* 6 Extending to more GSSL methods and datasets
* 7 Potential ethical and social implications
Overview

**Automated MolGraphEval pipeline.** In the codebase of MolGraphEval, we provide setup scripts (in "env/") for both the docker and conda virtual environment. The end-to-end benchmarking pipeline consists of three modules (in Fig. 7):

* Pre-training the GNN models (other GNN model classes such as MPNN, GCN, GAT are implemented in the codebase);
* Extracting the node/graph/pair-level embeddings from the pre-trained or the randomly initialised GNN models;
* Probing the quality of embeddings with the proposed metrics.

We have meticulously packaged the components of MolGraphEval for ease of access and potential extension. The pre-trained methods are housed in "src/pretrainers", model libraries in "src/models", pre-training and downstream datasets in "src/datasets", and probing metrics in "src/validation". This modular design allows flexibility and extensibility to incorporate new model architectures and datasets. All configurations, specified in YAML files and processed by argument parsers in "src/config", are managed by scripts (with templates provided in "script"). Furthermore, we have implemented loggers to chronicle training/validation/testing curves during both pre-training and probing. For added convenience, templates for visualising embeddings and analysing datasets are also available.

We open-sourced MolGraphEval in https://github.com/hansen7/MolGraphEval.

## Appendix B Pre-Training

We next describe the additional details of the pre-training used in the MolGraphEval benchmark.

### GEOM dataset

We avoid using the updated version of GEOM ('New drug-like molecules' and 'MoleculeNet') to remove the overlap between pre-training and downstream datasets. Compared with other molecular datasets that contain 3D conformation structures, GEOM [46] has the following advantages:

* **Preciseness.** Compared with toolkits like RDKIT or MMFF [77] (used in studies such as ChemRL-GEM [78]), DFT-based calculations (used in GEOM) will provide more precise computation results on the 3D molecular conformation structures [79, 80, 81, 82, 83]. Such errors in the molecular geometries have been proven harmful to property predictions (Appendix B of [84])
* **Comprehensive.** GEOM provides a more comprehensive collection in comparison with other quantum chemistry-based datasets (_e.g._, QM9 [85], Atom3D [86]) in terms of quantity and diversity. In comparison with ZINC15 and SAVI used in this concurrent study [49], GEOM provides accurate 3D conformation structures of molecules, which allows to compare more GSSL methods such as GraphMVP.

### GSSL methods

Self-Supervised Learning (SSL) generally bifurcates into contrastive and generative methodologies, each characterised by its unique supervised signal as highlighted by [87]. Contrastive SSL operates by contrasting representations at the inter-data level, while generative SSL emphasises intra-data level reconstruction. Both strategies have been the subject of comprehensive research.

**Contrastive GSSL** creates multi-layered views of each graph, each capturing different granularity levels, from nodes and subgraphs to the complete graph. It aligns representations for views originating from the same data and distinguishes those from unrelated datasets, targeting a unified

Figure 7: MolGraphEval.

embedding space. The effectiveness of various approaches largely hinges on their view design. For example, InfoGraph contrasts node and graph views, while GraphCL and JOAO delve into graph-level transformations. Further avenues to improve Contrastive GSSL include:

* CLEAR [88] captures graph structure at both global and local scopes, enhancing semantic information granularity and consistency between multiple views;
* PGCL [89] addresses sampling bias by clustering graphs into groups represented by prototype vectors, focusing on the dataset's global semantics;
* iGCL [90] employs a Siamese architecture to generate positive samples sans data augmentation. The proposed ID loss eschews negative sampling while promoting feature-wise discriminability.

**Generative GSSL** zeroes in on reconstructing the graph structure, striving to derive representations that capture the core characteristics of the data. Noteworthy examples in this category include EdgePred and AttrMask, which predict adjacency matrices and mask tokens, respectively. Meanwhile, GPT-GNN employs an auto-regressive approach tailored for holistic graph reconstruction. In line with this methodology, masked graph autoencoders, as cited in [91, 92, 93], have garnered considerable attention.

GROVER-Motif leverages domain-specific knowledge to extract motifs from molecules, assigning SSL the role of predicting motif presence. Diverging from the paradigms of contrastive and generative GSSL, recent explorations like [94] categorise this approach as predictive GSSL. In this framework, the supervisory signals are derived from self-generated labels.

There is a limited body of work dedicated to understanding GSSL methods. In a notable study, Akhondzadeh et al. [95] explored the use of probing tasks to measure and contrast the richness of graph representations derived from various models. A significant revelation from this study is that transformer-based GNNs capture chemically pertinent information more effectively than message-passing GNNs. Further integrating the data from 3D structures, Liu et al. [96] presented Geom3D -- a comprehensive framework for benchmarking geometric representation learning techniques applicable to molecules, proteins, and materials. This framework encompasses 16 cutting-edge geometric models and evaluates their efficacy across 46 diverse scientific challenges, spanning small molecules, proteins, and crystalline substances. An innovative aspect of Geom3D is its approach to categorising geometric models into three groups: invariant, spherical frame equivariant, and vector frame equivariant.

### Hyperparameters search

We search the optimal hyperparameters of pre-training methods, details are summarised in Tables 7 to 9. We select the best hyperparameter of each GSSL method based on their averaged score on downstream datasets (in Table 1, linear models).

\begin{table}
\begin{tabular}{l|l|l} \hline \hline
**Method** & **Hyperparameters** & **\# Models** \\ \hline EdgePred & Learning Rate & 15 \\ \hline AttrMask & Mask Rate, Learning Rate & 300 \\ \hline GPT-GNN & Learning Rate & 15 \\ \hline InfoGraph & Learning Rate & 15 \\ \hline GROVER & Learning Rate, “Contextual” or “Motif”-based Loss & 30 \\ \hline Cont.Pred & Learning Rate, Context Size, \# Negative Samples & 300 \\ \hline GraphCL & Learning Rate, Aug Strength, Aug Prob & 360 \\ \hline JOAO & Learning Rate, Gamma, Loss Version (V1 or V2) & 300 \\ \hline GraphMVP & Learning Rate, Temperature, Alpha2, \# Conformer & 540 \\ \hline Total & & **1875** \\ \hline \hline \end{tabular}
\end{table}
Table 7: Hyperparameters search space of GSSL.

We provide details in calculating the **1875** GNN configurations in Table 7. The number of probe models is calculated as follows: 1875 * 8 (MPP datasets) * 2 (Fix, FT) * 3 (Seed) + 9 (GSSL, Optimal) * 10 (Topological Metrics) * 3 (Seed) + 9 (GSSL, Optimal) * 24 (Substructure) * 3 (Seed) = **90918**. It takes over 4 terabytes to save these pre-trained models.

### Probe models

Ideally, probe models should be neither too simple to capture the representation's information, nor too powerful to learn precise property prediction themselves. If overly powerful, the probe's performance might not accurately reflect the information embedded in the representations. **In light of these complexities, we select a linear model as our probe, aligning with the choice prevalent in most probe studies.**

### Computation efficiency

We present the number of trainable parameters alongside the average training time per epoch (utilising a single A100 GPU) for each GSSL method in Table 10.

\begin{table}
\begin{tabular}{l|l} \hline \hline
**Hyperparameters** & **Range** \\ \hline Learning Rate, all but GraphMVP & [0.01, 0.005, 0.001, 0.0005, 0.0001] \\ \hline Learning Rate, GraphMVP & [0.001, 0.0005, 0.0001] \\ \hline Mask Rate & [0.05, 0.10,..., 0.95] \\ \hline Context Size & [2, 3, 4, 5] \\ \hline \# Negative Samples & [1, 2, 3, 4, 5] \\ \hline Aug Strength & [0.2, 0.4, 0.6, 0.8] \\ \hline Aug Probability & [0.1, 0.2,..., 1.0] \\ \hline Gamma & [0.1, 0.2,..., 1.0] \\ \hline Temperature & [0.1, 0.2, 0.5, 1, 2] \\ \hline Alpha2 & [0.1, 1, 10] \\ \hline \# Conformer & [1, 5, 10, 20] \\ \hline \hline \end{tabular}
\end{table}
Table 8: Range of Grid Search on Hyperparameters Space.

\begin{table}
\begin{tabular}{l|l} \hline \hline
**Method** & **Optimal Hyperparameters (left: probing / right: fine-tuning)** \\ \hline EdgePred & Learning Rate=1e-2/1e-2 \\ \hline AttrMask & Learning Rate=1e-4/5e-4, Mask Rate=0.85/0.50 \\ \hline GPT-GNN & Learning Rate=1e-2/1e-4 \\ \hline InfoGraph & Learning Rate=1e-4/1e-4 \\ \hline GROVER & Learning Rate=1e-4/1e-3, “Motif”/“Contextual”-based Loss \\ \hline Cont.Pred & Learning Rate=1e-3/5e-3, Context Size=1/1, \# Negative Samples=5/1 \\ \hline GraphCL & Learning Rate=1e-3/1e-3, Aug Strength=0.2/0.6, Aug Prob=0.8/0.5, \\ \hline JOAO & Learning Rate=1e-3/1e-3, Gamma=0.9/0.6, “V1”/“V1”-version Loss \\ \hline GraphMVP & Learning Rate=5e-4/5e-4, Alpha2=0.1/10.0, Temperature=0.1/0.2, \# Conformer=5/5 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Optimal hyperparameters based on **linear probing and finetuning scores** on MPP tasks.

### Practical guides for future research

In summary, for prospective advancements in Graph SSL, the following practical guidelines should be considered:

* Develop novel pretext tasks that emphasise beneficial invariances and geometry, as unveiled by MolGraphEval probes. This includes tasks that enhance substructure modelling and preserve local topology.
* Relying solely on either probing or fine-tuning may not provide a comprehensive understanding. It's noteworthy that weak probing performance doesn't necessarily correlate with subpar fine-tuning outcomes (_e.g._, GraphMVP). The method of choice should be based on the specific downstream task, whether it's property prediction, generation, optimisation, or interaction modelling.
* Innovate superior data augmentation techniques specifically for molecular graphs to produce valuable views for contrastive learning. Probes can serve as an instrumental means to assess the quality of these augmentations.
* The prevailing notion of achieving a more uniform embedding space is helpful doesn't always hold true in the context of molecular graphs.
* Additionally, probes can be harnessed as a potent tool to scrutinise the impacts of varied negative sampling and augmentation strategies, exemplified by the comparison between GraphCL and JOAO.

## Appendix C Randomised embeddings

**How GNN models are initialised.** We first analyse how weights in the GNNs are initialised (PyTorch and PyG).

* **Edge Embedding Layers** uses 'xavier uniform', essentially samples from uniform distribution
* **GNN Layers** in fact only have MLP weights (see PyG Doc), same initialisation as Linear layers.
* **Linear Layers** samples from uniform distribution for both weight and bias (PyTorch Doc)

Since all the weights (Edge embedding layers, GNN layers, and Linear layers) in the GINs are extracted from some uniform distribution of some positive ranges. As the GIN layer essentially consists of multiplications and additions, the expected statistics of the node embeddings from randomised GINs are proportional to the number of connected neighbours (i.e., node degrees). Therefore, the randomised embeddings form discriminative clusters in Fig. 4. As for other node-level metrics (in Fig. 8), we don't observe a good clustering formed from randomised embeddings.

\begin{table}
\begin{tabular}{l|c|c} \hline \hline
**Method** & **Number of Parameters (Million)** & **Training time (Second)** \\ \hline EdgePred & 7.462 & 101 \\ \hline AttrMask & 7.606 & 38 \\ \hline GPT-GNN & 7.606 & 972 \\ \hline InfoGraph & 7.823 & 40 \\ \hline GROVER & 7.566 & 39 \\ \hline Cont.Pred & 12.00 & 202 \\ \hline GraphCL & 8.186 & 65 \\ \hline JOAO & 8.186 & 382 \\ \hline GraphMVP & 15.84 & 119 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Computational efficiency of GSSL methods

[MISSING_PAGE_FAIL:21]

[MISSING_PAGE_EMPTY:22]

imidazole & Imidazole is an organic compound with the formula C3N2H4. It is a white or colourless solid that is soluble in water, producing a mildly alkaline solution. This ring system is present in important biological building blocks, such as histidine and the related hormone histamine. Many drugs contain an imidazole ring, such as certain antifungal drugs, the nitroimidazole series of antibiotics, and the sedative midazolam[wikipedia].

\begin{tabular}{p{56.9pt}|p{284.5pt}} \hline imide & In organic chemistry, an imide is a functional group consisting of two acyl groups bound to nitrogen. Being highly polar, imides exhibit good solubility in polar media. The N-H center for imides derived from ammonia is acidic and can participate in hydrogen bonding. Unlike the structurally related acid anhydrides, they resist hydrolysis and some can even be recrystallized from boiling water[wikipedia]. Immunomodulatory imide drugs (IMiDs) are a class of immunomododulatory drugs (drugs that adjust immune responses) containing an imide group. \\ \hline lactam & A beta-lactam (\(\beta\)-lactam) ring is a four-membered lactam. The \(\beta\)-lactam ring is part of the core structure of several antibiotic families, the principal ones being the penicillins, cephalosporions, carbapenems, and monobactams, which are, therefore, also called \(\beta\)-lactam antibiotics [wikipedia]. \\ \hline morpholine & Morpholine is an organic chemical compound having the chemical formula O(CH2CH2)2NH. Morpholine is a heterocycle featured in numerous approved and experimental drugs as well as bioactive molecules. It is often employed in the field of medicinal chemistry for its advantageous physicochemical, biological, and metabolic properties, as well as its facile synthetic routes [110]. \\ \hline NO (hydrox-ylamine) & Hydroxylamine is an organic compound with the formula NH 2OH. The material is a white crystalline, hygroscopic compound. Hydroxylamines and their derivatives are powerful aminating reagents, which are often used for arena C-H and X-H aminations (X=O, N, S, P) as well as Schmidt-type reaction46, serving as alternative ways to introduce amino groups on various chemical skeletons [111]. \\ \hline oxazole & Oxazoles is a doubly unsaturated 5-membered ring having one oxygen atom at position 1 and a nitrogen at position 3 separated by a carbon in-between. Substitution pattern in oxazole derivatives play a pivotal role in delineating the biological activities like antimicrobial, anticancer, antitubercular anti-inflammatory, antidiabetic, antiobesity and antioxidant etc [112]. \\ \hline piperdine & Piperidine is an organic compound with the molecular formula (CH2)5NH. This heterocyclic amine consists of a six-membered ring containing five methylene bridges (-CH2-) and one amine bridge (-NH-). Piperidine and its derivatives are ubiquitous building blocks in pharmaceuticals[26] and fine chemicals. The piperidine structure is found in, for example: Icaridin, SSRIs, stumulants and nootropics, SERM etc. Piperidine is also commonly used in chemical degradation reactions, such as the sequencing of DNA in the cleavage of particular modified nucleotides. Piperidine is also commonly used as a base for the deprotection of Fmoc-amino acids used in solid-phase peptide synthesis [wikipedia]. \\ \hline piperazine & Piperazine is an organic compound that consists of a six-membered ring containing two nitrogen atoms at opposite positions in the ring. Many currently notable drugs contain a piperazine ring as part of their molecular structure ("Substituted piperazine"). Examples include: Antianginals, Antidepressants, Antihistamines etc [wikipedia]. \\ \hline pyridine & Pyridine is a basic heterocyclic organic compound with the chemical formula C5H5N. Pyridine moieties are often used in drugs because of their characteristics such as basicity, water solubility, stability, and hydrogen bond-forming ability, and their small molecular size [113]. Pyridine-based ring systems are one of the most extensively used heterocycles in the field of drug design, primarily due to their profound effect on pharmacological activity, which has led to the discovery of numerous broad-spectrum therapeutic agents [114]. \\ \hline \end{tabular}

\begin{table}
\begin{tabular}{l|l l l l l l l l l l l l l} \hline \hline  & \multicolumn{2}{l}{lastam morpholine} & \multicolumn{2}{l}{NO} & \multicolumn{2}{l}{oxazole piperdine piperz} & \multicolumn{2}{l}{pyridine tetrazole thiazole thiophene} & \multicolumn{2}{l}{urea} \\ \hline Random & 0.018 & 0.031 & 0.022 & 0.009 & 0.212 & 0.058 & 0.176 & 0.014 & 0.040 & 0.052 & 0.045 \\ \hline EdgePred & 0.016 & 0.020 & 0.022 & 0.008 & 0.182 & 0.048 & 0.158 & 0.014 & 0.039 & 0.046 & 0.041 \\ attrMask & 0.016 & 0.028 & 0.022 & 0.008 & 0.192 & 0.053 & 0.174 & 0.014 & 0.038 & 0.044 & 0.044 \\ GPT-GNN & 0.012 & 0.021 & 0.022 & 0.008 & 0.177 & 0.049 & 0.128 & 0.014 & 0.039 & 0.041 & 0.037 \\ InfoGraph & 0.012 & 0.026 & 0.024 & 0.007 & 0.185 & 0.048 & 0.127 & 0.016 & 0.029 & 0.036 & 0.046 \\ Cont.Pred & 0.016 & 0.031 & 0.022 & 0.008 & 0.214 & 0.059 & 0.170 & 0.014 & 0.039 & 0.047 & 0.045 \\ GROVER & 0.014 & 0.022 & 0.028 & **0.004** & 0.158 & 0.043 & 0.096 & **0.005** & **0.018** & **0.018** & **0.014** \\ GraphCL & 0.014 & **0.019** & 0.021 & 0.006 & 0.169 & **0.031** & 0.084 & 0.009 & 0.023 & **0.018** & 0.023 \\ JOAO & 0.014 & 0.021 & **0.020** & 0.006 & 0.168 & 0.035 & **0.082** & 0.009 & 0.022 & **0.018** & 0.025 \\ GraphMVP & **0.011** & 0.021 & 0.022 & 0.006 & **0.155** & 0.038 & 0.129 & 0.009 & 0.027 & 0.025 & 0.028 \\ \hline SSL Worse (\#) & 0 & 0 & 2 & 0 & 1 & 1 & 0 & 1 & 0 & 0 & 1 \\ \hline \hline \end{tabular}
\end{table}
Table 13: Substructure detection, part II. We **bold** the best and underline the worst scores.

\begin{table}
\begin{tabular}{l|l} \hline \hline tetrazole & Tetrazoles are a class of synthetic organic heterocyclic compound, consisting of a 5-member ring of four nitrogen atoms and one carbon atom. Tetrazole derivatives are a prime class of heterocycles, very important to medicinal chemistry and drug design due to not only their bioisosterism to carboxylic acid and amide moieties but also to their metabolic stability and other beneficial physicochemical properties [115].

\begin{table}
\begin{tabular}{l|l} \hline \hline  & \multicolumn{1}{l}{alleplic  amide  amide } & \multicolumn{1}{l}{azo  benzene  epoxide } & \multicolumn{1}{l}{ether  furan } & \multicolumn{1}{l}{guanido  halogen } & \multicolumn{1}{l}{imidazole } & \multicolumn{1}{l}{imide} \\ \hline Random & 0.959 & 16.917 & 0.054 & 0.020 & 1.100 & 0.024 & 2.024 & 0.036 & 0.126 & 1.127 & 0.080 & 0.062 \\ \hline EdgePred & 0.780 & 14.173 & 0.046 & 0.018 & 0.797 & 0.021 & 1.608 & 0.033 & 0.098 & 0.939 & 0.074 & 0.031 \\ AttMask & 0.926 & 14.703 & 0.047 & 0.019 & 0.976 & 0.022 & 1.742 & 0.028 & 0.112 & 0.501 & 0.077 & 0.029 \\ GPT-GNN & 0.872 & 15.629 & 0.044 & 0.017 & 0.783 & 0.021 & 1.912 & 0.023 & 0.117 & 0.341 & 0.077 & 0.037 \\ InfoGraph & 0.740 & 6.747 & 0.050 & 0.019 & 0.583 & 0.022 & 1.128 & 0.021 & 0.086 & 0.706 & 0.062 & 0.038 \\ Cont.Pred & 1.040 & 16.636 & 0.053 & 0.020 & 0.980 & 0.023 & 1.787 & 0.034 & 0.126 & 1.075 & 0.078 & 0.033 \\ GROVER & 0.715 & **6.576** & **0.025** & **0.008** & 0.558 & 0.023 & **0.957** & **0.008** & **0.064** & **0.298** & 0.069 & **0.021** \\ GraphCL & 0.652 & 7.598 & 0.039 & 0.016 & **0.525** & 0.023 & 1.077 & 0.012 & 0.080 & 0.319 & 0.051 & 0.026 \\ JOAO & **0.654** & 7.926 & 0.043 & 0.015 & 0.531 & 0.023 & 1.071 & 0.013 & 0.085 & 0.310 & **0.048** & 0.026 \\ GraphMVP & 0.905 & 6.992 & 0.043 & 0.017 & 0.649 & **0.019** & 1.037 & 0.019 & 0.084 & 0.311 & 0.060 & 0.027 \\ \hline SSL Worse (\#) & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ \hline \hline \end{tabular}
\end{table}
Table 12: Substructure detection, part I. We **bold** the best and underline the worst scores.

### Cramer's V

**Cramer's V** quantifies the strength of the association between the molecular substructure counts (_i.e._, chemical fragments) and their biochemical properties. It is defined as:

\[V=\sqrt{\chi^{2}/\left(n\cdot\min(k-1,r-1)\right)}=\sqrt{\chi^{2}/n}\quad(r\equiv 2)\] (5)

where \(n\) is the sample size, \(k\) and \(r\) are the total number of substructure counts and property categories (binary), respectively. The Chi-squared statistics \(\chi^{2}\) is then calculated as:

\[\chi^{2}=\sum_{i,j}\left(n_{(i,j)}-n_{(i,\cdot)}\cdot n_{(\cdot,j)}/n\right)^{ 2}\Big{/}\left(n_{(i,\cdot)}\cdot n_{(\cdot,j)}/n\right)\] (6)

where \(n_{(i,j)}\) is the total occurrence for the pair of \((i,j)\). Here \(i\) is the specific count of a certain substructure, and \(j\) represents the certain outcome of a molecular biochemical property. Cramer's V value ranges from 0 to 1, representing the associated strength between two categorical variables.

### Distribution

We plot the distribution of 24 molecular substructures, the y-axis (_i.e._, counts) is log scale. Alongside each substructure name, we includes its average counts (_i.e._, ground truth). While certain substructures, such as amidine and azo, are infrequently found in the molecules from the eight downstream datasets, we note that others like Amide and Benzene are prevalent. These substructures are recognised for their significant relevance to molecular properties, as detailed in Table 4.

**Alylic Oxide**.

**Amide**.

\begin{table}
\begin{tabular}{l|c c c c c c c c|c c} \hline \hline Pre-training & BBBP & Tox21 & ToxCast & Sider & ClinTox & MUV & HIV & Bace & Avg (Task) & Avg (Data) \\ \hline allylic & 0.1602 & 0.1345 & 0.1156 & 0.1276 & 0.0935 & 0.0413 & 0.0280 & 0.1186 & 0.1144 & 0.1024 \\ amide & 0.2692 & 0.0490 & 0.0858 & 0.1841 & 0.1326 & 0.0235 & 0.0689 & 0.2556 & 0.0881 & 0.1336 \\ amidine & 0.0360 & 0.0291 & 0.0412 & 0.0323 & 0.0158 & 0.0117 & 0.0396 & 0.1328 & 0.0399 & 0.0423 \\ azo & 0.0400 & 0.0399 & 0.0393 & 0.0277 & 0.0123 & 0.0007 & 0.2082 & - & 0.0384 & 0.0526 \\ benzene & 0.1476 & 0.1632 & 0.1691 & 0.1149 & 0.1112 & 0.0289 & 0.1374 & 0.1091 & 0.1630 & 0.1227 \\ epoxide & 0.0273 & 0.0481 & 0.0449 & 0.0300 & 0.0049 & 0.0005 & 0.0086 & - & 0.0437 & 0.0235 \\ ether & 0.2314 & 0.0694 & 0.1060 & 0.1069 & 0.1023 & 0.0185 & 0.0498 & 0.1821 & 0.1034 & 0.1083 \\ furan & 0.0635 & 0.0257 & 0.0387 & 0.0227 & 0.0061 & 0.0311 & 0.0148 & 0.0135 & 0.0375 & 0.0270 \\ guanido & 0.0765 & 0.0201 & 0.0509 & 0.0715 & 0.0286 & 0.0057 & 0.0094 & 0.1088 & 0.0499 & 0.0464 \\ halogen & 0.1488 & 0.0849 & 0.1827 & 0.0773 & 0.0908 & 0.0143 & 0.0347 & 0.2353 & 0.1721 & 0.1086 \\ imidazole & 0.0601 & 0.0427 & 0.0492 & 0.0460 & 0.1212 & 0.0102 & 0.0398 & 0.1280 & 0.0483 & 0.0622 \\ imide & 0.0951 & 0.0246 & 0.0401 & 0.0428 & 0.0518 & 0.0094 & 0.0188 & - & 0.0392 & 0.0404 \\ lactam & 0.4263 & 0.0184 & 0.0116 & 0.0646 & 0.0543 & 0.0006 & 0.0048 & - & 0.0182 & 0.0830 \\ morpholine & 0.0512 & 0.0126 & 0.0343 & 0.0268 & 0.0425 & 0.0068 & 0.0101 & 0.0668 & 0.0329 & 0.0314 \\ N\_O & 0.0438 & 0.0195 & 0.0467 & 0.0391 & 0.0709 & 0.0195 & 0.0144 & 0.0537 & 0.0452 & 0.0385 \\ oxazole & 0.0126 & 0.0184 & 0.0321 & 0.0359 & 0.0123 & 0.0079 & 0.0080 & 0.0364 & 0.0312 & 0.0205 \\ piperdine & 0.1450 & 0.0305 & 0.0844 & 0.0575 & 0.0418 & 0.0079 & 0.0226 & 0.0935 & 0.0803 & 0.0604 \\ piperze & 0.0509 & 0.0214 & 0.0421 & 0.0776 & 0.0648 & 0.0111 & 0.0192 & 0.0063 & 0.0424 & 0.0367 \\ pyridine & 0.0598 & 0.0402 & 0.0549 & 0.0338 & 0.0833 & 0.0129 & 0.0300 & 0.1747 & 0.0529 & 0.0612 \\ tetrazole & 0.1161 & 0.0158 & 0.0251 & 0.0300 & 0.0286 & 0.0083 & 0.0123 & 0.0334 & 0.0247 & 0.0337 \\ thiazole & 0.1389 & 0.0521 & 0.0345 & 0.0445 & 0.0183 & 0.0118 & 0.0173 & 0.0539 & 0.0348 & 0.0464 \\ thiophene & 0.0356 & 0.0467 & 0.0472 & 0.0315 & 0.0113 & 0.0166 & 0.0081 & 0.0438 & 0.0456 & 0.0301 \\ urea & 0.0790 & 0.0236 & 0.0506 & 0.0471 & 0.0268 & 0.0079 & 0.0329 & 0.0516 & 0.0489 & 0.0399 \\ \hline \hline \end{tabular}
\end{table}
Table 14: Cramer’s V between molecular substructure counts and downstream properties.

**Amidine**.

**AZO**.

**Benzene**.

**Epoxide**.

**Ether**.

**Furan**.

**Guanido**.

[MISSING_PAGE_EMPTY:27]

**Piperzine**.

**Pyridine**.

**Tetrazole**.

**Thiazole**.

**Thiophene**.

**Urea**.

## Appendix E More results on metrics

### Topological metric

We have depicted the distribution of structural metrics and substructures with respect to the downstream datasets using histograms. Please note that the vertical axes may occasionally be represented on a logarithmic scale.

**Node Degree**.

[MISSING_PAGE_EMPTY:29]

### Spectrum

Our observations suggest a positive correlation between the magnitudes of singular values in pre-trained embedding spaces and their performance in the downstream MPP tasks.

We provide more visualisations of the spectrum of the embedding space from different datasets.

Figure 10: Spectrum of the GSSL embedding space on Toxcast dataset, Left: Node; Right: Graph.

\begin{table}
\begin{tabular}{l|c|c|c|c|c} \hline \hline Dimension & 1 & 50 & 100 & 200 & 300 \\ \hline Random & 1.96e-01/4.81e-02 & 8.14e-06/3.35e-07 & 1.08e-06/3.87e-08 & 9.10e-08/2.20e-09 & 2.44e-09/4.08e-11 \\ \hline EdgePred & 1.25e+00/2.12e-01 & 5.30e-03/5.96e-04 & 3.92e-04/3.56e-05 & 1.31e-05/7.10e-07 & 1.97e-07/6.79e-09 \\ AttrMask & 9.97e+01/7.57e+00 & 9.71e-03/1.16e-03 & 2.06e-03/2.02e-04 & 2.95e-04/1.96e-05 & 1.86e-05/9.00e-07 \\ GPT-GNN & 7.78e+01/2.46e+01 & 4.21e-03/6.04e-04 & 6.26e-04/7.11e-05 & 5.32e-05/4.40e-06 & 1.29e-06/6.50e-08 \\ InfoGraph & 1.29e+02/6.56e+01 & 9.52e-01/3.23e-01 & 2.68e-01/6.54e-02 & 2.43e-02/3.38e-03 & 2.80e-05/2.69e-06 \\ ContrPred & 1.81e+01/5.89e+00 & 3.40e-03/7.25e-03 & 4.36e-03/2.46e-06 & 1.65e-06/9.08e-08 & 1.14e-08/9.92e-10 \\ GROVER & 2.21e+02/9.86e+01 & 2.18e+00/5.1e-01 & 1.13e-01/2.04e-02 & 1.68e-01/9.79e-03 & 1.51e-03/1.36e-04 \\ GraphCL & 7.63e+01/3.40e+01 & 4.26e-01/7.27e-02 & 1.45e-02/1.76e-03 & 6.84e-04/5.11e-05 & 2.42e-05/1.37e-06 \\ JOAO & 8.12e+01/3.58e+01 & 4.16e-01/7.89e-02 & 2.18e-02/2.67e-03 & 8.63e-04/6.74e-05 & 2.64e-05/1.52e-06 \\ GraphMVP & 2.07e+01/1.17e+01 & 2.57e-01/8.22e-02 & 1.94e-02/4.00e-03 & 4.39e-05/5.75e-06 & 1.76e-06/1.71e-07 \\ \hline Correlation & 0.661/0.781 & 0.806/0.927 & 0.879/0.903 & 0.697/0.770 & 0.794/0.794 \\ \hline \hline \end{tabular}
\end{table}
Table 15: Benchmarking the magnitude of the singular values (node/graph) in the spectrum, BBBP.

Figure 9: Spectrum of the GSSL embedding space on Tox21 dataset, Left: Node; Right: Graph.

Figure 11: Spectrum of the GSSL embedding space on Sider dataset, Left: Node; Right: Graph.

Figure 14: Spectrum of the GSSL embedding space on HIV dataset, Left: Node; Right: Graph.

Figure 12: Spectrum of the GSSL embedding space on Clintox dataset, Left: Node; Right: Graph.

Figure 13: Spectrum of the GSSL embedding space on MUV dataset, Left: Node; Right: Graph.

Figure 15: Spectrum of the GSSL embedding space on BACE dataset, Left: Node; Right: Graph.

### Uniformity

We provide more comprehensive results on the uniformity metric of the embedding space in Table 16. The upper bound for uniformity is zero, while the lower bound, as derived from Corollary 1 in the Appendix of [6], is both data and hyperparameter-dependent.

## Appendix F Extending to more GSSL methods and datasets

We demonstrate that the proposed MolGraphEval benchmark can readily be extended to incorporate more pre-training methods, such as GraphMAE and GRCL, as well as larger datasets, like the 2M ZINC15. It's pertinent to note that the downstream datasets can also be employed for pre-training. Importantly, we observe that thorough optimisation of pre-training hyperparameters can serve as a crucial factor for performance improvements in pre-training, notwithstanding the advancements in the design of pre-training tasks.

## Appendix G Potential ethical and social implications

There are a few potential ethical considerations that could arise from the molecular graph representation learning methods discussed in :

**Bias and fairness**: We recognise that the molecular datasets used for pre-training and benchmarking may inadvertently encode biases or lack diversity, which could lead to models that unfairly under

\begin{table}
\begin{tabular}{l|c c c c c c c|c} \hline \hline Dataset & BBBP & Tox21 & ToxCast & Sider & ClinTox & MUV & HIV & Bace \\ \hline Random & -0.214 & -0.315 & -0.313 & -0.282 & -0.242 & -0.128 & -0.245 & -0.079 \\ \hline EdgePred & -2.319 & -2.969 & -2.909 & -2.424 & -2.384 & -2.634 & -2.804 & -1.653 \\ AttrMask & -8.332 & -9.348 & -9.560 & -7.786 & -8.399 & -9.241 & -9.979 & -6.389 \\ GPT-GNN & -5.714 & -5.701 & -5.773 & -5.550 & -5.708 & -5.279 & -5.068 & -4.977 \\ InfoGraph & -10.065 & -10.925 & -11.310 & -7.623 & -9.831 & -15.337 & -13.506 & -9.620 \\ Cont.Pred & -2.636 & -3.074 & -3.209 & -2.748 & -2.780 & -2.390 & -2.865 & -1.708 \\ GROVER & -10.208 & -12.142 & -12.356 & -11.907 & -9.975 & -15.772 & -14.244 & -10.512 \\ GraphCL & -10.010 & -11.073 & -11.458 & -10.155 & -9.845 & -13.390 & -12.530 & -8.513 \\ JOAO & -10.000 & -10.955 & -11.360 & -10.012 & -9.846 & -13.450 & -12.505 & -8.523 \\ GraphMVP & -8.864 & -9.770 & -9.863 & -8.782 & -8.825 & -12.405 & -11.301 & -7.540 \\ \hline Correlation & 0.842 & -0.600 & 0.097 & -0.210 & 0.309 & 0.169 & 0.821 & 0.285 \\ p-value & 0.002 & 0.067 & 0.789 & 0.559 & 0.384 & 0.641 & 0.004 & 0.425 \\ \hline \hline \end{tabular}
\end{table}
Table 17: **Evaluating GSSL methods on molecular property prediction tasks, on 2M ZINC15.** For each downstream dataset, we report the mean and standard deviation of the ROC-AUC scores over three random scaffold splits. The performance scores are based on the fixed pre-trained embeddings with linear probe models, we also report the average ROC-AUC scores with fine-tuned pre-trained GNN on MPP tasks (“Avg (FT)”).

\begin{table}
\begin{tabular}{l|c c c c c c c c|c} \hline \hline  & BBBP & Tox21 & ToxCast & Sider & ClinTox & MUV & HIV & Bace & Avg & Avg (FT) \\ \hline \# Molecules & 2,039 & 7,831 & 8,575 & 1,427 & 1,478 & 93,087 & 41,127 & 1,513 & – & – \\ \# Tasks & 1 & 12 & 617 & 27 & 2 & 17 & 1 & 1 & – & – \\ \hline Random & 50.7 \(\pm\)2.5 & 64.9 \(\pm\)0.5 & 53.2 \(\pm\)0.3 & 53.2 \(\pm\)1.1 & 63.1 \(\pm\)2.3 & 62.1 \(\pm\)1.3 & 66.1 \(\pm\)0.7 & 63.4 \(\pm\)1.8 & 59.60 & 66.16 \\ \hline AttrMask & 49.8 \(\pm\)0.6 & 66.7 \(\pm\)0.3 & 52.9 \(\pm\)0.4 & 53.8 \(\pm\)1.7 & 62.2 \(\pm\)2.9 & 52.8 \(\pm\)1.7 & 69.0 \(\pm\)1.4 & 66.6 \(\pm\)4.9 & 59.22 & 69.49 \\ GraphCL & 64.9 \(\pm\)0.5 & 70.5 \(\pm\)0.6 & 56.1 \(\pm\)0.2 & 58.0 \(\pm\)1.4 & 63.4 \(\pm\)3.1 & 61.2 \(\pm\)1.6 & 75.6 \(\pm\)0.9 & 70.9 \(\pm\)3.8 & 65.07 & 70.09 \\ GraphME & 58.6 \(\pm\)2.3 & 64.4 \(\pm\)0.6 & 55.3 \(\pm\)0.1 & 57.0 \(\pm\)1.8 & 67.4 \(\pm\)1.9 & 57.3 \(\pm\)1.5 & 71.6 \(\pm\)1.2 & 51.6 \(\pm\)3.6 & 60.41 & 68.91 \\ GRCL & 61.8 \(\pm\)2.1 & 67.5 \(\pm\)0.4 & 54.3 \(\pm\)0.4 & 55.8 \(\pm\)1.3 & 62.0 \(\pm\)2.4 & 61.9 \(\pm\)1.4 & 68.8 \(\pm\)0.9 & 68.6 \(\pm\)1.5 & 62.58 & 67.64 \\ \hline \hline \end{tabular}
\end{table}
Table 16: **Evaluating GSSL methods on uniformity**.

perform on certain data subsets. We have made efforts to use representative datasets, but further testing on diverse molecules and monitoring for fairness issues remain important future directions.

Relatedly, an inherent risk with data-driven methods is "stereotyping" along chemical lines, where models reinforce historical assumptions. We believe conscious testing on new classes of molecules can mitigate this. The goal is to build broad molecular understanding.

**Dual use**: As with many advances in chemistry ML, dual use concerns exist. While we aim to accelerate beneficial therapeutics, we strongly discourage misuse of these methods to design harmful substances. We support efforts toward responsible AI practices.

**Environmental impact**: Training large molecular graph models consumes significant computational resources and energy, efforts should be made to minimise the environmental impact, _e.g._, by using efficient methods and carbon-neutral compute.

**Privacy**: While molecular graph data is less sensitive than data about individuals, any efforts to link models back to original private datasets should be handled carefully.

Overall, we make efforts to align research practices with principles of sharing benefits, avoiding harm, mitigating biases, and practising transparency. We welcome feedback from the community as we continue working to address these ethical dimensions. Our goal is innovation that aligns with broad social good.

\begin{table}
\begin{tabular}{l|c c c|c c c|c c c} \hline \hline  & \multicolumn{3}{c|}{Node} & \multicolumn{3}{c|}{Pair} & \multicolumn{3}{c}{Graph} \\ \cline{2-10}  & Degree & Cent. & Cluster & Link & Jaccard & Katz & Diameter & Conn. & Cycle & Assort. \\ \hline Random & 0.001 & 0.008 & 0.003 & 0.078 & 0.012 & 0.017 & 177.924 & 0.087 & 2.933 & 0.029 \\ \hline AttrMask & 0.058 & 0.010 & 0.003 & 0.082 & 0.013 & 0.020 & 142.832 & 0.071 & 3.232 & 0.027 \\ GraphCL & 0.063 & 0.009 & 0.003 & 0.079 & 0.018 & 0.022 & 87.426 & 0.064 & 1.716 & 0.017 \\ GraphMAE & 0.032 & 0.011 & 0.004 & 6.885 & 0.215 & 0.038 & 123.677 & 0.066 & 3.094 & 0.025 \\ RGCL & 0.356 & 0.011 & 0.003 & 0.075 & 0.013 & 0.017 & 124.869 & 0.067 & 3.528 & 0.029 \\ \hline \hline \end{tabular}
\end{table}
Table 18: **Benchmarking topological properties, on 2M ZINC15.** We report the mean square error or the cross entropy on eight datasets (_i.e._, smaller is better).