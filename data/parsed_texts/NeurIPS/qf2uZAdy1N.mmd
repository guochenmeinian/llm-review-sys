# Reinforcement Learning under Latent Dynamics:

Toward Statistical and Algorithmic Modularity

 Philip Amortila

philipa4@illinois.edu

&Dylan J. Foster

dylanfoster@microsoft.com

&Nan Jiang

nanjiang@illinois.edu

&Akshay Krishnamurthy

akshaykr@microsoft.com

&Zakaria Mhammedi

mhammedi@google.com

The full (author-recommended) version of this paper can be found at: https://arxiv.org/pdf/2410.17904.

[MISSING_PAGE_FAIL:2]

This property, which we refer to as _algorithmic modularity_, enables modular, greatly simplified algorithm design, allowing one to use an arbitrary base algorithm for the base MDP class to solve the corresponding latent-dynamics problem. Algorithmic modularity is a stronger property than mere statistical modularity, and thus is subject to our statistical lower bound. Accordingly, we consider two settings that sidestep the lower bound through additional feedback and modeling assumptions. Our first algorithmic result considers _hindsight observability_[1], where latent states are revealed during training, but not at deployment (Theorem 4.1). Our second considers stronger function approximation conditions that enable the estimation of _self-predictive latent models_[1] through representation learning (Theorem A.1). Both results are _fully modular_: they transform _any_ sample-efficient algorithm for the base MDP class into a sample-efficient algorithm for the latent-dynamics setting. Thus, they constitute the first _general-purpose_ algorithms for RL under latent dynamics.

Together, we believe our results can serve as a foundation for further development of practical, general-purpose algorithms for RL under latent dynamics. To this end, we highlight a number of fascinating and challenging open problems for future research (Section 5).

## 2 Reinforcement Learning under General Latent Dynamics

In this section we formally introduce our framework, _reinforcement learning under general latent dynamics._

MDP preliminaries.We consider an episodic finite-horizon online reinforcement learning setting. With \(H\) denoting the horizon, a Markov decision process (MDP) \(M^{\star}=\big{\{}\mathcal{X},\mathcal{A},\{P_{h}^{\star}\}_{h=0}^{H},\{R_{h} ^{\star}\}_{h=1}^{H},H\big{\}}\) consists of a state space \(\mathcal{X}\), an action space \(\mathcal{A}\), a reward distribution \(R_{h}^{\star}:\mathcal{X}\times\mathcal{A}\to\Delta([0,1])\) (with expectation \(r_{h}^{\star}(x,a)\)), and a transition kernel \(P_{h}^{\star}:\mathcal{X}\times\mathcal{A}\to\Delta(\mathcal{X})\) (with the convention that \(P_{0}^{\star}(\cdot\mid\emptyset)\) is the initial state distribution).3

Footnote 3: To simplify presentation, we assume that \(\mathcal{X}\) and \(\mathcal{A}\) are countable; our results extend to handle continuous variables with an appropriate measure-theoretic treatment.

At the beginning of the episode, the learner selects a randomized, non-stationary _policy_\(\pi=(\pi_{1},\ldots,\pi_{H})\), where \(\pi_{h}:\mathcal{X}\to\Delta(\mathcal{A})\); we let \(\Pi_{\text{rns}}\) denote the set of all such policies. The episode evolves through the following process; beginning from \(x_{1}\sim P_{0}^{\star}(\cdot\mid\emptyset)\), the MDP generates a trajectory \((x_{1},a_{1},r_{1}),\ldots,(x_{H},a_{H},r_{H})\) via \(a_{h}\sim\pi_{h}(x_{h})\), \(r_{h}\sim R_{h}^{\star}(x_{h},a_{h})\), and \(x_{h+1}\sim P_{h}^{\star}(\cdot\mid x_{h},a_{h})\). We let \(\mathbb{P}^{{ M}^{\star},\pi}\) denote the law under this process, and let \(\mathbb{E}^{{ M}^{\star},\pi}\) denote the corresponding expectation, and likewise let \(\mathbb{P}^{{ M},\pi}\) and \(\mathbb{E}^{{ M}^{\star},\pi}\) denote the analogous laws and expectations in another MDP \(M\). We assume that \(\sum_{h=1}^{H}r_{h}\in[0,1]\) almost surely for any trajectory in \(M^{\star}\).

For a policy \(\pi\) and MDP \(M\), the expected reward for \(\pi\) is given by \(J^{{ M}}(\pi)\coloneqq\mathbb{E}^{{ M},\pi}\big{[}\sum_{h=1}^{H}r_{h}\big{]}\), and the value functions are given by \(V_{h}^{{ M},\pi}(x)\coloneqq\mathbb{E}^{{ M},\pi}\big{[}\sum_{h^{ \prime}=h}^{H}r_{h^{\prime}}\mid x_{h}=x\big{]}\), and \(Q_{h}^{{ M},\pi}(x,a)\coloneqq\mathbb{E}^{{ M},\pi} \big{[}\sum_{h^{\prime}=h}^{H}r_{h^{\prime}}\mid x_{h}=x,a_{h}=a\big{]}\). We let \(\pi_{{ M}}=\{\pi_{{ M},h}\}_{h=1}^{H}\) denote an optimal deterministic policy of \(M\), which maximizes \(V^{{ M},\pi}\) (over \(\pi\)) at all states (and in particular, satisfies \(\pi_{{ M}}\in\arg\max_{\pi\in\Pi_{\text{rns}}}J^{{ M}}(\pi)\)), and write \(Q^{{ M},\star}\coloneqq Q^{{ M},\pi_{{ M}}}\). For \(f:\mathcal{X}\times\mathcal{A}\to\mathbb{R}\), we write \(\pi_{f}(x)\coloneqq\arg\max_{a}f(x,a)\) as well as \(V_{f}(x)=\max_{a}f(x,a)\). For MDP \(M\), horizon \(h\in[H]\), and \(g:\mathcal{X}\to\mathbb{R}\), we let \(\mathcal{T}_{h}^{{ M}}\) denote the Bellman (optimality) operator defined via \([\mathcal{T}_{h}^{{ M}}g](x,a)=\mathbb{E}^{{ M}}[r_{h}+g(x_{h+1})\mid x_{h}=x,a_{h}=a],\) and we overload notation by letting \([\mathcal{T}_{h}^{{ M}}f](x,a)=[\mathcal{T}_{h}^{{ M}}V_{f}](x,a)\). We also let \(\mathcal{T}_{h}^{{ M},\pi}\) denote the Bellman _evaluation_ operator defined via \([\mathcal{T}_{h}^{{ M},\pi}f](x)=\mathbb{E}^{{ M}} \big{[}r_{h}+\mathbb{E}_{a^{\prime}\sim\pi_{h+1}(\cdot\mid x_{h+1})}[f(x_{h+1},a^{ \prime})]\mid x_{h}=x,a_{h}=a\big{]},\) for any \(\pi\in\Pi_{\text{rns}}\). We define the _occupancy measures_ for layer \(h\) via \(d_{h}^{{ M},\pi}(x)=\mathbb{P}^{{ M},\pi}[x_{h}=x]\) and \(d_{h}^{{ M},\pi}(x,a)=\mathbb{P}^{{ M},\pi}[x_{h}=x,a_{h}=a]\).

Online reinforcement learning.In online reinforcement learning, the learning algorithm Alg repeatedly interacts with an unknown MDP \(M^{\star}\) by executing a policy and observing the resulting trajectory. After \(T\) rounds of interaction, the algorithm outputs a final policy \(\widehat{\pi}\), with the goal of minimizing their _risk_, defined via

\[\texttt{Risk}(T,\texttt{Alg},M^{\star})\coloneqq J^{{ M}^{\star}}(\pi_{{ M}^{\star}})-J^{{ M}^{\star}}(\widehat{\pi}).\] (1)Framework: Reinforcement learning under general latent dynamics.In _reinforcement learning under general latent dynamics_, we consider MDPs \(M^{*}\) where the dynamics are governed by the evolution of an unobserved latent state \(s_{h}\), while the agent observes and acts on _observations_\(x_{h}\) generated from these latent states. Formally, a _latent-dynamics MDP_ consists of two ingredients: a _base MDP_\(M_{\mathrm{1at}}=\{\mathcal{S},\mathcal{A},\{P_{\mathrm{1at},h}\}_{h=0}^{H},\{R_{ \mathrm{1at},h}\}_{h=1}^{H},H\}\) defined over a _latent state space_\(\mathcal{S}\), and a _decodable emission process_\(\psi:=\{\psi_{h}:\mathcal{S}\rightarrow\Delta(\mathcal{X})\}_{h=1}^{H}\), which maps each latent state to a distribution over observations. The former is an arbitrary MDP defined over \(\mathcal{S}\), while the latter is defined as follows.

**Definition 2.1** (Emission process).: _An emission process is any function \(\psi:=\{\psi_{h}:\mathcal{S}\rightarrow\Delta(\mathcal{X})\}_{h=1}^{H}\), and is said to be decodable if_

\[\forall h,\forall s^{\prime}\neq s\in\mathcal{S}:\quad\mathrm{supp}\,\psi_{h} (s)\cap\mathrm{supp}\,\psi_{h}(s^{\prime})=\emptyset.\quad.\] (2)

_When \(\psi=\{\psi_{h}\}_{h=1}^{H}\) is decodable, we let \(\psi^{-1}:=\{\psi_{h}^{-1}:\mathcal{X}\rightarrow\mathcal{S}\}_{h=1}^{H}\) denote the associated decoder._

With this, we can formally introduce the notion of a latent-dynamics MDP.

**Definition 2.2** (Latent-dynamics MDP).: _For a base MDP\(M_{\mathrm{1at}}=\{\mathcal{S},\mathcal{A},\{P_{\mathrm{1at},h}\}_{h=0}^{H},\{R_{ \mathrm{1at},h}\}_{h=1}^{H},H\}\), and a decodable emission process \(\psi\), the latent-dynamics MDP\(\langle\!\langle M_{\mathrm{1at}},\psi\rangle\!\rangle:=\big{\{}\mathcal{X}, \mathcal{A},\{P_{\mathrm{obs},h}\}_{h=0}^{H},\{R_{\mathrm{obs},h}\}_{h=1}^{H},H\big{\}}\) is defined as the MDP where the latent dynamics evolve based on the agent's action \(a_{h}\in\mathcal{A}\) via the process \(s_{h+1}\sim P_{\mathrm{1at},h}(s_{h},a_{h})\) and \(r_{h}\sim R_{\mathrm{1at},h}(s_{h},a_{h})\). The latent state is not observed directly, and instead the agent observes \(x_{h}\in\mathcal{X}\) generated by the emission process \(x_{h}\sim\psi_{h+1}(s_{h})\).4_

Footnote 4: Equivalently the dynamics can be described via \(R_{\mathrm{obs},h}(x_{h},a_{h})=R_{\mathrm{1at}}(\psi_{h}^{-1}(x_{h}),a_{h})\) and \(P_{\mathrm{obs},h}(x_{h+1}\mid x_{h},a_{h})=P_{\mathrm{1at},h}(\psi_{h+1}^{-1} (x_{h+1})\mid\psi_{h}^{-1}(x_{h}),a_{h})\cdot\psi_{h+1}(x_{h+1}\mid\psi_{h+1}^{ -1}(x_{h+1}))\).

Note that under these dynamics, the decoder \(\psi^{-1}\) associated with \(\psi\) ensures that \(\psi_{h}^{-1}(x_{h})=s_{h}\) almost surely for all \(h\in[H]\). That is, the latent states can be uniquely decoded from the observations. To emphasize the distinction between the latent-dynamics MDP \(\langle\!\langle M_{\mathrm{1at}},\psi\rangle\!\rangle\) (which operates on the observable state space \(\mathcal{X}\)) and the MDP \(M_{\mathrm{1at}}\) (which operates on the latent state space \(\mathcal{S}\)), we refer to the latter as a _base MDP_ rather than, for example, a "latent MDP", and apply a similar convention to other latent objects whenever possible.5

Footnote 5: For example, in Section 4 we will be concerned with reductions from observation-space algorithms to “base algorithms” that operate on the latent state space.

Departing from prior work, we do not place any inherent restrictions on the base MDP, and in particular do not assume that the latent space is small (i.e., tabular). Rather, we aim to understand--in a unified fashion--what structural assumptions on the base MDP \(M_{\mathrm{1at}}\) are required to enable learnability under latent dynamics. To this end, it will be useful to considers specific _classes_ (i.e., subsets) of base MDPs \(\mathcal{M}_{\mathrm{1at}}\) and the classes of latent-dynamics MDPs they induce.

**Definition 2.3** (Latent-dynamics MDP class).: _Given a set of base MDPs\(\mathcal{M}_{\mathrm{1at}}\) and a set of decoders \(\Phi\subset\{\mathcal{X}\rightarrow\mathcal{S}\}\), we let_

\[\langle\!\langle M_{\mathrm{1at}},\Phi\rangle\!\rangle:=\{\langle\!\langle M_{ \mathrm{1at}},\psi\rangle\!\rangle:M_{\mathrm{1at}}\in\mathcal{M}_{\mathrm{1at}}, \psi\text{ is decodable},\ \psi^{-1}\in\Phi\}\] (3)

_denote the class of induced latent-dynamics MDPs._

Stated another way, \(\langle\!\langle M_{\mathrm{1at}},\Phi\rangle\!\rangle\) is the set of all latent-dynamics MDPs\(\langle\!\langle M_{\mathrm{1at}},\psi\rangle\!\rangle\) where (i) the base MDP \(M_{\mathrm{1at}}\) lies in \(\mathcal{M}_{\mathrm{1at}}\), and (ii), the emission process \(\psi\) is decodable, with the corresponding decoder belonging to \(\Phi\). The class \(\mathcal{M}_{\mathrm{1at}}\) represents our prior knowledge about the underlying MDP \(M_{\mathrm{1at}}\); concrete classes considered in prior work include tabular MDPs [1, 1, 1, 2, 3], linear dynamical systems [1, 1, 2, 3], and factored MDPs [1]. In particular, the class \(\mathcal{M}_{\mathrm{1at}}\) may itself warrant using function approximation. At the same time, the class \(\Phi\) represents our prior knowledge or inductive bias about the emission process, enabling representation learning. In what follows, we investigate what conditions on \(\mathcal{M}_{\mathrm{1at}}\) make the induced class \(\langle\!\langle M_{\mathrm{1at}},\Phi\rangle\!\rangle\) tractable, both statistically (statistical modularity; Section 3) and via reduction (algorithmic modularity; Section 4).

## 3 Statistical Modularity: Positive and Negative Results

This section presents our main statistical results. We begin by formally defining the notion of statistical modularity introduced in Section 1, present our main impossibility result (lower bound) and its implications (Section 3.2), then give positive results for the general class of _pushforward-coverable_ MDPs (Section 3.3).

[MISSING_PAGE_FAIL:5]

**Intuition for lower bound.** The intuition behind the lower bound in Theorem 3.1 is as follows: the unobserved latent state space consists of \(N=|\Phi|\) binary trees (indexed from \(1\) to \(N\)), each with \(N\) leaf nodes. The starting distribution is uniform over the roots of the \(N\) trees, and the agent receives a reward of \(1\) if and only if they navigate to the leaf node that corresponds to the index of their current tree. The observed state space is identical to the latent state space, but the emission process shifts the index of the tree by an amount which is unknown to the agent. Despite the base MDP being known and the decoder class satisfying realizability, the agent requires near-exhaustive search to identify the value of the shift and recover a near-optimal policy.

A taxonomy of statistical modularity.As a corollary, we prove that many (but not all) well-studied function approximation settings do not admit statistical modularity by embedding them into the lower bound construction of Theorem 3.1 (as well as a variant of the result, Theorem E.1). Our results are summarized in Figure 1. Our impossibility results highlight the following phenomenon: many MDP classes \(\mathcal{M}_{\mathrm{1at}}\) that place structural assumptions via the value functions (e.g., MDPs with linear-\(Q^{\star}/V^{\star}\)[13] or MDPs with a Bellman complete value function class of bounded eluder dimension [12, 2]) become intractable under latent dynamics. Intuitively, this is because it is not possible to take advantage of structure in value functions without learning a good representation, and, simultaneously, these assumptions are too weak by themselves to enable learning such a representation. Meanwhile, MDP classes \(\mathcal{M}_{\mathrm{1at}}\) that place structural assumptions on the transition distribution (e.g., MDPs with low state occupancy complexity [13] or low-rank MDPs [1]) are sometimes (but not always) tractable under latent dynamics.8

Footnote 8: If one is willing to pay for suboptimal \(|\mathcal{A}|\) factors, then more (but not all) classes become statistically tractable (e.g., linear MDPs [10] and MDPs with low state-action occupancy [13]).

We point to Appendix E.2 for background on all the settings in Figure 1 and proofs that they are (or are not) statistically modular. We remark that it is fairly straightforward to embed most of the MDP classes of Figure 1 into the construction of Theorem 3.1 since it only uses only a single base MDP \(M_{\mathrm{1at}}\), and we expect that many other base MDP classes can similarly be shown to be intractable. However, proving the _positive_ results in Figure 1 requires establishing several new results showing that certain base classes are tractable under latent dynamics; most notably, we next discuss the case of _pushforward coverability_.

### Upper bounds: Pushforward-coverable MDPs are statistically modular

Our main postive result concerning statistical modularity is to highlight _pushforward coverability_[12, 13, 14]--a strengthened version of the _coverability_ parameter introduced in Xie et al. [12]--as a general structural parameter that enables sample-efficient reinforcement learning under latent dynamics.

[MISSING_PAGE_FAIL:7]

Setup and O2L meta-algorithm.For the results in this section, we denote the (unknown) latent-dynamics MDP of interest by \(M^{*}_{\text{obs}}:=\langle\!\langle M^{*}_{\text{1at}},\psi^{*}\!\rangle\!\rangle\), and use \(\phi^{\star}:=(\psi^{\star})^{-1}\) to denote the true decoder. The O2L meta-algorithm (Algorithm 1) learns a near-optimal policy for \(M^{\star}_{\text{obs}}\) by alternating between performing representation learning and executing a black-box "base" RL algorithm (designed for the base MDP) on the learned representation; this approach is inspired by empirical methods that blend representation learning and RL in the latent space (e.g., [1, 13]).

```
1:input: Epochs \(T\), episodes \(K\), decoder set \(\Phi\), rep. learning oracle RepLearn, base alg. \(\textsc{Alg}_{\text{1at}}\).
2:for\(t=1,2,\cdots,T\)do
3:RepLearn chooses a representation \(\widehat{\phi}^{(t)}:\mathcal{X}\to\mathcal{S}\in\Phi\) based on data collected so far.
4: Initialize new instance of \(\textsc{Alg}_{\text{1at}}\).
5:for\(k=1,2,\cdots,K\)do//ALG1at plays \(K\) rounds in the "\(\widehat{\phi}^{(t)}\)-compressed dynamics."
6:\(\textsc{Alg}_{\text{1at}}\) chooses policy \(\pi^{(t,k)}_{\text{lat}}:\mathcal{S}\times[H]\to\Delta(\mathcal{A})\).
7:\(\textsc{Deploy}\ \pi_{\text{1at}}\circ\widehat{\phi}^{(t)}\) to collect trajectory \(\{x^{(t,k)}_{h},a^{(t,k)}_{h},r^{(t,k)}_{h}\}_{h=1}^{H}\).
8:\(\textsc{Update}\ \textsc{Alg}_{\text{1at}}\) with compressed trajectory \(\{\widehat{\beta}^{(t)}_{h}(x^{(t,k)}_{h}),a^{(t,k)}_{h},r^{(t,k)}_{h}\}_{h=1}^ {H}\).
9:endfor
10:\(\textsc{Alg}_{\text{1at}}\) returns final policy \(\widehat{\pi}^{(t)}:\mathcal{S}\times[H]\to\Delta(\mathcal{A})\), deploy \(\widehat{\pi}^{(t)}\circ\widehat{\phi}^{(t)}\) to collect one trajectory.
11:endfor
12:return\(\widehat{\pi}=\textsc{Unif}(\widehat{\pi}^{(1)}\circ\widehat{\phi}^{(1)}, \ldots,\widehat{\pi}^{(T)}\circ\widehat{\phi}^{(T)})\). ```

**Algorithm 1**O2L: Observable-to-Latent Reduction

Concretely, the algorithm takes as input a _representation learning oracle_RepLearn and a _base RL algorithm_Alg\(\textsc{Alg}_{\text{1at}}\) that operates in the latent space. In each _epoch_\(t\in[T]\), RepLearn produces a new representation \(\widehat{\phi}^{(t)}:\mathcal{X}\to\mathcal{S}\) based on data observed so far (potentially using additional side information, which we will elaborate on in the sequel). Then, the reduction involves \(\textsc{Alg}_{\text{1at}}\), using \(\widehat{\phi}^{(t)}\) to simulate access to the true latent states. In particular, \(\textsc{Alg}_{\text{1at}}\) runs for \(K\) episodes, where at each episode \(k\): (i) \(\textsc{Alg}_{\text{1at}}\) produces a latent policy \(\pi_{\text{1at}^{(t,k)}}:\mathcal{S}\times[H]\to\Delta(\mathcal{A})\), (ii) the latent policy is transformed into an observation-level policy via composition with \(\widehat{\phi}^{(t)}\), i.e. \(\pi_{\text{1at}^{(t,k)}}\circ\widehat{\phi}^{(t)}\), which is then deployed to produce a trajectory \(\{x^{(t,k)}_{h},a^{(t,k)}_{h},r^{(t,k)}_{h}\}_{h=1}^{H}\), and (iii) the trajectory is _compressed through_\(\widehat{\phi}^{(t)}\) and used to update \(\textsc{Alg}_{\text{1at}}\) via \(\{\widehat{\phi}^{(t)}_{h},(x^{(t,k)}_{h},a^{(t,k)}_{h},r^{(t,k)}_{h}\}_{h=1} ^{H}\) (cf. Line 8 of Algorithm 1).9 After the \(K\) rounds conclude, \(\textsc{Alg}_{\text{1at}}\) produces a final latent policy \(\widehat{\pi}^{(t)}_{\text{1at}}:\mathcal{S}\times[H]\to\Delta(\mathcal{A})\). The final policy \(\widehat{\pi}\) chosen by the O2L algorithm is a uniform mixture of \(\widehat{\pi}^{(t)}_{\text{1at}}\circ\widehat{\phi}^{(t)}\) over all the epochs.

Footnote 9: Note that, if \(\widehat{\phi}\) is inaccurate, the compressed trajectory cannot necessarily be viewed as being generated by a latent MDP, and must instead be viewed as coming from a _Partially Observed_ MDP (Appendix I.1.1).

The central assumption behind O2L is that the base algorithm \(\textsc{Alg}_{\text{1at}}\) can achieve low-risk in the underlying base MDP \(M^{\star}_{\text{1at}}\)_if given access to the true latent states_\(s_{h}=\phi^{\star}(x_{h})\). Beyond this assumption, we require that the representation learning oracle RepLearn can learn a sufficiently high-quality representation. In our applications, this will be made possible by assuming access to a realizable decoder class \(\Phi\) and two distinct assumptions: _hindsight observability_ (Section 4.1) and conditions enabling _self-predictive representation learning_ (Section 4.2). We will show that under these conditions, we can instantiate a representation learning oracle such that O2L inherits the sample complexity guarantee for \(\textsc{Alg}_{\text{1at}}\), thereby achieving algorithmic modularity.

### Algorithmic modularity via hindsight observability

Our first algorithmic result bypasses the hardness in Section 3 by considering the setting of _hindsight observability_, which has garnered recent interest in the context of POMDPs [1, 1, 13, 14]. Here, we assume that at training time (but not during deployment), the algorithm has access to additional feedback in the form of the true latent states, which are revealed at the end of each episode.

**Assumption 4.1** (Hindsight Observability [1]).: _The latent states \((\phi^{*}_{1}(x_{1}),\ldots,\phi^{*}_{H}(x_{H}))\) are revealed to the learner after each episode \((x_{1},a_{1},r_{1},\ldots,x_{H},a_{H},r_{H})\) concludes._

We emphasize that in the hindsight observability framework, the learner must still execute _observation-space policies_\(\pi_{\text{obs}}:\mathcal{X}\times[H]\to\Delta(\mathcal{A})\), as the latent states are only revealed _at the end of each episode_. Under hindsight observability, we can instantiate the representation learning oracle in O2L so that the reduction achieves low risk for _any choice of black-box base algorithm_\(\textsc{Alg}_{\tt 1at}\). In particular, we make use of _online_ classification oracles, which use the revealed latent states to achieve low classification loss with respect to \(\phi^{\star}\) under adaptively generated data. We first state a guarantee based on generic classification oracles, then instantiate it to give a concrete end-to-end sample complexity bound.

Formally, at each step \(t\), the online classification oracle, denoted via \(\textsc{Rep}_{\textsc{class}}\), is given the states and hindsight observations collected so far and produces a deterministic estimate \(\widehat{\phi}^{{}^{(t)}}=\textsc{Rep}_{\textsc{class}}(\{x_{b}^{(t)}, \phi_{h}^{(t)}(x_{b}^{(t)})\}_{i<t,h\leq H})\) for the true decoder \(\phi^{\star}\). We measure the regret of the oracle via the \(0/1\) loss for classification:

\[\textsc{Reg}_{\textsc{class}}(T)\coloneqq\sum_{t=1}^{T}\sum_{h=1}^{H}\mathbb{ E}_{\pi^{{}^{(t)}}\sim p^{{}^{(t)}}}\,\mathbb{E}^{\pi^{{}^{(t)}}}\Big{[} \mathbb{I}\{\widehat{\phi}_{h}^{{}^{(t)}}(x_{h})\neq\phi_{h}^{\star}(x_{h}) \}\Big{]},\]

where \(p^{{}^{(t)}}\) represents a randomization distribution over the policy \(\pi^{{}^{(t)}}\). Our reduction succeeds under the assumption that the oracle has low expected regret.

**Assumption 4.2**.: _For any (possibly adaptive) sequence \(\pi^{{}^{(t)}}\), with \(\pi^{{}^{(t)}}\sim p^{{}^{(t)}}\), the online classification oracle \(\textsc{Rep}_{\textsc{class}}\) has expected regret bounded by_

\[\mathbb{E}[\textsc{Reg}_{\textsc{class}}(T)]\leq\mathtt{Est}_{\textsc{class}}(T),\]

_where \(\mathtt{Est}_{\textsc{class}}(T)\) is a known upper bound._

We apply such an oracle within O2L as follows: at the end of each iteration \(t\in[T]\) in O2L, we sample \(k\sim[K]\) uniformly, and update the classification oracle with the trajectory \((x_{1}^{{}^{(t,k)}},a_{1}^{{}^{(t,k)}},r_{1}^{{}^{(t,k)}}),\ldots,(x_{H}^{{}^ {(t,k)}}a_{B}^{{}^{(t,k)}},r_{B}^{{}^{(t,k)}})\); see the proof of Theorem 4.1 for details. We let \(\mathtt{Risk}_{\textsc{obs}}(TK)\) denote the risk of the O2L reduction when run for \(T\) epochs of \(K\) episodes, and we let \(\mathtt{Risk}_{\star}(K)\coloneqq\mathbb{E}[\mathtt{Risk}(K,\textsc{Alg}_{ \tt 1at},M_{\tt 1at}^{\star})]\) denote the expected risk of \(\textsc{Alg}_{\tt 1at}\) when executed on \(M_{\tt 1at}^{\star}\) with access to the true latent states \(s_{h}=\phi^{\star}(x_{h})\) for \(K\) episodes.

**Theorem 4.1** (Risk bound for O2L under hindsight observability).: _Let \(\textsc{Alg}_{\tt 1at}\) be a base algorithm with base risk\(\mathtt{Risk}_{\star}(K)\), and \(\textsc{Rep}_{\textsc{class}}\) a representation learning oracle satisfying Assumption 4.2. Then Algorithm 1, with inputs \(T,K,\Phi\), \(\textsc{Rep}_{\textsc{class}}\), and \(\textsc{Alg}_{\tt 1at}\), has expected risk_

\[\mathbb{E}[\mathtt{Risk}_{\textsc{obs}}(TK)]\leq\mathtt{Risk}_{\star}(K)+ \frac{2K}{T}\mathtt{Est}_{\textsc{class}}(T).\]

This result shows that we can achieve sublinear risk under latent dynamics as long as (i) the base algorithm achieves sublinear risk \(\mathtt{Risk}_{\star}(K)\) given access to the true latent states, and (ii) the classification oracle achieves sublinear regret \(\mathtt{Est}_{\textsc{class}}(T)\). Notably, the result is fully modular, meaning we require no explicit conditions on the latent dynamics or the base algorithm, and is computationally efficient whenever the base algorithm and classification oracle are efficient.

To make Theorem 4.1 concrete, we next provide a representation learning oracle (ExpWeights.Dr; Algorithm 3 in Appendix G.1) based on a derandomization of the classical exponential weights mechanism, which satisfies Assumption 4.2 with \(\mathtt{Est}_{\textsc{class}}\lesssim H\log|\Phi|\) whenever it has access to a class \(\Phi\) that satisfies decoder realizability.

**Lemma 4.1** (Online classification via ExpWeights.Dr).: _Under decoder realizability \((\phi^{\star}\in\Phi)\), ExpWeights.Dr (Algorithm 3) satisfies Assumption 4.2 with10_

Footnote 10: In this section, the notations \(\widetilde{\mathcal{O}},\approx,\) and \(\lesssim\) ignore only constants and logarithmic factors of \(H\).

\[\mathtt{Est}_{\textsc{class}}(T)=\widetilde{\mathcal{O}}(H\log|\Phi|).\]

Instantiating Theorem 4.1 with the above representation learning oracle, we obtain the following algorithmic modularity result.

**Corollary 4.1** (Algorithmic modularity under hindsight observability).: _For any base algorithm \(\textsc{Alg}_{\tt 1at}\), under decoder realizability \((\phi^{\star}\in\Phi)\), O2L with inputs \(T,K,\Phi,\textsc{ExpWeights.Dr},\) and \(\textsc{Alg}_{\tt 1at}\) achieves_

\[\mathbb{E}[\mathtt{Risk}_{\textsc{obs}}(TK)]\lesssim\mathtt{Risk}_{\star}(K)+ \frac{HK\log|\Phi|}{T}.\]

_Consequently, for any \(\textsc{Alg}_{\tt 1at}\), setting \(T\approx KH\log|\Phi|/\mathtt{Risk}_{\star}(K)\) achieves \(\mathbb{E}[\mathtt{Risk}_{\textsc{obs}}(TK)]\lesssim\mathtt{Risk}_{\star}(K)\) with a number of trajectories \(TK=\widetilde{\mathcal{O}}\big{(}K^{2}H\log|\Phi|/\mathtt{Risk}_{\star}(K)\big{)}\)._Beyond achieving algorithmic modularity, this result shows that under hindsight observability, we can achieve strong statistical modularity (modulo possible \(H\) factors) for _every_ base MDP class \(\mathcal{M}_{1\mathrm{at}}\), an important result in its own right.11 As an example, suppose that \(\mathtt{Risk}_{\ast}(K)=\mathcal{O}(K^{-1/2})\), which is satisfied by many standard algorithms of interest [17; 18; 19; 20; 21]. Then, setting \(T\) according to Corollary 4.1 obtains an expected risk bound of \(\varepsilon\) using \(\mathcal{O}(H\log|\Phi|/\varepsilon^{\ast})\) trajectories.

Footnote 11: Formally, while we have defined the statistical modularity condition in terms of _high-probability_ risk bounds, it is straightforward to extend it to instead consider _expected_ risk bounds.

**Remark 4.1** (Online versus offline oracles).: Theorem 4.1 critically uses that assumption that \(\mathtt{REP}_{\mathtt{class}}\) satisfies an _online_ classification error bound to handle the fact that data is generated adaptively based on the estimators \(\widehat{\phi}^{(1)},\ldots,\widehat{\phi}^{(T)}\) it produces, which is by now a relatively standard technique in the design of interactive decision making algorithms [14; 15; 16]. We note that under coverability and other exploration conditions, online oracles for classification can be directly obtained from _offline_ (i.e. supervised) classification oracles [17; 18; 19].

### Algorithmic modularity via self-predictive estimation

We complement the above results by studying the general online RL setting _without_ hindsight observations. To address this more challenging setting, we design an _optimistic self-predictive estimation_ objective (Eq. (7)), which learns a representation by jointly fitting a decoder together with a latent model. We prove that any representation learning oracle that attains low regret with respect to this objective can be used in O2L to obtain observable-to-latent reductions for any low-risk base algorithm \(\mathtt{Alg}_{1\mathrm{at}}\) (for a formal statement, see Theorem A.1). We provide a (computationally inefficient) estimator (SelfPredict.Opt; Algorithm 4 in Appendix H.1) which we show attains low optimistic self-regret under certain statistical conditions (namely, coverability of the base MDP and a function approximation condition enabling us to express the self-prediction target as a latent model, see Lemma A.1 for a formal statement), thereby obtaining an end-to-end reduction for the general online RL setting. For lack of space, these results are deferred to Appendix A.

## 5 Discussion

Our work initiates the study of statistical and algorithmic modularity for reinforcement learning under general latent dynamics. Our positive and negative results serve as a first step toward a unified theory for reinforcement learning in the presence of high-dimensional observations. To this end, we close with some important future directions and open problems.

Statistical modularity.Can we obtain a unified characterization for the statistical complexity of RL under latent dynamics with a given class of base MDPs \(\mathcal{M}_{1\mathrm{at}}\)? Our results in Section 3 suggest that this will require new tools that go beyond existing notions of statistical complexity. Toward resolving this problem, concrete questions that are not yet understood include: (i) Is coverability [17] (as opposed to pushforward coverability) sufficient for learnability under latent dynamics? (ii) Is the _Exogenous Block MDP_ problem [17; 18]--a special case of our general framework--statistically tractable? Lastly, are there additional types of feedback that are weaker than hindsight observability, yet suffice to bypass the hardness results in Section 3?

Algorithmic modularity.Can we derive a unified representation learning objective that enables algorithmic modularity whenever statistical modularity is possible? Ideally, such an objective would be computationally tractable. Alternatively, can we show that algorithmic modularity fundamentally requires stronger modeling assumptions than statistical modularity? Toward addressing the problems above, a first step might be to understand: (i) What are the minimal statistical assumptions under which we can minimize the self-predictive objective in Section 4.2? (ii) How can we encourage finding good representations via self-prediction beyond the use of optimism over the base (latent) models; and (iii) when can we minimize self-prediction in a computationally efficient fashion?

#### Acknowledgements

Nan Jiang acknowledges funding support from NSF IIS-2112471, NSF CAREER IIS-2141781, Google Scholar Award, and Sloan Fellowship.

## References

* [ACK24] Philip Amortila, Tongyi Cao, and Akshay Krishnamurthy. "Mitigating Covariate Shift in Misspecified Regression With Applications to Reinforcement Learning". In: _Conference on Learning Theory_. 2024.
* [AFJSX24] Philip Amortila, Dylan J. Foster, Nan Jiang, Ayush Sekhari, and Tengyang Xie. "Harnessing Density Ratios for Online Reinforcement Learning". In: _International Conference on Learning Representations_. 2024.
* [AFK24] Philip Amortila, Dylan J Foster, and Akshay Krishnamurthy. "Scalable Online Exploration via Coverability". In: _International Conference on Machine Learning_. 2024.
* [AHKLLS14] Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. "Taming the monster: A fast and simple algorithm for contextual bandits". In: _International Conference on Machine Learning_. 2014.
* [AJKS22] Alekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. _Reinforcement learning: Theory and algorithms_. https://rltheorybook.github.io/. Version: January 31, 2022. 2022.
* [AJSWY20] Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. "Model-Based Reinforcement Learning with Value-Targeted Regression". In: _International Conference on Machine Learning_. 2020.
* [AKKS20] Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. "FLAMBE: Structural Complexity and Representation Learning of Low Rank MDPs". In: _Neural Information Processing Systems_. 2020.
* [AOM17] Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. "Minimax Regret Bounds for Reinforcement Learning". In: _International Conference on Machine Learning_. 2017.
* [AZ22] Alekh Agarwal and Tong Zhang. "Model-based RL with Optimistic Posterior Sampling: Structural Conditions and Sample Complexity". In: _Neural Information Processing Systems_. 2022.
* [Bak+22] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. "Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos". In: _Neural Information Processing Systems_. 2022.
* [BLM13] Stephane Boucheron, Gabor Lugosi, and Pascal Massart. _Concentration inequalities: A nonasymptotic theory of independence_. Oxford university press, 2013.
* [Bro+22] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jolilyn Perrala, Emily Perez, Karl Pertsch, Jornell Quianbao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. "RT-1: Robotics Transformer for Real-World Control at Scale". In: _arXiv:2212.06817_. 2022.
* [BRS24] Adam Block, Alexander Rakhlin, and Abhishek Shetty. "On the Performance of Empirical Risk Minimization with Smoothed Data". In: _Conference on Learning Theory_. 2024.
* [CBL06] Nicolo Cesa-Bianchi and Gabor Lugosi. _Prediction, Learning, and Games_. Cambridge university press, 2006.
* [CJ19] Jinglin Chen and Nan Jiang. "Information-Theoretic Considerations in Batch Reinforcement Learning". In: _International Conference on Machine Learning_. 2019.
* [DKJADL19] Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford. "Provably Efficient RL With Rich Observations via Latent State Decoding". In: _International Conference on Machine Learning_. 2019.

* [DMRV21] Omar Darwiche Domingues, Pierre Menard, Emilie Kaufmann, and Michal Valko. "Episodic Reinforcement Learning in Finite MDPs: Minimax Lower Bounds Revisited". In: _Algorithmic Learning Theory_. 2021.
* [DMRY20] Sarah Dean, Nikolai Matni, Benjamin Recht, and Vickie Ye. "Robust Guarantees for Perception-Based Control". In: _Learning for Dynamics and Control_. 2020.
* [DR21] Sarah Dean and Benjamin Recht. "Certainty Equivalent Perception-Based Control". In: _Learning for Dynamics and Control_. 2021.
* [Du+21] Simon S Du, Sham M Kakade, Jason D Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong Wang. "Bilinear Classes: A Structural Framework for Provable Generalization in RL". In: _International Conference on Machine Learning_. 2021.
* [DVRZ19] Shi Dong, Benjamin Van Roy, and Zhengyuan Zhou. "Provably Efficient Reinforcement Learning With Aggregated States". In: _arXiv:1912.06366_. 2019.
* [EFMKL22] Yonathan Efroni, Dylan J Foster, Dipendra Misra, Akshay Krishnamurthy, and John Langford. "Sample-Efficient Reinforcement Learning in the Presence of Exogenous Information". In: _Conference on Learning Theory_. 2022.
* [EMKAL22] Yonathan Efroni, Dipendra Misra, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. "Provable RL With Exogenous Distractors via Multistep Inverse Dynamics". In: _International Conference on Learning Representations_. 2022.
* [FGH23] Dylan J Foster, Noah Golowich, and Yanjun Han. "Tight Guarantees for Interactive Decision Making with the Decision-Estimation Coefficient". In: _Conference on Learning Theory_. 2023.
* [FGQRS23] Dylan J Foster, Noah Golowich, Jian Qian, Alexander Rakhlin, and Ayush Sekhari. "Model-Free Reinforcement Learning with the Decision-Estimation Coefficient". In: _Neural Information Processing Systems_. 2023.
* [FHQR24] Dylan J Foster, Yanjun Han, Jian Qian, and Alexander Rakhlin. "Online Estimation via Offline Estimation: An Information-Theoretic Framework". In: _arXiv:2404.10122_. 2024.
* [FKQR21] Dylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. "The Statistical Complexity of Interactive Decision Making". In: _arXiv:2112.13487_. 2021.
* [FR20] Dylan J Foster and Alexander Rakhlin. "Beyond UCB: Optimal and Efficient Contextual Bandits With Regression Oracles". In: _International Conference on Machine Learning_. 2020.
* [FR23] Dylan J Foster and Alexander Rakhlin. "Foundations of Reinforcement Learning and Interactive Decision Making". In: _arXiv:2312.16730_. 2023.
* [FWDYDY20] Fei Feng, Ruosong Wang, Wotao Yin, Simon S Du, and Lin Yang. "Provably Efficient Exploration for Reinforcement Learning Using Unsupervised Learning". In: _Neural Information Processing Systems_. 2020.
* [GCWXWB24] Jiacheng Guo, Minshuo Chen, Huan Wang, Caiming Xiong, Mengdi Wang, and Yu Bai. "Sample-Efficient Learning of POMDPs with Multiple Observations In Hindsight". In: _International Conference on Learning Representations_. 2024.
* [Gee00] S. A. van de Geer. _Empirical Processes in M-Estimation_. Cambridge University Press, 2000.
* [GKBNB19] Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G Bellemare. "DeepMDP: Learning Continuous Latent Space Models for Representation Learning". In: _International Conference on Machine Learning_. 2019.
* [GMR24] Noah Golowich, Ankur Moitra, and Dhruv Rohatgi. "Exploring and Learning in Sparse Linear MDPs Without Computationally Intractable Oracles". In: _Symposium on Theory of Computing_. 2024.
* [Guo+22] Zhaohan Guo, Shantanu Thakoor, Miruna Pislar, Bernardo Avila Pires, Florent Altche, Corentin Tallec, Alaa Saade, Daniele Calandriello, Jean-Bastien Grill, Yunhao Tang, Michal Valko, Remi Munos, Mohammad Gheshlaghi Azar, and Bilal Piot. "BYOL-Explore: Exploration by Bootstrapped Prediction". In: _Neural Information Processing Systems_. 2022.
* [Haf+19] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. "Learning Latent Dynamics for Planning From Pixels". In: _International Conference on Machine Learning_. 2019.

* [HLBN19] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. "Dream to Control: Learning Behaviors by Latent Imagination". In: _International Conference on Learning Representations_. 2019.
* [HLNB21] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. "Mastering Atari With Discrete World Models". In: _International Conference on Learning Representations_. 2021.
* [HLSW21] Botao Hao, Tor Lattimore, Csaba Szepesvari, and Mengdi Wang. "Online Sparse Reinforcement Learning". In: _International Conference on Artificial Intelligence and Statistics_. 2021.
* [HPBL23] Danijar Hafner, Jurgis Paskonis, Jimmy Ba, and Timothy Lillicrap. "Mastering Diverse Domains Through World Models". In: _arXiv:2301.04104_. 2023.
* [Jia24] Nan Jiang. "A Note on Loss Functions and Error Compounding in Model-based Reinforcement Learning". In: _arXiv:2404.09946_. 2024.
* [JKALS17] Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. "Contextual Decision Processes With Low Bellman Rank Are PAC-Learnable". In: _International Conference on Machine Learning_. 2017.
* [JLM21] Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. "Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms". In: _Neural Information Processing Systems_. 2021.
* [JYWJ20] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. "Provably Efficient Reinforcement Learning With Linear Function Approximation". In: _Conference on Learning Theory_. 2020.
* [KAL16] Akshay Krishnamurthy, Alekh Agarwal, and John Langford. "PAC Reinforcement Learning With Rich Observations". In: _Neural Information Processing Systems_. 2016.
* [KFPM21] Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra Malik. "RMA: Rapid Motor Adaptation for Legged Robots". In: _Robotics: Science and Systems_. 2021.
* [LADZ23] Jonathan Lee, Alekh Agarwal, Christoph Dann, and Tong Zhang. "Learning in POMPDs Is Sample-Efficient With Hindsight Observability". In: _International Conference on Machine Learning_. 2023.
* [Lam+24] Alex Lamb, Riashat Islam, Yonathan Efroni, Aniket Rajiv Didolkar, Dipendra Misra, Dylan J Foster, Lekan P Molu, Rajan Chari, Akshay Krishnamurthy, and John Langford. "Guaranteed Discovery of Control-Endogenous Latent States with Multi-Step Inverse Models". In: _Transactions on Machine Learning Research_. 2024.
* [LFDA16] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. "End-To-End Training of Deep Visuomotor Policies". In: _The Journal of Machine Learning Research_. 2016.
* [Li09] Lihong Li. _A Unifying Framework for Computational Reinforcement Learning Theory_. Rutgers, The State University of New Jersey, 2009.
* [LS20] Tor Lattimore and Csaba Szepesvari. _Bandit Algorithms_. Cambridge University Press, 2020.
* [LSA20] Michael Laskin, Aravind Srinivas, and Pieter Abbeel. "CURL: Contrastive Unsupervised Representations for Reinforcement Learning". In: _International Conference on Machine Learning_. 2020.
* [LXJZV24] Michael Lanier, Ying Xu, Nathan Jacobs, Chongjie Zhang, and Yevgeniy Vorobeychik. "Learning Interpretable Policies in Hindsight-Observable POMDPs through Partially Supervised Reinforcement Learning". In: _arXiv:2402.09290_. 2024.
* [MBFR23] Zak Mhammedi, Adam Block, Dylan J Foster, and Alexander Rakhlin. "Efficient Model-Free Exploration in Low-Rank MDPs". In: _Neural Information Processing Systems_. 2023.
* [MCKJA24] Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. "Model-Free Representation Learning and Exploration in Low-Rank Mdps". In: _Journal of Machine Learning Research_. 2024.

* [MFR23] Zakaria Mhammedi, Dylan J Foster, and Alexander Rakhlin. "Representation Learning With Multi-Step Inverse Kinematics: An Efficient and Optimal Approach to Rich-Observation RL". In: _International Conference on Machine Learning_. 2023.
* [MFR24] Zakaria Mhammedi, Dylan J Foster, and Alexander Rakhlin. "The Power of Resets in Online Reinforcement Learning". In: _arXiv:2404.15417_. 2024.
* [Mha+20] Zakaria Mhammedi, Dylan J Foster, Max Simchowitz, Dipendra Misra, Wen Sun, Akshay Krishnamurthy, Alexander Rakhlin, and John Langford. "Learning the Linear Quadratic Regulator From Nonlinear Observations". In: _Neural Information Processing Systems_. 2020.
* [MHKL20] Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford. "Kinematic State Abstraction and Provably Efficient Rich-Observation Reinforcement Learning". In: _International Conference on Machine Learning_. 2020.
* [MJTS20] Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. "Sample Complexity of Reinforcement Learning Using Linearly Combined Model Ensembles". In: _International Conference on Artificial Intelligence and Statistics_. 2020.
* [MLJL21] Dipendra Misra, Qinghua Liu, Chi Jin, and John Langford. "Provable Rich Observation Reinforcement Learning With Combinatorial Latent States". In: _International Conference on Learning Representations_. 2021.
* [Ni+24] Tianwei Ni, Benjamin Eysenbach, Erfan Seyedsalehi, Michel Ma, Clement Gehring, Aditya Mahajan, and Pierre-Luc Bacon. "Bridging State and History Representations: Understanding Self-Predictive RL". In: _arXiv:2401.08898_. 2024.
* [NRKFG22] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. "R3M: A Universal Visual Representation for Robot Manipulation". In: _arXiv:2203.12601_. 2022.
* [OVR16] Ian Osband and Benjamin Van Roy. "On Lower Bounds for Regret in Reinforcement Learning". In: _arXiv:1608.02732_. 2016.
* [PAED17] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. "Curiosity-Driven Exploration by Self-Supervised Prediction". In: _International Conference on Machine Learning_. 2017, pp. 2778-2787.
* [RH23] Philippe Rigollet and Jan-Christian Hutter. "High-dimensional statistics". In: _arXiv:2310.19244_. 2023.
* [RVR13] Daniel Russo and Benjamin Van Roy. "Eluder Dimension and the Sample Complexity of Optimistic Exploration". In: _Neural Information Processing Systems_. 2013.
* [SAGHCB20] Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bachman. "Data-Efficient Reinforcement Learning with Self-Predictive Representations". In: _International Conference on Learning Representations_. 2020.
* [Sch+20] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, and David Silver. "Mastering Atari, Go, Chess and Shogi by Planning With a Learned Model". In: _Nature_. 2020.
* [SJKAL19] Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. "Model-Based RL in Contextual Decision Processes: PAC Bounds and Exponential Improvements Over Model-Free Approaches". In: _Conference on Learning Theory_. 2019.
* [SLS23] Ming Shi, Yingbin Liang, and Ness Shroff. "Theoretical Hardness and Tractability of POMDPs in RL with Partial Hindsight State Information". In: _arXiv:2306.08762_. 2023.
* [SWFK24] Yuda Song, Lili Wu, Dylan J Foster, and Akshay Krishnamurthy. "Rich-Observation Reinforcement Learning with Continuous Latent Dynamics". In: _International Conference on Machine Learning_. 2024.
* [SZSBKS23] Yuda Song, Yifei Zhou, Ayush Sekhari, J Andrew Bagnell, Akshay Krishnamurthy, and Wen Sun. "Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient". In: _International Conference on Learning Representations_. 2023.

* [Tan+17] Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schulman, Filip DeTurck, and Pieter Abbeel. "# Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning". In: _Neural Information Processing Systems_. 2017.
* [Tan+23] Yunhao Tang, Zhaohan Daniel Guo, Pierre Harvey Richemond, Bernardo Avila Pires, Yash Chandak, Remi Munos, Mark Rowland, Mohammad Gheshlaghi Azar, Charline Le Lan, Clare Lyle, Andras Gyorgy, Shantanu Thakoor, Will Dabney, Bilal Piot, Daniele Calandriello, and Michal Valko. "Understanding Self-Predictive Learning for Reinforcement Learning". In: _International Conference on Machine Learning_. 2023.
* [UZS22] Masatoshi Uehara, Xuezhou Zhang, and Wen Sun. "Representation Learning for Online and Offline RL in Low-rank MDPs". In: _The Tenth International Conference on Learning Representations_. 2022.
* [WSD15] Niklas Wahlstrom, Thomas B Schon, and Marc Peter Deisenroth. "From Pixels to Torques: Policy Learning With Deep Dynamical Models". In: _International Conference on Machine Learning_. 2015.
* [WSY20] Ruosong Wang, Russ R Salakhutdinov, and Lin Yang. "Reinforcement Learning with General Value Function Approximation: Provably Efficient Approach via Bounded Eluder Dimension". In: _Neural Information Processing Systems_. 2020.
* [WYDW21] Tianhao Wu, Yunchang Yang, Simon Du, and Liwei Wang. "On Reinforcement Learning With Adversarial Corruption and Its Application to Block MDP". In: _International Conference on Machine Learning_. 2021.
* [XFBJK23] Tengyang Xie, Dylan J Foster, Yu Bai, Nan Jiang, and Sham M Kakade. "The Role of Coverage in Online Reinforcement Learning". In: _International Conference on Learning Representations_. 2023.
* [XJ21] Tengyang Xie and Nan Jiang. "Batch Value-Function Approximation With Only Realizability". In: _International Conference on Machine Learning_. 2021.
* [YFK21] Denis Yarats, Rob Fergus, and Ilya Kostrikov. "Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning From Pixels". In: _International Conference on Learning Representations_. 2021.
* [ZGS21] Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. "Nearly Minimax Optimal Reinforcement Learning for Linear Mixture Markov Decision Processes". In: _Conference on Learning Theory_. 2021.
* [Zha06] Tong Zhang. "From \(\epsilon\)-entropy to KL-entropy: Analysis of minimum information complexity density estimation". In: _The Annals of Statistics_. Vol. 34. 5. Institute of Mathematical Statistics, 2006, pp. 2180-2210.
* [Zha22] Tong Zhang. "Feel-Good Thompson Sampling for Contextual Bandits and Reinforcement Learning". In: _SIAM Journal on Mathematics of Data Science_. 2022.
* [ZMCGL21] Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. "Learning Invariant Representations for Reinforcement Learning Without Reconstruction". In: _International Conference on Learning Representations_. 2021.
* [ZSUWAS22] Xuezhou Zhang, Yuda Song, Masatoshi Uehara, Mengdi Wang, Alekh Agarwal, and Wen Sun. "Efficient Reinforcement Learning in Block MDPs: A Model-Free Representation Learning Approach". In: _International Conference on Machine Learning_. 2022.

###### Contents

* A Omitted Results from Section 4: Algorithmic Modularity via Self-predictive Estimation
* A.1 Self-predictive estimation
* A.2 Main result
* A.3 Instantiating the self-predictive estimation oracle
* B Additional Discussion of Related Work
* C Technical Tools
* D Structural Properties of Coverability and Mismatch Functions
* E Proofs and Additional Results for Section 3.2: Impossibility Results
* E.1 Additional Lower Bound
* E.2 Details for Figure 1
* E.3 Proofs for Lower Bounds (Theorems 3.1 and E.1)
* F Proofs for Section 3.3: Positive Results
* F.1 Technical Overview: Low-dimensional embeddings for pushforward-coverable MDPs.
* F.2 Proofs for Latent Model Class + Pushforward Coverability (Theorem 3.2)
* G Proofs and Additional Information for Section 4.1: Hindsight RL
* G.1 Pseudocode and Proofs for ExpWeights.Dr (Lemma 4.1)
* G.2 Proofs for O2L Under Hindsight Observability (Theorem 4.1)
* H Proofs for Appendix A: Self-Predictive Estimation
* H.1 Pseudocode and Proofs for SelfPredict.Opt (Lemma A.1)
* H.2 Proofs for Main Risk Bound (Theorem A.1)
* I Additional Results for Appendix A: Self-Predictive Estimation
	* I.1 O2L with Self-predictive Estimation and CorruptionRobust Base Algorithms
	* I.2 Proofs for Appendix I.1.2: Properties of \(\phi\)-compressed POMDPs
	* I.3 Proofs for Appendix I.1.3: Risk Bound Under CorruptionRobustness (Theorem H.1)
	* I.4 Proofs for Appendix I.1.4: Examples of CorruptionRobust Algorithms

Omitted Results from Section 4: Algorithmic Modularity via Self-predictive Estimation

In this section, we remove the assumption of hindsight observability used in Section 4.1 and instantiate O2L in the general _online RL_ setting. Rather than assume access to additional side-information, we adopt a _model-based representation learning_ approach, and augment our ability to perform representation learning by equipping the representation learning algorithm with a _set of base MDPs_\(\mathcal{M}_{\mathrm{1at}}\) in addition to the decoder class \(\Phi\). We will learn a representation by jointly fitting a decoder and the base (latent) dynamics, which is a common approach in practice [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]. We firstly present in Appendix A.1 a new notion of _optimistic self-predictive regret_ which combines self-predictive representation learning with a form of optimism over a learned latent model. We then show in Appendix A.2 that any representation learning oracle that attains low regret, when used within O2L (Algorithm 1), leads to observable-to-latent reductions that ensure low risk for _any_ base algorithm \(\mathtt{Alg}_{\mathrm{1at}}\), thereby achieving algorithmic modularity. Lastly, in Appendix A.3, we instantiate this oracle under natural structural and function approximation conditions, yielding end-to-end modularity and sample complexity guarantees.

### Self-predictive estimation

Our self-predictive representation learning oracles learn to fit a representation \(\phi\) such that the _induced latent transitions_ (\(\phi_{h}(x_{h})\) to \(\phi_{h+1}(x_{h+1})\)) can be accurately modeled by some base (latent) MDP \(M_{\mathrm{1at}}\in\mathcal{M}_{\mathrm{1at}}\). To describe the objective, let us first introduce some notation. For a given MDP \(M\) over either \(\mathcal{S}\) (resp. \(\mathcal{X}\)), we write \(M_{h}(r_{h},s_{h+1}\mid s_{h},a_{h})\) (resp. \(M_{h}(r_{h},x_{h+1}\mid x_{h},a_{h})\)) for the joint conditional distribution over rewards and next states. Next, for any \(\phi\in\Phi\), we define the _pushforward model_ for \(M^{\star}_{\mathrm{obs},h}\) induced by \(\phi\) via:

\[\big{[}\phi_{h+1}\sharp M^{\star}_{\mathrm{obs},h}\big{]}(r,s^{\prime}\mid x,a)\coloneqq\sum_{x^{\prime}:\phi_{h+1}(x^{\prime})=s^{\prime}}M^{\star}_{ \mathrm{obs},h}(r,x^{\prime}\mid x,a).\] (6)

The _pushforward model_ for \(\phi\) captures the forward probability of the estimated latent state \(\phi(x^{\prime})\) given a current observation \(x\). To measure distance between models, we will use squared Hellinger distance (e.g, Foster et al. [20]), defined via \(D^{2}_{\mathrm{H}}(\mathbb{P},\mathbb{Q})=\int(\sqrt{\frac{\mathrm{d}\mathbb{P }}{\mathrm{d}\nu}}-\sqrt{\frac{\mathrm{d}\mathbb{Q}}{\mathrm{d}\nu}})^{2} \mathrm{d}\nu\) for a common dominating measure \(\nu\). Then, for a base model \(M_{\mathrm{1at}}\) and a decoder \(\phi\), the _self-predictive error_ of \((M_{\mathrm{1at}},\phi)\), at state-action pair \(x_{h},a_{h}\), is given by

\[[\Delta_{h}(M_{\mathrm{1at}},\phi)](x_{h},a_{h})\coloneqq D^{2}_{\mathrm{H}} \big{(}M_{\mathrm{1at},h}(\phi_{h}(x_{h}),a_{h}),\big{[}\phi_{h+1}\sharp M^{ \star}_{\mathrm{obs},h}\big{]}(x_{h},a_{h})\big{)}.\]

This term captures the ability of \(M_{\mathrm{1at},h}(\phi_{h}(x_{h}),a_{h})\) to predict the next latent state \(\phi_{h+1}(x_{h+1})\) which is obtained by the pushforward model \(\big{[}\phi_{h+1}\sharp M^{\star}_{\mathrm{obs},h}\big{]}(x_{h},a_{h})\). Formally, in our model-based representation learning setup, we consider oracles which, for each iteration \(t\) within O2L, take as input the trajectories collected so far and produce an estimate \((\widehat{M}^{(t)}_{\mathrm{1at}},\widehat{\phi}^{(t)})\) for the decoder and base model. The representation learning oracle's _self-predictive regret_, for the sequence \((\widehat{M}^{(t)}_{\mathrm{1at}},\widehat{\phi}^{(t)})\), is then defined as

\[\mathsf{Reg}_{\mathrm{self}}(T)=\sum_{t=1}^{T}\sum_{h=0}^{H}\mathbb{E}_{\pi^{(t )}\sim p^{(t)}}\,\mathbb{E}^{\pi^{(t)}}\Big{[}[\Delta_{h}(\widehat{M}^{(t)}_{ \mathrm{1at}},\widehat{\phi}^{(t)})](x_{h},a_{h})\Big{]},\]

where \(p^{(t)}\) represents a randomization distribution over the policy \(\pi^{(t)}\).

On its own, minimizing this regret may lead to degenerate solutions, a widely observed phenomenon in practice [13]. For example, in a standard combination lock MDP (e.g., Agarwal et al.; Misra et al. [1], a degenerate decoder-model pair that maps all observations to a single latent state will have zero self-predictive loss until we reach the goal, which can take exponentially long.12 We address this via the notion of _optimistic estimation_ used in Zhang; Foster et al. [13, 20], which biases the objective towards latent models with high return. This leads to the following _optimistic self-predictive regret_, defined for a parameter \(\gamma>0\), via

\[\mathtt{Reg}_{\mathtt{self}\mathtt{;opt}}(T,\gamma) =\sum_{t=1}^{T}\sum_{h=0}^{H}\mathbb{E}_{\pi^{(t)}\sim p^{(t)}} \,\mathbb{E}^{\pi^{(t)}}\Big{[}[\Delta_{h}(\widehat{M}^{{}_{\mathtt{1at}}}_{ \mathtt{1at}},\widehat{\phi}^{{}^{(t)}})](x_{h},a_{h})\Big{]}\] \[\qquad+\gamma^{-1}(J^{M^{*}_{\mathtt{1at}}}(\pi_{M^{*}_{\mathtt{1at }}})-J^{\widehat{M}^{(t)}_{\mathtt{1at}}}(\pi_{\widehat{M}^{(t)}_{\mathtt{1at }}})).\] (7)

We assume going forward that \(\mathtt{Rep}_{\mathtt{self}\mathtt{;opt}}\) obtains low optimistic self-predictive regret; in Appendix A.3 we provide a maximum-likelihood-type estimator and conditions under which this holds.

**Assumption A.1**.: _For a parameter \(\gamma>0\) and any (possibly adaptive) sequence \(\pi^{(t)}\), with \(\pi^{(t)}\sim p^{(t)}\), the online representation learning oracle \(\mathtt{Rep}_{\mathtt{self}\mathtt{;opt}}\) is proper (i.e. outputs \(\widehat{M}^{(t)}_{\mathtt{1at}}\in\mathcal{M}_{\mathtt{1at}}\) for all \(t\in[T]\)) and satisfies_

\[\mathbb{E}\big{[}\mathtt{Reg}_{\mathtt{self}\mathtt{;opt}}(T,\gamma)\big{]} \leq\mathtt{Est}_{\mathtt{self}\mathtt{;opt}}(T,\gamma),\]

_where \(\mathtt{Est}_{\mathtt{self}\mathtt{;opt}}(T,\gamma)\) is a known upper bound._

We note that only the decoder \(\widehat{\phi}^{(t)}\) is used within O2L; the model \(\widehat{M}^{(t)}_{\mathtt{1at}}\) is only used for analysis (and possibly within the representation learner \(\mathtt{Rep}_{\mathtt{self}\mathtt{;opt}}\)).

### Main result

We now state the main guarantee for O2L with self-predictive representation learning. Recall that \(\mathtt{Risk}_{\mathtt{obs}}(TK)\) denotes the risk of the O2L reduction. Compared to the hindsight-observable setting, we require a slightly stronger performance guarantee from the base algorithm \(\mathtt{Alg}_{\mathtt{1at}}\): our result scales with the _worst-case_ expected risk for \(\mathtt{Alg}_{\mathtt{1at}}\) over all \(M_{\mathtt{1at}}\in\mathcal{M}_{\mathtt{1at}}\), defined via \(\mathtt{Risk}_{\mathtt{base}}(K):=\sup_{M_{\mathtt{1at}}\in\mathcal{M}_{ \mathtt{1at}}}\mathbb{E}[\mathtt{Risk}(K,\mathtt{Alg}_{\mathtt{1at}},M_{ \mathtt{1at}}))]\).

**Theorem A.1** (Risk bound for O2L under self-predictive estimation).: _Suppose \(\mathtt{Rep}_{\mathtt{self}\mathtt{;opt}}\) satisfies Assumption A.1 with parameter \(\gamma>0\). Then Algorithm 1, with inputs \(T,K,\Phi\), \(\mathtt{Rep}_{\mathtt{self}\mathtt{;opt}}\), and \(\mathtt{Alg}_{\mathtt{1at}}\) has expected risk_

\[\mathbb{E}[\mathtt{Risk}_{\mathtt{obs}}(TK)]\leq c_{1}\cdot\mathtt{Risk}_{ \mathtt{base}}(K)+c_{2}\gamma\cdot\frac{K}{T}\mathtt{Est}_{\mathtt{self} \mathtt{;opt}}(T,\gamma)+c_{3}\gamma^{-1}\cdot KH,\]

_for absolute constants \(c_{1},c_{2},c_{3}>0\)._

Theorem A.1 achieves sublinear risk as long as (i) the latent algorithm achieves sublinear risk \(\mathtt{Risk}_{\mathtt{base}}(K)\) given access to the true states, and (ii) the self-predictive representation learning oracle achieves sublinear regret \(\mathtt{Est}_{\mathtt{self}\mathtt{;opt}}(T,\gamma)\) for an appropriate choice of \(\gamma\).13 Intuitively, our result scales with \(\mathtt{Risk}_{\mathtt{base}}(K)\) instead of \(\mathtt{Risk}_{*}(K)\) due to potential symmetries in the self-predictive objective. For example, there might be a representation-model pair \((\widehat{M}_{\mathtt{1at}},\widehat{\phi})\) that is identical to \((M^{*}_{\mathtt{1at}},\phi^{*})\)_up to_ permutations of the latent state space; these cannot be distinguished by a representation learning oracle that does not observe the latent states directly, and thus the base algorithm may be tasked with solving either of these base MDPs. As with Theorem 4.1, this result achieves algorithmic modularity (since O2L inherits the risk of the base algorithm), and is computationally efficient whenever the base algorithm and self-predictive representation learning oracle are efficient.

Footnote 13: For example, in our estimator of Appendix A.3, we can first set \(\gamma\approx KH/\mathtt{Risk}_{\mathtt{base}}(K)\) so that the third term matches \(\mathtt{Risk}_{\mathtt{base}}(K)\), and then set \(T\) so that the second term does.

Let us provide some intuition behind the proof of Theorem A.1. Recall that, within the inner loop of O2L, the latent algorithm \(\mathtt{Alg}_{\mathtt{1at}}\) interacts with the \(\widehat{\phi}^{(t)}\)-compressed dynamics generated by compressing the observations \(x_{h},a_{h}\) through the current decoder \(\widehat{\phi}^{(t)}_{h}\) (Line 8). The crux of the analysis is the following observation: by the self-predictive representation learning guarantee, these dynamics, despite being possibly non-Markovian and generated from a POMDP (Definition I.1), are well approximated in squared Hellinger distance by the base model \(\widehat{M}^{(t)}_{\mathtt{1at}}\) estimated by \(\mathtt{Rep}_{\mathtt{self}\mathtt{;opt}}\) (cf. Lemma I.2). We can then show that \(\mathtt{Alg}_{\mathtt{1at}}\), when given data from the \(\widehat{\phi}^{(t)}\)-compressed dynamics, has risk (for solving \(\widehat{M}^{(t)}_{\mathtt{1at}}\)) that is proportional to: i) its base risk if it were to observe states from \(\widehat{M}^{(t)}_{\mathtt{1at}}\), and ii) the Hellinger distance between \(\widehat{M}^{(t)}_{\mathtt{1at}}\) and the process induced by its \(\widehat{\phi}^{(t)}\)-compresseddynamics. The last ingredient is the use of latent optimism in Eq. (7), through which the risk on \(M^{\star}_{\mathrm{1at}}\) is upper bounded by the risk on \(\widehat{M}^{({}_{\mathrm{1at}})}_{\mathrm{1at}}\).

In the above, showing that \(\textsc{Alg}_{\mathrm{1at}}\) obtains low risk for \(\widehat{M}^{({}_{\mathrm{1at}})}_{\mathrm{1at}}\) (despite given data from a different process) is done by establishing a certain form of _corruption robustness_ (Definition I.2). Indeed, Theorem A.1 is a special case of a more general theorem (Theorem A.1), which provides a bound that adapts to \(\textsc{Alg}_{\mathrm{1at}}\)'s level of robustness. We obtain Theorem A.1 by showing that _any algorithm_ satisfies the property we require (for a suitably slow rate), but we further show that tighter rates can be achieved by analyzing the specifics of various algorithms of interest (Appendix I.1.4).

### Instantiating the self-predictive estimation oracle

We now present an algorithm, SelfPredict.Opt (Algorithm 4 in Appendix H.1), which satisfies Assumption A.1 under additional technical conditions, allowing us to instantiate Theorem A.1 to give end-to-end guarantees. Before stating the main guarantee, we highlight a few technical difficulties regarding obtaining finite-sample guarantees for (online) self-predictive estimation, and use them to motivate our statistical assumptions and algorithm design.

The statistics of (online) self-predictive estimation.The first challenge is a realizability issue: when \(\phi\neq\phi^{\star}\), we may not even be able to represent the objective \(\phi\sharp M^{\star}_{\mathsf{obs}}\) as a latent model using only decoder and latent model realizability. Since we can never guarantee that \(\phi=\phi^{\star}\) exactly in the presence of statistical errors, we must introduce a modelling assumption which lets us capture the pushforward models \(\phi\sharp M^{\star}_{\mathsf{obs}}\). To this end, we introduce the mismatch functions, which are defined as follows.

**Definition A.1** (Mismatch functions).: _For a decodable emission process \(\psi^{\star}\) and decoder \(\phi\in\Phi\), the mismatch function for \(\phi\), \(\Gamma_{\phi}=\{\Gamma_{\phi,h}:\mathcal{S}\to\Delta(\mathcal{S})\}_{h=1}^{H}\), is defined, for every \(h\in[H]\), as the probability kernel_

\[\Gamma_{\phi,h}(s^{\prime}_{h}\mid s_{h})\coloneqq\mathbb{P}_{x_{h}\sim\psi^{ \star}_{h}(s_{h})}(\phi_{h}(x_{h})=s^{\prime}_{h}).\]

In the context of self-prediction, we show that the following _mismatch completeness_ assumption suffices to capture the pushforward models \(\phi\sharp M^{\star}_{\mathsf{obs}}\).

**Assumption A.2** (Mismatch completeness).: _We have a model class \(\mathcal{L}\) such that, for each \(\phi\in\Phi\), and \(M_{\mathrm{1at}}\in\mathcal{M}_{\mathrm{1at}}\), we have \(\Gamma_{\phi}\circ M_{\mathrm{1at}}\in\mathcal{L}\), where_

\[[\Gamma_{\phi}\circ M_{\mathrm{1at}}]_{h}(r_{h},s_{h+1}\mid s_{h},a_{h}) \coloneqq\sum_{s^{\prime}_{h+1}\in\mathcal{S}}M_{\mathrm{1at},h}(r_{h},s^{ \prime}_{h+1}\mid s_{h},a_{h})\Gamma_{\phi,h+1}(s_{h+1}\mid s^{\prime}_{h+1}).\]

In particular, Lemma D.8 establishes that

\[[\phi_{h+1}\sharp M^{\star}_{\mathsf{obs},h}](\cdot\mid x,a)=[\Gamma_{\phi} \circ M^{\star}_{\mathrm{1at}}]_{h}(\cdot\mid\phi^{\star}_{h}(x),a).\]

Accordingly, we view this assumption as a minimal way to realize the pushforward models \(\phi\sharp M^{\star}_{\mathsf{obs}}\).

The second challenge is a _double-sampling_ issue, which appears because the decoders in Eq. (7) are coupled at different horizons. We address this with a novel "debiased" maximum likelihood procedure that subtracts a form of excess risk (cf. Eq. (60)) to recover an unbiased estimator [14]. Our debiased estimator and the mismatch completeness assumption can be viewed as analogous to the techniques and assumptions that are required for squared Bellman error minimization in the context of value function approximation [13, 12].

The last issue stems from seeking an _online_ estimation guarantee: the policies chosen by the latent algorithm are a function of the estimated decoders, which precludes the use of randomized estimators (e.g. exponential weights). We bypass this issue by appealing to the structural condition of coverability [11], which allows us to restrict our attention to estimators that achieve low _offline_ estimation error (via Lemma C.7).14

Footnote 14: More generally, we expect that our results can be extended to any “decoupling coefficient” [14, 15].

**Definition A.2** (State Coverability).: _The state coverability coefficient for an MDP \(M\) and a policy class \(\Pi\) defined over a state space \(\mathcal{Z}\), \(C_{\mathsf{cov},\mathrm{st}}(M,\Pi)\), is given by_

\[C_{\mathsf{cov},\mathrm{st}}(M,\Pi)\coloneqq\max_{h\in[H]}\min_{\mu\in\Delta( \mathcal{Z})}\max_{\pi\in\Pi}\max_{z\in\mathcal{Z}}\biggl{\{}\frac{d_{h}^{M, \pi}(z)}{\mu(z)}\biggr{\}}.\] (8)We require coverability in \(M_{\text{obs}}^{\star}\) over the set of (observation-space) policies played by the O2L reduction (cf. Line 7). Again appealing to the mismatch functions, we can express this as an assumption about the base dynamics \(M_{\text{1at}}^{\star}\); we show (Lemma D.1) that the latter is _equivalent_ to assuming coverability in \(M_{\text{1at}}^{\star}\) over the set of stochastic policies

\[\Gamma_{\Phi}\circ\Pi_{\text{1at}}\coloneqq\Bigg{\{}[\Gamma_{\phi}\circ\pi_{ \text{1at}}]_{h}(a\mid s)=\sum_{s^{\prime}\in\mathcal{S}}\Gamma_{\phi,h}(s^{ \prime}\mid s)\pi_{\text{1at},h}(a\mid s^{\prime})\mid\phi\in\Phi,\pi_{\text{1at }}\in\Pi_{\text{1at}}\Bigg{\}},\] (9)

where \(\Pi_{\text{1at}}\) denotes the set of policies that \(\textsc{Alg}_{\text{1at}}\) may execute. While this set may appear complicated, it is sufficient to assume coverability over the set of all _deterministic_ non-stationary policies on \(M_{\text{1at}}^{\star}\).15

Footnote 15: This follows from Lemma D.3 by noting that each maximum on the right hand side of Eq. (13) is attained by a deterministic non-stationary policy.

Guarantee for our self-predictive estimation oracle.With these prerequisites, the main guarantee for our estimator, SelfPredict.Opt (Algorithm 4), is as follows.

**Lemma A.1** (Optimistic self-predictive estimation via SelfPredict.Opt).: _Let \(\Pi_{\text{1at}}\) denote the set of policies played by \(\textsc{Alg}_{\text{1at}}\), and \(C_{\text{cov},\text{st}}=C_{\text{cov},\text{st}}(M_{\text{1at}}^{\star},\Gamma _{\Phi}\circ\Pi_{\text{1at}})\) be the state coverability parameter on \(M_{\text{1at}}^{\star}\) over the set of stochastic policies \(\Gamma_{\Phi}\circ\Pi_{\text{1at}}\) (Eq. (9)). Then, for any \(\gamma>0\), under decoder realizability \((\phi^{\star}\in\Phi)\), base model realizability \((M_{\text{1at}}^{\star}\in\mathcal{M}_{\text{1at}})\), and mismatch function completeness with class \(\mathcal{L}_{\text{1at}}\) (Assumption A.2), the estimator in Algorithm 4 with inputs \(\Phi,\mathcal{M}_{\text{1at}},\mathcal{L}_{\text{1at}},\) and \(\gamma\) satisfies Assumption A.1 with16_

Footnote 16: In this section, the notations \(\widetilde{\mathcal{O}}\) and \(\lesssim\) ignores constants and logarithmic factors of: \(H,C_{\text{cov},\text{st}},|\mathcal{A}|,T,\) and \(\log(|\mathcal{M}_{\text{1at}}||\mathcal{L}_{\text{1at}}||\Phi|)\).

\[\mathtt{Est}_{\mathtt{self};\mathtt{opt}}(T,\gamma)=\widetilde{\mathcal{O}} \bigg{(}\sqrt{HC_{\text{cov},\text{st}}|\mathcal{A}|}T\log(|\mathcal{M}_{\text {1at}}||\mathcal{L}_{\text{1at}}||\Phi|)\bigg{)}.\]

Instantiating Theorem A.1 with the above representation learning oracle, we obtain the following algorithmic modularity result.

**Corollary A.1** (Algorithmic modularity via SelfPredict.Opt).: _Under the same conditions as in Lemma A.1, and for any base algorithm \(\textsc{Alg}_{\text{1at}}\), O2L with inputs \(T,K,\Phi,\textsc{SelfPredict.Opt},\) and \(\textsc{Alg}_{\text{1at}}\) achieves_

\[\mathbb{E}[\mathtt{Risk}_{\text{obs}}(TK)]\lesssim c_{1}\cdot\mathtt{Risk}_{ \text{base}}(K)+c_{2}\gamma\cdot\frac{K}{\sqrt{T}}\sqrt{HC_{\text{cov},\text{ st}}|\mathcal{A}|}\log(|\mathcal{M}_{\text{1at}}||\mathcal{L}_{\text{1at}}||\Phi|)+c_{3} \gamma^{-1}\cdot KH,\]

_for absolute constants \(c_{1},c_{2},c_{3}\). Consequently, for any \(\textsc{Alg}_{\text{1at}}\) with base risk \(\mathtt{Risk}_{\text{base}}(K)\), setting \(\gamma\) and \(T\) appropriately gives_

\[\mathbb{E}[\mathtt{Risk}_{\text{obs}}(TK)]\lesssim\mathtt{Risk}_{\text{base}}( K),\]

_with a number of trajectories \(TK=\widetilde{\mathcal{O}}(K^{5}H^{3}C_{\text{cov},\text{st}}|\mathcal{A}|\log^ {2}(|\mathcal{M}_{\text{1at}}||\mathcal{L}_{\text{1at}}||\Phi|)/\big{(} \mathtt{Risk}_{\text{base}}(K))^{4}\big{)}\)._

For example, if \(\textsc{Alg}_{\text{1at}}\) is a base algorithm with \(\mathtt{Risk}_{\text{base}}(K)=\mathcal{O}(K^{-1/2})\), setting \(\gamma\) and \(T\) appropriately gives an expected risk of \(\varepsilon\) with a number of trajectories \(TK=\widetilde{\mathcal{O}}\big{(}H^{3}C_{\text{cov},\text{st}}|\mathcal{A}|( \log(|\mathcal{M}_{\text{1at}}||\mathcal{L}_{\text{1at}}||\Phi|))^{2}/\varepsilon^ {14}\big{)}\). This result shows that statistical modularity can be achieved _up to \(\log(|\mathcal{L}_{\text{1at}}|)\) factors_ for every base MDP class \(\mathcal{M}_{\text{1at}}\) which is subsumed by coverability, including tabular MDPs and low-rank MDPs.17 Compared to our positive result for the case of pushforward coverability (Section 3.3), this imposes less dynamics assumptions (since coverability is implied by pushforward coverability) but requires more representational assumptions (namely, access to the mismatch-complete class \(\mathcal{L}_{\text{1at}}\)). We further remark that the mismatch completeness assumption always holds for i) the Block MDP setting, since we can always construct \(\mathcal{L}_{\text{1at}}\) such that \(\log(|\mathcal{L}_{\text{1at}}|)=\mathcal{O}(HS^{2})\), and ii) every MDP class \(\mathcal{M}_{\text{1at}}\) whenever we also have a realizable set of emission processes (\(\psi^{\star}\in\Psi\)), since we can construct \(\mathcal{L}_{\text{1at}}\) such that \(\log(|\mathcal{L}_{\text{1at}}|)=\log(|\Phi||\mathcal{M}_{\text{1at}}||\Psi|)\). However, the mismatch completeness assumption may be more general than either of these settings.

Footnote 17: This provides a partial answer to the “Model Class + Coverability” open question of Figure 1.

Our results can be viewed as providing a theoretical justification for self-predictive representation learning, which has been widely used in empirical works [11, 12]. We consider self-prediction's ability to obtain universal observable-to-latent reductions as a strong indicator that it merits further theoretical study. In particular, many empirical works propose heuristics to alleviate the degeneracy/non-uniqueness issues inherent with self-prediction [11, 12, 13, 14, 15, 16, 17, 18, 19, 20]. Our methods provide a principled way to address these, and it would be interesting to investigate whether this is also _empirically_ effective. In general, however, it is unclear whether our loss admits a computationally efficient implementation, due to the presence of optimism. Towards this, a fascinating direction for future work is understanding how self-predictive estimation can be used to obtain algorithmic modularity _without_ the addition of optimism over the base (latent) models.

Additional Discussion of Related Work

In this section, we discuss aspects of related work not already covered in greater detail.

Reinforcement learning under latent dynamics (or, with rich observations).Reinforcement learning under latent dynamics (or, with rich observations) has received extensive investigation in recent years, however most works have been focused on the _Block MDP_ model in which the latent state space is tabular/finite [16, 17, 18, 19, 20, 21] (see also the closely related framework of _Low-Rank MDPs_[12, 13, 14, 23]). Beyond tabular spaces, Dean et al., Dean et al., 2017; Mhammedi et al. [23, 24] consider continuous _linear_ dynamics, Misra et al. [20, 21] considers factored (but discrete) latent dynamics, Efroni et al., Efroni et al., 2017; Mhammedi et al. [23, 24] consider the Exogenous Block MDP problem in which a tabular latent state space is augmented with a non-controllable ("exogenous") factor, and Song et al. [20] consider Lipshitz continuous dynamics. To our knowledge, our work is the first to: i) explore reinforcement learning under general latent dynamics, in particular in settings where the latent space itself admits function approximation, and ii) take a more modular approach (cf. the taxonomy of Section3).

On the algorithmic side, the works of Uehara et al. [20] and Zhang et al. [20], which consider Low-Rank MDPs and Block MDPs respectively, can be viewed as interleaving representation learning with "latent" reinforcement learning algorithms that assume access to a good representation, and were an inspiration for this work. However, the algorithmic details and analyses are highly specialized to Block/Low-Rank MDPs, and unlikely to be directly applicable to reinforcement learning under general latent dynamics. Other works with a modular flavor include:

* Feng et al. [20] solve tabular Block MDPs by combining a black-box latent algorithm with an "unsupervised learning oracle" for representation learning. This approach only leads to guarantees for tabular Block MDPs, and it is unclear whether the unsupervised learning oracle their approach requires can be constructed in natural settings.
* Wu et al. [20] solve tabular block MDPs by combining a corruption-robust latent algorithm with a representation learning procedure based on clustering. Again, this work is restricted to the tabular setting, and requires a separation condition which may not be satisfied in general.

General complexity measures for reinforcement learning.Another line of research provides general complexity measures that enable sample-efficient reinforcement learning, including Bellman rank [17, 18, 21, 22], eluder dimension [21], coverability [20], and the Decision-Estimation Coefficient (DEC) [21, 23, 24]. Bellman rank and other complexity measures based on average Bellman error [17, 18, 20, 22] are insufficient to characterize learnability under general latent dynamics, as there are classes \(\mathcal{M}_{\mathrm{1at}}\) that are known to be learnable, yet do not have bounded Bellman rank or Bellman-Eluder dimension [23, 24]. Meanwhile, variants of Bellman rank based on squared Bellman error or related notions of error can [20, 21] address this problem for some settings, but satisfying the modeling/realizability assumptions (e.g., Bellman completeness) required by these methods in the latent-dynamics setting is non-trivial. For example, the crux of our sample complexity bounds under latent pushforward coverability in Section3 (Theorem3.2) is to prove a rather involved structural result which shows that Bellman completeness can indeed be satisfied under this assumption, but it is unclear whether these techniques can be applied to more general latent dynamics classes. We expect that it is possible to bound the Decision-Estimation Coefficient [21, 23, 24] for the framework, but deriving efficient algorithms using this framework is non-trivial.

## Appendix C Technical Tools

**Lemma C.1**.: _For any sequence of real-valued random variables \((X_{t})_{t\leq T}\) adapted to a filtration \((\mathscr{F}_{t})_{t\leq T}\), it holds that with probability at least \(1-\delta\),_

\[\sum_{t=1}^{T}X_{t}\leq\sum_{t=1}^{T}\log\bigl{(}\mathbb{E}_{t-1} \bigl{[}e^{X_{t}}\bigr{]}\bigr{)}+\log\bigl{(}\delta^{-1}\bigr{)}.\]

**Lemma C.2** (Freedman's inequality (e.g., Agarwal et al. [1])).: _Let \((X_{t})_{t\leq T}\) be a real-valued martingale difference sequence adapted to a filtration \((\mathscr{F}_{t})_{t\leq T}\). If \(|X_{t}|\leq R\) almost surely, then for any \(\eta\in(0,1/R)\), with probability at least \(1-\delta\),_

\[\sum_{t=1}^{T}X_{t}\leq\eta\sum_{t=1}^{T}\mathbb{E}_{t-1}\bigl{[} X_{t}^{2}\bigr{]}+\frac{\log\bigl{(}\delta^{-1}\bigr{)}}{\eta}.\]

**Lemma C.3** (Corollary of Lemma C.2).: _Let \((X_{t})_{t\leq T}\) be a sequence of random variables adapted to a filtration \((\mathscr{F}_{t})_{t\leq T}\). If \(0\leq X_{t}\leq R\) almost surely, then with probability at least \(1-\delta\),_

\[\sum_{t=1}^{T}X_{t}\leq\frac{3}{2}\sum_{t=1}^{T}\mathbb{E}_{t-1} [X_{t}]+4R\log\bigl{(}2\delta^{-1}\bigr{)},\]

_and_

\[\sum_{t=1}^{T}\mathbb{E}_{t-1}[X_{t}]\leq 2\sum_{t=1}^{T}X_{t}+8R \log\bigl{(}2\delta^{-1}\bigr{)}.\]

**Lemma C.4** (Lemma D.2 of Foster et al. [15]).: _Let \((\mathcal{X}_{1},\mathfrak{F}_{1}),\ldots,(\mathcal{X}_{n},\mathfrak{F}_{n})\) be a sequence of measurable spaces, and let \(\mathcal{X}^{{(i)}}=\prod_{i=1}^{n}\mathcal{X}_{t}\) and \(\mathfrak{F}^{{(i)}}=\otimes_{i=1}^{i}\mathfrak{F}_{t}\). For each \(i\), let \(P^{{(i)}}\) and \(Q^{{(i)}}\) be probability kernels from \((\mathcal{X}^{{(i-1)}},\mathfrak{F}^{{(i-1)}})\) to \((\mathcal{X}_{i},\mathfrak{F}_{i})\). Let \(P\) and \(Q\) be the laws of \(X_{1},\ldots,X_{n}\) under \(X_{i}\sim P^{{(i)}}(\cdot\mid X_{1:i-1})\) and \(X_{i}\sim Q^{{(i)}}(\cdot\mid X_{1:i-1})\), respectively. Then it holds that_

\[D_{\mathsf{H}}^{2}(P,Q)\leq 7\,\mathbb{E}_{P}\Biggl{[} \sum_{i=1}^{n}D_{\mathsf{H}}^{2}(P^{{(i)}}(\cdot\mid X_{1:i-1}),Q^{{(i)}}( \cdot\mid X_{1:i-1}))\Biggr{]}\]

**Lemma C.5** (Lemma A.11 of Foster et al. [15]).: _Let \(\mathbb{P}\) and \(\mathbb{Q}\) be probability measures on \((\mathcal{X},\mathfrak{F})\). For all \(h:\mathcal{X}\to\mathbb{R}\) with \(0\leq h(X)\leq R\) almost surely under \(\mathbb{P}\) and \(\mathbb{Q}\), we have_

\[\mathbb{E}_{\mathbb{P}}[h(X)]\leq 3\,\mathbb{E}_{\mathbb{Q}}[h(X)]+4RD_{ \mathsf{H}}^{2}(\mathbb{P},\mathbb{Q}).\]

**Lemma C.6** (Lemma 1 of Jiang et al. [15]).: _For any \(f:\mathcal{X}\times\mathcal{A}\to[0,1]\), \(\pi:\mathcal{S}\times[H]\to\Delta(\mathcal{A})\), we have_

\[\mathbb{E}_{x_{1}}[f(x_{1},\pi(x_{1}))]-J(\pi)=\sum_{h=1}^{H} \mathbb{E}^{\pi}[f(x_{h},a_{h})-\mathcal{T}^{\pi}f(x_{h},a_{h})].\]

**Lemma C.7** (Offline-to-online conversion under coverability [16, 17]).: _Let \(M\) be an MDP over state space \(\mathcal{Z}\), \(\Pi\) be a policy set, and \(C_{\mathsf{cov}}=C_{\mathsf{cov}}(M,\Pi)\) be the (state-action) coverability coefficient for \(M\) and \(\Pi\) (Definition D.3). Let \(p^{{(i)}}\in\Delta(\Pi)\) be a sequence of distributions over \(\Pi\), and \(g_{h}^{{(i)}}\colon\mathcal{Z}\times\mathcal{A}\to[0,1]\) be a sequence of functions. Then we have that_

\[\sum_{t=1}^{T}\sum_{h=1}^{H}\mathbb{E}_{\pi^{{(i)}}\sim p ^{{(i)}}}\,\mathbb{E}^{\pi^{{(i)}}}\bigl{[}g_{h}^{{(i)}}(x_{h},a_{h})\bigr{]}\] \[\qquad\qquad\leq\mathcal{O}\Biggl{(}\sqrt{HC_{\mathsf{cov}}\log(T )\sum_{t=1}^{T}\sum_{h=1}^{H}\sum_{i=1}^{t-1}\mathbb{E}_{\pi^{{(i)}}\sim p^{ {(i)}}}\,\mathbb{E}^{\pi^{{(i)}}}\bigl{[}g_{h}^{{(i)}}(x_{h},a_{h}) \bigr{]}}+HC_{\mathsf{cov}}\Biggr{)}.\]

[MISSING_PAGE_FAIL:24]

The result is obtained by noting that

\[\Gamma_{\phi}\circ\pi_{\mathrm{1st}}(a_{h-1}\mid s_{h-1}) =\sum_{s^{\prime}\in\mathcal{S}}\Gamma_{\phi}(s^{\prime}\mid s_{h- 1})\pi_{\mathrm{1st}}(a_{h-1}\mid s^{\prime})\] \[=\sum_{s^{\prime}\in\mathcal{S}}\sum_{x_{h-1}:\phi^{*}(x_{h-1})=s_ {h-1}}\psi(x_{h-1}\mid s_{h-1})\mathbb{I}\{\phi(x_{h-1})=s^{\prime}\}\pi_{ \mathrm{1st}}(a_{h}\mid s^{\prime})\] \[=\sum_{x_{h-1}:\phi^{*}(x_{h-1})=s_{h-1}}\psi(x_{h-1}\mid s_{h-1} )\pi_{\mathrm{1st}}(a_{h-1}\mid\phi(x_{h-1})),\]

where the second line follows from the definition of the mismatch functions. 

**Lemma D.3** (Equivalence of state coverability and cumulative state reachability).: _Let \(M\) be an MDP defined over a state space \(\mathcal{Z}\). The following definition is equivalent to Definition D.2:_

\[C_{\mathsf{cov},\mathsf{st}}(M,\Pi)\coloneqq\max_{h\in[H]}\sum_{z\in\mathcal{ Z}}\max_{\pi\in\Pi}d_{h}^{M,\pi}(z).\] (13)

Proof of Lemma D.3.: Straightforward adaptation of the proof of Lemma 3 from Xie et al. [13]. 

Proof of Lemma D.1.: Using Lemma D.2 and Lemma D.3, we have

\[C_{\mathsf{cov},\mathsf{st}}(M_{\mathrm{obs}},\Pi_{\mathrm{1st}} \circ\Phi) =\max_{h\in[H]}\sum_{x\in\mathcal{X}}\max_{\pi_{\mathrm{1st}},\phi }d_{\mathsf{obs}}^{\tau_{\mathrm{1st}}\circ\phi}(x)\] \[=\max_{h\in[H]}\sum_{x\in\mathcal{X}}\max_{\pi_{\mathrm{1st}}, \phi}\psi^{*}(x\mid\phi^{*}(x))d_{\mathrm{1st}}^{\Gamma_{\phi}\circ\pi_{ \mathrm{1st}}}(\phi^{*}(x))\] \[=\max_{h\in[H]}\sum_{s\in\mathcal{S}}\sum_{x:\phi^{*}(x)=s}\max_{ \pi_{\mathrm{1st}},\phi}\psi^{*}(x\mid s)d_{\mathrm{1st}}^{\Gamma_{\phi}\circ \pi_{\mathrm{1st}}}(s)\] \[=\max_{h\in[H]}\sum_{s\in\mathcal{S}}\max_{\pi_{\mathrm{1st}}, \phi}d_{\mathrm{1st}}^{\Gamma_{\phi}\circ\pi_{\mathrm{1st}}}(s)\sum_{x:\phi^{ *}(x)=s}\psi^{*}(x\mid s)\] \[=C_{\mathsf{cov},\mathsf{st}}(M_{\mathrm{1st}},\Gamma_{\Phi}\circ \Pi_{\mathrm{1st}}).\]

Lastly, we show that state-action coverability is bounded by state coverability times the size of the action set.

**Lemma D.4** (State-action coverability bound).: _For any MDP \(M\) and policy set \(\Pi\), we have_

\[C_{\mathsf{cov}}(M,\Pi)\leq C_{\mathsf{cov},\mathsf{st}}(M,\Pi)|\mathcal{A}|.\]

Proof of Lemma D.4.: Let \(\mu_{s}\in\Delta(\mathcal{Z})\) witness \(C_{\mathsf{cov},\mathsf{st}}(M,\Pi)\). Fix \(h\in[H]\), which we omit below for cleanliness. Then, we have

\[\min_{\mu_{s,a}\in\Delta(\mathcal{Z}\times\mathcal{A})}\max_{\pi \in\Pi}\max_{z,a\in\mathcal{Z}\times\mathcal{A}}\biggl{\{}\frac{d^{M,\pi}(z,a )}{\mu_{s,a}(z,a)}\biggr{\}} \leq\max_{\pi\in\Pi}\max_{z,a\in\mathcal{Z}\times\mathcal{A}} \biggl{\{}\frac{d^{M,\pi}(z)\pi(a\mid z)}{\mu_{s}(z)^{1/|\mathcal{A}|}}\biggr{\}}\] \[\leq|\mathcal{A}|\max_{\pi\in\Pi}\max_{z\in\mathcal{Z}}\biggl{\{} \frac{d^{M,\pi}(z)}{\mu_{s}(z)}\biggr{\}}\] \[=C_{\mathsf{cov},\mathsf{st}}(M,\Pi)|\mathcal{A}|.\]

**Lemma D.5** (Pushforward coverability is invariant to rich observations).: _Let \(C_{\mathsf{push}}(M)\) denote the pushforward coverability parameter for an MDP \(M\) (Definition 3.3), and \(M^{*}_{\mathsf{obs}}\coloneqq(\!(M^{*}_{\mathrm{1st}},\psi^{*})\!)\). Then, we have_

\[C_{\mathsf{push}}(M^{*}_{\mathsf{obs}})=C_{\mathsf{push}}(M^{*}_{\mathrm{1st }}).\]_Furthermore, letting \(\{\mu_{\mathtt{lat},h}\in\Delta(\mathcal{S})\}_{h\in[H]}\) denote the distribution which witnesses the right-hand-side, the left-hand-side is witnessed by the distribution_

\[\mu_{\mathsf{obs},h}(x)=\psi_{h}^{\star}(x\mid\phi_{h}^{\star}(x))\mu_{\mathtt{ lat},h}(\phi_{h}^{\star}(x)).\]

This follows from an analogous equivalence of pushforward coverability and cumulative _conditional reachability_.

**Lemma D.6** (Equivalence of pushforward coverability and cumulative conditional reachability).: _Let \(M\) be an MDP defined over a state space \(\mathcal{Z}\) with transition kernel \(P\). The following definition is equivalent to pushforward coverability (Definition 3.3):_

\[C_{\mathsf{push}}(M)\coloneqq\max_{h\in[H]}\sum_{z^{\prime}\in\mathcal{Z}} \max_{z,a\in\mathcal{Z}\times\mathcal{A}}P_{h}(z^{\prime}\mid z,a).\]

Proof of Lemma D.6.: Fix \(h\in[H]\), whose dependence we omit below. For the first direction, letting \(\mu\) denote the pushforward coverability distribution, we have:

\[\sum_{z^{\prime}\in\mathcal{Z}}\max_{z,a\in\mathcal{Z}\times \mathcal{A}}P(z^{\prime}\mid z,a)=\sum_{z^{\prime}\in\mathcal{Z}}\max_{z,a \in\mathcal{Z}\times\mathcal{A}}\frac{P(z^{\prime}\mid z,a)}{\mu(z^{\prime})} \mu(z^{\prime})\leq C_{\mathsf{push}}\sum_{z^{\prime}\in\mathcal{Z}}\mu(z^{ \prime})=C_{\mathsf{push}}.\]

For the second direction, taking \(\mu(z^{\prime})\propto\max_{z,a}P(z^{\prime}\mid z,a)\), we have

\[\min_{\mu\in\Delta(\mathcal{Z})}\max_{z,a,z^{\prime}\in\mathcal{Z }\times\mathcal{A}\times\mathcal{Z}}\frac{P(z^{\prime}\mid z,a)}{\mu(z^{ \prime})} \leq\max_{z,a,z^{\prime}\in\mathcal{Z}\times\mathcal{A}\times \mathcal{Z}}\frac{P(z^{\prime}\mid z,a)}{\max_{z,\tilde{a}}P(z^{\prime}\mid \tilde{z},\tilde{a})}\sum_{\tilde{z}^{\prime}}\max_{\tilde{z},\tilde{a}}P( \tilde{z}^{\prime}\mid\tilde{z},\tilde{a})\] \[\leq\sum_{z^{\prime}}\max_{z,a}P(z^{\prime}\mid z,a).\]

Proof of Lemma D.5.: This result follows by Lemma D.6 since,

\[C_{\mathsf{push}}(M_{\mathsf{obs}}) =\sum_{x^{\prime}\in\mathcal{X}}\max_{x,a}P_{\mathsf{obs}}(x^{ \prime}\mid x,a)\] \[=\sum_{s^{\prime}\in\mathcal{S}}\sum_{x^{\prime}:\phi^{\star}(x^ {\prime})=s^{\prime}}\max_{x,a}\psi^{\star}(x^{\prime}\mid s^{\prime})P_{ \mathtt{lat}}(s^{\prime}\mid\phi^{\star}(x),a)\] \[=\sum_{s^{\prime}\in\mathcal{S}}\max_{x,a}P_{\mathtt{lat}}(s^{ \prime}\mid\phi^{\star}(x),a)\sum_{x^{\prime}:\phi^{\star}(x^{\prime})=s^{ \prime}}\psi^{\star}(x^{\prime}\mid s^{\prime})\] \[=\sum_{s^{\prime}\in\mathcal{S}}\max_{s,a}P_{\mathtt{lat}}(s^{ \prime}\mid s,a)=C_{\mathsf{push}}(M_{\mathtt{lat}}).\]

We next show that the mismatch functions can be used to express the observation-level backups for any function of the decoders. For any \(g:\mathcal{S}\to\mathbb{R}\), \(h\in[H]\), we define the function \([\Gamma_{\phi,h}\circ g]:\mathcal{S}\to\mathbb{R}\)

\[[\Gamma_{\phi,h}\circ g](s)\coloneqq\sum_{s^{\prime}\in\mathcal{S}}\Gamma_{ \phi,h}(s^{\prime}\mid s)g(s^{\prime}).\]

We further overload the Bellman operator notation and define, for any \(g:\mathcal{S}\to\mathbb{R}\) and \(M_{\mathtt{lat}}=(r_{\mathtt{lat}},P_{\mathtt{lat}})\),

\[[\mathcal{T}_{h}^{M_{\mathtt{lat}}}g](s,a)=r_{\mathtt{lat}}(s,a)+\mathbb{E}_{ s^{\prime}\sim P_{\mathtt{lat}}(s,a)}[g(s^{\prime})].\]

**Lemma D.7**.: _Let \(M_{\mathsf{obs}}=\llangle(M_{\mathtt{lat}},\psi^{\star})\), \(\phi^{\star}\coloneqq(\psi^{\star})^{-1}\), \(\phi\in\Phi\), and \(\Gamma_{\phi}\) be the mismatch function for emission \(\psi^{\star}\) (Definition D.1). Then, for any \(f_{\mathtt{lat}}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}\), \(h\in[H]\), and \((x,a)\in\mathcal{X}\times\mathcal{A}\), we have_

\[\Big{[}\mathcal{T}_{h}^{M_{\mathsf{obs}}}(f_{\mathtt{lat}}\circ\phi_{h+1}) \Big{]}(x,a)=\Big{[}\mathcal{T}_{h}^{M_{\mathtt{lat}}}(\Gamma_{\phi,h+1}\circ V _{f_{\mathtt{lat}}})\Big{]}(\phi_{h}^{\star}(x),a).\]Proof of Lemma D.7.: Let \(f\coloneqq f_{\mathrm{1at}}\), \(h\in[H]\), and \((x,a)\in\mathcal{X}\times\mathcal{A}\) be given. Then, we have:

\[\Big{[}\mathcal{T}_{h}^{M_{\mathrm{obs}}}(f\circ\phi_{h+1})\Big{]}( x,a)\] \[\quad=r_{\mathrm{1at},h}(\phi_{h}^{\star}(x),a)+\mathbb{E}_{s_{h+ 1}\sim P_{\mathrm{lat},h}(\phi_{h}^{\star}(x),a)}\,\mathbb{E}_{x_{h+1}\sim\psi _{h+1}^{\star}(s_{h+1})}[V_{f}(\phi(x_{h+1}))]\] \[\quad=r_{\mathrm{1at},h}(\phi_{h}^{\star}(x),a)+\mathbb{E}_{s_{h+ 1}\sim P_{\mathrm{lat},h}(\phi_{h}^{\star}(x),a)}\left[\sum_{x_{h+1}\in \mathcal{X}}\psi^{\star}(x_{h+1}\mid s_{h+1})V_{f}(\phi(x_{h+1}))\right]\] \[\quad=r_{\mathrm{1at},h}(\phi_{h}^{\star}(x),a)+\mathbb{E}_{s_{h+ 1}\sim P_{\mathrm{lat},h}(\phi_{h}^{\star}(x),a)}\left[\sum_{s^{\prime}\in \mathcal{S}}\Gamma_{\phi}(s^{\prime}\mid s_{h+1})V_{f}(s^{\prime})\right]\] \[\quad=r_{\mathrm{1at},h}(\phi_{h}^{\star}(x),a)+\mathbb{E}_{s_{h+ 1}\sim P_{\mathrm{lat},h}(\phi_{h}^{\star}(x),a)}[\Gamma_{\phi}\circ V_{f}(s_{h +1})]\] \[\quad=\Big{[}\mathcal{T}_{h}^{M_{\mathrm{lat}}}(\Gamma_{\phi} \circ V_{f})\Big{]}(\phi_{h}^{\star}(x),a),\]

where the third line follows from the definition of the mismatch function \(\Gamma_{\phi}\). 

We next show that the mismatch functions can be used to realize the pushforward dynamics \(\phi_{h}^{\star}M_{\mathrm{obs}}^{\star}\), which we recall are defined as:

\[\big{[}\phi_{h}^{\sharp}M_{\mathrm{obs},h}^{\star}\big{]}(r,s^{\prime}\mid x, a)=\sum_{x^{\prime}:\phi(x^{\prime})=s^{\prime}}M_{\mathrm{obs},h}^{\star}(r,x^{ \prime}\mid x,a).\] (14)

We also recall the notation \(\left[\Gamma_{\phi,h+1}\circ M_{\mathrm{1at}}\right]_{h}\), defined via:

\[\left[\Gamma_{\phi}\circ M_{\mathrm{1at}}\right]_{h}(r_{h},s_{h+1}\mid s_{h}, a_{h})\coloneqq\sum_{s_{h+1}^{\star}\in\mathcal{S}}M_{\mathrm{1at},h}(r_{h},s_{h+1}^{ \prime}\mid s_{h},a_{h})\Gamma_{\phi,h+1}(s_{h+1}\mid s_{h+1}^{\prime}).\]

**Lemma D.8** (Pushforward model realizability via mismatch functions).: _For all \(\phi\in\Phi\), \(h\in[H]\), we have:_

\[[\phi_{h+1}\sharp M_{\mathrm{obs},h}^{\star}](\cdot\mid x,a)=\big{[}[\Gamma_{ \phi}\circ M_{\mathrm{1at}}^{\star}]_{h}\circ\phi_{h}^{\star}\big{]}(\cdot \mid x,a)\] (15)

Proof of Lemma D.8.: Note that \(\Gamma_{\phi}\) can alternatively be written as:

\[\Gamma_{\phi,h}(s_{h}^{\prime}\mid s_{h})=\sum_{x_{h}:\phi(x_{h})=s_{h}^{ \prime}}\psi_{h}^{\star}(x_{h}\mid s_{h}).\]

We have

\[\phi_{h+1}\sharp M_{\mathrm{obs},h}^{\star}(r_{h+1},s_{h+1}\mid x _{h},a_{h})\] \[\quad=\sum_{x_{h+1}:\phi_{h+1}(x_{h+1})=s_{h+1}}M_{\mathrm{obs},h }^{\star}(r_{h+1},x_{h+1}\mid x_{h},a_{h})\] \[\quad=\sum_{x_{h+1}:\phi_{h+1}(x_{h+1})=s_{h+1}}\left(\sum_{r,s^{ \prime}\in\mathbb{R}\times\mathcal{S}}M_{\mathrm{1at},h}^{\star}(r,s^{\prime} \mid\phi_{h}^{\star}(x_{h}),a_{h})\psi_{h+1}^{\star}(x_{h+1}\mid s^{\prime})\right)\] \[\quad=\sum_{r,s^{\prime}\in\mathbb{R}\times\mathcal{S}}M_{ \mathrm{1at},h}^{\star}(r,s^{\prime}\mid\phi_{h}^{\star}(x_{h}),a_{h})\sum_{x _{h+1}:\phi_{h+1}(x_{h+1})=s_{h+1}}\psi_{h+1}^{\star}(x_{h+1}\mid s^{\prime})\] \[\quad=\sum_{r,s^{\prime}\in\mathbb{R}\times\mathcal{S}}M_{ \mathrm{1at},h}^{\star}(r,s^{\prime}\mid\phi_{h}^{\star}(x_{h}),a_{h})\Gamma_{ \phi,h+1}(s^{\prime}\mid s_{h+1})\] \[\quad=\left[\Gamma_{\phi}\circ M_{\mathrm{1at}}^{\star}\right]_{h }(r,s_{h+1}\mid\phi_{h}^{\star}(x_{h}),a_{h}),\]

as desired.

[MISSING_PAGE_FAIL:28]

* Statistical modularity (\(\boldsymbol{\mathcal{V}}\)): This is obtained by noting that the observation-level dynamics also satisfy the low-rank property with the same dimension. Formally, letting \(P_{\text{obs}}\) be the transition kernel for \(\llllll_{\text{1at}},\psi^{\star}\rightharpoonup\) and \(\phi^{\star}=(\psi^{\star})^{-1}\), we have \[P_{\text{obs},h}(x_{h+1}\mid x_{h},a_{h}) =\sum_{s_{h+1}\in\mathcal{S}}P_{\text{1at},h}(s_{h+1}\mid\phi^{ \star}_{h}(x_{h}),a_{h})\psi^{\star}_{h+1}(x_{h+1}\mid s_{h+1})\] \[=\sum_{s_{h+1}\in\mathcal{S}}\langle\xi^{\star}_{\text{1at},h}( \phi^{\star}_{h}(x),a),\mu^{\star}_{\text{1at},h+1}(s_{h+1})\rangle\psi^{\star }_{h+1}(x_{h+1}\mid s_{h+1})\] \[=\bigg{\langle}\xi^{\star}_{\text{1at},h}(\phi^{\star}_{h}(x),a), \sum_{s_{h+1}\in\mathcal{S}}\mu^{\star}_{\text{1at},h+1}(s_{h+1})\psi^{\star }_{h+1}(x_{h+1}\mid s_{h+1})\bigg{\rangle}.\] Thus, the transition kernel \(P_{\text{obs}}\) is a low-rank MDP with \(\mu_{\text{obs},h+1}(x_{h+1})\coloneqq\sum_{s_{h+1}}\mu^{\star}_{\text{1at},h +1}(s_{h+1})\psi^{\star}_{h+1}(x_{h+1}\mid s_{h+1})\) and feature class \[\Xi_{\text{1at}}\circ\Phi=\Big{\{}\xi_{\text{1at}}\circ\phi=\{\xi_{h}\circ\phi_ {h}:x,a\mapsto\xi_{h}(\phi_{h}(x),a)\}_{h=1}^{H}\mid\xi_{\text{1at}}\in\Xi_{ \text{1at}},\phi\in\Phi\Big{\}}.\] Lastly, since \(r_{\text{obs}}=[r_{\text{1at}}\circ\phi^{\star}]\), the reward function is also linear with the same unknown feature class. Thus we can apply Vox directly on top of the observations, with the feature class \(\Xi_{\text{1at}}\circ\Phi\), which will achieve a complexity \(\mathsf{poly}(d,|\mathcal{A}|,H,\log|\Xi_{\text{1at}}|,\log|\Phi|,\varepsilon^ {-1},\log\bigl{(}\delta^{-1}\bigr{)})\).

**Known Deterministic MDP (\(|\mathcal{M}_{\text{1at}}|=1\)) (\(\boldsymbol{\mathcal{V}}\)).**

* Latent class \(\mathcal{M}_{\text{1at}}\): \(\mathcal{M}_{\text{1at}}=\{M_{\text{1at}}=(\mathcal{S},\mathcal{A},P_{\text{1at }},R_{\text{1at}},H)\}\) is a set of MDPs of size 1 with both deterministic rewards and deterministic transitions.
* Latent complexity \(\mathsf{comp}\): We take \(\mathsf{comp}(\mathcal{M}_{\text{1at}},\varepsilon,\delta)=0\), which is attainable as \(M_{\text{1at}}\) is known and we can simply deploy its optimal policy.
* Statistical modularity (\(\boldsymbol{\mathcal{V}}\)): We note that, due to determinism, the latent optimal policy can be chosen to be _open-loop_ without loss of generality, and thus will always experience the same trajectory \((s^{\star}_{1},a^{\star}_{1},\ldots,s^{\star}_{H},a^{\star}_{H})\). We can define the observation-level policy which commits to this same sequence of actions, i.e. \(\pi_{\text{obs},h}(x_{h})=a^{\star}_{h}\) for all \(x_{h}\). This will be an optimal policy for any \(M_{\text{obs}}=\llllll_{\text{1at}},\psi\rightharpoonup\), and can also be learned in 0 samples. **Low State Occupancy (\(\forall\,\pi:\mathcal{S}\to\Delta(\mathcal{A})\)) (\(\boldsymbol{\mathcal{V}}\)).**
* Latent class \(\mathcal{M}_{\text{1at}}\): \(\mathcal{M}_{\text{1at}}=\{M_{\text{1at}}=(\mathcal{S},\mathcal{A},P_{\text{1at }},R_{\text{1at}},H)\}\) is a set of MDPs for which we have a realizable value function class, and such that there exists a feature map \(\zeta_{\text{1at}}=\big{\{}\zeta_{\text{1at},h}:\mathcal{S}\to\mathbb{R}^{d} \big{\}}_{h=1}^{H}\) such that for all \(\pi:\mathcal{S}\to\Delta(\mathcal{A})\) and for all \(M_{\text{1at}}\in\mathcal{M}_{\text{1at}}\), we have \[\forall h\in[H]\ \exists\theta^{M_{\text{1at}},\pi}_{h}:\quad d^{M_{\text{1at}},\pi}_{h}( s)=\Big{\langle}\zeta_{\text{1at},h}(s),\theta^{M_{\text{1at}},\pi}_{h}\Big{\rangle}.\] Note that the feature map does not need to be known.
* Latent complexity \(\mathsf{comp}\): We take \(\mathsf{comp}(\mathcal{M}_{\text{1at}},\varepsilon,\delta)=\mathsf{poly}(d,| \mathcal{A}|,H,\log|\mathcal{F}_{\text{1at}}|,\varepsilon^{-1},\log\bigl{(} \delta^{-1}\bigr{)})\), which is attainable by the Bilin-Ucb algorithm of Du et al., since i) MDPs with this property have Bilinear rank bounded by \(d|\mathcal{A}|\) (see Definition 4.3 and Lemma 4.6 of Du et al. [4]), and ii) one can construct the value function class \(\mathcal{F}_{\text{1at}}=\{Q^{M_{\text{1at}}},\star\mid M_{\text{1at}}\in \mathcal{M}_{\text{1at}}\}\), which is realizable and has size \(\log|\mathcal{F}_{\text{1at}}|=\log|\mathcal{M}_{\text{1at}}|\).
* Statistical modularity (\(\boldsymbol{\mathcal{V}}\)): We firstly note that one can construct a realizable value function class for the set \(\llllllll_{\text{1at}},\rightharpoonup\), via the set \(\mathcal{F}_{\text{obs}}=\big{\{}Q^{M_{\text{1at}},\star}\circ\phi\mid M_{\text{ 1at}}\in\mathcal{M}_{\text{1at}},\phi\in\Phi\big{\}}\). This is realizable since, for any \(M_{\text{obs}}\coloneqq\llllll_{\text{1at}},\psi\rightharpoonup\), letting \(\phi^{\star}=\psi^{-1}\), we have \(Q^{M_{\text{obs}},\star}=Q^{M_{\text{1at}},\star}\circ\phi^{\star}\), and that this class has size \(\log|\mathcal{M}_{\text{1at}}||\Phi|\). We can then show that the occupancies \(d^{M_{\text{obs}},\star},f_{\text{obs}}\), for \(f_{\text{obs}}\in\mathcal{F}_{\text{obs}}\), can also be expressed as \(d\)-dimensional linear function for an appropriate choice of features, which will imply that the Bilin-Ucb algorithm run directly on \(M_{\text{obs}}\) will attain a complexity of \(\mathsf{poly}(d,|\mathcal{A}|,H,\log\mathcal{M}_{\text{1at}},\log\Phi,\varepsilon^{ -1},\log\bigl{(}\delta^{-1}\bigr{)})\). To obtain this, we recall the following lemma: **Lemma D.2**.: _Let \(\{\Gamma_{\phi}\}_{\phi\in\Phi}\) denote the mismatch functions for emission \(\psi^{\star}\), and let \(M_{\text{obs}}=\llllllll_{\text{1at}},\psi^{\star}\rightharpoonup\). Then, for any \(\pi_{\text{1at}}\in\Pi_{\text{1at}}\), \(\phi\in\Phi\), \(h\in[H]\), \(x\in\mathcal{X}\), we have_ \[d^{M_{\text{obs}},\pi_{\text{1at}}\circ\phi}_{h}(x)=\psi^{\star}_{h}(x\mid\phi^{ \star}_{h}(x))d^{M_{\text{1at}},\Gamma_{\phi}\circ\pi_{\text{1at}}}_{h}(\phi^{ \star}_{h}(x)).\]

[MISSING_PAGE_FAIL:30]

**Known Stochastic MDP** (\(|\mathcal{M}_{\mathtt{1at}}|=1\)) **()**.
* Latent class \(\mathcal{M}_{\mathtt{1at}}\): \(\mathcal{M}_{\mathtt{1at}}=\{M_{\mathtt{1at}}=(\mathcal{S},\mathcal{A},P_{ \mathtt{1at}},R_{\mathtt{1at}},H)\}\) is a set of MDPs of size 1.
* Latent complexity \(\mathtt{comp}\): We take \(\mathtt{comp}(\mathcal{M}_{\mathtt{1at}},\varepsilon,\delta)=0\), which is attainable as \(M_{\mathtt{1at}}\) is known and we can simply deploy its optimal policy.
* Statistical intractability (): This is precisely the setting of Theorem 3.1, which shows that at least \(\Omega(N/\log(N))\) samples will be needed, where \(N=|\Phi|\).

**Bellman rank (\(Q\)-type or \(V\)-type) ()**
* Latent assumption: \(\mathcal{M}_{\mathtt{1at}}=\{M_{\mathtt{1at}}=(\mathcal{S},\mathcal{A},P_{ \mathtt{1at}},R_{\mathtt{1at}},H)\}\) is a set of latent models such that each \(M_{\mathtt{1at}}\in\mathcal{M}_{\mathtt{1at}}\) has \(Q\)-type Bellman rank \(d\) or \(V\)-type Bellman rank \(d\)[13]. Letting \(\mathcal{F}\) be a realizable value function class for \(\mathcal{M}_{\mathtt{1at}}\), in the \(Q\)-type case, this means that the \(|\Pi_{\mathcal{F}}|\times|\mathcal{F}|\) matrix \[\mathcal{E}_{h}^{Q}(\pi,f)=\mathbb{E}^{\pi}\Big{[}f_{h}(s_{h},a_{h})-r_{h}- \max_{a^{\prime}}f_{h+1}(s_{h+1},a^{\prime})\Big{]},\] admits a rank \(d\) factorization. In the \(V\)-type case, the matrix \[\mathcal{E}_{h}^{V}(\pi,f)=\mathbb{E}_{s_{h}\sim d_{h}^{\pi},a_{h}\sim\pi_{f}} \Big{[}f_{h}(s_{h},a_{h})-r_{h}-\max_{a^{\prime}}f_{h+1}(s_{h+1},a^{\prime}) \Big{]}\] admits a rank-\(d\) matrix factorization.
* Latent complexity \(\mathtt{comp}\): We take \(\mathtt{comp}(\mathcal{M}_{\mathtt{1at}},\varepsilon,\delta)=\mathtt{poly}(d,H,|\mathcal{A}|\log|\mathcal{F}|,\varepsilon^{-1},\log\bigl{(}\delta^{-1}\bigr{)})\) for the \(V\)-type Bellman rank case, which is achievable by the Olive algorithm of Jiang et al. [10], and \(\mathtt{comp}(\mathcal{M}_{\mathtt{1at}},\varepsilon,\delta)=\mathtt{poly}(d,H,\log|\mathcal{F}|,\varepsilon^{-1},\log(\delta^{-1}))\) for \(Q\)-type Bellman rank, which is achievable by the Bilin-Ucb algorithm of Du et al. [11].
* Statistical intractability (): We note that the construction in Theorem 3.1 has \(|\mathcal{M}_{\mathtt{1at}}|=1\), which trivially has Bellman rank equal to \(1\), so Theorem 3.1 precludes statistical modularity with complexity \(\mathtt{comp}\).

**Eluder dimension + Bellman Completeness ()**

* Latent class \(\mathcal{M}_{\mathtt{1at}}\): \(\mathcal{M}_{\mathtt{1at}}=\{M_{\mathtt{1at}}=(\mathcal{S},\mathcal{A},P_{ \mathtt{1at}},R_{\mathtt{1at}},H)\}\) is a set of MDPs such that there is a function class \(\mathcal{F}_{\mathtt{1at}}\) satisfying \[\forall f_{\mathtt{1at}}\in\mathcal{F}_{\mathtt{1at}},M_{\mathtt{1at}}\in \mathcal{M}_{\mathtt{1at}}:\quad\mathcal{T}^{M_{\mathtt{1at}}}f_{\mathtt{1at}} \in\mathcal{F}_{\mathtt{1at}}.\] Furthermore, each \(M_{\mathtt{1at}}\in\mathcal{M}_{\mathtt{1at}}\) has Bellman-Eluder dimension bounded by \(d\) (see Definition 8 of [13]).
* Latent complexity \(\mathtt{comp}\): We take \(\mathtt{comp}(\mathcal{M}_{\mathtt{1at}},\varepsilon,\delta)=\mathtt{poly}(d,H,\log|\mathcal{F}|,\varepsilon^{-1},\log\bigl{(}\delta^{-1}\bigr{)})\), which is attainable by the Golf algorithm of Jin et al. [13].
* Statistical intractability (): As in the Bellman rank case, the construction in Theorem 3.1 has \(|\mathcal{M}_{\mathtt{1at}}|=1\), so we can take \(\mathcal{F}_{\mathtt{1at}}=\{Q^{M_{\mathtt{1at}},\star}\mid M_{\mathtt{1at}}\in \mathcal{M}_{\mathtt{1at}}\}\) which is evidently complete for \(\mathcal{T}^{M_{\mathtt{1at}}}\), and has Eluder dimension \(1\), so Theorem 3.1 precludes statistical modularity with complexity \(\mathtt{comp}\).

\(Q^{\star}\)**-irrelevant State Abstraction ()**

* Latent class \(\mathcal{M}_{\mathtt{1at}}\): \(M_{\mathtt{1at}}=(\mathcal{S},\mathcal{A},P_{\mathtt{1at}},R_{\mathtt{1at}},H)\) such that there is a known state abstraction function \(\zeta_{\mathtt{1at}}:\mathcal{S}\rightarrow\mathcal{Z}\) such that \(\zeta_{\mathtt{1at}}(s)=\zeta_{\mathtt{1at}}(s^{\prime})\) implies that \(Q^{M_{\mathtt{1at}},\star}(s,a)=Q^{M_{\mathtt{1at}},\star}(s^{\prime},a)\) for all \(a\in\mathcal{A}\).
* Latent complexity \(\mathtt{comp}\): We take \(\mathtt{comp}(\mathcal{M}_{\mathtt{1at}},\varepsilon,\delta)=\mathtt{poly}(| \mathcal{Z}|,|\mathcal{A}|,H,\varepsilon^{-1},\log\bigl{(}\delta^{-1}\bigr{)})\) which is attainable by the Olive algorithm of Jiang et al. [10].
* Statistical intractability (): We take \(\mathcal{M}_{\mathtt{1at}}=\{M_{\mathtt{1at}}\}\) as the MDP class from the construction of Theorem 3.1. Let \(Q^{\star}_{\mathtt{1at}}:=Q^{M_{\mathtt{1at}},\star}\). Note that we have \(Q^{\star}_{\mathtt{1at}}(s,a)\in\{0,1\}\) for all \(s,a\), so we can take a latent abstract state space \(\mathcal{Z}=\{(0,0),(0,1),(1,0),(1,1)\}\) and a state abstraction function \(\zeta_{\mathtt{1at}}\) such that \(\zeta_{\mathtt{1at}}(s)=(i,j)\) if \(Q^{\star}_{\mathtt{1at}}(s,0)=i\) and \(Q^{\star}_{\mathtt{1at}}(s,1)=j\). This satisfies the property of a \(Q^{\star}\)-irrelevant abstraction, since \(\zeta_{\mathtt{1at}}(s)=\zeta_{\mathtt{1at}}(s^{\prime})=(i,j)\) implies that \(Q^{\star}_{\mathtt{1at}}(s,0)=Q^{\star}_{\mathtt{1at}}(s^{\prime},0)=i\) and \(Q^{\star}_{\mathtt{1at}}(s,1)=Q^{\star}_{\mathtt{1at}}(s^{\prime},1)=j\). This has a constant-sized abstract space (\(|\mathcal{Z}|=4\)) and \(|\mathcal{A}|=2\), so Theorem 3.1 precludes statistical modularity with complexity \(\mathtt{comp}\).

**Linear Mixture MDP (x).**

* Latent class \(\mathcal{M}_{\mathrm{1at}}\): MDPs \(M_{\mathrm{1at}}=(\mathcal{S},\mathcal{A},P_{\mathrm{1at}},R_{\mathrm{1at}},H)\) such that there is a known feature map \(\zeta_{\mathrm{1at}}=\{\zeta_{\mathrm{1at},h}:s^{\prime},s,a\mapsto\mathbb{R}^{ d}\}_{h=1}^{H}\) such that \[\forall h\in[H],\exists\theta_{h}\in\mathbb{R}^{d}:\quad P_{\mathrm{1at},h}(s^{ \prime}\mid s,a)=\langle\zeta_{\mathrm{1at},h}(s^{\prime}\mid s,a),\theta_{h}\rangle\]
* Latent complexity \(\mathsf{comp}\): We take \(\mathsf{comp}(\mathcal{M}_{\mathrm{1at}},\varepsilon,\delta)=\mathsf{poly}(d,H, \varepsilon^{-1},\log\bigl{(}\delta^{-1}\bigr{)})\), which is attainable by the Ucrl-Vtr\({}^{+}\) algorithm of Zhou et al. [2018]
* Statistical intractability (x): We take \(\mathcal{M}_{\mathrm{1at}}=\{M_{\mathrm{1at}}\}\) to be the construction of Theorem 3.1. Here, there is a single latent model, so this is trivially embeddable with \(\zeta_{\mathrm{1at},h}(s^{\prime}\mid s,a)=P_{\mathrm{1at},h}^{\star}(s^{ \prime}\mid s,a)\in\mathbb{R}^{1}\). This has dimension \(d=1\), so Theorem 3.1 precludes statistical modularity with complexity \(\mathsf{comp}\).

**Linear \(Q^{\star}/V^{\star}\) (x).**

* Latent class \(\mathcal{M}_{\mathrm{1at}}\): MDPs \(M_{\mathrm{1at}}=(\mathcal{S},\mathcal{A},P_{\mathrm{1at}},R_{\mathrm{1at}},H)\) such that there are known features maps \(\alpha_{\mathrm{1at}}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}^{d}\) and \(\beta_{\mathrm{1at}}:\mathcal{S}\to\mathbb{R}^{d}\) such that for all \(M_{\mathrm{1at}}\in\mathcal{M}_{\mathrm{1at}}\), there exists unknown parameters \(\theta_{Q},\theta_{V}\in\mathbb{R}^{d}\) such that \(Q^{\beta_{\mathrm{1at}},\star}(s,a)=\langle\alpha_{\mathrm{1at}}(s,a),\theta_{ Q}\rangle\) and \(V^{\beta_{\mathrm{1at}},\star}(s)=\langle\beta_{\mathrm{1at}}(s),\theta_{V}\rangle\).
* Latent complexity \(\mathsf{comp}\): We take \(\mathsf{comp}(\mathcal{M}_{\mathrm{1at}},\varepsilon,\delta)=\mathsf{poly}(d,H, \varepsilon^{-1},\log\bigl{(}\delta^{-1}\bigr{)})\), which is attainable by the Bilin-Ucb algorithm of Du et al. [2018].
* Statistical intractability (x): We can take \(\mathcal{M}_{\mathrm{1at}}\) to be the latent MDP class from the construction of Theorem 3.1. Since there is a single latent model, this is trivially embeddable with dimension \(1\), i.e. we can take \(\zeta_{\mathrm{1at}}(s,a)=Q_{\mathrm{1at}}^{*}(s,a)\) and \(\beta_{\mathrm{1at}}(s)=V_{\mathrm{1at}}^{*}(s)\). This has dimension \(d=1\), so Theorem 3.1 precludes statistical modularity with complexity \(\mathsf{comp}\).

**Low State or State-Action Occupancy (\(\forall\,\pi_{{}_{M}}:M\in\mathcal{M}\)) (x).**

* Latent class \(\mathcal{M}_{\mathrm{1at}}\): In the Low State Occupancy model, \(\mathcal{M}_{\mathrm{1at}}=\{M_{\mathrm{1at}}=(\mathcal{S},\mathcal{A},P_{ \mathrm{1at}},R_{\mathrm{1at}},H)\}\) is a set of MDPs such that there exists a feature map \(\zeta_{\mathrm{1at}}^{V}=\bigl{\{}\zeta_{\mathrm{1at},h}:\mathcal{S}\to \mathbb{R}^{d}\bigr{\}}_{h=1}^{H}\) such that for all \(\pi\in\{\pi_{M_{\mathrm{1at}}}\mid M_{\mathrm{1at}}\in\mathcal{M}_{\mathrm{1at}}\}\) and for all \(M_{\mathrm{1at}}\in\mathcal{M}_{\mathrm{1at}}\), we have \[\forall h\in[H]\ \exists\theta_{h}^{M_{\mathrm{1at}},\pi}:\quad d_{h}^{M_{ \mathrm{1at}},\pi}(s)=\Bigl{\langle}\zeta_{\mathrm{1at},h}^{V}(s),\theta_{h}^{ M_{\mathrm{1at}},\pi}\Bigr{\rangle}.\] For the State-Action Occupancy model, we have that there exists a feature map \(\zeta_{\mathrm{1at}}^{Q}=\bigl{\{}\zeta_{\mathrm{1at},h}:\mathcal{S}\times \mathcal{A}\to\mathbb{R}^{d}\bigr{\}}_{h=1}^{H}\) such that for all \(\pi\in\{\pi_{M_{\mathrm{1at}}}\mid M_{\mathrm{1at}}\in\mathcal{M}_{\mathrm{1at}}\}\) and for all \(M_{\mathrm{1at}}\in\mathcal{M}_{\mathrm{1at}}\), we have \[\forall h\in[H]\ \exists\theta_{h}^{M_{\mathrm{1at}},\pi}:\quad d_{h}^{M_{ \mathrm{1at}},\pi}(s,a)=\Bigl{\langle}\zeta_{\mathrm{1at},h}^{Q}(s,a),\theta_{h }^{M_{\mathrm{1at}},\pi}\Bigr{\rangle}.\] Note that the feature map does not need to be known in either case.
* Latent complexity \(\mathsf{comp}\): We take \(\mathsf{comp}(\mathcal{M}_{\mathrm{1at}},\varepsilon,\delta)=\mathsf{poly}(d,| \mathcal{A}|,H,\log|\mathcal{F}_{\mathrm{1at}}|,\varepsilon^{-1},\log\bigl{(} \delta^{-1}\bigr{)})\) for the state occupancy case and \(\mathsf{comp}(\mathcal{M}_{\mathrm{1at}},\varepsilon,\delta)=\mathsf{poly}(d,H, \log|\mathcal{M}_{\mathrm{1at}}|,\varepsilon^{-1},\log\bigl{(}\delta^{-1} \bigr{)})\). Both are attainable by the Bilin-Ucb algorithm of Du et al., since i) MDPs with this property have Bilinear rank bounded by \(d|\mathcal{A}|\) and \(d\) respectively (see Definition 4.3 and Lemma 4.6 of [2018]), and ii) one can construct the value function class \(\mathcal{F}_{\mathrm{1at}}=\{Q^{M_{\mathrm{1at}},\star}\mid M_{\mathrm{1at}}\in \mathcal{M}_{\mathrm{1at}}\}\) which is realizable and has size \(\log|\mathcal{F}_{\mathrm{1at}}|=\log|\mathcal{M}_{\mathrm{1at}}|\).
* Intractability: We can take the construction of Theorem 3.1, which has \(|\mathcal{M}_{\mathrm{1at}}|=1\) and thus is trivially embeddable with dimension \(1\), i.e. we can take \(\zeta_{\mathrm{1at}}^{V}(s)=d^{M_{\mathrm{1at}},\pi_{M_{\mathrm{1at}}}}(s)\) and \(\zeta_{\mathrm{1at}}^{Q}(s,a)=d^{M_{\mathrm{1at}},\pi_{M_{\mathrm{1at}}}}(s,a)\).

**Bisimulation (?)**

* Latent class \(\mathcal{M}_{\mathrm{1at}}\): MDPs \(M_{\mathrm{1at}}=(\mathcal{S},\mathcal{A},P_{\mathrm{1at}},R_{\mathrm{1at}},H)\) such that there is a known state abstraction function \(\zeta_{\mathrm{1at}}:\mathcal{S}\to\mathcal{Z}\) such that \(\zeta_{\mathrm{1at}}(s)=\zeta_{\mathrm{1at}}(\bar{s})\) implies that \(R_{\mathrm{1at}}(s,a)=R_{\mathrm{1at}}(\bar{s},a)\) for all \(a\in\mathcal{A}\) as well as \(\sum_{s^{\prime}:\zeta_{\mathrm{1at}}(s^{\prime})=z^{\prime}}P_{\mathrm{1at}}(s^{ \prime}\mid s,a)=\sum_{s^{\prime}:\zeta_{\mathrm{1at}}(s^{\prime})=z^{\prime}}P_{ \mathrm{1at}}(s^{\prime}\mid\bar{s},a)\) for all \(z^{\prime}\).

* Latent complexity \(\mathsf{comp}\): We take \(\mathsf{comp}(\mathcal{M}_{\mathsf{1at}},\varepsilon,\delta)=\mathsf{poly}(| \mathcal{Z}|,|\mathcal{A}|,H,\varepsilon^{-1},\log\bigl{(}\delta^{-1}\bigr{)})\) which is attainable by the Olive algorithm of [13].
* Openness (?): A negative result does not follow from existing constructions, since the dynamics from the tree-based construction of Theorem 3.1 are not bisimilar unless \(|\mathcal{Z}|=|\mathcal{S}|\), which allows for the application of tabular methods. At the same time, a positive result does not follow from existing methods, since it is non-trivial to extend existing Block MDP methods to use the bisimulation state abstraction in a way that only pays for \(|\mathcal{Z}|\).

**Low State-Action Occupancy (\(\forall\pi:\mathcal{S}\to\Delta(\mathcal{A})\)) (?*)**

* Latent class \(\mathcal{M}_{\mathsf{1at}}\): \(\mathcal{M}_{\mathsf{1at}}=\{M_{\mathsf{1at}}=(\mathcal{S},\mathcal{A},P_{ \mathsf{1at}},R_{\mathsf{1at}},H)\}\) is a set of MDPs such that there exists a feature map \(\zeta_{\mathsf{1at}}^{Q}=\bigl{\{}\zeta_{\mathsf{1at},h}:\mathcal{S}\times \mathcal{A}\to\mathbb{R}^{d}\bigr{\}}_{h=1}^{H}\) such that for all \(\pi:\mathcal{S}\to\Delta(\mathcal{A})\) and for all \(M_{\mathsf{1at}}\in\mathcal{M}_{\mathsf{1at}}\), we have \[\forall h\in[H]\ \exists\theta_{h}^{M_{\mathsf{1at}},\pi}:\quad d_{h}^{M_{ \mathsf{1at}},\pi}(s,a)=\Bigl{\langle}\zeta_{\mathsf{1at},h}^{Q}(s,a),\theta_{ h}^{M_{\mathsf{1at}},\pi}\Bigr{\rangle}.\] Note that the feature map does not need to be known.
* We take \(\mathsf{comp}(\mathcal{M}_{\mathsf{1at}},\varepsilon,\delta)=\mathsf{poly}(d,H, \log|\mathcal{M}_{\mathsf{1at}}|,\varepsilon^{-1},\log(\delta^{-1}))\), which is attainable by the Bilin-Ucb algorithm of Du et al., since i) MDPs with this property have Bilinear rank bounded by \(d\) (see Definition 4.3 and Lemma 4.6 of [12]), and ii) one can construct a realizable value function class of size \(\log|\mathcal{F}|=\log|\mathcal{M}|\).
* Openness (?): A negative result does not follow from existing constructions, since the dynamics from the tree-based construction of Theorem 3.1 do not have linear occupancies for all \(\pi:\mathcal{S}\to\Delta(\mathcal{A})\) unless \(d=|\mathcal{S}|\), which allows for the application of tabular methods, and the dynamics from the bandit-based construction Theorem E.1 do not have linear occupancies for all \(\pi:\mathcal{S}\to\Delta(\mathcal{A})\) unless \(d=|\mathcal{A}|\). At the same time, unlike the low state occupancy case, a positive result does not follow as it is unclear if we can express the observation-space occupancies linearly.
* Statistical tractability with additional (suboptimal) \(|\mathcal{A}|\)-dependence (\(\checkmark\)): Note that we can reduce to the Low State Occupancy case (\(\checkmark\)), since \[d^{\pi}(s)=\sum_{a\in\mathcal{A}}d^{\pi}(s,a)=\left\langle\theta^{\pi},\sum_ {a\in\mathcal{A}}\zeta_{\mathsf{1at}}^{Q}(s,a)\right\rangle\coloneqq\bigl{\langle} \theta^{\pi},\zeta_{\mathsf{1at}}^{V}(s)\bigr{\rangle}.\] However, this blows up the feature norm bound of the feature map \(\zeta_{\mathsf{1at}}^{V}(s)\) by a factor of \(|\mathcal{A}|\), which will appear logarithmically in the bound obtained by Bilin-Ucb.

**Model class + Coverability (\(\forall\pi:\mathcal{S}\to\Delta(\mathcal{A})\)) (?).**

* Latent class \(\mathcal{M}_{\mathsf{1at}}\): \(\mathcal{M}_{\mathsf{1at}}=\{M_{\mathsf{1at}}=(\mathcal{S},\mathcal{A},P_{ \mathsf{1at}},R_{\mathsf{1at}},H)\}\) is a set of MDPs that all satisfy coverability with respect to all policies \(\pi_{\mathsf{1at}}:\mathcal{S}\to\Delta(\mathcal{A})\), i.e. we have \[\forall M_{\mathsf{1at}}\in\mathcal{M}_{\mathsf{1at}}:\quad C_{\mathsf{cov}}(M _{\mathsf{1at}})=\inf_{\mu_{h}\in\Delta(\mathcal{S}\times\mathcal{A})}\sup_{h \in[H]}\sup_{\pi:\mathcal{S}\to\Delta(\mathcal{A})}\left\|\frac{d_{h}^{M_{ \mathsf{1at}},\pi}}{\mu_{h}}\right\|_{\infty}<\infty\]
* Latent complexity \(\mathsf{comp}\): We take \(\mathsf{comp}(\mathcal{M}_{\mathsf{1at}},\varepsilon,\delta)=\mathsf{poly}(C_{ \mathsf{cov}},H,\log|\mathcal{M}_{\mathsf{1at}}|,\varepsilon^{-1},\log\bigl{(} \delta^{-1}\bigr{)})\), which is attainable by the Golf algorithm via the results of Xie, Foster, Bai, Jiang, and Kakade (see also Lemma F.3). We obtain this by noting that a realizable model class can be used to construct a realizable value function class \(\mathcal{F}\) and a complete value function class \(\mathcal{G}\) of sizes \(\log|\mathcal{F}|=\log|\mathcal{M}|\) and \(\log|\mathcal{G}|=\mathcal{O}(\log|\mathcal{M}|)\).
* Openness (?): A negative result does not follow from the existing constructions. The tree-based construction of Theorem 3.1 satisfies coverability with \(C_{\mathsf{cov}}=\exp(\Omega(H))\) and the bandit-based construction of Theorem E.1 satisfies coverability with \(C_{\mathsf{cov}}=|\mathcal{A}|\). In both cases, the lower bounds cannot be used to rule out statistical modularity with the above latent complexity. Similarly, it unclear how to obtain a positive result for the latent-dynamics class \(\langle\!\langle M_{\mathsf{1at}},\Phi\rangle\!\rangle\).

### Proofs for Lower Bounds (Theorems 3.1 and E.1)

#### e.3.1 Main lower bound (Theorem 3.1)

We will prove the following result.

**Theorem 3.1** (Impossibility of statistical modularity).: _For every \(N\geq 4\), there exists a decoder class \(\Phi\) with \(|\Phi|=N\) and a family of base MDPs \(\mathcal{M}_{\mathrm{1at}}\) satisfying (i) \(|\mathcal{M}_{\mathrm{1at}}|=1\), (ii) \(H\leq\mathcal{O}(\log(N))\), (iii) \(|\mathcal{S}|=|\mathcal{X}|\leq N^{2}\), (iv) \(|\mathcal{A}|=2\), and such that_

1. _For all_ \(\varepsilon,\delta>0\)_, we have_ \(\mathsf{comp}(\mathcal{M}_{\mathrm{1at}},\varepsilon,\delta)=0\)_._
2. _For an absolute constant_ \(c>0\)_,_ \(\mathsf{comp}(\llparent\mathcal{M}_{\mathrm{1at}},\Phi),c,c)\geq\Omega(N/\log( N))\)_._

**Proof.** Let \(N\) be given and assume without loss of generality that it is a power of \(2\). We first construct the class of latent-dynamics MDPs, following Song et al. [23].

Latent MDP.The construction has a single "known" latent MDP \(M_{\mathrm{1at}}\), so that the only uncertainty in the family of latent-dynamics MDPs we construct arises from the emission processes. We set \(\mathcal{M}_{\mathrm{1at}}=\{M_{\mathrm{1at}}\}\). Set \(H=\log_{2}(N)+1\) and \(\mathcal{A}=\{0,1\}\). We define the state space and latent transition dynamics as follows.

* The state space can be partitioned as \(\mathcal{S}=\mathcal{S}^{1},\ldots,\mathcal{S}^{N}\).
* Each block \(\mathcal{S}^{i}\) corresponds to a standard depth-\(H\) binary tree MDP with deterministic dynamics (e.g., Osband et al.; Domingues et al. [20]; DMKV21). There is a single "root" node at layer \(h=1\), which we denote by \(s^{i}_{\mathsf{root}}\), and \(N\) "leaf" nodes at layer \(H\), which we denote by \(\big{\{}s^{i,j}_{\mathsf{leaf}}\big{\}}_{j\in[N]}\). For each \(h=1,\ldots,H-1\), choosing action \(0\) leads to the left successor of the current state deterministically, and choosing action \(1\) leads to the right successor; this process continues until we reach a leaf node at layer \(H\).
* The initial state distribution is \(P_{\mathrm{1at},1}(\emptyset)=\mathsf{Unif}(s^{1}_{\mathsf{root}},\ldots,s^{ N}_{\mathsf{root}})\).
* There are no rewards for layers \(1,\ldots,H-1\). For layer \(H\), the reward is \[R_{H}(s^{i,j}_{\mathsf{leaf}},\cdot)=\mathbb{I}\{j=i\}.\] (17)

This construction can summarized as follows. At layer \(1\), we draw the index of one of \(N\) binary trees uniformly at random, and initialize into the root of the tree. From here, we receive a reward of \(1\) if we successfully navigate to the leaf node whose index agrees with the index of the tree itself, and receive a reward of \(0\) otherwise.

Note that the total number of latent states in this construction is \(|\mathcal{S}|=N\cdot|\mathcal{S}_{1}|=N(2N-1)\)

Observation space and decoder class.Let us introduce some additional notation. For each block \(\mathcal{S}^{i}\), let \(\mathcal{S}^{i}_{h}:=\{s^{i,j}_{h}\}_{j\in[2^{h-1}]}\) denote the states in block \(i\) that are reachable at layer \(h\), so that \(\mathcal{S}^{i}_{1}=\big{\{}s^{i}_{\mathsf{root}}\big{\}}\) and \(\mathcal{S}^{i}_{H}=\{s^{i,j}_{\mathsf{leaf}}\}_{j\in[N]}\). We define \(\mathcal{X}=\mathcal{S}\) so that \(|\mathcal{X}|\leq 4N^{2}\), and consider a class of emission processes corresponding to deterministic maps. Let \(\Sigma\) denote the set of cyclic permutations on \(N\) elements, excluding the identity permutation. That is, each \(\sigma_{i}\in\Sigma\) takes the form

\[\sigma_{i}:k\mapsto k+i\mod N\quad\text{ for }i\in\{1,\ldots,N\}.\]

For each \(\sigma\in\Sigma\), we consider the emission process

\[\psi^{\sigma}_{h}(\cdot\mid s^{\langle{i,j}\rangle}_{h})=\mathbb{I}_{s^{ \langle{\sigma(\sigma),j}\rangle}_{h}}.\]

That is, \(\psi^{\sigma}\) shifts the index of the binary tree containing \(s^{\langle{i,j}\rangle}_{h}\) according to \(\sigma\). Let \(\Psi=\{\psi^{\sigma}\mid\sigma\in\Sigma\}\). Consider the decoder class

\[\Phi=\Psi^{-1}\coloneqq\big{\{}s^{i}\mapsto s^{\psi^{-1}(i)}\mid\psi\in\Psi \big{\}},\]

which has \(|\Phi|=N\). We consider the class of rich-observation MDPs given by

\[\llparent\mathcal{M}_{\mathrm{1at}},\Phi\rrangle:=\big{\{}M^{i}:=\llparent M_{ \mathrm{1at}},\psi^{\sigma_{i}}\rrangle\mid\sigma_{i}\in\Sigma\big{\}}.\] (18)

It is clear that this class of rich-observation MDPs satisfies the decodability assumption for emissions \(\Psi\).

Sample complexity lower bound.To lower bound the sample complexity, we prove a lower bound on the constrained PAC Decision-Estimation Coefficient (DEC) of [13]. For an arbitrary MDP \(\overline{M}\) (defined over the space \(\mathcal{X}\)) and \(\varepsilon\in[0,2^{1/2}]\), define18

Footnote 18: For measures \(\mathbb{P}\) and \(\mathbb{Q}\), we define squared Hellinger distance by \(D^{2}_{\mathsf{H}}(\mathbb{P},\mathbb{Q})=\int(\sqrt{d\mathbb{P}}-\sqrt{d \mathbb{Q}})^{2}\).

\[\mathsf{dec}_{\varepsilon}(\mathcal{M},\overline{M})=\inf_{p,q\in\Delta( \Pi)}\sup_{M\in\mathcal{M}}\bigl{\{}\mathbb{E}_{\pi\sim p}[J^{ M}(\pi_{ M})-J^{ M}(\pi)]\mid\,\mathbb{E}_{\pi\sim q}\bigl{[}D^{2}_{\mathsf{H}}\bigl{(}M(\pi), \overline{M}(\pi)\bigr{)}\bigr{]}\leq\varepsilon^{2}\bigr{\}},\]

where \(M(\pi)\) denotes the law over trajectories \((x_{1},a_{1},r_{1}),\ldots,(x_{H},a_{H},r_{H})\) induced by executing the policy \(\pi\) in the MDP \(M\), \(J^{ M}(\pi)\) denotes the expected reward for policy \(\pi\) under \(M\), and \(\pi_{ M}\) denotes the optimal policy for \(M\). We further define

\[\mathsf{dec}_{\varepsilon}(\mathcal{M})=\sup_{\overline{M}}\mathsf{dec}_{ \varepsilon}(\mathcal{M},\overline{M}),\]

where the supremum ranges over all MDPs defined over \(\mathcal{X}\) and \(\mathcal{A}\). We now appeal to the following technical lemma.

**Lemma E.1**.: _For all \(\varepsilon^{2}\geq 4/N\), we have that \(\mathsf{dec}_{\varepsilon}(\langle\!\langle\mathcal{M}_{\mathrm{1st}},\Phi \rangle\!\rangle)\geq\frac{1}{2}\)._

In light of Lemma E.1, it follows from Theorem 2.1 in Foster et al. [13]19 that any PAC RL algorithm that uses \(T\) episodes of interaction for \(T\log(T)\leq c\cdot N\) must have \(\mathbb{E}[J^{ M}(\pi_{ M})-J^{ M}(\widehat{\pi})]\geq c^{\prime}\) for a worst-case MDP in \(\mathcal{M}\), where \(c,c^{\prime}>0\) are absolute constants. This implies that any PAC RL which has \(\mathbb{E}[J^{ M}(\pi_{ M})-J^{ M}(\widehat{\pi})]\leq c^{\prime}\) must have \(T\log(T)\geq c\cdot N\) and thus \(T\geq c\cdot N/\log(N)\).

Footnote 19: Theorem 2.1 in Foster et al. [13] is stated with respect to \(\sup_{\overline{M}\in\text{conv}(\mathcal{M})}\mathsf{dec}_{\varepsilon}( \mathcal{M},\overline{M})\), but the actual proof (Section 2.2) gives a stronger result that scales with \(\sup_{\overline{M}}\mathsf{dec}_{\varepsilon}(\mathcal{M},\overline{M})\).

Proof of Lemma e.1.Define \(\overline{M}_{\mathrm{1st}}\) as the latent-space MDP that has identical dynamics to \(M_{\mathrm{1st}}\) but, has zero reward in every state, and define \(\overline{M}:=\langle\!\langle\overline{M}_{\mathrm{1st}},\,\mathsf{id} \rangle\!\rangle\) as the rich-observation MDP obtained by composing \(\overline{M}_{\mathrm{1st}}\) with the "identity" emission process \(\mathsf{id}\) that sets \(x_{h}=s_{h}\). Observe that \(\overline{M}\) and \(M^{i}\), induce identical dynamics in observation space if rewards are ignored: For all policies \(\pi\),

\[\mathbb{P}^{\overline{M},\pi}[(x_{1},a_{1}),\ldots,(x_{H},a_{H})= \cdot]=\mathbb{P}^{M^{i},\pi}[(x_{1},a_{1}),\ldots,(x_{H},a_{H})=\cdot].\] (19)

It follows that for each \(i\), for all policies \(\pi\), we have

\[D^{2}_{\mathsf{H}}\bigl{(}M^{i}(\pi),\overline{M}(\pi)\bigr{)}\] \[=D^{2}_{\mathsf{H}}\bigl{(}(\langle\!\langle M_{\mathrm{1st}}, \psi_{i}\rangle\!\rangle)(\pi),(\langle\!\langle\overline{M}_{\mathrm{1st}}, \mathsf{id}\rangle\!\rangle)(\pi)\bigr{)}\] \[=\sum_{j=1}^{N}\mathbb{P}^{\overline{M},\pi}\bigl{[}x_{H}=s^{( \psi_{i}(j),j)}_{\mathsf{leaf}}\bigr{]}\cdot D^{2}_{\mathsf{H}}(\mathbb{I}_{1 },\mathbb{I}_{0})\] \[=2\sum_{j=1}^{N}\mathbb{P}^{\overline{M},\pi}\bigl{[}x_{H}=s^{( \psi_{i}(j),j)}_{\mathsf{leaf}}\bigr{]}\] \[=\frac{2}{N}\sum_{j=1}^{N}\mathbb{P}^{\overline{M},\pi}\bigl{[}x_ {H}=s^{(\psi_{i}(j),j)}_{\mathsf{leaf}}\mid x_{1}=s^{(\psi_{i}(j))}_{\mathsf{ root}}\bigr{]},\] (20) \[=\frac{2}{N}\sum_{j=1}^{N}\mathbb{P}^{\overline{M},\pi}\Bigl{[}x_ {H}=s^{(\psi_{i}^{-}\psi_{i}^{-}(j))}_{\mathsf{leaf}}\mid x_{1}=s^{(j)}_{ \mathsf{root}}\Bigr{]},\] (21)

since the learner receives identical feedback in the MDPs \(M^{i}\) and \(\overline{M}\) unless they reach the observation \(x_{H}=s^{(\psi_{i}(j),j)}_{\mathsf{leaf}}\) for some \(j\) (corresponding to latent state \(s^{(j,j)}_{\mathsf{leaf}}\) in \(M^{i}\)), in which case they receiver reward \(1\) in \(M^{i}\) but reward \(0\) in \(\overline{M}\). We now claim that for any \(q\in\Delta(\Pi)\), there exists a set of at least \(N/2\) indices \(\mathcal{I}_{q}\subset[N]\) such that

\[\mathbb{E}_{\pi\sim q}\bigl{[}D^{2}_{\mathsf{H}}\bigl{(}M^{i}(\pi), \overline{M}(\pi)\bigr{)}\bigr{]}\leq\frac{4}{N}\] (22)

[MISSING_PAGE_FAIL:36]

**Theorem E.1** (Alternative lower bound).: _For every \(N\geq 4\), there exists an emission class \(\Psi\) and a decoder class \(\Phi\) with \(|\Psi|=|\Phi|=N\) and a family of latent MDPs \(\mathcal{M}_{\mathrm{1at}}\) satisfying (i) \(|\mathcal{M}_{\mathrm{1at}}|=1\), (ii) \(H=1\), (iii) \(|\mathcal{S}|=|\mathcal{X}|=N\), (iv) \(|\mathcal{A}|=N\), and such that_

1. _For all_ \(\varepsilon,\delta>0\)_, we have_ \(\mathtt{comp}(\mathcal{M}_{\mathrm{1at}},\varepsilon,\delta)=0\)_._
2. _For an absolute constant_ \(c>0\)_,_ \(\mathtt{comp}((\!(\!(\mathcal{M}_{\mathrm{1at}},\Phi)\!),c,c)\geq\Omega(N/ \log(N))\)_._

**Proof of Theorem E.1.** We repeat more or less repeat the same proof as Theorem 3.1, but with the appropriate modifications to translate from the contextual tree-based construction in Theorem 3.1 to the contextual bandit-based construction in the theorem statement. Let \(N\) be given and assume without loss of generality that it is a power of \(2\).

Latent MDP.Our construction has a single "known" latent MDP \(M_{\mathrm{1at}}\); that is, the only uncertainty in the family of rich-observation MDPs we construct arises from the emission processes. Set \(\mathcal{M}_{\mathrm{1at}}=\{M_{\mathrm{1at}}\}\). Set \(H=1\) and \(\mathcal{A}=[N]\). We define the state space and latent transition dynamics as follows.

* The state space can be partitioned as \(\mathcal{S}=\mathcal{S}^{1},\ldots,\mathcal{S}^{N}\).
* Each block \(\mathcal{S}^{i}\) corresponds to a single state \(s^{i}\) with \(N\) actions denoted by \(a^{i}\), \(i\in[N]\).
* The initial state distribution is \(P_{\mathrm{1at},1}(\emptyset)=\mathtt{Unif}(s^{1},\ldots,s^{N})\).
* The reward function is \[R_{1}(s^{i},a^{j})=\mathbb{I}\{j=i\}.\] (23)

Informally, this construction can summarized as a contextual bandit (with uniform context distribution), with a reward of \(1\) if and only if we play the action corresponding to the index of the context drawn.

Note that the total number of latent states in this construction is \(|\mathcal{S}|=N\) and the number of actions is \(|\mathcal{A}|=N\).

Observation space and decoder class.We define \(\mathcal{X}=\mathcal{S}\) so that \(|\mathcal{X}|=|\mathcal{S}|\), and consider a class of emission processes corresponding to deterministic maps. Let \(\Sigma\) denote the set of cyclic permutations on \(N\) elements, excluding the identity permutation. That is, each \(\sigma_{i}\in\Sigma\) takes the form

\[\sigma_{i}:k\mapsto k+i\mod N,\quad\text{ for }i\in\{1,\ldots,N\}.\]

For each \(\sigma\in\Sigma\), we consider the emission process

\[\psi^{\sigma}(\cdot\mid s^{i})=\mathbb{I}_{s^{\sigma(i)}}(\cdot)\]

That is, \(\psi^{\sigma}\) shifts the context \(s^{i}\) according to \(\sigma\). Let \(\Psi=\{\psi^{\sigma}\mid\sigma\in\Sigma\}\). Consider the decoder class

\[\Phi=\Psi^{-1}\coloneqq\big{\{}s^{i}\mapsto s^{\psi^{-1}(i)}\mid\psi\in\Psi \big{\}},\]

which has \(|\Phi|=N\). We consider the class of rich-observation MDPs given by

\[\langle\!\langle\mathcal{M}_{\mathrm{1at}},\Phi\rangle\!\rangle\coloneqq\big{\{} M^{i}\coloneqq\langle\!M_{\mathrm{1at}},\psi^{\sigma_{i}}\rangle\!\mid \sigma_{i}\in\Sigma\big{\}}.\] (24)

It is clear that this class of rich-observation MDPs satisfies the decodability assumption for emissions \(\Psi\).

Sample complexity lower bound.To lower bound the sample complexity, we prove a lower bound on the constrained PAC Decision-Estimation Coefficient (DEC) of [13]. For an arbitrary MDP \(\overline{M}\) (defined over the space \(\mathcal{X}\)) and \(\varepsilon\in[0,2^{1/2}]\), define20

Footnote 20: For measures \(\mathbb{P}\) and \(\mathbb{Q}\), we define squared Hellinger distance by \(D^{2}_{\mathsf{H}}(\mathbb{P},\mathbb{Q})=\int(\sqrt{d\mathbb{P}}-\sqrt{d \mathbb{Q}})^{2}\).

\[\mathsf{dec}_{\varepsilon}(\mathcal{M},\overline{M})=\inf_{p,q\in\Delta( \Pi)}\sup_{M\in\mathcal{M}}\big{\{}\mathbb{E}_{\pi\sim p}[J^{M}(\pi_{{}_{M}})-J ^{M}(\pi)]\mid\mathbb{E}_{\pi\sim q}\big{[}D^{2}_{\mathsf{H}}\big{(}M(\pi), \overline{M}(\pi)\big{)}\big{]}\leq\varepsilon^{2}\big{\}},\]

where \(M(\pi)\) denotes the law over observations \((x_{1},a_{1},r_{1})\) induced by executing the policy \(\pi\) in the MDP \(M\), \(J^{M}(\pi)\) denotes the expected reward for policy \(\pi\) under \(M\), and \(\pi_{{}_{M}}\) denotes the optimal policy for \(M\). We further define

\[\mathsf{dec}_{\varepsilon}(\mathcal{M})=\sup_{\overline{M}}\mathsf{dec}_{ \varepsilon}(\mathcal{M},\overline{M}),\]

where the supremum ranges over all MDPs defined over \(\mathcal{X}\) and \(\mathcal{A}\). We now appeal to the following technical lemma.

**Lemma E.2**.: _For all \(\varepsilon^{2}\geq 4/N\), we have that \(\sup_{\overline{M}}\mathsf{dec}_{\varepsilon}(\mathcal{M},\overline{M})\geq \frac{1}{2}\)._

In light of Lemma E.2, it follows from Theorem 2.1 in Foster et al. [12]21 that any PAC RL algorithm that uses \(T\) episodes of interaction for \(T\log(T)\leq c\cdot N\) must have \(\mathbb{E}[J^{{ M}}(\pi_{{ M}})-J^{{ M}}(\widetilde{\pi})]\geq c^{\prime}\) for a worst-case MDP in \(\mathcal{M}\), where \(c,c^{\prime}>0\) are absolute constants. This implies that any PAC RL which has \(\mathbb{E}[J^{{ M}}(\pi_{{ M}})-J^{{ M}}( \widehat{\pi})]\leq c^{\prime}\) must have \(T\log(T)\geq c\cdot N\) and thus \(T\geq c\cdot N/\log(N)\). 

Footnote 21: Theorem 2.1 in Foster et al. [12] is stated with respect to \(\sup_{\overline{M}\in\text{conv}(\mathcal{M})}\mathsf{dec}_{\varepsilon}( \mathcal{M},\overline{M})\), but the actual proof (Section 2.2) gives a stronger result that scales with \(\sup_{\overline{M}}\mathsf{dec}_{\varepsilon}(\mathcal{M},\overline{M})\).

**Proof of Lemma E.2.** Define \(\overline{M}_{\mathrm{1at}}\) as the latent-space MDP that has identical dynamics to \(M_{\mathrm{1at}}\) but, has zero reward for every state-action pair, and define \(\overline{M}:=\big{\langle}\!\langle\overline{M}_{\mathrm{1at}},\mathrm{1d} \rangle\!\big{\rangle}\!\big{\rangle}\!\) as the rich-observation MDP obtained by composing \(\overline{M}_{\mathrm{1at}}\) with the identity emission process that sets \(x_{h}=s_{h}\). In the rest of the proof, we use the shorthand \(\psi_{i}\coloneqq\psi^{\sigma_{i}}\). Observe that \(\overline{M}\) and \(M^{i}\), induce identical dynamics in observation space if rewards are ignored, i.e. for all policies \(\pi:\mathcal{X}\to\Delta(\mathcal{A})\),

\[\mathbb{P}^{\mathbb{M},\pi}[(x_{1},a_{1})=\cdot]=\mathbb{P}^{{ M^{i}},\pi}[(x_{1},a_{1})=\cdot].\] (25)

It follows that for each \(i\), for all policies \(\pi\), we have

\[D_{\mathsf{H}}^{2}\big{(}M^{i}(\pi),\overline{M}(\pi)\big{)}\] \[=D_{\mathsf{H}}^{2}\big{(}((\!\langle M_{\mathrm{1at}},\psi_{i} \rangle\!)(\pi),(\!\big{\langle}\!\langle\overline{M}_{\mathrm{1at}},\mathrm{ 1d}\rangle\!\big{\rangle}\!)(\pi)\big{)}\] \[=\sum_{j=1}^{N}\mathbb{P}^{\mathbb{M},\pi}\Big{[}x_{1}=s^{\psi_{ i}(j)},a_{1}=a^{j}\Big{]}\cdot D_{\mathsf{H}}^{2}(\mathbb{I}_{1},\mathbb{I}_{0})\] \[=2\sum_{j=1}^{N}\mathbb{P}^{\mathbb{M},\pi}\Big{[}x_{1}=s^{\psi_{ i}(j)},a_{1}=a^{j}\Big{]}\] \[=\frac{2}{N}\sum_{j=1}^{N}\mathbb{P}^{\mathbb{M},\pi}\Big{[}a_{1} =a^{j}\mid x_{1}=s^{\psi_{i}(j)}\Big{]}\] \[=\frac{2}{N}\sum_{j=1}^{N}\mathbb{P}^{\mathbb{M},\pi}\Big{[}a_{1} =a^{\psi_{i}^{-1}(j)}\mid x_{1}=s^{j}\Big{]}\]

since the learner receives identical feedback in the MDPs \(M^{i}\) and \(\overline{M}\) unless they play the action \(a_{1}=a^{j}\) given observation \(x_{1}=s^{\psi_{i}(j)}\) (corresponding to latent state \(s^{i}\) in \(M^{i}\)), in which case they receiver reward \(1\) in \(M^{i}\) but reward \(0\) in \(\overline{M}\). We now claim that for any \(q\in\Delta(\Pi)\), there exists a set of at least \(N/2\) indices \(\mathcal{I}_{q}\subset[N]\) such that

\[\mathbb{E}_{\pi\sim q}\big{[}D_{\mathsf{H}}^{2}\big{(}M^{i}(\pi),\overline{M} (\pi)\big{)}\big{]}\leq\frac{4}{N}\] (26)

for all \(i\in\mathcal{I}_{q}\). To see this, note that by Eq. (21), we have

\[\mathbb{E}_{i\sim\mathsf{Unif}([N])}\,\mathbb{E}_{\pi\sim q}\big{[} D_{\mathsf{H}}^{2}\big{(}M^{i}(\pi),\overline{M}(\pi)\big{)}\big{]} \leq\,\mathbb{E}_{\pi\sim q}\Bigg{[}\frac{2}{N}\sum_{j=1}^{N} \frac{1}{N}\sum_{i=1}^{N}\mathbb{P}^{\mathbb{M},\pi}\Big{[}a_{1}=a^{\psi_{i}^ {-1}(j)}\mid x_{1}=j\Big{]}\Bigg{]}\] \[\leq\,\mathbb{E}_{\pi\sim q}\Bigg{[}\frac{2}{N}\sum_{j=1}^{N} \frac{1}{N}\Bigg{]}=\frac{2}{N}.\]

We conclude by Markov's inequality that \(\mathbb{P}_{i\sim\mathsf{Unif}([N])}\big{[}\mathbb{E}_{\pi\sim q}\big{[}D_{ \mathsf{H}}^{2}\big{(}M^{i}(\pi),\overline{M}(\pi)\big{)}\big{]}\geq 4/N\big{]} \leq 1/2\), giving \(\mathcal{I}_{q}\geq N/2\).

From Eq. (26), we conclude that for all \(\varepsilon^{2}\geq 4/N\),

\[\mathsf{dec}_{\varepsilon}(\!\langle\!\langle\mathcal{M}_{\mathrm{1at}},\Phi \rangle\!\rangle,\overline{M})\geq\inf_{q\in\Delta(\Pi)}\inf_{p\in\Delta(\Pi)} \sup_{i\in\mathcal{I}_{q}}\Bigl{\{}\mathbb{E}_{\pi\sim p}\Big{[}J^{{ M^{i}}}(\pi_{{ M^{i}}})-J^{{ M^{i}}}(\pi)\Big{]}\Bigr{\}}.\]To lower bound this quantity, observe that for any index \(i\) and any policy \(\pi\), we have

\[J^{ M^{i}}(\pi_{ M^{i}})-J^{ M^{i}}(\pi) =1-\frac{1}{N}\sum_{j=1}^{N}\mathbb{P}^{ M^{(i)},\pi}[a_{1}=a^{ {(j)}}\mid x_{1}=s^{{(\phi_{i}(j))}}]\] \[=1-\frac{1}{N}\sum_{j=1}^{N}\mathbb{P}^{\overline{ M},\pi}[a_{1}=a^{ {(j)}}\mid x_{1}=s^{{(\phi_{i}(j))}}]\] \[=1-\frac{1}{N}\sum_{j=1}^{N}\mathbb{P}^{\overline{ M},\pi}\Big{[}a_{1}=a^{ {(\phi_{i}^{-1}(j))}}\mid x_{1}=s^{{(j)}}\Big{]},\]

where the third inequality uses Eq. (25). We conclude that for any distribution \(p,q\in\Delta(\Pi)\),

\[\sup_{i\in\mathcal{I}_{q}}\Bigl{\{}\mathbb{E}_{\pi\sim p}\Bigl{[} J^{ M^{i}}(\pi_{ M^{i}})-J^{ M^{i}}(\pi)\Bigr{]}\Bigr{\}}\] \[\geq 1-\frac{1}{N}\sum_{j=1}^{N}\mathbb{E}_{i\sim\text{ln} \,\text{if}\,(\mathcal{I}_{q})}\,\mathbb{P}^{\overline{ M},\pi}\Bigl{[}a_{1}=a^{{(\psi_{i}^{-1}(j))}}\mid x_{1}=s^{{(j)}}\Bigr{]}\] \[=1-\frac{1}{N}\sum_{j=1}^{N}\frac{1}{|\mathcal{I}_{q}|}\sum_{i\in \mathcal{I}_{q}}\mathbb{P}^{\overline{ M},\pi}\Bigl{[}a_{1}=a^{{(\psi_{i}^{-1}(j))}}\mid x_{1}=s^{{(j)}}\Bigr{]}\geq 1- \frac{1}{|\mathcal{I}_{q}|}\geq\frac{1}{2}\]

as long as \(N\geq 4\). Since this lower bound holds uniformly for all \(q,p\in\Delta(\Pi)\), we conclude that

\[\text{dec}_{c}(\langle\!\langle\mathcal{M}_{\text{1at}},\Phi\rangle\!\rangle, \overline{M})\geq\frac{1}{2}.\]Proofs for Section 3.3: Positive Results

This section is dedicated to our upper bound establishing that pushforward-coverable MDPs are statistically modular (Theorem 3.2). We provide a technical overview in Appendix F.1, and provide a full proof in Appendix F.2.

### Technical Overview: Low-dimensional embeddings for pushforward-coverable MDPs.

The idea behind our positive result is to show that under the conditions of Theorem 3.2, it is possible to construct an (approximately) Bellman-complete value function class for the latent-dynamics MDP \(M^{\star}_{\text{obs}}\), at which point we can apply the Golf algorithm of Jin et al. [13]. We achieve this via two technical contributions. The first is the introduction of the _mismatch functions_\(\Gamma_{\phi}\), formally defined as follows.

**Definition F.1** (Mismatch functions).: _For a decodable emission process \(\psi^{\star}\) and decoder \(\phi\in\Phi\), the mismatch function for \(\phi\), \(\Gamma_{\phi}=\{\Gamma_{\phi,h}:\mathcal{S}\to\Delta(\mathcal{S})\}_{h=1}^{H}\), is defined, for every \(h\in[H]\), as the probability kernel_

\[\Gamma_{\phi,h}(s^{\prime}_{h}\mid s_{h})\coloneqq\mathbb{P}_{x_{h}\sim\psi^ {\star}_{h}(s_{h})}(\phi_{h}(x_{h})=s^{\prime}_{h}).\]

The mismatch functions allow us to express functions of the decoders as latent objects, and we revisit them in the context of self-predictive estimation (Appendix A). For the present result, we show (Lemma D.7) that the mismatch functions can capture the observation-level Bellman backups for any function of the decoders. That is, for any \(x_{h},a_{h}\), letting \(s_{h}=(\psi^{\star})^{-1}(x_{h})\) denote the true latent state, we have that for any \(f_{\text{1at}}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}\) and \(\phi\in\Phi\):

\[[\mathcal{T}^{M^{\star}_{\text{obs}}}_{h}(f_{\text{1at}}\circ\phi_{h+1})](x_{h },a_{h})=[\mathcal{T}^{M^{\star}_{\text{1at}}}_{h}(\Gamma_{\phi,h+1}\circ V_{f _{\text{1at}}})](s_{h},a_{h}).\] (27)

That is, the Bellman update of \(f_{\text{1at}}\circ\phi_{h+1}\) in the latent-dynamics MDP \(M^{\star}_{\text{obs}}\) can be expressed as a Bellman update in the base MDP \(M^{\star}_{\text{1at}}\) for a different (latent) function \(\Gamma_{\phi,h+1}\circ V_{f_{\text{1at}}}(s_{h+1})\coloneqq\sum_{s^{\prime}_{h +1}}\Gamma_{\phi,h+1}(s^{\prime}_{h+1}\mid s_{h+1})\max_{a^{\prime}}f_{\text{1at }}(s^{\prime}_{h+1},a^{\prime})\).

However, the mismatch functions \(\Gamma_{\phi}\) embed some knowledge of the emission process, and (with only decoder and base model realizability) are unknown to the learner. Our second technical contribution bypasses this by establishing a new structural property for pushforward-coverable MDPs (Lemma F.1): there exist low-dimensional linear embeddings of their transition kernels which can approximate Bellman backups for an arbitrary and _potentially unknown_ set of functions, as long as the set is not too large.

**Lemma F.1** (Pushforward-coverable MDPs admit low-dimensional embeddings).: _Let \(M\) be a known MDP with reward function \(r\), transition kernel \(P\), and pushforward coverability parameter \(C_{\text{push}}\). Let \(\mu=\{\mu_{h}\}_{h\in[H]}\) denote its pushforward coverability distribution (i.e. the minimizer of Definition 3.3) and \(\mathcal{F}\subseteq(\mathcal{S}\times[H]\to[0,1])\) be an arbitrary class of functions. Suppose that we sample \(W\in\{\pm 1\}^{d\times\mathcal{S}}\) as a matrix of independent Rademacher random variables, and define_

\[\psi_{h}(s,a)=r_{h}(s,a)\oplus\frac{1}{\sqrt{d}}W\Big{(}P_{h}(\cdot\mid s,a)/ \mu_{h}^{1/2}(\cdot)\Big{)}_{\cdot\in\mathcal{S}}\in\mathbb{R}^{d+1}.\]

_and_

\[w_{f,h}=1\oplus\frac{1}{\sqrt{d}}W\Big{(}\mu_{h}^{1/2}(\cdot)f_{h+1}(\cdot) \Big{)}_{\cdot\in\mathcal{S}}\in\mathbb{R}^{d+1}.\]

_Then for any \(\varepsilon_{\text{apx}}\in(0,1)\), as long as we set_

\[d\geq 2^{9}\frac{C_{\text{push}}\log\bigl{(}16|\mathcal{F}|H\delta^{-1}/ \varepsilon_{\text{apx}}\bigr{)}}{\varepsilon_{\text{apx}}},\]

_we have that for all \(f\in\mathcal{F}\) and \(h\in[H]\), with probability at least \(1-\delta\):_

\[\mathbb{E}_{\mu_{h}\otimes\text{unif}(\mathcal{A})}\Big{[}\bigl{(}\texttt{clip} _{[0,2]}[\langle w_{f,h},\psi_{h}(s,a)\rangle]-\mathcal{T}_{h}f_{h+1}(s,a) \bigr{)}^{2}\Big{]}\leq\varepsilon_{\text{apx}},\]

_as well as \(\max_{s,a,h}\lVert\psi_{h}(s,a)\rVert_{2}^{2}\leq C_{\text{push}}(16\log(| \mathcal{S}||\mathcal{A}|H)+11)\) and \(\max_{f,h}\lVert w_{f,h}\rVert_{2}^{2}\leq 16\log(|\mathcal{F}|H)+11\). We emphasize that the feature map \(\psi=\{\psi_{h}\}_{h=1}^{H}\) is oblivious to \(\mathcal{F}\), in the sense that it can be computed directly from \(M\) without any knowledge of \(\mathcal{F}\)._

We use this property, in conjunction with latent model realizability, to construct linear features that can approximate the right-hand-side of Eq. (27), thus yielding an (approximately) Bellman-complete value function class for the latent-dynamics MDP \(M^{\star}_{\text{obs}}\).

### Proofs for Latent Model Class + Pushforward Coverability (Theorem 3.2)

In this section, we establish positive results under latent MDP classes which satisfy pushforward coverability. We assume that every model in \(\mathcal{M}_{\mathtt{1at}}\) satisfies _pushforward coverability_, defined as follows:

**Definition F.2** (Pushforward coverability).: _The pushforward coverability coefficient \(C_{\textnormal{push}}\) for an MDP \(M\) with transition kernel \(P\) is defined by_

\[C_{\textnormal{push}}(M)=\max_{h\in[H]}\inf_{\mu\in\Delta( \mathcal{S})}\sup_{(s,a,s^{\prime})\in\mathcal{S}\times\mathcal{A}\times \mathcal{S}}\frac{P_{h-1}(s^{\prime}\mid s,a)}{\mu(s^{\prime})}.\] (28)

_The pushforward coverability coefficient for an MDP class \(\mathcal{M}\) is defined by_

\[C_{\textnormal{push}}(\mathcal{M})=\max_{M\in\mathcal{M}}C_{ \textnormal{push}}(M).\]

Note that for any MDP \(M\) we always have

\[C_{\textnormal{cov}}(M,\Pi_{\textnormal{rns}})\leq C_{ \textnormal{push}}(M)|\mathcal{A}|,\] (29)

where \(C_{\textnormal{cov}}\) is the state-action coverability coefficient (Definition D.3). Thus, an MDP with low pushforward coverability is also an MDP with low state-action coverability for all policies (upto a dependence on \(|\mathcal{A}|\)).

We will show the show the following result.

**Theorem 3.2** (Pushforward-coverable MDPs are statistically modular).: _Let \(\mathcal{M}_{\mathtt{1at}}\) be a base MDP class such that each \(M_{\mathtt{1at}}\in\mathcal{M}_{\mathtt{1at}}\) has pushforward coverability bounded by \(C_{\textnormal{push}}(M_{\mathtt{1at}})\leq C_{\textnormal{push}}\). Then, for any decoder class \(\Phi\), we have:_

1. \(\textnormal{\sf comp}(\mathcal{M}_{\mathtt{1at}},\varepsilon,\delta)\leq \textnormal{\sf poly}(C_{\textnormal{push}},|\mathcal{A}|,H,\log|\mathcal{M}_{ \mathtt{1at}}|,\varepsilon^{-1},\log(\delta^{-1}))\)_, and_
2. \(\textnormal{\sf comp}(\llparent\mathcal{M}_{\mathtt{1at}},\Phi\rrangle, \varepsilon,\delta)\leq\textnormal{\sf poly}(C_{\textnormal{push}},| \mathcal{A}|,H,\log|\mathcal{M}_{\mathtt{1at}}|,\log|\Phi|,\varepsilon^{-1}, \log(\delta^{-1}),\log\log|\mathcal{S}|)\)_._

The proof comes in three parts. We will firstly show that MDP that satisfies pushforward coverability admit low-dimensional feature maps that can approximate Bellman backups (Appendix F.2.1), then establish that a regret bound for the Golf algorithm [13] under misspecification (Appendix F.2.2), and then combine these ingredients (Appendix F.2.3).

#### f.2.1 A structural result: Pushforward-coverable MDPs are approximately low-rank

Our central technical result for this section is Lemma F.1, which is based on a variant of the Johnson-Lindenstrauss lemma and establishes that under pushforward coverability, we can define a linear feature class which satisfies an approximate form of Bellman completeness. We define the clipping operator via

\[\textnormal{\sf clip}_{[0,2]}(x)\coloneqq\max\{\min\{x,2\},0\}.\]

We prove the following lemma.

**Lemma F.1** (Pushforward-coverable MDPs admit low-dimensional embeddings).: _Let \(M\) be a known MDP with reward function \(r\), transition kernel \(P\), and pushforward coverability parameter \(C_{\textnormal{push}}\). Let \(\mu=\{\mu_{h}\}_{h\in[H]}\) denote its pushforward coverability distribution (i.e. the minimizer of Definition 3.3) and \(\mathcal{F}\subseteq\big{(}\mathcal{S}\times[H]\to[0,1]\big{)}\) be an arbitrary class of functions. Suppose that we sample \(W\in\{\pm 1\}^{d\times\mathcal{S}}\) as a matrix of independent Rademacher random variables, and define_

\[\psi_{h}(s,a)=r_{h}(s,a)\oplus\frac{1}{\sqrt{d}}W\Big{(}P_{h}( \cdot\mid s,a)/\mu_{h}^{1/2}(\cdot)\Big{)}_{\cdot\in\mathcal{S}}\in\mathbb{R} ^{d+1}.\]

_and_

\[w_{f,h}=1\oplus\frac{1}{\sqrt{d}}W\Big{(}\mu_{h}^{1/2}(\cdot)f_{ h+1}(\cdot)\Big{)}_{\cdot\in\mathcal{S}}\in\mathbb{R}^{d+1}.\]

_Then for any \(\varepsilon_{\textnormal{apx}}\in(0,1)\), as long as we set_

\[d\geq 2^{9}\frac{C_{\textnormal{push}}\log\bigl{(}16| \mathcal{F}|H\delta^{-1}/\varepsilon_{\textnormal{apx}}\bigr{)}}{\varepsilon_{ \textnormal{apx}}},\]_we have that for all \(f\in\mathcal{F}\) and \(h\in[H]\), with probability at least \(1-\delta\):_

\[\mathbb{E}_{\mu_{h}\otimes\text{\rm{Unif}}(\mathcal{A})}\bigg{[}\big{(}\mathsf{ Clip}_{[0,2]}[\langle w_{f,h},\psi_{h}(s,a)\rangle]-\mathcal{T}_{h}f_{h+1}(s,a) \big{)}^{2}\bigg{]}\leq\varepsilon_{\mathsf{apx}},\]

_as well as \(\max_{s,a,h}\lVert\psi_{h}(s,a)\rVert_{2}^{2}\leq C_{\mathsf{push}}(16\log( |\mathcal{S}||\mathcal{A}|H)+11)\) and \(\max_{f,h}\lVert w_{f,h}\rVert_{2}^{2}\leq 16\log(|\mathcal{F}|H)+11\). We emphasize that the feature map \(\psi=\left\{\psi_{h}\right\}_{h=1}^{H}\) is oblivious to \(\mathcal{F}\), in the sense that it can be computed directly from \(M\) without any knowledge of \(\mathcal{F}\)._

**Proof of Lemma F.1.** Fix \(h\in[H]\), whose dependence we omit for cleanliness. We begin by verifying that, in expectation, \(\langle w_{f},\psi(s,a)\rangle\) is equal to \(\mathcal{T}f(s,a)\). For this, note that

\[\langle w_{f},\psi(s,a)\rangle\] \[\quad=r(s,a)+\frac{1}{d}\sum_{i=1}^{d}\Biggl{(}\sum_{s^{\prime} \in\mathcal{S}}W_{i,s^{\prime}}\frac{P(s^{\prime}\mid s,a)}{\mu^{1/2}(s^{ \prime})}\Biggr{)}\Biggl{(}\sum_{s^{\prime\prime}\in\mathcal{S}}W_{i,s^{\prime \prime}}\mu^{1/2}(s^{\prime\prime})f(s^{\prime\prime})\Biggr{)}\] \[\quad=r(s,a)+\sum_{s^{\prime}\in\mathcal{S}}P(s^{\prime}\mid s,a )f(s^{\prime})+\frac{1}{d}\sum_{i=1}^{d}\sum_{s^{\prime}\in\mathcal{S}}\sum_{ \begin{subarray}{c}s^{\prime\prime}\in\mathcal{S}\\ s^{\prime\prime}\neq s^{\prime}\end{subarray}}W_{i,s^{\prime}}\frac{P(s^{ \prime}\mid s,a)}{\mu^{1/2}(s^{\prime})}W_{i,s^{\prime\prime}}\mu^{1/2}(s^{ \prime\prime})f(s^{\prime\prime}).\]

Consequently, we have

\[|\mathcal{T}f(s,a)-\langle w_{f},\psi(s,a)\rangle|=\Biggl{|}\frac{1}{d}\sum_{ i=1}^{d}\sum_{s^{\prime}\in\mathcal{S}}\sum_{\begin{subarray}{c}s^{\prime \prime}\in\mathcal{S}\\ s^{\prime\prime}\neq s^{\prime}\end{subarray}}W_{i,s^{\prime\prime}}\frac{P(s^{ \prime}\mid s,a)}{\mu^{1/2}(s^{\prime})}W_{i,s^{\prime\prime}}\mu^{1/2}(s^{ \prime\prime})f(s^{\prime\prime})\Biggr{|}.\] (30)

Note that this remaining noise term is zero-mean - we will show in the sequel that it can be made small by picking \(d\) appropriately. We next examine the norms of the vectors \(\psi(s,a)\) and \(w_{f}\). Note that we have

\[\lVert\psi(s,a)\rVert_{2}^{2} =\frac{1}{d}\sum_{i=1}^{d}\Biggl{(}\sum_{s^{\prime}\in\mathcal{S} }W_{i,s^{\prime\prime}}\frac{P(s^{\prime}\mid s,a)}{\mu^{1/2}(s^{\prime})} \Biggr{)}^{2}\] \[=\sum_{s^{\prime}\in\mathcal{S}}\frac{P^{2}(s^{\prime}\mid s,a)}{ \mu(s^{\prime})}+\frac{1}{d}\sum_{i=1}^{d}\sum_{s^{\prime}\in\mathcal{S}} \sum_{\begin{subarray}{c}s^{\prime\prime}\in\mathcal{S}\\ s^{\prime\prime}\neq s^{\prime}\end{subarray}}W_{i,s^{\prime}}W_{i,s^{\prime \prime}}\frac{P(s^{\prime}\mid s,a)}{\mu^{1/2}(s^{\prime})}\frac{P(s^{\prime \prime}\mid s,a)}{\mu^{1/2}(s^{\prime\prime})}\] \[\leq C_{\mathsf{push}}+\frac{1}{d}\sum_{i=1}^{d}\sum_{s^{\prime} \in\mathcal{S}}\sum_{\begin{subarray}{c}s^{\prime\prime}\in\mathcal{S}\\ s^{\prime\prime}\neq s^{\prime}\end{subarray}}W_{i,s^{\prime\prime}}W_{i,s^{ \prime\prime}}\frac{P(s^{\prime}\mid s,a)}{\mu^{1/2}(s^{\prime})}\frac{P(s^{ \prime\prime}\mid s,a)}{\mu^{1/2}(s^{\prime\prime})},\] (31)

where we have used that

\[\sum_{s^{\prime}\in\mathcal{S}}\frac{P^{2}(s^{\prime}\mid s,a)}{\mu(s^{\prime}) }\leq C_{\mathsf{push}}\sum_{s^{\prime}\in\mathcal{S}}P(s^{\prime}\mid s,a)= C_{\mathsf{push}}\]

by definition of pushforward coverability. Further note that we have

\[\lVert w_{f}\rVert_{2}^{2} =\frac{1}{d}\sum_{i=1}^{d}\Biggl{(}\sum_{s^{\prime}\in\mathcal{S}} W_{i,s^{\prime}}\mu^{1/2}(s^{\prime})f(s^{\prime})\Biggr{)}^{2}\] \[=\mathbb{E}_{s^{\prime}\sim_{\mu}}[f(s^{\prime})]+\frac{1}{d}\sum_ {i=1}^{d}\sum_{s^{\prime}\in\mathcal{S}}\sum_{\begin{subarray}{c}s^{\prime \prime}\in\mathcal{S}\\ s^{\prime\prime}\neq s^{\prime}\end{subarray}}W_{i,s^{\prime\prime}}W_{i,s^{ \prime\prime}}\mu^{1/2}(s^{\prime})f(s^{\prime})\cdot\mu^{1/2}(s^{\prime \prime})f(s^{\prime\prime})\] \[\leq 1+\frac{1}{d}\sum_{i=1}^{d}\sum_{s^{\prime}\in\mathcal{S}} \sum_{\begin{subarray}{c}s^{\prime\prime}\in\mathcal{S}\\ s^{\prime\prime}\neq s^{\prime}\end{subarray}}W_{i,s^{\prime\prime}}\mu^{1/2}(s^{ \prime})f(s^{\prime})\cdot\mu^{1/2}(s^{\prime\prime})f(s^{\prime\prime}).\] (32)

We will now appeal to the following technical lemma to upper bound Eq. (30), Eq. (31), and Eq. (32) by establishing that the Rademacher noise terms concentrate to their expectations. The proof of the lemma will be given in the sequel.

**Lemma F.2**.: _Let \(u,v\in\mathbb{R}^{n}\), and let \(W\in\left\{\pm 1\right\}^{d\times n}\) have independent Rademacher entries. Then with probability at least \(1-\delta\),_

\[\left|\frac{1}{d}\sum_{i\in[d]}\sum_{j\in[n]}\sum_{\begin{subarray}{c}k\in[n]\\ k\neq j\end{subarray}}W_{i,j}W_{i,k}u_{j}v_{k}\right|\leq\|u\|_{2}\|v\|_{2}\cdot \sqrt{\frac{32\log(2\delta^{-1})}{d}}+\|u\|_{2}^{2}\|v\|_{2}^{2}\cdot\frac{64 \log\bigl{(}2\delta^{-1}\bigr{)}}{d}.\] (33)

_Furthermore, for any set of vectors \(\mathcal{V}\subset\mathbb{R}^{n}\), we also have_

\[\frac{1}{d}\max_{v\in\mathcal{V}}\sum_{i\in[d]}\sum_{j\in[n]}\sum_{ \begin{subarray}{c}k\in[n]\\ k\neq j\end{subarray}}W_{i,j}W_{i,k}v_{j}v_{k}\] \[\leq\max_{v\in\mathcal{V}}\lVert v\rVert_{2}^{2}(16\log|\mathcal{ V}|+9)+\max_{v\in\mathcal{V}}\lVert v\rVert_{2}^{2}\cdot\sqrt{\frac{32\log(2 \delta^{-1})}{d}}+\max_{v\in\mathcal{V}}\lVert v\rVert_{2}^{4}\cdot\frac{64 \log\bigl{(}2\delta^{-1}\bigr{)}}{d}.\]

Let \((s,a)\in\mathcal{S}\times\mathcal{A}\) and \(f\in\mathcal{F}\). To bound \(\left|\langle\psi(s,a),w_{f}\rangle-\mathcal{T}f(s,a)\right|\) (cf. Eq. (30)), we apply the first bound of Lemma F.2 with \(u=\bigl{(}P(s^{\prime}\mid s,a)/\mu^{1/2}(s^{\prime})\bigr{)}_{s^{\prime}\in \mathcal{S}}\) and \(v=\bigl{(}\mu^{1/2}(s^{\prime})f(s^{\prime})\bigr{)}_{s^{\prime}\in\mathcal{S}}\), which gives

\[\left|\langle\psi(s,a),w_{f}\rangle-\mathcal{T}f(s,a)\right|\leq\sqrt{\frac{3 2C_{\text{push}}\log(2\delta^{-1})}{d}}+64C_{\text{push}}\frac{\log\bigl{(}2 \delta^{-1}\bigr{)}}{d}\coloneqq\varepsilon(\delta^{-1}),\] (34)

where we have again used that \(\lVert u\rVert_{2}^{2}=\sum_{s^{\prime}\in\mathcal{S}}\frac{P^{2}(s^{\prime} \mid s,a)}{\mu(s^{\prime})}\leq C_{\text{push}}\) and also that \(\lVert v\rVert_{2}^{2}=1\) since \(\lVert f\rVert_{\infty}\leq 1\) for all \(f\in\mathcal{F}\). To bound Eq. (31), we apply the second bound of Lemma F.2 with \(\mathcal{V}=\left\{\left(\frac{P_{h-1}(s^{\prime}\mid s,a)}{\mu_{h}^{1/2}(s^{ \prime})}\right)_{s^{\prime}\in\mathcal{S}}\right\}_{\begin{subarray}{c}s,a \in\mathcal{S}\times\mathcal{A}\\ h\times[H]\end{subarray}}\), which gives

\[\max_{s,a\in\mathcal{S}\times\mathcal{A},h\in[H]}\lVert\psi_{h}(s,a)\rVert_{2 }^{2}\leq C_{\text{push}}(16\log|\mathcal{S}||\mathcal{A}|H+9)+C_{\text{push}} \sqrt{\frac{32\log(2\delta^{-1})}{d}}+C_{\text{push}}^{2}\frac{64\log\bigl{(} 2\delta^{-1}\bigr{)}}{d}.\]

Lastly, to bound Eq. (32), we take \(\mathcal{V}=\left\{\left(\mu_{h}^{1/2}(s^{\prime})f_{h}(s^{\prime})\right)_{s^ {\prime}\in\mathcal{S}}\right\}_{\begin{subarray}{c}f\in\mathcal{F}\\ h\in[H]\end{subarray}}\) in Lemma F.2, which establishes that

\[\max_{f\in\mathcal{F},h\in[H]}\lVert w_{f,h}\rVert_{2}^{2}\leq 9+16\log| \mathcal{F}|H+\sqrt{\frac{32\log(2\delta^{-1})}{d}}+\frac{64\log\bigl{(}2 \delta^{-1}\bigr{)}}{d}.\]

Note that Eq. (34) establishes that the Bellman backup \(\mathcal{T}f(s,a)\) is well-approximated by \(\langle\psi(s,a),w_{f}\rangle\) only at a single state-action pair \((s,a)\). We can obtain an \(L_{\infty}\)-approximation guarantee by taking a union bound over \(\mathcal{S}\) and \(\mathcal{A}\), which would incur a dependence on \(\log|\mathcal{S}|\) in the final sample complexity. Here, we bypass this by instead requiring only an approximation guarantee under the \(L_{2}(\mu\otimes\mathsf{Unif}(\mathcal{A}))\) norm. Via (pushforward) coverability, this will ensure that \(\mathbb{E}^{\pi}\Bigl{[}(\langle w_{f},\psi(s,a)\rangle-\mathcal{T}f(s,a))^{2} \Bigr{]}\) is well-controlled for all policies \(\pi\), which will be sufficient for our downstream sample-complexity analysis of Golf. However, directly establishing an \(L_{2}(\mu\otimes\mathsf{Unif}(\mathcal{A}))\) approximation guarantee is technically challenging since it would require establishing a fourth-order (rather than second-order) equivalent of Eq. (33). The remainder of the proof will obtain an \(L_{2}(\mu\otimes\mathsf{Unif}(\mathcal{A}))\) approximation guarantee by instead sampling a dataset of size \(n\) from \(\mu\otimes\mathsf{Unif}(\mathcal{A})\) and taking a union bound over that dataset to ensure a uniform bound on all state-action pairs in that dataset. Via an additional concentration bound, this will ensure that the error is well-behaved under the \(L_{2}(\mu\otimes\mathsf{Unif}(\mathcal{A}))\) norm.

For each \(h\in[H]\), sample a dataset \(D=\{(s_{h}^{(i)},a_{h}^{(i)})\}_{i=1}^{n}\) i.i.d. from \(\mu_{h}\otimes\mathsf{Unif}(\mathcal{A})\). By a union bound over \(n\), \(\mathcal{F}\), and \(H\), we have that

\[\forall i\in[n],f\in\mathcal{F},h\in[H]:\quad\left|\bigl{\langle}\psi_{h}(s_{h }^{(i)},a_{h}^{(i)}),w_{f,h}\bigr{\rangle}-\mathcal{T}_{h}f_{h+1}(s_{h}^{(i)},a _{h}^{(i)})\bigr{|}\leq\varepsilon(n|\mathcal{F}|H\delta^{-1}),\] (35)

where we recall the definition of \(\varepsilon(\cdot)\) from Eq. (34). Now, let

\[X_{f,h}(s,a)\coloneqq\bigl{(}\mathtt{clip}_{[0,2]}[\langle\psi_{h}(s,a),w_{f,h} \rangle]-\mathcal{T}_{h}f_{h+1}(s,a)\bigr{)}^{2}.\]Note that \(|X_{f,h}(s,a)|\leq 4\) and

\[X_{f,h}(s,a)\leq(\langle\psi_{h}(s,a),w_{f,h}\rangle-\mathcal{T}_{h}f_{h+1}(s,a) )^{2},\]

since \(\mathcal{T}_{h}f_{h+1}(s,a)\in[0,2]\) and the clipping operator is 1-Lipshitz. Note that

\[\mathbb{E}_{(s,a)\sim\mu_{h}\otimes\mathfrak{Unif}(\mathcal{A})}[X_{f,h}(s,a)] \coloneqq\mathbb{E}_{\mu_{h}\otimes\mathfrak{Unif}(\mathcal{A})}\Big{[}\big{(} \mathtt{clip}_{[0,2]}[\langle\psi_{h}(s,a),w_{f}\rangle]-\mathcal{T}_{h}f_{h+1 }(s,a)\big{)}^{2}\Big{]},\]

where this expectation is only over the sampling of the data point \((s,a)\) (and not the Rademacher matrix \(W\)). Let

\[X_{i,f,h}\coloneqq X_{f,h}(s_{h}^{(i)},a_{h}^{(i)}).\]

By boundedness of \(X_{f,h}(s,a)\) and Hoeffding's inequality, we have that with probability at least \(1-\delta\):

\[\left|\frac{1}{n}\sum_{i=1}^{n}X_{i,f,h}-\mathbb{E}_{\mu\otimes\mathfrak{Unif }(\mathcal{A})}[X_{f,h}(s,a)]\right|\leq 4\sqrt{\frac{\log(2\delta^{-1})}{n}}.\]

Taking another union bound over \(\mathcal{F}\) and \(H\) as well as the event in Eq. (35) gives that

\[\forall f\in\mathcal{F},h\in[H]:\ \ \ \left|\frac{1}{n}\sum_{i=1}^{n}X_{i,f,h}- \mathbb{E}_{\mu\otimes\mathfrak{Unif}(\mathcal{A})}[X_{f,h}(s,a)]\right|\leq 4 \sqrt{\frac{\log(2|\mathcal{F}|H\delta^{-1})}{n}},\] (36)

and \(\forall i\in[n],f\in\mathcal{F},h\in[H]:\ \ \ X_{i,f,h}\leq\varepsilon^{2}(n| \mathcal{F}|H\delta^{-1}),\) (37)

recalling the definition of \(\varepsilon(\cdot)\) from Eq. (34). Then, re-arranging Eq. (36) gives us that

\[\mathbb{E}_{\mu\otimes\mathfrak{Unif}(\mathcal{A})}\Big{[}\big{(} \mathtt{clip}_{[0,2]}[\langle\psi_{h}(s_{h},a_{h}),w_{f}\rangle]-\mathcal{T}_ {h}f_{h+1}(s_{h},a_{h})\big{)}^{2}\Big{]}\] \[\qquad\leq\frac{1}{n}\sum_{i=1}^{n}X_{i,f,h}+4\sqrt{\frac{\log(2 |\mathcal{F}|H\delta^{-1})}{n}}\] \[\qquad\leq\varepsilon^{2}(n|\mathcal{F}|H\delta^{-1})+4\sqrt{ \frac{\log(2|\mathcal{F}|H\delta^{-1})}{n}},\] (38)

We now conclude the proof by picking \(n\) and \(d\) appropriately to ensure that the right-hand-side is bounded by \(\varepsilon_{\text{apx}}\), which will ensure the desired claim that

\[\mathbb{E}_{\mu\otimes\mathfrak{Unif}(\mathcal{A})}\Big{[}\big{(}\mathtt{clip }_{[0,2]}[\langle\psi_{h}(s_{h},a_{h}),w_{f}\rangle]-\mathcal{T}_{h}f_{h+1}(s_ {h},a_{h})\big{)}^{2}\Big{]}\leq\varepsilon_{\text{apx}}.\]

For convenience, we introduce absolute constants \(c\) and \(c^{\prime}\) whose precise values may change from line to line. We pick \(n=64\log\bigl{(}2|\mathcal{F}|H\delta^{-1}\bigr{)}/\varepsilon_{\text{apx}}^{2}\). Plugging this into (38) gives

\[\mathbb{E}_{\mu\otimes\mathfrak{Unif}(\mathcal{A})}\Big{[}\big{(}\mathtt{clip }_{[0,2]}[\langle\psi_{h}(s_{h},a_{h}),w_{f}\rangle]-\mathcal{T}_{h}f_{h+1}(s _{h},a_{h})\big{)}^{2}\Big{]}\leq\varepsilon^{2}(n|\mathcal{F}|H\delta^{-1})+ c\cdot\varepsilon\] (39)

Noting that \(n\leq 128\frac{|\mathcal{F}|H\delta^{-1}}{\varepsilon_{\text{apx}}^{2}}\) and plugging this into \(\varepsilon\) (Eq. (34)) gives

\[\varepsilon(n|\mathcal{F}|H\delta^{-1})\leq C_{\mathfrak{push}}^{1/2}\sqrt{ \frac{64\log(16|\mathcal{F}|H\delta^{-1}/\varepsilon_{\text{apx}})}{d}}+C_{ \mathfrak{push}}\frac{128\log\bigl{(}16|\mathcal{F}|H\delta^{-1}/\varepsilon_ {\text{apx}}\bigr{)}}{d}.\] (40)

Setting

\[d\geq 2^{9}\frac{C_{\mathfrak{push}}\log\bigl{(}16|\mathcal{F}|H\delta^{-1}/ \varepsilon_{\text{apx}}\bigr{)}}{\varepsilon_{\text{apx}}}\]

ensures that

\[\varepsilon^{2}(n|\mathcal{F}|H\delta^{-1})\leq\varepsilon(n|\mathcal{F}|H \delta^{-1})\leq\frac{\varepsilon_{\text{apx}}}{2}\] (41)

by Eq. (40). Combining Eq. (38) and Eq. (41), we get

\[\mathbb{E}_{\mu\otimes\mathfrak{Unif}(\mathcal{A})}\Big{[}\big{(}\mathtt{clip }_{[0,2]}[\langle\psi_{h}(s_{h},a_{h}),w_{f}\rangle]-\mathcal{T}_{h}f_{h+1}(s _{h},a_{h})\big{)}^{2}\Big{]}\leq\varepsilon_{\text{apx}},\] (42)as desired. It only remains to establish the concentration results of Lemma F.2. 

Proof of Lemma F.2.: We establish the first claim. Let \(i\in[d]\) be fixed, and consider the random variable

\[Z_{i}:=\sum_{j\in[n]}\sum_{\begin{subarray}{c}k\in[n]\\ k\neq j\end{subarray}}W_{i,j}W_{i,k}v_{j}u_{k}.\]

Note that \(\mathbb{E}[Z_{i}]=0\) by independence of \(W_{i,j}\) and \(W_{i,k}\) for every \(j\neq k\). By Exercise 6.9 of Boucheron et al. [1], we have that

\[\log\mathbb{E}[\exp(\lambda Z_{i})]\leq\frac{16\lambda^{2}}{2(1-64\|u\|_{2}^{2 }\|v\|_{2}^{2}\lambda)}\|u\|_{2}^{2}\|v\|_{2}^{2}.\]

Since \(Z_{i}\) are independent, it follows that

\[\log\mathbb{E}\Bigg{[}\exp\!\left(\lambda\sum_{i=1}^{d}Z_{i}\right)\Bigg{]} \leq\frac{16\lambda^{2}}{2(1-64\|u\|_{2}^{2}\|v\|_{2}^{2}\lambda)}\|u\|_{2}^{ 2}\|v\|_{2}^{2}d.\]

Hence, \(\sum_{i=1}^{d}Z_{i}\) is a sub-Gamma random variable with parameters \(\nu=16\|u\|_{2}^{2}\|v\|_{2}^{2}d\) and \(c=64\|u\|_{2}^{2}\|v\|_{2}^{2}\), and it follows from Equation (2.5) on page 29 of Boucheron et al. [1] that for all \(\varepsilon>0\),

\[\mathbb{P}\!\left(\sum_{i=1}^{d}Z_{i}\geq\|u\|_{2}\|v\|_{2}\sqrt{32d \varepsilon}+64\|u\|_{2}^{2}\|v\|_{2}^{2}\varepsilon\right)\leq e^{- \varepsilon}.\]

Taking a union bound, and using that the random variable is symmetric, we obtain the desired claim.

We now establish the second claim. Let \(\mathcal{V}\subset\mathbb{R}^{n}\) be a subset of vectors. Let \(i\in[d]\) be fixed, and re-consider the random variable

\[Z_{i}:=\max_{v\in\mathcal{V}}\sum_{j\in[n]}\sum_{\begin{subarray}{c}k\in[n]\\ k\neq j\end{subarray}}W_{i,j}W_{i,k}v_{j}v_{k}.\]

Again appealing to Exercise 6.9 of Boucheron et al. [1], we have that

\[\log\mathbb{E}[\exp(\lambda(Z_{i}-\mathbb{E}[Z_{i}]))] \leq\frac{16\lambda^{2}}{2(1-64B\lambda)}\,\mathbb{E}\Bigg{[} \max_{v\in\mathcal{V}}\sum_{j\in[n]}\sum_{\begin{subarray}{c}k\in[n]\\ k\neq j\end{subarray}}W_{i,j}W_{i,k}v_{j}^{2}v_{k}^{2}\Bigg{]}\] \[\leq\frac{16\lambda^{2}}{2(1-64B\lambda)}\,\mathbb{E}\Bigg{[} \max_{v\in\mathcal{V}}\sum_{j,k=1}^{n}v_{j}^{2}v_{k}^{2}\Bigg{]}\] \[=\frac{16\lambda^{2}}{2(1-64B\lambda)}\max_{v\in\mathcal{V}}\!\|v \|_{2}^{4}\]

where \(B:=\max_{v\in\mathcal{V}}\!\|v\|_{2}^{4}\). Since \(Z_{i}\) are independent, it follows that

\[\log\mathbb{E}\Bigg{[}\exp\!\left(\lambda\sum_{i=1}^{d}(Z_{i}-\mathbb{E}[Z_{i} ])\right)\Bigg{]}\leq\frac{16\lambda^{2}}{2(1-64B\lambda)}\max_{v\in\mathcal{ V}}\!\|v\|_{2}^{4}d.\]

Hence, \(\sum_{i=1}^{d}Z_{i}\) is a sub-Gamma random variable with parameters \(\nu=16\max_{v\in\mathcal{V}}\!\|v\|_{2}^{4}d\) and \(c=64\max_{v\in\mathcal{V}}\!\|v\|_{2}^{4}\), and it follows from Equation (2.5) on page 29 of Boucheron et al. [1] that for all \(\varepsilon>0\),

\[\mathbb{P}\!\left(\frac{1}{d}\sum_{i=1}^{d}Z_{i}\geq\,\mathbb{E}[Z_{i}]+\max_{ v\in\mathcal{V}}\!\|v\|_{4}^{2}\sqrt{\frac{32\varepsilon}{d}}+64\max_{v\in \mathcal{V}}\!\|v\|_{2}^{4}\frac{\varepsilon}{d}\right)\leq e^{-\varepsilon}.\]To conclude, it remains only to show the bound \(\mathbb{E}[Z_{i}]\leq\max_{v}\lVert v\rVert_{2}^{2}(16\log\lvert\mathcal{V} \rvert+9)\). This follows by a standard log-sum-exp approach. Below, we abbreviate \(\rho_{j}\coloneqq W_{i,j}\). We can observe that for any \(\lambda>0\):

\[\mathbb{E}[Z_{i}] =\mathbb{E}\left[\max_{v\in\mathcal{V}}\sum_{j\in[n]}\sum_{ \begin{subarray}{c}k\in[n]\\ k\neq j\end{subarray}}\rho_{j}\rho_{k}v_{j}v_{k}\right]\] \[\leq\frac{1}{\lambda}\log\left(\sum_{v\in\mathcal{V}}\mathbb{E} \left[\exp\left(\lambda\sum_{j\in[n]}\sum_{\begin{subarray}{c}k\in[n]\\ k\neq j\end{subarray}}\rho_{j}\rho_{k}v_{j}v_{k}\right)\right]\right)\] \[\leq\frac{1}{\lambda}\log\left(\sum_{v\in\mathcal{V}}\mathbb{E} \left[\exp\left(\lambda\left(\sum_{j=1}^{n}\rho_{j}v_{j}\right)^{2}\right) \right]\right)\] (43)

Note that \(X\coloneqq\sum_{j}\rho_{j}v_{j}\) is subGaussian with parameter \(\lVert v\rVert_{2}^{2}\), since:

\[\mathbb{E}\left[\exp\left(\lambda\sum_{j=1}^{n}\rho_{j}v_{j}\right)\right]= \prod_{j=1}^{n}\mathbb{E}[\exp(\lambda\rho_{j}v_{j})]\leq\prod_{j=1}^{n}\exp \left(\frac{\lambda^{2}v_{j}^{2}}{2}\right)=\exp\left(\frac{\lambda^{2}}{2} \lVert v\rVert_{2}^{2}\right).\]

Then, it follows (e.g. Lemma 1.12 of Rigollet et al. [14]) that \(X^{2}-\mathbb{E}[X^{2}]\) satisfies a sub-exponential MGF bound with parameter \(16\lVert v\rVert_{2}^{2}\), i.e.

\[\mathbb{E}[\exp\bigl{(}\lambda(X^{2}-\mathbb{E}[X^{2}])\bigr{)}]\leq\exp \biggl{(}\frac{256}{2}\lambda^{2}\lVert v\rVert_{2}^{4}\biggr{)}\qquad\forall \lvert\lambda\rvert\leq\frac{1}{16\lVert v\rVert_{2}^{2}}.\]

We also note that

\[\mathbb{E}[X^{2}]=\sum_{i,j=1}^{n}v_{i}v_{j}\,\mathbb{E}[\varepsilon_{i} \varepsilon_{j}]=\lVert v\rVert_{2}^{2}.\]

Adding and subtracting \(\mathbb{E}[X^{2}]\) in Eq. (43) gives

\[\leq\frac{1}{\lambda}\log\Biggl{(}\sum_{v\in\mathcal{V}}\mathbb{E} \bigl{[}\exp\bigl{(}\lambda\bigl{(}X^{2}-\lVert v\rVert_{2}^{2}\bigr{)}+ \lambda\lVert v\rVert_{2}^{2}\bigr{)}\bigr{]}\Biggr{)}\] \[=\frac{1}{\lambda}\log\Biggl{(}\sum_{v\in\mathcal{V}}\mathbb{E} \bigl{[}\exp\bigl{(}\lambda\bigl{(}X^{2}-\lVert v\rVert_{2}^{2}\bigr{)} \bigr{)}\bigr{]}\exp\bigl{(}\lambda\lVert v\rVert_{2}^{2}\bigr{)}\Biggr{)}\] \[\leq\frac{1}{\lambda}\log\Biggl{(}\sum_{v\in\mathcal{V}}\exp \bigl{(}128\lambda^{2}\lVert v\rVert_{2}^{4}+\lambda\lVert v\rVert_{2}^{2} \bigr{)}\Biggr{)}\qquad\forall\lvert\lambda\rvert\leq\frac{1}{16\max_{v} \lVert v\rVert_{2}^{2}}\] \[\leq\frac{1}{\lambda}\log\lvert\mathcal{V}\rvert+\max_{v}128 \lambda\lVert v\rVert_{2}^{4}+\max_{v}\lVert v\rVert_{2}^{2}\qquad\forall \lvert\lambda\rvert\leq\frac{1}{16\max_{v}\lVert v\rVert_{2}^{2}}\]

Picking \(\lambda=\frac{1}{16\max_{v}\lVert v\rVert_{2}^{2}}\) concludes the proof. 

#### f.2.2 Golf with on-policy misspecification

Consider the version of Golf[13] in Algorithm 2. We have the following guarantee for the regret of Golf, which extends Jin et al. [13] to allow for _on-policy_ misspecification.

**Lemma F.3**.: _Suppose that \(Q^{M^{\star}_{\text{obs}},\star}\in\mathcal{F}\) and \(\mathcal{G}\) satisfies \(\varepsilon_{\text{apx}}\)-completeness in the sense that for all \(h\in[H]\) and \(f\in\mathcal{F}_{h+1}\), there exists \(g\in\mathcal{G}_{h}\) such that \(\mathbb{E}^{\pi}\Bigl{(}g-\mathcal{T}_{h}^{M^{\star}_{\text{obs}}}f\Bigr{)}^ {2}\leq\varepsilon_{\text{apx}}^{2}\) for all \(\pi\in\Pi_{\mathcal{F}}\coloneqq\{\pi_{f}:f\in\mathcal{F}\}\). Let \(C_{\text{cov}}\coloneqq C_{\text{cov}}(M^{\star}_{\text{obs}},\Pi_{\mathcal{F}})\) (Definition D.3). Then for an appropriate choice of \(\beta\), Algorithm 2 ensures that_

\[\texttt{Reg}\leq H\sqrt{C_{\text{cov}}T\log(\lvert\mathcal{F}\rvert\lvert \mathcal{G}\lvert HT/\delta)}+HT\sqrt{C_{\text{cov}}\log(T)}\varepsilon_{\text{apx}}.\]

[MISSING_PAGE_FAIL:47]

Applying a change of measure argument on the second term then gives:

\[\sum_{t=1}^{T}\sum_{h=1}^{H}\mathbb{E}_{d_{h}^{(t)}}\Big{[}\widetilde {\delta}_{h}^{(t)}(x,a)\mathbb{I}\{t\geq\tau_{h}(x,a)\}\Big{]}\leq H \underbrace{\sqrt{\sum_{t=1}^{T}\sum_{x,a}\frac{\big{(}\mathbb{I}\{t \geq\tau_{h}(x,a)\}d_{h}^{(t)}(x,a)\big{)}^{2}}{\widetilde{d}_{h}^{(t)}(x,a)}}}_ {(\mathsf{A})}\] \[\times\underbrace{\sqrt{\sum_{t=1}^{T}\sum_{x,a}\widetilde{d}_{h} ^{(t)}(x,a)\Big{(}\widetilde{\delta}_{h}^{(t)}(x,a)\Big{)}^{2}}}_{(\mathsf{B})}\]

By the same reasoning as in Xie et al. [10], we have \((\mathsf{A})\leq\mathcal{O}(\sqrt{C_{\mathsf{cov}}\log(T)})\), and by Lemma F.4 we have \((\mathsf{B})\leq\mathcal{O}(\sqrt{\beta T})\). Using that \(\beta=\log(TH|\mathcal{F}|/\delta)+T\varepsilon_{\mathsf{apr}}^{2}\) gives the desired result. It remains to establish the concentration results of Lemma F.4. 

Proof of Lemma F.4.: For any function \(f\), define a random variable

\[X_{t}(h,f)=\big{(}f_{h}(s_{h}^{(t)},a_{h}^{(t)})-r_{h}^{(t)}-f_{h+1}(s_{h+1}^ {(t)})\big{)}^{2}-\big{(}\mathcal{T}_{h}f_{h+1}(s_{h}^{(t)},a_{h}^{(t)})-r_{h} ^{(t)}-f_{h+1}(s_{h+1}^{(t)})\big{)}^{2}.\]

Let \(\mathfrak{F}_{t,h}=\{s_{1}^{(t)},a_{1}^{(t)},r_{1}^{(t)},\ldots,s_{H}^{(t)},a_ {H}^{(t)},r_{H}^{(t)}\}_{i<t}\). Note that

\[\mathbb{E}\big{[}r_{h}^{(t)}+f_{h+1}(s_{h+1}^{(t)})\mid\mathfrak{F}_{t,h} \big{]}=\mathbb{E}^{\pi^{(t)}}[\mathcal{T}_{h}f(s_{h},a_{h})].\] (44)

and thus that

\[\mathbb{E}[X_{t}(h,f)\mid\mathfrak{F}_{t,h}]=\mathbb{E}^{\pi^{(t)}}\Big{[} \big{(}f_{h}(s_{h},a_{h})-\mathcal{T}_{h}f_{h}(s_{h},a_{h})\big{)}^{2}\Big{]}.\]

Next, note that

\[\mathsf{Var}[X_{t}(h,f)\mid\mathfrak{F}_{t,h}]\leq\mathbb{E}\Big{[} \big{(}f_{h}(s_{h}^{(t)},a_{h}^{(t)})-\mathcal{T}_{h}f_{h}(s_{h}^{(t)},a_{h}^{ (t)})\big{)}^{2}\big{(}f_{h}(s_{h}^{(t)},a_{h}^{(t)})+\mathcal{T}_{h}f_{h}(s_{ h}^{(t)},a_{h}^{(t)})+2\big{(}r_{h}^{(t)}-f_{h+1}(s_{h+1}^{(t)})\big{)}\big{)}^{2} \mid\mathfrak{F}_{t,h}\Big{]}\] \[\leq 16\,\mathbb{E}\Big{[}\big{(}f_{h}(s_{h}^{(t)},a_{h}^{(t)})- \mathcal{T}_{h}f_{h}(s_{h}^{(t)},a_{h}^{(t)})\big{)}^{2}\mid\mathfrak{F}_{t,h} \Big{]}=16\,\mathbb{E}[X_{t}(h,f)\mid\mathfrak{F}_{t,h}].\]

By Freedman's inequality (Lemma C.2, Lemma C.3), we have that with probability at least \(1-\delta\):

\[\left|\sum_{i<t}X_{i}(h,f)-\sum_{i<t}\mathbb{E}[X_{i}(h,f)\mid\mathfrak{F}_{i,h}]\right|\leq\mathcal{O}\Bigg{(}\sqrt{\log(1/\delta)\sum_{i<t}\mathbb{E}[X_{ i}(h,f)\mid\mathfrak{F}_{i,h}]}+\log(1/\delta)\Bigg{)}\]

Taking a union bound over \([T]\times[H]\times\mathcal{F}\), we have that for all \(t,h,f\), with probability at least \(1-\delta\):

\[\left|\sum_{i<t}X_{i}(h,f)-\sum_{i<t}\mathbb{E}^{\pi^{(i)}} \Big{[}\big{(}f_{h}(s_{h},a_{h})-\mathcal{T}_{h}f_{h}(s_{h},a_{h})\big{)}^{2} \Big{]}\right|\] (45) \[\leq\mathcal{O}\Bigg{(}\sqrt{\iota\sum_{i<t}\mathbb{E}^{\pi^{(i)} }\Big{[}\big{(}f_{h}(s_{h},a_{h})-\mathcal{T}_{h}f_{h}(s_{h},a_{h})\big{)}^{2} \Big{]}}+\iota\Bigg{)},\] (46)

where \(\iota=\log(|\mathcal{F}|HT/\delta)\). We now show that

\[\sum_{i<t}X_{i}(h,f^{(t)})\leq\beta+\mathcal{O}\big{(}T\varepsilon_{\mathsf{apr }}^{2}+\iota\big{)}=\mathcal{O}(\beta),\] (47)

which will imply, from Eq. (46), that

\[\sum_{i<t}\mathbb{E}^{\pi^{(t)}}\Big{[}\big{(}f_{h}(s_{h},a_{h})-\mathcal{T}_{h }f_{h}(s_{h},a_{h})\big{)}^{2}\Big{]}\leq\mathcal{O}(\iota+\beta)=\mathcal{O} (\beta),\]

as desired. To see Eq. (47), let

\[\Delta_{t}=\sum_{i<t}\bigl{(}\mathsf{apr}\big{[}\mathcal{T}_{h}f_{h+1}^{(t)} \big{]}\big{(}s_{h}^{(t)},a_{h}^{(t)})-r_{h}^{(t)}-f_{h+1}^{(t)}(s_{h+1}^{(t)}) \big{)}^{2}-\big{(}\mathcal{T}_{h}f_{h}^{(t)}(s_{h}^{(t)},a_{h}^{(t)})-r_{h}^{( t)}-f_{h+1}^{(t)}(s_{h+1}^{(t)})\big{)}^{2}\]and then note that:

\[\sum_{i<t}X_{i}(h,f^{{(t)}}) =\sum_{i<t}\bigl{(}f_{h}^{{(t)}}(s_{h}^{{(t)}},a_{h}^{{(t)}})-r _{h}^{{(t)}}-f_{h+1}^{{(t)}}(s_{h+1}^{{(t)}})\bigr{)}^{2}-\bigl{(}\mathcal{T}_{h} f_{h}^{{(t)}}(s_{h}^{{(t)}},a_{h}^{{(t)}})-r_{h}^{{(t)}}-f_{h+1}^{{(t)}}(s_{h+1}^{ {(t)}})\bigr{)}^{2}\] \[=\sum_{i<t}\bigl{(}f_{h}^{{(t)}}(s_{h}^{{(t)}},a_{h}^{{ (t)}})-r_{h}^{{(t)}}-f_{h+1}^{{(t)}}(s_{h+1}^{{(t)}})\bigr{)}^{2}\] \[\qquad-\sum_{i<t}\bigl{(}\mathsf{apx}\bigl{[}\mathcal{T}_{h}f_{h+1 }^{{(t)}}\bigr{]}(s_{h}^{{(t)}},a_{h}^{{(t)}})-r_{h}^{{(t)}}-f_{h+1}^{{ (t)}}(s_{h+1}^{{(t)}})\bigr{)}^{2}+\Delta_{t}\] \[\leq\sum_{i<t}\bigl{(}f_{h}^{{(t)}}(s_{h}^{{ (t)}},a_{h}^{{(t)}})-r_{h}^{{(t)}}-f_{h+1}^{{(t)}}(s_{h+1}^{ {(t)}})\bigr{)}^{2}\] \[\qquad-\inf_{g_{h}\in\mathcal{G}_{h}}\sum_{i<t}\bigl{(}g(s_{h}^{ {(t)}},a_{h}^{{(t)}})-r_{h}^{{(t)}}-f_{h+1}^{{(t)}}(s_{h+1}^{ {(t)}})\bigr{)}^{2}+\Delta_{t}\] \[\leq\beta+\Delta_{t}.\]

where the second-to-last line follows from \(\mathsf{apx}\bigl{[}\mathcal{T}_{h}f_{h+1}^{{(t)}}\bigr{]}\in\mathcal{G}\) and the last line follows from the definition of the confidence set. It remains to show that \(\Delta_{t}\leq\mathcal{O}(T\varepsilon_{\mathsf{apx}}^{2}+\iota)\), which we do via a similar concentration argument. Namely, let

\[Y_{t}(h,f)=\bigl{(}\mathsf{apx}[\mathcal{T}_{h}f_{h+1}](s_{h}^{{ (t)}},a_{h}^{{(t)}})-r_{h}^{{(t)}}-f_{h+1}^{{(k)}}(s_{h+1}^{ {(t)}})\bigr{)}^{2}-\bigl{(}\mathcal{T}_{h}f_{h}(s_{h}^{{(t)}},a_{h}^{{(t)}})- r_{h}^{{(t)}}-f_{h+1}^{{(k)}}(s_{h+1}^{{(t)}})\bigr{)}^{2},\]

and note that, as before,

\[\mathbb{E}[Y_{t}(h,f)\mid\mathfrak{F}_{t,h}]=\mathbb{E}^{\pi^{{(t)}}}\Bigl{[} \bigl{(}\mathsf{apx}[\mathcal{T}_{h}f_{h+1}](s_{h},a_{h})-\mathcal{T}_{h}f_{h} (s_{h},a_{h})\bigr{)}^{2}\Bigr{]},\]

and

\[\mathsf{Var}[Y_{t}(h,f)\mid\mathfrak{F}_{t,h}]\leq 16\,\mathbb{E}[Y_{t}(h,f) \mid\mathfrak{F}_{t,h}],\]

by the same calculation as earlier. Thus, by Freedman's inequality and a union bound, we have that, with probability at least \(1-\delta\),

\[\left|\sum_{i<t}Y_{t}(h,f)-\sum_{i<t}\mathbb{E}^{\pi^{{(t)}}} \Bigl{[}\bigl{(}\mathsf{apx}[\mathcal{T}_{h}f_{h+1}](s_{h},a_{h})-\mathcal{T} _{h}f_{h}(s_{h},a_{h})\bigr{)}^{2}\Bigr{]}\right|\] (48) \[\leq\mathcal{O}\Biggl{(}\sqrt{\iota\sum_{i<t}\mathbb{E}^{\pi^{{(t )}}}\Bigl{[}\bigl{(}\mathsf{apx}[\mathcal{T}_{h}f_{h+1}](s_{h},a_{h})- \mathcal{T}_{h}f_{h}(s_{h},a_{h})\bigr{)}^{2}\Bigr{]}}+\iota\Biggr{)},\] (49)

where \(\iota=\log(|\mathcal{F}|HT/\delta)\). Recalling the misspecification assumption, this implies that

\[\sum_{i<t}Y_{t}(h,f)\leq\mathcal{O}\bigl{(}t\varepsilon_{\mathsf{apx}}^{2}+ \iota\bigr{)},\]

for all \(h,f,t\), with high probability. This concludes the result for \((ii)\). For \((i)\), this follows identically to the proof of Lemma 40 in Jin et al. [17], since this only uses the property that \(Q^{\star}\in\mathcal{F}\).

#### f.2.3 Sample-efficient latent-dynamics RL under pushforward coverability

We conclude by combining the previous two results to obtain the main result for this section.

**Theorem 3.2** (Pushforward-coverable MDPs are statistically modular).: _Let \(\mathcal{M}_{\mathsf{1st}}\) be a base MDP class such that each \(M_{\mathsf{1st}}\in\mathcal{M}_{\mathsf{1st}}\) has pushforward coverability bounded by \(C_{\mathsf{push}}(M_{\mathsf{1st}})\leq C_{\mathsf{push}}\). Then, for any decoder class \(\Phi\), we have:_

1. \(\mathsf{comp}(\mathcal{M}_{\mathsf{1st}},\varepsilon,\delta)\leq\mathsf{poly}(C _{\mathsf{push}},|\mathcal{A}|,H,\log|\mathcal{M}_{\mathsf{1st}}|,\varepsilon^{-1 },\log\bigl{(}\delta^{-1}\bigr{)}\bigr{)}\)_, and_
2. \(\mathsf{comp}(\llparent\mathcal{M}_{\mathsf{1st}},\Phi\rrangle,\varepsilon, \delta)\leq\mathsf{poly}(C_{\mathsf{push}},|\mathcal{A}|,H,\log|\mathcal{M}_{ \mathsf{1st}}|,\log|\Phi|,\varepsilon^{-1},\log\bigl{(}\delta^{-1}\bigr{)}, \log\log|\mathcal{S}|)\)_._

**Proof of Theorem 3.2.** Let \(M_{\mathsf{obs}}^{\ast}\coloneqq\llparenthesis M_{\mathsf{1st}}^{\ast},\psi^{ \ast}\rrangle\in\llparenthesis\mathcal{M}_{\mathsf{1st}},\Phi\rrangle\) be the unknown latent-dynamics MDP. Define observation-level value functions

\[\mathcal{F}=\{Q^{M_{\mathsf{1st}},\star}\circ\phi\mid M_{\mathsf{1st}}\in \mathcal{M}_{\mathsf{1st}},\phi\in\Phi\},\]so that \(Q^{M^{\star}_{\text{\tiny{lat}}},\star}=Q^{M^{\star}_{\text{\tiny{lat}}},\star} \diamond\phi^{\star}\in\mathcal{F}\) via decoder and model realizability, and \(\log\lvert\mathcal{F}_{h}\rvert\leq\log\lvert\mathcal{M}_{\text{\tiny{lat}}} \rvert\rvert\Phi\rvert\). Consider any function class \(\mathcal{L}\subseteq\{\mathcal{S}\rightarrow[0,1]\}\) and MDP \(M_{\text{\tiny{1at}}}=(r_{\text{\tiny{1at}}},P_{\text{\tiny{1at}}})\). For a given value \(\varepsilon_{\text{\tiny{apx}}}>0\), setting \(d\) according to Lemma F.1 implies that there exists a \(d\)-dimensional feature map \(\varphi_{\text{\tiny{{{M_{\text{\tiny{lat}}}}}}},h}(s,a)\in\mathbb{R}^{d+1}\) such that for all \(\ell\in\mathcal{L}\) and \(h\in[H]\), there exists \(w_{\ell,h}\in\mathbb{R}^{d+1}\) such that

\[\mathbb{E}_{\mu_{M_{\text{\tiny{lat}}}}\otimes\text{\tiny{Unif}}(\mathcal{A})} \bigg{[}\Big{(}\texttt{clip}_{[0,2]}\big{[}\big{\langle}\varphi_{\text{\tiny{ M_{\tiny{lat}}}},h}(s,a),w_{\ell,h}\big{\rangle}\big{]}-\mathcal{T}^{M_{\text{\tiny{lat}}}}_{h} \ell_{h+1}(s,a)\Big{)}^{2}\bigg{]}\leq\varepsilon_{\text{\tiny{apx}}},\] (50)

where \(\mu_{M_{\text{\tiny{lat}}}}\) is the pushforward coverability distribution for \(M_{\text{\tiny{1at}}}\). Moreover, the map \(\varphi_{h}\) is explicitly computed as a function of \(M_{\text{\tiny{1at}}}\) by a randomized algorithm with success probability \(1-\delta\), with no knowledge of the class \(\mathcal{L}\) required. We consider the class

\[\mathcal{L}=\Bigg{\{}\Gamma_{\phi}\diamond Q^{M_{\text{\tiny{lat}}},\star}(s,a )\coloneqq\sum_{s^{\prime}\in\mathcal{S}}\Gamma_{\phi}(s^{\prime}\mid s)Q^{M_{ \text{\tiny{lat}}},\star}(s^{\prime},a)\mid\phi\in\Phi,M_{\text{\tiny{1at}}} \in\mathcal{M}_{\text{\tiny{1at}}}\Bigg{\}},\] (51)

where \(\Gamma_{\phi}:\mathcal{S}\rightarrow\Delta(\mathcal{S})\) is the mismatch function for decoder \(\phi\) and emission \(\psi^{\star}\), defined in Definition F.1. Note that \(\mathcal{L}\) has size \(\log\lvert\mathcal{L}\rvert\leq\log\lvert\mathcal{M}_{\text{\tiny{1at}}} \rvert\lvert\Phi\rvert\), and that we have

\[\mathcal{T}^{M_{\text{\tiny{obs}}}^{\star}}_{h}(Q^{M_{\text{\tiny{lat}}},\star }_{h}\diamond\phi_{h})(x,a)=\mathcal{T}^{M_{\text{\tiny{lat}}}^{\star}}_{h}( \Gamma_{\phi,h+1}\circ V^{M_{\text{\tiny{lat}}},\star}_{h})(\phi^{\star}_{h}(x ),a)\]

by Lemma D.7. By Lemma D.1 we have that \(\mu_{M_{\text{\tiny{obs}}}^{\star},h}(x)=\psi^{\star}_{h}(x\mid\phi^{\star}_{ h}(x))\mu_{M_{\text{\tiny{lat}}}^{\star},h}(\phi^{\star}_{h}(x))\) is the coverability distribution for MDP \(M_{\text{\tiny{obs}}}^{\star}\), and

\[\mathbb{E}_{\mu_{M_{\text{\tiny{lat}}}^{\star}}\otimes\text{\tiny{Unif}}( \mathcal{A})}[f(s,a)]=\mathbb{E}_{\mu_{M_{\text{\tiny{obs}}}^{\star}}\otimes \text{\tiny{Unif}}(\mathcal{A})}[f(\phi^{\star}(x),a)].\]

Now, define

\[\mathcal{G}_{\text{\tiny{{{M_{\text{\tiny{lat}}}}}}},h}=\Big{\{}(x,a)\mapsto \texttt{clip}_{[0,2]}[\langle\varphi_{\text{\tiny{M_{\tiny{lat}}}},h}(\phi(x),a ),w\rangle]\mid\phi\in\Phi,\left\lVert w\right\rVert_{2}^{2}\leq 11+16\log( \lvert\mathcal{M}_{\text{\tiny{1at}}}\rvert\lvert\Phi\rvert H)\Big{\}}.\]

Recall the definition of \(w_{f}\) (for \(f:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\)) from Lemma F.1, and note that by the norm bound \(\max_{\ell\in\mathcal{L}}\lVert w_{\ell}\rVert_{2}^{2}\leq 11+16\log(\lvert \mathcal{M}_{\text{\tiny{1at}}}\rvert\lvert\Phi\rvert H)\) given by Lemma F.1, we have \((x,a)\mapsto\langle\varphi_{M_{\text{\tiny{lat}}},h}(\phi(x),a),w_{\ell} \rangle\in\mathcal{G}_{h}\) for every \(\ell\in\mathcal{L}\). Next, note that by the norm bound \(\max_{s,a}\lVert\psi(s,a)\rVert_{2}^{2}\leq C_{\text{\tiny{push}}}(11+16\log( \lvert\mathcal{S}\rvert\lvert\mathcal{A}\rvert H))\), given by Lemma F.1, we have every \(g_{h}\in\mathcal{G}_{M_{\text{\tiny{lat}}},h}\) satisfies \(\lVert g_{h}\rVert_{\infty}\leq cC_{\text{\tiny{push}}}^{1/2}\log(\lvert \mathcal{M}_{\text{\tiny{1at}}}\rvert\lvert\Phi\rvert\lvert\mathcal{S}\rvert \lvert\mathcal{A}\rvert H)\coloneqq B\) for some absolute constant \(c\). Therefore, \(\mathcal{G}_{M_{\text{\tiny{lat}}},h}\) has size \(\log\lvert\mathcal{G}_{M_{\text{\tiny{lat}}},h}\rvert\leq\widetilde{O}(d\cdot \log(B)+\log\lvert\Phi\rvert)=\widetilde{O}(d\log\log(\lvert\mathcal{S}\rvert )+\log\lvert\Phi\rvert)\), where the \(\widetilde{O}\) notation ignores logarithmic factors of \(C_{\text{\tiny{push}}}\), \(\lvert\mathcal{A}\rvert\), \(\log\lvert\mathcal{M}_{\text{\tiny{1at}}}\rvert\), and \(\log\lvert\Phi\rvert\).22 Define \(\mathcal{G}_{h}=\cup_{M_{\text{\tiny{lat}}}\in\mathcal{M}_{\text{\tiny{1at}}}} \mathcal{G}_{M_{\text{\tiny{int}}},h}\), which has size \(\log\lvert\mathcal{G}_{h}\rvert\leq\log\lvert M_{\text{\tiny{1at}}}\rvert+( \widetilde{O}(d\log\log(\lvert\mathcal{S}\rvert)+\log\lvert\Phi\rvert))\). Together, these results with Lemma F.1 imply that for all \(f_{h+1}\in\mathcal{F}_{h+1}\), there exists \(g_{h}\in\mathcal{G}_{h}\) such that

Footnote 22: Formally, this requires a standard covering number argument; we omit the details.

\[\mathbb{E}_{\mu_{M_{\text{\tiny{obs}}}^{\star},h}\otimes\text{\tiny{Unif}}( \mathcal{A})}\bigg{[}\Big{(}g_{h}(x_{h},a_{h})-\Big{[}\mathcal{T}^{M_{\text{ \tiny{obs}}}^{\star}}_{h}f_{h+1}\Big{]}(x_{h},a_{h})\Big{)}^{2}\bigg{]}\leq \varepsilon_{\text{\tiny{apx}}}.\]

This, in turn, implies that for all \(\pi_{\text{\tiny{obs}}}\in\Pi_{\text{\tiny{rms}}}\) we have

\[\mathbb{E}^{\pi_{\text{\tiny{obs}}}}\bigg{[}\Big{(}g_{h}(x_{h},a_{h})-\Big{[} \mathcal{T}^{M_{\text{\tiny{obs}}}^{\star}}_{h}f_{h+1}\Big{]}(x_{h},a_{h}) \Big{)}^{2}\bigg{]}\leq C_{\text{\tiny{push}}}\lvert\mathcal{A}\rvert \varepsilon_{\text{\tiny{apx}}},\]

since \(\mu_{M_{\text{\tiny{obs}}}^{\star},h}\otimes\text{\tiny{Unif}}(\mathcal{A})\) satisfies coverability (Definition D.3) with parameter \(C_{\text{\tiny{cov}}}(M_{\text{\tiny{obs}}}^{\star},\Pi_{\text{\tiny{rms}}}) \leq C_{\text{\tiny{push}}}\lvert\mathcal{A}\rvert\) (Eq. (29)).

Then, it follows by Lemma F.3 that if we run Algorithm 2 with the classes \(\mathcal{F}\) and \(\mathcal{G}\) we will get

\[\texttt{Reg} \leq H\sqrt{C_{\text{\tiny{push}}}\lvert\mathcal{A}\rvert T\log( \lvert\mathcal{M}_{\text{\tiny{1at}}}\rvert\lvert\Phi\rvert HT/\delta)(d \log\log(\lvert\mathcal{S}\rvert)+\log\lvert\Phi\rvert)}+HT\sqrt{C_{\text{ \tiny{push}}}^{2}\lvert\mathcal{A}\rvert^{2}\log(T)\varepsilon_{\text{ \tiny{apx}}}}\] \[\leq H\sqrt{C_{\text{\tiny{push}}}^{5}\lvert\mathcal{A}\rvert T\log( \lvert\mathcal{M}_{\text{\tiny{1at}}}\rvert\lvert\Phi\rvert HT/\delta)\frac{\log \big{(}C_{\text{\tiny{push}}}^{2}\lvert\mathcal{M}_{\text{\tiny{1at}}}\rvert \lvert\Phi\rvert^{2}H\delta^{-1}/\varepsilon_{\text{\tiny{apx}}}\big{)}\log\log( \lvert\mathcal{S}

Choosing \(\varepsilon_{\mathsf{apx}}=\frac{1}{\sqrt{T}}\) to balance leads to

\[\mathsf{Reg} \lesssim HT^{3/4}\sqrt{C_{\text{push}}^{5}|\mathcal{A}|\log(| \mathcal{M}_{\text{1at}}||\Phi|HT/\delta)\log\bigl{(}C_{\text{push}}^{2}| \mathcal{M}_{\text{1at}}||\Phi|^{2}H\delta^{-1}T\bigr{)}\log\log(|\mathcal{S}|)}\] \[\qquad+HT^{3/4}\sqrt{C_{\text{push}}^{2}|\mathcal{A}|^{2}\log(T)}\] \[\lesssim HT^{3/4}\sqrt{C_{\text{push}}^{5}|\mathcal{A}|^{2}\log(| \mathcal{M}_{\text{1at}}||\Phi|HT/\delta)\log\bigl{(}TC_{\text{push}}^{2}| \mathcal{M}_{\text{1at}}||\Phi|^{2}H/\delta\bigr{)}\log\log(|\mathcal{S}|)},\]

which gives a risk bound of

\[\mathsf{Risk}\lesssim\frac{1}{T^{1/4}}H\sqrt{C_{\text{push}}^{5}|\mathcal{A}| ^{2}\log(|\mathcal{M}_{\text{1at}}||\Phi|HT/\delta)\log\bigl{(}TC_{\text{push}}^ {2}|\mathcal{M}_{\text{1at}}||\Phi|^{2}H/\delta\bigr{)}\log\log(|\mathcal{S}|)}.\]

Equating this to \(\varepsilon\) gives a sample complexity of

\[T=\mathsf{poly}(C_{\text{push}},A,H,\log|\mathcal{M}_{\text{1at}}|,\log|\Phi|,\varepsilon^{-1},\log\bigl{(}\delta^{-1}\bigr{)},\log\log(|\mathcal{S}|)),\]

as desired. Note that we have not made much effort to optimize the rate; in particular, a faster rate is likely possible by using the Golf.Dbr algorithm of Amortila et al. [1], which improves over the Golf algorithm under the presence of misspecification.

Proofs and Additional Information for Section 4.1: Hindsight RL

This appendix contains additional information and proofs related to algorithmic modularity under hindsight observations (Section 4.1), and is organized as follows:

* Appendix G.1 contains the pseudocode and proofs related to the online representation learning oracle ExpWeights.Dr (Lemma 4.1).
* Appendix G.2 contains the proof for our risk bound of the O2L algorithm under hindsight observability (Theorem 4.1).

### Pseudocode and Proofs for ExpWeights.Dr (Lemma 4.1)

``` input: Decoder set \(\Phi\) for\(t=1,2,\cdots,T\)do  Get dataset \(\big{\{}x_{h}^{(i)},\phi^{\star}(x_{h}^{(i)})\big{\}}_{i\in[t-1],h\in[H]}\) for\(h=1,\ldots,H\)do  For \(\phi\in\Phi\), compute \[q_{h}^{(i)}(\phi_{h})\propto\exp\Biggl{(}-\sum_{i=1}^{t-1}\mathbb{I}\big{[} \phi_{h}(x_{h}^{(i)})\neq\phi_{h}^{\star}(x_{h}^{(i)})\big{]}\Biggr{)},\] and set \[\bar{\phi}_{h}^{(t)}(x)=\operatorname*{arg\,max}_{s\in\mathcal{S}}\mathbb{P} _{\phi_{h}\sim q_{h}^{(t)}}(\phi_{h}(x)=s).\] (52) endfor  Return \(\bar{\phi}^{(t)}=\{\bar{\phi}_{h}^{(t)}\}_{h=1}^{H}\). endfor ```

**Algorithm 3** Derandomized Exponential Weights (ExpWeights.Dr)

The main result for this estimator is the following.

**Lemma 4.1** (Online classification via ExpWeights.Dr).: _Under decoder realizability \((\phi^{\star}\in\Phi)\), ExpWeights.Dr (Algorithm 3) satisfies Assumption 4.2 with23_

Footnote 23: In this section, the notations \(\widetilde{\mathcal{O}},\approx,\text{and}\lesssim\) ignore only constants and logarithmic factors of \(H\).

\[\mathtt{Est}_{\text{class}}(T)=\widetilde{\mathcal{O}}(H\log|\Phi|).\]

**Proof of Lemma 4.1.** For each \(h\in[H]\), consider the realizable online classification problem where \(x_{h}^{(t)}\sim d_{h}^{\pi^{(t)}}\), for \(\pi^{(t)}\) chosen adversarially, and \(y_{h}^{(t)}=\phi_{h}^{\star}(x_{h}^{(t)})\). Consider the exponential weights estimator

\[q_{h}^{(t)}(\phi)\propto\exp\Biggl{(}-\sum_{i=1}^{t-1}\mathbb{I}\big{[}\phi(x _{h}^{(i)})\neq\phi_{h}^{\star}(x_{h}^{(i)})\big{]}\Biggr{)}.\]

For every sequence \((x_{h}^{(t)})_{t=1}^{T}\), these distributions satisfy the deterministic regret bound

\[\sum_{t=1}^{T}\mathbb{E}_{\bar{\phi}_{h}^{(t)}\sim q_{h}^{(t)}}\Bigl{[} \mathbb{I}\Bigl{[}\widehat{\phi}_{h}^{(t)}(x_{h}^{(t)})\neq\phi_{h}^{\star}(x _{h}^{(t)})\Bigr{]}\Bigr{]}\leq 2\log|\Phi|,\]

by Corollary 2.3 of Cesa-Bianchi et al. [3]. Taking conditional expectations over \(x_{h}^{(t)}\sim d_{h}^{\pi^{(t)}}\) and using Lemma C.3 gives that with probability at least \(1-\delta\):

\[\sum_{t=1}^{T}\mathbb{E}_{\bar{\phi}_{h}^{(t)}\sim q_{h}^{(t)}}\mathbb{E}^{ \pi^{(t)}}\Bigl{[}\mathbb{I}\Bigl{[}\widehat{\phi}_{h}^{(t)}(x_{h})\neq\phi_{ h}^{\star}(x_{h})\Bigr{]}\Bigr{]}\leq 4\log|\Phi|+8\log\bigl{(}2\delta^{-1} \bigr{)}.\]

Taking a union bound over \(h\in[H]\) and summing over \(h\in[H]\) we obtain that with probability at least \(1-\delta\):

\[\sum_{t=1}^{T}\sum_{h=1}^{H}\mathbb{E}_{\bar{\phi}_{h}^{(t)}\sim q_{h}^{(t)}} \mathbb{E}^{\pi^{(t)}}\Bigl{[}\mathbb{I}\Bigl{[}\widehat{\phi}_{h}^{(t)}(x_{ h})\neq\phi_{h}^{\star}(x_{h})\Bigr{]}\Bigr{]}\leq 4H\log|\Phi|+8H\log\bigl{(}2H \delta^{-1}\bigr{)}.\]Now, recall that at each time \(t\), we define the improper decoder \(\bar{\phi}_{h}^{{(t)}}\) via:

\[\bar{\phi}_{h}^{{(t)}}(x)=\operatorname*{arg\,max}_{s\in\mathcal{ S}}\mathbb{P}_{\phi_{h}^{{(t)}}\sim q_{h}^{{(t)}}}(\phi_{h}^{{(t)}}(x)=s)\] (53)

Let \(\ell_{h}(x_{h},q_{h}^{{(t)}})=\mathbb{P}_{\phi_{h}^{{(t)}}\sim q _{h}^{{(t)}}}(\phi_{h}^{{(t)}}(x_{h})\neq\phi_{h}^{\star}(x_{h}))\). Note that \(\ell\) satisfies

\[\sum_{t=1}^{T}\sum_{h=1}^{H}\mathbb{E}_{\phi_{h}^{{(t)}}\sim q _{h}^{{(t)}}}\mathbb{E}^{{}^{\pi^{(t)}}}\big{[}\mathbb{I}\big{[}\phi_{h}^{{(t) }}(x_{h})\neq\phi_{h}^{\star}(x_{h})\big{]}\big{]} =\sum_{t=1}^{T}\sum_{h=1}^{H}\mathbb{E}^{{}^{{ (t)}}}\mathbb{E}_{\phi_{h}^{{(t)}}\sim q_{h}^{{(t)}}} \big{[}\mathbb{I}\big{[}\phi_{h}^{{(t)}}(x_{h})\neq\phi_{h}^{ \star}(x_{h})\big{]}\big{]}\] (54) \[=\sum_{t=1}^{T}\sum_{h=1}^{H}\mathbb{E}^{{}^{{ (t)}}}[\ell_{h}(x_{h},q_{h}^{{(t)}})].\] (55)

By abuse of notation we also denote \(\ell_{h}(x_{h},\bar{\phi}_{h})=\mathbb{I}\big{[}\bar{\phi}_{h}(x)\neq\phi^{ \star}(x)\big{]}\). We will show that

\[\forall x,t,h:\ell_{h}(x_{h},\bar{\phi}_{h}^{{(t)}}) \leq 2\ell_{h}(x_{h},q_{h}^{{(t)}}),\] (56)

from which we will obtain that with probability at least \(1-\delta\):

\[\mathtt{Reg}_{\text{class}}(T)=\sum_{t=1}^{T}\sum_{h=1}^{H}\mathbb{E}^{{}^{{ (t)}}}\big{[}\mathbb{I}\big{[}\bar{\phi}_{h}^{{(t)}}(x_{h})\neq\phi_{h}^{\star }(x_{h})\big{]}\big{]}\leq 8H\log|\Phi|+16H\log\bigl{(}2H\delta^{-1}\bigr{)}.\]

Integrating the high-probability regret bound gives

\[\mathbb{E}[\mathtt{Reg}_{\text{class}}(T)]=\mathcal{O}(H\log(H| \Phi|)),\]

as desired. Towards establishing Eq. (56), let us fix \(x\) and let \(s_{\max}\) denote the argmax in Eq. (53). There are two cases:

* \(\mathbb{P}_{\phi_{h}^{{(t)}}\sim q_{h}^{{(t)}}}(\phi_{h}^{ {(t)}}(x)=s_{\max})\geq\frac{1}{2}\):
* If \(s_{\max}=\phi^{\star}(x)\), \(\ell(x,\bar{\phi}_{h}^{{(t)}})=0\) so we are done.
* Otherwise, \(s_{\max}\neq\phi^{\star}(x)\) and we have \(\ell(x,\bar{\phi}_{h}^{{(t)}})=1\). However, since \(\phi^{\star}(x)\neq s_{\max}\) we have \(\phi_{h}^{{(t)}}(x)=s_{\max}\implies\phi_{h}^{{(t)}}(x)\neq\phi_{h}^{\star}(x)\) and so \[\mathbb{P}_{\phi_{h}^{{(t)}}\sim q_{h}^{{(t)}}}(\phi_{h}^{ {(t)}}(x)\neq\phi_{h}^{\star}(x))\geq\mathbb{P}_{\phi_{h}^{{(t)}}\sim q_{h}^{ {(t)}}}(\phi_{h}^{{(t)}}(x)=s_{\max})\geq\frac{1}{2}=\frac{1}{2} \ell(x,\bar{\phi}_{h}^{{(t)}}).\]
* \(\mathbb{P}_{\phi_{h}^{{(t)}}\sim q_{h}^{{(t)}}}(\phi_{h}^{ {(t)}}(x)=s_{\max})<\frac{1}{2}\):
* If \(s_{\max}=\phi_{h}^{\star}(x)\), \(\ell(x,\bar{\phi}_{h}^{{(t)}})=0\) so we are done.
* Otherwise, \(s_{\max}\neq\phi^{\star}(x)\) and we have \(\ell(x,\bar{\phi}_{h}^{{(t)}})=1\). However, by definition of \(s_{\max}\) as the mode we also have \[\mathbb{P}_{\phi_{h}^{{(t)}}\sim q_{h}^{{(t)}}}(\phi_{h}^{ {(t)}}(x)=\phi_{h}^{\star}(x))\leq\mathbb{P}_{\phi_{h}^{{(t)}}\sim q_{h}^{{(t)} }}(\phi_{h}^{{(t)}}(x)=s_{\max})<\frac{1}{2},\] so in particular we have \[\ell(x,q_{h}^{{(t)}})=\mathbb{P}_{\phi_{h}^{{(t)}}\sim q_{h}^{ {(t)}}}(\phi_{h}^{{(t)}}(x)\neq\phi_{h}^{\star}(x))>\frac{1}{2}=\frac{1}{2} \ell(x,\bar{\phi}_{h}^{{(t)}}).\]

### Proofs for O2L Under Hindsight Observability (Theorem 4.1)

**Theorem 4.1** (Risk bound for O2L under hindsight observability).: _Let \(\mathtt{Alg}_{\text{1at}}\) be a base algorithm with base risk \(\mathtt{Risk}_{\star}(K)\), and \(\mathtt{Rep}_{\text{class}}\) a representation learning oracle satisfying Assumption 4.2. Then Algorithm 1, with inputs \(T,K,\Phi\), \(\mathtt{Rep}_{\text{class}}\), and \(\mathtt{Alg}_{\text{1at}}\), has expected risk_

\[\mathbb{E}[\mathtt{Risk}_{\text{obs}}(TK)]\leq\mathtt{Risk}_{ \star}(K)+\frac{2K}{T}\mathtt{Est}_{\text{class}}(T).\]Proof of Theorem 4.1.: Let \((\widehat{\phi}^{(t)})_{t\in[T]}\) denote the decoders chosen by \(\textsc{Re}_{\textsc{Class}}\), and let \(\rho^{(t)}\) denote the distribution over decoders induced at time \(t\) from the interaction of \(\textsc{Re}_{\textsc{Class}}\), \(\textsc{Alg}_{\textsc{lat}}\), and \(M^{\star}_{\textsc{obs}}\). Let \(\pi^{(t,k)}_{\textsc{obs}}\coloneqq\widehat{\pi}^{(t,k)}_{\textsc{lat}}\circ \widehat{\phi}^{(t)}\) and \(p^{(t,k)}_{\textsc{obs}}\) denote the distribution over (observation-space) policies played at epoch \(t\) and episode \(k\), induced by the interaction of \(\textsc{Re}_{\textsc{Class}}\), \(\textsc{Alg}_{\textsc{lat}}\), and \(M^{\star}_{\textsc{obs}}\). We adopt the notation \(\pi^{(t,K+1)}_{\textsc{lat}}\coloneqq\widehat{\pi}^{(t)}_{\textsc{lat}}\sim p ^{(t,K+1)}_{\textsc{lat}}\) for the final policy output by \(\textsc{Alg}_{\textsc{lat}}\) in epoch \(t\) and \((x^{(t,K+1)}_{h},a^{(t,K+1)}_{h},r^{(t,K+1)}_{h})\) for the trajectory collected from that (observation-level) policy \(\widehat{\pi}^{(t)}_{\textsc{lat}}\circ\widehat{\phi}^{(t)}\). We firstly note that by assumption, we have the guarantee

\[\mathbb{E}\!\left[\sum_{t=1}^{T}\sum_{k=1}^{K+1}\sum_{h=1}^{H} \mathbb{E}_{\pi^{(t,k)}_{\textsc{obs}}\sim p^{(t,k)}_{\textsc{obs}}}\, \mathbb{E}^{\pi^{(t,k)}_{\textsc{obs}}}\Big{[}\mathbb{I}\!\left[\widehat{ \phi}^{(t)}_{h}(x_{h})\neq\phi^{\star}_{h}(x_{h})\right]\right]\right] \leq(K+1)\textsc{Est}_{\textsc{class}}(T)\] \[\leq 2K\textsc{Est}_{\textsc{class}}(T).\] (57)

which follows by applying Assumption 4.2 to the distributions \(\widehat{p}^{(t)}_{\textsc{obs}}=\frac{1}{(K+1)}\sum_{k=1}^{K+1}p^{(t,k)}_{ \textsc{obs}}\) and noting that

\[\sum_{t=1}^{T}\sum_{h=1}^{H}\frac{1}{K+1}\sum_{k=1}^{K+1}\mathbb{E }_{\pi^{(t,k)}_{\textsc{obs}}\sim p^{(t,k)}_{\textsc{lat}}}\,\mathbb{E}^{\pi^ {(t,k)}_{\textsc{obs}}}\Big{[}\mathbb{I}\!\left[\widehat{\phi}^{(t)}_{h}(x_{h} )\neq\phi^{\star}_{h}(x_{h})\right]\Big{]}\] \[=\sum_{t=1}^{T}\sum_{h=1}^{H}\mathbb{E}_{\widehat{\pi}^{(t)}_{ \textsc{obs}}\sim\widehat{p}^{(t)}_{\textsc{lat}}}\mathbb{E}^{\pi^{(t)}_{ \textsc{obs}}}\Big{[}\mathbb{I}\!\left[\widehat{\phi}^{(t)}_{h}(x_{h})\neq\phi ^{\star}_{h}(x_{h})\right]\Big{]}\leq\textsc{Est}_{\textsc{class}}(T).\]

Let \(\textsc{Risk}(K,\textsc{Alg}_{\textsc{lat}},\phi,M^{\star}_{\textsc{obs}})=J ^{M^{\star}_{\textsc{obs}}}(\pi^{\star}_{M^{\star}_{\textsc{obs}}})-J^{M^{ \star}_{\textsc{obs}}}(\widehat{\pi}_{\textsc{lat}}\circ\phi)\) be the random variable denoting the risk of the final policy output by \(\textsc{Alg}_{\textsc{lat}}\) after \(K\) rounds of interaction with \(M^{\star}_{\textsc{obs}}\) when given feature \(\phi\) in any epoch \(t\). For any \(\phi:\mathcal{X}\rightarrow\mathcal{S}\), let \(\mathbb{E}_{\phi}\) denote the law over trajectories \((x^{(k)}_{h},a^{(k)}_{h},r^{(k)}_{h})_{k\in[K+1],h\in[H]}\) and policies \((\pi^{(t)}_{\textsc{lat}}\circ\phi)_{h\in[K+1]}\) generated after \(K\) rounds of interaction when \(\textsc{Alg}_{\textsc{lat}}\) is given feature \(\phi\) in any epoch. (Recall that, for all of the above definitions, a new instance of \(\textsc{Alg}_{\textsc{lat}}\) is initialized at every epoch, so we do not have to specify _which epoch it is_, only the current feature \(\phi\)). Finally, let \(G_{t}\) be the "good" event

\[G_{t}=\Big{\{}\forall k\in[K+1],\forall h\in[H]:\ \widehat{\phi}^{(t)}_{h}(x^{(k,k)}_{h})= \phi^{\star}_{h}(x^{(t,k)}_{h})\Big{\}}.\]

Recall that, in any round \(t\), \(\textsc{Alg}_{\textsc{lat}}\) only observes the latent ("compressed") trajectories \((\widehat{\phi}^{(t)}_{h}(x^{(t,k)}_{h}),a^{(t,k)}_{h},r^{(t,k)}_{h})\) as history for choosing policies. We can therefore conclude that, when \(\widehat{\phi}^{(t)}(x^{(t,k)}_{h})=\phi^{\star}(x^{(t,k)}_{h})\) for all \(k\in[K+1],h\in[H]\), the distribution over final policies \(\widehat{\pi}^{(t)}_{\textsc{lat}}\) chosen by \(\textsc{Alg}_{\textsc{lat}}\) will be identical as if we had chosen \(\phi^{\star}\) as our decoder. In particular, this implies

\[\mathbb{E}_{\widehat{\phi}^{(t)}}\Big{[}\mathbb{I}\!\left\{G_{t} \right\}\!\textsc{Risk}(K,\textsc{Alg}_{\textsc{lat}},\widehat{\phi}^{(t)},M^{\star}_{\textsc{obs}})\Big{]} =\mathbb{E}_{\phi^{\star}}\!\left[\mathbb{I}\!\left\{G_{t}\right\} \!\textsc{Risk}(K,\textsc{Alg}_{\textsc{lat}},\phi^{\star},M^{\star}_{ \textsc{obs}})\right]\] \[\leq\textsc{Risk}_{\star}(K),\] (58)

where the second line simply follows by removing the indicator function, recalling that \(\textsc{Risk}_{\star}(K)=\mathbb{E}[\textsc{Risk}(K,\textsc{Alg}_{\textsc{lat} },M^{\star}_{\textsc{lat}})]\), and using that \(\textsc{Risk}(K,\textsc{Alg}_{\textsc{lat}},\phi^{\star},M^{\star}_{ \textsc{obs}})=\textsc{Risk}(K,\textsc{Alg}_{\textsc{lat}},M^{\star}_{\textsc{ lat}})\). Then, we have:

\[\mathbb{E}[\textsc{Risk}_{\textsc{obs}}(TK)] =\frac{1}{T}\sum_{t=1}^{T}\mathbb{E}_{\widehat{\phi}^{(t)}\sim\rho ^{(t)}}\Big{[}\mathbb{E}_{\widehat{\phi}^{(t)}}\Big{[}\textsc{Risk}(K, \textsc{Alg}_{\textsc{lat}},\widehat{\phi}^{(t)},M^{\star}_{\textsc{obs}}) \Big{]}\Big{]}\] \[\leq\frac{1}{T}\sum_{t=1}^{T}\mathbb{E}_{\widehat{\phi}^{(t)}\sim\rho ^{(t)}}\Big{[}\mathbb{E}_{\widehat{\phi}^{(t)}}\Big{[}\mathbb{I}\!\left\{G_{t} \right\}\!\textsc{Risk}(K,\textsc{Alg}_{\textsc{lat}},\widehat{\phi}^{(t)},M^{\star}_{\textsc{obs}})\Big{]}\Big{]}\] \[\qquad+\frac{1}{T}\sum_{t=1}^{T}\mathbb{E}_{\widehat{\phi}^{(t)} \sim\rho^{(t)}}\Big{[}\mathbb{E}_{\widehat{\phi}^{(t)}}[\mathbb{I}\!\left\{-\!G_{t} \right\}]\Big{]}\] \[\leq\frac{1}{T}\sum_{t=1}^{T}\textsc{Risk}_{\star}(K)+\frac{1}{T} \sum_{t=1}^{T}\mathbb{P}(\neg G_{t})\] \[=\textsc{Risk}_{\star}(K)+\frac{1}{T}\sum_{t=1}^{T}\mathbb{P}( \neg G_{t}),\]where the first equality applies the tower rule for conditional expectation, the second equality applies linearity of conditional expectations and the upper bound \(\mathtt{Risk}(K,\textsc{Alg}_{\mathtt{1at}},\widetilde{\phi}^{{}^{(t)}},M^{ \star}_{\mathsf{obs}})\leq 1\), and the third lines applies the upper bound Eq. (58). It remains to bound the last term. Here, note that by a union bound,

\[\mathbb{P}(\neg G_{t})\leq\mathbb{E}\Bigg{[}\sum_{k=1}^{K+1}\sum_{h=1}^{H} \mathbb{E}_{\pi^{(t,k)}\sim p^{(t,k)}}\,\mathbb{E}^{\pi^{(t,k)}}\,\mathbb{I} \Big{\{}\widehat{\phi}^{{}^{(t)}}(x_{h}^{(t,k)})\neq\phi^{\star}(x_{h}^{(t,k)} )\Big{\}}\Bigg{]},\]

where we have used that trajectory \(k\) in round \(t\) is sampled from policy \(\pi^{(t,k)}\), which is in turn sampled from \(p^{(t,k)}\). Summing over \(t\) and using the bound in Eq. (57) concludes the proof.

Proofs for Appendix A: Self-Predictive Estimation

This appendix contains additional information and proofs related to algorithmic modularity under self-predictive estimation (Appendix A), and is organized as follows:

* Appendix H.1 contains the pseudocode and proofs related to the online representation learning oracle SelfPredict.Opt (Lemma A.1).
* Appendix H.2 contains the proof for our risk bound of the O2L algorithm under self-predictive estimation (Theorem A.1).

### Pseudocode and Proofs for SelfPredict.Opt (Lemma A.1)

The pseudocode for our self-predictive estimation procedure is given in Algorithm 4.

```
1:input: Decoder set \(\Phi\), Latent model class \(\mathcal{M}_{\mathtt{1at}}\), Mismatch-complete class \(\mathcal{L}_{\mathtt{1at}}\), Optimism parameter \(\gamma\)
2:Set \(\beta\coloneqq\frac{1}{2}\sqrt{C_{\textsc{cov}}H\log(T)/T}\)
3:for\(t=1,2,\cdots,T\)do
4: Get dataset \(\mathcal{D}^{{(t)}}=\{x_{h}^{{(t)}},a_{h}^{{(t)}},r_{h}^{{(t)}},x_{h+1}^{{(t)} }\}_{i\in[t-1],h\in[H]}\)
5: Compute \[(\widehat{M}^{{(t)}},\widehat{\phi}^{{(t)}})=\operatorname*{ arg\,max}_{(M,\phi)\in(\mathcal{M}_{\mathtt{1at}},\Phi)}\Bigg{\{}(\gamma \beta)^{-1}J^{{ M}}(\pi_{{ M}})+\sum_{h=1}^{H}\sum_{i=1}^{n}\log\bigl{(}M_{h}(r_{h}^{{(t)}},\phi_{h+1}(x_{h+1}^{ {(t)}})\mid\phi_{h}(x_{h}^{{(t)}}),a_{h}^{{(t)}}) \bigr{)}\] (59) \[-\max_{(M^{\prime},\phi^{\prime})\in(\mathcal{L}_{\mathtt{1at}}, \Phi)}\sum_{i=1}^{n}\log\bigl{(}M_{h}^{\prime}(r_{h}^{{(t)}},\phi_{h+1}(x_{h+ 1}^{{(t)}})\mid\phi_{h}^{\prime}(x_{h}^{{(t)}}),a_{h}^{{ (t)}})\bigr{)}\Bigg{\}}.\] (60)
6: Return \(\widehat{\phi}^{{(t)}}=\left\{\widehat{\phi}^{{(t)}}_{h} \right\}_{h\in[H]}\).
7:endfor ```

**Algorithm 4** Optimistic Self-Predictive Latent Model Estimation (SelfPredict.Opt)

Our main result concerning the SelfPredict.Opt estimator for online optimistic self-predictive estimation is the following. We recall our notation for the instantaneous self-prediction error

\[[\Delta_{h}(M_{\mathtt{1at}},\phi)](x_{h},a_{h})\coloneqq D_{\mathsf{H}}^{2} \bigl{(}M_{\mathtt{1at},h}(\phi_{h}(x_{h}),a_{h}),\bigl{[}\phi_{h+1}\sharp M^{ \star}_{\mathsf{obs},h}\bigr{]}(x_{h},a_{h})\bigr{)}.\]

**Lemma A.1** (Optimistic self-predictive estimation via SelfPredict.Opt).: _Let \(\Pi_{\mathtt{1at}}\) denote the set of policies played by \(\mathtt{Alg}_{\mathtt{1at}}\), and \(C_{\textsc{cov},\mathtt{st}}=C_{\textsc{cov},\mathtt{st}}(M^{\star}_{\mathtt{1at }},\Gamma_{\Phi}\circ\Pi_{\mathtt{1at}})\) be the state coverability parameter on \(M^{\star}_{\mathtt{1at}}\) over the set of stochastic policies \(\Gamma_{\Phi}\circ\Pi_{\mathtt{1at}}\) (Eq. (9)). Then, for any \(\gamma>0\), under decoder realizability \((\phi^{\star}\in\Phi)\), base model realizability \((M^{\star}_{\mathtt{1at}}\in\mathcal{M}_{\mathtt{1at}})\), and mismatch function completeness with class \(\mathcal{L}_{\mathtt{1at}}\) (Assumption A.2), the estimator in Algorithm 4 with inputs \(\Phi,\mathcal{M}_{\mathtt{1at}},\mathcal{L}_{\mathtt{1at}},\) and \(\gamma\) satisfies Assumption A.1 with24_

Footnote 24: In this section, the notations \(\widetilde{\mathcal{O}}\) and \(\lesssim\) ignores constants and logarithmic factors of: \(H,C_{\textsc{cov},\mathtt{st}},|\mathcal{A}|,T,\) and \(\log(|\mathcal{M}_{\mathtt{1at}}||\mathcal{L}_{\mathtt{1at}}||\Phi|)\).

\[\mathtt{Est}_{\mathtt{self};\mathtt{opt}}(T,\gamma)=\widetilde{\mathcal{O}} \biggl{(}\sqrt{HC_{\textsc{cov},\mathtt{st}}|\mathcal{A}|T}\log(|\mathcal{M}_{ \mathtt{1at}}||\mathcal{L}_{\mathtt{1at}}||\Phi|)\biggr{)}.\]

**Proof of Lemma a.1.** We will firstly establish that the algorithm obtains low _offline_ estimation error.

**Lemma H.1** (SelfPredict.Opt attains low offline estimation error).: _For any \(\gamma>0\), under decoder realizability (\(\phi^{\star}\in\Phi\)), model realizability (\(M^{\star}_{\mathtt{1at}}\in\mathcal{M}_{\mathtt{1at}}\)), and mismatch function completeness with class \(\mathcal{L}_{\mathtt{1at}}\) (Assumption A.2), the estimator in Algorithm 4 with inputs \(\Phi\), \(M_{\mathtt{1at}}\), \(\mathcal{L}_{\mathtt{1at}}\), and \(\gamma\) satisfies_that for all \(t\in[T]\), with probability at least \(1-\delta\),_

\[\sum_{h=0}^{H}\sum_{i=1}^{t-1}\mathbb{E}_{\pi^{(t)}\sim p^{(t)}} \mathbb{E}^{\pi^{(t)}}\Big{[}[\Delta_{h}(\widehat{M}^{(t)},\widehat{\phi}^{(t) })](x_{h},a_{h})\Big{]}+\gamma^{-1}\Big{(}J^{M^{*}_{\text{lat}}}(\pi_{M^{*}_{ \text{lat}}})-J^{\widehat{M}^{(t)}}(\pi_{\widehat{M}^{(t)}})\Big{)}\] \[\qquad\qquad\leq\mathcal{O}\big{(}\log\bigl{(}|\mathcal{M}_{\text {1at}}||\mathcal{L}_{\text{1at}}||\Phi|HT\delta^{-1}\bigr{)}\big{)}.\] (61)

Given this result, we can appeal to offline-to-online conversions to establish the final result. Let \(C_{\text{cov}}\coloneqq C_{\text{cov}}(M^{\star}_{\text{obs}},\Pi_{\text{lat}} \circ\Phi)\) denote the (state-action) coverability coefficient in \(M^{\star}_{\text{obs}}\) over the set of policies \(\Pi_{\text{lat}}\circ\Phi\). Note that by Lemma D.1 we have \(C_{\text{cov},\text{st}}(M^{\star}_{\text{obs}},\Pi_{\text{lat}}\circ\Phi)=C _{\text{cov},\text{st}}\) and therefore by Lemma D.4 we have \(C_{\text{cov}}(M^{\star}_{\text{obs}},\Pi_{\text{lat}}\circ\Phi)\leq C_{\text {cov},\text{st}}|\mathcal{A}|\). Let \(\eta>0\) be a parameter to be chosen later, and \(\beta_{\text{off}}=\mathcal{O}\bigl{(}\log\bigl{(}|\mathcal{M}_{\text{1at}}|| \mathcal{L}_{\text{1at}}||\Phi|HT\delta^{-1}\bigr{)}\bigr{)}\) be the offline estimation error guaranteed by Lemma H.1. We abbreviate \(\alpha\coloneqq\sqrt{C_{\text{cov}}H\log(T)}\), \(\mathbb{E}^{p^{(t)}}[\cdot]\coloneqq\mathbb{E}_{\pi^{(t)}\sim p^{(t)}}\, \mathbb{E}^{\pi^{(t)}}[\cdot]\), and \(\mathbb{E}^{\widehat{p}^{(t)}}\coloneqq\sum_{i=1}^{t-1}\mathbb{E}_{\pi^{(t)} \sim p^{(t)}}\,\mathbb{E}^{\pi^{(t)}}[\cdot]\). Then, we have:

\[\sum_{t=1}^{T}\sum_{h=1}^{H}\mathbb{E}^{p^{(t)}}\Big{[}[\Delta_{h }(\widehat{M}^{(t)},\widehat{\phi}^{(t)})](x_{h},a_{h})\Big{]}+\gamma^{-1} \Big{(}J^{M^{*}_{\text{lat}}}(\pi_{M^{*}_{\text{lat}}})-J^{\widehat{M}^{(t)}} (\pi_{\widehat{M}^{(t)}})\Big{)}\] \[\leq\alpha\sqrt{\sum_{t=1}^{T}\sum_{h=1}^{H}\mathbb{E}^{\widehat{ p}^{(t)}}\Big{[}[\Delta_{h}(\widehat{M}^{(t)},\widehat{\phi}^{(t)})](x_{h},a_{h}) \Big{]}}+\gamma^{-1}\sum_{t=1}^{T}\Bigl{(}J^{M^{*}_{\text{lat}}}(\pi_{M^{*}_{ \text{lat}}})-J^{\widehat{M}^{(t)}}(\pi_{\widehat{M}^{(t)}})\Big{)}\] \[\qquad+\mathcal{O}(HC_{\text{cov}})\]

where in the first inequality we have used Lemma C.7 with \(g^{(t)}_{h}=\Delta_{h}(\widehat{M}^{(t)},\widehat{\phi}^{(t)})\) and in the second inequality we have used the AM-GM inequality with parameter \(\eta\). Collecting terms, we proceed via:

\[=\frac{\alpha\eta}{2}\sum_{t=1}^{T}\biggl{(}\sum_{h=1}^{H}\mathbb{ E}^{\widehat{p}^{(t)}}\Big{[}\Delta_{h}(\widehat{M}^{(t)},\widehat{\phi}^{(t)})(x_ {h},a_{h})\Big{]}+(\frac{\gamma\eta\alpha}{2})^{-1}\Bigl{(}J^{M^{*}_{\text{ int}}}(\pi_{M^{*}_{\text{lat}}})-J^{\widehat{M}^{(t)}}(\pi_{\widehat{M}^{(t)}}) \Bigr{)}\biggr{)}\] \[\qquad+\frac{\alpha}{2\eta}+\mathcal{O}(HC_{\text{cov}})\] \[\leq\frac{\alpha\eta}{2}T\beta_{\text{off}}+\frac{\alpha}{2\eta}+ \mathcal{O}(HC_{\text{cov}})\] \[\leq\mathcal{O}\biggl{(}\sqrt{C_{\text{cov},\text{st}}|\mathcal{A} |H\log(T)T}\beta_{\text{off}}+HC_{\text{cov},\text{st}}|\mathcal{A}|\biggr{)}\] \[\leq\mathcal{O}\biggl{(}\sqrt{HC_{\text{cov},\text{st}}| \mathcal{A}|T\log(T)}\log\bigl{(}|\mathcal{M}_{\text{1at}}||\mathcal{L}_{ \text{1at}}||\Phi|HT\delta^{-1}\bigr{)}\biggr{)},\]

where in the first inequality we have used Lemma H.1 and the definition of \(\gamma\) in Algorithm 4 (cf. Eq. (59)) and in the second inequality we have chosen \(\eta=\nicefrac{{1}}{{\sqrt{T}}}\) to balance the terms and used the bound \(C_{\text{cov}}\leq C_{\text{cov},\text{st}}|\mathcal{A}|\). We convert to an expected regret bound by picking \(\delta\) appropriately, which gives the final result. It remains to show Lemma H.1. 

Proof of Lemma H.1.: Fix an iteration \(t\in[T]\), and abbreviate \(\widehat{M}\coloneqq\widehat{M}^{(t)}\) and \(\widehat{\phi}\coloneqq\widehat{\phi}^{(t)}\). We follow the analysis of maximum likelihood estimation from Geer; Zhang; Agarwal et al. [20, 21, 22]. In particular, we quote Lemma 24 of [1], which in an abstract conditional estimation framework with density class \(\mathcal{F}\) states the following.

**Lemma H.2** (Lemma 24 of Agarwal et al. [1]).: _Let \(D=\{(x_{i},y_{i})\}\) be a dataset collected with \(x_{i}\sim p^{(i)}(x_{1:i-1},y_{1:i-1})\) and \(y_{i}\sim f^{*}(\cdot\mid x_{i})\), \(L(f,D)=\sum_{i=1}^{n}\ell(f,(x_{i},y_{i}))\) be any loss function that decomposes additively, \(\widehat{f}:D\to\mathcal{F}\) be an estimator, \(D^{\prime}\) be a tangent sequence \(D^{\prime}=\{(\widetilde{x}_{i},\widetilde{y}_{i})\}\)sampled independently via \(\widetilde{x}_{i}\sim p^{{}_{(i)}}(x_{1:i-1},y_{1:i-1})\) and \(\widetilde{y}_{i}\sim f^{\star}(\cdot\mid\widetilde{x}_{i})\). Then, with probability at least \(1-\delta\), we have_

\[-\log\mathbb{E}_{D^{\prime}}\exp\Bigl{(}L(\widehat{f}(D),D^{\prime})\Bigr{)} \leq-L(\widehat{f}(D),D)+\log\bigl{(}|\mathcal{F}|\delta^{-1}\bigr{)},\] (62)

For our purposes, we have that \(\mathcal{F}=\mathcal{M}_{\text{1st}}\circ\Phi\), the data distribution is collected adaptively (for each \(h\in[H]\)) via \(\pi^{{}_{(i)}}\sim p^{{}_{(i)}}\), \(x_{h}^{{}_{(i)}},a_{h}^{{}_{(i)}}\sim d_{h}^{M_{\text{obs}}^{\star},\pi^{{}_{(i) }}}\), and \(r_{h}^{{}_{(i)}},x_{h+1}^{{}_{(i)}}\sim M_{\text{obs}}^{\star}(\cdot\mid x_{h} ^{{}_{(i)}},a_{h}^{{}_{(i)}})\). For the loss function \(L\), we take

\[L((M,\phi),D) =-\sum_{h=0}^{H}\sum_{i=1}^{t}\log\Biggl{(}\frac{M_{\text{obs}}^{ \star}(r_{h+1}^{{}_{(i)}},\phi_{h+1}(x_{h+1}^{{}_{(i)}})\mid x_{h}^{{}_{(i)}}, a_{h}^{{}_{(i)}})}{[M_{h}\circ\phi_{h}](r_{h}^{{}_{(i)}},\phi_{h+1}(x_{h+1}^{{}_{ (i)}})\mid x_{h}^{{}_{(i)}},a_{h}^{{}_{(i)}})}\Biggr{)}\] \[\quad-\frac{\gamma^{-1}}{2}(J^{M_{\text{obs}}^{\star}}(\pi_{M_{ \text{int}}^{\star}})-J^{M}(\pi_{M})).\]

We begin by upper bounding the quantity \(-L((\widehat{M},\widehat{\phi})(D),D)\) appearing on the right-hand side of Eq. (62), or equivalently lower bounding \(L((\widehat{M},\widehat{\phi})(D),D)\). Let us abbreviate \(\widehat{V}=J^{\widehat{M}}(\pi_{\widetilde{M}})\) and \(V^{\star}=J^{M_{\text{int}}^{\star}}(\pi_{M_{\text{int}}^{\star}})\). Towards this, note that

\[L((\widehat{M},\widehat{\phi})(D),D) =\sum_{h=0}^{H}\sum_{i=1}^{t}\log\Bigl{(}\Bigl{[}\widehat{M}_{h} \circ\widehat{\phi}_{h}\Bigr{]}(r_{h}^{{}_{(i)}},\widehat{\phi}_{h+1}(x_{h+1}^ {{}_{(i)}})\mid x_{h}^{{}_{(i)}},a_{h}^{{}_{(i)}})\Bigr{)}\] \[\quad-\sum_{h=0}^{H}\sum_{i=1}^{t}\log\Bigl{(}M_{\text{obs}}^{ \star}(r_{h}^{{}_{(i)}},\widehat{\phi}_{h+1}(x_{h+1}^{{}_{(i)}})\mid x_{h}^{{ }_{(i)}},a_{h}^{{}_{(i)}})\Bigr{)}+\frac{\gamma^{-1}}{2}(\widehat{V}-V^{\star})\] \[\quad\geq\sum_{h=0}^{H}\sum_{i=1}^{t}\log\Bigl{(}\Bigl{[}\widehat {M}_{h}\circ\widehat{\phi}_{h}\Bigr{]}(r_{h}^{{}_{(i)}},\widehat{\phi}_{h+1}(x _{h+1}^{{}_{(i)}})\mid x_{h}^{{}_{(i)}},a_{h}^{{}_{(i)}})\Bigr{)}\] \[\quad+\frac{\gamma^{-1}}{2}(\widehat{V}-V^{\star})\] \[\quad\geq\sum_{h=0}^{H}\sum_{i=1}^{t}\log\bigl{(}\bigl{[}M_{\text{ lat},h}^{\star}\circ\phi_{h}^{\star}\bigr{]}(r_{h}^{{}_{(i)}},\phi_{h+1}^{\star}(x_{h+1}^ {{}_{(i)}})\mid x_{h}^{{}_{(i)}},a_{h}^{{}_{(i)}})\bigr{)}\] \[\quad-\sum_{h=0}^{H}\max_{[M^{\prime}\circ\phi^{\prime}]\in \mathcal{L}_{\text{lat}}\circ\Phi}\sum_{i=1}^{t}\log\bigl{(}[M_{h}^{\prime} \circ\phi_{h}^{\prime}](r_{h}^{{}_{(i)}},\phi_{h+1}^{\star}(x_{h+1}^{{}_{(i)}}) \mid x_{h}^{{}_{(i)}},a_{h}^{{}_{(i)}})\bigr{)}\] \[\quad+\frac{\gamma^{-1}}{2}(V^{\star}-V^{\star})\] \[=\sum_{h=0}^{H}\sum_{i=1}^{t}\log\bigl{(}\bigl{[}M_{\text{ lat},h}^{\star}\circ\phi_{h}^{\star}\bigr{]}(r_{h}^{{}_{(i)}},\phi_{h+1}^{\star}(x_{h+1}^ {{}_{(i)}})\mid x_{h}^{{}_{(i)}},a_{h}^{{}_{(i)}})\bigr{)}\] \[\quad-\sum_{h=0}^{H}\max_{[M^{\prime}\circ\phi^{\prime}]\in \mathcal{L}_{\text{lat}}\circ\Phi}\sum_{i=1}^{t}\log\bigl{(}[M_{h}^{\prime} \circ\phi_{h}^{\prime}](r_{h}^{{}_{(i)}},\phi_{h+1}^{\star}(x_{h+1}^{{}_{(i)}}) \mid x_{h}^{{}_{(i)}},a_{h}^{{}_{(i)}})\bigr{)},\]

where in the second line we have used Lemma D.8 with Assumption A.2 and in the third line we have used the ERM property of \(\widehat{M}\circ\widehat{\phi}\) together with decoder and model realizability. We claim that this implies

\[L((\widehat{M},\widehat{\phi})(D),D)\geq-\log\bigl{(}|\mathcal{L}_{\text{1st}} \circ\Phi|H\delta^{-1}\bigr{)}\] (63)

by concentration. Indeed, for each \(h\in[H],i\in[t]\), and \([M^{\prime}\circ\phi^{\prime}]\in\mathcal{L}_{\text{1st}}\circ\Phi\), let

\[Z_{i,h}^{[M^{\prime}\circ\phi^{\prime}]}=-\frac{1}{2}\log\Biggl{(}\frac{M_{ \text{obs}}^{\star}(r_{h}^{{}_{(i)}},\phi_{h+1}^{{}_{(i)}})\mid x_{h}^{{}_{(i)}},a _{h}^{{}_{(i)}})}{[M^{\prime}\circ\phi^{\prime}](r_{h}^{{}_{(i)}},\phi_{h+1}^{ \star}(x_{h+1}^{{}_{(i)}})\mid x_{h}^{{}_{(i)}},a_{h}^{{}_{(i)}})}\Biggr{)}\]

[MISSING_PAGE_FAIL:59]

where we have used that in the "tangent sequence" \(D^{\prime}\) the current sample \((\widehat{r}_{h}^{(i)},\widehat{x}_{h+1}^{(i)})\) is independent of \((r_{h}^{(i)},x_{h+1}^{(i)})\). To bound this term, we again appeal to Lemma H.3, concluding that

\[-\sum_{h=1}^{H}\sum_{i=1}^{t}\log\mathbb{E}_{\pi^{(i)}\sim p^{(i)}} \,\mathbb{E}^{\pi^{(i)}}\Bigg{[}\exp\left(-\frac{1}{2}\log\Bigg{(}\frac{M_{ \text{obs}}^{*}(r_{h}^{(i)},\widehat{\phi}_{h+1}(x_{h+1}^{(i)})\mid x_{h}^{(i )},a_{h}^{(i)})}{\Big{(}\widehat{M}_{h}\circ\widehat{\phi}_{h}\Big{]}(r_{h}^{ (i)},\widehat{\phi}_{h+1}(x_{h+1}^{(i)})\mid x_{h}^{(i)},a_{h}^{(i)})}\right) \Bigg{)}\Bigg{]}\] \[\geq\frac{1}{2}\sum_{h=1}^{H}\sum_{i=1}^{t}\mathbb{E}_{\pi^{(i)} \sim p^{(i)}}\,\mathbb{E}^{\pi^{(i)}}\Big{[}D_{\mathsf{H}}^{2}\Big{(}[ \widehat{M}_{h}\circ\widehat{\phi}_{h}](x_{h},a_{h}),\widehat{\phi}_{h+1} \sharp M_{\text{obs}}^{*}(x_{h},a_{h})\Big{)}\Big{]}\]

Combining everything, we have:

\[\frac{1}{2}\Bigg{(}\sum_{h=1}^{H}\sum_{i=1}^{t}\mathbb{E}_{\pi^{(i )}\sim p^{(i)}}\,\mathbb{E}^{\pi^{(i)}}\Big{[}D_{\mathsf{H}}^{2}\Big{(}\Big{[} \widehat{M}_{h}\circ\widehat{\phi}_{h}\Big{]}(x_{h},a_{h}),\widehat{\phi}_{h+ 1}\sharp M_{\text{obs}}^{*}(x_{h},a_{h})\Big{)}\Big{]}+\gamma^{-1}(V^{*}- \widehat{V})\Bigg{)}\] \[\leq\log\bigl{(}|\mathcal{L}_{\mathsf{1at}}||\Phi|H\delta^{-1} \bigr{)}+\log\bigl{(}|\mathcal{M}_{\mathsf{1at}}||\Phi|\delta^{-1}\bigr{)}\]

Taking an additional union bound over \(t\in[T]\), we have that with probability at least \(1-\delta\):

\[\sum_{h=1}^{H}\sum_{i=1}^{t}\mathbb{E}_{\pi^{(i)}\sim p_{h}^{(i)} }\,\mathbb{E}^{\pi^{(i)}}\Big{[}D_{\mathsf{H}}^{2}\Big{(}\Big{[}\widehat{M}_{ h}\circ\widehat{\phi}_{h}\Big{]}(x_{h},a_{h}),\widehat{\phi}_{h+1}\sharp M_{ \text{obs}}^{*}(x_{h},a_{h})\Big{)}\Big{]}\] \[\qquad\qquad\qquad+\gamma^{-1}\Bigl{(}J^{M_{\mathsf{1at}}^{*}}( \pi_{M_{\mathsf{1at}}^{*}})-J^{M^{(t)}}(\pi_{M^{(t)}})\Big{)}\leq\mathcal{O} \bigl{(}\log\bigl{(}|\mathcal{M}_{\mathsf{1at}}||\mathcal{L}_{\mathsf{1at}}|| \Phi|HT\delta^{-1}\bigr{)}\bigr{)},\]

for all \(t\in[T]\), as desired.

**Corollary A.1** (Algorithmic modularity via SelfPredict.Opt).: _Under the same conditions as in Lemma A.1, and for any base algorithm \(\mathtt{Alg}_{\mathsf{1at}}\), \(\mathtt{O2L}\) with inputs \(T,K,\Phi,\mathtt{SelfPredict.Opt},\) and \(\mathtt{Alg}_{\mathsf{1at}}\) achieves_

\[\mathbb{E}[\mathtt{Risk}_{\text{obs}}(TK)]\lesssim c_{1}\cdot\mathtt{Risk}_{ \text{base}}(K)+c_{2}\gamma\cdot\frac{K}{\sqrt{T}}\sqrt{HC_{\text{cov,st}}| \mathcal{A}|}\log(|\mathcal{M}_{\mathsf{1at}}||\mathcal{L}_{\mathsf{1at}}|| \Phi|)+c_{3}\gamma^{-1}\cdot KH,\]

_for absolute constants \(c_{1},c_{2},c_{3}\). Consequently, for any \(\mathtt{Alg}_{\mathsf{1at}}\) with base risk \(\mathtt{Risk}_{\text{base}}(K)\), setting \(\gamma\) and \(T\) appropriately gives_

\[\mathbb{E}[\mathtt{Risk}_{\text{obs}}(TK)]\lesssim\mathtt{Risk}_{\text{base}}( K),\]

_with a number of trajectories \(TK=\widetilde{\mathcal{O}}(K^{5}H^{3}C_{\text{cov,st}}|\mathcal{A}|\log^{2}(| \mathcal{M}_{\mathsf{1at}}||\mathcal{L}_{\mathsf{1at}}||\Phi|)/(\mathtt{ Risk}_{\text{base}}(K))^{4})\)._

Proof of Corollary A.1.: The first inequality simply follows by plugging the bound of \(\mathtt{Est}_{\mathsf{self,opt}}\) from Lemma A.1 into Theorem A.1. For the second inequality, let \(\Delta=c_{2}\sqrt{HC_{\text{cov,st}}|\mathcal{A}|}\log(|\mathcal{M}_{\mathsf{1 at}}||\mathcal{L}_{\mathsf{1at}}||\Phi|)\). The result follows by setting \(\gamma\) s.t. \(c_{3}\gamma^{-1}HK=\mathtt{Risk}_{\text{base}}(K)\) i.e. \(\gamma=c_{3}\frac{KH}{\mathtt{Risk}_{\text{base}}(K)}\), and \(T\) such that \(\frac{\gamma K\Delta}{\sqrt{T}}=\mathtt{Risk}_{\text{base}}(K)\) i.e. \(T=\frac{K^{4}\Delta^{2}\gamma^{2}}{\mathtt{Risk}_{\text{base}}(K)^{2}}=\frac{K ^{4}\Delta^{2}H^{2}}{(\mathtt{Risk}_{\text{base}}(K))^{4}}\). Then the result follows by direct substitution and by noting that \(\frac{K}{T}\leq 1\) since \(\mathtt{Risk}_{\text{base}}(K)\leq 1\).

### Proofs for Main Risk Bound (Theorem A.1)

Our main risk bound (Theorem A.1) follows as a special case of a more general theorem (Theorem H.1), which holds for algorithm that satisfies a property we refer to as \(\mathsf{CorruptionRobust}\)-ness (Definition I.2). We now state the more general theorem, postponing its proof (and a formal definition of corruption robustness) until Appendix I.

**Theorem H.1** (Risk bound for O2L under self-predictive estimation and CorruptionRobustness).: _Assume \(\textsc{Rep}_{\mathrm{self};\mathrm{got}}\) satisfies Assumption A.1 with parameter \(\gamma>0\) and that \(\mathcal{M}_{\mathrm{1at}}\) is realizable (i.e. \(M_{\mathrm{1at}}^{\star}\in\mathcal{M}_{\mathrm{1at}}\)). Furthermore, let \(\textsc{Alg}_{\mathrm{1at}}\) be CorruptionRobust (Definition I.2) with parameter \(\alpha\). Then, O2L (Algorithm 1) with inputs \(T,K,\Phi,\textsc{Alg}_{\mathrm{1at}}\), and \(\textsc{Rep}_{\mathrm{self};\mathrm{got}}\) has expected risk_

\[\mathbb{E}[\mathtt{Risk}_{\mathrm{obs}}(TK)]\leq c_{1}\cdot\mathtt{Risk}_{ \mathrm{base}}(K)+c_{2}\gamma\cdot\frac{K}{T}\mathtt{Est}_{\mathrm{self}; \mathrm{opt}}(T,\gamma)+c_{3}\gamma^{-1}\cdot\big{(}\alpha^{2}+H\big{)}\] (66)

_for absolute constants \(c_{1},c_{2},c_{3}>0\)._

Our main risk bound (Theorem A.1) follows from the following lemma, which establishes that _any_\(\textsc{Alg}_{\mathrm{1at}}\) is CorruptionRobust in the sense of Definition I.2 for a sufficiently large corruption robustness parameter. Below, for any POMDP \(\widetilde{M}\) over state-action space \(\mathcal{S}\times\mathcal{A}\), we write \(\widetilde{M}(s_{1:h},a_{1:h})\) for the conditional probability over reward \(r_{h}\) and \(s_{h+1}\) given \(s_{1:h},a_{1:h}\), i.e. \(\widetilde{M}_{h}(s_{1:h},a_{1:h})=\widetilde{M}_{h}(r_{h},s_{h+1}=\cdot\mid s _{1:h},a_{1:h})\).

**Lemma H.4**.: _Let \(M^{\star}\) be any reference MDP and \(\widetilde{M}\) be any POMDP with the same state and action space. Then for any algorithm \(\textsc{Alg}_{\mathrm{1at}}\), we have_

\[\mathbb{E}^{\widetilde{M},\textsc{Alg}_{\mathrm{1at}}}[\mathtt{ Risk}_{M^{\star}}(K)] \leq c_{1}\,\mathbb{E}^{M^{\star},\textsc{Alg}_{\mathrm{1at}}}[ \mathtt{Risk}_{M^{\star}}(K)]\]

_where \(c_{1},c_{2}>0\) are absolute constants. In particular, \(\textsc{Alg}_{\mathrm{1at}}\) is CorruptionRobust (Definition I.2) with \(\alpha=c_{2}\sqrt{KH}\)._

Proof of Lemma H.4.: Let us abbreviate \(\textsc{Alg}:=\textsc{Alg}_{\mathrm{1at}}\). For \(i\in[K]\), let \(\tau^{(i)}\) denote the trajectory \((s_{1}^{(i)},a_{1}^{(i)},r_{1}^{(i)},\ldots,s_{H}^{(i)},a_{H}^{(i)},r_{H}^{(i)})\). Let \(\mathbb{P}\coloneqq\mathbb{P}^{M^{\star},\textsc{Alg}}\) denote the law of \(\{(\pi^{(i)},\tau^{(i)})\}_{i\in[K]}\) under Alg in the true MDP \(M^{\star}\), and \(\mathbb{Q}\coloneqq\mathbb{P}^{\widetilde{M},\textsc{Alg}}\) denote the law of \(\{(\pi^{(i)},\tau^{(i)})\}_{i\in[K]}\) under Alg under the POMDP \(\widetilde{M}\). Let us write \(M^{\star}(\pi)\) and \(\widetilde{M}(\pi)\) for the laws of trajectory \(\tau\) sampled from policy \(\pi\) in \(M^{\star}\) or \(\widetilde{M}\) respectively. Let \(\widehat{\pi}\) denote the policy output by the algorithm after \(K\) rounds of interaction with the environment. By Lemma C.5 we have

\[\mathbb{E}^{\widetilde{M},\textsc{Alg}}\Big{[}J^{M^{\star}}(\pi_{M^{\star}})- J^{M^{\star}}(\widehat{\pi})\Big{]}\leq 3\,\mathbb{E}^{M^{\star},\textsc{Alg}} \Big{[}J^{M^{\star}}(\pi_{M^{\star}})-J^{M^{\star}}(\widehat{\pi})\Big{]}+4D_ {\mathsf{H}}^{2}\Big{(}\mathbb{P}^{M^{\star},\textsc{Alg}},\mathbb{P}^{ \widetilde{M},\textsc{Alg}}\Big{)}.\]

By the subadditivity property for squared Hellinger distance (Lemma C.4) applied to the sequence \(\pi^{(1)},\tau^{(1)},\ldots,\pi^{(K)},\tau^{(K)}\), we have

\[D_{\mathsf{H}}^{2}\Big{(}\mathbb{P}^{M^{\star},\textsc{Alg}}, \mathbb{P}^{\widetilde{M},\textsc{Alg}}\Big{)} \leq 7\,\mathbb{E}^{\widetilde{M},\textsc{Alg}}\Bigg{[}\sum_{k=1}^{K}D_ {\mathsf{H}}^{2}(\mathbb{P}(\pi^{(k)}\mid\pi^{(1:h-1)},\tau^{(1:k-1)}), \mathbb{Q}(\pi^{(k)}\mid\pi^{(1:k-1)},\tau^{(1:h-1)}))+\] \[\qquad\qquad\qquad\qquad\qquad D_{\mathsf{H}}^{2}(\mathbb{P}(\tau ^{(k)}\mid\pi^{(1:k)},\tau^{(1:k-1)}),\mathbb{Q}(\tau^{(k)}\mid\pi^{(1:k)}, \tau^{(1:k-1)}))\Bigg{]}\] \[=7\,\mathbb{E}^{\widetilde{M},\textsc{Alg}}\Bigg{[}\sum_{k=1}^{K }D_{\mathsf{H}}^{2}\Big{(}M^{\star}(\pi^{(k)}),\widetilde{M}(\pi^{(k)})\Big{)} \Bigg{]}\] \[\leq 49\,\mathbb{E}^{\widetilde{M},\textsc{Alg}}\Bigg{[}\sum_{k=1 }^{K}\sum_{h=1}^{K}\mathbb{E}^{\widetilde{M},\pi^{(k)}}\Big{[}D_{\mathsf{H}}^{2} \Big{(}M_{h}^{\star}(s_{h},a_{h}),\widetilde{M}_{h}(s_{1:h},a_{1:h})\Big{)} \Big{]}\Bigg{]}\]

where in the second step we have used that \(\mathbb{P}(\pi^{(k)}\mid\pi^{(1:k)},\tau^{(1:k-1)})=\mathbb{Q}(\pi^{(k)}\mid\pi^ {(1:k)},\tau^{(1:k-1)})\) since the histories are equivalent, in the third step we have used that the trajectories are generated by the MDP/PODMP \(M^{\star}\) and \(\widetilde{M}\), respectively, in the fourth step we have again applied the subadditivity property of the squared Hellinger distance (Lemma C.4) to the sequence \((s_{1},a_{1},r_{1},\ldots,s_{H},a_{H},r_{H})\).

**Theorem A.1** (Risk bound for O2L under self-predictive estimation).: _Suppose \(\mathtt{Rep}_{\mathtt{Self};\mathtt{opt}}\) satisfies Assumption A.1 with parameter \(\gamma>0\). Then Algorithm 1, with inputs \(T,K,\Phi\), \(\mathtt{Rep}_{\mathtt{Self};\mathtt{opt}}\), and \(\mathtt{Alg}_{\mathtt{1st}}\) has expected risk_

\[\mathbb{E}[\mathtt{Risk}_{\mathtt{obs}}(TK)]\leq c_{1}\cdot\mathtt{Risk}_{ \mathtt{base}}(K)+c_{2}\gamma\cdot\frac{K}{T}\mathtt{Est}_{\mathtt{self}; \mathtt{opt}}(T,\gamma)+c_{3}\gamma^{-1}\cdot KH,\]

_for absolute constants \(c_{1},c_{2},c_{3}>0\)._

Proof of Theorem A.1.: This follows from Theorem H.1 as well as Lemma H.4, by taking \(\alpha=c_{2}\sqrt{KH}\) and simplifying.

Additional Results for Appendix A: Self-Predictive Estimation

This section contains a more general result for algorithmic modularity under self-predictive estimation (Theorem H.1), from which our main result is derived as a special case, along with associated background, applications, and proofs. This section is organized as follows.

* Appendix I.1 presents: definitions for the \(\phi\)-compressed POMDP and CorruptionRobust algorithms (Appendix I.1.1), statements for properties of the \(\phi\)-compressed dynamics (Appendix I.1.2). The risk bound for O2L under self-predictive estimation and CorruptionRobustness (Theorem H.1) is given in Appendix I.1.3, and a statement that the Golf algorithm is CorruptionRobust (Appendix I.1.4).
* Appendix I.2 presents for the proofs for the properties of the \(\phi\)-compressed POMDPs.
* Appendix I.3 presents a proof for the risk bound of O2L under self-predictive estimation and CorruptionRobustness.
* Appendix I.4 presents a proof that the Golf algorithm is CorruptionRobust.

### O2L with Self-predictive Estimation and CorruptionRobust Base Algorithms

#### i.1.1 Definitions: \(\phi\)-compressed POMDP and CorruptionRobustness

Consider iteration \(k\in[K]\) of epoch \(t\in[T]\) within O2L. Suppose that RepLearn has chosen decoder \(\phi=\phi^{(t)}:\mathcal{X}\rightarrow\mathcal{S}\). Then, the latent algorithm has observed the data \(\mathcal{D}^{(t,k)}=\{\phi(x_{h}^{(t,k)},\ldots,x_{h}^{(t,k)},r_{h}^{(t,k)}, \phi(x_{h+1}^{(t,k)})\}\) collected from the preceding policies in the epoch: \(\pi_{\mathrm{lat}}^{(t,1)}\circ\phi^{(t)},\ldots,\pi_{\mathrm{lat}}^{(t,k-1)} \circ\phi^{(t)}\) (Line 8). Due to possible inaccuracies in the decoder \(\phi\), the dataset \(\mathcal{D}^{(t,k)}\) may not be generated from a Markovian process and must instead be viewed as being generated from a POMDP, formally defined as follows.

**Definition I.1** (\(\phi\)-compressed POMDP).: _The \(\phi\)-compressed POMDP \(\widetilde{M}_{\phi}^{\star}\) induced by \(M_{\mathrm{obs}}^{\star}\) and \(\phi\) is defined by:_

1. _Latent state space_ \(\mathcal{X}\)__
2. _Action space_ \(\mathcal{A}\)__
3. _Observation state space_ \(\mathcal{S}\)__
4. _Latent reward functions_ \(R_{\mathrm{obs},h}^{\star}:\mathcal{X}\times\mathcal{A}\rightarrow[0,1]\)__
5. _Latent dynamics_ \(P_{\mathrm{obs},h}^{\star}:\mathcal{X}\times\mathcal{A}\rightarrow\Delta( \mathcal{X})\)__
6. _(Deterministic) observation function_ \(\mathcal{O}_{h}:\mathcal{X}\rightarrow\mathcal{S}\) _defined by_ \(\mathcal{O}_{h}(x)=\phi_{h}(x)\)_,_
7. _Horizon_ \(H\)__
8. _Initial latent distribution_ \(P_{\mathrm{obs}}^{\star}(x_{0}\mid\emptyset)\)__

Note that the latent space _for the POMDP_ is the observation space of the latent-dynamics MDP \(M_{\mathrm{obs}}^{\star}\), and vice-versa; we adopt this terminology because--from the perspective of the base algorithm, the observations \(x_{h}\) can be viewed as a Markovian (yet partially observed process) that generates the learned states \(\phi(x_{h})\) on which the algorithm acts. We write \(\widetilde{P}_{\phi}^{\pi_{\mathrm{lat}}}\coloneqq\mathbb{P}^{\widetilde{M}_ {\phi}^{\star},\pi_{\mathrm{lat}}}\) for the probability distribution over trajectories \((x_{h},s_{h},a_{h},r_{h})_{h\in[H]}\) in the \(\phi\)-compressed POMDP when playing policy \(\pi_{\mathrm{lat}}:\mathcal{S}\times[H]\rightarrow\Delta(\mathcal{A})\), where \(x_{h}\in\mathcal{X}\) are the POMDP's latent states, \(s_{h}\in\mathcal{S}\) are the observed states, and \(a_{h}\in\mathcal{A}\) are the actions. We let \(\widetilde{\widetilde{\mathcal{E}}}_{\phi}^{\pi_{\mathrm{lat}}}\coloneqq \mathbb{E}^{\widetilde{M}_{\phi}^{\star},\pi_{\mathrm{lat}}}\) denote the corresponding expectation. We write \(\widetilde{P}_{\phi,h}(s_{h+1}\mid s_{1:h},a_{1:h})=\widetilde{P}_{\phi}^{\pi_ {\mathrm{lat}}}(s_{h+1}\mid s_{1:h},a_{1:h})\) and \(\widetilde{r}_{\phi,h}(r_{h}\mid s_{1:h},a_{1:h})=\widetilde{P}_{\phi}^{\pi_{ \mathrm{lat}}}(r_{h}\mid s_{1:h},a_{1:h})\) for the conditional distributions of next states and rewards given the first \(h\) state-action pairs, which are policy-independent. We also write \(\widetilde{M}_{\phi}^{\star}(r_{h},s_{h+1}\mid s_{1:h},a_{1:h})=\widetilde{r}_{ \phi,h}(r_{h}\mid s_{1:h},a_{1:h})\widetilde{P}_{\phi,h}(s_{h+1}\mid s_{1:h},a _{1:h})\) for the joint one-step probability. We will abbreviate \(\widetilde{M}_{\phi}^{\star}(s_{1:h},a_{1:h})\coloneqq\widetilde{M}_{\phi}^{ \star}(r_{h},s_{h+1}=\cdot\mid s_{1:h},a_{1:h})\).

Note that for any \(\pi_{\mathrm{lat}}\), \(\widetilde{P}^{\pi_{\mathrm{lat}}}_{\phi,h}(s_{h+1}\mid s_{h},a_{h})\) is a well-defined (Markovian, policy-dependent) probability kernel, which is equivalent to

\[\widetilde{P}^{\pi_{\mathrm{lat}}}_{\phi,h}(s_{h+1}\mid s_{h},a_{h}) =\sum_{s_{1:h-1},a_{1:h-1}}\widetilde{P}^{\pi_{\mathrm{lat}}}_{ \phi,h}(s_{1:h-1},a_{1:h-1}\mid s_{h},a_{h})\widetilde{P}_{\phi,h}(s_{h+1}\mid s _{1:h},a_{1:h})\] (67) \[=\widetilde{\mathbb{E}}^{\pi_{\mathrm{lat}}}_{\phi}\Big{[} \widetilde{P}_{\phi,h}(s_{h+1}\mid s_{1:h},a_{1:h})\mid s_{h},a_{h}\Big{]}\] (68)

Similarly, \(\widetilde{r}^{\pi_{\mathrm{lat}}}_{\phi,h}(r_{h}\mid s_{h},a_{h})\) is a Markovian and policy-dependent reward distribution which is equivalent to

\[\widetilde{r}^{\pi_{\mathrm{lat}}}_{\phi,h}(r_{h}\mid s_{h},a_{h}) =\sum_{s_{1:h-1},a_{1:h-1}}\widetilde{P}^{\pi_{\mathrm{lat}}}_{ \phi,h}(s_{1:h-1},a_{1:h-1}\mid s_{h},a_{h})\tilde{r}_{\phi,h}(r_{h}\mid s_{1: h},a_{1:h})\] (69) \[=\widetilde{\mathbb{E}}^{\pi_{\mathrm{lat}}}_{\phi}[\widetilde{r} _{\phi,h}(r_{h}\mid s_{1:h},a_{1:h})\mid s_{h},a_{h}].\] (70)

Finally, we let

\[\widetilde{M}^{\pi_{\mathrm{lat}},\star}_{\phi,h}(r_{h},s_{h+1}\mid s_{h},a_{h })=\widetilde{\mathbb{E}}^{\pi_{\mathrm{lat}}}_{\phi}\Big{[}\widetilde{M}^{ \star}_{\phi}(r_{h},s_{h+1}\mid s_{1:h},a_{1:h})\mid s_{h},a_{h}\Big{]}\] (71)

denote the associated one-step model over joint rewards and transitions.

Our CorruptionRobustness condition asserts that the agent--when observing data from the \(\phi^{(\textsc{i})}\)-compressed dynamics \(\widetilde{M}^{\star}_{\phi}\)--attains a risk bound for \(M_{\mathrm{1at}}\) which is proportional to its risk when observing data from \(M_{\mathrm{1at}}\) itself, plus a term that captures the degree of misspecification between \(\widetilde{M}^{\star}_{\phi}\) and \(M_{\mathrm{1at}}\).

**Definition I.2** (CorruptionRobust algorithm).: _We say that \(\mathtt{Alg}_{\mathrm{lat}}\) is_ CorruptionRobust _with parameters \(\alpha\) and \(\mathtt{Risk}_{\mathrm{base}}\) if there exists a constant \(c_{1}\) such that, for any \((\phi,M_{\mathrm{1at}})\in\Phi\times\mathcal{M}_{\mathrm{1at}}\), we have_

\[\mathbb{E}^{\widetilde{M}^{\star}_{\phi},\,\textsc{ALG}_{ \mathrm{lat}}}[\mathtt{Risk}(K,\textsc{Alg}_{\mathrm{lat}},M_{\mathrm{1at}})] \leq c_{1}\cdot\mathtt{Risk}_{\mathrm{base}}(K)\] \[\quad\quad\quad+\alpha\,\mathbb{E}^{\widetilde{M}^{\star}_{\phi}, \textsc{ALG}_{\mathrm{lat}}}\Bigg{[}\sqrt{\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E }_{\pi^{(\textsc{lat})}_{\mathrm{lat}}\sim p^{(k)}}}\,\widetilde{\mathbb{E}}^{ \pi^{(k)}_{\phi}}_{\phi}\Big{[}D_{\mathsf{H}}^{2}\Big{(}M_{\mathrm{1at},h}(s_{h },a_{h}),\widetilde{M}^{\star}_{\phi,h}(s_{1:h},a_{1:h})\Big{)}\Big{]}\Bigg{]},\]

_where we recall the definition of the random variable \(\mathtt{Risk}(K,\textsc{Alg}_{\mathrm{1at}},M_{\mathrm{1at}})\) from Eq. (1), the expectation \(\mathbb{E}^{\widetilde{M}^{\star}_{\phi},\textsc{ALG}_{\mathrm{1at}}}\) denotes the interaction protocol of \(\mathtt{Alg}_{\mathrm{lat}}\) in the \(\phi\)-compressed dynamics \(\widetilde{M}^{\star}_{\phi}\), and \(p^{(\textsc{k})}\) denotes the randomization distribution over latent policies that \(\mathtt{Alg}_{\mathrm{1at}}\) plays._

#### i.1.2 Basic properties of the \(\phi\)-compressed dynamics (Definition I.1)

We establish a number of basic properties for the \(\phi\)-compressed POMDP and their relation to the self-prediction guarantee obtained by \(\mathtt{Rep}_{\mathtt{self};\mathtt{opt}}\). These properties are proved in Appendix I.2. Firstly, we have the following change-of-measure lemma:

**Lemma I.1** (Change of measure lemma).: _For any \(\phi\in\Phi\), \(f\in[\mathcal{S}\times\mathcal{A}\to[0,1]]\), \(h\in[H]\), and \(\pi_{\mathrm{1at}}\in[\mathcal{S}\times[H]\to\Delta(\mathcal{A})]\), we have:_

\[\widetilde{\mathbb{E}}^{\pi_{\mathrm{lat}}}_{\phi}[f(s_{h},a_{h})]=\mathbb{E}^ {\pi_{\mathrm{lat}}\circ\phi}[[f\circ\phi](x_{h},a_{h})].\] (72)

The next lemma states that the kernels of the \(\phi\)-compressed POMDP are well-approximated by the (Markovian) latent model fit by \(\mathtt{Rep}_{\mathtt{self};\mathtt{opt}}\). We recall the instantaneous self-prediction error

\[[\Delta_{h}(M_{\mathrm{1at}},\phi)](x_{h},a_{h})\coloneqq D_{\mathsf{H}}^{2} \big{(}M_{\mathrm{1at},h}(\phi_{h}(x_{h}),a_{h}),\big{[}\phi_{h+1}\sharp M^{ \star}_{\mathrm{obs},h}\big{]}(x_{h},a_{h})\big{)}.\]

**Lemma I.2** (Near-markovianity of the \(\phi\)-compressed dynamics).: _For any decoder \(\phi\), base model \(M_{\mathrm{1at}}\), and policy \(\pi_{\mathrm{1at}}:\mathcal{S}\times[H]\to\Delta(\mathcal{A})\), we have:_

\[\sum_{h=0}^{H}\widetilde{\mathbb{E}}^{\pi_{\mathrm{lat}}}_{\phi}\Big{[}D_{ \mathsf{H}}^{2}\Big{(}M_{\mathrm{1at},h}(s_{h},a_{h}),\widetilde{M}^{\star}_{ \phi,h}(s_{1:h},a_{1:h})\Big{)}\Big{]}\leq\sum_{h=0}^{H}\mathbb{E}^{\pi_{ \mathrm{1at}}\circ\phi}[[\Delta_{h}(M_{\mathrm{1at}},\phi)](x_{h},a_{h})].\] (73)_Furthermore, we also have_

\[\sum_{h=0}^{H}\widetilde{\mathbb{E}}_{\phi}^{\pi_{\mathrm{lat}}}\Big{[}D_{\mathrm{ H}}^{2}\Big{(}M_{\mathrm{1at},h}(s_{h},a_{h}),\widetilde{M}_{\phi,h}^{\star,\pi_{ \mathrm{lat}}}(s_{h},a_{h})\Big{)}\Big{]}\leq\sum_{h=0}^{H}\mathbb{E}^{\pi_{ \mathrm{lat}}\circ\phi}[[\Delta_{h}(M_{\mathrm{1at}},\phi)](x_{h},a_{h})].\] (74)

A corollary is the following lemma establishing errors between expectations under \(M_{\mathrm{1at}}\), the model estimated by \(\textsc{Rep}_{\mathrm{self};\mathrm{opt}}\), and those under the \(\phi\)-compressed POMDP \(\widetilde{M}_{\phi}^{\star}\).

**Lemma I.3** (Simulation lemma).: _For any latent model \(M_{\mathrm{1at}}\) with Markovian transition kernel \(\{P_{\mathrm{1at},h}\}_{h\in[H]}\), latent policy \(\pi_{\mathrm{1at}}:\mathcal{S}\times[H]\to\Delta(\mathcal{A})\), and decoder \(\phi\in\Phi\), we have that for all \(f:\mathcal{S}\times\mathcal{A}\to[0,1]\):_

\[|\mathbb{E}^{M_{\mathrm{1at}},\pi_{\mathrm{lat}}}[f(s_{h},a_{h})] -\widetilde{\mathbb{E}}_{\phi}^{\pi_{\mathrm{lat}}}[f(s_{h},a_{h})]|\] \[\qquad\leq\sum_{h^{\prime}<h}\mathbb{E}^{\pi_{\mathrm{lat}}\circ \phi}\Big{[}\big{[}\big{[}P_{\mathrm{1at}}\circ\phi\big{]}_{h}(x_{h^{\prime}}, a_{h^{\prime}})-\phi_{h+1}\sharp P_{\mathrm{obs},h}^{\star}(x_{h^{\prime}},a_{h^{ \prime}})\big{\|}_{\mathrm{tv}}\Big{]},\] (75)

_and thus for any sequence of policies \(\pi_{\mathrm{1at}}^{(\prime)}\), latent models \(M_{\mathrm{1at}}^{(\prime)}\), and decoders \(\phi^{(t)}\), we have:_

\[\sum_{t=1}^{T}\sum_{h=0}^{H}\mathbb{E}^{M_{\mathrm{1at}}^{(\prime )},\pi_{\mathrm{lat}}^{(t)}}[f(s_{h},a_{h})]-\widetilde{\mathbb{E}}_{\phi^{(t) }}^{\pi_{\mathrm{lat}}^{(t)}}[f(s_{h},a_{h})]|\] (76) \[\qquad\leq H\sqrt{TH}\sqrt{\sum_{t=1}^{T}\sum_{h=0}^{H}\mathbb{E }^{\pi_{\mathrm{lat}}^{(t)}\circ\phi^{(t)}}\big{[}[\Delta_{h}(M_{\mathrm{1at}}^ {(\prime)},\phi^{(t)})](x_{h},a_{h})\big{]}}.\] (77)

#### i.1.3 Risk bound for O2L under CorruptionRobustness

We state the main risk bound for O2L under self-predictive estimation and the above definition of corruption robustness.

**Theorem H.1** (Risk bound for O2L under self-predictive estimation and CorruptionRobustness).: _Assume \(\textsc{Rep}_{\mathrm{self};\mathrm{opt}}\) satisfies Assumption A.1 with parameter \(\gamma>0\) and that \(\mathcal{M}_{\mathrm{1at}}\) is realizable (i.e. \(M_{\mathrm{1at}}^{\star}\in\mathcal{M}_{\mathrm{1at}}\)). Furthermore, let \(\textsc{Alg}_{\mathrm{1at}}\) be CorruptionRobust (Definition I.2) with parameter \(\alpha\). Then, O2L (Algorithm 1) with inputs \(T,K,\Phi,\textsc{Alg}_{\mathrm{1at}}\), and \(\textsc{Rep}_{\mathrm{self};\mathrm{opt}}\) has expected risk_

\[\mathbb{E}[\textsc{Risk}_{\mathrm{obs}}(TK)]\leq c_{1}\cdot\textsc{Risk}_{ \mathrm{base}}(K)+c_{2}\gamma\cdot\frac{K}{T}\textsc{Est}_{\mathrm{self}; \mathrm{opt}}(T,\gamma)+c_{3}\gamma^{-1}\cdot\big{(}\alpha^{2}+H\big{)}\] (66)

_for absolute constants \(c_{1},c_{2},c_{3}>0\)._

#### i.1.4 Examples of CorruptionRobust algorithms

In this section, we establish that the Golf algorithm satisfies the CorruptionRobust definition (Definition I.2) with a parameter \(\alpha\approx K^{-1/2}\). This improves upon the rate that would be obtained by invoking the generic guarantee in Lemma H.4. We expect that several other algorithms can be analyzed in a similar way, thereby leading to tight rates in the same fashion. We restate the pseudocode in Algorithm 5 for convenience.

Let \(M_{\mathrm{1at}}=(r_{\mathrm{1at}},P_{\mathrm{1at}})\) be given, and we let \(Q_{\mathrm{1at}}^{\star}\coloneqq Q^{M_{\mathrm{lat}},\star}\), and \(\mathcal{T}_{\mathrm{1at},h}f(s,a)\coloneqq r_{\mathrm{1at},h}(s,a)+\mathbb{E}_ {s^{\prime}\sim P_{\mathrm{1at},h}(s,a)}[V_{f}(s^{\prime})]\). We assume that the algorithm has a latent function class \(\mathcal{F}_{\mathrm{alg}}\) which realizes \(Q_{\mathrm{1at}}^{\star}\), as well as a helper class \(\mathcal{G}_{\mathrm{alg}}\) which is \(\mathcal{T}_{\mathrm{1at}}\)-complete for \(\mathcal{F}_{\mathrm{alg}}\).

**Assumption I.1** (\(\mathcal{T}_{\mathrm{1at}}\)-completeness).: _We have:_

\[Q_{\mathrm{1at}}^{\star}\in\mathcal{F}_{\mathrm{alg}},\quad\text{and}\quad \mathcal{T}_{\mathrm{1at}}\mathcal{F}_{\mathrm{alg}}\subseteq\mathcal{G}_{ \mathrm{alg}}.\]

For our analysis of Golf, it is most natural to quantify the corruption levels in the following way.

**Assumption I.2** (Corruption levels of \(M_{\mathrm{1at}}\) and \(\widetilde{M}_{\phi}^{\star}\)).: _Let \(\varepsilon_{\mathsf{rep}}^{2}\) be such that, for any sequence of policies \(\pi_{\mathrm{1at}}^{(k)}\) played by the algorithm when interacting with the \(\phi\)-compressed POMDP, we have_

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi_{\mathrm{lat}}^{(k)} \sim p_{\mathrm{lat}}^{(k)}}\widetilde{\mathbb{E}}_{\phi}^{\pi_{\mathrm{lat}}^{(k) }}\bigg{[}(r_{\mathrm{1at},h}(s_{h},a_{h})-\widetilde{r}_{\phi,h}^{\pi_{\mathrm{ old}}^{(k)}}(s_{h},a_{h}))^{2}+\Big{\|}P_{\mathrm{1at},h}(s_{h},a_{h})- \widetilde{P}_{\phi,h}^{\pi_{\mathrm{old}}^{(k)}}(s_{h},a_{h})\Big{\|}_{ \mathrm{tv}}^{2}\bigg{]}\] \[\qquad\qquad\leq\varepsilon_{\mathsf{rep}}^{2}.\] (78)

[MISSING_PAGE_FAIL:66]

Let \(\tilde{d}_{h}^{\pi_{\text{lat}}}(s,a)=\widetilde{P}_{\phi}^{\pi_{\text{lat}}}(S_{h} =s,A_{h}=a)\) be the marginalized occupancy measure for in the \(\phi\)-compressed POMDP \(\widetilde{M}_{\phi}^{\star}\). We write \(d_{h}^{\pi_{\text{lat}}\circ\phi}\coloneqq d_{h}^{M_{\text{obs}},\pi_{\text{lat} }\circ\phi}\). The left-hand side in Eq. (72) is equal to:

\[\widetilde{\mathbb{E}}_{\phi}^{\pi_{\text{lat}}}[f(s_{h},a_{h})]=\sum_{s\in \mathcal{S},a\in\mathcal{A}}\tilde{d}_{h}^{\pi_{\text{lat}}}(s,a)f(s,a),\]

Meanwhile, the right-hand side is equal to:

\[\mathbb{E}_{h}^{\pi_{\text{lat}}\circ\phi}[[f\circ\phi](x_{h},a_{h})]=\sum_{s \in\mathcal{S},a\in\mathcal{A}}f(s,a)\sum_{x:\phi(x)=s}d_{h}^{\pi_{\text{lat} }\circ\phi}(x,a).\]

So it only remains to show that, for each \(s\in\mathcal{S}\) and \(a\in\mathcal{A}\), we have \(\tilde{d}_{h}^{\pi_{\text{lat}}}(s,a)=\sum_{x:\phi(x)=s}d_{h}^{\pi_{\text{lat }}\circ\phi}(x,a)\). Firstly, note that it is enough to show that \(\sum_{x_{h}:\phi(x_{h})=s_{h}}d_{h}^{\pi_{\text{lat}}\circ\phi}(x_{h})=\tilde {d}_{h}^{\pi_{\text{lat}}}(s_{h})\), since \(\tilde{d}_{h}^{\pi_{\text{lat}}}(s_{h},a_{h})=\tilde{d}_{h}^{\pi_{\text{lat} }}(s_{h})\pi_{\text{lat}}(a_{h}\mid s_{h})\) and \(\sum_{x_{h}:\phi(x_{h})=s_{h}}d_{h}^{\pi_{\text{lat}}\circ\phi}(x_{h},a_{h})= \sum_{x_{h}:\phi(x_{h})=s_{h}}d_{h}^{\pi_{\text{lat}}\circ\phi}(x_{h})\pi_{ \text{lat}}(a_{h}\mid\phi(x_{h}))=\pi_{\text{lat}}(a_{h}\mid s_{h})\sum_{x_{h}: \phi(x_{h})=s_{h}}d_{h}^{\pi_{\text{lat}}\circ\phi}(x_{h})\). Toward this, we have:

\[\sum_{x_{h}:\phi(x_{h})=s_{h}}d_{h}^{\pi_{\text{lat}}\circ\phi}(x_ {h}) =\sum_{x_{h}:\phi(x_{h})=s_{h}}\sum_{x_{h-1},a_{h-1}\in\mathcal{X }\times\mathcal{A}}d_{h-1}^{\pi_{\text{lat}}\circ\phi}(x_{h-1},a_{h-1})P_{ \text{obs},h}^{\star}(x_{h}\mid x_{h-1},a_{h-1})\] \[=\sum_{x_{h-1},a_{h-1}\in\mathcal{X}\times\mathcal{A}}d_{h-1}^{ \pi_{\text{lat}}\circ\phi}(x_{h-1},a_{h-1})\sum_{x_{h}:\phi(x_{h})=s_{h}}P_{ \text{obs},h}^{\star}(x_{h}\mid x_{h-1},a_{h-1})\] \[=\sum_{x_{h-1},a_{h-1}\in\mathcal{X}\times\mathcal{A}}d_{h-1}^{ \pi_{\text{lat}}\circ\phi}(x_{h-1},a_{h-1})P_{\text{obs},h}^{\star}(\phi(x_{h}) =s_{h}\mid x_{h-1},a_{h-1})\]

At the same time,

\[\tilde{d}_{h}^{\pi_{\text{lat}}}(s_{h}) =\widetilde{P}_{\phi}^{\pi_{\text{lat}}}(S_{h}=s_{h})\] \[=\sum_{\widetilde{x},\widetilde{a}}\widetilde{P}_{\phi}^{\pi_{ \text{lat}}}(X_{h-1}=\widetilde{x},A_{h-1}=\widetilde{a})P_{\phi}^{\pi_{\text{lat }}}(S_{h}=s_{h}\mid X_{h-1}=\widetilde{x},A_{h-1}=\widetilde{a})\] \[=\sum_{\widetilde{x},\widetilde{a}}\widetilde{P}_{\phi}^{\pi_{ \text{lat}}}(X_{h-1}=\widetilde{x},A_{h-1}=\widetilde{a})P_{\text{obs}}^{\star} (\phi(x_{h})=s_{h}\mid x_{h-1},a_{h-1}),\]

where in the second equality we have used the definition of the observation function \(s_{h}=\mathcal{O}(x_{h})=\phi(x_{h})\).

To conclude, it remains to show that for all \(h\), we have:

\[d_{h}^{\pi_{\text{lat}}\circ\phi}(x_{h},a_{h})=\widetilde{P}_{\phi}^{\pi_{\text{lat }}}(X_{h}=x_{h},A_{h}=a_{h}).\]

We do this by induction. Again, note that it is sufficient to establish \(d_{h}^{\pi_{\text{lat}}\circ\phi}(x_{h})=\widetilde{P}_{\phi}^{\pi_{\text{lat}}} (X_{h}=x_{h})\). The case \(h=1\) is clear. For the general case, we have:

\[d_{h}^{\pi_{\text{lat}}\circ\phi}(x_{h}) =\sum_{x_{h-1},a_{h-1}\in\mathcal{X}\times\mathcal{A}}d_{h-1}^{ \pi_{\text{lat}}\circ\phi}(x_{h-1},a_{h-1})P_{\text{obs}}^{\star}(x_{h}\mid x_{h -1},a_{h-1})\] \[=\sum_{x_{h-1},a_{h-1}\in\mathcal{X}\times\mathcal{A}}\widetilde{P} _{\phi}^{\pi_{\text{lat}}}(X_{h}=x_{h-1},A_{h-1}=a_{h-1})P_{\text{obs}}^{\star}(x_{ h}\mid x_{h-1},a_{h-1})\] \[=\sum_{x_{h-1},a_{h-1}\in\mathcal{X}\times\mathcal{A}}\widetilde{P} _{\phi}^{\pi_{\text{lat}}}(X_{h}=x_{h-1},A_{h-1}=a_{h-1})\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\times\widetilde{P}_{\phi}^{\pi_{\text{lat}}}(X_{h}=x_{h}\mid X_{h-1}=x_{h -1},A_{h-1}=a_{h-1})\] \[=\widetilde{P}_{\phi}^{\pi_{\text{lat}}}(X_{h}=x_{h}).\]

**Lemma I.2** (Near-markovianity of the \(\phi\)-compressed dynamics).: _For any decoder \(\phi\), base model \(M_{\mathrm{1at}}\), and policy \(\pi_{\mathrm{1at}}:\mathcal{S}\times[H]\to\Delta(\mathcal{A})\), we have:_

\[\sum_{h=0}^{H}\widetilde{\mathbb{E}}_{\phi}^{\pi_{\mathrm{1at}}}\Big{[}D_{ \mathsf{H}}^{2}\Big{(}M_{\mathrm{1at},h}(s_{h},a_{h}),\widetilde{M}_{\phi,h}^{ \star}(s_{1:h},a_{1:h})\Big{)}\Big{]}\leq\sum_{h=0}^{H}\mathbb{E}^{\pi_{ \mathrm{1at}}\circ\phi}[[\Delta_{h}(M_{\mathrm{1at}},\phi)](x_{h},a_{h})].\] (73)

_Furthermore, we also have_

\[\sum_{h=0}^{H}\widetilde{\mathbb{E}}_{\phi}^{\pi_{\mathrm{1at}}}\Big{[}D_{ \mathsf{H}}^{2}\Big{(}M_{\mathrm{1at},h}(s_{h},a_{h}),\widetilde{M}_{\phi,h}^ {\star,\pi_{\mathrm{1at}}}(s_{h},a_{h})\Big{)}\Big{]}\leq\sum_{h=0}^{H} \mathbb{E}^{\pi_{\mathrm{1at}}\circ\phi}[[\Delta_{h}(M_{\mathrm{1at}},\phi)](x _{h},a_{h})].\] (74)

**Proof of Lemma I.2.** We begin with the first event. Note that, for any \(\pi_{\mathrm{1at}}\), the PODMP kernel \(\widetilde{M}_{\phi,h}^{\star}(r_{h},s_{h+1}=\cdot\mid s_{1:h},a_{1:h})\) can be written as:

\[\widetilde{M}_{\phi,h}^{\star}(r_{h},s_{h+1}=\cdot\mid s_{1:h},a_{ 1:h})=\sum_{x_{h},a_{h}\in\mathcal{A}^{\star}\times\mathcal{A}}\widetilde{P}_ {\phi}^{\pi_{\mathrm{1at}}}(r_{h},s_{h+1}=\cdot\mid x_{h},a_{h},s_{1:h},a_{1:h})\] \[\times\widetilde{P}_{\phi}^{\pi_{\mathrm{1at}}}(x_{h},a_{h}\mid s _{1:h},a_{1:h})\] \[=\sum_{x_{h},a_{h}\in\mathcal{A}^{\star}\times\mathcal{A}} \widetilde{P}_{\phi}^{\pi_{\mathrm{1at}}}(r_{h},s_{h+1}=\cdot\mid x_{h},a_{h}) \widetilde{P}_{\phi}^{\pi_{\mathrm{1at}}}(x_{h},a_{h}\mid s_{1:h},a_{1:h}),\]

where we have used \(\widetilde{M}(r_{h},s_{h+1}=\cdot\mid s_{1:h},a_{1:h})=\widetilde{P}_{\phi}^{ \pi_{\mathrm{1at}}}(r_{h},s_{h+1}=\cdot\mid s_{1:h},a_{1:h})\), the law of total probability, and that \(x_{h},a_{h}\) is a sufficient statistic for \(r_{h}\) and \(s_{h+1}\). We further note that

\[\widetilde{P}_{\phi}^{\pi_{\mathrm{1at}}}(r_{h},s_{h+1}=\cdot\mid x_{h},a_{h}) =M_{\mathsf{obs},h}^{\star}(r_{h},\phi_{h+1}(x_{h+1})=\cdot\mid x_{h},a_{h}),\] (79)

since \(s_{h+1}=\mathcal{O}_{h+1}(x_{h+1})=\phi_{h+1}(x_{h+1})\) is a deterministic function of \(x_{h+1}\) and \(r_{h},x_{h+1}\sim M_{\mathsf{obs},h}^{\star}(x_{h},a_{h})\). Thus, for a fixed \(h\) and \(t\), and omitting the \(h\) indices on the decoder \(\phi\) for cleanliness, the expectation in equation Eq. (73) becomes:

\[\widetilde{\mathbb{E}}_{\phi}^{\pi_{\mathrm{1at}}}\Big{[}D_{ \mathsf{H}}^{2}\Big{(}M_{\mathrm{1at},h}(s_{h},a_{h}),\widetilde{M}_{\phi,h}^{ \star}(r_{h},s_{h+1}=\cdot\mid s_{1:h},a_{1:h})\Big{)}\Big{]}\] \[\leq\sum_{s_{1:h},a_{1:h}\in(\mathcal{S}\times\mathcal{A})^{h}} \widetilde{P}_{\phi}^{\pi_{\mathrm{1at}}}(s_{1:h},a_{1:h})\sum_{x_{h},a_{h}} \widetilde{P}_{\phi}^{\pi_{\mathrm{1at}}}(x_{h},a_{h}\mid s_{1:h},a_{1:h})\] \[\times D_{\mathsf{H}}^{2}\Big{(}M_{\mathrm{1at},h}(s_{h},a_{h}), \widetilde{P}_{\phi}^{\pi_{\mathrm{1at}}}(r_{h},s_{h+1}=\cdot\mid x_{h},a_{h}) \Big{)}\] (Jensen) \[=\sum_{\begin{subarray}{c}s_{1:h},a_{1:h}\in(\mathcal{S}\times \mathcal{A})^{h}\\ x_{h},a_{h}\in\mathcal{X}\times\mathcal{A}\end{subarray}}\widetilde{P}_{\phi}^{ \pi_{\mathrm{1at}}}(s_{1:h},a_{1:h})\widetilde{P}_{\phi}^{\pi_{\mathrm{1at}}}(x _{h},a_{h}\mid s_{1:h},a_{1:h})\] \[\times D_{\mathsf{H}}^{2}\Big{(}M_{\mathrm{1at},h}(\phi(x_{h}),a_ {h}),\widetilde{P}_{\phi}^{\pi_{\mathrm{1at}}}(r_{h},\phi(x_{h+1})=\cdot\mid x_{h},a_{h})\Big{)}\] \[=\sum_{x_{h},a_{h}\in\mathcal{X}\times\mathcal{A}}\widetilde{P}_{ \phi}^{\pi_{\mathrm{1at}}}(x_{h},a_{h})D_{\mathsf{H}}^{2}\big{(}M_{\mathrm{1at} }(\phi(x_{h}),a_{h}),M_{\mathsf{obs},h}^{\star}(r_{h},\phi(x_{h+1})=\cdot\mid x_{ h},a_{h})\big{)}\] (Simplifying & Eq. (79)) \[=\mathbb{E}^{\pi_{\mathrm{1at}}\circ\phi}\big{[}D_{\mathsf{H}}^{2} \big{(}M_{\mathrm{1at}}(\phi(x_{h}),a_{h}),M_{\mathsf{obs},h}^{\star}(r_{h}, \phi(x_{h+1})=\cdot\mid x_{h},a_{h})\big{)}\big{]}\] (Change of measure (Lemma I.1)) \[=\mathbb{E}^{\pi_{\mathrm{1at}}\circ\phi}\big{[}D_{\mathsf{H}}^{2} \big{(}M_{\mathrm{1at}}(\phi(x_{h}),a_{h}),\phi_{\mathsf{H}}^{\star}M_{ \mathsf{obs},h}^{\star}(x_{h},a_{h})\big{)}\big{]},\] (By definition of \[\phi_{\mathsf{H}}^{\star}M_{\mathsf{obs}}^{\star}\] ) as desired. Summing over \(h\in[H]\) we obtain the desired bound. The bound Eq. (74) is a consequence of Eq. (73) and the data-processing inequality. Namely, using the definition of \(\widetilde{M}_{\phi,h}^{\star,\pi_{\mathrm{1at}}}\) from Eq. (71) and the joint convexity of the squared Hellinger distance we have:

\[D_{\mathsf{H}}^{2}\Big{(}M_{\mathrm{1at},h}(\cdot\mid s_{h},a_{h}), \widetilde{M}_{\phi,h}^{\star,\pi_{\mathrm{1at}}}(\cdot\mid s_{h},a_{h})\Big{)}\] \[\qquad\leq\widetilde{\mathbb{E}}_{\phi}^{\pi_{\mathrm{1at}}}\Big{[} D_{\mathsf{H}}^{2}\Big{(}M_{\mathrm{1at},h}(\cdot\mid s_{h},a_{h}),\widetilde{M}_{\phi,h}^{ \star}(\cdot\mid s_{1:h},a_{1:h})\Big{)}\mid s_{h},a_{h}\Big{]}.\] (80)Thus, we have

\[\mathbb{E}_{\phi}^{\pi_{\text{lat}}}\Big{[}D_{\mathsf{H}}^{2}\Big{(}M _{\mathsf{1at},h}(\cdot\mid s_{h},a_{h}),\widetilde{M}_{\phi,h}^{*}(\cdot\mid s _{h},a_{h})\Big{)}\Big{]}\] \[\quad\leq\mathbb{E}_{\phi}^{\pi_{\text{lat}}}\Big{[}\mathbb{E}_{ \phi}^{\pi_{\text{lat}}}\Big{[}D_{\mathsf{H}}^{2}\Big{(}M_{\mathsf{1at},h}( \cdot\mid s_{h},a_{h}),\widetilde{M}_{\phi,h}^{*}(\cdot\mid s_{1:h},a_{1:h}) \Big{)}\mid s_{h},a_{h}\Big{]}\Big{]}\] \[\quad=\mathbb{E}_{\phi}^{\pi_{\text{lat}}}\Big{[}D_{\mathsf{H}}^{ 2}\Big{(}M_{\mathsf{1at},h}(\cdot\mid s_{h},a_{h}),\widetilde{M}_{\phi,h}^{*}( \cdot\mid s_{1:h},a_{1:h})\Big{)}\Big{]},\]

as desired. 

**Lemma I.3** (Simulation lemma).: _For any latent model \(M_{\mathsf{1at}}\) with Markovian transition kernel \(\{P_{\mathsf{1at},h}\}_{h\in[H]}\), latent policy \(\pi_{\mathsf{1at}}:\mathcal{S}\times[H]\to\Delta(\mathcal{A})\), and decoder \(\phi\in\Phi\), we have that for all \(f:\mathcal{S}\times\mathcal{A}\to[0,1]\):_

\[|\mathbb{E}^{M_{\mathsf{1at}},\pi_{\mathsf{1at}}}[f(s_{h},a_{h}) ]-\widetilde{\mathbb{E}}_{\phi}^{\pi_{\mathsf{1at}}}[f(s_{h},a_{h})]|\] \[\quad\leq\sum_{h^{\prime}<h}\mathbb{E}^{\pi_{\mathsf{1at}}\phi \phi}\Big{[}\big{[}\big{\|}[P_{\mathsf{1at}}\circ\phi]_{h}(x_{h^{\prime}},a_{ h^{\prime}})-\phi_{h+1}\sharp P_{\mathsf{obs},h}^{*}(x_{h^{\prime}},a_{h^{\prime}}) \big{\|}_{\mathfrak{tv}}\Big{]},\] (75)

_and thus for any sequence of policies \(\pi_{\mathsf{1at}}^{(\mathsf{t})}\), latent models \(M_{\mathsf{1at}}^{(\mathsf{t})}\), and decoders \(\phi^{(\mathsf{t})}\), we have:_

\[\sum_{t=1}^{T}\sum_{h=0}^{H} \mathbb{E}^{M_{\mathsf{1at}}^{(\mathsf{t})},\pi_{\mathsf{1at}}^{ (\mathsf{t})}}[f(s_{h},a_{h})]-\widetilde{\mathbb{E}}_{\phi^{(\mathsf{t})}}^{ \pi_{\mathsf{1at}}^{(\mathsf{t})}}[f(s_{h},a_{h})]|\] (76) \[\quad\leq H\sqrt{TH}\sqrt{\sum_{t=1}^{T}\sum_{h=0}^{H}\mathbb{E}^ {\pi_{\mathsf{1at}}^{(\mathsf{t})}\circ\phi^{(\mathsf{t})}}\big{[}[\Delta_{h}( M_{\mathsf{1at}}^{(\mathsf{t})},\phi^{(\mathsf{t})})](x_{h},a_{h})\big{]}}.\] (77)

Proof of Lemma I.3.: Firstly note that, from Lemma I.1, the left-hand-side of Eq. (75) is equivalent to

\[|\mathbb{E}^{M_{\mathsf{1at}},\pi_{\mathsf{1at}}}[f(s_{h},a_{h})]-\widetilde{ \mathbb{E}}_{\phi}^{\pi_{\mathsf{1at}}}[f(s_{h},a_{h})]|=|\mathbb{E}^{M_{ \mathsf{1at}},\pi_{\mathsf{1at}}}[f(s_{h},a_{h})]-\mathbb{E}^{M_{\mathsf{obs}} ^{*},\pi_{\mathsf{1at}}\phi}[[f\circ\phi](x_{h},a_{h})]|\] (81)

For any \(\pi_{\mathsf{1at}}:\mathcal{S}\times[H]\to\Delta(\mathcal{A})\), let \(d_{\mathsf{1at},h}^{\pi_{\mathsf{1at}}}=d_{h}^{M_{\mathsf{1at}},\pi_{\mathsf{1at }}}\) denote the occupancy in \(M_{\mathsf{1at}}\), and similarly for any \(\pi_{\mathsf{obs}}:\mathcal{X}\times[H]\to\Delta(\mathcal{A})\) let \(\sigma^{\pi_{\mathsf{obs}}}_{\mathsf{obs},h}(x_{h},a_{h})=d_{h}^{M_{\mathsf{ dis}}^{*},\pi_{\mathsf{obs}}}(x_{h},a_{h})\) denote the occupancy in \(M_{\mathsf{obs}}^{*}\). We overload notation by letting \(d_{\mathsf{obs},h}^{\pi_{\mathsf{1at}}\phi\phi}(s,a)\coloneqq\sum_{x:\phi(x)=s }d_{\mathsf{obs},h}^{\pi\circ\phi}(x,a)\). We will establish the stronger result that

\[\Big{\|}d_{\mathsf{1at},h}^{\pi_{\mathsf{1at}}}(\cdot)-d_{\mathsf{obs},h}^{\pi _{\mathsf{1at}}\phi\phi}(\cdot)\Big{\|}_{\mathfrak{tv}}\leq\sum_{h^{\prime}<h} \mathbb{E}^{\pi_{\mathsf{1at}}\phi\phi}\big{[}\big{\|}[P_{\mathsf{1at}}\circ \phi](x_{h^{\prime}},a_{h^{\prime}})-\phi\sharp P_{\mathsf{obs}}^{*}(x_{h^{ \prime}},a_{h^{\prime}})\big{\|}_{\mathfrak{tv}}\big{]},\] (82)

where the \(\mathfrak{tv}\) norm on the left-hand-side is over \(\mathcal{S}\times\mathcal{A}\). Note that this implies the desired bound on Eq. (81) by Holder's inequality. We prove this by induction over \(h\). For the base case (\(h=0\)), we have:

\[\sum_{s_{1},a_{1}}\Bigg{|}d_{\mathsf{1at},1}^{\pi_{\mathsf{1at}}}( s_{1},a_{1})-d_{\mathsf{obs}}^{\pi_{\mathsf{1at}}\circ\phi}(s_{1},a_{1})\Bigg{|}\] \[\quad=\sum_{s_{1},a_{1}}\Bigg{|}P_{\mathsf{1at},0}(s_{1}\mid\emptyset )\pi_{\mathsf{1at}}(a_{1}\mid s_{1})-\sum_{x_{1}=\phi(x_{1})=s_{1}}d_{\mathsf{ obs}}^{\pi_{\mathsf{1at}}\circ\phi}(x_{1},a_{1})\Bigg{|}\] \[\quad=\sum_{s_{1},a_{1}}\Bigg{|}P_{\mathsf{1at},0}(s_{1}\mid \emptyset)\pi_{\mathsf{1at}}(a_{1}\mid s_{1})-\sum_{x_{1}=\phi(x_{1})=s_{1}}P_{ \mathsf{obs},0}^{*}(x_{1}\mid\emptyset)\pi_{\mathsf{1at}}(a_{1}\mid\phi(x_{1})) \Bigg{|}\] \[\quad=\sum_{s_{1}}\Bigg{|}P_{\mathsf{1at},0}(s_{1}\mid\emptyset)- \phi_{1}\sharp P_{\mathsf{obs},0}^{*}(s_{1}\mid\emptyset)\Bigg{|}\sum_{a_{1}}\pi_{ \mathsf{1at}}(a_{1}\mid s_{1})\] \[\quad=\big{\|}P_{\mathsf{1at},0}(\emptyset)-\phi_{1}\sharp P_{\mathsf{ obs},0}^{*}(\emptyset)\big{\|}_{\mathfrak{tv}}.\]For the general case, let us further overload notation by letting \(d^{\pi\circ\phi}_{\text{obs},h}(s_{h})=\sum_{a_{h}}d^{\pi\circ\phi}_{\text{obs},h }(s_{h},a_{h})\) and \(P^{\star}_{\text{obs}}(s_{h}\mid x_{h-1},a_{h-1})=\phi\sharp P^{\star}_{\text{ obs}}(s_{h}\mid x_{h-1},a_{h-1})=\sum_{x_{h};\phi(x_{h})=s_{h}}P^{\star}_{\text{ obs}}(x_{h}\mid x_{h-1},a_{h-1})\). Let us also abbreviate \(\pi:=\pi_{\text{lat}}\). Firstly note that it is sufficient to establish the result for \(\sum_{s_{h}\in\mathcal{S}}\left|d^{\pi}_{\text{lat},h}(s_{h})-d^{\pi\circ \phi}_{\text{obs},h}(s_{h})\right|\), since

\[\sum_{s_{h},a_{h}\in\mathcal{S}\times\mathcal{A}}\left|d^{\pi}_{ \text{lat},h}(s_{h},a_{h})-d^{\pi\circ\phi}_{\text{obs},h}(s_{h},a_{h})\right| =\sum_{s_{h},a_{h}\in\mathcal{S}\times\mathcal{A}}\left|d^{\pi}_{ \text{lat},h}(s_{h})-d^{\pi\circ\phi}_{\text{obs},h}(s_{h})\right|\pi(a_{h} \mid s_{h})\] \[=\sum_{s_{h}\in\mathcal{S}}\left|d^{\pi}_{\text{lat},h}(s_{h})-d ^{\pi\circ\phi}_{\text{obs},h}(s_{h})\right|.\]

Below, all summations over \(s_{h}\) (resp. \(x_{h}\)) with domain unspecified are over \(\mathcal{S}\) (resp. \(\mathcal{X}\)), and likewise for summations over \(s_{h},a_{h}\) or \(x_{h},a_{h}\). We have:

\[\sum_{s_{h}}\left|d^{\pi}_{\text{lat},h}(s_{h})-d^{\pi\circ\phi}_{ \text{obs},h}(s_{h})\right|\] \[=\sum_{s_{h}}\left|d^{\pi}_{\text{lat},h}(s_{h})-\sum_{x_{h};\phi (x_{h})=s_{h}}d^{\pi\circ\phi}_{\text{obs},h}(x_{h})\right|\] \[=\sum_{s_{h}}\left|\sum_{s_{h-1},a_{h-1}}d^{\pi}_{\text{lat},h}(s_ {h-1},a_{h-1})P_{\text{lat},h}(s_{h}\mid s_{h-1},a_{h-1})\right.\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad-\sum_{x_{h};\phi (x_{h})=s_{h}}\sum_{x_{h-1},a_{h-1}}d^{\pi\circ\phi}_{\text{obs},h}(x_{h-1},a_ {h-1})P^{\star}_{\text{obs},h}(x_{h}\mid x_{h-1},a_{h-1})\right|\] \[=\sum_{s_{h}}\left|\sum_{s_{h-1},a_{h-1}}d^{\pi}_{\text{lat},h}(s _{h-1},a_{h-1})P_{\text{lat},h}(s_{h}\mid s_{h-1},a_{h-1})\right.\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad-\sum_{x_{h-1},a _{h-1}}d^{\pi\circ\phi}_{\text{obs},h}(x_{h-1},a_{h-1})P^{\star}_{\text{obs},h }(s_{h}\mid x_{h-1},a_{h-1})\Bigg{|}\] \[=\sum_{s_{h}}\left|\sum_{s_{h-1},a_{h-1}}d^{\pi}_{\text{lat},h}(s _{h-1},a_{h-1})P_{\text{lat},h}(s_{h}\mid s_{h-1},a_{h-1})\right.\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\left.-\sum_{x_{h -1},a_{h-1}}d^{\pi\circ\phi}_{\text{obs},h}(x_{h-1},a_{h-1})P^{\star}_{\text{ obs},h}(s_{h}\mid\phi(x_{h-1}),a_{h-1})\right.\] \[\leq\sum_{s_{h-1},a_{h-1}}\left|d^{\pi}_{\text{lat},h}(s_{h-1},a_ {h-1})-\sum_{x_{h-1}:\phi(x_{h-1})=s_{h-1}}d^{\pi\circ\phi}_{\text{obs},h}(x_{h-1 },a_{h-1})\right|\sum_{s_{h}}P_{\text{lat},h}(s_{h}\mid s_{h-1},a_{h-1})\] \[\quad+\sum_{s_{h}}\left|\sum_{x_{h-1},a_{h-1}}d^{\pi\circ\phi}_{ \text{obs},h}(x_{h-1},a_{h-1})\big{(}(P_{\text{lat},h}\circ\phi)(s_{h}\mid x_{h -1},a_{h-1})-P^{\star}_{\text{obs},h}(s_{h}\mid x_{h-1},a_{h-1})\right)\right|\] \[\leq\left\|d^{\pi}_{\text{lat},h-1}(\cdot)-d^{\pi\circ\phi}_{ \text{obs},h-1}(\phi^{-1}(\cdot))\right\|_{\text{tv}}+\mathbb{E}^{\pi\circ\phi} \Big{[}\big{[}[P_{\text{lat},h}\circ\phi](x_{h-1},a_{h-1})-\phi^{\sharp}_{ \text{tr}}P^{\star}_{\text{obs},h}(x_{h-1},a_{h-1})\big{\|}_{\text{tv}}\Big{]}.\]From which it follows that, for each \(h\), we have:

\[\Big{\|}d^{\pi}_{\mathrm{1at},h}(\cdot)-d^{\pi\circ\phi}_{\mathrm{ obs},h}(\phi^{-1}(\cdot))\Big{\|}_{\mathrm{tv}} \leq\sum_{h^{\prime}<h}\mathbb{E}^{\pi\circ\phi}\Big{[}\big{\|}[P_{ \mathrm{1at}}\circ\phi]_{h^{\prime}}(x_{h^{\prime}},a_{h^{\prime}})-\phi_{h^{ \prime}+1}\sharp P^{\star}_{\mathrm{obs},h^{\prime}}(x_{h^{\prime}},a_{h^{ \prime}})\big{\|}_{\mathrm{tv}}\Big{]}\] \[\leq\sum_{h^{\prime}\in[H]}\mathbb{E}^{\pi\circ\phi}\Big{[}\big{\|} [P_{\mathrm{1at}}\circ\phi]_{h^{\prime}}(x_{h^{\prime}},a_{h^{\prime}})-\phi_{h ^{\prime}+1}\sharp P^{\star}_{\mathrm{obs},h^{\prime}}(x_{h^{\prime}},a_{h^{ \prime}})\big{\|}_{\mathrm{tv}}\Big{]}.\]

### Proofs for Appendix I.1.3: Risk Bound Under CorruptionRobustness (Theorem H.1)

**Theorem H.1** (Risk bound for O2L under self-predictive estimation and CorruptionRobustness).: _Assume \(\textsc{Rep}_{\mathrm{self};\mathrm{opt}}\) satisfies Assumption A.1 with parameter \(\gamma>0\) and that \(\mathcal{M}_{\mathrm{1at}}\) is realizable (i.e. \(M^{\star}_{\mathrm{1at}}\in\mathcal{M}_{\mathrm{1at}}\)). Furthermore, let \(\textsc{Alg}_{\mathrm{1at}}\) be \(\mathsf{CorruptionRobust}\) (Definition I.2) with parameter \(\alpha\). Then, O2L (Algorithm 1) with inputs \(T,K,\Phi,\textsc{Alg}_{\mathrm{1at}}\), and \(\textsc{Rep}_{\mathrm{self};\mathrm{opt}}\) has expected risk_

\[\mathbb{E}[\mathtt{Risk}_{\mathrm{obs}}(TK)]\leq c_{1}\cdot\mathtt{Risk}_{ \mathrm{base}}(K)+c_{2}\gamma\cdot\frac{K}{T}\mathtt{Est}_{\mathrm{self}; \mathrm{opt}}(T,\gamma)+c_{3}\gamma^{-1}\cdot\big{(}\alpha^{2}+H\big{)}\] (66)

_for absolute constants \(c_{1},c_{2},c_{3}>0\)._

Proof of Theorem H.1.: Let us write \(\pi^{(t,k+1)}_{\mathrm{1at}}=\widehat{\pi}^{(t)}_{\mathrm{1at}}\) and, for any \(t,k\in[T]\times[K+1]\), \(\pi^{(t,k)}_{\mathrm{obs}}:=\pi^{(t,k)}_{\mathrm{1at}}\circ\phi^{(t)}\). Let \(p^{(t,k)}_{\mathrm{obs}}\) denote the distributions of played policies \(\pi^{(t,k)}_{\mathrm{obs}}\) induced by the interaction of \(\textsc{Alg}_{\mathrm{1at}}\) and \(\textsc{Rep}_{\mathrm{self};\mathrm{opt}}\) inside the O2L algorithm. Let us write the online sum of self-prediction errors as

\[\varepsilon^{2}_{\mathsf{rep}}\coloneqq\sum_{t=1}^{T}\sum_{k=1}^{K+1}\sum_{h=0 }^{H}\mathbb{E}_{\pi^{(t,k)}_{\mathrm{obs}}\sim p^{(t,k)}}\,\mathbb{E}^{\pi^{ (t,k)}_{\mathrm{obs}}}\big{[}D^{2}_{\mathsf{H}}\big{(}[M^{(t)}_{\mathrm{1at}} \circ\phi^{(t)}]_{h}(x_{h},a_{h}),\phi^{(t)}_{h+1}\sharp M^{\star}_{\mathrm{ obs},h}(x_{h},a_{h})\big{)}\big{]}\] (83)

Since the final output policy of O2L satisfies \(\widehat{\pi}_{\mathrm{1at}}=\mathsf{Unif}(\widehat{\pi}^{(1)}_{\mathrm{1at}},\ldots,\widehat{\pi}^{(T)}_{\mathrm{1at}})\) (Line 12), we have

\[\mathbb{E}[\mathtt{Risk}_{\mathrm{obs}}(TK)]=\frac{1}{T}\sum_{t=1}^{T} \mathbb{E}\Big{[}J^{M^{\star}_{\mathrm{obs}}}(\pi^{\star}_{\mathrm{obs}})-J^ {M^{\star}_{\mathrm{obs}}}(\widehat{\pi}^{(t)}_{\mathrm{obs}})\Big{]}.\]

We take the following decomposition on the risk

\[J^{M^{\star}_{\mathrm{obs}}}(\pi^{\star}_{\mathrm{obs}})-J^{M^{ \star}_{\mathrm{obs}}}(\widehat{\pi}^{(t)}_{\mathrm{obs}}) =J^{M^{\star}_{\mathrm{1at}}}(\pi_{M^{\star}_{\mathrm{1at}}})-J^{M ^{(t)}_{\mathrm{1at}}}(\pi_{M^{(t)}_{\mathrm{1at}}})+\underbrace{J^{M^{(t)}_{ \mathrm{1at}}}(\pi_{M^{(t)}_{\mathrm{1at}}})-J^{M^{(t)}_{\mathrm{1at}}}( \widehat{\pi}^{(t)}_{\mathrm{1at}})}_{\mathcal{A}_{\mathrm{t}}}\] \[\quad+\underbrace{J^{M^{(t)}_{\mathrm{1at}}}(\widehat{\pi}^{(t)}_{ \mathrm{1at}})-J^{M^{\star}_{\mathrm{obs}}}(\widehat{\pi}^{(t)}_{\mathrm{ obs}})}_{\mathrm{B}_{\mathrm{t}}}.\] (84)

We will show that \(\mathbb{E}\Big{[}\sum_{t=1}^{T}\mathrm{A}_{\mathrm{t}}\Big{]}\lesssim T \mathtt{Reg}_{\mathrm{base}}(K)+\alpha\sqrt{T}\,\mathbb{E}[\varepsilon_{\mathsf{ rep}}]\) and that \(\mathbb{E}\Big{[}\sum_{t=1}^{T}\mathrm{B}_{\mathrm{t}}\Big{]}\lesssim\sqrt{TH}\, \mathbb{E}[\varepsilon_{\mathsf{rep}}]\), then return to the first term \(J^{M^{\star}_{\mathrm{1at}}}(\pi_{M^{\star}_{\mathrm{1at}}})-J^{M^{(t)}_{ \mathrm{1at}}}(\pi_{M^{(t)}_{\mathrm{1at}}})\) at the end of the proof.

To bound \(\mathbb{E}\Big{[}\sum_{t=1}^{T}A_{t}\Big{]}\), we note that

\[\sum_{t=1}^{T}\mathbb{E}[A_{t}]\leq c_{1}T\mathtt{Risk}_{\mathrm{ base}}(K)+\] \[\qquad\qquad\qquad\alpha\sum_{t=1}^{T}\mathbb{E}\Bigg{[}\sqrt{\sum_{k=1}^{K}\sum_{h=1}^{H} \mathbb{E}_{\pi^{(t,k)}_{\mathrm{1at}}\sim p^{(t,k)}_{\mathrm{1at}}}\,\mathbb{E} ^{\pi^{(t,k)}_{\mathrm{1at}}\circ\phi^{(t)}}\big{[}[\Delta_{h}(M^{(t)}_{ \mathrm{1at}},\phi^{(t)})](x_{h},a_{h})\big{]}\Bigg{]}\] \[\leq c_{1}T\mathtt{Risk}_{\mathrm{base}}(K)+\alpha\sqrt{T}\, \mathbb{E}\Bigg{[}\sqrt{\sum_{t=1}^{T}\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{ \pi^{(t,k)}_{\mathrm{obs}}\sim p^{(t,k)}_{\mathrm{obs}}}\,\mathbb{E}^{\pi^{(t,k )}_{\mathrm{obs}}}\big{[}[\Delta_{h}(M^{(t)}_{\mathrm{1at}},\phi^{(t)})](x_{h},a _{h})\big{]}\Bigg{]}\] \[\leq c_{1}T\mathtt{Risk}_{\mathrm{base}}(K)+\alpha\sqrt{T}\, \mathbb{E}[\varepsilon_{\mathsf{rep}}].\]where the first line follows from the CorruptionRobust definition (Definition I.2), the second line follows from Lemma I.2, the third line follows by Cauchy-Schwartz, and the last line recalls the definition of \(\varepsilon_{\mathsf{rep}}\) from Eq. (83).

For the term \(\sum_{t=1}^{T}B_{t}\), for any \(\pi_{\mathrm{1at}}:\mathcal{S}\times[H]\to\Delta(\mathcal{A})\) we let \(Q_{\mathrm{1at}^{(t)},h}^{\pi_{\mathrm{1at}}^{(t)},h}=\mathcal{T}_{h}^{M_{ \mathrm{1at}}^{(t)}}Q_{\mathrm{1at}^{(t)},h+1}^{\pi_{\mathrm{1at}}}\) be the \(Q^{\pi_{\mathrm{1at}}}\) function of the latent MDP \(M_{\mathrm{1at}}^{(t)}\). Note that

\[\sum_{t=1}^{T}\Bigl{\{}J^{M_{\mathrm{1at}}^{(t)}}(\widehat{\pi}_{ \mathrm{1at}}^{(t)})-\mathbb{E}^{\widehat{\pi}_{\mathrm{1at}}^{(t)}\phi^{(t)} }\Bigl{[}[Q_{\mathrm{1at}^{(t)}}^{\widehat{\pi}_{\mathrm{1at}}^{(t)}}\circ \phi^{(t)}]_{1}(x_{1},a_{1})\Bigr{]}\Bigr{\}}\] (85) \[\qquad=\sum_{t=1}^{T}\mathbb{E}^{M_{\mathrm{1at}}^{(t)},\widehat{ \pi}_{\mathrm{1at}}^{(t)}}\Bigl{[}Q_{\mathrm{1at}^{(t)},1}^{\widehat{\pi}_{ \mathrm{1at}}^{(t)}}(s_{1},a_{1})\Bigr{]}-\mathbb{E}^{\widehat{\pi}_{\mathrm{1at }}^{(t)}\phi^{(t)}}\Bigl{[}[Q_{\mathrm{1at}^{(t)}}^{\widehat{\pi}_{\mathrm{1at }}^{(t)}}\circ\phi^{(t)}]_{1}(x_{1},a_{1})\Bigr{]}\] \[\qquad\leq\sum_{t=1}^{T}\mathbb{E}^{\widehat{\pi}_{\mathrm{1at}}^ {(t)}\phi^{(t)}}\bigl{[}\bigl{\|}[P_{\mathrm{1at}}^{(t)}\circ\phi^{(t)}]_{0}( \emptyset)-\phi_{\mathrm{1}}^{(t)}\sharp P_{\mathrm{obs},0}^{\star}(\emptyset) \bigr{\|}_{\mathrm{tv}}\bigr{]}\qquad\qquad\text{(by Lemma I.3)}\] \[\qquad\leq\sum_{t=1}^{T}\sum_{h=0}^{H}\mathbb{E}^{\widehat{\pi}_ {\mathrm{1at}}^{(t)}\circ\phi^{(t)}}\bigl{[}\bigl{\|}[P_{\mathrm{1at}}^{(t)} \circ\phi^{(t)}]_{h}(x_{h},a_{h})-\phi_{h+1}^{(t)}\sharp P_{\mathrm{obs},h}^{ \star}(x_{h},a_{h})\bigr{\|}_{\mathrm{tv}}\bigr{]}\] \[\qquad\leq\sqrt{TH}\varepsilon_{\mathsf{rep}},\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad 

[MISSING_PAGE_FAIL:73]

**Lemma I.4** (Misspecification guarantee for \(\mathcal{T}_{\mathtt{1at}}\)).: \[\forall f:\mathcal{S}\times\mathcal{A}\to[0,1]:\quad\sum_{k=1}^{K} \sum_{h=1}^{H}\widetilde{\mathbb{E}}_{\phi}^{{}^{(k)}}\bigg{[}\Big{(}\mathcal{T }_{\mathtt{1at},h}f(s_{h},a_{h})-\widetilde{\mathcal{T}}_{\phi,h}^{{}^{(k)}}f( s_{h},a_{h})\Big{)}^{2}\bigg{]}\leq\mathcal{O}(\varepsilon_{\mathtt{rep}}^{2}).\]

Proof of Lemma I.4.: Follows from Assumption I.2 and the definitions of \(\widetilde{\mathcal{T}}_{\phi,h}^{{}^{(k)}}\) and \(\mathcal{T}_{\mathtt{1at},h}\). 

We begin with the following lemmas, which will be proved in the sequel.

**Lemma I.5** (Optimism).: _For the choice of \(\beta\) in Theorem I.1, with probability at least \(1-\delta\), we have that for all \(k\in[K]\):_

\[Q_{\mathtt{1at}}^{\star}\in\mathcal{F}^{{}^{(k)}}.\]

**Lemma I.6** (Small in-sample squared Bellman errors).: _With probability at least \(1-\delta\), we have that for all \(k\in[K]\), \(h\in[H]\), and \(f\in\mathcal{F}^{{}^{(k)}}\):_

\[\sum_{i=1}^{k-1}\widetilde{\mathbb{E}}_{\phi}^{{}^{(k)}} \bigg{[}\Big{(}f(s_{h},a_{h})-\widetilde{\mathcal{T}}_{\phi,h}^{{}^{(k)}}f(s_ {h},a_{h})\Big{)}^{2}\bigg{]}\leq\mathcal{O}(\beta).\]

Let us write \(\pi_{\text{obs}}^{{}^{(k)}}\coloneqq\pi^{{}^{(k)}}\circ\phi\). Let us introduce the shorthand \(\bar{d}_{\text{obs},h}^{{}^{(k)}}\coloneqq\sum_{i=1}^{k-1}d_{\text{obs},h}^{{} ^{(k)}}\), where \(d_{\text{obs}}^{{}^{\pi}}\) is the occupancy for \(M_{\text{obs}}^{\star}\), and also the burn-in time

\[\kappa_{h}(x,a)\coloneqq\min\Biggl{\{}k:\sum_{i=1}^{k-1}d_{\text{obs},h}^{{}^{ (k)}}(x,a)\geq C_{\text{cov}}\mu_{h}^{\star}(x,a)\Biggr{\}}.\]

Let us recall, from the analysis of [13], that for any \(h\in[H]\) and \(f:\mathcal{S}\times\mathcal{A}\to[0,1]\) we have

\[\sum_{k=1}^{K}\mathbb{E}^{{}^{(k)}}[f(s_{h},a_{h})\mathbb{I}\{k <\kappa_{h}(s_{h},a_{h})\}]\leq 2C_{\text{cov}},\] (91)

as well as

\[\sum_{h=1}^{H}\sum_{k=1}^{K}\sum_{s,a}\frac{(d_{h}^{{}^{(k)}}(x,a )\mathbb{I}\{k\geq\kappa_{h}(x,a)\})^{2}}{\bar{d}_{h}^{{}^{(k)}}(x,a)}\leq O( HC_{\text{cov}}\log(K)).\] (92)\[\sum_{k}J^{M_{\text{lat}}}(\pi_{M_{\text{lat}}})-J^{M_{\text{lat}}}(\pi^{ {(k)}}) \leq\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}^{M_{\text{lat}},\pi^{ {(k)}}}[f^{{(k)}}(s_{h},a_{h})-\mathcal{T}_{\text{lat}}f^{{(k)}}(s_{h},a_{h})]\] (Optimism (Lemma I.5)) \[\leq\sum_{k=1}^{K}\sum_{h=1}^{H}\widetilde{\mathbb{E}}_{\phi}^{ \pi^{(k)}}[f^{{(k)}}(s_{h},a_{h})-\mathcal{T}_{\text{lat}}f^{{(k)}}(s_{h},a_{h })]+H^{3/2}\sqrt{K\varepsilon_{\text{rep}}^{2}}\] (Simulation Lemma Lemma I.3) \[=\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}^{\pi^{{(k)}} \circ\phi}[[(f^{{(k)}}-\mathcal{T}_{\text{lat}}f^{{(k)}})\circ\phi](x_{h},a_{ h})]+H^{3/2}\sqrt{K\varepsilon_{\text{rep}}^{2}}\] (Change of measure Lemma I.1) \[\leq\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}^{\pi^{{(k)}} \circ\phi}[[(f^{{(k)}}-\mathcal{T}_{\text{lat}}f^{{(k)}})\circ\phi](x_{h},a_{ h})\mathbb{I}[k\geq\kappa_{h}(x_{h},a_{h})\}]\] \[\qquad+2HC_{\text{cov}}+H^{3/2}\sqrt{K\varepsilon_{\text{rep}}^{2 }}\qquad\qquad\text{(Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{eq:Burn-in time Eq.~{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{eq:Burn-in time Eq Eq.~{eq:Burn-in time Eq Eq.~{}\eqref{eq:Burn-in time Eq Eq.~{eq:Burn-in time Eq Eq.~{}\eqref{eq:Burn-in time Eq Eq.~{eq:Burn-in Eq Eq Eq.~{}eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{eq:Burn-in time Eq Eq.~{}eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{eqeq:Burn-in Eq Eq Eq.~{}eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{eq:Burn-in Eq Eq.~{}eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{}\eqref{eq:Burn-in time Eq.~{eq:Burn-in time Eq Eq.~{eq:Burn-in Eq Eq.~{eq:Burn-in Eq Eq Eq Eq.~{ eqwhere we have used that, from Lemma I.6, we have:

\[\sum_{h=1}^{H}\sum_{k=1}^{K}\sum_{i=1}^{k-1}\widetilde{\mathbb{E}}_{\phi}^{{}^{(k) }}\bigg{[}\Big{(}f^{{}^{(k)}}(s_{h},a_{h})-\widetilde{\mathcal{T}}_{\phi}^{{}^{ (k)}}f^{{}^{(k)}}(s_{h},a_{h})\Big{)}^{2}\bigg{]}\leq\mathcal{O}(\beta HK).\]

This gives an upper bound on the regret of

\[\sum_{k=1}^{K}J^{M_{\text{lat}}}(\pi_{M_{\text{lat}}})-J^{M_{\text{lat}}}(\pi^{{ }^{(k)}})\leq\mathcal{O}\Big{(}H\sqrt{C_{\text{cov}}K\log(K)\beta}+H^{3/2}\sqrt {K\varepsilon_{\text{rep}}^{2}}\Big{)}.\]

Using that \(\beta=\mathcal{O}\Big{(}\!\log\!\left(\frac{|\mathcal{F}||\mathcal{G}|HK}{ \delta}\right)+\varepsilon_{\text{rep}}\Big{)}\)and simplifying gives

\[\sum_{k=1}^{K}J^{M_{\text{lat}}}(\pi_{M_{\text{lat}}})-J^{M_{\text{lat}}}(\pi^ {{}^{(k)}})\leq\mathcal{O}\Big{(}H\sqrt{C_{\text{cov}}K\log(K)\log(|\mathcal{ F}||\mathcal{G}|HK/\delta)}\Big{)}+\mathcal{O}\Big{(}H^{3/2}\sqrt{KC_{\text{cov}}\log(K) \varepsilon_{\text{rep}}^{2}}\Big{)},\]

as desired. It only remains to establish the concentrations results.

Concentration analysis.We establish the concentration results of Lemma I.5 and Lemma I.6.

Proof of Lemma I.6.: Let

\[X_{k}(h,f)=\big{(}f_{h}(s_{h}^{{}^{(k)}},a_{h}^{{}^{(k)}})-r_{h}^{{}^{(k)}}-f_{ h+1}(s_{h+1}^{{}^{(k)}})\big{)}^{2}-\Big{(}\widetilde{\mathcal{T}}_{\phi}^{{}^{ (k)}}f_{h}(s_{h}^{{}^{(k)}},a_{h}^{{}^{(k)}})-r_{h}^{{}^{(k)}}-f_{h+1}(s_{h+1}^ {{}^{(k)}})\Big{)}^{2}.\]

Let \(\mathfrak{F}_{k,h}=\{s_{1}^{{}^{(i)}},a_{1}^{{}^{(i)}},r_{1}^{{}^{(i)}},\ldots,s_{H}^{{}^{(i)}},a_{H}^{{}^{(i)}},r_{H}^{{}^{(i)}}\}_{i=1}^{k}.\) Note that

\[\mathbb{E}\big{[}r_{h}^{{}^{(k)}}+f_{h+1}(s_{h+1}^{{}^{(k)}}) \mid\mathfrak{F}_{k,h}\big{]} =\mathbb{E}\big{[}r_{h}^{{}^{(k)}}+f_{h+1}(s_{h+1}^{{}^{(k)}}) \mid\pi^{{}^{(k)}}\big{]}\] \[=\mathbb{E}\big{[}\mathbb{E}\big{[}r_{h}^{{}^{(k)}}+f_{h+1}(s_{h+ 1}^{{}^{(k)}})\mid s_{h}^{{}^{(k)}},a_{h}^{{}^{(k)}},\pi^{{}^{(k)}}\big{]} \mid\pi^{{}^{(k)}}\big{]}\] \[=\mathbb{E}\Big{[}\widetilde{\mathcal{T}}_{\phi}^{{}^{(k)}}f(s_{ h}^{{}^{(k)}},a_{h}^{{}^{(k)}})\mid\pi^{{}^{(k)}}\Big{]}\] \[=\widetilde{\mathbb{E}}_{\phi}^{{}^{(k)}}\Big{[}\widetilde{ \mathcal{T}}_{\phi}^{{}^{(k)}}f(s_{h},a_{h})\Big{]},\]

and thus that

\[\mathbb{E}[X_{k}(h,f)\mid\mathfrak{F}_{k,h}]\]

Next, note that

\[\mathsf{Var}[X_{k}(h,f)\mid\mathfrak{F}_{k,h}] \leq\mathbb{E}\Big{[}(X_{k}(h,f))^{2}\mid\mathfrak{F}_{k,h}\Big{]}\] \[\leq 16\,\mathbb{E}\bigg{[}\Big{(}f_{h}(s_{h}^{{}^{(k)}},a_{h}^{{ }^{(k)}})-\widetilde{\mathcal{T}}_{\phi}^{{}^{(k)}}f_{h}(s_{h}^{{}^{(k)}},a_{h }^{{}^{(k)}})\Big{)}^{2}\mid\mathfrak{F}_{k,h}\bigg{]}\] \[=16\,\mathbb{E}[X_{k}(h,f)\mid\mathfrak{F}_{k,h}].\]

By Freedman's inequality (Lemma C.2, Lemma C.3), we have that with probability at least \(1-\delta\):

\[\left|\sum_{t<k}X_{t}(h,f)-\sum_{t<k}\mathbb{E}[X_{t}(h,f)\mid \mathfrak{F}_{t,h}]\right|\leq\mathcal{O}\Bigg{(}\sqrt{\log(1/\delta)\sum_{t<k} \mathbb{E}[X_{t}(h,f)\mid\mathfrak{F}_{t,h}]}+\log(1/\delta)\Bigg{)}\]

Taking a union bound over \([K]\times[H]\times\mathcal{F}\), we have that for all \(k,h,f\), with probability at least \(1-\delta\):

\[\left|\sum_{t<k}X_{t}(h,f)-\sum_{t<k}\widetilde{\mathbb{E}}_{\phi} ^{{}^{(k)}}\bigg{[}\Big{(}f_{h}(s_{h},a_{h})-\widetilde{\mathcal{T}}_{\phi}^{ {}^{(k)}}f_{h}(s_{h},a_{h})\Big{)}^{2}\bigg{]}\right|\] (96) \[\leq\mathcal{O}\Bigg{(}\sqrt{\iota\sum_{t<k}\widetilde{\mathbb{E} }_{\phi}^{{}^{(k)}}\bigg{[}\Big{(}f_{h}(s_{h},a_{h})-\widetilde{\mathcal{T}}_{ \phi}^{{}^{(k)}}f_{h}(s_{h},a_{h})\Big{)}^{2}\bigg{]}}+\iota\Bigg{)},\] (97)

where \(\iota=\log(|\mathcal{F}|HK/\delta)\). We now show that

\[\sum_{t<k}X_{t}(h,f^{{}^{(k)}})\leq\beta+\mathcal{O}(\varepsilon_{\text{rep}}+ \iota)=\mathcal{O}(\beta),\] (98)

[MISSING_PAGE_EMPTY:77]

from which the conclusion will follow. We show that

\[\sum_{t<k}\frac{\left(g(s_{h}^{(t)},a_{h}^{(t)})-r_{h}^{(t)}-Q_{\text{1at},h+1}^{ \star}(s_{h+1}^{(t)})\right)^{2}-\left(\widetilde{\mathcal{T}}_{\phi}^{\tau^{(t )}}Q_{\text{1at},h}^{\star}(s_{h}^{(t)},a_{h}^{(t)})-r_{h}^{(t)}-Q_{\text{1at},h }^{\star}(s_{h+1}^{(t)})\right)^{2}}{=W_{t}(h,g)}\geq-\beta/2,\] (101)

and also that

\[\sum_{t<k}\frac{\left(\widetilde{\mathcal{T}}_{\phi}^{\tau^{(t)}}Q_{\text{1at},h}^{\star}(s_{h}^{(t)},a_{h}^{(t)})-r_{h}^{(t)}-Q_{\text{1at},h}^{\star}(s_{h+ 1}^{(t)})\right)^{2}-\left(Q_{\text{1at},h}^{\star}(s_{h}^{(t)},a_{h}^{(t)})-r _{h}^{(t)}-Q_{\text{1at},h}^{\star}(s_{h+1}^{(t)})\right)^{2}}{=V_{t}(h)}\geq- \beta/2.\] (102)

For Eq. (101), note that

\[\mathbb{E}[W_{t}(h,g)\mid\mathcal{F}_{t,h}]=\widetilde{\mathbb{E}}_{\phi}^{ \tau^{(t)}}\bigg{[}\Big{(}g_{h}(s_{h},a_{h})-\widetilde{\mathcal{T}}_{\phi,h} ^{\tau^{(t)}}Q_{\text{1at},h}^{\star}(s_{h},a_{h})\Big{)}^{2}\bigg{]},\] (103)

and that \(\text{Var}\,[W_{t}(h,g)\mid\mathcal{F}_{t,h}]\leq 16\,\mathbb{E}[W_{t}(h,g) \mid\mathcal{F}_{t,h}]\). By Freedman, this gives

\[\left|\sum_{t<k}W_{t}(h,g)-\sum_{t<k}\mathbb{E}[W_{t}(h,g)\mid \mathfrak{F}_{t,h}]\right| \leq\mathcal{O}\Bigg{(}\sqrt{\iota\sum_{t<k}\mathbb{E}[W_{t}(h,g) \mid\mathcal{F}_{t,h}]}+\iota\Bigg{)}\] \[\leq\frac{1}{2}\,\mathbb{E}[W_{t}(h,g)\mid\mathcal{F}_{t,h}]+ \mathcal{O}(\iota),\]

or in other words

\[\sum_{t<k}W_{t}(h,g)\geq\frac{1}{2}\sum_{t<k}\mathbb{E}[W_{t}(h,g)\mid \mathfrak{F}_{t,h}]-\mathcal{O}(\iota)\geq-\mathcal{O}(\iota),\]

using the non-negativity of Eq. (103). For Eq. (102), note that

\[\mathbb{E}[V_{t}(h)\mid\mathfrak{F}_{t,h}]=-\widetilde{\mathbb{E}}_{\phi}^{ \tau^{(t)}}\bigg{[}\Big{(}\mathcal{T}_{\text{1at},h}Q_{\text{1at},h}^{\star}- \widetilde{\mathcal{T}}_{\phi,h}^{\tau^{(t)}}Q_{\text{1at},h}^{\star}\Big{)}^ {2}\bigg{]}\geq-\varepsilon_{\text{rep}},\] (104)

and that \(\text{Var}\,[V_{t}(h)\mid\mathfrak{F}_{t,h}]\leq 16\widetilde{\mathbb{E}}_{\phi}^{ \tau^{(t)}}\bigg{[}\Big{(}\mathcal{T}_{\text{1at},h}Q_{\text{1at},h}^{\star}- \widetilde{\mathcal{T}}_{\phi,h}^{\tau^{(t)}}Q_{\text{1at},h}^{\star}\Big{)}^ {2}\bigg{]}\). By Freedman, this gives

\[\left|\sum_{t<k}V_{t}(h)-\sum_{t<k}\mathbb{E}[V_{t}(h)\mid \mathfrak{F}_{t,h}]\right|\] (105) \[\leq\mathcal{O}\Bigg{(}\sqrt{\iota\sum_{t<k}\widetilde{\mathbb{E} }_{\phi}^{\tau^{(t)}}\bigg{[}\Big{(}\mathcal{T}_{\text{1at}}Q_{\text{1at},h+1}^ {\star}(s_{h},a_{h})-\widetilde{\mathcal{T}}_{\phi,h}^{\tau^{(t)}}Q_{\text{1at},h+1}^{\star}(s_{h},a_{h})\Big{)}^{2}\bigg{]}}+\iota\Bigg{)}\] (106) \[=\mathcal{O}(\varepsilon_{\text{rep}}+\iota),\] (107)

or in other words

\[\sum_{t<k}V_{t}(h)\geq\sum_{t<k}\mathbb{E}[V_{t}(h)\mid\mathfrak{F}_{t,h}]- \mathcal{O}(\varepsilon_{\text{rep}}+\iota)\geq-\mathcal{O}(\varepsilon_{ \text{rep}}+\iota),\]

where we have used Eq. (104).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: All results in this paper are of a theoretical nature, and the stated contributions in the abstract and introduction are given in a precise, formal language. Guidelines:
2. The answer NA means that the abstract and introduction do not include the claims made in the paper.
3. The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.
4. The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.
5. It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
6. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: All results in this paper are of a theoretical nature - we precisely state the conditions under which our results hold. Guidelines:
7. The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
8. The authors are encouraged to create a separate "Limitations" section in their paper.
9. The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
10. The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
11. The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
12. The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
13. If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
14. While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Each theoretical result is stated with all necessary assumptions and is accompanied by complete (and correct) proofs. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: The paper does not include experiments. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

* We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
* **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: The paper does not include experiments requiring code. Guidelines:
* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
* **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: The paper does not include experiments. Guidelines:
* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
* **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines:* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines:
* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conforms with the NeurIPS Code of Etichs. Guidelines:
* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts**Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This is a primarily theoretical work. Guidelines:
* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This is a purely theoretical work, and as such poses no such risks. Guidelines:
* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets.

Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsouring nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects**Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

Answer: [NA]

Justification: The paper does not involve crowdsourcing nor research with human subjects.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.