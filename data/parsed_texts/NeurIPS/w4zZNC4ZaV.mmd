# How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources

 Yizhong Wang\({}^{*}\)\({}^{\clubsuit}\)   Hamish Ivison\({}^{*}\)\({}^{\clubsuit}\)   Pradeep Dasigi\({}^{\clubsuit}\)   Jack Hessel\({}^{\clubsuit}\)

**Tushar Khot\({}^{\clubsuit}\)   Khyathi Raghavi Chandu\({}^{\clubsuit}\)   David Wadden\({}^{\clubsuit}\)   Kelsey MacMillan\({}^{\clubsuit}\)   Noah A. Smith\({}^{\clubsuit\)   Iz Beltagy\({}^{\clubsuit}\)   Hannaneh Hajishirzi\({}^{\clubsuit}\)**

\({}^{\clubsuit}\)Allen Institute for AI  \({}^{\clubsuit}\)University of Washington

{yizhongw,hamishi}@allenai.org

Equal contribution.

###### Abstract

In this work we explore recent advances in instruction-tuning language models on a range of open instruction-following datasets. Despite recent claims that open models can be on par with state-of-the-art proprietary models, these claims are often accompanied by limited evaluation, making it difficult to compare models across the board and determine the utility of various resources. We provide a large set of instruction-tuned models from 6.7B to 65B parameters in size, trained on 12 instruction datasets ranging from manually curated (e.g., OpenAssistant) to synthetic and distilled (e.g., Alpaca) and systematically evaluate them on their factual knowledge, reasoning, multilinguality, coding, safety, and open-ended instruction following abilities through a collection of automatic, model-based, and human-based metrics. We further introduce Tulu\({}^{\clubsuit}\), our best performing instruction-tuned model suite finetuned on a combination of high-quality open resources.

Our experiments show that different instruction-tuning datasets can uncover or enhance specific skills, while no single dataset (or combination) provides the best performance across all evaluations. Interestingly, we find that model and human preference-based evaluations fail to reflect differences in model capabilities exposed by benchmark-based evaluations, suggesting the need for the type of systemic evaluation performed in this work. Our evaluations show that the best model in any given evaluation reaches on average 87% of ChatGPT performance, and 73% of GPT-4 performance, suggesting that further investment in building better base models and instruction-tuning data is required to close the gap. We release our instruction-tuned models, including a fully finetuned 65B Tulu\({}^{\clubsuit}\), along with our code, data, and evaluation framework to facilitate future research.2

Footnote 2: https://github.com/allenai/open-instruct

## 1 Introduction

The latest generation of large language models has brought unprecedented attention to the potential of language technologies. To support imperative user requests and a chat interface, these models often undergo an _instruction-tuning_ step which involves training on supervised input/output pairs. Recent instruction tuning corpora are often gathered via crowdsourcing (Dolly [12], Open Assistant [26]) or via distillation from another model (Alpaca [43], Vicuna [8]). However, while some public, instruction-tuned models are advertised as comparable to powerful closed-source proprietary models such as ChatGPT, most experiments that support such claims only cover a small set of tasks, and mostly rely on model-based evaluation metrics [8; 56]. We contend that the evaluation setup should

[MISSING_PAGE_FAIL:2]

### Instruction Datasets

We attempt to collect a representative sample of different styles of datasets (listed in Table 1), including datasets: (1) created by researchers from existing NLP datasets (SuperNI [48], Flan V2 [31]); (2) written by humans from scratch for the purpose of instruction tuning (Dolly [12], Open Assistant 1 [26]); (3) generated by proprietary models (Self-Instruct [47], Unnatural Instructions [23], Alpaca [43], Baize [52], GPT4-Alpaca [36]); (4) comprised of user-shared prompts accompanied by model-generated completions (ShareGPT3[8]); (5) built for specific skills (CoT [50] for chain-of-thought, Code-Alpaca [6] for code generation). See Appendix C for further details.

Footnote 3: ShareGPT (https://sharegpt.com/) data was used to build the Vicuna model [8], but the exact dataset has not been released. We instead use a reproduced version from https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/tree/main/HTML_cleaned_raw_dataset, and follow Vicuna to split the long conversations into blocks with a maximum length of 2048 tokens.

### Pretrained Models

We primarily use the LLaMa suite [44; 45], a series of pretrained models ranging in size from 6.7B to 65B parameters. We initially experimented with the LLaMa-1 models for the first version of this paper and added LLaMa-2 in our camera ready, which use similar numbers of parameters but were trained over significantly more tokens. These models represent the largest, highest-quality pretrained models available to the community (albeit under restrictive licensing). We also consider OPT [54] and Pythia [4] models with a size comparable to the LLaMa 6.7B model, to examine the effect of different base models. For simplicity, we will round all the sizes to the nearest integer number. We note several ongoing efforts to pre-train similar- or better-quality models [18; 33; 1]. We believe our findings should hold for these models and future stronger open base models.

## 3 Training Models with Various Datasets

### Unifying the Format

We format all datasets to follow a chatbot-style schema to unify the varied styles and formats of the instruction datasets, shown in Figure 1. This allows us to fit arbitrary rounds of interactions between the user and the language model (a.k.a. "assistant") into one input sequence and encode them together with a causal language model. We add special tokens <|user|> and <|assistant|> before user utterances and target assistant responses respectively, and an end-of-text marker </s> at the end of each assistant output, which, at inference time, will stop the model's response for each round.

\begin{table}
\begin{tabular}{l l c c c c} \hline \hline
**Datasets** & **Sourced from** & **\# Instances** & \(\bar{N}_{rounds}\) & \(L_{p_{\textit{proxy}}}\) & \(L_{\textit{completion}}\) \\ \hline SuperNI [48] & NLP datasets + Human-written Instructions & 96,913 & 1.0 & 291.1 & 38.7 \\ ColT [50] & NLP datasets + Human-written CoIs & 100,000 & 1.0 & 266.0 & 53.2 \\ Flan V2 [31] & NLP datasets + Human-written Instructions & 100,000 & 1.0 & 355.7 & 31.2 \\ Dolly [12] & Human-written from scratch & 15,011 & 1.0 & 118.1 & 91.3 \\ Open Assistant 1 [26] & Human-written from scratch & 34,795 & 1.6 & 34.8 & 212.5 \\ Self-instruct [47] & Generated w/ vanilla GPT3 LM & 82,439 & 1.0 & 41.5 & 29.3 \\ Unnatural Instructions [23] & Generated w/ Davinci-002 & 68,478 & 1.0 & 107.8 & 23.6 \\ Alpaca [43] & Generated w/ Davinci-003 & 52,002 & 1.0 & 27.8 & 64.6 \\ Code-Alpaca [6] & Generated w/ Davinci-003 & 20,022 & 1.0 & 35.6 & 67.8 \\ GPT4-Alpaca [36] & Generated w/ Davinci-003 + GPT4 & 52,002 & 1.0 & 28.0 & 161.8 \\ Baize [52] & Generated w/ ChatGPT & 210,311 & 3.1 & 17.6 & 52.8 \\ ShareGPT3 [FOOT 3]Footnote 3: ShareGPT (https://sharegpt.com/) data was used to build the Vicuna model [8], but the exact dataset has not been released. We instead use a reproduced version from https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/tree/main/HTML_cleaned_raw_dataset, and follow Vicuna to split the long conversations into blocks with a maximum length of 2048 tokens.

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Base LMs** & **\# Params** & **\# Tokens** \\ \hline  & 6.7B & 1.0T \\  & 13.0B & 1.0T \\ LLaMa [44] & 32.5B & 1.4T \\  & 65.2B & 1.4T \\ \hline LLaMa-2 [45] & 6.7B & 2.0T \\  & 13.0B & 2.0T \\ \hline OPT [54] & 6.7B & 180B \\ \hline Pythia [4] & 6.9B & 300B \\ \hline \hline \end{tabular}
\end{table}
Table 2: Base models that we finetuned in this work.

### Model Training Details

During training, we compute loss only on tokens after <|assistant|> and before the next <|user|> token. More formally, we consider an instruction dataset as consisting of \(N\) tuples, each with \(i\) turns, \(\{(x_{1}^{j},y_{1}^{j},x_{2}^{j},y_{2}^{j},...x_{i}^{j},y_{i}^{j})\}_{j=1}^{N}\), where \(x_{i}\) is a user prompt and \(y_{i}\) the desired output. For most instances, \(i=1\), and we train the model to output \(y^{j}\) given \(x^{j}\). However, in the case of conversation datasets, we train the model to predict \(y_{i}^{j}\) given some conversation history \(x_{1}^{j},y_{1}^{j},x_{2}^{j},...,x_{i}^{j}\). We train decoder-only models, and use teacher-forcing with loss masking to train the models, where we mask all tokens belonging to the input sequence(s) \(x_{i}\). Given \(X\) as the tokens belonging to the input, and \(Y\) as the target tokens, the loss function is:

\[L=-\sum_{j}\log p_{\theta}(t_{j}\mid t_{<j})\times\begin{cases}1&\text{if }t_{j} \in Y\\ 0&\text{otherwise}\end{cases}\]

where \(t_{j}\) is the \(j\)th input token (belonging to \(X\) or \(Y\)). See Appendix SSD for further training details.

### Tulu\(\bm{\boxplus}\): a Better Instruction-Tuned Model by Combining Resources

Existing studies [48; 31] (and our own evaluation below) have shown that increasing the diversity of instructions can effectively improve the performance of instruction tuning. Following this motivation, we create two mixtures of datasets:

**Human data mixture**, which comprises the best human-authored datasets, including FLAN V2, CoT, Dolly, and Open Assistant 1 (we exclude SuperNI as FLAN V2 includes most tasks in SuperNI);

**Human+GPT data mixture**, which comprises the human mixture and three additional datasets that have generations by OpenAI GPT models, including GPT4-Alpaca, Code-Alpaca, and ShareGPT.

For both mixtures, we concatenate datasets and leave exploring more complex sampling mixtures to future work. We name LLaMa models trained on the Human+GPT data mixture \(\textbf{Tulu}\boxplus^{\prime}\), after a hybrid camel resulting from interbreeding between different species. We differentiate the Tulu models trained from the LLaMa-2 base models by versioning them as **Tulu-1.1**.

## 4 Evaluation Setup

Evaluation of instruction-following models remains a challenging problem due to the enormous scope of "generality" and its open-ended nature. However, we argue that general-purpose models should be able to perform some core tasks before they can generalize to satisfy various practical needs. As such, we set up a multi-faceted evaluation to cover several key aspects of capabilities covering core abilities and open-ended instruction following. Our evaluations closely follow prior work on evaluating instruction-tuned models [9; 2; 47; 8; 16], but serve as the first one to compile them together for systematic evaluation.

### Facets of Evaluation

**Factual knowledge** is essential for language models to serve users' information needs. We use the Massive Multitask Language Understanding dataset (MMLU [22]) for measuring models' factual knowledge. MMLU consists of a set of questions about 57 subjects ranging in difficulty from elementary levels to professional levels, and its multiple-choice format makes it suitable for probing models' knowledge without worrying about the open-endedness of generations.

Figure 1: An example from ShareGPT data. We use <|role|> to set the boundary between messages. The entire sequence is encoded together, and loss is computed on the assistant parts (colored in blue).

**Reasoning** is another fundamental ability for models, especially for solving complex tasks. We use the test split of Grade School Math dataset (GSM [11]) to evaluate models' mathematical reasoning capabilities. We also adopt Big-Bench-Hard (BBH [42]), which contains 23 challenging tasks from Big-Bench [41], to evaluate models' general reasoning capabilities.

**Multilinguality** acts as an important perspective of models for serving people from different backgrounds. We use TyDiQA [10], a multilingual question answering benchmark covering 11 typologically diverse languages for testing how much models can process non-English text. We use the gold-passage setup where one passage containing the reference answer is given.

**Coding** is a particular application that people have used language models for and might be important for integrating these models with external tools [5]. We use the HumanEval dataset [7] to evaluate the models' capability to generate functionally correct programs from docstrings. To avoid ambiguity with our human evaluation, we call this dataset Codex-Eval in this paper.

**Open-ended instruction following.** While the performance on the benchmarks above quantifies the models' ability at specific skills, it may not reflect how well the models can handle instructions from real users, which cover highly diverse requests and are often open-ended. For example, the popular ShareGPT dataset contains instances of users asking for programming help, resume formatting tips, educational role-playing, pronunciation suggestion, fanfiction writing, and more. We evaluate such open-ended instructability of models using both model-based evaluation (SS4.2) and human evaluation (SS4.3), both of which consist of multiple test sets from existing studies [47; 8; 26; 3; 19].

**Safety** is of particular concern regarding the fast-developing language models to ensure the ethical and proper use of them. Following LLAMa-2 [45], we employ ToxiGen [21] to measure the amount of toxic language and hate speech generation across different groups when the models are prompted to do so. We also adopt TruthfulQA [30] to measure how well models can avoid generating known falsehoods due to misconceptions or false beliefs while providing useful information.

For all the benchmark-based evaluations, we follow their standard metrics, while we subsample some benchmarks to a reasonable size to improve the efficiency of doing chain-of-thought reasoning. We refer the reader to Appendix SSA for the setup details.

### Model-Based Evaluation using GPT-4

To evaluate the open-ended instructability, we first adopt a model-based approach introduced in AlpacaEval [27]. The test set consists of 805 instructions, with 252 instructions from the Self-Instruct evaluation [47], 188 from the Open Assistant evaluation [26], 129 from the helpful evaluation by Anthropic [3], 80 from the Vicuna evaluation [8], and 156 from the Koala evaluation [19].

We use their simulated GPT-4 annotator, which computes the win rate of the testing model as judged by GPT-4 when compared to the outputs produced by Davinci-003. We use the AlpacaEval codebase and prompts [27] to make our scores directly comparable to those on the AlpacaEval leaderboard4 When doing pairwise comparisons with GPT-4, the orders of model outputs are randomized to avoid position bias during evaluation [46]. We do not evaluate vanilla LLAMA models due to them having little instruction-following ability without further prompt engineering.

Footnote 4: https://tatsu-lab.github.io/alpaca_eval/

### Human Evaluation

To further test the quality of the open-ended generations, we conduct a human evaluation based on 332 instructions that combine the Self-Instruct evaluation set [47] and Vicuna evaluation set [8]. Inspired by Bai et al. [3], we design a similar interface (Figure 5) for gathering human judgments of model outputs along the following dimensions. We note that we evaluated based on our fine-tuned LLAMa-1 models, as LLAMA-2 was not available at the time of this experiment.

**Individual acceptability.** We ask human raters to assess whether each system's responses were acceptable in isolation. This is a binary decision, and we ask the raters to mark a response as acceptable if and only if the response answered the request in the query, had no significant errors, and did not have repetitive information.

**Pairwise preference.** We then ask humans to compare the outputs of two systems and select which one they think is more helpful. This is a 5-way decision, and the raters could select if one of the responses is "clearly" or "slightly" better than the other or if it is a tie implying that both responses were equally good or bad.

To get a more reliable evaluation, we recruited a group of 18 expert annotators who are researchers at AI2 or students at UW. All of them are fluent English speakers, holding bachelor's degrees or above.

## 5 Results

### Analysis of Instruction Tuning Datasets and Base Models

To understand how the instruction datasets listed in Table 1 contribute to model abilities, we evaluated LLaMa 13B models trained on these datasets using our evaluation suite. Table 3 shows the results on our benchmark evaluation set, with more extensive results in App. F. We find that:

**There is not a single best instruction tuning dataset across all tasks**. Different datasets enable different capabilities in the model. Noteworthy examples include training on CoT being particularly helpful for mathematical reasoning in GSM and Code-Alpaca being helpful for Codex-Eval. We hypothesize that success on these tasks, which are significantly different from the rest of the evaluation tasks, calls for training sets where these tasks are well-represented. Apart from constructing task-specific datasets manually, distilling task-specific data from large models also appears to be an effective way to ensure this (e.g., CodeAlpaca is distilled from Davinci-003).

**Combining datasets results in the best overall performance on the benchmark tasks.** While models trained on our combination datasets are often not the best model for a single task (being the best only in 2 out of 6 evaluation settings), they are the best when measuring average performance across tasks. This suggests that future work into better dataset mixing or instruction-tuning modular

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & **MMLU** & **GSM** & **BBH** & **TydQA** & **Codes-Eval** & **AlpacaEval** & **Average** \\  & **(factuality)** & **(reasoning)** & **(reasoning)** & **(multilinguality)** & **(coding)** & **(open-ended)** & **Average** \\ \cline{2-8}  & **EM** & **EM** & **EM** & **F1** & **P@10** & **Win \%** vs** & \\  & **(0-shot)** & **(8-shot, CoT)** & **(3-shot, CoT)** & **(1-shot, GP)** & **(0-shot)** & **Davinci-003** & & \\ \hline Pythia 6.9B & 34.8 & 16.0 & 29.2 & 32.8 & 20.9 & 23.5 & 26.2 \\ OPT 6.7B & 32.6 & 13.5 & 27.9 & 24.1 & 8.9 & 25.9 & 22.2 \\ LLAMA 7B & 44.8 & 25.0 & 38.5 & 43.5 & 29.1 & 48.6 & 38.3 \\ LLAMA-2 7B & **49.2** & **37.0** & **44.2** & **52.8** & **33.9** & **57.3** & **45.7** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Performance of different base models after training on the Human+GPT data mixture.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & **MMLU** & **GSM** & **BBH** & **TydQA** & **Code-Eval** & **AlpacaEval** & **Average** \\  & **(factuality)** & **(reasoning)** & **(reasoning)** & **(multilinguality)** & **(coding)** & **(open-ended)** & **Average** \\  & **EM** & **EM** & **EM** & **F1** & **P@10** & **Win \%** & \\  & **(0-shot)** & **(8-shot, CoT)** & **(3-shot, CoT)** & **(1-shot, GP)** & **(0-shot)** & **Davinci-003** & & \\ \hline Vanilla LLaMa 13B & 42.3 & 14.5 & 39.3 & 43.2 & 28.6 & - & - \\ +SuperNI & 49.7 & 40.0 & 43.5 & **50.2** & 12.9 & 4.2 & 20.9 \\ +CoT & 44.2 & 40.0 & 41.9 & 47.8 & 23.7 & 6.0 & 33.9 \\ +Flan V2 & **50.6** & 20.0 & 40.8 & 47.2 & 16.8 & 3.2 & 29.8 \\ +Dolly & 45.6 & 18.0 & 28.4 & 46.5 & 31.0 & 13.7 & 30.5 \\ +Open Assistant 1 & 43.3 & 15.0 & 39.6 & 33.4 & 31.9 & 58.1 & 36.9 \\ +Self-instruct & 30.4 & 11.0 & 30.7 & 41.3 & 12.5 & 5.0 & 21.8 \\ +Unatural Instructions & 46.4 & 8.0 & 33.7 & 40.9 & 23.9 & 8.4 & 26.9 \\ +Alpaca & 45.0 & 9.5 & 36.6 & 31.1 & 29.9 & 21.9 & 29.0 \\ +Code-Alpaca & 42.5 & 13.5 & 35.6 & 38.9 & 34.2 & 15.8 & 30.1 \\ +GPT4-Alpaca & 46.9 & 16.5 & 38.8 & **23.5** & **36.6** & 63.1 & 37.6 \\ +Baze & 43.7 & 10.0 & 38.7 & 33.6 & 28.7 & 21.9 & 29.4 \\ +ShareGPT & 49.3 & 27.0 & 40.4 & 30.5 & 34.1 & **70.5** & 42.0 \\ +Human data mix. & 50.2 & 38.5 & 39.6 & 47.0 & 25.0 & 35.0 & 39.2 \\ +Human+GPT data mix. & 49.3 & **40.5** & **43.3** & 45.6 & 35.9 & 56.5 & **45.2** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparison of different instruction tuning datasets, showing that different instruction-tuning datasets can excel in different aspects, and mixtures perform best on average. Cells are blue if the finetuning boosts the vanilla LLAMA performance, and orange if the finetuning hurts the performance.

models (e.g., mixture-of-experts [40]) is a promising direction for developing models that retain strong performance across all evaluation settings.

**Base model quality is extremely important for downstream performance.** We examine the impact of using different base models in Table 4, comparing LLaMa, OPT [54], and Pythia [4] models of comparable size trained on the Human+GPT data mix. Across all evaluation settings, we find that using LLaMa performs best by a significant margin, likely due to the fact that LLaMa is pretrained on significantly more tokens than the other models (see Table 2). This suggests that models pretrained on larger (or potentially higher-quality) corpora are preferable as base models for instruction tuning. The later addition of LLaMa-2 confirms this finding by showing a significant improvement can come from only the base model upgrade.

**Some datasets degrade vanilla model performance.** Notably, most datasets we evaluate cause degradation in performance on GSM and TydiQA over the vanilla base model. We hypothesise this is due to data style and quality. Many of the datasets we examine contain little to no examples of chain-of-thought-style reasoning and contain little to no multilingual data. As such, training on these datasets likely results in some forgetting of the CoT or multilingual abilities previously held by the model, resulting in degraded performance. Additionally, we note that self-instruct appears to cause degradations across most tasks, which we hypothesise is due to the relatively poor quality of the original self-instruct data, being generated by a weaker model (base GPT-3) than the other GPT-distilled datasets.

### Pushing the Limits of Open Models

Having established that (a) using a broad mix of data is best, and (b) using LLaMa as the base model is preferable to other open alternatives, we compare the performance of models trained on the Human+GPT data mix (Tulu models) across all LLaMa sizes in Table 5. We find that:

**Instruction tuning brings large benefits on top of LLaMa models at all sizes.** On average, all LLaMa models improve considerably after instruction tuning.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & **MMLU** & **GSM** & **BBH** & **TydiQA** & **Codex-Eval** & **AlpacaEval** & **Average** \\  & **(factuality)** & **(reasoning)** & **(reasoning)** & **(multilinguality)** & **(coding)** & **(open-ended)** & **Average** \\ \hline  & **EM** & **EM** & **EM** & **F1** & **P@0** & **Win \% vs** & \\  & **(0-shot)** & **(8-shot, CoT)** & **(3-shot, CoT)** & **(1-shot, GP)** & **(0-shot)** & **Davinci-003** & \\ \hline \multicolumn{6}{c}{**Vanilla LLaMa models \(\downarrow\)**} \\ \hline LLaMa 7B & 31.5 & 10.0 & 33.0 & 38.4 & 20.5 & - & - \\ LLaMa 13B & 42.3 & 14.5 & 39.3 & 43.2 & 28.6 & - & - \\ LLaMa 30B & 54.6 & 36.0 & 49.5 & 55.3 & 42.8 & - & - \\ LLaMa 65B & 58.7 & 50.0 & 58.1 & 56.8 & 46.9 & - & - \\ LLaMa-2 7B & 41.8 & 12.0 & 39.3 & 51.2 & 26.8 & - & - \\ LLaMa-2 13B & 52.0 & 25.0 & 48.9 & 56.5 & 32.5 & - & - \\ \hline \multicolumn{6}{c}{**65B models trained on alternate data mixtures \(\downarrow\)**} \\ \hline ShareGPT 65B & 61.3 (+2.6) & 59.0 (+9.0) & 55.8 (-2.3) & 31.6 (-25.2) & 56.2 (+9.3) & 73.6 & 56.3 \\ Human mix. 65B & 60.4 (+1.7) & 60.0 (+10.0) & 54.8 (-3.3) & 58.3 (+1.7) & 44.6 (-2.3) & 43.4 & 53.6 \\ \hline \multicolumn{6}{c}{**\(\#\) models trained on our final Human+GPT data mixture \(\downarrow\)**} \\ \hline Tulu \(\textcircled{\char 7}\)B & 44.8 (+13.3) & 25.0 (+15.0) & 38.5 (+5.5) & 43.5 (+5.1) & 29.1 (+8.6) & 48.6 & 38.3 \\ Tulu \(\textcircled{\char 7}\)B & 49.3 (+7.0) & 40.5 (+26.0) & 43.3 (+4.0) & 45.6 (+2.4) & 35.9 (+7.3) & 56.5 & 45.2 \\ Tulu \(\textcircled{\char 30B}\) & 57.7 (+3.1) & 53.0 (+17.0) & 51.9 (+2.4) & 51.9 (-3.4) & 48.0 (+5.2) & 62.3 & 54.1 \\ Tulu \(\textcircled{\char 65B}\) & 59.2 (+0.5) & 59.0 (+9.0) & 54.4 (+3.7) & 56.6 (+0.2) & 49.4 (+2.5) & 61.8 & 56.7 \\ \hline \multicolumn{6}{c}{**\(\#\) models trained on our final Human+GPT data mixture using LLaMa-2 \(\downarrow\)**} \\ \hline Tulu-1.1 \(\textcircled{\char 7}\)B & 49.2 (+7.4) & 37.0 (+25.0) & 44.2 (+4.9) & 52.8 (+1.6) & 33.9 (+7.1) & 57.3 & 45.7 \\ Tulu-1.1 \(\textcircled{\char 13B}\) & 52.3 (+0.3) & 53.0 (+28.0) & 50.6 (+1.7) & 58.8 (+2.3) & 38.9 (+7.4) & 64.0 & 52.9 \\ \hline \multicolumn{6}{c}{**Proprietary models \(\downarrow\)**} \\ \hline ChatGPT & 67.9 & 76.0 & 66.1 & 51.9 & 88.4 & 83.6 & 72.3 \\ GPT-4 & 82.4 & 92.5 & 88.0 & 70.8 & 94.1 & 93.5 & 86.9 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Performance of Tulu and other of our trained models to vanilla LLaMa models and the state-of-the-art proprietary models across evaluation settings. See Table 8 for a complete list.

**Smaller models benefit most from instruction tuning.** We find that relative improvements from instruction tuning are largest for the smallest models, and shrink as models get larger. Notably, the 65B LLaMA model performs comparably or better than the 65B Tulu model on MMLU, BBH, and TydiQA. This suggests that **instruction-tuning does not help to enhance strong capabilities already present in the original model**, and also highlights that care must be taken during finetuning to avoid forgetting the base model's original capabilities.

**Tulu still lags behind state-of-the-art proprietary models.** Despite the impressive performance of Tulu 65B, it lags behind ChatGPT and GPT-4 in all evaluation settings, contrary to prior claims that models trained on these open resources can match ChatGPT [56, 8]. We note **we cannot discount the possibility that either ChatGPT or GPT-4 was trained on significant portions of our evaluation suite.** However, the presence of a significant gap between Tulu models and ChatGPT matches our findings in the model and human-based evaluations, which are less likely to be compromised.

### Evaluation of Potential Risks and Harms

We evaluate our models on ToxiGen and TruthfulQA to measure the degree to which different datasets are likely to yield models that generate toxic language or misinformation. We find that:

**Trends remain similar to capability-focused benchmarks.** Similarly to the results in Sec. 4.1, we find that GPT-distilled datasets yield the best overall performance and that there is a large variance in performance across datasets.

**Models trained on GPT-sourced data yield less toxic generations than GPT.** Larger models trained on GPT-distilled data appear to refuse to produce toxic generations almost entirely, despite the fact that ChatGPT and GPT-4 produce toxic generations a non-trivial amount of the time. We hypothesise this is due to our models overfitting on refusal-style behaviour, refusing to generate anything moderately toxic, while GPT models balance refusal behaviour with helpfulness to a greater extent.

**TruthfulQA performance does not scale.** Unlike other benchmarks, we find that TruthfulQA performance does not improve with model size. Further examining this, we find that larger models do output more correct facts, but also tend to provide large improvements to the model capabilities tested in Table 3.

**Datasets that encourage long, diverse generations perform best**. Intrigued by ShareGPT's performance, we plot the average number of unique tokens in model generations against the AlpacaEval win-rate in Figure 2. We find that the evaluation is **strongly correlated with the average number of unique tokens** (Pearson correlation of 0.96, \(p\ll 0.05\)). Given GPT-4's strong performance on other tasks, we do not believe that GPT-4 evaluation is merely counting unique tokens, but this result highlights how model preference scores do not necessarily reward only model capabilities.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & \multicolumn{2}{c}{**ToxiGen (\(\downarrow\))**} & \multicolumn{2}{c}{**TruthfulQA (\(\uparrow\))**} \\ \hline
**Model \(\downarrow\)** & **7B** & **13B** & **7B** & **13B** \\ \hline LLaMA & 85.4 & 82.6 & 26.2 & 23.6 \\ + SuperNI & 85.3 & 77.3 & 26.7 & 26.2 \\ + CoT & 63.0 & 43.9 & 35.1 & 35.5 \\ + Flan V2 & 77.5 & 61.4 & 33.2 & 33.4 \\ + Dolly & 72.1 & 78.9 & 30.1 & 32.9 \\ + Open Assistant 1 & 39.2 & 5.2 & 40.9 & 48.6 \\ + Self-instruct & 89.0 & 89.3 & 22.4 & 22.4 \\ + Unnatural Inst. & 35.8 & 55.7 & 27.3 & 31.7 \\ + Alpaca & 63.2 & 58.1 & 33.5 & 39.8 \\ + Code-Alpaca & 84.3 & 92.0 & 25.1 & 26.7 \\ + GPT-Alpaca & **3.9** & 1.2 & **51.2** & 56.7 \\ + Baize & 77.2 & 41.2 & 42.4 & 43.9 \\ + ShareGPT & 5.5 & 2.5 & 45.3 & **60.0** \\ \hline + Human mix. & 51.8 & 76.9 & 34.1 & 32.1 \\ + Tulu\(\not\)\(\cong\) & 10.6 & **0.1** & 44.6 & 41.6 \\ \hline ChatGPT & 27.7 & & 75.2 \\ GPT-4 & 10.6 & & 82.3 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Performance of models on ToxiGen (% toxic generations, lower is better) and TruthfulQA (% truthful and informative answers, higher is better). See Table 9 and Table 10 for the full breakdown of these two evaluations.

[MISSING_PAGE_FAIL:9]

Related Work

Instruction Tuning of LMsFinetuning language models on diverse instruction sets alongside regular samples has been shown to greatly improve zero-shot performance on unseen tasks [39; 51; 49; 32; 9; 48], and serves as a good base for further finetuning in supervised settings [31]. Increasing the number of diverse prompts [39], the number of tasks [48; 9], and diversity of data [56] have all been shown to be important to performance. More recently, a growing number of models have made use of model-generated instruction-augmented data [47; 23; 25; 53], most often generated or collected from larger proprietary models such as ChatGPT or GPT-4 [8; 15; 43; 52; 36; inter alia]. Despite the explosion of models and datasets, evaluation remains inconsistent and difficult, with different evaluation setups used across models. Prior work has examined models trained on varying dataset sources with the aim of identifying 'the best mixture' [31; 24], but is often limited to examining only benchmark performance, and covers a smaller number of instruction sources than in this work. QLoRA [14] also explores (quantized and parameter-efficient) instruction-tuning of recent models and datasets, but explores a smaller range of models, datasets, and evaluations than this work.

Evaluation of LMsGiven the success of LMs on NLP and instruction-following tasks, many evaluation frameworks have been proposed. Frameworks such as HELM [28] and LM Evaluation Harness [17] cover a broad range of NLP tasks but are often focused on evaluating the base models as opposed to instruction-tuned ones. Similar to our work, Chung et al. [9] focus on a series of benchmark evaluations focused around factuality and reasoning, but largely neglect open-ended instruction following abilities. Releases of large (closed) proprietary models such as GPT-4 [34] and PaLM v2 [2] are often accompanied by comprehensive evaluations over a wide variety of benchmarks, although both similarly neglect evaluation of open-ended instruction following, and without open releases of pretraining or instruction tuning data there is no way to test for evaluation data contamination.

Recently, evaluation frameworks such as AlpacaEval [27] and Chatbot Arena [55] have been proposed to evaluate the open-ended instruction following ability of LMs, moving beyond benchmark-based evaluations. These either make use of other models (in the case of AlpacaEval) or humans (in the case of Chatbot Arena) as annotators for judging model generations. We make use of this recent work and evaluate our models on traditional benchmarks, model-based evaluation, and human-based evaluation. Concurrent to this work, Gudibande et al. [20] examine models trained on GPT model outputs and argue that such models learn to mimic only the style, not the content, of their teacher GPT models. While we similarly find that existing datasets fail to train models close to strong proprietary models, the diversity of performance we observe across datasets suggests that significant performance improvements can be achieved through imitation data, so long as it contains a diverse and wide-ranging set of skills and domains.

## 7 Conclusion

In this work, we provide an extensive evaluation of a wide variety of publicly-available resources for instruction-tuning models, and compare them to the strongest proprietary models currently available. We find that using strong base models is vital to performance, combining datasets works best on average (but does result in slight performance drops compared to best performance in specific tasks), and our strongest open models do not yet match ChatGPT or GPT-4. Furthermore, we believe that our evaluation highlights the need for the continued development of strong base models and broader, diverse datasets. Finally, we hope that our evaluation and released code and models enable more comprehensive evaluations and spur research to close these gaps and shed insights on all large language models, closed or open.

## Acknowledgments

Work at UW was partially supported by the Office of Naval Research under MURI grant N00014-18-1-2670, Defense Advanced Research Projects Agency (DARPA) under Contract No. FA8650-23-C-7316 and MCS program through NIWC Pacific (N66001-19-2-4031), NSF IIS-2044660, and a gift from Apple. We thank colleagues at AI2 and UW NLP for their constructive feedback and intellectual support. We are particularly grateful to Tim Dettmers for his suggestions on efficient inference techniques, and Artidoro Pagnoni for providing the reproduced FLAN V2 dataset. We also acknowledge support from AMD and CSC's LUMI cluster, and the Beaker team at AI2, which provided the essential computational infrastructure for our experiments. Finally, we are sincerely thankful for the following contributors to our human evaluation: Valentina Pyatkin, Clara Na, Yuling Gu, Yuchen Lin, Haiyan He, David Graham, Hao Peng, Hyunwoo Kim, Alisa Liu, Youngjae Yu, Tal August, and Egor Klevak.

## References

* Almazrouei et al. [2023] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru, M. Debbah, E. Goffinet, D. Heslow, J. Launay, Q. Malartic, B. Noune, B. Pannier, and G. Penedo. Falcon-40B: an open large language model with state-of-the-art performance. Huggingface Model Release, 2023. URL https://huggingface.co/titiuae/falcon-40b.
* Anil et al. [2023] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al. Palm 2 technical report. _arXiv preprint arXiv:2305.10403_, 2023.
* Bai et al. [2022] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. _arXiv preprint arXiv:2204.05862_, 2022.
* Biderman et al. [2023] S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O'Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In _International Conference on Machine Learning_, pages 2397-2430. PMLR, 2023.
* Cai et al. [2023] T. Cai, X. Wang, T. Ma, X. Chen, and D. Zhou. Large language models as tool makers. _arXiv preprint arXiv:2305.17126_, 2023.
* Chaudhary [2023] S. Chaudhary. Code alpaca: An instruction-following llama model for code generation. GitHub repository, 2023. URL https://github.com/sahil280114/codealpaca.
* Chen et al. [2021] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. _arXiv preprint arXiv:2107.03374_, 2021.
* Chiang et al. [2023] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgtp quality. Blog post, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.
* Chung et al. [2022] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma, et al. Scaling instruction-finetuned language models. _arXiv preprint arXiv:2210.11416_, 2022.
* Clark et al. [2020] J. H. Clark, E. Choi, M. Collins, D. Garrette, T. Kwiatkowski, V. Nikolaev, and J. Palomaki. TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages. _TACL_, 2020. URL https://arxiv.org/abs/2003.05002.
* Cobbe et al. [2021] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. _arXiv preprint arXiv:2110.14168_, 2021.
* Databricks [2023] Databricks. Free dolly: Introducing the world's first truly open instruction-tuned llm. Blog post, 2023. URL https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm.
* Dettmers et al. [2022] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. LLM.int8(0): 8-bit matrix multiplication for transformers at scale. In _Advances in Neural Information Processing Systems_, 2022.
* Dettmers et al. [2023] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of quantized llms. _arXiv preprint arXiv:2305.14314_, 2023.

* Ding et al. [2023] N. Ding, Y. Chen, B. Xu, S. Hu, Y. Qin, Z. Liu, M. Sun, and B. Zhou. Ultrachat: A large-scale auto-generated multi-round dialogue data. GitHub Repository, 2023. URL https://github.com/thunlp/ultrachat.
* Dubois et al. [2023] Y. Dubois, X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin, P. Liang, and T. B. Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. _arXiv preprint arXiv:2305.14387_, 2023.
* Gao et al. [2021] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, K. McDonell, N. Mueminghoff, J. Phang, L. Reynolds, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. A framework for few-shot language model evaluation, Sept. 2021. URL https://doi.org/10.5281/zenodo.5371628.
* Geng and Liu [2023] X. Geng and H. Liu. Openllama: An open reproduction of llama. GitHub Repository, 2023. URL https://github.com/openlm-research/open_llama.
* Geng et al. [2023] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine, and D. Song. Koala: A dialogue model for academic research. Blog post, April 2023. URL https://bair.berkeley.edu/blog/2023/04/03/koala/.
* Gudibande et al. [2023] A. Gudibande, E. Wallace, C. Snell, X. Geng, H. Liu, P. Abbeel, S. Levine, and D. Song. The false promise of imitating proprietary llms. _arXiv preprint arXiv:2305.15717_, 2023.
* Hartvigsen et al. [2022] T. Hartvigsen, S. Gabriel, H. Palangi, M. Sap, D. Ray, and E. Kamar. TOXIGEN: Controlling Language Models to Generate Implied and Adversarial Toxicity. In _ACL_, 2022. URL https://arxiv.org/abs/2203.09509.
* Hendrycks et al. [2020] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. In _International Conference on Learning Representations_ (ICLR), 2020.
* Honovich et al. [2022] O. Honovich, T. Scialom, O. Levy, and T. Schick. Unnatural instructions: Tuning language models with (almost) no human labor. _arXiv preprint arXiv:2212.09689_, 2022.
* Iyer et al. [2022] S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang, Q. Liu, P. S. Koura, et al. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. _arXiv preprint arXiv:2212.12017_, 2022.
* Koksal et al. [2023] A. Koksal, T. Schick, A. Korhonen, and H. Schutze. Longform: Optimizing instruction tuning for long text generation with corpus extraction. _arXiv preprint arXiv:2304.08460_, 2023.
* Kopf et al. [2023] A. Kopf, Y. Kilcher, D. von Rutte, S. Anagnostidis, Z.-R. Tam, K. Stevens, A. Barhoum, N. M. Duc, O. Stanley, R. Nagyfi, et al. Openassistant conversations-democratizing large language model alignment. _arXiv preprint arXiv:2304.07327_, 2023.
* Li et al. [2023] X. Li, T. Zhang, Y. Dubois, R. Taori, I. Gulrajani, C. Guestrin, P. Liang, and T. B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. Github repository, 2023. URL https://github.com/tatsu-lab/alpaca_eval.
* Liang et al. [2022] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Kumar, B. Newman, B. Yuan, B. Yan, C. Zhang, C. Cosgrove, C. D. Manning, C. R'e, D. Acosta-Navas, D. A. Hudson, E. Zelikman, E. Durmus, F. Ladhak, F. Rong, H. Ren, H. Yao, J. Wang, K. Santhanam, L. J. Orr, L. Zheng, M. Yuksekgonul, M. Suzgun, N. S. Kim, N. Guha, N. S. Chatterji, O. Khattab, P. Henderson, Q. Huang, R. Chi, S. M. Xie, S. Santurkar, S. Ganguli, T. Hashimoto, T. F. Icard, T. Zhang, V. Chaudhary, W. Wang, X. Li, Y. Mai, Y. Zhang, and Y. Koreeda. Holistic evaluation of language models. _Annals of the New York Academy of Sciences_, 2022.
* Lin et al. [2022] S. Lin, J. Hilton, and O. Evans. TruthfulQA: Measuring how models mimic human falsehoods. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 3214-3252, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.229. URL https://aclanthology.org/2022.acl-long.229.

* [30] S. Lin, J. Hilton, and O. Evans. Truthfulqa: Measuring how models mimic human falsehoods. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 3214-3252, 2022.
* [31] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay, D. Zhou, Q. V. Le, B. Zoph, J. Wei, et al. The fian collection: Designing data and methods for effective instruction tuning. _arXiv preprint arXiv:2301.13688_, 2023.
* [32] S. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi. Cross-Task Generalization via Natural Language Crowdsourcing Instructions. In _Annual Meeting of the Association for Computational Linguistics (ACL)_, 2022.
* [33] MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms. Blog post, 2023. URL https://www.mosaicml.com/blog/mpt-7b.
* [34] OpenAI. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [35] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training Language Models to Follow Instructions with Human Feedback. In _Advances in Neural Information Processing Systems_ (NeurIPS), 2022.
* [36] B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with gpt-4. _arXiv preprint arXiv:2304.03277_, 2023.
* [37] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training trillion parameter models. In _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis_, SC '20. IEEE Press, 2020. ISBN 9781728199986.
* [38] J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, 2020.
* [39] V. Sanh, A. Webson, C. Raffel, S. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, A. Raja, M. Dey, M. S. Bari, C. Xu, U. Thakker, S. S. Sharma, E. Szczechla, T. Kim, G. Chhablani, N. Nayak, D. Datta, J. Chang, M. T.-J. Jiang, H. Wang, M. Manica, S. Shen, Z. X. Yong, H. Pandey, R. Bawden, T. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli, T. Fevry, J. A. Fries, R. Teehan, T. L. Scao, S. Biderman, L. Gao, T. Wolf, and A. M. Rush. Multitask Prompted Training Enables Zero-Shot Task Generalization. In _International Conference on Learning Representations_ (ICLR), 2022.
* [40] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In _International Conference on Learning Representations_, 2017.
* [41] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, and et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. _arXiv preprint arXiv:2206.04615_, 2022.
* [42] M. Suzgun, N. Scales, N. Scharli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. _arXiv preprint arXiv:2210.09261_, 2022.
* [43] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto. Stanford alpaca: An instruction-following llama model. GitHub repository, 2023. URL https://github.com/tatsu-lab/stanford_alpaca.
* [44] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [45] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.

* Wang et al. [2023] P. Wang, L. Li, L. Chen, D. Zhu, B. Lin, Y. Cao, Q. Liu, T. Liu, and Z. Sui. Large language models are not fair evaluators. _arXiv preprint arXiv:2305.17926_, 2023.
* Wang et al. [2022] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct: Aligning language model with self generated instructions. _arXiv preprint arXiv:2212.10560_, 2022.
* Wang et al. [2022] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Arunkumar, A. Ashok, A. S. Dhanasekaran, A. Naik, D. Stap, et al. Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ Tasks. In _EMNLP_, 2022.
* Wei et al. [2022] J. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned Language Models are Zero-Shot Learners. In _International Conference on Learning Representations_ (ICLR), 2022.
* Wei et al. [2022] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. _arXiv preprint arXiv:2201.11903_, 2022.
* Weller et al. [2020] O. Weller, N. Lourie, M. Gardner, and M. E. Peters. Learning from task descriptions. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 1361-1375, Online, Nov. 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.105. URL https://aclanthology.org/2020.emnlp-main.105.
* Xu et al. [2023] C. Xu, D. Guo, N. Duan, and J. McAuley. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. _arXiv preprint arXiv:2304.01196_, 2023.
* Xu et al. [2023] C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, and D. Jiang. Wizardlm: Empowering large language models to follow complex instructions. _arXiv preprint arXiv:2304.12244_, 2023.
* Zhang et al. [2022] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, et al. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_, 2022.
* Zheng et al. [2023] L. Zheng, Y. Sheng, W.-L. Chiang, H. Zhang, J. E. Gonzalez, and I. Stoica. Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings. Blog post, May 2023. URL https://lmsys.org/blog/2023-05-03-arena/.
* Zhou et al. [2023] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat, P. Yu, L. Yu, et al. Lima: Less is more for alignment. _arXiv preprint arXiv:2305.11206_, 2023.

**Supplementary Material**

## Appendix A Limitations

Despite the comprehensiveness of our evaluations, we note that we did not exhaustively cover all possible evaluations: for example, we do not explicitly evaluate models on their multi-turn dialogue abilities nor their summarization abilities. Instead, we focus on a core set of capabilities we believe important, and cover broad open-ended tasks via our model and human preference-based evaluations.

We also note that we do not cover all possible instruction datasets and open models released recently, due to the computational cost of doing this. Instead, we focus on a wide set of datasets we believe are broadly representative of the type of open instruction datasets available (human-authored, skill-targeted, GPT-distilled, etc), and focused on the strongest base model widely available when performing experiments. Future work could investigate whether more recent strong base models (e.g., the Falcon model [1]), or other instruction datasets, perform significantly better or differently from the models explored in this work.

Finally, we note that open-ended instruction-based evaluation is highly subjective and difficult due to its extremely open-ended nature. There is likely no one answer that is definitively the best for any given query, and different annotators (whether they be human or model) will have different biases and preferences. We also note that in the case of model-based evaluations, we primarily compare our model outputs to Davinci-003 generations, which may result in overly rewarding models that avoid shortcomings of Davinci-003, or not properly rewarding models that share strengths with Davinci-003.

Despite not being completely exhaustive in this work, we believe that by covering a broad range of models, it still serves as a useful and important contribution in showing what type of open resources work, and where future community efforts should go (better base models, more diverse instruction-tuning datasets).

## Appendix B Broader Impact

We believe that a rigorous evaluation of existing resources is broadly positive, exposing the strengths and deficiencies of currently widely-available resources. Furthermore, as all resources used are widely available, the harm posed by training these models is fairly small. We do note that training and releasing especially large instruction-tuned models without well-tested guides carries a degree of risk, and such initially release our largest models with a gated setup (requiring users to apply for access and be manually approved) to limit potential harms.

## Appendix C Instruction Datasets Details

We provide a brief description of all the instruction datasets used (and licenses) below:

* **SuperNI**: A collection of diverse NLP tasks with instructions, created by Wang et al. [48]. The dataset uses the Apache-2.0 license.
* **CoT**: A collection of datasets annotated with chain-of-thoughts [50]. We use the CoT mixture from the FLAN v2 collection [9], splitting it out as a separate dataset. The FLAN mixture is released under the Apache-2.0 license, although the component datasets may not use this license.
* **Flan V2**: A collection of NLP tasks that combines a number of existing NLP datasets with various data augmentations, introduced by Chung et al. [9]. The mixture is released under the Apache-2.0 license, although the component datasets may not use this license.
* **Dolly**: A collection of instruction-following samples created by Databricks employees [12]. The dataset is released under the Creative Commons Attribution-ShareAlike 3.0 Unported License.
* **Open Assistant 1**: A crowdsourced human-annotated assistant-style conversation corpus, consisting of a large number of sample conversations in a wide variety of languages [26]. The dataset is released under the Apache-2.0 license.
* **Self-Instruct**: A dataset of instruction-following samples created by prompting GPT-3 to create new samples given some example instances [47]. The dataset is released under the Apache-2.0 license.

* **Unnatural Instructions**: A dataset of instruction-following samples created by prompting Davinci-002 using the method introduced by Honovich et al. [23]. The dataset is released under the MIT license.
* **Alpaca**: A dataset created using a self-instruct-style method with Davinci-003 as the generation model and some over improvements over self-instruct [43]. The dataset is released under a Attribution-NonCommercial 4.0 International (CC BY-NC 4.0) license.
* **Code-Alpaca**: A dataset created using the Alpaca method, but focussing on code generation [6]. The dataset is released under the Apache-2.0 license.
* **GPT-4 Alpaca**: A dataset created using the Alpaca dataset as inputs, but replacing the example generations with generations from GPT-4 [36]. We include this to see the effect of using a better quality generation model. The dataset is released under the Apache-2.0 license.
* **Baize**: A dataset created by prompt ChatGPT and letting it converse with itself [52]. The dataset is released under the GNU General Public License v3.0.
* **ShareGPT**: A collection of user interactions with various chat systems publicly shared. We use the 'html-cleaned' variant available at https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/tree/main/HTML_cleaned_raw_dataset. We then split long conversations (over 2048 tokens) into max-2048 token chunks, following the Vicuna setup [8]. We do not do any further filtering of samples. This dataset is released under the Apache-2.0 license.

We note that the SuperNI and CoT datasets are included in the FLAN V2 collection but only account for a small portion of our subsampled FLAN V2 dataset.

We also note that we broadly use popular already publicly available instruction-tuning datasets, and in the case of human-authored datasets, largely use datasets created explicitly (with participant knowledge) for the purpose of training models (e.g., Dolly, Open Assistant 1). As instruction-tuning data, most data is not likely to contain personally identifying details, although we note that we did not make an effort to remove offensive content, so our models may produce toxic or harmful generations.

## Appendix D Model Training Details and Compute

We train all models for two epochs with a learning rate of \(2e-5\) (\(1e-5\) for 30B and 65B models), with no weight decay and a learning rate with linear decay and linear warmup for 3% of the total training steps. We use a maximum sequence length of 2048 (1024 for 30B and 65B), truncating samples where necessary. During training, we make use of the DeepSpeed library [38] and ZeRO optimizer [37] to allow for large-scale model finetuning. In all cases, we fully finetune models. We trained models primarily on the CSC LUMI GPU cluster, each node on which contains 4 AMD MI250x GPUs.

## Appendix E Evaluation Setups

We provide further details on the evaluation setups used below. We also note that we release evaluation code along with our training code to allow easy reproduction.

* **MMLU**: We use the official MMLU evaluation script and prompts available at https://github.com/hendrycks/test, with modifications to allow for batch processing. We evaluate using 0 and 5 few-shot examples, following the original setup of MMLU.
* **GSM**: We evaluate models on the test set of GSM. Following Wei et al. [50], we evaluate with and without chain-of-thought (CoT vs Direct). Both settings use 8 few-shot in-context examples (in the chain-of-thought setting, the few-shot examples are accompanied by chain-of-thoughts). Because all answers in GSM are numbers, we extract the last number in the model response as the final answer. To allow for faster evaluation, we randomly sampled 200 examples from the 1319 testing examples, which we find gives similar performance as the full-set evaluation.
* **BBH**: We follow the setup described in the original paper Suzgun et al. [42], and evaluate with and without chain-of-thought (CoT vs Direct). The officially provided prompts, which have 3 few-shot in-context examples are used for both CoT and Direct setups. For the CoT setup, we extract the first word after the phrase 'So the answer is', or the entire response if there is no such substring present.

* **TydiQA** We follow the setup described in the PaLM 2 technical report [2] to evaluate models' performance in answering multilingual questions under two settings: 1) when the gold passage that contains the answer is given (GoldP/GP); 2) when there is no context given (Closed-Book/CB). One in-context example is used to familiarize the model with the answering format.
* **Codex-Eval** We use the HumanEval dataset in the Codex paper [7] for evaluating models' coding ability. The dataset contains 164 programming problems, where models are prompted to complete the Python function given its docstring. Following the original paper, we compute unbiased estimates of pass@k to measure the functional correctness of models' outputs. We report both pass@1 and pass@10. The pass@1 results were obtained by sampling with a temperature of 0.1 and the pass@10 results with a temperature of 0.8.
* **ToxiGen** We follow the setup in Touvron et al. [45], but use the original set of prompts from Hartvigsen et al. [21], which are designed to elicit toxic generations for certain groups. We take only the prompts designed to produce toxic language ('hateful' prompts) and use 500 prompts per group to reduce evaluation costs. For base language models, we pass in the original ToxiGen prompts unchanged and greedily decode up to the first new line (or a maximum of 512 tokens). For instruction-tuned models, we place the prompt in the corresponding template, and ask the model to complete the prompt, until the model generates a stop token (or a maximum of 512 tokens). We pass the generated text into a roberta-large model trained to detect toxic content finetuned as part of Hartvigsen et al. [21]5. We then report the percentage of generations deemed toxic by the classifier.
* **TruthfulQA** Following Touvron et al. [45], we mainly use the generation setting of TruthfulQA [30]. The TruthfulQA dataset contains 818 questions, which are used to prompt the tested model to generate answers. We use the default QA prompt format with 6 in-context QA examples. We follow the official script in their official implementation 6 to do greedy decoding and answer postprocessing. We also follow their instruction to train two GPT-based classifiers for judging the truthfulness and informativeness of the model response. We report the rate of the responses being truthful (% Truthful), informative (% Informative), and both (% Informative and Truthful) as our metrics. Following Touvron et al. [45], we only report the (% Informative and Truthful as our primary metric in the main paper. Footnote 5: https://huggingface.co/tomh/toxigen_roberta
* **AlpacaEval** We use the package provided by Li et al. [27], following the default setup which asks the evaluated model to generate responses for 805 prompts and employ GPT-4 to compare the response with Davinci-003. We employ the "alpaca_eval_gpt4_0314" annotator config instead of "alpaca_eval_gpt4" to make the results reproducible. We allow the evaluated model to generate up to 8192 tokens, without specifying special stop sequences. The reported win-rate is the percentage of model generations that GPT-4 reports as being preferred over the generations from Davinci-003.

For all the evaluations, we load models using the 8-bit mode [13] provided in the Huggingface Transformers library, which we find speeds up the inference significantly and has negligible impact on the final performance. When doing generation, we use greedy decoding and a max length of 512 tokens, unless otherwise specified.

## Appendix F Overview of All Automatic Evaluation Results

Table 8 presents a compilation of the results of all models trained as part of this work on all the core capability evaluation benchmarks. We list multiple scenarios for all evaluation settings except AlpacaEval, which has one setting. Please refer to SE for the meanings of the reported metrics. We also calculate an average across benchmarks in Table 8. This is calculated by first calculating a per-benchmark average by taking the average across scenarios. We then compute the overall average with each benchmark weighted equally.

Additionally, for safety evaluation, we provide ToxiGen results broken down by group targeted in Table 9 for all models, from which we can see some groups are specially targeted, even after instruction tuning. We all provide full TruthfulQA results in Table 10. The results are broken down into % informative and % truthful - see Lin et al. [29] for details on these metrics.

[MISSING_PAGE_FAIL:19]

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & \% **Informative** & \% **Truthful** & \% **Informative** \\  & & & **and Truthful** \\ \hline \multicolumn{4}{c}{Proprietary models \(\downarrow\)} \\ \hline GPT-4 & 99.5 & 82.7 & **82.3** \\ ChatGPT & 96.0 & 79.2 & 75.2 \\ \hline \multicolumn{4}{c}{LLMa 65B models \(\downarrow\)} \\ \hline Vanilla LLMa & 85.8 & 45.2 & 31.2 \\ ShareGPT & 86.8 & 76.6 & **63.5** \\ Human mix & 98.0 & 42.2 & 40.4 \\ H+GPT mix (\(\widehat{\bm{\varepsilon}}\)) & 90.5 & 58.3 & 48.7 \\ \hline \multicolumn{4}{c}{LLMa 30B models \(\downarrow\)} \\ \hline Vanilla LLMaMa & 92.0 & 35.7 & 28.3 \\ ShareGPT & 71.0 & 81.4 & **52.5** \\ Human mix & 98.2 & 43.2 & 41.5 \\ H+GPT mix (\(\widehat{\bm{\varepsilon}}\)) & 92.8 & 53.2 & 46.0 \\ \hline \multicolumn{4}{c}{LLMa 13B models \(\downarrow\)} \\ \hline Vanilla LLMaMa & 95.1 & 30.8 & 26.2 \\ SuperNI & 96.8 & 27.8 & 25.1 \\ CoI & 92.7 & 41.6 & 35.5 \\ Flan V2 & 91.2 & 42.1 & 33.4 \\ Dolly & 98.8 & 34.1 & 32.9 \\ Open Assistant 1 & 91.3 & 57.2 & 48.6 \\ ShareGPT & 91.2 & 68.5 & **60.0** \\ Self-instruct & 93.4 & 28.8 & 22.4 \\ Umat. Instruct. & 84.6 & 46.9 & 31.7 \\ Alpaca & 99.9 & 39.9 & 39.8 \\ Code-Alpaca & 98.9 & 27.5 & 26.7 \\ GPT4-Alpaca & 87.5 & 69.0 & 56.7 \\ Maize & 87.9 & 56.1 & 43.9 \\ Human mix. & 98.4 & 33.3 & 32.1 \\ H+GPT mix (\(\widehat{\bm{\varepsilon}}\)) & 94.6 & 47.0 & 41.6 \\ \hline \multicolumn{4}{c}{LLMa-2 13B models \(\downarrow\)} \\ \hline Vanilla LLMaMa 2 & 99.0 & 32.1 & 31.1 \\ H+GPT mix (\(\widehat{\bm{\varepsilon}}\)) & 96.7 & 48.3 & **45.3** \\ \hline \multicolumn{4}{c}{LLMa 7B models \(\downarrow\)} \\ \hline Vanilla LLMaMa & 96.7 & 26.4 & 23.6 \\ SuperNI & 98.0 & 28.4 & 26.7 \\ CoI & 93.5 & 40.3 & 35.1 \\ Flan V2 & 96.1 & 36.1 & 33.2 \\ Dolly & 98.5 & 31.5 & 30.1 \\ Open Assistant 1 & 92.0 & 48.5 & 40.9 \\ ShareGPT & 76.4 & 68.5 & 45.3 \\ Self-instruct & 96.5 & 25.5 & 22.4 \\ Unmat. Instruct. & 89.8 & 37.0 & 27.3 \\ Alpaca & 98.8 & 34.8 & 33.5 \\ Code-Alpaca & 99.1 & 25.9 & 25.1 \\ GPT4-Alpaca & 84.2 & 66.7 & **51.2** \\ Baire & 88.5 & 53.7 & 42.4 \\ Human mix & 97.7 & 36.2 & 34.1 \\ H+GPT mix (\(\widehat{\bm{\varepsilon}}\)) & 98.2 & 46.3 & 44.6 \\ \hline \multicolumn{4}{c}{LLMa-2 7B models \(\downarrow\)} \\ \hline Vanilla LLMaMa 2 & 93.0 & 33.4 & 26.7 \\ H+GPT mix (\(\widehat{\bm{\varepsilon}}\)) & 97.7 & 43.2 & **40.0** \\ \hline \hline \end{tabular}
\end{table}
Table 10: TruthfulQA results across models. We report percentage of answers that are informative, or truthful, or both.

Human Evaluation Details

### Setup

Here we provide more details for the human evaluation described in SS4.3. Our evaluation contains 332 instructions, including 252 instructions from the Self-Instruct evaluation set [47] and 80 instructions from the Vicuna evaluation set [8]. Our evaluation is conducted for three pairs of models: 1) Tulu 65B vs ChatGPT, 2) Tulu 65B vs Tulu 7B, 3) Tulu 65B v.s. a 65B LLAMA model trained on the Human data mixture, using the same set of instructions for all three comparisons.

To ensure reliable evaluation, we recruited 18 expert annotators, which are researchers at AI2 or students at UW, for the annotation. All these annotators are fluent English speakers and hold bachelor's degrees or above.

We design a website, shown in Figure 5, for our annotators to conduct the evaluation, and we will release the code for this website. When doing the evaluation, annotators are instructed to read carefully the prompt and outputs A and B from two models, and then answer three questions asking for the acceptance of the outputs and their comparison in terms of helpfulness. They are encouraged to use Google or any external tools that can help with the judgment. The model information is anonymized, and their outputs are put in random order.

### Inter-Annotator Agreement

We measure the agreement of our annotators on a subset of 119 examples (63 instances randomly sampled from the ChatGPT3 vs Tulu 65B comparison, and 59 instances randomly sampled from the Tulu 65B vs Tulu 7B comparison). We assign two annotators for each of these examples and compute their agreement for both the acceptance evaluation and pairwise comparison evaluation.

Figure 5: The website interface for our human evaluation (see App. G for details). Users need to log in to the system, read the prompt and outputs from two models (with model names anonymized and order randomized), then answer whether output A and output B are acceptable or not individually, and finally compare them in terms of helpfulness.

The annotators achieve an agreement of 0.84 for whether a model output should be accepted or not. For the pairwise comparison, following Zhou et al. [56], we report a tie-discounted accuracy, which assigns one point if both annotators agreed, half a point if either annotator (but not both) labeled a tie, and zero point otherwise. We also merged "clearly better" and "slightly better" together, so our final options will be simply comparing which of A and B is better, or a tie. Our annotators achieved an agreement of 0.72 for this pairwise comparison.

Although these numbers show reasonable agreement, we also note that there is a large extent of subjectivity in human evaluation. This noise level also indicates that some prior work [55, 8] that uses a small number of examples for human evaluation might not be reliable enough. We suggest that the community needs to further improve the reliability and scalability of human evaluation for instruction-following models.

## Appendix H Further Investigation of Figure 2

To further investigate the degree to which the number of unique tokens is being used by GPT-4 as a marker of quality, we created a dummy evaluator that compares two model outputs, and assigns a win to the output with more unique tokens. We plot the win-rate calculated using this dummy evaluator against the win-rate calculated using GPT-4 in Figure 6.

We find that while the dummy evaluator generally over-estimates the win-rate, the trend is still strikingly linear. We note that the \(R^{2}\) for the trendline is.91, suggesting that the unique token count explains a large proportion of the variance in the win rates. Based on this, we believe that the number of unique tokens is certainly a key preference that GPT-4 cares about in its evaluation, although it is still not the only important feature.

## Appendix I Model Licenses

We provide brief information about the licenses of the underlying models we make use of in this work below.

* **LLAMa**: The LLAMa model weights are released under a custom license that allows using the model for non-commercial research purposes.
* **LLAMa-2**: The LLAMa-2 model weights are released under a custom license that allows for commercial and research uses with some limitations (e.g., having less than 700 million monthly active users if used in a commercial application), and explicitly allows for redistribution of the weights.

Figure 6: Win-rate scores of all models judged by the dummy evaluator against win-rate of all models using the GPT-4 evaluator.

* **Pythia**: The Pythia weights are released under the Apache-2.0 license.
* **OPT**: The OPT model weights are released under a custom license that allow only using the model for non-commercial research purposes.