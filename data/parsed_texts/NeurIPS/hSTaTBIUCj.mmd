_Imagine That!_ Abstract-to-Intricate Text-to-Image Synthesis with Scene Graph Hallucination Diffusion

 Shengqiong Wu Hao Fei Hanwang Zhang Tat-Seng Chua

\({}^{1}\)NExT++, School of Computing, National University of Singapore

\({}^{2}\) School of Computer Science and Engineering, Nanyang Technological University

swu@u.nus.edu {haofei37, dcscts}@nus.edu.sg hanwangzhang@ntu.edu.sg

Hao Fei is the corresponding author.

\({}^{1}\)NExT++, School of Computing, National University of Singapore

\({}^{2}\) School of Computer Science and Engineering, Nanyang Technological University

swu@u.nus.edu {haofei37, dcscts}@nus.edu.sg hanwangzhang@ntu.edu.sg

###### Abstract

In this work, we investigate the task of text-to-image (T2I) synthesis under the abstract-to-intricate setting, i.e., _generating intricate visual content from simple abstract text prompts_. Inspired by human imagination intuition, we propose a novel scene-graph hallucination (SGH) mechanism for effective abstract-to-intricate T2I synthesis. SGH carries out scene hallucination by expanding the initial scene graph (SG) of the input prompt with more feasible specific scene structures, in which the structured semantic representation of SG ensures high controllability of the intrinsic scene imagination. To approach the T2I synthesis, we deliberately build an SG-based hallucination diffusion system. First, we implement the SGH module based on the discrete diffusion technique, which evolves the SG structure by iteratively adding new scene elements. Then, we utilize another continuous-state diffusion model as the T2I synthesizer, where the overt image-generating process is navigated by the underlying semantic scene structure induced from the SGH module. On the benchmark COCO dataset, our system outperforms the existing best-performing T2I model by a significant margin, especially improving on the abstract-to-intricate T2I generation. Further in-depth analyses reveal how our methods advance.2

Footnote 2: Code is available at https://github.com/ChocoWu/T2I-Salad

## 1 Introduction

The task of generating images from natural language descriptions, known as text-to-image (T2I) synthesis, has attracted significant attention [4, 17, 52]. To approach T2I, various generative models have been explored, including generative adversarial networks (GANs) [48, 52, 61], variational autoencoders (VAEs) [50], flow-based models [3], and auto-regressive models (ARMs) [10, 41], all of which aim to generate realistic images in high quality and high faithfulness. Most recently, diffusion-based models have been proposed, which simulate the physical process of gas diffusion for image generation [23]. Diffusion models have shown unprecedented performance in image synthesis over existing methods, becoming the current state-of-the-art (SoTA) T2I solution [2, 9, 19, 47, 42].

As a long-reached viewpoint [28, 53, 37], sound T2I systems should not only achieve high-quality image generation in simple straightforward visual scenery but be more capable of synthesizing realistic images with complex scenes. Typically, detailed textual descriptions are necessarily needed to prompt the synthesis process with adequate details for high-quality vision generation. However, in a realistic world, it could also be ubiquitous to produce intricate visions without relying on lengthy elaborate prompts. For example, users may prefer T2I systems to synthesize well-detailed images while not taking too much time to write descriptions in detail. More crucially, due to the natural modality asymmetry between language and vision, even some simple words can intrinsically describe or represent abstract visual scenes with rich and complex details. Whenever mentioning the wordswith specific scenes, such as _classroom, kitchen, office_, or actional verbs e.g., _traveling, shopping_, there is always a picture with multifaceted scenes and rich-detailed backgrounds. In a word, _it is worth investigating generating intricate images from succinct abstract prompts_.

Yet existing prevailing approaches, even the SoTA diffusion models, may largely fail the abstract-to-intricate T2I, due to the lack of necessary details of input prompts (cf. Figure 1(a)). One intuitive workaround is to directly enrich the texts, i.e., adding more details for the prompts. Specifically, existing works either consider inserting additional adjectives and attributives to modify the original mentions and scenes [8], or concatenating raw sentences with more tangible explanations and contexts that are elicited from external large language models [48], e.g., ChatGPT [38]. Unfortunately, due to the intrinsic grammar and linguistics rules, such text-based prompt enrichment can be subject to the issue of lower controllability. One problem is the _visual distraction_, where the main focus of the resulting image is dominated by other newly-added trivial contents, when aggressively inserting intermediate descriptive components into the raw texts, as exemplified in Figure 1(a). Besides, directly appending new textual descriptions would increase the prompt length and then lead to _incorrect binding_ of attributes or relations, i.e., making the image deviate from the original user intention.

As a reference, intuitively, we human beings would tackle the abstract-to-intricate T2I as a two-step painting process, i.e., from _semantic interpretation_ to _scene imagination_. In the semantic interpretation, a painter always first translates the succinct textual prompt into a structured skeleton that represents the semantic scene of key mention objects and their relations. Then, based on the initial scene, the painter mentally completes the abstract scene with more concrete and valid details. With the enriched scene structure, the final vision can be more accurately and easily rendered. Motivated by such human intuition, in this work, we propose a **scene-graph hallucination** (SGH) method for achieving effective abstract-to-intricate T2I. As illustrated in Figure 1(b), we first investigate representing the input prompt with its scene graph (SG) [46]. SG advances in depicting the intrinsic semantics of texts (or vision) with structured representations, e.g., objects, attributes, and relationships, enabling fine-grained control of the semantic scene [53]. Based on the SG of input text we then carry out scene hallucination, expanding the initial SG with more possible specific scene structures. Also with the SG representation, the imagination process can be much more accurate and controllable.

To implement the overall idea, we develop an SG-based **h**allucination **d**iffusion system (namely, **Salad**) for high-quality image synthesis. Salad is a fully diffusion-based T2I system, which mainly consists of a scene-driven T2I module and an SGH module. As shown in Figure 2, we first take advantage of the SoTA latent diffusion model [9] as our backbone T2I synthesizer, in which the overt image-generating process is controlled and navigated by the underlying semantic scene structure.

Figure 1: (a) An example of abstract-to-intricate T2I synthesis. All images are generated by the Latent diffusion Model (LDM) [42]. LDM fails to accurately render the abstract contexts, e.g., ‘give a presentation’ and ‘office’ of the original prompt. Raw prompts can be enriched via descriptive insertion [8], or addition [48]. Enriched contexts are in _blue_. (b) We illustrate the human intuition on the abstract-to-intricate T2I process: we always first grasp the semantic structure of the original prompt text, i.e., scene graph (SG), and then carry out imagination with more complete scenes based on the SG. Here the glowing nodes and edges are enriched ones.

On the other hand, we design the SGH module based on the modality-agnostic discrete diffusion model [25], which evolves and completes the initial SG structure of the input prompt by iteratively generating new scene elements (i.e., SG nodes). During the SG imagination process, the resulting structural representation at each step is fused into the T2I synthesizer via a hierarchical integration strategy. Further, we devise a scene sampling mechanism, via which the SGH module can generate various SG imaginations, and thus help achieve diversified image synthesis during inference.

We conduct experiments on COCO [33], the widely-used T2I dataset. Results show that Salad outperforms all the existing strong-performing T2I systems with significant margins. Further analysis reveals that modeling the SG structures helps synthesize high-quality images with stronger semantic controllability. Our proposed SGH mechanism is effective in inducing sound SG structures, helping produce more realistic images from short abstract text prompts. And the scene sampling strategy aids diversified T2I generation. In summary, this paper contributes in five aspects:

* We are the first to study the novel setup of intricate image synthesis from abstract texts.
* We solve the abstract-to-intricate T2I with a novel SG hallucination mechanism, which is implemented via discrete diffusion technique, performing scene enrichment with reasonable imagination.
* We propose a diffusion-based model with a hierarchical scene integration strategy for highly controllable and scalable image generation.
* We devise a scene sampling mechanism to generate various scene graphs for diversified image syntheses during inference.
* Our framework achieves new SoTA results in the abstract-to-intricate T2I generation.

## 2 Preliminary

### Scene Graph Representation

The SG (denoted as \(G\)) [45] represents the semantic relationships among scene objects in a structure, where there are three types of nodes, i.e., **object**, **attribute**, and **relation**, cf. Figure 1(b). We formulate the object node set as \(\{o_{1},\cdots,o_{N}\}\), where \(o_{n}\) denotes \(n\)-th object node; the attribute node set as \(\{a_{1,1},\cdots,a_{N,M}\}\), where \(a_{n,m}\) means \(m\)-th attribute node of the \(n\)-th object node; the relation node set as \(\{r_{1,1},\cdots,r_{N,N}\}\), where \(r_{i,j}\) means object node \(o_{i}\) connects to the object \(o_{j}\). All nodes come with a category label \(l^{o/a/r}\), and each type of node has its own unique category vocabulary. For example as in Figure 2 right bottom, the object node \(o_{1}\) with category label \(l^{o}_{1}\) associated with two attribute nodes \(a_{1,1}\) and \(a_{1,M}\), with category label \(l^{a}_{1}\) and \(l^{a}_{23}\), respectively. And the object node \(o_{1}\) connects to the object node \(o_{2}\) through an edge \(r_{1,2}\) with the category label \(l^{r}_{7}\).

### Diffusion Models

Diffusion models (DMs) [23] learn to convert a simple Gaussian distribution into a data distribution. Technically, DMs consist of a forward (diffusion) process and a reverse (denoising) process. In the forward process, the given data \(\bm{x}_{0}\sim q(\bm{x}_{0})\) is gradually corrupted into an approximately standard normal distribution \(\bm{x}_{T}\sim p(\bm{x}_{T})\) over \(T\) steps by increasingly adding noisy, formulated as

Figure 2: Overall framework of our proposed SG-based hallucination diffusion system (Salad).

\(q(\bm{x}_{1:T}|\bm{x}_{0})=\prod_{t=1}^{T}q(\bm{x}_{t}|\bm{x}_{t-1})\). The learned reverse process \(p_{\theta}(\bm{x}_{0:T})=p(\bm{x}_{T})\prod_{t=1}^{T}p_{\theta}(\bm{x}_{t-1}|\bm {x}_{t})\) gradually reduces the noise towards the data distribution. To improve the fit of a generative model to the data distribution, a variational upper bound on the negative log-likelihood is optimized:

\[\mathcal{L}_{vlb}=\mathbb{E}_{q(\bm{x}_{1:T}|\bm{x}_{0})}\Big{[}\text{log} \frac{q(\bm{x}_{T}|\bm{x}_{0})}{p_{\theta}(\bm{x}_{T})}+\sum_{t=2}^{T}\text{ log}\frac{q(\bm{x}_{t-1}|\bm{x}_{t},\bm{x}_{0})}{p_{\theta}(\bm{x}_{t-1}|\bm{x}_{t })}-\log p_{\theta}(\bm{x}_{0}|\bm{x}_{1})\Big{]}\,,\] (1)

where \(p_{\theta}(\cdot)\) is estimated by a denoising network, which can be a time-conditional U-Net [43]. Recently, latent diffusion models (LDMs) [42; 14; 19] are introduced to adopt DMs to operate in an efficient, low-dimensional latent space, where a pre-trained encoder \(\mathcal{E}\) maps the given data \(\bm{x}_{0}\) into a latent code \(z_{0}=\mathcal{E}(\bm{x}_{0})\), and a decoder reconstructs the final output image from the denoised latent \(\mathcal{D}(\bm{z}_{0})\sim\bm{x}_{0}\). Due to the higher computation sufficiency, this work thus takes the LDM as T2I backbone. Besides, DMs also have been extended to operate in discrete state spaces [1], i.e., performing diffusion and denoising processes over discrete variables, which have demonstrated competitive performances for discrete data, such as text [24] and layout [5]. Hence, we also adopt the discrete DMs to realize the SG induction process. Appendix SSA.1 gives more technical details to the discrete diffusion models.

## 3 Methodology

Formally, T2I aims to generate an image \(x\) that faithfully reflects the desired content in the input prompt text \(y\). To approach abstract-to-intricate T2I, we propose an SG-based hallucination diffusion system (Salad), which is shown in Figure 2. The salad consists of two major modules. First, the SGH (cf. SS3.1) is responsible for enriching the initial SG of the text prompt via a discrete diffusion model. Then, built upon an LDM (cf. SS2.2), the Scene-driven Image Synthesis module (SIS, cf. SS3.2) performs denoising for image synthesis, during which the derived SG features is fused via a hierarchical scene integration mechanism. The underlying SGH closely collaborates with the upper SIS at each step, and thus the semantic scene skeleton takes fine-grained control of the overt vision rendering. We also describe the optimization (cf. SS3.3), and the scene sampling strategy (cf. SS3.4).

### Scene Graph Hallucination (SGH)

As aforementioned, we formulate the SGH as a discrete denoising diffusion process [25] (cf. Fig. 2). Specifically, in the forward process, the SG of the gold image, marked as \(G_{0}\), will be corrupted into a sequence of increasingly noisy latent variables \(G_{1:T}=\{G_{1},G_{2},\cdots,G_{T}\}\), where each SG node \(s^{*}_{t,j}\in G_{t},*\in\{o,a,r\}\) (\(t\) is diffusion step, \(j\) is the node index) takes a discrete value with \(K^{*}\) category labels, and \(o,a,r\) denotes the nodes' type, i.e., object (\(o\)), attribute (\(a\)), and relation (\(r\)). For simplicity, we omit subscripts \(j\) and superscripts \(*\) in the following description. The discrete diffusion process can be parameterized with a multinomial categorical transition matrix:

\[q(s_{t}|s_{t-1})=\mathcal{B}^{\top}(s_{t})\cdot\bm{Q}_{t}\cdot\mathcal{B}(s_{ t-1}),\] (2)

where \(\mathcal{B}(s_{t})\) denotes the column one-hot vector of \(s_{t}\). And \(\bm{Q}_{t}\) is the transition matrix, with \([\bm{Q}_{t}]_{ij}=q(s_{t}=j|s_{t-1}=i)\) representing the probabilities that \(s_{t-1}\) transitions to \(s_{t}\). Due to the property of the Markov chain, the cumulative probability of \(s_{t}\) at arbitrary timestep from \(s_{0}\) can be derived as \(q_{(}s_{t}|s_{0})=\mathcal{B}^{\top}(s_{t})\cdot\bm{\dot{Q}}_{t}\cdot\mathcal{B }(s_{0})\), where \(\bm{\dot{Q}}_{t}=\bm{Q}_{t}\bm{Q}_{t-1}\cdots\bm{Q}_{1}\). Inspired by [1; 19], we employ a _mask-and-replace_ strategy to design the \(\bm{Q}_{t}\). For each node \(s_{t}\), we define three probabilities: 1) a probability of \(\gamma_{t}\) to transition to [MASK] node, 2) a probability of \(K\beta_{t}\) be resampled uniformly over all the \(K\) categories, and 3) a probability of \(\alpha_{t}=1-K\beta_{t}-\gamma_{t}\) to stay the same node. Notedly, the [MASK] node never transits to other states. Hence, the transition matrix \(\bm{Q}_{t}\) can be formulated as3:

Footnote 3: Appendix SSA.1 provides detailed formulation of the discrete diffusion process.

\[\bm{Q}_{t}=\begin{bmatrix}\alpha_{t}+\beta_{t}&\beta_{t}&\beta_{t}&\cdots&0\\ \beta_{t}&\alpha_{t}+\beta_{t}&\beta_{t}&\cdots&0\\ \vdots&\vdots&\vdots&\ddots&\vdots\\ \gamma_{t}&\gamma_{t}&\gamma_{t}&\cdots&1\end{bmatrix}\,.\] (3)

The aforementioned discrete state-space models assume that all the standard nodes are switchable by corruption. However, as stated in SS2.1, there are three different SG nodes under separate categories. Hence, we apply three disjoint corruption matrices \(\bm{Q}_{t}^{o}\in\mathbb{R}^{K^{\alpha}\times K^{o}}\), \(\bm{Q}_{t}^{a}\in\mathbb{R}^{K^{a}\times K^{a}},\bm{Q}_{t}^{r}\in\mathbb{R}^{K^ {r}\times K^{r}}\) for object, attribute, and relation nodes, respectively, where \(K^{o},K^{\alpha},K^{r}\) denotes the size of category labels of three node types respectively.

In the denoising process, we introduce an SG decoder as the neural approximator to estimate the distribution \(p_{\theta}(s_{t-1}|s_{t},y)\). As shown in Figure 2, SG decoder first employs an adaptive normalization (AdaLN) to inject the timestep information. A text cross-attention (Text-CA) integrates the input prompt \(y\). Then, a graph cross-attention (Graph-CA) is devised to take in the induced SG (\(G_{t+1}\)) in the previous \(t\)+1 timestep:

\[\tilde{\bm{H}}^{*}=\text{Graph-CA}(G_{t+1},\bm{H}^{*})\,,\] (4)

where \(\bm{H}^{*}\) are the features yielded from Text-CA. Graph-CA consistently results the overall picture of the last SG for a more coherent generation when making the current decision.

Intuitively, among the object, attribute, and relation nodes, the object nodes always come first to determine the scene subjects, followed by their modifier attributes, and then the relations between objects. Thus, instead of parallel induction of three node types, we follow this SG node-type dependence (NTD) intuition, and design an NTD cross-attention (NTD-CA) for the \(*\)-type node induction (\(*\) can be object or attribute):

\[\hat{\bm{H}}^{*}=\begin{cases}\text{NTD-CA}(s_{t}^{o},\tilde{\bm{H}}^{*})\,,& *=a\\ \text{NTD-CA}(s_{t}^{o}\oplus s_{t}^{a},\tilde{\bm{H}}^{*})\,.&*=r\end{cases}\] (5)

Note that we stack multiple layers of the above calculations as one SG decoder. For each state of node types \(\hat{\bm{s}}^{*}_{t}\), a softmax function is put on to obtain the category label distributions: \(\hat{\bm{s}}^{*}_{t}=\text{Softmax}(\hat{\bm{H}}^{*})\).

Following [1], we optimize the SG decoder by minimizing the variational lower bound \(\mathcal{L}_{vlb}\) (Eq. 1). Also the parameterization trick [19] is leveraged to encourage the system to predict the noiseless node distribution \(p_{\theta}(\tilde{s}_{0}|s_{t},y)\) at each reverse step, which is taken as an auxiliary learning objective to be incorporated with \(\mathcal{L}_{vlb}\):

\[\begin{split}\mathcal{L}_{SGH}&=\mathcal{L}_{vlb}+ \lambda_{1}\,logp_{\theta}(s_{0}|s_{t},y)\,,\\ \mathcal{L}_{vlb}&=\mathcal{L}_{0}+\mathcal{L}_{1}+ \cdots+\mathcal{L}_{T-1}+\mathcal{L}_{T}\,,\\ \mathcal{L}_{0}&=-logp_{\theta}(s_{0}|s_{1},y)\,,\\ \mathcal{L}_{t-1}&=D_{KL}((q(s_{t-1}|s_{t},s_{0}))||(p _{\theta}(s_{t-1}|s_{t},y))\,,\\ \mathcal{L}_{T}^{o}&=D_{KL}(q_{(S}T|s_{0})||p_{\theta}(s_{T}))\,, \end{split}\] (6)

where \(\lambda_{1}\) is a hyper-parameter for controlling the learning components.

### Scene-driven Image Synthesis (SIS)

With the enriched SG from SGH in each denoising step at hand, the backbone T2I diffusion carries out the image synthesis with the guidance of that SG. We design a hierarchical scene integration (HSI) strategy to ensure the highly effective integration of SG features. Specifically, we consider the fusion at four different hierarchical levels, i.e., object (\(L^{o}\)), relation (\(L^{r}\)), region (\(L^{c}\)), and global levels (\(L^{g}\)) with each focusing on different context scopes, as illustrated in Figure 3. We maintain the representations of these three levels as the keys \(\bm{K}_{L^{i}}\) & values \(\bm{V}_{L^{i}}\) via CLIP encoder [40], which are then integrated together via the Transformer attention of U-Net in LDM:

\[\tilde{\bm{H}}=\sum_{i\in\{o,r,c,g\}}\text{Attn}(\bm{H},\bm{K}_{L^{i}},\bm{V} _{L^{i}})=\sum_{i\in\{o,r,c,g\}}\text{Softmax}(\frac{\bm{H}\bm{K}_{L^{i}}^{ \top}}{\sqrt{d}})\bm{V}_{L^{i}}\,,\] (7)

where \(\bm{H}\) is the visual query vectors from the ResNet block in LDM. The above hierarchical scene integration is carried out for both the downsampling and upsampling processes in U-Net. By denoising \(T\) steps, the system finally produces the desired image. Appendix SSA.2 gives more details of this part.

### Warm-start Training

To ensure stable learning of the overall system, we take a warm-start training strategy. Firstly, the SGH is separately updated via \(\mathcal{L}_{SGH}\) (Eq. 6) based on the abstract-to-intricate SG pair annotations,

Figure 3: Hierarchical scene integration (HSI) fuses the SG features under multiple levels: 1) objects (with attributes), 2) relational triplets (i.e., _subject-predicate-object_), 3) regional neighbors, and 4) the whole SG.

until it has converged. Then, both the SIS and SGH modules are optimized jointly by minimizing:

\[\mathcal{L}=\lambda_{2}\mathcal{L}_{SGH}+\mathcal{L}_{SIS}\,,\quad\text{where }\mathcal{L}_{SIS}=\mathbb{E}_{\pi\sim\mathcal{E}(\bm{x}_{0}),\epsilon\sim \mathcal{N}(0,\bm{I}),t}\|\epsilon-\epsilon_{\theta}(\bm{z}_{t},G_{t},y,t)\|_{ 2}^{2}\,.\] (8)

Here we follow [23] to optimize SIS with a simple surrogate objective that calculates the mean-squared error loss, and \(G_{t}\) is the intermediate SG by SGH at timestep \(t\), which can be derived from the \(s_{t}^{*},*\in\{o,a,r\}\) (cf. Figure 2). \(\epsilon\) is the noise in SIS, and \(\epsilon_{\theta}(\cdot)\) denotes the U-Net (cf. Figure 3).

### Inference with Scene Sampling

During inference, we further aim to endow the SGH with diversified SG enrichment, and thus lead to T2I diversification. Intuitively, given an abstract prompt, there is often more than one possibility of the potential scenes to imagine. Also, it can be observed that in the denoising process, the diffusion model has a larger potential of divergence only at its earlier stage, while the generation tends to be more stable and certain when the iteration grows. Correspondingly, we expect the scene sampling to start in the early denoising steps, and gradually be more determining. Thus, we design a scene-sampling mechanism. First, instead of picking the best prediction of the category of any node \(\hat{\bm{s}}_{t,j}^{*}\), we take the top-\(A\) category candidates with corresponding probability distribution \(\Psi\) based on the category distribution \(\hat{\bm{s}}_{t,j}^{*}\in\mathbb{R}^{K^{*}}\) of node \(s_{t,j}^{*}\). Then, we perform sampling over these candidates with a dynamic probability:

\[\rho_{t,j}^{*}=e^{-\eta\cdot t}\cdot\Psi+(1-e^{-\eta\cdot t})\frac{1}{A}\,,\] (9)

where \(*\in\{o,a,r\}\), and \(\eta\) is a temperature. It is an annealing process, i.e., when \(t\)=\(T\) (starting denoising) more random sampling is preferable, while \(t\) approaches 0 (denoising ends), SGH tends to be more decisive. Figure 4 exemplifies the mechanism with the _object_ type of nodes (\(\hat{\bm{s}}_{t,n}^{o}\)).

## 4 Experiments

### Settings

Data and ResourceWe conduct T2I generation experiments mainly on the COCO [33] dataset. We also prepare the abstract-to-intricate SG pair annotations for training the SGH module, where we employ an external textual SG parser [46] and a visual SG parser [59] on the paired images and texts in COCO, to obtain the initial SG and imagined SG, respectively. To enlarge the abstract-to-intricate SG pairs, we further extend Visual Genome (VG) [30]. Besides, to simulate the abstract-to-intricate T2I scenario, we further manually extract a subset of text-image pairs from raw COCO data (named COCO-A2I), in which the texts are short and abstract,4 while the images are comparatively complex and intricate. Appendix SSB.1 shows all the dataset details.

Footnote 4: We select the texts that mainly include two types of words, i.e., Place Nouns (e.g., office, airport, bathroom) and Progressive Verbs (e.g., traveling, knitting, gardening).

Baseline and EvaluationWe make comparisons with three types of existing strong-performing T2I models. 1) **GAN-based models**: AttnGAN [52], ObjGAN [32], DFGAM [48], OPGAN [22], 2) **Auto-aggressive model**: DALL-E[41], and CogView [10]. 3) **Diffusion-based models**: LDM [42], VQ-diffusion [19], LDM-G and Frido [14] with classifier-free guidance. Note that LDM-G and Frido are the current SoTA T2I synthesizers. In addition, two types of **text-based enrichment approaches** are included as baselines: stable-diffusion prompt generator (SD-PG) and SPY inspired by [6]. The enriched text prompts are then utilized as inputs for Frido to generate the final images. We adopt three standard metrics to measure image synthesis performance: 1) **Inception score (IS)**[44], 2) **Frechet Inception Distance (FID)**[21] and 3) **CLIP score**. Moreover, we use **GLIP**[31] to measure the fine-grained '_object-attribute_' grounding in images, and **Triplet Recall (TriRec.)** measure the '_subject-predicate-object_' triplet recall between two SGs. We also adopt the **Learned Perceptual Image Patch Similarity (LPIPS)**[60] for diversifying generation evaluation. Detailed definitions of evaluation metrics are shown in Appendix SSB.2.

Figure 4: Illustration of the scene sampling mechanism.

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

**P Q2:** _Does SGH indeed induce intricate and reasonable SGs?_ Firstly, in Figure 7 we explore whether the SGH can produce new SG structures during the T2I process, on the COCO and COCO-A2I datasets separately. As seen, after scene enrichment, the average numbers of all three element types (object, attribute, and relation) substantially increase. Notably, the addition is more evident on the COCO-A2I set, where scene enrichment is more needed.

Next, we examine if the newly imagined SG structures provide reasonable scenes to the input prompt. We reach this by measuring the recall rate (TriRec.) of the '_subject-predicate-object_' pairs between two SGs, i.e., the predicted and the gold SGs. Assuming the SGs (denoted as \(G^{\text{I}}\)) of gold images entail reasonable scenes, we consider two types of SGs from our 'prediction': 1) SGs induced by SGH (\(\overline{G}\)), and 2) SGs parsed from our synthesized images (\(G^{\text{I}}\)). We also compare with the text-enriched SPY method. As shown in Table 4, we first observe that the induced SGs highly align with the gold SGs, with 82.01 TriRec. score, indicating the induced SGs are sensible for generating high-quality images. Moreover, by comparing the TriRec. scores (\(G^{\text{I}}\)_vs._\(G^{\text{I}}\)) between SPY and Salad (i.e., 78.61 vs. 86.04), we learn that the synthesized images coordinated by imagined SGs more correspond to the gold images in terms of semantic scene structure. This also suggests that the SGH can induce valid SGs which favorably guide the image generation process.

**P Q3:** **Does sampling strategy helps diversified T2I?** To measure the effectiveness of the sampling strategy in diversifying image generation given the same prompt, we consider both the qualitative analyses and human evaluation, where the former calculates the LPIPS score [60] to assess the perceptual similarity between two images in deep feature space. As shown in Figure 8, our Salad model significantly outperforms LDM and Frido in terms of diversity score and human evaluation, and things go worse upon removing the scene sampling strategy. This demonstrates that our proposed scene sampling mechanism is effective in helping diversified T2I. In Appendix SSC.2 we show more examples of diversified generations via the scene sampling mechanism.

## 5 Related Work

T2I is a long-standing topic in computer vision and multimodal communities. Arrays of explorations have been devoted to achieving stronger image synthesis performances with various deep generative models, such as GANs [4, 17, 48, 52, 61], VAEs [29], flow-based approaches [11, 12] and ARMs [7, 13, 39, 49]. More recently, the diffusion denoising probabilistic methods (DDPMs) have revealed the greatest potentials on image synthesis, in which the optimal density estimation is more naturally achieved with a fixed diffusion process to transform an image into a Gaussian noise [9, 19, 23, 42, 47]. This work follows the line of diffusion methods and takes the SoTA latent diffusion model (LDM) [42, 14, 19] as the T2I backbone.

Generating high-quality images with complex scenes is the key criterion of a sound T2I system [14, 15, 26]. Many efforts have been paid for synthesizing more realistic and nature-looking images in sophisticated scenes, yet most of which are conditioned on taking the detailed descriptions as inputs [6, 51]. Thus, how to generate high-quality images in intricate scenes from succinct and abstract prompts becomes a meaningful yet challenging task. In this work, we introduce a scene hallucination mechanism, which, built upon the SG structure, performs more accurate and controllable scene completion and eventually helps generate intricate images of higher quality. We consider the SG representations [27] for the input prompt texts as well as the guidance of image synthesis. SG advances in intrinsically describing the semantic structures of scenes for texts or images [28, 53], enabling more fine-grained control of complex scenes [53], and thus aiding the final image generation.

\begin{table}
\begin{tabular}{c c c} \hline  & \(\overline{G}\)_vs._\(G^{\text{I}}\) & \(G^{\text{I}}\)_vs._\(G^{\text{I}}\) \\ \hline SPY & - & 78.61 \\ Salad & 82.01 & 86.04 \\ \hline \end{tabular}
\end{table}
Table 4: Comparing the gold SGs (\(G^{I}\)) with our induced SG (\(\overline{G}\)), and the SG (\(G^{\overline{I}}\)) of generated image with TriRec. metric.

Figure 8: Comparison of diversity using diversity score (LPIPS) and human evaluation.

Figure 7: The average number of three types of SG nodes.

Scene graph (SG) is a type of structured data that represents multiple objects and their complex relationships in the vision or language scenes, wherein the nodes denote objects & attributes and the edges depict relationships between objects [27]. As intrinsically describing the semantic structures of scene semantics for the given texts or images, SG has been widely utilized as a type of external feature being integrated into downstream applications for enhancements, e.g., image retrieval [28; 56], image generation [28; 53] and image captioning [54; 36; 55]. In this work, we consider the SG representations for the input sentences. Compared to the linear sequential nature of the text, SG offers a more intuitive manner to represent the scene semantics in a structured format, enabling more fine-grained control of complex scenes [53], and thus aiding the final image synthesis.

Our SGH mechanism also relates to the research of SG enrichment or imagination. While existing methods mostly approach the task by incrementally parsing SG elements [16; 57; 58], such greedy-increment paradigm may largely suffer from trapping in locally optimal SG generation, thus leading to inferior SG imagination. Instead, in this paper, we implement SGH as a discrete denoising&diffusion process. Discrete diffusion technique [25] is the latest introduced method that replaces the continuous state in standard diffusion models with a discrete one. During each denoising step, the entire SG structure is updated and optimized from a global viewpoint, so as to yield a more reasonable enrichment of scene structure. Besides, both the T2I and the SGH are modeled as the same diffusion process in our framework, where the two processes are well synchronized, such that the underlying SG features can perfectly guide the T2I synthesis at each step. To our knowledge, we are the first to investigate SG induction using discrete diffusion models.

## 6 Conclusion

In this work, we explore the text-to-image synthesis task under the abstract-to-intricate setup. Drawing inspiration from human intuition, we propose a scene-graph hallucination mechanism, which carries out scene imagination based on the initial scene graph of the input prompt, expanding the starting SG with more possible specific scene structures. We then develop an SG-based hallucination diffusion system for the abstract-to-intricate T2I, which mainly includes an SG-guided T2I module and an SGH module. Specifically, we design the SGH module based on the discrete diffusion technique, which evolves the initial SG structure by iteratively adding new scene elements. Then, we utilize another continuous diffusion model as the T2I synthesizer, where the overt image-generating process is navigated by the underlying semantic scene structure induced by the SGH module. On the standard COCO dataset, our system shows great superiority in the abstract-to-intricate T2I generation. Further analyses demonstrate that our SG-based hallucination mechanism is able to generate logically sound SG structures, which in return helps produce high-quality scene-riched images.

## 7 Broader Impact

BenefitsThe current text-conditioned image generation approaches largely fail to the abstract-to-intricate T2I due to a lack of necessary details of input prompts. In this work, we propose a novel scene-graph hallucination mechanism inspired by human imagination intuition, which expands upon the initial scene graph from the text prompts to generate more feasible and specific scene structures. Furthermore, the enriched timestep-wised SG is leveraged to navigate the T2I generation process, leading to synthesizing more intricate images. Our study demonstrates that hallucinating images based on scene graph structures offer scalability, and modeling these structures enhances the generation of high-quality images with improved semantic controllability.

Potential weaknessThere can be two potential weaknesses that warrant consideration in our system. Firstly, the effectiveness of our system relies heavily on the quality of scene graph hallucination (SGH), yet the absence of a dedicated dataset for the SGH task poses a challenge in training the SGH module. However, we can leverage the richly annotated Visual Genome (VG) dataset, commonly used for training visual SG parsers, to provide initial training for the SGH module under an unconditional setting. Secondly, the training process of a diffusion model for text-to-image (T2I) generation entails substantial computational resources, resulting in increased energy consumption, CO\({}_{2}\) emissions, and potential environmental pollution.

## Acknowledgements

This research is supported by NExT++ Lab and CCF-Baidu Open Fund.

## References

* Austin et al. [2021] Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. In _Proceedings of the NeurIPS_, pages 17981-17993, 2021.
* Bar-Tal et al. [2023] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. _CoRR_, abs/2302.08113, 2023.
* Bashiri et al. [2021] Mohammad Bashiri, Edgar Y. Walker, Konstantin-Klemens Lurz, Akshay Jagadish, Talianh Muhammad, Zhiwei Ding, Zhuokun Ding, Andreas S. Tolias, and Fabian H. Sinz. A flow-based latent state generative model of neural population responses to natural images. In _Proceedings of the NeurIPS_, pages 15801-15815, 2021.
* Brock et al. [2019] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In _Proceedings of the ICLR_, 2019.
* Chai et al. [2023] Shang Chai, Liansheng Zhuang, and Fengying Yan. Layoutdm: Transformer-based diffusion model for layout generation. _CoRR_, 2023.
* Chakrabarty et al. [2023] Tuhin Chakrabarty, Arkady Saakyan, Olivia Winn, Artemis Panagopoulou, Yue Yang, Marianna Apidianaki, and Smaranda Muresan. 1 spy a metaphor: Large language models and diffusion models co-create visual metaphors. _OpenReview_, 2023.
* Chen et al. [2020] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In _Proceedings of the ICML_, pages 1691-1703, 2020.
* Cheng et al. [2020] Jun Cheng, Fuxiang Wu, Yanling Tian, Lei Wang, and Dapeng Tao. Rifegan: Rich feature generation for text-to-image synthesis from prior knowledge. In _Proceedings of the CVPR_, pages 10908-10917, 2020.
* Dhariwal and Nichol [2021] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat gans on image synthesis. In _Proceedings of the NeurIPS_, pages 8780-8794, 2021.
* Ding et al. [2021] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. Cogview: Mastering text-to-image generation via transformers. In _Proceedings of the NeurIPS_, pages 19822-19835, 2021.
* Dinh et al. [2015] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: non-linear independent components estimation. In _Proceedings of the ICLR_, 2015.
* Dinh et al. [2017] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In _Proceedings of the ICLR_, 2017.
* Esser et al. [2021] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In _Proceedings of the CVPR_, pages 12873-12883, 2021.
* Fan et al. [2022] Wan-Cyuan Fan, Yen-Chun Chen, Dongdong Chen, Yu Cheng, Lu Yuan, and Yu-Chiang Frank Wang. Frido: Feature pyramid diffusion for complex scene image synthesis. _CoRR_, abs/2208.13753, 2022.
* Feng et al. [2022] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun R. Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional text-to-image synthesis. _CoRR_, abs/2212.05032, 2022.
* Garg et al. [2021] Sarthak Garg, Helisa Dhamo, Azade Farshad, Sabrina Musatian, Nassir Navab, and Federico Tombari. Unconditional scene graph generation. In _Proceedings of the ICCV_, pages 16342-16351, 2021.
* Goodfellow et al. [2014] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In _Proceedings of the NeurIPS_, pages 2672-2680, 2014.
* Gu et al. [2019] Jiuxiang Gu, Shafiq R. Joty, Jianfei Cai, Handong Zhao, Xu Yang, and Gang Wang. Unpaired image captioning via scene graph alignments. In _Proceedings of the ICCV_, pages 10322-10331, 2019.
* Gu et al. [2022] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In _Proceedings of the CVPR_, pages 10686-10696, 2022.
* Hessel et al. [2021] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. In _Proceedings of the EMNLP_, pages 7514-7528, 2021.

* Heusel et al. [2017] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In _Proceedings of the NeurIPS_, pages 6626-6637, 2017.
* Hinz et al. [2022] Tobias Hinz, Stefan Heinrich, and Stefan Wermter. Semantic object accuracy for generative text-to-image synthesis. _IEEE Trans. Pattern Anal. Mach. Intell._, 44(3):1552-1565, 2022.
* Ho et al. [2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In _Proceedings of the NeurIPS_, 2020.
* Hoogeboom et al. [2021] Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forre, and Max Welling. Argmax flows and multinomial diffusion: Towards non-autoregressive language models. _CoRR_, 2021.
* Inoue et al. [2023] Naoto Inoue, Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, and Kota Yamaguchi. Layoutdm: Discrete diffusion model for controllable layout generation. _CoRR_, abs/2303.08137, 2023.
* Jimenez [2023] Alvaro Barbero Jimenez. Mixture of diffusers for scene composition and high resolution image generation. _CoRR_, abs/2302.02412, 2023.
* Johnson et al. [2015] Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Image retrieval using scene graphs. In _Proceedings of the CVPR_, pages 3668-3678, 2015.
* Johnson et al. [2018] Justin Johnson, Agrim Gupta, and Li Fei-Fei. Image generation from scene graphs. In _Proceedings of the CVPR_, pages 1219-1228, 2018.
* Kingma and Welling [2014] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In _Proceedings of the ICLR_, 2014.
* Krishna et al. [2017] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. _International Journal of Computer Vision_, 123(1):32-73, 2017.
* Li et al. [2022] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and Jianfeng Gao. Grounded language-image pre-training. In _Proceedings of the CVPR_, pages 10955-10965, 2022.
* Li et al. [2019] Wenbo Li, Pengchuan Zhang, Lei Zhang, Qiuyuan Huang, Xiaodong He, Siwei Lyu, and Jianfeng Gao. Object-driven text-to-image synthesis via adversarial training. In _Proceedings of the CVPR_, pages 12174-12182, 2019.
* Lin et al. [2014] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In _Proceedings of the ECCV_, volume 8693, pages 740-755, 2014.
* Loshchilov and Hutter [2019] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _Proceedings of the ICLR_, 2019.
* Marcheggiani and Titov [2017] Diego Marcheggiani and Ivan Titov. Encoding sentences with graph convolutional networks for semantic role labeling. In _Proceedings of the EMNLP_, pages 1506-1515, 2017.
* Milewski et al. [2020] Victor Siemen Janusz Milewski, Marie-Francine Moens, and Iacer Calixto. Are scene graphs good enough to improve image captioning? In _Proceedings of the AACL_, pages 504-515, 2020.
* Nichol et al. [2022] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and editing with text-guided diffusion models. In _Proceedings of the ICML_, pages 16784-16804, 2022.
* Ouyang et al. [2022] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _arXiv preprint arXiv:2203.02155_, 2022.
* Parmar et al. [2018] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In _Proceedings of the ICML_, pages 4052-4061, 2018.
* Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In _Proceedings of the ICML_, pages 8748-8763, 2021.

* [41] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _Proceedings of the ICML_, pages 8821-8831, 2021.
* [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the CVPR_, pages 10674-10685, 2022.
* [43] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _Proceedings of the MICCAI_, pages 234-241, 2015.
* [44] Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In _Proceedings of the NeurIPS_, pages 2226-2234, 2016.
* [45] Brigit Schroeder and Subarna Tripathi. Structured query-based image retrieval using scene graphs. In _Proceedings of the CVPR_, pages 680-684, 2020.
* [46] Sebastian Schuster, Ranjay Krishna, Angel X. Chang, Li Fei-Fei, and Christopher D. Manning. Generating semantically precise scene graphs from textual descriptions for improved image retrieval. In _Proceedings of the EMNLP_, pages 70-80, 2015.
* [47] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _Proceedings of the ICLR_, 2021.
* [48] Ming Tao, Hao Tang, Songsong Wu, Nicu Sebe, Fei Wu, and Xiao-Yuan Jing. DF-GAN: deep fusion generative adversarial networks for text-to-image synthesis. _CoRR_, abs/2008.05865, 2020.
* [49] Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Koray Kavukcuoglu, Oriol Vinyals, and Alex Graves. Conditional image generation with pixelcnn decoders. In _Proceedings of the NeurIPS_, pages 4790-4798, 2016.
* [50] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In _Proceedings of the NeurIPS_, pages 6306-6315, 2017.
* [51] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aherman. P+: extended textual conditioning in text-to-image generation. _CoRR_, abs/2303.09522, 2023.
* [52] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In _Proceedings of the CVPR_, pages 1316-1324, 2018.
* [53] Ling Yang, Zhilin Huang, Yang Song, Shenda Hong, Guohao Li, Wentao Zhang, Bin Cui, Bernard Ghanem, and Ming-Hsuan Yang. Diffusion-based scene graph to image generation with masked contrastive pre-training. _CoRR_, abs/2211.11138, 2022.
* [54] Xu Yang, Kaihua Tang, Hanwang Zhang, and Jianfei Cai. Auto-encoding scene graphs for image captioning. In _Proceedings of the CVPR_, pages 10685-10694, 2019.
* [55] Xu Yang, Chongyang Gao, Hanwang Zhang, and Jianfei Cai. Hierarchical scene graph encoder-decoder for image paragraph captioning. In _Proceedings of the ACM MM_, pages 4181-4189, 2020.
* [56] Sangwoong Yoon, Woo-Young Kang, Sungwook Jeon, SeongEun Lee, Changjin Han, Jonghun Park, and Eun-Sol Kim. Image-to-image retrieval by learning similarity between scene graphs. In _Proceedings of the AAAI_, pages 10718-10726, 2021.
* [57] Jiaxuan You, Rex Ying, Xiang Ren, William L. Hamilton, and Jure Leskovec. Graphrnn: Generating realistic graphs with deep auto-regressive models. In _Proceedings of the ICML_, pages 5694-5703, 2018.
* [58] Xiang Yu, Ruoxin Chen, Jie Li, Jiawei Sun, Shijing Yuan, Huxiao Ji, Xinyu Lu, and Chentao Wu. Zero-shot scene graph generation with knowledge graph completion. In _Proceedings of the ICME_, pages 1-6, 2022.
* [59] Rowan Zellers, Mark Yatskar, Sam Thomson, and Yejin Choi. Neural motifs: Scene graph parsing with global context. In _Proceedings of the CVPR_, pages 5831-5840, 2018.
* [60] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _Proceedings of the CVPR_, pages 586-595, 2018.
* [61] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. DM-GAN: dynamic memory generative adversarial networks for text-to-image synthesis. In _Proceedings of the CVPR_, pages 5802-5810, 2019.

Extension of Technical Details

We in this part extend the specific details of our method techniques.

### Discrete Diffusion Model for Scene Graph Hallucination

Here, we detail the forward and reverse processes in the discrete diffusion model [1] for SGH.

Forward ProcessIn the forward process, we consider a node \(s^{*}_{t,j}\in G_{t},*\in\{o,a,r\}\) (\(t\) is diffusion step, \(j\) is the node index) takes a scalar discrete value with \(K^{*}\) categories, \(s^{*}_{t,j}\in 1,\cdots,K^{*}\). Without introducing confusion, we omit subscripts \(j\) and superscripts \(*\) in the following description. We define the probabilities that \(s_{t-1}\) transits to \(s_{t}\) using the matrices \([\bm{Q}_{t}]_{ij}=q(s_{t}=j|s_{t-1}=i)\), then we can write the forward Markov diffusion process:

\[q(s_{t}|s_{t-1})=\mathcal{B}^{\top}(s_{t})\bm{Q}_{t}\mathcal{B}(s_{t-1}),\] (10)

where \(\mathcal{B}(s_{t})\) denotes the one-hot column vector which length is \(K\). The categorical distribution over \(s_{t}\) is given by the vector \(\bm{Q}_{t}\mathcal{B}(s_{t-1})\). Due to the property of the Markov chain, starting from \(s_{0}\), we can derive the cumulative probability of \(s_{t}\) at arbitrary \(t\)-timestep:

\[q(s_{t}|s_{0})=\mathcal{B}^{\top}(s_{t})\cdot\bar{\bm{Q}}_{t}\cdot\mathcal{B} (s_{0})\,,\text{where}\ \bar{\bm{Q}}_{t}=\bm{Q}_{t}\bm{Q}_{t-1}\cdots\bm{Q}_{1}\,.\] (11)

Besides, by conditioning on \(s_{0}\), the posterior of this diffusion process is tractable:

\[q(s_{t-1}|s_{t},s_{0})=\frac{q(s_{t}|s_{t-1},s_{0})q(s_{t-1}|s_{0})}{q(s_{t}|s_ {0})}=\frac{(\mathcal{B}^{\top}(s_{t})\bm{Q}_{t}\mathcal{B}(s_{t-1}))(\mathcal{ B}^{\top}(s_{t-1})\bar{\bm{Q}}_{t}\bm{v}(s_{0}))}{\mathcal{B}^{\top}(s_{t}) \bar{\bm{Q}}_{t}\mathcal{B}(s_{0})}\,.\] (12)

Note that the transition matrix \(\bm{Q}_{t}\) is capable of controlling the data corruption and denoising process, thus it should be carefully designed such that it is not too difficult for the reverse network to recover the signal from noises. We follow [1, 19] that exploits a _mask-and-replace_ strategy to design a \(\bm{Q}_{t}\), which can be defined as:

\[\bm{Q}_{t}=\begin{bmatrix}\alpha_{t}+\beta_{t}&\beta_{t}&\beta_{t}&\cdots&0\\ \beta_{t}&\alpha_{t}+\beta_{t}&\beta_{t}&\cdots&0\\ \vdots&\vdots&\vdots&\ddots&\vdots\\ \gamma_{t}&\gamma_{t}&\gamma_{t}&\cdots&1\end{bmatrix}\,,\] (13)

where for each node \(s_{t}\), there are three probabilities, i.e., a probability of \(\gamma_{t}\) to transition to [MASK] node, a probability of \(K\beta_{t}\) be resampled uniformly over all the \(K\) categories, and a probability of \(\alpha_{t}=1-K\beta_{t}-\gamma_{t}\) to stay the same node. Notedly, the [MASK] node never transitions to other states. The aforementioned discrete state-space models assume that all the nodes are switchable by corruption. However, it is unreasonable that an object node, 'bed', transitions to a relation node, 'in'. To avoid such invalid transition, we propose to apply disjoint corruption matrices \(\bm{Q}^{\sigma}_{t},\bm{Q}^{\bar{\sigma}}_{t},\bm{Q}^{\tau}_{t}\) for object, attribute, and relation nodes, respectively.

Reverse ProcessIn the reverse process, an SG decoder is introduced as a denoising network to estimate the posterior distribution \(p_{\theta}(\cdot)\), which takes the node token \(s_{t}\), time step \(t\) and text prompt \(y\). Specifically, each layer of the SG decoder contains two parts: 1) an adaptive normalization (AdaLN), and 2) a transformer block. AdaLN is applied to inject the timestep information:

\[\bm{H}^{*}=\text{AdaLN}(s_{t},t)=\bm{a}_{t}\text{LayerNorm}(\text{CLIP}(s_{t} ))+\bm{b}_{t}\,,\] (14)

where \(\text{CLIP}(\cdot)\) denotes an encoding layer for projecting the node token, and \(\bm{a}_{t}\) and \(\bm{b}_{t}\) are obtained from a linear projection of the timestep embedding. Each transformer block contains 1) a full self-attention, i.e., \(\bm{H}^{*}=\text{Softmax}(\frac{\bm{H}^{*}\cdot\bm{H}^{\top}}{\sqrt{d_{1}}}) \cdot\bm{H}^{*}\). 2) a text-cross-attention (Text-CA) integrates the input prompt \(y\):

\[\bm{H}^{*}=\text{Text-CA}(y,\bm{H}^{*})=\text{Softmax}(\frac{\bm{H}^{*}\cdot \bm{H}^{y}{}^{\top}}{\sqrt{d_{1}}})\cdot\bm{H}^{y}\,,\] (15)

where \(\bm{H}^{y}\) is a conditional feature sequence yielded by a text encoder first takes the text prompts \(y\) as input. Then, 3) a graph cross-attention (Graph-CA) is devised to take the induced SG (\(G_{t+1}\)) in the previous \(t+1\) timestep:

\[\bar{\bm{H}}^{*}=\text{Graph-CA}(G_{t+1},\bm{H}^{*})=\text{Softmax}(\frac{\bm{ H}^{*}\cdot\bm{H}^{G}{}^{\top}}{\sqrt{d_{1}}}\cdot\bm{E}^{G})\cdot\bm{H}^{G}\,,\] (16)where \(\bm{H}^{G}\) and \(\bm{E}^{G}\) is the embedding representation of the nodes and edges in \(G_{t+1}\), respectively, Next, 4) a node-type dependency cross-attention (NTD-CA) is designed for the \(*\)-type node induction (* can be object or attribute):

\[\hat{\bm{H}}^{*}=\begin{cases}\text{NTD-CA}(s_{t}^{o},\tilde{\bm{H}}^{*})= \text{Softmax}(\frac{\hat{\bm{H}}^{*}\cdot(\text{CLIP}(s_{t}^{o})^{\top})}{ \sqrt{d_{1}}})\cdot\text{CLIP}(s_{t}^{o})\,,&*=a\\ \text{NTD-CA}(s_{t}^{o}\oplus s_{t}^{a},\tilde{\bm{H}}^{*})=\text{Softmax}( \frac{\hat{\bm{H}}^{*}(\text{CLIP}(s_{t}^{o})\oplus\text{CLIP}(s_{t}^{o}))^{ \top}}{\sqrt{d_{1}}})\cdot(\text{CLIP}(s_{t}^{o})\oplus\text{CLIP}(s_{t}^{o}) )\,.&*=r\end{cases}\] (17)

Finally, a softmax function is put on to obtain the category label distributions:

\[\hat{\bm{s}}_{t}^{*}=\text{Softmax}(\hat{\bm{H}}^{*})\,.\] (18)

Following [1, 19], the SG decoder is optimized by minimizing the variational lower bound (VLB):

\[\mathcal{L}_{\text{\tiny{vlb}}} =\mathcal{L}_{0}+\mathcal{L}_{1}+\cdots+\mathcal{L}_{T-1}+ \mathcal{L}_{T},\] (19) \[\mathcal{L}_{0} =-logp_{\theta}(s_{0}|s_{1},y),\] (20) \[\mathcal{L}_{t-1} =D_{KL}((q(s_{t-1}|s_{t},s_{0}))||(p_{\theta}(s_{t-1}|s_{t},y)),\] (21) \[\mathcal{L}_{T}^{o} =D_{KL}(q_{t}(s_{T}|s_{0})||p_{\theta}(s_{T})),\] (22)

where \(p_{\theta}(s_{T})\) is the stationary distribution at timestep \(T\). To estimate the \(p_{\theta}(s_{T})\), we extend the forward process by appending a rank-one matrix \(\bm{Q}_{t+1}\) that ignores \(s_{T}\) and produces a deterministic \(s_{T+1}\in G_{T+1}\) where \(G_{T+1}\) is the initial SG parsed from the text prompt \(y\), then learning the reverse step \(p_{\theta}(s_{T}|s_{T+1})=p_{\theta}(s_{T})\). Furthermore, we follow the reparameterization trick proposed in [19], which lets the network predict the noiseless token distribution \(p_{\theta}(\tilde{s}_{0}|s_{t},y)\) at each reverse step so as to gain better quality. Based on the reparameterization trick, an auxiliary objective is introduced:

\[\mathcal{L}_{s_{0}}=-logp_{\theta}(s_{0}|s_{t},y)\,,\] (23)

To combine this loss with \(\mathcal{L}_{\text{\tiny{vlb}}}\), the final training objects are defined as follows:

\[\mathcal{L}_{SGH}=\mathcal{L}_{\text{\tiny{vlb}}}+\lambda_{1}\mathcal{L}_{s_{ 0}}\,,\] (24)

where \(\lambda_{1}\) is a hyper-parameter to control the effect of the auxiliary loss \(\mathcal{L}_{s_{0}}\).

### Scene-driven Image Synthesis

As described in SS3.2, we propose a hierarchical scene integration strategy to effectively integrate the SG features. Here, we give more details on this part. Firstly, as shown in Figure 9, the SG is able to represent four different hierarchical levels of semantics corresponding to the image:

* **Object level**. In the process of image synthesis, a crucial aspect lies in accurately generating each specified individual object, corresponding to the object node and associated attribute notes in SG, for example, _black Tshirt_, _old building_, _young man_, _luggage bag_, _sunglass_.
* **Relation level**. A high-quality image is not only the generation of objects but also their intricate relationships which connect two objects, akin to the _subject-predicate-object_ triplets found in the SG, such as _man wear Tshirt_, _man in front of building_, _man carrying on luggage bad_, _man wear sunglass_.
* **Region level**. Regional image generation focuses on multiple objects and entangled relationships among them, which aligns with the presence of overlapping relation triplets7 in SG. Here we adopt two overlapped relation triplets as the region representation of the image, such as _man wear Tshirt in front of building_, _man wear Tshirt carrying on luggage bag_, _man wear

Figure 9: Different semantic levels in the SG: objects, relation, region, and global levels.

_Third wear sunglasss_, _man in front of building carrying on luggage bag_, _man in front of building wear sunglass, man carrying on luggage bag wear sunglass._
* **Global level**. The whole SG provides global semantics to guide the generation of images.

Then, we technically extract a collection of concepts from the four semantics levels in the SG, denoted as \(L^{o}\) (object level), \(L^{r}\) (relation level), \(L^{c}\) (region level), and \(L^{g}\) (global level). We encode each concept separately:

\[\begin{split}&\bm{U}_{L^{i}}=\{\bm{u}_{1},\bm{u}_{2},\cdots\},\ \bm{u}_{j}=\text{CLIP}(c_{j})\,,\\ &\text{where }c_{j}\in L^{i}\,;j=1,2,\cdots,|L^{i}|\,;i\in\{o,r,c,g\} \end{split}\] (25)

We further maintain the representation of these four levels as the keys \(\bm{K}_{L^{i}}\) and values \(\bm{V}_{L^{i}}\) by linear transformations:

\[\bm{K}_{L^{i}}=\text{Linear}(\bm{U}_{L^{i}});\ \bm{V}_{L^{i}}=\text{Linear}(\bm{U}_ {L^{i}})\,,\] (26)

where \(i\in\{o,r,c,g\}\). Next, we integrate these features via the Transformer attention of U-Net in LDM, which is formulated in Eq. (7).

## Appendix B Detailed Experiment Settings

### Datasets

COCOThe training and validation data numbers in COCO are 83K and 41k, respectively. We note that, in the evaluation phase, models are evaluated on the full COCO 2014 validation set.

Visual Genome (VG)Visual Genome [30] version 1.4 (VG) comprises 108,077 images annotated with scene graphs. Following previous work [28], we use object and relationship categories occurring at least 2,000 and 500 times respectively in the dataset, leaving 178 objects and 45 relationship types, and we ignore small objects, and use images with between 5 and 30 objects and at least three relationships, this leaves us with 62,565 images with an average of 10 objects and 5 relationships per image.

Construction of COCO-A2IWe elaborate on the process of constructing the **COCO-A2I** dataset in the following three steps:

* First, we consider Place Nouns and Progressive Verbs are two types of abstract words that can depict intricate images. Therefore, we pre-define the candidate list of Place Nouns: _street_, _sidewalk_, _kitchen_, _restroom_, _bathroom_, _living room_, _bedroom_, _hostel_, _house_, _office_, _bank_, and the candidate list of Progressive verbs: _traveling_, _knitting_, _gardening_, _shopping_, _presenting_, _drawing_, _baking_, _studying_, etc.
* Second, we select the COCO valid dataset in that captions of instances contain the words in the candidate list, obtaining the primary-filtering dataset.
* Third, we filter out the primary-filtering dataset in which the number of words in the captions of instances is more than 10 and the number of bounding boxes is less than 6, obtaining the final target COCO-A2I dataset.

After the three-step pipeline, we obtain 2,005 text-image pairs.

In Figure 10, we show some examples, where the images tend to contain intricate content, including multiple objects, attributes, and relationships while the corresponding text prompts are relatively short and abstract.

### Evaluation Metric Implication

We employ **Frechet Inception Distance (FID)**[21], **Inception score (IS)**[44], **CLIP score**[20], and **GLIP**[31] used in [14] to quantitatively evaluate the quality of the generated images, and **Learned Perceptual Image Patch Similarity (LPIPS)**[60] utilized in [42] to quantify the diversity of the generated images. Additionally, we introduce **Triplet Recall (TriRec.)** to measure the percentage of the correct relation triplet among all the relations. Technically, given a set of ground truth triplets (_subject-predicate-object_), denoted \(GT\), and the TriRec. is computed as TriRec.=\(|PT\cap GT|/|GT|\), where \(PT\) are the relation triplets extracted from the generated images by a visual SG parser.

[MISSING_PAGE_FAIL:17]

Figure 11: Visualization of T2I generation process at \(t\)=\(100,90,\cdots\), 0, along with the enriched SG at \(t\)=\(100,70,50,20\).

Figure 12: More samples of abstract-to-intricate T2I generation via scene sampling mechanism.

Figure 13: More generated samples by our model.