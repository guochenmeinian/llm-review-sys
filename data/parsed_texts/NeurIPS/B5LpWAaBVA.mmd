# Online Nonstochastic

Model-Free Reinforcement Learning

 Udaya Ghai \({}^{\dagger}\)

Amazon

ughai@amazon.com

&Arushi Gupta \({}^{\dagger}\)

Princeton University & Google DeepMind

arushig@princeton.edu

&Wenhan Xia

Princeton University & Google DeepMind

wxia@princeton.edu

&Karan Singh

Carnegie Mellon University

karansingh@cmu.edu

&Elad Hazan

Princeton University & Google DeepMind

ehazan@princeton.edu

Work performed while at Princeton University and Google. \(\dagger\) denotes equal contribution.

###### Abstract

We investigate robust model-free reinforcement learning algorithms designed for environments that may be dynamic or even adversarial. Traditional state-based policies often struggle to accommodate the challenges imposed by the presence of unmodeled disturbances in such settings. Moreover, optimizing linear state-based policies pose an obstacle for efficient optimization, leading to nonconvex objectives, even in benign environments like linear dynamical systems.

Drawing inspiration from recent advancements in model-based control, we introduce a novel class of policies centered on disturbance signals. We define several categories of these signals, which we term pseudo-disturbances, and develop corresponding policy classes based on them. We provide efficient and practical algorithms for optimizing these policies.

Next, we examine the task of online adaptation of reinforcement learning agents in the face of adversarial disturbances. Our methods seamlessly integrate with any black-box model-free approach, yielding provable regret guarantees when dealing with linear dynamics. These regret guarantees unconditionally improve the best-known results for bandit linear control in having no dependence on the state-space dimension. We evaluate our method over various standard RL benchmarks and demonstrate improved robustness.

## 1 Introduction

Model-free reinforcement learning in time-varying responsive dynamical systems is a statistically and computationally challenging problem. In contrast, model based control of even unknown and changing linear dynamical systems has enjoyed recent successes. In particular, new techniques from online learning have been applied to these linear dynamical systems (LDS) within the framework of online nonstochastic control. A comprehensive survey can be found in Hazan and Singh (2022). The key innovation in the aforementioned framework is the introduction of a new policy class called Disturbance-Action Control (DAC), which achieves a high degree of representational capacity without compromising computational efficiency. Moreover, efficient gradient-based algorithms canbe employed to obtain provable regret bounds for this approach, even in the presence of adversarial noise. Crucially, these methods rely on the notion of disturbance, defined to capture unmodeled deviations between the observed and nominal dynamics, and its availability to the learner.

This paper explores the potential of applying these disturbance-based techniques, which have proven effective in model-based control, to model-free reinforcement learning. However, it is not immediately clear how these methods can be adapted to model-free RL, as the disturbances in model-free RL are unknown to the learner.

We therefore develop the following approach to this challenge: instead of relying on a known disturbance, we create a new family of signals, which we call "Pseudo-Disturbances", and define policies that use "Pseudo-Disturbance" features to produce actions. The advantage of this approach is that it has the potential to produce more robust policies. Again inspired by model-based methods, we aim to augment existing reinforcement learning agents with a "robustness module" that serves two purposes. Firstly, it can filter out adversarial noise from the environment and improve agent performance in noisy settings. Secondly, in cases where the environment is benign and simple, such as a linear dynamical system, the augmented module will achieve a provably optimal solution. We also empirically evaluate the performance of our method on OpenAI Gym environments.

### Our Contributions

In this work, we make the following algorithmic and methodological contributions:

* In contrast to state-based policies commonly used in RL, Section 3 defines the notion of a **disturbance-based policy**. These policies augment traditional RL approaches that rely strictly on state feedback.
* We develop **three distinct and novel methods** (Sections 3.1, 3.2, 3.3) to estimate the Pseudo-Disturbance in the model-free RL setting.
* We develop a **new algorithm**, MF-GPC (Algorithm 1), which adapts existing RL methods to take advantage of our Pseudo-Disturbance framework.
* We **empirically evaluate** our method on OpenAI Gym environments in Section 5. We find that our adaptation applied on top of a DDPG baseline performs better than the baseline, significantly so in same cases, and has better robustness characteristics.
* We prove that the proposed algorithm achieves **sublinear regret** for linear dynamics in Theorem 4. These regret bounds improve upon the best-known for bandit linear control in terms of their dependence on state space dimension (Appendix E). Notably, our bounds have **no dependence on the state dimension**, reducing the state-of-the-art regret bound by factors of \(\sqrt{d_{x}}\) for convex losses and \(d_{x}^{2/3}\) if losses are additionally smooth, signalling that our methodology is better suited to challenging high-dimensional under-actuated settings.

### Pseudo-Disturbance based RL

A fundamental primitive of the non-stochastic control framework is the _disturbance_. In our RL setting, the system evolves according to the following equation

\[\mathbf{x}_{t+1}=f(\mathbf{x}_{t},\mathbf{u}_{t})+\mathbf{w}_{t}\;,\]

where \(\mathbf{x}_{t}\) is the state, \(\mathbf{u}_{t}\) is control signal, and \(\mathbf{w}_{t}\) is a bounded, potentially adversarially chosen, disturbance. Using knowledge of the dynamics, \(f\), non-stochastic control algorithms first compute \(\mathbf{w}_{t}\), and then compute actions via DAC, as follows

\[\mathbf{u}_{t}=\pi_{\text{base}}(\mathbf{x}_{t})+\sum_{i=1}^{h}M_{i}^{t} \mathbf{w}_{t-i}\;.\]

Here \(\pi_{\text{base}}\) is a baseline linear controller, and \(M^{t}\) are matrices, learned via gradient descent or similar algorithms. For linear systems, the DAC law is a convex relaxation of linear policies, which allows us to prove regret bounds against powerful policy classes using tools from online convex optimization.

To generalize this approach, without a model or knowledge of the dynamics function \(f\), both defining and obtaining this disturbance in order to implement DAC or similar policies becomes unclear. Toaddress this, we introduce the concept of a _Pseudo-Disturbance_ (PD) and provide three distinct variants, each representing a novel signal in reinforcement learning. These signals have various advantages and disadvantages depending on the available environment:

1. The first notion is based on the gradient of the temporal-difference error. It assumes the availability of a value function oracle that can be evaluated or estimated online or offline using any known methodology.
2. The second notion also assumes the availability of a black-box value function oracle/generator. We assign artificial costs over the states and generate multiple auxiliary value functions to create a "value vector." The Pseudo-Disturbance is defined as the difference between the value vector at consecutive states. This signal's advantage is that it does not require any zero-order optimization mechanism for estimating the value function's gradient.
3. The third notion assumes the availability of an environment simulator. The Pseudo-Disturbance is defined as the difference between the true state and the simulated state for a specific action.

For all these Pseudo-Disturbance variants, we demonstrate how to efficiently compute them (under the appropriate assumption of either a value function oracle or simulator). We provide a reduction from any RL algorithm to a PD-based robust counterpart that converts an RL algorithm into one that is also robust to adversarial noise. Specifically, in the special case of linear dynamical systems our algorithm has provable regret bounds. The formal description of our algorithm, as well as a theorem statement, are given in Section 4. For more general dynamical systems, the learning problem is provably intractable. Nonetheless, we demonstrate the efficacy of these methods empirically.

### Related Work

Model-free reinforcement learning.Reinforcement learning (Sutton and Barto, 2018) approaches are classified as model-free or model-based (Janner et al., 2019; Ha and Schmidhuber, 2018; Osband and Van Roy, 2014), dependent on if they attempt to explicitly try to learn the underlying transition dynamics an agent is subject to. While the latter is often more sample efficient (Wang et al., 2019), model-free approaches scale better in that their performance does not prematurely saturate and keeps improving with number of episodes (Duan et al., 2016). In this paper, we focus on adaption to unknown, arbitrary disturbances for model-free reinforcement learning algorithms, which can be viewed as a tractable restriction of the challenging adversarial MDP setting (Abbasi Yadkori et al., 2013). Model-free approaches may further be divided into policy-based (Schulman et al., 2015, 2017), value-based approaches (Mnih et al., 2013), and actor-critic approaches (Barth-Maron et al., 2018; Lillicrap et al., 2016); the latter use a learnt value function to reduce the variance for policy optimization.

Robust and Adaptive reinforcement learning.Motivated by minimax performance criterion in robust control (Zhang et al., 2021; Morimoto and Doya, 2005) introduced to a minimax variant of Q-learning to enhance of he robust of policies learnt from off-policy samples. This was later extended to more tractable formulations and structured uncertainty sets in Tessler et al. (2019); Mankowitz et al. (2019); Pinto et al. (2017); Zhang et al. (2021); Tamar et al. (2013), including introductions of model-based variants (Janner et al., 2019). Another approach to enhance the robustness is Domain Randomization (Tobin et al., 2017; Akkaya et al., 2019; Chen et al., 2021), wherein a model is trained in a variety of randomized environments in a simulator, and the resulting policy becomes robust enough to be applied in the real world. Similarly, adversarial training (Mandlekar et al., 2017; Vinitsky et al., 2020; Agarwal et al., 2021) has been shown to improve performance in out-of-distribution scenarios. In contrast to the previously mentioned approaches, our proposed approach only adapts the policy to observed disturbances at test time, and does not require a modification of the training procedure. This notably means that the computational cost and sample requirement of the approach matches that of vanilla RL in training, and has the benefit of leveraging recent advances in mean-reward RL, which is arguably better understood and more studied. Adaption of RL agents to new and changing environments has been similarly tackled through the lens of Meta Learning and similar approaches (Wang et al., 2016; Nagabandi et al., 2018; Pritzel et al., 2017; Agarwal et al., 2021).

Online nonstochastic control.The presence of arbitrary disturbances during policy execution had been for long in the fields of robust optimization and control (Zhou and Doyle, 1998). In contrast to minimax objectives considered in robust control, online nonstochastic control algorithms (see Hazan and Singh (2022) for a survey) are designed to minimize regret against a benchmark policy class, and thus compete with the best policy from the said class determined posthoc. When the benchmark policy class is sufficiently expressive, this approach has the benefit of robustness against adversarially chosen disturbances (i.e. non-Gaussian and potentially adaptively chosen (Ghai et al., 2021)), while distinctly not sacrificing performance in the typical or average case. The first nonstochastic control algorithm with sublinear regret guarantees was proposed in Agarwal et al. (2019) for linear dynamical systems. It was subsequently extended to partially observed systems (Simchowitz et al., 2020), unknown systems (Hazan et al., 2020), multi-agent systems (Ghai et al., 2022) and the time-varying case (Minasyan et al., 2021). The regret bound was improved to a logarithmic rate in Simchowitz (2020) for strongly convex losses. Chen et al. (2021) extend this approach to non-linearly parameterized policy classes, like deep neural networks. Bandit versions of the nonstochastic control setting have also been studied (Gradu et al., 2020; Cassel and Koren, 2020; Sun et al., 2023) and are particularly relevant to the RL setting, which only has access to scalar rewards.

### Paper Outline

After some basic definitions and preliminaries in Section 2, we describe the new Pseudo-Disturbance signals and how to create them in a model-free reinforcement learning environment in Section 3. In Section 4 we give a unified meta-algorithm that exploits these signals and applies them as an augmentation to any given RL agent. In Section 5 we evaluate our methods empirically.

An overview of notation can be found in Appendix A. Appendix B contains additional experimental details. Generalization of our algorithm to discrete spaces is provided in Appendix C. Proofs for Section 3 are provided in Appendix D, while the main theore is proved in Appendix E.

## 2 Setting and Preliminaries

Consider an agent adaptively choosing actions in a dynamical system with adversarial cost functions. We use notation from the control literature: \(\mathbf{x}_{t}\in\mathbb{R}^{d_{x}}\) is a vector representation of the state2 at time \(t\), \(\mathbf{u}_{t}\in\mathbb{R}^{d_{u}}\) is the corresponding action. Formally, the evolution of the state will follow the equations

Footnote 2: Although we consider continuous state and action spaces in this section and the remainder of the main paper, we handle discrete spaces in Appendix C.

\[\mathbf{x}_{t+1}=f(\mathbf{x}_{t},\mathbf{u}_{t})+\mathbf{w}_{t},\]

where \(\mathbf{w}_{t}\) is an arbitrary (even adversarial) disturbance the system is subject to at time \(t\). Following this evolution, the agent suffers a cost of \(c_{t}(\mathbf{x}_{t},\mathbf{u}_{t})\).

In this work, we adapt model-free reinforcement learning algorithms to this more challenging case. The (easier) typical setting for model-free methods assume, in contrast, that the disturbance \(\mathbf{w}_{t}\) is sampled _iid_ from a distribution \(\mathcal{D}\), and that the cost functions \(c(\mathbf{x},\mathbf{u})\) is fixed and known. Central to the study of model-free methods are the notions of the state and state-action value functions, defined as the discounted sum of future costs acquired by starting at any state (or state-action pair) and thereafter following the policy \(\pi\). For any policy \(\pi\), we denote the state and state-action value functions, which are mappings from state or state/action pair to the real numbers, as

\[Q_{\pi}(\mathbf{x},\mathbf{u})=\mathbb{E}\left[\left.\sum_{t=0}^{\infty} \gamma^{t}c(\mathbf{x}_{t}^{\pi},\mathbf{u}_{t}^{\pi})\right|\mathbf{x}_{0}^{ \pi}=\mathbf{x},\mathbf{u}_{0}^{\pi}=\mathbf{u}\right]\,V_{\pi}(\mathbf{x})=\mathbb{E} \left[\left.\sum_{t=0}^{\infty}\gamma^{t}c(\mathbf{x}_{t}^{\pi},\mathbf{u}_{t}^ {\pi})\right|\mathbf{x}_{0}^{\pi}=\mathbf{x}\right]\,\]

where expectations are taken over random transitions in the environment and in the policy.

A special case we consider is that of linear dynamical systems. In these special instances the state involves linearly according to a linear transformation parameterized by matrices \(A,B\), i.e.

\[\mathbf{x}_{t+1}=A\mathbf{x}_{t}+B\mathbf{u}_{t}+\mathbf{w}_{t}.\]Pseudo-Disturbance Signals and Policies

In this section we describe the three different Pseudo-Disturbance (PD) signals we can record in a general reinforcement learning problem. As discussed, the motivation for this signal comes from the framework of online nonstochastic control. We consider dynamical systems with an additive misspecification or noise structure,

\[\mathbf{x}_{t+1}=f(\mathbf{x}_{t},\mathbf{u}_{t})+\mathbf{w}_{t},\]

where the perturbation \(\mathbf{w}_{t}\) does not depend on the state. Using perturbations rather than state allows us to avoid recursive structure that makes the optimization landscape challenging and nonconvex. As discussed, we introduce Pseudo-Disturbance signals \(\hat{\mathbf{w}}_{t}\in\mathbb{R}^{d_{w}}\) in lieu of the true disturbances. We note that the PD dimensionality \(d_{w}\) need not be the same as that of the true disturbance, \(d_{x}\).

An important class of policies that we consider henceforth is linear in the Pseudo-Disturbance, i.e.

\[\Pi_{\text{DAC}}=\left\{\left.\pi(\mathbf{x}_{1:t})=\pi_{\text{base}}(\mathbf{ x}_{t})+\sum_{i=1}^{h}M_{i}\hat{\mathbf{w}}_{t-i}\right|M_{i}\in\mathbb{R}^{d_ {u}\times d_{w}}\right\}.\]

Here \(\Pi_{\text{DAC}}\) denotes the policy class of Disturbance-Action-Control. The fact that \(\mathbf{w}_{t}\) does not depend on our actions allows for convex optimization of linear disturbance-action controllers in the setting of linear dynamical systems, see e.g. Hazan and Singh (2022).

We would like to capture the essence of this favorable phenomenon in the context of model free RL, but what would replace the perturbations \(\mathbf{w}_{t}\) without a dynamics model \(f\)? That's the central question of this section, and we henceforth give three different proposal for this signal.

An important goal in constructing these signals is that **in the case of linear dynamical systems, it recovers the perturbation**. This will enable us to prove regret bounds in the case the environment is an LDS.

### Pseudo-Disturbance Class I: Value-Function Gradients

The first signal we consider is based on the gradient of the value function. The value function maps the state onto a scalar, and this information is insufficient to recreate the perturbation even if the underlying environment is a linear dynamical system. To exact a richer signal, we thus consider the gradient of the value function with respect to the action and state. The basic goal is to implement the following equation

\[\hat{\mathbf{w}}_{t}=\nabla_{\mathbf{u}}(\gamma V_{\pi}(f(\mathbf{x}_{t}, \mathbf{u})+\mathbf{w}_{t})-(Q_{\pi}(\mathbf{x}_{t},\mathbf{u})-c(\mathbf{x}_ {t},\mathbf{u}))|_{\mathbf{u}=\mathbf{u}_{t}}\,\]

where \(f(\mathbf{x}_{t},\mathbf{u})+\mathbf{w}_{t}\) represents the counterfactual next state after playing \(\mathbf{u}\) at state \(\mathbf{x}_{t}\). Note, this signal is a gradient of the temporal-difference error (Sutton and Barto, 2018), in fact being syntactically similar to expected SARSA. If \(\mathbf{w}_{t}\) was in fact (_iid_) stochastic with \(V_{\pi}\), \(Q_{\pi}\) as corresponding value functions, this term on expectation would be zero. Therefore, this signal on average measures deviation introduced in \(\mathbf{x}_{t+1}\) due to arbitrary or adversarial \(\mathbf{w}_{t}\). We can also view this expression as

\[\hat{\mathbf{w}}_{t}=\nabla_{\mathbf{u}}(\gamma V_{\pi}(f(\mathbf{x}_{t}, \mathbf{u})+\mathbf{w}_{t})-\gamma V_{\pi}(f(\mathbf{x}_{t},\mathbf{u})))|_{ \mathbf{u}=\mathbf{u}_{t}}\.\]

\(V_{\pi}\) is quadratic in the linear quadratic regulator setting, so this becomes a linear function of \(\mathbf{w}_{t}\). Computing \(\nabla_{\mathbf{u}}V_{\pi}(f(\mathbf{x}_{t},\mathbf{u})+\mathbf{w}_{t})|_{ \mathbf{u}=\mathbf{u}_{t}}\) analytically would require knowledge of the dynamics, but luckily this can be efficiently estimated online. Using a policy \(\pi\), with noised actions \(\mathbf{u}_{t}=\pi(\mathbf{x}_{t})+\mathbf{n}_{t}\), for \(\mathbf{n}_{t}\sim\mathcal{N}(0,\Sigma)\) we have the following PD estimates:

\[\boxed{\hat{\mathbf{w}}_{t}=\gamma V_{\pi}(\mathbf{x}_{t+1})\Sigma^{-1} \mathbf{n}_{t}-\nabla_{\mathbf{u}}(Q_{\pi}(\mathbf{x}_{t},\mathbf{u})-c( \mathbf{x}_{t},\mathbf{u}))|_{\mathbf{u}=\mathbf{u}_{t}}\,}\] (1)

\[\boxed{\hat{\mathbf{w}}_{t}=(c(\mathbf{x}_{t},\mathbf{u}_{t})+\gamma V_{\pi}( \mathbf{x}_{t+1})-Q_{\pi}(\mathbf{x}_{t},\mathbf{u}_{t}))\Sigma^{-1}\mathbf{n}_ {t}\.}\] (2)

These are zeroth-order gradient estimators (see (Liu et al., 2020) for a more detailed exposition). Intuitively, the second estimator may have lower variance as the expected SARSA error can be much smaller than the magnitude of the value function. An additional benefit is that this implementation only requires a scalar cost signal without needing access to a differentiable cost function.

The most important property of this estimator is that it, in expectation, it produces a signal that is a linearly transformation of the true disturbance if the underlying setting is a linear dynamical system. This is formalized in the following lemma.

**Lemma 1**.: _Consider a time-invariant linear dynamical systems with system matrices \(A,B\) and quadratic costs, along with a linear baseline policy \(\pi\) defined by control law \(\mathbf{u}_{t}=-K_{\pi}\mathbf{x}_{t}\). In expectation, the pseudo disturbances (1) and (2) are linear transformations of the actual perturbation_

\[\mathbb{E}[\hat{\mathbf{w}}_{t}|\mathbf{x}_{t}]=T\mathbf{w}_{t},\]

_where \(T\) is a fixed linear operator that depends on the system._

### Pseudo-Disturbance Class II: Vector Value Functions

The second approach derives a signal from auxiliary value functions. Concretely, instead of scalar-valued cost function \(c:\mathbb{R}^{d_{x}}\rightarrow\mathbb{R}\), consider a vector-valued cost function \(\mathbf{c}:\mathbb{R}^{d_{x}}\rightarrow\mathbb{R}^{d_{w}}\). For such vector-valued cost, we introduce vectorized value and state-action value functions as

\[V^{\mathbf{c}}_{\pi}:\mathbb{R}^{d_{x}}\rightarrow\mathbb{R}^{d_{w}}\,\ Q^{ \mathbf{c}}_{\pi}:\mathbb{R}^{d_{x}}\times\mathbb{R}^{d_{w}}\rightarrow\mathbb{ R}^{d_{w}}\.\]

In particular, we have

\[Q^{\mathbf{c}}_{\pi}(\mathbf{x},\mathbf{u})=\mathbb{E}\left[\left.\sum_{t=0}^ {\infty}\gamma^{t}\mathbf{c}(\mathbf{x}^{\tau}_{t})\right|\mathbf{x}^{\tau}_ {0}=\mathbf{x},\mathbf{u}^{\tau}_{0}=\mathbf{u}\right]\,V^{\mathbf{c}}_{\pi}(\mathbf{x})= \mathbb{E}\left[\left.\sum_{t=0}^{\infty}\gamma^{t}\mathbf{c}(\mathbf{x}^{\tau }_{t})\right|\mathbf{x}^{\tau}_{0}=\mathbf{x}\right]\.\]

Our PD signal is then

\[\boxed{\hat{\mathbf{w}}_{t}=\mathbf{c}(\mathbf{x}_{t})+\gamma\mathbf{V}^{ \mathbf{c}}_{\pi}(\mathbf{x}_{t+1})-\mathbf{Q}^{\mathbf{c}}_{\pi}(\mathbf{x}_ {t},\mathbf{u}_{t})\.}\] (3)

In contrast to the first approach, for a fixed set of cost functions, this approach provides a deterministic PD-signal. This is very beneficial, as at inference time the DAC policy can be run without injecting additional noise and without requiring a high variance stochastic signal. This does come at a cost, as this method requires simultaneous off-policy evaluation for many auxiliary value functions (each corresponding to a different scalar cost) before DAC can be run via \(Q\)-function evaluations at inference, both of which can be significantly more expensive than the first approach.

For the case of linear dynamical systems, if we use _linear_ costs on top of a linear base policy, this approach can recover the disturbances up to a linear transformation. It can be seen that the values corresponding to a linear cost function \(c\) are linear functions of the state, and hence the vectorized versions are also linear functions of state. We formalize this as follows:

**Lemma 2**.: _Consider a time-invariant linear dynamical systems with system matrices \(A,B\), along with a linear baseline policy \(\pi\) defined by control law \(\mathbf{u}_{t}=-K_{\pi}\mathbf{x}_{t}\). Let \(\mathbf{V}^{\mathbf{c}}_{\pi}\) and \(\mathbf{Q}^{\mathbf{c}}_{\pi}\) be value functions for \(\pi\) for i.i.d. zero mean noise with linear costs \(\mathbf{c}(x):=Lx\), then the PD-signal (3) is a linear transformation_

\[\hat{\mathbf{w}}_{t}=T\mathbf{w}_{t},\]

_where \(T\) is a fixed linear operator that depends on the system and baseline policy \(\pi\). In addition, if \(L\) is full rank and the closed loop dynamics are stable, then \(T\) is full rank._

### Pseudo-Disturbance Class III: Simulator Based

The last Pseudo-Disturbance signal we consider requires a potentially inaccurate simulator. It is intuitive, particularly simple to implement, and yet comes with theoretical guarantees.

The Pseudo-Disturbance is taken to be the difference between the actual state reached in an environment, and the expected state, over the randomness in the environment. To compute the expected state, we require the simulator \(f_{\text{sim}}\) initialized at the current state. Formally,

\[\boxed{\hat{\mathbf{w}}_{t}=\mathbf{x}_{t+1}-f_{\text{sim}}(\mathbf{x}_{t}, \mathbf{u}_{t}).}\] (4)

The simplicity of this PD is accompanied by a simple lemma on its characterization of the disturbance in a dynamical system, even if that system is time varying, as follows,

**Lemma 3**.: _Suppose we have a simulator \(f_{\text{sim}}\) such that \(\forall\mathbf{x},\mathbf{u},\|f_{\text{sim}}(\mathbf{x},\mathbf{u})-f( \mathbf{x},\mathbf{u})\|\ \leq\ \delta\), then Pseudo-Disturbance (4) is approximately equal to the actual perturbation \(\|\bar{\mathbf{w}}_{t}-\mathbf{w}_{t}\|\ \leq\ \delta\)._

### Merits of different Pseudo-Disturbance signals

Each of the three PD signals described in this section offers something a bit different. PD3 offers the most direct disturbance signal, but comes with the requirement of a simulator. If the simulator is very accurate, this is likely the strongest signal, though this method may not be suitable with a large sim-to-real gap. PD1 and PD2 on the other hand, do not require a simulator but also have a natural trade off. PD1 is simpler and easier to add on top of an existing policy. However, it uses zeroth-order estimation, so the guarantees only hold in expectation and it may have high variance. On the other hand, PD2 is not a stochastic estimate, but it requires auxiliary value estimation from the base policy. This may come at the cost of additional space and computational complexity. In many cases, this can be handled using the same deep Q-network except with a wider head, which may not be so onerous. We note that PD2 **does not** require specific domain engineered signals for the auxiliary rewards. For example, using the coordinates of the state representation was enough to demonstrate improvements over baselines in our experiments. For richer, higher dimensional (visual) state spaces, this can be generalized using neural representations of state as the auxiliary reward, achieved by modulating the PD2 disturbance dimension to account for the fact that the underlying dynamics are simpler.

## 4 Meta Algorithm and Main Theorem

In this section we define a meta-algorithm for general reinforcement learning. The algorithm takes as an input an existing RL method, that may or may not have theoretical guarantees. It adds an additional layer on top, which estimates the Pseudo-Disturbances according to one of the three methods in the previous section. It then uses an online gradient method to optimize a linear policy in the past Pseudo-Disturbances. This can be viewed as a zeroth-order model-free version of the Gradient Perturbation Controller (GPC) [Agarwal et al., 2019].

The algorithm is formally defined in Algorithm 1. A typical choice of the parametrization \(\pi(\cdot|M)\) is a linear function of a window of past disturbances (ie. Disturbance Action Control [Agarwal et al., 2019]).

\[\pi(\mathbf{w}_{t-1:t-h}|M_{1:h})=\sum_{i=1}^{h}M_{i}\mathbf{w}_{t-i}.\] (5)

```
1:Input: Memory parameter \(h\), learning rate \(\eta\), exploration noise covariance \(\Sigma\), initialization \(M_{1:h}^{1}\in\mathbb{R}^{d_{u}\times d_{u}\times h}\), initial value and \(Q\) functions, base RL algorithm \(\mathcal{A}\).
2:for\(t=1\dots T\)do
3: Use action \(\mathbf{u}_{t}=\pi_{\text{base}}(\mathbf{x}_{t})+\pi(\hat{\mathbf{w}}_{t-1:t-h }|M^{t})+\mathbf{n}_{t}\), where \(\mathbf{n}_{t}\) is _iid_ Gaussian, i.e. \[\mathbf{n}_{t}\sim\mathcal{N}(0,\Sigma)\]
4: Observe state \(\mathbf{x}_{t+1}\), and cost \(c_{t}=c_{t}(\mathbf{x}_{t},\mathbf{u}_{t})\).
5: Compute Pseudo-Disturbance [see (2),(3),(4)] \[\hat{\mathbf{w}}_{t}=\text{PD-estimate}(\mathbf{x}_{t+1},\mathbf{x}_{t}, \mathbf{u}_{t},c_{t},\mathbf{n}_{t}).\]
6: Update policy parameters using the stochastic gradient estimate (see Section 4.1) \[M^{t+1}\gets M^{t}-\eta\;c_{t}(\mathbf{x}_{t},\mathbf{u}_{t})\Sigma^{-1} \sum_{j=0}^{h-1}\mathbf{n}_{t-i}\otimes J_{i}^{t},\] where \(\otimes\) is an outer product and \(J_{i}^{t}=\hat{\mathbf{w}}_{t-i-1:t-h-i}\) for (5), and more generally, \[J_{i}^{t}=\left.\frac{\partial\pi(\hat{\mathbf{w}}_{t-i-1:t-h-i}|M_{i})}{ \partial M}\right|_{M=M^{t}}.\]
7:endfor
8:Optionally, update the policy \(\pi_{\text{base}}\) and its \(Q,V\) functions using \(\mathcal{A}\) so that they are Bellman consistent, i.e. they satisfy the policy version of Bellman equation. ```

**Algorithm 1** MF-GPC (Model-Free Gradient Perturbation Controller)We prove the following theorem for the case of linear dynamics:

**Theorem 4** (Informal Statement (see Theorem 8)).: _If the underlying dynamics are linear with the state evolution specified as_

\[\mathbf{x}_{t+1}=A\mathbf{x}_{t}+B\mathbf{u}_{t}+\mathbf{w}_{t},\]

_with \(d_{\min}=\min\{d_{x},d_{u}\}\), then then as long as the Pseudo-Disturbance signal \(\hat{\mathbf{w}}_{t}\) satisfies \(\hat{\mathbf{w}}_{t}=T\mathbf{w}_{t}\), for some (possibly unknown) invertible map \(T\), Algorithm 1 generates controls \(\mathbf{u}_{t}\) such that for any sequence of bounded (even adversarial) \(\mathbf{w}_{t}\) such that the following holds_

\[\sum_{t}c_{t}(\mathbf{x}_{t},\mathbf{u}_{t})\ \leq\ \min_{\pi\in\Pi^{O_{AC}}} \sum_{t}c_{t}(\mathbf{x}_{t}^{\pi},\mathbf{u}_{t}^{\pi})+\widetilde{\mathcal{ O}}(\sqrt{d_{u}d_{\min}}T^{3/4}),\]

_for any any sequence of convex costs \(c_{t}\), where the policy class \(DAC\) refers to all policies \(\pi\) that produce a control as a linear function of \(\mathbf{w}_{t}\). Further, if the costs \(c_{t}\) are \(L\)-smooth, the regret for Algorithm 1 admits an improved upper bound of \(\widetilde{\mathcal{O}}((d_{u}d_{\min}T)^{2/3})\)._

In particular, the above theorem implies the stated regret bounds when the Pseudo-Disturbance is estimated as described in Equations 3 (Vector Value Function-based) and 4 (Simulator-based).

The regret bounds in Theorem 4 are strict improvements over state-of-the-art bounds in terms of dimension dependence; the latter operate with explicit descriptions of disturbances. This is achieved by using a better choice of gradient estimator, using exploration in action-space rather than parameter space. As a result, our bounds have no dependence on the state dimension since \(d_{\min}\ \leq\ d_{u}\). As an instructive case, for high-dimensional underactuated systems, where \(d_{u}<d_{x}\), our regret bounds scale as \(\tilde{O}(d_{u}T^{3/4})\) in contrast to \(\tilde{O}(d_{u}d_{x}^{1/2}T^{3/4})\) for convex costs from [Gradu et al., 2020, Cassel and Koren, 2020], and as \(\tilde{O}(d_{u}^{4/3}T^{2/3})\) for smooth costs improving over \(\tilde{O}(d_{u}^{4/3}d_{x}^{2/3}T^{2/3})\) from [Cassel and Koren, 2020]. Note that the ratio by which we improve here can be unbounded, with larger improvements for high-dimensional (\(d_{x}\gg 1\)) systems. See Appendix E.2 for further details, comparisons and proofs.

### Derivation of update

In the algorithm, the key component is computing an approximate policy gradient of the cost. A complete theoretical analysis of our algorithm can be found in Appendix E, but we provide a brief sketch of the gradient calculation. Let \(J_{t}(M)\) denote the expected counterfactual cost \(c_{t}\) of following policy \(M\) with the same observed disturbances \(w_{t}\). We first note that if the dynamics are suitably stabilized (which should be done by \(\pi_{\text{base}}\)), the state and cost can be approximated as a function \(C\) of a small window of previous controls.

\[J_{t}(M)=\mathbb{E}_{\mathbf{n}_{1:t}}[c_{t}(\mathbf{x}_{t}^{M},\mathbf{u}_{t} ^{M})]\approx\mathbb{E}_{\mathbf{n}_{t-h:t}}[C(\mathbf{u}_{t}(M)+\mathbf{n}_{ t},\dots,\mathbf{u}_{t-h}(M)+\mathbf{n}_{t-h})]\;,\]

where we use \(u_{t-i}(M)\) as a shorthand for \(\pi(\hat{\mathbf{w}}_{t-i-1:t-h-i}|M)\). The expression here is that of a Gaussian smoothed function, which allows us to get the following unbiased single point gradient estimate

\[\nabla_{\mathbf{u}_{i}}\mathbb{E}_{\mathbf{n}_{t-h:t}}[C(\mathbf{u}_{t}+ \mathbf{n}_{t},\dots,\mathbf{u}_{t-h}+\mathbf{n}_{t-h})]=\mathbb{E}_{\mathbf{ n}_{t-h:t}}[\Sigma^{-1}C(\mathbf{u}_{t}+\mathbf{n}_{t},\dots,\mathbf{u}_{t-h}+ \mathbf{n}_{t-h})\mathbf{n}_{i}]\;.\]

We use a single sample to get a stochastic gradient. Using the chain rule, which involves an outer product due to the tensor structure of \(M\), we get stochastic gradients with respect to \(M\) as follows

\[\widehat{\nabla_{M}}J_{t}(M)\approx C(\mathbf{u}_{t}(M)+\mathbf{n}_{t},\dots, \mathbf{u}_{t-h}(M)+\mathbf{n}_{t-h})\Sigma^{-1}\sum_{i=0}^{h-1}\mathbf{n}_{t- i}\otimes\frac{\partial\pi(\hat{\mathbf{w}}_{t-i-1:t-h-i}|M)}{\partial M}\;.\]

Finally, we note that \(M^{t}\) is slowly moving because of gradient descent, so we can approximate

\[c_{t}(\mathbf{x}_{t},\mathbf{u}_{t})\approx C(\mathbf{u}_{t}(M^{t})+\mathbf{n }_{t},\dots,\mathbf{u}_{t-h}(M^{t})+\mathbf{n}_{t-h}).\]

Putting everything together, we have

\[\widehat{\nabla_{M}}J_{t}(M)\Big{|}_{M=M^{t}}\approx\ c_{t}(\mathbf{x}_{t}, \mathbf{u}_{t})\Sigma^{-1}\sum_{i=0}^{h-1}\mathbf{n}_{t-i}\otimes\left.\frac{ \partial\pi(\hat{\mathbf{w}}_{t-i-1:t-h-i}|M)}{\partial M}\right|_{M=M^{t}}.\] (6)

## 5 Experiments

We apply the MF-GPC Algorithm 1 to various OpenAI Gym (Brockman et al., 2016) environments. We conduct our experiments in the research-first modular framework Acme (Hoffman et al., 2020). We pick \(h=5\) and use the DDPG algorithm (Lillicrap et al., 2016) as our underlying baseline. We update the \(M\) matrices every 3 episodes instead of continuously to reduce runtime. We also apply weight decay to line 6 of Algorithm 1. Our implementation of PD1 is based on Equation 2. PD2 can be implemented with any vector of rewards. We choose linear function \(L\) given in Lemma 2 to be the identity function. Hence \(\mathbf{c}\) in Equation 3 reduces to the state \(x_{t}\) itself. We pick \(\mathbf{V}\) and \(\mathbf{Q}\) network architectures to be the first \(d_{x}\) units of the last layer of the critic network architecture. We train for 1e7 steps as a default (this is also the default in the Acme code) and if performance has not converged we extend to 1.5e7 steps. Because the \(M\) matrices impact the exploration of the algorithm, we tune the exploration parameter \(\sigma\) for both DDPG and MF-GPC. For the baseline DDPG, we typically explore \(\sigma\in\{0.15,0.2,0.25\}\). More experimental details may be found in Appendix Section B.

Results for Noisy Hopper, Walker 2D, and AntWe create a noisy Hopper, Walker 2D, and Ant environments by adding a Uniform random variable \(U[-0.1,0.1]\) to the state. The noise is added at every step for both the DDPG baseline and our MF-GPC. We plot the results for PD2, and PD3 in Figure 1. We find that PD2 and PD3 perform relatively well in these settings. Graphs depicting all runs for different \(\sigma\) are available in Appendix Section B. MF-GPC is not guaranteed to improve performance in realistic RL settings. We find that generally PD1 does not perform well e.g. in Figure 2 a) and some examples where applying it yields performance similar to baseline are given in Appendix Section B. This is likely due to the high variance of the PD estimate. We find that neither our method nor the baseline is too sensitive to our hyper-parameter tuning (Figure 2 b) ), possibly because we start with the default Acme parameters which are already well tuned for the noiseless environment.

Linear Dynamical SystemsWe evaluate our methods on both low dimensional (\(d_{x}=2,d_{u}=1\)) and a higher dimensional (\(d_{x}=10,d_{u}=5\)) linear systems with sinusoidal disturbances to demonstrate the improvements in dimension of our method (labeled RBPC) over BPC (Gradu et al., 2020). We use the full information GPC (Agarwal et al., 2019) and LQR as baselines using implementations from Gradu et al. (2021). While performance is comparable to BPC on the small system, on the larger system, BPC could not be tuned to learn while RBPC improves upon the LQR

Figure 1: Episode return for best performing MF-GPC model versus best performing baseline DDPG model for various OpenAI Gym environments and pseudo-estimation methods. Environment and pseudo-estimation method shown in title. Results averaged over 25 seeds. Shaded areas represent confidence intervals. We find that PD2 and PD3 perform well in these settings.

baseline (see Figure 3). In both experiments, \(h=5\) and the learning rate and exploration noise is tuned.

## 6 Conclusion

We have described a new approach for model-free RL based on recent exciting advancements in model based online control. Instead of using state-based policies, online nonstochastic control proposes the use of disturbance-based policies. To create a disturbance signal without a model, we define three possible signals, called Pseudo-Disturbances, each with its own merits and limitations. We give a generic (adaptable) REINFORCE-based method using the PD signals with provable guarantees: if the underlying MDP is a linear dynamical system, we recover and improve the strong guarantees of online nonstochastic control. Preliminary promising experimental results are discussed. We believe this is a first step in the exciting direction of applying tried-and-tested model-based control techniques for general reinforcement learning.

## Acknowledgments and Disclosure of Funding

Elad Hazan acknowledges funding from the ONR award N000142312156, the NSF award 2134040, and Open Philanthropy.

## References

* Yadkori et al. (2013) Yasin Abbasi Yadkori, Peter L Bartlett, Varun Kanade, Yevgeny Seldin, and Csaba Szepesvari. Online learning in markov decision processes with adversarially chosen transition probability distributions. _Advances in neural information processing systems_, 26, 2013.
* Agarwal et al. (2019) Naman Agarwal, Brian Bullins, Elad Hazan, Sham Kakade, and Karan Singh. Online control with adversarial disturbances. In _International Conference on Machine Learning_, pages 111-119. PMLR, 2019.
* Agarwal et al. (2021) Naman Agarwal, Elad Hazan, Anirudha Majumdar, and Karan Singh. A regret minimization approach to iterative learning control. In _International Conference on Machine Learning_, pages 100-109. PMLR, 2021.

Figure 3: Comparison on low dimensional (left) vs high dimensional (rights) LDS.

Figure 2: Left: Episode return for PD1 for Noisy Hopper. We find that PD1 is not effective for RL settings. Right: Hyper-parameter search for PD3 on Noisy Walker. We find that neither Meta-GPC nor the baseline DDPG algorithm is too sensitive to tuning.

Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik's cube with a robot hand. _arXiv preprint arXiv:1910.07113_, 2019.
* Barth-Maron et al. [2018] Gabriel Barth-Maron, Matthew W Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva Tb, Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. Distributed distributional deterministic policy gradients. _arXiv preprint arXiv:1804.08617_, 2018.
* Bertsekas [2012] Dimitri Bertsekas. _Dynamic programming and optimal control: Volume I_, volume 1. Athena scientific, 2012.
* Brockman et al. [2016] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. _CoRR_, abs/1606.01540, 2016. URL http://arxiv.org/abs/1606.01540.
* Cassel and Koren [2020] Asaf Cassel and Tomer Koren. Bandit linear control. _Advances in Neural Information Processing Systems_, 33:8872-8882, 2020.
* Cassel et al. [2022] Asaf Cassel, Alon Cohen, and Tomer Koren. Rate-optimal online convex optimization in adaptive linear control. _arXiv e-prints_, pages arXiv-2206, 2022.
* Chen et al. [2021a] Xiaoyu Chen, Jiachen Hu, Chi Jin, Lihong Li, and Liwei Wang. Understanding domain randomization for sim-to-real transfer. _arXiv preprint arXiv:2110.03239_, 2021a.
* Chen et al. [2021b] Xinyi Chen, Edgar Minasyan, Jason D Lee, and Elad Hazan. Provable regret bounds for deep online learning and control. _arXiv preprint arXiv:2110.07807_, 2021b.
* Duan et al. [2016] Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In _International conference on machine learning_, pages 1329-1338. PMLR, 2016.
* 08 June 2021.
* Ghai et al. [2022] Udaya Ghai, Udari Madhushani, Naomi Leonard, and Elad Hazan. A regret minimization approach to multi-agent control. In _International Conference on Machine Learning_, pages 7422-7434. PMLR, 2022.
* Goodfellow et al. [2014] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. _arXiv preprint arXiv:1412.6572_, 2014.
* Gradu et al. [2020] Paula Gradu, John Hallman, and Elad Hazan. Non-stochastic control with bandit feedback. _Advances in Neural Information Processing Systems_, 33:10764-10774, 2020.
* Gradu et al. [2021] Paula Gradu, John Hallman, Daniel Suo, Alex Yu, Naman Agarwal, Udaya Ghai, Karan Singh, Cyril Zhang, Anirudha Majumdar, and Elad Hazan. Deluca-a differentiable control library: Environments, methods, and benchmarking. _arXiv preprint arXiv:2102.09968_, 2021.
* Ha and Schmidhuber [2018] David Ha and Jurgen Schmidhuber. World models. _arXiv preprint arXiv:1803.10122_, 2018.
* Hazan and Singh [2022] Elad Hazan and Karan Singh. Introduction to online nonstochastic control. _arXiv preprint arXiv:2211.09619_, 2022.
* Hazan et al. [2020] Elad Hazan, Sham Kakade, and Karan Singh. The nonstochastic control problem. In _Algorithmic Learning Theory_, pages 408-421. PMLR, 2020.
* Hazan et al. [2016] Elad Hazan et al. Introduction to online convex optimization. _Foundations and Trends(r) in Optimization_, 2(3-4):157-325, 2016.
* Hazan et al. [2017]Matthew W Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Nikola Momchev, Danila Sinopalnikov, Piotr Stanczyk, Sabela Ramos, Anton Raichuk, Damien Vincent, et al. Acme: A research framework for distributed reinforcement learning. _arXiv preprint arXiv:2006.00979_, 2020.
* Janner et al. (2019) Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. _Advances in neural information processing systems_, 32, 2019.
* Lillicrap et al. (2016) Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. _ICLR_, 2016.
* Liu et al. (2020) Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred O Hero III, and Pramod K Varshney. A primer on zeroth-order optimization in signal processing and machine learning: Principals, recent advances, and applications. _IEEE Signal Processing Magazine_, 37(5):43-54, 2020.
* Mandlekar et al. (2017) Ajay Mandlekar, Yuke Zhu, Animesh Garg, Li Fei-Fei, and Silvio Savarese. Adversarially robust policy learning: Active construction of physically-plausible perturbations. In _2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 3932-3939. IEEE, 2017.
* J Mankowitz et al. (2019) Daniel J Mankowitz, Nir Levine, Rae Jeong, Yuanyuan Shi, Jackie Kay, Abbas Abdolmaleki, Jost Tobias Springenberg, Timothy Mann, Todd Hester, and Martin Riedmiller. Robust reinforcement learning for continuous control with model misspecification. _arXiv preprint arXiv:1906.07516_, 2019.
* Minasyan et al. (2021) Edgar Minasyan, Paula Gradu, Max Simchowitz, and Elad Hazan. Online control of unknown time-varying dynamical systems. _Advances in Neural Information Processing Systems_, 34:15934-15945, 2021.
* Mnih et al. (2013) Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. _arXiv preprint arXiv:1312.5602_, 2013.
* Morimoto and Doya (2005) Jun Morimoto and Kenji Doya. Robust reinforcement learning. _Neural computation_, 17(2):335-359, 2005.
* Nagabandi et al. (2018) Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Learning to adapt in dynamic, real-world environments through meta-reinforcement learning. _arXiv preprint arXiv:1803.11347_, 2018.
* Osband and Van Roy (2014) Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the eluder dimension. _Advances in Neural Information Processing Systems_, 27, 2014.
* Pinto et al. (2017) Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforcement learning. In _International Conference on Machine Learning_, pages 2817-2826. PMLR, 2017.
* Pritzel et al. (2017) Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adria Puigdomenech Badia, Oriol Vinyals, Demis Hassabis, Daan Wierstra, and Charles Blundell. Neural episodic control. In _International conference on machine learning_, pages 2827-2836. PMLR, 2017.
* Schulman et al. (2015) John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In _International conference on machine learning_, pages 1889-1897. PMLR, 2015.
* Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* Simchowitz (2020) Max Simchowitz. Making non-stochastic control (almost) as easy as stochastic. _Advances in Neural Information Processing Systems_, 33:18318-18329, 2020.
* Szegedy et al. (2015)Max Simchowitz, Karan Singh, and Elad Hazan. Improper learning for non-stochastic control. In _Conference on Learning Theory_, pages 3320-3436. PMLR, 2020.
* Sun et al. (2023) Y. Jennifer Sun, Stephen Newman, and Elad Hazan. Optimal rates for bandit nonstochastic control. _arXiv preprint arXiv:2305.15352_, 2023.
* Sutton and Barto (2018) Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* Tamar et al. (2013) Aviv Tamar, Huan Xu, and Shie Mannor. Scaling up robust mdps by reinforcement learning. _arXiv preprint arXiv:1306.6189_, 2013.
* Tessler et al. (2019) Chen Tessler, Yonathan Efroni, and Shie Mannor. Action robust reinforcement learning and applications in continuous control. In _International Conference on Machine Learning_, pages 6215-6224. PMLR, 2019.
* Tobin et al. (2017) Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In _2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)_, pages 23-30. IEEE, 2017.
* Vinitsky et al. (2020) Eugene Vinitsky, Yuqing Du, Kanaad Parvate, Kathy Jang, Pieter Abbeel, and Alexandre Bayen. Robust reinforcement learning using adversarial populations. _arXiv preprint arXiv:2008.01825_, 2020.
* Wang et al. (2016) Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. _arXiv preprint arXiv:1611.05763_, 2016.
* Wang et al. (2019) Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking model-based reinforcement learning. _arXiv preprint arXiv:1907.02057_, 2019.
* Zhang et al. (2021) Huan Zhang, Hongge Chen, Duane Boning, and Cho-Jui Hsieh. Robust reinforcement learning on state observations with learned optimal adversary. _arXiv preprint arXiv:2101.08452_, 2021.
* Zhou and Doyle (1998) Kemin Zhou and John Comstock Doyle. _Essentials of robust control_, volume 104. Prentice hall Upper Saddle River, NJ, 1998.

###### Contents

* 1 Introduction
	* 1.1 Our Contributions
	* 1.2 Pseudo-Disturbance based RL
	* 1.3 Related Work
	* 1.4 Paper Outline
* 2 Setting and Preliminaries
* 3 Pseudo-Disturbance Signals and Policies
	* 3.1 Pseudo-Disturbance Class I: Value-Function Gradients
	* 3.2 Pseudo-Disturbance Class II: Vector Value Functions
	* 3.3 Pseudo-Disturbance Class III: Simulator Based
	* 3.4 Merits of different Pseudo-Disturbance signals
* 4 Meta Algorithm and Main Theorem
	* 4.1 Derivation of update
* 5 Experiments
* 6 Conclusion
* A Notation
* B Experiments
* B.1 Hopper
* B.2 Noisy Hopper
* B.3 Noisy Walker 2D and Ant
* B.4 Experiments with adversarial noise
* C Discrete State and Action Spaces
* D Pseudo-Disturbance Proofs
* D.1 Proof of Lemma 1
* D.2 Proof of Lemma 2
* D.3 Proof of Lemma 3
* E Main Result and Dimension-efficient Bandit GPC
* E.1 Main Result
* E.2 Dimension-Efficient Bandit GPC
* E.3 Idealized Cost and Proof of Theorem 9
* E.4 Proof of Supporting Claims

## Appendix A Notation

We use the following notation consistently throughout the paper:

\begin{tabular}{l|l} \hline
**Symbol** & **Semantics** \\ \hline \(\otimes\) & outer product \\ \(f\) & dynamics/transition function \\ \(f_{\text{sim}}\) & simulator dynamics/transition function \\ \(d_{x}\) & state dimension \\ \(d_{u}\) & control dimension \\ \(d_{w}\) & pseudo-disturbance \\ \(d_{\text{min}}\) & \(\min(d_{x},d_{u})\) \\ \(\mathbf{x}_{t}\in\mathbb{R}^{d_{x}}\) & state at time \(t\) \\ \(\mathbf{u}_{t}\in\mathbb{R}^{d_{u}}\) & control at time \(t\) \\ \(\mathbf{w}_{t}\in\mathbb{R}^{d_{w}}\) & perturbation (disturbance) at time \(t\) \\ \(c_{t}\) & instantaneous cost at time \(t\) \\ \(\widehat{\mathbf{w}}_{t}\in\mathbb{R}^{d_{w}}\) & pseudo-disturbance at time \(t\) \\ \(\mathbf{n}_{t}\in\mathbb{R}^{d_{u}}\) & Gaussian exploration noise at time \(t\) \\ \(A,B,C\) & system matrices for linear dynamical system \\ \(h\) & history length (i.e., number of parameters) in a policy class \\ \(M^{t}_{1:h}\) & \(h\)-length sequence of matrices used by MF-GPC at time \(t\) \\ \(\mathcal{M}\) & policy class of \(h\)-length matrices \\ \(\gamma\) & discount factor \\ \(V_{\pi},Q_{\pi}\) & state and state-action value functions for \(\pi\) \\ \(\mathbf{v}_{\pi}^{\pi},\mathbf{Q}_{\pi}^{\pi}\) & vectorized value and \(Q\) functions for \(\pi\) for reward vectors \(\mathbf{r}(x)\) \\ \(y_{t}\) & idealized state \\ \(\tilde{C}_{t}\) & stationary idealized cost (function of single \(M\)) at time \(t\) \\ \(C_{t}\) & non-stationary idealized cost (function of memory) at time \(t\) \\ \(C_{t,\delta}\) & Smoothed \(C_{t}\) using noised controls \\ \(F_{t}\) & idealized cost as a function of last controls at time \(t\) \\ \(F_{t,\delta}\) & smoothed \(F_{t}\) \\ \(\|\cdot\|\) & spectral norm \\ \(\cdot\|_{F}\) & Frobenius norm \\ \hline \end{tabular}

## Appendix B Experiments

We test the performance of our method on various OpenAI Gym environments. We conduct our experiments in the research-first modular framework Acme [10]. We pick \(h=5\) and use the DDPG algorithm [11] as our underlying baseline. We update the \(M\) matrices every 3 episodes instead of continuously to reduce runtime. We also apply weight decay to line 6 of Algorithm 1.

Our implementation is based on the Acme implementation of D4PG. The policy and critic networks both have the default sizes of \(256\times 256\times 256\). We use the Acme default number of atoms as 51 for the network. We run in the distributed setting with 4 agents. The underlying learning rate of the D4PG implementation is left at \(3e-04\). The exploration parameter, \(\sigma\) is tuned.

**Plotting** We use a domain invariant exponential smoother with a small smoothing parameter of 0.1. The smoothing is applied before the mean is taken over the data. To construct the confidence intervals, we take the following steps 1) smooth the data 2) linearly interpolate each run of the data to produce a fine grid of values 3) calculate \(\sigma/\sqrt{N}\) on interpolated data.

### Hopper

The OpenAI Gym Hopper environment is a two dimensional one legged figure that consists of four body parts, namely a torso, thigh, leg and foot.

### Noisy Hopper

We create a Noisy Hopper environment by adding a Uniform random variable \(U[-0.1,0.1]\) to qpos during training and evaluation. The noise is added at every step, and both DDPG and MF-GPC are evaluated on the same noisy environment.

**PD1** We implement PD1 for Noisy Hopper according to Equation 2. We see our results in Figure 6 in the first column. We tune \(\sigma\) for the D4PG baseline in the set \(\{0.1,0.15,0.2,0.25,0.3\}\). We find that the \(\sigma=0.1\) performs the best. We tune \(\sigma\) for MF-GPC in the set \(\{0.1,0.15,0.2\}\). We find that the \(\sigma=0.1\) setting performs the best. We averaged our results over 25 seeds. Solid line represents the mean return over all runs and shaded areas represent standard error. We find that MF-GPC run with PD-1 has a small advantage compared to the DDPG baseline on Noisy Hopper. **Removing outliers for PD1** for this specific experiment, we notice that some runs seem to be outliers. Therefore, when plotting, we remove the lowest 2 runs across all groups (of both the baseline and our method). Complete raw data (with outliers included) can be seen in Figure 4.

**PD-2** PD2 can be implemented with any vector of rewards. We choose linear function \(L\) given in Lemma 2 to be the identity function. Hence \(\mathbf{c}\) in Equation 3 reduces to the state \(x_{t}\) itself. We pick \(\mathbf{V}\) and \(\mathbf{Q}\) to be the first \(d_{x}\) units of the last layer of the critic network. If \(d_{x}\) is larger than the number of atoms of the critic network (51) we take all 51 nodes from the critic network. We find that a default \(\sigma=0.15\) performs well for MF-GPC so we do not tune \(\sigma\) further.

**PD-3** In practice the expectation in Equation 4 requires estimation. We use an average over 4 copies of the environment for this estimation.

### Noisy Walker 2D and Ant

We follow the basic procedure for Hopper but train for 15 million steps instead of 10 million. We report our results for the hyper-parameter sweeps in columns 2 and 3 of Figure 5. We find that PD2 and PD3 perform relatively well in these settings.

**PD-2** PD2 can be implemented with any vector of rewards. We choose linear function \(L\) given in Lemma 2 to be the identity function. Hence \(\mathbf{c}\) in Equation 3 reduces to the state \(x_{t}\) itself. Recall that \(d_{x}\) is the dimension of the state space. We pick \(\mathbf{V}\) and \(\mathbf{Q}\) to be the first \(d_{x}\) units of the last layer of the critic network. If \(d_{x}\) is larger than the number of atoms of the critic network (51) we take all 51 nodes from the critic network.

Figure 4: Raw data for PD Estimate 1 on Hopper. Each plot represents either the baseline or our method for a different setting of the exploration parameter, \(\sigma\). We find that there are some outlier runs (for example the horizontal line in subfig f). We remove the two lowest return runs from each group before plotting.

**PD-3** In practice the expectation in Equation 4 requires estimation. We use an average over 4 copies of the environment for this estimation. For Noisy Ant, we find that a default \(\sigma=0.15\) performs well for MF-GPC so we do not tune \(\sigma\) further.

### Experiments with adversarial noise

We run MF-GPC on top of a DDPG baseline for the inverted pendulum environment with 1) fast gradient sign method noise (Goodfellow et al., 2014) and 2) noise from a discretized sinusoid. We plot our results in Figure 7

Figure 5: Episode return for best performing MF-GPC model versus best performing baseline DDPG model for various OpenAI Gym environments and pseudo-estimation methods. Environment and pseudo-estimation method shown in title. Results averaged over 25 seeds. Shaded areas represent confidence intervals. We find that PD2 and PD3 perform well in these settings.

Figure 6: Left: Episode return for PD1 for Noisy Hopper. Right: Episode return for PD1 for Noisy Half Cheetah. We find that PD1 is not effective for RL settings.

Figure 7: Results from our method on the inverted pendulum environment for fast gradient sign method noise and sinusoid noise.

Discrete State and Action Spaces

In this section, we consider differentiable parameterized random policies. For finite action space \(\mathcal{U}\), let \(\Delta^{\mathcal{U}}=\{p:\mathcal{U}\rightarrow[0,1]|\sum_{a\in\mathcal{U}}p(a)=1\}\) be the probability simplex over \(\mathcal{U}\). Our policies, will be parameterized by \(M\) and depend on a window of past Pseudo-disturbances, providing the following distribution over actions.

\[\mathbf{u}\sim\pi(\cdot|\hat{\mathbf{w}}_{t-1:t-h},M)\in\Delta^{\mathcal{U}}\] (7)

The baseline policy, would be built into \(\pi\) in this setting. For example, we could have a softmax neural net policy, and our algorithm adds a residual correction to the logits.

Implementation of PD signals in discrete spacesFor discrete spaces, PD-2 defined via (3) is well defined, as we can still create auxiliary \(Q\) functions in the discrete space to produce our signal. Because, we no longer use Gaussian noise, PD-1 (2) can be modified as follows:

\[\boxed{\hat{\mathbf{w}}_{t}=(c(\mathbf{x}_{t},\mathbf{u}_{t})+\gamma V_{\pi}( \mathbf{x}_{t+1})-Q_{\pi}(\mathbf{x}_{t},\mathbf{u}_{t}))\nabla_{M}\left. \log(\pi(\mathbf{u}_{t}|\hat{\mathbf{w}}_{t-h:t-1},M))\right|_{M=M_{t}}}\end\] (8)

For our zeroth order gradient, we use the REINFORCE:

```
1:Input: Memory parameter \(h\), learning rate \(\eta\), initialization \(M^{1}_{1:h}\), initial value and \(Q\) functions, base RL algorithm \(\mathcal{A}\).
2:for\(t=1\ldots T\)do
3: Sample action \(\mathbf{u}_{t}\sim\pi(\cdot|\hat{\mathbf{w}}_{t-1:t-h},M^{t})\)
4: Observe state \(\mathbf{x}_{t+1}\), and cost \(c_{t}=c_{t}(\mathbf{x}_{t},\mathbf{u}_{t})\).
5: Compute Pseudo-Disturbance [see (3), (8)] \[\hat{\mathbf{w}}_{t}=\text{PD-estimate}(\mathbf{x}_{t+1},\mathbf{x}_{t}, \mathbf{u}_{t},c_{t}).\]
6: Update policy parameters using the stochastic gradient estimate \[M^{t+1}\gets M^{t}-\eta\left.c_{t}(\mathbf{x}_{t},\mathbf{u}_{t})\sum_{j=0 }^{h-1}\nabla_{M}\log(\pi(\mathbf{u}_{t-j}|\hat{\mathbf{w}}_{t-j-1:t-j-h},M)) \right|_{M=M_{t}}\;.\]
7:endfor
8: Optionally, update the baseline policy parameters and its \(Q,V\) functions using \(\mathcal{A}\) so that they are Bellman consistent, i.e. they satisfy the policy version of Bellman equation. ```

**Algorithm 2** DMF-GPC (Discrete Model-Free Gradient Perturbation Controller)

## Appendix D Pseudo-Disturbance Proofs

In this appendix, we have the deferred proofs from Section 3. For convenience, the lemmas have also been restated.

### Proof of Lemma 1

**Lemma 5**.: _For time-invariant linear dynamical systems with system matrices \(A,B\) and quadratic costs, in expectation, the pseudo disturbances (1) and (2) is a linear transformation of the actual perturbation_

\[\mathbb{E}[\hat{\mathbf{w}}_{t}|\mathbf{x}_{t}]=T\mathbf{w}_{t},\]

_where \(T\) is a fixed linear operator that depends on the system._

Proof.: Recall from the theory of the linear quadratic regulator that the value function of an infinite horizon LDS is quadratic Bertsekas (2012),

\[V(\mathbf{x})=\mathbf{x}^{\top}P\mathbf{x}.\]Thus,

\[\mathbb{E}[\nabla_{\mathbf{u}}(Q(\mathbf{x}_{t},\mathbf{u})-c( \mathbf{x}_{t},\mathbf{u}))|_{\mathbf{u}=\mathbf{u}_{t}}] =\mathbb{E}[\gamma B^{\top}P(A\mathbf{x}_{t}+B\mathbf{u}_{t})]\] \[=\mathbb{E}[\gamma B^{\top}P(A\mathbf{x}_{t}+B\pi(\mathbf{x}_{t}) +B\mathbf{n}_{t})]\] \[=\gamma B^{\top}P(A\mathbf{x}_{t}+B\pi(\mathbf{x}_{t})).\]

By the definition of the signal, we have that

\[\mathbb{E}[\hat{\mathbf{w}}_{t}|\mathbf{x}_{t}] =\gamma\mathbb{E}[V(\mathbf{x}_{t+1})\Sigma^{-1}\mathbf{n}_{t}- \nabla_{\mathbf{u}}(Q(\mathbf{x}_{t},\mathbf{u})-c(\mathbf{x}_{t},\mathbf{u}) )|_{\mathbf{u}=\mathbf{u}_{t}}]\] \[=\gamma\mathbb{E}[V(\mathbf{x}_{t+1})\Sigma^{-1}\mathbf{n}_{t}| \mathbf{x}_{t}]-\gamma B^{\top}P(A\mathbf{x}_{t}+B\pi(\mathbf{x}_{t})).\]

Writing the quadratic value function also to the first term, and denoting \(\|\mathbf{x}\|_{P}^{2}=\mathbf{x}^{\top}P\mathbf{x}\), we have that

\[\mathbb{E}[V(\mathbf{x}_{t+1})\Sigma^{-1}\mathbf{n}_{t}|\mathbf{x }_{t}] =\mathbb{E}[\|A\mathbf{x}_{t}+B\pi(\mathbf{x}_{t})+\mathbf{w}_{t} +B\mathbf{n}_{t}\|_{P}^{2}\Sigma^{-1}\mathbf{n}_{t}|\mathbf{x}_{t}]\] \[=\Sigma^{-1}\mathbb{E}[\mathbf{n}_{t}\mathbf{n}_{t}^{\top}]B^{ \top}P(A\mathbf{x}_{t}+B\pi(\mathbf{x}_{t})+\mathbf{w}_{t})\] \[=B^{\top}P(A\mathbf{x}_{t}+B\pi(\mathbf{x}_{t})+\mathbf{w}_{t})\]

We thus conclude,

\[\mathbb{E}[\hat{\mathbf{w}}_{t}|\mathbf{x}_{t}] =\gamma\mathbb{E}[V(\mathbf{x}_{t+1})\Sigma^{-1}\mathbf{n}_{t}| \mathbf{x}_{t}]-\gamma B^{\top}P(A\mathbf{x}_{t}+B\pi(\mathbf{x}_{t}))\] \[=\gamma B^{\top}P(A\mathbf{x}_{t}+B\pi(\mathbf{x}_{t})+\mathbf{w }_{t})-\gamma B^{\top}P(A\mathbf{x}_{t}+B\pi(\mathbf{x}_{t}))\] \[=\gamma B^{\top}P\mathbf{w}_{t}=T\mathbf{w}_{t}\]

as needed. 

### Proof of Lemma 2

**Lemma 6**.: _Consider a time-invariant linear dynamical systems with system matrices \(A,B\), along with a linear baseline policy \(\pi\) defined by control law \(\mathbf{u}_{t}=-K_{\pi}\mathbf{x}_{t}\). Let \(\mathbf{V}_{\pi}^{\mathbf{c}}\) and \(\mathbf{Q}_{\pi}^{\mathbf{c}}\) be value functions for \(\pi\) for i.i.d. zero mean noise with linear costs \(\mathbf{c}(x):=Lx\), then the PD-signal (3) is a linear transformation_

\[\hat{\mathbf{w}}_{t}=T\mathbf{w}_{t},\]

_where \(T\) is a fixed linear operator that depends on the system and baseline policy \(\pi\). In addition, if \(L\) is full rank and the closed loop dynamics are stable, then \(T\) is full rank._

Proof.: We first note that for linear rewards, value functions for i.i.d. zero mean noise are equivalent the value functions without noise. As such, we have the identity

\[\mathbf{Q}_{\pi}^{\mathbf{c}}(\mathbf{x}_{t},\mathbf{u}_{t})=\mathbf{c}( \mathbf{x}_{t})+\gamma\mathbf{V}_{\pi}^{\mathbf{c}}(A\mathbf{x}_{t}+B\mathbf{ u}_{t})\;,\]

and so, we can rewrite or PD-signal as

\[\hat{\mathbf{w}}_{t} =\mathbf{V}_{\pi}^{\mathbf{c}}(\mathbf{x}_{t+1})-\mathbf{V}_{\pi }^{\mathbf{c}}(A\mathbf{x}_{t}+B\mathbf{u}_{t})\] \[=\mathbf{V}_{\pi}^{\mathbf{c}}(A\mathbf{x}_{t}+B\mathbf{u}_{t}+ \mathbf{w}_{t})-\mathbf{V}_{\pi}^{\mathbf{c}}(A\mathbf{x}_{t}+B\mathbf{u}_{t})\;.\]

Now, it remains to show that \(\mathbf{V}_{\pi}^{\mathbf{c}}\) is a fixed linear transformation. Indeed, we have

\[\mathbf{V}_{\pi}^{\mathbf{c}}(x)=\sum_{t=0}^{\infty}\gamma^{i}\mathbf{c}(A_{ \pi}^{t}x)=\sum_{t=0}^{\infty}\gamma^{i}LA_{\pi}^{t}x=L(I-\gamma A_{\pi})^{-1}x\;,\]

where \(A_{\pi}=A-BK_{\pi}\) is the closed loop dynamics matrix. We now have

\[\hat{\mathbf{w}}_{t} =\mathbf{V}_{\pi}^{\mathbf{c}}(A\mathbf{x}_{t}+B\mathbf{u}_{t}+ \mathbf{w}_{t})-\mathbf{V}_{\pi}^{\mathbf{c}}(A\mathbf{x}_{t}+B\mathbf{u}_{t})\] \[=L(I-\gamma A_{\pi})^{-1}\left[(A\mathbf{x}_{t}+B\mathbf{u}_{t}+ \mathbf{w}_{t})-(A\mathbf{x}_{t}+B\mathbf{u}_{t})\right]\] \[=L(I-\gamma A_{\pi})^{-1}\mathbf{w}_{t}\;.\]

Now, stability of \(\pi\) dictates \((I-\gamma A_{\pi})\) is full rank (even for \(\gamma=1\)), so if \(L\) is full rank, \(L(I-\gamma A_{\pi})^{-1}\) is full rank.

### Proof of Lemma 3

**Lemma 7**.: _Suppose we have a simulator \(f_{sim}\) such that \(\forall\mathbf{x},\mathbf{u},\|f_{sim}(\mathbf{x},\mathbf{u})-f(\mathbf{x}, \mathbf{u})\|\leq\ \delta\), then Pseudo-Disturbance (4) is approximately equal to the actual perturbation \(\|\widehat{\mathbf{w}}_{t}-\mathbf{w}_{t}\|\ \leq\ \delta\)._

Proof.: This Lemma is immediate from the definition of the dynamics as \(\mathbf{x}_{t+1}=f(\mathbf{x}_{t},\mathbf{u}_{t})+\mathbf{w}_{t}\). 

## Appendix E Main Result and Dimension-efficient Bandit GPC

Below, we formally state and prove the main result. Subsequent sections attest to the fact that this regret bound is an unconditional improvement over the best known Gradu et al. (2020); Cassel and Koren (2020) in terms of its dependence on dimension and applicability to high-dimensional systems, even for the well-studied setting of linear control.

### Main Result

Using Theorem 9 which we state and prove in subsequent sections, we can prove the main result.

**Theorem 8**.: _Consider a modification of Algorithm 3 implemented using \(\hat{\mathbf{w}}_{t}\) in place of \(\mathbf{w}_{t}\) and choose the step size as \(\eta=\sqrt{\frac{d_{min}}{d_{u}}}T^{-3/4}\), and the exploration radius as \(\delta=\sqrt{d_{u}d_{min}}T^{-1/4}\)._

_If the underlying dynamics are linear and satisfy the assumptions in Section E.2 then as long as the Pseudo-Disturbance signal \(\hat{\mathbf{w}}_{t}\) satisfies \(\hat{\mathbf{w}}_{t}=\mathcal{T}\mathbf{w}_{t}\), for some (possibly unknown) invertible map \(\mathcal{T}\), with \(\mathbf{u}_{t}\) such that for any sequence of bounded (even adversarial) \(\mathbf{w}_{t}\) such that the following holds_

\[\sum_{t}c_{t}(\mathbf{x}_{t},\mathbf{u}_{t})-\inf_{M\in\mathcal{M}}\sum_{t}c_{ t}(\mathbf{x}_{t}^{M},\mathbf{u}_{t}^{M})\ \leq\ \widetilde{\mathcal{O}}(\sqrt{d_{u}d_{min}}\mathrm{poly}(\|\mathcal{T}\|\| \mathcal{T}^{-1}\|)T^{3/4}),\]

_for any any sequence of convex costs \(c_{t}\). Further, if the costs \(c_{t}\) are \(L\)-smooth, the regret can be improved upper bound of_

\[\sum_{t}c_{t}(\mathbf{x}_{t},\mathbf{u}_{t})-\inf_{M\in\mathcal{M}}\sum_{t}c_{ t}(\mathbf{x}_{t}^{M},\mathbf{u}_{t}^{M})\ \leq\ \widetilde{\mathcal{O}}(\mathrm{poly}(\|\mathcal{T}\|\|\mathcal{T}^{-1}\|)(d _{u}d_{min}T)^{2/3})\.\]

Proof.: This follows from that fact that an invertible linear transformation \(\mathcal{T}\) of \(\mathbf{w}_{t}\) does not change the expressiveness of a DAC policy class \(\mathcal{M}\) except through constants related to the norm of \(\mathcal{T}\). More specifically, given a DAC policy \(M_{1:h}\) that acts on true disturbances \(\mathbf{w}_{s}\), the same exact controls are produced by a DAC policy \(M^{\prime}\) with \(M^{\prime}_{i}=M_{i}\mathcal{T}^{-1}\) acting on \(\hat{\mathbf{w}}_{s}=\mathcal{T}\mathbf{w}_{s}\). The disturbances are also scaled by \(\mathcal{T}\). by As such, we can attain equivalent regret bounds with a new policy class \(\mathcal{M}^{\prime}\) with diameter scaled by \(\|\mathcal{T}^{-1}\|\) and new bound on disturbances \(W^{\prime}=\|\mathcal{T}\|W\). In Theorem 9, the hidden dependence on the DAC diameter and disturbance size are polynomial, yielding at most \(\mathrm{poly}(\|\mathcal{T}\|\|\mathcal{T}^{-1}\|)\) scaling in the regret. 

### Dimension-Efficient Bandit GPC

Under bandit feedback, the learner can only observe the cost it incurs, and does not have access to function value oracles or gradients of the cost functions. This setting has been studied in detail in linear control subject to adversarial disturbances using both dynamic and static regret settings; we restrict our attention to the latter.

A key characteristic of our proposed algorithm is that it performs exploration in the action space, rather than the policy space. This enables us to obtain a favorable trade-off between the quality of the proxy of the gradient and the amount of modification the objective (via randomized smoothing) is subject to. Leveraging this property, we show that our approach obtains a better regret bound than the best known (Cassel and Koren, 2020; Gradu et al., 2020) in the literature. In particular, the best known regret bounds for this setting scale as \(O(\sqrt{d_{x}d_{u}d_{min}}T^{3/4})\). In contrast, we offer a regret bound of \(O(\sqrt{d_{u}d_{min}}T^{3/4})\). This is both a quantitative and a qualitative improvement, and carries over to the case of smooth costs too. In particular, since our bound has no dependence on \(d_{x}\) whatsoever, it is equally applicable to the high-dimensional setting (\(d_{x}\gg 1\)), which existingmethodologies fail to scale to in the bandit setting. We stress that this improvement in the upper bound stems from the _right_ algorithm design, and not just a tighter analysis.

In this section, we analyze Algorithm 3, a minimally modified version of Algorithm 1, which uses delayed gradient updates. As a convention, we hold \(\mathbf{w}_{t}=0\) for \(t<0\) when defining DAC policies in early rounds.

```
1:Input: Memory parameter \(h\), learning rate \(\eta\), exploration size \(\delta\), initialization \(M^{1}_{1:h}\in\mathbb{R}^{d_{u}\times d_{x}\times h}\), and convex policy set \(\mathcal{M}\).
2:for\(t=1\ldots T\)do
3: Use action \(\mathbf{u}_{t}=\sum_{i=1}^{h}M^{t}_{i}\mathbf{w}_{t-i}+\delta\mathbf{n}_{t}\), where \(\mathbf{n}_{t}\) is drawn _iid_ from a sphere uniformly, i.e. \[\mathbf{n}_{t}\sim\text{Unif}(\mathbb{S}_{d_{u}}).\]
4: Observe state \(\mathbf{x}_{t+1}\), and cost \(c_{t}=c_{t}(\mathbf{x}_{t},\mathbf{u}_{t})\).
5: Store the stochastic gradient estimate \[\widehat{\nabla}_{t}=\frac{d_{u}c_{t}(\mathbf{x}_{t},\mathbf{u}_{t})}{\delta} \sum_{i=0}^{h-1}\mathbf{n}_{t-i}\otimes\mathbf{w}_{t-i-1:t-i-h}\]
6: Update using delayed gradient with euclidean projection onto \(\mathcal{M}\) \[M^{t+1}=\Pi_{\mathcal{M}}\big{[}M^{t}-\eta\widehat{\nabla}_{t-h}\big{]}\]
7:endfor ```

**Algorithm 3** Bandit GPC

We make the following assumptions pertaining to costs and linear dynamics:

1. The underlying dynamics are assumed to be time-invariant and linear, i.e. \[\mathbf{x}_{t+1}=A\mathbf{x}_{t}+B\mathbf{u}_{t}+\mathbf{w}_{t},\] where \(\mathbf{x}_{t},\mathbf{w}_{t}\in\mathbb{R}^{d_{x}},\mathbf{u}_{t}\in\mathbb{ R}^{d_{u}},\|u_{t}\|\,\leq\,W\) and \(\|B\|\,\leq\,\kappa\).
2. The linear system is \((\kappa,\alpha)\)-strongly stable: \(\exists Q,L\) such that \[A=QLQ^{-1},\] where \(\|Q^{-1}\|,\|Q\|\,\leq\,\kappa,\,\|L\|\,\leq\,1-\alpha\).
3. The time-varying online cost functions \(c_{t}(\mathbf{x}_{t},\mathbf{u}_{t})\) are convex and satisfy for all \(\|\mathbf{x}\|,\|\mathbf{u}\|\,\leq\,D\) that \[c_{t}(\mathbf{x},\mathbf{u})\,\leq\,C\max\{D^{2},1\},\quad\text{ and}\quad\|\nabla_{\mathbf{u}}c(\mathbf{x},\mathbf{u})\|,\|\nabla_{\mathbf{x}}c( \mathbf{x},\mathbf{u})\|\,\leq\,G\max\{D,1\}.\]
4. We define our comparator set \(\mathcal{M}=\mathcal{M}_{1}\times\cdots\times\mathcal{M}_{h}\) where \[\mathcal{M}_{i}=\{M\in\mathbb{R}^{d_{u}\times d_{x}}:\|M\|\,\leq\,2\kappa^{4}( 1-\alpha)^{i}\}\] as in [12]. Let \(d_{min}=\min\{d_{x},d_{u}\}\).

The second assumption may be relaxed to that of stabilizability, the case when the linear system by itself might be unstable, however the learner is provided with a suboptimal linear controller \(K_{0}\) such that \(A+BK_{0}\) is \((\kappa,\alpha)\)-strongly stable, via a blackbox reduction outlined in Proposition 6 (Appendix A) in Cassel et al. (2022).

**Theorem 9**.: _Choosing the step size as \(\eta=\sqrt{\frac{d_{min}}{d_{u}}}T^{-3/4}\), and the exploration radius as \(\delta=\sqrt{d_{u}d_{min}}T^{-1/4}\), the regret of Algorithm 3 is upper bounded as_

\[\text{Regret}(\mathcal{A})=\mathbb{E}\left[\sum_{t=1}^{T}c_{t}(\mathbf{x}_{t},\mathbf{u}_{t})\right]-\inf_{M\in\mathcal{M}}\sum_{t=1}^{T}c_{t}(\mathbf{x} _{t}^{M},\mathbf{u}_{t}^{M})\leq\widetilde{\mathcal{O}}(\sqrt{d_{u}d_{min}}T ^{3/4}).\]

_Furthermore, if the costs \(c_{t}\) is \(L\)-smooth, then choosing \(\delta=(d_{u}d_{min})^{1/3}T^{-1/6},\eta=d_{min}^{1/3}/(d_{u}^{2/3}T^{2/3})\), the regret incurred by the algorithm admits an tighter upper bound of_

\[\text{Regret}(\mathcal{A})=\mathbb{E}\left[\sum_{t=1}^{T}c_{t}(\mathbf{x}_{t},\mathbf{u}_{t})\right]-\inf_{M\in\mathcal{M}}\sum_{t=1}^{T}c_{t}(\mathbf{x} _{t}^{M},\mathbf{u}_{t}^{M})\leq\widetilde{\mathcal{O}}((d_{u}d_{min}T)^{2/3}).\]

### Idealized Cost and Proof of Theorem 9

We will prove our result by creating a proxy loss with memory which accurately estimates the cost, showing that our update provides a low bias gradient estimate with suitably small variance. This will allow us to prove a regret bound on our proxy-losses, which we then translate to a regret bound on the policy itself.

Following (Agarwal et al., 2019; Cassel and Koren, 2020), we introduce a transfer matrix that describes the effect of recent disturbances on the state.

**Definition 10**.: For any \(i<2h\), define the disturbance-state transfer matrix Let

\[\Psi_{i}(M^{1:h})=A^{i}\mathbf{1}_{i\;\leq\;h}+\sum_{j=1}^{h}A^{j}BM_{i-j-1}^{ h-j+1}\mathbf{1}_{i-j-1\in[1,h]}\]

We can also create a transfer matrix for the effect of injected noise in the control on the state:

**Definition 11**.: The noise transfer matrix is defined as \(\Phi_{i}=A^{i}B\).

We have the following representation of the state

\[\mathbf{x}_{t+1}=A^{h+1}\mathbf{x}_{t-h}+\sum_{i=0}^{2h}\Psi_{i}(M^{1:h}) \mathbf{w}_{t-i}+\delta\sum_{i=0}^{h-1}\Phi_{i}\mathbf{n}_{t-i}\] (9)

We are also interested in counterfactual state trajectories using non-stationary DAC policies. In particular, we have

**Definition 12**.: The idealized state using policies \(M^{1:h}\) is defined as

\[y_{t+1}(M^{1:h})=\sum_{i=0}^{2h}\Psi_{i}(M^{1:h})\mathbf{w}_{t-i}\;.\]

Similarly, the idealized cost is defined as

\[C_{t}(M^{1:h})=c_{t}\left(y_{t}(M^{1:h}),\sum_{i=1}^{h}M_{i}^{h}\mathbf{w}_{t- i}\right).\]

The univariate generalization of the idealized state and cost are

\[\tilde{y}_{t}(M)=y_{t}(\underbrace{M,M,\dots M}_{h\text{ times}}),\quad\widetilde{C}_{t}(M)=C_{t}( \underbrace{M,M,\dots M}_{h\text{ times}}).\]

We also define \(F_{t}(\mathbf{u}_{1:h})=c_{t}(\sum_{i=0}^{h-1}A^{i}(B\mathbf{u}_{i}+\mathbf{w }_{t-1-i}),\mathbf{u}_{h})\) representing the instantaneous cost as a function of the last \(h\) controls.

We note that

\[\widetilde{C}_{t}(M)=F_{t}(\sum_{i=0}^{h-1}M_{i}w_{t-i-h},\sum_{i=0}^{h-1}M_{ i}w_{t-i-h+1},\dots\sum_{i=0}^{h-1}M_{i}w_{t-i-1})\]

We now define a smoothed version of \(F_{t}\), \(F_{t,\delta}\) and a smoothed version of \(\tilde{C}_{t}\), \(\tilde{C}_{t,\delta}\) that uses \(F_{t,\delta}\).

\[F_{t,\delta}(\mathbf{u}_{1:h}) =\mathbb{E}_{\mathbf{n}_{1:h}\sim\mathbb{S}_{d_{u}}}[F_{t}( \mathbf{u}_{1:h}+\delta\mathbf{n}_{1:h})]\] \[\widetilde{C}_{t,\delta}(M) =F_{t,\delta}(\sum_{i=0}^{h-1}M_{i}w_{t-i-h},\sum_{i=0}^{h-1}M_{ i}w_{t-i-h+1}\dots\sum_{i=0}^{h-1}M_{i}w_{t-i-1})\]

We also use the following notation for idealized costs fixing a realization of the exploration noise:

\[C_{t}(M|\mathbf{n}_{1:h})=F_{t,\delta}(\sum_{i=0}^{h-1}M_{i}w_{t-i-h}+ \mathbf{n}_{1},\sum_{i=0}^{h-1}M_{i}w_{t-i-h+1}+\mathbf{n}_{2},\dots,\sum_{i=0 }^{h-1}M_{i}w_{t-i-1}+\mathbf{n}_{h})\;.\]Since \(\delta=o(1)\), the contribution to the state space is negligible, we can use bounds from Definition 5 of (Cassel and Koren, 2020). In particular, we will use

\[h=\alpha^{-1}\log(2\kappa^{3}T),\quad D_{x,u}=\max(10\alpha^{-1}\kappa^{4}W(h \kappa+1),1)\] (10)

Proof of Theorem 9.: First, we state a bound on how large the states can get when modifying a DAC policy online.

**Lemma 13**.: _Suppose controls are played according to \(\mathbf{u}_{t}=\sum_{i=1}^{h}M_{i}^{t}\mathbf{w}_{t-i}+\delta\mathbf{n}_{t}\) where \(\mathbf{n}_{t}\sim\mathbb{S}_{d_{u}}\) and \(\delta=o(1)\), then_

1. \(\|\mathbf{x}_{t}\|,\|\mathbf{u}_{t}\|\leq D_{x,u}\) _and_ \(|c_{t}(\mathbf{x}_{t},\mathbf{u}_{t})|\leq CD_{x,u}^{2}\)__
2. _Let_ \(x_{t}^{M},u_{t}^{M}\) _correspond to the counterfactual trajectory, playing DAC with parameter_ \(M\) _for all time, then_ \(|c_{t}(\mathbf{x}_{t}^{M},\mathbf{u}_{t}^{M})-\tilde{C}_{t}(M)|\leq\frac{GD_{ x,u}^{2}}{T}\)__
3. \(|c_{t}(\mathbf{x}_{t},\mathbf{u}_{t})-C_{t}(M^{t-h+1:t}|\mathbf{n}_{t-h+1:t})| \leq\frac{GD_{x,u}^{2}}{T}\)__

The next lemma quantifies both the degree to which an idealized notion of cost tracks the true cost incurred for a DAC policy, and the resultant quality of gradient estimates thus obtained.

**Lemma 14**.: _For all \(t\), \(C_{t,\delta}\) is convex and_

\[\|\nabla\widetilde{C}_{t,\delta}(M^{t})-\mathbb{E}[\widehat{\nabla}_{t}]\| \leq\frac{2\eta d_{u}h^{4}W^{2}\kappa^{3}\widehat{G}GD_{x,u}}{\delta}\,\]

_and for all \(M\in\mathcal{M}\),_

\[|\widetilde{C}_{t,\delta}(M)-\widetilde{C}_{t}(M)|\leq\delta hGD_{x,u}\kappa^ {3}\.\]

We begin by observing that for any \(t\) using Lemma 13.3 and the second part of Lemma 14, we have

\[|c_{t}(\mathbf{x}_{t}^{M},\mathbf{u}_{t}^{M})-\widetilde{C}_{t,\delta}(M)|\ \leq\ \frac{GD_{x,u}^{2}}{T}+\delta hGD_{x,u}\kappa^{3}.\]

A analogous result on the difference between true and idealized costs is stated below, but this time for the online algorithm itself which employs a changing sequence of DAC policies.

**Lemma 15**.: \[|c_{t}(\mathbf{x}_{t},\mathbf{u}_{t})-\widetilde{C}_{t}(M^{t}|\mathbf{n}_{t-h +1:t})|\leq\frac{GD_{x,u}^{2}}{T}+\eta GD_{x,u}W\kappa^{3}h^{2}\widehat{G}\]

Similarly, we have using Lemma 15 for any \(t\) that

\[|c_{t}(\mathbf{x}_{t},\mathbf{u}_{t})-\widetilde{C}_{t,\delta}(M^{t})|\ \leq\ \frac{GD_{x,u}^{2}}{T}+\eta GD_{x,u}W\kappa^{3}h^{2}\widehat{G}+\delta hGD_{x,u}\kappa^{3}.\]

Using this display, we decompose the regret of the algorithm as stated below.

\[\mathbb{E}\left[\sum_{t=1}^{T}c_{t}(\mathbf{x}_{t},\mathbf{u}_{t })\right]-\inf_{M\in\mathcal{M}}\sum_{t=1}^{T}c_{t}(\mathbf{x}_{t}^{M},\mathbf{ u}_{t}^{M})\] \[\leq \sum_{t=1}^{T}\widetilde{C}_{t,\delta}(M^{t})-\inf_{M\in\mathcal{ M}}\sum_{t=1}^{T}\widetilde{C}_{t,\delta}(M)+2GD_{x,u}^{2}+\eta GD_{x,u}W \kappa^{3}h^{2}\widehat{G}T+2\delta hGD_{x,u}\kappa^{3}T\]

Next, we use the following regret bound on an abstract implementation of online gradient descent with delayed updates, which we specialize subsequently to our setting.

**Lemma 16**.: _Consider a delayed gradient update in Online Gradient Descent, executed as_

\[M^{t+1}=\Pi_{\mathcal{M}}\left[M^{t}-\eta\widehat{\nabla}_{t-h}\right]\]

_where \(\|\mathbb{E}\nabla_{t}-\nabla\widetilde{C}_{t,\delta}(M^{t})\|\leq\varepsilon\), \(\|\widehat{\nabla}_{t}\|\leq\widehat{G}\), \(D_{\mathcal{M}}=\max_{M\in\mathcal{M}}\|M\|\). Additionally, if \(\max_{M\in\mathcal{M}}\|\nabla\widetilde{C}_{t,\delta}(M)\|\leq\,G_{ \mathcal{M}}\), then we have for any \(\eta>0\) that_

\[\mathbb{E}\left[\sum_{t=1}^{T}\widetilde{C}_{t,\delta}(M^{t})-\sum_{t=1}^{T} \widetilde{C}_{t,\delta}(M^{*})\right]\ \leq\ 2\varepsilon D_{\mathcal{M}}T\sqrt{hd_{min}}+2\eta h^{2}d_{\min}\widehat{G} ^{2}T+\frac{2hd_{min}D_{\mathcal{M}}^{2}}{\eta}\]

Now, we invoke the regret upper bound from Lemma 16 to arrive at

\[\mathbb{E}\left[\sum_{t=1}^{T}c_{t}(\mathbf{x}_{t},\mathbf{u}_{t })\right]-\inf_{M\in\mathcal{M}}\sum_{t=1}^{T}c_{t}(\mathbf{x}_{t}^{M}, \mathbf{u}_{t}^{M})\] \[\leq\ 2\varepsilon D_{\mathcal{M}}T\sqrt{hd_{min}}+2\eta h^{2}d_{ \min}\widehat{G}^{2}T+\frac{2hd_{min}D_{\mathcal{M}}^{2}}{\eta}\] \[\ \ \ \ +2GD_{x,u}^{2}+\eta GD_{x,u}W\kappa^{3}h^{2}\widehat{G}T+ 2\delta hGD_{x,u}\kappa^{3}T\]

Finally, we plug the value of \(\widehat{G}\) from Lemma 17, and \(\varepsilon\) from the first part of Lemma 14.

**Lemma 17**.: _The stochastic gradients produced by Algorithm 3 satisfy the following bound_

\[\|\widehat{\nabla}_{t}\|\ \leq\ \widehat{G}:=\frac{d_{u}h^{2}WGD_{x,u}^{2}}{\delta}\]

As evident from the definition of \(\mathcal{M}\), \(D_{\mathcal{M}}=2\sqrt{h}\kappa^{4}\). Setting \(\eta=\sqrt{d_{min}/d_{u}}T^{-3/4},\delta=\sqrt{d_{u}d_{min}}T^{-1/4}\) yields the result of \(O(T^{3/4})\) regret for (possibly) non-smooth costs.

For the second part of the claim, we show an improved analogue of the second part of Lemma 14.

**Lemma 18**.: _As long as \(c_{t}\) is \(L\)-smooth, for all \(M\in\mathcal{M}\), \(|\widetilde{C}_{t,\delta}(M)-\widetilde{C}_{t}(M)|\leq 25L\kappa^{8}W^{2}h^{2} \delta^{2}/\alpha\)._

Using this, in a manner similar to the derivation for non-smooth costs, we arrive at

\[\mathbb{E}\left[\sum_{t=1}^{T}c_{t}(\mathbf{x}_{t},\mathbf{u}_{t })\right]-\inf_{M\in\mathcal{M}}\sum_{t=1}^{T}c_{t}(\mathbf{x}_{t}^{M}, \mathbf{u}_{t}^{M})\] \[\leq\ 2\varepsilon D_{\mathcal{M}}T\sqrt{hd_{min}}+2\eta h^{2}d_{ \min}\widehat{G}^{2}T+\frac{2hd_{min}D_{\mathcal{M}}^{2}}{\eta}\] \[\ \ \ \ +2GD_{x,u}^{2}+\eta GD_{x,u}W\kappa^{3}h^{2}\widehat{G}T +50L\kappa^{8}W^{2}h^{2}\delta^{2}T/\alpha\]

In this case, we set \(\delta=(d_{u}d_{min})^{1/3}T^{-1/6},\eta=d_{min}^{1/3}/(d_{u}^{2/3}T^{2/3})\) to arrive at the final bound as stated in the claim. 

### Proof of Supporting Claims

Proof of Lemma 13.: The properties follow from Lemma 6 in [Cassel and Koren, 2020], while using the fact that \(\delta=o(1)\). 

Proof of Lemma 14.: Using the chain rule, we note that

\[\nabla\widetilde{C}_{t}(M)=\sum_{i=1}^{h}\Big{(}\left.\nabla_{\mathbf{u}_{i}}F_ {t}(\mathbf{u}_{1:h})\right|_{\mathbf{u}_{h}=\sum_{j=0}^{h-1}M_{j}\mathbf{w}_ {t-h+h-j-1}\forall k}\Big{)}\otimes\mathbf{w}_{t-h+i-1:t-2h+i}\]

Now, we note that the smoothed function \(F_{t,\delta}\) will satisfy

\[|F_{t,\delta}(\mathbf{u}_{1:h})-F_{t}(\mathbf{u}_{1:h})|\leq\delta hG_{F}\,\quad| \widetilde{C}_{t,\delta}(M)-\widetilde{C}_{t}(M)|\leq\delta hG_{F}\]where \(G_{F}\) is the Lipschitz constant of \(F_{t}\) with respect to a single \(u\). This follows, by a hybrid-like argument smoothing one argument at a time using standard smoothing results (see e.g. [14] Fact 3.2).We note that \(G_{F}\) can be bound by \(GD_{x,u}\kappa^{3}\). Furthermore, this smoothing preserves convexity of \(F_{t,\delta}\) and composition of a linear and convex function is convex, so \(\widetilde{C}_{t,\delta}\) also remains convex.

The gradients of the smoothed function then has the following form due to Lemma 6.7 from Hazan et al. [2016].

\[\nabla_{\mathbf{u}_{i}}F_{t,\delta}(\mathbf{u}_{1:h}) =\mathbb{E}_{\mathbf{n}_{1:h}\sim\mathbb{S}_{d_{u}}}\big{[}\frac{ d_{u}}{\delta}F_{t}(\mathbf{u}_{1:h}+\delta\mathbf{n}_{1:h})\mathbf{n}_{i} \big{]}\] \[\nabla\widetilde{C}_{t,\delta}(M) =\mathbb{E}_{\mathbf{n}_{1:h}\sim\mathbb{S}_{d_{u}}}\left[\frac{ d_{u}}{\delta}\sum_{i=1}^{h}\left(\underbrace{F_{t}(\mathbf{u}_{1:h}+\delta \mathbf{n}_{1:h})|_{\mathbf{u}_{k}=\sum_{j=0}^{h-1}M_{j}\mathbf{w}_{t-h+h-j-1} \forall k}}_{C_{t}(M|\mathbf{n}_{1:h})}\right)\mathbf{n}_{i}\otimes\mathbf{w} _{t-h+i-1:t-2h+i}\right]\,.\]

Rearranging, we have

\[\nabla\widetilde{C}_{t,\delta}(M^{t}) =\mathbb{E}_{\mathbf{n}_{t-h+1:t}\sim\mathbb{S}_{d_{u}}}\left[ \frac{d_{u}C_{t}(M^{t}|\mathbf{n}_{t-h+1:t})}{\delta}\sum_{j=0}^{h-1}\mathbf{ n}_{t-i}\otimes\mathbf{w}_{t-i-1:t-h-i}\right]\]

Now, to relate this to \(\mathbb{E}_{\mathbf{n}_{1:h}\sim\mathbb{S}_{d_{u}}}[\widehat{\nabla}_{t}]\), we note in expression for \(\nabla\widetilde{C}_{t,\delta}(M^{t})\), we bound \(c_{t}(\mathbf{x}_{t},\mathbf{u}_{t}|\mathbf{n}_{t-h+1:t})-C_{t}(M^{t}| \mathbf{n}_{t-h+1:t})\) via Lemma 15. Using bounds on \(\mathbf{w},\mathbf{n}\) along with this bound, we have

\[\|\nabla\widetilde{C}_{t,\delta}(M^{t})-\mathbb{E}[\widehat{\nabla}_{t}]\| \,\leq\,\frac{d_{u}h^{2}W}{\delta}\left(\frac{GD_{x,u}^{2}}{T}+\eta GD_{x,u}W \kappa^{3}h^{2}\widehat{G}\right)\leq\frac{2\eta d_{u}h^{4}W^{2}\kappa^{3} \widehat{G}GD_{x,u}}{\delta}\]

Proof of Lemma 15.: We start with triangle inequality

\[|c_{t}(\mathbf{x}_{t},\mathbf{u}_{t})-\widetilde{C}_{t}(M^{t})| \leq|c_{t}(\mathbf{x}_{t},\mathbf{u}_{t})-\widetilde{C}_{t}(M^{t-h:t}| \mathbf{n}_{t-h+1:t})|+|\widetilde{C}_{t}(M^{t-h:t}|\mathbf{n}_{t-h+1:t})- \widetilde{C}_{t}(M^{t}|\mathbf{n}_{t-h+1:t})|\]

The first term is handled via Lemma 13, so we only need to bound the second term.

\[|\widetilde{C}_{t}(M^{t-h:t})-\widetilde{C}_{t}(M^{t})| =|c_{t}(y_{t}(M^{t-h:t}),\sum_{i=1}^{h}M_{i}^{t}\mathbf{w}_{t-i} )-c_{t}(\tilde{y}_{t}(M^{t}),\sum_{i=1}^{h}M_{i}^{t}\mathbf{w}_{t-i})|\] \[\leq GD_{x,u}\|y_{t}(M^{t-h:t})-\tilde{y}_{t}(M^{t})\|\] \[=GD_{x,u}\|\sum_{i=1}^{2h}\Psi_{i}(M^{t-h:t})\mathbf{w}_{t-i}-\sum_ {i=1}^{2h}\Psi_{i}(M^{t}\ldots M^{t})\mathbf{w}_{t-i}\|\] \[=GD_{x,u}\|\sum_{i=1}^{2h}\Psi_{i}(M^{t-h:t}-(M^{t}\ldots M^{t})) \mathbf{w}_{t-i}\|\] \[\leq GD_{x,u}\|\sum_{i=1}^{2h}\Psi_{i}(M^{t-h:t}-(M^{t}\ldots M^{t }))\mathbf{w}_{t-i}\|\]Now we note that each matrix \(M_{i}^{s}\), only occurs in one term of the form \(A^{k}BM_{i}^{s}\mathbf{w}_{l}\), so we can refine the bound above to

\[|\widetilde{C}_{t}(M^{t-h:t})-\widetilde{C}_{t}(M^{t})| \leq GD_{x,u}W\kappa^{3}(1-\alpha)\sum_{i=1}^{h}\|M^{t-i}-M^{t}\|\] \[\leq GD_{x,u}W\kappa^{3}(1-\alpha)\sum_{i=1}^{h}\sum_{s=t-i}^{t} \|\eta\widehat{\nabla}_{s-h}\|\] \[\leq\eta GD_{x,u}W\kappa^{3}h^{2}\widehat{G}\.\]

Combining, we have

\[|c_{t}(\mathbf{x}_{t},\mathbf{u}_{t})-\widetilde{C}_{t}(M^{t})|\leq\frac{GD_{ x,u}^{2}}{T}+\eta GD_{x,u}W\kappa^{3}h^{2}\widehat{G}\]

Proof of Lemma 16.: Since \(c_{t}\) is convex, so is \(\widetilde{C}_{t,\delta}\). Using this fact and the observation that \(M^{t}\) is independent of \(\mathbf{n}_{t:t-h}\) used to construct \(\widehat{\nabla}_{t}\) due to the delayed update of gradients, we have

\[\widetilde{C}_{t,\delta}(M^{t})-\widetilde{C}_{t,\delta}(M^{*})\] \[\leq \langle\nabla\widehat{C}_{t,\delta}(M^{t}),M^{t}-M^{*}\rangle\] \[\leq \mathbb{E}(\widehat{\nabla}_{t},M^{t}-M^{*})+2\varepsilon D_{ \mathcal{M}}\sqrt{hd_{min}}\] \[\leq \mathbb{E}(\widehat{\nabla}_{t},M^{t+h}-M^{*})+\|\widehat{\nabla }_{t}\|_{F}\|M^{t+h}-M^{t}\|_{F}+2\varepsilon D_{\mathcal{M}}\sqrt{hd_{min}}\] \[\leq \mathbb{E}(\widehat{\nabla}_{t},M^{t+h}-M^{*})+\eta h^{2} \widehat{G}^{2}d_{min}+2\varepsilon D_{\mathcal{M}}\sqrt{hd_{min}}\]

The gradient update can be rewritten as

\[\langle\widehat{\nabla}_{t},M^{t+h}-M^{*}\rangle \leq \frac{\|M^{t+h}-M^{*}\|_{F}^{2}-\|M^{t+h}-\eta\widehat{\nabla}_{ t}-M^{*}\|_{F}^{2}}{2\eta}+\frac{\eta\widehat{G}^{2}hd_{min}}{2}\] \[\leq \frac{\|M^{t+h}-M^{*}\|_{F}^{2}-\|M^{t+h+1}-M^{*}\|_{F}^{2}}{2 \eta}+\frac{\eta\widehat{G}^{2}hd_{min}}{2},\]

where we use the fact that the projection operator is non-expansive, hence \(M^{t+h+1}\) is closer in Euclidean distance to \(M^{*}\) than \(M^{t+h}-\eta\widehat{\nabla}_{t}\). Telescoping this, we have for any \(M^{*}\) that

\[\mathbb{E}\left[\sum_{t=1}^{T}\widetilde{C}_{t,\delta}(M^{t})-\sum_{t=1}^{T} \widetilde{C}_{t,\delta}(M^{*})\right]\ \leq\ 2\varepsilon D_{\mathcal{M}}T\sqrt{hd_{min}}+2\eta h^{2}d_{\min} \widehat{G}^{2}T+\frac{2hd_{min}D_{\mathcal{M}}^{2}}{\eta}\]

Proof of Lemma 17.: Plugging in line 6 from Algorithm 3 and using our bounds on the cost, we have

\[\|\widehat{\nabla}_{t}\|\ \leq\ \frac{d_{u}GD_{x,u}^{2}}{\delta}\sum_{j=0}^{h-1} \|\mathbf{n}_{t-i}\|\|\mathbf{w}_{t-i-1:t-h-i}\|\leq\frac{d_{u}h^{2}WGD_{x,u}^ {2}}{\delta}\.\]

Proof of Lemma 18.: We first make note of the following characterization of idealized costs under smoothness due to Cassel and Koren (2020) (Lemma 7.2, therein).

**Lemma 19** (Cassel and Koren (2020)).: _If \(c_{t}\) is \(L\)-smooth, then the smoothed and non-smoothed variants of the idealized costs \(\widetilde{C}_{t},F_{t},\widetilde{C}_{t,\delta},F_{t,\delta}\) are \(L^{\prime}\)-smooth, where \(L^{\prime}=25L\kappa^{8}W^{2}h/\alpha\)._Note that \(\widetilde{C}_{t},\widetilde{C}_{t,\delta}\) only differ in that the latter is a noise-smoothed version of the former. Let \(\mathbf{u}_{1:h}=\left[\sum_{i=0}^{h-1}M_{i}\mathbf{w}_{t-i-h},\sum_{i=0}^{h-1} M_{i}\mathbf{w}_{t-i-h+1}\cdots\sum_{i=0}^{h-1}M_{i}\mathbf{w}_{t-i-1}\right]\). Using the fact the noise \(\mathbf{n}_{1:h}\) is zero-mean and independent of \(\mathbf{u}_{1:h}\), we create a second-order expansion using Taylor's theorem to conclude

\[|\widetilde{C}_{t,\delta}(M)-\widetilde{C}_{t}(M)|\] \[= |\mathbb{E}_{\mathbf{n}_{1:h}\sim\mathbb{S}_{d_{u}}}F_{t}( \mathbf{u}_{1:h}+\delta\mathbf{n}_{1:h})-F_{t}(\mathbf{u}_{1:h})|\] \[\leq \big{|}\underbrace{\mathbb{E}_{\mathbf{n}_{1:h}\sim\mathbb{S}_{d _{u}}}\langle\nabla F_{t}(\mathbf{u}_{1:h}),\delta\mathbf{n}_{1:h}\rangle}_{=0 }|+\frac{L^{\prime}}{2}\|\delta\mathbf{n}_{1:h}\|_{F}^{2}\] \[\leq L^{\prime}\delta^{2}h.\]