# Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision Language Models

 Hyunsik Chae\({}^{\dagger}\), Seungwoo Yoon\({}^{\dagger}\), Chloe Yewon Chun\({}^{\dagger}\),

Gyehun Go\({}^{\dagger}\), Yongin Cho\({}^{\dagger}\), Gyeongmin Lee\({}^{\dagger}\), Ernest K. Ryu\({}^{\star}\)

\({}^{\dagger}\)Seoul National University, \({}^{\star}\)UCLA, Department of Mathematics

https://github.com/Atomic-Visual-Skills/AVS

###### Abstract

Recent Vision Language Models (VLMs) have demonstrated impressive multimodal comprehension and reasoning capabilities, but they often struggle with trivially simple visual tasks. In this work, we introduce the Atomic Visual Skills Benchmark (AVSBench) to evaluate whether VLMs possess capabilities to understand basic geometric features, which we refer to as atomic visual skills. Specifically, we systematically categorize the atomic visual skills and handcraft a set of 5,073 diverse questions designed to assess each individual atomic visual skill. Using AVSBench, we evaluate the current leading VLMs and find that they struggle with most of these atomic visual skills that are obvious to humans.

## 1 Introduction

Recent Vision Language Models (VLMs), also referred to more generally as Multimodal Large Language Models (MLLM), integrate vision components into language models and demonstrate an impressive breadth of multimodal comprehension and reasoning capabilities [7]. At the same time, however, VLMs often struggle with trivially easy visual tasks as shown in Figure 1, a puzzling phenomenon that seems almost contradictory to their remarkable performance [11, 43]. We propose two hypotheses to explain the observed shortcomings of current vision-language models:

_Hypothesis 1: The comprehension of complex visual diagrams requires the composition of smaller atomic visual skills._

_Hypothesis 2: Current vision language models are incapable of such atomic visual skills._

In this work, we introduce the Atomic Visual Skills Benchmark (AVSBench) to test _Hypothesis 2_. AVSBench is designed to rigorously evaluate VLMs' ability to comprehend fundamental geometric features, which we refer to as _atomic visual skills_. We systematically categorize 36 atomic visual skills that encompass diagrams arising in high school-level geometry and _handcraft_ a set of 5,073 diverse questions designed to assess the understanding of the individual atomic visual skills.

We then evaluate the state-of-the-art VLMs on AVSBench, and the results clearly support _Hypothesis 2_. While our problems are designed to be trivial to humans, VLMs struggle; state-of-the-art models like Gemini-1.5-pro and GPT-4o score around 70%-75% on problems with the "easy" categorization, score around 60% on the "medium" problems, and 30% on "hard" problems. The confirmation of _Hypothesis 2_ also lends support to _Hypothesis 1_, and suggests a promising direction of future work of training vision language models specifically on atomic visual skills to improve their performance in comprehending complex visual diagrams.

## 2 Atomic Visual Skills Benchmark (AVSBench)

Many visual reasoning tasks in existing benchmarks, such as the ones listed in Section A, are composite tasks that can be broken down into more elementary components. This observation leads us to define a set of _atomic visual skills_ based on the following criteria: (i) each skill is intuitive and trivial for adult humans, (ii) each skill cannot be decomposed further, or doing so would be unnatural, and (iii) the list of atomic visual skills should comprehensively cover the abilities required for comprehending geometric diagrams arising in high-school level mathematics. While this definition is not a fully rigorous one, we found it to be sufficiently clear and substantive for our work.

Using these criteria, we identified 36 atomic visual skills, including the ability to understand concepts such as angle, boundary, orthogonality, curvature, and direction. The complete list and further illustrations are provided in D.

For adult humans, these skills are trivially simple and require little to no reasoning to perform. Therefore, we use the term _comprehension_ instead of _reasoning_ to emphasize our belief that these skills do not require much reasoning or thinking to perform, for both humans and VLMs. This belief is partially supported by Findings 3 of Section 3.1.

We then constructed the Atomic Visual Skills Benchmark (AVSBench) to evaluate VLMs' ability to perform the 36 atomic visual skills. AVSBench, as summarized in Figure 2, comprises 5,073 new handcrafted image-question-answer triplets with the following characteristics:

Figure 1: Examples of AVSBench problems and responses by GPT-4o. Other state-of-the-art models exhibit similar failures. These examples demonstrate a deficiency in the VLMs’ understanding of basic geometric concepts.

Figure 2: List of 36 atomic visual skills and the number of easy, medium, and hard problems for each skill. The difficulty is judged by the authors. We provide a total of 5,073 new handcrafted problems.

* **Originality.** All images and questions are newly generated, ensuring that they are free from data contamination concerns.
* **Diversity.** Although we focus on the set of only 36 skills, the problems feature diverse expressions and formats, as illustrated by the sample problems in C.
* **Skill isolation.** Each question targets a specific atomic skill, minimizing the overlap with other skills. Recognizing the impossibility of achieving complete isolation, our method incorporates diverse tasks to mitigate the influence of any task or their relevant overlapping skills. For instance, to minimize the influence of other skills while evaluating the cardinal skill, we asked about cardinals of various concepts and objects, from colors to points, lines, and other figures.
* **Focus on high school geometry.** We focus on the visual skills required to solve high school-level geometry problems for the following reasons: (i) the scope of the high school mathematics curriculum is more or less clearly defined, (ii) (as our results of Section 3 show) these atomic visual skills are sufficiently challenging for current VLMs, (iii) the range of skills is broad enough to be applicable to other visual comprehension tasks, such as interpreting charts, tables, and scientific or mathematical figures.

## 3 Current vision language models struggle with atomic visual skills

We evaluate three types of VLMs on AVSBench: (i) state-of-the-art proprietary models: GPT-4o [40; 39] and Gemini-1.5-pro [49], (ii) popular mid-sized open-weight models: LLaVA-Next (7B, 13B) [30], LLaVA-OneVision (7B) [26], Phi-3.5-Vision (4B) [1], InternVL2 (8B) [10], Deepseek-VL (7B) [31], and (iii) VLMs specifically trained for geometry or other visual data: Math-LLaVA (13B) [47], Table-LLaVA (7B) [60]. Further details of model versions are provided in Section E.

The evaluation protocol consists of three steps. First, we provide the VLM with the image-question pair and solicit a response. As we further discuss later, we also explore the effect of the chain-of-thought (CoT) prompting [54; 22]. Second, we extract the answer from the VLM's response using GPT-4o mini [40]. Third, we ask GPT-4o mini to score the answer by comparing the extracted answer with the answer key. We award 1 point for a correct answer and 0 points otherwise without any partial credit. More details on our evaluation protocol are provided in F.

### Experimental results and findings

Figure 3 presents the results comparing the selected VLMs and the baseline corresponding to random guessing. Details including exact values are provided in G. On "easy" problems, models with about 10B parameters score between 32.5% and 51.0% while closed-source models, including GPT-4o and Gemini-1.5-pro, achieve over 70%, far above random chance (22.4%). On "medium" problems, models with about 10B parameters score between 23.8% and 37.8%, slightly above random chance

Figure 3: Evaluation results on AVSBench. +_CoT_ implies the performance of the model on the right with chain-of-thought (CoT) prompting [22]. The area ratios of each colored section are aligned with the actual ratio of problem counts. Details about the models including their full name are in E. Full quantitative results are illustrated in Table 5.

(19.1%). Closed-source models achieve between 58.6% and 64.6%. For "hard" problems, most open models score close to random chance (11.7%). The closed-source models GPT-4o (32.3%) and Gemini-1.5-pro (26.9%) scored significantly better than random chance but clearly struggled.

Findings 1: Models share strengths and weaknesses.Figure 4 presents the accuracies of selected models on each skill. The performances across skills varied significantly. For example, most VLMs performed well on OCR, Absolute Position, and Shapes, but performed poorly on tangency, parallel, and angle. Interestingly, the different models largely shared the same set of skills they did well on and the same set they found challenging.

Findings 2: Domain-specific models are not better.Surprisingly, Math-LLaVA [47] and Table-LLaVA [60], which are VLMs specifically trained for geometry or visual data, did not perform better than general VLMs of similar size, on almost any skills within AVSBench as the results of Table 5 show.

Findings 3: Chain-of-thought is not helpful in enhancing atomic visual skills.We also evaluated the best-performing models--GPT-4o and Gemini-1.5-pro--with chain-of-thought (CoT) prompting [22] on AVSBench. Surprisingly, CoT did not help for most skills, and for some skills, it even worsened the performance, as shown in Table 5. This contrasts with prior work, which found CoT to be beneficial for certain visual reasoning tasks [32, 53]. We attribute this difference to our hypothesis that the atomic visual skills of AVSBench require simple "comprehension" and, therefore, do not benefit from the additional "reasoning" steps afforded by CoT prompting. More concrete comparison, see G.

## 4 Conclusion

We present the Atomic Visual Skills Benchmark (AVSBench), a benchmark designed to rigorously evaluate VLMs' ability to perform atomic visual skills in a decomposed manner. We then show that current state-of-the-art VLMs struggle with such atomic visual skills.

The failure of VLMs to carry out such simple atomic visual tasks raises the question: How is it that VLMs are sometimes successful at performing complex visual tasks? For this, we hypothesize that the existing impressive performance on complex tasks is due to overfitting or unimodal shortcuts. Indeed, recent studies such as [11, 18, 57, 50] report that VLMs tend to depend on language shortcuts, as we further reference and discuss in Section A of the appendix.

Recall that our _Hypothesis 1_ posits that the atomic visual skills are necessary subcomponents for comprehending complex visual diagrams. In future work, we plan to train and fine-tune VLMs directly on the atomic visual skills and ascertain _Hypothesis 1_.

Figure 4: Accuracies of a leading model, 3 outstanding models, and random chance on each skill. The skills are ordered in descending order of accuracy, averaged over all models.

## References

* [1] M. Abdin, J. Aneja, H. Awadalla, A. Awadallah, A. A. Awan, N. Bach, A. Bahree, A. Bakhtiari, J. Bao, H. Behl, A. Benhaim, M. Bilenko, J. Bjorck, S. Bubeck, M. Cai, Q. Cai, V. Chaudhary, D. Chen, D. Chen, W. Chen, Y. Chen, Y. Chen, H. Cheng, P. Chopra, X. Dai, M. Dixon, R. Eldan, V. Fragoso, J. Gao, M. Gao, M. Gao, A. Garg, A. Del Giorno, A. Goswami, S. Gunasekar, E. Haider, J. Hao, R. J. Hewett, W. Hu, J. Huynh, D. Iter, S. A. Jacobs, M. Javaheripi, X. Jin, N. Karampatziakis, P. Kauffmann, M. Khademi, D. Kim, Y. J. Kim, L. Kurilenko, J. R. Lee, Y. T. Lee, Y. Li, Y. Li, C. Liang, L. Liden, X. Lin, Z. Lin, C. Liu, L. Liu, M. Liu, W. Liu, X. Liu, C. Liu, P. Madan, A. Mahmoudzadeh, D. Majercak, M. Mazzola, C. C. T. Mendes, A. Mitra, H. Modi, A. Nguyen, B. Norick, B. Patra, D. Perez-Becker, T. Portet, R. Pryzant, H. Qin, M. Radimilac, L. Ren, G. de Rosa, C. Rosset, S. Roy, O. Ruwase, O. Saarlikiv, A. Saied, A. Salim, M. Santacroce, S. Shah, N. Shang, H. Sharma, Y. Shen, S. Shukla, X. Song, M. Tanaka, A. Tuipini, P. Vaddamun, C. Wang, G. Wang, L. Wang, S. Wang, X. Wang, Y. Wang, R. Ward, W. Wen, P. Witte, H. Wu, X. Wu, M. Wyatt, B. Xiao, C. Xu, J. Xu, W. Xu, J. Xue, S. Yadav, F. Yang, J. Yang, Y. Yang, Z. Yang, D. Yu, L. Yuan, C. Zhang, C. Zhang, J. Zhang, L. L. Zhang, Y. Zhang, Y. Zhang, Y. Zhang, and X. Zhou. Phi-3 technical report: A highly capable language model locally on your phone. _arXiv:2404.14219_, 2024.
* [2] Z. Allen-Zhu and Y. Li. Physics of language models: Part 3.2, knowledge manipulation. _arXiv:2309.14402_, 2023.
* [3] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh. VQA: Visual question answering. _International Conference on Computer Vision_, 2015.
* [4] S. Arora and A. Goyal. A theory for emergence of complex skills in language models. _arXiv:2307.15936_, 2023.
* [5] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, and C. Sutton. Program synthesis with large language models. _arXiv:2108.07732_, 2021.
* [6] L. Berglund, M. Tong, M. Kaufmann, M. Balesni, A. C. Stickland, T. Korbak, and O. Evans. The reversal curse: LLMs trained on "A is B" fail to learn "B is A". _International Conference on Learning Representations_, 2024.
* [7] F. Bordes, R. Y. Pang, A. Ajay, A. C. Li, A. Bardes, S. Petryk, O. Manas, Z. Lin, A. Mahmoud, B. Jayaraman, M. Ibrahim, M. Hall, Y. Xiong, J. Lebensold, C. Ross, S. Jayakumar, C. Guo, D. Bouchacourt, H. Al-Tahan, K. Padthe, V. Sharma, H. Xu, X. E. Tan, M. Richards, S. Lavoie, P. Astolfi, R. A. Hemmat, J. Chen, K. Tirumala, R. Assouel, M. Moayeri, A. Talattof, K. Chaudhuri, Z. Liu, X. Chen, Q. Garrido, K. Ullrich, A. Agrawal, K. Saenko, A. Celikyilmaz, and V. Chandra. An introduction to vision-language modeling. _arXiv:2405.17247_, 2024.
* [8] J. Cao and J. Xiao. An augmented benchmark dataset for geometric question answering through dual parallel text encoding. _International Conference on Computational Linguistics_, 2022.
* [9] J. Chen, T. Li, J. Qin, P. Lu, L. Lin, C. Chen, and X. Liang. UniGeo: Unifying geometry logical reasoning via reformulating mathematical expression. _Computer Vision and Pattern Recognition_, 2022.
* [10] Z. Chen, W. Wang, H. Tian, S. Ye, Z. Gao, E. Cui, W. Tong, K. Hu, J. Luo, Z. Ma, et al. How far are we to GPT-4v? closing the gap to commercial multimodal models with open-source suites. _arXiv:2404.16821_, 2024.
* [11] X. Fu, Y. Hu, B. Li, Y. Feng, H. Wang, X. Lin, D. Roth, N. A. Smith, W.-C. Ma, and R. Krishna. BLINK: Multimodal large language models can see but not perceive. _arXiv:2404.12390_, 2024.
* [12] O. Golovneva, Z. Allen-Zhu, J. Weston, and S. Sukhbaatar. Reverse training to nurse the reversal curse. _arXiv:2403.13799_, 2024.
* [13] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. _Computer Vision and Pattern Recognition_, 2017.

- the rise of code intelligence. _arXiv:2401.14196_, 2024.
* [15] D. Gurari, Q. Li, A. J. Stangl, A. Guo, C. Lin, K. Grauman, J. Luo, and J. P. Bigham. VizWiz grand challenge: Answering visual questions from blind people. _Computer Vision and Pattern Recognition_, 2018.
* [16] M. Hanna, O. Liu, and A. Variengien. How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. _arXiv:2305.00586_, 2023.
* [17] T. He, D. Doshi, A. Das, and A. Gromov. Learning to Grok: Emergence of in-context learning and skill composition in modular arithmetic tasks. _arXiv:2406.02550_, 2024.
* [18] C.-Y. Hsieh, J. Zhang, Z. Ma, A. Kembhavi, and R. Krishna. SugarCrepe: Fixing hackable benchmarks for vision-language compositionality. _arXiv:2306.14610_, 2023.
* [19] K. Kafle, B. Price, S. Cohen, and C. Kanan. DVQA: Understanding data visualizations via question answering. _Computer Vision and Pattern Recognition_, 2018.
* [20] M. Kazemi, H. Alvari, A. Anand, J. Wu, X. Chen, and R. Soricut. GeomVerse: A systematic evaluation of large models for geometric reasoning. _arXiv:2312.12241_, 2023.
* [21] A. Kembhavi, M. Salvato, E. Kolve, M. Seo, H. Hajishirzi, and A. Farhadi. A diagram is worth a dozen images. _arXiv:1603.07396_, 2016.
* [22] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot reasoners. _arXiv:2205.11916_, 2022.
* [23] B. Lake and M. Baroni. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. _International Conference on Machine Learning_, 2018.
* [24] N. Lee, K. Sreenivasan, J. D. Lee, K. Lee, and D. Papailiopoulos. Teaching arithmetic to small transformers. _International Conference on Learning Representations_, 2024.
* [25] M. Lewis, N. V. Nayak, P. Yu, Q. Yu, J. Merullo, S. H. Bach, and E. Pavlick. Does CLIP bind concepts? probing compositionality in large image models. _arXiv:2212.10537_, 2022.
* [26] B. Li, Y. Zhang, D. Guo, R. Zhang, F. Li, H. Zhang, K. Zhang, Y. Li, Z. Liu, and C. Li. LLAVA-OneVision: Easy visual task transfer. _arXiv:2408.03326_, 2024.
* [27] Z. Lin, X. Chen, D. Pathak, P. Zhang, and D. Ramanan. Revisiting the role of language priors in vision-language models. _arXiv:2306.01879_, 2024.
* [28] Z. Lin and K. Lee. Dual operating modes of in-context learning. _arXiv:2402.18819_, 2024.
* [29] H. Liu, C. Li, Y. Li, B. Li, Y. Zhang, S. Shen, and Y. J. Lee. LLAVA-NeXT: Improved reasoning, OCR, and world knowledge, 2024.
* [30] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. _Neural Information Processing Systems_, 2023.
* [31] H. Lu, W. Liu, B. Zhang, B. Wang, K. Dong, B. Liu, J. Sun, T. Ren, Z. Li, H. Yang, Y. Sun, C. Deng, H. Xu, Z. Xie, and C. Ruan. DeepSeek-VL: Towards real-world vision-language understanding. _arXiv:2403.05525_, 2024.
* [32] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao. MathVista: Evaluating mathematical reasoning of foundation models in visual contexts. _International Conference on Learning Representations_, 2024.
* [33] Z. Ma, J. Hong, M. O. Gul, M. Gandhi, I. Gao, and R. Krishna. CREPE: Can vision-language foundation models reason compositionally? _Computer Vision and Pattern Recognition_, 2023.

* [34] A. Masry, X. L. Do, J. Q. Tan, S. Joty, and E. Hoque. ChartQA: A benchmark for question answering about charts with visual and logical reasoning. _Findings of the Association for Computational Linguistics_, 2022.
* [35] N. Methani, P. Ganguly, M. M. Khapra, and P. Kumar. PlotQA: Reasoning over scientific plots. _Conference on Applications of Computer Vision_, 2020.
* [36] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? _arXiv:2202.12837_, 2022.
* [37] M. Okawa, E. S. Lubana, R. Dick, and H. Tanaka. Compositional abilities emerge multiplicatively: Exploring diffusion models on a synthetic task. _Neural Information Processing Systems_, 2023.
* [38] S. Ontanon, J. Ainslie, V. Cvicek, and Z. Fisher. Making transformers solve compositional tasks. _arXiv:2108.04378_, 2021.
* [39] OpenAI. GPT-4 technical report. _arXiv:2303.08774_, 2024.
* [40] OpenAI. GPT-4o system card. https://openai.com/research/gpt-4o-system-card, Aug 2024.
* [41] R. Paiss, A. Ephrat, O. Tov, S. Zada, I. Mosseri, M. Irani, and T. Dekel. Teaching CLIP to count to ten. _International Conference on Computer Vision_, 2023.
* [42] O. Press, M. Zhang, S. Min, L. Schmidt, N. A. Smith, and M. Lewis. Measuring and narrowing the compositionality gap in language models. _arXiv:2210.03350_, 2022.
* [43] P. Rahmanzadehgervi, L. Bolton, M. R. Taesiri, and A. T. Nguyen. Vision language models are blind. _arXiv:2407.06581_, 2024.
* [44] R. Ramesh, E. S. Lubana, M. Khona, R. P. Dick, and H. Tanaka. Compositional capabilities of autoregressive transformers: A study on synthetic, interpretable tasks. _International Conference on Machine Learning_, 2024.
* [45] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, R. Sauvestre, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton, M. Bhatt, C. C. Ferrer, A. Grattafiori, W. Xiong, A. Defossez, J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, and G. Synnaeve. Code Llama: Open foundation models for code. _arXiv:2308.12950_, 2023.
* [46] J. Shen, Y. Yuan, S. Mirzoyan, M. Zhang, and C. Wang. Measuring vision-language stem skills of neural models. _International Conference on Learning Representations_, 2024.
* [47] W. Shi, Z. Hu, Y. Bin, J. Liu, Y. Yang, S.-K. Ng, L. Bing, and R. K.-W. Lee. Math-LLaVA: Bootstrapping mathematical reasoning for multimodal large language models. _arXiv:2406.17294_, 2024.
* [48] J. Song, Z. Xu, and Y. Zhong. Out-of-distribution generalization via composition: a lens through induction heads in transformers. _arXiv:2408.09503_, 2024.
* [49] Gemini Team. Gemini: A family of highly capable multimodal models. _arXiv:2312.11805_, 2024.
* [50] T. Thrush, R. Jiang, M. Bartolo, A. Singh, A. Williams, D. Kiela, and C. Ross. Winoground: Probing vision and language models for visio-linguistic compositionality. _Computer Vision and Pattern Recognition_, 2022.
* [51] S. Tong, E. Brown, P. Wu, S. Woo, M. Middepogu, S. C. Akula, J. Yang, S. Yang, A. Iyer, X. Pan, A. Wang, R. Fergus, Y. LeCun, and S. Xie. Cambrian-1: A fully open, vision-centric exploration of multimodal LLMs. _arXiv:2406.16860_, 2024.
* [52] S. Tong, Z. Liu, Y. Zhai, Y. Ma, Y. LeCun, and S. Xie. Eyes wide shut? exploring the visual shortcomings of multimodal LLMs. _Computer Vision and Pattern Recognition_, 2024.

* [53] X. Wang, Z. Hu, P. Lu, Y. Zhu, J. Zhang, S. Subramaniam, A. R. Loomba, S. Zhang, Y. Sun, and W. Wang. SciBench: Evaluating college-level scientific problem-solving abilities of large language models. _International Conference on Machine Learning_, 2024.
* [54] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and Denny Zhou. Chain-of-Thought prompting elicits reasoning in large language models. _Neural Information Processing Systems_, 2024.
* [55] Z. Xu, Z. Shi, and Y. Liang. Do large language models have compositional ability? an investigation into limitations and scalability. _ICLR Workshop on Mathematical and Empirical Understanding of Foundation Models_, 2024.
* [56] M. Yukekgoul, F. Bianchi, P. Kalluri, D. Jurafsky, and J. Zou. When and why vision-language models behave like bags-of-words, and what to do about it? _International Conference on Learning Representations_, 2023.
* [57] R. Zhang, D. Jiang, Y. Zhang, H. Lin, Z. Guo, P. Qiu, A. Zhou, P. Lu, K. Chang, P. Gao, et al. MathVerse: Does your multi-modal LLM truly see the diagrams in visual math problems? _arXiv:2403.14624_, 2024.
* [58] H. Zhao, S. Kaur, D. Yu, A. Goyal, and S. Arora. Can models learn skill composition from examples? _ICML Workshop on LLMs and Cognition_, 2024.
* [59] T. Zhao, T. Zhang, M. Zhu, H. Shen, K. Lee, X. Lu, and J. Yin. VL-Checklist: Evaluating pre-trained vision-language models with objects, attributes and relations. _arXiv:2207.00221_, 2022.
* [60] M. Zheng, X. Feng, Q. Si, Q. She, Z. Lin, W. Jiang, and W. Wang. Multimodal table understanding. _arXiv:2406.08100_, 2024.

Prior works

VLM benchmarks and language shortcuts.Existing VLM benchmarks evaluate models on their ability to solve diverse vision-language problems from general real-world tasks [3; 13; 15], tasks that require specific skills such as high-school geometry [32; 20; 9; 8], analyzing charts and tables [34; 60; 35], and other scientific visual data [19; 21]. However, most VLM benchmarks do not contain a mechanism for verifying whether a correct solution is based on correctly comprehending the visual information, allowing the models to sometimes rely on linguistic biases to find a solution [7]. Lin et al. [27] revealed that by simply avoiding implausible or less fluent sentences, blind language models can distinguish the correct description of an image from wrong ones on CREPE [33], VL-Checklist [59], and ARO[56]. Mathverse [57] observed that, when solving geometry problems, VLMs rely mostly on textual inputs without correctly interpreting diagrams.

Some recent work has started to seek unbiased ways to measure visual capabilities. Winoground [50] prevents choosing image captions based on the plausibility of the sentence structure, by providing two images with same objects or concepts but with different relationships. Blink [11] and CV-Bench [51] present novel vision-oriented tasks with minimized effects of linguistic biases.

Compositional reasoning.There has been intensive recent research on the compositional capabilities of Language Models [4; 55; 17; 48; 44; 58; 23; 38; 42]. VLMs have additionally shown compositional capabilities in visual tasks [25; 37; 33; 59; 56]. However, such studies left the visual portion with less attention, thus vulnerable to linguistic shortcuts such as removing grammatically wrong sentences or choosing more realistic sentences as answers. To mitigate this issue, Sugar-Crepe [18] generated sentences with ChatGPT to provide incorrect captions of given images, with different compositional structures while as realistic as the ground truths.

Research on atomic skills of LLMs.To understand the capabilities of LLMs, there has been prior work on studying LLMs in simple idealized experiments. This includes research on in-context learning [28; 36], arithmetic (addition and multiplication) [38; 16; 24], fact search and reverse fact search [2; 6; 12], and programming [5; 45; 14].

However, there have been far fewer studies of this kind for vision language models. Paiss et al. [41] focused on counting objects in image and suggested CountBench. Shen et al. [46] suggest a skill-based approach to evaluating VLMs, but their list of skills is not atomic. CV-Bench [51] evaluates 4 vision-centric skills: spatial relationship, object count, depth order, and relative distance. MMVP [52] challenges VLMs to understand 9 visual patterns.

BlindTest [43] observed failures of VLMs with 7 simple tasks focusing on fundamental geometric features, some of which share similar approaches with AVSBench. While these tasks are novel and effective, they lack diversity in color, shape, or word choice. Their Task 1 for instance, uses only one red and one blue line, each with exactly one sharp turn. For Task 3, they adopted only three types of strings and a red circle to generate visual context. Moreover, these 7 tasks are insufficient to collectively evaluate the full spectrum of visual capabilities, leading to limited scope in evaluation objectives. In contrast, our AVSBench offers a systematic, comprehensive framework for evaluating a comprehensive set of visual skills of VLMs with an extensive dataset that is rich in color, shape, and other variations.

## Appendix B Problem difficulty categorization

The problems are categorized into three difficulty levels: easy, medium, and hard. Problems categorized as easy or medium should be quickly solvable by humans, whereas hard questions, although more time-consuming, are designed to be clear and easily verifiable. We clarify that the difficulty levels were determined by the authors, so there is a degree of subjectivity to the categorization.

Sample problems

We present 99 sample problems to provide the readers with the general characteristics of our AVS-Bench dataset, along with the responses of Gemini-1.5-pro[49], GPT-4o[40], and LLaVA-Next-13B[29]. For readability, we report a summary rather than the full text of the model response. See Figure 5 for examples of full model responses.

Figure 5: Full responses of VLMs and scoring by GPT-4o mini.

Figure 6: Sample problems from AVSBench and responses from Gemini-1.5-pro, GPT-4o, and LLaVA-Next-13B, 1/11.

Figure 7: Sample problems from AVSBench and responses from Gemini-1.5-pro, GPT-4o, and LLaVA-Next-13B, 2/11.

Figure 8: Sample problems from AVSBench and responses from Gemini-1.5-pro, GPT-4o, and LLaVA-Next-13B, 3/11.

Figure 9: Sample problems from AVSBench and responses from Gemini-1.5-pro, GPT-4o, and LLaVA-Next-13B, 4/11.

Figure 10: Sample problems from AVSBench and responses from Gemini-1.5-pro, GPT-4o, and LLaVA-Next-13B, 5/11.

Figure 11: Sample problems from AVSBench and responses from Gemini-1.5-pro, GPT-4o, and LLaVA-Next-13B, 6/11.

Figure 12: Sample problems from AVSBench and responses from Gemini-1.5-pro, GPT-4o, and LLaVA-Next-13B, 7/11.

Figure 13: Sample problems from AVSBench and responses from Gemini-1.5-pro, GPT-4o, and LLaVA-Next-13B, 8/11.

Figure 14: Sample problems from AVSBench and responses from Gemini-1.5-pro, GPT-4o, and LLaVA-Next-13B, 9/11.

Figure 15: Sample problems from AVSBench and responses from Gemini-1.5-pro, GPT-4o, and LLaVA-Next-13B, 10/11.

Figure 16: Sample problems from AVSBench and responses from Gemini-1.5-pro, GPT-4o, and LLaVA-Next-13B, 11/11.

Descriptions of the atomic visual skills

In this section, we provide detailed definitions of the 36 atomic visual skills, together with a corresponding problem sample from AVSBench.

1. Angle is a skill to understand how an angle is visually represented. Angle is the primary factor in how a polygon looks, how two or more objects are related, and many other situations. 
2. Direction is an ability to recognize linear direction in an image. It is a fundamental skill in human vision, supporting representation of linearity and multi-dimensional relations. 
3. Boundary is a skill to understand the ends of objects or areas, and to detect visual representation of edges. The skill is used in distinguishing between distinct objects, or detecting boundaries between spaces. 

Q) Let's say there are farms called A, B, C, D, E, F, and G, as shown in the image. Black curves represent their fences. From which farm are the sheep unable to escape?4. Cardinal is a field about counting distinct objects or specified concepts. Mastery of cardinals implies measuring quantities or dealing with multiple objects. Especially, it should take into account everything that satisfies given conditions, giving a difference from the skill of understanding _Ordinals_.
5. Congruence is a skill of detecting objects with the exact same scale and shape, and understanding their correspondence. Congruence is a primary component of visualizing various symmetries including translation, rotation or flipping. Congruence is distinguished from other equivalence because it requires the objects to be equal at all levels of measurement.
6. Convexity is a skill of understanding convexity of given shapes. The skill is also closely related to detecting bumps or indentations and understanding convex and concave functions.

7. Intersection is a mastery of detecting intersections of lines and curves. The skill is necessary for interpreting relationships among 1-dimensional objects, and also among higher dimensional objects from 1-dimensional representations of their boundaries.
8. Line is a skill to detect line segments and understand their roles in the image. This skill is a fundamental unit in understanding various objects as polygons, graphs and diagrams.
9. OCR is a skill to detect and read characters from visual inputs.

10. Ordinal is a skill to count certain objects or concepts in a given order. Mastery of this skill requires not just counting but also focusing on specific portions and order of targets, giving a difference from Cardinal Understanding.

11. Overlap skill is about correctly recognizing two or more objects sharing a common area. The skill is crucial in understanding overlapping shapes or complex shapes such as diagrams.

12. Interior is a skill of distinguishing between interior and exterior of the target area. This skill is essential in perceiving different areas.

13. Interior is a skill of distinguishing between interior and exterior of the target area. This skill is essential in perceiving different areas.

14. Interior is a skill of distinguishing between interior and exterior of the target area. This skill is essential in perceiving different areas.

15. Interior is a skill of distinguishing between interior and exterior of the target area. This skill is essential in perceiving different areas.

16. Interior is a skill of distinguishing between interior and exterior of the target area. This skill is essential in perceiving different areas.

17. Interior is a skill of distinguishing between interior and exterior of the target area. This skill is essential in perceiving different areas.

18. Interior is a skill of distinguishing between interior and exterior of the target area. This skill is essential in perceiving different areas.

[MISSING_PAGE_EMPTY:26]

16. Rotation is an ability to identify changes in positions and angles induced by rotation, and detecting the axis of rotation.

17. Rotational Symmetry is a field of symmetric representations with respect to rotations. The skill involves understanding invariant geometric features under specified rotations.

18. Symbol is a skill to detect symbols, understand their roles in the image, and combine them with other visual information to attain the correct interpretation of the image.

19.

19. Texture is a skill to understand textures of objects in the image. The skill is essential as texture is another main component of visual representation of objects, and is used to distinguish different objects with same shapes, such as line styles.
20. Width is a skill to understand thickness and width of objects or areas. The skill is essential in measuring area or proportion of images together with length understanding.
21. Adjacency is a skill to recognize when two or more objects are next to each other. The skill is crucial in understanding features induced by close positions such as forming clusters.

[MISSING_PAGE_FAIL:29]

25. Orthogonality is a skill to identify orthogonal relations of objects in the image, including a right angle formed by two lines. Understanding orthogonality is fundamental in geometry, design, and engineering.
26. Tangency is a skill to detect tangent objects. This skill focuses on geometric representation of tangent curves or boundaries, and is different from understanding adjacency that rather focuses on positional information.
27. Connectedness is a skill to identify connected components and detect links between objects. This is crucial in understanding interactions and distinguishing distinct components.

28. Parallel is a skill to recognize parallel lines or curves. This is essential in identifying fundamental objects like squares.
29. Similarity is a skill to understand equivalence of geometric representations independent of scale. It also involves understanding of rescaling or comparing aspect ratios.
30. Color is an ability to perceive, distinguish different colors, and understand the change in saturation and brightness.

31. Coordinate is a skill to recognize and acquire correct information upon coordinate systems. We provide and acquire information about different systems such as polar coordinates.
32. Point is a fundamental capability to detect points and understand their roles in the image. It also involves understanding nodes in different graphs.
33. Shape is a skill to understand details of shapes and compare different shapes independently of positions or tilts. It also involves identifying popular shapes such as triangles, rectangles, circles, and stars.

34.

[MISSING_PAGE_EMPTY:33]

Model versions

We evaluated closed-source models ChatGPT [40], Gemini [49] and open-weight models LLaVA-NeXT [29], LLaVA-OneVision [26], Math-LLaVA [47], Table-LLaVA [60], Phi-3.5-Vision [1], InternVL2 [10], DeepSeek-VL [31]. Tables 1 and 2 describe further details about the model sizes and versions. For closed-source models, we used the commercial APIs. All models' temperatures were set to \(0\).

## Appendix F Further details on evaluation process

We used GPT-4o mini to extract answers from model responses and to judge correctness. Few-shot in-context learning prompts we provided to GPT-4o mini as described in Tables 3 and 4. To verify the reliability of this pipeline, we randomly selected \(128\) problems from our dataset and compared the scores from GPT-4o mini with human annotations. Reassuringly, GPT-4o mini and the human annotators agreed on the scoring of the \(128\) problems. We attribute this high level of reliability, in part, to the straightforward and clear design of our questions and answers.

\begin{table}
\begin{tabular}{c|c} \hline
**Model Name** & **Version** \\ \hline ChatGPT & gpt-4o-2024-05-13 \\  & gpt-4o-mini-2024-07-18 (For scoring) \\ Gemini & gemini-1.5-pro-001 \\ \hline \end{tabular}
\end{table}
Table 1: Versions of closed-source models

\begin{table}
\begin{tabular}{c|c} \hline
**Version** & **Model Size(s)** \\ \hline LLaVA-NeXT & 7B, 13B \\ LLaVA-OneVision & 7B \\ Math-LLaVA & 13B \\ Table-LLaVA & 7B \\ Phi-3.5-Vision-Instruct & 4B \\ InternVL2 & 8B \\ DeepSeek-VL & 7B \\ \hline \end{tabular}
\end{table}
Table 2: Versions and model sizes of open-weight models

\begin{table}
\begin{tabular}{c l} \hline \hline
**Element** & **Prompt** \\ \hline System prompt & Imagine you are an intelligent teacher. Thoroughly read the provided \\  & instruction to ensure a solid understanding of the information provided. \\ \hline Task description & Please read the following example. Then extract the answer from the \\  & model response and type it at the end of the prompt. If the question \\  & requires a full sentence with a correct word filled in, please provide the \\  & word only. \\  & \{_examples_\} \\  & Question: \{_question_\} \\  & Model response: \{_model response_\} \\  & Extracted Answer: \\ \hline Examples & **Question:** There is a single rectangle with multiple color layers in the \\  & image. What is the color of the boundary of the rectangle? The answer \\  & should be one of ’red’, ’yellow’, ’green’, or ‘blue’. \\  & **Model response:** The color of the boundary of the circle is red. \\  & **Extracted answer:** red \\  & **Question:** How many line segments are in the image? Answer should \\  & be a number. \\  & **Model response:** There are 4 dashed line segments in the image. \\  & **Extracted answer:** 4 \\  & **Question:** Choose the word in parentheses that correctly describes the image. Rewrite the sentence with the chosen word. \\  & In the image, shape (A/B) has sides curved inward. (Unit: \$) \\  & **Model response:** In the image, shape B has sides curved inward. \\  & **Extracted answer:** B \\  & **Question:** Choose the phrase in parentheses that correctly describes the image. Rewrite the sentence with the chosen phrase. \\  & In the given image, the green arrow (is longer than/has the same length as/is shorter than) the black arrow. \\  & **Model response:** In the given image, the green arrow is longer than the \\  & **Extracted answer:** is longer than \\  & **Question:** In this image, choose the path which is a single line segment \\  & between points A and B from the following options. Provide your \\  & answer as a single uppercase letter: (A) the purple path (B) the blue path (C) the green path (D) the red path \\  & **Model response:** B \\  & **Extracted answer:** B \\  & **Question:** Choose the most appropriate color to fill in the box marked with ’?’ in the image. The answer is one of ’a’, ’b’, ‘c’, or ‘d’. \\  & **Model response:** The correct color to fill in the box marked with ’?’ is \\  & (a) blue. The colors are following a gradient pattern from red, to a more \\  & purple hue, and finally to blue. The logical next color in the sequence \\  & **Extracted answer:** a \\  & **Question:** There is a book in the image. What is the color of the book in the image? Choose answer from the number of the option and give your \\  & answer in “1”, “2”, “3”, or “4”. (1) red (2) yellow (3) blue (4) green \\  & **Model response:** The color of the guitar in the image is (2) yellow. \\  & **Extracted answer:** 2 \\ \hline \hline \end{tabular}
\end{table}
Table 3: System prompt, task description, and examples used to prompt GPT-4o mini for answer extraction.

\begin{table}
\begin{tabular}{c l} \hline \hline
**Element** & **Prompt** \\ \hline System prompt & Imagine you are an intelligent teacher. Thoroughly read the provided \\  & instruction to ensure a solid understanding of the information provided. \\ \hline Task description & The [Standard Answer] is the correct answer to the question, and the \\  & [Model Answer] is the answer generated by a model for that question. \\  & Thoroughly read both the [Standard Answer] and the [Model Answer]. \\  & Assess the consistency of the information provided in these two \\  & \\  & \\  & \\  & \\  & \\  & \\  & \\  & \\  & \\  & \\  & \\  & \\  & \\  & \\ \hline Examples & **[Standard Answer]** a \\  & **[Model Answer]** a \\  & **Judgment:** 1 \\  & **[Standard Answer]** 1 \\  & **[Model Answer]** 4 \\  & **Judgment:** 0 \\  & **[Standard Answer]** circle \\  & **[Model Answer]** the circle \\  & **Judgment:** 1 \\  & **[Standard Answer]** 4 \\  & **[Model Answer]** shape 4 \\  & **Judgment:** 1 \\  & **[Standard Answer]** line segment B and C \\  & **[Model Answer]** B, C \\  & **Judgment:** 1 \\  & **[Standard Answer]** three \\  & **[Model Answer]** 3 \\  & **Judgment:** 1 \\  & **[Standard Answer]** [’ac', 'ca'] \\  & **[Model Answer]** ca \\  & **Judgment:** 1 \\ \hline \hline \end{tabular}
\end{table}
Table 4: System prompt, task description, and examples used to prompt GPT-4o mini for judgment.

Results on 36 skills and Further Analysis

In this section, we provide full details of Section 3 and further analysis of Findings 3.

Table 5 presents our full evaluation results on AVSBench. "Random Chance" represents the expected accuracy of randomly choosing multiple-choice questions and scoring 0 from all non-multiple-choice questions. The subcolumn named "overall" indicates the accuracy across all problems of its corresponding skill. The column named "**TOTAL**" describes the overall accuracy across all problems in AVSBench, involving all skills.

As mentioned in Findings 3, CoT did not provide meaningful performance gains when used with GPT-4o and Gemini-1.5-pro. Gemini-1.5-pro had only a \(0.1\%\) gain from applying CoT, and GPT-4o had a \(1\%\) gain. However, CoT worsened performances of skills including OCR, Length, and Symbol. As in the case of Figure 17, by inspecting the responses of GPT-4o with and without CoT prompting, we observe that the additional reasoning steps afforded by CoT are not helpful in comprehending visual inputs.

Figure 17: Responses of GPT-4o on a problem from AVSBench with and without chain-of-thought (CoT) prompt.

[MISSING_PAGE_EMPTY:38]