# A Walsh Hadamard Derived Linear Vector Symbolic Architecture

 Mohammad Mahmudul Alam\({}^{1}\), Alexander Oberle\({}^{1}\), Edward Raff\({}^{1,2}\), Stella Biderman\({}^{2}\),

Tim Oates\({}^{1}\), James Holt\({}^{3}\)

\({}^{1}\)University of Maryland, Baltimore County,\({}^{2}\)Booz Allen Hamilton,

\({}^{3}\) Laboratory for Physical Sciences

m256@umbc.edu, aoberle1@umbc.edu, Raff_Edward@bah.com,

biderman_stella@bah.com, oates@cs.umbc.edu, holt@lps.umd.edu

###### Abstract

Vector Symbolic Architectures (VSAs) are one approach to developing Neuro-symbolic AI, where two vectors in \(\mathbb{R}^{d}\) are 'bound' together to produce a new vector in the same space. VSAs support the commutativity and associativity of this binding operation, along with an inverse operation, allowing one to construct symbolic-style manipulations over real-valued vectors. Most VSAs were developed before deep learning and automatic differentiation became popular and instead focused on efficacy in hand-designed systems. In this work, we introduce the Hadamard-derived linear Binding (HLB), which is designed to have favorable computational efficiency, and efficacy in classic VSA tasks, and perform well in differentiable systems. Code is available at https://github.com/FutureComputing4AI/Hadamard-derived-Linear-Binding.

## 1 Introduction

Vector Symbolic Architectures (VSAs) are a unique approach to performing symbolic style AI. Such methods use a binding operation \(\mathcal{B}:\mathbb{R}^{d}\times\mathbb{R}^{d}\longrightarrow\mathbb{R}^{d}\), where \(\mathcal{B}(x,y)=z\) denotes that two concepts/vectors \(x\) and \(y\) are connected to each other. In VSA, any arbitrary concept is assigned to vectors in \(\mathbb{R}^{d}\) (usually randomly). For example, the sentence "the fat cat and happy dog" would be represented as \(\mathcal{B}(\mathit{fat},\mathit{cat})+\mathcal{B}(\mathit{happy},\mathit{dog })=\bm{S}\). One can then ask, "what was happy" by _unbinding_ the vector for happy, which will return a noisy version of the vector bound to happy. The unbinding operation is denoted \(\mathcal{B}^{\star}(x,y)\), and so applying \(\mathcal{B}^{\star}(S,\mathit{happy})\approx\mathit{dog}\).

Because VSAs are applied over vectors, they offer an attractive platform for neuro-symbolic methods by having natural symbolic AI-style manipulations via differentiable operations. However, current VSA methods have largely been derived for classical AI tasks or cognitive science-inspired work. Many such VSAs have shown issues in numerical stability, computational complexity, or otherwise lower-than-desired performance in the context of a differentiable system.

As noted in [39], most VSAs can be viewed as a linear operation where \(\mathcal{B}(a,b)=a^{\top}Gb\) and \(\mathcal{B}^{\star}(a,b)=a^{\top}Gb\), where \(G\) and \(F\) are \(d\times d\) matrices. Hypothetically, these matrices could be learned via gradient descent, but would not necessarily maintain the neuro-symbolic properties of VSAs without additional constraints. Still, the framework is useful as all popular VSAs we are aware fit within this framework. By choosing \(G\) and \(F\) with specified structure, we can change the computational complexity from \(\mathcal{O}(d^{2})\), down to \(\mathcal{O}(d)\) for a diagonal matrix.

In this work, we derive a new VSA that has multiple desirable properties for both classical VSA tasks, and in deep-learning applications. Our method will have only \(\mathcal{O}(d)\) complexity for the binding step, is numerically stable, and equals or improves upon previous VSAs on multiple recent deeplearning applications. Our new VSA is derived from the Walsh Hadamard transform, and so we term our method the Hadamard-derived linear Binding (HLB) as it will avoid the \(\mathcal{O}(d\log d)\) normally associated with the Hadamard transform, and has better performance than more expensive VSA alternatives.

Related work to our own will be reviewed in section 2, including our baseline VSAs and their definitions. Our new HLB will be derived in section 3, showing it theoretically desirable properties. section 4 will empirically evaluate HLB in classical VSA benchmark tasks, and in two recent deep learning tasks, showing improved performance in each scenario. We then conclude in section 5.

## 2 Related Work

Smolensky [38] started the VSA approach with the Tensor Product Representation (TPR), where \(d\) dimensional vectors (each representing some concept) were bound by computing an outer product. Showing distributivity (\(\mathcal{B}(\bm{x},\bm{y}+\bm{z})=\mathcal{B}(\bm{x},\bm{y})+\mathcal{B}(\bm{x },\bm{z})\)) and associativity, this allowed specifying logical statements/structures [13]. However, for \(\rho\) total items to be bound together, it was impractical due to the \(\mathcal{O}(d^{\rho})\) complexity. [36; 25; 24] have surveyed many of the VSAs available today, but our work will focus on three specific alternatives, as outlined in Table 1. The Vector-Derived Transformation Binding (VTB) will be a primary comparison because it is one of the most recently developed VSAs, which has shown improvements in what we will call "classic" tasks, where the VSA's symbolic like properties are used to manually construct a series of binding/unbinding operations that accomplish a desired task. Note, that the VTB is unique in it is non-symmetric (\(\mathcal{B}(\bm{x},\bm{y})\neq\mathcal{B}(\bm{y},\bm{x})\)). Ours, and most others, are symmetric.

Next is the Holographic Reduced Representation (HRR) [32], which can be defined via the Fourier transform \(\mathcal{F}(\cdot)\). One derives the inverse operation of the HRR by defining the one vector \(\vec{\mathrm{I}}\) as the identity vector and then solving \(\mathcal{F}(\bm{a}^{*})_{i}\mathcal{F}(\bm{a})_{i}=1\). We will use a similar approach to deriving HLB but replacing the Fourier Transform with the Hadamard transform, making the HRR a key baseline. Last, the Multiply Add Permute (MAP) [10] is derived by taking only the diagonal of the tensor product from [38]'s TPR. This results in a surprisingly simple representation of using element-wise multiplication for both binding/unbinding, making it a key baseline. The MAP binding is also notable for its continuous (MAP-C) and binary (MAP-B) forms, which will help elucidate the importance of the difference in our unbinding step compared to the initialization avoiding values near zero. HLB differs in devising for the unbinding step, and we will later show an additional corrective term that HLB employs for \(\rho\) different items bound together, that dramatically improve performance.

Our motivation for using the Hadamard Transform comes from its parallels to the Fourier Transform (FT) used to derive the HRR and the HRR's relatively high performance. The Hadamard matrix has a simple recursive structure, making analysis tractable, and its transpose is its own inverse, which simplifies the design of the inverse function \(\mathcal{B}^{*}\). Like the FT, WHT can be computed in log-linear time, though in our case, the derivation results in linear complexity as an added benefit. The WHT is already associative and distributive, making less work to obtain the desired properties. Finally, the WHT involves only \(\{-1,1\}\) values, avoiding numerical instability that can occur with the HRR/FT.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Method & \begin{tabular}{c} \(\mathcal{B}(x,y)\) \\ \end{tabular} & \begin{tabular}{c} Unbind \(\mathcal{B}^{*}(x,y)\) \\ \end{tabular} & 
\begin{tabular}{c} Init \(x\) \\ \end{tabular} \\ \hline HRR & \(\mathcal{F}^{-1}(\mathcal{F}(\bm{x})\odot\mathcal{F}(\bm{y}))\) & \(\mathcal{F}^{-1}(\mathcal{F}(\bm{x})\oplus\mathcal{F}(\bm{y}))\) & \(x_{i}\sim\mathcal{N}(0,1/d)\) \\ VTB & \(V_{y}x\) & \(V_{y}^{\top}x\) & \(\tilde{x}_{i}\sim\mathcal{N}(0,1)\to x=\bm{\hat{x}}/\|\bm{\hat{x}}\|_{2}\) \\ MAP-C & \(x\odot y\) & \(x\odot y\) & \(x_{i}\sim\mathcal{U}(-1,1)\) \\ MAP-B & \(x\odot y\) & \(x\odot y\) & \(x_{i}\sim\{-1,1\}\) \\ HLB & \(x\odot y\) & \(x\oplus y\) & \(x_{u}\sim\{\mathcal{N}(-\mu,1/d),\ \mathcal{N}(\mu,1/d)\}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: The binding and initialization mechanisms for our new HLB with baseline methods. HLB is related to the HRR in being derived via a similar approach, but replacing the Fourier transform \(\mathcal{F}(\cdot)\) with the Hadamard transform (which simplifies out). The MAP is most similar to our approach in mechanics, but the difference in derived unbinding steps leads to dramatically different performance. The VTB is the most recently developed VSA in modern use. The matrix \(V_{\bm{y}}\) of VTB is a block-diagonal matrix composed from the values of the \(\bm{y}\) vector, which we refer the reader to [12] for details. The TorchHD library [15] is used for implementations of prior methods.

This work shows that these motivations are well founded, as they result in a binding with comparable or improved performance in our testing.

Our interest in VSAs comes from their utility in both classical symbolic tasks and as useful priors in designing deep learning systems. In classic tasks VSAs are popular for designing power-efficient systems from a finite set of operations [14; 23; 17; 30]. HRRs, in particular, have shown biologically plausible models of human cognition [21; 5; 40; 6] and solving cognitive science tasks [8]. In deep learning the TPR has inspired many prior works in natural language processing [34; 16; 35]. To wit, The HRR operation has seen the most use in differentiable systems [43; 41; 42; 27; 29; 33; 1; 2; 28]. To study our method, we select two recent works that make heavy use of the neuro-symbolic capabilities of HRRs. First, an Extreme Multi-Label (XML) task that uses HRRs to represent an output space of tens to hundreds of thousands of classes \(C\) in a smaller dimension \(d<C\)[9], and an information privacy task that uses the HRR binding as a kind of "encrypt/decrypt" mechanism for heuristic security [3]. We will explain these methods in more detail in the experimental section.

## 3 Methodology

First, we will briefly review the definition of the Hadamard matrix \(H\) and its important properties that make it a strong candidate from which to derive a VSA. With these properties established, we will begin by deriving a VSA we term HLB where binding and unbinding are the same operation in the same manner as which the original HRR can be derived [32]. Any VSA must introduce noise when vectors are bound together, and we will derive the form of the noise term as \(\eta^{\circ}\). Unsatisfied with the magnitude of this term, we then define a projection step for the Hadamard matrix in a similar spirit to '[9]'s complex-unit magnitude projection to support the HRR and derive an improved operation with a new and smaller noise term \(\eta^{\pi}\). This will give us the HLB bind/unbind steps as noted in Table 1.

Hadamard \(H_{d}\) is a square matrix of size \(d\times d\) of orthogonal rows consisting of only \(+1\) and \(-1\)s given in Equation 1 where \(d=2^{n}\)\(\forall\)\(n\in\mathbb{N}:n\geq 0\). Bearing in mind that Hadamard or Walsh-Hadamard Transformation (WHT) can be equivalent to discrete multi-dimensional Fourier Transform (FT) when applied to a \(d\) dimensional vector [26], it has additional advantages over Discrete Fourier Transform (DFT). Unlike DFT, which operates on complex \(\mathbb{C}\) numbers and requires irrational multiplications, WHT only performs calculations on real \(\mathbb{R}\) numbers with addition and subtraction operators and does not require any irrational multiplication.

\[H_{1}=[1]\qquad H_{2}=\begin{bmatrix}1&1\\ 1&-1\end{bmatrix}\qquad\cdots\qquad H_{2^{n}}=\begin{bmatrix}H_{2^{n-1}}&H_{2 ^{n-1}}\\ H_{2^{n-1}}&-H_{2^{n-1}}\end{bmatrix}\] (1)

Vector symbolic architectures (VSA), for instance, Holographic Reduced Representations (HRR) employs circular convolution to represent compositional structure which is computed using Fast Fourier Transform (FFT) [32]. However, it can be numerically unstable due to irrational multiplications of complex numbers. Prior work [9] devised a projection step to mitigate the numerical instability of the FFT and it's inverse, but we instead ask if re-deriving the binding/unbinding operations may yield favorable results if we use the favorable properties of the Hadamard transform as given in Lemma 3.1.

**Lemma 3.1** (Hadamard Properties).: _Let \(H\) be the Hadamard matrix of size \(d\times d\) that holds the following properties for \(x,y\in\mathbb{R}^{d}\). First, \(H(Hx)=dx\), and second \(H(x+y)=Hx+Hy\)._

The bound composition of two vectors into a single vector space is referred to as Binding. The knowledge retrieval from a bound representation is known as Unbinding. We define the binding function by replacing the Fourier transform in circular convolution with the Hadamard transform given in Definition 3.1. We will denote the binding function four our specific method by \(\mathcal{B}\) and the unbinding function by \(\mathcal{B}^{*}\).

**Definition 3.1** (Binding and Unbinding).: The binding of vectors \(x,y\in\mathbb{R}^{d}\) in Hadamard domain is defined in Equation 2 where \(\odot\) is the elementwise multiplication. The unbinding function is defined in a similar fashion, i.e., \(\mathcal{B}=\mathcal{B}^{*}\). In the context of binding, \(\mathcal{B}(x,y)\) combines the vectors \(x\) and \(y\), whereas in the context of unbinding \(\mathcal{B}^{*}(x,y)\) refers to the retrieval of the vector associated with \(y\) from \(x\).

\[\mathcal{B}(x,y)=\frac{1}{d}\cdot H(Hx\odot Hy)\] (2)Now, we will discuss the binding of \(\rho\) different representations, which will become important later in our analysis but is discussed here for adjacency to the binding definition. Composite representation in vector symbolic architectures is defined by the summation of the bound vectors. We define a parameter \(\rho\in\mathbb{N}:\rho\geqslant 1\) that denotes the number of vector pairs bundled in a composite representation. Given vectors \(x_{i},y_{i}\in\mathbb{R}^{d}\) and \(\forall\ i\in\mathbb{N}:1\leqslant i\leqslant\rho\), we can define the composite representation \(\chi\) as

\[\underset{\rho=1}{\chi}=\mathcal{B}(x_{1},y_{1})\qquad\underset{\rho=2}{ \chi}=\mathcal{B}(x_{1},y_{1})+\mathcal{B}(x_{2},y_{2})\qquad\cdots\qquad \chi_{\rho}=\sum_{i=1}^{\rho}\mathcal{B}(x_{i},y_{i})\] (3)

Next, we require the unbinding operation, which is defined via an inverse function via the following theorem. This will give a symbolic form of our unbinding step that retrieves the original component \(\bm{x}\) being searched for, as well as a necessary noise component \(\bm{\eta}^{\circ}\), which must exist whenever \(\rho\geqslant 2\) items are bound together without expanding the dimension \(d\).

**Theorem 3.1** (Inverse Theorem).: _Given the identity function \(Hx\cdot Hx^{\dagger}=\mathds{1}\) where \(x^{\dagger}\) is the inverse of \(x\) in Hadamard domain, then \(\mathcal{B}^{*}(\mathcal{B}(x_{1},y_{1})+\cdots+\mathcal{B}(x_{\rho},y_{\rho} ),y_{i}^{\dagger})=\begin{cases}x_{i}&\text{if }\quad\rho=1\\ x_{i}+\eta_{i}^{\circ}&\text{else }\rho>1\end{cases}\) where \(x_{i},y_{i}\in\mathbb{R}^{d}\) and \(\eta_{i}^{\circ}\) is the noise component._

Proof of Theorem 3.1.: We start from the identity function \(Hx\cdot Hx^{\dagger}=\mathds{1}\) and thus \(Hx^{\dagger}=\frac{\mathds{1}}{Hx}\). Now using Equation 2 we get,

\[\mathcal{B}^{*}(\mathcal{B}(x_{1},y_{1})+\cdots+\mathcal{B}(x_{ \rho},y_{\rho}),y_{i}^{\dagger})=\frac{1}{d}\cdot H((Hx_{1}\odot Hy_{1}+ \cdots+Hx_{\rho}\odot Hy_{\rho})\odot\frac{1}{Hy_{i}})\] \[=\frac{1}{d}\cdot H(Hx_{i}+\frac{1}{Hy_{i}}\odot\underset{j\neq i }{\sum_{j=1}^{\rho}(Hx_{j}\odot Hy_{j})})=x_{i}+\frac{1}{d}\cdot H(\frac{1}{Hy _{i}}\odot\underset{j\neq i}{\sum_{j=1}^{\rho}(Hx_{j}\odot Hy_{j})})\quad Lemma\ref{lem:Hx}\] \[=\begin{cases}x_{i}&\text{if }\quad\rho=1\\ x_{i}+\eta_{i}^{\circ}&\text{else }\rho>1\end{cases}\]

To reduce the noise component and improve the retrieval accuracy, [9, 32] proposes a projection step to the input vectors by normalizing them by the absolute value in the Fourier domain. While such identical normalization is not useful in the Hadamard domain since it will only transform the elements to \(+1\) and \(-1\)s, we will define a projection step with only the Hadamard transformation without normalization given in Definition 3.2.

**Definition 3.2** (Projection).: The projection function of \(x\) is defined by \(\pi(x)=\frac{1}{d}\cdot Hx\).

If we apply the Definition 3.2 to the inputs in Theorem 3.1 then we get

\[\mathcal{B}^{*}(\mathcal{B}(\pi(x_{1}),\pi(y_{1}))+\cdots+\mathcal{ B}(\pi(x_{\rho}),\pi(y_{\rho})),\pi(y_{i})^{\dagger}) =\mathcal{B}^{*}(\frac{1}{d}\cdot H(x_{1}\odot y_{1}+\cdots x_{ \rho}\odot y_{\rho}),\frac{1}{y_{i}})\] \[=\frac{1}{d}\cdot H(\frac{1}{y_{i}}\odot(x_{1}\odot y_{1}+\cdots x _{\rho}\odot y_{\rho}))\] (4)

The retrieved value would be projected onto the Hadamard domain as well and to get back the original data we apply the reverse projection. Since the inverse of the Hadamard matrix is the Hadamard matrix itself, in the reverse projection step we just apply the Hadamard transformation again which derives the output to

\[H(\frac{1}{d}\cdot H(\frac{1}{y_{i}}\odot(x_{1}\odot y_{1}+\cdots x _{\rho}\odot y_{\rho}))) =\frac{1}{y_{i}}\odot(x_{1}\odot y_{1}+\cdots+x_{\rho}\odot y_{ \rho})\] (5) \[=\begin{cases}x_{i}&\text{if }\quad\rho=1\\ x_{i}+\sum\limits_{j=1,\ j\neq i}^{\rho}\frac{x_{j}y_{j}}{y_{i}}&\text{else }\rho>1 \end{cases}\quad=\begin{cases}x_{i}&\text{if }\quad\rho=1\\ x_{i}+\eta_{i}^{\pi}&\text{else }\rho>1\end{cases}\]

where \(\eta_{i}^{\pi}\) is the noise component due to the projection step. In expectation, \(\eta_{i}^{\pi}<\eta_{i}^{\circ}\) (see Appendix A). Thus, the projection step diminishes the accumulated noise. More interestingly, the retrieved output term does not contain any Hadamard matrix. Therefore, we can recast the initial binding definition by multiplying the query vector \(y_{i}\) to the output of Equation 5 which makes the binding function as the sum of the element-wise product of the vector pairs and the compositional structure a linear time \(\mathcal{O}(n)\) representation. Thus, the redefinition of the binding function is \(\mathcal{B}^{\prime}(x,y)=x\odot y\) and \(\rho\) bundle of the vector pairs is \(\chi^{\prime}_{\rho}=\sum_{i=1}^{\rho}(x_{i}\odot y_{i})\). Consequently, the unbinding would be a simple element-wise division of the bound representation by the query vector, i.e, \(\mathcal{B}^{*\prime}(x,y)=x\odot\frac{1}{y}\) where \(x\) and \(y\) are the bound and query vector, respectively.

### Initialization of HLB

For the binding and unbinding operations to work, vectors need to have an expected value of zero. However, since we would divide the bound vector with query during unbinding, values close to zero would destabilize the noise component and create numerical instability. Thus, we define a Mixture of \(\mathcal{N}\)ormal Distribution (MiND) with an expected value of zero but an absolute mean greater than zero given in Equation 6 where \(\mathcal{U}\) is the Uniform distribution. Considering half of the elements are sampled for a normal distribution of mean \(-\mu\) and the rest of the half with a mean of \(\mu\), the resulting vector has a zero mean with an absolute mean of \(\mu\). The properties of the vectors sampled from a MiND distribution are given in Properties 3.1.

\[\Omega(\mu,1/d)=\begin{cases}\mathcal{N}(-\mu,1/d)&\text{if}\quad\mathcal{U}(0,1)>0.5\\ \mathcal{N}(\,\mu,1/d)&\text{else}\ \mathcal{U}(0,1)\leq 0.5\end{cases}\] (6)

**Properties 3.1** (Initialization Properties).: _Let \(x\in\mathbb{R}^{d}\) sampled from \(\Omega(\mu,1/d)\) holds the following properties. \(\mathrm{E}[x]=0,\ \mathrm{E}[|x|]=\mu\), and \(\left\|x\right\|_{2}=\sqrt{\mu^{2}d}\)_

### Similarity Augmentation

In VSAs, it is common to measure the similarity with an extracted embedding \(\hat{\bm{x}}\) with some other vector \(\bm{x}\) using the cosine similarity. For our HLB, we devise a correction term when it is known that \(\rho\) items have been bound together to extract \(\hat{\bm{x}}\), i.e., \(\mathcal{B}^{\bullet}(\chi_{\rho},\bm{z})=\hat{\bm{x}}\). Then if \(\hat{\bm{x}}\) is the noisy version of the true bound term \(\bm{x}\), we want \(\mathrm{cossim}(\hat{\bm{x}},\bm{x})=1\), and \(\mathrm{cossim}(\hat{\bm{x}},\bm{y})=0,\forall\bm{y}\neq\bm{x}\). We achieve this by instead computing \(\mathrm{cossim}(\hat{\bm{x}},\bm{x})\cdot\sqrt{\rho}\), and the derivation of this corrective term is given by Theorem 3.2.

**Theorem 3.2** (\(\phi-\rho\) Relationship).: _Given \(x_{i},y_{i}\thicksim\Omega(\mu,1/d)\ \forall\ i\in\mathbb{N}:1\leq i\leq\rho\), the cosine similarity \(\phi\) between the original \(x_{i}\) and retrieved vector \(\hat{x_{i}}\) is approximately equal to the inverse square root of the number of vector pairs in a composite representation \(\rho\) given by \(\phi\approx\frac{1}{\sqrt{\rho}}\)._

Proof of Theorem 3.2.: We start with the definition of cosine similarity and insert the value of \(\hat{x_{i}}\). The step-by-step breakdown is shown in Equation 7.

\[\phi=\frac{\sum\limits_{i}^{d}x_{i}\cdot\hat{x_{i}}}{\left\|x_{i}\right\|_{2} \cdot\left\|\hat{x_{i}}\right\|_{2}}=\frac{\sum\limits_{i=1,\,j\neq i}^{d} \frac{x_{i}y_{j}}{y_{i}}}{\left\|x_{i}\right\|_{2}\cdot\left\|x_{i}\right\|_{2 }\cdot\left\|x_{i}+\sum\limits_{j=1,\,j\neq i}^{\rho}\frac{x_{j}y_{j}}{y_{i}} \right\|_{2}}=\frac{\sum\limits_{i=1,\,j\neq i}^{d}x_{i}\cdot\sum\limits_{j=1, \,j\neq i}^{d}\frac{x_{j}y_{j}}{y_{i}}}{\left\|x_{i}\right\|_{2}\cdot\left\|x_ {i}+\sum\limits_{j=1,\,j\neq i}^{\rho}\frac{x_{j}y_{j}}{y_{i}}\right\|_{2}}\] (7)

Employing Properties 3.1 we can derive that \(\left\|x_{i}\right\|_{2}=\sqrt{\sum x_{i}\cdot x_{i}}=\sqrt{\mu^{2}d}\) and \(\left\|\frac{x_{j}y_{j}}{x_{i}}\right\|=\sqrt{\mu^{2}d}\). Thus, the square of the \(\left\|x_{i}+\sum\limits_{j=1,\,j\neq i}^{\rho}\frac{x_{j}y_{j}}{y_{i}}\right\|_ {2}\) can be expressed as

\[=\left\|x_{i}\right\|_{2}^{2}+\sum\limits_{j=1,\,j\neq i}^{\rho}\left\|\frac{x_ {j}y_{j}}{y_{i}}\right\|_{2}^{2}+\ 2\cdot\underbrace{\sum\limits_{a}^{d}x_{i}\left(\sum \limits_{j=1,\,j\neq i}^{\rho}\frac{x_{j}y_{j}}{y_{i}}\right)}_{\alpha}+\underbrace{ \sum\limits_{\begin{subarray}{c}j=1,\,j\neq i\\ j\neq i\end{subarray}}^{d}\sum\limits_{l=1}^{\rho-1}\frac{x_{j}y_{j}}{y_{i}} \cdot\frac{x_{l}y_{l}}{y_{i}}}_{\beta}\] (8)

\[=\mu^{2}d+(\rho-1)\cdot\mu^{2}d+2\alpha+2\beta\quad=\rho\cdot\mu^{2}d+2\alpha+2\beta\]

Therefore, using Equation 7 and Equation 8 we can write that

\[\mathrm{E}[\phi]=\frac{\mu^{2}d+\alpha}{\sqrt{\mu^{2}d}\cdot\sqrt{\rho\cdot\mu^{2 }d+2\alpha+2\beta}}\approx\,^{1}\frac{\mu^{2}d}{\sqrt{\mu^{2}d}\cdot\sqrt{\rho \cdot\mu^{2}d}}=\frac{\mu^{2}d}{\sqrt{\rho}\cdot\mu^{2}d}=\frac{1}{\sqrt{\rho}}\qqed\]The experimental result of the \(\phi-\rho\) relationship closely follows the theoretical expectation provided in Appendix C which also indicates that the approximation is valid. Since, we know from Theorem 3.2 that similarity score \(\phi\) drops by the inverse square root of the number of vector pairs in a composite representation \(\rho\), in places where \(\rho\) is known or can be estimated from \(\left\|\chi_{\rho}\right\|_{2}\approx\mu^{2}\sqrt{\rho\cdot d}\) (proof in Appendix B), it can be used to update the cosine similarity multiplying the scores by \(\sqrt{\rho}\). Equation 9 shows the updated similarity score where in a positive case \((+)\), \(\phi\) would be close to \(1/\sqrt{\rho}\) and in a negative case \((-)\), \(\phi\) would be close to zero.

\[\phi^{\prime}=\phi\times\sqrt{\rho}\qquad\phi^{\prime}_{(+)}=\phi_{-\frac{1}{ \sqrt{\rho}}}\times\sqrt{\rho}\ \approx 1\qquad\phi^{\prime}_{(-)}=\phi_{\to 0} \times\sqrt{\rho}\quad\approx 0\] (9)

Empirical results of \(\phi^{\prime}\) for varying \(n\) and \(\rho\) are visualized and verified by a heatmap. In a composite representation \(\chi^{\prime}_{\rho}=\sum_{i=1}^{\rho}(x_{i}\odot y_{i})\), when unbinding is applied using the query \(y_{i}\), i.e., \(\mathcal{B^{*}}^{\prime}(\chi^{\prime}_{\rho},y_{i})=\hat{x_{i}}\), a positive case is a similarity between \(x_{i}\) and \(\hat{x_{i}}\). On the contrary, similarity between \(\hat{x_{i}}\) and any \(x_{j}\) where \(j\in\{1,2,\cdots,\rho\}\) and \(j\neq i\), is a negative case. Mean cosine similarity scores of \(100\) trials for both positive and negative cases in presented in Figure 1 where the scores for the positive cases are in the \(\mathsf{red}\)\((\approx 1)\) shades and the scores for the negative cases are in the blue \((\approx 0)\) shades.

## 4 Empirical Results

### Classical VSA Tasks

A common VSA task is, given a bundle (addition) of \(\rho\) pairs of bound vectors \(\bm{s}=\sum_{i=1}^{\rho}\mathcal{B}(\bm{x}_{i},\bm{y}_{i})\), given a query \(\bm{x}_{q}\in\bm{s}\), \(\mathsf{c}\) can the corresponding vector \(\bm{y}_{q}\) be correctly retrieved from the bundle. To test this, we perform an experiment similar to one in [37]. We first create a pool \(P\) of \(N=1000\) random vectors, then sample (with replacement) \(p\) pairs of vectors for \(p\in\{1,2,\cdots,25\}\). The pairs are bound together and added to create a composite representation \(\bm{s}\). Then, we iterate through all left pairs \(\bm{x}_{q}\) in the composite representation and attempt to retrieve the corresponding \(\bm{y}_{q},\forall q\in[1,p]\). A retrieval is considered correct if \(\mathcal{B^{*}}(\bm{s},\bm{x}_{q})^{\top}\bm{y}_{q}>\mathcal{B^{*}}(\bm{s}, \bm{x}_{q})^{\top}\bm{y}_{j},\forall j\neq q\). The total accuracy score for the bundle is recorded, and the experiment is repeated for \(50\) trials. Experiments are performed to

Figure 1: Empirical comparison of the corrected cosine similarity scores between \(\phi^{\prime}_{(+)}\) (on top) and \(\phi^{\prime}_{(-)}\) (on bottom) for varying \(n\) and \(\rho\) shown in heatmap. The dimension, i.e., \(d=2^{n}\) is varied from \(2\) to \(1024\)\((n\in\{1,2,\cdots,10\})\) and the number of vector pairs bundled is varied from \(1\) to \(50\). This shows that we can accurately identify when a vector \(\bm{x}\) has been bound to a VSA or not when we keep track of how many pairs of terms \(\rho\) are included.

compare HRR [32], VTB [12], MAP [10], and our HLB VSAs. For each VSA, at each dimension of the vector, the area under the curve (AUC) of the accuracy vs. the no. of bound terms plot is computed, and the results are shown in Figure 2. In general, HLB has comparable performance to HRR and VTB, and performs better than MAP.

The scenario we just considered looked at bindings of only two items together, summed of many pairs of bindings. [12] proposed addition evaluations over sequential bindings that we now consider. In the _random_ case we have an initial vector \(\bm{b}_{0}\), and for \(p\) rounds, we will modify it by a random vector \(\bm{x}_{t}\) such that \(\bm{b}_{t+1}=\mathcal{B}(\bm{b}_{t},\bm{x}_{t})\), after which we unbind each \(\bm{x}_{t}\) to see how well the previous \(\bm{b}_{t}\) is recovered. In the _auto binding_ case, we use a single random vector \(\bm{x}\) for all \(p\) rounds.

In this task, we are concerned with the quality of the similarity score in random/auto-binding, as we want \(\mathcal{B}^{\star}(\bm{b}_{t+1},\bm{x}_{t})^{\top}\bm{b}_{t}=1\). For VSAs with approximate unbinding procedures, such as HRR, VTB, and MAP-C, the returned value will be 1 if \(p=1\) but will decay as \(p\) increases. HLBuses an exact unbinding procedure so that the returned value is expected to be 1 \(\forall\)\(p\). We are also interested in the magnitude of the vectors \(\|\mathcal{B}^{\star}(\bm{b}_{t+1},\bm{x}_{t})\|_{2}\), where an ideal VSA has a constant magnitude that does not explode/vanish as \(p\) increases.

Figure 3 shows that HLB maintains a stable magnitude regardless of the number of bound vectors in both cases. This property arises due to the properties of the distribution shown in Properties 3.1. As all components have an expected absolute value of 1, the product of all components also has an expected absolute value of 1. Thus, the norm of the binding is simply \(\sqrt{d}\). It also shows HLB maintains the desired similarity score as \(p\) increases. Combined with Figure 1 that shows the scores are near-zero when an item is not present, HLB has significant advantages in consistency for designing VSA solutions.

Figure 3: When repeatedly binding different random (left) or a single vector (right), HLB consistently returns the ideal similarity score of 1 for a present item (top row) and has a constant magnitude (bottom row), avoiding exploding/vanishing values.

Figure 2: The area under the accuracy curve due to the change of no. of bundled pairs \(\rho\) for dimensions \(d\). All the dimensions are chosen to be perfect squares due to the constraint of VTB.

### Deep Learning with Hadamard-derived Linear Binding

Two recent methods that integrate HRR with deep learning are tested to further validate our approach and briefly summarized in the two sub-sections below. In each case, we run all four VSAs and see that HLB either matches or exceeds the performance of other VSAs. In every experiment, the standard method of sampling vectors from each VSA is followed as outlined in Table 1. All the experiments are performed on a single NVIDIA TESLA PH402 GPU with \(32\)GB memory.

#### 4.2.1 Connectionist Symbolic Pseudo Secrets

When running on low-power computing environments, it is often desirable to offload the computation to a third-party cloud environment to get the answer faster and use fewer local resources. However, this may be problematic if one does not fully trust the available cloud environments. Homomorphic encryption (HE) is the ideal means to alleviate this problem, providing cryptography for computations. HE is currently more expensive to perform than running a neural network itself [11], defeating its own utility in this scenario. Connectionist Symbolic Pseudo Secrets (CSPS) [3] provides a heuristic means of obscuring the nature of the input (content), and output (number of classes/prediction), while also reducing the total local compute required.

CSPS mimics a "one-time-pad" by taking a random VSA vector \(\bm{s}\) as the _secret_ and binding it to the input \(\bm{x}\). The value \(\mathcal{B}(\bm{s},\bm{x})\) obscures the original \(\bm{x}\), and the third-party runs the bulk of the network on their platform. A result \(\bm{\tilde{y}}\) is returned, and a small local network computes the final answer after unbinding with the secret \(\mathcal{B}^{*}(\bm{\tilde{y}},\bm{s})\). Other than changing the VSA used, we follow the same training, testing, architecture size, and validation procedure of [3].

CSPS experimented with 5 datasets MNIST, SVHN, CIFAR-10 (CR10), CIFAR-100 (CR100), and Mini-ImageNet (MIN). First, we look at the accuracy of each method, which is lower due to the noise of the random vector \(\bm{s}\) added at test time since no secret VSA is ever reused. The results are shown in Table 2, where HLB outperforms all prior methods significantly. Notably, the MAP VSA is second best despite being one of the older VSAs, indicating its similarity to HLB in using a simple binding procedure, and thus simple gradient may be an important factor in this scenario.

However, improved accuracy is not useful in this scenario if more information is leaked. The test in this scenario, as proposed by [3], is to calculate the Adjusted Rand Index (ARI) after attempting to cluster the inputs \(\bm{x}\) and the outputs \(\bm{\hat{y}}\), which are available/visible to the snooping third-party. To be successful, the ARI must be near zero (indicating random label assignment) for both inputs and outputs.

We use K-means, Gaussian Mixture Model (GMM), Birch [45], and HDBSCAN [7] as the clustering algorithms and specify the true number of classes to each method to maximize attacker success (information they would not know). The results can be found in Table 3, where the top rows indicate the clustering of the input \(\mathcal{B}(\bm{x},\bm{s})\), and the bottom rows the clustering of the output \(\bm{\hat{y}}\). All the numbers are percentages \((\%)\), showing all methods do a good job at hiding information from the adversary (except on the MNIST dataset, which is routinely degenerate).

The MNIST result is a good reminder that CSPS security is heuristic, not guaranteed. Nevertheless, we see HLB has consistently close-to-zero scores for SVHN, CIFARs, and Mini-ImageNet, indicating

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline \multirow{2}{*}{Dataset} & Dims/ & CSPS + HRR & \multicolumn{2}{c}{CSPS + VTB} & CSPS + MAP-C & CSPS + MAP-B & CSPS + HLB \\ \cline{3-11}  & Labels & Top@1 & Top@5 & Top@1 & Top@5 & Top@1 & Top@5 & Top@1 & Top@5 & Top@1 & Top@5 \\ \hline MNIST & \(28^{2}/10\) & \(98.51\) & – & \(98.44\) & – & \(98.46\) & – & \(98.40\) & – & \(\bm{98.73}\) & – \\ SVHN & \(32^{2}/10\) & \(88.44\) & – & \(19.59\) & – & \(79.95\) & – & \(92.43\) & – & \(\bm{94.53}\) & – \\ CR10 & \(32^{2}/10\) & \(78.21\) & – & \(74.22\) & – & \(76.69\) & – & \(82.83\) & – & \(\bm{83.81}\) & – \\ CR100 & \(32^{2}/100\) & \(48.84\) & \(75.82\) & \(35.87\) & \(61.79\) & \(56.77\) & \(81.52\) & \(57.76\) & \(84.63\) & \(\bm{58.82}\) & \(\bm{87.50}\) \\ MIN & \(84^{2}/100\) & \(40.99\) & \(66.99\) & \(45.81\) & \(73.52\) & \(52.22\) & \(78.63\) & \(57.91\) & \(82.81\) & \(\bm{59.48}\) & \(\bm{83.35}\) \\ \hline GM & & \(67.14\) & \(71.26\) & \(47.24\) & \(67.40\) & \(70.89\) & \(80.06\) & \(75.90\) & \(83.72\) & \(\bm{77.17}\) & \(\bm{85.40}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Accuracy comparison of the proposed HLB with HRR, VTB, MAP-C, and MAP-B in CSPS. The dimensions of the inputs along with the no. of classes are listed in the Dims/Labels column. The last row shows the geometric mean of the results.

that its improved accuracy with simultaneously improved security. This also validates the use of the VSA in deep learning architecture design and the efficacy of our approach.

#### 4.2.2 Xtreme Multi-Label Classification

Extreme Multi-label (XML) is the scenario where, given a single input of size \(d\), \(C>>d\) classes are used to predict. This is common in e-commerce applications where new products need to be tagged, and an input on the order of \(d\approx 5000\) is relatively small compared to \(C\geq\) 100,000 or more classes. This imposes unique computational constraints due to the output space being larger than the input space and is generally only solvable because the output space is sparse -- often less than 100 relevant classes will be positive for any one input. VSAs have been applied to XML by exploiting the low positive class occurrence rate to represent the problem symbolically [9].

While many prior works focus on innovative strategies to cluster/make hierarchies/compress the penultimate layer[19, 20, 31, 18, 44, 22], a neuro-symbolic approach was proposed by [9]. Given \(K\) total possible classes, they assigned each class a vector \(\bm{c}_{k}\) to be each class's representation, and the set of all classes \(\bm{a}=\sum_{k=1}^{K}\bm{c}_{k}\).

The VSA trick used by [9] was to define an additional "present" class \(\bm{p}\) and a "missing" class \(\bm{m}\). Then, the target output of the network \(f(\cdot)\) is itself a vector composed of two parts added together. First, \(\mathcal{B}(\bm{p},\sum_{k}\bm{c}_{k})\) represents all _present_ classes, and so the sum is over a finite smaller set. Then the absent classes compute the _missing_ representing \(\mathcal{B}(\bm{m},\bm{a}-\sum_{k}\bm{c}_{k})\), which again only needs to compute over the finite set of present classes, yet represents the set of all non-present classes by exploiting the symbolic properties of the VSA.

For XML classification, we have a set of \(K\) classes that will be present for a given input, where \(K\approx 10\) is the norm. Yet, there will be \(L\) total possible classes where \(L\geq 100,000\) is quite common. Forming a normal linear layer to produce \(L\) outputs is the majority of computational work and memory use in standard XML models, and thus the target for reduction. A VSA can be used to side-step this cost, as shown by [9], by leveraging the symbolic manipulation of the outputs. First, consider the target label as a vector \(\bm{s}\in\mathds{R}^{d}\) such that \(d\ll L\). By defining a VSA vector to represent "present" and "missing" classes as \(\mathbf{p}\) and \(\mathbf{m}\), where each class is given its own vector \(\bm{c}_{1,\cdots,L}\), we can shift the computational complexity form \(\mathcal{O}(L)\) to \(\mathcal{O}(K)\) by manipulating the "missing" classes as the compliment of the present classes as shown in Equation 10.

\begin{table}
\begin{tabular}{l|r r r r r r r r r r} \hline \hline \multirow{2}{*}{Clustering Methods} & \multicolumn{5}{c}{HRR} & \multicolumn{5}{c}{VTB} \\ \cline{2-11}  & MNIST & SVHN & CR10 & CR100 & MIN & MNIST & SVHN & CR10 & CR100 & MIN \\ \hline K-Means & \(-0.02\) & \(-0.01\) & \(0.18\) & \(0.54\) & \(0.42\) & \(-0.00\) & \(-0.01\) & \(-0.01\) & \(0.02\) & \(0.00\) \\ GMM & \(0.01\) & \(0.00\) & \(0.09\) & \(0.61\) & \(0.44\) & \(4.67\) & \(1.37\) & \(-0.01\) & \(0.02\) & \(0.01\) \\ Brach & \(0.20\) & \(0.00\) & \(0.14\) & \(0.45\) & \(0.35\) & \(0.02\) & \(0.03\) & \(0.04\) & \(0.08\) & \(0.03\) \\ HDBSCAN & \(0.00\) & \(-0.24\) & \(1.23\) & \(0.01\) & \(0.02\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) \\ \hline K-Means & \(1.28\) & \(0.06\) & \(0.21\) & \(0.03\) & \(0.08\) & \(8.52\) & \(0.13\) & \(1.11\) & \(0.05\) & \(0.12\) \\ GMM & \(1.28\) & \(0.06\) & \(0.17\) & \(0.04\) & \(0.09\) & \(8.63\) & \(0.14\) & \(1.63\) & \(0.05\) & \(0.00\) \\ Brach & \(1.51\) & \(0.03\) & \(0.13\) & \(0.05\) & \(0.07\) & \(3.24\) & \(0.00\) & \(0.64\) & \(0.06\) & \(0.17\) \\ HDBSCAN & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.09\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) \\ \hline \hline \multicolumn{11}{c}{Clustering Methods} & \multicolumn{5}{c}{MLP} & \multicolumn{5}{c}{HBL} \\ \cline{2-11}  & MNIST & SVHN & CR10 & CR100 & MIN & MNIST & SVHN & CR10 & CR100 & MIN \\ \hline K-Means & \(0.17\) & \(0.01\) & \(0.01\) & \(0.00\) & \(0.00\) & \(0.09\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) \\ GMM & \(3.39\) & \(-0.01\) & \(0.01\) & \(0.00\) & \(0.00\) & \(2.53\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) \\ Biech & \(0.84\) & \(-0.00\) & \(0.00\) & \(0.01\) & \(0.00\) & \(0.83\) & \(0.00\) & \(0.00\) & \(0.01\) & \(0.00\) \\ HDBSCAN & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) \\ \hline K-Means & \(15.91\) & \(0.09\) & \(0.00\) & \(0.03\) & \(0.01\) & \(13.07\) & \(-0.04\) & \(0.01\) & \(0.02\) & \(-0.00\) \\ GMM & \(42.43\) & \(0.11\) & \(0.00\) & \(0.03\) & \(0.00\) & \(14.96\) & \(-0.04\) & \(0.01\) & \(0.02\) & \(0.00\) \\ Biech & \(7.09\) & \(-0.07\) & \(-0.02\) & \(0.01\) & \(-0.00\) & \(18.44\) & \(-0.07\) & \(0.00\) & \(0.01\) & \(0.02\) & \(0.02\) \\ HDBSCAN & \(0.48\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) & \(7.60\) & \(0.01\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Clustering results of the main network inputs (top rows) and outputs (bottom rows) in terms of Adjusted Rand Index (ARI). Because CSPS is trying to hide information, scores near zero are better. Cell color corresponds to the cell absolute value, with blue indicating lower ARI and red indicating higher ARI. All numbers in percentages, and show HLB is better at information hiding.

[MISSING_PAGE_EMPTY:10]

## References

* [1] Mohammad Mahmudul Alam, Edward Raff, Stella Biderman, Tim Oates, and James Holt. Recasting self-attention with holographic reduced representations. In _Proceedings of the 40th International Conference on Machine Learning_, ICML'23. JMLR.org, 2023.
* [2] Mohammad Mahmudul Alam, Edward Raff, and Tim Oates. Towards generalization in subitizing with neuro-symbolic loss using holographic reduced representations. _Neuro-Symbolic Learning and Reasoning in the era of Large Language Models_, 2023.
* [3] Mohammad Mahmudul Alam, Edward Raff, Tim Oates, and James Holt. Deploying convolutional networks on untrusted platforms using 2D holographic reduced representations. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 367-393. PMLR, 17-23 Jul 2022.
* [4] K. Bhatia, K. Dahiya, H. Jain, P. Kar, A. Mittal, Y. Prabhu, and M. Varma. The extreme classification repository: Multi-label datasets and code, 2016.
* [5] Peter Blouw and Chris Eliasmith. A neurally plausible encoding of word order information into a semantic vector space. _35th Annual Conference of the Cognitive Science Society_, 35:1905-1910, 2013.
* [6] Peter Blouw, Eugene Solodkin, Paul Thagard, and Chris Eliasmith. Concepts as semantic pointers: A framework and computational model. _Cognitive Science_, 40(5):1128-1162, 7 2016.
* [7] Ricardo JGB Campello, Davoud Moulavi, and Jorg Sander. Density-based clustering based on hierarchical density estimates. In _Pacific-Asia conference on knowledge discovery and data mining_, pages 160-172. Springer, 2013.
* [8] C. Eliasmith, T. C. Stewart, X. Choo, T. Bekolay, T. DeWolf, Y. Tang, and D. Rasmussen. A large-scale model of the functioning brain. _Science_, 338(6111):1202-1205, 11 2012.
* [9] Ashwinkumar Ganesan, Hang Gao, Sunil Gandhi, Edward Raff, Tim Oates, James Holt, and Mark McLean. Learning with holographic reduced representations. _Advances in neural information processing systems_, 34:25606-25620, 2021.
* [10] Ross W. Gayler. Multiplicative binding, representation operators & analogy (workshop poster), 1998.
* [11] Ran Gilad-Bachrach, Nathan Dowlin, Kim Laine, Kristin Lauter, Michael Naehrig, and John Wernsing. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy. In _International conference on machine learning_, pages 201-210. PMLR, 2016.
* [12] Jan Gosmann and Chris Eliasmith. Vector-derived transformation binding: An improved binding operation for deep symbol-like processing in neural networks. _Neural Comput._, 31(5):849-869, 2019.
* [13] Klaus Greff, Sjoerd van Steenkiste, and Jurgen Schmidhuber. On the binding problem in artificial neural networks. _arXiv_, 2020.
* [14] Robert Guirado, Abbas Rahimi, Geethan Karunaratne, Eduard Alarcon, Abu Sebastian, and Sergi Abadal. Whype: A scale-out architecture with wireless over-the-air majority for scalable in-memory hyperdimensional computing. _IEEE Journal on Emerging and Selected Topics in Circuits and Systems_, 13(1):137-149, 2023.
* [15] Mike Heddes, Igor Nunes, Pere VergAos, Denis Kleyko, Danny Abraham, Tony Givargis, Alexandru Nicolau, and Alexander Veidenbaum. Torchhd: An open source python library to support research on hyperdimensional computing and vector symbolic architectures. _Journal of Machine Learning Research_, 24(255):1-10, 2023.

* [16] Qiuyuan Huang, Paul Smolensky, Xiaodong He, Li Deng, and Dapeng Wu. Tensor product generation networks for deep nlp modeling. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_, pages 1263-1273. Association for Computational Linguistics, jun 2018.
* [17] Mohsen Imani, Deqian Kong, Abbas Rahimi, and Tajana Rosing. Voicehd: Hyperdimensional computing for efficient speech recognition. In _2017 IEEE International Conference on Rebooting Computing (ICRC)_, pages 1-8. IEEE, nov 2017.
* [18] Himanshu Jain, Venkatesh Balasubramanian, Bhanu Chunduri, and Manik Varma. Slice: Scalable linear extreme classifiers trained on 100 million labels for related searches. In _Proceedings of the twelfth ACM international conference on web search and data mining_, pages 528-536, 2019.
* [19] Himanshu Jain, Yashoteja Prabhu, and Manik Varma. Extreme multi-label loss functions for recommendation, tagging, ranking & other missing label applications. In _Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining_, pages 935-944, 2016.
* [20] Ankit Jalan and Purushottam Kar. Accelerating extreme classification via adaptive feature agglomeration. _arXiv preprint arXiv:1905.11769_, 2019.
* [21] Michael N. Jones and Douglas J.K. Mewhort. Representing word meaning and order information in a composite holographic lexicon. _Psychological Review_, 114(1):1-37, 2007.
* [22] Armand Joulin, Moustapha Cisse, David Grangier, Herve Jegou, et al. Efficient softmax approximation for gpus. In _International conference on machine learning_, pages 1302-1310. PMLR, 2017.
* [23] Denis Kleyko, Mike Davies, Edward Paxon Frady, Pentti Kanerva, Spencer J. Kent, Bruno A. Olshausen, Evgeny Osipov, Jan M. Rabaey, Dmitri A. Rachkovskij, Abbas Rahimi, and Friedrich T. Sommer. Vector symbolic architectures as a computing framework for emerging hardware. _Proceedings of the IEEE_, 110(10):1538-1571, 2022.
* [24] Denis Kleyko, Dmitri Rachkovskij, Evgeny Osipov, and Abbas Rahimi. A survey on hyperdimensional computing aka vector symbolic architectures, part ii: Applications, cognitive models, and challenges. _ACM Comput. Surv._, 55(9), jan 2023.
* [25] Denis Kleyko, Dmitri A. Rachkovskij, Evgeny Osipov, and Abbas Rahimi. A survey on hyperdimensional computing aka vector symbolic architectures, part i: Models and data transformations. _ACM Comput. Surv._, 55(6), dec 2022.
* [26] Kunz. On the equivalence between one-dimensional discrete walsh-hadamard and multidimensional discrete fourier transforms. _IEEE Transactions on Computers_, 100(3):267-268, 1979.
* [27] Siyu Liao and Bo Yuan. Circconv: A structured convolution with low complexity. _Proceedings of the AAAI Conference on Artificial Intelligence_, 33(01):4287-4294, Jul. 2019.
* [28] Mohammad Mahmudul Alam, Edward Raff, Stella R Biderman, Tim Oates, and James Holt. Holographic global convolutional networks for long-range prediction tasks in malware detection. In _Proceedings of The 27th International Conference on Artificial Intelligence and Statistics_, volume 238 of _Proceedings of Machine Learning Research_, pages 4042-4050. PMLR, 02-04 May 2024.
* [29] Nicolas Menet, Michael Hersche, Geethan Karunaratne, Luca Benini, Abu Sebastian, and Abbas Rahimi. Mimonets: Multiple-input-multiple-output neural networks exploiting computation in superposition. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 39553-39565. Curran Associates, Inc., 2023.

* Neubert et al. [2016] Peer Neubert, Stefan Schubert, and Peter Protzel. Learning vector symbolic architectures for reactive robot behaviours. In _Proc. of Intl. Conf. on Intelligent Robots and Systems (IROS) Workshop on Machine Learning Methods for High-Level Cognitive Capabilities in Robotics_, 2016.
* Niculescu-Mizil and Abbasnejad [2017] Alexandru Niculescu-Mizil and Ehsan Abbasnejad. Label filters for large scale multilabel classification. In _Artificial intelligence and statistics_, pages 1448-1457. PMLR, 2017.
* Plate [1995] Tony A Plate. Holographic reduced representations. _IEEE Transactions on Neural networks_, 6(3):623-641, 1995.
* Understanding Deep Learning Through Empirical Falsification" at NeurIPS 2022 Workshops_, volume 187 of _Proceedings of Machine Learning Research_, pages 1-11. PMLR, 03 Dec 2023.
* Schlag et al. [2021] Imanol Schlag, Kazuki Irie, and Jurgen Schmidhuber. Linear transformers are secretly fast weight programmers. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 9355-9366. PMLR, 2021.
* Schlag and Schmidhuber [2018] Imanol Schlag and Jurgen Schmidhuber. Learning to reason with third order tensor products. In _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* Schlegel et al. [2020] Kenny Schlegel, Peer Neubert, and Peter Protzel. A comparison of vector symbolic architectures. _arXiv_, 2020.
* Schlegel et al. [2022] Kenny Schlegel, Peer Neubert, and Peter Protzel. A comparison of vector symbolic architectures. _Artificial Intelligence Review_, 55(6):4523-4555, Aug 2022.
* Smolensky [1990] Paul Smolensky. Tensor product variable binding and the representation of symbolic structures in connectionist systems. _Artificial Intelligence_, 46(1):159-216, 1990.
* Steinberg and Sompolinsky [2022] Julia Steinberg and Haim Sompolinsky. Associative memory of structured knowledge. _Scientific Reports_, 12(1), December 2022.
* Stewart and Eliasmith [2014] Terrence C. Stewart and Chris Eliasmith. Large-scale synthesis of functional spiking neural circuits. _Proceedings of the IEEE_, 102(5):881-898, 2014.
* Tay et al. [2018] Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. Learning to attend via word-aspect associative fusion for aspect-based sentiment analysis. _Proceedings of the AAAI Conference on Artificial Intelligence_, 32(1), Apr. 2018.
* Vashishth et al. [2020] Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, Nilesh Agrawal, and Partha Talukdar. Interactice: Improving convolution-based knowledge graph embeddings by increasing feature interactions. In _Proceedings of the 34th AAAI Conference on Artificial Intelligence_, pages 3009-3016. AAAI Press, 2020.
* Yamaki et al. [2023] Ryosuke Yamaki, Tadahiro Taniguchi, and Daichi Mochihashi. Holographic CCG parsing. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 262-276, Toronto, Canada, July 2023. Association for Computational Linguistics.
* You et al. [2019] Ronghui You, Zihan Zhang, Zive Wang, Suyang Dai, Hiroshi Mamitsuka, and Shanfeng Zhu. Attentionxml: Label tree-based attention-aware deep model for high-performance extreme multi-label text classification. _Advances in neural information processing systems_, 32, 2019.
* Zhang et al. [1996] Tian Zhang, Raghu Ramakrishnan, and Miron Livny. Birch: an efficient data clustering method for very large databases. _ACM sigmod record_, 25(2):103-114, 1996.

Noise Decomposition

When a single vector pair is combined, one of the vector pairs can be exactly retrieved with the help of the other component and the inverse function, recalling the retrieved output does not contain any noise component for a single pair of vectors, i.e., \(\rho=1\). However, when more than one vector pairs are bundled, noise starts to accumulate. In this section, we will uncover the noise components accumulated with and without the projection to the inputs and analyze their impact on expectation. We first start with the noise component without the projection step \(\eta_{i}^{\circ}\).

\[\eta_{i}^{\circ}=\frac{1}{d}\cdot H(\frac{1}{Hy_{i}}\odot\sum_{ \begin{subarray}{c}j=1\\ j\neq i\end{subarray}}^{\rho}(Hx_{j}\odot Hy_{j})\] (12)

Let, set the value of \(n\) to be \(1\) thus, \(d=2^{n}=2\) and the number of vector pairs \(\rho=2\), i.e., \(\chi_{\rho=2}=\mathcal{B}(x_{1},y_{1})+\mathcal{B}(x_{2},y_{2})\). We want to retrieve \(x_{1}\) using the query \(y_{1}\), thereby, the expression of \(\eta_{i}^{\circ}\) is uncovered step by step for \(\rho=2\) shown in Equation 13.

\[\begin{split}\eta_{i}^{\circ}&=\frac{1}{d}\cdot H (\frac{1}{Hy_{1}}\odot(Hx_{2}\odot Hy_{2}))\\ &=\frac{1}{d}\cdot\sqrt{d}\cdot H\left(\begin{array}{cc}\frac{ 1}{y_{1}^{(0)}+y_{1}^{(1)}}&(x_{2}^{(0)}+x_{2}^{(1)})\cdot(y_{2}^{(0)}+y_{2}^{ (1)})\\ \frac{1}{y_{1}^{(0)}-y_{1}^{(1)}}&(x_{2}^{(0)}-x_{2}^{(1)})\cdot(y_{2}^{(0)}-y_ {2}^{(1)})\end{array}\right)\\ &=\frac{1}{d}\cdot d\cdot\left(\begin{array}{cc}(\frac{(x_{2}^{(0)}+x_{2}^{(1 )})\,(y_{2}^{(0)}+y_{2}^{(1)})\,(y_{1}^{(0)}-y_{1}^{(1)})+(x_{2}^{(0)}-x_{1}^{(1 )})\,(y_{2}^{(0)}-y_{2}^{(1)})\,(y_{1}^{(0)}+y_{1}^{(1)})}{(y_{1}^{(0)}+y_{1}^{ (1)})\,(y_{1}^{(0)}-y_{1}^{(1)})}\\ (\frac{(x_{2}^{(0)}+x_{2}^{(1)})\,(y_{2}^{(0)}+y_{2}^{(1)})\,(y_{1}^{(0)}-y_{1 }^{(1)})-(x_{2}^{(0)}-x_{2}^{(1)})\,(y_{2}^{(0)}-y_{2}^{(1)})\,(y_{1}^{(0)}+y_{ 1}^{(1)})}{(y_{1}^{(0)}+y_{1}^{(1)})\,(y_{1}^{(0)}-y_{1}^{(1)})}\end{array} \right)\\ &=\left(\begin{array}{cc}\frac{\varphi_{1}}{\prod_{k=1}^{d}(Hy_{1})_{k}}\\ \frac{-\varphi_{2}}{\prod_{k=1}^{d}(Hy_{1})_{k}}\end{array}\right)\\ &=\frac{\mathcal{P}(x_{2},y_{2},y_{1})}{\prod_{k=1}^{d}(Hy_{1})_{k}}\end{split}\] (13)

Here, \(\varphi_{k}\ \forall\ k\in\mathbb{N}:1\leq k\leq d\) are the polynomials comprises of \((x_{2},\ y_{2})\), and the query vector \(y_{1}\). \(\mathcal{P}\) is the vector of polynomials consisting of \(\varphi_{k}\). From the noise expression, we can observe that the numerator is a polynomial and the denominator is the product of all the elements of the Hadamard transformation of the query vector. This is true for any value of \(n\) and \(\rho\). Thus, in general, for any query \(y_{i}\) we can express \(\eta_{i}^{\circ}\) as shown in Equation 14.

\[\eta_{i}^{\circ}=\frac{\overset{\rho}{j=1,\ j\neq i}(x_{j},y_{j},y_{i})}{\prod _{k=1}^{d}(Hy_{i})_{k}}\] (14)

The noise accumulated after applying the projection to the inputs is quite straightforward as given in Equation 15.

\[\eta_{i}^{\pi}=\frac{\sum\limits_{j=1,\ j\neq i}^{\rho}(x_{j} \odot y_{j})}{y_{i}}\] (15)

Although the vectors \(x_{i},y_{i}\ \forall\ i\in\mathbb{N}:1\leq i\leq\rho\) are sampled from a MiND with an expected value of \(0\) given in Equation 6, the sample mean of \(x_{i}\) or \(y_{i}\) would be \(\hat{\mu}\approx 0\) but \(\hat{\mu}\neq 0\). Both the numerator of \(\eta_{i}^{\circ}\) and \(\eta_{i}^{\pi}\) are the polynomials thus the expected value would be very close to \(0\). However, the expected value of the denominator of \(\eta_{i}^{\circ}\) would be \(\operatorname{E}[\prod_{k=1}^{d}(Hy_{i})_{k}]=\prod_{k=1}^{d}\operatorname{E}[(Hy _{i})_{k}]=\hat{\mu}^{d}\) whereas the expected value of the denominator of \(\eta_{i}^{\pi}\) is \(\operatorname{E}[y_{i}]=\hat{\mu}\). Since, \(\hat{\mu}^{d}<\hat{\mu}\), hence, in expectation \(\eta_{i}^{\pi}<\eta_{i}^{\circ}\). This is also verified by an empirical study where \(n\), i.e., the dimension \(d=2^{n}\) is varied along with the no. of bound vector pairs \(\rho\) and the amount of absolute mean noise in retrieval is estimated.

Figure 4 shows the heatmap visualization of the noise for both \(\eta_{i}^{\circ}\) and \(\eta_{i}^{\pi}\) in natural log scale. The amount of noise accumulated without any projection to the inputs is much higher compared to the noise accumulation with the projection. For varying \(n\) and \(\rho\), the maximum amount of noise accumulated when projection is applied is \(7.18\) and without any projection, the maximum amount of noise is \(19.38\). Also, most of the heatmap of \(\eta_{i}^{\pi}\) remains in the blue region whereas as \(n\) and \(\rho\) increase, the heatmap of \(\eta_{i}^{\circ}\) moves towards the red region. Therefore, it is evident that the projection to the inputs diminishes the amount of accumulated noise with the retrieved output.

## Appendix B Norm Relation

**Theorem B.1** (\(\chi_{\rho}-\rho\) Relationship).: _Given \(x_{i},y_{i}\sim\Omega(\mu,1/d)\in\mathbb{R}^{d}\)\(\forall\)\(i\in\mathbb{N}:1\leq i\leq\rho\), the norm of the composite representation \(\chi_{\rho}\) is proportional to \(\sqrt{\rho}\) and approximately equal to the \(\mu^{2}\sqrt{\rho\cdot d}\)._

Proof of Theorem b.1.: Given \(\chi_{\rho}\) is the composite representation of the bound vectors, i.e., the summation of \(\rho\) no. of individual bound terms. First, let's compute the norm of the single bound term as shown in Equation 16.

\[\left\|\mathcal{B}(x_{i},y_{i})\right\|_{2} =\left\|x_{i}\cdot y_{i}\right\|_{2}\] \[=\sqrt{(x_{i}^{(1)}y_{i}^{(1)})^{2}+(x_{i}^{(2)}y_{i}^{(2)})^{2} +\cdots+(x_{i}^{(d)}y_{i}^{(d)})^{2}}\] \[=\sqrt{(\pm\mu^{2})^{2}+(\pm\mu^{2})^{2}+\cdots+(\pm\mu^{2})^{2}} \quad\left[E[x^{(1)}]\cdot E[y^{(1)}]=\pm\mu\cdot\pm\mu=\pm\mu^{2}\right]\] \[=\sqrt{\mu^{4}d}\] (16)

Figure 4: Heatmap of the empirical comparison of the noise components \(\eta_{i}^{\circ}\) and \(\eta_{i}^{\pi}\) for varying \(n\) and \(\rho\) shown in natural logarithm scale. The dimension, i.e., \(d=2^{n}\) is varied from \(2\) to \(1024\) (\(n\in\{1,2,\cdots,10\}\)) and the number of vector pairs bundled is varied from \(2\) to \(50\).

Now, let's expand and compute the square norm of the composite representation given in Equation 17.

\[\left\|\chi_{\rho}\right\|_{2}^{2} =\left|\mathcal{B}(x_{1},y_{1})+\mathcal{B}(x_{2},y_{2})+\cdots+ \mathcal{B}(x_{\rho},y_{\rho})\right\|_{2}^{2}\] \[=\left|\mathcal{B}(x_{1},y_{1})\right\|_{2}^{2}+\left\|\mathcal{B }(x_{2},y_{2})\right\|_{2}^{2}+\cdots+\left|\mathcal{B}(x_{\rho},y_{\rho}) \right\|_{2}^{2}\ +\ \xi\] \[\text{where }\xi\text{ is the rest of the terms of square expansion.}\] \[=\mu^{4}d+\mu^{4}d+\cdots+\mu^{4}d+\xi\] (17) \[=\rho\cdot\mu^{4}d+\xi\] \[\left\|\chi_{\rho}\right\|_{2} =\sqrt{\rho\cdot\mu^{4}d+\xi}\] \[\approx\sqrt{\rho\cdot\mu^{4}d}\quad\text{ [ \ \xi is the noise term and discarded to make an approximation ]}\] \[=\mu^{2}\sqrt{\rho\cdot d}\quad\square\]

Thus, given the composite representation and the mean of the MiND distribution, we can estimate the no. of bound terms bundled together by \(\rho\approx\left\|\chi_{\rho}\right\|_{2}^{2}/\mu^{4}d\). \(\square\)

Figure 5 shows the comparison between the theoretical relationship and actual experimental results where the norm of the composite representation is computed for \(\mu=0.5\) and \(\rho=\{1,2,\cdots,200\}\). The figure indicates that the theoretical relationship aligns with the experimental results. However, as the number of bundled pair increases, the variation in the norm increases. This is because of making the approximation by discarding \(\xi\) in Equation 17.

Figure 5: Comparison between the theoretical and experimental relationship of Theorem B.1. The norm of the composite representation of the bound vectors is computed for no. of bundled vectors from \(1\) to \(200\) of dimension \(d=1024\). The figure shows how the experimental value of the norm closely follows the theoretical relation between \(\left\|\chi_{\rho}\right\|_{2}\) and \(\rho\).

Cosine Relation

Theorem 3.2 shows how the cosine similarity \(\phi\) between the original \(x_{i}\) and retrieved vector \(\hat{x_{i}}\) is approximately equal to the inverse square root of the number of vector pairs in a composite representation \(\rho\). In this section, we will perform an empirical analysis of the theorem and compare it with the theoretical results. For \(\rho=\{1,2,\cdots,50\}\), similarity score \(\phi\) is calculated for vector dimension \(d=512\). Additionally, the theoretical cosine similarity score is also calculated using the value of \(\phi\) following the theorem. Figure 6 shows the comparison between the two results where the experimental result closely follows the theoretical result. The figure also shows the standard deviation for \(100\) trials, indicating a minute change from the actual value.

Figure 6: Comparison between the theoretical and experimental \(\phi-\rho\) relationship. Vectors of dimension \(d=512\) are combined and retrieved with a varied number of vectors from \(1\) to \(50\). The zoom portion shows how closely experimental results match with the theoretical conclusion.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The paper claims to present a new Hadamard-derived linear vector symbolic architecture in the abstract and introduction which accurately reflects the contribution and scope of the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Limitations are described in the paper. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: All the theoretical results and proofs are provided in the Methodology section. (See section 3) Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The experimental setup is based on previous papers. All the self-used parameters are discussed in the paper. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: All the code is provided with the supplemental material. Data is publicly available and instruction is provided on how to get the data. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Experimental details and their sources are cited in the Empirical results section. (see section 4) Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: See Figure 2, Figure 3 Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See section 4. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We followed the NeurIPS code of conduct. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The work has no negative societal impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: No high-risk data or models are used. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Original paper and code are cited in the paper. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Documentation of the code is provided. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No Crowdsourcing and Research with Human Subjects are used. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: No participants are used. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.