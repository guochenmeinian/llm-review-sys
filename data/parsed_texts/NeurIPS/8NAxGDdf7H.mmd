# Few-Shot Class-Incremental Learning via

Training-Free Prototype Calibration

 Qi-Wei Wang, Da-Wei Zhou, Yi-Kai Zhang, De-Chuan Zhan, Han-Jia Ye

State Key Laboratory for Novel Software Technology

Nanjing University, Nanjing, 210023, China

{wangqiwei,zhoudw,zhangyk,zhandc,yehj}@lamda.nju.edu.cn

Han-Jia Ye is the corresponding author.

###### Abstract

Real-world scenarios are usually accompanied by continuously appearing classes with scarce labeled samples, which require the machine learning model to incrementally learn new classes and maintain the knowledge of base classes. In this Few-Shot Class-Incremental Learning (FSCIL) scenario, existing methods either introduce extra learnable components or rely on a frozen feature extractor to mitigate catastrophic forgetting and overfitting problems. However, we find a tendency for existing methods to misclassify the samples of new classes into base classes, which leads to the poor performance of new classes. In other words, the strong discriminability of base classes distracts the classification of new classes. To figure out this intriguing phenomenon, we observe that although the feature extractor is only trained on base classes, it can surprisingly represent the _semantic similarity_ between the base and _unseen_ new classes. Building upon these analyses, we propose a _simple yet effective_ Training-frEE calibratioN (TEEN) strategy to enhance the discriminability of new classes by fusing the new prototypes (_i.e._, mean features of a class) with weighted base prototypes. In addition to standard benchmarks in FSCIL, TEEN demonstrates remarkable performance and consistent improvements over baseline methods in the few-shot learning scenario. Code is available at: https://github.com/wangkiw/TEEN

## 1 Introduction

Deep Neural Networks (_i.e._, DNNs) have achieved impressive success in various applications [16, 19, 36, 55, 54, 5], but they usually rely heavily on _static and pre-collected large-scale_ datasets (_e.g._, ImageNet [11]) to achieve this success. However, the data in real-world scenarios usually arrive continuously. For example, the face recognition system is required to authenticate existing users, and meanwhile, new users are continually added [30]. The scene where the model is required to continually learn new knowledge and maintain the ability on old tasks is referred to as _Class-Incremental Learning (CIL)_[41]. The main challenge of CIL is the notorious _catastrophic forgetting problem_[14], where the model forgets old knowledge as it learns new ones.

Many methods have been designed to overcome _catastrophic forgetting_ in CIL from different perspectives, _e.g._, knowledge distillation [35], parameter regularization [22, 53], and network expansion [50, 47, 46]. These methods require new tasks to contain _sufficient_ labeled data for supervised training. However, collecting enough labeled data in some scenarios is challenging, making the conventional CIL methods hard to deploy [30]. For example, the face recognition system can only collect very few facial images of a new user due to privacy reasons. Therefore, a more realistic and practical incremental learning paradigm, _Few-Shot Class-Incremental Learning (FSCIL)_[42], isproposed to address the problem of class-incremental learning with limited labeled data. Figure 0(a) gives a detailed illustration of FSCIL.

In addition to the _catastrophic forgetting problem_[14], FSCIL will suffer the _overfitting problem_ because the model can easily overfit the very few labeled data of new tasks. Previous works [57, 58, 61] have adopted prototype-based methods [40, 33] to conquer the limited data problem. These methods freeze the feature extractor trained on base classes when dealing with new classes and use the prototypes of new classes as the corresponding classifier weights. The frozen feature extractor can alleviate the _catastrophic forgetting problem_ and the prototype classifier can circumvent the _overfitting problem_. However, the inherent problem of learning with few-shot data is challenging to depict precisely the semantic information of a new category with limited data, making prototypes of new classes inevitably biased [27]. An intuitive reason for the biased prototypes is that the feature extractor has not been optimized for the new classes. For example, the feature extractor trained on "cat" and "dog" can not precisely depict the feature of a new class "bird" especially when the instances of "bird" are incredibly scarce 2. Many prototype adjustment methods [27, 57, 58, 15, 61, 1, 24] are dedicated to rectifying the biased representations of the new classes. These methods usually focus on designing complex pre-training algorithms to enhance the compatibility of representations [58, 59, 10], or complicated trainable modules that better adapt the representation of the new classes [57, 15, 61, 1], all of which rely on _significant training costs_ in exchange for improved model performance.

Footnote 2: “new class” and “novel class” in this paper mean the classes emerging after the base session.

However, we find that although existing methods perform well on the widely used performance measure (_i.e._, the average accuracy across all classes), _they usually exhibit poor performance on new classes_, which suggests that the calibration ability of existing methods could be improved. In other words, existing methods neglect the performance of new classes. A direct reason for this negligence is the current _unified_ performance measure is easily overwhelmed by the dominated base classes (_e.g._, 60 base classes in the CIFAR100 dataset). However, the more recent tasks are usually more crucial in some real-world applications. For example, the new users in the recommendation system are usually more important and need more attention [45]. The _important yet neglected_ new classes' performance inspires us to pay more attention to it.

To better understand the performance of current FSCIL methods [42, 6, 9, 58, 57], we explicitly evaluate and analyze the low performance of existing methods on _new classes_ and empirically demonstrate _the instances of new classes are prone to be predicted into base classes_. To further understand this intriguing phenomenon, we revisit the representation of the feature extractor and find that the feature extractor trained only on base classes can already represent the semantic similarity between the base and new classes well. As shown in Figure 0(b), although the novel classes are unavailable during the training of the feature extractor, the _semantic similarity_ between the base and novel classes can also be well-represented. However, existing methods have been obsessed with designing complex learning modules and disregard this off-the-shelf _semantic similarity_.

Figure 1: (a) The base session contains _sufficient_ samples of base classes for training. The incremental sessions (_i.e._, sessions after the base session) only contain _few-shot_ samples of new classes. FSCIL aims to obtain a unified classifier over all seen classes. Notably, the data from previous sessions are unavailable. (b) For example, considering the new classes “Plain”, “Plate” and “Porcupine” in CIFAR100 [23], the corresponding most similar base classes selected by the similarity can depict the really semantic similarity. The three most similar base classes are selected by computing the cosine similarity between base prototypes and novel prototypes. The results show _the feature extractor only trained on the base classes can also well represent the semantic similarity between the base and new classes_.

Based on the above analysis, we propose to leverage the overlooked semantic similarity to explicitly enhance the discriminability of new classes. Specifically, we propose a _simple yet effective_ Training-frEE prototype calibratioN (_i.e._, TEEN) strategy for biased prototypes of new classes by fusing the biased prototypes with weighted base prototypes, where the weights for the base prototypes are _class-specific and semantic-aware_. Notably, TEEN does not rely on any extra optimization procedure or model parameters once the feature extractor is trained on the base classes with a vanilla supervised optimization objective. Besides, the excellent calibration ability and training-free property make it a plug-and-play module.

Our contributions can be summarized as follows:

* We empirically find that the lower performance of new classes is due to misclassifying the samples of new classes into corresponding similar base classes, which is intriguing and missing in existing studies.
* We propose _a simple yet effective training-free_ calibration strategy for new prototypes, which not only achieved a higher average accuracy but also improved _the accuracy of new classes_ (**10.02% \(\sim\) 18.40% better than the runner-up FSCIL method**).
* We validate TEEN on benchmark datasets under the standard FSCIL scenario. Besides, TEEN shows competitive performance under the few-shot learning scenario. The consistent improvements demonstrate TEEN's remarkable calibration ability.

## 2 Related Works

### Class Incremental Learning

The model in the Class-Incremental Learning (_i.e._, CIL) scenario is required to learn new classes without forgetting old ones. Save representative instances in old tasks (_i.e._, exemplars) and replay them in new tasks is a simple and effective way to maintain the model's discriminability ability on old tasks [20; 2; 37]. Furthermore, knowledge distillation [17; 35; 25; 49] is widely used to maintain the knowledge of the old model by enforcing the output logits between the old model and the new one to be consistent. iCaRL [35] uses the knowledge distillation as a regularization item and replays the exemplars when learning the feature representation. Many methods follow this line and design more elaborate strategies to replay exemplars [29] and distill knowledge [21; 12]. Recently, model expansion [28; 50; 47; 46; 39; 60] have been confirmed to be effective in CIL. The most representative method [50] saves a single backbone and freezes it for each incremental task. The frozen backbone substantially alleviates the catastrophic forgetting problem.

### Few-Shot Learning

The model in Few-Shot Learning (_i.e._, FSL) [48] scenario is required to learn new classes with limited labeled data. Existing methods usually achieve this goal either from the perspective of optimization [13; 31; 3] or metric learning [40; 43; 26; 27; 56]. The core thought of optimization-based methods is to equip the model with the ability to fast adaptation with limited data. The metric-based methods focus on learning a unified and general distance measure to depict the semantic similarity between instances. Besides, [51] proposes to use Gaussian distribution to model each feature dimension of a specific class and sample augmented features from the calibrated distribution. Based on these augmented features, [51] train a logistic regression and achieve competitive performance. However, TEEN can outperform [51] in most settings and without any training cost when recognizing new classes.

### Few-Shot Class-Incremental Learning

The model in Few-Shot Class-Incremental Learning (_i.e._, FSCIL) [42] scenario is required to incrementally learn new knowledge with limited labeled data. Many existing methods are dedicated to designing learning modules to train a more powerful feature extractor [58] or adapting the representation of the instances of new classes [57; 9; 61; 1]. Besides, TOPIC [42] utilizes a neural gas network to alleviate the challenging problems in FSCIL. [8] adopts word embeddings as semantic information and introduces a distillation-based FSCIL method. IDLVQ [6] proposes to utilize quantized reference vectors to compress the old knowledge and improve the performance in FSCIL. However, all these methods overlook the abundant semantic information in base classes and the poor performance in new classes. In this study, we aim to take a small step toward filling this gap. The most related work to TEEN is [1]. However, it relies on optimizing a regularization-based objective function to implicitly utilize the semantic information. As opposed to [1], TEEN takes advantage of the empirical observation and gets rid of the optimization procedure and is thus more efficient and effective.

## 3 Preliminaries

### Definition and notations

In FSCIL [42], we assume there exists \(T\) sessions in total, including a base session (_i.e._, the first session) and \(T-1\) incremental sessions (_i.e._, sessions after the first session). We denote the training data in the base session as \(\mathcal{D}_{0}\) and the training data in the incremental sessions as \(\{\mathcal{D}_{1},\mathcal{D}_{2},\ldots,\mathcal{D}_{T-1}\}\). For the training data \(\mathcal{D}_{i}\) in the \(i\)-th session, we further notate it with \(\{(x_{j},y_{j})\}_{j=1}^{N}\) and corresponding label space with \(\mathcal{C}_{i}\). Note that only training data \(\mathcal{D}_{i}\) is available in the \(i\)-th session. Accordingly, the testing data and testing label space in session \(i\) can be denoted as \(\mathcal{D}_{i}^{test}\) and \(\mathcal{C}_{i}^{test}\). To better evaluate the model's discriminability on all seen tasks, the testing label space \(\mathcal{C}_{i}^{test}\) of \(i\)-th session contains all seen classes during training, _i.e._, \(\mathcal{C}_{i}^{test}=\bigcup_{j=0}^{i}\mathcal{C}_{j}\). An incremental session can also be denoted as a \(N\)-way \(K\)-shot classification task, _i.e._, \(N\) classes and \(K\) labeled examples for each class. Note that the training label spaces between different sessions are disjoint, _i.e._, for any \(i,j\in[0,T-1]\) and \(i\neq j,\mathcal{C}_{i}\bigcap\mathcal{C}_{j}=\varnothing\).

Compared to conventional CIL, FSCIL only requires the model to learn new classes with _limited_ labeled data. On the other hand, compared to conventional Few-Shot Learning (FSL), FSCIL requires the model to continually learn the knowledge of new classes while _retaining the knowledge of previously seen classes_. We introduce the related works on CIL, FSL and FSCIL in supplementary due to the space limitation.

The model in FSCIL can be decoupled into a feature encoder \(\phi_{\theta}(\cdot)\) with parameters \(\theta\) and a linear classifier \(W\). Given a sample \(x_{j}\in\mathbb{R}^{D}\), the feature of \(x_{j}\) can be denoted as \(\phi_{\theta}(x_{j})\in\mathbb{R}^{d}\). For a \(N\)-class classification task, the output logits of a sample \(x_{j}\) can be denoted as \(\mathcal{O}_{j}=W^{\top}\phi_{\theta}(x_{j})\in\mathbb{R}^{N}\) where \(W\in\mathbb{R}^{d\times N}\).

### Prototypical Network

ProtoNet [40] is a widely used method in few-shot learning problems. It computes the mean feature \(c_{k}\) of a class \(k\) (_e.g._, class prototype) and uses the class prototype to represent the corresponding class:

\[c_{k}=\frac{1}{\text{Num}_{k}}\sum_{y_{j}=k}\phi_{\theta}(x_{j})\] (1)

Num\({}_{k}\) denotes the number of samples in class \(k\). For a classification task with \(N\) classes, the classifier can be represented by the \(N\) prototypes, _i.e._, \(W=[c_{1},c_{2},\ldots,c_{N}]\). Following the [58; 57; 61; 1], we freeze the feature extractor \(\phi_{\theta}\) trained on the base task and plug the class prototype into the classifier while dealing with a new class. The frozen feature extractor can alleviate catastrophic forgetting and the plug-in updating of the classifier can circumvent the overfitting problem relatively.

## 4 A Closer Look at FSCIL

In this session, we comprehensively analyze current FSCIL methods from a _decoupled_ perspective. Although the previous updating paradigm of _extractor-frozen and prototypes-plugged_ can achieve adequate average accuracy in all classes, there also exist some shortcomings in it. In this section, we empirically show that the current methods (_e.g._, [57; 58]) are generally not effective in new classes. Furthermore, we take a step toward understanding the reason for the low performance in new classes.

### Understanding the reason for poor performance in new classes

To better understand the performance of existing methods, we first measure the performance by average accuracy on all classes, base classes and new classes, respectively. As illustrated in Figure 2,our first observation is that **the average accuracy across base classes is extremely higher than the accuracy in new classes**. The inconsistent performance between the base and new classes is caused by the frozen feature extractor and biased new prototypes. The former forces the model to _overfit_ base classes and the latter causes the model to _underfit_ the new classes.

To determine the real cause of the low performance on new classes, we further investigate "_What base classes are the new classes incorrectly predicted into?_" and get our second observation.

Our second observation is that **the prototype-based classifier misclassifies the new classes to their corresponding most similar base classes with high probability**, _i.e._, many instances of new classes are closer to their nearest base prototype than corresponding target prototypes

To verify this observation, we first analyze the detailed prediction results from a decoupled perspective. Specifically, we treat all base classes as a "positive class" and all new classes as a "negative class" and transform the FSCIL problem into a two-class classification task. We compute the false negative rate (_i.e._, **FNR**) and false positive rate (_i.e._, **FPR**) of each binary prediction task on each incremental session. The FNR and FPR in the confusion matrix is defined as follows:

\[\text{FNR}=\frac{\text{FN}}{\text{TP}+\text{FN}}\times 100\%,\text{ FPR}=\frac{\text{FP}}{\text{FP}+\text{TN}}\times 100\%\] (2)

As shown in Table 1, the FPR is far greater than FNR illustrating new classes are generally misclassified into base classes but the base classes are generally misclassified into base classes. On the basis of this conclusion, we further explore the details of misclassification. To better illustrate the analysis, we define "misclassified **To** most similar **B**ase classes **R**atio" (_i.e._, **TBR** ) for new classes and "misclassified **To** most similar **N**ew classes **R**atio" (_i.e._, **TNR** ) for base classes. Specifically, considering the misclassified instances in base classes and new classes respectively, the TBR and

\begin{table}
\begin{tabular}{c|c c|c c|c c|c c|c c|c c|c c} \hline \hline Session & 1 & & 2 & 3 & 4 & 5 & & 6 & 7 & & 8 \\ \hline
**FNR/FPR** & FNR & FPR & FNR & FPR & FNR & FPR & FNR & FPR & FNR & FPR & FNR & FPR & FNR & FPR \\ \hline ProtoNet [40] & 2.13 67.40 & 4.42 67.40 & 6.68 60.67 & 8.28 & 58.90 & 10.25 56.68 & 11.70 54.47 & 12.57 51.66 & 13.78 51.80 & \\ CEC [57] & 2.32 70.40 & 4.38 66.20 & 6.18 62.20 & 7.50 58.65 & 9.72 56.00 & 11.30 53.60 & 12.12 51.40 & 13.48 51.78 & \\ FACT [58] & 2.05 66.60 & 3.88 61.70 & 5.58 56.80 & 7.23 55.05 & 8.85 53.64 & 9.83 51.13 & 10.45 48.83 & 11.75 49.27 & \\ \hline TEEN & 4.03 **57.40** & 7.40 **52.50** & 9.35 **45.00** & 11.58 **40.75** & 14.00 **40.80** & 15.78 **39.23** & 16.33 **36.91** & 18.75 **36.65** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Detailed prediction results of **False Negative Rate/False Positive Rate** (%) on CIFAR100 [23] dataset. The analysis results are from session 1 because new classes do not exist in session 0. Exceedingly high **FPR** and relatively low **FNR** show the instances of new classes are easily misclassified into base classes and the instances of base classes are also easily misclassified into base classes. TEEN can achieve relatively lower **FPR** than baseline methods, which demonstrates the validity of the proposed calibration strategy. Please refer to the supplementary for results on _mini_ImageNet and CUB200.

\begin{table}
\begin{tabular}{c|c c|c c|c c|c c|c c|c c|c c} \hline \hline Session & 1 & & 2 & 3 & 4 & 5 & 6 & 7 & & 8 \\ \hline
**TNR/TBR** & TNR & TBR & TNR & TBR & TNR & TBR & TNR & TBR & TNR & TBR & TNR & TBR & TNR & TBR \\ \hline ProtoNet [40] & 3.47 & 73.01 & 5.02 & 61.79 & 7.06 & 55.05 & 8.08 & 52.95 & 7.14 & 53.27 & 8.56 & 51.96 & 8.53 & 49.18 & 9.21 & 50.91 \\ CEC [57] & 2.87 & 71.70 & 4.53 & 61.08 & 5.66 & 58.26 & 6.35 & 55.16 & 6.07 & 54.12 & 6.92 & 52.53 & 8.24 & 49.96 & 8.20 & 51.64 \\ FACT [58] & 2.33 & 70.00 & 3.32 & 61.01 & 5.50 & 55.17 & 6.88 & 50.55 & 6.57 & 50.52 & 7.72 & 49.30 & 8.88 & 46.99 & 9.31 & 48.48 \\ \hline TEEN & 2.16 & **66.77** & 2.53 & **55.14** & 3.55 & **44.87** & 3.85 & **37.68** & 3.57 & **39.47** & 4.02 & **37.07** & 4.76 & **33.83** & 4.85 & **35.04** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Detailed prediction results of **TNR/TBR** (%) on CIFAR100 [23] dataset. The analysis results are from session 1 because new classes do not exist in session 0. For new classes, we only consider the 10 most similar base classes out of 60 base classes. For base classes, we suppose \(C_{i}\) new classes exist in the current incremental session \(i\). We only consider the most similar \([20\%\times C_{i}]\) new classes. Class similarity adopts cosine similarity between different class prototypes. TEEN can achieve relatively lower **TBR** than baseline methods, which demonstrates the validity of the proposed calibration strategy. Please refer to the supplementary for results on _mini_ImageNet and CUB200.

Figure 2: The performance of FACT [58] on CIFAR100 dataset. **NAcc,BAcc,AvgAcc** mean the average accuracy on _new classes, base classes, and all classes_, respectively. _and all classes_, respectively.

TNR are defined as follows:

\[\text{TBR}=\frac{M_{b}}{N_{n}}\times 100\%,\text{ TNR}=\frac{M_{n}}{N_{b}}\times 100\%\] (3)

\(N_{n}\) means the number of misclassified instances of new classes. \(M_{b}\) is the number of instances misclassified into most similar base classes in \(N_{n}\) samples. \(M_{n}\) and \(N_{b}\) have similar meaning. As shown in Table 2, the TBR is higher and the TNR is relatively low, which strongly underpins our second observation. To our knowledge, we are the first to explain the reason for the low performance of new classes in existing methods and observe that the samples of new classes are easily misclassified into the most similar base classes.

To summarize, we empirically demonstrate that existing methods perform poorly on new classes and find that this is because the model tends to misclassify new class samples into the most similar base classes. Combining existing analysis (_i.e._, _the samples of new classes tend to be classified into the most similar base classes_) and observations (_i.e._, _the well-trained feature extractor on base classes can also well represent the semantic similarity between the base and new classes_), we propose to use the frozen feature extractor as a _bridge_ and calibrate the biased prototypes of new classes by fusing the biased new prototypes with the well-calibrated base prototypes.

## 5 Similarity-based Prototype Calibration

Based on the common assumptions in FSCIL [42], sufficient instances of base classes are available during the training of the feature extractor. We argue the _sufficient_ data in the base session contains abundant semantic information (_i.e._, a _sufficient_ number of classes) and the base prototypes are well-calibrated (_i.e._, a _sufficient_ number of instances for each class). The above analysis leads to a natural question:

_Can we leverage the well-calibrated prototypes in the base session for a new prototype calibration?_

As the previous methods overlook the low performance of new classes caused by the corresponding biased prototypes, we propose to _explicitly calibrate_ these biased prototypes with the help of the well-calibrated base prototypes. The off-the-shelf semantic similarity serves as a bridge between the base prototypes and the new ones. In the following sections, we introduce the details of fusing the well-calibrated base prototypes and ill-calibrated new prototypes to calibrate the biased ones. Afterwards, we analyze the effects of this training-free prototype calibration on new classes and base classes, respectively.

### Fusing the biased prototypes with calibration item

We assume there exist \(B\) classes in the base session and \(C\) classes in an incremental session, _e.g._, \(B=60,C=5\) in the CIFAR100 dataset. Without loss of generality, we only consider the base session and the first incremental session for simplification. Other incremental sessions can be obtained in the same way. Following the notations in section 3.1, the empirical prototype of \(i\)-th class can

Figure 3: (a) The \(\bigcirc\) represents the samples of new classes and the \(\bigtriangleup\) represents the samples of base classes. The dotted lines between samples and prototypes (_i.e._, \(c_{n}c_{n}^{base},\bar{c}_{n}\)) represent the corresponding classification results. Blue and purple dotted lines represent samples that are correctly classified, and red dotted lines represent misclassification. The yellow circles (_i.e._, \(\mathbf{W}\)\(\rightarrow\)\(\mathbf{R}\) samples) and green triangles (_i.e._, \(\mathbf{R}\)\(\rightarrow\)\(\mathbf{W}\) samples) in the right figure are samples with prediction changes after calibration. (b) The detailed ratio (%) of base and new classes with regard to three types of samples(_i.e._, \(\mathbf{UC}\) samples, \(\mathbf{W}\)\(\rightarrow\)\(\mathbf{R}\) samples, \(\mathbf{R}\)\(\rightarrow\)\(\mathbf{W}\) samples). Only two incremental sessions (_i.e._, session 1 and session 2) of CIFAR100 are listed here for convenience. Please refer to the supplementary for results on more incremental sessions and more datasets.

be notated as \(c_{i}\). Therefore, the base prototypes are \(c_{b}(0\leq b\leq B-1)\) and the new prototypes are \(c_{n}(B\leq n\leq B+C-1)\). As the base session contains _sufficient_ classes and _sufficient_ samples for each class, the model trained on the base session can capture the distribution of base classes and obtain well-calibrated prototypes for base classes. Generally, the empirical prototype of base classes can be regarded as approximately consistent with the expected class representation. However, due to the limited data in incremental sessions, the empirical prototypes of the novel classes are considered to be severely biased.

Based on the analysis of prototypes, we only calibrate biased prototypes in incremental sessions. Given a new class prototype \(c_{n}(B\leq n\leq B+C-1)\), the calibrated new prototype \(\bar{c}_{n}\) can be notated by:

\[\bar{c}_{n}=(1-\alpha)c_{n}+\alpha\Delta c_{n}\] (4)

The calibration item \(\Delta c_{n}\) is a component of base prototypes. The hyperparameter \(\alpha\) controls the calibration strength of biased prototypes. Smaller \(\alpha\) means the calibrated prototype reflects more of the original biased prototype, while larger \(\alpha\) means the calibrated prototype heavily incorporates the base prototypes. Motivated by the observations in section 4.1, the similarity between well-calibrated base prototypes and ill-calibrated new prototypes contains auxiliary side information about the new classes. Therefore, we use weighted base prototypes to represent the calibration item \(\Delta c_{n}\) and enhance the discriminability of biased prototypes. Specifically, we compute the cosine similarity \(S_{b,n}\) between a new class prototype \(c_{n}\) and a base prototype \(c_{b}\):

\[S_{b,n}=\frac{c_{b}\cdot c_{n}}{\|c_{b}\|\cdot\|c_{n}\|}\cdot\tau\] (5)

where \(\tau\) (\(\tau>0\)) is the scaling hyperparameter controlling the weight distribution's sharpness. The weight of a new prototype \(c_{n}\) with respect to a base prototype \(c_{b}\) is the softmax results over all base prototypes:

\[w_{b,n}=\frac{e^{S_{b,n}}}{\sum_{i=0}^{B-1}e^{S_{i,n}}}\] (6)

Finally, the calibration of biased prototypes of new classes can be formulated as follows:

\[\bar{c}_{n}=(1-\alpha)\,c_{n}+\alpha\,\Delta c_{n}=(1-\alpha)\,c_{n}+\alpha\, \sum_{b=1}^{B-1}w_{b,n}c_{b}\] (7)

Notably, the above prototype rectification procedure is a _training-free_ calibration strategy because it does not introduce any learning component or training parameters. Figure 3a gives an intuitive description of the calibration effect.

### Effect of calibrated prototypes

Intuitively, \(w_{b,n}\) is larger when a new prototype \(c_{n}\) and base prototype \(c_{b}\) are more similar. Given a new prototype \(c_{n}\), we assume _the most similar base prototype_ with \(c_{n}\) is the base prototype \(c_{n}^{base}\). Therefore, given a new prototype \(c_{n}\), \(\sum_{b=1}^{B-1}w_{b,n}c_{b}\approx c_{n}^{base}\) when the scaling hyperparameter \(\tau\) is large enough. From the perspective of a biased prototype, a calibrated prototype \(\bar{c}_{n}\) will be aligned to its most similar base prototypes \(c_{n}\) with a proper \(\tau\).

To further comprehend the effect of TEEN on the predictions of base and new classes respectively, we define three types of test samples according to whether the prediction results change after TEEN: with unchanged predictions (_i.e._, **UC** samples), with prediction going from right to wrong (_i.e._, **R\(\rightarrow\)W** samples), with predictions going from wrong to right (_i.e._, **W\(\rightarrow\)R** samples). We analyze the specifics of these three types of samples in detail.

Intuitively, some calibrated prototypes of new classes are aligned to base prototypes and reduce the discriminability of base prototypes. Oppositely, aligning the biased prototype to most similar base prototypes can calibrate the prediction of new classes. As shown in Table 3b, we observe the **W\(\rightarrow\)R** samples are mainly from new classes and the **R\(\rightarrow\)W** samples are mainly from base classes. Besides, extensive comparison results on the benchmark datasets (_i.e._, Table 3 and Figure 4) show that the negative effect of TEEN is negligible due to the significance of TEEN.

## 6 Experiments

In this section, we first introduce the main experiment details of FSCIL in section 6.1, which include the implementation and performance detail. Subsequently, we introduce the FSL performance of TEEN in section 6.2, which consistently shows the effectiveness of TEEN. Finally, we evaluate TEEN by a comprehensive ablation study in section 6.3.

### Main experimental details of FSCIL

#### 6.1.1 Implementation Details

**Datasets and baseline details:** Following previous methods [42; 57; 58], we evaluate TEEN on CIFAR100 [23], CUB200-2011 [44], _mini_ImageNet [38]. We keep the dataset split consistent with existing methods [57; 58]. Notably, each benchmark dataset is divided into subsets containing nonoverlapping label space. For example, CIFAR100 is divided into 60 classes for the base session and the left 40 classes are divided into eight 5-way 5-shot few-shot classification tasks. To validate the performance of TEEN, we compare TEEN with popular CIL methods [35; 4; 18], FSL methods [26; 56; 43], and FSCIL methods [42; 57; 58]. Please refer to the supplementary material for more datasets and baseline details.

**Training details:** All experiments are conducted with PyTorch [32] on a single NVIDIA 3090. The training of the feature extractor uses vanilla cross-entropy loss as the objective function. It does not evolve any extra complex pretraining module [58; 59; 10; 57], thus making TEEN more efficient and elegant. In addition, we adopt the cosine similarity to measure the similarity between the instances and class prototypes. Following [42; 57; 58], we use ResNet20 [16] for CIFAR100, pre-trained ResNet18 [16] for CUB200 and randomly initialized ResNet18 [16] for _mini_ImageNet. All compared methods use _the same backbone network and initialization_ for a fair comparison. We set \(\alpha=0.5,\tau=16\) for _mini_ImageNet and CUB200, \(\alpha=0.1,\tau=16\) for CIFAR100. We train the feature extractor on CUB200 with a learning rate of 0.004, batch size of 128, and epochs of 400. Please refer to the supplementary for more training details on CIFAR100 and _mini_ImageNet.

Figure 4: Top-1 average accuracy on all seen classes in each incremental session. We annotate the performance gap between TEEN and the runner-up method by \(\uparrow\).

\begin{table}
\begin{tabular}{l c c c c c c c c c c c} \hline \hline \multicolumn{1}{c}{\multirow{2}{*}{Method}} & \multicolumn{8}{c}{Accuracy in each session (\%) \(\uparrow\)} & \multirow{2}{*}{PD \(\downarrow\)} & \multirow{2}{*}{\(\Delta\) PD} \\ \cline{2-2} \cline{5-12}  & 0 & 1 & & 2 & 3 & 4 & 5 & 6 & 7 & 8 & \\ \hline iCARL [35] & 61.31 & 46.32 & 42.94 & 37.63 & 30.49 & 24.00 & 20.89 & 18.80 & 17.21 & 44.10 & **+22.65** \\ EEIL [4] & 61.31 & 46.58 & 44.00 & 37.29 & 33.14 & 27.12 & 24.10 & 21.57 & 19.58 & 41.73 & **+20.28** \\ Rebalancing [18] & 61.31 & 47.80 & 39.31 & 31.91 & 25.68 & 21.35 & 18.67 & 17.24 & 14.17 & 47.14 & **+25.69** \\ \hline TOPIC [42] & 61.31 & 50.09 & 45.17 & 41.16 & 37.48 & 35.52 & 32.19 & 29.46 & 24.42 & 36.89 & **+15.44** \\ Decoupled-NegCosine [26] & 71.68 & 66.64 & 62.57 & 58.82 & 55.91 & 52.88 & 49.41 & 47.50 & 45.81 & 25.87 & **+4.42** \\ Decoupled-Cosine [43] & 70.37 & 65.45 & 61.41 & 58.00 & 54.81 & 51.89 & 49.10 & 47.27 & 45.63 & 24.74 & **+3.29** \\ Decoupled-DecepEMD [56] & 69.77 & 64.59 & 60.21 & 56.63 & 53.16 & 50.13 & 47.79 & 45.42 & 43.41 & 26.36 & **+4.91** \\ CEC [57] & 72.00 & 66.83 & 62.97 & 59.43 & 56.70 & 53.73 & 51.19 & 49.24 & 47.63 & 24.37 & **+2.92** \\ FACT [58] & 72.56 & 69.63 & 66.38 & 62.77 & 60.6 & 57.33 & 54.34 & 52.16 & 50.49 & 22.07 & **+0.62** \\ \hline TEEN & **73.53** & **70.55** & **66.37** & **63.23** & **60.53** & **57.95** & **55.24** & **53.44** & **52.08** & **21.45** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Detailed average accuracy of each incremental session on _mini_ImageNet dataset. Please refer to the supplementary for results on CUB200 and CIFAR100. The results of compared methods are cited from [42; 57; 58]. \(\uparrow\) means higher accuracy is better. \(\downarrow\) means lower PD is better.

#### 6.1.2 Comparison results

In this section, we conduct overall comparison experiments on different performance measures. These different performance measures focus on different aspects of the methods. For example, the widely-used average accuracy across all classes (_i.e._, **AvgAcc**) is difficult to reflect the performance of new classes because the base classes take a large percentage of all classes (_e.g._, 60 base classes of 100 classes in CIFAR100). Following [58], the Harmonic mean (_i.e._, **HMean**) is used to evaluate the balanced performance between the base and new classes. Besides the above performance measures, we also additionally evaluate the different methods of their performance on new classes (_i.e._, **NAccc**). Following [42; 57; 58], we also measure the degree of forgetting by the **Per**formance **D**ropping Rate (_i.e._, **PD**). The PD is defined as \(PD=Acc_{0}-Acc_{-1}\), _i.e._, the average accuracy dropping between the first session (_i.e._, \(Acc_{0}\)) and the last session (_i.e._, \(Acc_{-1}\)). The detailed comparison results of **PD** and **AvgAcc** are reported in Table 3, and the detailed comparison results of **NAccc** and **HMean** are reported in Table 4. These experimental results from different performance measures all demonstrate the effectiveness of TEEN. Besides, significantly lower FPR and TBR in Table 1 and Table 2 also verify the effective calibration of TEEN.

Notably, many previous state-of-the-art FSCIL methods (_e.g._, [57; 58]) usually design complex pretraining algorithms to enhance the extendibility of feature space, which may harm the discriminability of base classes. This elaborate pretraining stage may lead to an inconsistent but _negligible_ performance in the base session. Besides, PD\(\downarrow\) and \(\Delta\)PD in Table 3 are not affected by the pretrained results and also show TEEN outperforms previous state-of-the-art methods.

### Comparison results of FSL

FSL can be approximated as measuring only the performance of the new classes in the first incremental session of FSCIL. Besides, FSL itself also faces the challenge of biased class prototypes due to the few-shot data. Therefore, we validate TEEN in the setting of FSL and report the compared results in Table 5. To ensure a fair comparison, we strictly followed the experimental setup of [51]. The comparison results in Table 5 show that TEEN can easily _outperform the previous state-of-the-art method_[51] in several experimental settings. Notably, TEEN _does not involve any additional training cost when recognizing new classes_, and the only time cost lies in feature extraction. Therefore, once the sample features are extracted, **the inference cost of** TEEN **can be considered negligible** compared to previous methods that require heavy training.

### Ablation Study

**The influence of \(\alpha\) and \(\tau\):** Notably, the proposed TEEN does not involve additional training-based modules or procedures after pretraining on base classes. When TEEN incrementally learns new classes, only the scaling temperature \(\tau\) and the coefficient of calibration item \(\alpha\) need to be determined.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c} \hline \hline Session & 1 & & 2 & & 3 & 4 & & 5 & 6 \\ \hline
**HMean/NAce** & HMean & NAcc & HMean & NAcc & HMean & NAcc & HMean & NAcc & HMean & NAcc \\ \hline CEC [57] & 30.72 & 19.60 & 30.05 & 19.10 & 29.86 & 19.00 & 29.41 & 18.65 & 27.15 & 16.88 & 27.36 & 17.07 \\ FACT [58] & 30.60 & 19.20 & 27.84 & 17.10 & 25.89 & 15.67 & 23.85 & 14.20 & 22.01 & 12.92 & 20.65 & 12.00 \\ \hline TEEN & **50.04** & **38.00** & **46.67** & **34.60** & **44.72** & **32.67** & **43.53** & **31.55** & **41.75** & **29.80** & **39.22** & **27.37** \\ \(\Delta\) & +19.32 & +18.4 & +16.62 & +15.5 & +14.86 & +13.67 & +14.12 & +12.9 & +14.6 & +12.92 & +11.86 & +10.3 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Detailed results of **HMean** and **NAccc** on _mini_ImageNet. The best results are in bold and the runner-up results are in underlines. The \(\Delta\) measures the performance gap between the best and second-best results on the corresponding session. Due to space limitations, the performance on only six incremental sessions is presented. Please refer to the supplementary for more detailed results on CUB200 and CIFAR100.

\begin{table}
\begin{tabular}{l|c c|c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{2}{c}{_mini_ImageNet} & \multicolumn{2}{c}{_CUB_} \\  & 5w1s & 5w5s & 5w1s & 5w5s \\ \hline ProtoNet [40] & 54.16 & 73.68 & 72.99 & 86.64 \\ NegCosine [26] & 62.33 & 80.94 & 72.66 & 89.40 \\ LR with DC [51] & **68.57** & 82.88 & 79.56 & 90.67 \\ \hline TEEN & 65.70 & **83.11** & **81.44** & **91.04** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Few-Shot Leaning performance of classification accuracy (%) on _mini_ImageNet and CUB. The results of compared methods are cited from [51]. 5w1s and 5w5s mean 5way-1shot and 5way-5shot, respectively. The best results are in bold and the runner-up results are in underlines.

Specifically, we select \(\tau\) from \(\{8,16,32,64\}\) and \(\alpha\) from \(\{0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9\}\). Figure 4(a) and Figure 4(b) show how the hyper-parameters \(\alpha\) and \(\tau\) influence the average accuracy on all classes and new classes, respectively. It also demonstrates that a proper \(\alpha\) can improve the performance of new classes, which confirms the base prototypes can benefit the performance of new classes. Compared with larger \(\tau\), relatively smaller \(\tau\) can smooth the weight distribution. The smaller \(\tau\) with stronger performance further confirm the effectiveness of utilizing the abundant semantic information from base classes.

**The influence of similarity-based weight:** To further validate the effectiveness of the semantic similarity between the base and new classes, we remove the Equation 6 and directly align the new prototypes to their corresponding \(K\) most similar base class prototypes, _i.e._, \(\bar{\mathbf{c}}_{n}=\alpha\,\mathbf{c}_{n}+(1-\alpha)\,\sum_{k=1}^{K}\mathbf{ c}_{k}\). We denote this **Simple** version of TEEN _without similarity-based weight_ as SimTEEN. We select \(K\) from \(\{1,3,5,7,9\}\). Figure 4(c) and Figure 4(d) show how the similarity-based weight (_i.e._, Equation 6) influences the average accuracy on all classes and new classes, respectively. Notably, as shown in Figure 4(d), there is a significant drop, particularly on new classes, after removing the similarity-based weight. This phenomenon confirms that TEEN can achieve calibration of new prototypes with the help of well-represented semantic similarity between the base and new classes.

## 7 Conclusion

Few-shot class-incremental learning is of great importance to real-world learning scenarios. In this study, we first observe existing methods usually exhibit poor performance in new classes and the samples of new classes are easily misclassified into most similar base classes. We further find that although the feature extractor trained on base classes can not well represent new classes directly, it can properly represent the semantic similarity between the base and new classes. Based on these analyses, we propose a simple yet effective training-free prototype calibration strategy (_i.e._, TEEN) for biased prototypes of new classes. TEEN obtains competitive results in FSCIL and FSL scenarios.

**Limitations:** The FSCIL methods mentioned in this paper all select the base and new classes from the _same_ dataset. In other words, current FSCIL methods assume the model pre-trains in the same domain, increasing the restrictions on pre-training data collection. Therefore, a more realistic scenario is to pre-train on a dataset that is independent of the subsequent data distribution and then perform few-shot class-incremental learning on a target dataset that we expect. The intricate problem of cross-domain few-shot class-incremental learning will be thoroughly investigated in our future works.

## Acknowledgements

This work is partially supported by National Key R&D Program of China (2022ZD0114805), NSFC (62376118, 62006112, 62250069), Young Elite Scientists Sponsorship Program of Jiangsu Association for Science and Technology 2021-020, Collaborative Innovation Center of Novel Software Technology and Industrialization.

Figure 5: (a) and (b) show the influence of \(\alpha\) and \(\tau\) on the last session’s average accuracy in _all classes_ (_i.e._, AvgAcc) and _new classes_ (_i.e._, NAcc), respectively. (c) and (d) show the influence of similarity-based weight in TEEN on the last session’s average accuracy in _all classes_ (_i.e._, AvgAcc) and _new classes_ (_i.e._, NAcc), respectively. It demonstrates that replacing the softmax-based weight with a hard one-shot weight will drastically reduce the performance of TEEN.

## References

* [1]A. Arjovsky, E. Akyurek, D. Whiaya, and J. Andreas (2022) Subspace regularizers for few-shot class incremental learning. In ICLR, Cited by: SS1, SS2.
* [2]R. Aljundi, M. Lin, B. Goujaud, and Y. Bengio (2019) Gradient based sample selection for online continual learning. In NeurIPS, pp. 11816-11825. Cited by: SS1, SS2.
* [3]A. Antoniou, H. Edwards, and A. J. Storkey (2019) How to train your maml. In ICLR, Cited by: SS1, SS2.
* [4]F. M. Castro, M. J. Martin-Jimenez, N. Guil, C. Schmid, and K. Alahari (2018) End-to-end incremental learning. In ECCV, pp. 233-248. Cited by: SS1, SS2.
* [5]W. Chao, H. Ye, D. Zhan, M. E. Campbell, and K. Q. Weinberger (2020) Revisiting meta-learning as supervised learning. CoRRabs/2002.00573. Cited by: SS1, SS2.
* [6]K. Chen and C. Lee (2020) Incremental few-shot learning via vector quantization in deep embedded space. In ICLR, Cited by: SS1, SS2.
* [7]T. Chen, S. Kornblith, M. Norouzi, and G. E. Hinton (2020) A simple framework for contrastive learning of visual representations. In ICML, pp. 1597-1607. Cited by: SS1, SS2.
* [8]A. Cheraghian, S. Rahman, P. Fang, S. Kumar Roy, L. Petersson, and M. Harandi (2021) Semantic-aware knowledge distillation for few-shot class-incremental learning. In CVPR, pp. 2534-2543. Cited by: SS1, SS2.
* [9]A. Cheraghian, S. Rahman, S. Ramasinghe, P. Fang, C. Simon, L. Petersson, and M. Harandi (2021) Synthesized feature based few-shot class-incremental learning on a mixture of subspaces. In ICCV, pp. 8641-8650. Cited by: SS1, SS2.
* [10]Z. Chi, L. Gu, H. Liu, Y. Wang, Y. Yu, and J. Tang (2022) Metafscil: a meta-learning approach for few-shot class incremental learning. In CVPR, pp. 14146-14155. Cited by: SS1, SS2.
* [11]J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei (2009) ImageNet: a large-scale hierarchical image database. In CVPR, pp. 248-255. Cited by: SS1, SS2.
* [12]A. Douillard, M. Cord, C. Ollion, T. Robert, and E. Valle (2020) Podnet: pooled outputs distillation for small-tasks incremental learning. In ECCV, pp. 86-102. Cited by: SS1, SS2.
* [13]C. Finn, P. Abbeel, and S. Levine (2017) Model-agnostic meta-learning for fast adaptation of deep networks. In ICML, pp. 1126-1135. Cited by: SS1, SS2.
* [14]R. M. French and N. Chater (2002) Using noise to compute error surfaces in connectionist networks: a novel means of reducing catastrophic forgetting. Neural Computation14 (7), pp. 1755-1769. Cited by: SS1, SS2.
* [15]S. Gidaris and N. Komodakis (2018) Dynamic few-shot visual learning without forgetting. In CVPR, pp. 4367-4375. Cited by: SS1, SS2.
* [16]K. He, X. Zhang, S. Ren, and J. Sun (2016) Deep residual learning for image recognition. In CVPR, pp. 770-778. Cited by: SS1, SS2.
* [17]G. E. Hinton, O. Vinyals, and J. Dean (2015) Distilling the knowledge in a neural network. CoRRabs/1503.02513. Cited by: SS1, SS2.
* [18]S. Hou, X. Pan, C. C. Loy, Z. Wang, and D. Lin (2019) Learning a unified classifier incrementally via rebalancing. In CVPR, pp. 831-839. Cited by: SS1, SS2.
* [19]G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger (2017) Densely connected convolutional networks. In CVPR, pp. 2261-2269. Cited by: SS1, SS2.
* [20]D. Ieele and A. Cosgun (2018) Selective experience replay for lifelong learning. In AAAI, pp. 3302-3309. Cited by: SS1, SS2.
* [21]M. Kang, J. Park, and B. Han (2022) Class-incremental learning by knowledge distillation with adaptive feature consolidation. In CVPR, pp. 16050-16059. Cited by: SS1, SS2.
* [22]J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramdna, A. E. Grabska-Barwinska, et al. (2017) Overcoming catastrophic forgetting in neural networks. PNAS114 (13), pp. 3521-3526. Cited by: SS1, SS2.
* [23]A. Krizhevsky, G. Hinton, et al. (2009) Learning multiple layers of features from tiny images. Cited by: SS1, SS2.
* [24]A. Kukleva, H. Kuehne, and B. Schiele (2021) Generalized and incremental few-shot learning by explicit learning and calibration without forgetting. In ICCV, pp. 9000-9009. Cited by: SS1, SS2.
* [25]Z. Li and D. Hoiem (2016) Learning without forgetting. In ECCV, pp. 614-629. Cited by: SS1, SS2.
* [26]B. Liu, Y. Cao, Y. Lin, Q. Li, Z. Zhang, M. Long, and H. Hu (2020) Negative margin matters: understanding margin in few-shot classification. In ECCV, pp. 438-455. Cited by: SS1, SS2.
* [27]J. Liu, L. Song, and Y. Qin (2020) Prototype rectification for few-shot learning. In ECCV, pp. 741-756. Cited by: SS1, SS2.
* [28]Y. Liu, B. Schiele, and Q. Sun (2021) Adaptive aggregation networks for class-incremental learning. In CVPR, pp. 2544-2553. Cited by: SS1, SS2.
* [29]Y. Liu, B. Schiele, and Q. Sun (2021) RMM: reinforced memory management for class-incremental learning. In NeurIPS, pp. 3478-3490. Cited by: SS1, SS2.
* [30]S. Madhavan and N. Kumar (2021) Incremental methods in face recognition: a survey. Artificial Intelligence Review54 (1), pp. 253-303. Cited by: SS1, SS2.
* [31]A. Nichol, J. Achiam, and J. Schulman (2018) On first-order meta-learning algorithms. CoRRabs/1803.02999. Cited by: SS1, SS2.
* [32]A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Z. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala (2018) PyTorch: an imperative style, high-performance deep learning library. InNeurIPS_, pages 8024-8035, 2019.
* [33] Hang Qi, Matthew Brown, and David G. Lowe. Low-shot learning with imprinted weights. In _CVPR_, pages 5822-5830, 2018.
* [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, pages 8748-8763, 2021.
* [35] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl: Incremental classifier and representation learning. In _CVPR_, pages 5533-5542, 2017.
* [36] Shaoqing Ren, Kaiming He, Ross B Girshick, and Jian Sun. Faster R-CNN: towards real-time object detection with region proposal networks. In _NIPS_, pages 91-99, 2015.
* [37] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy P. Lillicrap, and Gregory Wayne. Experience replay for continual learning. In _NeurIPS_, pages 348-358, 2019.
* [38] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. _IJCV_, 115(3):211-252, 2015.
* [39] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In _ICML_, pages 4548-4557, 2018.
* [40] Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical networks for few-shot learning. In _NIPS_, pages 4077-4087, 2017.
* [41] Hai-Long Sun, Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan. Pilot: A pre-trained model-based continual learning toolbox. _arXiv preprint arXiv:2309.07117_, 2023.
* [42] Xiaoyu Tao, Xiaopeng Hong, Xinyuan Chang, Songlin Dong, Xing Wei, and Yihong Gong. Few-shot class-incremental learning. In _CVPR_, pages 12180-12189, 2020.
* [43] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. In _NIPS_, pages 3630-3638, 2016.
* [44] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. Caltech-ucsd birds-200-2011 dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.
* [45] Feng Wang, Weike Pan, and Li Chen. Recommendation for new users with partial preferences by integrating product reviews with static specifications. In _UMAP_, pages 281-288, 2013.
* [46] Fu-Yun Wang, Da-Wei Zhou, Liu Liu, Han-Jia Ye, Yatao Bian, De-Chuan Zhan, and Peilin Zhao. Beef: Bi-compatible class-incremental learning via energy-based expansion and fusion. In _ICLR_, 2023.
* [47] Fu-Yun Wang, Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan. Foster: Feature boosting and compression for class-incremental learning. In _ECCV_, pages 398-414, 2022.
* [48] Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Generalizing from a few examples: A survey on few-shot learning. _ACM Computing Surveys_, 53(3):1-34, 2020.
* [49] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu. Large scale incremental learning. In _CVPR_, pages 374-382, 2019.
* [50] Shipeng Yan, Jiangwei Xie, and Xuming He. DER: Dynamically expandable representation for class incremental learning. In _CVPR_, pages 3014-3023, 2021.
* [51] Shuo Yang, Lu Liu, and Min Xu. Free lunch for few-shot learning: Distribution calibration. In _ICLR_, 2021.
* [52] Yibo Yang, Haobo Yuan, Xiangtai Li, Zhouchen Lin, Philip Torr, and Dacheng Tao. Neural collapse inspired feature-classifier alignment for few-shot class-incremental learning. In _ICLR_, 2023.
* [53] Yang Yang, Da-Wei Zhou, De-Chuan Zhan, Hui Xiong, and Yuan Jiang. Adaptive deep models for incremental learning: Considering capacity scalability and sustainability. In _KDD_, pages 74-82, 2019.
* [54] Han-Jia Ye, De-Chuan Zhan, Yuan Jiang, and Zhi-Hua Zhou. Heterogeneous few-shot model rectification with semantic mapping. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 43(11):3878-3891, 2021.
* [55] Han-Jia Ye, De-Chuan Zhan, Nan Li, and Yuan Jiang. Learning multiple local metrics: Global consideration helps. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 42(7):1698-1712, 2020.
* [56] Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen. Deepcepd: Few-shot image classification with differentiable earth mover's distance and structured classifiers. In _CVPR_, pages 12203-12213, 2020.
* [57] Chi Zhang, Nan Song, Guosheng Lin, Yun Zheng, Pan Pan, and Yinghui Xu. Few-shot incremental learning with continually evolved classifiers. In _CVPR_, pages 12455-12464, 2021.
* [58] Da-Wei Zhou, Fu-Yun Wang, Han-Jia Ye, Liang Ma, Shiliang Pu, and De-Chuan Zhan. Forward compatible few-shot class-incremental learning. In _CVPR_, pages 9036-9046, 2022.
* [59] Da-Wei Zhou, Han-Jia Ye, Liang Ma, Di Xie, Shiliang Pu, and De-Chuan Zhan. Few-shot class-incremental learning by sampling multi-phase tasks. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(11):12816-12831, 2023.
* [60] Da-Wei Zhou, Yuanhan Zhang, Jingyi Ning, Han-Jia Ye, De-Chuan Zhan, and Ziwei Liu. Learning without forgetting for vision-language models. _arXiv preprint arXiv:2305.19270_, 2023.
* [61] Kai Zhu, Yang Cao, Wei Zhai, Jie Cheng, and Zheng-Jun Zha. Self-promoted prototype refinement for few-shot class-incremental learning. In _CVPR_, pages 6801-6810, 2021.

**Supplementary materials**

## Appendix A Discussion

### Implicit Semantic Similarity

Notably, TEEN demonstrates that _the off-the-shelf feature extractor_ is already capable of representing semantic similarity without the need for any additional training costs or auxiliary side information (_e.g._, the textual information of the class name [8]).

### Efficiency

The only time cost in TEEN is the pre-training cost on the base classes. Furthermore, the training cost of the vanilla cross-entropy loss function is relatively small compared to other more complex training paradigms. Additionally, the calibration in TEEN does not evolve any training cost and update procedure. Therefore, the cost of incrementally learning new classes of TEEN can be considered negligible compared with existing methods.

### Combination with better representation

In addition to combining with existing prototype-based methods, TEEN can also be combined with more powerful pre-trained models (_e.g._, self-supervised pre-trained model [7]). Notably, the recent popular vision-language models (_e.g._, CLIP [34]) can also be seen as a powerful pre-trained model and provide better representation.

## Appendix B A Closer Look at FSCIL

In this section, we show the observations (_i.e._, **Section 3** of the main paper) on more datasets. As shown in Table 10, the existing prototype-based FSCIL methods are extremely prone to misclassify new classes into base classes. Considering the misclassified instances of base classes, we quantitatively show that _the samples of new classes are usually misclassified into base classes_. These observations in the additional benchmark dataset are consistent with observations in the main paper, which further confirms the universality of our observation.

To further understand the aforementioned observation, we further analyze the question _What base classes are the new classes incorrectly predicted into?_ and come to our second observation: **The prototype-based classifier misclassifies the new classes to their corresponding most similar base classes with a high probability.** The detailed analysis results in Table 11 further verify the correctness of our observation.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{8}{c}{Accuracy in each session (\%) \(\uparrow\)} & \multicolumn{8}{c}{PD \(\downarrow\)} & \(\Delta\) PD \\ \cline{2-11}  & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & \\ \hline iCARL [35] & 64.10 & 53.28 & 41.69 & 34.13 & 27.93 & 25.06 & 20.41 & 15.48 & 13.73 & 50.37 & **+28.09** \\ EEIL [4] & 64.10 & 53.11 & 43.71 & 35.15 & 28.96 & 24.98 & 21.01 & 17.26 & 15.85 & 48.25 & **+25.97** \\ Rebalancing [18] & 64.10 & 53.05 & 43.96 & 36.97 & 31.61 & 26.73 & 21.23 & 16.78 & 13.54 & 50.56 & **+28.28** \\ \hline TOPIC [42] & 64.10 & 55.88 & 47.07 & 45.16 & 40.11 & 36.38 & 33.96 & 31.55 & 29.37 & 34.73 & **+12.45** \\ Decoupled-NegCosine [26] & 74.36 & 68.23 & 62.84 & 59.24 & 55.32 & 52.88 & 50.86 & 48.98 & 46.66 & 27.70 & **+5.42** \\ Decoupled-Cosine [43] & 74.55 & 67.43 & 63.63 & 59.55 & 56.11 & 53.80 & 51.68 & 49.67 & 47.68 & 26.87 & **+5.99** \\ Decoupled-DeepEMD [56] & 69.75 & 65.06 & 61.20 & 57.21 & 53.88 & 51.40 & 48.80 & 46.84 & 44.41 & 25.34 & **+3.06** \\ CEC [57] & 73.07 & 68.88 & 65.26 & 61.19 & 58.09 & 55.57 & 53.22 & 51.34 & 49.14 & 23.93 & **+1.65** \\ FACT [58] & 74.60 & 72.09 & 67.56 & 63.52 & 61.38 & 58.36 & 56.28 & 54.24 & 52.10 & 22.50 & **+0.22** \\ \hline TEEN & **74.92** & **72.65** & **68.74** & **65.01** & **62.01** & **59.29** & **57.90** & **54.76** & **52.64** & **22.28** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Detailed average accuracy of each incremental session on CIFAR100 dataset.The results of compared methods are cited from [42, 57, 58]. \(\uparrow\) means higher accuracy is better. \(\downarrow\) means lower PD is better.

## Appendix C Experiments

### Details of experiments

**Dataset details:** Following previous methods [42; 57; 58], we evaluate TEEN on CIFAR100 [23], CUB200-2011 [44], _mini_ImageNet [38]. CIFAR100 [23] contains 100 classes and each class contains 500 images for training and 100 images for testing. The image size of CIFAR100 is \(3\times 32\times 32\). The CUB200-2011 [44] is a widely-used fine-grained dataset and is also a benchmark dataset of few-shot image classification. It contains 200 classes and all images of these classes belong to birds. The _mini_ImageNet [38] are sampled from the raw ImageNet [11] and contains 100 classes.

In FSCIL, each benchmark dataset is divided into different subsets. Each subset contains specific classes and the label space of different subsets is nonoverlapping. Specifically, CIFAR100 is divided into 60 classes for the base session and the remaining 40 classes are divided into eight 5-way 5-shot few-shot classification tasks. CUB200 is divided into 100 base classes for the base session and the remaining 100 classes are divided into ten 10-way 5-shot few-shot classification tasks. _mini_ImageNet is divided into 60 base classes for the base session, and the remaining 40 classes are divided into eight 5-way 5-shot few-shot classification tasks. The splitting details (_i.e._, the class order and the selection of support data in incremental sessions) follow the previous methods [42; 57; 58]. Notably, **no old samples are saved** to assist in maintaining the discriminability of previous classes.

**Baseline details:** Following previous methods [42; 57; 58; 61], we compare TEEN with popular CIL methods, FSL methods, and FSCIL methods. For CIL methods, we select iCaRL [35], EEIL [4] and Rebalancing [18] as our baseline methods. For methods based on few-shot, we adopt Decoupled-NegativeCosine [26], Decoupled-Cosine [43] and Decoupled-DeepEMD [56] as our baseline methods. For FSCIL methods, we adopt TOPIC [42], CEC [57] and FACT [58] methods as our baseline methods.

**Training details:** The training of the feature extractor uses vanilla cross-entropy loss as the objective function. In addition, we adopt the cosine similarity to measure the feature of instances to class prototypes. Following [42; 57; 58], we use ResNet20 [16] for CIFAR100, pre-trained ResNet18 [16] for CUB200 and randomly initialized ResNet18 [16] for _mini_ImageNet. All compared methods use **the same backbone network and same initialization** for a fair comparison.

For the hyperparameters setting, we set \(\alpha=0.5,\tau=16\) for _mini_ImageNet and CUB200, \(\alpha=0.1,\tau=16\) for CIFAR100. We train the feature extractor on CUB200 with a learning rate of 0.004,

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{8}{c}{Accuracy in each session (\%) \(\uparrow\)} & \multirow{2}{*}{PD \(\downarrow\)} & \multirow{2}{*}{\(\Delta\) PD} \\ \cline{2-2} \cline{5-12}  & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ \hline iCaRL [35] & 68.68 & 52.65 & 48.61 & 44.16 & 36.62 & 29.52 & 27.83 & 26.26 & 24.01 & 23.89 & 21.16 & 47.52 & **+29.39** \\ EELI. [4] & 68.68 & 53.63 & 47.91 & 44.20 & 36.30 & 27.46 & 25.93 & 24.70 & 23.95 & 24.13 & 22.11 & 46.57 & **+28.44** \\ Rebalancing [18] & 68.68 & 57.12 & 44.21 & 28.78 & 26.71 & 25.65 & 24.62 & 21.52 & 20.12 & 20.06 & 19.87 & 48.81 & **+30.68** \\ \hline TOPIC [42] & 68.68 & 62.49 & 54.81 & 49.99 & 45.25 & 41.40 & 38.35 & 35.36 & 32.22 & 28.31 & 26.26 & 42.40 & **+24.27** \\ Decoupled-NegCosine\({}^{\dagger}\)[26] & 74.96 & 70.57 & 66.62 & 61.32 & 60.09 & 56.60 & 55.03 & 52.78 & 51.50 & 50.08 & 48.47 & **26.49** & **+7.36** \\ Decoupled-Cosine [43] & 75.52 & 70.95 & 66.46 & 61.20 & 80.6 & 56.88 & 55.04 & 53.49 & 51.94 & 50.93 & 49.31 & 26.21 & **+8.08** \\ Decoupled-DeepEMD [56] & 75.35 & 70.69 & 66.68 & 62.34 & 59.76 & 56.54 & 54.61 & 52.52 & 50.73 & 49.20 & 47.60 & 27.75 & **+9.62** \\ CEC [57] & 75.85 & 71.94 & 68.50 & 63.50 & 62.43 & 58.27 & 57.73 & 55.81 & 58.43 & 53.52 & 52.28 & 23.57 & **+5.44** \\ FACT [58] & 75.90 & 73.23 & 70.84 & 68.13 & 65.56 & 62.15 & 61.74 & 59.83 & 58.41 & 57.89 & 56.94 & **+0.83** \\ \hline TEEN & **77.26** & **76.13** & **72.81** & **68.16** & **67.77** & **64.40** & **63.25** & **62.29** & **61.19** & **60.32** & **59.31** & **18.33** \\ \hline \hline \end{tabular}
\end{table}
Table 7: Detailed average accuracy of each incremental session on CUB200 dataset.The results of compared methods are cited from [42; 57; 58]. \(\uparrow\) means higher accuracy is better. \(\downarrow\) means lower PD is better.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c} \hline \hline Session & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & \multicolumn{1}{c|}{} \\ \hline
**HIMean/NAec** & HMean & NAec & HMean & NAec & HMean & NAec & HMean & NAec & HMean & NAec & HMean & NAec & HMean & NAec \\ \hline CEC [57] & 47.04 & 34.80 & 40.35 & 28.10 & 36.61 & 24.13 & 36.49 & 24.65 & 36.56 & 24.84 & 37.07 & 25.40 & 36.41 & 24.83 & 35.49 & 24.08 \\ FACT [58] & 43.04 & 30.00 & 38.77 & 26.10 & 34.70 & 22.60 & 34.44 & 22.45 & 35.22 & 26.46 & 35.44 & 23.37 & 33.79 & 22.06 & 33.05 & 21.48 \\ \hline TEEN & **47.77** & **34.82** & **42.99** & **30.10** & **41.99** & **27.53** & **39.52** & **27.00** & **39.66** & **27.28** & **39.65** & **27.37** & **37.71** & **25.57** & **37.11** & **25.13** \\ \(\Delta\) & **+0.73** & **+0.02** & **+2.64** & **+2.00** & **+1.18** & **+3.40** & **+3.03** & **+2.35** & **+3.10** & **+2.44** & **+2.58** & **+1.97** & **+1.30** & **+0.74** & **+1.62** & **+1.05** \\ \hline \hline \end{tabular}
\end{table}
Table 8: Detailed results of **HIMean** and **NAec** on _mini_ImageNet. The best results are in bold and the runner-up results are in underlines. The \(\Delta\) measures the performance gap between the best and second-best results on the corresponding session. Due to space limitations, the performance on only six incremental sessions is presented. Please refer to the supplementary for more detailed results on CUB200 and CIFAR100.

[MISSING_PAGE_FAIL:15]

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c} \hline \hline Session & 1 & & 2 & 3 & 4 & 5 & & 6 & 7 & & 8 \\ \hline
**FNR/FPR** & FNR & FPR & FNR & FPR & FNR & FPR & FNR & FPR & FNR & FPR & FNR & FPR & FNR & FPR \\ \hline ProtoNet [40] & 2.55 & 71.60 & 3.98 & 67.30 & 4.83 & 66.07 & 5.83 & 62.05 & 6.48 & 59.16 & 7.18 & 58.17 & 7.93 & 54.83 & 8.85 & 52.23 \\ CEC [57] & 3.45 & 68.40 & 5.60 & 65.50 & 7.03 & 61.93 & 7.73 & 58.30 & 8.47 & 56.68 & 9.58 & 54.50 & 10.22 & 52.54 & 10.93 & 50.05 \\ FACT [58] & 2.07 & 72.40 & 3.42 & 70.60 & 4.22 & 70.13 & 4.55 & 69.40 & 4.97 & 68.28 & 5.12 & 68.17 & 5.33 & 66.51 & 5.58 & 66.55 \\ \hline TEEN & 8.02 & **46.40** & 11.35 & **38.60** & 13.12 & **37.53** & 15.32 & **35.20** & 16.47 & **32.48** & 17.38 & **31.57** & 18.63 & **28.03** & 19.97 & **26.35** \\ \hline \hline \end{tabular}
\end{table}
Table 10: Detailed prediction results of **False Negative Rate/False Positive Rate** (%) on _mini_ImageNet dataset. The analysis results are from session 1 because new classes do not exist in session 0. Exceedingly high **FPR** and relatively low **FNR** show the instances of new classes are easily misclassified into base classes and the instances of base classes are also easily misclassified into base classes. TEEN can achieve relatively lower **FPR** than baseline methods, which demonstrates the validity of the proposed calibration strategy.

Figure 8: We compare FACT without (_i.e._, **FACT** in figures) and with TEEN (_i.e._, **FACT w/ TEEN** on CIFAR100 dataset. The FACT benefits from the well-calibration effect of TEEN and achieves better-balanced performance between the base and new classes

Figure 6: We compare FACT without (_i.e._, **FACT** in figures) and with TEEN (_i.e._, **FACT w/ TEEN** on CUB200 dataset. The FACT benefits from the well-calibration effect of TEEN and achieves better-balanced performance between the base and new classes

\begin{table}
\begin{tabular}{c|c c|c c|c c|c c|c c|c c|c c} \hline \hline Session & \multicolumn{2}{c|}{1} & \multicolumn{2}{c|}{2} & \multicolumn{2}{c|}{3} & \multicolumn{2}{c|}{4} & \multicolumn{2}{c|}{5} & \multicolumn{2}{c|}{6} & \multicolumn{2}{c}{7} & \multicolumn{2}{c}{8} \\ \hline
**TNR/TBR** & TNR & TBR & TNR & TBR & TNR & TBR & TNR & TBR & TNR & TBR & TNR & TBR & TNR & TBR \\ \hline ProtoNet [40] & 3.47 & 73.01 & 5.02 & 61.79 & 7.06 & 55.05 & 8.08 & 52.95 & 7.14 & 53.27 & 8.56 & 51.96 & 8.53 & 49.18 & 9.21 & 50.91 \\ CEC [57] & 2.87 & 71.70 & 4.53 & 61.08 & 5.66 & 58.26 & 6.35 & 55.16 & 6.07 & 54.12 & 6.92 & 52.53 & 8.24 & 49.96 & 8.20 & 51.64 \\ FACT [58] & 2.33 & 70.00 & 3.32 & 61.01 & 5.50 & 55.17 & 6.88 & 50.55 & 6.57 & 50.52 & 7.72 & 49.30 & 8.88 & 46.99 & 9.31 & 48.48 \\ \hline TEEN & 2.16 & **66.77** & 2.53 & **55.14** & 3.55 & **44.87** & 3.85 & **37.68** & 3.57 & **39.47** & 4.02 & **37.07** & 4.76 & **33.83** & 4.85 & **35.04** \\ \hline \hline \end{tabular}
\end{table}
Table 11: Detailed prediction results of **TNR/TBR** (%) on _mini_ImageNet dataset. The analysis results are from session 1 because new classes do not exist in session 0. For new classes, we only consider the 10 most similar base classes out of 60 base classes. For base classes, we suppose \(C_{i}\) new classes exist in the current incremental session \(i\). We only consider the most similar \(\lfloor 20\%\times C_{i}\rfloor\) new classes. Class similarity adopts cosine similarity between different class prototypes. TEEN can achieve relatively lower **TBR** than baseline methods, which demonstrates the validity of the proposed calibration strategy.