# Better by Default: Strong Pre-Tuned MLPs and Boosted Trees on Tabular Data

David Holzmuller

SlerRA Team, Inria Paris

Ecole Normale Superieure

PSL University

&Leo Grinsztajn

SODA Team, Inria Saclay

Ingo Steinwart

University of Stuttgart

Faculty of Mathematics and Physics

Institute for Stochastics and Applications

Work done partially while still at University of Stuttgart.

###### Abstract

For classification and regression on tabular data, the dominance of gradient-boosted decision trees (GBDTs) has recently been challenged by often much slower deep learning methods with extensive hyperparameter tuning. We address this discrepancy by introducing (a) RealMLP, an improved multilayer perceptron (MLP), and (b) strong meta-tuned default parameters for GBDTs and RealMLP. We tune RealMLP and the default parameters on a meta-train benchmark with 118 datasets and compare them to hyperparameter-optimized versions on a disjoint meta-test benchmark with 90 datasets, as well as the GBDT-friendly benchmark by Grinsztajn et al. (2022). Our benchmark results on medium-to-large tabular datasets (1K-500K samples) show that RealMLP offers a favorable time-accuracy tradeoff compared to other neural baselines and is competitive with GBDTs in terms of benchmark scores. Moreover, a combination of RealMLP and GBDTs with improved default parameters can achieve excellent results without hyperparameter tuning. Finally, we demonstrate that some of RealMLP's improvements can also considerably improve the performance of TabR with default parameters.

## 1 Introduction

Perhaps the most common type of data in practical machine learning (ML) is tabular data, characterized by a fixed number of features (columns) that can take different types such as numerical or categorical, as well as a lack of the spatiotemporal structure found in image or text data. The moderate dimension and lack of symmetries make tabular data accessible to a wide variety of machine learning methods. Although tabular data is very diverse and no method is dominant on all datasets, gradient-boosted decision trees (GBDTs) exhibit excellent results on benchmarks [18, 43, 58, 69], although their superiority has been challenged by a variety of deep learning methods [3].

While many architectures for neural networks (NNs) have been proposed [3], variants of the simple multilayer perceptron (MLP) have repeatedly been shown to be good baselines for tabular NNs [15, 16, 30, 54]. Moreover, in terms of training time, MLPs are often slower than GBDTs but still considerably faster than many other architectures [18, 43]. Therefore, we study how MLPs can be improved in terms of architecture, training, preprocessing, hyperparameters, and initialization. We also demonstrate that at least some of these improvements can successfully improve TabR [17].

Even with fast and accurate NNs, the cost of extensive hyperparameter optimization can be problematic and hinder the adoption of new methods. To address this issue, we investigate the potential of better dataset-independent default parameters for MLPs and GBDTs. Specifically, we compare the library defaults (D) to our tuned defaults (TD) and (dataset-dependent) hyperparameter optimization (HPO). Unlike McElfresh et al. [43], who argue in favor HPO on GBDTs over trying NNs, our results show a better time-accuracy trade-off for trying different (tuned) default models, as is done by modern AutoML systems [10; 11].

### Contribution

The problem of finding better default parameters can be seen as a meta-learning problem [64]. We employ a meta-train benchmark consisting of 118 datasets on which the default hyperparameters are optimized, and a disjoint meta-test benchmark consisting of 90 datasets on which they are evaluated. We consider separate default parameters for classification, optimized for classification error, and for regression, optimized for RMSE. Our benchmarks do not contain missing numerical values, and we restrict ourselves to sizes between 1K and 500K samples, cf. Section 2.

In Section 3, we introduce **RealMLP**, which improves on standard MLPs through a **bag of tricks** and **better default parameters**, tuned entirely on the meta-train benchmark. We introduce many **novel or nonstandard components**, such as preprocessing using robust scaling and smooth clipping, a new numerical embedding variant, a diagonal weight layer, new schedules, different initialization methods, etc. Our benchmark results demonstrate that it often outperforms other comparably fast NNs from the literature and can be competitive with GBDTs. To demonstrate that our bag of tricks is useful for other models, we introduce **RealTabR-D**, a version of TabR [17] including some of our tricks that, despite less extensive tuning, achieves excellent benchmark results.

In Section 4, we provide **new default parameters**, tuned on the meta-train benchmark, for XGBoost [9], LightGBM [31], and CatBoost [51]. While they cannot match HPO on average, they outperform the library defaults on the meta-test benchmark.

In Section 5, we evaluate these and other models on the meta-test **benchmark** and the benchmark by Grinsztajn et al. [18]. We also investigate several possibilities for algorithm selection and ensembling, demonstrating that algorithm selection over default methods provides a better time-performance tradeoff than HPO, thanks to our new improved default parameters and MLP.

The code for our benchmarks, including scikit-learn interfaces for the models, is available at

[https://github.com/dholzmueller/pytabkit](https://github.com/dholzmueller/pytabkit)

Our code and data are archived at [https://doi.org/10.18419/darus-4555](https://doi.org/10.18419/darus-4555).

### Related Work

Neural networksBorisov et al. [3] review deep learning on tabular data and identify three main classes of methods: Data transformation methods, specialized architectures, and regularization models. In particular, recent research has mainly focused on specialized architectures based on attention [1; 7; 15; 27], including attention between datapoints [17; 37; 53; 56; 60]. However, these methods are usually significantly slower than MLPs or even GBDTs [17; 18; 43]. Our research instead expands on improvements to MLPs for tabular data such as the SELU activation function [35], bias initialization methods [61], regularization methods [30], categorical embedding layers [19], and numerical embedding layers [16].

BenchmarksShwartz-Ziv and Armon [58] benchmarked three deep learning methods and noticed that they performed better on the datasets from their own papers than on other datasets. We address this issue by using more datasets and evaluating our methods on datasets that they were not tuned on. Grinsztajn et al. [18], McElfresh et al. [43], and Ye et al. [69] propose larger benchmarks and find that GBDTs still outperform deep learning methods on average, analyzing why and when this is the case. Kohli et al. [36] also emphasize the need for large benchmarks. We evaluate our methods on the benchmark by Grinsztajn et al. [18] as well as datasets from the AutoML benchmark [13] and the OpenML-CTR23 regression benchmark [12].

Better defaultsProbst et al. [50] study the tunability of ML methods, i.e., the difference in benchmark scores between the best fixed hyperparameters and tuned hyperparameters. While their approach involves finding better defaults, they do not evaluate them on a separate meta-test benchmark, only consider classification, and do not provide defaults for LightGBM, CatBoost, and NNs.

Meta-learningThe problem of finding the best fixed hyperparameters is a meta-learning problem [4; 64]. Although we do not introduce or employ a fully automated method to find good defaults, we use a meta-learning benchmark setup to properly evaluate them. Wistuba et al. [66] and Pfisterer et al. [49] learn portfolios of configurations and van Rijn et al. [63] learn symbolic defaults, but neither of these papers considers GBDTs or NNs. Salinas and Erickson [55] learn large portfolios of configurations on an extensive benchmark, without studying the best defaults for individual model families. Such portfolios are successfully applied in modern AutoML methods [10; 11]. At the other end of the meta-learning spectrum, TabPFN [23] meta-learns a (tuning-free) learning method on small synthetic datasets. Unlike TabPFN, we only meta-learn hyperparameters and can therefore use fewer but larger and more realistic meta-train datasets, resulting in methods that scale to larger datasets.

## 2 Methodology

To evaluate a fixed hyperparameter configuration \(\mathcal{H}\), we need a collection \(\mathcal{B}^{\mathrm{train}}\) of benchmark datasets and a scoring function that computes a benchmark score \(\mathcal{S}(\mathcal{B}^{\mathrm{train}},\mathcal{H})\) by aggregating the errors attained by the method with hyperparameters \(\mathcal{H}\) on each dataset. However, when optimizing \(\mathcal{H}\) on \(\mathcal{B}^{\mathrm{train}}\), we might overfit to the benchmark and therefore ideally need a second benchmark \(\mathcal{B}^{\mathrm{test}}\) to get an unbiased score for \(\mathcal{H}\). We refer to \(\mathcal{B}^{\mathrm{train}},\mathcal{B}^{\mathrm{test}}\) as meta-train and meta-test benchmarks and subdivide them into classification and regression benchmarks \(\mathcal{B}^{\mathrm{train}}_{\mathrm{class}}\), \(\mathcal{B}^{\mathrm{train}}_{\mathrm{reg}}\), \(\mathcal{B}^{\mathrm{test}}_{\mathrm{class}}\), and \(\mathcal{B}^{\mathrm{test}}_{\mathrm{reg}}\). We also use the Grinsztajn et al. [18] benchmark \(\mathcal{B}^{\mathrm{Grinsztajn}}\), which allows us to run more expensive baselines, since it limits training set sizes to 10K samples and contains fewer datasets due to more strict dataset inclusion criteria. Since \(\mathcal{B}^{\mathrm{train}}\) contains groups of datasets that are variants of the same dataset, for example by using different columns as targets, we use weighting factors inversely proportional to the group size.

Table 1 shows some characteristics of the considered benchmarks. The meta-test benchmark includes datasets that are more extreme in several dimensions, allowing us to test whether our default parameters generalize "out of distribution". For all datasets, we remove rows with missing numerical values and encode missing categorical values as a separate category.

### Benchmark Data Selection

The meta-train set consists of medium-sized datasets from the UCI Repository [32], adapted from Steinwart [61]. The meta-test set consists of the datasets from the AutoML Benchmark [13] as well as the OpenML-CTR23 regression benchmark [12] with a few modifications: we subsample some large datasets and remove datasets that are already contained in the meta-train set, are too small, or have categories with too large cardinality. More details on the datasets and preprocessing can be found in Appendix C.3.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline  & \(\mathcal{B}^{\mathrm{train}}_{\mathrm{class}}\) & \(\mathcal{B}^{\mathrm{test}}_{\mathrm{class}}\) & \(\mathcal{B}^{\mathrm{Grinsztajn}}_{\mathrm{class}}\) & \(\mathcal{B}^{\mathrm{train}}_{\mathrm{reg}}\) & \(\mathcal{B}^{\mathrm{test}}_{\mathrm{reg}}\) & \(\mathcal{B}^{\mathrm{Grinsztajn}}_{\mathrm{reg}}\) \\ \hline \#datasets & 71 & 48 & 18 & 47 & 42 & 28 \\ \#dataset groups & 46 & 48 & 18 & 26 & 42 & 28 \\ min \#samples & 1847 & 1000 & 3434 & 3338 & 1030 & 4052 \\ max \#samples & 45222 & 500000 & 500000 & 48204 & 500000 & 500000 \\ max \#classes & 26 & 355 & 2 & 0 & 0 & 0 \\ max \#features & 561 & 10000 & 419 & 520 & 4991 & 359 \\ max \#categories & 41 & 7019 & 14 & 38 & 359 & 20 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Characteristics of the meta-train and meta-test sets.

### Aggregate Benchmark Score

To optimize the default parameters, we need to define a single benchmark score. To this end, we evaluate a method on \(N_{\mathrm{splits}}=10\) random training-validation-test splits (60%-20%-20%) on each dataset. As metrics on individual dataset splits, we use classification error (\(100\%-\) accuracy) or 1-AUROC(one-vs-rest) for classification and

\[\mathrm{nRMSE}\coloneqq\frac{\text{RMSE}}{\text{standard deviation of targets}}=\sqrt{1-R^{2}}\]

for regression. There are various options to aggregate these errors into a single score. Some, such as average rank or mean normalized error, depend on which other methods are included in the evaluation, hindering an independent optimization. We would like to use the geometric mean error because arguably, an error reduction from \(0.02\) to \(0.01\) is more valuable than an error reduction from \(0.42\) to \(0.41\). However, since the geometric mean error is too sensitive to cases with zero error (especially for classification error), we instead use a _shifted geometric mean error_, where a small value \(\varepsilon\coloneqq 0.01\) is added to the errors \(\mathrm{err}_{ij}\) before taking the geometric mean:

\[\mathrm{SGM}_{\varepsilon}\coloneqq\exp\left(\sum_{i=1}^{N_{\mathrm{datasets} }}\frac{w_{i}}{N_{\mathrm{splits}}}\sum_{j=1}^{N_{\mathrm{spits}}}\log( \mathrm{err}_{ij}+\varepsilon)\right).\]

Here, we use weights \(w_{i}=1/N_{\mathrm{datasets}}\) on the meta-test set and Grinsztajn et al. [18] benchmark. On the meta-train set, we make the \(w_{i}\) dependent on the number of related datasets, cf. Appendix C.3. In Appendix B.10, we present results for other aggregation strategies.

## 3 Improving Neural Networks

The following section presents RealMLP-TD, our improved MLP with tuned defaults, which was designed based on experiments on the meta-train benchmark. A simplified version called RealMLP-TD-S is also described. To demonstrate that our improvements can be useful for other architectures, we introduce RealTabR-D, a version of TabR that includes some of our improvements but has not been tuned as extensively as RealMLP-TD.

Data preprocessingIn the first step of RealMLP, we apply one-hot encoding to categorical columns with at most eight distinct values (not counting missing values). Binary categories are encoded to a single feature with values \(\{-1,1\}\). Missing values in categorical columns are encoded to zero. After that, all numerical columns, including the one-hot encoded ones, are preprocessed independently as follows: Let \(x_{1},\dots,x_{n}\in\mathbb{R}\) be the values in column \(i\), and let \(q_{p}\) be the \(p\)-quantile of \((x_{1},\dots,x_{n})\) for \(p\in[0,1]\). Then,

\[x_{j,\mathrm{processed}} \coloneqq f(s_{j}\cdot(x_{j}-q_{1/2})),\ \ \ f(x)\coloneqq\frac{x}{\sqrt{1+( \frac{x}{3})^{2}}},\] \[s_{j} \coloneqq\begin{cases}\frac{1}{q_{3}/4-q_{1}/4}&,\text{ if }q_{3 /4}\neq q_{1/4}\\ \frac{2}{q_{1}-q_{0}}&,\text{ if }q_{3/4}=q_{1/4}\text{ and }q_{1}\neq q_{0}\\ 0&,\text{ otherwise.}\end{cases}\]

In scikit-learn [48], this corresponds to applying a RobustScaler (first case) or MinMaxScaler (second case), and then the function \(f\), which smoothly clips its input to the range \((-3,3)\). Smooth clipping functions like \(f\) have been used by, e.g., Holzmuller et al. [24] and Hafner et al. [20]. Intuitively, when features have large outliers, smooth clipping prevents the outliers from affecting the result too strongly, while robust scaling prevents the outliers from affecting the inlier scaling.

NN architectureOur architecture, visualized in Figure 1 (a), is a multilayer perceptron (MLP) with three hidden layers containing 256 neurons each, except for the following additions and modifications:

* RealMLP-TD employs categorical embedding layers [19] to embed the remaining categorical features with cardinality \(>8\).
* For numerical features, excluding the one-hot encoded ones, we introduce PBLD (periodic bias linear DenseNet) embeddings, which concatenate the original value to the PL embeddings proposed by Gorishniy et al. [16] and use a different periodic embedding with biases,inspired by Huang et al. [26] and Rahimi and Recht [52], respectively. PBLD embeddings apply separate small two-layer MLPs to each feature \(x_{i}\) as \[\left(x_{i},\mathbf{W}_{\text{emb}}^{(2,i)}\cos(2\pi\mathbf{w}_{\text{emb}}^{(1,i)}x_{i }+\mathbf{b}_{\text{emb}}^{(1,i)})+\mathbf{b}_{\text{emb}}^{(2,i)}\right)\in\mathbb{R}^ {4}.\] For efficiency reasons, we use 4-dimensional embeddings with \(\mathbf{w}_{\text{emb}}^{(1,i)},\mathbf{b}_{\text{emb}}^{(1,i)}\in\mathbb{R}^{16},\mathbf{ b}_{\text{emb}}^{(2,i)}\in\mathbb{R}^{3},\mathbf{W}_{\text{emb}}^{(2,i)}\in \mathbb{R}^{3\times 16}\).
* To encourage (soft) feature selection, we introduce a scaling layer before the first linear layer, which is simply a matrix-vector product with a diagonal weight matrix. In other words, it computes \(x_{i,\text{out}}=s_{i}\cdot x_{i,\text{in}}\), with a learnable scaling factor \(s_{i}\) for each feature \(i\). We found it beneficial to use a larger learning rate for this layer.
* Our linear layers use the neural tangent parametrization (NTP) as proposed by Jacot et al. [28], i.e., they compute \(\mathbf{z}^{(l+1)}=d_{l}^{-1/2}\mathbf{W}^{(l)}\mathbf{x}^{(l)}+\mathbf{b}^{(l)}\), where \(d_{l}\) is the dimension of the layer input \(\mathbf{x}^{(l)}\). The motivation behind the use of the NTP here is that it effectively modifies the learning rate for the weight matrices depending on the input dimension \(d_{l}\), hopefully preventing too large steps whenever the number of columns is large. We did not observe improvements when using the Adam version of the maximal update parametrization [68].

Figure 1: **Components of RealMLP-TD. Part (c) shows the result of adding one component in each step, where the best default learning rate is found separately for each step. The vanilla MLP uses categorical embeddings, a quantile transform to preprocess numerical features, default PyTorch initialization, ReLU activation, early stopping, and is optimized with Adam with default parameters. For more details, see Appendix A.4. The error bars are approximate 95% confidence intervals for the limit #splits \(\rightarrow\infty\), see Appendix C.6.**

* RealMLP-TD uses parametric activation functions inspired by PReLU [21]. In general, for an activation function \(\sigma\), we define a parametric version with separate learnable \(\alpha_{i}\) for each neuron \(i\): \[\sigma_{\alpha_{i}}(x_{i})=(1-\alpha_{i})x_{i}+\alpha_{i}\sigma(x_{i})\;.\] When \(\alpha_{i}=1\), this recovers \(\sigma\), and when \(\alpha_{i}=0\), the activation function is linear. As activation functions, we use SELU [35] for classification and Mish [45] for regression.
* We use dropout after each activation function. We do not use the Alpha-dropout variant originally proposed for SELU [35], as we were not able to obtain good results with it.
* For regression, at test time, the MLP outputs are clipped to the observed range during training. (We observed that this is mainly helpful for suboptimal hyperparameters.)

InitializationThe parameters \(s_{i}\) of the scaling layer are initialized to \(1\), making it an identity function at initialization. Similarly, the parameters \(\alpha_{i}\) of the parametric activation functions are initialized to \(1\), recovering the standard activation functions at initialization. We initialize weights and biases in a data-dependent fashion during a forward pass on the (possibly subsampled) training set. We rescale rows of standard-normal-initialized weight matrices to scale the variance of the output pre-activations over the dataset to one. For the biases, we use the data-dependent he+5 initialization method [called hull+5 in 61].

TrainingLike Gorishniy et al. [15], we use the AdamW optimizer [34, 40]. We set its momentum hyperparameters to \(\beta_{1}=0.9\) and \(\beta_{2}=0.95\) instead of the default \(\beta_{2}=0.999\). The idea to use a smaller value for \(\beta_{2}\) is adopted from the fastai tabular MLP [25]. RealMLP is optimized for 256 epochs with a batch size of 256. As a loss function for classification, we use softmax + cross-entropy with label smoothing [62] with parameter \(\varepsilon=0.1\). For regression, we use the MSE loss and affinely transform the targets to have zero mean and unit variance on the training and validation set.

HyperparametersWe allow parameter-specific scheduled hyperparameters computed in each iteration using a base value, optional parameter-specific factors, and a schedule, as

\[\operatorname{base\_value}\,\cdot\,\operatorname{param\_factor}\,\cdot\, \operatorname{schedule}\left(\frac{\operatorname{iteration}}{\#\operatorname{ iterations}}\right),\]

allowing us, for example, to use a high learning rate factor for scaling layer parameters. Because we do not tune the number of epochs separately on each dataset, we use a multi-cycle learning rate schedule, providing multiple valleys that are usually preferable for stopping the training, while allowing high learning rates in between. Our schedule is similar to Loshchilov and Hutter [39] and Smith [59], but with a simpler analytical expression:

\[\operatorname{coslog}_{k}(t)\coloneqq\frac{1}{2}(1-\cos(2\pi\log_{2}(1+(2^{k} -1)t)))\;.\]

We set \(k=4\) to obtain four cycles as shown in Figure 1 (b). To allow stopping at different levels of regularization, we schedule dropout and weight decay using the following schedule, cf. Figure 1 (b):2

Footnote 2: inspired by a similar schedule in [https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer](https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer)

\[\operatorname{flat\_cos}(t)\coloneqq\frac{1}{2}(1+\cos(\pi(\max\{1,2t\}-1) )).\]

The detailed hyperparameters can be found in Table A.1.

Best-epoch selectionDue to the multi-cycle learning rate schedule, we do not perform classical early stopping. Instead, we always train for the full 256 epochs and then revert the model to the epoch with the lowest validation error, which in this paper is based on classification error, or RMSE for regression. In case of a tie, we found it beneficial to use the last of the tied best epochs.

RealMLP-TD-SSince certain aspects of RealMLP-TD are somewhat complex to implement, we introduce a simplified (and faster) variant called RealMLP-TD-S in Appendix A. Among the simplifications are: omitting embedding layers, using non-parametric activations, using a simpler initialization method, and omitting dropout and weight decay.

RealTabR-DFor RealTabR-D, we adapt TabR-S-D by using our numerical preprocessing, setting Adam's \(\beta_{2}\) to \(0.95\), using our scaling layer with a modification to amplify the effective learning rate by a factor of 96, adding PBLD embeddings for numerical features, and adding label smoothing for classification. More details can be found in Appendix A.3.

## 4 Gradient-Boosted Decision Trees

To find better default hyperparameters for GBDTs, we employ a semi-automatic approach: We use hyperparameter optimization libraries like hyperopt [2] and SMAC3 [38] to explore a reasonably large hyperparameter space, evaluating the benchmark score of each configuration on the meta-train benchmarks, and then perform some small manual adjustments like rounding the best obtained hyperparameters. To balance efficiency and accuracy, we fix the number of estimators to 1000 and use the hist method for XGBoost. We only consider the libraries' default tree-building strategies since it is one of their main differences. The tuned defaults (TD) for LightGBM (LGBM), XGBoost (XGB), and CatBoost can be found in Table C.1, C.2, and C.3, respectively.

While some of the obtained hyperparameter values might be sensitive to the tuning and benchmark setup, we observe some general trends. First, row subsampling is used in all tuned defaults, while column subsampling is rarely applied. Second, trees are generally allowed to be deeper for regression than for classification. Third, the Bernoulli bootstrap in CatBoost is competitive with the Bayesian bootstrap while also being faster.

## 5 Experiments

In the following, we evaluate different methods with library defaults (D), tuned defaults (TD), and hyperparameter optimization (HPO). Recall that TD uses fixed parameters optimized on the meta-train benchmarks, while HPO tunes hyperparameters on each dataset split independently. All methods except random forests select the best iteration/epoch on the validation set of the respective dataset split based on accuracy / RMSE. All NN-based regression methods standardize the labels for training.

### Methods

We provide methods in the following variants:

* **D**: Default parameters, taken from the original library if possible (Appendix C.1).
* **TD**: Tuned default parameters from Section 3 and Section 4.
* **HPO**: Hyperparameters optimized separately for every train-test split on every dataset, using 50 steps of random search. Search spaces are specified in Appendix C.2 and are usually adapted from original or popular papers.

As tree-based methods, we use XGBoost (**XGB**), LightGBM (**LGBM**), and **CatBoost** from the respective libraries, as well as random forest (**RF**) from scikit-learn. The variant **XGB-PBB-D** uses meta-learned default parameters from Probst et al. [50]. For neural methods, we compare to **MLP**, **ResNet**, and FT-Transformer (**FTT**) from Gorishniv et al. [15], **MLP-PLR** from Gorishniv et al. [16], as well as **TabR** and **TabR-S** (without numerical embeddings) from Gorishniv et al. [17]. We compare these methods to **RealMLP** and **RealTabR** from Section 3. In addition, we investigate **Best**, which on each dataset split selects the method with the best validation score out of XGB, LGBM, CatBoost, and MLP-PLR (for Best-D) or RealMLP (for Best-TD and Best-HPO). **Ensemble** builds a weighted ensemble out of the same methods as Best, using the method of Caruana et al. [5] with 40 greedy selection steps as in Salinas and Erickson [55].

We do not run FTT, RF-HPO, and TabR-HPO on all benchmarks since some benchmarks (especially meta-test) are more expensive to run and these methods may run into out-of-memory errors.

### Results

Figure 2 shows the results of the aforementioned methods on all benchmarks, along with their runtimes on a CPU. _Note that XGB results on some (mainly meta-test) datasets are affected by a bug in handling rare categories, see Appendix B._

Figure 2: **Benchmark scores on all benchmarks vs. average training time.** The \(y\)-axis shows the shifted geometric mean (\(\mathrm{SGM}_{\varepsilon}\)) classification error (left) or nRMSE (right) as explained in Section 2.2. The \(x\)-axis shows average training times per 1000 samples (measured on \(\mathcal{B}^{\mathrm{train}}\) for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \(\rightarrow\infty\), see Appendix C.6. _Note that XGB results on some (mainly meta-test) datasets are affected by a bug in handling rare categories, see Appendix B._

How good are tuned defaults on new datasets?To answer this question, we compare the relative gaps between TD and HPO benchmark scores on the meta-test benchmarks to those on the meta-train benchmarks. The gap between RealMLP-HPO and RealMLP-TD is not much larger on the meta-test benchmarks, indicating that the tuned defaults transfer very well to the meta-test benchmark. For GBDTs, tuned defaults are competitive with HPO on the meta-train set, but not as good on the meta-test set. Still, they are considerably better than the untuned defaults on the meta-test set. Note that we did not limit the TD parameters to the literature search spaces for the HPO models (cf. Appendix C.2); for example, XGB-TD uses a smaller value of min_child_weight for classification and CatBoost-TD uses deeper trees and Bernoulli boosting. The XGBoost defaults XGB-PBB-D from Probst et al. [50] outperform XGB-TD on \(\mathcal{B}^{\mathrm{test}}_{\mathrm{class}}\), perhaps because their benchmark is more similar to \(\mathcal{B}^{\mathrm{test}}_{\mathrm{class}}\) or because XGB-PBB-D uses more estimators (4168) and deeper trees.

RealMLP and RealTabR perform strongly among NNs.On most benchmarks, RealMLP-TD and RealTabR-D bring considerable improvements over MLP-PLR-D and TabR-S-D, at slightly larger runtimes, respectively. Similarly, RealMLP-HPO improves the results of MLP-PLR-HPO. TabR and FTT are notably slower than MLP-based methods on CPUs, while the difference is less pronounced on GPUs (Figure C.2). While RealMLP-TD beats TabR-S-D on many benchmarks, RealTabR-D performs even better on four out of six benchmarks, especially all regression benchmarks. On the Grinsztajn et al. [18] benchmark where we can afford to run more baselines, TabR-HPO performs best according to many aggregation metrics. It performs especially well on the _electricity_ dataset, where MLPs struggle to learn high-frequency patterns [18].

RealMLP and RealTabR are competitive with tree-based models.On the meta-train and meta-test benchmarks, RealMLP and RealTabR perform better than GBDTs in terms of shifted geometric mean error, while also being comparable or slightly better in terms of other aggregations like mean normalized error (Appendix B.10) or win-rates (Appendix B.12). On the Grinsztajn et al. [18] benchmark, RealMLP performs worse than CatBoost for classification and comparably for regression, while RealTabR-D performs comparably to CatBoost-TD for classification and better for regression.

Among GBDTs, CatBoost defaults are better and slower.Several papers have found CatBoost to perform favorably among GBDTs while being more computationally expensive to train [8, 33, 43, 51, 69]. We observe the same for our tuned defaults on most benchmarks.

Figure 3: **Benchmark scores vs. average training time for AUC. Methods labeled “no LS” deactivate label smoothing. Stopping and best-epoch selection are performed on accuracy, while HPO is performed on AUC. See Figure B.3 for stopping on cross-entropy. The \(y\)-axis shows the shifted geometric mean (\(\mathrm{SGM}_{e}\)) \(1-\mathrm{AUC}\) as explained in Section 2.2. The \(x\)-axis shows average training times per 1000 samples (measured on \(\mathcal{B}^{\mathrm{train}}\) for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \(\rightarrow\infty\), see Appendix C.6.**

Simply trying all default algorithms is faster and very often better than (naive) single-algorithm HPO.When comparing Best-TD to 50-step HPO on RealMLP or GBDTs, we notice that Best-TD is faster on average, while also being competitive with the best of the HPO models. In comparison, Best-D is often outperformed by RealMLP-HPO. We also note that ensemble selection [5] usually gives 0-3% improvement on the benchmark score compared to selecting the best model, and can potentially be further improved [6]. Unlike McElfresh et al. [43], who argue in favor of CatBoost-HPO over trying NNs, our results favor model portfolios as used in modern AutoML systems [10].

Analyzing NN improvementsFigure 1 (c) shows how adding the proposed RealMLP components to a simple MLP improves the meta-train benchmark performance. However, these results depend on the order in which components are added, which is addressed by a separate ablation study in Appendix B. For example, the large weight decay value makes RealMLP-TD sensitive to changes in some other hyperparameters like \(\beta_{2}\). We also show in Appendix B.8 that our architectural improvements alone are beneficial when applied to MLP-D directly, although non-architectural aspects are at least as important. In particular, our numerical preprocessing is easy to adopt and often beneficial for other NNs as well (Appendix B.7). The scaling layer and PBLD embeddings are easy to use and turned out to be effective within RealTabR-D as well. If affordable, larger stopping patiences and the use of (cyclic) learning rate schedules can be useful, while label smoothing is influential but can be detrimental for metrics like AUROC (Figure 3, Appendix B.5).

Dependence on benchmark choicesWe observe that choices in benchmark design can affect the interpretation of the results. The use of different aggregation metrics than the shifted geometric mean reduces the advantage of TD methods (Appendix B.10). For classification, using AUROC instead of classification error (Figure 3, Appendix B.5) favors GBDTs. Different dataset selection and preprocessing criteria on different benchmarks lead to large differences between benchmarks in the average errors, as indicated by the \(y\)-axis scaling in Figure 2.

Further insightsIn Appendix B, we present additional experimental results. We compare bagging and refitting for RealMLP-TD and LGBM-TD, finding that refitting multiple models is often better on average. We demonstrate that GBDTs benefit from high early stopping patiences for classification, especially when using accuracy as the stopping metric. When considering AUROC as a stopping metric, we show that stopping on cross-entropy is preferable to accuracy (Appendix B.5).

LimitationsWhile our benchmarks cover medium-to-large tabular datasets in standard settings, it is unclear to which extent the obtained defaults can generalize to very small datasets, distribution shifts, datasets with missing numerical values, and other metrics such as log-loss. Additionally, runtimes and the resulting tradeoffs may change with different parallelization, hardware, or (time-aware) HPO algorithms. For computational reasons, we only use a single training-validation split per train-test split. This means that HPO can overfit the validation set more easily than in a cross-validation setup. While we extensively benchmark different NN models from the literature, we do not attempt to equalize non-architectural aspects, and our work should therefore not be seen as a comparison of architectures. We compared to TabR-S-D as a recent promising method with good default parameters [17, 69]. However, due to a surge of recently published deep tabular models [e.g., 7, 8, 29, 33, 41, 57, 67], it is unclear what the current "best" deep tabular model is. In particular, ExcelFormer [7] also promises strong-performing default parameters. For GBDTs, due to the cost of running the benchmarks, our limits on the depth and number of trees are on the lower side of the literature.

## 6 Conclusion

In this paper, we studied the potential of improved default parameters for GBDTs and an improved MLP, evaluated on a large separate meta-test benchmark as well as the benchmark by Grinsztajn et al. [18], and investigated the time-accuracy tradeoffs of various algorithm selection and ensembling scenarios. Our improved MLP mostly outperforms other NNs from the literature with moderate runtime and is competitive with GBDTs in terms of benchmark scores. Since many of the proposed improvements to NNs are orthogonal to the improvements in other papers, they offer exciting opportunities for combinations, as we demonstrated with our RealTabR variant. While the "NNs vs GBDTs" debate remains interesting, our results demonstrate that with good default parameters, it is worth trying both algorithm families even with a moderate training time budget.

## Acknowledgments and Disclosure of Funding

We thank Gael Varoquaux, Frank Sehnke, Katharina Strecker, Ravid Shwartz-Ziv, Lennart Purucker, and Francis Bach for helpful discussions. We thank Katharina Strecker for help with code refactoring.

Funded by Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy - EXC 2075 - 390740016. The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting David Holzmuller. LG acknowledges support in part by the French Agence Nationale de la Recherche under Grant ANR-20-CHIA-0026 (LearnI). Part of this work was performed on the computational resource bwUniCluster funded by the Ministry of Science, Research and the Arts Baden-Wurttemberg and the Universities of the State of Baden-Wurttemberg, Germany, within the framework program bwHPC. Part of this work was performed using HPC resources from GENCI-IDRIS (Grant 2023-AD011012804R1 and 2024-AD011012804R2).

Contribution statementDH and IS conceived the project. DH implemented and experimentally validated the newly proposed methods and wrote the initial paper draft. DH and LG contributed to benchmarking, plotting, and implementing baseline methods. LG and IS helped revise the draft. IS supervised the project and contributed dataset downloading code.

## References

* [1] Sercan O. Arik and Tomas Pfister. TabNet: Attentive interpretable tabular learning. In _AAAI Conference on Artificial Intelligence_, 2021.
* [2] James Bergstra, Daniel Yamins, and David Cox. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. In _International Conference on Machine Learning_, 2013.
* [3] Vadim Borisov, Tobias Leemann, Kathrin Sessler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci. Deep neural networks and tabular data: A survey. _IEEE Transactions on Neural Networks and Learning Systems_, 2022.
* [4] Pavel Brazdil, Christophe Giraud Carrier, Carlos Soares, and Ricardo Vilalta. _Metalearning: Applications to Data Mining_. Springer Science & Business Media, 2008.
* [5] Rich Caruana, Alexandru Niculescu-Mizil, Geoff Crew, and Alex Ksikes. Ensemble selection from libraries of models. In _International Conference on Machine Learning_, 2004.
* [6] Rich Caruana, Art Munson, and Alexandru Niculescu-Mizil. Getting the most out of ensemble selection. In _International Conference on Data Mining_, pages 828-833. IEEE, 2006.
* [7] Jintai Chen, Jiahuan Yan, Qiyuan Chen, Danny Z. Chen, Jian Wu, and Jimeng Sun. Can a Deep Learning Model be a _Sure Bet_ for Tabular Prediction? In _Conference on Knowledge Discovery and Data Mining_. ACM, August 2024.
* [8] Kuan-Yu Chen, Ping-Han Chiang, Hsin-Rung Chou, Ting-Wei Chen, and Tien-Hao Chang. Trompt: Towards a better deep neural network for tabular data. In _International Conference on Machine Learning_, 2023.
* [9] Tianqi Chen and Carlos Guestrin. XGBoost: A scalable tree boosting system. In _International Conference on Knowledge Discovery and Data Mining_, 2016.
* [10] Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy, Mu Li, and Alexander Smola. AutoGluon-Tabular: Robust and accurate AutoML for structured data. In _7th ICML Workshop on Automated Machine Learning_, 2020.
* [11] Matthias Feurer, Katharina Eggensperger, Stefan Falkner, Marius Lindauer, and Frank Hutter. Auto-sklearn 2.0: Hands-free automl via meta-learning. _The Journal of Machine Learning Research_, 23(261), 2022.
* [12] Sebastian Felix Fischer, Matthias Feurer, and Bernd Bischl. OpenML-CTR23-A curated tabular regression benchmarking suite. In _AutoML Conference 2023 (Workshop)_, 2023.

* Gijsbers et al. [2024] Pieter Gijsbers, Marcos LP Bueno, Stefan Coors, Erin LeDell, Sebastien Poirier, Janek Thomas, Bernd Bischl, and Joaquin Vanschoren. AMLB: an AutoML benchmark. _Journal of Machine Learning Research_, 25(101):1-65, 2024. URL [https://www.jmlr.org/papers/v25/22-0493.html](https://www.jmlr.org/papers/v25/22-0493.html).
* Gneiting and Raftery [2007] Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation. _Journal of the American Statistical Association_, 102(477):359-378, 2007.
* Gorishniy et al. [2021] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Revisiting deep learning models for tabular data. _Neural Information Processing Systems_, 2021.
* Gorishniy et al. [2022] Yury Gorishniy, Ivan Rubachev, and Artem Babenko. On embeddings for numerical features in tabular deep learning. _Neural Information Processing Systems_, 2022.
* Gorishniy et al. [2024] Yury Gorishniy, Ivan Rubachev, Nikolay Kartashev, Daniil Shlenskii, Akim Kotelnikov, and Artem Babenko. TabR: Tabular deep learning meets nearest neighbors. In _International Conference on Learning Representations_, 2024.
* Grinsztajn et al. [2022] Leo Grinsztajn, Edouard Oyallon, and Gael Varoquaux. Why do tree-based models still outperform deep learning on typical tabular data? _Neural Information Processing Systems_, 2022.
* Guo and Berkhahn [2016] Cheng Guo and Felix Berkhahn. Entity embeddings of categorical variables. _arXiv:1604.06737_, 2016.
* Hafner et al. [2023] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. _arXiv:2301.04104_, 2023.
* He et al. [2015] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In _IEEE International Conference on Computer Vision_, pages 1026-1034, 2015.
* Herbold [2020] Steffen Herbold. Autorank: A Python package for automated ranking of classifiers. _Journal of Open Source Software_, 5(48):2173, 2020. doi: 10.21105/joss.02173. URL [https://doi.org/10.21105/joss.02173](https://doi.org/10.21105/joss.02173). Publisher: The Open Journal.
* Hollmann et al. [2022] Noah Hollmann, Samuel Muller, Katharina Eggensperger, and Frank Hutter. TabPFN: A transformer that solves small tabular classification problems in a second. In _International Conference on Learning Representations_, 2022.
* Holzmuller et al. [2023] David Holzmuller, Viktor Zaverkin, Johannes Kastner, and Ingo Steinwart. A framework and benchmark for deep batch active learning for regression. _Journal of Machine Learning Research_, 24(164), 2023.
* Howard and Gugger [2020] Jeremy Howard and Sylvain Gugger. Fastai: A layered API for deep learning. _Information_, 11(2):108, 2020.
* Huang et al. [2017] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks. In _Computer Vision and Pattern Recognition_, pages 4700-4708, 2017.
* Huang et al. [2020] Xin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar Karnin. TabTransformer: Tabular data modeling using contextual embeddings. _arXiv:2012.06678_, 2020.
* Jacot et al. [2018] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. _Neural Information Processing Systems_, 2018.
* Joseph and Raj [2024] Manu Joseph and Harsh Raj. GANDALF: Gated Adaptive Network for Deep Automated Learning of Features. _arXiv:2207.08548_, 2024.
* Kadra et al. [2021] Arlind Kadra, Marius Lindauer, Frank Hutter, and Josif Grabocka. Well-tuned simple nets excel on tabular datasets. In _Neural Information Processing Systems_, 2021.

* Ke et al. [2017] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. LightGBM: A highly efficient gradient boosting decision tree. In _Neural Information Processing Systems_, 2017.
* Kelly et al. [2024] Markelle Kelly, Rachel Longjohn, and Kolby Nottingham. The UCI Machine Learning Repository. URL [https://archive.ics.uci.edu](https://archive.ics.uci.edu).
* Kim et al. [2024] Myung Jun Kim, Leo Grinsztajn, and Gael Varoquaux. CARTE: pretraining and transfer for tabular learning. In _International Conference on Machine Learning_, 2024.
* Kingma and Ba [2015] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _International Conference on Learning Representations_, 2015.
* Klambauer et al. [2017] Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. In _Neural Information Processing Systems_, 2017.
* Kohli et al. [2024] Ravin Kohli, Matthias Feurer, Katharina Eggensperger, Bernd Bischl, and Frank Hutter. Towards Quantifying the Effect of Datasets for Benchmarking: A Look at Tabular Machine Learning. In _ICLR 2024 Data-centric Machine Learning Research Workshop_, 2024.
* Kossen et al. [2021] Jannik Kossen, Neil Band, Clare Lyle, Aidan N. Gomez, Thomas Rainforth, and Yarin Gal. Self-attention between datapoints: Going beyond individual input-output pairs in deep learning. In _Neural Information Processing Systems_, 2021.
* Lindauer et al. [2022] Marius Lindauer, Katharina Eggensperger, Matthias Feurer, Andre Biedenkapp, Difan Deng, Carolin Benjamins, Tim Ruhkopf, Rene Sass, and Frank Hutter. SMAC3: A versatile Bayesian optimization package for hyperparameter optimization. _Journal of Machine Learning Research_, 23(54), 2022.
* Loshchilov and Hutter [2017] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In _International Conference on Learning Representations_, 2017.
* Loshchilov and Hutter [2018] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _International Conference on Learning Representations_, 2018.
* Marton et al. [2024] Sascha Marton, Stefan Ludtke, Christian Bartelt, and Heiner Stuckenschmidt. GRANDE: Gradient-based decision tree ensembles for tabular data. In _International Conference on Learning Representations_, 2024.
* McCarter [2023] Calvin McCarter. The kernel density integral transformation. _Transactions on Machine Learning Research_, 2023.
* McElfresh et al. [2023] Duncan McElfresh, Sujay Khandagale, Jonathan Valverde, Vishak Prasad C, Ganesh Ramakrishnan, Micah Goldblum, and Colin White. When do neural nets outperform boosted trees on tabular data? In _Neural Information Processing Systems_, 2023.
* Mishkin and Matas [2016] Dmytro Mishkin and Jiri Matas. All you need is a good init. In _International Conference on Learning Representations_, 2016.
* Misra [2020] Diganta Misra. Mish: A self regularized non-monotonic activation function. In _British Machine Vision Conference_, 2020.
* Moritz et al. [2018] Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, and Michael I. Jordan. Ray: A distributed framework for emerging AI applications. In _USENIX Symposium on Operating Systems Design and Implementation_, 2018.
* Paszke et al. [2019] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, and Luca Antiga. PyTorch: An imperative style, high-performance deep learning library. _Neural Information Processing Systems_, 32, 2019.
* Pedregosa et al. [2011] Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, and Vincent Dubourg. Scikit-learn: Machine learning in Python. _Journal of Machine Learning Research_, 12(85), 2011.

* [49] Florian Pfisterer, Jan N. Van Rijn, Philipp Probst, Andreas C. Muller, and Bernd Bischl. Learning multiple defaults for machine learning algorithms. In _Genetic and Evolutionary Computation Conference_, July 2021.
* [50] Philipp Probst, Anne-Laure Boulesteix, and Bernd Bischl. Tunability: Importance of hyperparameters of machine learning algorithms. _Journal of Machine Learning Research_, 20(53), 2019.
* [51] Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey Gulin. CatBoost: Unbiased boosting with categorical features. In _Neural Information Processing Systems_, 2018.
* [52] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In _Neural Information Processing Systems_, 2007.
* [53] Hubert Ramsauer, Bernhard Schafl, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, Thomas Adler, David Kreil, and Michael K. Kopp. Hopfield networks is all you need. In _International Conference on Learning Representations_, 2020.
* [54] Ivan Rubachev, Nikolay Kartashev, Yury Gorishniy, and Artem Babenko. TabReD: Analyzing Pitfalls and Filling the Gaps in Tabular Deep Learning Benchmarks. _arXiv:2406.19380_, 2024.
* [55] David Salinas and Nick Erickson. TabRepo: A large scale repository of tabular model evaluations and its AutoML applications. In _AutoML Conference_, 2024.
* [56] Bernhard Schafl, Lukas Gruber, Angela Bitto-Nemling, and Sepp Hochreiter. Modern Hopfield networks as memory for iterative learning on tabular data. In _NeurIPS Workshop on Associative Memory & Hopfield Networks in 2023_, 2023.
* [57] Junhong Shen, Liam Li, Lucio M. Dery, Corey Staten, Mikhail Khodak, Graham Neubig, and Ameet Talwalkar. Cross-modal fine-tuning: Align then refine. In _International Conference on Machine Learning_, 2023.
* [58] Ravid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. _Information Fusion_, 81:84-90, 2022.
* [59] Leslie N. Smith. Cyclical learning rates for training neural networks. In _Winter Conference on Applications of Computer Vision_, 2017.
* [60] Gowthami Somepalli, Micah Goldblum, Avi Schwarzschild, C. Bayan Bruss, and Tom Goldstein. SAINT: Improved neural networks for tabular data via row attention and contrastive pre-training. In _NeurIPS 2022 Table Representation Learning Workshop_, 2022.
* [61] Ingo Steinwart. A sober look at neural network initializations. _arXiv:1903.11482_, 2019.
* [62] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In _Computer Vision and Pattern Recognition_, 2016.
* [63] Jan N. van Rijn, Florian Pfisterer, Janek Thomas, Andreas Muller, Bernd Bischl, and Joaquin Vanschoren. Meta learning for defaults: Symbolic defaults. In _NeurIPS 2018 Workshop on Meta-Learning_, 2018.
* [64] Joaquin Vanschoren. Meta-learning: A survey. _arXiv:1810.03548_, 2018.
* [65] Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. OpenML: Networked science in machine learning. _ACM SIGKDD Explorations Newsletter_, 15(2):49-60, 2014. Publisher: ACM New York, NY, USA.
* [66] Martin Wistuba, Nicolas Schilling, and Lars Schmidt-Thieme. Learning hyperparameter optimization initializations. In _International Conference on Data Science and Advanced Analytics_, pages 1-10, 2015.

* [67] Chenwei Xu, Yu-Chao Huang, Jerry Yao-Chieh Hu, Weijian Li, Ammar Gilani, Hsi-Sheng Goan, and Han Liu. BiSHop: Bi-directional cellular learning for tabular data with generalized sparse modern Hopfield model. In _International Conference on Machine Learning_, 2024.
* [68] Ge Yang, Edward Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tuning large neural networks via zero-shot hyperparameter transfer. In _Neural Information Processing Systems_, 2021.
* [69] Han-Jia Ye, Si-Yang Liu, Hao-Run Cai, Qi-Le Zhou, and De-Chuan Zhan. A closer look at deep learning on tabular data. _arXiv:2407.00956_, 2024.

## Appendix Contents.

* A Further Details on Neural Networks
* A.1 RealMLP-TD Details
* A.2 RealMLP-TD-S Details
* A.3 RealTabR-D Details
* A.4 Details on Cumulative Ablation
* A.5 Discussion
* B More Experiments
* B.1 MLP Ablations
* B.2 MLP Preprocessing
* B.3 Bagging, Refitting, and Ensembling
* B.4 Early stopping for GBDTs
* B.5 Results for AUROC
* B.6 Results Without Missing-Value Datasets
* B.7 Comparing Preprocessing Methods for NNs
* B.8 Results for Varying Architecture
* B.9 Comparing HPO Methods
* B.10 More Time-Error Plots
* B.11 Critical Difference Diagrams
* B.12 Win-rate Plots
* C Benchmark Details
* C.1 Default Configurations
* C.2 Hyperparameter Optimization
* C.3 Dataset Selection and Preprocessing
* C.4 Comparison with Standard Grinsztajn et al. [18] Benchmark
* C.5 Closer-to-original Version of the Grinsztajn et al. [18] Benchmark
* C.6 Confidence Intervals
* C.7 Time Measurements
* C.8 Compute Resources
* C.9 Used Libraries
* D Results for Individual Datasets
* E Broader Impact
Further Details on Neural Networks

The detailed hyperparameter settings for RealMLP-TD and RealMLP-TD-S are listed in Table 1.

### RealMLP-TD Details

ArchitectureTo make the binary and multi-class cases more similar, we use two output neurons in the binary case, using the same loss function as in the multi-class case.

InitializationWe initialize categorical embedding parameters from \(\mathcal{N}(0,1)\). We initialize the components of \(\mathbf{w}_{\text{emb}}^{(1,i)}\) from \(\mathcal{N}(0,0.1^{2})\) and of \(\mathbf{b}_{\text{emb}}^{(1,i)}\) from \(\mathcal{U}[-\pi,\pi]\). The other numerical embedding parameters are initialized according to PyTorch's default initialization, that is, from the uniform distribution \(\mathcal{U}[-1/\sqrt{16},1/\sqrt{16}]\). For weights and biases of the linear layers, we use a data-dependent initialization. The initialization is performed on the fly during a first forward pass of the network on the training set (which can be subsampled adaptively not to use more than 1 GB of RAM). We realize this by providing fit_transform() methods similar to a pipeline in scikit-learn. For the weight matrices, we use a custom two-step procedure: First, we initialize all entries from \(\mathcal{N}(0,1)\). Then, we rescale each row of the weight matrix such that the outputs \(\frac{1}{\sqrt{d_{\mathcal{U}}}}\mathbf{W}^{(1)}\mathbf{x}_{j}^{(l)}\) have variance \(1\) over the dataset (i.e. when considering the sample index \(j\in\{1,\dots,n\}\) as a uniformly distributed random variable). This is somewhat similar to the LSUV initialization method [44]. For the biases, we use the data-dependent he+5 initialization method [called hull+5 in 61].

TrainingWe implement weight decay as in PyTorch using \(\theta\leftarrow\theta-\operatorname{lr}\cdot\operatorname{wd}\cdot\theta\), which includes the learning rate unlike the original version [40].

### RealMLP-TD-S Details

For RealMLP-TD-S, we make the following changes compared to RealMLP-TD:

* We apply one-hot encoding to all categorical variables and do not apply categorical embeddings.
* We do not apply numerical embeddings.
* We use the standard non-parametric versions of the SELU and Mish activation functions.
* We do not use dropout and weight decay.
* We use simpler weight and bias initializations: We initialize weights and biases from \(\mathcal{N}(0,1)\), except in the last layer, where we initialize them to zero.
* We do not clip the outputs, even in the regression case.
* We apply a different base learning rate in the regression case.

### RealTabR-D Details

To obtain RealTabR-D, we modify TabR-S-D in the following ways:

* We replace the standard numerical preprocessing (a modified quantile transform) with our robust scaling and smooth clipping.
* We set Adam's \(\beta_{2}\) to \(0.95\) instead of \(0.999\).
* We use our scaling layer, but modify it to obtain a higher effective learning rate. We do this by modifying the forward pass to \[x_{i,\operatorname{out}}=\gamma\cdot s_{i}\cdot x_{i,\operatorname{in}}\;,\] while initializing \(s_{i}\) to \(1/\gamma\). This will multiply the gradients of \(s_{i}\) by \(\gamma\), which will be ignored by Adam's normalization (when neglecting Adam's \(\varepsilon\) parameter). It will also multiply the optimizer updates by \(\gamma\), leading to approximately the same effect as multiplying the learning rate by \(\gamma\). However, a difference is that multiplying the learning rate by \(\gamma\) will also lead to stronger weight decay updates in PyTorch's AdamW implementation, while the introduction of \(\gamma\) does not increase the relative magnitude of weight decay updates. We chose the version with \(\gamma\) for simplicity of implementation. While RealMLP-TD uses a learning rate factor of \(6\) for the scaling layer, it uses a higher base learning rate due to the use of the neural tangent parametrization. For all layers except the first one, which have width 256, the neural tangent parametrization in RealMLP-TD uses a factor similar to \(\gamma\), which is set to \(1/16=1/\sqrt{256}\). Hence, RealMLP-TD without NTP should use a base learning rate for these layers that is smaller by a factor of \(1/16\), and therefore use a learning rate factor of \(6\cdot 16=96\) for the scaling layer. Consequently, we set \(\gamma\coloneqq 96\) for RealTabR-D without further tuning, noting that it performed significantly better than \(\gamma=6\) on the meta-train benchmarks.
* We use our PBLD embeddings for numerical features before the scaling layer, instead of no numerical embeddings in TabR-S-D. In order to make every experiment run on a GPU with 24GB RAM, we decrease the dimension of the hidden embedding layer from 16 to 8, although using 16 would have performed slightly better in our experiments on the meta-train benchmarks.
* For classification, we use label smoothing with parameter \(\varepsilon=0.1\).

Since we adapted hyperparameters like learning rate and weight decay from TabR-S-D without meta-learning them, we refer to the resulting method as RealTabR-D and not RealTabR-TD. We did not include other tricks from RealMLP-TD for various reasons:

* Brief experiments with NTP and the Mish activation deteriorated the performance.
* Parametric activations and increased stopping patience showed small improvements but were excluded due to a larger runtime.
* Other tricks were not tried due to limited time of experimentation, expected increases in the already somewhat large runtime, and/or implementation complexity.

### Details on Cumulative Ablation

Here, we provide more details on the vanilla MLP and the ablation steps from Figure 1 (c). For each step, we choose the best default learning rate out of a learning rate grid, using \(\{0.0004,0.0007,0.001,0.0015,0.0025,0.004,0.007,0.01,0.015\}\) for NNs using standard parametrization and \(\{0.01,0.02,0.03,0.04,0.07,0.1,0.2,0.3,0.4\}\) for NNs using neural tangent parametrization.

* Vanilla MLP: We use three hidden layers with 256 hidden neurons in each layer, just like RealMLP-TD, and the ReLU activation function. Each linear layer uses standard parametrization and the PyTorch default initialization, which is uniform from \([-1/\sqrt{\text{fan\_in}},1/\sqrt{\text{fan\_in}}]\) for both weights and biases, where fan_in is the input dimension. Categorical features are embedded using embedding layers, using eight-dimensional embeddings for each feature. Numerical features are transformed using a scikit-learn QuantieTransformer to approximately normal-distributed features. Optimization is performed using Adam with constant learning rate and default parameters \(\beta_{1}=0.9,\beta_{2}=0.999,\varepsilon=10^{-8}\) for at most 256 epochs with batch size 256, with constant learning rate. If the best validation error (classification error or RMSE) does not improve for 40 epochs, training is stopped. In each case, the model is reverted to the parameters of the epoch with the best validation score, using the first best epoch in case of a tie.
* Robust scale + smooth clip: We replace the QuantieTransformer with robust scaling and smooth clipping.
* One-hot for small cat.: As in RealMLP-TD, we use one-hot encoding for categories with at most eight values, not counting missing values.
* No early stopping: We always train the full 256 epochs.
* Last best epoch: In case of a tie, we use the last of the best epochs.
* \(\text{coslog}_{4}\) lr sched: We use the \(\text{coslog}_{4}\) learning rate schedule instead of a constant one.
* Adam \(\beta_{2}=0.95\): We set \(\beta_{2}=0.95\).
* Label smoothing (class.): We enable label smoothing with \(\varepsilon=0.1\) in the classification case.
* Output clipping (reg.): For regression, outputs are clipped to the min-max range observed during training.
* NT parametrization: We use the neural tangent parametrization for linear layers, setting the bias learning rate factor to \(0.1\).
* Act. fn. SELU / Mish: We change the activation function from ReLU to SELU (classification) or Mish (regression).
* Parametric act. fn.: We use parametric versions of the activation functions, with a learning rate factor of \(0.1\) for the parameters.
* Scaling layer: We use a scaling layer with a learning rate factor of \(6\) before the first linear layer.
* Num. embeddings: PL: We apply the PL embeddings [16] to numerical features.
* Num. embeddings: PBLD: We apply our PBLD embeddings instead.
* Dropout \(p=0.15\): We apply dropout with probability \(0.15\).
* Dropout sched: \(\text{flat\_cos}\): We apply the \(\text{flat\_cos}\) schedule to the dropout probability.
* Weight decay \(\text{wd}=0.02\): We apply weight decay (as in AdamW, PyTorch version) with value \(0.02\).
* wd sched: \(\text{flat\_cos}\): We apply the \(\text{flat\_cos}\) schedule to weight decay.
* Bias init: he+5: We apply the he+5 bias initialization method from Steinwart [61] (originally called hull+5).
* Weight init: data-driven: We apply our data-driven weight initialization method.

### Discussion

Here, we discuss some of the design decisions behind RealMLP-TD and possible trade-offs. First, our implementation allows us to train RealMLP-TD in a vectorized fashion on multiple train-validation-test splits at the same time. On the one hand, this can lead to speedups on GPUs when training multiple models in parallel, including on the benchmarks. On the other hand, it can hinder the implementation of certain methods like patience-based early stopping or loss-based learning rate schedules. While our ablations in Appendix B.1 show the advantage of our multi-cycle schedule over decreasing learning rate schedules, the latter ones could potentially enable a faster average training time through low-patience early stopping. An interesting follow-up question could be whether the multi-cycle schedule still works well with larger-patience early stopping.

Regarding categorical embeddings, our meta-train benchmark does not contain many high-cardinality categorical variables, and we were not able to conclude whether categorical embeddings are helpful or harmful compared to one-hot encoding (see Appendix B.1). Our motivation to include categorical embeddings stems from Guo and Berkhahn [19] as well as their potential to be more efficient for high-cardinality categorical variables. However, in practice, we find pure one-hot encoding to be faster on most datasets. Regarding the embedding size, we found that 4 already gave good results for numerical embeddings and decided to use 8 for categorical variables.

Additionally, other speed-accuracy tradeoffs are possible. Especially for regression, we observed that more epochs and larger hidden layers can be helpful. When faster networks are desired, the omission of numerical and categorical embedding layers as well as parametric activations from RealMLP-TD can be helpful, while the other omissions in RealMLP-TD-S do not considerably affect the training time. Of course, using larger batch sizes can also be helpful for larger datasets.

One caveat for classification is that cross-entropy with label smoothing is not a proper scoring rule, that is, in the infinite-sample limit, it is not minimized by the true probabilities \(P(y|x)\)[14]. Hence, label smoothing might not be suitable when other classification error metrics are used, as demonstrated in Appendix B.5 for AUROC.

## Appendix B More Experiments

In this section, we present more experimental results. Note that XGBoost results are affected by a bug where, if a categorical value is not present in the training or validation set, it could cause adjacent categorical values to be encoded differently during training, validation, and evaluation. This affects the results mainly on the meta-test benchmarks, where the SGM scores for XGB-TD and XGB-D are around 2% lower after fixing the bug. These differences are not large enough to affect our qualitative conclusions. Due to the large computational cost, we did not rerun XGB-HPO and XGB-PBB-D after fixing the bug, and we provide the old XGB-TD and XGB-D results for a fair comparison to XGB-HPO and XGB-PBB-D.

### MLP Ablations

To assess the importance of different improvements in RealMLP-TD, we perform an ablation study. We perform the ablation study only on the _meta-train_ benchmarks, first because they are considerably faster to run, and second because we tune the default parameters only on the meta-train benchmarks. Since the hyperparameters of RealMLP-TD have been tuned on the meta-train benchmarks, the ablation scores are not unbiased but represent some of the considerations that have been made when tuning the defaults. For each ablation, we multiply the default learning rate by learning rate factors from the grid \(\{0.1,0.15,0.25,0.35,0.5,0.7,1.0,1.4,2.0,3.0,4.0\}\) and pick the best one. Table B.1 shows the results of the ablation study in terms of the relative increase of the benchmark score for each ablation.

In general, we observe that ablations often lead to much larger changes for regression than for classification. Perhaps this is because nRMSE is more sensitive to outliers compared to classification error. Another factor could be that the classification benchmark contains more datasets than the regression benchmark. For the specific ablations, we observe a few things:

* For the **numerical embeddings**, we see that PBLD outperforms PL, PLR, and no numerical embeddings. Contrary to Gorishniy et al. [16], PL embeddings perform better than PLRembeddings in our setting. While the configurations with PLR and no numerical embeddings appear extremely bad for regression, we observed that they can perform more benignly with lower weight decay values.
* Using the Adam default value of \(\beta_{2}=0.999\) instead of our default \(\beta_{2}=0.95\) leads to considerably worse performance, especially for regression. As for numerical embeddings, we observed that the difference is less pronounced at lower weight decay values.
* Using a cosine decay **learning rate schedule** instead of our multi-cycle schedule leads to small deteriorations. A constant learning rate schedule performs even worse, especially for regression.
* Not employing **label smoothing** for classification is detrimental by around 1.8%.
* The **learnable scaling** layer yields improvements around 1.2% on both benchmarks.
* The use of **parametric activations** results in a considerable 4.8% improvement for regression but is insignificant for classification. We observed that parametric activations can sometimes alleviate optimization difficulties with weight decay.
* The differences between **activation functions** are rather small. For classification, Mish is competitive with SELU in this ablation but we found it to be worse in some other hyperparameter settings, so we keep SELU as the default. For regression, Mish performs best.
* For **dropout** and **weight decay**, we observe that they yield comparable but not always significant benefits for classification and regression. Scheduling dropout and weight decay parameters with the flat_cos schedule is helpful for regression, but not for classification in this setting.
* When comparing the **standard parametrization** (SP) to the neural tangent parametrization (NTP), we disable weight decay for a fair comparison. Moreover, for SP, we set the learning rate factors for weight and bias layers to \(1/16=1/\sqrt{256}\). This is because, for the weights in NTP, the effective updates by Adam are damped by this factor in all hidden layers except the first one. Compared to NTP without weight decay, SP without weight decay performs insignificantly worse on both benchmarks. It is unclear to us why the parametrization, which has a considerable influence on how the effective learning speed of the first linear layer scales with the number of features, is apparently of little importance.
* When comparing the data-dependent **initialization** of RealMLP-TD to a vanilla initialization with standard normal weights and zero biases, we see that the data-dependent initialization gains around 1% on both benchmarks.
* For selecting the best epoch, we consider selecting the **first best epoch** instead of the last best epoch in case of a tie. This is only relevant for classification metrics like classification error, where ties are somewhat likely to occur, especially on small and "easy" datasets. We observe a non-significant 0.4% deterioration in the benchmark score.
* We do not observe a significant difference when using **one-hot encoding** for all categorical variables, since our benchmarks contain only very few datasets with large-cardinality categorical variables.

### MLP Preprocessing

In Table 2, we compare different preprocessing methods for numerical features. Since we want to compare these methods in a relatively conventional setting, we apply them to RealMLP-TD-S (without numerical embeddings) and before one-hot encoding. We compare the following methods:

* Robust scaling and smooth clipping, our method used in RealMLP-TD and RealMLP-TD-S and described in Section 3.
* Robust scaling without smooth clipping.
* Standardization, i.e. subtracting the mean and dividing by the standard deviation. If the standard deviation of a feature is zero, we set the feature to zero.
* Standardization followed by smooth clipping.
* The quantile transformation from scikit-learn [48] with normal output distribution, which is popular in recent works [16, 17, 43, 18].
* A variant of the quantile transform, which we call the RTDL version, used by Gorishniy et al. [15] and Gorishniy et al. [17]. This version uses a dataset-size-dependent number of quantiles and adds some noise before fitting the transformation. It also uses a normal output distribution.

* The recent kernel density integral transform [42] with normal output distribution, which interpolates between the quantile transformation and min-max scaling, with default parameter \(\alpha=1\).

Table B.2 shows that on the meta-train benchmark, robust scaling and smooth clipping performs best for both classification and regression.

### Bagging, Refitting, and Ensembling

In our benchmark, for each training-test split, we only train one model on one training-validation split for efficiency reasons. However, ensembling and cross-validation techniques usually allow additional improvements to models. Here, we study multiple variants for RealMLP-TD and LGBM-TD. Let \(\mathcal{D}\) be the available data for training and validation, split into five equal-size subsets \(\mathcal{D}_{1},\ldots,\mathcal{D}_{5}\). (When \(|\mathcal{D}|\) is not divisible by five, \(\mathcal{D}_{1}\cup\ldots\cup\mathcal{D}_{5}\subsetneq\mathcal{D}\) since we need equal-size validation sets for vectorized NNs.) Let \(f_{\mathcal{D},t}(X)\) be the predictions on inputs \(X\) of the model trained on training set \(\mathcal{D}\) after \(t\in\{1,\ldots,T\}\) epochs (for NNs) or iterations (for LGBM). For classification, we consider the class probabilities as predictions. Let \(L_{\mathcal{D}^{\prime}}(f_{\mathcal{D},t})\) be the loss of \(f_{\mathcal{D},t}\) on dataset \(\mathcal{D}^{\prime}\). Then, we compare the test errors of an ensemble of \(M=1\) or \(M=5\) models, trained using bagging or refitting, with individual or joint stopping (best-epoch selection), which is formally given as follows:

\[y_{\text{pred}}\coloneqq\frac{1}{M}\sum_{i=1}^{M}f_{\mathcal{ \tilde{D}}_{i},t_{i}^{*}}(X_{\text{test}}),\qquad(M\text{ models})\]

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & \multicolumn{2}{c}{meta-train-class} & \multicolumn{2}{c}{meta-train-reg} \\ Ablation & Error increase in \% & best lr factor & Error increase in \% & best lr factor \\ \hline MLP-TD (without ablation) & 0.0 [0.0, 0.0] & 1.0 & 0.0 [0.0, 0.0] & 1.0 \\ Num. embeddings: PL & 0.7 [-0.0, 1.4] & 1.0 & 0.5 [-0.5, 1.6] & 1.0 \\ Num. embeddings: PLR & 4.2 [2.8, 5.7] & 1.0 & 19.0 [13.7, 24.5] & 0.25 \\ Num. embeddings: None & 2.3 [1.7, 2.9] & 1.0 & 20.6 [19.4, 21.8] & 0.25 \\ Adam \(\beta_{2}=0.999\) instead of \(\beta_{2}=0.95\) & 2.0 [1.6, 2.4] & 2.0 & 22.8 [21.3, 24.4] & 0.35 \\ Learning rate schedule = cosine decay & 1.1 [0.6, 1.5] & 1.0 & 0.4 [-0.5, 1.2] & 3.0 \\ Learning rate schedule = constant & 1.8 [0.9, 2.8] & 0.25 & 13.5 [11.9, 15.0] & 0.15 \\ No label smoothing & 1.8 [1.2, 2.5] & 4.0 & & \\ No learnable scaling & 1.4 [0.7, 2.1] & 2.0 & 1.0 [-0.0, 2.0] & 2.0 \\ Non-parametric activation & 0.5 [-0.2, 1.2] & 3.0 & 4.8 [3.4, 6.2] & 0.35 \\ Activation=Mish & -0.0 [-0.6, 0.6] & 3.0 & & \\ Activation=ReLU & 0.5 [-0.1, 1.2] & 2.0 & 0.7 [-0.1, 1.6] & 1.0 \\ Activation=SELU & & 2.3 [1.2, 3.6] & 1.0 \\ No dropout & 0.8 [0.2, 1.3] & 3.0 & 0.8 [-0.5, 2.1] & 1.4 \\ Dropout prob. 0.15 (constant) & -0.1 [-1.0, 0.8] & 1.4 & 3.6 [3.0, 4.2] & 1.0 \\ No weight decay & 0.8 [-0.2, 1.8] & 0.5 & 0.9 [-0.1, 1.9] & 0.5 \\ Weight decay = 0.02 (constant) & -0.3 [-0.7, 0.1] & 3.0 & 3.1 [1.7, 4.4] & 1.4 \\ Standard param + no weight decay & 1.1 [0.2, 2.1] & 0.5 & 1.3 [0.7, 1.8] & 0.7 \\ No data-dependent init & 0.9 [0.1, 1.8] & 3.0 & 1.2 [0.2, 2.2] & 1.4 \\ First best epoch instead of last best & 0.4 [-0.1, 1.0] & 4.0 & 0.0 [-0.0, 0.0] & 1.0 \\ Only one-hot encoding & -0.0 [-0.1, 0.0] & 1.0 & 0.0 [-0.0, 0.0] & 1.0 \\ \hline \hline \end{tabular}
\end{table}
Table B.1: **Ablation experiments for RealMLP-TD.** We re-tune the learning rate (picking the one with the best \(\mathrm{SGM}_{e}\) benchmark score) for each ablation separately. For each ablation, we specify the increase in the benchmark score (\(\mathrm{SGM}_{e}\)) relative to RealMLP-TD, with approximate 95% confidence intervals (Appendix C.6), and the best learning rate factor found. In the cases where values are missing, the corresponding option is already the default.

\[\tilde{\mathcal{D}}_{i} :=\begin{cases}\mathcal{D}\setminus\mathcal{D}_{i}&\text{(bagging)} \\ \mathcal{D}&\text{(refitting)},\end{cases}\] \[t_{i}^{*} :=\begin{cases}\operatorname*{argmin}_{t\in\{1,\ldots,T\}}L_{ \mathcal{D}_{i}}(f_{\mathcal{D}\setminus\mathcal{D}_{i},t})&\text{(indiv. stopping)}\\ \operatorname*{argmin}_{t\in\{1,\ldots,T\}}\sum_{j=1}^{5}L_{\mathcal{D}_{j}}(f _{\mathcal{D}\setminus\mathcal{D}_{j},t})&\text{(joint stopping)}.\end{cases}\]

Here, each model is trained with a different random seed. For LGBM, since we use an early stopping patience of 300 for each of the individual models, the \(\operatorname*{argmin}\) in the definition of \(t_{i}^{*}\) can only go up to the minimum stopping iteration \(T\) across the considered models.

The results of our experiments can be found in Table B.3 for LGBM-TD and in Table B.4 for RealMLP-TD. As expected, five models are considerably better than one. We find that refitting is mostly better than bagging, although a disadvantage of refitted models is that no validation scores are available, and it is unclear how HPO would affect this comparison. Comparing individual stopping to joint stopping, we find that individual stopping has a slight advantage in five-model bagging, while joint stopping performs better for single-model refitting. In the other two scenarios, joint stopping appears slightly better for RealMLP-TD and slightly worse for LGBM-TD. We also observe that the benefit of using five models instead of one appears to be larger for RealMLP-TD than for LGBM-TD.

### Early stopping for GBDTs

In Figure B.1 and Figure B.2, we study the influence of different early stopping patiences and metrics on the resulting benchmark performance of XGB-TD, LGBM-TD, and CatBoost-TD. While the regression results only deteriorate slightly for low patiences of 10 or 20 iterations, classification results are much more hurt by low patiences. In the classification setting, we evaluate the use of

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & Error **increase** relative to robust scale + smooth clip in \% \\ Method & meta-train-class & meta-train-reg \\ \hline Robust scale + smooth clip & **0.0** [0.0, 0.0] & **0.0** [0.0, 0.0] \\ Robust scale & 0.5 [-0.4, 1.4] & 9.5 [4.4, 14.8] \\ Standardize + smooth clip & 1.6 [0.9, 2.2] & 1.2 [0.6, 1.8] \\ Standardize & 2.1 [1.2, 3.0] & 8.8 [3.9, 13.9] \\ Quantile transform (output dist. = normal) & 2.3 [1.5, 3.2] & 6.3 [5.5, 7.0] \\ Quantile transform (RTDL version) & 2.6 [1.5, 3.7] & 2.6 [0.4, 4.8] \\ KDI transform (\(\alpha=1\), output dist. = normal) & 4.9 [3.8, 6.0] & 4.4 [2.6, 6.2] \\ \hline \hline \end{tabular}
\end{table}
Table B.3: **Improvements for LGBM-TD by bagging or (ensembled) refitting.** We perform 5-fold cross-validation, stratified for classification, and 5-fold refitting. We compare bagging vs. refitting, one model vs. five models, and individual stopping vs. joint stopping. The table shows the relative reduction in shifted geometric mean benchmark scores, including approximated 95% confidence intervals (Appendix C.6). In each column, the best score is highlighted in bold, and errors whose confidence interval contains the best score are underlined.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & Error **reduction** relative to 1 fold in \% \\ Method & meta-train-class & meta-test-class & meta-train-reg & meta-test-reg \\ \hline LGBM-TD (bagging, 1 model, indiv. stopping) & -0.0 [0.0, 0.0] & -0.0 [-0.0, 0.0] & -0.0 [-0.0, 0.0] & -0.0 [-0.0, 0.0] \\ LGBM-TD (bagging, 1 model, joint stopping) & -0.2 [-0.4, 0.1] & -0.7 [-1.3, -0.2] & 0.0 [-0.0, 0.0] & 0.3 [-0.2, 0.8] \\ LGBM-TD (bagging, 5 models, indiv. stopping) & 3.4 [3.0, 3.7] & 4.1 [3.6, 4.5] & **5.3** [4.5, 6.0] & 4.0 [3.6, 4.5] \\ LGBM-TD (bagging, 5 models, joint stopping) & 3.2 [2.8, 3.5] & 3.3 [2.9, 3.6] & 5.2 [4.5, 5.9] & 4.1 [3.7, 4.5] \\ LGBM-TD (refitting, 1 model, indiv. stopping) & 4.8 [4.1, 5.5] & 1.4 [-0.9, 3.6] & 3.8 [2.0, 5.5] & 4.0 [3.3, 4.8] \\ LGBM-TD (refitting, 1 model, joint stopping) & 5.0 [4.5, 5.5] & 4.3 [4.1, 4.6] & 3.7 [2.1, 5.3] & 4.1 [3.2, 4.9] \\ LGBM-TD (refitting, 5 models, indiv. stopping) & **5.6** [5.2, 6.1] & **6.0** [5.3, 6.7] & 5.2 [3.6, 6.7] & **5.5** [4.7, 6.4] \\ LGBM-TD (refitting, 5 models, joint stopping) & **5.4** [5.0, 5.9] & 5.9 [5.6, 6.1] & 5.2 [3.6, 6.7] & **5.5** [4.6, 6.3] \\ \hline \hline \end{tabular}
\end{table}
Table B.2: **Effects of different preprocessing methods for numerical features for RealMLP-TD-S.** We report the relative increase in the shifted geometric mean benchmark scores compared to the standard method used in RealMLP-TD and RealMLP-TD-S, which is robust scaling and smooth clipping. We also report approximate 95% confidence intervals. To have a more common setting, we do not apply the preprocessing methods to one-hot encoded categorical features. In each column, the best score is highlighted in bold, and errors whose confidence interval contains the best score are underlined.

different losses for early stopping and for best-epoch selection: classification error, Brier score, and cross-entropy loss. In each case, cross-entropy loss is used as the training loss, and classification error is used for evaluating the models on the test sets in the computation of the benchmark score. We observe that models stopped on classification error strongly deteriorate at low patiences (\(\lesssim 100\)), while our default patience of 300 achieves close-to-optimal results. Models stopped on cross-entropy loss deteriorate much less at low patiences, but achieve roughly 2% worse benchmark score at high patiences. Stopping on Brier loss achieves very good high-patience performance and is still only slightly more sensitive to the patience than stopping on cross-entropy loss. An interesting follow-up question would be if HPO can attenuate the differences between different settings.

### Results for AUROC

For classification, there are many different metrics to capture model performance. In the main paper, we use classification error to evaluate models. All TD configurations were tuned for classification error, early stopping and best-epoch selection were performed for classification error, and HPO was performed for classification error. Here, we evaluate models on the area under the ROC curve, also known as AUROC, AUC ROC, or AUC. For the multi-class case, we use the one-vs-rest formulation of AUC, which is faster to evaluate than one-vs-one. Higher AUC values are better and the optimal value is \(1\). Since we are interested in the shifted geometric mean error, we use \(1-\mathrm{AUC}\) instead.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & \multicolumn{4}{c}{Error **reduction** relative to 1 fold in \%} \\ Method & meta-train-class & meta-test-class & meta-train-reg & meta-test-reg \\ \hline RealMLP-TD (bagging, 1 model, indiv. stopping) & -0.0 [-0.0, -0.0] & -0.0 [-0.0, -0.0] & -0.0 [-0.0, -0.0] & -0.0 [-0.0, -0.0] \\ RealMLP-TD (bagging, 1 model, joint stopping) & 1.6 [0.9, 2.4] & 0.7 [0.0, 1.4] & 0.6 [0.1, 1.0] & -0.1 [-1.0, 0.7] \\ RealMLP-TD (bagging, 5 models, indiv. stopping) & 6.7 [6.1, 7.3] & 7.7 [6.9, 8.6] & 6.7 [6.2, 7.2] & 5.1 [4.0, 6.2] \\ RealMLP-TD (bagging, 5 models, joint stopping) & 6.7 [6.1, 7.4] & 8.3 [6.2, 8.3] & 6.7 [6.2, 7.2] & 4.8 [3.7, 5.8] \\ RealMLP-TD (refitting, 1 model, indiv. stopping) & 2.8 [1.7, 3.9] & 3.2 [18.4, 4.6] & 2.8 [1.7, 3.8] & 1.3 [-0.5, 3.0] \\ RealMLP-TD (refitting, 1 model, joint stopping) & 5.3 [4.5, 6.1] & 4.7 [3.9, 5.4] & 4.5 [3.5, 5.6] & 2.6 [0.9, 4.2] \\ RealMLP-TD (refitting, 5 models, indiv. stopping) & 7.6 [6.6, 8.5] & 8.8 [7.9, 9.6] & 8.5 [7.9, 9.1] & 5.3 [3.9, 6.7] \\ RealMLP-TD (refitting, 5 models, joint stopping) & **8.2** [7.5, 8.9] & 8.6 [7.9, 9.3] & **8.7** [8.0, 9.4] & **5.7** [4.5, 6.9] \\ \hline \hline \end{tabular}
\end{table}
Table B.4: **Improvements for RealMLP-TD by bagging or (ensembled) refitting.** We perform 5-fold cross-validation, stratified for classification, and 5-fold refitting. We compare bagging vs. refitting, one model vs. five models, and individual stopping vs. joint stopping. The table shows the relative reduction in shifted geometric mean benchmark scores, including approximated 95% confidence intervals (Appendix C.6). In each column, the best score is highlighted in bold, and errors whose confidence interval contains the best score are underlined.

We compare two settings:

1. A variant of the original setting where early stopping and the selection of the best epoch/iteration is based on accuracy but HPO is performed on \(1-\mathrm{AUC}\). (Thanks to using random search, we do not have to re-run the HPO for this.)
2. A setting where we use the cross-entropy loss for stopping and selecting the best epoch/iteration. While it would be possible to stop on AUC directly, this can be significantly slower since AUC is slower to evaluate. We do not perform HPO in this setting since it is expensive to run.

In both settings, we also evaluate RealMLP without label smoothing (no ls). Figure 3 shows the results optimized for accuracy and Figure B.3 shows the results optimized for cross-entropy. We make a few observations:

* Stopping for cross-entropy generally performs better than stopping for classification error.
* Label smoothing harms RealMLP for AUC, perhaps because the stopping metric does not use label smoothing, or because it encourages near-constant logits in areas where the model is relatively certain.
* Tuned defaults are mostly still better than the library defaults, except for XGBoost on \(\mathcal{B}^{\mathrm{test}}_{\mathrm{class}}\).
* RealMLP without label smoothing is still competitive with GBDTs on the meta-test benchmark but does not perform better than GBDTs unlike what we observed for classification error.

### Results Without Missing-Value Datasets

To assess whether the results are influenced by our choices in missing value handling and exclusion, Figure B.4 presents results on all meta-test datasets that originally did not contain missing values. Only six meta-test datasets originally contain missing values: Three from \(\mathcal{B}^{\mathrm{test}}_{\mathrm{class}}\) (kick, okcupid-stem, and porto-seguro) and three from \(\mathcal{B}^{\mathrm{test}}_{\mathrm{reg}}\) (fps_benchmark, house_prices_nominal, SAT11-HAND-runtime-regression). While RealMLP deteriorates slightly, especially due to the exclusion of fps_benchmark, qualitative takeaways remain similar.

### Comparing Preprocessing Methods for NNs

In the other sections of this paper, we run each NN using the preprocessing from the respective paper that introduced it. Specifically, we use robust scaling and smooth clipping for RealMLP and the RTDL version of the quantile transform for the other papers (see also Appendix B.2). Here,

Figure B.2: **Effect of stopping patiences on the performance of GBDTs on \(\mathcal{B}^{\mathrm{train}}_{\mathrm{reg}}\).** We run the TD configurations of XGB, LGBM, and CatBoost with different early stopping patiences (early_stopping_rounds). As in the remainder of the paper, we use RMSE for early stopping and best-epoch selection. The \(y\)-axis reports the relative increase in the benchmark score relative to stopping on classification error with patience \(1000\) (i.e., never stopping early). The shaded areas are approximate 95% confidence intervals, cf. Appendix C.6.

we evaluate if robust scaling and smooth clipping can improve MLP, ResNet, MLP-PLR, FTT, and TabR-S as well. This also yields a more direct comparison of the architectures, although the nets still differ in other aspects such as initialization and regularization.

Figure B.5 includes results with robust scaling and smooth clipping (RS+SC) for MLP, ResNet, MLP-PLR, FTT, and TabR-S. While the results look promising for some methods (MLP, TabR) and not so promising for others (MLP-PLR), at least without re-tuning their default parameters, our results also show that trying both preprocessing methods can already give considerable improvements on most benchmarks.

### Results for Varying Architecture

Table B.5 shows the effects of including the preprocessing and architecture of RealMLP within other models. In particular, we study the benefits of our architectural changes, cf. Figure 1 (c),

Figure B.3: **Benchmark scores on classification benchmarks vs. average training time for AUC, optimized for cross-entropy.** BestModel-TD uses RealMLP-TD without label smoothing. The \(y\)-axis shows the shifted geometric mean \((\mathrm{SGM}_{e})\)\(1-\mathrm{AUC}\) as explained in Section 2.2. The \(x\)-axis shows average training times per 1000 samples (measured on \(\mathcal{B}^{\mathrm{train}}\) for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \(\rightarrow\infty\), see Appendix C.6.

Figure B.4: **Benchmark scores on \(\mathcal{B}^{\mathrm{test}}_{\mathrm{class}}\) and \(\mathcal{B}^{\mathrm{test}}_{\mathrm{reg}}\) without missing value datasets vs. average training time.** The \(y\)-axis shows the shifted geometric mean \((\mathrm{SGM}_{e})\) classification error (left) or nRMSE (right) as explained in Section 2.2. The \(x\)-axis shows average training times per 1000 samples (measured on \(\mathcal{B}^{\mathrm{train}}\) for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \(\rightarrow\infty\), see Appendix C.6.

Figure B.5: **Benchmark scores on all benchmarks vs. average training time.** Compared to Figure 2, additional results for robust scale + smooth clip (RS+SC) preprocessing are included. The \(y\)-axis shows the shifted geometric mean (\(\mathrm{SGM}_{\mathrm{\varepsilon}}\)) classification error (left) or nRMSE (right) as explained in Section 2.2. The \(x\)-axis shows average training times per 1000 samples (measured on \(\mathcal{B}^{\mathrm{train}}\) for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \(\rightarrow\infty\), see Appendix C.6.

when applied directly to the setting of MLP-D. To this end, we approximately reproduce MLP-D in our codebase without weight decay (since the optimal value changes when including the NTP) and with marginally different early stopping thresholding logic. We also determine the best default learning rate on the meta-train benchmark, similar to Appendix A.4. Our reproduction achieves benchmark scores within 1% of the benchmark scores of the MLP-D (RS+SC) version. Adding the PL embeddings from Gorishny et al. [16] with our default settings sometimes gives good results but is significantly worse on \(\mathcal{B}^{\mathrm{test}}_{\mathrm{class}}\), indicating that they need more tuning. In contrast, incorporating the RealMLP architectural changes (including their associated learning rate factors) improves scores on all benchmarks by around 5% or more, although they alone do not match the results of TabR-S-D. However, the non-architectural changes in RealMLP-TD make an even larger difference.

### Comparing HPO Methods

In Figure B.6, we compare two different HPO methods for GBDTs:

* Random search (HPO), as used in the main paper, with 50 steps.
* Tree parzen estimator (HPO-TPE) as implemented in hyperopt [2], with 50 steps. The first 20 of these steps use random search.

While TPE often performs slightly better, the differences in benchmark scores are relatively small.

### More Time-Error Plots

Here, we provide more time-vs-error plots. Figure B.7 shows results for the arithmetic mean error, Figure B.8 shows results for the arithmetic mean rank, and Figure B.9 shows results for the arithmetic mean normalized error. For the normalized error, the scores are affinely rescaled on each dataset split such that the worst score is \(1\) and the best score is \(0\).

### Critical Difference Diagrams

Figure B.10 analyzes the external validity of differences in average ranks between methods, i.e., whether they will generalize to new datasets from a distribution. While establishing external validity requires a large number of datasets, our meta-test benchmarks show at least the improvements of RealMLP-TD over MLP-D to be externally valid.

### Win-rate Plots

For pairs of methods, we analyze the percentage of (dataset, split) combinations on which the first method has a lower error than the second method. We plot these win-rates in marix plots: Figure B.11 shows the results on \(\mathcal{B}^{\mathrm{train}}_{\mathrm{class}}\), Figure B.12 shows the results on \(\mathcal{B}^{\mathrm{test}}_{\mathrm{class}}\), Figure B.13 shows the results on \(\mathcal{B}^{\mathrm{train}}_{\mathrm{class}}\), Figure B.13 shows the results on \(\mathcal{B}^{\mathrm{test}}_{\mathrm{class}}\), and Figure B.16 shows the results on \(\mathcal{B}^{\mathrm{Grinnsztajn}}_{\mathrm{reg}}\).

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & \multicolumn{4}{c}{Error reduction relative to MLP-D in \(\sharp\)} \\ Method & meta-train-class & meta-train-reg & meta-test-class & meta-test-reg \\ \hline MLP-D & -0.0 [-0.0, -0.0] & -0.0 [-0.0, -0.0] & -0.0 [-0.0, -0.0] & -0.0 [-0.0, -0.0] \\ MLP-D (RS+SC) & 1.5 [-0.7, 2.4] & -1.5 [-1.9, -1.2] & -0.7 [-1.6, -2.0] & 4.3 [-3.4, 5.2] \\ MLP-D (RS+SC, no, not, mut-tuned to) & 2.5 [1.8, 3.1] & -1.0 [-1.5, -0.5] & -1.6 [-2.7, -0.6] & 4.3 [-3.1, 5.3] \\ MLP-D (RS+SC, no, mut-tuned to, PL embeddings) & 4.6 [4.0, -5.2] & -1.5 [-1.9, -1.0] & -10.9 [-1.2, -3.9] & 5.4 [-4.0, 6.9] \\ MLP-D (RS+SC, no, not, mut-tuned lr, RealMLP architecture) & 7.7 [6.9, 8.5] & 1.04 [9.4, 11.3] & 2.0 [-2.4, 1.4] & 9.6 [8.6, 1.0] \\ RealMLP-D & 12.6 [11.9, 13.2] & 13.8 [13.2, 14.4] & 9.8 [14.1, -1.2] & 13.2 [12.1, 14.3] \\ RealMLP-TD & **16.9** [11.1, 17.6] & **2.12** [12.2, 22.9] & **15.2** [14.1] & **14.9** [14.0, 1.58] \\ TabR-S-D & 9.1 [8.2, 10.1] & **18.8** [18.

Figure B.6: **Benchmark scores of selected methods on \(\mathcal{B}^{\text{train}}_{\text{class}}\), \(\mathcal{B}^{\text{train}}_{\text{reg}}\), \(\mathcal{B}^{\text{test}}_{\text{class}}\), and \(\mathcal{B}^{\text{test}}_{\text{reg}}\) vs. average training time. The \(y\)-axis shows the shifted geometric mean (\(\text{SGM}_{\varepsilon}\)) classification error (left) or nRMSE (right) as explained in Section 2.2. The \(x\)-axis shows average training times per 1000 samples (measured on \(\mathcal{B}^{\text{train}}\) for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \(\rightarrow\infty\), see Appendix C.6.**

Figure B.7: **Benchmark scores (arithmetic mean) vs. average training time.** The \(y\)-axis shows the _arithmetic mean_ classification error (left) or nRMSE (right). The \(x\)-axis shows average training times per 1000 samples (measured on \(\mathcal{B}^{\mathrm{train}}\) for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \(\rightarrow\infty\), see Appendix C.6.

Figure B.8: **Benchmark scores (ranks) vs. average training time.** The \(y\)-axis shows the _arithmetic mean_ rank, averaged over all splits and datasets. The \(x\)-axis shows average training times per 1000 samples (measured on \(\mathcal{B}^{\mathrm{train}}\) for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \(\rightarrow\infty\), see Appendix C.6.

Figure B.9: **Benchmark scores (normalized errors) vs. average training time.** The \(y\)-axis shows the _arithmetic mean normalized_ error, averaged over all splits and datasets. Errors are normalized by rescaling the lowest error to zero and the largest error to one. The \(x\)-axis shows average training times per 1000 samples (measured on \(\mathcal{B}^{\mathrm{train}}\) for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \(\rightarrow\infty\), see Appendix C.6.

Figure 10: **Critical difference diagrams on all benchmarks.** The plots show the average rank of methods on each benchmark. Horizontal bars indicate groups of algorithms that are not statistically significantly different at a 95% confidence level according to a Friedman test and post-hoc Nemenyi test implemented in autorank[22].

Figure B.11: **Percentages of wins of row algorithms vs column algorithms on \(\mathcal{B}^{\text{train}}_{\text{class}}\).** Wins are averaged over all datasets and splits. Ties count as half-wins. Methods are sorted by average win-rate (i.e., the average of the values in the row). When averaging, we use dataset-dependent weighting as explained in Section C.3.1.

Figure B.12: **Percentages of wins of row algorithms vs column algorithms on \(\mathcal{B}^{\mathrm{test}}_{\mathrm{class}}\).** Wins are averaged over all datasets and splits. Ties count as half-wins. Methods are sorted by average win-rate (i.e., the average of the values in the row).

Figure B.13: **Percentages of wins of row algorithms vs column algorithms on \(\mathcal{B}_{\mathrm{class}}^{\mathrm{Grinztajn}}\).** Wins are averaged over all datasets and splits. Ties count as half-wins. Methods are sorted by average win-rate (i.e., the average of the values in the row). When averaging, we use dataset-dependent weighting as explained in Section C.3.1.

Figure B.14: **Percentages of wins of row algorithms vs column algorithms on \(\mathcal{B}^{\text{train}}_{\text{reg}}\).** Wins are averaged over all datasets and splits. Ties count as half-wins. Methods are sorted by average win-rate (i.e., the average of the values in the row). When averaging, we use dataset-dependent weighting as explained in Section C.3.1.

Figure B.15: **Percentages of wins of row algorithms vs column algorithms on \(\mathcal{B}^{\text{test}}_{\text{reg}}\).** Wins are averaged over all datasets and splits. Ties count as half-wins. Methods are sorted by average win-rate (i.e., the average of the values in the row).

Figure B.16: **Percentages of wins of row algorithms vs column algorithms on \(\mathcal{B}^{\text{Grinsztajn}}_{\text{reg}}\).** Wins are averaged over all datasets and splits. Ties count as half-wins. Methods are sorted by average win-rate (i.e., the average of the values in the row).

Benchmark Details

### Default Configurations

The parameters for RealMLP-TD and RealMLP-TD-S have already been given in Table A.1. Table C.1 shows the hyperparameters of LGBM-TD and LGBM-D. Table C.2 shows the hyperparameters of XGB-TD and XGB-D. Table C.3 shows the hyperparameters of CatBoost-TD and CatBoost-D. The parameters for LGBM-D, XGB-D, and CatBoost-D have been taken from the respective libraries at the time of writing and are given here for completeness. We also provide tables for MLP-D (Table C.4), ResNet-D (Table C.6), MLP-PLR-D (Table C.5), FTT-D (Table C.7), TabR-S-D (Table C.8), and RealTabR-D (Table C.9). By "RTDL quantile transform", we refer to the version adding noise before fitting the quantile transform.

For XGB-PBB-D, we use the default parameters from Probst et al. [50], with the following modifications: We use hist gradient boosting since it is the new default in XGBoost 2.0. Moreover, since we have high-cardinality categories, we limit one-hot encoding to categories with less than 20 distinct values (not counting missing values) and use XGBoost's native categorical feature handling for the remaining categorical features. For RF-D, we use the default parameters from scikit-learn, do not give RF-D access to the validation set (to make it more similar to other methods that do not use nested cross-validation), and encode categorical columns using ordinal encoding with a random shuffling of categories.

### Hyperparameter Optimization

For all methods, we run 50 steps of random search with the search spaces presented in the following. The search spaces for LGBM-HPO (Table C.10), XGB-HPO (Table C.11), and CatBoost-HPO (Table C.12) are adapted from the "tree-friendly" literature, using n_estimators=1000 in each case. The search space for RF-HPO (Table C.13) is taken from Grinsztajn et al. [18].

For RealMLP-HPO, we provide a custom search space specified in Table C.14. The search spaces for MLP-HPO (Table C.15), MLP-PLR-HPO (Table C.16), ResNet-HPO (Table C.17), FTT-HPO (Table C.18), and TabR-HPO (Table C.19) are adapted from the literature, with minor modifications to decrease RAM usage.

\begin{table}
\begin{tabular}{c c c} \hline Hyperparameter & \multicolumn{2}{c}{CatBoost-TD} & CatBoost-D \\  & classif. & reg. & \\ \hline boosting\_type & Plain & Plain & Plain \\ bootstrap\_type & Bernoulli & Bernoulli & Bayesian \\ max\_depth & 7 & 9 & 6 \\ learning\_rate & 0.08 & 0.09 & automatic \\ subsample & 0.9 & 0.9 & — \\ bagging\_temperature & — & — & 1.0 \\ l2\_leaf\_reg & 1e-5 & 1e-5 & 3.0 \\ random\_strength & 0.8 & 0.0 & 1.0 \\ one\_hot\_max\_size & 15 & 20 & 2 \\ leaf\_estimation\_iterations & 1 & 20 & None \\ _n\_estimators_ & _1000_ & _1000_ & _1000_ \\ _max\_bin_ & _254_ & _254_ & _256_ \\ _od\_wait_ & _300_ & _300_ & _None_ \\ _od\_type_ & _Iter_ & _Iter_ & _Iter_ \\ \hline \end{tabular}
\end{table}
Table C.3: **Hyperparameters for CatBoost-TD and CatBoost-D.** Italic hyperparameters have not been tuned for CatBoost-TD.

\begin{table}
\begin{tabular}{c c} \hline Hyperparameter & Value \\ \hline lr scheduler & None \\ Activation & ReLU \\ Normalization & BatchNorm \\ n\_layers & 2 \\ d\_layers & [128, 128] \\ d\_hidden\_factor & 2 \\ hidden\_dropout & 0.25 \\ residual\_dropout & 0.1 \\ lr & 1e-3 \\ weight\_decay & 0.01 \\ Optimizer & AdamW \\ d\_embedding & 8 \\ batch\_size & 128 \\ max\_epochs & 1000 \\ early stopping patience & 20 \\ Preprocessing & RTDL quantile transform \\ \hline \end{tabular}
\end{table}
Table C.6: Hyperparameters for ResNet-D, adapted from McElfresh et al. [43].

\begin{table}
\begin{tabular}{c c} \hline Hyperparameter & Value \\ \hline MLP hyperparameters & same as in Table C.4 \\ Num. emb. type & PLR \\ Num. emb. initialization \(\sigma\) & 1e-2 \\ Num. emb. \#frequencies & 48 \\ Num. emb. dimension & 24 \\ \hline \end{tabular}
\end{table}
Table C.5: Hyperparameters for MLP-PLR-D. The MLP hyperparameters are taken from Table C.4 and the PLR embedding hyperparameters are taken as the defaults of the library associated with Gorishniy et al. [16].

\begin{table}
\begin{tabular}{c c} \hline Hyperparameter & Value \\ \hline lr scheduler & None \\ Activation & ReLU \\ Normalization & BatchNorm \\ n\_layers & 2 \\ d\_layers & [128, 128] \\ d\_hidden\_factor & 2 \\ hidden\_dropout & 0.25 \\ residual\_dropout & 0.1 \\ lr & 1e-3 \\ weight\_decay & 0.01 \\ Optimizer & AdamW \\ d\_embedding & 8 \\ batch\_size & 128 \\ max\_epochs & 1000 \\ early stopping patience & 20 \\ Preprocessing & RTDL quantile transform \\ \hline \end{tabular}
\end{table}
Table C.7: Hyperparameter search space for FTT-D, adapted from Gorishniy et al. [15]. Differences to Gorishniy et al. [15] are: We limit the number of epochs to 300 as in Grinsztajn et al. [18], we fix the batch size to 256 (Gorishniy et al. [15] use dataset-dependent batch sizes and Grinsztajn et al. [18] uses 512). We do not adopt the larger patience from Grinsztajn et al. [18].

\begin{table}
\begin{tabular}{c c} \hline Hyperparameter & Value \\ \hline num\_embeddings & PBLD \\ num. emb. \#frequencies & 8 \\ num. emb. d\_embedding & 4 \\ num. emb. frequency\_scale & 0.1 \\ Preprocessing & robust scale + smooth clip \\ Add scaling layer & yes \\ Scaling layer lr factor & 96 \\ Label smoothing epsilon & 0.1 (for classification) \\ Other hyperparameters & as in Table C.8 \\ \hline \end{tabular}
\end{table}
Table C.9: Hyperparameters for RealTabR-D.

\begin{table}
\begin{tabular}{c c} \hline Hyperparameter & Value \\ \hline num\_embeddings & None \\ d\_main & 265 \\ context\_dropout & 0.38920071545944357 \\ d\_multiplier & 2.0 \\ encoder\_n\_blocks & 0 \\ predictor\_n\_blocks & 1 \\ mixer\_normalization & auto \\ dropout0 & 0.38852797479169876 \\ dropout1 & 0.0 \\ normalization & LayerNorm \\ activation & ReLU \\ batch\_size & 128 if \(N_{\mathrm{train}}\) < 10K else 256 if \(N_{\mathrm{train}}\) < 30K \\ else 512 if \(N_{\mathrm{train}}\) < 200K else 1024 \\ patience & 16 \\ n\_epochs & 100,000 \\ context\_size & 96 \\ optimizer & AdamW \\ lr & 0.0003121273641315169 \\ weight\_decay & 1.2260352006404615e-06 \\ Preprocessing & RTDL quantile transform \\ \hline \end{tabular}
\end{table}
Table C.8: Hyperparameters for TabR-S-D, taken from Gorishniy et al. [17]. The criterion on batch sizes is inferred to match the batch sizes used in the original paper.

\begin{table}
\begin{tabular}{c c} \hline Hyperparameter & Value \\ \hline num\_embeddings & PBLD \\ num. emb. \#frequencies & 8 \\ num. emb. d\_embedding & 4 \\ num. emb. frequency\_scale & 0.1 \\ Preprocessing & robust scale + smooth clip \\ Add scaling layer & yes \\ Scaling layer lr factor & 96 \\ Label smoothing epsilon & 0.1 (for classification) \\ Other hyperparameters & as in Table C.8 \\ \hline \end{tabular}
\end{table}
Table C.10: Hyperparameter each space for LGBM-HPO, adapted from Prokhorenkova et al. [51] with 1000 estimators instead of 5000.

\begin{table}
\begin{tabular}{c c} \hline Hyperparameter & Space \\ \hline n\_estimators & 250 \\ max\_depth & Choice([None, 2, 3, 4], p=[0.7, 0.1, 0.1, 0.1]) \\ criterion & Choice([gini, entropy]) if classification \\  & else Choice([squared\_error, absolute\_error]) \\ max\_features & Choice([sqrt, sqrt, log2, None, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]) \\ min\_samples\_split & Choice([2, 3], p=[0.95, 0.05]) \\ min\_samples\_leaf & LogUniformInt[1.5, 50.5] \\ bootstrap & Choice(True, False) \\ min\_impurity\_decrease & Choice([0, 0.01, 0.02, 0.05], p=[0.85, 0.05, 0.05, 0.05]) \\ \hline \end{tabular}
\end{table}
Table C.13: Hyperparameter search space for RF-HPO, taken from Grinsztajn et al. [18].

\begin{table}
\begin{tabular}{c c} \hline Hyperparameter & Space \\ \hline tree\_method & hist \\ n\_estimators & 1000 \\ early\_stopping\_rounds & 300 \\ max\_depth & UniformInt[1, 11] \\ learning\_rate & LogUniform[1e-5, 0.7] \\ subsample & Uniform[0.5, 1] \\ colsample\_bytree & Uniform[0.5, 1] \\ colsample\_bylevel & Uniform[0.5, 1] \\ min\_child\_weight & LogUniformInt[1, 100] \\ alpha & LogUniform[1e-8, 1e-2] \\ lambda & LogUniform[1, 4] \\ gamma & LogUniform[1e-8, 7.0] \\ \hline \end{tabular}
\end{table}
Table C.11: Hyperparameter search space for XGB-HPO, adapted from Grinsztajn et al. [18]. We use the hist method, which is the new default in XGBoost 2.0 and supports native handling of categorical values, while the old auto method selection is not available in XGBoost 2.0. We also increase early_stopping_rounds to 300.

\begin{table}
\begin{tabular}{c c} \hline Hyperparameter & Space \\ \hline boosting\_type & Plain \\ bootstrap\_type & Bayesian \\ n\_estimators & 1000 \\ max\_depth & 6 \\ od\_wait & 300 \\ od\_type & Iter \\ learning\_rate & LogUniform[\(e^{-5}\), 1] \\ bagging\_temperature & Uniform[0, 1] \\ l2\_leaf\_reg & LogUniform[1, 10] \\ random\_strength & UniformInt[1, 20] \\ one\_hot\_max\_size & UniformInt[0, 25] \\ leaf\_estimation\_iterations & UniformInt[1, 20] \\ \hline \end{tabular}
\end{table}
Table C.12: Hyperparameter search space for CatBoost-HPO, adapted from Shwartz-Ziv and Armon [58], who did not specify the number of estimators.

\begin{table}
\begin{tabular}{c c c} \hline \hline Hyperparameter & \multicolumn{2}{c}{Space} \\  & \(N\leq 100,000\) & \(N>100,000\) \\ \hline n\_layers & UniformInt[1, 8] & UniformInt[1, 16] \\ d\_hidden\_layers & UniformInt[1, 512] & UniformInt[1, 1024] \\ d\_first\_layer & UniformInt[1, 512] & UniformInt[1, 1024] \\ d\_last\_layer & UniformInt[1, 512] & UniformInt[1, 1024] \\ dropout & Choice(0, Uniform[0, 0.5]) & \\ lr & LogUniform[1e-5, 1e-2] & \\ weight decay & Choice(0, LogUniform[1e-6, 1e-3]) & \\ d\_embedding & UniformInt[1, 64] & \\ batch\_size & 128 if \(N_{\text{train}}\) < 10K else 256 if \(N_{\text{train}}\) < 30K & \\  & else 512 if \(N_{\text{train}}\) < 100K else 1024 & \\ lr\_scheduler & None & \\ Optimizer & AdamW & \\ max \#epochs & 400 & \\ early stopping patience & 16 & \\ Preprocessing & RTDL quantile transform & \\ \hline \hline \end{tabular}
\end{table}
Table C.16: Hyperparameter search space for MLP-PLR-HPO, adapted from Gorishniy et al. [16]. Differences to Gorishniy et al. [16] are: (1) For the MLP part of the search space, we use the same space as for MLP, which includes categorical embeddings and slightly different ranges for some hyperparameters. (2) We shrank the search space for \(\sigma\), as recommended by one of the authors in private communication. (3) We reduced the maximum embedding dimension from 128 to 64 to avoid RAM issues on datasets with many numerical features.

\begin{table}
\begin{tabular}{c c c} \hline \hline Hyperparameter & \multicolumn{2}{c}{classif.} & reg. \\ \hline Num. embedding type & Choice([None, PLD, PL, PL, LR]) & same \\ Use scaling layer & Choice([True, False], \(\text{p=[0.6, 0.4]}\)) & same \\ Learning rate & LogUniform[(2e-2, 3e-1)] & same \\ Dropout prob. & Choice([0, 0.15, 0.3], p=[0.3, 0.5, 0.2]) & same \\ Activation fct. & Choice([ReLU, SEND, Mish]) & same \\ Hidden layer sizes & Choice([256, 256], [64, 64, 64, 64, 64], [512]], p=[0.6, 0.2, 0.2) & same \\ Weight decay & Choice([0.0, 2e-2]) & \\ \(\boldsymbol{w}_{\text{e}}^{(1,4)}\) init std. & LogUniform[(0.05, 0.5]) & \\ Label smoothing \(\varepsilon\) & Choice([0.0, 0.1], p=[0.3, 0.7]) & no label smoothing \\ \hline \hline \end{tabular}
\end{table}
Table C.14: Hyperparameter search space for RealMLP-HPO. The remaining hyperparameters are set as in RealMLP-TD. For best performance, it might be beneficial to use a larger search space for the init standard deviation of the first embedding layer, and to tune the embedding dimensions, as in Table C.15.

\begin{table}
\begin{tabular}{c c} \hline \hline Hyperparameter & Space \\  & \(N\leq 100,000\) & \(N>100,000\) \\ \hline n\_layers & UniformInt[1, 8] & UniformInt[1, 16] \\ d\_hidden\_layers & UniformInt[1, 512] & UniformInt[1, 1024] \\ d\_hidden\_factor & UniformInt[1, 4] \\ hidden\_dropout & Uniform[0, 0.5] \\ residual\_dropout & Choice(0, Uniform[0, 0.5]) \\ lr & LogUniform[1e-5, 1e-2] \\ weight decay & Choice(0, LogUniform[1e-6, 1e-3]) \\ d\_embedding & UniformInt[1, 64] \\ batch\_size & 128 if \(N_{\text{train}}\) < 10K else 256 if \(N_{\text{train}}\) < 30K \\  & else 512 if \(N_{\text{train}}\) < 100K else 1024 \\ activation & ReLU \\ normalization & BatchNorm \\ lr\_scheduler & None \\ Optimizer & AdamW \\ max \#epochs & 400 \\ early stopping patience & 16 \\ Preprocessing & RTDL quantile transform \\ \hline \hline \end{tabular}
\end{table}
Table C.18: Hyperparameter search space for FFT-HPO, adapted from Gorishniy et al. [17]. Differences to Gorishniy et al. [17] are: We limit the number of epochs to 400, and the batch size choices might differ slightly since the criterion in Gorishniy et al. [17] is unclear to us.

\begin{table}
\begin{tabular}{c c} \hline \hline Hyperparameter & Space \\  & \(N\leq 100,000\) & \(N>100,000\) \\ \hline n\_layers & UniformInt[1, 8] & UniformInt[1, 16] \\ d\_hidden\_layers & UniformInt[1, 512] & UniformInt[1, 1024] \\ d\_hidden\_factor & UniformInt[1, 4] \\ hidden\_dropout & Uniform[0, 0.5] \\ residual\_dropout & Choice(0, Uniform[0, 0.5]) \\ lr & LogUniform[1e-5, 1e-2] \\ weight decay & Choice(0, LogUniform[1e-6, 1e-3]) \\ d\_embedding & UniformInt[1, 64] \\ batch\_size & 128 if \(N_{\text{train}}\) < 10K else 256 if \(N_{\text{train}}\) < 30K \\  & else 512 if \(N_{\text{train}}\) < 100K else 1024 \\ activation & ReLU \\ normalization & BatchNorm \\ lr\_scheduler & None \\ Optimizer & AdamW \\ max \#epochs & 400 \\ early stopping patience & 16 \\ Preprocessing & RTDL quantile transform \\ \hline \hline \end{tabular}
\end{table}
Table C.17: Hyperparameter search space for ResNet-HPO, adapted from Gorishniy et al. [15]. We reduced the embedding dimension upper bound, the maximum number of epochs, and the number of layers to have a more acceptable runtime on the meta-test benchmarks. As in the original paper, the size of the first and the last layers are tuned and set separately, while the size for “in-between” layers is the same for all of them.

\begin{table}
\begin{tabular}{c c} \hline \hline Hyperparameter & Space \\ \hline d\_main & UniformInt[96, 384] \\ context\_dropout & Uniform[0.0, 0.6] \\ dropout0 & Uniform[0.0, 0.6] \\ dropout1 & 0.0 \\ lr & LogUniform[3e-5, 1e-3] \\ weight\_decay & Choice(0, LogUniform[1e-6, 1e-4]) \\ encoder\_n\_blocks & UniformInt[0, 1] \\ predictor\_n\_blocks & UniformInt[1, 2] \\ num. emb. type & PLR \\ num. emb. n\_frequencies & UniformInt[16, 96] \\ num. emb. d\_embedding & UniformInt[16, 65] \\ num. emb. frequency\_scale & LogUniform[1e-2, 1e2] \\ num. emb. lite & True \\ \hline \hline \end{tabular}
\end{table}
Table C.19: Hyperparameter search space for TabR-HPO, taken from Gorishniy et al. [17]. Non-specified hyperparameters are chosen as in TabR-S-D (Table C.8). For the weight decay, we used an upper bound of 1e-4 as used in the original code, and not 1e-3 as specified in the paper.

### Dataset Selection and Preprocessing

#### c.3.1 Meta-train Benchmarks

For the meta-train benchmarks, we adapt code from Steinwart [61] to collect all datasets from the UCI repository that follow certain criteria:

* Between 2,500 and 50,000 samples.
* Number of features at most 1,000.
* Labeled as classification or regression task.
* Description made it straightforward to convert the original dataset into a numeric.csv format.
* Uploaded before 2019-05-08.

We remove rows with missing values and keep only those datasets that still have at least 2,500 samples.3 Some datasets are labeled both as regression and classification datasets, in which case we use them for both. Some datasets contain different versions (e.g., different target columns), in which case we use all of them. To avoid biasing the results towards one dataset, we compute benchmark scores using weights proportional to \(1/\#\)versions. In total, we obtain 71 classification datasets (including versions) out of 46 original datasets, and 47 regression datasets (including versions) out of 26 original datasets. Tables C.20 and C.21 summarize key characteristics of these datasets. We count datasets with the same prefix (before the first underscore) as being versions of the same dataset for weighting, except for the two "facebook" datasets in \(\mathcal{B}^{\text{train}}_{\text{reg}}\), which we count as distinct because they are taken from different sources. For regression, we standardize the targets to have mean zero and variance 1 on the whole dataset. This does not introduce leakage since all neural networks standardize regression targets based on the training set, and tree-based methods are invariant to affine rescaling.

Footnote 3: We noticed later that the ozone_level_1hr and ozone_level_8hr datasets contain less than 2,500 samples, but we decided to keep them since we already used them for tuning the hyperparameters.

During earlier development of the MLP, the meta-train benchmark used to include an epileptic seizure recognition dataset, which has since been removed from the UCI repository, hence we do not report results on it.

#### c.3.2 Meta-test Benchmarks

The meta-test benchmarks consist of datasets from the AutoML Benchmark [13] and additional regression datasets from the OpenML-CTR23 benchmark [12], obtained from OpenML [65].

We make the following modifications:

* We use brazilian_houses from OpenML-CTR23 and exclude Brazilian_houses from the AutoML regression benchmark, since the latter contains three additional features that should not be used for predicting the target.
* We use another version of the sarcos dataset where the original test set is not included, since the original test set consists of duplicates of training samples.
* We excluded the following datasets because versions of them were already contained in the meta-training set:
* For classification: kr-vs-kp, wilt, ozone-level-8hr, first-order-theorem-proving, GesturePhaseSegmentationProcessed, PhishingWebsites, wine-quality-white, nomao, bankmarketing, adult
* For regression: wine_quality, abalone, OnlineNewsPopularity, Brazilian_houses, physicochemical_protein, naval_propulsion_plant, superconductivity, white_wine, red_wine, grid_stability

We preprocess the datasets as follows:

* We remove rows with missing continuous values
* We subsample large datasets to contain at most 500,000 samples. Since the donis dataset was particularly slow to train with GBDT models due to its 355 classes, we subsampled it to 100,000 samples.
* We encode missing categorical values as a separate category.
* For regression, we standardize the targets to have mean zero and variance 1. This does not introduce leakage since all neural networks standardize regression targets based on the training set, and tree-based methods are invariant to affine rescaling.

\begin{table}
\begin{tabular}{c c c c c c} \hline Name & \#samples & \#num. features & \#cat. features & largest \#categories & \#classes \\ \hline abalone & 4177 & 8 & 0 & & 3 \\ adult & 45222 & 7 & 7 & 41 & 2 \\ numran,calls\_families & 7127 & 22 & 0 & & 3 \\ anuran\_calls\_genus & 6073 & 22 & 0 & & 5 \\ anuran\_calls\_species & 5696 & 22 & 0 & & 7 \\ avila & 20867 & 10 & 0 & & 12 \\ bank\_marketing & 41579 & 12 & 5 & 11 & 2 \\ bank\_marketing\_additional & 39457 & 19 & 3 & 11 & 2 \\ chess & 3196 & 1 & 31 & 3 & 2 \\ chess\_kurk & 28056 & 3 & 3 & 8 & 18 \\ crowd\_sourced\_mapping & 10494 & 28 & 0 & & 4 \\ default\_credit\_card & 30000 & 23 & 1 & 2 & 2 \\ egg\_gute & 14980 & 14 & 0 & & 2 \\ electrical\_grid\_stability\_simmitted & 10000 & 12 & 0 & & 2 \\ facebook\_live\_sellers\_shailand\_status & 6622 & 9 & 0 & & 2 \\ firm\_teacher\_chive & 10800 & 0 & 16 & 2 & 4 \\ first\_order\_theorem\_growing & 6118 & 51 & 0 & & 2 \\ gas\_sensor\_drift\_class & 13910 & 128 & 0 & & 6 \\ gesture\_phase\_segmentation\_raw & 9900 & 19 & 0 & & 5 \\ gesture\_phase\_segmentation\_v3 & 9873 & 32 & 0 & & 5 \\ hrm2 & 17898 & 8 & 0 & & 2 \\ hrman\_activity\_marphone & 10299 & 561 & 0 & & 6 \\ indoor\_loc\_building & 21048 & 470 & 50 & 2 & 3 \\ indoor\_loc\_relutive & 21048 & 470 & 50 & 2 & 3 \\ insurance\_benchmark & 9822 & 80 & 4 & 5 & 2 \\ landsat\_satimage & 6435 & 36 & 0 & & 6 \\ letter\_recognition & 20000 & 16 & 0 & & 26 \\ madon & 2600 & 500 & 0 & & 2 \\ magic\_gamma\_telescope & 19020 & 10 & 0 & & 2 \\ mushroom & 8124 & 0 & 21 & 12 & 2 \\ mask & 6598 & 166 & 0 & & 2 \\ nomao & 34465 & 118 & 2 & 2 & 2 \\ nursery & 12960 & 7 & 1 & 2 & 4 \\ occupancy\_detection & 20560 & 7 & 0 & & 2 \\ online\_shoppers\_attention & 12330 & 16 & 2 & 3 & 2 \\ optical\_recognition\_handwritten\_digits & 5620 & 59 & 3 & 2 & 10 \\ ozone\_level\_thr & 1848 & 72 & 0 & & 2 \\ ozone\_level\_thr & 1847 & 72 & 0 & & 2 \\ page\_blocks & 5473 & 10 & 0 & & 5 \\ pen\_recognition\_handwritten\_characters & 10992 & 16 & 0 & & 10 \\ phishing & 11055 & 8 & 22 & 2 & 2 \\ polish\_companies\_bankruptcy\_1year & 7027 & 64 & 0 & & 2 \\ polish\_companies\_bankruptcy\_2year & 10173 & 64 & 0 & & 2 \\ polish\_companies\_bankruptcy\_3year & 10503 & 64 & 0 & & 2 \\ polish\_companies\_bankruptcy\_4year & 9792 & 64 & 0 & & 2 \\ polish\_companies\_bankruptcy\_5year & 5910 & 64 & 0 & & 2 \\ seismic\_bumps & 2584 & 12 & 3 & 2 & 2 \\ skill\_credit\_card & 3338 & 18 & 0 & & 7 \\ smartphone\_human\_activity\_postual & 5744 & 561 & 0 & & 6 \\ smartphone\_human\_activity\_postual & 10411 & 561 & 0 & & 6 \\ spambase & 4601 & 57 & 0 & & 2 \\ superconductivity\_class & 21263 & 81 & 0 & & 2 \\ thyroid\_all\_hp & 3621 & 6 & 17 & 5 & 2 \\ thyroid\_all\_hyp & 3621 & 6 & 17 & 5 & 2 \\ thyroid\_all\_hyp & 3621 & 6 & 17 & 5 & 3 \\ thyroid\_all\_hyp & 3621 & 6 & 17 & 5 & 2 \\ thyroid\_all\_emp & 7200 & 6 & 11 & 3 & 3 \\ thyroid\_all\_dis & 3621 & 6 & 17 & 5 & 2 \\ thyroid\_hyp & 2700 & 7 & 14 & 3 & 2 \\ thyroid\_sik\_eu & 3621 & 6 & 17 & 5 & 2 \\ thyroid\_sik\_eu\_ & 3163 & 8 & 18 & 2 & 2 \\ turkiv\_sudent\_evaluation & 5820 & 32 & 0 & & 3 \\ wall\_follow\_robot\_2 & 5456 & 2 & 0 & & 4 \\ wall\_follow\_robot\_24 & 5456 & 24 & 0 & & 4 \\ wall\_follow\_robot\_4 & 5456 & 4 & 0 & & 4 \\ waveform & 5000 & 21 & 0 & & 3 \\ waveform\_noise & 5000 & 40 & 0 & & 3 \\ wilt & 4839 & 5 & 0 & & 2 \\ wine\_quality\_all & 6497 & 11 & 1 & 2 & 7 \\ wine\_quality\_type & 6497 & 11 & 0 & & 2 \\ wine\_quality\_white & 4898 & 11 & 0 & & 7 \\ \hline \end{tabular}
\end{table}
Table C.20: Datasets in the meta-train classification benchmark.

After preprocessing, we

* exclude datasets with less than 1,000 samples, these were
* for classification: albert, APSFailtere, arcene, Australian, blood-transfusion-service-center, eucalyptus, KDDCup09_appetency, KDDCup09-Upselling, micro-mass, vehicle
* for regression: boston, cars, colleges, energy_efficiency, forest_fires, Moneyball, QSAR_fish_toxicity, sensory, student_performance_por, tecator, us_crime
* exclude datasets that have more than 10,000 features after one-hot encoding. These were Amazon_employee_access, Click_prediction_small, and sf-police-incidents (all classification).

#### c.3.3 Grinsztajn et al. [18] Benchmarks

We select the datasets as follows:

* We use the newer version of the benchmark on OpenML.
* When a dataset is used both in benchmarks with and without categorical features, we use the version with categorical features.
* We exclude the eye_movements dataset since a leak in the dataset was reported by Gorishniy et al. [17].

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Name & \#samples & \#num. features & \#cat. features & largest Retargetories \\ \hline \hline air\_quality\_bc & 8991 & 10 & 0 & \\ air\_quality\_co2 & 7634 & 10 & 0 & \\ air\_quality\_no2 & 7715 & 10 & 0 & \\ air\_quality\_noa & 7718 & 10 & 0 & \\ appliances\_energy & 19735 & 29 & 0 & \\ being\_on25 & 41757 & 12 & 0 & \\ bike\_sharing\_goal\_small & 17379 & 9 & 3 & 2 \\ bike\_sharing\_total & 17379 & 9 & 3 & 2 \\ carbon\_unno\_intes\_u & 10721 & 5 & 0 & \\ carbon\_unno\_intes\_v & 10721 & 5 & 0 & \\ carbon\_unno\_intes\_w & 10721 & 5 & 0 & \\ chess\_kirk & 28056 & 3 & 3 & 8 \\ cycle\_power\_plant & 9568 & 4 & 0 & \\ electrical\_path\_stability\_simulated & 10600 & 12 & 0 & \\ facebook\_coinc\_volume\_volume & 40499 & 38 & 2 & 7 \\ facebook\_loc\_sellers\_thailand\_shares & 7050 & 9 & 0 & \\ five\_cities\_being\_p25 & 19062 & 14 & 0 & \\ five\_cities\_cheagh\_p25 & 21074 & 14 & 0 & \\ five\_cities\_changing\_p25 & 20074 & 14 & 0 & \\ five\_cities\_shangha\_p25 & 21436 & 14 & 0 & \\ five\_cities\_shewing\_p25 & 19038 & 14 & 0 & \\ gus\_sensor\_diff\_class & 13910 & 128 & 0 & \\ gus\_sensor\_diff\_conc & 13910 & 128 & 0 & \\ indoor\_loc\_at & 21048 & 470 & 50 & 2 \\ indoor\_loc\_lat & 21048 & 470 & 50 & 2 \\ insurance\_benchmark & 9822 & 80 & 4 & 5 \\ mem\_intentea\_traffic\_volume\_long & 48204 & 6 & 2 & 38 \\ metro\_intea\_traffic\_volume\_short & 48204 & 6 & 2 & 11 \\ naval\_population\_comp & 11934 & 14 & 0 & \\ naval\_population\_path\_path & 11934 & 14 & 0 & \\ nursery & 12960 & 7 & 1 & 2 \\ online\_news\_popularity & 39644 & 4 & 3 & 7 \\ parking\_binningham & 35171 & 5 & 0 & \\ partismen\_motor & 5875 & 18 & 1 & 2 \\ partismen\_social & 5875 & 18 & 1 & 2 \\ protein\_brat\_y\_structure & 45730 & 9 & 0 & \\ skill\_raft & 3338 & 18 & 0 & \\ san2101\_dong & 4137 & 17 & 0 & \\ san2101\_room & 4137 & 17 & 0 & \\ superconductivity & 21263 & 81 & 0 & \\ taxel\_review\_ratings & 5456 & 23 & 0 & \\ wall\_follow\_brat\_2 & 5456 & 2 & 0 & \\ wall\_follow\_robot\_24 & 5456 & 24 & 0 & \\ wall\_follow\_robot\_34 & 5456 & 4 & 0 & \\ wine\_quality\_all & 6497 & 11 & 1 & 2 \\ wine\_quality\_white & 4898 & 11 & 0 & \\ \hline \hline \end{tabular}
\end{table}
Table 21: Datasets in the meta-train regression benchmark.

\begin{table}
\begin{tabular}{c c c c c c c} \hline Name & \#samples & \#num. features & \#cat. features & largestcategories & \#classes & OpenML task ID \\ \hline Bioresponse & 3751 & 1776 & 0 & & 2 & 359967 \\ Diabetes130US & 101766 & 13 & 36 & 789 & 3 & 211986 \\ Fashion-MNIST & 70000 & 784 & 0 & & 10 & 359976 \\ Higgs & 500000 & 28 & 0 & & 2 & 360114 \\ Internet-Advertisements & 3279 & 3 & 1555 & 2 & 2 & 359966 \\ KDDCup99 & 500000 & 32 & 9 & 65 & 21 & 360112 \\ MiniBooNE & 130064 & 50 & 0 & & 2 & 359990 \\ Satellite & 5100 & 36 & 0 & & 2 & 359975 \\ ada & 4147 & 48 & 0 & & 2 & 190411 \\ airlines & 500000 & 3 & 4 & 293 & 2 & 189354 \\ amazon-commerce-reviews & 1500 & 10000 & 0 & & 50 & 10090 \\ car & 1728 & 0 & 6 & 4 & 4 & 359960 \\ christine & 5418 & 1599 & 37 & 2 & 2 & 359973 \\ churn & 5000 & 16 & 4 & 10 & 2 & 359968 \\ cmc & 1473 & 2 & 7 & 4 & 3 & 359959 \\ cane-9 & 1080 & 856 & 0 & & 9 & 359957 \\ connect-4 & 67557 & 0 & 42 & 3 & 3 & 359977 \\ covertype & 500000 & 10 & 44 & 2 & 7 & 7593 \\ credit-g & 1000 & 7 & 13 & 10 & 2 & 168757 \\ dilbert & 10000 & 2000 & 0 & & 5 & 168909 \\ dionis & 100000 & 60 & 0 & & 355 & 189355 \\ dna & 3186 & 0 & 180 & 2 & 3 & 359964 \\ fabert & 8237 & 800 & 0 & & 7 & 168910 \\ gina & 3153 & 970 & 0 & & 2 & 189922 \\ guillermo & 20000 & 4296 & 0 & & 2 & 359988 \\ helena & 65196 & 27 & 0 & & 100 & 359984 \\ jannis & 83733 & 54 & 0 & & 4 & 211979 \\ jannine & 2984 & 8 & 136 & 2 & 2 & 168911 \\ jungle\_ches\_2pes\_raw\_endgame\_complete & 44819 & 6 & 0 & & 3 & 359981 \\ kc1 & 2109 & 21 & 0 & & 2 & 359962 \\ kick & 72600 & 14 & 18 & 1054 & 2 & 359991 \\ madeline & 3140 & 259 & 0 & & 2 & 190392 \\ mfeat-factors & 2000 & 216 & 0 & & 10 & 359961 \\ numerai28.6 & 96320 & 21 & 0 & & 2 & 167120 \\ okcupid-stem & 50788 & 2 & 17 & 7019 & 3 & 359993 \\ pct & 1458 & 37 & 0 & & 2 & 359998 \\ philippine & 5832 & 308 & 0 & & 2 & 190410 \\ phoneme & 5404 & 5 & 0 & & 2 & 168350 \\ porto-sguro & 453046 & 26 & 31 & 102 & 2 & 360113 \\ qvar-biodeg & 1055 & 41 & 0 & & 2 & 359956 \\ riccardo & 20000 & 4296 & 0 & & 2 & 359989 \\ robert & 10000 & 7200 & 0 & & 10 & 359986 \\ segment & 2310 & 16 & 0 & & 7 & 359963 \\ shuttle & 58000 & 9 & 0 & & 7 & 359987 \\ steel-plates-fault & 1941 & 27 & 0 & & 7 & 168784 \\ syhline & 5124 & 20 & 0 & & 2 & 359972 \\ volkert & 58310 & 180 & 0 & & 10 & 359985 \\ yeast & 1484 & 8 & 0 & & 10 & 2073 \\ \hline \end{tabular}
\end{table}
Table C.22: Datasets in the meta-test classification benchmark.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Name & Examples & fnum. features & feat. features & largestcategories & cleanless & OpenML task ID \\ \hline Airlines\_DepDelay\_10M & 500000 & 6 & 3 & 359 & 35929 \\ Alistuse\_Class\_Security & 188318 & 14 & 116 & 326 & 233212 \\ Bavzincellaella\_Twitter & 500000 & 77 & 0 & & 233213 \\ MIP-2016-regression & 1090 & 143 & 1 & 5 & 360945 \\ Mercedes\_Bent\_Greer, Manufacturing & 4209 & 368 & 8 & 47 & 233215 \\ OSAR-TID-10980 & 5766 & 1024 & 0 & & 360933 \\ OSAR-TID-11 & 5742 & 1024 & 0 & & 360932 \\ SST11-HAND-runtime-regression & 1725 & 115 & 1 & 15 & 359948 \\ Santander\_transaction\_value & 4459 & 4991 & 0 & & 233214 \\ Yohanda & 40000 & 100 & 0 & & 317614 \\ airfoil\_self\_noise & 1503 & 5 & 0 & & 361235 \\ auction\_verification & 2043 & 5 & 2 & 6 & 361236 \\ black\_friday & 166821 & 5 & 4 & 7 & 359937 \\ benzilan\_houses & 10692 & 5 & 4 & 35 & 361267 \\ california\_housing & 20640 & 8 & 0 & & 361255 \\ concrete\_compressive\_strength & 1030 & 8 & 0 & & 361237 \\ cps88wages & 28155 & 2 & 4 & 4 & 361261 \\ cpu\_activity & 8192 & 21 & 0 & & 361256 \\ diamonds & 53940 & 6 & 3 & 8 & 361257 \\ elevators & 16599 & 18 & 0 & & 359936 \\ fits & 19178 & 27 & 1 & 163 & 361272 \\ fps\_benchmark & 2592 & 29 & 14 & 24 & 361268 \\ geographical\_origin\_of\_music & 1059 & 116 & 0 & & 361243 \\ health\_insurance & 2272 & 4 & 7 & 6 & 361269 \\ house\_16H & 22784 & 16 & 0 & & 359952 \\ house\_prices\_rominal & 1121 & 36 & 43 & 25 & 359951 \\ house\_sales & 21613 & 20 & 1 & 70 & 359949 \\ kinkan & 8192 & 8 & 0 & & 361258 \\ kings\_county & 21613 & 17 & 4 & 70 & 361266 \\ miani\_housing & 13932 & 15 & 0 & & 361260 \\ nyc-text-green-dec-2016 & 500000 & 9 & 9 & 259 & 359943 \\ pol & 15000 & 48 & 0 & & 359946 \\ punday32ah & 8192 & 32 & 0 & & 361299 \\ guike & 2178 & 3 & 0 & & 359930 \\ sarcos & 4484 & 21 & 0 & & 361011 \\ scomb & 1156 & 1 & 4 & 17 & 361264 \\ solar\_flare & 1066 & 2 & 8 & 6 & 361244 \\ space\_ga & 3107 & 6 & 0 & & 361623 \\ top\_2\_1 & 8885 & 266 & 0 & & 359939 \\ video\_transcoding & 68784 & 16 & 2 & 4 & 361253 \\ wave\_energy & 72000 & 48 & 0 & & 361253 \\ ypop\_4\_1 & 8885 & 251 & 0 & & 359940 \\ \hline \hline \end{tabular}
\end{table}
Table C.24: Datasets in the Grinsztajn et al. [18] classification benchmark.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Name & Examples & fnum. features & feat. features & largestcategories & cleanless & OpenML task ID \\ \hline Bioresponse & 3434 & 419 & 0 & & 2 & 361276 \\ Diabetes130US & 71090 & 7 & 0 & & 2 & 361273 \\ Higgs & 500000 & 24 & 0 & & 2 & 361069 \\ MagicFleescope & 13376 & 10 & 0 & & 2 & 361065 \\ MinIBoNE & 72998 & 50 & 0 & & 2 & 361068 \\ albert & 58252 & 21 & 10 & 14 & 2 & 361282 \\ bank-marketing & 10578 & 7 & 0 & & 2 & 361066 \\ california & 20634 & 8 & 0 & & 2 & 361277 \\ compas-tao-years & 4966 & 3 & 8 & 2 & 2 & 361286 \\ covertype & 423680 & 10 & 44 & 2 & 2 & 361113 \\ credit & 16714 & 10 & 0 & & 2 & 361055 \\ default-of-credit-card-clients & 13722 & 20 & 1 & 2 & 2 & 361283 \\ electricity & 38474 & 7 & 1 & 7 & 2 & 36110 \\ helec & 10000 & 22 & 0 & & 2 & 36127

\begin{table}
\begin{tabular}{c c c c c c} Name & \#samples & \#num. features & \#cat. features & largest \#categories & OpenML task ID \\ \hline Ailerons & 13750 & 33 & 0 & & 361077 \\ Airlines\_DepDelay\_1M & 500000 & 5 & 0 & & 361293 \\ Allstate\_Claims\_Severity & 188318 & 14 & 110 & 20 & 361292 \\ Bike\_Sharing\_Demand & 17379 & 6 & 5 & 4 & 361099 \\ Brazilian\_houses & 10692 & 8 & 3 & 5 & 361098 \\ Mercedes\_Benz\_Greener\_Manufacturing & 4209 & 0 & 359 & 12 & 361097 \\ MiamiHousing2016 & 13932 & 13 & 0 & & 361087 \\ SGEMM\_GPU\_kernel\_performance & 241600 & 3 & 6 & 2 & 361104 \\ abalone & 4177 & 7 & 1 & 3 & 361288 \\ analcatdata\_supereme & 4052 & 2 & 5 & 2 & 361093 \\ cpu\_act & 8192 & 21 & 0 & & 361072 \\ delays\_zurich\_transport & 500000 & 8 & 3 & 7 & 361291 \\ diamonds & 53940 & 6 & 3 & 8 & 361096 \\ elevators & 16599 & 16 & 0 & & 361074 \\ house\_16H & 22784 & 16 & 0 & & 361079 \\ house\_sales & 21613 & 15 & 2 & 2 & 361102 \\ houses & 20640 & 8 & 0 & & 361078 \\ medical\_charges & 163065 & 3 & 0 & & 361294 \\ nyc-taxi-green-dec-2016 & 500000 & 9 & 7 & 5 & 361101 \\ particulate-matter-ukair-2017 & 394299 & 3 & 3 & 12 & 361103 \\ pol & 15000 & 26 & 0 & & 361073 \\ seatflecrime6 & 52031 & 2 & 2 & 17 & 361289 \\ sulfur & 10081 & 6 & 0 & & 361085 \\ superconductor & 21263 & 79 & 0 & & 361088 \\ topo\_2\_1 & 8885 & 252 & 3 & 2 & 361287 \\ visualizing\_soil & 8641 & 3 & 1 & 2 & 361094 \\ wine\_quality & 6497 & 11 & 0 & & 361076 \\ ypop\_4\_1 & 8885 & 42 & 0 & & 361279 \\ \end{tabular}
\end{table}
Table C.25: Datasets in the Grinsztajn et al. [18] regression benchmark.

### Comparison with Standard Grinsztajn et al. [18] Benchmark

Here, we compare two versions of the Grinsztajn et al. [18] benchmark:

1. The "new" version, using our benchmarking setup with the datasets of the Grinsztajn et al. [18] benchmark. This version is used in all plots except Figure C.2 and Figure C.3.
2. The "old" version, which is a slightly modified version of the original code, described in Appendix C.5.

The corresponding results for the most comparable metrics are shown in Figure C.1 for the new paper version, and Figure C.2 for the old version. We decided to use the new version for multiple reasons, including a more realistic validation setting, having the exact same baselines, and having more options for evaluation and plotting. Here is a list of differences in our adapted version:

* In the new version, we removed the eye_movements dataset due to a leak reported in Gorishniy et al. [17].
* We subsample datasets after downloading them to 500K samples (all train-test splits are performed on the same 500K samples).
* We always standardize targets, to make our results independent of the scaling of the datasets. (In contrast, HPO methods on the original benchmark have standardization as an option in their tuning space.)
* We limit training+validation set sizes to 13333, such that at most 10K samples are used for training. Of these samples, we always use 25% for validation, unlike the original benchmark, which limits the training and validation set sizes separately to 10K and 50K samples.
* The new version does not use separate validation sets for early stopping and for HPO, which avoids unfairly disadvantage D and TD methods compared to HPO methods.
* With the new version, we mostly report results using different aggregation strategies and using nRMSE instead of \(R^{2}\) for regression, but try to provide comparable aggregated metrics in Figure C.1.
* The new version uses different random hyperparameter configurations on different train-test splits, which should provide more accurate results and allows computing confidence intervals as in Appendix C.6.
* The new version uses ten train-test splits on all datasets, instead of a smaller dataset-size-dependent number.
* The new version measures all runtimes on the CPU, while the old version measures NN runtimes on the GPU.
* The old version uses slightly different baseline configurations:
* The old version uses (in the code) a simplified version of the MLP without dropout and without weight decay.
* The old version sometimes replaces search spaces like Choice([0, LogUniform[1e-6, 1e-3]]) with more simple spaces.
* The new version doesn't use as large categorical embedding sizes for ResNet and MLP models (up to 64 instead of [64, 512]).
* The old version uses larger stopping patiences for default models than in the original literature [15].
* In the old version, ResNet-HPO tunes the normalization, unlike the original paper [15].
* In the old version, the batch size is tuned for some models.
* In the old version, XGBoost uses the _exact_ tree method with one-hot encoding, while in the new version, we use the _hist method_ that supports native categorical feature handling. This makes XGBoost slower but also more accurate in the older version.
* The old version uses different versions of quantile preprocessing for NN methods, while we use the RTDL quantile transform for all methods except RealMLP.

Both the new and the old version use early stopping and best-epoch selection on accuracy (for classification) / RMSE (for regression).

### Closer-to-original Version of the Grinsztajn et al. [18] Benchmark

In the following, we document the benchmark settings for obtaining the results in Figure C.2 and Figure C.3. The results were obtained using a modification of the original code.

The datasets are taken from the benchmarks described in Grinsztajn et al. [18]. When a dataset is used both in benchmarks with and without categorical features, we use the version with categorical features. We preprocess the datasets following the same steps as in Grinsztajn et al. [18]:

* For neural networks, we quantile-transform the features to have a Gaussian distribution. For TabR [17], we use the modified quantile transform from the TabR paper. For RealMLP, we use the preprocessing described in Section 3, namely robust scaling and smooth clipping.
* For neural networks, we add as a hyperparameter the possibility to normalize the target variable for the model fit and transform it back for evaluation (via scikit-learn's Transformed

Figure C.1: **Benchmark scores (custom normalized errors) vs. average training time. The \(y\)-axis shows the _arithmetic mean_ normalized error as described in Appendix C.5, averaged over all splits and datasets. Errors are normalized by rescaling the lowest error to zero and the largest error to one. The \(x\)-axis shows average training times per 1000 samples (measured on \(\mathcal{B}^{\text{train}}\) for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \(\rightarrow\infty\), see Appendix C.6.**

Figure C.2: **Results on the benchmarks of Grinsztajn et al. [18], using closer-to-original settings (Appendix C.5). The \(y\)-axis (inverted) shows the normalized accuracy / R2 score used in the original paper (see Appendix C.5). The \(x\)-axis shows average training times per 1000 samples, using GPUs for NNs as in Grinsztajn et al. [18], see Appendix C.5.**

TargetRegressor and StandardScaler, which differs from the QuantileTransformer from the original paper, as we found it to work better). The same standardization is also applied to all default-parameter versions of neural networks.
* For models that do not handle categorical variables natively, we encode categorical features using OneHotEncoder from scikit-learn.
* Train size is restricted to 10,000 samples and test and validation size to 50,000 samples.

Note that the datasets from the original benchmark are already slightly preprocessed, e.g., heavy-tailed targets are standardized and missing values are removed. More details can be found in the original paper.

Results normalizationFor Figure C.2, as in the original paper, we normalize the R2 or accuracy score for each dataset before averaging them. We use an affine normalization between 0 and 1, 1 corresponding to the score of the best model for each dataset, and 0 corresponding to the score of the worst model (for classification) and the 10th percentile of the scores (for regression). We use slightly different percentiles compared to the original paper as we normalize across the scores of the tuned and default models, and not all steps of the random search, which reduces the number of outliers. Other aggregation metrics are shown in Appendix B.10.

Time measurementWe follow the original paper and run neural networks on a GPU and the other models on 1 core of an AMD EPYC 7742 64-Core processor, and we average the time across all random steps (for each random step, the time is averaged across splits). To compute the runtime of neural networks, we restrict ourselves to steps ran on the same GPU model (NVIDIA A100-40GB), which means that we exclude datasets for which we have less than 15 steps of each model on this GPU (leaving us with 11 datasets for classification and 15 for regression). We then compute the average runtime per 1000 samples on each dataset and average them.

Other detailsWe rerun classification results for neural networks compared to the original results to early stop on accuracy rather than on cross-entropy, to make results more comparable with the rest of this paper.

At [https://github.com/LeoGrin/tabular-benchmark/tree/better_by_default](https://github.com/LeoGrin/tabular-benchmark/tree/better_by_default), we provide code for the adapted original Grinsztajn et al. [18] benchmark.

Figure C.3: **Results on the benchmarks of Grinsztajn et al. [18], for classification (left) and regression (right), using the closer-to-original settings (Appendix C.5).** The plot is similar to the one in the main part of Grinsztajn et al. [18], with our algorithms added. The \(y\)-axis shows the result of the best (on val, but evaluated on test) hyperparameter combination up to n steps of random step (\(x\)-axis). As in the original paper, we normalize each score between the max and the 10% quantile (classification) or 50% (regression), and truncate scores below 0 for regression.

### Confidence Intervals

Here, we specify how our confidence intervals are computed. Let \(X_{ij}\) denote the score (error/rank) of a method on dataset \(i\) and split \(j\), with \(i\in\{1,\ldots,n\}\) and \(j\in\{1,\ldots,m\}\). Then, the benchmark score \(\mathcal{S}\) can be written as

\[\mathcal{S}=g\left(\sum_{i=1}^{n}\frac{w_{i}}{m}\sum_{j=1}^{m}f(X_{ij})\right), \tag{1}\]

where \(f=g=\mathrm{id}\) for the arithmetic mean. For the shifted geometric mean, we instead have \(g=\exp\) and \(f(x)=\log(x+\varepsilon)\), \(\varepsilon=0.01\). We interpret the benchmark datasets as fixed, but the splits as random. For each dataset \(i\), \(X_{i1},\ldots,X_{im}\) are i.i.d. random variables. We first take the dataset averages

\[Z_{j}\coloneqq\sum_{i=1}^{n}w_{i}f(X_{ij})\.\]

The random variables \(X_{1j},\ldots,X_{nj}\) are independent but not identically distributed. Still, for lack of a better option, we assume that the \(Z_{j}\) are normally distributed with unknown mean and variance. We know that the \(Z_{j}\) are i.i.d., hence we use the confidence intervals from the Student's \(t\)-distribution for normally distributed random variables with unknown mean and variance. This gives us a confidence interval \([a,b]\) for \(\frac{1}{m}\sum_{j=1}^{m}Z_{j}\). Since \(g\) is increasing, we hence obtain a confidence interval \([g(a),g(b)]\) for \(\mathcal{S}=g\left(\frac{1}{m}\sum_{j=1}^{m}Z_{j}\right)\).

Comparison of two methodsWe often compute the error increase in % in the benchmark score of method A compared to method B with the shifted geometric mean, given by

\[100\cdot\left(\frac{\mathcal{S}^{(A)}}{\mathcal{S}^{(B)}}-1\right)\.\]

Here, we leverage that the shifted geometric mean uses \(g=\exp\) to write

\[\frac{\mathcal{S}^{(A)}}{\mathcal{S}^{(B)}}=g\left(\sum_{i=1}^{n}\frac{w_{i}}{ m}\sum_{j=1}^{m}(f(X_{ij}^{(A)})-f(X_{ij}^{(B)}))\right)\,\]

which is of the same form as Eq. (1). Hence, we obtain confidence intervals for this quantity using the same method.

### Time Measurements

For our meta-train and meta-test benchmarks, we report training times measured as follows: We run all methods on a single compute node with a 32-core AMD Ryzen Threadripper Pro 3975 WX CPU, using 32 threads for GBDTs and the PyTorch default settings for NNs. No method is run on GPUs. We run methods sequentially on one split on each dataset of the meta-train-class and meta-train-reg benchmarks. For random-search-based HPO methods, we only run one (TabR-HPO, FTT-HPO) or two (other methods) random search steps and extrapolate the runtime to 50 steps. Runtimes for combinations of models (Best and Ensemble) are computed as the sum of the individual runtimes. We compute the runtime per 1000 samples on each dataset and then average them. For simplicity, we do not use the dataset-dependent weighting employed otherwise on the meta-train benchmark.

### Compute Resources

While we did not measure compute resources precisely, our experiments required at least around 3000 hours on RTX 3090 GPUs and other GPUs, as well as roughly 10,000 hours on HPC CPU nodes (32-64 cores).

### Used Libraries

Our implementation uses various libraries, out of which we would like to particularly acknowledge PyTorch [47], Scikit-learn [48], Ray [46], XGBoost [9], LightGBM [31], and CatBoost [51]. For using XGBoost, LightGBM, and CatBoost, we adapted wrapping code from the CatBoost quality benchmarks [51].

Results for Individual Datasets

Here, we provide and compare the results of central methods per dataset. Figures D.1 - D.7 show scatterplot comparisons for different models.

Table D.1 and Table D.2 show results on \(\mathcal{B}^{\mathrm{train}}_{\mathrm{class}}\). Table D.3 and Table D.4 show results on \(\mathcal{B}^{\mathrm{train}}_{\mathrm{reg}}\). Table D.5 and Table D.6 show results on \(\mathcal{B}^{\mathrm{test}}_{\mathrm{class}}\). Table D.7 and Table D.8 show results on \(\mathcal{B}^{\mathrm{test}}_{\mathrm{reg}}\). Table D.9 and Table D.10 show results on \(\mathcal{B}^{\mathrm{Grinsztajn}}_{\mathrm{class}}\). Table D.11 and Table D.12 show results on \(\mathcal{B}^{\mathrm{Grinsztajn}}_{\mathrm{reg}}\).

Figure D.1: **Best-TD vs CatBoost-HPO on individual datasets.** Each point represents the errors of both models on a dataset, averaged across 10 train-valid-test splits. The black line represents equal errors (\(x=y\)).

Figure D.2: **Best-TD vs Best-HPO on individual datasets.** Each point represents the errors of both models on a dataset, averaged across 10 train-valid-test splits. The black line represents equal errors (\(x=y\)).

Figure D.3: **RealMLP-TD vs CatBoost-TD on individual datasets.** Each point represents the errors of both models on a dataset, averaged across 10 train-valid-test splits. The black line represents equal errors (\(x=y\)).

Figure D.4: **RealMLP-HPO vs CatBoost-HPO on individual datasets. Each point represents the errors of both models on a dataset, averaged across 10 train-valid-test splits. The black line represents equal errors (\(x=y\)).**

Figure D.5: **Ensemble-TD vs Best-TD on individual datasets.** Each point represents the errors of both models on a dataset, averaged across 10 train-valid-test splits. The black line represents equal errors (\(x=y\)).

Figure D.6: **RealMLP-TD vs RealMLP-HPO on individual datasets.** Each point represents the errors of both models on a dataset, averaged across 10 train-valid-test splits. The black line represents equal errors (\(x=y\)).

Figure D.7: **CatBoost-TD vs CatBoost-HPO on individual datasets.** Each point represents the errors of both models on a dataset, averaged across 10 train-valid-test splits. The black line represents equal errors (\(x=y\)).

[MISSING_PAGE_FAIL:66]

[MISSING_PAGE_FAIL:67]

[MISSING_PAGE_EMPTY:68]

[MISSING_PAGE_FAIL:69]

[MISSING_PAGE_EMPTY:70]

[MISSING_PAGE_FAIL:71]

[MISSING_PAGE_EMPTY:72]

[MISSING_PAGE_EMPTY:73]

[MISSING_PAGE_EMPTY:74]

[MISSING_PAGE_FAIL:75]

Broader Impact

We present NN models with an improved speed-accuracy tradeoff and hope that this can reduce the resource consumption of tabular models in applications and further benchmarks. While tabular ML has many potential applications, we feel that none must be particularly highlighted here.

## NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: See the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We provide a paragraph on limitations in Section 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: We do not have theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems.

* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide implementation details in the paper and the appendix, especially Appendix C. We cannot provide all details on dataset preprocessing, but these are provided with the code. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide code for the meta-train and meta-test benchmarks in the supplementary material. For the camera-ready version, we will provide more complete documentation, the code for the Grinsztajn et al. [18] benchmark, and the experimental data. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. ** While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: These details are provided in the paper and appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We provide error bars for fixed datasets, quantifying the uncertainty over the random splits, as described in the appendix. We also provide critical-difference diagrams in Appendix B.11. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources**Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: We did not track these resources in detail, but a rough estimate for the total resources can be found in Appendix C.8. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [NA] Justification: The research appears to conform to the code of ethics but we did not check all \(\approx\) 200 datasets used in this paper. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See Appendix E, but this work is foundational research and the impact is unclear. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards**Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We only use publicly available tabular datasets without obvious safety risks and do not release pretrained models. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [No] Justification: We use \(\approx\) 200 datasets from online repositories, so we cite the repositories / benchmark curators but not the individual datasets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We only provide download code for existing datasets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA]Justification:

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.