# AutoGuide: Automated Generation and

Selection of Context-Aware Guidelines for

Large Language Model Agents

 Yao Fu Dong-Ki Kim Jaekyeom Kim Sungryull Sohn

Lajanugen Logeswaran Kyunghoon Bae Honglak Lee

\({}^{1}\)University of Michigan LG AI Research

Equal contribution.

###### Abstract

Recent advances in large language models (LLMs) have empowered AI agents capable of performing various sequential decision-making tasks. However, effectively guiding LLMs to perform well in unfamiliar domains like web navigation, where they lack sufficient knowledge, has proven to be difficult with the demonstration-based in-context learning paradigm. In this paper, we introduce a novel framework, called AutoGuide, which addresses this limitation by automatically generating context-aware guidelines from offline experiences. Importantly, each context-aware guideline is expressed in concise natural language and follows a conditional structure, clearly describing the context where it is applicable. As a result, our guidelines facilitate the provision of relevant knowledge for the agent's current decision-making process, overcoming the limitations of the conventional demonstration-based learning paradigm. Our evaluation demonstrates that AutoGuide significantly outperforms competitive baselines in complex benchmark domains, including real-world web navigation.

## 1 Introduction

Recent advances in large language models (LLMs) have empowered AI agents to address various sequential decision-making tasks and applications [1, 2]. The foundation of these successes involves the planning and reasoning capabilities of pre-trained LLMs, enabling agents to execute effective policies [3, 4]. The predominant approach to leveraging these (typically closed source) models for sequential decision making tasks is to provide demonstrations in the form of in-context examples. However, direct application of this learning paradigm can be limited, especially in target domains where the LLM has insufficient prior knowledge such as in web navigation, where LLM agents generally achieve low success rates due to diverse and dynamic contents [5, 6, 7, 8]. Providing all available experiences as demonstrations to an agent can further be unsuccessful due to context length limitations, prompt sensitivity, and difficulty with complex reasoning [9, 10, 11, 12].

On the other hand, LLMs excel in interpreting concise instructions provided as natural language, an ability that is also reinforced in the instruction-tuning phase of LLMs. Inpired by this, we explore data-driven strategies that leverage offline experiences to extract actionable knowledge to help guide LLM agents. As offline experiences implicitly convey valuable knowledge about desirable and undesirable policies in domains, they promise to serve as a useful resource for improving an LLM agent's decision-making in situations where the pre-trained LLM lacks understanding. Despite this potential benefit, a critical challenge lies in effectively extracting the implicit information embedded in offline data.

To address the challenge of extracting knowledge from offline data, we propose a novel framework, called AutoGuide. Specifically, AutoGuide automatically derives a comprehensive set of context-aware guidelines from offline experiences. Our method applies these context-conditional guidelines to enhance the performance of an LLM agent by retrieving guidelines relevant to the agent's current state and incorporating them into the prompt during testing (see Figure 1). Notably, we generate context-aware guidelines in concise natural language statements, effectively compressing knowledge in offline data. Moreover, context-aware guidelines clearly describe the contexts where they are applicable, so AutoGuide enables an LLM agent to select pertinent guidelines for its current decision-making process. As a result, AutoGuide achieves the highest success rates compared to competitive baselines in complex sequential decision-making benchmark environments.

**Our contribution.** In summary, we present the following main contributions in this paper:

* **Principled method based on context-aware guidelines (Section 3):** We develop two modules to automatically generate context-aware guidelines from offline experiences: the context identification module for identifying the context of a given trajectory, and the guideline extraction module for extracting a desired guideline corresponding to that context. The outcome is a set of domain knowledge in concise natural language that enhances decision-making by providing pertinent information.
* **Comprehensive evaluation of AutoGuide (Section 4.2):** We show AutoGuide's capability in extracting helpful context-aware guidelines in various interactive benchmark domains, including navigating real-world web domains (e.g., GitHub). Our results highlight the effectiveness of AutoGuide, which significantly outperforms baselines without context-aware guidelines.
* **Analyses with important perspectives (Section 4.3):** We study various aspects of AutoGuide, such as the significance of determining the applicability of each guideline based on generated contexts. We also investigate the generalization ability of context-aware guidelines and demonstrate that our guidelines enhance the performance across out-of-domain tasks.

## 2 Related Work

**LLM-based agents.** Language models have recently been shown to possess strong priors for sequential decision-making tasks, which has given rise to LLM-powered agents [1, 2, 13, 14]. Agents

Figure 1: AutoGuide aims to extract the implicit knowledge embedded in offline experiences and help the decision-making process of an LLM agent. Specifically, our method generates a comprehensive set of context-aware guidelines from offline data and explicitly identifies when each guideline is applicable by generating its corresponding context. Our context-aware guidelines enable providing pertinent guidelines at test time by identifying the context of the current trajectory, leading to correct decision-making compared to baselines without context-aware guidelines.

need to possess various skills to be effective in practice including planning [3; 15; 16], reasoning [4; 17], tool manipulation [18; 19; 20; 21], code generation [22; 16], among others. In this work, we focus on building effective agents for web [23; 8] and embodied [24] environments.

**Self-reflection from past experiences.** An important capability for agents to succeed is the ability to learn from past experiences and update their behavior based on feedback. Self-feedback [25; 26; 27] has emerged as an effective technique where a model inspects its own incorrect predictions, reflects on it to identify what went wrong and attempts to improve its prediction. While self-feedback provides intra-task (i.e., per-episode) knowledge within a task based on immediate feedback, our approach offers an orthogonal and complementary aspect of inter-task knowledge (over multiple tasks) by considering multiple train tasks in offline data. AutoGuide enhances learning efficiency and credit assignment by utilizing detailed feedback from multiple tasks. However, these self-feedback approaches are complementary to AutoGuide and can be used in conjunction with our approach, as shown in our experiments (see Section 4.2).

**Leveraging natural language guidance.** Natural Language can be a rich source of information for agents to learn to act efficiently. Prior work has explored the notion of learning from human-written text manuals, which describe details about the environment [28; 29; 30]. Recent work has explored automatically generating such guidance in the form of chain-of-thought reasoning [4; 23], which emulates a thought process or rationale for agent's predictions. In contrast to approaches which generate such guidance dynamically on the fly by imitating example guidance demonstrations provided by a human, our approach carefully compares trajectories in offline data to generate appropriate guidance and uses these guidelines for predicting better actions. ExpeL [31] proposed a related approach to derive guidelines. In contrast to ExpeL, where all guidelines are provided to an agent as a prompt, our guideline selection process is contextual, where guidelines relevant to the agent's current state are retrieved and used for prediction. We show that this substantially improves over ExpeL's non-contextual guideline-based approach.

## 3 AutoGuide: Principled Method Based on Context-Aware Guidelines

Our work is motivated by the increasing availability of offline experiences that agents or humans naturally accumulate through their interactions with the environment. AutoGuide aims to leverage this offline data to improve the decision-making of an LLM agent by generating helpful context-aware guidelines. This section details how AutoGuide automatically constructs these guidelines and applies them to guide action generation at test time.

### Problem Statement

Formally, AutoGuide is given offline data \(\mathcal{D}_{\mathrm{train}}\!=\!(\bm{\tau}^{1},...,\bm{\tau}^{N})\) that consist of \(N\) trajectories from training tasks. Each trajectory \(\bm{\tau}\!=\!(x_{0},a_{0},r_{0},...,r_{T})\) is a sequence of observations, actions, and rewards following the partially observable Markov decision process [32]. The return of a trajectory is defined as the sum of rewards obtained throughout the trajectory: \(R(\tau)\!=\!\sum_{t=0}^{T}r_{t}\). The objective of AutoGuide is to distill knowledge from offline experiences into a useful natural language format, such that the extracted information helps to maximize the expected return \(\mathbb{E}_{\tau}[R(\tau)]\) during test time.

Figure 2: Context-aware guideline generation process based on a pair of contrastive trajectories \(\bm{\tau}_{+}^{i}\) and \(\bm{\tau}_{-}^{i}\). In this example, the two trajectories start deviating from each other at \(t\!=\!0\). The context identification module generates a description of the context at \(t\!=\!0\) given \(\bm{\tau}_{:0}^{i}\), and the guideline extraction module generates the corresponding guideline for that context.

``` Input:Offline data \(\mathcal{D}_{\mathrm{train}}\), context identification module \(\mathcal{M}_{\mathrm{context}}\), guideline extraction module \(\mathcal{M}_{\mathrm{guideline}}\) Initialize context-aware guideline dictionary \(\mathcal{G}\) for Each pair \(\bm{\tau}_{+}^{i},\bm{\tau}_{-}^{i}\in\mathcal{D}_{\mathrm{train}}\)do # Identify the context from a trajectory  Find the deviating timestep \(t\) from \(\bm{\tau}_{+}^{i}\) and \(\bm{\tau}_{-}^{i}\) \(\texttt{context}\leftarrow\mathcal{M}_{\mathrm{context}}(\bm{\tau}_{+}^{i})\) # Check if the current context matches any existing contexts if\(\texttt{context}\notin\mathcal{G}\)then \(\mathcal{G}[\texttt{context}]=\{\}\) endif # Generate the context-aware guideline guideline \(\texttt{guideline}\texttt{\leftarrow}\mathcal{M}_{\mathrm{guideline}}(\bm{\tau}_{+}^ {i},\bm{\tau}_{-}^{i},\texttt{context})\) \(\mathcal{G}[\texttt{context}]\texttt{\leftarrow}\mathcal{G}[\texttt{context}] \texttt{\cup}\{\texttt{guideline}\}\) endfor Return Context-aware guideline dictionary \(\mathcal{G}\) ```

**Algorithm 1** Extracting context-aware guidelines from offline data

### Extraction of Context-Aware Guidelines

AutoGuide generates a set of context-aware guidelines by utilizing pairs of contrastive trajectories from offline data. Each context-aware guideline is expressed in concise natural language and follows a conditional structure, clearly describing the context in which the guideline is applicable. Intuitively, contrasting a pair of trajectories with different returns provides important information about when and which actions are effective or ineffective in maximizing expected returns. Building on this insight, we develop two modules for automatically extracting context-aware guidelines (see Figure 2):

**Context identification module.** This module is responsible for abstracting the given partial trajectory into its _context_, a concise natural language description of the agent's state. More specifically, for a timestep \(t\) and the corresponding trajectory \(\bm{\tau}_{:t}^{i}\coloneqq(x_{0},a_{0},...,x_{t})\), we prompt LLMs to clearly describe the agent's status:

\[\texttt{context}\leftarrow\mathcal{M}_{\mathrm{context}}(\bm{\tau}_{:t}^{i}),\] (1)

Our prompt templates for the context identification module are shown in Appendix C.1.

**Guideline extraction module.** This module aims to generate a desired guideline corresponding to a specific context. Let \(\bm{\tau}_{+}^{i}\) and \(\bm{\tau}_{-}^{i}\) represent a contrasting pair of trajectories for the same task \(i\) in offline data \(\mathcal{D}_{\mathrm{train}}\), where \(R(\bm{\tau}_{+}^{i})>R(\bm{\tau}_{-}^{i})\). We want to contrast the pair of trajectories to find desired behaviors at an important timestep. To do this, we compare these two trajectories to find the deviation timestep \(t\) at which they begin to diverge due to different actions. Then we apply the context identification module to summarize the context for the shared part of the trajectory \(\bm{\tau}_{:t}^{i}\). Eventually, we extract a useful natural language guideline by examining the paired contrastive trajectories \(\bm{\tau}_{+}^{i}\) and \(\bm{\tau}_{-}^{i}\) with respect to the context:

\[\texttt{guideline}\leftarrow\mathcal{M}_{\mathrm{guideline}}(\bm{\tau}_{+}^{i},\bm{\tau}_{-}^{i},\texttt{context}),\] (2)

where we refer to Appendix C.2 for our prompt template. As an example, the paired trajectories in Figure 2 deviate from timestep \(t=0\), for which the context is summarized as _On the Reddit main page_. This module then generates the following context-aware guideline: _When on the Reddit main page, if you want to navigate to a specific forum, you should click on "link 'Forums'" located right above "link 'Wiki'"_.

**Construction of context-aware guidelines.** We collect context-aware guidelines \(\mathcal{G}\) by iterating through available pairs in the paired offline data and organize the guidelines in a dictionary format, using the context as the key and the corresponding guidelines as the value (see Algorithm 1). In particular, we observe that the context identification module occasionally produces contexts that describe the same situation but are expressed slightly differently. To minimize redundancy, we employ an LLM to determine if the current context corresponds to any previously identified context.

If a match is found, we reuse the existing context; otherwise, we introduce a new context into our dictionary \(\mathcal{G}\). The specific prompt template for this context-matching procedure is in Appendix C.3.

### Applying Context-Aware Guidelines at Test Time

After extracting a set of context-aware guidelines \(\mathcal{G}\) from offline experiences, our method employs these guidelines to enhance the decision-making of an LLM agent during testing. At each timestep, AutoGuide identifies the context of the current test trajectory \(\boldsymbol{\tau}\) up to timestep \(t\) (which represents the agent's interactions up to the current time-step) using our context identification module \(\mathcal{M}_{\mathrm{context}}\). Our guideline selection module \(\mathcal{M}_{\mathrm{select}}\) then selects relevant guidelines for context from \(\mathcal{G}\). More specifically, the module applies context as the key to fetch a set of possible guidelines \(\mathcal{G}[\textsc{context}]\). If there are more than \(k\) guidelines in \(\mathcal{G}[\textsc{context}]\), \(\mathcal{M}_{\mathrm{select}}\) prompts an LLM to choose top-\(k\) guidelines for the specific \(\boldsymbol{\tau}\):

\[\textsc{relevant guidelines}\leftarrow\mathcal{M}_{\mathrm{select}}( \textsc{context},\boldsymbol{\tau};\mathcal{G},k),\] (3)

where Appendix C.4 details the prompt template for this selection procedure. Subsequently, AutoGuide incorporates both the context and relevant guidelines into the agent's action generation prompt. Therefore, the agent selects an action by considering the provided context and guidelines (see Figure 1 for an example). This process iterates until the end of the test trajectory (see Algorithm 2).

**Key benefits of AutoGuide**. First, the extraction of context-aware guidelines in AutoGuide offers the inherent benefit of providing relevant guidelines for the context of interest. This capability is important since neglecting the specific context in which a guideline applies can confuse the agent's decision-making process. The second key benefit is the generation of concise natural language guidelines, which can be seamlessly incorporated into any prompt-based LLM agent. Lastly, AutoGuide generates guidelines at the individual context level rather than at the trajectory level. Given that a single incorrect action can lead to a complete failure, it is essential to provide detailed assistance in each action selection process. With these advantages, we demonstrate in the next section that our approach significantly enhances the performance of LLM agents.

## 4 Evaluation

This section demonstrates the efficacy of AutoGuide by conducting experiments on a diverse suite of sequential decision-making benchmark domains. We also perform important analyses about AutoGuide, such as the ablation study of different AutoGuide components, comparison to

Figure 3: Sequential decision-making benchmark domains considered in our work: ALFWorld [24], WebShop [33], WebArena [8], and multi-modal real-world websites. Graphic credit: [34; 33; 8; 35].

in-context learning, and generalization to out-of-domain tasks. We refer to Appendix B for additional experimental details.

### Evaluation Setup

#### 4.1.1 Sequential Decision-Making Benchmark Domains

We consider the following interactive sequential decision-making benchmarks to study various aspects of AutoGuide (see Figure 3):

\(\bullet\)**ALFWorld [24]:** In this embodied benchmark, an LLM agent interacts with an environment to carry out household tasks, such as placing a pan on the dining table. Observations and actions are expressed in natural language statements, and the agent must navigate through the space and manipulate objects to successfully complete the tasks.

\(\bullet\)**WebShop [33]:** This interactive web environment simulates the task of online shopping on an e-commerce website. The agent's goal is to understand a text instruction and buy a product that meets specified criteria. This involves querying the website's search engine, understanding the descriptions and details of each item, and selecting necessary options.

\(\bullet\)**WebArena [8]:** This web-based benchmark introduces realistic environments by replicating the functionality and data found in popular web domains (e.g., Gitlab, Reddit, Wikipedia). Compared to WebShop, WebArena presents more challenges and difficulties for an LLM agent due to its large observation and action space, along with tasks that involve longer planning horizons. We focus oe the Reddit domain for the main WebArena experiments.

\(\bullet\)**Real-world multi-modal websites:** Finally, we consider evaluating AutoGuide on a variety of real-world website tasks. These span from a collaborative software development platform (e.g., GitHub) to a flight search engine (e.g., Google Flights) and an online education platform (e.g., Coursera). Please refer to Appendix B.4 for example tasks. In particular, in comparison to WebShop and WebArena, we design our tasks to be multi-modal such that the agent must consider both visual (e.g., images) and textual information (e.g., HTML) to complete these tasks.

#### 4.1.2 Baselines

We compare AutoGuide against the following baseline approaches to study the effect of context-aware guidelines (refer to Appendix B for more details):

\(\bullet\)**ReAct [23]:** This LLM-based planning method integrates reasoning and acting to address sequential decision-making tasks. However, it does not leverage offline experiences and thus suffers from the limited understanding of pre-trained LLMs in downstream domains.

\(\bullet\)**Expel [31]:** This method also extracts natural language knowledge from offline data. However, it fails to consider the applicability of guidelines and does not generate context-aware guidelines. Instead, it provides all guidelines to an LLM agent without filtering out irrelevant ones based on the current context. Expel has two contributions, the guideline generation and in-context example selection module. Because the latter is orthogonal to our analysis and can be seamlessly combined with our method, we consider Expel with guidelines in our experiments.

\(\bullet\)**Reflexion [27]:** This approach converts environmental feedback into text statements to assist an LLM agent (e.g., ReAct) in the next trial. The baseline generates valuable feedback about solving a specific test task. We demonstrate how context-aware guidelines derived by AutoGuide can be combined with the feedback.

#### 4.1.3 Implementation

We collect offline experiences either by running ReAct and Reflexion, or incorporating human demonstrations. We use ReAct with GPT-3.5-turbo as our base LLM agent for WebShop and ALFWorld and GPT-4-turbo for WebArena. For each benchmark, we apply the same GPT model for action generation, context identification, and guideline selection. We extract context-aware guidelines from offline data with GPT-4-turbo and evaluate their effectiveness by applying them to the test set with non-overlapping tasks. We refer to Appendix B for more details.

### Main Results

**Q1.**_How effective is AutoGuide compared to baselines without context-aware guidelines?_

To answer this question, we compare methods on ALFWorld, WebShop, and WebArena benchmarks. The performance on the test datasets is presented in Table 1. There are three notable observations:

1. **Effectiveness of context-aware guidelines.** Our approach surpasses baseline performance in both ALFWorld and WebShop, achieving the highest test rewards and success rates in Table 1. These results highlight the effectiveness of employing context-aware guidelines in language-based decision-making domains. To further examine the action selection differences among ReAct, ExpeL, and our method, we present their trajectories in Figure 4. We observed that ReAct makes common mistakes such as trying to take soapbar that is not visible, or taking a soapbottle instead of soapbar due to their similar names. Both ExpeL and AutoGuide improve on this by extracting guidelines from similar mistakes in the offline experience. However, ExpeL often erroneously applies incorrect guidelines

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline \multirow{2}{*}{Algorithm} & Offline & Context & ALFWorld [24] & WebShop [33] & WebArena [8] \\ \cline{3-7}  & data? & aware? & Success Rate (SR)\(\uparrow\) & Reward\(\uparrow\) & SR\(\uparrow\) & SR\(\uparrow\) \\ \hline ReAct [23] & ✗ & ✗ & 54.5\% & 66.4 & 30\% & 8.0\% \\ ExpeL [31] & ✓ & ✗ & 59.0\% & 60.9 & 35\% & 21.8\% \\ AutoGuide & ✓ & ✓ & **79.1\%** & **73.4** & **46\%** & **47.1\%** \\ \hline ReAct [23] & + Reflexion [27] & ✗ & ✗ & 67.2\% & 77.1 & 51\% & N/A \\ ExpL [31] & + Reflexion [27] & ✓ & ✗ & 71.6\% & 71.7 & 42\% & N/A \\ AutoGuide & + Reflexion [27] & ✓ & ✓ & **88.1\%** & **81.4** & **57\%** & N/A \\ \hline \hline \end{tabular}
\end{table}
Table 1: Test reward and success rate on ALFWorld, WebShop, and WebArena. The base agent model for ALFWorld and WebShop is GPT-3.5-turbo and for WebArena is GPT-4-turbo. Reflexion is done by GPT-4-turbo for at most 3 trials. In our experiments, due to token limit of GPT, we did not experiment with Reflexion on WebArena tasks.

Figure 4: Trajectories of ReAct, ExpeL, and AutoGuide from the same test task. ReAct (Left) chose the wrong item, consequently failing the task in the end. ExpeL (middle) was confused by guidelines that were irrelevant to current context, leading to incorrect reasoning and actions. AutoGuide (right) selects relevant guidelines to the agent’s context, enabling the agent to accomplish the task.

due to the availability of all guidelines at each timestep. In Figure 4, ExpeL mistakenly attends to the second guideline _"ensure to specify the item's number and location..."_, leading to wrong reasoning and action. AutoGuide presents relevant guidelines at necessary moments, enabling accurate task completion by avoiding the mistakes seen in ExpeL and ReAct.

2. **Importance of providing pertinent knowledge.** ExpeL approach helps ReAct by extracting knowledge from offline experiences, but its impact is not as significant as AutoGuide. Recall that for ExpeL, the guidelines are neither generated for specific contexts at training time nor selected to only provide context-aware guidelines at test time. As a result, irrelevant guidelines can be introduced to an agent, potentially causing confusion for the agent. Consequently, the result highlights the significance of providing relevant guidelines conditioned on contexts for LLM agents.

3. **Scalability to complex environments.** We conduct experiments on WebArena-Reddit, which features more diverse tasks on realistic and complex websites requiring longer action sequences. This domain has a larger observation space and a more complex action space (e.g., scrolling). Table 1 presents the results, where AutoGuide achieves the highest success rate with a significant margin when compared to ReAct and ExpeL. We observe that ReAct scores low task success rate (8.0%) in WebArena due to the complex observation and action spaces and longer task horizon. In ExpeL, the issue of presenting all guidelines to an agent is exacerbated in the WebArena compared to simpler environments like ALFWorld and WebShop. WebArena's wide variety of tasks across different domains requires a larger number of guidelines to cover the knowledge needed for all tasks and domains. This results in either an overload of irrelevant guidelines that could mislead the agent or a lack of crucial information when the number of guidelines is limited, as suggested in ExpeL [31]. In contrast, AutoGuide achieves a more significant performance enhancement (47.1%) compared to ExpeL (21.8%) by efficiently providing pertinent guidelines and minimizing the burden on context capacity. We refer to Figure 14 for a list of example contexts and guidelines.

**Q2.**_How does AutoGuide perform when combined with test-time self-feedback approaches?_

Our context-aware guidelines effectively provide _inter-task_ knowledge by considering multiple tasks in offline data. Meanwhile, self-feedback methods (e.g., Reflexion) offer _intra-task_ knowledge based on environmental feedback during test time. In this question, we explore the effectiveness of integrating both inter-task and intra-task information. The results presented in Table 1 demonstrate that the combination of AutoGuide with Reflexion achieves the highest performance in the WebShop and ALFWorld benchmarks. Hence, we find that our context-aware guidelines positively complement the intra-task knowledge of Reflexion. Another observation from Table 1 is that, while ExpeL + Reflexion outperforms ExpeL alone, this combination is not as effective as other approaches. This limitation may stem from ExpeL introducing irrelevant knowledge, potentially leading to conflicts with Reflexion's feedback and having an adverse impact on the decision-making process.

**Q3.**_Can AutoGuide generate context-aware guidelines for multi-modal inputs?_

Going beyond text-only inputs is an essential step toward building capable agents for solving real-world environments and tasks. We test AutoGuide in a complex multi-modal setting, where each observation includes image and text information. Specifically, we introduce a set of real-world website navigation tasks in 3 domains: GitHub, Google Flights, and Coursera. For these multi-modal tasks, we employ the Set-of-Marks (SoM) agent [36, 5] as our base method. The SoM prompting improves the visual grounding capabilities of large multi-modal models such as GPT-4V by adding visually distinguishable marks to image inputs [36]. We apply AutoGuide with GPT-4V to generate natural language context-aware guidelines from collected trajectories with both image and text observations.

\begin{table}
\begin{tabular}{c c c} \hline \hline \multirow{2}{*}{Algorithm} & \multicolumn{2}{c}{WebShop} \\ \cline{2-3}  & Reward\(\dagger\) & SR\(\dagger\) \\ \hline ReAct (1-shot) & 66.4 & 30\% \\ ReAct (2-shot) & 66.0 & 35\% \\ ReAct (4-shot) & 70.2 & 37\% \\ ReAct (6-shot) & 71.0 & 38\% \\ AutoGuide & **73.4** & **46\%** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Ablation study of AutoGuide using various top-\(k\) values.

Table 2 shows the effectiveness of AutoGuide, demonstrating its generalization ability to complex real-world multi-modal settings. We refer to Figure 15 for example context-aware guidelines.

### Analyses of AutoGuide

**Q4.**_How does AutoGuide compare to ReAct with varying numbers of in-context examples?_

Table 3 shows that, while increasing the number of in-context examples for ReAct gradually improves performance, there is a plateau at a certain number of shots. Additionally, ReAct with more than 6 shots often exceeds the token limit of GPT-3.5-turbo. These results indicate that directly inputting raw trajectories into ReAct for in-context learning is not an effective way to fully leverage offline data. In contrast, AutoGuide extracts knowledge from entire training trajectories by summarizing them into concise context-aware guidelines, making them easy to integrate with prompt-based agents.

**Q5.**_How does altering the number of top-\(k\) guidelines impact the performance of AutoGuide?_

We conducted an ablation study on WebShop using various values of \(k\) in Table 4. We find that employing context-aware guidelines consistently outperforms the no-guideline baseline (\(k=0\); ReAct). The \(k=3\) yields the best performance. The largest \(k\) value of 5 can lead an LLM agent to overthink, potentially resulting in a slight decrease in performance. Conversely, a smaller \(k\), like \(k\!=\!1\), may cause LLM to overlook additional helpful guidelines, leading to slightly worse performance.

**Q6.**_How do AutoGuide's context-aware guidelines generalize to out-of-domain environments?_

We conduct an experiment to further demonstrate AutoGuide's out-of-domain capability across different domains but relevant tasks. We extract context-aware guidelines from WebShop and apply them to WebArena-Shopping, which is a distinct domain with variations in observation/action spaces, task intentions, and episodic horizons. For this domain adaptation case, we additionally incorporate a grounding module to align the context-aware guidelines from WebShop to WebArena's observations based on GPT-4-Turbo. As shown in Table 5, the transferred guidelines bring a notable improvement in success rates compared to the ReAct baseline in WebArena Shopping.

**Q7.**_How does each component of_ AutoGuide _contribute to the final results?_

We evaluate the impact of different components within AutoGuide on its performance in WebShop, as detailed in Table 6. We examine two variants: ReAct+CI and ReAct+GES. The ReAct+CI, which incorporates contexts into observations without guidelines, shows improvement over ReAct. This suggests that contexts enhance decision-making by verifying the current state before action selection. ReAct+GES, which generates guidelines from trajectories without contexts and employs GPT-3.5-turbo for guideline selection, also enhances performance but is less effective than the full AutoGuide. This indicates that choosing relevant guidelines based on the trajectory alone is more challenging than using contexts. Therefore, integrating both context summaries and guidelines is crucial for maximizing the benefits of AutoGuide.

## 5 Conclusion

We present AutoGuide, an effective framework for exploiting important domain knowledge from offline experiences for improving decision-making with pre-trained LLMs. We proposed to generate context-aware guidelines that can be incorporated into prompts for LLM agents. As AutoGuide extracts the guidelines by contrasting trajectories in offline data, the resulting context-aware guidelines carry critical information for preventing failures in the domains. For inference, it provides the guidelines pertinent to each of the different context that LLM agents encounter, which can make

\begin{table}
\begin{tabular}{c c c c} \hline \hline Algorithm & CI & GES & WebShop SR\(\uparrow\) \\ \hline ReAct & ✗ & ✗ & 30\% \\ ReAct + CI & ✓ & ✗ & 36\% \\ ReAct + GES & ✗ & ✓ & 37\% \\ AutoGuide & ✓ & ✓ & **46\%** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Ablation study of AutoGuide, analyzing each module’s contribution in WebShop. CI denotes our context identification module, and GES denotes the guideline extraction and selection modules.

\begin{table}
\begin{tabular}{c c} \hline \hline Algorithm & WebArena–Shopping \\ \cline{2-3}  & SR\(\uparrow\) \\ \hline ReAct & 10.2\% \\ AutoGuide & 20.4\% \\ \hline \hline \end{tabular}
\end{table}
Table 5: Out-of-distribution generalization of context-aware guidelines from WebShop on the 98 WebArena–Shopping tasks that have a product in the intent template.

pre-trained LLMs strong decision-making agents in the downstream domains. Empirically, we showed that AutoGuide outperforms strong baselines by a large margin and achieves outstanding performance in decision-making benchmarks.

## 6 Acknowledgements

This work was supported in part by LG AI Research.

## References

* [1] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. _arXiv preprint arXiv:2308.11432_, 2023.
* [2] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey. _arXiv preprint arXiv:2309.07864_, 2023.
* [3] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, et al. Do as i can, not as is say: Grounding language in robotic affordances. In _Conference on Robot Learning_, pages 287-318. PMLR, 2023.
* [4] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837, 2022.
* [5] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. _arXiv preprint arXiv:2401.13649_, 2024.
* [6] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. _arXiv preprint arXiv:2306.06070_, 2023.
* [7] Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowdhery, Sharan Narang, Noah Fiedel, and Aleksandra Faust. Understanding HTML with large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 2803-2821, Singapore, December 2023. Association for Computational Linguistics.
* [8] Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: A realistic web environment for building autonomous agents. _arXiv preprint arXiv:2307.13854_, 2023.
* [9] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In _ACL_, 2022.
* [10] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. _arXiv preprint arXiv:2301.00234_, 2022.
* [11] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In _EMNLP_, 2022.
* [12] Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbic Bradley, Roberta Raileanu, and Robert McHardy. Challenges and applications of large language models, 2023.
* [13] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v(ision) is a generalist web agent, if grounded. _arXiv preprint arXiv:2401.01614_, 2024.

* [14] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. Agentuning: Enabling generalized agent abilities for llms. _arXiv preprint arXiv:2310.12823_, 2023.
* [15] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In _International Conference on Machine Learning_, pages 9118-9147. PMLR, 2022.
* [16] Lajanugen Logeswaran, Yao Fu, Moontae Lee, and Honglak Lee. Few-shot subgoal planning with language models. In _NAACL: HLT_, 2022.
* [17] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In _International Conference on Machine Learning_, pages 10764-10799. PMLR, 2023.
* [18] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toollm: Facilitating large language models to master 16000+ real-world apis. _arXiv preprint arXiv:2307.16789_, 2023.
* [19] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model connected with massive apis. _arXiv preprint arXiv:2305.15334_, 2023.
* [20] Aaron Parisi, Yao Zhao, and Noah Fiedel. Talm: Tool augmented language models. _arXiv preprint arXiv:2205.12255_, 2022.
* [21] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. _arXiv preprint arXiv:2302.04761_, 2023.
* [22] Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner: Adaptive planning from feedback with language models. _arXiv preprint arXiv:2305.16653_, 2023.
* [23] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. _arXiv preprint arXiv:2210.03629_, 2022.
* [24] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In _ICLR_, 2021.
* [25] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimi Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. _arXiv preprint arXiv:2303.17651_, 2023.
* [26] Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. _arXiv preprint arXiv:2303.17491_, 2023.
* [27] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [28] SRK Branavan, David Silver, and Regina Barzilay. Learning to win by reading manuals in a monte-carlo framework. _Journal of Artificial Intelligence Research_, 43:661-704, 2012.
* [29] Austin W Hanjie, Victor Y Zhong, and Karthik Narasimhan. Grounding language to entities and dynamics for generalization in reinforcement learning. In _International Conference on Machine Learning_, pages 4051-4062. PMLR, 2021.
* [30] Victor Zhong, Tim Rocktaschel, and Edward Grefenstette. Rtfm: Generalising to new environment dynamics via reading. In _ICLR_, pages 1-17. ICLR, 2020.
* [31] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: LIm agents are experiential learners. _arXiv preprint arXiv:2308.10144_, 2023.

* [32] Richard S. Sutton and Andrew G. Barto. _Reinforcement Learning: An Introduction_. The MIT Press, second edition, 2018.
* [33] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webbsp: Towards scalable real-world web interaction with grounded language agents. _Advances in Neural Information Processing Systems_, 35:20744-20757, 2022.
* [34] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. In _The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2020.
* [35] Google Flights. https://www.google.com/travel/flights. Accessed: 2024-05-21.
* [36] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. _arXiv preprint arXiv:2310.11441_, 2023.

Limitation and Broader Impacts

**Limitation.** The performance of AutoGuide depends on the diversity of offline experiences. As such, one important direction for improvement is to automatically collect diverse offline experiences through continual learning, where we iteratively generate guidelines and use them to gather more trajectories with high rewards. Another avenue is the need for quantifying the quality of generated contexts and guidelines. Currently, apart from applying context-aware guidelines to ReAct and measuring test time performance, there lacks a standardized method for quantifying the quality of generated contexts and selected guidelines. Introducing a quantifiable metric to approximate the quality could pave the way for new optimization approaches such as reinforcement learning.

**Broader impact.** This paper introduces research aimed at enhancing the decision-making capabilities of LLMs. In terms of societal impact, while we develop a generic LLM-based autonomous agent, having biased offline datasets may lead to making decisions with suboptimal outcomes. Additionally, autonomous agents may be misused for malicious applications. To mitigate these risks, potential solutions would include diversifying datasets, implementing ethical oversight, ensuring transparency and accountability, engaging with stakeholders for a broader perspective, and incorporating security measures to prevent misuse. We believe that the research community, including ourselves, should responsibly advance LLM-based agent research, prioritizing societal well-being and ethical considerations.

## Appendix B Evaluation Details

### ALFWorld [24]

#### b.1.1 Environment details

Each task in ALFWorld starts with a description of the specific environment and the goal to achieve. At each timestep, an agent can choose one of the following actions to interact with the objects and receptacles in the environment:

* go to [recep]
* take [object] from [recep]
* put [object] in/on [recep]
* open/close/use [recep]
* clean/heat/cool [object] with [recep]

Alternatively, the agent can generate a think action for planning and reflection, which helps with decision-making but does not change the environment itself. After one action is performed, the environment returns an observation that describes view changes.

Following ReAct, we concatenate a list of (observation, action) pairs to show the entire trajectory up to the current timestep for LLM agents to generate the next action. We experiment on 134 unseen test tasks with 6 categories of \(\mathrm{pick\_and\_place}\), \(\mathrm{pick\_clean\_then\_place}\), \(\mathrm{pick\_heat\_then\_place}\), \(\mathrm{pick\_cool\_then\_place}\), \(\mathrm{look\_at\_obj}\), and \(\mathrm{pick\_two\_obj}\). For each task, the agent is allowed to take a maximum of 50 actions.

#### b.1.2 Baseline and Models

For ALFWorld tasks, we follow the same setting as ReAct by providing 2 in-context examples for each of the 6 task categories. The original results of ReAct in their paper are produced based on Text-Davinci-002. However, this GPT version is no longer available, so we apply gpt-3.5-turbo-instruct instead to generate actions. For \(\mathrm{ExpeL}\), we directly take the guidelines from their appendix and append them to the ReAct agent at test time.

#### b.1.3 Implementation details ofAutoGuide

We run the first 100 training tasks of ALFWorld to collect \((\bm{\tau}_{+},\bm{\tau}_{-})\) pairs with ReAct+Reflexion and extract context-dependant guidelines on the collected data. For context identification, we provide 2-shot demonstrations for each of the 6 task categories. The corresponding prompt templates can be found in appendix C. All parameter details are shown in table 7.

### WebShop [33]

#### b.2.1 Environment details

WebShop provides an e-commerce environment, where the objective is to find and buy the product that matches the task-specific Instruction. The agent can select one of the following actions to perform:

* search[query]
* click[button]

Following ReAct, the agent can generate think actions to do planning or reflection. After buying a product, the environment returns a reward showing how well the bought product matches the target one in type, price, buying options, and attributes. The reward is calculated by:

\[r\!=\!r_{type}\cdot\frac{|U_{att}\cap Y_{att}|\!+\!|U_{opt}\cap Y_{opt}|\!+\!1 [y_{price}\leq u_{price}]}{|U_{att}|\!+\!|U_{opt}|\!+\!1}\]

where y is the bought product and u is the target product. Same as ALFWorld, for WebShop, the agent takes (\(\mathrm{obs}_{t}\), \(\mathrm{act}_{t}\)) pairs for every previous timestep \(t\) as input to generate the next action.

#### b.2.2 Baseline and Models

Following ReAct, experiments are done in a one-shot setting. We apply gpt-3.5-turbo-0613 to generate actions, but when the token number exceeds the token limit (for example, for the n-shot ReAct experiments in table 1), we use the 16k version of gpt-3.5-turbo-0613 instead. For ExpeL, we could not find how many training tasks the framework used for training. Therefore, we directly apply the guidelines from the appendix of their paper at test time. We only consider ExpeL with guidelines, not ExpeL with in-context example selection in our experiments for a fair comparison. The in-context example selection method is orthogonal to our work and can be easily combined with our method. For Reflexion, as shown in their paper, their 2-shot Reflexion prompt does not work well on WebShop. Therefore, we re-write the prompt and apply gpt-4-1106-preview to generate episode-level reflections for all Reflexion experiments. Following Reflexion and ExpeL, the evaluation is done on 100 test tasks. The maximum number of allowed actions for each task is 15. At the same time, each search action shows the top 3 products for the search query. Please refer to Table 8 for more details about the experiments.

#### b.2.3 Implementation details of AutoGuide

We randomly sample 50 training tasks from the training set of WebShop, on which we run ReAct+Reflexion to collect pairs and generate guidelines. The context identification prompt is one-shot, which is shown in Appendix C. At test time, we ask gpt-3.5-turbo-0613 to select each state's most relevant top 2 guidelines.

\begin{table}
\begin{tabular}{c|c} \hline \hline Parameter name & Value \\ \hline Allowed Episode Length & 50 \\ n-shots & 2 \\ Agent Model & gpt-3.5-turbo-instruct \\ Context Identification Model & gpt-3.5-turbo-instruct \\ Guideline Selection Model & gpt-3.5-turbo-instruct \\ Guideline Extraction Model & gpt-4-1106-preview \\ Reflexion Model & gpt-4-1106-preview \\ top-k guideline selection & 2 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Experiment hyperparameters on ALFWorld. The maximum allowed episode length and n-shots follow the same setup in ReAct.

### WebArena [8]

#### b.3.1 Environment details

WebArena provides web-based benchmark environments that closely follow the data and functionality of real-world websites. Unlike other benchmarks like WebShop that provide clean text of website information as observation, WebArena's webpage content is represented as an accessibility tree, which is a subset of the DOM tree with useful elements of a webpage. For our expeirments, we focus on WebArena Reddit, which simulates the real Reddit websites with users, forums, and posts with abundant text information.

For each task in WebArena, the agent is expected to achieve a task-specific intent. At each timestep, WebArena provides a list of opened tabs, the accessibility tree of the focused webpage, and the URL of the current page as observation. For WebArena, each observation is long. Therefore, following the baseline in WebArena, at each timestep, we only provide the observation of the current timestep to the agent. We additionally provide up to 5 past actions for the agent to understand what it did in the past. The allowed actions in WebArena include the following:

* goto [url]
* click [element_id]
* type [element_id] [text] [1 for enter or 0 for not enter]
* press [key_combination]
* scroll [up or down]
* go_back

The maximum allowed number of actions for a single task is 20. Note that WebArena does not provide training tasks, but the work provides 19 demonstrations for Reddit, each of a different category. Therefore, we set these 19 tasks as the training tasks and then test on the rest 87 tasks.

#### b.3.2 Baseline and Models

We directly run the two-shot ReAct-style baseline in the official codebase of WebArena using gpt4-preview-1106. For ExpeL, the original paper does not include experiments on WebArena, therefore we try our best to implement our own version and run on the same training tasks as our method.

#### b.3.3 Implementation details of AutoGuide

We directly run ReAct on the tasks with \(\bm{\tau}_{+}\) to collect \(\bm{\tau}_{-}\) actions and generate guidelines correspondingly. We provide a 5-shot prompt for the context identification module, which is shown in Figure 7. At test time, the top 2 guidelines at each timestep are selected to guide action generation.

\begin{table}
\begin{tabular}{c|c} \hline \hline Parameter name & Value \\ \hline Allowed Episode Length & 15 \\ \# of Search Results & 3 \\ n-shots & 1 \\ Agent Model & gpt-3.5-turbo-0613 \\ Context Identification Model & gpt-3.5-turbo-0613 \\ Guideline Selection Model & gpt-3.5-turbo-0613 \\ Guideline Extraction Model & gpt-4-1106-preview \\ Reflexion Model & gpt-4-1106-preview \\ top-k guideline selection & 2 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Experiment hyperparameters on WebShop. The maximum allowed episode length, the number of search results per page, and n-shots follow the same setup in ReAct.

### Real Websites

#### b.4.1 Environment details

We design a set of real-world website navigation tasks from 3 domains: Software Development (GitHub), Travel (Google Flights), and Education (Coursera), which have 30, 20, and 20 test tasks accordingly. Here are some example tasks:

* Navigate to the repository for the Python library Seaborn with the most stars and show me all the open issues labeled with bug.
* Go to the GitHub org for Spotify and open the pinned project with the most stars for me.
* Google Flights
* Find me the one-way flight from Hong Kong to Paris departing on Oct 15th 2024 with the least emmisions.
* Show me the booking options of the one-way flight departing from Auckland on September 11, 2024, and arriving in Rome with the earliest departure time on that day.

* Show me a Cybersecurity course that can finish within 1 month and show me all the reviews for the selected course.
* Find me a Coursera guided project that covers Unity and show me its main page.

We follow the action space design of Visual WebArena [5], which has the following action types available:

* goto [url]
* click [element_id]
* hover [element_id]
* type [element_id] [text] [1 for enter or 0 for not enter]
* press [key_combination]
* scroll [up or down]
* tab_focus [tab_index]
* close_tab
* go_back
* go_forward

As the real websites are constantly and dynamically changing, we evaluate the completed task with human experts.

#### b.4.2 Baseline and Models

We directly run the two-shot SoM algorithm in the official codebase of Visual WebArena. The only modification we made from the original codebase is the bounding box detection algorithm, in

\begin{table}
\begin{tabular}{c|c} \hline \hline Parameter name & Value \\ \hline Allowed Episode Length & 20 \\ n-shots & 2 \\ Agent Model & gpt-4-1106-preview \\ Context Identification Model & gpt-4-1106-preview \\ Guideline Selection Model & gpt-4-1106-preview \\ Guideline Extraction Model & gpt-4-1106-preview \\ top-k guideline selection & 2 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Experiment hyperparameters on WebArena. The number of shots follows the same setup in ReAct.

[MISSING_PAGE_FAIL:17]

### Context Identification Prompt for WebShop

Figure 5: Our prompt template for context identification (Equation (1)) in the WebShop domain.

### Context Identification Prompt for ALFWorld

Figure 6: Our prompt template for context identification (Equation (1)) in the ALFWorld domain.

[MISSING_PAGE_EMPTY:20]

[MISSING_PAGE_EMPTY:21]

Figure 11: Our prompt template for context matching (Sections 3.2 and 3.3) in all the WebShop, ALFWorld, and WebArena domains.

Figure 12: Our prompt template for selecting the most relevant context-aware guidelines during the test time (Equation (3) from Section 3.3) in the WebShop and ALFWorld domains.

Figure 13: Our prompt template for selecting the most relevant context-aware guidelines during the test time (Equation (3)) in the WebArena domain.

**Contact On the Reddit main page.**

**Contact On the Reddit main page, if you want to change your bio, you should click on the "link 'Profile", which is located in the user menu dropdown, right above the "link 'My account". The correct action format to do this is "click [profile_link_id]".**

**When on the Reddit main page, if you want to navigate to a specific forum, you should click on the "link 'Forming', which is located at the early part of the observation, right above the link 'My'. The correct action format to do this is "click [link_id]".**

**When on the Reddit main page, if you want to create a new forum, you should click on the "link 'Forming', which is located near the top of the observation, right above the "link 'Wait'. The correct action format to do this is "click [link_id]".**

**When on the Reddit main page, if you want to click all submissions created by a specific user in a specific subreddit, you can directly navigate to the user's page with the action format "goto [url]", replacing [url] with the user's page [URL, formatted as http://platform_domain/user/custemame.**

**Contact On the overview page of a Reddit user.**

**Contact On the overview page of a Reddit user.**

**Contact On the description of the "Submissions' tab to filter the content by the user's submissions. The correct action format to do this is "click [link_id]", where [link_id] is the ID of the "Submissions' link in the main content area of the page.**

**Contact On the biography edit page of a Reddit user.**

**Contact On the biography edit page of a Reddit user, if you want to change the biography text, you should do these sequentially: 1. Click on the textbox "Bigography to focus it, located in the main section of the page, with action "click [textbox_id]".** 2. Select all text inside the textbox using the action "press [Metaa]".** 3. Clear the selected text with the action "press [Backspace]".** 4. Type the new biography content into the textbox "Bigography with action "type [textbox_id] [new_content] [1]".** 5. Click on the button "Snev, located below the textbox "Bigography", to submit the changes with action "click [button_id]". After these steps, issue a stop action when the task is complete.**

**Contact On the page listing all forums on a Reddit-like platform.**

**Contact On the page listing all forums on a Reddit-like platform.**

**Contact On the description of the "link 'Alphabetical", which is located in the main area, to sort forums alphabetically. The correct action format to do this is "click [link_id]".** 2. After the forums are sorted, click on the specific "link'storm_name's" that you wish to navigate to. The correct action format to do this is "click [link_id]".**

**Contact On the Reddit page to create submission.**

**Contact Aware Guadined**

**When on the Reddit page to create submission, if you have filled in the "Title" and "Body" textbooks but do not see the button 'Create submission', you should scroll down to reveal more of the page. The correct action format to do this is "script [down]". After scrolling, if you want to submit the post, you should click on the button 'Create submission', which is located after the "Body" textbook. The correct action format to submit the post is "click [button_id]".**

**Contact On the main page of a Reddit forum.**

**Contact On the description of the "link 'Working', which is located in the main page of a Reddit forum,**

**Contact Aware Guadined**

**When on the main page of a Reddit forum, if you want to find posts related to a specific topic among the top posts, you should first sort the posts by their popularity to ensure you are viewing the most relevant content. To do this, you can: 1. click on the "button Sort by: hot", which is located in the main section of the page, below the forum heading and above the first article. The correct action format to do this is "click [id]".** 2. After the sorting options have expanded, click on the "link "Top", which will appear as a new option under the sorting button. This action should be repeated once. The correct action format to do this is "click [id]".**

**When on the main page of a Reddit forum, if you want to create a new portal, you should click on the "link 'Submit", which is located in the early part of the observation, right below the "link 'Notifications [0]". The correct action format to do this is "click [link_id]".**

**Contact On the submissions page of a Reddit user.**

**Contact Aware Guadined**

**When on the submissions page of a Reddit user, if you want to perform an action on specific subreddit submissions but do not see them, you should scroll down to reveal more submissions. The correct action format to do this is " scroll [down]". After scrolling, if you find a submission from the desired subreddit, such as "UrightingWeise", and need to downsee it, click on the button 'Downsee' located at the end of the submission's details. The correct action format to do this is "click [downnote_id]". Once all required actions are performed on the submission, issue a stop action with the format "stop [1]".**

**Contact On the submissions page of a Reddit forum sorted by top.**

**Contact Aware Guadined**

**When on the submissions page of a Reddit forum sorted by top, if you want to review posts but see "There's nothing here. ", you should expand the time range to view more posts. Do this by clicking on the button "From: Part 24 hours", which is located in the main section of the page, below the heading "//books" and above the StaticText "There's nothing here...". The correct action format to do this is "click [button_id]". After expanding the time range, if you find a post that meets the task criteria, issue a stop action.**

**Contact On the submissions page of a Reddit forum sorted by new.**

**Contact Own Guadined**

When on the submissions page of a Reddit forum sorted by new, if you want to upvote the newest post and you have already clicked the upvote button for the first article entry, you should issue the stop action. The correct action format to do this is "stop".**

**Contact On the page of a Reddit post.**

**Contact Aware Guadined**

When on the page of a Reddit post, if you have already navigated to the "Submit" link, filled in the image [URL and title, and clicked the 'Create submission' button, you should consider the task complete. The correct action format to do this is "stop".**

**Contact On the description of the "Working', which is located in the main page of a Reddit forum.**

**Contact Own Guadined**

When on the submissions page of a Reddit forum, if you want to perform an action on specific subreddit submissions but do not see them, you should scroll down to reveal more submissions. The correct action format to do this is "click [down]". After scrolling, if you find a submission from the desired subreddit, such as "UrightingWeise", and need to downsee it, click on the button 'Downsee' located at the end of the submission's details. The correct action format to do this is "click [downnote_id]". Once all required actions are performed on the submission, issue a stop action with the format "stop [1]".**

**Contact On the submissions page of a Reddit forum sorted by top.**

**Contact Own Guadined**

When on the submissions page of a Reddit forum sorted by top, if you want to review posts but see "There's nothing here. ", you should expand the time range to view more posts. Do this by clicking on the button "From: Part 24 hours", which is located in the main section of the page, below the heading "////books" and above the StaticText "There's nothing here...". The correct action format to do this is "click [button_id]". After expanding the time range, if you find a post that meets the task criteria, issue a stop action.**

**Contact On the submissions page of a Reddit forum sorted by new.**

**Contact Own Guadined**

When on the submissions page of a Reddit forum sorted by new, if you want to upvote the newest post and you have already clicked the upvote button for the first article entry, you should issue the stop action. The correct action format to do this is "stop".**

**Contact On the description of the "Working', which is located in the main page of a Reddit post.**

**Contact Own Guadined**

When on the page of a Reddit post, if you want to change your bio, you should click on the "link 'Profile", which is located in the user menu dropdown, right above the "link 'My account". The correct action format to do this is "click [profile_link_id]".**

**Contact Own Guadined**

When on the description of the "Working', which is located in the main page of a Reddit user, if you want to navigate to a specific subreddit, you should first navigate to the "Submissions" tab to filter the content by the user's submissions. The correct action format to do this is "click [link_id]", where [link_id] is the ID of the 'Submissions' link in the main content area of the page.**

**Contact Own Guadined**

When on the biography edit page of a Reddit user, if you want to change the biography text, you should do these sequentially: 1. Click on the textbox "Bigography to focus it, located in the main section of the page, with action "click [backbox_id]". 2. Select all text inside the textbox using the action "press [Metaa]". Clear the selected text with the action "press [Backspace]". 4. Type the new biography content into the textbox "Bigography", to submit the changes with action "click [button_id]". After these steps, issue a stop action when the task is complete.**

**Contact On the page listing all forums on a Reddit-like platform.**

**Contact Own Guadined**

When on the page listing all forums on a Reddit-like platform, if you want to navigate to a specific forum, you should do these sequentially: 1. click on the "link [Alphabetical", which is located in the main area, to sort forums alphabetically. The correct action format to do this is "click [link_id]".**

**Contact Own Guadined**

When on the main page of a Reddit forum, if you want to find posts related to a specific topic among the top posts, you should first sort the posts by their popularity to ensure you are viewing the most relevant content. To do this, you can: 1. click on the "button Sort by: hot", which is located in the main section of the page, below the forum heading and above the first article. The correct action format to do this is "click [id]".**

**Contact Own Guadined**

When on the description of the "Working', which is located in the main section of the page, below the forum heading and above the first article. The correct action format to do this is "click [join]".**

**Contact Own Guadined**

When on the submissions page of a Reddit forum sorted by new, if you want to move it to the next article entry, you should issue the stop action. The correct action format to do this is "click [link_id]".**

**Contact Own Guadined**

When on the description of the "Working', which is located in the main section of the page, below the heading "////books" and above the StaticText "There's nothing here. ". The correct action format to do this is "click [button_id]". After expanding the time range, if you find a post that meets the task criteria, issue a stop action.**

**Contact Own Guadined**

When on the submissions page of a Reddit forum sorted by new, if you want to upvote the newest post and you have already clicked the upvote button for the first article entry, you should issue the stop action. The correct action format to do this is "stop".**

**Contact Own Guadined**

When on the description of the "Working', which is located in the main section of the page, below the heading "//books" and above the StaticText "There's nothing here. ". The correct action format to do this is "click [button_id]". After expanding the time range, if you find a post that meets the task criteria, issue a stop action.**

**Contact Own Guadined**

When on the submissions page of a Reddit forum sorted by new, if you want to upvote the newest post and you have already clicked the upvote button for the first article entry, you should issue the stop action. The correct action format to do this is "stop".**

**Contact Own Guadined**

When on the description of the "Working', which is located in the main section of the page, below the heading "//books" and above the StaticText "There's nothing here. ". The correct action format to do this is "click [button_id]". After expanding the time range, if you find a post that meets the task criteria, issue a stop action.**

**Contact Own Guadined**

When on the submissions page of a Reddit forum sorted by new, if you want to

[MISSING_PAGE_FAIL:24]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In the abstract and introduction, we clearly outline our main contribution: extracting context-aware guidelines from offline data to enhance the decision-making of LLM-based agents. We support the effectiveness of our method through empirical evaluations in Section 4. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We outline potential limitations of AutoGuide in Appendix A. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA] Justification: This paper does not include theoretical contributions. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Section 4.1 and Appendix B provide detailed information (e.g., model versions, hyperparameters, number of training data) for reproducing our experimental results. Additionally, Appendix C contains our prompt templates. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: We intend to release the source code upon acceptance. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We detail our experimental settings in Section 4.1 and Appendix B. Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: We provide the average rewards and success rates calculated across test tasks in our results, but we do not include the variance. Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Our experiments are based on GPT, and we detail the versions of GPT in Appendix B. Guidelines:

* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our work conforms the NeurIPS Code of Ethics. Guidelines:

* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We provide the broader impacts of our work in Appendix A. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper makes the algorithmic contribution and does not release new data or models. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We reference the original papers and sources throughout the paper. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.