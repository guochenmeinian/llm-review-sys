# Unified lower bounds for interactive high-dimensional estimation under information constraints

 Jayadev Acharya

Cornell University

acharya@cornell.edu

&Clement L. Canonne

University of Sydney

clement.canonne@sydney.edu.au

Ziteng Sun

Google Research, New York

zitengsun@google.com

&Himanshu Tyagi

Indian Institute of Science, Bangalore

htyagi@iisc.ac.in

###### Abstract

We consider distributed parameter estimation using interactive protocols subject to _local information constraints_ such as bandwidth limitations, local differential privacy, and restricted measurements. We provide a unified framework enabling us to derive a variety of (tight) minimax lower bounds for different parametric families of distributions, both continuous and discrete, under any \(\ell_{p}\) loss. Our lower bound framework is versatile and yields "plug-and-play" bounds that are widely applicable to a large range of estimation problems. In particular, our approach recovers bounds obtained using data processing inequalities and Cramer-Rao bounds, two other alternative approaches for proving lower bounds in our setting of interest. Further, for the families considered, we complement our lower bounds with matching upper bounds.

## 1 Introduction

We consider the problem of parameter estimation under _local information constraints_, where the estimation algorithm has access to only limited information about each sample. These constraints can be of various types, including communication constraints, where each sample must be described using a few (_e.g._, constant number of) bits; (local) privacy constraints, where each sample is obtained from a different user and the users seek to reveal as little as possible about their specific data; as well as many others, _e.g._, noisy communication channels, or limited types of data access such as linear measurements. Such problems have received significant attention in recent years, motivated by applications such as data analytics in distributed systems and federated learning.

Our main focus is on information-theoretic lower bounds for the minimax error rates (or, equivalently, the sample complexity) of these problems. Several recent works have provided bounds that apply to specific constraints or work for specific parametric estimation problems, sometimes without allowing for interactive protocols. Indeed, handling interactive protocols is technically challenging, and several results in prior work exhibit flaws in their analysis. In particular, even the most basic Gaussian mean estimation problem using interactive communication remains, surprisingly, open.

We present general, "plug-and-play" lower bounds for parametric estimation under information constraints that can be used for any local information constraint and allows for _interactive_ protocols. Our abstract bound requires very simple (and natural) assumptions to hold for the underlying parametric family; in particular, we do not require technical "regularity" conditions that are common in asymptotic statistics.

We apply our general bound to canonical problems of high-dimensional mean estimation and distribution estimation, under privacy and communication constraints, for the entire family of \(\ell_{p}\) loss functions for \(p\geq 1\). In addition, we provide complementary schemes that show that our lower bounds are tight for most settings of interest.

### Our results

Our main contribution is a general approach to establish lower bounds in distributed information-constrained parameter estimation. The setup is described in detail in Section 2 and is illustrated in Fig. 1. In short, independent samples \(X^{n}=(X_{1},\ldots,X_{n})\) are generated from an unknown distribution \(\mathbf{p}\) from a parametric family \(\mathcal{P}_{\Theta}=\{\mathbf{p}_{\theta},\theta\in\Theta\}\) of distributions. Only limited information \(Y_{i}\) about datum \(X_{i}\) is available to the algorithm. The goal is to estimate the underlying parameter \(\theta\) associated with \(\mathbf{p}\). Furthermore, we consider interactive estimation, wherein \(Y_{i}\) can depend on \(Y_{1},\ldots,Y_{i-1}\). Our general lower bound, which we develop in Section 3, takes the following form: Consider a collection of distributions \(\{\mathbf{p}_{z}\}_{z\in\{-1,+1\}^{k}}\subseteq\mathcal{P}_{\Theta}\) contained in the parametric family. This collection represents a "difficult subproblem" that underlies the parametric estimation problem being considered; such constructions are often used when deriving information-theoretic lower bounds (_e.g._, in Assouad's method [33]). Note that each coordinate of \(z\) represents, in essence, a different "direction" of uncertainty for the parameter space. The difficulty of the estimation problem can be related to the difficulty of determining a randomly chosen \(z\) (or most of the coordinates of \(z\)), denoted \(Z\), by observing samples from \(\mathbf{p}_{z}\). Once \(Z=z\) is fixed, \(n\) independent samples \(X^{n}=(X_{1},\ldots,X_{n})\) are generated from \(\mathbf{p}_{z}\) and the limited information \(Y_{i}\) about \(X_{i}\) is passed to an estimator.

Our most general result, stated as Theorem 1, is an upper bound for the average discrepancy, an average distance quantity related to average probability of error in determining coordinates of \(Z\) by observing the limited information \(Y^{n}=(Y_{1},\ldots,Y_{n})\). Our bounding term reflects the underlying information constraints using a quantity that captures how "aligned" we can make our information about the sample to the uncertainty in different coordinates of \(Z\). Importantly, our results hold under minimal assumptions. In particular, in contrast to many previous works, our results do not require any "bounded ratio" assumption on the collection \(\{\mathbf{p}_{z}\}_{z\in\{-1,+1\}^{k}}\), which would ask that the density function change by at most a constant factor if we modify one coordinate of \(z\). When we impose additional structure for \(\mathbf{p}_{z}\) - such as orthogonality of the random changes in density when we modify different coordinates of \(z\) and, more stringently, independence and subgaussianity of these changes - we get concrete bounds which are readily applicable to different problems. These plug-and-play bounds are stated as consequences of our main result in Theorem 2. The interested reader can also directly consider the applications to local privacy (Corollary 1) or communication constraints (Corollary 2).

We demonstrate the versatility of the framework by showing that it readily yields tight (and in some cases nearly tight) bounds for parameter estimation (both in the sparse and dense cases) for several fundamental families of continuous and discrete distributions, several families of information constraints such as communication and local differential privacy (LDP), and for the family of \(\ell_{p}\) loss functions for \(p\geq 1\), all when interactive protocols are allowed. To complement our lower bounds, we provide algorithms (protocols) which attain the stated rates, thus establishing optimality of our results.1We discuss these results in Section 5, where we provide the corresponding statements. In terms of the applications, our contributions are two-fold:

Footnote 1: Up to a logarithmic factor in the case of \(\ell_{\infty}\) loss, or, for some of our bounds, with a mild restriction on \(n\) being large enough.

1. We obtain several results from a diverse set of prior works in a unified fashion as simple corollaries of our main result: As discussed further in Section 1.2, the lower bounds for mean estimation for product Bernoulli under \(\ell_{2}\) loss and those for estimation of discrete distributions under \(\ell_{1}\) and \(\ell_{2}\) losses, for both communication and LDP constraints, were known from previous work. However, our approach allows us to easily recover those results and extend them to arbitrary \(\ell_{p}\) losses, with interaction allowed, in a unified fashion.
2. Our bounds also yield new lower bounds for some canonical problems. The prototypical example being mean estimation for high-dimensional Gaussian distributions under information constraints. As discussed in the next section, while some prior work claimed lower bounds for this problem, their arguments appear to be flawed - at a high level, due to the "bounded ratio" assumption their techniques rely on, which Gaussian distributions do not satisfy, and which our framework does not require. To the best of our knowledge our work is the first to obtain those lower bounds for interactive mean estimation of high-dimensional Gaussian distributions under communication or local privacy constraints.

### Previous and related work

There is a significant amount of work in the literature dedicated to parameter estimation under various constraints and settings. Here, we restrict our discussion to works that are most relevant to the current paper, with a focus on the interactive setting (either the sequential or blackboard model; see Section 2 for definitions).

The work arguably closest to ours is the recent work [5], which focuses on density estimation and goodness-of-fit testing of discrete distributions, under the \(\ell_{1}\) metric, for sequentially interactive protocols under general local information constraints (including, as special cases, local privacy and communication constraints, as in the present paper). This work can be seen as a significant generalization of the techniques of [5], allowing us to obtain lower bounds for estimation in a variety of settings, notably high-dimensional parameter estimation.

Among other works on high-dimensional mean estimation under communication constraints, [17; 10] consider communication-constrained Gaussian mean estimation in the blackboard communication model, under \(\ell_{2}\) loss. The protocols for the upper bounds in these works do not require interactivity and are complemented with lower bounds which show that the bounds are tight up to constant factors in the dense case and up to logarithmic factors in the sparse case. However, the proof of the lower bound in [10] seems to present a gap (specifically, in the truncation argument of [10, Theorem 4.3]), as confirmed in personal communication with the authors. Correcting the issue in the truncation argument would lead to a result significantly weaker than the claimed lower bound, and it is unclear whether this can be fixed using the techniques from that paper.

In this work, we present interactive protocols for the sparse case which improve over the noninteractive protocols and strenghten the upper bounds by a logarithmic factor in the interactive case (see Remark 1). Further, using our general framework, we establish a nearly-matching lower bound for the problem, recovering the rate lower bound originally claimed in [10] up to a logarithmic factor. In a slightly different setting, [26] considers the mean estimation problem for product Bernoulli distributions when the mean vector is \(1\)-sparse, under \(\ell_{2}\) loss. The lower bound in [26], too, allows sequentially interactive protocols. In the blackboard communication model, [19] and [18] obtained tight bounds for mean estimation and density estimation under \(\ell_{2}\) and \(\ell_{1}\) loss, respectively.

Turning to local privacy, [23] provide upper bounds (as well as some partial lower bounds) for one-dimensional Gaussian mean estimation under LDP under the \(\ell_{2}\) loss, in the sequentially interactive model. Recent works of [7] and [8] obtain lower bounds for mean estimation in the blackboard communication model and under LDP, respectively, for both Gaussian and product Bernoulli distributions; as well as density estimation for discrete distributions. Their approach is based on the classic Cramer-Rao bound and, as such, is tied inherently to the use of the \(\ell_{2}\) loss. In a recent independent work, [25] extended these methods to obtain lower bounds under general \(\ell_{p}\) loss under communication constraints, which are tight for Gaussian mean estimation under noninteractive protocols. [14], by developing a locally private counterpart of some of the well-known information-theoretic tools for establishing statistical lower bounds (namely, Le Cam, Fano, and Assouad), establish tight or nearly tight bounds for several mean estimation problems in the LDP setting.

More recently, drawing on machinery from the communication complexity literature, [13] develop a methodology for proving lower bounds under LDP constraints in the blackboard communication model. They obtain lower bounds for mean estimation of product Bernoulli distributions under general \(\ell_{p}\) losses which match ours (in the high-privacy regime, _i.e._, small \(\varepsilon\)). Similar to the results under communication constraints [17; 10], their approach relies heavily on the assumption that the distributions on each coordinate are independent, which fails to generalize to discrete distributions. Further, their bounds are tailored to the LDP constraints and do not seem to extend to arbitrary information constraints. Finally, while [13] also claims tight bounds for mean estimation of Gaussian and sparse Gaussian distributions under \(\ell_{2}\) loss, their argument invokes the analogous (flawed) result from [10] and thus it is unclear whether the stated lower bound can be shown using their techniques.

Finally, we mention that very recently, following the appearance of [5], an updated version of [19] appeared online as [21], which has similar results as ours for the high-dimensional mean estimation problem under communication constraints. Both our work and [21] build upon the framework presented for the discrete setting in [5]. Moreover, their work still need the "bounded ratio", and hence their lower bound for sparse Gaussian family only works for noninteractive protocols.

**Notation.** Hereafter, we write \(\log\) and \(\ln\) for the binary and natural logarithms, respectively. For distributions \(\mathbf{p}_{1},\mathbf{p}_{2}\) over \(\mathcal{X}\), denote their Kullback-Leibler divergence (in nats) by \(\mathrm{D}(\mathbf{p}_{1}\|\mathbf{p}_{2})\), and their Hellinger distance by \(\mathrm{d}_{\mathrm{H}}(\mathbf{p}_{1},\mathbf{p}_{2}):=(\frac{1}{2}\int( \sqrt{\frac{\mathrm{d}\mathbf{p}_{1}}{\mathrm{d}\lambda}}-\sqrt{\frac{ \mathrm{d}\mathbf{p}_{2}}{\mathrm{d}\lambda}})^{2}\,\mathrm{d}\lambda)^{1/2}\,,\) where we assume \(\mathbf{p}_{1},\mathbf{p}_{2}\ll\lambda\) for some underlying measure \(\lambda\) on \(\mathcal{X}\). Further, we denote the Shannon entropy of a random variable \(X\) by \(H(X)\) and the mutual information between \(X\) and \(Y\) by \(I(X;Y)\); we will sometimes write \(H(\mathbf{p})\) for the entropy of a random variable with distribution \(\mathbf{p}\). We refer the reader to [12] for details on these notions and their properties, which will be used throughout. Given two functions \(f,g\), we write \(f\lesssim g\) if there exists an absolute constant \(C>0\) such that \(f(x)\leq Cg(x)\) for all \(x\); and \(f\asymp g\) if \(f\lesssim g\) and \(f\gtrsim g\) both hold. Finally, we use the standard asymptotic notation \(O(f)\), \(\Omega(f)\), \(\Theta(f)\).

**Organization.** In Section 2, we formalize our setting of interactive inference under local information constraints. The general lower bound framework and results are presented in Section 3, where we provide implications of the general result under additional structures and specific information constraints such as local privacy (LDP) and communication constraints. Finally, we use our framework to readily derive lower bounds for a wide range of applications in Section 5. Due to space constraints, all proofs, as well as our upper bounds (algorithms) are provided in the Supplement, where we also discuss how our techniques compare with other existing approaches for proving lower bounds under information constraints, namely, those based on strong data processing inequalities (SDPI) or on the van Trees inequality.

## 2 The setup

We consider standard parametric estimation problems. For some \(\Theta\subseteq\mathbb{R}^{d}\), let \(\mathcal{P}_{\Theta}=\{\mathbf{p}_{\theta},\theta\in\Theta\}\) be a family of distributions over some measurable space \((\mathcal{X},\mathfrak{X})\), namely each \(\mathbf{p}_{\theta}\) is a distribution over \((\mathcal{X},\mathfrak{X})\). Suppose \(n\) independent samples \(X^{n}=(X_{1},\ldots,X_{n})\) from an unknown \(\mathbf{p}_{\theta}\in\mathcal{P}_{\Theta}\) are obtained. The goal in parametric estimation is to design estimators \(\hat{\theta}:\mathcal{X}^{n}\to\Theta\), and form estimates \(\hat{\theta}(X^{n})\) of \(\theta\) using independent samples \(X^{n}\) from \(\mathbf{p}_{\theta}\). We illustrate our results using two specific distribution families: discrete probability mass functions (pmfs) and high-dimensional product distributions with unknown mean vectors. We will describe the precise minimax setting in detail later in this section.

We are interested in an information-constrained setting, where we do not have direct access to the samples \(X^{n}\) from \(\mathbf{p}_{\theta}\). Instead, we can only obtain limited information about each datapoint \(X_{i}\). Following [3], we model these information constraints by specifying an allowed set of _channels_\(\mathcal{W}\) with input alphabet \(\mathcal{X}\) and some output space \(\mathcal{Y}\).2 Each sample \(X_{i}\) is passed through a channel from \(\mathcal{W}\), chosen appropriately, and its output \(Y_{i}\) is the observation we get. This setting is quite general and captures as special cases the popular communication and privacy constraints, as we will describe momentarily.

Footnote 2: Formally, a channel is a Markov kernel \(W\colon\mathfrak{Y}\times\mathcal{X}\to[0,1]\), which we assume to be absolutely continuous with respect to some underlying measure \(\mu\) on \((\mathcal{Y},\mathfrak{Y})\). When clear from the context, we will drop the reference to the \(\sigma\)-algebras \(\mathfrak{X}\) and \(\mathfrak{Y}\); in particular, in the case of finite \(\mathcal{X}\) or \(\mathcal{Y}\).

We now formally describe the setting, which is illustrated in Fig. 1. \(n\) i.i.d. samples \(X_{1},\ldots,X_{n}\) from an unknown distribution \(\mathbf{p}_{\theta}\in\mathcal{P}_{\Theta}\) are observed by players (users) where player \(t\) observes \(X_{t}\). Player \(t\in[n]\) selects a channel \(W_{t}\in\mathcal{W}\) and sends the message \(Y_{t}\) to a referee, where \(Y_{t}\) is drawn from the probability measure \(W_{t}(\cdot\mid X_{t},Y_{1},\ldots,Y_{t-1})\). The referee observes \(Y^{n}:=(Y_{1},\ldots,Y_{n})\) and seeks to estimate the parameter \(\theta\).

The freedom allowed in the choice of \(W_{t}\) at the players gives rise to various communication protocols. We focus on _interactive protocols_, where channels are chosen by one player at a time, and they can use all previous messages to make this choice. We describe this class of protocols below, where we further allow each player \(t\) to have a different set of constraints \(\mathcal{W}_{t}\) (_e.g._, a different communication budget), and \(W_{t}\) must be in \(\mathcal{W}_{t}\). For simplicity of exposition, and as these already encapsulate most of the difficulties, we focus here on the case of _sequentially_ interactive protocols, which has been widely considered in the literature and captures many settings of interest. However, we emphasize that our results extend to the more general class of fully interactive protocols (see Supplement).

_Definition 1_ (Sequentially Interactive Protocols).: Let \(X_{1},\ldots,X_{n}\) be i.i.d. samples from \(\mathbf{p}_{\theta}\), \(\theta\in\Theta\). A _sequentially interactive protocol \(\Pi\) using \(\mathcal{W}^{n}=(\mathcal{W}_{1},\ldots,\mathcal{W}_{n})\)_ involves mutually independent random variables \(U,U_{1},\ldots,U_{n}\) (independent of the input \(X_{1},\ldots,X_{n}\)) and mappings \(g_{t}\colon(U,U_{t})\mapsto W_{t}\in\mathcal{W}_{t}\) for selecting the channel in round \(t\in[n]\). In round \(t\), player \(t\) uses the channel \(W_{t}\) to produce the message (output) \(Y_{t}\) according to the probability distribution \(W_{t}(\cdot\mid X_{t},Y_{1},\ldots,Y_{t-1})\). The messages \(Y^{n}=(Y_{1},\ldots,Y_{n})\) received by the referee and the public randomness \(U\) (available to all players) constitute the _transcript_ of the protocol \(\Pi\); the private randomness \(U_{1},\ldots,U_{n}\) (where \(U_{t}\) is local to player \(t\)) is not part of the transcript. In other words, the channel at player \(t\) as a (randomized) mapping \(W\colon\mathcal{X}\times\mathcal{Y}^{t-1}\to\mathcal{Y}\), which depends on input \(x\in\mathcal{X}\) and the previous \(t-1\) messages \(y^{t-1}\in\mathcal{Y}^{t-1}\) outputs some \(y\in\mathcal{Y}\).

For concreteness, we now instantiate this definition for the two aforementioned types of information constraints, communication and (local) privacy.

Communication constraintsLet \(\mathcal{Y}:=\{0,1\}^{*}=\bigcup_{m=0}^{\infty}\{0,1\}^{m}\). For \(\ell\geq 1\) and \(t\geq 1\), let

\[\mathcal{W}^{\mathrm{comm},\ell}:=\{W\colon\mathcal{X}\times\mathcal{Y}^{*} \to\{0,1\}^{\ell}\}\] (1)

be the family of channels with input alphabet \(\mathcal{X}\) and output alphabet the set of all \(\ell\)-bit strings. This captures the constraint where the message from each player can be at most \(\ell\) bits long, and corresponds to the choice \(\mathcal{W}^{n}=(\mathcal{W}^{\mathrm{comm},\ell},\ldots,\mathcal{W}^{ \mathrm{comm},\ell})\). Note that allowing a different communication budget to each player can be done by setting \(\mathcal{W}^{n}=(\mathcal{W}^{\mathrm{comm},\ell_{1}},\ldots,\mathcal{W}^{ \mathrm{comm},\ell_{n}})\).

Local differential privacy constraintsFor \(\varepsilon>0\) and \(t\geq 1\), a channel \(W\colon\mathcal{X}\times\mathcal{Y}^{t-1}\to\mathcal{Y}\) is _\(\varepsilon\)-locally differentially private (LDP)_[16, 15, 24] if

\[\sup_{S\in\mathfrak{V}}\sup_{y^{t-1}\in\mathcal{Y}^{t-1}}\frac{W(S\mid x_{1}, y^{t-1})}{W(S\mid x_{2},y^{t-1})}\leq e^{\varepsilon},\quad\forall x_{1},x_{2}\in \mathcal{X}.\] (2)

We denote by \(\mathcal{W}^{\mathrm{priv},\varepsilon}\) the set of all \(\varepsilon\)-LDP channels. For sequentially interactive protocols, the \(\varepsilon\)-LDP condition is captured by setting \(\mathcal{W}^{n}=(\mathcal{W}^{\mathrm{priv},\varepsilon},\ldots,\mathcal{W}^{ \mathrm{priv},\varepsilon})\). As before, one can allow different privacy parameters for each player by setting \(\mathcal{W}^{n}=(\mathcal{W}^{\mathrm{priv},\varepsilon_{1}},\ldots,\mathcal{W }^{\mathrm{priv},\varepsilon_{n}})\).

Finally, we formalize the interactive parametric estimation problem for the family \(\mathcal{P}_{\Theta}=\{\mathbf{p}_{\theta},\theta\in\Theta\}\). We consider the problem of estimating \(\theta\) under \(\ell_{p}\) loss. For \(p\in[1,\infty)\), the \(\ell_{p}\) distance between

Figure 1: The information-constrained distributed model. In the interactive setting, \(W_{t}\) can depend on the previous messages \(Y_{1},\ldots,Y_{t-1}\) (dotted, upwards arrows).

\(u,v\in\mathbb{R}^{d}\) is \(\ell_{p}(u,v)=\left\|u-v\right\|_{p}=\left(\sum_{i=1}^{d}|u_{i}-v_{i}|^{p}\right) ^{1/p}.\) This definition extends in a natural way to \(p=\infty\) by taking the limit.3

Footnote 3: By Hölder’s inequality, we also have \(\ell_{\infty}(u,v)\leq\ell_{p}(u,v)\leq d^{1/p}\ell_{\infty}(u,v)\) for all \(p\geq 1\) and \(u,v\in\mathbb{R}^{d}\), which implies that for \(p:=\log d\) we have \(\ell_{\infty}(u,v)\leq\ell_{p}(u,v)\leq 2\ell_{\infty}(u,v)\): i.e., \(\ell_{\log d}(u,v)\) gives a factor-2 approximation of the \(\ell_{\infty}\) loss. This further extends to \(s\)-sparse vectors, with a factor \(s^{1/p}\) instead of \(d^{1/p}\).

_Definition 2_ (Sequentially Interactive Estimates).: Fix \(d\in\mathbb{N}\) and \(p\in[0,\infty]\). Given a family \(\mathcal{P}_{\Theta}\) of distributions on \(\mathcal{X}\), with \(\Theta\subset\mathbb{R}^{d}\), an _estimate_ for \(\mathcal{P}_{\Theta}\) consists of a sequentially interactive protocol \(\Pi\) with transcript \((Y^{n},U)\) and estimator \(\hat{\theta}\colon(Y^{n},U)\mapsto\hat{\theta}(Y^{n},U)\in\Theta\). The referee observes the transcript \((Y^{n},U)\) and forms the estimate \(\hat{\theta}(Y^{n},U)\) of the unknown \(\theta\). Further, for \(n\in\mathbb{N}\) and \(\gamma\in(0,1)\), \((\Pi,\hat{\theta})\) constitutes an \((n,\gamma)\)-_estimator_ for \(\mathcal{P}_{\Theta}\) using \(\mathcal{W}\) under \(\ell_{p}\) loss if for every \(\theta\in\Theta\) the transcript \((Y^{n},U)\) of \(\Pi\) satisfies

\[\mathbb{E}_{\mathbf{p}_{\theta}^{n}}\Big{[}\ell_{p}(\theta,\hat{\theta}(Y^{n},U))^{p}\Big{]}^{1/p}\leq\gamma.\]

Note that the expectation is over the input \(X^{n}\sim\mathbf{p}_{\theta}^{n}\) for the protocol \(\Pi\) and the randomness of \(\Pi\).

## 3 Main result: The information contraction bound

Our main result is a unified framework to bound the information revealed about the unknown \(\theta\) by the transcript of the messages obtained via the constraints defined by the channel family \(\mathcal{W}\). The framework is versatile and provides tight bounds for several families of continuous and discrete distributions, several families of information constraints such as communication and local differential privacy, and for the family of \(\ell_{p}\) loss functions for \(p\geq 1\).

Our approach at a high-level proceeds as below: We first consider the "pertubation space" \(\mathcal{Z}:=\{-1,+1\}^{k}\), for some suitable \(k\). We associate with each \(z\in\mathcal{Z}\) a parameter \(\theta_{z}\in\Theta\), and refer to \(\mathbf{p}_{\theta_{z}}\) simply as \(\mathbf{p}_{z}\). These distributions are designed in a way that the distance between \(\theta_{z}\) and \(\theta_{z^{\prime}}\) is large when the Hamming distance between \(z\) and \(z^{\prime}\) is large. With this, the difficulty of estimating \(\theta\) will be captured in the difficulty of estimating the associated \(z\). This will make our approach compatible with the standard Assouad's method for deriving lower bounds (_cf_. [33]).

Then, we let \(Z=(Z_{1},\ldots,Z_{k})\) be a random variable over \(\mathcal{Z}\). Under some assumptions on the distribution of \(Z\), we will bound the information between the individual \(Z_{i}\)s and the transcript \((Y^{n},U)\) induced by a family of channels \(\mathcal{W}\). Combining the two steps above provides us with the desired lower bounds. Formally, let \(\mathcal{Z}:=\{-1,+1\}^{k}\) for some \(k\) and \(\{\mathbf{p}_{z}\}_{z\in\mathcal{Z}}\) (where \(\mathbf{p}_{z}=\mathbf{p}_{\theta_{z}}\)) be a collection of distributions over \(\mathcal{X}\), indexed by \(z\in\mathcal{Z}\). For \(z\in\mathcal{Z}\), denote by \(z^{\otimes i}\in\mathcal{Z}\) the vector obtained by flipping the sign of the \(i\)th coordinate of \(z\). To bound the information that can be obtained about the underlying \(z\) from the observations, we make the following assumptions:

**Assumption 1** (Densities Exist).: _For every \(z\in\mathcal{Z}\) and \(i\in[k]\) it holds that \(\mathbf{p}_{z^{\otimes i}}\ll\mathbf{p}_{z}\), and there exist measurable functions \(\phi_{z,i}\colon\mathcal{X}\to\mathbb{R}\) such that \(\frac{\mathrm{d}\mathbf{p}_{z^{\otimes i}}}{\mathrm{d}\mathbf{p}_{z}}=1+\phi _{z,i}\)._

The functions \(\phi_{z,i}\) capture the change in density when the coordinate \(i\) is flipped. In our applications below, we will have discrete distributions or continuous densities, and the Radon-Nikodym derivatives above can be replaced with the corresponding ratios between the pmfs and pdfs, respectively.

**Assumption 2** (Orthogonality).: _There exists some \(\alpha^{2}\geq 0\) such that, for all \(z\in\mathcal{Z}\) and distinct \(i,j\in[k]\), \(\mathbb{E}_{\mathbf{p}_{z}}[\phi_{z,i}\phi_{z,j}]=0\) and \(\mathbb{E}_{\mathbf{p}_{z}}[\phi_{z,i}^{2}]\leq\alpha^{2}\)._

Note that from Assumption 1 we have that \(\mathbb{E}_{\mathbf{p}_{z}}[\phi_{z,i}]=0\) for each \(i\). In conjunction with Assumption 2 this implies that for any fixed \(z\in\mathcal{Z}\), the family \((1,\phi_{z,1},\ldots,\phi_{z,k})\) is orthogonal and uniformly bounded in \(L^{2}(\mathcal{X},\mathbf{p}_{z})\). Taken together, Assumption 1 and Assumption 2 roughly say that the densities can be decomposed into uncorrelated "perturbations" across coordinates of \(\mathcal{Z}\). In later sections, we will show that for several families, such as discrete distributions, product Bernoulli distributions, and spherical Gaussians, well-known constructions for lower bounds satisfy these assumptions.

Our first bound given in (3) only requires Assumption 1; by imposing the additional structure of Assumption 2, we obtain the more specialized bound given in (4). Interestingly, (3) can be strengthened further when the following subgaussianity assumption holds.

**Assumption 3** (Subgaussianity).: _There exists some \(\sigma\geq 0\) such that, for all \(z\in\mathcal{Z}\), the random vector \(\phi_{z}(X):=(\phi_{z,i}(X))_{i\in[k]}\in\mathbb{R}^{k}\) is \(\sigma^{2}\)-subgaussian for \(X\sim\mathbf{p}_{z}\).4_

Footnote 4: Recall that a r.v. \(Y\) is \(\sigma^{2}\)-subgaussian if \(\mathbb{E}[Y]=0\) and \(\mathbb{E}[e^{\lambda Y}]\leq e^{\sigma^{2}\lambda^{2}/2}\) for all \(\lambda\in\mathbb{R}\); and that a vector-valued r.v. \(Y\) is \(\sigma^{2}\)-subgaussian if its projection \(\langle Y,v\rangle\) is \(\sigma^{2}\)-subgaussian for every unit vector \(v\).

Let \(Z=(Z_{1},\ldots,Z_{k})\) be a random variable over \(\mathcal{Z}\) such that \(\Pr[\,Z_{i}=1\,]=\tau\) for all \(i\in[k]\) and the \(Z_{i}\)s are all independent; we denote this distribution by \(\operatorname{Rad}(\tau)^{\otimes k}\). Our main result is an upper bound on the average amount of information that can be obtained about a coordinate of \(Z\) from the transcript \((Y^{n},U)\) of a sequentially interactive protocol, as a function of the information constraint channels and \(\phi_{Z,i}\)s. This result only requires Assumption 1, and is presented below in its most general form, suited to applications beyond those discussed in the current paper.

**Theorem 1** (Information contraction bound: Technical form).: _Fix \(\tau\in(0,1/2]\). Let \(\Pi\) be a sequentially interactive protocol using \(\mathcal{W}^{n}\), and let \(Z\) be a random variable on \(\mathcal{Z}\) with distribution \(\operatorname{Rad}(\tau)^{\otimes k}\). Let \((Y^{n},U)\) be the transcript of \(\Pi\) when the input \(X_{1},\ldots,X_{n}\) is i.i.d. with common distribution \(\mathbf{p}_{Z}\), with density function \(\mathbf{p}_{Z}^{Y^{n}}\). Then, under Assumption 1,_

\[\left(\frac{1}{k}\sum_{i=1}^{k}\operatorname{d}_{\mathrm{TV}}\!\left(\mathbf{ p}_{+i}^{Y^{n}},\mathbf{p}_{-i}^{Y^{n}}\right)\right)^{2}\leq\frac{7}{k}\sum_{i= 1}^{n}\max_{z\in\mathcal{Z}}\max_{W\in\mathcal{W}_{i}}\sum_{i=1}^{k}\int_{ \mathcal{Y}}\frac{\mathbb{E}_{\mathbf{p}_{z}}[\phi_{z,i}(X)W(y\mid X)]^{2}}{ \mathbb{E}_{\mathbf{p}_{z}}[W(y\mid X)]}\operatorname{d}\!\mu\,\] (3)

_where \(\mathbf{p}_{+i}^{Y^{n}}:=\mathbb{E}\!\left[\mathbf{p}_{Z}^{Y^{n}}\mid Z_{i}=+ 1\right]\), \(\mathbf{p}_{-i}^{Y^{n}}:=\mathbb{E}\!\left[\mathbf{p}_{Z}^{Y^{n}}\mid Z_{i}=- 1\right]\)._

We now instantiate this result, invoking Assumptions 2 and 3, to give simple "plug-and-play" bounds which can be applied readily to several inference problems and information constraints.

**Theorem 2**.: _Fix \(\tau\in(0,1/2]\). Let \(\Pi\) be a sequentially interactive protocol using \(\mathcal{W}^{n}\), and let \(Z\) be a random variable on \(\mathcal{Z}\) with distribution \(\operatorname{Rad}(\tau)^{\otimes k}\). Let \((Y^{n},U)\) be the transcript of \(\Pi\) when the input \(X_{1},\ldots,X_{n}\) is i.i.d. with common distribution \(\mathbf{p}_{Z}\). Then, under Assumptions 1 and 2, we have_

\[\left(\frac{1}{k}\sum_{i=1}^{k}\operatorname{d}_{\mathrm{TV}}\!\left(\mathbf{ p}_{+i}^{Y^{n}},\mathbf{p}_{-i}^{Y^{n}}\right)\right)^{2}\leq\frac{7}{k}\alpha^{2} \sum_{t=1}^{n}\max_{z\in\mathcal{Z}}\max_{W\in\mathcal{W}_{t}}\int_{\mathcal{Y }}\frac{\operatorname{Var}_{\mathbf{p}_{z}}[W(y\mid X)]}{\mathbb{E}_{\mathbf{p }_{z}}[W(y\mid X)]}\operatorname{d}\!\mu\.\] (4)

_Moreover, if Assumption 3 holds as well, we have_

\[\left(\frac{1}{k}\sum_{i=1}^{k}\operatorname{d}_{\mathrm{TV}}\!\left(\mathbf{ p}_{+i}^{Y^{n}},\mathbf{p}_{-i}^{Y^{n}}\right)\right)^{2}\leq\frac{14}{k}\sigma^{2} \sum_{t=1}^{n}\max_{z\in\mathcal{Z}}\max_{W\in\mathcal{W}_{t}}I(\mathbf{p}_{z };W),\] (5)

_where \(I(\mathbf{p}_{z};W)\) denotes the mutual information \(I(X;Y)\) between the input \(X\sim\mathbf{p}_{z}\) and the output \(Y\) of the channel \(W\) with \(X\) as input._

As an illustrative and important corollary, we now derive the implications of this theorem for communication and privacy constraints. For both constraints our tight (or nearly tight) bounds in Section 5 follow directly from these corollaries.

**Corollary 1** (Local privacy constraints).: _For \(\mathcal{W}=\mathcal{W}^{\mathrm{priv},\varepsilon}\) and any family of distributions \(\{\mathbf{p}_{z},z\in\{-1,+1\}^{k}\}\) satisfying Assumptions 1 and 2, with the notation of Theorem 2, we have_

\[\left(\frac{1}{k}\sum_{i=1}^{k}\operatorname{d}_{\mathrm{TV}}\!\left(\mathbf{ p}_{+i}^{Y^{n}},\mathbf{p}_{-i}^{Y^{n}}\right)\right)^{2}\leq\frac{7}{k}n\alpha^{2} \big{(}(e^{\varepsilon}-1)^{2}\wedge e^{\varepsilon}\big{)}.\] (6)

_Moreover, if Assumption 3 holds as well, we have_

\[\left(\frac{1}{k}\sum_{i=1}^{k}\operatorname{d}_{\mathrm{TV}}\!\left(\mathbf{ p}_{+i}^{Y^{n}},\mathbf{p}_{-i}^{Y^{n}}\right)\right)^{2}\leq\frac{14}{k}n\sigma^{2}\varepsilon.\] (7)

**Corollary 2** (Communication constraints).: _For any family of channels \(\mathcal{W}\) with finite output space \(\mathcal{Y}\) and any family of distributions \(\{\mathbf{p}_{z},z\in\{-1,+1\}^{k}\}\) satisfying Assumptions 1 and 2, with the notation of Theorem 2, we have_

\[\left(\frac{1}{k}\sum_{i=1}^{k}\operatorname{d}_{\mathrm{TV}}\!\left(\mathbf{ p}_{+i}^{Y^{n}},\mathbf{p}_{-i}^{Y^{n}}\right)\right)^{2}\leq\frac{7}{k}n\alpha^{2}| \mathcal{Y}|.\] (8)_Moreover, if Assumption 3 holds as well, we have_

\[\left(\frac{1}{k}\sum_{i=1}^{k}\mathrm{d}_{\mathrm{TV}}\Big{(}\mathbf{p}_{+i}^{Y^ {n}},\mathbf{p}_{-i}^{Y^{n}}\Big{)}\right)^{2}\leq\frac{14}{k}n\sigma^{2}\log| \mathcal{Y}|.\] (9)

## 4 An Assouad-type bound

In the previous section we provided an upper bound on \(\frac{1}{k}\sum_{i=1}^{k}\mathrm{d}_{\mathrm{TV}}\big{(}\mathbf{p}_{+i}^{Y^{n} },\mathbf{p}_{-i}^{Y^{n}}\big{)}\). We now prove a lower bound for this quantity in terms of the parameter estimation task we set out to solve. This is an "Assouad's lemma-type" bound, which when combined with Theorem 2 will establish the bounds for \(n\); and, reorganizing, the minimax rate lower bounds. To state the result, we require the following assumption, which relates the \(\ell_{p}\) distance between parameters \(\theta_{z}\)s to the distance between \(z\)s.

**Assumption 4** (Additive loss).: _Fix \(p\in[1,\infty)\). For every \(z,z^{\prime}\in\mathcal{Z}\subset\{-1,+1\}^{k}\),_

\[\ell_{p}(\theta_{z},\theta_{z^{\prime}})=4\gamma\bigg{(}\frac{\mathrm{d}_{ \mathrm{Ham}}(z,z^{\prime})}{\tau k}\bigg{)}^{1/p},\]

_where \(\mathrm{d}_{\mathrm{Ham}}(z,z^{\prime}):=\sum_{i=1}^{k}\mathds{1}\{z_{i}\neq z _{i}^{\prime}\}\) denotes the Hamming distance._

**Lemma 1** (Assouad-type bound).: _Let \(p\geq 1\) and assume that \(\{\mathbf{p}_{z},z\in\mathcal{Z}\}\), \(\tau\in[0,1/2]\) satisfy Assumption 4. Let \(Z\) be a random variable on \(\mathcal{Z}=\{-1,+1\}^{k}\) with distribution \(\mathrm{Rad}(\tau)^{\otimes k}\). Suppose that \((\Pi,\hat{\theta})\) constitutes an \((n,\gamma)\)-estimator of \(\mathcal{P}_{\Theta}\) using \(\mathcal{V}^{n}\) under \(\ell_{p}\) loss (see Definition 2) and \(\Pr_{Z}[\mathbf{p}_{Z}\in\mathcal{P}_{\Theta}\,]\geq 1-\tau/4\). Then the transcript \((Y^{n},U)\) of \(\Pi\) satisfies_

\[\frac{1}{k}\sum_{i=1}^{k}\mathrm{d}_{\mathrm{TV}}\Big{(}\mathbf{p}_{+i}^{Y^{n }},\mathbf{p}_{-i}^{Y^{n}}\Big{)}\geq\frac{1}{4},\]

_where \(\mathbf{p}_{+i}^{Y^{n}}:=\mathbb{E}\big{[}\mathbf{p}_{Z}^{Y^{n}}\bigm{|}Z_{i} =+1\big{]}\), \(\mathbf{p}_{-i}^{Y^{n}}:=\mathbb{E}\big{[}\mathbf{p}_{Z}^{Y^{n}}\bigm{|}Z_{i} =-1\big{]}\)._

## 5 Applications

We now consider three distribution families: product Bernoulli and Gaussian distributions with identity covariance matrix (and \(s\)-sparse mean vectors), and discrete distributions (multinomials), to illustrate the generality and efficacy of our bounds. We describe these three families below, before addressing each of them in their respective subsection. Due to space constraints, we present the proof of the upper bounds in Appendix C, and proof of the lower bounds in Appendix G.

**Sparse Product Bernoulli (\(\mathcal{B}_{d,s}\)).**: Let \(1\leq s\leq d\), \(\Theta=\big{\{}\,\theta\in[-1,1]^{d}:\,\left\|\theta\right\|_{0}\leq s\,\big{\}}\), and \(\mathcal{X}=\{-1,1\}^{d}\). Let \(\mathcal{P}_{\Theta}:=\mathcal{B}_{d,s}\) be the family of \(d\)-dimensional \(s\)-sparse product Bernoulli distributions over \(\mathcal{X}\). Namely, for \(\theta=(\theta_{1},\ldots,\theta_{d})\in\Theta\), the distribution \(\mathbf{p}_{\theta}\) is equal to \(\otimes_{j=1}^{d}\,\mathrm{Rad}(\frac{1}{2}(\theta_{j}+1))\): a distribution on \(\{-1,+1\}^{d}\) such that the marginal distributions are independent, and for which the mean of the \(j\)th marginal is \(\theta_{j}\).
**Sparse Gaussian (\(\mathcal{G}_{d,s}\)).**: Let \(1\leq s\leq d\), \(\Theta=\big{\{}\,\theta\in[-1,1]^{d}:\,\left\|\theta\right\|_{0}\leq s\,\big{\}}\), and \(\mathcal{X}=\mathbb{R}^{d}\). Let \(\mathcal{P}_{\Theta}:=\mathcal{G}_{d,s}\) be the family of \(d\)-dimensional spherical Gaussian distributions with bounded \(s\)-sparse mean. That is, for \(\theta\in\Theta\), \(\mathbf{p}_{\theta}=\mathcal{G}(\theta,\mathbb{I})\) with mean \(\theta\) and covariance matrix \(\mathbb{I}\). We note that this general formulation assumes \(\left\|\theta\right\|_{\infty}\leq 1\) (from the choice of \(\Theta\)).5

Footnote 5: This assumption that the mean is bounded, in our case in \(\ell_{\infty}(\mathbf{0},1)\), is standard, and necessary in order to obtain finite upper bounds: indeed, a packing argument shows that if the mean is assumed to be in a ball of radius \(R\), then a \(\log^{\Omega(1)}R\) dependence in the sample complexity is necessary in both the communication-constrained and LDP settings. Our choice of radius \(1\) is arbitrary, and our upper bounds can be generalized to any \(R\geq 1\).
**Discrete distributions (\(\Delta_{d}\)).**: Let \(\Theta\ =\ \Big{\{}\,\theta\in[0,1]^{d}:\,\sum_{i=1}^{d}\theta_{i}=1\,\Big{\}}\ \subseteq\ \mathbb{R}^{d}\) and \(\mathcal{X}\ =\{1,2,\ldots,d\}\). Let \(\mathcal{P}_{\Theta}:=\Delta_{d}\), where \(\Delta_{d}\) is the standard \((d-1)\)-simplex of all probability mass functions over \(\mathcal{X}\). Namely, the distribution \(\mathbf{p}_{\theta}\) is a distribution on \(\mathcal{X}\), where, for \(j\in[d]\), the probability assigned to the element \(j\) is \(\mathbf{p}_{\theta}(j)=\theta_{j}\). For a unified presentation, we view \(\theta\) as the mean vector of the "categorical distribution," namely the distribution of vector \((\mathds{1}\{X=x\},x\in\mathcal{X})\) for \(X\) with distribution \(\mathbf{p}_{\theta}\).

We now define our measure of interest, the minimax error rate of mean estimation.

_Definition 3_ (Minimax rate of mean estimation).: Let \(\mathcal{P}\) be a family of distributions parameterized by \(\Theta\subseteq\mathbb{R}^{d}\). For \(p\in[1,\infty]\), \(n\in\mathbb{N}\), and a family of channels \(\mathcal{W}\), the minimax error rate of mean estimation for \(\mathcal{P}\) using \(\mathcal{W}^{n}\) under \(\ell_{p}\) loss, denoted \(\mathcal{E}_{p}(\mathcal{P},\mathcal{W},n)\), is the least \(\gamma\in(0,1]\) such that there exists an \((n,\gamma)\)-estimator for \(\mathcal{P}\) using \(\mathcal{W}\) under \(\ell_{p}\) loss (see Definition 2).

We obtain lower bounds on the minimax rate of mean estimation for the different families above by specializing our general bound. Importantly, our methodology is not specific to \(\ell_{p}\) losses, and can be used for arbitrary additive losses such as (squared) Hellinger or, indeed, for any loss function for which an analogue of Lemma 1 can be derived.

Product Bernoulli family.We first establish the following bounds for \(\mathcal{B}_{d,s}\) under privacy and communication constraints.

**Theorem 3** (Product Bernoulli).: _Fix \(p\in[1,\infty)\). For \(4\log d\leq s\leq d\), \(\varepsilon\in(0,\infty)\), and \(\ell\geq 1\),_

\[\sqrt{\frac{ds^{2/p}}{n(\varepsilon^{2}\wedge\varepsilon)}}\wedge 1\lesssim \mathcal{E}_{p}(\mathcal{B}_{d,s},\mathcal{W}^{\mathrm{priv},\varepsilon},n) \lesssim\sqrt{\frac{ds^{2/p}}{n(\varepsilon^{2}\wedge 1)}}\] (10)

_and_

\[\sqrt{\frac{ds^{2/p}}{n\ell}\vee\frac{s^{2/p}\log\frac{2d}{s}}{n}}\wedge 1 \lesssim\mathcal{E}_{p}(\mathcal{B}_{d,s},\mathcal{W}^{\mathrm{comm},\ell},n) \lesssim\sqrt{\frac{ds^{2/p}}{n\ell}\vee\frac{s^{2/p}\log\frac{2d}{s}}{n}}\] (11)

_For \(p=\infty\), we have the upper bounds_

\[\mathcal{E}_{\infty}(\mathcal{B}_{d,s},\mathcal{W}^{\mathrm{priv},\varepsilon },n)=O\Bigg{(}\sqrt{\frac{d\log s}{n\varepsilon^{2}}}\Bigg{)}\quad\text{and} \quad\mathcal{E}_{\infty}(\mathcal{B}_{d,s},\mathcal{W}^{\mathrm{comm},\ell}, n)=O\Bigg{(}\sqrt{\frac{d\log s}{n\ell}\vee\frac{\log d}{n}}\Bigg{)},\]

_while the lower bounds given in Eqs. (10) and (11) hold for \(p=\infty\), too.6_

Footnote 6: That is, the upper and lower bounds only differ by a \(\log s\) factor for \(p=\infty\).

_Remark 1_.: Previous work had shown, in the simpler _noninteractive_ model, a rate lower bound scaling as \(\sqrt{ds/(n\ell)\log(2d/s)}\) for the specific case of \(\ell_{2}\) loss (see, for instance, [21, Theorem 7] for the sparse Gaussian case, which implies the Bernoulli one). An analogous phenomenon was observed for local privacy (_e.g._, [14]). Thus, by removing this logarithmic factor from the upper bound, our result establishes the first (to the best of our knowledge) separation between interactive and noninteractive protocols for sparse mean estimation under communication or local privacy constraints.

_Remark 2_.: Although we stated for simplicity the lower bounds of Theorem 3 in the case where all \(n\) players have a similar local constraints (_i.e._, same privacy parameter \(\varepsilon\), or same bandwidth constraint \(\ell\)), it is immediate to check from the application of Theorem 2 that the result extends to different constraints for each player; replacing \(n(\varepsilon^{2}\wedge\varepsilon)\) and \(n\ell\) in the statement by \(\sum_{t=1}^{n}\varepsilon_{t}^{2}\wedge\varepsilon_{t}\) and \(\sum_{t=1}^{n}\ell_{t}\), respectively. A similar remark applies to the Gaussian and discrete families.

Gaussian family.We derive a lower bound for \(\mathcal{E}_{p}(\mathcal{G}_{d,s},\mathcal{W},n)\) under local privacy (captured by \(\mathcal{W}=\mathcal{W}^{\mathrm{priv},\varepsilon}\)) and communication (captured by \(\mathcal{W}=\mathcal{W}^{\mathrm{comm},\ell}\)) constraints.7 Recall that for product Bernoulli mean estimation we had optimal bounds for both privacy and communication constraints for all finite \(p\). For Gaussians, we will obtain tight bounds for privacy constraints for \(\varepsilon\in(0,1]\). However, for communication constraints and privacy constraints when \(\varepsilon\geq 1\), our bounds for Gaussian distributions lose a (single) logarithmic factor in some parameter regimes.

Footnote 7: As in the Bernoulli case, we here focus for simplicity on the case where the communication (resp., privacy) parameters are the same for all players, but our lower bounds easily extend.

**Theorem 4** (Gaussian distributions).: _Fix \(p\in[1,\infty)\). For \(4\log d\leq s\leq d\), under LDP constraints, when \(\varepsilon\in(0,1]\),_

\[\sqrt{\frac{ds^{2/p}}{n\varepsilon^{2}}}\wedge 1\lesssim\mathcal{E}_{p}( \mathcal{G}_{d,s},\mathcal{W}^{\mathrm{priv},\varepsilon},n)\lesssim\sqrt{ \frac{ds^{2/p}}{n\varepsilon^{2}}}\] (12)

_and when \(\varepsilon>1\),_

\[\sqrt{\frac{ds^{2/p}}{n\varepsilon\log\left(nd\right)}}\wedge 1\lesssim \mathcal{E}_{p}(\mathcal{G}_{d,s},\mathcal{W}^{\mathrm{priv},\varepsilon},n) \lesssim\sqrt{\frac{ds^{2/p}}{n}}\] (13)_Under communication constraints,_

\[\sqrt{\frac{ds^{2/p}}{n\ell\log(dn)}\lor\frac{s^{2/p}\log\frac{2d}{s}}{ n}}\wedge 1\lesssim\mathcal{E}_{p}(\mathcal{G}_{d,s},\mathcal{W}^{\mathrm{comm},\ell},n) \lesssim\sqrt{\frac{ds^{2/p}}{n\ell}\lor\frac{s^{2/p}\log\frac{2d}{s}}{n}}\] (14)

_The same bound as in Theorem 3 hold for \(p=\infty\)._

We emphasize that, as discussed in Sections 1.1 and 1.2, to the best of our knowledge our results provides the first lower bounds for interactive Gaussian mean estimation under these constraints.

**Discrete distribution estimation.** We are able to derive a lower bound for \(\mathcal{E}_{p}(\Delta_{d},\mathcal{W},n)\), the minimax rate for discrete density estimation, under local privacy and communication constraints. In the interest of space, we focus here on two important corollaries; first, for the case of total variation distance (\(\ell_{1}\)), where combining it with known upper bounds we obtain optimal bounds for all \(\varepsilon>0\). In particular, for \(\varepsilon\in(0,1]\) (high-privacy regime) we retrieve the lower bound established in [5], which matches the upper bound from [4]. For \(\varepsilon>1\) (low-privacy regime), our bound matches the upper bound for the noninteractive case, established in [32, 1], showing that even in this low-privacy regime interactivity cannot lead to better rates, except maybe up to constant factors.

**Corollary 3** (Total variation distance).: _For \(\varepsilon>0\), we have_

\[\mathcal{E}_{1}(\Delta_{d},\mathcal{W}^{\mathrm{priv},\varepsilon},n)\asymp\sqrt{\frac{d^{2}}{n((e^{\varepsilon}-1)^{2}\wedge e^{\varepsilon}) }}\wedge 1\,.\] (15)

_For \(\ell\geq 1\),_

\[\mathcal{E}_{1}(\Delta_{d},\mathcal{W}^{\mathrm{comm},\ell},n) \asymp\sqrt{\frac{d}{n(2^{\ell}\wedge d)}}\wedge 1\,.\] (16)

For the case of \(\ell_{2}\) estimation, we also obtain order-optimal bounds:

**Corollary 4** (\(\ell_{2}\) density estimation).: _For \(\varepsilon>0\), we have_

\[\mathcal{E}_{2}(\Delta_{d},\mathcal{W}^{\mathrm{priv},\varepsilon},n)\asymp\sqrt{\frac{d}{n(e^{\varepsilon}-1)^{2}\wedge e^{\varepsilon})}} \wedge\sqrt[4]{\frac{1}{n(e^{\varepsilon}-1)^{2}\wedge e^{\varepsilon})}} \wedge 1\,.\] (17)

_For \(\ell\geq 1\),_

\[\mathcal{E}_{2}(\Delta_{d},\mathcal{W}^{\mathrm{comm},\ell},n) \asymp\sqrt{\frac{d}{n(2^{\ell}\wedge d)}}\wedge\sqrt[4]{\frac{1}{n(2^{\ell} \wedge d)}}\wedge 1\,.\] (18)

## 6 Acknowledgment

The authors would like to thank Yanjun Han, and the anonymous reviewers for helpful comments on an earlier version of this paper.

## References

* Acharya et al. [2019] Jayadev Acharya, Ziteng Sun, and Huanyu Zhang. Hadamard response: Estimating distributions privately, efficiently, and with little communication. In Kamalika Chaudhuri and Masashi Sugiyama, editors, _Proceedings of Machine Learning Research_, volume 89 of _Proceedings of Machine Learning Research_, pages 1120-1129. PMLR, 16-18 Apr 2019. URL http://proceedings.mlr.press/v89/acharya19a.html.
* Acharya et al. [2020] Jayadev Acharya, Clement L. Canonne, and Himanshu Tyagi. Distributed signal detection under communication constraints. volume 125 of _Proceedings of Machine Learning Research_, pages 41-63. PMLR, July 2020. URL http://proceedings.mlr.press/v125/acharya20b.html.
* Acharya et al. [2020] Jayadev Acharya, Clement L. Canonne, and Himanshu Tyagi. Inference under information constraints I: Lower bounds from chi-square contraction. _IEEE Trans. Inform. Theory_, 66(12):7835-7855, 2020. ISSN 0018-9448. doi: 10.1109/TIT.2020.3028440. URL https://doi.org/10.1109/TIT.2020.3028440. Preprint available at arXiv:abs/1812.11476.

* Acharya et al. [2021] Jayadev Acharya, Clement L. Canonne, Cody Freitag, Ziteng Sun, and Himanshu Tyagi. Inference under information constraints III: local privacy constraints. _IEEE J. Sel. Areas Inf. Theory_, 2(1):253-267, 2021.
* Acharya et al. [2022] Jayadev Acharya, Clement L. Canonne, Yuhan Liu, Ziteng Sun, and Himanshu Tyagi. Interactive inference under information constraints. _IEEE Trans. Inform. Theory_, 68(1):502-516, 2022. ISSN 0018-9448. doi: 10.1109/tit.2021.3123905. URL https://doi.org/10.1109/tit.2021.3123905.
* Bar-Yossef et al. [2004] Ziv Bar-Yossef, T. S. Jayram, Ravi Kumar, and D. Sivakumar. An information statistics approach to data stream and communication complexity. _J. Comput. System Sci._, 68(4):702-732, 2004. ISSN 0022-0000. doi: 10.1016/j.jcss.2003.11.006. URL https://doi.org/10.1016/j.jcss.2003.11.006.
* Barnes et al. [2019] Leighton P. Barnes, Yanjun Han, and Ayfer Ozgur. Fisher information for distributed estimation under a blackboard communication protocol. In _ISIT_, pages 2704-2708. IEEE, 2019.
* Barnes et al. [2020] Leighton Pate Barnes, Wei-Ning Chen, and Ayfer Ozgur. Fisher information under local differential privacy. _IEEE J. Sel. Areas Inf. Theory_, 1(3):645-659, 2020.
* Boucheron et al. [2013] Stephane Boucheron, Gabor Lugosi, and Pascal Massart. _Concentration Inequalities: A Nonasymptotic Theory of Independence_. OUP Oxford, 2013.
* Braverman et al. [2016] Mark Braverman, Ankit Garg, Tengyu Ma, Huy L. Nguyen, and David P. Woodruff. Communication lower bounds for statistical estimation problems via a distributed data processing inequality. In _Symposium on Theory of Computing Conference, STOC'16_, pages 1011-1020. ACM, 2016.
* Canonne et al. [2020] Clement L. Canonne, Gautam Kamath, Audra McMillan, Jonathan Ullman, and Lydia Zakynthinou. Private identity testing for high-dimensional distributions. In _Advances in Neural Information Processing Systems 33_, 2020. To appear. Preprint available at arXiv:abs/1905.11947.
* Cover and Thomas [2006] Thomas M. Cover and Joy A. Thomas. _Elements of information theory_. Wiley-Interscience [John Wiley & Sons], Hoboken, NJ, second edition, 2006. ISBN 978-0-471-24195-9; 0-471-24195-4.
* Duchi and Rogers [2019] John Duchi and Ryan Rogers. Lower bounds for locally private estimation via communication complexity. In Alina Beygelzimer and Daniel Hsu, editors, _Proceedings of the Thirty-Second Conference on Learning Theory_, volume 99 of _Proceedings of Machine Learning Research_, pages 1161-1191, Phoenix, USA, June 2019. PMLR.
* Duchi et al. [2018] John C. Duchi, Michael I. Jordan, and Martin J. Wainwright. Minimax optimal procedures for locally private estimation. _J. Amer. Statist. Assoc._, 113(521):182-201, 2018. ISSN 0162-1459.
* Dwork et al. [2006] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In _Theory of cryptography_, volume 3876 of _Lecture Notes in Comput. Sci._, pages 265-284. Springer, Berlin, 2006.
* Evfimievski et al. [2003] Alexandre V. Evfimievski, Johannes Gehrke, and Ramakrishnan Srikant. Limiting privacy breaches in privacy preserving data mining. In _PODS_, pages 211-222. ACM, 2003.
* Garg et al. [2014] Ankit Garg, Tengyu Ma, and Huy L. Nguyen. On communication cost of distributed statistical estimation and dimensionality. In _Advances in Neural Information Processing Systems 27_, pages 2726-2734, 2014.
* Han et al. [2018] Yanjun Han, Pritam Mukherjee, Ayfer Ozgur, and Tsachy Weissman. Distributed statistical estimation of high-dimensional and non-parametric distributions. In _Proceedings of the 2018 IEEE International Symposium on Information Theory (ISIT'18)_, pages 506-510, 2018.
* Han et al. [2018] Yanjun Han, Ayfer Ozgur, and Tsachy Weissman. Geometric lower bounds for distributed parameter estimation under communication constraints. In _Proceedings of the 31st Conference on Learning Theory, COLT 2018_, volume 75 of _Proceedings of Machine Learning Research_, pages 3163-3188. PMLR, 2018.

* Han et al. [2018] Yanjun Han, Ayfer Ozgur, and Tsachy Weissman. Geometric lower bounds for Distributed Parameter Estimation under Communication Constraints. _ArXiv e-prints_, abs/1802.08417v1, February 2018. First version (https://arxiv.org/abs/1802.08417v1).
* Han et al. [2021] Yanjun Han, Ayfer Ozgur, and Tsachy Weissman. Geometric lower bounds for distributed parameter estimation under communication constraints. _IEEE Transactions on Information Theory_, 67(12), 2021.
* Jayram [2009] T. S. Jayram. Hellinger strikes back: a note on the multi-party information complexity of AND. In _Approximation, randomization, and combinatorial optimization_, volume 5687 of _Lecture Notes in Comput. Sci._, pages 562-573. Springer, Berlin, 2009. doi: 10.1007/978-3-642-03685-9_42. URL https://doi.org/10.1007/978-3-642-03685-9_42.
* Joseph et al. [2019] Matthew Joseph, Janardhan Kulkarni, Jieming Mao, and Steven Z. Wu. Locally private gaussian estimation. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems 32_, pages 2984-2993. Curran Associates, Inc., 2019.
* Kasiviswanathan et al. [2011] Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. What can we learn privately? _SIAM J. Comput._, 40(3):793-826, 2011. ISSN 0097-5397.
* Sarbu and Zaidi [2021] Septimia Sarbu and Abdellatif Zaidi. On learning parametric distributions from quantized samples. In _Proceedings of the 2021 IEEE International Symposium on Information Theory (ISIT'21)_, June 2021.
* Shamir [2014] Ohad Shamir. Fundamental limits of online and distributed algorithms for statistical learning and estimation. In _Advances in Neural Information Processing Systems 27_, pages 163-171, 2014.
* van Handel [2016] Ramon van Handel. _Probability in High Dimensions_. December 2016. URL https://web.math.princeton.edu/~rvan/APC550.pdf.
* Wainwright [2009] Martin J. Wainwright. Information-theoretic limits on sparsity recovery in the high-dimensional and noisy setting. _IEEE Trans. Inform. Theory_, 55(12):5728-5741, 2009. ISSN 0018-9448. doi: 10.1109/TIT.2009.2032816. URL https://doi.org/10.1109/TIT.2009.2032816.
* Warner [1965] Stanley L. Warner. Randomized response: A survey technique for eliminating evasive answer bias. _Journal of the American Statistical Association_, 60(309):63-69, 1965. doi: 10.1080/01621459.1965.10480775.
* Wu [2020] Yihong Wu. Lecture notes on: Information-theoretic methods for high-dimensional statistics, 2020. URL http://www.stat.yale.edu/~yw562/teaching/it-stats.pdf.
* Xu and Raginsky [2017] Aolin Xu and Maxim Raginsky. Information-theoretic lower bounds on Bayes risk in decentralized estimation. _IEEE Transactions on Information Theory_, 63(3):1580-1600, 2017.
* Ye and Barg [2018] Min Ye and Alexander Barg. Optimal schemes for discrete distribution estimation under locally differential privacy. _IEEE Trans. Inform. Theory_, 64(8):5662-5676, 2018. ISSN 0018-9448. doi: 10.1109/TIT.2018.2809790. URL https://doi.org/10.1109/TIT.2018.2809790.
* Yu [1997] Bin Yu. Assouad, Fano, and Le Cam. In _Festschrift for Lucien Le Cam_, pages 423-435. Springer, 1997. doi: 10.1007/978-1-4612-1880-7_29. URL http://dx.doi.org/10.1007/978-1-4612-1880-7_29.
* Zhang et al. [2013] Yuchen Zhang, John Duchi, Michael I. Jordan, and Martin J. Wainwright. Information-theoretic lower bounds for distributed statistical estimation with communication constraints. In _Advances in Neural Information Processing Systems 26_, pages 2328-2336, 2013.

Fully interactive model

In this appendix, we describe how to extend our results, presented in the sequentially interactive model, to the more general interactive setting. We first formally define this setting and the corresponding notion of protocols. Hereafter, we use \({}^{*}\) for the Kleene star operation, _i.e._, \(V^{*}=\bigcup_{n=0}^{\infty}V^{n}\).

_Definition 4_ (Interactive Protocols).: Let \(X_{1},\ldots,X_{n}\) be i.i.d. samples from \(\mathbf{p}_{\theta}\), \(\theta\in\Theta\), and \(\mathcal{W}^{*}\) be a collection of sequences of pairs of channel families and players; that is, each element of \(\mathcal{W}^{*}\) is a sequence \((\mathcal{W}_{t},j_{t})_{t\in\mathbb{N}}\) where \(j_{t}\in[n]\). An _interactive protocol \(\Pi\) using \(\mathcal{W}^{*}\)_ comprises a random variable \(U\) (independent of the input \(X_{1},\ldots,X_{n}\)) and, for each \(t\in\mathbb{N}\), mappings

\[\sigma_{t} :Y_{1},\ldots,Y_{t-1},U\mapsto N_{t}\in[n]\cup\{\bot\}\] \[g_{t} :Y_{1},\ldots,Y_{t-1},U\mapsto W_{t}\]

with the constraint that \(((W_{1},N_{1}),\ldots,(W_{t},N_{t}))\) must be consistent with some sequence from \(\mathcal{W}^{*}\); that is, there exists \(((\mathcal{W}_{s},j_{s}))_{s\in\mathbb{N}}\in\mathcal{W}^{*}\) such that \(W_{s}\in\mathcal{W}_{s}\) and \(N_{s}=j_{s}\) for all \(1\leq s\leq t\). These two mappings respectively indicate (i) whether the protocol is to stop (symbol \(\bot\)), and, if not, which player is to speak at round \(t\in\mathbb{N}\), and (ii)) which channel this player selects at this round.

In round \(t\), if \(N_{t}=\bot\), the protocol ends. Otherwise, player \(N_{t}\) (as selected by the protocol, based on the previous messages) uses the channel \(W_{t}\) to produce the message (output) \(Y_{t}\) according to the probability measure \(W_{t}(\cdot\mid X_{N_{t}})\). We further require that \(T:=\inf\{\ t\in\mathbb{N}:\ N_{t}=\bot\ \}\) is finite a.s. The messages \(Y^{T}=(Y_{1},\ldots,Y_{T})\) received by the referee and the public randomness \(U\) constitute the _transcript_ of the protocol \(\Pi\).

In other terms, the channel used by the player \(N_{t}\) speaking at time \(t\) is a Markov kernel

\[W_{t}\colon\mathfrak{Y}_{t}\times\mathcal{X}\times\mathcal{Y}^{t-1}\to[0,1]\,,\]

with \(\mathcal{Y}_{t}\subseteq\mathcal{Y}\); and, for player \(j\in[n]\), the allowed subsequences \((\mathcal{W}_{t},j_{t})_{t\in\mathbb{N};j_{t}=j}\) capture the possible sequences of channels allowed to the player. As an example, if we were to require that any single player can speak at most once, then for every \(j\in[n]\) and every \((\mathcal{W}_{t},j_{t})_{t\in\mathbb{N}}\in\mathcal{W}^{n}\), we would have \(\sum_{t=1}^{\infty}\mathds{1}\{j_{t}=j\}\leq 1\).

In the interactive model, we can then capture the constraint that each player must communicate at most \(\ell\) bits in total by letting \(\mathcal{W}^{n}\) be the set of sequences \((\mathcal{W}^{\mathrm{comm},\ell_{t}}_{t},j_{t})_{t\in\mathbb{N}}\) such that

\[\forall j\in[n],\qquad\sum_{t=1}^{\infty}\ell_{t}\cdot\mathds{1}\{j_{t}=j\} \leq\ell\,.\]

In the simpler sequentially interactive model, this condition simply becomes the choice of \(\mathcal{W}^{n}=(\mathcal{W}^{\mathrm{comm},\ell},\ldots,\mathcal{W}^{ \mathrm{comm},\ell})\).

### Lower Bounds under Full Interactive Model

Next we discuss how our technique extends to the full interactive model. For any full interactive protocol \(\Pi\), let \(Y^{*}\in\mathcal{Y}^{*}\) be the message sequence generated by the protocol. Then, for all \(y^{*}\in\mathcal{Y}^{*}\), we have

\[\Pr_{X^{n}\sim\mathbf{p}}\big{[}\,Y^{*}=y^{*}\,\big{]}=\mathbb{E}_{X^{n}\sim \mathbf{p}}\Bigg{[}\prod_{t=1}^{\infty}W_{t}\big{(}y_{t}\mid X_{\sigma_{t}(y^{ t-1})},y^{t-1}\big{)}\Bigg{]}.\]

The following lemma states that if \(X^{n}\) are generated from a product distribution, the distribution of the transcript satisfies a property similar to the "cut-and-paste" property from [6].

**Lemma 2** ([20]).: _If \(X^{n}\sim\mathbf{p}=\otimes_{t=1}^{n}\mathbf{p}_{t}\), the transcript of the protocol satisfies_

\[\Pr_{X^{n}\sim\mathbf{p}}\big{[}\,Y^{*}=y^{*}\,\big{]}=\prod_{t=1}^{n} \mathbb{E}_{X_{t}\sim\mathbf{p}_{t}}[g_{t}(y^{*},X_{t})],\] (19)

_where \(g_{t}(y^{*},x_{t})=\prod_{j=1}^{\infty}W_{j}(y_{j}\mid x_{t},y^{j-1})\mathds{1 }\big{\{}\sigma_{j}(y^{j-1})=t\big{\}}\)._

Hence, when \(X^{n}\sim\mathbf{p}_{z}^{\otimes n}\) we have

\[\mathbf{p}_{z}^{y^{*}}:=\Pr_{X^{n}\sim\mathbf{p}_{z}^{\otimes n}}[\,Y^{*}=y^{ *}\,]=\prod_{t=1}^{n}\mathbb{E}_{X_{t}\sim\mathbf{p}_{z}}[g_{t}(y^{*},X_{t})].\]Here we can define a similar notion of "channel" for a communication protocol \(\Pi\) for the \(i\)th player when the underlying distribution is \(\mathbf{p}_{z}\) by setting

\[\tilde{W}_{t,\mathbf{p}_{z}}(y^{*}\mid x)=g_{t}(y^{*},x)\Bigg{(} \prod_{j\neq t}\mathbb{E}_{X_{j}\sim\mathbf{p}_{z}}[g_{j}(y^{*},X_{j})]\Bigg{)}.\] (20)

Then we have, for all \(t\in[n]\),

\[\mathbb{E}_{X_{t}\sim\mathbf{p}_{z}}\Big{[}\tilde{W}_{t,\mathbf{p}_{z}}(y^{*} \mid X_{t})\Big{]}=\Pr_{X^{*}\sim\mathbf{p}_{z}^{\otimes n}}[Y^{*}=y^{*}].\]

We proceed to prove a bound similar to Theorem 1 in terms of the "channel" defined in Eq. (20), as stated below.

**Theorem 5** (Information contraction bound).: _Fix \(\tau\in(0,1/2]\). Let \(\Pi\) be a fully interactive protocol using \(\mathcal{W}^{n}\), and let \(Z\) be a random variable on \(\mathcal{Z}\) with distribution \(\mathrm{Rad}(\tau)^{\otimes k}\). Let \((Y^{*},U)\) be the transcript of \(\Pi\) when the input \(X_{1},\dots,X_{n}\) is i.i.d. with common distribution \(\mathbf{p}_{Z}\). Then, under Assumption 1,_

\[\Bigg{(}\frac{1}{k}\sum_{i=1}^{k}\mathrm{d}_{\mathrm{TV}}\Big{(} \mathbf{p}_{+i}^{Y^{*}},\mathbf{p}_{-i}^{Y^{*}}\Big{)}\Bigg{)}^{2}\] \[\leq\frac{7}{k}\alpha^{2}\sum_{j=1}^{n}\max_{z\in\mathcal{Z}}\max _{(\mathcal{W}_{i},j_{t})_{t\in\mathbb{N}}\in\mathcal{W}^{n}}\sum_{i=1}^{k} \int_{y^{*}\in\mathcal{Y}^{*}}\frac{\mathbb{E}_{\mathbf{p}_{z}}\Big{[}\phi_{z,i}(X)\tilde{W}_{j,\mathbf{p}_{z}}(y^{*}\mid X)\Big{]}^{2}}{\mathbb{E}_{ \mathbf{p}_{z}}\Big{[}\tilde{W}_{j,\mathbf{p}_{z}}(y^{*}\mid X)\Big{]}}\, \mathrm{d}\mu\,\]

_where \(\mathbf{p}_{+i}^{Y^{*}}:=\mathbb{E}\big{[}\mathbf{p}_{Z}^{Y^{*}}\bigm{|}Z_{i}=1 \big{]}\), \(\mathbf{p}_{-i}^{Y^{*}}:=\mathbb{E}\big{[}\mathbf{p}_{Z}^{Y^{*}}\bigm{|}Z_{i}= 1\big{]}\)._

We can see the bound is in identical form to Theorem 1 except that we replace each player's channel with the \(\tilde{W}_{j,\mathbf{p}_{z}}(y^{*}\mid X)\) we defined. Other similar bounds in Section 3 can also be derived under additional assumptions and specific constraints. We present the proof for Theorem 5 below and omit the detailed statements and proof for other bounds.

Proof.: Analogously to Eq. (36), we can get

\[\frac{1}{k}\Bigg{(}\sum_{i=1}^{k}\mathrm{d}_{\mathrm{TV}}\Big{(} \mathbf{p}_{+i}^{Y^{*}},\mathbf{p}_{-i}^{Y^{*}}\Big{)}\Bigg{)}^{2} \leq 14\sum_{t=1}^{n}\mathbb{E}_{Z}\Bigg{[}\sum_{i=1}^{k}\mathrm{d}_{ \mathrm{H}}\Big{(}\mathbf{p}_{Z}^{Y^{*}},\mathbf{p}_{t\gets Z^{\otimes t}} ^{Y^{*}}\Big{)}^{2}\Bigg{]}\] (21)

For all \(z\in\{-1,+1\}^{k}\) and \(i,t\), by the definition of Hellinger distance and Eq. (19), we have

\[2\mathrm{d}_{\mathrm{H}}\Big{(}\mathbf{p}_{z}^{Y^{*}},\mathbf{p }_{t\gets z^{\otimes t}}^{Y^{*}}\Big{)}^{2} =\int_{y^{*}\in\mathcal{Y}^{*}}\prod_{\begin{subarray}{c}1\leq j \leq n\\ j\neq t\end{subarray}}\mathbb{E}_{X_{j}\sim\mathbf{p}_{z}}[g_{j}(y^{*},X_{j})] \bigg{(}\sqrt{\mathbb{E}_{X_{t}\sim\mathbf{p}_{z}\otimes t}[g_{t}(y^{*},X_{t}) ]}-\sqrt{\mathbb{E}_{X_{t}\sim\mathbf{p}_{z}}[g_{t}(y^{*},X_{t})]}\bigg{)}^{2} \,\mathrm{d}\mu\] \[\leq\int_{y^{*}\in\mathcal{Y}^{*}}\Big{(}\prod_{j\neq t}\mathbb{ E}_{X_{j}\sim\mathbf{p}_{z}}[g_{j}(y^{*},X_{j})]\Big{)}\bigg{(}\frac{(\mathbb{E}_{X_{t} \sim\mathbf{p}_{z}}[g_{t}(y^{*},X_{t})]-\mathbb{E}_{X_{t}\sim\mathbf{p}_{z} \otimes t}[g_{t}(y^{*},X_{t})])^{2}}{\mathbb{E}_{X_{t}\sim\mathbf{p}_{z}}[g_{ t}(y^{*},X_{t})]}\bigg{)}\,\mathrm{d}\mu\,,\]

Proceeding from above, we get under Assumption 1,

\[2\mathrm{d}_{\mathrm{H}}\Big{(}\mathbf{p}_{z}^{Y^{*}},\mathbf{p }_{t\gets z^{\otimes t}}^{Y^{*}}\Big{)}^{2} \leq\alpha^{2}\int_{y^{*}\in\mathcal{Y}^{*}}\left(\prod_{j\neq t} \mathbb{E}_{X_{j}\sim\mathbf{p}_{z}}[g_{j}(y^{*},X_{j})]\right)\bigg{(}\frac{ \mathbb{E}_{X_{t}\sim\mathbf{p}_{z}}[\phi_{z,i}(X_{t})g_{t}(y^{*},X_{t})]^{2}} {\mathbb{E}_{X_{t}\sim\mathbf{p}_{z}}[g_{t}(y^{*},X_{t})]}\bigg{)}\,\mathrm{d}\mu\] \[=\alpha^{2}\int_{y^{*}\in\mathcal{Y}^{*}}\frac{\mathbb{E}_{X_{t} \sim\mathbf{p}_{z}}\Big{[}\phi_{z,i}(X_{t})g_{t}(y^{*},X_{t})\prod_{j\neq t} \mathbb{E}_{X_{j}\sim\mathbf{p}_{z}}[g_{j}(y^{*},X_{j})]\Big{]}^{2}}{\mathbb{E} _{X_{t}\sim\mathbf{p}_{z}}\Big{[}g_{t}(y^{*},X_{t})\prod_{j\neq t}\mathbb{E}_{X_{j} \sim\mathbf{p}_{z}}[g_{j}(y^{*},X_{j})]\Big{]}}\,\mathrm{d}\mu\] \[=\alpha^{2}\int_{y^{*}\in\mathcal{Y}^{*}}\frac{\mathbb{E}_{X_{t} \sim\mathbf{p}_{z}}\Big{[}\phi_{z,i}(X_{t})\tilde{W}_{t,\mathbf{p}_{z}}(y^{*} \mid X)\Big{]}^{2}}{\mathbb{E}_{X_{t}\sim\mathbf{p}_{z}}\big{[}\tilde{W}_{t, \mathbf{p}_{z}}(y^{*}\mid X)\Big{]}}\,\mathrm{d}\mu\,.\]

Plugging the above bound into Eq. (21), we can obtain the bound in Theorem 5 by taking the maximum over all \(z\in\{-1,+1\}^{k}\) and all possible channel sequences.

A measure change bound

We here provide a variant of Talagrand's transportation-cost inequality which is used in deriving Eq. (5) (under Assumption 3) in the second part of Theorem 2. We note that this type of result is not novel, and can be derived from standard arguments in the literature (see, e.g., [9, Chapter 8] or [27, Chapter 4]). However, the lemma below is specifically tailored for our purposes, and we provide the proof for completeness. A similar bound was derived in [2], where Gaussian mean testing under communication constraints was considered.

**Lemma 3** (A measure change bound).: _Consider a random variable \(X\) taking values in \(\mathcal{X}\) and with distribution \(P\). Let \(\Phi\colon\mathcal{X}\to\mathbb{R}^{k}\) be such that the random vector \(\Phi(X)\) is \(\sigma^{2}\)-subgaussian. Then, for any function \(a\colon\mathcal{X}\to[0,\infty)\) such that \(\mathbb{E}[a(X)]<\infty\), we have_

\[\frac{\|\mathbb{E}[\Phi(X)a(X)]\|_{2}^{2}}{\mathbb{E}[a(X)]^{2}}\leq 2\sigma^{ 2}\frac{\mathbb{E}[a(X)\ln a(X)]}{\mathbb{E}[a(X)]}+2\sigma^{2}\ln\frac{1}{ \mathbb{E}[a(X)]}.\]

Proof.: By an application of Gibb's variational principle (_cf._[9, Corollary 4.14]) the following holds: For a random variable \(Z\) and distributions \(P\) and \(Q\) on the underlying probability space satisfying \(Q\ll P\) (that is, such that \(Q\) is absolutely continuous with respect to \(P\)), we have

\[\lambda\mathbb{E}_{Q}[Z]\leq\ln\mathbb{E}_{P}\big{[}e^{\lambda Z}\big{]}+ \mathrm{D}(Q\|P).\]

To apply this bound, set \(P\) to be the distribution of \(X\) and let \(Q\ll P\) be defined using its density (Radon-Nikodym derivative) with respect to \(P\) given by

\[\frac{\mathrm{d}Q}{\mathrm{d}P}=\frac{a(X)}{\mathbb{E}_{P}[a(X)]}.\]

Now, note that for any unit vector \(v\), we have, setting \(Z=v^{\intercal}\Phi(X)\) and using the \(\sigma^{2}\)-subgaussianity of \(\Phi(X)\), that

\[\lambda\mathbb{E}_{Q}[v^{\intercal}\Phi(X)]\leq\ln\mathbb{E}_{P}\Big{[}e^{ \lambda v^{\intercal}\Phi(X)}\Big{]}+\mathrm{D}(Q\|P)\leq\frac{\sigma^{2} \lambda^{2}}{2}+\mathrm{D}(Q\|P).\]

In particular, for \(\lambda=\frac{1}{\sigma}\sqrt{2\mathrm{D}(Q\|P)}\), we get

\[\mathbb{E}_{Q}[v^{\intercal}\Phi(X)]\leq\sigma\sqrt{2\mathrm{D}(Q\|P)}.\]

Applying this to the unit vector \(v:=\frac{\mathbb{E}_{Q}[\Phi(X)]}{\|\mathbb{E}_{Q}[\Phi(X)]\|_{2}}\) then yields

\[\|\mathbb{E}_{Q}[\Phi(X)]\|_{2}\leq\sigma\sqrt{2\mathrm{D}(Q\|P)}.\]

To conclude, it then suffices to observe that

\[\mathrm{D}(Q\|P)=\frac{\mathbb{E}_{P}[a(X)\ln a(X)]}{\mathbb{E}_{P}[a(X)]}+\ln \frac{1}{\mathbb{E}_{P}[a(X)]}.\]

The proof is completed by combining the bounds above, as \(\mathbb{E}_{Q}[\Phi(X)]=\frac{\mathbb{E}_{P}[\Phi(X)a(X)]}{\mathbb{E}_{P}[a(X)]}\). 

## Appendix C Upper bounds

We now describe and analyze the interactive algorithms for the estimation tasks we consider.

### Product Bernoulli Distributions

Recall that \(\mathcal{B}_{d,s}\), the family of \(d\)-dimensional \(s\)-sparse product Bernoulli distributions, is defined as

\[\mathcal{B}_{d,s}:=\left\{\bigotimes_{j=1}^{d}\mathrm{Rad}(\frac{1}{2}(\mu_{j }+1))\,:\ \mu\in[-1,1]^{d},\left\|\mu\right\|_{0}\leq s\right\}\,.\] (22)

We now provide the interactive protocols achieving the upper bounds of Theorem 3 for sparse product Bernoulli mean estimation under LDP and communication constraints.

Our protocols has two ingredients described below:

**Estimating non-zero mean coordinates.**In this step we will start with \(S_{0}=[d]\), the set of all possible coordinates. Then we will iteratively prune the set \(S_{0}\to S_{1}\to\ldots\to S_{T}\), such that \(|S_{T}|=3s\) (this step is skipped if \(s\geq d/3\)) is a good estimate for the set of coordinates with non-zero mean.
* **Estimating the non-zero means.** We then estimate the means of the coordinates in \(S_{T}\), which is equivalent to solving a dense mean estimation problem in \(3s\) dimensions.

In the next two sections, we provide the details of the algorithm that matches the lower bounds obtained in Section 5 for interactive protocols under LDP and communication constraints respectively.

#### c.1.1 LDP constraints

In this subsection, we will focus on the case \(\varepsilon\in(0,1]\) (high-privacy regime). For the case \(\varepsilon>1\), we rely a privatization of the communication-limited algorithm, which will be discussed at the end of Appendix C.1.2. Our protocol for Bernoulli mean estimation under LDP constraints is described in Algorithm 1. As stated above, in each round \(t=1,\ldots,T\), for each \(j\in S_{t-1}\) a new group of players apply the well known binary Randomized Response (RR) mechanism [29, 24] to their \(j\)th coordinate. Using these messages we then guess a set of coordinates with highest possible means (in absolute value) and prune the set to \(S_{t}\). This is done in Lines 2-6 of Algorithm 1.

In Lines 7-12, the algorithm uses the same approach to estimate the means of coordinates within \(S_{T}\) and sets remaining coordinates to zero.

The privacy guarantee follows immediately from that of the RR mechanism, and further, this only requires one bit of communication per player.

```
0:\(n\) players, dimension \(d\), sparsity parameter \(s\), privacy parameter \(\varepsilon\).
1: Set \(T:=\log_{3}\frac{d}{3s}\), \(\alpha:=\frac{e^{\varepsilon}}{1+e^{\varepsilon}}\), \(S_{0}=[d]\), \(N_{0}:=\frac{n}{6d}\).
2:for\(t=1,2,\ldots,T\)do
3:for\(j\in S_{t-1}\)do
4: Get a group of new players \(G_{t,j}\) of size \(N_{t}=N_{0}\cdot 2^{t}\).
5: Player \(i\in G_{t,j}\), upon observing \(X_{i}\in\{-1,+1\}^{d}\) sends the message \(Y_{i}\in\{-1,+1\}\) such that \[Y_{i}=\begin{cases}(X_{i})_{j}&\text{w.p. $\alpha$,}\\ -(X_{i})_{j}&\text{w.p. $1-\alpha$.}\end{cases}\] (23)
6: Set \(M_{t,j}:=\sum_{i\in G_{t,j}}Y_{i}\). Let \(S_{t}\subseteq S_{t-1}\) be the set of the \(|S_{t-1}|/3\) indices with the largest \(|M_{t,j}|\).
7:for\(j\in S_{T}\)do
8: Get a group of new players \(G_{T,j},j\in S_{T}\) of size \(N_{T+1}=N_{0}\cdot 2^{T}\).
9: Player \(i\in G_{T,j}\), sends the message \(Y_{i}\in\{-1,+1\}\) according to Eq. (23) and \(M_{T,j}:=\sum_{i\in G_{T,j}}Y_{i}\)
10:for\(j\in[d]\)do
11: \[\widehat{\mu}_{j}=\begin{cases}\frac{M_{j,T}}{(2\alpha-1)N_{T+1}}&\text{if $j \in S_{T}$,}\\ 0&\text{otherwise.}\end{cases}\]
12:return\(\widehat{\mu}\). ```

**Algorithm 1** LDP protocol for mean estimation for the product of Bernoulli family

The performance guarantee of Algorithm 1 is stated below, which matches the lower bounds obtained in Section 5.

**Proposition 1**.: _Fix \(p\in[1,\infty]\). For \(n\geq 1\) and \(\varepsilon\in(0,1]\), Algorithm 1 is an \((n,\gamma)\)-estimator using \(\mathcal{W}_{\varepsilon}\) under \(\ell_{p}\) loss for \(\mathcal{B}_{d,s}\) with \(\gamma=O\!\left(\sqrt{\frac{p\!ds^{2/p}}{n\varepsilon^{2}}}\right)\) for \(p\leq 2\log s\) and \(\gamma=O\!\left(\sqrt{\frac{d\log s}{n\varepsilon^{2}}}\right)\) for \(p>2\log s\)._Proof.: The total number of players used by Algorithm 1 uses is

\[\sum_{t=1}^{T+1}|S_{t-1}|\cdot N_{t}=|S_{0}|\cdot N_{0}\cdot\sum_{t=1}^{T+1}\frac{ 2^{t}}{3^{t-1}}\leq 6|S_{0}|\cdot N_{0}=n.\]

To prove the utility guarantee, we bound the estimation error in the estimated set \(S_{T}\) and the error outside the set \(S_{T}\) in the following lemma.

**Lemma 4**.: _Let \(S_{T}\) be the subset obtained from the first stage of Algorithm 1. Then,_

\[\max\Bigg{\{}\mathbb{E}\Bigg{[}\sum_{j\notin S_{T}}|\mu_{j}-\widehat{\mu}_{j}| ^{p}\Bigg{]},\mathbb{E}\Bigg{[}\sum_{j\in S_{T}}|\mu_{j}-\widehat{\mu}_{j}|^{p }\Bigg{]}\Bigg{\}}=O\Bigg{(}s\bigg{(}\frac{pd}{n\varepsilon^{2}}\bigg{)}^{p/2} \Bigg{)}.\]

The proposition follows directly from the lemma. Indeed, for \(p>2\log s\), by monotonicity of \(\ell_{p}\) norms we have \(\left\|\mu-\hat{\mu}\right\|_{p}\leq\left\|\mu-\hat{\mu}\right\|_{p^{\prime}}\) for all \(p^{\prime}\leq p\), and thus choosing \(p^{\prime}:=2\log s\) is sufficient to obtain the stated bound. 

Proof of Lemma 4.: We prove the bound on each term individually. The first term captures the performance of our estimator within coordinates in \(S_{T}\) and the second term states that we do not "prune" too many coordinates with high non-zero means.

**Bounding the first term.** For \(j\notin S_{T}\), we output \(\widehat{\mu}_{j}=0\). Therefore,

\[\mathbb{E}\Bigg{[}\sum_{j\notin S_{T}}|\mu_{j}-\widehat{\mu}_{j}|^{p}\Bigg{]} =\sum_{j}\mathbb{E}[|\mu_{j}-\widehat{\mu}_{j}|^{p}\cdot\mathds{1}\{j \notin S_{T}\}]=\sum_{j}|\mu_{j}|^{p}\cdot\Pr[\,j\notin S_{T}\,].\]

Since \(\mu\) is \(s\)-sparse, it will suffice to show that for all \(j\) with \(|\mu_{j}|>0\),

\[|\mu_{j}|^{p}\cdot\Pr[\,j\notin S_{T}\,]=O\Bigg{(}\bigg{(}\frac{pd}{n \varepsilon^{2}}\bigg{)}^{p/2}\Bigg{)}.\] (24)

Let

\[H:=20\sqrt{\frac{d}{n(2\alpha-1)^{2}}}.\]

Note that for \(\varepsilon\in(0,1]\), we have \(2\alpha-1\geq\frac{\varepsilon-1}{e+1}\varepsilon\). Therefore, if \(|\mu_{j}|\leq H\), then Eq. (24) holds since \(\Pr[\,j\notin S\,]\leq 1\). We hereafter assume \(|\mu_{j}|>H\), and let \(\mu_{j}=\beta_{j}H\) with \(\beta_{j}>1\). Let \(E_{t,j}\) be the event that coordinate \(j\) is removed in round \(t\) given that \(j\in S_{t-1}\). Then we have

\[\Pr[\,j\notin S_{T}\,]\leq\sum_{t=1}^{T}\Pr[\,E_{t,j}\,].\]

We proceed to bound each \(\Pr[\,E_{t,j}\,]\) separately. Note that for \(i\in G_{t,j}\), \(Y_{i}\in\{-1,+1\}\) and by Eq. (23)

\[\mathbb{E}[Y_{i}]=(2\alpha-1)\cdot\mu_{j}=(2\alpha-1)\beta_{j}H.\] (25)

Let \(a_{t,j}\) be the number of coordinates \(j^{\prime}\) with \(\mu_{j^{\prime}}=0\) and \(|M_{t,j^{\prime}}|\geq\frac{1}{2}N_{t}(2\alpha-1)\beta_{j}H\). Since we select the \(|S_{t-1}|/3\) coordinates with the largest magnitude of the sum, for \(j\notin S_{t}\) to happen at least one of the following must occur: (i) \(a_{t,j}>\frac{1}{3}|S_{t-1}|-s\), or (ii) \(M_{t,j}<\frac{1}{2}N_{t}(2\alpha-1)\beta_{j}H\).

By Hoeffding's inequality, we have

\[\Pr\bigg{[}\,M_{t,j}<\frac{1}{2}N_{t}(2\alpha-1)\beta_{j}H\,\bigg{]}\leq\exp \biggl{(}-\frac{1}{8}N_{t}((2\alpha-1)\beta_{j}H)^{2}\bigg{)}<\exp\bigl{(}-5 \cdot 2^{t}\beta_{j}^{2}\bigr{)}.\]

Let \(p_{t,j}:=e^{-5\cdot 2^{t}\beta_{j}^{2}}\). Similarly, for any \(j^{\prime}\) such that \(\mu_{j^{\prime}}=0\),

\[\Pr\biggl{[}\,|M_{t,j^{\prime}}|\geq\frac{1}{2}N_{t}(2\alpha-1)\beta_{j}H\, \biggr{]}\leq 2p_{t,j}.\]Since all coordinates are independent, \(a_{t,j}\) is binomially distributed with mean at most \(2p_{t,j}|S_{t-1}|\). By Markov's inequality, we get

\[\Pr\biggl{[}\,a_{t,j}>\frac{1}{3}|S_{t-1}|-s\,\biggr{]}\leq\frac{\mathbb{E}[a_{t,j}]}{|S_{t-1}|/3-s}\leq p_{t,j},\]

recalling that \(|S_{t-1}|=d3^{t-1}\geq 9s\). By a union bound and summing over \(t\in[T]\), we get

\[\Pr\bigl{[}\,j\notin S_{T}\,\bigr{]}\leq\sum_{t=1}^{T}\Pr\bigl{[}\,E_{t,j}\, \bigr{]}\leq\sum_{t=1}^{T}3p_{t,j}=3\sum_{t=1}^{T}\exp\bigl{(}-2^{t}\cdot 5 \beta_{j}^{2}\bigr{)}\leq 6\exp\bigl{(}-5\beta_{j}^{2}\bigr{)}.\]

Not that for \(x>0\), \(x^{p}e^{-x^{2}}\leq\bigl{(}\frac{p}{2e}\bigr{)}^{p/2}\). Hence

\[|\mu_{j}|^{p}\cdot\Pr\bigl{[}\,j\notin S_{T}\,\bigr{]}\leq 6H^{p}\beta_{j}^{p}e^ {-5\beta_{j}^{2}}\leq\biggl{(}C\frac{pd}{n\varepsilon^{2}}\biggr{)}^{p/2},\]

for some absolute constant \(C>0\), completing the proof.

**Bounding the second term.** Note that \(S_{T}\) is a random variable itself. We show that the bound holds for any realization of \(S_{T}\). We need the following result which follows from standard moment bounds on binomial distributions.

**Fact 1**.: _Let \(p\geq 1\), \(m\in\mathbb{N}\), \(0\leq q\leq 1\), and \(N\sim\mathrm{Bin}(m,q)\). Then, \(\mathbb{E}[|N-mq|^{p}]\leq 2^{-p/2}m^{p/2}p^{p/2}\)._

Applying this with \(m=N_{T}\geq\frac{n}{6d}\), the transformation from Bernoulli to \(\{-1,+1\}\), and the scaling by \(2\alpha-1\), yields for \(j\in S_{T}\), and using Eq. (25)

\[\mathbb{E}[|\mu_{j}-\widehat{\mu}_{j}|^{p}]\leq\biggl{(}\frac{p}{(n/6d)(2 \alpha-1)^{2}}\biggr{)}^{p/2}.\]

Upon summing over \(j\in S_{T}\), we obtain

\[\mathbb{E}\left[\sum_{j\in S_{T}}|\mu_{j}-\widehat{\mu}_{j}|^{p}\right]\leq 3 s\cdot\biggl{(}\frac{6(e+1)^{2}d}{(e-1)^{2}n\varepsilon^{2}}\biggr{)}^{p/2} \leq 3\cdot 6^{p}\cdot s\biggl{(}\frac{pd}{n\varepsilon^{2}}\biggr{)}^{p/2}.\qed\]

#### c.1.2 Communication constraints

In Algorithm 2 we propose a protocol to estimate the mean of product Bernoulli distributions under \(\ell\)-bit communication constraints. As mentioned in the previous subsection, the \(\varepsilon\)-LDP algorithm with \(\varepsilon>1\) will follow from a simple modification of the communication-constrained one; we discuss how to privatize the latter to obtain the former at the end of the section. As in the LDP case when \(\varepsilon\in(0,1]\), in 2-10 the algorithm iteratively prunes an initial set \(S_{0}=[d]\) to obtain a set \(S_{T}\) of size \(\max\{3s,\ell\}\), which denotes the set of potential non-zero coordinates. We then estimate the mean of coordinates in \(S_{T}\). If \(\ell>3s\), then we can directly send the values of all coordinates in \(S_{T}\) and use it for estimation; otherwise, when \(3s>\ell\), we again partition \(S_{T}\) into sets of size \(\ell\) and each player sends the bits of its sample in this set. This is done in Lines 11-18. We state the performance of Algorithm 2 below.

**Proposition 2**.: _Fix \(p\in[1,\infty]\). For \(n\geq 1\) and \(\ell\leq d\), we have Algorithm 2 is an \((n,\gamma)\)-estimator using \(\mathcal{W}_{\ell}\) under \(\ell_{p}\) loss for \(\mathcal{B}_{d,s}\) with \(\gamma=\text{O}\biggl{(}\sqrt{\frac{pds^{2/p}}{n\ell}+\frac{(p+\log(2\ell/s)) s^{2/p}}{n}}\biggr{)}\) for \(p\leq 2\log s\) and \(\gamma=\text{O}\biggl{(}\sqrt{\frac{d\log s}{n\ell}+\frac{\log\ell}{n}}\biggr{)}\) for \(p>2\log s\)._

When \(\ell\leq 3s\), the bound we get is \(\gamma\lesssim\sqrt{\frac{pds^{2/p}}{n\ell}}\). The analysis is almost identical to the case under LDP constraints, since in both cases, the information we get about coordinate \(j\) are samples from a Rademacher distribution with mean \((2\alpha-1)\mu_{j}\). There are only two differences. (i) \(\alpha=1\) instead of \(\Theta\big{(}\varepsilon^{2}\big{)}\). (ii) There is a factor of \(\ell\) more players in the corresponding groups. Combing both factors, we can obtain the desired bound by replacing \(\varepsilon^{2}\) by \(\ell\). We omit the detailed proof in this case.

When \(\ell>3s\), after \(T\asymp\log(d/\ell)\) rounds, we can find a subset \(S_{T}\) of size \(\ell\) which contains most of the coordinates with large biases. The protocol then asks new players to send all coordinates within \(S_{T}\) using \(\ell\) bits. In this case, it would be enough to prove Lemma 5 since for the coordinates outside \(S_{T}\), we can show the error is small following exactly the same steps as the proof for bouding the first term in Lemma 4 as we explained in the case when \(\ell\leq 3s\).

**Lemma 5**.: _Let \(S_{T}\) be the subset obtained from the first stage of Algorithm 2, we have_

\[\mathbb{E}\left[\sum_{j\in S_{T}}|\mu_{j}-\widehat{\mu}_{j}|^{p}\right]=O \left(s\Bigg{(}\frac{p+\log\frac{2\ell}{s}}{n}\Bigg{)}^{p/2}\right).\]

Proof.: Similar to Lemma 4, we will prove that the statement is true for any realization of \(S_{T}\), which is a stronger statement than the claim.

\[\mathbb{E}\left[\sum_{j\in S_{T}}|\mu_{j}-\widehat{\mu}_{j}|^{p}\right] =\mathbb{E}\left[\sum_{j\in S_{T}}|\mu_{j}-\widehat{\mu}_{j}|^{p} \mathds{1}\{j\in S_{T+1}\}\right]+\mathbb{E}\left[\sum_{j\in S_{T}}|\mu_{j}|^{ p}\mathds{1}\{j\notin S_{T+1}\}\right]\] \[\leq\mathbb{E}\left[\sum_{j\in S_{T+1}}|\mu_{j}-\widehat{\mu}_{j} |^{p}\right]+\sum_{j\in S_{T}}|\mu_{j}|^{p}\Pr[\,j\notin S_{T+1}\,].\]Fix \(S_{T+1}\). For each \(j\in S_{T+1}\), \(M_{T+1,j}\) is binomially distributed with mean \(\mu_{j}\) and \(n/2\) trials. By similar computations as Lemma 4, we have

\[\mathbb{E}\left[\sum_{j\in S_{T+1}}\lvert\mu_{j}-\widehat{\mu}_{j}\rvert^{p} \right]=O\bigg{(}s\Big{(}\frac{p}{n}\Big{)}^{p/2}\bigg{)}.\] (26)

Next we show for all \(j\in S_{T}\) such that \(\mu_{j}\neq 0\),

\[\lvert\mu_{j}\rvert^{p}\Pr[\,j\notin S_{T+1}\,]\leq 2\bigg{(}\frac{p\lor 64\ln \frac{2\ell}{s}}{n}\bigg{)}^{p/2}.\] (27)

If \(\lvert\mu_{j}\rvert\leq H^{\prime}:=8\sqrt{\frac{\ln\frac{2\ell}{n}}{n}}\), Eq. (27) always holds since \(\Pr[\,j\notin S\,]\leq 1\). Hence we hereafter assume that \(\lvert\mu_{j}\rvert>H^{\prime}\), and write \(\mu_{j}=\beta_{j}H^{\prime}\) for some \(\beta_{j}>1\).

Let \(a_{T+1,j}\) be the number of coordinates \(j^{\prime}\) with \(\mu_{j^{\prime}}=0\) and \(\lvert M_{T+1,j^{\prime}}\rvert\geq\frac{n}{2}\cdot\frac{\beta_{j}H^{\prime}}{2}\). Then since \(S_{T+1}\) contains the top \(3s\) coordinates with the largest magnitude of the sum, we have \(j\notin S_{T+1}\) happens only if at least one of the following occurs (i) \(a_{T+1,j}>2s\), or (ii) \(M_{T+1,j}<\frac{n}{2}\cdot\frac{\beta_{j}H^{\prime}}{2}\).

By Hoeffding's inequality, we have

\[\Pr\bigg{[}\,M_{T+1,j}<\frac{n}{2}\cdot\frac{\beta_{j}H^{\prime}}{2}\,\bigg{]} \leq\exp\!\left(-\frac{1}{2}\cdot\frac{n}{2}\cdot\left(\frac{\beta_{j}H^{ \prime}}{2}\right)^{2}\right)=\left(\frac{2\ell}{s}\right)^{-4\beta_{j}^{2}}: =p_{T+1,j}.\]

Similarly, for any \(j^{\prime}\) such that \(\mu_{j^{\prime}}=0\),

\[\Pr\bigg{[}\,\lvert M_{T+1,j^{\prime}}\rvert\geq\frac{n}{2}\cdot\frac{\beta_ {j}H^{\prime}}{2}\,\bigg{]}\leq 2p_{T+1,j}.\]

Since all coordinates are independent, \(a_{T+1,j}\) is binomially distributed with mean at most \(2p_{T+1,j}\ell\), and therefore, by Markov's inequality,

\[\Pr[\,a_{T+1,j}>2s\,]\leq\frac{2p_{T+1,j}\ell}{2s}\leq\left(\frac{2\ell}{s} \right)^{1-4\beta_{j}^{2}}\leq\left(\frac{2\ell}{s}\right)^{-3\beta_{j}^{2}}\]

the last step since \(\beta_{j}>1\). By a union bound, we have

\[\Pr[\,j\notin S_{T}\,]\leq\Pr[\,a_{T+1,j}>2s\,]+\Pr\bigg{[}\,M_{T+1,j}<\frac{ 1}{4}\frac{n}{2}\cdot\frac{\beta_{j}H^{\prime}}{2}\,\bigg{]}\leq 2\bigg{(} \frac{2\ell}{s}\bigg{)}^{-3\beta_{j}^{2}}.\]

Using the inequality \(x^{p}a^{-x^{2}}\leq\big{(}\frac{p}{2e\ln a}\big{)}^{p/2}\) which holds for all \(x>0\), we get overall

\[\lvert\mu_{j}\rvert^{p}\cdot\Pr[\,j\notin S_{T}\,]\leq 2H^{\prime p}\beta_{j}^{p} \bigg{(}\frac{2\ell}{s}\bigg{)}^{-4\beta_{j}^{2}}\leq 2\Big{(}\frac{p}{en} \bigg{)}^{p/2},\]

establishing Eq. (27). Combining Eq. (26) and Eq. (27) concludes the proof Lemma 5 since there are at most \(s\) unbiased coordinates. 

Algorithm under LDP with \(\varepsilon>1\)To get a \(\varepsilon\)-LDP algorithm in the regime \(\varepsilon>1\) (low-privacy regime), we perform the following changes to obtain a private algorithm from Algorithm 2:

* Each user independently flips each coordinate of their local sample to get \(Z_{i}\) where, for all \(x\in[d]\), \((Z_{i})_{x}=(X_{i})_{x}\) with probability \(\frac{\varepsilon}{e+1}\) and \((Z_{i})_{x}=1-(X_{i})_{x}\) with probability \(\frac{1}{e+1}\) (note that this corresponds to applying Randomized Response independently to each bit with privacy parameter \(1\)).
* Users then follow Algorithm 2 with the setting \(\ell=\lfloor\varepsilon\rfloor\) and local data \(\{Z_{i}\}_{i\in[n]}\), and obtain estimate \(\widehat{\mu}\).
* The final estimate is then \(\frac{e+1}{e-1}\widehat{\mu}\).

The privacy guarantee of the algorithm comes from the fact that Algorithm 2 sends at most \(\ell=\lfloor\varepsilon\rfloor\) coordinates of each \(Z_{i}\), and for any \(S\) with \(|S|\leq\lfloor\varepsilon\rfloor\)

\[\frac{\Pr[\,\{(Z_{i})_{x}\}_{x\in S}\mid X_{i}\,]}{\Pr[\,\{(Z_{i})_{x}\}_{x\in S }\mid X_{i}^{\prime}\}=\prod_{x\in S}\frac{\Pr[\,(Z_{i})_{x}\mid(X_{i})_{x}\,] }{\Pr[\,(Z_{i})_{x}\mid(X_{i}^{\prime})_{x}\,]}\leq e^{\lfloor\varepsilon \rfloor}.\]

The utility guarantee follows from observing that \(\mu_{Z}=\frac{e-1}{e+1}\mu\) and hence any \(\ell_{p}\) error guarantee will be preserved up to a constant.

### Gaussian Mean Estimation

Recall that \(\mathcal{G}_{d,s}\) denotes the family of \(d\)-dimensional spherical Gaussian distributions with \(s\)-sparse mean in \([-1,1]^{d}\), _i.e._,

\[\mathcal{G}_{d,s}=\{\ \mathcal{G}(\mu,\mathbb{I}):\ \left\|\mu\right\|_{ \infty}\leq 1,\left\|\mu\right\|_{0}\leq s\ \}\.\] (28)

We will prove the following results for LDP and communication constraints, respectively.

**Proposition 3**.: _Fix \(p\in[1,\infty]\). For \(n\geq 1\) and \(\varepsilon\in(0,1]\), there exists an \((n,\gamma)\)-estimator using \(\mathcal{W}_{\varepsilon}\) under \(\ell_{p}\) loss for \(\mathcal{G}_{d,s}\) with \(\gamma=O\bigg{(}\sqrt{\frac{\rho ds^{2/p}}{n\varepsilon^{2}}}\bigg{)}\) for \(p\leq 2\log s\) and \(\gamma=O\bigg{(}\sqrt{\frac{d\log s}{n\varepsilon^{2}}}\bigg{)}\) for \(p>2\log s\)._

**Proposition 4**.: _Fix \(p\in[1,\infty]\). For \(n\geq 1\) and \(\ell\leq d\), there exists an \((n,\gamma)\)-estimator using \(\mathcal{W}_{\ell}\) under \(\ell_{p}\) loss for \(\mathcal{G}_{d,s}\) with \(\gamma=O\bigg{(}\sqrt{\frac{\rho ds^{2/p}}{n\ell}+\frac{(p+\log(2\ell/s))s^{2/ p}}{n}}\bigg{)}\) for \(p\leq 2\log s\) and \(\gamma=O\bigg{(}\sqrt{\frac{d\log s}{n\ell}+\frac{\log\ell}{n}}\bigg{)}\) for \(p>2\log s\)._

We reduce the problem of Gaussian mean estimation to that of Bernoulli mean estimation and then invoke Propositions 1 and 2 from the previous section. At the heart of the reduction is a simple idea that was used in, _e.g._, [10, 2, 11]: the sign of a Gaussian random variable already preserves sufficient information about the mean. Details follow.

Let \(\mathbf{p}\in\mathcal{G}_{d,s}\) with mean \(\mu(\mathbf{p})=(\mu(\mathbf{p})_{1},\ldots,\mu(\mathbf{p})_{d})\). For \(X\sim\mathbf{p}\), let \(Y=(\operatorname{sign}(X_{i}))_{i\in[d]}\in\{-1,+1\}^{d}\) be a random variable indicating the signs of the \(d\) coordinates of \(X\). By the independence of the coordinates of \(X\), note that \(Y\) is distributed as a product Bernoulli distribution (in \(\mathcal{B}_{d}\)) with mean vector \(\nu(\mathbf{p})\) given by

\[\nu(\mathbf{p})_{i}=2\Pr_{X\sim\mathbf{p}}[\,X_{i}>0\,]-1=\operatorname{Erf} \bigg{(}\frac{\mu(\mathbf{p})_{i}}{\sqrt{2}}\bigg{)},\qquad i\in[d],\] (29)

and, since \(|\mu(\mathbf{p})_{i}|\leq 1\), we have \(\nu(\mathbf{p})\in[-\eta,\eta]^{d}\), where \(\eta:=\operatorname{Erf}\big{(}1/\sqrt{2}\big{)}\approx 0.623\). Moreover, it is immediate to see that each player, given a sample from \(\mathbf{p}\), can convert it to a sample from the corresponding product Bernoulli distribution. We now show that a good estimate for \(\nu(\mathbf{p})\) yields a good estimate for \(\mu(\mathbf{p})\).

**Lemma 6**.: _Fix any \(p\in[1,\infty)\), and \(\mathbf{p}\in\mathcal{G}_{d}\). For \(\widehat{\nu}\in[-\eta,\eta]^{d}\), define \(\widehat{\mu}\in[-1,1]^{d}\) by \(\widehat{\mu}_{i}:=\sqrt{2}\operatorname{Erf}^{-1}(\widehat{\nu}_{i}),\) for all \(i\in[d]\). Then_

\[\left\|\mu(\mathbf{p})-\widehat{\mu}\right\|_{p}\leq\sqrt{\frac{e\pi}{2}}\cdot \left\|\nu(\mathbf{p})-\widehat{\nu}\right\|_{p}.\]

Proof.: By computing the maximum of its derivative,8 we observe that the function \(\operatorname{Erf}^{-1}\) is \(\frac{\sqrt{e\pi}}{2}\). Lipschitz on \([-\eta,\eta]\). By the definition of \(\widehat{\mu}\) and recalling Eq. (29), we then have

Footnote 8: Specifically, we have that \(\max_{x\in[-\eta,\eta]}\operatorname{Erf}^{-1}(x)=1/\sqrt{2}\) by definition of \(\eta\) and monotonicity of \(\operatorname{Erf}\). Recalling then that, for all \(x\in[-\eta,\eta]\), \((\operatorname{Erf}^{-1})^{\prime}(x)=\frac{1}{\operatorname{Erf}^{\prime}( \operatorname{Erf}^{-1}(x))}=\frac{\sqrt{\pi}}{2}e^{(\operatorname{Erf}^{-1}( x))^{2}}\leq\frac{\sqrt{\pi}}{2}e^{\frac{1}{2}}\), we get the Lipschitzness claim.

\[\left\|\mu(\mathbf{p})-\widehat{\mu}\right\|_{p}^{p}=\sum_{i=1}^{d}\left|\mu( \mathbf{p})_{i}-\widehat{\mu}_{i}\right|^{p}=2^{p/2}\cdot\sum_{i=1}^{d}\left| \operatorname{Erf}^{-1}(\nu_{i})-\operatorname{Erf}^{-1}(\widehat{\nu}_{i}) \right|^{p}\leq\left(\frac{e\pi}{2}\right)^{p/2}\cdot\sum_{i=1}^{d}\left|\nu_ {i}-\widehat{\nu}_{i}\right|^{p},\]

where we used the fact that \(\nu,\widehat{\nu}\in[-\eta,\eta]^{d}\).

[MISSING_PAGE_EMPTY:22]

In applications, we expect the dependence of \(\xi_{z,i}^{\gamma}\) on \(\gamma\) to be "mild," and, in essence, the assumption above provides a linear expansion of the term \(\alpha_{z,i}\phi_{z,i}\) from Assumption 1 as a function of the perturbation parameter \(\gamma\). Assuming that the densities are differentiable as a function of \(\theta\), for the distribution \(\mathbf{p}_{\theta}^{W}\) of the output of a channel \(W\) with input \(X\sim\mathbf{p}_{\theta}\), we get

\[\frac{\partial\mathbf{p}_{\theta}^{W}(y)}{\partial\theta_{i}} =z_{i}\lim_{\gamma\to 0}\frac{\mathbf{p}_{\theta_{z}}^{W}(y)- \mathbf{p}_{\theta_{z}\phi_{z}}^{W}(y)}{\gamma}\] \[=z_{i}\lim_{\gamma\to 0}\mathbb{E}_{\mathbf{p}_{z}}\big{[}(\xi_{z,i }^{\gamma}(X)+\gamma\psi_{z,i}^{\gamma}(X))W(y\mid X)\big{]}\] \[=z_{i}\mathbb{E}_{\mathbf{p}_{\theta}}\big{[}\xi_{z,i}W(y\mid X) \big{]},\]

where we used Eq. (30), the fact that \(\lim_{\gamma\to 0}\theta_{z}=\theta\), the fact that \(\mathbb{E}_{\mathbf{p}_{z}}\big{[}\psi_{z,i}^{\gamma}(X)W(y\mid X)\big{]}\leq c \sqrt{\mathbb{E}_{\mathbf{p}_{z}}[W(y\mid X)^{2}]}\leq c\), and the dominated convergence theorem. Thus, we get

\[\mathrm{Tr}\big{(}J^{W}(\theta)\big{)}=\sum_{i=1}^{k}\int_{\mathcal{Y}}\frac{ \mathbb{E}_{\mathbf{p}_{\theta}}[\xi_{z,i}(X)W(y\mid X)]^{2}}{\mathbb{E}_{ \mathbf{p}_{z}}[W(y\mid X)]}\,\mathrm{d}\mu\,.\] (31)

Our information contraction bound will be seen later (Section 5) to yield lower bounds for expected estimation error. For concreteness, we give a preview of a version here. We assume for simplicity that \(\mathcal{W}_{i}=\mathcal{W}\) for all \(t\) and consider the \(\ell_{2}\) loss function for the dense (\(\tau=1/2\)) case. By following the proof of Lemma 1 below, given an \((n,\gamma)\)-estimator \(\hat{\theta}=\hat{\theta}(Y^{n},U)\) of \(\mathcal{P}_{\Theta}\) using \(\mathcal{W}^{n}\) under \(\ell_{2}\) loss, we can find an estimator \(\hat{Z}=\hat{Z}(Y^{n},U)\) such that

\[\gamma^{2}\sum_{i=1}^{k}\mathrm{Pr}\Big{[}\,\hat{Z}_{i}\neq Z_{i}\,\Big{]}= \mathbb{E}\Big{[}\big{\|}\theta_{Z}-\theta_{Z}\big{\|}_{2}^{2}\Big{]}\leq 4 \gamma^{2},\]

whereby

\[\frac{1}{k}\sum_{i=1}^{k}\mathrm{d}_{\mathrm{TV}}\Big{(}\mathbf{p}_{+i}^{Y^{n }},\mathbf{p}_{-i}^{Y^{n}}\Big{)}\geq 1-\frac{2}{k}\sum_{i=1}^{k}\mathrm{Pr} \Big{[}\,\hat{Z}_{i}\neq Z_{i}\,\Big{]}\geq 1-\frac{8\gamma^{2}}{k\gamma^{2}}.\]

Upon setting \(\gamma:=4\gamma/\sqrt{k}\), we get that the left-side of Eq. (3) is bounded below by \(1/4\). For the same \(\gamma\) and under Eq. (30), the right-side evaluates to

\[\frac{4\gamma^{2}n}{k}\max_{z\in\mathcal{Z}}\max_{W\in\mathcal{W} }\sum_{i=1}^{k}\int_{\mathcal{Y}}\frac{\mathbb{E}_{\mathbf{p}_{z}}\big{[}(\xi_ {z,i}^{\gamma}(X)+\gamma\psi_{z,i}^{\gamma}(X))W(y\mid X)\big{]}^{2}}{\mathbb{ E}_{\mathbf{p}_{z}}[W(y\mid X)]}\,\mathrm{d}\mu\] \[\leq\frac{8\gamma^{2}n}{k}\max_{z\in\mathcal{Z}}\max_{W\in \mathcal{W}}\sum_{i=1}^{k}\int_{\mathcal{Y}}\frac{\mathbb{E}_{\mathbf{p}_{z}} \big{[}\xi_{z,i}^{\gamma}(X)W(y\mid X)\big{]}^{2}+\gamma^{2}\mathbb{E}_{ \mathbf{p}_{z}}\big{[}\psi_{z,i}^{\gamma}(X)W(y\mid X)\big{]}^{2}}{\mathbb{ E}_{\mathbf{p}_{z}}[W(y\mid X)]}\,\mathrm{d}\mu\] \[\leq\frac{128\gamma^{2}n}{k^{2}}\left(\max_{z\in\mathcal{Z}}\max_ {W\in\mathcal{W}}\sum_{i=1}^{k}\int_{\mathcal{Y}}\frac{\mathbb{E}_{\mathbf{p}_ {z}}\big{[}\xi_{z,i}^{\gamma}(X)W(y\mid X)\big{]}^{2}}{\mathbb{E}_{\mathbf{p}_ {z}}[W(y\mid X)]}\,\mathrm{d}\mu+c^{2}\gamma^{2}\right),\]

where we used \((a+b)^{2}\leq 2(a^{2}+b^{2})\) and

\[\int_{\mathcal{Y}}\frac{\mathbb{E}_{\mathbf{p}_{z}}\big{[}\psi_{z,i}^{\gamma}( X)W(y\mid X)\big{]}^{2}}{\mathbb{E}_{\mathbf{p}_{z}}[W(y\mid X)]}\,\mathrm{d}\mu\leq \int_{\mathcal{Y}}\mathbb{E}_{\mathbf{p}_{z}}\big{[}\psi_{z,i}^{\gamma}(X)^{2 }W(y\mid X)\big{]}\,\mathrm{d}\mu=\mathbb{E}_{\mathbf{p}_{z}}\big{[}\psi_{z,i }^{\gamma}(X)^{2}\big{]}\leq c^{2}.\]

Therefore, Eq. (3) yields

\[\gamma^{2}\geq\frac{k^{2}}{256\cdot n\,\left(\max_{z\in\mathcal{Z}}\max_{W \in\mathcal{W}}\sum_{i=1}^{k}\int_{\mathcal{Y}}\frac{\mathbb{E}_{\mathbf{p}_{z}} \big{[}\xi_{z,i}^{\gamma}(X)W(y\mid X)\big{]}^{2}}{\mathbb{E}_{\mathbf{p}_{z }}[W(y\mid X)]}\,\mathrm{d}\mu+c^{2}\right)}.\]

This bound is, in effect, the same as the van Trees inequality with \(\mathrm{Tr}\big{(}J^{W}(\theta)\big{)}\) replaced by

\[g(\gamma):=\sum_{i=1}^{k}\int_{\mathcal{Y}}\frac{\mathbb{E}_{\mathbf{p}_{z}}[ \phi_{z,i}(X)W(y\mid X)]^{2}}{\mathbb{E}_{\mathbf{p}_{z}}[W(y\mid X)]}\, \mathrm{d}\mu\,.\]In fact, in view of Eq. (31), \(\operatorname{Tr}\bigl{(}J^{W}(\theta)\bigr{)}=\lim_{\gamma\to 0}g(\gamma)=:g(0)\). Thus, our general lower bound will recover van Trees inequality-based bounds when Eq. (30) holds and \(g(\gamma)\approx g(0)\). We note that Eq. (30) holds for all the families considered in this paper (see Eq. (40) for product Bernoulli, Eq. (42) for Gaussian, and Eq. (50) for discrete distributions). We close this discussion by noting that results in Section 3 are obtained by deriving bounds for \(g(\gamma)\) which apply for all \(\gamma\) and, therefore, also for \(g(0)=\operatorname{Tr}\bigl{(}J^{W}(\theta)\bigr{)}\).

## Appendix E Missing proofs in Section 3

### Proof of Theorem 1

Consider \(Z=(Z_{1},\ldots,Z_{k})\in\{-1,1\}^{k}\) where \(Z_{1},\ldots,Z_{k}\) are i.i.d. with \(\Pr[\,Z_{i}=1\,]=\tau\). For a fixed \(i\in[k]\), let

\[\mathbf{p}_{+i}^{Y^{n}} :=\mathbb{E}_{Z}\Bigl{[}\mathbf{p}_{Z}^{Y^{n}}\mid Z_{i}=+1\Bigr{]} =\sum_{z:z_{i}=+1}\Big{(}\prod_{j\neq i}\tau^{\frac{1+z_{j}}{2}}( 1-\tau)^{\frac{1-z_{j}}{2}}\Big{)}\mathbf{p}_{z}^{Y^{n}}\] \[\mathbf{p}_{-i}^{Y^{n}} :=\mathbb{E}_{Z}\Bigl{[}\mathbf{p}_{Z}^{Y^{n}}\mid Z_{i}=-1\Bigr{]} =\sum_{z:z_{i}=-1}\Big{(}\prod_{j\neq i}\tau^{\frac{1+z_{j}}{2}}( 1-\tau)^{\frac{1-z_{j}}{2}}\Big{)}\mathbf{p}_{z}^{Y^{n}},\]

the partial mixtures of message distributions conditioned on \(Z_{i}\). We will rely on the following lemma, which relates the desired average discrepancy between the \(\mathbf{p}_{+i}^{Y^{n}}\) and \(\mathbf{p}_{-i}^{Y^{n}}\)'s to the sum of \(n\) "local" discrepancy measures (in the form of Hellinger distances between local messages). Each local measure can then be easily bounded in terms of the density \(\mathbf{p}_{z}\) and the channel \(W\) to get the desired bound.

**Lemma 7**.: _With the notation of Theorem 1, we have_

\[\Biggl{(}\frac{1}{k}\sum_{i=1}^{k}\mathrm{d}_{\mathrm{TV}}\Bigl{(}\mathbf{p}_{ +i}^{Y^{n}},\mathbf{p}_{-i}^{Y^{n}}\Bigr{)}\Biggr{)}^{2}\leq\frac{14}{k}\sum _{t=1}^{n}\max_{z\in\mathcal{Z}}\max_{W\in\mathcal{W}_{t}}\sum_{i=1}^{k} \mathrm{d}_{\mathrm{H}}\bigl{(}\mathbf{p}_{z}^{W},\mathbf{p}_{z^{\otimes i}} ^{W}\bigr{)}^{2},\] (32)

_where \(\mathbf{p}_{z}^{W}\) denotes the distribution of \(Y\sim W(\cdot\mid X)\) when \(X\sim\mathbf{p}_{z}\)._

The proof of the lemma is rather involved and constitutes the core of the argument. We defer it to the end of the section and show first how it implies Theorem 1. For all \(z\) and \(W\), we have

\[\mathrm{d}_{\mathrm{H}}\bigl{(}\mathbf{p}_{z}^{W},\mathbf{p}_{z^{ \otimes i}}^{W}\bigr{)}^{2} =\frac{1}{2}\int_{\mathcal{Y}\mathcal{Y}}\Biggl{(}\sqrt{\mathbb{E} _{\mathbf{p}_{z}}[W(y\mid X)]}-\sqrt{\mathbb{E}_{\mathbf{p}_{z^{\otimes i}}}[W (y\mid X)]}\Biggr{)}^{2}\,\mathrm{d}\mu\] \[=\frac{1}{2}\int_{\mathcal{Y}}\Biggl{(}\frac{\mathbb{E}_{\mathbf{ p}_{z}}[W(y\mid X)]-\mathbb{E}_{\mathbf{p}_{z^{\otimes i}}}[W(y\mid X)]}{ \sqrt{\mathbb{E}_{\mathbf{p}_{z}}[W(y\mid X)]}+\sqrt{\mathbb{E}_{\mathbf{p}_{ z^{\otimes i}}}[W(y\mid X)]}}\Biggr{)}^{2}\,\mathrm{d}\mu\] \[\leq\frac{1}{2}\int_{\mathcal{Y}}\frac{(\mathbb{E}_{\mathbf{p}_{ z}}[W(y\mid X)]-\mathbb{E}_{\mathbf{p}_{z^{\otimes i}}}[W(y\mid X)])^{2}}{ \mathbb{E}_{\mathbf{p}_{z}}[W(y\mid X)]}\,\mathrm{d}\mu\,.\] (33)

Moreover, under Assumption 1; for any \(W\in\mathcal{W}_{t}\) and \(y\in\mathcal{Y}\),

\[\mathbb{E}_{\mathbf{p}_{z^{\otimes i}}}[W(y\mid X)]=\mathbb{E}_{\mathbf{p}_{z} }\Biggl{[}\frac{\mathrm{d}\mathbf{p}_{z^{\otimes i}}}{\mathrm{d}\mathbf{p}_{z}} \cdot W(y\mid X)\Biggr{]}=\mathbb{E}_{\mathbf{p}_{z}}\bigl{[}(1+\phi_{z,i}(X)) \cdot W(y\mid X)\bigr{]}\,.\]

Plugging this back into (33), we get

\[\mathrm{d}_{\mathrm{H}}\bigl{(}\mathbf{p}_{z}^{W},\mathbf{p}_{z^{ \otimes i}}^{W}\bigr{)}^{2}\leq\frac{1}{2}\int_{\mathcal{Y}}\frac{\mathbb{E}_{ \mathbf{p}_{z}}[\phi_{z,i}(X)W(y\mid X)]^{2}}{\mathbb{E}_{\mathbf{p}_{z}}[W( y\mid X)]}\,\mathrm{d}\mu\,.\]

Combining this with Lemma 7 concludes the proof of Theorem 1.

Proof of Lemma 7.: Our first step is to use the Cauchy-Schwarz inequality, followed by an inequality relating total variation and Hellinger distances:

\[\frac{1}{k}\bigg{(}\sum_{i=1}^{k}\mathrm{d}_{\mathrm{TV}}\Big{(} \mathbf{p}_{+i}^{Y^{n}},\mathbf{p}_{-i}^{Y^{n}}\Big{)}\bigg{)}^{2} \leq\sum_{i=1}^{k}\mathrm{d}_{\mathrm{TV}}\Big{(}\mathbf{p}_{+i}^ {Y^{n}},\mathbf{p}_{-i}^{Y^{n}}\Big{)}^{2}\] \[\leq 2\sum_{i=1}^{k}\mathbb{E}_{Z}\bigg{[}\mathrm{d}_{\mathrm{H}} \Big{(}\mathbf{p}_{Z}^{Y^{n}},\mathbf{p}_{Z^{\otimes i}}^{Y^{n}}\Big{)}^{2}\mid Z _{i}=+1\bigg{]}\] \[=2\sum_{i=1}^{k}\mathbb{E}_{Z}\bigg{[}\mathrm{d}_{\mathrm{H}} \Big{(}\mathbf{p}_{Z}^{Y^{n}},\mathbf{p}_{Z^{\otimes i}}^{Y^{n}}\Big{)}^{2} \bigg{]}\,,\] (34)

where the last inequality uses joint convexity of squared Hellinger distance, and the final identity is due to independence of each coordinate of \(Z\) and symmetry of Hellinger whereby \(\mathbb{E}_{Z}\Big{[}\mathrm{d}_{\mathrm{H}}\big{(}\mathbf{p}_{Z}^{Y^{n}}, \mathbf{p}_{Z^{\otimes i}}^{Y^{n}}\big{)}^{2}\mid Z_{i}=+1\Big{]}=\mathbb{E}_{ Z}\Big{[}\mathrm{d}_{\mathrm{H}}\big{(}\mathbf{p}_{Z}^{Y^{n}},\mathbf{p}_{Z^{ \otimes i}}^{Y^{n}}\big{)}^{2}\mid Z_{i}=-1\Big{]}\).

In order to bound the resulting terms of the sum, we will rely on the so-called _cut-paste_ property of Hellinger distance [6]. Before doing so, we will require an additional piece of notation: for fixed \(z\in\mathcal{Z}\), \(i\in[k]\), \(t\in[n]\), let \(\mathbf{p}_{t\gets z^{\otimes i}}^{Y^{n}}\) denote the message distribution where player \(t\) gets a sample from \(\mathbf{p}_{z^{\otimes i}}\) and all other players get samples from \(\mathbf{p}_{z}\). That is, for all \(y^{n}\in\mathcal{Y}^{n}\), the density of \(\mathbf{p}_{t\gets z^{\otimes i}}^{Y^{n}}\) with respect to the underlying product measure \(\mu^{\otimes n}\) is given by

\[\frac{\mathrm{d}\mathbf{p}_{t\gets z^{\otimes i}}^{Y^{n}}}{ \mathrm{d}\mu^{\otimes n}}(y^{n})=\mathbb{E}_{X_{t}\sim\mathbf{p}_{z^{\otimes i }}}\Big{[}W^{y^{t-1}}(y_{t}\mid X_{t})\Big{]}\cdot\prod_{j\neq t}\mathbb{E}_{ X_{j}\sim\mathbf{p}_{z}}\Big{[}W^{y^{j-1}}(y_{j}\mid X_{j})\Big{]}.\] (35)

The following lemma, due to [22], allows us to relate \(\mathrm{d}_{\mathrm{H}}\big{(}\mathbf{p}_{z}^{Y^{n}},\mathbf{p}_{z^{\otimes i }}^{Y^{n}}\big{)}\), the distance between message distributions when all players get observations from \(\mathbf{p}_{z}\), or all from \(\mathbf{p}_{z^{\otimes i}}\), to the distances \(\mathrm{d}_{\mathrm{H}}\big{(}\mathbf{p}_{z}^{Y^{n}},\mathbf{p}_{t\gets z ^{\otimes i}}^{Y^{n}}\big{)}\) where only _one_ of the \(n\) players gets a sample from \(\mathbf{p}_{z^{\otimes i}}\).

**Lemma 8** ([22, Theorem 7]).: _There exists \(c_{\mathrm{H}}>0\) such that for all \(z\in\mathcal{Z}\) and \(i\in[k]\),_

\[\mathrm{d}_{\mathrm{H}}\Big{(}\mathbf{p}_{z}^{Y^{n}},\mathbf{p}_{z^{\otimes i }}^{Y^{n}}\Big{)}^{2}\leq c_{\mathrm{H}}\sum_{t=1}^{n}\mathrm{d}_{\mathrm{H}} \Big{(}\mathbf{p}_{z}^{Y^{n}},\mathbf{p}_{t\gets z^{\otimes i}}^{Y^{n}} \Big{)}^{2}.\]

_Moreover, one can take \(c_{\mathrm{H}}=2\prod_{t=1}^{\infty}\frac{1}{1-2^{-t}}<7\)._

Combining Eq. (34) and Lemma 8, we get

\[\frac{1}{k}\bigg{(}\sum_{i=1}^{k}\mathrm{d}_{\mathrm{TV}}\Big{(} \mathbf{p}_{+i}^{Y^{n}},\mathbf{p}_{-i}^{Y^{n}}\Big{)}\bigg{)}^{2} \leq 14\sum_{i=1}^{k}\sum_{t=1}^{n}\mathbb{E}_{Z}\bigg{[} \mathrm{d}_{\mathrm{H}}\Big{(}\mathbf{p}_{Z}^{Y^{n}},\mathbf{p}_{t\gets Z ^{\otimes i}}^{Y^{n}}\Big{)}^{2}\bigg{]}\] \[=14\sum_{t=1}^{n}\mathbb{E}_{Z}\bigg{[}\sum_{i=1}^{k}\mathrm{d}_{ \mathrm{H}}\Big{(}\mathbf{p}_{Z}^{Y^{n}},\mathbf{p}_{t\gets Z^{\otimes i}}^ {Y^{n}}\Big{)}^{2}\bigg{]}.\] (36)In view of bounding the RHS of (36) term by term, fix \(j\in[n]\) and \(z\in\mathcal{Z}\). Recalling the expression of \(\mathbf{p}^{Y^{n}}_{t\gets z^{\otimes t}}\) from (35), unrolling the definition of Hellinger distance, and recalling (35), we have

\[2\sum_{i=1}^{k}\mathrm{d}_{\mathrm{H}}\Big{(}\mathbf{p}^{Y^{n}}_{ z},\mathbf{p}^{Y^{n}}_{t\gets z^{\otimes t}}\Big{)}^{2}\] \[=\sum_{i=1}^{k}\int_{\mathcal{Y}^{n}}\left(\sqrt{\frac{\mathrm{d }\mathbf{p}^{Y^{n}}_{z}}{\mathrm{d}\mu^{\otimes n}}}-\sqrt{\frac{\mathrm{d} \mathbf{p}^{Y^{n}}_{t\gets z^{\otimes t}}}{\mathrm{d}\mu^{\otimes n}}} \right)^{2}\mathrm{d}\mu^{\otimes n}\] \[=\sum_{i=1}^{k}\int_{\mathcal{Y}^{n}}\prod_{j\neq t}\mathbb{E}_{ \mathbf{p}_{z}}\Big{[}W^{y^{j-1}}(y_{j}\mid X)\Big{]}\underbrace{\left(\sqrt {\mathbb{E}_{\mathbf{p}_{z}}\left[W^{y^{t-1}}(y_{t}\mid X)\right]}-\sqrt{ \mathbb{E}_{\mathbf{p}_{z^{\otimes t}}}\left[W^{y^{t-1}}(y_{t}\mid X)\right]} \right)^{2}}_{:=f_{i,t}(y^{t-1},y_{t})}\mathrm{d}\mu^{\otimes n}\] \[=\sum_{i=1}^{k}\int_{\mathcal{Y}^{t-1}}\prod_{j<t}\mathbb{E}_{ \mathbf{p}_{z}}\Big{[}W^{y^{j-1}}(y_{j}\mid X)\Big{]}\int_{\mathcal{Y}}f_{i,t} (y^{t-1},y_{t})\int_{\mathcal{Y}^{m-t}}\prod_{j>t}\mathbb{E}_{\mathbf{p}_{z}} \Big{[}W^{y^{j-1}}(y_{j}\mid X)\Big{]}\,\mathrm{d}\mu^{\otimes(t-1)}\, \mathrm{d}\mu\,\mathrm{d}\mu^{\otimes(n-t)}\] \[=\sum_{i=1}^{k}\int_{\mathcal{Y}^{t-1}}\prod_{j<t}\mathbb{E}_{ \mathbf{p}_{z}}\Big{[}W^{y^{j-1}}(y_{j}\mid X)\Big{]}\sum_{i=1}^{k}\int_{ \mathcal{Y}}f_{i,t}(y^{t-1},y_{t})\mathrm{d}\mu\,\mathrm{d}\mu^{\otimes(t-1)}\,\]

where the second-to-last identity uses the observation that, for any fixed \(y^{t}\in\mathcal{Y}^{t}\),

\[\int_{\mathcal{Y}^{n-t}}\prod_{j>t}\mathbb{E}_{\mathbf{p}_{z}}\Big{[}W^{y^{j- 1}}(y_{j}\mid X)\Big{]}\,\mathrm{d}\mu^{\otimes(n-t)}=1,\]

which in turn follows upon taking marginal integrals for each coordinate. We then get from the pointwise inequality \(\sum_{i=1}^{k}\int_{\mathcal{Y}^{t-1}}f_{i,t}(y^{t-1},y_{t})\,\mathrm{d}\mu \leq\sup_{y^{\prime}\in\mathcal{Y}^{t-1}}\sum_{i=1}^{k}\int_{\mathcal{Y}}f_{i,t}(y^{\prime},y_{t})\,\mathrm{d}\mu\) that

\[2\sum_{i=1}^{k}\mathrm{d}_{\mathrm{H}}\Big{(}\mathbf{p}^{Y^{n}}_ {z},\mathbf{p}^{Y^{n}}_{t\gets z^{\otimes t}}\Big{)}^{2} \leq\int_{\mathcal{Y}^{t-1}}\prod_{j<t}\mathbb{E}_{\mathbf{p}_{z} }\Big{[}W^{y^{j-1}}(y_{j}\mid X)\Big{]}\sup_{y^{\prime}\in\mathcal{Y}^{t-1}} \sum_{i=1}^{k}\biggl{(}\int_{\mathcal{Y}}f_{i,t}(y^{\prime},y_{t})\,\mathrm{d} \mu\biggr{)}\,\mathrm{d}\mu^{\otimes(t-1)}\] \[=\sup_{y^{\prime}\in\mathcal{Y}^{t-1}}\sum_{i=1}^{k}\int_{ \mathcal{Y}}\biggl{(}\sqrt{\mathbb{E}_{\mathbf{p}_{z}}[W^{y^{\prime}}(y\mid X) ]}-\sqrt{\mathbb{E}_{\mathbf{p}_{z^{\otimes t}}}[W^{y^{\prime}}(y\mid X)]} \biggr{)}^{2}\,\mathrm{d}\mu\] \[\leq\sup_{W\in\mathcal{W}_{t}}\sum_{i=1}^{k}\int_{\mathcal{Y}} \biggl{(}\sqrt{\mathbb{E}_{\mathbf{p}_{z}}[W(y\mid X)]}-\sqrt{\mathbb{E}_{ \mathbf{p}_{z^{\otimes t}}}[W(y\mid X)]}\biggr{)}^{2}\,\mathrm{d}\mu\] \[=2\cdot\sup_{W\in\mathcal{W}_{t}}\sum_{i=1}^{k}\mathrm{d}_{ \mathrm{H}}\big{(}\mathbf{p}^{W}_{z},\mathbf{p}^{W}_{z^{\otimes t}}\big{)}^{2}.\] (37)

the second identity follows upon taking marginal integrals, and by replacing \(f_{i,t}\) by its definition; and the second inequality using that \(\Big{\{}\ W^{y^{\prime}}\ :\ y^{\prime}\in\mathcal{Y}^{t-1}\ \Big{\}}\subseteq\mathcal{W}_{t}\), so that we are taking a supremum over a larger set.

Plugging this back into (36) and upper bounding the inner expectation by a maximum concludes the proof of the lemma.

### Proof of Theorem 2

Our starting point is Eq. (3) which holds under Assumption 1. We will bound the right-hand-side of Eq. (3) under assumptions of orthogonality and subgaussianity to prove the two bounds in Theorem 2.

First, under orthogonality (Assumption 2), we apply Bessel's inequality to Eq. (3). For a fixed \(z\in\mathcal{Z}\), write \(\psi_{z,i}=\frac{\phi_{z,i}}{\sqrt{\mathbb{E}_{\mathbf{p}_{z}}\left[\phi_{z,i}^ {2}\right]}}\), and complete \((1,\psi_{z,1},\ldots,\psi_{z,k})\) to get an orthonormal basis \(\mathcal{B}\) for \(L^{2}(\mathcal{X},\mathbf{p}_{z})\). Fix any \(W\in\mathcal{W}\) and \(y\in\mathcal{Y}\), and, for brevity, define \(a\colon\mathcal{X}\to\mathbb{R}\) as \(a(x)=W(y\mid x)\). Then, we have

\[\sum_{i=1}^{k}\mathbb{E}\left[\phi_{z,i}(X)a(X)\right]^{2} \leq\alpha^{2}\sum_{i=1}^{k}\mathbb{E}\left[\psi_{z,i}(X)a(X) \right]^{2}=\alpha^{2}\sum_{i=1}^{k}\left\langle a,\psi_{z,i}\right\rangle^{2} =\alpha^{2}\sum_{i=1}^{k}\left\langle a-\mathbb{E}[a],\psi_{z,i}\right\rangle^ {2}\] \[\leq\alpha^{2}\sum_{\psi\in\mathcal{B}}\left\langle a-\mathbb{E}[ a],\psi\right\rangle^{2}=\alpha^{2}\operatorname{Var}[a(X)],\]

where for the second identity we used the assumption that \(\left\langle\mathbb{E}[a],\psi_{z,i}\right\rangle=0\) for all \(i\in[k]\) (since \(1\) and \(\psi_{z,i}\) are orthogonal). This establishes Eq. (4).

Turning to Eq. (5), suppose that Assumption 3 holds. Fix \(z\in\mathcal{Z}\), and consider any \(W\in\mathcal{W}\) and \(y\in\mathcal{Y}\). Upon applying Lemma 4 of the Supplement (See Supplement (Appendix B) for the precise statement and proof) to the \(\sigma^{2}\)-subgaussian random vector \(\phi_{z}(X)\) and with \(a(x)\) set to \(W(y\mid x)\in[0,1]\), we get that

\[\sum_{i=1}^{k}\mathbb{E}_{\mathbf{p}_{z}}\left[\phi_{z,i}(X)W(y \mid X)\right]^{2} =\left\lVert\mathbb{E}_{\mathbf{p}_{z}}[\phi_{z}(X)W(y\mid X)] \right\rVert_{2}^{2}\] \[\leq 2\sigma^{2}\mathbb{E}_{\mathbf{p}_{z}}[W(y\mid X)]\cdot \mathbb{E}_{\mathbf{p}_{z}}\bigg{[}W(y\mid X)\log\frac{W(y\mid X)}{\mathbb{E} _{\mathbf{p}_{z}}[W(y\mid X)]}\bigg{]}\]

Integrating over \(y\in\mathcal{Y}\), this gives

\[\int_{\mathcal{Y}}\frac{\sum_{i=1}^{k}\mathbb{E}_{\mathbf{p}_{z }}[\phi_{z,i}(X)W(y\mid X)]^{2}}{\mathbb{E}_{\mathbf{p}_{z}}[W(y\mid X)]}\, \mathrm{d}\mu \leq 2\sigma^{2}\cdot\int_{\mathcal{Y}}\mathbb{E}_{\mathbf{p}_{z} }\bigg{[}W(y\mid X)\log\frac{W(y\mid X)}{\mathbb{E}_{\mathbf{p}_{z}}[W(y\mid X )]}\bigg{]}\,\mathrm{d}\mu\] \[=2\sigma^{2}I(\mathbf{p}_{z};W),\]

which yields the claimed bound.

### Proof of Corollary 1

For any \(W\in\mathcal{W}^{\mathrm{priv},\varepsilon}\), the \(\varepsilon\)-LDP condition from Eq. (2) can be seen to imply that, for every \(y\in\mathcal{Y}\),

\[W(y\mid x_{1})-W(y\mid x_{2})\leq(e^{\varepsilon}-1)W(y\mid x_{3}),\qquad \forall x_{1},x_{2},x_{3}\in\mathcal{X}\,.\]

By taking expectation over \(x_{3}\) then again either over \(x_{1}\) or \(x_{2}\) (all distributed according to \(\mathbf{p}_{z}\)), this yields

\[\left\lvert W(y\mid x)-\mathbb{E}_{\mathbf{p}_{z}}[W(y\mid X)]\right\rvert \leq(e^{\varepsilon}-1)\mathbb{E}_{\mathbf{p}_{z}}[W(y\mid X)],\qquad\forall x \in\mathcal{X}\,.\]

Squaring and taking the expectation on both sides, we obtain

\[\operatorname{Var}_{\mathbf{p}_{z}}[W(y\mid X)]\leq(e^{\varepsilon}-1)^{2} \,\mathbb{E}_{\mathbf{p}_{z}}[W(y\mid X)]^{2}.\]

Dividing by \(\mathbb{E}_{\mathbf{p}_{z}}[W(y\mid X)]\), summing over \(y\in\mathcal{Y}\), and using \(\int_{\mathcal{Y}}\mathbb{E}_{\mathbf{p}_{z}}[W(y\mid X)]\,\mathrm{d}\mu=1\) gives

\[\int_{\mathcal{Y}}\frac{\operatorname{Var}_{\mathbf{p}_{z}}[W(y\mid X)]}{ \mathbb{E}_{\mathbf{p}_{z}}[W(y\mid X)]}\,\mathrm{d}\mu\leq(e^{\varepsilon}-1 )^{2}\int_{\mathcal{Y}}\mathbb{E}_{\mathbf{p}_{z}}[W(y\mid X)]\,\mathrm{d}\mu=(e ^{\varepsilon}-1)^{2},\]

thus establishing (6). For the bound of \(e^{\varepsilon}\), observe that, for all \(y\in\mathcal{Y}\),

\[\operatorname{Var}_{\mathbf{p}_{z}}[W(y\mid X)]\leq\mathbb{E}_{\mathbf{p}_{z}} \big{[}W(y\mid X)^{2}\big{]}\leq e^{\varepsilon}\min_{x\in\mathcal{X}}W(y\mid x )\mathbb{E}_{\mathbf{p}_{z}}[W(y\mid X)].\]Hence

\[\int_{\mathcal{Y}}\frac{\operatorname{Var}_{\mathbf{p}_{z}}[W(y\mid X)]}{\mathbb{ E}_{\mathbf{p}_{z}}[W(y\mid X)]}\,\mathrm{d}\mu\leq e^{\varepsilon}\int_{ \mathcal{Y}}\min_{x\in\mathcal{X}}W(y\mid x)\,\mathrm{d}\mu\leq e^{\varepsilon} \cdot\min_{x\in\mathcal{X}}\int_{\mathcal{Y}}W(y\mid x)\,\mathrm{d}\mu=e^{ \varepsilon}.\]

The bound (7) (under Assumption 3) will follow from (5), and the relation between differential privacy and KL divergence. Indeed, the mutual information \(I(\mathbf{p}_{z};W)\) can be rewritten as the expected (over \(X\sim\mathbf{p}_{Z}\)) KL divergence between the distribution \(\mathbf{p}^{W}_{Z}:=W(\cdot\mid X)\) over \(\mathcal{Y}\) induced by the channel \(W\) on input \(X\), and the distribution \(\mathbf{p}_{Z}^{W}:=\mathbb{E}_{X^{\prime}\sim\mathbf{p}_{z}}[W(\cdot\mid X^{ \prime})]\) over \(\mathcal{Y}\) induced by the input distribution \(\mathbf{p}_{z}\) and the channel \(W\):

\[I(\mathbf{p}_{z};W)=\mathbb{E}_{X\sim\mathbf{p}_{z}}\big{[}\mathrm{D}\big{(} \mathbf{p}^{W,X}\|\mathbf{p}_{z}^{W}\big{)}\big{]}=\mathbb{E}_{X\sim\mathbf{ p}_{z}}\bigg{[}\mathbb{E}_{Y\sim\mathbf{p}^{W,X}}\bigg{[}\ln\frac{W(Y\mid X)}{ \mathbb{E}_{X^{\prime}\sim\mathbf{p}_{z}}[W(Y\mid X^{\prime})]}\bigg{]}\bigg{]} \bigg{]}\,;\]

but the \(\varepsilon\)-LDP condition from Eq. (2) guarantees that the log-likelihood ratio in the inner expectation is (almost surely) at most \(\varepsilon\), so that \(I(\mathbf{p}_{z};W)\leq\varepsilon\) for every \(z\) and \(W\in\mathcal{W}^{\mathrm{priv},\varepsilon}\). This yields (7).

### Proof of Corollary 2

In view of (4), to establish (8), it suffices to show that \(\frac{\operatorname{Var}_{\mathbf{p}_{z}}[W(y\mid X)]}{\mathbb{E}_{\mathbf{p} _{z}}[W(y\mid X)]}\leq 1\) for every \(y\in\mathcal{Y}\). Since \(W(y\mid x)\in(0,1]\) for all \(x\in\mathcal{X}\) and \(y\in\mathcal{Y}\), so that

\[\operatorname{Var}_{\mathbf{p}_{z}}[W(y\mid X)]\leq\mathbb{E}_{\mathbf{p}_{z} }\big{[}W(y\mid X)^{2}\big{]}\leq\mathbb{E}_{\mathbf{p}_{z}}[W(y\mid X)].\]

The second bound (under Assumption 3) will follow from (5). Indeed, recalling that the entropy of the output of a channel is bounded below by the mutual information between input and the output, we have \(I(\mathbf{p}_{z};W)\leq H(\mathbf{p}_{z}^{W})\), where \(\mathbf{p}_{z}^{W}:=\mathbb{E}_{\mathbf{p}_{z}}[W(\cdot\mid X)]\) is the distribution over \(\mathcal{Y}\) induced by the input distribution \(\mathbf{p}_{z}\) and the channel \(W\). Using the fact that the entropy of a distribution over \(\mathcal{Y}\) is at most \(\log|\mathcal{Y}|\) in (5) gives (9).

## Appendix F Missing proofs in Section 4

### Proof of Lemma 1

Given an \((n,\gamma)\)-estimator \((\Pi,\hat{\theta})\), define an estimate \(\hat{Z}\) for \(Z\) as

\[\hat{Z}:=\operatorname*{argmin}_{z\in\mathcal{Z}}\Big{\|}\theta_{z}-\hat{ \theta}(Y^{n},U)\Big{\|}_{p}.\]

By the triangle inequality,

\[\big{\|}\theta_{Z}-\theta_{\hat{Z}}\big{\|}_{p}\leq\Big{\|}\theta_{Z}-\hat{ \theta}(Y^{n},U)\Big{\|}_{p}+\Big{\|}\theta_{\hat{Z}}-\hat{\theta}(Y^{n},U) \Big{\|}_{p}\leq 2\Big{\|}\hat{\theta}(Y^{n},U)-\theta_{Z}\Big{\|}_{p}.\]

Since \((\Pi,\hat{\theta})\) is an \((n,\gamma)\)-estimator under \(\ell_{p}\) loss for \(\mathcal{P}_{\Theta}\),

\[\mathbb{E}_{Z}\Big{[}\mathbb{E}_{\mathbf{p}_{Z}}\Big{[}\big{\|} \theta_{Z}-\theta_{\hat{Z}}\big{\|}_{p}^{p}\Big{]}\Big{]} \leq 2^{p}\gamma^{p}\Pr[\,\mathbf{p}_{Z}\in\mathcal{P}_{\Theta}\,]+ \max_{z\neq z^{\prime}}\|\theta_{z}-\theta_{z^{\prime}}\|_{p}^{p}\Pr[\, \mathbf{p}_{Z}\notin\mathcal{P}_{\Theta}\,]\] \[\leq 2^{p}\gamma^{p}+4^{p}\gamma^{p}\frac{1}{\tau}\cdot\frac{\tau}{4}\] (38) \[\leq\frac{3}{4}4^{p}\gamma^{p},\] (39)

where Eq. (38) follows from Assumption 4 and \(\Pr[\,\mathbf{p}_{Z}\in\mathcal{P}_{\Theta}\,]\geq 1-\tau/4\). Next, for \(p\in[1,\infty)\), by Assumption 4, \(\big{\|}\theta_{Z}-\theta_{\hat{Z}}\big{\|}_{p}^{p}\geq\frac{4^{p}\gamma^{p}}{\tau k }\sum_{i=1}^{k}\mathbf{1}\Big{\{}Z_{i}\neq\hat{Z}_{i}\Big{\}}\). Combining with Eq. (39) this shows that \(\frac{1}{\tau k}\sum_{i=1}^{k}\Pr[\,Z_{i}\neq\hat{Z}_{i}\,]\leq\frac{3}{4}\,.\)

Furthermore, since the Markov relation \(Z_{i}-(Y^{n},U)-\hat{Z}_{i}\) holds for all \(i\), we can lower bound \(\Pr\Big{[}\,Z_{i}\neq\hat{Z}_{i}\,\Big{]}\) using the standard relation between total variation distance and hypothesis testing as follows, using that \(\tau\leq 1/2\) in the second inequality:

\[\Pr\!\Big{[}\,Z_{i}\neq\hat{Z}_{i}\,\Big{]} \geq\tau\Pr\!\Big{[}\,\hat{Z}_{i}=-1\,\Big{|}\,\,Z_{i}=1\,\Big{]}+(1 -\tau)\Pr\!\Big{[}\,\hat{Z}_{i}=1\,\Big{|}\,\,Z_{i}=-1\,\Big{]}\] \[\geq\tau\Big{(}\Pr\!\Big{[}\,\hat{Z}_{i}=-1\,\Big{|}\,\,Z_{i}=1\, \Big{]}+\Pr\!\Big{[}\,\hat{Z}_{i}=1\,\Big{|}\,\,Z_{i}=-1\,\Big{]}\Big{)}\] \[\geq\tau\Big{(}1-\mathrm{d}_{\mathrm{TV}}\Big{(}\mathbf{p}_{+i}^ {Y^{n}},\mathbf{p}_{-i}^{Y^{n}}\Big{)}\Big{)}\,.\]

Summing over \(1\leq i\leq k\) and combining it with the previous bound, we obtain

\[\frac{3}{4}\geq\frac{1}{\tau k}\sum_{i=1}^{k}\Pr\!\Big{[}\,Z_{i}\neq\hat{Z}_{i }\,\Big{]}\geq 1-\frac{1}{k}\sum_{i=1}^{k}\mathrm{d}_{\mathrm{TV}}\Big{(} \mathbf{p}_{+i}^{Y^{n}},\mathbf{p}_{-i}^{Y^{n}}\Big{)}\]

and reorganizing proves the result.

## Appendix G Missing statements and proofs in Section 5

### Proof of Theorem 3

Fix \(p\in[1,\infty)\). Let \(k=d\), \(\mathcal{Z}=\{-1,+1\}^{d}\), and \(\tau=\frac{s}{2d}\); and suppose that, for some \(\gamma\in(0,1/8]\), there exists an \((n,\gamma)\)-estimator for \(\mathcal{B}_{d,s}\) under \(\ell_{p}\) loss. We fix a parameter \(\gamma\in(0,1/2]\), which will be chosen as a function of \(\gamma,d,p\) later. Consider the set of \(2^{d}\) product Bernoulli distributions \(\{\mathbf{p}_{z}\}_{z\in\mathcal{Z}}\), where \(\mu(\mathbf{p}_{z})=\mu_{z}:=\frac{1}{2}\gamma(z+\mathbf{1}_{d})\) (so the sparsity of the mean vector is equal to the number of positive coordinates of \(z\)). We have, for \(z\in\mathcal{Z}\),

\[\mathbf{p}_{z}(x)=\frac{1}{2^{d}}\prod_{i=1}^{d}\biggl{(}1+\frac{1}{2}\gamma( z_{i}+1)x_{i}\biggr{)},\qquad x\in\mathcal{X}.\]

It follows for \(z\in\mathcal{Z}\) and \(i\in[d]\) that

\[\mathbf{p}_{z^{\oplus}}(x)=\frac{1+\frac{1}{2}\gamma(1-z_{i})x_{i}}{1+\frac{1 }{2}\gamma(1+z_{i})x_{i}}\mathbf{p}_{z}(x)=\biggl{(}1-\gamma\frac{z_{i}x_{i}}{ 1+\frac{1}{2}\gamma(1+z_{i})x_{i}}\biggr{)}\mathbf{p}_{z}(x)=(1+\phi_{z,i}(x)) \mathbf{p}_{z}(x)\] (40)

where \(\phi_{z,i}(x):=-\frac{\gamma z_{i}x_{i}}{1+\frac{1}{2}\gamma(1+z_{i})x_{i}}\). We can verify that, for \(i\neq j\),

\[\mathbb{E}_{\mathbf{p}_{z}}[\phi_{z,i}(X)]=0,\quad\mathbb{E}_{\mathbf{p}_{z}} \bigl{[}\phi_{z,i}(X)^{2}\bigr{]}=\frac{\gamma^{2}}{1-\frac{1}{2}\gamma^{2}(1 +z_{i})},\text{ and }\mathbb{E}_{\mathbf{p}_{z}}[\phi_{z,i}(X)\phi_{z,j}(X)]=0,\]

so that Assumptions 1 and 2 are satisfied for \(\alpha^{2}:=2\gamma^{2}\). Moreover, using, _e.g._, Hoeffding's lemma (_cf_. [9]), for \(\gamma<1\), the random vector \(\phi_{z}(X)=(\phi_{z,i}(X))_{i\in[d]}\) is \(\frac{\gamma^{2}}{(1-\gamma^{2})^{2}}\)-subgaussian. Thus, Assumption 3 holds as well, and we can invoke both parts of Theorem 2.

Let \(\left\|z\right\|_{+}:=\left|\{i\in[d]\,\left|\,\,z_{i}=1\}\right|\right|\), so that \(\left\|\mu_{z}\right\|_{0}=\sum_{i=1}^{d}\frac{1}{2}(1+z_{i})=\left\|z\right\|_ {+}\). The next claim, which follows from standard bounds for binomial random variables, states that when \(Z\sim\mathrm{Rad}(\tau)^{\otimes d}\), \(\mu_{Z}\) is \(s\)-sparse with high probability.

**Fact 2**.: _Let \(Z\sim\mathrm{Rad}(\tau)^{\otimes d}\), where \(\tau d\geq 4\log d\). Then \(\Pr\!\left[\left\|Z\right\|_{+}\leq 2\tau d\,\right]\geq 1-\tau/4\)._

Hence the construction satisfies \(\Pr_{Z}\!\big{[}\,\mathbf{p}_{Z}\in\mathcal{B}_{d,s}\,\big{]}\leq 1-\tau/4\), as required in Lemma 1.

We now choose \(\gamma=\gamma(p):=\frac{4\gamma}{(s/2)^{1/p}}\in(0,1/2]\), which implies that Assumption 4 holds since

\[\ell_{p}(\mu(\mathbf{p}_{z}),\mu(\mathbf{p}_{z^{\prime}}))=\gamma\,\mathrm{d }_{\mathrm{Ham}}(z,z^{\prime})^{1/p}=4\gamma\biggl{(}\frac{\mathrm{d}_{\mathrm{ Ham}}(z,z^{\prime})}{\tau d}\biggr{)}^{1/p}.\]

Therefore, we can apply Lemma 1 as well. For \(\mathcal{W}^{\mathrm{priv},\varepsilon}\), we prove the two parts of the lower bound separately, depending on whether \(\varepsilon\leq 1\). First, upon combining the bounds obtained by Corollary 1 and Lemma 1 (specifically, for the former, (6)), we get

\[d\leq 112n\alpha^{2}(e^{\varepsilon}-1)^{2},\]whereby, upon recalling that \(\alpha^{2}=2\gamma^{2}\), and using the value of \(\gamma=\gamma(p)\) above, it follows that

\[\frac{1}{3584}\cdot\frac{d(s/2)^{\frac{2}{p}}}{n(e^{\varepsilon}-1)^{2}}\leq \gamma^{2}.\]

Thus, \(\mathcal{E}_{p}(\mathcal{B}_{d,s},\mathcal{W}^{\mathrm{priv},\varepsilon},n)= \Omega\bigg{(}\sqrt{\frac{ds^{2/p}}{n\varepsilon^{2}}}\bigg{)}\) for \(\varepsilon\in(0,1]\). For the second part of the bound, which dominates for \(\varepsilon>1\), observe that Assumption 3 holds with \(\sigma^{2}:=\frac{\gamma^{2}}{(1-\gamma^{2})^{2}}\leq 2\gamma^{2}\); allowing us to apply the second part of Corollary 1, (7), which as before combined with Lemma 1 yields

\[d\leq 224n\sigma^{2}\varepsilon\leq 448n\gamma^{2}\varepsilon,\]

and again from the setting of \(\gamma\) we get \(\mathcal{E}_{p}(\mathcal{B}_{d,s},\mathcal{W}^{\mathrm{priv},\varepsilon},n)= \Omega\bigg{(}\sqrt{\frac{ds^{2/p}}{n\varepsilon}}\bigg{)}\).

Similarly, for \(\mathcal{W}^{\mathrm{comm},\ell}\), again since Assumption 3 holds with \(\sigma^{2}\leq 2\gamma^{2}\), upon combining the bounds obtained by Corollary 2 and Lemma 1, we get

\[\frac{ds^{\frac{2}{p}}}{28672n\ell}\leq\gamma^{2},\]

which gives \(\mathcal{E}_{p}(\mathcal{B}_{d,s},\mathcal{W}^{\mathrm{comm},\ell},n)=\Omega \big{(}\sqrt{\frac{ds^{2/p}}{n\ell}}\wedge 1\big{)}\). Finally, note that for \(\ell\geq d\), the lower bound follows from the minimax rate in the unconstrained setting, which can be seen to be \(\Omega\big{(}\sqrt{s^{2/p}\log(2d/s)/n}\big{)}\)[28, 30]. This completes the proof.

This handles the case \(p\in[1,\infty)\). For \(p=\infty\), the lower bounds immediately follow from plugging \(p=\log s\) in the previous expressions, as discussed in Footnote 3.

### Proof of Theorem 4

We denote the mean by \(\mu\) instead of \(\theta\), denote the estimator by \(\hat{\mu}\), and consider the minimax error rate \(\mathcal{E}_{p}(\mathcal{G}_{d,s},\mathcal{W},n)\) of mean estimation for \(\mathcal{P}_{\Theta}=\mathcal{G}_{d,s}\) using \(\mathcal{W}\) under \(\ell_{p}\) loss.

Proof of Theorem 4.: Let \(\varphi\) denote the probability density function of the standard Gaussian distribution \(\mathcal{G}(\mathbf{0},\mathbb{I})\). Fix \(p\in[1,\infty)\). Let \(k=d\), \(\mathcal{Z}=\{-1,+1\}^{d}\), and \(\tau=\frac{s}{2d}\); and suppose that, for some \(\gamma\in(0,1/8]\), there exists an \((n,\gamma)\)-estimator for \(\mathcal{G}_{d,s}\) under \(\ell_{p}\) loss. We fix a parameter \(\gamma:=\gamma(p):=\frac{4\gamma}{(s/2)^{1/p}}\in(0,1/2]\), and consider the set of distributions \(\{\mathbf{p}_{z}\}_{z\in\mathcal{Z}}\) of all \(2^{d}\) spherical Gaussian distributions with mean \(\mu_{z}:=\gamma(z+\mathbf{1}_{d})\), where \(z\in\mathcal{Z}\). Again, note that \(\left\|\mu_{z}\right\|_{0}=\sum_{i=1}^{d}\mathds{1}\{z_{i}=1\}=\left\|z\right\|_ {+}\), and Fact 2 applies here too. Then by the definition of Gaussian density, for \(z\in\mathcal{Z}\),

\[\mathbf{p}_{z}(x)=e^{-\gamma^{2}\left\|\mu_{z}\right\|_{2}^{2}/2}\cdot e^{ \gamma\left\langle x,z+\mathbf{1}_{d}\right\rangle}\cdot\varphi(x).\] (41)

Therefore, for \(z\in\mathcal{Z}\) and \(i\in[d]\), we have

\[\mathbf{p}_{z^{\oplus i}}(x)=e^{-2\gamma x_{1}z_{i}}e^{2\gamma^{2}z_{i}}\cdot \mathbf{p}_{z}(x)=(1+\phi_{z,i}(x))\cdot\mathbf{p}_{z}(x),\] (42)

where \(\phi_{z,i}(x):=1-e^{-2\gamma x_{i}z_{i}}e^{2\gamma^{2}z_{i}}\). By using the Gaussian moment-generating function, for \(i\neq j\),

\[\mathbb{E}_{\mathbf{p}_{z}}[\phi_{z,i}(X)]=0,\quad\mathbb{E}_{\mathbf{p}_{z} }\big{[}\phi_{z,i}(X)^{2}\big{]}=e^{4\gamma^{2}}-1,\text{ and }\mathbb{E}_{\mathbf{p}_{z}}[\phi_{z,i}(X)\phi_{z,j}(X)]=0,\]

so that Assumptions 1 and 2 are satisfied for \(\alpha^{2}:=e^{4\gamma^{2}}-1\). By our choice of \(\gamma\) and the assumption on \(\gamma\), one can check that Assumption 4 holds:

\[\ell_{p}(\mu(\mathbf{p}_{z}),\mu(\mathbf{p}_{z^{\prime}}))=4\gamma\bigg{(} \frac{\mathrm{d}_{\mathrm{Ham}}(z,z^{\prime})}{\tau d}\bigg{)}^{1/p}.\]

Moreover, similar to the product of Bernoulli case, using Fact 2, we can show that \(\Pr_{Z}[\,\mathbf{p}_{Z}\in\mathcal{G}_{d,s}\,]\leq 1-\tau/4\). This allows us to apply Lemma 1.

#### g.2.1 Privacy constraints for \(\varepsilon\in(0,1)\)

For \(\mathcal{W}^{\mathrm{priv},\varepsilon}\), upon combining the bounds obtained by Corollary 1 and Lemma 1, we get

\[d\leq 112n\alpha^{2}(e^{\varepsilon}-1)^{2},\]

whereby, upon noting that \(\alpha^{2}=e^{4\gamma^{2}}-1\leq 8\gamma^{2}\) holds since \(\gamma\leq 1/2\), and using the value of \(\gamma=\gamma(p)\) above, it follows that

\[\gamma^{2}\geq\frac{d(s/2)^{\frac{2}{p}}}{14336\cdot n(e^{\varepsilon}-1)^{2}}.\]

Thus, \(\mathcal{E}_{p}(\mathcal{G}_{d,s},\mathcal{W}^{\mathrm{priv},\varepsilon},n)= \Omega\bigg{(}\sqrt{\frac{ds^{2/p}}{ne^{2}}}\wedge 1\bigg{)}\). This establishes the lower bounds for \(\mathcal{W}^{\mathrm{priv},\varepsilon}\). (Recall that the bound for \(p=\infty\) then follows from setting \(p=\log d\).)

#### g.2.2 Communication constraints, and privacy constraints for \(\varepsilon\geq 1\)

For these cases, to prove a lower bound with the desired dependence on \(\varepsilon\) or \(\ell\), we will need to use the tighter bounds in Corollaries 1 and 2 which hold only under Assumption 3. This, however, leads to an issue: the random vector \(\phi_{z}(X)=(\phi_{z,i}(X))_{i\in[d]}\) is not subgaussian, due to the one-sided exponential growth, and therefore Assumption 3 does not hold.

To overcome this and still obtain a linear dependence on \(\ell\) (or \(\varepsilon\)) (instead of the suboptimal \(2^{\ell}\) (or \(e^{\varepsilon}\))), we will consider instead the class of "truncated" Gaussian distributions, whose corresponding \(\phi\) functions are subgaussian; and argue that these truncated distributions are close enough to the original Gaussian distributions such a lower bound in the truncated case implies one in the original Gaussian case.

In particular, we consider the following collection of truncated Gaussian distributions. For \(z\in\mathcal{Z}\), let \(\mathbf{p}_{z}\) be the density function of a spherical Gaussian distribution with mean \(\mu_{z}\) as defined in Eq. (41). For a truncation bound \(B\), let \(\mathbf{p}_{z,B}\) be the distribution of \(X\sim\mathbf{p}_{z}\) conditioned on the event that \(\left\|X\right\|_{\infty}\leq B\). That is, we have, for \(x\in\mathbb{R}^{d}\),

\[\mathbf{p}_{z,B}(x)=C_{z}\mathbf{p}_{z}(x)\mathds{1}\{\left\|X\right\|_{ \infty}\leq B\},\]

where \(C_{z}=1/\Pr_{X\sim\mathbf{p}_{z}}\left[\left\|X\right\|_{\infty}\leq B\right.]\). Then the following bound follows from standard Gaussian concentration bound on each dimension and a union bound over all dimensions.

**Fact 3**.: _Setting \(B:=4\sqrt{\ln(dn)}\), we have, for every \(z\in\mathcal{Z}\), \(\mathrm{d}_{\mathrm{TV}}(\mathbf{p}_{z,B},\mathbf{p}_{z})\leq\frac{1}{d^{ \prime}n^{8}}\)._

Let \(\mathbf{p}_{z,B}^{Y^{n}}\) be the distribution of the messages obtained by executing the protocol when each user gets a sample from \(\mathbf{p}_{z,B}\) and let the corresponding mixtures be denoted by \(\mathbf{p}_{+i,B}^{Y^{n}}\) and \(\mathbf{p}_{-i,B}^{Y^{n}}\). Then we have

\[\mathrm{d}_{\mathrm{TV}}\Big{(}\mathbf{p}_{+i}^{Y^{n}},\mathbf{p }_{-i}^{Y^{n}}\Big{)} \leq\mathrm{d}_{\mathrm{TV}}\Big{(}\mathbf{p}_{+i,B}^{Y^{n}}, \mathbf{p}_{-i,B}^{Y^{n}}\Big{)}+\mathrm{d}_{\mathrm{TV}}\Big{(}\mathbf{p}_{+ i}^{Y^{n}},\mathbf{p}_{+i,B}^{Y^{n}}\Big{)}+\mathrm{d}_{\mathrm{TV}}\Big{(} \mathbf{p}_{-i,B}^{Y^{n}},\mathbf{p}_{-i}^{Y^{n}}\Big{)}\] \[\leq\mathrm{d}_{\mathrm{TV}}\Big{(}\mathbf{p}_{+i,B}^{Y^{n}}, \mathbf{p}_{-i,B}^{Y^{n}}\Big{)}+\max_{z}\Big{\{}\mathrm{d}_{\mathrm{TV}} \Big{(}\mathbf{p}_{z}^{Y^{n}},\mathbf{p}_{z,B}^{Y^{n}}\Big{)}+\mathrm{d}_{ \mathrm{TV}}\Big{(}\mathbf{p}_{z,B}^{Y^{n}},\mathbf{p}_{z}^{Y^{n}}\Big{)}\Big{\}}\] \[\leq\mathrm{d}_{\mathrm{TV}}\Big{(}\mathbf{p}_{+i,B}^{Y^{n}}, \mathbf{p}_{-i,B}^{Y^{n}}\Big{)}+2n\max_{z}\mathrm{d}_{\mathrm{TV}}(\mathbf{p}_ {z,B},\mathbf{p}_{z})\] \[\leq\mathrm{d}_{\mathrm{TV}}\Big{(}\mathbf{p}_{+i,B}^{Y^{n}}, \mathbf{p}_{-i,B}^{Y^{n}}\Big{)}+\frac{2}{d^{\prime}n^{7}}.\]

The third inequality follows from data processing inequality and the fourth inequality follows from subadditivity of TV distance.

Combining this with Lemma 1, for any protocol that correctly learns the Gaussian family, we must have

\[\frac{1}{d}\sum_{i=1}^{d}\mathrm{d}_{\mathrm{TV}}\Big{(}\mathbf{p}_{+i,B}^{Y^{n} },\mathbf{p}_{-i,B}^{Y^{n}}\Big{)}\geq\frac{1}{8}.\] (43)Next we show that the \(\phi\) functions corresponding to \(\mathbf{p}_{z,B}\)'s are subgaussian and establish the corresponding upper bounds on the average information bound above. Note that

\[\phi_{z,i}^{B}(x):=\frac{\mathbf{p}_{z^{\oplus i}}^{B}(x)}{\mathbf{ p}_{z}^{B}(x)}-1=\frac{C_{z^{\oplus i}}}{C_{z}}e^{-2\gamma x_{i}z_{i}}e^{2\gamma^{2}z_ {i}}\mathds{1}\{\left\lVert x\right\rVert_{\infty}\leq B\}-1\] (44)

By the inequality \(|ab-1|\leq|a|\cdot|b-1|+|a-1|\), we have have, for all \(z\in\mathcal{Z}\),

\[\left|\frac{C_{z^{\oplus i}}}{C_{z}}-1\right| \leq\frac{1}{C_{z}}|C_{z^{\oplus i}}-1|+\left|\frac{1}{C_{z}}-1 \right|\leq\left|\frac{1}{\Pr_{X\sim\mathbf{p}_{z}\oplus i}\left\lVert X \right\rVert_{\infty}\leq B}-1\right|+\left|\Pr_{X\sim\mathbf{p}_{z}}\left[ \left\lVert X\right\rVert_{\infty}\leq B\right.\right]-1\] \[\leq\frac{10}{d^{7}n^{7}}.\]

Moreover, for all \(z\in\mathcal{Z}\), for \(\gamma\leq\frac{1}{3B}\),

\[\left|e^{-2\gamma x_{i}z_{i}}e^{2\gamma^{2}z_{i}}\mathds{1}\{ \left\lVert x\right\rVert_{\infty}\leq B\}-1\right|\leq\left|e^{2\gamma^{2}+2 \gamma B}-1\right|\leq\left|e^{3\gamma B}-1\right|\leq 6\gamma B.\] (45)

Hence, applying the inequality \(|ab-1|\leq|a|\cdot|b-1|+|a-1|\) again on Eq. (44), we have for \(\gamma\leq\frac{1}{3B}\),

\[|\phi_{z,i}^{B}(x)|\leq 12\gamma B+\frac{10}{d^{7}n^{7}}.\]

Thus, we get that for all \(z\in\mathcal{Z},i\in[d]\), \(\phi_{z,i}^{B}\) is subgaussian with proxy \(\sigma_{B}=12\gamma B+\frac{10}{d^{7}n^{7}}\).

Under communication constraints, applying Corollary 2, we get

\[\left(\frac{1}{d}\sum_{i=1}^{d}\mathrm{d}_{\mathrm{TV}}\Big{(} \mathbf{p}_{+i,B}^{\gamma^{n}},\mathbf{p}_{-i,B}^{\gamma^{n}}\Big{)}\right)^{2 }\leq\frac{14}{d}\sigma_{B}^{2}n\ell.\]

To conclude, we observe that by plugging our setting of \(\gamma=\gamma(p)\) in the above inequality, we must have

\[\gamma^{2}\geq\frac{d(s/2)^{\frac{2}{p}}}{14336\cdot n\cdot B^{2}\ell}\]

in order to satisfy Eq. (43), hence proving the desired lower bound. The lower bound for LDP with \(\varepsilon>1\) follows similarly by applying Corollary 1. 

### Detailed results for discrete family

We derive a lower bound for \(\mathcal{E}_{p}(\Delta_{d},\mathcal{W},n)\), the minimax rate for discrete density estimation, under local privacy and communication constraints.

**Theorem 6**.: _Fix \(p\in[1,\infty)\). For \(\varepsilon>0\), and \(\ell\geq 1\), we have_

\[\mathcal{E}_{p}(\Delta_{d},\mathcal{W}^{\mathrm{priv},\varepsilon},n)\gtrsim \sqrt{\frac{d^{2/p}}{n((e^{\varepsilon}-1)^{2}\wedge e^{\varepsilon})}\wedge \left(\frac{1}{n((e^{\varepsilon}-1)^{2}\wedge e^{\varepsilon})}\right)^{\frac {p-1}{p}}}\wedge 1\] (46)

_and_

\[\mathcal{E}_{p}(\Delta_{d},\mathcal{W}^{\mathrm{comm},\ell},n)\gtrsim\sqrt{ \frac{d^{2/p}}{n2^{\ell}}\wedge\left(\frac{1}{n2^{\ell}}\right)^{\frac{p-1}{p} }}\wedge 1\,.\] (47)

In particular, for \(n\big{(}(e^{\varepsilon}-1)^{2}\wedge e^{\varepsilon}\big{)}\geq d^{2}\) and \(n(2^{\ell}\wedge d)\geq d^{2}\), the first term of the corresponding lower bounds dominates. Before turning to the proof of this theorem, we note that Corollary 3 and Corollary 4 are direct corollaries of the theorem.

We now establish Theorem 6.

Proof of Theorem 6.: Fix \(p\in[1,\infty)\), and suppose that, for some \(\gamma\in(0,1/16]\), there exists an \((n,\gamma)\)-estimator for \(\Delta_{d}\) under \(\ell_{p}\) loss. Set

\[D:=d\wedge\left\lceil\left(\frac{1}{16\gamma}\right)^{\frac{p}{p-1}}\right\rceil\]and assume, without loss of generality, that \(D\) is even. By definition, we then have \(\gamma\in(0,1/(16D^{1-1/p})]\) and \(D\leq d\); we can therefore restrict ourselves to the first \(D\) elements of the domain, embedding \(\Delta_{D}\) into \(\Delta_{d}\), to prove our lower bound.

Let \(k=\frac{D}{2}\), \(\mathcal{Z}=\{-1,+1\}^{D/2}\), and \(\tau=\frac{1}{2}\); and suppose that, for some \(\gamma\in(0,1/(16D^{1-1/p})]\), there exists an \((n,\gamma)\)-estimator for \(\Delta_{D}\) under \(\ell_{p}\) loss. (We will use the fact that \(\gamma\leq 1/(16D^{1-1/p})\) for Eq. (49) to be a valid distribution with positive mass, as we will need \(|\gamma|\leq\frac{1}{D}\); and to bound \(\alpha^{2}\) later on, as we will require \(|\gamma|\leq\frac{1}{2D}\).) Define \(\gamma=\gamma(p)\) as

\[\gamma(p):=\frac{4\cdot 2^{1/p}\gamma}{D^{1/p}},\] (48)

which implies \(\gamma\in[0,1/(2D)]\). Consider the set of \(D\)-ary distributions \(\mathcal{P}^{\gamma}_{\mathrm{Discrete}}=\{\mathbf{p}_{z}\}_{z\in\mathcal{Z}}\) defined as follows. For \(z\in\mathcal{Z}\), and \(x\in\mathcal{X}=[D]\)

\[\mathbf{p}_{z}(x)=\left\{\begin{array}{ll}\frac{1}{D}+\gamma z_{i},&\text{ if }x=2i,\\ \frac{1}{D}-\gamma z_{i},&\text{ if }x=2i-1.\end{array}\right.\] (49)

For \(z\in\mathcal{Z}\) and \(i\in[D/2]\), we have

\[\mathbf{p}_{z^{\oplus}:}(x) =\bigg{(}1-\frac{2D\gamma z_{i}}{1+D\gamma z_{i}}\mathds{1}\{x=2 i\}+\frac{2D\gamma z_{i}}{1-D\gamma z_{i}}\mathds{1}\{x=2i-1\}\bigg{)}\mathbf{p}_{z}(x)\] \[=(1+\phi_{z,i}(x))\mathbf{p}_{z}(x),\] (50)

where

\[\phi_{z,i}(x):=z_{i}\cdot\frac{2D\gamma}{1-D^{2}\gamma^{2}}((1+D\gamma z_{i}) \mathds{1}\{x=2i-1\}-(1-D\gamma z_{i})\mathds{1}\{x=2i\}).\]

Once again, we can verify that for \(i\neq j\)

\[\mathbb{E}_{\mathbf{p}_{z}}[\phi_{z,i}(X)]=0,\quad\mathbb{E}_{\mathbf{p}_{z}} \big{[}\phi_{z,i}(X)^{2}\big{]}=\frac{8\gamma^{2}D}{1-\gamma^{2}D^{2}},\text{ and }\mathbb{E}_{\mathbf{p}_{z}}[\phi_{z,i}(X)\phi_{z,j}(X)]=0,\]

so that Assumptions 1 and 2 are satisfied for \(\alpha^{2}:=16\gamma^{2}D\) (using that \(D\gamma\leq 1/2\) to simplify the bound).10 Thus, we can invoke the first part of Theorem 2. Note that Assumption 4 holds, since \(\ell_{p}(\mathbf{p}_{z},\mathbf{p}_{z^{\prime}})=\gamma\,\mathrm{d}_{\mathrm{ Ham}}(z,z^{\prime})^{1/p}=4\gamma\bigg{(}\frac{\mathrm{d}_{\mathrm{Ham}}\big{(}z,z^{ \prime}\big{)}}{\tau D}\bigg{)}^{1/p}\). Therefore, we can apply Lemma 1 as well.

Footnote 10: It is worth noting that Assumption 3 will not hold for any useful choice of the subgaussianity parameter.

For \(\mathcal{W}^{\mathrm{priv},\varepsilon}\), by combining the bounds obtained by Corollary 1 and Lemma 1, we get

\[D\leq 56n\alpha^{2}\big{(}(e^{\varepsilon}-1)^{2}\wedge e^{\varepsilon}\big{)},\]

whereby, upon recalling the value of \(\alpha^{2}\) and using the setting of \(\gamma=\gamma(p)\) from Eq. (48), it follows that

\[\gamma^{2}\geq\frac{D^{\frac{2}{p}}}{7168\cdot 2^{2/p}\cdot n((e^{\varepsilon}-1)^ {2}\wedge e^{\varepsilon})}\asymp\frac{d^{2/p}\wedge\gamma^{-2/(p-1)}}{n((e^{ \varepsilon}-1)^{2}\wedge e^{\varepsilon})}.\]

Thus we obtain the bound Eq. (46) as claimed.

Similarly, for \(\mathcal{W}^{\mathrm{comm},\ell}\), upon combining the bounds obtained by Corollary 2 and Lemma 1 and recalling that \(|\mathcal{Y}|=2^{\ell}\), we get

\[\gamma^{2}\geq\frac{D^{\frac{2}{p}}}{7168\cdot 2^{2/p}\cdot n2^{\ell}},\]

which gives \(\mathcal{E}_{p}(\Delta_{D},\mathcal{W}^{\mathrm{comm},\ell},n)=\Omega\bigg{(} \sqrt{\frac{d^{2/p}}{n2^{2}}\wedge\big{(}\frac{1}{n2^{2}}\big{)}^{\frac{p-1}{p }}}\bigg{)}\),11 concluding the proof. 

Footnote 11: Finally, note that we could replace the quantity \(2^{\ell}\) above by \(2^{\ell}\wedge d\), or even \(2^{\ell}\wedge D\), as for \(2^{\ell}\geq D\) there is no additional information any player can send beyond the first \(\log_{2}D\) bits, which encode their full observation. However, this small improvement would lead to more cumbersome expressions, and not make any difference for the main case of interest, \(p=1\).