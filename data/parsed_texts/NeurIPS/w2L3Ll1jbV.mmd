# Adversarially Robust Multi-task Representation Learning

 Austin Watkins

Johns Hopkins University

Baltimore, MD 21218

awatki29@jhu.edu

&Thanh Nguyen-Tang

Johns Hopkins University

Baltimore, MD 21218

nguyent@cs.jhu.edu

&Enayat Ullah

Meta\({}^{*}\)

enayat@meta.com

&Raman Arora

Johns Hopkins University

Baltimore, MD 21218

arora@cs.jhu.edu

Work done while the author was at the Johns Hopkins University.

###### Abstract

We study adversarially robust transfer learning, wherein, given labeled data on multiple (source) tasks, the goal is to train a model with small robust error on a previously unseen (target) task. In particular, we consider a multi-task representation learning (MTRL) setting, i.e., we assume that the source and target tasks admit a simple (linear) predictor on top of a shared representation (e.g., the final hidden layer of a deep neural network). In this general setting, we provide rates on the excess adversarial (transfer) risk for Lipschitz losses and smooth non-negative losses. These rates show that learning a representation using adversarial training on diverse tasks helps protect against inference-time attacks in data-scarce environments. Additionally, we provide novel rates for the single-task setting.

## 1 Introduction

In many real-world applications, we typically have a scarcity of data for the intended task. Consider, for example, settings where the learner's environment is evolving rapidly or where access to high-quality labeled data is expensive or infeasible due to a lack of expertise or computational limitations. These problems are typically studied under the framework of transfer learning [6; 22; 23; 40; 19]. Such approaches aim to leverage plentiful labeled data from the source domain to learn models that can handle distribution shifts and work well on the target domain despite having a small labeled dataset for the target task.

As transfer learning methods have proven successful in various applications [16; 18; 27], there is a growing effort to utilize these approaches in high-risk environments like healthcare, medicine, transportation, and finance. Any vulnerability of these systems provides malicious agents with tempting targets. Consequently, the users of these ML systems may find their health and financial well-being potentially jeopardized. Additionally, institutions that deploy these systems risk public relations crises and lawsuits. A particular concern is attacks that happen after a model is deployed. Such attacks are called "inference-time attacks" where, for example, a malicious agent attacks a large language model (LLM) chatbot, a self-driving car, or a fraud-detection system. Much of the literature focuses on inference-time attacks that add small perturbations to the model's input. Prior work has demonstrated that this can cause ML models to act unpredictably [8; 35].

While several works have focused on imparting ML algorithms with robustness to adversarial attacks [21, 46, 41, 28, 10], there is little emphasis on adversarially robust transfer learning, i.e., ensuring robustness to tasks with little (or no) supervision by leveraging labeled data from related tasks. In this paper, we study this problem from a theoretical perspective, building on prior work on this topic [13, 26, 12, 42]. We focus on transfer learning via learning a representation that provides a method for sharing knowledge between different, albeit related, tasks [7, 45, 14, 29, 49]. A common approach to achieve this has been termed multi-task representation learning (MTRL) [9, 24, 11]. In practice, the class of representations is a complex model like a deep neural network, and the predictors trained on top of them are simple linear models [7, 14]. Such a paradigm offers hope that we can pool our data, potentially providing a substantial benefit if performed well.

In this work, we are interested in answering the following question: _can multi-task learning be used to learn complex representations that facilitate robust transfer while also benefiting from the diversity of source data?_ We answer in the affirmative. Consider a class of representations \(\mathcal{H}\) from \(\mathbb{R}^{d}\) to a lower dimensional space \(\mathbb{R}^{k}\), and a class of real-valued predictors \(\mathcal{F}\) trained on top of it. Let \(t\) be the number of source tasks, \(n\) be the number of samples per source task, and \(m\) be the number of samples for the target task. For exposition, let \(C(\cdot)\) be the complexity of a function class that is independent of \(t,n,m\) and the adversarial attack. For Lipschitz losses, we bound the excess transfer risk for the adversarial loss by

\[\tilde{O}\Bigg{(}\sqrt{\frac{dC(\mathcal{F})}{n}+\frac{dC(\mathcal{H})}{nt}}+ \sqrt{\frac{kC(\mathcal{F})}{m}}\Bigg{)}.\]

For smooth and nonnegative losses, we bound the excess transfer risk for the adversarial loss by

\[\tilde{O}\Bigg{(}\sqrt{\boldsymbol{L}_{\text{tar}}^{\star}}\sqrt{\frac{kC( \mathcal{F})}{m}}+\frac{kC(\mathcal{F})}{m}+\frac{1}{\nu}\Bigg{(}\sqrt{ \boldsymbol{L}_{\text{src}}^{\star}}\sqrt{\frac{dC(\mathcal{F})}{n}+\frac{ dC(\mathcal{H})}{nt}}+\frac{dC(\mathcal{F})}{m}+\frac{dC(\mathcal{H})}{nt} \Bigg{)}+\varepsilon\Bigg{)},\]

where \(\nu\) and \(\varepsilon\) quantify task relatedness, \(\boldsymbol{L}_{\text{src}}^{\star}\) is the average best possible adversarial risk for the source tasks, and \(\boldsymbol{L}_{\text{tar}}^{\star}\) is the best possible adversarial risk for the target task2. The second bound is called an optimistic rate [34]. Both \(\boldsymbol{L}_{\text{src}}^{\star}\) and \(\boldsymbol{L}_{\text{tar}}^{\star}\) allow the rate above to interpolate between a slow and fast rate depending on how difficult it is to be adversarially robust within the setting. Both of these rates show the benefit of pooling \(nt\) data to learn a feature function that assists in mitigating adversarial attacks.

Footnote 2: For illustration, we use the “Gaussian chain rule” [37] to decompose \(\mathcal{F}\circ\mathcal{H}\). Due to space limitations, henceforth we refrain from it except when indicated. For more details, see the paragraph after Assumption 3.

In the process of showing the above, we establish several results that we believe are of independent interest. A more complete list of our contributions follows.

1. We show bounds on the excess transfer risk for the adversarial loss class for both Lipschitz losses (Theorem 2) and smooth nonnegative losses (Theorem 5).
2. Foundational to Theorems 2 and 5 are Lemmas 1 and 3, resp. These lemmas are similar to results in prior work [25, Lemma 4.4 and Lemma 6.5.]. However, ours are less restrictive because we remove a Lipschitzness assumption on the adversarial loss class. In Appendix B.2, we provide an example of an attack model for which our lemma applies but the previous lemmas do not.
3. In our general attack model (Assumption 4) and both Lipschitz losses and smooth nonnegative losses, we bound the sample-dependent Rademacher complexity of the adversary loss class by the worst-case Rademacher complexity times a multiplicative factor attributed to the adversarial attack. This latter factor for many common attacks has a dimensional dependence of \(\sqrt{d}\log d\). Additionally, when the loss function is smooth and nonnegative, our bound is a sub-root function, which is suitable for optimistic rates.
4. We provide a framework for studying adversarial robustness in MTRL, which consists of several foundational contributions, e.g., Theorems 1 and 4, Definition 1, Assumption 4.A, Algorithm 1.

### Some core difficulties and techniques

* A core part of our arguments is a pair of covering number lemmas that convert from the adversarial loss class into the standard loss class at the expense of inflating the data. However, after applying this result to the integrand of Dudley's integral, the sample complexity depends on the radius of the cover of the function class. This dependency makes bounding the integral difficult.

Prior work either invoked a model or made a parametric assumption to bound this integral [25]. In contrast, we bound the integral in more generality. We elaborate on these difficulties and detail our technique in Proof Sketch 5.
* We use a comparison inequality from a celebrated work [31] in a novel way, to our knowledge, that allows a decomposition that separates function class complexity and attack complexity. Unfortunately, this is not a full decomposition due to a weak \(\log\log\) dependence between the factors. However, we show that this dependence can be handled appropriately.
* We present a reduction from multi-task learning to single-task learning, see the proof of Theorem 3, that we believe is of independent interest. In particular, this observation allows simplification of prior work [37, 39] due to these works' use of worst-case complexity.

### Prior work

**Adversarial attack.** Prior work has shown that the normalization of model weights, data, and the definition of robustness can significantly impact the dimensional cost of achieving adversarial robustness. Although not directly comparable, it is informative to contrast the generalization bounds for linear classifiers presented by [5] and [17]. Both works perform Rademacher complexity based analysis with losses that are Lipschitz and satisfy certain monotonic properties. However, they mainly differ in how they normalize the quantities involved. Let \(p\) and \(q\) be Holder-conjugates. [5] consider \(\left\lVert\cdot\right\rVert_{p}\) linear classifiers with \(\left\lVert\cdot\right\rVert_{q}\) normalized data being attacked with \(\left\lVert\cdot\right\rVert_{\infty}\) perturbations. They show that the Rademacher complexity of suitably transformed version of the linear model, after applying Talagrand's contraction lemma, has a tight \(d^{1/q}\) dimensional dependence. Thus, when \(p=1\), there is no dimensional dependence. On the other hand, [17] consider \(\left\lVert\cdot\right\rVert_{2}\) and \(\left\lVert\cdot\right\rVert_{p}\) bounded linear classifiers with data being \(\left\lVert\cdot\right\rVert_{2}\) bounded and being attacked with \(\left\lVert\cdot\right\rVert_{q}\) perturbations. They show that the Rademacher complexity of similarly transformed function class has no dimensional dependence. Both rates lead to generalization bounds on the robust risk. Taken together, these works demonstrate how prior research has strongly leveraged the relationship between the norm on the attack, which we cannot control, and the model and data norms. Both works also provide rates for neural networks. [5] provides a lower bound, showing that a certain variational version of a neural network has a \(\sqrt{d}\) lower bound. [17] shows that their rate has an \(\sqrt{d}\) upper bound via a "tree transformation" which is then used to bound the robust risk. Finally, both works consider the optimization of surrogate losses for neural networks.

**Adversarial transfer.** Theoretical analyses of adversarial transfer in MTRL remain relatively scarce, despite many empirical studies [33, 47, 43, 1, 38, 32]. Two works that provide theoretical insights into this problem are [13] and [26]. First, [13] is perhaps the first theoretical study of this problem. Specifically, they study a shared linear projection onto a smaller dimensional subspace with linear classifiers trained on top of it, analogous to the regression study in [36, 15]. Under \(\left\lVert\cdot\right\rVert_{\infty}\) or \(\left\lVert\cdot\right\rVert_{2}\) perturbation attacks, they show that the transfer risk of the adversarial loss decays as \(\sqrt{k/m}+\sqrt{k^{2}d/nt}\) along with multiplicative constants that depend on task diversity and can reduce the rate as more diverse tasks are gathered. In addition, they study the combination of semi-supervised learning and adversarial training and show their complementary qualities. Second, [26] considers a composite model, with a linear model being trained on top of a neural network. For additive perturbations and Lipschitz losses, they provide two results showing that the robustness of the predictor is bounded by the robustness of the representation it is trained on. Their first result, with high probability, bounds the difference between the adversarial loss of the end-to-end predictor and the standard loss in terms of the average Euclidean difference in the representations over the predictions. Their second result, applied to classification, provides a sufficient condition for robustness in terms of a bound on the aforementioned Euclidean difference. These results are independent of the method used for training the parameters of the representation.

While not strictly in the MTRL setting, [12] works with a non-composite model and gives a sufficient condition for robust transfer in terms of the discrepancy between the symmetric difference hypothesis space. They give generalization bounds using Rademacher complexity. Also, [42] studies the connection between domain transfer and adversarial robustness, showing that robustness is neither necessary nor sufficient for domain transferability.

**Optimistic rates.** Optimistic rates are a type of self-normalized inequality that bounds the excess risk that interpolates between two rates of decay. The prototypical example being the use of a smooth nonnegative losses to give a rate where a fast \(\mathcal{O}(1/n)\) is achieved when task is realizable and a standard \(\mathcal{O}(1/\sqrt{n})\) when it is not [34]. Optimistic rates have been shown for linear regression with Gaussian data [48], multi-output prediction [30], adversarial robustness [25], and multi-task learning [39]. The typical way to achieve optimistic rates is via local Rademacher complexity machinery [4].

## 2 Problem setup and preliminaries

Let \(\mathcal{X}\subseteq\mathbb{R}^{d}\) and \(\mathcal{Y}\subseteq\mathbb{R}\) denote the input and the label spaces, respectively. Let \(\mathcal{H}\) be a class of representation maps from \(\mathbb{R}^{d}\) to \(\mathbb{R}^{k}\), and \(\mathcal{F}\) and \(\mathcal{F}_{0}\) each be a class of predictors from \(\mathbb{R}^{k}\) to \(\mathcal{Y}\).3 For a loss function \(\ell:\mathbb{R}\times\mathcal{Y}\to\mathbb{R}\), denote \(\ell_{y}:=\ell(\cdot,y)\) so we can represent the functions consistently as a composition. Let \(\left\lVert\cdot\right\rVert_{2}\) and \(\left\lVert\cdot\right\rVert_{\infty}\) denote the Euclidean norm and the uniform norm, respectively. We use the convention that \(\hat{\mathcal{O}}(\cdot)\) hides log terms from the usual asymptotic notation.

Footnote 3: As in prior work [37; 39], we allow different predictor classes for the source and target tasks.

Each source task is represented by a distribution \(\{P_{j}\}\) over \(\mathcal{X}\times\mathcal{Y}\), for \(j=1,\ldots,t\), and the target task is represented by probability distribution \(P_{0}\). Following prior work [37; 39], we make the following assumptions for all tasks \(P_{0},\ldots,P_{t}\). We assume that (a) the marginal distribution over \(\mathcal{X}\) is the same; (b) there exists a common representation \(h^{\star}\in\mathcal{H}\) and task-specific predictors \(f_{j}^{\star}\in\mathcal{F}\), \(f_{0}^{\star}\in\mathcal{F}_{0}\) such that \(P_{j}\) can be decomposed as \(P_{j}(x,y)=P_{x}(x)P_{y|x}(y|f_{j}^{\star}\circ h^{\star}(x))\); and (c) the predictor \(f_{j}^{\star}\circ h^{\star}\) is the optimal4 in-class predictor for its respective task w.r.t. \(\ell\). The assumption above implies that any additional noise in \(y\) is independent of \(x\) because \(y\) depends on \(x\) only via \(f_{j}^{\star}\circ h^{\star}(x)\). As noted in [39], the second assumption above does not imply the third.

Footnote 4: This assumption is not strictly necessary but it helps making the optimistic rates more interpretable.

We assume that we have access to \(n\) training examples for each source task drawn i.i.d. from the respective distributions \(P_{1},\ldots,P_{t}\), and \(m\) examples for the target task \(P_{0}\). We use \((x_{j}^{i},y_{j}^{i})\) to denote the \(i^{\text{th}}\) training example for the \(j^{\text{th}}\) task.

**Adversarial attacks and adversarial training.** We formulate our attack with a function class \(\mathcal{A}\subseteq\{A:\mathcal{X}\to\mathcal{X}\}\). For example, \(\mathcal{A}=\{x\mapsto x+\delta\mid\left\lVert\delta\right\rVert_{\infty} \leq 0.01,x+\delta\in\mathcal{X}\}\) for additive \(\left\lVert\cdot\right\rVert_{\infty}\) attacks. Our goal is to learn a composite predictor \(\hat{f}\circ\hat{h}\in\mathcal{F}_{0}\circ\mathcal{H}\) which performs well and is robust to these adversarial attacks, i.e., it has a small adversarial risk defined formally as follows. Let \(f_{0}\in\mathcal{F}_{0}\) and \(\bm{f}=(f_{1},\ldots,f_{t})\in\mathcal{F}^{\otimes t}\). Then, the adversarial population risk and empirical risk for the target and the source tasks are defined as follows.

\[R_{\text{tar}}(f_{0},h,\mathcal{A}) \coloneqq\mathbb{E}_{(x,y)\sim P_{0}}\bigg{[}\max_{A\in\mathcal{A }}(\ell_{y}\circ f_{0}\circ h\circ A)(x)\bigg{]},\] \[R_{\text{src}}(\bm{f},h,\mathcal{A}) \coloneqq\frac{1}{t}\sum_{j=1}^{t}\mathbb{E}_{(x,y)\sim P_{j}} \bigg{[}\max_{A\in\mathcal{A}}(\ell_{y}\circ f_{j}\circ h\circ A)(x)\bigg{]},\] \[\hat{R}_{\text{tar}}(f_{0},h,\mathcal{A}) \coloneqq\frac{1}{m}\sum_{i=1}^{m}\bigg{[}\max_{A\in\mathcal{A}} \Bigl{(}\ell_{y_{0}^{i}}\circ f_{0}\circ h\circ A\Bigr{)}(x_{0}^{i})\bigg{]},\] \[\hat{R}_{\text{src}}(\bm{f},h,\mathcal{A}) \coloneqq\frac{1}{nt}\sum_{j=1}^{t}\sum_{i=1}^{n}\bigg{[}\max_{A\in \mathcal{A}}\Bigl{(}\ell_{y_{j}^{i}}\circ f_{j}\circ h\circ A\Bigr{)}(x_{j}^{i })\bigg{]}.\]

We also make natural modifications of the two-stage learning procedure used in [37; 39].

**Algorithm 1** (Two-stage adversarial MTRL).: \[(\hat{\bm{f}},\hat{h}) \in\operatorname*{arg\,min}_{\bm{f}\in\mathcal{F}^{\otimes t},h, \mathcal{H}}\hat{R}_{\text{src}}(\bm{f},h,\mathcal{A})\] (Stage 1) \[\hat{f}_{0} \in\operatorname*{arg\,min}_{\bm{f}\in\mathcal{F}_{0}}\ \hat{R}_{\text{tar}} \Bigl{(}f_{0},\hat{h},\mathcal{A}\Bigr{)}\] (Stage 2)

In Stage 1, we perform empirical risk minimization over the adversarial loss class for the combined \(t\) tasks. After Stage 1 we have \(t\) compositions \(\hat{f}_{1}\circ\hat{h},\ldots,\hat{f}_{t}\circ\hat{h}\) which minimize the average risk above. Now, in Stage 2, we fix the representation \(\hat{h}\) learned from the source tasks and perform empirical risk minimization again to find a new predictor for the target task. The final predictor for the target task is \(\hat{f}_{0}\circ\hat{h}\).

**Adversarial task diversity.** Naturally, if all of our tasks are drastically different we expect Algorithm 1 to perform poorly. Therefore, it is crucial to quantify the relationship between the tasks. Prior work in the linear setting gives sufficient properties between the tasks to provide provable rates [15, 36]. However, these assumptions are in terms of spectral properties and therefore not suitable in a more general setting. A more general notion of task relatedness called _task diversity_ was introduced in [37]. This relationship between tasks was shown to be sufficient for Lipschitz losses [37] and smooth nonnegative losses [39]. Yet, it was not clear if these guarantees hold in an adversarial setting. To close the gap, we introduce a new notion of _adversarial task diversity.5_

Footnote 5: Note that this is slightly different than the definitions introduced in [37], here we are using a product space structure like [39] to simplify the problem.

**Definition 1** (Robust \((\nu,\varepsilon,\mathcal{A})\)-task diversity ).: _The tasks \(\{P_{j}\}_{i=1}^{t}\) are \((\nu,\varepsilon,\mathcal{A})\)-diverse over \(P_{0}\), if for the corresponding \(\boldsymbol{f}^{\star}\in\mathcal{F}^{\otimes t},\boldsymbol{f}_{0}^{\star} \in\mathcal{F}_{0}\) and representation \(h^{\star}\in\mathcal{H}\), we have that for all \(h^{\prime}\in\mathcal{H}\)_

\[\inf_{f^{\prime}\in\mathcal{F}_{0}}R_{\mathrm{tar}}(f^{\prime},h^{\prime}, \mathcal{A})-R_{\mathrm{tar}}(f_{0}^{\star},h^{\star},\mathcal{A})\leq\nu^{-1 }(\inf_{\boldsymbol{f}^{\prime}\in\mathcal{F}^{\otimes t}}R_{\mathrm{src}} \big{(}\boldsymbol{f}^{\prime},h^{\prime},\mathcal{A}\big{)}-R_{\mathrm{src} }(\boldsymbol{f}^{\star},h^{\star},\mathcal{A}))+\varepsilon.\]

**Loss class and dataset notation.** Let a function class \(\mathcal{Q}\) be a function class and its \(t\)-fold Cartesian product be \(\mathcal{Q}^{\otimes t}\). We use the following notation for the standard MTRL loss class.

\[\mathcal{L}\big{(}\mathcal{Q}^{\otimes t}\big{)}\coloneqq\{(x_{1},\ldots,x_{t })\mapsto((\ell_{y_{1}}\circ q_{1})(x_{1}),\ldots,(\ell_{y_{t}}\circ q_{t})(x _{t}))\mid q\in\mathcal{Q}^{\otimes t}\}.\]

We define an adversarial counterpart to the MTRL loss class (above) as follows.

\[\mathcal{L}_{\mathcal{A}}\big{(}\mathcal{Q}^{\otimes t}\big{)}\coloneqq\{(x_{ 1},\ldots,x_{t})\mapsto(\max_{A\in\mathcal{A}}(\ell_{y_{1}}\circ q_{1}\circ A )(x_{1}),\ldots,\max_{A\in\mathcal{A}}(\ell_{y_{t}}\circ q_{t}\circ A)(x_{t} ))\mid q\in\mathcal{Q}^{\otimes t}\}.\]

We define the _function class \(\mathcal{Q}\) restricted by functional \(V:\mathbb{R}^{t}\to\mathbb{R}\) at \(r\)_ as \(\mathcal{Q}|_{r}=\{q\in\mathcal{Q}^{\otimes t}\mid V(q)\leq r\}\). We will consider \(V\) to be a multiple (by a factor of \(b\)) of the adversarial or standard risk. The Rademacher complexity of this restricted functional class yields local Rademacher complexity. When using local Rademacher complexity it is common to bound it by a sub-root function. A function \(\psi:[0,\infty)\to[0,\infty)\) is sub-root if it is nonnegative, nondecreasing, and if \(r\mapsto\psi(r)/\sqrt{r}\) is nonincreasing for \(r>0\). Sub-root functions are continuous and have unique fixed points [4]. Given \(x\in\mathcal{X}\) let \(C_{x}(\varepsilon)\) be a proper \(\|\cdot\|_{\mathcal{A}}\)-cover of \(\mathcal{A}(x)\) at scale \(\varepsilon\). Since \(C_{x}(\varepsilon)\) is proper, this cover is realized by some subset of \(\mathcal{A}\). Let \(C_{\mathcal{A}(x)}(\varepsilon)\) be this subset of \(\mathcal{A}\). For a dataset \(S\coloneqq\{(x_{i},y_{i})\}_{i\in[n]}\), we rotate \(S_{\mathcal{A}}(\varepsilon)\coloneqq\{(A(x_{i}),y_{i})\mid i\in[n],A\in C_{ \mathcal{A}(x_{i})}(\varepsilon)\}\) to represent the approximate inflation of \(S\) with respect to \(\mathcal{A}\) at radius \(\varepsilon\). Our convention is to have all covers be minimal when minimality can be achieved.

**Complexities.** Next, we introduce the notions of complexities of functions classes that we will utilize. First, we give the Rademacher based complexities suitable in MTRL on a fixed set of inputs. Let \(\mathcal{Q}\) be a class of vector-valued functions from \(\mathcal{Z}\) to \(\mathbb{R}^{q}\). Denote the \(p\)-fold Cartesian product of \(\mathcal{Q}\) as \(\mathcal{Q}^{\otimes p}\). For \(\boldsymbol{Z}=\big{(}z_{j}^{i}\big{)}_{j\in[p],i\in[n]}\), where \(z_{j}^{i}\in\mathcal{Z}\), define the data-dependent Rademacher width and data-dependent Rademacher complexity, respectively, as \(\hat{\mathfrak{R}}(\mathcal{Q}^{\otimes p},\boldsymbol{Z})\coloneqq\mathbb{E }_{\sigma_{i,j,k}}[\sup_{\boldsymbol{q}\in\mathcal{Q}^{\otimes p}}(np)^{-1} \sum_{i,j,k=1}^{n,p,q}\sigma_{ijk}(q_{j}(z_{j}^{i}))_{k}]\) and \(|\hat{\mathfrak{R}}|(\mathcal{Q}^{\otimes p},\boldsymbol{Z})\coloneqq\mathbb{E }_{\sigma_{i,j,k}}[\sup_{\boldsymbol{q}\in\mathcal{Q}^{\otimes p}}|(np)^{-1} \sum_{i,j,k=1}^{n,p,q}\sigma_{ijk}(q_{j}(z_{j}^{i}))_{k}]|,\) where \(\sigma_{i,j,k}\) are i.i.d. Rademacher random variables. In contrast to the convention we will place a particular emphasis on the dataset, and therefore, it is prominent in the notation. We define the worst-case Rademacher width and the worst-case Rademacher complexities as \(\hat{\mathfrak{R}}(\mathcal{Q}^{\otimes p},n)\coloneqq\sup_{\boldsymbol{Z}} \hat{\mathfrak{R}}(\mathcal{Q}^{\otimes p},\boldsymbol{Z})\) and \(|\hat{\mathfrak{R}}|(\mathcal{Q}^{\otimes p},n)\coloneqq\sup_{|\boldsymbol{Z} |=n}|\hat{\mathfrak{R}}|(\mathcal{Q}^{\otimes p},\boldsymbol{Z})\).

Next, using the same notation, we define the empirical covering number of vector-valued function class. With \(\varepsilon>0\), define \(\mathcal{N}_{2}(\mathcal{Q}^{\otimes t},\varepsilon,\boldsymbol{Z})\) be the cardinality of a minimal cover \(C\) such that for all \(\boldsymbol{q}\in\mathcal{Q}^{\otimes t}\) there is a \(\tilde{\boldsymbol{q}}\in C\) such that \((np)^{-1}\sum_{i,j,k=1}^{n,p,q}((q_{j}(z_{j}^{i}))_{k}-(\tilde{q}_{j}(z_{j}^{i} ))_{k})^{2}\leq\varepsilon^{2}\) and \(\mathcal{N}_{\infty}(\mathcal{Q}^{\otimes t},\varepsilon,\boldsymbol{Z})\), similarly, but with \(\max_{i,j,k}|(q_{j}(z_{j}^{i}))_{k}-(\tilde{q}_{j}(z_{j}^{i}))_{k}|\leq\varepsilon\) as the norm instead.

Finally, we define the fat-shattering dimension. Let \(\mathcal{Q}\) be a class of functions from \(\mathcal{Z}\) to \(\mathbb{R}\) and let \(\boldsymbol{Z}=\{z_{1},\ldots,z_{m}\}\) where \(\boldsymbol{Z}\subseteq\mathcal{Z}\). Then, for \(\gamma>0\), we say that \(\boldsymbol{Z}\) is \(\gamma\)-shattered by \(\mathcal{Q}\), if there exist \(r_{1},\ldots,r_{m}\), such that for all \(b\in\{0,1\}^{m}\) there is a \(q_{b}\in\mathcal{Q}\) such that for all \(i\in[m]\) we have \(f_{b}(x_{i})\geq r_{i}+\gamma\) if \(b_{i}=1\) and \(f_{b}(x_{i})\leq r_{i}-\gamma\) if \(b_{i}=0\). Let \(\operatorname{vc}_{\boldsymbol{Z}}(\mathcal{Q},\gamma)\) be the cardinality of the largest subset of \(\boldsymbol{Z}\) that is \(\gamma\)-shattered by \(\mathcal{Q}\). If a largest subset does not exists, let \(\operatorname{vc}_{\boldsymbol{Z}}(\mathcal{Q},\gamma)=\infty\).

**Assumptions.** We make the following assumptions on the loss function, hypothesis classes, and the adversarial attack function. The first two assumptions regarding if the loss is Lipschitz or smooth nonnegative are standard.

**Assumption 1** (Lipschitz loss).: _The loss function \(\ell:\mathbb{R}\times\mathcal{Y}\to\mathbb{R}\) is \(L_{\ell}\)-Lipschitz and \(b\)-bounded i.e., \(|\ell(y^{\prime},y)|\leq b<\infty,\;\forall y^{\prime}\in\mathbb{R},y\in \mathcal{Y}\) and \(|\ell_{y}(y_{1},y)-\ell_{y}(y_{2},y)|\leq L_{\ell}|y_{1}-y_{2}|\) for all \(y_{1},y_{2}\in\mathbb{R},y\in\mathcal{Y}\)._

**Assumption 2** (Smooth nonnegative loss).: _The loss function \(\ell:\mathbb{R}\times\mathcal{Y}\to\mathbb{R}\) is nonnegative, \(b\)-bounded, and \(H\)-smooth, i.e., \(0\leq\ell(y^{\prime},y)\leq b<\infty\) for all \(y^{\prime}\in\mathbb{R},y\in\mathcal{Y}\) and \(\big{|}\ell^{\prime}_{y}(y_{1},y)-\ell^{\prime}_{y}(y_{2},y)\big{|}\leq H|y_{ 1}-y_{2}|\) for all \(y_{1},y_{2}\in\mathbb{R},y\in\mathcal{Y}\)._

The next assumptions are that the predictor and representation function classes are Lipschitz.

**Assumption 3** (Hypotheses and feature maps are Lipschitz).:
1. _All functions in_ \(\mathcal{F}\) _and_ \(\mathcal{F}_{0}\) _are_ \(\|\cdot\|\)_-Lipschitz for some_ \(0<L_{\mathcal{F}}<\infty\)_, i.e.,_ \(|f(z_{1})-f(z_{2})|\leq L_{\mathcal{F}}\|z_{1}-z_{2}\|\) _for all_ \(z_{1},z_{2}\in\text{Dom}(f)\) _and_ \(f\in\mathcal{F}\cup\mathcal{F}_{0}\)_._
2. _All functions in_ \(\mathcal{H}\) _are_ \((\|\cdot\|_{\mathcal{A}},\|\cdot\|)\)_-Lipschitz for some_ \(0<L_{\mathcal{H}}<\infty\)_, i.e.,_ \(\|h(z_{1})-h(z_{2})\|\leq L_{\mathcal{H}}\|z_{1}-z_{2}\|_{\mathcal{A}}\) _for all_ \(z_{1},z_{2}\in\text{Dom}(h)\) _and_ \(h\in\mathcal{H}\)_._

If the norm in Assumption 3A is Euclidean and additionally any \(f\circ h\in\mathcal{F}\circ\mathcal{H}\) is \(D\)-bounded over \(\mathcal{X}\) w.r.t. \(\|\cdot\|_{2}\) then these assumptions are sufficient to apply the so called Gaussian chain rule [37]. This allows more interpretability since the statistical cost of learning can be broken up into two terms, for \(\mathcal{F}\) and \(\mathcal{H}\) separately, in a rather intuitive way. We do not apply this theorem primarily due to space limitations. However, for illustrative reasons, we will assume these assumptions hold when we analyse the asymptotics of Theorems 2 and 5.

Finally, we make the following assumptions on the adversary.

**Assumption 4** (Bounded within-domain adversarial attacks).:
1. \(\mathcal{A}(x)\) _is totally bounded w.r.t_ \(\|\cdot\|_{\mathcal{A}}\) _for all_ \(\mathcal{X}\)_. That is, for all_ \(\varepsilon>0\) _a finite number of_ \(\|\cdot\|_{\mathcal{A}}\) _balls of radius_ \(\varepsilon\) _cover_ \(\mathcal{A}(x)\) _for all_ \(x\in\mathcal{X}\)_._
2. _The attack function cannot perturb outside of the input domain_ \(\mathcal{X}\)_, i.e.,_ \(\mathcal{A}\subseteq\{A:\mathcal{X}\to\mathcal{X}\}\) _._

First, Assumption 4A trivially implies that there exists a \(\Delta<\infty\) such that \(\sup_{A\in\mathcal{A},x\in\mathcal{X}}\|A(x)\|_{\mathcal{A}}\leq\Delta\). Second, Assumption 4B is a reasonable assumption as we discuss in Appendix A.

## 3 Adversarial multi-task representation learning

In this section, we give our main adversarial MTRL results. Given robust task diversity (Definition 1) holds, these theorems show that with high probability, adversarial excess transfer risk decays with the sample size and is bounded by the complexity of the non-adversarial loss class and an additional factor derived from the adversarial attack. First, we show our result for Lipschitz losses, and then for smooth nonnegative losses.

### Lipschitz losses

Lipschitz losses, like ramp loss, are frequently used in machine learning. We will start with a uniform convergence bound that is foundational to our study of Lipschitz losses. The following result bounds the adversarial excess transfer risk by the Rademacher complexity of the adversarial loss class for the source tasks and target task. This result is an adversarially robust version of the main MTRL result in [37] before utilizing the Lipschitzness of the loss. Alternatively, it can be seen as an MTRL version of Corollary 1 in [44].

**Theorem 1**.: _Let \(\hat{h}\) and \(\hat{f}_{0}\) be the learned representation and target predictor, as described Algorithm 1. Under Assumption 1 and that \(\boldsymbol{f}^{\star}\) is \((\nu,\varepsilon,\mathcal{A})\)-diverse over \(\mathcal{F}_{0}\) w.r.t. \(h^{\star}\), then, with probability _at least \(1-2\delta\), we have that \(R_{\mathrm{tar}}\Big{(}\hat{f}_{0},\hat{h},\mathcal{A}\Big{)}-R_{\mathrm{tar}}(f _{0}^{\star},h^{\star},\mathcal{A})\) is bounded by_

\[\nu^{-1}(8\hat{\mathfrak{R}}(\mathcal{L}_{\mathcal{A}}\big{(}\mathcal{F}^{ \otimes t}(\mathcal{H})\big{)},n)+8b\sqrt{\log(2/\delta)/nt})+8\sup_{h\in \mathcal{H}}\widehat{\mathfrak{N}}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}_{0} \circ h),m)+8b\sqrt{\log(2/\delta)/m}+\varepsilon.\]

We now apply the above to Lipschitz losses classes. But, before we do, let us define a special function that features prominently in our work. Let the function \(\Lambda_{\mathcal{A}}(\rho,L,n,\beta)\) be mapped to

\[\bigg{(}\log\log\!\Big{|}S_{\mathcal{A}}\bigg{(}\frac{eb}{4c\sqrt{nL}}\bigg{)} \bigg{|}+\frac{c\rho}{\beta}\bigg{)}\bigg{(}\frac{8}{c}+40\frac{\sqrt{eC}}{c} \sqrt{\log\!\bigg{(}\frac{4c\sqrt{n}}{e}\bigg{|}S_{\mathcal{A}}\bigg{(}\frac{ eb}{4c\sqrt{nL}}\bigg{)}\bigg{|}\bigg{)}}\log\!\bigg{(}\frac{16\rho c\beta n}{e^{2}} \bigg{)}\bigg{)}.\]

When needed, we will use \(\Lambda_{\mathcal{A}}^{\mathrm{src}}(\cdot,\cdot,\cdot,\cdot)\) or \(\Lambda_{\mathcal{A}}^{\mathrm{tar}}(\cdot,\cdot,\cdot,\cdot)\) to indicate which data is being inflated.

**Theorem 2**.: _Under the setting of Theorem 1 along with Assumption 3, Assumption 4, \(|S_{\mathcal{A}}\big{(}eb/4c\sqrt{nt}L_{3}\big{)}|,|S_{\mathcal{A}}(eb/4c\sqrt {m}L_{1})|\geq e^{\varepsilon},\)\(\mathrm{vc}_{\mathcal{A}}(\mathcal{L}(\mathcal{F}\circ\mathcal{H}),\beta_{1}b), \mathrm{vc}_{\mathcal{X}}(\mathcal{L}(\mathcal{F}_{0}\circ\mathcal{H}),\beta_ {2}b)\geq 1\), then the Rademacher complexities \(\widehat{\mathfrak{R}}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}^{\otimes t}( \mathcal{H})),n)\) and \(\widehat{\mathfrak{N}}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}_{0}\circ h),m)\) in Theorem 1 are, respectively, bounded by_

\[2|\widehat{\mathfrak{R}}|\big{(}\mathcal{L}\big{(}\mathcal{F}^{\otimes t}( \mathcal{H})\big{)},n\big{)}\Lambda_{\mathcal{A}}^{\mathrm{src}}(2^{-1},L_{3},nt,\beta_{1})\text{ and }\text{ }2\sup_{h\in\mathcal{H}}|\widehat{\mathfrak{N}}|( \mathcal{L}(\mathcal{F}_{0}\circ h),m)\Lambda_{\mathcal{A}}^{\mathrm{tar}} \big{(}2^{-1},L_{1},m,\beta_{2}\big{)},\] (1)

_where \(L_{3}=L_{\ell}L_{\mathcal{F}}L_{\mathcal{H}}\), \(L_{1}=L_{\ell}L_{\mathcal{F}}\), and \(C,c\) are absolute constants._

If we ignore the two factors that come from the adversarial complexity, the result is similar to prior work in the non-adversarial setting of [37]. In fact, if \(\mathcal{A}\) exclusively contains the identity function, then we recover the non-adversarial version modulo log factors.

The assumptions on the fat-shattering dimension and size of the inflated dataset are in several of our theorems. It should be noted that they are technical and mild. First, \(|S_{\mathcal{A}}(1/\sqrt{n})|\) is usually exponential in \(d\), and, therefore, practically always larger than \(e^{e}<16\). Also, the assumption trivially holds if the dataset is larger than \(16\). Second, \(\mathrm{vc}_{\mathcal{X}}(\mathcal{L}(\mathcal{F}\circ\mathcal{H}),\beta b)\geq 1\) is just a parametric version of the assumption that the fat-shattering dimension is nonzero. Practically speaking, \(\beta\) being \(1\) or \(1/2\) is reasonable and does not impact the bound in any meaningful way.

A dimensionality analysis on the first expression in Equation (1).Recall that \(\mathcal{A}(x)\) is totally bounded for all \(x\in\mathcal{X}\) w.r.t. \(\left\|\cdot\right\|_{\mathcal{A}}\). So, over the sample \(S\), the attack class perturbs the points only so far. That is, \(\sup_{x\in S,A^{\prime},A\in\mathcal{A}}\!\left\|A(x)-A^{\prime}(x)\right\|_{ \mathcal{A}}\leq\Delta<\infty\) for some \(\Delta\). This gives us a radius for which we place \(n\) balls of radius \(\Delta\) to cover \(\mathcal{A}(S)\). By standard volume arguments, e.g. Lemma A.8 in [20], each of these balls can be \(eb/4c\sqrt{nt}L_{3}\)-covered by \((12c\sqrt{nt}L_{3}\Delta/eb)^{d}\) many points. Thus, \(n(12c\sqrt{nt}L_{3}\Delta/eb)^{d}\) bounds the cardinality of the inflated dataset \(S_{\mathcal{A}}\big{(}eb/4c\sqrt{nt}L_{3}\big{)}\). By using this inequality in Equation (1), \(\widehat{\mathfrak{R}}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}^{\otimes t}( \mathcal{H})),S)\) is bounded by \(|\widehat{\mathfrak{R}}|(\mathcal{L}(\mathcal{F}^{\otimes t}(\mathcal{H})), nt)\sqrt{d\log(nL_{3}\Delta/b)}\log(d\log(nL_{3}\Delta/b))\log(n),\) where we set \(\beta_{1}=1\), ignored constants and lower-order terms. Therefore, as a function of \(d\), the adversarial training costs at least a factor of \(\sqrt{d}\log d\) more in comparison to the standard non-adversarial loss. The analysis of the second expression in Equation (1) is similar but the dimensional dependence is \(\sqrt{k}\log k\) because the inflation happens in the image of the representation space.

The dimensionality and sample size dependencies of Equation (1).Like in the introduction, let \(C(\cdot)\) be the complexity of a function class which is independent of the sample complexity and the adversarial attack. Generally, \(|\widehat{\mathfrak{R}}|(\mathcal{L}(\mathcal{F}^{\otimes t}(\mathcal{H})),n)\) decays as \(\mathcal{O}(\sqrt{C(\mathcal{F}^{\otimes t}(\mathcal{H}))/nt})\) and \(|\widehat{\mathfrak{R}}|(\mathcal{L}(\mathcal{F}_{0}\circ h),m)\) as \(\mathcal{O}(\sqrt{C(\mathcal{F}_{0})/m})\). If the image of the source tasks predictors is bounded and \(\|\cdot\|_{2}\)-Lipschitz, we can decompose \(|\widehat{\mathfrak{R}}|(\mathcal{L}(\mathcal{F}^{\otimes t}(\mathcal{H})),n)\) into \(\mathcal{O}(\sqrt{C(\mathcal{H})/nt}+\sqrt{O(C(\mathcal{F}))/n})\) by using the Gaussian chain rule [37]. Additionally, the adversarial robustness factors contribute \(\tilde{O}(\sqrt{d})\) for the source tasks and \(\tilde{O}(\sqrt{k})\) for the target task because of the dimensionality analysis in the prior paragraph. Taken together, in this setting, the adversarial excess transfer risk decays as \(\tilde{O}\Big{(}\sqrt{dC(\mathcal{F})/n+dC(\mathcal{H})/nt}+\sqrt{kC( \mathcal{F}_{0})/m}\Big{)}\). See Appendix B.1 for a detailed comparison of the above rate to the linear setting studied in [13].

Proof Sketch 1.: _The result follows by bounding the adversarial Rademacher complexities by calling Theorem 3 twice. However, for the Rademacher complexity w.r.t. the target task one must make the observation that we can treat \(\hat{h}\circ\mathcal{A}(x)\) as the attack function class for all \(x\). This function is totally bounded by Assumption 3.B and therefore we can proceed as normal. See Appendix E.1 for details._The proof above depends on the following bound on the Rademacher complexity of the multi-task function class \(\mathcal{L}_{\mathcal{A}}(\mathcal{F}^{\otimes t}(\mathcal{H}))\). To achieve this bound, we reduce the multi-task setting to the single-task setting. Although simple, this reduction simplifies arguments made in prior work [37; 39][39] these works use worst-case complexity.

**Theorem 3**.: _Under the setting of Theorem 2, \(\mathfrak{R}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}^{\otimes t}(\mathcal{H})),S)\) is bounded by \(2|\mathfrak{R}|(\mathcal{L}(\mathcal{F}^{\otimes t}(\mathcal{H})),n)\Lambda_{ \mathcal{A}}^{\mathrm{src}}\big{(}2^{-1},L_{3},nt,\beta_{1}\big{)},\) where \(L_{3}=L_{\ell}L_{\mathcal{F}}L_{\mathcal{H}}\)._

**Proof Sketch 2**.: _The proof reduces the multi-task setting to the single-task setting by observing that the data-dependent multi-task Rademacher complexity is equivalent to the data-dependent single-task Rademacher complexity on a dataset with an additional immutable component indexing the task. See Appendix E.1 for details._

The proof of Theorem 3 immediately implies we can use the machinery in the single-task setting literature. This is also true in the non-adversarial setting. Therefore, the above reduction technique applies to situations outside adversarial robustness. However, the above argument does not work when not using worst-case Rademacher complexity because \(\tilde{S}\) would not be a sample of i.i.d. random variables. Yet the above method is a convenient tool as we are using worst-case Rademacher complexity because our analysis goes through the fat-shattering dimension. In Appendix D, we give a short remark on a notion of fat-shattering dimension suitable for vector-valued classes.

### Smooth and nonnegative losses

Like in the case of Theorem 1, we extend prior work to the adversarially robust learning setting. Since much of what we discussed about sample complexity and dimensionality parameters in the prior section also applies here, this section will be more brief.

**Theorem 4**.: _Let \(\hat{h}\) and \(\hat{f}_{0}\) be the learned representation and target predictor, as described Algorithm 1. Let \(\psi_{1}\) and \(\psi_{2}\) be sub-root functions such that \(\psi_{1}(r)\geq\varrho\mathfrak{R}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}^{ \otimes t}(\mathcal{H})\mid_{r}),n)\) and \(\psi_{2}(r)\geq b\sup_{h\in\mathcal{H}}\mathfrak{R}(\mathcal{L}_{\mathcal{A}} (\mathcal{F}_{0}\circ h\mid_{r}),m)\) with \(r_{1}^{\star}\) and \(r_{2}^{\star}\) the fixed points of \(\psi_{1}(r)\) and \(\psi_{2}(r)\), respectively. Under Assumption 2 and that \(\bm{f}^{\star}\) is \((\nu,\varepsilon,\mathcal{A})\)-diverse over \(\mathcal{F}_{0}\) w.r.t. \(h^{\star}\), then, with probability at least \(1-2e^{-\delta}\), we have that \(R_{\mathrm{tar}}\Big{(}\hat{f}_{0},\hat{h},\hat{\mathcal{A}}\Big{)}-R_{ \mathrm{tar}}(f_{0}^{\star},h^{\star},\mathcal{A})\) is bounded by,_

\[\sqrt{R_{\mathrm{tar}}(f_{0}^{\star},h^{\star},\mathcal{A})}\Bigg{(} 9\sqrt{\frac{b\delta}{m}}+219\sqrt{\frac{r_{1}^{\star}}{b}}\Bigg{)}+\frac{171b \delta}{m}+\frac{21967r_{1}^{\star}}{2b}\] \[+\frac{1}{\nu}\Bigg{(}\sqrt{R_{\mathrm{src}}(\bm{f}^{\star},h^{ \star},\mathcal{A})}\Bigg{(}6\sqrt{\frac{b\delta}{nt}}+146\sqrt{\frac{r_{2}^{ \star}}{b}}\Bigg{)}+\frac{102b\delta}{nt}+\frac{217r_{2}^{\star}}{b}.\Bigg{)}+\varepsilon.\]

Theorem 4 requires bounding the local Rademacher complexity of the adversarial loss class by a sub-root function, which is the main challenge in applying this result. We show that such a bound is obtained if the predictor classes are Lipschitz and the loss is smooth and nonnegative.

**Theorem 5**.: _Under the setting of Theorem 4 along with Assumption 3, Assumption 4, \(|S_{\mathcal{A}}(eb/4c\sqrt{nt}L_{2})|,|S_{\mathcal{A}}(eb/4c\sqrt{mL_{ \mathcal{F}}})|\geq e^{\varepsilon}\), \(\mathrm{vc}_{\mathcal{A}}(\mathcal{L}(\mathcal{F}\circ\mathcal{H}),\beta_{1}b), \mathrm{vc}_{\mathcal{X}}(\mathcal{L}(\mathcal{F}_{0}\circ\mathcal{H}),\beta_ {2}b)\geq 1\), then the fixed points \(\sqrt{r_{2}^{\star}/b}\) and \(\sqrt{r_{1}^{\star}/b}\) in Theorem 4 are, respectively, bounded by_

\[2\sqrt{12H}|\mathfrak{R}|(\mathcal{F}^{\otimes t}(\mathcal{H}),n )\Lambda_{\mathcal{A}}^{\mathrm{src}}\Big{(}(24Hb)^{-1/2},L_{2},nt,\beta_{1} \Big{)}\]

_and \(2\sqrt{12H}|\mathfrak{R}|(\mathcal{F}_{0}\circ h,m)\Lambda_{\mathcal{A}}^{ \mathrm{tar}}\big{(}(24Hb)^{-1/2},L_{\mathcal{F}},m,\beta_{2}\big{)}\), where \(L_{2}=L_{\mathcal{F}}L_{\mathcal{H}}\)._

Comparing Theorems 2 and 5, the latter has twice as many complexity terms: two for the target task and two for the source tasks. Therefore, we will start our asymptotic analysis of Theorem 5 from where the analysis after Theorem 2 left off. For the target task terms, one of these terms is multiplied by the square root of the adversarial risk for the best-in-class predictor and representation, i.e., \(R_{\mathrm{tar}}(f_{0}^{\star},h^{\star},\mathcal{A})\). This factor is zero when an adversarially robust classifier exists. This leaves only one complexity term remaining for the target task, which is a squared version of the last one. Thus, in this setting, the bound is a fast rate in the number of target samples \(m\). This reasoning shows the value of learning a robust representation because it takes fewer samples to learn a good predictor.

Similarly, the bound is a fast rate in the number of source tasks \(t\) and respective samples per task \(n\) if there exist predictors with zero adversarial risk on the source tasks. Thus, the bound on the excess transfer risk of the adversarial loss class decays as \(\tilde{O}(dC(\mathcal{F})/n+dC(\mathcal{H})/nt+kC(\mathcal{F}_{0})/m),\) when robust predictors and representations exist in our chosen classes.

**Proof Sketch 3**.: _The proof is like the argument for Theorem 2, but we use Theorem 6 not Theorem 3._

In the setting above, we now give our bound on the local Rademacher complexity of the adversarial loss class. Importantly, the bound is a sub-root function in \(r\).

**Theorem 6**.: _Under the setting of Theorem 5, \(\hat{\mathfrak{R}}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}^{\otimes t}( \mathcal{H})\mid_{r}),S)\) is bounded by \(2\sqrt{12H}|\hat{\mathfrak{R}}|(\mathcal{F}^{\otimes t}(\mathcal{H}),n) \Lambda_{\mathcal{A}}^{\mathrm{src}}\big{(}(24Hb)^{-1/2},L_{2},nt,\beta_{1} \big{)},\) where \(L_{2}=L_{\mathcal{F}}L_{\mathcal{H}}.\)_

**Proof Sketch 4**.: _Use the reduction in the proof of Theorem 3, then use Theorem 8._

## 4 Standard adversarial learning

### Lipschitz losses

In this section, we bound the adversarial Rademacher complexity for the standard single-task setting under the assumption of a Lipschitz loss. Such bounds immediately give results for the excess risk of the adversarial loss class via a uniform convergence guarantee like Corollary 1 in [44]. The results and arguments for smooth and nonnegative losses are similar. See Section 4.2 for details.

The following result bounds the sample-dependent Rademacher complexity for the adversarial loss class by two factors: the worst-case Rademacher complexity for the non-adversarial loss class and a function that encodes the power of the attack model.

**Theorem 7**.: _Let \(\mathcal{F}\) be \(L_{\mathcal{F}}\)-Lipschitz w.r.t. \(\left\|\cdot\right\|_{\mathcal{A}}\). Under Assumption 1, Assumption 4, \(|S_{\mathcal{A}}(eb/4c\sqrt{n}L_{1})|\geq e^{e}\), \(\mathrm{vc}_{\mathcal{X}}(\mathcal{L}(\mathcal{F}),\beta b)\geq 1\), then \(\hat{\mathfrak{R}}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}),S)\) is bounded by_

\[2|\hat{\mathfrak{R}}|(\mathcal{L}(\mathcal{F}),n)\Lambda_{\mathcal{A}}\big{(} 2^{-1},L_{1},n,\beta\big{)},\] (2)

_where \(L_{1}=L_{\ell}L_{\mathcal{F}}\) and \(C,c\) are absolute constants._

Many of the remarks in Section 3 apply to Theorem 7. See Appendices B.2 and B.3 for detailed comparisons to [25] and [4], respectively.

**Proof Sketch 5**.: _The difficulty of studying adversarial robustness originates from the variational component of the adversarial loss class. Under certain assumptions, we can remove the \(\max\) function with an appropriate "inflating" of the dataset by using a covering number argument inspired by [25]. The proof of the result below is in the appendix._

**Lemma 1**.: _Let \(\mathcal{F}\) be \(L_{\mathcal{F}}\)-Lipschitz w.r.t. \(\left\|\cdot\right\|_{\mathcal{A}}\). Under Assumption 1, and Assumption 4, we have \(\mathcal{N}_{\infty}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}),\varepsilon,S) \leq\mathcal{N}_{\infty}(\mathcal{L}(\mathcal{F}),\varepsilon/2,S_{\mathcal{A} }(\varepsilon/2L_{1})),\) where \(L_{1}=L_{\ell}L_{\mathcal{F}}\)._

_Although we find Lemma 1 intuitive, it does result in a difficulty that we demonstrate below. First, as expected, we can apply Lemma 6, Dudley's integral; then Lemma 1. These steps are shown in the inequality below._

\[\hat{\mathfrak{R}}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}),S)\leq 4\alpha+ \frac{10}{\sqrt{n}}\int_{\alpha}^{b}\sqrt{\log\mathcal{N}_{\infty}\bigg{(} \mathcal{L}(\mathcal{F}),\frac{\varepsilon}{2},S_{\mathcal{A}}\bigg{(}\frac{ \varepsilon}{2L_{1}}\bigg{)}\bigg{)}}\ d\varepsilon\] (3)

_The above removes the variational component of the loss at the expense of "inflating" the data. Yet, notice that now this "inflated" data \(S_{\mathcal{A}}(\cdot)\) is a function of \(\varepsilon\). This makes the integral more difficult to study because, after applying other techniques from prior work [34, 39], one cannot move the sample complexity out of the integral. This difficulty is resolved in prior work by either invoking a model (e.g., \(\mathcal{F}\) being linear predictors) or making a parametric assumption. Both of these approaches are used in [25], with the parametric assumption being \(\mathcal{N}_{\infty}(\mathcal{L}(\mathcal{F}),\varepsilon/2,S_{\mathcal{A}} (\varepsilon/2L_{1}))\lesssim\varepsilon^{-2}\)._

_Alternatively, we provide an approach that overcomes these challenges while retaining the generality of the results. Starting at Equation (3), our approach is to decouple the complexity of the class and the properties of the inflated dataset. We observe that a weak decoupling can be achieved by the use of a comparison inequality from a \(\left\|\cdot\right\|_{\infty}\) cover to the fat-shattering dimension. In particular, such a comparison inequality is the following special case 6 of a celebrated result in [31]._

**Lemma 2** (Rudelson and Vershynin (2006)).: _Suppose \(\mathcal{F}\) is uniformly \(B\) bounded for \(B>0\), then, for all \(\xi>0\), we have \(\log\mathcal{N}_{\infty}(\mathcal{F},\varepsilon,S)\leq Cv\log(Bn/v\varepsilon) \log^{\xi}(n/v)\) for \(0<\varepsilon<B\) where \(v=\mathrm{vc}_{\mathcal{X}}(\mathcal{F},\epsilon\xi\varepsilon)\) and \(C,c>0\) are universal constants._

_If we apply Lemma 2 to the integrand of Equation (3), then \(\mathcal{N}_{\infty}(\mathcal{L}(\mathcal{F}),\varepsilon/2,S_{\mathcal{A}}( \varepsilon/2L_{1}))\leq C\mathrm{vc}_{\mathcal{X}}(\mathcal{L}(\mathcal{F}), c\xi\varepsilon/2)\log(2b|S_{\mathcal{A}}(\varepsilon/2L_{1})|/ \varepsilon)\log^{\xi}(|S_{\mathcal{A}}(\varepsilon/2L_{1})|),\) where, for illustrative purposes, we removed the fat-shattering dimension factors from the denominators by assuming that \(\mathrm{vc}_{\mathcal{X}}(\mathcal{L}(\mathcal{F}),c\xi\varepsilon/2)\geq 1\). (In general, we must be more nuanced than this by handling cases - see the full proof for more details.) This decomposition allows us to handle each factor in turn in a way that was not possible before. Yet, the picture is more complicated than this, for instance, \(\xi\) can depend weakly on \(S_{\mathcal{A}}(\cdot)\). Nevertheless, from here, the complete proof - see Appendix E.2 - proceeds with a more traditional analysis and relies on other standard lemmas. The above weak decomposition illuminates the significance of Assumption 4.B in our analysis. In particular, when the image of all attacks is in \(\mathcal{X}\), we have that the points fat-shattered are within \(\mathcal{X}\), not \(\mathcal{A}(\mathcal{X})\), i.e., we have \(\mathrm{vc}_{\mathcal{X}}(\mathcal{L}(\mathcal{F}),2)\) nor \(\mathrm{vc}_{\mathcal{A}(\mathcal{X})}(\mathcal{L}(\mathcal{F}),\cdot)\). If Assumption 4.B does not hold, there is a stronger dependence between the complexity of the attack and the complexity of the predictors._

To our knowledge, this is a novel use of this family of fat-shattering comparison inequalities, and our techniques derive novel rates in the single-task setting while retaining the generality. The authors of (31, p. 607) conjecture that the \(\log^{\xi}(n/v)\) factor in Lemma 2 can be removed, although it is unclear what the nature of a \(\xi\) like dependence would be. Naturally, their conjecture implies another: that, for our setting, the Rademacher complexity of the adversarial loss class is \(\mathfrak{R}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}),S)\) is \(\mathcal{O}(\sqrt{d})\).

### Smooth nonnegative losses in the single-task setting

In this section, we bound the local Rademacher complexity of the adversarial loss class for smooth nonnegative losses in the standard single-task setting. Many of the remarks in Section 4.1 apply here too. Thus, we will forgo much of this repetitive commentary for the sake of space. First, we bound the adversarial local Rademacher complexity of the adversarial loss class.

**Theorem 8**.: _Under the setting of Lemma 3 along with \(|S_{\mathcal{A}}(eb/4c\sqrt{n}L_{\mathcal{F}})|\geq e^{\varepsilon}\) and \(\mathrm{vc}_{\mathcal{X}}(\mathcal{F},b\beta)\geq 1\), then we have \(\mathfrak{R}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}\mid_{r}),S)\) is bounded by \(\sqrt{12H}|\mathfrak{R}|(\mathcal{F},n)\Lambda_{\mathcal{A}}\big{(}(24Hb)^{-1/ 2},L_{\mathcal{F}},n,\beta\big{)}\), where \(C,c\) are absolute constants._

A non-adversarial version of Theorem 8 was shown in the seminal (34) with a bound of order \(\tilde{\mathcal{O}}(\sqrt{Hr}|\mathfrak{R}|(\mathcal{F},n))\). In comparison, in the adversarial setting, the expression in Theorem 8 has the additional factor due to the adversary which is of order \(\tilde{\mathcal{O}}(\sqrt{d})\). Importantly, our bound is also a sub-root function in \(r\) and, thus, suitable for optimistic rates derived from local Rademacher complexity. See Appendix B.2 for a comparison of these results with those in (25).

**Proof Sketch 6**.: _The proof proceeds by using Lemma 3 and analysis similar to the proof of Lemma 1._

Our proof of Theorem 8 depends on the following covering number lemma, which similar to Lemma 6.5. in (25). The proof of this lemma is in Appendix E.2.3.

**Lemma 3**.: _Let \(\mathcal{F}\) be a class of predictors and let \(\mathcal{F}\mid_{r}\) be all functions in \(\mathcal{F}\) with empirical adversarial risk less than \(r\) on \(S\). If \(\mathcal{F}\mid_{r}\) is \(L_{\mathcal{F}}\)-Lipschitz w.r.t. \(\left\|\cdot\right\|_{\mathcal{A}}\), then under Assumption 2 and Assumption 4, we have \(\mathcal{N}_{2}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}\mid_{r}),\varepsilon,S) \leq\mathcal{N}_{\infty}\Big{(}\mathcal{F},\varepsilon/2\sqrt{12Hr},S_{ \mathcal{A}}\Big{(}\varepsilon/2\sqrt{12Hr}L_{\mathcal{F}}\Big{)}\Big{)}\)._

## 5 Conclusion

In this work, we have shown several theorems that demonstrate that a representation derived from adversarial training can assist in defending against adversaries on downstream tasks. Such theorems show how utilizing diverse tasks can assist in learning robust representations in data-scarce or high-stake domains. Some additional questions are how to optimally select source tasks to maximally assist in learning a robust representation and if the assumption of the attack residing within the data domain can be relaxed while retaining our \(\tilde{\mathcal{O}}(\sqrt{d})\) rate in general settings. Our main technical innovation is using a celebrated fat-shattering inequality (31) to carefully control the inflation of the dataset. In doing so we have also shown several novel rates in the single-task setting.

## Acknowledgments and Disclosure of Funding

This research was supported, in part, by the DARPA GARD award HR00112020004, NSF CAREER award IIS-1943251, funding from the Institute for Assured Autonomy (IAA) at JHU, and the Spring'22 workshop on "Learning and Games" at the Simons Institute for the Theory of Computing.

## References

* [1]K. Alhamoud, H. Abed Al K. Hammoud, M. Alfarra, and B. Ghanem (2023) Generalizability of adversarial robustness under distribution shifts. Trans. Mach. Learn. Res.2023. Cited by: SS1.
* [2]N. Alon, S. Ben-David, N. Cesa-Bianchi, and D. Haussler (1993) Scale-sensitive dimensions, uniform convergence, and learnability. In 34th Annual Symposium on Foundations of Computer Science, Palo Alto, California, USA, 3-5 November 1993, pp. 292-301. Cited by: SS1.
* theoretical foundations. Cambridge University Press. Cited by: SS1.
* [4]P. L. Bartlett, O. Bousquet, and S. Mendelson (2002) Localized rademacher complexities. In Computational Learning Theory, 15th Annual Conference on Computational Learning Theory, COLT 2002, Sydney, Australia, July 8-10, 2002, Proceedings, H. Walls and M. A. Walker (Eds.), Lecture Notes in Computer Science, Vol. 2375, pp. 44-58. Cited by: SS1.
* [5]P. L. Bartlett and S. Mendelson (2002) Rademacher and gaussian complexities: risk bounds and structural results. J. Mach. Learn. Res.3, pp. 463-482. Cited by: SS1.
* [6]S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira (2006) Analysis of representations for domain adaptation. In Advances in Neural Information Processing Systems 19, Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 4-7, 2006, pp. 137-144. Cited by: SS1.
* [7]Y. Bengio, A. C. Courville, and P. Vincent (2013) Representation learning: a review and new perspectives. IEEE Trans. Pattern Anal. Mach. Intell.35 (8), pp. 1798-1828. Cited by: SS1.
* European Conference, ECML PKDD 2013, Prague, Czech Republic, September 23-27, 2013, Proceedings, Part III, H. Walls and M. A. Walker (Eds.), Lecture Notes in Computer Science, Vol. 387-402, pp. 387-402. Cited by: SS1.
* [9]Y. Chen, K. G. Jamieson, and S. S. Du (2022) Active multi-task representation learning. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, H. Walls and M. A. Walker (Eds.), Lecture Notes in Computer Science, Vol. 162, pp. 3271-3298. Cited by: SS1.
* [10]J. Cohen, E. Rosenfeld, and J. Z. Kolter (2019) Certified adversarial robustness via randomized smoothing. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, H. Walls and M. A. Walker (Eds.), Lecture Notes in Computer Science, Vol. 97, pp. 1310-1320. Cited by: SS1.
* [11]L. Collins, H. Hassani, M. Soltanolkotabi, A. Mokhtari, and S. Shakkottai (2024) Provable multi-task representation learning by two-layer relu neural networks. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024, Cited by: SS1.
* [12]Y. Deng, N. Gazzagnadou, J. Hong, M. Mahdavi, and L. Lyu (2023) On the hardness of robustness transfer: a perspective from rademacher complexity over symmetric difference hypothesis space. CoRRabs/2302.12351. Cited by: SS1.
** [13] Zhun Deng, Linjun Zhang, Kailas Vodrahalli, Kenji Kawaguchi, and James Y. Zou. Adversarial training helps transfer learning via better representations. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 25179-25191, 2021.
* [14] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In _Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014_, volume 32 of _JMLR Workshop and Conference Proceedings_, pages 647-655. JMLR.org, 2014.
* [15] Simon Shaolei Du, Wei Hu, Sham M. Kakade, Jason D. Lee, and Qi Lei. Few-shot learning via learning the representation, provably. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.
* MICCAI 2017
- 20th International Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings, Part III_, volume 10435 of _Lecture Notes in Computer Science_, pages 516-524. Springer, 2017.
* [17] Justin Khim and Po-Ling Loh. Adversarial risk bounds for binary classification via function transformation. _CoRR_, abs/1810.09519, 2018.
* [18] Xiaofeng Liu, Xiongchang Liu, Bo Hu, Wenxuan Ji, Fangxu Xing, Jun Lu, Jane You, C.-C. Jay Kuo, Georges El Fakhri, and Jonghye Woo. Subtype-aware unsupervised domain adaptation for medical diagnosis. In _Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021_, pages 2189-2197. AAAI Press, 2021.
* [19] Xiaofeng Liu, Chae Hwa Yoo, Fangxu Xing, Hyejin Oh, Georges El Fakhri, Je-Won Kang, and Jonghye Woo. Deep unsupervised domain adaptation: A review of recent advances and perspectives. _CoRR_, abs/2208.07422, 2022.
* [20] Philip M. Long and Hanie Sedghi. Generalization bounds for deep convolutional neural networks. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020.
* May 3, 2018, Conference Track Proceedings_. OpenReview.net, 2018.
* [22] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation with multiple sources. In _Advances in Neural Information Processing Systems 21, Proceedings of the Twenty-Second Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 8-11, 2008_, pages 1041-1048. Curran Associates, Inc., 2008.
* The 22nd Conference on Learning Theory, Montreal, Quebec, Canada, June 18-21, 2009_, 2009.
* [24] Yuren Mao, Weiwei Liu, and Xuemin Lin. Adaptive adversarial multi-task representation learning. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 6724-6733. PMLR, 2020.

* [25] Waleed Mustafa, Yunwen Lei, and Marius Kloft. On the generalization analysis of adversarial learning. In _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 16174-16196. PMLR, 2022.
* 16, 2023_, 2023.
* [27] Maithra Raghu, Chiyuan Zhang, Jon M. Kleinberg, and Samy Bengio. Transfusion: Understanding transfer learning for medical imaging. In _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 3342-3352, 2019.
* [28] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Semidefinite relaxations for certifying robustness to adversarial examples. In _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada_, pages 10900-10910, 2018.
* [29] Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. CNN features off-the-shelf: An astounding baseline for recognition. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR Workshops 2014, Columbus, OH, USA, June 23-28, 2014_, pages 512-519. IEEE Computer Society, 2014.
* [30] Henry W. J. Reeve and Ata Kaban. Optimistic bounds for multi-output prediction. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 8030-8040. PMLR, 2020.
* [31] Mark Rudelson and Roman Vershynin. Combinatorics of random processes and sections of convex bodies. _Annals of Mathematics_, 164:603-648, 2004.
* [32] Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry. Do adversarially robust imagenet models transfer better? In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* [33] Ali Shafahi, Parsa Saadatpanah, Chen Zhu, Amin Ghiasi, Christoph Studer, David W. Jacobs, and Tom Goldstein. Adversarially robust transfer learning. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020.
* [34] Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Smoothness, low noise and fast rates. In _Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia, Canada_, pages 2199-2207. Curran Associates, Inc., 2010.
* [35] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In _2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings_, 2014.
* [36] Nilesh Tripuraneni, Chi Jin, and Michael I. Jordan. Provable meta-learning of linear representations. In _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 10434-10443. PMLR, 2021.
* [37] Nilesh Tripuraneni, Michael I. Jordan, and Chi Jin. On the theory of transfer learning: The importance of task diversity. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.

* [38] Francisco Utrera, Evan Kravitz, N. Benjamin Erichson, Rajiv Khanna, and Michael W. Mahoney. Adversarially-trained deep nets transfer better: Illustration on image classification. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.
* 16, 2023_, 2023.
* [40] Garrett Wilson and Diane J. Cook. A survey of unsupervised deep domain adaptation. _ACM Trans. Intell. Syst. Technol._, 11(5):51:1-51:46, 2020.
* [41] Eric Wong and J. Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In _Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018_, volume 80 of _Proceedings of Machine Learning Research_, pages 5283-5292. PMLR, 2018.
* [42] Xiaojun Xu, Jacky Y. Zhang, Evelyn Ma, Hyun Ho Son, Sanmi Koyejo, and Bo Li. Adversarially robust models may not transfer better: Sufficient conditions for domain transferability from the view of regularization. In _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 24770-24802. PMLR, 2022.
* [43] Yutaro Yamada and Mayu Otani. Does robustness on imagenet transfer to downstream tasks? In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pages 9205-9214. IEEE, 2022.
* [44] Dong Yin, Kannan Ramchandran, and Peter L. Bartlett. Rademacher complexity for adversarially robust generalization. In _Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA_, volume 97 of _Proceedings of Machine Learning Research_, pages 7085-7094. PMLR, 2019.
* [45] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In _Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada_, pages 3320-3328, 2014.
* [46] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan. Theoretically principled trade-off between robustness and accuracy. In _Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA_, volume 97 of _Proceedings of Machine Learning Research_, pages 7472-7482. PMLR, 2019.
* [47] Jiaming Zhang, Jitao Sang, Qi Yi, Yunfan Yang, Huiwen Dong, and Jian Yu. Imagenet pre-training also transfers non-robustness. In _Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023_, pages 3436-3444. AAAI Press, 2023.
* [48] Lijia Zhou, Frederic Koehler, Danica J. Sutherland, and Nathan Srebro. Optimistic rates: A unifying theory for interpolation learning and regularization in linear regression. _CoRR_, abs/2112.04470, 2021.
* [49] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning. _Proc. IEEE_, 109(1):43-76, 2021.

###### Contents

* 1 Introduction
	* 1.1 Some core difficulties and techniques
	* 1.2 Prior work
* 2 Problem setup and preliminaries
* 3 Adversarial multi-task representation learning
	* 3.1 Lipschitz losses
	* 3.2 Smooth and nonnegative losses
* 4 Standard adversarial learning
	* 4.1 Lipschitz losses
	* 4.2 Smooth nonnegative losses in the single-task setting
* 5 Conclusion
* A A remark about attack functions
* B Technical comparisons to prior work
* B.1 Comparison to [13]
* B.2 Comparison to [25]
* B.3 Comparison to [44]
* C Lemmas
* D Vector-valued fat-shattering dimension digression
* E Proofs
* E.1 Proofs in Section 3
* E.2 Proofs in Section 4.1
* E.2.1 Structural Results
* E.2.2 Lipschitz Losses
* E.2.3 Smooth and nonnegative losses
* F NeurIPS Paper Checklist

### Glossary

\(|\hat{\mathfrak{R}}|(\mathcal{Q}^{\otimes p},\bm{Z})\)The data-dependent Rademacher complexity of \(\mathcal{Q}^{\otimes p}\) for \(\bm{Z}=\big{(}z_{j}^{i}\big{)}_{j\in[p],i\in[n]}\), i.e.,

\[|\hat{\mathfrak{R}}|\big{(}\mathcal{Q}^{\otimes p},\bm{Z}\big{)}\coloneqq\mathbb{ E}_{\sigma_{i,j,k}}\Bigg{[}\sup_{\bm{q}\in\mathcal{Q}^{\otimes p}}\Bigg{|} \frac{1}{np}\sum_{i,j,k=1}^{n,p,q}\sigma_{ijk}\big{(}q_{j}\big{(}z_{j}^{i}\big{)} \big{)}_{k}\Bigg{|}\Bigg{]},\]

where \(z_{j}^{i}\in\mathcal{Z}\) and \(\sigma_{i,j,k}\) are i.i.d. Rademacher random variables.

\(\hat{R}_{\mathrm{src}}(f_{0},h,\mathcal{A})\)The empirical adversarial risk for the source tasks.

\(\hat{R}_{\mathrm{tar}}(\bm{f},h,\mathcal{A})\)The empirical adversarial risk for the target task.

\(\Lambda_{\mathcal{A}}(\rho,L,n,\beta)\)The function that maps to

\[\bigg{(}\log\log\!\bigg{|}S_{\mathcal{A}}\bigg{(}\frac{eb}{4c\sqrt{n}L}\bigg{)} \bigg{|}+\frac{c\rho}{\beta}\bigg{)}\Bigg{(}\frac{8}{c}+40\frac{\sqrt{cC}}{c} \sqrt{\log\!\bigg{(}\frac{4c\sqrt{n}}{e}\bigg{|}S_{\mathcal{A}}\bigg{(}\frac{ eb}{4c\sqrt{n}L}\bigg{)}\bigg{|}\bigg{)}}\log\!\bigg{(}\frac{16\rho c\beta n}{e^{2}} \bigg{)}\Bigg{)}.\]

This function encapsulates the cost of being robust to adversarial attacks within our rates.

\(\mathcal{L}_{\mathcal{A}}(\cdot)\)The adversarial loss class:

\(R_{\mathrm{src}}(\bm{f},h,\mathcal{A})\)The adversarial risk for the source tasks.

\(R_{\mathrm{src}}(\bm{f},h,\mathcal{A})\)The adversarial risk for the target task.

\(\mathcal{N}_{p}(\mathcal{Q},\varepsilon,\bm{Z})\)The covering number of \(\mathcal{Q}\) at scale \(\varepsilon\) w.r.t. the empirical \(\left\|\cdot\right\|_{p}\) on \(\bm{Z}\).

\(\hat{\mathfrak{R}}(\mathcal{Q}^{\otimes p},\bm{Z})\)The data-dependent Rademacher width of \(\mathcal{Q}^{\otimes p}\) for \(\bm{Z}=\big{(}z_{j}^{i}\big{)}_{j\in[p],i\in[n]}\), i.e.,

\[\hat{\mathfrak{R}}(\mathcal{Q}^{\otimes p},\bm{Z})\coloneqq\mathbb{E}_{\sigma _{i,j,k}}\Bigg{[}\sup_{\bm{q}\in\mathcal{Q}^{\otimes p}}\frac{1}{np}\sum_{i,j,k=1}^{n,p,q}\sigma_{ijk}\big{(}q_{j}\big{(}z_{j}^{i}\big{)}\big{)}_{k}\Bigg{]},\]

where \(z_{j}^{i}\in\mathcal{Z}\) and \(\sigma_{i,j,k}\) are i.i.d. Rademacher random variables.

\(\operatorname{vc}_{\mathcal{X}}(\mathcal{Q},\varepsilon)\) The fat-shattering dimension of \(\mathcal{Q}\) at scale \(\varepsilon\) with points from \(\mathcal{X}\).

\(S_{\mathcal{A}}(\varepsilon)\) The inflated dataset w.r.t. attacks \(\mathcal{A}\), sample \(S\), and scale \(\varepsilon\). See Section 2.

\(f_{j}^{\star}\) The ground truth predictor for task \(j\).

\(\ell\) The loss function \(\ell:\mathbb{R}\times\mathcal{Y}\to\mathbb{R}\). We also use the notation \(\ell_{y}(x)\coloneqq\ell(x,y)\).

\(\mathcal{L}(\cdot)\) The non-adversarial loss class:

\(\mathcal{L}\big{(}\mathcal{Q}^{\otimes t}\big{)}\coloneqq\big{\{}(x_{1}, \ldots,x_{t})\mapsto((\ell_{y_{1}}\circ q_{1})(x_{1}),\ldots,(\ell_{y_{t}} \circ q_{t})(x_{t}))\mid q\in\mathcal{Q}^{\otimes t}\big{\}}\).

\(\mathcal{A}\) The function class containing the attack functions that are maps from \(\mathcal{X}\) to \(\mathcal{X}\).

An attack function \(A:\mathcal{X}\to\mathcal{X}\) in \(\mathcal{A}\).

The common representation for all tasks.

\(L_{1}\): \(L_{1}\coloneqq L_{\ell}L_{\mathcal{F}}\)

The Lipschitz constant for the predictors \(\mathcal{F}\cup\mathcal{F}_{0}\) w.r.t. \(\|\cdot\|\): a positive \(L_{\mathcal{F}}\) such that \(|f(z_{1})-f(z_{2})|\leq L_{\mathcal{F}}\|z_{1}-z_{2}\|\) for all \(z_{1},z_{2}\in\text{Dom}(f)\) and \(f\in\mathcal{F}\cup\mathcal{F}_{0}\).

The Lipschitz constant for the loss function \(\ell\): a positive \(L_{\ell}\) such that \(|\ell_{y}(y_{1},y)-\ell_{y}(y_{2},y)|\leq L_{\ell}|y_{1}-y_{2}|\) for all \(y_{1},y_{2}\in\mathbb{R},y\in\mathcal{Y}\).

The Lipschitz constant for the representations \(\mathcal{H}\) w.r.t. the norms \((\|\cdot\|_{\mathcal{A}},\|\cdot\|)\): a positive \(L_{\mathcal{H}}\) such that \(\|h(z_{1})-h(z_{2})\|\leq L_{\mathcal{H}}\|z_{1}-z_{2}\|_{\mathcal{A}}\) for all \(z_{1},z_{2}\in\text{Dom}(h)\) and \(h\in\mathcal{H}\).

\(L_{2}\): \(L_{2}\coloneqq L_{\mathcal{F}}L_{\mathcal{H}}\)

The smoothness constant for a smooth loss: a positive \(H\) such that \(\big{|}\ell_{y}^{\prime}(y_{1},y)-\ell_{y}^{\prime}(y_{2},y)\big{|}\leq H|y_{ 1}-y_{2}|\) for all \(y_{1},y_{2}\in\mathbb{R},y\in\mathcal{Y}\).

The dimension of the embedding space, i.e., the dimension of the image of the representations.

The representations class that consists of functions from \(\mathbb{R}^{d}\) to \(\mathbb{R}^{k}\).

A hypothesis class consisting of functions from \(\mathcal{X}\) to \(\mathcal{Y}\).

A hypothesis class for the target task consisting of functions from \(\mathcal{X}\) to \(\mathcal{Y}\).

A the dimension of the space from which the data is drawn.

The input space, i.e., \(\mathcal{X}\subseteq\mathbb{R}^{d}\)

The label space, i.e., \(\mathcal{Y}\subseteq\mathbb{R}\)

The bound on the loss function: a positive \(b\) such that \(|\ell(y^{\prime},y)|\leq b<\infty\) for all \(y^{\prime}\in\mathbb{R},y\in\mathcal{Y}\)

The cartesian product of \(\mathcal{F}\) for \(t\) times.

The number of source tasks.

The number of samples drawn for each source task.

The number of samples drawn for the target task.

## Appendix A A remark about attack functions

While Assumption 4.B is not made in several works on adversarial robustness in the linear setting [4, 13], it is made in at least one other work [25, p. 3]. It seems that the literature has yet to formally made a distinction between these two settings. Therefore, we would like to make a few short remarks about how Assumption 4.B is natural and mild.

First, in practice, \(\mathcal{X}\) is frequently known. Therefore, if one seeks to protect against adversarial attacks, protections can be put in place for inputs that are not in \(\mathcal{X}\). Thus, since the adversary wants to avoid detection, it is reasonable for an adversary to only perturb within the input domain. Second, many attacks in the empirical literature naturally fall into this category, e.g., a sticker placed on a stop sign. In addition, for image classification with \(\mathcal{X}\) as the space of images, the famous examples of images [35] that have been slightly perturbed by definition fall into this category. Also, a key difficulty of being robust to adversarial attacks is caused by the curse of dimensionality. Specifically, in classification, this manifests in the distance from any point to the decision boundary being exceedingly small. This is also true under Assumption 4.B. Finally, in practice, it is common to preprocess the input before passing it to the model, e.g., subtracting the mean, removing outliers, dividing by the standard deviation, and clipping. So, it is a reasonable assumption that the input to the model, adversarial or not, will often be within the same space.

## Appendix B Technical comparisons to prior work

### Comparison to [13]

To our knowledge, the first work to study adversarial attacks in the MTRL regime is [13]. Although we both consider MTRL, there are several differences: they consider classification with representation functions that are orthonormal matrices and the predictors as linear maps, \(\left\lVert\cdot\right\rVert_{\infty}\) attacks, a spectral condition to relate the tasks similar to [15, 36], the loss function \((x,y)\mapsto-yf(x)\), and a sub-Gaussian generative model. Ignoring these differences, they show that the excess transfer risk decays as \(\sqrt{k/m}+\sqrt{k^{2}d/nt}\). In comparison, if we instantiate our function classes as they have, \(C(\mathcal{F})\) is \(\mathcal{O}(k)\) and \(C(\mathcal{H})\) is \(\mathcal{O}(dk^{2})\) (see e.g., [37, p. 7]). Plugging these values into the expression from the asymptotic analysis of the Lipschitz loss, our rate is \(\tilde{\mathcal{O}}(\sqrt{dk/n}+dk/\sqrt{nt}+k/\sqrt{m})\). We observe a \(\sqrt{d}\) and \(\sqrt{k}\) gap between the source tasks and target task rate, respectively, compared to their rate. We believe the additional \(\sqrt{dk/n}\) term in our rate is an artifact of using the Gaussian chain rule [37].

### Comparison to [25]

Ignoring our MTRL results, the paper most similar to our own is [25], which does a substantial analysis of adversarial attacks in the multi-class setting. Like in Appendix B.3, we must be careful due to different assumptions. In particular, our results hold for a subtly different attack model. Let us define their notation for illustrative purposes. They define an attack as \(A:\mathcal{X}\times\mathcal{B}\to\mathcal{X}\) where \(\mathcal{B}\) is the "noise class." They require that \(\delta\mapsto\ell(f(A(x,\delta)),y)\) is \(L\)-Lipschitz for \(\left\lVert\cdot\right\rVert_{\delta}\) and, similar to our own assumption, the existence of a cover of \(\mathcal{B}\). Yet, unlike Lemma 4.4 of [25], we do not require the Lipschitz assumption. Consequently, our analogous lemma, Lemma 1, applies to attack models for which theirs does not. As an example, define an attack \(A(x,\delta)\) to be \(x\) if \(\delta=0\) otherwise \(x+\delta/\|\delta\|_{2}\) where \(\mathcal{B}=\{\delta\ |\ \|\delta\|_{2}\leq 1\}\). The attack is not continuous and, therefore, not Lipschitz. So the Lipschitz assumption \(\delta\mapsto\ell(f(A(x,\delta)),y)\) does not hold for non-trivial losses and predictors.

Ignoring these differences, they state their main generalization bound as, with probability at least \(1-\delta\) over the training data \(S\), for all \(f\in\mathcal{F}\),

\[R_{\mathrm{adv}}(f)-\hat{R}_{\mathrm{adv}}(f)\leq 3\sqrt{\frac{\log(2/ \delta)}{2n}}+\inf_{\alpha>0}\biggl{(}8\alpha+\frac{24}{\sqrt{n}}\int_{\alpha }^{1}\sqrt{\log\mathcal{N}_{\infty}\Bigl{(}\tilde{\mathcal{G}}_{\mathrm{adv} },\frac{\varepsilon}{2},\tilde{S}\Bigr{)}}d\varepsilon\biggr{)},\]

where

\[\tilde{\mathcal{G}}_{\mathrm{adv}}\,=\{(z,\delta)\mapsto\ell(f(A(x,\delta)), y):f\in\mathcal{F}\}\text{ and }\tilde{S}=\Bigl{\{}\Bigl{(}x_{i},\tilde{\delta},y_{i}\Bigr{)}:i\in[n],\tilde{ \delta}\in C_{\mathcal{B}}(\varepsilon/2L)\Bigr{\}}.\]

Their work then proceeds to bound this integral under additional assumptions like instantiating \(\mathcal{F}\) to be a specific model or making parametric assumptions. In comparison, we bound the above integral by Equation (2). We believe this new bound provides new insights into the cost of being adversarially robust that were not clear before because of the additional assumptions made for the analysis. In particular, our work shows that under mild assumptions, for a large attack model, the dimensional dependence attributed to the attack is \(\tilde{O}(\sqrt{d})\).

Now let us make a comparison of our optimistic rate to Theorem 6.8 in [25], which is also an optimistic rate. Their work introduces a robust version of the self-bounding Lipschitz loss [30], of which a smooth loss is a special case. They bound the local Rademacher complexity of this loss class by a sub-root function (see Lemma 6.6) under the assumption that \(\mathcal{N}_{\infty}(\mathcal{L}(\mathcal{F}),\varepsilon/2,S_{A}(\varepsilon/ 2L_{1}))\leq R_{b_{1}}/\varepsilon^{2}\) for \(\varepsilon\in[b_{1},b_{2}]\) and that \(R_{b_{1}}\) does not depend on \(m\). However, due to this parametric assumption, it is unclear what the worst-case dimensional dependence one has to pay before instantiating a model and attack. In comparison, our approach does not require such assumptions, and we can now state that, in our setting, the worst-case dimensional dependence due to the adversary is \(\tilde{O}(\sqrt{d})\).

### Comparison to [44]

Our work is not strictly comparable to [44] due to making different assumptions. They consider linear predictors and \(\left\lVert\cdot\right\rVert_{\infty}\) perturbation attacks, i.e., \(\mathcal{A}=\left\{x\mapsto x+\delta\mid\left\lVert\delta\right\rVert_{\infty }\leq\Delta\right\}\). This can imply that if \(x\in\mathcal{X}\) it is not necessarily true that \(A(x)\in\mathcal{X}\) for some \(A\in\mathcal{A}\) which violates Assumption 4.B. Ignoring this important difference, we instantiate \(\mathcal{F}\) to be \(\left\lVert\cdot\right\rVert_{p}\)-bounded linear predictors and \(\mathcal{X}\) to be \(\left\lVert\cdot\right\rVert_{q}\)-bounded, where \(p,q\) are Holder-conjugates. Note Equation (2) has dimensionality dependence of \(\tilde{\mathcal{O}}(\sqrt{d})\) because, after applying Talagrand's contraction inequality, \(\left\lvert\tilde{\mathfrak{R}}\right\rvert(\mathcal{L}(\mathcal{F}),n)\) has at most logarithmic dimensionality dependence for \(p=1\) or \(p=2\), In contrast, they have a dimensional dependence of \(d^{1/q}\). So, ignoring logs, we match their bound when \(p=2\), and our bound is worse by \(\sqrt{d}\) when \(p=1\). Further, these two bounds behave differently as a function of the perturbation distance. We have a logarithmic dependence in \(\Delta\), whereas they have linear dependence.

## Appendix C Lemmas

In this section, we list several lemmas from prior work that we use in our proofs.

**Lemma 4** (Lemma A.3. in [34]).: _For any hypothesis class \(\mathcal{F}\), any sample size \(n\) and any \(\varepsilon>\left\lvert\tilde{\mathfrak{R}}\right\rvert(\mathcal{F},n)\) we have that_

\[\operatorname{vc}_{\mathcal{X}}(\mathcal{F},\varepsilon)\leq\frac{4n|\tilde{ \mathfrak{R}}|(\mathcal{F},n)^{2}}{\varepsilon^{2}}\]

**Lemma 5** (Lemma B.1 in [34]).: _For any \(H\)-smooth nonnegative function \(f:\mathbb{R}\mapsto\mathbb{R}\) and any \(t,r\in\mathbb{R}\) we have that_

\[(f(t)-f(r))^{2}\leq 6H(f(t)+f(r))(t-r)^{2}.\]

**Lemma 6** (Lemma A.1. in [34]).: _For any function class \(\mathcal{F}\) containing functions \(f:\mathcal{X}\mapsto\mathbb{R}\) and \(S=(x_{1},\ldots,x_{n})\), we have that_

\[\hat{\mathfrak{R}}(\mathcal{F},n)\leq\inf_{\alpha\geq 0}\left\{4\alpha+10\int_{ \alpha}^{\sup_{f\in\mathcal{F}}\sqrt{\mathbb{E}[f^{2}]}}\sqrt{\frac{\log \mathcal{N}_{2}(\mathcal{F},\varepsilon,S)}{n}}d\varepsilon\right\}\]

## Appendix D Vector-valued fat-shattering dimension digression

To our knowledge, the literature has not yet defined a notion of fat-shattering dimension suitable for vector-valued classes. In this section, we extend the fat-shattering dimension [2] to vector-valued functions. Let \(\mathcal{Q}=\{\boldsymbol{q}=(g_{1},\ldots,g_{t})\}\) be a class of vector valued functions and \(S\coloneqq(x_{j}^{i})_{(j,i)=(1,1)}^{(t,n_{j})}\) be points in the domain of the coordinates of \(\mathcal{Q}\), where \(n_{j}\) are positive integers. We say that \(S\) is \(\gamma\)-shattered by \(\mathcal{Q}\) if there exists reals \(r_{j}^{i}\) for \(j\in[t]\) and \(i\in[n_{j}]\) such that for all \(b\in\{0,1\}^{\sum_{j}n_{j}}\) there is a \(\boldsymbol{q}_{|_{b}}\in\mathcal{Q}\) such that

\[\boldsymbol{g}_{|_{b}}(x_{j}^{i}) \geq r_{j}^{i}+\gamma\text{ if }b_{j}^{i}=1\text{ and }\] \[\boldsymbol{g}_{|_{b}j}(x_{j}^{i}) \leq r_{j}^{i}-\gamma\text{ if }b_{j}^{i}=0.\]

Let \(\operatorname{vc}_{\mathcal{X}}(\mathcal{Q},\gamma)\) be the cardinality of the largest set \(\gamma\)-shattered.

Indeed, the authors proved foundational lemmas of the form found in [2, 3] in an effort to show Theorem 3 before realizing the lifting argument. In fact, note that \(\operatorname{vc}_{\mathcal{X}}(\mathcal{Q},\gamma)=\operatorname{vc}_{ \mathcal{X}\times\mathbb{N}}\!\left(\tilde{\mathcal{Q}},\gamma\right)\), where the left is the vector-valued fat-shattering dimension and the right is the real-valued fat-shattering dimension with \(\tilde{\mathcal{Q}}\) being defined as in the proof of Theorem 3.

This definition has several immediate desirable constructionist properties. Let \(\mathcal{F}\) be a real-valued function class whose domain is \(\mathcal{X}\).

1. If \(\mathrm{vc}_{\mathcal{X}}(\mathcal{F},\gamma)=d\), then \(\mathrm{vc}_{\mathcal{X}}(\mathcal{F}^{\otimes t},\gamma)=td\): use the same sample that shattered the real-valued function class \(t\) times.
2. If \(\mathcal{Q}\) is a vector valued function class and \(\mathrm{vc}_{\mathcal{X}}(\mathcal{Q},\gamma)=d\), the function class restricted to one coordinate has fat-shattering dimension \(\lceil d/t\rceil\) by the pigeonhole principle.

Although suitable for multi-task learning, the class of functions defined above is a strict subset of classes defined for multi-output prediction [30]. The fat-shattering dimension is a special case, i.e., one-dimensional, of the combinatorial dimension. In the multi-output setting, we conjecture that the more general notion of combinatorial dimension is the correct characterization.

## Appendix E Proofs

### Proofs in Section 3

Here we give some proofs not provided in the main body of the paper.

**Theorem 1**.: _Let \(\hat{h}\) and \(\hat{f}_{0}\) be the learned representation and target predictor, as described Algorithm 1. Under Assumption 1 and that \(\bm{f}^{\star}\) is \((\nu,\varepsilon,\mathcal{A})\)-diverse over \(\mathcal{F}_{0}\) w.r.t. \(h^{\star}\), then, with probability at least \(1-2\delta\), we have that \(R_{\mathrm{tar}}\big{(}\hat{f}_{0},\hat{h},\mathcal{A}\big{)}-R_{\mathrm{tar} }(f_{0}^{\star},h^{\star},\mathcal{A})\) is bounded by_

\[\nu^{-1}(8\hat{\mathfrak{R}}(\mathcal{L}_{\mathcal{A}}\big{(} \mathcal{F}^{\otimes t}(\mathcal{H})\big{)},n)+8b\sqrt{\log(2/\delta)/nt})+8 \sup_{h\in\mathcal{H}}\hat{\mathfrak{R}}(\mathcal{L}_{\mathcal{A}}(\mathcal{F} _{0}\circ h),m)+8b\sqrt{\log(2/\delta)/m}+\varepsilon.\]

The proof proceeds exactly as the main generalization bound within in [37, pp. 12-14], we note that the adversarial attack does not change that the assumption that the loss is bounded and Lipschitz. Therefore, two assumptions prevent us from applying this result directly. First, we use \((\nu,\varepsilon,\mathcal{A})\)-diversity instead of \((\nu,\varepsilon)\)-diversity. Second, we use the two-stage adversarial training ERM instead of the non-adversarial two-stage ERM. If we follow the same structure of the proof in the non-adversarial setting, these differences are of no consequence. Indeed, if we apply the traditional risk decomposition, utilize the definition of ERMs, use symmetrization on both the source tasks and target task, and, finally, apply the diversity assumption to connect the source tasks to the target tasks, this completes the proof.

**Theorem 2**.: _Under the setting of Theorem 1 along with Assumption 3, Assumption 4, \(|S_{\mathcal{A}}\big{(}eb/4c\sqrt{nt}L_{3}\big{)}|,|S_{\mathcal{A}}(eb/4c\sqrt {mL}_{1})|\geq e^{\varepsilon}\), \(\mathrm{vc}_{\mathcal{X}}(\mathcal{L}(\mathcal{F}\circ\mathcal{H}),\beta_{1}b ),\mathrm{vc}_{\mathcal{X}}(\mathcal{L}(\mathcal{F}_{0}\circ\mathcal{H}), \beta_{2}b)\geq 1\), then the Rademacher complexities \(\hat{\mathfrak{R}}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}^{\otimes t}( \mathcal{H})),n)\) and \(\hat{\mathfrak{R}}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}_{0}\circ h),m)\) in Theorem 1 are, respectively, bounded by_

\[2|\hat{\mathfrak{R}}|\big{(}\mathcal{L}\big{(}\mathcal{F}^{\otimes t}( \mathcal{H})\big{)},n\big{)}\Lambda_{\mathcal{A}}^{\mathrm{src}}(2^{-1},L_{3}, nt,\beta_{1})\text{ and }\text{ }2\sup_{h\in\mathcal{H}}|\hat{\mathfrak{R}}|(\mathcal{L}(\mathcal{F}_{0}\circ h ),m)\Lambda_{\mathcal{A}}^{\mathrm{tar}}\big{(}2^{-1},L_{1},m,\beta_{2}\big{)},\] (1)

_where \(L_{3}=L_{\ell}L_{\mathcal{F}}L_{\mathcal{H}}\), \(L_{1}=L_{\ell}L_{\mathcal{F}}\), and \(C,c\) are absolute constants._

Proof of Theorem 2.: Given Theorem 1, all that remains is to bound adversarial Rademacher complexity of the source tasks and target task. We will bound the Rademacher complexity for the target task. The proof for the source tasks complexity is similar, but we do not push forward the geometry of the attack into the embedding space, i.e., for the source tasks, we must only invoke Theorem 3.

Recall by Assumption 3.B we have that \(\mathcal{H}\) is \((\left\lVert\cdot\right\rVert_{\mathcal{A}},\left\lVert\cdot\right\rVert)\) Lipschitz. Also, by Assumption 4.A, \(\mathcal{A}(x)\) is totally bounded for all \(x\in\mathcal{X}\) w.r.t \(\left\lVert\cdot\right\rVert_{\mathcal{A}}\). Lipschitz functions are uniformly continuous, and uniformly continuous functions preserve the property of being totally bounded. So, \(\hat{h}\circ\mathcal{A}(x)\) is totally bounded for all \(x\in\mathcal{X}\) w.r.t \(\left\lVert\cdot\right\rVert\). Observe that \(\max_{h\in\mathcal{H}}\hat{\mathfrak{R}}(\mathcal{L}_{\mathcal{A}}(\mathcal{F }_{0}\circ h),m)=\max_{h\in\mathcal{H}}\hat{\mathfrak{R}}(\mathcal{L}_{h \circ\mathcal{A}}(\mathcal{F}_{0},m),\) i.e., we can take the perspective that \(\hat{h}\circ\mathcal{A}\) is the attack function class. Now, we apply Theorem 3 with \(\mathcal{H}\) containing only the identity and \(t=1\), we have \(\max_{h\in\mathcal{H}}\hat{\mathfrak{R}}(\mathcal{L}_{h\circ\mathcal{A}}( \mathcal{F}_{0}),m)\leq 2\sup_{h\in\mathcal{H}}\hat{\mathfrak{R}}|(\mathcal{L}( \mathcal{F}_{0}\circ h),m)\Lambda_{\mathcal{A}}^{\mathrm{tar}}\big{(}2^{-1},L_{1},m,\beta_{2}\big{)}\). The Rademacher complexity is still a function of \(h\) because the worst-case Rademacher complexity is in terms of the image of \(h\) on \(\mathcal{X}\). 

**Theorem 3**.: _Under the setting of Theorem 2, \(\hat{\mathfrak{R}}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}^{\otimes t}(\mathcal{H}) ),S)\) is bounded by \(2|\hat{\mathfrak{R}}|(\mathcal{L}(\mathcal{F}^{\otimes t}(\mathcal{H})),n) \Lambda_{\mathcal{A}}^{\mathrm{src}}\big{(}2^{-1},L_{3},nt,\beta_{1}\big{)},\) where \(L_{3}=L_{\ell}L_{\mathcal{F}}L_{\mathcal{H}}\)._Proof.: We observe that \(\mathfrak{R}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}^{\otimes t}(\mathcal{H})),S)= \mathfrak{R}(\mathcal{L}_{\mathcal{L}}(\tilde{\mathcal{Q}}),\tilde{S})\) and \(\mathfrak{R}(\mathcal{L}(\mathcal{F}^{\otimes t}(\mathcal{H})),S)=\mathfrak{R} (\mathcal{L}\Big{(}\tilde{\mathcal{Q}}\Big{)},\tilde{S}),\) where \(T\in[t]\) and \(\tilde{S}\coloneqq\big{\{}(x_{1}^{\dagger},1),\ldots,(x_{t}^{\dagger},1), \ldots,(x_{t}^{\dagger},t),\ldots,(x_{t}^{\dagger},t)\big{\}}\), \(\tilde{\mathcal{Q}}\coloneqq\Big{\{}x,T\mapsto\sum_{j}^{t}\mathbbm{1}_{T=j}f_{ j}(x)\mid\bm{f}\circ h\in\mathcal{F}^{\otimes t}(\mathcal{H})\Big{\}}\), \(\tilde{\mathcal{A}}\coloneqq\{x,T\mapsto A(x)|A\in\mathcal{A}\}\), and \(\tilde{S}_{\mathcal{A}}(\cdot)\coloneqq S_{\mathcal{A}}(\cdot)\).

We consider the new lifted component of \(\tilde{S}\) as immutable. That is, it does not change when taking worst-case Rademacher complexity. We have reduced our problem to the standard single-task setting, and we can apply our result below Theorem 7, which gives the result. 

**Theorem 4**.: _Let \(\hat{h}\) and \(\hat{f}_{0}\) be the learned representation and target predictor, as described Algorithm 1. Let \(\psi_{1}\) and \(\psi_{2}\) be sub-root functions such that \(\psi_{1}(r)\geq b\mathfrak{R}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}^{\otimes t }(\mathcal{H})\mid_{r}),n)\) and \(\psi_{2}(r)\geq b\sup_{h\in\mathcal{H}}\mathfrak{R}(\mathcal{L}_{\mathcal{A}}( \mathcal{F}_{0}\circ h\mid_{r}),m)\) with \(r_{1}^{\star}\) and \(r_{2}^{\star}\) the fixed points of \(\psi_{1}(r)\) and \(\psi_{2}(r)\), respectively. Under Assumption 2 and that \(\bm{f}^{\star}\) is \((\nu,\varepsilon,\mathcal{A})\)-diverse over \(\mathcal{F}_{0}\) w.r.t. \(h^{\star}\), then, with probability at least \(1-2e^{-\delta}\), we have that \(R_{\mathrm{tar}}\Big{(}\hat{f}_{0},\hat{h},\mathcal{A}\Big{)}-R_{\mathrm{tar} }(f_{0}^{\star},h^{\star},\mathcal{A})\) is bounded by,_

\[\sqrt{R_{\mathrm{tar}}(f_{0}^{\star},h^{\star},\mathcal{A})}\Bigg{(} 9\sqrt{\frac{b\delta}{m}}+219\sqrt{\frac{r_{1}^{\star}}{b}}\Bigg{)}+\frac{171b \delta}{m}+\frac{21967r_{1}^{\star}}{2b}\] \[+\frac{1}{\nu}\Bigg{(}\sqrt{R_{\mathrm{src}}(\bm{f}^{\star},h^{ \star},\mathcal{A})}\Bigg{(}6\sqrt{\frac{b\delta}{nt}}+146\sqrt{\frac{r_{2}^{ \star}}{b}}\Bigg{)}+\frac{102b\delta}{nt}+\frac{217r_{2}^{\star}}{b}.\Bigg{)}+\varepsilon.\]

The remarks made after Theorem 1 are equally applicable here. That is, the proof proceeds exactly as the main generalization bound within in [39, pp. 32-34], because the adversarial loss does not impact the argument. Besides the usual risk decomposition and symmetrization, the additional step is showing a result analogous to Theorem 12 in [39]. Yet, the proof of this theorem is first applying Bernstein inequality, then doing routine computations, so this too goes through. Therefore, a routine argument shows the above result.

### Proofs in Section 4.1

In this section we give proofs not in the main text of the paper.

#### e.2.1 Structural Results

In this section, we give some of our structural results foundational to our analysis of both Lipschitz losses and smooth nonnegative losses. The following two lemmas are essentially the complete and generalized argument given in Proof Sketch 5.

**Lemma 7**.: _Let \(s(\cdot)\) be a function from the reals to finite subsets of \(\mathcal{X}\) such that if \(\varepsilon_{1}<\varepsilon_{2}\) we have that \(|s(\varepsilon_{2})|\leq|s(\varepsilon_{1})|\). Let \(\varepsilon,\kappa,\lambda,\rho\) be positive reals and \(n\) a positive integer. Under Assumption 1 along with \(\max\{\alpha_{1},\alpha_{2}\}\leq\varepsilon\leq\kappa\), \(\Big{|}s(\frac{cb}{4c\sqrt{n}\rho\lambda})\Big{|}\geq e^{e}\) and \(\operatorname{vc}_{\mathcal{X}}(\mathcal{L}(\mathcal{F}),\beta b)\geq 1\), then_

\[\log\mathcal{N}_{\infty}\Big{(}\mathcal{L}(\mathcal{F}),\rho\varepsilon,\Big{|} s\Big{(}\frac{\varepsilon}{\lambda}\Big{)}\Big{|}\Big{)}\leq\max\biggl{\{} \frac{1}{\xi_{1}^{2}},\frac{1}{\xi_{2}^{2}}\biggr{\}}4Cen|\mathfrak{R}|( \mathcal{F},n)^{2}\biggl{(}\frac{1}{c\varepsilon\rho}\biggr{)}^{2}\log\biggl{(} \frac{4c\sqrt{n}}{e}\bigg{|}s\biggl{(}\frac{cb}{4c\sqrt{n}\rho\lambda}\biggr{)} \bigg{|}\biggr{)}\]

_where_

* \(\xi_{1}\coloneqq 1/\log\log\Bigl{|}s\Big{(}\frac{cb}{4c\sqrt{n}\rho\lambda} \Big{)}\Big{|}\)_,_
* \(\xi_{2}\coloneqq b\beta/c\kappa\rho\)_,_
* \(\alpha_{1}\coloneqq\frac{|\mathfrak{R}|(\mathcal{F},n)}{c\xi_{1}\rho},\)__
* \(\alpha_{2}\coloneqq\frac{|\mathfrak{R}|(\mathcal{F},n)}{c\xi_{2}\rho}\)_,_
* _and_ \(c,C\) _are absolute constants._Proof.: We first apply Lemma 2. That is, for \(\xi>0\) we have

\[\log\mathcal{N}_{\infty}\Big{(}\mathcal{L}(\mathcal{F}),\rho\varepsilon,s\Big{(} \frac{\varepsilon}{\lambda}\Big{)}\Big{)}\leq C\underbrace{\mathrm{vc}_{ \mathcal{X}}(\mathcal{L}(\mathcal{F}),c\xi\rho\varepsilon)}_{A}\underbrace{ \log\Biggl{(}\frac{b\big{|}s\big{(}\frac{\varepsilon}{\lambda}\big{)}\big{|}} {\rho\varepsilon\mathrm{vc}_{\mathcal{X}}(\mathcal{L}(\mathcal{F}),c\xi\rho \varepsilon)}\Biggr{)}\log^{\xi}\Biggl{(}\frac{\big{|}s\big{(}\frac{ \varepsilon}{\lambda}\big{)}\big{|}}{\mathrm{vc}_{\mathcal{X}}(\mathcal{L}( \mathcal{F}),c\xi\rho\varepsilon)}\Biggr{)}}_{\xi}\]

We will ensure that this expression is well defined by enforcing bounds on various variables e.g. that the fat-shattering dimension is greater than one. Yet, assume for now that everything is well behaved.

First, to bound \(A\), to apply Lemma 4 we need that

\[|\hat{\mathfrak{R}}|(\mathcal{F},n)\leq c\xi\rho\varepsilon\]

so, with some abuse of notation, we will set

\[\alpha\coloneqq\frac{|\hat{\mathfrak{R}}|(\mathcal{F},n)}{c\xi\rho}\]

Now, applying Lemma 4, we have

\[A\leq 4n|\hat{\mathfrak{R}}|(\mathcal{F},n)^{2}\biggl{(}\frac{1}{c\xi\rho \varepsilon}\biggr{)}^{2}.\]

Now to bound B, we want to bound these log factors by a function that is constant in \(\varepsilon\) and \(\alpha\).

We use that, by Khintchine's inequality, \(\alpha>\frac{eb}{4c\sqrt{n}\xi\rho}\).

Using this inequality and since \(\alpha\leq\varepsilon\) and \(\big{|}s\big{(}\frac{\varepsilon}{\lambda}\big{)}\big{|}\leq\big{|}s\big{(} \frac{\alpha}{\lambda}\big{)}\big{|}\leq\Big{|}s\Big{(}\frac{eb}{4c\sqrt{n}\xi \rho\lambda}\Big{)}\Big{|}\) and \(\mathrm{vc}_{\mathcal{X}}(\mathcal{L}(\mathcal{F}),c\xi\rho\kappa)\leq\mathrm{vc }_{\mathcal{X}}(\mathcal{L}(\mathcal{F}),c\xi\rho\varepsilon)\), we can simplify the log factors, which implies

\[B\leq\log\Biggl{(}\frac{4c\sqrt{n}\xi\Big{|}s\Big{(}\frac{eb}{4c\sqrt{n}\xi \rho\lambda}\Big{)}\Big{|}}{e\mathrm{vc}_{\mathcal{X}}(\mathcal{L}(\mathcal{F }),c\xi\rho\kappa)}\Biggr{)}\log^{\xi}\Biggl{(}\frac{\big{|}s\Big{(}\frac{eb}{4 c\sqrt{n}\xi\rho\lambda}\Big{)}\Big{|}}{\mathrm{vc}_{\mathcal{X}}(\mathcal{L}( \mathcal{F}),c\xi\rho\kappa)}\Biggr{)}.\]

Combining both of these arguments gives

\[\log\mathcal{N}_{\infty}\Big{(}\mathcal{L}(\mathcal{F}),\rho\varepsilon,s \Big{(}\frac{\varepsilon}{\lambda}\Big{)}\Big{)}\leq C4n|\hat{\mathfrak{R}}|( \mathcal{F},n)^{2}\biggl{(}\frac{1}{c\xi\rho\varepsilon}\biggr{)}^{2}\log \Biggl{(}\frac{4c\sqrt{n}\xi\Big{|}s\Big{(}\frac{eb}{4c\sqrt{n}\xi\rho\lambda} \Big{)}\Big{|}}{e\mathrm{vc}_{\mathcal{X}}(\mathcal{L}(\mathcal{F}),c\xi\rho \kappa)}\Biggr{)}\log^{\xi}\Biggl{(}\frac{\big{|}s\Big{(}\frac{eb}{4c\sqrt{n} \xi\rho\lambda}\Big{)}\big{|}}{\mathrm{vc}_{\mathcal{X}}(\mathcal{L}(\mathcal{ F}),c\xi\rho\kappa)}\Biggr{)}.\]

Let \(\xi_{M}=\arg\max_{\xi\in(0,\infty)}\{\xi\ |\ \mathrm{vc}_{\mathcal{X}}( \mathcal{L}(\mathcal{F}),c\xi\rho\kappa)\geq 1\}\).

Let \(\xi_{1}\coloneqq 1/\log\log\Bigl{|}s\Big{(}\frac{eb}{4c\sqrt{n}\rho\lambda} \Big{)}\Big{|}\). Note that \(s\Big{(}\frac{eb}{4c\sqrt{n}\rho\lambda}\Big{)}\) is sufficiently large by assumption for this expression to be well defined.

**Case #1**: Suppose \(\xi_{1}\in(0,\xi_{M}]\). Using this value for the expression we derived above we have that

\[\log\mathcal{N}_{\infty}\Big{(}\mathcal{L}(\mathcal{F}),\rho\varepsilon,s\Big{(} \frac{\varepsilon}{\lambda}\Big{)}\Big{)}\leq C4n|\hat{\mathfrak{R}}|( \mathcal{F},n)^{2}\biggl{(}\frac{1}{c\xi_{1}\rho\varepsilon}\biggr{)}^{2}\log \Biggl{(}\frac{4c\sqrt{n}\xi_{1}\Big{|}s\Big{(}\frac{eb}{4c\sqrt{n}\xi_{1}\rho \lambda}\Big{)}\Big{|}}{e\mathrm{vc}_{\mathcal{X}}(\mathcal{L}(\mathcal{F}),c \xi_{1}\rho\kappa)}\Biggr{)}\log^{\xi_{1}}\Biggl{(}\frac{\big{|}s\Big{(}\frac{ eb}{4c\sqrt{n}\xi_{1}\rho\lambda}\Big{)}\big{|}}{\mathrm{vc}_{\mathcal{X}}( \mathcal{L}(\mathcal{F}),c\xi_{1}\rho\kappa)}\Biggr{)}.\]

Focusing on the log factors again, we can use that \(\xi_{1}\in(0,\xi_{M}]\) to simplify with

\[\log\mathcal{N}_{\infty}\Big{(}\mathcal{L}(\mathcal{F}),\rho\varepsilon,s\Big{(} \frac{\varepsilon}{\lambda}\Big{)}\Big{)}\leq C4n|\hat{\mathfrak{R}}|(\mathcal{ F},n)^{2}\biggl{(}\frac{1}{c\xi_{1}\rho\varepsilon}\biggr{)}^{2}\log \biggl{(}\frac{4c\sqrt{n}\xi_{1}}{e}\bigg{|}s\biggl{(}\frac{eb}{4c\sqrt{n}\xi_{1} \rho\lambda}\biggr{)}\bigg{|}\biggr{)}\log^{\xi_{1}}\biggl{(}\bigg{|}s\biggl{(} \frac{eb}{4c\sqrt{n}\xi_{1}\rho\lambda}\biggr{)}\bigg{|}\biggr{)}.\]Furthermore, since \(\xi_{1}\leq 1\) we can simplify further,

\[\log\!\left(\frac{4c\sqrt{n}\xi_{1}}{e}\bigg{|}s\!\left(\frac{eb}{4c\sqrt{n}\xi_{1} \rho\lambda}\right)\bigg{|}\right)\log^{\xi_{1}}\!\left(\bigg{|}s\!\left(\frac{ eb}{4c\sqrt{n}\xi_{1}\rho\lambda}\right)\bigg{|}\right)\leq\log\!\left(\frac{4c \sqrt{n}}{e}\bigg{|}s\!\left(\frac{eb}{4c\sqrt{n}\rho\lambda}\right)\bigg{|} \right)\log^{\xi_{1}}\!\left(\bigg{|}s\!\left(\frac{eb}{4c\sqrt{n}\rho\lambda} \right)\bigg{|}\right)\!.\]

Finally,

\[\log^{\xi_{1}}\!\left(\bigg{|}s\!\left(\frac{eb}{4c\sqrt{n}\rho\lambda}\right) \bigg{|}\right)=e\]

shows that

\[\log\mathcal{N}_{\infty}\!\left(\mathcal{L}(\mathcal{F}),\rho\varepsilon,s \!\left(\frac{\varepsilon}{\lambda}\right)\right)\leq 4Cen|\widehat{\mathfrak{R}}|( \mathcal{F},n)^{2}\!\left(\frac{1}{c\xi_{1}\rho\varepsilon}\right)^{2}\log\! \left(\frac{4c\sqrt{n}}{e}\bigg{|}s\!\left(\frac{eb}{4c\sqrt{n}\rho\lambda} \right)\bigg{|}\right)\]

**Case #2**: Suppose \(\xi_{1}\not\in(0,\xi_{M}]\). Recall that \(\mathrm{vc}_{\mathcal{X}}(\mathcal{L}(\mathcal{F}),b\beta)\geq 1\). This implies that \(\xi_{2}=b\beta/c\rho\kappa\in(0,\xi_{M})\). Also by monotonicity of fat-shattering dimension we have that \(\xi_{2}<\xi_{1}\leq 1\). This implies that we can make the same argument as we made for the first case with the slight modification that

\[\log^{\xi_{1}}\!\left(\bigg{|}s\!\left(\frac{eb}{4c\sqrt{n}\rho\lambda}\right) \bigg{|}\right)<e,\]

which doesn't change the final result.

Taking the maximum over both the cases shows that

\[\log\mathcal{N}_{\infty}\!\left(\mathcal{L}(\mathcal{F}),\rho\varepsilon, \Big{|}s\!\left(\frac{\varepsilon}{\lambda}\right)\Big{|}\right)\leq\max\! \left\{\frac{1}{\xi_{1}^{2}},\frac{1}{\xi_{2}^{2}}\right\}\!4Cen|\widehat{ \mathfrak{R}}|(\mathcal{F},n)^{2}\!\left(\frac{1}{c\varepsilon\rho}\right)^{2 }\log\!\left(\frac{4c\sqrt{n}}{e}\bigg{|}s\!\left(\frac{eb}{4c\sqrt{n}\rho \lambda}\right)\bigg{|}\right)\]

The next theorem uses the comparison inequality given in Lemma 7 inside Dudley's integral along various observations about the quantities involved to bound the desired integral.

**Lemma 8** (A Dudley's integral computation).: _Let \(s(\cdot)\) be a function from the reals to finite subsets of \(\mathcal{X}\) such that if \(\varepsilon_{1}<\varepsilon_{2}\) we have that \(|s(\varepsilon_{2})|\leq|s(\varepsilon_{1})|\). Let \(\varepsilon,\kappa,\lambda,\rho\) be positive reals and \(n\) a positive integer. Under Assumption 1 along with \(\Big{|}s\!\left(\frac{eb}{4c\sqrt{n}\rho\lambda}\right)\Big{|}\geq e^{e}\) and \(\mathrm{vc}_{\mathcal{X}}(\mathcal{L}(\mathcal{F}),\beta b)\geq 1\), then_

\[\inf_{\alpha>0}\bigg{\{}4\alpha+\frac{10}{\sqrt{n}}\int_{\alpha}^{\kappa}\sqrt{ \log\mathcal{N}_{\infty}\!\left(\mathcal{L}(\mathcal{F}),\rho\varepsilon,s \!\left(\frac{\varepsilon}{\lambda}\right)\right)}\;d\varepsilon\bigg{\}}\]

_is bounded by_

\[|\widehat{\mathfrak{R}}|(\mathcal{F},n)\!\left(\log\log s\!\left(\frac{eb}{4c \sqrt{n}\rho\lambda}\right)+\frac{c\kappa\rho}{\beta b}\right)\!\left(\frac{ 4}{c\rho}+20\frac{\sqrt{cC}}{c\rho}\sqrt{\log\!\left(\frac{4c\sqrt{n}}{e}s\! \left(\frac{eb}{4c\sqrt{n}\lambda\rho}\right)\right)}\log\!\left(\frac{16c\beta \rho\kappa n}{e^{2}b}\right)\right)\!,\]

_where \(c,C\) are absolute constants._

Proof of Lemma 8.: Starting with

Focusing on the integrand, we now apply Lemma 7.

[MISSING_PAGE_FAIL:24]

Now fix \(\varepsilon>0\). Now let \(C_{\mathcal{A}}(\varepsilon/2L_{1})\) be a cover of \(\mathcal{A}\) at scale \(\varepsilon/2L_{1}\) w.r.t. \(\left\|\cdot\right\|_{\mathcal{A}}\) and our sample \(S\). Let \(\tilde{A_{i}}\) be in the cover above and \(\varepsilon/2L_{1}\) close to the attack on \(x^{i}\), so

\[\max_{i\in[n]}\max_{A\in\mathcal{A}}\Bigl{|}L_{1}\Bigl{\|}A(x^{i})-\tilde{A}_{ i}(x^{i})\Bigr{\|}_{\mathcal{A}}\Bigr{|}\leq\frac{\varepsilon}{2}.\]

Now construct an \(\left\|\cdot\right\|_{\infty}\) cover of \(\mathcal{L}(\mathcal{F})\) at radius \(\varepsilon/2\) on the image of all \(C_{\mathcal{A}}(\varepsilon/2L_{1})\) on \(S\). Call this set \(S_{\mathcal{A}}\Bigl{(}\frac{\varepsilon}{2L_{1}}\Bigr{)}\), which constructs our "inflated sample". Now we pick the \(\tilde{f}\) to be \(\varepsilon/2\) close to \(f\) on our inflated sample. Therefore, by definition, we have,

\[\max_{i\in[n]}\max_{A\in\mathcal{A}}\Bigl{|}\Bigl{(}\ell_{y^{i}}\circ f\circ \tilde{A_{i}}\Bigr{)}(x^{i})-\Bigl{(}\ell_{y^{i}}\circ\tilde{f}\circ\tilde{A_ {i}}\Bigr{)}(x^{i})\Bigr{|}\leq\frac{\varepsilon}{2}\]

So the Equation (4) is upper bounded by \(\varepsilon\).

Therefore we have shown a appropriate cover of \(\mathcal{L}(\mathcal{F})\) on the specified larger sample is a cover of our adversarial loss class. More formally,

\[\mathcal{N}_{\infty}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}),\varepsilon,S) \leq\mathcal{N}_{\infty}\Bigl{(}\mathcal{L}(\mathcal{F}),\frac{\varepsilon}{ 2},S_{\mathcal{A}}\biggl{(}\frac{\varepsilon}{2L_{1}}\Bigr{)}\Bigr{)},\]

which shows the result.

**Theorem 7**.: _Let \(\mathcal{F}\) be \(L_{\mathcal{F}}\)-Lipschitz w.r.t. \(\left\|\cdot\right\|_{\mathcal{A}}\). Under Assumption 1, Assumption 4, \(|S_{\mathcal{A}}(eb/4c\sqrt{n}L_{1})|\geq e^{e}\), \(\mathrm{vc}_{\mathcal{A}}(\mathcal{L}(\mathcal{F}),\beta b)\geq 1\), then \(\mathfrak{R}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}),S)\) is bounded by_

\[2|\mathfrak{R}|(\mathcal{L}(\mathcal{F}),n)\Lambda_{\mathcal{A}}\bigl{(}2^{-1 },L_{1},n,\beta\bigr{)},\] (2)

_where \(L_{1}=L_{\ell}L_{\mathcal{F}}\) and \(C,c\) are absolute constants._

Proof of Theorem 7.: Applying Lemma 6 and Lemma 1 shows the following.

\[\mathfrak{R}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}),S) \leq 4\alpha+\frac{10}{\sqrt{n}}\int_{\alpha}^{b}\sqrt{\log \mathcal{N}_{2}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}),\varepsilon,S)}\;d\varepsilon\] \[\leq 4\alpha+\frac{10}{\sqrt{n}}\int_{\alpha}^{b}\sqrt{\log \mathcal{N}_{\infty}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}),\varepsilon,S)}\;d\varepsilon\] \[\leq 4\alpha+\frac{10}{\sqrt{n}}\int_{\alpha}^{b}\sqrt{\log \mathcal{N}_{\infty}\biggl{(}\mathcal{L}(\mathcal{F}),\frac{\varepsilon}{2},S _{\mathcal{A}}\biggl{(}\frac{\varepsilon}{2L_{1}}\biggr{)}\biggr{)}}\;d\varepsilon\] (5)

Now using Lemma 8 with \(\rho=1/2\) and \(\lambda=2L_{1}\) and \(\kappa=b\) and \(s(\cdot)=S_{\mathcal{A}}(\cdot)\), we have that Equation (5) is bounded by

\[2|\mathfrak{R}|(\mathcal{L}(\mathcal{F}),n)\] \[\quad\biggl{(}\log\log\biggl{|}S_{\mathcal{A}}\biggl{(}\frac{eb} {4c\sqrt{n}L_{1}}\biggr{)}\biggr{|}+\frac{c}{2\beta}\biggr{)}\biggl{(}\frac{8} {c}+40\frac{\sqrt{ec}}{c}\sqrt{\log\biggl{(}\frac{4c\sqrt{n}}{e}\biggl{|}S_{ \mathcal{A}}\biggl{(}\frac{eb}{4c\sqrt{n}L_{1}}\biggr{)}\biggr{|}\biggr{)}} \log\biggl{(}\frac{8c\beta n}{e^{2}}\biggr{)}\biggr{)}\] \[=2|\mathfrak{R}|(\mathcal{L}(\mathcal{F}),n)\Lambda_{\mathcal{A}} \bigl{(}2^{-1},L_{1},n,\beta\bigr{)}\]

#### e.2.3 Smooth and nonnegative losses

**Lemma 3**.: _Let \(\mathcal{F}\) be a class of predictors and let \(\mathcal{F}\mid_{r}\) be all functions in \(\mathcal{F}\) with empirical adversarial risk less than \(r\) on \(S\). If \(\mathcal{F}\mid_{r}\) is \(L_{\mathcal{F}}\)-Lipschitz w.r.t. \(\left\|\cdot\right\|_{\mathcal{A}}\), then under Assumption 2 and Assumption 4, we have \(\mathcal{N}_{2}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}\mid_{r}),\varepsilon,S) \leq\mathcal{N}_{\infty}\Bigl{(}\mathcal{F},\varepsilon/2\sqrt{12Hr},S_{ \mathcal{A}}\Bigl{(}\varepsilon/2\sqrt{12Hr}L_{\mathcal{F}}\Bigr{)}\Bigr{)}\)._Proof of Lemma 3.: First note that for generic \(\tilde{f}\in\mathcal{F}\mid_{r}\) and \(\tilde{A}_{i}\in\mathcal{A}\) by applying Lemma 5, that the following chain of inequalities holds:

\[\frac{1}{n}\sum_{i=1}^{n}\biggl{(}\max_{A\in\mathcal{A}}\bigl{(} \ell_{y^{i}}\circ f\circ A\bigr{)}(x^{i})-\Bigl{(}\ell_{y^{i}}\circ\tilde{f} \circ\tilde{A}_{i}\Bigr{)}(x^{i})\biggr{)}^{2}\] \[\leq\frac{1}{n}\sum_{i=1}^{n}\max_{A\in\mathcal{A}}\Bigl{(}\bigl{(} \ell_{y^{i}}\circ f\circ A\bigr{)}(x^{i})-\Bigl{(}\ell_{y^{i}}\circ\tilde{f} \circ\tilde{A}_{i}\Bigr{)}(x^{i})\Bigr{)}^{2}\] \[\leq 6H\max_{i\in[n]}\max_{A\in\mathcal{A}}\Bigl{(}(f\circ A)(x^ {i})-\Bigl{(}\tilde{f}\circ\tilde{A}_{i}\Bigr{)}(x^{i})\Bigr{)}^{2}\frac{1}{n} \sum_{i=1}^{n}\max_{A\in\mathcal{A}}\bigg{\{}\Bigl{(}\bigl{(}\ell_{y^{i}} \circ f\circ A\bigr{)}(x^{i})+\Bigl{(}\ell_{y^{i}}\circ\tilde{f}\circ\tilde{A} _{i}\Bigr{)}(x^{i})\Bigr{)}\bigg{\}}\] \[\leq 6H\max_{i\in[n]}\max_{A\in\mathcal{A}}\Bigl{(}(f\circ A)(x^ {i})-\Bigl{(}\tilde{f}\circ\tilde{A}_{i}\Bigr{)}(x^{i})\Bigr{)}^{2}\biggl{(} \frac{1}{n}\sum_{i=1}^{n}\max_{A\in\mathcal{A}}\bigg{\{}\bigl{(}\ell_{y^{i}} \circ f\circ A\bigr{)}(x^{i})\bigg{\}}+\frac{1}{n}\sum_{i=1}^{n}\Bigl{(}\ell_{ y^{i}}\circ\tilde{f}\circ\tilde{A}_{i}\Bigr{)}(x^{i})\biggr{)}\] \[\leq 12Hr\max_{i\in[n]}\max_{A\in\mathcal{A}}\Bigl{(}(f\circ A)(x^ {i})-\Bigl{(}\tilde{f}\circ\tilde{A}_{i}\Bigr{)}(x^{i})\Bigr{)}^{2}\]

So, taking the square root of both sides,

\[\sqrt{\frac{1}{n}\sum_{i=1}^{n}\biggl{(}\max_{A\in\mathcal{A}} \bigl{(}\ell_{y^{i}}\circ f\circ A\bigr{)}(x^{i})-\max_{A\in\mathcal{A}}\Bigl{(} \ell_{y^{i}}\circ\tilde{f}\circ\tilde{A}_{i}\Bigr{)}(x^{i})\biggr{)}^{2}}\] (6) \[\leq\sqrt{12Hr}\max_{i\in[n]}\max_{A\in\mathcal{A}}\Bigl{|}(f \circ A)(x^{i})-\Bigl{(}\tilde{f}\circ\tilde{A}_{i}\Bigr{)}(x^{i})\Bigr{|}\]

Now by triangle inequality and Lipschitzness of \(\mathcal{F}\mid_{r}\),

\[\max_{i\in[n]}\max_{A\in\mathcal{A}}\Bigl{|}(f\circ A)(x^{i})- \Bigl{(}\tilde{f}\circ\tilde{A}_{i}\Bigr{)}(x^{i})\Bigr{|}\] (7) \[=\max_{i\in[n]}\max_{A\in\mathcal{A}}\bigg{\{}\biggl{|}(f\circ A)( x^{i})-\Bigl{(}f\circ\tilde{A}_{i}\Bigr{)}(x^{i})+\Bigl{(}f\circ\tilde{A}_{i} \Bigr{)}(x^{i})-\Bigl{(}\tilde{f}\circ\tilde{A}_{i}\Bigr{)}(x^{i})\Bigr{|} \bigg{\}}\] \[\leq L_{\mathcal{F}}\max_{i\in[n]}\max_{A\in\mathcal{A}}\Bigl{\|} A(x^{i})-\tilde{A}_{i}(x^{i})\Bigr{\|}_{\mathcal{A}}+\max_{i\in[n]}\bigg{\{} \biggl{|}\Bigl{(}f\circ\tilde{A}_{i}\Bigr{)}(x^{i})-\Bigl{(}\tilde{f}\circ \tilde{A}_{i}\Bigr{)}(x^{i})\Bigr{|}\bigg{\}}\]

Fix \(\varepsilon>0\). Let \(\mathcal{C}_{\mathcal{A}}(\varepsilon/2L_{\mathcal{F}}\sqrt{12Hr})\) be a cover of \(\mathcal{A}\) at scale \(\varepsilon/2L_{\mathcal{F}}\sqrt{12Hr}\) w.r.t. \(\left\lVert\cdot\right\rVert_{\mathcal{A}}\). Let \(\mathcal{C}\) be a cover of \(\mathcal{F}\circ\mathcal{C}_{\mathcal{A}}(\varepsilon/2L_{\mathcal{F}}\sqrt{12Hr})\) with all \(\mathcal{C}_{\mathcal{A}}(\varepsilon/2L_{\mathcal{F}}\sqrt{12Hr})\) being included in the cover at scale \(\varepsilon/2\sqrt{12Hr}\) w.r.t. \(S\) and \(\left\lVert\cdot\right\rVert_{\infty}\).

Pick each \(\tilde{A}_{i}\in\mathcal{C}_{\mathcal{A}}(\varepsilon/2L_{\mathcal{F}}\sqrt{12Hr})\) such that

\[L_{\mathcal{F}}\max_{i\in[n]}\max_{A\in\mathcal{A}}\Bigl{\|}A(x^{i})-\tilde{A} _{i}(x^{i})\Bigr{\|}_{\mathcal{A}}\leq\varepsilon/2\sqrt{12Hr}\]

and, with \(\tilde{A}_{i}\) fixed, pick \(\tilde{f}\circ\tilde{A}\in\mathcal{C}\) such that

\[\max_{i\in[n]}\bigg{\{}\biggl{|}\Bigl{(}f\circ\tilde{A}_{i}\Bigr{)}(x^{i})- \Bigl{(}\tilde{f}\circ\tilde{A}_{i}\Bigr{)}(x^{i})\biggr{|}\bigg{\}}\leq \varepsilon/2\sqrt{12Hr}.\]Therefore \(\max_{i\in[n]}\max_{A\in\mathcal{A}}\Bigl{|}(f\circ A)(x^{i})-\Bigl{(}\tilde{f} \circ\tilde{A}_{i}\Bigr{)}(x^{i})\Bigr{|}\leq\varepsilon/2\sqrt{12Hr}\) So \(\mathcal{C}\) covers \(\mathcal{L}_{\mathcal{A}}(\mathcal{F}\mid_{r})\) at scale \(\varepsilon\), so

\[\mathcal{N}_{2}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}\mid_{r}),\varepsilon,S) \leq\mathcal{N}_{\infty}\biggl{(}\mathcal{F},\frac{\varepsilon}{2\sqrt{12Hr}},S _{\mathcal{A}}\biggl{(}\frac{\varepsilon}{2\sqrt{12Hr}L_{\mathcal{F}}}\biggr{)} \biggr{)}\]

**Theorem 8**.: _Under the setting of Lemma 3 along with \(|S_{\mathcal{A}}(eb/4c\sqrt{n}L_{\mathcal{F}})|\geq e^{e}\) and \(\operatorname{vc}_{\mathcal{X}}(\mathcal{F},b\beta)\geq 1\), then we have \(\mathfrak{R}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}\mid_{r}),S)\) is bounded by \(\sqrt{12H}|\mathfrak{R}|(\mathcal{F},n)\Lambda_{\mathcal{A}}\bigl{(}(24Hb)^{ -1/2},L_{\mathcal{F}},n,\beta\bigr{)}\), where \(C,c\) are absolute constants._

Proof.: Let \(\xi_{M}=\operatorname*{arg\,max}_{\xi\in(0,\infty)}\Bigl{\{}\xi\mid \operatorname{vc}_{\mathcal{X}}\Bigl{(}\mathcal{F},\xi\frac{c\sqrt{b}}{2\sqrt{1 2H}}\Bigr{)}\geq 1\Bigr{\}}\).

Applying Lemma 6 and Lemma 3 we have

\[\mathfrak{R}(\mathcal{L}_{\mathcal{A}}(\mathcal{F}\mid_{r}),S) \leq 4\alpha+\frac{10}{\sqrt{n}}\int_{\alpha}^{b}\sqrt{\log \mathcal{N}_{2}(\mathcal{L}_{\mathcal{A}}(\mathcal{F})\mid_{r},\varepsilon,S)} \;d\varepsilon\] \[\leq 4\alpha+\frac{10}{\sqrt{n}}\int_{\alpha}^{\sqrt{br}}\sqrt{ \log\mathcal{N}_{\infty}\biggl{(}\mathcal{F},\frac{\varepsilon}{2\sqrt{12Hr}},S_{\mathcal{A}}\biggl{(}\frac{\varepsilon}{2L_{\mathcal{F}}\sqrt{12Hr}} \biggr{)}\biggr{)}}\;d\varepsilon\] \[\;.\]

Now using Lemma 8, which is our general bound of this type of integral, we have with \(\rho=1/2\sqrt{12Hr}\) and \(\lambda=2L_{\mathcal{F}}\sqrt{12Hr}\) and \(\kappa=\sqrt{br}\) and \(s(\cdot)=S_{\mathcal{A}}(\cdot)\), the integral is bounded by

\[\sqrt{12H}|\mathfrak{R}|(\mathcal{F},n)\] \[\quad\biggl{(}\log\log\biggl{|}S_{\mathcal{A}}\biggl{(}\frac{eb}{ 4c\sqrt{n}L_{\mathcal{F}}}\biggr{)}\biggr{|}+\frac{c}{\beta 2\sqrt{12Hr}}\biggr{)}\Biggl{(}\frac{8}{c}+40 \frac{\sqrt{cC}}{c}\sqrt{\log\biggl{(}\frac{4c\sqrt{n}}{e}\biggl{|}S_{ \mathcal{A}}\biggl{(}\frac{eb}{4c\sqrt{n}L_{\mathcal{F}}}\biggr{)}\biggr{|} \biggr{)}}\log\biggl{(}\frac{16c\beta n}{2\sqrt{12Hr}e^{2}}\biggr{)}\Biggr{)}\] \[=\sqrt{12H}|\mathfrak{R}|(\mathcal{F},n)\Lambda_{\mathcal{A}} \Bigl{(}(24Hb)^{-1/2},L_{\mathcal{F}},n,\beta\Bigr{)}.\]

## Appendix F NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our theorems are in the setting of adversarially robust transfer learning and the asymptotic rates given in the introduction directly follow from our theorems (as explained in the main body). Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Our theorem statements are clearly qualified with the assumptions and algorithm they use. When our assumptions differ from prior work, we discuss this in detail in Appendix B. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We provide proofs for all our results. Due to considering Lipschitz and smooth nonnegative losses there are a few proofs that are trivially nearly identical, when this is the case we clearly indicate this and remark exactly what to modify. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See, in particular, the second paragraph in the introduction. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.