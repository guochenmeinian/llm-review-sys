# SDP4Bit: Toward 4-bit Communication Quantization

in Sharded Data Parallelism for LLM Training

Jinda Jia

Indiana University

jindjia@iu.edu

&Cong Xie1

ByteDance Inc.

cong.xie@bytedance.com

&Hanlin Lu

ByteDance Inc.

hanlin.lu@bytedance.com

&Daoce Wang

Indiana University

daocwang@iu.edu

&Hao Feng

Indiana University

haofeng@iu.edu

&Chengming Zhang

University of Houston

czhang59@cougarnet.uh.edu

&Baixi Sun

Indiana University

sunbaix@iu.edu

&Haibin Lin

ByteDance Inc.

haibin.lin@bytedance.com

&Zhi Zhang

ByteDance Inc.

zhangzhi.joshua@bytedance.com

&Xin Liu

ByteDance Inc.

liuxin.ai@bytedance.com &Dingwen Tao

Indiana University

ditao@iu.edu

Equal Contribution.

###### Abstract

Recent years have witnessed a clear trend towards language models with an ever-increasing number of parameters, as well as the growing training overhead and memory usage. Distributed training, particularly through Sharded Data Parallelism (ShardedDP) which partitions optimizer states among workers, has emerged as a crucial technique to mitigate training time and memory usage. Yet, a major challenge in the scalability of ShardedDP is the intensive communication of weights and gradients. While compression techniques can alleviate this issue, they often result in worse accuracy. Driven by this limitation, we propose **SDP4Bit** (Toward 4Bit Communication Quantization in Sharded Data Parallelism for LLM Training), which effectively reduces the communication of weights and gradients to nearly 4 bits via two novel techniques: quantization on weight differences, and two-level gradient smooth quantization. Furthermore, SDP4Bit presents an algorithm-system co-design with runtime optimization to minimize the computation overhead of compression. In addition to the theoretical guarantees of convergence, we empirically evaluate the accuracy of SDP4Bit on the pre-training of GPT models with up to 6.7 billion parameters, and the results demonstrate a negligible impact on training loss. Furthermore, speed experiments show that SDP4Bit achieves up to 4.08\(\times\) speedup in end-to-end throughput on a scale of 128 GPUs.

## 1 Introduction

Large Language Models (LLMs) are increasingly utilized across various applications, leading to a trend toward larger model sizes. This expansion in model size significantly escalates training overheads, making the process more costly and resource-intensive. To mitigate the time-consumingnature of training LLMs, it is common to employ multiple GPUs in a data-parallel configuration. However, naive Data Parallelism (DP) necessitates that each GPU replicates the entire optimizer states, a strategy often impractical due to the limited memory capacity of individual GPUs. This limitation becomes particularly critical with the substantial size of modern LLMs.

Sharded Data Parallelism (ShardedDP) evolves from naive DP to reduce the memory footprint by sharding optimizer states among GPUs. However, the sharding mechanism significantly changes the communication pattern of DP, which brings up new challenges in system optimization. As a result, ShardedDP suffers from heavy communication overheads of both weights and gradients, particularly when inter-node bandwidth is limited. This can significantly increase the end-to-end (E2E) training time, especially when using a small gradient accumulation step.

Quantization is a widely used strategy to reduce the communication overhead of naive DP, albeit with some accuracy loss. Unfortunately, few prior studies have specifically addressed the issue of communication reduction in ShardedDP. Recently, QSDP [18] and ZeRO++ [32] attempted to quantize the communication of ShardedDP to Int4. However, when pushing the communication ratio to its limits, both QSDP and ZeRO++ fail to maintain comparable training loss to the baseline. Furthermore, ZeRO++ lacks theoretical convergence guarantees, and QSDP is limited to one specific quantizer called "random shift" and strong assumptions. Thus, _there is no effective solution to reduce ShardedDP's communication to nearly 4 bits without compromising the training loss_.

To address these issues, this paper proposes a novel communication reduction strategy, **SDP4Bit**. SDP4Bit comprises two main techniques: (1) **Quantization on Weight Differences**: Instead of directly quantizing weights, we apply 4-bit quantization to compress the weight differences between current and previous iterations; (2) **Two-Level Gradient Smooth Quantization**: We apply 8-bit quantization to intra-node gradients and 4-bit quantization to inter-node gradients, with Hadamard Transform for smoothing the outliers. To the best of our knowledge, SDP4Bit is the first work to successfully reduce both gradients and weights to nearly 4 bits without compromising training accuracy. As shown in Figure 1, the training validation loss for GPT-6.7B using SDP4Bit is closely aligned with full precision training. Our main contributions are summarized as follows:

* We propose a low-bit (i.e., nearly 4-bit) communication reduction strategy for ShardedDP that preserves E2E training accuracy.
* We establish a convergence guarantee for the proposed strategy, showing the same convergence rate as the ordinary Stochastic Gradient Descent (SGD), with extended choices of biased compressors and weaker assumptions compared to the previous theoretical results.
* We implement our method within the Megatron-LM framework and enhance it with runtime optimizations such as buffer reuse, operation pruning, and kernel fusion.
* Our results validate that SDP4Bit successfully compresses the communication of weights and gradients to nearly 4 bits, with a negligible impact on final loss. Notably, compared to non-quantized baseline, it achieves 4.08\(\times\) speedup for a GPT-18B model trained on 128 H800 GPUs.

## 2 Preliminaries

### Sharded Data Parallelism

Sharded Data Parallelism (ShardedDP) modifies traditional Data Parallelism (DP) to reduce the memory footprint per GPU. Unlike traditional DP, which duplicates high-precision optimizer states (typically including model weights and momentum variables in Float32) on each GPU, ShardedDP partitions them across all GPUs. Each GPU manages \(\frac{1}{P}\) of the optimizer states, hence reducing the corresponding memory footprint by a factor of \(\frac{1}{P}\), where \(P\) represents the number of GPUs involved.

Figure 1: Training validation loss for GPT-6.7B; SDP4Bit is closely aligned with full precision training.

With high-precision model weights sharded across GPUs (referred to as **"main weights"**), an all-gather operation is required to collect the weights for the forward-backward steps (typically in relatively low precision, such as Float16, referred to as **"model weights"**). For gradient synchronization, a reduce-scatter operation is performed before the optimization steps to ensure that each GPU has the corresponding hard of averaged gradients. In summary, each iteration necessitates an all-gather for weights and a reduce-scatter for gradients.

Driven by the need to train larger models within the constraints of GPU memory, ShardedDP is incorporated into several popular training frameworks, including Megatron-LM (Distributed Optimizer), DeepSpeed (ZeRO), and PyTorch (FSDP), each with slightly different implementation strategies. Notably, ZeRO-3 and FSDP release the collected weights after each computation to enhance memory efficiency, necessitating additional weight collective communication during the backward pass. Conversely, ZeRO-2 and Megatron-LM retain the collected weights throughout, thus eliminating the need for weight collection during the backward pass. Our weight reduction strategy is particularly well-suited for Megatron-LM, as it maintains a full model's weights at all times (see Algorithm 2). Additionally, Megatron-LM provides flexible parallelism support, such as tensor parallelism, which partitions models vertically to alleviate memory limitations. This approach enables the training of larger models compared to DeepSpeed.

```
0: worker: \(p\), weight in shard \(p\): \(w[p]\), local gradient on worker \(p\): \(g_{model}^{p}\), global gradient in shard \(p\): \(g_{main}[p]\), weight difference: \(d\)
1:function CompressedForwardPass
2:\(\hat{w}_{main}[p]\leftarrow\text{QuantizeWeights}(w_{main}[p])\)
3:\(w_{model}\leftarrow\text{AllGather}(\hat{w}_{main}[p])\)
4:\(output^{p}\leftarrow\text{ForwardPass}(w_{model},input^{p})\)
5:\(\text{free}(w_{model})\)
6:function CompressedBackwardPass
7:\(\hat{w}_{main}[p]\leftarrow\text{QuantizeWeights}(w_{main}[p])\)
8:\(w_{model}\leftarrow\text{AllGather}(\hat{w}_{main}[p])\)
9:\(g_{model}^{p}\leftarrow\text{Gradient}(w_{model},output^{p})\)
10:\(\text{free}(w_{model})\)
11:\(\tilde{g}_{model}^{p}\leftarrow\text{QuantizeGradients}(g_{model}^{p})\)
12:\(g_{main}[p]\leftarrow\text{ReduceScatterTwoAllGather}(\tilde{g}_{model}^{p})\)
13:\(w_{main}[p]\leftarrow\text{Optimizer}(g_{main}[p],w_{main}[p])\) ```

**Algorithm 1**QSDP / ZeRO++

### Quantization

Quantization is a commonly used strategy in data compression. In this paper, we explore symmetric linear (integer) quantization due to its low overhead and latency. It is defined as follows:

\[x_{\text{int}}=\text{round}\left(\frac{x}{s}\cdot(2^{k-1}-1)\right),\quad s= \max(x),\]

where \(k\) represents the bit-width of the quantized values, and \(s\) is referred to as "scales".

Additionally, group-wise quantization [25] is employed to minimize quantization error by dividing the data into multiple groups and quantizing each group individually. This approach results in a lower compression ratio due to the need to store additional scales.

### Collective Reduction Communication with Quantization

State-of-the-art (SOTA) collective communication libraries (e.g., NCCL, Gloo) employ a ring-based algorithm for its optimal bandwidth [21]. This algorithm executes reduce-scatter operations across \(P-1\) rounds, during which each GPU sends local data and aggregates the received data. When quantization is applied, this necessitates \(P-1\) rounds of quantization and dequantization, potentially leading to error propagation and increased latency [11]. Some strategies replace reduce-scatter with all-to-all communication, but this increases inter-node communication, typically with lower bandwidth.

ZeRO++ [32] modifies this approach by substituting the conventional reduce-scatter (used by QSDP) with two all-to-all operations (shown in Algorithm 1, with different colors to distinguish ZeRO++from QSDP). The first operation is confined within each node, and post-reduction, the data size is diminished to \(\frac{1}{N}\), where \(N\) is the number of GPUs per node. The subsequent all-to-all operation occurs between GPUs across different nodes that share the same local rank. In ZeRO++, each all-to-all operation follows a 4-bit quantization step to minimize communication data size.

As shown in Figure 5, while this method efficiently integrates quantization into reduce-scatter without augmenting inter-node communication, _the repeated 4-bit quantization steps can accumulate quantization errors, potentially leading to suboptimal training outcomes_.

## 3 Methodology

### Quantization on Weight Differences (_qWD_)

As discussed in Section 2.1, ShardedDP requires each GPU to send/receive updated weights (main weights) to/from other GPUs in each iteration. _However, weights generally exhibit a wide range and directly applying 4-bit quantization leads to significant quantization errors_. Even with group-wise quantization, a gap in E2E training loss compared to full precision remains despite using small group sizes.

**qWD:** To address this issue, we quantize weight differences instead of the original weights during communication. As illustrated in Figure 2 and Algorithm 2, after the optimizer step, each GPU calculates the differences between the main weights and the model weights. These differences are then quantized and all-gathered across all GPUs. After all-gathering, each GPU dequantizes the received data to obtain the weight differences. These differences are then added to the model weights to obtain the updated weights.

There are two main benefits to applying quantization to weight differences.

**1)** In practice, weight differences are generally easier to quantize. As shown in Figure 4, weight differences are more uniformly distributed in a smaller range compared to the weights themselves, resulting in smaller errors for INT4 quantization. Furthermore, since intuitively the magnitudes of weight differences are smaller than those of weights themselves (informally supposed that \(\|\delta w_{t}\|=\|w_{t}-w_{t-1}\|<\|w_{t}\|\)) and the relative quantization errors are similar between weights and weight differences (informally supposed that \(\frac{\|q(\delta w_{t})-\delta w_{t}\|}{\|\delta w_{t}\|}\approx\frac{\|q(w_{ t})-w_{t}\|}{\|w_{t}\|}\)), the weight differences compression potentially has a smaller error relative to the weights themselves: \(\frac{\|q(\delta v_{t})-\delta w_{t}\|}{\|w_{t}\|}\lessq\frac{\|q(w_{t})-w_{t}\|}{ \|w_{t}\|}\), where \(q(\cdot)\) is the quantization function.

**2)** In theory, weight differences compression improves convergence compared to naive weight compression. When extended to biased compressors, we present theoretical guarantees for convergence at the same rate as ordinary SGD, as detailed in Section 4.2. In contrast, we demonstrate that using biased compressors directly on weights can lead to convergence failure, as illustrated in an example in Section 4.1. This proves that biased compressors are not compatible with QSDP or ZeRO++.

### Two-Level Gradient Smooth Quantization

#### 3.2.1 Two-level Gradient Quantization (_TLq_)

As discussed in Section 2.3, the 2-step all-to-all communication strategy benefits gradient communication when quantization is applied. However, it also introduces error accumulation due to consecutive 4-bit quantization steps, necessitating additional rounds of training and communication that diminish the per-iteration communication savings. We observe that _applying ULq, which quantizes gradients to extremely low precision, such as 4-bit, leads to noticeable deviations in training loss_ compared to full-precision training, as illustrated in Figure 5.

_TLq:_ Instead of employing a global 4-bit quantization strategy for both inter-node and intra-node all-to-all communications, we propose a two-level precision quantization strategy. This approach balances performance and accuracy by enhancing accuracy without introducing additional overhead. For intra-node all-to-all communication, gradients are quantized to INT8 before sending. After receiving the data, each GPU dequantizes the received data back to the full precision (i.e., FP32) for local reduction. This reduced data is then quantized to INT4 to minimize inter-node communication overhead. The detailed methodology is depicted in Figure 3. Since the two all-to-all operations utilize different network bandwidths, their communications can be effectively overlapped (see Table 4).

#### 3.2.2 _TLq_ with Hadamard Smoother (_TLq-Hs_)

While _TLq_ brings the training loss closer to the baseline, it does not achieve perfect alignment. In gradient quantization, _outliers can significantly amplify quantization errors_. Although group-wise quantization isolates outliers to minimize their impact on the precision of values in other groups, the values within the same group remain affected.

_TLq-HS:_ To mitigate the outlier issue, we apply Hadamard transform [9] to the gradients before quantization. The Hadamard transform, a specific type of generalized Fourier transform, exhibits properties such as \(H=H^{T}\) and \(H\cdot H^{T}=I\), distributing outlier information across nearby elements and effectively smoothing them out. For a detailed description of the methodology, see Algorithm 3.

```
0:gradient\(grad\)
1:functionTLq-HS
2:\(\hat{g}\leftarrow\) Hadamard\((grad)\)
3:\(\hat{q}g_{bsit}\leftarrow\) QuantizeRN(\(\hat{g}\))
4:\(list(\hat{q}g_{bsit\_intra})\leftarrow\)IntraAlltoAll(\(\hat{q}g_{bsit}\))
5:\(list(\hat{g}_{intra})\leftarrow\)Dequantize(\(list(\hat{q}g_{bsit\_intra})\))
6:\(list(\hat{g}_{intra})\leftarrow\)Hadamard(\(list(\hat{g}_{intra})\))
7:\(\hat{g}_{reduced}\leftarrow\)Reduction(\(list(\hat{g}_{intra})\))
8:\(\hat{g}_{reduced}\leftarrow\)Hadamard(\(\hat{g}_{reduced}\))
9:\(\hat{q}g_{abit}\leftarrow\)QuantizeBH(\(\hat{g}_{reduced}\))
10:\(list(\hat{q}g_{abit\_inter})\leftarrow\)InterAlltoAll(\(\hat{q}g_{abit}\))
11:\(list(\hat{g}_{inter})\leftarrow\)Dequantize(\(list(\hat{q}g_{abit\_inter})\))
12:\(\hat{g}_{final\_reduced}\leftarrow\)Reduction(\(list(\hat{g}_{inter})\))
13:\(g_{final\_reduced}\leftarrow\)Hadamard(\(\hat{g}_{final\_reduced}\)) ```

**Algorithm 3** TLq with Hadamard Smoother

### Performance Optimizations in Implementation

**Optimizing Memory Efficiency for Weight Differences Computation:** After the all-gather communication, each GPU receives weight differences from the others, which are then added to the model weights to update them. As discussed in Section 2.1, with ShardedDP enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for forward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in Megatron-LM, our implementation eliminates the need for additional buffers to retain model weights for calculating the differences, thus enhancing memory efficiency.

```
0:gradient\(grad\)
1:functionTLq-HS
2:\(\hat{g}\leftarrow\)Hadamard\((grad)\)
3:\(\hat{q}g_{bsit}\leftarrow\)QuantizeRN(\(\hat{g}\))
4:\(list(\hat{q}g_{bsit\_intra})\leftarrow\)IntraAlltoAll(\(\hat{q}g_{bsit}\))
5:\(list(\hat{g}_{intra})\leftarrow\)Dequantize(\(list(\hat{q}g_{bsit\_intra})\))
6:\(list(\hat{g}_{intra})\leftarrow\)Hadamard(\(list(\hat{g}_{intra})\))
7:\(\hat{g}_{reduced}\leftarrow\)Reduction(\(list(\hat{g}_{intra})\))
8:\(\hat{g}_{reduced}\leftarrow\)Hadamard(\(\hat{g}_{reduced}\))
9:\(\hat{q}g_{abit}\leftarrow\)QuantizeBH(\(\hat{g}_{reduced}\))
10:\(list(\hat{q}g_{abit\_inter})\leftarrow\)InterAlltoAll(\(\hat{q}g_{abit}\))
11:\(list(\hat{g}_{inter})\leftarrow\)Dequantize(\(list(\hat{q}g_{abit\_inter})\))
12:\(\hat{g}_{final\_reduced}\leftarrow\)Reduction(\(list(\hat{g}_{inter})\))
13:\(g_{final\_reduced}\leftarrow\)Hadamard(\(\hat{g}_{final\_reduced}\)) ```

**Algorithm 4** TLq with Hadamard Smoother

### Performance Optimizations in Implementation

**Optimizing Memory Efficiency for Weight Differences Computation:** After the all-gather communication, each GPU receives weight differences from the others, which are then added to the model weights to update them. As discussed in Section 2.1, with ShardedDP enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for forward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in Megatron-LM, our implementation eliminates the need for additional buffers to retain model weights for calculating the differences, thus enhancing memory efficiency.

**Optimizing Memory Efficiency for Weight Differences Computation:** After the all-gather communication, each GPU receives weight differences from the others, which are then added to the model weights to update them. As discussed in Section 2.1, with ShardedDP enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for forward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in Megatron-LM, our implementation eliminates the need for additional buffers to retain model weights for calculating the differences, thus enhancing memory efficiency.

**Optimizing Memory Efficiency for Weight Differences Computation:** After the all-gather communication, each GPU receives weight differences from the others, which are then added to the model weights to update them. As discussed in Section 2.1, with ShardedDP enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for forward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in Megatron-LM, our implementation eliminates the need for additional buffers to retain model weights for calculating the differences, thus enhancing memory efficiency.

**Optimizing Memory Efficiency for Weight Differences Computation:** After the all-gather communication, each GPU receives weight differences from the others, which are then added to the model weights to update them. As discussed in Section 2.1, with ShardedDP enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for forward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in Megatron-LM, we implement the need for additional buffers to retain model weights for calculating the differences, thus enhancing memory efficiency.

**Optimizing Memory Efficiency for Weight Differences Computation:** After the all-gather communication, each GPU receives weight differences from the others, which are then added to the model weights to update them. As discussed in Section 2.1, with ShardedDP enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for forward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in Megatron-LM, we implement the need for additional buffers to retain model weights for calculating the differences, thus enhancing memory efficiency.

**Optimizing Memory Efficiency for Weight Differences Computation:** After the all-gather communication, each GPU receives weight differences from the others, which are then added to the model weights to update them. As discussed in Section 2.1, with ShardedDP enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for forward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in Megatron-LM, we implement the need for additional buffers to retain model weights for calculating the differences, thus enhancing memory efficiency.

**Optimizing Memory Efficiency for Weight Differences Computation:** After the all-gather communication, each GPU receives weight differences from the others, which are then added to the model weights to update them. As discussed in Section 2.1, with ShardedDP enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for forward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in Megatron-LM, we implement the need for additional buffers to retain model weights for calculating the differences, thus enhancing memory efficiency.

**Optimizing Memory Efficiency for Weight Differences Computation:** After the all-gather communication, each GPU receives weight differences from the others, which are then added to the model weights to update them. As discussed in Section 2.1, with ShardedDP enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for forward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in Megatron-LM, our implementation eliminates the need for additional buffers to retain model weights for calculating the differences, thus enhancing memory efficiency.

**Optimizing Memory Efficiency for Weight Differences Computation:** After the all-gather communication, each GPU receives weight differences from the others, which are then added to the model weights to update them. As discussed in Section 2.1, with ShardedDP enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for forward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in Megatron-LM, we implement the need for additional buffers to update them. As discussed in Section 2.1, Sharded enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for backward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in Megatron-LM, we implement the need for additional buffers to retain model weights for calculating the differences, thus enhancing memory efficiency.

**Optimizing Memory Efficiency for Weight Differences Computation:** After the all-gather communication, each GPU receives weight differences from the others, which are then added to the model weights to update them. As discussed in Section 2.1, with ShardedDP enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for forward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in Megatron-LM, we implement the need for additional buffers to retain model weights for calculating the differences, thus enhancing memory efficiency.

**Optimizing Memory Efficiency for Weight Differences Computation:** After the all-gather communication, each GPU receives weight differences from the others, which are then added to the model weights to update them. As discussed in Section 2.1, with ShardedDP enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for forward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in Megatron-LM, we implement the need for additional buffers to update them. As discussed in Section 2.1, Sharded enabled in Megatron-LM, we implement the need for additional buffers to update them. As discussed in Section 2.2, Sharded enabled in Megatron-LM, we implement the need for additional buffers to update them. As discussed in Section 2.3, Sharded enabled in Megatron-LM, we implement the need for additional buffers to update them. As discussed in Section 2.4, Sharded in Megatron-LM, we implement the need for additional buffers to update them. As discussed in Section 2.5, Sharded enabled in Megatron-LM, we implement the need for additional buffers to update them. As discussed in Section 2.6, Sharded enabled in Megatron-LM, we implement the need for additional buffers to update them. As discussed in Section 2.7, Sharded enabled in Megatron-LM, we implement the need for additional buffers to update them. As discussed in Section 2.8, Sharded enabled in Megatron-LM, we implement the need for additional buffers to update them. As discussed in Section 2.9, Sharded enabled in Megatron-

**Simplifying Hadamard Transforms:** In a naive implementation, the Hadamard transform would be applied at each step before quantization and after dequantization. However, by leveraging the orthogonality of the Hadamard transform, i.e. \(H\cdot H=I\), we omit the transform after the intra-node all-to-all dequantization (Algorithm 3, Line 6) and before the inter-node quantization (Algorithm 3, Line 8). Furthermore, by utilizing the distributive property, i.e., \(\sum_{i}Hg_{i}=H\sum_{i}g_{i}\), we move the second Hadamard transform from after the inter-node dequantization (Algorithm 3, Line 11) to after the final reduction (Algorithm 3, Line 12). These simplifications reduce the unnecessary computational overhead associated with repeated Hadamard transforms.

**Fusing GPU Kernels with Group Size Alignment:** To mitigate additional data movement from global memory--which typically exhibits the slowest memory bandwidth--, we fuse the Hadamard transform with the (de)quantization operations into a single CUDA kernel. This fusion allows the operations to run nearly as fast as the quantization operation alone. It is worth noting that, for this fusion to be efficient, there must be an alignment between the two. Specifically, the size of the quantization group must be divisible by the size of the Hadamard matrix, ensuring that memory traffic remains within the kernel block. We choose \(H\) to be small (e.g., 32\(\times\)32) because, at this size, the transform operation on the GPU is typically memory-bound and incurs minimal overhead. While larger \(H\) sizes offer better smoothing capabilities, we find that a 32\(\times\)32 matrix is sufficient to effectively smooth outliers in gradients.

## 4 Theoretical Analysis

### Counterexample of Biased Weight Compression

One of the advantages of our proposed weight difference compression is the compatibility to both biased and unbiased compressors. Note that using biased compressors directly on weight compression incurs issues in convergence under standard assumptions, as avoided in QSDP [18] or ZeRO++ [32]. We illustrate such issues in the following toy example.

**Counterexample 4.1**.: Consider a least square problem with \(w^{*}=(0,0)^{\top}\): \(\min_{w\in\mathbb{R}^{2}}\left[f(w)=\|w\|^{2}\right]\), and stochastic gradient \(g(w)=(4w_{1},0)^{\top}\) with probability 0.5 and \(g(w)=(0,4w_{2})^{\top}\) with probability 0.5, thus \(\mathbb{E}[g(w)]=\nabla_{w}f(w)\). We use the initial value \(w_{init}=(1,-1)^{\top}\), the learning rate \(\eta<0.125\), and the following nearest ternary quantizer: \(s=\max(|w|),q(w)=round(w/s)*s\), where \(|\cdot|\) is element-wise absolute value, and \(round(\cdot)\) quantizes each element to the nearest value in \(\{-1,0,1\}\). It is easy to check that for SGD under such settings, the weights before quantization will be either \((1-4\eta,-1)^{\top}\) or \((1,-1+4\eta)^{\top}\), resulting in \(q(w)=(1,-1)^{\top}\), which means that SGD with ternary weight quantization gets stuck at the initial value in this case, while SGD without weight quantization and SGD with weight difference quantization both converge to the optimal.

### Convergence Analysis

To theoretically analyze the convergence of our distributed training algorithm with communication compression, we focus on the following SGD variant with gradient compression and weight difference compression. We use SGD to solve the following optimization problem: \(f^{*}=\min_{w}f(w)\), where \(f(w)\) is the objective function, \(w\in\mathbb{R}^{d}\) is the model parameter.

```
1:Initialize main parameter weights \(w_{0}\)
2:Initialize compressed parameter weights \(\tilde{w}_{0}\gets w_{0}\)
3:for all iteration \(t\in[T]\)do
4: Compute gradient: \(g_{t-1}=\nabla f(\tilde{w}_{t-1};\zeta_{t-1})\)
5: Compress gradient: \(\tilde{g}_{t-1}=\mathcal{U}_{g}(g_{t-1})\)
6: Update main weights: \(w_{t}\gets w_{t-1}-\eta\tilde{g}_{t-1}\)
7: Compress weight difference: \(\tilde{\Delta}_{t}=\mathcal{C}_{w}(w_{t}-\tilde{w}_{t-1})\)
8: Update compressed weights: \(\tilde{w}_{t}\leftarrow\tilde{w}_{t-1}+\tilde{\Delta}_{t}\)
9:endfor ```

**Algorithm 4** SGD with SDP4BitNote that we use unbiased compressors for gradient reduction, and arbitrary (potentially biased) compressors for weight collection. We formally define these two classes of compressors as follows.

**Definition 4.1** (Unbiased \(\kappa\)-approximate compressor [1]).: An operator \(\mathcal{U}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}\) is a \(\kappa\)-approximate compressor for \(\kappa\geq 0\) if \(\mathbb{E}[\mathcal{U}(v)]=v\) and \(\mathbb{E}[\mathcal{U}(v)-v]^{2}\leq\kappa\|v\|^{2},\forall v\in\mathbb{R}^{d}\).

**Definition 4.2** (\(\delta\)-approximate compressor [13]).: An operator \(\mathcal{C}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}\) is a \(\delta\)-approximate compressor for \(\delta\in[0,1]\) if \(\mathbb{E}[\mathcal{C}(v)-v]^{2}\leq(1-\delta)\|v\|^{2},\forall v\in\mathbb{ R}^{d}\).

_Remark 4.1_.: Note that, in a certain sense, the class of \(\delta\)-approximate compressors contains the class of unbiased compressors. It is easy to check that any \(\kappa\)-approximate unbiased compressor \(\mathcal{U}\) can be converted to a \(\frac{1}{1+\kappa}\)-approximate biased compressor \(\mathcal{C}(v)=\frac{1}{1+\kappa}\mathcal{U}(v)\). Furthermore, the class of \(\delta\)-approximate compressors typically provides more options such as top-\(k\) sparsifiers, and top-\(k\) low-rank compressors. Thus, we consider arbitrary (biased or unbiased) \(\delta\)-approximate compressors for weight compression in our theoretical analysis.

_Remark 4.2_.: For distributed training with \(P\) workers, we define the compressed gradient as \(\tilde{g}_{t}=\mathcal{U}_{g}(g_{t})=\frac{1}{P}\sum_{i\in[P]}\mathcal{U}_{g} ^{\prime}(g_{t,i})\), where \(g_{t}=\frac{1}{P}\sum_{i\in[P]}g_{t,i}\), and \(g_{t,i}\) is the stochastic gradient from the \(i\)th worker in \(t\) iteration. We assume that \(\mathcal{U}_{g}\) is an unbiased \(\kappa\)-approximate compressor of the average gradient \(g_{t}\).

**Assumption 4.1**.: (Smoothness) We assume that \(f(x)\) is \(L\)-smooth: \(\|\nabla f(x)-\nabla f(y)\|\leq L\|x-y\|,\forall x,y\in\mathbb{R}^{d}\), which implies \(f(y)-f(x)\leq\langle\nabla f(x),y-x\rangle+\frac{L}{2}\|y-x\|^{2}\).

**Assumption 4.2**.: For any stochastic gradient \(\nabla f(w;\zeta)\), where \(\zeta\) is an independent random sample, we assume unbiasedness \(\mathbb{E}[\nabla f(w;\zeta)|w]=\nabla f(w)\), and bounded variance \(\mathbb{E}[\nabla f(w;\zeta)-\nabla f(w)\|^{2}|w]\leq\rho\|\nabla f(w)\|^{2}+ \sigma^{2}\) ([27], Assumption 3).

We derive the following error bounds on the convergence of SDP4Bit under the above assumptions. All proofs can be found in Appendix A.

**Theorem 4.1** (Convergence error bound).: _For arbitrary non-convex function under Assumption 4.1 and Assumption 4.2, taking learning rate \(\eta\leq\frac{1}{10L\left(\frac{2}{4}+\rho\kappa+\rho+\kappa\right)}\), Algorithm 4 converges to a critical point with the following error bound:_

\[\frac{\sum_{t=0}^{T}\mathbb{E}[\|\nabla f(\tilde{w}_{t})\|^{2}]}{T+1}\leq \frac{80L\left(\frac{2}{4}+\rho\kappa+\rho+\kappa\right)\left(f(w_{0})-f^{*} \right)}{T+1}+4\sigma\sqrt{\frac{(11-\delta)(\kappa+1)L(f(w_{0})-f^{*})}{T+1}}.\]

_Remark 4.3_.: Note that compared to QSDP [18], our convergence analysis does not require Polyak-Lojasiewicz condition or the specific choice of weight quantization (random shift). In other words, Theorem 4.1 shows that our proposed algorithm has the same \(\mathcal{O}\left(\frac{1}{\sqrt{T}}\right)\) convergence rate as ordinary SGD for general non-convex functions, but under much weaker assumptions compared to QSDP.

## 5 Evaluation

### Experimental Setup

**Hardware:** The experiments are conducted on two different clusters to evaluate SDP4Bit across varying network environments: **1)** 16 nodes, each node equipped with 4 Nvidia A100-SXM4-40GB GPUs. All nodes are interconnected with a 100 Gbps Slingshot10 network, providing slower inter-node bandwidth. **2)** 16 nodes, each node equipped with 8 Nvidia H800-SXM5-80GB GPUs. Each node is connected using 8 InfiniBand links, achieving a total bandwidth of 3.2 Tbps, providing higher inter-node bandwidth.

**Baselines:** We use BFloat16/Float32 (weights/gradients) mixed-precision in Megatron-LM [26] as our basic _Baseline_ for both accuracy and E2E throughput analysis. Within each set of experiments, we ensure consistent hyper-parameters to ensure fairness. Detailed parameters are provided in Appendix D. Additionally, we implement another baseline for comparison in Megatron-LM, using the same quantization strategy in ZeRO++, employing 4-bit quantization for both weights (group-wise weight quantization, refered to as \(qW\)) and gradients (twice all-to-all with uniform level 4-bit quantization, refer to as _ULq_).

**Dataset and Models:** To demonstrate that SDP4Bit does not adversely affect end-to-end training loss, we conduct pre-training on GPT-series [23] models ranging from 125M to 6.7B parameters

[MISSING_PAGE_FAIL:8]

### Throughput Evaluation

Next, we evaluate the improved E2E throughput, measured in FLOPS per second, of SDP4Bit on both hardware platforms. For all tests, the results are averaged over 10 iterations after 20 warm-up iterations. As shown in Table 2, SDP4Bit achieves an E2E training speedup of up to 4.08\(\times\). For models with the same model parallel configuration (e.g., 1.3B and 2.7B; 13B and 18B), both the E2E throughput and speedup from SDP4Bit increase as the model size grows due to larger models having higher computational efficiency but also encountering increased communication overhead.

The throughput of the 1.3B, 2.7B, and 6.7B models across the two platforms indicates that SDP4Bit provides a more significant speedup when network bandwidth is lower. This is because lower bandwidth results in higher communication overhead, which SDP4Bit effectively reduces through efficient quantization techniques.

In addition, we demonstrate the scalability of SDP4Bit using GPT models of 6.7B and 13B parameters, with tests conducted on up to 128 GPUs, as shown in Figure 8. Under low bandwidth conditions, SDP4Bit achieves an average speedup of 3.40\(\times\) for the 6.7B model and 2.49\(\times\) for the 13B model. In high-bandwidth InfiniBand environments, the speedup averages 3.08\(\times\) for the 6.7B model and 3.73\(\times\) for the 13B model. The comparatively lower speedup for the 13B model under low bandwidth conditions can be attributed to the introduction of pipeline parallelism, which diminishes the proportion of communication handled by ShardedDP. Overall, SDP4Bit consistently maintains stable speedup performance across various GPU numbers and network environments.

### Ablation Study

**Components Breakdown.** Figure 8 demonstrates the throughput improvement of qWD, TLq-HS, and their combination (SDP4Bit) on two different platforms. qWD alone provides a speedup ranging from 1.1\(\times\) to 1.2\(\times\), while TLq-HS alone results in an E2E speedup of 1.4\(\times\) to 1.8\(\times\). The notable benefit from gradient quantization stems from the high communication overhead associated with Float32 gradients in baseline training, which is higher compared to BFloat16 weights. When they are applied together, SDP4Bit achieves a more substantial speedup, ranging from 1.6\(\times\) to 2.4\(\times\).

**TLq-HS vs. ULq.** Table 4 compares gradient quantization between TLq-HS and ULq. The results show that although TLq-HS employs 8-bit quantization for intra-node gradient communication, it introduces negligible overhead compared to 4-bit communication. This is due to 1) the high bandwidth of intra-node communication and 2) the fact that most intra-node communication is overlapped with the slower inter-node communication.

**Hadamard Kernel Fusion.** Table 4 shows that, compared to the SDP4Bit without fusing Hadamard Transform kernel, our optimized SDP4Bit reduces gradient communication overhead by 29%. Additionally, we provide a throughput comparison in Table 5 to further illustrate the impact of the Hadamard transformation. The results confirm that our Hadamard kernel fusion effectively reduces the overhead, making the transformation nearly zero-overhead and even matching the performance of quantization without the Hadamard transformation.

**Convergence with Different Group Sizes.** Table 3 examines the impact of various quantization granularities on the end-to-end validation loss during the pre-training of the GPT-125M model. For TLq-HS, a gradient quantization group size of 128 presents sufficient, with smaller sizes yielding no significant accuracy improvements. For qWD, a quantization group size of 2048 achieves training accuracy comparable to the baseline. Table 3 also presents the 4-bit weight quantization (\(qW\)) while using small group size. It is evident that even with very small group size (e.g., 32), direct 4-bit quantization leads to a significant gap in accuracy compared to the baseline, making 4-bit quantization for weights suboptimal.

## 6 Related Work

Apart from ZeRO++ [32] and QSDP [18], which are specifically designed for communication compression in ShardedDP, most previous studies have focused on traditional DP, primarily utilizing gradient compression. This includes both unbiased compression techniques [1; 33; 38; 5], which employ randomized compressors, and biased compression methods with error compensation [12; 31; 30; 29; 24] that require extra storage for residual errors, making them less suitable for resource-intensive training of LLMs. Other strategies like local optimization or federated learning reduce communication frequency rather than volume [16; 28; 35; 34; 2; 20], but increase memory use, complicating their application in LLM training. In addition, techniques like low-precision training [19; 22] and parameter-efficient fine-tuning [10; 3; 14] minimize the volume of trainable variables to reduce communication. In a different vein, weight quantization for inference has also been explored [7; 6; 39; 37; 4], employing more resource-intensive methods compared to those used in training to fine-tune compression parameters.

The Hadamard transform has been applied to machine learning data, as seen in HQ-MM's [36] compression of activations and THC's [15] gradient communication within a parameter server framework. Unlike THC, SDP4Bit enhances collective communication operations and GPU optimization.

## 7 Conclusion

In this paper, we propose SDP4Bit, a communication reduction strategy for Sharded Data Parallelism. SDP4Bit reduces both weight and gradient communication to nearly 4 bits while maintaining model accuracy comparable to the baseline. We implemented SDP4Bit in Megatron-LM and optimized it to reduce quantization overhead. Specifically, our experimental results demonstrate a training speedup of up to 4.08 \(\times\) on 128 GPUs. This paper focuses on LLM pre-training, but we plan to extend our work to other models and areas such as MoE, computer vision, and fine-tuning in the future.

## References

* [1] Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding. In _NeurIPS_, 2017.
* [2] Debraj Basu, Deepesh Data, Can Karakus, and Suhas Diggavi. Qsparse-local-SGD: Distributed SGD with Quantization, Sparsification and Local Computations. In _NeurIPS_, 2019.
* [3] Tim Dettmers, Arridoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient Finetuning of Quantized LLMs. In _NeurIPS_, 2024.
* [4] Peiyan Dong, Lei Lu, Chao Wu, Cheng Lyu, Geng Yuan, Hao Tang, and Yanzhi Wang. Pack-QViT: Faster Sub-8-bit Vision Transformers via Full and Packed Quantization on the Mobile. In _NeurIPS_, 2024.
* [5] Fartash Faghri, Iman Tabrizian, Ilia Markov, Dan Alistarh, Daniel M Roy, and Ali Ramezani-Kebrya. Adaptive Gradient Quantization for Data-Parallel SGD. In _NeurIPS_, 2020.
* [6] Elias Frantar and Dan Alistarh. SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot. In _ICML_, 2023.
* [7] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: ACCURATE POSTTRAINING QUANTIZATION FOR GENERATIVE PRE-TRAINED TRANSFER. In _ICLR_, 2023.
* [8] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800GB Dataset of Diverse Text for Language Modeling. _arXiv preprint arXiv:2101.00027_, 2020.
* [9] A Hedayat and Walter Dennis Wallis. Hadamard matrices and their applications. _The annals of statistics_, pages 1184-1238, 1978.
* [10] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. LoRA: Low-Rank Adaptation of Large Language Models. In _ICLR_, 2021.
* [11] Jiajun Huang, Sheng Di, Xiaodong Yu, Yujia Zhai, Jinyang Liu, Yafan Huang, Ken Raffenetti, Hui Zhou, Kai Zhao, Zizhong Chen, et al. gZCCL: Compression-Accelerated Collective Communication Framework for GPU Clusters. _arXiv preprint arXiv:2308.05199_, 2023.
* [12] Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi. Error Feedback Fixes SignSGD and other Gradient Compression Schemes. In _ICML_, 2019.
* [13] Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian U. Stich, and Martin Jaggi. Error Feedback Fixes SignSGD and other Gradient Compression Schemes. In _ICML_, 2019.
* [14] Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, and Dongsoo Lee. Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization. In _NeurIPS_, 2024.
* [15] Minghao Li, Ran Ben Basat, Shay Vargaftik, ConLam Lao, Kevin Xu, Michael Mitzenmacher, and Minlan Yu. THC: Accelerating Distributed Deep Learning Using Tensor Homomorphic Compression. In _NSDI_, 2024.
* [16] Tao Lin, Sebastian U Stich, Kumar Kshitij Patel, and Martin Jaggi. Don't Use Large Minibatches, Use Local SGD. In _ICLR_, 2020.
* [17] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* [18] Ilia Markov, Adrian Vladu, Qi Guo, and Dan Alistarh. Quantized Distributed Training of Large Models with Convergence Guarantees. In _ICML_, 2023.
* [19] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed Precision Training. _arXiv preprint arXiv:1710.03740_, 2017.

* [20] Giorgi Nadiradze, Amirmojtaba Sabour, Peter Davies, Shigang Li, and Dan Alistarh. Asynchronous Decentralized SGD with Quantized and Local Updates. In _NeurIPS_, 2021.
* [21] Pitch Patarasuk and Xin Yuan. Bandwidth Optimal All-reduce Algorithms for Clusters of Workstations. _Journal of Parallel and Distributed Computing_, 69(2):117-124, 2009.
* [22] Houwen Peng, Kan Wu, Yixuan Wei, Guoshuai Zhao, Yuxiang Yang, Ze Liu, Yifan Xiong, Ziyue Yang, Bolin Ni, Jingcheng Hu, Ruihang Li, Miaosen Zhang, Chen Li, Jia Ning, Ruizhe Wang, Zheng Zhang, Shuguang Liu, Joe Chau, Han Hu, and Peng Cheng. FP8-LM: Training FP8 Large Language Models. _arXiv preprint arXiv:2310.18313_, 2023.
* [23] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language Models are Unsupervised Multitask Learners. 2019.
* [24] Peter Richtarik, Igor Sokolov, and Ilyas Fatkhullin. EF21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedback. In _NeurIPS_, 2021.
* [25] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT. In _AAAI_, 2020.
* [26] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism. _arXiv preprint arXiv:1909.08053_, 2019.
* [27] Sebastian U. Stich and Sai Praneeth Karimireddy. The Error-Feedback Framework: Better Rates for SGD with Delayed Gradients and Compressed Communication. _JMLR_, 2020.
* [28] Sebastian Urban Stich. Local SGD Converges Fast and Communicates Little. In _ICLR_, 2019.
* [29] Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, and Yuxiong He. 1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed. In _ICML_, 2021.
* [30] Hanlin Tang, Chen Yu, Xiangru Lian, Tong Zhang, and Ji Liu. DOUBLESQUEEZE: Parallel Stochastic Gradient Descent with Double-pass Error-Compensated Compression. In _ICML_, 2019.
* [31] Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization. In _NeurIPS_, 2019.
* [32] Guanhua Wang, Heyang Qin, Sam Ade Jacobs, Connor Holmes, Samyam Rajbhandari, Olatunji Ruwase, Feng Yan, Lei Yang, and Yuxiong He. ZeRO++: Extremely Efficient Collective Communication for Giant Model Training. In _ICML_, 2023.
* [33] Hongyi Wang, Scott Sievert, Shengchao Liu, Zachary Charles, Dimitris Papailiopoulos, and Stephen Wright. ATOMO: Communication-efficient Learning via Atomic Sparsification. In _NeurIPS_, 2018.
* [34] Blake Woodworth, Kumar Kshitij Patel, and Nathan Srebro. Minibatch vs Local SGD for Heterogeneous Distributed Learning. In _NeurIPS_, 2020.
* [35] Blake Woodworth, Kumar Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins, Brendan Mcmahan, Ohad Shamir, and Nathan Srebro. Is local SGD better than minibatch SGD? In _ICML_, 2020.
* [36] Haocheng Xi, Changhao Li, Jianfei Chen, and Jun Zhu. Training Transformers with 4-bit Integers. In _NeurIPS_, 2023.
* [37] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models. In _ICML_, 2023.

* [38] Jihao Xin, Marco Canini, Peter Richtarik, and Samuel Horvath. Global-QSGD: Practical Floatless Quantization for Distributed Learning with Theoretical Guarantees. _arXiv preprint arXiv:2305.18627_, 2023.
* [39] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers. In _NeurIPS_, 2022.

**Appendix**

## Appendix A Proofs

We use the following lemma (simplified from [27], Lemma 14) without proof.

**Lemma A.1**.: _For every non-negative sequence \(\{r_{t}\}_{t\geq 0}\) and any parameters \(a\geq 0\), \(c\geq 0\), \(T\geq 0\), there exists a constant \(\eta\leq\frac{1}{a}\), such that_

\[\frac{1}{T+1}\sum_{t=0}^{T}\left(\frac{r_{t}-r_{t+1}}{\eta}+c\eta\right)=\frac {1}{T+1}\frac{r_{0}-r_{T+1}}{\eta}+c\eta\leq\frac{ar_{0}}{T+1}+\frac{2\sqrt{ cr_{0}}}{\sqrt{T+1}}.\]

**Theorem 4.1** (Convergence error bound).: _For arbitrary non-convex function under Assumption 4.1 and Assumption 4.2, taking learning rate \(\eta\leq\frac{1}{10L\left(\frac{2}{3}+\rho\kappa+\rho+\kappa\right)}\), Algorithm 4 converges to a critical point with the following error bound:_

\[\frac{\sum_{t=0}^{T}\mathbb{E}[\|\nabla f(\tilde{w}_{t})\|^{2}]}{T+1}\leq\frac {80L\left(\frac{2}{6}+\rho\kappa+\rho+\kappa\right)\left(f(w_{0})-f^{*} \right)}{T+1}+4\sigma\sqrt{\frac{(11-\delta)(\kappa+1)L(f(w_{0})-f^{*})}{T+1}}.\]

Proof.: By using smoothness (Assumption 4.1), we have

\[f(w_{t+1})\leq f(w_{t})-\eta\left\langle\nabla f(w_{t}),\mathcal{U}_{g}(g_{t })\right\rangle+\frac{\eta^{2}L}{2}\|\mathcal{U}_{g}(g_{t})\|^{2}.\]

Taking expectation w.r.t. the random compressor \(\mathcal{U}_{g}\), we have

\[\mathbb{E}_{gc}[f(w_{t+1})]\] \[\leq f(w_{t})-\eta\left\langle\nabla f(w_{t}),g_{t}\right\rangle+ \frac{\eta^{2}L}{2}\mathbb{E}_{gc}\|\mathcal{U}_{g}(g_{t})\|^{2}\] \[=f(w_{t})-\eta\left\langle\nabla f(w_{t}),g_{t}\right\rangle+ \frac{\eta^{2}L}{2}[\|g_{t}\|^{2}+\mathbb{E}_{gc}\|\mathcal{U}_{g}(g_{t})-g_{t }\|^{2}]\] \[\leq f(w_{t})-\eta\left\langle\nabla f(w_{t}),g_{t}\right\rangle+ \frac{\eta^{2}L(\kappa+1)}{2}\|g_{t}\|^{2}.\]Conditional on \(w_{t}\), taking expectation on the random sample \(\zeta_{t}\), we have

\[\mathbb{E}_{\zeta}[\mathbb{E}_{gc}[f(w_{t+1})]]\] \[\leq f(w_{t})-\eta\left\langle\nabla f(w_{t}),\nabla f(\tilde{w}_{ t})\right\rangle+\frac{\eta^{2}L(\kappa+1)}{2}\mathbb{E}_{\zeta}\|g_{t}\|^{2}\] \[=f(w_{t})-\eta\left\langle\nabla f(w_{t}),\nabla f(\tilde{w}_{t}) \right\rangle+\frac{\eta^{2}L(\kappa+1)}{2}\mathbb{E}_{\zeta}\|g_{t}-\nabla f (\tilde{w}_{t})+\nabla f(\tilde{w}_{t})\|^{2}\] \[=f(w_{t})-\eta\left\langle\nabla f(w_{t}),\nabla f(\tilde{w}_{t}) \right\rangle+\frac{\eta^{2}L(\kappa+1)}{2}\mathbb{E}_{\zeta}[\|g_{t}-\nabla f (\tilde{w}_{t})\|^{2}+\|\nabla f(\tilde{w}_{t})\|^{2}]\] \[\leq f(w_{t})-\eta\left\langle\nabla f(w_{t}),\nabla f(\tilde{w}_{ t})\right\rangle+\frac{\eta^{2}L(\kappa+1)(\rho+1)}{2}\|\nabla f(\tilde{w}_{t})\|^{2}+ \frac{\eta^{2}L(\kappa+1)\sigma^{2}}{2}\] \[\leq f(w_{t})-\eta\left\langle\nabla f(w_{t}),\nabla f(\tilde{w}_ {t})\right\rangle+\frac{\eta^{2}L(\kappa+1)(\rho+1)}{2}\|\nabla f(\tilde{w}_{t })\|^{2}+\frac{\eta^{2}L(\kappa+1)\sigma^{2}}{2}\] \[=f(w_{t})-\eta\left\langle\nabla f(w_{t})-\nabla f(\tilde{w}_{t}) +\nabla f(\tilde{w}_{t}),\nabla f(\tilde{w}_{t})\right\rangle+\frac{\eta^{2}L (\kappa+1)(\rho+1)}{2}\|\nabla f(\tilde{w}_{t})\|^{2}\] \[\quad+\frac{\eta^{2}L(\kappa+1)\sigma^{2}}{2}\] \[=f(w_{t})-\eta\left\langle\nabla f(w_{t})-\nabla f(\tilde{w}_{t} ),\nabla f(\tilde{w}_{t})\right\rangle-\eta\|\nabla f(\tilde{w}_{t})\|^{2}+ \frac{\eta^{2}L(\kappa+1)(\rho+1)}{2}\|\nabla f(\tilde{w}_{t})\|^{2}\] \[\quad+\frac{\eta^{2}L(\kappa+1)\sigma^{2}}{2}\] \[=f(w_{t})-\eta\left(1-\frac{\eta L(\kappa+1)(\rho+1)}{2}\right) \|\nabla f(\tilde{w}_{t})\|^{2}-\eta\left\langle\nabla f(w_{t})-\nabla f( \tilde{w}_{t}),\nabla f(\tilde{w}_{t})\right\rangle\] \[\quad+\frac{\eta^{2}L(\kappa+1)\sigma^{2}}{2}\] \[\leq f(w_{t})-\eta\left(1-\frac{\eta L(\kappa+1)(\rho+1)}{2} \right)\|\nabla f(\tilde{w}_{t})\|^{2}+\frac{\eta}{2}\|\nabla f(w_{t})-\nabla f (\tilde{w}_{t})\|^{2}\] \[\quad+\frac{\eta}{2}\|\nabla f(\tilde{w}_{t})\|^{2}+\frac{\eta^{2 }L(\kappa+1)\sigma^{2}}{2}\triangleright\langle a,b\rangle\leq\tfrac{1}{2}\|a\|^ {2}+\tfrac{1}{2}\|b\|^{2}\] \[=f(w_{t})-\frac{\eta}{2}\left[1-\eta L(\kappa+1)(\rho+1)\right] \|\nabla f(\tilde{w}_{t})\|^{2}+\frac{\eta}{2}\|\nabla f(w_{t})-\nabla f( \tilde{w}_{t})\|^{2}+\frac{\eta^{2}L(\kappa+1)\sigma^{2}}{2}.\]

Again using smoothness, and taking \(\eta\leq\frac{1}{2L(\rho+1)(\kappa+1)}\), we have \(-\frac{\eta}{2}\left[1-\eta L(\kappa+1)(\rho+1)\right]\leq-\frac{\eta}{4}\), and, we have

\[\mathbb{E}_{\zeta}[\mathbb{E}_{gc}[f(w_{t+1})]]\] \[\leq f(w_{t})-\frac{\eta}{2}\left[1-\eta L(\kappa+1)(\rho+1) \right]\|\nabla f(\tilde{w}_{t})\|^{2}+\frac{\eta L^{2}}{2}\|w_{t}-\tilde{w}_ {t}\|^{2}+\frac{\eta^{2}L(\kappa+1)\sigma^{2}}{2}\] \[\leq f(w_{t})-\frac{\eta}{2}\left[1-\eta L(\kappa+1)(\rho+1) \right]\|\nabla f(\tilde{w}_{t})\|^{2}+\frac{\eta L^{2}}{2}\|e_{t}\|^{2}+ \frac{\eta^{2}L(\kappa+1)\sigma^{2}}{2}\] \[\leq f(w_{t})-\frac{\eta}{4}\|\nabla f(\tilde{w}_{t})\|^{2}+ \frac{\eta L^{2}}{2}\|e_{t}\|^{2}+\frac{\eta^{2}L(\kappa+1)\sigma^{2}}{2},\]

where we define the sequence

\[e_{t}=w_{t}-\tilde{w}_{t},e_{0}=0.\]

Now we establish the upper bound of the sequence \(\|e_{t}\|^{2}\) as follows.

First, using \(w_{t+1}=w_{t}-\eta\mathcal{U}_{g}(g_{t})\) and \(\tilde{w}_{t+1}=\tilde{w}_{t}+\mathcal{C}_{w}(w_{t+1}-\tilde{w}_{t})\), we have the following equations:

\[w_{t+1}-\tilde{w}_{t+1}=e_{t+1}=w_{t}-\tilde{w}_{t}-\eta\mathcal{U}_{g}(g_{t})- \mathcal{C}_{w}(w_{t+1}-\tilde{w}_{t})=e_{t}-\eta\mathcal{U}_{g}(g_{t})- \mathcal{C}_{w}(e_{t}-\eta\mathcal{U}_{g}(g_{t}))\]Taking expectation w.r.t. the random compressor \(\mathcal{C}_{w}\), we have

\[\mathbb{E}_{wc}[\|e_{t+1}\|^{2}]\] \[=\mathbb{E}_{wc}[\|e_{t}-\eta\mathcal{U}_{g}(g_{t})-\mathcal{C}_{ w}(e_{t}-\eta\mathcal{U}_{g}(g_{t}))\|^{2}]\] \[\leq(1-\delta)\|e_{t}-\eta\mathcal{U}_{g}(g_{t})\|^{2}.\]

Taking expectation w.r.t. the random compressor \(\mathcal{U}_{g}\), we have

\[\mathbb{E}_{gc}[\mathbb{E}_{wc}[\|e_{t+1}\|^{2}]]\] \[\leq(1-\delta)\mathbb{E}_{gc}[\|e_{t}-\eta\mathcal{U}_{g}(g_{t}) \|^{2}]\] \[=(1-\delta)\mathbb{E}_{gc}[\|e_{t}-\eta g_{t}+\eta g_{t}-\eta \mathcal{U}_{g}(g_{t})\|^{2}]\] \[=(1-\delta)\|e_{t}-\eta g_{t}\|^{2}+(1-\delta)\eta^{2}\mathbb{E} _{gc}[\|g_{t}-\mathcal{U}_{g}(g_{t})\|^{2}]\] \[\leq(1-\delta)\|e_{t}-\eta g_{t}\|^{2}+(1-\delta)\eta^{2}\kappa\| g_{t}\|^{2}.\]

Conditional on \(w_{t}\), taking expectation on the random sample \(\zeta_{t}\), we have

\[\mathbb{E}_{\zeta}[\mathbb{E}_{gc}[\mathbb{E}_{wc}[\|e_{t+1}\|^{2 }]]]\] \[\leq(1-\delta)\mathbb{E}_{\zeta}[\|e_{t}-\eta\nabla f(\tilde{w}_{ t})+\eta\nabla f(\tilde{w}_{t})-\eta g_{t}\|^{2}]+(1-\delta)\eta^{2}\kappa \mathbb{E}_{\zeta}[\|g_{t}-\nabla f(\tilde{w}_{t})+\nabla f(\tilde{w}_{t})\|^ {2}]\] \[=(1-\delta)\|e_{t}-\eta\nabla f(\tilde{w}_{t})\|^{2}+(1-\delta)( \kappa+1)\eta^{2}\mathbb{E}_{\zeta}[\|g_{t}-\nabla f(\tilde{w}_{t})\|^{2}]+(1 -\delta)\eta^{2}\kappa\|\nabla f(\tilde{w}_{t})\|^{2}\] \[\leq(1-\delta)\|e_{t}-\eta\nabla f(\tilde{w}_{t})\|^{2}+(1-\delta )(\kappa+1)\eta^{2}(\rho\|\nabla f(\tilde{w}_{t})\|^{2}+\sigma^{2})+(1-\delta )\eta^{2}\kappa\|\nabla f(\tilde{w}_{t})\|^{2}\] \[=(1-\delta)\|e_{t}-\eta\nabla f(\tilde{w}_{t})\|^{2}+(1-\delta) \eta^{2}(\rho\kappa+\rho+\kappa)\|\nabla f(\tilde{w}_{t})\|^{2}+(1-\delta)( \kappa+1)\eta^{2}\sigma^{2}.\]

With \(\forall b>0\), we have

\[\mathbb{E}_{\zeta}[\mathbb{E}_{gc}[\mathbb{E}_{wc}[\|e_{t+1}\|^{ 2}]]]\] \[\leq(1-\delta)(1+b)\|e_{t}\|^{2}+(1-\delta)(1+b^{-1})\|\eta\nabla f (\tilde{w}_{t})\|^{2}+(1-\delta)\eta^{2}(\rho\kappa+\rho+\kappa)\|\nabla f( \tilde{w}_{t})\|^{2}\] \[\quad+(1-\delta)(\kappa+1)\eta^{2}\sigma^{2}\] \[=(1-\delta)(1+b)\|e_{t}\|^{2}+(1-\delta)\eta^{2}[1+b^{-1}+(\rho \kappa+\rho+\kappa)]\|\nabla f(\tilde{w}_{t})\|^{2}+(1-\delta)(\kappa+1)\eta ^{2}\sigma^{2}.\]

Then, by taking \(b=\frac{\delta}{2(1-\delta)}\), we have \((1-\delta)(1+b)=1-\frac{\delta}{2}\;,1+b^{-1}=\frac{2-\delta}{\delta}\leq\frac {2}{\delta}\), and

\[\mathbb{E}_{\zeta}[\mathbb{E}_{gc}[\mathbb{E}_{wc}[\|e_{t+1}\|^{ 2}]]]\] \[\leq(1-\frac{\delta}{2})\|e_{t}\|^{2}+(1-\delta)\eta^{2}\left( \frac{2}{\delta}+\rho\kappa+\rho+\kappa\right)\|\nabla f(\tilde{w}_{t})\|^{2} +(1-\delta)(\kappa+1)\eta^{2}\sigma^{2}.\]

We simplify the notation by denoting \(\mathbb{E}[\|e_{t+1}\|^{2}]=\mathbb{E}_{\zeta}[\mathbb{E}_{gc}[\mathbb{E}_{wc} [\|e_{t+1}\|^{2}]]]\), and then unroll the sequence of \(e_{t}\) back to \(t=0\).

\[\mathbb{E}[\|e_{t+1}\|^{2}]\] \[\leq\sum_{\tau=0}^{t}(1-\frac{\delta}{2})^{t-\tau}\left[(1-\delta )\eta^{2}\left(\frac{2}{\delta}+\rho\kappa+\rho+\kappa\right)\|\nabla f( \tilde{w}_{\tau})\|^{2}+(1-\delta)(\kappa+1)\eta^{2}\sigma^{2}\right]\] \[\leq(1-\delta)\eta^{2}\left(\frac{2}{\delta}+\rho\kappa+\rho+ \kappa\right)\sum_{\tau=0}^{t}(1-\frac{\delta}{2})^{t-\tau}\|\nabla f(\tilde{w}_ {\tau})\|^{2}+(1-\delta)(\kappa+1)\eta^{2}\sigma^{2}\sum_{\tau=0}^{t}(1-\frac{ \delta}{2})^{t-\tau}\] \[\leq(1-\delta)\eta^{2}\left(\frac{2}{\delta}+\rho\kappa+\rho+ \kappa\right)\sum_{\tau=0}^{t}(1-\frac{\delta}{2})^{t-\tau}\|\nabla f(\tilde{w}_ {\tau})\|^{2}+\frac{2(1-\delta)(\kappa+1)\eta^{2}\sigma^{2}}{\delta}.\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\triangleright\sum_{\tau=0}^{t}(1- \frac{\delta}{2})^{t-\tau}\leq\frac{1}{1-(1-\frac{\delta}{2})}\]

[MISSING_PAGE_EMPTY:17]

Finally, using Lemma A.1, we have

\[\frac{1}{T+1}\sum_{t=0}^{T}\mathbb{E}[\|\nabla f(\tilde{w}_{t})\|^{2}]\] \[\leq\frac{8}{T+1}\frac{\mathbb{E}[f(w_{0})]-f^{*}+f^{*}-\mathbb{E} [f(w_{T+1})]}{\eta}+\frac{8(11-\delta)(\kappa+1)L\eta\sigma^{2}}{20}\] \[\leq\frac{80L\left(\frac{2}{3}+\rho\kappa+\rho+\kappa\right)(f(w_ {0})-f^{*})}{T+1}+4\sigma\sqrt{\frac{(11-\delta)(\kappa+1)L(f(w_{0})-f^{*})}{T +1}}.\]

## Appendix B Other Evaluation Results

To further demonstrate the effectiveness of SDP4Bit in enhancing training efficiency, we present the relationship between wall clock time and training loss in Figure 9.

To further illustrate the impact of the Hadamard transformation on (de)quantization performance, we provide (de)quantization throughput experiment in Table 5, which is tested on an A100 GPU.

## Appendix C Notations in Training

\begin{table}
\begin{tabular}{|c|c|c c c|} \hline \hline
**Input/Output Size** & \multicolumn{2}{c|}{**Quantization**} & \multicolumn{2}{c|}{**Dequantization**} \\  & **w/o Had.** & **w/ Had.** & **w/o Had.** & **w/ Had.** \\ \hline
8 MB & 305.6\(\pm\)10.9 & 301.8\(\pm\)10.6 & 367.7\(\pm\)10.6 & 359.6\(\pm\)9.6 \\
16 MB & 389.0\(\pm\)12.8 & 387.1\(\pm\) 8.2 & 428.0\(\pm\)10.6 & 428.6\(\pm\)7.6 \\
64 MB & 494.8\(\pm\) 3.7 & 493.7\(\pm\) 2.6 & 505.3\(\pm\) 2.1 & 505.6\(\pm\)2.2 \\
512 MB & 682.1\(\pm\) 0.8 & 681.6\(\pm\) 1.2 & 685.1\(\pm\) 0.8 & 685.2\(\pm\)0.6 \\
1024 MB & 686.5\(\pm\) 1.2 & 686.3\(\pm\) 0.4 & 688.0\(\pm\) 0.3 & 688.0\(\pm\)0.3 \\
2048 MB & 688.6\(\pm\) 0.2 & 688.6\(\pm\) 0.2 & 689.5\(\pm\) 0.2 & 689.4\(\pm\)0.2 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Notations in experiments.

\begin{table}
\begin{tabular}{|c|c|} \hline \hline qW & original ZeRO++ int4 _weight_ quantization \\ \hline qWD & weight difference int4 quantization \\ \hline ULq & original ZeRO++ uniform-level Int4-Int4 all-to-all _gradient_ quantization \\ \hline TLq & two-level Int8-Int4 all-to-all _gradient_ quantization \\ \hline TLq-HS & two-level Int8-Int4 all-to-all _gradient_ quantization with Hadamard Smoother \\ \hline \hline \end{tabular}
\end{table}
Table 5: (De)quantization Throughput with/without Hadamard, including std. dev.

Figure 9: Comparison of validation loss versus wall-clock time for Baseline, ZeRO++ and SDP4Bit on the GPT-6.7B model.

[MISSING_PAGE_FAIL:19]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The claims in the abstract and introduction accurately shows the contribution in the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In paper title, Section 1 Introduction, and Section 7 Conclusion, we state that this work focuses on the specific problem of communication compression of ShardDP on the training of LLMs. Furthermore, in Section 2.2 Quantization, we state that this work only considers symmetric linear quantization, and in Section 6 Related Work we explain that there are other types of compression methods that are not used in our algorithms. In Section 5.1 Experimental Setup, we list the models and the dataset we used in the experiments. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The full set of assumptions could be found in Section 4.2 Convergence Analysis. A complete proof could be found in Appendix A. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The detailed experiment configurations and hyperparameters could be found in Section 5.1 Experimental Setup, and Appendix B, respectively. Furthermore, the source code could be found in the supplementary materials, with a README file containing the instructions to run our experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The dataset could be downloaded from hugginface website. The source code could be found in the supplementary materials, with a README file containing the instructions to run our experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All of these detailed hyperparameters and configurations could be found in Section 5.1 Experimental Setup, and Appendix B. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report the experiment results with standard deviations in Section 5 Evaluation. For the figures and plots, we found that the standard deviations and error bars are barely visible since they are too small compared to the average values.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The detailed information about the computer resources could be found in Section 5.1 Experimental Setup, Hardware. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the NeurIPS Code of Ethics, and we do not think this work would have any negative social impact or potential harmful consequences. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?Answer: [NA] Justification: The goal of this work is to make model training faster with the model accuracy aligned with the original one. This work does not publish any new dataset or new model or the corresponding new application. This work simply proposes new algorithms that accelerate the training of existing models, thus having no social impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper does not release any new dataset or new model. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have cited the original paper that produced the code package or dataset, and included the license in the source code package. Guidelines: ** The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We have included the license in the source code package, which is aligned with the code packages that our work is based on. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA]Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.