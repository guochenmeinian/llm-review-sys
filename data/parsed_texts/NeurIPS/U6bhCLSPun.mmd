# Last-Iterate Convergent Policy Gradient

Primal-Dual Methods for Constrained MDPs

Dongsheng Ding

University of Pennsylvania

dongshed@seas.upenn.edu

&Chen-Yu Wei

University of Virginia

chenyu.wei@virginia.edu

&Kaiqing Zhang

University of Maryland, College Park

kaiqing@umd.edu

&Alejandro Ribeiro

University of Pennsylvania

aribeiro@seas.upenn.edu

Alphabetical order.

###### Abstract

We study the problem of computing an optimal policy of an infinite-horizon discounted constrained Markov decision process (constrained MDP). Despite the popularity of Lagrangian-based policy search methods used in practice, the oscillation of policy iterates in these methods has not been fully understood, bringing out issues such as violation of constraints and sensitivity to hyper-parameters. To fill this gap, we employ the Lagrangian method to cast a constrained MDP into a constrained saddle-point problem in which max/min players correspond to primal/dual variables, respectively, and develop two single-time-scale policy-based primal-dual algorithms with non-asymptotic convergence of their policy iterates to an optimal constrained policy. Specifically, we first propose a regularized policy gradient primal-dual (RPG-PD) method that updates the policy using an entropy-regularized policy gradient, and the dual variable via a quadratic-regularized gradient ascent, simultaneously. We prove that the policy primal-dual iterates of RPG-PD converge to a regularized saddle point with a sublinear rate, while the policy iterates converge sublinearly to an optimal constrained policy. We further instantiate RPG-PD in large state or action spaces by including function approximation in policy parametrization, and establish similar sublinear last-iterate policy convergence. Second, we propose an optimistic policy gradient primal-dual (OPG-PD) method that employs the optimistic gradient method to update primal/dual variables, simultaneously. We prove that the policy primal-dual iterates of OPG-PD converge to a saddle point that contains an optimal constrained policy, with a linear rate. To the best of our knowledge, this work appears to be the first non-asymptotic policy last-iterate convergence result for single-time-scale algorithms in constrained MDPs. We further validate the merits and the effectiveness of our methods in computational experiments.

## 1 Introduction

Constrained Markov decision process (Constrained MDP) is the classical model for constrained dynamic systems in the early stochastic control literature (e.g., [1, 2, 3, 4, 5]) and the recent constrained reinforcement learning (RL) literature (e.g., [6, 7, 8, 9, 10, 11]). It is applicable to many constrained control problems by integrating other system specifications in constraints, and admits a natural extension of constrained optimization and Lagrangian in policy space. Lagrangian-based policy search methods, especially policy-based primal-dual methods that work simultaneously withprimal/dual variables, lie at the heart of recent successes of constrained MDPs, e.g., navigation [12], autonomous driving [13, 14], robotics [15], and finance [16]; see [17, 18, 19, 20] for more examples.

Despite the popularity of policy-based primal-dual algorithms, classical asymptotic convergence assumes that primal-dual updates are in two-time-scale1type [21, 6, 22, 16, 9] (and/or work in two nested loops2), and considerable global non-asymptotic convergence guarantee is measured via an average of past objective/constraint functions [23, 24, 25, 26] or a mixture of past policies [27, 28]. These results are unfavorable in constrained dynamic systems, especially safety-critical ones, due to three reasons: (i) Average and mixture performance of non-asymptotic convergence conceals oscillating (or even overshooting) objective/constraint functions of immediate policy iterates [29, 30], and oscillation-incurred constraint violation impedes a policy iterate being optimal; (ii) Asymptotic convergence is not instructive, because arbitrarily slow convergence, and oscillation and overshoot in any finite time can happen; (iii) Two-time-scale algorithms including algorithms with nested loops are sensitive to hyper-parameters and are therefore typically difficult to tune [16, 9]. Thus, we ask the following question in constrained MDPs:

Footnote 1: One update has relatively very large/small (fast/slow) stepsize than the other.

Can the _policy iterates_ of a _single-time-scale_ policy-based primal-dual algorithm

converge to an optimal constrained policy with _non-asymptotic rate_?

By "single-time-scale", we refer to the classical methods [31, 32, 33] that iterate primal/dual variables concurrently (with the same constant stepsize). Only partial answers to this question are provided in recent studies [34, 35, 36] since they either do not work in the single-time-scale scheme or they do not have non-asymptotic convergence guarantees. In this work, we provide an affirmative answer in two methodologies. First, we initiate the design and analysis of single-time-scale policy-based primal-dual algorithms via regularization, while previous works [24, 28, 34] rely on two-time-scale schemes. Second, inspired by convex minimax optimization [37, 38, 39], we propose a new optimistic policy gradient for a single-time-scale policy-based primal-dual algorithm that solves a class of non-convex minimax problem. While preparing our work, we noticed a contemporaneous work [40], which has empirically validated the effectiveness of other optimistic methods in constrained MDPs, has further inspired the pursuit of our contributions, as outlined in detail below.

**Contributions**. To compute an optimal policy of an infinite-horizon discounted constrained MDP, we employ the Lagrangian method to cast it into a constrained saddle-point problem in which max/min players correspond to primal/dual variables, propose two single-time-scale policy-based primal-dual algorithms, and prove global non-asymptotic convergence of their policy iterates.

* _Nearly dimension-free sublinear last-iterate policy convergence._ We propose a regularized policy gradient primal-dual (RPG-PD) method that updates the policy using an entropy-regularized policy gradient, and the dual using a quadratic-regularized gradient ascent, simultaneously. We prove that the policy primal-dual iterates of RPG-PD converge to a regularized saddle point with a sublinear rate, and the policy iterates converge to an optimal constrained policy sublinearly.
* _Sublinear last-iterate policy convergence with function approximation._ We generalize RPG-PD for constrained MDPs with large state/action spaces by including function approximation in policy parametrization. We prove that the policy primal-dual iterates of an inexact RPG-PD converge to a regularized saddle point with a sublinear rate, but up to a function approximation error, and the policy iterates converge sublinearly to an optimal constrained policy when the error is small.
* _Problem-dependent linear last-iterate policy convergence._ We propose an optimistic policy gradient primal-dual (OPG-PD) method that employs the optimistic gradient method to update the primal/dual, simultaneously. We prove that the policy primal-dual iterates of OPG-PD converge to a saddle point that contains an optimal constrained policy with a problem-dependent linear rate.

While last-iterate convergence is of importance in its own right, by adding proper conservatism in the constraint, both methods can ensure _no_ constraint violation for the _last policy iterate_, which perhaps is best for safety-critical tasks [10, 20]. As far as we know, this work shows the first non-asymptotic and policy last-iterate convergence for single-time-scale algorithms in the constrained MDP literature. We further exhibit the merits and the effectiveness of our methods in experiments.

**Technical comparisons with prior art**. Although global asymptotic last-iterate convergence has been established for single-time-scale algorithms very recently [36; 40], and value-average or policy-mixture non-asymptotic convergence have been established for other algorithms [23; 24; 25; 27; 28; 41; 42; 26], these studies did not investigate global _non-asymptotic_ and _last-iterate_ convergence for _single-time-scale_ algorithms. Our results not only strengthen these prior guarantees, but also set up a new framework for analyzing policy-based primal-dual algorithms via the distance of primal-dual iterates to a saddle point that contains an optimal constrained policy. Our RPG-PD and OPG-PD keep the simplicity of single-time-scale primal-dual methods and output a nearly-optimal policy in the last iterate, which is more convenient than the history-average policies [28; 24] or the policies from subroutines [34; 35]. Compared with the policy-based methods [40], our OPG-PD is a projected policy gradient method that enjoys policy last-iterate convergence with linear rate. Compared with the constrained saddle-point problems [38; 39], our minimax optimization that results from constrained MDP is _non-convex_. Hence, our OPG-PD extends the last-iterate convergence guarantee from convex minimax optimization to a class of non-convex ones, while preserving a linear rate. Compared with the analysis in the two-player zero-sum Markov game [43; 44], there is no reduction from constrained MDPs to per-state bilinear games. Please see more details in Appendix A.

## 2 Preliminaries

We consider an infinite-horizon discounted constrained Markov decision process [3; 5; 8] - CMDP \((\,S,A,P,r,u,b,\gamma,\rho\,)\) - where \(S\) and \(A\) are state/action spaces, \(P\) is a transition kernel that specifies the transition probability \(P(s^{\prime}\,|\,s,a)\) from state \(s\) to next state \(s^{\prime}\) under action \(a\in A\), \(r\), \(u\)\(:S\times A\rightarrow[0,1]\) are reward/utility functions, \(b\) is a constraint threshold, \(\gamma\in[0,1)\) is a discount factor, and \(\rho\) is an initial state distribution. A stationary stochastic policy \(\pi:S\rightarrow\Delta(A)\) determines a probability distribution over the action space \(A\) based on current state, i.e., \(a_{t}\sim\pi(\cdot\,|\,s_{t})\) at time \(t\), where \(\Delta(A)\) is a probability simplex over \(A\). Let \(\Pi\) be the set of all possible stochastic policies. A policy \(\pi\in\Pi\), together with the initial state distribution \(\rho\), induces a distribution over trajectories \(\tau=\{(s_{t},a_{t},r_{t},u_{t})\}_{t=\,0}^{\infty}\), where \(s_{0}\sim\rho\), \(a_{t}\sim\pi(\cdot\,|\,s_{t})\), \(r_{t}=r(s_{t},a_{t})\), \(u_{t}=u(s_{t},a_{t})\) and \(s_{t+1}\sim P(\cdot\,|\,s_{t},a_{t})\) for all \(t\geq 0\).

Given a policy \(\pi\), the value functions \(V_{r}^{\pi}\), \(V_{u}^{\pi}:S\rightarrow\mathbb{R}\) associated with the reward function \(r\) or the utility function \(u\) are given by the expected sums of discounted rewards or utilities under policy \(\pi\):

\[V_{r}^{\pi}(s) := \mathbb{E}\left[\,\sum_{t\,=\,0}^{\infty}\gamma^{t}r(s_{t},a_{t}) \,|\,s_{0}=s\,\right]\,\,\,\text{and}\,\,\,V_{u}^{\pi}(s)\,\,\,:=\,\,\, \mathbb{E}\left[\,\sum_{t\,=\,0}^{\infty}\gamma^{t}u(s_{t},a_{t})\,|\,s_{0}=s\,\right]\]

where the expectation \(\mathbb{E}\) is over the randomness of the trajectory \(\tau\) induced by \(\pi\). Their expected values under \(\rho\) are \(V_{r}^{\pi}(\rho):=\mathbb{E}_{s\sim\rho}[\,V_{r}^{\pi}(s)\,]\) and \(V_{u}^{\pi}(\rho):=\mathbb{E}_{s\sim\rho}[\,V_{u}^{\pi}(s)\,]\). It is useful to introduce the discounted state visitation distribution, \(d_{s_{0}}^{\pi}(s)=(1-\gamma)\sum_{t\,=\,0}^{\infty}\gamma^{t}\text{Pr}(s_{t} =s\,|\,\pi,s_{0})\) which adds up discounted probabilities of visiting \(s\) in the execution of \(\pi\) starting from \(s_{0}\). Denote \(d_{\rho}^{\pi}(s):=\mathbb{E}_{s_{0}\sim\rho}[\,d_{s_{0}}^{\pi}(s)\,]\) and thus \(d_{\rho}^{\pi}(s)\geq(1-\gamma)\rho(s)\) for any \(\rho\) and \(s\). Furthermore, for the reward function \(r\), we introduce the state-action value function \(Q_{r}^{\pi}\): \(S\times A\rightarrow\mathbb{R}\) when the agent begins with a state-action pair \((s,a)\) and follows a policy \(\pi\), and the associated advantage function \(A_{r}^{\pi}\): \(S\times A\rightarrow\mathbb{R}\),

\[Q_{r}^{\pi}(s,a) := \mathbb{E}\left[\,\sum_{t\,=\,0}^{\infty}\gamma^{t}r(s_{t},a_{t}) \,|\,s_{0}=s,a_{0}=a\,\right]\,\,\text{and}\,\,\,A_{r}^{\pi}(s,a)\,\,\,:=\,\,\,Q_ {r}^{\pi}(s,a)-V_{r}^{\pi}(s).\]

Similarly, we define \(Q_{u}^{\pi}:S\times A\rightarrow\mathbb{R}\) and \(A_{u}^{\pi}:S\times A\rightarrow\mathbb{R}\) for the utility function \(u\).

In this work, we aim to find a policy solution \(\pi^{\star}\) of a constrained policy optimization problem,

\[\operatorname*{maximize}_{\pi\,\in\,\Pi}\,\,\,V_{r}^{\pi}(\rho) \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \,\By the method of Lagrange multipliers [45], we dualize the constraint in (1) and present a standard Lagrangian \(L(\pi,\lambda):=\bar{V}_{r}^{\pi}(\rho)+\lambda V_{g}^{\pi}(\rho)\), where \(\pi\in\Pi\) is the primal variable and \(\lambda\in[0,\infty]\) is the dual variable or the Lagrangian multiplier. Introduction of the Lagrangian \(L(\pi,\lambda)\) interprets Problem (1) as a max-min problem: \(\operatorname*{maximize}_{\pi\,\in\,\Pi}\operatorname*{minimize}_{\lambda\, \in\,[0,\infty]}L(\pi,\lambda)\), and thus we can view the Lagrangian \(L(\pi,\lambda)\) as a value function with a composite function \(r+\lambda g\),

\[\operatorname*{maximize}_{\pi\,\in\,\Pi}\,\operatorname*{minimize}_{\lambda \,\in\,[0,\infty]}\,\,V_{r+\lambda g}^{\pi}(\rho).\] (2)

However, it's defective to view Problem (2) as a standard MDP problem by fixing a dual variable \(\lambda\), even the optimal one; also see [9, 46, 47, 30]. This is often referred to as the _scalarization fallacy_[46]; see Appendix B.2 for the detail. From the perspective of game theory, we instead view \(V_{r+\lambda g}^{\pi}(\rho)\): \(\Pi\times[\,0,\infty\,]\to\mathbb{R}\) as a payoff function for a two-player zero-sum game in which max-player is the policy \(\pi\in\Pi\) and min-player is the dual variable \(\lambda\in[\,0,\infty\,]\), and study its saddle points. To proceed, we assume feasibility for Problem (1) throughout our analysis.

**Assumption 1** (Feasibility).: _There exists a policy \(\bar{\pi}\in\Pi\) and \(\xi>0\) such that \(V_{g}^{\bar{\pi}}(\rho)\geq\xi\)._

Feasibility mirrors the Slater condition in the duality analysis of constrained optimization [45]. It can be verified by solving an unconstrained MDP problem with respect to \(V_{g}^{\pi}(\rho)\).

A saddle point \((\pi^{\prime},\lambda^{\prime})\) satisfies \(V_{r+\lambda^{\prime}g}^{\pi}(\rho)\leq V_{r+\lambda^{\prime}g}^{\pi^{\prime}} (\rho)\leq V_{r+\lambda g}^{\pi^{\prime}}(\rho)\) for all \(\pi\in\Pi\), \(\lambda\in[0,\infty]\), or equivalently, \(\pi^{\prime}\) is the max-min point, i.e., \(\pi^{\prime}\in\operatorname*{argmax}_{\pi\,\in\,\Pi}V_{r+\lambda^{\prime}g} ^{\pi}(\rho)\) and \(\lambda^{\prime}\) is the min-max point, i.e., \(\lambda^{\prime}\in\operatorname*{argmin}_{\lambda\,\in\,[0,\infty]}V_{r+ \lambda g}^{\pi^{\prime}}(\rho)\). To view Problem (2) as a saddle-point problem, we denote \(V_{P}^{\pi}(\rho):=\inf_{\lambda\,\in\,[0,\infty]}V_{r+\lambda g}^{\pi}(\rho)\) as the primal function which takes \(V_{r}^{\pi}(\rho)\) when \(V_{g}^{\pi}(\rho)\geq 0\) and \(-\infty\) otherwise, and \(V_{D}^{\lambda}(\rho):=\max_{\pi\,\in\,\Pi}V_{r+\lambda g}^{\pi}(\rho)\) as the dual function. Let an optimal dual variable be \(\lambda^{\star}\in\operatorname*{argmin}_{\lambda\,\in\,[0,\infty]}V_{D}^{ \lambda}(\rho)\). For Problem (1) under Assumption 1, strong duality holds in policy space [12, Theorem 3] and optimal dual variables are bounded [48, Lemma 3].

**Lemma 1** (Strong duality/Saddle point existence and boundedness).: _Let Assumption 1 hold. Then, (i) strong duality holds for Problem (1), i.e., \(V_{P}^{\pi^{\star}}(\rho)=V_{D}^{\lambda^{\star}}(\rho)\); (ii) optimal dual variables are bounded, i.e., \(\lambda^{\star}\in[\,0,(V_{r}^{\pi^{\star}}-V_{r}^{\bar{\pi}})/\xi\,]\)._

Let the set of max-min points be \(\Pi^{\star}:=\operatorname*{argmax}_{\pi\,\in\,\Pi}\min_{\lambda\,\in\,[0, \infty]}V_{r+\lambda g}^{\pi}(\rho)\) and the set of min-max points be \(\Lambda^{\star}:=\operatorname*{argmin}_{\lambda\,\in\,[0,\infty]}\max_{\pi\, \in\,\Pi}V_{r+\lambda g}^{\pi}(\rho)\). From Lemma 1 (ii), \(\Lambda^{\star}\) is contained in a bounded interval \(\Lambda:=[\,0,1/((1-\gamma)\xi)\,]\). Lemma 1 (i) shows that any pair \((\pi^{\star},\lambda^{\star})\in\Pi^{\star}\times\Lambda^{\star}\) solves the following constrained saddle-point problem,

\[\operatorname*{maximize}_{\pi\,\in\,\Pi}\,\operatorname*{minimize}_{\lambda\, \in\,\Lambda}\,\,\,V_{r+\lambda g}^{\pi}(\rho)\,\,\,=\,\,\,\operatorname*{ minimize}_{\lambda\,\in\,\Lambda}\,\,\operatorname*{maximize}_{\pi\,\in\,\Pi}\,\,\,V_{r+ \lambda g}^{\pi}(\rho).\] (3)

Any saddle points associated with the set \(\Lambda^{\star}\) are captured by Problem (3) due to the invariance of saddle points, and searching for any pair \((\pi^{\star},\lambda^{\star})\in\Pi^{\star}\times\Lambda^{\star}\) is sufficient by the interchangeability of saddle points; see Lemmas 8-9 in Appendix B.3 for the properties of saddle points. Thus, we view the policy (primal) as max-player and the dual as min-player in a zero-sum game.

Three structural properties from constrained MDPs distinguish Problem (3) from recent last-iterate convergence for learning in zero-sum games (e.g., [37, 38, 43, 39, 44]): (i) Two players are _asymmetric_. One plays a stochastic policy that affects the transition dynamics and the other selects an action in a continuous interval that only changes the payoff; (ii) Problem (3) is a _non-convex_ game, because of the non-concavity of the payoff \(V_{r+\lambda g}^{\pi}(\rho)\) in policy \(\pi\) (e.g., [49, Lemma 1]); (iii) A saddle-point policy for Problem (3) cannot be _uniformly_ max-min optimal, i.e., being optimal _across all states_, since an optimal policy often depends on the initial state distribution \(\rho\) in a constrained MDP; see Appendix B.1. Hence, known last-iterate results in zero-sum convex games or symmetric Markov games that admit uniformly optimal policies can't be applied and new techniques are required to address this non-standard saddle-point problem, which warrants our contributions in this work.

**Warm-up: Indirect policy search in occupancy-measure space**. Finding a saddle point of a non-convex game is hard in general [50]. Nevertheless, Problem (1) can be rewritten as a linear program regarding the occupancy measure [5], which permits _indirectly_ searching for a saddle point of a bilinear Lagrangian [51, 42, 36]. These asymptotic or average-iterate convergence results can be easily strengthened by applying last-iterate convergence results for bilinear games (e.g., [38, 39]) to be non-asymptotic and last-iterate. By doing so, we state an optimistic primal-dual (OPD) method (18) in Appendix B.4. Compared with a contemporaneous work [40], OPD is free of projection to an occupancy measure set, and enjoys strengthened linear convergence.

OPD is an _indirect_ policy search method that iterates using occupancy measure-based gradients, not policy-based gradients. It is crucial to develop _direct_ policy search methods that are widely-used in RL, which is our focus. We propose two such methods in Section 3 and Section 4, respectively.

## 3 Policy Last-Iterate Convergence: Regularized Method

Towards achieving policy last-iterate convergence, a practical strategy is using regularization [52] to "convexify" Problem (3). We present a regularized method - Regularized Policy Gradient Primal-Dual (RPG-PD) - that converges to a saddle point that yields an optimal constrained policy.

### Regularized policy gradient primal-dual method

We introduce a regularized Lagrangian \(L_{\tau}(\pi,\lambda):=V^{\pi}_{r+\lambda_{g}}(\rho)+\tau(\mathcal{H}(\pi)+ \frac{1}{2}\lambda^{2})\) by adding a regularization term \(\mathcal{H}(\pi)+\frac{1}{2}\lambda^{2}\) onto the original Lagrangian \(V^{\pi}_{r+\lambda_{g}}(\rho)\), where \(\tau\) is a regularization parameter, and \(\mathcal{H}(\pi):=\mathbb{E}\big{[}\sum_{t\,=\,0}^{\infty}-\gamma^{t}\log\pi( a_{t}\,|\,s_{t})\big{]}\) is an entropy-like regularization term [52]. We now introduce a regularized constrained saddle-point problem,

\[\underset{\pi\,\in\,\Pi}{\text{maximize}}\ \ \underset{\lambda\,\in\,\Lambda}{ \text{minimize}}\ \ L_{\tau}(\pi,\lambda)\ =\ \underset{\lambda\,\in\,\Lambda}{\text{minimize}}\ \ \underset{\pi\,\in\,\Pi}{\text{maximize}}\ \ L_{\tau}(\pi,\lambda).\] (4)

Problem (4) is well-defined, since there exists a saddle point for \(L_{\tau}(\pi,\lambda)\) over \(\Pi\times\Lambda\) and it is unique; see Appendix C.1 for proof. A saddle point \((\pi^{\star}_{\tau},\lambda^{\star}_{\tau})\), i.e., \(\pi^{\star}_{\tau}=\operatorname*{argmax}_{\pi\,\in\,\Pi}\min_{\lambda\,\in\, \Lambda}L_{\tau}(\pi,\lambda)\) and \(\lambda^{\star}_{\tau}=\operatorname*{argmin}_{\lambda\,\in\,\Lambda}\max_{ \pi\,\in\,\Pi}L_{\tau}(\pi,\lambda)\), satisfies a sandwich-like property,

\[V^{\pi}_{r+\lambda^{\star}_{\tau}g}(\rho)-\tau\mathcal{H}(\pi^{\star}_{\tau}) \ \leq\ V^{\pi^{\star}_{\tau}}_{r+\lambda_{\tau}g}(\rho)\ \leq\ V^{\pi^{\star}_{\tau}}_{r+\lambda_{g}}(\rho)+\frac{\tau}{2}\lambda^{2} \ \text{ for all }(\pi,\lambda)\in\Pi\times\Lambda\] (5)

that states that \((\pi^{\star}_{\tau},\lambda^{\star}_{\tau})\) is a saddle point of the original Lagrangian \(V^{\pi}_{r+\lambda_{g}}(\rho)\), up to two \(\tau\)-terms. We thus propose a regularized policy gradient primal-dual (RPG-PD) method by maintaining a sequence for policy and dual variables each: \(\{\pi_{t}\}_{t\,\geq\,0}\) for the policy-player, and \(\{\lambda_{t}\}_{t\,\geq\,0}\) for the dual-player,

\[\pi_{t+1}(\cdot\,|\,s) = \underset{\pi(\cdot\,|\,s)\,\in\,\hat{\Delta}(A)}{\operatorname*{ argmin}}\left\{\sum_{a}\pi(a\,|\,s)Q^{\pi_{t}}_{r+\lambda_{t}g+\tau\psi_{t}}(s,a) \,-\,\frac{1}{\eta}\operatorname*{KL}(\pi(\cdot\,|\,s),\pi_{t}(\cdot\,|\,s))\right\}\] (6a) \[\lambda_{t+1} = \underset{\lambda\,\in\,\Lambda}{\operatorname*{argmin}}\left\{ \lambda\Big{(}V^{\pi_{t}}_{g}(\rho)+\tau\lambda_{t}\Big{)}\,+\,\frac{1}{2\eta }\left(\lambda-\lambda_{t}\right)^{2}\right\},\] (6b)

where the gradient direction \(Q^{\pi_{t}}_{r+\lambda_{t}g+\tau\psi_{t}}(s,a)\) is the state-action value function under a composite function \(r+\lambda_{t}g+\tau\psi_{t}\) in which \(\psi_{t}(s,a):=-\log\pi_{t}(a\,|\,s)\), \(\operatorname*{KL}(p,p^{\prime}):=\sum_{a}p_{a}\log\frac{p_{a}}{p^{\prime}_{a}}\) is the Kullback-Leibler (KL) divergence, \(\hat{\Delta}(A):=\{\pi(\cdot\,|\,s)\in\Delta(A)\,|\,\pi(a\,|\,s)\geq\frac{ \epsilon_{0}}{|A|},a\in A\}\) is a restricted probability simplex set with parameter \(\epsilon_{0}\in(0,1)\), \(\eta\) is the stepsize, and \((\pi_{0}(\cdot\,|\,s),\lambda_{0})\in\hat{\Delta}(A)\times\Lambda\) is an initial point. Projecting the policy iterate to the simplex set \(\hat{\Delta}(A)\) ensures the boundedness of the gradient. Primal update (6a) works as the classical mirror descent with KL divergence [53] with a projection onto the set \(\hat{\Delta}(A)\). Dual update (6b) performs typical projected gradient descent. Hence, RPG-PD is a single-time-scale method. RPG-PD simplifies the two-time-scale method [34] to be single-time-scale and generalize the single-time-scale methods [23; 54] with regularization.

### Policy last-iterate convergence

In Theorem 2, we show that the primal-dual iterates of RPG-PD converge in the last iterate; see Appendix C.2 for proof. We characterize the convergence via a distance metric \(\Phi_{t}=\operatorname*{KL}_{t}(\rho)+\frac{1}{2}(\lambda^{\star}_{\tau}- \lambda_{t})^{2}\), where \(\operatorname*{KL}_{t}(\rho):=(1/(1-\gamma))\sum_{s}d^{\pi^{\star}_{\tau}}_{ \rho}(s)\operatorname*{KL}_{t}(s)\) and \(\operatorname*{KL}_{t}(s):=\operatorname*{KL}(\pi^{\star}_{\tau}(\cdot\,|\,s),\pi_{t}(\cdot\,|\,s))\).

**Theorem 2** (Linear convergence of RPG-PD).: _Let Assumption 1 hold. If we set the stepsize \(\eta\leq 1/C_{\tau,\xi,\epsilon_{0}}\), then the primal-dual iterates of RPG-PD (6) satisfy_

\[\Phi_{t+1} \leq \mathrm{e}^{-\eta\tau t}\,\Phi_{1}\,+\,\frac{\eta}{\tau}\,\max \big{(}\,(C_{\tau,\xi,\epsilon_{0}})^{2},\,(C^{\prime}_{\tau,\xi})^{2}\,\big{)}\]

_where \(C_{\tau,\xi,\epsilon_{0}}:=(1+1/((1-\gamma)\xi)+\tau\log|A|)/(1-\gamma)-\tau\log( \epsilon_{0}/|A|)\), \(C^{\prime}_{\tau,\xi}:=(1+\tau/\xi)/(1-\gamma)\)._Theorem 2 states that the primal-dual iterates of RPG-PD converge to a neighborhood of \((\pi_{\tau}^{\star},\lambda_{\tau}^{\star})\) in a linear rate. The size of neighborhood scales with \(\eta/\tau+\eta\tau(1+\log^{2}\epsilon_{0})\) and the convergence rate is \(\eta\tau\). Even if \(\epsilon_{0}\) is very small, the \(\log\epsilon_{0}\)-term is almost a constant. If we take \(\eta=\min(\epsilon\tau,1/C_{\tau,\xi,\epsilon_{0}})\) and \(\epsilon_{0}=\epsilon\), then after \(O(1/\epsilon)\) iterations the RPG-PD's primal-dual iterate \((\pi_{t},\lambda_{t})\) is \(\epsilon\)-close to \((\pi_{\tau}^{\star},\lambda_{\tau}^{\star})\), i.e., \(\Phi_{t}=O(\epsilon)\) for any \(t\geq(1/(\epsilon\tau^{2}))\log(1/\epsilon)\). For small \(\tau\), we can translate the policy convergence for the value functions in Corollary 3; see Appendix C.3 for proof.

**Corollary 3** (Nearly-optimal constrained policy).: _Let Assumption 1 hold. For small \(\epsilon>0\), if we take \(\eta=\Theta(\epsilon^{4})\), \(\tau=\Theta(\epsilon^{2})\), and \(\epsilon_{0}=\epsilon\), then the policy iterates of RPG-PD (6) satisfy_

\[V_{r}^{\pi^{*}}(\rho)-V_{r}^{\pi_{t}}(\rho)\ \leq\ \epsilon\ \text{ and }\ -V_{g}^{\pi_{t}}(\rho)\ \leq\ \epsilon\ \text{ for any }t=\Omega\left(\,\frac{1}{\epsilon^{6}}\log^{2}\frac{1}{\epsilon}\,\right)\]

_where \(\Omega(\cdot)\) only has some problem-dependent constant._

Corollary 3 states that the last policy iterate of RPG-PD is an \(\epsilon\)-optimal policy for Problem (1) after \(\Omega(1/\epsilon^{6})\) iterations. Compared with the single-time-scale methods [23; 54], RPG-PD improves the convergence from average-value (or regret-type) to _last policy iterate_. Not just being theoretically stronger, the last-iterate convergence is more appealing since it captures the stability of trajectories of an algorithm [29; 40]. Compared with the two-time-scale methods [28; 24; 34; 35], RPG-PD is free of nested loops, and uniform ergodicity and exploratory initial state distribution. We notice that the dual methods [28; 24] yield history-average policies and the dual methods [34; 35] return policies from a subroutine. In contrast, RPG-PD outputs a nearly-optimal policy in the last iterate, the first-of-its-kind in the constrained MDP literature, albeit the rate is worse than the average ones [23; 54].

To get zero constraint violation, i.e., \(V_{g}^{\pi_{t}}(\rho)\geq 0\) at some \(t\), it is straightforward to employ a conservative constraint \(V_{g^{\prime}}^{\pi}(\rho)\geq 0\) with \(g^{\prime}:=g-(1-\gamma)\delta\) for some \(\delta>0\). When \(\epsilon\) is small enough, there always exists some \(\delta\) such that the policy iterates of RPG-PD (6) satisfy \(V_{r}^{\pi^{*}}(\rho)-V_{r}^{\pi_{t}}(\rho)\leq\epsilon\) and \(V_{g}^{\pi_{t}}(\rho)\geq 0\) for large \(t\); see Appendix C.4 for proof. Our zero constraint violation ensures the last policy iterate of RPG-PD to satisfy the constraint, which is not the zero average constraint violation in the episodic setting [55; 56]. Compared with the zero constraint violation of a policy induced by an average of past occupancy measures [42], RPG-PD's zero constraint violation directly settles the policy iterates down, which appears to be the first policy-based zero constraint violation.

Last but not least, the iteration complexity of RPG-PD is nearly-free of the MDP dimension, except for an \(\log|A|\)-term, which inherits the dimension-free property of the NPG methods [49; 57; 23]. Hence, it is ready to view RPG-PD as a variant of NPG methods and generalize RPG-PD to constrained MDPs with large state spaces in the function approximation setting.

### Linear function approximation case

To deal with large state spaces, we use a parametrized policy \(\pi_{\theta}\) with \(\theta\in\mathbb{R}^{d}\) for RPG-PD (6) without restricting \(\Delta(A)\), where \(d\) is much smaller than the size of state/action spaces. To introduce function approximation, we begin with a tabular softmax policy \(\pi_{\theta}(a\,|\,s)=\frac{\exp(\theta_{s,a})}{\sum_{a^{\prime}}\exp(\theta_ {s,a^{\prime}})}\) for all \((s,a)\in S\times A\) and \(\theta\in\mathbb{R}^{|S||A|}\). Connecting NPG to mirror descent [49; 58], we express RPG-PD (6) as a NPG method with the following update; see Appendix C.5 for proof,

\[\theta_{t+1} = \theta_{t}\,+\,\eta\,(1-\gamma)F_{\rho}(\theta_{t})^{\dagger}\cdot \nabla_{\theta}L_{\tau}(\pi_{\theta_{t}},\lambda_{t})\] (7a) \[\lambda_{t+1} = \mathcal{P}_{\Lambda}\left(\,(1-\eta\tau)\lambda_{t}-\eta V_{g}^{ \pi_{\theta_{t}}}(\rho)\,\right)\] (7b)

where \(F_{\rho}(\theta_{t})^{\dagger}\cdot\nabla_{\theta}L_{\tau}(\pi_{\theta_{t}}, \lambda_{t})\) is a NPG direction, and \(F_{\rho}(\theta)\) is the Fisher information matrix for a policy \(\pi_{\theta}\), i.e., \(F_{\rho}(\theta):=\,\mathbb{E}_{s\sim d_{\theta}^{\pi_{\theta}}}\mathbb{E}_{a \sim\pi_{\theta}(\cdot\,|\,s)}[\,\nabla_{\theta}\log\pi_{\theta}(a\,|\,s)( \nabla_{\theta}\log\pi_{\theta}(a\,|\,s))^{\top}\,]\). A useful property of (7a) is that NPG can be related to a linear regression. For any policy \(\pi_{\theta}\) and a state-action value function \(Q^{\pi_{\theta}}\), the associated compatible function approximation error is \(E_{Q}(w,\theta,\nu):=\mathbb{E}_{(s,a)\sim\,\nu^{\prime}}[\,(w^{\top}\nabla \log\pi_{\theta}(a\,|\,s)-Q^{\pi_{\theta}}(s,a))^{2}\,]\), where \(\nu(s,a)=d_{\rho}^{\pi_{\theta}}(s)\pi_{\theta}(a\,|\,s)\) is a state-action distribution. It is known that (7a) is equivalent to \(\theta_{t+1}=\theta_{t}+\eta w_{\tau}^{\star}\), where \(w_{\tau}^{\star}\in\operatorname*{argmin}_{w\in\,\mathbb{R}^{d}}E_{Q}(w,\theta_ {t},\nu_{t})\) in which \(Q^{\pi_{\theta_{t}}}(s,a)=Q_{r+\lambda_{t}g+\tau\psi_{t}}(s,a)\) and \(\nu_{t}(s,a)=d_{\rho}^{\pi_{\theta_{t}}}(s)\pi_{\theta_{t}}(a\,|\,s)\) (e.g., [59, Lemma 1]). In practice, only an approximate minimizer \(w_{\tau}^{\star}\) is available if a sample-based algorithm is used, e.g., \(w_{t}\approx\operatorname*{argmin}_{\|w\|\,\leq\,W}E_{Q}(w,\theta_{t},\nu_{t})\), where \(W>0\).

A useful generalization of the softmax policy to large state spaces is the log-linear policy based on linear function approximation. Let \(\phi_{s,a}\in\mathbb{R}^{d}\) be a feature map with \(\|\phi_{s,a}\|\leq 1\) for each state-action pair \((s,a)\). A log-linear policy \(\pi_{\theta}\): \(S\rightarrow\Delta(A)\) is parametrized by a parameter \(\theta\in\mathbb{R}^{d}\),

\[\pi_{\theta}(a\,|\,s) = \frac{\exp\left(\phi_{s,a}^{\top}\,\theta\,\right)}{\sum_{a^{ \prime}}\exp\left(\phi_{s,a^{\prime}}^{\top}\,\theta\,\right)}\,\,\,\text{for all}\,\,(s,a)\in S\times A\]

which takes the tabular softmax policy as a special case, i.e., \(\phi_{s,a}\) is an indicator function. We notice that \(\nabla_{\theta}\log\pi_{\theta}(a\,|\,s)=\phi_{s,a}-\mathbb{E}_{a^{\prime} \sim\pi_{\theta}(\cdot\,|\,s)}\,[\,\phi_{s,a^{\prime}}\,]\). Since the log-linear policy is invariant to any action-independent term, it is convenient to replace \(\nabla_{\theta}\log\pi_{\theta}(a\,|\,s)\) by \(\phi_{s,a}\) and we introduce a simplified compatible function approximation error, \(\mathcal{E}_{Q}(w,\theta,\nu):=\mathbb{E}_{(s,a)\sim\nu}\,[\,(\phi_{s,a}^{\top }w-Q^{\pi_{\theta}}(s,a))^{2}\,]\). Thus, we can take \(w_{t}^{\star}\in\operatorname*{argmin}_{w\,\in\,\mathbb{R}^{d}}\mathcal{E}_{Q }(w,\theta_{t},\nu_{t})\) in which \(Q^{\pi_{\theta_{t}}}(s,a)=Q_{r+\lambda_{t}g+\tau\psi_{t}}^{\pi_{\theta_{t}}}(s,a)\) and \(\nu_{t}(s,a)=d_{\rho}^{\pi_{\theta_{t}}}(s)\,\pi_{\theta_{t}}(a\,|\,s)\) to update \(\theta_{t+1}=\theta_{t}+\eta w_{t}^{\star}\). Using the log-linear policy class, we replace the primal gradient direction of RPG-PD (6) by its linear function approximation \(\phi_{s,a}^{\top}w_{t}^{\star}\),

\[\pi_{\theta_{t+1}}(\cdot\,|\,s) = \operatorname*{argmax}_{\pi(\cdot\,|\,s)\,\in\,\hat{\Delta}(A)} \left\{\sum_{a}\pi(a\,|\,s)\phi_{s,a}^{\top}w_{t}^{\star}\,-\,\frac{1}{\eta} \,\text{KL}(\pi(\cdot\,|\,s),\pi_{\theta_{t}}(\cdot\,|\,s))\right\}\] (8)

which, together with Dual update (6b), leads to a general version of RPG-PD. The set \(\hat{\Delta}(A)\) ensures bounded true gradient direction \(Q_{r+\lambda_{t}g+\tau\psi_{t}}^{\pi_{\theta_{t}}}(s,a)\). When there is no function approximation error, (8) reduces to Primal update (6a). In practice, we can only compute \(w_{t}^{\star}\) approximately via

\[w_{t}\,\approx\,\operatorname*{argmin}_{\|w\|\,\leq\,W}\,\mathcal{E}_{Q}(w, \theta_{t},d_{t,\nu})\]

which leads to an inexact RPG-PD: Primal update (8) in which \(w_{t}^{\star}\) is replaced by \(w_{t}\) and Dual update (6b), where \(d_{t,\nu}:=(1-\gamma)\mathbb{E}_{(s_{0},a_{0})\sim\nu}\sum_{t=0}^{\infty}\gamma ^{t}\text{Pr}(s_{t}=s,a_{t}=a\,|\,\pi_{\theta_{t}},s_{0},a_{0})\) is a state-action distribution starting from any distribution \(\nu\). Noticeably, \(d_{t,\nu}\) is more general than \(\nu_{t}\). To control the function approximation error, we divide \(\mathcal{E}_{Q}(w_{t},\theta_{t},d_{t,\nu})\) into a statistical error \(\mathcal{E}_{Q}(w_{t},\theta_{t},d_{t,\nu})-\mathcal{E}_{Q}(w_{t}^{\star}, \theta_{t},d_{t,\nu})\) that is similar to the excess risk in supervised learning, and an approximation error \(\mathcal{E}_{Q}(w_{t}^{\star},\theta_{t},d_{t,\nu})\) that captures how well a linear function \((w_{t}^{\star})^{\top}\phi_{s,a}\) approximates the true value function under \(d_{t,\nu}\). If the on-policy distribution \(d_{t,\nu}\) in \(\mathcal{E}_{Q}(w_{t}^{\star},\theta_{t},d_{t,\nu})\) is replaced by \(\nu^{\star}(s,a)=d_{\rho}^{\pi^{\star}_{\rho}}(s)\,\text{Unif}_{A}(a)\), we define a transfer error \(\mathcal{E}_{Q}(w_{t}^{\star},\theta_{t},\nu^{\star})\). Let the covariance matrix of \(\phi_{s,a}\) in any state-action distribution \(\nu\) be \(\Sigma_{\nu}:=\mathbb{E}_{(s,a)\sim\nu}\,[\,\phi_{s,a}\phi_{s,a}^{\top}]\), and the relative condition number between \(\Sigma_{\nu}\) and \(\Sigma_{\nu^{\star}}\) be \(\kappa_{\nu}:=\max_{w\,\in\,\mathbb{R}^{d}}\frac{w^{\top}\Sigma_{\nu^{\star} }w}{w^{\top}\Sigma_{\nu}w}\).

We make an assumption on the statistical error, the transfer error, and the relative condition number.

**Assumption 2**.: _(i) There exist \(\epsilon_{\text{stat}}\), \(\epsilon_{\text{bias}}>0\) such that \(\mathbb{E}\,[\,\mathcal{E}_{Q}(w_{t},\theta_{t},d_{t,\nu})-\mathcal{E}_{Q}(w_ {t}^{\star},\theta_{t},d_{t,\nu})\,]\leq\epsilon_{\text{stat}}\) and \(\mathbb{E}\,[\,\mathcal{E}_{Q}(w_{t}^{\star},\theta_{t},\nu^{\star})\,]\leq \epsilon_{\text{bias}}\); (ii) The relative condition number is finite, i.e., \(\kappa_{\nu}<\infty\)._

We assess the convergence of inexact RPG-PD via the distance metric \(\mathbb{E}\,[\,\Phi_{t}\,]:=\mathbb{E}\,[\,\text{KL}_{t}(\rho)\,]+\frac{1}{2} \mathbb{E}\,[\,(\lambda_{\tau}^{\star}-\lambda_{t})^{2}\,]\), where the expectation \(\mathbb{E}\) is over the randomness of computing \(w_{t}\) via a sample-based algorithm. We state the convergence in Theorem 4 and delay its proof to Appendix C.6.

**Theorem 4** (Linear convergence of inexact RPG-PD).: _Let Assumptions 1-2 hold. If we take the stepsize \(\eta\leq 1/C_{W}\), then the primal-dual iterates of inexact RPG-PD satisfy_

\[\mathbb{E}[\,\Phi_{t+1}\,] \leq \mathrm{e}^{-\eta\tau t}\mathbb{E}[\,\Phi_{1}\,]\,+\,\frac{\eta}{ \tau}\max\big{(}\,(C_{W})^{2},(C_{r,\tau}^{\prime})^{2}\,\big{)}\,+\,\frac{2}{ \tau}\left(\sqrt{|A|\epsilon_{\text{bias}}}+\sqrt{|A|\kappa_{\nu}\epsilon_{ \text{stat}}}\right)\]

_where \(C_{W}:=2W/(1-\gamma)\) and \(C_{\tau,\xi}^{\prime}:=(1+\tau/\xi)/(1-\gamma)\)._

Theorem 4 states that the primal-dual iterates of inexact RPG-PD converge to a neighborhood of \((\pi_{\tau}^{\star},\lambda_{\tau}^{\star})\) in a linear rate. The convergence rate is \(\eta\tau\) and the size of neighborhood scales with a sum of an \(\eta/\tau\)-term and an \(1/\tau\)-term that amplifies the effect of function approximation \((\epsilon_{\text{stat}},\epsilon_{\text{bias}})\). We note that, Theorem 4 does not require the strong duality in the parametrized policy class, and the function approximation error includes the duality gap caused by the inexpensiveness of function class and the policy representation error caused by the restricted policy set \(\hat{\Delta}(A)\). When there is no function approximation error, Theorem 4 has a similar result as Theorem 2. It is important to control \((\epsilon_{\text{stat}},\epsilon_{\text{bias}})\) to be small: (i) Application of stochastic gradient methods to the linear regression leads to \(\epsilon_{\text{stat}}=O(1/\sqrt{K})\) or \(O(1/K)\), where \(K\) is the number of gradient steps, and thus, it is easy to control \(\epsilon_{\text{stat}}\); (ii) When \(\epsilon_{0}\) is very small, the parametrized policy iterate can be contained in \(\hat{\Delta}(A)\), and thus \(\epsilon_{\text{bias}}\) becomes zero in some cases, e.g., tabular softmax case [49] or low-rank MDPs [60; 61] with \(d\geq|A|\); it can be made very small if the function class is rich, e.g., wide neural networks [62]. When the errors are small, it is ready to establish Corollary 5; see Appendix C.7 for proof.

**Corollary 5** (Nearly-optimal constrained policy).: _Let Assumptions 1-2 hold and \(\epsilon_{\text{stat}}\), \(\epsilon_{\text{bias}}=O(\epsilon^{8})\) for small \(\epsilon\), \(\epsilon_{0}>0\). If we take the stepsize \(\eta=\Theta(\epsilon^{4})\) and \(\tau=\Theta(\epsilon^{2})\), then the policy iterates of inexact RPG-PD satisfy_

\[\mathbb{E}\left[\,V_{r}^{\pi^{*}}(\rho)-V_{r}^{\pi_{\theta_{t}}}(\rho)\, \right]\ \leq\ \epsilon\ \ \text{ and }\ \mathbb{E}\left[\,-V_{g}^{\pi_{\theta_{t}}}(\rho)\,\right]\ \leq\ \epsilon\ \ \text{ for any }t=\Omega\left(\,\frac{1}{\epsilon^{6}}\log^{2}\frac{1}{\epsilon}\,\right)\]

_where \(\Omega(\cdot)\) only has some problem-dependent constant._

Corollary 5 states that the iteration complexity in Corollary 3 holds in the function approximation case. When \(\epsilon\) is small enough, we can design a conservative constraint such that the policy iterates of inexact RPG-PD satisfy \(V_{r}^{\pi^{*}}(\rho)-V_{r}^{\pi_{\theta_{t}}}(\rho)\leq\epsilon\) and \(V_{g}^{\pi_{\theta_{t}}}(\rho)\geq 0\) for large \(t\); see Appendix C.8 for proof. Compared with the zero average constraint violation [63], this appears to be the first policy-based zero constraint violation result in the function approximation setting. Moreover, we extend inexact RPG-PD to be a sample-based algorithm and provide its sample complexity in Appendix C.9.

## 4 Policy Last-Iterate Convergence: Optimistic Method

Having established sublinear policy last-iterate convergence via regularization, we turn to the optimistic gradient method [37] for a faster rate. We propose an optimistic method - Optimistic Policy Gradient Primal-Dual (OPG-PD) - that converges an optimal constrained policy at a linear rate.

### Optimistic policy gradient primal-dual method

We propose an optimistic policy gradient primal-dual (OPG-PD) method by maintaining two sequences for policy and dual variables each: \(\{\pi_{t}\}_{t\,\geq\,1}\) and \(\{\hat{\pi}_{t}\}_{t\,\geq\,1}\) for the policy-player, and \(\{\lambda_{t}\}_{t\,\geq\,1}\) for the dual-player,

\[\pi_{t}(\cdot\,|\,s) = \operatorname*{argmax}_{\pi(\cdot\,|\,s)\,\in\,\Delta(A)}\left\{ \sum_{a}\pi(a\,|\,s)Q_{r+\lambda_{t-1}g}^{\pi_{t-1}}(s,a)\,-\,\frac{1}{2\eta} \,\left\|\pi(\cdot\,|\,s)-\hat{\pi}_{t}(\cdot\,|\,s)\right\|^{2}\right\}\] (9a) \[\hat{\pi}_{t+1}(\cdot\,|\,s) = \operatorname*{argmax}_{\pi(\cdot\,|\,s)\,\in\,\Delta(A)}\left\{ \sum_{a}\pi(a\,|\,s)Q_{r+\lambda_{t}g}^{\pi_{t}}(s,a)\,-\,\frac{1}{2\eta}\, \left\|\pi(\cdot\,|\,s)-\hat{\pi}_{t}(\cdot\,|\,s)\right\|^{2}\right\}\] (9b) \[\lambda_{t} = \operatorname*{argmin}_{\lambda\,\in\,\Lambda}\left\{\lambda\,V_{ g}^{\pi_{t-1}}(\rho)\,+\,\frac{1}{2\eta}\,(\lambda-\hat{\lambda}_{t})^{2}\right\}\] \[\hat{\lambda}_{t+1} = \operatorname*{argmin}_{\lambda\,\in\,\Lambda}\left\{\lambda\,V_{ g}^{\pi_{t}}(\rho)\,+\,\frac{1}{2\eta}\,(\lambda-\hat{\lambda}_{t})^{2}\right\}\]

where \(\eta\) is the stepsize and \((\hat{\pi}_{0},\hat{\lambda}_{0})=(\pi_{0},\lambda_{0})\in\Pi\times\Lambda\) is the initial point. OPG-PD concurrently works with two primal iterates and two dual iterates, and each two are updated consecutively to stabilize the algorithm dynamics. The "optimistic" in optimization, e.g., [64] views \((\hat{\pi}_{t+1},\hat{\lambda}_{t+1})\)-update as a real policy gradient step and \((\pi_{t},\lambda_{t})\)-update as a prediction step that generates an intermediate iterate \((\pi_{t},\lambda_{t})\). Not policy gradient at \((\hat{\pi}_{t},\hat{\lambda}_{t})\), the real step uses a policy gradient at \((\pi_{t},\lambda_{t})\) from prediction, exhibiting the optimism towards the prediction. Specifically, Primal update (9a) works as the projected \(Q\)-ascent [58; 65], an application of the classical mirror descent with Euclidean distance [53], where the projection onto a probability simplex can be solved efficiently [66]. Dual update (9b) performs standard projected gradient descent. We note that OPG-PD is different from the one-step multiplicative weights update in the policy-based ReLOAD [40].

When there is no MDP transition dynamics, i.e., constrained bandit [40], last-iterate convergence of OPG-PD to a saddle point is known in the minimax optimization [67; 68; 38; 39], because Problem (3) reduces to a bilinear zero-sum game in this case. However, it is prohibitive to apply such bilinear game results to the Lagrangian \(V_{r+\lambda g}^{\pi}(s)\) in every state \(s\), as has been done for zero-sum Markov games [43; 44]. The main reason for this is that there may not exist an optimal constrained policy that is uniformly optimal across all states; see Appendix B.2.

### Policy last-iterate convergence

We define the distribution mismatch coefficient over \(\rho\) as \(\kappa_{\rho}\,:=\sup_{\pi\,\in\,\Pi}\left\|d_{\rho}^{\pi}/\rho\right\|_{\infty}\), which is the maximum distribution mismatch of policy \(\pi\) relative to \(\rho\), where \(d_{\rho}^{\pi}/\rho\) is divided per state. Hence, \(\left\|d_{\rho}^{\pi}/d_{\rho}^{\pi^{*}}\right\|_{\infty}\leq\kappa_{\rho}/(1- \gamma)\) for any policy \(\pi\in\Pi\) and \(\kappa_{\rho}\leq 1/\rho_{\min}\) where \(\rho_{\min}:=\min_{s}\rho(s)\). The projection operator \(\mathcal{P}_{X}\) on a closed convex set \(X\) defines \(\mathcal{P}_{X}(x):=\operatorname*{argmin}_{x^{\prime}\in\,X}\|x^{\prime}-x\|\).

We state the policy last-iterate convergence of OPG-PD (9) in Theorem 6.

**Theorem 6** (Linear convergence of OPG-PD).: _Let Assumption 1 hold. Assume the optimal state visitation distribution be unique, i.e., \(d_{\rho}^{\pi}=d_{\rho}^{\pi^{*}}\) for any \(\pi\in\Pi^{\star}\), and \(\rho_{\min}>0\). If we set the stepsize \(\eta\,\leq\,\min\left(1/(4\sqrt{\iota}),(1-\gamma)^{3}/(4|A|),(1-\gamma)^{3}/( 2\kappa_{\rho})\right)\), where \(\iota>0\) is defined in Appendix D.1, then the primal-dual iterates of OPG-PD (9) satisfy_

\[\frac{1}{2(1-\gamma)}\sum_{s}d_{\rho}^{\pi^{*}}(s)\left\|\mathcal{P}_{\Pi^{ \star}}(\hat{\pi}_{t}(\iota\,|\,s))-\hat{\pi}_{t}(\cdot\,|\,s)\right\|^{2}\,+ \,\frac{1}{2}(\mathcal{P}_{\Lambda^{\star}}(\hat{\lambda}_{t})-\hat{\lambda}_ {t})^{2} \leq \left(\frac{1}{1+C}\right)^{t}\]

_where \(C:=\min(7(1-\gamma)/8,7\eta^{2}(1-\gamma)^{2}(C_{\rho,\xi})^{2}\rho_{\min}/(6 \kappa_{\rho,\gamma})^{2})\) in which \(C_{\rho,\xi}\) and \(\kappa_{\rho,\gamma}\) are given by \(C_{\rho,\xi}:=c\rho_{\min}/(2\sqrt{|S||A|})/(1+1/((1-\gamma)\xi))\), \(\kappa_{\rho,\gamma}:=\max(\kappa_{\rho}/(1-\gamma),1)\), and \(c>0\) is a problem-dependent constant from Lemma 26._

Theorem 6 states that the primal-dual iterates of OPG-PD converge to \(\Pi^{\star}\times\Lambda^{\star}\) in a linear rate, or putting it differently, (9) is contracting to a set of optimal primal/dual variables. The rate is governed by a problem-dependent constant. Proof of Theorem 6 is provided in Appendix D. A key to our analysis is to bridge the per-state policy gradient update and the policy improvement for \(V_{r+\lambda g}^{\pi}(\rho)\) that is non-convex in policy \(\pi\), which departs from the convex last-iterate analysis [38; 39]. In addition, we address two technical difficulties. First, the lack of uniformly optimal policies prevents learning an optimal policy from per-state bilinear games in zero-sum Markov games [43; 44]. Instead, we characterize the proximity of primal-dual iterates to a saddle point supported by an optimal state visitation distribution \(d_{\rho}^{\pi^{*}}\). Second, Problem (3) is an asymmetric game since one plays a stochastic policy over a finite set of discrete actions and controls the transition dynamics, but the other selects an action in a continuous interval. Thus, our dual-player analysis handles the long-term effect of the policy-player, which did not appear in the symmetric game [43; 44].

A direct corollary of Theorem 6 is stated below; see Appendix D.3 for the proof.

**Corollary 7** (Nearly-optimal constrained policy).: _Let Assumption 1 hold and the optimal state visitation distribution be unique, i.e., \(d_{\rho}^{\pi}=d_{\rho}^{\pi^{*}}\) for any \(\pi\in\Pi^{\star}\), and \(\rho_{\min}>0\). If we use the stepsize \(\eta\) from Theorem 6, then the policy iterates of OPG-PD (9) satisfy_

\[V_{r}^{\pi^{*}}(\rho)-V_{r}^{\hat{\pi}_{t}}(\rho)\ \leq\ \epsilon\ \text{ and }\ -V_{g}^{\hat{\pi}_{t}}(\rho)\ \leq\ \epsilon\ \text{ for any }t=\Omega\left(\log^{2}\frac{1}{\epsilon}\,\right)\]

_where \(\Omega(\cdot)\) only has some problem-dependent constant._

Corollary 7 states that the last policy iterate of OPG-PD is an \(\epsilon\)-optimal policy for Problem (1) after an almost constant number of iterations, which improves the sublinear rate in Corollary 3. OPG-PD also improves the average-value convergence of the single-time-scale methods [23; 54] and the two-time-scale methods [28; 34; 35; 24], and matches the last-iterate convergence rate of the two-time-scale methods [34; 35]. We stress that our last-iterate convergence indicates the stability of whole primal-dual iterates, which is not the last policy iterate from a subroutine [34; 35]. Again, when \(\epsilon\) is small, we can design a conservative constraint such that the policy iterates of OPG-PD satisfy \(V_{r}^{\pi^{*}}(\rho)-V_{r}^{\hat{\pi}_{t}}(\rho)\leq\epsilon\) and \(V_{g}^{\hat{\pi}_{t}}(\rho)\geq 0\) for large \(t\); see Appendix D.4 for the proof.

## 5 Computational Experiment

We validate the effectiveness of RPG-PD (6) and OPG-PD (9) by comparing them with typical primal-dual methods in Figure 1. A few observations are in order. The initial oscillation of RPG-PD(--) is damped, and OPG-PD (--) is almost free of oscillation as PID Lagrangian (....). However, oscillation of NPG-PD (--) causes its last-iterate policy violating the constraint \(V_{\tau}^{\pi}(\rho)\geq 0\). OPG-PD (--) reaches the maximum reward value in four methods and RPG-PD (--) converges to a slightly smaller value due to regularization, while both meet the constraint at the end. However, PID Lagrangian (...) is highly sub-optimal. Hence, our methods OPG-PD and RPG-PD can overcome oscillation and approach a nearly-optimal constrained policy in the last-iterate fashion.

We showcase the linear convergence of OPG-PD (9) with three constant stepsizes in Figure 2. Three policy optimality gaps decrease linearly in the logarithmic scale plot, which verifies the linear last-iterate convergence of OPG-PD's policy iterates in Theorem 6. Noticeably, there is no oscillation behavior in OPG-PD's policy iterates, which perhaps is best for learning constraints [29, 10]. We also see that a large stepsize \(\eta=0.2\) improves the convergence, which is reflected by our rate.

Please see Appendix E for more details of this experiment, more baselines, and sensitivity analysis.

## 6 Concluding Remarks

We have presented two single-time-scale policy-based primal-dual methods for finding an optimal policy of a constrained MDP, with global non-asymptotic and last-iterate policy convergence guarantees. Our first regularized method enjoys a nearly dimension-free sublinear rate, while our second optimistic method possesses a linear rate that is problem-dependent. Our work stimulates a number of compelling future directions: (i) Our problem setting circumvents the exploration difficulty, which leaves online exploration open; (ii) Our convergence rates are not as sharp as solving convex-concave minimax optimization problems, regarding the order or instance-related constant; (iii) Last-iterate convergence is under-examined in constrained MDPs with other constraints, and unexplored for other gradient methods.

Figure 1: Convergence performance of RPG-PD, OPG-PD, and primal-dual methods. Learning curves of our RPG-PD (—) and OPG-PD (—), and NPG-PD [23] (—) and PID Lagrangian [29] (...) methods. The horizontal axes mean the policy iterations \(\{\pi_{t}\}_{t\geq 0}\) that are generated by each method and the vertical axes mean the value functions of the policy iterates \(\{\pi_{t}\}_{t\geq 0}\): reward value \(V_{\tau}^{\pi_{t}}(\rho)\) (Left) and utility value \(V_{g}^{\pi_{t}}(\rho)\) (Right). In this experiment, we use the same stepsize \(\eta=0.1\) for all methods, the regularization parameter \(\tau=0.08\) for RPG-PD, and the uniform initial distribution \(\rho\).

Figure 2: Convergence performance of OPG-PD with stepsize \(\eta\): (\(\eta=0.05\),...), (\(\eta=0.1\), --), (\(\eta=0.2\), --). The horizontal axis represents the policy iterations \(\{\pi_{t}\}_{t\geq 0}\) that are generated by OPG-PD and the vertical axis means the policy optimality gap that measures the distance of the policy iterates \(\{\pi_{t}\}_{t\geq 0}\) to an optimal policy \(\pi^{\star}\): \(\sum_{s}\left\|\pi_{t}(\cdot\,|\,s)-\pi^{\star}(\cdot\,|\,s)\right\|^{2}\). In this experiment, we take the initial distribution \(\rho\) to be a uniform one.

## Acknowledgments and Disclosure of Funding

D. Ding and A. Ribeiro were supported by THEORINET Simons-NSF MoDL, and DCIST CRA. K. Zhang acknowledges the support from Simons-Berkeley Research Fellowship and Northrop Grumman - Maryland Seed Grant Program.

## References

* [1] Frederick J Beutler and Keith W Ross. Optimal policies for controlled Markov chains with a constraint. _Journal of Mathematical Analysis and Applications_, 112(1):236-252, 1985.
* [2] Keith W Ross. Randomized and past-dependent policies for Markov decision processes with multiple constraints. _Operations Research_, 37(3):474-477, 1989.
* [3] Alexey Piunovskiy. Control of random sequences in problems with constraints. _Theory of Probability & Its Applications_, 38(4):751-762, 1994.
* [4] Eugene A Feinberg and Adam Shwartz. Constrained discounted dynamic programming. _Mathematics of Operations Research_, 21(4):922-945, 1996.
* [5] Eitan Altman. _Constrained Markov decision processes_, volume 7. CRC press, 1999.
* [6] Vivek S Borkar. An actor-critic algorithm for constrained Markov decision processes. _Systems & Control Letters_, 54(3):207-213, 2005.
* [7] Naoki Abe, Prem Melville, Cezar Pendus, Chandan K Reddy, David L Jensen, Vince P Thomas, James J Bennett, Gary F Anderson, Brent R Cooley, Melissa Kowalczyk, et al. Optimizing debt collections using constrained reinforcement learning. In _Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 75-84, 2010.
* [8] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In _Proceedings of the International Conference on Machine Learning_, pages 22-31, 2017.
* [9] Chen Tessler, Daniel J Mankowitz, and Shie Mannor. Reward constrained policy optimization. In _Proceedings of the International Conference on Learning Representations_, 2019.
* [10] Gabriel Dulac-Arnold, Nir Levine, Daniel J Mankowitz, Jerry Li, Cosmin Paduraru, Sven Gowal, and Todd Hester. Challenges of real-world reinforcement learning: definitions, benchmarks and analysis. _Machine Learning_, 110(9):2419-2468, 2021.
* [11] Amol Mandhane, Anton Zhernov, Maribeth Rauh, Chenjie Gu, Miaosen Wang, Flora Xue, Wendy Shang, Derek Pang, Rene Claus, Ching-Han Chiang, et al. Muzero with self-competition for rate control in VP9 video compression. _arXiv preprint arXiv:2202.06626_, 2022.
* [12] Santiago Paternain, Miguel Calvo-Fullana, Luiz FO Chamon, and Alejandro Ribeiro. Safe policies for reinforcement learning via primal-dual methods. _IEEE Transactions on Automatic Control_, 2022.
* [13] Xiangkun He, Haohan Yang, Zhongxu Hu, and Chen Lv. Robust lane change decision making for autonomous vehicles: An observation adversarial reinforcement learning approach. _IEEE Transactions on Intelligent Vehicles_, 2022.
* [14] Ziqing Gu, Lingping Gao, Haitong Ma, Shengbo Eben Li, Sifa Zheng, Wei Jing, and Junbo Chen. Safe-state enhancement method for autonomous driving via direct hierarchical reinforcement learning. _IEEE Transactions on Intelligent Transportation Systems_, 2023.

* [15] Dan A Calian, Daniel J Mankowitz, Tom Zahavy, Zhongwen Xu, Junhyuk Oh, Nir Levine, and Timothy Mann. Balancing constraints and rewards with meta-gradient D4PG. In _Proceedings of the International Conference on Learning Representations_, 2021.
* [16] Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained reinforcement learning with percentile risk criteria. _The Journal of Machine Learning Research_, 18(1):6070-6120, 2017.
* [17] Javier Garcia and Fernando Fernandez. A comprehensive survey on safe reinforcement learning. _Journal of Machine Learning Research_, 16(1):1437-1480, 2015.
* [18] Frits De Nijs, Erwin Walraven, Mathijs De Weerdt, and Matthijs Spaan. Constrained multiagent Markov decision processes: A taxonomy of problems and algorithms. _Journal of Artificial Intelligence Research_, 70:955-1001, 2021.
* [19] Lukas Brunke, Melissa Greeff, Adam W Hall, Zhaocong Yuan, Siqi Zhou, Jacopo Panerati, and Angela P Schoellig. Safe learning in robotics: From learning-based control to safe reinforcement learning. _Annual Review of Control, Robotics, and Autonomous Systems_, 5:411-444, 2022.
* [20] Shangding Gu, Long Yang, Yali Du, Guang Chen, Florian Walter, Jun Wang, Yaodong Yang, and Alois Knoll. A review of safe reinforcement learning: Methods, theory and applications. _arXiv preprint arXiv:2205.10330_, 2022.
* a gradient approach. In _Proceedings of the 41st IEEE Conference on Decision and Control_, volume 2, pages 1940-1945, 2002.
* [22] Shalabh Bhatnagar and K Lakshmanan. An online actor-critic algorithm with function approximation for constrained Markov decision processes. _Journal of Optimization Theory and Applications_, 153:688-708, 2012.
* [23] Dongsheng Ding, Kaiqing Zhang, Tamer Basar, and Mihailo R Jovanovic. Natural policy gradient primal-dual method for constrained Markov decision processes. _Proceedings of the Advances in Neural Information Processing Systems_, 33:8378-8390, 2020.
* [24] Tao Liu, Ruida Zhou, Dileep Kalathil, PR Kumar, and Chao Tian. Policy optimization for constrained MDPs with provable fast global convergence. _arXiv preprint arXiv:2111.00552_, 2021.
* [25] Arushi Jain, Sharan Vaswani, Reza Babanezhad, Csaba Szepesvari, and Doina Precup. Towards painless policy optimization for constrained MDPs. In _Proceedings of the Uncertainty in Artificial Intelligence_, pages 895-905, 2022.
* [26] Sihan Zeng, Thinh T Doan, and Justin Romberg. Finite-time complexity of online primal-dual natural actor-critic algorithm for constrained Markov decision processes. In _Proceedings of the IEEE 61st Conference on Decision and Control_, pages 4028-4033, 2022.
* [27] Yi Chen, Jing Dong, and Zhaoran Wang. A primal-dual approach to constrained Markov decision processes. _arXiv preprint arXiv:2101.10895_, 2021.
* [28] Tianjiao Li, Ziwei Guan, Shaofeng Zou, Tengyu Xu, Yingbin Liang, and Guanghui Lan. Faster algorithm and sharper analysis for constrained Markov decision process. _arXiv preprint arXiv:2110.10351_, 2021.
* [29] Adam Stooke, Joshua Achiam, and Pieter Abbeel. Responsive safety in reinforcement learning by PID Lagrangian methods. In _Proceedings of the International Conference on Machine Learning_, pages 9133-9143, 2020.

* [30] Miguel Calvo-Fullana, Santiago Paternain, Luiz FO Chamon, and Alejandro Ribeiro. State augmented constrained reinforcement learning: Overcoming the limitations of learning with rewards. _IEEE Transactions on Automatic Control_, 2023.
* [31] Kenneth J. Arrow, Leonid Hurwicz, and Hirofumi Uzawa. _Studies in linear and non-linear programming_. Stanford University Press, 1958.
* [32] Galina M Korpelevich. The extragradient method for finding saddle points and other problems. _Matecon_, 12:747-756, 1976.
* [33] Angelia Nedic and Asuman Ozdaglar. Subgradient methods for saddle-point problems. _Journal of Optimization Theory and Applications_, 142:205-228, 2009.
* [34] Donghao Ying, Yuhao Ding, and Javad Lavaei. A dual approach to constrained Markov decision processes with entropy regularization. In _Proceedings of the International Conference on Artificial Intelligence and Statistics_, pages 1887-1909, 2022.
* [35] Egor Gladin, Maksim Lavrik-Karmazin, Karina Zainullina, Varvara Rudenko, Alexander Gasnikov, and Martin Takac. Algorithm for constrained Markov decision process with linear convergence. In _Proceedings of the International Conference on Artificial Intelligence and Statistics_, pages 11506-11533. PMLR, 2023.
* [36] Tianqi Zheng, Pengcheng You, and Enrique Mallada. Constrained reinforcement learning via dissipative saddle flow dynamics. _arXiv preprint arXiv:2212.01505_, 2022.
* [37] Yu-Guan Hsieh, Franck Iutzeler, Jerome Malick, and Panayotis Mertikopoulos. On the convergence of single-call stochastic extra-gradient methods. _Proceedings of the Advances in Neural Information Processing Systems_, 32, 2019.
* [38] Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Linear last-iterate convergence in constrained saddle-point optimization. In _Proceedings of the International Conference on Learning Representations_, 2020.
* [39] Yang Cai, Argyris Oikonomou, and Weiqiang Zheng. Tight last-iterate convergence of the extragradient and the optimistic gradient descent-ascent algorithm for constrained monotone variational inequalities. _arXiv preprint arXiv:2204.09228_, 2022.
* [40] Ted Moskovitz, Brendan O'Donoghue, Vivek Veeriah, Sebastian Flennerhag, Satinder Singh, and Tom Zahavy. ReLOAD: Reinforcement learning with optimistic ascent-descent for last-iterate convergence in constrained MDPs. _arXiv preprint arXiv:2302.01275_, 2023.
* [41] Sharan Vaswani, Lin Yang, and Csaba Szepesvari. Near-optimal sample complexity bounds for constrained MDPs. In _Proceedings of the Advances in Neural Information Processing Systems_, volume 35, pages 3110-3122, 2022.
* [42] Qinbo Bai, Amrit Singh Bedi, Mridul Agarwal, Alec Koppel, and Vaneet Aggarwal. Achieving zero constraint violation for constrained reinforcement learning via primal-dual approach. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 3682-3689, 2022.
* [43] Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Last-iterate convergence of decentralized optimistic gradient descent/ascent in infinite-horizon competitive Markov games. In _Proceedings of the Conference on learning theory_, pages 4259-4299, 2021.
* [44] Zhuoqing Song, Jason D Lee, and Zhuoran Yang. Can we find Nash equilibria at a linear rate in Markov games? In _Proceedings of the International Conference on Learning Representations_, 2023.
* [45] Dimitri P Bertsekas. _Constrained optimization and Lagrange multiplier methods_. Academic Press, 2014.

* [46] Csaba Szepesvari. Constrained MDPs and the reward hypothesis. http://readingsml.blogspot.com/2020/03/constrained-mdps-and-reward-hypothesis.html, 2020. Access on January 21, 2023.
* [47] Tom Zahavy, Brendan O'Donoghue, Guillaume Desjardins, and Satinder Singh. Reward is enough for convex MDPs. _Proceedings of the Advances in Neural Information Processing Systems_, 34:25746-25759, 2021.
* [48] Dongsheng Ding, Kaiqing Zhang, Jiali Duan, Tamer Basar, and Mihailo R Jovanovic. Convergence and sample complexity of natural policy gradient primal-dual methods for constrained MDPs. _arXiv preprint arXiv:2206.02346_, 2022.
* [49] Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. _The Journal of Machine Learning Research_, 22(1):4431-4506, 2021.
* [50] Constantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis. The complexity of constrained min-max optimization. In _Proceedings of the Annual ACM SIGACT Symposium on Theory of Computing_, pages 1466-1478, 2021.
* [51] Yujia Jin and Aaron Sidford. Efficiently solving MDPs with stochastic mirror descent. In _Proceedings of the International Conference on Machine Learning_, pages 4890-4900, 2020.
* [52] Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi. Fast global convergence of natural policy gradient methods with entropy regularization. _Operations Research_, 70(4):2563-2578, 2022.
* [53] Sebastien Bubeck. Convex optimization: Algorithms and complexity. _Foundations and Trends(r) in Machine Learning_, 8(3-4):231-357, 2015.
* [54] Dongsheng Ding and Mihailo R Jovanovic. Policy gradient primal-dual mirror descent for constrained MDPs with large state spaces. In _Proceedings of the IEEE 61st Conference on Decision and Control_, pages 4892-4897, 2022.
* [55] Tao Liu, Ruida Zhou, Dileep Kalathil, Panganamala Kumar, and Chao Tian. Learning policies with zero or bounded constraint violation for constrained MDPs. _Proceedings of the Advances in Neural Information Processing Systems_, 34:17183-17193, 2021.
* [56] Honghao Wei, Xin Liu, and Lei Ying. Triple-Q: A model-free algorithm for constrained reinforcement learning with sublinear regret and zero constraint violation. In _Proceedings of the International Conference on Artificial Intelligence and Statistics_, pages 3274-3307, 2022.
* [57] Shicong Cen, Yuting Wei, and Yuejie Chi. Fast policy extragradient methods for competitive games with entropy regularization. _Proceedings of the Advances in Neural Information Processing Systems_, 34:27952-27964, 2021.
* [58] Jalaj Bhandari and Daniel Russo. On the linear convergence of policy gradient methods for finite mdps. In _Proceedings of the International Conference on Artificial Intelligence and Statistics_, pages 2386-2394, 2021.
* [59] Rui Yuan, Simon Shaolei Du, Robert M Gower, Alessandro Lazaric, and Lin Xiao. Linear convergence of natural policy gradient methods with log-linear policies. In _Proceedings of the International Conference on Learning Representations_, 2022.
* [60] Lin Yang and Mengdi Wang. Sample-optimal parametric Q-learning using linearly additive features. In _Proceedings of the International Conference on Machine Learning_, pages 6995-7004, 2019.

* [61] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In _Proceedings of the Conference on Learning Theory_, pages 2137-2143, 2020.
* [62] Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global optimality and rates of convergence. In _Proceedings of the International Conference on Learning Representations_, 2020.
* [63] Qinbo Bai, Amrit Singh Bedi, and Vaneet Aggarwal. Achieving zero constraint violation for constrained reinforcement learning via conservative natural policy gradient primal-dual algorithm. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 6737-6744, 2023.
* [64] Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In _Proceedings of the Conference on Learning Theory_, pages 993-1019. PMLR, 2013.
* [65] Lin Xiao. On the convergence rates of policy gradient methods. _Journal of Machine Learning Research_, 23(282):1-36, 2022.
* [66] Laurent Condat. Fast projection onto the simplex and the \(l_{1}\) ball. _Mathematical Programming_, 158(1-2):575-585, 2016.
* [67] C Daskalakis and Ioannis Panageas. Last-iterate convergence: Zero-sum games and constrained min-max optimization. In _Proceedings of the Innovations in Theoretical Computer Science (ITCS) Conference_, 2019.
* [68] Qi Lei, Sai Ganesh Nagarajan, Ioannis Panageas, et al. Last iterate convergence in no-regret learning: constrained min-max optimization for convex-concave landscapes. In _Proceedings of the International Conference on Artificial Intelligence and Statistics_, pages 1441-1449, 2021.
* [69] Dongsheng Ding, Kaiqing Zhang, Tamer Basar, and Mihailo R Jovanovic. Convergence and optimality of policy gradient primal-dual method for constrained Markov decision processes. In _Proceedings of the American Control Conference_, pages 2851-2856, 2022.
* [70] Ruida Zhou, Tao Liu, Dileep Kalathil, Panganamala Kumar, and Chao Tian. Anchor-changing regularized natural policy gradient for multi-objective reinforcement learning. In _Proceedings of the Advances in Neural Information Processing Systems_, 2022.
* [71] Aria HasanzadeZonuzy, Dileep Kalathil, and Srinivas Shakkottai. Model-based reinforcement learning for infinite-horizon discounted constrained Markov decision processes. In _Proceedings of the International Joint Conference on Artificial Intelligence_, 2021.
* [72] Leonid Denisovich Popov. A modification of the Arrow-Hurwicz method for search of saddle points. _Mathematical Notes of the Academy of Sciences of the USSR_, 28:845-848, 1980.
* [73] Ming Yu, Zhuoran Yang, Mladen Kolar, and Zhaoran Wang. Convergent policy optimization for safe reinforcement learning. In _Proceedings of the Advances in Neural Information Processing Systems_, volume 32, 2019.
* [74] Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J Ramadge. Projection-based constrained policy optimization. In _Proceedings of the International Conference on Learning Representations_, 2020.
* [75] Yiming Zhang, Quan Vuong, and Keith Ross. First order constrained optimization in policy space. In _Proceedings of the Advances in Neural Information Processing Systems_, volume 33, pages 15338-15349, 2020.
* [76] Long Yang, Yu Zhang, Jiaming Ji, Juntao Dai, and Weidong Zhang. CUP: A conservative update policy algorithm for safe reinforcement learning, 2022.

* [77] Tengyu Xu, Yingbin Liang, and Guanghui Lan. CRPO: A new approach for safe reinforcement learning with convergence guarantee. In _Proceedings of the International Conference on Machine Learning_, pages 11480-11491, 2021.
* [78] Yongshuai Liu, Jiaxin Ding, and Xin Liu. IPO: Interior-point policy optimization under constraints. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 4940-4947, 2020.
* [79] Yonathan Efroni, Shie Mannor, and Matteo Pirotta. Exploration-exploitation in constrained MDPs. _arXiv preprint arXiv:2003.02189_, 2020.
* [80] Dongsheng Ding, Xiaohan Wei, Zhuoran Yang, Zhaoran Wang, and Mihailo R Jovanovic. Provably efficient safe exploration via primal-dual policy optimization. In _Proceedings of the International Conference on Artificial Intelligence and Statistics_, pages 3304-3312, 2021.
* [81] Liyuan Zheng and Lillian Ratliff. Constrained upper confidence reinforcement learning. In _Proceedings of the Learning for Dynamics and Control_, pages 620-629, 2020.
* [82] Shuang Qiu, Xiaohan Wei, Zhuoran Yang, Jieping Ye, and Zhaoran Wang. Upper confidence primal-dual reinforcement learning for CMDP with adversarial loss. In _Proceedings of the Advances in Neural Information Processing Systems_, volume 33, pages 15277-15287, 2020.
* [83] Kiante Brantley, Miro Dudik, Thodoris Lykouris, Sobhan Miryoosefi, Max Simchowitz, Aleksandrs Slivkins, and Wen Sun. Constrained episodic reinforcement learning in concave-convex and knapsack settings. In _Proceedings of the Advances in Neural Information Processing Systems_, volume 33, pages 16315-16326, 2020.
* [84] Tiancheng Yu, Yi Tian, Jingzhao Zhang, and Suvrit Sra. Provably efficient algorithms for multi-objective competitive RL. In _Proceedings of the International Conference on Machine Learning_, pages 12167-12176, 2021.
* [85] Thiago D Simao, Nils Jansen, and Matthijs TJ Spaan. AlwaysSafe: Reinforcement learning without safety constraint violations during training. In _Proceedings of the 20th International Conference on Autonomous Agents and Multi-Agent Systems_, pages 1226-1235, 2021.
* [86] Krishna C Kalagarla, Rahul Jain, and Pierluigi Nuzzo. A sample-efficient algorithm for episodic finite-horizon MDP with constraints. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 8030-8037, 2021.
* [87] Sobhan Miryoosefi and Chi Jin. A simple reward-free approach to constrained reinforcement learning. In _Proceedings of the International Conference on Machine Learning_, pages 15666-15698, 2022.
* [88] Honghao Wei, Xin Liu, and Lei Ying. A provably-efficient model-free algorithm for infinite-horizon average-reward constrained Markov decision processes. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 3868-3876, 2022.
* [89] Liyu Chen, Rahul Jain, and Haipeng Luo. Learning infinite-horizon average-reward Markov decision process with constraints. In _Proceedings of the International Conference on Machine Learning_, pages 3246-3270, 2022.
* [90] Archana Bura, Aria HasanzadeZonuzy, Dileep Kalathil, Srinivas Shakkottai, and Jean-Francois Chamberland. DOPE: Doubly optimistic and pessimistic exploration for safe reinforcement learning. In _Proceedings of the Advances in Neural Information Processing Systems_, volume 35, pages 1047-1059, 2022.
* [91] Arnob Ghosh, Xingyu Zhou, and Ness Shroff. Provably efficient model-free constrained RL with linear function approximation. In _Proceedings of the Advances in Neural Information Processing Systems_, 2022.

* [92] Rahul Singh, Abhishek Gupta, and Ness B Shroff. Learning in constrained Markov decision processes. _IEEE Transactions on Control of Network Systems_, 10(1):441-453, 2022.
* [93] Honghao Wei, Arnob Ghosh, Ness Shroff, Lei Ying, and Xingyu Zhou. Provably efficient model-free algorithms for non-stationary CMDPs. In _Proceedings of the International Conference on Artificial Intelligence and Statistics_, pages 6527-6570, 2023.
* [94] Jacopo Germano, Francesco Emanuele Stradi, Gianmarco Genalti, Matteo Castiglioni, Alberto Marchesi, and Nicola Gatti. A best-of-both-worlds algorithm for constrained MDPs with long-term constraints. _arXiv preprint arXiv:2304.14326_, 2023.
* [95] Krishna C Kalagarla, Rahul Jain, and Pierluigi Nuzzo. Safe posterior sampling for constrained MDPs with bounded constraint violation. _arXiv preprint arXiv:2301.11547_, 2023.
* [96] Soumyajit Guin and Shalabh Bhatnagar. A policy gradient approach for finite horizon constrained Markov decision processes. _arXiv preprint arXiv:2210.04527_, 2022.
* [97] Santiago Paternain, Luiz Chamon, Miguel Calvo-Fullana, and Alejandro Ribeiro. Constrained reinforcement learning has zero duality gap. In _Proceedings of the Advances in Neural Information Processing Systems_, 2019.
* [98] Wenhao Zhan, Shicong Cen, Baihe Huang, Yuxin Chen, Jason D Lee, and Yuejie Chi. Policy mirror descent for regularized reinforcement learning: A generalized framework with linear convergence. _SIAM Journal on Optimization_, 33(2):1061-1091, 2023.
* [99] Guanghui Lan. Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes. _Mathematical Programming_, 198(1):1059-1106, 2023.
* [100] Paul Tseng. On linear convergence of iterative methods for the variational inequality problem. _Journal of Computational and Applied Mathematics_, 60(1-2):237-252, 1995.
* [101] Yu Malitsky. Projected reflected gradient methods for monotone variational inequalities. _SIAM Journal on Optimization_, 25(1):502-520, 2015.
* [102] Jacob Abernethy, Kevin A Lai, and Andre Wibisono. Last-iterate convergence rates for min-max optimization: Convergence of Hamiltonian gradient descent and consensus optimization. In _Proceedings of the Algorithmic Learning Theory_, pages 3-47, 2021.
* [103] Yang Cai, Argyris Oikonomou, and Weiqiang Zheng. Accelerated algorithms for monotone inclusions and constrained nonconvex-nonconcave min-max optimization. _arXiv preprint arXiv:2206.05248_, 2022.
* [104] Quoc Tran-Dinh. Extragradient-type methods with O\((1/k)\)-convergence rates for co-hypomonotone inclusions. _arXiv preprint arXiv:2302.04099_, 2023.
* [105] Stephen J Wright. _Primal-dual interior-point methods_. SIAM, 1997.
* [106] Michele Benzi, Gene H Golub, and Jorg Liesen. Numerical solution of saddle point problems. _Acta Numerica_, 14:1-137, 2005.
* [107] Kenneth J Arrow and Leonid Hurwicz. Reduction of constrained maxima to saddle-point problems. In _Traces and Emergence of Nonlinear Programming_, pages 61-80. 2013.
* [108] Kazufumi Ito and Karl Kunisch. _Lagrange multiplier approach to variational problems and applications_. SIAM, 2008.
* [109] Bingsheng He and Xiaoming Yuan. On the acceleration of augmented Lagrangian method for linearly constrained optimization. _Optimization Online_, 3, 2010.

* [110] Alp Yurtsever, Quoc Tran Dinh, and Volkan Cevher. A universal primal-dual convex optimization framework. In _Proceedings of the Advances in Neural Information Processing Systems_, volume 28, 2015.
* [111] Bingsheng He and Xiaoming Yuan. On non-ergodic convergence rate of Douglas-Rachford alternating direction method of multipliers. _Numerische Mathematik_, 130(3):567-577, 2015.
* [112] Ya-Feng Liu, Xin Liu, and Shiqian Ma. On the nonergodic convergence rate of an inexact augmented Lagrangian framework for composite convex programming. _Mathematics of Operations Research_, 44(2):632-650, 2019.
* [113] Yangyang Xu. Iteration complexity of inexact augmented Lagrangian methods for constrained convex programming. _Mathematical Programming_, 185:199-244, 2021.
* [114] Quoc Tran Dinh. Non-ergodic alternating proximal augmented Lagrangian algorithms with optimal rates. _Proceedings of the Advances in Neural Information Processing Systems_, 31, 2018.
* [115] Quoc Tran-Dinh and Yuzixuan Zhu. Augmented Lagrangian-based decomposition methods with non-ergodic optimal rates. _arXiv preprint arXiv:1806.05280_, 2018.
* [116] Shoham Sabach and Marc Teboulle. Faster Lagrangian-based methods in convex optimization. _SIAM Journal on Optimization_, 32(1):204-227, 2022.
* [117] Tao Zhang, Yong Xia, and Shiru Li. Lagrangian-based methods in convex optimization: prediction-correction frameworks with non-ergodic convergence rates. _arXiv preprint arXiv:2304.02459_, 2023.
* [118] Andrew Cotter, Heinrich Jiang, and Karthik Sridharan. Two-player games for efficient non-convex constrained optimization. In _Proceedings of the Algorithmic Learning Theory_, pages 300-332, 2019.
* [119] Richard Bellman and Stuart Dreyfus. Functional approximations and dynamic programming. _Mathematics of Computation_, 13(68):247-251, 1959.
* [120] Martin L Puterman. _Markov decision processes: discrete stochastic dynamic programming_. John Wiley & Sons, 2014.
* [121] Wenhao Zhan, Baihe Huang, Audrey Huang, Nan Jiang, and Jason Lee. Offline reinforcement learning with realizability and single-policy concentrability. In _Proceedings of the Conference on Learning Theory_, pages 2730-2775, 2022.
* [122] John Nash Jr. Non-cooperative games. _Annals of Mathematics_, 54(2):286-295, 1951.
* [123] Vivek S Borkar. A convex analytic approach to Markov decision processes. _Probability Theory and Related Fields_, 78(4):583-602, 1988.
* [124] Junyu Zhang, Amrit Singh Bedi, Mengdi Wang, and Alec Koppel. Cautious reinforcement learning via distributional risk in the dual domain. _IEEE Journal on Selected Areas in Information Theory_, 2(2):611-626, 2021.
* [125] Qinbo Bai, Amrit Singh Bedi, Mridul Agarwal, Alec Koppel, and Vaneet Aggarwal. Achieving zero constraint violation for concave utility constrained reinforcement learning via primal-dual approach. 2022.
* [126] Donghao Ying, Mengzi Amy Guo, Yuhao Ding, Javad Lavaei, and Zuo-Jun Shen. Policy-based primal-dual methods for convex constrained Markov decision processes. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 10963-10971, 2023.
* [127] Maurice Sion. On general minimax theorems. _Pacific Journal of Mathematics_, 8(1):171-176, 1958.

* [128] Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In _Proceedings of the International Conference on Machine Learning_, pages 928-936, 2003.
* [129] Yasushi Terazono and Ayumu Matani. Continuity of optimal solution functions and their conditions on objective functions. _SIAM Journal on Optimization_, 25(4):2050-2060, 2015.
* [130] Simon Lacoste-Julien, Mark Schmidt, and Francis Bach. A simpler approach to obtaining an O\((1/t)\) convergence rate for the projected stochastic subgradient method. _arXiv preprint arXiv:1212.2002_, 2012.
* [131] Yuhao Ding, Junzi Zhang, and Javad Lavaei. Beyond exact gradients: Convergence of stochastic soft-max policy gradient methods with entropy regularization. _arXiv preprint arXiv:2110.10117_, 2021.
* [132] Haipeng Luo. Introduction to online optimization/learning: Lecture 1. https://haipeng-luo.net/courses/CSCI659/2022_fall/lectures/lecture1.pdf, 2022. Access on January 21, 2023.

**Supplementary Materials for**

**"Last-iterate Convergent Policy Gradient Primal-Dual Methods**

**for Constrained MDPs"**

## Appendix A More Comparisons and Additional Related Works

In this section, we discuss more comparison details and other related works.

**Last-iterate and value-average (or policy-mixture) performance in constrained MDPs.** There has been a flurry of research activities in studying convergence behaviors of direct policy search or policy gradient-based algorithms for constrained MDPs in the infinite-horizon discounted setting. There are two main streams: (i) Lagrangian-based policy search and (ii) Approximate constrained policy search.

1. **Lagrangian-based policy search.** In the Lagrangian-based framework, last-iterate performance has been established as asymptotic convergence for several policy-based primal-dual algorithms, e.g., naive policy gradient-based primal-dual method [21] and actor-critic variants of policy gradient primal-dual methods [6, 22, 16, 9]. These studies rely on modeling primal-dual updates in two separate time scales and/or two nested loops as continuous-time gradient flow dynamics, and their asymptotic convergence is often restricted to some stationary points. We notice that recent global asymptotic convergence results [36, 40] are in terms of occupancy measure iterates instead of instantaneous policy iterates, as highlighted in Table 1. In contrast, our first method: OPD strengthens the asymptotic last-iterate convergence to non-asymptotic and last-iterate convergence with a linear rate. To provide efficiency and optimality performances, it is crucial to develop algorithms with global non-asymptotic convergence guarantees. Several recent policy-based primal-dual algorithms have been proved to converge with non-asymptotic convergence rates, e.g., policy gradient primal-dual method [69], natural policy gradient-based or policy mirror-descent style primal-dual methods [23, 27, 54], accelerated natural policy gradient-based primal-dual methods [24, 25, 28], actor-critic version of natural policy gradient-based primal-dual

\begin{table}
\begin{tabular}{c|c|c|c} \hline
**Iterate Type** & **Method** & **Single-time-scale** & **Complexity** \\ \hline \hline \multirow{4}{*}{
\begin{tabular}{c} Occupancy \\ measure \\ \end{tabular} } & Saddle flow dynamics [36] & Yes & asymptotic \\ \cline{2-4}  & ReLOAD [40] & Yes & asymptotic \\ \cline{2-4}  & OPD (18) & Yes & \(\Omega\left(\log^{2}\frac{1}{\epsilon}\right)\) \\ \hline \hline \multirow{4}{*}{Policy} & Dual descent [34] & No & \(\Omega\left(\log^{2}\frac{1}{\epsilon}\right)\) \\ \cline{2-4}  & Cutting-plane [35] & No & \(\Omega\left(\log^{3}\frac{1}{\epsilon}\right)\) \\ \cline{2-4}  & Policy-based ReLOAD [40] & Yes & — \\ \cline{2-4}  & RPG-PD (6) & Yes & \(\Omega\left(\frac{1}{\epsilon^{6}}\log^{2}\frac{1}{\epsilon}\right)\) \\ \cline{2-4}  & OPG-PD (9) & Yes & \(\Omega\left(\log^{2}\frac{1}{\epsilon}\right)\) \\ \hline \end{tabular}
\end{table}
Table 1: Iteration complexities of our methods and representative algorithms for solving a constrained MDP problem: \(\operatorname*{maximize}_{\pi}V_{r}^{\pi}(\rho)\,\operatorname*{subject}\, \operatorname*{to}\,V_{g}^{\pi}(\rho)\,\geq 0\) with reward/utility functions \(r\in[0,1]\), \(g\in[-1,1]\). The iteration complexity is the number of gradient-based updates for an algorithm to output the last policy-iterate \(\pi_{t}\) that satisfies \(V_{r}^{\pi}(\rho)-V_{r}^{\pi_{t}}(\rho)\leq\epsilon\) and \(-V_{g}^{\pi_{t}}(\rho)\leq\epsilon\).

method [26], and anchor-changing natural policy gradient-based primal-dual method [70]. These studies characterize non-asymptotic convergence to an optimal constrained policy regarding the average of value functions, except for [27] in which the convergence is for a mixture of past policies and [28] in which the convergence is for a policy induced by a history-weighted occupancy measure. Similar non-asymptotic convergence can also be found in an occupancy measure-based primal-dual method [42], where the convergence is in terms of the average of occupancy measures. Besides, sublinear non-asymptotic convergence can be found in generative model-based methods [71, 41], regarding a mixture of past policies. In contrast, our two policy-based methods: RPG-PD and OPG-PD strengthen the sublinear non-asymptotic convergence of average value functions or a mixture of past policies to sublinear and linear non-asymptotic convergence of policy iterates. In particular, we exploit the regularization technique [52] and the optimistic gradient method [72, 37] to augment typical policy-based primal-dual methods with novel identification of decreasing distances of policy iterates to an optimal constrained policy, which allows for stronger convergence.

Instead of working with policy primal and dual variables both, Lagrangian-based dual method formulates a constrained MDP as a convex dual problem that enables classical dual ascent method [12]. Despite guaranteed convergence in dual space from convex optimization, it is challenging to compute an optimal constrained policy in primal policy space even if we use an optimal dual variable [9, 47, 30]. Recently, regularization [52] has been used in dual ascent methods [34, 35] in which policy last-iterates of natural policy gradient-based subroutines can be nearly-optimal. These dual-based algorithms [34, 35] work with two nested loops and their non-asymptotic last-iterate convergence relies on tuning loop parameters optimally. In contrast, our two policy-based methods: RPG-PD and OPG-PD remove the double loop requirement as listed in Table 1, which permits outputting the last-iterate policy as a nearly-optimal constrained policy. In Table 1, we see that our RPG-PD method has a worse rate while OPG-PD achieves a linear rate which is similar as the dual-based methods [34, 35]. Importantly, our non-asymptotic convergence characterizes the stability of primal-dual iterates generated by the algorithms, which is theoretically stronger and more appealing in practice.
2. **Approximate constrained policy search**. Approximation of constrained MDPs with surrogate functions has been shown to be effective, e.g., constrained policy optimization [8], successive convex relaxation [73], projection-based constrained policy optimization [74], first-order constrained optimization [75], and conservative policy update [76]. These studies have shown impressive empirical performance by iteratively solving an approximated constrained optimization problem and such performance is characterized in the worst-case policy improvement, except for [73] in which local asymptotic convergence is established, which leaves non-asymptotic convergence of policy iterates open. A related approach is the primal method [77] that treats a constrained MDP as an unconstrained one and corrects policy iterates whenever constraint violation happens. Non-asymptotic convergence of primal method has been established in terms of mixture of past policies. Another approximation method is the interior-point policy optimization [78] that solves an unconstrained MDP by adding a logarithm barrier function into the objective function, while convergence of this method is unknown. In contrast, we have supported our methods with non-asymptotic last-iterate convergence. Since Lagrangian-based methods are typically used to solve an approximated constrained optimization problem [8, 74, 75, 76], our methodologies can be applied to these methods for better convergence, which we leave as future work.

For constrained MDPs in the finite-horizon episodic setting and the total or average-reward settings, there is a rich line of works that have developed learning algorithms with non-asymptotic convergence guarantees in terms of the average of value functions [79, 80, 81, 82, 83, 84, 85, 55, 86, 87, 88, 89, 56, 90, 91, 92, 93, 94, 95], except for [96] in which global asymptotic convergence of a policy gradient method has been established in the finite-horizon constrained MDP setting. Although being not directly comparable, their non-asymptotic and last-iterate convergence are not established yet, to the best of our knowledge.

**Lagrangian-based policy gradient methods in constrained MDPs**. Our methods are closely pertinent to Lagrangian-based policy gradient methods for solving constrained MDPs in the infinite-horizon discounted setting, e.g., policy gradient-based primal-dual methods [21, 6, 22, 16, 9, 69],natural policy gradient-based or mirror-descent style primal-dual methods [23, 27, 28, 24, 54, 25, 26], dual descent methods [97, 34, 35, 12]. Regarding algorithm implementation, primal-dual methods [21, 6, 22, 16, 9, 23, 27, 28, 24, 54, 25, 26, 69] work with primal-dual iterates simultaneously in a single loop, which is similar to the classical gradient-based primal-dual methods in constrained optimization [31, 32, 33], while diminishing stepsizes in different speeds is often required in many of them [21, 6, 16, 9]; dual descent methods [97, 34, 35, 12] intermittently operate the dual iterate only after a sufficient number of primal iterations, which often adds difficulty of tuning hyper-parameters of nested loops in practice. It is worth mentioning that, it is convenient to view such dual descent methods as primal-dual methods that update primal variable faster than iterating dual variable. With respect to convergence analysis, stochastic approximation has been widely used to establish asymptotic convergence of several primal-dual methods [6, 22, 16, 9] by analyzing the stability of limiting gradient flow dynamics, while recent methods [28, 24, 54, 25, 26, 69, 34, 35] exploit the connection between policy gradient and mirror-descent in convex optimization to prove non-asymptotic convergence. However, non-asymptotic convergence of primal-dual methods only characterizes the average of value functions [23, 54, 25, 26, 69] or a mixture of past policies [27, 28, 24] because of the dual update that results from regulating constraint violation. For the dual descent methods [34, 35], non-asymptotic convergence is characterized in terms of last-iterate policies that are computed by approximately solving unconstrained RL problems with fixed dual variables. Therefore, designing Lagrangian-based policy gradient methods that enjoy the single-loop simplicity and the non-asymptotic convergence of policy iterates is challenging, because of the oscillation and overshoot issues of updating primal-dual variables simultaneously [29, 40]. In this work, we have established non-asymptotic and last-iterate convergence of two single-loop Lagrangian-based policy gradient methods via the regularization and optimistic gradient techniques and our analysis builds on the mirror-descent analysis for policy gradient methods [49, 98, 65, 99] while focusing on the distance of policy iterates, which is stronger than the prior art. Compared with recent efforts [40, 36] as shown in Table 1, our algorithms are simpler and our theoretical guarantees are stronger.

**Gradient-based methods with last-iterate convergence for learning in games**. Since the Lagrangian-based approach for constrained MDPs can be viewed as solving a constrained saddle-point problem, another line of related work is gradient-based methods for solving saddle-point (or minimax optimization) problems with last-iterate convergence. Last-iterate convergence of gradient-based methods has been established in several scenarios, e.g., linear rates of extragradient methods for strongly convex problems [100, 101], asymptotic convergence of optimistic multiplicative weights updates for convex problems [67, 68], linear rates of optimistic gradient methods for convex problems [43], and lower bound-matching rates of extragradient and optimistic gradient methods for convex problems [39]. These studies focus on convex-concave saddle-point problems except for [102, 103, 104] in which non-asymptotic last-iterate convergence is achievable for saddle-point problems with special non-convexity structure. In contrast, our constrained saddle-point problem that result from constrained MDP is non-convex in policy primal variable, which prevents direct application of these last-iterate results. A slightly twisted exception is that our constrained saddle-point problem can be reformulated to be convex in occupancy measure instead of policy, which leads the second and third methods in Table 1. To solve our Lagrangian-based constrained saddle-point problem in policy space, our first policy-based method: RPG-PD relaxes the non-convexity by adding regularization into the objective function and we provide sublinear last-iterate policy convergence guarantee using the distance analysis for policy primal-dual iterates. To improve the convergence rate, we further develop another policy-based method: OPG-PD that extends the optimistic gradient method [37] for a class of non-convex constrained saddle-point problems. This extension departs from previous extensions for zero-sum Markov games [43, 44], because lacking of uniformly optimal policies prevents learning an optimal policy from per-state bilinear games. Instead, we provide a new distance analysis of policy primal-dual iterates of an optimistic gradient method for solving a new class of constrained non-convex saddle-point problems, with linear last-iterate policy convergence.

**Non-asymptotic last-iterate (or non-ergodic) convergence in constrained optimization**. Reduction of constrained optimization to saddle-point problems is a classical idea to solve constrained optimization problems by developing primal-dual algorithms, e.g., primal-dual interior-point methods [105], Uzawa and Arrow-Hurwicz algorithms [31, 106, 107], and Lagrange multiplier methods [108, 45]. Inspired by the Lagrange multiplier methods, many recent studies on constrained optimization have significantly advanced primal-dual algorithms with last-iterate convergence, e.g., accelerated augmented Lagrangian method [109], accelerated universal primal dual gradient method [110], Douglas-Rachford alternating direction method [111], inexact augmented Lagrangian method [112; 113], alternating proximal augmented Lagrangian algorithm [114], augmented Lagrangian-based decomposition method [115], faster Lagrangian method [116], and prediction-correction-based primal-dual method [117]. However, all these studies build on augmented Lagrangian methods to solve the classical convex optimization problems with linear constraints. In comparison, we have studied a class of _non-convex_ constrained optimization problems that result from constrained MDPs using the standard Lagrange multiplier method. We notice that a Lagrangian-based two-player game has been used to study general non-convex constrained optimization problems with average performance analysis [118]. Our two policy-based primal-dual methods with sublinear and linear last-iterate convergence appear to be the first global non-asymptotic and last-iterate convergence result in non-convex constrained optimization.

## Appendix B Proofs in Section 2

In this section, we make some helpful observations and provide proofs of the claims in Section 2.

### Lack of uniformly optimal stationary policies in constrained MDPs

In unconstrained MDPs, there always exists an optimal policy that is optimal simultaneously for all states (e.g., see [119] and [120, Chapter 6]). In contrast, this is not true anymore for constrained MDPs. To see this, we adopt a counter-example from [46] and investigate it in the constrained MDP setting.

We introduce a constrained MDP with two states: Left (\(L\)) and Right (\(R\)), in Figure 3. In each state, there are two actions \(\{L,R\}\). The MDP transition dynamics is deterministic. In state \(L\), if the agent chooses action \(L\), then the next state is \(L\) and the reward/utility \((0,0)\) is received; otherwise, action \(R\) leads to next state \(R\) and reward/utility \((1,-1)\). In state \(R\), no matter which action the agent takes, the next state is \(R\) and the reward/utility \((1,1)\) is received.

Since the state \(R\) is trivial, a stationary policy \(\pi\) can be represented by the probability of taking action \(L\) in state \(L\) denoted by \(p\). With a slight abuse of notation, we use notion \(\rho\) to represent the probability of starting off from state \(L\). Thus, we can compute the value functions as follows via the Bellman equations, i.e., \(V(s)=\sum_{a}\pi(a\,|\,s)(r(s,a)+\gamma\sum_{s^{\prime}}P(s^{\prime}\,|\,s,a)V (s))\) for all \(s\).

\[V_{r}(L) = \underbrace{p\times(0+\gamma(V_{r}(L)\times 1\,+\,V_{r}(R) \times 0))}_{\text{take action }L}+\underbrace{(1-p)\left(1+\gamma(V_{r}(L)\times 0 +V_{r}(R)\times 1)\right)}_{\text{take action }R}\] \[V_{r}(R) = \frac{1}{1-\gamma}\]

and

\[V_{g}(L) = \underbrace{p\times(0+\gamma(V_{g}(L)\times 1+V_{g}(R)\times 0))} _{\text{take action }L}+\underbrace{(1-p)\left(-1+\gamma(V_{g}(L)\times 0 +V_{g}(R)\times 1)\right)}_{\text{take action }R}\] \[V_{g}(R) = \frac{1}{1-\gamma}\]

or, equivalently,

\[V_{r}(L) = \frac{1-p}{1-\gamma p}\times\frac{1}{1-\gamma},\quad V_{r}(R) = \frac{1}{1-\gamma}\]

Figure 3: An example of a constrained MDP that has the objective function \(V_{r}^{\pi}(\rho)\) and the constraint set \(\{\pi\in\Pi\,|\,V_{g}^{\pi}(\rho)\geq 0\}\). The pair \((a,r,g)\) associated with a directed arrow represents \((\text{reward, utility})\) received when an action \(a\) at a certain state is taken.

\[\text{and}\quad V_{g}(L) = \frac{1-p}{1-\gamma p}\times\frac{2\gamma-1}{1-\gamma},\quad V_{g}(R) = \frac{1}{1-\gamma}.\]

1. It is easy to check a basic case: \(\gamma=0\)[46]. We can compute the value functions as follows, \[V_{r}^{p}(\rho) = (1-p)\rho\,+\,(1-\rho)\] \[V_{g}^{p}(\rho) = -(1-p)\rho\,+\,(1-\rho).\] Feasibility of the policy \(p\) requires that \(V_{g}^{p}(\rho)\geq 0\), i.e., \[p\;\geq\;\frac{2\rho-1}{\rho}\;\text{ for any }\rho\in\left[\frac{1}{2},1\right].\] (10) Hence, the maximum \(V_{r}^{p}(\rho)\) within the feasible region can be reached at the optimal policy, \[p^{\star} = \frac{2\rho-1}{\rho}.\] Therefore, the optimal policy \(p^{\star}\) depends on the initial state distribution \(\rho\). Moreover, except that \(\rho=1\) or \(\frac{1}{2}\), the optimal policy \(p^{\star}\) is a stochastic policy and is unique.
2. A slightly more general case is given by \(\gamma=\frac{1}{4}\). Thus, we can compute the value functions as follows, \[V_{r}^{p}(\rho) = \frac{4}{3}\times\frac{1-p}{1-p/4}\times\rho\,+\,\frac{4}{3} \times(1-\rho)\] \[V_{g}^{p}(\rho) = -\frac{2}{3}\times\frac{1-p}{1-p/4}\times\rho\,+\,\frac{4}{3} \times(1-\rho).\] Feasibility of the policy \(p\) requires that \(V_{g}^{p}(\rho)\geq 0\), i.e., \[p \geq 2\times\frac{3\rho-2}{3\rho-1}\;\text{ for any }\rho\in\left(\frac{1}{3},1\right].\] In particular, if we take \(\rho=\frac{7}{9}\), then \(p\geq\frac{1}{2}\). In this case, the maximization of \(V_{r}^{p}(\rho)\) yields an optimal policy \(p^{\star}=\frac{1}{2}\), which is a uniform policy and is unique.

### Scalarization fallacy in constrained MDPs

In constrained RL, scalarization is often used to reduce a constrained MDP problem to a standard unconstrained one, which might permit many unconstrained RL algorithms [8, 9]. Unfortunately, as pointed out in the literature (e.g., [46, Part 4] and [30]), such a reduction does not necessarily provide a solution to the original constrained MDP problem. It is easy to see this from the previous examples in Figure 3. In the basic case: \(\gamma=0\), if we take \(\rho=\frac{3}{4}\), then from (10) the optimal policy is given by \(p^{\star}=\frac{2}{3}\), which is a stochastic policy; we see a uniform optimal policy when \(\gamma=\frac{1}{4}\). By shaping a composite function \(r+\lambda g\) with some fixed \(\lambda\in[0,\infty]\), the scalarization method aims to solve the following unconstrained MDP problem,

\[\mathop{\mathrm{maximize}}_{\pi\,\in\,\Pi}\;\;V_{r+\lambda g}^{\pi}(\rho).\] (11)

However, by the optimality of dynamic programming [120, Chapter 6], an optimal policy is given in a deterministic form which has been widely used in theory and practice. Therefore, solving the above scalarized version of a constrained MDP problem does not necessarily provide an optimal solution for the original constrained MDP problem. We also notice that this phenomenon is reported in recent empirical studies [9, 30] and a more formal statement [47, Lemma 1]. Hence, it can be infeasible for dual descent methods [12, 30] to find an optimal constrained policy, because solving Problem (11) often yields a deterministic policy, which can be sub-optimal for a constrained MDP with a unique stochastic optimal policy, e.g., constrained MDP examples in Appendix B.1.

### Properties of saddle points

First, we state the invariance property of saddle points [121] for our constrained saddle-point problem (3). By the invariance property of saddle points, we can restrict the problem domain without changing the saddle-point property when the original saddle points are contained in the restricted domain. Let the set of max-min points be \(\Pi^{\star}:=\operatorname*{argmax}_{\pi\,\in\,\Pi}\min_{\lambda\,\in\,[0,\infty ]}V^{\pi}_{r+\lambda g}(\rho)\) and the set of min-max points be \(\Lambda^{\star}:=\operatorname*{argmin}_{\lambda\,\in\,[0,\infty]}\max_{\pi \,\in\,\Pi}V^{\pi}_{r+\lambda g}(\rho)\).

**Lemma 8** (Invariance of saddle points).: _Let \((\pi^{\star},\lambda^{\star})\in\Pi^{\star}\times\Lambda^{\star}\) be a saddle point of \(V^{\pi}_{r+\lambda g}(\rho)\) over \(\Pi\times[0,+\infty]\). For any subset \(\Lambda^{\prime}\subset[0,+\infty]\), if \((\pi^{\star},\lambda^{\star})\in\Pi\times\Lambda^{\prime}\), then \((\pi^{\star},\lambda^{\star})\) is a saddle point of \(V^{\pi}_{r+\lambda g}(\rho)\) over \(\Pi\times\Lambda^{\prime}\)._

Proof.: From the saddle-point property of \((\pi^{\star},\lambda^{\star})\), we have

\[\pi^{\star}\ \in\ \operatorname*{argmax}_{\pi\,\in\,\Pi}V^{\pi}_{r+\lambda^{ \star}g}(\rho)\ \ \text{and}\ \ \lambda^{\star}\ \in\ \operatorname*{argmin}_{\lambda\,\in\,[0,+\infty]}V^{\pi^{\star}}_{r+\lambda g }(\rho)\]

It is straightforward to see that

\[V^{\pi}_{r+\lambda^{\star}g}(\rho)\ \leq\ V^{\pi^{\star}}_{r+\lambda^{\star}g}( \rho)\ \ \text{for any}\ \pi\in\Pi.\] (12)

Since \(\Lambda^{\prime}\subset[0,+\infty]\) and \(\lambda^{\star}\in\Lambda^{\prime}\), \(V^{\pi^{\star}}_{r+\lambda^{\star}g}(\rho)=\min_{\lambda\,\in\,\Lambda^{\prime }}V^{\pi^{\star}}_{r+\lambda g}(\rho)\). Hence,

\[V^{\pi^{\star}}_{r+\lambda^{\star}g}(\rho)\ \leq\ V^{\pi^{\star}}_{r+\lambda g}( \rho)\ \ \text{for any}\ \lambda\in\Lambda^{\prime}.\] (13)

Finally, combining (12) and (13) defines \((\pi^{\star},\lambda^{\star})\) as a saddle point of of \(V^{\pi}_{r+\lambda g}(\rho)\) over \(\Pi\times\Lambda^{\prime}\). 

We next show the interchangeability of saddle points in two-player zero-sum games [122] for our _non-convex_ game.

**Lemma 9** (Interchangeability of saddle points).: _Let \((\pi^{\star},\lambda^{\star})\), \((\bar{\pi}^{\star},\bar{\lambda}^{\star})\in\Pi^{\star}\times\Lambda^{\star}\) be two saddle points of \(V^{\pi}_{r+\lambda g}(\rho)\) over \(\Pi\times[0,+\infty]\). Then, both \((\pi^{\star},\bar{\lambda}^{\star})\) and \((\bar{\pi}^{\star},\lambda^{\star})\) are saddle points of \(V^{\pi}_{r+\lambda g}(\rho)\) over \(\Pi\times[0,+\infty]\)._

Proof.: By the definition of saddle points \((\pi^{\star},\lambda^{\star})\) and \((\bar{\pi}^{\star},\bar{\lambda}^{\star})\),

\[V^{\pi}_{r+\lambda^{\star}g}(\rho)\ \leq\ V^{\pi^{\star}}_{r+\lambda^{\star}g}(\rho) \ \leq\ V^{\pi^{\star}}_{r+\lambda g}(\rho)\ \ \text{for all}\ \pi\in\Pi\ \text{and}\ \lambda\in[0,\infty]\] \[V^{\pi}_{r+\bar{\lambda}^{\star}g}(\rho)\ \leq\ V^{\bar{\pi}^{\star}}_{r+\bar{ \lambda}^{\star}g}(\rho)\ \leq\ V^{\bar{\pi}^{\star}}_{r+\lambda g}(\rho)\ \ \text{for all}\ \pi\in\Pi\ \text{and}\ \lambda\in[0,\infty].\]

Then,

\[V^{\bar{\pi}^{\star}}_{r+\bar{\lambda}^{\star}g}(\rho)\ \leq\ V^{\bar{\pi}^{\star}}_{r+\lambda^{\star}g}(\rho) \ \leq\ V^{\pi^{\star}}_{r+\lambda^{\star}g}(\rho)\] \[V^{\pi^{\star}}_{r+\lambda^{\star}g}(\rho)\ \leq\ V^{\pi^{\star}}_{r+\bar{ \lambda}^{\star}g}(\rho)\ \leq\ V^{\bar{\pi}^{\star}}_{r+\bar{\lambda}^{\star}g}(\rho).\]

Therefore,

\[V^{\pi}_{r+\bar{\lambda}^{\star}g}(\rho)\leq V^{\bar{\pi}^{\star}}_{r+\bar{ \lambda}^{\star}g}(\rho)\leq V^{\pi^{\star}}_{r+\lambda^{\star}g}(\rho)\leq V^{ \pi^{\star}}_{r+\bar{\lambda}^{\star}g}(\rho)\leq V^{\bar{\pi}^{\star}}_{r+ \bar{\lambda}^{\star}g}(\rho)\leq V^{\pi^{\star}}_{r+\lambda^{\star}g}(\rho) \leq V^{\pi^{\star}}_{r+\lambda^{\star}g}(\rho)\]

for all \(\pi\in\Pi\) and \(\lambda\in[0,\infty]\), which shows that \((\pi^{\star},\bar{\lambda}^{\star})\) is a saddle point of \(V^{\pi}_{r+\lambda g}(\rho)\) over \(\Pi\times[0,+\infty]\).

Similarly, we can prove it for \((\bar{\pi}^{\star},\lambda^{\star})\). 

### Constrained MDPs in occupancy-measure space

In the analytic approach [123, 5], the value functions in Problem (1) are bilinear in the occupancy measure \(V^{\pi}_{\circ}(\rho)=\langle\circ,q\rangle\) for \(\diamond=r\) or \(u\), where \(q^{\pi}\): \(S\times A\to\mathbb{R}\) is an (un-normalized) occupancy measure over the state-action space,

\[q^{\pi}(s,a)\ =\ \sum_{t\,=\,0}^{\infty}\gamma^{t}\text{Pr}(s_{t}=s,a_{t}=a\,|\,\pi,s_{0} \sim\rho)\] (14)which adds up discounted probabilities of visiting \((s,a)\) in the execution of \(\pi\). Furthermore, we let the operator \(P^{\top}\): \(\mathbb{R}^{|S||A|}\to\mathbb{R}^{|S|}\) be \((P^{\top}q)(s):=\sum_{s^{\prime},a^{\prime}}P(s\,|\,s^{\prime},a^{\prime})q(s^{ \prime},a^{\prime})\), and the operator \(E^{\top}\): \(\mathbb{R}^{|S||A|}\to\mathbb{R}^{|S|}\) be \((E^{\top}q)(s):=\sum_{a}q(s,a)\) or simply \(q(s)\). With a slight abuse of notation, we denote \(\rho=[\,\rho(s_{1}),\ldots,\rho(s_{|S|})\,]^{\top}\). A valid occupancy measure \(q^{\pi}\in\mathbb{R}^{|S||A|}\) satisfies the the Bellman flow equations,

\[\mathcal{Q}\ :=\ \big{\{}\,q\,\in\,\mathbb{R}^{|S||A|}\,|\,E^{\top}q=\gamma P^{ \top}q+\rho\,\,\text{and}\,\,q\geq 0\,\big{\}}.\] (15)

It is known that the Bellman flow constraint is necessary and sufficient for any \(q\in\mathbb{R}^{|S||A|}\) to be a valid occupancy measure (e.g., [124, Lemma 1]).

Let the concatenation of \(r(s,a)\), \(u(s,a)\) for all \((s,a)\) be \(r\), \(u\in\mathbb{R}^{|S||A|}\), respectively. In the occupancy measure space, the goal of a constrained MDP is to find a solution \(q^{\star}\) of a linear program,

\[\operatorname*{maximize}_{q\,\in\,\mathcal{Q}}\ \langle r,q\rangle\qquad \operatorname{subject\,\,to}\ \ \langle u,q\rangle\ \geq\ b.\] (16)

Denoting \(g\): \(S\times A\to[-1,1]\) as \(g:=u-(1-\gamma)b\), we simplify the constraint \(\langle u,q\rangle\geq b\) as \(\langle g,q\rangle\geq 0\). By the method of Lagrange multipliers, we dualize two constraints in (16) and introduce a standard Lagrangian,

\[L(q,\lambda,\mu) := \langle\,r+\lambda g,q\,\rangle\,+\,\mu^{\top}\big{(}\,\rho-(E- \gamma P)^{\top}q\,\big{)}\]

where \(\pi\in\Pi\) is the primal variable, \(\lambda\in[0,\infty]\) is the dual variable for the constraint \(\langle g,q\rangle\geq 0\), and \(\mu\) is the dual variable for the equality constraint in \(\mathcal{Q}\). Since the strong duality holds for any feasible linear program, the boundedness of \(\lambda^{\star}\) in Lemma 1 holds for \(L(q,\lambda,\mu)\). Thus, any saddle point \((q^{\star},\lambda^{\star},\mu^{\star})\) is also a max-min and min-max point, i.e., \(q^{\star}\) is the occupancy measure associated with the optimal policy \(\pi^{\star}\), and \((\lambda^{\star},\mu^{\star})\in\operatorname*{argmin}_{\lambda\,\geq\,0,\mu} \max_{q\,\geq\,0}L(q,\lambda,\mu)\). Boundedness of \(q^{\star}\) is straightforward from (14),

\[q^{\star}\ \in\ Q\ :=\ \left\{q\in\mathbb{R}^{|S||A|}\,\big{|}\,0\leq q(s,a) \leq\frac{1}{1-\gamma},\forall(s,a)\in S\times A\right\}\]

which allows us further restrict \(q\in Q\subset\mathcal{Q}\). We next show boundedness of \((\lambda^{\star},\mu^{\star})\) in Lemma 10.

**Lemma 10** (Boundedness).: _Let Assumption 1 hold. Then, \(\lambda^{\star}\in\Lambda\) and \(\mu^{\star}\in M\), where \(\Lambda\) is stated below (3) and \(M:=\{\mu\,|\,|\mu(s)|\leq\mu_{\max},\forall s\in S\}\) where \(\mu_{\max}:=\frac{1-\gamma+1/\xi}{(1-\gamma)^{2}}\)._

Proof.: From the saddle-point property of \((q^{\star},\lambda^{\star},\mu^{\star})\), we have

\[q^{\star} \in \operatorname*{argmax}_{q\,\in\,Q}\ L(q,\lambda^{\star},\mu^{ \star})\] \[(\lambda^{\star},\mu^{\star}) \in \operatorname*{argmin}_{\lambda\,\geq\,0,\mu}\ L(q^{\star},\lambda,\mu)\]

equivalently, for any \(q\in Q\), \(\partial_{q}L(q^{\star},\lambda^{\star},\mu^{\star})^{\top}(q-q^{\star})\leq 0\), and for any \(\mu\) and \(\lambda\geq 0\), \(\partial_{\mu}L(q^{\star},\lambda^{\star},\mu^{\star})^{\top}(\mu-\mu^{\star})+ \partial_{\lambda}L(q^{\star},\lambda^{\star},\mu^{\star})(\lambda-\lambda^{ \star})\geq 0\). Hence, for any \(q\in Q\),

\[\langle r+(\gamma P-E)\mu^{\star}+\lambda^{\star}g,q-q^{\star}\rangle\ \leq\ 0.\]

Arbitrary \(q\in Q\) demands the inequality \(r+(\gamma P-E)\mu^{\star}+\lambda^{\star}g\leq 0\). By the definition of occupancy measure, we know that for any \(s\in S\) there exists an action \(a\in A\) such that \(q^{\star}(s,a)>0\). Thus, the equality \(r+(\gamma P-E)\mu^{\star}+\lambda^{\star}g=0\) must hold at such state-action pairs in which we represent the associated reward and transition by \((\bar{r},\bar{P})\). Hence,

\[\left\|\bar{r}+\lambda^{\star}g\right\|_{\infty} = \left\|(\gamma\bar{P}-\bar{E})\mu^{\star}\right\|_{\infty}\] \[\geq (1-\gamma)\left\|\mu^{\star}\right\|_{\infty}.\]

Thus, \((1-\gamma)\left\|\mu^{\star}\right\|_{\infty}\leq 1+\lambda^{\star}\). However, by Assumption 1, \(L(\bar{q},\lambda^{\star},\mu^{\star})\leq L(q^{\star},\lambda^{\star},\mu^{ \star})\). Hence,

\[\langle r,q^{\star}-\bar{q}\rangle\,+\,\lambda^{\star}\,\langle g,q^{\star}- \bar{q}\rangle\ \geq\ 0\]

which, together with the feasibility of \(\bar{q}\) and the optimality of \(q^{\star}\), imply that \(0\leq\lambda^{\star}\xi\leq\langle r,q^{\star}-\bar{q}\rangle\). Hence, \(0\leq\lambda^{\star}\leq\frac{1}{(1-\gamma)\xi}\), which further yields a bound on \(\left\|\mu^{\star}\right\|_{\infty}\).

We now obtain a constrained saddle-point problem in terms of the \(q\)-based Lagrangian,

\[\operatorname*{maximize}_{q\,\in\,Q}\,\operatorname*{minimize}_{\lambda\,\in\, \Lambda,\,\mu\,\in\,M}\,\,\,L(q,\lambda,\mu)\,\,=\,\operatorname*{minimize}_{ \lambda\,\in\,\Lambda,\,\mu\,\in\,M}\,\,\operatorname*{maximize}_{q\,\in\,Q} \,\,\,L(q,\lambda,\mu)\] (17)

where we take bounded polytopes \(Q\) and \(\Lambda\times M\) such that they contain \(q^{\star}\) and \((\lambda^{\star},\mu^{\star})\), respectively.

For notational brevity, we introduce \(z=(q,\lambda,\mu)\), \(Z:=Q\times\Lambda\times M\), and \(Z^{\star}:=Q^{\star}\times\Lambda^{\star}\times M^{\star}\) which contains all saddle points \(z^{\star}:=(q^{\star},\lambda^{\star},\mu^{\star})\). Let the gradient of \(L(q,\lambda,\mu)\) be

\[F(q,\lambda,\mu)\,\,:=\,\left[\begin{array}{c}-\nabla_{q}L(q,\lambda,\mu)\\ \\ \nabla_{(\lambda,\mu)}L(q,\lambda,\mu)\end{array}\right].\]

Due to the bilinearity of \(L(q,\lambda,\mu)\) over a compact domain, \(L(q,\lambda,\mu)\) has a gradient Lipschitz constant \(L_{f}\). Let \(\mathcal{P}_{X}\) be the projection operator onto a set \(X\), i.e., \(\mathcal{P}_{X}(x):=\operatorname*{argmin}_{x^{\prime}\,\in\,X}\|x^{\prime}-x\|\). Since the \(q\)-based Lagrangian \(L(q,\lambda,\mu)\) is bilinear and the domains are polytopes, Problem (17) satisfies the metric subregularity condition [38, Theorem 5].

**Lemma 11** (Metric subregularity condition).: _Let Assumption 1 hold. Then, the gradient function \(F(z)\) satisfies that for any \(z\in Z/Z^{\star}\) with \(z^{\star}=\mathcal{P}_{Z^{\star}}(z)\),_

\[\sup_{z^{\prime}\,\in\,Z}\frac{F(z)^{\top}(z-z^{\prime})}{\|z-z^{\prime}\|}\, \,\geq\,\,C\,\|z-z^{\star}\|\]

_where \(C>0\) is a problem-dependent constant._

Hence, application of the optimistic gradient method [38] to Problem (17) yields an Optimistic Primal-Dual (OPD) algorithm that begins with two initial tuples of primal/dual variables \((q_{0},\lambda_{0},\mu_{0})=(\hat{q}_{1},\hat{\lambda}_{1},\hat{\mu}_{1})\in Z\), and performs two gradient steps for each primal/dual variable at time \(t\geq 1\),

\[\begin{array}{rcl}z_{t}&=&\mathcal{P}_{Z}\left(\,\hat{z}_{t}\,-\,\eta F(z_{t -1})\,\right)\\ \\ \hat{z}_{t+1}&=&\mathcal{P}_{Z}\left(\,\hat{z}_{t}\,-\,\eta F(z_{t})\,\right) \end{array}\] (18)

where \(\eta\) is the stepsize. Let the squared distance of a point \(z\in Z\) to the set \(Z^{\star}\) be \(\text{dist}^{2}(z,Z^{\star}):=\left\|z-\mathcal{P}_{Z^{\star}}(z)\right\|^{2}\). It is straightforward to employ [38, Theorem 8] to claim last-iterate convergence guarantee of OPD (18) below.

**Theorem 12** (Linear convergence of OPD).: _Let Assumption 1 hold. If the stepsize \(\eta\) in OPD (18) satisfies \(\eta<\frac{1}{8L_{f}}\), then the iterates \(\{z_{t}\}_{t\,\geq\,0}\) converge to the set of saddle points \(Z^{\star}\) linearly,_

\[\text{dist}^{2}(z_{t},Z^{\star})\,\,\leq\,\,C_{1}\left(\frac{1}{1+C_{2}} \right)^{t}\]

_where \(C_{1}=64\,\text{dist}^{2}(\hat{z}_{1},Z^{\star})\) and \(C_{2}=\min(\frac{16n^{2}C^{2}}{81},\frac{1}{2})\) for a problem-dependent constant \(C>0\)._

Theorem 12 shows that the primal-dual iterates of OPD converge to the saddle point set \(Z^{\star}\) in linear rate. Compared with a contemporaneous work [40], OPD is free of projection to a occupancy measure set, and enjoys non-asymptotic and last-iterate linear convergence. If the underlying policy is recovered via \(\pi_{t}(a\,|\,s)=\frac{q_{t}(s,a)}{\sum_{x^{\prime}}q_{t}(s,a^{\prime})}\) for all \((s,a)\), these policy iterates \(\pi_{t}\) associated with the occupancy measure iterates \(q_{t}\) also converge to an optimal constrained policy \(\pi^{\star}\).

**Corollary 13** (Nearly-optimal constrained policy).: _Let Assumption 1 hold. Assume \(\rho_{\min}:=\min_{s}\rho(s)>0\) and re-define \(Q:=\{q\in\mathbb{R}^{|S||A|}\,|\,0\leq q(s,a)\leq 1/(1-\gamma),q(s)\geq\rho_{\min}/(1- \gamma),\forall(s,a)\in S\times A\}\). If we set the stepsize \(\eta\) in a similar way as in Theorem 12, then the recovered policy iterates \(\{\pi_{t}\}_{t\,\geq\,0}\) of OPD (18) satisfy_

\[V_{r}^{\pi^{\star}}(\rho)-V_{r}^{\pi_{t}}(\rho)\,\,\leq\,\,\epsilon\,\,\text{ and}\,\,\,-V_{g}^{\pi_{t}}(\rho)\,\,\leq\,\,\epsilon\,\,\,\,\text{ for any }t=\Omega\left(\,\log^{2}\frac{1}{\epsilon}\,\right)\]

_where \(\pi_{t}(a\,|\,s)=\frac{q_{t}(s,a)}{\sum_{a^{\prime}}q_{t}(s,a^{\prime})}\) for all \((s,a)\), and \(\Omega(\cdot)\) only has some problem-dependent constant._Proof.: The key to our proof is to connect the occupancy measure iterates \(q_{t}\) with associated policy iterates \(\pi_{t}\). It is straightforward that Theorem 12 continues to hold, with a further restricted the domain \(Q\). If we set the stepsize \(\eta\) in a similar way as in Theorem 12, then for any \(t=\Omega(\log\frac{1}{\epsilon})\),

\[\max\Big{\{}\left\|\mathcal{P}_{Q^{*}}(q_{t})-q_{t}\right\|^{2},\ \left\|\mathcal{P}_{\Lambda^{*}}(\lambda_{t})-\lambda_{t}\right\|^{2},\ \left\|\mathcal{P}_{M^{*}}(\mu_{t})-\mu_{t}\right\|^{2}\Big{\}} = O(\epsilon).\]

Let \(\pi_{t}^{\star}\) be a policy that is associated with the occupancy measure \(q^{\pi_{t}^{\star}}=\mathcal{P}_{Q^{*}}(q_{t})\) and \(\pi_{t}\) be a policy that is associated with the occupancy measure iterate \(q_{t}\), i.e., \(\pi_{t}(a\,|\,s)=\frac{q_{t}(s,a)}{\sum_{a^{\prime}}q_{t}(s,a^{\prime})}\) for all \((s,a)\). We denote \((\lambda_{t}^{\star},\mu_{t}^{\star}):=(\mathcal{P}_{\Lambda^{*}}(\lambda_{t}),\mathcal{P}_{M^{*}}(\mu_{t}))\). Thus,

\[\sum_{s}d_{\rho}^{\pi_{t}^{\star}}(s)\,\|\,\pi_{t}(\cdot\,|\,s)- \pi_{t}^{\star}(\cdot\,|\,s)\|\] \[= \sum_{s}d_{\rho}^{\pi_{t}^{\star}}(s)\left\|\frac{q_{t}(s,\cdot) }{q_{t}(s)}-\frac{q^{\pi_{t}^{\star}}(s,\cdot)}{q^{\pi_{t}^{\star}}(s)}\right\|\] \[\leq \sum_{s}d_{\rho}^{\pi_{t}^{\star}}(s)\frac{\left\|q_{t}(s,\cdot)- q^{\pi_{t}^{\star}}(s,\cdot)\right\|q_{t}(s)}{q_{t}(s)q^{\pi_{t}^{\star}}(s)}\,+ \,\sum_{s}d_{\rho}^{\pi_{t}^{\star}}(s)\,\frac{\left\|q_{t}(s,\cdot)\right\| \left|q_{t}(s)-q^{\pi_{t}^{\star}}(s)\right|}{q_{t}(s)q^{\pi_{t}^{\star}}(s)}\] \[\leq \sum_{s}\frac{\left\|q_{t}(s,\cdot)-q^{\pi_{t}^{\star}}(s,\cdot) \right\|}{q^{\pi_{t}^{\star}}(s)}\,+\,\sqrt{|A|}\sum_{s}\frac{\left|q_{t}(s)- q^{\pi_{t}^{\star}}(s)\right|}{q_{t}(s)}\] \[\leq \frac{\sqrt{|A|}}{\rho_{\min}}\sum_{s}\Big{(}\left\|q_{t}(s,\cdot )-q^{\pi_{t}^{\star}}(s,\cdot)\right\|+\left|q_{t}(s)-q^{\pi_{t}^{\star}}(s) \right|\Big{)}\] \[\leq \frac{\sqrt{|A|}}{\rho_{\min}}\left\|q_{t}-q^{\pi_{t}^{\star}} \right\|\] \[= \frac{2\sqrt{|S||A|}}{\rho_{\min}}\left\|q_{t}-\mathcal{P}_{ \mathcal{Q}^{*}}(q_{t})\right\|\]

where the first inequality is due to triangle inequality, we use the fact: \((1-\gamma)q^{\pi_{t}^{\star}}(s)=d_{\rho}^{\pi_{t}^{\star}}(s)\), \(d_{\rho}^{\pi_{t}^{\star}}(s)\leq 1\), and \(\left\|q_{t}(s,\cdot)\right\|\leq\frac{\sqrt{|A|}}{1-\gamma}\) in the second inequality, the third inequality is due to that \(q^{\pi_{t}^{\star}}(s)\), \(q_{t}(s)\geq\rho_{\min}\), and \(\rho_{\min}>0\), we apply Cauchy-Schwarz inequality in the fourth inequality, and finally we combine two square root terms by relaxing the first one in the last inequality.

First, we have

\[V_{r}^{\pi_{t}^{\star}}(\rho)-V_{r}^{\pi_{t}}(\rho) = \frac{1}{1-\gamma}\sum_{s,a}d_{\rho}^{\pi_{t}^{\star}}(s)\left( \pi_{t}^{\star}(a\,|\,s)-\pi_{t}(a\,|\,s)\right)Q_{r}^{\pi_{t}}(s,a)\] \[\leq \frac{1}{(1-\gamma)^{2}}\sum_{s}d_{\rho}^{\pi_{t}^{\star}}(s) \left\|\pi_{t}^{\star}(\cdot\,|\,s)-\pi_{t}(\cdot\,|\,s)\right\|_{1}\] \[\leq \frac{\sqrt{|A|}}{(1-\gamma)^{2}}\sum_{s}d_{\rho}^{\pi_{t}^{ \star}}(s)\left\|\pi_{t}^{\star}(\cdot\,|\,s)-\pi_{t}(\cdot\,|\,s)\right\|\]

where the equality is due to performance difference lemma in Lemma 28, we use Cauchy-Schwarz inequality in the first inequality, and the second inequality is due to \(\left\|x\right\|_{1}\leq\sqrt{d}\left\|x\right\|_{2}\) for \(x\in\mathbb{R}^{d}\), which shows \(V_{r}^{\pi_{t}^{\star}}(\rho)-V_{r}^{\pi_{t}}(\rho)\leq O(\sqrt{\epsilon})\). By the optimality of \(\pi_{t}^{\star}\), \(V_{r}^{\pi_{t}^{\star}}(\rho)=V_{r}^{\pi^{\star}}(\rho)\). Therefore, \(V_{r}^{\pi^{\star}}(\rho)-V_{r}^{\pi_{t}}(\rho)\leq O(\sqrt{\epsilon})\).

Second, we have

\[-V_{g}^{\pi_{t}}(\rho) = \underbrace{-V_{g}^{\pi_{t}^{\star}}(\rho)}_{\text{(i)}}+ \underbrace{V_{g}^{\pi_{t}^{\star}}(\rho)-V_{g}^{\pi_{t}}(\rho)}_{\text{(ii)}}.\]

Similar to bounding \(V_{r}^{\pi_{t}^{\star}}(\rho)-V_{r}^{\pi_{t}}(\rho)\), we can show that (ii) \(\leq O(\sqrt{\epsilon})\). By the optimality of \(\pi_{t}^{\star}\), we have \(V_{g}^{\pi_{t}^{\star}}(\rho)\geq 0\). Therefore, \(-V_{g}^{\pi_{t}}(\rho)\leq O(\sqrt{\epsilon})\).

Finally, we replace the accuracy \(\sqrt{\epsilon}\) by \(\epsilon\) and combine all big O notation to conclude the proof.

Corollary 13 states that after an almost constant number of iterations a policy induced by the last occupancy measure iterate of OPD is an \(\epsilon\)-optimal constrained policy for Problem (1). We notice that recent last-iterate convergence result for convex-concave saddle-point problems [39] is also applicable to Problem (17), which provides the optimal rate without problem-dependent constants. It is worth mentioning that direct application of such last-iterate convergence results in convex minimax optimization to constrained MDPs with general utilities [125, 126] and convex MDPs [124, 47] in occupancy-measure space is also straightforward. We omit these exercises in this paper, and focus on the design and analysis of algorithms in policy space.

## Appendix C Proofs in Section 3

In this section, we provide proofs of the claims in Section 3.

### Existence and uniqueness of regularized saddle points

**Lemma 14** (Existence and uniqueness).: _There exists a unique primal-dual pair \((\bar{\pi},\bar{\lambda})\in\Pi\times\Lambda\) such that \(L_{\tau}(\bar{\pi},\lambda)\geq L_{\tau}(\bar{\pi},\bar{\lambda})\geq L_{\tau} (\pi,\bar{\lambda})\) for any \(\pi\in\Pi\) and \(\lambda\in\Lambda\)._

Proof.: We first re-write the regularized Lagrangian \(L_{\tau}(\pi,\lambda)\) in terms of occupancy measure \(q\in\mathcal{Q}\),

\[L_{\tau}(\pi,\lambda) = \langle r+\lambda g,q\rangle\,+\,\tau\left(\mathcal{H}(\pi)+\frac {1}{2}\lambda^{2}\right)\]

\[\mathcal{H}(\pi) = -\sum_{s,a}q(s,a)\log\frac{q(s,a)}{\sum_{a^{\prime}}q(s,a^{ \prime})}\ :=\ \mathcal{H}(q).\]

We next use notation \(L_{\pi}(q,\lambda)\) to represent \(L_{\tau}(\pi,\lambda)\). We first check that \(\mathcal{H}(q)\) is a concave function,

\[\mathcal{H}(\alpha q_{1}+(1-\alpha)q_{2})\] \[= -\sum_{s,a}\left(\alpha q_{1}(s,a)+(1-\alpha)q_{2}(s,a)\right) \log\frac{\alpha q_{1}(s,a)+(1-\alpha)q_{2}(s,a)}{\alpha\sum_{a^{\prime}}q_{1} (s,a^{\prime})+(1-\alpha)\sum_{a^{\prime}}q_{2}(s,a^{\prime})}\] \[\geq -\sum_{s,a}\alpha q_{1}(s,a)\log\frac{\alpha q_{1}(s,a)}{\alpha \sum_{a^{\prime}}q_{1}(s,a^{\prime})}-\sum_{s,a}(1-\alpha)q_{2}(s,a)\log\frac{ (1-\alpha)q_{2}(s,a)}{(1-\alpha)\sum_{a^{\prime}}q_{2}(s,a^{\prime})}\] \[= \alpha\mathcal{H}(q_{1})+(1-\alpha)\mathcal{H}(q_{2})\]

for any \(q_{1}\), \(q_{2}\in\mathcal{Q}\) and \(\alpha\in[\,0,1\,]\), where the inequality is because of the log sum inequality \((\sum_{i}a_{i})\ln\frac{\sum_{i}a_{i}}{\sum_{i}b_{i}}\leq\sum_{i}a_{i}\ln \frac{a_{i}}{b_{i}}\) for non-negative \(a_{i}\) and \(b_{i}\), and the equality holds if and only if

\[\frac{q_{1}(s,a)}{\sum_{a^{\prime}}q_{1}(s,a^{\prime})} = \frac{q_{2}(s,a)}{\sum_{a^{\prime}}q_{2}(s,a^{\prime})}\quad\text {for all }s,a.\]

Therefore, \(L_{\tau}(q,\lambda)\) is concave in \(q\in\mathcal{Q}\) and strongly convex in \(\lambda\in\Lambda\). We notice that \(\mathcal{Q}\) is a polytope and \(\Lambda\) is a bounded interval. By Sion's minimax theorem [127], \(L_{\tau}(q,\lambda)\) has a saddle point \((\bar{q},\bar{\lambda})\in\mathcal{Q}\times\Lambda\). From the one-to-one correspondence between policy and occupancy measure, \(\bar{q}\) induces a policy \(\bar{\pi}\) and \((\bar{\pi},\bar{\lambda})\) serves as a saddle point of \(L_{\tau}(\pi,\lambda)\), which proves the existence of saddle points.

To show the uniqueness of \((\bar{\pi},\bar{\lambda})\) (or \((\bar{q},\bar{\lambda})\)), it is sufficient to show that \(L_{\tau}(q,\lambda)\) is strictly concave in \(q\) and strictly convex in \(\lambda\). The second argument is straightforward from the strong convexity of \(L_{\tau}(q,\lambda)\) in \(\lambda\). We next show the first argument that \(L_{\tau}(q,\lambda)\) is strictly concave in \(q\) for any \(\lambda\in\Lambda\). Assume that there are two (different) policies \(\pi_{1}\) and \(\pi_{2}\) and the associated two occupancy measures are \(q_{1}\) and \(q_{2}\). From the concavity of \(H(q)\), there is another occupancy measure 

[MISSING_PAGE_FAIL:30]

\(\tau\log\frac{e_{n}}{|A|}:=C_{r,\xi,\epsilon_{0}}\). For the term (i),

\[L_{\tau}(\pi_{\tau}^{\star},\lambda_{t})-L_{\tau}(\pi_{t},\lambda_ {t})\] \[= V_{r+\lambda_{t}g}^{\pi_{\tau}^{\star}}(\rho)\,-\,V_{r+\lambda_{t}g }^{\pi_{t}}(\rho)\] \[-\,\frac{\tau}{1-\gamma}\sum_{s,a}d_{\rho}^{\pi_{\tau}^{\star}}(s) \pi_{\tau}^{\star}(a\,|\,s)\log\pi_{\tau}^{\star}(a\,|\,s)\,+\,\frac{\tau}{1- \gamma}\sum_{s,a}d_{\rho}^{\pi_{t}}(s)\pi_{t}(a\,|\,s)\log\pi_{t}(a\,|\,s)\] \[= V_{r+\lambda_{t}g+\gamma\psi_{t}}^{\pi_{\tau}^{\star}}(\rho)\,- \,V_{r+\lambda_{t}g+\tau\psi_{t}}^{\pi_{\tau}}(\rho)\,-\,\tau V_{\psi_{t}}^{ \pi_{\tau}^{\star}}(\rho)\,+\,\tau V_{\psi_{t}}^{\pi_{t}}(\rho)\] \[-\,\frac{\tau}{1-\gamma}\sum_{s,a}d_{\rho}^{\pi_{\tau}^{\star}}(s )\pi_{\tau}^{\star}(a\,|\,s)\log\pi_{\tau}^{\star}(a\,|\,s)\,+\,\frac{\tau}{1- \gamma}\sum_{s,a}d_{\rho}^{\pi_{t}}(s)\pi_{t}(a\,|\,s)\log\pi_{t}(a\,|\,s)\] \[= \frac{1}{1-\gamma}\sum_{s,a}d_{\rho}^{\pi_{\tau}^{\star}}(s)\,( \pi_{\tau}^{\star}(a\,|\,s)-\pi_{t}(a\,|\,s))\,Q_{r+\lambda_{t}g+\tau\psi_{t}} ^{\pi_{t}}(s,a)\] \[+\,\frac{\tau}{1-\gamma}\sum_{s,a}d_{\rho}^{\pi_{\tau}^{\star}}(s )\pi_{\tau}^{\star}(a\,|\,s)\log\pi_{t}(a\,|\,s)\,-\,\frac{\tau}{1-\gamma}\sum _{s,a}d_{\rho}^{\pi_{\tau}}(s)\pi_{t}(a\,|\,s)\log\pi_{t}(a\,|\,s)\] \[-\,\frac{\tau}{1-\gamma}\sum_{s,a}d_{\rho}^{\pi_{\tau}^{\star}}(s )\pi_{\tau}^{\star}(a\,|\,s)\log\pi_{\tau}^{\star}(a\,|\,s)\,+\,\frac{\tau}{1- \gamma}\sum_{s,a}d_{\rho}^{\pi_{s}}(s)\pi_{t}(a\,|\,s)\log\pi_{t}(a\,|\,s)\] \[= \frac{1}{1-\gamma}\sum_{s,a}d_{\rho}^{\pi_{\tau}^{\star}}(s)\,( \pi_{\tau}^{\star}(a\,|\,s)-\pi_{t}(a\,|\,s))\,Q_{r+\lambda_{t}g+\tau\psi_{t}} ^{\pi_{t}}(s,a)-\frac{\tau}{1-\gamma}\sum_{s}d_{\rho}^{\pi_{\tau}^{\star}}(s) \mbox{KL}_{t}(s)\] \[\leq \sum_{s}d_{\rho}^{\pi_{\tau}^{\star}}(s)\left(\frac{\mbox{KL}_{t }(s)-\mbox{KL}_{t+1}(s)}{\eta(1-\gamma)}\right)\,+\,\eta(C_{\tau,\xi,\epsilon _{0}})^{2}\,-\,\frac{\tau}{1-\gamma}\sum_{s}d_{\rho}^{\pi_{\tau}^{\star}}(s) \mbox{KL}_{t}(s)\] \[= \sum_{s}d_{\rho}^{\pi_{\tau}^{\star}}(s)\left(\frac{(1-\eta\tau) \mbox{KL}_{t}(s)-\mbox{KL}_{t+1}(s)}{\eta(1-\gamma)}\right)\,+\,\eta(C_{\tau, \xi,\epsilon_{0}})^{2}\] \[= \frac{(1-\eta\tau)\mbox{KL}_{t}(\rho)-\mbox{KL}_{t+1}(\rho)}{\eta (1-\gamma)}\,+\,\eta(C_{\tau,\xi,\epsilon_{0}})^{2}.\]

where the first two equalities are because of the entropy regularization \({\cal H}(\pi)\), we apply the performance difference lemma in Lemma 28 and \(\psi_{t}(s,a)=-\log\pi_{t}(a\,|\,s)\) to the third equality, the first inequality is due to an application of Lemma 27 with \(x^{\star}=\pi_{\tau}^{\star}(\cdot\,|\,s)\), \(x=\pi_{t}(\cdot\,|\,s)\), \(g=-Q_{r+\lambda_{t}g+\tau\psi_{t}}^{\pi_{t}}(s,a)/(1-\gamma)\), and \(\eta\leq 1/C_{\tau,\xi,\epsilon_{0}}\).

Similarly, for the term (ii),

\[L_{\tau}(\pi_{t},\lambda_{t})-L_{\tau}(\pi_{t},\lambda_{\tau}^{ \star})\] \[= V_{r+\lambda_{t}g}^{\pi_{t}}(\rho)\,-\,V_{r+\lambda_{t}g}^{\pi_{t }}(\rho)\,+\,\frac{1}{2}\tau(\lambda_{t})^{2}\,-\,\frac{1}{2}\tau(\lambda_{\tau} ^{\star})^{2}\] \[= (\lambda_{t}-\lambda_{\tau}^{\star})V_{g}^{\pi_{t}}(\rho)\,+\, \frac{1}{2}\tau(\lambda_{t})^{2}\,-\,\frac{1}{2}\tau(\lambda_{\tau}^{\star})^{2}\] \[= (\lambda_{t}-\lambda_{\tau}^{\star})\Big{(}V_{g}^{\pi_{t}}(\rho)+ \tau\lambda_{t}\Big{)}\,-\,\frac{1}{2}\tau(\lambda_{t}-\lambda_{\tau}^{\star})^{2}\] \[\leq \frac{(\lambda_{\tau}^{\star}-\lambda_{t})^{2}-(\lambda_{\tau}^{ \star}-\lambda_{t+1})^{2}}{2\eta}\,+\,\frac{1}{2}\eta(C_{\tau,\xi}^{\prime})^{ 2}\,-\,\frac{1}{2}\tau(\lambda_{t}-\lambda_{\tau}^{\star})^{2}\] \[= \frac{(1-\eta\tau)(\lambda_{\tau}^{\star}-\lambda_{t})^{2}-(\lambda_ {\tau}^{\star}-\lambda_{t+1})^{2}}{2\eta}\,+\,\frac{1}{2}\eta(C_{\tau,\xi}^{ \prime})^{2}\]

where the inequality is due to the standard descent lemma [128] and \(V_{g}^{\pi_{t}}(\rho)+\tau\lambda_{t}\leq\frac{1}{1-\gamma}(1+\frac{\tau}{\xi}):=C_ {\tau,\xi}^{\prime}\).

Using the definition \(\Phi_{t}:=\mbox{KL}_{t}(\rho)+\frac{1}{2}(\lambda_{\tau}^{\star}-\lambda_{t})^{2}\), we combine the two inequalities above to show that,

\[\Phi_{t+1} \leq (1-\eta\tau)\Phi_{t}\,-\,\eta(L_{\tau}(\pi_{\tau}^{\star},\lambda_{t })-L_{\tau}(\pi_{t},\lambda_{\tau}^{\star}))\,+\,\eta^{2}\max\left((C_{\tau,\xi, \epsilon_{0}})^{2},(C_{\tau,\xi}^{\prime})^{2}\right)\] \[\leq (1-\eta\tau)\Phi_{t}\,+\,\eta^{2}\max\left((C_{\tau,\xi,\epsilon _{0}})^{2},(C_{\tau,\xi}^{\prime})^{2}\right).\]where the second inequality is due to Lemma 14. If we expand the inequality above recursively, then,

\[\Phi_{t+1} \leq (1-\eta\tau)\Phi_{t}\,+\,\eta^{2}\max\left((C_{\tau,\xi,\epsilon_{ 0}})^{2},(C^{\prime}_{\tau,\xi})^{2}\right)\] \[\leq (1-\eta\tau)^{2}\Phi_{t-1}\,+\,(\eta^{2}+\eta^{2}(1-\eta\tau)) \max\left((C_{\tau,\xi,\epsilon_{0}})^{2},(C^{\prime}_{\tau,\xi})^{2}\right)\] \[\leq \cdots\] \[\leq (1-\eta\tau)^{t}\Phi_{1}\,+\,\left(\eta^{2}\left(1+(1-\eta\tau)+( 1-\eta\tau)^{2}+\cdots\right)\right)\max\left((C_{\tau,\xi,\epsilon_{0}})^{2}, (C^{\prime}_{\tau,\xi})^{2}\right)\] \[\leq (1-\eta\tau)^{t}\Phi_{1}\,+\,\frac{\eta}{\tau}\max\left((C_{\tau,\xi,\epsilon_{0}})^{2},(C^{\prime}_{\tau,\xi})^{2}\right)\] \[\leq \mathrm{e}^{-\eta\tau t}\Phi_{1}\,+\,\frac{\eta}{\tau}\max\left( (C_{\tau,\xi,\epsilon_{0}})^{2},(C^{\prime}_{\tau,\xi})^{2}\right)\]

which completes the proof. 

### Proof of Corollary 3

Proof.: According to Theorem 2, if we take \(\tau=\Theta(\epsilon)\), \(\eta=\Theta(\epsilon^{2})\), and \(\epsilon_{0}=\epsilon\), then \(\Phi_{t+1}=O(\epsilon)\) for any \(t=\Omega(\frac{1}{\epsilon^{3}}\log\frac{1}{\epsilon})\), where \(\Omega(\cdot)\) hides some problem-dependent constant. We next consider a primal-dual iterate \((\pi_{t},\lambda_{t})\) for some \(t=\Omega(\frac{1}{\epsilon^{3}}\log\frac{1}{\epsilon})\). It is straightforward to check that

\[\text{KL}_{t}(\rho) = O(\epsilon)\,\,\,\text{and}\,\,\,\frac{1}{2}(\lambda_{\tau}^{ \star}-\lambda_{t})^{2}\,\,\,=\,\,\,O(\epsilon).\]

First, we have

\[V_{r}^{\pi^{\star}}(\rho)-V_{r}^{\pi_{t}}(\rho) = \underbrace{V_{r}^{\pi^{\star}}(\rho)-V_{r}^{\pi_{r}^{\star}}( \rho)}_{\text{(i)}}+\underbrace{V_{r}^{\pi_{r}^{\star}}(\rho)-V_{r}^{\pi_{t}} (\rho)}_{\text{(ii)}}.\] (20)

For the term (ii), because \(\text{KL}_{t}(\rho)=O(\epsilon)\), we have

\[\text{(ii)} = \frac{1}{1-\gamma}\sum_{s,a}d_{\rho}^{\pi_{\tau}^{\star}}(s)\,( \pi_{\tau}^{\star}(a\,|\,s)-\pi_{t}(a\,|\,s))\,Q_{r}^{\pi_{\text{i}}}(s,a)\] \[\leq \frac{1}{(1-\gamma)^{2}}\sum_{s}d_{\rho}^{\pi_{\rho}^{\star}}(s) \left\|\pi_{\tau}^{\star}(\cdot\,|\,s)-\pi_{t}(\cdot\,|\,s)\right\|_{1}\] \[\leq \frac{1}{(1-\gamma)^{2}}\sum_{s}d_{\rho}^{\pi_{\tau}^{\star}}(s) \sqrt{2\,\text{KL}_{t}(s)}\] \[\leq \frac{1}{(1-\gamma)^{2}}\sqrt{2\sum_{s}d_{\rho}^{\pi_{\tau}^{ \star}}(s)\text{KL}_{t}(s)}\] \[= \frac{1}{(1-\gamma)^{2}}\sqrt{2\,\text{KL}_{t}(\rho)}\]

where we use Cauchy-Schwarz inequality in the first and third inequalities, and the second inequality is due to Pinsker's inequality, which shows \(V_{r}^{\pi_{\tau}^{\star}}(\rho)-V_{r}^{\pi_{t}}(\rho)\leq O(\sqrt{\epsilon})\). For the term (i), if we take \(\pi=\pi^{\star}\) in (5), then,

\[V_{r}^{\pi^{\star}}(\rho)-\tau\mathcal{H}(\pi_{\tau}^{\star}) \leq V_{r}^{\pi_{\tau}^{\star}}(\rho)\,+\,\lambda_{\tau}^{\star}\left(V_{g }^{\pi_{\tau}^{\star}}(\rho)-V_{g}^{\pi^{\star}}(\rho)\right).\]

Meanwhile, if we take \(\lambda=0\) in (5), then \(\lambda_{\tau}^{\star}V_{g}^{\pi_{\tau}^{\star}}(\rho)\leq 0\). We notice that the feasibility of \(\pi^{\star}\) yields \(V_{g}^{\pi^{\star}}(\rho)\geq 0\). Hence,

\[\text{(i)} = V_{r}^{\pi^{\star}}(\rho)-V_{r}^{\pi_{\tau}^{\star}}(\rho)\,\,\,\leq \,\,\tau\mathcal{H}(\pi_{\tau}^{\star}).\]

We now substitute the upper bounds of (i) and (ii) above into (20) to obtain \(V_{r}^{\pi^{\star}}(\rho)-V_{r}^{\pi_{t}}(\rho)\leq O(\sqrt{\epsilon})\), where we take \(\tau=\Theta(\epsilon)\).

Second, we have

\[-V_{g}^{\pi_{t}}(\rho) = \underbrace{-\,V_{g}^{\pi^{\star}_{*}}(\rho)}_{\mbox{(iii)}}+ \underbrace{V_{g}^{\pi^{\star}_{*}}(\rho)-V_{g}^{\pi_{t}}(\rho)}_{\mbox{(iv)}}.\] (21)

Similar to bounding \(V_{r}^{\pi^{\star}_{*}}(\rho)-V_{r}^{\pi_{t}}(\rho)\), we can show that (iv) \(\leq O(\sqrt{\epsilon})\). Let \(\lambda_{\max}:=\frac{1}{(1-\gamma)\xi}\). For the term (iii), if we take \(\lambda=\lambda_{\max}\) in (5), then,

\[-(\lambda_{\max}-\lambda_{\tau}^{\star})V_{g}^{\pi^{\star}_{*}}(\rho) \leq \frac{\tau}{2}(\lambda_{\max})^{2}.\]

By the definition \(\lambda_{\tau}^{\star}:=\operatorname*{argmin}_{\lambda\,\in\,\Lambda}\left\{ \lambda V_{g}^{\pi^{\star}_{*}}(\rho)+\frac{\tau}{2}\lambda^{2}\right\}\), there are three values for \(\lambda_{\tau}^{\star}\) to take: \(-\frac{V_{g}^{\pi^{\star}_{*}}(\rho)}{\tau}\), or \(0\), or \(\lambda_{\max}\) as follows

1. When \(0<-\frac{V_{g}^{\pi^{\star}_{*}}(\rho)}{\tau}<\lambda_{\max}\), \(\lambda_{\tau}^{\star}=-\frac{V_{g}^{\pi^{\star}_{*}}(\rho)}{\tau}\) which shows that \(\lambda_{\max}-\lambda_{\tau}^{\star}>0\);
2. When \(-\frac{V_{g}^{\pi^{\star}_{*}}(\rho)}{\tau}\leq 0\), \(\lambda_{\tau}^{\star}=0\) which shows that \(\lambda_{\max}-\lambda_{\tau}^{\star}=\lambda_{\max}>0\);
3. When \(-\frac{V_{g}^{\pi^{\star}_{*}}(\rho)}{\tau}\geq\lambda_{\max}\), \(\lambda_{\tau}^{\star}=\lambda_{\max}\). In this case, using (5) with \(\pi=\pi^{\star}\) leads to \[\lambda_{\max}(V_{g}^{\pi^{\star}}(\rho)-V_{g}^{\pi^{\star}_{*}}(\rho))\ \leq\ V_{r}^{\pi^{\star}_{*}}(\rho)-V_{r}^{\pi^{\star}}(\rho)+\tau\mathcal{H} (\pi_{\tau}^{\star}).\] (22) Meanwhile, for any saddle point \((\pi^{\star},\lambda^{\star})\in\Pi^{\star}\times\Lambda^{\star}\), \(V_{r}^{\pi^{\star}_{*}}(\rho)-V_{r}^{\pi^{\star}}(\rho)\leq\lambda^{\star}(V_{ g}^{\pi^{\star}_{*}}(\rho)-V_{g}^{\pi^{\star}_{*}}(\rho))\), which in conjunction with (22) and \(V_{g}^{\pi^{\star}}(\rho)\geq 0\) yields, \[-(\lambda_{\max}-\lambda^{\star})V_{g}^{\pi^{\star}_{*}}(\rho) \leq \tau\mathcal{H}(\pi_{\tau}^{\star}).\] It is easy to see that we can always take \(\lambda^{\star}<\lambda_{\max}\). In fact, except for \(\lambda^{\star}=0\), we know that \(\lambda^{\star}V_{g}^{\pi^{\star}}(\rho)\leq 0\) leads to \(V_{g}^{\pi^{\star}}(\rho)=0\). By the definition \(\lambda^{\star}\in\operatorname*{argmin}_{\lambda\,\in\,\Lambda}V_{r+\lambda g }^{\pi^{\star}_{*}}(\rho)\), any \(\lambda^{\star}<\lambda_{\max}\) is a min-max point. Therefore, (iii) \[\leq \frac{\tau}{(\lambda_{\max}-\lambda^{\star})}\mathcal{H}(\pi_{\tau }^{\star}).\]

By combining three cases above, we conclude that (iii) \(\leq O(\tau)=O(\epsilon)\).

We now substitute the upper bounds of (iii) and (iv) above into (21) to obtain \(-V_{g}^{\pi_{t}}(\rho)\leq O(\sqrt{\epsilon})\).

Finally, we replace the accuracy \(\sqrt{\epsilon}\) by \(\epsilon\) and combine all big O notation to conclude the proof. 

### Zero constraint violation of RPG-PD (6)

**Corollary 15** (Zero constraint violation).: _Let Assumption 1 hold. For small \(\epsilon\), there exists \(\delta>0\) such that if we instead use the conservative constraint \(V_{g^{\prime}}^{\pi}(\rho)\geq 0\) for \(g^{\prime}=g-(1-\gamma)\delta\), and take \(\eta=\Theta(\epsilon^{4})\), \(\tau=\Theta(\epsilon^{2})\), and \(\epsilon_{0}=\epsilon\), then the policy iterates of RPG-PD (6) satisfy_

\[V_{r}^{\pi^{\star}}(\rho)-V_{r}^{\pi_{t}}(\rho)\ \leq\ \epsilon\ \mbox{and}\ \ -V_{g}^{\pi_{t}}(\rho)\ \leq\ 0\ \ \mbox{for any}\ t=\Omega\left(\,\frac{1}{\epsilon^{6}}\log^{2}\frac{1}{\epsilon}\,\right)\]

_where \(\Omega(\cdot)\) only has some problem-dependent constant._

Proof.: We apply the conservatism to the translated constraint \(V_{g}^{\pi}(\rho)\geq 0\) in Problem (1). Specifically, for any \(\delta<\min(\xi,1)\), we let \(g^{\prime}:=g-(1-\gamma)\delta\) and define a conservative constraint,

\[V_{g^{\prime}}^{\pi} := V_{g}^{\pi}(\rho)-\delta\ \geq\ 0.\]

It is straightforward to see that Assumption 1 ensures that \(V_{g^{\prime}}^{\pi_{t}}(\rho)\geq 0\) is strictly feasible for a new slack variable \(\xi^{\prime}:=\xi-\delta\). We now can apply RPG-PD (6) to a new regularized Lagrangian,

\[L_{\tau}^{\prime}(\pi,\lambda) := V_{r+\lambda g^{\prime}}^{\pi}(\rho)\,+\,\tau\left(\,\mathcal{H}( \pi)+\frac{1}{2}\lambda^{2}\,\right)\]and Corollary 3 holds if we replace \(g\) in RPG-PD by \(g^{\prime}\). Thus,

\[V_{r}^{\pi_{\delta}^{\star}}(\rho)-V_{r}^{\pi_{t}}(\rho)\ \leq\ \epsilon\ \ \text{ and }\ -V_{g^{\prime}}^{\pi_{t}}(\rho)\ \leq\ \epsilon\ \ \text{ for any }t=\Omega\left(\,\frac{1}{\epsilon^{6}}\log^{2}\frac{1}{ \epsilon}\,\right)\]

where \(\Omega(\cdot)\) hides some problem-dependent constant, and \(\pi_{\delta}^{\star}\) is an optimal policy to the \(\delta\)-perturbed constrained policy optimization problem,

\[\operatorname*{maximize}_{\pi\,\in\,\Pi}\ V_{r}^{\pi}(\rho)\qquad\mathrm{ subject\ to\ }V_{g}^{\pi}(\rho)-\delta\ \geq\ 0.\] (23)

We notice that the above \(\Omega(\cdot)\) has \(\xi^{\prime}\)-dependence and we denote it by \(\Xi(\xi^{\prime})\), where \(\Xi\): \(\mathbb{R}_{+}\to\mathbb{R}_{+}\) is a positive function. Thus, we select \(\delta\) such that \(\delta\geq\epsilon\Xi(\xi^{\prime})\), which is always possible for small enough \(\epsilon\), for instance, \(\delta=\frac{\xi}{2}\) and \(\xi^{\prime}=\frac{\xi}{2}\). Hence, if we take \(\delta=\frac{\xi}{2}\) and such small \(\epsilon\), then,

\[-V_{g^{\prime}}^{\pi_{t}}(\rho) =\ -\ V_{g}^{\pi_{t}}(\rho)\,+\,\delta\ \leq\ \ \epsilon\Xi(\xi^{\prime})\ \ \text{ for any }t=\Omega\left(\frac{1}{\epsilon^{6}}\log^{2}\frac{1}{ \epsilon}\right)\]

which shows that \(V_{g}^{\pi_{t}}(\rho)\geq 0\) for some large \(t\).

The rest is to show that \(V_{r}^{\pi^{\star}}(\rho)-V_{r}^{\pi_{t}}(\rho)\leq O(\epsilon)\). We notice that \(\pi^{\star}\) is an optimal policy to Problem (23) when \(\delta=0\). Let \(q^{\star}\) and \(q^{\star}_{\delta}\) be associated occupancy measures of policies \(\pi^{\star}\) and \(\pi_{\delta}^{\star}\). In the occupancy measure space, Problem (23) becomes a linear program and it has a solution \(q^{\star}_{\delta}\). Thus, we can view \(q^{\star}_{\delta}\) as a \(\delta\)-perturbed solution of a convex optimization problem in which all functions are continuously differentiable and the domain is convex and compact. It is known from [129, Theorem 3.1] that the optimal solution \(q^{\star}_{\delta}\) is continuous in \(\delta\), which implies that for any \(\epsilon>0\), there exists \(\delta^{\prime}>0\) such that \(|\langle r,q^{\star}\rangle-\langle r,q^{\star}_{\delta}\rangle|\leq O(\epsilon)\) for any \(\delta<\delta^{\prime}\). Thus, \(|V_{r}^{\pi^{\star}}(\rho)-V_{r}^{\pi_{\delta}^{\star}}(\rho)|\leq O(\epsilon)\) for small enough \(\epsilon\). Therefore,

\[V_{r}^{\pi^{\star}}(\rho)-V_{r}^{\pi_{t}}(\rho) \leq V_{r}^{\pi_{\delta}^{\star}}(\rho)-V_{r}^{\pi_{t}}(\rho)\,+\,|V_ {r}^{\pi^{\star}}(\rho)-V_{r}^{\pi_{\delta}^{\star}}(\rho)|\ \leq\ O(\epsilon)\]

for some large \(t\). Collecting all conditions on \(\delta\) leads to our final choice of \(\delta=\min(\frac{\xi}{2},1,\delta^{\prime})\). Finally, we combine all big O notation to complete the proof. 

### Reduction of RPG-PD (6) as a NPG variant (7)

We introduce some useful notation in the regularized MDP [52]. Let \(V_{r}^{\pi}(\rho):=V_{r+\lambda g}^{\pi}(\rho)+\tau\mathcal{H}(\pi)\). We introduce the soft-\(Q\) value function \(Q_{\tau}^{\pi}\): \(S\times A\to\mathbb{R}\) and \(V_{\tau}^{\pi}\): \(S\to\mathbb{R}\) via Bellman equations,

\[Q_{\tau}^{\pi}(s,a) = r(s,a)+\lambda g(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim P(\,|\,s,a)}\,[\,V_{\tau}^{\pi}(s^{\prime})\,]\] \[V_{\tau}^{\pi}(s) = \mathbb{E}_{a\sim\pi(\,|\,s)}\,[\,-\tau\log\pi(a\,|\,s)+Q_{\tau}^ {\pi}(s,a)\,]\,.\]

We also define \(A_{\tau}^{\pi}(s,a):=Q_{\tau}^{\pi}(s,a)-\tau\log\pi(a\,|\,s)-V_{\tau}^{\pi}(s)\). Hence,

\[Q_{\tau+\lambda g+\tau\psi}^{\pi}(s,a) = Q_{\tau}^{\pi}(s,a)-\tau\log\pi(a\,|\,s)\] \[A_{\tau+\lambda g+\tau\psi}^{\pi}(s,a) = Q_{\tau+\lambda g+\tau\psi}^{\pi}(s,a)-V_{r+\lambda g+\tau\psi}^ {\pi}(s)\]

where \(\psi(s,a):=-\log\pi(a\,|\,s)\) for all \((s,a)\in S\times A\).

Setting \(\epsilon_{0}=0\), it is easy to show that RPG-PD (6) is a case of (7) in the tabular case by introducing the softmax policy that is widely used in policy optimization. A softmax policy \(\pi_{\theta}\): \(S\to\Delta(A)\) is parametrized by a parameter \(\theta\in\mathbb{R}^{|S||A|}\) via a softmax function,

\[\pi_{\theta}(a\,|\,s) = \frac{\exp(\theta_{s,a})}{\sum_{a^{\prime}}\exp(\theta_{s,a^{ \prime}})}\ \ \text{for all }(s,a)\in S\times A.\]

With a slight abuse of notation, we also use notation \(\pi_{\theta}\) as a vector in \(\mathbb{R}^{|S||A|}\).

**Lemma 16**.: _Set \(\epsilon_{0}=0\). Under the softmax policy parametrization, RPG-PD (6) is equivalent to (7)._Proof.: Dual update (7b) is straightforward. We next show the equivalence for the primal update by applying the softmax function to both sides of Primal update (7a).

We first notice that \(F_{\rho}(\theta_{t})^{\dagger}\cdot\nabla_{\theta}L_{\tau}(\pi_{\theta_{t}}, \lambda_{t})=F_{\rho}(\theta_{t})^{\dagger}\cdot\nabla_{\theta}V_{\tau}^{\pi_{ \theta}}(\rho)\). Thus,

\[\exp(\theta_{t+1,s,a}) = \exp(\theta_{t,s,a})\exp\big{(}\eta(1-\gamma)(F_{\rho}(\theta_{t} )^{\dagger}\cdot\nabla_{\theta}L_{\tau}(\pi_{\theta_{t}},\lambda_{t}))_{s,a} \big{)}\] \[= \exp(\theta_{t,s,a})\exp\big{(}\eta A_{\tau}^{\pi_{\theta_{t}}}(s,a)+\eta c(s)\big{)}\]

where \(c(s)\) is an action-independent constant, and the second equality is the property of natural policy gradient (e.g., Lemma 29). Hence, after normalization over actions and some re-arrangement, we have

\[\pi_{\theta_{t+1}}(a\,|\,s) = \pi_{\theta_{t}}(a\,|\,s)\frac{\exp\big{(}\eta A_{\tau}^{\pi_{ \theta_{t}}}(s,a)+\eta c(s)\big{)}}{\sum_{a^{\prime}}\pi_{\theta_{t}}(a^{ \prime}\,|\,s)\exp\big{(}\eta A_{\tau}^{\pi_{\theta_{t}}}(s,a^{\prime})+\eta c (s)\big{)}}\] \[= \pi_{\theta_{t}}(a\,|\,s)\frac{\exp\big{(}\eta Q_{\tau}^{\pi_{ \theta_{t}}}(s,a)-\eta\tau\log\pi_{\theta_{t}}(a\,|\,s)\big{)}}{\sum_{a^{ \prime}}\pi_{\theta_{t}}(a^{\prime}\,|\,s)\exp\big{(}\eta Q_{\tau}^{\pi_{ \theta_{t}}}(s,a^{\prime})-\eta\tau\log\pi_{\theta_{t}}(a^{\prime}\,|\,s) \big{)}}\] \[= \pi_{\theta_{t}}(a\,|\,s)\frac{\exp\big{(}\eta Q_{\tau+\lambda_{t }g+\tau\psi_{t}}^{\pi_{\theta_{t}}}(s,a)\big{)}}{\sum_{a^{\prime}}\pi_{\theta _{t}}(a^{\prime}\,|\,s)\exp\big{(}\eta Q_{\tau+\lambda_{t}g+\tau\psi_{t}}^{\pi_ {\theta_{t}}}(s,a^{\prime})\big{)}}\]

which is an explicit form of the policy update in (6). Since the above derivation holds in both directions, the proof is complete. 

### Proof of Theorem 4

Proof.: We utilize the decomposition of the primal-dual gap as in (19) and analyze the term (i) and the term (ii), separately.

For the term (i), we have

\[L_{\tau}(\pi_{\tau}^{\star},\lambda_{t})-L_{\tau}(\pi_{\theta_{t}}, \lambda_{t})\] \[= V_{r+\lambda_{t}g}^{\pi_{\tau}^{\star}}(\rho)-V_{r+\lambda_{t}g}^{ \pi_{\theta_{t}}}(\rho)\] \[-\frac{\tau}{1-\gamma}\sum_{s,a}d_{\rho}^{\pi_{\tau}^{\star}}(s) \pi_{\tau}^{\star}(a\,|\,s)\log\pi_{\tau}^{\star}(a\,|\,s)\,+\,\frac{\tau}{1- \gamma}\sum_{s,a}d_{\rho}^{\pi_{\theta_{t}}}(s)\pi_{\theta_{t}}(a\,|\,s)\log \pi_{\theta_{t}}(a\,|\,s)\] \[= V_{r+\lambda_{t}g+\tau\psi_{t}}^{\pi_{\tau}^{\star}}(\rho)-V_{r+ \lambda_{t}g+\tau\psi_{t}}^{\pi_{\theta_{t}}}(\rho)-\tau V_{\psi_{t}}^{\pi_{ \tau}^{\star}}(\rho)+\tau V_{\psi_{t}}^{\pi_{\theta_{t}}}(\rho)\] \[-\frac{\tau}{1-\gamma}\sum_{s,a}d_{\rho}^{\pi_{\tau}^{\star}}(s) \pi_{\tau}^{\star}(a\,|\,s)\log\pi_{\tau}^{\star}(a\,|\,s)+\frac{\tau}{1- \gamma}\sum_{s,a}d_{\rho}^{\pi_{\theta_{t}}}(s)\pi_{\theta_{t}}(a\,|\,s)\log \pi_{\theta_{t}}(a\,|\,s)\] \[= \frac{1}{1-\gamma}\sum_{s,a}d_{\rho}^{\pi_{\tau}^{\star}}(s)\left( \pi_{\tau}^{\star}(a\,|\,s)-\pi_{\theta_{t}}(a\,|\,s)\right)Q_{r+\lambda_{t}g+ \tau\psi_{t}}^{\pi_{\theta_{t}}}(s,a)\] \[+\frac{\tau}{1-\gamma}\sum_{s,a}d_{\rho}^{\pi_{\tau}^{\star}}(s) \pi_{\tau}^{\star}(a\,|\,s)\log\pi_{\theta_{t}}(a\,|\,s)\,-\,\frac{\tau}{1- \gamma}\sum_{s,a}d_{\rho}^{\pi_{\theta_{t}}}(s)\pi_{\theta_{t}}(a\,|\,s)\log \pi_{\theta_{t}}(a\,|\,s)\] \[-\frac{\tau}{1-\gamma}\sum_{s,a}d_{\rho}^{\pi_{\tau}^{\star}}(s) \pi_{\tau}^{\star}(a\,|\,s)\log\pi_{\tau}^{\star}(a\,|\,s)\,+\,\frac{\tau}{1- \gamma}\sum_{s,a}d_{\rho}^{\pi_{\theta_{t}}}(s)\pi_{\theta_{t}}(a\,|\,s)\log \pi_{\theta_{t}}(a\,|\,s)\] \[= \frac{1}{1-\gamma}\sum_{s,a}d_{\rho}^{\pi_{\tau}^{\star}}(s) \left(\pi_{\tau}^{\star}(a\,|\,s)-\pi_{\theta_{t}}(a\,|\,s)\right)\phi_{s,a}^ {\top}w_{t}\,-\,\frac{\tau}{1-\gamma}\sum_{s}d_{\rho}^{\pi_{\tau}^{\star}}(s) \mbox{KL}_{t}(s)\] \[+\frac{1}{1-\gamma}\sum_{s,a}d_{\rho}^{\pi_{\tau}^{\star}}(s) \left(\pi_{\tau}^{\star}(a\,|\,s)-\pi_{\theta_{t}}(a\,|\,s)\right)\left(Q_{r+ \lambda_{t}g+\tau\psi_{t}}^{\pi_{\theta_{t}}}(s,a)-\phi_{s,a}^{\top}w_{t}\right)\] \[\leq \sum_{s}d_{\rho}^{\pi_{\tau}^{\star}}(s)\left(\frac{\mbox{KL}_{t }(s)-\mbox{KL}_{t+1}(s)}{\eta(1-\gamma)}\right)\,+\,\eta(C_{W})^{2}\,-\,\frac{ \tau}{1-\gamma}\sum_{s}d_{\rho}^{\pi_{\tau}^{\star}}(s)\mbox{KL}_{t}(s)\] \[+\frac{1}{1-\gamma}\sum_{s,a}d_{\rho}^{\pi_{\tau}^{\star}}(s) \left(\pi_{\tau}^{\star}(a\,|\,s)-\pi_{\theta_{t}}(a\,|\,s)\right)\left(Q_{r+ \lambda_{t}g+\tau\psi_{t}}^{\pi_{\theta_{t}}}(s,a)-\phi_{s,a}^{\top}w_{t}\right)\] \[= \sum_{s}d_{\rho}^{\pi_{\tau}^{\star}}(s)\left(\frac{(1-\eta\tau) \mbox{KL}_{t}(s)-\mbox{KL}_{t+1}(s)}{\eta(1-\gamma)}\right)\,+\,\eta(C_{W})^{2}\] \[+\,\frac{1}{1-\gamma}\sum_{s,a}d_{\rho}^{\pi_{\tau}^{\star}}(s) \left(\pi_{\tau}^{\star}(a\,|\,s)-\pi_{\theta_{t}}(a\,|\,s)\right)\left(Q_{r+ \lambda_{t}g+\tau\psi_{t}}^{\pi_{\theta_{t}}}(s,a)-\phi_{s,a}^{\top}w_{t}\right),\]

where the first two equalities are because of the entropy regularization \(\mathcal{H}(\pi)\), we apply performance difference lemma in Lemma 28 and \(\psi_{t}(s,a)=-\log\pi_{\theta_{t}}(a\,|\,s)\) to the third equality, the inequality is due to an application of Lemma 27 with \(x^{\star}=\pi_{\tau}^{\star}(\cdot\,|\,s)\), \(x=\pi_{\theta_{t}}(\cdot\,|\,s)\), \(g=-\phi_{s,a}^{\top}w_{t}\), and \(\eta\leq 1/C_{W}\), where \(|\phi_{s,a}^{\top}w_{t}/(1-\gamma)|\leq 2W/(1-\gamma):=C_{W}\). Moreover, the cross term has the following decomposition,

\[\sum_{s,a}d_{\rho}^{\pi_{\tau}^{\star}}(s)\left(\pi_{\tau}^{\star} (a\,|\,s)-\pi_{\theta_{t}}(a\,|\,s)\right)\left(Q_{r+\lambda_{t}g+\tau\psi_{t} }^{\pi_{\theta_{t}}}(s,a)-\phi_{s,a}^{\top}w_{t}\right)\] \[= \underbrace{\sum_{s,a}d_{\rho}^{\pi_{\tau}^{\star}}(s)\pi_{\tau}^{ \star}(a\,|\,s)\left(Q_{r+\lambda_{t}g+\tau\psi_{t}}^{\pi_{\theta_{t}}}(s,a)- \phi_{s,a}^{\top}w_{t}^{\star}\right)}_{\mbox{(a)}}\,+\,\underbrace{\sum_{s,a}d_{ \rho}^{\pi_{\tau}^{\star}}(s)\pi_{\theta_{t}}(a\,|\,s)\phi_{s,a}^{\top}\left(w_{ t}-w_{t}^{\star}\right)}_{\mbox{(b)}}\] \[+\,\underbrace{\sum_{s,a}d_{\rho}^{\pi_{\tau}^{\star}}(s)\pi_{ \tau}^{\star}(a\,|\,s)\phi_{s,a}^{\top}\left(w_{t}^{\star}-w_{t}\right)}_{ \mbox{(c)}}\,+\,\underbrace{\sum_{s,a}d_{\rho}^{\pi_{\tau}^{\star}}(s)\pi_{ \theta_{t}}(a\,|\,s)\left(\phi_{s,a}^{\top}w_{t}^{\star}-Q_{r+\lambda_{t}g+ \tau\psi_{t}}^{\pi_{\theta_{t}}}(s,a)\right)}_{\mbox{(d)}}.\]We now deal with the four terms (a), (b), (c), and (d), separately. For the term (a),

\[|\mbox{(a)}| \leq \sum_{s,a}d_{\rho^{\star}}^{\pi_{*}^{\star}}(s)\pi_{*}^{\star}(a\,| \,s)\,\Big{|}Q_{r+\lambda_{t}g+\tau\psi_{t}}^{\pi_{\theta_{t}}}(s,a)-\phi_{s,a}^ {\top}w_{t}^{\star}\Big{|}\] \[\leq \sqrt{\sum_{s,a}\frac{(d_{\rho^{\star}}^{\pi_{*}^{\star}}(s)\pi_{ *}^{\star}(a\,|\,s))^{2}}{d_{\rho}^{\pi_{*}^{\star}}(s)\mbox{Unif}_{A}(a)}\, \sum_{s,a}d_{\rho}^{\pi_{*}^{\star}}(s)\mbox{Unif}_{A}(a)\left(Q_{r+\lambda_{t }g+\tau\psi_{t}}^{\pi_{\theta_{t}}}(s,a)-\phi_{s,a}^{\top}w_{t}^{\star}\right)^ {2}}\] \[= \sqrt{\sum_{s,a}\frac{(d_{\rho}^{\pi_{*}^{\star}}(s)\pi_{*}^{ \star}(a\,|\,s))^{2}}{d_{\rho}^{\pi_{*}^{\star}}(s)\mbox{Unif}_{A}(a)}}\mathcal{ E}_{Q}(w_{t}^{\star},\theta_{t},\nu^{\star})\] \[\leq \sqrt{\sum_{s,a}\frac{d_{\rho^{\star}}^{\pi_{*}^{\star}}(s)\pi_{ *}^{\star}(a\,|\,s)}{\mbox{Unif}_{A}(a)}}\mathcal{E}_{Q}(w_{t}^{\star},\theta_ {t},\nu^{\star})\] \[= \sqrt{|A|\mathcal{E}_{Q}(w_{t}^{\star},\theta_{t},\nu^{\star})},\]

where we recall the definition of \(\mathcal{E}_{Q}(w_{t}^{\star},\theta_{t},\nu^{\star})\). Similarly, we can bound the term (d) by \(|\mbox{(d)}|\leq\sqrt{|A|\mathcal{E}_{Q}(w_{t}^{\star},\theta_{t},\nu^{\star})}\). For the term (b),

\[|\mbox{(b)}| \leq \sum_{s,a}d_{\rho^{\star}}^{\pi_{*}^{\star}}(s)\pi_{\theta_{t}}(a \,|\,s)|\phi_{s,a}^{\top}\left(w_{t}-w_{t}^{\star}\right)|\] \[\leq \sqrt{\sum_{s,a}\frac{(d_{\rho^{\star}}^{\pi_{*}^{\star}}(s)\pi_{ \theta_{t}}(a\,|\,s))^{2}}{d_{\rho^{\star}}^{\pi_{*}^{\star}}(s)\mbox{Unif}_{ A}(a)}\sum_{s,a}d_{\rho}^{\pi_{*}^{\star}}(s)\mbox{Unif}_{A}(a)\left(\phi_{s,a}^{ \top}\left(w_{t}-w_{t}^{\star}\right)\right)^{2}}\] \[= \sqrt{\sum_{s,a}\frac{(d_{\rho^{\star}}^{\pi_{*}^{\star}}(s)\pi_{ \theta_{t}}(a\,|\,s))^{2}}{d_{\rho}^{\pi_{*}^{\prime}}(s)\mbox{Unif}_{A}(a)} \left\|w_{t}-w_{t}^{\star}\right\|_{\Sigma_{\nu^{\star}}}^{2}}\] \[\leq \sqrt{|A|\sum_{s,a}\frac{d_{\rho^{\star}}^{\pi_{*}^{\star}}(s)\pi _{\theta_{t}}(a\,|\,s)}{\mbox{Unif}_{A}(a)}\left\|w_{t}-w_{t}^{\star}\right\|_ {\Sigma_{\nu^{\star}}}^{2}}\] \[\leq \sqrt{|A|\sum_{t=0}^{\left|u_{t}-w_{t}^{\star}\right\|_{\Sigma_{ \mu_{*}}}^{2}}},\]

where we recall the definition of \(\kappa_{\nu}\) to obtain the last line. Similarly, we can bound the term (c) by \(|\mbox{(c)}|\leq\sqrt{|A|\left\|w_{t}-w_{t}^{\star}\right\|_{\Sigma_{\nu^{\star} }}^{2}}\). Moreover, the optimality of \(w_{t}^{\star}\in\operatorname*{argmin}_{w\,\in\,\mathbb{R}^{d}}\mathcal{E}_{Q} (w,\theta_{t},d_{t,\nu})\) yields,

\[(w-w_{t}^{\star})^{\top}\nabla_{w}\mathcal{E}_{Q}(w_{t}^{\star},\theta_{t},d_{ t,\nu})\;\geq\;0,\;\;\mbox{ for any }\left\|w\right\|\leq W\]

which further implies that for any \(\|w\|\leq W\),

\[\mathcal{E}_{Q}(w,\theta_{t},d_{t,\nu})-\mathcal{E}_{Q}(w_{t}^{ \star},\theta_{t},d_{t,\nu})\] \[= \mathbb{E}_{(s,a)\,\sim\,d_{t,\nu}}\left[\,\left(\phi_{s,a}^{ \top}w-\phi_{s,a}^{\top}w_{t}^{\star}+\phi_{s,a}^{\top}w_{t}^{\star}-Q_{r+ \lambda_{t}g+\tau\psi_{t}}^{\pi_{\theta_{t}}}(s,a)\right)^{2}\,\right]\,- \,\mathcal{E}_{Q}(w_{t}^{\star},\theta_{t},d_{t,\nu})\] \[= \mathbb{E}_{(s,a)\,\sim\,d_{t,\nu}}\left[\,\left(\phi_{s,a}^{ \top}w-\phi_{s,a}^{\top}w_{t}^{\star}\right)^{2}\,\right]\,+\,2(w-w_{t}^{ \star})^{\top}\mathbb{E}_{(s,a)\,\sim\,d_{t,\nu}}\left[\,\phi_{s,a}\left(\phi_ {s,a}^{\top}w_{t}^{\star}-Q_{r+\lambda_{t}g+\tau\psi_{t}}^{\pi_{\theta_{t}}}(s,a) \right)\,\right]\] \[= \mathbb{E}_{(s,a)\,\sim\,d_{t,\nu}}\left[\,\left(\phi_{s,a}^{ \top}w-\phi_{s,a}^{\top}w_{t}^{\star}\right)^{2}\,\right]\,+\,2(w-w_{t}^{ \star})^{\top}\nabla_{w}\mathcal{E}_{Q}(w_{t}^{\star},\theta_{t},d_{t,\nu})\] \[\geq \mathbb{E}_{(s,a)\,\sim\,d_{t,\nu}}\left[\,\left(\phi_{s,a}^{ \top}w-\phi_{s,a}^{\top}w_{t}^{\star}\right)^{2}\,\right]\] \[= \|w-w_{t}^{\star}\|_{\Sigma_{d_{t,\nu}}}^{2}.\]

Therefore,

\[|\mbox{(b)}| \leq \sqrt{|A|\kappa_{\nu}\left\|w_{t}-w_{t}^{\star}\right\|_{\Sigma_{ d_{t,\nu}}}^{2}} \leq \sqrt{|A|\kappa_{\nu}\left(\mathcal{E}_{Q}(w_{t},\theta_{t},d_{t,\nu})- \mathcal{E}_{Q}(w_{t}^{\star},\theta_{t},d_{t,\nu})\right)}.\]With a similar reasoning, we can bound the term (c) by

\[|(\mbox{c})| \leq \sqrt{|A|\kappa_{\nu}\left(\mathcal{E}_{Q}(w_{t},\theta_{t},d_{t, \nu})-\mathcal{E}_{Q}(w_{t}^{\star},\theta_{t},d_{t,\nu})\right)}.\]

By applying the upper bounds above to the cross term, and then taking expectation over the randomness in \(w_{t}\), we have

\[\mathbb{E}\left[\,L_{\tau}(\pi_{\tau}^{\star},\lambda_{t})-L_{ \tau}(\pi_{\theta_{t}},\lambda_{t})\,\right]\] \[\leq \frac{(1-\eta\tau)\mathbb{E}[\mbox{KL}_{t}(\rho)]-\mathbb{E}[ \mbox{KL}_{t+1}(\rho)]}{\eta}\,+\,\eta(C_{W})^{2}\,+\,2\mathbb{E}\left[\, \sqrt{|A|\mathcal{E}_{Q}(w_{t}^{\star},\theta_{t},\nu^{\star})}\,\right]\] \[\,+\,2\mathbb{E}\left[\,\sqrt{|A|\kappa_{\nu}\left(\mathcal{E}_{Q }(w_{t},\theta_{t},d_{t,\nu})-\mathcal{E}_{Q}(w_{t}^{\star},\theta_{t},d_{t, \nu})\right)}\,\right]\] \[\leq \frac{(1-\eta\tau)\mathbb{E}[\mbox{KL}_{t}(\rho)]-\mathbb{E}[ \mbox{KL}_{t+1}(\rho)]}{\eta}\,+\,\eta(C_{W})^{2}\,+\,2\sqrt{|A|\mathbb{E} \left[\,\mathcal{E}_{Q}(w_{t}^{\star},\theta_{t},\nu^{\star})\,\right]}\] \[\,+\,2\sqrt{|A|\kappa_{\nu}\mathbb{E}\left[\,\mathcal{E}_{Q}(w_{t },\theta_{t},d_{t,\nu})-\mathcal{E}_{Q}(w_{t}^{\star},\theta_{t},d_{t,\nu})\, \right]}\] \[\leq \frac{(1-\eta\tau)\mathbb{E}[\mbox{KL}_{t}(\rho)]-\mathbb{E}[ \mbox{KL}_{t+1}(\rho)]}{\eta}\,+\,\eta(C_{W})^{2}\,+\,2\sqrt{|A|\epsilon_{ \rm bias}}\,+\,2\sqrt{|A|\kappa_{\nu}\epsilon_{\rm stat}}.\]

Similarly, for the term (ii),

\[L_{\tau}(\pi_{\theta_{t}},\lambda_{t})-L_{\tau}(\pi_{\theta_{t}},\lambda_{\tau}^{\star})\] \[= V_{r+\lambda_{t}g}^{\pi_{\theta_{t}}}(\rho)-V_{r+\lambda_{t}g}^{ \pi_{\theta_{t}}}(\rho)\,+\,\frac{1}{2}\tau(\lambda_{t})^{2}\,-\,\frac{1}{2} \tau(\lambda_{\tau}^{\star})^{2}\] \[= (\lambda_{t}-\lambda_{\tau}^{\star})V_{g}^{\pi_{\theta_{t}}}( \rho)\,+\,\frac{1}{2}\tau(\lambda_{t})^{2}\,-\,\frac{1}{2}\tau(\lambda_{\tau} ^{\star})^{2}\] \[= (\lambda_{t}-\lambda_{\tau}^{\star})\Big{(}V_{g}^{\pi_{\theta_{t} }}(\rho)+\tau\lambda_{t}\Big{)}\,-\,\frac{1}{2}\tau(\lambda_{t}-\lambda_{\tau} ^{\star})^{2}\] \[\leq \frac{(\lambda_{\tau}^{\star}-\lambda_{t})^{2}-(\lambda_{\tau}^{ \star}-\lambda_{t+1})^{2}}{2\eta}\,+\,\frac{1}{2}\eta(C_{\tau,\xi}^{\prime}) ^{2}\,-\,\frac{1}{2}\tau(\lambda_{t}-\lambda_{\tau}^{\star})^{2}\] \[= \frac{(1-\eta\tau)(\lambda_{\tau}^{\star}-\lambda_{t})^{2}-( \lambda_{\tau}^{\star}-\lambda_{t+1})^{2}}{2\eta}\,+\,\frac{1}{2}\eta(C_{ \tau,\xi}^{\prime})^{2}\]

where the inequality is due to the standard descent lemma [128] and \(V_{g}^{\pi_{\theta_{t}}}(\rho)+\tau\lambda_{t}\leq\frac{1}{1-\gamma}(1+\frac{ \tau}{\xi}):=C_{\tau,\xi}^{\prime}\). By taking expectation over the randomness in \(w_{t}\),

\[\mathbb{E}\left[\,L_{\tau}(\pi_{\theta_{t}},\lambda_{t})-L_{\tau}(\pi_{\theta_{ t}},\lambda_{\tau}^{\star})\,\right] \leq \frac{(1-\eta\tau)\mathbb{E}\left[(\lambda_{\tau}^{\star}-\lambda_ {t})^{2}\right]-\mathbb{E}\left[(\lambda_{\tau}^{\star}-\lambda_{t+1})^{2} \right]}{2\eta}\,+\,\,\frac{1}{2}\eta(C_{\tau,\xi}^{\prime})^{2}.\]

Using the definition \(\mathbb{E}\left[\,\Phi_{t}\,\right]:=\,\mathbb{E}\left[\,\mbox{KL}_{t}(\rho) \,\right]+\frac{1}{2}\mathbb{E}\left[(\lambda_{\tau}^{\star}-\lambda_{t})^{2} \right]\), we combine the two inequalities above to show that,

\[\mathbb{E}\left[\,\Phi_{t+1}\,\right] \leq (1-\eta\tau)\mathbb{E}\left[\,\Phi_{t}\,\right]\,-\,\eta\mathbb{E }\left[\,L_{\tau}(\pi_{\tau}^{\star},\lambda_{t})-L_{\tau}(\pi_{\theta_{t}}, \lambda_{\tau}^{\star})\,\right]\,+\,\eta^{2}\max\left((C_{W})^{2},(C_{\tau,\xi} ^{\prime})^{2}\right)\] \[+\,2\eta\sqrt{|A|\epsilon_{\rm bias}}\,+\,2\eta\sqrt{|A|\kappa_{ \nu}\epsilon_{\rm stat}}\] \[\leq (1-\eta\tau)\mathbb{E}\left[\,\Phi_{t}\,\right]\,+\,\eta^{2}\max \left((C_{W})^{2},(C_{\tau,\xi}^{\prime})^{2}\right)\,+\,2\eta\sqrt{|A|\epsilon_{ \rm bias}}\,+\,2\eta\sqrt{|A|\kappa_{\nu}\epsilon_{\rm stat}},\]where the second inequality is due to Lemma 14. If we expand the inequality above recursively, then,

\[\mathbb{E}\left[\,\Phi_{t+1}\,\right] \leq (1-\eta\tau)\mathbb{E}\left[\,\Phi_{t}\,\right]\,+\,\eta^{2}\max \left((C_{W})^{2},(C^{\prime}_{\tau,\xi})^{2}\right)\,+\,2\eta\sqrt{|A|\epsilon _{\text{bias}}}\,+\,2\eta\sqrt{|A|\kappa_{\nu}\epsilon_{\text{stat}}}\] \[\leq (1-\eta\tau)^{2}\mathbb{E}\left[\,\Phi_{t-1}\,\right]\,+\,(\eta^{ 2}+\eta^{2}(1-\eta\tau))\max\left((C_{W})^{2},(C^{\prime}_{\tau,\xi})^{2}\right)\] \[\,+\,2\eta\,(1+(1-\eta\tau))\left(\sqrt{|A|\epsilon_{\text{bias}} }+2\sqrt{|A|\kappa_{\nu}\epsilon_{\text{stat}}}\right)\] \[\leq \cdots\] \[\leq (1-\eta\tau)^{t}\mathbb{E}\left[\,\Phi_{1}\,\right]\,+\,\frac{ \eta}{\tau}\max\left((C_{W})^{2},(C^{\prime}_{\tau,\xi})^{2}\right)\,+\,\frac {2}{\tau}\left(\sqrt{|A|\epsilon_{\text{bias}}}+\sqrt{|A|\kappa_{\nu}\epsilon _{\text{stat}}}\right)\] \[\leq (1-\eta\tau)^{t}\mathbb{E}\left[\,\Phi_{1}\,\right]\,+\,\frac{ \eta}{\tau}\max\left((C_{W})^{2},(C^{\prime}_{\tau,\xi})^{2}\right)\,+\,\frac {2}{\tau}\left(\sqrt{|A|\epsilon_{\text{bias}}}+\sqrt{|A|\kappa_{\nu}\epsilon _{\text{stat}}}\right)\] \[\leq \mathrm{e}^{-\eta\tau t}\mathbb{E}\left[\,\Phi_{1}\,\right]\,+\, \frac{\eta}{\tau}\max\left((C_{W})^{2},(C^{\prime}_{\tau,\xi})^{2}\right)\,+ \,\frac{2}{\tau}\left(\sqrt{|A|\epsilon_{\text{bias}}}+\sqrt{|A|\kappa_{\nu} \epsilon_{\text{stat}}}\right)\]

which completes the proof. 

### Proof of Corollary 5

Proof.: The proof is similar to the proof of Corollary 3, except that we take the expectation over the randomness of computing \(w_{t}\) via a sample-based algorithm.

According to Theorem 4 and \(\epsilon_{\text{stat}}=O(\epsilon^{4})\), \(\epsilon_{\text{stat}}=O(\epsilon^{4})\), if we take \(\tau=\Theta(\epsilon)\) and \(\eta=\Theta(\epsilon^{2})\), then \(\mathbb{E}[\Phi_{t+1}]=O(\epsilon)\) for any \(t=\Omega\left(\frac{1}{\epsilon^{3}}\log\frac{1}{\epsilon}\right)\). We next consider a primal-dual iterate \((\pi_{\theta_{t}},\lambda_{t})\) for some \(t=\Omega\left(\frac{1}{\epsilon^{3}}\log\frac{1}{\epsilon}\right)\). It is straightforward to check that

\[\mathbb{E}\left[\,\mathrm{KL}_{t}(\rho)\,\right] = O(\epsilon)\ \text{ and }\ \frac{1}{2}\mathbb{E}\left[\,(\lambda_{\tau}^{\star}-\lambda_{t})^{2}\,\right] = O(\epsilon).\]

First, we have

\[\mathbb{E}\left[\,V_{r}^{\pi^{\star}}(\rho)-V_{r}^{\pi_{\theta_{t}}}(\rho)\,\right] = \underbrace{V_{r}^{\pi^{\star}}(\rho)-V_{r}^{\pi_{\tau}^{\star}} (\rho)}_{\text{(i)}}+\underbrace{\mathbb{E}\left[\,V_{r}^{\pi_{\tau}^{\star}} (\rho)-V_{r}^{\pi_{\theta_{t}}}(\rho)\,\right]}_{\text{(ii)}}.\] (24)

For the term (ii), because of \(\mathrm{KL}_{t}(\rho)=O(\epsilon)\), we have

\[\text{(ii)} = \mathbb{E}\left[\,\frac{1}{1-\gamma}\sum_{s,a}d_{\rho}^{\pi_{\tau }^{\star}}(s)\,(\pi_{\tau}^{\star}(a\,|\,s)-\pi_{\theta_{t}}(a\,|\,s))\,Q_{ \tau}^{\pi_{\theta_{t}}}(s,a)\,\right]\] \[\leq \frac{1}{(1-\gamma)^{2}}\sum_{s}d_{\rho}^{\pi_{\tau}^{\star}}(s) \mathbb{E}\left[\,\|\pi_{\tau}^{\star}(\cdot\,|\,s)-\pi_{\theta_{t}}(\cdot\,| \,s)\|_{1}\,\right]\] \[\leq \frac{1}{(1-\gamma)^{2}}\sum_{s}d_{\rho}^{\pi_{\tau}^{\star}}(s) \sqrt{2\,\mathbb{E}\left[\,\mathrm{KL}_{t}(s)\,\right]}\] \[\leq \frac{1}{(1-\gamma)^{2}}\sqrt{2\sum_{s}d_{\rho}^{\pi_{\tau}^{ \star}}(s)\mathbb{E}\left[\,\mathrm{KL}_{t}(s)\,\right]}\] \[= \frac{1}{(1-\gamma)^{2}}\sqrt{2\,\mathbb{E}\left[\,\mathrm{KL}_{ t}(\rho)\,\right]}\]

where we use Cauchy-Schwarz inequality in the first and third inequalities, and the second inequality is due to Pinsker's inequality and Jensen's inequality, which shows \(\mathbb{E}\left[\,V_{r}^{\pi_{\tau}^{\star}}(\rho)-V_{r}^{\pi_{\theta_{t}}}( \rho)\,\right]\leq O(\sqrt{\epsilon})\). For the term (i), if we take \(\pi=\pi^{\star}\) in (5), then,

\[V_{r}^{\pi^{\star}}(\rho)-\tau\mathcal{H}(\pi_{\tau}^{\star}) \leq V_{r}^{\pi_{\tau}^{\star}}(\rho)\,+\,\lambda_{\tau}^{\star}\left(V_{g }^{\pi_{\tau}^{\star}}(\rho)-V_{g}^{\pi_{\tau}^{\star}}(\rho)\right).\]Meanwhile, if we take \(\lambda=0\) in (5), then \(\lambda_{\tau}^{*}V_{g}^{\pi_{\tau}}(\rho)\leq 0\). We notice that the feasibility of \(\pi^{*}\) yields \(V_{g}^{\pi^{*}}(\rho)\geq 0\). Hence,

\[\text{(i)} = V_{r}^{\pi^{*}}(\rho)-V_{r}^{\pi_{\tau}^{*}}(\rho)\ \ \leq\ \tau\mathcal{H}(\pi_{\tau}^{*}).\]

We now substitute the upper bounds of (i) and (ii) into (24) to obtain \(\mathbb{E}\left[\,V_{r}^{\pi^{*}}(\rho)-V_{r}^{\pi_{\theta_{t}}}(\rho)\, \right]\leq O(\sqrt{\epsilon})\), where we take \(\tau=\Theta(\epsilon)\).

Second, we have

\[\mathbb{E}\left[\,-V_{g}^{\pi_{\theta_{t}}}(\rho)\,\right] = \underbrace{-\,V_{g}^{\pi_{\tau}^{*}}(\rho)}_{\text{(iii)}}+ \underbrace{\mathbb{E}\left[\,V_{g}^{\pi_{\tau}^{*}}(\rho)-V_{g}^{\pi_{\theta_ {t}}}(\rho)\,\right]}_{\text{(iv)}}.\] (25)

Similar to bounding \(\mathbb{E}\left[\,V_{r}^{\pi_{\tau}^{*}}(\rho)-V_{r}^{\pi_{\theta_{t}}}(\rho) \,\right]\), we can show that \(\mathbb{E}\left[\text{(iv)}\right]\leq O(\sqrt{\epsilon})\). For the term (iii), we can show that (iii) \(\leq O(\tau)=O(\epsilon)\) in a similar way as dealing with (iii) in (21).

We now substitute the upper bounds of (iii) and (iv) above into (25) to obtain \(\mathbb{E}\left[\,-V_{g}^{\pi_{\theta_{t}}}(\rho)\,\right]\leq O(\sqrt{ \epsilon})\).

Finally, we replace the accuracy \(\sqrt{\epsilon}\) by \(\epsilon\) and combine all big O notation to conclude the proof. 

### Zero constraint violation of inexact RPG-PD

**Corollary 17** (Zero constraint violation).: _Let Assumptions 1-2 hold and \(\epsilon_{\text{stat}}\), \(\epsilon_{\text{bias}}=O(\epsilon^{8})\) for small \(\epsilon\), \(\epsilon_{0}>0\). For small \(\epsilon\), there exists \(\delta>0\) such that if we instead use the conservative constraint \(V_{g}^{\pi}(\rho)\geq 0\) for \(g^{\prime}=g-(1-\gamma)\delta\), and take the stepsize \(\eta=\Theta(\epsilon^{4})\) and \(\tau=\Theta(\epsilon^{2})\), then the policy iterates of inexact RPG-PD satisfy_

\[\mathbb{E}\left[\,V_{r}^{\pi^{*}}(\rho)-V_{r}^{\pi_{\theta_{t}}}(\rho)\, \right]\ \leq\ \epsilon\ \text{ and }\ \mathbb{E}\left[\,-V_{g}^{\pi_{\theta_{t}}}(\rho)\,\right]\ \leq\ 0\ \ \text{ for any }t=\Omega\left(\,\frac{1}{\epsilon^{6}}\log^{2}\frac{1}{\epsilon}\,\right)\]

_where \(\Omega(\cdot)\) only has some problem-dependent constant._

Proof.: We apply the conservatism to the translated constraint \(V_{g}^{\pi}(\rho)\geq 0\) in Problem (1). Specifically, for any \(\delta<\min(\xi,1)\), we let \(g^{\prime}:=g-(1-\gamma)\delta\) and define a conservative constraint,

\[V_{g^{\prime}}^{\pi} := V_{g}^{\pi}(\rho)-\delta\ \ \geq\ \ 0.\]

It is straightforward to see that Assumption 1 ensures that \(V_{g^{\prime}}^{\pi_{t}}(\rho)\geq 0\) is strictly feasible for a new slack variable \(\xi^{\prime}:=\xi-\delta\). We now can apply inexact RPG-PD (8) to a new regularized Lagrangian,

\[L_{\tau}^{\prime}(\pi,\lambda) := V_{r+\lambda g^{\prime}}^{\pi}(\rho)\,+\,\tau\left(\,\mathcal{H}( \pi)+\frac{1}{2}\lambda^{2}\,\right)\]

and Corollary 5 holds if we replace \(g\) in inexact RPG-PD by \(g^{\prime}\) and \(\epsilon_{\text{stat}}\), \(\epsilon_{\text{bias}}=O(\epsilon^{8})\) for small \(\epsilon>0\). Thus,

\[\mathbb{E}\left[\,V_{r}^{\pi_{\delta}^{*}}(\rho)-V_{r}^{\pi_{\theta_{t}}}(\rho )\,\right]\ \leq\ \epsilon\ \text{ and }\ \mathbb{E}\left[\,-V_{g^{\prime}}^{\pi_{\theta_{t}}}(\rho)\,\right]\ \leq\ \epsilon\ \ \text{ for any }t=\Omega\left(\,\frac{1}{\epsilon^{6}}\log^{2}\frac{1}{\epsilon}\,\right)\]

where \(\Omega(\cdot)\) has some problem-dependent constant, and \(\pi_{\delta}^{*}\) is an optimal policy to the \(\delta\)-perturbed constrained policy optimization problem,

\[\operatorname*{maximize}_{\pi\,\in\,\Pi}\ V_{r}^{\pi}(\rho)\qquad\mathrm{ subject}\ \mathrm{to}\ V_{g}^{\pi}(\rho)-\delta\ \geq\ 0.\] (26)

We notice that the above \(\Omega(\cdot)\) has \(\xi^{\prime}\)-dependence and we denote it by \(\Xi(\xi^{\prime})\), where \(\Xi\): \(\mathbb{R}_{+}\to\mathbb{R}_{+}\) is a positive function. Thus, we select \(\delta\) such that \(\delta\geq\epsilon\Xi(\xi^{\prime})\), which is always possible for small enough \(\epsilon\), for instance, \(\delta=\frac{\xi}{2}\) and \(\xi^{\prime}=\frac{\xi}{2}\). Hence, if we take \(\delta=\frac{\xi}{2}\) and such small \(\epsilon\), then

\[\mathbb{E}\left[\,-V_{g^{\prime}}^{\pi_{\theta_{t}}}(\rho)\,\right] = \mathbb{E}\left[\,-V_{g}^{\pi_{\theta_{t}}}(\rho)\,\right]\,+\, \delta\ \leq\ \ \epsilon\Xi(\xi^{\prime})\ \ \text{ for any }t=\Omega\left(\frac{1}{\epsilon^{6}}\log^{2}\frac{1}{\epsilon}\right)\]

which shows that \(\mathbb{E}\left[V_{g}^{\pi_{\theta_{t}}}(\rho)\right]\geq 0\) for some large \(t\).

The rest is to show that \(\mathbb{E}\left[V_{r}^{\pi^{*}}(\rho)-V_{r}^{\pi_{\theta_{t}}}(\rho)\right]\leq O(\epsilon)\). We notice that \(\pi^{\star}\) is an optimal policy to Problem (26) when \(\delta=0\). Let \(q^{\star}\) and \(q^{\star}_{\delta}\) be associated occupancy measures of policies \(\pi^{\star}\) and \(\pi^{\star}_{\delta}\). In the occupancy measure space, Problem (26) becomes a linear program and it has a solution \(q^{\star}_{\delta}\). Thus, we can view \(q^{\star}_{\delta}\) as a \(\delta\)-perturbed solution of a convex optimization problem in which all functions are continuous differentiable and the domain is convex and compact. It is known from [129, Theorem 3.1] that the optimal solution \(q^{\star}_{\delta}\) is continuous in \(\delta\), which implies that for any \(\epsilon>0\), there exists \(\delta^{\prime}\) such that \(|\langle r,q^{\star}\rangle-\langle r,q^{\star}_{\delta}\rangle|\leq O(\epsilon)\) for any \(\delta<\delta^{\prime}\). Thus, \(|V_{r}^{\pi^{*}}(\rho)-V_{r}^{\pi^{\sharp}_{\delta}}(\rho)|\leq O(\epsilon)\) for small enough \(\epsilon\). Therefore,

\[\mathbb{E}\left[\,V_{r}^{\pi^{*}}(\rho)-V_{r}^{\pi_{\theta_{t}}}( \rho)\,\right] \leq \mathbb{E}\left[\,V_{r}^{\pi^{\sharp}_{\delta}}(\rho)-V_{r}^{\pi _{\theta_{t}}}(\rho)\,\right]\,+\,|V_{r}^{\pi^{*}}(\rho)-V_{r}^{\pi^{\sharp}_ {\delta}}(\rho)|\,\,\,\leq\,\,\,O(\epsilon)\]

for some large \(t\). Collecting all conditions on \(\delta\) leads to our final choice of \(\delta=\min(\frac{\xi}{2},1,\delta^{\prime})\). Finally, we combine all big O notation to complete the proof. 

### Sample-based inexact RPG-PD algorithm

We generalize the inexact RPG-PD to be a sample-based algorithm that only takes sample-based estimates. We propose a sample-based RPG-PD with linear function approximation as follows,

\[\pi_{\theta_{t+1}}(\cdot\,|\,s) = \operatorname*{argmax}_{\pi(\,\cdot\,|\,s)\,\in\,\hat{\Delta}(A )}\left\{\sum_{a}\pi(a\,|\,s)\phi_{s,a}^{\top}\hat{w}_{t}\,-\,\frac{1}{\eta} \operatorname{\mathbf{KL}}(\pi(\cdot\,|\,s),\pi_{\theta_{t}}(\cdot\,|\,s))\right\}\] (27a) \[\lambda_{t+1} = \operatorname*{argmin}_{\lambda\,\in\,\Lambda}\left\{\lambda\Big{(} \widehat{V}_{g}^{\pi_{\theta_{t}}}(\rho)+\tau\lambda_{t}\Big{)}\,+\,\frac{1}{ 2\eta}\,(\lambda-\lambda_{t})^{2}\right\},\] (27b)

where \(\hat{w}_{t}\) and \(\hat{V}_{g}^{\pi_{t}}(\rho)\) are the sample-based estimates of NPG directions and value functions. It is standard to assume that there exists a policy simulator that generates policy rollouts for any given policies [49]. At time \(t\), we can estimate \(\hat{w}_{t}\) by solving the regression problem \(\mathcal{E}_{Q}(w,\theta_{t},d_{t,\nu})=\mathbb{E}_{(s,a)\,\sim\,d_{t,\nu}} \big{[}\,(\phi_{s,a}^{\top}w-Q^{\pi_{\theta_{t}}}(s,a))^{2}\,\big{]}\) with \(Q^{\pi_{\theta_{t}}}(s,a)=Q_{r+\lambda_{t}g+\tau\hat{v}_{t}}^{\pi_{\theta_{t}}} (s,a)\) via a projected stochastic gradient descent (SGD) method,

\[w_{t}^{k+1} = \mathcal{P}_{\|w\|\,\leq\,W}\left(\,w_{t}^{k}-\alpha\,G_{t}^{k}\,\right)\]

where \(k\geq 0\) counts the number of SGD iterations, and \(G_{t}^{k}\) is a sample-based estimate of the population gradient \(\nabla_{w}\mathcal{E}_{Q}(w,\theta_{t},d_{t,\nu})\),

\[G_{t}^{k} = 2\,\Big{(}\,\phi_{s,a}^{\top}w_{t}^{k}-Q_{r+\lambda_{t}g+\tau \hat{v}_{t}}^{\pi_{\theta_{t}}}(s,a)\,\Big{)}\,\phi_{s,a}.\]

From the projected SGD result [130], we use a weighted average \(\frac{2}{K(K+1)}\sum_{k\,=\,0}^{K-1}(k+1)w_{t}^{k}\) as our \(\hat{w}_{t}\). We note that \(Q_{r+\lambda_{t}g+\tau\hat{v}_{t}}^{\pi_{\theta_{t}}}(s,a)\) is a sum of a soft-\(Q\) value function associated with a composite function \(r+\lambda_{t}g-\tau\log\pi_{\theta_{t}}\), and \(-\tau\log\pi_{\theta_{t}}\). Thus, we can estimate the value function \(\hat{Q}_{r+\lambda_{t}g+\tau\hat{v}_{t}}^{\pi_{\theta_{t}}}(s,a)\) using policy rollouts in Algorithm 2, which provides an unbiased estimate and it has bounded variance [131],

\[\mathbb{E}\left[\,\hat{Q}_{r+\lambda_{t}g+\tau\hat{v}_{t}}^{\pi_{ \theta_{t}}}(s,a)\,\big{|}\,s,a\,\right] = Q_{r+\lambda_{t}g+\tau\hat{v}_{t}}^{\pi_{\theta_{t}}}(s,a)\,\,\, \text{and}\,\,\,\mathbb{E}\left[\,G_{t}^{k}\,\right] = \nabla_{w}\mathcal{E}_{Q}(w_{t}^{k},\theta_{t},d_{t,\nu})\]

where the expectation \(\mathbb{E}\) is taken over the randomness of drawing \((s,a)\sim d_{t,\nu}\). Another value function \(\hat{V}_{g}^{\pi_{\theta_{t}}}(\rho)\) can be estimated using policy rollouts in Algorithm 3, \(\mathbb{E}\left[\,\hat{V}_{g}^{\pi_{\theta_{t}}}(\rho)\,\right]=V_{g}^{\pi_{ \theta_{t}}}(\rho)\), which is also unbiased and has bounded variance [23]. Hence, simply replacing all population quantities in in inexact RPG-PD by their sample-based estimates leads to a sample-based inexact RPG-PD that is detailed in Algorithm 1.

We are now ready to establish the sample complexity of Algorithm 1 by exploiting the projected SGD result [130].

**Corollary 18** (Sample complexity of inexact RPG-PD).: _Let Assumptions 1-2 hold. Assume \(\Sigma_{\nu}=\mathbb{E}_{(s,a)\,\sim\,\nu}\left[\,\phi_{s,a}\phi_{s,a}^{\top} \,\right]\geq\kappa_{0}\,I\) for any state-action distribution \(\nu\) and some \(\kappa_{0}>0\). If we take the ```
1:Input: Learning rate \(\eta\), number of SGD iterations \(K\), SGD learning rate \(\alpha\).
2: Initialize \(\theta_{0}=0\), \(\lambda_{0}=0\),
3:for\(t=0,\ldots,T-1\)do
4: Initialize \(w_{t}^{0}=0\).
5:for\(k=0,1,\ldots,K-1\)do
6: Estimate \(\hat{Q}_{r+\lambda_{t}g+\tau\psi_{t}}^{\pi_{\theta_{t}}}(s,a)\) for some \((s,a)\sim d_{t,\nu}\), using Algorithm 2 with policy \(\pi_{\theta_{t}}\).
7: Perform projected SGD step with \(\alpha_{k}=\frac{2}{\kappa_{0}(k+2)}\), \[w_{t}^{k+1} = \mathcal{P}_{\|w\|\,\leq\,W}\left(w_{t}^{k}\,-\,2\alpha_{k}\left( \phi_{s,a}^{\top}w_{t}^{k}-\hat{Q}_{r+\lambda_{t}g+\tau\psi_{t}}^{\pi_{\theta_{ t}}}(s,a)\right)\phi_{s,a}\right).\]
8:endfor
9: Set \(\hat{w}_{t}=\frac{2}{K(K+1)}\sum_{k\,=\,0}^{K-1}(k+1)w_{t}^{k}\).
10: Estimate \(\hat{V}_{g}^{\pi_{\theta_{t}}}(\rho)\) using Algorithm 3 with policy \(\pi_{\theta_{t}}\).
11: Perform inexact RPG-PD update, \[\pi_{\theta_{t+1}}(\cdot\,|\,s) = \operatorname*{argmax}_{\pi(\cdot\,|\,s)\,\in\,\hat{\Delta}(A)} \left\{\sum_{a}\pi(a\,|\,s)\phi_{s,a}^{\top}\hat{w}_{t}\,-\,\frac{1}{\eta}\, \text{KL}(\pi(\cdot\,|\,s),\pi_{\theta_{t}}(\cdot\,|\,s))\right\}\] \[\lambda_{t+1} = \mathcal{P}_{\Lambda}\left(\,(1-\eta\tau)\lambda_{t}-\eta\hat{V} _{g}^{\pi_{\theta_{t}}}(\rho)\,\right).\]
12:endfor ```

**Algorithm 1** Sample-based inexact RPG-PD algorithm with log-linear policy parametrization

```
1:Input: Initial state-action distribution \(\nu\), policy \(\pi\), dual variable \(\lambda\), regularization parameter \(\tau\), discount factor \(\gamma\).
2: Sample \((s_{0},a_{0})\sim\nu\), execute the policy \(\pi\) with probability \(\gamma\) at each step \(h\); otherwise, accept \((s_{h},a_{h})\) as the sample.
3: Start with \((s_{h},a_{h})\), execute the policy \(\pi\) with the termination probability \(1-\sqrt{\gamma}\). Once terminated, add all composite values \(\gamma^{(k-h)/2}(r+\lambda g+\tau\psi)\) from step \(k=h+1\) onwards and \(-\tau\log\pi(a_{h}\,|\,s_{h})\) as \(\hat{Q}_{r+\lambda g+\tau\psi}^{\pi}(s_{h},a_{h})\).
4:Output:\((s_{h},a_{h})\) and \(\hat{Q}_{r+\lambda g+\tau\psi}^{\pi}(s_{h},a_{h})\). ```

**Algorithm 2** Unbiased estimate \(Q\)stepsize \(\eta\leq 1/C_{W}\), then the primal-dual iterates of sample-based inexact RPG-PD in Algorithm 1 satisfy_

\[\mathbb{E}[\,\Phi_{t+1}\,] \leq \mathrm{e}^{-\eta\tau t}\mathbb{E}[\,\Phi_{1}\,]\,+\,\frac{\eta}{ \tau}\max\big{(}\,(C_{W})^{2},(C^{\prime}_{\tau,\xi})^{2}\,\big{)}\,+\,\frac{C_ {W,\xi,\tau,\epsilon_{0}}}{\tau}\sqrt{\frac{|A|\kappa_{\nu}}{K+1}}\,+\,\frac{2 }{\tau}\sqrt{|A|\epsilon_{\mathrm{bias}}}\]

_where \(C_{W}:=2W/(1-\gamma)\), \(C_{W,\xi,\tau,\epsilon_{0}}:=8(W+2/(\xi(1-\sqrt{\gamma})^{2})+\tau(2\log|A|+| \log\epsilon_{0}|)/((1-\sqrt{\gamma})^{2}\xi))/\sqrt{\kappa_{0}}\), and \(C^{\prime}_{\tau,\xi}:=(1+\tau/\xi)/(1-\gamma)\)._

Proof.: The proof is based on the proof of Theorem 4. Additionally, we have to take the randomness of sample-based estimates into account and bound the statistical error \(\epsilon_{\text{stat}}\) using the projected SGD result [130]. We first check all conditions of the projected SGD [130] are indeed satisfied by the SGD step in Algorithm 1: (i) The domain \(\|w\|\leq W\) is convex and bounded; (ii) The gradient \(G_{t}^{k}\) is an unbiased estimate of the population gradient; (iii) The minimizer of \(\mathcal{E}_{Q}(w,\theta_{t},d_{t,\nu})\) is unique, since \(\Sigma_{d_{t,\nu}}\geq\kappa_{0}I\) for some \(\kappa_{0}>0\); (iv) The squared norm of the estimated gradient \(G_{t}^{k}\) is bounded or the gradient has bounded variance. To show (iv), it is sufficient to check that

\[\mathbb{E}\left[\,\left\|\phi_{s,a}^{\top}\phi_{s,a}w_{t}^{k} \right\|^{2}\,\right] \leq \mathbb{E}\left[\,\left\|\phi_{s,a}^{\top}\phi_{s,a}\right\|^{2} \,\right\|w_{t}^{k}\,\right\|^{2}\,\right] \leq W^{2}\] \[\mathbb{E}\left[\,\left\|\hat{Q}_{r+\lambda_{t}g+\tau\psi_{t}}^{ \pi_{\theta_{t}}}(s,a)\phi_{s,a}\right\|^{2}\,\right] \leq \mathbb{E}\left[\,\left(\hat{Q}_{r+\lambda_{t}g+\tau\psi_{t}}^{ \pi_{\theta_{t}}}(s,a)\right)^{2}\,\right]\] \[\leq \mathbb{E}\left[\,\left(\hat{Q}_{r+\lambda_{t}g+\tau\psi_{t}}^{ \pi_{\theta_{t}}}(s,a)\right)^{2}\,\right]\]

where we use the boundedness of \(\left\|w_{t}^{k}\right\|\leq W\) and \(\|\phi_{s,a}\|\leq 1\). We notice that \(|r+\lambda_{t}g|\leq\frac{2}{(1-\gamma)\xi}\). By [131, Lemma 3.5], we know that

\[\mathbb{E}\left[\,\left(\hat{Q}_{r+\lambda_{t}g+\tau\psi_{t}}^{ \pi_{\theta_{t}}}(s,a)\right)^{2}\,\right]\ \leq\ 4\left(\frac{2/((1-\gamma)\xi)}{1-\sqrt{\gamma}}\right)^{2}+4\left(\frac{ \tau\log|A|}{1-\sqrt{\gamma}}\right)^{2}+2\left(\tau\log\frac{\epsilon_{0}}{|A |}\right)^{2}\]

where the first two terms in the upper bound is due to the variance of soft-\(Q\) value function, and the last term is due to \(|\log\pi_{\theta_{t}}|\leq|\log\frac{\epsilon_{0}}{|A|}|\) for \(\pi_{\theta_{t}}\in\hat{\Delta}(A)\). Hence, the estimated gradient \(G_{t}^{k}\) has bounded second-order moment (or variance), which verifies (iv). From the projected SGD result [130], if the SGD stepsize \(\alpha_{k}=\frac{2}{k+2}\), then

\[\mathbb{E}\left[\,\mathcal{E}_{Q}(\hat{w}_{t},\theta_{t},d_{t, \nu})-\mathcal{E}_{Q}(w_{t}^{\star},\theta_{t},d_{t,\nu})\,\right] \leq \frac{16\left(W+\frac{2}{\xi(1-\sqrt{\gamma})^{2}}+\frac{\tau(2 \log|A|+|\log\epsilon_{0}|)}{(1-\sqrt{\gamma})^{2}\xi}\right)^{2}}{\kappa_{0} (K+1)}\]

which leads to \(\sqrt{\epsilon_{\text{stat}}}\leq\frac{4(W+2/(\xi(1-\sqrt{\gamma})^{2})+\tau( 2\log|A|+|\log\epsilon_{0}|)/((1-\sqrt{\gamma})^{2}\xi))}{\sqrt{\kappa_{0}(K +1)}}\). Substituting the bound of \(\sqrt{\epsilon_{\text{stat}}}\) into the proof of Theorem 4 yields our desired result. 

Corollary 18 states a similar result as Theorem 4. The effect of using sample-based estimates to update inexact RPG-PD appears as the number \(K\) of SGD steps at each time \(t\). Thus, we can interpret the iteration complexity in Corollary 5 in terms of the number of SGD steps by taking \(\epsilon_{\text{stat}}=O(\epsilon^{8})\), i.e., \(K=\Omega(\frac{1}{\epsilon^{8}})\). Thus, whenever \(\epsilon_{\text{bias}}=O(\epsilon^{8})\) for small \(\epsilon>0\), if we take \((\eta,\tau)\) in Corollary 5 and \(K=\Omega(\frac{1}{\epsilon^{8}})\) for Algorithm 1, then,

\[\mathbb{E}\left[\,V_{r}^{\pi^{\star}}(\rho)-V_{r}^{\pi_{\theta_{t}}}(\rho)\, \right]\ \leq\ \epsilon\ \ \text{and}\ \ \mathbb{E}\left[\,-V_{g}^{\pi_{\theta_{t}}}(\rho)\,\right]\ \leq\ \epsilon\ \ \text{for any}\ t=\Omega\left(\,\frac{1}{\epsilon^{6}}\log^{2}\frac{1}{\epsilon}\,\right)\]

where \(\Omega(\cdot)\) only has some problem-dependent constant. In other words, the total number of policy rollouts or sampled trajectories \(tK=\Omega\left(\frac{1}{\epsilon^{1}}\right)\) is required for Algorithm 1 to output an \(\epsilon\)-optimal constrained policy. Furthermore, the zero constraint violation in Corollary 17 can be interpreted similarly. When \(\epsilon\) is small enough, we can design a conservative constraint such that the policy iterates of Algorithm 1 satisfy \(V_{r}^{\pi^{\star}}(\rho)-V_{r}^{\pi_{t}}(\rho)\leq\epsilon\) and \(V_{g}^{\pi_{\theta_{t}}}(\rho)\geq 0\) for some \(t\), \(K\) that satisfy \(tK=\Omega\left(\frac{1}{\epsilon^{1}\epsilon^{4}}\right)\). This appears to be the first sample-based zero constraint violation result for constrained MDPs in the function approximation setting. We leave achieving the optimal sample complexity as future work.

Proofs in Section 4

In this section, we provide proofs for the claims in Section 4.

### Preliminary last-iterate analysis of OPG-PD (9)

To measure the proximity of the primal-dual iterates of OPG-PD (9) to the optimal pair \((\pi^{\star},\lambda^{\star})\), we introduce the following two distance metrics \(\Theta_{t}\) and \(\zeta_{t}\) at time \(t\geq 1\),

\[\Theta_{t} := \frac{1}{2(1-\gamma)}\sum_{s}d_{\rho}^{\pi^{\star}}(s)\left\|\hat{ \pi}_{t}(\cdot\,|\,s)-\mathcal{P}_{\Pi^{\star}}(\hat{\pi}_{t}(\cdot\,|\,s)) \right\|^{2}\,+\,\frac{1}{2}(\hat{\lambda}_{t}-\mathcal{P}_{\Pi^{\star}}(\hat{ \lambda}_{t}))^{2}\] \[+\,\frac{1}{16(1-\gamma)}\sum_{s}d_{\rho}^{\pi^{\star}}(s)\left\| \hat{\pi}_{t}(\cdot\,|\,s)-\pi_{t-1}(\cdot\,|\,s)\right\|^{2}\,+\,\frac{1}{16}( \hat{\lambda}_{t}-\lambda_{t-1})^{2}\] \[\zeta_{t} := \frac{1}{2(1-\gamma)}\sum_{s}d_{\rho}^{\pi^{\star}}(s)\left\| \hat{\pi}_{t+1}(\cdot|s)-\pi_{t}(\cdot|s)\right\|^{2}\,+\,\frac{1}{2}(\hat{ \lambda}_{t+1}-\lambda_{t})^{2}\] \[+\,\frac{1}{2(1-\gamma)}\sum_{s}d_{\rho}^{\pi^{\star}}(s)\left\| \pi_{t}(\cdot|s)-\hat{\pi}_{t}(\cdot|s)\right\|^{2}\,+\,\frac{1}{2}(\lambda_{t }-\hat{\lambda}_{t})^{2}\]

and a problem-dependent constant \(\iota\),

\[\iota := \max\left(\,\frac{2\kappa_{\rho}^{2}|A|}{(1-\gamma)^{6}},\;\frac {8\gamma^{2}\sqrt{|A|}\kappa_{\rho}}{(1-\gamma)^{6}}\left(1+\frac{1}{(1- \gamma)^{2}\xi^{2}}\right),\;\frac{4}{(1-\gamma)^{3}}\,\right).\] (28)

**Lemma 19**.: _Let the optimal state visitation distribution be unique, i.e., \(d_{\rho}^{\pi}=d_{\rho}^{\pi^{\star}}\) for any \(\pi\in\Pi^{\star}\). If we set the stepsize \(\eta\leq 1/(4\sqrt{\iota})\), then the primal-dual iterates of OPG-PD (9) satisfy_

\[\Theta_{t+1} \leq \Theta_{t}\,-\,\frac{7}{16}\,\zeta_{t}\quad\text{for all $t\geq 1$}.\] (29)

Proof.: We begin with the standard decomposition of the primal-dual gap at time \(t\geq 1\),

\[V_{r+\lambda_{t}g}^{\pi^{\star}}(\rho)-V_{r+\lambda^{\star}g}^{ \pi_{t}}(\rho) = \underbrace{V_{r+\lambda_{t}g}^{\pi^{\star}}(\rho)-V_{r+\lambda_{ t}g}^{\pi_{t}}(\rho)}_{\text{(i)}}+\underbrace{V_{r+\lambda_{t}g}^{\pi_{t}}(\rho)-V_{r+ \lambda^{\star}g}^{\pi_{t}}(\rho)}_{\text{(ii)}}\] (30)

and we next deal with (i) and (ii), separately.

For the first term (i),

\[V_{r+\lambda_{t}g}^{\pi^{\star}}(\rho)-V_{r+\lambda_{t}g}^{\pi_{ t}}(\rho)\] \[= \frac{1}{1-\gamma}\sum_{s,a}d_{\rho}^{\pi^{\star}}(s)(\pi^{\star}( a\,|\,s)-\pi_{t}(a\,|\,s))Q_{r+\lambda_{t}g}^{\pi_{t}}(s,a)\] \[= \frac{1}{1-\gamma}\sum_{s,a}d_{\rho}^{\pi^{\star}}(s)(\pi^{\star} (a\,|\,s)-\hat{\pi}_{t+1}(a\,|\,s))Q_{r+\lambda_{t}g}^{\pi_{t}}(s,a)\] \[+\,\frac{1}{1-\gamma}\sum_{s,a}d_{\rho}^{\pi^{\star}}(s)(\hat{\pi} _{t+1}(a\,|\,s)-\pi_{t}(a\,|\,s))Q_{r+\lambda_{t-1}g}^{\pi_{t-1}}(s,a)\] \[+\,\frac{1}{1-\gamma}\sum_{s,a}d_{\rho}^{\pi^{\star}}(s)(\hat{\pi} _{t+1}(a\,|\,s)-\pi_{t}(a\,|\,s))\left(Q_{r+\lambda_{t}g}^{\pi_{t}}(s,a)-Q_{r+ \lambda_{t-1}g}^{\pi_{t-1}}(s,a)\right)\] \[\leq \frac{1}{2\eta(1-\gamma)}\sum_{s}d_{\rho}^{\pi^{\star}}(s)\left( \left\|\pi^{\star}(\cdot\,|\,s)-\hat{\pi}_{t}(\cdot\,|\,s)\right\|^{2}-\left\| \pi^{\star}(\cdot\,|\,s)-\hat{\pi}_{t+1}(\cdot\,|\,s)\right\|^{2}-\left\|\hat{ \pi}_{t+1}(\cdot\,|\,s)-\hat{\pi}_{t}(\cdot\,|\,s)\right\|^{2}\right)\] \[+\,\frac{1}{1-\gamma}\sum_{s}d_{\rho}^{\pi^{\star}}(s)(\hat{\pi} _{t+1}(a\,|\,s)-\pi_{t}(a\,|\,s))\left(Q_{r+\lambda_{t}g}^{\pi_{t}}(s,a)-Q_{r+ \lambda_{t-1}g}^{\pi_{t-1}}(s,a)\right)\]where the first equality is due to performance difference lemma in Lemma 28, the second equality is because of some re-arrangement, the inequality is from an application of Lemma 24 to \(\hat{\pi}_{t+1}(\cdot\,|\,s)\) and \(\pi_{t}(\cdot\,|\,s)\):

\[\sum_{a}(\pi^{\star}(a\,|\,s)-\hat{\pi}_{t+1}(a\,|\,s))Q_{r+\lambda _{t}g}^{\pi_{t}}(s,a)\] \[\leq \frac{1}{2\eta}\left(\|\pi^{\star}(\cdot|s)-\hat{\pi}_{t}(\cdot|s )\|^{2}-\|\pi^{\star}(\cdot|s)-\hat{\pi}_{t+1}(\cdot|s)\|^{2}-\|\hat{\pi}_{t+1} (\cdot|s)-\hat{\pi}_{t}(\cdot|s)\|^{2}\right)\] \[\sum_{a}(\hat{\pi}_{t+1}(a\,|\,s)-\pi_{t}(a\,|\,s))Q_{r+\lambda_{ t-1}g}^{\pi_{t-1}}(s,a)\] \[\leq \frac{1}{2\eta}\left(\|\hat{\pi}_{t+1}(\cdot|s)-\hat{\pi}_{t}( \cdot|s)\|^{2}-\|\hat{\pi}_{t+1}(\cdot|s)-\pi_{t}(\cdot|s)\|^{2}-\|\pi_{t}( \cdot|s)-\hat{\pi}_{t}(\cdot|s)\|^{2}\right).\]

Similarly, we deal with the second term (ii) with some re-arrangement, and apply Lemma 24 to \(\hat{\lambda}_{t+1}\) and \(\lambda_{t}\),

\[V_{r+\lambda_{t}g}^{\pi_{t}}(\rho)-V_{r+\lambda^{\star}g}^{\pi_ {t}}(\rho) = (\lambda_{t}-\lambda^{\star})V_{g}^{\pi_{t}}(\rho)\] \[= (\lambda_{t}-\hat{\lambda}_{t+1})V_{g}^{\pi_{t-1}}(\rho)\,+\,( \lambda_{t}-\hat{\lambda}_{t+1})\left(V_{g}^{\pi_{t}}(\rho)-V_{g}^{\pi_{t-1}}( \rho)\right)\] \[+(\hat{\lambda}_{t+1}-\lambda^{\star})V_{g}^{\pi_{t}}(\rho)\] \[\leq \frac{1}{2\eta}\left((\hat{\lambda}_{t+1}-\hat{\lambda}_{t})^{2} -(\hat{\lambda}_{t+1}-\lambda_{t})^{2}-(\lambda_{t}-\hat{\lambda}_{t})^{2}\right)\] \[+(\lambda_{t}-\hat{\lambda}_{t+1})\left(V_{g}^{\pi_{t}}(\rho)-V_ {g}^{\pi_{t-1}}(\rho)\right)\] \[+\frac{1}{2\eta}\left((\lambda^{\star}-\hat{\lambda}_{t})^{2}-( \lambda^{\star}-\hat{\lambda}_{t+1})^{2}-(\hat{\lambda}_{t+1}-\hat{\lambda}_{t })^{2}\right).\]

On the other hand, from Lemma 25, we have

\[\|\hat{\pi}_{t+1}(\cdot\,|\,s)-\pi_{t}(\cdot\,|\,s)\|_{1} \leq \eta\left\|Q_{r+\lambda_{t}g}^{\pi_{t}}(s,\cdot)-Q_{r+\lambda_{t- 1}g}^{\pi_{t-1}}(s,\cdot)\right\|_{\infty}\] (33)

and

\[|\lambda_{t}-\hat{\lambda}_{t+1}| \leq \eta|V_{g}^{\pi_{t}}(\rho)-V_{g}^{\pi_{t-1}}(\rho)|.\] (34)

Hence,

\[\sum_{s,a}d_{\rho}^{\pi^{\star}}(s)(\hat{\pi}_{t+1}(a\,|\,s)-\pi _{t}(a\,|\,s))\left(Q_{r+\lambda_{t}g}^{\pi_{t}}(s,a)-Q_{r+\lambda_{t-1}g}^{ \pi_{t-1}}(s,a)\right)\] \[\leq \eta\sum_{s}d_{\rho}^{\pi^{\star}}(s)\left\|Q_{r+\lambda_{t}g}^{ \pi_{t}}(s,\cdot)-Q_{r+\lambda_{t-1}g}^{\pi_{t-1}}(s,\cdot)\right\|_{\infty}^{2}\] \[\leq 2\eta\sum_{s}d_{\rho}^{\pi^{\star}}(s)\left(\left\|(\lambda_{t} -\lambda_{t-1})Q_{g}^{\pi_{t}}(s,\cdot)\right\|_{\infty}^{2}+\left\|Q_{r+ \lambda_{t-1}g}^{\pi_{t}}(s,\cdot)-Q_{r+\lambda_{t-1}g}^{\pi_{t-1}}(s,\cdot) \right\|_{\infty}^{2}\right)\] \[\leq \frac{2\eta}{(1-\gamma)^{2}}(\lambda_{t}-\lambda_{t-1})^{2}\,+\, \frac{4\eta\gamma^{2}}{(1-\gamma)^{4}}\left(1+\frac{1}{(1-\gamma)^{2}\xi^{2}} \right)\max_{s}\left\|\pi_{t}(\cdot\,|\,s)-\pi_{t-1}(\cdot\,|\,s)\right\|_{1} ^{2}\] \[\leq \frac{2\eta}{(1-\gamma)^{2}}(\lambda_{t}-\lambda_{t-1})^{2}\,+\, \frac{4\eta\gamma^{2}\kappa_{\rho}}{(1-\gamma)^{5}}\left(1+\frac{1}{(1-\gamma)^ {2}\xi^{2}}\right)\sum_{s}d_{\rho}^{\pi^{\star}}(s)\left\|\pi_{t}(\cdot\,|\,s )-\pi_{t-1}(\cdot\,|\,s)\right\|_{1}^{2}\] \[\leq \frac{4\eta}{(1-\gamma)^{2}}\left((\lambda_{t}-\hat{\lambda}_{t}) ^{2}+(\hat{\lambda}_{t}-\lambda_{t-1})^{2}\right)\] \[+\,\frac{8\eta\gamma^{2}\sqrt{|A|}\kappa_{\rho}}{(1-\gamma)^{5}} \left(1+\frac{1}{(1-\gamma)^{2}\xi^{2}}\right)\sum_{s}d_{\rho}^{\pi^{\star}}(s) \left(\left\|\pi_{t}(\cdot\,|\,s)-\hat{\pi}_{t}(\cdot\,|\,s)\right\|^{2}+\| \hat{\pi}_{t}(\cdot\,|\,s)-\pi_{t-1}(\cdot\,|\,s)\|^{2}\right)\]

where the first inequality is due to Cauchy-Schwarz inequality and (33), we subtract and add \(Q_{r+\lambda_{t-1}g}^{\pi_{t}}\) and apply the inequality \((x+y)^{2}\leq 2x^{2}+2y^{2}\) in the second inequality, the third inequality is due to Lemma 22 and the boundedness of value functions and dual variables, the fourth inequality comes from the property of \(\kappa_{\rho}\), and the last inequality is due to the inequality \((x+y)^{2}\leq 2x^{2}+2y^{2}\) and the inequality \(\left\|p-p^{\prime}\right\|_{1}\leq\sqrt{|A|}\left\|p-p^{\prime}\right\|_{2}\) for two probability distributions \(p\) and \(p^{\prime}\). Meanwhile, using a similar reasoning, we can derive that

\[(\lambda_{t}-\hat{\lambda}_{t+1})\left(V_{g}^{\pi_{t}}(\rho)-V_{g }^{\pi_{t-1}}(\rho)\right)\] \[\leq \eta\left(V_{g}^{\pi_{t}}(\rho)-V_{g}^{\pi_{t-1}}(\rho)\right)^{2}\] \[\leq \frac{\eta\kappa_{\rho}^{2}}{(1-\gamma)^{6}}\left(\sum_{s}d_{ \rho}^{\pi^{*}}(s)\left\|\pi_{t}(\cdot\,|\,s)-\pi_{t-1}(\cdot\,|\,s)\right\|_{ 1}\right)^{2}\] \[\leq \frac{\eta\kappa_{\rho}^{2}}{(1-\gamma)^{6}}\sum_{s}\left(\sqrt {d_{\rho}^{\pi^{*}}(s)}\left\|\pi_{t}(\cdot\,|\,s)-\pi_{t-1}(\cdot\,|\,s) \right\|_{1}\right)^{2}\] \[\leq \frac{2\eta\kappa_{\rho}^{2}|A|}{(1-\gamma)^{6}}\sum_{s}d_{\rho }^{\pi^{*}}(s)\left(\left\|\pi_{t}(\cdot\,|\,s)-\hat{\pi}_{t}(\cdot\,|\,s) \right\|^{2}+\left\|\hat{\pi}_{t}(\cdot\,|\,s)-\pi_{t-1}(\cdot\,|\,s)\right\| ^{2}\right)\]

where the first inequality is due to Cauchy-Schwarz inequality and (34), the second inequality is due to Lemma 22, the third inequality is an application of Cauchy-Schwarz inequality, and the last inequality is due to the inequality \((x+y)^{2}\leq 2x^{2}+2y^{2}\) and the inequality \(\left\|p-p^{\prime}\right\|_{1}\leq\sqrt{|A|}\left\|p-p^{\prime}\right\|_{2}\) for two probability distributions \(p\) and \(p^{\prime}\).

We set notation,

\[\iota := \max\left(\,\frac{2\kappa_{\rho}^{2}|A|}{(1-\gamma)^{6}}\,,\,\frac {8\gamma^{2}\sqrt{|A|}\kappa_{\rho}}{(1-\gamma)^{6}}\left(1+\frac{1}{(1- \gamma)^{2}\xi^{2}}\right),\,\,\frac{4}{(1-\gamma)^{3}}\,\right).\]

After applying the established inequalities above to (31) and (32), we combine them into (30) as,

\[V_{r+\lambda_{t}g}^{\pi^{*}}(\rho)-V_{r+\lambda^{*}g}^{\pi_{t}}(\rho)\] \[\leq \frac{1}{2\eta(1-\gamma)}\sum_{s}d_{\rho}^{\pi^{*}}(s)\left(\| \pi^{*}(\cdot\,|\,s)-\hat{\pi}_{t}(\cdot\,|\,s)\|^{2}-\|\pi^{*}(\cdot\,|\,s)- \hat{\pi}_{t+1}(\cdot\,|\,s)\|^{2}-\|\hat{\pi}_{t+1}(\cdot\,|\,s)-\hat{\pi}_{t }(\cdot\,|\,s)\|^{2}\right)\] \[+\frac{1}{2\eta(1-\gamma)}\sum_{s}d_{\rho}^{\pi^{*}}(s)\left(\| \hat{\pi}_{t+1}(\cdot\,|\,s)-\hat{\pi}_{t}(\cdot\,|\,s)\|^{2}-\|\hat{\pi}_{t+ 1}(\cdot\,|\,s)-\pi_{t}(\cdot\,|\,s)\|^{2}-\|\pi_{t}(\cdot\,|\,s)-\hat{\pi}_{t }(\cdot\,|\,s)\|^{2}\right)\] \[+\frac{1}{2\eta}\left((\hat{\lambda}_{t+1}-\hat{\lambda}_{t})^{2} -(\hat{\lambda}_{t+1}-\lambda_{t})^{2}-(\lambda_{t}-\hat{\lambda}_{t})^{2}\right)\] \[+\frac{1}{2\eta}\left((\lambda^{\star}-\hat{\lambda}_{t})^{2}-( \lambda^{\star}-\hat{\lambda}_{t+1})^{2}-(\hat{\lambda}_{t+1}-\hat{\lambda}_{t })^{2}\right)\] \[+\eta\iota(\lambda_{t}-\hat{\lambda}_{t})^{2}+\eta\iota(\hat{ \lambda}_{t}-\lambda_{t-1})^{2}\] \[+\eta\iota\sum_{s}d_{\rho}^{\pi^{*}}(s)\left(\left\|\pi_{t}( \cdot\,|\,s)-\hat{\pi}_{t}(\cdot\,|\,s)\right\|^{2}+\left\|\hat{\pi}_{t}(\cdot \,|\,s)-\pi_{t-1}(\cdot\,|\,s)\right\|^{2}\right).\]

We notice that \(V_{r+\lambda_{t}g}^{\pi^{*}}(\rho)-V_{r+\lambda^{*}g}^{\pi_{t}}(\rho)\geq 0\) and non-expansiveness of projection operators \(\mathcal{P}_{\Pi^{*}}\), \(\mathcal{P}_{\Lambda^{*}}\),

\[\|\mathcal{P}_{\Pi^{*}}(\hat{\pi}_{t+1}(\cdot\,|\,s))-\hat{\pi}_{t +1}(\cdot\,|\,s)\| \leq \|\pi^{*}(\cdot\,|\,s)-\hat{\pi}_{t+1}(\cdot\,|\,s)\|\] \[\left\|\mathcal{P}_{\Lambda^{*}}(\hat{\lambda}_{t+1})-\hat{\lambda}_ {t+1}\right\| \leq \left\|\lambda^{\star}-\hat{\lambda}_{t+1}\right\|\]for any \(\pi^{\star}\in\Pi^{\star}\) and \(\lambda^{\star}\in\Lambda^{\star}\). If we take \(\pi^{\star}=\mathcal{P}_{\Pi^{\star}}(\hat{\pi}_{t}(\cdot\,|\,s))\) and \(\lambda^{\star}=\mathcal{P}_{\Lambda^{\star}}(\hat{\lambda}_{t})\), then after some re-arrangement, we have

\[\frac{1}{2(1-\gamma)}\sum_{s}d_{\rho}^{\pi^{\star}}(s)\,\| \mathcal{P}_{\Pi^{\star}}(\hat{\pi}_{t+1}(\cdot\,|\,s))-\hat{\pi}_{t+1}(\cdot \,|\,s)\|^{2}\,+\,\frac{1}{2}(\mathcal{P}_{\Lambda^{\star}}(\hat{\lambda}_{t+1 })-\hat{\lambda}_{t+1})^{2}\] (35a) \[\leq \frac{1}{2(1-\gamma)}\sum_{s}d_{\rho}^{\pi^{\star}}(s)\,\| \mathcal{P}_{\Pi^{\star}}(\hat{\pi}_{t}(\cdot\,|\,s))-\hat{\pi}_{t+1}(\cdot \,|\,s)\|^{2}\,+\,\frac{1}{2}(\mathcal{P}_{\Lambda^{\star}}(\hat{\lambda}_{t} )-\hat{\lambda}_{t+1})^{2}\] \[\leq \frac{1}{2(1-\gamma)}\sum_{s}d_{\rho}^{\pi^{\star}}(s)\,\| \mathcal{P}_{\Pi^{\star}}(\hat{\pi}_{t}(\cdot\,|\,s))-\hat{\pi}_{t}(\cdot|s) \|^{2}\,+\,\frac{1}{2}(\mathcal{P}_{\Lambda^{\star}}(\hat{\lambda}_{t})-\hat{ \lambda}_{t})^{2}\] \[-\,\frac{1}{2(1-\gamma)}\sum_{s}d_{\rho}^{\pi^{\star}}(s)\,\| \hat{\pi}_{t+1}(\cdot\,|\,s)-\pi_{t}(\cdot\,|\,s)\|^{2}\,-\,\frac{1}{2}(\hat{ \lambda}_{t+1}-\lambda_{t})^{2}\] \[-\,\left(\frac{1}{2(1-\gamma)}-\eta^{2}\iota\right)\sum_{s}d_{ \rho}^{\pi^{\star}}(s)\,\|\pi_{t}(\cdot\,|\,s)-\hat{\pi}_{t}(\cdot\,|\,s)\|^{2 }\,-\,\left(\frac{1}{2}-\eta^{2}\iota\right)(\lambda_{t}-\hat{\lambda}_{t})^{2}\] \[+\eta^{2}\iota\sum_{s}d_{\rho}^{\pi^{\star}}(s)\,\|\hat{\pi}_{t}( \cdot\,|\,s)-\pi_{t-1}(\cdot\,|\,s)\|^{2}\,+\,\eta^{2}\iota(\hat{\lambda}_{t} -\lambda_{t-1})^{2}\]

where we also use the assumption \(d_{\rho}^{\pi^{\star}}=d_{\rho}^{\pi}\) for any \(\pi\in\Pi^{\star}\). By taking \(\eta^{2}\iota\leq\frac{1}{16}\) and using notation \(\Theta_{t}\) and \(\zeta_{t}\), we have \(\Theta_{t+1}\leq\Theta_{t}-\frac{1}{16}\zeta_{t}\). 

To show the convergence, we next relate \(\zeta_{t}\) with \(\Theta_{t+1}\). An intermediate step is to show that \(\zeta_{t}\) has the following lower bound.

**Lemma 20**.: _Let \(\kappa_{\rho,\gamma}:=\max(\frac{\kappa_{\rho}}{1-\gamma},1)\). If we set the stepsize \(\eta\leq\min(\frac{(1-\gamma)^{3}}{4|A|},\frac{(1-\gamma)^{3}}{2\kappa_{\rho}})\), then the primal-dual iterates of OPG-PD (9) satisfy_

\[\sum_{s}d_{\rho}^{\pi^{\star}}(s)\left(\|\pi_{t}(\cdot\,|\,s)- \hat{\pi}_{t}(\cdot\,|\,s)\|^{2}\,+\,\|\hat{\pi}_{t+1}(\cdot\,|\,s)-\pi_{t}( \cdot\,|\,s)\|^{2}\right)+\left(|\lambda_{t}-\hat{\lambda}_{t}|^{2}+|\lambda_{ t}-\hat{\lambda}_{t+1}|^{2}\right)\] (35b) \[\geq \frac{\eta^{2}}{9\kappa_{\rho,\gamma}^{2}}\frac{\left[V_{r+\hat{ \lambda}_{t+1}g}^{\pi}(\rho)-V_{r+\lambda g}^{\hat{\pi}_{t+1}}(\rho)\right]_{+ }^{2}}{\left(\max_{s}\|\pi(\cdot\,|\,s)-\hat{\pi}_{t+1}(\cdot\,|\,s)\|+|\lambda -\hat{\lambda}_{t+1}|\right)^{2}}\ \ \text{for all }(\pi,\lambda)\neq(\hat{\pi}_{t+1},\hat{ \lambda}_{t+1}).\]

Proof.: From the optimality of \(\hat{\pi}_{t+1}(\cdot\,|\,s)\) in OPG-PD, we know that for any \(\pi\in\Pi\),

\[\langle\hat{\pi}_{t+1}(\cdot\,|\,s)-\hat{\pi}_{t}(\cdot\,|\,s),\pi( \cdot\,|\,s)-\hat{\pi}_{t+1}(\cdot\,|\,s)\rangle\] (35c) \[\geq \eta\langle Q_{r+\lambda_{t}g}^{\pi_{t}}(s,\cdot),\pi(\cdot\,|\,s )-\hat{\pi}_{t+1}(\cdot\,|\,s)\rangle\] \[= \eta\langle Q_{r+\hat{\lambda}_{t}g}^{\hat{\pi}_{t+1}}(s,\cdot), \pi(\cdot\,|\,s)-\hat{\pi}_{t+1}(\cdot\,|\,s)\rangle\,+\,\eta\langle Q_{r+ \lambda_{t}g}^{\pi_{t}}(s,\cdot)-Q_{r+\lambda_{t}g}^{\hat{\pi}_{t+1}}(s,\cdot), \pi(\cdot\,|\,s)-\hat{\pi}_{t+1}(\cdot\,|\,s)\rangle\] \[+\,\eta\langle Q_{r+\lambda_{t}g}^{\hat{\pi}_{t+1}}(s,\cdot)-Q_{r+ \lambda_{t+1}g}^{\hat{\pi}_{t+1}}(s,\cdot),\pi(\cdot\,|\,s)-\hat{\pi}_{t+1}( \cdot\,|\,s)\rangle.\]

Similarly, from the optimality of \(\pi_{t+1}(\cdot\,|\,s)\) in OPG-PD, we know that for any \(\pi\in\Pi\),

\[\langle\pi_{t+1}(\cdot\,|\,s)-\hat{\pi}_{t+1}(\cdot\,|\,s),\pi( \cdot\,|\,s)-\pi_{t+1}(\cdot\,|\,s)\rangle\] (35d) \[\geq \eta\langle Q_{r+\lambda_{t}g}^{\pi_{t}}(s,\cdot),\pi(\cdot\,|\,s)- \pi_{t+1}(\cdot\,|\,s)\rangle\] \[= \eta\langle Q_{r+\lambda_{t+1}g}^{\pi_{t+1}}(s,\cdot),\pi(\cdot\,| \,s)-\pi_{t+1}(\cdot\,|\,s)\rangle\,+\,\eta\langle Q_{r+\lambda_{t}g}^{\pi_{t}}(s, \cdot)-Q_{r+\lambda_{t}g}^{\pi_{t+1}}(s,\cdot),\pi(\cdot\,|\,s)-\pi_{t+1}(\cdot\,| \,s)\rangle\] \[+\,\eta\langle Q_{r+\lambda_{t}g}^{\pi_{t+1}}(s,\cdot)-Q_{r+ \lambda_{t+1}g}^{\pi_{t+1}}(s,\cdot),\pi(\cdot\,|\,s)-\pi_{t+1}(\cdot\,|\,s)\rangle.\]On the other hand, from the optimality of \(\hat{\lambda}_{t+1}\) in OPG-PD, we know that for any \(\lambda\in\Lambda\),

\[(\hat{\lambda}_{t+1}-\hat{\lambda}_{t})(\lambda-\hat{\lambda}_{t+1}) \geq \eta V_{g}^{\pi_{t}}(\rho)(\hat{\lambda}_{t+1}-\lambda)\] (36a) \[= \eta V_{g}^{\hat{\pi}_{t+1}}(\rho)(\hat{\lambda}_{t+1}-\lambda)\,+ \,\eta(V_{g}^{\pi_{t}}(\rho)-V_{g}^{\hat{\pi}_{t+1}}(\rho))(\hat{\lambda}_{t+1 }-\lambda)\] and the optimality of \[\lambda_{t+1}\] in OPG-PD yields, \[(\lambda_{t+1}-\hat{\lambda}_{t})(\lambda-\lambda_{t+1}) \geq \eta V_{g}^{\pi_{t}}(\rho)(\lambda_{t+1}-\lambda)\] \[= \eta V_{g}^{\pi_{t+1}}(\rho)(\lambda_{t+1}-\lambda)\,+\,\eta(V_{ g}^{\pi_{t}}(\rho)-V_{g}^{\pi_{t+1}}(\rho))(\lambda_{t+1}-\lambda).\] (36b)

First, we take the expectation of (35a) over some state distribution \(d_{\rho}^{\pi}\) on both sides and add it to (36a),

\[\frac{1}{1-\gamma}\sum_{s}d_{\rho}^{\pi}(s)\langle\hat{\pi}_{t+1 }(\cdot\,|\,s)-\hat{\pi}_{t}(\cdot\,|\,s),\pi(\cdot\,|\,s)-\hat{\pi}_{t+1}( \cdot\,|\,s)\rangle\,+\,(\hat{\lambda}_{t+1}-\hat{\lambda}_{t})(\lambda-\hat{ \lambda}_{t+1})\] (37) \[\geq \frac{\eta}{1-\gamma}\sum_{s}d_{\rho}^{\pi}(s)\langle Q_{r+\hat{ \lambda}_{t+1}g}^{\hat{\pi}_{t+1}}(s,\cdot),\pi(\cdot\,|\,s)-\hat{\pi}_{t+1}( \cdot\,|\,s)\rangle\] \[+\frac{\eta}{1-\gamma}\sum_{s}d_{\rho}^{\pi}(s)\langle Q_{r+\hat{ \lambda}_{t}g}^{\hat{\pi}_{t+1}}(s,\cdot)-Q_{r+\hat{\lambda}_{t+1}g}^{\hat{\pi} _{t+1}}(s,\cdot),\pi(\cdot\,|\,s)-\hat{\pi}_{t+1}(\cdot\,|\,s)\rangle\] \[+\,\eta V_{g}^{\hat{\pi}_{t+1}}(\rho)(\hat{\lambda}_{t+1}-\lambda )\,+\,\eta(V_{g}^{\pi_{t}}(\rho)-V_{g}^{\hat{\pi}_{t+1}}(\rho))(\hat{\lambda} _{t+1}-\lambda)\] \[\geq \eta\left(V_{r+\hat{\lambda}_{t+1}g}^{\pi}(\rho)-V_{r+\hat{ \lambda}_{t+1}g}^{\hat{\pi}_{t+1}}(\rho)\right)\] \[-\,\frac{\eta}{1-\gamma}\sum_{s}d_{\rho}^{\pi}(s)\left\|Q_{r+ \lambda_{t}g}^{\pi_{t}}(s,\cdot)-Q_{r+\lambda_{t}g}^{\hat{\pi}_{t+1}}(s,\cdot )\right\|_{\infty}\left\|\pi(\cdot\,|\,s)-\hat{\pi}_{t+1}(\cdot\,|\,s)\right\|_ {1}\] \[-\,\frac{\eta}{1-\gamma}\sum_{s}d_{\rho}^{\pi}(s)\left\|Q_{r+ \lambda_{t}g}^{\hat{\pi}_{t+1}}(s,\cdot)-Q_{r+\hat{\lambda}_{t+1}g}^{\hat{ \pi}_{t+1}}(s,\cdot)\right\|\left\|\pi(\cdot\,|\,s)-\hat{\pi}_{t+1}(\cdot\,|\, s)\right\|\] \[+\,\eta V_{g}^{\hat{\pi}_{t+1}}(\rho)(\hat{\lambda}_{t+1}-\lambda )\,-\,\eta|V_{g}^{\pi_{t}}(\rho)-V_{g}^{\hat{\pi}_{t+1}}(\rho)||\hat{\lambda} _{t+1}-\lambda|\] \[\geq \eta\left(V_{r+\hat{\lambda}_{t+1}g}^{\pi}(\rho)-V_{r+\hat{ \lambda}_{t+1}g}^{\hat{\pi}_{t+1}}(\rho)\right)\,+\,\eta V_{g}^{\hat{\pi}_{t+1 }}(\rho)(\hat{\lambda}_{t+1}-\lambda)\] \[-\,\frac{\eta\gamma|A|}{(1-\gamma)^{3}}\max_{s}\left\|\pi_{t}( \cdot\,|\,s)-\hat{\pi}_{t+1}(\cdot\,|\,s)\right\|\sum_{s}d_{\rho}^{\pi}(s) \left\|\pi(\cdot\,|\,s)-\hat{\pi}_{t+1}(\cdot\,|\,s)\right\|\] \[-\,\frac{\eta}{(1-\gamma)^{2}}\sum_{s}d_{\rho}^{\pi}(s)|\lambda_{t }-\hat{\lambda}_{t+1}|\left\|\pi(\cdot\,|\,s)-\hat{\pi}_{t+1}(\cdot\,|\,s)\right\|\] \[-\,\frac{\eta\kappa_{\rho}\sqrt{|A|}}{(1-\gamma)^{3}}\sum_{s}d_{ \rho}^{\pi^{*}}(s)\left\|\pi_{t}(\cdot\,|\,s)-\hat{\pi}_{t+1}(\cdot\,|\,s)\right\| \left|\hat{\lambda}_{t+1}-\lambda\right|\]

where the second inequality is due to performance difference lemma in Lemma 28 and Cauchy-Schwarz inequality, and the last inequality results from Lemma 22 and the inequality \(\left\|p-p^{\prime}\right\|_{1}\leq\sqrt{|A|}\left\|p-p^{\prime}\right\|_{2}\) for two probability distributions \(p\) and \(p^{\prime}\). By using the inequality \((a+b)(c+d)\) for \(a\geq 0\), \(b\geq 0\), \(c\geq 0\), and \(d\geq 0\), with some re-arrangement, we have

\[\left(\max_{s}\|\pi(\cdot\,|\,s)-\hat{\pi}_{t+1}(\cdot\,|\,s)\|+| \lambda-\hat{\lambda}_{t+1}|\right)\times\] \[\quad\left[\frac{\kappa_{\rho}}{1-\gamma}\sum_{s}d_{\rho}^{\pi^{ *}}(s)\left(\frac{1}{1-\gamma}\,\|\hat{\pi}_{t+1}(\cdot\,|\,s)-\hat{\pi}_{t}( \cdot\,|\,s)\|+\frac{2\eta|A|}{(1-\gamma)^{3}}\,\|\hat{\pi}_{t+1}(\cdot\,|\,s) -\pi_{t}(\cdot\,|\,s)\|\right)\right.\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\left.+\,|\hat{ \lambda}_{t+1}-\hat{\lambda}_{t}|\,+\,\frac{\eta\kappa_{\rho}}{(1-\gamma)^{3} }|\lambda_{t}-\hat{\lambda}_{t+1}|\right]\] \[\geq \max_{s}\|\pi(\cdot\,|\,s)-\hat{\pi}_{t+1}(\cdot\,|\,s)\|\sum_{s }d_{\rho}^{\pi^{*}}(s)\times\] \[\quad\left(\frac{1}{1-\gamma}\,\|\hat{\pi}_{t+1}(\cdot\,|\,s)- \hat{\pi}_{t}(\cdot\,|\,s)\|+\frac{\eta\gamma|A|}{(1-\gamma)^{3}}\,\|\hat{ \pi}_{t+1}(\cdot\,|\,s)-\pi_{t}(\cdot\,|\,s)\|+\frac{\eta}{(1-\gamma)^{2}}| \lambda_{t}-\hat{\lambda}_{t+1}|\right)\] \[\quad+\,|\lambda-\hat{\lambda}_{t+1}|\left(|\hat{\lambda}_{t+1}- \hat{\lambda}_{t}|+\frac{\eta\kappa_{\rho}\sqrt{|A|}}{(1-\gamma)^{3}}\sum_{s} d_{\rho}^{\pi^{*}}(s)\,\|\hat{\pi}_{t+1}(\cdot\,|\,s)-\pi_{t}(\cdot\,|\,s)\|\right)\] \[\geq \max_{s}\|\pi(\cdot\,|\,s)-\hat{\pi}_{t+1}(\cdot\,|\,s)\|\sum_{s }d_{\rho}^{\pi}(s)\times\] \[\quad\left(\frac{1}{1-\gamma}\,\|\hat{\pi}_{t+1}(\cdot\,|\,s)- \hat{\pi}_{t}(\cdot\,|\,s)\|+\frac{\eta\gamma|A|}{(1-\gamma)^{3}}\max_{s}\| \hat{\pi}_{t+1}(\cdot\,|\,s)-\pi_{t}(\cdot\,|\,s)\|+\frac{\eta}{(1-\gamma)^{2} }|\lambda_{t}-\hat{\lambda}_{t+1}|\right)\] \[\quad+\,|\lambda-\hat{\lambda}_{t+1}|\left(|\hat{\lambda}_{t+1}- \hat{\lambda}_{t}|+\frac{\eta\kappa_{\rho}\sqrt{|A|}}{(1-\gamma)^{3}}\sum_{s} d_{\rho}^{\pi^{*}}(s)\,\|\hat{\pi}_{t+1}(\cdot\,|\,s)-\pi_{t}(\cdot\,|\,s)\|\right)\] \[\geq \eta\left(V_{r+\hat{\lambda}_{t+1g}}^{\pi}(\rho)-V_{r+\lambda g }^{\hat{\pi}_{t+1}}(\rho)\right)\]

where the second inequality comes from the property of \(\kappa_{\rho}\) and the last inequality is straightforward from (37). We take \(\eta>0\) such that \(\max\left(\frac{2\eta|A|}{(1-\gamma)^{3}},\frac{\eta\kappa_{\rho}}{(1-\gamma) ^{3}}\right)\leq\frac{1}{2}\) and denote \(\kappa_{\rho,\gamma}:=\max\left(\frac{\kappa_{\rho}}{1-\gamma},1\right)\). If we take the square of both sides of the inequality above, then the second product argument has the following upper bound,

\[\left(\sum_{s}d_{\rho}^{\pi^{*}}(s)\left(\|\hat{\pi}_{t+1}(\cdot \,|\,s)-\hat{\pi}_{t}(\cdot\,|\,s)\|+\frac{2\eta|A|}{(1-\gamma)^{3}}\,\|\hat{ \pi}_{t+1}(\cdot\,|\,s)-\pi_{t}(\cdot\,|\,s)\|\right)\,+\,|\hat{\lambda}_{t+1}- \hat{\lambda}_{t}|\,+\,\frac{\eta\kappa_{\rho}}{(1-\gamma)^{3}}|\lambda_{t}- \hat{\lambda}_{t+1}|\right)^{2}\] \[\leq \left(\sum_{s}d_{\rho}^{\pi^{*}}(s)\left(\|\hat{\pi}_{t+1}(\cdot \,|\,s)-\hat{\pi}_{t}(\cdot\,|\,s)\|+\frac{1}{2}\,\|\hat{\pi}_{t+1}(\cdot\,| \,s)-\pi_{t}(\cdot\,|\,s)\|\right)+|\hat{\lambda}_{t+1}-\hat{\lambda}_{t}|+\frac {1}{2}|\lambda_{t}-\hat{\lambda}_{t+1}|\right)^{2}\] \[\leq \left(\sum_{s}d_{\rho}^{\pi^{*}}(s)\left(\|\pi_{t}(\cdot\,|\,s)- \hat{\pi}_{t}(\cdot\,|\,s)\|+\frac{3}{2}\,\|\hat{\pi}_{t+1}(\cdot\,|\,s)-\pi_{t}( \cdot\,|\,s)\|\right)+|\lambda_{t}-\hat{\lambda}_{t}|+\frac{3}{2}|\lambda_{t}- \hat{\lambda}_{t+1}|\right)^{2}\] \[\leq \left(\frac{3}{2}\sum_{s}d_{\rho}^{\pi^{*}}(s)\left(\|\pi_{t}( \cdot\,|\,s)-\hat{\pi}_{t}(\cdot\,|\,s)\|+\|\hat{\pi}_{t+1}(\cdot\,|\,s)-\pi_{t}( \cdot\,|\,s)\|\right)+\frac{3}{2}\left(|\lambda_{t}-\hat{\lambda}_{t}|+|\lambda _{t}-\hat{\lambda}_{t+1}|\right)\right)^{2}\] \[\leq 9\sum_{s}d_{\rho}^{\pi^{*}}(s)\left(\|\pi_{t}(\cdot\,|\,s)-\hat{ \pi}_{t}(\cdot\,|\,s)\|^{2}+\|\hat{\pi}_{t+1}(\cdot\,|\,s)-\pi_{t}(\cdot\,|\,s)\| ^{2}\right)+9\left(|\lambda_{t}-\hat{\lambda}_{t}|^{2}+|\lambda_{t}-\hat{ \lambda}_{t+1}|^{2}\right)\]

where we use the inequality \((x+y)^{2}\leq 2x^{2}+2y^{2}\) and Jensen's inequality. 

Recall the definition of \(\kappa_{\rho}\) and \(\kappa_{\rho}\leq 1/\rho_{\min}\), where \(\rho_{\min}:=\min_{s}\rho(s)\).

**Lemma 21**.: _Assume \(\rho_{\min}>0\). For any \(t\geq 1\), the primal-dual iterates of OPG-PD (9) satisfy_

\[\sup_{\pi\,\in\,\Pi,\,\lambda\,\in\,\Lambda}\frac{V_{r+\hat{\lambda} _{t+1g}}^{\pi}(\rho)-V_{r+\lambda g}^{\hat{\pi}_{t+1}}(\rho)}{\max_{s}\|\pi( \cdot\,|\,s)-\hat{\pi}_{t+1}(\cdot\,|\,s)\|+|\lambda-\hat{\lambda}_{t+1}|}\] \[\geq C_{\rho,\xi}\left(\sum_{s}d_{\rho}^{\pi^{*}}(s)\,\|\hat{\pi}_{t+1} (\cdot\,|\,s)-\mathcal{P}_{\Pi^{*}}(\hat{\pi}_{t+1}(\cdot\,|\,s))\|+|\hat{ \lambda}_{t+1}-\mathcal{P}_{\Lambda^{*}}(\hat{\lambda}_{t+1})|\right)\]

_where \(C_{\rho,\xi}:=c\rho_{\min}/(2\sqrt{|S||A|})/(1+1/((1-\gamma)\xi))\) in which \(c>0\) is described in Lemma 26._Proof.: Denote \(V^{\star}:=V^{\pi^{\star}}_{r+\lambda^{\star}g}(\rho)\) and \(D_{\max}:=\max_{\pi,\pi^{\prime}\,\in\,\Pi,\lambda,\lambda^{\prime}\,\in\,\Lambda }(\max_{s}\left\|\pi(\cdot\,|\,s)-\pi^{\prime}(\cdot\,|\,s)\right\|+|\lambda- \lambda^{\prime}|)\). We observe that if we can prove that there exist constants \(c_{1},\,c_{2}>0\) such that

\[\max_{\pi\,\in\,\Pi}V^{\pi}_{r+\hat{\lambda}_{t+1}g}(\rho)-V^{ \star} \geq c_{1}|\hat{\lambda}_{t+1}-\mathcal{P}_{\Lambda^{\star}}(\hat{ \lambda}_{t+1})|\] (38a) \[V^{\star}-\min_{\lambda\,\in\,\Lambda}V^{\hat{\pi}_{t+1}}_{r+ \lambda g}(\rho) \geq c_{2}\sum_{s}d^{\pi^{\star}}_{\rho}(s)\left\|\hat{\pi}_{t+1}(\cdot \,|\,s)-\mathcal{P}_{\Pi^{\star}}(\hat{\pi}_{t+1}(\cdot\,|\,s))\right\|,\] (38b)

then,

\[\sup_{\pi\,\in\,\Pi,\,\lambda\,\in\,\Lambda}\,\frac{V^{\pi}_{r+ \hat{\lambda}_{t+1}g}(\rho)-V^{\hat{\pi}_{t+1}}_{r+\lambda g}(\rho)}{\max_{s} \left\|\pi(\cdot\,|\,s)-\hat{\pi}_{t+1}(\cdot\,|\,s)\right\|+|\lambda-\hat{ \lambda}_{t+1}|}\] \[\geq \frac{1}{D_{\max}}\sup_{\pi\,\in\,\Pi,\,\lambda\,\in\,\Lambda}V^{ \pi}_{r+\hat{\lambda}_{t+1}g}(\rho)-V^{\hat{\pi}_{t+1}}_{r+\lambda g}(\rho)\] \[= \frac{1}{D_{\max}}\left(\max_{\pi\,\in\,\Pi}V^{\pi}_{r+\hat{ \lambda}_{t+1}g}(\rho)-V^{\star}\right)\,+\,\frac{1}{D_{\max}}\left(V^{\star} -\min_{\lambda\,\in\,\Lambda}V^{\hat{\pi}_{t+1}}_{r+\lambda g}(\rho)\right)\] \[\geq \frac{\min(c_{1},c_{2})}{D_{\max}}\left(\sum_{s}d^{\pi^{\star}}_{ \rho}(s)\left\|\hat{\pi}_{t+1}(\cdot\,|\,s)-\mathcal{P}_{\Pi^{\star}}(\hat{ \pi}_{t+1}(\cdot\,|\,s))\right\|+|\hat{\lambda}_{t+1}-\mathcal{P}_{\Lambda^{ \star}}(\hat{\lambda}_{t+1})|\right)\]

which proves the lemma by taking \(C_{\rho,\xi}:=\frac{\min(c_{1},c_{2})}{D_{\max}}\).

We next prove (38) using the bilinear game result in Lemma 26. By the linear program formulation of constrained MDP, we can express the value function in terms of the occupancy measure \(q^{\pi}\), e.g., \(V^{\pi}_{r+\lambda g}(\rho)=\langle q^{\pi},r+\lambda g\rangle\), where \(q^{\pi}\) is the occupancy measure that lives in a polytope \(\mathcal{Q}\) that is given by (15). Hence, the constrained saddle-point problem (3) reduces to,

\[\max_{q^{\pi}\,\in\,\mathcal{Q}}\,\,\,\min_{\lambda\,\in\,\Lambda}\,\,\langle q ^{\pi},r+\lambda g\rangle = \min_{\lambda\,\in\,\Lambda}\,\,\max_{q^{\pi}\,\in\,\mathcal{Q}} \,\,\langle q^{\pi},r+\lambda g\rangle.\]

We notice that the game value keeps the same as \(V^{\star}\) that is achieved at \((q^{\pi^{\star}},\lambda^{\star})\), where \(q^{\pi^{\star}}\) is the occupancy measure under the policy \(\pi^{\star}\). Let the set of occupancy measures associated with \(\Pi^{\star}\) be \(\mathcal{Q}^{\star}\). According to Lemma 26, we know that there exists a constant \(c>0\) such that

\[\max_{q^{\pi}\,\in\,\mathcal{Q}}\langle q^{\pi},r+\lambda g \rangle-V^{\star} \geq c|\lambda-\mathcal{P}_{\Lambda^{\star}}(\lambda)|\] (39a) \[V^{\star}-\min_{\lambda\,\in\,\Lambda}\langle q^{\pi},r+\lambda g\rangle \geq c\left\|q^{\pi}-\mathcal{P}_{\mathcal{Q}^{\star}}(q^{\pi})\right\|.\] (39b)

It is more straightforward to see (38a) from (39a) if we take \(\lambda=\hat{\lambda}_{t+1}\) and \(c_{1}=c\). We next show (38b) using (39b) by taking \(\pi^{\star}(\cdot\,|\,s)\) to be the policy associated with \(\mathcal{P}_{\mathcal{Q}^{\star}}(q^{\pi})\),

\[\sum_{s}d^{\pi^{\star}}_{\rho}(s)\left\|\pi(\cdot\,|\,s)-\pi^{ \star}(\cdot\,|\,s)\right\|\] \[= \sum_{s}d^{\pi^{\star}}_{\rho}(s)\left\|\frac{q^{\pi}(s,\cdot)}{q^ {\pi}(s)}-\frac{q^{\pi^{\star}}(s,\cdot)}{q^{\pi^{\star}}(s)}\right\|\] \[\leq \sum_{s}d^{\pi^{\star}}_{\rho}(s)\frac{\left\|q^{\pi}(s,\cdot)-q^ {\pi^{\star}}(s,\cdot)\right\|q^{\pi^{\star}}(s)}{q^{\pi}(s)q^{\pi^{\star}}(s)} \,+\,\sum_{s}d^{\pi^{\star}}_{\rho}(s)\,\frac{\left\|q^{\pi^{\star}}(s,\cdot) \right\|\left|q^{\pi}(s)-q^{\pi^{\star}}(s)\right|}{q^{\pi}(s)q^{\pi^{\star}}( s)}\] \[\leq \sum_{s}\frac{\left\|q^{\pi}(s,\cdot)-q^{\pi^{\star}}(s,\cdot) \right\|}{q^{\pi}(s)}\,+\,\sum_{s}\frac{\left|q^{\pi}(s)-q^{\pi^{\star}}(s) \right|}{q^{\pi}(s)}\] \[\leq \frac{1}{\rho_{\min}}\sum_{s}\left(\left\|q^{\pi}(s,\cdot)-q^{\pi ^{\star}}(s,\cdot)\right\|+\left|q^{\pi}(s)-q^{\pi^{\star}}(s)\right|\right)\] \[\leq \frac{1}{\rho_{\min}}\left(\sqrt{|S|}\sqrt{\sum_{s}\left\|q^{\pi}(s,\cdot)-q^{\pi^{\star}}(s,\cdot)\right\|^{2}}+\sqrt{|S||A|}\sqrt{\sum_{s,a}|q^{ \pi}(s,a)-q^{\pi^{\star}}(s,a)|^{2}}\right)\] \[= \frac{2\sqrt{|S||A|}}{\rho_{\min}}\left\|q^{\pi}-q^{\pi^{\star}}\right\|\] \[= \frac{2\sqrt{|S||A|}}{\rho_{\min}}\left\|q^{\pi}-\mathcal{P}_{ \mathcal{Q}^{\star}}(q^{\pi})\right\|\]where the first inequality is due to triangle inequality, we use the fact: \((1-\gamma)q^{\pi^{*}}(s)=d_{\rho}^{\pi^{*}}(s)\), \(d_{\rho}^{\pi^{*}}(s)\leq 1\), and \(\left\|q^{\pi^{*}}(s,\cdot)\right\|\leq 1\) in the second inequality, the third inequality is due to that \(q^{\pi}(s)\geq\rho_{\min}\), and we apply Cauchy-Schwarz inequality in the last inequality. Hence, we can further lower bound (39b),

\[V^{\star}-\min_{\lambda\,\in\,\Lambda}\langle q^{\pi},r+\lambda g\rangle \geq c\,\|q^{\pi}-\mathcal{P}_{\mathcal{Q}^{\star}}(q^{\pi})\|\] \[\geq \frac{c\rho_{\min}}{2\sqrt{|S||A|}}\sum_{s}d_{\rho}^{\pi^{*}}(s) \left\|\pi(\cdot\,|\,s)-\pi^{\star}(\cdot\,|\,s)\right\|\] \[\geq \frac{c\rho_{\min}}{2\sqrt{|S||A|}}\sum_{s}d_{\rho}^{\pi^{*}}(s) \left\|\pi(\cdot\,|\,s)-\mathcal{P}_{\Pi^{*}}(\pi(\cdot\,|\,s))\right\|\]

which yields (38b) using \(c_{2}=\frac{c\rho_{\min}}{2\sqrt{|S||A|}}\).

Finally, we combine all selected constants and take \(D_{\max}=1+\frac{1}{(1-\gamma)\xi}\) to conclude the proof. 

**Lemma 22**.: _For any policies \(\pi\) and \(\pi^{\prime}\), we have_

\[\left\|Q_{r}^{\pi}(\cdot,\cdot)-Q_{r}^{\pi^{\prime}}(\cdot,\cdot )\right\|_{\infty} \leq \frac{\gamma}{(1-\gamma)^{2}}\max_{s}\left\|\pi(\cdot\,|\,s)-\pi^{ \prime}(\cdot\,|\,s)\right\|_{1}\] \[|V_{g}^{\pi}(\rho)-V_{g}^{\pi^{\prime}}(\rho)| \leq \frac{\kappa_{\rho}}{(1-\gamma)^{3}}\sum_{s}d_{\rho}^{\pi^{*}}(s) \left\|\pi(\cdot\,|\,s)-\pi^{\prime}(\cdot\,|\,s)\right\|_{1}.\]

Proof.: By the Bellman equations, for each pair \((s,a)\),

\[Q_{r}^{\pi}(s,a) = r(s,a)\,+\,\gamma\sum_{s^{\prime},a^{\prime}}P(s^{\prime}\,|\,s, a)\pi(a^{\prime}\,|\,s^{\prime})Q_{r}^{\pi^{\prime}}(s^{\prime},a^{\prime})\] \[Q_{r}^{\pi^{\prime}}(s,a) = r(s,a)\,+\,\gamma\sum_{s^{\prime},a^{\prime}}P(s^{\prime}\,|\,s, a)\pi^{\prime}(a^{\prime}\,|\,s^{\prime})Q_{r}^{\pi^{\prime}}(s^{\prime},a^{ \prime}).\]

Hence, for each pair \((s,a)\),

\[|Q_{r}^{\pi}(s,a)-Q_{r}^{\pi^{\prime}}(s,a)| \leq \gamma\sum_{s^{\prime},a^{\prime}}P(s^{\prime}\,|\,s,a)\left|\pi( a^{\prime}\,|\,s^{\prime})Q_{r}^{\pi}(s^{\prime},a^{\prime})-\pi^{\prime}(a^{ \prime}\,|\,s^{\prime})Q_{r}^{\pi^{\prime}}(s^{\prime},a^{\prime})\right|\] \[\leq \gamma\sum_{s^{\prime},a^{\prime}}P(s^{\prime}\,|\,s,a)\left|\pi (a^{\prime}\,|\,s^{\prime})-\pi^{\prime}(a^{\prime}\,|\,s^{\prime})\right|Q_ {r}^{\pi^{\prime}}(s^{\prime},a^{\prime})\] \[+\,\gamma\sum_{s^{\prime},a^{\prime}}P(s^{\prime}\,|\,s,a)\pi^{ \prime}(a^{\prime}\,|\,s^{\prime})\left|Q_{r}^{\pi}(s^{\prime},a^{\prime})-Q _{r}^{\pi^{\prime}}(s^{\prime},a^{\prime})\right|\] \[\leq \frac{\gamma}{1-\gamma}\max_{s}\left\|\pi(\cdot\,|\,s)-\pi^{ \prime}(\cdot\,|\,s)\right\|_{1}\,+\,\gamma\left\|Q_{r}^{\pi}(\cdot,\cdot)-Q_{ r}^{\pi^{\prime}}(\cdot,\cdot)\right\|_{\infty}\]

which yields the first inequality.

To show the second inequality, we employ the performance difference lemma to show that,

\[|V_{g}^{\pi}(\rho)-V_{g}^{\pi^{\prime}}(\rho)| \leq \frac{1}{1-\gamma}\sum_{s,a}d_{\rho}^{\pi}(s)|\pi(a\,|\,s)-\pi^{ \prime}(a\,|\,s)||Q_{g}^{\pi^{\prime}}(s,a)|\] \[\leq \frac{1}{(1-\gamma)^{2}}\sum_{s}d_{\rho}^{\pi}(s)\left\|\pi(\cdot \,|\,s)-\pi^{\prime}(\cdot\,|\,s)\right\|_{1}\] \[\leq \frac{1}{(1-\gamma)^{2}}\sum_{s}d_{\rho}^{\pi}(s)\left\|\pi(\cdot \,|\,s)-\pi^{\prime}(\cdot\,|\,s)\right\|_{1}\]

where we replace \(d_{\rho}^{\pi}\) by \(d_{\rho}^{\pi^{*}}\) using the inequality \(\left\|d_{\rho}^{\pi}/d_{\rho}^{\pi^{*}}\right\|_{\infty}\leq\kappa_{\rho}/(1- \gamma)\) for any policy \(\pi\in\Pi\).

### Proof of Theorem 6

Proof.: From the non-increasing relation (29) in Lemma 19, we have

\[\frac{1}{2(1-\gamma)}\sum_{s}d_{\rho}^{\pi^{*}}\left(s\right)\left\|\hat{\pi}_{t+ 1}(\cdot|s)-\pi_{t}(\cdot|s)\right\|^{2}\;+\;\frac{1}{2}(\hat{\lambda}_{t+1}- \lambda_{t})^{2}\;\;\leq\;\;\zeta_{t}\;\;\leq\;\;\frac{16}{7}\Theta_{t}\;\; \leq\;\;\frac{16}{7}\Theta_{1}.\]

Meanwhile, from Lemma 20, we obtain that

\[\zeta_{t} \geq \frac{1}{4}\sum_{s}d_{\rho}^{\pi^{*}}\left(s\right)\left\|\hat{\pi }_{t+1}(\cdot\,|\,s)-\pi_{t}(\cdot\,|\,s)\right\|^{2}\,+\,\frac{1}{4}(\hat{ \lambda}_{t+1}-\lambda_{t})^{2}\] \[\;\;+\frac{1}{4}\sum_{s}d_{\rho}^{\pi^{*}}\left(s\right)\left( \left\|\hat{\pi}_{t+1}(\cdot\,|\,s)-\pi_{t}(\cdot\,|\,s)\right\|^{2}+\left\| \pi_{t}(\cdot\,|\,s)-\hat{\pi}_{t}(\cdot\,|\,s)\right\|^{2}\right)\] \[\;\;+\frac{1}{4}\left((\hat{\lambda}_{t+1}-\lambda_{t})^{2}+( \lambda_{t}-\hat{\lambda}_{t})^{2}\right)\] \[\geq \frac{1}{4}\sum_{s}d_{\rho}^{\pi^{*}}\left(s\right)\left\|\hat{\pi }_{t+1}(\cdot\,|\,s)-\pi_{t}(\cdot\,|\,s)\right\|^{2}\,+\,\frac{1}{4}(\hat{ \lambda}_{t+1}-\lambda_{t})^{2}\] \[\;\;+\frac{\eta^{2}}{36\kappa_{\rho,\gamma}^{2}}\sup_{\pi\,\in\, \Pi,\lambda\,\in\,\Lambda}\frac{\left[V_{r+\hat{\lambda}_{t+1}g}^{\pi}(\rho)-V _{r+\lambda g}^{\hat{\pi}_{t+1}}(\rho)\right]_{+}^{2}}{\left(\max_{s}\left\| \pi(\cdot\,|\,s)-\hat{\pi}_{t+1}(\cdot\,|\,s)\right\|+\left|\lambda-\hat{ \lambda}_{t+1}\right|\right)^{2}}\] \[\geq \frac{1}{4}\sum_{s}d_{\rho}^{\pi^{*}}\left(s\right)\left\|\hat{ \pi}_{t+1}(\cdot\,|\,s)-\pi_{t}(\cdot\,|\,s)\right\|^{2}\,+\,\frac{1}{4}(\hat{ \lambda}_{t+1}-\lambda_{t})^{2}\] \[\;\;+\frac{\eta^{2}C_{\rho,\xi}^{\prime}}{36\kappa_{\rho,\gamma}^ {2}}\left(\sum_{s}d_{\rho}^{\pi^{*}}(s)\left\|\hat{\pi}_{t+1}(\cdot\,|\,s)- \mathcal{P}_{\Pi^{*}}(\hat{\pi}_{t+1}(\cdot\,|\,s))\right\|^{2}+(\hat{\lambda }_{t+1}-\mathcal{P}_{\Lambda^{*}}(\hat{\lambda}_{t+1}))^{2}\right)\] \[\geq (1-\gamma)\min\left(2,\frac{4\eta^{2}C_{\rho,\xi}^{\prime}}{9 \kappa_{\rho,\gamma}^{2}}\right)\Theta_{t+1}\]

where the third inequality is due to Lemma 21, the inequalities: \((x+y)^{2}\geq x^{2}+y^{2}\) for \(x\), \(y\geq 0\), and \(d_{\rho}^{\pi^{*}}(s)\geq(1-\gamma)\rho_{\min}\), and \(C_{\rho,\xi}^{\prime}:=(1-\gamma)C_{\rho,\xi}^{2}\rho_{\min}\). Hence, the relation (29) reduces into,

\[\Theta_{t+1} \leq \Theta_{t}\,-\,(1-\gamma)\min\left(\frac{7}{8},\frac{7\eta^{2}C_{ \rho,\xi}^{\prime}}{36\kappa_{\rho,\gamma}^{2}}\right)\Theta_{t+1}\]

which implies that \(\Theta_{t}\) is decreasing to zero at an exponential rate. 

### Proof of Corollary 7

Proof.: According to Theorem 6, if we take the same stepsize \(\eta\), then for any \(t=\Omega(\log\frac{1}{\epsilon})\),

\[\frac{1}{2(1-\gamma)}\sum_{s}d_{\rho}^{\pi^{*}}(s)\left\|\mathcal{P}_{\Pi^{*}}( \hat{\pi}_{t}(\cdot\,|\,s))-\hat{\pi}_{t}(\cdot\,|\,s)\right\|^{2}\;\;=\;\;O( \epsilon)\;\;\mbox{and}\;\;\frac{1}{2}(\mathcal{P}_{\Lambda^{*}}(\hat{ \lambda}_{t})-\hat{\lambda}_{t})^{2}\;\;=\;\;O(\epsilon).\]

Let \(\hat{\pi}_{t}^{*}(\cdot\,|\,s):=\mathcal{P}_{\Pi^{*}}(\hat{\pi}_{t}(\cdot\,|\, s))\) and \(\hat{\lambda}_{t}^{*}:=\mathcal{P}_{\Lambda^{*}}(\hat{\lambda}_{t})\). Because of the interchangeability of saddle points from Lemma 9, \((\hat{\pi}_{t}^{*},\hat{\lambda}_{t}^{*})\) is a saddle point in the set \(\Pi^{*}\times\Lambda^{*}\) for any \(t\geq 0\).

First, we have

\[V_{r}^{\hat{\pi}_{t}^{*}}(\rho)-V_{r}^{\hat{\pi}_{t}}(\rho) = \frac{1}{1-\gamma}\sum_{s,a}d_{\rho}^{\hat{\pi}_{t}^{*}}(s)\left( \hat{\pi}_{t}^{*}(a\,|\,s)-\hat{\pi}_{t}(a\,|\,s)\right)Q_{r}^{\pi_{t}}(s,a)\] \[\leq \frac{1}{(1-\gamma)^{2}}\sum_{s}d_{\rho}^{\hat{\pi}_{t}^{*}}(s) \left\|\hat{\pi}_{t}^{*}(\cdot\,|\,s)-\hat{\pi}_{t}(\cdot\,|\,s)\right\|_{1}\] \[\leq \frac{\sqrt{|A|}}{(1-\gamma)^{2}}\sum_{s}d_{\rho}^{\hat{\pi}_{t}^{* }}(s)\left\|\hat{\pi}_{t}^{*}(\cdot\,|\,s)-\hat{\pi}_{t}(\cdot\,|\,s)\right\|\] \[\leq \frac{\sqrt{|A|}}{(1-\gamma)^{2}}\sqrt{\sum_{s}d_{\rho}^{\hat{\pi} _{t}^{*}}(s)\left\|\hat{\pi}_{t}^{*}(\cdot\,|\,s)-\hat{\pi}_{t}(\cdot\,|\,s) \right\|^{2}}\]where we use Cauchy-Schwarz inequality in the first and third inequalities, and the second inequality is due to \(\left\|x\right\|_{1}\leq\sqrt{d}\left\|x\right\|_{2}\) for \(x\in\mathbb{R}^{d}\), which shows \(V_{r}^{\hat{\pi}_{t}^{*}}(\rho)-V_{r}^{\hat{\pi}_{t}}(\rho)\leq O(\sqrt{\epsilon})\). By the saddle-point property of \((\hat{\pi}_{t}^{*},\hat{\lambda}_{t}^{*})\), \(\hat{\pi}_{t}^{*}\) is also an optimal constrained policy, i.e., \(V_{r}^{\hat{\pi}_{t}^{*}}(\rho)=V_{r}^{\pi^{*}}(\rho)\). Therefore, \(V_{r}^{\pi^{*}}(\rho)-V_{r}^{\hat{\pi}_{t}}(\rho)\leq O(\sqrt{\epsilon})\).

Second, we have

\[-V_{g}^{\hat{\pi}_{t}}(\rho) = \underbrace{-V_{g}^{\hat{\pi}_{t}^{*}}(\rho)}_{\text{(i)}}+ \underbrace{V_{g}^{\hat{\pi}_{t}^{*}}(\rho)-V_{g}^{\hat{\pi}_{t}}(\rho)}_{ \text{(ii)}}.\]

Similar to bounding \(V_{r}^{\hat{\pi}_{t}^{*}}(\rho)-V_{r}^{\hat{\pi}_{t}}(\rho)\), we can show that (ii) \(\leq O(\sqrt{\epsilon})\). By the saddle-point property of \((\hat{\pi}_{t}^{*},\hat{\lambda}_{t}^{*})\), \(V_{g}^{\hat{\pi}_{t}^{*}}(\rho)\geq 0\). Therefore, \(-V_{g}^{\hat{\pi}_{t}}(\rho)\leq O(\sqrt{\epsilon})\).

Finally, we replace the accuracy \(\sqrt{\epsilon}\) by \(\epsilon\) and combine all big O notation to conclude the proof. 

### Zero constraint violation of OPG-PD (9)

**Corollary 23** (Zero constraint violation).: _Let Assumption 1 hold and the optimal state visitation distribution be unique, i.e., \(d_{\rho}^{\pi}=d_{\rho}^{\pi^{*}}\) for any \(\pi\in\Pi^{*}\), and \(\rho_{\min}>0\). For small \(\epsilon\), there exists \(\delta>0\) such that if we instead use the conservative constraint \(V_{g^{\prime}}^{\pi}(\rho)\geq 0\) for \(g^{\prime}=g-(1-\gamma)\delta\), and take the stepsize \(\eta\) from Theorem 6, then the policy iterates of OPG-PD satisfy_

\[V_{r}^{\pi^{*}}(\rho)-V_{r}^{\hat{\pi}_{t}}(\rho)\ \leq\ \epsilon\ \text{ and }\ -V_{g}^{\hat{\pi}_{t}}(\rho)\ \leq\ 0\ \text{ for any }t=\Omega\left(\log^{2}\frac{1}{\epsilon}\right)\]

_where \(\Omega(\cdot)\) only omits some problem-dependent constant._

Proof.: We apply the conservatism to the translated constraint \(V_{g}^{\pi}(\rho)\geq 0\) in Problem (1). Specifically, for any \(\delta<\min(\xi,1)\), we let \(g^{\prime}:=g-(1-\gamma)\delta\) and define a conservative constraint,

\[V_{g^{\prime}}^{\pi} := V_{g}^{\pi}(\rho)-\delta\ \geq\ 0.\]

It is straightforward to see that Assumption 1 ensures that \(V_{g^{\prime}}^{\pi_{t}}(\rho)\geq 0\) is strictly feasible for a new slack variable \(\xi^{\prime}:=\xi-\delta\). We now can apply OPG-PD (9) to a new Lagrangian,

\[L^{\prime}(\pi,\lambda)\ :=\ V_{r+\lambda g^{\prime}}^{\pi}(\rho)\]

and Corollary 7 holds if we replace \(g\) in OPG-PD by \(g^{\prime}\). Thus,

\[V_{r}^{\pi_{\delta}^{*}}(\rho)-V_{r}^{\hat{\pi}_{t}}(\rho) \leq O(\epsilon)\ \text{ and }\ -V_{g^{\prime}}^{\hat{\pi}_{t}}(\rho) \leq O(\epsilon)\ \text{ for any }t=\Omega\left(\log^{2}\frac{1}{\epsilon}\right)\]

where \(\Omega(\cdot)\) hides some problem-dependent constant, and \(\pi_{\delta}^{*}\) is an optimal policy to the \(\delta\)-perturbed constrained policy optimization problem,

\[\mathop{\mathrm{maximize}}_{\pi\in\Pi}\ V_{r}^{\pi}(\rho)\qquad \mathrm{subject\ to\ }V_{g}^{\pi}(\rho)-\delta\ \geq\ 0.\] (40)

We notice that the above \(\Omega(\cdot)\) has some problem-dependent constant \(\Upsilon>0\). Thus, we select \(\delta\) such that \(\delta\geq\epsilon\Upsilon\), which is always possible for small enough \(\epsilon\), for instance \(\delta=\frac{\xi}{2}\) and \(\xi^{\prime}=\frac{\xi}{2}\). Hence, if we take \(\delta=\frac{\xi}{2}\) and such small \(\epsilon\), then,

\[-V_{g^{\prime}}^{\hat{\pi}_{t}}(\rho) = -V_{g}^{\hat{\pi}_{t}}(\rho)\,+\,\delta\ \ \leq\ \ \epsilon\Upsilon\ \text{ for any }t=\Omega\left(\log^{2}\frac{1}{\epsilon}\right)\]

which shows that \(V_{g}^{\hat{\pi}_{t}}(\rho)\geq 0\) for some large \(t\).

The rest is to show that \(V_{r}^{\pi^{*}}(\rho)-V_{r}^{\hat{\pi}_{t}}(\rho)\leq O(\epsilon)\). We notice that \(\pi^{\star}\) is an optimal policy to Problem (40) when \(\delta=0\). Let \(q^{\star}\) and \(q^{\star}_{\delta}\) be associated occupancy measures of policies \(\pi^{\star}\) and \(\pi_{\delta}^{*}\). In the occupancy measure space, Problem (40) becomes a linear program and it has a solution \(q^{\star}_{\delta}\). Thus, we can view \(q^{\star}_{\delta}\) as a \(\delta\)-perturbed solution of a convex optimization problem in which all functions are continuous differentiable and the domain is convex and compact. It is known from [129, Theorem 3.1] that the optimal solution \(q^{\star}_{\delta}\) is continuous in \(\delta\), which implies that for any \(\epsilon>0\), there exists \(\delta^{\prime}\) such that \(|\langle r,q^{\star}\rangle-\langle r,q_{\delta}^{\star}\rangle|\leq O(\epsilon)\) for any \(\delta<\delta^{\prime}\). Thus, \(|V_{r}^{\pi^{\star}}(\rho)-V_{r}^{\pi_{\delta}}(\rho)|\leq O(\epsilon)\) for small enough \(\epsilon\). Therefore,

\[V_{r}^{\pi^{\star}}(\rho)-V_{r}^{\tilde{\pi}_{t}}(\rho) \leq V_{r}^{\pi_{\delta}^{\star}}(\rho)-V_{r}^{\tilde{\pi}_{t}}(\rho) \,+\,|V_{r}^{\pi^{\star}}(\rho)-V_{r}^{\pi_{\delta}^{\star}}(\rho)| \leq O(\epsilon)\]

for some large \(t\). Collecting all conditions on \(\delta\) leads to our final choice of \(\delta=\min(\frac{\delta}{2},1,\delta^{\prime})\). Finally, we combine all big O notation to complete the proof. 

## Appendix E Other Computational Experiments

In this section, we report details of our experimental setup and additional experimental results that verify merits and effectiveness of our methods: RPG-PD (6) and OPG-PD (9). We implement RPG-PD in the form of NPG [49] and the restricted probability simplex \(\hat{\Delta}(A)\) by restraining policy parameter to be bounded.

To properly assess the convergence performance, our experiment is a tabular constrained MDP with a randomly generated transition kernel, a discount factor \(\gamma=0.9\), uniform rewards \(r\in[0,1]\) and utilities \(g\in[-1,1]\), and a uniform initial state distribution \(\rho\). The constraint is \(V_{g}^{\pi}(\rho)\geq 0\). To check feasibility, we employ the standard policy iteration procedure to solve a standard MDP problem with respect to \(V_{g}^{\pi}(\rho)\). If feasible, we then solve a linear program in occupancy-measure space to find the optimal reward value \(V_{r}^{\pi^{\star}}(\rho)\) at an optimal policy \(\pi^{\star}\) induced by the optimal occupancy measure. Throughout all experiments, the random seed is fixed and \(V_{r}^{\pi^{\star}}(\rho)\) takes the value \(8.16\) at an optimal policy \(\pi^{\star}\).

We compare our methods RPG-PD and OPG-PD with three typical learning algorithms in the constrained MDP literature: (i) primal-dual methods [23; 29]; (ii) dual methods [28; 24; 34]; and (iii) primal method [77]. We have reported our comparison for primal-dual methods in Section 5 and Figure 1. We now report our comparison experiments on other two baseline methods: (ii) dual methods [28; 24; 34] and (iii) primal method [77], in Section E.1. In addition, we showcase the last-iterate zero constraint violation performance of our methods in Section E.2, and conduct sensitivity analysis of our methods to regularization and stepsize in Section E.3 and Section E.4, together with a variant of OPG-PD and policy-based ReLOAD [40]. All the experiments were conducted on an Apple MacBook Pro 2017 laptop equipped with a 2.3 GHz Dual-Core Intel Core i5 in Jupyter Notebook.

### Last-iterate convergence comparison with other baselines

We report our comparison for dual methods in Figure 4. We notice that dual methods [28; 24; 34] work in a double-loop fashion, where a (regularized) NPG subroutine is executed to perform the dual update. To make a fair comparison, the number of dual updates and the number of NPG steps are set to be \(53\) and \(20\) to ensure that dual methods take the same number of policy gradient updates: \(1060\), as RPG-PD and OPG-PD, and we evaluate all policy iterates inside the NPG subroutines of dual methods.

In Figure 4, RPG-PD (\(-\) -) and OPG-PD (\(-\) -) outperform PMD-PD [24] (\(-\) -), AR-CPO [28] (\(--\) -), and Accelerated Dual [34] (\(\cdots\) -) in several aspects. First, we see that the known oscillation behavior in primal-dual methods also shows up in these dual methods, which could result from that we can't exactly evaluate the search direction of the dual update via a NPG subroutine. Thus, dual methods can be viewed an instantiation of two-time-scale methods in which the primal update performs faster than the dual update. Regarding this, RPG-PD and OPG-PD show outstanding performance in suppressing oscillation behavior. Second, since RPG-PD and OPG-PD are single-time-scale primal-dual methods, it is easy to tune algorithmic parameters compared with double-loop dual methods. For instance, in Accelerated Dual, it is relatively difficult to find a set of algorithmic parameters that avoid the sub-optimal performance in Figure 4. Third, OPG-PD reaches the maximum reward value 8.16 and RPG-PD converges to a slightly smaller reward value because of regularization, while both methods enjoy the utility constraint satisfaction in the last-iterate fashion, which seems to be difficult for dual methods because of oscillating utility values. Hence, using the same number of policy gradient updates, we have verified that OPG-PD and RPG-PD also can outperform several dual methods by yielding an optimal constrained policy in the last-iterate fashion.

We report our comparison for a primal method in Figure 5. In Figure 5, RPG-PD (\(\boldsymbol{-}\boldsymbol{-}\)) and OPG-PD (\(\boldsymbol{-}\boldsymbol{\cdot}\)) outperform CRPO [77] (\(\boldsymbol{-}\boldsymbol{\cdot}\)). We notice that although CRPO [77] works in a single-time-scale fashion as RPG-PD and OPG-PD, the policy has to be updated by alternatively using the gradient directions of reward/utility value functions. To ensure constraint satisfaction, CRPO often switches between the gradient directions of reward/utility value functions depending on the amount of constraint violation. As a result, we see that CRPO reaches a slightly lower reward value than OPG-PD's, and the constraint satisfaction is relatively conservative and has mild oscillation behavior. In contrast, OPG-PD achieves the maximum reward value 8.16 and RPG-PD converges to a slightly smaller reward value because of regularization, while both methods enjoy the utility constraint satisfaction in the last-iterate fashion. Last but not least, we have supported RPG-PD and OPG-PD with a policy last-iterate convergence theory, while such theory is unknown for CRPO as far as we know.

Figure 4: Convergence performance of RPG-PD, OPG-PD, and dual methods. Learning curves of our RPG-PD (\(\boldsymbol{-}\boldsymbol{-}\)) and OPG-PD (\(\boldsymbol{-}\)), and PMD-PD [24] (\(\boldsymbol{-}\boldsymbol{-}\)), AR-CPO [28] (\(\boldsymbol{-}\boldsymbol{-}\)), and Accelerated Dual [34] (\(\cdots\)) methods. The horizontal axes represent the policy iterations \(\{\pi_{t}\}_{t\geq 0}\) that are generated by each method and the vertical axes mean the value functions of the policy iterates \(\{\pi_{t}\}_{t\geq 0}\): reward value \(V_{r}^{\pi_{t}}(\rho)\) (Left) and utility value \(V_{g}^{\pi_{t}}(\rho)\) (Right). In this experiment, for RPG-PD and OPG-PD, we use the stepsize \(\eta=0.1\) and the regularization parameter \(\tau=0.08\) for RPG-PD, and the initial distribution \(\rho\) is uniform. For PMD-PD, AR-CPO, and Accelerated Dual, we use the stepsize \(\eta=0.1\) for the dual update, the regularized NPG stepsize \(\alpha=1\), and the regularization parameter \(\tau=0.08\), and the uniform initial distribution \(\rho\).

Figure 5: Convergence performance of RPG-PD, OPG-PD, and primal methods. Learning curves of our RPG-PD (\(\boldsymbol{-}\boldsymbol{-}\)) and OPG-PD (\(\boldsymbol{-}\)), and CRPO [77] (\(\boldsymbol{-}\boldsymbol{-}\)) methods. The horizontal axes represent the policy iterations \(\{\pi_{t}\}_{t\geq 0}\) that are generated by each method and the vertical axes mean the value functions of the policy iterates \(\{\pi_{t}\}_{t\geq 0}\): reward value \(V_{r}^{\pi_{t}}(\rho)\) (Left) and utility value \(V_{g}^{\pi_{t}}(\rho)\) (Right). In this experiment, for RPG-PD and OPG-PD, we use the stepsize \(\eta=0.1\) and the regularization parameter \(\tau=0.08\) for RPG-PD, and the initial distribution \(\rho\) is uniform. For CRPO, we update the policy via the NPG step with stepsize \(\eta=0.1\) and the uniform initial distribution \(\rho\).

### Last-iterate zero constraint violation comparison

In this experiment, we continue our previous tabular constrained MDP with a random transition, a discount factor \(\gamma=0.9\), uniform rewards \(r\in[0,1]\) and utilities \(g\in[-1,1]\), and an uniform initial state distribution \(\rho\). Instead of the nominal constraint \(V_{g}^{\pi}(\rho)\geq 0\), we use a conservative constraint \(V_{g^{\prime}}^{\pi}(\rho)\geq 0\) in RPG-PD and OPG-PD, where \(g^{\prime}:=g-(1-\gamma)\delta\) is a conservative utility function and \(\delta\) is the conservative parameter. To get zero constraint violation regarding the nominal constraint, we apply RPG-PD and OPG-PD to the conservative constraint \(V_{g}^{\pi}(\rho)\geq\delta\) where we take the conservative parameter \(\delta=0.1\). As above, we compare conservative RPG-PD and OPG-PD with three typical learning algorithms in the constrained MDP literature: (i) primal-dual methods [23; 29]; (ii) dual methods [28; 24; 34]; and (iii) primal approach [77]. We report our comparison for primal-dual methods in Figure 6, for dual methods [28; 24; 34] in Figure 7, and for primal approach [77] in Figure 8. We observe that conservative RPG-PD and OPG-PD achieve similar performance regarding the oscillation suppression and the optimality of reward values as shown in Figure 1, Figure 4, and Figure 5. Interestingly, in Figure 6, Figure 7, and Figure 8, RPG-PD and OPG-PD converge to a utility value that is strictly above zero, i.e., \(V_{g}^{\pi_{t}}(\rho)>0\) for large \(t\), which is not guaranteed in many of other methods. To sum up, we have confirmed that RPG-PD and OPG-PD can ensure zero constraint violation of instantaneous policy iterates in a finite number of training time.

### Sensitivity of RPG-PD (\(6\)) to regularization and stepsize

In this experiment, we use our previous tabular constrained MDP with a random transition, a discount factor \(\gamma=0.9\), uniform rewards \(r\in[0,1]\) and utilities \(g\in[-1,1]\), and an uniform initial state distribution \(\rho\). The constraint is \(V_{g}^{\pi}(\rho)\geq 0\). We recall that when we set the regularization parameter \(\tau=0\), RPG-PD becomes a policy-based primal-dual method [23] that often suffers the oscillation issue as shown in Figure 1. However, larger regularization usually bias regularized methods more to sub-optimal solutions. Hence, it is important to reveal how the last-iterate convergence of RPG-PD depends on the regularization parameter \(\tau\) and the stepsize \(\eta\).

We first repeat executing RPG-PD with a fixed stepsize \(\eta=0.1\), but varying three different regularization parameters \(\tau\in\{0.1,0.05,0.01\}\). In Figure 9, RPG-PD damps initial oscillations successfully for \(\tau=0.1\) and \(0.05\), and oscillates for \(\tau=0.01\). When we decrease \(\tau\) from \(0.1\) to \(0.05\), the reward value RPG-PD converges to becomes higher, and the utility value's oscillation is slightly amplified initially, but damped eventually. When \(\tau\) is further reduced to \(0.01\), although the reward value reaches a value around the optimal reward value \(8.16\), the utility value behaves oscillating.

Figure 6: Convergence performance of RPG-PD, OPG-PD, and primal-dual methods. Learning curves of our RPG-PD () and OPG-PD (), and NPG-PD [23] () and PID Lagrangian [29] () methods. The horizontal axes represent the policy iterations \(\{\pi_{t}\}_{t\,\geq\,0}\) that are generated by each method and the vertical axes mean the value functions of the policy iterates \(\{\pi_{t}\}_{t\,\geq\,0}\): reward value \(V_{g}^{\pi_{t}}(\rho)\) (Left) and utility value \(V_{g}^{\pi_{t}}(\rho)\) (Right). In this experiment, we apply RPG-PD and OPG-PD to a conservative constraint \(V_{g}^{\pi}(\rho)\geq\delta\), and we take the conservative parameter \(\delta=0.1\), the same stepsize \(\eta=0.1\) for all methods, the regularization parameter \(\tau=0.08\) for RPG-PD, and the uniform initial distribution \(\rho\).

A reason for this is that RPG-PD with a relatively small regularization parameter (compared to the stepsize) works as usual un-regularized single-time-scale primal-dual methods. Hence, increasing the regularization parameter, \(\tau=0.1\) can accelerate the convergence and attenuate the oscillation more effectively, although it makes the reward value more sub-optimal, which is also clearly shown in Figure 10.

To demonstrate the convergence of RPG-PD's policy iterates, we measure the policy optimality gap via the squared norm distance of policy iterates to an optimal policy \(\pi^{\star}\) that is obtained from an occupancy-measure-based linear program. From the optimality gap in Figure 10, we see that three policy optimality gaps decrease sublinearly to some constants in the logarithmic scale plot, which verifies the sublinear last-iterate convergence of RPG-PD's policy iterates in Theorem 2. Hence, we

Figure 8: Convergence performance of RPG-PD, OPG-PD, and primal methods. Learning curves of our RPG-PD (\(-\)) and OPG-PD (\(\blacklozenge\)), and CRPO [77] (\(\blacklozenge\)) methods. The horizontal axes represent the policy iterations \(\{\pi_{t}\}_{t\,\geq\,0}\) that are generated by each method and the vertical axes mean the value functions of the policy iterates \(\{\pi_{t}\}_{t\,\geq\,0}\): reward value \(V_{r}^{\pi_{t}}(\rho)\) (Left) and utility value \(V_{g}^{\pi_{t}}(\rho)\) (Right). In this experiment, we apply RPG-PD and OPG-PD to a conservative constraint \(V_{g}^{\pi}(\rho)\geq\delta\), and we take the conservative parameter \(\delta=0.1\), the stepsize \(\eta=0.1\), the regularization parameter \(\tau=0.08\) for RPG-PD, and the initial distribution \(\rho\) is uniform. For CRPO, we update the policy via the NPG step with stepsize \(\eta=0.1\) and the uniform initial distribution \(\rho\).

Figure 7: Convergence performance of RPG-PD, OPG-PD, and dual methods. Learning curves of our RPG-PD (\(-\)) and OPG-PD (\(-\)), and PMD-PD [24] (\(-\)), AR-CPO [28] (\(-\)), and Accelerated Dual [34] (\(\cdots\)) methods. The horizontal axes represent the policy iterations \(\{\pi_{t}\}_{t\,\geq\,0}\) that are generated by each method and the vertical axes mean the value functions of the policy iterates \(\{\pi_{t}\}_{t\,\geq\,0}\): reward value \(V_{r}^{\pi_{t}}(\rho)\) (Left) and utility value \(V_{g}^{\pi_{t}}(\rho)\) (Right). In this experiment, we apply RPG-PD and OPG-PD to a conservative constraint \(V_{g}^{\pi}(\rho)\geq\delta\), and we take the stepsize \(\eta=0.1\), the conservative parameter \(\delta=0.1\), the regularization parameter \(\tau=0.08\) for RPG-PD, and the initial distribution \(\rho\) is uniform. For PMD-PD, AR-CPO, and Accelerated Dual, we use the stepsize \(\eta=0.1\) for the dual update, the regularized NPG stepsize \(\alpha=1\), and the regularization parameter \(\tau=0.08\), and the uniform initial distribution \(\rho\).

[MISSING_PAGE_EMPTY:58]

### Sensitivity of OPG-PD (9) to stepsize

In this experiment, we use our previous tabular constrained MDP with a random transition, a discount factor \(\gamma=0.9\), uniform rewards \(r\in[0,1]\) and utilities \(g\in[-1,1]\), and an uniform initial state distribution \(\rho\). The constraint is \(V_{g}^{\pi}(\rho)\geq 0\). We repeat executing OPG-PD by varying three different stepsizes \(\eta\in\{0.05,0.1,0.2\}\). To demonstrate the optimality of OPG-PD's policy iterates, we measure the policy optimality gap via the squared norm distance of policy iterates to an optimal policy \(\pi^{\star}\) that is obtained from an occupancy-measure-based linear program. To demonstrate the optimality of OPG-PD's policy iterates, we measure the policy optimality gap via the squared norm distance of policy iterates to an optimal policy \(\pi^{\star}\) that is obtained from an occupancy-measure-based linear program. From the policy optimality gap in Figure 2, we see that three policy optimality gaps decrease linearly in the logarithmic scale plot, which verifies the linear last-iterate convergence of

Figure 11: Convergence performance of RPG-PD with regularization parameter \(\tau\): (\(\tau=0.1\), \(\boldsymbol{-}\)), (\(\tau=0.05\), \(\boldsymbol{-}\)), (\(\tau=0.01\), \(\cdots\)). The horizontal axes represent the policy iterations \(\{\pi_{t}\}_{t\,\geq\,0}\) that are generated by RPG-PD and the vertical axes mean the value functions of the policy iterates \(\{\pi_{t}\}_{t\,\geq\,0}\): reward value \(V_{\tau}^{\pi_{t}}(\rho)\) (Left) and utility value \(V_{g}^{\pi_{t}}(\rho)\) (Right). In this experiment, we fix the same stepsize \(\eta=0.01\) and take the regularization parameter \(\tau\) among \(0.1\), \(0.05\), and \(0.01\), and the uniform initial distribution \(\rho\).

Figure 12: Convergence performance of RPG-PD with regularization parameter \(\tau\): (\(\tau=0.1\), \(\boldsymbol{-}\)), (\(\tau=0.05\), \(\boldsymbol{-}\)), (\(\tau=0.01\), \(\cdots\)). The horizontal axis represents the policy iterations \(\{\pi_{t}\}_{t\,\geq\,0}\) that are generated by RPG-PD and the vertical axis means the policy optimality gap that measures the distance of the policy iterates \(\{\pi_{t}\}_{t\,\geq\,0}\) to an optimal policy \(\pi^{\star}\): \(\sum_{s}\left\|\pi_{t}(\cdot\,|\,s)-\pi^{\star}(\cdot\,|\,s)\right\|^{2}\). In this experiment, we fix the stepsize \(\eta=0.01\) and take the regularization parameter among \(0.1\), \(0.05\), and \(0.01\), and the uniform initial distribution \(\rho\).

OPG-PD's policy iterates in Theorem 6. Furthermore, in Figure 13 we show the optimality of OPG-PD's policy iterates by plotting the optimality gap \(|V_{r}^{\pi^{*}}(\rho)-V_{r}^{\pi_{t}}(\rho)|\) and the constraint violation \(|V_{g}^{\pi_{t}}(\rho)|\), where \(V_{r}^{\pi^{*}}(\rho)=8.16\). We see that the optimality gap and the constraint violation both decay asymptotically in linear rates in spite of some oscillations, and larger stepsize enjoys faster convergence. It is worth mentioning that, these descending oscillations actually show that value functions are approaching the optimal ones. Hence, we have verified the linear last-iterate convergence of OPG-PD's policy iterates in Theorem 6 using a range of constant stepsizes.

Last but not least, we extend this experiment for a variant of OPG-PD that is based on the multiplicative weights update (MWU). We call this variant as an optimistic multiplicative weights update primal-dual (OMWU-PD) method which maintains two sequences for policy and dual variables each: \(\{\pi_{t}\}_{t\geq 1}\) and \(\{\hat{\pi}_{t}\}_{t\geq 1}\) for the policy-player, and \(\{\lambda_{t}\}_{t\geq 1}\) and \(\{\hat{\lambda}_{t}\}_{t\geq 1}\) for the dual-player,

\[\pi_{t}(\cdot\,|\,s) = \operatorname*{argmax}_{\pi(\cdot\,|\,s)\,\in\,\Delta(A)}\left\{ \sum_{a}\pi(a\,|\,s)Q_{r+\lambda_{t-1}g}^{\pi_{t-1}}(s,a)\,-\,\frac{1}{\eta} \operatorname{KL}(\pi(\cdot\,|\,s),\hat{\pi}_{t}(\cdot\,|\,s))\right\}\] (41a) \[\hat{\pi}_{t+1}(\cdot\,|\,s) = \operatorname*{argmax}_{\pi(\cdot\,|\,s)\,\in\,\Delta(A)}\left\{ \sum_{a}\pi(a\,|\,s)Q_{r+\lambda_{t}g}^{\pi_{t}}(s,a)\,-\,\frac{1}{\eta} \operatorname{KL}(\pi(\cdot\,|\,s),\hat{\pi}_{t}(\cdot\,|\,s))\right\}\] (41b) \[\lambda_{t} = \operatorname*{argmin}_{\lambda\,\in\,\Lambda}\left\{\lambda\,V_ {g}^{\pi_{t-1}}(\rho)\,+\,\frac{1}{2\eta}\,(\lambda-\hat{\lambda}_{t})^{2}\right\}\] \[\hat{\lambda}_{t+1} = \operatorname*{argmin}_{\lambda\,\in\,\Lambda}\left\{\lambda\,V_ {g}^{\pi_{t}}(\rho)\,+\,\frac{1}{2\eta}\,(\lambda-\hat{\lambda}_{t})^{2}\right\}\]

where \(\eta\) is the stepsize and \((\hat{\pi}_{0},\hat{\lambda}_{0})=(\pi_{0},\lambda_{0})\in\Pi\times\Lambda\) is the initial point. OMWU-PD concurrently works with two primal iterates and two dual iterates, which is similar to OPG-PD. The only difference is that Primal update (41a) works as the NPG or MWU updates [23, 54], instead of the projected \(Q\)-ascent [58, 65], which is also different from the one-step optimistic MWU in policy-based ReLOAD [40].

To further investigate the applicability of optimistic gradient methods, we execute OMWU-PD in the same setting and report our result in Figure 14 and Figure 15. We see that the policy optimality gaps decay sublinearly in Figure 14, and the optimality gap and the constraint violation asymptotically decay in sublinear rates in Figure 15. As a comparison, we repeat the same experiment for policy-based ReLOAD [40], which is different from our OMWU-PD in using one-step optimistic gradient updates. In Figure 16 and Figure 17, we observe sublinear decay of policy optimality gap, optimality gap and constraint violation, where are similar as shown in Figure 14 and Figure 15.

The empirical results from two MWU-based optimistic primal-dual algorithms are suggestive. First, MWU-based optimistic algorithms can also have policy last-iterate convergence to an optimal policy. Second, convergence rates of MWU-based optimistic algorithms look slower than OPG-PD's under the same constant stepsize, which we leave as an immediate future work to establish sublinear convergence rates for MWU-based optimistic primal-dual algorithms.

## Appendix F Supporting Lemmas

In this section, we collect some lemmas that are helpful to our analysis.

### Lemmas in optimization

For any convex differentiable function \(\psi\colon X\to\mathbb{R}\), the Bregman divergence of \(x\), \(x^{\prime}\in X\) is given by \(D_{\psi}(x^{\prime},x):=\psi(x^{\prime})-\psi(x)-\langle\nabla\psi(x),x^{\prime} -x\rangle\). When \(\psi\) is \(\sigma\)-strongly convex, \(D_{\psi}(x^{\prime},x)\geq\frac{\sigma}{2}\left\|x^{\prime}-x\right\|^{2}\) for any \(x^{\prime}\), \(x\in X\). Specifically, when \(\psi(x)=\frac{1}{2}\left\|x\right\|^{2}\), \(D_{\psi}(x^{\prime},x)=\frac{1}{2}\left\|x^{\prime}-x\right\|^{2}\).

**Lemma 24**.: _Let \(X\) be a convex set. If \(x^{\prime}=\operatorname*{argmin}_{\bar{x}\,\in\,X}\langle\bar{x},g\rangle+D_ {\psi}(\bar{x},x)\), then for any \(x^{\star}\in X\),_

\[\langle x^{\prime}-x^{\star},g\rangle\ \leq\ D_{\psi}(x^{\star},x)-D_{\psi}(x^{ \star},x^{\prime})-D_{\psi}(x^{\prime},x).\]

Proof.: See [38, Lemma 10]. 

**Lemma 25**.: _Assume that \(D_{\psi}(x,x^{\prime})\geq\frac{1}{2}\left\|x-x^{\prime}\right\|_{p}^{2}\) for some \(\psi\) and \(p\geq 1\). If_

\[x_{1}\ =\ \operatorname*{argmin}_{\bar{x}\,\in\,X}\,\langle\bar{x},g_{1} \rangle+D_{\psi}(\bar{x},x)\ \text{and}\ \ x_{2}\ =\ \operatorname*{argmin}_{\bar{x}\,\in\,X}\,\langle\bar{x},g_{2} \rangle+D_{\psi}(\bar{x},x)\]

Figure 16: Convergence performance of policy-based ReLOAD [40] with stepsize \(\eta\): (\(\eta=0.05\),...), (\(\eta=0.1\), --), (\(\eta=0.2\), --). The horizontal axis represents the policy iterations \(\{\pi_{t}\}_{t\,\geq\,0}\) that are generated by ReLOAD and the vertical axis means the policy optimality gap that measures the distance of the policy iterates \(\{\pi_{t}\}_{t\,\geq\,0}\) to an optimal policy \(\pi^{\star}\colon\sum_{s}\text{KL}(\pi^{\star}(\cdot\,|\,s),\pi_{t}(\cdot\,|\, s))\). In this experiment, we take the stepsize \(\eta\) among \(0.05\), \(0.1\), and \(0.2\), and the uniform initial distribution \(\rho\).

[MISSING_PAGE_FAIL:63]