# Minimum-Risk Recalibration of Classifiers

Zeyu Sun

University of Michigan

zeyusun@umich.edu

&Dogyoon Song

University of Michigan

dogyoons@umich.edu

&Alfred Hero

University of Michigan

hero@eecs.umich.edu

Equal contribution.

###### Abstract

Recalibrating probabilistic classifiers is vital for enhancing the reliability and accuracy of predictive models. Despite the development of numerous recalibration algorithms, there is still a lack of a comprehensive theory that integrates calibration and sharpness (which is essential for maintaining predictive power). In this paper, we introduce the concept of minimum-risk recalibration within the framework of mean-squared-error (MSE) decomposition, offering a principled approach for evaluating and recalibrating probabilistic classifiers. Using this framework, we analyze the uniform-mass binning (UMB) recalibration method and establish a finite-sample risk upper bound of order \(\tilde{O}(B/n+1/B^{2})\) where \(B\) is the number of bins and \(n\) is the sample size. By balancing calibration and sharpness, we further determine that the optimal number of bins for UMB scales with \(n^{1/3}\), resulting in a risk bound of approximately \(O(n^{-2/3})\). Additionally, we tackle the challenge of label shift by proposing a two-stage approach that adjusts the recalibration function using limited labeled data from the target domain. Our results show that transferring a calibrated classifier requires significantly fewer target samples compared to recalibrating from scratch. We validate our theoretical findings through numerical simulations, which confirm the tightness of the proposed bounds, the optimal number of bins, and the effectiveness of label shift adaptation.

## 1 Introduction

Generating reliable probability estimates alongside accurate class labels is crucial in classification tasks. A probabilistic classifier is considered "well calibrated" when its predicted probabilities closely align with the empirical frequencies of the corresponding labels [9]. Calibration is highly desirable, particularly in high-stakes applications such as meteorological forecasting [32, 34, 10, 15], econometrics [16], personalized medicine [25, 24], and natural language processing [37, 7, 11, 53]. Unfortunately, many machine learning algorithms lack inherent calibration [18].

To tackle this challenge, various methods have been proposed for designing post hoc recalibration functions. These functions are used to assess calibration error [52, 42, 17, 4], detect miscalibration [30], and provide post-hoc recalibration [50, 51, 18, 48, 29, 21]. Despite the rapid development of recalibration algorithms, there is still a lack of a comprehensive theory that encompasses both calibration and sharpness (retaining predictive power) from a principled standpoint. Furthermore, existing methods often rely on diverse calibration metrics [17, 4], and the selection of hyperparameters is often based on heuristic approaches [44, 20] without rigorous justifications. This highlights theneed for identifying an optimal metric to evaluate calibration, which can facilitate the development of a unified theory and design principles for recalibration functions.

In addition, the deployment of machine learning models to data distributions that differ from the training phase is increasingly common. These distribution shifts can occur naturally due to factors such as seasonality or other variations, or they can be induced artificially through data manipulation methods such as subsampling or data augmentation. Distribution shifts pose challenges to the generalization of machine learning models. Therefore, it becomes necessary to adapt the trained model to these new settings. One significant category of distribution shifts is label shift, where the marginal probabilities of the classes differ between the training and test sets while the class conditional feature distributions remain the same. With calibrated probabilistic predictions, label shift can be adjusted assuming access to class marginal probabilities [12]. However, miscalibration combined with label shift is common and remains a challenging problem [2; 14; 47; 41].

In this paper, we aim to address these issues in a twofold manner. Firstly, we develop a unified framework for recalibration that incorporates both calibration and sharpness in a principled manner. Secondly, we propose a composite estimator for recalibration in the presence of label shift that converges to the optimal recalibration. Our framework enables the adaptation of a classifier to the label-shifted domain in a sample-efficient manner.

### Related work

Recalibration algorithms.Recalibration methods can be broadly categorized into parametric and nonparametric approaches. Parametric methods model the recalibration function in a parametric form and estimate the parameters using calibration data. Examples of parametric methods include Platt scaling [40], temperature scaling [18], Beta calibration [28], and Dirichlet calibration [27]. However, it has been reported that scaling methods are often less calibrated than supposed, and quantifying the degree of miscalibration can be challenging [29]. In contrast, nonparametric recalibration methods do not assume a specific parametric form for the recalibration function. These methods include histogram binning [50], isotonic regression [51], kernel density estimation [52; 42], splines [23], Gaussian processes [49], among others. Hybrid approaches, integrating both parametric and nonparametric techniques, have also been proposed. For instance, Kumar et al. [29] combine nonparametric histogram binning with parametric scaling to reduce variance and improve recalibration performance. Nevertheless, this hybrid approach is biased when its parametric assumptions fail. In this work, we consider a nonparametric histogram binning method called uniform-mass binning (UMB), which is asymptotically unbiased.

Histogram binning method.Histogram binning methods are widely used for recalibration due to their simplicity and adaptability. The binning schemes can be pre-specified (e.g., uniform-width binning [18]), data-dependent (e.g., uniform-mass binning [50]), or algorithm-induced [51]. When selecting a binning scheme, it is crucial to consider the trade-off between approximation and estimation. Coarser binning reduces estimation error (variance), leading to improved calibration, but at the expense of increased approximation error (bias), which diminishes sharpness. Thus, determining the optimal binning scheme and hyperparameters, such as the number of bins (\(B\)), remains an active area of research. [36] proposes a Bayesian binning method, but verifying the priors is often challenging. [44] suggests choosing the largest \(B\) that preserves monotonicity, which is heuristic and computationally inefficient. [20] offers a heuristic for choosing the largest \(B\) subject to a calibration constraint, lacking a quantitative characterization of sharpness. Our work builds upon the existing upper bounds for calibration risks of binning methods [29; 20] and derived upper bounds for a complementary risk component known as the sharpness risk. We quantitatively characterize the calibration-sharpness tradeoff, which yields an optimal choice for the number of bins that achieves the minimum risk.

Adaptation to label shift.Label shift presents a challenge in generalizing models trained on one distribution (source) to a different distribution (target). As such, adapting to label shift has received considerable attention in the literature [12; 45; 31; 3; 2; 14]. In practical scenarios, it is common to encounter model miscalibration and label shift simultaneously [47; 41]. Empirical observations have highlighted the crucial role of probability calibration in label shift adaptation [2; 13], which is justified by subsequent theories [14]. However, to the best of our knowledge, there has been no prior work that specifically addresses the recalibration with a limited amount of labeled data from the target distribution. Our theoretical analysis points out that using only target labels achieves risk bounds of the same order as the methods using only target features [31; 3; 14].

### Contributions

This paper contributes to the theory of recalibration across three key dimensions.

Firstly, we develop a comprehensive theory for recalibration in binary classification by adopting the mean-squared-error (MSE) decomposition framework commonly used in meteorology and space weather forecasting [5; 33; 8; 46]. Our approach formulates the probability recalibration problem as the minimization of a specific risk function, which can be orthogonally decomposed into calibration and sharpness components.

Secondly, utilizing the aforementioned framework, we derive a rigorous upper bound on the finite-sample risk for uniform-mass binning (UMB) (Theorem 1). Furthermore, we minimize this risk bound and demonstrate that the optimal number of bins for UMB, balancing calibration and sharpness, scales on the order of \(n^{1/3}\), yielding the risk bound of order \(n^{-2/3}\), where \(n\) denotes the sample size.

Lastly, we address the challenge of recalibrating classifiers for label shift when only a limited labeled sample from the target distribution is available, a challenging situation for a direct recalibration approach. We propose a two-stage approach: first recalibrating the classfier on the abundant source-domain data, and then transfering it to the label-shifted target domain. We provide a finite-sample guarantee for the risk of this composite procedure (Theorem 2). Notably, to control the risk under \(\varepsilon\), our approach requires a much smaller sample size from the target distribution than a direct recalibration on the target sample (\(\Omega(\varepsilon^{-1})\) vs. \(\Omega(\varepsilon^{-3/2})\), cf. Remark 4).

### Organization

This paper is organized as follows. In Section 2, we introduce notation and provide an overview of calibration and sharpness. Section 3 introduces the notion of minimum-risk recalibration by defining the recalibration risk that takes into account both calibration and sharpness. In Section 4, we describe the uniform-mass histogram binning method for recalibration and provide a risk upper bound with rate analysis. We extend our approach to handle label shift in Section 5. To validate our theory and framework, we present numerical experiments in Section 6. Finally, in Section 7, we conclude the paper with a discussion and propose future research directions.

## 2 Preliminaries

### Notation

Let \(\mathbb{N}\) and \(\mathbb{R}\) denote the set of positive integers and the set of real numbers, respectively. For \(n\in\mathbb{N}\), let \([n]:=\{1,\ldots,n\}\). For \(x\in\mathbb{R}\), let \(\lfloor x\rfloor=\max\{m\in\mathbb{Z}:m\leq x\}\). For any finite set \(\mathcal{S}=\{s_{i}:i\in[n]\}\subset\mathbb{R}\) and any \(k\in[n]\), we let \(s_{(k)}\) denote the \(k\)-th order statistic, which is the \(k\)-th smallest element in \(\mathcal{S}\).

Let \((\Omega,\mathcal{E},P)\) denote a generic probability space. For an event \(A\in\mathcal{E}\), the indicator function \(\mathds{1}_{A}:\Omega\to\{0,1\}\) is defined such that \(\mathds{1}_{A}(x)=1\) if and only if \(x\in A\). We write \(A\) happens \(P\)-almost surely if \(P(A)=1\). For a probability measure \(P\), define \(\mathbb{E}_{P}\) as the expectation, with subscript \(P\) omitted when the underlying probability measure is clear. For a probability measure \(P\) and a random variable \(X:\Omega\to\mathbb{R}^{d}\), let \(P_{X}:=P\circ X^{-1}\) be the probability measure induced by \(X\).

Letting \(f,g:\mathbb{R}\to\mathbb{R}\), we write \(f(x)=O(g(x))\) as \(x\to\infty\) if there exist \(M>0\) and \(x_{0}>0\) such that \(|f(x)|\leq Mg(x)\) for all \(x\geq x_{0}\). Likewise, we write \(f(x)=\Omega(g(x))\) if \(g(x)=O(f(x))\). We write \(f(x)\asymp g(x)\) if \(f(x)=O(g(x))\) and \(g(x)=O(f(x))\). We write \(f(x)=\tilde{O}(g(x))\) if there is \(k\geq 1\) such that \(f(x)=O(g(x)\log^{k}(g(x)))\).

### Calibration and sharpness

Consider the binary classification problem; let \(X\in\mathcal{X}\) and \(Y\in\mathcal{Y}:=\{0,1\}\) denote the feature and label random variables. Letting \(P\) denote a probability measure, we want to construct a function \(f:\mathcal{X}\to\mathcal{Z}=[0,1]\) that estimates the conditional probability, _i.e._,

\[f(X)\approx P[Y=1\mid X].\] (1)Since estimating the probability conditioned on the high dimensional \(X\) is difficult, the notion of calibration captures the intuition of (1) in a weaker sense [33; 9; 19].

**Definition 1**.: _A function \(f:\mathcal{X}\to\mathcal{Z}\) is (perfectly) calibrated with respect to probability measure \(P\), if_

\[f(X)=P\left[Y=1\mid f(X)\right]\quad\text{ $P$-almost surely.}\]

Calibration itself does not guarantee a useful predictor. For instance, a constant predictor \(f(X)=\mathbb{E}Y\) is perfectly calibrated, but it does not change with the features. Such a predictor lacks sharpness [26], also known as resolution [35], another desired property which measures the variance in the target \(Y\) explained by the probabilistic prediction \(f(X)\).

**Definition 2**.: _The sharpness of a function \(f:\mathcal{X}\to\mathcal{Z}\) with respect to probability measure \(P\) refers to the quantity_

\[\mathrm{Var}\big{(}\mathbb{E}[Y\mid f(X)]\big{)}=\mathbb{E}\Big{[}\big{(} \mathbb{E}[Y\mid f(X)]-\mathbb{E}[Y]\big{)}^{2}\Big{]}.\]

The following decomposition of the mean squared error (MSE) suggests why it is desirable for a classifier \(f\) to be calibrated and have high sharpness; note that \(\mathrm{Var}[\overline{Y}]\) is a quantity intrinsic to the problem, unrelated to \(f\).

\[\mathrm{MSE}(f):=\underbrace{\mathbb{E}\big{[}\big{(}Y-f(X)\big{)}^{2}\big{]} }_{\text{mean-squared error}}=\mathrm{Var}[Y]-\underbrace{\mathrm{Var}\left( \mathbb{E}\big{[}Y\mid f(X)\big{]}\right)}_{\text{sharpness}}+\underbrace{ \mathbb{E}\big{[}\big{(}f(X)-\mathbb{E}[Y\mid f(X)]\big{)}^{2}\big{]}}_{ \text{lack of calibration}}\] (2)

## 3 Optimal recalibration

For an arbitrary predictor \(f:\mathcal{X}\to\mathcal{Z}\), the aim of recalibration is to identify a post-processing function \(h:\mathcal{Z}\to\mathcal{Z}\) such that \(h\circ f\) is perfectly calibrated while maintaining the sharpness of \(f\) as much as possible. The calibration and sharpness can be evaluated using the following two notions of risks. We suppress the dependency of risks on \(f\) and \(P\) when it leads to no confusion.

**Definition 3**.: _Let \(f:\mathcal{X}\to\mathcal{Z}\) and \(h:\mathcal{Z}\to\mathcal{Z}\). The calibration risk of \(h\) over \(f\) under \(P\) is defined as_

\[R^{\mathrm{cal}}(h)=R_{P}^{\mathrm{cal}}(h;f):=\mathbb{E}_{P}\left[\big{(}h \circ f(X)-\mathbb{E}_{P}\left[Y\mid h\circ f(X)\right]\big{)}^{2}\right].\] (3)

**Definition 4**.: _Let \(f:\mathcal{X}\to\mathcal{Z}\) and \(h:\mathcal{Z}\to\mathcal{Z}\). The sharpness risk of \(h\) over \(f\) under \(P\) is defined as_

\[R^{\mathrm{eha}}(h)=R_{P}^{\mathrm{sha}}(h;f):=\mathbb{E}_{P}\left[\big{(} \mathbb{E}_{P}\left[Y\mid h\circ f(X)\right]-\mathbb{E}_{P}\left[Y\mid f(X) \right]\big{)}^{2}\right].\] (4)

Note that the calibration risk \(R^{\mathrm{cal}}(h;f)=0\) if and only if \(h\circ f\) is perfectly calibrated, cf. Definition 1. The sharpness risk \(R^{\mathrm{sha}}(h;f)\) quantifies the decrement in sharpness of \(f\) incurred by applying the recalibration map \(h\), and \(R^{\mathrm{sha}}(h;f)=0\) when \(h\) is injective [29].

Next we define a comprehensive notion of risk that we will use to evaluate recalibration functions.

**Definition 5**.: _Let \(f:\mathcal{X}\to\mathcal{Z}\) and \(h:\mathcal{Z}\to\mathcal{Z}\). The recalibration risk of \(h\) over \(f\) under \(P\) is defined as_

\[R(h)=R_{P}(h;f):=\mathbb{E}_{P}\left[(h\circ f(X)-\mathbb{E}_{P}[Y\mid f(X)])^ {2}\right].\] (5)

The following proposition shows that the recalibration risk can be decomposed into calibration risk and sharpness risk. The proof is deferred to Appendix A.

**Proposition 1** (Decomposition of recalibration risk).: _For any \(f:\mathcal{X}\to\mathcal{Z}\) and any \(h:\mathcal{Z}\to\mathcal{Z}\),_

\[R_{P}(h;f)=R_{P}^{\mathrm{cal}}(h;f)+R_{P}^{\mathrm{sha}}(h;f).\] (6)

Note that \(R_{P}(h)=0\) if and only if \(R_{P}^{\mathrm{cal}}(h)=0\) and \(R_{P}^{\mathrm{sha}}(h)=0\). This happens if and only if \(h\circ f\) is calibrated, and the recalibration \(h\) preserves the sharpness of \(f\) in predicting \(Y\).

If \(R(h)=0\), then we call \(h\) an _optimal recalibration function_ (or _minimum-risk recalibration function_) of \(f\) under \(P\). Let \(h_{f,P}^{*}:\mathcal{Z}\to\mathcal{Z}\) be the function

\[h_{f,P}^{*}(z)=\begin{cases}\mathbb{E}_{P}[Y\mid f(X)=z],&\text{if $z\in \mathrm{supp}\,P_{f(X)}$},\\ 0,&\text{if $z\not\in\mathrm{supp}\,P_{f(X)}$}.\end{cases}\] (7)Then \(R(h^{*}_{f,P})=0\). Indeed, \(h\) is a minimum-risk recalibration function of \(f\) if and only if \(h=h^{*}_{f,P}\)\(P_{Z}\)-almost surely.

**Problem 1** (Recalibration).: _Suppose that we have a measurable function \(f:\mathcal{X}\to\mathcal{Z}\) and a dataset \(\mathcal{D}=\left\{(x_{i},y_{i}):i\in[n]\right\}\) that is an independent and identically distributed (IID) sample drawn from \(P\). The goal of_ recalibration _is to estimate a recalibration function \(\hat{h}\approx h^{*}_{f,P}\) using \(f\) and \(\mathcal{D}\)._

Problem 1 can be viewed as a regression problem, where we estimate the function form of \(\mathbb{E}[Y\mid Z]\) from data \(\{(z_{i},y_{i}):i\in[n]\}\), where \(z_{i}=f(x_{i})\), \(\forall i\in[n]\).

## 4 Recalibration via uniform-mass binning

### Uniform-mass binning algorithm for recalibration

Given a dataset of prediction-label pairs \(\{(z_{i},y_{i})\in\mathcal{Z}\times\mathcal{Y}:i\in[n]\}\), the histogram binning calibration method partitions \(\mathcal{Z}=[0,1]\) into a set of smaller bins, and estimate \(\mathbb{E}[Y\mid Z]\) by taking the average in each bin. We consider the uniform mass binning, constructed using quantiles of predicted probabilities.

**Definition 6** (Uniform mass binning).: _Let \(S=\{z_{i}\in[0,1]:i\in[n]\}\). A binning scheme \(\mathcal{B}=\{I_{1},I_{2},\ldots,I_{B}\}\) is the \(\operatorname{uniform-mass\ binning}\) (UMB) scheme induced by \(S\) if_

\[I_{1}=[u_{0},u_{1}],\qquad\mathrm{and}\qquad I_{b}=(u_{b-1},u_{b}]\;\;\forall b \in[B]\setminus\{1\},\] (8)

_where \(u_{0}=0\), \(u_{B}=1\), and \(u_{b}=z_{(\lfloor nb/B\rfloor)}\) for \(b\in[B-1]\)._

For our subsequent discussions, we make the following assumptions on the distribution of \(Y\) and \(Z\):

* The cumulative distribution function of \(Z\), denoted by \(F_{Z}\), is absolutely continuous.
* \(h^{*}_{f,P}\), defined in (7), is monotonically non-decreasing on \(\operatorname{supp}P_{Z}\).
* There exists \(K>0\) such that if \(z_{1}\leq z_{2}\), then \(h^{*}_{f,P}(z_{2})-h^{*}_{f,P}(z_{1})\leq K\cdot\big{(}F_{Z}(z_{2})-F_{Z}(z_{ 1})\big{)}\).

Assumption (A1) is made for the sake of analytical convenience without loss of generality; see discussions in [20, Appendix C]. An important implication of Assumption (A1) is that all intervals in \(\mathcal{B}\) are non-empty \(P_{Z}\)-almost surely if \(S=\{Z_{i}:i\in[n]\}\) are IID under \(P_{Z}\). Assumption (A2) makes sure \(Z\) is informative to preserve the rankings of \(P[Y\mid Z]\)[51]. Lastly, Assumption (A3) posits that \(Z\) is sufficiently informative that \(P[Y=1\mid Z=z]\) does not change to rapidly in any interval \(I\) where \(P_{Z}(I)\) is small. This assumption is mild but not trivial; see Appendix B.2.

Now we describe how to construct a recalibration function \(\hat{h}\) using UMB.

**Algorithm.**

1. Given \(\{(z_{i},y_{i}):i\in[n]\}\), and \(B\in\mathbb{N}\), let \(\mathcal{B}=\{I_{1},I_{2},\ldots,I_{B}\}\) be the UMB scheme of size \(B\) induced by \(\{z_{i}:i\in[n]\}\).
2. Let \(\hat{h}=\hat{h}_{\mathcal{B}}:\mathcal{Z}\to\mathcal{Z}\) be a function such that \[\hat{h}(z)=\sum_{I\in\mathcal{B}}\hat{\mu}_{I}\cdot\mathds{1}_{I}(z)\quad \text{where}\quad\hat{\mu}_{I}:=\frac{\sum_{i=1}^{n}y_{i}\cdot\mathds{1}_{I}(z _{i})}{\sum_{i=1}^{n}\mathds{1}_{I}(z_{i})},\;\;\forall I\in\mathcal{B}.\] (9)

### Theoretical guarantees

Here we establish a high-probability upper bound on the recalibration risk \(R(\hat{h})\) for the UMB estimator, \(\hat{h}\) defined in (9), which converges to \(0\) as the sample size \(n\) increases to \(\infty\).

**Theorem 1**.: _Let \(P\) be a probability measure and \(f:\mathcal{X}\to\mathcal{Z}\) be a measurable function. Suppose that (A1) & (A2) hold. Let \(\mathcal{B}\) be the UMB scheme induced by an IID sample of \(f(X)\), and let \(\hat{h}=\hat{h}_{\mathcal{B}}\) be the recalibration function based on \(\mathcal{B}\), cf. (9). Then there exists a universal constant \(c>0\) such that for any \(\delta\in(0,1)\), if \(n\geq c\cdot|\mathcal{B}|\log\big{(}\frac{2|\mathcal{B}|}{\delta}\big{)}\), then with probability at least \(1-\delta\),_

\[R^{\mathrm{cal}}(\hat{h})\leq\left(\sqrt{\frac{\log\left(4|\mathcal{B}|/\delta \right)}{2(\lfloor n/|\mathcal{B}|\rfloor-1)}}+\frac{1}{\lfloor n/|\mathcal{B} |\rfloor}\right)^{2},\quad\text{and}\quad R^{\mathrm{sha}}(\hat{h})\leq \begin{cases}\frac{2}{|\mathcal{B}|},\\ \frac{8K^{2}}{|\mathcal{B}|^{2}}\end{cases}\text{ if \eqref{eq:R^{\mathrm{sha}} holds}.}\]

**Remark 1**.: _We note that the upper bound on \(R^{\mathrm{cal}}(\hat{h})\) in Theorem 1 coincides with the result presented in [20] up to a constant factor in the failure probability. However, Theorem 1 provides an additional upper bound on \(R^{\mathrm{sha}}(\hat{h})\), thereby effectively managing the overall recalibration risk \(R(\hat{h})\)._

Optimal choice of the number of bins \(|\mathcal{B}|\).Based on Theorem 1, for any fixed sample size \(n\), as the number of bins \(B=|\mathcal{B}|\) increases, the calibration risk bound increases, while the sharpness risk bound decreases. This trade-off suggests we may get an optimal number of bins \(B\) by minimizing the upper bound for the overall risk, \(R(\hat{h})=R^{\mathrm{cal}}(\hat{h})+R^{\mathrm{sha}}(\hat{h})\).

For the simplicity of our analysis, we assume \(n\) is divisible by \(B\), and Assumption (A3) holds. Firstly, observe that \(n/B\geq 2\), and thus, \(n/B-1\geq n/(2B)\). Also, since \(B\geq 1\) and \(\delta<1\), \(\log(\frac{4B}{\delta})\geq 1\), it follows that \(\frac{1}{n/B}\leq\sqrt{\frac{1}{2(n/B-1)}\log\left(\frac{4B}{\delta}\right)} \leq\sqrt{\frac{1}{n/B}\log\left(\frac{4B}{\delta}\right)}\). Thus, we obtain a simplified risk bound \(R(\hat{h})\leq\zeta(B;n,\delta)\) where \(\zeta(B;n,\delta):=\frac{4B}{n}\log\left(\frac{4B}{\delta}\right)+\frac{8K^{2 }}{B^{2}}\). With \(\log B\) terms ignored, minimizing \(\zeta(B;n,\delta)\) over \(B\) yields optimal \(B^{*}\) and resulting risk bounds, respectively:

\[B^{*}\asymp n^{1/3}\cdot\log^{-1/3}(1/\delta),\] (10) \[R(\hat{h})\asymp R^{\mathrm{cal}}(\hat{h})\asymp R^{\mathrm{sha }}(\hat{h})=O\left({B^{*}}^{-2}\right)=O\left({n}^{-2/3}\cdot\log^{2/3}(1/ \delta)\right).\] (11)

Furthermore, the asymptotic risk bound in (11) implies that \(R(\hat{h})\), \(R^{\mathrm{cal}}(\hat{h})\), and \(R^{\mathrm{sha}}(\hat{h})\) are all bounded by \(\varepsilon\) with high probability if the sample size

\[n=\Omega(\varepsilon^{-3/2}).\] (12)

Comparison with the hybrid method [29].Kumar et al. [29] proposed a hybrid recalibration approach that involves fitting a recalibration function in a parametric family \(\mathcal{H}\), which is then discretized using UMB. Note that the hypothesis class \(\mathcal{H}\) may or may not include the optimal recalibration function \(h_{f,P}^{*}\). If \(h_{f,P}^{*}\in\mathcal{H}\), using a similar analysis as above, we derive their high probability risk bound as \(O\left(\frac{1}{n}\log\frac{B}{\delta}+\frac{1}{B^{2}}\right)\) under Assumption (A3), which achieves \(\tilde{O}(n^{-1})\) when \(B^{*}\asymp\sqrt{n}\), exhibiting a faster decay than our \(R(\hat{h})=O(n^{-2/3})\). While the faster rate is anticipated from employing parametric methods, we note that when \(h_{f,P}^{*}\notin\mathcal{H}\), their method exhibits inherent bias (approximation error) induced by parametric function fitting, whereas our method is asymptotically unbiased. This distinction is corroborated by numerical simulations in Appendix E.2.

Proof sketch of Theorem 1.First, we demonstrate that the uniform-mass binning scheme \(\mathcal{B}=\{I_{b}\}_{b=1}^{B}\) satisfies two regularity conditions with high probability, when the sample size \(n\) is not too small. Specifically, we show that (i) \(\mathcal{B}\) is 2-well-balanced [29] with respect to \(f(X)\), resulting in \(B\) bins having comparable probabilities (Lemma 3); and that (ii) the empirical mean in each bin of \(\mathcal{B}\) uniformly concentrates to the population conditional mean of \(Y\) conditioned on \(f(X)\) being contained within the bin (Lemma 4). Thereafter, we prove that if \(\mathcal{B}\) satisfies these two properties, then the calibration risk \(R^{\mathrm{cal}}\) and the sharpness risk \(R^{\mathrm{sha}}\) can be upper bounded as stated in Theorem 1; see Lemmas 5 and 6 (or Lemma 7 when (A3) holds). The detailed proof is in Appendix B.

## 5 Recalibration under label shift

This section extends the results from Section 4 to address label shift. In Section 5.1, we introduce the label shift assumption (Definition 7) and reframe the recalibration problem accordingly. We show that the optimal recalibration function in this context can be expressed as a composition of the optimal recalibration function (cf. Section 3) and a shift correction function. Building on this observation, we propose a two-stage estimator in Section 5.2, where each stage estimates one of the component functions. The composite estimator's overall performance is supported by theoretical guarantees.

### Revisiting the problem formulation

Let \(P\) and \(Q\) denote the probability measures of the source and the target domains, respectively. We assume \(P\) and \(Q\) satisfy the label shift assumption defined below.

**Definition 7** (Label shift).: _Probability measures \(P\) and \(Q\) are said to satisfy the label shift assumption if the following two conditions are satisfied:_

* \(P[X\in B\mid Y=k]=Q[X\in B\mid Y=k]\) _for all_ \(k\in\{0,1\}\) _and all_ \(B\in\mathcal{B}(\mathcal{X})\)_._
* \(P[Y=1]\in(0,1)\) _and_ \(Q[Y=1]\in(0,1)\)_._

According to Condition (B1), the class conditional distributions remain the same, while the marginal distribution of the classes may change. Condition (B2) requires all classes to be present in the source population, which is a standard regularity assumption in the discussion of label shift [31, 14]; it also posits the presence of every class in the target population.

Optimal recalibration under label shift.Under the label shift assumption between \(P\) and \(Q\), we define the label shift correction function \(g^{*}:\mathcal{Z}\to\mathcal{Z}\) such that

\[g^{*}(z)=\frac{w_{1}^{*}z}{w_{1}^{*}z+w_{0}^{*}(1-z)}\qquad\text{where}\qquad w _{k}^{*}=\frac{Q[Y=k]}{P[Y=k]},\ \ \forall k\in\{0,1\}.\] (13)

The conditional probabilities under \(P\) and \(Q\) can be related [45] as follows:

\[Q[Y=1\mid X\in B]=g^{*}\Big{(}P[Y=1\mid X\in B]\Big{)},\qquad\forall B\in \mathcal{B}(\mathcal{X}).\] (14)

Recall that the optimal recalibration function for a predictor \(f:\mathcal{X}\to\mathcal{Z}\) under probability measure \(P\) is defined as \(h_{f,P}^{*}(z)=P[Y=1\mid f(X)=z]\); see (7). In the presence of a label shift between \(P\) and \(Q\), we may write the optimal recalibration function for \(f\) under \(Q\) as

\[h_{f,Q}^{*}=g^{*}\circ h_{f,P}^{*}\] (15)

because \(h_{f,Q}^{*}(z)\ \overset{(a)}{=}\ Q[Y=1\mid f(X)=z]\ \overset{(b)}{=}\ g^{*}\left(P[Y=1\mid f(X)=z]\ \right)\ \overset{(c)}{=}\ \left(g^{*}\circ h_{f,P}^{*}\right)(z)\), where (a) and (c) follows from the definition of \(h_{f,P}^{*}\) and (b) is due to (14).

Recalling the definition of the risk \(R_{P}(h;f)\) from (5), we observe that \(R_{Q}(h_{f,Q}^{*};f)=0\), which is consistent with the risk characterization of the optimal recalibration. Our goal is to estimate the optimal recalibration function \(h_{f,Q}^{*}=g^{*}\circ h_{f,P}^{*}\) from data.

**Problem 2** (Recalibration under label shift).: _Suppose that we have a measurable function \(f:\mathcal{X}\to\mathcal{Z}\) and two IID datasets \(\mathcal{D}_{P}=(x_{i},y_{i})_{i=1}^{n_{P}}\sim P\) and \(\mathcal{D}_{Q}=(x_{i}^{\prime},y_{i}^{\prime})_{i=1}^{n_{Q}}\sim Q\). The goal of_ recalibration under label shift _is to estimate \(\hat{h}\approx h_{f,Q}^{*}\) using \(f\), \(\mathcal{D}_{P}\) and \(\mathcal{D}_{Q}\)._

**Remark 2**.: _The source (training) dataset \(\mathcal{D}_{P}\) may not be accessible due to privacy protections, proprietary data, or practical constraints, as is often the case when recalibrating a pre-trained black box classifier to new data. In these cases, it suffices to have estimates of the recalibration function \(h_{f,P}^{*}\) and the marginal probabilities \(P[Y=k],\ k\in\{0,1\}\) under \(P\), for our method and analysis._

### Two-stage recalibration under label shift

Method.We propose a composite estimator of \(h_{f,Q}^{*}=g^{*}\circ h_{f,P}^{*}\), which comprises two estimators \(\hat{g}\approx g^{*}\) and \(\hat{h}_{P}\approx h_{f,P}^{*}\). Here we describe a procedure to produce this composite estimator.

1. Use \(\mathcal{D}_{P}\) to construct \(\hat{h}_{P}:\mathcal{Z}\to\mathcal{Z}\), the estimated recalibration function (9) (for \(f\) under \(P\)).
2. Use \(\mathcal{D}_{P}\) and \(\mathcal{D}_{Q}\) to construct \(\hat{g}:\mathcal{Z}\to\mathcal{Z}\) such that \[\hat{g}(z)=\frac{\hat{w}_{1}z}{\hat{w}_{1}z+\hat{w}_{0}(1-z)}\qquad\text{where} \qquad\hat{w}_{k}=\frac{\hat{Q}[Y=k]}{\hat{P}[Y=k]},\ \ \forall k\in\{0,1\},\] (16) where \(\hat{P}[Y=k]:=\frac{1}{|\mathcal{D}_{P}|}\sum_{i=1}^{|\mathcal{D}_{P}|}\mathds{1 }[y_{i}=k]\) and \(\hat{Q}[Y=k]:=\frac{1}{|\mathcal{D}_{Q}|}\sum_{i=1}^{|\mathcal{D}_{Q}|} \mathds{1}[y_{i}^{\prime}=k]\) are the empirical estimates of the class marginal probabilities.
3. Let \[\hat{h}_{Q}=\hat{g}\circ\hat{h}_{P}.\] (17)

Note that the recalibration estimator \(\hat{h}_{P}\) (Step 1) remains the same with that in Section 4.1. Furthermore, the shift correction estimator \(\hat{g}\) (Step 2) is a plug-in estimator of the label shift correction function \(g^{*}\) in (14) based on the estimated weights, \(\hat{w}_{1}\) and \(\hat{w}_{0}\).

Theory.We present a recalibration risk upper bound for the proposed two-stage estimator. We let \(p_{k}:=P[Y=k]\), \(q_{k}:=Q[Y=k]\), and \(w^{*}_{k}=\frac{q_{k}}{p_{k}}\) for \(k\in\{0,1\}\). Moreover, we let \(p_{\min}:=\min_{k}p_{k}\), \(q_{\min}:=\min_{k}p_{k}\), \(w^{*}_{\min}:=\min_{k}w^{*}_{k}\) and \(w^{*}_{\max}:=\max_{k}w^{*}_{k}\).

**Theorem 2** (Convergence of \(\hat{h}_{Q}\)).: _Let \(P,Q\) be probability measures and let \(f:\mathcal{X}\to\mathcal{Z}\) be a measurable function. Let \(\mathcal{D}_{P}\sim P\) be an IID sample of size \(n_{P}\) and \(\mathcal{D}_{Q}\sim Q\) be an IID sample of size \(n_{Q}\). Suppose that Assumptions (B1) & (B2) hold. Let \(\mathcal{B}\) be the UMB scheme induced by \(\mathcal{D}_{P}\). Let \(\hat{h}_{P}=\hat{h}_{P,\mathcal{B}}\) be the recalibration function (9) based on \(\mathcal{B}\), and let \(\hat{g}\) denote the shift correction function as defined in (16). Then_

\[R_{Q}\big{(}\hat{g}\circ\hat{h}_{P}\big{)}\leq 2\left\{\left(\frac{\rho_{0}- \rho_{1}}{\rho_{0}+\rho_{1}}\right)^{2}+\frac{{w^{*}_{\max}}^{3}}{{w^{*}_{ \min}}^{2}}\cdot R_{P}\big{(}\hat{h}_{P};f\big{)}\right\}\qquad\text{where} \qquad\rho_{k}:=\frac{\hat{w}_{k}}{w^{*}_{k}},\;\;k\in\{0,1\}.\] (18)

_Furthermore, suppose that (A1), (A2) & (A3) hold. Then there exists a universal constant \(c>0\) such that for any \(\delta\in(0,1)\), if_

\[n_{P}\geq\max\left\{c,\;\frac{27}{p_{\min}}\right\}\cdot|\mathcal{B}|\log \left(\frac{4|\mathcal{B}|}{\delta}\right)\qquad\text{and}\qquad n_{Q}\geq \frac{27}{q_{\min}}\log\left(\frac{16}{\delta}\right),\]

_then with probability at least \(1-\delta\),_

\[R_{Q}\big{(}\hat{g}\circ\hat{h}_{P}\big{)}\leq 2\frac{{w^{*}_{ \max}}^{3}}{{w^{*}_{\min}}^{2}}\cdot\left\{\left(\sqrt{\frac{1}{2(\lfloor n_{ P}/|\mathcal{B}|\rfloor-1)}\log\left(\frac{8|\mathcal{B}|}{\delta}\right)}+ \frac{1}{\lfloor n_{P}/|\mathcal{B}|\rfloor}\right)^{2}+\frac{8K^{2}}{| \mathcal{B}|^{2}}\right\}\\ +54\max\left\{\frac{1}{p_{\min}\cdot n_{P}},\;\frac{1}{q_{\min} \cdot n_{Q}}\right\}\cdot\log\left(\frac{16}{\delta}\right).\] (19)

**Remark 3**.: _Note that \(\rho_{k}\to 1\) as \(n_{P},n_{Q}\to\infty\), and thus, the upper bound (18) reduces to \(2\frac{{w^{*}_{\max}}^{3}}{{w^{*}_{\min}}^{2}}\cdot R_{P}\big{(}\hat{h}_{P};f \big{)}\). Moreover, when \(P=Q\), we have \(w^{*}_{\min}=w^{*}_{\max}=1\), and this further simplifies to the recalibration risk without label shift, up to multiplicative constant \(2\)._

**Remark 4** (Target sample complexity).: _Assume that \(n_{P}\geq n_{Q}\) and the number of bins satisfies \(|\mathcal{B}|\asymp n_{P}^{1/3}\). Then (19) implies \(R_{Q}\big{(}\hat{g}\circ\hat{h}_{P};f\big{)}=O\big{(}n_{P}^{-2/3}+n_{Q}^{-1} \big{)}\). This result indicates that the proposed recalibration method using (17) requires a significantly smaller target sample size \(n_{Q}=\Omega(\varepsilon^{-1})\) to control the risk, as compared to \(n_{Q}=\Omega(\varepsilon^{-3/2})\) in (12)._

**Remark 5** (Comparison with label shift using unlabeled target data).: _When the source sample size \(n_{P}\) is sufficiently large, we achieve a risk of \(R_{Q}(\hat{g}\circ\hat{h}_{P})=O(n_{Q}^{-1})\) with high probability. It is important to note that in this scenario, we only utilize the labels from the target sample to address label shift. Remarkably, the same rate applies when employing the algorithms proposed in [31, 3, 14], which solely rely on features from the target sample. For a proof sketch, please refer to Appendix D._

## 6 Numerical experiments

In this section, we present the results of our numerical simulations conducted to validate and reinforce the theoretical findings discussed earlier. The simulations are based on a family of recalibration functions called beta calibration [28]: \(\mathcal{H}_{\text{beta}}=\{h_{\text{beta}}(z;a,b,c):a\geq 0,b\geq 0,c\in\mathbb{R}\}\), where \(h_{\text{beta}}(\cdot;a,b,c):[0,1]\to[0,1]\) is defined as

\[h_{\text{beta}}(z;a,b,c)=\frac{1}{1+1/\left(e^{c}\frac{z^{a}}{(1-z)^{b}} \right)}.\] (20)

In addition, consider the joint distributions \(\mathcal{D}(\pi)\) of \(X\) and \(Y\), where \(Y\sim\text{Bernoulli}(\pi)\), \(X\mid Y=0\sim N(-2,1)\), and \(X\mid Y=1\sim N(2,1)\), and a pre-trained probabilistic classifier \(f(x)=\sigma(x):=1/(1+e^{-x})\). To accommodate the limitations of space, we summarize the results in Figure 1, 2, 3, and Table 1, 2, providing a concise overview. Detailed information about the simulation settings, implementation details, and further experimental results and discussions can be found in Appendix E. Our simulation code is available at https://github.com/ZeyuSun/calibration_label_shift.

\begin{table}

\end{table}
Table 1: 90%-quantiles of the risks of Platt scaling [40], the hybrid method [29], uniform-width binning (UWB) [18], and uniform-mass binning (UMB) over 100 random calibration datasets drawn from \(Z\sim\text{Uniform}[0,1]\), and **(a)**\(Y\mid Z\sim\text{Bernoulli}(h_{\text{beta}}(z;4,4,0))\), or **(b)**\(Y\mid Z\sim\text{Bernoulli}(h_{\text{beta}}(z;0.1,4,0))\). While Platt scaling and the hybrid method achieve lower \(R\) under the correct parametric assumption, UWB and UMB may outperform when the parametric assumption fails.

\begin{table}

\end{table}
Table 2: Risks under label shift from \(\mathcal{D}(0.5)\) to \(\mathcal{D}(0.1)\), with \(n_{P}=10^{3}\) and \(n_{Q}=10^{2}\). Standard deviations are computed from 10 random realizations. Label-Shift, only applying an injective \(\hat{g}\), achieves \(R^{\text{sha}}=0\) but incurs high \(R^{\text{cal}}\). Source, recalibrated with \(B=n_{P}^{1/3}\) on \(\mathcal{D}_{P}\), incurs high \(R^{\text{cal}}\). Target, recalibrated with \(B=n_{Q}^{1/3}\) on \(\mathcal{D}_{Q}\), incurs low \(R^{\text{cal}}\). Our proposed Composite achieves the lowest nonzero \(R^{\text{sha}}\) and lowest \(R^{\text{cal}}\).

Figure 1: Medians (solid lines) and 10-90 percentile ranges (shaded areas) of quadrature estimates of population risks over 10 realizations and theoretical risk upper bounds (\(\delta=0.1\)) (dashed lines) for various \(n\) and \(B\). **(a)-(c)** The empirical rates, \(R^{\text{cal}}=O(n^{-0.99}B^{0.98})\) and \(R^{\text{sha}}=O(B^{-1.83})\), align with theoretically predicted rates, \(\tilde{O}(B/n)\) and \(O(B^{-2})\), in Thm. 1. **(d)** The empirically observed \(R\) and our upper bound exhibit similar trends as a function of \(B\).

Figure 2: The optimal number of bins \(B\) for different sample sizes \(n\) plotted in linear scale (_left_) and log scale (_middle_), and the population risk for various combinations of \(n\) and \(B\), with the optimal \(B\) marked by black dots (_right_). Note that the risk surface is relatively smooth around its minimum, suggesting the robustness of the optimal \(B\).

## 7 Discussion

This paper presents a comprehensive theory for recalibration, considering both calibration and sharpness within the mean-squared-error (MSE) decomposition framework. We use this framework to quantify the optimal calibration-sharpness balance and establish a rigorous upper bound on the finite-sample risk for uniform-mass binning (UMB). Additionally, we address the challenge of recalibration under label shift with limited access to labeled target data. Our proposed two-stage approach effectively estimates the recalibration function using ample data from the source domain and adjusts for the label shift using target domain data. Importantly, our findings suggest that transferring a calibrated classifier requires a significantly smaller target sample than recalibrating from scratch on the new domain. Numerical simulations confirm the tightness of the finite sample bounds, validate the optimal number of bins, and demonstrate the effectiveness of the label shift adaptation.

In concluding this paper, we identify several promising directions for future research.

Relaxation of the assumptionsIt would be worthwhile to explore whether or not the assumptions made in our analysis could be relaxed. For instance, the widely adopted monotonicity assumption (A2) and its variants [51, 52, 43, 44] may not hold in real-world settings. Thus, exploring potential relaxations of this assumption is valuable. In addition, a mild but non-trivial smoothness assumption (A3) (c.f. Remark 7 and 8) is introduced to obtain a tight sharpness risk upper bound (c.f. Remark 9); investigating its practical implications and exploring potential relaxations could be interesting future work.

Calibration-sharpness framework analysisApplying our framework to analyze recalibration methods beyond UMB, such as isotonic regression [51] and kernel density estimation [52], can offer further insights into their performance and properties, providing guidance to practitioners in selecting suitable algorithms based on specific conditions and requirements.

Multiclass probability recalibrationThe concept of calibration considered in this work extends to multi-class classification settings, known as canonical calibration [48]. Weaker, but more tractable notions of multi-class calibration have also been explored in the literature [48, 21]. While partition-based methods, as multi-class extensions of binning methods, are known to have consistency [48] and vanishing calibration error [41], establishing upper bounds for their sharpness risk remains difficult. Additionally, designing a partition scheme in a multidimensional space is a challenging task [21]; the interplay between calibration and sharpness could potentially guide the development of partition strategies that balance both aspects in multi-class classification.

Applications to real-world dataInvestigating the calibration-sharpness tradeoff in real-world applications, which often involve multiple classes, presents an interesting challenge. It is crucial to develop effective estimators for both calibration risk and sharpness risk in such scenarios. While estimators for calibration risk exist (e.g., binning-based and KDE-based) [6, 29, 52, 42] and a lower bound for sharpness risk has been established [39], a direct estimator of sharpness risk is still lacking.

Figure 3: Optimal recalibration function \(h^{*}\) and recalibration function estimates by Platt Scaling [40], the hybrid method [29], UWB, and UMB when the parametric assumption is **(a)** correct and **(b)** misspecified. UMB traces the \(h^{*}\) in both cases, whereas the hybrid method traces Platt scaling, exhibiting an intrinsic bias from \(h^{*}\) in **(b)**.

## Acknowledgments and Disclosure of Funding

The research in this paper was partially supported by ARO grants W911NF-23-1-0343 and W911NF-19-1-0269 and by DOE grant DE-NA0003921.

## References

* [1] John Aitchison and Sheng M Shen. Logistic-normal distributions: Some properties and uses. _Biometrika_, 67(2):261-272, 1980.
* [2] Amr Alexandari, Anshul Kundaje, and Avanti Shrikumar. Maximum likelihood with bias-corrected calibration is hard-to-beat at label shift adaptation. In _International Conference on Machine Learning_, pages 222-232. PMLR, 2020.
* [3] Kamyar Azizzadenesheli, Anqi Liu, Fanny Yang, and Animashree Anandkumar. Regularized learning for domain adaptation under label shifts. In _International Conference on Learning Representations_, 2018.
* [4] Jaroslaw Blasiok, Parikshit Gopalan, Lunjia Hu, and Preetum Nakkiran. A unifying theory of distance from calibration. In _Proceedings of the 55th Annual ACM Symposium on Theory of Computing_, pages 1727-1740, 2023.
* [5] Glenn W Brier et al. Verification of forecasts expressed in terms of probability. _Monthly Weather Review_, 78(1):1-3, 1950.
* [6] Jochen Brocker. Estimating reliability and resolution of probability forecasts through decomposition of the empirical score. _Climate dynamics_, 39:655-667, 2012.
* [7] Dallas Card and Noah A Smith. The importance of calibration for estimating proportions from annotations. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_, pages 1636-1646, 2018.
* [8] Misty D Crown. Validation of the NOAA Space Weather Prediction Center's solar flare forecasting look-up table and forecaster-issued probabilities. _Space Weather_, 10(6), 2012.
* [9] A Philip Dawid. The well-calibrated Bayesian. _Journal of the American Statistical Association_, 77(379):605-610, 1982.
* [10] Morris H DeGroot and Stephen E Fienberg. The comparison and evaluation of forecasters. _Journal of the Royal Statistical Society: Series D (The Statistician)_, 32(1-2):12-22, 1983.
* [11] Shrey Desai and Greg Durrett. Calibration of pre-trained transformers. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 295-302, 2020.
* [12] Charles Elkan. The foundations of cost-sensitive learning. In _International Joint Conference on Artificial Intelligence_, volume 17, pages 973-978, 2001.
* [13] Andrea Esuli, Alessio Molinari, and Fabrizio Sebastiani. A critical reassessment of the Saerens-Latinne-Decaestecker algorithm for posterior probability adjustment. _ACM Transactions on Information Systems (TOIS)_, 39(2):1-34, 2020.
* [14] Saurabh Garg, Yifan Wu, Sivaraman Balakrishnan, and Zachary Lipton. A unified view of label shift estimation. _Advances in Neural Information Processing Systems_, 33:3290-3300, 2020.
* [15] Tilmann Gneiting and Adrian E Raftery. Weather forecasting with ensemble methods. _Science_, 310(5746):248-249, 2005.
* [16] Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation. _Journal of the American Statistical Association_, 102(477):359-378, 2007.

* [17] Sebastian Gruber and Florian Buettner. Better uncertainty calibration via proper scores for classification and beyond. _Advances in Neural Information Processing Systems_, 35:8618-8632, 2022.
* [18] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In _International Conference on Machine Learning_, pages 1321-1330. PMLR, 2017.
* [19] Chirag Gupta, Aleksandr Podkopaev, and Aaditya Ramdas. Distribution-free binary classification: Prediction sets, confidence intervals and calibration. _Advances in Neural Information Processing Systems_, 33:3711-3723, 2020.
* [20] Chirag Gupta and Aaditya Ramdas. Distribution-free calibration guarantees for histogram binning without sample splitting. In _International Conference on Machine Learning_, pages 3942-3952. PMLR, 2021.
* [21] Chirag Gupta and Aaditya Ramdas. Top-label calibration and multiclass-to-binary reductions. In _International Conference on Learning Representations_, 2022.
* [22] Chirag Gupta and Aaditya Ramdas. Online Platt scaling with calibeating. In _International Conference on Machine Learning_, 2023.
* [23] Kartik Gupta, Amir Rahimi, Thalaiyasingam Ajanthan, Thomas Mensink, Cristian Sminchisescu, and Richard Hartley. Calibration of neural networks using splines. In _International Conference on Learning Representations_, 2020.
* [24] Yingxiang Huang, Wentao Li, Fima Macheret, Rodney A Gabriel, and Lucila Ohno-Machado. A tutorial on calibration measurements and calibration models for clinical prediction models. _Journal of the American Medical Informatics Association_, 27(4):621-633, 2020.
* [25] Xiaoqian Jiang, Melanie Osl, Jihoon Kim, and Lucila Ohno-Machado. Calibrating predictive model estimates to support personalized medicine. _Journal of the American Medical Informatics Association_, 19(2):263-274, 2012.
* [26] Volodymyr Kuleshov and Percy S Liang. Calibrated structured prediction. _Advances in Neural Information Processing Systems_, 28, 2015.
* [27] Meelis Kull, Miquel Perello Nieto, Markus Kangsepp, Telmo Silva Filho, Hao Song, and Peter Flach. Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with Dirichlet calibration. _Advances in Neural Information Processing Systems_, 32, 2019.
* [28] Meelis Kull, Telmo M Silva Filho, and Peter Flach. Beyond sigmoids: How to obtain well-calibrated probabilities from binary classifiers with beta calibration. _Electronic Journal of Statistics_, 11:5052-5080, 2017.
* [29] Ananya Kumar, Percy S Liang, and Tengyu Ma. Verified uncertainty calibration. _Advances in Neural Information Processing Systems_, 32, 2019.
* [30] Donghwan Lee, Xinmeng Huang, Hamed Hassani, and Edgar Dobriban. T-Cal: An optimal test for the calibration of predictive models. _arXiv preprint arXiv:2203.01850_, 2022.
* [31] Zachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift with black box predictors. In _International Conference on Machine Learning_, pages 3122-3130. PMLR, 2018.
* [32] Allan H Murphy. A new vector partition of the probability score. _Journal of Applied Meteorology and Climatology_, 12(4):595-600, 1973.
* [33] Allan H Murphy and Edward S Epstein. Verification of probabilistic predictions: A brief review. _Journal of Applied Meteorology and Climatology_, 6(5):748-755, 1967.
* [34] Allan H Murphy and Robert L Winkler. Reliability of subjective probability forecasts of precipitation and temperature. _Journal of the Royal Statistical Society Series C: Applied Statistics_, 26(1):41-47, 1977.

* [35] Allan H Murphy and Robert L Winkler. A general framework for forecast verification. _Monthly weather review_, 115(7):1330-1338, 1987.
* [36] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated probabilities using Bayesian binning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 29, 2015.
* [37] Khanh Nguyen and Brendan O'Connor. Posterior calibration and exploratory analysis for natural language processing models. In _Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing_, pages 1587-1598, 2015.
* [38] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. _Journal of Machine Learning Research_, 12:2825-2830, 2011.
* [39] Alexandre Perez-Lebel, Marine Le Morvan, and Gael Varoquaux. Beyond calibration: Estimating the grouping loss of modern neural networks. In _The Eleventh International Conference on Learning Representations_, 2023.
* [40] John Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. _Advances in Large Margin Classifiers_, 10(3):61-74, 1999.
* [41] Aleksandr Podkopaev and Aaditya Ramdas. Distribution-free uncertainty quantification for classification under label shift. In _Uncertainty in Artificial Intelligence_, pages 844-853. PMLR, 2021.
* [42] Teodora Popordanoska, Raphael Sayer, and Matthew Blaschko. A consistent and differentiable \(l_{p}\) canonical calibration error estimator. _Advances in Neural Information Processing Systems_, 35:7933-7946, 2022.
* [43] Amir Rahimi, Amirreza Shaban, Ching-An Cheng, Richard Hartley, and Byron Boots. Intra order-preserving functions for calibration of multi-class neural networks. _Advances in Neural Information Processing Systems_, 33:13456-13467, 2020.
* [44] Rebecca Roelofs, Nicholas Cain, Jonathon Shlens, and Michael C Mozer. Mitigating bias in calibration error estimation. In _International Conference on Artificial Intelligence and Statistics_, pages 4036-4054. PMLR, 2022.
* [45] Marco Saerens, Patrice Latinne, and Christine Decaestecker. Adjusting the outputs of a classifier to new a priori probabilities: A simple procedure. _Neural Computation_, 14(1):21-41, 2002.
* [46] Zeyu Sun, Monica G Bobra, Xiantong Wang, Yu Wang, Hu Sun, Tamas Gombosi, Yang Chen, and Alfred Hero. Predicting solar flares using CNN and LSTM on two solar cycles of active region data. _The Astrophysical Journal_, 931(2):163, 2022.
* [47] Junjiao Tian, Yen-Cheng Liu, Nathaniel Glaser, Yen-Chang Hsu, and Zsolt Kira. Posterior re-calibration for imbalanced datasets. _Advances in Neural Information Processing Systems_, 33:8101-8113, 2020.
* [48] Juozas Vaicenavicius, David Widmann, Carl Andersson, Fredrik Lindsten, Jacob Roll, and Thomas Schon. Evaluating model calibration in classification. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 3459-3467. PMLR, 2019.
* [49] Jonathan Wenger, Hedvig Kjellstrom, and Rudolph Triebel. Non-parametric calibration for classification. In _International Conference on Artificial Intelligence and Statistics_, pages 178-190. PMLR, 2020.
* [50] Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers. In _Proceedings of the Eighteenth International Conference on Machine Learning_, pages 609-616, 2001.
* [51] Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multiclass probability estimates. In _Proceedings of the eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 694-699, 2002.

* Zhang et al. [2020] Jize Zhang, Bhavya Kailkura, and T Yong-Jin Han. Mix-n-match: Ensemble and compositional methods for uncertainty calibration in deep learning. In _International Conference on Machine Learning_, pages 11117-11128. PMLR, 2020.
* Zhao et al. [2021] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In _International Conference on Machine Learning_, pages 12697-12706. PMLR, 2021.

Proof of Proposition 1

Proof of Proposition 1.: First of all, we recall the definition of the two risks from (3) and (4):

\[R^{\mathrm{cal}}(h;f) =\mathbb{E}\left[\left(f(X)-\mathbb{E}\left[Y\mid f(X)\right] \right)^{2}\right]\] \[R^{\mathrm{sha}}(h;f) =\mathbb{E}\left[\left(\mathbb{E}\left[Y\mid h\circ f(X)\right]- \mathbb{E}\left[Y\mid f(X)\right]\right)^{2}\right].\]

To keep our notation concise, we use \(Z=f(X)\) as a shorthand notation, and also let \(Y_{Z}:=\mathbb{E}[Y\mid Z]\) and \(Y_{h(Z)}=\mathbb{E}[Y\mid h(Z)]\) throughout this proof. We can decompose the recalibration risk from Definition 5:

\[R(h) =\mathbb{E}[(h(Z)-Y_{Z})^{2}]\] \[=\mathbb{E}[(h(Z)-Y_{h(Z)}+Y_{h(Z)}-Y_{Z})^{2}]\] \[=\mathbb{E}[(h(Z)-Y_{h(Z)})^{2}]+\mathbb{E}[(Y_{h(Z)}-Y_{Z})^{2} ]+2\mathbb{E}[(h(Z)-Y_{h(Z)})(Y_{h(Z)}-Y_{Z})]\] \[=\mathbb{E}[(h(Z)-Y_{h(Z)})^{2}]+\mathbb{E}[(Y_{h(Z)}-Y_{Z})^{2} ]+2\mathbb{E}[\mathbb{E}[(h(Z)-Y_{h(Z)})(Y_{h(Z)}-Y_{Z})\mid h(Z)]]\] \[=\mathbb{E}[(h(Z)-Y_{h(Z)})^{2}]+\mathbb{E}[(Y_{h(Z)}-Y_{Z})^{2} ]+2\mathbb{E}[(h(Z)-Y_{h(Z)})(Y_{h(Z)}-\mathbb{E}[Y_{Z}\mid h(Z)])]\] \[=\mathbb{E}[(h(Z)-Y_{h(Z)})^{2}]+\mathbb{E}[(Y_{h(Z)}-Y_{Z})^{2} ]+2\mathbb{E}[(h(Z)-Y_{h(Z)})(Y_{h(Z)}-Y_{h(Z)})]\] \[=\underbrace{\mathbb{E}[(h(Z)-Y_{h(Z)})^{2}]}_{R^{\mathrm{cal}}(h )}+\underbrace{\mathbb{E}[(Y_{h(Z)}-Y_{Z})^{2}]}_{R^{\mathrm{sha}}(h)}.\]

## Appendix B Proof of Theorem 1

In this section, we present a proof of Theorem 1. Let \((Y,Z)\in\mathcal{Y}\times\mathcal{Z}\) be random variables that admits a joint distribution \(P_{Y,Z}\), which we assume to be fixed throughout this section. Let \(S=\{(y_{i},z_{i})\in\mathcal{Y}\times\mathcal{Z}:i\in[n]\}\) and let \(\mathcal{B}=\{I_{1},I_{2},\ldots,I_{B}\}\) be the uniform-mass binning scheme (cf. Definition 6) of size \(B\) induced by (\(z_{i}\)'s in) \(S\). Note that if \(S\) is a random sample from \(P_{Y,Z}\), then the binning scheme \(\mathcal{B}\) induced by \(S\) is also a random variable following a derived distribution. To facilitate our analysis, we introduce the notion of well-balanced binning.

**Definition 8** (Well-balanced binning; [29]).: _Let \(B\in\mathbb{N}\), let \(Z\) be a random variable that takes value in \([0,1]\), and let \(\alpha\in\mathbb{R}\) such that \(\alpha\geq 1\). A binning scheme \(\mathcal{B}\) of size \(B\) is \(\alpha\)-well-balanced with respect to \(Z\) if_

\[\frac{1}{\alpha B}\leq P[Z\in I_{b}]\leq\frac{\alpha}{B},\qquad\forall b\in[B].\]

In addition, we define two (parameterized families of) Boolean-valued functions \(\Phi_{\mathrm{balance}}\) and \(\Phi_{\mathrm{approx}}\) as follows: for any binning scheme \(\mathcal{B}\),

\[\forall\alpha\in\mathbb{R},\quad\Phi_{\mathrm{balance}}(\mathcal{B };\alpha) :=\mathds{1}\left\{\frac{1}{\alpha|\mathcal{B}|}\leq P\left[Z\in I \right]\leq\frac{\alpha}{|\mathcal{B}|},\ \ \forall I\in\mathcal{B}\right\},\] (21) \[\forall\varepsilon\in\mathbb{R},\quad\Phi_{\mathrm{approx}}( \mathcal{B};\varepsilon) :=\mathds{1}\left\{\max_{I\in\mathcal{B}}\left|\hat{\mu}_{I}-\mu_{I} \right|\leq\varepsilon\right\},\] (22)

where \(\mathds{1}(A)=1\) if and only if the predicate \(A\) is true, and for each interval \(I\in\mathcal{B}\),

\[\hat{\mu}_{I}=\frac{\sum_{i=1}^{n}y_{i}\cdot\mathds{1}_{I}(z_{i})}{\sum_{i=1}^ {n}\cdot\mathds{1}_{I}(z_{i})}\qquad\text{and}\qquad\mu_{I}=\mathbb{E}_{(Y,Z) \sim P_{Y,Z}}\left[Y\cdot\mathds{1}_{I}(Z)\right].\] (23)

Note that if \(\Phi_{\mathrm{balance}}(\mathcal{B};\alpha)=1\) for \(\alpha\geq 1\), then \(\mathcal{B}\) is \(\alpha\)-well-balanced with respect to \(Z\) (cf. Definition 8). Also, if \(\Phi_{\mathrm{approx}}(\mathcal{B};\varepsilon)=1\) for \(\varepsilon\geq 0\), then the conditional empirical mean of \(Y\) in each bin \(I\in\mathcal{B}\) approximates the conditional expectation with error at most \(\varepsilon\), uniformly for all bins.

The rest of this section is organized as follows. In Section B.1, we ensure that for an appropriate choice of \(\alpha,\varepsilon\in\mathbb{R}\), it holds with high probability (with respect to the randomness in \(\mathcal{B}\)) that \(\Phi_{\mathrm{balance}}(\mathcal{B};\alpha)=\Phi_{\mathrm{approx}}(\mathcal{B };\varepsilon)=1\). In Section B.2, we establish upper bounds on the reliability risk \(R^{\mathrm{cal}}\) and the sharpness risk \(R^{\mathrm{sha}}\) under the premise that \(\Phi_{\mathrm{balance}}(\mathcal{B};\alpha)=\Phi_{\mathrm{approx}}(\mathcal{B };\varepsilon)=1\). Finally, in Section B.3, we conclude the proof of Theorem 1 by combining these results together.

### High-probability certification of the conditions

Well-balanced binning scheme.First of all, we observe that the uniform-pass binning scheme \(\mathcal{B}\) induced by an IID random sample from \(P_{Y,Z}\) is 2-well-balanced with high probability, if the sample size is sufficiently large. Here we paraphrase a result from [29] in our language.

**Lemma 3** ([29, Lemma 4.3]).: _Let \(S=\{Z_{i}:i\in[n]\}\) be an IID sample drawn from \(P_{Z}\) and let \(\mathcal{B}\) be the uniform-mass binning scheme of size \(B\) induced by \(S\). There exists a universal constant \(c^{\prime}>0\) such that for any \(\delta\in(0,1)\), if \(n\geq c^{\prime}\cdot B\log(B/\delta)\), then \(\Phi_{\mathrm{balance}}(\mathcal{B},2)=1\) with probability at least \(1-\delta\)._

Lemma 3 states that

\[n\geq c^{\prime}\cdot B\log\left(\frac{B}{\delta}\right)\qquad \Longrightarrow\qquad P\left[\text{$\mathcal{B}$ is 2-well-balanced with respect to $P_{Z}$}\right]\geq 1-\delta.\]

While the value of the universal constant \(c\) was not specified in the original reference [29], we remark that one may set, for example, \(c^{\prime}=2420\), which can be verified by following their proof with \(c^{\prime}\) kept explicit.

The proof of Lemma 3 in [29] relies on a discretization argument that considers a fine-grained cover of \(\mathcal{Z}=[0,1]\) consisting of disjoint intervals--namely, \(\left\{I_{j}^{\prime}:j\in[10B]\right\}\) such that \(P[Z\in I_{j}^{\prime}]=\frac{1}{10B}\) for all \(j\in[10B]\)--and then approximates each \(I_{b}\) by a subset of the cover. As the authors of [29] remarked, this argument provides a tighter sample complexity upper bound than naively applying Chernoff bounds or a standard VC dimension argument, which would yield an upper bound of order \(O\left(B^{2}\log\left(\frac{B}{\delta}\right)\right)\). We omit the proof of Lemma 3 and refer interested readers to the referenced paper [29] for more details.

Uniform concentration of bin-wise means.Next, we argue that for the uniform-mass binning scheme \(\mathcal{B}\) induced by an IID sample, the conditional empirical means of each bin concentrates to the population conditional expectation, uniformly for all bins in \(\mathcal{B}\). Here we restate a result from [20].

**Lemma 4** ([20, Corollary 1]).: _Let \(P_{Z}\) be an absolutely continuous probability measure on \(\mathcal{Z}=[0,1]\), and \(S=\{Z_{i}:i\in[n]\}\) be an IID sample drawn from \(P_{Z}\). Let \(B\in\mathbb{N}\) such that \(B\leq\frac{n}{2}\) and \(\mathcal{B}\) be the uniform-mass binning scheme of size \(B\) induced by \(S\). Then for any \(\delta\in(0,1)\),_

\[P\left[\Phi_{\mathrm{approx}}(\mathcal{B};\varepsilon_{\delta})=1\right]\geq 1 -\delta\qquad\text{where}\qquad\varepsilon_{\delta}=\sqrt{\frac{1}{2(\lfloor n /B\rfloor-1)}\log\left(\frac{2B}{\delta}\right)}+\frac{1}{\lfloor n/B\rfloor}.\] (24)

Lemma 4 states that under the mild regularity condition of \(P_{Z}\) being absolutely continuous, the uniform-mass binning accurately approximates all bin-wise conditional means as long as there are at least two samples per bin in the sense that

\[n\geq 2B\qquad\Longrightarrow\qquad P\left[\sup_{b\in[B]}|\hat{\mu}_{b}-\mu_{ b}|\leq\sqrt{\frac{1}{2(\lfloor n/B\rfloor-1)}\log\left(\frac{2B}{\delta} \right)}+\frac{1}{\lfloor n/B\rfloor}\right]\geq 1-\delta.\]

### Conditional upper bounds on reliability risk and sharpness risk

In this section, we establish upper bounds on the reliability risk \(R^{\mathrm{cal}}\) and the sharpness risk \(R^{\mathrm{sha}}\) for \(\hat{h}\) under the premise that \(\Phi_{\mathrm{balance}}(\mathcal{B};\alpha)=1\) and \(\Phi_{\mathrm{approx}}(\mathcal{B};\varepsilon)=1\) for appropriate parameters \(\alpha,\varepsilon\in\mathbb{R}\).

Preparation.To avoid clutter in the lemma statements to follow, here we recall our problem setting and set several notation that will be used throughout this section. Recall that \(P=P_{X,Y}\) is a joint distribution on \(\mathcal{X}\times\mathcal{Y}\) and let \(f:\mathcal{X}\to\mathcal{Z}\) is a measurable function. In addition, we let \(\tilde{S}=\left\{(x_{i},y_{i})\in\mathcal{X}\times\mathcal{Y}:i\in[n]\right\}\) be an IID sample drawn from \(P\), and let \(S=\left\{(z,y)\in\mathcal{Z}\times\mathcal{Y}:(x,y)\in\tilde{S}\text{ and }z=f(x)\right\}\). Let \(\mathcal{B}\) be the uniform-mass binning scheme induced by (\(z\)'s in) \(S\), and let \(\hat{h}=\hat{\mathcal{B}}:\mathcal{Z}\to\mathcal{Z}\) be the recalibration function derived from \(\mathcal{B}\) as we described in Section 4.1; see (9). The dependence among \(P,f,\tilde{S},S,\mathcal{B}\), and \(\hat{h}\) are summarized by a diagram in Figure 4.

Furthermore, we define the index function for a binning scheme to facilitate our analysis.

**Definition 9**.: _Let \(\mathcal{B}\) be a binning scheme. The index function for \(\mathcal{B}\) is the function \(\beta:\mathcal{Z}\to[[\mathcal{B}]]\) such that_

\[\beta(z)=\sum_{I\in\mathcal{B}}\mathds{1}_{(0,\sup I]}(z).\] (25)

**Remark 6**.: _Note that \(\beta\) is a measurable function and defines an index function that identifies which bin of \(\mathcal{B}\) the argument \(z\in[0,1]\) belongs to. Specifically, suppose that \(\mathcal{B}=\{I_{1},\ldots,I_{B}\}\) for some \(B\in\mathbb{N}\) and there exists \(u_{0},u_{1},\ldots,u_{B}\in[0,1]\) such that (i) \(0=u_{0}<u_{1}<\cdots<u_{B}=1\) and (ii) \(I_{b}=(u_{b-1},u_{b}]\) for all \(b\in[B]\setminus\{1\}\) and \(I_{1}=[u_{0},u_{1}]\). Then \(\beta(z)=b\) if and only if \(z\in I_{b}\)._

#### b.2.1 Calibration risk upper bound

We observe that if a binning scheme \(\mathcal{B}\) produces empirical means \(\hat{\mu}_{I}\) that approximate the true means \(\mu_{I}\) with error at most \(\varepsilon\), then the calibration risk is upper bounded by \(\varepsilon^{2}\).

**Lemma 5** (Calibration risk bound).: _For any \(\varepsilon\geq 0\), if \(\Phi_{\mathrm{approx}}(\mathcal{B};\varepsilon)=1\), then_

\[R^{\mathrm{cal}}(\hat{h};f,P)\leq\varepsilon^{2}.\]

Proof of Lemma 5.: To begin with, we recall the definition of the calibration risk (Definition 3), and let \(Z=f(X)\). Then we may write

\[R^{\mathrm{cal}}\big{(}\hat{h};f,P\big{)} =\mathbb{E}\left[\left(\hat{h}(Z)-\mathbb{E}[Y\mid\hat{h}(Z)] \right)^{2}\right]\] \[=\mathbb{E}\left[\mathbb{E}\left[\left(\hat{h}(Z)-\mathbb{E}[Y \mid\hat{h}(Z)]\right)^{2}\Big{|}\,\beta(Z)\right]\right] \quad\therefore\text{the law of total expectation}\] \[=\mathbb{E}\left[\left(\hat{\mu}_{I_{\beta(Z)}}-\mu_{I_{\beta(Z) }}\right)^{2}\right] \text{cf.\ }\eqref{eq:R^{\mathrm{cal}}}\] \[\leq\max_{I\in\mathcal{B}}\big{(}\hat{\mu}_{I}-\mu_{I}\big{)}^{2}.\]

Note that if \(\Phi_{\mathrm{approx}}(\mathcal{B};\varepsilon)=1\), then \(\max_{I\in\mathcal{B}}\big{(}\hat{\mu}_{I}-\mu_{I}\big{)}^{2}\leq\varepsilon^ {2}\). 

We remark that the proof of Lemma 5 is a simple application of applying Holder's inequality. Also, we note that a similar argument was considered in [20, Proposition 1] to establish the inequalities between the \(L^{p}\)-counterparts of the calibration risk, which they call the \(\ell_{p}\)-expected calibration error (ECE). In this work, we focus on the case \(p=2\).

#### b.2.2 Sharpness risk upper bound

Next, we present an upper bound for the sharpness risk that diminishes as the binning scheme \(\mathcal{B}\) becomes more balanced.

**Lemma 6** (Sharpness risk bound).: _Suppose that the optimal post-hoc recalibration function \(h_{f,P}^{*}\), cf. (7), is monotonically non-decreasing. Let \(\alpha\in\mathbb{R}\) such that \(\alpha\geq 1\). If \(\Phi_{\mathrm{balance}}(\mathcal{B},\alpha)=1\), then_

\[R^{\mathrm{sha}}(\hat{h};f,P)\leq\frac{\alpha}{|\mathcal{B}|}.\]

Proof of Lemma 6.: Letting \(Z=f(X)\), we can write the sharpness risk of \(\hat{h}\) over \(f\) with repsect to \(P\) as

\[R^{\mathrm{sha}}(\hat{h};f,P):=\mathbb{E}\left[\left(\mathbb{E}\big{[}Y\mid \hat{h}(Z)\big{]}-\mathbb{E}\left[Y\mid Z\right]\right)^{2}\right].\]

[MISSING_PAGE_FAIL:18]

[MISSING_PAGE_FAIL:19]

Additionally, if the Assumption (A3) also holds, then we obtain a stronger upper bound on \(R^{\mathrm{sha}}(\hat{h};f,P)\) by Lemma 7:

\[R^{\mathrm{sha}}(\hat{h};f,P)\leq\frac{8K^{2}}{|\mathcal{B}|^{2}}.\]

## Appendix C Proof of Theorem 2

This section contains a proof of Theorem 2. Prior to the proof, in Section C.1, we provide several lemmas that will be useful in our proof. Thereafter, we present a proof of Theorem 2 in its entirety in Section C.2.

### Useful lemmas

#### c.1.1 Concentration of \(\hat{w}_{k}\) to \(w_{k}^{*}\)

First of all, we recall the binomial Chernoff bound, which is a classical result about the concentration of measures that can be found in standard textbooks on probability theory.

**Lemma 8** (Binomial Chernoff bound).: _Let \(X_{i}\) be IID Bernoulli random variables with parameters \(p\in(0,1)\), and let \(S_{n}:=\frac{1}{n}\sum_{i=1}^{n}X_{i}\). Then for any \(\delta\in\mathbb{R}\) such that \(0<\varepsilon<1\),_

\[P\left[S_{n}\geq(1+\varepsilon)p\right] \leq\exp\left(-\frac{\varepsilon^{2}p}{3}n\right),\] \[P\left[S_{n}\leq(1-\varepsilon)p\right] \leq\exp\left(-\frac{\varepsilon^{2}p}{2}n\right).\]

It follows from Lemma 8 that for any \(\varepsilon,\delta\in(0,1)\),

\[n\geq\frac{3}{\varepsilon^{2}p}\log\left(\frac{2}{\delta}\right)\qquad \Longrightarrow\qquad P\left(\frac{|S_{n}-p|}{p}>\varepsilon\right)\leq\delta.\] (27)

Let \(P,Q\) be two distributions on \(\mathcal{Y}=\{0,1\}\), and let \(\mathcal{D}_{P}\sim P\), \(\mathcal{D}_{Q}\sim Q\) denote IID samples of size \(n_{P}\), \(n_{Q}\), respectively. Recall from (13) and (16) that for each \(k\in\{0,1\}\), we define

\[w_{k}^{*}=\frac{P_{Q}[Y=k]}{P_{P}[Y=k]},\qquad\text{and}\qquad\hat{w}_{k}= \frac{P_{\mathcal{D}_{Q}}[Y=k]}{P_{\mathcal{D}_{P}}[Y=k]}.\]

Then, we let

\[\rho_{0}:=\frac{\hat{w}_{0}}{w_{0}^{*}}\qquad\text{and}\qquad\rho_{1}:=\frac{ \hat{w}_{1}}{w_{1}^{*}}.\] (28)

Now we define another parameterized family of Boolean-valued functions \(\Phi_{\mathrm{ratio}}(\mathcal{D}_{P},\mathcal{D}_{Q};\beta)\) as follows. Given \(\mathcal{D}_{P}\sim P\), \(\mathcal{D}_{Q}\sim Q\), and \(\beta\in\mathbb{R}\) such that \(1<\beta\leq 2\),

\[\Phi_{\mathrm{ratio}}(\mathcal{D}_{P},\mathcal{D}_{Q};\beta):=\mathds{1}\left\{ \frac{1}{\beta}\leq\rho_{k}\leq\beta,\ \ \forall k\in\{0,1\}\right\}.\] (29)

**Corollary 9**.: _Let \(P,Q\) be two distributions on \(\mathcal{Y}=\{0,1\}\), and let \(\mathcal{D}_{P}\sim P\), \(\mathcal{D}_{Q}\sim Q\) denote IID samples of size \(n_{P}\), \(n_{Q}\), respectively. For each \(k\in\{0,1\}\), let \(p_{k}:=P_{P}[Y=k]\) and \(q_{k}:=P_{Q}[Y=k]\). Likewise, we let \(\hat{p}_{k}=\frac{1}{n_{P}}\sum_{y_{i}\in\mathcal{D}_{P}}\mathds{1}\{y_{i}=k\}\) and \(\hat{q}_{k}=\frac{1}{n_{Q}}\sum_{y_{i}\in\mathcal{D}_{Q}}\mathds{1}\{y_{i}=k\}\). For any \(\delta\in(0,1)\) and any \(\beta\in(1,2]\), if_

\[n_{P}\geq\frac{27}{(\beta-1)^{2}\min\{p_{0},p_{1}\}}\log\left(\frac{8}{\delta }\right)\quad\text{and}\quad n_{Q}\geq\frac{27}{(\beta-1)^{2}\min\{q_{0},q_{1 }\}}\log\left(\frac{8}{\delta}\right),\]

_then_

\[P\left(\Phi_{\mathrm{ratio}}(\mathcal{D}_{P},\mathcal{D}_{Q};\beta)=1\right) \geq 1-\delta.\]Proof of Corollary 9.: Let \(\varepsilon=\frac{\beta-1}{3}\). Since \(\frac{1+x}{1-x}\leq 1+3x\) for all \(x\in[0,1/3]\), we have \(\frac{1}{\beta}\leq\frac{1-\varepsilon}{1+\varepsilon}<\frac{1+\varepsilon}{1- \varepsilon}\leq\beta\). Then it follows from (27) that for each \(k\in\{0,1\}\),

\[n_{P} \geq\frac{3}{\varepsilon^{2}p_{k}}\log\left(\frac{8}{\delta} \right) \implies P\left(\frac{|\dot{p}_{k}-p_{k}|}{p_{k}}>\varepsilon\right)\leq \frac{\delta}{4},\] \[n_{Q} \geq\frac{3}{\varepsilon^{2}q_{k}}\log\left(\frac{8}{\delta} \right) \implies P\left(\frac{|\dot{q}_{k}-q_{k}|}{q_{k}}>\varepsilon\right)\leq \frac{\delta}{4}.\]

Applying the union bound, we obtain the following implication:

\[n_{P} \geq\frac{3}{\varepsilon^{2}\min\{p_{0},p_{1}\}}\log\left(\frac{ 8}{\delta}\right)\text{ and }n_{Q}\geq\frac{3}{\varepsilon^{2}\min\{q_{0},q_{1}\}}\log\left(\frac{8}{ \delta}\right)\] \[\implies P\left(\max_{k\in\{0,1\}}\frac{|\dot{p}_{k}-p_{k}|}{p_{k}}> \varepsilon\text{ or }\max_{k\in\{0,1\}}\frac{|\dot{q}_{k}-q_{k}|}{q_{k}}> \varepsilon\right)\leq\delta\] \[\implies P\left(\max_{k\in\{0,1\}}\rho_{k}>\frac{1+\varepsilon}{1- \varepsilon}\text{ or }\min_{k\in\{0,1\}}\rho_{k}<\frac{1-\varepsilon}{1+\varepsilon}\right)\leq\delta\] \[\implies P\left(\max_{k\in\{0,1\}}\rho_{k}>\beta\text{ or }\min_{k\in\{0,1\}}\rho_{k}<\frac{1}{\beta}\right)\leq\delta.\]

#### c.1.2 Regularity of the Shift Correction Function

**Lemma 10**.: _Let \(w=(w_{0},w_{1})\in\mathbb{R}^{2}\) such that \(w_{0},w_{1}>0\) and \(w_{0}+w_{1}=1\). The function \(g_{w}:[0,1]\to[0,1]\) such that \(g_{w}(z)=\frac{w_{1}z}{w_{1}z+w_{0}(1-z)}\) is \(L\)-Lipschitz where \(L=\max\left\{\frac{w_{1}}{w_{0}},\frac{w_{0}}{w_{1}}\right\}\)._

Proof of Lemma 10.: First of all, consider the first-order derivative of \(g_{w}\):

\[\frac{d}{dz}g_{w}(z)=\frac{w_{1}\cdot\left[w_{1}z+w_{0}(1-z)\right]-w_{1}z \cdot(w_{1}-w_{0})}{\left[w_{1}z+w_{0}(1-z)\right]^{2}}=\frac{w_{1}w_{0}}{ \left[w_{1}z+w_{0}(1-z)\right]^{2}}.\]

We observe that \(g_{w}\) is monotone increasing as \(\frac{d}{dz}g_{w}(z)>0\) for all \(z\in[0,1]\). Next, we consider the second-order derivative of \(g_{w}\):

\[\frac{d^{2}}{dz^{2}}g_{w}(z)=\frac{2w_{0}w_{1}\cdot(w_{0}-w_{1})}{\left[w_{1} z+w_{0}(1-z)\right]^{3}}\begin{cases}>0,&\forall z\in[0,1]&\text{if }w_{0}>w_{1},\\ =0,&\forall z\in[0,1]&\text{if }w_{0}=w_{1},\\ <0,&\forall z\in[0,1]&\text{if }w_{0}<w_{1}.\end{cases}\]

Therefore,

\[\sup_{z\in[0,1]}\frac{d}{dz}g_{w}(z)=\begin{cases}\frac{d}{dz}g_{w}(z)\bigg{|} _{z=1}=\frac{w_{0}}{w_{1}}&\text{if }w_{0}>w_{1},\\ \frac{d}{dz}g_{w}(z)\bigg{|}_{z=0}=\frac{w_{1}}{w_{0}}&\text{if }w_{0}\leq w_{1}. \end{cases}\]

**Lemma 11**.: _Let \(P,Q\) be joint distributions of \((X,Y)\in\mathcal{X}\times\{0,1\}\), and let \(w_{k}=\frac{P[Y=k]}{Q[Y=k]}\) for \(k\in\{0,1\}\). If \(P,Q\) satisfy the label shift assumption (Definition 7), i.e., if Assumptions (B1) and (B2) hold, then for any measurable function \(f:\mathcal{X}\to\mathbb{R}\), the following two-sided inequality holds:_

\[\min_{k\in\{0,1\}}w_{k}\leq\frac{\mathbb{E}_{Q}[f(X)]}{\mathbb{E}_{P}[f(X)]} \leq\max_{k\in\{0,1\}}w_{k}.\] (30)

Proof of Lemma 11.: First of all, we observe that

\[\mathbb{E}_{Q}\left[f(X)\right] =\mathbb{E}_{Q}\left[\mathbb{E}_{Q}\big{[}f(X)\mid Y\big{]}\right] \text{ by the law of total expectation}\] \[=\sum_{k=0}^{1}P_{Q}[Y=k]\cdot\mathbb{E}_{Q}\big{[}f(X)\mid Y \big{]}\] \[=\sum_{k=0}^{1}\big{(}w_{k}\cdot P_{P}[Y=k]\big{)}\cdot\mathbb{E }_{P}\big{[}f(X)\mid Y\big{]}.\quad\text{by definition of }w_{k}\text{ \& the label shift assumption}\]

Thus, it follows that \(\min_{k}w_{k}\cdot\mathbb{E}_{P}[f(X)]\leq\mathbb{E}_{Q}[f(X)]\leq\max_{k}w_{k} \cdot\mathbb{E}_{P}[f(X)]\)

### Completing the proof of Theorem 2

Proof of Theorem 2.: This proof is presented in four steps. In Step 1, we establish a simple upper bound for the risk \(R_{Q}(\hat{h}_{Q};f)\) that consists of two error terms: the first term quantifies the error introduced by the estimated label shift correction, \(\hat{g}\), while the second term quantifies the error due to the estimated recalibration function, \(\hat{h}_{P}\). In Steps 2 and 3, we derive separate upper bounds for these two error terms. Finally, in Step 4, we combine the results from Steps 1-3 to obtain a comprehensive upper bound for \(R_{Q}\), which concludes the proof.

Step 1. Decomposition of \(R_{Q}\).Recalling the definition of the risk \(R_{Q}\), cf. (5), we obtain the following inequality:

\[R_{Q}(\hat{h}_{Q};f) =\mathbb{E}_{Q}\left[\left(\hat{h}_{Q}\circ f(X)-\mathbb{E}_{Q}[ Y|f(X)]\right)^{2}\right]\] \[=\mathbb{E}_{Q}\left[\left(\hat{g}\circ\hat{h}_{P}\circ f(X)-g^{ *}\circ\hat{h}_{P}\circ f(X)+g^{*}\circ\hat{h}_{P}\circ f(X)-\mathbb{E}_{Q}[ Y|f(X)]\right)^{2}\right]\] \[\overset{(a)}{\leq}2\cdot\Bigg{\{}\underbrace{\mathbb{E}_{Q} \left[\left(\hat{g}\circ\hat{h}_{P}\circ f(X)-g^{*}\circ\hat{h}_{P}\circ f(X )\right)^{2}\right]}_{=:T_{1}}\] (31) \[\qquad+\underbrace{\mathbb{E}_{Q}\left[\left(g^{*}\circ\hat{h}_ {P}\circ f(X)-\mathbb{E}_{Q}[Y|f(X)]\right)^{2}\right]}_{=:T_{2}}\Bigg{\}},\] (32)

where (a) follows from the simple inequality \((a+b)^{2}\leq 2(a^{2}+b^{2})\) for all \(a,b\in\mathbb{R}\).

In Step 2 and Step 3 of this proof, we establish separate upper bounds for the two terms, \(T_{1}\), \(T_{2}\).

Step 2. An upper bound for \(T_{1}\).Recall from (13) and (16) that

\[g^{*}(z) =\frac{w_{1}^{*}z}{w_{1}^{*}z+w_{0}^{*}(1-z)}\] where \[w_{k}^{*} =\frac{Q[Y=k]}{P[Y=k]},\ \ \forall k\in\{0,1\},\] \[\hat{g}(z) =\frac{\hat{w}_{1}z}{\hat{w}_{1}z+\hat{w}_{0}(1-z)}\] where \[\hat{w}_{k} =\frac{\hat{Q}[Y=k]}{\hat{P}[Y=k]},\ \ \forall k\in\{0,1\}.\]

Let

\[\rho_{0}:=\frac{\hat{w}_{0}}{w_{0}^{*}}\] and \[\rho_{1}:=\frac{\hat{w}_{1}}{w_{1}^{*}}.\] (33)

Then we observe that for any \(z\in(0,1)\),

\[|\hat{g}(z)-g^{*}(z)| =\left|\frac{\hat{w}_{1}z}{\hat{w}_{1}z+\hat{w}_{0}(1-z)}-\frac{w _{1}^{*}z}{w_{1}^{*}z+w_{0}^{*}(1-z)}\right|\] \[=\left|\frac{\left(\hat{w}_{1}w_{0}^{*}-w_{1}^{*}\hat{w}_{0} \right)\cdot z(1-z)}{\left[\hat{w}_{1}z+\hat{w}_{0}(1-z)\right]\cdot\left[w_{1 }^{*}z+w_{0}^{*}(1-z)\right]}\right|\] \[\leq\left|\frac{\left(\hat{w}_{1}w_{0}^{*}-w_{1}^{*}\hat{w}_{0} \right)\cdot z(1-z)}{\left(\hat{w}_{1}w_{0}^{*}+w_{1}^{*}\hat{w}_{0}\right) \cdot z(1-z)}\right|\] \[=\frac{\hat{w}_{1}w_{0}^{*}-w_{1}^{*}\hat{w}_{0}}{\hat{w}_{1}w_{ 0}^{*}+w_{1}^{*}\hat{w}_{0}}\Bigg{|}\] \[=\frac{\left|\rho_{0}-\rho_{1}\right|}{\rho_{0}+\rho_{1}}.\]

Moreover, \(\hat{g}(0)=g^{*}(0)=0\) and \(\hat{g}(1)=g^{*}(1)=1\). Letting \(Z_{h}:=\hat{h}_{P}\circ f(X)\), we obtain

\[T_{1}=\mathbb{E}_{Q}\left[\left(\hat{g}(Z_{h})-g^{*}(Z_{h})\right)^{2}\right] \leq\left(\frac{\rho_{0}-\rho_{1}}{\rho_{0}+\rho_{1}}\right)^{2}.\] (34)

It remains to establish probabilistic tail bounds for \(\rho_{0},\rho_{1}\), which we will accomplish in Step 4 of this proof.

[MISSING_PAGE_EMPTY:23]

[MISSING_PAGE_EMPTY:24]

Details on the experiments

In Section E.1 and E.3, we consider a family of joint distributions \(\mathcal{D}(\pi)\) of \(X\) and \(Y\), where \(Y\sim\text{Bernoulli}(\pi)\), \(X\mid Y=0\sim N(-2,1)\), and \(X\mid Y=1\sim N(2,1)\). Suppose we are given \(f(x)=\sigma(x):=1/(1+e^{-x})\), for \(x\in\mathbb{R}\), as a probabilistic classifier. The optimal recalibration function can be derived as

\[h_{f,P}^{*}(z)=P[Y=1\mid f(X)=z]=\sigma(4\sigma^{-1}(z)).\] (41)

In Section E.2, we consider a parametric family of recalibration functions called beta calibration [28]: \(\mathcal{H}_{\text{beta}}=\{h_{\text{beta}}(\cdot;a,b,c):a\geq 0,b\geq 0,c \in\mathbb{R}\}\), where \(h_{\text{beta}}(\cdot;a,b,c):[0,1]\rightarrow[0,1]\) is defined as

\[h_{\text{beta}}(z;a,b,c)=\frac{1}{1+1/\left(e^{c}\frac{z^{a}}{(1-z)^{b}} \right)}.\] (42)

In addition, consider a subfamily \(\mathcal{H}_{\text{logit-normal}}\subset\mathcal{H}_{\text{beta}}\) defined as \(\mathcal{H}_{\text{logit-normal}}=\{h_{\text{logit-normal}}(\cdot;a,c):=h_{ \text{beta}}(\cdot;a,a,c)=\sigma(a\sigma^{-1}(\cdot)+c):a\geq 0,c\in\mathbb{R}\}\)3. Apparently, the optimal recalibration function in Equation (41), \(h_{f,P}^{*}\in\mathcal{H}_{\text{logit-normal}}\).

Footnote 3: We say \(Z\sim\text{Logit-Normal}(\mu,\tau^{2})\) if \(\sigma^{-1}(Z)\sim N(\mu,\tau^{2})\)[1]. Similar to beta calibration [28], we adopt the name logit-normal calibration after a simple example: if \(Y=\text{Bernoulli}(0.5)\), \(Z\mid Y=i\sim\text{Logit-Normal}(\mu_{i},\tau_{i}^{2})\) for \(i\in\{0,1\}\), then the optimal recalibration function \(\mathbb{E}[Y\mid Z=z]=h_{\text{logit-normal}}(z;a,c)\) for some \(a\), \(c\) depending on \(\mu_{i}\)s and \(\tau_{i}\)s.

### Verifying results for UMB

First, we recalibrate \(f\) on data distributed as \(\mathcal{D}(0.5)\) using UMB.

Verifying the risk convergence in Theorem 1We vary \(n\in[10^{2},10^{7}]\) and \(B\in[6,10^{3}]\) in the log scale. For each combination of \((n,B)\), we use UMB to recalibrate \(f\) on data generated from \(\mathcal{D}(0.5)\), and compute quadrature estimates of population \(R^{\text{cal}}(\hat{h})\), \(R^{\text{cha}}(\hat{h})\), and \(R(\hat{h})\), as well as their high probability bounds based on Theorem 1. The constant \(K\) in Assumption (A3) is selected by numerical maximization as

\[K=\max_{0\leq z_{1}<z_{2}\leq 1}\frac{h^{*}(z_{2})-h^{*}(z_{1})}{P[Z\in[z_{1},z_ {2}]]}.\]

Figure 1 shows the bounds follow the same trends as their associated population quantities, providing valid upper bounds in all cases.

Verifying the optimal choice of the number of bins.We find empirically optimal \(B^{*\text{experiment}}\) that achieves the minimal risk for each choice of \(n\). We compute the theoretically optimal choice of the number of bins, \(B^{*\text{theory}}\), by minimizing the finite-sample upper bounds. Figure 2 shows \(B^{*\text{experiment}}\) follows the same trend with \(B^{*\text{theory}}\), both scales in \(O(n^{1/3})\).

### Comparing recalibration methods

To highlight the benefits and drawbacks of UMB's nonparametric nature, we compare UMB with (semi-)parametric recalibration methods in scenarios where the parametric assumption is correct and where it is misspecified. We compare the method under study, uniform-mass binning (UMB), with 3 other recalibration methods: uniform-width binning (UWB) [18], Platt scaling [40]4, and a hybrid parametric-binning method [29]. Note that Platt scaling and the hybrid method adopt the parametric assumption \(h^{*}\in\mathcal{H}_{\text{logit-normal}}\).

Footnote 4: The original Platt scaling operates on outputs of real-valued SVM outputs [40]. For probabilistic classifiers, we follow [38; 29; 22] and implement Platt scaling by first transforming probabilities onto the real line via the logit transform \(\sigma^{-1}\).

For the first setting, we construct optimal recalibration function \(h^{*}\in\mathcal{H}_{\text{logit-normal}}\) so that the parametric assumption of Platt scaling and the hybrid method holds. In particular, we consider the distribution \(Z\in\text{Uniform}[0,1]\) and \(Y\mid Z\sim\text{Bernoulli}(h_{\text{logit-normal}}(Z;a,c))\) with \(a=4\) and \(c=0\). For the second setting, we construct \(h^{*}\in\mathcal{H}_{\text{beta}}\) but \(h^{*}\notin\mathcal{H}_{\text{logit-normal}}\) so that the parametric assumption fails. In particular, we consider the distribution \(Z\sim\text{Uniform}[0,1]\) and \(Y\mid Z\sim\text{Bernoulli}(h_{\text{beta}}(z;a,b,c))\) with \(a=0.1\), \(b=4\), and \(c=0\). For each setting, we fix calibration sample size to be \(n=5000\).

Risks as functions of the number of bins \(B\)We traverse the number of bins \(B\in[10,2000]\) in the log scale and compare how each method behaves as \(B\) changes. When the parametric assumption is correct, the hybrid method achives significantly lower \(R^{\mathrm{cal}}\) and overall \(R\) than UMB and UWB for sufficiently large number of bins (Figure 4(a)), an advantage highlighted in [29]. In contrast, when the parametric assumption fails, the binning methods UMB and UWB has better performance with the optimal number of bins (Figure 4(b)). This is because Platt scaling and hybrid methods are intrinsicly biased when \(h^{*}\notin\mathcal{H}_{\text{logit-normal}}\), as noted in Section 4.2.

Quantitative results of risks under optimal \(B\)For each setting, we fix \(B\) that achieves low recalibration risk for UWB and UMB in Figure 5. Specifically, we choose \(B=2\left\lfloor n^{1/3}\right\rfloor=34\) for the correct parametric assumption setting, and \(B=\left\lfloor n^{1/3}\right\rfloor=17\) for the misspecified parametric assumption setting. Then, for each setting, we compare the 90% quantiles of risks of each recalibration method fitted on 100 random replicates of calibration datasets of size \(n=5000\).

Table 1 quantititively verifies that Platt and the Hybrid method achieves lower \(R^{\mathrm{cal}}\) and overall \(R\) if the parametric assumption is correct, and UWB and UMB achieves lower \(R^{\mathrm{cal}}\) and overall \(R\) when the parametric assumption fails.

Visualization of calibration curvesWe fix the calibration dataset and visualize the calibration curves for all methods under the two settings. Figure 3 shows that the binning methods (UWB and UMB) closely track the optimal recalibration function \(h^{*}\) in both settings. In contrast, the hybrid approach follows the Platt scaling estimates, leading to an inherent bias from \(h^{*}\) when the parametric assumption is invalid (Figure 2(b)).

### Comparing recalibration schemes under label shift

We consider the label shift with source distribution \(\mathcal{D}(0.5)\) and target distribution \(\mathcal{D}(\pi_{Q})\), where \(\pi_{Q}\) varies in \(\{0.01,0.05,0.1,0.2,0.3,0.4,0.5\}\). The results where \(\pi_{Q}>0.5\) can be inferred by symmetry and hence not experimented. We vary \(n_{P}\) in \(\{10,10^{3},10^{5},10^{7}\}\) and \(n^{Q}\) in \(\{10,10^{3},10^{5}\}\).

Figure 5: Risks vs. number of bins \(B\).

Aside from our proposed recalibration function \(\hat{h}_{Q}=\hat{g}\circ\hat{h}_{P}\) (17), referred to as Composite, we consider three other calibration approaches as baselines: (1) Source, denoted as \(\hat{h}_{P}\), which is only calibrated on the source data, (2) Label-Shift, denoted as \(\hat{g}\), which performs label shift correction without calibration, and (3) Target, denoted as \(\hat{g}_{Q}^{\text{target}}\), which is only calibrated on the target data.

The number of bins \(B\) are chosen to be \(n_{P}^{1/3}\) for Composite and Source, and \(n_{Q}^{1/3}\) for Target.

Table 2 shows the risks for different approaches with \(\pi_{Q}=0.1\), \(n_{P}=10^{3}\), and \(n_{Q}=10^{2}\). In terms of \(R^{\text{cal}}\), Composite performs the best, as it is calibrated to the target distribution by taking advantage of the abundant source data. In terms of \(R^{\text{sha}}\), Label-Shift achieves \(R^{\text{sha}}=0\) due to the strictly increasing \(\hat{g}\), but it suffers from high \(R^{\text{cal}}\). Composite and Source achieve smaller \(R^{\text{sha}}\) than Target, as a result of using more bins on a larger sample. Considering the combined impact of calibration and sharpness, our approach Composite attains the lowest overall recalibration risk \(R\) as well as MSE.

Figure 6 shows the optimal recalibration function \(h^{*}\) and the recalibration functions for the four approaches. It can be seen that Composite best estimates \(h^{*}\) with the highest resolution.