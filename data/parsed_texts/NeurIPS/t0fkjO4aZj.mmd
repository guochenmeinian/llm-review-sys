# A Unified Framework for Information-Theoretic Generalization Bounds

Yifeng Chu   Maxim Raginsky

{ychu26,maxim}@illinois.edu

Department of Electrical and Computer Engineering and Coordinated Science Laboratory, University of Illinois, Urbana, IL 61801, USA.

###### Abstract

This paper presents a general methodology for deriving information-theoretic generalization bounds for learning algorithms. The main technical tool is a probabilistic decorrelation lemma based on a change of measure and a relaxation of Young's inequality in \(L_{\psi_{p}}\) Orlicz spaces. Using the decorrelation lemma in combination with other techniques, such as symmetrization, couplings, and chaining in the space of probability measures, we obtain new upper bounds on the generalization error, both in expectation and in high probability, and recover as special cases many of the existing generalization bounds, including the ones based on mutual information, conditional mutual information, stochastic chaining, and PAC-Bayes inequalities. In addition, the Fernique-Talagrand upper bound on the expected supremum of a subgaussian process emerges as a special case.

## 1 Introduction

The generalization error of a learning algorithm is a useful proxy for evaluating the performance of the learned model on previously unseen data. Formally, it is defined as the expected (absolute) difference between the population risk and the empirical risk of the hypothesis returned by the algorithm. One of the classical methods for estimating the generalization error is via uniform convergence of various empirical processes indexed by the hypothesis class [1, 2]. For example, in the analysis of Empirical Risk Minimization, one can estimate the expected generalization error via Rademacher averages, which can be bounded from above using chaining techniques [3].

However, the bounds based on uniform convergence are often too pessimistic and may even become vacuous when the hypothesis space is extremely large, a typical situation with deep neural net models. For this reason, it is preferable to obtain algorithm-dependent generalization bounds that take into account the joint distribution of the training samples and of the output hypothesis. In this context, one capitalizes on the intuition that the generalization ability of a learning algorithm should be related to the amount of information the output hypothesis reveals about the training data. This idea, which has origins in the work on PAC-Bayes methods [4, 5], is the basis of the growing literature on information-theoretic generalization bounds, first proposed in [6] and further developed in [7, 8, 9, 10, 11, 12, 13, 14, 15, 16] and many other works.

In fact, it is possible to effectively combine the information-theoretic approach with the classical framework based on various measures of complexity of the hypothesis class: One can use chaining techniques to successively approximate the hypothesis class by simpler model classes, which can then be analyzed using information-theoretic tools. This methodology, again originating in the PAC-Bayes literature [17], has been developed recently in [18, 19, 20, 21]. Our goal in this work is to develop these ideas further by giving a unified framework for information-theoretic generalization bounds, from which many of the existing results emerge as special cases.

### The main idea, informally

The main idea behind our framework is surprisingly simple. We first give an abstract description and then show how it can be particularized to various settings of interest. Let \((X_{t})_{t\in T}\) be a centered (zero-mean) stochastic process defined on a probability space \((\Omega,\mathcal{A},\mathbb{P})\) and indexed by the elements of some set \(T\). Let \(Q\) be a _Markov kernel_ from \(\Omega\) to \(T\), i.e., a measurable mapping taking each \(\omega\in\Omega\) to a probability measure \(Q(\cdot|\omega)\) on \(T\). Together, \(\mathbb{P}\) and \(Q\) define a probability measure \(\mathbb{P}\otimes Q\) on the product space \(\Omega\times T\). The mathematical object we would like to study is the expected value

\[\langle\mathbb{P}\otimes Q,X\rangle:=\int_{\Omega\times T}X_{t}(\omega)Q( \mathrm{d}t|\omega).\mathbb{P}(\mathrm{d}\omega).\]

For example, assuming that there exists a measurable map \(\tau^{*}:\Omega\to T\), such that

\[X_{\tau^{*}(\omega)}(\omega)=\sup_{t\in T}X_{t}(\omega),\qquad\mathbb{P}- \text{a.s.}\] (1)

we can take \(Q(A|\omega):=\mathbf{1}_{\{\tau^{*}(\omega)\in A\}}\) for all measurable subsets \(A\) of \(T\). Then

\[\langle\mathbb{P}\otimes Q,X\rangle=\mathbf{E}\big{[}\sup_{t\in T}X_{t}\big{]}\]

is the expected supremum of \(X_{t}\), the central object of study in the theory of generic chaining, where \((T,d)\) is a metric space and increments \(X_{u}-X_{v}\) are "stochastically small" relative to \(d(u,v)\). Alternatively, consider a statistical learning problem with instance space \(\mathcal{Z}\), hypothesis space \(\mathcal{W}\), and loss function \(\ell:\mathcal{W}\times\mathcal{Z}\to\mathbb{R}_{+}\). Let \(P_{Z}\) be the (unknown) probability law of the problem instances in \(\mathcal{Z}\). Then we could take \(\Omega=\mathcal{Z}^{n}\), \(\mathbb{P}=P_{Z}^{\otimes n}\), \(T=\mathcal{W}\), and

\[X_{w}=\frac{1}{n}\sum_{i=1}^{n}\big{(}L(w)-\ell(w,Z_{i})\big{)},\]

where \(L(w):=\mathbf{E}_{Z\sim P_{Z}}[\ell(w,Z)]\) is the _population risk_ of \(w\). Let \(Q\) be a (randomized) learning algorithm that associates to each sample \(S=(Z_{1},\ldots,Z_{n})\sim\mathbb{P}\) a probability measure \(Q(\cdot|S)\) on the hypothesis space \(\mathcal{W}\). Then

\[\langle\mathbb{P}\otimes Q,X\rangle=\mathbf{E}\Big{[}\frac{1}{n}\sum_{i=1}^{n }\big{(}L(W)-\ell(W,Z_{i})\big{)}\Big{]}\]

is the expected generalization error of \(Q\). In either case, we can proceed to analyze \(\langle\mathbb{P}\otimes Q,X\rangle\) via a combination of the following two steps:

* **Decorrelation** -- We can remove the correlations encoded in \(\mathbb{P}\otimes Q\) by choosing a convenient product measure \(\mathbb{P}\otimes\mu\) on \(\Omega\times T\), so that (roughly) \[\langle\mathbb{P}\otimes Q,X\rangle\lesssim\sqrt{D(\mathbb{P}\otimes Q| \mathbb{P}\otimes\mu)}+\text{Error}\] provided the process \((X_{t})_{t\in T}\) is regular enough for the error term to be small. Here, we use the relative entropy (or information divergence) \(D(\cdot|\cdot)\) to illustrate the key idea with a minimum of detail; the precise description is given in Section 3.
* **Chaining in the space of measures** -- Since the process \((X_{t})_{t\in T}\) is centered and \(\mathbb{P}\otimes\mu\) is a product measure, we automatically have \(\langle\mathbb{P}\otimes\mu,X\rangle=0\) even though \(\langle\mathbb{P}\otimes Q,X\rangle\neq 0\). We can therefore interpolate between \(\mathbb{P}\otimes Q\) and \(\mathbb{P}\otimes\mu\) along a (possibly infinite) sequence \(Q_{0},Q_{1},\ldots,Q_{K}\) of Markov kernels, such that \(\mathbb{P}\otimes Q_{K}=\mathbb{P}\otimes Q\), \(\mathbb{P}\otimes Q_{0}=\mathbb{P}\otimes\mu\), and the differences \(\langle\mathbb{P}\otimes Q_{k},X\rangle-\langle\mathbb{P}\otimes Q_{k-1},X\rangle\) are suitably small. Telescoping, we get \[\langle\mathbb{P}\otimes Q,X\rangle=\sum_{k=1}^{K}\big{(}\langle\mathbb{P} \otimes Q_{k},X\rangle-\langle\mathbb{P}\otimes Q_{k-1},X\rangle\big{)}.\] For each \(k\), we then apply the decorrelation procedure to the _increment process_\((X_{u}-X_{v})_{u,v\in T}\), with \(\mathbb{P}\) as before and with a suitably chosen family of couplings of \(Q_{k}(\cdot|\omega)\) and \(Q_{k-1}(\cdot|\omega)\). This step can be combined effectively with other techniques, such as symmetrization.

## 2 Preliminaries

Basic definitions.All measurable spaces in this paper are assumed to be standard Borel spaces. The set of all Borel probability measures on a space \(\mathcal{X}\) will be denoted by \(\mathcal{P}(\mathcal{X})\). A _Markov kernel_ from \((\mathcal{X},\mathcal{A})\) to \((\mathcal{Y},\mathcal{B})\) is a mapping \(P_{Y|X}:\mathcal{B}\times\mathcal{X}\to[0,1]\), such that \(P_{Y|X=x}(\cdot):=P_{Y|X}(\cdot|x)\) is an element of \(\mathcal{P}(\mathcal{Y})\) for every \(x\in\mathcal{X}\) and the map \(x\mapsto P_{Y|X=x}(B)\) is measurable for every \(B\in\mathcal{B}\). The set of all such Markov kernels will be denoted by \(\mathcal{M}(\mathcal{Y}|\mathcal{X})\).

The product of \(P_{X}\in\mathcal{P}(\mathcal{X})\) and \(P_{Y|X}\in\mathcal{M}(\mathcal{Y}|\mathcal{X})\) is the probability measure \(P_{X}\otimes P_{Y|X}\in\mathcal{P}(\mathcal{X}\times\mathcal{Y})\) defined on product sets \(A\times B\subseteq\mathcal{X}\times\mathcal{Y}\) by \((P_{X}\otimes P_{Y|X})(A\times B):=\int_{A}P_{Y|X=x}(B)P_{X}(\mathrm{d}x)\) and then extended to all Borel subsets of \(\mathcal{X}\times\mathcal{Y}\) by countable additivity. This defines a joint probability law for a random element \((X,Y)\) of \(\mathcal{X}\times\mathcal{Y}\), so that \(P_{X}\) is the marginal law of \(X\), \(P_{Y|X}\) is the conditional law of \(Y\) given \(X\), and \(P_{Y}(\cdot)=\int_{\mathcal{X}}P_{Y|X=x}(\cdot)P_{X}(\mathrm{d}x)\) is the marginal law of \(Y\). The product measure \(P_{X}\otimes P_{Y}\), under which \(X\) and \(Y\) are independent, is a special case of this if we interpret \(P_{Y}\) as a trivial Markov kernel with \(P_{Y|X=x}=P_{Y}\) for all \(x\).

A _coupling_ of \(\mu\in\mathcal{P}(\mathcal{X})\) and \(\nu\in\mathcal{P}(\mathcal{Y})\) is a probability measure \(P\in\mathcal{P}(\mathcal{X}\times\mathcal{Y})\), such that \(P(\cdot\times\mathcal{Y})=\mu(\cdot)\) and \(P(\mathcal{X}\times\cdot)=\nu(\cdot)\). We will denote the set of all couplings of \(\mu\) and \(\nu\) by \(\Pi(\mu,\nu)\). Let the space \(\mathcal{X}\cup\mathcal{Y}\) be equipped with a metric \(d\), and let \(\mathcal{P}_{p}\), for \(p\geq 1\), denote the space of all probability measures \(\rho\) on \(\mathcal{X}\cup\mathcal{Y}\), for which there exists some \(z_{0}\in\mathcal{X}\cup\mathcal{Y}\) such that \(\int_{\mathcal{X}\cup\mathcal{Y}}d(z,z_{0})^{p}\rho(\mathrm{d}z)<\infty\). Then the _\(p\)-Wasserstein distance_ between \(\mu\in\mathcal{P}(\mathcal{X})\cap\mathcal{P}_{p}\) and \(\nu\in\mathcal{P}(\mathcal{Y})\cap\mathcal{P}_{p}\) is given by

\[\mathsf{W}_{p}(\mu,\nu):=\inf_{\pi\in\Pi(\mu,\nu)}\Bigg{(}\int d(x,y)^{p}\pi( \mathrm{d}x,\mathrm{d}y)\Bigg{)}^{1/p}\]

(see [22, 23] for details).

\(L^{p}\) and \(L_{\psi_{p}}\) spaces.The \(L^{p}(\mu)\) norms of \(f:\mathcal{X}\to\mathbb{R}\), for \(\mu\in\mathcal{P}(\mathcal{X})\) and \(p\geq 1\), are defined as

\[\|f\|_{L^{p}(\mu)}:=\Big{(}\int_{\mathcal{X}}|f|^{p}\,\mathrm{d}\mu\Big{)}^{1/p}\]

whenever the expectation on the right-hand side exists. We will often use the linear functional notation for expectations, i.e., \(\langle\mu,f\rangle=\int_{\mathcal{X}}f\,\mathrm{d}\mu\).

For \(p\geq 1\), define the function \(\psi_{p}:\mathbb{R}_{+}\to\mathbb{R}_{+}\) by \(\psi_{p}(x):=\exp(x^{p})-1\). Its inverse is given by \(\psi_{F}^{-1}(x)=\big{(}\log(x+1)\big{)}^{1/p}\), where \(\log\) will always denote natural logarithms. Some useful properties of these two functions are collected in Appendix A of Supplementary Material. The function \(\psi_{p}\) arises in the context of controlling the tail behavior of random variables (see [1, 24, 25] for details). The _Orlicz \(\psi_{p}\)-norm_ of a real-valued random variable \(X\) is defined as

\[\|X\|_{\psi_{p}}:=\inf\Big{\{}c>0:\mathbf{E}\Big{[}\psi_{p}\Big{(}\frac{|X|}{c} \Big{)}\Big{]}\leq 1\Big{\}},\]

and the tails of \(X\) satisfy \(\mathbf{P}\|X\|\geq u\|\leq Ke^{-Cu^{p}}\) for all \(u\geq 0\) and some \(K,C>0\) if and only if \(\|X\|_{\psi_{p}}<\infty\). The Orlicz space \(L_{\psi_{p}}\) is the space of all random variables \(X\) with \(\|X\|_{\psi_{p}}<\infty\). In particular, if \(X\) is \(\sigma\)-subgaussian, i.e., \(\mathbf{P}\|X\|\geq u\|\leq 2e^{-u^{2}/2\sigma^{2}}\) for all \(u\geq 0\), then \(\|X\|_{\psi_{2}}\leq\sqrt{6}\sigma\); conversely, every \(X\in L_{\psi_{2}}\) is \(\sigma\)-subgaussian with \(\sigma\leq c\|X\|_{\psi_{2}}\) for some absolute constant \(c>0\).

Information-theoretic quantities.The relative entropy (or information divergence) \(D(\mu\|\nu)\) between two probability measures \(\mu,\nu\) on the same space \(\mathcal{X}\) is defined as

\[D(\mu\|\nu):=\Big{\langle}\mu,\log\frac{\mathrm{d}\mu}{\mathrm{d}\nu}\Big{\rangle}\]

if \(\mu\ll\nu\) (i.e., \(\mu\) is absolutely continuous w.r.t. \(\nu\)), and \(D(\mu\|\nu):=+\infty\) otherwise. The following inequality will be useful (proofs of all results are in Appendix B of the Supplementary Material):

**Proposition 1**.: _If \(\mu\ll\nu\), then for any \(p\geq 1\)_

\[\Big{\langle}\mu,\psi_{p}^{-1}\Big{(}\frac{\mathrm{d}\mu}{\mathrm{d}\nu}\Big{)} \Big{\rangle}\leq\big{(}D(\mu\|\nu)+1\big{)}^{1/p}.\]The _conditional divergence_ between \(P_{V|U},Q_{V|U}\in\mathcal{M}(\mathcal{V}|\mathcal{U})\) given \(P_{U}\in\mathcal{P}(\mathcal{U})\) is defined by

\[D(P_{V|U}\|Q_{V|U}|P_{U}):=D(P_{U}\otimes P_{V|U}\|P_{U}\otimes Q_{V|U}).\]

The mutual information \(I(X;Y):=D(P_{Y|X}\|P_{Y}|P_{X})\) and conditional mutual information \(I(X;Y|Z):=D(P_{Y|XZ}\|P_{Y|Z}|P_{XZ})\) are special cases of the above definition, and the identities

\[D(P_{Y|X}\|Q_{Y}|P_{X}) =I(X;Y)+D(P_{Y}\|Q_{Y})\] (2) \[D(P_{Y|XZ}\|Q_{Y|Z}|P_{XZ}) =I(X;Y|Z)+D(P_{Y|Z}\|Q_{Y|Z}|P_{Z}).\] (3)

hold whenever all the quantities are finite. See, e.g., [26] for details.

## 3 The decorrelation lemma

All of our subsequent developments make use of the following _decorrelation lemma_:

**Lemma 1**.: _Let \(\mu,\nu\) be two probability measures on a space \(\mathcal{X}\) such that \(\mu\ll\nu\), and let \(f,g:\mathcal{X}\to\mathbb{R}_{+}\) be two nonnegative measurable functions. Then the following inequalities hold:_

\[\langle\mu,fg\rangle\leq 2^{1/p}\Big{\langle}\mu,f\psi_{p}^{-1}\Big{(} \frac{\mathrm{d}\mu}{\mathrm{d}\nu}\Big{)}\Big{\rangle}+\langle\nu,f\psi_{p}( g)\rangle\] (4)

_and_

\[\langle\mu,fg\rangle\leq 2^{1/p}\|f\|_{L^{2}(\nu)}+4^{1/p}\Big{\langle}\mu,f \psi_{p}^{-1}\Big{(}\frac{\mathrm{d}\mu}{\mathrm{d}\nu}\Big{)}\Big{\rangle}+4 ^{1/p}\|f\|_{L^{1}(\mu)}\big{(}\log\langle\nu,\exp(g^{p})\rangle\big{)}^{1/p}.\] (5)

The proof makes extensive use of various properties of \(\psi_{p}\) and \(\psi_{p}^{-1}\). In particular, Eq. (4) is a relaxation of the Young-type inequality \(xy\leq\psi_{p}^{*}(x)+\psi_{p}(y)\), where \(\psi_{p}^{*}(x):=\sup_{y\geq 0}(xy-\psi_{p}(y))\) is the (one-sided) Lengendre-Fenchel conjugate of \(\psi_{p}\). (We refer the reader to [13] for another use of duality in Orlicz spaces in the context of generalization bounds.)

Every use of Lemma 1 in the sequel will be an instance of the following scheme: Let \(P_{X}\in\mathcal{P}(\mathcal{X})\), \(Q_{Y}\in\mathcal{P}(\mathcal{Y})\), and \(P_{Y|X}\in\mathcal{M}(\mathcal{Y}|\mathcal{X})\) be given, such that \(P_{Y|X=x}\ll Q_{Y}\) for all \(x\in\mathcal{X}\). Let \((X,Y,\bar{Y})\) be a random element of \(\mathcal{X}\times\mathcal{Y}\times\mathcal{Y}\) with joint law \(P_{X}\otimes P_{Y|X}\otimes Q_{Y}\); in particular, \(\bar{Y}\) is independent of \((X,Y)\). Furthermore, let \(f:\mathcal{Y}\to\mathbb{R}_{+}\) and \(g:\mathcal{X}\times\mathcal{Y}\to\mathbb{R}_{+}\) be given, such that \(\mathbf{E}[\psi_{p}(g(X,y))]\leq 1\) for all \(y\in\mathcal{Y}\). Then, applying Lemma 1 conditionally on \(X=x\) with \(\mu=P_{Y|X=x}\), \(\nu=Q_{Y}\), \(f\), and \(g(x,\cdot)\), and then taking expectations w.r.t. \(P_{X}\), we obtain

\[\mathbf{E}[f(Y)g(X,Y)]\leq 2^{1/p}\mathbf{E}\Bigg{[}f(Y)\psi_{p}^{-1}\Bigg{(} \frac{\mathrm{d}P_{Y|X}}{\mathrm{d}Q_{Y}}(Y)\Bigg{)}\Bigg{]}+\mathbf{E}[f( \bar{Y})].\]

In specific cases, the quantity on the right-hand side can be further upper-bounded in terms of the information divergences \(D(P_{Y|X}\|Q_{Y})\) using Proposition 1.

## 4 Some estimates for the absolute generalization error

We adopt the usual set-up for the analysis of (possibly randomized) learning algorithms and their generalization error. Let an instance space \(\mathcal{Z}\), a hypothesis space \(\mathcal{W}\), and a nonnegative loss function \(\ell:\mathcal{W}\times\mathcal{Z}\to\mathbb{R}_{+}\) be given. A _learning algorithm_ is a Markov kernel \(P_{W|S}\) from the product space \(\mathcal{Z}^{n}\) into \(\mathcal{W}\), which takes as input an \(n\)-tuple \(S=(Z_{1},\ldots,Z_{n})\) of i.i.d. random elements of \(\mathcal{Z}\) with unknown marginal probability law \(P_{Z}\) and generates a random element \(W\) of \(\mathcal{W}\). We define the _empirical risk_ and the _expected_ (or _population_) _risk_ of each \(w\in\mathcal{W}\) by

\[L_{n}(w):=\langle P_{n},\ell(w,\cdot)\rangle=\frac{1}{n}\sum_{i=1}^{n}\ell(w,Z_ {i}),\qquad L(w):=\langle P_{Z},\ell(w,\cdot)\rangle=\mathbf{E}[\ell(w,Z)]\]

where \(P_{n}\) is the empirical distribution of \(S\), and the _pointwise generalization error_ by

\[\mathrm{gen}(w,S):=L(w)-L_{n}(w).\]It will also be convenient to introduce an auxiliary \(n\)-tuple \(S^{\prime}=(Z_{1}^{\prime},\ldots,Z_{n}^{\prime})\sim P_{Z}^{\otimes n}\), which is independent of \((S,W)\sim P_{Z}^{\otimes n}\otimes P_{W|S}\). We will use \(\tilde{S}\) to denote the pair \((S^{\prime},S)\) and write \(L_{n}^{\prime}(w)\) for the empirical risk of \(w\) on \(S^{\prime}\).

As a first illustration of our general approach, we show that it can be used to recover some existing results on the generalization error, including the bounds of Xu and Raginsky [7] involving the mutual information and the bounds of Steinke and Zakynthinou [10] involving the conditional mutual information. We start with the following estimate on the expected value of \(|\mathrm{gen}(W,S)|\):

**Theorem 1**.: _Assume the random variables \(\ell(w,Z)\), \(w\in\mathcal{W}\), are \(\sigma\)-subgaussian when \(Z\sim P_{Z}\). Let a learning algorithm \(P_{W|S}\) be given. Then, for any \(Q_{W}\in\mathcal{P}(\mathcal{W})\),_

\[\mathbf{E}[|\mathrm{gen}(W,S)|]\leq\sqrt{\frac{12\sigma^{2}}{n}}\Bigg{(} \mathbf{E}\Bigg{[}\psi_{2}^{-1}\Bigg{(}\frac{\mathrm{d}P_{W|S}}{\mathrm{d}Q_{ W}}\Bigg{)}\Bigg{]}+1\Bigg{)},\] (6)

_where the expectation on both sides is w.r.t. \(P_{S}\otimes P_{W|S}=P_{Z}^{\otimes n}\otimes P_{W|S}\)._

The key step in the proof is to apply the decorrelation lemma, conditionally on \(S\), to \(\mu=P_{W|S}\), \(\nu=Q_{W}\), \(f(w)=\sigma\sqrt{6/n}\), and \(g(w)=\frac{|\mathrm{gen}(w,S)|}{\sigma\sqrt{6/n}}\). The same subgaussianity assumption was also made by Xu and Raginsky [7]. Minimizing the right-hand side of (6) over \(Q_{W}\), we recover their generalization bound up to a multiplicative constant and an extra \(O(1/\sqrt{n})\) term (which is unavoidable since we are bounding the expected _absolute_ generalization error):

**Corollary 1**.: _Under the assumptions of Theorem 1,_

\[\mathbf{E}[|\mathrm{gen}(W,S)|]\leq\sqrt{\frac{24\sigma^{2}}{n}}\big{(}I(W;S)+ 4\big{)}.\] (7)

A notable shortcoming of Theorem 1 and Corollary 1 is that they yield vacuous bounds whenever the mutual information \(I(W;S)\) is infinite, which will be the case, e.g., when the marginal probability laws \(P_{Z}\) and \(P_{W}\) are nonatomic (i.e., assign zero mass to singletons) and the learning algorithm is deterministic. To remove this drawback, we will use an elegant auxiliary randomization device introduced by Steinke and Zakynthinou [10].

Let \(\varepsilon=(\varepsilon_{1},\ldots,\varepsilon_{n})\) be an \(n\)-tuple of i.i.d. Rademacher random variables, i.e., \(\mathbf{P}[\varepsilon_{i}=\pm 1]=1/2\), independent of \(\tilde{S}\). For each \(i\) let \(\tilde{Z}_{i}^{1}:=Z_{i}\) and \(\tilde{Z}_{i}^{-1}:=Z_{i}^{\prime}\) and let \(\bar{P}=\bar{P}_{\tilde{S}\epsilon W}\) be the joint probability law of \((\tilde{S},\varepsilon,W)\), such that \(\bar{P}_{\tilde{S}\varepsilon}=P_{\tilde{S}}\otimes P_{\varepsilon}\) and \(\bar{P}_{W|\tilde{S}\varepsilon}:=P_{W|\tilde{S}^{\varepsilon}}\) where \(S^{\varepsilon}:=(\tilde{Z}_{1}^{\varepsilon_{1}},\ldots,\tilde{Z}_{n}^{ \varepsilon_{n}})\). In other words, under \(\bar{P},\tilde{S}\) and \(\varepsilon\) are independent and have their respective marginal distributions, while \(W\) is generated by feeding the learning algorithm \(P_{W|S}\) with the tuple \(\tilde{S}^{\varepsilon}\). Consequently, \(W\) is independent of \(\tilde{S}^{-\varepsilon}=(\tilde{Z}_{1}^{-\varepsilon_{1}},\ldots,\tilde{Z}_{n }^{-\varepsilon_{n}})\). Then, letting \(P\) be the joint law of \((\tilde{S},W)\), we have

\[\mathbf{E}_{P}[|\mathrm{gen}(W,S)|] =\mathbf{E}_{P}\big{|}\mathbf{E}_{P}[L_{n}^{\prime}(W)-L_{n}(W)|S,W]\big{|}\] \[\leq\mathbf{E}_{P}|L_{n}^{\prime}(W)-L_{n}(W)|\] \[=\mathbf{E}_{\bar{P}}\Big{|}\frac{1}{n}\sum_{i=1}^{n}\Big{(}\ell( W,\tilde{Z}_{i}^{-\varepsilon_{i}})-\ell(W,\tilde{Z}_{i}^{\varepsilon_{i}}) \Big{)}\Big{|}\] \[=\mathbf{E}_{\bar{P}}\Big{|}\frac{1}{n}\sum_{i=1}^{n}\varepsilon_ {i}\Big{(}\ell(W,Z_{i}^{\prime})-\ell(W,Z_{i})\Big{)}\Big{|}.\]

Thus, all the analysis can be carried out w.r.t. \(\bar{P}\), as in the following:

**Theorem 2**.: _Assume there exists a function \(\Delta:\mathcal{Z}\times\mathcal{Z}\to\mathbb{R}_{+}\), such that \(|\ell(w,z)-\ell(w,z^{\prime})|\leq\Delta(z,z^{\prime})\) for all \(w\in\mathcal{W}\) and \(z,z^{\prime}\in\mathcal{Z}\). Then for any Markov kernel \(Q_{W|\tilde{S}}\) with access to \(\tilde{S}\) but not to \(\varepsilon\) we have_

\[\mathbf{E}_{P}[|\mathrm{gen}(W,S)|]\leq\frac{\sqrt{12}}{n}\mathbf{E}_{P}\Bigg{[} \|\Delta(\tilde{S})\|_{\ell^{2}}\Bigg{(}\psi_{2}^{-1}\Bigg{(}\frac{\mathrm{d} \bar{P}_{W|\tilde{S}\varepsilon}}{\mathrm{d}Q_{W|\tilde{S}}}\Bigg{)}+1\Bigg{)} \Bigg{]},\] (8)

_where \(\|\Delta(\tilde{s})\|_{\ell^{2}}:=\big{(}\sum_{i=1}^{n}\Delta(z_{i},z_{i}^{ \prime})^{2}\big{)}^{1/2}\)._The same assumption on \(\ell\) was also made in [10]. Optimizing over \(Q_{W|\tilde{S}}\), we can recover their Theorem 5.1 (again, up to a multiplicative constant and a \(O(1/\sqrt{n})\) fluctuation term):

**Corollary 2**.: _Under the assumptions of Theorem 2,_

\[\mathbf{E}_{P}[|\mathrm{gen}(W,S)|]\leq\sqrt{\frac{24}{n}\mathbf{E}[\Delta^{2}( Z,Z^{\prime})]\big{(}I(W;\varepsilon|\tilde{S})+4\big{)}},\] (9)

_where \(Z\) and \(Z^{\prime}\) are independent samples from \(P_{Z}\) and where the conditional mutual information is computed w.r.t. \(\tilde{P}\)._

The main advantage of using conditional mutual information is that it never exceeds \(n\log 2\) (of course, the bound is only useful if \(I(W;\varepsilon|\tilde{S})=o(n)\)).

## 5 Estimates using couplings

We now turn to the analysis of \(\mathbf{E}[\mathrm{gen}(W,S)]\) using couplings. The starting point is the following observation: With \((S^{\prime},S,W)\) be constructed as before, consider the quantities

\[\tilde{L}_{n}(w):=L^{\prime}_{n}(w)-L_{n}(w)\equiv\frac{1}{n}\sum_{i=1}^{n} \big{(}\ell(w,Z^{\prime}_{i})-\ell(w,Z_{i})\big{)}.\]

Then, using the fact that \(\langle P_{\tilde{S}}\otimes Q_{W},\tilde{L}_{n}\rangle=0\) for any \(Q_{W}\in\mathcal{P}(\mathcal{W})\), we have

\[\mathbf{E}[\mathrm{gen}(W,S)] =\langle P_{\tilde{S}}\otimes P_{W|S},\tilde{L}_{n}\rangle- \langle P_{\tilde{S}}\otimes Q_{W},\tilde{L}_{n}\rangle\] \[=\int_{\mathcal{Z}\times\mathcal{Z}}P_{\tilde{S}}(\mathrm{d} \tilde{s})\big{(}\langle P_{W|S=s},\tilde{L}_{n}\rangle-\langle Q_{W},\tilde{ L}_{n}\rangle\big{)}.\] (10)

This suggests the idea of introducing, for each \(s\in\mathcal{Z}^{n}\), a coupling of \(P_{W|S=s}\) and \(Q_{W}\), i.e., a probability law \(P_{UV|S=s}\) for a random element \((U,V)\) of \(\mathcal{W}\times\mathcal{W}\) with marginals \(P_{U}=P_{W|S=s}\) and \(P_{V}=Q_{W}\). We then have the following:

**Theorem 3**.: _For \(u,v\in\mathcal{W}\) and \(\tilde{s}=(s,s^{\prime})\in\mathcal{Z}^{n}\times\mathcal{Z}^{n}\), define_

\[\sigma^{2}(u,v,\tilde{s}):=\sum_{i=1}^{n}\Big{(}\big{(}\ell(u,z^{\prime}_{i}) -\ell(v,z^{\prime}_{i})\big{)}-\big{(}\ell(u,z_{i})-\ell(v,z_{i})\big{)}\Big{)} ^{2}\,.\] (11)

_Then, for any \(Q_{W}\in\mathcal{P}(\mathcal{W})\), any family of couplings \(P_{UV|S=s}\in\Pi(P_{W|S=s},Q_{W})\) depending measurably on \(s\in\mathcal{Z}^{n}\), and any \(\mu_{UV}\in\mathcal{P}(\mathcal{W}\times\mathcal{W})\),_

\[\mathbf{E}[\mathrm{gen}(W,S)]\leq\frac{\sqrt{24}}{n}\mathbf{E}\Bigg{[}\sigma (U,V,\tilde{S})\psi_{2}^{-1}\Bigg{(}\frac{\mathrm{d}P_{UV|S}}{\mathrm{d}\mu_{ UV}}\Bigg{)}+\sqrt{\mathbf{E}[\sigma^{2}(\bar{U},\bar{V},\tilde{S})|\tilde{S}]} \Bigg{]},\] (12)

_where the expectation on the right-hand side is w.r.t. the joint law of \((U,V,\bar{U},\bar{V},\tilde{S})\), under which \((S,U,V)\) are distributed according to \(P_{S}\otimes P_{UV|S}\), \((\bar{U},\bar{V})\) are distributed according to \(\mu_{UV}\) independently of \((U,V,S)\), and \(S^{\prime}\) is distributed according to \(P_{S}\) independently of everything else._

The proof makes essential use of symmetrization using an auxiliary \(n\)-tuple \(\varepsilon\) of i.i.d. Rademacher random variables, which allows us to apply Lemma 1 conditionally on \(\tilde{S}\).

The coupling-based formulation looks rather complicated compared to the setting of Section 4. However, being able to choose not just the "prior" \(Q_{W}\), but also the couplings \(P_{UV|S}\) of \(P_{W|S}\) and \(Q_{W}\) and the reference measure \(\mu_{UV}\), allows us to overcome some of the shortcomings of the set-up of Section 4. Consider, for example, the case when the learning algorithm ignores the data, i.e., \(P_{W|S}=P_{W}\). Then we can choose \(Q_{W}=P_{W}\), \(P_{UV|S}(\mathrm{d}u,\mathrm{d}v)=P_{W}(\mathrm{d}u)\otimes\delta_{u}(\mathrm{ d}v)\), where \(\delta_{u}\) is the Dirac measure concentrated on the point \(u\), and \(\mu_{UV}=P_{UV}\) (since the latter does not depend on \(S\)). With these choices, \(U=V\) and \(\bar{U}=\bar{V}\) almost surely, so the right-hand side of (12) is identically zero. By contrast, the bounds of Theorems 1 and 2 always include an additional \(O(1/\sqrt{n})\) term even when \(W\) and \(\tilde{S}\) are independent.

Moreover, Theorem 3 can be used to recover the bounds of Theorems 1 and 2 up to multiplicative constants. For example, to recover Theorem 1, we apply Theorem 3 with \(P_{UV|S}=P_{W|S}\otimes Q_{W}\), \(\mu_{UV}=Q_{W}\otimes Q_{W}\), and with an estimate on \(\sigma(U,V,\tilde{S})\) based on the subgaussianity of \(\ell(w,Z)\).

For a more manageable bound that will be useful later, let us define the following for \(u,v\in\mathcal{W}\):

\[d_{S,\ell}(u,v) :=\bigg{(}\frac{1}{n}\sum_{i=1}^{n}\big{(}\ell(u,Z_{i})-\ell(v,Z_{ i})\big{)}^{2}\bigg{)}^{1/2}\equiv\|\ell(u,\cdot)-\ell(v,\cdot)\|_{L^{2}(P_{n})}\] \[d_{\ell}(u,v) :=\bigg{(}\mathbf{E}\big{[}\big{(}\ell(u,Z)-\ell(v,Z)\big{)}^{2} \big{]}\bigg{)}^{1/2}\equiv\|\ell(u,\cdot)-\ell(v,\cdot)\|_{L^{2}(P_{Z})},\]

**Corollary 3**.: _Under the assumptions of Theorem 3,_

\[\mathbf{E}[\mathrm{gen}(W,S)]\leq\sqrt{\frac{48}{n}}\mathbf{E}\bigg{[}\big{(} d_{\ell}(U,V)+d_{S,\ell}(U,V)\big{)}\psi_{2}^{-1}\bigg{(}\frac{\mathrm{d}P_{UV|S}} {\mathrm{d}\mu_{UV}}\bigg{)}+d_{\ell}(\bar{U},\bar{V})\bigg{]}.\]

## 6 Refined estimates via chaining in the space of measures

We now combine the use of couplings as in Section 5 with a chaining argument. The basic idea is as follows: Instead of coupling \(P_{W|S}\) with \(Q_{W}\) directly, we interpolate between them using a (possibly infinite) sequence of Markov kernels \(P_{W|S}^{0},P_{W|S}^{1},\ldots,P_{W|S}^{K}\), such that \(P_{W|S}^{0}=Q_{W}\) and \(P_{W|S}^{K}=P_{W|S}\) (or \(\lim_{k\to\infty}P_{W|S}^{k}=P_{W|S}\) in an appropriate sense, e.g., weakly for each \(S\), if the sequence is infinite). Given any such sequence, we telescope the terms in (10) as follows:

\[\mathbf{E}[\mathrm{gen}(W,S)]=\int_{\mathcal{Z}\times\mathcal{Z}}P_{\tilde{S}} (\mathrm{d}\tilde{s})\sum_{k=1}^{K}\bigg{(}\langle P_{W|S=s}^{k},\tilde{L}_{n }\rangle-\langle P_{W|S=s}^{k-1},\tilde{L}_{n}\rangle\bigg{)}.\]

For each \(k\), we can now choose a family of random couplings \(P_{W_{k}W_{k-1}|S}\in\Pi(P_{W|S}^{k},P_{W|S}^{k-1})\) and a deterministic probability measure \(\rho_{W_{k}W_{k-1}}\in\mathcal{P}(\mathcal{W}\times\mathcal{W})\). The following is an immediate consequence of applying Corollary 3 to each summand:

**Theorem 4**.: _Let \(P_{W|S}\), \(Q_{W}\), \(P_{W_{k}W_{k-1}|S}\), and \(\rho_{W_{k}W_{k-1}}\) be given as above. Then_

\[\mathbf{E}[\mathrm{gen}(W,S)]\] \[\leq\sqrt{\frac{48}{n}}\sum_{k=1}^{K}\mathbf{E}\bigg{[}\big{(}d_{ \ell}(W_{k},W_{k-1})+d_{S,\ell}(W_{k},W_{k-1})\big{)}\psi_{2}^{-1}\bigg{(} \frac{\mathrm{d}P_{W_{k}W_{k-1}|S}}{\mathrm{d}\rho_{W_{k}W_{k-1}}}\bigg{)}+d_ {\ell}(\bar{W}_{k},\bar{W}_{k-1})\bigg{]},\]

_where in the \(k\)th term on the right-hand side \((S,W_{k},W_{k-1})\) are jointly distributed according to \(P_{S}\otimes P_{W_{k}W_{k-1}|S}\) and \((\bar{W}_{k},\bar{W}_{k-1})\) are jointly distributed according to \(\rho_{W_{k}W_{k-1}}\)._

Apart from Theorem 1, we have been imposing only minimal assumptions on \(\ell\) and then using symmetrization to construct various subgaussian random variables conditionally on \(W\) and \(\tilde{S}\). For the next series of results, we will assume something more, namely that \((\mathcal{W},d)\) is a metric space and that the following holds for the centered loss \(\bar{\ell}(w,z):=\ell(w,z)-\mathbf{E}[\ell(w,Z)]\):

\[\bigg{\|}\sum_{i=1}^{n}(\bar{\ell}(u,Z_{i})-\bar{\ell}(v,Z_{i}))\bigg{\|}_{ \psi_{2}}\leq\sqrt{n}d(u,v),\quad\forall u,v\in\mathcal{W}.\] (13)

In other words, the centered empirical process \(\frac{1}{\sqrt{n}}\sum_{i=1}^{n}\bar{\ell}(w,Z_{i})\) indexed by the elements of \((\mathcal{W},d)\) is a subgaussian process [1, 2, 3].

**Theorem 5**.: _Assume (13). Then_

\[\mathbf{E}[\mathrm{gen}(W,S)]\leq\sqrt{\frac{2}{n}}\sum_{k=1}^{K}\mathbf{E} \Bigg{[}d(W_{k},W_{k-1})\psi_{2}^{-1}\bigg{(}\frac{\mathrm{d}P_{W_{k}W_{k-1}|S} }{\mathrm{d}\rho_{W_{k}W_{k-1}}}\bigg{)}+d(\bar{W}_{k},\bar{W}_{k-1})\Bigg{]}\] (14)

As a byproduct, we recover the stochastic chaining bounds of Zhou et al. [20] (which, in turn, subsume the bounds of Asadi et al. [18]):

**Corollary 4**.: _Let \(P_{Z}\) and \(P_{W|S}\) be given, and let \(P_{W}\) be the marginal law of \(W\). Let \(\big{(}P_{W_{k}|S}\big{)}_{k\geq 0}\) be a sequence of Markov kernels satisfying the following conditions: (i) \(P_{W_{0}|S}=P_{W}\); (ii) \(P_{W_{k}|S}\xrightarrow{k\to\infty}P_{W|S}\); (iii) for every \(k\geq 1\), \(S-W-W_{k}-W_{k-1}\) is a Markov chain. Then_

\[\mathbf{E}[\mathrm{gen}(W,S)] \leq\sqrt{\frac{2}{n}}\sum_{k=1}^{\infty}\mathbf{E}\Big{[}d(W_{k},W_{k-1})\Big{(}\sqrt{D(P_{S|W_{k}}||P_{S})}+1\Big{)}\Big{]}\] (15) \[\leq\sqrt{\frac{2}{n}}\sum_{k=1}^{\infty}\sqrt{\mathbf{E}[d^{2}(W _{k},W_{k-1})]}(\sqrt{I(W_{k};S)}+2).\] (16)

Finally, we give an estimate based on \(2\)-Wasserstein distances (cf. Section 2 for definitions and notation). Let \(\mathsf{W}_{2}(\cdot,\cdot)\) be the \(2\)-Wasserstein distance on \(\mathcal{P}_{2}(\mathcal{W})\) induced by the metric \(d\) on \(\mathcal{W}\). A (constant-speed) _geodesic_ connecting two probability measures \(P,Q\in\mathcal{P}_{2}(\mathcal{W})\) is a continuous path \([0,1]\ni t\mapsto\rho_{t}\in\mathcal{P}_{2}(\mathcal{W})\), such that \(\rho_{0}=P\), \(\rho_{1}=Q\), and \(\mathsf{W}_{2}(\rho_{s},\rho_{t})=(t-s)\mathsf{W}_{2}(P,Q)\) for all \(0\leq s\leq t\leq 1\)[22, 23]. Then we have the following corollary of Theorem 5:

**Corollary 5**.: _Let \(P_{Z}\) and \(P_{W|S}\) be given, and let \(P_{W}\) be the marginal law of \(W\). With respect to 2-Wasserstein distance, let \((P_{W_{k}|S})_{0\leq k\leq K}\) be some points on the constant-speed geodesic \((\rho_{t})_{t\in[0,1]}\) with endpoints \(\rho_{0}=P_{W_{0}|S}=P_{W|S}\) and \(\rho_{1}=P_{W_{K}|S}=P_{W}\) (where \(K\) may be infinite), i.e., there exist some \(t_{0}=0<t_{1}<\dots<t_{k}<\dots<t_{K}=1\), such that \(P_{W_{k}|S}=\rho_{t_{k}}\) for \(k=0,1,\dots\). For each \(k\) let \(P_{W_{k}W_{k-1}|S}\) be the optimal coupling between the neighboring points \(P_{W_{k-1}|S}\) and \(P_{W_{k}|S}\), i.e., the one that achieves \(\mathsf{W}_{2}(P_{W_{k-1}|S},P_{W_{k}|S})\). Then_

\[\mathbf{E}[\mathrm{gen}(W,S)]\] \[\leq\sqrt{\frac{2}{n}}\Bigg{(}2\,\mathbf{E}[\mathsf{W}_{2}(P_{W|S },P_{W})]+\sum_{k=1}^{K}\mathbf{E}\Big{[}\mathsf{W}_{2}(P_{W_{k}|S},P_{W_{k-1} |S})\sqrt{D(P_{W_{k}W_{k-1}|S}||P_{W_{k}W_{k-1}})}\,\Big{]}\Bigg{)}.\] (17)

Observe that the first term on the right-hand side of (17) is the expected \(2\)-Wasserstein distance between the posterior \(P_{W|S}\) and the prior \(P_{W}\), while the second term is a sum of "divergence weighted" Wasserstein distances. Also note that the form of the second term is in the spirit of the Dudley entropy integral [1, 2, 3], where the Wasserstein distance corresponds to the radius of the covering ball and the square root of the divergence corresponds to square root of the metric entropy. We should also point out that the result in Corollary 5 does not require Lipschitz continuity of the loss function \(\ell(w,z)\) w.r.t. the hypothesis \(w\in\mathcal{W}\), except in a weaker stochastic sense as in (13), in contrast to some existing works that obtain generalization bounds using Wasserstein distances [27, 28].

## 7 Tail estimates

Next, we turn to high-probability tail estimates on \(\mathrm{gen}(W,S)\). We start with the following simple observation: Assume \(\ell(w,Z)\) is \(\sigma\)-subgaussian for all \(w\in\mathcal{W}\) when \(Z\sim P_{Z}\). Then, for any \(Q_{W}\in\mathcal{P}(\mathcal{W})\) such that \(P_{W|S=s}\ll Q_{W}\) for all \(s\in\mathcal{Z}^{n}\), we have

\[\mathbf{E}\Bigg{[}\exp\Bigg{(}\frac{\mathrm{gen}^{2}(W,S)}{6\sigma^{2}/n}-\log \Big{(}1+\frac{\mathrm{d}P_{W|S}}{\mathrm{d}Q_{W}}(W)\Big{)}\Bigg{)}\Bigg{]} \leq\mathbf{E}\Bigg{[}\exp\Bigg{(}\frac{\mathrm{gen}^{2}(\bar{W},S)}{6\sigma^{ 2}/n}\Bigg{)}\Bigg{]}\leq 1\]

with \(\bar{W}\sim Q_{W}\) independent of \((S,W)\). Thus, by Markov's inequality, for any \(0<\delta<1\),

\[\mathbf{P}\Bigg{[}|\mathrm{gen}(W,S)|>\sigma\sqrt{\frac{6}{n}}\Bigg{(}\psi_{2 }^{-1}\Big{(}\frac{\mathrm{d}P_{W|S}}{\mathrm{d}Q_{W}}(W)\Big{)}+\sqrt{\log \frac{1}{\delta}}\Bigg{)}\Bigg{]}\leq\delta.\]

In other words, \(|\mathrm{gen}(W,S)|\lesssim\frac{\sigma}{\sqrt{n}}\psi_{2}^{-1}\big{(}\frac{ \mathrm{d}P_{W|S}}{\mathrm{d}Q_{W}}(W)\big{)}\) with high \(P_{SW}\)-probability. Similar observations are made by Hellstrom and Durisi [9] with \(Q_{W}=P_{W}\), giving high-probability bounds of the form \(|\mathrm{gen}(W,S)|\lesssim\sqrt{\frac{\sigma^{2}D(P_{W|S}||P_{W})}{n}}\). Generalization bounds in terms of the divergence \(D(P_{W|S}||P_{W})\) are also common in the PAC-Bayes literature [4, 5]. Moreover, using the inequality (5) in Lemma 1, we can give high \(P_{S}\)-probability bounds on the conditional expectation

\[\langle P_{W|S},|\mathrm{gen}(W,S)|\rangle=\langle P_{W|S},|L(W)-L_{n}(W)|\rangle.\]

**Theorem 6**.: _Assume \(\ell(w,Z)\) is \(\sigma\)-subgaussian for all \(w\) when \(Z\sim P_{Z}\). Then, for any \(Q_{W}\in\mathcal{P}(\mathcal{W})\), the following holds with \(P_{S}\)-probability of at least \(1-\delta\):_

\[\langle P_{W|S},|\mathrm{gen}(W,S)|\rangle\leq\sqrt{\frac{24\sigma^{2}}{n}} \Big{(}\Big{\langle}P_{W|S},\psi_{2}^{-1}\Big{(}\frac{\mathrm{d}P_{W|S}}{ \mathrm{d}Q_{W}}\Big{)}\Big{\rangle}+1+\sqrt{\log\frac{2}{\delta}}\Big{)}.\]

Another type of result that appears frequently in the literature on PAC-Bayes methods pertains to so-called _transductive bounds_, i.e., inequalities for the difference between

\[\langle P_{n}^{\prime}\otimes P_{W|S},\ell\rangle-\langle P_{n}^{\prime} \otimes Q_{W},\ell\rangle\equiv\frac{1}{n}\sum_{i=1}^{n}\mathbf{E}[\ell(W,Z_{ i}^{\prime})-\ell(\bar{W},Z_{i}^{\prime})|\tilde{S}],\]

and

\[\langle P_{n}\otimes P_{W|S},\ell\rangle-\langle P_{n}\otimes Q_{W},\ell\rangle \equiv\frac{1}{n}\sum_{i=1}^{n}\mathbf{E}[\ell(W,Z_{i})-\ell(\bar{W},Z_{i})| \tilde{S}],\]

where \(Q_{W}\) is some fixed "prior" and where \(\bar{W}\sim Q_{W}\) is independent of \((S^{\prime},S,W)\). Using our techniques, we can give the following general transductive bound:

**Theorem 7**.: _Let \(P_{W|S}\) and \(Q_{W}\) be given and take any \((P_{W_{k}W_{k-1}|S})_{k=1}^{K}\) and \((\rho_{W_{k}W_{k-1}})_{k=1}^{K}\) as in Theorem 4. Also, let \(\bm{p}=(p_{1},p_{2},\ldots)\) be a strictly positive probability distribution on \(\mathbb{N}\). Then the following holds with \(P_{S}\)-probability at least \(1-\delta\):_

\[\Big{(}\langle P_{n}^{\prime}\otimes P_{W|S},\ell\rangle-\langle P _{n}^{\prime}\otimes Q_{W},\ell\rangle\Big{)}-\Big{(}\langle P_{n}\otimes P_{W |S},\ell\rangle-\langle P_{n}\otimes Q_{W},\ell\rangle\Big{)}\] \[\leq\sqrt{\frac{96}{n}}\sum_{k=1}^{K}\Bigg{(}\sqrt{\langle\rho_{ W_{k}W_{k-1}},d_{\tilde{S},\ell}^{2}\rangle}+\Big{\langle}P_{W_{k}W_{k-1}|S},d_{ \tilde{S},\ell}\psi_{2}^{-1}\Big{(}\frac{\mathrm{d}P_{W_{k}W_{k-1}|S}}{ \mathrm{d}\rho_{W_{k}W_{k-1}}}\Big{)}\Big{\rangle}\] \[\qquad\qquad\qquad\qquad+\big{\langle}P_{W_{k}W_{k-1}|S},d_{ \tilde{S},\ell}\big{\rangle}\sqrt{\log\frac{2}{p_{k}\delta}}\Bigg{)},\]

_where_

\[d_{\tilde{S},\ell}^{2}(u,v):=\frac{1}{2n}\sum_{i=1}^{n}\Big{(}\big{(}\ell(u,Z_ {i})-\ell(v,Z_{i})\big{)}^{2}+\big{(}\ell(u,Z_{i}^{\prime})-\ell(v,Z_{i}^{ \prime})\big{)}^{2}\Big{)}.\]

This result subsumes some existing transductive PAC-Bayes estimates, such as Theorem 2 of Audibert and Bousquet [17]. Let us briefly explain how we can recover this result from Theorem 7. Assume that \(\mathcal{W}\) is countable and let \((\mathcal{A}_{k})\) be an increasing sequence of finite partitions of \(\mathcal{W}\) with \(\mathcal{A}_{0}=\{\mathcal{W}\}\). For each \(k\) and each \(w\in\mathcal{W}\), let \(A_{k}(w)\) be the unique set in \(\mathcal{A}_{k}\) containing \(w\). Choose a representative point in each \(A\in\mathcal{A}_{k}\) and let \(\mathcal{W}_{k}\) denote the set of all such representatives, with \(\mathcal{W}_{0}=\{w_{0}\}\). Take \(P_{W_{\infty}|S}=P_{W|S}\) and \(P_{W_{0}}=Q_{W}=\delta_{w_{0}}\). Now, for each \(k\geq 0\), we take \(P_{W_{k}|S}\) as the _projection_ of \(P_{W|S}\) onto \(\mathcal{W}_{k}\), i.e., the finite mixture

\[P_{W_{k}|S}:=\sum_{w\in\mathcal{W}_{k}}P_{W|S}(A_{k}(w))\delta_{w}.\]

Moreover, given some "prior" \(\pi\in\mathcal{P}(\mathcal{W}),\) we can construct a sequence \((\pi_{k})_{k=0}^{\infty}\) of probability measures with \(\pi_{\infty}=\pi\) and \(\pi_{0}=\delta_{w_{0}}\), such that \(\pi_{k}\) is a projection of \(\pi\) onto \(\mathcal{W}_{k}\). Now observe that, for each \(k\), \(S-W_{\infty}-W_{k}-W_{k-1}\) is a Markov chain. Indeed, if we know \(P_{W_{k}|S}\), then we can construct \(P_{W_{k}|S}\) for any \(\ell<k\) without knowledge of \(S\). With these ingredients in place, let us choose \(P_{W_{k}W_{k-1}|S}=P_{W_{k-1}|W_{k}}\otimes P_{W_{k}|S}\) and \(\rho_{W_{k}W_{k-1}}=\pi_{k}\otimes P_{W_{k-1}|W_{k}}\). Then, using Cauchy-Schwarz and Jensen, we conclude that the following holds with \(P_{S}\)-probability at least \(1-\delta\):

\[\Big{(}\langle P_{n}^{\prime}\otimes P_{W|S},\ell\rangle-\langle P _{n}^{\prime}\otimes\delta_{w_{0}},\ell\rangle\Big{)}-\Big{(}\langle P_{n} \otimes P_{W|S},\ell\rangle-\langle P_{n}\otimes\delta_{w_{0}},\ell\rangle\Big{)}\] \[\leq\sqrt{\frac{96}{n}}\sum_{k=1}^{\infty}\Bigg{(}\sqrt{\langle\pi _{k}\otimes P_{W_{k-1}|W_{k}},d_{\tilde{S},\ell}^{2}\rangle}\] \[\qquad\qquad+\sqrt{2\langle P_{W_{k}|S}\otimes P_{W_{k-1}|W_{k}},d_ {\tilde{S},\ell}^{2}\rangle\Big{(}D(P_{W_{k}|S}\|\pi_{k})+\log\frac{2e}{p_{k} \delta}\Big{)}}\Bigg{)}.\]

This recovers [17, Thm. 2] up to an extra term that scales like \(\frac{1}{\sqrt{n}}\sum_{k}\sqrt{\langle\pi_{k}\otimes P_{W_{k-1}|W_{k}},d_{ \tilde{S},\ell}^{2}\rangle}\).

The Fernique-Talagrand bound

As a bonus, we show that a combination of decorrelation and chaining in the space of measures can be used to recover the upper bounds of Fernique [29] and Talagrand [30] on the expected supremum of a stochastic process in terms of majorizing measures (see Eq. (19) below and also [31, 32]).

For simplicity, let \((T,d)\) be a finite metric space with \(\mathrm{diam}(T)=\sup\{d(u,v):u,v\in T\}<\infty\). Let \(B(t,r)\) denote the ball of radius \(r\geq 0\) centered at \(t\in T\), i.e., \(B(t,r):=\{u\in T:d(u,t)\leq r\}\). Let \((X_{t})_{t\in T}\) be a centered stochastic process defined on some probability space \((\Omega,\mathcal{A},\mathbb{P})\) and satisfying

\[\mathbf{E}\!\left[\psi_{p}\!\left(\frac{|X_{u}-X_{v}|}{d(u,v)}\right)\right] \leq 1,\qquad\forall u,v\in T\] (18)

for some \(p\geq 1\). Then we can obtain the following result using chaining in the space of measures and decorrelation estimates:

**Theorem 8**.: _Let \(\tau\) be a random element of \(T\), i.e., a measurable map \(\tau:\Omega\to T\) with marginal probability law \(\nu\). Then for any \(\mu\in\mathcal{P}(T)\) we have_

\[\mathbf{E}\!\left[X_{\tau}\right]\lesssim\mathrm{diam}(T)+\int_{T}\int_{0}^{ \mathrm{diam}(T)}\left(\log\frac{1}{\mu(B(t,\varepsilon))}\right)^{1/p} \mathrm{d}\varepsilon\,\nu(\mathrm{d}t).\]

Applying Theorem 8 to \(\tau^{*}\) defined in (1) and then minimizing over \(\mu\), we recover a Fernique-Talagrand type bound on the expected supremum of \(X_{t}\):

\[\mathbf{E}\!\left[\sup_{t\in T}X_{t}\right]=\mathbf{E}\!\left[X_{\tau^{*}} \right]\lesssim\mathrm{diam}(T)+\inf_{\mu\in\mathcal{P}(T)}\sup_{t\in T}\int_ {0}^{\mathrm{diam}(T)}\left(\log\frac{1}{\mu(B(t,\varepsilon))}\right)^{1/p} \mathrm{d}\varepsilon.\] (19)

## 9 Conclusion and future work

In this paper, we have presented a unified framework for information-theoretic generalization bounds based on a combination of two key ideas (decorrelation and chaining in the space of measures). However, our method has certain limitations, which we plan to address in future work. For example, it would be desirable to cover the case of processes satisfying Bernstein-type (mixed \(\psi_{1}\) and \(\psi_{2}\)) increment conditions. It would also be of interest to see whether there are any connections to the convex-analytic approach of Lugosi and Neu [33]. Finally, since our method seamlessly interpolates between Fernique-Talagrand type bounds and information-theoretic bounds, we plan to use it to further develop the ideas of Hodgkinson et al. [21], who were the first to combine these two complementary approaches to analyze the generalization capabilities of iterative learning algorithms.

## Acknowledgments

This work was supported by the Illinois Institute for Data Science and Dynamical Systems (iDS\({}^{2}\)), an NSF HDR TRIPODS institute, under award CCF-1934986. The authors would like to thank Matus Telgarsky for some valuable suggestions and the anonymous reviewers for pointing out some relevant work that was not cited in the original submission.

## References

* [1] Aad W. van der Vaart and Jon A. Wellner. _Weak Convergence and Empirical Processes_. Springer, 1996.
* [2] Evarist Gine and Richard Nickl. _Mathematical Foundations of Infinite-Dimensional Statistical Models_. Cambridge University Press, 2016.
* [3] Michel Talagrand. _Upper and Lower Bounds for Stochastic Processes: Modern Methods and Classical Problems_. Springer, 2014.
* [4] David A. McAllester. PAC-Bayesian model averaging. In _Conference on Learning Theory_, 1999.
* [5] Tong Zhang. Information-theoretic upper and lower bounds for statistical estimation. _IEEE Transactions on Information Theory_, 52(4):1307-1321, April 2006.
* [6] Daniel Russo and James Zou. Controlling Bias in Adaptive Data Analysis Using Information Theory. In _Conference on Artificial Intelligence and Statistics_, 2016.
* [7] Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learning algorithms. In _Advances in Neural Information Processing Systems_, 2017.
* [8] Yuheng Bu, Shaofeng Zou, and Venugopal V. Veeravalli. Tightening Mutual Information Based Bounds on Generalization Error. _IEEE Journal on Selected Areas in Information Theory_, 1(1):121-130, May 2020.
* [9] Fredrik Hellstrom and Giuseppe Durisi. Generalization Bounds via Information Density and Conditional Information Density. _IEEE Journal on Selected Areas in Information Theory_, 1(3):824-839, November 2020.
* [10] Thomas Steinke and Lydia Zakynthinou. Reasoning About Generalization via Conditional Mutual Information. arXiv:2001.09122, June 2020.
* [11] Hassan Hafez-Kolahi, Zeinab Gologoni, Shohreh Kasaei, and Mahdieh Soleymani. Conditioning and processing: Techniques to improve information-theoretic generalization bounds. In _Advances in Neural Information Processing Systems_, 2020.
* [12] Mahdi Haghifam, Jeffrey Negrea, Ashish Khisti, Daniel M Roy, and Gintare Karolina Dziugaite. Sharpened generalization bounds based on conditional mutual information and an application to noisy, iterative algorithms. In _Advances in Neural Information Processing Systems_, 2020.
* [13] Amedeo Roberto Esposito, Michael Gastpar, and Ibrahim Issa. Generalization error bounds via Renyi-, \(f\)-divergences and maximal leakage. _IEEE Transactions on Information Theory_, 67(8):4986-5004, August 2021.
* [14] Mahdi Haghifam, Gintare Karolina Dziugaite, Shay Moran, and Dan Roy. Towards a unified information-theoretic framework for generalization. In _Advances in Neural Information Processing Systems_, 2021.
* [15] Hrayr Harutyunyan, Maxim Raginsky, Greg Ver Steeg, and Aram Galstyan. Information-theoretic generalization bounds for black-box learning algorithms. In _Advances in Neural Information Processing Systems_, 2021.
* [16] Ruida Zhou, Chao Tian, and Tie Liu. Individually conditional individual mutual information bound on generalization error. _IEEE Transactions on Information Theory_, 68(5):3304-3316, May 2022.
* [17] Jean-Yves Audibert and Olivier Bousquet. Combining PAC-Bayesian and Generic Chaining Bounds. _Journal of Machine Learning Research_, 8(32):863-889, 2007.
* [18] Amir Asadi, Emmanuel Abbe, and Sergio Verdu. Chaining Mutual Information and Tightening Generalization Bounds. In _Advances in Neural Information Processing Systems_, 2018.
* [19] Eugenio Clerico, Amitis Shidani, George Deligiannidis, and Arnaud Doucet. Chained generalisation bounds. In _Conference on Learning Theory_, 2022.
* [20] Ruida Zhou, Chao Tian, and Tie Liu. Stochastic Chaining and Strengthened Information-Theoretic Generalization Bounds. In _IEEE International Symposium on Information Theory_, January 2022.

* [21] Liam Hodgkinson, Umut Simsekli, Rajiv Khanna, and Michael W. Mahoney. Generalization Bounds using Lower Tail Exponents in Stochastic Optimizers. In _International Conference on Machine Learning_, July 2022.
* [22] Luigi Ambrosio, Nicola Gigli, and Giuseppe Savare. _Gradient Flows in Metric Spaces and in the Space of Probability Measures_. Birkhauser, 2008.
* [23] Cedric Villani. _Topics in Optimal Transportation_. American Mathematics Society, Providence, RI, 2003.
* [24] Stephane Boucheron, Gabor Lugosi, and Pascal Massart. _Concentration Inequalities: A Nonasymptotic Theory of Independence_. Oxford University Press, 2013.
* [25] Roman Vershynin. _High-Dimensional Probability_. Cambridge University Press, 2018.
* [26] Yury Polyanskiy and Yihong Wu. _Information Theory: From Coding to Learning_. Cambridge University Press, forthcoming. URL: https://people.lids.mit.edu/yp/homepage/data/itbook-export.pdf.
* [27] Hao Wang, Mario Diaz, Jose Candido S. Santos Filho, and Flavio P. Calmon. An information-theoretic view of generalization via Wasserstein distance'. In _IEEE International Symposium on Information Theory_, 2019.
* [28] Borja Rodrigues-Galvez, German Bassi, Ragnar Thobaben, and Mikael Skoglund. Tighter expected generalization error bounds via Wasserstein distance. In _Advances in Neural Information Processing Systems_, 2021.
* [29] Xavier Fernique. Regularite des trajectoires des fonctions aleatoires Gaussiennes. In X. M. Fernique, J. P. Conze, J. Gani, and P. L. Hennequin, editors, _Ecole d'Ete de Probabilites de Saint-Flour IV--1974_, Lecture Notes in Mathematics, pages 1-96. Springer, 1975.
* [30] Michel Talagrand. Regularity of gaussian processes. _Acta Mathematica_, 159:99-149, 1987.
* [31] Witold Bednorz. A theorem on majorizing measures. _The Annals of Probability_, 34(5), September 2006.
* [32] Witold Bednorz. The majorizing measure approach to sample boundedness. _Colloquium Mathematicae_, 139(2):205-227, 2015.
* [33] Gabor Lugosi and Gergely Neu. Generalization bounds via convex analysis. In _Conference on Learning Theory_, 2022.

Some elementary facts

We first list some useful inequalities for \(\psi_{p}\) and \(\psi_{p}^{-1}\). Note that the estimates may not be the sharpest, but they suffice for our needs.

**Proposition A.2**.: _For \(p\geq 1\) and \(x\geq 0\), let \(\psi_{p}(x)=\exp(x^{p})-1\) and let \(\psi_{p}^{-1}(x)=(\log(x+1))^{1/p}\) be its inverse. Then we have the following:_

1. \(\psi_{p}^{2}(x/2^{1/p})\leq\psi_{p}(x)\)_._
2. \(x\psi_{p}(x/4^{1/p})\leq 2^{1/p}\psi_{p}(x/2^{1/p})\)_._
3. _for_ \(x\geq 0\) _and_ \(q\geq 1\)_,_ \(\psi_{p}^{-1}(x^{q})\leq q^{1/p}\psi_{p}^{-1}(x)\)_._
4. _For_ \(x\geq 1\)_,_ \(\psi_{p}^{-1}(x)\leq(\log(x))^{1/p}+1\)_._

Proof.:
1. For any \(x\geq 0\), \[\psi_{p}(x) =\exp(x^{p})-1=(\exp(x^{p}/2)-1)(\exp(x^{p}/2)+1)\geq(\exp(x^{p}/2 )-1)^{2}\] \[=\psi_{p}^{2}(x/2^{1/p}).\]
2. We only need to consider the case \(x\geq 1\) since otherwise the inequality is obvious. Since \(y\leq 2(\exp(y/4)+1)\) for all \(y\geq 1\), we have \[x\leq 2^{1/p}(\exp(x^{p}/4)+1)^{1/p}\leq 2^{1/p}(\exp(x^{p}/4p)+1)\leq 2^{1/p}( \exp(x^{p}/4)+1).\] Then \[x\psi_{p}(x/4^{1/p}) =x(\exp(x^{p}/4)-1)\] \[\leq 2^{1/p}(\exp(x^{p}/4)+1)(\exp(x^{p}/4)-1)\] \[=2^{1/p}(\exp(x^{p}/2)-1)\] \[=2^{1/p}\psi_{p}(x/2^{1/p}).\]
3. Since \(x\geq 0\) and \(q\geq 1\), \[\psi_{p}^{-1}(x^{q})=(\log(1+x^{q}))^{1/p}\leq(\log(1+x)^{q})^{1/p}=q^{1/p}\psi _{p}^{-1}(x).\]
4. When \(x\geq 1\), \[\operatorname{e}\!x\geq x+1\implies\log x+1\geq\log(x+1)\implies\log^{1/p}(x) +1\geq\psi_{p}^{-1}(x).\]

The following simple result is for converting between sums and integrals:

**Proposition A.3**.: _For any \(r\geq 2\), \(K\in\mathbb{N}\), and a continuous nonincreasing \(f:(0,+\infty)\to(0,+\infty)\), we have_

\[\sum_{k=1}^{K}r^{-k}f(r^{-k})\leq r\int_{0}^{1}f(\varepsilon)\,\mathrm{d} \varepsilon\leq r^{2}\sum_{k=0}^{\infty}r^{-k}f(r^{-k})\] (A.1)

Proof.: Using the monotonicity of \(f\), we have

\[\sum_{k=1}^{K}r^{-k}f(r^{-k}) \leq\sum_{k=1}^{K}r^{-k}(r-1)f(r^{-k})\leq r\sum_{k=1}^{K}\int_{ r^{-k-1}}^{r^{-k}}f(\varepsilon)\,\mathrm{d}\varepsilon\] \[\leq r\int_{0}^{1}f(\varepsilon)\mathrm{d}\varepsilon \leq r\sum_{k=0}^{\infty}\int_{r^{-k}}^{r^{-k+1}}f(\epsilon)\, \mathrm{d}\varepsilon\leq r^{2}\sum_{k=0}^{\infty}r^{-k}f(r^{-k}).\]Omitted proofs

### Proofs for Section 2

Proof of Proposition 1.It follows from the inequality \(x\log(x+1)\leq x\log x+1\), \(x\geq 0\), that

\[\frac{\mathrm{d}\mu}{\mathrm{d}\nu}\log\left(\frac{\mathrm{d}\mu}{\mathrm{d}\nu} +1\right)\leq\frac{\mathrm{d}\mu}{\mathrm{d}\nu}\log\frac{\mathrm{d}\mu}{ \mathrm{d}\nu}+1.\]

Using this and Jensen's inequality, we get

\[\left\langle\mu,\psi_{p}^{-1}\Big{(}\frac{\mathrm{d}\mu}{ \mathrm{d}\nu}\Big{)}\right\rangle =\left\langle\mu,\Big{(}\log\Big{(}\frac{\mathrm{d}\mu}{\mathrm{d} \nu}+1\Big{)}\Big{)}^{1/p}\right\rangle\] \[=\Big{(}\Big{\langle}\nu,\frac{\mathrm{d}\mu}{\mathrm{d}\nu}\log \Big{(}\frac{\mathrm{d}\mu}{\mathrm{d}\nu}+1\Big{)}\Big{)}\Big{\rangle}^{1/p}\] \[\leq\Big{(}\Big{\langle}\nu,\frac{\mathrm{d}\mu}{\mathrm{d}\nu} \log\frac{\mathrm{d}\mu}{\mathrm{d}\nu}\Big{\rangle}+1\Big{)}^{1/p}\] \[=\big{(}D(\mu\|\nu)+1\big{)}^{1/p}.\]

### Proofs for Section 3

Proof of Lemma 1.To prove (4), we start with the Young-type inequality

\[xy\leq\psi_{p}^{*}(x)+\psi_{p}(y),\qquad x,y\geq 0\]

where

\[\psi_{p}^{*}(x):=\sup_{y\geq 0}\big{(}xy-\psi_{p}(y)\big{)}\]

is the (one-sided) Legendre-Fenchel conjugate of \(\psi_{p}\). While a closed-form expression for \(\psi_{p}^{*}\) is not available, we claim that we can bound it from above as \(\psi_{p}^{*}(x)\leq 2^{1/p}x\psi_{p}^{-1}(x)\), resulting in

\[xy\leq 2^{1/p}x\psi_{p}^{-1}(x)+\psi_{p}(y).\] (B.1)

To establish the claim, we write

\[\sup_{y\geq 0}\big{(}xy-\psi_{p}(y)\big{)}=\sup_{y\geq 0}\big{(}xy-(e^{y^{p}/2}- 1)(e^{y^{p}/2}+1)\big{)}\]

and consider two cases:

* if \(y\leq 2^{1/p}\psi_{p}^{-1}(x)\), then \[xy-(e^{y^{p}/2}-1)(e^{y^{p}/2}+1)\leq 2^{1/p}x\psi_{p}^{-1}(x).\]
* if \(y>2^{1/p}\psi_{p}^{-1}(x)\), then \[xy-(e^{y^{p}/2}-1)(e^{y^{p}/2}+1)\leq(e^{y^{p}/2}-1)(y-(e^{y^{p}/2}+1))\leq 0.\]

Applying (B.1) with \(x=\frac{\mathrm{d}\mu}{\mathrm{d}\nu}\) and \(y=g\) gives

\[g\frac{\mathrm{d}\mu}{\mathrm{d}\nu}\leq 2^{1/p}\frac{\mathrm{d}\mu}{\mathrm{d} \nu}\psi_{p}^{-1}\Big{(}\frac{\mathrm{d}\mu}{\mathrm{d}\nu}\Big{)}+\psi_{p}(g),\]

so that

\[\left\langle\mu,fg\right\rangle =\left\langle\nu,fg\frac{\mathrm{d}\mu}{\mathrm{d}\nu}\right\rangle\] \[=2^{1/p}\Big{\langle}\mu,f\psi_{p}^{-1}\Big{(}\frac{\mathrm{d} \mu}{\mathrm{d}\nu}\Big{)}\Big{\rangle}+\left\langle\nu,f\psi_{p}(g)\right\rangle.\]To prove (5), define the event

\[E:=\Bigg{\{}\frac{\mathrm{d}\mu}{\mathrm{d}\nu}\geq\frac{\exp(g^{p}/4)-1}{\langle \nu,\exp(g^{p})\rangle}\Bigg{\}}.\]

Then, since \(\langle\nu,\exp(g^{p})\rangle\geq 1\),

\[\int_{E}fg\,\mathrm{d}\mu \leq 4^{1/p}\int f\Big{(}\log\Big{(}\frac{\mathrm{d}\mu}{\mathrm{d} \nu}\langle\nu,\exp(g^{p})\rangle+1\Big{)}\Big{)}^{1/p}\,\mathrm{d}\mu\] \[\leq 4^{1/p}\int f\Big{(}\log\Big{(}\frac{\mathrm{d}\mu}{\mathrm{d }\nu}+1\Big{)}\Big{)}^{1/p}\,\mathrm{d}\mu+4^{1/p}\int f\,\mathrm{d}\mu\cdot \Big{(}\log\langle\nu,\exp(g^{p})\rangle\Big{)}^{1/p}\] \[=4^{1/p}\Big{\langle}\mu,f\psi_{p}^{-1}\Big{(}\frac{\mathrm{d} \mu}{\mathrm{d}\nu}\Big{)}\Big{\rangle}+4^{1/p}\|f\|_{L^{1}(\mu)}\big{(}\log \langle\nu,\exp(g^{p})\rangle\big{)}^{1/p}.\]

On the other hand,

\[\int_{E^{c}}fg\,\mathrm{d}\mu \leq\int fg\frac{\exp(g^{p}/4)-1}{\langle\nu,\exp(g^{p})\rangle} \,\mathrm{d}\nu\] \[\leq 2^{1/p}\int f\frac{\exp(g^{p}/2)}{\langle\nu,\exp(g^{p}) \rangle}\,\mathrm{d}\nu\] \[\leq 2^{1/p}\|f\|_{L^{2}(\nu)},\]

where the first inequality is by the definition of \(E\), the second inequality follows from Proposition A.2(ii), and the third inequality is by Cauchy-Schwarz. Putting everything together, we get (5).

### Proofs for Section 4

Proof of Theorem 1.It follows from the independence of \(Z_{1},\ldots,Z_{n}\) that \(\mathrm{gen}(w,S)\) is \((\sigma/\sqrt{n})\)-subgaussian, so

\[\mathbf{E}\Bigg{[}\psi_{2}\Bigg{(}\frac{|\mathrm{gen}(w,S)|}{ \sigma\sqrt{6/n}}\Bigg{)}\Bigg{]}\leq 1,\qquad\forall w\in\mathcal{W}.\] (B.2)

Using Lemma 1 with \(\mu=P_{W|S}\), \(\nu=Q_{W}\), \(f(w)=\sigma\sqrt{6/n}\), and \(g(w)=\frac{|\mathrm{gen}(w,S)|}{\sigma\sqrt{6/n}}\), we have

\[\langle P_{W|S},|\mathrm{gen}(\cdot,S)|\rangle\]

Taking expectations of both sides w.r.t. \(P_{S}\) and using Fubini's theorem and (B.2), we get (6).

Proof of Corollary 1.Applying Proposition 1 conditionally on \(S\) gives

\[\Big{\langle}P_{W|S},\psi_{2}^{-1}\Big{(}\frac{\mathrm{d}P_{W|S} }{\mathrm{d}Q_{W}}\Big{)}\Big{\rangle}\leq\sqrt{D(P_{W|S}\|Q_{W})+1},\]

where the divergence \(D(P_{W|S}\|Q_{W})\), being a function of \(S\), is a random variable. Substituting this into (6) and using Jensen's inequality, the definition of conditional divergence, and \(\sqrt{a+b}\leq\sqrt{a}+\sqrt{b}\leq\sqrt{2(a+b)}\), we get

\[\mathbf{E}[\|\mathrm{gen}(W,S)|]\leq\sqrt{\frac{24\sigma^{2}}{n} \left(D(P_{W|S}\|Q_{W}|P_{S})+4\right)}.\]

Taking the infimum of both sides w.r.t. \(Q_{W}\) and using (2), we get (7).

Proof of Theorem 2.For each fixed \((w,\tilde{s})\), the random variable \(\delta(w,\tilde{s},\varepsilon):=|\sum_{i=1}^{n}\varepsilon_{i}(\ell(w,z_{i}^{ \prime})-\ell(w,z_{i}))|\) is \(\sigma(w,\tilde{s})\)-subgaussian, where \(\sigma(w,\tilde{s}):=\big{(}\sum_{i=1}^{n}(\ell(w,z_{i}^{\prime})-\ell(w,z_{i} ))^{2}\big{)}^{1/2}\). Thus,

\[\mathbf{E}_{\varepsilon}[\zeta(w,\tilde{s},\varepsilon)]:=\mathbf{E}_{ \varepsilon}\Bigg{[}\psi_{2}\Bigg{(}\frac{\delta(w,\tilde{s},\varepsilon)}{ \sqrt{6}\sigma(w,\tilde{s})}\Bigg{)}\Bigg{]}\leq 1,\qquad\forall(w,\tilde{s}).\] (B.3)

Applying Lemma 1 conditionally on \((\tilde{S},\varepsilon)\) with \(\mu=\bar{P}_{W|\tilde{S}\varepsilon}\), \(\nu=Q_{W|\tilde{S}}\), \(f(w)=\sigma(w,\tilde{S})\), \(g(w)=\zeta(w,\tilde{S},\varepsilon)\), we obtain

\[\big{\langle}\bar{P}_{W|\tilde{S}\varepsilon},\sigma(\cdot, \tilde{S})\zeta(\cdot,\tilde{S},\varepsilon)\big{\rangle}\] \[\leq\sqrt{2}\Bigg{\langle}\bar{P}_{W|\tilde{S}\varepsilon},\sigma (\cdot,\tilde{S})\psi_{2}^{-1}\Bigg{(}\frac{\mathrm{d}\bar{P}_{W|\tilde{S}, \varepsilon}}{\mathrm{d}Q_{W|\tilde{S}}}\Bigg{)}\Bigg{\rangle}+\Big{\langle}Q_ {W|\tilde{S}},\sigma(\cdot,\tilde{S})\psi_{2}(\zeta(\cdot,\tilde{S}, \varepsilon))\Big{\rangle}\] \[\leq\sqrt{2}\|\Delta(\tilde{S})\|_{\ell^{2}}\left(\Bigg{\langle} \bar{P}_{W|\tilde{S}\varepsilon},\psi_{2}^{-1}\Bigg{(}\frac{\mathrm{d}\bar{P}_ {W|\tilde{S},\varepsilon}}{\mathrm{d}Q_{W|\tilde{S}}}\Bigg{)}\right)+\Big{\langle} Q_{W|\tilde{S}},\psi_{2}(\zeta(\cdot,\tilde{S},\varepsilon))\Big{\rangle}\Bigg{\rangle}.\]

Taking expectations of both sides w.r.t. \(\tilde{S}\) and \(\varepsilon\), then using Fubini's theorem, (B.3), and the inequality \(\mathbf{E}_{P}[|\mathrm{gen}(W,S)|]\leq\frac{1}{n}\mathbf{E}_{P}[\delta(W, \tilde{S},\varepsilon)]\), we obtain (8).

Proof of Corollary 2.For any \(Q_{W|\tilde{S}}\), using Proposition 1, Cauchy-Schwarz, and the independence of \((Z_{i}^{\prime},Z_{i})\), we have

\[\mathbf{E}_{\bar{P}}\Bigg{[}\|\Delta(\tilde{S})\|_{\ell^{2}}\psi _{2}^{-1}\Bigg{(}\frac{\mathrm{d}\bar{P}_{W|\tilde{S},\varepsilon}}{\mathrm{d }Q_{W|\tilde{S}}}\Bigg{)}\Bigg{]}\] \[\leq\sqrt{\mathbf{E}_{\bar{P}}[\|\Delta(\tilde{S})\|_{\ell^{2}}^ {2}]\big{(}D(\bar{P}_{W|\tilde{S}\varepsilon}\|Q_{W|\tilde{S}}|\bar{P}_{ \tilde{S}\varepsilon})+1\big{)}}\] \[=\sqrt{n\mathbf{E}[\Delta(Z,Z^{\prime})^{2}]\big{(}D(\bar{P}_{W| \tilde{S}\varepsilon}\|Q_{W|\tilde{S}}|\bar{P}_{\tilde{S}\varepsilon})+1 \big{)}}.\]

Substituting this estimate into (8), taking the infimum of both sides w.r.t. \(Q_{W|\tilde{S}}\), and using (3), we get (9).

### Proofs for Section 5

Proof of Theorem 3.Let

\[\delta(u,v,z,z^{\prime}) :=\big{(}\ell(u,z^{\prime})-\ell(v,z^{\prime})\big{)}-\big{(}\ell (u,z)-\ell(v,z)\big{)},\] \[\delta(u,v,\tilde{s}) :=\sum_{i=1}^{n}\delta(u,v,z_{i},z_{i}^{\prime}),\] \[\zeta(u,v,\tilde{s}) :=\frac{|\delta(u,v,\tilde{s})|}{\sqrt{6}\sigma(u,v,\tilde{s})}.\]

For each fixed \((u,v)\in\mathcal{W}^{2}\), \(\delta(u,v,Z_{i},Z_{i}^{\prime})\), \(1\leq i\leq n\), are i.i.d. symmetric random variables. Therefore, introducing a tuple \(\varepsilon=(\varepsilon_{1},\ldots,\varepsilon_{n})\) of i.i.d. Rademacher random variables independent of everything else and using the fact that the joint distributions of \(\big{(}\delta(u,v,Z_{i},Z_{i}^{\prime})\big{)}_{i=1}^{n}\) and \(\big{(}\varepsilon_{i}\delta(u,v,Z_{i},Z_{i}^{\prime})\big{)}_{i=1}^{n}\) are the same, we see that

\[\mathbf{E}[\psi_{2}(\zeta(u,v,\tilde{S}))]=\mathbf{E}_{\tilde{S}}\mathbf{E}_{ \varepsilon}\Bigg{[}\psi_{2}\Bigg{(}\frac{|\sum_{i=1}^{n}\varepsilon_{i} \delta(u,v,Z_{i},Z_{i}^{\prime})|}{\sqrt{6}\sigma(u,v,\tilde{S})}\Bigg{)} \Bigg{]}\leq 1,\]

where the inequality follows from the fact that, conditionally on \(S\) and \(S^{\prime}\), the random variables \(\sum_{i=1}^{n}\varepsilon_{i}\delta(u,v,Z_{i},Z_{i}^{\prime})\) are \(\sigma(u,v,\tilde{S})\)-subgaussian.

Now, given \(Q_{W}\in\mathcal{P}(\mathcal{W})\) and a family of couplings \(P_{UV|S=s}\in\Pi(P_{W|S=s},Q_{W})\), it follows from the above definitions and from (10) that

\[\mathbf{E}[\mathrm{gen}(W,S)]\leq\frac{1}{n}\mathbf{E}[|\delta(U,V,\tilde{S}) |]=\frac{\sqrt{6}}{n}\mathbf{E}[\sigma(U,V,\tilde{S})\zeta(U,V,\tilde{S})].\] (B.4)Picking any \(\rho_{UV}\in\mathcal{P}(\mathcal{W}\times\mathcal{W})\) such that \(P_{UV|S=s}\ll\rho_{UV}\) for all \(s\in\mathcal{Z}^{n}\) and applying Lemma 1, we get

\[\langle P_{UV|S},\sigma(\cdot,\bar{S})\zeta(\cdot,\bar{S})\rangle\] \[\qquad\leq 2\bigg{\langle}P_{UV|S},\sigma(\cdot,\bar{S})\psi_{2}^{ -1}\bigg{(}\frac{\mathrm{d}P_{UV|S}}{\mathrm{d}\rho_{UV}}\bigg{)}\bigg{\rangle} +\sqrt{2}\bigg{\langle}\rho_{UV},\sigma(\cdot,\bar{S})\psi_{2}\bigg{(}\frac{ \zeta(\cdot,\bar{S})}{\sqrt{2}}\bigg{)}\bigg{\rangle}.\]

Using the inequality \(\psi_{2}^{2}(x/\sqrt{2})\leq\psi_{2}(x)\) (see Proposition A.2(i)), Cauchy-Schwarz, and (B.4), we have

\[\mathbf{E}\bigg{[}\sigma(u,v,\tilde{S})\psi_{2}\bigg{(}\frac{\zeta(u,v,\tilde{ S})}{\sqrt{2}}\bigg{)}\bigg{]}\leq\sqrt{\mathbf{E}[\sigma^{2}(u,v,\tilde{S})]}, \qquad\forall(u,v)\in\mathcal{W}\times\mathcal{W}.\]

Putting everything together and taking expectations w.r.t. \(S\) and \(S^{\prime}\), we obtain (12).

Proof of Corollary 3.For \(\sigma\) defined in Theorem 3, we have

\[\sigma^{2}(u,v,\tilde{S})\leq 2\sum_{i=1}^{n}\Big{(}\big{(}\ell(u,Z_{i}^{ \prime})-\ell(v,Z_{i}^{\prime})\big{)}^{2}+\big{(}\ell(u,Z_{i})-\ell(v,Z_{i}) \big{)}^{2}\Big{)}.\]

Taking conditional expectations given \(U,V,S\) and using Jensen's inequality gives

\[\mathbf{E}[\sigma(U,V,\tilde{S})|U,V,S] \leq\sqrt{\mathbf{E}[\sigma^{2}(U,V,\tilde{S})|U,V,S]}\] \[\leq\sqrt{2n}\big{(}d_{\ell}(U,V)+d_{S,\ell}(U,V)\big{)}.\]

An analogous argument gives

\[\sqrt{\mathbf{E}[\sigma^{2}(\bar{U},\bar{V},\tilde{S})|\bar{U},\bar{V}]}\leq 2 \sqrt{n}d_{\ell}(\bar{U},\bar{V}).\]

Substituting these estimates into (12) gives the desired result.

### Proofs for Section 6

Proof of Theorem 5.Using the definition of \(\bar{\ell}\), we have

\[\mathbf{E}[\mathrm{gen}(W,S)]=\frac{1}{n}\sum_{k=1}^{K}\mathbf{E}\Bigg{[} \sum_{i=1}^{n}\big{(}\bar{\ell}(W_{k},Z_{i})-\bar{\ell}(W_{k-1},Z_{i})\big{)} \Bigg{]}.\]

Applying Lemma 1 conditionally on \(S\) with \(f(u,v)=d(u,v)\), \(g(u,v)=\frac{|\sum_{i=1}^{n}(\bar{\ell}(u,Z_{i})-\bar{\ell}(v,Z_{i}))|}{\sqrt{ n}d(u,v)}\), \(\mu=P_{W_{k}W_{k-1}|S}\) and \(\nu=\rho_{W_{k}W_{k-1}}\), taking expectations w.r.t. \(P_{S}\), and using (13) gives the desired result.

Proof of Corollary 4.For each \(k\geq 1\), let \(\rho_{W_{k}W_{k-1}}=P_{W_{k}W_{k-1}}\). Then

\[\frac{\mathrm{d}P_{W_{k}W_{k-1}|S}}{\mathrm{d}P_{W_{k}W_{k-1}}}=\frac{ \mathrm{d}P_{W_{k}W_{k-1}}S}{\mathrm{d}(P_{W_{k}W_{k-1}}\otimes P_{S})}=\frac {\mathrm{d}P_{S|W_{k}W_{k-1}}}{\mathrm{d}P_{S}}=\frac{\mathrm{d}P_{S|W_{k}}}{ \mathrm{d}P_{S}},\]

where we have made use of Bayes' rule and the fact that \(S\perp\!\!\!\perp W_{k-1}|W_{k}\). Using this in (14) together with Proposition 1 gives (15). An application of Cauchy-Schwarz and Jensen gives (16).

Proof of Corollary 5.For each \(k\geq 1\), let \(\rho_{W_{k}W_{k-1}}=P_{W_{k}W_{k-1}}\). Notice that, by disintegration and the choice of couplings,

\[\mathbf{E}[d(\bar{W}_{k},\bar{W}_{k-1})]=\mathbf{E}\Big{[}\int d(u,v)P_{W_{k}W _{k-1}|S}(\mathrm{d}u,\mathrm{d}v)\Big{]}\leq\mathbf{E}[\mathsf{W}_{2}(P_{W_{k }|S},P_{W_{k-1}|S})],\]

where we have used the fact that \(\mathsf{W}_{2}(\cdot,\cdot)\) dominates \(\mathsf{W}_{1}(\cdot,\cdot)\)[23]. Since \(P_{W_{k}|S}\) are points on the geodesic connecting \(P_{W|S}\) and \(P_{W}\), we have

\[\sum_{k=1}^{K}\mathsf{W}_{2}(P_{W_{k}|S},P_{W_{k-1}|S})=\sum_{k=1}^{K}(t_{k}-t_ {k-1})\mathsf{W}_{2}(P_{W|S},P_{W})=\mathsf{W}_{2}(P_{W|S},P_{W}).\]

Using this together with Cauchy-Schwarz and Proposition 1, we obtain (17).

### Proofs for Section 7

Proof of Theorem 6.Applying (5) conditionally on \(S\) with \(f(w)=\sigma\sqrt{6/n}\), \(g(w)=\frac{|\mathrm{gen}(w,S)|}{\sigma\sqrt{6/n}}\), \(\mu=P_{W|S}\), and \(\nu=Q_{W}\), we have

\[\langle P_{W|S},|\mathrm{gen}(W,S)|\rangle\] \[\leq\sqrt{\frac{24\sigma^{2}}{n}}\Bigg{(}1+\Bigg{\langle}P_{W|S},\psi_{2}^{-1}\Big{(}\frac{\mathrm{d}P_{W|S}}{\mathrm{d}Q_{W}}\Big{)}\Bigg{\rangle} +\Bigg{(}\log\Bigg{\langle}Q_{W},\exp\Big{(}\frac{\mathrm{gen}^{2}(\cdot,S)}{ 6\sigma^{2}/n}\Big{)}\Bigg{\rangle}\Bigg{)}^{1/2}\Bigg{)}.\]

Since \(\ell(w,S)\) is \((\sigma/\sqrt{n})\)-subgaussian for all \(w\), Markov's inequality gives, for any \(0<\delta<1\),

\[\mathbf{P}\Bigg{[}\Big{\langle}Q_{W},\exp\Big{(}\frac{\mathrm{gen}^{2}(\cdot,S )}{6\sigma^{2}/n}\Big{)}\Big{\rangle}>\frac{2}{\delta}\Bigg{]}\leq\frac{ \delta}{2}\Big{\langle}P_{S}\otimes Q_{W},\exp\Big{(}\frac{\mathrm{gen}^{2}( \cdot,\cdot)}{6\sigma^{2}/n}\Big{)}\Big{\rangle}\leq\delta,\]

which concludes the proof.

Proof of Theorem 7.The argument is almost identical to the proof of Theorem 3, with the difference that (5) is used for decorrelation.

To lighten the notation, let \(\pi_{k}^{S}:=P_{W_{k}W_{k-1}|S}\) and \(\rho_{k}:=\rho_{W_{k}W_{k-1}}\). Use the same definitions of \(\delta,\sigma,\zeta\) as in the proof of Theorem 3. Then, applying (5) with \(f(\cdot)=\sigma(\cdot,\tilde{S})\), \(g(\cdot)=\zeta(\cdot,\tilde{S})\), \(\mu=\pi_{k}^{S}\), \(\nu=\rho_{k}\), we have

\[\langle\pi_{k}^{S},\sigma(\cdot,\tilde{S})\zeta(\cdot,\tilde{S})\rangle\leq \sqrt{2}\|\sigma(\cdot,\tilde{S})\|_{L^{2}(\rho_{k})}+2\Big{\langle}\pi_{k}^{S},\sigma(\cdot,\tilde{S})\psi_{2}^{-1}\Big{(}\frac{\mathrm{d}\pi_{k}^{S}}{ \mathrm{d}\rho_{k}}\Big{)}\Big{\rangle}\]

\[+2\|\sigma(\cdot,\tilde{S})\|_{L^{1}(\pi_{k}^{S})}\sqrt{\log\langle\rho_{k}, \exp\big{(}\zeta^{2}(\cdot,\tilde{S})\big{)}\rangle}.\]

By Markov's inequality and the union bound, for any \(0<\delta<1\),

\[\mathbf{P}\Big{[}\exists k\text{ s.t. }\Big{\langle}\rho_{k},\exp\big{(} \zeta^{2}(\cdot,\tilde{S})\big{)}\Big{\rangle}>\frac{2}{p_{k}\delta}\Big{]} \leq\sum_{k}\frac{p_{k}\delta}{2}\Big{\langle}P_{\tilde{S}}\otimes\rho_{k}, \exp\big{(}\zeta^{2}(\cdot,\cdot)\big{)}\Big{\rangle}\leq\delta.\]

Using this together with the estimate \(\sigma(\cdot,\tilde{S})\leq 2\sqrt{n}d_{\tilde{S},\ell}(\cdot)\) yields the result in the statement.

### Proofs for Section 8

Proof of Theorem 8.Without loss of generality, we assume \(\mathrm{diam}(T)=1\). Let \(Q\) be the Markov kernel from \(\Omega\) to \(T\) defined by \(Q(\cdot|\omega)=\delta_{\tau(\omega)}(\cdot)\); in particular, \(\nu(\cdot)=\int_{\Omega}\mathbb{P}(\mathrm{d}\omega)Q(\cdot|\omega)\).

Fix some \(r\geq 2\). For each \(k\geq 0\) and each \(t\in T\), let \(B_{k}(t):=B(t,r^{-k})\). Since \(T\) is finite, there exists some \(K\in\mathbb{N}\), such that \(B_{K}(t)=\{t\}\) for all \(t\in T\). Let \(\mu\in\mathcal{P}(T)\) be given. Define the following sequence of Markov kernels from \(\Omega\) to \(T\):

\[Q_{k}(\cdot|\omega):=\frac{\mu(\cdot\cap B_{k}(\tau(\omega)))}{\mu(B_{k}(\tau( \omega)))},\qquad k=0,\dots,K.\]

Observe that \(Q_{0}=\mu\) and \(Q_{K}=Q\). Then, since

\[\langle\mathbb{P}\otimes\mu,X\rangle=\int_{\Omega\times T}\mathbb{P}(\mathrm{d }\omega)\mu(\mathrm{d}t)X_{t}(\omega)=\int_{T}\mu(\mathrm{d}t)\mathbf{E}[X_{t} ]=0,\]

we can write

\[\mathbf{E}[X_{\tau}] =\langle\mathbb{P}\otimes Q,X\rangle-\langle\mathbb{P}\otimes \mu,X\rangle\] \[=\langle\mathbb{P}\otimes Q_{K},X\rangle-\langle\mathbb{P}\otimes Q _{0},X\rangle\] \[=\sum_{k=1}^{K}\langle\mathbb{P}\otimes Q_{k}-\mathbb{P}\otimes Q _{k-1},X\rangle\] \[=\sum_{k=1}^{K}\int_{\Omega}\left(\int_{T}X_{t}(\omega)Q_{k}( \mathrm{d}t|\omega)-\int_{T}X_{t}(\omega)Q_{k-1}(\mathrm{d}t|\omega)\right) \mathbb{P}(\mathrm{d}\omega)\] \[\leq\sum_{k=1}^{K}\int_{\Omega}\int_{T\times T}|X_{u}(\omega)-X_{v}( \omega)|Q_{k}(\mathrm{d}u|\omega)Q_{k-1}(\mathrm{d}v|\omega)\mathbb{P}(\mathrm{d }\omega).\]

[MISSING_PAGE_EMPTY:19]