# Sirius: Contextual Sparsity with Correction

for Efficient LLMs

 Yang Zhou\({}^{1}\), Zhuoming Chen\({}^{1}\), Zhaozhuo Xu\({}^{2}\), Xi Victoria Lin\({}^{3}\), Beidi Chen\({}^{1,3}\)

\({}^{1}\)Carnegie Mellon Univeristy

\({}^{2}\)Stevens Institute of Technology

\({}^{3}\)FAIR at Meta

{yangzho6, zhuominc, beidic}@andrew.cmu.edu

zxu79@stevens.eduvictorialin@meta.com

###### Abstract

With the blossom of large language models (LLM), inference efficiency becomes increasingly important. Various approximate methods are proposed to reduce the cost at inference time. Contextual Sparsity (CS) is appealing for its training-free nature and its ability to reach a higher compression ratio seemingly without significant performance degradation. However, after a comprehensive evaluation of contextual sparsity methods on various complex generation tasks, we find that although CS succeeds in prompt-understanding tasks, it significantly degrades the model performance for reasoning, deduction, and knowledge-based tasks. Despite the gap in end-to-end accuracy, we observed that sparse models and original models often share the general problem-solving logic and require only a few token corrections to recover the original model performance. This paper introduces Sirius1, an efficient correction mechanism, which significantly boosts CS models on reasoning tasks while maintaining its efficiency gain. Sirius is evaluated on 6 models with 8 difficult generation tasks in reasoning, deduction, and coding and shows consistent effectiveness and efficiency. Also, we carefully develop a system implementation for Sirius and show that Sirius delivers theoretical latency reduction with roughly 20% reduction in latency for 8B model on-chip and 35% reduction in latency for 70B model offloading. We open-source our implementation of Sirius at [https://github.com/Infini-AI-Lab/Sirius.git](https://github.com/Infini-AI-Lab/Sirius.git).

Footnote 1: We draw inspiration from the astronomical concept, in which Sirius refers to a two-body star system, where one is the brightest star ever detected, while the other is a dim star.

## 1 Introduction

Large Language Models (LLM), such as OpenAI et al. (2024) (GPT-4), Team et al. (2024) (Gemini), and Touvron et al. (2023) (Llama) have demonstrated their proficiency in a wide range of natural language processing applications such as content creation, summarization, and impressive and complex reasoning tasks. However, their deployment is very challenging, especially in latency-sensitive settings (Kaplan et al., 2020). Exploiting the model sparsity is a natural way to reduce the model parameter size and computational cost with a long history (LeCun et al., 1989; Tibshirani, 1996). More recently, many studies have shown that _contextual sparsity_(Liu et al., 2023; Li et al., 2022; Dong et al., 2024; Lee et al., 2024), which highly correlates to the prompt or the context, can greatly speed up LLM inference without quality degradation.

However, in this paper, we first demonstrate a critical and fundamental problem with _contextual sparsity_ (CS): while generally robust in classification tasks and generation tasks that mainly rely on prompt understanding (e.g., summarization, chat question-answering), we found that CS models struggle at high-level reasoning and understanding tasks.

For example, in Figure 1 (a), we contrast between the Text Summarization task (CNN/DailyMail) and Arithmetic Reasoning (GSM8K) with contextual sparsity methods on Llama-3-8B-Instruct. Varying sparsity levels, Llama-3-8B-Instruct with contextual sparsity performs consistently worse on GSM8K than CNN/DailyMail. With roughly 50% sparsity globally, the sparse model degradation is still reasonable on text summarization (right axis in color coral) compared to almost collapsed on arithmetic reasoning (left axis in color green). **However, for reasoning tasks, can we simply live with a higher density ratio to preserve more performance?** Unfortunately, the answer is NO. Besides the significant efficiency loss, shown in Figure 1 (b), the Llama-3-70B-Instruct model with contextual sparsity also crashes at around 50% sparsity globally. The 50% sparse model still has 4\(\times\) the parameter size compared to the smaller dense model (Llama-3-8B-Instruct), while still performing worse on GSM8K-CoT, rendering contextual sparsity utterly not useful for complex reasoning tasks.

We conduct an in-depth study on the CS model failure cases and notice that the overall reasoning pathway of these sparse models is usually sound and adheres to the full model. The fatal mistakes are always caused by some middle tokens and propagate towards the end, examples can be seen in Figure 4. Following this observation, we conduct a simple experiment with 65% Contextual Sparse Llama-3-8B-Instruct on GSM8K as presented in Figure 1 (c). We run both the sparse and the full models together for the same prompt and compare two generation output token-by-token.

Surprisingly, the trend increases steeply with the percentage of corrected tokens. Correction of only 6% tokens in the sparse model's generation recovers most of GSM8K accuracy (\(<\)5% to full), and 11% to recover the full performance. The results show potential for an efficient and powerful correction mechanism to maintain the sparse efficiency while boosting its performance. Contextual sparsity uses a dynamic sparsity pattern and naturally requires the full model to be in GPU memory during runtime, allowing the full model to be used efficiently for infrequent correction. Even though only very few need to be corrected, locating these mistaken tokens efficiently turns out to be challenging.

Ideally, we want a correction system to have the following properties: 1) **Effective**, the sparse model quality degradation can be improved to the full model vicinity; 2) **Cheap**, the full model only gives minimal intervention; 3) **Adaptive**, the system is efficient across various reasoning datasets.

In this paper, we carefully analyze and formulate correction efficiency in Section 2. We extensively categorize the strengths and weaknesses of CS in Section 3. In Section 4, We systematically design Sirius, a correction method covering all three desired properties. (**When?**) In Section 4.1, we show that the sparse model can be both confident or uncertain when making mistakes, rendering the signal from sparse unreliable for determining when to correct. Sirius is a period-based method with the period as a hyperparameter. (**How?**) In Sections 4.1 and B.2, we introduce novel KV Cache direct rewriting, minimal rollbacks, and hardware-efficient tree building to help increase the effective period of full model correction, thus, ensuring the correction efficiency.

Figure 1: Contextual sparse models struggle at challenging text generation tests that require high-level reasoning and understanding, e.g. GSM8K. On these tasks, contextually sparse models lead to significant quality degradation. In (a), we contrast CS Llama-3-8B-Instruct on GSM8K (green) and CNN DailyMail (coral). (b) Contextual Sparsity Llama-3-70B-Instruct crashes at 50% global sparsity, making the smaller dense model Llama-3-8B-Instruct (green star) a significantly more efficient choice than the sparse 70B model. (c) Sparse model crashing at reasoning tasks has patterns, and ideally only correcting 11% unlikely tokens recovers the sparse model performance fully.

In Section 5, we empirically evaluated Sirius on 6 different models with 8 different reasoning tasks and showed that Sirius is generally effective and efficient. On GSM8K and Llama-3-8B-Instruct specifically, we boost the fine-grained sparsity from 58% to 72% with 4% increase in effective parameter size and coarse-grained sparsity from 38% to 70% with the cost of 5% effective parameter size. We also show that Sirius delivers the promised efficiency on mainstream GPUs in both on-chip and offloading settings.

## 2 Related Works and Problem Formulation

In this section, we first present the classification of the prior Contextual Sparsity methods and narrate important efficiency metrics in. **Also, we present careful analysis and quantitative comparisons on why Speculative Decoding is inefficient in recovering contextual sparsity.** Due to space constraints, we refer to Appendix A.2. For extended related works on model compression, contextual sparsity, and speculative decoding, we present in Appendix A.1.

**Contextual Sparsity Classification** - Contextual sparsity (CS) methods are usually training-free, easy to use, and seemingly effective, making them highly attractive to ML practitioners looking to reduce LLM inference costs. CS exists naturally in MLP layers of the LLM, which occupies roughly 70% of the LLM total weights (Dong et al., 2024; Lee et al., 2024)). The contextual sparsity selection is as follows: given the context, only a limited number of the most relevant neurons are selected based on the input activation. The rest contributed to the output far less is discarded. We refer to two main directions of contextual sparsity methods as **Coarse-grained Sparsity** (CSparse) Methods (Dong et al. (2024)) - that within the same input prompt, the sparsity pattern is fixed for all tokens generated. **Fine-grained Sparsity** (FSparse) Methods (Lee et al. (2024)) - that exploits the per-token sparsity to save resources.

**Average Parameters Used Per Token** - A key metric is used to evaluate the efficiency of our proposed method, the Average Parameter Used per token decoded (later referred to as APU). LLM inference is memory I/O bound (Leviathan et al., 2023; Kim et al., 2023). The latency of generating every single token is dominated by the memory loading time from the GPU HBM to SRAM. On the other hand, Sirius relies on full model parallel verifying a chunk of tokens. Although from the FLOPs standpoint, the amount of compute performed per evaluation step is the number of input token times of a single token input process, the latency of parallel verification is still roughly the same as taking a single token (Verified further in 10, length 64 is only 1.1 ms longer than length 1), because the inference is memory bound.

Sirius operates in the memory-bound regime (single inference sequence length smaller than or equal to 64). Thus, the average parameter count of a model gives us a rough judgment of the latency of inference. Formally, for a full LLM to have \(C_{full}\) number of parameters, and its sparse counterpart of a certain predetermined sparsity \(C_{sparse}\). The average advancement length (later we refer to as AAL) in the number of tokens between two consecutive LLM corrections can be represented as \(n_{AAL}\)

Figure 2: Overview of Sirius. Contextual Sparsity requires full model weights to be placed on the GPU memory. While the sparse model doesn’t perform well on complex reasoning tasks, Sirius uses the Full Model to correct the Sparse model. The full model is called fairly infrequently. During the correction, the Full Model will rewrite the KV Cache, interleave with high-quality tokens to the sparse outputs, and then roll back only when the token is deemed extremely unlikely by the Full Model.

The average parameters used per token (APU) are the following

\[\text{APU}=\frac{n_{sparse}C_{sparse}+C_{full}}{n_{AAL}} \tag{1}\]

We want the metric to be as small as possible, and obviously, we want \(n_{AAL}\) to be as large as possible.

Another thing to note is that we always compare the system's APU against the full model's APU, which is \(C_{full}\). If we divided the above equation by \(C_{full}\), we can have an equivalent parameter density of the system defined based on \(I_{globalsparsity}\), which is \(C_{sparse}/C_{full}\).

\[\text{Effective Density}=\frac{n_{sparse}I_{globalssparsity}+1}{n_{AAL}} \tag{2}\]

Later, if we use period \(n_{period}\), the equation can be rewritten as

\[\text{Effective Density}=\frac{(n_{period}-1)I_{globalssparsity}+1}{n_{AAL}} \tag{3}\]

Later when presenting Sirius, we mainly specify \(n_{period}\) with \(n_{AAL}\) to evaluate its efficiency. Notice that \(I_{globalsparsity}\) is determined by the sparsity method, Sirius cannot change it anymore.

## 3 Observations

In this section, we present a detailed study of the strengths and weaknesses of Contextual Sparsity (CS). 3.1 presents the strengths of CS. 3.2 presents the weaknesses of CS. Additionally, we show that given the similar parameter size, the more well-trained the model is, the more CS degradation will be for the model. Due to limited space, we present the details in Appendix B.1. 3.3 shows our findings when looking into the failure cases of the CS model in complex reasoning generation tasks.

In the following series of experiments, we build our implementation2 of fine-grained sparsity based on Lee et al. (2024) and coarse-grained sparsity based on Dong et al. (2024). The default sparsity for both methods is 50% for the MLP component of the model (whole MLP for coarse-grained sparsity and Up and Down linear layers only for fine-grained sparsity). We mainly use this default setting in most experiment tables in the paper without explicitly mentioning it. Otherwise, we will explicitly specify the different sparsity levels we used.

Footnote 2: Since Lee et al. (2024) doesn’t open-source its implementation and it relies on the threshold for determining the sparsity pattern, replicating the method isn’t straightforward. Using a threshold also increases the difficulty of determining the actual density of the sparse model. Our implementation uses topk on the Gate Layer activations. The rest is implemented as described in the original method.

### Contextual Sparsity: Where Does It Succeed?

For tasks on prompt understanding, CS generally performs well and gives consistent and strong output. We evaluate CS models on machine summarization (CNNDailyMail et al. (2017)), and Conversational Question Answering (CoQA et al. (2019)). The results show that the correctly selected contextual sparsity in the MLP layers and the full attention layers can fully extract and understand the local prompt information. More details are presented in Figure 3, where we show that by varying the sparsity level, the language model's performance on CNN/DailyMail is robust even when the activation sparsity drops to below 20%, which translates to around 44% global density.

For tasks accessing factuality and hallucination, we select the generation portion of the TruthfulQA dataset (Lin et al., 2022). Results are shown in Table 1, where we evaluate the techniques on 5 different LLMs. Interestingly, we find that the Fine-grained sparsity is often better than the dense model baseline across different models. This finding is consistent with previous works Laser (Sharma et al., 2023) and Dola (Chuang et al., 2024). They both observed that compressing the original LLM in a carefully designed way would lead to improvement in factuality and better de-hallucination. Laser comes from the low-rank approximation of the MLP layers, while Dola proposes a factuality-aware layer-skipping algorithm. Based on their findings, hallucination occurs when parts of the weights aren't as well-versed in the given input as the other parts. They expose the "averaging" effect that blurs the factuality of the output. Removing these neurons gives rise to better factuality and less hallucination. Our studies look at the same problem from a neuron sparsity standpoint.

### Contextual Sparsity: Where Does It Fail?

On the other hand, contextual sparsity severely struggles when the generation tasks rely solely on the model's own reasoning and world knowledge understanding ability. Here we show the Llama-3-8B-Instruct and the Llama-2-7B-Chat models in Table 1, refer to Table 12 for evaluations on more models. Notice that since fine-grained sparsity method needs the activation from Gate MLP for selecting sparsity, while coarse-grained sparsity has a predetermined pattern after prefilling and can sparsify the Gate MLP. Even though both are at 50% activation sparsity, the coarse-grained sparsity method effectively achieves higher parameter savings than fine-grained sparsity in practice. Here we evaluate the sparse techniques using 5-shot CoT on the GSM8K dataset (Cobbe et al., 2021). We found that across all the models we evaluated, both sparsity methods lead to significant accuracy degradation. We include HumanEval (Chen et al., 2021), a coding task that requires complex reasoning and planning ability. We found that both sparsity methods exhibit similar performance degradation when it comes to coding. Shown in Figure 3, two tasks see sparsity significantly drop performance after 50% activation sparsity.

For knowledge recall and world knowledge understanding, we specifically test on MMLU-Flan-CoT (Chung et al., 2022) the CoT text generation version of the MMLU dataset (Hendrycks et al., 2021). Table 1 shows the results. Stronger models like Llama-3-8B-Instruct suffer from significant

\begin{table}
\begin{tabular}{l|l|l|l} \hline
**Where CS Succeeds** & **CNN/DailyMail** & **CoQA** & **TruthfulQA** \\ Experiment Settings & Unitxt Rouge & EM/F1 & Rouge-1/2 ACC \\ \hline
**Llama-3-8B-Instruct** & 0.1237 & 0.6153/0.7825 & 0.4945/0.3647 \\
**Llama-3-8B-Instruct-CSparse** & 0.1144 & 0.6633/0.7977 & 0.4725/0.3403 \\
**Llama-3-8B-Instruct-FSparse** & 0.1166 & 0.6625/0.7984 & 0.5043/0.3305 \\ \hline
**Llama-2-7B-Chat** & 0.1489 & 0.5982/0.7580 & 0.4480/0.3831 \\
**Llama-2-7B-Chat-CSparse** & 0.1448 & 0.6117/0.7639 & 0.4529/0.3843 \\
**Llama-2-7B-Chat-FSparse** & 0.1521 & 0.5898/0.7540 & 0.4565/0.3660 \\ \hline
**Where CS Fails** & **GSM8K** & **HumanEval** & **MMLU\({}^{*}\)** \\ Experiment Settings & ACC (strict/flexible) & Pass@1 (GD) & Accuracy \\ \hline
**Llama-3-8B-Instruct** & 0.7551/0.7544 & 0.560 & 0.6231 \\
**Llama-3-8B-Instruct-CSparse** & 0.3859/0.3874 & 0.207 & 0.5558 \\
**Llama-3-8B-Instruct-FSparse** & 0.5868/0.5891 & 0.457 & 0.5304 \\ \hline
**Llama-2-7B-Chat** & 0.2396/0.2462 & 0.140 & 0.492 \\
**Llama-2-7B-Chat-CSparse** & 0.1334/0.1380 & 0.067 & 0.4637 \\
**Llama-2-7B-Chat-FSparse** & 0.1979/0.2017 & 0.134 & 0.4768 \\ \hline \end{tabular}

* **MMLU** is a classification task, not generation tasks. We use **MMLU-FLAN-COT**

\end{table}
Table 1: We show the difference between cases when Contextual Sparsity (CS) succeeds or fails. CS is generally good at prompt understanding tasks and tasks that measure the trustworthiness of the language models while not good at tasks that require reasoning and world knowledge understanding.

Figure 3: We contrast between Contextual Sparsity on prompt understanding task and complex generation tasks that require reasoning. (a) Both CSparse and FSparse are robust on CNN/DailyMail for various sparsity; (b) and (c) Show that both CSparse and FSparse crash on GSM8K and HumanEval at the global sparsity that they are still robust in prompt understanding tasks.

degradation too. Furthermore, we found that given the similar parameter size, the more well-trained the models are, the higher its degradation from the contextual sparsity, more details in Appendix B.1.

### A Closer Look on GSM8K Quality Degradation

To study the inability of the sparse model in deduction, we conduct a case study on the sequence-level coarse-grained sparsity methods Dong et al. (2024) with the Llama-3-8B-Instruct model. We visually inspect extensive cases where the sparse model and dense differ in answers. Generally, the sparse model always produces highly similar answers to the dense model: the similar approach or logic flow when approaching the same problem and even the same number of sentences before the first mistake occurs or in success cases. However, the key differences are usually caused by the following three categories of small token-level mistakes: (1) frequent miscalculation in the intermediate steps, (2) wrong reasoning in intermediate steps, and (3) insensible and random statements. For each of the above-summarized cases, we find failure question-answer pairs provided in Figure 4. These mistakes happen in the middle of arguments and propagate to the wrong end result.

Similar observations can also be found for fine-grained sparse methods with different model types. _Interestingly, we find that even with these mistakes, the sparse model can still fully generate coherent tokens and make further reasoning assuming their prior steps are correct._

We hypothesize that the gap between the full model and these sparse counterparts is at these key tokens. The following simple experiment is conducted to further verify our hypothesis. We run the coarse-grained sparse model and the full model with the same input prompt and for every token the sparse model generates, the full model is used to check the likelihood of these decoded tokens, mainly removing tokens with low likelihood. By varying the likelihood threshold, we can control the frequency of the correction. The experiments are conducted for both Llama-3-8B-Instruct and Llama-2-7B-Chat Touvron et al. (2023) models with coarse-grained sparsity. The results are shown in Figure 1(c). In both cases, we found that a very small amount of correction would drastically improve the sparse model performance, showing a steep gradient when the percentage of corrected tokens is small. With merely 10% of tokens needing to be corrected, the sparse model can completely match the full model's performance. The experiment verifies our hypothesis that by correcting the small portion of key tokens, the sparse model can meet the large model's performance.

## 4 Methods

Though we find a minor portion of tokens needed to be corrected for the contextual sparsity model to fully recover performance, the challenge remains: how to locate these mistaken tokens with the minimal number of parallel verification rounds of the full model? In this section, we show that the sparse model provides signals that cannot be trusted 4.1. Then, we describe in detail the various correction techniques in 4.1. Because of the space limit, we put how to boost the sparse generation with hardware-efficient tree building B.2.

Figure 4: Examples of contextual sparse model making the identified three different types of mistakes. Most mistakes occur because the model makes calculation mistakes or has a wrong reasoning step compared to the full model. We also observe that there are rare cases where the model makes insensible statements in the middle that make the end result wrong.

### Sparse Model's Self-Awareness Cannot Be Trusted

Intuitively, rather than fixing the \(n_{sparse}\) number, letting the system decide when to call the LLM for evaluation would then give more flexible \(n_{sparse}\). Nevertheless, we argue that the sparse model's output probability distribution cannot be used as a metric for accuracy decisions. We empirically experiment with various methods to utilize the information contained in the sparse model's output distribution. However, varying the threshold leads to \(n_{sparse}\) being too short when the threshold is strict or failing to correct when the threshold is lenient. We then discovered that the sparse model has very limited self-awareness of its own mistakes. To make the observation concrete, we present a small example in Figure 6 a piece of text where the sparse model makes a mistake while the full model succeeds. The red bars signify the error location. The token entropy is neither high nor at zero, making it impossible to effectively use a threshold to control the number \(n_{sparse}\).

### How to Correct the Sparse Output Tokens

The full overview of Sirius is presented in Figure 2 and Algorithm 1. The full model is called once every kernel size. The KV cache is shared between the sparse and the full model. The KV cache is mostly populated by the sparse model, which is called for every token. During correction, the full model takes in the last kernel size of tokens and generates its KVs for the past kernel size tokens in parallel, these KVs are directly written to their corresponding positions in the shared KV Cache. Empirically, we found that full model's KV helps the sparse model's output. When LLM is called to evaluate the sparse model's output, it uses its own predicted likelihood to determine whether to accept or reject the sparse model's past output. The decision is based on comparing the likelihood against a preset threshold. Detailed ablation for threshold is in B.3. Besides the above-mentioned techniques, we also found that the "second/third" choices of the sparse models' rejected token position offer \(>\) 80% coverage of the LLM accepted tokens. The observation motivates us to build a hardware-friendly tree on the sparse model generating side that doesn't sacrifice the performance while significantly boosting the \(n_{AAL}\) or efficiency. Due to the space limit, a great amount of details is in Appendix B.2.

## 5 Experiments

In this section, we empirically evaluate Sirius to correct CS models on various generation tasks in complex reasoning. We show that Sirius is consistent in various tasks, effective in helping CS models recover their performance, and efficient in correction with low additional overhead.

* In 5.1, we evaluated Sirius on six models with 8 different datasets. Sirius is consistently effective and efficient. Specifically, on GSM8K, Sirius corrects FSparse Llama-3-8B-Instruct from 58% accuracy to 72% with only 4% increase in parameter density and corrects CSparse model from 38% to 70% with 5% density.
* In 5.2, we presents more details on our system implementation for Sirius. We show that Sirius delivers its theoretical efficient promise, achieving roughly 20% reduction in latency compared to full on-chip on various hardware. Sirius further achieves 35% speedup to full in offloading settings.

Figure 5: In (a), we present an example that illustrates why the signals from the sparse model are unreliable. It is a figure plotting entropy versus generated tokens. At the tokens where the sparse made the mistake (red), the entropy isn’t in large spikes which signifies chaos and low confidence, rather it is even quite low, compared to nearby entropy spikes. In (b) and (c), we view Sirius as a compression method by itself. We compare Sirius with contextual sparse methods and show that given the same parameter used, Sirius performs better than Contextual Sparse Methods on GSM8K.

* We also present ablation with rich details on how each component of Sirius contributes to its performance and how threshold is used to trade off efficiency and performance. Due to space limit, we place it in Appendix B.3.

### Sirius Significantly Recovers CS Degradation with Low Cost

**Models and Datasets** - To comprehensively evaluate Sirius performance, we deploy six mainstream LLMs with sizes ranging from 7B to 13B: Llama-2-7B, 13B, and Llama-3-8B with their instruction finetuned counterparts, all from Llama family. Following Wei et al. (2022) in LLM reasoning, we also tested CS models on two popular types of reasoning generation tasks: arithmetic and commonsense reasoning. On the Arithmetic side, besides GSM8K, we also evaluate CS models on AQua-RAT. On the Common Sense side, we use CSQA Saha et al. (2018), StrategyQAGeva et al. (2021), Date, and Sports, last two from Big Bench Suite bench authors (2023). Most of these tasks are originally classification tasks. Following the instruction in Wei et al. (2022), we manually compose COT prompts to transform these into logic argument generation tasks. Besides, CS models do not perform well in coding, which requires forming logical arguments and planning. We select HumanEval Chen et al. (2021) and MBPP+ Liu et al. (2023) to evaluate Sirius.

For arithmetic reasoning and coding, we use 50% neuron sparsity for both CSparse and FSparse. FSparse relies on the gate layer to be dense, leading to higher global density than CSparse. Since commonsense reasoning tasks are generally less logically challenging comparatively, we lowered the neuron sparsity level to 40%.

**Main Results** - Due to space limits, we only select the best treewidth of Sirius for GSM8K, CSQA, and HumanEval for the main results in Table 2. Extensive studies on the rest 5 datasets with different treewidth are presented in the Appendix C. From Table 2, we can see that Sirius is consistently effective and efficient across all different classes of tasks. Specifically for Llama-3-8B-Instruct, besides GSM8K, Sirius corrects FSparse and CSparse, on CSQA, from 61% and 64% accuracy to 70% with cost only 3% sparsity for FSparse and 7% for CSparse respectively. On HumanEval, Sirius corrects FSparse from 45% to 61% with 4% sparsity overhead even surpassing the full model's performance,

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|c|} \multicolumn{7}{c}{**GSM8K**} \\ \hline \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Full Perf.**} & \multirow{2}{*}{**CSparse Perf.**} & \multicolumn{2}{c|}{**CSparse Density**} & \multirow{2}{*}{**Sirius Perf.**} & \multirow{2}{*}{**AAL**} & **Effective Density** \\ \cline{1-1} \cline{5-7}  & & & & & & \\ \hline Llama-3-8B-Instruct & 0.7536 & 0.3844 & 0.65 & 0.7051 (8) & 15.22/16 & 0.706 \\ Llama-3-8B & 0.4966 & 0.2085 & 0.65 & 0.4177 (8) & 15.29/16 & 0.703 \\ Llama-2-7B-Chat & 0.2403 & 0.1334 & 0.69 & 0.2244 (8) & 15.00/16 & 0.757 \\ Llama-2-7B & 0.1357 & 0.0758 & 0.69 & 0.1183 (6) & 15.87/16 & 0.715 \\ Llama-2-13B-Chat & 0.3548 & 0.2714 & 0.68 & 0.3381 (4) & 15.34/16 & 0.730 \\ Llama-2-13B & 0.2282 & 0.1759 & 0.68 & 0.2418 (1) & 15.34/16 & 0.730 \\ \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Full Perf.**} & \multirow{2}{*}{**FSparse Perf.**} & \multirow{2}{*}{**FSparse Density**} & \multirow{2}{*}{**Sirius Perf.**} & \multirow{2}{*}{**AAL**} & **Effective Density** \\ \cline{1-1} \cline{5-7}  & & & & & & \\ \hline Llama-3-8B-Instruct & 0.7536 & 0.5868 & 0.76 & 0.7278 (4) & 15.37/16 & 0.807 \\ Llama-3-8B & 0.4966 & 0.3199 & 0.76 & 0.4579 (2) & 15.03/16 & 0.825 \\ Llama-2-7B-Chat & 0.2403 & 0.1971 & 0.79 & 0.2388 (6) & 15.69/16 & 0.819 \\ Llama-2-7B & 0.1357 & 0.1137 & 0.79 & 0.1410 (4) & 15.91/16 & 0.807 \\ Llama-2-13B-Chat & 0.3548 & 0.3222 & 0.78 & 0.3533 (1) & 15.08/16 & 0.842 \\ Llama-2-13B & 0.2282 & 0.2191 & 0.78 & 0.2372 (4) & 15.92/16 & 0.797 \\ \hline \multicolumn{7}{c}{**CSUA**} \\ \hline \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Full Perf.**} & \multirow{2}{*}{**CSparse Perf.**} & \multirow{2}{*}{**CSparse Density**} & \multirow{2}{*}{**Sirius Perf.**} & \multirow{2}{*}{**AAL**} & **Effective Density** \\ \cline{1-1} \cline{5-7}  & & & & & & \\ \hline Llama-3-8B-Instruct & 0.7073 & 0.6470 & 0.58 & 0.7076 (8) & 14.76/16 & 0.657 \\ Llama-3-8B & 0.6437 & 0.5585 & 0.58 & 0.6429 (8) & 15.43/16 & 0.628 \\ Llama-2-7B-Chat & 0.6248 & 0.5200 & 0.62 & 0.6175 (8) & 15.07/16 & 0.683 \\ Llama-2-7B & 0.4742 & 0.4414 & 0.62 & 0.4742 (8) & 15.80/16 & 0.652 \\ Llama-2-13B-Chat & 0.6879 & 0.5536 & 0.61 & 0.6691 (4) & 11.43/12 & 0.674 \\ Llama-2-13B & 0.6109 & 0.5601 & 0.61 & 0.6060 (4) & 15.72/16 & 0.645 \\ \hline \multicolumn{7}{c}{**HumanEval**} \\ \hline \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Full Perf.**} & \multirow{2}{*}{**FSparse Perf.**} & \multirow{2}{*}{**FSparse Density**} & \multirow{2}{*}{**Sirius Perf.**} & \multirow{2}{*}{**AAL**} & **Effective Density** \\ \cline{1-1} \cline{5-7}  & & & & & & \\ \hline Llama-3-8B-Instruct & 0.561 & 0.207 & 0.65 & 0.524 (8) & 14.67/16 & 0.733 \\ Llama-3-8B & 0.262 & 0.067 & 0.65 & 0.243 (8) & 15.10/16 & 0.691 \\ Llama-2-7B-Chat & 0.140 & 0.067 & 0.69 & 0.159 (8) & 10.88/12 & 0.789 \\ Llama-2-7B & 0.116 & 0.079 & 0.69 & 0.128 (8) & 14.84/16 & 0.765 \\ Llama-2-13B-Chat & 0.189 & 0.122 & 0.68 & 0.171 (8) & 11.12/12 & 0.762 \\ Llama-2-13B & 0.262 & 0.067 & 0.68 & 0.244 (8) & 15.10/16 & 0.741 \\ \hline \multicolumn{7}{c}{**FSparse Density**} & \multirow{2}{*}{**Sirius Perf.**} & \multirow{2}{*}{**AAL**} & **Effective Density** \\ \cline{1-1} \cline{5-7}  & & & & & & \\ \hline Llama-3-8B-Instruct & 0.561 & 0.457 & 0.76 & 0.616 (6) & 15.42/16 & 0.804 \\ Llama-3-8B & 0.262 & 0.189 & 0.76 & 0.298 (6) & 15.54/16 & 0.797 \\ Llama-2-7B-Chat & 0.140 & 0.134 & 0.79 & 0.165 (6) & 15.27/16 & 0.841 \\ Llama-2-7B & 0.116 & 0.116 & 0.79 & 0.165 (6) & 15.86/16 & 0.810 \\ Llama-2-13B-Chat & 0.189 & 0.146 & 0.78 & 0.183 (6) & 15.34/16 & 0.827 \\ Llama-2-13B & 0.246 & 0.233 & 0.78 & 0.259 (4) & 15.85/16 & 0.801 \\ \hline \end{tabular}
\end{table}
Table 2: We show Sirius effectiveness and efficiency in the following table. We select GSM8K for Arithmetic Reasoning, CSQA for Commonsense Reasoning, and HumanEval for code generation. Under the "Sirius Perf. " column, A(B) is shown. A denotes the accuracy after Sirius correction in the dataset evaluated, while (B) represents the optimal treewidth selected under the current model dataset settings. Under the column of "AAL", X/Y is shown, where X is the AAL, while Y is the period. **GSM8K**and from 20% to 52% with 8% sparsity as cost. Besides, Llama-3-8B-Instruct, Sirius corrects all 6 models with additional sparsity overhead smaller than 10% across these three datasets, further showing its strong efficiency. Besides results in Table 2, in Appendix C, we show that Sirius consistently shows great effectiveness with high efficiency across the rest of the 5 datasets.

### Wallclock Speedup

Here we show that Sirius delivers its promised efficiency claim for on-chip and offloading settings. Because the fine-grained sparsity Lee et al. (2024) relies on a custom CUDA kernel to achieve the target generation speedup not open-sourced, we focus on coarse-grained sparsity on GSM-8K COT, and the input sequence length with average prefill length 900.

Firstly, we consider the on-chip setting running Llama-3-8B-Instruct on a single GPU. The sparse model (APU 0.65) achieves 36.01% accuracy on GSM8K-COT, while the full model achieves 76.12% accuracy on GSM8K-COT. With kernel size 10, Sirius achieves 0.74 APU with accuracy 71.27% accuracy. We use torch compile to optimize the inference latency and limit the overhead other than running model inference. The average latency generated per token is used to compute latency. Results are shown in Table 3. On average, Sirius delivers the promised latency reduction from APU calculations. The speedup ratio on A40 and L40 closely aligns with the theoretical APU reported. On the other hand, A100 and H100 compute MLP more efficiently than it compute attention, making the latency ratio between computing MLP and attention not perfectly aligned with their ratio in parameter size. Therefore, we see that even the sparse model baseline has slightly higher latency as expected. We increase the kernel size from 10 to 16 for these two devices, where accuracy reaches 0.7089 and the AAL reaches 13.67. For A100 and H100, building a hardware-efficient tree is nearly free of cost and highly effective. For numbers in Table 3, we use the width 4 tree that boosts the AAL to 15.01 out of 16. More details are in Appendix B.2.

Secondly, we consider the offloading setting which is the only way for resource-limited users to run 70B models by loading only the weights in use to GPU memory, while the others are offloaded to the CPU. Results are shown in Table 4. We use a single L40 48GB with a PCIe bus bandwidth of 25 GB/s to run Llama-3-70B-Instruct with batch size 1. Llama-3-70B-Instruct has roughly 80% of parameters to be MLP, which gives the theoretical APU for Griffin to be 0.6. Sparse + Sirius gives 0.649 APU, which is roughly what our system achieved.

## 6 Conclusion

We observe that contextual sparse methods significantly degrade reasoning and deduction tasks. Sirius, an efficient correction mechanism, enables accurate LLM inference with contextual sparsity. With roughly \(11\%\) to \(18\%\) sparsity increase, Sirius improves fine-grained and coarse-grained sparsity significantly in their performance while maintaining their efficiency gain.

Further, Sirius is still relying on rollback to correct the tokens that are deemed unlikely, which is inefficient. On the other hand, making the weak-strong model synergy systems that match the performance of the strong while keeping the efficiency of the weak, without strictly matching the strong models' output distribution remains an interesting and unsolved problem. We leave these topics to future works.

\begin{table}
\begin{tabular}{l|c|c|c} \hline \hline
**Settings** & **Sparse** & **Sirius** & **Full** \\ \hline
**Performance** & 0.7407 & 0.8719 & 0.9014 \\ \hline
**Latency (s)** & 3.57 s & 3.68 s & 5.72 s \\ \hline
**Ratio to Full** & 0.6241 & 0.6434 & 1.0 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Llama-3-70B-Instruct with Offloading.

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c|c|c|c} \hline \hline
**Settings** & **ACC** & **A40** & **Ratio** & **L40** & **Ratio** & **A100** & **Ratio** & **H100** & **Ratio** \\ \hline
**CSparse** & 0.3601 & 20.7 ms & 0.66 & 15.6 ms & 0.67 & 9.6 ms & 0.72 & 6.6 & 0.76 \\ \hline
**Sirius** & 0.7127 & 24.1 ms & 0.78 & 18.2 ms & 0.78 & 11.1 ms & 0.83 & 7.7 ms & 0.88 \\ \hline
**Full** & 0.7612 & 30.9 ms & 1.0 & 23.2 ms & 1.0 & 13.3 ms & 1.0 & 8.6 ms & 1.0 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Performance and Speedup Ratios on GSM8K-COT with Different Hardware Configurations.

Acknowledgement

We would like to thank Feng Liang and Yunong Liu for their helpful feedback during the exploration and writing. We also want to give a special thanks to Hanshi Sun for providing insights and suggestions for efficient implementation and speedup.

## References

* B. bench authors (2023)Beyond the imitation game: quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research. Cited by: SS1.
* C. Chen, S. Borgeaud, G. Irving, J. Lespiau, L. Sifre, and J. Jumper (2023)Accelerating large language model decoding with speculative sampling. Cited by: SS1.
* M. Chen, J. Tworek, H. Yuan, H. P. Quan, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, M. Krueger, M. Petrov, H. Khlaaf, G. Sastry, B. Mishkin, S. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Dannzis, E. Barnes, E. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Sainuders, W. Hesse, C. Carr, A. N. Leike, J. Achiam, N. Misra, V. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, R. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba (2021)Evaluating large language models trained on code. Cited by: SS1.
* Y. Chuang, Y. Xie, H. Luo, Y. Kim, and P. He (2024)Dola: decoding by contrasting layers improves factuality in large language models. Cited by: SS1.
* H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, S. Dehghani, A. Brahma, S. Su, Z. Dai, M. Suzgun, X. Chen, A. Chowdhery, A. Castro-Ros, M. Pellat, K. Robinson, D. Valler, G. Narang, A. Mishra, V. Yu, Y. Zhao, A. Huang, A. Huang, H. Dai, S. Yu, E. H. Petrov, J. Dean, J. Devlin, A. Roberts, D. Zhou, Q. V. Le, and J. Wei (2022)Scaling instruction-finetuned language models. External Links: 2201.01806 Cited by: SS1.
* K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman (2021)Training verifiers to solve math word problems. Cited by: SS1.
* H. Dong, B. Chen, and Y. Chi (2024)Prompt-prompted mixture of experts for efficient llm generation. Cited by: SS1.
* E. Frantz and D. Alistarh (2023)SparseM: massive language models can be accurately pruned in one-shot. Cited by: SS1.
* M. Freitag and Y. Al-Onaizan (2017)Beam search strategies for neural machine translation. arXiv preprint arXiv:1702.01806. Cited by: SS1.
* M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant (2021)Did Aristotle use a Laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics (TACL). Cited by: SS1.
* D. Hendrycks, S. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt (2021)Measuring massive multitask language understanding. Cited by: SS1.
* J. Kaplan, S. McCandlish, T. Henighan, T. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei (2020)Scaling laws for neural language models. arXiv preprint arXiv:2001.08361. Cited by: SS1.
* S. Kim, K. Mangalam, S. Moon, J. Malik, M. W. Mahoney, A. Gholami, and K. Keutzer (2023)Speculative decoding with big little decoder. Cited by: SS1.
* Y. LeCun, J. Denker, and S. Solla (1989)Optimal brain damage. Advances in neural information processing systems2. Cited by: SS1.
* J. Lee, D. Lee, G. Zhang, M. Tiwari, and A. Mirhoseini (2024)Cats: contextually-aware thresholding for sparsity in large language models. Cited by: SS1.
* Y. Leviathan, M. Kalman, and Y. Matias (2023)Fast inference from transformers via speculative decoding. Cited by: SS1.
* Z. Li, C. You, S. Bhojanapalli, D. Li, A. S. Rawat, S. J. Reddi, K. Ye, F. Chern, R. Yu, R. Guo, and S. Kumar (2022)The lazy neuron phenomenon: on emergence of activation sparsity in transformers. Cited by: SS1.
* S. Lin, J. Hilton, and O. Evans (2022)Truthfulqa: measuring how models mimic human falsehoods. Cited by: SS1.

Ling, W., Yogatama, D., Dyer, C., and Blunsom, P. (2017). Program induction by rationale generation: Learning to solve and explain algebraic word problems. _arXiv preprint arXiv:1705.04146_.
* Liu et al. (2023a) Liu, J., Xia, C. S., Wang, Y., and Zhang, L. (2023a). Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In _Thirty-seventh Conference on Neural Information Processing Systems_.
* Liu et al. (2023b) Liu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z., Shrivastava, A., Zhang, C., Tian, Y., Re, C., and Chen, B. (2023b). Deja vu: Contextual sparsity for efficient llms at inference time.
* Ma et al. (2023) Ma, X., Fang, G., and Wang, X. (2023). Llm-pruner: On the structural pruning of large language models.
* OpenAI et al. (2024) OpenAI, Achiam, J., et al. (2024). Gpt-4 technical report.
* Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of machine learning research_, 21(140):1-67.
* Reddy et al. (2019) Reddy, S., Chen, D., and Manning, C. D. (2019). Coqa: A conversational question answering challenge.
* Saha et al. (2018) Saha, A., Pahuja, V., Khapra, M. M., Sankaranarayanan, K., and Chandar, S. (2018). Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph.
* See et al. (2017) See, A., Liu, P. J., and Manning, C. D. (2017). Get to the point: Summarization with pointer-generator networks.
* Sharma et al. (2023) Sharma, P., Ash, J. T., and Misra, D. (2023). The truth is in there: Improving reasoning in language models with layer-selective rank reduction.
* Song et al. (2023) Song, Y., Mi, Z., Xie, H., and Chen, H. (2023). Powerinfer: Fast large language model serving with a consumer-grade gpu.
* Sun et al. (2024a) Sun, H., Chen, Z., Yang, X., Tian, Y., and Chen, B. (2024a). Triforce: Lossless acceleration of long sequence generation with hierarchical speculative decoding.
* Sun et al. (2024b) Sun, M., Liu, Z., Bair, A., and Kolter, J. Z. (2024b). A simple and effective pruning approach for large language models.
* Team et al. (2024) Team, G., Anil, R., et al. (2024). Gemini: A family of highly capable multimodal models.
* Tibshirani (1996) Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 58(1):267-288.
* Touvron et al. (2022) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Keker, V., Khabsa, M., Kolumann, I., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molbyboe, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. (2023). Llama 2: Open foundation and fine-tuned chat models.
* Wei et al. (2022) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. (2022). Chain-of-thought prompting elicits reasoning in large language models. _Advances in neural information processing systems_, 35:24824-24837.
* Xia et al. (2024) Xia, M., Gao, T., Zeng, Z., and Chen, D. (2024). Sheared llama: Accelerating language model pre-training via structured pruning.
* Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. (2022). Opt: Open pre-trained transformer language models.

## Appendix

### Appendix Table of Contents

* A Additional Background
* 1. Extended Related Works
* 2. Why Not Using Speculative Decoding to Correct the Sparse Model?
* B Supplemental Experiments
	* 2.1 Given Similar Parameters, More Well-trained Parameters Suffers More
	* 2.2 Hardware-Friendly Tree Building Process
	* 2.3 Ablation: Various Aspects of Sirius is Tested and Challenged
	* 2.4 Large Model Experiments
	* 2.5 Variable Sequence Length with Batch Size One
	* 2.6 Error Occurs at Which Position inside a Chunk
	* 2.7 Miscellaneous Results
	* 2.8 Llama-2 and Llama-3 Models on GSM8K-COT
	* 3.1 Additional Results on Reasoning
		* 3.2.1 Arithmetic Reasoning
	* 3.3 CommonSense Reasoning
	* 3.4 Code

## Appendix A Additional Background

### Extended Related Works

**Pruning in LLM** Sparsity in neural networks has been widely studied. In the context of LLM, sparsity is studied under two branches - unstructured and structured. On the unstructured sparsity side, Frantar and Alistarh (2023) (SparseGPT) is a ground-breaking work that formulates pruning as a solving a series of sparse regression problems and proposes a fine solver for the problem. Sun et al. (2024) (Wanda) introduces input activations into the pruning decision and achieves strong results in inducing LLM sparsity. On the structured side, LLMPruner Ma et al. (2023) and Sheared Llama Xia et al. (2024) each proposes different meticulous pruning algorithms and restoring weights through either parameter-efficient finetuning or efficient full weights training.

**Contextual Sparsity** Many recent works on LLM sparsity notice that the sparse pattern is highly related to the input or context. Deja Vu Liu et al. (2023) revealed that for OPT models Zhang et al. (2022) the contextual sparsity is as high as 85%, meaning that 80% of the parameters can be pruned that won't hurt the token decoded quality given the prompt. Deja Vu formulates the problem of neuron selection as a near-neighbor search problem: finding neurons that are the most similar to the input activations. PowerInfer Song et al. (2023) extends the contextual sparsity to benefit the heterogeneous setting. Compared to the rest of the model, MLP layers tend to possess significant contextual sparsity and can be effectively exploited in a training-free manner. Concurrently, Griffin Dong et al. (2024) discovers the phenomenon of flocking, where MLP neurons have temporal locality, where given a fixed prompt, similar neurons tend to get activated throughout the following generation. Flocking is shown to occur in most activation types and open-source LLMs. Griffin selects the same set of heated neurons with 50% sparsity throughout the generation of each input prompt, which we refer to as coarse-grained sparsity. CATS Lee et al. (2024) successfully exploits per-token contextual sparsity in the MLP layers for inference latency reduction. They resample a new set of neurons per every new input token, which we categorize it as fine-grained contextual sparsity. Our paper mainly focuses on the training-free MLP sparsity techniques. Although these recent works show minimal accuracy degradation in classification and easy text summarization tasks, they both severely degrade in generation quality under tasks that require high-level reasoning and understanding ability. Our work serves as a low-cost complementary tool, aiming to push these elegant and promising techniques for mainstream use cases.

Also, previous contextual sparsity methods haven't fully and exhaustively evaluated their benefits and limitations in downstream generation tasks. To fully study this technique, we extensively go throughopen-source LLMs in diverse performance and sizes on diverse generation tasks and datasets to locate where these sparse models maintain the performance or fail.

**Speculative Decoding** Besides model compression techniques, Speculative decoding Leviathan et al. (2023), Chen et al. (2023), Kim et al. (2023) is another important LLM inference latency reduction method. Compared to LLM, small transformer models are much more computationally accessible and can effectively model short-range tokens. Therefore, smaller models are asked to speculate short-term future tokens, which the LLM takes in in parallel to trade in FLOPs with memory loading time. During verification, most speculative decoding methods pursue lossless acceleration, leading to frequent rollback during rejection. In contrast, Sirius solves a very different problem. Our method aims to maximally preserve the efficiency of sparse models while boosting its performance. Sparse models, pruned directly from LLM, are much stronger at modeling a longer range of text than draft models, thus requiring much less help from the LLM. Our work aims to find the minimum amount of LLM overhead while boosting its performance to the LLM level. Given the resemblance and relevance of Speculative Decoding to our method Sirius, we will elaborate more in-depth on their differences and Speculative Decoding's inefficiencies when it comes to helping the Sparse method in A.2.

### Why Not Using the Speculative Decoding to Correct the Sparse Model?

When Speculative Decoding is used to correct sparse using the full model, we will show that the efficiency of the overall process will be largely limited. We followed the common practice from speculative decoding and measured the acceptance rate on different datasets C4 Raffel et al. (2020) and GSM8K Cobbe et al. (2021). Take the Coarse-grained sparse model as an example. For Llama-3-8B as the full model, the 50% sparse (APU 0.65) model will produce an acceptance rate of 0.71 on C4 and 0.89 on GSM8K. Speculative decoding also use parallel verification in the period-basis. Naturally, to keep the system efficiency high, we need to (1) enlarge the period and (2) increase the average number of tokens accepted (AAL) given the gamma (period - 1) value. Take the acceptance rate of 0.89 on GSM8K as an example, following the formulation in Leviathan et al. (2023), we can calculate the expected number of accepted tokens for every gamma term in the Speculative Decoding literature. AAL = \(\frac{1-\alpha^{(\gamma+1)}}{1-\alpha}\). The trend (green) is plotted in Figure 6

We can notice the trend that the average advance length starts to plateau as the gamma becomes larger. Take the gamma of 16 as an example, the period is then 17. The average advance length is only 7.84. The APU is (16 * 0.65 + 1)/7.84 = 1.45, which is larger than the full model 1. The blue line in Figure 6 shows the relationship between APU and gamma.

Because of the plateauing effect, for an acceptance rate of 0.89, the best gamma is 2 (period = 3). The optimal APU is 0.86, compared with 0.65 coarse-grained sparse APU. A similar picture can be applied to Fine-grained sparsity as well. The key reasons for the observation are two-fold: (1) the contextually sparse models are too big to be the draft model of the speculative decoding system and to have a large period; (2) Speculative decoding preserves the original model's performance so that the acceptance criteria are usually very strict, which is also not suitable for large period and high average advance length. Following the same spirit, Sun et al. (2024) also uses a large draft model to do self-speculation, but for them, the authors select gamma = 1 to achieve the optimal speedup of their system. **In contrast, Sirius brings <0.76 APU in this case with period \(\geq\)10.** Specifically, with a threshold of 0.1, Sirius can correct Llama-3-8B coarse-grained sparsity from 20.85% to 43.9%, compared to the 49.66% full model. With a period of 16 tokens (gamma = 15), Sirius on average can accept 13.4 tokens out of a kernel size of 16 and over 9 tokens out of a kernel size of 10, translating to APU < 0.76, significantly lower than SD does.

Figure 6: Speculative Decoding has limitation in efficiency when correcting sparse models.

Supplemental Details in Sirius observations, Design, and Experiments

In this section, we provide several supplemental experiments to the picture. First, we run Sirius on Llama-3-70B. However, because of computational limits, we cannot run Sirius with the tree on Llama-3-70B with the scale we did for other models. Nevertheless, we do show that 70B has roughly the same pattern as we have seen before, large model sparsity also somehow struggles on reasoning tasks. Second, we provide additional proof for the parallel verification efficiency statement. After that, I show results on where the error is located in the chunk size of 16 tokens. The error is distributed almost uniformly. Last but not least, we also apply Sirius on datasets that are reasoning. Lastly, we provide more results on the comparison between models of similar size but have a huge performance gap. We show that given the similar parameter size, the trend is for a more well-trained, powerful model to degrade more from contextual sparsity. We present here more illustration on contextual sparsity and Llama-2-7B-Chat experiments with fixing only a minor portion of tokens is shown in Figure 7.

### Given Similar Parameter Size Well-trained Models Suffer More

We observe another interesting phenomenon: given the similar parameter size, the more well-trained the model is, the more performance degradation contextual sparsity would make on the full models. Here we present two pairs of results. First, we look at the performance between Llama-3-8B-Instruct and Llama-2-7B-Chat with Llama-3-70B-Instruct and Llama-2-70B-Chat. All models are evaluated on GSM8K-COT. We draw these models in CSParse in Figure 8, and the readers can find more results in Appendix B.8. We can see figures from top to bottom, where even at lower density (more elements are not selected), Llama-2-7B-Chat and Llama-2-70B-Chat suffer from less performance degradation (blue) compared to the Llama-3-8B-Instruct and Llama-3-70B-Instruct models. Furthermore, suppose we focus on Llama-3-70B-Instruct for global density at 60% or lower. In that case, the performance (coral) is degraded significantly, which is comparable or even lower to Llama-3-8B-Instruct full model performance at 0.76, Even at 50% density, the 70B model still has more than 40B parameters, much more expensive than the 8B model. The observation fully manifests the difficulty of using CS in complex reasoning tasks.

### Hardware Friendly Tree Building Process

In this section, we first look at the insights behind whether building the tree can help efficiency, then we detail the specific steps towards tree pruning.

The goal for the Sirius system is to make \(n_{AAL}\) to be as large as possible. Despite the full model sharing KVs with the sparse model, Sirius still encounters costly rollbacks because of sparse greedily decoded tokens being rejected. Interestingly, we look closely into where the sparse model is likely to make a mistake on GSM8K and AQuA-RAT-COT Ling et al. (2017) with Sirius on Llama-3-8B-Instruct and a kernel size of 16. More details are shown in Appendix B.6. The error distributes almost uniformly across all positions of the kernel size. Also, when the token makes the mistake, besides the greedily decoded

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Sparsity** & \(2^{nd}\)**Hit** & \(3^{rd}\)**Hit** & **Miss** & **Coverage\%** \\ \hline
**FSparse** & 79\% & 11\% & 9\% & 90\% \\
**Csparse** & 65\% & 17\% & 16\% & 82\% \\ \hline \hline \end{tabular}
\end{table}
Table 5: The second and third most likely tokens from sparse models offer potential for boosting efficiency.

Figure 7: (a) Illustration on why Contextual Sparsity has uneven performance on different tasks. The activation heat map (red) has the brighter the color the larger in magnitude. On top, we also show the neuron sparsity selected. The graph points signify that the pattern in the prompt understanding task is easier to capture. (b) An additional graph of correcting Csparse Llama-2-7B-Chat. It is similar to the previous experiment on 8B. Only 10% tokens being corrected results in complete performance recovery.

tokens, we find that other tokens of lower likelihood offer the potential to boost efficiency. Surprisingly, we found that out of the cases where the greedily decoded tokens are rejected, the probability that the second or third most likely tokens from the sparse being accepted by the full model is reasonably high.

Shown in Table 5, we test on part of the GSM8K rejected cases. The "Second Hit" is defined as the count of the second most likely tokens being accepted by the full model when the greedily decoded token is rejected, while the "Third Hit" is defined as the count of the third most likely token being accepted when the first two are rejected. Both sparsity method has a high acceptance rate, or "Coverage", from the second and third most likely tokens when the most likely token is rejected, showing huge potential for gains in efficiency.

To capitalize the potential from the second to third tokens, we propose to build a tree during the sparse generating process (lines 6 to 11 in Algorithm 1. The tree algorithm is similar to Beam Search Freitag and Al-Onaizan (2017). However, to make sure that the tree building and tree parallel correction processes can achieve speedup over cases that don't build trees, we impose strong restrictions on the tree structure we build. For a fixed kernel size, we limit every step to having a fixed number of leaves, or treewidth, through tree pruning based on ranking the cumulative log-likelihood of the path. The resulting tree has a fixed shape for a given kernel size and tree width, but only the interconnection pattern between steps varies based on the pruning and ranking within each step. The details are illustrated in Figure 9. During verification, out of the treewidth complete paths, we select the one that reaches the longest advance length. In practice, we found that for kernel size 16, when the treewidth is increased to

Figure 8: Given the similar model parameters, the more well-trained the model is, the worse the degradation would be. (Compare the figures vertically between Llama-3 and Llama-2 family models).

Figure 9: Illustration of Tree Building Process.

8, the optimal verification tree is around 64. From Section A.2, we see that the parallel verification of the tree of 64 roughly equals the time the full input 1 token.

Therefore, a treewidth of 8 is set as the maximum treewidth when building the tree for kernel size 16 for later. We show that building a tree makes the system significantly more efficient while retaining correction effects.

In Table 6, we present more details complementing Table 3 on the use of tree-building for A100 and H100. We ablate the use of a width 4 tree by not using the tree under kernel size 16. We observe that the speedup improvements solely from the increase in AAL from 13.67 to 15.01 for kernel size 16.

### Ablation: Various Aspects of Sirius Are Tested and Challenged

**Probing Components** To understand the contribution and the utility of each component of Sirius, we ablate all components of Sirius in Table 7. We started by only letting the LLM correct the token it is evaluating (interleaving only). Then, we add on top of it the KV cache correction, and then the rollback. All these three techniques are effective when applied solely. Rollback seems to be the most effective technique. Even when applied alone, rollback asserts significant correction to both the CSparse and FSparse models. Interestingly, KV Cache is also effective alone, bringing a 12% increment for CSparse and an 11% accuracy increase for FSparse. Relatively, interleaving is the weakest. Surprisingly, adding both KV rewriting and rollback is only marginally better than rollback alone. Although it is tempting to think KV Cache rewriting is not useful with rollback, the improvement KV Cache Rewriting brings is a gain in efficiency. When adding the KV Cache Rewriting on top of Roll Back and interleave it significantly improves the efficiency of the correction. For CSparse, adding KV rewrite increases AAL from 12.77 to 13.80.

**Likelihood threshold to balance Correction and Efficiency** We found that the likelihood threshold is important for managing the Sirius correction and efficiency tradeoff. We present results in Table 8. We ablate this setting on a 30% subsampled GSM8K dataset, and only strict accuracy is reported. The

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c} \hline \hline
**Settings** & **ACC** & **A100** & **Ratio** & **H100** & **Ratio** \\ \hline
**CSparse** & 0.3601 & 9.6 & 0.72 & 6.6 & 0.76 \\ \hline
**Sirius** (No Tree) & 0.7127 & 11.8 & 0.88 & 8.2 & 0.95 \\ \hline
**Sirius** (With Tree) & 0.7089 & 11.1 & 0.83 & 7.7 & 0.88 \\ \hline
**Full** & 0.7612 & 13.3 & 1.0 & 8.6 & 1.0 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Performance and Speedup Ratios on GSM8K-COT with tree building, latency measurement in millisecond.

\begin{table}
\begin{tabular}{l|c|l|l} \hline \hline
**CSparse** & **GSM8K 20\%** & **FSparse** & **GSM8K 20\%** \\ \hline
**Llama3-8B-Instruct** & 0.7538/0.7538 & **Llama3-8B-Instruct** & 0.7538/0.7538 \\ \hline + CSparse & 0.3674/0.3674 & + FSparse & 0.5644/0.5644 \\ \hline + CSparse + Interleave & 0.3826/0.3826 & + FSparse + Interleave & 0.6288/0.6288 \\ \hline + CSparse + KV Rewrite & 0.4735/0.4735 & + FSparse + KV Rewrite & 0.6629/0.6629 \\ \hline + Csparse + KV Rewrite & 0.4886/0.4886 & + FSparse + KV Rewrite & 0.6780/0.6818 \\ \hline + Interleave & & + Interleave & 0.6780/0.6818 \\ \hline + CSparse + Roll back & & + FSparse + Roll back & \\ + Interleave & & 0.6591/0.6591 & + Interleave & 0.7273/0.7273 \\ \hline + CSparse + KV Rewrite & & + FSparse + KV Rewrite & \\ + Interleave + Rollback & & + Interleave + Rollback & 0.7273/0.7311 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Ablation on Components in Sirius.

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c|c} \hline \hline
**Threshold** & **Full** & **Sparse** & **0.05** & **0.1** & **0.3** & **0.5** & **0.7** & **0.9** \\ \hline
**Accuracy** & 0.7803 & 0.5884 & 0.7247 & 0.7399 & 0.7399 & 0.7677 & 0.7702 & 0.7652 \\ \hline
**AAL** & N/A & N/A & 15.2 & 14.6 & 11.6 & 8.5 & 6.2 & 4.2 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Ablation on the threshold for correction (FSparse Llama-3-8B-Instruct).

performance is the score, while the efficiency is measured by Average Advance Length (AAL). We can find that with the increase of threshold, the scores generally improve, while the efficiency metric decreases.

**Building Wider Tree** We study the effect of increasing the treewidth. In fact, for every number from Sirius in Table 2, we are selecting from a group of results by different treewidth. We present all of this treewidth and its corresponding accuracy and efficiency numbers in the Appendix C. _Importantly, raising treewidth always improves AAL._ Although different choices of treewidth usually give similar accuracy scores, there is hardly a pattern on which treewidth always gives the best accuracy. The optimal treewidth can only be found through empirical studies.

### Large Model Experiments

To diversify the evaluation of Sirius, we also evaluate Sirius's Effectiveness on the Llama-3-70B-Instruct model. MMLU is subsampled 10%, while CNN/DailyMail is subsampled 30%. The following table contrasts with Llama-3-8B-Instruct. We use strict match/flexible extract accuracy for GSM-8K-COT, accuracy for MMLU, F1/EM score for CoQA, Rouge-1/2/L score for CNN/DailyMail, and Rouge-1/2 ACC for TruthfulQA.

### Variable Sequence Length with Batch Size One

Here we show the benchmark latency on A100, where the input tensor to Llama-3-8B-Instruct has a shape of batch size 1 and a different input sequence length. To get the hardware optimal readings, we use torch compile to compile the whole forward pass of the model. We show that the latency only goes up insignificantly to 64, but the trend of increment to 96 is a bit steep.

### Error Occurs At Which Position inside a Chunk

We look at the distribution of where the error would be inside a kernel of 16 tokens. We run through Sirius with a kernel size of 16 on the entire GSM-8K and AQuA-RAT-COT dataset. The histogram is shown in Figure 10. We found that the error occurs in a uniform pattern, where it is hard to see any particular region where the tokens are likely to occur the most.

### Miscellaneous Results

Besides, the results on the complex reasoning tasks, we evaluate Sirius on slightly more diverse tasks in Table 12.

\begin{table}
\begin{tabular}{c|c} \hline \hline
**Input Sequence Length** & **A100 Latency (ms)** \\ \hline
1 & 0.0133 \\ \hline
2 & 0.0135 \\ \hline
4 & 0.0136 \\ \hline
8 & 0.0138 \\ \hline
16 & 0.0140 \\ \hline
32 & 0.0149 \\ \hline
64 & 0.0144 \\ \hline
96 & 0.0171 \\ \hline \hline \end{tabular}
\end{table}
Table 10: A100 Latency versus Input Sequence Length.

\begin{table}
\begin{tabular}{l|l|l|l|l|l|l} \hline \hline  & **GSM-8K-COT** & **MMLU** & **CoQA** & **CNN/DailyMail** & **TruthfulQA** \\ \hline
**Llama-3-70B-In** & 0.9014/0.9022 & 0.7456 & 0.6567/0.8069 & 0.1016/0.0206/0.0964 & 0.5116/0.4247 \\ \hline
**+ CSparse** & 0.7407/0.7483 & 0.7018 & 0.6497/0.8046 & 0.1019/0.0208/0.0967 & 0.4541/0.3807 \\ \hline
**+ FSparse** & 0.8726/0.8772 & 0.7193 & 0.6497/0.8035 & 0.1015/0.0206/0.0963 & 0.4835/0.3905 \\ \hline
**Llama-3-8B-In** & 0.7612/0.7672 & 0.6272 & 0.6153/0.7825 & 0.1015/0.2044/0.0963 & 0.4945/0.3647 \\ \hline
**+ CSparse** & 0.3601/0.3647 & 0.5307 & 0.6003/0.7735 & 0.1016/0.0206/0.0964 & 0.5067/0.3953 \\ \hline
**+ FSparse** & 0.6103/0.6202 & 0.4825 & 0.5828/0.7577 & 0.1017/0.0204/0.0965 & 0.5202/0.3941 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Large model results on miscellaneous datasets.

### Llama-2 and Llama-3 Models on GSM8K-COT

Here we present more experiments for the comparison between Llama-2 and Llama-3 family models, which is first mentioned in Section B.1, where we also include FSparse methods together with the CSparse method. The results are in Table 11.

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|} \hline
**Llama-3-70B-Instruct** & **Accuracy** & **Degradation** & **Llama-3-8B-Instruct** & **Accuracy** & **Degradation** \\ \hline
**Full** & 0.9205 & & **Full** & 0.7462 & \\ \hline
**Csparse 60\%** & 0.8144 & 0.1061 & & & \\ \hline
**Csparse 50\%** & 0.7652 & 0.1553 & **Csparse 50\%** & 0.3636 & 0.3826 \\ \hline
**Csparse 40\%** & 0.6023 & 0.3182 & **Csparse 40\%** & 0.1856 & 0.5606 \\ \hline
**Csparse 30\%** & 0.3144 & 0.6061 & **Csparse 30\%** & 0.0644 & 0.6818 \\ \hline
**Fsparse 50\%** & 0.8864 & 0.0341 & **Fsparse 50\%** & 0.6477 & 0.0985 \\ \hline
**Fsparse 40\%** & 0.8485 & 0.0720 & **Fsparse 40\%** & 0.4053 & 0.3409 \\ \hline
**Fsparse 30\%** & 0.7386 & 0.1819 & **Fsparse 30\%** & 0.0265 & 0.7197 \\ \hline
**Fsparse 20\%** & 0.2803 & 0.6402 & & & \\ \hline
**Llama-2-70B-Chat** & **Accuracy** & **Degradation** & **Llama-2-7B-Chat** & **Accuracy** & **Degradation** \\ \hline
**Full** & 0.4508 & & **Full** & 0.1856 & \\ \hline
**Csparse 50\%** & 0.3939 & 0.0569 & **Csparse 50\%** & 0.1515 & 0.0341 \\ \hline
**Csparse 40\%** & 0.3447 & 0.1061 & **Csparse 40\%** & 0.1098 & 0.0758 \\ \hline
**Csparse 30\%** & 0.2689 & 0.1819 & **Csparse 30\%** & 0.0720 & 0.1136 \\ \hline
**Fsparse 50\%** & 0.3864 & 0.0644 & **Fsparse 50\%** & 0.1629 & 0.0227 \\ \hline
**Fsparse 40\%** & 0.3902 & 0.0606 & **Fsparse 40\%** & 0.1364 & 0.0492 \\ \hline
**Fsparse 30\%** & 0.2689 & 0.1819 & **Fsparse 30\%** & 0.1212 & 0.0644 \\ \hline \hline \end{tabular}
\end{table}
Table 11: Detail on Llama-2 and Llama-3 family models with CS.

Figure 10: We look at the histogram of the number of errors versus the position among a period of sixteen tokens on average. We have two different datasets of Arithmetic Reasoning GSM-8K and AQuA-RAT-COT. We can see that the number of errors is distributed almost evenly for both datasets.

## Appendix C Additional Results on Reasoning

Due to page restrictions, we only show GSM8K, CSQA, and HumanEval in the paper. Below we show additional results to the numbers presented in the paper. **We present tables of a similar format. Please notice that the leftmost column writes a number that represents the treewidth in the given settings.** Also, we show the results of Sirius on the other five datasets AQuA-RAT-COT (Arithmetic Reasoning), Sports (Commonsense Reasoning), Date (Commonsense Reasoning), and StrategyQA (CommonSense Reasoning), and MBPP+ (coding).

### Arithmetic Reasoning

In this section, we present GSM8K and AQuA RAT COT evaluation results with the efficiency metric AAL. Sirius is shown to be effective on these two reasoning tasks about arithmetic. Below we show the raw AAL score associated with efficiency for all models and the performance of different treewidths.

### CommonSense Reasoning

We followed the COT paper and evaluated Sirius on CSQA, Sports, StrategyQA, and Dates. Sparse methods are capable of outputting high-quality output similar to the full model at the 0.5 mark, which is different than on other datasets. However, we tune the sparsity level to 0.4 (0.6 dense, 0.4 removed), and it starts to have performance degradation. Sirius can compensate them with relatively high efficiency)

### Code

We also have a coding portion that evaluates Sirius on HumanEval. Sirius performs well similar to other datasets. Besides, we also have results on MBPP+. The results show Sirius effectiveness and efficiency again.

\begin{table}
\begin{tabular}{l|c|c|c} \hline \hline
**Experiment Setting** & **CoQA** & **AGIEval (Math)** & **MMLU-FLAN-COT** \\ \hline Llama-2-7B-Chat & 0.5982/0.7580 & 0.072 & 0.4925 \\ Llama-2-7B-Chat-FSparse & 0.5898/0.7540 & 0.077 & 0.4768 \\ Llama-2-7B-Chat-FSparse-Sirius & 0.5908/0.7540 & 0.081 & 0.4670 \\ Llama-2-7B-Chat-CSparse & 0.6117/0.7639 & 0.065 & 0.4637 \\ Llama-2-7B-Chat-CSparse-Sirius & 0.6117/0.7664 & 0.078 & 0.4794 \\ \hline Llama-3-8B-Instruct & 0.6153/0.7825 & 0.213 & 0.6231 \\ Llama-3-8B-Instruct-FSparse & 0.5828/0.7577 & 0.172 & 0.5304 \\ Llama-3-8B-Instruct-FSparse-Sirius & 0.5868/0.7591 & 0.196 & 0.5709 \\ Llama-3-8B-Instruct-CSparse & 0.6003/0.7735 & 0.154 & 0.5558 \\ Llama-3-8B-Instruct-CSparse-Sirius & 0.6005/0.7728 & 0.178 & 0.6003 \\ \hline Llama-2-13B-Chat & 0.6408/0.7896 & 0.092 & 0.5317 \\ Llama-2-13B-Chat-FSparse & 0.6320/0.7837 & 0.087 & 0.5082 \\ Llama-2-13B-Chat-FSparse-Sirius & 0.6340/0.7859 & 0.089 & 0.5219 \\ Llama-2-13B-Chat-CSparse & 0.6350/0.7841 & 0.088 & 0.5127 \\ Llama-2-13B-Chat-CSparse-Sirius & 0.6363/0.7847 & 0.1 & 0.5127 \\ \hline Llama-2-7B & 0.6388/0.7735 & 0.101 & 0.4520 \\ Llama-2-7B-FSparse & 0.6352/0.7697 & 0.09 & 0.4435 \\ Llama-2-7B-FSparse-Sirius & 0.6352/0.7697 & 0.092 & 0.4415 \\ Llama-2-7B-CSparse & 0.6338/0.7700 & 0.086 & 0.4213 \\ Llama-2-7B-CSparse-Sirius & 0.6372/0.7709 & 0.093 & 0.4317 \\ \hline Llama-3-8B & 0.6727/0.8055 & 0.163 & 0.5754 \\ Llama-3-8B-FSparse & 0.6625/0.7984 & 0.152 & 0.5349 \\ Llama-3-8B-FSparse-Sirius & 0.6625/0.7984 & 0.154 & 0.5532 \\ Llama-3-8B-CSparse & 0.6633/0.7977 & 0.131 & 0.5049 \\ Llama-3-8B-CSparse-Sirius & 0.6670/0.7995 & 0.15 & 0.5428 \\ \hline \hline \end{tabular}
\end{table}
Table 12: Miscellaneous Results: 5 models on different Three Different datasets.

[MISSING_PAGE_FAIL:21]

[MISSING_PAGE_FAIL:22]

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline
**Experiment Settings** & \multicolumn{2}{c|}{**Llama-3-8B-Instruct-FSparse**} & \multicolumn{2}{c|}{**Llama-3-8B-Instruct-CSparse**} \\ \cline{2-5}
**treewidth** & **Performance** & **AAL (out of 16)** & **Performance** & **AAL (out of 16)** \\ \hline Original Performance & 0.707341 & N/A & 0.707341 & N/A \\ \hline Sparse Performance & 0.615889 & N/A & 0.647011 & N/A \\ \hline
1 & 0.699427 & 12.2108 & 0.724816 & 11.0512 \\ \hline
4 & 0.687961 & 13.2734 & 0.709255 & 13.5876 \\ \hline
6 & 0.714169 & 13.7842 & 0.720721 & 13.3097 \\ \hline
8 & 0.710893 & 14.1173 & 0.707617 & 14.76893 \\ \hline  & \multicolumn{2}{c|}{**Llama-3-8B-FSparse**} & \multicolumn{2}{c|}{**Llama-3-8B-CSparse**} \\ \cline{2-5}  & **Performance** & **AAL (out of 16)** & **Performance** & **AAL (out of 16)** \\ \hline Original Performance & 0.643735 & N/A & 0.643735 & N/A \\ \hline Sparse Performance & 0.53317 & N/A & 0.558559 & N/A \\ \hline
1 & 0.638821 & 15.0088 & 0.618346 & 12.7426 \\ \hline
4 & 0.630631 & 14.6151 & 0.63964 & 14.8704 \\ \hline
6 & 0.625717 & 14.9905 & 0.640459 & 15.1968 \\ \hline
8 & 0.617527 & 15.2534 & 0.642916 & 15.4355 \\ \hline  & \multicolumn{2}{c|}{**Llama-2-13B-Chat-FSparse**} & \multicolumn{2}{c|}{**Llama-2-13B-Chat-CSparse**} \\ \cline{2-5}  & **Performance** & **AAL (out of 16)** & **Performance** & **AAL (out of 12)** \\ \hline Original Performance & 0.687961 & N/A & 0.687961 & N/A \\ \hline Sparse Performance & 0.53317 & N/A & 0.553645 & N/A \\ \hline
1 & 0.657658 & 13.0868 & 0.649468 & 9.2183 \\ \hline
4 & 0.669124 & 14.309 & 0.669124

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline
**Experiment Settings** & \multicolumn{2}{c|}{**Llama-3-BB-Instruct-FSparse**} & \multicolumn{2}{c|}{**Llama-3-8B-Instruct-CSparse**} \\ \cline{2-5}
**treewidth** & **Performance** & **AAL (out of 16)** & **Performance** & **AAL (out of 16)** \\ \hline Original Performance & 0.943299 & N/A & 0.943299 & N/A \\ \hline Sparse Performance & 0.864948 & N/A & 0.879381 & N/A \\ \hline
1 & 0.937113 & 12.3652 & 0.946392 & 9.95237 \\ \hline
4 & 0.941237 & 14.5248 & 0.943299 & 11.5858 \\ \hline
6 & 0.942268 & 14.8651 & 0.943299 & 14.0954 \\ \hline
8 & 0.939175 & 14.9832 & 0.941237 & 14.7718 \\ \hline  & \multicolumn{2}{c|}{**Llama-3-8B-FSparse**} & \multicolumn{2}{c|}{**Llama-3-8B-CSparse**} \\ \cline{2-5}  & **Performance** & **AAL (out of 16)** & **Performance** & **AAL (out of 16)** \\ \hline Original Performance & 0.898969 & N/A & 0.898969 & N/A \\ \hline Sparse Performance & 0.748454 & N/A & 0.720619 & N/A \\ \hline
1 & 0.86653 & 15.5259 & 0.845361 & 13.5897 \\ \hline
4 & 0.849485 & 15.5917 & 0.847423 & 15.2325 \\ \hline
6 & 0.863918 & 15.5256 & 0.843299 & 15.4376 \\ \hline
8 & 0.869072 & 15.6014 & 0.841237 & 15.5023 \\ \hline  & \multicolumn{2}{c|}{**Llama-2-13B-Chat-FSparse**} & \multicolumn{2}{c|}{**Llama-2-13B-Chat-CSparse**} \\ \cline{2-5}  & **Performance** & **AAL (out of 16)** & **Performance** & **AAL (out of 12)** \\ \hline Original Performance & 0.742268 & N/A & 0.742268 & N/A \\ \hline Sparse Performance & 0.690722 & N/A & 0.584536 & N/A \\ \hline
1 & 0.710309 & 13.9767 & 0.717659 & 7.72686 \\ \hline
4 & 0.735052 & 14.9247 & 0.7289

[MISSING_PAGE_FAIL:25]

[MISSING_PAGE_FAIL:26]

[MISSING_PAGE_FAIL:27]

[MISSING_PAGE_FAIL:28]

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA]  means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes]  to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

* Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Guidelines:
* The answer NA means that the abstract and introduction do not include the claims made in the paper.
* The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.
* The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.
* It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
* **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [No] Justification: [TODO]Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
* **Theory Assumptions and Proofs*
* Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
* **Experimental Result Reproducibility*
* Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Guidelines:
* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
* **Open access to data and code*
* Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: Guidelines:
* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
F. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
* **Experiment Statistical Significance*
* Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Guidelines:
* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
* **Experiments Compute Resources*
* Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Guidelines:
* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).

1. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
2. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
3. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper poses no such risks.

* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licensees for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification:Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.