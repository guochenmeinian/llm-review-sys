# Curvature Filtrations for Graph Generative Model Evaluation

Joshua Southern

Imperial College London

jks17@ic.ac.uk

&Jeremy Wayland

Helmholtz Munich & Technical University of Munich

jeremy.wayland@tum.de

&Michael Bronstein

University of Oxford

michael.bronstein@cs.ox.ac.uk

These authors contributed equally.

&Bastian Rieck

Helmholtz Munich & Technical University of Munich

bastian.rieck@tum.de

These authors jointly directed the work.

###### Abstract

Graph generative model evaluation necessitates understanding differences between graphs on the distributional level. This entails being able to harness salient attributes of graphs in an efficient manner. Curvature constitutes one such property that has recently proved its utility in characterising graphs. Its expressive properties, stability, and practical utility in model evaluation remain largely unexplored, however. We combine graph curvature descriptors with emerging methods from topological data analysis to obtain robust, expressive descriptors for evaluating graph generative models.

## 1 Introduction

Graph-structured data are prevalent in a number of different domains, including social networks [48], bioinformatics [38, 59], and transportation [33]. The ability to generate new graphs from a distribution is a burgeoning technology with promising applications in molecule or protein design [34, 35], as well as circuit design [47]. To compare graph generative models and advance research in this area, it is essential to have a metric that can measure the distance between a set of generated and reference graphs, with enough _expressivity_ to critically evaluate model performance. Typically, this is done by using a set of descriptor functions, which map a graph to a high-dimensional representation in \(\mathds{R}^{d}\). An evaluator function, such as the _maximum mean discrepancy_[27, MMD], may then be used to get a distance between two distributions of graphs by comparing their vectorial representations [44, 53]. This state of the art was recently critiqued by O'Bray et al. [54] since it (i) requires numerous parameter and function choices, (ii) is limited by the _expressivity_ of the descriptor function, and (iii) does not come equipped with _stability guarantees_.

We propose to overcome these issues through topological data analysis (TDA), which is capable of capturing multi-scale features of graphs while being more expressive than simple descriptor functions. TDA is built on the existence of a function of the form \(f\colon V\to\mathds{R}\), or \(f\colon E\to\mathds{R}\) on a graph \(G=(V,E)\). This is used to obtain a _filtration_, i.e. an ordering of subgraphs, resulting in a set of topological descriptors, the _persistence diagrams_. Motivated by their expressive power and computational efficiency, we use recent notions of discrete curvature [19, 24, 55] to define filtrations and calculate _persistence landscapes_[10] from the persistence diagrams, thus obtaining a descriptor whose Banach space formulation permits statistical calculations. Our proposed method comes equipped with stability guarantees, can count certain substructures and measure important graph characteristics, and can be used to evaluate a variety of statistical tests since it permits computing distances between graph distributions.

Our **contributions** are as follows:

* We provide a thorough theoretical analysis of the stability and expressivity of recent notions of discrete curvature, showing their fundamental utility for graph learning applications.
* Using discrete curvature and TDA, we develop a new metric for graph generative model evaluation.
* Our experiments reveal our metric is robust and expressive, thus improving upon current approaches that use simple graph descriptor and evaluator functions.

## 2 Background

The topological descriptors used in this paper are based on _computational topology_ and _discrete curvature_. We give an overview of these areas and briefly comment on previous work that makes use of _graph statistics_ in combination with MMD. In the following, we consider undirected graphs, denoted by \(G=(V,E)\), with a set of vertices \(V\) and a set of edges \(E\subseteq V\times V\).

### MMD and Metrics Based On Graph Statistics

MMD is a powerful method for comparing different distributions of structured objects. It employs _kernel functions_, i.e. positive definite similarity functions, and can thus be directly combined with standard graph kernels [8] for graph distribution comparison. However, there are subtle issues when calculating kernels in \(\mathds{R}^{d}\): Gaussian kernels on geodesic metric spaces, for instance, have limited applicability when spaces have non-zero curvature [23], and certain kernels in the literature are indefinite, thus violating one of the tenets of MMD [54]. Despite these shortcomings, MMD is commonly used to evaluate graph generative models [17; 71]. This is accomplished by extracting a feature vector from each graph, such as the clustering coefficient or node degree, and subsequently calculating empirical MMD values between generated and reference samples. Some works [49] combine multiple structural properties of graphs into a single metric through the Kolmorogov-Smirnov (KS) multidimensional distance [39]. Combining graph statistics into a single measure has also led to metrics between molecular graphs, such as the quantitative estimate of drug-likeness (QED), which is a common measure in drug discovery [5]. However, these simple statistics, even when considered jointly, often lack expressivity, have no stability guarantees, and their use with MMD requires numerous parameter and function choices [54].

### Computational Topology

Computational topology assigns _invariants_--characteristic properties that remain unchanged under certain transformations--to topological spaces. For graphs, the simplest invariants are given by the \(0\)-dimensional (\(\beta_{0}\)) and \(1\)-dimensional (\(\beta_{1}\)) Betti numbers. These correspond to the number of connected components and number of cycles, respectively, and can be computed efficiently. Their limited expressivity can be substantially increased when paired with a scalar-valued _filtration function_\(f\colon E\to\mathds{R}\).3 Since \(f\) can only attain a finite number of values \(a_{0},a_{1},a_{2},\ldots\) on the graph, this permits calculating a _graph filtration_\(\emptyset\subseteq G_{0}\subseteq G_{1}\ldots\subseteq G_{k-1}\subseteq G_{k }=G\), where each \(G_{i}:=(V_{i},E_{i})\), with \(E_{i}:=\{e\in E\mid f(e)\leq a_{i}\}\) and \(V_{i}:=\{v\in V\mid\exists e\in E_{i}\text{ s.t. }v\in e\}\). This _sublevel set filtration4_ permits tracking topological features, such as cycles, via _persistent homology_[21]. If a topological feature appears for the first time in \(G_{i}\) and disappears in \(G_{j}\), we represent the feature as a tuple \((a_{i},a_{j})\), which we can collect in a _persistence diagram_\(D\). Persistent homology thus tracks changes in connected components and cycles over the complete filtration, measured using a filtration function \(f\). Persistence diagrams form a metric space, with the distance between them given by the _bottleneck distance_, defined as \(d_{B}(D,D^{\prime}):=(\inf_{\eta\colon D\to D^{\prime}}\sup_{x\in D}\|x-\eta(x )\|_{\infty})\), where \(\eta\) ranges over all bijections between the two diagrams. A seminal _stability theorem_[14] states that the bottleneck distance between persistence diagrams \(D_{f},D_{g}\), generated from two functions \(f\) and \(g\) on the same graph, is upper-bounded by \(d_{B}(D_{f},D_{g})\leq||f-g||_{\infty}\). The infinity norm of the functions, a geometrical quantity, hence limits the topological variation. In practice, we convert persistence diagrams to an equivalent representation, the _persistence landscape_[10], which is more amenable to statistical analyses and the integration into machine learning pipelines.

Advantageous Properties.Persistent homology satisfies expressivity and stability properties. The choice of filtration \(f\) affects expressivity: with the right filtration, persistent homology can be _more_ expressive than the \(1\)-WL test [31; 57], which is commonly used to bound the expressivity of graph neural networks [50; 52; 70]. Thus, given a suitable filtration, we can create a robust and expressive metric for comparing graphs. Moreover, as we will later see, TDA improves the expressivity of _any_\(f\), meaning that even if the function on its own is not capable of distinguishing between different graphs, using it in a filtration context can overcome these deficiencies.

### Discrete Curvature

While filtrations can be learnt [29; 31], in the absence of a well-defined learning task for graph generative model evaluation, we opt instead to employ existing functions that exhibit suitable expressivity properties. Of particular interest are functions based on _discrete curvature_, which was shown to be an expressive feature for graph and molecular learning tasks [65; 68; 69]. Curvature is a fundamental concept in differential geometry and topology, making it possible to distinguish between different types of manifolds. There are a variety of different curvature formulations with varying properties, with _Ricci curvature_ being one of the most prominent. Roughly speaking, Ricci curvature is based on measuring the differences in the growth of volumes in a space as compared to a model Euclidean space. While originally requiring a smooth setting, recent work started exploring how to formulate a theory of Ricci curvature in the discrete setting [16; 19; 24; 45; 55; 62]. Intuitively, discrete curvature measures quantify a notion of similarity between node neighbourhoods, the discrete concept corresponding to 'volume.' They tend to be _larger_ for structures where there are overlapping neighbourhoods such as cliques, _smaller_ (or zero) for grids and _lowest_ (or negative) for tree-like structures. Ricci curvature for graphs provides us with sophisticated tools to analyse the neighbourhood of an edge and recent works have shown the benefits of using some of these curvature-based methods in combination with Graph Neural Networks (GNNs) to boost performance [65], assess differences between real-world networks [61], or enable _graph rewiring_ to reduce over-squashing in GNNs [64]. However, the representational power and stability properties of these measures remain largely unexplored. Subsequently, we will focus on three different types of curvature, (i) Forman-Ricci curvature [24], (ii) Ollivier-Ricci curvature [55], and (iii) Resistance curvature [19]. We find these three notions to be prototypical of discrete curvature measures, increasing in complexity and in their ability to capture _global_ properties of a graph.

Forman-Ricci Curvature.The _Forman(-Ricci) curvature_ for an edge \((i,j)\in E\) is defined as

\[\kappa_{\text{FR}}(i,j):=4-d_{i}-d_{j}+3|\#_{\Delta}|, \tag{1}\]

where \(d_{i}\) is the degree of node \(i\) and \(|\#_{\Delta}|\) is the number of \(3\)-cycles (i.e. triangles) containing the adjacent nodes.

Ollivier-Ricci Curvature.Ollivier introduced a notion of curvature for metric spaces that measures the Wasserstein distance between Markov chains, i.e. random walks, defined on two nodes [55]. Let \(G\) be a graph with some metric \(d_{G}\), and \(\mu_{v}\) be a probability measure on G for node \(v\in V\). The _Ollivier-Ricci curvature_ of _any_5 pair \(i,j\in V\times V\) with \(i\neq j\) is then defined as

Footnote 5: In contrast to other notions of curvature, Ollivier–Ricci curvature is defined for _both_ edges and non-edges.

\[\kappa_{\text{OR}}(i,j):=1-\frac{1}{d_{G}(i,j)}W_{1}(\mu_{i},\mu_{j}), \tag{2}\]

where \(W_{1}\) refers to the first _Wasserstein distance_ between \(\mu_{i}\), \(\mu_{j}\). Eq. (2) defines the Ollivier-Ricci (OR) curvature in a general setting outlined by Hoorn et al. [30]; this is in contrast to the majority of previous works in the graph setting which specify \(d_{G}\) to be the shortest-path distance and \(\mu_{i},\mu_{j}\) to be uniform probability measures in the \(1\)-hop neighbourhood of the node. Extending the probability measures so that they act on larger locality scales is known to be beneficial for characterising graphs [4; 36; 26]. We assume this general setting to define different notions of OR curvature on the graph, permitting us the flexibility of altering the probability measure and the metric.

Resistance Curvature.The resistance curvature for edges of a graph, as established in Devriendt and Lambiotte [19], is inspired by Ohm's Law and the concept of effective resistance, a well-studied, global metric between nodes in a weighted graph that quantifies the resistance exerted by the network when current flows between nodes. For a graph \(G=(V,E)\), let \(R_{ij}\) be the resistance distance between nodes \(i,j\in V\), defined in Eq.9. The _node resistance curvature_ of a node \(i\in V\) is then defined as \(p_{i}:=1-\frac{1}{2}\sum_{j\sim i}R_{ji}\). The curvature of an edge \((i,j)\in E\), what we refer to as _resistance curvature_, is then defined as

\[\kappa_{\text{R}}(i,j):=\frac{2(p_{i}+p_{j})}{R_{ij}} \tag{3}\]

The resistance curvature of an edge is related to the average distance between the neighbourhoods of the nodes connected by the edge.

## 3 Our Method

Each notion of discrete curvature yields a scalar-valued function on the edges of a graph. We use these functions to define a family of _curvature filtrations_ based on sublevel sets. In combination with persistent homology, this enables us to assess the structural properties of a given graph at multiple scales, measured via curvature. Using metrics on aggregated topological signatures--here, in the form of _persistence landscapes_--we may then compare two distributions of graphs. Specifically, we propose the following scheme for graph generative model evaluation:

1. Given a specific curvature filtration, we generate a set of _persistence diagrams_ that encode the persistent homology in dimensions \(0\) and \(1\) for each graph in the distribution. Each diagram tracks the lifespan of connected components and cycles as they appear in the filtration of a given graph, resulting in a multi-scale summary of the graph's structure.
2. To permit an analysis on the distributional level, we convert each diagram into a more suitable representation, namely a _persistence landscape_. As functional summaries of topological information, persistence landscapes allow for easy calculation of averages and distances [10].
3. Finally, we conduct statistical comparisons between graph distributions, e.g. permutation tests using \(p\)-norms, in this latent space, providing an _expressive_ metric for evaluating generative models.

Figure1 illustrates our proposed pipeline using Forman-Ricci curvature. For choosing a notion of curvature in practice, implementation details, and computational performance we refer the reader to Section3.3, AppendixA, and AppendixG respectively. We also make our framework publicly available.6

Footnote 6: Source code is available at [https://github.com/aidos-lab/CFGMME](https://github.com/aidos-lab/CFGMME).

### Stability

We first discuss the general stability of topological calculations, proving that changes in topological dissimilarity are bounded by changes in the filtration functions. Moreover, we show that filtrations based on discrete curvature satisfy stability properties if the underlying graph is modified.

Topological Features.Using the persistent homology stability theorem [14], we know that if two curvature filtrations are similar on a graph, their persistence diagrams will also be similar. However, the theorem only holds for two different functions on the _same_ graph, whereas the distributional case has not yet been addressed by the literature. Our theorem provides a (coarse) upper bound.

Figure 1: An overview of our pipeline for evaluating graph generative models using discrete curvature. We depict a graph’s edges being coloured by \(\kappa_{\text{FR}}\), as described in Eq.1.The ordering on edges gives rise to a _curvature filtration_, followed by a corresponding persistence diagram and landscape. For graph generative models, we select a curvature, apply this framework element-wise, and evaluate the similarity of the generated and reference distributions by comparing their average landscapes.

**Theorem 1**.: _Given graphs \(F=(V_{F},E_{F})\) and \(G=(V_{G},E_{G})\) with filtration functions \(f,g\), and corresponding persistence diagrams \(D_{f},D_{g}\), we have \(d_{B}(D_{f},D_{g})\leq\max\{\operatorname{dis}(f,g),\operatorname{dis}(g,f)\}\), where \(\operatorname{dis}(f,g):=|\max_{x\in E_{F}}f(x)-\min_{y\in E_{G}}g(y)|\) and vice versa for \(\operatorname{dis}(g,f)\)._

This upper bound implies that changes on the level of persistence diagrams are bounded by changes between the input functions. The stability of our method thus hinges on the stability of the filtration functions, so we need to understand the behaviour of curvature under perturbations. Following previous work [54], we aim to understand and quantify the stability of our method in response to adding and deleting edges in the graph. Our stability theorems establish bounds on various discrete curvature measures for _finite_, _unweighted_, _connected_ graphs in response to these perturbations. We restrict the outcome of a perturbation to graphs of the form \(G^{\prime}=(V,E^{\prime})\) that satisfy \(|E|\neq|E^{\prime}|\) and do not change the number of connected components of a graph. Our theoretical results bound the new curvature \(\kappa^{\prime}\) according to the structural properties of \(G\). For an exhaustive list of theorems and proofs, as well as experiments analysing the change in curvature for perturbed Erdos-Renyi (ER) graphs, see Appendix B and Appendix D.

Forman-Ricci Curvature.We first analyse Forman-Ricci curvature \(\kappa_{\text{FR}}\), and prove that it is stable with respect to adding and deleting edges.

**Theorem 2**.: _If \(G^{\prime}\) is the graph generated by **edge addition**, then the updated Forman curvature \(\kappa^{\prime}_{\text{FR}}\) for pre-existing edges \((i,j)\in E\) can be bounded by \(\kappa_{\text{FR}}(i,j)-1\leq\kappa^{\prime}_{\text{FR}}(i,j)\leq\kappa_{\text {FR}}(i,j)+2\). If \(G^{\prime}\) is the graph generated by **edge deletion**, then the updated Forman curvature \(\kappa^{\prime}_{\text{FR}}\) for pre-existing edges \((i,j)\in E\) can be bounded by \(\kappa_{\text{FR}}(i,j)-2\leq\kappa^{\prime}_{\text{FR}}(i,j)\leq\kappa_{\text {FR}}(i,j)+1\)._

Ollivier-Ricci Curvature.Let \(\mathcal{G}=(G,d_{G},\mu)\) be a triple for specifying Ollivier-Ricci curvature calculations, with \(G\) denoting an unweighted, connected graph, \(d_{G}\) its associated graph metric, and \(\mu:=\{\mu_{v}\mid v\in V\}\) a collection of probability measures at each node. Furthermore, let \(\delta_{i}\) denote the Dirac measure at node \(i\) and \(J(i):=W_{1}(\delta_{i},\mu_{i})\) the corresponding jump probability in the graph \(G\) as defined by Ollivier [55]. Following an edge addition or deletion, we consider an updated triple \(\mathcal{G}^{\prime}=(G^{\prime},d_{G^{\prime}},\mu^{\prime})\), and remark that this yields an updated Wasserstein distance \(W^{\prime}_{1}\), calculated in terms of the new graph metric \(d_{G^{\prime}}\).

**Theorem 3**.: _Given a perturbation (either **edge addition** or **edge deletion**) producing \(\mathcal{G}^{\prime}\), the Ollivier-Ricci curvature \(\kappa^{\prime}_{\text{OR}}(i,j)\) of a pair \((i,j)\) can be bounded via_

\[1-\frac{1}{d_{G^{\prime}}(i,j)}\big{[}2W^{\prime}_{\max}+W^{\prime}_{1}(\mu_{ i},\mu_{j})\big{]}\leq\kappa^{\prime}_{\text{OR}}(i,j)\leq\frac{J^{\prime}(i) +J^{\prime}(j)}{d_{G^{\prime}}(i,j)}, \tag{4}\]

_where \(J^{\prime}(v):=W^{\prime}_{1}(\delta_{v},\mu^{\prime}_{v})\) refers to the new jump probabilities and \(W^{\prime}_{\max}:=\max_{x\in V}W^{\prime}_{1}(\mu_{x},\mu^{\prime}_{x})\) denotes the maximal reaction to the perturbation (measured using the updated Wasserstein distance)._

Resistance Curvature.Let \(G\) be an unweighted, connected graph with a resistance distance \(R_{ij}\) and resistance curvature \(\kappa_{\text{R}}(i,j)\) for each \((i,j)\in E\). Furthermore, let \(d_{x}\) denote the degree for node \(x\in V\). We find that \(\kappa_{\text{R}}\) is well-behaved under these perturbations, in the sense that edge additions can only _increase_ the curvature, and edge deletions can only _decrease_ it. For edge additions, we obtain the following bound (see Appendix B.1.3 for the corresponding bound for edge deletions).

**Theorem 4**.: _If \(G^{\prime}\) is the graph generated by **edge addition**, then \(\kappa^{\prime}_{\text{R}}\geq\kappa_{\text{R}}\),with the following bound:_

\[|\kappa^{\prime}_{\text{R}}(i,j)-\kappa_{\text{R}}(i,j)|\leq\frac{\Delta_{ \operatorname{add}}(d_{i}+d_{j})}{R_{ij}-\Delta_{\operatorname{add}}}, \tag{5}\]

_where \(\Delta_{\operatorname{add}}:=\max_{i,j\in V}\big{(}R_{ij}-\frac{1}{2}\big{(} \frac{1}{d_{i}+1}+\frac{1}{d_{j}+1}\big{)}\big{)}\)._

The implications of this section are that (i) the stability of our topological calculations largely hinges on the stability of the functions being used to define said filtrations, and (ii) _all_ discrete curvature measures satisfy stability properties with respect to changes in graph connectivity, making curvature-based filtrations highly robust.

### Expressivity

A metric between distributions should be non-zero when the distributions differ. For this to occur, our metric needs to be able to distinguish non-isomorphic graphs and be sufficiently expressive. Hornet al. [31] showed that persistent homology with an appropriate choice of filtration is strictly more expressive than \(1\)-WL, the \(1\)-dimensional Weisfeiler-Le(h)man test for graph isomorphism. A similar expressivity result can be obtained for using resistance curvature as a node feature [65], underlining the general utility of curvature. We have the following general results concerning the expressivity or discriminative power of our topological representations.

**Theorem 5**.: _Given two graphs \(F=(V_{F},E_{F})\) and \(G=(V_{G},E_{G})\) with scalar-valued filtration functions \(f,g\), and their respective persistence diagrams \(D_{f},D_{g}\), we have \(d_{B}(D_{f},D_{g})\geq\inf_{\eta}:\,E_{F}\to E_{G}\sup_{x\in E_{F}}|f(x)-g(\eta (x))|,\) where \(\eta\) ranges over all maps from \(E_{F}\) to \(E_{G}\)._

Theorem 5 implies that topological distances are generally more discriminative than the distances between the filtration functions. Thus, calculating topological representations of graphs based on a class of functions improves discriminative power. To further understand the expressive power of curvature filtrations, we analyse strongly-regular graphs, which are often used for studying GNN expressivity as they cannot be distinguished by \(k\)-WL, the \(k\)-dimensional Weisfeiler-Le(h)man test, if \(k\leq 3\)[2, 7, 52]. Additionally, we explore how curvature filtrations can count substructures, an important tool for evaluating and comparing expressivity [56]. To the best of our knowledge, ours is the first work to explore discrete curvature and curvature-based filtrations in this context.

Distinguishing Strongly-Regular Graphs.Strongly-regular graphs are often used to assess the expressive power of graph learning algorithms, constituting a class of graphs that are particularly hard to distinguish. We briefly recall some definitions. A connected graph \(G\) with diameter \(D\) is called _distance regular_ if there are integers \(b_{i}\), \(c_{i}\), (\(0\leq i\leq D\)) such that for any two vertices \(x,y\in V\) with \(d(x,y)=i\), there are \(c_{i}\) neighbours of \(y\) in \(k_{i-1}(x)\) and \(b_{i}\) neighbours of \(y\) in \(k_{i+1}(x)\). For a distance-regular graph, the intersection array is given by \(\{b_{0},b_{1},\ldots,b_{D-1};c_{1},c_{2},\ldots,c_{D}\}\). A graph is called _strongly regular_ if it is distance regular and has a diameter of 2 [18]. We first state two theoretical results about curvature.

**Theorem 6** (Expressivity of curvature notions).: _Both Forman-Ricci curvature and Resistance curvature cannot distinguish distance-regular graphs with the same intersection array, whereas Ollivier-Ricci curvature can distinguish the Rook and Shrikhande graphs, which are strongly-regular graphs with the same intersection array._

The Rook and Shrikhande graph _cannot_ be distinguished by \(2\)-WL [9, 52]. However, OR curvature is sensitive to differences in their first-hop peripheral subgraphs [22], thus distinguishing them. This result shows the limitations of Forman-Ricci and Resistance curvature, as well as the benefits of using Ollivier-Ricci curvature. We show further experiments with curvature notions on strongly-regular graphs in the experimental section, observing improvements whenever we use them as filtrations.

Counting Substructures.Evaluating the ability of curvature to encode structural information is a crucial aspect for understanding its expressivity and validating its overall utility in graph learning. It has previously been shown that incorporating structural information of graphs can extend the expressive power of message-passing neural networks [11, 25, 42]. Additionally, Bouritsas et al. [9] show how GNNs can be (i) strictly more expressive than \(1\)-WL by counting local isomorphic substructures (e.g. cliques), and (ii) exhibit predictive performance improvements when adding such substructure counts to the node features. For instance, some strongly-regular graphs can be distinguished by counting \(4\)-cliques. Discrete curvature measures are informed by these local substructures and have been shown to improve expressivity beyond \(1\)-WL [65] when included as a node feature. Nevertheless, there is limited work exploring what substructure information curvature carries, which would allow us to describe the expressivity of the measure. Moreover, persistent homology can track the number of cycles over the filtration of interest, allowing additional structural information to be encoded at multiple scales. Thus, we will explore the extent to which substructures can be counted for different curvatures (with and without a topological component) in a subsequent experiment, providing evidence on the expressive power of curvature filtrations. See Table 4 for our experimental results, and Appendix E for a more detailed discussion on the tendencies of each curvature notion when counting substructures.

### Choosing a Curvature Notion in Practice

In Section 3.1, we showed that all three of our prototypical curvature notions exhibit advantageous stability properties, thus implying that they may all be used _reliably_ within our method to measure the distance between two sets of graphs. However, which curvature should be chosen in practice? In general, we find that the answer to this question lies at the intersection of _expressivity_ and _scalability_, but depends ultimately on the nature of the experiment at hand. Nevertheless, we aim to provide an intuition for all three notions, along with a general recommendation. We hope this, in conjunction with the experimental and computational complexity results in Section 4 and Appendix G will help practitioners in making an optimal choice.

Comparison.Forman-Ricci is arguably the simplest and most local notion of curvature. Though limited in expressivity when compared to Ollivier-Ricci curvature, it is by far the most efficient to compute. Resistance curvature, by contrast, is the most global notion, making it sensitive to large substructures. However, computing the effective resistance metric on a graph requires inverting the Laplacian, making it far less efficient than Forman and Ollivier-Ricci, especially for large graphs. Finally, we have Ollivier-Ricci curvature, which we have found to be the most expressive based on its ability to (i) distinguish strongly-regular graphs, and (ii) count substructures. It is also the most versatile, given the option to adapt the underlying probability measure; this comes at the cost of lower computational performance in comparison to Forman--Ricci curvature, though.

Given its high expressivity, as well as its overall experimental and computational performance, we recommend using **Ollivier-Ricci** curvature whenever feasible.

## 4 Experiments

We have proven the theoretical stability of discrete curvature notions under certain graph perturbations. We also illustrated their ability to distinguish distance-regular and strongly-regular graphs. Subsequently, we will discuss empirical experiments to evaluate these claims and to further test the utility of our methods.

### Distinguishing Strongly-Regular Graphs

In addition to the theoretical arguments outlined, we explore the ability of our method to distinguish strongly-regular graphs in a subset of data sets, i.e. sr16622, sr261034, sr281264, and sr401224. These data sets are known to be challenging to classify since they cannot be described in terms of the \(1\)-WL test [7]. Our main goal is _not_ to obtain the best accuracy, but to show how the discriminative power of discrete curvature can be improved by using it in a filtration context. Table 1 depicts the results of our classification experiment. We perform a pairwise analysis of _all graphs_ in the data set, calculating distances based on histograms of discrete curvature measurements, or based on the bottleneck distance between persistence diagrams ('Filtrations'). Subsequently, we count all non-zero distances (\(>1\times 10^{-8}\) to correct for precision errors). Our main observation is that combining TDA with curvature is always better than or equal to curvature without TDA. Similar to our theoretical predictions, both resistance curvature and Forman curvature fail to distinguish any of the graphs without TDA. We therefore show the benefits from an expressivity point of view for using discrete curvature as a filtration. Notably, we achieve the best results with OR curvature, which is particularly flexible since it permits changing the underlying _probability measure_. Using a probability measure based on random walks (see Appendix F) takes into account higher-order neighbourhoods and improves discriminative power (on sr261034, the pairwise success rate drops to 0.0/0.2 with raw/TDA values, respectively, if the uniform probability measure is used).

### Expressivity experiments with the BREC dataset

We evaluate discrete curvatures and their filtrations on the BREC data set, which was recently introduced to evaluate GNN expressiveness [66]. The data set consists of different categories of graph pairs (Basic, Regular, and Extension), which are distinguishable by 3-WL but not by 1-WL, as well as Strongly-Regular (STR) and CFI graph pairs, which are indistinguishable using 3-WL. We explore the ability of curvature filtrations to distinguish these graph pairs and compare them

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Method & sr16622 & sr261034 & sr281264 & sr401224 \\ \hline \(\kappa_{\text{SR}}\) & 0.00 & 0.00 & 0.00 & 0.00 \\ \(\kappa_{\text{OR}}\) & **1.00** & 0.78 & **1.00** & 0.00 \\ \(\kappa_{\text{R}}\) & 0.00 & 0.00 & 0.00 & 0.00 \\ \hline Filtration (\(\kappa_{\text{RA}}\)) & **1.00** & 0.20 & 0.00 & **0.93** \\ Filtration (\(\kappa_{\text{OR}}\)) & **1.00** & **0.89** & **1.00** & **0.93** \\ Filtration (\(\kappa_{\text{R}}\)) & **1.00** & 0.20 & 0.00 & **0.93** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Success rate (\(\uparrow\)) of distinguishing pairs of strongly-regular graphs when using either raw curvature values or a curvature filtration.

to substructure counting, \(S_{3}\) and \(S_{4}\), which involves enumerating all 3-node/4-node substructures around nodes in combination with the WL algorithm. These approaches, unlike discrete curvature, have limited practical applications due to their high computational complexity. Similar to our experiments on strongly-regular graphs, we calculate Wasserstein distances based on histograms of OR curvature measurements between the pairs of graphs. Subsequently, we count all non-zero distances (\(>1\times 10^{-8}\) to correct for precision errors). Our main observations from Table 2 are that curvature _can_ distinguish graphs which are 3-WL indistinguishable. Additionally, we observe improvements in success rate using the filtration on the Basic, Regular, STR and Extension graph pairs. Moreover, our curvature-based approach performs competitively and sometimes even better than \(S_{4}\), which has been shown to be extremely effective in graph learning tasks [9]. Despite its empirical prowess, \(S_{4}\) is computationally expensive, making it an _infeasible_ measure in many applications. Discrete curvature and its filtrations, by contrast, scale significantly better.

### Behaviour with Respect to Perturbations

To explore the behaviour of curvature descriptors under perturbations, we analyse the correlation of our metric when adding and removing edges in the 'Community Graph' data set: we increase the fraction of edges added or removed from \(0.0\) to \(0.95\), measuring the distance between the perturbed graphs and the original graphs for each perturbation level. Following O'Bray et al. [54], we require a suitable distance measure to be highly correlated with increasing amounts of perturbation. We compare to current approaches that use descriptor functions with MMD. As Table 3 shows, a curvature filtration yields a higher correlation than curvature in combination with MMD, showing the benefits of employing TDA from a stability perspective. Additionally, curvature filtrations improve upon the normalized Laplacian and clustering coefficient (again, we used MMD for the comparison of these distributions). OR curvature exhibits particularly strong results when adding/removing edges, even surpassing the local degrees of a graph (which, while being well-aligned with perturbations of the graph structure, suffer in terms of overall expressivity).

### Counting Substructures

The ability of a descriptor to count local substructures is important for evaluating its expressive power [3]. We follow Chen et al. [12], who assess the ability of GNNs to count substructures such as triangles, chordal cycles, stars and tailed triangles. This is achieved by generating regular graphs with random edges removed, and counting the number of occurrences of each substructure in a given graph. To assess the power of curvature to capture such local structural features, we use the same experimental setup and pass the edge-based curvatures through a simple 1-layer MLP to output the substructure count. Additionally, we evaluate the effect of using curvature as a filtration.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Method & Basic (56) & Regular (50) & STR (50) & Extension (97) & CFI (97) \\ \hline
1-WL & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
3-WL & **1.00** & **1.00** & 0.00 & **1.00** & **0.59** \\ \hline \(S_{3}\) & 0.86 & 0.96 & 0.00 & 0.05 & 0.00 \\ \(S_{4}\) & 1.00 & 0.98 & **1.00** & 0.84 & 0.00 \\ \hline \(\kappa_{\text{OR}}\) & 1.00 & 0.96 & 0.06 & 0.93 & 0.00 \\ \(\kappa_{\text{ER}}\) & 0.96 & 0.92 & 0.00 & 0.52 & 0.00 \\ \(\kappa_{\text{R}}\) & **1.00** & **1.00** & 0.00 & **1.00** & 0.04 \\ \hline \hline Filtration (\(\kappa_{\text{OR}}\)) & **1.00** & **1.00** & 0.08 & 0.95 & 0.00 \\ Filtration (\(\kappa_{\text{R}}\)) & 0.98 & 0.96 & 0.04 & 0.61 & 0.00 \\ Filtration (\(\kappa_{\text{R}}\)) & **1.00** & **1.00** & 0.04 & **1.00** & 0.04 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Success rate (\(\uparrow\)) of distinguishing pairs of graphs in the BREC dataset when using different discrete curvatures and their filtrations.

\begin{table}
\begin{tabular}{l c c} \hline \hline Measure & Adding Edges & Removing Edges \\ \hline Laplacian & \(0.457\pm 0.013\) & \(0.420\pm 0.000\) \\ Clust. Coeff. & \(0.480\pm 0.012\) & \(0.504\pm 0.020\) \\ Degrees & \(0.761\pm 0.003\) & \(0.995\pm 0.000\) \\ \hline \(\kappa_{\text{ER}}\) & \(0.420\pm 0.000\) & \(0.432\pm 0.003\) \\ \(\kappa_{\text{OR}}\) & \(0.903\pm 0.005\) & \(0.910\pm 0.002\) \\ \(\kappa_{\text{R}}\) & \(0.420\pm 0.004\) & \(0.441\pm 0.005\using the curvature alone, the only exception being Forman curvature for counting \(4\)-cycles. We leave a more detailed investigation of these phenomena for future work.

### Synthetic Graph Generative Model Evaluation

To have a ground truth for a graph distribution, we tested our metric's ability to _distinguish graphons_. Following the approach suggested by Sabanayagam et al. [60], we generate four graphons, \(W_{1}(u,v)=uv\), \(W_{2}(u,v)=\exp\{-\max(u,v)^{0.75}\}\), \(W_{3}(u,v)=\exp\{-0.5*(\min(u,v)+u^{0.5}+v^{0.5})\}\) and \(W_{4}(u,v)=\|u-v\|\). Sampling from these graphons produces dense graphs, and we control their size to be between \(9\) and \(37\) nodes, thus ensuring that we match the sizes of molecular graphs in the ZINC data set [32], an important application for generative models.

We perform experiments by considering all combinations of three and four graphons. We generate distances between graphs with our method as well as other kernel-based approaches, and use spectral clustering to separate the distributions. We measure the performance of the algorithms using the Adjusted Rand Index (ARI) of the predicted clusters, comparing to three state of the art, kernel-based approaches: (i) Wasserstein Weisfeiler-Le(h)man graph kernels [63], (ii) graph neural tangent kernels [20], and (iii) Tree Mover's Distance [13]. From Figure 1(a), we find that a filtration based on OR curvature is better able to distinguish and cluster graphons than the previously-described approaches based on kernels and it performs best for all sets of graphons. We also observe that OR curvature performs better than other discrete curvatures, with resistance curvature achieving higher ARI than Forman curvature. Notice that unlike these kernel approaches, our method can be easily extended to provide a proper metric between distributions of graphs.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Method & Triangle & Tailed Tri. & Star & \(4\)-Cycle \\ \hline Trivial Predictor & 0.88 & 0.90 & 0.81 & 0.93 \\ GCN & 0.42 & 0.32 & **0.18** & **0.28** \\ \hline \(\kappa_{\text{FR}}\) & 0.54 & 0.56 & 0.72 & 0.53 \\ \(\kappa_{\text{OR}}\) & 0.33 & 0.31 & 0.40 & 0.31 \\ \(\kappa_{\text{R}}\) & 0.59 & 0.50 & 0.72 & 0.47 \\ \hline Filtration (\(\kappa_{\text{FR}}\)) & 0.45 & 0.52 & 0.49 & 0.60 \\ Filtration (\(\kappa_{\text{OR}}\)) & **0.23** & **0.24** & 0.34 & 0.31 \\ Filtration (\(\kappa_{\text{OR}}\)) & 0.47 & 0.48 & 0.36 & 0.42 \\ \hline \hline \end{tabular}
\end{table}
Table 4: MAE (\(\downarrow\)) for counting substructures based on raw curvature values and curvature filtrations. The ‘Trivial Predictor’ always outputs the mean training target.

Figure 2: (a) Adjusted Rand Index (\(\uparrow\)) for clustering sets of four graphons. We compare our curvature filtrations (b) to kernel-based methods. (b) Permutation testing values (\(\downarrow\)) for distinguishing different bioinformatics data sets. Position (\(i,j\)) in each matrix denotes a permutation test between data set \(i\) and data set \(j\).

### Real-World Graph Generative Model Evaluation

By converting the generated persistence diagrams into a _persistence landscape_, we can generate an average topological descriptor for all graphs in a distribution [10]. This allows us to calculate norms between graph distributions, making it possible to perform two-sample and permutation testing, unlike a majority of kernel-based approaches, which provide a distance between _all_ individual graphs, or would require MMD to assess mean similarities. We randomly sample ten graphs from four different bioinformatics data sets, i.e. KKI, PROTEINS, Peking, and ENZYMES [51]. We measure the distance between two data sets using the \(L^{P}\) norm between their average persistence landscapes. We then permute graphs from both samples, randomly selecting sets of equal size, measure their respective distances, and finally aggregate the fraction of distances which are higher than the original. A low fraction indicates that distances are _lower_ for permutations than between the original sets of graphs, suggesting that the metric can distinguish between the two distributions. We compare our approach to previous methods that combine graph statistics with MMD. Using a significance threshold of \(p<0.05\), we see in Figure 1(b) that both Forman and the OR curvature are able to distinguish _all but one pair of data sets, an improvement over all the other approaches_. In general, we find that fractions are lower using curvature filtrations than graph statistic based approaches, demonstrating the utility of our approach.

## 5 Conclusion

We have described the first thorough analysis of both _stability_ and _expressivity_ of discrete curvature notions and their filtration formulations on graphs. We believe this to be important for the community in multiple contexts, ranging from improving expressivity of GNNs to understanding the robustness of curvature on graphs. Using curvature filtrations and their topological descriptors (here: _persistence landscapes_), we develop a new metric to measure distances between graph distributions. Our metric can be used for evaluating graph generative models, is robust and expressive, and improves upon current approaches based on graph descriptor and evaluator functions. We have also demonstrated clear advantages over state-of-the-art methods that combine graph statistics with MMD, providing instead a metric with (i) well-understood parameter and function choices, (ii) stability guarantees, (iii) added expressivity, and (iv) improved computational performance. Most notably, we scale _significantly_ better for large populations of molecular-sized graphs (see Appendix G for more details), which we consider crucial for current graph generative model applications. We hope that our pipeline will provide a principled, interpretable, and scalable method for practitioners to use when evaluating the performance of graph generative models.

Future work could explore other representations [1, 58], focus on different filtration constructions [15], new curvature measures, or further extend our stability and expressivity results (for instance to the setting of weighted graphs). Given the beneficial performance and flexibility of Ollivier-Ricci curvature in our experiments, we believe that changing--or _learning_--the probability measure used in its calculation could lead to further improvements in terms of expressivity, for example. Another relevant direction involves incorporating node and edge features into the distance measure and applying the model to specific use cases such as evaluating molecule generation. We envision that this could be done using bi-filtrations or by considering node features for the curvature calculations.

## Acknowledgments and Disclosure of Funding

The authors want to thank the anonymous reviewers for their comments, which helped us improve the paper. We are also grateful for the area chair who believed in this work. The authors are grateful to Fabrizio Frasca and Leslie O'Bray for valuable feedback on early versions of the manuscript. J.S. is supported by the UKRI CDT in AI for Healthcare [http://ai4health.io](http://ai4health.io) (Grant No. P/S023283/1). M.B. is supported in part by the ERC Consolidator grant No. 724228 (LEMAN) and the EPSRC Turing AI World-Leading Researcher Fellowship. B.R. is supported by the Bavarian State government with funds from the _Hightech Agenda Bavaria_. He wishes to dedicate this paper to his son Andrin.

## References

* Adams et al. [2017] Henry Adams, Tegan Emerson, Michael Kirby, Rachel Neville, Chris Peterson, Patrick Shipman, Sofya Chepushtanova, Eric Hanson, Francis Motta, and Lori Ziegelmeier. Persistence images: A stable vector representation of persistent homology. _Journal of Machine Learning Research_, 18(8):1-35, 2017.
* Arvind et al. [2018] V. Arvind, Frank Fuhlbruck, Johannes Kobler, and Oleg Verbitsky. On Weisfeiler-Leman invariance: Subgraph counts and related graph properties, 2018. URL [https://arxiv.org/abs/1811.04801](https://arxiv.org/abs/1811.04801).
* Barcelo et al. [2021] Pablo Barcelo, Floris Goerts, Juan Reutter, and Maksimilian Ryschkov. Graph neural networks with local graph parameters, 2021. URL [https://arxiv.org/abs/2106.06707](https://arxiv.org/abs/2106.06707).
* Benjamin et al. [2021] Sophia Benjamin, Arushi Mantri, and Quinn Perian. On the Wasserstein distance between \(k\)-step probability measures on finite graphs, 2021. URL [https://arxiv.org/abs/2110.10363](https://arxiv.org/abs/2110.10363).
* Bickerton et al. [2012] G. Richard Bickerton, Gaia V. Paolini, Jeremy Besnard, Sorel Muresan, and Andrew L Hopkins. Quantifying the chemical beauty of drugs. _Nature Chemistry_, 4(2):90-98, 2012.
* Biggs [1993] Norman L. Biggs. Potential theory on distance-regular graphs. _Combinatorics, Probability and Computing_, 2(3):243-255, 1993. doi: 10.1017/S096354830000064X.
* Bodnar et al. [2021] Cristian Bodnar, Fabrizio Frasca, Yuguang Wang, Nina Otter, Guido F Montufar, Pietro Lio, and Michael Bronstein. Weisfeiler and Lehman go topological: Message passing simplicial networks. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 1026-1037. PMLR, 2021.
* Borgwardt et al. [2020] Karsten Borgwardt, Elisabetta Ghisu, Felipe Llinares-Lopez, Leslie O'Bray, and Bastian Rieck. Graph kernels: State-of-the-art and future challenges. _Foundations and Trends(r) in Machine Learning_, 13(5-6):531-712, 2020. doi: 10.1561/2200000076.
* Bouritsas et al. [2022] Giorgos Bouritsas, Fabrizio Frasca, Stefanos P. Zafeiriou, and Michael Bronstein. Improving graph neural network expressivity via subgraph isomorphism counting. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, pages 1-1, 2022. doi: 10.1109/TPAMI.2022.3154319.
* Bubenik [2015] Peter Bubenik. Statistical topological data analysis using persistence landscapes. _Journal of Machine Learning Research_, 16:77-102, 2015.
* Chen et al. [2019] Ting Chen, Song Bian, and Yizhou Sun. Are powerful graph neural nets necessary? A dissection on graph classification, 2019. URL [https://arxiv.org/abs/1905.04579](https://arxiv.org/abs/1905.04579).
* Chen et al. [2020] Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can graph neural networks count substructures?, 2020. URL [https://arxiv.org/abs/2002.04025](https://arxiv.org/abs/2002.04025).
* Chuang and Jegelka [2022] Ching-Yao Chuang and Stefanie Jegelka. Tree mover's distance: Bridging graph metrics and stability of graph neural networks. In _Advances in Neural Information Processing Systems_, volume 35, 2022.
* Cohen-Steiner et al. [2007] David Cohen-Steiner, Herbert Edelsbrunner, and John Harer. Stability of persistence diagrams. _Discrete & Computational Geometry_, 37(1):103-120, 2007. doi: 10.1007/s00454-006-1276-5.
* Cohen-Steiner et al. [2009] David Cohen-Steiner, Herbert Edelsbrunner, and John Harer. Extending persistence using Poincare and Lefschetz duality. _Foundations of Computational Mathematics_, 9(1):79-103, 2009. doi: 10.1007/s10208-008-9027-z.
* Coupette et al. [2023] Corinna Coupette, Sebastian Dalleiger, and Bastian Rieck. Ollivier-Ricci curvature for hypergraphs: A unified framework. In _International Conference on Learning Representations (ICLR)_, 2023. URL [https://openreview.net/forum?id=sPCKNl5qDps](https://openreview.net/forum?id=sPCKNl5qDps).
* Dai et al. [2020] Hanjun Dai, Azade Nazi, Yujia Li, Bo Dai, and Dale Schuurmans. Scalable deep generative modeling for sparse graphs, 2020. URL [https://arxiv.org/abs/2006.15502](https://arxiv.org/abs/2006.15502).

* Van Dam et al. [2016] Edwin R. Van Dam, Jack H. Koolen, and Hajime Tanaka. Distance-regular graphs. _The Electronic Journal of Combinatorics_, 1000, apr 2016. doi: 10.37236/4925.
* Devriendt and Lambiotte [2022] Karel Devriendt and Renaud Lambiotte. Discrete curvature on graphs from the effective resistance. _Journal of Physics: Complexity_, 3(2):025008, 2022. doi: 10.1088/2632-072X/ac730d.
* Du et al. [2019] Simon S. Du, Kangcheng Hou, Russ R. Salakhutdinov, Barnabas Poczos, Ruosong Wang, and Keyulu Xu. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* Edelsbrunner and Harer [2010] Herbert Edelsbrunner and John Harer. _Computational topology: An introduction_. American Mathematical Society, Providence, RI, USA, 2010.
* Feng et al. [2022] Jiarui Feng, Yixin Chen, Fuhai Li, Anindya Sarkar, and Muhan Zhang. How powerful are k-hop message passing graph neural networks. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 4776-4790. Curran Associates, Inc., 2022.
* Feragen et al. [2015] Aasa Feragen, Francois Lauze, and Soren Hauberg. Geodesic exponential kernels: When curvature and linearity conflict. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 3032-3042, 2015.
* Forman [2003] Robin Forman. Bochner's method for cell complexes and combinatorial ricci curvature. _Discrete & Computational Geometry_, 29(3):323-374, 2003. doi: 10.1007/s00454-002-0743-x.
* Geerts et al. [2020] Floris Geerts, Filip Mazowiecki, and Guillermo A. Perez. Let's agree to degree: Comparing graph convolutional networks in the message-passing framework, 2020. URL [https://arxiv.org/abs/2004.02593](https://arxiv.org/abs/2004.02593).
* Gosztolai and Arnaudon [2021] Adam Gosztolai and Alexis Arnaudon. Unfolding the multiscale structure of networks with dynamical Ollivier-Ricci curvature. _Nature Communications_, 12(1):4561, 2021. ISSN 2041-1723. doi: 10.1038/s41467-021-24884-1.
* Gretton et al. [2012] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and Alexander Smola. A kernel two-sample test. _Journal of Machine Learning Research_, 13(25):723-773, 2012.
* Guo et al. [2018] Ji-Ming Guo, Pan-Pan Tong, Jianxi Li, Wai Chee Shiu, and Zhi-Wen Wang. The effect on eigenvalues of connected graphs by adding edges. _Linear Algebra and its Applications_, 548:57-65, July 2018. doi: 10.1016/j.laa.2018.02.012.
* Hofer et al. [2020] Christoph D. Hofer, Florian Graf, Bastian Rieck, Marc Niethammer, and Roland Kwitt. Graph filtration learning. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning (ICML)_, number 119 in Proceedings of Machine Learning Research, pages 4314-4323. PMLR, 2020.
* van der Hoorn et al. [2023] Pim van der Hoorn, Gabor Lippner, Carlo Trugenberger, and Dmitri Krioukov. Ollivier curvature of random geometric graphs converges to Ricci curvature of their Riemannian manifolds. _Discrete & Computational Geometry_, 70(3):671-712, 2023. doi: 10.1007/s00454-023-00507-y.
* Horn et al. [2022] Max Horn, Edward De Brouwer, Michael Moor, Yves Moreau, Bastian Rieck, and Karsten Borgwardt. Topological graph neural networks. In _International Conference on Learning Representations (ICLR)_, 2022. URL [https://openreview.net/forum?id=oxxUMeFwEHd](https://openreview.net/forum?id=oxxUMeFwEHd).
* Irwin and Shoichet [2005] John J. Irwin and Brian K. Shoichet. ZINC-a free database of commercially available compounds for virtual screening. _Journal of Chemical Information and Modeling_, 45(1):177-182, 2005.
* Jiang and Luo [2022] Weiwei Jiang and Jiayun Luo. Graph neural network for traffic forecasting: A survey. _Expert Systems with Applications_, 207:117921, 2022. doi: 10.1016/j.eswa.2022.117921.

* Jin et al. [2018] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph generation. In Jennifer Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning (ICML)_, volume 80 of _Proceedings of Machine Learning Research_, pages 2323-2332. PMLR, 2018.
* Jin et al. [2022] Wengong Jin, Jeremy Wohlwend, Regina Barzilay, and Tommi S. Jaakkola. Iterative refinement graph neural network for antibody sequence-structure co-design. In _International Conference on Learning Representations (ICLR)_, 2022. URL [https://openreview.net/forum?id=LIZbhrE_2A](https://openreview.net/forum?id=LIZbhrE_2A).
* Jiradilok and Kamtue [2021] Pakawut Jiradilok and Supanat Kamtue. Transportation distance between probability measures on the infinite regular tree, 2021. URL [https://arxiv.org/abs/2107.09876](https://arxiv.org/abs/2107.09876).
* Jost and Liu [2014] Jurgen Jost and Shiping Liu. Ollivier's Ricci curvature, local clustering and curvature-dimension inequalities on graphs. _Discrete & Computational Geometry_, 51(2):300-322, 2014. doi: 10.1007/s00454-013-9558-1.
* Jumper et al. [2021] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zidek, Anna Potapenko, Alex Bridgland, Clemens Meyer, Simon A. A. Kohl, Andrew J. Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Ellen Clancy, Michal Zeilinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W. Senior, Koray Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis. Highly accurate protein structure prediction with AlphaFold. _Nature_, 596(7873):583-589, 2021. doi: 10.1038/s41586-021-03819-2.
* Justel et al. [1997] Ana Justel, Daniel Pena, and Ruben Zamar. A multivariate Kolmogorov-Smirnov test of goodness of fit. _Statistics & Probability Letters_, 35(3):251-259, 1997. doi: 10.1016/S0167-7152(97)00020-5.
* Kipf and Welling [2017] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _International Conference on Learning Representations (ICLR)_, 2017. URL [https://openreview.net/forum?id=SJU4ayYg1](https://openreview.net/forum?id=SJU4ayYg1).
* Koolen et al. [2013] Jack H. Koolen, Greg Markowsky, and Jongyook Park. On electric resistances for distance-regular graphs. _European Journal of Combinatorics_, 34(4):770-786, 2013. doi: 10.1016/j.ejc.2012.12.001.
* Li et al. [2019] Michael Lingzhi Li, Meng Dong, Jiawei Zhou, and Alexander M. Rush. A hierarchy of graph neural networks based on learnable local features, 2019. URL [https://arxiv.org/abs/1911.05256](https://arxiv.org/abs/1911.05256).
* Li et al. [2020] Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec. Distance encoding: Design provably more powerful neural networks for graph representation learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems (NeurIPS)_, volume 33, pages 4465-4478. Curran Associates, Inc., 2020.
* Liao et al. [2019] Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, Charlie Nash, William L. Hamilton, David Duvenaud, Raquel Urtasun, and Richard S. Zemel. Efficient graph generation with graph recurrent attention networks, 2019. URL [https://arxiv.org/abs/1910.00760](https://arxiv.org/abs/1910.00760).
* Liu et al. [2018] Shiping Liu, Florentin Munch, and Norbert Peyerimhoff. Bakry-Emery curvature and diameter bounds on graphs. _Calculus of Variations and Partial Differential Equations_, 57(2), 2018. doi: 10.1007/s00526-018-1334-x.
* Lovasz [1996] L. Lovasz. Random walks on graphs: A survey. In D. Miklos, V. T. Sos, and T. Szonyi, editors, _Combinatorics, Paul Erdos is Eighty_, volume 2, pages 353-398. Janos Bolyai Mathematical Society, Budapest, 1996.
* Mirhoseini et al. [2021] Azalia Mirhoseini, Anna Goldie, Mustafa Yazgan, Joe Wenjie Jiang, Ebrahim Songhori, Shen Wang, Young-Joon Lee, Eric Johnson, Omkar Pathak, Azade Nazi, Jiwoo Pak, Andy Tong, Kavya Srinivasa, William Hang, Emre Tuncer, Quoc V. Le, James Laudon, Richard Ho, Roger Carpenter, and Jeff Dean. A graph placement methodology for fast chip design. _Nature_, 594(7862):207-212, 2021. doi: 10.1038/s41586-021-03544-w.

* Monti et al. [2019] Federico Monti, Fabrizio Frasca, Davide Eynard, Damon Mannion, and Michael M. Bronstein. Fake news detection on social media using geometric deep learning, 2019. URL [https://arxiv.org/abs/1902.06673](https://arxiv.org/abs/1902.06673).
* Moreno et al. [2018] Sebastian Moreno, Jennifer Neville, and Sergey Kirshner. Tied Kronecker product graph models to capture variance in network populations. _ACM Transactions on Knowledge Discovery from Data_, 12(3), 2018. doi: 10.1145/3161885.
* Morris et al. [2019] Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and Leman go neural: Higher-order graph neural networks. _Proceedings of the AAAI Conference on Artificial Intelligence_, 33(1):4602-4609, 2019. doi: 10.1609/aaai.v33i01.33014602.
* Morris et al. [2020] Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. TUDataset: A collection of benchmark datasets for learning with graphs, 2020. URL [https://arxiv.org/abs/2007.08663](https://arxiv.org/abs/2007.08663).
* Morris et al. [2021] Christopher Morris, Yaron Lipman, Haggai Maron, Bastian Rieck, Nils M. Kriege, Martin Grohe, Matthias Fey, and Karsten Borgwardt. Weisfeiler and Leman go machine learning: The story so far, 2021. URL [https://arxiv.org/abs/2112.09992](https://arxiv.org/abs/2112.09992).
* Niu et al. [2020] Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon. Permutation invariant graph generation via score-based generative modeling, 2020. URL [https://arxiv.org/abs/2003.00638](https://arxiv.org/abs/2003.00638).
* O'Bray et al. [2022] Leslie O'Bray, Max Horn, Bastian Rieck, and Karsten Borgwardt. Evaluation metrics for graph generative models: Problems, pitfalls, and practical solutions. In _International Conference on Learning Representations (ICLR)_, 2022.
* Ollivier [2007] Yann Ollivier. Ricci curvature of metric spaces. _Comptes Rendus Mathematique_, 345(11):643-646, 2007. doi: 10.1016/j.crma.2007.10.041.
* Papp and Wattenhofer [2022] Pal Andras Papp and Roger Wattenhofer. A theoretical comparison of graph neural network extensions, 2022. URL [https://arxiv.org/abs/2201.12884](https://arxiv.org/abs/2201.12884).
* Rieck [2023] Bastian Rieck. On the expressivity of persistent homology in graph learning, 2023. URL [https://arxiv.org/abs/2302.09826](https://arxiv.org/abs/2302.09826).
* Rieck et al. [2020] Bastian Rieck, Filip Sadlo, and Heike Leitte. Topological machine learning with persistence indicator functions. In Hamish Carr, Issei Fujishiro, Filip Sadlo, and Shigeo Takahashi, editors, _Topological Methods in Data Analysis and Visualization V_, pages 87-101. Springer, Cham, Switzerland, 2020. doi: 10.1007/978-3-030-43036-8_6.
* Ruiz et al. [2021] Camilo Ruiz, Marinka Zitnik, and Jure Leskovec. Identification of disease treatment mechanisms through the multiscale interactome. _Nature Communications_, 12(1):1796, 2021.
* Sabanayagam et al. [2022] Mahalakshmi Sabanayagam, Leena Chennuru Vankadara, and Debarghya Ghoshdastidar. Graphon based clustering and testing of networks: Algorithms and theory. In _International Conference on Learning Representations (ICLR)_, 2022. URL [https://openreview.net/forum?id=sTNHCrIKDQc](https://openreview.net/forum?id=sTNHCrIKDQc).
* Samal et al. [2018] Areejit Samal, R. P. Sreejith, Jiao Gu, Shiping Liu, Emil Saucan, and Jurgen Jost. Comparative analysis of two discretizations of Ricci curvature for complex networks. _Scientific Reports_, 8(1), 2018. doi: 10.1038/s41598-018-27001-3.
* Saucan et al. [2021] Emil Saucan, Areejit Samal, and Jurgen Jost. A simple differential geometry for complex networks. _Network Science_, 9(S1):S106-S133, 2021. doi: 10.1017/nws.2020.42.
* Togninalli et al. [2019] Matteo Togninalli, Elisabetta Ghisu, Felipe Llinares-Lopez, Bastian Rieck, and Karsten Borgwardt. Wasserstein Weisfeiler-Lehman graph kernels. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems (NeurIPS)_, volume 32. Curran Associates, Inc., 2019.

* Topping et al. [2022] Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael M. Bronstein. Understanding over-squashing and bottlenecks on graphs via curvature. In _International Conference on Learning Representations (ICLR)_, 2022. URL [https://openreview.net/forum?id=7UmjRGzp-A](https://openreview.net/forum?id=7UmjRGzp-A).
* Velingker et al. [2022] Ameya Velingker, Ali Kemal Sinop, Ira Ktena, Petar Velickovic, and Sreenivas Gollapudi. Affinity-aware graph networks, 2022. URL [https://arxiv.org/abs/2206.11941](https://arxiv.org/abs/2206.11941).
* Wang and Zhang [2023] Yanbo Wang and Muhan Zhang. Towards better evaluation of gnn expressiveness with BREC dataset, 2023. URL [https://arxiv.org/abs/2304.07702](https://arxiv.org/abs/2304.07702).
* Watts and Strogatz [1998] Duncan J. Watts and Steven H. Strogatz. Collective dynamics of'small-world' networks. _Nature_, 393(6684):440-442, 1998. doi: 10.1038/30918.
* Wee and Xia [2021] JunJie Wee and Kelin Xia. Forman persistent Ricci curvature (FPRC)-based machine learning models for protein-ligand binding affinity prediction. _Briefings in Bioinformatics_, 22(6):bbab136, 2021. doi: 10.1093/bib/bbab136.
* Wee and Xia [2021] JunJie Wee and Kelin Xia. Ollivier persistent Ricci curvature-based machine learning for the protein-ligand binding affinity prediction. _Journal of Chemical Information and Modeling_, 61(4):1617-1626, 2021. doi: 10.1021/acs.jcim.0c01415. URL [https://doi.org/10.1021/acs.jcim.0c01415](https://doi.org/10.1021/acs.jcim.0c01415).
* Xu et al. [2019] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _International Conference on Learning Representations_, 2019. URL [https://openreview.net/forum?id=ryGs6iA5Km](https://openreview.net/forum?id=ryGs6iA5Km).
* You et al. [2018] Jiaxuan You, Rex Ying, Xiang Ren, William Hamilton, and Jure Leskovec. GraphRNN: Generating realistic graphs with deep auto-regressive models. In Jennifer Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning (ICML)_, volume 80 of _Proceedings of Machine Learning Research_, pages 5708-5717. PMLR, 2018.

Pseudocode

Here we give pseudocode for various parts of the method, highlighting the most relevant aspects of using curvature filtrations to evaluate graph generative models. First, we outline a crucial part of our evaluation framework in Algorithm1: how we compute summary topological descriptors for sets of graphs. This algorithm, taken from Bubenik [10], assumes a list of precomputed persistence landscapes, one for each graph. Algorithm2, on the other hand, outlines our procedure for generating a distance between two sets of graphs using their summary topological descriptors.

```
\(\Lambda\) is a list of persistence landscapes \(\bar{\Lambda}\) is the average persistence landscape function ComputeAverageLandscape(\(\Lambda\)) \(n\leftarrow\) length of \(\Lambda\) \(D\leftarrow\) maximum homology degree occurring in \(\Lambda\) \(\bar{\Lambda}\leftarrow\) empty persistence landscape with \(D\) homology dimensions. \(\lambda^{i}(k,t)\leftarrow\) the piecewise-linear function encoding hom-deg \(k\) contained in \(\lambda^{i}\in\Lambda\). \(R\leftarrow\) maximal domain for each function contained in \(\Lambda\). \(k\gets 0\) while\(k\leq D\)do for\(t\in R\)do \(\bar{\Lambda}(k,t)\leftarrow\frac{1}{n}\sum_{i}^{n}\lambda^{i}(k,t)\) endfor \(k\gets k+1\) endwhile return\(\bar{\Lambda}\) endfunction
```

**Algorithm 1** Compute Average of Persistence Landscapes

Recall that persistence landscapes are _a collection of piecewise linear functions_ that encode the homological information tracked by a specified filtration. Thus, to compute the average landscape \(\bar{\Lambda}\) at each dimension, we simply sum the piecewise linear functions, and divide by the total number of landscapes in consideration. To understand our method for comparing sets of graphs, and thus evaluating graph generative models, based on average persistence landscapes see Algorithm2.

## Appendix B Additional Proofs on Stability and Expressivity

### Stability Proofs

**Theorem 1**.: _Given graphs \(F=(V_{F},E_{F})\) and \(G=(V_{G},E_{G})\) with filtration functions \(f,g\), and corresponding persistence diagrams \(D_{f},D_{g}\), we have \(d_{B}(D_{f},D_{g})\leq\max\{\operatorname{dis}(f,g),\operatorname{dis}(g,f)\}\), where \(\operatorname{dis}(f,g):=|\max_{x\in E_{F}}f(x)-\min_{y\in E_{G}}g(y)|\) and vice versa for \(\operatorname{dis}(g,f)\)._

Proof.: Considering the calculation of persistence diagrams based on scalar-valued filtrations functions, every point in the persistence diagram \(D_{f}\) can be written as a tuple of the form \((f(e_{F}),f(e^{\prime}_{F}))\), with \(e_{F},e^{\prime}_{F}\in E_{F}\); the sample applies for \(D_{g}\). The inner distance between such tuples that occur in the bottleneck distance calculation can thus be written as

\[\|(f(e_{F}),f(e^{\prime}_{F}))-(g(e_{G}),g(e^{\prime}_{G}))\|_{\infty}. \tag{6}\]

The maximum distance that can be achieved using this expression is determined by the maximum variation of the functions, expressed via \(\operatorname{dis}(f,g)\) and \(\operatorname{dis}(g,f)\), respectively. 

Edge Filtrations versus Vertex FiltrationsOur results are structured to address filtrations built by a function on the _edges_ of a graph \(G=(V,E)\), \(f\colon E\to\mathds{R}\). This matches our notions of discrete curvature, which are also defined edge-wise. \(f\) gives an explicit ordering on \(E\) and thus an induced ordering on \(V\) given by:

\[v\leq v^{\prime}\iff\sum_{e\in E_{v}}f(e)\leq\sum_{e^{\prime}\in E_{v^{\prime} }}f(e^{\prime})\]where \(E_{x}\) is the set of edges incident to \(x\in V\). However, one can also define a filtration directly over _vertices_ with a scalar valued function \(h\colon V\to\mathds{R}\). By assumption, \(h\) can attain only a finite number of values, call the unique values \(b_{1},b_{2},\ldots b_{k}\). Thus, we can also compute a filtration \(\emptyset\subseteq G_{0}\subseteq G_{1}\ldots\subseteq G_{k-1}\subseteq G_{k}=G\), where each \(G_{i}:=(V_{i},E_{i})\), with \(V_{i}:=\{v\in V\mid h(v)\leq b_{i}\}\) and \(E_{i}:=\{e\in E\mid\max_{v\in e}h(v)\leq b_{i}\}\). Similarly, the explicit ordering on \(V\) given by \(h\) induces an ordering on \(E\):

\[e\leq e^{\prime}\iff\max_{v\in e}h(v)\leq\max_{v^{\prime}\in e^{\prime}}h(v^{ \prime})\]

The key idea here is that _either choice gives rise to an ordering of both edges, and vertices_ that are used to calculate persistent homology of the graph. This means that the arguments for Theorem 1 and Theorem 5 also bound the bottle-neck distance for persistence diagrams generated using vertex filtrations.

Graph perturbations.Here we explicitly specify a common framework used in the proofs for stability of curvature functions. As mentioned in the main text, we consider perturbations to _unweighted_, _connected_ graphs \(G=(V,E)\), with \(|V|=n\) and \(|E|=m\). In the case of _edge addition_, let \(i*\) and \(j*\) be arbitrary vertices that we wish to connect with a _new_ edge, forming our new graph \(G^{\prime}=(V,E^{\prime})\) where \(E^{\prime}=E\cup(i*,j*)\) such that \(|E^{\prime}|=m+1\). For _edge deletion_, we similarly let \((i*,j*)\in E\) be the edge we delete such that \(E^{\prime}\subset E\) and \(|E^{\prime}|=m-1\). Moreover, we only consider edges \((i*,j*)\) that leave \(G^{\prime}\) connected.

#### b.1.1 Forman-Ricci Curvature

**Theorem 2**.: _If \(G^{\prime}\) is the graph generated by **edge addition**, then the updated Forman curvature \(\kappa^{\prime}_{\text{FR}}\) for pre-existing edges \((i,j)\in E\) can be bounded by \(\kappa_{\text{FR}}(i,j)-1\leq\kappa^{\prime}_{\text{FR}}(i,j)\leq\kappa_{\text{ FR}}(i,j)+2\). If \(G^{\prime}\) is the graph generated by **edge deletion**, then the updated Forman curvature \(\kappa^{\prime}_{\text{FR}}\) for pre-existing edges \((i,j)\in E\) can be bounded by \(\kappa_{\text{FR}}(i,j)-2\leq\kappa^{\prime}_{\text{FR}}(i,j)\leq\kappa_{\text {FR}}(i,j)+1\)._

Proof.: We first handle the case of **edge addition**, using the graph perturbation framework specified above. By definition \(\kappa_{\text{FR}}(i,j)\) depends _only_ on the degrees of the source and target \((i,j)\in E\) and the number of triangles formed using \((i,j)\), \(|\#_{\Delta_{ij}}|=|N(i)\cap N(j)|\), where \(N(i)\), \(N(j)\) are the set of neighbouring nodes for \(i,j\) respectively. This is a local computation- all relevant information can be computed in the subgraph surrounding the inserted edge \((i*,j*)\). Thus, in order to understand stability of \(\kappa_{\text{FR}}(i,j)\), we need to understand how \(N(i)\) and \(N(j)\) change under graph perturbations. For our new graph \(G^{\prime}\), the only edges with potential to change their curvature lie in the set:

\[E_{*}:=\{(u,v)\in E|u,v\in N(i*)\cup N(j*)\}\]

For the new edge \((i*,j*)\in E_{*}\), we can directly compute \(\kappa_{\text{FR}}(i*,j*)\) based on the original structure of the graph. However, in terms of stability we are interested in the other members of \(E_{*}\), i.e. edges in the original graph. The analysis of \(E_{*}\) can be split into two cases: one of the nodes is \(i*\) or \(j*\) or neither is.

_Case 1_: WLOG assume \((i,j)=(i*,j)\in E_{*}\). Clearly, \(d^{\prime}_{i}=d_{i}+1\). As for \(|\#^{\prime}_{\Delta_{ij}}|\), this can maximally be increased by 1 in the case that \(j*\in N(j)\), else the triangle count stays the same.

_Case 2_: Let \((i,j)\in E_{*}\) where \(i,j\in V\setminus\{i*,j*\}\). In this case, there is no change to the degree nor the number \(|\#^{\prime}_{\Delta_{uv}}|\).

Thus _Case 1_ defines the bounds which are dictated as follows: if \((i*,j*)\) forms a new triangle, our curvature can increase by 2, and if no triangle is formed the curvature can decrease by 1 in response to the increased degree. Thus we can bound \(\kappa^{\prime}_{\text{FR}}(i,j):=4-d^{\prime}_{i}-d^{\prime}_{j}+3|\#^{\prime} _{\Delta_{ij}}|\) as follows:

\[\kappa_{\text{FR}}(i,j)-1\leq\kappa^{\prime}_{\text{FR}}(i,j)\leq\kappa_{\text {FR}}(i,j)+2\]

The case of **edge deletion** can be handled similarly. Again, we need only consider the edges in \(E_{*}\), as defined in the proof above, and can make the same case argument.

_Case 1_ WLOG assume \((i,j)=(i*,j)\in E_{*}\). Clearly, \(d^{\prime}_{i}=d_{i}-1\). As for \(|\#^{\prime}_{\Delta_{ij}}|\), this can maximally be decreased by 1.

_Case 2_: Let \((i,j)\in E_{*}\) where \(i,j\in V\setminus\{i*,j*\}\). Degree and number of triangles do not change in response to the perturbation.

Again, _Case 1_ gives rise to the following bounds hold for \(\kappa^{\prime}_{\text{FR}}\):

\[\kappa_{\text{FR}}(i,j)-2\leq\kappa^{\prime}_{\text{FR}}(i,j)\leq\kappa_{\text {FR}}(i,j)+1\]

#### b.1.2 Ollivier-Ricci Curvature

The definition of \(\kappa_{\text{OR}}\) establishes a relationship between the graph metric \(d_{G}\), the Wasserstein distance \(W_{1}\), the probability distributions \(\mu_{i},\mu_{j}\) at nodes \(i,j\) and the curvature. Given that we are considering unweighted, and connected graphs we know that \((V,d_{G})\) is a well-defined metric space and therefore \(W_{1}\) (as defined in [55]) defines the \(L_{1}\) transportation distance between two probability measures \(\mu_{i},\mu_{j}\) with respect to the metric \(d_{G}\). This is relevant for a much larger class of graph metrics than just the standard choice of the shortest path distance. We use results from [55] and the metric properties of \(W_{1}\) and \(d_{G}\) on graphs to bound the potential changes in \(\kappa_{\text{OR}}\) following an edge perturbation.

**Lemma 1**.: _Consider the triple \(\mathcal{G}=(G,d_{G},\mu)\). Let \(\delta_{i}\) denote the Dirac measure at node \(i\) and \(J(i)\) := \(W_{1}(\delta_{i},\mu_{i})\) the corresponding jump probability in the graph \(G\). The Ollivier-Ricci curvature\(\kappa_{OR}(i,j)\) satisfies the following Bonnet-Myers inspired upper bound:_

\[\kappa_{OR}(i,j)\leq\frac{J(i)+J(j)}{d_{G}(i,j)} \tag{7}\]

Proof.: Rearranging the original definition for OR curvature gives:

\[W_{1}(\mu_{i},\mu_{j})=d_{G}(i,j)(1-\kappa_{OR}(i,j))\]

By definition of the \(W_{1}\), we have \(d_{G}(i,j)=W_{1}(\delta_{i},\delta_{j})\). Using this and the fact that \(W_{1}\) satisfies the triangle inequality property, we can construct the desired upper bound on \(\kappa_{OR}\):

\[d_{G}(i,j)\leq W_{1}(\delta_{i},\mu_{i})+W_{1}(\mu_{i},\mu_{j})+W_{1}(\delta_{j},\mu_{j})\] \[d_{G}(i,j)\leq J(i)+d_{G}(i,j)(1-\kappa_{OR}(i,j))+J(j)\] \[d_{G}(i,j)(1-(1-\kappa_{OR}(i,j)))\leq J(i)+J(j)\] \[\kappa_{OR}(i,j)\leq\frac{J(i)+J(j)}{d_{G}(i,j)}\]

**Theorem 3**.: _Given a perturbation (either **edge addition** or **edge deletion**) producing \(\mathcal{G}^{\prime}\), the Ollivier-Ricci curvature \(\kappa^{\prime}_{OR}(i,j)\) of a pair \((i,j)\) can be bounded via_

\[1-\frac{1}{d_{G^{\prime}}(i,j)}\big{[}2W^{\prime}_{\max}+W^{\prime}_{1}(\mu_{i },\mu_{j})\big{]}\leq\kappa^{\prime}_{OR}(i,j)\leq\frac{J^{\prime}(i)+J^{ \prime}(j)}{d_{G^{\prime}}(i,j)}, \tag{4}\]

_where \(J^{\prime}(v):=W^{\prime}_{1}(\delta_{v},\mu^{\prime}_{v})\) refers to the new jump probabilities and \(W^{\prime}_{\max}:=\max_{x\in V}W^{\prime}_{1}(\mu_{x},\mu^{\prime}_{x})\) denotes the maximal reaction to the perturbation (measured using the updated Wasserstein distance)._

Proof.: We first prove the _upper bound_. Given that \(G^{\prime}\) is still connected (by assumption), and both \(W^{\prime}_{1}\) and \(d_{G^{\prime}}\) still satisfy the metric axioms, this result follows directly from Lemma1. For proving the _lower bound_, recall from Section3.1 that \(\mathcal{G}^{\prime}=(G^{\prime},d_{G^{\prime}},\mu^{\prime})\) specifies the behaviour of the new graph metric \(d_{G^{\prime}}\) and the and the updated probability measure \(\mu^{\prime}\) in response to the perturbation. Moreover, this defines a new Wasserstein distance \(W^{\prime}_{1}\) and we will show that the maximum reaction (as evaluated by \(W^{\prime}_{1}\)) to the perturbation \(W^{\prime}_{\max}:=\max_{x\in V}W^{\prime}_{1}(\mu^{\prime}_{x},\mu_{x})\) can be used to express a general lower bound for OR curvature in the event of a perturbation. As per Eq.2, we can define our curvature following the perturbation as:

\[\kappa^{\prime}_{OR}(i,j)=1-\frac{1}{d_{G^{\prime}}(i,j)}W^{\prime}_{1}(\mu^{ \prime}_{i},\mu^{\prime}_{j}) \tag{8}\]

Once again, we can make use of the metric properties of \(W^{\prime}_{1}\), to establish the lower bound as

\[\kappa^{\prime}_{OR}(i,j) \geq 1-\frac{1}{d_{G^{\prime}}(i,j)}\big{[}W^{\prime}_{1}(\mu_{i}, \mu^{\prime}_{i})+W^{\prime}_{1}(\mu_{j},\mu^{\prime}_{j})+W^{\prime}_{1}(\mu _{i},\mu_{j})\big{]}\] \[\geq\frac{1}{d_{G^{\prime}}(i,j)}\big{[}2W^{\prime}_{\max}+W^{ \prime}_{1}(\mu_{i},\mu_{j})\big{]}.\]

#### b.1.3 Resistance Curvature

**A brief clarification on inverting edge weights.** The common practice when computing effective resistance is to invert the edge weights of a graph in order to get a resistance. Given the spirit of resistance from circuit theory, we know that a high resistance should make it difficult for current to pass between nodes. Analogously when thinking about our graph as a markov chain, this would correspond to a low transition probability. So, if we think about our edge weights as coming from some kernel where higher similarity results in a higher edge weight, then we should definitely invert our edge weights to get to resistance. However, in the case that our edge weights represent the cost of travelling between nodes, then this is a suitable proxy for resistance in which case inverting the nodes is unnecessary. In order to achieve the theoretical properties of curvature with well known examples described in [19], we _do not_ invert the edge weights in our experiments. Which meansthat the curvature itself interprets the edge weights themselves as a cost/resistance; we think it is an important point to specify especially given the similarity to markov chains and the borrowed terminology from circuit theory.

**Definitions.** The resistance distance, intuitively, measures how well connected two nodes are in a graph. It is defined in [19] as:

\[R_{ij}:=(\mathbf{e}_{i}-\mathbf{e}_{j})^{\intercal}Q^{\dagger}(\mathbf{e}_{i}- \mathbf{e}_{j}) \tag{9}\]

Here \(Q\) is the normalized laplacian (weighted degrees on the diagonal, see [19]), \(Q^{\dagger}\) the Moore-Penrose inverse, and \(\mathbf{e}_{i}\) is \(i^{th}\) unit vector. This is the main feature that will be studied to understand the stability of the curvature measure, and can be computed for any two nodes in a connected component of a graph.

Recalling the equations for node resistance curvature and resistance curvature, i.e. Eq.3, it becomes clear that the main task is to understand _how the resistance distance changes in response to perturbations_. The results below from [46], are crucial for our proofs. Let \(C(i,j)\) be the commute time between nodes \(i,j\in V\). It is important to note that these results depend on the _normalized Laplacian_, defined in [46] as \(N=D^{\frac{1}{2}}AD^{\frac{1}{2}}\), with eigenvalues \(\lambda_{i}\), ordered such that \(\lambda_{1}\geq\lambda_{2}\geq....\). Here, \(D\) is the diagonal matrix with inverse degrees and \(A\) the adjancecy matrix. Also, as is consistent with the rest of the paper, assume our graph has \(n\) nodes and \(m\) edges, and \(d_{i}\) is the degree at node \(i\in V\).

**Proposition 1**.: _For a graph \(G\), let \(N=D^{\frac{1}{2}}AD^{\frac{1}{2}}\) be the normalized Laplacian with eigen values \(\lambda_{1}\geq\lambda_{2}\geq\cdots\geq\lambda_{n}\). Then, the commute time in \(G\) between nodes \(i,j\) is subject to the following bounds:_

\[m\big{(}\frac{1}{d_{s}}+\frac{1}{d_{t}}\big{)}\leq C(i,j)\leq\frac{2m}{1- \lambda_{2}}\big{(}\frac{1}{d_{s}}+\frac{1}{d_{t}}\big{)} \tag{10}\]

**Proposition 2**.: _Consider the unweighted graph G, where each edge represents a unit resistance, i.e we consider each edge in the graph to be artificially weighted with value 1. Then the following equality holds for the commute time between nodes \(i,j\):_

\[C(i,j)=2mR_{ij} \tag{11}\]

**Proposition 3**.: _If \(G^{\prime}\) arises from a graph G by adding a new edge, then the commute time \(C^{\prime}(i,j)\) between any two nodes in \(G^{\prime}\) is bounded by:_

\[C^{\prime}(i,j)\leq(1+\frac{1}{m})C(i,j) \tag{12}\]

For proofs of these propositions, we refer the reader to [46]. These results create a direct connection between commute times and resistance distance, and gives insight into how commute time reacts under edge addition, and we use them directly to generate our bounds for resistance curvature.

**Theorem 4**.: _If \(G^{\prime}\) is the graph generated by **edge addition**, then \(\kappa^{\prime}_{R}\geq\kappa_{R}\),with the following bound:_

\[|\kappa^{\prime}_{R}(i,j)-\kappa_{R}(i,j)|\leq\frac{\Delta_{\mathrm{add}}(d_{i }+d_{j})}{R_{ij}-\Delta_{\mathrm{add}}}, \tag{5}\]

_where \(\Delta_{\mathrm{add}}:=\max_{i,j\in V}\big{(}R_{ij}-\frac{1}{2}\big{(}\frac{1} {d_{i}+1}+\frac{1}{d_{j}+1}\big{)}\big{)}\)._

Proof.: Let \(R^{\prime}_{ij}\) be the resistance distance in \(G^{\prime}\). Likewise, let \(C(i,j)\) be the commute distance in \(G\) between nodes \(i,j\) and \(C^{\prime}(i,j)\) be the commute time in \(G^{\prime}\). Then Eq.11 and Eq.12 ensure that \(R^{\prime}_{ij}\) is bounded above, by the original resistance distance in \(G\):

\[2(m+1)R^{\prime}_{ij} \leq 2m(1+\frac{1}{m})R_{ij}\] \[R^{\prime}_{ij} \leq R_{ij}\]

This follows our intuition of resistance distance very well: with the addition of an edge nodes can only get more connected. Eq.10 also gives a nice lower bound:

\[(m+1)\big{(}\frac{1}{d^{\prime}_{i}}+\frac{1}{d^{\prime}_{j}}\big{)} \leq C^{\prime}(i,j)\] \[\frac{1}{2}\big{(}\frac{1}{d^{\prime}_{i}}+\frac{1}{d^{\prime}_{ j}}\big{)} \leq R^{\prime}_{ij}\]In the case that we are adding a single edge, it is often the case that node degrees remain constant. However, the nodes that are connected by the new edge, \((i*,j*)\in E^{\prime}\setminus E\), increase such that \(d^{\prime}_{i*}=d_{i*}+1\) and \(d^{\prime}_{j*}=d_{j*}+1\). Thus, the following lower bound holds in general for \(R^{\prime}_{ij}\) and we can remain agnostic to the precise location of the new edge:

\[\frac{1}{2}\big{(}\frac{1}{d_{i}+1}+\frac{1}{d_{j}+1}\big{)}\leq R^{\prime}_{ ij}\leq R_{ij} \tag{13}\]

And likewise, after adding \(p\) edges:

\[\frac{1}{2}\big{(}\frac{1}{d_{i}+p}+\frac{1}{d_{j}+p}\big{)}\leq R^{p}_{ij}\leq R _{ij}\]

So the bounds of our _perturbed_ resistance distance \(R^{\prime}_{ij}\) are determined by the initial network structure (\(R_{ij}\)) and the number connections each specific vertex has. Naturally, certain node pairs will be more strongly affected by the addition of an edge. We can define the maximum reaction to perturbation across pairs as follows:

\[\Delta_{add}:=\max_{i,j\in V}\bigg{(}R_{ij}-\frac{1}{2}\big{(}\frac{1}{d_{i}+ 1}+\frac{1}{d_{j}+1}\big{)}\bigg{)} \tag{14}\]

This can be used to bound node resistance curvature. In an unweighted graph, we have

\[p_{i}=1-\frac{1}{2}\sum_{j\sim i}R_{ij}\]

\[p^{\prime}_{i}=1-\frac{1}{2}\sum_{j\sim i}R^{\prime}_{ij}\]

For \(G\) and \(G^{\prime}\) respectively. Given that resistance can only increase, \(p_{i}\) is clearly an lower bound for \(p^{\prime}_{i}\). Certainly an upper bound occurs when when the resistance between each one of i's neighbors maximally decreases. Thus we get the following inequality:

\[p_{i}\leq p^{\prime}_{i}\leq p_{i}+\frac{d_{i}}{2}\Delta_{add} \tag{15}\]

Finally this gives the desired bound on \(\kappa^{\prime}_{\text{R}}\):

\[\kappa_{\text{R}}(i,j)\leq\kappa^{\prime}_{\text{R}}(i,j)\leq\kappa_{\text{R}} (i,j)+\frac{\Delta_{add}(d_{i}+d_{j})}{R_{ij}-\Delta_{add}}\]

**Theorem 7**.: _If \(G^{\prime}\) is the graph generated by **edge deletion**, then \(\kappa^{\prime}_{\text{R}}\leq\kappa_{\text{R}}\), bounded by:_

\[|\kappa^{\prime}_{\text{R}}(i,j)-\kappa_{\text{R}}(i,j)|\leq\frac{1}{R_{ij}+ \Delta_{del}}\Big{[}\frac{2}{R_{ij}}(2R_{ij}+\Delta_{del})(p_{i}+p_{j})-\Delta _{del}(d_{i}+d_{j})\Big{]},\]

_where \(\Delta_{del}=\frac{2}{1-\lambda_{2}}-\min_{i,j\in V}(R_{ij})\) and \(\lambda_{2}\) is the second largest eigenvalue of \(N\)._

Proof.: Now we can beg the question of how effective resistance changes when we remove an edge. By inverting our initial argument in above proof of Theorem 4, we know that after removing an edge our resistance distance can only increase. Formally, \(R_{ij}\leq R^{\prime}_{ij}\). For the upper bound, we can once again make an argument using Eq. (10), this time relying on the other half of the inequality. Here we need to also mention the normalized Laplacian \(N^{\prime}\) for \(G^{\prime}\), with eigenvalues \(\lambda^{\prime}_{1}\geq\lambda^{\prime}_{2}\geq...\geq\lambda^{\prime}_{n}\).

\[C^{\prime}(i,j) \leq\frac{2(m-1)}{1-\lambda_{2}^{\prime}}\big{(}\frac{1}{d_{i}^{ \prime}}+\frac{1}{d_{j}^{\prime}}\big{)}\] \[R_{ij}^{\prime} \leq\frac{1}{1-\lambda_{2}^{\prime}}\big{(}\frac{1}{d_{i}^{\prime }}+\frac{1}{d_{j}^{\prime}}\big{)}\]

Again, we know that only the two unique vertices (\(i*,j*\)) that shared an edge will have affected degrees, s.t \(d_{i*}^{\prime}=d_{i*}-1\) and \(d_{j*}^{\prime}=d_{j*}-1\). Moreover, from Guo et al. [28], we know that \(\lambda_{2}\geq\lambda_{2}^{\prime}\). So we can loosely bound the \(R_{ij}^{\prime}\) as follows:

\[R_{ij}\leq R_{ij}^{\prime}\leq\frac{2}{1-\lambda_{2}} \tag{16}\]

In fact, this applies to any number of edge deletions, as long as \(G^{\prime}\) stays connected. Again, we can define a maximum possible change in resistance distance across the graph:

\[\Delta_{del}=\max_{i,j\in V}(\frac{2}{1-\lambda_{2}}-R_{ij})=\frac{2}{1- \lambda_{2}}-\min_{i,j\in V}(R_{ij}) \tag{17}\]

This leads to the following bounds on node and edge curvature, and completes the proof:

\[p_{i}-\frac{d_{i}}{2}\Delta_{del} \leq p_{i}^{\prime}\leq p_{i}\] \[(1-\lambda_{2})\big{[}p_{i}+p_{j}-\frac{\Delta_{del}}{2}(d_{i}+d _{j})\big{]} \leq\kappa_{\mathsf{R}}^{\prime}(i,j)\leq\kappa_{\mathsf{R}}(i,j)\] \[\kappa_{\mathsf{R}}(i,j)-\frac{1}{R_{ij}+\Delta_{del}}\big{[}\frac {2}{R_{ij}}(2R_{ij}+\Delta_{del})(p_{i}+p_{j})-\Delta_{del}(d_{i}+d_{j})\big{]} \leq\kappa_{\mathsf{R}}^{\prime}(i,j)\leq\kappa_{\mathsf{R}}(i,j)\]

### Expressivity Proofs

**Theorem 5**.: _Given two graphs \(F=(V_{F},E_{F})\) and \(G=(V_{G},E_{G})\) with scalar-valued filtration functions \(f,g\), and their respective persistence diagrams \(D_{f},D_{g}\), we have \(d_{B}(D_{f},D_{g})\geq\inf_{\eta\colon E_{F}\to E_{G}}\sup_{x\in E_{F}}|f(x)-g (\eta(x))|,\) where \(\eta\) ranges over all maps from \(E_{F}\) to \(E_{G}\)._

Proof.: Considering the calculation of persistence diagrams based on scalar-valued filtrations functions, every point in the persistence diagram \(D_{f}\) can be written as a tuple of the form \((f(e_{F}),f(e_{F}^{\prime}))\), with \(e_{F},e_{F}^{\prime}\in E_{F}\); the sample applies for \(D_{g}\). The inner distance between such tuples that occur in the bottleneck distance calculation can thus be written as

\[\|(f(e_{F}),f(e_{F}^{\prime}))-(g(e_{G}),g(e_{G}^{\prime}))\|_{\infty}, \tag{18}\]

which we can rewrite to \(\max_{C\colon E_{F}\to E_{G}}\{f(x)-g(C(x))\}\) for a general map \(C\) induced by the bijection of the bottleneck distance. Not every map is induced by a bijection, though. Hence, if we maximise over _arbitrary_ maps between the edge sets, we are guaranteed to never exceed the bottleneck distance. 

## Appendix C Additional Proofs for Distinguishing Strongly-Regular Graphs

**Theorem 6** (Expressivity of curvature notions).: _Both Forman-Ricci curvature and Resistance curvature_ cannot _distinguish distance-regular graphs with the same intersection array, whereas Ollivier-Ricci curvature_ can _distinguish the Rook and Shrikhande graphs, which are strongly-regular graphs with the same intersection array._

Proof.: We first show the part of the statement relating to the _Forman-Ricci curvature_. Given a distance-regular graph \(G\) with \(N\) vertices and intersection array \(\{b_{0},b_{1},\ldots,b_{D-1};c_{1},c_{2},\ldots,c_{D}\}\). Let \(i,j\) be adjacent nodes in \(G\). For a regular graph, we have \(d_{i}=d_{j}=b_{0}\), where \(b_{0}\) is a constant.

[MISSING_PAGE_FAIL:23]

can also be shown that quadrangles and pentagons influence the OR curvature, further enhancing the expressivity of this type of curvature [37].

This is the most global perspective one can achieve using \(\kappa_{\text{OR}}\) with uniform probability measures, since polygons with more than five edges do not impact the curvature valuation.

However, by changing the probability measure used by \(\kappa_{\text{OR}}\), we can shift the focus towards even larger substructures. For example, the \(n\)th power of the transition matrix provides information on the number of \(n\)-paths and can therefore provide substructure information for cycles of size \(n\)[43]. _Resistance curvature_, by contrast, is biased towards the largest substructures. Due to the 'global' nature of the resistance distance metric, \(\kappa_{\text{R}}\) assigns cycles of size \(\geq 5\) a positive curvature. Moreover, in a locally finite graph, one cannot use \(\kappa_{\text{R}}\) to establish a non-trivial bound on the number of triangles (consider creating an infinite cycle between two nodes).

## Appendix F Probability Measure for Ollivier-Ricci Curvature and Counting Substructures

The Ollivier-Ricci curvature is of particular interest because of its flexibility. While the predominant probability measure \(\mu\) used by the community is _uniform_ for each node, i.e. each of the node's neighbours is chosen with probability being proportional to the degree of the node. We experimented with different probability measures, one being based on expanding \(\mu\) to the two-hop neighbourhood of a vertex, the other one being based on random walk probabilities. Specifically, for a node \(x\) and a positive integer \(m\), we calculate \(\mu_{\text{RW}}\) as

\[\mu_{\text{RW}}(y):=\sum_{k\leq m}\phi_{k}(x,y), \tag{20}\]

with \(\phi_{k}(x,y)\) denoting the probability of reaching node \(y\) in a \(k\)-step random walk that starts from node \(x\). Subsequently, we normalise Eq. (20) to ensure that it is a valid probability distribution. In our experiments, we set \(m=2\), meaning that at most \(2\)-step random walks will be considered. As shown in the main paper, this formulation leads to an increase in expressivity, and we expect that further exploration of the probability measures will be a fruitful direction for the future.

We now explore to what extent the ability of the curvature to count substructures can also be improved in this way. To do this, we used powers of the transition matrix as the probability measure, as it has been shown that the \(n\)th power provides information on the number of \(n\)-paths and can therefore provide substructure information for cycles of size \(n\)[43]. We find that powers of the transition matrix larger than \(1\) can perform better for counting the substructures, particularly for substructures larger than 3-cycles. There is also a difference between Regular and Erdos-Renyi (ER) graphs as the best transition power tends to be higher for ER graphs. We hypothise that this may have something to do with the mixing time of the graph, as large powers should converge to the stationary distribution, and regular graphs are more 'expander-like'. The best results are obtained by taking multiple landscapes using the transition matrix powers (up to \(n=5\)) and then averaging them. We show that combined with a single layer MLP, this method can perform better than using Graph Neural Network based approaches and OR curvature with the uniform measure.

\begin{tabular}{l|l l l l} Method & \multicolumn{3}{c}{Optimal Transition Power ER Regular} \\ \hline Triangle & 2 & 1 & & \\ Tailed Triangle & 4 & 3 & & \\ Star & 4 & 2 & & \\ Chordal Cycle & 2 & 2 & & \\
4-Cycle & 8 & 3 & & \\ \hline \end{tabular}

## Appendix G Computational Complexity

Persistence diagrams of \(1\)-dimensional simplicial complexes, i.e. graphs, can be computed in \(\mathcal{O}(m\log m)\) time where \(m\) denotes the number of edges. Empirically, when calculating different curvature measures for different sizes of graphs, we find that Forman curvature scales well to large graphs, whereas OR and resistance curvatures can be used for smaller graphs and in cases that require a more expressive measure. Note that there are significantly faster ways to calculate resistance curvature as an approximation [65]. A majority of works on GGMs focus on small molecule generation, where any of these curvatures can be used with minimal pre-computation. Table 5 depicts the computational complexity of various curvature calculations on Erdos-Renyi graphs whilst Table 6 and Table 7 compares the complexity to methods based on MMD. We find that calculating persistence diagrams, turning these to persistence landscapes, averaging these and then calculating a distance takes a similar amount of time compared to MMD for different sizes of graphs and for different numbers of graphs in the reference set. Interestingly, our approach scales better than MMD as both the number of graphs in the reference set increases and when the size of the graphs increases. This will be important for comparing distributions of large data sets such as the commonly used Zinc dataset or QM9. Overall, we find that our method can be easily applied in practical use cases, especially given that models for graph generation typically generate graphs with well under 1000 nodes.

\begin{table}
\begin{tabular}{r r r r r r r} \hline \hline Number of Graphs & Degree + MMD & Orbit + MMD & Curvature + MMD & Curvature + Landscapes \\ \hline \(20\) & \(3.6\) & ms & \(217.0\) & ms & \(4.2\) & ms & \(12.5\) & ms \\ \(50\) & \(10.9\) & ms & \(459.0\) & ms & \(12.4\) & ms & \(20.9\) & ms \\ \(100\) & \(34.1\) & ms & \(887.0\) & ms & \(37.9\) & ms & \(34.4\) & ms \\ \(200\) & \(120.0\) & ms & \(1960.0\) & ms & \(133.0\) & ms & \(80.4\) & ms \\ \(500\) & \(678.0\) & ms & \(6740.0\) & ms & \(727.0\) & ms & \(144.0\) & ms \\ \(1000\) & \(2620.0\) & ms & \(19\,900.0\) & ms & \(2680.0\) & ms & \(359.0\) & ms \\ \hline \hline \end{tabular}
\end{table}
Table 6: Computation time for different numbers of Erdős–Rényi graphs in a reference set (\(n=10\) and \(p=0.3\)) with different distribution distance measures

\begin{table}
\begin{tabular}{r r r r} \hline \hline Number of Anghs & Curvature + MMD & Curvature + Landscapes \\ \hline \(10\) & \(2.3\) & ms & \(9.0\) & ms \\ \(20\) & \(4.5\) & ms & \(12.5\) & ms \\ \(50\) & \(15.8\) & ms & \(20.9\) & ms \\ \(100\) & \(93.6\) & ms & \(34.4\) & ms \\ \(200\) & \(556.0\) & ms & \(80.4\) & ms \\ \(500\) & \(727.0\) & msEthical Concerns

We have proposed a general framework for comparing graph distributions focusing primarily on method and theoretical development rather than on potential applications. We currently view drug discovery as being one of the main application areas, where further experiments may be required, but we have no evidence that our method enhances biases or causes harm in any way.