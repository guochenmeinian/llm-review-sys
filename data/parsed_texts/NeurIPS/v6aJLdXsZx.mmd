# arXiVeri: Automatic table verification with GPT

 Gyungin Shin\({}^{1,4}\)&Weidi Xie\({}^{2,3}\)&Samuel Albanie\({}^{4}\)

\({}^{1}\)Visual Geometry Group, University of Oxford

\({}^{2}\)Cooperative Medianet Innovation Center, Shanghai Jiao Tong University

\({}^{3}\)Shanghai AI Laboratory

\({}^{4}\)CAML, University of Cambridge

###### Abstract

Without accurate transcription of numerical data in scientific documents, a scientist cannot draw accurate conclusions. Unfortunately, the process of copying numerical data from one paper to another is prone to human error. In this paper, we propose to meet this challenge through the novel task of _automatic table verification_ (_AutoTV_), in which the objective is to verify the accuracy of numerical data in tables by cross-referencing cited sources. To support this task, we propose a new benchmark, _arXiVeri_, which comprises tabular data drawn from open-access academic papers on arXiv. We introduce metrics to evaluate the performance of a table verifier in two key areas: (i) _table matching_, which aims to identify the source table in a cited document that corresponds to a target table, and (ii) _cell matching_, which aims to locate shared cells between a target and source table and identify their row and column indices accurately. By leveraging the flexible capabilities of modern large language models (LLMs), we propose simple baselines for table verification. Our findings highlight the complexity of this task, even for state-of-the-art LLMs like OpenAI's GPT-4. The code and benchmark is made publicly available.1

Footnote 1: Code and benchmark are available at https://github.com/caml-lab/research/tree/main/arxiveri

## 1 Introduction

Many areas of scientific research employ numerical data to analyse, summarise and communicate findings. When a researcher proposes a new framework, model or algorithm, it is often informative to compare their contribution with prior work by comparing performance metrics. These performance metrics are typically collated in tables that are interleaved with the body of text contained within scientific manuscripts. In practice, to enable the comparison, it is common for the researcher to manually copy performance metrics from the original manuscript into their own manuscript. While pragmatic, this copying process is susceptible to human error. When errors are introduced, the conclusions drawn from the comparisons are also affected. Given the importance of transferring such data correctly, there is a need for mechanisms that ensure its fidelity, but such tooling is not yet available. In short, we lack a "spell checker" for manually copied scientific data.

On first sight, the problem appears simple--after all, verifying that two numbers are equal is not a mathematically complicated task. However, in practice, it is beset with technical difficulties. Tables in the scientific literature are designed to be readable for a human audience rather than machine parsers. As such, they can vary significantly in layout, design, naming convention and manuscript location. The same numerical data may itself be reported at different levels of precision, using percentages, fractions or decimals and in absolute or relative metrics.

To meet these challenges we propose the task of _automatic table verification_--authenticating the numerical data encapsulated in tables by cross-verifying the referred sources. Specifically, we tackle this task with Large Language Models (LLMs) inspired by their strong performance in many text-based processing tasks [33, 31, 28, 18, 6, 19, 30, 21, 4, 22]. To facilitate evaluation of this task and address the incumbent challenges, we introduce _arXiVeri_, a succinct benchmark composed of tabulardata extracted from open-access academic papers on arXiv. We further propose evaluation metrics for gauging the efficacy of the verification system in two key dimensions: _table matching_ and _cell matching_. The former involves identifying the equivalent source table in a cited document for a given target table, while the latter aims to pair shared cells between a target and source table, and accurately identify their respective indices (see Fig. 1). Our experimental findings underscore the complexity inherent to the task (with frontier models such as GPT-4 [18] struggling in many cases), indicating that there is considerable room for further research progress.

Our contributions can be summarised as follows: (i) We introduce a new and challenging task called _Automatic Table Verification_ (AutoTV), paving the way for advancements in automatic data verification in scientific documents; (ii) To stimulate further research in the AutoTV field, we introduce a benchmark dataset named _arXiVeri_, comprised of 3.8K target-source cell pairs and 158 target-source table pairs, sourced from publicly accessible papers on arXiv; (iii) To facilitate the assessment of AutoTV, we define a set of evaluation metrics for table matching and cell matching sub-tasks. In addition, we provide baselines to underpin future comparisons. (iv) Finally, we conduct a range of ablation studies to evaluate the key components of our approach which bring noticeable performance gains.

## 2 Related work

Our work is connected to large language models (LLMs) for scientific research, table detection and table structure recognition, and automating human labour with LLMs, which we describe next.

**Large language models for scientific research.** LLMs have been adapted for scientific research through various avenues, such as utilising models pretrained on scientific text to enhance performance in scientific NLP tasks [2; 29]. There are also notable advances in the biomedical sector, where specialised LLMs pretrained on biomedical text have demonstrated considerable improvements [25]. Additionally, the compilation of extensive academic paper corpora equipped with metadata and structured full text is proving to be an invaluable resource for academic research and text mining [16]. Alongside these advancements, there are investigations into the consequences of model scaling in scientific applications, evaluating the relationship between model size and performance [11]. Our research further develops this field by presenting a new challenge: automatic table verification in scientific documents, highlighting the essential role of data accuracy and integrity via cross-referencing cited sources.

**Tasks for tables in a single document.** Recent advancements in table-related tasks have primarily focused on detecting tables within documents and understanding their structure within a single document. Early efforts developed practical algorithms for detecting tables in heterogeneous documents [24], which later evolved with the incorporation of deep learning, specifically using Convolutional Neural Networks (CNNs), to enhance detection in PDF documents by combining visual features with non-visual information [10]. Subsequent research introduced end-to-end deep learning systems capable of not only detecting tables but also recognising their structure in document

Figure 1: (Left) **Table matching**: given a target table from one paper and a list of source tables from another paper cited in the target table, the verifier needs to identify the source table containing numeric data, specifically floating point numbers, that supports the data presented in the target table. (Right) **Cell matching**: given a target table and a source table, the verifier needs to identify and locate cells that hold the same semantic content in both tables, subsequently outputting the respective row and column indices of these matching cells in each table. The cells that are emphasised in red depict the instances of hard negative cases. Best viewed in colour.

images, without the need for metadata or heuristics [23]. Later work tackled both table detection and structure recognition simultaneously [20]. Prior work has also explored dataset construction for table extraction from unstructured documents [26]. However, the central theme of these works has remained the detection and structural understanding of tables within a single document. In contrast, we focus on table verification _across documents_.

**Task automation with LLMs.** LLMs have significantly impacted various automation tasks. For instance, Codex [5], an LLM fine-tuned on code from GitHub, exhibits proficient Python code generation capabilities, automating a task typically requiring human expertise. In the domain of data annotation, traditionally a labour-intensive task, LLMs like ChatGPT have demonstrated the potential to outperform human crowd-workers in speed, accuracy, and cost-effectiveness [8]. More recently, experiments have been conducted with GPT-4 to assess its ability to assist with Neural Architecture Search (NAS) [34] and interpreting neurons [3]. Our work also targets automation, offering a novel application of LLMs to automate the intricate task of table verification in scientific documents.

## 3 Automatic Table Verification

In this section, we define the proposed task of _automatic table verification_ (Sec. 3.1) and metrics to evaluate the performance of a verifier on this task (Sec. 3.2). Then we describe our approach to tackle AutoTV (Sec. 3.3).

### Task definition

The high-level objective of Automatic Table Verification (AutoTV) is to confirm that a document, referenced in a table (termed the target table) within a separate document, contains a corroborative table (termed the source table) which supports the cited information. When such a source table exists, AutoTV aims to identify matching cells between the source and target tables.

Our focus is particularly on instances within academic papers, where precise referencing of numeric data (e.g., floating point numbers) in tables is vital for comparative analysis. We observe that such in-table citations in academic literature occur for various reasons, including: attributing a specific approach to its original paper and quoting numerical data from an experimental result. The primary focus of table verification is the latter case, where a verifier is tasked with solving two sub-tasks (see Fig. 1): (i) **Table matching**: detecting a table in the cited document that matches a table in the referring document and if no such table exists, stating that there is no match; (ii) **Cell matching**: identifying correspondences between cells with a floating point number in the source and target tables that share the same semantic meaning. This implies not only identical numeric values, but also similar meanings as suggested by their respective table headers. The process includes pinpointing the location of such cells by providing their respective row and column indices in each table.

We note that these sub-tasks pose distinct challenges. First, there may not be a source table that matches the target table (e.g., the table citation may simply attribute to another document rather than quoting numbers). Second, multiple cells within a table (e.g., source table) can share the same numeric value, making it ambiguous how to pair those cells with ones in another table (e.g., target table). Third, a table can have a complex structure, with a single cell spanning across multiple rows and/or columns or featuring multiple headers, making it difficult to identify cell locations.

### Evaluation metrics

To quantitatively measure performance of a verifier on AutoTV, we define four metrics including _table matching accuracy_ for table matching, _cell matching recall_, _cell matching precision_, and _F-1 score_ for cell matching as follows.

**Table matching accuracy** evaluates the verifier's ability to accurately identify a source table that matches a given target table, or to determine that no such source table exists in the cited document. Formally, given a set of all target tables \(\mathcal{T}_{t}\), a target table \(t\in\mathcal{T}_{t}\) with a set of \(N_{t}\) in-table references \(\mathcal{R}_{t}=\{r_{i}|1\leq i\leq N_{t}\}\), a set of candidate source tables \(\mathcal{T}_{s;r_{i}}\) from a cited document \(r_{i}\), and a verifier \(\Phi(\cdot,prompt;\theta)\), the table detection accuracy (\(Acc.\)) is defined as:

\[Acc.\ =\ \frac{\sum\limits_{t\in\mathcal{T}_{t}}\sum\limits_{r_{i}\in \mathcal{R}_{t}}\delta[s_{r_{i};t}=\hat{s}_{r_{i};t}]}{|\mathcal{T}_{t}|}, \quad\hat{s}_{r_{i};t}=\Phi(t,\mathcal{T}_{s;r_{i}},prompt;\theta)\] (1)

where \(\delta[\cdot]\), \(\hat{s}_{r_{i};t}\) and \(s_{r_{i};t}\) denote the Kronecker delta function, the detected source table and the ground-truth source table in the cited document which matches the given target table \(t\), respectively.

**Cell matching recall** quantifies the percentage of target-source cell matches that are accurately identified (i.e., true positives) among a ground-truth set of cell matches across a source table and a target table. Let us denote the ground-truth set of \(N_{r_{i};t}\) paired cells between a target table \(t\) and a source table \(s_{r_{i};t}\) in a cited document \(r_{i}\) as \(\mathcal{C}_{r_{i};t}=\{(c_{t},c_{r_{i};t})_{j}|1\leq j\leq N_{r_{i};t}\}\) and a set of \(\hat{N}_{r_{i};t}\) detected cell matches as \(\hat{\mathcal{C}}_{r_{i};t}=\{(\hat{c}_{t},\hat{c}_{r_{i};t})_{j}|1\leq j\leq \hat{N}_{r_{i};t}\}\) where \(c_{t}\) and \(c_{r_{i};t}\) represent the row and column indices of a cell in the target and source tables, resp. Then, the cell matching recall (\(Recall\)) is defined as:

\[Recall = \frac{\sum\limits_{t\in\mathcal{T}_{t}}\sum\limits_{r_{i}\in \mathcal{R}_{t}}|\mathcal{C}_{r_{i};t}\cap\hat{\mathcal{C}}_{r_{i};t}|}{\sum \limits_{t\in\mathcal{T}_{t}}\sum\limits_{r_{i}\in\mathcal{R}_{t}}|\mathcal{ C}_{r_{i};t}|},\;\hat{\mathcal{C}}_{r_{i};t}=\Phi(t,s_{r_{i};t},prompt;\theta)\] (2)

**Cell matching precision** measures how many target-source cell pairs are true positives among all the detected target-source cell pairs. Using the same notation as above, the cell matching precision (\(Prec.\)) is defined as:

\[Prec. = \frac{\sum\limits_{t\in\mathcal{T}_{t}}\sum\limits_{r_{i}\in \mathcal{R}_{t}}|\mathcal{C}_{r_{i};t}\cap\hat{\mathcal{C}}_{r_{i};t}|}{\sum \limits_{t\in\mathcal{T}_{t}}\sum\limits_{r_{i}\in\mathcal{R}_{t}}|\hat{ \mathcal{C}}_{r_{i};t}|},\;\hat{\mathcal{C}}_{r_{i};t}=\Phi(t,s_{r_{i};t},prompt ;\theta)\] (3)

\(\mathbf{F_{1}}\) **score** is a harmonic mean of the cell matching recall and precision to encapsulate both the measures in a single metric:

\[F_{1}\;score = 2\frac{Prec.\times Recall}{Prec.+Recall}\] (4)

**Remark.** All four metrics have a fixed range of [0, 1], with higher values being better. The text prompt, denoted by \(prompt\), provided to the verifier may vary with the task, i.e., table matching and cell matching.

### Baseline methods

To tackle AutoTV, we propose baseline approaches for table matching and cell matching as follows.

**Table matching.** We utilise a text embedding model (e.g., OpenAI's text-embedding-ada-002) to embed a target table alongside a set of candidate source tables from a document cited in the target table, including their respective captions. It is worth mentioning that the tables are in HTML format by default which we extract during the data collection process (detailed in Sec. 4.1). Subsequently, we rank the candidate tables based on their cosine similarities with the target table in the embedding space, selecting the one with the highest similarity score that also shares at least one

\begin{table}
\begin{tabular}{l l} \hline \hline
**Target-source cell matching** \\ \hline
**Input** & a target table (target\_table), a source table (source\_table) \\ \hline
**System** & You are a helpful assistant. \\
**User** & Compare the following target and source tables and identify cells that contain floating point numbers with the same meaning present in both tables. Return the matched cells in a Python dictionary with the following format: \\  & \{ \\  & (target\_table\_box, target\_table\_column\_index): \\  & (source\_table\_box, source\_table\_column\_index), \\  &... \\  & \} \\  & Use 0-based indexing, including headers, rowspan, and colspan attributes. Locate as many matching cell pairs as possible. If no matches are found, return an empty dictionary ({}). \\  & The target table and its caption: {target\_table} \\  & The source table and its caption: {source\_table} \\ \hline
**GPT-4** & Answer \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Text prompt used for the cell matching task.** We apply a regular expression to the answer string of the model to ensure the final result follows the specified Python dictionary format.

floating point number with the target table. As the final step, we label the prediction as "no match found" if the chosen table's similarity score falls below a specified threshold.

In addition, we consider weighting each candidate table based on the number of floating point numbers that it shares with the target table before ranking them with their cosine similarity. In essence, we multiply the similarity score for a candidate table by a weight, which is determined by the number of floating-point numbers shared with the target table. Specifically, we sort the candidate tables based on the number of shared floats and assign each table with a weight between 0 and 1 according to their rank such that the table with the most shared floats is assigned with 1. These weights are evenly distributed with intervals of 1 divided by the count of candidate tables that share at least one floating point number with the target table. For tables that do not share any floating points, we assign a weight of 0. We show the effect of the weighting in Sec. 5.2.

Importantly, in all cases, floating point numbers are normalised to account for potential unit differences between the source and target tables.

**Cell matching.** We employ GPT-4 to extract matches between target-source cells containing a floating point number present in both target and source tables. As detailed in Sec. 3.2, each match is depicted as a pair of row and column indices for a cell in the target table and its corresponding cell in the source table. To facilitate this, we direct GPT-4 to generate a string representing a Python dictionary where keys denote cell indices in the target table and values represent indices in the source table as shown in Tab. 1. We then extract this dictionary using a regular expression that conforms to a specified dictionary pattern. For cell matching, we experiment with different types of commonly used table formats including HTML, CSV, or Markdown and show the effect of the table format in Sec. 5.2.

## 4 arXiVeri benchmark

Here, we introduce a benchmark composed of academic papers from arXiv, termed _arXiVeri_, for measuring performance of a verifier on the proposed AutoTV task. We first detail the data collection process (Sec. 4.1) and provide statistics of the arXiVeri benchmark (Sec. 4.2).

Figure 2: **Data collection pipeline for the arXiVeri benchmark.** Top: We randomly select open-access papers under the CC-BY license from arXiv and extract tables with in-table references (i.e. target tables) from an HTML5 version of the selected papers. Then, we repeat the process to retrieve the cited papers and their tables. Bottom left: To identify a _candidate_ source table, which supports a target table, we pick one which has the most cells which are shared with the target table. Bottom right: Given the target and the candidate source table, we manually pair the common cells between them. If no paired cells are identified, we conclude that the candidate source table is a false positive and the source paper does not contain any matching source table for the target table. See the text for the details. Best viewed in colour.

### Data collection

As shown in Fig. 2, our data collection process is composed of three steps: (i) target and source paper retrieval, (ii) target-source table matching, and (iii) manual cell paring, as detailed next.

**Target and source paper retrieval.** We begin by collecting recent arXiv papers published in 2022 under the CC-BY license, using the open-source arXiv API.2 We specifically focus on papers categorised as cs.CV, which had the highest number of submissions on arXiv in 2022. We extract tables along with their captions from each paper's HTML5 format using ar5iv,3 which enables us to isolate tables from other elements such as main text by accessing the appropriate HTML tags (e.g., <table>). Subsequently, we employ an Elasticsearch-based arXiv search system to retrieve papers cited in each table (termed the source papers) with their title available in the "References" section of the referring paper (termed the target paper). As the title of a cited paper in the References section is often presented with irrelevant information (e.g., a url to a code) for the search system, we utilise GPT-3.5-turbo to extract the title from the whole reference information of a cited paper. Importantly, to increase the benchmark's complexity, we omit the cited paper if it does not contain a table sharing at least one cell value with the target table, or if it does not contain more than one table.

Footnote 2: https://github.com/lukasschwab/arxiv.py

Footnote 3: https://ar5iv.labs.arxiv.org

**Target-source table matching.** Given a table in a target paper (i.e., the target table) and a source paper which is cited in the target table, we select a table among the set of tables extracted from the source paper (using the same method described above) that supports the target table (i.e., the source table). Specifically, we choose a table that has the highest number of shared floating-point numbers with the target table to be the _candidate_ source table. By iterating through all the references in a target table, we identify a corresponding candidate source table in each cited paper.

It is important to note that a target paper can have multiple tables referring to the same source paper, resulting in several potential table matchings between the target and source papers. In such cases, we choose the matching with the highest number of overlapping floating-point numbers per one target-source paper pair to increase the diversity of papers in the arXiVeri benchmark.

**Manual cell pairing.** In the final step of the collection process, we manually match cells that are commonly found in both target and candidate source tables. To determine a correct cell pair, we compare two cells from the tables and mark them as a match if they meet all of the following conditions:

1. Both cells must represent the same value with an identical meaning, as indicated by their respective row and column headers.
2. Each cell must not contain more than one floating-point number for different metrics, avoiding the use of delimiters such as a comma (',') or a slash ('/').
3. If both cells have the same number of significant digits and the same unit, they must be exactly identical; for example, '12.3' and '12.4' would be treated as an incorrect pair.

The first condition ensures that matched cells have the same meaning as well as value, as determined by their row and column headers. The second condition aims to remove ambiguity during the evaluation step by avoiding cases where a single cell with multiple values is mapped to several cells in another table. The third condition accounts for potential discrepancies in rounding methods or mistakes, requiring matched cells to have the exact values given the same significant digits. If no such cell pairs are found between the target and candidate source tables, we regard the table pair does not have a source table from the source paper for the target table. On the other hand, if there is at least one cell pair, we treat the candidate source table as the source table (for the target table).

**Post-processing.** To ensure that the models used in our experiments can process each table and its caption as input, we filter out tables whose token length, including their captions, exceeds 3,072, as estimated by a tokeniser (i.e., tiktoken4)

Footnote 4: https://github.com/openai/tiktoken

### Statistics

We annotate a total of 3.8K cell pairs from 158 target-source table pairs, involving 110 different target papers and 158 distinct source papers. As illustrated in Fig. 3, we make three observations: (i) source papers contain an average of 4.6 tables, with three being the most frequent number of tables in a source paper; (ii) on average, there are 19.5 cell pairs between a target and a source table with the minimum and maximum number of cell pairs being 1 and 84, resp.; (iii) the dimensions of tables in the dataset exhibit a considerable range, with the smallest table measuring 4 by 5 and the largest reaching 20 by 19. On average, tables tend to fall around the size of 15.9 by 8.0.

## 5 Experiments

In this section, we first provide implementation details in Sec. 5.1 and conduct ablation studies in Sec. 5.2 to investigate each component of our approach for the proposed AutoTV task.

### Implementation details

For the task of table matching, we employ four different text embedding models. These include OpenAI's text-embedding-ada-002 which has an output dimension of 1536, as well as three models from Cohere: embed-multilingual-v2.0, embed-english-v2.0, and embed-english-light-v2.0, with respective output dimensions of 768, 4096, and 1024. For the cell matching task, we employ the gpt-4-0314 model with a maximum length of 8,192 tokens and set the temperature parameter \(\tau\) to 0, unless specified otherwise. To further minimize variability in the model's performance, we report the average score obtained by running each model three times across our experiments.

### Ablation study

**Effect of text embedding models on table matching.** To investigate the influence of selecting different text embedding models, we evaluate four different models, as depicted in Tab. 2 (left). Alongside, we measure the performance of two baseline strategies: (i) "random", which selects a table from a candidate set of source tables, including "no match", and (ii) "overlap", which chooses a table that shares the most floating point numbers with the target table. As can be seen, each of the four embedding models significantly outperforms the baseline strategies by a margin of 12.7-15.8%. Among them, the embed-english-light-v2.0 model from Cohere demonstrates the best performance.

**Effect of weighting on table matching.** As described in Sec. 3.3, we further refine our approach by weighting each candidate table based on the number of shared floating point numbers with the target table. Tab. 2 (right) illustrates the impact of this weighting mechanism on the performance of each embedding model. Notably, implementing this weighting strategy improves performance across all four embedding models, underscoring its effectiveness.

**Effect of table format and providing cell indices.** In the cell matching task, we explore three different table formats--HTML, CSV, and Markdown--for feeding tables to GPT-4. We posit that to enable the model to accurately identify a cell's location, providing explicit row and column indices for each cell could be beneficial. To verify this hypothesis, we also assess performance of the model when row and column indices are explicitly specified on the left and top of a table, resp.

From our results in Tab. 3 (left), we can see that the choice of format significantly influences the model's performance with the HTML format yielding the best performance in the absence of cell indices. We conjecture that this is because the HTML format contains more distinctive delimiters such as <tr> and <td> for table rows and table columns compared to CSV or Markdown where the model has to infer a cell location by counting a line break character and a comma ('.'), which can appear in other parts of the input than the actual table (e.g., text prompt and caption). Indeed, when cell indices are provided with an input table, we can observe that both of the CSV and Markdown formats have significant boost in all of the \(Recall\), \(Prec.\), and \(F_{1}\) metrics, outperforming the HTML format. Examples of each format are provided in the supplementary materials.

**Effect of temperature.** In addition, we experiment with the temperature parameter in GPT-4, which modulates the randomness of the model's output. High values (nearing 1) introduce diversity, while low values (tending towards 0) enhance deterministic behavior. As shown in Tab. 3 (right), we

Figure 3: **Data statistics of the arXiveri dataset.** Left: A histogram illustrating the count distribution of tables within source papers. Middle: A histogram representing 3.8K of shared cells between target and source tables. Right: A distribution plot of table dimensions (rows and columns), with colour indicating table size, and the average dimensions marked in red. Best viewed in colour.

[MISSING_PAGE_EMPTY:8]

**Acknowledgements and disclosure of funding.** This work was performed using resources provided by the Cambridge Service for Data Driven Discovery (CSD3) operated by the University of Cambridge Research Computing Service (www.csd3.cam.ac.uk), provided by Dell EMC and Intel using Tier-2 funding from the Engineering and Physical Sciences Research Council (capital grant EP/T022159/1), and DiRAC funding from the Science and Technology Facilities Council (www.dirac.ac.uk). GS would like to thank Vishaal Udandarao for thorough proof-reading and Zheng Fang for the invaluable support. SA would like to acknowledge the support of Z. Novak and N. Novak in enabling his contribution. SA was supported by a Newton Grant.

## References

* [1]M. Bain, A. Nagrani, G. Varol, and A. Zisserman (2021) Frozen in time: a joint video and image encoder for end-to-end retrieval. arXiv:2104.00650. Cited by: SS1.
* [2]I. Beltagy, K. Lo, and A. Cohan (2019) Scibert: pretrained language model for scientific text. In EMNLP, Cited by: SS1.
* [3]S. Bills, N. Cammarata, D. Mossing, H. Tillman, L. Gao, G. Goh, I. Sutskever, J. Leike, J. Wu, and W. Saunders (2023) Language models can explain neurons in language models. Cited by: SS1.
* [4]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei (2020) Language models are few-shot learners. In NeurIPS, Cited by: SS1.
* [5]M. Chen, J. Tworek, H. Jun, Q. Yuan, H. Ponde de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khalaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. Stuck, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. Hebgen Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba (2021) Evaluating large language models trained on code. arXiv:2107.03374. Cited by: SS1.
* [6]A. Chowdhory, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. Won Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Lemskaya, S. Ghemawat, S. Dey, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omerick, A. M. Dai, T. Sankaranarayana Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean, S. Petrov, and N. Fiedel (2022) Palm: scaling language modeling with pathways. arXiv:2204.02311. Cited by: SS1.
* [7]T. Fu, L. Li, Z. Gan, K. Lin, W. Yang Wang, L. Wang, and Z. Liu (2021) Violet: end-to-end video-language transformers with masked visual-token modeling. arXiv:2111.1268. Cited by: SS1.
* [8]F. Gilard, M. Alizadeh, and M. Kubli (2023) Chatgpt outperforms crowd-workers for text-annotation tasks. arXiv:2303.15056. Cited by: SS1.
* [9]A. Gokul, K. Kalliromitis, S. Li, Y. Kato, K. Kozuka, T. Darrell, and C. J. Reed (2022) Refine and represent: region-to-object representation learning. arXiv:2208.11821. Cited by: SS1.
* [10]L. Hao, L. Gao, X. Yi, and Z. Tang (2016) A table detection method for pdf documents based on convolutional neural networks. In DASW, Cited by: SS1.
* [11]Z. Hong, A. Ajith, G. Pavlakosi, E. Duede, K. Chard, and I. Foster (2023) The diminishing returns of masked language models to science. arXiv:2205.11342. Cited by: SS1.
* [12]O. J. Henaff, S. Koppula, J. Alayrac, A. van den Oord, O. Vinyals, and J. Carreira (2021) Efficient visual pretraining with contrastive detection. arXiv:2103.10957. Cited by: SS1.
* [13]S. Khamekhem Jenni, M. A. Souibgui, Y. Kessentini, and A. Fornes (2021) Enhance to read better: an improved generative adversarial network for handwritten document image enhancement. arXiv:2105.12710. Cited by: SS1.
* [14]Y. Li and Y. Duan (2022) Multi-scale network with attentional multi-resolution fusion for point cloud semantic segmentation. arXiv:2206.13628. Cited by: SS1.

[MISSING_PAGE_POST]

* [17] Muhammad Akhtar Munir, Muhammad Haris Khan, M. Saquib Sarfraz, and Mohsen Ali. Synergizing between self-training and adversarial learning for domain adaptive object detection. _arXiv:2110.00249_, 2021.
* [18] OpenAI. Gpt-4 technical report. _arXiv:2303.08774_, 2023.
* [19] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In _NeurIPS_, 2022.
* [20] Devashish Prasad, Ayan Gadpal, Kshitij Kapadni, Manish Vasive, and Kavira Sultanpure. Cascadetabnet: An approach for end to end table detection and structure recognition from image-based documents. In _CVPRW_, 2020.
* [21] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorina Li, Alguguina Koncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angelikl Lazaridou, Arthur Mensch, Jean-Bapitste Lesqui, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher. _arXiv:2112.11446_, 2022.
* [22] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _JMLR_, 2020.
* [23] Sebastian Schreiber, Stefan Agne, Ivo Wolf, Andreas Dengel, and Sheraz Ahmed. Deepdesrt: Deep learning for detection and structure recognition of tables in document images. In _ICDAR_, 2017.
* [24] Faisal Shafait and Ray Smith. Table detection in heterogeneous documents. In _Table Detection in Heterogeneous Documents_, 2010.
* [25] Hoo-Chang Shin, Yang Zhang, Evelina Bakhturina, Raul Puri, Mostofa Patwary, Mohammad Shoeybi, and Raghav Mani. BioMegatron: Larger biomedical domain language model. In _EMNLP_, 2020.
* [26] Brandon Smock, Rohith Pesala, and Robin Abraham. Pubbats-1m: Towards comprehensive table extraction from unstructured documents. In _CVPR_, 2022.
* [27] Mohamed Ali Souibgui, Sanket Biswas, Sana Khamekhem Jenni, Yousri Kessentini, Alicia Fornes, Josep Llados, and Umapada Pal. Docentr: An end-to-end document image enhancement transformer. _arXiv:2201.10252_, 2022.
* [28] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. _arXiv:2211.09085_, 2022.
* [29] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. _arXiv preprint arXiv:2211.09085_, 2022.
* [30] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Reneilto Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. Lamad: Language models for dialog applications. _arXiv:2201.08239_, 2022.
* [31] Hugo Touvron, Thibaut Lavril, Gautier Izcard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. _arXiv:2302.13971_, 2023.
* [32] Hongwei Xue, Yuchong Sun, Bei Liu, Jianlong Fu, Ruihua Song, Houqiang Li, and Jiebo Luo. Clip-vip: Adapting pre-trained image-text model to video-language representation alignment. _arXiv:2209.06430_, 2023.
* [33] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, PengZhang, Yuxiao Dong, and Jie Tang. Glm-130b: An open bilingual pre-trained model. _arXiv:2210.02414_, 2022.
* [34] Mingkai Zheng, Xiu Su, Shan You, Fei Wang, Chen Qian, Chang Xu, and Samuel Albanie. Can gpt-4 perform neural architecture search? _arXiv:2304.10970_, 2023.
* [35] Yangtao Zheng, Di Huang, Songtao Liu, and Yunhong Wang. Cross-domain object detection through coarse-to-fine feature adaptation. _arXiv:2003.10275_, 2020.

## Appendix A Appendices

In this supplementary material, we first describe the limitations (Sec. A) and broader impacts (Sec. B) of our work. Next, we provide additional statistics for the arXiVeri dataset in Sec. C and show examples of tables in different formats in Sec. D. Then we describe details of the prompts used for our ablation study in Sec. E and provide additional examples of predictions made by GPT-4 on the cell matching task in Sec. F. Lastly, we visualise an actual case where we find human errors in the process of quoting numeric data between tables in Sec. G.

## Appendix A Limitations

We acknowledge several limitations to our approach. First, our task of automatic table verification currently only processes tables in text formats like HTML, CSV, or Markdown. This means our approach may not be suitable for table data embedded within images or PDF files, which are common formats in many documents. Second, the data collection pipeline for our arXiVeri benchmark is specifically designed to operate with arXiv papers that can be successfully transformed from PDF to HTML format via ar5iv. While this conversion allows us to cleanly extract tables with appropriate tags (e.g., <table>), this process may exclude certain papers if the conversion is unsuccessful, which could limit the diversity of table types included in the benchmark and potentially introduce a selection bias. Third, while the benchmark includes data from academic papers on arXiv, it may not fully encapsulate the variety of tables encountered across different domains. This could restrict the generality of our dataset. Future work should aim to extend our data collection pipeline to cater to a broader range of table sources. Lastly, we have limited insight into the GPT-4 inference process via OpenAI's API, and it is unclear if our encoded text is pre-processed or the model's output is post-processed. This can potentially affect the reproducibility of the experiments.

## Appendix B Broader impacts

The automatic table verification proposed in this work has potential utility for many domains. In scientific research, it can reduce human error in data transcription and thus prevent such errors from influencing the interpretation of empirical data. In industries such as finance, healthcare, and engineering, it can ensure data accuracy, preventing costly mistakes.

Turning to negative impacts, the deployment of table verification (particularly while it remains far from perfect) may produce an elevated risk of over-reliance on the technology (with fewer "sanity checks" performed by researchers). More broadly, task automation may contribute to potential job displacement.

Finally, we note that care must be taken to mitigate privacy risks when deploying table verification across sensitive documents.

## Appendix C Additional statistics

Here, we provide further details of statistics for the proposed benchmark, arXiVeri. In Fig. 5, we display the histogram of source paper counts per target table (left) and the distribution of matched cell locations in tables (right). We observe that (i) most target tables have a single source paper which is retrieved through the data collection pipeline (as described in Sec. 4 of the main paper) with the maximum number of source papers being 4; (ii) the locations of paired cells range from 1 to 33 for row indices, and from 1 to 20 for column indices, with an average cell location of approximately 7.3 and 4.9, respectively. Note that this is due to the fact that the average dimensions of the tables in the arXiVeri benchmark is 15.9 by 8.0, causing the average location of matching cells to fall short of the average dimensions of the tables.

Figure 5: **Additional statistics for the arXiVeri benchmark.**

[MISSING_PAGE_EMPTY:13]

[MISSING_PAGE_EMPTY:14]

## Appendix F More qualitative examples

In Fig. 7, we present examples of successful applications of GPT-4 for cell matching, while Fig. 8 showcases two typical instances where GPT-4 failed to correctly perform the task. As observed in the successful cases, the model adeptly pairs cells from the target and source tables based on the shared meaning and value of the cells, even in the presence of hard negatives (i.e., cells highlighted in red) that exhibit the same value but differ in meaning.

In the failure cases, we observe that GPT-4 often matches cells based solely on their meanings, as defined by their respective table headers, despite the fact that the actual cell values between the pair differ. Another frequent type of error involves the model inaccurately locating cell indices.

Figure 7: **Two successful examples of cell matching predicted by GPT-4.** Even though there exist cells in both the target and source tables with identical values but different meanings as indicated by their respective table headers (marked in red), the model appropriately pairs only those cells that share both meaning and value. In both cases, the target tables are located on the left and the source tables on the right. The shaded area in the top left table denote cells that have been omitted for visual clarity. The tables are from [32, 1, 27, 13]. Best viewed in colour.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Method & mIoU & OA & ceil. & floor & wall & beam & col. & wind. & door & chair & table & book. & sofa & board & clut. \\ \hline PointNet [6] & 41.1 & 49.0 & 88.8 & 97.3 & 69.8 & 0.1 & 3.9 & 46.3 & 10.8 & 52.6 & 58.9 & 40.3 & 5.9 & 26.4 & 33.2 \\ KPConv (rigid) [10] & 65.4 & 92.6 & 97.3 & 81.4 & 0.0 & 10.5 & 54.5 & 69.5 & 90.1 & 80.2 & 74.6 & 66.4 & 63.7 & 58.1 \\ KPConv (deform) [10] & 67.1 & 92.8 & 97.3 & 82.4 & 0.0 & 2.9 & 58.0 & 69.0 & 91.0 & 81.5 & 73.3 & 73.4 & 60.7 & 58.9 \\ SPH-GCN [21] & 59.5 & 87.7 & 93.3 & 87.1 & 81.1 & 70.0 & 33.2 & 47.8 & 47.8 & 79.7 & 89.9 & 83.2 & 71.5 & 81.1 & 81.7 \\ SPNet [11] & 69.9 & 90.3 & 94.5 & 98.3 & 84.0 & 0.0 & 24.0 & 59.7 & 99.8 & 89.6 & 81.0 & 55.1 & 52.4 & 80.4 & 80.4 & 80.4 \\ Ours & 70.7 & 91.0 & 94.7 & 98.2 & 86.2 & 10.0 & 45.8 & 56.4 & 79.1 & 81.1 & 83.5 & 93.3 & 79.4 & 79.8 & 83.2 \\ \hline \hline \end{tabular} TABLE 1: Semantic segmentation quantitative predictions for the split [1, 1], tested in the \(m\)-\(m\

[MISSING_PAGE_EMPTY:17]