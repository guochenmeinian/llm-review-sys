# DEFT: Efficient Fine-Tuning of Diffusion Models by Learning the Generalised \(h\)-transform

Alexander Denker

University College London

a.denker@ucl.ac.uk

&Francisco Vargas*

University of Cambridge

fav25@cam.ac.uk

&Shreyas Padhy*

University of Cambridge

sp2058@cam.ac.uk

&Kieran Didi*

University of Cambridge

ked48@cam.ac.uk

&Simon Mathis*

University of Cambridge

svm34@cam.ac.uk

&Vincent Dutordoir

University of Cambridge

vd309@cam.ac.uk

&Riccardo Barbano

Atinary Technologies

rbarbano@atinary.com

&Emile Mathieu

University of Cambridge

ebm32@cam.ac.uk

&Urszula Julia Komorowska

University of Cambridge

ujk21@cam.ac.uk

&Pietro Lio

University of Cambridge

pl219@cam.ac.uk

equal contributions

###### Abstract

Generative modelling paradigms based on denoising diffusion processes have emerged as a leading candidate for _conditional_ sampling in inverse problems. In many real-world applications, we often have access to large, expensively trained unconditional diffusion models, which we aim to exploit for improving conditional sampling. Most recent approaches are motivated heuristically and lack a unifying framework, obscuring connections between them. Further, they often suffer from issues such as being very sensitive to hyperparameters, being expensive to train or needing access to weights hidden behind a closed API. In this work, we unify conditional training and sampling using the mathematically well-understood _Doob's h-transform_. This new perspective allows us to unify many existing methods under a common umbrella. Under this framework, we propose DEFT _(Doob's h-transform Efficient FineTuning)_, a new approach for conditional generation that simply fine-tunes a very small network to quickly learn the conditional \(h\)-transform, while keeping the larger unconditional network unchanged. DEFT is much faster than existing baselines while achieving state-of-the-art performance across a variety of linear and non-linear benchmarks. On _image reconstruction_ tasks, we achieve speedups of up to 1.6\(\times\), while having the best perceptual quality on natural images and reconstruction performance on medical images. Further, we also provide initial experiments on protein motif scaffolding and outperform reconstruction guidance methods.

## 1 Introduction

Denoising diffusion models are a powerful class of generative models where noise is gradually added to data samples until they converge to pure noise. The time reversal of this noising process thenallows noise to be transformed into samples. This process has been widely successful in generating high-quality images [28] and has more recently shown promise in designing protein backbones that have been validated in experimental protein design workflows [77]. Recently, there has been much interest in _conditioning_ the time reversal process, in order to generate samples that are subject to an observed condition. Conditional sampling requires the posterior score \(\nabla_{\bm{x}}\ln p_{t}(\bm{x}|\bm{Y}=\bm{y})\), given some observation \(\bm{y}\). As diffusion models typically approximate the score of the underlying distribution, i.e., \(s_{t}^{\theta^{*}}(\bm{x})\approx\nabla_{\bm{x}}\ln p_{t}(\bm{x})\), a pre-trained diffusion model can be leveraged using Bayes' theorem

\[\nabla_{\bm{x}}\ln p_{t}(\bm{x}|\bm{Y}=\bm{y})\approx s_{t}^{ \theta^{*}}(\bm{x})+\nabla_{\bm{x}}\ln p_{t}(\bm{Y}=\bm{y}|\bm{x}),\] (1)

to approximate the posterior score. The time-dependent likelihood \(\nabla_{\bm{x}}\ln p_{t}(\bm{Y}=\bm{y}|\bm{x})\) is often termed _guidance_ due to its interpretation to guide the reverse process to the conditioned inputs, and is unfortunately analytically intractable. To tackle this problem, several approximations for the _guidance_ have been proposed; see, for example, [12; 22; 29; 56; 65; 69] and further discussion in Appendix B. Instead of relying on the decomposition (1), another line of work aims to learn the posterior score directly [5; 27], which requires expensive training for new conditional sampling tasks, and access to large amounts of paired data points.

In the setting of conditional generation with diffusion models, our primary goal is to leverage large pre-trained foundation models which are prevalent in applications, but which typical front-end users are not able to backpropagate through, making approaches like [12; 22] infeasible. This might be due to their prohibitive computation times or because they lie behind an API preventing the usage of autodiff frameworks.

In this work, we propose a unified framework for conditional generation using Doob's \(h\)-transform, a well-known result in the stochastic differential equations (SDE) literature [14; 55; 61; 78]. Under this framework, we propose DEFT _(Doob's \(h\)-transform Efficient FineTuning)_, an algorithm that estimates the time-dependent likelihood directly from data, i.e., \(h^{*}=\nabla_{\bm{x}}\ln p_{t}(\bm{Y}=\bm{y}|\bm{x})\), while being able to leverage an existing pre-trained unconditional model. We learn the guidance term (\(h\)-transform) efficiently using 1) smaller networks, and 2) a small training dataset of paired data points and corresponding observations. Furthermore, through connections to stochastic control, we propose a novel network architecture for general-purpose fine-tuning, which, in conjunction with our

Figure 1: DEFT reverse diffusion setup. The pre-trained unconditional diffusion model \(s_{t}^{\theta}\) and the fine-tuned \(h\)-transform \(h_{t}^{\phi}\) are combined at every sampling step. We propose a special network to parametrise the \(h\)-transform including the guidance term \(\nabla_{\hat{\bm{x}}_{0}}\ln p(\bm{y}|\hat{\bm{x}}_{0})\) as part of the architecture. Here \(\hat{\bm{x}}_{0}\) denotes the unconditional denoised estimate given \(s_{t}^{\theta}(\bm{x}_{t})\). During training, we only need to fine-tune \(h_{t}^{\phi}\) (usually \(4\)-\(9\%\) the size of \(s_{t}^{\theta}\)) using a small dataset of paired measurements, keeping \(s_{\theta}^{t}\) fixed. During sampling, we do not need to backpropagate through either model, resulting in speed-ups during evaluation.

proposed loss, achieves competitive results across a series of inverse problems in imaging and protein design, while having a much lower computational cost.

## 2 Conditioning diffusions via the \(h\)-transform

In this section, we explore the formal mechanism to condition the boundary points of an SDE mathematically, and connect it to existing methodologies for conditioning diffusions in generative modelling. For a more rigorous background to denoising diffusion models, see Appendix A. Let us first recap the score-based generative modelling framework of [68]; we start with a forward SDE, which progressively transforms the target distribution \(\mathcal{P}_{0}\) (e.g. \(\mathcal{P}_{0}=p_{\mathrm{data}}\))

\[\mathrm{d}\bm{X}_{t}=f_{t}(\bm{X}_{t})\,\mathrm{d}t+\sigma_{t}\overline{ \mathrm{d}\mathbf{W}}_{t},\quad\bm{X}_{0}\sim\mathcal{P}_{0},\] (2)

with drift \(f_{t}\) and diffusion \(\sigma_{t}\). Under some regular assumptions, there exists a corresponding reverse SDE with corresponding drift \(\bar{b}_{t}\)[2], that allows us to take samples from \(\mathcal{P}_{T}\) (typically \(\mathcal{N}(0,\mathbf{I})\)) and denoise them to generate samples from \(\mathcal{P}_{0}\),

\[\mathrm{d}\bm{X}_{t}=\left(f_{t}(\bm{X}_{t})-\sigma_{t}^{2}\nabla_{\bm{X}_{t} }\ln p_{t}(\bm{X}_{t})\right)\,\mathrm{d}t+\sigma_{t}\overline{\mathrm{d} \mathbf{W}}_{t},\quad\bm{X}_{T}\sim\mathcal{P}_{T},\] (3)

where the time flows backwards, and \(\bar{b}_{t}=f_{t}(\bm{X}_{t})-\sigma_{t}^{2}\nabla_{\bm{X}_{t}}\ln p_{t}(\bm{ X}_{t})\). The goal of conditional sampling is to condition the reverse SDE on a particular observation, i.e., to produce samples that satisfy constraints. For example, we might want to use (3) to generate samples where we already know some dimensions of the sample (e.g. knowing some pixels of the image a-priori in image inpainting). Doob's \(h\)-transform [55; 14] provides a formal mechanism for conditioning an SDE to hit an event at a given time. We will show that existing methods for conditional generative modelling arise as approximate instances of this proposed framework. Formally, we have:

**Proposition 2.1**.: _(Doob's \(h\)-transform [55]) Consider the reverse SDE in Eqn. (3). The conditioned process \(\bm{X}_{t}|\bm{X}_{0}\in B\) is a solution of_

\[\mathrm{d}\bm{H}_{t}=\left(\bar{b}_{t}(\bm{H}_{t})-\sigma_{t}^{2}\overline{ \nabla_{\bm{H}_{t}}\ln\bar{p}_{0|t}(\bm{X}_{0}\in B|\bm{H}_{t})}\right)\, \mathrm{d}t+\sigma_{t}\overline{\mathrm{d}\mathbf{W}}_{t},\quad\bm{H}_{T}\sim \mathcal{P}_{T},\] (4)

_with a backward drift \(\bar{b}_{t}(\bm{H}_{t})=f_{t}(\bm{H}_{t})-\sigma_{t}^{2}\nabla_{\bm{H}_{t}}\ln p _{t}(\bm{H}_{t})\), such that \(\mathrm{Law}\left(\bm{H}_{s}|\bm{H}_{t}\right)=\bar{p}_{s|t,0}(\bm{x}_{s}|\bm{x }_{t},\bm{x}_{0}\in B)\) and \(\mathbb{P}(\bm{X}_{0}\in B)=1\)._

Note, that we will refer to the conditional process with \(\bm{H}_{t}\) and to the unconditional process with \(\bm{X}_{t}\). Doob's \(h\)-transform shows that by conditioning a diffusion process to hit a particular event \(\bm{X}_{0}\in B\) at a boundary time, the resulting conditional process is itself an SDE with an _additional drift term_ (shown in the blue box above). Furthermore, the resulting SDE will hit the specified event within a finite time \(T\). The function \(h(t,\bm{H}_{t})\triangleq\overline{P}_{0|t}(\bm{X}_{0}\in B\mid\bm{H}_{t})\) is referred to as the \(h\)_-transform_[55; 14]. See also Appendix C.3 for a discussion about the connection to reconstruction guidance methods.

Rather than conditioning an SDE on a deterministic event, one is often interested in a posterior arising from noisy observations (e.g. noisy inverse problems)

\[\bm{Y}=\mathrm{noisy}(\mathcal{A}(\bm{X}_{0})),\ \bm{X}_{0}\sim p_{\mathrm{data}},\] (5)

where \(\mathcal{A}\) is a forward operator, "noisy" describes a noise process and unlike the classical \(h\)-transform, we are not enforcing a deterministic condition such as \(\mathcal{A}(\bm{X}_{0})=\bm{Y}\). We typically assume we can evaluate and sample from the likelihood \(p(\bm{y}|\bm{X}=\bm{x}_{0})\). Our goal is to sample from the posterior \(p(\bm{x}_{0}|\bm{Y}=\bm{y})=p(\bm{y}|\bm{x}_{0})p_{\mathrm{data}}(\bm{x}_{0})/ p(\bm{y})\). Sampling from the posterior \(p(\bm{x}_{0}|\bm{Y}=\bm{y})\) can be achieved by a generalisation of the \(h\)-transform that build on results in [75], given as follows:

**Proposition 2.2**.: _(Generalised \(h\)-transform) Given the following backwards SDE with marginals \(p_{t}\)_

\[\mathrm{d}\bm{X}_{t}=\bar{b}_{t}(\bm{X}_{t})\,\mathrm{d}t+\sigma_{t}\ \overline{\mathrm{d} \mathbf{W}}_{t},\quad\bm{X}_{T}\sim\mathbb{P}_{T},\] (6)

_then it follows that the backward SDE_

\[\bm{H}_{T} \sim Q_{T}^{f_{t}}[p(\bm{x}_{0}|\bm{y})]=\int\bar{p}_{T|0}(\bm{x} |\bm{x}_{0})p(\bm{x}_{0}|\bm{y})\mathrm{d}\bm{x}_{0}\] \[\mathrm{d}\bm{H}_{t} =\left(\bar{b}_{t}(\bm{H}_{t})-\sigma_{t}^{2}\,\nabla_{\bm{H}_{t}} \ln p_{y|t}(\bm{Y}=\bm{y}|\bm{H}_{t})\,\right)\,\mathrm{d}t+\sigma_{t}\ \overline{ \mathrm{d}\mathbf{W}}_{t},\] (7)_satisfies \(\mathrm{Law}\left(\bm{H}_{0}\right)=p(\bm{x}_{0}|\bm{Y}=\bm{y})\) with \(p_{y|t}(\bm{Y}=\bm{y}|\cdot)\) and \(\int p(\bm{Y}=\bm{y}|\bm{x}_{0})\overline{p}_{0|t}(\bm{x}_{0}|\cdot)\mathrm{d} \bm{x}_{0}\). We recover guidance based diffusions via \(\bar{b}_{t}(\bm{H}_{t})=f_{t}(\bm{H}_{t})-\sigma_{t}^{2}\nabla_{\bm{H}_{t}}\ln p _{t}(\bm{H}_{t})\)._

Here \(Q_{T}^{f_{t}}[\pi(\bm{x}_{0})]=\int\bar{p}_{T|0}(\bm{x}|\bm{x}_{0})\pi(\bm{x} _{0})\mathrm{d}\bm{x}_{0}\) is the transition operator of the forward process. Note, that the initial distribution \(Q_{T}^{f_{t}}[p(\bm{x}_{0}|\bm{y})]\) of the controlled SDE differs from the unconditional SDE. However, in Proposition G.2 we show that for the VP-SDE the difference between them gets exponentially small for increasing \(T\). To summarise, the above result gives a generalisation of the \(h\)-transform that allows sampling from posteriors; notice that it recovers the traditional \(h\)-transform in the no-noise setting. Whilst this more general formulation of the \(h\)-transform has been explored in unconditional generative modelling [78], this is the first work to cast conditional generative modelling in this light. We refer to the term in blue as the _generalised \(h\)-transform_ henceforth.

Proposition 2.2 provides theoretical backing to methodologies such as DPS [12] or IIGDM [65], in which the reverse SDE (7) is used to solve noisy inverse problems. For a careful derivation of Proposition 2.2 see Appendix D. While prior works have explored using Bayes' rule to decompose the conditional score, we provide rigorous arguments for intermediate steps, and carefully formalise the connection between conditional generative modelling and the \(h\)-transform, providing a concise result. This framework is flexible enough to also encompass prior work on conditional score matching, see e.g., [5; 27], and the discussion Appendix H.

## 3 Learning the generalised \(h\)-transform

Prior works either learn the posterior score from scratch, see e.g. [5; 27], or use approximations to the generalised \(h\)-transform, see e.g. [12; 32]. Instead, we propose a method to learn the generalised \(h\)-transform. We refer to this process as fine-tuning, as the pre-trained unconditional network remains unchanged and only the approximation to the generalised \(h\)-transform is learned. Our main result is given in the following theorem, where we give several representations of the generalised \(h\)-transform.

**Theorem 3.1**.: _(Representations of conditional SDE sampling) For a given \(\bm{y}\sim\text{noisy}(\mathcal{A}(\bm{x}_{0}))\), let \(\mathbb{Q}\) be the path measure of the conditional SDE_

\[\mathrm{d}\bm{H}_{t}=\left(f_{t}(\bm{H}_{t})-\sigma_{t}^{2}\left(\nabla_{\bm{ H}_{t}}\ln p_{t}(\bm{H}_{t})+h_{t}(\bm{H}_{t})\right)\right)\mathrm{d}t+ \sigma_{t}\;\overline{\mathrm{d}\mathbf{W}}_{t},\] (8)

_where \(\bm{H}_{T}\sim Q_{T}^{f_{t}}[p(\bm{x}_{0}|\bm{y})]\). The generalised \(h\)-transforms admits the following representations:_

1. _The path measure induced by the_ \(h\)_-transformed SDE satisfies_ \(\mathrm{d}\mathbb{Q}^{*}=\mathrm{d}\mathbb{P}^{\frac{\mathrm{d}p(\bm{x}_{0}| \bm{y})}{\mathrm{d}\mathbb{P}_{0}}}\)_, where_ \(\mathbb{P}\) _is the path measure of the unconditioned SDE and_ \(\mathbb{P}_{0}\) _is it's time_ \(0\) _marginal._
2. _The_ \(h\)_-transform admits a denoising score matching representation_ \[h_{t}^{*}=\operatorname*{arg\,min}_{\begin{subarray}{c}\bm{h}_{t}\in\mathcal{H }\end{subarray}}\mathcal{L}_{\text{SM}}^{\bm{y}}(h_{t})\] \[\mathcal{L}_{\text{SM}}^{\bm{y}}(h_{t})\coloneqq\operatorname*{ \mathbb{E}}_{\begin{subarray}{c}\bm{X}_{0}\sim p(\bm{x}_{0}|\bm{y})\\ t\sim(0,T),\bm{H}_{t}\sim p_{t|0}(\bm{x}_{t}|\bm{x}_{0})\end{subarray}}\left[ \left\|(h_{t}(\bm{H}_{t})+\nabla_{\bm{H}_{t}}\ln p_{t}(\bm{H}_{t}))-\nabla_{ \bm{H}_{t}}\ln\bar{p}_{t|0}(\bm{H}_{t}|\bm{X}_{0})\right\|^{2}\right]\]
3. _The_ \(h\)_-transform admits the following_ **stochastic control** _formulation_ \[h_{t}^{*}=\operatorname*{arg\,min}_{\begin{subarray}{c}\bm{h}_{t}\in\mathcal{H }\end{subarray}}\left\{\mathcal{L}_{\text{SC}}^{\bm{y}}(h_{t})\coloneqq\mathbb{ E}_{\mathbb{Q}}\left[\frac{1}{2}\int_{0}^{T}\sigma_{t}^{2}||h_{t}(\bm{H}_{t})||^{2} \mathrm{d}t\right]-\mathbb{E}_{\bm{H}_{0}\sim\mathbb{Q}_{0}}[\ln p(\bm{y}| \bm{H}_{0})]\right\},\] _where_ \(\mathbb{Q}\) _is the path measure for the conditional SDE being controlled._
4. _The path measure induced by the_ \(h\)_-transformed SDE solves the a Schrodinger bridge problem with boundary conditions_ \(\mathbb{Q}_{0}=Q_{T}^{f_{t}}[p(\bm{x}_{0}|\bm{y})]\approx\mathcal{N}(0,I)\)_,_ \(\mathbb{Q}_{T}=p(\bm{x}_{0}|\bm{y})\) _and with the unconditional process_ \(\mathbb{P}\) _as its reference._

Here, 4) and 1) follow directly from [7; 73]. For the proof 2) see Appendix D.2 and for 3) see Appendix G. Under appropriate conditions on the likelihood, the space of admissible controlscan be taken to be the set of \(C_{1}\)-vector fields with linear growth in space; see [48]. In the following sections, we will discuss the representations in 2) and 3) in more detail.

### DEFT: Fine-tuning by score matching

The score matching objective in Theorem 3.12) offers a simulation-free loss function to estimate the generalised \(h\)-transform. While the theorem's formulation focuses on learning the \(h\)-transform for a specific measurement \(\bm{y}\), this loss function can naturally be extended and amortized over the full range of measurements, i.e.,

\[\min_{h\in\mathcal{H}}\mathbb{E}_{\bm{y}\sim\bm{Y}}[\mathcal{L}_{\text{SM}}^{ \bm{y}}(h)],\] (9)

to obtain \(h_{t}^{*}(\bm{x},\bm{y})=\nabla_{\bm{x}}\ln p_{t}(\bm{y}|\bm{x})\). Further, for settings where the operator may vary, we can additionally amortise over the forward operator \(\mathcal{A}\sim p\) and learn \(h_{t}^{*}(\bm{x},\bm{y},\mathcal{A})=\nabla_{\bm{x}}\ln p_{t}(\bm{y}|\bm{x}, \mathcal{A})\). We exploit this to amortise over inpainting masks, see Section 4.1, and motif scaffolding, see Section 4.3. For the DDPM [28] discretisation of the SDE and a pre-trained epsilon matching model \(\epsilon_{t}^{\theta^{*}}\), the fine-tuning objective (9) reduces to

\[\min_{\phi}\mathbb{E}_{(\bm{X}_{0},\bm{Y}),\epsilon,t}\left[\left\|(h_{t}^{ \phi}(\bm{H}_{t},\bm{Y})+\epsilon_{t}^{\theta^{*}}(\bm{H}_{t}))-\epsilon\right\| ^{2}\right],\] (10)

with \(\bm{H}_{t}=\sqrt{\bar{\alpha}_{t}}\bm{X}_{0}+\sqrt{1-\bar{\alpha}_{t}}\epsilon\), \((\bm{X}_{0},\bm{Y})\sim p(\bm{x}_{0},\bm{y}),\epsilon\sim\mathcal{N}(0,\mathbf{ I})\), where \(h_{t}^{\phi}\) represents the neural network used to approximate the generalised \(h\)-transform. Note that the loss function (10) only requires evaluation of the pre-trained model, without needing to backpropagate through the weights \(\theta^{*}\), which is often quite expensive and sometimes impossible in closed APIs. Training under the DDPM discretisation can be performed according to Algorithm 5. Sampling with DEFT is further explained in Algorithm 6, and pictorially represented in Figure 1. As an additional insight into the behaviour of the \(h\)-transform that makes it more flexible and capable of modelling non-linear tasks than standard reconstruction guidance methods, we show that the \(h\)-transform can be interpreted as a correction term for the Tweedie estimate [20]. We can express the conditional Tweedie's estimate as

\[\mathbb{E}[\bm{x}_{0}|\bm{x}_{t},\bm{y}] \approx\hat{\bm{x}}_{0}(\bm{x}_{t},\bm{y})\] (11) \[=\frac{\bm{x}_{t}-\sqrt{1-\bar{\alpha}_{t}}\left(h_{t}^{\phi^{*} }(\bm{x}_{t},\bm{y})+\epsilon_{t}^{\theta^{*}}(\bm{x}_{t})\right)}{\sqrt{\bar {\alpha}_{t}}}=\hat{\bm{x}}_{0}(\bm{x}_{t})-\frac{\sqrt{1-\bar{\alpha}_{t}}}{ \sqrt{\bar{\alpha}_{t}}}h_{t}^{\phi^{*}}(\bm{x}_{t},\bm{y}),\]

where \(\hat{\bm{x}}_{0}(\bm{x}_{t})\) is the unconditional Tweedie estimate. Equation (11) highlights that the \(h\)-transform can also be interpreted as a correction factor to the unconditional denoised estimate, similar to [52; 80].

### Connections to variational inference and stochastic control

A limitation to the fine-tuning objective with DEFT is that it requires a small dataset of paired datapoints and measurements. In this section, we propose an alternative approach by expressing the solution to the conditional sampling problem as a stochastic optimal control objective, which is highlighted in Theorem 3.13). This allows us to learn the \(h\)-transform by optimising a variational inference-type problem. Importantly, this stochastic control objective only requires the availability of a single noisy observation \(\bm{y}\) instead of a paired fine-tuning dataset. Further, the stochastic control objective can even be used in other conditional sampling tasks, for example in reward tilted distributions, i.e. where the goal is to sample from \(\pi(\bm{x})\propto e^{r(\bm{x})}p_{\text{data}}(\bm{x})\). Here \(e^{r(\bm{x})}\) serves the same purpose as the likelihood, but there is no explicit measurement \(\bm{y}\)[18].

However, the stochastic control objective is not directly applicable for high-dimensional training, as the complete chain \(\{\bm{H}_{t}\}_{t}\) must be kept in memory and backpropagated through or adjoint methods have to be used [36]. In Appendix G.3, we discuss several alternatives and present experiments for scaling up the above objective, e.g., methods like VarGrad [53] and Trajectory Balance [42]. VarGrad allows to detach the trajectory from the gradient computation, drastically reducing the memory footprint. We discuss concurrent work in G.1 and G.2. Further, we show initial experiments for conditional sampling in G.4. The stochastic control objective serves as a conceptual bridge between sampling from unnormalised densities using diffusion models [74; 75; 81] and conditional score-based generative modelling.

### Likelihood-informed inductive bias

If the likelihood is differentiable, we can impose an inductive bias on the \(h\)-transform approximation. Specifically, the generalized \(h\)-transform can be expressed as an expectation, and we can apply the DPS approximation [12] as follows

\[\nabla_{\bm{x}_{t}}\ln p_{y|t}(\bm{y}|\bm{x}_{t})=\nabla_{\bm{x}_{t} }\ln\mathbb{E}_{\bm{x}_{0}\sim p(\bm{x}_{0}|\bm{x}_{t})}[p(\bm{y}|\bm{x}_{0})] \approx\nabla_{\bm{x}_{t}}\ln p(\bm{y}|\mathbb{E}[\hat{\bm{x}}_{0} |\bm{x}_{t}])\] \[\approx\nabla_{\bm{x}_{t}}\ln p(\bm{y}|\hat{\bm{x}}_{0}(\bm{x}_{t })),\]

where we use Tweedie's estimate based on the pre-trained unconditional diffusion model in the last step. The DPS approximation has been validated in many different conditional sampling tasks, so it would make for a good initialisation of the learned \(h\)-transform. However, the DPS approximation requires the Jacobian of the unconditional model, which is expensive to compute and known to be poorly conditioned. Further, in applications where we only have access to the forward pass of the unconditional model, the Jacobian is infeasible to compute. Similar to [50], we found that omitting this term still leads to an expressive architecture, while greatly reducing the computational cost. Thus, we propose the following network architecture

\[h_{t}^{\phi}(\bm{x}_{t},\bm{y})=\text{NN}_{1}^{\phi}(\bm{x}_{t}, \hat{\bm{x}}_{0}(\bm{x}_{t}),\nabla_{\hat{\bm{x}}_{0}}\ln p(\bm{y}|\hat{\bm{x} }_{0}(\bm{x}_{t})),t)+\text{NN}_{2}^{\phi}(t)\nabla_{\hat{\bm{x}}_{0}}\ln p( \bm{y}|\hat{\bm{x}}_{0}(\bm{x}_{t})),\] (12)

to parametrise the \(h\)-transform, where the last layer of \(\text{NN}_{1}^{\phi}\) is initialised with \(\bm{0}\) and \(\text{NN}_{2}^{\phi}\) is initialised to output \(1\). This initialisation provides a computationally efficient approximation to the \(h\)-transform, which still guides the sampling.

This type of network architecture has been proposed within the sampling community to apply diffusion models to normalising constant estimation [49; 74; 81]. The theoretical connection to stochastic control in Section 3.2, motivates us further to adapt the architectures from the sampling field to the conditional generative modelling setting. We ablate the different components of our proposed architecture in Appendix F.1 and find that the additional components greatly improve performance empirically.

## 4 Experiments

We evaluate the DEFT framework from Section 3.1 on both linear and non-linear natural and medical image reconstruction tasks, as well as the motif scaffolding problem in protein design. Further, in Appendix H.2 we provide a comparison of the conditional training framework with DEFT on the Flowers[47] image dataset. We provide our code https://github.com/alexdenker/DEFT.

### Image reconstruction

We test a wide variety of both linear and non-linear image reconstruction tasks on the \(256\times 256\)px ImageNet dataset [58]. We make use of a pre-trained unconditional diffusion model with \(\sim 500\)M parameters [16]2. We perform all our evaluations on a \(1k\) subset of the validation set3. For all inverse problems under consideration, the \(h\)-transform was trained on a separate \(1\)k subset of the validation set. For linear inverse problems, we compare against IIGDM [65], DDRM [32], DPS [12] and RED-diff [44]. Additionally, we evaluate I\({}^{2}\)SB [39]. The performance of I\({}^{2}\)SB can be seen as an upper-bound to DEFT, as it is a conditional diffusion trained on the complete ImageNet dataset. For non-linear tasks, we only compare against DPS and RED-diff as both IIGDM and DDRM are not directly applicable to non-linear forward operators. For DEFT we make use of the DDIM sampling scheme with \(100\) time steps [64]. For the comparison methods we used the same hyperparameters as in [44] without further tuning, including the number of sampling steps (1000 for DPS and RED-Diff, 20 for DDRM and 100 for IIGDM).

Footnote 2: Checkpoints are available at https://github.com/openai/guided-diffusion

Footnote 3: https://bit.ly/eval-pix2pix

We compute PSNR and SSIM, which are commonly used distortion measures, along with perceptual metrics such as Learned Perceptual Image Patch Similarity (LPIPS) [83], Kernel Inception Distance (KID) [8], and top-1 classifier accuracy of a pre-trained ResNet50 model [26]. There is a well-known tradeoff between optimising distortion metrics versus perceptual quality, and depending on the task, one may wish for better performance along one axis at the cost of the other. For natural image tasksinvolving in-painting and super-resolution, it is common to prefer "natural"-looking images, which score better on perceptual similarity, whereas for tasks involving (medical) image reconstruction it is standard to prefer a lower distortion metric [9]. Further, we calculate the total time (including training for DEFT) for evaluation \(1\)k validation images in the "Time (hrs)" row. Furthermore, we report the effective time taken to sample a single image in the "Time per sample (s)" row. This time is calculated by fitting the largest batch size of validation images that fit on a single A100 GPU and dividing the time taken for the batch by the batch size.

InpaintingFirst, we evaluate DEFT on the linear inverse problem of image inpainting. We make use of the inpainting masks for the 1k subset used by [59]\({}^{2}\), which includes masks that obscure \(20\%-30\%\) of the image. Results are shown in Table 1, including the computational time for sampling all \(1000\) validation images. For DEFT, this computational time additionally includes the \(3.9\) hrs of training time of the \(h\)-transform additionally with the \(1.2\) hrs of evaluation. Even with the added training time, we reduce the overall computational time for DEFT, compared to DPS and RED-diff. A visual comparison is provided in Figure 2. Further, in Figure 8 in the Appendix, we show the diversity of samples using different initial seeds. Even though IIGDM and DDRM are faster methods, they perform significantly worse, and are only applicable for linear inverse problems. Inpainting is a task that prefers "natural"-looking image samples, and DEFT outperforms all other methods on perceptual metrics such as LPIPS and KID, being a close second on top-1 accuracy.

Super-resolutionFor another linear inverse problem, we evaluate \(4\)x noiseless super-resolution. Here, the forward operator is given by a bicubic downsampling. The results are presented in Table 1. While DEFT has a lower PSNR compared to the baseline methods, we see significant improvement on perceptual quality metrics (KID, LPIPS, and top-1 accuracy). We show visual results in Figure 9.

High dynamic rangeFor the first non-linear tasks, we make use of the high dynamic range (HDR) task described in [44]. Here, the forward operator is given by \(\mathcal{A}(\bm{x})=\text{clip}(2\bm{x};-1,1)\), where \(\bm{x}\) denotes the RGB image scaled to the range \([-1,1]\). The results are presented in Table 2. We observe

\begin{table}
\begin{tabular}{l c c c c c c c c c c c} \hline \hline  & & \multicolumn{6}{c}{**Inpainting**} & \multicolumn{6}{c}{**Super-resolution**} \\  & DPS & IIGDM & DDRM & RED-diff & DEFT & \(\text{FSB}\) & DPS & IGDM & DDRM & RED-diff & DEFT & \(\text{FSB}\) \\ \hline PSNR (\(\uparrow\)) & 21.27 & 20.30 & 20.72 & **23.29** & 22.18 & 23.26 & 24.83 & 25.25 & 25.32 & **25.95** & 24.92 & **23.95** \\ SSIM (\(\uparrow\)) & 0.67 & 0.82 & 0.83 & **0.87** & 0.85 & 0.86 & 0.71 & 0.73 & 0.72 & **0.75** & 0.71 & 0.64 \\ KID (\(\downarrow\)) & 15.28 & 4.50 & 2.50 & 0.86 & **0.29** & 0.238 & 10.01 & 10.9 & 14.0 & 10.0 & **1.78** & 0.004 \\ LPIPS (\(\downarrow\)) & 0.26 & 0.12 & 0.14 & 0.10 & **0.09** & 0.068 & 0.16 & 0.15 & 0.23 & 0.25 & **0.12** & 0.11 \\ top-1 (\(\uparrow\)) & 58.2 & 67.8 & 68.6 & **72.0** & 71.7 & 74.5 & 71.5 & 71.02 & 63.9 & 66.7 & **71.9** & 71.6 \\ \hline Time (hrs) (\(\downarrow\)) & 30.72 & 2.83 & **0.33** & 7.86 & 5.2 & **N/A** & 30.72 & 2.83 & **0.33** & 7.86 & 5.2 & **N/A** \\ Time per sample (s) (\(\downarrow\)) & 100.6 & 10.2 & 1.22 & 28.3 & 4.36 & **N/A** & 100.6 & 10.2 & 1.22 & 28.3 & 4.36 & **N/A** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Results on inpainting and 4x super-resolution. Best values are shown in **bold**, second best values are underlined. We report both the total time to sample \(1\)k images, and the time per sample in seconds. The time to sample includes the training time for DEFT. These tasks aim to generate “natural”-looking images and therefore perceptual similarity metrics (KID, LPIPS and top-1) are more relevant. I\({}^{2}\)SB (grey column) can be considered an upper bound on performance.

Figure 2: Results for inpainting. We show the ground truth with the inpainting mask superimposed.

that DPS struggles with this specific non-linear tasks, while DEFT achieves good results. We show a visual comparison in the Appendix, see Figure 10.

Phase retrievalThe goal in phase retrieval is to recover the image from intensity measurements only, i.e., the forward operator is given by \(\mathcal{A}(\bm{x})=|\mathcal{F}\bm{x}|\), with \(\mathcal{F}\) as the Fourier transform. We study the same 2x oversampling setting as in [12, 44]. Phase retrieval is a challenging non-linear inverse problem, as the forward operator is invariant to translations, global phase shifts and complex conjugation. In Figure 7 in the appendix, we show samples for different initial seeds and observe a wide variety of image quality. We also observe this behaviour for the baseline methods. However, DEFT is able to achieve better performance compared to RED-diff and DPS, see also Table 2. However, there is room for further improvement to achieve good reconstructions on a consistent basis.

Non-linear deblurringThe non-linear deblurring task was originally proposed by [12]. Here, the forward operator is defined by a trained neural network [70], resulting in a highly non-linear blur. Quantitative results are presented in Table 2. This non-linear reconstruction task was also evaluated for RED-diff in [44]. However, we found that the forward operator of the original implementation4 leads to a nearly trivial reconstruction task. In Appendix F.2, we show results with the code from [44], while Table 2 shows the results with our implementation of the forward operator. Further, in Figure 3 we provide a visual comparison, where DEFT is able to recover the ground truth quite faithfully.

Footnote 4: https://github.com/NVlabs/RED-diff/tree/master

Ablation: Size of fine-tuning datasetAs DEFT requires a dataset for fine-tuning, we ablate the number of training samples. We trained DEFT on a subset of 10, 100 and 200 ImageNet images for Inpainting. We see improvements of all metrics, when training on a larger dataset. The results are presented in Table 3. For the KID, we outperform RED-diff (KID: 0.86) even when trained on only 200 images. However, even with 10 images, we perform quite competitively, showcasing that our method is very sample-efficient when it comes to learning a conditional transform.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{**HDR**} & \multicolumn{3}{c}{**Phase retrieval**} & \multicolumn{3}{c}{**Non-linear Deblurring**} \\  & DPS & RED-diff & DEFT & DPS & RED-diff & DEFT & DPS & RED-diff & DEFT \\ \hline PSNR (\(\uparrow\)) & \(7.94\) & \(25.23\) & \(\bm{28.51}\) & \(9.99\) & \(10.53\) & \(\bm{13.03}\) & \(17.57\) & \(21.21\) & \(\bm{25.16}\) \\ SSIM (\(\uparrow\)) & \(0.21\) & \(0.79\) & \(\bm{0.89}\) & \(0.12\) & \(0.17\) & \(\bm{0.32}\) & \(0.39\) & \(0.53\) & \(\bm{0.79}\) \\ KID (\(\downarrow\)) & \(272.5\) & \(1.2\) & \(\bm{0.10}\) & \(93.2\) & \(114.0\) & \(\bm{80.89}\) & \(12.89\) & \(66.8\) & \(\bm{0.34}\) \\ LPIPS (\(\downarrow\)) & \(0.72\) & \(0.1\) & \(\bm{0.04}\) & \(0.66\) & \(\bm{0.6}\) & \(\bm{0.52}\) & \(0.42\) & \(0.42\) & \(\bm{0.09}\) \\ top-1 (\(\uparrow\)) & \(4.0\) & \(68.5\) & \(\bm{74.0}\) & \(1.5\) & \(7.2\) & \(\bm{13.1}\) & \(30.2\) & \(23.5\) & \(\bm{69.9}\) \\ \hline Time (hrs) (\(\downarrow\)) & \(30.7\) & \(7.9\) & \(\bm{5.2}\) & \(30.7\) & \(7.9\) & \(\bm{5.2}\) & \(30.9\) & \(8.1\) & \(\bm{5.2}\) \\ Time per sample (s) (\(\downarrow\)) & \(100.4\) & \(28.3\) & \(\bm{4.4}\) & \(100.6\) & \(28.4\) & \(\bm{4.4}\) & \(101.2\) & \(30.4\) & \(\bm{4.6}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results on different non-linear image reconstruction tasks. Best values are shown in **bold**, second best values are underlined.

Figure 3: Results for non-linear deblurring. We show both the ground truth, the measurements and samples for DPS, RED-diff and DEFT. DEFT is able to reconstruct high-quality images.

### Computed tomography

We are evaluating DEFT both on the 2016 American Association of Physicists in Medicine (AAPM) grand challenge dataset [45], and the LoDoPab-CT dataset [35], for details see Appendix E.1. For the unconditional models we make use of the attention U-Net architecture [16]. For the model trained on AAPM, we use exactly the same architecture (\(\approx 374\) params.) as in [11], while for LoDoPab-CT we use a smaller model (\(\approx 133\)M params.). For the forward operator, we use a parallel-beam radon transform with \(60\) angles and add Gaussian noise with \(\sigma_{y}=1.0\), which corresponds to approx. \(3.5\%\) relative noise. We compare against DPS [12] and RED-diff [44], where the parameters were obtained using a grid search on a subset of the validation set to maximise the PSNR. In Table 4 we present PSNR and SSIM, in addition to the sampling time, and provide a visual comparison Figure 4. For both datasets, we choose the same DEFT architecture with about \(23\)M parameters.In the Appendix F, we perform an ablation regarding the parametrisation of DEFT, see Table 6. In particular, these results show the necessity of providing the unconditional Tweedie estimate \(\hat{\bm{x}}_{0}\) as input to the \(h\)-transform in (12). We observe almost a \(8\)dB difference in PSNR for models without the Tweedie estimate and the log-likelihood term.

### Conditional protein design: motif scaffolding

We evaluate DEFT on the contiguous motifs of the RFDiffusion benchmark [77]. In this motif scaffolding task, we sample protein \(C_{\alpha}\) atom coordinates \(\bm{x}\in\mathbb{R}^{d}\) such that the generated backbone contains a targeted motif, i.e. a subset of \(C_{\alpha}\) coordinates \(\bm{y}\in\mathbb{R}^{n}\), similar to an image outpainting task. The forward operator is therefore given by \(\bm{y}=\mathcal{A}(\bm{x})=\Lambda\bm{x}\), where \(\mathrm{A}\in\{0,1\}^{n\times d}\) denotes a masking matrix which only selects the \(n\) observed \(C_{\alpha}\) coordinates.

We leverage the pretrained Genie diffusion model which is an unconditional model for protein backbone generation [37]. To apply DEFT to it, we use a downsized version of the unconditional base model as our \(h\)-transform model which only uses 200k instead of the original 4.1M parameters. To adopt this model to the DEFT algorithm, we modify the SE(3)-invariant encoder by adding additional conditional pair feature networks for the motif coordinates as well as the unconditional Tweedie estimate \(\hat{\bm{x}}_{0}\), similar to the setting in the previous experiments. As per Section 3.3, we add a time-dependent likelihood approximation term to the \(h\)-transform network. We train the \(h\)-transform

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{3}{c}{AAPM} & \multicolumn{3}{c}{LoDoPab-CT} \\  & DPS & RED-diff & DEFT & DPS & RED-diff & DEFT \\ \hline PSNR & \(33.11\) & \(\bm{34.85}\) & \(34.73\) & \(34.16\) & \(34.95\) & \(\bm{35.81}\) \\ SSM & \(0.855\) & \(0.865\) & \(\bm{0.887}\) & \(0.846\) & \(0.849\) & \(\bm{0.876}\) \\ Time (s) & \(208.9\) & \(83.4\) & \(\bm{16.3}\) & \(156.8\) & \(70.1\) & \(\bm{13.8}\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results for CT on AAPM and LoDoPab-CT and sampling time per image on a single GeForce RTX 3090. Best values are shown in **bold**, second best values are underlined. For DEFT we use \(100\) DDIM steps, while RED-diff and DPS use \(1000\) time steps.

Figure 4: Reconstructions for computed tomography on LoDoPab-CT

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multicolumn{4}{c}{**DEFT on ImageNet for Inpainting**} \\ Number of images & 10 & 100 & 200 & 1000 \\ \hline PSNR \((\uparrow)\) & \(20.87\) & \(20.99\) & \(22.11\) & \(22.18\) \\ SSIM \((\uparrow)\) & \(0.83\) & \(0.84\) & \(0.847\) & \(0.85\) \\ KID \((\downarrow)\) & \(1.85\) & \(0.978\) & \(0.401\) & \(0.29\) \\ LPIPS \((\downarrow)\) & \(0.123\) & \(0.112\) & \(0.096\) & \(0.09\) \\ top-1 \((\uparrow)\) & \(68.8\) & \(69.6\) & \(70.6\) & \(71.7\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Varying the size of the fine-tuning dataset for DEFT for Inpainting on ImageNet.

network on the same SCOPe dataset as in [17]. More details on the training details can be found in App. E.2. We compare DEFT against DPS [12] and a previously published version of Genie that was trained in an amortised fashion [17]. The guidance parameter of DPS was tuned over 5 different experiment runs. The amortised model serves here as an upper limit of how well DEFT can perform with Genie as a base model.

The overall in-silico success, defined by scRNA\(<2\)A and motifRMSD\(<1\)A, is provided in Figure 5. In the Appendix, we provide a detailed breakdown of these results, see Figure 13. Further, in Figure 13 and Figure 15 we provide a comparison of the task 1YCR for the different methods. We observe that DEFT outperforms DPS, solving 10 out of the 12 tasks compared to only 5 tasks for DPS. While it has lower success rates than the amortised model, it still solves all but two tasks in that benchmark with only 9% of the parameter count and significantly shorter training time compared to the amortised model (\(800\) epochs for DEFT vs \(2100\) epochs for amortised). The low performance of DPS indicates that the base Genie model is limiting the performance here and may partly explain the performance difference between DEFT and amortised training. Exploring DEFT with a more capable base model is therefore another promising avenue for research. Excitingly, the lower training time and data requirements of DEFT enable fine-tuning a model on specific protein families for particular applications, a task that is left for future work.

## 5 Conclusion

We presented a unified mathematical framework, based on Doob's \(h\)-transform, to better understand and classify different conditional diffusion methods. Under this framework, we proposed DEFT, a novel parameter-efficient conditional fine-tuning method that does not require backpropagation through large pre-trained score networks, resulting in efficient sampling. We evaluated DEFT on several image reconstruction tasks and showed that it reliably outperformed standard methods, both in time, reconstruction quality and perceptual similarity metrics. While DEFT requires additional training on a small dataset of paired measurements, we find that it is still faster than many existing baselines due to being able to use fewer sampling steps during evaluation, and not needing to backpropagate during evaluation.

Limitations and future workThe DEFT framework uses a (small) fine-tuning dataset, in contrast to zero-shot conditional sampling approaches, e.g., DPS [12] or IIGDM [65]. Fine-tuning on small datasets may have the risk of overfitting to biases inherent in the data. In contrast to zero-shot conditional sampling, DEFT assumes no knowledge of the forward operator. However, the forward operator can be incorporated as an inductive bias within the network architecture to improve performance. We also proposed a zero-shot approach through the optimal control loss in Section 3.2, which only needs a single observation \(\bm{y}\) to learn the \(h\)-transform. Though we show promising results scaling this approach to the MNIST dataset in Appendix H, the computational burden of simulating the full SDE at each iteration is still high, which might make this optimal control loss infeasible for high-dimensional data. However, there is promising recent work on partial trajectory optimisation [79], which may reduce the computational burden of the stochastic control objective, making it competitive with existing methods.

Figure 5: Comparison of DPS, DEFT and amortised training for motif scaffolding for 12 contiguous targets. 4% and 9% are the relative sizes of the h-transform compared to the unconditional model.

## Acknowledgements

Alexander Denker acknowledges support by the EPSRC programme grant EP/V026259/1. Shreyas Padhy is funded by the University of Cambridge Harding Distinguished Postgraduate Scholars Programme.

## References

* Albergo et al. [2023] Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework for flows and diffusions. _arXiv preprint arXiv:2303.08797_, 2023.
* Anderson [1982] Brian D.O. Anderson. Reverse-time diffusion equation models. _Stochastic Processes and their Applications_, 12(3):313-326, 1982.
* Arridge et al. [2019] Simon Arridge, Peter Maass, Ozan Oktem, and Carola-Bibiane Schonlieb. Solving inverse problems using data-driven models. _Acta Numerica_, 28:1-174, 2019.
* Bakry et al. [2014] Dominique Bakry, Ivan Gentil, Michel Ledoux, et al. _Analysis and geometry of Markov diffusion operators_, volume 103. Springer, 2014.
* Batzolis et al. [2021] Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schonlieb, and Christian Etmann. Conditional image generation with score-based diffusion models. _arXiv preprint arXiv:2111.13606_, 2021.
* Berner et al. [2022] Julius Berner, Lorenz Richter, and Karen Ullrich. An optimal control perspective on diffusion-based generative modeling. In _NeurIPS 2022 Workshop on Score-Based Methods_, 2022.
* Bernton et al. [2019] Espen Bernton, Jeremy Heng, Arnaud Doucet, and Pierre E Jacob. Schrodinger bridge samplers. _arXiv preprint_, 2019.
* Binkowski et al. [2018] Mikolaj Binkowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. _arXiv preprint arXiv:1801.01401_, 2018.
* Blau and Michaeli [2018] Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 6228-6237, 2018.
* De Bortoli [2022] Valentin De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. _Transactions on Machine Learning Research_, 2022. ISSN 2835-8856.
* Chung et al. [2022] Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving diffusion models for inverse problems using manifold constraints. In _NeurIPS_, 2022.
* Chung et al. [2023] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. In _ICLR_, 2023.
* Clark et al. [2024] Kevin Clark, Paul Vicol, Kevin Swersky, and David J. Fleet. Directly fine-tuning diffusion models on differentiable rewards. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=1vmSEVL19f.
* De Bortoli et al. [2021] Valentin De Bortoli, Arnaud Doucet, Jeremy Heng, and James Thornton. Simulating diffusion bridges with score matching. _arXiv preprint arXiv:2111.07243_, 2021.
* De Bortoli et al. [2021] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion Schrodinger bridge with applications to score-based generative modeling. _NeurIPS_, 2021.
* Dhariwal and Nichol [2021] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _NeurIPS_, 2021.
* Didi et al. [2023] Kieran Didi, Francisco Vargas, Simon V Mathis, Vincent Dutordoir, Emile Mathieu, Urszula J Komorowska, and Pietro Lio. A framework for conditional diffusion modelling with applications in motif scaffolding for protein design. _arXiv preprint arXiv:2312.09236_, 2023.
* Domingo-Enrich et al. [2024] Carles Domingo-Enrich, Michal Drozdzal, Brian Karrer, and Ricky TQ Chen. Adjoint matching: Fine-tuning flow and diffusion generative models with memoryless stochastic optimal control. _arXiv preprint arXiv:2409.08861_, 2024.

* Dutordoir et al. [2023] Vincent Dutordoir, Alan Saul, Zoubin Ghahramani, and Fergus Simpson. Neural diffusion processes. In _ICML_, pages 8990-9012. PMLR, 2023.
* Efron [2011] Bradley Efron. Tweedie's formula and selection bias. _Journal of the American Statistical Association_, 106(496):1602-1614, 2011.
* Fienup [1982] James R Fienup. Phase retrieval algorithms: a comparison. _Applied optics_, 21(15):2758-2769, 1982.
* Finzi et al. [2023] Marc Anton Finzi, Anudhyan Boral, Andrew Gordon Wilson, Fei Sha, and Leonardo Zepeda-Nunez. User-defined event sampling and uncertainty quantification in diffusion models for physical dynamical systems. In _ICML_, pages 10136-10152. PMLR, 2023.
* Fleming and Rishel [2012] Wendell H Fleming and Raymond W Rishel. _Deterministic and stochastic optimal control_, volume 1. Springer Science & Business Media, 2012.
* Han et al. [2022] Xizewen Han, Huangjie Zheng, and Mingyuan Zhou. Card: Classification and regression diffusion models. _NeurIPS_, 35:18100-18115, 2022.
* Hauptmann et al. [2020] Andreas Hauptmann, Jonas Adler, Simon Arridge, and Ozan Oktem. Multi-scale learned iterative reconstruction. _IEEE transactions on computational imaging_, 6:843-856, 2020.
* He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* Ho and Salimans [2021] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In _NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications_, 2021.
* Ho et al. [2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _NeurIPS_, 33:6840-6851, 2020.
* Jalal et al. [2021] Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G Dimakis, and Jon Tamir. Robust compressed sensing mri with deep generative priors. _NeurIPS_, 34:14938-14954, 2021.
* Jumper et al. [2021] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zidek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. _Nature_, 596(7873):583-589, 2021.
* Kappen [2005] Hilbert J Kappen. Linear theory for control of nonlinear stochastic systems. _Physical review letters_, 95(20):200201, 2005.
* Kawar et al. [2022] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models. In _ICLR Workshop on Deep Generative Models for Highly Structured Data_, 2022.
* Kingma and Ba [2015] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _ICLR 2015_, 2015.
* LeCun and Cortes [2010] Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.lecun.com/exdb/mnist/.
* Leuschner et al. [2021] Johannes Leuschner, Maximilian Schmidt, Daniel Otero Baguer, and Peter Maass. Lodopab-ct, a benchmark dataset for low-dose computed tomography reconstruction. _Scientific Data_, 8(1):109, 2021.
* Li et al. [2020] Xuechen Li, Ting-Kam Leonard Wong, Ricky T. Q. Chen, and David K. Duvenaud. Scalable gradients and variational inference for stochastic differential equations. In _Symposium on Advances in Approximate Bayesian Inference_, pages 1-28. PMLR, 2020.
* Lin and Alquraishi [2023] Yeqing Lin and Mohammed Alquraishi. Generating novel, designable, and diverse protein structures by equivariantly diffusing oriented residue clouds. In _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 20978-21002. PMLR, 23-29 Jul 2023.

* [38] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. _arXiv preprint arXiv:2210.02747_, 2022.
* [39] Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos A. Theodorou, Weili Nie, and Anima Anandkumar. I2sb: image-to-image schrodinger bridge. In _Proceedings of the 40th International Conference on Machine Learning_, ICML'23. JMLR.org, 2023.
* [40] Xingchao Liu and Lemeng Wu. Learning diffusion bridges on constrained domains. In _ICLR_, 2023.
* [41] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. _arXiv preprint arXiv:2209.03003_, 2022.
* [42] Nikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory balance: Improved credit assignment in gflownets. _NeurIPS_, 35:5955-5967, 2022.
* [43] Nikolay Malkin, Salem Lahlou, Tristan Deleu, Xu Ji, Edward Hu, Katie Everett, Dinghuai Zhang, and Yoshua Bengio. Gflownets and variational inference. _arXiv preprint arXiv:2210.00580_, 2022.
* [44] Morteza Mardani, Jiaming Song, Jan Kautz, and Arash Vahdat. A variational perspective on solving inverse problems with diffusion models. In _ICLR_, 2024.
* [45] Cynthia H McCollough, Adam C Bartley, Rickey E Carter, Baiyu Chen, Tammy A Drees, Phillip Edwards, David R Holmes III, Alice E Huang, Farhana Khan, Shuai Leng, et al. Low-dose ct for the detection and classification of metastatic liver lesions: results of the 2016 low dose ct grand challenge. _Medical physics_, 44(10):e339-e352, 2017.
* [46] Xiangming Meng and Yoshiyuki Kabashima. Diffusion model based posterior sampling for noisy linear inverse problems. _arXiv preprint arXiv:2211.12343_, 2022.
* [47] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In _Indian Conference on Computer Vision, Graphics and Image Processing_, Dec 2008.
* [48] Nikolas Nusken and Lorenz Richter. Solving high-dimensional Hamilton-Jacobi-Bellman PDEs using neural networks: perspectives from the theory of controlled diffusions and measures on path space. _Partial Differential Equations and Applications_, 2(4):1-48, 2021.
* [49] Angus Phillips, Hai-Dang Dau, Michael John Hutchinson, Valentin De Bortoli, George Deligiannidis, and Arnaud Doucet. Particle denoising diffusion sampler. _arXiv preprint arXiv:2402.06320_, 2024.
* [50] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In _ICLR_, 2023.
* [51] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 8821-8831. PMLR, 18-24 Jul 2021.
* [52] Sriram Ravula, Brett Levac, Ajil Jalal, Jon Tamir, and Alex Dimakis. Optimizing sampling patterns for compressed sensing MRI with diffusion generative models. In _NeurIPS 2023 Workshop on Deep Learning and Inverse Problems_, 2023.
* [53] Lorenz Richter, Ayman Boustati, Nikolas Nusken, Francisco Ruiz, and Omer Deniz Akyildiz. Vargrad: a low-variance gradient estimator for variational inference. _NeurIPS_, 33:13481-13492, 2020.
* [54] Lorenz Richter, Julius Berner, and Guan-Horng Liu. Improved sampling via learned diffusions. _arXiv preprint arXiv:2307.01198_, 2023.
* [55] L Chris G Rogers and David Williams. _Diffusions, Markov processes and martingales: Volume 2, Ito calculus_, volume 2. Cambridge university press, 2000.

* [56] Litu Rout, Negin Raoof, Giannis Daras, Constantine Caramanis, Alex Dimakis, and Sanjay Shakkottai. Solving linear inverse problems provably via posterior sampling with latent diffusion models. _NeurIPS_, 36, 2024.
* [57] Francois Rozet and Gilles Louppe. Score-based data assimilation. _arXiv preprint arXiv:2306.10574_, 2023.
* [58] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. _IJCV_, 115:211-252, 2015.
* [59] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In _ACM SIGGRAPH 2022 conference proceedings_, pages 1-10, 2022.
* [60] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. _TPAMI_, 45(4):4713-4726, 2022.
* [61] Simo Sarkka and Arno Solin. _Applied stochastic differential equations_, volume 10. Cambridge University Press, 2019.
* [62] Mathis Simon V, Julia Komorowska Urszula, Jamnik Mateja, and Lio Pietro. Normal mode diffusion: Towards dynamics-informed protein design. _The 2023 ICML Workshop on Computational Biology. Baltimore, Maryland, USA, 2023. C_, 2023.
* [63] Vignesh Ram Somnath, Matteo Pariset, Ya-Ping Hsieh, Maria Rodriguez Martinez, Andreas Krause, and Charlotte Bunne. Aligned diffusion schrodinger bridges. _arXiv preprint arXiv:2302.11419_, 2023.
* [64] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _ICLR_, 2021.
* [65] Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion models for inverse problems. In _ICLR_, 2022.
* [66] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. _Advances in neural information processing systems_, 33:12438-12448, 2020.
* [67] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _ICLR_, 2021.
* [68] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _ICLR_, 2021.
* [69] Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solving inverse problems in medical imaging with score-based generative models. In _ICLR_, 2022.
* [70] Phong Tran, Anh Tuan Tran, Quynh Phung, and Minh Hoai. Explore image deblurring via encoded blur kernel space. In _Proceedings of the IEEE/CVF CVPR_, pages 11956-11965, 2021.
* [71] Belinda Tzen and Maxim Raginsky. Theoretical guarantees for sampling and inference in generative models with latent diffusions. In _Conference on Learning Theory_, pages 3084-3114. PMLR, 2019.
* [72] Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel Lee Diamant, Alex M Tseng, Tommaso Biancalani, and Sergey Levine. Fine-tuning of continuous-time diffusion models as entropy-regularized control. _arXiv preprint arXiv:2402.15194_, 2024.
* [73] Francisco Vargas, Pierre Thodoroff, Austen Lamacraft, and Neil Lawrence. Solving Schrodinger bridges via maximum likelihood. _Entropy_, 23(9):1134, 2021.

* [74] Francisco Vargas, Will Sussman Grathwohl, and Arnaud Doucet. Denoising diffusion samplers. In _ICLR_, 2023.
* [75] Francisco Vargas, Andrius Ovsianas, David Fernandes, Mark Girolami, Neil D Lawrence, and Nikolas Nusken. Bayesian learning via neural Schrodinger-Follmer flows. _Statistics and Computing_, 33(1):3, 2023.
* [76] Francisco Vargas, Shreyas Padhy, Denis Blessing, and Nikolas Nusken. Transport meets variational inference: Controlled monte carlo diffusions. In _ICLR_, 2024.
* [77] Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo design of protein structure and function with rdfdiffusion. _Nature_, pages 1-3, 2023.
* [78] Mao Ye, Lemeng Wu, and Qiang Liu. First hitting diffusion models for generating manifold, graph and categorical data. In _NeurIPS_, 2022.
* [79] Dinghuai Zhang, Ricky Tian Qi Chen, Cheng-Hao Liu, Aaron Courville, and Yoshua Bengio. Diffusion generative flow samplers: Improving learning signals through partial trajectory optimization. _arXiv preprint arXiv:2310.02679_, 2023.
* [80] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In _Proceedings of the IEEE/CVF ICCV_, pages 3836-3847, 2023.
* [81] Qinsheng Zhang and Yongxin Chen. Path integral sampler: A stochastic control approach for sampling. In _ICLR_, 2022. URL https://openreview.net/forum?id=_uCb2ynRu7Y.
* [82] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. _arXiv preprint arXiv:2204.13902_, 2022.
* [83] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 586-595, 2018.
* [84] Linqi Zhou, Aaron Lou, Samar Khanna, and Stefano Ermon. Denoising diffusion bridge models. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=FKKsTayvGo.

Background on diffusion formulations

### Recap - continuous and discrete diffusion formulations

The discretised DDPM versions with various discrete time schedules amount to the time-dependent OU process

\[\mathrm{d}\mathbf{X}_{t}=-\frac{\beta(t)}{2}\bm{X}_{t}\mathrm{d}t+ \sqrt{\beta(t)}\,\overline{\mathrm{d}\mathbf{W}}_{t},\] (13)

where choosing different time schedules amounts to choosing different functions \(\beta(t)\). This process gives rise to the following transition probabilities

\[p(\bm{x},t|\bm{x}_{0},0) =\overline{p}_{t|0}(\bm{x}|\bm{x}_{0})\] (14) \[=\mathcal{N}\left(\bm{x}_{0}e^{-\int_{0}^{T}\frac{\beta(t)}{2} \mathrm{d}s},\bm{I}\int_{0}^{T}\beta(t)e^{-\int_{0}^{T-t}\beta(s)\mathrm{d}s} \mathrm{d}t\right)\] (15) \[=\mathcal{N}\left(\bm{x}_{0}e^{-\int_{0}^{T}\frac{\beta(s)}{2} \mathrm{d}s},\left(1-e^{-\int_{0}^{T}\beta(s)\mathrm{d}s}\right)\bm{I}\right),\] (16)

see also Appendix B in [67]. With \(\bar{\alpha}(t)=e^{-\int_{0}^{T}\beta(s)\mathrm{d}s}\), this is the familiar form ([28]):

\[p(\bm{x},t|\bm{x}_{0},0)=\overline{p}_{t|0}(\bm{x}|\bm{x}_{0})= \mathcal{N}\left(\bm{x}_{0}\sqrt{\bar{\alpha}(t)},(1-\bar{\alpha}(t))\bm{I} \right),\] (17)

with \(\bar{\alpha}(t)\) time-dependent and we can therefore choose different functional forms for the noise schedule by either choosing the transition parameters \(\beta(t)\) or the cumulative parameters \(\alpha(t)\).

If we define the noise schedule in terms of \(\beta(t)\), the time-dependent OU process is immediately apparent (see (13)). If we define the noise schedule in terms of \(\bar{\alpha}(t)\), the mean and variance of the corresponding OU process can simply be obtained from

\[\beta(t)=-\frac{\mathrm{d}}{\mathrm{d}t}\left[\ln\bar{\alpha}(t) \right].\] (18)

### Score, noise and mean diffusion formulations

The score-based model used for generation at inference time can be parametrised to model different quantities. The three most common one are the score, the noise and the mean. Using the score based SDE formulation we parametrise the network as the score that is the network approximates the quantity:

\[\nabla_{\bm{x}}\ln p_{t}(\bm{x}_{t})\approx s_{t}^{\theta}(\bm{x }_{t})\] (19)

Moving on to the DDPM formulation one typically trains a noise prediction network instead which is proportional to the score

\[s_{t}^{\theta}(\bm{x}_{t})=-\frac{1}{\sqrt{1-\bar{\alpha}(t)}} \epsilon_{t}^{\theta}(\bm{x}_{t}).\] (20)

This formulation is typically preferable for training as it is known to learn a less stiff vector field [82]. Finally in its most naive form DDPM also admits a mean matching formulation

\[\mu_{0}^{\theta}(\bm{x}_{t},t)=\frac{1}{\sqrt{\alpha_{t}}}\left( \bm{x}_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}(t)}}\epsilon_{t}^{\theta} (\bm{x}_{t})\right),\] (21)

whilst not the ideal parametrisation for direct training, it is a useful expression/macro, for expressing the sampling updates more succinctly.

In this work we parametrise \(\epsilon_{t}^{\theta}\) directly with our novel architecture for finetuning, using a noise matching objective as in DDPM, however we allude to and use the above parametrisations across our propositions and novel architectures. See Algorithm 1 for training and Algorithm 2 for sampling.

Related Work Discussion

A common distinction to all the works we will discuss in this section is that they all either train the conditional network from scratch or they initialise the conditional network with a pretrained score and fully train a conditional network. This is in stark contrast to DEFT which completely freezes the unconditional score and trains a highly efficient network to learn the \(h\)-transform which ranges from \(4-17\%\) in total parameter size of the pretrained unconditional score network.

Classifier free guidanceMethodologies such as classifier free guidance [27] do not model the forward operator explicitly. As a result, if these methods are applied to settings such as motif-scaffolding or image out-painting (where the conditioning is on a subset of the random variable), these methodologies would only denoise the scaffolding and the missing image patches. This is different to our approach which adds noise to both motif and scaffolding and then proceeds to denoise both jointly as part of the same space. In a way, one can view RFDiffusion's conditional training as an application of classifier-free guidance to this subset conditioning setting.

Image 2 Image Schrodinger Bridges (I2SB [39])I2SB and more generally aligned Schrodinger Bridges [63] are a recently proposed class of conditional generative models based on ideas from Schrodinger bridges. The premise of these methods is that they aim to learn an interpolating diffusion between a clean data sample and a altered or corrupted data sample. This is in contrast to our framework: we consider an unconditioned SDE and condition it to hit an event at a particular time, thus learning an interpolating distribution between noise and an un-corrupted target distribution. This results in several algorithmic differences:

* At its core, I2SB treat \(\bm{Y}=\mathcal{A}(\bm{X}_{0})+\eta\) and \(\bm{X}_{0}\) as source and target distributions respectively; thus, at sampling time, \(\bm{Y}\) is provided to the learned SDE which generates approximate samples from \(\mathrm{Law}\left(\bm{X}_{0}\right)\). However, in our approach, the source distribution is \(\mathcal{N}(0,\mathbf{I})\) and we pass \(\bm{Y}\) to the score network to then obtain approximate samples from \(\mathrm{Law}\left(\bm{X}_{0}\right)\).
* The score network in I2SB is a function only of \(\bm{X}_{t}\) and not \(\bm{Y}=\mathcal{A}(\bm{X}_{0})+\eta\). This means that in I2SB, the network is parametrised as \(\epsilon_{t}^{\theta}(\bm{X}_{t})\), whilst in our setting we parametrise as \(\epsilon_{t}^{\theta}(\bm{X}_{t},\mathcal{A}(\bm{X}_{0})+\eta,\mathcal{A})\). In the case of completion tasks like motif-scaffolding or image out-painting, our paramerisation looks something like \(\epsilon_{t}^{\theta}(\bm{X}_{t},\bm{X}_{0}^{\mathrm{mask}},\mathrm{mask})\). This makes the task much easier for the network as we effectively provide it with a binary variable indicating which parts of the image are conditioned and which are not. In I2SB, the network must learn this on its own. Furthermore, as we show in Prop. H.1, adding this to the network parametrisation is essential to allow recovering the true conditional score.
* The training procedure in I2SB uses the diffusion bridge \(p(\bm{x}_{t}|\bm{x}_{0},\bm{y})\) to add noise to both the source and target distributions, whilst our forward process is given by the transition density of an OU process \(p(\bm{x}_{t}|\bm{x}_{0})\) and is identical to standard DDPM/VP-SDE [28, 68] noise adding procedures.
* Finally and most importantly I2SB does full fine-tuning firstly initialising with a pretrained score and training all parameters of this large pretrained network to learn an unconditional network, this requires longer training times and significantly larger networks as they must be at least the same size as the unconditional score network.

To summarise: whilst both methodologies employ similar mathematical methodologies (e.g. Diffusion Bridges [14]), their ideations and resulting methods are fundamentally different: on one side, [39] learns an interpolating distribution between the unconditioned \(p(\bm{x}_{0})\) and conditioned \(p(\bm{y}|\bm{x}_{0})\) samples. On the other, we learn a denoising procedure that directly samples from the posterior \(p(\bm{x}_{0}|\bm{y})\); via this, we derive and explain most popular approaches for conditioning denoising diffusion models as part of our framework.

It's important to highlight that another akin approach to I2SB, also based on the h-transform but leveraging VP-SDes, was recently proposed in [84].

Cde[5] Similar to classifier free guidance. CDE [5] trains a conditional network from scratch without leveraging a pretrained unconditional score. For more details on CDE please see our detailed discussion in Appendix H.

First Hitting DiffusionsA line of generative modelling methods proposed in [40; 78] utilise the \(h\)-transform for unconditional generative modelling in the following settings:

* Hitting the target distribution \(p_{\mathrm{data}}\) in a finite amount of time \([0,T]\) via time reversing an h-transformed VP-SDE conditioned to hit \(0\) at time \(T\).
* Constraining a diffusion process at time \(T\) to lie in a subset of the reals \(\Omega\subseteq\mathbb{R}^{d}\).

Whilst the aforementioned work uses a similar methodology and theory the focus is more in line with unconditional generative modelling rather than our setting which seeks to sample from the posterior arising in an inverse problem / conditional generative modelling.

RFDiffusionAs highlighted in Alg. 3 and in contrast to amortised training, RFDiffusion [77] does not noise the motif coordinates \(\bm{X}_{0}^{[M]}\) with the forward OU-Process, instead it directly aims to sample from \(p(\bm{X}_{t}^{[\backslash M]}|\bm{X}_{0}^{[M]})\) and estimate this score while keeping the motif fixed. We can relate this approach to our amortised learning of Doob's \(h\)-transform, by noting that RF diffusion can be understood as learning the marginal conditional score

\[p(\bm{x}_{t}^{[\backslash M]}|\bm{x}_{0}^{[M]})=\int\overbrace{p(\bm{x}_{t}| \bm{x}_{0}^{[M]})}^{\propto h(t,\bm{x}_{t})p_{t}(\bm{x}_{t})}\ d\bm{x}_{t}^{[M]}.\] (22)

This can be viewed as RFDiffusion estimating a marginal counterpart of our amortised \(h\)-transform approach. See Algs. 3 and 4 for more details on how these approaches differ in a pseudo-code implementation.

## Appendix C Doob's \(h\)-transform

### Doob's \(h\)-transform intuition

Doob's transform provides a formal mechanism for conditioning a stochastic differential equation (SDE) to hit an event at a given time. The \(h\)-transform drift decomposes into two terms via Bayes rule, a conditional and a prior score

\[\nabla_{\bm{H}_{t}}\ln\overset{\rightharpoonup}{P}_{0|t}(\bm{X}_{0}\in B \mid\bm{H}_{t})=\nabla_{\bm{H}_{t}}\ln\overset{\rightharpoonup}{P}_{t|0}(\bm{ H}_{t}\mid\bm{X}_{0}\in B)-\nabla_{\bm{H}_{t}}\ln P_{t}(\bm{H}_{t}),\] (23)

whereby the conditional score ensures that the event is hit at the specified boundary time, while the prior score ensures it is time-reversal of the correct forward process [14]. Doob's \(h\)-transform adds a new drift to the SDE which amounts to two terms (via Bayes Theorem), a conditional and an unconditional score

\[\nabla\ln\overset{\rightharpoonup}{P}_{0|t}(\bm{X}_{0}\in B|\cdot)=\nabla\ln \overset{\rightharpoonup}{P}_{t|0}(\cdot|\bm{X}_{0}\in B)-\nabla\ln P_{t}(\cdot).\] (24)

Interestingly, these two terms provide for a unique intuition: the Doob's transform SDE is the time reversal of the forward SDE corresponding to (3), that is the time reversal of the forward SDE

\[\mathrm{d}\bm{X}_{t}=\bar{b}_{t}(\bm{X}_{t})\,\mathrm{d}t+\sigma_{t}\overline {\mathrm{d}}\mathbf{W}_{t},\quad\bm{X}_{0}\sim\overset{\rightharpoonup}{P}_{0 }(\cdot|\bm{X}_{0}\in B),\] (25)

coincides with the Doob transformed SDE (4) [14]. Thus we can view Doob's transform as the following series of steps:

1. Time reverse the SDE we want to condition ((4) to (25)).
2. Impose the condition via ancestral sampling from the conditioned distribution/posterior.
3. Time reverse once more to be in the same time direction as we started.

### Example: Truncated normal

Here for illustrative purposes we frame the problem of sampling from a truncated normal distribution as simulating an SDE that is given by Doob's h-transform.

Let's remind that a 1d truncated normal distribution had a density \(p(x|a,b)\propto\mathds{1}_{x\in(a,b)}(x)\mathcal{N}(x|\mu,\sigma^{2})\). Now, let's assume a data distribution \(p_{0}(x)=\mathcal{N}(\mu,\sigma^{2})\) which is noised with an OU process (13). Thus we have that \(p(x_{0}|x_{t})=\mathcal{N}(x_{0}|\hat{\mu}_{0|t}(x_{t}),\hat{\sigma}_{0|t}(x_{ t})^{2})\) is Gaussian, and so is \(p(x_{t})=\mathcal{N}(x_{t}|\hat{\mu}_{t},\hat{\sigma}_{t}^{2})\). Let's add the constraint that the process hit at time \(t=0\) the event \(\bm{X}_{0}\in(a,b)\).

\[\mathrm{d}\bm{H}_{t}=\beta(t)\left(\frac{\bm{H}_{t}}{2}+\nabla_{ \bm{H}_{t}}\ln\overline{P}_{t}(\bm{H}_{t})-\nabla_{\bm{H}_{t}}\ln\overline{P}_ {0|t}(\bm{X}_{0}\in(a,b)\mid\bm{H}_{t})\right)\,\mathrm{d}t+\sqrt{\beta(t)} \,\overline{\mathrm{d}\mathbf{W}_{t}},\] (26)

We have that the h-transform is given by

\[h(t,\bm{H}_{t}) =\overline{P}_{0|t}(\bm{X}_{0}\in(a,b)|\bm{H}_{t})=\int\mathds{1} _{x\in(a,b)}(\bm{x}_{0})\overline{p}_{0|t}(\bm{x}_{0}|\bm{H}_{t})\mathrm{d}\bm {x}_{0}\] \[=\int\mathds{1}_{x\in(a,b)}(\bm{x}_{0})\mathcal{N}(x|\hat{\mu}_{ 0|t}(\bm{H}_{t}),\hat{\sigma}_{0|t}(\bm{H}_{t})^{2})\mathrm{d}\bm{x}_{0}\] \[=\frac{1}{\hat{\sigma}_{0|t}(\bm{H}_{t})}\frac{\phi\left(\frac{ \bm{H}_{t}-\hat{\mu}_{0|t}(\bm{H}_{t})}{\hat{\sigma}_{0|t}(\bm{H}_{t})}\right) }{\Phi\left(\frac{b-\hat{\mu}_{0|t}(\bm{H}_{t})}{\hat{\sigma}_{0|t}(\bm{H}_{t} )}\right)-\Phi\left(\frac{a-\hat{\mu}_{0|t}(\bm{H}_{t})}{\hat{\sigma}_{0|t}( \bm{H}_{t})}\right)}\] (27)

where \(\phi(\xi)=\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}\xi^{2}\right)\) is the pdf of a standard normal distribution, \(\Phi(\xi)=\frac{1}{2}\left(1+\text{erf}(\xi/\sqrt{2})\right)\) its cumulative function. The corrective drift term due to the h-transform can then be computed via autograd. The unconditional score term can be computed in closed form.

### Doob's \(h\)-transform classical noiseless setting

We now consider events of the form \(\bm{X}_{0}\in B\) which are described by an equality constraint \(\mathcal{A}(\bm{X}_{0})=\bm{y}\) with \(\mathcal{A}\) a known _measurement_ (or _forward_) operator and \(\bm{y}\) an observation, which is a common setup in inverse problems such as inpainting or super-resolution.

**Corollary C.1**.: _Consider the reverse SDE (3), then it follows that_

\[\mathrm{d}\bm{H}_{t}=(\bar{b}_{t}(\bm{H}_{t})-\sigma_{t}^{2}\nabla_{\bm{H}_{t} }\ln\overline{P}_{0|t}(\mathcal{A}(\bm{X}_{0})=\bm{y}\mid\bm{H}_{t}))\, \mathrm{d}t+\sigma_{t}\overline{\mathrm{d}\mathbf{W}_{t}},\] (28)

_with \(\bar{b}_{t}(\bm{H}_{t})=f_{t}(\bm{H}_{t})-\sigma_{t}^{2}\nabla_{\bm{H}_{t}}\ln p _{t}(\bm{H}_{t})\) satisfies \(\mathrm{Law}\left(\bm{H}_{s}|\bm{H}_{t}\right)=\mathrm{Law}\left(\bm{X}_{s}| \bm{X}_{t},\mathcal{A}(\bm{X}_{0})=\bm{y}\right)\) thus \(\mathrm{Law}\left(\bm{H}_{0}\right)=\mathrm{Law}\left(\bm{X}_{0}|\mathcal{A}( \bm{X}_{0})=\bm{y}\right)\)._

Sampling from (28) directly provides samples \(\bm{x}\sim p_{\mathrm{data}}\) which also satisfy \(\mathcal{A}(\bm{x})=\bm{y}\). Crucially, this SDE is guaranteed to hit the conditioning in finite time, unlike prior equilibrium-motivated approaches [12; 19; 22; 24; 46; 65].

Reconstruction guidanceTo get better insight into the challenge of sampling via Doob's \(h\)-transform in (28) let us re-express the \(h\)-transform as

\[\overline{P}_{0|t}(\mathcal{A}(\bm{X}_{0})=\bm{y}\mid\bm{H}_{t})=\int\mathds{1 }_{\mathcal{A}(\bm{x}_{0})=\bm{y}}(\bm{x}_{0})\overline{p}_{0|t}(\bm{x}_{0}| \bm{H}_{t})\mathrm{d}\bm{x}_{0},\] (29)

where in the case of denoising diffusion models \(\bar{p}_{0|t}(\bm{x}_{0}|\cdot)\) is the transition density of the reverse SDE (3). In practice, one does not have access to this transition density - i.e. we can sample from this distribution, but we cannot easily get its value at a certain point. This makes it difficult to approximate the integral. To alleviate this, a strand of recent works [12; 22; 57; 65] have proposed to apply a Gaussian approximation of \(\overline{p}_{0|t}(\bm{x}_{0}|\cdot)\approx\mathcal{N}(\bm{x}_{0}\mid\mathbb{E} [\bm{X}_{0}|\bm{X}_{t}=\cdot],\Gamma_{t})\) leveraging Tweedie's formula and the pre-trained score network. This line of work is referred as _reconstruction guidance_. We note that whilst proposing to approximate the quantity \(\overline{P}_{0|t}(\mathcal{A}(\bm{X}_{0})=\bm{y}|\cdot)\), they do not make the connection to Doob's transform and thus are unable to provide guarantees on the conditional sampling that Cor. C.1 provides. Overall, the Gaussian-based approximations of Doob's \(h\)-transform lead to reconstruction guidance-based approaches

\[\mathrm{d}\bm{H}_{t}=\left(\bar{b}_{t}(\bm{H}_{t})+\sigma_{t}^{2}\nabla_{\bm{H}_{ t}}||\bm{y}-\mathrm{A}\mathbb{E}[\bm{X}_{0}|\bm{X}_{t}=\bm{H}_{t}]||_{\Gamma_{t}}^{ 2}\right)\mathrm{d}t+\sigma_{t}\overline{\mathrm{d}\mathbf{W}_{t}},\bm{X}_{T} \sim\mathcal{P}_{T},\] (30)

where \(\Gamma_{t}\) acts as a guidance scale [57; 62], and \(\mathrm{A}\) is a matrix if \(\mathcal{A}\) is linear otherwise \(\mathrm{A}=\mathrm{d}\mathcal{A}(\mathbb{E}[\bm{X}_{0}|\bm{X}_{t}=\bm{H}_{t}])\).

Proofs

### Proof of Proposition 2.2

Proof.: Starting from the unconditioned reverse SDE

\[\mathrm{d}\bm{X}_{t}=\bar{b}_{t}(\bm{X}_{t})\,\mathrm{d}t+\sigma_{t}\,\,\bar{ \mathrm{d}\mathbf{W}}_{t},\quad\bm{X}_{T}\sim\mathbb{P}_{T}=Q_{T}^{f_{t}}[p(\bm {x}_{0})],\] (31)

we consider its reversal, the forward SDE, but we change its initial from distribution \(p(\bm{x}_{0})\) to the target posterior \(p(\bm{x}_{0}|\bm{y})\), i.e.,

\[\mathrm{d}\bm{H}_{t}=f_{t}(\bm{H}_{t})\,\mathrm{d}t+\sigma_{t}\,\,\bar{ \mathrm{d}\mathbf{W}}_{t},\quad\bm{H}_{0}\sim\frac{p(\bm{y}|\bm{x}_{0})p_{ \mathrm{data}}(\bm{x}_{0})}{p(\bm{y})},\] (32)

where \(\bar{b}_{t}(\bm{H}_{t})=f_{t}(\bm{H}_{t})-\sigma_{t}^{2}\nabla_{\bm{H}_{t}}\ln p _{t}(\bm{H}_{t})\).

Now let us use \(p_{t|y}(\bm{x}|\bm{y})=\int p(\bm{x}_{t}|\bm{x}_{0})\mathrm{d}p(\bm{x}_{0}|\bm {y})\) to denote the marginal of the above SDE and as before \(p_{t}\) to denote the marginal of the reference starting at the data distribution. Then it follows that

\[p_{t|y}(\bm{x}|\bm{y}) =p_{t}(\bm{x})p(\bm{y})^{-1}\int\frac{\bar{p}_{t|0}(\bm{x}|\bm{x} _{0})}{p_{t}(\bm{x})}p(\bm{y}|\bm{x}_{0})p_{\mathrm{data}}(\bm{x}_{0})d\bm{x} _{0}\] (33) \[=p_{t}(\bm{x})p(\bm{y})^{-1}\int\bar{p}_{0|t}(\bm{x}_{0}|\bm{x})p (\bm{y}|\bm{x}_{0})d\bm{x}_{0}\] (34)

and thus the score of the reference starting at the posterior is given by:

\[\nabla_{\bm{x}}\ln p_{t|y}(\bm{x}|\bm{y})=\nabla_{\bm{x}}\ln p_{t}(\bm{x})+ \nabla_{\bm{x}}\ln\int\bar{p}_{0|t}(\bm{x}_{0}|\bm{x})p(\bm{y}|\bm{x}_{0})d\bm {x}_{0}\] (35)

Now that we have the score of the SDE in Equation 32 we can reverse it yet another time to obtain the conditional backwards SDE:

\[\bm{H}_{T}\sim Q_{T}^{f_{t}}[p(\bm{x}_{0}|\bm{y})]=\int\bar{p}_{T|0}(\bm{x}| \bm{x}_{0})p(\bm{x}_{0}|\bm{y})\mathrm{d}\bm{x}_{0}\]

\[\mathrm{d}\bm{H}_{t}=\left(f_{t}(\bm{H}_{t})-\sigma_{t}^{2}\left(\nabla_{\bm{ H}_{t}}\ln p_{t}(\bm{H}_{t})+\nabla_{\bm{H}_{t}}\ln p_{y|t}(\bm{Y}=\bm{y}|\bm{H}_ {t})\right)\right)\,\mathrm{d}t+\sigma_{t}\,\,\bar{\mathrm{d}\mathbf{W}}_{t},\]

\[\mathrm{d}\bm{H}_{t}=\left(\bar{b}_{t}(\bm{H}_{t})-\sigma_{t}^{2}\nabla_{\bm{ H}_{t}}\ln p_{y|t}(\bm{Y}=\bm{y}|\bm{H}_{t})\right)\,\mathrm{d}t+\sigma_{t}\,\, \bar{\mathrm{d}\mathbf{W}}_{t},\] (36)

Where it is very important to notice that the backward SDE starts a the terminal distribution of the conditional forward SDE \(Q_{T}^{f_{t}}[p(\bm{x}_{0}|\bm{y})]\) and not \(\mathbb{P}_{T}=Q_{T}^{f_{t}}[p(\bm{x}_{0})]\), for a VP-SDE this happens to approximately be \(\mathcal{N}(0,I)\) in both cases (i.e. \(Q_{T}^{f_{t}}[p(\bm{x}_{0}|\bm{y})]\approx Q_{T}^{f_{t}}[p(\bm{x}_{0})]\approx \mathcal{N}(0,I)\)) however more generally it is not and one needs to be careful.

Note that this remark highlights that the score used in DPS [12] (i.e. \(\nabla_{\bm{x}}\ln p_{t|y}(\bm{x}|\bm{y})\)) is in fact the score of an OU process starting at \(p(\bm{x}_{0}|\bm{y})\) notice the cancellation going from Equations (33) to (34) was only possible since the prior in our target posterior is the initial distribution for the forward SDE (in their case an OU-process) with marginal \(p_{t}\), these considerations are subtle yet important and omitted in prior works. Whilst this is akin the relationship motivated in DPS as \(\nabla_{\bm{x}}\ln p_{t|y}(\bm{x}|\bm{y})=\nabla_{\bm{x}}\ln p_{t}(\bm{x})+ \nabla_{\bm{x}}\ln p_{t}(\bm{y}|\bm{x})\), DPS fails to convey that this is in fact the score of a VP-SDE with the posterior \(p(\bm{x}_{0}|\bm{y})\) as its initial distribution.

### Proof of Theorem 3.12)

Proof.: The idea of the proof is similar to Proposition H.1. Here, we directly proof the amortised variation from Equation (9). First, we state the well known score matching identity for the posterior score:

\[\nabla_{\bm{x}_{t}}\ln p_{t}(\bm{x}|\bm{y})=\int\nabla_{\bm{x}_{t}}\ln\bar{p}_ {t|0}(\bm{x}_{t}|\bm{x}_{0})\bar{p}_{0|t}(\bm{x}_{0}|\bm{x}_{t},\bm{y})\mathrm{ d}\bm{x}_{0}.\]

Via the mean squared error property of the conditional expectation we get the minimiser as

\[\mathbb{E}\left[\nabla_{\bm{H}_{t}}\ln\bar{p}_{t|0}(\bm{H}_{t}|\bm{X}_{0})- \nabla_{\bm{H}_{t}}\ln p_{t}(\bm{H}_{t})\Big{|}\bm{Y}=\bm{y},\bm{H}_{t}=\bm{x}\right]\]Then

\[h^{*}_{t}(\bm{x},\bm{y}) =\int(\nabla_{\bm{x}}\ln\bar{p}_{t|0}(\bm{x}|\bm{x}_{0})-\nabla_{\bm {x}}\ln p_{t}(\bm{x}))\bar{p}_{0|t}(\bm{x}_{0}|\bm{H}_{t}=\bm{x},\bm{Y}=\bm{y}) \mathrm{d}\bm{x}_{0}\] \[=\int\nabla_{\bm{x}}\ln\bar{p}_{t|0}(\bm{x}|\bm{x}_{0})\bar{p}_{0| t}(\bm{x}_{0}|\bm{H}_{t}=\bm{x},\bm{Y}=\bm{y})\mathrm{d}\bm{x}_{0}-\nabla_{ \bm{x}}\ln p_{t}(\bm{x})\] \[=\nabla_{\bm{x}}\ln p_{t}(\bm{x}|\bm{y})-\nabla_{\bm{x}}\ln p_{t} (\bm{x})=\nabla_{\bm{x}}\ln p_{t}(\bm{y}|\bm{x}).\]

## Appendix E Experimental details

### Image Experiments

In the image experiment, we use the DDPM [28] formulation for the diffusion model with \(N=1000\) steps, a linear \(\beta\)-schedule with \(\beta_{0}=10^{-4}\) and \(\beta_{N}=2\cdot 10^{-2}\). In all our imaging experiments the \(h\)-transform was implemented according to the parametrisation in Section 3.3 using an attention U-Net [16] for NN\({}_{1}^{\phi}\). The network NN\({}_{2}^{\phi}\) in the residual pathway was implemented as a small three layer fully connected network with SiLU activation functions, predicting a single scalar value. The final layer in NN\({}_{1}^{\phi}\) was initialised with zeros. All weights in NN\({}_{2}^{\phi}\) where initialised with zeros, expect for the bias in the last layer, which was initialised to \(0.01\). We found that this initialisation was stable over all imaging tasks, i.e., close to the unconditional model. All tasks on ImageNet were noiseless, i.e., the log-likelihood \(p(\bm{x}|\bm{y})\) would be a Delta function. However, for the guidance term \(\nabla_{\bm{x}_{0}}\ln p(\bm{y}|\hat{\bm{x}}_{0})\) we always approximated the log-likelihood using a Gaussian with \(\sigma_{y}=1.0\).

Computed TomographyThe 2016 American Association of Physicists in Medicine (AAPM) grand challenge dataset [45] contains CT scans of \(10\) patients. We only make use of the abdomen scans. We use the scans of \(9\) patients to train both the unconditional model and the \(h\)-transform. The remaining patient, i.e., id L035 with \(87\) slices, is used for testing only. We used a bilinear interpolation to resize the images to \(256\times 256\)px. The unconditional model follows the same architecture as in [11] with about \(374\)M parameters. The model was trained for \(300\) epochs using the Adam optimiser [33]. We used exponential moving average on the weights as in [66] with a parameter of \(0.999\).

The LoDoPab-CT dataset [35] is a collection of human chest CT scans. We resize the images to \(256\times 256\)px using a bilinear interpolation. The training set contains \(35820\) images and was used to train the unconditional diffusion model. The validation set contains \(3522\) images and was used to train the \(h\)-transform model and for a hyperparameter search for DPS and RED-diff. Finally, we take \(178\) images (every \(20\)th) from the test set to compute the final results. The unconditional model is an attention U-Net [16] with about \(133\)M parameters. The unconditional model was trained for \(75\) epochs, corresponding to about \(671625\) gradient steps using the Adam optimiser [33]. We used exponential moving average on the weights as in [66] with a parameter of \(0.999\).

For both datasets we used the same parametrisation for the \(h\)-transform, with an attention U-Net [16] and a residual path as in Section 3.3. The \(h\)-transform has about \(23\)M parameters. Thus, the \(h\)-transform is about \(6\%\) and \(17\%\) the size of the unconditional model for AAPM and LoDoPab-CT, respectively. For LoDoPab-CT, the \(h\)-transform was trained on a set of \(3522\) images, while for AAPM we trained the \(h\)-transform on the same dataset as the unconditional model. The forward operator \(A\) is linear and we added Gaussian noise with standard deviation \(\sigma_{y}=1.0\). Thus, the gradient of the log likelihood reduces to

\[\nabla_{\hat{\bm{x}}_{0}}\ln p(\bm{y}|\hat{\bm{x}}_{0})=-\frac{1}{2\sigma_{y}^{ 2}}A^{*}(A\hat{\bm{x}}_{0}-\bm{y}),\]

where \(A^{*}\) denotes the backprojection. Here, instead of the backprojection, we made use of the filtered backprojection, which is a common technique for computed tomography [25].

InpaintingWe made use of the pre-trained ImageNet dataset. For this task, we also amortised over the different sampling masks. We implemented the \(h\)-transform using an attention U-Net [16] as the base network NN\({}_{1}\) with an initial convolution with \(13\) channels, i.e., the noisy image \(\bm{x}_{t}\), the denoised estimate \(\hat{\bm{x}}_{0}\), the observation \(\bm{y}\) (where the missing pixels filled with zeros), the mask \(M\) and the cheap guidance term \(\nabla_{\hat{\bm{x}}_{0}}\ln p(\bm{y}|\hat{\bm{x}}_{0})\). The pre-trained unconditional score model has 550M parameters, and we use a fine-tuning network with 23M parameters (\(4.2\%\) of the size). The fine-tuning network was trained for 200 epochs using a batch size of 16, and the Adam optimiser with a learning rate of \(5e^{-4}\) and annealing.

Super-resolutionWe made use of the pre-trained ImageNet dataset. We implemented the \(h\)-transform using an attention U-Net [16] as the base network NN\({}_{1}\) with an initial convolution with 9 channels, i.e., the noisy image \(\bm{x}_{t}\), the denoised estimate \(\hat{\bm{x}}_{0}\), the observation \(\bm{y}\) bilinear upsampled to the original size and the cheap guidance term \(\nabla_{\hat{\bm{x}}_{0}}\ln p(\bm{y}|\hat{\bm{x}}_{0})\). The pre-trained unconditional score model has 550M parameters, and we use a fine-tuning network with 23M parameters (\(4.2\%\) of the size). The fine-tuning network was trained for 200 epochs using a batch size of 16, and the Adam optimiser with a learning rate of \(5e^{-4}\) and annealing.

HdrFor HDR the forward operator is given by \(\mathcal{A}(\bm{x})=\text{clip}(2x;-1,1)\) for the RGB image scaled to \([-1,1]\). This leads to a reduction of high intensity values. We approximated the cheap guidance term

\[\nabla_{\hat{\bm{x}}_{0}}\ln p(\bm{y}|\hat{\bm{x}}_{0})\approx 0.5(\mathcal{A} (\hat{\bm{x}}_{0})-\bm{y}),\]

and found this to achieve good results. The pre-trained unconditional score model has 550M parameters, and we use a fine-tuning network with 23M parameters (\(4.2\%\) of the size). The fine-tuning network was trained for 200 epochs using a batch size of 16, and the Adam optimiser with a learning rate of \(5e^{-4}\) and annealing.

Phase retrievalThe \(h\)-transform was again implemented as an attention U-Net [16] as the base network NN\({}_{1}\). The observations in phase retrieval corresponds to the magnitude values of the Fourier transform. In our initial experiments, we feed these observation directly into the model. However, this model failed to create convincing reconstruction. Instead we used two rough reconstruction. For the first initial reconstruction, we used the phase of the unconditional Tweedie estimate \(\hat{\bm{x}}_{0}\) and the magnitude of the observation to construct an image. For the second initial reconstruction, we ran \(350\) steps of an ER algorithm [21] with a random initialisation. Further, we used the cheap guidance term \(\nabla_{\hat{\bm{x}}_{0}}\|\bm{y}-\mathcal{A}(\hat{\bm{x}}_{0})\|_{2}^{2}\), calculated using torch autograd. The \(h\)-transform had about \(23\)M parameter, i.e., \(4\%\) of the size of the unconditional model.

Non-linear DeblurringFor non-linear deblurring we found that there was little improvement when using the cheap guidance term \(\nabla_{\hat{\bm{x}}_{0}}\|\bm{y}-\mathcal{A}(\hat{\bm{x}}_{0})\|_{2}^{2}\), calculated using torch autograd. As the forward operator is defined by a trained neural network this additional autograd term adds to the computational expense. We found that the DEFT provided results of a similar quality, with a reduced computational burden, by using \(\mathcal{A}(\mathcal{A}(\hat{\bm{x}}_{0})-\bm{y})\) as a rough approximation. The pre-trained unconditional score model has 550M parameters, and we use a fine-tuning network with 23M parameters (\(4.2\%\) of the size). The fine-tuning network was trained for 200 epochs using a batch size of 16, and the Adam optimiser with a learning rate of \(5e^{-4}\) and annealing.

### Protein Motif Scaffolding

Diffusion processWe use a discrete-time DDPM [28] formulation for the diffusion model with \(N=1000\) steps and cosine \(\beta\)-schedule [16].

Noise modelThe denoising model \(\varepsilon_{\theta}\) is adapted from the Genie diffusion model [37]. In Genie, the denoiser architecture consists of an SE(3)-invariant encoder and an SE(3)-equivariant decoder. While the network uses Frenet-Serret frames as intermediate representations, the diffusion process itself is defined in Euclidean space over the \(C_{\alpha}\) coordinates. Similar to AlphaFold2, the denoiser network consists of a single representation track that is initialised via a single feature network and a pair representation track that is initialised via a pair feature network. These two representations are further transformed via a pair transform network and are used in the decoder for noise prediction via IPA [30].

To evaluate unconditional sampling-based methods, we used a pre-trained version of the unconditional Genie model.

To evaluate the Amortised approach (Alg. 4), we perform a minor modification to the unconditional Genie model as described in [17]: we add an additional conditional pair feature network that takes the motif frames as input with the ground truth coordinates for the motif and 0 as values for all other coordinates that are not part of the motif. The output of this motif-conditional pair feature network is concatenated with the output of the unconditional pair feature network to form an intermediate dimension of twice the channel size compared to the unconditional model before being linearly projected down to the channel size of the unconditional model. From then onward the output is processed by the remaining Genie components as in the unconditional model. The implementation is therefore similar to the image case, where the motif features are presented as additional input and the model learns to use these for reconstructing the motif. This minor alteration of the Genie architecture means that the amortised network has \(4.162\)M parameters while the unconditional Genie networks have \(4.087\)M parameters (\(\sim\) 1.8% fewer). In 80% of the training steps for the amortised model, we pass a condition to the network. The other 20% contains an empty mask consisting of only 0's.

For the DEFT implementation, we follow a similar way of feeding in the additional inputs via conditional pair feature networks, but as part of the downsized h-transform model as described in the main text.

MetricsWe measure the performance of the methods across two axes: designability and success rate. To assess whether a particular protein scaffold is _designable_, we run the same pipeline as [37], consisting of an inverse folding generated \(C_{\alpha}\) backbones with ProteinMPNN and then re-folding the designed sequences via ESMFold. The considered metrics and their corresponding thresholds are the following:

* scTM > 0.5: This refers to the TM-score between the structure that's been designed and the predicted structure based on self-consistency as previously described. The scTM-score ranges from 0 to 1. Higher scores indicate a higher likelihood that the input structure can be designed.
* scRMSD < 2 A : The scRMSD metric is akin to the scTM metric. However, it uses the RMSD (Root Mean Square Deviation) to measure the difference between the designed and predicted structures, instead of the TM-score. This metric is more stringent than scTM as RMSD, being a local metric, is more sensitive to minor structural variances.
* pLDDT > 70 and pAE < 5: Both scTM and scRMSD metrics depend on a structure prediction method like AlphaFold2 or ESMFold to be reliable. Hence, additional confidence metrics such as pLDDT and pAE are employed to ascertain the reliability of the self-consistency metrics.

In addition, we want to judge whether the motif scaffolding was successful or not. Therefore, similar to previous work by [77], we calculate the motifRMSD between the predicted design structure and the original input motif and judge samples with < 1 A motifRMSD as a successful motif scaffold.

We follow previous work and call a sample a "success" if scRMSD < 2 A and motifRMSD < 1 A. Similar to previous work we call a task "solved" if among 100 samples for this task at least 1 sample is a success.

## Appendix F Additional Results

### Ablation of the DEFT parametrisation

The parametrisation of the \(h\)-transform is motivated by the sampling theory in Section 3.3. We evaluate different parametrisations of this choice for the CT experiment on LoDoPub-CT. For all architecture choices, we used the same training setup. For quantitative results, see Table 6. In particular, we see that the naive choice NN\({}_{1}^{\phi}(\bm{x},A^{*}\bm{y},t)\) only achieves a PSNR of \(26.62\)dB. Note, that we do not input the observations directly into the network, but first transform them using the backprojection, which is a standard technique for computed tomography reconstruction [3]. In CT the observations correspond to sinograms and have a different geometry to the images. In difference, if we add the unconditional Tweedie estimate \(\hat{\bm{x}}_{0}\) to the model architecture, we get an improvement to \(34.04\)dB. This shows that it is beneficial to supply the \(h\)-transform with the information of the unconditional diffusion model. Further, given our architecture, i.e., adding the additional information of the log-likelihood term, achieves a PSNR of \(35.81\)dB. Further, we show the learning residual scaling network \(\text{NN}_{2}^{\phi}\) in Figure 6. We observe a similar behaviour for all tasks,i.e., at the start of sampling \(t\approx 1000\) the scaling network assigned a small weighting to the guidance part, which increases during sampling (\(t\to 0\)).

### Non-linear Deblurring: Implementation from [44]

We find that the original implementation of the non-linear forward operator from the codebase provided in [44] results in an almost trivial reconstruction task, due to incorrect loading of weights for the neural network used for the non-linear deblurring. As a result, both RED-diff and DEFT can achieve highly performant results on this trivial task, as shown in Table 7. For the non-linear deblurring tasks in the main text in Table 2, we load the weights for the neural network correctly, and see much lower performance, corresponding to the difficulty of the non-linear task.

## Appendix G Generalised \(h\)-transform and Stochastic Control

Thanks to our formal framework in this section we develop a new VI objective for learning the conditional score in the noisy inverse problems setting. That is by minimising the following ELBO

\begin{table}
\begin{tabular}{l l l l} \hline \hline Parametrisation & & **PSNR** & **SSIM** \\ \hline \(\text{NN}_{1}^{\phi}(\bm{x},\hat{\bm{x}}_{0},\nabla_{\hat{\bm{x}}_{0}}\ln p (\bm{y}|\hat{\bm{x}}_{0}),t)+\text{NN}_{2}^{\phi}(t)\nabla_{\hat{\bm{x}}_{0}} \ln p(\bm{y}|\hat{\bm{x}}_{0})\) & \(35.81\) & \(0.876\) \\ \(\text{NN}_{1}^{\phi}(\bm{x},\nabla_{\hat{\bm{x}}_{0}}\ln p(\bm{y}|\hat{\bm{x} }_{0}),t)+\text{NN}_{2}^{\phi}(t)\nabla_{\hat{\bm{x}}_{0}}\ln p(\bm{y}|\hat{ \bm{x}}_{0})\) & \(35.74\) & \(0.875\) \\ \(\text{NN}_{1}^{\phi}(\bm{x},\hat{\bm{x}}_{0},A^{*}\bm{y},t)\) & & \(34.04\) & \(0.851\) \\ \(\text{NN}_{1}^{\phi}(\bm{x},A^{*}\bm{y},t)\) & & \(26.62\) & \(0.724\) \\ \hline \hline Method & **PSNR (\(\uparrow\))** & **SSIM (\(\uparrow\))** & **LPIPS (\(\downarrow\))** & **Time in hrs (\(\downarrow\))** \\ \hline RED-diff & \(45.00\) & \(0.98\) & \(1.2e^{-3}\) & \(7.9\) \\ DEFT & \(64.64\) & \(0.99\) & \(1.0e^{-5}\) & \(5.2\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: PSNR and SSIM for computed tomography on LoDoPab-CT with different parametrisation of the \(h\)-transform.

Figure 6: The trained residual scaling network \(\text{NN}_{2}^{\phi}\) in the DEFT architecture (see Section 3.3) for computed tomography reconstruction on LoDoPab-CT and AAPM.

with respect to an additional fine-tuning network, one can learn the conditional score

\[h^{*}=\operatorname*{arg\,min}_{f}\mathbb{E}_{\mathbb{Q}}\left[\frac{1}{2}\int_{0 }^{T}\sigma_{t}^{2}\|h(\bm{H}_{t})\|^{2}\mathrm{d}t\right]-\mathbb{E}_{\bm{H}_ {o}\sim\mathbb{Q}_{0}}[\ln p(\bm{y}|\bm{H}_{0})],\] (37)

where \(\bm{H}_{t}\) follows the unconditioned score SDE with an added control \(h\). This objective provides a way to learn the conditioned SDE from the unconditioned one, without making Gaussian approximations. We formalise this connection in the following proposition.

**Proposition G.1**.: _The following stochastic control problem_

\[h^{*}=\operatorname*{arg\,min}_{f}\mathbb{E}_{\mathbb{Q}}\left[\frac{1}{2}\int _{0}^{T}\sigma_{t}^{2}\|h(\bm{H}_{t})\|^{2}\mathrm{d}t\right]-\mathbb{E}_{\bm {H}_{0}\sim\mathbb{Q}_{0}}[\ln p(\bm{y}|\bm{H}_{0})]\] (38)

_with_

\[\bm{H}_{T}\sim Q_{T}^{f_{t}}[p(\bm{x}_{0}|\bm{y})]\] \[\mathrm{d}\bm{H}_{t}=\left(f_{t}(\bm{H}_{t})-\sigma_{t}^{2}( \nabla_{\bm{H}_{t}}\ln p_{t}(\bm{H}_{t})+h_{t}(\bm{H}_{t}))\right)\,\mathrm{d} t+\sigma_{t}\ \overline{\mathrm{d}\mathbf{W}}_{t},\] (39)

Figure 8: Diversity of samples for inpainting. On the left, we show the inpainting mask as a overlay over the ground truth.

Figure 7: Example reconstructions for phase retrieval. Phase retrieval has many local minima, which fully satisfy the data consistency constraints (e.g., complex conjugate, global phase sign). We often see flipped (and perturbed) images as our reconstruction. In some instances, even only one colour channel is flipped. This leads to a strong diversity of samples.

is minimised by the conditional score SDE in Equation (28), that is_

\[h_{t}^{*}(\bm{x})=\nabla_{\bm{x}}\ln\mathbb{E}_{\bm{X}_{0}\sim p_{0|t}(\cdot|\bm{ x})}|p(\bm{y}|\bm{X}_{0})]=\nabla_{\bm{x}}\ln p_{y|t}(\bm{y}|\bm{x}).\] (40)

_Furthermore, \(h^{*}\) solves an associated half-bridge problem [7] with the SDE in Eqn. (3) as its reference process and \(p(\bm{x}_{0}|\bm{y})\) as its source distribution._

Proof.: The derivation for this objective is inspired by the sequential Bayesian learning scheme proposed in Lemma 1, Appendix B of [75].

Let \(\mathbb{P}\) denote the distribution for the forward SDE in Eqn. (25). Now consider the following variational problem termed a half-bridge [7, 15, 73].

\[\mathbb{Q}^{*}=\operatorname*{arg\,min}_{\mathbb{Q}:\mathbb{Q}_{0}=p(\bm{x}_{ 0}|\bm{y})}D_{\mathrm{KL}}(\mathbb{Q}||\mathbb{P})\] (41)

where the constraint enforces that at time \(0\) we hit the target posterior \(p(\bm{x}|\bm{y})\) then via standard results in half bridges we know that the above optimisation problem has an unconstrained formulation (e.g.

Figure 10: Results for non-linear HDR. Similar to [44], we found that DPS does not converge to a good solution, returning often black or images not consistent with the ground truth. Given that the forward operator \(\mathcal{A}(\bm{x})=\text{clip}(2\bm{x}|-1,1)\) has a zero gradient for \(|\bm{x}|\geq 0.5\) the guidance term is often not informative.

Figure 9: Results for 4x super-resolution.

see [74]) that is \(\mathrm{d}\mathbb{Q}^{*}=\mathrm{d}\mathbb{P}\frac{\mathrm{d}p(\bm{x}|\bm{y})}{ \mathrm{d}\mathbb{P}_{0}}\) Now following [75] we notice that we can cancel the \(p_{\mathrm{data}}\) prior in the posterior term:

\[\mathrm{d}\mathbb{P}\frac{\mathrm{d}p(\bm{x}|\bm{y})}{\mathrm{d}\mathbb{P}_{0}}= \mathrm{d}\mathbb{P}\frac{\mathrm{d}p(\bm{x}|\bm{y})}{\mathrm{d}p_{\mathrm{data }}}=\mathrm{d}\mathbb{P}\frac{\mathrm{d}p(\bm{y}|\bm{x})}{\mathrm{d}p(\bm{y})}\] (42)

and thus:

\[\mathbb{Q}^{*}=\arg\min_{\mathbb{Q}}D_{\mathrm{KL}}(\mathbb{Q}||\mathbb{P})- \mathbb{E}_{\bm{H}_{0}\sim\mathbb{Q}_{0}}[\ln p(\bm{y}|\bm{H}_{0})]\] (43)

with \(\mathbb{Q}_{T}=\mathrm{Law}\left(\bm{X}_{T}\right)\approx\mathcal{N}(0, \mathbf{I})\) when \(\bm{X}_{0}\sim p(\bm{x}|\bm{y})\). Furthermore, we can parametrise \(\mathbb{P}\) as :

\[\mathrm{d}\bm{X}_{t}=\left(f_{t}(\bm{X}_{t})-\sigma_{t}^{2}\nabla_{\bm{X}_{t}} \ln p_{t}(\bm{X}_{t})\right)\mathrm{d}t+\sigma_{t}\overline{\mathrm{d}\bm{W}_{ t}},\quad\bm{X}_{0}\sim Q_{T}^{f_{t}}[p_{\mathrm{data}}(\bm{x}_{0})]\] (44)

and thus \(\mathbb{Q}\) as

\[\bm{H}_{T}\sim Q_{T}^{f_{t}}[p(\bm{x}_{0}|\bm{y})]\] \[\mathrm{d}\bm{H}_{t}=\left(f_{t}(\bm{H}_{t})-\sigma_{t}^{2}( \nabla_{\bm{H}_{t}}\ln p_{t}(\bm{H}_{t})+h_{t}(\bm{H}_{t}))\right)\,\mathrm{d }t+\sigma_{t}\,\,\overline{\mathrm{d}\bm{W}}_{t},\] (45)

then via Girsanov Theorem we can re-express the KL term in Eqn. (43) as

\[h^{*}=\operatorname*{arg\min}_{h}\mathbb{E}_{\mathbb{Q}}\left[\frac{1}{2}\int_ {0}^{T}\sigma_{t}^{2}\|h(\bm{H}_{t})\|^{2}\mathrm{d}t\right]-\mathbb{E}_{\bm {H}_{0}\sim\mathbb{Q}_{0}}[\ln p(\bm{y}|\bm{H}_{0})].\] (46)

Now noticing that Eqn. (46) is a standard stochastic control problem [48, 31] we can characterise its minimiser as (using Theorem 2.2 in [48] and the Hopf-Cole transform [23])

\[h_{t}^{*}(\bm{x})=\nabla_{\bm{x}}\ln\mathbb{E}_{\bm{X}_{0}\sim p_{0|t}(\cdot| \bm{x})}[p(\bm{y}|\bm{X}_{0})],\] (47)

and thus the SDE in Eqn. 45 with \(h_{t}^{*}\) hits the target posterior \(p(\bm{y}|\bm{x}_{0})\) at time \(0\) as it is the minimiser of half-bridge posed in Eqn. (41). 

Notice that in the case of a VP-SDE, the stochastic control objective reduces to:

\[\operatorname*{arg\min}_{h}\mathbb{E}_{\mathbb{Q}}\left[\int_{0}^{T}\frac{ \beta_{t}}{2}||h(\bm{H}_{t})||^{2}\mathrm{d}t\right]-\mathbb{E}_{\bm{H}_{0} \sim\mathbb{Q}_{0}}[\ln p(\bm{y}|\bm{H}_{0})]\] (48)

Discretisation of inverse problem objectiveFollowing [74] we will discretise the objective presented in Eqn. (48). Let us consider the pre-trained score SDE with an added tuning network:

\[\bm{H}_{T}\sim\mathcal{N}(0,I)\] \[\mathrm{d}\bm{H}_{t}=-\beta_{t}(\bm{H}_{t}+2s_{\theta}(\bm{H}_{t}) +2h_{\phi}(\bm{H}_{t}))\,\mathrm{d}t+\sqrt{2\beta_{t}}\,\overline{\mathrm{d} \bm{W}}_{t},\] (49)

now using an exponential-like discretisation [10] (Ideally we want to discretise in the same way we trained the model):

\[\bm{H}_{t_{K}}\sim\mathcal{N}(0,I)\] \[\bm{H}_{t_{k-1}}=\left(\sqrt{1-\alpha_{k}}\bm{H}_{t_{k}}+2(1- \sqrt{1-\alpha_{k}})\left(s_{\theta^{*}}(\bm{H}_{t_{k}})+h_{\phi}(\bm{H}_{t_{k }})\right)\right)+\sqrt{\alpha_{k}}\varepsilon_{k},\] (50)

where \(\alpha_{k}=1-\exp\left(\int_{t_{k-1}}^{t_{k}}\beta_{s}\mathrm{d}s\right)\), note we will denote the distribution of the above discrete time chain as \(q_{\phi}\). Now if we follow the sketch in Proposition 3 of [74] the discretised objective then becomes:

\[\operatorname*{arg\min}_{\phi}\mathbb{E}_{\bm{H}\sim q_{\phi}}\left[2\sum_{k=1 }^{K}\frac{\lambda_{k}^{2}}{\alpha_{k}}||h_{\phi}(k,\bm{H}_{t_{k}})||^{2}-\ln p (\bm{y}|\bm{H}_{0})\right]\] (51)

where \(\lambda_{k}=1-\sqrt{1-\alpha_{k}}\). For a more stable/simple objective following [74] we can make the approximation \(\lambda_{k}=1-\sqrt{1-\alpha_{k}}\approx\alpha_{k}/2\) for small time steps. This leads to the following iteration (which is possibly more akin to the training update being used):

\[\bm{H}_{t_{K}}\sim\mathcal{N}(0,I)\] \[\bm{H}_{t_{k-1}}=\left(\sqrt{1-\alpha_{k}}\bm{H}_{t_{k}}+\alpha_{k} \left(s_{\theta^{*}}(\bm{H}_{t_{k}})+h_{\phi}(\bm{H}_{t_{k}})\right)\right)+ \sqrt{\alpha_{k}}\varepsilon_{k},\] (52)

and objective:

\[\operatorname*{arg\min}_{\phi}\mathbb{E}_{\bm{H}\sim q_{\phi}}\left[\sum_{k=1 }^{K}\frac{\alpha_{k}}{2}||h_{\phi}(k,\bm{H}_{t_{k}})||^{2}-\ln p(\bm{y}|\bm{H}_ {0})\right].\] (53)Discrete Time IntuitionFor further intuition, we will provide a discrete-time derivation as to how this objective arises. Consider the following discrete-time VP-SDE (i.e let. \(p_{k+1|k}(\bm{h}_{t_{k+1}}|\bm{h}_{t_{k}})=\mathcal{N}(\bm{h}_{t_{k+1}}|\sqrt{1- \alpha_{k}}\bm{h}_{t_{k}},\alpha_{k})\)) starting from the posterior:

\[p(\bm{h}_{t_{1}:t_{k}})=\frac{p(\bm{y}|\bm{h}_{0})p_{\mathrm{data}}(\bm{h}_{0})} {p(\bm{y})}\prod_{k=1}^{K}p_{k+1|k}(\bm{h}_{t_{k+1}}|\bm{h}_{t_{k}})\] (54)

Now applying Bayes rule and (2, Section 5) \(p_{k+1|k}(\bm{h}_{t_{k+1}}|\bm{h}_{t_{k}})=\frac{p_{k|k+1}^{\mathrm{uncnd}}(\bm {h}_{t_{k+1}})p_{k+1}^{\mathrm{uncnd}}(\bm{h}_{t_{k+1}})}{p_{k}^{\mathrm{uncnd} }(\bm{h}_{t_{k}})}\) and telescoping to cancel the marginals we have:

\[p(\bm{h}_{t_{1}:t_{k}}) =\frac{p_{K}^{\mathrm{uncnd}}(\bm{h}_{T})p(\bm{y}|\bm{h}_{0})\, \bm{\rho}_{\mathrm{data}}(\bm{h}_{0})}{p(\bm{y})\,\,\,\bm{\rho}_{\mathrm{data} }(\bm{h}_{0})}\prod_{k=1}^{K}p_{k|k+1}^{\mathrm{uncnd}}(\bm{h}_{t_{k}}|\bm{h}_ {t_{k+1}})\] (55) \[=\frac{p_{K}^{\mathrm{uncnd}}(\bm{h}_{T})p(\bm{y}|\bm{h}_{0})}{p( \bm{y})}\prod_{k=1}^{K}p_{k|k+1}^{\mathrm{uncnd}}(\bm{h}_{t_{k}}|\bm{h}_{t_{k+ 1}}),\] (56)

where \(p_{k|k+1}^{\mathrm{uncnd}}(\bm{h}_{t_{k}}|\bm{h}_{t_{k+1}})\) is the transition density of the unconditional score SDE and \(p_{k}^{\mathrm{uncnd}}(\bm{h}_{t_{k}})\) correspond to its marginals. Now we would like to learn a backwards process that matches the above process (reverses the VP-SDE starting from the posterior). We can do so by minimising the KL:

\[D_{\mathrm{KL}}(q^{\phi}||p)\propto\mathbb{E}_{q}\left[\ln\frac{\prod_{k}q_{k |k+1}^{\phi}(\bm{h}_{t_{k}}|\bm{h}_{t_{k+1}})}{\prod_{k}p_{k|k+1}^{\mathrm{ uncnd}}(\bm{h}_{t_{k}}|\bm{h}_{t_{k+1}})}-\ln p(\bm{y}|\bm{h}_{0})\right]\] (57)

where we can approximate the score transition via:

\[p_{k-1|k}^{\mathrm{uncnd}}(\bm{h}_{t_{k}}|\bm{h}_{t_{k+1}})\approx\mathcal{N} (\bm{h}_{t_{k-1}}|\sqrt{1-\alpha_{k}}\bm{h}_{t_{k}}+2(1-\sqrt{1-\alpha_{k}})s _{\theta^{*}}(\bm{h}_{t_{k}}),\alpha_{k})\]

and parametrise the new conditional denoiser as

\[q_{k-1|k}^{\phi}(\bm{h}_{t_{k}}|\bm{h}_{t_{k+1}})=\mathcal{N}(\bm{h}_{t_{k-1} }|\sqrt{1-\alpha_{k}}\bm{h}_{t_{k}}+2(1-\sqrt{1-\alpha_{k}})\left(s_{\theta^{* }}(\bm{h}_{t_{k}})+h_{\phi}(\bm{h}_{t_{k}})\right),\alpha_{k})\]

making these two substitutions will lead to the objective in Eqn. (53).

### Related Work

Fine-tuning diffusion models via a optimal control perspective, e.g., devolved in [6], has received a lot of attention in recent years. In particular, in the context of fine-tuning with respect to a differentiable reward function, i.e., considering a tilted posterior [18],

\[\pi(\bm{x}_{0})=\frac{e^{r(\bm{x}_{0})}p_{\mathrm{data}}(\bm{x}_{0})}{\mathcal{ Z}},\] (58)

where \(e^{r(\bm{x}_{0})}\) serves an equivalent role to the likelihood \(p(\bm{y}|\bm{x}_{0})\) in our setting.

The DRAFT framework [13] proposes a heuristic method to estimate the \(h\)-transform by only optimising the reward function. We want to highly that concurrently [72] develop the same stochastic control formulation as we do, and arrive at the same insight that the optimal starting distribution is given by \(p_{T}=Q_{T}^{f_{t}}[\pi]\), however, they chose to learn this distribution which we argue is not necessary for diffusion models due to the mixing property of the OU process leading to a negligible error by approximating \(Q_{T}^{f_{t}}[\pi]\approx\mathcal{N}(0,I)\) with a Gaussian.

### Connection to [18] - Value Function Bias

In [18], it is argued that minimising the stochastic control objective does not lead to hitting the posterior \(p(\bm{x}_{0}|\bm{y})\) at time \(t=0\) due to bias introduced by the value function. We can apply Proposition G.1 to the tilted posterior \(\pi(\bm{x}_{0})\) from (58) which yields the following objective:

\[h^{*}=\operatorname*{arg\,min}_{h}\mathbb{E}_{\mathbb{Q}}\left[\frac{1}{2}\int_ {0}^{T}\sigma_{t}^{2}||h(\bm{H}_{t})||^{2}\mathrm{d}t\right]-\mathbb{E}_{\bm{ H}_{0}\sim\mathbb{Q}_{0}}[r(\bm{H}_{0})]\] (59)with

\[\bm{H}_{T} \sim Q_{T}^{f_{t}}[p(\bm{x}_{0}|\bm{y})]\] \[\mathrm{d}\bm{H}_{t} =\left(f_{t}(\bm{H}_{t})-\sigma_{t}^{2}(\nabla_{\bm{H}_{t}}\ln p_{ t}(\bm{H}_{t})+h_{t}(\bm{H}_{t}))\right)\,\mathrm{d}t+\sigma_{t}\ \ \widetilde{\mathrm{d}\mathbf{W}}_{t},\] (60)

then by Theorem 2.1 in [71] the optimal transition density of the controlled process is given by (\(s\leq t\) and let \(\tilde{h}(\bm{x}_{t})=\ln p_{y|t}(\bm{y}|\bm{x}_{t})\)):

\[p_{s|t}^{*}(\bm{x}|\bm{y})=e^{\tilde{h}(\bm{x},s)-\tilde{h}(\bm{y},t)}p_{s|t}^ {\mathrm{ref}}(\bm{x}|\bm{y})\] (61)

where the log of \(h\)-transform \(\tilde{h}\) (note we have used \(\tilde{h}\) to denote the log of the \(h\)-transform) coincides with the negative of the value function in [18, 71]. Then for \(s=0\) and \(t=T\) this induces the following joint distribution:

\[p_{0,T}^{*}(\bm{x}|\bm{y})=e^{r(\bm{x})-\ln\mathcal{Z}-\tilde{h}(\bm{y},T)}p_{ s|t}^{\mathrm{ref}}(\bm{x}|\bm{y})p_{T}(\bm{y})\] (62)

where [18] argue that in general the term \(e^{-h(\bm{y},T)}\) induces a bias such that when we marginalise out \(\bm{y}\), \(p_{0}^{*}(\bm{x})\) is not the tilted distribution, more precisely:

\[p_{0}^{*}(\bm{x})=e^{r(\bm{x})-\ln\mathcal{Z}}\int e^{-\tilde{h}(\bm{y},T)}p_{ 0|T}^{\mathrm{ref}}(\bm{x}|\bm{y})p_{T}(\bm{y})\mathrm{d}\bm{y}\neq\frac{e^{r (\bm{x})}p_{\mathrm{data}}(\bm{x})}{\mathcal{Z}}\] (63)

However, let's look more closely as to why this is the case; first let us re-express the h-transform at time \(T\) as a ratio of densities:

\[e^{\tilde{h}(\bm{y},T)} =\int\frac{\mathrm{d}\pi}{\mathrm{d}p_{\mathrm{data}}}(\bm{y}_{0 })p_{0|T}^{\mathrm{ref}}(\bm{y}_{0}|\bm{y}_{T})\mathrm{d}\bm{y}_{0}\] (64) \[=\int\frac{e^{r(\bm{y}_{0})}p_{\mathrm{data}}(\bm{y}_{0})}{ \mathcal{Z}p_{\mathrm{data}}(\bm{y}_{0})}p_{0|T}^{\mathrm{ref}}(\bm{y}_{0}| \bm{y}_{T})\mathrm{d}\bm{y}_{0}\] (65) \[=\frac{1}{p_{T}^{\mathrm{ref}}(\bm{y}_{T})}\int\pi(\bm{y}_{0})p_{ T|0}^{\mathrm{ref}}(\bm{y}_{T}|\bm{y}_{0})\mathrm{d}\bm{y}_{0}\] (66) \[=\frac{Q_{T}^{f_{t}}[\pi](\bm{y}_{T})}{p_{T}^{\mathrm{ref}}(\bm{y }_{T})}\] (67)

substituting back into (62) and marginalizing we have:

\[p_{0}^{*}(\bm{x})=e^{r(\bm{x})-\ln\mathcal{Z}}\int p_{0|T}^{\mathrm{ref}}(\bm{ x}|\bm{y})p_{T}(\bm{y})\frac{p_{T}^{\mathrm{ref}}(\bm{y})}{Q_{T}^{f_{t}}[\pi]( \bm{y})}\mathrm{d}\bm{y}\] (68)

now notice \(p_{T}(\bm{y})\) is the distribution we simulate our stochastic control from which in our case we have chosen to be \(p_{T}=Q_{T}^{f_{t}}[\pi]\) making the cancellation

\[p_{0}^{*}(\bm{x})=e^{r(\bm{x})-\ln\mathcal{Z}}\int p_{0|T}^{\mathrm{ref}}(\bm{ x}|\bm{y})p_{T}^{\mathrm{ref}}(\bm{y})\mathrm{d}\bm{y}=\frac{e^{r(\bm{x})}p_{ \mathrm{data}}(\bm{x})}{\mathcal{Z}}\] (69)

Leading us to the following remark

**Remark 1**.: _The choice of setting \(p_{T}=Q_{T}^{f_{t}}[\pi]\) leads to removing the value function bias [18] in Equation 63. Note the authors of [18] pursue a different avenue for removing this bias by altering the noise of the controlled SDE._

**Proposition G.2**.: _(Value function bias when approximating \(Q_{T}^{f_{t}}[\pi]\approx\mathcal{N}(0,I)\)) In practice, we often do not have access to \(Q_{T}^{f_{t}}[\pi]\) and thus we may make an approximation with some tractable distribution \(p_{T}\), then we can bound the value function bias as follows_

\[\left\|p_{0}^{*}(\bm{x})-\frac{e^{r(\bm{x})}p_{\mathrm{data}}(\bm{x})}{ \mathcal{Z}}\right\|_{\mathrm{TV}}\leq\left\|p_{T}-Q_{T}^{f}[\pi]\right\|_{ \mathrm{TV}},\] (70)

_then in the VP-SDE with a time homogenous \(\beta_{t}=\beta\) based diffusion model (for simplicity), where chose \(p_{T}=\mathcal{N}(0,I)\) we can obtain a tight bound,_

\[\left\|p_{0}^{*}(\bm{x})-\frac{e^{r(\bm{x})}p_{\mathrm{data}}(\bm{x})}{ \mathcal{Z}}\right\|_{\mathrm{TV}}\leq Ce^{-\beta T}\] (71)

_for some constant \(C>0\), thus the value function bias [18] is exponentially small for score based diffusion models._Proof.: Applying Theorem 17 from [15] we have:

\[\left\|p_{0}^{*}(\bm{x})-\frac{e^{r(\bm{x})}p_{\mathrm{data}}(\bm{x })}{\mathcal{Z}}\right\|_{\mathrm{TV}} =\left\|P_{0}^{f_{t}+h_{t}^{*}}[\mathcal{N}(0,I)]-P_{0}^{f_{t}+h _{t}^{*}}[Q^{f}[\pi]]\right\|_{\mathrm{TV}}\] (72) \[\leq\left\|\mathcal{N}(0,I)-Q_{T}^{f}[\pi]\right\|_{\mathrm{TV}} =Ce^{-\beta T},\] (73)

the final equality follows from the mixing properties of the OU process [4], where

\[P_{0}^{f_{t}+h_{t}^{*}}[\mu](\bm{x})=\int p_{0|T}^{*}(\bm{x}|\bm{y })\mu(\bm{x})\mathrm{d}\bm{y}\] (74)

Finally we want to highlight that the benefits of Proposition G.2 can only be leveraged in score matching settings where we have a clear characterisation of the forward process and we are able to tractably characterise \(p_{0}^{*}\) however in settings such as flow matching [38, 41] and stochastic interpolants [1] where the forward process is not explicitly characterised we wither have to learn \(p_{0}^{*}\) like in [72] or use a memoryless noise schedule as proposed in [18].

### Scaling up the Control Objective

Naively trying to minimise Eqn. (37) is demanding, as the full chain has to be kept in memory, which is infeasible for high dimensional problems. To alleviate this problem, one could make use of the stochastic adjoint sensitivity method [36], in which an adjoint SDE is solved to estimate the gradients of the stochastic control loss in Theorem 3.13). This method has the advantage of a constant memory cost. However, the computational cost increases as both the reverse SDE and the adjoint SDE must be simulated. Instead, we discuss two alternative approaches to reduce the memory requirements.

VarGradWe can make use of a VarGrad [53, 54] type loss to reduce the memory requirements. In contrast to the KL loss of Eqn. (37), then the VarGrad loss is given by:

\[D_{\mathrm{logvar}}(\mathbb{Q},\mathbb{P};\mathbb{W})=\mathbb{E} _{\bm{H}_{0:T}^{g_{t}}\sim\mathbb{W}}\left[\left(\ln\frac{\mathrm{d}\mathbb{Q} }{\mathrm{d}\mathbb{P}}(\bm{H}_{0:T}^{g_{t}})-\mathbb{E}\left[\ln\frac{ \mathrm{d}\mathbb{Q}}{\mathrm{d}\mathbb{P}}(\bm{H}_{0:T}^{g_{t}})\right] \right)^{2}\right],\] (75)

where \(\mathbb{Q}\) and \(\mathbb{P}\) are defined as in the proof of Proposition G.1, i.e., \(\mathbb{Q}\) is given by the conditional SDE and \(\mathbb{P}\) is given by the unconditional SDE. The RND in Eqn. (77) is evaluated at the trajectory of a reference process \(\mathbb{W}=\mathrm{Law}(\bm{H}_{0:T}^{g_{t}})\), given by

\[\bm{H}_{T} \sim Q_{T}^{f_{t}}[p(\bm{x}_{0}|\bm{y})]\] \[\mathrm{d}\bm{H}_{t} =\left(f_{t}(\bm{H}_{t})-\sigma_{t}^{2}(\nabla_{\bm{H}_{t}}\ln p_{ t}(\bm{H}_{t})+g_{t}(\bm{H}_{t}))\right)\,\mathrm{d}t+\sigma_{t}\,\,\overline{ \mathrm{d}\mathbf{W}}_{t}.\] (76)

The Radon-Nikodym derivative (RND) in Eqn (75) can be evaluated as (Using the RND for time reverse SDEs see Equation 64 in [76]):

\[\ln\frac{\mathrm{d}\mathbb{Q}}{\mathrm{d}\mathbb{P}}(\bm{H}_{0:T} ^{g_{t}}) =-\frac{1}{2}\int_{0}^{T}\sigma_{t}^{2}\|h_{t}(\bm{H}_{t}^{g_{t}} )\|^{2}\mathrm{d}t+\int_{0}^{T}\sigma_{t}^{2}(g_{t}^{\top}h_{t})(\bm{H}_{t}^{ g_{t}})\mathrm{d}t-\ln p(\bm{y}|\bm{x}_{0}^{g_{t}})\] \[+\int_{0}^{T}\sigma_{t}h_{t}^{\top}(\bm{H}_{t}^{g_{t}})\,\overline {\mathrm{d}\mathbf{W}}_{t},\] (77)

for the reference process \(\mathbb{W}\). The core advantage of VarGrad is that we can choose this reference process. In particular, the choice \(g_{t}=\mathrm{stop\_grad}(h_{t})\) gives us a way to detach the trajectories, saving us from having to score all gradients in memory.

Trajectory BalanceAn alternative to the VarGrad loss in Eqn. (75) is the following trajectory balance [42, 43] loss

\[\mathcal{L}_{\mathrm{TB}}^{\mathbb{W}}(\mathbb{Q},\mathbb{P};k)= \mathbb{E}_{\bm{H}_{0:T}^{g_{t}}\sim\mathbb{W}}\left[\left(\ln\frac{\mathrm{d} \mathbb{Q}}{\mathrm{d}\mathbb{P}}(\bm{H}_{0:T}^{g_{t}})-k\right)^{2}\right],\] (78)

where \(k\in\mathbb{R}\) is a learnable parameter and \(\mathbb{W}\) is the same reference process as above. We again choose \(g_{t}=\mathrm{stop\_grad}(h_{t})\) This loss is also motivated by a valid divergence, see [48]. In difference to the VarGrad loss (75), the inner expectation is exchanged with \(k\), which approximates a running mean. In practice, we optimise \(k\) and and \(h_{t}\) at the same time.

Trajectory SubsamplingWe observed that backpropagating gradients for only a random subset of discrete timesteps in the RND can be an effective strategy. This subsampling reduces memory costs at the expense of increased variance and the introduction of a small bias. Nevertheless, the reduced memory usage per example enables larger batch sizes, which can mitigate these effects. Notably, we found that backpropagating for only \(20\%\) of the timesteps still achieves comparable performance.

### Stochastic Optimal Control - Experiments

We provide some initial proof of concept experiments on the MNIST dataset [34] of handwritten digits. In particular, we make use of a parallel beam Radon transform with \(5\) angles as the forward operator and perturb the observations with \(10\%\) additive Gaussian noise. All SDEs are discretised using an Euler-Maruyama scheme and the integrals are estimated using simple quadrature rules. We use a non-equidistant time grid according to a square root function, i.e., let \(0=t_{0}\leq\cdots\leq t_{K-1}=1\) be an equidistant grid of \([0,1]\) for \(K\) time points. We then use \(t_{0}^{2},t_{1}^{2},\ldots,t_{K-1}^{2}\) as the time points for evaluating the SDEs. This gives us a finer discretisation closer to \(t=0\). The unconditional MNIST model is based on the attention U-Net architecture [16] with about 3M parameters. The \(h\)-transform is implemented using the same DEFT parametrisation as in Section 3.3 with about \(70\,000\) parameters. We compare EM-backprop, i.e., directly backpropagating through the discrete SDE solver (see e.g. [36]), with VarGrad and TrajectoryBalance from Section G.3. For EM-backprop we use a batch size of \(16\) and use \(60\)-\(80\) time steps. Instead, for VarGrad and TrajectoryBalance we were able to use a batch size of \(26\) and \(80\)-\(140\) time steps. For training we used a single GeForce RTX 3090 and the training time took about \(1\)h. The results are presented in Figure 11, where the loss trajectory of Eqn. (37) and the mean PSNR of samples is shown. Both VarGrad and TrajectoryBalance are able to minimise the stochastic optimal control objective to the same extend as EM-backprop. Further, in Figure 12 we provide an overview of the TrajectoryBalance training. Here, we observe that \(k\) is working as a estimator of the mean RND with a lower variance.

## Appendix H Amortised Conditional Training

In this section, we discuss an objective for learning the full conditional score at training time in an amortised fashion instead of enforcing the constraint during inference time as before in reconstruction guidance approaches. This objective is akin to CDE [5] with the difference that we propose amortising over the the forward operator, for example in image inpainting or motif-scaffolding.

Note that since \(\overline{P}_{0|t}(\bm{Y}=\bm{y}|\bm{X}_{t}=\bm{x})=\overline{P}_{t|0}(\bm{x }|\bm{Y}=\bm{y})p_{0}(\bm{Y}=\bm{y})/p_{t}(\bm{X}_{t}=\bm{x})\), we can re-express the Doob's transformed SDE of a reversed OU process as:

\[\mathrm{d}\bm{H}_{t}=-\beta_{t}\left(\bm{H}_{t}+2\nabla_{\bm{H}_{t}}\ln \overline{P}_{t|0}(\bm{H}_{t}|\bm{Y}=\bm{y})\right)\,\mathrm{d}t+\sqrt{2\beta _{t}}\,\,\overline{\mathrm{d}\mathbf{W}}_{t},\,\,\,\,\bm{H}_{T}\sim\mathrm{ Law}\left(\bm{X}_{T}\right).\]

Figure 11: Left: Tracking the stochastic optimal control loss (37) for the three methods. Right: Mean PSNR of samples.

**Proposition H.1**.: _The minimiser of_

\[f^{*} = \operatorname*{arg\,min}_{h}\mathbb{E}_{\begin{subarray}{c}(\bm{X}_ {0},\bm{Y})\sim p(\bm{x}_{0},\bm{Y})\\ t\sim\mathrm{U}(0,T),\bm{H}_{t}\sim p_{t0}(\bm{x}_{t}|\bm{x}_{0})\end{subarray}} \left[||h(t,\bm{H}_{t},\bm{y})-\nabla_{\bm{H}_{t}}\ln\bar{p}_{t|0}(\bm{H}_{t}| \bm{X}_{0})||^{2}\right],\] (79) _is given by the conditional score \[f^{*}_{t}(\bm{x},\bm{y})=\nabla_{\bm{x}}\ln\bar{p}_{t|0}(\bm{x}|\bm{Y}=\bm{y}).\]_

Proof.: Via the mean squared error property of the conditional expectation the minimiser is given by:

\[h^{*}_{t}(\bm{x},\bm{y})=\mathbb{E}\left[\nabla_{\bm{H}_{t}}\ln\bar{p}_{t|0}( \bm{X}_{t}|\bm{X}_{0})|\bm{Y}=\bm{y},\bm{H}_{t}=\bm{x}\right]\] (80)

Then:

\[h^{*}_{t}(\bm{x},\bm{y})=\int\nabla_{\bm{x}}\ln\bar{p}_{t|0}(\bm {x}|\bm{x}_{0})\bar{p}_{0|t}(\bm{x}_{0}|\bm{H}_{t}=\bm{x},\bm{Y}=\bm{y})\mathrm{ d}\bm{x}_{0}\] \[=\int\frac{\nabla_{\bm{x}}\bar{p}_{t|0}(\bm{x}|\bm{x}_{0})}{\bar{p }_{t|0}(\bm{x}|\bm{x}_{0})}\frac{\bar{p}_{t|0}(\bm{H}_{t}=\bm{x}|\bm{x}_{0},\bm {Y}=\bm{y})p(\bm{x}_{0}|\bm{Y}=\bm{y},)}{p(\bm{H}_{t}=\bm{x}|\bm{Y}=\bm{y})} \mathrm{d}\bm{x}_{0}\] \[=\frac{1}{p(\bm{H}_{t}=\bm{x}|\bm{Y}=\bm{y})}\int\frac{\nabla_{ \bm{x}}\bar{p}_{t|0}(\bm{x}|\bm{x}_{0})}{\bar{p}_{t|0}(\bm{x}|\bm{x}_{0})}\bar{ p}_{t|0}(\bm{H}_{t}=\bm{x}|\bm{x}_{0})p(\bm{x}_{0}|\bm{Y}=\bm{y})\mathrm{d}\bm{x}_{0}\] \[=\frac{1}{p(\bm{H}_{t}=\bm{x}|\bm{Y}=\bm{y})}\nabla_{\bm{x}}\int \bar{p}_{t|0}(\bm{x}|\bm{x}_{0})p(\bm{x}_{0}|\bm{Y}=\bm{y})\mathrm{d}\bm{X}_{0}\] \[=\frac{1}{\bar{p}(\bm{X}_{t}=\bm{x}|\bm{Y}=\bm{y})}\nabla_{\bm{x} }\bar{p}(\bm{X}_{t}=\bm{x}|\bm{Y}=\bm{y})\] (81) \[=\nabla_{\bm{x}}\ln\bar{p}(\bm{X}_{t}=\bm{x}|\bm{Y}=\bm{y}),\]

where we use that \(\bar{p}_{t|0}(\bm{H}_{t}=\bm{x}|\bm{x}_{0},\bm{Y}=\bm{y})=\bar{p}_{t|0}(\bm{H} _{t}=\bm{x}|\bm{x}_{0})\) as \(\bm{H}_{t}\) is independent from \(\bm{Y}\) given \(\bm{X}_{0}\). 

As with DEFT, for settings where \(\mathcal{A}\) varies like in image completion we sample \(\mathcal{A}\) randomly and amortise it over our learned \(h\)-transform, i.e. estimating \(h^{*}_{t}(\bm{x},\bm{y},\bm{A})\).

We refer to this approach as _amortised_ learning for conditional sampling, since practically the neural network approximating the (conditional) score is amortised over \(\mathcal{A}\) and \(\bm{y}\), instead of learning a separate network for each condition. This approach is also reminiscent of 'classifier free guidance' [27] where the score network is amortised over some auxiliary variable (e.g. as in text-to-image models [51]), or of RFDiffusion [77] where proteins are designed given a specific subset motif, or similar to [5]. See also Appendix B for a discussion of related conditional training methodologies.

Figure 12: Training using TrajectoryBalance. Left: The trajectory balance objective loss (78) over training steps. Right: The mean RND and the trained \(k\). We see that \(k\) follows the mean RND. However, it has a smaller variance.

Note that conditional amortised learning is different to 'classifier free guidance' as \(\mathcal{A}\) is assumed to be known (e.g. an inpainting mask). Also note that due to its formulation, classifier guidance would be unable to noise a subset of \(\bm{X}\) (the motif) as we do and would instead be more akin to RFDiffusion.

### Relationship to Conditional denoising estimator (CDE)

Conditional denoising estimator (CDE) [5] is the adaptation of [60, 27] to inverse problem-like settings, deriving a variation of classifier-free guidance to a measurement model styled scenario. Whilst they do not focus on the measurement model, they estimate a very similar quantity as our Proposition 2.5

\[f^{\mathrm{CDE}}(\bm{x},\bm{y})=\nabla_{\bm{x}}\ln\bar{p}_{t|0}(\bm{x}|\bm{Y}= \bm{y})\] (82)

In contrast to to the amortised conditional training:

\[f^{\mathrm{amortised}}(\bm{x},\bm{y},\bm{A})=\nabla_{\bm{x}}\ln\bar{p}_{t|0} (\bm{x}|\bm{Y}=\bm{y},\mathcal{A}=\bm{A})\] (83)

when explicitly considering the distribution over the measurement model, one can see that the quantities are related to one another via marginalizing the measurement model \(p_{\mathcal{A}}\). This introduces several practical and conceptual differences:

* If we consider in/out painting as an example, the score network estimating \(f^{\mathrm{CDE}}\) is not explicitly aware of where in the image the missing pixels are. As a result, it must perform inference over \(\mathcal{A}\) (effectively marginalizing it) in order to know where to complete the image. This is clearly a much harder task for a single network to learn than conditioning on \(\mathcal{A}\) where we provide this information.
* Viewed under the lens of the h-transform, \(f^{\mathrm{CDE}}\) can be viewed as amortising the event \(\mathcal{A}(\bm{X}_{0})=\bm{y}\) for random \(\mathcal{A}\). It therefore falls under the soft constraint settings since \(\mathcal{A}(\bm{X}_{0})|\bm{X}_{0}\) is not a delta. Our quantity \(f^{\mathrm{amortised}}\) is amortising over \(\bm{A}(\bm{X}_{0})=\bm{y}\) for deterministic \(\bm{A}\) and is therefore part of the more classical hard constraint domain of Doobs transform. We believe amortising over these simpler deterministic events can offer an advantage in making the problem easier to learn.

### Comparison to \(h\)-transform fine-tuning

We compare the amortised training framework against our \(h\)-transform fine-tuning on the Flowers dataset. The preprocessing procedure consisted of centrally cropping the image to size \(64\times 64\), and rescaling to pixel values \([-1,1]\). The dataset is split into three parts containing \(6149\), \(1020\) and \(1020\) images each. We use the first part to train the unconditional and amortised model. The second part is used for the \(h\)-transform fine-tuning. The third part is used for evaluation.

For this experiment, we choose both an inpainting and an outpainting task. For the inpainting task a random \(18\times 18\)px patch from the image is removed. In difference, for outpainting only a random \(18\times 18\)px patch remains and the rest of the image is removed. Thus, the outpainting tasks tests better the generational capabilities of our framework. For the unconditional and amortised model, we use a standard attention U-Net [16] in the discrete DDPM framework. Both the unconditional and the amortised model have about \(24\)M parameters. There is a minor difference due the fact that the unconditional network has \(3\) input channels and the amortised model has \(7\) input channels, i.e., the noisy image, the observations and the mask. The \(h\)-transform is implemented according to the parametrisation in Section 3.3 with an attention U-Net [16] for \(\text{NN}^{\phi}_{1}\). In total, the \(h\)-transform has about \(4\)M parameters, i.e., about \(18\%\) of the size of the amortised model. We evaluate three different settings for the amortised model:

* Amortised (20x, full data): trained on the full training dataset for \(1200\) epochs,
* Amortised (2x): trained on the fine-tuning dataset for \(700\) epochs,
* Amortised (1x): trained on the fine-tuning dataset with the same computational budget as DEFT (\(300\) epochs).

We trained all models on a single GeForce RTX 3090. Training time for Amortised (20x, full data) and the unconditional model was about 50h. The training time for Amortised (2x) was \(4.5\)h, while the fine-tuning and Amortised (1x) took about \(2.5\)h.

[MISSING_PAGE_EMPTY:34]

Figure 14: Comparison of the amortised model, DPS and DEFT (9%) on the task 1YCR. We see the general trend for DPS that for low guidance scales the samples have high designability but do not adhere to the motif constraint, while for higher guidance scales they adhere to the motif constraint but have low designability.

Figure 15: Comparison of samples from the amortised model, DPS and DEFT (9%) on the tasks 6EXZ med. One can see that while the amortised and DEFT samples incorporate the motif into a realistic backbone, this is not the case for DPS. We generally observed that at small guidance scales DPS produced realistic backbones without the desired motif and at high guidance scales it placed the motif into an unrealistic backbone.

Algorithms

In this section, we reformulate multiple algorithms from the literature under our common framework as a reference for practitioners. In these algorithms, we use the following conventions: our dataset is drawn from the law \(\mathcal{P}_{\text{data}}\), but we can only sample from the simpler law \(\mathcal{P}_{\text{sampling}}\) at inference time, which is often chosen as multivariate standard normal \(\mathcal{P}_{\text{sampling}}=\mathcal{N}(0,\mathbf{I})\). Therefore, we construct a forward noising process \(\mathcal{P}_{\text{data}}\to\mathcal{P}_{\text{sampling}}\) that is parametrised via the noise schedule \(\beta_{t}=\beta(t),\bar{\alpha}_{t}=\bar{\alpha}(t)\) and try to learn the reverse denoising process \(\mathcal{P}_{\text{sampling}}\to\mathcal{P}_{\text{data}}\). Due to this notion of "forward", and to keep consistency with the literature on denoising diffusion models, we explicate the nomenclature \(\mathcal{P}_{\text{data}}=\mathcal{P}_{0}\) and \(\mathcal{P}_{\text{sampling}}=\mathcal{P}_{T}\).

There is an additional law \(\mathcal{P}_{\text{noise}}\) that is sometimes confused with \(\mathcal{P}_{\text{sampling}}\) since in practice both are often chosen as \(\mathcal{N}(0,\mathbf{I})\), but they are two distinct laws that could in principle be different. \(\mathcal{P}_{\text{noise}}\) is the law from which the noise added during the forward noising process as well as the during the reverse diffusion process is drawn from.

```
0:Dataset drawn from law \(\mathcal{P}_{\text{data}}=\mathcal{P}_{0}\)\(\triangleright\)Dataset law \(\mathcal{P}_{\text{data}}\)
0:Noise schedule \(\beta_{t}=\beta(t),\bar{\alpha}_{t}=\bar{\alpha}(t)\), parametrising process \(\mathcal{P}_{\text{data}}\to\mathcal{P}_{\text{sampling}}\)
0: Untrained noise predictor function \(\epsilon_{t}^{\theta}(\bm{x})\) with parameters \(\theta\)
1:repeat
2:\(\bm{x}_{0}\sim\mathcal{P}_{0}=\mathcal{P}_{\text{data}}\)
3:\(t\sim\text{Uniform}(\{1,...,T\})\)
4:\(\triangleright\) Forward noise sample, \(x_{t}\sim\widetilde{p}_{t|0}(x_{0})\)\(\triangleright\)Often Brownian motion, \(\mathcal{P}_{\text{noise}}=\mathcal{N}(0,\mathbf{I})\)
5:\(\bm{\varepsilon}_{t}\sim\mathcal{P}_{\text{noise}}\)
6:\(\bm{x}_{t}\leftarrow\sqrt{\bar{\alpha}_{t}}\bm{x}_{0}+\sqrt{1-\bar{\alpha}_{ t}}\bm{\varepsilon}_{t}\)
7:\(\triangleright\)Estimate noise of noised sample
8:\(\hat{\bm{\varepsilon}}_{\theta}\leftarrow\epsilon_{t}^{\theta}(\bm{x}_{t})\)
9: Take gradient descent step on \(\nabla_{\theta}L(\bm{\varepsilon}_{t},\hat{\bm{\varepsilon}}_{\theta})\)\(\triangleright\)Typically, loss \(L(\bm{x}_{\text{true}},\bm{x}_{\text{pred}})=||x_{\text{true}}-x_{\text{pred}}||^{2}\)
10:until converged or max epoch reached ```

**Algorithm 1** Unconditional training of denoising diffusion models [28]

```
0:Unconditionally trained noise predictor \(\epsilon_{t}^{\theta}(\bm{x}_{t})\)
0:Noise schedule \(\beta_{t}=\beta(t),\bar{\alpha}_{t}=\bar{\alpha}(t)\), parametrising process \(\mathcal{P}_{\text{data}}\to\mathcal{P}_{\text{sampling}}\)
1:\(\triangleright\)Sample a starting point \(x_{T}\)\(\triangleright\)Often \(\mathcal{P}_{T}=\mathcal{N}(0,\mathbf{I})\)
2:\(\bm{x}_{T}\sim\mathcal{P}_{T}=\mathcal{P}_{\text{sampling}}\)
3:\(\triangleright\)Iteratively denoise for \(T\) steps\(\triangleleft\)
4:for\(t\) in \((\bm{T},\bm{T}-1,\dots,1)\)do
5:\(\triangleright\)Predict noise with learned network
6:\(\hat{\bm{\varepsilon}}_{\theta}\leftarrow\epsilon_{t}^{\theta}(\bm{x}_{t})\)
7:\(\triangleright\)Denoise sample with learned reverse process \(x_{t-1}\sim\widetilde{p}_{t-1|t}(x_{t})\)\(\triangleleft\)
8:\(\triangleright\)Perform reverse drift
9:\(\bm{x}_{t-1}\leftarrow\frac{1}{\sqrt{1-\beta_{t}}}\left(\bm{x}_{t}-\frac{\bm{ \beta}_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\bm{\varepsilon}_{\theta}\right)\)
10:\(\triangleright\)Perform reverse diffusion, which is often Brownian motion in \(\mathbb{R}^{n}\), i.e. \(\mathcal{P}_{\text{noise}}=\mathcal{N}(0,\mathbf{I})\)\(\bm{\varepsilon}_{t}\sim\mathcal{P}_{\text{noise}}\) if\(t>1\) else\(\bm{\varepsilon}_{t}\leftarrow\mathbf{0}\)
11:\(\bm{x}_{t-1}\leftarrow\bm{x}_{t-1}+\sigma_{t}\bm{\varepsilon}_{t}\)\(\triangleright\)A common choice is \(\sigma_{t}=\beta(t)\)
12:return\(\bm{x}_{0}\) ```

**Algorithm 2** Unconditional sampling with denoising diffusion models [28]```
0:Dataset drawn from \(\mathcal{P}_{\text{data}}\)\(\triangleright\)Dataset law \(\mathcal{P}_{\text{data}}\)
0:Noise schedule \(\beta_{t}=\beta(t),\bar{\alpha}_{t}=\bar{\alpha}(t)\), parametrising process \(\mathcal{P}_{\text{data}}\rightarrow\mathcal{P}_{\text{sampling}}\)
0: Untrained conditional noise predictor function \(\mathbf{f}_{\theta}(\bm{x},\mathbf{t},\mathbf{\underline{M}})\) with parameters \(\theta\)
1:repeat
2:\(\bm{x}_{0}\sim\mathcal{P}_{0}=\mathcal{P}_{\text{data}}\)
3:\(t\sim\text{Uniform}(\{1,...,T\})\)
4:\(\overline{\bm{x}}_{0}^{[M]}\cup\bm{x}_{0}^{[M]}\leftarrow\bm{x}_{0}\)\(\triangleright\)Randomly partition data point into motif and rest
5:\(\triangleright\)Forward noise the non-motif rest via sampling from \(\widetilde{p}_{0|t}(\bm{x}_{0})\)\(\triangleleft\)
6:\(\bm{\varepsilon}_{t}\sim\mathcal{P}_{\text{noise}}\)
7:\(\bm{x}_{t}^{[M]}\leftarrow\sqrt{\bar{\alpha}_{t}}\bm{x}_{0}^{[M]}+\sqrt{1- \bar{\alpha}_{t}}\bm{\varepsilon}_{t}^{[M]}\)
8:\(\triangleright\)Combine unnoised motif with noised rest and set timestep of motif part to 0
9:\(\bm{x}_{t}\leftarrow\bm{x}_{0}^{[M]}\cup\bm{x}_{t}^{[M]}\)
10:\(\bm{t}^{[M]}\gets 0\)
11:\(\bm{\hat{e}}_{\theta}\leftarrow\mathbf{f}_{\theta}(\bm{x}_{t},\mathbf{\underline {t}},\mathbf{\underline{M}})\)\(\triangleright\)Estimate noise of sample with noised rest
12: Take gradient descent step on \(\nabla_{\theta}L(\bm{\varepsilon},\bm{\hat{e}}_{\theta})\)\(\triangleright\)Typically, \(L(x_{\text{true}},x_{\text{pred}})=||x_{\text{true}}-x_{\text{pred}}||^{2}\)
13:until converged or max epoch reached ```

**Algorithm 4**\(|\) Amortised training - i.e. Doob's \(h\)-transform conditional training for motif-scaffolding

```
0:Dataset drawn from \(\mathcal{P}_{\text{data}}\)\(\triangleright\)Dataset law \(\mathcal{P}_{\text{data}}\)
0:Noise schedule \(\beta_{t}=\beta(t),\bar{\alpha}_{t}=\bar{\alpha}(t)\), parametrising process \(\mathcal{P}_{\text{data}}\rightarrow\mathcal{P}_{\text{sampling}}\)
0: Untrained amortised noise predictor function \(\mathbf{f}_{\theta}(\bm{x},\mathbf{t},\mathbf{\underline{x}}^{[M]},\mathbf{ \underline{M}})\) with parameters \(\theta\)
1:repeat
2:\(\bm{x}_{0}\sim\mathcal{P}_{0}=\mathcal{P}_{\text{data}}\)
3:\(t\sim\text{Uniform}(\{1,...,T\})\)
4:\(\overline{\bm{x}}_{0}^{[M]}\cup\mathbf{x}_{0}^{[M]}\leftarrow\bm{x}_{0}\)\(\triangleright\)Randomly partition data point into motif and rest
5:\(\triangleright\)Forward noise full sample via sampling from \(\widetilde{p}_{0|t}(\bm{x}_{0})\)\(\triangleleft\)
6:\(\bm{\varepsilon}_{t}\sim\mathcal{P}_{\text{noise}}\)
7:\(\bm{x}_{t}\leftarrow\sqrt{\bar{\alpha}_{t}}\bm{x}_{0}+\sqrt{1-\bar{\alpha}_{t}} \bm{\varepsilon}_{t}\)
8:\(\triangleright\)Estimate noise of sample with original motif as additional input
9:\(\bm{\hat{e}}_{\theta}\leftarrow\mathbf{f}_{\theta}(\bm{x}_{t},\mathbf{\underline {t}},\mathbf{\underline{x}}_{0}^{[M]},\mathbf{\underline{M}})\)
10: Take gradient descent step on \(\nabla_{\theta}L(\bm{\varepsilon},\bm{\hat{e}}_{\theta})\)\(\triangleright\)Typically, \(L(x_{\text{true}},x_{\text{pred}})=||x_{\text{true}}-x_{\text{pred}}||^{2}\)
11:until converged or max epoch reached ```

**Algorithm 5**\(|\) Amortised training - i.e. Doob's \(h\)-transform conditional training for motif-scaffolding
```
0: Dataset drawn from \(\mathcal{P}_{\text{data}}\)\(\triangleright\) Dataset law \(\mathcal{P}_{\text{data}}\)
0: Noise schedule \(\beta_{t}=\beta(t),\bar{\alpha}_{t}=\bar{\alpha}(t)\), parametrising process \(\mathcal{P}_{\text{data}}\rightarrow\mathcal{P}_{\text{sampling}}\)
0: Trained noise predictor function \(\epsilon_{t}^{\theta}(\bm{x})\) with parameters \(\theta\)
0: Untrained \(h\)-transform \(h_{t}^{\phi}(\bm{x},\hat{\bm{x}}_{0},\bm{y})\) with parameters \(\phi\)
1:repeat
2:\(\bm{x}_{0}\sim\mathcal{P}_{0}=\mathcal{P}_{\text{data}}\)
3:\(t\sim\text{Uniform}(\{1,...,T\})\)
4:\(\bm{y}\sim p(\bm{y}|\bm{x}_{0})\)\(\triangleright\) Simulate observations
5:\(\triangleright\) Forward noise full sample via sampling from \(\widetilde{p}_{0|t}(x_{0})\)\(\triangleleft\)
6:\(\bm{\varepsilon}_{t}\sim\mathcal{P}_{\text{noise}}\)
7:\(\bm{x}_{t}\leftarrow\sqrt{\bar{\alpha}_{t}}\bm{x}_{0}+\sqrt{1-\bar{\alpha}_{ t}}\bm{\varepsilon}_{t}\)
8:\(\bm{\hat{\varepsilon}}_{\theta}\leftarrow\epsilon_{t}^{\theta}(\bm{x}_{t})\)\(\triangleright\) Estimate noise of sample with pretrained model
9:\(\bar{\bm{x}}_{0}\leftarrow(\bm{x}_{t}-\sqrt{1-\bar{\alpha}_{t}}\bm{\hat{ \varepsilon}}_{\theta})/\sqrt{\bar{\alpha}_{t}}\)
10:\(\hat{\epsilon}_{\phi}\gets h_{t}^{\phi}(\bm{x}_{t},\hat{\bm{x}}_{0},\bm{y})\)\(\triangleright\) Estimate noise of sample with \(h\)-transform
11: Take gradient descent step w.r.t. \(\phi\) on \(\nabla_{\theta}L(\bm{\varepsilon},\hat{\bm{\varepsilon}}_{\theta}+\hat{\bm{ \varepsilon}}_{\phi})\)\(\triangleright\) Typically, \(L(\bm{x}_{\text{true}},\bm{x}_{\text{pred}})=||\bm{x}_{\text{true}}-\bm{x}_{ \text{pred}}||^{2}\)
12:until converged or max epoch reached ```

**Algorithm 6**\(|\)\(h\)-transform DDIM sampling (new)

```
0: Trained \(h\)-transform \(h_{t}^{\phi}(\bm{x},\hat{\bm{x}}_{0},\bm{y})\) with parameters \(\phi\)
0: Unconditionally trained noise predictor \(\epsilon_{t}^{\theta}(\bm{x}_{t})\)
0: Noise schedule \(\beta_{t}=\beta(t),\bar{\alpha}_{t}=\bar{\alpha}(t)\), parametrising process \(\mathcal{P}_{\text{data}}\rightarrow\mathcal{P}_{\text{sampling}}\)
0: Schedule \(\sigma_{t}=\sigma(t)\)
0: Observation \(\bm{y}\)
1:\(\triangleright\) Sample a starting point \(x_{T}\)\(\triangleleft\)
2:\(\bm{x}_{T}\sim\mathcal{P}_{T}=\mathcal{P}_{\text{sampling}}\)\(\triangleright\) Often \(\mathcal{P}_{T}=\mathcal{N}(0,\mathbf{I})\)
3:\(\triangleright\) Iteratively denoise for \(T\) steps\(\triangleleft\)
4:for\(t\) in \((T,T-1,\ldots,1)\)do
5:\(\triangleright\) Predict unconditional noise with learned network
6:\(\hat{\bm{\varepsilon}}_{\theta}\leftarrow\epsilon_{t}^{\theta}(\bm{x}_{t})\)
7:\(\hat{\bm{x}}_{0}\leftarrow\dfrac{\bm{x}_{t}-\sqrt{1-\bar{\alpha}_{t}}\bm{\hat{ \varepsilon}}_{\theta}}{\sqrt{\bar{\alpha}_{t}}}\)
8:\(\hat{\bm{\varepsilon}}_{\phi}\gets h_{t}^{\phi}(\bm{x}_{t},\hat{\bm{x}}_{0},\bm{y})\)
9:\(\triangleright\) Estimate posterior noise
10:\(\hat{\bm{\epsilon}}\leftarrow\hat{\bm{\varepsilon}}_{\theta}+\hat{\bm{ \epsilon}}_{\phi}\)
11:\(\bm{\varepsilon}_{t}\sim\mathcal{P}_{\text{noise}}\) if \(t>1\) else \(\bm{\varepsilon}_{t}\gets 0\)
12:\(\bm{x}_{t-1}\leftarrow\sqrt{\bar{\alpha}_{t-1}}\left(\dfrac{\bm{x}_{t}-\sqrt{1- \bar{\alpha}_{t}}\hat{\bm{\epsilon}}}{\sqrt{\bar{\alpha}_{t}}}\right)+\sqrt{1- \bar{\alpha}_{t-1}-\sigma_{t}^{2}}\hat{\bm{\epsilon}}+\sigma_{t}\bm{\varepsilon }_{t}\)
13:return\(\bm{x}_{0}\) ```

**Algorithm 7**\(|\)\(h\)-transform DDIM sampling (new)

[MISSING_PAGE_EMPTY:39]

**Algorithm 10**  Reconstruction Guidance (i.e. Moment Matching (MM) Approximation to \(h\)-transform, DPS [12]) for motif scaffolding

```
1:Unconditionally trained noise predictor \(\epsilon_{t}^{\theta}(\bm{x}_{t})\), target motif/context \(\bm{x}_{0}^{[M]}\).
2:Noise schedule \(\beta_{t}=\beta(t),\bar{\alpha}_{t}=\bar{\alpha}(t)\), parameterising process \(\mathcal{P}_{\text{data}}\rightarrow\mathcal{P}_{\text{sammine}}\)
3:Guidance scale (schedule) \(\gamma_{t}=\gamma(t)\)
4:Conditioning loss \(l(\bm{x}_{\text{true}},\bm{x}_{\text{pred}})\). e.g, Gaussian MM \(l(\bm{x}_{\text{true}},\bm{x}_{\text{pred}})=||\bm{x}_{\text{true}}-\bm{x}_{ \text{pred}}||^{2}\)
5:\(\triangleright\) Sample a starting point \(x_{T}\)\(\triangleright\)\(\operatorname{Often}\mathcal{P}_{T}=\mathcal{N}(0,\mathbf{I})\)
6:\(\triangleright\) Iteratively denoise and condition for \(T\) steps
7:for\(t\) in \((T,T-1,\dots,1)\)do
8:\(\bm{\hat{e}}_{\theta}\leftarrow\bm{\epsilon}_{t}^{\theta}(\bm{x}_{t})\)\(\triangleright\) Predict noise with learned network
9:\(\triangleright\) Predict noise with learned network
10:\(\triangleright\) Estimate current denoised estimate via Tweedie's formula
11:\(\widehat{\bm{x}}_{0}(\bm{x}_{t},\hat{\bm{\varepsilon}}_{\theta})\leftarrow \frac{1}{\sqrt{\alpha_{t}}}(\bm{x}_{t}-\sqrt{1-\bar{\alpha}_{t}}\bm{\hat{ \varepsilon}}_{\theta})\)\(\triangleright\) c.f. also eq. 15 in [28]
12:\(\triangleright\) Perform gradient descent step towards data consistency
13:\(\bm{x}_{t}\leftarrow\bm{x}_{t}-\gamma_{t}\nabla_{x}l(\mathcal{A}(\widehat{ \bm{x}}_{0}),\bm{y})\)\(\triangleright\) Requires backprop through \(\hat{e}_{t}^{\theta}\) via e.g. \(L_{2}\) loss
14:\(\triangleright\) Denoise sample with learned reverse process \(\bm{x}_{t-1}\sim\widetilde{p}_{t-1|t}(\bm{x}_{t})\)\(\triangleleft\)
15:\(\bm{x}_{t-1}\leftarrow(1-\beta_{t})^{-1/2}\left(\bm{x}_{t}-\beta_{t}(1-\bar{ \alpha}_{t})^{-1/2}\bm{\hat{\varepsilon}}_{\theta}\right)\)\(\triangleright\) Perform reverse drift
16:\(\triangleright\) Perform reverse diffusion, which is often Brownian motion in \(\mathbb{R}^{n}\), i.e. \(\mathcal{P}_{\text{noise}}=\mathcal{N}(0,\mathbf{I})\)\(\triangleleft\)
17:\(\bm{\varepsilon}_{t}\sim\mathcal{P}_{\text{noise}}\) if \(t>1\) else\(\bm{\varepsilon}_{t}\gets 0\)
18:\(\bm{x}_{t-1}\leftarrow\bm{x}_{t-1}+\sigma_{t}\bm{\varepsilon}_{t}\)\(\triangleright\) A common choice is \(\sigma_{t}=\beta(t)\)
19:return\(\bm{x}_{0}\) ```

**Algorithm 10**  Reconstruction Guidance (i.e. Moment Matching (MM) Approximation to \(h\)-transform, DPS [12]) for motif scaffolding

```
1:Unconditionally trained noise predictor \(\epsilon_{t}^{\theta}(\bm{x}_{t})\), target motif/context \(\bm{x}_{0}^{[M]}\).
2:Noise schedule \(\beta_{t}=\beta(t),\bar{\alpha}_{t}=\bar{\alpha}(t)\), parameterising process \(\mathcal{P}_{\text{data}}\rightarrow\mathcal{P}_{\text{sammine}}\)
3:Guidance scale (schedule) \(\gamma_{t}=\gamma(t)\)
4:Conditioning loss \(l(\bm{x}_{\text{true}},\bm{x}_{\text{pred}})\). e.g, Gaussian MM \(l(\bm{x}_{\text{true}},\bm{x}_{\text{pred}})=||\bm{x}_{\text{true}}-\bm{x}_{ \text{pred}}||^{2}\)
5:\(\triangleright\) Sample a starting point \(x_{T}\)\(\triangleright\)\(\operatorname{Often}\mathcal{P}_{T}=\mathcal{N}(0,\mathbf{I})\)
6:\(\triangleright\) Iteratively denoise and condition for \(T\) steps
7:for\(t\) in \((T,T-1,\dots,1)\)do
8:\(\hat{\bm{\varepsilon}}_{\theta}\leftarrow\epsilon_{t}^{\theta}(\bm{x}_{t})\)\(\triangleright\) Predict noise with learned network
9:\(\triangleright\) Estimate current denoised estimate via Tweedie's formula
10:\(\hat{\bm{x}}_{0}(\bm{x}_{t},\hat{\bm{\varepsilon}}_{\theta})\leftarrow\frac{1}{ \sqrt{\alpha_{t}}}(\bm{x}_{t}-\sqrt{1-\bar{\alpha}_{t}}\bm{\hat{\varepsilon}}_{ \theta})\)\(\triangleright\) c.f. also eq. 15 in [28]
11:\(\triangleright\) Perform gradient descent step towards condition on motif dimensions \(M\)\(\triangleleft\)
12:\(\bm{x}_{t}\leftarrow\bm{x}_{t}-\gamma_{t}\nabla_{x}l(\bm{x}_{0}^{[M]},\hat{\bm{ x}}_{0}^{[M]}(\bm{x}_{t},\hat{\bm{\varepsilon}}_{\theta}))\)\(\triangleright\) Requires backprop through \(\hat{e}_{t}^{\theta}\) via e.g., \(L_{2}\) loss
13:\(\triangleright\) Denoise sample with learned reverse process \(\bm{x}_{t-1}\sim\widetilde{p}_{t-1|t}(\bm{x}_{t})\)\(\triangleleft\)
14:\(\bm{x}_{t-1}\leftarrow(1-\beta_{t})^{-1/2}\left(\bm{x}_{t}-\beta_{t}(1-\bar{ \alpha}_{t})^{-1/2}\bm{\hat{\varepsilon}}_{\theta}\right)\)\(\triangleright\) Perform reverse drift
15:\(\triangleright\) Perform reverse diffusion, which is often Brownian motion in \(\mathbb{R}^{n}\), i.e. \(\mathcal{P}_{\text{noise}}=\mathcal{N}(0,\mathbf{I})\)\(\triangleleft\)
16:\(\bm{e}_{t}\sim\mathcal{P}_{\text{noise}}\) if \(t>1\) else\(\bm{\varepsilon}_{t}\gets 0\)
17:\(\bm{x}_{t-1}\leftarrow\bm{x}_{t-1}+\sigma_{t}\bm{\varepsilon}_{t}\)\(\triangleright\) A common choice is \(\sigma_{t}=\beta(t)\)
18:return\(\bm{x}_{0}\) ```

**Algorithm 11**  Reconstruction Guidance (i.e. Moment Matching (MM) Approximation to \(h\)-transform, DPS [12]) for motif scaffolding

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We propose a new framework for conditional sampling from diffusion models, referred to as _DEFT_ in the paper. This is stated both in the abstract and introduction, see 1. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Limitation of _DEFT_ are discussed in a special paragraph in Section 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: The assumptions for the proposition are always stated in the corresponding block. Proofs for statements are provided in Appendix D, with an exception for the optimal control loss. The derivation for this loss function is given in Appendix G. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We clearly explain the settings and choices of our experiments either directly in the main paper, see Section 4, or in Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide anonymised code for our experiments here, as well as details on how to use the datasets we refer to. All datasets we use are publicly available and open-source. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Full training details are provided in the released code base. Further, the setup is explained in both the experimental section, see Section 4, or in Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Due to the computational expense needed for sampling, providing error bars for conditional sampling results is not standard. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide training times and experimental setup in the relevant sections. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The authors have reviewed the NeurIPS ethics guidelines and conducted the research according to these guidelines. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: As DEFT is a generative model, it suffers from the sample issues as other generative approaches. The model can easily reproduce biases inherent in the training data. Further, due to the ability for conditional sampling, i.e., drawing samples according to specific constraints, the method can be used for "deep fakes" or misinformation. However, these impacts do not only apply to DEFT, but to other conditional sampling method as well. We added a sentence to the conclusion.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We only use pre-trained diffusion models, which are already publicly available in open-source code bases or publicly available datasets. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Where applicable, we provide references to the code bases, datasets and implementation used. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL.

* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide a framework for conditional sampling from diffusion models. The codebase is provided as an anonymized Github repository. We re-use datasets and models with open licenses. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines:* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.