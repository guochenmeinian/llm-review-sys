# SAMoSSA: Multivariate Singular Spectrum Analysis

with Stochastic Autoregressive Noise

 Abdullah Alomar

MIT

aalomar@mit.edu

&Munther Dahleh

MIT

dahlen@mit.edu

&Sean Mann

MIT

seanmann@mit.edu

&Devavrat Shah

MIT

devavrat@mit.edu

###### Abstract

The well-established practice of time series analysis involves estimating deterministic, non-stationary _trend_ and _seasonality_ components followed by learning the residual stochastic, stationary components. Recently, it has been shown that one can learn the deterministic non-stationary components accurately using multivariate Singular Spectrum Analysis (mSSA) in the absence of a correlated stationary component; meanwhile, in the absence of deterministic non-stationary components, the Autoregressive (AR) stationary component can also be learnt readily, e.g. via Ordinary Least Squares (OLS). However, a theoretical underpinning of multi-stage learning algorithms involving both deterministic and stationary components has been absent in the literature despite its pervasiveness. We resolve this open question by establishing desirable theoretical guarantees for a natural two-stage algorithm, where mSSA is first applied to estimate the non-stationary components despite the presence of a correlated stationary AR component, which is subsequently learned from the residual time series. We provide a finite-sample forecasting consistency bound for the proposed algorithm, SAMoSSA, which is data-driven and thus requires minimal parameter tuning. To establish theoretical guarantees, we overcome three hurdles: (i) we characterize the spectra of Page matrices of stable AR processes, thus extending the analysis of mSSA; (ii) we extend the analysis of AR process identification in the presence of arbitrary bounded perturbations; (iii) we characterize the out-of-sample or forecasting error, as opposed to solely considering model identification. Through representative empirical studies, we validate the superior performance of SAMoSSA compared to existing baselines. Notably, SAMoSSA's ability to account for AR noise structure yields improvements ranging from 5% to 37% across various benchmark datasets.

## 1 Introduction

**Background.** Multivariate time series have often been modeled as mixtures of stationary stochastic processes (e.g. AR process) and deterministic non-stationary components (e.g. polynomial and harmonics). To handle such mixtures, classical time series forecasting algorithms first attempt to estimate and then remove the non-stationary components. For example, before fitting an Autoregressive Moving-average (ARMA) model, polynomial trend and seasonal components must be estimated and then removed from the time series. Once the non-stationary components have been eliminated1, the ARMA model is learned. This estimation procedure often relies on domain knowledge and/or fine-tuning, and theoretical analysis for such multiple-stage algorithms is limited in the literature.

Footnote 1: Note that Autoregressive Integrated Moving-average (ARIMA) and Seasonal ARIMA models use differencing in conjunction with unit root tests to remove _stochastic_ non-stationary components, and are not suited for the model we consider.

Prior work presented a solution for estimating non-stationary deterministic components without domain knowledge or fine-tuning using mSSA [3]. This framework systematically models a wideclass of (deterministic) non-stationary multivariate time series as linear recurrent formulae (LRF), encompassing a wide class of spatio-temporal factor models that includes harmonics, exponentials, and polynomials. However, mSSA, both algorithmically and theoretically, does not handle additive correlated stationary noise, an important noise structure in time series analysis. Indeed, all theoretical results of mSSA are established in the noiseless setting [16] or under the assumption of independent and identically distributed (i.i.d.) noise [4, 3].

On the other hand, every stable stationary process can be approximated as a finite order AR process [8, 26]. The classical OLS procedure has been shown to accurately learn finite order AR processes [17]. However, the feasibility of identifying AR processes in the presence of a non-stationary deterministic component has not been addressed.

In summary, despite the pervasive practice of first estimating non-stationary deterministic components and then learning the stationary residual component, neither an elegant unified algorithm nor associated theoretical analyses have been put forward in the literature.

A step towards resolving these challenges is to answer the following questions: (i) Can mSSA consistently estimate non-stationary deterministic components in the presence of correlated stationary noise? (ii) Can the AR model be accurately identified using the residual time series, after removing the non-stationary deterministic components estimated by mSSA, potentially with errors? (iii) Can the out-of-sample or forecasting error of such a multi-stage algorithm be analyzed?

In this paper, we resolve all three questions in the affirmative: we present SAMoSSA, a two-stage procedure, where in the first stage, we apply mSSA on the observations to extract the non-stationary components; and in the second stage, the stationary AR process is learned using the residuals.

**Setup.** We consider the discrete time setting where we observe a multivariate time series \(Y(t)\coloneqq[y_{1}(t),\ldots,y_{N}(t)]\in\mathbb{R}^{N}\) at each time index \(t\in[T]\coloneqq\{1,\ldots,T\}\) where \(T\geq N\)2. For each \(n\in[N]\), and each timestep \(t\in[T]\), the observations take the form

Footnote 2: if \(T<N\), we divide the \(N\) time series into \(\lceil N/T\rceil\) sets of time series where this condition will hold.

\[y_{n}(t)=f_{n}(t)+x_{n}(t),\] (1)

where \(f_{n}:\mathbb{Z}^{+}\to\mathbb{R}\) denotes the non-stationary deterministic component, and \(x_{n}(t)\) is a stationary AR noise process of order \(p_{n}\) (\(\mathrm{AR}(p_{n})\)). Specifically, each \(x_{n}(t)\) is governed by

\[x_{n}(t)=\sum_{i=1}^{p_{n}}\alpha_{ni}x_{n}(t-i)+\eta_{n}(t),\] (2)

where \(\eta_{n}(t)\) refers to the per-step noise, modeled as mean-zero i.i.d. random variables, and \(\alpha_{ni}\in\mathbb{R}\ \forall\ i\in[p_{n}]\) are the parameters of the \(n\)-th \(\mathrm{AR}(p_{n})\) process.

**Goal.** For each \(n\in[N]\), our objective is threefold. The first is estimating \(f_{n}(t)\) from the noisy observations \(y_{n}(t)\) for all \(t\in[T]\). The second is identifying the AR process' parameters \(\alpha_{ni}\ \forall\ i\in[p_{n}]\). The third is out-of-sample forecasting of \(y_{n}(t)\) for \(t>T\).

**Contributions.** The main contributions of this work is SAMoSSA, an elegant two-stage algorithm, which manages to learn both non-stationary deterministic and stationary stochastic components of the underlying time series. A detailed summary of the contributions is as follows.

_(a) Estimating non-stationary component with mSSA under AR noise._ The prior theoretical results for mSSA are established in the deterministic setting [16] or under the assumption of i.i.d. noise [4, 3]. In Theorem 4.1, we establish that the mean squared estimation error scales as \(\sim 1/\sqrt{NT}\) under AR noise - the same rate was achieved by [3] under i.i.d. noise. Key to this result is establishing spectral properties of the "Page" matrix of AR processes, which may be of interest in its own right (see Lemma A.2).

_(b) Estimating AR model parameters under bounded perturbation._ We bound the estimation error for the OLS estimator of AR model parameters under _arbitrary and bounded_ observation noise, which could be of independent interest. Our results build upon the recent work of [17] which derives similar results but _without_ any observation noise. In that sense, ours can be viewed as a robust generalization of [17].

_(c) Out-of-sample finite-sample consistency of SAMoSSA._ We provide a finite-sample analysis for the forecasting error of SAMoSSA- such analysis of two-stage procedures in this setup is nascent despite its ubiquitous use in practice. Particularly, we establish in Theorem 4.4 that the out-of-sample forecasting error for SAMoSSA for the \(T\) time-steps ahead scales as \(\sim\frac{1}{T}+\frac{1}{\sqrt{NT}}\) with high probability.

_(d) Empirical results._ We demonstrate superior performance of SAMoSSA using both real-world and synthetic datasets. We show that accounting for the AR noise structure, as implemented in SAMoSSA, consistently enhances the forecasting performance compared to the baseline presented in [3]. Specifically, these enhancements range from 5% to 37% across benchmark datasets.

**Related Work.** Time series analysis is a well-developed field. We focus on two pertinent topics.

_mSSA._ Singular Spectrum Analysis (SSA), and its multivariate extension mSSA, are well-studied methods for time series analysis which have been used heavily in a wide array of problems including imputation [4; 3], forecasting [18; 2], and change point detection [22; 6]. Refer to [16; 15] for a good overview of the SSA literature. The classical SSA method consists of the following steps: (1) construct a Hankel matrix of the time series of interest; (2) apply Singular Value Decomposition (SVD) on the constructed Hankel matrix, (3) group the singular triplets to separate the different components of the time series; (4) learn a linear model for each component to forecast. mSSA is an extension of SSA which handles multiple time series simultaneously and attempts to exploit the shared structure between them [18]. The only difference between mSSA and SSA is in the first step, where Hankel matrices of individual series are "stacked" together to create a single stacked Hankel matrix. Despite its empirical success, mSSA's classical analysis has mostly focused on identifying which time series have a low-rank Hankel representation, and defining sufficient _asymptotic_ conditions for signal extraction, i.e., when the various time series components are separable. All of the classical analysis mostly focus on the deterministic case, where no observation noise is present.

Recently, a variant of mSSA was introduced for the tasks of forecasting and imputation [3]. This variant, which we extend in this paper, uses the Page matrix representation instead of the Hankel matrix, which enables the authors to establish finite-sample bounds on the imputation and forecasting error. However, their work assumes the observation noise to be i.i.d., and does not accommodate correlated noise structure, which is often assumed in the time series literature [26]. Our work extends the analysis in Agarwal et al. [3] to observations under AR noise structure. We also extend the analysis of the forecasting error by studying how well we can learn (and forecast) the AR noise process with perturbed observations.

_Estimating AR parameters._ AR processes are ubiquitous and of interest in many fields, including time series analysis, control theory, and machine learning. In these fields, it is often the goal to estimate the parameters of an AR process from a sample trajectory. Estimation is often carried out through OLS, which is asymptotically optimal [11]. The asymptotic analysis of the OLS estimator is well established, and recently, given the newly developed statistical tools of high dimensional probability, cf. [31; 29], various recent works have tackled its finite-time analysis as well. For example, several works established results for the finite-time identification for general first order vector AR systems [20; 19; 27; 24; 21]. Further, Gonzalez et al. [17] provides a finite-time bound on the deviation of the OLS estimate for general \(\mathrm{AR}(p)\) processes. To accommodate our setting, we extend the results of [17] in two ways. First, we extend the analysis to accommodate any sub-gaussian noise instead of assuming gaussianity; Second, and more importantly, we extend it to handle arbitrary bounded observation errors in the sampled trajectory, which can be of independent interest.

## 2 Model

**Deterministic Non-Stationary Component.** We adopt the spatio-temporal factor model of [3; 6] described next. The spatio-temporal factor model holds when two assumptions are satisfied: the first assumption concerns the spatial structure, i.e., the structure across the \(N\) time series \(f_{1},\ldots,f_{N}\); the second assumption pertains to the "temporal" structure. Before we describe the assumptions, we first define a key time series representation: the Page matrix.

**Definition 2.1** (Page Matrix).: _Given a time series \(f:\mathbb{Z}^{+}\to\mathbb{R}\), and an initial time index \(t_{0}>0\), the Page matrix representation over the \(T\) entries \(f(t_{0}),\ldots,f(t_{0}+T-1)\) with parameter \(1\leq L\leq T\) is given by the matrix \(\mathbf{Z}(f,L,T,t_{0})\in\mathbb{R}^{L\times\lfloor T/L\rfloor}\) with \(\mathbf{Z}(f,L,T,t_{0})_{ij}=f(t_{0}+i-1+(j-1)\times L)\) for \(i\in[L],\ j\in[\lfloor T/L\rfloor]\)._In words, to construct the \(L\times\lfloor T/L\rfloor\) Page matrix of the entries \(f(t_{0}),\ldots,f(t_{0}+T-1)\), partition them into \(\lfloor T/L\rfloor\) segments of \(L\) contiguous entries and concatenate these segments column-wise (see Figure 1). Now we introduce the main assumptions for \(f_{1},\ldots,f_{N}\).

**Assumption 2.1** (Spatial structure).: _For each \(n\in[N]\), \(f_{n}(t)=\sum_{r=1}^{R}u_{nr}w_{r}(t)\) for some \(u_{nr}\in\mathbb{R}\) and \(w_{r}:\mathbb{Z}^{+}\to\mathbb{R}\)._

Assumption 2.1 posits that each time series \(f_{n}(\cdot)\)\(\forall n\in[N]\) can be described as a linear combination of \(R\) "fundamental" time series. The second assumption relates to the temporal structure of the fundamental time series \(w_{r}(\cdot)\)\(\forall r\in[R]\), which we describe next.

**Assumption 2.2** (Temporal structure).: _For each \(r\in[R]\) and for any \(T>1,\ 1\leq L\leq T,t_{0}>0\), \(\text{rank}(\mathbf{Z}(w_{r},L,T,t_{0}))\leq G\)._

Assumption 2.2 posits that the Page matrix of each "fundamental" time series \(w_{r}\) has finite rank. While this imposed temporal structure may seem restrictive at first, it has been shown that many standard functions that model time series dynamics satisfy this property [4, 3]. These include any finite sum of products of harmonics, low-degree polynomials, and exponential functions (refer to Proposition 2.1 in [3]).

**Stochastic Stationary Component.** We adopt the following two assumptions about the AR processes \(x_{n}\)\(\forall n\in[N]\). We first assume that these AR processes are stationary (see Definition H.4 in Appendix H).

**Assumption 2.3** (Stationarity and distinct roots).: \(x_{n}(t)\) _is a stationary \(\mathrm{AR}(p_{n})\) process \(\forall n\in[N]\). That is, let \(\lambda_{ni}\in\mathbb{C}\), \(i\in[p_{n}]\) denote the roots of \(g_{n}(z)\coloneqq z^{p_{n}}-\sum_{i=1}^{p_{n}}\alpha_{ni}z^{p_{n}-i}\). Then, \(|\lambda_{ni}|<1\)\(\forall i\in[p_{n}]\). Further, \(\forall n\in[N]\), the roots of \(g_{n}(z)\) are distinct._

Further, we assume the per-step noise \(\eta_{n}(t)\) are i.i.d. sub-gaussian random variables (see Definition H.1 in Appendix H).

**Assumption 2.4** (Sub-gaussian noise).: _For \(n\in[N],t\in[T]\), \(\eta_{n}(t)\) are zero-mean i.i.d. sub-gaussian random variables with variance \(\sigma^{2}\)._

**Model Implications.** In this section, we state two important implications of the model stated above. The first is that the stacked Page matrix defined as

\[\mathbf{Z}_{f}(L,T,t_{0})=\left[\mathbf{Z}(f_{1},L,T,t_{0})\quad\mathbf{Z}(f_ {2},L,T,t_{0})\quad\ldots\quad\mathbf{Z}(f_{n},L,T,t_{0})\right],\]

is low-rank. Precisely, we recall the following Proposition stated in [3].

**Proposition 2.1** (Proposition 2 in [3] ).: _Let Assumptions 2.1 and 2.2 hold. Then for any \(L\leq\lfloor\sqrt{T}\rfloor\) with any \(T\geq 1\), \(t_{0}>0\), the rank of the Page matrix \(\mathbf{Z}(f_{n},L,T,t_{0})\) for \(n\in[N]\) is at most \(R\times G\). Further, the rank of the stacked Page matrix \(\mathbf{Z}_{f}(L,T,t_{0})\) is \(k\leq R\times G\)._

Throughout, we will use the shorthand \(\mathbf{Z}_{f}:=\mathbf{Z}_{f}(L,T,1)\) and \(\mathbf{Z}_{f}(t_{0})\coloneqq\mathbf{Z}_{f}(L,T,t_{0})\)3. The second implication is that there exists a linear relationship between the last row of \(\mathbf{Z}_{f}\) and its top \(L-1\) rows. The following proposition establishes this relationship. First, Let \([\mathbf{Z}_{f}]_{L}\) denote the \(L\)-th row of \(\mathbf{Z}_{f}\) and let \(\mathbf{Z}^{\prime}_{f}\in\mathbb{R}^{(L-1)\times(N\lfloor T/L\rfloor)}\) denote the sub-matrix that consist of the top \(L-1\) rows of \(\mathbf{Z}_{f}\).

Footnote 3: We will use the same shorthand for the stacked page matrices of \(y\) and \(x\): namely, \(\mathbf{Z}_{y}\) and \(\mathbf{Z}_{x}\).

**Proposition 2.2** (Proposition 3 in [3] ).: _Let Assumptions 2.1 and 2.2 hold. Then, for \(L>RG\), there exists \(\beta^{*}\in\mathbb{R}^{L-1}\) such that \([\mathbf{Z}_{f}]_{L}^{\top}.=\mathbf{Z^{\prime}_{f}}^{\top}\beta^{*}\). Further, \(\|\beta^{*}\|_{0}\leq RG\)._

## 3 Algorithm

The proposed algorithm provides two main functionalities. The first one is _decomposing_ the observations \(y_{n}(t)\) into an estimate of the non-stationary and stationary components for \(t\leq T\). The second is forecasting \(y_{n}(t)\) for \(t>T\), which involves learning a forecasting model for both \(f_{n}(t)\) and \(x_{n}(t)\).

**Univariate Case**. For ease of exposition, we will first describe the algorithm for the univariate case (\(N=1\)). The algorithm has the following parameters: \(1<L\leq\sqrt{T}\), and \(1\leq\hat{k}\leq L\) (referto Appendix B.2 for how to choose these parameters). For clarity, we will assume, without loss of generality, that \(L\) is chosen such that \(T/L\) is an integer4. In the first step of the algorithm, we transform the observations \(y_{1}(t),t\in[T]\) into the \(L\times T/L\) Page matrix \(\mathbf{Z}(y_{1},L,T,1)\). We will use the shorthand \(\mathbf{Z}_{y_{1}}:=\mathbf{Z}(y_{1},L,T,1)\) henceforth.

Footnote 4: Otherwise, one can apply this algorithm to the two ranges \(\{1,\ldots,L\times\lfloor T/L\rfloor\}\) and \(\{(T\mod L)+1,\ldots,T\}\).

_Decomposition_. We compute the SVD \(\mathbf{Z}_{y_{1}}=\sum_{\ell=1}^{L}s_{\ell}u_{\ell}v_{\ell}^{\top}\), where \(s_{1}\geq s_{2}\cdots\geq s_{L}\geq 0\) denote its ordered singular values, and \(u_{\ell}\in\mathbb{R}^{L},v_{\ell}\in\mathbb{R}^{T/L}\) denote its left and right singular vectors, respectively, for \(\ell\in[L]\). Then, we obtain \(\widehat{\mathbf{Z}}_{f_{1}}\) by retaining the top \(\hat{k}\) singular components of \(\mathbf{Z}_{y_{1}}\) (i.e., by applying Hard Singular Value Thresholding (HSVT) with threshold \(\hat{k}\)). That is, \(\widehat{\mathbf{Z}}_{f_{1}}=\sum_{\ell=1}^{\hat{k}}s_{\ell}u_{\ell}v_{\ell}^ {\top}\).

We denote by \(\widehat{f}_{1}(t)\) and \(\widehat{x}_{1}(t)\) the _estimates_ of \(f_{1}(t)\) and \(x_{1}(t)\) respectively. We read off the estimates \(\widehat{f}_{1}(t)\) directly from the matrix \(\widehat{\mathbf{Z}}_{f_{1}}\) using the entry that corresponds to \(t\in[T]\). More precisely, for \(t\in[T]\), \(\widehat{f}_{1}(t)\) equals the entry of \(\widehat{\mathbf{Z}}_{f_{1}}\) in row \((t-1\mod L)+1\) and column \(\lceil t/L\rceil\), while \(\widehat{x}_{1}(t)=y_{1}(t)-\widehat{f}_{1}(t)\).

_Forecasting_. To forecast \(y_{1}(t)\) for \(t>T\), we produce a forecast for both \(\widehat{f}_{1}(t)\) and \(\widehat{x}_{1}(t)\). Both forecasts are performed through linear models (\(\widehat{\beta}\) for \(\widehat{f}_{1}(t)\) and \(\widehat{\alpha}_{1}\) for \(\widehat{x}_{1}(t)\).) For \(\widehat{f}_{1}(t)\), we first learn a linear model \(\widehat{\beta}\) defined as

\[\widehat{\beta}=\operatorname*{argmin}_{\beta\in\mathbb{R}^{L-1}}\quad\sum_{ m=1}^{T/L}(y_{1}(Lm)-\beta^{\top}\widehat{F}_{m})^{2},\] (3)

where \(\widehat{F}_{m}=[\widehat{f}_{1}(L(m-1)+1),\ldots,\widehat{f}_{1}(L\times m-1)]\) for \(m\in[T/L]\)5 We then use \(\widehat{\beta}\) and the \(L-1\) lagged observations to produce \(\widehat{f}_{1}(t)\). That is \(\widehat{f}_{1}(t)=\widehat{\beta}^{\top}Y_{1}(t-1),\) where \(Y_{1}(t-1)=[y_{1}(t-1),\ldots,y_{1}(t-L)]\).

Footnote 5: In the forecasting algorithm, the estimates \([\widehat{f}_{1}(L(m-1)+1),\ldots,\widehat{f}_{1}(L\times m-1)]\) are obtained by applying HSVT on a sub-matrix of \(\mathbf{Z}_{y_{1}}\) which consists of its first \(L-1\) rows. This is done to establish the theoretical results as it helps us avoid dependencies in the noise between \(y_{1}(Lm)\) and \(\widehat{F}_{m}\) for \(m\in[T/L]\).

For \(\widehat{x}_{1}(t)\), we first estimate the parameters for the \(\operatorname{AR}(p_{1})\) process. Specifically, define the \(p_{1}\)-dimensional vectors \(\widetilde{X}_{1}(t)\coloneqq[\widehat{x}_{1}(t),\ldots,\widehat{x}_{1}(t-p_ {1}+1)]^{6}\). Then, define the OLS estimate as,

\[\widehat{\alpha}_{1}=\operatorname*{argmin}_{\alpha\in\mathbb{R}^{p_{1}}} \quad\sum_{t=p_{1}}^{T-1}(\widehat{x}_{1}(t+1)-\alpha^{\top}\widehat{X}_{1}(t ))\cdot\] (4)

Then, let \(\widetilde{x}_{1}(t^{\prime})=y_{1}(t^{\prime})-\widehat{f}_{1}(t^{\prime})\) for any \(t^{\prime}<t\), 7. We then use \(\widehat{\alpha}_{1}\) and the \(p_{1}\) lagged entries of \(\widetilde{x}_{1}(\cdot)\) to produce \(\widehat{x}_{1}(t)\). That is \(\widehat{x}_{1}(t)=\widehat{\alpha}_{1}^{\top}\widetilde{X}_{1}(t-1)\), where \(\widetilde{X}_{1}(t-1)=[\widetilde{x}_{1}(t-1),\ldots,\widetilde{x}_{1}(t-p_ {1})]\). Finally, produce the forecast \(\widehat{y}_{1}(t)=\widehat{f}_{1}(t)+\widehat{x}_{1}(t)\).

Footnote 7: Note the subtle difference between \(\widehat{x}_{1}(t)\) and \(\widetilde{x}_{1}(t)\) – precisely, for \(t>T\), \(\widehat{x}_{1}(t)\) is an estimate of \(x_{1}(t)\) before observing \(y_{1}(t)\) (i.e., a forecast), whereas \(\widetilde{x}_{1}(t)\) is an estimate of \(x_{1}(t)\) after observing \(y_{1}(t)\).

**Multivariate Case.** The key change for the case \(N>1\) is the use of the stacked Page matrix \(\mathbf{Z}_{y}\), which is the column-wise concatenation of the Page matrices induced by individual time series. Specifically, consider the stacked Page matrix \(\mathbf{Z}_{y}\in\mathbb{R}^{L\times NT/L}\) defined as

\[\mathbf{Z}_{y}=[\mathbf{Z}(y_{1},L,T,1)\quad\mathbf{Z}(y_{2},L,T,1)\quad \ldots\quad\mathbf{Z}(y_{n},L,T,1)]\,.\] (5)

_Decomposition_. The procedure for learning the non-stationary component \(f_{1},\ldots,f_{N}\) for \(t\leq T\) is similar to that of the univariate case. Specifically, we perform HSVT with threshold \(\hat{k}\) on \(\mathbf{Z}_{y}\) to produce \(\widehat{\mathbf{Z}}_{y}\). Then, we read off the _estimate_ of \(f_{n}(t)\) from \(\widehat{\mathbf{Z}}_{y}\). Specifically, for \(t\in[T]\), and \(n\in[N]\), let \(\widehat{\mathbf{Z}}_{f_{n}}\) refer to sub-matrix of \(\widehat{\mathbf{Z}}_{f}\) induced by selecting only its \([(n-1)\times(T/L)+1,\ldots,n\times T/L]\)columns. Then for \(t\in[T]\), \(\widehat{f}_{n}(t)\) equals the entry of \(\widehat{\mathbf{Z}}_{f_{n}}\) in row \((t-1\mod L)+1\) and column \(\lceil t/L\rceil\). We then produce an _estimate_ for \(x_{n}(t)\) as \(\widehat{x}_{n}(t)=y_{n}(t)-\widehat{f}_{n}(t)\).

_Forecasting._ To forecast \(y_{n}(t)\) for \(t>T\), as in the univariate case, we learn a linear model \(\widehat{\beta}\) defined as

\[\widehat{\beta}=\operatorname*{argmin}_{\beta\in\mathbb{R}^{L-1}}\sum_{m=1}^ {N\times T/L}(y_{m}-\beta^{\top}\widehat{F}_{m})^{2},\] (6)

where \(y_{m}\) is the \(m\)-th component of \([y_{1}(L),\;y_{1}(2\times L),\ldots,y_{1}(T),\;y_{2}(L),\ldots,y_{2}(T),\ldots, y_{N}(T)]\in\mathbb{R}^{N\times T/L}\), and \(\widehat{F}_{m}\in\mathbb{R}^{L-1}\) corresponds to the vector formed by the entries of the first \(L-1\) rows in the \(m\)th column of \(\widehat{\mathbf{Z}}_{f}\)8 for \(m\in[N\times T/L]\). We then use \(\widehat{\beta}\) to produce \(\widehat{f}_{n}(t)=\widehat{\beta}^{\top}Y_{n}(t-1)\), where again \(Y_{n}(t-1)\) is the vector of the \(L-1\) lags of \(y_{n}(t-1)\). That is \(Y_{n}(t-1)=[y_{n}(t-1))\ldots y_{n}(t-L)]\). Then, for each \(n\in[N]\), we estimate \(\alpha_{ni}\;\forall i\in[p_{n}]\), the parameters for the \(n\)-th AR\((p_{n})\) process. Let \(\widehat{\alpha}_{n}\) denote the OLS estimate defined as

Footnote 8: To be precise, \(\widehat{\mathbf{Z}}_{f}\) here is the truncated SVD of a sub-matrix of \(\mathbf{Z}_{y}\) which consist of its first \(L-1\) rows.

\[\widehat{\alpha}_{n}=\operatorname*{argmin}_{\alpha\in\mathbb{R}^{p_{n}}}\quad \sum_{t=p_{n}}^{T-1}(\widehat{x}_{n}(t+1)-\alpha^{\top}\widehat{X}_{n}(t))^{2}.\] (7)

Then, produce a forecast for \(x_{n}\) as \(\widehat{x}_{n}(t)=\widehat{\alpha}_{n}^{\top}\widehat{X}_{n}(t-1),\) where again \(\widetilde{X}_{n}(t-1)=[\widetilde{x}_{n}(t-1),\ldots,\widetilde{x}_{n}(t-p_ {n})]\). Finally, produce the forecast for \(y_{n}\) as \(\widehat{y}_{n}(t)=\widehat{f}_{n}(t)+\widehat{x}_{n}(t)\). For a visual depiction of the algorithm, refer to Figure 1.

## 4 Results

In this section, we provide finite-sample high probability bounds on the following quantities:

**1. Estimation error of non-stationary component.** First, we give an upper bound for the estimation error of each one of \(f_{1}(t),\ldots,f_{N}(t)\) for \(t\in[T]\). Specifically, we upper bound the following metric

\[\mathsf{EstErr}(N,T,n)=\frac{1}{T}\sum_{t=1}^{T}(\widehat{f}_{n}(t)-f_{n}(t) )^{2},\] (8)

Figure 1: A visual depiction of SAMoSSA. The algorithm five steps are (i) transform the time series into its stacked Page matrix representation; (ii) decompose time series into non-stationary and stationary components (iii) estimate \(\widehat{\beta}\); (iv) estimates \(\hat{\alpha}_{n}\;\forall n\in[N]\); (v) produce the forecast \(\widehat{y}_{n}(t)\) for \(t>T\).

where \(\widehat{f}_{n}(t)\)\(n\in[N],t\in[T]\) are the estimates produced by the algorithm we proposed in Section 3.

**2. Identification of AR parameters.** Second, we analyze the accuracy of the estimates \(\widehat{\alpha}_{n}\)\(\forall n\in[N]\) produced by the proposed algorithm. Specifically, we upper bound the following metrics

\[\left\|\widehat{\alpha}_{n}-\alpha_{n}\right\|_{2}\qquad\forall n\in[N],\] (9)

where \(\alpha_{n}=[\alpha_{n1},\ldots,\alpha_{np_{n}}]\).

**3. Out-of-sample forecasting error.** Finally, we provide an upper bound for the forecasting error of \(y_{1}(t),\ldots,y_{N}(t)\) for \(t\in\{T+1,\ldots,2T\}\). Specifically, we upper bound the following metric

\[\mathsf{ForErr}(N,T)=\frac{1}{NT}\sum_{n=1}^{N}\sum_{t=T+1}^{2T}\left( \widehat{y}_{n}(t)-\mathbb{E}\left[y_{n}(t)\ |\ y_{n}(t-1),\ldots,y_{n}(1)\right]\right)^{2},\] (10)

where \(\widehat{y}_{n}(\cdot)\)\(\forall\)\(n\in[N],t\in\{T+1,\ldots,2T\}\) are the forecasts produced by the algorithm we propose in Section 3. Before stating the main results, we state key additional assumptions.

**Assumption 4.1** (Balanced spectra).: _Let \(\mathbf{Z}_{f}(t_{0})\) denote the \(L\times NT/L\) stacked Page matrix associated with all \(N\) time series \(f_{1}(\cdot),\ldots,f_{N}(\cdot)\) for their \(T\) consecutive entries starting from \(t_{0}\). Let \(k=\text{rank}(\mathbf{Z}_{f}(t_{0}))\), and let \(\sigma_{k}(\mathbf{Z}_{f}(t_{0}))\) denote the \(k\)-th singular value for \(\mathbf{Z}_{f}(t_{0})\). Then, for any \(t_{0}>0\), \(\mathbf{Z}_{f}(t_{0})\) is such that \(\sigma_{k}(\mathbf{Z}_{f}(t_{0}))\geq\gamma\sqrt{NT}/\sqrt{k}\) for some absolute constant \(\gamma>0\)._

This assumption holds whenever the the non-zero singular values are "well-balanced", a standard assumption in the matrix/tensor estimation literature [3, 6]. Note that this assumption, as stated, ensures balanced spectra for the stacked Page matrix of _any_ set of \(T\) consecutive entries of \(f_{1}(\cdot),\ldots,f_{N}(\cdot)\).

Finally, we will impose an additional necessary restriction on the complexity of the \(N\) time series \(f_{1}(t),\ldots,f_{N}(t)\) for \(t>T\) (as is done in [3, 5]). Let \(\mathbf{Z}^{\prime}_{f}\) denote the \((L-1)\times(NT/L)\) matrix formed using the top \(L-1\) rows of \(\mathbf{Z}_{f}\). Further, for any \(t_{0}\in[T+1]\), let \(\mathbf{Z}^{\prime}_{f}(t_{0})\) denote the \((L-1)\times(NT/L)\) matrix formed using the top \(L-1\) rows of \(\mathbf{Z}_{f}(t_{0})\). For any matrix \(\bm{M}\), let \(\text{colspan}(\bm{M})\) denote the subspace spanned by the its columns. We assume the following property.

**Assumption 4.2** (Subspace inclusion).: _For any \(t_{0}\in[T+1]\), \(\text{colspan}(\mathbf{Z}^{\prime}_{f}(t_{0}))\subseteq\text{colspan}(\mathbf{ Z}^{\prime}_{f})\)._

This assumption is necessary as it requires the stacked Page matrix of the out-of-sample time series \(\mathbf{Z}^{\prime}_{f}(t_{0})\) to be only as "rich" as that of the stacked Page matrix of the "in-sample" time series \(\mathbf{Z}^{\prime}_{f}\).

### Main Results

First, recall that \(R\) is defined in Assumption 2.1, \(G\) in Assumption 2.2, \(k\) is in Proposition 2.1, while \(\gamma\) is defined in Assumption 4.1. Further, recall that \(\lambda_{ni}\) for \(i\in[p_{n}]\) are the roots of the characteristic polynomial of the \(n\)-th AR process, as defined in Assumption 2.3. Throughout, let \(c\) and \(C\) be absolute constants, \(f_{\max}\coloneqq\max_{n,t\leq 2T}\left|f_{n}(t)\right|\), \(p\coloneqq\max_{n}p_{n}\), \(\alpha_{\max}=\max_{n}\left\|\alpha_{n}\right\|_{2}\), and \(C(f_{\max},\gamma)\) denote a constant that depends only (polynomially) on model parameters \(f_{\max}\) and \(\gamma\). Last but not least, define the key quantity

\[\sigma_{x}\coloneqq\frac{c_{\lambda}\sigma}{(1-\lambda_{\star})}\]

where \(\lambda_{\star}=\max_{i,n}\left|\lambda_{ni}\right|\) and \(c_{\lambda}\) is a constant that depends only on \(\lambda_{ni}\)9. Note that \(\sigma_{x}^{2}\) is a key quantity that we use to bound important spectral properties of AR processes, as Lemma A.2 establishes.

Footnote 9: See Appendix A for an explicit expression of \(c_{\lambda}\).

#### 4.1.1 Estimation Error of Non-Stationary Component

**Theorem 4.1** (Estimation error of \(f\)).: _Assume access to the observations \(y_{1}(t),\cdots,y_{N}(t)\) for \(t\in[T]\) as defined in (1). Let assumptions 2.1, 2.2, 2.3, 2.4 and 4.1 hold. Let \(L=\sqrt{NT}\) and \(\hat{k}=k\), then, for any \(n\in[N]\), with probability of at least \(1-\frac{c}{(NT)^{1/3}}\)_

\[\mathsf{EstErr}(N,T,n)\leq C(f_{\max},\gamma)\bigg{(}\frac{\sigma_{x}^{4}GR \log(NT)}{\sqrt{NT}}\bigg{)}.\] (11)This theorem implies that the mean squared error of estimating \(f\) scales as \(\tilde{O}\left(\frac{1}{\sqrt{NT}}\right)\)10 with high probability. The proof of Theorem 4.1 is in Appendix C.

Footnote 10: The \(\tilde{O}(\cdot)\) notation is analogous to the standard \(O(\cdot)\) while ignoring log dependencies.

#### 4.1.2 Identification of AR Processes

**Theorem 4.2** (AR Identification).: _Let the conditions of Theorem 4.1 hold. Let \(\widehat{\alpha}_{n}\) be as defined in (7), and \(\alpha_{n}=\left[\alpha_{n1},\ldots,\alpha_{np_{n}}\right]\) as defined in (2). Then, for a sufficiently large T such that \(\frac{\log(T)}{\sqrt{T}}<\frac{C\sigma^{2}}{p\sigma_{x}^{2}\alpha_{\max}^{2} \left(\frac{\lambda_{\min}(\Psi)}{\lambda_{\max}(\Gamma)}\right)}\), and for any \(n\in[N]\) where \(\mathsf{EstErr}(N,T,n)\leq\frac{\sigma^{2}\lambda_{\min}(\Psi)}{6p}\), we have with probability of at least \(1-\frac{c}{T^{10}}\),_

\[\|\widehat{\alpha}_{n}-\alpha_{n}\|_{2}^{2}\leq\frac{Cp}{\lambda_{\min}(\Psi) }\left(\frac{p}{T}\log\left(\frac{T\lambda_{\max}(\Psi)}{\lambda_{\min}(\Psi) }\right)+\frac{\sigma_{x}^{2}\mathsf{EstErr}(N,T,n)\log(T)}{\sigma^{2}\min \{1,\sigma^{2}\lambda_{\min}(\Psi)\}}\right).\] (12)

Recall that \(\widehat{\alpha}_{n}\) is estimated using \(\widehat{x}_{n}(\cdot)\), a perturbed version of the true AR process \(x_{n}(\cdot)\). This perturbation comes from the estimation error of \(x_{n}(\cdot)\), which is a consequence of \(\mathsf{EstErr}(N,T,n)\). Hence, it is not surprising that \(\mathsf{EstErr}(N,T,n)\) shows up in the upper bound. Indeed, the upper bound we provide here consists of two terms: the first, which scales as \(\tilde{O}\left(\frac{1}{T}\right)\) is the bound one would get with access to the true AR process \(x_{n}(\cdot)\), as shown in [17]. The second term characterizes the contribution of the perturbation caused by the estimation error, and it scales as \(O\left(\mathsf{EstErr}(N,T,n)\right)\), which we show is \(\tilde{O}\left(\frac{1}{\sqrt{NT}}\right)\) with high probability in Theorem 4.1. Also, note that the theorem holds when the estimation error is sufficiently small (\(\mathsf{EstErr}(N,T,n)\leq\frac{\sigma^{2}\lambda_{\min}(\Psi)}{6p}\)). This condition ensures that the perturbation arising from the estimation error remains below the lower bound for the minimum eigenvalue of the sample covariance matrix associated with the autoregressive processes. Note that \(\lambda_{\max}(\Psi),\lambda_{\min}(\Psi)\) and \(\lambda_{\max}(\Gamma)\) are quantities that relate to the parameters of the AR(\(p\)) processes and their "controllability Gramian" as we detail in Appendix A.1. The proof of Theorem 4.2 is in Appendix D.

#### 4.1.3 Out-of-sample Forecasting Error

We first establish an upper bound on the error of our estimate \(\widehat{\beta}\).

**Theorem 4.3** (Model Identification (\(\beta^{*}\))).: _Let the conditions of Theorem 4.1 hold. Let \(\widehat{\beta}\) be defined as in (6), and \(\beta^{*}\) as defined in Proposition 2.2. Then, with probability of at least \(1-\frac{c}{(NT)^{10}}\)_

\[\|\widehat{\beta}-\beta^{*}\|_{2}^{2}\leq C(f_{\max},\gamma)\bigg{(}\frac{ \sigma_{x}^{4}G^{2}R^{2}\log(NT)}{\sqrt{NT}}\bigg{)}\max\{\|\beta^{*}\|_{1}^{2 },1\}.\] (13)

This shows that the error of estimating \(\beta^{*}\) (in squared euclidean norm) scales as \(\tilde{O}\left(\frac{1}{\sqrt{NT}}\right)\).

**Theorem 4.4**.: _Let the conditions of Theorem 4.2 and Assumption 4.2 hold. Then, with probability of at least \(1-\frac{c}{T^{10}}\)_

\[\mathsf{ForErr}(N,T)\leq\tilde{C}G^{3}R^{3}p^{2}\sigma_{x}^{6}\Bigg{(}\frac{ p\sigma^{2}\log\left(T\right)}{T}+\frac{GR\sigma_{x}^{6}}{\min\left\{ \sigma^{2},\sigma^{4}\right\}}\frac{\log(NT)^{2}}{\sqrt{NT}}\Bigg{)},\]

_where \(c\) is an absolute constant, and \(\tilde{C}\) denotes a constant that depends only (polynomially) on \(f_{\max},\gamma,\lambda_{\min}(\Psi),\lambda_{\max}(\Psi),\beta^{*}\) and \(\alpha_{\max}\)._

This theorem establishes that the forecasting error for the next \(T\) time steps scales as \(\tilde{O}\left(\frac{1}{T}+\frac{1}{\sqrt{NT}}\right)\) with high probability. Note that the \(\tilde{O}\left(\frac{1}{T}\right)\) term is a function of \(T\) only, as it is a consequence of the error incurred when we estimate each \(\alpha_{n}\). Recall that we estimate \(\alpha_{n}\) separately for \(n\in[N]\) using the available (perturbed) \(T\) observation of each process. The \(\tilde{O}\left(\frac{1}{\sqrt{NT}}\right)\) term on the other hand is a consequence of the error incurred when learning and forecasting \(f_{1},\ldots,f_{n}\), which is done collectively across the \(N\) time series. Finally, Theorem 4.4 implies that when \(N=\Theta(T)\), the forecasting error scales as \(\tilde{O}\left(\frac{1}{T}\right)\). The proof of Theorems 4.3 and 4.4 are in Appendix F and G, respectively.

## 5 Experiments

In this section, we support our theoretical results through several experiments using synthetic and real-world data. In particular, we draw the following conclusions:

1. Our results align with numerical simulations concerning the estimation of non-stationary components under AR stationary noise and the accuracy of AR parameter estimation (Section 5.1).
2. Modeling and learning the autoregressive process in SAMoSSA led to a consistent improvement over mSSA. The improvements range from 5% to 37% across standard datasets (Section 5.2).

### Model Estimation

**Setup.** We generate a synthetic multivariate time series (\(N=10\)) that is a mixture of both harmonics and an AR noise process. The AR process is stationary, and its parameter is chosen such that \(\lambda^{*}=\max_{n,i}\left|\lambda_{ni}\right|\) is one of three values \(\{0.3,0.6,0.95\}\). Refer to Appendix B.1 for more details about the generating process. We then evaluate the estimation error \(\mathsf{EstErr}(N,T,1)\) and the AR parameter estimation error \(\left\|\alpha_{1}-\widehat{\alpha}_{1}\right\|_{2}\) as we increase \(T\) from \(200\) to \(500000\).

**Results.** Figure 1(a) visualizes the mean squared estimation error of \(f_{1}\), while Figure 1(b) shows the AR parameter estimation error. The solid lines in both figures indicate the mean across ten trials, whereas the shaded areas cover the minimum and maximum error across the ten trials. We find that, as the theory suggests, the estimation error for both the non-stationary component and the AR parameter decay to zero as \(NT\) increases. We also see that the estimation error is inversely proportional to \((1-\lambda^{*})\), which is being reflected in the AR parameter estimation error as well.

### Forecasting

We showcase SAMoSSA's forecasting performance relative to standard algorithms. Notably, we compare SAMoSSA to the mSSA variant in [3] to highlight the value of learning the autoregressive process.

**Setup.** The forecasting ability of SAMoSSA was compared against (i) mSSA [3], (ii) ARIMA, a very popular classical time series prediction algorithm, (iii) Prophet [28], (iv) DeepAR [23] and (v) LSTM [14]on three real-life datasets and one synthetic dataset containing both deterministic trend/seasonality and a stationary noise process (see details in Appendix B). Each dataset was split into train, validation, and test sets (see Appendix B.1). Each dataset has multiple time series, so we

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & Traffic & Electricity & Exchange & Synthetic \\ \hline SAMoSSA & 0.776 & **0.829** & 0.731 & **0.476** \\ mSSA & 0.747 & 0.605 & 0.674 & 0.366 \\ ARIMA & 0.723 & \textless{}-10 & **0.756** & 0.305 \\ Prophet & 0.462 & 0.197 & \textless{}-10 & -0.445 \\ DeepAR & **0.824** & 0.764 & 0.579 & 0.323 \\ LSTM & 0.821 & -1.261 & -1.825 & 0.381 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Performance of algorithms on various datasets, measured by mean \(R^{2}\).

Figure 2: The error in SAMoSSA’s estimation of the non-stationary components and the AR parameters decays to zero as \(NT\) increases as the theorem suggests.

aggregate the performance of each algorithm using the mean \(R^{2}\) score. We use the \(R^{2}\) score since it is invariant to scaling and gives a reasonable baseline: a negative value indicates performance inferior to simply predicting the mean.

**Results.** In Table 1 we report the mean \(R^{2}\) for each method on each dataset. We highlight that, across all datasets, SAMoSSA consistently performs the best or is otherwise very competitive with the best. The results also underscore the significance of modeling and learning the autoregressive process in real-world datasets. Notably, learning the autoregressive model in SAMoSSA consistently led to an improvement over mSSA, with the increase in \(R^{2}\) values ranging from 5% to 37%. We note that this improvement is due to the fact that mSSA, as described in [3], overlooks any potential structure in the stochastic processes \(x_{1}(\cdot),\dots,x_{N}(\cdot)\) and assumes i.i.d.mean-zero noise process. While in SAMoSSA, we attempt to capture the structure of \(x_{1}(\cdot),\dots,x_{N}(\cdot)\) through the learned AR process.

## 6 Discussion and Limitations

We presented SAMoSSA, a two-stage procedure that effectively handles mixtures of deterministic non-stationary and stationary AR processes with minimal model assumptions. We analyze SAMoSSA's ability to estimate non-stationary components under stationary AR noise, the error rate of AR system identification via OLS under observation errors, and a finite-sample forecast error analysis.

We note that our results can be readily adapted to accommodate (i) _approximate low-rank_ settings (as in the model by Agarwal et al [3]); and (ii) scenarios with incomplete data. We do not discuss these settings to focus on our core contributions, but they represent valuable directions for future studies.

Our analysis reveals some limitations, providing avenues for future research. One limitation of our model is that it only considers stationary stochastic processes. Consequently, processes with non-stationary stochastic trends, such as a random walk, are not incorporated. Investigating the inclusion of such models and their interplay with the SSA literature is a worthy direction for future work. Second, our model assume non-interaction between the \(N\) stationary processes \(x_{1},\dots,x_{N}\). Yet, it might be plausible to posit the existence of interactions among them, possibly through a vector AR model (VAR). Examining this setting represents another compelling direction for future work.

## References

* [1] Y. Abbasi-yadkori, D. Pal, and C. Szepesvari. Improved algorithms for linear stochastic bandits. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 24. Curran Associates, Inc., 2011.
* [2] A. Agarwal, A. Alomar, and D. Shah. tspdb: Time series predict db. In _NeurIPS 2020 Competition and Demonstration Track_, pages 27-56. PMLR, 2021.
* [3] A. Agarwal, A. Alomar, and D. Shah. On multivariate singular spectrum analysis and its variants. _ACM SIGMETRICS Performance Evaluation Review_, 50(1):79-80, 2022.
* [4] A. Agarwal, M. J. Amjad, D. Shah, and D. Shen. Model agnostic time series analysis via matrix estimation. _Proceedings of the ACM on Measurement and Analysis of Computing Systems_, 2(3):1-39, 2018.
* [5] A. Agarwal, D. Shah, and D. Shen. On principal component regression in a high-dimensional error-in-variables setting. _arXiv preprint arXiv:2010.14449_, 2020.
* [6] A. Alanqary, A. Alomar, and D. Shah. Change point detection via multivariate singular spectrum analysis. _Advances in Neural Information Processing Systems_, 34:23218-23230, 2021.
* [7] A. Alexandrov, K. Benidis, M. Bohlke-Schneider, V. Flunkert, J. Gasthaus, T. Januschowski, D. C. Maddix, S. Rangapuram, D. Salinas, J. Schulz, et al. Gluonts: Probabilistic time series models in python. _arXiv preprint arXiv:1906.05264_, 2019.
* [8] T. W. Anderson. _The statistical analysis of time series_. John Wiley & Sons, 2011.
* [9] F. Chollet. keras. https://github.com/fchollet/keras, 2015. Online; accessed 25 February 2020.

* [10] C. Davis and W. M. Kahan. The rotation of eigenvectors by a perturbation. iii. _SIAM Journal on Numerical Analysis_, 7(1):1-46, 1970.
* [11] J. Durbin. Estimation of parameters in time-series regression models. _Journal of the royal statistical society: Series B (Methodological)_, 22(1):139-153, 1960.
* [12] Facebook. Prophet. https://github.com/facebook/prophet, 2020.
* [13] M. Gavish and D. L. Donoho. The optimal hard threshold for singular values is \(4/\sqrt{3}\). _IEEE Transactions on Information Theory_, 60(8):5040-5053, 2014.
* [14] F. A. Gers, J. Schmidhuber, and F. Cummins. Learning to forget: Continual prediction with lstm. 1999.
* [15] N. Golyandina, A. Korobeynikov, and A. Zhigljavsky. _Singular spectrum analysis with R_. Springer, 2018.
* [16] N. Golyandina, V. Nekrutkin, and A. A. Zhigljavsky. _Analysis of time series structure: SSA and related techniques_. Chapman and Hall/CRC, 2001.
* [17] R. A. Gonzalez and C. R. Rojas. A finite-sample deviation bound for stable autoregressive processes. In _Learning for Dynamics and Control_, pages 191-200. PMLR, 2020.
* [18] H. Hassani and R. Mahmoudvand. Multivariate singular spectrum analysis: A general view and new vector forecasting approach. _International Journal of Energy and Statistics_, 1(01):55-83, 2013.
* [19] Y. Jedra and A. Proutiere. Sample complexity lower bounds for linear system identification. In _2019 IEEE 58th Conference on Decision and Control (CDC)_, pages 2676-2681. IEEE, 2019.
* [20] Y. Jedra and A. Proutiere. Finite-time identification of stable linear systems optimality of the least-squares estimator. In _2020 59th IEEE Conference on Decision and Control (CDC)_, pages 996-1001. IEEE, 2020.
* [21] Y. Jedra and A. Proutiere. Finite-time identification of linear systems: Fundamental limits and optimal algorithms. _IEEE Transactions on Automatic Control_, 2022.
* [22] Microsoft. nimbusml package, ssa class, 2020. https://docs.microsoft.com/en-us/python/api/nimbusml/nimbusml.timeseries.ssachangepointdetector.
* [23] D. Salinas, V. Flunkert, J. Gasthaus, and T. Januschowski. Deepar: Probabilistic forecasting with autoregressive recurrent networks. _International Journal of Forecasting_, 2019.
* [24] T. Sarkar and A. Rakhlin. Near optimal finite time identification of arbitrary linear dynamical systems. In _International Conference on Machine Learning_, pages 5610-5618. PMLR, 2019.
* [25] S. Seabold and J. Perktold. statsmodels: Econometric and statistical modeling with python. In _9th Python in Science Conference_, 2010.
* [26] R. H. Shumway, D. S. Stoffer, and D. S. Stoffer. _Time series analysis and its applications_, volume 3. Springer, 2000.
* [27] M. Simchowitz, H. Mania, S. Tu, M. I. Jordan, and B. Recht. Learning without mixing: Towards a sharp analysis of linear system identification, 2018.
* [28] S. J. Taylor and B. Letham. Forecasting at scale. _The American Statistician_, 72(1):37-45, 2018.
* [29] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. _arXiv preprint arXiv:1011.3027_, 2010.
* [30] R. Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* [31] M. J. Wainwright. _High-dimensional statistics: A non-asymptotic viewpoint_, volume 48. Cambridge university press, 2019.

* [32] P.-A. Wedin. Perturbation bounds in connection with singular value decomposition. _BIT Numerical Mathematics_, 12(1):99-111, 1972.
* [33] P.-A. Wedin. Perturbation theory for pseudo-inverses. _BIT Numerical Mathematics_, 13:217-232, 1973.

Concentration inequalities for AR processes

In this section, we present two important lemmas that are central to the main results. Before stating the two lemmas, we start with key important quantities of AR Processes.

### Controllability Gramian of AR Processes

In this section, we explain the notation of key quantities used in the main results. First, note that the process in (2) can be written in matrix-vector form by defining a transition matrix and input vector

\[\bm{A}_{n}=\begin{bmatrix}\alpha_{n1:m(p_{n}-1)}^{\top}&\alpha_{np_{n}}\\ I_{p_{n}-1}&0_{p_{n}-1}\end{bmatrix},\qquad B_{n}=\begin{bmatrix}1\\ 0_{p_{n}-1}\end{bmatrix},\]

where \(0_{p_{n}-1}\) is the column vector of \(p_{n}-1\) zeroes. Then, with \(X_{n}(t)\coloneqq[x_{n}(t),\ldots,x_{n}(t-p_{n}+1)]\),

\[X_{n}(t)=\bm{A}_{n}X_{n}(t-1)+B_{n}\eta_{nt}.\]

With this setup, we define two important matrices,

\[\Gamma_{n}=\sum_{t=0}^{\infty}\bm{A}_{n}^{t}(\bm{A}_{n}^{\top})^{t},\qquad \Psi_{n}=\sum_{t=0}^{\infty}\bm{A}_{n}^{t}B_{n}B_{n}^{\top}(\bm{A}_{n}^{\top}) ^{t},\] (14)

where \(\Psi_{n}\) is known as the _controllability Gramian_ of the \(n\)-th \(\mathrm{AR}(p_{n})\) process \(x_{n}\). For a positive semi-definite matrix \(\bm{M}\), let \(\lambda_{\max}(\bm{M})\geq 0\) and \(\lambda_{\min}(\bm{M})\geq 0\) be its maximum and minimum eigenvalues, respectively. We define \(\lambda_{\max}(\Psi)\coloneqq\max_{n}\lambda_{\max}(\Psi_{n})\) and \(\lambda_{\min}(\Psi)\coloneqq\min_{n}\lambda_{\min}(\Psi_{n})\). We also define \(\lambda_{\max}(\Gamma)\) and \(\lambda_{\min}(\Gamma)\) analogously for \(\Gamma\).

### Key Lemmas

Now, we present two important lemmas that are central to the main results. The first lemma states that a vector of consecutive entries of a stationary autoregressive process is a sub-gaussian vector with variance proxy \(\sigma_{x}^{2}\).

**Lemma A.1**.: _Let assumptions 2.3 and 2.4 hold, then, for any \(n\in[N],t\geq 1,K\geq 1\), the vector \([x_{n}(t),\ldots,x_{n}(t+K)]\) is a sub-gaussian vector with variance proxy \(\sigma_{x}^{2}\)._

This lemma, along with the well known bound on the norm of sub-gaussian vectors, is key to the bounds presented in Section 4.1. Further, this Lemma, along with an \(\varepsilon\)-net argument, allows us to bound the operator norm of the stacked Page matrix of the AR processes \(x_{1}(t),\ldots,x_{N}(t)\). Specifically, we establish next that its operator norm grows as \(O(\sqrt{NT/L})\) with high probability.

**Lemma A.2**.: _For any \(T>1\), \(1\leq L\leq\sqrt{NT}\), and \(t_{0}>0\), let_

\[\mathbf{Z}_{x}(t_{0})=[\mathbf{Z}(x_{1},L,T,t_{0})\quad\mathbf{Z}(x_{2},L,T,t_ {0})\quad\ldots\quad\mathbf{Z}(x_{n},L,T,t_{0})]\]

_be the \(L\times NT/L\) stacked Page matrix of the AR processes \(x_{1}(t),\ldots,x_{N}(t)\), as defined in (2). Let Assumptions 2.3 and 2.4 hold. Then, each row \([\mathbf{Z}_{x}(t_{0})]_{i.}\) is a sub-gaussian vector with variance proxy \(\sigma_{x}^{2}\). Further, With probability \(1-\exp{(-cNT/L)}\),_

\[\left\|\mathbf{Z}_{x}(t_{0})\right\|_{2}\leq 2c\sigma_{x}\sqrt{NT/L}.\]

Next, we prove Lemma A.1 and Lemma A.2. Before stating the proofs, we start with the important helper Lemma A.3.

**Lemma A.3** (\(\mathrm{MA}(\infty)\) representation of an \(\mathrm{AR}(p)\) process).: _Let \(x(t)\) be a stationary \(\mathrm{AR}(p)\) process defined as_

\[x(t)=\sum_{i=1}^{p}\alpha_{i}x(t-i)+\eta(t).\]

_Assume that the roots \((\lambda_{1},\ldots,\lambda_{p})\) of its characteristic polynomial \(g(z)\coloneqq z^{p}-\sum_{i=1}^{p}\alpha_{i}z^{p-i}\) are distinct. Then, \(x(t)\) has the \(\text{MA}(\infty)\) representation_

\[x(t)=\sum_{k=0}^{\infty}\beta_{k}\eta(t-k),\]

_with \(\beta_{k}\coloneqq\sum_{i=1}^{p}a_{i}\lambda_{i}^{k}\), \(a_{i}\coloneqq\prod_{1\leq j\leq p,\ j\neq i}\left(1-\frac{\lambda_{j}}{ \lambda_{i}}\right)^{-1}\)._Proof.: First, note that that we can write \(x(t)\) in lag operator form as

\[f(L)\;x(t)\triangleq\left(1-\sum_{i=1}^{p}\alpha_{i}L^{i}\right)x(t)=\eta(t).\] (15)

Here, \(L\) is a lag operator such that \(Lx(t)=x(t-1)\). Note that since we have assumed the AR process to be stationary, all the roots of \(g(z)\) lie inside the unit circle [26]. Using the roots of \(g(z)\), (15) can be written as,

\[\prod_{i=1}^{p}(1-\lambda_{i}L)\;x(t)=\eta(t).\]

Further, using the partial fraction decomposition we can write the process as

\[x_{t} =\frac{1}{\prod_{i=1}^{p}(1-\lambda_{i}L)}\eta(t)\] \[=\sum_{i=1}^{p}\frac{a_{i}}{1-\lambda_{i}L}\eta(t)\]

where,

\[a_{i}=\prod_{1\leq j\leq p,\;j\neq i}\left(1-\frac{\lambda_{j}}{\lambda_{i}} \right)^{-1}\]

Recall that we can expand each fraction as

\[\frac{a_{i}}{1-\lambda_{i}L}=a_{i}\left(\sum_{k=0}^{\infty}\lambda_{i}^{k}L^{ k}\right).\]

Thus, finally, we can write \(x(t)\) as,

\[x(t)=\sum_{k=0}^{\infty}\beta_{k}\eta(t-k),\]

with \(\beta_{k}\coloneqq\sum_{i=1}^{p}a_{i}\lambda_{i}^{k}\). 

### Proof of Lemma a.1

Proof.: First, let \(X_{n}(t,K)=[x_{n}(t),\ldots,x_{n}(T+K)]\) be the vector of interest. Let \(E_{n}(t,K)=[\eta_{n}(t),\ldots,\eta_{n}(T+K)]\), then using Lemma A.3, we can write \(X_{n}(t,K)\) as

\[X_{n}(t,K)=\sum_{k=0}^{\infty}\beta_{nk}E_{n}(t-k,K)\]

Note that \(\beta_{nk}E_{n}(t-k,K)\) is clearly a sub-gaussian vector with variance proxy \(\beta_{nk}^{2}\sigma^{2}\). Further note that the sum of two sub-gaussian random vectors, not necessarily independent, is also a sub-gaussian random vector. To see this, suppose \(\mathbf{x}\) and \(\mathbf{y}\) are both sub-gaussian vectors with variance proxies \(\sigma_{x}^{2}\) and \(\sigma_{y}^{2}\). Then for any \(\mathbf{v}\in\mathcal{S}^{K}\),

\[\mathbf{v}^{T}(\mathbf{x}+\mathbf{y}) =\mathbf{v}^{T}\mathbf{x}+\mathbf{v}^{T}\mathbf{y}\] \[\sim\text{subG}\left((\sigma_{x}+\sigma_{y})^{2}\right).\]

Thus, \(X_{n}(t,K)\), for any \(n\), is also a sub-gaussian random vector with variance proxy

\[\left(\sum_{k=0}^{\infty}|\beta_{nk}|\sigma\right)^{2}\leq\sigma^{2}\left( \sum_{k=0}^{\infty}\max_{n}|\beta_{nk}|\right)^{2}.\]Now we are interested in bounding \(\sum_{k=0}^{\infty}\max_{n}|\beta_{nk}|\). To do so, first recall that \(\beta_{nk}\coloneqq\sum_{i=1}^{p_{n}}a_{ni}\lambda_{ni}^{k}\) where \(\lambda_{ni}\in\mathbb{C}\), \(i\in[p_{n}]\) denote the roots of \(g_{n}(z)\) and \(a_{i}=\prod_{1\leq j\leq p_{n},\ j\neq i}\left(1-\frac{\lambda_{nj}}{\lambda_{ ni}}\right)^{-1}\). Let \(\lambda_{\star}=\max_{i,n}|\lambda_{ni}|\) and \(c_{\lambda_{n}}=\sum_{i=1}^{p_{n}}\left|\prod_{1\leq j\leq p,\ j\neq i}\left(1- \frac{\lambda_{nj}}{\lambda_{ni}}\right)^{-1}\right|\). Then, we can bound \(\beta_{nk}\) as

\[|\beta_{nk}| \leq\lambda_{\star}^{k}\sum_{i=1}^{p_{n}}|a_{i}|\] \[\leq c_{\lambda_{n}}\lambda_{\star}^{k}.\]

Thus, with \(c_{\lambda}=\max_{n}c_{\lambda_{n}}\)

\[\sum_{k=0}^{\infty}\max_{n}|\beta_{nk}|\leq\frac{c_{\lambda}}{1-\lambda_{ \star}}.\] (16)

Hence, \(X_{n}(t,K)\) is a sub-gaussian random vector with variance proxy \(\sigma_{x}^{2}\coloneqq\frac{c_{\lambda}^{2}\sigma^{2}}{(1-\lambda_{\star})^{ 2}}\). 

### Proof of Lemma a.2

Proof.: Let \(q=NT/L\). We will find an upper bound on the operator norm by following two steps: (i) we will represent each \(x_{n}(t)\)\(\forall n\in[N]\) as an MA\((\infty)\) process; then, (ii) we will bound the operator norm of the stacked Page matrix of this MA\((\infty)\) processes.

**Step 1: \(\text{MA}(\infty)\) representation for \(\text{AR}(p)\).**

As a direct application of Lemma A.3, we can write \(x_{n}(t)\) as,

\[x_{n}(t)=\sum_{k=0}^{\infty}\beta_{nk}\eta_{n}(t-k),\]

with \(\beta_{nk}\coloneqq\sum_{i=1}^{p}a_{ni}\lambda_{ni}^{k}\), \(a_{ni}\coloneqq\prod_{1\leq j\leq p,\ j\neq i}\left(1-\frac{\lambda_{nj}}{ \lambda_{ni}}\right)^{-1}\)

**Step 2: Bound the Page matrix for the MA\((\infty)\) Page matrix.**

First, let's consider, without loss of generality, the case when \(t_{0}=1\). That is, let's consider \(\mathbf{Z}_{x}\coloneqq\mathbf{Z}_{x}(1)\). To get the desired bound, we will first obtain a high probability bound on the Euclidean norm of \(\mathbf{Z}_{x}^{\top}\mathbf{v}\) for any vector \(\mathbf{v}\in\mathcal{S}^{L-1}\coloneqq\{\mathbf{v}\in\mathbb{R}^{L}:\| \mathbf{v}\|_{2}=1\}\). Then, we will bound the operator norm using this and an \(\varepsilon\)-net argument.

Note that by step (1), we have

\[x_{n}(t)=\sum_{k=0}^{\infty}\beta_{nk}\eta_{n}(t-k).\]

Now define \(\bm{E}_{i}\) as the analogous stacked Page matrix for \(\eta_{1}(t),\ldots,\eta_{N}(t)\) for \(t\in\{i,\ldots,i+T-1\}\). Observe that \(\bm{E}_{i}\) are simply identically distributed (but not independent) matrices of i.i.d. sub-gaussian random variables. Then we can write

\[\mathbf{Z}_{x}=\sum_{k=0}^{\infty}\bm{E}_{1-k}\bm{B}_{k},\] (17)

where \(\bm{B}_{k}\in\mathbb{R}^{q\times q}\) is a diagonal matrix with the \(i\)-th diagonal \([\bm{B}_{k}]_{ii}=\beta_{\lceil\frac{iN}{q}\rceil k}\). Then

\[\left\|\mathbf{Z}_{x}^{\top}\mathbf{v}\right\|_{2}=\left\|\sum_{k=0}^{\infty} \bm{B}_{k}\bm{E}_{1-k}^{\top}\mathbf{v}\right\|_{2}.\]It is easy to verify that \(\bm{E}_{i}^{\top}\mathbf{v}\) is a _sub-gaussian random vector_ (see Definition H.3) with variance proxy \(\sigma^{2}\), because for any \(\mathbf{u}\in\mathcal{S}^{q-1}\) we have (using \(\bm{E}\) as a shorthand for \(\bm{E}_{i}\))

\[\mathbf{v}^{T}\bm{E}\mathbf{u} =\sum_{i=1}^{L}\sum_{j=1}^{q}v_{i}u_{j}E_{ij}\] \[\sim\text{subG}\left(\sum_{i=1}^{L}\sum_{j=1}^{q}v_{i}^{2}u_{j}^ {2}\sigma^{2}\right)\] \[\sim\text{subG}(\sigma^{2}).\]

Using the above, \(\bm{B}_{k}\bm{E}_{i}^{\top}\mathbf{v}\) is \(\sim\text{subG}(\max_{n}|\beta_{nk}|^{2}\sigma^{2})\). Since \(\bm{\mathrm{Z}}_{x}\mathbf{v}\) is a sum of sub-gaussian random vectors, it is also a sub-gaussian random vector with variance proxy \(\sigma_{x}^{2}\) as we showed in the proof of Lemma A.1.

Using Lemma H.2, we get, with probability at least \(1-\delta\),

\[\left\|\bm{\mathrm{Z}}_{x}^{\top}\mathbf{v}\right\|_{2}\leq 4\sigma_{x}\sqrt{q} +2\sigma_{x}\sqrt{\log\left(\frac{1}{\delta}\right)}\]

Equivalently, for \(t\geq 0\), it holds that

\[\mathbb{P}\left(\left\|\bm{\mathrm{Z}}_{x}\mathbf{v}\right\|_{2}-4\sigma_{x} \sqrt{q}>t\right)\leq\exp\left(-\frac{t^{2}}{4\sigma_{x}^{2}}\right).\]

Now we have shown that for any _fixed_\(\mathbf{v}\in\mathcal{S}^{L-1}\), the quantity \(\left\|\bm{\mathrm{Z}}_{x}\mathbf{v}\right\|_{2}\) grows as \(O\left(\sqrt{q}\right)\) with high probability. What remains is to apply the union bound over an \(\varepsilon\)-net to extend this to the _maximum_ over \(\mathcal{S}^{L-1}\), i.e. the operator norm of \(\bm{\mathrm{Z}}_{x}\).

Let \(\Sigma\) be a maximal \(1/2\)-net of \(\mathcal{S}^{L-1}\), i.e. a set of points in \(\mathcal{S}^{L-1}\) spaced at least \(1/2\) from each other such that no other point can be added without violating this property. Now consider \(\mathbf{v}_{\star}\) such that \(\left\|\bm{\mathrm{Z}}_{x}^{\top}\mathbf{v}_{\star}\right\|_{2}=\left\|\bm{ \mathrm{Z}}_{x}\right\|_{2}\). Since \(\Sigma\) is maximal, there is some \(\mathbf{v}\in\Sigma\) within \(1/2\) of \(\mathbf{v}_{\star}\). Since

and

\[\left\|\bm{\mathrm{Z}}_{x}^{\top}(\mathbf{v}-\mathbf{v}_{\star})\right\|_{2} \geq\left\|\bm{\mathrm{Z}}_{x}^{\top}\mathbf{v}_{\star}\right\|_{2}-\left\| \bm{\mathrm{Z}}_{x}^{\top}\mathbf{v}\right\|_{2}=\left\|\bm{\mathrm{Z}}_{x} \right\|_{2}-\left\|\bm{\mathrm{Z}}_{x}^{\top}\mathbf{v}\right\|_{2},\]

we have that

\[\left\|\bm{\mathrm{Z}}_{x}^{\top}\mathbf{v}\right\|_{2}\geq\left\|\bm{\mathrm{ Z}}_{x}\right\|_{2}/2,\]

i.e. taking the maximum over \(\Sigma\) is equivalent to doing so over \(\mathcal{S}^{L-1}\) up to a factor of two. Using the fact that \(\left|\Sigma\right|\leq C^{L}\) for some absolute constant \(C\), the fact that \(L<q\), and using a sufficiently large absolute constant \(c>0\) we have

\[\mathbb{P}\left(\left\|\bm{\mathrm{Z}}_{x}\right\|_{2}>2c\sigma_{x} \sqrt{q}\right) \leq\sum_{\mathbf{v}\in\Sigma}\mathbb{P}\left(\left\|\bm{\mathrm{Z }}_{x}\mathbf{v}\right\|_{2}>c\sigma_{x}\sqrt{q}\right)\] \[\leq 2C^{L}\exp\left(-c^{\prime}q\right)\] \[=\exp\left(-c^{\prime\prime}q\right).\]

where \(c^{\prime\prime}>0\) and \(c^{\prime}>0\) are absolute constants. Recalling that \(q=NT/L\) concludes the proof for the operator norm.

Finally, we show that each row \([\bm{\mathrm{Z}}_{x}(t_{0})]_{i\cdot}\) in the stacked page matrix is a sub-gaussian vector with variance proxy \(\sigma_{x}^{2}\). Consider, without loss of generality, the first row \([\bm{\mathrm{Z}}_{x}]_{1\cdot}\). Using (17), we have for any \(\mathbf{v}\in\mathcal{S}^{L-1}\)

\[\mathbf{v}^{\top}[\bm{\mathrm{Z}}_{x}]_{1\cdot}=\sum_{k=0}^{\infty}\mathbf{v}^ {\top}[\bm{E}_{1-k}\bm{B}_{k}]_{1\cdot},\]

note that \(\mathbf{v}^{\top}[\bm{E}_{1-k}\bm{B}_{k}]_{1\cdot}\) is \(\sim\mathrm{subG}(\max_{n}\beta_{nk}^{2}\sigma^{2})\), and hence, using the same argument for the sum of dependent sub-gaussian random variables used in the proof above, we have \(\mathbf{v}^{\top}[\bm{\mathrm{Z}}_{x}]_{1\cdot}\sim\mathrm{subG}(\sigma_{x}^{2})\).

Experiment details

In Appendix B.1, we detail the datasets used. In Appendix B.2, we explain the implementations and hyperparameter choices for our method and benchmark algorithms. In Appendix B.3, we describe our hyperparameter tuning strategy.

### Datasets

In the model estimation experiments, we generate a synthetic multivariate time series (\(N=10\)) that is a mixture of both harmonics and an \(\mathrm{AR}(2)\) process. The harmonics are generated as follow: we first generate \(R=3\) fundamental time series of the form \(g_{k}(t)=\sin\left(\omega_{k}t+\phi_{k}\right)\) for \(k\in[R]\), where \(\omega_{k}\) and \(\phi_{k}\) are uniformly randomly sampled on the ranges \([2\pi/100,2\pi/50]\), \([0,2\pi]\) respectively. We then sample the \(N=10\) series using \(f_{k}(t)=\sum_{i=1}^{R}c_{ki}g_{k}(t)\) for \(k\in[N]\), where the mixture coefficients \(c_{ki}\) are independent standard normal random variables. The AR process is stationary, and its parameter is chosen such that \(\lambda^{*}=\max_{n,i}|\lambda_{ni}|\) is set to one of \(\{0.3,0.6,0.95\}\). The noise of the process has a variance \(\sigma^{2}=0.2\).

In the forecasting experiments, we evaluate our method and benchmark algorithms on three real-world datasets and one synthetic dataset. The preprocessing steps and setup of each dataset are described below.

**Traffic Dataset.** This public dataset obtained from the UCI repository shows the occupancy rate of traffic lanes in San Francisco. The data is sampled every 15 minutes but to be consistent with previous work, we aggregate the data into hourly data and use the first 10248 time-points for training, the next 48 points for validation, and another 48 points for testing in the forecasting experiments. Specifically, in our testing period, we do 1-hour ahead forecasts for the next 48 hours.

**Electricity Dataset.** This is a public dataset obtained from the UCI repository which shows the 15-minutes electricity load of 370 households. We aggregate the data into hourly intervals and use the first 25824 time-points for training, the next 48 points for validation, and another 48 points for testing in the forecasting experiments. Specifically, in our testing period, we do 1-hour ahead forecasts for the next 48 hours.

**Exchange Dataset.** This is a dataset containing the daily exchange rates of eight foreign currencies, including those of Australia, the UK, Canada, Switzerland, China, Japan, New Zealand and Singapore, between 1990 and 2016. We standardize each time series to have zero mean and unit variance since the range of typical exchange rates varies greatly across the currencies. We use the first 7528 time-points for training, the next 30 for validation, and the final 30 for testing. All forecasts are made 1-day ahead.

**Synthetic Dataset.** We generate \(R\) fundamental time series of the form \(g_{k}(t)=\sin\left(\omega_{k}t+\phi_{k}\right)+m_{k}t\) for \(k\in[R]\), where \(\omega_{k}\), \(\phi_{k}\), and \(m_{k}\) are uniformly randomly sampled on the ranges \([2\pi/100,2\pi/10]\), \([0,2\pi]\), and \([-5\times 10^{-4},5\times 10^{-4}]\) respectively. We then sample \(N\) mixtures of the form \(f_{k}(t)=\sum_{i=1}^{R}c_{ki}g_{k}(t)\) for \(k\in[N]\), where the mixture coefficients \(c_{ki}\) are independent standard normals. We then inject \(\mathrm{AR}(1)\) autoregressive noise to each series to produce \(y_{k}(t)=f_{k}(t)+x_{k}(t)\) for \(k\in[N]\), where \(\{x_{k}(t)\}_{k\in[N]}\) are independent processes, each with a fixed autoregressive coefficient \(\alpha=-0.5\) and noise variance \(\sigma^{2}=1\). We fix the coefficient to ensure that each time series has significantly autocorrelated noise that can be exploited to improve predictions.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline Dataset & No. time series & Training period & Validation period & Test period \\ \hline Traffic & 963 & 1 to 10248 & 10249 to 10296 & 10296 to 10344 \\ Electricity & 370 & 1 to 25824 & 25825 to 25872 & 25872 to 25919 \\ Exchange & 8 & 1 to 7528 & 7529 to 7558 & 7559 to 7588 \\ Synthetic & 25 & 1 to 10000 & 10001 to 10025 & 10026 to 10050 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Dataset and training/validation/test split details.

### Algorithms

In this section, we discuss the implementations used and hyperparameter tuning details for each algorithm that was evaluated.

**SAMoSSA & mSSA.** Since mSSA is a special case of SAMoSSA where the second stage - fitting autoregressive models - is skipped, we use one in-house implementation to evaluate both algorithms. The relevant hyperparameters are as follows:

1. _The number of retained singular values,_ \(k\). This hyperparameter is relevant for both algorithms. We select \(k\) using one of three methods: (i) the data-driven procedure suggested in [13] which computes a threshold based on the shape and median singular value of the page matrix; (ii) picking \(k\) as the minimum number of singular values required to capture \(90\%\) of the spectral energy of the page matrix; (iii) picking a fixed low rank, specifically \(k=5\).
2. _The shape parameter of the stacked Page matrix._ This is the ratio between the number of columns and rows of the stacked Page matrix used to carry out the matrix estimation procedure. While our theoretical analysis fixes this value to \(1\) (recall that we set \(L=\sqrt{NT}\)), in practice we have found that a slightly wider Page matrix can improve performance. This parameter is chosen from \(\{1,3,5\}\).
3. _The number of autoregressive lag coefficients,_ \(p\). This hyperparameter is only relevant for SAMoSSA. While each time series does not necessarily need to use the same value, we enforce this to be the case for computational efficiency. We choose \(p\in\{0,1,2,3\}\), i.e., the model is allowed to not fit a residual model if it brings no apparent benefit, corresponding to the case of \(p=0\).

**Prophet.** We use Prophet's Python library with the parameters selected using a grid search of the following parameters as suggested in [12]:

1. _Changepoint prior scale._ This parameter determines how much the trend changes at the detected trend changepoints. We choose this parameter from \(\{0.001,0.05,0.2\}\).
2. _Seasonality prior scale._ This parameter controls the magnitude of the seasonality. We choose this parameter from \(\{0.01,10\}\).
3. _Seasonality Mode._ We choose between an "additive" and "multiplicative" seasonality term.

The parameters for each time series are chosen independently.

**ARIMA.** We used the ARIMA implementation of the Python library statsmodels[25]. The hyperparameters are grid searched independently for each time series, and are as follows:

1. _Autoregressive order._ This is the number of autoregressive lag coefficients fitted, chosen from \(\{1,2,3\}\).
2. _Differencing order._ This denotes the number of times the time series is differenced before a model is fitted, and we choose between 0 (no differencing) and 1.
3. _Moving average order._ This is the number of moving average lag coefficients, chosen from \(\{1,2,3\}\).

**LSTM.** We use Keras implementation [9]. We perform a grid search on the number of layers \(\{2,3,4\}\).

**DeepAR.** We use the implementation provided by the GluonTS package [7]. We use the default parameters.

### Parameter Selection

We tune hyperparameters for all algorithms evaluated using rolling cross validation. During the validation step, each model is fitted only once on the training set; then, it repeatedly makes one-step-ahead forecasts and is afterwards provided with the realized value, in order to use the updated data to make the next forecast. During test time, the model is fitted on the training _and_ validation sets, and similarly repeatedly makes one-step-ahead forecasts with the fitted parameters. For the models that choose one set of hyperparameters for all \(N\) time series, the mean \(R^{2}\) score over all time series on the validation set predictions is the metric of choice.

Proof of Theorem 4.1

In this section, we provide the proof for Theorem 4.1. While the analysis generally follows the argument for the imputation error in [3], we adapt it for our different assumptions and metric of interest. **Metric**. First, recall that our metric of interest is

\[\mathsf{EstErr}(N,T,n)=\frac{1}{T}\sum_{t=1}^{T}(\widehat{f}_{n}(t)-f_{n}(t))^{ 2}.\] (18)

Recall that \(\widehat{\mathbf{Z}}_{f}\) is the stacked Page matrix of \(\widehat{f}_{1},\ldots,\widehat{f}_{N}\), and \(\mathbf{Z}_{f}\) is the stacked Page matrix of \(f_{1},\ldots,f_{N}\). To bound this error, we first establish a deterministic bound through a general lemma for Hard Singular Value Thresholding (HSVT).

### Deterministic Bound

We state the following result, a more general version of which is stated in [3].

**Lemma C.1** ([3]).: _For \(k\geq 1\), let \(\bm{Y}=\bm{M}+\bm{E}\in\mathbb{R}^{q\times p}\) with \(\text{rank}(\bm{M})=k\). Let \(\mathbf{U}_{k}\bm{\Sigma}_{k}\mathbf{V}_{k}{}^{\top}\) and \(\widehat{\mathbf{U}}_{k}\widehat{\bm{\Sigma}}_{k}\widehat{\mathbf{V}}_{k}^{\top}\) denote the top k singular components of the SVD of \(\bm{M}\) and \(\bm{Y}\) respectively. Then, the HSVT estimate \(\widehat{\bm{M}}=\widehat{\mathbf{U}}_{k}\widehat{\bm{\Sigma}}_{k}\widehat{ \mathbf{V}}_{k}^{\top}\) is such that for all \(j\in[q]\),_

\[\|\widehat{\bm{M}}_{j\cdot}^{\top}-\bm{M}_{j\cdot}^{\top}\|_{2}^{2}\leq 2 \frac{\|\bm{E}\|_{2}^{2}}{\sigma_{k}(\bm{M})^{2}}\left(\left\|\bm{E}_{j\cdot}^ {\top}\right\|_{2}^{2}+\left\|\bm{M}_{j\cdot}^{\top}\right\|_{2}^{2}\right)+2 \Big{\|}\mathbf{V}_{k}\mathbf{V}_{k}{}^{\top}\bm{E}_{j\cdot}^{\top}\Big{\|}_{2 }^{2},\]

Proof.: First, note that

\[\widehat{\bm{M}}_{j\cdot}^{\top}-\bm{M}_{j\cdot}^{\top}=\Big{(}\widehat{ \mathbf{V}}_{k}\widehat{\mathbf{V}}_{k}^{\top}\bm{Y}_{j\cdot}^{\top}-\widehat{ \mathbf{V}}_{k}\widehat{\mathbf{V}}_{k}^{\top}\bm{M}_{j\cdot}^{\top}\Big{)}+ \Big{(}\widehat{\mathbf{V}}_{k}\widehat{\mathbf{V}}_{k}^{\top}\bm{M}_{j\cdot }^{\top}-\bm{M}_{j\cdot}^{\top}\Big{)},\]

where \(\widehat{\bm{M}}_{j\cdot}^{\top}=\widehat{\mathbf{V}}_{k}\widehat{\mathbf{V}}_ {k}^{\top}\bm{Y}_{j\cdot}^{\top}\) by definition. Note that the vector \(\Big{(}\widehat{\mathbf{V}}_{k}\widehat{\mathbf{V}}_{k}^{\top}\bm{M}_{j\cdot }^{\top}-\bm{M}_{j\cdot}^{\top}\Big{)}\) is in the span of a subspace orthogonal to \(\widehat{\mathbf{V}}_{k}\widehat{\mathbf{V}}_{k}^{\top}\), and hence we have by the Pythagorean theorem,

\[\Big{\|}\widehat{\bm{M}}_{j\cdot}^{\top}-\bm{M}_{j\cdot}^{\top} \Big{\|}_{2}^{2} =\Big{\|}\widehat{\mathbf{V}}_{k}\widehat{\mathbf{V}}_{k}^{\top} \mathbf{Y}_{j\cdot}^{\top}-\widehat{\mathbf{V}}_{k}\widehat{\mathbf{V}}_{k}^{ \top}\bm{M}_{j\cdot}^{\top}\Big{\|}_{2}^{2}+\Big{\|}\widehat{\mathbf{V}}_{k} \widehat{\mathbf{V}}_{k}^{\top}\bm{M}_{j\cdot}^{\top}-\bm{M}_{j\cdot}^{\top} \Big{\|}_{2}^{2}\] \[=\Big{\|}\widehat{\mathbf{V}}_{k}\widehat{\mathbf{V}}_{k}^{\top} \bm{E}_{j\cdot}^{\top}\Big{\|}_{2}^{2}+\Big{\|}\widehat{\mathbf{V}}_{k} \widehat{\mathbf{V}}_{k}^{\top}\bm{M}_{j\cdot}^{\top}-\bm{M}_{j\cdot}^{\top} \Big{\|}_{2}^{2}.\] (19)

The first term can be futher decomposed as

\[\Big{\|}\widehat{\mathbf{V}}_{k}\widehat{\mathbf{V}}_{k}^{\top} \bm{E}_{j\cdot}^{\top}\Big{\|}_{2}^{2} \leq 2\Big{\|}\widehat{\mathbf{V}}_{k}\widehat{\mathbf{V}}_{k}^{\top} \bm{E}_{j\cdot}^{\top}-\mathbf{V}_{k}\mathbf{V}_{k}{}^{\top}\bm{E}_{j\cdot}^{ \top}\Big{\|}_{2}^{2}+2\Big{\|}\mathbf{V}_{k}\mathbf{V}_{k}{}^{\top}\bm{E}_{j \cdot}^{\top}\Big{\|}_{2}^{2}\] \[\leq 2\Big{\|}\widehat{\mathbf{V}}_{k}\widehat{\mathbf{V}}_{k}^{\top} -\mathbf{V}_{k}{}^{\top}\Big{\|}_{2}^{2}\Big{\|}\bm{E}_{j\cdot}^{\top}\Big{\|} _{2}^{2}+2\Big{\|}\mathbf{V}_{k}\mathbf{V}_{k}{}^{\top}\bm{E}_{j\cdot}^{\top} \Big{\|}_{2}^{2}\] (20)

Next, we bound the first term on the right hand side of (20). To that end, by Wedin \(\sin\Theta\) Theorem (see [10; 32]) and recalling \(\text{rank}(\bm{M})=k\),

\[\big{\|}\widehat{\mathbf{V}}_{k}\widehat{\mathbf{V}}_{k}^{\top}-\mathbf{V}_{k} \mathbf{V}_{k}{}^{\top}\big{\|}_{2}\leq\frac{\|\bm{E}\|_{2}}{\sigma_{k}(\bm{M})}\]

Then it follows that

\[\Big{\|}\widehat{\mathbf{V}}_{k}\widehat{\mathbf{V}}_{k}^{\top} \bm{E}_{j\cdot}^{\top}\Big{\|}_{2}^{2} \leq 2\frac{\|\bm{E}\|_{2}^{2}}{\sigma_{k}(\bm{M})^{2}}\Big{\|}\bm{E}_{j \cdot}^{\top}\Big{\|}_{2}^{2}+2\Big{\|}\mathbf{V}_{k}\mathbf{V}_{k}{}^{\top} \bm{E}_{j\cdot}^{\top}\Big{\|}_{2}^{2}\] (21)

Then, we turn to bounding the second term from (19).

\[\Big{\|}\widehat{\mathbf{V}}_{k}\widehat{\mathbf{V}}_{k}^{\top} \bm{M}_{j\cdot}^{\top}-\bm{M}_{j\cdot}^{\top}\Big{\|}_{2}^{2} =\Big{\|}\widehat{\mathbf{V}}_{k}\widehat{\mathbf{V}}_{k}^{\top} \bm{M}_{j\cdot}^{\top}-\mathbf{V}_{k}\mathbf{V}_{k}{}^{\top}\bm{M}_{j\cdot}^{ \top}\Big{\|}_{2}^{2}\] \[\leq\Big{\|}\widehat{\mathbf{V}}_{k}\widehat{\mathbf{V}}_{k}^{\top} -\mathbf{V}_{k}\mathbf{V}_{k}{}^{\top}\Big{\|}_{2}^{2}\Big{\|}\bm{M}_{j\cdot}^{ \top}\Big{\|}_{2}^{2}\] \[\leq\frac{\|\bm{E}\|_{2}^{2}}{\sigma_{k}(\bm{M})^{2}}\Big{\|} \bm{M}_{j\cdot}^{\top}\Big{\|}_{2}^{2}.\] (22)Using (19), (21), and (22), we have,

\[\left\|\widehat{\bm{M}}_{j\cdot}^{\top}-\bm{M}_{j\cdot}^{\top}\right\|_{2}^{2} \leq 2\frac{\|\bm{E}\|_{2}^{2}}{\sigma_{k}(\bm{M})^{2}}\left(\left\|\bm{E}_{ j\cdot}^{\top}\right\|_{2}^{2}+\left\|\bm{M}_{j\cdot}^{\top}\right\|_{2}^{2} \right)+2\Big{\|}\mathbf{V}_{k}\mathbf{V}_{k}{}^{\top}\bm{E}_{j\cdot}^{\top} \Big{\|}_{2}^{2},\] (23)

which completes the proof. 

Now, Let's apply Lemma C.1 to our setting. Let \(\mathbf{Z}_{x}\) be the stacked page matrix for \(x_{1}(t),\ldots,x_{n}(t)\). Using Lemma C.1, and setting \(\bm{Y}=\mathbf{Z}_{y}^{\top}\), \(\bm{M}=\mathbf{Z}_{f}^{\top}\), \(\bm{E}=\mathbf{Z}_{x}^{\top}\), and reusing \(\mathbf{U}_{k}\bm{\Sigma}_{k}\mathbf{V}_{k}{}^{\top}\) to represent the top \(k\) singular components of the SVD of \(\mathbf{Z}_{f}\), results in the following deterministic bound for HSVT with rank set to \(k\),

\[\left\|\left[\widehat{\mathbf{Z}}_{f}\right]_{\cdot j}-[\mathbf{Z}_{f}]_{\cdot j }\right\|_{2}^{2}\leq 2\frac{\|\mathbf{Z}_{x}\|_{2}^{2}}{\sigma_{k}( \mathbf{Z}_{f})^{2}}\left(\left\|[\mathbf{Z}_{x}]_{\cdot j}\right\|_{2}^{2}+ \left\|[\mathbf{Z}_{f}]_{\cdot j}\right\|_{2}^{2}\right)+2\Big{\|}\mathbf{U}_ {k}\mathbf{U}_{k}{}^{\top}[\mathbf{Z}_{x}]_{\cdot j}\Big{\|}_{2}^{2},\] (24)

### Deterministic To High-Probability

Next, we convert the bound in (24) to a bound in expectation (as well as one in high-probability) for \(\|\widehat{\mathbf{Z}}_{f}-\mathbf{Z}_{f}\|_{2,\infty}\). In particular, we establish

**Theorem C.1**.: _Let assumptions 2.1, 2.2, 2.3, and 2.4 hold. Let \(\sigma_{x}=\frac{c_{\lambda}\sigma}{(1-\lambda_{\star})}\), where \(\lambda_{\star}=\max_{i,n}|\lambda_{ni}|\) and \(c_{\lambda}=\max_{n}\sum_{i=1}^{p_{n}}\left|\prod_{1\leq j\leq p,\;j\neq i} \left(1-\frac{\lambda_{ni}}{\lambda_{ni}}\right)^{-1}\right|\). Then, the HSVT estimate \(\widehat{\mathbf{Z}}_{f}\) with parameter \(k\) is such that_

\[\max_{j\in[q]}\frac{1}{L}\Big{\|}[\widehat{\mathbf{Z}}_{f}]_{\cdot j}-[ \mathbf{Z}_{f}]_{\cdot j}\Big{\|}_{2}^{2}\leq\frac{C\sigma_{x}^{2}(NT)^{2}}{L ^{3}\sigma_{k}(\mathbf{Z}_{f})^{2}}\left(\sigma_{x}^{2}+f_{\max}^{2}\right)+ \frac{C\sigma_{x}^{2}k\log(NT/L)}{L},\] (25)

_with probability of at least \(1-\frac{c}{(NT)^{11}}\)._

Proof.: We start by identifying certain high probability events. Subsequently, using these events and (24), we will conclude the proof.

**High Probability Events.** Let \(q\coloneqq NT/L\) be the number of columns in the stacked page matrix \(\widehat{\mathbf{Z}}_{f}\), and let \(C>0\) be some positive absolute constant. Define

\[E_{1} :=\Big{\{}\|\mathbf{Z}_{x}\|_{2}\leq C\sigma_{x}\sqrt{q}\Big{\}},\] \[E_{2} :=\Big{\{}\|\mathbf{Z}_{x}\|_{\infty,2},\|\mathbf{Z}_{x}\|_{2, \infty}\leq C\sigma_{x}\sqrt{q}\Big{\}},\] \[E_{3} :=\Big{\{}\max_{j\in[q]}\|\mathbf{U}_{k}\mathbf{U}_{k}{}^{\top}[ \mathbf{Z}_{x}]_{\cdot j}\|_{2}^{2}\leq C\sigma_{x}^{2}k\log(q)\Big{\}},\]

**Lemma C.2**.: _For some positive constant \(c_{1}>0\) and \(C>0\) large enough in definitions of \(E_{1},E_{2},\) and \(E_{3}\),_

\[\mathbb{P}(E_{1}) \geq 1-2e^{-c_{1}q},\] \[\mathbb{P}(E_{2}) \geq 1-2e^{-c_{1}q},\] \[\mathbb{P}(E_{3}) \geq 1-\frac{c_{1}}{(NT)^{11}}.\]

Proof.: We bound the probability of events above below.

**Bounding \(\bm{E}_{1}\).** This is an immediate consequence of Lemma A.2.

**Bounding \(\bm{E}_{2}\).** Recall that we assume \(L\leq q\). Observe that for any matrix \(A\in\mathbb{R}^{L\times q}\), \(\|A\|_{\infty,2},\ \|A\|_{2,\infty}\leq\|A\|_{2}\). Thus using the argument to bound \(\bm{E}_{1}\), concludes the proof.

**Bounding \(\bm{E}_{3}\)**. Let \(u_{1},\ldots,u_{k}\) be the orthonormal basis for \(\mathbf{U}_{k}\). Then, consider for \(j\in[q]\),

\[\|\mathbf{U}_{k}\mathbf{U}_{k}\mathbf{U}_{k}^{\top}[\mathbf{Z}_{x}]_{\cdot j}\|_ {2}^{2}=\sum_{i=1}^{k}\lVert u_{i}u_{i}^{\top}[\mathbf{Z}_{x}]_{\cdot j}\rVert_ {2}^{2}\leq\sum_{i=1}^{k}\left(u_{i}^{\top}[\mathbf{Z}_{x}]_{\cdot j}\right)^{ 2}\ =\ \sum_{i=1}^{k}Z_{i}^{2},\]

where \(Z_{i}=u_{i}^{\top}[\mathbf{Z}_{x}]_{\cdot j}\). As shown in Lemma A.1, \([\mathbf{Z}_{x}]_{\cdot j}\) is a sub-gaussian vector with variance proxy \(\sigma_{x}^{2}\). Using Lemma H.4, we have

\[\mathbb{P}\Big{(}\sum_{i=1}^{k}Z_{i}^{2}>t\Big{)}\leq c\exp\left(-\frac{t}{16k \sigma_{x}^{2}}\right).\]

Therefore, for choice of \(t=C\sigma_{x}^{2}k\log(q)\) with large enough constant \(C>368\), we have,

\[\mathbb{P}\Big{(}\sum_{i=1}^{k}Z_{i}^{2}>C\sigma_{x}^{2}k\log(q)\Big{)}\leq \frac{c_{1}}{q^{23}}.\]

Recalling that \(L\leq q\), and taking a union bound over all \(j\in[q]\), we have that

\[\mathbb{P}\Big{(}E_{4}^{c}\Big{)}\leq\frac{c_{1}}{(qL)^{11}}.\]

The following is an immediate corollary of the above stated bounds.

**Corollary C.1**.: _Let \(E:=E_{1}\cap E_{2}\cap E_{3}\). Then,_

\[\mathbb{P}(E^{c})\leq\frac{C_{1}}{(NT)^{11}},\] (26)

_where \(C_{1}\) is an absolute positive constant._

Thus, under event \(E\), and using (24), we have, with probability \(1-\frac{c}{(NT)^{10}}\),

\[\max_{j\in[q]}\Big{\lVert}[\widehat{\mathbf{Z}}_{f}]_{\cdot j}-[ \mathbf{Z}_{f}]_{\cdot j}\Big{\rVert}_{2}^{2} \leq\frac{C\sigma_{x}^{2}qr}{\sigma_{k}(\mathbf{Z}_{f})^{2}} \left(qr\sigma_{x}^{2}+qf_{\max}^{2}\right)\] \[+C\sigma_{x}^{2}k\log(q)\]

Recall that \(q=NT/L\) then,

\[\max_{j\in[q]}\frac{1}{L}\Big{\lVert}[\widehat{\mathbf{Z}}_{f}]_{\cdot j}-[ \mathbf{Z}_{f}]_{\cdot j}\Big{\rVert}_{2}^{2}\leq\frac{C\sigma_{x}^{2}(NT)^{2 }}{L^{3}\sigma_{k}(\mathbf{Z}_{f})^{2}}\left(\sigma_{x}^{2}+f_{\max}^{2}\right) +\frac{C\sigma_{x}^{2}k\log(NT/L)}{L}\] (27)

This completes the proof of Theorem C.1. Finally, now we are ready to bound \(\mathsf{EstErr}(N,T)\). First, let \(\Omega_{n}=\{\frac{(n-1)T}{L}+1,\ldots,\frac{nT}{L}\}\) be the set of columns in the stacked Page matrix \(\mathbf{Z}_{f}\) that belongs to the \(n\)-th time series \(f_{n}(t)\). Note that

\[\frac{1}{T}\sum_{t=1}^{T}(\widehat{f}_{n}(t)-f_{n}(t))^{2} =\frac{1}{T}\sum_{j\in\Omega_{n}}\Big{\lVert}[\widehat{\mathbf{Z} }_{f}]_{\cdot j}-[\mathbf{Z}_{f}]_{\cdot j}\Big{\rVert}_{2}^{2}\] \[\leq\frac{1}{T}\sum_{j\in\Omega_{n}}\frac{C\sigma_{x}^{2}(NT)^{2 }}{L^{2}\sigma_{k}(\mathbf{Z}_{f})^{2}}\left(\sigma_{x}^{2}+f_{\max}^{2}\right) +C\sigma_{x}^{2}k\log(NT/L)\] \[\leq\frac{C\sigma_{x}^{2}(NT)^{2}}{L^{3}\sigma_{k}(\mathbf{Z}_{f} )^{2}}\left(\sigma_{x}^{2}+f_{\max}^{2}\right)+\frac{C\sigma_{x}^{2}k\log(NT/L) }{L}.\]

Choosing \(L=\sqrt{NT}\), and using \(\sigma_{k}(\mathbf{Z}_{f})^{2}\geq\frac{\gamma^{2}NT}{k}\), we get,

\[\frac{1}{T}\sum_{t=1}^{T}(\widehat{f}_{n}(t)-f_{n}(t))^{2}\leq\frac{Cf_{\max}^ {2}\gamma^{2}\sigma_{x}^{4}k\log(NT)}{\sqrt{NT}}.\]

Finally and setting \(k=RG\) concludes the proof.

Proof of Theorem 4.2

In this section, we prove Theorem 4.2, which bounds the estimation error \(\left\|\widehat{\alpha}_{n}-\alpha_{n}\right\|_{2}\forall n\in[N]\). First, recall the OLS estimate \(\widehat{\alpha}_{n}=[\widehat{\alpha}_{n,1},\widehat{\alpha}_{n,2},\ldots \widehat{\alpha}_{n,p}]\) defined as

\[\widehat{\alpha}_{n}=\operatorname*{argmin}_{\alpha\in\mathbb{R}^{p}}\quad \sum_{t=p}^{T-1}(\widehat{x}_{n}(t+1)-\alpha^{\top}\widehat{X}_{n}(t))^{2},\] (28)

where \(\widehat{X}_{n}(t)\coloneqq[\widehat{x}_{n}(t),\ldots,\widehat{x}_{n}(t-p+1)]\). Alternatively, the OLS estimate can be defined as,

\[\widehat{\alpha}_{n}=(\widetilde{\boldsymbol{X}}_{n}^{\top}\widetilde{ \boldsymbol{X}}_{n})^{-1}\widetilde{\boldsymbol{X}}_{n}^{\top}\widehat{Y}_{n},\] (29)

where \(\widehat{\boldsymbol{X}}_{n}\in\mathbb{R}^{(T-p)\times p}\) is the row-wise concatenation of \(\widehat{X}_{n}(t)\) for \(t\in\{p,p+1,\ldots,T-1\}\), and \(\widehat{Y}_{n}=[\widehat{x}_{n}(p+1),\ldots,\widehat{x}_{n}(T)]\). Further, consider the OLS estimate with access to the true \(\operatorname{AR}(p)\) series \(x_{n}(\cdot)\). Precisely, let \(\bar{\alpha}_{n}\) defined as

\[\bar{\alpha}_{n}=(\boldsymbol{X}_{n}^{\top}\boldsymbol{X}_{n})^{-1} \boldsymbol{X}_{n}^{\top}Y_{n},\] (30)

where \(\boldsymbol{X}_{n}\in\mathbb{R}^{(T-p)\times p}\) is the row-wise concatenation of \(X_{n}(t)\coloneqq[x_{n}(t),\ldots,x_{n}(t-p+1)]\) for \(t\in\{p,p+1,\ldots,T-1\}\), and \(Y_{n}=[x_{n}(p+1),\ldots,x_{n}(T)]\).

In what follows, as we do the analysis for one process \(x_{n}\), we drop the subscript \(n\). We note that the analysis holds for all \(n\in[N]\). First, note that,

\[\left\|\widehat{\alpha}-\alpha\right\|_{2}^{2}\leq 2\left\|\widehat{\alpha}- \bar{\alpha}\right\|_{2}^{2}+2\left\|\bar{\alpha}-\alpha\right\|_{2}^{2}\] (31)

To bound \(\left\|\bar{\alpha}-\alpha\right\|_{2}^{2}\), we use Corollary E.2 which states that with probability \(1-\frac{c}{T^{\top 1}}\),

\[\left\|\bar{\alpha}-\alpha\right\|_{2}^{2}\leq C\frac{p^{2}}{T\lambda_{\min}( \Psi)}\log\left(\frac{T\lambda_{\max}(\Psi)}{\lambda_{\min}(\Psi)}\right).\] (32)

Next, to bound \(\left\|\widehat{\alpha}-\bar{\alpha}\right\|_{2}^{2}\), let \(\boldsymbol{M}^{\dagger}\coloneqq(\boldsymbol{M}^{\top}\boldsymbol{M})^{-1} \boldsymbol{M}^{\top}\) denote the Moore-Penrose inverse of a matrix \(\boldsymbol{M}\). Further, let \(\delta(t)=x(t)-\widehat{x}(t)=\widehat{f}(t)-f(t)\), and \(\boldsymbol{\Delta}=\boldsymbol{X}-\widetilde{\boldsymbol{X}}\), i.e., \(\boldsymbol{\Delta}\) is the row-wise concatenation of \(\Delta(t)\coloneqq[\delta(t),\ldots,\delta(t-p+1)]^{\top}\) for \(t\in\{p,p+1,\ldots,T-1\}\). Then, with \(\Delta_{Y}\coloneqq[\delta(p+1),\ldots,\delta(T)]\),we have

\[\left\|\widehat{\alpha}-\bar{\alpha}\right\|_{2} =\left\|\widehat{\boldsymbol{X}}^{\dagger}\widehat{Y}-\boldsymbol {X}^{\dagger}Y\right\|_{2}\] \[=\left\|\widehat{\boldsymbol{X}}^{\dagger}\Delta_{Y}+(\boldsymbol {X}^{\dagger}-\widehat{\boldsymbol{X}}^{\dagger})Y\right\|_{2}\] \[\leq\left\|\widehat{\boldsymbol{X}}^{\dagger}\Delta_{Y}\right\|_{ 2}+\left\|(\boldsymbol{X}^{\dagger}-\widehat{\boldsymbol{X}}^{\dagger})Y \right\|_{2}\] \[\leq\left\|\widehat{\boldsymbol{X}}^{\dagger}\right\|_{2}\left\| \Delta_{Y}\right\|_{2}+2\,\max\left\{\left\|\boldsymbol{X}^{\dagger}\right\|_ {2}^{2},\left\|\widehat{\boldsymbol{X}}^{\dagger}\right\|_{2}^{2}\right\} \left\|\boldsymbol{\Delta}\right\|_{2}\left\|Y\right\|_{2}\] (33)

Where Lemma H.6 is used in the last inequality. Note that,

\[\left\|\boldsymbol{X}^{\dagger}\right\|_{2}^{-1}=\inf_{v\in\mathbb{R}^{p}: \left\|v\right\|_{2}=1}\left\|\boldsymbol{X}v\right\|_{2}=\sqrt{\lambda_{\min}( \boldsymbol{X}^{\top}\boldsymbol{X})}\] (34)

\[\left\|\widehat{\boldsymbol{X}}^{\dagger}\right\|_{2}^{-1} =\inf_{v\in\mathbb{R}^{p}:\left\|v\right\|_{2}=1}\left\| \widehat{\boldsymbol{X}}v\right\|_{2}\] \[=\inf_{v\in\mathbb{R}^{p}:\left\|v\right\|_{2}=1}\left\|( \boldsymbol{X}-\boldsymbol{\Delta})v\right\|_{2}\] \[\geq\sqrt{\lambda_{\min}(\boldsymbol{X}^{\top}\boldsymbol{X})}- \left\|\boldsymbol{\Delta}\right\|_{2}\] (35)Using (33), (34), (35), and the theorem conditions, we get

\[\left\|\widehat{\alpha}-\bar{\alpha}\right\|_{2}\leq\frac{\left\| \Delta_{Y}\right\|_{2}}{\sqrt{\lambda_{\min}(\bm{X}^{\top}\bm{X})-\left\|\bm{ \Delta}\right\|_{2}}}+\frac{2\left\|\bm{\Delta}\right\|_{2}\left\|Y\right\|_{2 }}{\min\left\{\lambda_{\min}(\bm{X}^{\top}\bm{X}),\left(\sqrt{\lambda_{\min}( \bm{X}^{\top}\bm{X})}-\left\|\bm{\Delta}\right\|_{2}\right)^{2}\right\}}.\] (36)

Using Corollary E.1 we get that with probability \(1-\frac{c}{T^{\mathrm{T}\mathrm{T}}}\)

\[\lambda_{\min}(\bm{X}^{\top}\bm{X})\geq\frac{1}{2}\sigma^{2}(T-p) \lambda_{\min}(\Psi).\] (37)

Now, note that \(\left\|\Delta_{Y}\right\|_{2}^{2}=\sum_{t=p+1}^{T}(\widehat{f}(t)-f(t))^{2}\), and

\[\left\|\bm{\Delta}\right\|_{2}^{2}\leq\left\|\bm{\Delta}\right\|_ {F}^{2}\leq p\sum_{t=1}^{T}(\widehat{f}(t)-f(t))^{2}.\] (38)

Recall that \(\sum_{t=1}^{T}(\widehat{f}(t)-f(t))^{2}=T\mathsf{EstErr}(N,T,n)\), which we will refer to herein as \(\mathsf{EstErr}(N,T)\) as we drop the dependence on \(n\). Therefore, we have,

\[\left\|\bm{\Delta}\right\|_{2}^{2} \leq pT\mathsf{EstErr}(N,T)\] (39) \[\left\|\Delta_{Y}\right\|_{2}^{2} \leq T\mathsf{EstErr}(N,T).\] (40)

Finally, using Lemma H.4, with probability at least \(1-\frac{c}{T^{\mathrm{T}}}\) it holds for an absolute constant \(C>4\),

\[\left\|Y\right\|_{2} \leq C\sigma_{x}\sqrt{T\log(T)}.\] (41)

First, note that, using (37) and (39), we have

\[\sqrt{\lambda_{\min}(\bm{X}^{\top}\bm{X})}-\left\|\bm{\Delta} \right\|_{2} \geq\frac{\sigma\sqrt{(T-p)\lambda_{\min}(\Psi)}}{\sqrt{2}}-\sqrt {pT\mathsf{EstErr}(N,T)}\] \[\geq C\sigma\sqrt{T\lambda_{\min}(\Psi)},\] (42)

for sufficiently small \(\mathsf{EstErr}(N,T)\) such that \(\mathsf{EstErr}(N,T)\leq\frac{\sigma^{2}\lambda_{\min}(\Psi)}{6p}\). Now using (36), (37), (39), (40), (41), and (42) we have,

\[\left\|\widehat{\alpha}-\bar{\alpha}\right\|_{2} \leq\frac{C\sigma_{x}\sqrt{p\mathsf{EstErr}(N,T)\log(T)}}{\min \{\sigma^{2}\lambda_{\min}(\Psi),\sigma\sqrt{\lambda_{\min}(\Psi)}\}}\]

Therefore,

\[\left\|\widehat{\alpha}-\bar{\alpha}\right\|_{2}^{2} \leq C\frac{p\log(T)}{\lambda_{\min}(\Psi)}\frac{\sigma_{x}^{2}}{ \sigma^{2}}\mathsf{EstErr}(N,T)\max\left\{1,\frac{1}{\sigma\lambda_{\min}( \Psi)}\right\}\] (43)

finally, using (31), (32), and (43), we get with probability \(1-\frac{c}{T^{\mathrm{T}\mathrm{T}}}\),

\[\left\|\widehat{\alpha}-\alpha\right\|_{2}^{2} \leq C\frac{p\log(T)}{\lambda_{\min}(\Psi)}\frac{\sigma_{x}^{2}}{ \sigma^{2}}\mathsf{EstErr}(N,T)\max\left\{1,\frac{1}{\sigma^{2}\lambda_{\min }(\Psi)}\right\}+\frac{Cp^{2}}{T\lambda_{\min}(\Psi)}\log\left(\frac{T\lambda_ {\max}(\Psi)}{\lambda_{\min}(\Psi)}\right),\] (44)

which completes the proof of Theorem 4.2.

Identification of AR processes

In this section, we state key theorems for the identification of AR processes without the perturbation we consider in this paper. The theorems in this section and their proofs follow closely the ones in [17], we modify them to accommodate general sub-gaussian noise and state them fully for completeness.

First, consider the stationary \(\mathrm{AR}(p)\) process defined by the transition matrix

\[\bm{A}=\begin{bmatrix}\alpha_{1:p-1}^{\top}&\alpha_{p}\\ I_{p-1}&0_{p-1}\end{bmatrix},\] (45)

where \(0_{p-1}\) is the column vector of \(p-1\) zeroes, and input vector

\[B=\begin{bmatrix}1\\ 0_{p-1}\end{bmatrix}.\] (46)

Then, with \(X(t)\coloneqq[x(t),\dots,x(t-p+1)]\), the AR process is described as

\[X(t)=\bm{A}X(t-1)+B\eta_{t}.\] (47)

With \(\eta_{t}\) being a sub-gaussian random variable with variance proxy \(\sigma^{2}\) With this setup, we define two important matrices,

\[\Gamma =\sum_{t=0}^{\infty}\bm{A}^{t}(\bm{A}^{\top})^{t}\] (48) \[\Psi =\sum_{t=0}^{\infty}\bm{A}^{t}BB^{\top}(\bm{A}^{\top})^{t},\] (49)

where \(\Psi\) is known as the _controllability Granian_.

**Theorem E.1**.: _Consider the \(\mathrm{AR}(p)\) process described in (47), \(\{\eta_{t}\}\) is an i.i.d. mean-zero sub-gaussian noise with variance \(\sigma^{2}\). Given \(0<\epsilon\leq 1\), define the following quantities:_

\[V_{\ell} :=\sigma^{2}(T-p)\left(\Psi-\epsilon\Gamma\right)\] \[V_{u} :=\sigma^{2}(T-p)\left(\Psi+\epsilon\Gamma\right)\] \[\delta(\epsilon,T) :=c\exp\left(-\tilde{c}\frac{\epsilon\sqrt{T}}{p}\right)\] \[T_{0}(\epsilon) \coloneqq\frac{75^{2}\sigma_{x}^{4}p^{2}\log(p)^{2}}{\sigma^{4} \epsilon^{2}}\max\{1,\|\alpha\|_{2}^{4}\}\]

_where \(c\) and \(\tilde{c}\) are constants that depends only on the process coefficients \(\alpha_{1},\dots,\alpha_{p}\). Then, for \(T>T_{0}(\epsilon)\) and all valid values of \(\epsilon\) such that \(V_{\ell}\succ 0\), we have_

\[\mathbb{P}\left(V_{\ell}\preceq\mathbf{X}^{\top}\bm{X}\preceq V_{u}\right) \geq 1-\delta(\epsilon,T).\]

**Remark E.1**.: _To get \(V_{\ell}\succ 0\), one must choose a sufficiently small \(\epsilon\) such that \((\Psi-\epsilon\Gamma)\succ 0\). Using Weyl's inequality, we have_

\[\lambda_{\min}(\Psi-\epsilon\Gamma)\geq\lambda_{\min}(\Psi)-\epsilon\lambda_ {\max}(\Gamma).\] (50)

_This suggest that \(\epsilon<\frac{\lambda_{\min}(\Psi)}{\lambda_{\max}(\Gamma)}\) yields \(V_{\ell}\succ 0\). For example, a choice of \(\epsilon=\frac{\lambda_{\min}(\Psi)}{2\lambda_{\max}(\Gamma)}\) will yield \(V_{\ell}\succeq\frac{1}{2}\sigma^{2}(T-p)\lambda_{\min}(\Psi)I_{p}\). Note that this ensures \(\epsilon\leq 1/2\), since \(\epsilon\leq\frac{\lambda_{\max}(\Psi)}{\lambda_{\max}(\Gamma)}\) and \(\Gamma\succeq\Psi\) as can be seen from_

\[z_{i}^{\top}(\Gamma-\Psi)z =\sum_{t=0}^{\infty}z_{i}^{\top}A^{t}(A^{\top})^{t}z-z_{i}^{\top} A^{t}BB^{\top}(A^{\top})^{t}z\] \[=\sum_{t=0}^{\infty}\left\|(A^{\top})^{t}z\right\|_{2}^{2}-\left(B ^{\top}(A^{\top})^{t}z\right)^{2}\] \[\geq\sum_{t=0}^{\infty}\left\|(A^{\top})^{t}z\right\|_{2}^{2}- \underbrace{\|B\|_{2}^{2}}_{1}\left\|(A^{\top})^{t}z\right\|_{2}^{2}\] \[\geq 0.\]Proof.: First, write

\[\mathbf{X}^{\top}\bm{X}=\sum_{t=p}^{T-1}X_{t}X_{t}^{\top}.\]

Expanding the summand using the process dynamics yields

\[X_{t+1}X_{t+1}^{\top} =(AX_{t}+B\eta_{t+1})(AX_{t}+B\eta_{t+1})^{\top}\] \[=AX_{t}X_{t}^{\top}A^{\top}+\eta_{t+1}AX_{t}B^{\top}+\eta_{t+1}BX _{t}^{\top}A^{\top}+\eta_{t+1}^{2}BB^{\top}.\]

Now, defining \(V_{T}=\mathbf{X}^{\top}\bm{X}/(T-p)\), we can sum the expression above over \(t=p-1,\ldots,T-2\) to get

\[V_{T}=AV_{T}A^{\top} +\underbrace{\frac{1}{T-p}\left(A(X_{p-1}X_{p-1}^{\top}-X_{T-1}X_ {T-1}^{\top})A^{\top}+\sum_{t=p-1}^{T-2}(\eta_{t+1}AX_{t}B^{\top}+\eta_{t+1} BX_{t}^{\top}A^{\top}+\eta_{t+1}^{2}BB^{\top})\right)}_{E_{T}}.\]

This is called a Lyapunov equation, and due to the stability of the process it has solution \(V_{T}=\sum_{t=0}^{\infty}A^{t}E_{T}(A^{\top})^{t}\). The key step is to argue that \(E_{T}\) converges to \(\sigma^{2}BB^{\top}\) with high probability, which will allow us to show that \(V_{T}\) converges to \(\sigma^{2}\Psi\). To do this, we will split the terms in \(E_{T}\) into three groups, handling them separately. For some \(\epsilon>0\), we will bound the probability of the following events:

\[\mathcal{E}_{1} \coloneqq\left\{\rho\left(A(X_{p-1}X_{p-1}^{\top}-X_{T-1}X_{T-1}^ {\top})A^{\top}\right)\leq\epsilon\sigma^{2}(T-p)/3\right\},\] \[\mathcal{E}_{2} \coloneqq\left\{\rho\left(\sum_{t=p-1}^{T-2}\left(\eta_{t+1}^{2} BB^{\top}-\sigma^{2}BB^{\top}\right)\right)\leq\epsilon\sigma^{2}(T-p)/3\right\},\] \[\mathcal{E}_{3} \coloneqq\left\{\rho\left(\sum_{t=p-1}^{T-2}\eta_{t+1}AX_{t}B^{ \top}+\eta_{t+1}BX_{t}^{\top}A^{\top}\right)\leq\epsilon\sigma^{2}(T-p)/3 \right\},\]

where \(\rho\left(\cdot\right)\) denotes the spectral radius of a matrix.

**Lemma E.1**.: _With the setup above, it holds that_

\[\mathbb{P}\left(\mathcal{E}_{1}\right)\geq 1-c\exp\left(-\frac{ \epsilon\sigma^{2}\sqrt{T}}{3\sigma_{x}^{2}p(\left\lVert\alpha\right\rVert_{ 2}^{2}+1)}\right)\]

_for some absolute constant \(c>0\)._

Proof.: Note that

\[\rho\left(A(X_{p-1}X_{p-1}^{\top}-X_{T-1}X_{T-1}^{\top})A^{\top} \right)\leq\epsilon\sigma^{2}(T-p)/3\] \[\iff\rho\left(A(X_{p-1}X_{p-1}^{\top}+X_{T-1}X_{T-1}^{\top})A^{ \top}\right)\leq\epsilon\sigma^{2}(T-p)/3\] \[\iff\left\lVert AX_{p-1}\right\rVert_{2}^{2}+\left\lVert AX_{T-1 }\right\rVert_{2}^{2}\leq\epsilon\sigma^{2}(T-p)/3\] \[\iff\left\lVert X_{p-1}\right\rVert_{2}^{2}+\left\lVert X_{T-1} \right\rVert_{2}^{2}\leq\frac{\epsilon\sigma^{2}(T-p)}{3\left\lVert A\right\rVert _{2}}\] \[\iff\left\lVert X_{p-1}\right\rVert_{2}^{2}\vee\left\lVert X_{T-1 }\right\rVert_{2}^{2}\leq\frac{\epsilon\sigma^{2}(T-p)}{6\left\lVert A\right\rVert _{2}},\]

so, considering the fact that \(X_{p-1}\) and \(X_{T-1}\) are identically distributed by stationarity, it suffices to upper bound the probability of

\[\mathcal{E}^{\prime}=\left\{\left\lVert X_{p-1}\right\rVert_{2}^{2}>\frac{ \epsilon\sigma^{2}(T-p)}{6\left\lVert A\right\rVert_{2}}\right\}.\]

First, recall that \(X_{p-1}\) is a sub-gaussian vector with variance proxy \(\frac{\sigma^{2}\gamma^{2}}{(1-\lambda^{*})^{2}}\coloneqq\sigma_{x}^{2}\) (see Lemma A.2). Thus, using Lemma H.2, for \[\mathbb{P}\left(\left\|X_{p-1}\right\|_{2}^{2}\geq\frac{\epsilon \sigma^{2}(T-p)}{6\left\|A\right\|_{2}}\right) \leq c\exp\left(-\frac{\epsilon\sigma^{2}(T-p)}{96\sigma_{2}^{2}p \left\|A\right\|_{2}}\right)\] \[\leq c\exp\left(-\frac{\epsilon\sigma^{2}\sqrt{T}}{3\sigma_{2}^{2 }\left\|A\right\|_{2}}\right),\]

where in the last inequality, we used the assumption \(T>T_{0}(\epsilon)>75^{2}p^{2}\). Finally, note that \(\left\|A\right\|_{2}\leq\mathrm{Tr}(A)=\left\|\alpha\right\|_{2}^{2}+p-1\leq p (\left\|\alpha\right\|_{2}^{2}+1).\) Recalling that \(\mathbb{P}\left(\mathcal{E}_{1}\right)>1-2\mathbb{P}\left(\mathcal{E}^{\prime}\right)\) concludes the proof. 

**Lemma E.2**.: _Continuing with the setup above, it holds that_

\[\mathbb{P}\left(\mathcal{E}_{2}\right)\geq 1-c\exp\bigg{(}\frac{-\epsilon \sqrt{T-p}}{48}\bigg{)}.\]

Proof.: Since the spectral radius of \(BB^{\top}\) is unity, \(\mathcal{E}_{2}\) is equivalent to the complement of the event

\[\mathcal{E}^{\prime}=\left\{\left|(T-p)^{-1}\sum_{t=p-1}^{T-2}\left(\eta_{t+1 }^{2}-\sigma^{2}\right)\right|>\epsilon\sigma^{2}/3\right\}.\]

Since \(\eta_{t+1}\) is sub-gaussian with variance proxy \(\sigma^{2}\), \(\eta_{t+1}^{2}-\sigma^{2}\) is sub-exponential with parameter \(16\sigma^{2}\). Thus, Lemma H.5 applies, and therefore,

\[\mathbb{P}\left(\mathcal{E}^{\prime}\right)\leq c\exp\bigg{(}\frac{-\epsilon \sqrt{T-p}}{48}\bigg{)}.\] (51)

**Lemma E.3**.: _With the same setup, we have_

\[\mathbb{P}\left(\mathcal{E}_{3}\right)\geq 1-c\exp\left(-\frac{\epsilon \sigma^{2}\sqrt{T}}{75p\sigma_{x}^{2}\left(2+\left\|\alpha\right\|_{2}^{2} \right)}\right)\]

_as long as_

\[T\geq T_{0}(\epsilon)\coloneqq\frac{75^{2}\sigma_{x}^{4}p^{2}\log(p)^{2}}{ \sigma^{4}\epsilon^{2}}\left(2+\left\|\alpha\right\|_{2}^{2}\right)^{2}.\]

Proof.: Recall the definition

\[\mathcal{E}_{3}\coloneqq\left\{\rho\left(\sum_{t=p-1}^{T-2}\eta_{t+1}AX_{t}B^ {\top}+\eta_{t+1}BX_{t}^{\top}A^{\top}\right)\leq\epsilon\sigma^{2}(T-p)/3 \right\}.\]

For the sake of a variational analysis of the spectral radius, consider any vector \(q=[q_{1}\;\tilde{q}^{\top}]^{\top}\in\mathbb{R}^{p}\) such that \(\left\|q\right\|_{2}=1\). For notational simplicity. Then

\[q^{\top}\underbrace{\left(\sum_{t=p-1}^{T-2}\eta_{t+1}AX_{t}B^{\top}+\eta_{t+ 1}BX_{t}^{\top}A^{\top}\right)}_{M+M^{\top}}q\leq 2\left\|Mq\right\|_{2}.\]

Next, we aim to bound \(\left\|Mq\right\|_{2}\). First note that only the first column of \(M\) is nonzero which evaluates to

\[M_{\cdot,1}=\eta_{t+1}\cdot\begin{bmatrix}\sum_{t=p-1}^{T-2}\alpha^{\top}X_{t }\\ \sum_{t=p-1}^{T-2}x_{t}\\ \vdots\\ \sum_{t=p-1}^{T-2}x_{t-p+1}\end{bmatrix}\] (52)

Therefore, we have \(Mq=q_{1}M_{\cdot,1}\), which implies that for any unit vector \(q\),

\[q^{\top}\big{(}M+M^{\top}\big{)}q\leq 2\sqrt{\sum_{i=1}^{p}M_{i,1}^{2}}.\]Therefore, we will bound \(\mathbb{P}\left(\mathcal{E}_{3}^{c}\right)\) by

\[\mathbb{P}\left(\mathcal{E}_{3}^{c}\right) \leq\mathbb{P}\left(4\sum_{i=1}^{p}M_{i,1}^{2}\geq\epsilon^{2} \sigma^{4}(T-p)^{2}/9\right)\] \[\leq\sum_{i=1}^{p}\mathbb{P}\left(\left|M_{i,1}\right|\geq\frac{ \epsilon\sigma^{2}(T-p)}{6\sqrt{p}}\right)\] (53)

First note that each term \(\mathbb{P}\left(\left|M_{i,1}\right|\geq\frac{\epsilon\sigma^{2}(T-p)}{6\sqrt {p}}\right)\) can be upper bounded by \(2\mathbb{P}\left(\sum_{t=p}^{T-1}z_{i}^{\top}X_{t-1}\eta_{t}\geq\frac{ \epsilon\sigma^{2}(T-p)}{6\sqrt{p}}\right)\) where \(z_{1}=\alpha\) and \(z_{i}=e_{i-1}\) for \(i>1\) where \(e_{i}\) is the \(i\)-th standard basis vector. Using Lemma H.7 and choosing \(Z_{t}=z_{i}^{\top}X_{t-1}\) and \(W_{t}=\eta_{t}\), we have that

\[\mathbb{P}\left(\left\{\sum_{t=p}^{T-1}\eta_{t}z_{i}^{\top}X_{t-1}\geq\kappa \right\}\cap\left\{\sum_{t=p}^{T-1}(z_{i}^{\top}X_{t-1})^{2}\leq\lambda\right\} \right)\leq\exp{\left(-\frac{\kappa^{2}}{2\sigma^{2}\lambda}\right)},\]

and thus

\[\mathbb{P}\left(\sum_{t=p}^{T-1}\eta_{t}z_{i}^{\top}X_{t-1}\geq \kappa\right) \leq\exp{\left(-\frac{\kappa^{2}}{2\sigma^{2}\lambda}\right)}+ \mathbb{P}\left(\sum_{t=p}^{T-1}(z_{i}^{\top}X_{t-1})^{2}>\lambda\right)\] \[\leq\exp{\left(-\frac{\kappa^{2}}{2\sigma^{2}\lambda}\right)}+ \mathbb{P}\left(\sum_{t=p}^{T-1}\left\|X_{t-1}\right\|_{2}^{2}>\lambda/\left\| z_{i}\right\|_{2}^{2}\right).\]

What is left is to bound \(\mathbb{P}\left(\sum_{t=0}^{T-2}x_{t}^{2}>\frac{\lambda}{p\left\|z\right\|_{2} ^{2}}\right).\) Using Lemma H.2, we get,

\[\mathbb{P}\left(\sum_{t=0}^{T-2}x_{t}^{2}>\frac{\lambda}{p\left\|z\right\|_{2} ^{2}}\right)\leq c\exp{\left(-\frac{\lambda}{16pT\sigma_{x}^{2}\left\|z \right\|_{2}^{2}}\right)}\] (54)

Choosing \(\kappa=\epsilon\sigma^{2}(T-p)/(6\sqrt{p})\) and \(\lambda=\left\|z\right\|_{2}^{2}\epsilon\sigma^{2}T^{3/2}\) yields

\[\mathbb{P}\left(\sum_{t=p}^{T-1}\eta_{t}z_{i}^{\top}X_{t-1}\geq\frac{ \epsilon\sigma^{2}(T-p)}{6\sqrt{p}}\right)\leq\exp{\left(-\frac{\epsilon(T-p) ^{2}}{72p\left\|z\right\|_{2}^{2}T^{3/2}}\right)}+c\exp{\left(-\frac{\epsilon \sigma^{2}\sqrt{T}}{16p\sigma_{x}^{2}}\right)}\,.\]

Which implies, that for \(T>50p\), we have,

\[\mathbb{P}\left(\sum_{t=p}^{T-1}\eta_{t}z_{i}^{\top}X_{t-1}\geq\frac{ \epsilon\sigma^{2}(T-p)}{6\sqrt{p}}\right)\leq c\exp{\left(-\frac{\epsilon \sqrt{T}}{p}\min{\left\{\frac{1}{75\left\|z\right\|_{2}^{2}},\frac{\sigma^{2}} {16\sigma_{x}^{2}}\right\}}\right)},\] (55)

for some absolute constants \(c,C>0\). Now, note that

\[\min{\left\{\frac{1}{75\left\|z\right\|_{2}^{2}},\frac{\sigma^{2} }{16\sigma_{x}^{2}}\right\}} \geq\frac{1}{75}\min{\left\{\frac{1}{\left\|z_{i}\right\|_{2}^{2 }},\frac{\sigma^{2}}{\sigma_{x}^{2}}\right\}}\] \[\geq\frac{1}{75}\frac{\sigma^{2}}{\sigma_{x}^{2}}\min{\left\{ \frac{1}{\left\|z_{i}\right\|_{2}^{2}},1\right\}}\] \[\geq\frac{1}{75}\frac{\sigma^{2}}{\sigma_{x}^{2}}\frac{1}{1+\left\| z_{i}\right\|_{2}^{2}}\] \[\geq\frac{1}{75}\frac{\sigma^{2}}{\sigma_{x}^{2}}\frac{1}{2+\left\| \alpha\right\|_{2}^{2}}.\]Thus, using (55) and (53), we have

\[\mathbb{P}\left(\mathcal{E}_{3}^{c}\right) \leq\sum_{i=1}^{p}\mathbb{P}\left(\left|M_{i,1}\right|\geq\frac{ \epsilon\sigma^{2}(T-p)}{6\sqrt{p}}\right)\] (56) \[\leq\sum_{i=1}^{p}2\mathbb{P}\left(\sum_{t=p}^{T-1}z_{i}^{\top}X_ {t-1}\eta_{t}\geq\frac{\epsilon\sigma^{2}(T-p)}{6\sqrt{p}}\right)\] (57) \[\leq cp\exp\left(-\frac{\epsilon\sigma^{2}\sqrt{T}}{75p\sigma_{x} ^{2}\left(2+\left\|\alpha\right\|_{2}^{2}\right)}\right).\] (58)

Using the condition \(T>T_{0}(\epsilon)\) concludes the proof. 

Now we argue that the sum of the probabilities deduced from lemmas (E.1), (E.2), and (E.3) can be expressed in the simple form of \(\delta(\epsilon,T)\) as defined above. First, from lemma (E.1) we have

\[\mathbb{P}\left(\mathcal{E}_{1}^{\prime}\right)\leq c\exp\left(-\frac{1}{3( \left\|\alpha\right\|_{2}^{2}+1)}\frac{\sigma^{2}}{\sigma_{x}^{2}}\frac{ \epsilon\sqrt{T}}{p}\right).\]

From lemma (E.2), and \(T>T_{0}(\epsilon)>50p\) we get

\[\mathbb{P}\left(\mathcal{E}_{2}^{\prime}\right)\leq c\exp\left(\frac{- \epsilon\sqrt{T-p}}{48}\right)\leq c\exp\left(\frac{-\epsilon\sqrt{T}}{49} \right).\]

Lastly, lemma (E.3) says, for \(T\geq T_{0}(\epsilon)\),

\[\mathbb{P}\left(\mathcal{E}_{3}^{\prime}\right)\leq c\exp\left(-\frac{ \epsilon\sigma^{2}\sqrt{T}}{75p\sigma_{x}^{2}\left(2+\left\|\alpha\right\|_{2 }^{2}\right)}\right)\]

Thus we can write

\[\mathbb{P}\left(\mathcal{E}_{1}^{\prime}\cup\mathcal{E}_{2}^{\prime}\cup \mathcal{E}_{3}^{\prime}\right)\leq c\exp\left(-\frac{1}{75\left(2+\left\| \alpha\right\|_{2}^{2}\right)}\frac{\epsilon\sigma^{2}\sqrt{T}}{p\sigma_{x}^{ 2}}\right)\coloneqq\delta(\epsilon,T).\]

Where in the statement we set \(\tilde{c}\coloneqq\frac{1}{75\left(2+\left\|\alpha\right\|_{2}^{2}\right)} \frac{\sigma^{2}}{\sigma_{x}^{2}}\). Using lemmas (E.1), (E.2), and (E.3) and the subadditivity of the spectral radii of symmetric matrices, we get with probability at least \(1-\delta(\epsilon,T)\) that

\[\mathcal{E}_{1}\cap\mathcal{E}_{2}\cap\mathcal{E}_{3} \implies\rho\left(E_{T}-\sigma^{2}BB^{\top}\right)\leq\epsilon \sigma^{2}\] \[\implies\sigma^{2}(BB^{\top}-\epsilon I)\preceq E_{T}\preceq \sigma^{2}(BB^{\top}+\epsilon I)\] \[\implies\sigma^{2}(T-p)(\Psi-\epsilon\Gamma)\preceq\mathbf{X}^{ \top}\mathbf{X}\preceq\sigma^{2}(T-p)(\Psi+\epsilon\Gamma).\]

Finally, we establish the following corollary, which is an immediate consequence of Theorem E.1.

**Corollary E.1**.: _Consider the \(\mathrm{AR}(p)\) process \(x_{t}\) described in (A.1). Let \(\bm{X}\in\mathbb{R}^{(T-p)\times p}\) such that \([\bm{X}]_{ij}=x_{i+j-1}\). Then, for sufficiently large \(T\) such that \(\frac{\log(T)}{\sqrt{T}}<\frac{\tilde{c}}{22p}\left(\frac{\lambda_{\min}(\Psi)} {\lambda_{\max}(\Gamma)}\right)\), we have that with probability \(1-\frac{\tilde{c}}{T^{11}}\),_

\[\lambda_{\min}(\bm{X}^{\top}\bm{X}) \geq\frac{1}{2}\sigma^{2}(T-p)\lambda_{\min}(\Psi),\] (59) \[\lambda_{\max}(\bm{X}^{\top}\bm{X}) \leq\frac{3}{2}\sigma^{2}(T-p)\lambda_{\max}(\Psi),\] (60)

_where \(c,\,C\) are constants and \(\tilde{c}\coloneqq\frac{1}{75\left(2+\left\|\alpha\right\|_{2}^{2}\right)} \frac{\sigma^{2}}{\sigma_{x}^{2}},\,\psi\) (the controllability Gramian) and \(\Gamma\) are defined in (14)._Proof.: Note that according to Theorem E.1, Weyl's inequality (see Lemma H.3), and using \(\epsilon=\frac{11p\log(T)}{\tilde{\varepsilon}\sqrt{T}}\) we have, with probability \(1-\frac{c}{T^{D}}\),

\[\lambda_{\min}(\boldsymbol{X}^{\top}\boldsymbol{X}) \geq\sigma^{2}(T-p)\left(\lambda_{\min}(\Psi)-\epsilon\lambda_{ \max}(\Gamma)\right)\] (61) \[\geq\sigma^{2}(T-p)\left(\lambda_{\min}(\Psi)-11\frac{p\log(T)}{ \tilde{\varepsilon}\sqrt{T}}\lambda_{\max}(\Gamma)\right)\] (62) \[\geq\frac{1}{2}\sigma^{2}(T-p)\left(\lambda_{\min}(\Psi)\right).\] (63)

Also,

\[\lambda_{\max}(\boldsymbol{X}^{\top}\boldsymbol{X}) \leq\sigma^{2}(T-p)\left(\lambda_{\min}(\Psi)+\epsilon\lambda_{ \max}(\Gamma)\right)\] (64) \[\leq\frac{3}{2}\sigma^{2}(T-p)\left(\lambda_{\min}(\Psi)\right).\] (65)

What is left is to check the condition on \(T\) and \(\epsilon\). Note that Theorem E.1 requires \(T>\frac{75^{2}\sigma_{x}^{2}p^{2}\log(p)^{2}}{\sigma^{4}\epsilon^{2}}\max\{1, \left\|\alpha\right\|_{2}^{4}\}\), which with our choice of \(\epsilon\) implies \(T>p\). To see that the choice of \(\epsilon\) is valid, note that \(\frac{11p\log(T)}{\tilde{\varepsilon}\sqrt{T}}\leq\frac{\lambda_{\min}(\Psi)} {2\lambda_{\max}(\Gamma)}\), which validates the choice of \(\epsilon\).

**Theorem E.2**.: _Consider the \(\mathrm{AR}(p)\) process described in (47). where \(\{\eta_{t}\}\) is an i.i.d. mean-zero sub-gaussian noise with variance \(\sigma^{2}\). Let \(\alpha\coloneqq[\alpha_{1},\ldots,\alpha_{p}]^{\top}\) and \(\bar{\alpha}\) be its least squares estimator. If \(T\geq T_{0}(\epsilon)\), then for any unit vector \(w\in\mathcal{S}_{p}\),_

\[\mathbb{P}\left(\left|w^{\top}\left(\bar{\alpha}-\alpha\right)\right|>2\sigma \left\|w^{\top}V_{\ell}^{-1/2}\right\|_{2}\sqrt{\log\left(\frac{\det\left(V_{ u}V_{\ell}^{-1}+I_{p}\right)^{1/2}}{\delta(\epsilon,T)}\right)}\right)\leq 2\delta( \epsilon,T),\]

_where \(V_{\ell},V_{u}\)\(T_{0}(\epsilon)\), and \(\delta(\epsilon,T)\) are as described in Theorem E.1._

Proof.: Write \(E=[\eta_{p+1},\ldots,\eta_{T}]^{\top}\). Using Cauchy-Schwarz, we have

\[\left|w^{\top}(\hat{\alpha}(T)-\alpha)\right| =\left|w^{\top}(\mathbf{X}^{\top}\mathbf{X})^{-1}\mathbf{X}^{ \top}E\right|\] \[\leq\left\|w^{\top}(\mathbf{X}^{\top}\mathbf{X})^{-1/2}\right\|_ {2}\left\|(\mathbf{X}^{\top}\mathbf{X})^{-1/2}\mathbf{X}^{\top}E\right\|_{2}.\]

Now we will need to introduce a lemma from [1]:

**Lemma E.4**.: _Let \(\{\mathcal{F}_{t}\}_{t=0}^{\infty}\) be a filtration. Let \(\{\eta_{t}\}_{t=1}^{\infty}\) be a real-valued stochastic process such that \(\eta_{t}\) is \(\mathcal{F}_{t}\)-measurable and is conditionally sub-gaussian with variance proxy \(\sigma^{2}>0\). Further, let \(\{X_{t}\}_{t=0}^{\infty}\) be an \(\mathbb{R}^{d}\)-valued stochastic process such that \(X_{t}\) is \(\mathcal{F}_{t-1}\)-measurable. Consider any positive definite matrix \(V\in\mathbb{R}^{d\times d}\). For any \(t\geq 1\), define_

\[\bar{V}_{t}=V+\sum_{s=1}^{t}X_{s}X_{s}^{\top},\quad S_{t}=\sum_{s=1}^{t}\eta_{s }X_{s}.\]

_Then for any \(\delta>0\), with probability at least \(1-\delta\) for all \(t\geq 1\),_

\[\|S_{t}\|_{\bar{V}_{t}^{-1}}^{2}\leq 2\sigma^{2}\left(\frac{\det\left(\bar{V}_{ t}\right)^{1/2}\det\left(V\right)^{-1/2}}{\delta}\right).\]

We will apply this result to bound the second term above. Choosing \(V=V_{\ell}\), \(\sum_{s=1}^{t}X_{s}X_{s}^{\top}=\mathbf{X}^{\top}\mathbf{X}\), and \(S_{t}=\mathbf{X}^{\top}E\), we have with probability at least \(1-\delta(\epsilon,T)\)

\[\|\mathbf{X}^{\top}E\|_{(\mathbf{X}^{\top}\mathbf{X}+V_{\ell})^{-1}}\leq\sqrt {2\sigma^{2}\log\left(\frac{\det\left(\mathbf{X}^{\top}\mathbf{X}+V_{\ell} \right)^{1/2}\det\left(V_{\ell}\right)^{-1/2}}{\delta(\epsilon,T)}\right)}.\]

Now further condition on the event of Theorem E.1, which occur together with the condition above with probability at least \(1-2\delta(\epsilon,T)\). Recall, then, that \(\mathbf{X}^{\top}\mathbf{X}+V_{\ell}\preceq 2\mathbf{X}^{\top}\mathbf{X}\), and so \((\mathbf{X}^{\top}\mathbf{X}+V_{\ell})^{-1}\succeq\frac{1}{2}(\mathbf{X}^{ \top}\mathbf{X})^{-1}\). This means that

\[\left\|(\mathbf{X}^{\top}\mathbf{X})^{-1/2}\mathbf{X}^{\top}E\right\| _{2} =\|\mathbf{X}^{\top}E\|_{(\mathbf{X}^{\top}\mathbf{X})^{-1}}\] \[\leq\sqrt{2}\|\mathbf{X}^{\top}E\|_{(\mathbf{X}^{\top}\mathbf{X} +V_{\ell})^{-1}}\] \[\leq 2\sigma\sqrt{\log\left(\frac{\det\left(\mathbf{X}^{\top} \mathbf{X}+V_{\ell}\right)^{1/2}\det\left(V_{\ell}\right)^{-1/2}}{\delta( \epsilon,T)}\right)}\] \[=2\sigma\sqrt{\log\left(\frac{\det\left(\mathbf{X}^{\top} \mathbf{X}V_{\ell}^{-1}+I_{p}\right)^{1/2}}{\delta(\epsilon,T)}\right)}\] \[\leq 2\sigma\sqrt{\log\left(\frac{\det\left(V_{u}V_{\ell}^{-1}+I_ {p}\right)^{1/2}}{\delta(\epsilon,T)}\right)}.\]

Finally, it is clear that under the conditions in Theorem E.1 we have \(\left\|w^{\top}(\mathbf{X}^{\top}\mathbf{X})^{-1/2}\right\|_{2}\leq\left\|w^{ \top}V_{\ell}^{-1/2}\right\|_{2}\).

**Corollary E.2**.: _Let \(\epsilon=12\frac{p\log(T)}{\tilde{c}\lor T}\), where \(\tilde{c}\) is as defined in Theorem (E.1). Then, for \(T\) such that \(\frac{\log(T)}{\sqrt{T}}<\frac{\tilde{c}}{24p}\left(\frac{\lambda_{\min}( \Psi)}{\lambda_{\max}(\Gamma)}\right)\) we have_

\[\mathbb{P}\left(\left\|\bar{\alpha}-\alpha\right\|_{2}>Cp\sqrt{\frac{1}{T \lambda_{\min}(\Psi)}\log\left(\frac{T\lambda_{\max}(\Psi)}{\lambda_{\min}( \Psi)}\right)}\right)\leq\frac{c}{T^{11}},\]

Proof.: First, note that with the choice of \(\epsilon=12\frac{p\log(T)}{\tilde{c}\lor T}\) we have,

\[\delta(\epsilon,T) =c\exp(-\log(T^{12}))\] \[=\frac{c}{T^{12}}\] (66)

Hence, \(-\log(\delta(\epsilon,T))=C\log(T)\). Further,

\[\left\|w^{\top}V_{\ell}^{-1/2}\right\|_{2} \leq\left\|V_{\ell}^{-1/2}\right\|_{2}\] \[=\sqrt{\lambda_{\max}(V_{\ell}^{-1})}\] \[=\frac{1}{\sqrt{\lambda_{\min}(V_{\ell})}}\] \[\leq\frac{1}{\sigma\sqrt{(T-p)\left(\lambda_{\min}(\Psi)- \epsilon\lambda_{\max}(\Gamma)\right)}}\] \[\leq\frac{2}{\sigma\sqrt{(T-p)\lambda_{\min}(\Psi)}},\] (67)

where the last inequality holds with probability \(1-\delta(\epsilon,T)\) using Theorem E.1. Finally, consider

\[\log\left(\det\left(V_{u}V_{\ell}^{-1}+I_{p}\right)^{1/2}\right) =\frac{1}{2}\log\left(\frac{\det\left(V_{u}+V_{\ell}\right)}{\det\left(V_{ \ell}\right)}\right)\] \[=\frac{1}{2}\log\left(\frac{2^{p}\sigma^{2p}(T-p)^{p}\det\left( \Psi\right)}{\det\left(V_{\ell}\right)}\right)\] (68)

Note that, again by our assumed condition on \(\epsilon\),

\[\det\left(V_{\ell}\right) \geq\frac{1}{2}\det\left(\Psi\right)\] \[\geq\frac{1}{2}\sigma^{2p}(T-p)^{p}\lambda_{\min}(\Psi)^{p}\] (69)

Using (69) and (68), we have,

\[\log\left(\det\left(V_{u}V_{\ell}^{-1}+I_{p}\right)^{1/2}\right) \leq\frac{1}{2}\log\left(\frac{2^{p+1}\det\left(\Psi\right)}{ \lambda_{\min}(\Psi)^{p}}\right)\] \[\leq p\log\left(\frac{4\lambda_{\max}(\Psi)}{\lambda_{\min}(\Psi)}\right)\] (70)

From (70), (67) and (66), we have,

\[2\sigma\left\|w^{\top}V_{\ell}^{-1/2}\right\|_{2}\sqrt{\log\left( \frac{\det\left(V_{u}V_{\ell}^{-1}+I_{p}\right)^{1/2}}{\delta(\epsilon,T)} \right)}\] \[\leq\sqrt{\frac{Cp}{(T-p)\lambda_{\min}(\Psi)}}\sqrt{\log\left( \frac{4\lambda_{\max}(\Psi)}{\lambda_{\min}(\Psi)}\right)+\log(T)}.\] (71)Using (71) and Theorem E.2, we get that with probability \(1-\frac{2c}{T^{12}}\)

\[\left|\bar{\alpha}_{i}(T)-\alpha_{i}\right|\leq\sqrt{\frac{Cp}{(T-p)\lambda_{ \min}(\Psi)}}\sqrt{\log\left(\frac{4\lambda_{\max}(\Psi)}{\lambda_{\min}(\Psi)} \right)+\log(T)},\] (72)

For any \(i\in[p]\). This implies,

\[\left\|\bar{\alpha}-\alpha\right\|_{2}\leq\sqrt{\sum_{i=1}^{p}|\hat{\alpha}_{ i}(T)-\alpha_{i}|^{2}}\leq\sqrt{\frac{Cp^{2}}{(T-p)\lambda_{\min}(\Psi)}} \sqrt{\log\left(\frac{4\lambda_{\max}(\Psi)}{\lambda_{\min}(\Psi)}\right)+ \log(T)},\] (73)

with probability \(1-\frac{c}{T^{11}}\) which completes the proof.

Proof of Theorem 4.3

### Setup

Recall \(\beta^{*}\) as defined in Proposition 2.2, and its estimate \(\widehat{\beta}\) defined as

\[\widehat{\beta}=\operatorname*{argmin}_{\beta\in\mathbb{R}^{L-1}}\quad\sum_{m=1 }^{N\times T/L}(y_{m}-\beta^{\top}\widehat{F}_{m})^{2}.\] (74)

To establish this bound, we first define (and recall) the following notations,

* Recall that \(\mathbf{Z}_{y}=[\mathbf{Z}(y_{1},L,T,1)\quad\mathbf{Z}(y_{2},L,T,1)\quad\dots \quad\mathbf{Z}(y_{n},L,T,1)]\) is the stacked Page matrix of \(y_{1}(t),\dots,y_{n}(t)\ \forall t\in[T]\).
* Let \(\mathbf{Z}_{x}\) be the stacked Page matrix of \(x_{1}(t),\dots,x_{n}(t)\ \forall t\in[T]\).
* Let \(\mathbf{Z}_{f}\) be the stacked Page matrix of \(f_{1}(t),\dots,f_{n}(t)\ \forall t\in[T]\). That is, \(\mathbf{Z}_{y}=\mathbf{Z}_{f}+\mathbf{Z}_{x}\).
* Let \(\mathbf{Z}^{\prime}_{y}\in\mathbb{R}^{(L-1)\times(NT/L)}\), be the sub-matrix obtained by dropping the \(L\)-th row from the stacked Page matrix \(\mathbf{Z}_{y}\). Define \(\mathbf{Z}^{\prime}_{f}\) and \(\mathbf{Z}^{\prime}_{x}\) analogously.
* Let \(\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}\) denote the SVD of \(\mathbf{Z}^{\prime}_{f}\).
* let \(\mathbf{V}^{\perp}\) and \(\mathbf{U}^{\perp}\) be matrices of orthonormal basis vectors that span the null space of \(\mathbf{Z}^{\prime}_{f}\) and \(\mathbf{Z}^{\prime}_{f}{}^{\top}\), respectively.
* Let \(\widetilde{\mathbf{U}}\widetilde{\mathbf{\Sigma}}\widetilde{\mathbf{V}}^{\top}\) denote the top k singular components of the SVD of \(\mathbf{Z}^{\prime}_{y}\), while \(\widetilde{\mathbf{U}}^{\perp}\widetilde{\mathbf{\Sigma}}^{\perp}(\widetilde{ \mathbf{V}}^{\perp})^{\top}\) denote the remaining \(L-k-1\) components such that \(\mathbf{Z}^{\prime}_{y}=\widetilde{\mathbf{U}}\widetilde{\mathbf{\Sigma}} \widetilde{\mathbf{V}}^{\top}+\widetilde{\mathbf{U}}^{\perp}\widetilde{ \mathbf{\Sigma}}^{\perp}(\widetilde{\mathbf{V}}^{\perp})^{\top}\).
* Let \(\widetilde{\mathbf{Z}}^{\prime}_{f}\) be the HSVT estimate of \(\mathbf{Z}^{\prime}_{f}\) with parameter \(k\). That is \(\widehat{\mathbf{Z}}^{\prime}_{f}=\widetilde{\mathbf{U}}\widetilde{\mathbf{ \Sigma}}\widetilde{\mathbf{V}}^{\top}\).

### Deterministic Bound

First, given the notation above, the solution for (74), can be written as,

\[\widehat{\beta}=\left(\widehat{\mathbf{Z}}^{\prime}_{f}\right)^{\top,\dagger }[\mathbf{Z}_{y}]_{L}.=\widetilde{\mathbf{U}}\widetilde{\mathbf{\Sigma}}^{-1} \widetilde{\mathbf{V}}^{\top}[\mathbf{Z}_{y}]_{L}.\] (75)

Then, let's consider the following expansion for \(\|\widehat{\beta}-\beta^{*}\|_{2}\),

\[\|\widehat{\beta}-\beta^{*}\|_{2}^{2} =\|\widetilde{\mathbf{U}}^{\perp}(\widetilde{\mathbf{U}}^{\perp} )^{\top}(\widehat{\beta}-\beta^{*})+\widetilde{\mathbf{U}}(\widetilde{ \mathbf{U}})^{\top}(\widehat{\beta}-\beta^{*})\|_{2}^{2}\] \[=\|\widetilde{\mathbf{U}}^{\perp}(\widetilde{\mathbf{U}}^{\perp} )^{\top}(\widehat{\beta}-\beta^{*})\|_{2}^{2}+\|\widetilde{\mathbf{U}}( \widetilde{\mathbf{U}})^{\top}(\widehat{\beta}-\beta^{*})\|_{2}^{2}\] \[=\|\widetilde{\mathbf{U}}^{\perp}(\widetilde{\mathbf{U}}^{\perp} )^{\top}(\widehat{\beta}-\beta^{*})\|_{2}^{2}+\|\widetilde{\mathbf{U}}^{\top} (\widehat{\beta}-\beta^{*})\|_{2}^{2}\] \[=\|\widetilde{\mathbf{U}}^{\perp}(\widetilde{\mathbf{U}}^{\perp} )^{\top}\beta^{*}\|_{2}^{2}+\|\widetilde{\mathbf{U}}^{\top}(\widehat{\beta}- \beta^{*})\|_{2}^{2},\] (76)

where the last equality follow from the fact that \((\widetilde{\mathbf{U}}^{\perp})^{\top}\widehat{\beta}=\left((\widetilde{ \mathbf{U}}^{\perp})^{\top}\widetilde{\mathbf{U}}\right)\widetilde{\mathbf{ \Sigma}}^{-1}\widetilde{\mathbf{V}}^{\top}[\mathbf{Z}_{y}]_{L}=0\). Now, consider the first term in (76), and note that \(\beta^{*}=\mathbf{U}\mathbf{U}^{\top}\beta^{*}\),

\[\|\widetilde{\mathbf{U}}^{\perp}(\widetilde{\mathbf{U}}^{\perp} )^{\top}\beta^{*}\|_{2} =\left\|\mathbf{U}^{\perp}(\mathbf{U}^{\perp})^{\top}\mathbf{U} \mathbf{U}^{\top}\beta^{*}+\left(\widetilde{\mathbf{U}}^{\perp}(\widetilde{ \mathbf{U}}^{\perp})^{\top}-\mathbf{U}^{\perp}(\mathbf{U}^{\perp})^{\top} \right)\beta^{*}\right\|_{2}\] \[\leq\left\|\left(\widetilde{\mathbf{U}}^{\perp}(\widetilde{ \mathbf{U}}^{\perp})^{\top}-\mathbf{U}^{\perp}(\mathbf{U}^{\perp})^{\top} \right)\beta^{*}\right\|_{2}\] \[\leq\left\|\widetilde{\mathbf{U}}^{\perp}(\widetilde{\mathbf{U}}^{ \perp})^{\top}-\mathbf{U}^{\perp}(\mathbf{U}^{\perp})^{\top}\right\|_{2}\| \beta^{*}\|_{2}\] \[=\left\|\widetilde{\mathbf{U}}\widetilde{\mathbf{U}}^{\top}- \mathbf{U}\mathbf{U}^{\top}\right\|_{2}\left\|\beta^{*}\right\|_{2}.\] (77)Next, by Wedin \(\sin\Theta\) Theorem (see [10, 32]) we have:

\[\left\|\widetilde{\mathbf{U}}\widetilde{\mathbf{U}}^{\top}- \mathbf{U}\mathbf{U}^{\top}\right\|_{2}\left\|\beta^{*}\right\|_{2} \leq\frac{\|\mathbf{Z}_{f}^{\prime}-\mathbf{Z}_{f}^{\prime}\|_{2}}{ \sigma_{k}(\mathbf{Z}_{f}^{\prime})}\left\|\beta^{*}\right\|_{2}\] \[=\frac{\|\mathbf{Z}_{f}^{\prime}\|_{2}}{\sigma_{k}(\mathbf{Z}_{f }^{\prime})}\left\|\beta^{*}\right\|_{2}.\] (78)

That is,

\[\|\widetilde{\mathbf{U}}^{\perp}(\widetilde{\mathbf{U}}^{\perp}) ^{\top}\beta^{*}\|_{2} \leq\frac{\|\mathbf{Z}_{x}^{\prime}\|_{2}}{\sigma_{k}(\mathbf{Z}_{f }^{\prime})}\left\|\beta^{*}\right\|_{2}.\] (79)

What is left is bounding \(\|\widetilde{\mathbf{U}}^{\top}(\widehat{\beta}-\beta^{*})\|_{2}^{2}\). To that end, first consider

\[\|(\widehat{\mathbf{Z}}_{f}^{\prime})^{\top}(\widehat{\beta}- \beta^{*})\|_{2}^{2} \leq 2\|(\widehat{\mathbf{Z}}_{f}^{\prime})^{\top}\widehat{\beta}- (\mathbf{Z}_{f}^{\prime})^{\top}\beta^{*}\|_{2}^{2}+2\|(\mathbf{Z}_{f}^{ \prime})^{\top}\beta^{*}-(\widehat{\mathbf{Z}}_{f}^{\prime})^{\top}\beta^{*} \|_{2}^{2}\] \[\leq 2\|(\widehat{\mathbf{Z}}_{f}^{\prime})^{\top}\widehat{\beta}- (\mathbf{Z}_{f}^{\prime})^{\top}\beta^{*}\|_{2}^{2}+2\|\mathbf{Z}_{f}^{\prime} -\widehat{\mathbf{Z}}_{f}^{\prime}\|_{2,\infty}^{2}\|\beta^{*}\|_{1}^{2}.\] (80)

Also, consider

\[\|(\widehat{\mathbf{Z}}_{f}^{\prime})^{\top}(\widehat{\beta}- \beta^{*})\|_{2}^{2} =(\widehat{\beta}-\beta^{*})^{\top}\widetilde{\mathbf{U}} \widetilde{\mathbf{\Sigma}}^{2}\widetilde{\mathbf{U}}^{\top}(\widehat{\beta}- \beta^{*})\] \[\geq\sigma_{k}(\widehat{\mathbf{Z}}_{f}^{\prime})^{2}\|\widetilde {\mathbf{U}}^{\top}(\widehat{\beta}-\beta^{*})\|_{2}^{2}.\] (81)

From (81) and (80) we get,

\[\|\widetilde{\mathbf{U}}^{\top}(\widehat{\beta}-\beta^{*})\|_{2}^{2} \leq\frac{2}{\sigma_{k}(\widehat{\mathbf{Z}}_{f}^{\prime})^{2}}\left(\|( \widehat{\mathbf{Z}}_{f}^{\prime})^{\top}\widehat{\beta}-(\mathbf{Z}_{f}^{ \prime})^{\top}\beta^{*}\|_{2}^{2}+\|\mathbf{Z}_{f}^{\prime}-\widehat{\mathbf{ Z}}_{f}^{\prime}\|_{2,\infty}^{2}\|\beta^{*}\|_{1}^{2}\right).\] (82)

First, recall that \([\mathbf{Z}_{y}]_{L}:+[\mathbf{Z}_{x}]_{L}\), and that by definition, \((\mathbf{Z}_{f}^{\prime})^{\top}\beta^{*}=[\mathbf{Z}_{f}]_{L}\). Then the term \(\|(\widehat{\mathbf{Z}}_{f}^{\prime})^{\top}\widehat{\beta}-(\mathbf{Z}_{f}^ {\prime})^{\top}\beta^{*}\|_{2}^{2}\) can be bounded as follows. First, consider

\[\|(\widehat{\mathbf{Z}}_{f}^{\prime})^{\top}\widehat{\beta}-( \mathbf{Z}_{f}^{\prime})^{\top}\beta^{*}\|_{2}^{2}\] \[=\|(\widehat{\mathbf{Z}}_{f}^{\prime})^{\top}\widehat{\beta}-[ \mathbf{Z}_{y}]_{L}\|_{L}^{2}-\|[\mathbf{Z}_{x}]_{L}\|_{2}^{2}+2\left(( \widehat{\mathbf{Z}}_{f}^{\prime})^{\top}\widehat{\beta}-(\mathbf{Z}_{f}^{ \prime})^{\top}\beta^{*}\right)^{\top}[\mathbf{Z}_{x}]_{L}.\] \[\leq\|(\widehat{\mathbf{Z}}_{f}^{\prime})^{\top}\beta^{*}-[ \mathbf{Z}_{y}]_{L}\|_{2}^{2}-\|[\mathbf{Z}_{x}]_{L}\|_{2}^{2}+2\left(( \widehat{\mathbf{Z}}_{f}^{\prime})^{\top}\widehat{\beta}-(\mathbf{Z}_{f}^{ \prime})^{\top}\beta^{*}\right)^{\top}[\mathbf{Z}_{x}]_{L}.\] \[=\|(\widehat{\mathbf{Z}}_{f}^{\prime})^{\top}\beta^{*}-[\mathbf{ Z}_{x}]_{L}\cdot[-\mathbf{Z}_{f}]_{L}\|_{2}^{2}-\|[\mathbf{Z}_{x}]_{L}\|_{2}^{2}+2 \left((\widehat{\mathbf{Z}}_{f}^{\prime})^{\top}\widehat{\beta}-(\mathbf{Z}_{f} ^{\prime})^{\top}\beta^{*}\right)^{\top}[\mathbf{Z}_{x}]_{L}.\] \[=\left\|\left(\widehat{\mathbf{Z}}_{f}^{\prime}-\mathbf{Z}_{f}^{ \prime}\right)^{\top}\beta^{*}-[\mathbf{Z}_{x}]_{L}\right\|_{2}^{2}-\|[ \mathbf{Z}_{x}]_{L}\|_{2}^{2}+2\left((\widehat{\mathbf{Z}}_{f}^{\prime})^{\top} \widehat{\beta}-(\mathbf{Z}_{f}^{\prime})^{\top}\beta^{*}\right)^{\top}[ \mathbf{Z}_{x}]_{L}.\] \[=\left\|\left(\widehat{\mathbf{Z}}_{f}^{\prime}-\mathbf{Z}_{f}^{ \prime}\right)^{\top}\beta^{*}\right\|_{2}^{2}+2\left((\widehat{\mathbf{Z}}_{f }^{\prime})^{\top}\widehat{\beta}-(\widehat{\mathbf{Z}}_{f}^{\prime})^{\top}\beta^{ *}\right)^{\top}[\mathbf{Z}_{x}]_{L}.\] (83)

Note that \((\widehat{\mathbf{Z}}_{f}^{\prime})^{\top}\widehat{\beta}=\widetilde{\mathbf{V}} \widetilde{\mathbf{\Sigma}}\widetilde{\mathbf{U}}^{\top}\widehat{\beta}= \widetilde{\mathbf{V}}\widetilde{\mathbf{\Sigma}}\widetilde{\mathbf{U}}^{\top} \widetilde{\mathbf{U}}\widetilde{\mathbf{\Sigma}}^{\top}\widetilde{\mathbf{V}}^{ \top}[\mathbf{Z}_{y}]_{L}\). \(=\widetilde{\mathbf{V}}\widetilde{\mathbf{V}}^{\top}[\mathbf{Z}_{y}]_{L}\)., and \((\widehat{\mathbf{Z}}_{f}^{\prime})^{\top}\beta^{*}=\widetilde{\mathbf{V}} \widetilde{\mathbf{\Sigma}}\widetilde{\mathbf{U}}^{\top}\widetilde{\mathbf{V}}^{ \top}\), thus,

\[\left((\widehat{\mathbf{Z}}_{f}^{\prime})^{\top}\widehat{\beta}-( \widehat{\mathbf{Z}}_{f}^{\prime})^{\top}\beta^{*}\right)^{\top}[\mathbf{Z}_{x}]_{L}. =[\mathbf{Z}_{y}]_{L}^{\top}\widetilde{\mathbf{V}}\widetilde{ \mathbf{V}}^{\top}[\mathbf{Z}_{x}]_{L}.-\beta^{*}{}^{\top}\widetilde{\mathbf{U}} \widetilde{\mathbf{\Sigma}}\widetilde{\mathbf{V}}^{\top}[\mathbf{Z}_{x}]_{L}.\] \[=\left([\mathbf{Z}_{y}]_{L}^{\top}\widetilde{\mathbf{V}}-\beta^{* }{}^{\top}\widetilde{\mathbf{U}}\widetilde{\mathbf{\Sigma}}\right)\widetilde{ \mathbf{V}}^{\top}[\mathbf{Z}_{x}]_{L}.\] \[=\left([\mathbf{Z}_{f}]_{L}^{\top}\widetilde{\mathbf{V}}+[ \mathbf{Z}_{x}]_{L}^{\top}\widetilde{\mathbf{V}}-\beta^{*}{}^{\top}\widetilde{ \mathbf{U}}\widetilde{\mathbf{\Sigma}}\right)\widetilde{\mathbf{V}}^{\top}[ \mathbf{Z}_{x}]_{L}.\] \[=\left(\beta^{*Finally, from (82) and (83) we get

\[\|\widetilde{\mathbf{U}}^{\top}(\widehat{\beta}-\beta^{*})\|_{2}^{2} \leq\frac{2}{\sigma_{k}(\widehat{\mathbf{Z}}_{f}^{\prime})^{2}} \Bigg{(}2\|\mathbf{Z}_{f}^{\prime}-\widehat{\mathbf{Z}}_{f}^{\prime}\|_{2, \infty}^{2}\|\beta^{*}\|_{1}^{2}\] \[+\|\mathbf{Z}_{f}^{\prime}-\widehat{\mathbf{Z}}_{f}^{\prime}\|_{2, \infty}\|\beta^{*}\|_{1}\left\|[\mathbf{Z}_{x}]_{L}^{\top}\widetilde{\mathbf{ V}}\right\|_{2}+\left\|[\mathbf{Z}_{x}]_{L}^{\top}\widetilde{\mathbf{V}}\right\|_{2}^{ 2}\Bigg{)}\] \[\leq\frac{4}{\sigma_{k}(\widehat{\mathbf{Z}}_{f}^{\prime})^{2}} \Bigg{(}2\|\mathbf{Z}_{f}^{\prime}-\widehat{\mathbf{Z}}_{f}^{\prime}\|_{2, \infty}^{2}\|\beta^{*}\|_{1}^{2}+\left\|[\mathbf{Z}_{x}]_{L}^{\top}\widetilde {\mathbf{V}}\right\|_{2}^{2}\Bigg{)}.\]

The term \(\left\|[\mathbf{Z}_{x}]_{L}^{\top}\widetilde{\mathbf{V}}\right\|_{2}^{2}\) can be further decomposed as

\[\left\|[\mathbf{Z}_{x}]_{L}^{\top}\widetilde{\mathbf{V}}\right\|_{2}^{2}\leq 2 \left\|[\mathbf{Z}_{x}]_{L}^{\top}.(\widehat{\mathbf{V}}\widetilde{\mathbf{V}} ^{\top}-\mathbf{V}\mathbf{V}^{\top})\right\|_{2}^{2}+2\left\|[\mathbf{Z}_{x}] _{L}^{\top}\mathbf{V}\mathbf{V}^{\top}\right\|_{2}^{2}\]

Using Wedin \(\sin\Theta\) Theorem,

\[\left\|\widetilde{\mathbf{V}}\widetilde{\mathbf{V}}^{\top}-\mathbf{V}\mathbf{ V}^{\top}\right\|_{2}\leq\frac{\|\mathbf{Z}_{x}^{\prime}\|_{2}}{\sigma_{k}( \mathbf{Z}_{f}^{\prime})}\]

Therefore,

\[\|\widetilde{\mathbf{U}}^{\top}(\widehat{\beta}-\beta^{*})\|_{2}^{2}\leq\frac{ 8}{\sigma_{k}(\widehat{\mathbf{Z}}_{f}^{\prime})^{2}}\Bigg{(}\|\mathbf{Z}_{f} ^{\prime}-\widehat{\mathbf{Z}}_{f}^{\prime}\|_{2,\infty}^{2}\|\beta^{*}\|_{1 }^{2}+\frac{\|\mathbf{Z}_{x}^{\prime}\|_{2}^{2}}{\sigma_{k}(\mathbf{Z}_{f}^{ \prime})^{2}}\left\|[\mathbf{Z}_{x}]_{L}.\right\|_{2}^{2}+\left\|[\mathbf{Z}_ {x}]_{L}^{\top}\mathbf{V}\right\|_{2}^{2}\Bigg{)}.\] (85)

From (76), (79), and (85) we have

\[\|\widehat{\beta}-\beta^{*}\|_{2}^{2} \leq\frac{\|\mathbf{Z}_{x}^{\prime}\|_{2}^{2}}{\sigma_{k}( \mathbf{Z}_{f}^{\prime})^{2}}\left(\|\beta^{*}\|_{2}^{2}+8\frac{\|[\mathbf{Z}_ {x}]_{L}.\|_{2}^{2}}{\sigma_{k}(\widehat{\mathbf{Z}}_{f}^{\prime})^{2}}\right)\] \[+\frac{8}{\sigma_{k}(\widehat{\mathbf{Z}}_{f}^{\prime})^{2}} \Bigg{(}2\|\mathbf{Z}_{f}^{\prime}-\widehat{\mathbf{Z}}_{f}^{\prime}\|_{2, \infty}^{2}\|\beta^{*}\|_{1}^{2}+\left\|[\mathbf{Z}_{x}]_{L}^{\top}\mathbf{V }\right\|_{2}^{2}\Bigg{)}.\] (86)

### High probability bound

Let \(q\coloneqq NT/L\) be the number of columns in the stacked page matrix \(\widehat{\mathbf{Z}}_{f}^{\prime}\), and let \(C>0\) be some positive absolute constant. Define

\[E_{1}^{\prime} :=\Big{\{}\|\mathbf{Z}_{x}^{\prime}\|_{2}\leq C\sigma_{x}\sqrt{q} \Big{\}},\] \[E_{2}^{\prime} :=\Big{\{}\left\|[\mathbf{Z}_{x}]_{L}.\right\|_{2}\leq C\sigma_{x} \sqrt{q}\Big{\}},\] \[E_{3}^{\prime} :=\Big{\{}\|\mathbf{Z}_{f}^{\prime}-\widehat{\mathbf{Z}}_{f}^{ \prime}\|_{2,\infty}^{2}\leq\frac{C\sigma_{x}^{2}q^{2}}{\sigma_{k}(\mathbf{Z}_ {f})^{2}}\left(\sigma_{x}^{2}+f_{\max}^{2}\right)+C\sigma_{x}^{2}k\log(q) \Big{\}}\] \[E_{4}^{\prime} :=\Big{\{}\left\|[\mathbf{Z}_{x}]_{L}^{\top}\mathbf{V}\right\|_{2 }\leq C\sigma_{x}\sqrt{k\log(q)}\Big{\}}.\]

**Lemma F.1**.: _For some positive constant \(c_{1}>0\) and \(C>0\) large enough in definitions of \(E_{1}^{\prime},E_{2}^{\prime},E_{3}^{\prime}\) and \(E_{4}^{\prime}\),_

\[\mathbb{P}(E_{1}^{\prime}) \geq 1-2e^{-c_{1}q},\] \[\mathbb{P}(E_{2}^{\prime}) \geq 1-2e^{-c_{1}q},\] \[\mathbb{P}(E_{3}^{\prime}) \geq 1-\frac{c_{1}}{(NT)^{11}},\] \[\mathbb{P}(E_{4}^{\prime}) \geq 1-\frac{c_{1}}{(NT)^{11}}.\]Proof.: We bound the probability of events above below.

**Bounding \(\bm{E}_{1}^{\prime}\) and \(\bm{E}_{2}^{\prime}\).** Note that \(\mathbf{Z}_{x}^{\prime}\) is a sub-matrix of \(\mathbf{Z}_{x}\), and its operator norm is bounded by that of \(\mathbf{Z}_{x}\). Hence, as the probability is an immediate consequence of Lemma A.2. The second event probability follow from the fact that \(\left\|[\mathbf{Z}_{x}]_{L}\right\|_{2}\leq\left\|\mathbf{Z}_{x}\right\|_{2}\).

**Bounding \(\bm{E}_{3}^{\prime}\).** This is immediate from Corollary C.1 and its consequence in (27).

**Bounding \(\bm{E}_{4}^{\prime}\).** First, recall that \([\mathbf{Z}_{x}]_{L_{\cdot}}\) is a \(q\)-dimensional sub-gaussian vector with variance proxy \(\sigma_{x}^{2}\). That is, for any unit vector \(\mathbf{v}\in\mathcal{S}^{q-1},[\mathbf{Z}_{x}]_{L_{\cdot}}\mathbf{v}\sim \mathrm{subG}(\sigma_{x}^{2})\). Now, recall that \(\mathbf{V}\in\mathbb{R}^{q\times k}\) is a set of \(k\) orthonormal vectors \(\mathbf{V}_{i},i\in[k]\). Let \(Z_{i}=[\mathbf{Z}_{x}]_{L_{\cdot}}\mathbf{V}_{i}\). Clearly, \(Z_{i}\sim\mathrm{subG}(\sigma_{x}^{2})\). Therefore, \([\mathbf{Z}_{x}]_{L_{\cdot}}\mathbf{V}\) is a \(k\)-dimensional vector with dependent sub-gaussian entries with variance proxy \(\sigma_{x}^{2}\). Using Lemma H.4, we have

\[\mathbb{P}\Big{(}\|[\mathbf{Z}_{x}]_{L_{\cdot}}\mathbf{V}\|_{2}>t\Big{)}\leq c \exp\left(-\frac{t^{2}}{16k\sigma_{x}^{2}}\right).\]

Therefore, for choice of \(t=C\sigma_{x}\sqrt{k\log(q)}\) with large enough constant \(C\geq 19\), we have,

\[\mathbb{P}\Big{(}\|[\mathbf{Z}_{x}]_{L_{\cdot}}\mathbf{V}\|_{2}>C\sigma_{x} \sqrt{k\log(q)}\Big{)}\leq\frac{c}{q^{22}}.\]

Recalling that \(q>L\), and \(q=NT/L\), concludes the proof. 

Let \(E^{\prime}:=E_{1}^{\prime}\cap E_{2}^{\prime}\cap E_{3}^{\prime}\cap E_{4}^{\prime}\). Then, under event \(E\), and using (86), we have, with probability \(1-\frac{c}{(NT)^{11}}\),

\[\|\widehat{\beta}-\beta^{*}\|_{2}^{2} \leq\frac{C\sigma_{x}^{2}}{\sigma_{k}(\mathbf{Z}_{f}^{\prime})^{ 2}}\Bigg{(}q\left\|\beta^{*}\right\|_{2}^{2}+\frac{q}{\sigma_{k}(\mathbf{Z}_{ f}^{\prime})^{2}}\Bigg{)}\] \[+\frac{C\sigma_{x}^{2}}{\sigma_{k}(\mathbf{Z}_{f}^{\prime})^{2}} \Bigg{(}\bigg{(}\frac{q^{2}}{\sigma_{k}(\mathbf{Z}_{f})^{2}}\left(\sigma_{x}^{ 2}+f_{\max}^{2}\right)+k\log(q)\bigg{)}\max\{\|\beta^{*}\|_{1}^{2},1\}\Bigg{)}.\]

Choosing \(L=\sqrt{NT}\), and \(k=RG\), recalling Assumption 4.1, and using Weyl's lemma (H.3) to bound \(|\sigma_{k}(\mathbf{\widehat{Z}}_{f}^{\prime})^{2}-\sigma_{k}(\mathbf{Z}_{f} ^{\prime})^{2}|\) we get,

\[\|\widehat{\beta}-\beta^{*}\|_{2}^{2} \leq\frac{C\gamma^{2}\sigma_{x}^{2}RG}{NT}\Bigg{(}\sqrt{NT}\left\| \beta^{*}\right\|_{2}^{2}+RG\log\left(NT\right)\] \[+\bigg{(}\gamma^{2}RG\left(\sigma_{x}^{2}+f_{\max}^{2}\right)+RG \log(NT)\bigg{)}\|\beta^{*}\|_{1}^{2}\Bigg{)}\] \[\leq\frac{C\gamma^{4}f_{\max}^{2}\sigma_{x}^{4}R^{2}G^{2}\log(NT )}{\sqrt{NT}}\max\{\|\beta^{*}\|_{1}^{2},1\},\]

which concludes the proof of Theorem 4.3.

Proof of Theorem 4.4

In this section, we prove an upper bound on the forecasting error,

\[\mathsf{ForErr}(N,T)=\frac{1}{NT}\sum_{n=1}^{N}\sum_{t=T+1}^{2T}(\widehat{y}_{n}(t )-\bar{y}_{n}(t))^{2},\]

where \(\bar{y}_{n}(t)=\mathbb{E}\left[y_{n}(t)|y_{n}(1),\ldots,y_{n}(t-1)\right]=f_{n} (t)+\bar{x}_{n}(t)\), where \(\bar{x}_{n}(t):=\mathbb{E}\left[x_{n}(t)|x_{n}(1),\ldots,x_{n}(t-1)\right]\). First, recall that for \(t>T\),

\[\widehat{y}_{n}(t)=\widehat{f}_{n}(t)+\widehat{x}_{n}(t)=\widehat{\beta}^{ \top}Y_{n}(t-1)+\widehat{\alpha}_{n}^{\top}\widetilde{X}_{n}(t-1),\]

where \(Y_{n}(t-1)=[y_{n}(t-(L-1)),\ldots,y_{n}(t-1)]\), and \(\widetilde{X}_{n}(t-1)=[\widetilde{x}_{n}(t-p),\ldots,\widetilde{x}_{n}(t-1)]\). Then, we have,

\[\frac{1}{NT}\sum_{n=1}^{N}\sum_{t=T+1}^{2T}(\widehat{y}_{n}(t)-y_{n}(t))^{2} \leq\frac{2}{NT}\sum_{n=1}^{N}\sum_{t=T+1}^{2T}(\widehat{f}_{n}(t)-f_{n}(t))^ {2}+(\widehat{x}_{n}(t)-\bar{x}_{n}(t))^{2}.\]

Now, we will bound each error term separately.

### Bounding Forecasting Error of \(f\)

To bound the forecasting error of \(f\), we use the following Lemma.

**Lemma G.1**.: _Let the conditions of Theorem 4.1 hold. Then, with probability \(1-\frac{c}{(NT)^{11}}\)_

\[\frac{1}{NT}\sum_{n=1}^{N}\sum_{t=T+1}^{2T}\left(\widehat{f}_{n}(t)-f_{n}(t) \right)^{2}\leq C(f_{\max},\gamma)R^{3}G^{3}\sigma_{x}^{6}\left(\frac{\max\{ \|\beta^{*}\|_{1}^{2},1\}\log(NT)}{\sqrt{NT}}\right),\]

_where \(c\) is an absolute constant, and \(C(f_{\max},\gamma)\) denotes a constant that depends only (polynomially) on model parameters \(f_{\max},\gamma\)._

Proof.: The proof of this lemma is similar to that in [3], but we adapt it for our different settings. First, note that, where we assume that \(NT/L\) is an integer,

\[\frac{1}{NT}\sum_{n=1}^{N}\sum_{t=T+1}^{2T}\left(\widehat{f}_{n}(t)-f_{n}(t) \right)^{2}=\frac{1}{L}\sum_{\ell=1}^{L}\frac{L}{NT}\sum_{n=1}^{N}\sum_{m=0}^{ T/L-1}\left(\widehat{f}_{n}(T+mL+\ell)-f_{n}(T+mL+\ell)\right)^{2}.\]

In what follows, we provide an upper bound for the inner sum \(\frac{L}{NT}\sum_{n=1}^{N}\sum_{m=0}^{T/L-1}\left(\widehat{f}_{n}(T+mL+\ell)-f_ {n}(T+mL+\ell)\right)^{2}\) for all \(\ell\in[L]\). Next, we show, without loss of generality, the case for \(\ell=1\). To establish this bound, we first define (and recall) the following notations,

* Recall that \(\mathbf{Z}_{y}=[\mathbf{Z}(y_{1},L,T)\quad\mathbf{Z}(y_{2},L,T)\quad\ldots \quad\mathbf{Z}(y_{n},L,T)]\) is the stacked Page matrix of \(y_{1}(t),\ldots,y_{n}(t)\) for \(t\in[T]\).
* Similarly, let \(\mathbf{Z}_{y,out}\) denote the stacked Page matrix of the out-of-sample observations \(y_{1}(t),\ldots,y_{n}(t)\) for \(t\in\{T-L+2,\ldots,2T-L+1\}\), i.e., \(\mathbf{Z}_{y,out}\in\mathbb{R}^{L\times NT/L}\).
* Let \(\mathbf{Z}_{x}\) be the stacked Page matrix of \(x_{1}(t),\ldots,x_{n}(t)\) for \(t\in[T]\), and \(\mathbf{Z}_{x,out}\) be the stacked Page matrix of \(x_{1}(t),\ldots,x_{n}(t)\) for \(t\in\{T-L+2,\ldots,2T-L+1\}\).
* Similarly, let \(\mathbf{Z}_{f}\) and \(\mathbf{Z}_{f,out}\) be defined similarly but for \(f_{1}(t),\ldots,f_{n}(t)\).
* Let \(\mathbf{Z}^{\prime}_{y}\in\mathbb{R}^{(L-1)\times(NT/L)}\), be the a sub-matrix obtained by dropping the \(L\)-th row from the stacked Page matrix \(\mathbf{Z}_{y}\). Define \(\mathbf{Z}^{\prime}_{y,out}\), \(\mathbf{Z}^{\prime}_{f}\), \(\mathbf{Z}^{\prime}_{f,out}\), \(\mathbf{Z}^{\prime}_{x}\), and \(\mathbf{Z}^{\prime}_{x,out}\) analogously.

* Let \(\mathbf{U}\bm{\Sigma}\mathbf{V}^{\top}\) and \(\mathbf{U}_{out}\bm{\Sigma}_{out}\mathbf{V}_{out}{}^{\top}\) denote the SVD of \(\mathbf{Z}^{\prime}_{f}\) and \(\mathbf{Z}^{\prime}_{f,out}\) respectively.
* let \(\mathbf{V}^{\perp}\) and \(\mathbf{U}^{\perp}\) be matrices of orthonormal basis vectors that span the null space of \(\mathbf{Z}^{\prime}_{f}\) and \(\mathbf{Z}^{\prime}_{f}{}^{\top}\), respectively. Define \(\mathbf{V}_{out}{}^{\perp}\) and \(\mathbf{U}^{\perp}_{out}\) similarly for the matrix \(\mathbf{Z}^{\prime}_{f,out}\).
* Let \(\widetilde{\mathbf{U}}\widetilde{\bm{\Sigma}}\widetilde{\mathbf{V}}^{\top}\) denote the top \(k\) singular components of the SVD of \(\mathbf{Z}^{\prime}_{y}\), while \(\widetilde{\mathbf{U}}^{\perp}\widetilde{\bm{\Sigma}}^{\perp}(\widetilde{ \mathbf{V}}^{\perp})^{\top}\) denote the remaining \(L-k-1\) components such that \(\mathbf{Z}^{\prime}_{y}=\widetilde{\mathbf{U}}\widetilde{\bm{\Sigma}} \widetilde{\mathbf{V}}^{\top}+\widetilde{\mathbf{U}}^{\perp}\widetilde{\bm{ \Sigma}}^{\perp}(\widetilde{\mathbf{V}}^{\perp})^{\top}\).
* Similarly, Let \(\widetilde{\mathbf{U}}_{out}\widetilde{\bm{\Sigma}}_{out}\widetilde{\bm{V}}^{\top} _{out}\) denote the top k singular components of the SVD of \(\mathbf{Z}^{\prime}_{y,out}\), while \(\widetilde{\mathbf{U}}^{\perp}_{out}\widetilde{\bm{\Sigma}}^{\perp}_{out}( \widetilde{\mathbf{V}}^{\perp}_{out})^{\top}\) denote the remaining \(L-k-1\) components such that \(\mathbf{Z}^{\prime}_{y,out}=\widetilde{\mathbf{U}}_{out}\widetilde{\bm{\Sigma}} _{out}\widetilde{\mathbf{V}}^{\top}_{out}+\widetilde{\mathbf{U}}^{\perp}_{out} \widetilde{\bm{\Sigma}}^{\perp}_{out}(\widetilde{\mathbf{V}}^{\perp}_{out})^{\top}\).
* Let \(\widehat{\mathbf{Z}}_{f}=\widetilde{\mathbf{U}}\widetilde{\bm{\Sigma}} \widetilde{\mathbf{V}}^{\top}\) and \(\widehat{\mathbf{Z}}_{f,out}=\widetilde{\mathbf{U}}_{out}\widetilde{\bm{ \Sigma}}_{out}\widetilde{\mathbf{V}}^{\top}_{out}\).

Recall that we are interested in a high-probability bound for the following out-of-sample prediction error:

\[\frac{L}{NT}\sum_{n=1}^{N}\sum_{m=0}^{T/L-1}\bigg{(}\widehat{f}_{n}(T+mL+1)-f_{ n}(T+mL+1)\bigg{)}^{2}.\] (87)

Recall that \(\widehat{f}_{n}(t)=\widehat{\beta}^{\top}Y_{n}(t-1)\) and hence we can rewrite (87) as

\[\sum_{n=1}^{N}\sum_{m=0}^{T/L}\bigg{(}\widehat{f}_{n}(T+mL+1)-f_{ n}(T+mL+1)\bigg{)}^{2} =\left\|\mathbf{Z}^{\prime}_{y,out}{}^{\top}\widehat{\beta}-[ \mathbf{Z}_{f,out}]_{L}^{\top}\right\|_{2}^{2}\] (88) \[=\left\|\mathbf{Z}^{\prime}_{y,out}{}^{\top}\widehat{\beta}- \mathbf{Z}^{\prime}_{f,out}{}^{\top}\beta^{*}\right\|_{2}^{2}\] (89)

Next, we derive a deterministic upper found for the expression above.

**Deterministic Bound.** Through adding and subtracting \(\widehat{\mathbf{Z}}^{\top}_{f,out}\widehat{\beta}\) and triangle inequality, we have

\[\left\|(\mathbf{Z}^{\prime}_{y,out})^{\top}\widehat{\beta}-(\mathbf{Z}^{ \prime}_{f,out})^{\top}\beta^{*}\right\|_{2} \leq\left\|(\mathbf{Z}^{\prime}_{y,out}-\widehat{\mathbf{Z}}_{f,out})^{\top }\widehat{\beta}\right\|_{2}+\left\|\widehat{\mathbf{Z}}^{\top}_{f,out} \widehat{\beta}-(\mathbf{Z}^{\prime}_{f,out})^{\top}\beta^{*}\right\|_{2}\] (90)

Next, we proceed to bound each of the two terms on the right hand side.

_First term: \(\left\|(\mathbf{Z}^{\prime}_{y,out}-\widehat{\mathbf{Z}}_{f,out})^{\top} \widehat{\beta}\right\|_{2}\)._

\[\left\|(\mathbf{Z}^{\prime}_{y,out}-\widehat{\mathbf{Z}}_{f,out})^{ \top}\widehat{\beta}\right\|_{2}^{2} =\|\widetilde{\mathbf{V}}^{\perp}_{out}\widetilde{\bm{\Sigma}}^{ \perp}_{out}(\widetilde{\mathbf{U}}^{\perp}_{out})^{\top}\widehat{\beta}\|_{2}^ {2}\] \[\leq\|\widetilde{\bm{\Sigma}}^{\perp}_{out}\|_{2}^{2}\|(\widetilde{ \mathbf{U}}^{\perp}_{out})^{\top}\widehat{\beta}\|_{2}^{2}.\] (91)

Note that \(\|\widetilde{\bm{\Sigma}}^{\perp}_{out}\|_{2}\) equals the \((k+1)\)-th singular value of \(\mathbf{Z}^{\prime}_{y,out}\). Using Weyl's inequality (see Lemma H.3), and recalling that \(\mathbf{Z}^{\prime}_{y,out}=\mathbf{Z}^{\prime}_{f,out}+\mathbf{Z}^{\prime}_{x,out}\), and that the rank of \(\mathbf{Z}^{\prime}_{f,out}\) is \(k\), we can bound the \((k+1)\)-th singular value of \(\mathbf{Z}^{\prime}_{y,out}\) by the operator norm of \(\mathbf{Z}^{\prime}_{x,out}\). That is,

\[\|\widetilde{\bm{\Sigma}}^{\perp}_{out}\|_{2}^{2}\leq\|\mathbf{Z}^{\prime}_{x, out}\|_{2}^{2}.\] (92)

Next, we bound the term \(\|(\widetilde{\mathbf{U}}^{\perp}_{out})^{\top}\widehat{\beta}\|_{2}^{2}\).

\[\|(\widetilde{\mathbf{U}}_{out}^{\perp})^{\top}\widehat{\beta}\|_{2}^{2} =\|\widetilde{\mathbf{U}}_{out}^{\perp}(\widetilde{\mathbf{U}}_{out }^{\perp})^{\top}\widehat{\beta}\|_{2}^{2}\] \[=\|\widetilde{\mathbf{U}}_{out}^{\perp}(\widetilde{\mathbf{U}}_{ out}^{\perp})^{\top}\beta^{*}+\widetilde{\mathbf{U}}_{out}^{\perp}(\widetilde{ \mathbf{U}}_{out}^{\perp})^{\top}(\widehat{\beta}-\beta^{*})\|_{2}^{2}\] \[\leq 2\|\widetilde{\mathbf{U}}_{out}^{\perp}(\widetilde{\mathbf{U}} _{out}^{\perp})^{\top}\beta^{*}\|_{2}^{2}+2\|\widehat{\beta}-\beta^{*}\|_{2} ^{2}.\] (93)

Further expanding the first term,

\[\|\widetilde{\mathbf{U}}_{out}^{\perp}(\widetilde{\mathbf{U}}_{ out}^{\perp})^{\top}\beta^{*}\|_{2} =\|\widetilde{\mathbf{U}}_{out}^{\perp}(\widetilde{\mathbf{U}}_{ out}^{\perp})^{\top}\mathbf{U}_{out}(\mathbf{U}_{out})^{\top}\beta^{*}\|_{2}\] \[\leq\left\|\mathbf{U}_{out}^{\perp}(\mathbf{U}_{out}^{\perp})^{ \top}\mathbf{U}_{out}(\mathbf{U}_{out})^{\top}\beta^{*}\right\|_{2}\] \[+\left\|\left(\widetilde{\mathbf{U}}_{out}^{\perp}(\widetilde{ \mathbf{U}}_{out}^{\perp})^{\top}-\mathbf{U}_{out}^{\perp}(\mathbf{U}_{out}^{ \perp})^{\top}\right)\mathbf{U}_{out}(\mathbf{U}_{out})^{\top}\beta^{*}\right\| _{2}\] \[\leq\left\|\left(\widetilde{\mathbf{U}}_{out}^{\perp}(\widetilde{ \mathbf{U}}_{out}^{\perp})^{\top}-\mathbf{U}_{out}^{\perp}(\mathbf{U}_{out}^{ \perp})^{\top}\right)\beta^{*}\right\|_{2}\] \[\leq\left\|\widetilde{\mathbf{U}}_{out}^{\perp}(\widetilde{ \mathbf{U}}_{out}^{\perp})^{\top}-\mathbf{U}_{out}^{\perp}(\mathbf{U}_{out}^{ \perp})^{\top}\right\|_{2}\left\|\beta^{*}\right\|_{2}\] \[=\left\|\widetilde{\mathbf{U}}_{out}\widetilde{\mathbf{U}}_{out}^{ \top}-\mathbf{U}_{out}\mathbf{U}_{out}^{\top}\right\|_{2}\left\|\beta^{*}\right\| _{2}.\]

Where in the first equality we use the fact that \(\beta^{*}=\mathbf{U}_{out}(\mathbf{U}_{out})^{\top}\beta^{*}\), i.e., \(\beta^{*}\) lives in the column space of \(\mathbf{Z}_{f,out}^{\prime}\) (Assumption 4.2).

Next, by Wedin \(\sin\Theta\) Theorem (see [10, 32]) we bound \(\left\|\widetilde{\mathbf{U}}_{out}\widetilde{\mathbf{U}}_{out}^{\top}- \mathbf{U}_{out}\mathbf{U}_{out}^{\top}\right\|_{2}\) as follows:

\[\left\|\widetilde{\mathbf{U}}_{out}\widetilde{\mathbf{U}}_{out}^{\top}- \mathbf{U}_{out}\mathbf{U}_{out}^{\top}\right\|_{2}\left\|\beta^{*}\right\|_{2 }\leq\frac{\|\mathbf{Z}_{x,out}\|_{2}}{\sigma_{k}(\mathbf{Z}_{f,out})}\left\| \beta^{*}\right\|_{2}\] (94)

Using this definition, (91), (92), (93), and (94) we have

\[\left\|(\mathbf{Z}_{y,out}^{\prime}-\widehat{\mathbf{Z}}_{f,out})^{\top} \widehat{\beta}\right\|_{2}^{2}\leq 2\|\mathbf{Z}_{x,out}^{\prime}\|_{2}^{2} \Bigg{(}\frac{\|\mathbf{Z}_{x,out}\|_{2}^{2}\left\|\beta^{*}\right\|_{2}^{2}}{ \sigma_{k}(\mathbf{Z}_{f,out})^{2}}+\|\widehat{\beta}-\beta^{*}\|_{2}^{2} \Bigg{)}.\] (95)

_Second term: \(\left\|\widehat{\mathbf{Z}}_{f,out}^{\top}\widehat{\beta}-(\mathbf{Z}_{f,out}^ {\prime})^{\top}\beta^{*}\right\|_{2}\)._ To bound the second term, we follow a similar proof to that shown in [5].

\[\|\widehat{\mathbf{Z}}_{f,out}^{\top}\widehat{\beta}-(\mathbf{Z}_{ f,out}^{\prime})^{\top}\beta^{*}\|_{2}^{2} =\|\widehat{\mathbf{Z}}_{f,out}^{\top}\widehat{\beta}-\widehat{ \mathbf{Z}}_{f,out}^{\top}\beta^{*}+\widehat{\mathbf{Z}}_{f,out}^{\top}\beta^{ *}-(\mathbf{Z}_{f,out}^{\prime})^{\top}\beta^{*}\|_{2}^{2}\] \[\leq 2\|\widehat{\mathbf{Z}}_{f,out}^{\top}(\widehat{\beta}-\beta^{ *})\|_{2}^{2}+2\|(\mathbf{Z}_{f,out}^{\prime}-\widehat{\mathbf{Z}}_{f,out})^{ \top}\beta^{*}\|_{2}^{2}\] \[\leq 2\|\widehat{\mathbf{Z}}_{f,out}^{\top}(\widehat{\beta}-\beta^ {*})\|_{2}^{2}+2\|\mathbf{Z}_{f,out}^{\prime}-\widehat{\mathbf{Z}}_{f,out}^{ \prime}\|_{2,\infty}^{2}\|\beta^{*}\|_{1}^{2}.\] (96)

Next, we bound the term \(\|\widehat{\mathbf{Z}}_{f,out}^{\top}(\widehat{\beta}-\beta^{*})\|_{2}^{2}\).

\[\|\widehat{\mathbf{Z}}_{f,out}^{\top}(\widehat{\beta}-\beta^{*})\| _{2}^{2} \leq\|(\widehat{\mathbf{Z}}_{f,out}-\mathbf{Z}_{f,out}^{\prime}+ \mathbf{Z}_{f,out}^{\prime})^{\top}(\widehat{\beta}-\beta^{*})\|_{2}^{2}\] (97) \[\leq 2\|(\widehat{\mathbf{Z}}_{f,out}-\mathbf{Z}_{f,out}^{\prime})^{ \top}(\widehat{\beta}-\beta^{*})\|_{2}^{2}+2\|{\mathbf{Z}_{f,out}^{\prime}}^{ \top}(\widehat{\beta}-\beta^{*})\|_{2}^{2}\] (98) \[\leq 2\|\mathbf{Z}_{f,out}^{\prime}\|_{2}^{2}\|\widehat{\beta}-\beta^{ *}\|_{2}^{2}+2\|{\mathbf{Z}_{f,out}^{\prime}}^{\prime}(\widehat{\beta}-\beta^{*})\|_{2}^ {2}.\] (99)

Next, we bound \(\|{\mathbf{Z}_{f,out}^{\prime}}^{\top}(\widehat{\beta}-\beta^{*})\|_{2}^{2}\). Recall that \(\mathbf{U}\) spans the column space of \(\mathbf{Z}_{f,out}^{\prime}\). Thus \({\mathbf{Z}_{f,out}^{\prime}}^{\top}={\mathbf{Z}_{f,out}^{\prime}}^{\prime} \mathbf{U}\mathbf{U}^{\top}\), therefore,

\[\|\mathbf{Z}_{f,out}^{\top}(\widehat{\beta}-\beta^{*})\|_{2}^{2} =\|{\mathbf{Z}_{f,out}^{\prime}}^{\prime}\mathbf{U}\mathbf{U}^{ \top}(\widehat{\beta}-\beta^{*})\|_{2}^{2}\] \[\leq\|{\mathbf{Z}_{f,out}^{\prime}}\|_{2}^{2}\|\mathbf{U}\mathbf{U} ^{\top}(\widehat{\beta}-\beta^{*})\|_{2}^{2}.\] (100)Now recall that we denote the top \(k\) left singular vectors of \(\mathbf{Z}_{y}^{\prime}\) by \(\widehat{\mathbf{U}}\), then consider

\[\|\mathbf{U}\mathbf{U}^{\top}(\widehat{\beta}-\beta^{*})\|_{2}^{2} =\|(\mathbf{U}\mathbf{U}^{\top}+\widehat{\mathbf{U}}\widehat{ \mathbf{U}}^{\top}-\widehat{\mathbf{U}}\widehat{\mathbf{U}}^{\top})(\widehat{ \beta}-\beta^{*})\|_{2}^{2}\] \[\leq 2\|\mathbf{U}\mathbf{U}^{\top}-\widehat{\mathbf{U}}\widehat{ \mathbf{U}}^{\top}\|_{2}^{2}\|\widehat{\beta}-\beta^{*}\|_{2}^{2}+2\|\widehat {\mathbf{U}}\widehat{\mathbf{U}}^{\top}(\widehat{\beta}-\beta^{*})\|_{2}^{2}.\] (101)

Recall that, as we prove in (85) in Appendix F,

\[\|\widehat{\mathbf{U}}\widetilde{\mathbf{U}}^{\top}(\widehat{ \beta}-\beta^{*})\|_{2}^{2}\leq\frac{8}{\sigma_{k}(\mathbf{Z}_{f}^{\prime})^{ 2}}\Bigg{(}\|\mathbf{Z}_{f}^{\prime}-\widehat{\mathbf{Z}}_{f}^{\prime}\|_{2, \infty}^{2}\|\beta^{*}\|_{1}^{2}+\frac{\|\mathbf{Z}_{x}^{\prime}\|_{2}^{2}}{ \sigma_{k}(\mathbf{Z}_{f}^{\prime})^{2}}\,\|[\mathbf{Z}_{x}]_{L}\|_{2}^{2}+ \big{\|}[\mathbf{Z}_{x}]_{L}^{\top}\mathbf{V}\big{\|}_{2}^{2}\Bigg{)}.\] (102)

Using (101), (102) and Wedin \(\sin\Theta\) Theorem, we obtain,

\[\|\mathbf{U}\mathbf{U}^{\top}(\widehat{\beta}-\beta^{*})\|_{2}^{2} \leq 2\frac{\|\mathbf{Z}_{x}^{\prime}\|_{2}^{2}}{\sigma_{k}( \mathbf{Z}_{f}^{\prime})^{2}}\Bigg{(}\|\widehat{\beta}-\beta^{*}\|_{2}^{2}+ \frac{8\,\|[\mathbf{Z}_{x}]_{L}.\|_{2}^{2}}{\sigma_{k}(\mathbf{Z}_{f}^{\prime} )^{2}}\Bigg{)}\] \[+\frac{8}{\sigma_{k}(\widehat{\mathbf{Z}}_{f}^{\prime})^{2}} \Bigg{(}\|\mathbf{Z}_{f}^{\prime}-\widehat{\mathbf{Z}}_{f}^{\prime}\|_{2, \infty}^{2}\|\beta^{*}\|_{1}^{2}+\big{\|}[\mathbf{Z}_{x}]_{L}^{\top}\mathbf{V }\big{\|}_{2}^{2}\Bigg{)}.\] (103)

Therefore, using (96), (100), and (103), we have

\[\|\widehat{\mathbf{Z}}_{f,out}^{\top}\widehat{\beta}-(\mathbf{Z}_ {f,out}^{\prime})^{\top}\beta^{*}\|_{2}^{2}\] \[\leq 2||\widehat{\mathbf{Z}}_{f,out}^{\top}(\widehat{\beta}- \beta^{*})\|_{2}^{2}+2\|\mathbf{Z}_{f,out}^{\prime}-\widehat{\mathbf{Z}}_{f, out}^{\prime}\|_{2,\infty}^{2}\|\beta^{*}\|_{1}^{2}\] \[\leq 44\|\mathbf{Z}_{x,out}^{\prime}\|_{2}^{2}\|\widehat{\beta}- \beta^{*}\|_{2}^{2}\] \[+4\|\mathbf{Z}_{f,out}^{\prime}\|_{2}^{2}\frac{\|\mathbf{Z}_{x}^{ \prime}\|_{2}^{2}}{\sigma_{k}(\mathbf{Z}_{f}^{\prime})^{2}}\Bigg{(}\|\widehat {\beta}-\beta^{*}\|_{2}^{2}+\frac{8\,\|[\mathbf{Z}_{x}]_{L}.\|_{2}^{2}}{ \sigma_{k}(\mathbf{Z}_{f}^{\prime})^{2}}\Bigg{)}\] \[+16\|\mathbf{Z}_{f,out}^{\prime}\|_{2}^{2}\frac{1}{\sigma_{k}( \widehat{\mathbf{Z}}_{f}^{\prime})^{2}}\Bigg{(}2\|\mathbf{Z}_{f}^{\prime}- \widehat{\mathbf{Z}}_{f}^{\prime}\|_{2,\infty}^{2}\|\beta^{*}\|_{1}^{2}+\big{\| }[\mathbf{Z}_{x}]_{L}^{\top}\mathbf{V}\big{\|}_{2}^{2}\Bigg{)}\] \[+2\|\mathbf{Z}_{f,out}^{\prime}-\widehat{\mathbf{Z}}_{f,out}^{ \prime}\|_{2,\infty}^{2}\|\beta^{*}\|_{1}^{2}.\] (104)

_Combining_. Incorporating the two bounds in (95) and (104) yields,

\[\Big{\|}(\mathbf{Z}_{y,out}^{\prime})^{\top}\widehat{\beta}-( \mathbf{Z}_{f,out}^{\prime})^{\top}\beta^{*}\Big{\|}_{2}^{2} \leq c\|\mathbf{Z}_{x,out}^{\prime}\|_{2}^{2}\Bigg{(}\frac{\| \mathbf{Z}_{x,out}\|_{2}^{2}\,\|\beta^{*}\|_{2}^{2}}{\sigma_{k}(\mathbf{Z}_{f, out})^{2}}+\|\widehat{\beta}-\beta^{*}\|_{2}^{2}\Bigg{)}\] \[+c\|\mathbf{Z}_{f,out}^{\prime}\|_{2}^{2}\left(\frac{\|\mathbf{Z} _{x}^{\prime}\|_{2}^{2}}{\sigma_{k}(\mathbf{Z}_{f}^{\prime})^{2}}\|\widehat{ \beta}-\beta^{*}\|_{2}^{2}+\frac{\|\mathbf{Z}_{x}^{\prime}\|_{2}^{2}\,\|[ \mathbf{Z}_{x}]_{L}.\|_{2}^{2}}{\sigma_{k}(\mathbf{Z}_{f}^{\prime})^{4}}\right)\] \[+c\frac{\|\mathbf{Z}_{f,out}^{\prime}\|_{2}^{2}}{\sigma_{k}( \widehat{\mathbf{Z}}_{f}^{\prime})^{2}}\Bigg{(}2\|\mathbf{Z}_{f}^{\prime}- \widehat{\mathbf{Z}}_{f}^{\prime}\|_{2,\infty}^{2}\|\beta^{*}\|_{1}^{2}+\big{\|} [\mathbf{Z}_{x}]_{L}^{\top}\mathbf{V}\big{\|}_{2}^{2}\Bigg{)}.\] \[+c\|\mathbf{Z}_{f,out}^{\prime}-\widehat{\mathbf{Z}}_{f,out}^{ \prime}\|_{2,\infty}^{2}\|\beta^{*}\|_{1}^{2}.\] (105)

For some absolute constant \(c>0\).

**High probability bound.** With our choice \(L=\sqrt{NT}\), Let \(q\coloneqq\sqrt{NT}\) be the number of columns in the stacked page matrix \(\mathbf{Z}_{y}\) and subsequently \(q+1\) to be the number of columns in the stacked page matrix \(\mathbf{Z}_{y,out}\). Let \(C>0\) be some positive absolute constant, and \(C(f_{\max},\gamma)\) be a constant that depends only on model parameters \(f_{\max}\) and \(\gamma\). Define

\[\bar{E}_{1} :=\Big{\{}\|\mathbf{Z}_{x}^{\prime}\|_{2}\leq C\sigma_{x}\sqrt{q} \Big{\}},\] (106) \[\bar{E}_{2} :=\Big{\{}\left\|\mathbf{Z}_{x,out}^{\prime}\right\|_{2}\leq C \sigma_{x}\sqrt{q}\Big{\}},\] (107) \[\bar{E}_{3} :=\Big{\{}\left\|\widehat{\beta}-\beta^{*}\right\|_{2}\leq C(f_{ \max},\gamma)\bigg{(}\frac{\sigma_{x}^{4}G^{2}R^{2}\log(q)}{q}\bigg{)}\max\{\| \beta^{*}\|_{1}^{2},1\}\Big{\}},\] (108) \[\bar{E}_{4} :=\Big{\{}\|\mathbf{Z}_{f}^{\prime}-\widehat{\mathbf{Z}}_{f}^{ \prime}\|_{2,\infty}^{2}\leq\frac{C\sigma_{x}^{2}q^{2}}{\sigma_{k}(\mathbf{Z}_ {f})^{2}}\left(\sigma_{x}^{2}+f_{\max}^{2}\right)+C\sigma_{x}^{2}k\log(q) \Big{\}}\] (109) \[\bar{E}_{5} :=\Big{\{}\|\mathbf{Z}_{f,out}^{\prime}-\widehat{\mathbf{Z}}_{f,out}^{\prime}\|_{2,\infty}^{2}\leq\frac{C\sigma_{x}^{2}q^{2}}{\sigma_{k}( \mathbf{Z}_{f})^{2}}\left(\sigma_{x}^{2}+f_{\max}\right)+C\sigma_{x}^{2}k\log (q)\Big{\}}\] (110) \[\bar{E}_{6} :=\Big{\{}\left\|[\mathbf{Z}_{x}]_{L}^{\top}.\mathbf{V}\right\|_ {2}\leq C\sigma_{x}\sqrt{k\log(q)}\Big{\}},\] (111) \[\bar{E}_{7} :=\Big{\{}\left\|[\mathbf{Z}_{x}]_{L}.\right\|_{2}\leq C\sigma_{x }\sqrt{q}\Big{\}},\] (112)

First, with \(c_{1}>0\) is an absolute constant, recall from Lemma F.1,

\[\mathbb{P}(\bar{E}_{1}) \geq 1-2e^{-c_{1}q},\] \[\mathbb{P}(\bar{E}_{4}) \geq 1-\frac{c_{1}}{(NT)^{11}},\] \[\mathbb{P}(\bar{E}_{6}) \geq 1-\frac{c_{1}}{(NT)^{11}},\] \[\mathbb{P}(\bar{E}_{7}) \geq 1-2e^{-c_{1}q},\]

Further, note that \(\mathbf{Z}_{x,out}^{\prime}\) is a sub-matrix of \(\mathbf{Z}_{x,out}\), hence its operator norm is bounded by that of \(\mathbf{Z}_{x,out}\). Therefore, as an immediate consequence of Lemma A.2,

\[\mathbb{P}(\bar{E}_{2})\geq 1-2e^{-c_{1}q}.\]

The probability of event \(\bar{E}_{3}\) is bounded by Theorem 4.3 as

\[\mathbb{P}(\bar{E}_{3})\geq 1-\frac{c_{1}}{(NT)^{11}}.\]

Finally, as an immediate results of Corollary C.1 and its consequence in (27), we have

\[\mathbb{P}(\bar{E}_{5})\geq 1-\frac{c_{1}}{(NT)^{11}}.\]

Now consider the event \(\bar{E}:=\bar{E}_{1}\cap\bar{E}_{2}\cap\bar{E}_{3}\cap\bar{E}_{4}\cap\bar{E}_ {5}\cap\bar{E}_{6}\cap\bar{E}_{7}\). Then, under event \(\bar{E}\), and using (105), we have, with probability \(1-\frac{c}{(NT)^{11}}\),

\[\Big{\|}(\mathbf{Z}_{y,out}^{\prime})^{\top}\widehat{\beta}-( \mathbf{Z}_{f,out}^{\prime})^{\top}\beta^{*}\Big{\|}_{2}^{2} \leq C(f_{\max},\gamma)\sigma_{x}^{2}\Bigg{(}\frac{\sigma_{x}^{ 2}q^{2}\left\|\beta^{*}\right\|_{2}^{2}}{\sigma_{k}(\mathbf{Z}_{f,out})^{2}}+ \left(\sigma_{x}^{4}G^{2}R^{2}\log(q)\right)\max\{\|\beta^{*}\|_{1}^{2},1\} \Bigg{)}\] \[+C(f_{\max},\gamma)\|\mathbf{Z}_{f,out}^{\prime}\|_{2}^{2}\left( \frac{\sigma_{x}^{2}}{\sigma_{k}(\mathbf{Z}_{f}^{\prime})^{2}}\bigg{(}\sigma _{x}^{4}G^{2}R^{2}\log(q)\bigg{)}\max\{\|\beta^{*}\|_{1}^{2},1\}\right)\] \[+c\|\mathbf{Z}_{f,out}^{\prime}\|_{2}^{2}\frac{\sigma_{x}^{4}q^{ 2}}{\sigma_{k}(\mathbf{Z}_{f}^{\prime})^{4}}\] \[+c\frac{\|\mathbf{Z}_{f,out}^{\prime}\|_{2}^{2}}{\sigma_{k}( \widehat{\mathbf{Z}}_{f}^{\prime})^{2}}\|\beta^{*}\|_{1}^{2}\left(\frac{C \sigma_{x}^{2}q^{2}}{\sigma_{k}(\mathbf{Z}_{f})^{2}}\left(\sigma_{x}^{2}+f_{ \max}^{2}\right)+C\sigma_{x}^{2}k\log(q)\right)\] \[+c\frac{\|\mathbf{Z}_{f,out}^{\prime}\|_{2}^{2}}{\sigma_{k}( \widehat{\mathbf{Z}}_{f}^{\prime})^{2}}\sigma_{x}^{2}k\log(q)\] \[+c\frac{\sigma_{x}^{2}q^{2}}{\sigma_{k}(\mathbf{Z}_{f})^{2}} \left(\sigma_{x}^{2}+f_{\max}^{2}\right)\|\beta^{*}\|_{1}^{2}\] (113) \[+c\sigma_{x}^{2}k\log(q)\|\beta^{*}\|_{1}^{2}.\] (114)Recalling \(q=\sqrt{NT}\), \(k=RG\), and Assumption 4.1, we get,

\[\left\|(\mathbf{Z}_{y,out}^{\prime})^{\top}\widehat{\beta}-(\mathbf{Z}_{f,out}^{ \prime})^{\top}\beta^{*}\right\|_{2}^{2}\leq C(f_{\max},\gamma)R^{3}G^{3} \sigma_{x}^{6}\max\{\|\beta^{*}\|_{1}^{2},1\}\log(NT).\] (115)

Therefore, with probability \(1-c/(NT)^{11}\),

\[\frac{L}{NT} \sum_{n=1}^{N}\sum_{m=0}^{T/L-1}\left(\widehat{f}_{n}(T+mL+1)-f_{n }(T+mL+1)\right)^{2}\] (116) \[\leq C(f_{\max},\gamma)R^{3}G^{3}\sigma_{x}^{6}\left(\frac{\max\{ \|\beta^{*}\|_{1}^{2},1\}\log(NT)}{\sqrt{NT}}\right).\] (117)

Further, note that the same argument can be used to bound the sum \(\frac{L}{NT}\sum_{n=1}^{N}\sum_{m=0}^{T/L-1}\left(\widehat{f}_{n}(T+mL+\ell)- f_{n}(T+mL+\ell)\right)^{2}\) for all \(\ell\in[L]\). Thus, with probability \(1-cL/(NT)^{11}>1-c/(NT)^{10}\), we have,

\[\frac{2}{NT}\sum_{n=1}^{N}\sum_{t=T+1}^{2T}\left(\widehat{f}_{n}(t)-f_{n}(t) \right)^{2}\leq C(f_{\max},\gamma)R^{3}G^{3}\sigma_{x}^{6}\left(\frac{\max\{\| \beta^{*}\|_{1}^{2},1\}\log(NT)}{\sqrt{NT}}\right).\] (118)

### Bounding Forecasting Error of \(x\)

Now, we bound the error of forecasting \(x_{n}(t)\). First, recall \(\widetilde{x}_{n}(t)=y_{n}(t)-\widehat{f}_{n}(t)\), and let \(\widetilde{X}_{n}(t)\coloneqq[\widetilde{x}_{n}(t),\ldots,\widetilde{x}_{n}(t -p+1)]\) and \(X_{n}(t)\coloneqq[x_{n}(t),\ldots,x_{n}(t-p+1)]\). Now, let \(\bm{X}_{n}\), \(\widetilde{\bm{X}}_{n}\) and \(\bm{\Delta}_{n}\), all in \(\mathbb{R}^{(T-p)\times p}\), be the row-wise concatenations of \(X_{n}(t)\), \(\widetilde{X}_{n}(t)\), and \(\Delta_{n}(t)\coloneqq X_{n}(t)-\widetilde{X}_{n}(t)\) for \(t\in\{T+p,T+L,\ldots,2T+p-1\}\).

Then, we have, where \(\bar{x}_{n}(t)=\mathbb{E}\left[x_{n}(t)|x_{n}(1),\ldots,x_{n}(t-1)\right]\)

\[\sum_{t=T+1}^{2T}\left(\hat{x}_{n}(t)-\bar{x}_{n}(t)\right)^{2} =\left\|\widehat{\alpha}_{n}^{\top}\widetilde{\bm{X}}_{n}^{\top} -\alpha_{n}^{\top}\bm{X}_{n}^{\top}\right\|_{2}^{2}\] (119) \[=\left\|\widehat{\alpha}_{n}^{\top}\widetilde{\bm{X}}_{n}^{\top} -\widehat{\alpha}_{n}^{\top}\bm{X}_{n}^{\top}+\widehat{\alpha}_{n}^{\top}\bm {X}_{n}^{\top}-\alpha_{n}^{\top}\bm{X}_{n}^{\top}\right\|_{2}^{2}\] (120) \[\leq 2\left(\left\|\widehat{\alpha}_{n}^{\top}\widetilde{\bm{X}}_ {n}^{\top}-\widehat{\alpha}_{n}^{\top}\bm{X}_{n}^{\top}\right\|_{2}^{2}+\left\| \widehat{\alpha}_{n}^{\top}\bm{X}_{n}^{\top}-\alpha_{n}^{\top}\bm{X}_{n}^{ \top}\right\|_{2}^{2}\right)\] (121) \[\leq 2\left(\left\|\widehat{\alpha}_{n}^{\top}\widetilde{\bm{X}}_ {n}^{\top}-\widehat{\alpha}_{n}^{\top}\bm{X}_{n}^{\top}\right\|_{2}^{2}+\left\| \widehat{\alpha}_{n}^{\top}-\alpha_{n}^{\top}\right\|_{2}^{2}\left\|\bm{X}_{n} ^{\top}\right\|_{2}^{2}\right)\] (122) \[\leq 4\left(\left\|\bm{\Delta}_{n}\right\|_{2}^{2}\left(\left\| \alpha_{n}\right\|_{2}^{2}+\left\|\widehat{\alpha}_{n}-\alpha_{n}\right\|_{2}^{ 2}\right)+\left\|\bm{X}_{n}\right\|_{2}^{2}\left\|\widehat{\alpha}_{n}-\alpha _{n}\right\|_{2}^{2}\right).\] (123)

First note that

\[\sum_{n=1}^{N}\left\|\bm{\Delta}_{n}\right\|_{2}^{2} \leq\sum_{n=1}^{N}\left\|\bm{\Delta}_{n}\right\|_{F}^{2}\] (124) \[\leq p\sum_{n=1}^{N}\sum_{t=T-p+1}^{2T-1}(x_{n}(t)-\widetilde{x}_ {n}(t))^{2}\] (125) \[=p\sum_{n=1}^{N}\sum_{t=T-p+1}^{2T-1}(f_{n}(t)-\widehat{f}_{n}(t)) ^{2}.\] (126)Therefore,

\[\sum_{n=1}^{N}\sum_{t=T+1}^{2T}\left(\widehat{x}_{n}(t)-\bar{x}_{n}(t) \right)^{2} \leq 4p\max_{n}\left(\left\|\alpha_{n}\right\|_{2}^{2}+\left\| \widehat{\alpha}_{n}-\alpha_{n}\right\|_{2}^{2}\right)\sum_{n=1}^{N}\sum_{t=T- p+1}^{2T-1}\left(f_{n}(t)-\widehat{f}_{n}(t)\right)^{2}\] (127) \[+\sum_{n=1}^{N}\left\|\bm{X}_{n}\right\|_{2}^{2}\left\|\widehat{ \alpha}_{n}-\alpha_{n}\right\|_{2}^{2}\] (128)

**High probability bound.** For the high probability bound, consider the event \(\tilde{E}=\tilde{E}_{1}\cap\tilde{E}_{2}\cap\tilde{E}_{3}\) where

\[\tilde{E}_{1} :=\bigcap_{n=1}^{N}\Big{\{}\left\|\widehat{\alpha}_{n}-\alpha_{n} \right\|_{2}^{2}\leq C(f_{\max},\gamma)\frac{GRp}{\lambda_{\min}(\Psi)}\frac{ \sigma_{x}^{6}}{\sigma^{2}}\frac{\log(NT)^{2}}{\sqrt{NT}}\max\bigg{\{}1,\frac{ 1}{\sigma^{2}\lambda_{\min}(\Psi)}\bigg{\}}+\frac{Cp^{2}}{T\lambda_{\min}( \Psi)}\log\left(\frac{T\lambda_{\max}(\Psi)}{\lambda_{\min}(\Psi)}\right) \Big{\}}\] (129) \[\tilde{E}_{2} :=\Big{\{}\sum_{n=1}^{N}\sum_{t=T-p+1}^{2T-1}\left(f_{n}(t)- \widehat{f}_{n}(t)\right)^{2}\leq C(f_{\max},\gamma)R^{3}G^{3}\sigma_{x}^{6} \left(\max\{\left\|\beta^{*}\right\|_{1}^{2},1\}\sqrt{NT}\log(NT)\right) \Big{\}}\] (130) \[\tilde{E}_{3} :=\bigcap_{n=1}^{N}\Big{\{}\left\|\bm{X}_{n}\right\|_{2}^{2}\leq \frac{3}{2}\sigma^{2}(T-p)\lambda_{\max}(\Psi)\Big{\}},\] (131)

Note that \(\mathbb{P}(\tilde{E}_{1})\geq 1-\frac{CN}{T^{11}}\) using Theorems 4.2, 4.1 and the the union bound. Further, note that \(\mathbb{P}(\tilde{E}_{2})\geq 1-\frac{C}{(NT)^{10}}\) using both Lemma G.1 and Theorem 4.1. Further, note that according to Corollary E.1, with probability \(1-\frac{c}{T^{11}}\), we have

\[\left\|\bm{X}_{n}\right\|_{2}^{2}=\lambda_{\max}(\bm{X}_{n}^{\top}\bm{X}_{n}) \leq\frac{3}{2}\sigma^{2}(T-p)\lambda_{\max}(\Psi).\] (132)

Therefore, under \(\tilde{E}\), and recalling \(N<T\), we have with probability \(1-\frac{C}{T^{10}}\),

\[\frac{1}{NT}\sum_{n=1}^{N}\sum_{t=T+L}^{2T+L}\left(\widehat{x}_{n }(t)-x_{n}(t)\right)^{2}\] (133) \[\leq C(f_{\max},\gamma)G^{3}R^{3}p^{2}\frac{\lambda_{\max}(\Psi) }{\lambda_{\min}(\Psi)}\sigma_{x}^{6}\max\{\left\|\beta^{*}\right\|_{1}^{2},1 \}\Bigg{(}\frac{p\sigma^{2}}{T}\log\left(\frac{T\lambda_{\max}(\Psi)}{ \lambda_{\min}(\Psi)}\right)\] (134) \[+\frac{GR\alpha_{\max}^{2}}{\min\left\{1,\sigma^{2}\lambda_{\min} (\Psi)\right\}}\frac{\sigma_{x}^{6}}{\sigma^{2}}\frac{\log(NT)^{2}}{\sqrt{NT} }\Bigg{)}.\] (135)

where \(\alpha_{\max}=\max_{n}\left\|\alpha_{n}\right\|_{2}\) therefore, with probability \(1-\frac{C}{T^{10}}\), we have

\[\frac{2}{NT}\sum_{n=1}^{N}\sum_{t=T+1}^{2T}\left(\widehat{y}_{n}( t)-y_{n}(t)\right)^{2}\] (136) \[\leq C(f_{\max},\gamma)G^{3}R^{3}p^{2}\frac{\lambda_{\max}(\Psi) }{\lambda_{\min}(\Psi)}\sigma_{x}^{6}\max\{\left\|\beta^{*}\right\|_{1}^{2},1 \}\Bigg{(}\frac{p\sigma^{2}}{T}\log\left(\frac{T\lambda_{\max}(\Psi)}{ \lambda_{\min}(\Psi)}\right)\] (137) \[+\frac{GR\left\|\alpha_{n}\right\|_{2}^{2}}{\min\left\{1,\sigma^{2 }\lambda_{\min}(\Psi)\right\}}\frac{\sigma_{x}^{6}}{\sigma^{2}}\frac{\log(NT)^ {2}}{\sqrt{NT}}\Bigg{)}\] (138)

which completes the proof.

## Appendix H Helper Lemmas and Definitions

**Definition H.1** (Sub-gaussian random variable [30; 31]).: _A zero-mean random variable \(X\in\mathbb{R}\) is said to be sub-gaussian with variance proxy \(\sigma^{2}\) (denoted as \(X\sim\text{subG}(\sigma^{2})\)) if its moment generating function satisfies_

\[\operatorname{\mathbbm{I}\!E}[\exp(sX)]\leq\exp\left(\frac{\sigma^{2}s^{2}}{2} \right)\forall s\in\mathbb{R}.\]

**Definition H.2** (Sub-exponential random variable [30; 31]).: _A zero-mean random variable \(X\in\mathbb{R}\) is said to be sub-exponential with parameter \(\nu\) (denoted as \(X\sim\text{subE}(\nu)\)) if its moment generating function satisfies_

\[\operatorname{\mathbbm{I}\!E}[\exp(sX)]\leq\exp\left(\frac{\nu^{2}s^{2}}{2} \right)\forall|s|\leq\frac{1}{\nu}\]

**Lemma H.1** ([31; 30; 6]).: _Let \(X\sim\text{subG}(\sigma_{s}^{2})\) and \(Z\sim N(0,\sigma_{g}^{2})\). For \(i\in[N]\), let \(Y_{i}\sim\text{subE}(\nu_{i})\). Then:_

1. \(Z\sim subG(\sigma_{g}^{2})\)__
2. \(Z\sim subE(\sigma_{g})\)__
3. \(X^{2}-\operatorname{\mathbbm{E}}[X^{2}]\sim\text{subE}(16\sigma_{s}^{2})\)__
4. \(\sum_{i=1}^{N}Y_{i}\sim subE(\sum_{i=1}^{N}\nu_{i})\)__

_If \(Y_{i}\) are independent:_

1. \(\sum_{i=1}^{N}Y_{i}\sim subE((\sum_{i=1}^{N}\nu_{i}^{2})^{1/2})\)__

**Definition H.3** (Sub-gaussian vector).: _A random vector \(X\in\mathbb{R}^{d}\) is a sub-gaussian random vector with parameter \(\sigma^{2}\) if_

\[v^{T}X\sim\text{subG}(\sigma_{s}^{2}),\forall v\in\mathbb{S}^{d-1}\]

_where \(\mathbb{S}^{d-1}=\big{\{}x\in\mathbb{R}^{d}:\|x\|=1\big{\}}\)._

**Lemma H.2** (Norm of sub-gaussian vector).: _Let \(X\in\mathbb{R}^{d}\) be a sub-gaussian random vector with parameter \(\sigma^{2}\). Then, with probability at least \(1-\delta\) for \(\delta\in(0,1)\) :_

\[\|X\|_{2}\leq 4\sigma\sqrt{d}+2\sigma\sqrt{\log\left(\frac{1}{\delta} \right)}.\]

_This implies that for any \(t>2\sigma\),_

\[\|X\|_{2}\leq t,\]

_with probability at least \(1-\exp(-\frac{t^{2}}{16d\sigma^{2}})\)._

**Definition H.4** (Stationary process [26]).: _A process \(x(t)\in\mathbb{R}\) is said to be stationary if its expectation does not depend on \(t\) and its autocovariance function depend only on the time difference. That is,_

\[\operatorname{\mathbbm{I}\!E}[x(t)] =\mu \forall t\] \[\operatorname{\mathbbm{I}\!E}([x(t)-\mu)(x(t+j)-\mu)] =\gamma(j) \forall t\text{ and any }j.\]

**Lemma H.3** (Weyl's inequality).: _Given \(\bm{A},\bm{B}\in\mathbb{R}^{m\times n}\), let \(\sigma_{i}\) and \(\widehat{\sigma}_{i}\) be the \(i\)-th singular values of \(\bm{A}\) and \(\bm{B}\), respectively, in decreasing order and repeated by multiplicities. Then for all \(i\in[m\wedge n]\),_

\[|\sigma_{i}-\widehat{\sigma}_{i}|\leq\|\bm{A}-\bm{B}\|_{2}.\]

**Lemma H.4**.: _Let \(X=[X_{1},\ldots,X_{n}]\) where each \(X_{i},i\in[n]\) is a sub-gaussian (not necessarily independent) random variable with variance proxy \(\sigma^{2}\) and \(\operatorname{\mathbbm{E}}\left[X_{i}^{2}\right]=\gamma\). Then with probability at least \(1-c\exp\left(-\frac{t^{2}}{16n\sigma^{2}}\right)\),_

\[\|X\|_{2}\leq t\] (139)Proof.: From Lemma H.1, we can see that \(\|X\|_{2}^{2}-n\gamma=\sum_{i=1}^{n}x_{i}^{2}-n\gamma\) is sub-exponential with parameter \(16n\sigma^{2}\). Let \(d\coloneqq\sum_{i=1}^{n}x_{i}^{2}-n\gamma\), then

\[\mathbb{P}\left(d>t^{\prime}\right)\leq\frac{\mathbf{E}[\exp(sd)]}{\exp(st^{ \prime})}\leq\exp\left(\frac{\nu^{2}s^{2}}{2}-st^{\prime}\right)\qquad\forall| s|\leq\frac{1}{\nu}.\] (140)

Choosing \(t^{\prime}=t^{2}-n\gamma\), \(\nu=16n\sigma^{2}\), and \(s=\frac{1}{16n\sigma^{2}}\), we get,

\[\mathbb{P}\left(d>t^{2}-n\gamma\right)=\mathbb{P}\left(\|X\|_{2}^ {2}>t^{2}\right) \leq\exp\left(\frac{1}{2}\right)\exp\left(-\frac{t^{2}-n\gamma}{16 n\sigma^{2}}\right)\] (141) \[\leq\exp\left(\frac{1}{2}\right)\exp\left(\frac{\gamma}{16\sigma ^{2}}\right)\exp\left(-\frac{t^{2}}{16n\sigma^{2}}\right)\] (142) \[\leq c\exp\left(-\frac{t^{2}}{16n\sigma^{2}}\right)\] (143)

where the last inequality follows from the fact the second moment of \(x_{1}\) is bounded by its variance proxy up to a multiplicative constant. 

**Lemma H.5**.: _Let \(X_{1},\ldots,X_{n}\) be \(i.i.d.\) sub-exponential random variables with parameter \(\nu\). Let \(S=\sum_{i=1}^{n}X_{i}\). Then_

\[\mathbb{P}\left(|S|\leq nt\right)\geq 1-c\exp\left(-\frac{t\sqrt{n}}{\nu} \right).\] (144)

Proof.: Recall that the sum of n \(i.i.d.\) sub-exponential random variables is also a \(x\) sub-exponential random variable with parameter \(\nu\sqrt{n}\) (see Lemma H.1). Thus, using the definition of sub-exponential random variable, we have

\[\mathbb{P}\left(S>nt\right)\leq\frac{\mathbf{E}[\exp(\lambda S)]}{\exp(\lambda nt )}\leq\exp\left(\frac{n\nu^{2}\lambda^{2}}{2}-\lambda nt\right)\qquad\forall| s|\leq\frac{1}{\sqrt{n\nu}}.\] (145)

Choosing \(\lambda=\frac{1}{\sqrt{n\nu}}\), we get,

\[\mathbb{P}\left(S>nt\right) \leq\exp\left(\frac{1}{2}\right)\exp\left(-\frac{nt}{\sqrt{n\nu}}\right)\] (146) \[\leq 2\exp\left(\frac{-t\sqrt{n}}{\nu}\right)\!.\] (147)

Applying the same inequality to \(-S\) completes the proof. 

**Lemma H.6**.: _[_33_]_ _Let \(\bm{A}\in\mathbb{R}^{m\times n}\) and \(\bm{B}=\bm{A}+\bm{E}\). Then_

\[\left\|\bm{B}^{\dagger}-\bm{A}^{\dagger}\right\|_{2}\leq 2\max\left\{\left\|\bm {A}^{\dagger}\right\|_{2}^{2},\left\|\bm{B}^{\dagger}\right\|_{2}^{2}\right\} \left\|E\right\|_{2},\]

_where \(\bm{M}^{\dagger}\coloneqq(\bm{M}^{\top}\bm{M})^{-1}\bm{M}^{\top}\) denote the Moore-Penrose inverse of \(\bm{M}\)._

**Lemma H.7**.: _[Lemma 4.2 of [27]] Let \(\{\mathcal{F}_{t}\}_{t\geq 0}\) be a filtration, and \(\{Z_{t}\}_{t\geq 1}\) and \(\{W_{t}\}_{t\geq 1}\) be real-valued processes adapted to \(\mathcal{F}_{t}\) and \(\mathcal{F}_{t+1}\) respectively. Moreover, assume \(W_{t}|\mathcal{F}_{t}\) is mean zero and \(\sigma^{2}\)-sub-gaussian. Then, for any positive real numbers \(\alpha,\beta\) we have_

\[\mathbb{P}\left(\left\{\sum_{t=1}^{T}Z_{t}W_{t}\geq\alpha\right\}\cap\left\{ \sum_{t=1}^{T}Z_{t}^{2}\leq\beta\right\}\right)\leq\exp\left(-\frac{\alpha^{2} }{2\sigma^{2}\beta}\right)\!.\] (148)