# Gaussian Mixture Solvers for Diffusion Models

 Hanzhong Guo\({}^{*}\)\({}^{1,3}\), Cheng Lu\({}^{4}\), Fan Bao\({}^{4}\), Tianyu Pang\({}^{2}\), Shuicheng Yan\({}^{2}\),

**Chao Du\({}^{1}\)\({}^{2}\), Chongxuan Li\({}^{1,3}\)**

\({}^{1}\)Gaoling School of Artificial Intelligence, Renmin University of China

\({}^{2}\)Sea AI Lab, Singapore

\({}^{3}\)Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China

\({}^{4}\)Tsinghua University

{allanguo, tianyupang, yansc, duchao}@sea.com; lucheng.lc15@gmail.com; bf19@mails.tsinghua.edu.cn; chongxuanli@ruc.edu.cn

Work done during an internship at Sea AI Lab. \({}^{\dagger}\)Correspondence to Chao Du and Chongxuan Li.

###### Abstract

Recently, diffusion models have achieved great success in generative tasks. Sampling from diffusion models is equivalent to solving the reverse diffusion stochastic differential equations (SDEs) or the corresponding probability flow ordinary differential equations (ODEs). In comparison, SDE-based solvers can generate samples of higher quality and are suited for image translation tasks like stroke-based synthesis. During inference, however, existing SDE-based solvers are severely constrained by the efficiency-effectiveness dilemma. Our investigation suggests that this is because the Gaussian assumption in the reverse transition kernel is frequently violated (even in the case of simple mixture data) given a limited number of discretization steps. To overcome this limitation, we introduce a novel class of SDE-based solvers called _Gaussian Mixture Solvers (GMS)_ for diffusion models. Our solver estimates the first three-order moments and optimizes the parameters of a Gaussian mixture transition kernel using generalized methods of moments in each step during sampling. Empirically, our solver outperforms numerous SDE-based solvers in terms of sample quality in image generation and stroke-based synthesis in various diffusion models, which validates the motivation and effectiveness of GMS. Our code is available at https://github.com/Guohanzhong/GMS.

## 1 Introduction

In recent years, deep generative models and especially (score-based) diffusion models [35; 14; 37; 18] have made remarkable progress in various domains, including image generation [15; 7], audio generation [22; 32], video generation [16], 3D object generation [31], multi-modal generation [42; 5; 12; 43], and several downstream tasks such as image translation [28; 45] and image restoration [19].

Sampling from diffusion models can be interpreted as solving the reverse-time diffusion stochastic differential equations (SDEs) or their corresponding probability flow ordinary differential equations (ODEs) [37]. SDE-based and ODE-based solvers of diffusion models have very different properties and application scenarios In particular, SDE-based solvers usually perform better when given a sufficient number of discretization steps [18]. Indeed, a recent empirical study [24] suggests that SDE-based solvers can potentially generate high-fidelity samples with realistic details and intricate semantic coherence from pre-trained large-scale text-to-image diffusion models [34]. Besides, SDE-based solvers are preferable in many downstream tasks such as stroke-based synthesis [27], image translation [45], and image manipulation [20].

Despite their wide applications, SDE-based solvers face a significant trade-off between efficiency and effectiveness during sampling, since an insufficient number of steps lead to larger discretization errors. To this end, Bao et al. [3] estimate the optimal variance of the Gaussian transition kernel in the reverse process instead of using a handcraft variance to reduce the discretization errors. Additionally, Bao et al. [2] explore the optimal diagonal variance when dealing with an imperfect noise network and demonstrate state-of-the-art (SOTA) performance with a few steps among other SDE-based solvers. Notably, these solvers all assume that the transition kernel in the reverse process is Gaussian.

In this paper, we systematically examine the assumption of the Gaussian transition kernel and reveal that it can be easily violated under a limited number of discretization steps even in the case of simple mixture data. To this end, we propose a new type of SDE-based solver called _Gaussian Mixture Solver (GMS)_, which employs a more expressive Gaussian mixture transition kernel in the reverse process for better approximation given a limited number of steps (see visualization results on toy data in Fig. 1). In particular, we first learn a noise prediction network with multiple heads that estimate the high-order moments of the true reverse transition kernel respectively. For sampling, we fit a Gaussian mixture transition kernel in each step via the _generalized methods of the moments_[11] using the predicted high-order moments of the true reverse process.

To evaluate GMS, we compare it against a variety of baselines [14; 2; 17; 36] in terms of several widely used metrics (particularly sample quality measured by FID) in the tasks of generation and stroke-based synthesis on multiple datasets. Our results show that GMS outperforms state-of-the-art SDE-based solvers [14; 2; 17] in terms of sample quality with the limited number of discretization steps (e.g., \(<100\)). For instance, GMS improves the FID by 4.44 over the SOTA SDE-based solver [2] given 10 steps on CIFAR10. Furthermore, We evaluate GMS on a stroke-based synthesis task. The findings consistently reveal that GMS achieves higher levels of realism than all aforementioned SDE-based solvers as well as the widely adopted ODE-based solver DDIM [36] while maintaining comparable computation budgets and faithfulness scores (measured by \(L_{2}\) distance). Such empirical findings validate the motivation and effectiveness of GMS.

## 2 Background

In this section, we provide a brief overview of the (score-based) diffusion models, representative SDE-based solvers for diffusion models, and applications of such solvers.

### Diffusion models

Diffusion models gradually perturb data with a forward diffusion process and then learn to reverse such process to recover the data distribution. Formally, let \(x_{0}\in\mathbb{R}^{n}\) be a random variable with unknown data distribution \(q(x_{0})\). Diffusion models define the forward process \(\left\{x_{t}\right\}_{t\in[0,1]}\) indexed by time \(t\), which perturbs the data by adding Gaussian noise to \(x_{0}\) with

\[q(x_{t}|x_{0})=\mathcal{N}(x_{t}|a(t)x_{0},\sigma^{2}(t)I).\] (1)

Figure 1: **Sampling on a mixture of Gaussian.** GMS (**ours**) and SN-DDPM excel in fitting the true distribution when the transition kernel is Gaussian, with a sufficient number of sampling steps (c). However, GMS outperforms SN-DDPM [2] when sampling steps are limited and the reverse transition kernel deviates from Gaussian (a-b).

In general, the function \(a(t)\) and \(\sigma(t)\) are selected so that the logarithmic signal-to-noise ratio \(\log\frac{a^{2}(t)}{\sigma^{2}(t)}\) decreases monotonically with time \(t\), causing the data to diffuse towards random Gaussian noise [21]. Furthermore, it has been demonstrated by Kingma et al. [21] that the following SDE shares an identical transition distribution \(q_{t|0}(x_{t}|x_{0})\) with Eq. (1):

\[\mathrm{d}x_{t}=f(t)x_{t}\mathrm{d}t+g(t)\mathrm{d}\omega,\quad x_{0}\sim q(x_ {0}),\] (2)

where \(\omega\in\mathbb{R}^{n}\) is a standard Wiener process and

\[f(t)=\frac{\mathrm{d}\log a(t)}{\mathrm{d}t},\quad g^{2}(t)=\frac{\mathrm{d} \sigma^{2}(t)}{\mathrm{d}t}-2\sigma^{2}(t)\frac{\mathrm{d}\log a(t)}{\mathrm{ d}t}.\] (3)

Let \(q(x_{t})\) be the marginal distribution of the above SDE at time \(t\). Its reversal process can be described by a corresponding continuous SDE which recovers the data distribution [37]:

\[\mathrm{d}x=\left[f(t)x_{t}-g^{2}(t)\nabla_{x_{t}}\log q(x_{t})\right] \mathrm{d}t+g(t)\mathrm{d}\bar{\omega},\quad x_{1}\sim q(x_{1}),\] (4)

where \(\bar{\omega}\in\mathbb{R}^{n}\) is a reverse-time standard Wiener process. The only unknown term in Eq. (4) is the score function \(\nabla_{x_{t}}\log q(x_{t})\). To estimate it, existing works [14; 37; 18] train a noise network \(\epsilon_{\theta}(x_{t},t)\) to obtain a scaled score function \(\sigma(t)\nabla_{x_{t}}\log q(x_{t})\) using denoising score matching [38]:

\[\mathcal{L}(\theta)=\int_{0}^{1}w(t)\mathbb{E}_{q(x_{0})}\mathbb{E}_{q( \epsilon)}[\|\epsilon_{\theta}(x_{t},t)-\epsilon\|_{2}^{2}]\mathrm{d}t,\] (5)

where \(w(t)\) is a weighting function, \(q(\epsilon)\) is standard Gaussian distribution and \(x_{t}\sim q(x_{t}|x_{0})\) follows Eq. (1). The optimal solution of the optimization objective Eq. (5) is \(\epsilon_{\theta}(x_{t},t)=-\sigma(t)\nabla_{x_{t}}\log q(x_{t})\).

Hence, samples can be obtained by initiating the process with a standard Gaussian variable \(x_{1}\), then substituting \(\nabla_{x_{t}}\log q(x_{t})\) with \(-\frac{\epsilon_{\theta}(x_{t},t)}{\sigma(t)}\) and discretizing reverse SDE Eq. (4) from \(t=1\) to \(t=0\) to generate \(x_{0}\).

### SDE-based solvers for diffusion models

The primary objective of SDE-based solvers lies in decreasing discretization error and therefore minimizing function evaluations required for convergence during the process of discretizing Eq. (4). Discretizing the reverse SDE in Eq. (4) is equivalent to sample from a Markov chain \(p(x_{0:1})=p(x_{1})\prod_{t_{i-1},t_{i}\in\mathcal{S}_{t_{i}}}p(x_{t_{i-1}}|x_ {t_{i}})\) with its trajectory \(S_{t}=[0,t_{1},t_{2},...,t_{i},..,1]\). Song et al. [37] proves that the conventional ancestral sampling technique used in the DPMs [14] that models \(p(x_{t_{i-1}}|x_{t_{i}})\) as a Gaussian distribution, can be perceived as a first-order solver for the reverse SDE in Eq. (4). Bao et al. [3] finds that the optimal variance of \(p(x_{t_{i-1}}|x_{t_{i}})\sim\mathcal{N}(x_{t_{i-1}}|\mu_{t_{i-1}|t_{i}},\Sigma _{t_{i-1}|t_{i}}(x_{t_{i}}))\) is

\[\Sigma^{*}_{t_{i-1}|t_{i}}(x_{t_{i}})=\lambda_{t_{i}}^{2}+\gamma_{t_{i}}^{2} \frac{\sigma^{2}(t_{i})}{\alpha(t_{i})}\left(1-\mathbb{E}_{q(x_{t_{i}})}\left[ \frac{t}{d}\left\|\mathbb{E}_{q(x_{0}|x_{t_{i}})}[\epsilon_{\theta}(x_{t_{i}}, t_{i})]\right\|_{2}^{2}\right]\right),\] (6)

where, \(\gamma_{t_{i}}=\sqrt{\alpha(t_{i-1})}-\sqrt{\sigma^{2}(t_{i-1})-\lambda_{t_{i} }^{2}}\sqrt{\frac{\alpha(t_{i})}{\sigma^{2}(t_{i})}}\), \(\lambda_{t_{i}}^{2}\) is the variance of \(q(x_{t_{i-1}}|x_{t_{i}},x_{0})\). AnalyticDPM [3] offers a significant reduction in discretization error during sampling and achieves faster convergence with fewer steps. Moreover, SN-DDPM [2] employs a Gaussian transition kernel with an optimal diagonal covariance instead of an isotropic covariance. This approach yields improved sample quality and likelihood compared to other SDE-based solvers within a limited number of steps.

### Applications of SDE-based solvers for stroke-based synthesis

The stroke-based image synthesis is a representative downstream task suitable for SDE-based solvers. It involves the user providing a full-resolution image \(x^{(g)}\) through the manipulation of RGB pixels, referred to as the guided image. The guided image \(x^{(g)}\) possibly contains three levels of guidance: a high-level guide consisting of coarse colored strokes, a mid-level guide comprising colored strokes on a real image, and a low-level guide containing image patches from a target image.

SDEdit [27] solves the task by first starting from the guided image \(x^{(g)}\) and adding Gaussian noise to disturb the guided images to \(x_{t}\) and \(q(x^{(g)}(t_{0})|x^{(g)})\sim\mathcal{N}(x_{t_{0}}|a(t_{0})x^{(g)},\sigma^{2} (t)I)\) same with Eq. (1). Subsequently, it solves the corresponding reverse stochastic differential equation (SDE) up to \(t=0\) to generate the synthesized image \(x(0)\) discretizing Eq. (4). Apart from the discretization steps taken by the SDE-based solver, the key hyper-parameter for SDEdit is \(t_{0}\), the time step from which we begin the image synthesis procedure in the reverse SDE.

## 3 Gaussian mixture solvers for diffusion models

In this section, we first show through both theoretical and empirical evidence, that the true reverse transition kernel can significantly diverge from Gaussian distributions as assumed in previous SOTA SDE-based solvers [3; 2], indicating that the reverse transition can be improved further by employing more flexible distributions (see Sec. 3.1). This motivates us to propose a novel class of SDE-based solvers, dubbed _Gaussian Mixture Solvers_, which determines a Gaussian mixture distribution via the _generalized methods of the moments_[11] using higher-order moments information for the true reverse transition (see Sec. 3.2). The higher-order moments are estimated by a noise prediction network with multiple heads on training data, as detailed in Sec. 3.3.

### Suboptimality of Gaussian distributions for reverse transition kernels

As described in Sec. 2, existing state-of-the-art SDE-based solvers for DPMs [3; 2] approximate the reverse transition \(q(x_{t_{i-1}}|x_{t_{i}})\) using Gaussian distributions. Such approximations work well when the number of discretization steps in these solvers is large (e.g., \(1000\) steps). However, for smaller discretization steps (such as when faster sampling is required), the validity of this Gaussian assumption will be largely broken. We demonstrate this theoretically and empirically below.

First, observe that \(q(x_{s}|x_{t})\)1 can be expressed as \(q(x_{s}|x_{t})=\int q(x_{s}|x_{t},x_{0})q(x_{0}|x_{t})\mathrm{d}x_{0}\), which is non-Gaussian for a general data distribution \(q(x_{0})\). For instance, for a mixture of Gaussian or a mixture of Dirac \(q(x_{0})\), we can prove that the conditional distributions \(q(x_{s}|x_{t})\) in the reverse process are non-Gaussian, as characterized by Proposition 3.1, proven in Appendix A.1.

Footnote 1: To enhance the clarity of exposition, we introduce the notation \(s\doteq t_{i-1}\) and \(t\doteq t_{i}\) to denote two adjacent time steps in the trajectory. Consequently, we refer to \(q(x_{s}|x_{t})\) as the reverse transition kernel in this context.

**Proposition 3.1** (Mixture Data Have non-Gaussian Reverse Kernel).: _Assume \(q(x_{0})\) is a mixture of Dirac or a mixture of Gaussian distribution and the forward process is defined in Eq. (1). The reverse transition kernel \(q(x_{s}|x_{t}),s<t\) is a Gaussian mixture instead of a Gaussian._

Empirically, we do not know the distributions of the real data (e.g., high-dimensional images) and cannot obtain an explicit form of \(q(x_{s}|x_{t})\). However, even in such cases, it is easy to validate that \(q(x_{s}|x_{t})\) are non-Gaussian. In particular, note that the third-order moment of one Gaussian distribution (\(M_{3}^{(G)}\)) can be represented by its first-order moment (\(M_{1}\)) and its second-order moment (\(M_{2}\)) 2, which motivates us to check whether the first three order moments of \(q(x_{s}|x_{t})\) satisfy the relationship induced by the Gaussian assumption. We perform an experiment on CIFAR-10 and estimate the first three orders of moments \(\hat{M_{1}}\), \(\hat{M_{2}}\), \(\hat{M_{3}}\) for \(q(x_{s}|x_{t})\) from data by high-order noise networks (see details in Sec. D.2). As shown in Fig. 2, we plot the mean and median of \(l_{2}\)-norm

Figure 2: **Empirical evidence of suboptimality of Gaussian kernel on CIFAR10.** (a) and (b) plot the logarithm of the image-wise mean and median of \(L_{2}\)-norm between Gaussian-assumed third-order moments and estimated third-order moments. Clearly, as the number of sampling steps decreases, the disparity between the following two third-order moments increases, denoting that the true transition kernel diverges further from the Gaussian distribution. See Appendix D.2 for more details.

between the estimated third-order moment \(\hat{M}_{3}\) and third-order moment \(M_{3}^{(G)}\) calculated under the Gaussian assumption given the different number of steps at different time steps \(t\). In particular, when the number of steps is large (i.e., \(\#\)Step=\(1000\)), the difference between the two calculation methods is small. As the number of steps decreases, the \(l_{2}\)-norm increases, indicating that the true reverse distribution \(q(x_{s}|x_{t})\) is non-Gaussian. With the time step closer to \(t=0\), the \(l_{2}\)-norm increases too.

Both the theoretical and empirical results motivate us to weaken the Gaussian assumption in SDE-based solvers for better performance, especially when the step size is large.

### Sampling with Gaussian mixture transition kernel

There are extensive choices of \(p(x_{s}|x_{t})\) such that it is more powerful and potentially fits \(q(x_{s}|x_{t})\) better than a Gaussian. In this paper, we choose a simple mixture of the Gaussian model as follows:

\[p(x_{s}|x_{t})=\sum_{i=1}^{M}w_{i}\mathcal{N}(x_{s}|\mu_{i}(x_{t}),\Sigma_{i}( x_{t})),\quad\sum_{i=1}^{M}w_{i}=1,\] (7)

where \(w_{i}\) is a scalar and \(\mu_{i}(x_{t})\) and \(\Sigma_{i}(x_{t})\) are vectors. The reasons for choosing a Gaussian mixture model are three-fold. First, a Gaussian mixture model is multi-modal (e.g., see Proposition 3.1), potentially leading to a better performance than Gaussian with few steps. Second, when the number of steps is large and the Gaussian is nearly optimal [35; 3], a designed Gaussian mixture model such as our proposed kernel in Eq. (9) can degenerate to a Gaussian when the mean of two components are same, making the performance unchanged. Third, a Gaussian mixture model is relatively easy to sample. For the sake of completeness, we also discuss other distribution families in Appendix C.

Traditionally, the EM algorithm is employed for estimating the Gaussian mixture [26]. However, it is nontrivial to apply EM here because we need to learn the reverse transition kernel in Eq. (7) for all time step pairs \((s,t)\) by individual EM processes where we need to sample multiple \(x_{s}\)3 given a \(x_{t}\) to estimate the parameters in the Gaussian mixture indexed by \((s,t)\). This is time-consuming, especially in a high-dimensional space (e.g., natural images) and we present an efficient approach.

Footnote 3: Note that \(x_{s}\) serves as the “training sample” in the corresponding EM process.

For improved flexibility in sampling and training, diffusion models introduce the parameterization of the noise network \(\epsilon(x_{t},t)\)[14] or the data prediction network \(x_{0}(x_{t},t)\)[18]. With such a network, the moments under the \(q(x_{s}|x_{t})\) measure can be decomposed into moments under the \(q(x_{0}|x_{t})\) measure so that sampling any \(x_{t}\) to \(x_{s}\) requires only a network whose inputs are \(x_{t}\) and \(t\), such as the decomposition shown in Eq. (10). In previous studies, Gaussian transition kernel was utilized, allowing for the distribution to be directly determined after obtaining the estimated first-order moment and handcrafted second-order moment [14] or estimated first-order and second-order moment [3]. In contrast, such a feature is not available for the Gaussian mixtures transition kernel in our paper.

Here we present the method to determine the Gaussian mixture given a set of moments and we will discuss how to obtain the moments by the parameterization of the noise network in Sec. 3.3. Assume the length of the estimated moments set is \(N\) and the number of parameters in the Gaussian mixture is \(d\). We adopt a popular and theoretically sound method called the generalized method of moments (GMM) [11] to learn the parameters by:

\[\min_{\theta}Q(\theta,M_{1},...,M_{N})=\min_{\theta}[\frac{1}{N_{c}}\sum_{i=1} ^{N_{c}}g(x_{i},\theta)]^{T}W[\frac{1}{N_{c}}\sum_{i=1}^{N_{c}}g(x_{i},\theta)],\] (8)

where \(\theta\in\mathbb{R}^{d\times 1}\) includes all parameters (e.g., the mean of each component) in the Gaussian mixture defined in Eq. (7). For instance, \(d=2M*D_{\text{data}}+M-1\) in Eq. (7), where \(D_{\text{data}}\) represents the number of dimensions of the data and \(\theta\) contains \(M\) mean vectors with \(D_{\text{data}}\) dimensions, \(M\) variance vectors of with \(D_{\text{data}}\) dimensions (considering only the diagonal elements), and \(M-1\) weight coefficients which are scalar. The component of \(g(x_{i},\theta)\in\mathbb{R}^{N\times 1}\) is defined by \(g_{n}(x_{i},\theta)=M_{n}(x_{i})-M_{n}^{(GM)}(\theta)\), where \(M_{n}(x_{i})\) is the \(n\)-th order empirical moments stated in Sec. 3.3 and \(M_{n}^{(GM)}(\theta)\) is the \(n\)-th order moments of Gaussian mixture under \(\theta\), and \(W\) is a weighted matrix, \(N_{c}\) is the number of samples.

Theoretically, the parameter \(\hat{\theta}_{\text{GMM}}\) obtained by GMM in Eq. (8) consistently converges to the potential optimal parameters \(\theta^{*}\) for Gaussian mixture models given the moments' condition because \(\sqrt{N_{c}}(\hat{\theta}_{\text{GMM}}-\theta^{*})\overset{d}{\rightarrow} \mathcal{N}(0,\forall(\theta_{GMM}))\), as stated in Theorem 3.1 in Hansen [11].

Hence, we can employ GMM to determine a Gaussian mixture transition kernel after estimating the moments of the transition kernel. To strike a balance between computational tractability and expressive power, we specifically focus on the first three-order moments in this work and define a Gaussian mixture transition kernel with two components shown in Eq. (9) whose vectors \(\mu_{t}^{(1)}\), \(\mu_{t}^{(2)}\), and \(\sigma_{t}^{2}\) are parameters to be optimized. This degree of simplification is acceptable in terms of its impact. Intuitively, such a selection has the potential to encompass exponential modes throughout the entire trajectory. Empirically, we consistently observe that utilizing a bimodal Gaussian mixture yields favorable outcomes across all experimental configurations.

\[p(x_{s}|x_{t})=\frac{1}{3}\mathcal{N}(\mu_{t}^{(1)},\sigma_{t}^{2})+\frac{2}{3 }\mathcal{N}(\mu_{t}^{(2)},\sigma_{t}^{2}),\] (9)

meanwhile, under the simplification in our paper, the number of parameters \(d=3*D_{\text{data}}\) in our Gaussian transition kernel is equal to the number of moments condition \(N=3*D_{\text{data}}\). According to proposition 3.2, under the selection of arbitrary weighted weights, the asymptotic mean (asymptoticly consistent) and asymptotic variance of the estimator remain consistent, proof in Appendix A.2. Hence, any choice of weighted weights is optimal. To further streamline optimization, we set \(W=I\).

**Proposition 3.2** (Any weighted matrix is optimal for \(d=N\)).: _Assume the number of parameters \(d\) equals the number of moments condition \(N\), and \(\epsilon_{b}^{\alpha}(x_{t},t)\overset{p}{\sim}\mathbb{E}_{q(x_{0}|x_{t})}[ \text{diag}(\epsilon\otimes^{n-1}\epsilon)]\) (where \(\otimes^{n}\) denotes \(n\)-fold outers product) which denotes \(n\)-th order noise network converging in probability. The asymptotic variance \(\mathbb{V}(\theta_{GMM})\) and the convergence speed of GMM remain the same no matter which weighted matrix is adopted in Eq. (8). Namely, any weighted matrix is optimal._

What's more, we provide a detailed discussion on the selection of parameters such as the choice of different parameters to optimize and the different choices of weight \(w_{i}\) in the Gaussian mixture in Appendix E.1. Combining with Sec. 4.1, our empirical findings illustrate the efficacy of the GMS across various benchmarks via using the Gaussian transition kernel in Eq. (9) fitted by the objective function shown in Eq. (31) in Appendix B.1 via the ADAN [41] as optimization method. Details regarding the parameters for this optimizer are provided in Appendix E.5.

### Estimating high-order moments for non-Gaussian reverse process

In Sec. 3.2, we have explored the methodology for determining a Gaussian mixture transition kernel given a set of moments \(M_{1},...,M_{n}\). In the subsequent section, we will present an approach for estimating these moments utilizing noise networks and elucidate the process of network learning.

Given the forward process described by Eq. (1), it can be inferred that both \(q(x_{t}|x_{s})\) and \(q(x_{s}|x_{t},x_{0})\) follow Gaussian distributions. Specifically, \(q(x_{t}|x_{s})\sim\mathcal{N}(x_{t}|a_{t|s}x_{s},\sigma_{t|s}^{2})\)[21], where \(a_{t|s}=\frac{\alpha(t)}{\alpha(s)}\) and \(\sigma_{t|s}^{2}=\sigma^{2}(t)-a_{t|s}^{2}\sigma^{2}(s)\). Consequently, we can deduce that \(\mathbb{E}_{q(x_{s}|x_{t})}[x_{s}\otimes^{n}x_{s}]=\mathbb{E}_{q(x_{0}|x_{t})q (x_{s}|x_{t},x_{0})}[x_{s}\otimes^{n}x_{s}]\), where \(\otimes^{n}\) denotes \(n\)-fold outer product. Thus, we can first utilize the Gaussian property of \(q(x_{s}|x_{t},x_{0})\) and employ an explicit formula to calculate the moments under the measure of \(q(x_{s}|x_{t},x_{0})\). What's more, we only consider the diagonal elements of higher-order moments for computational efficiency, similar to Bao et al. [2] since estimating full higher-order moments results in escalated output dimensions (e.g., quadratic growth for covariance and cubic for the third-order moments) and thus requires substantial computational demands. The expression of diagonal elements of the third-order moment of the reverse transition kernel can be derived as:

\[\hat{M}_{3}=\mathbb{E}_{q(x_{s}|x_{t})}[\text{diag}(x_{s}\otimes x _{s}\otimes x_{s})]=\mathbb{E}_{q(x_{0}|x_{t})}\mathbb{E}_{q(x_{s}|x_{t},x_{0} )}[\text{diag}(x_{s}\otimes x_{s}\otimes x_{s})]=\] (10) \[\underbrace{[(\frac{a_{t|s}\sigma_{s}^{2}}{\sigma_{t}^{2}})^{3} \text{diag}(x_{t}\otimes x_{t}\otimes x_{t})+3\lambda_{t}^{2}\frac{a_{t|s} \sigma_{s}^{2}}{\sigma_{t}^{2}}x_{t}]}_{\text{Constant term}}\] \[+\underbrace{[\underbrace{3a_{t|s}^{2}\sigma_{s}^{4}a_{s}^{2}\beta _{t|s}^{2}}_{\sigma_{t}^{2}}(\text{diag}(x_{t}\otimes x_{t}))+\frac{a_{s|0} \beta_{t|s}}{\sigma_{t}^{2}}I]\odot\mathbb{E}_{q(x_{0}|x_{t})}[x_{0}]}_{\text {Linear term in }x_{0}}\] \[+3\underbrace{\frac{a_{t|s}\sigma_{s}^{2}}{\sigma_{t}^{2}}(\frac{ a_{s|0}\beta_{t|s}}{\sigma_{t}^{2}})^{2}x_{t}\odot\mathbb{E}_{q(x_{0}|x_{t})}[ \text{diag}(x_{0}\otimes x_{0})]}_{\text{Quadratic term in }x_{0}}+\underbrace{(\frac{a_{s|0}\beta_{t|s}}{ \sigma_{t}^{2}})^{3}\mathbb{E}_{q(x_{0}|x_{t})}[\text{diag}(x_{0}\otimes x_{0 }\otimes x_{0})]}_{\text{Cubic term in }x_{0}},\]where \(\otimes\) is the outer product \(\odot\) is the Hadamard product. Additionally, \(\lambda_{t}^{2}\) corresponds to the variance of \(q(x_{s}|x_{t},x_{0})\), and further elaboration on this matter can be found in Appendix B.

In order to compute the \(n\)-th order moment \(\hat{M}_{n}\), as exemplified by the third-order moment in Eq. (10), it is necessary to evaluate the expectations \(\mathbb{E}_{q(x_{0}|x_{t})}[x_{0}],\ldots,\mathbb{E}_{q(x_{0}|x_{t})}[\text{ diag}(x_{0}\otimes^{n-1}x_{0})]\). Furthermore, by expressing \(x_{0}\) as \(x_{0}=\frac{1}{\sqrt{\alpha(t)}}(x_{t}-\sigma(t)\epsilon)\), we can decompose \(\hat{M}_{n}\) into a combination of terms \(\mathbb{E}_{q(x_{0}|x_{t})}[\epsilon],\ldots,\mathbb{E}_{q(x_{0}|x_{t})}[( \epsilon\odot^{n-1}\epsilon)]\), which are learned by neural networks. The decomposition of third-order moments \(\hat{M}_{3}\), as outlined in Eq. (29), is provided to illustrate this concept. Therefore in training, we learn several neural networks \(\{\epsilon_{\theta}^{n}\}_{n=1}^{N}\) by training on the following objective functions:

\[\min_{\left\{\epsilon_{\theta}^{n}\right\}_{n=1}^{N}}\mathbb{E}_{t}\mathbb{E }_{q(x_{0})q(\epsilon)}\left\|\epsilon^{n}-\epsilon_{\theta}^{n}(x_{t},t) \right\|_{2}^{2},\] (11)

where \(\epsilon\sim\mathcal{N}(0,I)\), \(x_{t}=\alpha(t)x_{0}+\sigma(t)\epsilon\), and \(\epsilon^{n}\) denotes \(\epsilon\odot^{n-1}\epsilon\). After training, we can use \(\{\epsilon_{\theta}^{n}\}_{n=1}^{N}\) to replace \(\{\mathbb{E}_{q(x_{0}|x_{t})}[\epsilon\odot^{n-1}\epsilon]\}_{n=1}^{N}\) and estimate the moments \(\{\hat{M}_{n}\}_{n=1}^{N}\) of reverse transition kernel.

However, in the present scenario, it is necessary to infer the network a minimum of \(n\) times in order to make a single step of GMS. To mitigate the high cost of the aforementioned overhead in sampling, we adopt the two-stage learning approach proposed by Bao et al. [2]. Specifically, in the first stage, we optimize the noise network \(\epsilon_{\theta}(x_{t},t)\) by minimizing the expression \(\min\mathbb{E}_{t}\mathbb{E}_{q(x_{0})q(\epsilon)}\left\|\epsilon-\epsilon_{ \theta}(x_{t},t)\right\|_{2}^{2}\) or by utilizing a pre-trained noise network as proposed by [14, 37]. In the second stage, we utilize the optimized network as the backbone and keep its parameters \(\theta\) fixed, while adding additional heads to generate the \(n\)-th order noise network \(\epsilon_{\theta,\phi_{n}}^{n}\left(x_{t},t\right)\).

\[\epsilon_{\theta,\phi_{n}}^{n}\left(x_{t},t\right)=\text{NN}\left(\epsilon_{ \theta}\left(x_{t},t\right),\phi_{n}\right),\] (12)

where NN is the extra head, which is a small network such as convolution layers or small attention block, parameterized by \(\phi_{n}\), details in Appendix E.3. We present the second stage learning procedure in Algorithm 1. Upon training all the heads, we can readily concatenate the outputs of different heads, as the backbone of the higher-order noise network is shared. By doing so, we obtain the assembled noise network \(f^{N}\left(x_{t},t\right)\). When estimating the \(k\)-th order moments, it suffices to extract only the first \(k\) components of the assembled noise network.

\[f^{N}\left(x_{t},t\right)=\text{concat}(\underbrace{[\epsilon_{\theta}(x_{t},t),\epsilon_{\theta,\phi_{2}}^{2}\left(x_{t},t\right),..,\epsilon_{\theta, \phi_{k}}^{k}\left(x_{t},t\right)}_{\text{Required for estimating }\hat{M}_{k}},.., \epsilon_{\theta,\phi_{N}}^{N}\left(x_{t},t\right)]),\] (13)To this end, we outline the GMS sampling process in Algorithm 2, where \(f_{[2]}^{3}(x_{t},t)\) represents \(\text{concat}([\epsilon_{\theta}(x_{t},t),\epsilon_{\theta,\phi_{2}}^{2}(x_{t},t )])\). In comparison to existing methods with the same network structure, we report the additional memory cost of the assembled noise network in Appendix E.6.

## 4 Experiment

In this section, we first illustrate that GMS exhibits superior sample quality compared to existing SDE-based solvers when using both linear and cosine noise schedules [14; 30]. Additionally, we evaluate various solvers in stroke-based image synthesis (i.e., SDEdit) and demonstrate that GMS surpasses other SDE-based solvers, as well as the widely used ODE-based solver DDIM [36].

### Sample quality on image data

In this section, we conduct a quantitative comparison of sample quality using the widely adopted FID score [13]. Specifically, we evaluate multiple SDE-based solvers, including a comparison with DDPM [14] and Extended AnalyticDPM [2] (referred to as SN-DDPM) using the even trajectory.

As shown in Tab. 1, GMS demonstrates superior performance compared to DDPM and SN-DDPM under the same number of steps in CIFAR10 and ImageNet 64 \(\times\) 64. Specifically, GMS achieves a remarkable 4.44 improvement in FID given 10 steps on CIFAR10. Appendix E.7 illustrates in more detail the improvement of GMS when the number of sampling steps is limited. Meanwhile, we conduct GMS in ImageNet 256 \(\times\) 256 via adopting the U-ViT-Huge [4] backbone as the noise network in Appendix E.8. Furthermore, taking into account the additional time required by GMS, our method still exhibits improved performance, as detailed in Appendix E.9. For integrity, we provide a comparison with other SDE-based solvers based on continuous time diffusion such as Gotta Go Fast [17], EDM [18] and SEED [10] in Appendix E.10 and shows that GMS largely outperforms other SDE-based solvers when the number of steps is less than 100. In Appendix G, we provide generated samples from GMS.

### Stroke-based image synthesis based on SDEdit [27]

**Evaluation metrics.** We evaluate the editing results based on realism and faithfulness similar with Meng et al. [27]. To quantify the realism of sample images, we use FID between the generated images and the target realistic image dataset. To quantify faithfulness, we report the \(L_{2}\) distance summed over all pixels between the stroke images and the edited output images.

SDEdit [27] applies noise to the stroke image \(x^{g}\) at time step \(t_{0}\) using \(\mathcal{N}(x_{t_{0}}^{g}|\alpha(t)x^{g},\sigma^{2}(t)I)\) and discretize the reverse SDE in Eq. (4) for sampling. Fig. 9 demonstrates the significant impact of \(t_{0}\) on the realism of sampled images. As \(t_{0}\) increases, the similarity to real images decreases. we choose

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline  & \multicolumn{8}{c}{CIFAR10 (LS)} \\ \cline{2-10} \# timesteps \(K\) & 10 & 20 & 25 & 40 & 50 & 100 & 200 & 1000 \\ \hline DDPM, \(\tilde{\beta}_{t}\) & 43.14 & 25.28 & 21.63 & 15.24 & 15.21 & 10.94 & 8.23 & 5.11 \\ DDPM, \(\beta_{t}\) & 233.41 & 168.22 & 125.05 & 82.31 & 66.28 & 31.36 & 12.96 & 3.04 \\ SN-DDPM & 21.87 & 8.32 & 6.91 & 4.99 & 4.58 & 3.74 & 3.34 & 3.71 \\ GMS (**ours**) & **17.43** & **7.18** & **5.96** & **4.52** & **4.16** & **3.26** & **3.01** & **2.76** \\ \hline \hline \end{tabular} 
\begin{tabular}{l c c c c c c c c c c c} \hline \hline  & \multicolumn{8}{c}{CIFAR10 (CS)} \\ \cline{2-10} \# timesteps \(K\) & 10 & 25 & 50 & 100 & 200 & 1000 & 25 & 50 & 100 & 200 & 400 & 4000 \\ \hline DDPM, \(\tilde{\beta}_{t}\) & 34.76 & 16.18 & 11.11 & 8.38 & 6.66 & 4.92 & 29.21 & 21.71 & 19.12 & 17.81 & 17.48 & 16.55 \\ DDPM, \(\beta_{t}\) & 205.31 & 84.71 & 37.35 & 14.81 & 5.74 & **3.34** & 170.28 & 83.86 & 45.04 & 28.39 & 21.38 & 16.38 \\ SN-DDPM & 16.33 & 6.05 & 4.19 & 3.83 & 3.72 & 4.08 & 27.58 & 20.74 & 18.04 & 16.72 & 16.37 & 16.22 \\ GMS (**ours**) & **13.80** & **5.48** & **4.00** & **3.46** & **3.34** & 4.23 & **26.50** & **20.13** & **17.29** & **16.60** & **15.98** & **15.79** \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Comparison with competitive SDE-based solvers w.r.t. FID score \(\downarrow\) on CIFAR10 and ImageNet 64\(\times\)64. our GMS outperforms existing SDE-based solvers in most cases. SN-DDPM denotes Extended AnalyticDPM from Bao et al. [2].**the range from \(t_{0}=0.3T\) to \(t_{0}=0.5T\) (\(T=1000\) in our experiments) for our further experiments since sampled images closely resemble the real images in this range.

Fig. 3(a) illustrates that when using the same \(t_{0}\) and the same number of steps, edited output images from GMS have lower faithfulness but with higher realism. This phenomenon is likely attributed to the Gaussian noise introduced by the SDE-based solver during each sampling step. This noise causes the sampling to deviate from the original image (resulting in low faithfulness) but enables the solver to transition from the stroke domain to the real image domain. Fig. 3(b) to Fig. 3(d) further demonstrates this phenomenon to a certain extent. The realism of the sampled images generated by the SDE-based solver escalates with an increase in the number of sampling steps. Conversely, the realism of the sampled images produced by the ODE-based solver diminishes due to the absence of noise, which prevents the ODE-based solver from transitioning from the stroke domain to the real image domain. Additionally, in the SDEdit task, GMS exhibits superior performance compared to SN-DDPM [2] in terms of sample computation cost. Fig. 4 shows the samples using DDIM and GMS when \(t_{0}=400\) and the number of steps is \(40\).

## 5 Related work

**Faster solvers.** In addition to SDE-based solvers, there are works dedicated to improving the efficiency of ODE-based solvers [25; 23; 8]. Some approaches use explicit reverse transition kernels, such as those based on generative adversarial networks proposed by Xiao et al. [40] and Wang et al. [39]. Gao et al. [9] employ an energy function to model the reverse transition kernel. Zhang and Chen [44] use a flow model for the transition kernel.

**Non-Gaussian diffusion.** Apart from diffusion, some literature suggests using non-Gaussian forward processes, which consequently involve non-Gaussian reverse processes. Bansal et al. [1] introduce a generalized noise operator that incorporates noise. Nachmani et al. [29] incorporate Gaussian mixture or Gamma noise into the forward process. While these works replace both the forward and reverse processes with non-Gaussian distributions, our approach aims to identify a suitable combination of non-Gaussian distributions to model the reverse process.

## 6 Conclusion

This paper presents a novel Gaussian mixture solver (GMS) for diffusion models. GMS relaxes the Gaussian reverse kernel assumption to reduce discretization errors and improves the sample quality under the same sampling steps. Experimental results show that GMS outperforms existing SDE-based solvers, achieving a remarkable 4.44 improvement in FID compared to the state-of-the-art SDE-based solver proposed by Bao et al. [2] given 10 steps. Furthermore, due to the presence of noise, SDE-based solvers prove more suitable for stroke-based synthesis tasks and GMS still outperforms state-of-the-art SDE-based solvers.

Figure 3: **Result among different solvers in SDEdit. \(t_{0}\) denotes the time step of the start of reverse. (a): The points on each line represent the same \(t_{0}\) and the number of sampling steps. We select \(t_{0}=[300,400,500]\), number of steps \(=[50,100]\). (b): When \(t_{0}=300\), the effect of DDIM diminishes more prominently with the increase in the number of steps.**

**Limitations and broader impacts.** While GMS enhances sample quality and potentially accelerates inference speed compared to existing SDE-based solvers, employing GMS still fall short of real-time applicability. Like other generative models, diffusion models can generate problematic fake content, and the use of GMS may amplify these undesirable effects.

## Acknowledgement

This work was supported by NSF of China (Nos. 62076145); Beijing Outstanding Young Scientist Program (No. BJJWZYJH012019100020098); Major Innovation & Planning Interdisciplinary Platform for the "Double-First Class" Initiative, Renmin University of China; the Fundamental Research Funds for the Central Universities, and the Research Funds of Renmin University of China (No. 22XNKJ13). C. Li was also sponsored by Beijing Nova Program (No. 20220484044).

## References

* [1] Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Cold diffusion: Inverting arbitrary image transforms without noise. _arXiv preprint arXiv:2208.09392_, 2022.
* [2] Fan Bao, Chongxuan Li, Jiacheng Sun, Jun Zhu, and Bo Zhang. Estimating the optimal covariance with imperfect mean in diffusion probabilistic models. _arXiv preprint arXiv:2206.07309_, 2022.
* [3] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-dpm: an analytic estimate of the optimal reverse variance in diffusion probabilistic models. _arXiv preprint arXiv:2201.06503_, 2022.

Figure 4: **SDEdit samples of GMS and DDIM. Due to the presence of noise, SDE-based solvers, such as GMS OURS, generate images with more details.*** [4] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: A vit backbone for diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22669-22679, 2023.
* [5] Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, and Jun Zhu. One transformer fits all distributions in multi-modal diffusion at scale. _arXiv preprint arXiv:2303.06555_, 2023.
* [6] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. _arXiv preprint arXiv:2003.04297_, 2020.
* [7] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _Advances in Neural Information Processing Systems_, 34:8780-8794, 2021.
* [8] Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Genie: Higher-order denoising diffusion solvers. _arXiv preprint arXiv:2210.05475_, 2022.
* [9] Ruiqi Gao, Yang Song, Ben Poole, Ying Nian Wu, and Diederik P Kingma. Learning energy-based models by diffusion recovery likelihood. _arXiv preprint arXiv:2012.08125_, 2020.
* [10] Martin Gonzalez, Nelson Fernandez, Thuy Tran, Elies Gherbi, Hatem Hajri, and Nader Masmoudi. Seeds: Exponential sde solvers for fast high-quality sampling from diffusion models. _arXiv preprint arXiv:2305.14267_, 2023.
* [11] Lars Peter Hansen. Large sample properties of generalized method of moments estimators. _Econometrica: Journal of the econometric society_, pages 1029-1054, 1982.
* [12] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. _arXiv preprint arXiv:2208.01626_, 2022.
* [13] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. _Advances in neural information processing systems_, 30, 2017.
* [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.
* [15] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. _J. Mach. Learn. Res._, 23(47):1-33, 2022.
* [16] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. _arXiv preprint arXiv:2204.03458_, 2022.
* [17] Alexia Jolicoeur-Martineau, Ke Li, Remi Piche-Taillefer, Tal Kachman, and Ioannis Mitliagkas. Gotta go fast when generating data with score-based models. _arXiv preprint arXiv:2105.14080_, 2021.
* [18] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. _arXiv preprint arXiv:2206.00364_, 2022.
* [19] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models. _arXiv preprint arXiv:2201.11793_, 2022.
* [20] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. _arXiv preprint arXiv:2210.09276_, 2022.
* [21] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. _Advances in neural information processing systems_, 34:21696-21707, 2021.
* [22] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. _arXiv preprint arXiv:2009.09761_, 2020.

* [23] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. _arXiv preprint arXiv:2202.09778_, 2022.
* [24] Cheng Lu, 2023. https://github.com/huggingface/diffusers/pull/3344.
* [25] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. _arXiv preprint arXiv:2206.00927_, 2022.
* [26] Geoffrey J McLachlan and Thriyambakam Krishnan. _The EM algorithm and extensions_. John Wiley & Sons, 2007.
* [27] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Image synthesis and editing with stochastic differential equations. _arXiv preprint arXiv:2108.01073_, 2021.
* [28] Eliya Nachmani and Shaked Dovrat. Zero-shot translation using diffusion models. _arXiv preprint arXiv:2111.01471_, 2021.
* [29] Eliya Nachmani, Robin San Roman, and Lior Wolf. Non gaussian denoising diffusion models. _arXiv preprint arXiv:2106.07582_, 2021.
* [30] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In _International Conference on Machine Learning_, pages 8162-8171. PMLR, 2021.
* [31] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. _arXiv preprint arXiv:2209.14988_, 2022.
* [32] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-tts: A diffusion probabilistic model for text-to-speech. In _International Conference on Machine Learning_, pages 8599-8608. PMLR, 2021.
* [33] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. _arXiv preprint arXiv:1710.05941_, 2017.
* [34] Alex Shonenkov, Misha Konstantinov, Daria Bakshandaeva, Christoph Schuhmann, Ksenia Ivanova, and Nadiia Klokova, 2023. https://github.com/deep-floyd/IF.
* [35] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Machine Learning_, pages 2256-2265. PMLR, 2015.
* [36] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_, 2020.
* [37] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.
* [38] Pascal Vincent. A connection between score matching and denoising autoencoders. _Neural computation_, 23(7):1661-1674, 2011.
* [39] Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Diffusion-gan: Training gans with diffusion. _arXiv preprint arXiv:2206.02262_, 2022.
* [40] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion gans. _arXiv preprint arXiv:2112.07804_, 2021.
* [41] Xingyu Xie, Pan Zhou, Huan Li, Zhouchen Lin, and Shuicheng Yan. Adan: Adaptive nesterov momentum algorithm for faster optimizing deep models. _arXiv preprint arXiv:2208.06677_, 2022.
* [42] Xingqian Xu, Zhangyang Wang, Eric Zhang, Kai Wang, and Humphrey Shi. Versatile diffusion: Text, images and variations all in one diffusion model. _arXiv preprint arXiv:2211.08332_, 2022.

* [43] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. _arXiv preprint arXiv:2302.05543_, 2023.
* [44] Qinsheng Zhang and Yongxin Chen. Diffusion normalizing flow. _Advances in Neural Information Processing Systems_, 34:16280-16291, 2021.
* [45] Min Zhao, Fan Bao, Chongxuan Li, and Jun Zhu. Egsde: Unpaired image-to-image translation via energy-guided stochastic differential equations. _arXiv preprint arXiv:2207.06635_, 2022.

Proof

### Proof of Proposition 3.1

When \(q(x_{0})\) is a mixture of Dirac distribution which means that \(q(x_{0})=\sum_{i=1}^{M}w_{i}\delta(x-x_{i}),\sum_{i=1}^{M}w_{i}=1\), which has total \(M\) components, and when the forward process \(q(x_{t}|x_{0})\) is a Gaussian distribution as Eq. (1), the reverse process \(q(x_{s}|x_{t})\) would be:

\[q(x_{s}|x_{t})=\int q(x_{s}|x_{t},x_{0})q(x_{0}|x_{t})dx_{0}=\int q (x_{s}|x_{t},x_{0})q(x_{0})q(x_{t}|x_{0})/q(x_{t})dx_{0}\] \[=1/q(x_{t})\int q(x_{s}|x_{t},x_{0})q(x_{0})q(x_{t}|x_{0})dx_{0}=1 /q(x_{t})\sum_{i=1}^{M}w_{i}q(x_{s}|x_{t},x_{0}^{i})q(x_{t}|x_{0}^{i})\]

According to the definition of forward process, the distribution \(q(x_{t}|x_{0}^{i})=\mathcal{N}(x_{t}|\sqrt{\bar{a}_{t}}x_{0}^{i},(1-\bar{a}_{ t})I)\). Due to the Markov property of forward process, when \(t>s\), we have \(q(x_{s},x_{t}|x_{0})q(x_{s}|x_{0})q(x_{t}|x_{s})\). The term \(q(x_{s}|x_{t},x_{0})\) would be viewed as a Bayesian posterior resulting from a prior \(q(x_{s}|x_{0})\), updated with a likelihood term \(q(x_{t}|x_{s})\). And therefore

\[q(x_{s}|x_{t},x_{0}^{i})=N(x_{s}|\mu_{q}(x_{t},x_{0}),\Sigma_{q}(x_{t},x_{0})I )=\mathcal{N}(x_{s}|\frac{a_{t|s}\sigma_{s}^{2}}{\sigma_{t}^{2}}x_{t}+\frac{a _{s}\sigma_{t|s}^{2}}{\sigma_{t}^{2}}x_{0},\frac{\sigma_{s}^{2}\sigma_{t|s}^{ 2}}{\sigma_{t}^{2}}I)\]

It is easy to prove that, the distribution \(q(x_{s}|x_{t})\) is a mixture of Gaussian distribution:

\[q(x_{s}|x_{t})\propto\sum_{i=1}^{M}w_{i}q(x_{s}|x_{t},x_{0}^{i})q (x_{t}|x_{0}^{i})\] \[=\sum_{i=1}^{M}w_{i}\mathcal{N}(x_{t}|\sqrt{\bar{a}_{t}}x_{0}^{i},(1-\bar{a}_{t})I)*\mathcal{N}(x_{s}|\mu_{q}(x_{t},x_{0}),\Sigma_{q}(x_{t},x_{ 0}))\]

When \(t\) is large, \(s\) is small, \(\sigma_{t|s}^{2}\) would be large, meaning that the influence of \(x_{0}^{i}\) would be large.

Secondly, when \(q(x_{0})\) is a mixture of Gaussian distribution which means that \(q(x_{0})=\sum_{i=1}^{M}w_{i}\mathcal{N}(x_{0}^{i}|\mu_{i},\Sigma_{i}),\sum_{i= 1}^{M}w_{i}=1\). For simplicity of analysis, we assume that this distribution is a one-dimensional distribution or that the covariance matrix is a high-dimensional Gaussian distribution with a diagonal matrix \(\Sigma_{i}=\text{diag}_{i}(\sigma^{2})\). Similar to the situation above, for each dimension in the reverse process:

\[q(x_{s}|x_{t})=1/q(x_{t})\int q(x_{s}|x_{t},x_{0})q(x_{0})q(x_{t }|x_{0})dx_{0}\] \[=\sum_{i=1}^{M}w_{i}/q(x_{t})\int q(x_{s}|x_{t},x_{0})\mathcal{N} (x_{0}|\mu_{i},\Sigma_{i})q(x_{t}|x_{0})dx_{0}\] \[=\sum_{i=1}^{M}w_{i}/q(x_{t})\int\frac{1}{\sqrt{2\pi}\sigma_{q}} e^{-\frac{(x_{t}^{i}-\mu_{q})^{2}}{\sigma_{q}^{2}}}\frac{1}{\sqrt{2\pi}\sigma_{t}} e^{-\frac{(x_{t}^{i}-\mu_{t})^{2}}{\sigma_{t}^{2}}}\frac{1}{\sqrt{2\pi}\sqrt{1- \bar{a}_{t}}}e^{-\frac{(x_{t}-\sqrt{\pi}\pi\bar{a}_{t})^{2}}{1-\bar{a}_{t}}}dx _{0}\] \[=\sum_{i=1}^{M}w_{i}/q(x_{t})\int Z_{i}\frac{1}{\sqrt{2\pi}\sigma_{ x}}e^{-\frac{(x_{t}^{i}-\mu_{q})^{2}}{\sigma_{q}^{2}}}e^{-\frac{(x_{s}-\mu(x_{t}))^{2}}{ \sigma(x_{t})}}dx_{0}\]

And \(q(x_{s}|x_{t})\) could be a Gaussian mixture which has \(M\) component.

### Proof of Proposition 3.2

Recall that our objective function is to find optimal parameters:

\[\hat{\theta}=\underset{\theta}{\text{argmin}}[\underbrace{Q_{N_{c}}(\theta)}_ {1\times 1}]=\underset{\theta}{\text{argmin}}[\underbrace{g_{N_{c}}(\theta)^{T}}_{1 \times N}\underbrace{W_{N_{c}}g_{N_{c}}(\theta)}_{1\times N}]\] (14)where \(g_{M}(\theta)\) is the moment conditions talked about in Sec. 3.2, \(W\) is the weighted Matrix, \(N_{c}\) is the sample size. When solving such problems using optimizers, which are equivalent to the one we used when selecting \(\theta_{GMM}\) such that \(\frac{\partial Q_{T}(\theta_{GMM})}{\partial\theta}=0\), and its first derivative:

\[\underbrace{\frac{\partial Q_{N_{c}}(\theta)}{\partial\theta}}_{d\times 1}= \begin{pmatrix}\frac{\partial Q_{N_{c}}(\theta)}{\partial\theta_{1}}\\ \frac{\partial Q_{N_{c}}(\theta)}{\partial\theta_{2}}\\ \frac{\partial Q_{N_{c}}(\theta)}{\partial\theta_{3}}\end{pmatrix},\frac{ \partial Q_{N_{c}}(\theta)}{\partial\theta_{m}}=2\underbrace{[\frac{1}{N_{c} }\sum_{i=1}^{N_{c}}\frac{\partial g(x_{i},\theta)}{\partial\theta_{m}}]^{T}} _{1\times N}\underbrace{W_{N_{c}}[\frac{1}{N_{c}}\sum_{i=1}^{M}g(x_{i},\theta )]}_{N\times 1}\] (15)

its second derivative (the Hessian matrix):

\[\underbrace{\frac{\partial^{2}Q_{N_{c}}(\theta)}{\partial\theta^{2}}}_{d \times d}=\begin{pmatrix}\frac{\partial^{2}Q_{N_{c}}(\theta)}{\partial\theta_ {1}}&\frac{\partial^{2}Q_{N_{c}}(\theta)}{\partial\theta_{1}\theta_{2}}&...& \frac{\partial^{2}Q_{N_{c}}(\theta)}{\partial\theta_{1}\theta_{d}}\\ \frac{\partial^{2}Q_{N_{c}}(\theta)}{\partial\theta^{2}\theta_{1}}&...&...&....\\...&...&...&..\\...&...&...&\frac{\partial^{2}Q_{N_{c}}(\theta)}{\partial\theta_{d}\theta_{d} }\end{pmatrix}\] (16)

\[,\frac{\partial^{2}Q_{N_{c}}(\theta)}{\partial\theta_{i}\theta_{j}} =2[\frac{1}{M}\sum_{i=1}^{N_{c}}\frac{\partial g(x_{i},\theta)}{ \partial\theta_{i}}]^{T}W_{N_{c}}[\frac{1}{N_{c}}\sum_{i=1}^{N_{c}}\frac{ \partial g(x_{i},\theta)}{\partial\theta_{j}}]\] \[+2[\frac{1}{N_{c}}\sum_{i=1}^{N_{c}}\frac{\partial^{2}g(x_{i}, \theta)}{\partial\theta_{i}\theta_{j}}]W_{N_{c}}[\frac{1}{N_{c}}\sum_{i=1}^{N _{c}}g(x_{i},\theta)].\]

By Taylor's expansion of the gradient around optimal parameters \(\theta_{0}\), we have:

\[\frac{\partial Q_{N_{c}}(\theta_{GMM})}{\partial\theta}-\frac{ \partial Q_{N_{c}}(\theta_{0})}{\partial\theta}\approx\frac{\partial^{2}Q_{N _{c}}(\theta_{0})}{\partial\theta\partial\theta^{T}}(\theta_{GMM}-\theta)\] (17) \[\longmapsto(\theta_{GMM}-\theta)\approx-(\frac{\partial^{2}Q_{N_{ c}}(\theta_{0})}{\partial\theta\partial\theta^{T}})^{-1}\frac{\partial Q_{N_{c}}( \theta_{0})}{\partial\theta}.\]

Consider one element of the gradient vector \(\frac{\partial Q_{N_{c}}(\theta_{0})}{\partial\theta_{m}}\)

\[\frac{\partial Q_{N_{c}}(\theta_{0})}{\partial\theta_{m}}=2[\frac{1}{M}\sum_{i =1}^{N_{c}}\frac{\partial g(x_{i},\theta_{0})}{\partial\theta_{m}}]^{T}\underbrace {W_{N_{c}}[\frac{1}{N_{c}}\sum_{i=1}^{N_{c}}g(x_{i},\theta_{0})]}_{\frac{P}{P }\mathbb{E}[g(x_{i},\theta_{0})]=0}.\] (18)

Consider one element of the Hessian matrix \(\frac{\partial^{2}Q_{N_{c}}(\theta_{0})}{\partial\theta_{i}\partial\theta_{j}}\)

\[\frac{\partial^{2}Q_{N_{c}}(\theta_{0})}{\partial\theta_{i}\partial \theta_{j}^{T}} =2[\frac{1}{N_{c}}\sum_{i=1}^{N_{c}}\frac{\partial g(x_{i},\theta_{ 0})}{\partial\theta_{i}}]^{T}W_{N_{c}}[\frac{1}{N_{c}}\sum_{i=1}^{N_{c}}\frac{ \partial g(x_{i},\theta_{0})}{\partial\theta_{j}}]\] (19) \[+2[\frac{1}{N_{c}}\sum_{i=1}^{N_{c}}\frac{\partial^{2}g(x_{i}, \theta_{0})}{\partial\theta_{i}\theta_{j}}]^{T}W_{N_{c}}[\frac{1}{N_{c}}\sum_{i= 1}^{N_{c}}g(x_{i},\theta_{0})]\stackrel{{ p}}{{\to}}2\Gamma_{0,i}^{T}W \Gamma_{0,j}.\]

Therefore, it is easy to prove that \(\theta_{GMM}-\theta_{0}\stackrel{{ p}}{{\to}}0\), and uses law of large numbers we could obtain,

\[\sqrt{T}\frac{\partial Q_{T}(\theta_{0})}{\partial\theta_{m}}=2[\frac{1}{M} \sum_{i=1}^{M}\frac{\partial g(x_{i},\theta_{0})}{\partial\theta_{m}}]^{T} \underbrace{W_{M}[}_{\stackrel{{ p}}{{\to}}\mathbb{E}(\frac{ \partial g(X,\theta_{0})}{\partial\theta_{m}})=\Gamma_{0,m}}]^{T}\underbrace {W_{M}[}_{\stackrel{{ d}}{{\to}}\mathcal{N}(0,\mathbb{E}(g(X, \theta_{0})g(X,\theta_{0})^{T}))}]\stackrel{{ d}}{{\to}}2\Gamma_{0,m}^{T}W \mathcal{N}(0,\Phi_{0}),\] (20)

therefore, we have

\[\sqrt{T}(\theta_{GMM}-\theta_{0})\approx-(\frac{\partial^{2}Q_{ T}(\theta_{0})}{\partial\theta\theta\partial\theta^{T}})^{-1}\sqrt{T}\frac{ \partial Q_{T}(\theta_{0})}{\partial\theta}\] (21) \[\stackrel{{ d}}{{\to}}\mathcal{N}(0,(\Gamma_{0}^{T}W \Gamma_{0})^{-1}\Gamma_{0}^{T}W\Phi_{0}W\Gamma_{0}(\Gamma_{0}^{T}W\Gamma_{0})^{-1 }).\]When the number of parameters \(d\) equals the number of moment conditions \(N\), \(\Gamma_{0}\) becomes a (nonsingular) square matrix, and therefore,

\[\sqrt{T}(\theta_{GMM}-\theta_{0}) \stackrel{{ d}}{{\rightarrow}}\mathcal{N}(0,(\Gamma_{ 0}^{T}W\Gamma_{0})^{-1}\Gamma_{0}^{T}W\Phi_{0}W\Gamma_{0}(\Gamma_{0}^{T}W \Gamma_{0})^{-1})\] (22) \[=\mathcal{N}(0,\Gamma_{0}^{-1}W^{-1}(\Gamma_{0}^{T})^{-1}\Gamma_ {0}^{T}W\Phi_{0}W\Gamma_{0}\Gamma_{0}^{-1}W^{-1}(\Gamma_{0}^{T})^{-1})\] \[=\mathcal{N}(0,\Gamma_{0}^{-1}\Phi_{0}(\Gamma_{0}^{T})^{-1}),\]

which means that the foregoing observation suggests that the selection of \(W_{N_{c}}\) has no bearing on the asymptotic variance of the GMM estimator. Consequently, it implies that regardless of the specific method employed to determine \(W_{N_{c}}\), provided the moment estimates are asymptotically consistent, \(W_{N_{c}}\) serves as the optimal weight matrix, even when dealing with small samples.

The moments utilized for fitting via the generalized method of moments in each step are computed based on the noise network's first \(n\)-th order. To ensure adherence to the aforementioned proposition, it is necessary to assume that the \(n\)-th order of the noise network converges with probability to \(\mathbb{E}_{q(x_{0}|x_{n})}[\text{diag}(\epsilon\otimes^{n-1}\epsilon)]\), details in Appendix B. Consequently, the \(n\)-th order moments derived from the noise networks converge with probability to the true moments. Therefore, any choice of weight matrix is optimal.

### Non-Gaussian distribution of transition kernel within large discretization steps

we can apply Bayes' rule to the posterior distribution \(q(x_{t}|x_{t+\Delta t})\) as follows:

\[q(x_{t}|x_{t+\Delta t}) =\frac{q(x_{t+\Delta t}|x_{t})q(x_{t})}{q(x_{t+\Delta t})}=q(x_{t+ \Delta t}|x_{t})\exp(\log(q(x_{t}))-\log(q(x_{t+\Delta t})))\] (23) \[\propto\exp\Biggl{(}-\frac{\|x_{t+\Delta t}-x_{t}-f_{t}(x_{t}) \Delta t\|^{2}}{2g_{t}^{2}\Delta t}+\log p(x_{t})-\log(x_{t+\Delta t})\Biggr{)},\]

where \(\Delta t\) is the step size, \(q(x_{t})\) is the marginal distribution of \(x_{t}\). When \(x_{t+\Delta t}\) and \(x_{t}\) are close enough, using Taylor expansion for \(\log p(x_{t+\Delta t})\), we could obtain:

\[\log p(x_{t+\Delta t}) \approx\log p(x_{t})+(x_{t+\Delta t}-x_{t})\nabla_{x_{t}}\log p( x_{t})+\Delta t\frac{\partial}{\partial t}\log p(x_{t}),\] (24) \[q(x_{t}|x_{t+\Delta t}) \propto\exp\Biggl{(}-\frac{\left\|x_{t+\Delta t}-x_{t}-[f_{t}(x_ {t})-g_{t}^{2}\nabla_{x_{t}}\log p(x_{t})]\Delta t\right\|^{2}}{2g_{t}^{2} \Delta t}+O(\Delta t)\Biggr{)}.\] (25)

By ignoring the higher order terms, the reverse transition kernel will be Gaussian distribution. However, as \(\Delta t\) increases, the higher-order terms in the Taylor expansion cannot be disregarded, which causes the reverse transition kernel to deviate from a Gaussian distribution.

Empirically, from Fig. 2 in our paper, we observe that as \(T\) decreases, the reverse transition kernel increasingly deviates from a Gaussian distribution. For more details, please refer to Appendix D.2.

## Appendix B Calculation of the first order moment and higher order moments

Suppose the forward process is a Gaussian distribution same with the Eq. (1) as \(q(x_{t}|x_{s})=N(x_{t}|a(t)x_{t-1},\sigma(t)I)\).

And let \(1\geq t>s\geq 0\) always satisfy, \(q(x_{t}|x_{s})=N(x_{t}|a_{t|s}x_{s},\beta_{t|s}I)\), where \(a_{t|s}=a_{t}/a_{s}\) and \(\beta_{t|s}=\sigma_{t}^{2}-a_{t|s}^{2}\sigma_{s}^{2}\), \(\sigma_{s}=\sqrt{1-a(s)}\),\(\sigma_{t}=\sqrt{1-a(t)}\). It's easy to prove that the distribution \(q(x_{t}|x_{0})\), \(q(x_{s}|x_{t},x_{0})\) are also a Gaussian distribution [21]. Therefore, the mean of \(x_{s}\) under the measure \(q(x_{s}|x_{t})\) would be

\[\mathbb{E}_{q(x_{s}|x_{t})}[x_{s}] =\mathbb{E}_{q(x_{0}|x_{t})}E_{q(x_{s}|x_{t},x_{0})}[x_{s}]\] (26) \[=\mathbb{E}_{q(x_{0}|x_{t})}[\frac{1}{a_{t|s}}(x_{t}-\frac{\beta _{t|s}}{\sigma_{t}}\epsilon_{t})]\] \[=\frac{1}{a_{t|s}}(x_{t}-\frac{\beta_{t|s}}{\sigma_{t}}\mathbb{E}_ {q(x_{0}|x_{t})}[\epsilon_{t}]).\]

[MISSING_PAGE_FAIL:17]

the \(n\)-order moment, we will use \(\mathbb{E}_{q(x_{0}|x_{1})}[\epsilon_{t}],...,\mathbb{E}_{q(x_{0}|x_{t})}[\epsilon_ {t}\odot^{n-1}\epsilon_{t}]\). Bao et al. [2] put forward using a sharing network and using the MSE loss to estimate the network to obtain the above information about different orders of noise.

The function \(h(f_{[1]}^{3},f_{[2]}^{3},f_{[3]}^{3})\) in Algo. 2 is defined as \(h(f_{[1]}^{3}(x_{t},t),f_{[2]}^{3}(x_{t},t),f_{[3]}^{3}(x_{t},t))=M_{1}(f_{[1] }^{3}(x_{t},t)),M_{2}(f_{[2]}^{3}(x_{t},t)),M_{3}(f_{[3]}^{3}(x_{t},t))\), where \(M_{1}(f_{[1]}^{3}(x_{t},t))=\mathbb{E}_{q(x_{s}|x_{t})}[x_{s}]\) in Eq. (26), \(M_{2}(f_{[2]}^{3}(x_{t},t))=\text{Cov}_{q(x_{s}|x_{t})}[x_{s}]\) in Eq. (27) and \(M_{3}(f_{[3]}^{3}(x_{t},t))=\hat{M}_{3}=\mathbb{E}_{q(x_{s}|x_{t})}[\text{diag }(x_{s}\otimes x_{s})]\) in Eq. (28)

### Objective function for Gaussian mixture transition kernel with two components

Recall that the general objective function to fit a Gaussian mixture transition kernel in each sampling step via GMM is shown in Eq. (30).

\[\min_{\theta}Q(\theta,M_{1},...,M_{N})=\min_{\theta}[\frac{1}{N_{c}}\sum_{i=1 }^{N_{c}}g(x_{i},\theta)]^{T}W[\frac{1}{N_{c}}\sum_{i=1}^{N_{c}}g(x_{i},\theta )],\] (30)

where \(A^{T}\) denotes the transpose matrix of matrix A. In our paper, we propose to use the first three moments to fit a Gaussian mixture transition kernel \(p(x_{s}|x_{t})=\frac{1}{3}\mathcal{N}(\mu_{t}^{(1)},\sigma_{t}^{2})+\frac{2}{ 3}\mathcal{N}(\mu_{t}^{(2)},\sigma_{t}^{2})\) in each sampling step from \(x_{t}\) to \(x_{s}\). Therefore, the final objective function as follow:

\[\min_{\mu_{t}^{(1)},\mu_{t}^{(2)},\sigma_{t}^{2}}[\begin{pmatrix}M _{1}-(\frac{1}{3}\mu_{t}^{(1)}+\frac{2}{3}\mu_{t}^{(2)})\\ M_{2}-(\frac{1}{3}[(\mu_{t}^{(1)})^{2}+\sigma_{t}^{2}]+\frac{2}{3}[(\mu_{t}^{(2 )})^{2}+\sigma_{t}^{2}])\\ M_{3}-(\frac{1}{3}[(\mu_{t}^{(1)})^{3}+3\mu_{t}^{(1)}\sigma_{t}^{2}]+\frac{2}{3 }[(\mu_{t}^{(2)})^{3}+3\mu_{t}^{(2)}\sigma_{t}^{2}])\end{pmatrix}]^{T}I\] (31) \[[\begin{pmatrix}M_{1}-(\frac{1}{3}\mu_{t}^{(1)}+\frac{2}{3}\mu_{t} ^{(2)})\\ M_{2}-(\frac{1}{3}[(\mu_{t}^{(1)})^{2}+\sigma_{t}^{2}]+\frac{2}{3}[(\mu_{t}^{(2 )})^{2}+\sigma_{t}^{2}])\\ M_{3}-(\frac{1}{3}[(\mu_{t}^{(1)})^{3}+3\mu_{t}^{(1)}\sigma_{t}^{2}]+\frac{2}{3 }[(\mu_{t}^{(2)})^{3}+3\mu_{t}^{(2)}\sigma_{t}^{2}]),\end{pmatrix}]\]

where to simplify the analysis, we use the scalar form of parameters \(\mu_{t}^{(1)},\mu_{t}^{(2)},\sigma_{t}^{2}\) as representation.

## Appendix C Modeling reverse transition kernel via exponential family

Analysis in Sec. 3.1 figures out that modeling reverse transition kernel via Gaussian distribution is no longer sufficient in fast sampling scenarios. In addition to directly proposing the use of a Gaussian Mixture for modeling, we also analyze in principle whether there are potentially more suitable distributions i.e., the feasibility of using them for modeling.

We would turn back to analyzing the original objective function of DPMs to find a suitable distribution. The forward process \(q(x_{t}|x_{s})=N(x_{t}|a_{t|s}x_{s},\beta_{t|s}I)\), consistent with the definition in Appendix B. DPMs' goal is to optimize the modeled reverse process parameters to maximize the variational bound \(L\) in Ho et al. [14]. And the ELBO in Ho et al. [14] can be re-written to the following formula:

\[L=D_{\mathrm{KL}}(q(x_{T})||p(x_{T}))+\mathbb{E}_{q}[\sum_{t\geq 1}D_{\mathrm{KL} }(q(x_{s}|x_{t})||p(x_{s}|x_{t}))]+H(x_{0}),\] (32)

where \(q_{t}\doteq q(x_{t})\) is the true distribution and \(p_{t}\doteq p(x_{t})\) is the modeled distribution, and the minimum problem could be transformed into a sub-problem, proved in Bao et al. [3]:

\[\min_{\{\theta\}}L\Leftrightarrow\min_{\{\theta_{s|t}\}_{t=1}^{T}}D_{\mathrm{ KL}}(q(x_{s}|x_{t})||p_{\theta_{s|t}}(x_{s}|x_{t})).\] (33)

We have no additional information besides when the reverse transition kernel is not Gaussian. But Lemma. C.3 proves that when the reverse transition kernel \(p_{\theta_{s|t}}(x_{s}|x_{t})\) is exponential family \(p_{\theta_{t}}(x_{s}|x_{t})=p(x_{t},\theta_{s|t})=h(x_{t})\exp\Bigl{(}\theta_{s |t}^{T}t(x_{t})-\alpha(\theta_{s|t})\Bigr{)}\), solving the sub-problem Eq. (33) equals to solve the following equations, which is to match moments between the modeled distribution and true distribution:

\[\mathbb{E}_{q(x_{s}|x_{t})}[t(x_{s})]=\mathbb{E}_{p(x_{t},\theta_{s|t})}[t(x_{s} )].\] (34)When \(t(x)=(x,..,x^{n})^{T}\), solving Eq.(34) equals to match the moments of true distribution and modeled distribution.

Meanwhile, Gaussian distribution belongs to the exponential family with \(t(x)=(x,x^{2})^{T}\) and \(\theta_{t}=(\frac{\mu_{t}}{\sigma_{t}^{2}},\frac{-1}{2\sigma_{t}^{2}})^{T}\), details in Lemma. C.2. Therefore, when modeling the reverse transition kernel as Gaussian distribution, the optimal parameters are that make its first two moments equal to the true first two moments of the real reverse transition kernel \(q(x_{s}|x_{t})\), which is consistent with the results in Bao et al. [3] and Bao et al. [2].

The aforementioned discussion serves as a motivation to acquire higher-order moments and identify a corresponding exponential family, which surpasses the Gaussian distribution in terms of complexity. However, proposition C.1 shows that finding such exponential family distribution with higher-order moments is impossible.

**Proposition C.1** (Infeasibility of exponential family with higher-order moments.).: _Given the first \(n\)-th order moments. It's non-trivial to find an exponential family distribution for \(\min D_{\mathrm{KL}}(q||p)\) when \(n\) is odd. And it's hard to solve \(\min D_{\mathrm{KL}}(q||p)\) when \(n\) is even._

### Proof of Proposition c.1

**Lemma C.2**.: _(Gaussian Distribution belongs to Exponential Family). Gaussian distribution \(p(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp\!\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right)\) is exponential family with \(t(x)=(x,x^{2})^{T}\) and \(\theta=(\frac{\mu}{\sigma^{2}},-\frac{1}{2\sigma^{2}})^{T}\)_

Proof.: For simplicity, we only prove one-dimensional Gaussian distribution. We could obtain:

\[p(x) =\frac{1}{\sqrt{2\pi}\sigma}\exp\!\left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right)\] (35) \[=\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\!\left(-\frac{1}{2\sigma^{2 }}(x^{2}-2\mu x+\mu^{2})\right)\] \[=\exp\!\left(\log\!\left(2\pi\sigma^{2}\right)^{-1/2}\right)\exp \!\left(-\frac{1}{2\sigma^{2}}(x^{2}-2\mu x)-\frac{\mu^{2}}{\sigma^{2}}\right)\] \[=\exp\!\left(\log\!\left(2\pi\sigma^{2}\right)^{-1/2}\right)\exp \!\left(-\frac{1}{2\sigma^{2}}(-2\mu\quad 1)(x\quad x^{2})^{T}-\frac{\mu^{2}}{ \sigma^{2}}\right)\] \[=\exp\!\left((\frac{\mu}{\sigma^{2}}\quad\frac{-1}{2\sigma^{2}})( x\quad x^{2})^{T}-(\frac{\mu^{2}}{2\sigma^{2}}+\frac{1}{2}\log\!\left(2\pi \sigma^{2}\right))\right)\!,\]

where \(\theta=(\frac{\mu}{\sigma^{2}},\frac{-1}{2\sigma^{2}})^{T}\) and \(t(x)=(x,x^{2})^{T}\) 

**Lemma C.3**.: _(The Solution for Exponential Family in Minimizing the KL Divergence). Suppose that \(p(x)\) belongs to exponential family \(p(x,\theta)=h(x)\exp\!\left(\theta^{T}t(x)-\alpha(\theta)\right)\), and the solution for minimizing the \(E_{q}[\log p]\) is \(E_{q}[t(x)]=E_{p(x,\theta)}[t(x)]\)._

Proof.: An exponential family \(p(x,\eta)=h(x)\exp\!\left(\eta^{T}t(x)-\alpha(\eta)\right)\propto f(x,\eta)= h(x)\exp\!\left(\eta^{T}t(x)\right)\) with log-partition \(\alpha(\eta)\). And we could obtain its first order condition on \(E_{q}[\log p]\) as:

\[\bigtriangledown_{\eta}\log f(x,\eta) =\bigtriangledown_{\eta}(\log h(x)+\eta^{T}t(x))=t(x)\] (36) \[\bigtriangledown_{\eta}\alpha(\eta) =\bigtriangledown_{\eta}\log\!\left(\int f(x,\eta)dx\right)= \frac{\int\bigtriangledown_{\eta}f(x,\eta)dx}{\int f(x,\eta)dx}\] (37) \[=e^{-\alpha(\eta)}\int t(x)f(x,\eta)dx=\int t(x)p(x,\eta)dx= \mathbb{E}_{p(x,\eta)}[t(x)]\]In order to minimize the \(D_{\mathrm{KL}}(q||p)=\int q\log(q/p)=-\mathbb{E}_{q}[\log p]\), we have:

\[\mathbb{E}_{q}[\log p]=\int dq\log(h(x))+\int dq(\eta^{T}t(x)- \alpha(\eta)\] \[\implies\frac{\partial}{\partial\eta}\mathbb{E}_{q}[\log p]=\int dq [\frac{\partial}{\partial\eta}(\eta^{T}x-\alpha(\eta))]=0\] \[\implies\int dq(x-\mathbb{E}_{p(x,\eta)}[t(x)])=\mathbb{E}_{q}[t( x)]-\mathbb{E}_{p(x,\eta)}[t(x)]=0\] \[\implies\mathbb{E}_{q}[t(x)]=\mathbb{E}_{p(x,\eta)}[t(x)]\]

For the second-order condition, we have the following:

\[\frac{\partial^{2}}{\partial\eta^{2}}\alpha(\eta) =\frac{\partial}{\partial\eta}\int dp(x,\eta)t(x)\] (38) \[=\int\frac{\partial}{\partial\eta}h(x)\exp\bigl{(}\eta^{T}t(x)- \alpha(\eta)\bigr{)}t(x)dx\] \[=\int h(x)t(x)\frac{\partial}{\partial\eta}\exp\bigl{(}\eta^{T}t (x)-\alpha(\eta)\bigr{)}dx\] \[=\int h(x)t(x)\exp\bigl{(}\eta^{T}t(x)-\alpha(\eta)\bigr{)}dx(t(x )-\mathbb{E}_{p}[t(x)])\] \[=\int p(x,\eta)dx(t^{2}(x)-t(x)\mathbb{E}_{p}[t(x)])\] \[=\mathbb{E}_{p(x,\eta)}[t^{2}(x)]-\mathbb{E}_{p(x,\eta)}[t(x)]^{ 2}=Cov_{p(x,\eta)}[t(x)]\geq 0\]

Therefore, the second-order condition for the cross entropy would be:

\[\frac{\partial^{2}}{\partial\eta^{2}}\mathbb{E}_{q}[\log p] =\frac{\partial}{\partial\eta}(\mathbb{E}_{q}[t(x)]-\mathbb{E}_{ p(x,\eta)}[t(x)])\] (39) \[=-\int\frac{\partial}{\partial\eta}p(x,\eta)t(x)dx\] \[=-\frac{\partial^{2}}{\partial\eta^{2}}\alpha(\eta)=-Cov_{p(x, \eta)}[t(x)]\leq 0\]

When we assume that the reverse process is Gaussian, the solution to Eq. (33) equals to match the moment of true distribution and modeled distribution \(\mu=E_{q}[x]\), \(\Sigma=Cov_{q}[x]\). 

**Lemma C.4**.: _(Infeasibility of the exponential family with higher-order moments). Suppose given the first \(N\)-th order moments \(M_{i},i=1,..,N\) and modeled \(p\) as an exponential family. It is nontrivial to solve the minimum problem \(E_{q}[\log p]\) when \(N\) is odd and it's difficult to solve when \(N\) is even._

Proof.: While given the mean, covariance, and skewness of the data distribution, assume that we could find an exponential family that minimizes the KL divergence, so that the distribution would satisfy the following form:

\[L(p,\hat{\lambda})=D_{\mathrm{KL}}(q||p)-\hat{\lambda}^{T}(\int pt -m)\Rightarrow\frac{\partial}{\partial p}L(p,\hat{\lambda})=\log\frac{p(x)}{h (x)}+1-\hat{\lambda}^{T}t=0\] (40) \[\Rightarrow p(x)=h(x)\exp\Bigl{(}\hat{\lambda}^{T}t-1\Bigr{)}\]

where, \(t(x)=(x,x^{2},x^{3})\), \(p=h(x)\exp\bigl{(}\lambda_{0}+\lambda_{1}x+\lambda_{2}x^{2}+\lambda_{3}x^{3} \bigr{)}\) and \(\int dpx^{3}=M_{3}\). However, when \(\lambda_{3}\) is not zero, \(\int p=\infty\) and density can't be normalized. The situation would be the same given an odd-order moment.

Similarly, given a more fourth-order moment, we could derive that \(\lambda_{3}=0\) above, and we should solve an equation \(\int dpx^{4}=M_{4}\) and \(p=h(x)\exp\bigl{(}\lambda_{0}+\lambda_{1}x+\lambda_{2}x^{2}+\lambda_{4}x^{4} \bigr{)}\). Consider such function:

\[Z(\lambda)=\int_{-\infty}^{\infty}dx\exp\bigl{(}-x^{2}-\lambda x^{4}\bigr{)}, \lambda>0\] (41)When \(\lambda\longrightarrow 0\), we could obtain \(\lim_{\lambda\to 0}Z(\lambda)=\sqrt{\pi}\) For other cases, the lambda can be expanded and then integrated term by term, which gives \(Z(\lambda)\sim\sum_{n=0}^{\infty}\frac{(-\lambda)^{n}}{n!}\Gamma(2n+1/2)\), but this function However, the radius of convergence of this level is \(0\), so when the \(\lambda\) takes other values, we need to propose a reasonable expression for the expansion after the analytic extension. Therefore, for solving the equation \(\int dpx^{4}=M_{4}\), there is no analytical solution first, and the numerical solution also brings a large computational effort. 

## Appendix D More information about Fig. 1 and Fig. 2

### Experiment in Toy-data

To illustrate the effectiveness of our method, we first compare the results of different solvers on one-dimensional data.

The distribution of our toy-data is \(q(x_{0})=0.4\mathcal{N}(-0.4,0.12^{2})+0.6\mathcal{N}(0.3,0.05^{2})\) and we define our solvers in each step as \(p(x_{s}|x_{t})=\frac{1}{3}\mathcal{N}(\mu_{t}^{(1)},\sigma_{t}^{2})+\frac{2}{3 }\mathcal{N}(\mu_{t}^{(1)},\sigma_{t}^{2})\) with vectors \(\mu_{t}^{(1)}\), \(\mu_{t}^{(2)}\) and \(\sigma_{t}^{2}\), which can not overfit the ground truth.

We then train second and third-order noise networks on the one-dimensional Gaussian mixture whose density is multi-modal. We use a simple MLP neural network with Swish activation [33].

Moreover, we experiment with our solvers in 8-Gaussian. The result is shown in Tab. 2. GMS outperforms Extended AnalyticDPM (SN-DDPM) [2] as presented in Tab. 2, with a bandwidth of \(1.05\sigma L^{-0.25}\), where \(\sigma\) is the standard deviation of data and \(L\) is the number of samples.

### Experiment in Fig. 2

In this section, we will provide a comprehensive explanation of the procedures involved in computing the discrepancy between two third-order moment calculation methods, as depicted in Fig. 2.

The essence of the calculation lies in the assumption that the reverse transition kernel follows a Gaussian distribution. By employing the following equations (considering only the diagonal elements of higher-order moments), we can compute the third-order moment using the first two-order moments:

\[\mathbb{E}_{q(x_{t_{i-1}}|x_{t_{i}})}[x_{t_{i-1}}\odot x_{t_{i-1}}\odot x_{t_ {i-1}}]_{\text{G}}\doteq M_{G}=\mu\odot\mu\odot\mu+3\mu\odot\Sigma,\] (42)

where \(\mu\) is the first-order moment and \(\Sigma\) is the diagonal elements of second order moment, which can be calculated by the Eq. (26) and Eq. (27). Meanwhile, we can calculate the estimated third-order moment \(\hat{M}_{3}\) by Eq. (28).

We use the pre-trained noise network from Ho et al. [14] and the second-order noise network form Bao et al. [2] and train the third-order noise network in CIFAR10 with the linear noise schedule.

Given that all higher-order moments possess the same dimension as the first-order moment \(\mu\), we can directly compare the disparity between different third-order moment calculation methods using the Mean Squared Error (MSE).

Thus, to quantify the divergence between the reverse transition kernel \(q(x_{s}|x_{t})\) and the Gaussian distribution, we can utilize the following equation:

\[\text{D}_{s|t}=\log\Bigl{(}\mathbb{E}_{q(x_{s}|x_{t})}[x_{s}\odot x_{s}\odot x _{s}]_{\text{G}}-\hat{M}_{3}\Bigr{)}^{2},\] (43)

where \(\hat{M}_{3}\) is obtained via Eq. (28), and we can start at different time step \(t\) and choose a corresponding \(s\) to calculate the \(\text{D}_{s|t}\) and draw different time step and step size \(t-s\) and we can derive Fig. 2.

\begin{table}
\begin{tabular}{l r r r r} \hline \hline  & \multicolumn{4}{c}{8-Gaussian} \\ \cline{2-5} \# \(K\) & 5 & 10 & 20 & 40 \\ \hline SN-DDPM & -0.7885 & 0.0661 & 0.0258 & 0.1083 \\ GMS & **-0.6304** & **0.0035** & **0.0624** & **0.1127** \\ \hline \end{tabular}
\end{table}
Table 2: **Comparison with SN-DDPM w.r.t. Likelihood \(\mathbb{E}_{q}[\log p_{\theta}(x)]\uparrow\) on 8-Gaussian.** GMS outperforms SN-DDPM.

Experimental details

### More discussion on weight of Gaussian mixture

From Proposition 3.2, we know that when the number of parameters in the Gaussian mixture equals the number of moment conditions, any choice of weight matrix is optimal. Therefore, we will discuss the choice of parameters to optimize in this section. As we have opted for a Gaussian mixture with two components \(q(x_{s}|x_{t})=\omega_{1}\mathcal{N}(\mu^{(1)}_{s|t},\Sigma^{(1)}_{s|t})+ \omega_{2}\mathcal{N}(\mu^{(2)}_{s|t},\Sigma^{(2)}_{s|t})\) as our foundational solvers, there exist five parameters (considered scalar, with the vector cases being analogous) available for optimization.

Our primary focus is on optimizing the mean and variance of the two components, as optimizing the weight term would require solving the equation multiple times. Additionally, we have a specific requirement that our Gaussian mixture can converge to a Gaussian distribution at the conclusion of optimization, particularly when the ground truth corresponds to a Gaussian distribution. In Tab. 3, we show the result of different choices of parameters in the Gaussian mixture.

When a parameter is not accompanied by a superscript, it implies that both components share the same value for that parameter. On the other hand, if a parameter is associated with a superscript, and only one moment contains that superscript, it signifies that the other moment directly adopts the true value for that parameter.

It is evident that the optimization of the mean value holds greater significance. Therefore, our subsequent choices for optimization are primarily based on the first set of parameters \(\mu^{(1)}_{s|t}\),\(\mu^{(2)}_{s|t}\Sigma_{s|t}\). Another crucial parameter to consider is the selection of weights \(\omega_{i}\). In Tab. 4, we show the result while changing the weight of the Gaussian mixture and the set of weight \(\omega_{1}=\frac{1}{3}\), \(\omega_{2}=\frac{1}{2}\) performs best among different weight.

What's more, we observe that such choice of weights \((\frac{1}{3},\frac{1}{2})\) consistently yielded superior performance among different datasets. Identifying an optimal weight value remains a promising direction for further enhancing the GMS.

### Details of pre-trained noise networks

In Tab. 5, we list details of pre-trained noise prediction networks used in our experiments.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & \(\omega_{1}=\frac{1}{100}\), \(\omega_{2}=\frac{99}{100}\) & \(\omega_{1}=\frac{1}{5}\), \(\omega_{2}=\frac{4}{5}\) & \(\omega_{1}=\frac{1}{3}\), \(\omega_{2}=\frac{2}{3}\) & \(\omega_{1}=\frac{1}{2}\), \(\omega_{2}=\frac{1}{2}\) \\ \hline CIFAR10 (LS) & **4.63** & **4.20** & **4.17** & **4.26** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results among different weight choices in CIFAR10 (LS), the number of steps is 50.

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & \(\mu^{(1)}_{s|t}\),\(\mu^{(2)}_{s|t}\),\(\Sigma_{s|t}\) & \(\mu^{(1)}_{s|t}\),\(\Sigma^{(1)}_{s|t}\),\(\Sigma^{(2)}_{s|t}\) & \(\mu_{s|t}\),\(\Sigma^{(1)}_{s|t}\),\(\Sigma^{(2)}_{s|t}\) \\ \hline CIFAR10 (LS) & **4.17** & 10.12 & 4.22 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Results among different parameters in CIFAR10 (LS), the number of steps is 50. The weight of Gaussian mixture is \(\omega_{1}=\frac{1}{3}\) and \(\omega_{2}=\frac{2}{3}\)

### Details of the structure of the extra head

In Tab. 6, we list structure details of \(\mathrm{NN}_{1}\), \(\mathrm{NN}_{2}\) and \(\mathrm{NN}_{3}\) of prediction networks used in our experiments.

### Training Details

We use a similar training setting to the noise prediction network in [30] and [2]. On all datasets, we use the ADAN optimizer [41] with a learning rate of \(10^{-4}\); we train 2M iterations in total for a higher order of noise network; we use an exponential moving average (EMA) with a rate of \(0.9999\). We use a batch size of 64 on ImageNet 64X64 and 128 on CIFAR10. We save a checkpoint every 50K iterations and select the models with the best FID on 50k generated samples. Training one noise network on CIFAR10 takes about 100 hours on one A100. Training on ImageNet 64x64 takes about 150 hours on one A100.

### Details of Parameters of Optimizer in Sampling

In Tab. 7, we list details of the learning rate, learning rate schedule, and warm-up steps for different experiments.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & \(\mathrm{NN}_{1}\) & \(\mathrm{NN}_{2}\) (Noise) & \(\mathrm{NN}_{3}\) (Noise) \\ \hline CIFAR10 (LS) & None & Conv & Res+Conv \\ CIFAR10 (CS) & None & Conv & Res+Conv \\ ImageNet 64x64 & None & Res+Conv & Res+Conv \\ \hline \hline \end{tabular}
\end{table}
Table 6: \(\mathrm{NN}_{1}\) represents noise prediction networks and \(\mathrm{NN}_{2}\), \(\mathrm{NN}_{3}\) represent networks for estimating the second- and the third-order of noise, which used in our experiments. Conv denotes the convolution layer. Res denotes the residual block. None denotes using the original network without additional parameters

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & Learning rate & lr Schedule & min lr & warm-up steps \\ \hline CIFAR10 & max(0.16-\({}_{t}\)*0.16,0.12) & COS & 0.1 & 18 \\ ImageNet 64\(\times\)64 & max(0.1-\({}_{t}\)*0.1,0.06) & COS & 0.04 & 18 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Details of Parameters of Optimizer used in our experiments. lr Schedule means the learning rate schedule. min lr means the minimum learning rate while using the learning rate schedule, \(\iota_{t}\) is a function with the second order growth function of sampling steps \(t\).

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & \# TIMESTEPS \(N\) & Noise Schedule & optimizer for GMM \\ \hline CIFAR10 (LS) & 1000 & LS & ADAN \\ CIFAR10 (CS) & 1000 & CS & ADAN \\ ImageNet 64x64 & 4000 & CS & ADAN \\ \hline \hline \end{tabular}
\end{table}
Table 5: Details of noise prediction networks used in our experiments. LS means the linear schedule of \(\sigma(t)\)[14] in the forward process of discrete time step (see Eq. (1)). CS means the cosine schedule of \(\sigma(t)\)[30] in the forward process of discrete timesteps (see Eq. (1)).

where COS represents the cosine learning rate schedule [6]. We find that the cosine learning rate schedule works best. The cos learning rate could be formulated as follows:

\[\alpha_{i+1}=\begin{cases}\frac{i}{I_{w}}\alpha_{i}&\mathrm{if}\ \ \ \ \ i\leq I_{w}\\ \\ \max((0.5\cos\left(\frac{i-I_{w}}{I-I_{w}}\pi\right)+1)\alpha_{t},\alpha_{\min} )&\mathrm{else}\end{cases}\] (44)

where, \(a_{t}\) is the learning rate after \(t\) steps, \(I_{w}\) is the warm-up steps, \(\alpha_{\min}\) is the minimum learning rate, \(I\) is the total steps.

### Details of memory and time cost

In Tab. 8, we list the memory of models (with the corresponding methods) used in our experiments. The extra memory cost higher-order noise prediction network is negligible.

In Fig. 5, we present a comprehensive breakdown of the time of optimization process within GMS at each sampling step. Subfigure (a) in Fig. 5 illustrates that when the batch size is set to 100, the time of optimizing approximately 320 steps to fit a Gaussian mixture transition kernel at each step is equivalent to the time needed for one network inference, which means that the additional time of GMS for each sampling steps is about \(10\%\) when the number of optimizing steps is 30.

Meanwhile, Subfigure (a) in Fig. 5 elucidates the relation between sampling time and the number of sampling steps for both GMS and SN-DDPM. It is noteworthy that the optimization steps employed in GMS remain fixed at 25 per sampling step, consistent with our setting in experiments. Evidently, as the number of sampling steps escalates, GMS demonstrates a proportional increase in computational overhead, consistently maintaining this overhead within a 10\(\%\) margin of the original computational cost.

Figure 5: **Sampling/optimizing steps and time (in seconds).** (a). The correlation between the number of optimizing steps and the corresponding optimizing time for GMS. Notably, the optimizing. (b). The correlation between the number of sampling steps and the corresponding sampling time for a single batch is observed. Notably, the sampling time demonstrates nearly linear growth with the increase in the number of sampling steps for both solvers.

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & Noise prediction & Noise \& SN prediction & Noise \& SN prediction \\  & network & networks & networks \\  & (all baselines) & SNDDPM & (GMDDPM) \\ \hline CIFAR10 (LS) & 50.11 MB & 50.11 MB & 50.52 MB (+0.8\%) \\ CIFAR10 (CS) & 50.11 MB & 50.11 MB & 50.52 MB (+0.8\%) \\ ImageNet 64\(\times\)64 & 115.46 & 115.87 MB & 116.28 (+0.7\%) \\ \hline \hline \end{tabular}
\end{table}
Table 8: Model size (MB) for different models. The model of SNDDPM denotes the model that would predict noise and the square of noise; The model of GMDDPM denotes the model that would predict noise, the square of noise, and the third power of noise.

Since many parts in the GMS introduce additional computational effort, Fig. 6 provides detailed information on the additional computational time of the GMS and SN-DDPM relative to the DDPM, assuming that the inference time of the noise network is unit one.

Emphasizing that the extra time is primarily for reference, most pixels can work well with a Gaussian distribution. By applying a threshold and optimizing only when the disparity between the reverse transition kernel and Gaussian exceeds it, we can conserve about 4\(\%\) of computational resources without compromising result quality.

### The reduction in the FID on CIFAR10 for GMS compared to SN-DDPM

To provide a more intuitive representation of the improvement of GMS compared to SN-DDPM at different sampling steps, we have constructed Fig. 7 below. As observed from Fig. 7, GMS exhibits a more pronounced improvement when the number of sampling steps is limited. However, as the number of sampling steps increases, the improvement of GMS diminishes, aligning with the hypothesis of our non-Gaussian reverse transition kernel as stated in the main text. However, we respectively clarify that due to the nonlinear nature of the FID metric, the relative/absolute FID improvements are not directly comparable across different numbers of steps

### Additional results with latent diffusion

we conduct an experiment on ImageNet 256 \(\times\) 256 with the backbone noise network as U-ViT-Huge [4]. We train extra the second-order and the third-order noise prediction heads with two transformer blocks with the frozen backbone. The training iterations for each head is 150K and other training parameters are the same as the training of the backbone of U-ViT-Huge (details in [4]). We will evaluate the sampling efficiency of GMS under conditional sampling (classifier-free guidance scale is 0.4) and unconditional sampling in Tab. 9

Figure 6: Time cost distribution for GMS.

Figure 7: The reduction in the FID on CIFAR10 for GMS compared to SN-DDPM. With a restricted number of sampling steps, the improvement exhibited by GMS surpasses that achieved with an ample number of sampling steps.

### Additional results with same calculation cost

Since GMS will cost more computation in the process of fitting the Gaussian mixture, we use the maximum amount of computation required (i.e., an additional 10% of computation is needed) for comparison, and for a fair comparison, we let the other solvers take 10% more sampling steps. Our GMS still outperforms existing SDE-based solvers with the same (maximum) computation cost in Tab. 10.

In order to provide a more intuitive representation for comparing different methods under the same sampling time, we have generated Fig. 8. Subfig. (a) of Fig. 8 illustrates that at each step, there is an additional computational time cost incurred for GMS to fit the Gaussian mixture transition kernel. This computational overhead exhibits a nearly linear growth pattern with the increase in the number of sampling steps.

Subfig. (b) of Fig. 8 offers a valuable perspective on the connection between approximate sampling time and sampling quality for two GMS and SN-DDPM. It becomes apparent that GMS consistently exhibits superior performance when compared to SN-DDPM with identical computational resources. Furthermore, the data reveals that the magnitude of improvement introduced by GMS is more substantial when the number of sampling steps is limited, as opposed to scenarios with a higher number of sampling steps.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline  & \multicolumn{8}{c}{CIFAR10 (LS)} \\ \# timesteps \(K\) & 11(10) & 22(20) & 28(25) & 44(40) & 55(50) & 110(100) & 220(200) & 1000(1000) \\ \hline SN-DDPM\({}^{*}\) & 17.56 & 7.74 & 6.76 & 4.81 & 4.23 & 3.60 & 3.20 & 3.65 \\ GMS (**ours**) & **17.43** & **7.18** & **5.96** & **4.52** & **4.16** & **3.26** & **3.01** & **2.76** \\ \hline \hline \end{tabular}
\end{table}
Table 10: **Fair comparison with competitive SDE-based solvers w.r.t. FID score \(\downarrow\) on CIFAR10.** SN-DDPM denotes Extended AnalyticDPM from [2]. The number of sampling steps for the GMS is indicated within parentheses, while for other solvers, it is represented outside of parentheses.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline  & \multicolumn{8}{c}{Cond, CFG=0.4} & Uncond \\ \# timesteps \(K\) & 15 & 20 & 25 & 40 & 50 & 100 & 200 & 25 & 40 & 50 & 100 \\ \hline DDPM, \(\tilde{\beta}_{t}\) & 6.48 & 5.30 & 4.86 & 4.42 & 4.27 & 3.93 & 3.32 & 8.62 & 6.47 & 5.97 & 5.04 \\ SN-DDPM & 4.40 & 3.36 & 3.10 & 2.99 & 2.97 & 2.93 & 2.82 & 8.19 & 5.73 & 5.32 & 4.60 \\ GMS (**ours**) & **4.01** & **3.07** & **2.89** & **2.88** & **2.85** & **2.81** & **2.74** & **7.78** & **5.42** & **5.03** & **4.45** \\ \hline \hline \end{tabular}
\end{table}
Table 9: **Comparison with DDPM and SN-DDPM w.r.t. FID score \(\downarrow\) on ImageNet 256 \(\times\) 256 by using latent diffusion.** Uncond denotes the unconditional sampling, and Cond denotes sampling combining 90\(\%\) conditional sampling with classifier-free guidance equals 0.4 and 10\(\%\) unconditional sampling, consistent with Bao et al. [4].

### Compare with continuous time SDE-based solvers

For completeness, we compare the sampling speed of GMS and non-improved reverse transition kernel in Tab. 11, and it can be seen that within 100 steps, our method outperforms other SDE-based solvers. It is worth noting that the EDM-SDE [18] is based on the \(x_{0}\) prediction network, while SEED [10] and GMS are based on Ho et al. [14]'s pre-trained model which is a discrete-time noise network.

### Codes and License

In Tab.12, we list the code we used and the license.

## Appendix F SDEdit

Fig. 9 illustrates one of the comprehensive procedures of SDEdit. Given a guided image, SDEdit initially introduces noise at the level of \(\sigma_{t_{0}}\) to the original image \(x_{0}\), where \(\sigma\) denotes the noise schedules of forward process in diffusion models. Subsequently, using this noisy image \(x_{t_{0}}\) and then discretizes it via the reverse SDE to generate the final image. Fig. 9 shows that the choice of \(t_{0}\) will

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & \multicolumn{5}{c}{CIFAR10} \\ \cline{2-7} \# timesteps \(K\) & 10 & 20 & 40 & 50 & 100 \\ \hline SEED-2 [10] & 481.09 & 305.88 & 51.41 & 11.10 & 3.19 \\ SEED-3 [10] & 483.04 & 462.61 & 247.44 & 62.62 & 3.53 \\ EDM-SDE [18] & 35.07 & 14.04 & 9.56 & 5.12 & **2.99** \\ GMS (**ours**) & **17.43** & **7.18** & **4.52** & **4.16** & 3.26 \\ \hline \hline \end{tabular}
\end{table}
Table 11: **Comparison with SDE-based solvers [18, 10] w.r.t. FID score \(\downarrow\) on CIFAR10.**

Figure 8: (a). The correlation between the number of sampling steps and the corresponding sampling time for a single batch is observed. Notably, the sampling time demonstrates nearly linear growth with the increase in the number of sampling steps for both solvers.(b).The relation between the sample quality (in FID) and the sampling time (in seconds) of GMS and SN-DDPM on CIFAR10.)can greatly will greatly affect the realism of sample images. With the increase of \(t_{0}\), the similarity between sample images and the real image is decreasing. Hence, apart from conducting quantitative evaluations to assess the fidelity of the generated images, it is also crucial to undertake qualitative evaluations to examine the outcomes associated with different levels of fidelity. Taking all factors into comprehensive consideration, we have selected the range of \(t_{0}\) from 0.3T to 0.5T in our experiments.

Besides experiments on LSUN 256\(\times\)256, we also carried out an experiment of SDEdit on ImageNet 64\(\times\)64. In Tab. 13, we show the FID score for different methods in different \(t_{0}\) and different sample steps. Our method outperforms other SDE-based solvers as well.

## Appendix G Samples

From Fig. 10 to Fig. 12, we show generated samples of GMS under a different number of steps in CIFAR10 and ImageNet 64\(\times\)64. Here we use \(K\) to denote the number of steps for sampling.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & \multicolumn{3}{c}{ImageNet 64X64, \(t_{0}=1200\)} \\ \cline{2-5} \# \(K\) & 26(28) & 51(55) & 101(111) & 201(221) \\ \hline DDPM,\(\tilde{\beta}_{n}\) & 21.37 & 19.15 & 18.85 & 18.15 \\ DDIM & 21.87 & 21.81 & 21.95 & 21.90 \\ SN-DDPM & 20.76 & 18.67 & 17.50 & 16.88 \\ GMS & **20.50** & **18.37** & **17.18** & **16.83** \\ \hline \hline \end{tabular}
\end{table}
Table 13: **Comparison with competitive methods in SDEdit w.r.t. FID score \(\downarrow\) on ImageNet 64\(\times\)64.** ODE-based solver is worse than all SDE-based solvers. With nearly the same computation cost, our GMS outperforms existing methods in most cases.

Figure 9: \(t_{0}\) denotes the timestep to noise the stroke

Figure 11: Generated samples on ImageNet 64\(\times\)64.

Figure 12: Generated samples on ImageNet 64\(\times\)64.

Figure 10: Generated samples on CIFAR10 (LS)