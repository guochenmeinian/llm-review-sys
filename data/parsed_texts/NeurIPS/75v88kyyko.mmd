# Hierarchical clustering with dot products

recovers hidden tree structure

 Annie Gray

School of Mathematics

University of Bristol, UK

annie.gray@bristol.ac.uk &Alexander Modell

Department of Mathematics

Imperial College London, UK

a.modell@imperial.ac.uk &Patrick Rubin-Delanchy

School of Mathematics

University of Bristol, UK

patrick.rubin-delanchy@bristol.ac.uk &Nick Whiteley

School of Mathematics

University of Bristol, UK

nick.whiteley@bristol.ac.uk

###### Abstract

In this paper we offer a new perspective on the well established agglomerative clustering algorithm, focusing on recovery of hierarchical structure. We recommend a simple variant of the standard algorithm, in which clusters are merged by maximum average dot product and not, for example, by minimum distance or within-cluster variance. We demonstrate that the tree output by this algorithm provides a bona fide estimate of generative hierarchical structure in data, under a generic probabilistic graphical model. The key technical innovations are to understand how hierarchical information in this model translates into tree geometry which can be recovered from data, and to characterise the benefits of simultaneously growing sample size and data dimension. We demonstrate superior tree recovery performance with real data over existing approaches such as UPGMA, Ward's method, and HDBSCAN.

## 1 Introduction

Hierarchical structure is known to occur in many natural and man-made systems [32], and the problem considered in this paper is how to recover this structure from data. Hierarchical clustering algorithms, [25, 36, 37, 41] are very popular techniques which organise data into nested clusters, used routinely by data scientists and machine learning researchers, and are easily accessible through open source software packages such as scikit-learn [38]. We focus on perhaps the most popular family of such techniques: agglomerative clustering [17], [24, Ch.3], in which clusters of data points are merged recursively.

Agglomerative clustering methods are not model-based procedures, but rather simple algorithms. Nevertheless, in this work we uncover a new perspective on agglomerative clustering by introducing a general form of generative statistical model for the data, but without assuming specific parametric families of distributions (e.g., Gaussian). In our model (section 2.1), hierarchy takes the form of a tree defining the conditional independence structure of latent variables, using elementary concepts from probabilistic graphical modelling [30, 28]. In a key innovation, we then augment this conditional independence tree to form what we call a _dendrogram_, whose geometry is related to population statistics of the data. The new insight which enables tree recovery in our setting (made precise and explained in section 3) is that _dot products between data vectors reveal heights of most recent common ancestors in the dendrogram_.

We suggest an agglomerative algorithm which merges clusters according to highest sample average dot product (section 2.2). This is in contrast to many existing approaches which quantify dissimilaritybetween data vectors using Euclidean distance. We also consider the case where data are preprocessed by reducing dimension using PCA. We mathematically analyse the performance of our dot product clustering algorithm and establish that under our model, with sample size \(n\) and data dimension \(p\) growing simultaneously at appropriate rates, the merge distortion [20; 27] between the algorithm output and underlying tree vanishes. In numerical examples with real data (section 4), where we have access to labels providing a notion of true hierarchy, we compare performance of our algorithm against existing methods in terms of a Kendall \(\tau_{b}\) correlation performance measure, which quantifies association between ground-truth and estimated tree structure. We examine statistical performance with and without dimension reduction by PCA, and illustrate how dot products versus Euclidean distances relate to semantic structure in ground-truth hierarchy.

Related workAgglomerative clustering methods combine some dissimilarity measure with a 'linkage' function, determining which clusters are combined. Popular special cases include UPGMA [44] and Ward's method [48], against which we make numerical comparisons (section 4). Owing to the simple observation that, in general, finding the largest dot product between data vectors is not equivalent to finding the smallest Euclidean distance, we can explain why these existing methods may not correctly recover tree structure under our model (appendix E). Popular density-based clustering methods methods include CURE [22], OPTICS [7], BIRCH [52] and HDBSCAN [9]. In section 4 we discuss the extent to which these methods can and cannot be compared against ours in terms of tree recovery performance.

Existing theoretical treatments of hierarchical clustering involve different mathematical problem formulations and assumptions to ours. One common setup is to assume an underlying ultrametric space whose geometry specifies the unknown tree, and/or to study tree recovery as \(n\to\infty\) with respect to a cost function, e.g, [10; 16; 43; 11; 14; 15; 31; 12]. An alternative problem formulation addresses recovery of the cluster tree of the probability density from which it is assumed data are sampled [42; 20; 27]. The unknown tree in our problem formulation specifies conditional independence structure, and so it has a very different interpretation to the trees in all these cited works. Moreover, our data are vectors in \(\mathbb{R}^{p}\), and \(p\to\infty\) is a crucial aspect of our convergence arguments, but in the above cited works \(p\) plays no role or is fixed. Our definition of dendrogram is different to that in e.g. [10]: we do not require all leaf vertices to be equidistant from the root - a condition which arises as the "fixed molecular clock" hypothesis [18; 34] in phylogenetics. We also allow data to be sampled from non-leaf vertices. There is an enormous body of work on tree reconstruction methods in phylogenetics, e.g. listed at [4], but these are mostly not general-purpose solutions to the problem of inferring hierarchy. Associated theoretical convergence results are limited in scope, e.g, the famous work [18] is limited to a fixed molecular clock, five taxa and does not allow observation error.

## 2 Model and algorithm

### Statistical model, tree and dendrogram

Where possible, we use conventional terminology from the field of probabilistic graphical models, e.g., [28] to define our objects and concepts. Our model is built around an unobserved tree \(\mathcal{T}=(\mathcal{V},\mathcal{E})\), that is a directed acyclic graph with vertex and edge sets \(\mathcal{V}\) and \(\mathcal{E}\), with two properties: \(\mathcal{T}\) is connected (ignoring directions of edges), and each vertex has at most one parent, where we say \(u\) is a parent of \(v\) if there is an edge from \(u\) to \(v\). We observe data vectors \(\mathbf{Y}_{i}\in\mathbb{R}^{p}\), \(i=1,\ldots,n\), which we model as:

\[\mathbf{Y}_{i}=\mathbf{X}(Z_{i})+\mathbf{S}(Z_{i})\mathbf{E}_{i},\] (1)

comprising three independent sources of randomness:

* \(Z_{1},\ldots,Z_{n}\) are i.i.d., discrete random variables, with distribution supported on a subset of vertices \(\mathcal{Z}\subseteq\mathcal{V}\), \(|\mathcal{Z}|<\infty\);
* \(\mathbf{X}(v)\coloneqq[X_{1}(v)\;\cdots\;X_{p}(v)]^{\top}\) is an \(\mathbb{R}^{p}\)-valued random vector for each vertex \(v\in\mathcal{V}\);
* \(\mathbf{E}_{1},\ldots,\mathbf{E}_{n}\) are i.i.d, \(\mathbb{R}^{p}\)-valued random vectors. The elements of \(\mathbf{E}_{i}\) are i.i.d., zero mean and unit variance. For each \(z\in\mathcal{Z}\), \(\mathbf{S}(z)\in\mathbb{R}^{p\times p}\), is a deterministic matrix.

For each \(v\in\mathcal{Z}\), one can think of the vector \(\mathbf{X}(v)\) as the random centre of a "cluster", with correlation structure of the cluster determined by the matrix \(\mathbf{S}(z)\). The latent variable \(Z_{i}\) indicates which cluster the \(i\)th data vector \(\mathbf{Y}_{i}\) is associated with. The vectors \(\mathbf{X}(v)\), \(v\in\mathcal{V}\setminus\mathcal{Z}\), correspond to unobserved vertices in the underlying tree. As an example, \(\mathcal{Z}\) could be the set of leaf vertices of \(\mathcal{T}\), but neither our methods nor theory require this to be the case. Throughout this paper, we assume that the tree \(\mathcal{T}\) determines two distributional properties of \(\mathbf{X}\). Firstly, we assume \(\mathcal{T}\) is the conditional independence graph of the collection of random variables \(X_{j}\coloneqq\{X_{j}(v);v\in\mathcal{V}\}\) for each \(j\), that is,

**A1**.: _for all \(j=1,\ldots,p\), the marginal probability density or mass function of \(X_{j}\) factorises as:_

\[p(x_{j})=\prod_{v\in\mathcal{V}}p\left(x_{j}(v)|x_{j}(\mathrm{Pa}_{v})\right),\]

_where \(\mathrm{Pa}_{v}\) denotes the parent of vertex \(v\)._

However we do not necessarily require that \(X_{1},\ldots,X_{p}\) are independent or identically distributed. Secondly, we assume

**A2**.: _for each \(j=1\,\ldots,p\), the following martingale-like property holds:_

\[\mathbb{E}\left[X_{j}(v)|X_{j}(\mathrm{Pa}_{v})\right]=X_{j}(\mathrm{Pa}_{v}),\]

_for all vertices \(v\in\mathcal{V}\) except the root._

The conditions **A1** and **A2** induce an additive hierarchical structure in \(\mathbf{X}\). For any distinct vertices \(u,v\in\mathcal{V}\) and \(w\) an ancestor of both, **A1** and **A2** imply that given \(X_{j}(w)\), the increments \(X_{j}(u)-X_{j}(w)\) and \(X_{j}(v)-X_{j}(w)\) are conditionally independent and both conditionally mean zero.

To explain our algorithm we need to introduce the definition of a _dendrogram_, \(\mathcal{D}=(\mathcal{T},h)\), where \(h:\mathcal{V}\rightarrow\mathbb{R}_{+}\) is a function which assigns a height to each vertex of \(\mathcal{T}\), such that \(h(v)\geq h(\mathbf{Pa}_{v})\) for any vertex \(v\in\mathcal{V}\) other than the root. The term "dendrogram" is derived from the ancient Greek for "tree" and "drawing", and indeed the numerical values \(h(v)\), \(v\in\mathcal{V}\), can be used to construct a drawing of \(\mathcal{T}\) where height is measured with respect to some arbitrary baseline on the page, an example is shown in figure 1(a). With \(\langle\cdot,\cdot\rangle\) denoting the usual dot product between vectors, the function

\[\alpha(u,v)\coloneqq\frac{1}{p}\mathbb{E}\left[\langle\mathbf{X}(u),\mathbf{X }(v)\rangle\right],\quad u,v\in\mathcal{V},\] (2)

will act as a measure of affinity underlying our algorithm. The specific height function we consider is \(h(v)\coloneqq\alpha(v,v)\). The martingale property **A2** ensures that this height function satisfies \(h(v)\geq h(\text{Pa}_{v})\) as required, see lemma 2 in appendix C. In appendix E we also consider cosine similarity as an affinity measure, and analyse its performance under a multiplicative noise model, cf. the additive model (1).

### Algorithm

Combining the model (1) with (2) we have \(\alpha(Z_{i},Z_{j})=p^{-1}\mathbb{E}[\langle\mathbf{Y}_{i},\mathbf{Y}_{j} \rangle|Z_{i},Z_{j}]\) for \(i\neq j\in[n]\), \([n]\coloneqq\{1,\ldots,n\}\). The input to algorithm 1 is an estimate \(\hat{\alpha}(\cdot,\cdot)\) of all the pairwise affinities \(\alpha(Z_{i},Z_{j})\), \(i\neq j\in[n]\). We consider two approaches to estimating \(\alpha(Z_{i},Z_{j})\), with and without dimension reduction by uncentered PCA. For some \(r\leq\min\{p,n\}\), let \(\mathbf{V}\in\mathbb{R}^{p\times r}\) denote the matrix whose columns are orthonormal eigenvectors of \(\sum_{i=1}^{n}\mathbf{Y}_{i}\mathbf{Y}_{i}^{\top}\) associated with its \(r\) largest eigenvalues. Then \(\zeta_{i}\coloneqq\mathbf{V}^{\top}\mathbf{Y}_{i}\) is \(r\)-dimensional vector of principal component scores for the \(i\)th data vector. The two possibilities for \(\hat{\alpha}(\cdot,\cdot)\) we consider are:

\[\hat{\alpha}_{\text{data}}(i,j)\coloneqq\frac{1}{p}\langle\mathbf{Y}_{i}, \mathbf{Y}_{j}\rangle,\qquad\hat{\alpha}_{\text{pca}}(i,j)\coloneqq\frac{1}{p} \langle\zeta_{i},\zeta_{j}\rangle.\] (3)

In the case of \(\hat{\alpha}_{\text{pca}}\) the dimension \(r\) must be chosen. Our theory in section 3.2 assumes that \(r\) is chosen as the rank of the matrix with entries \(\alpha(u,v),u,v\in\mathcal{Z}\), which is at most \(|\mathcal{Z}|\). In practice \(r\) usually must be chosen based on the data, we discuss this in appendix A.

Algorithm 1 returns a dendrogram \(\hat{\mathcal{D}}=(\hat{\mathcal{T}},\hat{h})\), comprising a tree \(\hat{\mathcal{T}}=(\hat{\mathcal{V}},\hat{\mathcal{E}})\) and height function \(\hat{h}\). Each vertex in \(\hat{\mathcal{V}}\) is a subset of \([n]\), thus indexing a subset of the data vectors \(\mathbf{Y}_{1},\ldots,\mathbf{Y}_{n}\). The leaf vertices are the singleton sets \(\{i\}\), \(i\in[n]\), corresponding to the data vectors themselves. As algorithm 1 proceeds, vertices are appended to \(\hat{\mathcal{V}}\), edges are appended to \(\hat{\mathcal{E}}\), and the domain of the function \(\hat{\alpha}(\cdot,\cdot)\) is extended as affinities between elements of \(\hat{\mathcal{V}}\) are computed. Throughout the paper we simplify notation by writing \(\hat{\alpha}(i,j)\) as shorthand for \(\hat{\alpha}(\{i\},\{j\})\) for \(i,j\in[n]\), noting that each argument of \(\hat{\alpha}(\cdot,\cdot)\) is in fact a subset of \([n]\).

**Implementation using scikit-learn** The presentation of algorithm 1 has been chosen to simplify its theoretical analysis, but alternative formulations of the same method may be much more computationally efficient in practice. In appendix B we outline how algorithm 1 can easily be implemented using the AgglomerativeClustering class in scikit-learn [38].

## 3 Performance Analysis

### Merge distortion is upper bounded by affinity estimation error

In order to explain the performance of algorithm 1 we introduce the _merge height_ functions:

\[m(u,v) \coloneqq h(\text{most recent common ancestor of $u$ and $v$}), u,v\in\mathcal{V},\] \[\hat{m}(u,v) \coloneqq\hat{h}(\text{most recent common ancestor of $u$ and $v$}), u,v\in\hat{\mathcal{V}}.\]

To simplify notation we write \(\hat{m}(i,j)\) as shorthand for \(\hat{m}(\{i\},\{j\})\), for \(i,j\in[n]\). The discrepancy between any two dendrograms whose vertices are in correspondence can be quantified by _merge distortion_[20] - the maximum absolute difference in merge height across all corresponding pairs ofvertices. [20] advocated merge distortion as a performance measure for cluster-tree recovery, which is different to our model-based formulation, but merge distortion turns out to be a useful and tractable performance measure in our setting too. As a preface to our main theoretical results, lemma 1 explains how the geometry of the dendrogram \(\mathcal{D}\), in terms of merge heights, is related to population statistics of our model. Defining \(d(u,v)\coloneqq h(u)-h(w)+h(v)-h(w)\) for \(u\neq v\in\mathcal{V}\), where \(w\) is the most recent common ancestor of \(u\) and \(v\), we see \(d(u,v)\) is the vertical distance on the dendrogram from \(u\) down to \(w\) then back up to \(v\), as illustrated in figure 1(a).

**Lemma 1**.: _For any two vertices \(u,v\in\mathcal{V}\),_

\[m(u,v)=\frac{1}{p}\mathbb{E}\left[\langle\mathbf{X}(u),\mathbf{X}(v)\rangle \right]=\alpha(u,v),\quad d(u,v)=\frac{1}{p}\mathbb{E}\left[\left\|\mathbf{X}( u)-\mathbf{X}(v)\right\|^{2}\right].\] (4)

The proof is in appendix C. Considering the first two equalities in (4), it is natural to ask if the estimated affinities \(\hat{\alpha}(\cdot,\cdot)\) being close to the true affinities \(\alpha(\cdot,\cdot)\) implies a small merge distortion between \(\hat{m}(\cdot,\cdot)\) and \(m(\cdot,\cdot)\). This is the subject of our first main result, theorem 1 below. In appendix E we use the third equality in (4) to explain why popular agglomerative techniques such as UPGMA [44] and Ward's method [48] which merge clusters based on proximity in Euclidean distance may enjoy limited success under our model, but in general do not correctly recover tree structure.

Let \(b\) denote the minimum branch length of \(\mathcal{D}\), that is, \(b=\min\{h(v)-h(\mathbf{Pa}_{v})\}\), where the minimum is taken over all vertices in \(\mathcal{V}\) except the root.

**Theorem 1**.: _Let the function \(\hat{\alpha}(\cdot,\cdot)\) given as input to algorithm 1 be real-valued and symmetric but otherwise arbitrary. For any \(z_{1},\ldots,z_{n}\in\mathcal{Z}\), if_

\[\max_{i,j\in[n],i\neq j}|\alpha(z_{i},z_{j})-\hat{\alpha}(i,j)|<b/2,\]

_then the dendrogram returned by algorithm 1 satisfies_

\[\max_{i,j\in[n],i\neq j}|m(z_{i},z_{j})-\hat{m}(i,j)|\leq\max_{i,j\in[n],i\neq j }|\alpha(z_{i},z_{j})-\hat{\alpha}(i,j)|.\] (5)

The proof is in appendix C.

### Affinity estimation error vanishes with increasing dimension and sample size

Our second main result, theorem 2 below, concerns the accuracy of estimating the affinities \(\alpha(\cdot,\cdot)\) using \(\hat{\alpha}_{\text{data}}\) or \(\hat{\alpha}_{\text{pca}}\) as defined in (3). We shall consider the following technical assumptions.

**A3** (Mixing across dimensions).: _For mixing coefficients \(\varphi\) satisfying \(\sum_{k\geq 1}\varphi^{1/2}(k)<\infty\) and all \(u,v\in\mathcal{Z}\), the sequence \(\{(X_{j}(u),X_{j}(v));j\geq 1\}\) is \(\varphi\)-mixing._

**A4** (Bounded moments).: _For some \(q\geq 2\), \(\sup_{j\geq 1}\max_{v\in\mathcal{Z}}\mathbb{E}[|X_{j}(v)|^{2q}]<\infty\) and \(\mathbb{E}[|\mathbf{E}_{11}|^{2q}]<\infty\), where \(\mathbf{E}_{11}\) is the first element of the vector \(\mathbf{E}_{1}\)._

**A5** (Disturbance control).: \(\max_{v\in\mathcal{Z}}\|\mathbf{S}(v)\|_{\mathrm{op}}\in O(1)\) _as \(p\to\infty\), where \(\|\cdot\|_{\mathrm{op}}\) is the spectral norm._

**A6** (PCA rank).: _The dimension \(r\) chosen in definition of \(\hat{\alpha}_{\mathrm{pca}}\), see (3), is equal to the rank of the matrix with entries \(\alpha(u,v)\), \(u,v\in\mathcal{Z}\)._

The concept of \(\varphi\)-mixing is a classical weak-dependence condition, e.g. [19, 39]. **A3** implies that for each \(j\geq 1\), \((X_{j}(u),X_{j}(v))\) and \((X_{j+\delta}(u),X_{j+\delta}(v))\) are asymptotically independent as \(\delta\to\infty\). However, it is important to note that \(\hat{\alpha}_{\text{data}}\) and \(\hat{\alpha}_{\text{pca}}\) in (3), and hence the operation of algorithm 1 with these inputs, are invariant to permutation of the data dimensions \(j=1,\ldots,p\). Thus our analysis under **A3** only requires there is _some_ permutation of dimensions under which \(\varphi\)-mixing holds. **A4** is a fairly mild integrability condition. **A5** allows control of magnitudes of the "disturbance" vectors \(\mathbf{Y}_{i}-\mathbf{X}(Z_{i})=\mathbf{S}(Z_{i})\mathbf{E}_{i}\). Further background and discussion of assumptions is given in appendix C.2.

**Theorem 2**.: _Assume that **A1**-**A5** hold and let \(q\) be as in **A4**. Then_

\[\max_{i,j\in[n],i\neq j}|\alpha(Z_{i},Z_{j})-\hat{\alpha}_{\text{data}}(i,j)| \in O_{\mathbb{P}}\left(\frac{n^{2/q}}{\sqrt{p}}\right).\] (6)

_If additionally **A3** is strengthened from \(\varphi\)-mixing to independence, \(\mathbf{S}(v)=\sigma\mathbf{I}_{p}\) for some constant \(\sigma\geq 0\) and all \(v\in\mathcal{Z}\) (in which case **A5** holds), and **A6** holds, then_

\[\max_{i,j\in[n],i\neq j}|\alpha(Z_{i},Z_{j})-\hat{\alpha}_{\text{pca}}(i,j)| \in O_{\mathbb{P}}\left(\sqrt{\frac{nr}{p}}+\sqrt{\frac{r}{n}}\right).\] (7)The proof of theorem 2 is in appendix C.2. We give an original and self-contained proof of (6). To prove (7) we use a recent uniform-concentration result for principal component scores from [49]. Overall, theorem 2 says that affinity estimation error vanishes if the dimension \(p\) grows faster enough relative to \(n\) (and \(r\) in the case of \(\hat{\alpha}_{\mathrm{pca}}\), noting that under **A6**, \(r\leq|\mathcal{Z}|\), so it is sensible to assume \(r\) is much smaller than \(n\) and \(p\)). The argument of \(O_{\mathbb{P}}(\cdot)\) is the convergence rate; if \((X_{p,n})\) is some collection of random variables indexed by \(p\) and \(n\), \(X_{p,n}\in O_{\mathbb{P}}(n^{2/q}/\sqrt{p})\) means that for any \(\epsilon>0\), there exists \(\delta\) and \(M\) such that \(n^{2/q}/\sqrt{p}\geq M\) implies \(\mathbb{P}(|X_{p,n}|>\delta)<\epsilon\). We note the convergence rate in (6) is decreasing in \(q\) where as rate in (7) is not. It is an open mathematical question whether (7) can be improved in this regard; sharpening the results of Whiteley et al. [49] used in the proof of (7) seems very challenging. However, when \(q=2\), (6) gives \(O_{\mathbb{P}}\left(n/\sqrt{p}\right)\) compared to \(O_{\mathbb{P}}\left(\sqrt{nr/p}+\sqrt{r/n}\right)\) in (7), i.e. an improvement from \(n\) to \(\sqrt{nr}\) in the first term. We explore empirical performance of \(\hat{\alpha}_{\mathrm{data}}\) versus \(\hat{\alpha}_{\mathrm{pca}}\) in section 4.2.

### Interpretation

By combining theorems 1 and 2, we find that when \(b>0\) is constant and \(\hat{\alpha}\) is either \(\hat{\alpha}_{\text{data}}\) or \(\hat{\alpha}_{\text{pca}}\), the merge distortion

\[\max_{i,j\in[n],i\neq j}|m(Z_{i},Z_{j})-\hat{m}(i,j)|\] (8)

converges to zero at rates given by the r.h.s of (6) and (7). To gain intuition into what (8) tells us about the resemblance between \(\hat{\mathcal{D}}\) and \(\mathcal{D}\), it is useful to consider an intermediate dendrogram illustrated in figure 1(b) which conveys the realized values of \(Z_{1},\ldots,Z_{n}\). This dendrogram is constructed from \(\mathcal{D}\) by adding a leaf vertex corresponding to each observation \(\mathbf{Y}_{i}\), with parent \(Z_{i}\) and height \(p^{-1}\mathbb{E}[\|\mathbf{Y}_{i}\|^{2}|Z_{1},\ldots,Z_{n}]=h(Z_{i})+p^{-1} \mathrm{tr}[\mathbf{S}(Z_{i})^{\top}\mathbf{S}(Z_{i})]\), and deleting any \(v\in\mathcal{Z}\) such that \(Z_{i}\neq v\) for all \(i\in[n]\) (e.g., vertex \(c\) in figure 1(b)). The resulting merge height between the vertices corresponding to \(\mathbf{Y}_{i}\) and \(\mathbf{Y}_{j}\) is \(m(Z_{i},Z_{j})\). (8) being small implies this must be close to \(\hat{m}(i,j)\) in \(\hat{\mathcal{D}}\) as in figure 1(c). Moreover, in the case \(\hat{\alpha}=\hat{\alpha}_{\text{data}}\), the height \(\hat{h}(\{i\})\) of leaf vertex \(\{i\}\) in \(\hat{\mathcal{D}}\) is upper bounded by \(p^{-1}\|\mathbf{Y}_{i}\|^{2}\), which under our statistical assumptions is concentrated around \(p^{-1}\mathbb{E}[\|\mathbf{Y}_{i}\|^{2}|Z_{1},\ldots,Z_{n}]\), i.e., the heights of the leaves in figure 1(c) approximate those of the corresponding leaves in figure 1(b).

Overall we see that \(\hat{\mathcal{D}}\) in figure 1(c) approximates the dendrogram in figure 1(b), and in turn \(\mathcal{D}\). However even if \(m(Z_{i},Z_{j})=\hat{m}(i,j)\) for all \(i,j\), the tree output from algorithm 1, \(\hat{\mathcal{T}}\), may not be isomorphic (i.e., equivalent up to relabelling of vertices) to the tree in figure 1(b); \(\hat{\mathcal{T}}\) is always binary and has \(2n-1\) vertices, whereas the tree in figure 1(b) may not be binary, depending on the underlying \(\mathcal{T}\) and the realization of \(Z_{1},\ldots,Z_{n}\). This reflects the fact that merge distortion, in general, is a _pseudometric_ on dendrograms. However, if one restricts attention to specific classes of true dendrograms \(\mathcal{D}\), for instance binary trees with non-zero branch lengths, then asymptotically algorithm 1 can recover them exactly. We explain this point further in appendix C.3.

## 4 Numerical experiments

We explore the numerical performance of algorithm 1 in the setting of five data sets summarised below. The real datasets used are open source, and full details of data preparation and sources are given in appendix D.

**Simulated data.** A simple tree structure with vertices \(\mathcal{V}=\{1,2,3,4,5,6,7,8\}\), edge set \(\mathcal{E}=\{6{\rightarrow}1,6{\rightarrow}2,6{\rightarrow}3,7{ \rightarrow}4,7{\rightarrow}5,8{\rightarrow}6,8{\rightarrow}7\}\) and \(\mathcal{Z}=\{1,2,3,4,5\}\) (the leaf vertices). \(Z_{1},\ldots,Z_{n}\) are drawn from the uniform distribution on \(\mathcal{Z}\). The \(X_{j}(v)\) are Gaussian random variables, independent across \(j\). Full details of how these variables are sampled are in appendix D. The elements of \(\mathbf{E}_{i}\) are standard Gaussian, and \(\mathbf{S}(v)=\sigma\mathbf{I}_{p}\) with \(\sigma=1\).

**20 Newsgroups.** We used a random subsample of \(n=5000\) documents from the well-known 20 Newsgroups data set [29]. Each data vector corresponds to one document, capturing its \(p=12818\) Term Frequency Inverse Document Frequency features. The value of \(n\) was chosen to put us in the regime \(p\geq n\), to which our theory is relevant - see section 3.2. Some ground-truth labelling of documents is known: each document is associated with \(1\) of \(20\) newsgroup topics, organized at two hierarchical levels.

**Zebrafish gene counts.** These data comprise gene counts in zebrafish embryo cells taken from their first day of development [47]. As embryos develop, cells differentiate into various types with specialised, distinct functions, so the data are expected to exhibit tree-like structure mapping these changes. We used a subsample such that \(n=5079\) and \(p=5498\) to put us in the \(p\geq n\) regime. Each cell has two labels: the tissue that the cell is from and a subcategory of this.

**Amazon reviews.** This dataset contains customer reviews on Amazon products [1]. A random sample of \(n=5000\) is taken and each data vector corresponds to one review with \(p=5594\) Term Frequency Inverse Document Frequency features. Each product reviewed has labels which make up a three-level hierarchy of product types.

**S&P 500 stock returns.** The data are \(p=1259\) daily returns between for \(n=368\) stocks which were constituents of the S&P 500 market index [2] between \(2013\) to \(2018\). The two-level hierarchy of stock sectors by industries and sub-industries follows the Global Industry Classification Standard [3].

### Comparing algorithm 1 to existing methods

We numerically compare algorithm 1 against three very popular variants of agglomerative clustering: UPGMA with Euclidean distance, Ward's method, and UPGMA with cosine distance. These are natural comparators because they work by iteratively merging clusters in a manner similar to algorithm 1, but using different criteria for choosing which clusters to merge. In appendix E we complement our numerical results with mathematical insights into how these methods perform under our modelling assumptions. Numerical results for other linkage functions and distances are given in appendix D. Several popular density-based clustering methods use some hierarchical structure, such as CURE [22], OPTICS [7] and BIRCH [52] but these have limitations which prevent direct comparisons: they aren't equipped with a way to simplify the structure into a tree, which it is our aim to recover, and only suggest extracting a flat partition based on a density threshold. HDBSCAN [9] is a density-based method that doesn't have these limitations, and we report numerical comparisons against it.

**Kendall \(\tau_{b}\) ranking correlation.** For real data some ground-truth hierarchical labelling may be available but ground-truth merge heights usually are not. We need a performance measure to quantitatively compare methods operating on such data. Commonly used clustering performance measures such as the Rand index [40] and others [23; 21] allow pairwise comparisons between partitions, but do not capture information about hierarchical structure. The cophenetic correlation coefficient [45] is commonly used to compare dendrograms, but relies on an assumption that points close in Euclidean distance should be considered similar which is incompatible with our notion of dot product affinity. To overcome these obstacles we formulate a performance measure as follows. For each of \(n\) data points, we rank the other \(n-1\) data points according to the order in which they merge with it in the ground-truth hierarchy. We then compare these ground truth rankings to those obtained from a given hierarchical clustering algorithm using the Kendall \(\tau_{b}\) correlation coefficient [26]. This outputs a value in the interval \([-1,1]\), with \(-1\), \(1\) and \(0\) corresponding to negative, positive and lack of association between the ground-truth and algorithm-derived rankings. We report the mean association value across all \(n\) data points as the overall performance measure. Table 1 shows results with raw data vectors \(\mathbf{Y}_{1:n}\) or PC scores \(\zeta_{1:n}\) taken as input to the various algorithms. For all the data sets except S&P 500, algorithm 1 is found to recover hierarchy more accurately than other methods. We include the results for the S&P 500 data to give a balanced scientific view, and in appendix E we discuss why our modelling assumptions may not be appropriate for these data, thus explaining the limitations of algorithm 1.

### Simulation study of dot product estimation with and without PCA dimension reduction

For high-dimensional data, reducing dimension with PCA prior to clustering may reduce overall computational cost. Assuming \(\zeta_{1:n}\) are obtained from, e.g., a partial SVD, in time \(O(npr)\), the time complexity of evaluating \(\hat{\alpha}_{\text{pca}}\) is \(O(npr+n^{2}r)\), versus \(O(n^{2}p)\) for \(\hat{\alpha}_{\text{data}}\), although this ignores the cost of choosing \(r\). In table 1 we see for algorithm 1, the results for input \(\mathbf{Y}_{1:n}\) are very similar to those for \(\zeta_{1:n}\). To examine this more closely and connect our findings to theorem 2, we now compare \(\hat{\alpha}_{\text{data}}\) and \(\hat{\alpha}_{\text{pca}}\) as estimates of \(\alpha\) through simulation. The model is as described at the start of section 4. In figure 2(a)-(b), we see that when \(p\) is growing with \(n\), and when \(p\) is constant, the \(\hat{\alpha}_{\text{pca}}\) error is very slightly smaller than the \(\hat{\alpha}_{\text{data}}\) error. By contrast, in figure 2(c), when \(n=10\) isfixed, we see that the \(\hat{\alpha}_{\text{pca}}\) error is larger than that for \(\hat{\alpha}_{\text{data}}\). This inferior performance of \(\hat{\alpha}_{\text{pca}}\) for very small and fixed \(n\) is explained by \(n\) appearing in the denominator of the second term in the rate \(O_{\mathbb{P}}(\sqrt{nr/p}+\sqrt{r/n})\) for \(\hat{\alpha}_{\text{pca}}\) in theorem 2 versus \(n\) appearing only in the numerator of \(O_{\mathbb{P}}(n^{2/q}/\sqrt{p})\) for \(\hat{\alpha}_{\text{data}}\). Since it is Gaussian, this simulation model has finite exponential-of-quadratic moments, which is a much stronger condition than **A4**; we conjecture the convergence rate in this Gaussian case is \(O_{\mathbb{P}}(\sqrt{\log n/p})\) for \(\hat{\alpha}_{\text{data}}\), which would be consistent with figure 2(a). These numerical results seem to suggest the rate for \(\hat{\alpha}_{\text{pca}}\) is similar, thus the second result of theorem 2 may not be sharp.

### Comparing dot product affinities and Euclidean distances for the 20 Newsgroups data

In this section we expand on the results in table 1 for the 20 Newsgroups data, by exploring how inter-topic and intra-topic dot product affinities and Euclidean distances relate to ground-truth labels. Most existing agglomerative clustering techniques quantify dissimilarity using Euclidean distance. To compare dot products and Euclidean distances, figures 3(a)-(b) show, for each topic, the top five topics with the largest average dot product and smallest average Euclidean distance respectively. We see that clustering of semantically similar topic classes is apparent when using dot products but not when using Euclidean distance. Note that the average dot product affinity between comp.windows.x and itself is not shown in figure 3(b), but is shown in 3(a), by the highest dark blue square in the comp.windows.x column. In appendix D we provide additional numerical results illustrating that with \(n\) fixed, performance in terms of \(\tau_{b}\) correlation coefficient increases with \(p\).

\begin{table}
\begin{tabular}{c c|c c c c c} \hline \hline Data & Input & Dot product & UPGMA w/ & HDBSCAN & UPGMA w/ & Ward \\  & & & cos. dist. & & Eucl. dist. & \\ \hline \multirow{2}{*}{Newsgroups} & \(\mathbf{Y}_{1:n}\) & 0.26 (2.9) & 0.26 (2.9) & -0.010 (0.65) & 0.23 (2.7) & 0.18 (2.5) \\  & \(\zeta_{1:n}\) & 0.24 (2.6) & 0.18 (1.9) & -0.016 (1.9) & 0.038 (1.5) & 0.19 (2.7) \\ \hline \multirow{2}{*}{Zebrafish} & \(\mathbf{Y}_{1:n}\) & 0.34 (3.4) & 0.25 (3.1) & 0.023 (2.9) & 0.27 (3.2) & 0.30 (3.8) \\  & \(\zeta_{1:n}\) & 0.34 (3.4) & 0.27 (3.2) & 0.11 (2.8) & 0.16 (2.5) & 0.29 (3.8) \\ \hline \multirow{2}{*}{Reviews} & \(\mathbf{Y}_{1:n}\) & 0.15 (2.5) & 0.12 (1.9) & 0.014 (1.1) & 0.070 (1.5) & 0.10 (1.8) \\  & \(\zeta_{1:n}\) & 0.14 (2.4) & 0.14 (2.4) & -0.0085 (0.78) & 0.14 (2.6) & 0.12 (2.4) \\ \hline \multirow{2}{*}{S\&P 500} & \(\mathbf{Y}_{1:n}\) & 0.34 (10) & 0.34 (10) & 0.14 (9.3) & 0.34 (1) & 0.35 (10) \\  & \(\zeta_{1:n}\) & 0.36 (9.4) & 0.42 (11) & 0.33 (13) & 0.39 (11) & 0.39 (11) \\ \hline \multirow{2}{*}{Simulated} & \(\mathbf{Y}_{1:n}\) & 0.86 (1) & 0.81 (2) & 0.52 (8) & 0.52 (8) & 0.52 (8) \\  & \(\zeta_{1:n}\) & 0.86 (1) & 0.81 (2) & 0.52 (8) & 0.52 (8) & 0.52 (8) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Kendall \(\tau_{b}\) ranking performance measure. For the dot product method, i.e., algorithm 1, \(\mathbf{Y}_{1:n}\) as input corresponds to using \(\hat{\alpha}_{\text{data}}\), and \(\zeta_{1:n}\) corresponds to \(\hat{\alpha}_{\text{pca}}\). The mean Kendall \(\tau_{b}\) correlation coefficient is reported alongside the standard error (numerical value shown is the standard error\(\times 10^{3}\)).

Figure 3: Analysis of the 20 Newsgroups data. Marker shapes correspond to newsgroup classes and marker colours correspond to topics within classes. The first/second columns show results for dot products/Euclidean distances respectively. First row: for each topic (\(x\)-axis), the affinity/distance (\(y\)-axis) to the top five best-matching topics, calculated using average linkage of PC scores between documents within topics. Second row: average affinity/distance between documents labelled ‘comp.windows.x’ and all other topics. Third row: dendrograms output from algorithm 1 and UPGMA applied to cluster topics.

For one topic ('comp.windows.x') the results are expanded in figures 3(c)-(d) to show the average dot products and average Euclidean distances to all other topics. Four out of the five topics with the largest dot product affinity belong to the same 'comp' topic class and other one is a semantically similar'sci.crypt' topic. Whereas, the other topics in the same 'comp' class are considered dissimilar in terms of Euclidean distance.

In order to display visually compact estimated dendrograms, we applied algorithm 1 and UPGMA in a semi-supervised setting where each topic is assigned its own PC score, taken to be the average of the PC scores of the documents in that topic, and then the algorithms are applied to cluster the topics. The results are shown in figures 3(e)-(f) (for ease of presentation, leaf vertex 'heights' are fixed to be equal).

## 5 Limitations and opportunities

Our algorithm is motivated by modelling assumptions. If these assumptions are not appropriate for the data at hand, then the algorithm cannot be expected to perform well. A notable limitation of our model is that \(\alpha(u,v)\geq 0\) for all \(u,v\in\mathcal{V}\) (see lemma 3 in appendix C). This is an inappropriate assumption when there are strong negative cross-correlations between some pairs of data vectors, and may explain why our algorithm has inferior performance on the S&P 500 data in table 1. Further discussion is given in appendix E. A criticism of agglomerative clustering algorithms in their basic form is that their computational cost scales faster than \(O(n^{2})\). Approximations to standard agglomerative methods which improve computational scalability have been proposed [33; 5; 35]. Future research could investigate analogous approximations and speed-up of our method. Fairness in hierarchical clustering has been recently studied in cost function-based settings by [6] and in greedy algorithm settings by [13]. Future work could investigate versions of our algorithm which incorporate fairness measures.

## References

* [1] Amazon product reviews dataset. https://www.kaggle.com/datasets/kashnitsky/hierarchical-text-classification,. Accessed: 2023-05-16.
* [2] S&P500 stock data. https://www.kaggle.com/datasets/camnugent/sandp500,. Accessed: 2023-05-16.
* [3] S&P500 hierarchy. https://en.wikipedia.org/wiki/List_of_S%26P_500_companies,. Accessed: 2023-05-16.
* [4] Wikipedia list of phylogenetics software. https://en.m.wikipedia.org/wiki/List_of_phylogenetics_software. Accessed: 2023-05-16.
* [5] Amir Abboud, Vincent Cohen-Addad, and Hussein Houdrouge. Subquadratic high-dimensional hierarchical clustering. _Advances in Neural Information Processing Systems_, 32, 2019.
* [6] Sara Ahmadian, Alessandro Epasto, Marina Knittel, Ravi Kumar, Mohammad Mahdian, Benjamin Moseley, Philip Pham, Sergei Vassilvitskii, and Yuyan Wang. Fair hierarchical clustering. _Advances in Neural Information Processing Systems_, 33:21050-21060, 2020.
* [7] Mihael Ankerst, Markus M Breunig, Hans-Peter Kriegel, and Jorg Sander. Optics: Ordering points to identify the clustering structure. _ACM Sigmod record_, 28(2):49-60, 1999.
* [8] Matteo Barigozzi, Haeran Cho, and Dom Owens. Fnets: Factor-adjusted network estimation and forecasting for high-dimensional time series. _arXiv preprint arXiv:2201.06110_, 2022.
* [9] Ricardo JGB Campello, Davoud Moulavi, and Jorg Sander. Density-based clustering based on hierarchical density estimates. In _Advances in Knowledge Discovery and Data Mining: 17th Pacific-Asia Conference, PAKDD 2013, Gold Coast, Australia, April 14-17, 2013, Proceedings, Part II 17_, pages 160-172. Springer, 2013.
* [10] Gunnar E Carlsson, Facundo Memoli, et al. Characterization, stability and convergence of hierarchical clustering methods. _J. Mach. Learn. Res._, 11(Apr):1425-1470, 2010.
* [11] Moses Charikar and Vaggos Chatziafratis. Approximate hierarchical clustering via sparsest cut and spreading metrics. In _Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms_, pages 841-854. SIAM, 2017.
* [12] Vaggos Chatziafratis, Mohammad Mahdian, and Sara Ahmadian. Maximizing agreements for ranking, clustering and hierarchical clustering via max-cut. In _International Conference on Artificial Intelligence and Statistics_, pages 1657-1665. PMLR, 2021.
* [13] Anshuman Chhabra and Prasant Mohapatra. Fair algorithms for hierarchical agglomerative clustering. In _2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA)_, pages 206-211. IEEE, 2022.
* [14] Vincent Cohen-Addad, Varun Kanade, and Frederik Mallmann-Trenn. Hierarchical clustering beyond the worst-case. _Advances in Neural Information Processing Systems_, 30, 2017.
* [15] Vincent Cohen-Addad, Varun Kanade, Frederik Mallmann-Trenn, and Claire Mathieu. Hierarchical clustering: Objective functions and algorithms. _Journal of the ACM (JACM)_, 66(4):1-42, 2019.
* [16] Sanjoy Dasgupta. A cost function for similarity-based hierarchical clustering. In _Proceedings of the forty-eighth annual ACM symposium on Theory of Computing_, pages 118-127, 2016.
* [17] William HE Day and Herbert Edelsbrunner. Efficient algorithms for agglomerative hierarchical clustering methods. _Journal of classification_, 1(1):7-24, 1984.
* [18] Ronald W DeBry. The consistency of several phylogeny-inference methods under varying evolutionary rates. _Molecular Biology and Evolution_, 9(3):537-551, 1992.
* [19] Roland L Dobrushin. Central limit theorem for nonstationary markov chains. i. _Theory of Probability & Its Applications_, 1(1):65-80, 1956.

* Eldridge et al. [2015] Justin Eldridge, Mikhail Belkin, and Yusu Wang. Beyond hartigan consistency: Merge distortion metric for hierarchical clustering. In _Conference on Learning Theory_, pages 588-606. PMLR, 2015.
* Fowlkes and Mallows [1983] Edward B Fowlkes and Colin L Mallows. A method for comparing two hierarchical clusterings. _Journal of the American statistical association_, 78(383):553-569, 1983.
* Guha et al. [1998] Sudipto Guha, Rajeev Rastogi, and Kyuseok Shim. Cure: An efficient clustering algorithm for large databases. _ACM Sigmod record_, 27(2):73-84, 1998.
* Hubert and Arabie [1985] Lawrence Hubert and Phipps Arabie. Comparing partitions. _Journal of classification_, 2:193-218, 1985.
* Jain and Dubes [1988] Anil K Jain and Richard C Dubes. _Algorithms for clustering data_. Prentice-Hall, Inc., 1988.
* Johnson [1967] Stephen C Johnson. Hierarchical clustering schemes. _Psychometrika_, 32(3):241-254, 1967.
* Kendall [1948] Maurice George Kendall. _Rank correlation methods_. Griffin, 1948.
* Kim et al. [2016] Jisu Kim, Yen-Chi Chen, Sivaraman Balakrishnan, Alessandro Rinaldo, and Larry Wasserman. Statistical inference for cluster trees. _Advances in Neural Information Processing Systems_, 29, 2016.
* Koller and Friedman [2009] Daphne Koller and Nir Friedman. _Probabilistic graphical models: principles and techniques_. MIT press, 2009.
* Lang [1995] Ken Lang. Newsweeder: Learning to filter netnews. In _Machine learning proceedings 1995_, pages 331-339. Elsevier, 1995.
* Lauritzen [1996] Steffen L Lauritzen. _Graphical models_, volume 17. Clarendon Press, 1996.
* Manghiuc and Sun [2021] Bogdan-Adrian Manghiuc and He Sun. Hierarchical clustering: \(o(1)\)-approximation for well-clustered graphs. _Advances in Neural Information Processing Systems_, 34:9278-9289, 2021.
* Mengistu et al. [2016] Henok Mengistu, Joost Huizinga, Jean-Baptiste Mouret, and Jeff Clune. The evolutionary origins of hierarchy. _PLoS computational biology_, 12(6):e1004829, 2016.
* Monath et al. [2019] Nicholas Monath, Ari Kobren, Akshay Krishnamurthy, Michael R Glass, and Andrew McCallum. Scalable hierarchical clustering with tree grafting. In _Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 1438-1448, 2019.
* Moorjani et al. [2016] Priya Moorjani, Carlos Eduardo G Amorim, Peter F Arndt, and Molly Przeworski. Variation in the molecular clock of primates. _Proceedings of the National Academy of Sciences_, 113(38):10607-10612, 2016.
* Moseley et al. [2021] Benjamin Moseley, Sergei Vassilvitskii, and Yuyan Wang. Hierarchical clustering in general metric spaces using approximate nearest neighbors. In _International Conference on Artificial Intelligence and Statistics_, pages 2440-2448. PMLR, 2021.
* Murtagh [1983] Fionn Murtagh. A survey of recent advances in hierarchical clustering algorithms. _The computer journal_, 26(4):354-359, 1983.
* Murtagh and Contreras [2012] Fionn Murtagh and Pedro Contreras. Algorithms for hierarchical clustering: an overview. _Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery_, 2(1):86-97, 2012.
* Pedregosa et al. [2011] Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. _the Journal of machine Learning research_, 12:2825-2830, 2011.
* Peligrad [1985] Magda Peligrad. An invariance principle for \(\varphi\)-mixing sequences. _The Annals of Probability_, pages 1304-1313, 1985.
* Rand [1971] William M Rand. Objective criteria for the evaluation of clustering methods. _Journal of the American Statistical association_, 66(336):846-850, 1971.

* [41] Chandan K Reddy and Bhanukiran Vinzamuri. A survey of partitional and hierarchical clustering algorithms. In _Data clustering_, pages 87-110. Chapman and Hall/CRC, 2018.
* [42] Alessandro Rinaldo, Aarti Singh, Rebecca Nugent, and Larry Wasserman. Stability of density-based clustering. _Journal of Machine Learning Research_, 13:905, 2012.
* [43] Aurko Roy and Sebastian Pokutta. Hierarchical clustering via spreading metrics. _Advances in Neural Information Processing Systems_, 29, 2016.
* [44] R. R. Sokal and C. D. Michener. A statistical method for evaluating systematic relationships. _University of Kansas Science Bulletin_, 38:1409-1438, 1958.
* [45] Robert R Sokal and F James Rohlf. The comparison of dendrograms by objective methods. _Taxon_, pages 33-40, 1962.
* [46] Baris Sumengen, Anand Rajagopalan, Gui Citovsky, David Simcha, Olivier Bachem, Pradipta Mitra, Sam Blasiak, Mason Liang, and Sanjiv Kumar. Scaling hierarchical agglomerative clustering to billion-sized datasets. _arXiv preprint arXiv:2105.11653_, 2021.
* [47] Daniel E Wagner, Caleb Weinreb, Zach M Collins, James A Briggs, Sean G Megason, and Allon M Klein. Single-cell mapping of gene expression landscapes and lineage in the zebrafish embryo. _Science_, 360(6392):981-987, 2018.
* [48] Joe H Ward Jr. Hierarchical grouping to optimize an objective function. _Journal of the American statistical association_, 58(301):236-244, 1963.
* [49] Nick Whiteley, Annie Gray, and Patrick Rubin-Delanchy. Discovering latent topology and geometry in data: a law of large dimension. _arXiv preprint arXiv:2208.11665_, 2022.
* [50] F Alexander Wolf, Philipp Angerer, and Fabian J Theis. Scanpy: large-scale single-cell gene expression data analysis. _Genome biology_, 19:1-5, 2018.
* [51] Wang Xuejun, Hu Shuhe, Yang Wenzhi, and Shen Yan. On complete convergence for weighted sums of-mixing random variables. _Journal of Inequalities and Applications_, 2010:1-13, 2010.
* [52] Tian Zhang, Raghu Ramakrishnan, and Miron Livny. Birch: an efficient data clustering method for very large databases. _ACM sigmod record_, 25(2):103-114, 1996.

[MISSING_PAGE_FAIL:14]

Proof.: If \(u=v\), \(\alpha(u,v)\geq 0\) holds immediately from the definition of \(\alpha\) in (2). For \(u\neq v\) with most recent common ancestor \(w\),

\[\mathbb{E}[\langle\mathbf{X}(u),\mathbf{X}(v)\rangle] =\sum_{j=1}^{p}\mathbb{E}\left[\mathbb{E}[X_{j}(u)X_{j}(v)|X_{j}( w)]\right]\] \[=\sum_{j=1}^{p}\mathbb{E}\left[\mathbb{E}[X_{j}(u)|X_{j}(w)] \mathbb{E}[X_{j}(v)|X_{j}(w)]\right]\] \[=\sum_{j=1}^{p}\mathbb{E}[|X_{j}(w)|^{2}]\geq 0,\]

where the second equality uses **A1** together with standard conditional independence aguments, and the third equality uses **A2**.

Proof of lemma 1.: Let \(w\) be the most recent common ancestor of \(u\) and \(v\). For each \(j=1,\ldots,p\), the property **A1** together with standard conditional independence arguments imply that \(X_{j}(u)\) and \(X_{j}(v)\) are conditionally independent given \(X_{j}(w)\), and the property **A2** implies that \(\mathbb{E}[X_{j}(u)|X_{j}(w)]=\mathbb{E}[X_{j}(v)|X_{j}(w)]=X_{j}(w)\). Therefore, by the tower property of conditional expectation,

\[\mathbb{E}[X_{j}(u)X_{j}(v)] =\mathbb{E}\left[\mathbb{E}[X_{j}(u)X_{j}(v)|X_{j}(w)]\right]\] \[=\mathbb{E}\left[\mathbb{E}[X_{j}(u)|X_{j}(w)]\mathbb{E}[X_{j}(v )|X_{j}(w)]\right]\] \[=\mathbb{E}[X_{j}(w)^{2}].\]

Hence, using the definitions of the merge height \(m\), the height \(h\) and the affinity \(\alpha\),

\[m(u,v)=h(w)=\alpha(w,w)=\frac{1}{p}\sum_{j=1}^{p}\mathbb{E}[X_{j}(w)^{2}]=\frac {1}{p}\sum_{j=1}^{p}\mathbb{E}[X_{j}(u)X_{j}(v)]=\alpha(u,v),\]

which proves the first equality in the statement. The second equality is the definition of \(\alpha\).

For the third equality in the statement, we have

\[d(u,v) =h(u)+h(v)-2h(w)\] \[=\alpha(u,u)+\alpha(v,v)-2\alpha(u,v)\] \[=\frac{1}{p}\mathbb{E}\left[\left\langle\mathbf{X}(u),\mathbf{X }(u)\right\rangle\right]+\frac{1}{p}\mathbb{E}\left[\left\langle\mathbf{X}(v), \mathbf{X}(v)\right\rangle\right]-2\frac{1}{p}\mathbb{E}\left[\left\langle \mathbf{X}(u),\mathbf{X}(v)\right\rangle\right]\] \[=\frac{1}{p}\mathbb{E}\left[\left\|\mathbf{X}(u)-\mathbf{X}(v) \right\|^{2}\right],\]

where the first equality uses the definition of \(d\), and the second equality uses the definition of \(h\) and \(h(w)=m(u,v)=\alpha(u,v)\). 

### Proof of Theorem 1

The following lemma establishes an identity concerning the affinities computed in algorithm 1 which will be used in the proof of theorem 1.

**Lemma 4**.: _Let \(P_{m}\), \(m\geq 0\), be the sequence of partitions of \([n]\) constructed in algorithm 1. Then for any \(m\geq 0\),_

\[\hat{\alpha}(u,v)=\frac{1}{|u||v|}\sum_{i\in u,j\in v}\hat{\alpha}(i,j),\quad \text{for all distinct pairs }u,v\in P_{m}.\] (9)

Proof.: The proof is by induction on \(m\). With \(m=0\), (9) holds immediately since \(P_{0}=\{\{1\},\ldots,\{n\}\}\). Now suppose (9) holds at step \(m\). Then for any distinct pair \(w,w^{\prime}\in P_{m+1}\)either \(w\) or \(w^{\prime}\) is the result of merging two elements of \(P_{m}\), or \(w\) and \(w^{\prime}\) are both elements of \(P_{m}\). In the latter case the induction hypothesis immediately implies:

\[\hat{\alpha}(w,w^{\prime})=\frac{1}{|w||w^{\prime}|}\sum_{i\in w,j\in w^{\prime} }\hat{\alpha}(i,j).\]

In the case that \(w\) or \(w^{\prime}\) is the result of a merge, suppose w.l.o.g. that \(w=u\cup v\) for some \(u,v\in P_{m}\) and \(w^{\prime}\in P_{m}\). Then by definition of \(\hat{\alpha}\) in algorithm 1,

\[\hat{\alpha}(w,w^{\prime}) =\frac{|u|}{|w|}\hat{\alpha}(u,w^{\prime})+\frac{|v|}{|w|}\hat{ \alpha}(v,w^{\prime})\] \[=\frac{|u|}{|w|}\frac{1}{|u||w^{\prime}|}\sum_{i\in u,j\in w^{ \prime}}\hat{\alpha}(i,j)+\frac{|v|}{|w|}\frac{1}{|v||w^{\prime}|}\sum_{i\in v,j\in w^{\prime}}\hat{\alpha}(i,j)\] \[=\frac{1}{|w||w^{\prime}|}\sum_{i\in w,j\in w^{\prime}}\hat{ \alpha}(i,j),\]

where the final equality uses \(w=u\cup v\). The induction hypothesis thus holds at step \(m+1\). 

The following proposition establishes the validity of the height function constructed in algorithm 1. Some of the arguments used in this proof are qualitatively similar to those used to study reducible linkage functions by, e.g., Sumengen et al. [46], see also historical references therein.

**Proposition 1**.: _With \(\hat{\mathcal{V}}\) the vertex set and \(\hat{h}\) the height function constructed in algorithm 1 with any symmetric, real-valued input \(\hat{\alpha}(\cdot,\cdot)\), it holds that \(\hat{h}(v)\geq\hat{h}(\mathrm{Pa}_{v})\) for all vertices \(v\in\hat{\mathcal{V}}\) except the root._

Proof.: The required inequality \(\hat{h}(v)\geq\hat{h}(\mathrm{Pa}_{v})\) holds immediately for all the leaf vertices \(v\in P_{0}=\{\{1\},\ldots,\{n\}\}\) by the definition of \(\hat{h}\) in algorithm 1. All the remaining vertices in the output tree, i.e., those in \(\hat{\mathcal{V}}\setminus P_{0}\), are formed by merges over the course of the algorithm. For \(m\geq 0\) let \(w_{m}=u_{m}\cup v_{m}\) denote the vertex formed by merging some \(u_{m},v_{m}\in P_{m}\). Then \(w_{m}=\mathrm{Pa}_{u_{m}}\) and \(w_{m}=\mathrm{Pa}_{v_{m}}\). Each \(u_{m}\) is either a member of \(P_{0}\) or equal to \(w_{m^{\prime}}\) for some \(m^{\prime}<m\). The same is true of each \(v_{m}\). It therefore suffices to show that \(\hat{h}(w_{m})\geq\hat{h}(w_{m+1})\) for \(m\geq 0\), where by definition in the algorithm, \(\hat{h}(w_{m})=\hat{\alpha}(u_{m},v_{m})\). Also by definition in the algorithm, \(\hat{h}(w_{m+1})\) is the largest pairwise affinity between elements of \(P_{m+1}\). Our objective therefore is to upper-bound this largest affinity and compare it to \(\hat{h}(w_{m})=\hat{\alpha}(u_{m},v_{m})\).

The affinity between \(w_{m}=u_{m}\cup v_{m}\) and any other element \(w^{\prime}\) of \(P_{m+1}\) (which must also be an element of \(P_{m}\)) is, by definition in the algorithm,

\[\hat{\alpha}(w_{m},w^{\prime}) =\frac{|u_{m}|}{|u_{m}|+|v_{m}|}\hat{\alpha}(u_{m},w^{\prime})+ \frac{|v_{m}|}{|u_{m}|+|v_{m}|}\hat{\alpha}(v_{m},w^{\prime})\] \[\leq\max\{\hat{\alpha}(u_{m},w^{\prime}),\hat{\alpha}(v_{m},w^{ \prime})\}\] \[\leq\hat{\alpha}(u_{m},v_{m}),\]

where the last inequality holds because \(u_{m},v_{m}\), by definition, have the largest affinity amongst all elements of \(P_{m}\). For the same reason, the affinity between any two distinct elements of \(P_{m+1}\) neither of which is \(w_{m}\) (and therefore both of which are elements of \(P_{m}\)) is upper-bounded by \(\hat{\alpha}(u_{m},v_{m})\). We have therefore established \(\hat{h}(w_{m})=\hat{\alpha}(u_{m},v_{m})\geq\hat{h}(w_{m+1})\) as required, and this completes the proof.

Proof of theorem 1.: Let us introduce some definitions used throughout the proof.

\[M\coloneqq\max_{i\neq j}|\hat{\alpha}(i,j)-\alpha(z_{i},z_{j})|.\] (10)

We arbitrarily chose and then fix \(i,j\in[n]\) with \(i\neq j\), and define

\[H\coloneqq m(z_{i},z_{j}),\qquad\hat{H}\coloneqq\hat{m}(i,j).\] (11)Let \(u\) denote the most recent common ancestor of the leaf vertices \(\{i\}\) and \(\{j\}\) in \(\tilde{\mathcal{D}}\) and let \(m\geq 1\) denote the step of the algorithm at which \(u\) is created by a merge, that is \(m=\min\{m^{\prime}\geq 1:u\in P_{m^{\prime}}\}\). We note that by construction, \(u\) is equal to the union of all leaf vertices with ancestor \(u\), and by definition of \(\hat{h}\) in algorithm 1 \(\hat{h}(u)=\hat{H}\).

Let \(v\) denote the most recent common ancestor of \(z_{i}\) and \(z_{j}\) in \(\mathcal{D}\), which has height \(h(v)=H\).

Lower bound on \(m(z_{i},z_{j})-\hat{m}(i,j)\).There is no partition of \(u\) into two non-empty sets \(A,B\subseteq[n]\) such that \(\hat{\alpha}(k,l)<\hat{H}\) for all \(k\in A\) and \(l\in B\). We prove this by contradiction. Suppose that such a partition exists. There must be a step \(m^{\prime}\leq m\) at which some \(A^{\prime}\subseteq A\) is merged some \(B^{\prime}\subseteq B\). The vertex \(w\) formed by this merge would have height

\[\hat{h}(w) =\hat{\alpha}(A^{\prime},B^{\prime})\] \[=\frac{1}{|A^{\prime}||B^{\prime}|}\sum_{k\in A^{\prime},l\in B^{ \prime}}\hat{\alpha}(k,l)<\hat{H}=\hat{h}(u),\]

where the first equality is the definition of \(\hat{h}(w)\) in the algorithm and the second equality holds by lemma 4. However, in this construction \(u\) is an ancestor of \(w\), and \(\hat{h}(w)<\hat{h}(u)\) therefore contradicts the result of proposition 1.

As a device to be used in the next step of the proof, consider an undirected graph with vertex set \(u\), in which there is an edge between two vertices \(k\) and \(l\) if and only if \(\hat{\alpha}(k,l)\geq\hat{H}\). Then, because there is no partition as established above, this graph must be connected. Now consider a second undirected graph, also with vertex set \(u\), in which there is any edge between two vertices \(k\) and \(l\) if and only if \(\alpha(z_{k},z_{l})\geq\hat{H}-M\). Due to the definition of \(M\) in (10), any edge in the first graph is an edge in the second, so the second graph is connected too. Let \(k\), \(l\), and \(\ell\) be any distinct members of \(u\). Using the fact established in lemma 1 that \(\alpha(z_{k},z_{l})\) and \(\alpha(z_{l},z_{\ell})\) are respectively the merge heights in \(\mathcal{D}\) between \(z_{k}\) and \(z_{l}\), and \(z_{l}\) and \(z_{\ell}\), it can be seen that if there are edges between \(k\) and \(l\) and between \(l\) and \(\ell\) in the second graph, there must also be an edge in that graph between \(k\) and \(\ell\). Combined with the connectedness, this implies that the second graph is complete, so that \(\alpha(z_{k},z_{l})\geq\hat{H}-M\) for all distinct \(k,l\in u\). In particular \(\alpha(z_{i},z_{j})\geq\hat{H}-M\), and since \(m(z_{i},z_{j})=\alpha(z_{i},z_{j})\), we find

\[m(z_{i},z_{j})-\hat{m}(i,j)\geq-M.\] (12)

Upper bound on \(m(z_{i},z_{j})-\hat{m}(i,j)\).Let \(S_{v}=\{i\in[n]:z_{i}=v\text{ or }z_{i}\text{ has ancestor }v\text{ in }\mathcal{D}\}\). For \(k,l\in S_{v}\), lemma 1 tells us \(\alpha(z_{k},z_{l})\) is the merge height between \(z_{k}\) and \(z_{l}\), so \(\alpha(z_{k},z_{l})\geq H\). Using (10), we therefore have

\[\hat{\alpha}(k,l)\geq H-M,\quad\forall k,l\in S_{v}.\] (13)

It follows from the definition of \(\hat{h}\) in the algorithm that if \(S_{v}=[n]\), the heights of all vertices in \(\hat{\mathcal{D}}\) are greater than or equal to \(H-M\). This implies \(\hat{H}\geq H-M\). In summary, we have shown that when \(S_{v}=[n]\),

\[m(z_{i},z_{j})-\hat{m}(i,j)\leq M.\] (14)

It remains to consider the case \(S_{v}\neq[n]\). The proof of the same upper bound (14) in this case is more involved. In summary, we need to establish that the most recent common ancestor of \(\{i\}\) and \(\{j\}\) in \(\tilde{\mathcal{D}}\) has height at least \(H-M\). The main idea of the proof is to consider the latest step of the algorithm at which a vertex with height at least \(H-M\) is formed by a merge, and show the partition formed by this merge contains the most recent common ancestor of \(\{i\}\) and \(\{j\}\), or an ancestor thereof.

To this end let \(m^{*}\) denote the latest step in algorithm 1 at which the vertex formed, \(w^{*}\), has height greater than or equal to \(H-M\). To see that \(m^{*}\) must exist, notice

\[\max_{k\neq l\in[n]}\hat{\alpha}(k,l)\geq\alpha(z_{i},z_{j})-M,\] (15)

by definition of \(M\) in (10). Combined with the definition of \(\hat{h}\) in algorithm 1, the vertex formed by the merge at step \(1\) of the algorithm therefore has height greater than or equal to \(H-M\). Therefore \(m^{*}\) is indeed well-defined.

Our next objective is to show that the partition \(P_{m^{*}}\) formed at step \(m^{*}\) contains an element which itself contains both \(i\) and \(j\). We proceed by establishing some facts about \(S_{v}\) and \(P_{m^{*}}\).

Let \(\bar{S}_{v}\coloneqq[n]\setminus S_{v}\). For \(k\in S_{v}\), \(l\in\bar{S}_{v}\), \(v\) cannot be an ancestor of \(z_{l}\), by lemma 1\(\alpha(z_{k},z_{l})\) is the merge height of \(z_{k}\) and \(z_{l}\), and \(b\) is the minimum branch length in \(\mathcal{D}\), so we have \(\alpha(z_{k},z_{l})\leq H-b\). From (10) we then find

\[\hat{\alpha}(k,l)\leq H-b+M,\quad\forall\,k\in S_{v},l\in\bar{S}_{v}.\] (16)

We claim that no element of \(P_{m^{*}}\) can contain both an element of \(S_{v}\) and an element of \(\bar{S}_{v}\). We prove this claim by contradiction. If such an element of \(P_{m^{*}}\) did exist, there would be a step \(m^{\prime}\leq m^{*}\) at which some \(A^{\prime}\subseteq S_{v}\) is merged with some \(B^{\prime}\subseteq\bar{S}_{v}\). But the vertex \(w^{\prime}\) formed by this merge would be assigned height \(\hat{h}(w^{\prime})=\hat{\alpha}(A^{\prime},B^{\prime})\leq H-b+M<H-M\), where the first inequality uses lemma 4 and (16), and the second inequality uses the assumption of the theorem that \(M<b/2\). Recalling the definition of \(w^{\star}\) we have \(\hat{h}(w^{*})\geq H-M\). We therefore see that \(w^{*}\) is an ancestor of \(w^{\prime}\) with \(\hat{h}(w^{*})>\hat{h}(w^{\prime})\), contradicting the result of proposition 1.

Consider the elements of \(P_{m^{*}}\), denoted \(A\) and \(B\), which contain \(i\) and \(j\) respectively. We claim that \(A=B\). We prove this claim by contradiction. Suppose \(A\neq B\). As established in the previous paragraph, neither \(A\) nor \(B\) can contain an element of \(\bar{S}_{v}\). Therefore, using lemma 4 and (13),

\[\hat{\alpha}(A,B)=\frac{1}{|A||B|}\sum_{k\in A,l\in B}\hat{\alpha}(k,l)\geq H-M.\]

Again using the established fact that no element of \(P_{m^{*}}\) can contain both an element of \(S_{v}\) and an element of \(\bar{S}_{v}\), \(m^{*}\) cannot be the final step of the algorithm, since that would require \(P_{m^{*}}=\{[n]\}\). Therefore \(\hat{\alpha}(A,B)\) is one of the affinities which algorithm 1 would maximise over at step \(m^{*}+1\), so the height of the vertex formed by a merge at step \(m^{*}+1\) would be greater than or equal to \(H-M\), which contradicts the definition of \(m^{*}\). Thus we have proved there exists an element of \(P_{m^{*}}\) which contains both \(i\) and \(j\). This element must be the most recent common ancestor of \(\{i\}\) and \(\{j\}\), or an ancestor thereof. Also, this element must have been formed by a merge at a step less than or equal to \(m^{\star}\) and so must have height greater than or equal to \(H-M\). Invoking proposition 1 we have thus established \(\hat{H}\geq H-M\). In summary, in the case \(S_{v}\neq[n]\), we have shown

\[m(z_{i},z_{j})-\hat{m}(i,j)\leq M.\] (17)

Combining the lower bound (12) with the upper bounds (14), (17) and the fact that \(i,j\) were chosen arbitrarily, completes the proof.

### Supporting material and proof for Theorem 2

#### Definitions and interpretation for assumptions A3 and A5

We recall the definition of \(\varphi\)-mixing from, e.g., [39]. For a sequence of random variables \(\{\xi_{j};j\geq 1\}\), define:

\[\varphi(k)\coloneqq\sup_{j\geq 1}\sup_{A\in\mathcal{F}_{1}^{j},B\in\mathcal{ F}_{\gamma+k}^{\infty},\mathbb{P}(A)>0}\left|\mathbb{P}(B|A)-\mathbb{P}(B) \right|.\]

where \(\mathcal{F}_{i}^{j}\) is the \(\sigma\)-algebra generated by \(\xi_{i},\dots,\xi_{j}\). Then \(\{\xi_{j};j\geq 1\}\) is said to be \(\varphi\)-mixing if \(\varphi(k)\searrow 0\) as \(k\to\infty\).

To interpret assumption **A5** notice

\[\mathbb{E}[\|\mathbf{S}(Z_{i})\mathbf{E}_{i}\|^{2}|Z_{1},\dots,Z_{n}]\leq\| \mathbf{S}(Z_{i})\|_{\mathrm{op}}^{2}\mathbb{E}[\|\mathbf{E}_{i}\|^{2}]\leq \max_{v\in\mathcal{Z}}\|\mathbf{S}(v)\|_{\mathrm{op}}^{2}\,p\,\mathbb{E}[| \mathbf{E}_{11}|^{2}],\]

where the first inequality uses the independence of \(\mathbf{E}_{i}\) and \(Z_{i}\) and the second inequality uses the fact that the elements of the vectors \(\mathbf{E}_{i}\) are i.i.d. Since \(\mathbf{Y}_{i}-\mathbf{X}(Z_{i})=\mathbf{S}(Z_{i})\mathbf{E}_{i}\), **A5** thus implies \(\mathbb{E}[\|\mathbf{Y}_{i}-\mathbf{X}(Z_{i})\|^{2}]\in O(p)\) as \(p\to\infty\), which can be viewed as a natural growth rate since \(p\) is the dimension of the disturbance vector \(\mathbf{Y}_{i}-\mathbf{X}(Z_{i})\). In the proof of proposition 2 below, **A5** is used in a similar manner to control dot products of the form \(\langle\mathbf{Y}_{i}-\mathbf{X}_{i},\mathbf{X}_{j}\rangle\) and \(\langle\mathbf{Y}_{i}-\mathbf{X}_{i},\mathbf{Y}_{j}-\mathbf{X}_{j}\rangle\).

Proof of Theorem 2.: For the first claim of the theorem, proposition 2 combined with the tower property of conditional expectation imply that for any \(\delta>0\),

\[\mathbb{P}\left(\max_{1\leq i<j\leq n}\left|p^{-1}\left\langle\mathbf{ Y}_{i},\mathbf{Y}_{j}\right\rangle-\alpha(Z_{i},Z_{j})\right|\geq\delta\right)\\ \leq\frac{1}{\delta^{q}}\frac{1}{p^{q/2}}\frac{n(n-1)}{2}C(q, \varphi)M(q,\mathbf{X},\mathbf{E},\mathbf{S}),\] (18)

from which (6) follows.

The second claim of the theorem is in essence a corollary to [49][Thm 1]. A little work is needed to map the setting of the present work on to the setting of Whiteley et al. [49][Thm 1]. To see the connection, we endow the finite set \(\mathcal{Z}\) in the present work with the discrete metric: \(d_{\mathcal{Z}}(u,v)\coloneqq 0\) for \(u\neq v\), and \(d_{\mathcal{Z}}(v,v)=0\). Then \((\mathcal{Z},d_{\mathcal{Z}})\) is a compact metric space, and in the setting specified in the statement of theorem 2 where **A3** is strengthened to independence, \(s=p\) and \(\mathbf{S}(v)=\sigma\mathbf{I}_{p}\) for all \(v\in\mathcal{Z}\), the variables \(\mathbf{Y}_{1}\ldots,\mathbf{Y}_{n}\); \(\{\mathbf{X}(v),v\in\mathcal{Z}\}\); \(\mathbf{E}_{1},\ldots,\mathbf{E}_{n}\) exactly follow the Latent Metric Model of Whiteley et al. [49].

Moreover, according to the description in section 2.1, the variables \(Z_{1},\ldots,Z_{n}\) are i.i.d. according to a probability distribution supported on \(\mathcal{Z}\). As in [49], by Mercer's theorem there exists a feature map \(\phi:\mathcal{Z}\to\mathbb{R}^{r}\) associated with this probability distribution, such that \(\left\langle\phi(u),\phi(v)\right\rangle=\alpha(u,v)\), for \(u,v\in\mathcal{Z}\). Here \(r\), as in **A6**, is the rank of the matrix with elements \(\alpha(u,v)\), which is at most \(\mathcal{Z}\).

Theorem 1 of [49] in this context implies there exists a random orthogonal matrix \(\mathbf{Q}\in\mathbb{R}^{r\times r}\) such that

\[\max_{i\in[n]}\left\|p^{-1/2}\mathbf{Q}\zeta_{i}-\phi(Z_{i})\right\|\in O_{ \mathbb{P}}\left(\sqrt{\frac{nr}{p}}+\sqrt{\frac{r}{n}}\right).\] (19)

Consider the bound:

\[\left|\hat{\alpha}_{\text{pca}}(i,j)-\alpha(Z_{i},Z_{j})\right| =\left|\frac{1}{p}\langle\zeta_{i},\zeta_{j}\rangle-\langle\phi( Z_{i}),\phi(Z_{j})\rangle\right|\] \[\leq\left|\left\langle p^{-1/2}\mathbf{Q}\zeta_{i}-\phi(Z_{i}),p^ {-1/2}\mathbf{Q}\zeta_{j}\right\rangle\right|\] \[\quad+\left|\left\langle\phi(Z_{i}),p^{-1/2}\mathbf{Q}\zeta_{j}- \phi(Z_{j})\right\rangle\right|\] \[\leq\left\|p^{-1/2}\mathbf{Q}\zeta_{i}-\phi(Z_{i})\right\|\left( \left\|p^{-1/2}\mathbf{Q}\zeta_{j}-\phi(Z_{j})\right\|+\left\|\phi(Z_{j}) \right\|\right)\] \[\quad+\left\|\phi(Z_{i})\right\|\left\|p^{-1/2}\mathbf{Q}\zeta_{i }-\phi(Z_{i})\right\|,\]

where orthogonality of \(\mathbf{Q}\) has been used, and the final inequality uses Cauchy-Schwarz and the triangle inequality for the \(\|\cdot\|\) norm. Combining the above estimate with (19), the bound:

\[\max_{i\in[n]}\|\phi(Z_{i})\|^{2}\leq\max_{v\in\mathcal{Z}}\|\phi (v)\|^{2}=\max_{v\in\mathcal{Z}}\alpha(v,v)\\ \leq\sup_{j\geq 1}\max_{v\in\mathcal{Z}}\mathbb{E}[|X_{j}(v)|^{2}] \leq\sup_{j\geq 1}\max_{v\in\mathcal{Z}}\mathbb{E}[|X_{j}(v)|^{2q}]^{1/q}\] (20)

and **A4** completes the proof of the second claim of the theorem.

**Proposition 2**.: _Assume the model in section 2.1 satisfies assumptions **A3**-**A5**, and let \(\varphi\) and \(q\) be as in **A3** and **A4**. Then there exists a constant \(C(q,\varphi)\) depending only on \(q\) and \(\varphi\) such that for any \(\delta>0\),_

\[\mathbb{P}\left(\max_{1\leq i<j\leq n}\left|p^{-1}\left\langle \mathbf{Y}_{i},\mathbf{Y}_{j}\right\rangle-\alpha(Z_{i},Z_{j})\right|\geq\delta \right|Z_{1},\ldots,Z_{n}\right)\\ \leq\frac{1}{\delta^{q}}\frac{1}{p^{q/2}}\frac{n(n-1)}{2}C(q, \varphi)M(q,\mathbf{X},\mathbf{E},\mathbf{S})\] (21)_where_

\[M(q,\mathbf{X},\mathbf{E},\mathbf{S}) \coloneqq\sup_{k\geq 1}\max_{v\in\mathcal{Z}}\mathbb{E}\left[|X_{k}(v)|^ {2q}\right]\] \[\quad+\mathbb{E}\left[|\mathbf{E}_{11}|^{q}\right]\left(\sup_{p \geq 1}\max_{v\in\mathcal{Z}}\left\|\mathbf{S}(v)\right\|_{\mathrm{op}}^{q} \right)\sup_{k\geq 1}\max_{v\in\mathcal{Z}}\mathbb{E}\left[|X_{k}(v)|^{q}|\right]\] \[\quad+\mathbb{E}\left[|\mathbf{E}_{11}|^{2q}\right]\sup_{p\geq 1} \max_{v\in\mathcal{Z}}\left\|\mathbf{S}(v)\right\|_{\mathrm{op}}^{2q}.\]

Proof.: Fix any \(i,j\) such that \(1\leq i<j\leq n\). Consider the decomposition:

\[p^{-1}\left\langle\mathbf{Y}_{i},\mathbf{Y}_{j}\right\rangle-\alpha(Z_{i},Z_{j })=\sum_{k=1}^{4}\Delta_{k}\]

where

\[\Delta_{1} \coloneqq p^{-1}\left\langle\mathbf{X}(Z_{i}),\mathbf{X}(Z_{j}) \right\rangle-\alpha(Z_{i},Z_{j})\] \[\Delta_{2} \coloneqq p^{-1}\left\langle\mathbf{X}(Z_{i}),\mathbf{S}(Z_{j}) \mathbf{E}_{j}\right\rangle\] \[\Delta_{3} \coloneqq p^{-1}\left\langle\mathbf{X}(Z_{j}),\mathbf{S}(Z_{i}) \mathbf{E}_{i}\right\rangle\] \[\Delta_{4} \coloneqq p^{-1}\left\langle\mathbf{S}(Z_{i})\mathbf{E}_{i}, \mathbf{S}(Z_{j})\mathbf{E}_{j}\right\rangle\]

The proof proceeds by bounding \(\mathbb{E}[|\Delta_{k}|^{q}|Z_{1},\ldots,Z_{n}]\) for \(k=1,\ldots,4\). Writing \(\Delta_{1}\) as

\[\Delta_{1}=\frac{1}{p}\sum_{k=1}^{p}\Delta_{1,k},\qquad\Delta_{1,k}\coloneqq X _{k}(Z_{i})X_{k}(Z_{j})-\mathbb{E}\left[\left.X_{k}(Z_{i})X_{k}(Z_{j})\right|Z _{1},\ldots,Z_{n}\right].\]

we see that \(\Delta_{1}\) is a sum \(p\) random variables each of which is conditionally mean zero given \(Z_{1},\ldots,Z_{n}\). Noting that the two collections of random variables \(\{Z_{1},\ldots,Z_{n}\}\) and \(\{\mathbf{X}(v);v\in\mathcal{V}\}\) are independent (as per the description of the model in section 2.1), under assumption **A3** we may apply a moment inequality for \(\varphi\)-mixing random variables [51][Lemma 1.7] to show that there exists a constant \(C_{1}(q,\varphi)\) depending only on \(q,\varphi\) such that

\[\mathbb{E}\left[\left.|\Delta_{1}|^{q}\right|Z_{1},\ldots,Z_{n}\right]\] \[\leq C_{1}(q,\varphi)\left\{\frac{1}{p^{q}}\sum_{k=1}^{p}\mathbb{ E}\left[\left.|\Delta_{1,k}|^{q}\right|Z_{1},\ldots,Z_{n}\right]+\left( \frac{1}{p^{2}}\sum_{k=1}^{p}\mathbb{E}\left[\left.|\Delta_{1,k}|^{2}\right|Z _{1},\ldots,Z_{n}\right]\right)^{q/2}\right\}\] \[\leq C_{1}(q,\varphi)\left\{\frac{1}{p^{q}}\sum_{k=1}^{p}\mathbb{ E}\left[\left.|\Delta_{1,k}|^{q}\right|Z_{1},\ldots,Z_{n}\right]+\frac{1}{p^{q/2}} \frac{1}{p}\sum_{k=1}^{p}\mathbb{E}\left[\left.|\Delta_{1,k}|^{q}\right|Z_{1}, \ldots,Z_{n}\right]\right\}\] \[\leq 2C_{1}(q,\varphi)\frac{1}{p^{q/2}}\sup_{k\geq 1}\mathbb{ E}\left[\left.|\Delta_{1,k}|^{q}\right|Z_{1},\ldots,Z_{n}\right]\] \[\leq 2^{q+1}C_{1}(q,\varphi)\frac{1}{p^{q/2}}\sup_{k\geq 1}\max_{v \in\mathcal{Z}}\mathbb{E}\left[\left.|X_{k}(v)|^{2q}\right],\] (22)

where second inequality holds by two applications of Jensen's inequality and \(q\geq 2\), and the final inequality uses the fact that for \(a,b\geq 0\), \((a+b)^{q}\leq 2^{q-1}(a^{q}+b^{q})\), the Cauchy-Schwartz inequality, and the independence of \(\{Z_{1},\ldots,Z_{n}\}\) and \(\{\mathbf{X}(v);v\in\mathcal{V}\}\).

For \(\Delta_{2}\), we have

\[\Delta_{2}\coloneqq\frac{1}{p}\sum_{k=1}^{p}\Delta_{2,k},\qquad\Delta_{2,k} \coloneqq[\mathbf{S}(Z_{j})^{\top}\mathbf{X}(Z_{i})]_{k}\mathbf{E}_{jk},\]

where \([\cdot]_{k}\) denotes the \(k\)th element of a vector. Since the three collections of random variables, \(\{Z_{1},\ldots,Z_{n}\}\), \(\{\mathbf{X}(v);v\in\mathcal{Z}\}\) and \(\{\mathbf{E}_{1},\ldots,\mathbf{E}_{n}\}\) are mutually independent, and the elements of each vector \(\mathbf{E}_{j}\in\mathbb{R}^{p}\) are mean zero and independent, we see that given \(\{Z_{1},\ldots,Z_{n}\}\) and \(\{\mathbf{X}(v);v\in\mathcal{V}\}\), \(\Delta_{2}\) is a simple average of conditionally independent and conditionally mean-zerorandom variables. Applying the Marcinkiewicz-Zygmund inequality we find there exists a constant \(C_{2}(q)\) depending only on \(q\) such that

\[\mathbb{E}\left[\left|\Delta_{2}\right|^{q}\right|Z_{1},\ldots,Z_{n },\mathbf{X}(v);v\in\mathcal{Z}\right]\\ \leq C_{2}(q)\mathbb{E}\left[\left|\frac{1}{p^{2}}\sum_{k=1}^{p} \left|\Delta_{2,k}\right|^{2}\right|^{q/2}\right|Z_{1},\ldots,Z_{n},\mathbf{X} (v);v\in\mathcal{Z}\right].\] (23)

Noting that \(q\geq 2\) and applying Minkowski's inequality to the r.h.s. of (23), then using the independence of \(\{Z_{1},\ldots,Z_{n}\}\), \(\{\mathbf{X}(v);v\in\mathcal{Z}\}\) and \(\{\mathbf{E}_{1},\ldots,\mathbf{E}_{n}\}\) and the i.i.d. nature of the elements of the vector \(\mathbf{E}_{j}\),

\[\mathbb{E}\left[\left|\frac{1}{p^{2}}\sum_{k=1}^{p}\left|\Delta_ {2,k}\right|^{2}\right|^{q/2}\right|Z_{1},\ldots,Z_{n},\mathbf{X}(v);v\in \mathcal{Z}\right]^{2/q}\] \[\leq\frac{1}{p^{2}}\sum_{k=1}^{p}\mathbb{E}\left[\left|\Delta_{2,k}\right|^{q}\right|Z_{1},\ldots,Z_{n},\mathbf{X}(v);v\in\mathcal{Z}\right]^ {2/q}\] \[=\frac{1}{p^{2}}\sum_{k=1}^{p}\mathbb{E}\left[\left|[\mathbf{S}( Z_{j})^{\top}\mathbf{X}(Z_{i})]_{k}\right|^{q}\left|\mathbf{E}_{jk}\right|^{q} \right|Z_{1},\ldots,Z_{n},\mathbf{X}(v);v\in\mathcal{Z}\right]^{2/q}\] \[=\frac{1}{p^{2}}\mathbb{E}\left[\left|\mathbf{E}_{11}\right|^{q} \right|^{2/q}\sum_{k=1}^{p}\left|[\mathbf{S}(Z_{j})^{\top}\mathbf{X}(Z_{i})]_{ k}\right|^{2}\] \[=\frac{1}{p^{2}}\mathbb{E}\left[\left|\mathbf{E}_{11}\right|^{q} \right|^{2/q}\left\|\mathbf{S}(Z_{j})^{\top}\mathbf{X}(Z_{i})\right\|^{2}\] \[\leq\frac{1}{p^{2}}\mathbb{E}\left[\left|\mathbf{E}_{11}\right|^{ q}\right]^{2/q}\max_{v\in\mathcal{Z}}\left\|\mathbf{S}(v)\right\|_{\mathrm{op}}^ {2}\left\|\mathbf{X}(Z_{i})\right\|^{2}.\]

Substituting into (23) and using the tower property of conditional expectation we obtain:

\[\mathbb{E}\left[\left|\Delta_{2}\right|^{q}\right|Z_{1},\ldots,Z_ {n}\right]\] \[\leq\frac{1}{p^{q/2}}\mathbb{E}\left[\left|\mathbf{E}_{11}\right|^ {q}\right]\max_{v\in\mathcal{Z}}\left\|\mathbf{S}(v)\right\|_{\mathrm{op}}^{q} \frac{1}{p^{q/2}}\mathbb{E}\left[\left\|\mathbf{X}(Z_{i})\right\|^{q}\right|Z_ {1},\ldots,Z_{n}\right]\] \[=\frac{1}{p^{q/2}}\mathbb{E}\left[\left|\mathbf{E}_{11}\right|^{ q}\right]\max_{v\in\mathcal{Z}}\left\|\mathbf{S}(v)\right\|_{\mathrm{op}}^{q} \mathbb{E}\left[\left.\left(\frac{1}{p}\sum_{k=1}^{p}\left|X_{k}(Z_{j})\right|^ {q}\right)^{q/2}\right|Z_{1},\ldots,Z_{n}\right]\] \[\leq\frac{1}{p^{q/2}}\mathbb{E}\left[\left|\mathbf{E}_{11}\right|^ {q}\right]\max_{v\in\mathcal{Z}}\left\|\mathbf{S}(v)\right\|_{\mathrm{op}}^{q} \sup_{k\geq 1}\max_{v\in\mathcal{Z}}\mathbb{E}\left[\left|X_{k}(v)\right|^{q}\right]\] (24)

where the second inequality holds by Jensen's inequality (recall \(q\geq 2\)). Since the r.h.s. of (24) does not depend on \(i\) or \(j\), the same bound holds with \(\Delta_{2}\) on the l.h.s. replaced by \(\Delta_{3}\).

Turning to \(\Delta_{4}\), we have

\[\Delta_{4}\coloneqq\frac{1}{p}\left\langle\mathbf{S}(Z_{i})\mathbf{E}_{i}, \mathbf{S}(Z_{j})\mathbf{E}_{j}\right\rangle=\frac{1}{p}\sum_{1\leq k,\ell\leq p }\Delta_{4,k,\ell},\qquad\Delta_{4,k,\ell}\coloneqq\mathbf{E}_{ik}\mathbf{E}_ {j\ell}[\mathbf{S}(Z_{i})^{\top}\mathbf{S}(Z_{j})]_{k\ell}.\]

Noting that \(i\neq j\), and that the elements of \(\mathbf{E}_{i}\) and \(\mathbf{E}_{j}\) are independent, identically distributed, and mean zero, we see that \(\Delta_{4}\) is a sum of \(p^{2}\) random variables which are all conditionally mean zero and conditionally independent given \(Z_{1},\ldots,Z_{n}\). The Marcinkiewicz-Zygmund inequality gives:

\[\mathbb{E}\left[\left|\Delta_{4}\right|^{q}\right|Z_{1},\ldots,Z_{n}\right]\leq C _{2}(q)\mathbb{E}\left[\left|\frac{1}{p^{2}}\sum_{1\leq k,\ell\leq s}^{p} \left|\Delta_{4,k,\ell}\right|^{2}\right|^{q/2}\right|Z_{1},\ldots,Z_{n}\right].\] (25)Applying Minkowski's inequality to the r.h.s. of (25),

\[\mathbb{E}\left[\left|\frac{1}{p^{2}}\sum_{1\leq k,\ell\leq p}|\Delta _{4,k,\ell}|^{2}\right|^{q/2}\right|Z_{1},\ldots,Z_{n}\right]^{2/q}\] \[\leq\frac{1}{p^{2}}\sum_{1\leq k,\ell\leq p}\mathbb{E}\left[\left| \Delta_{4,k,\ell}\right|^{q}\left|Z_{1},\ldots,Z_{n}\right|^{2/q}\right.\] \[=\frac{1}{p^{2}}\sum_{1\leq k,\ell\leq p}\mathbb{E}\left[\left| \mathbf{E}_{ik}\right|^{q}\left|\mathbf{E}_{j\ell}\right|^{q}\left|[\mathbf{S} (Z_{i})^{\top}\mathbf{S}(Z_{j})]_{k\ell}\right|^{q}\right|Z_{1},\ldots,Z_{n} \right]^{2/q}\] \[=\frac{1}{p^{2}}\mathbb{E}\left[\left|\mathbf{E}_{11}\right|^{q} \right]^{4/q}\sum_{1\leq k,\ell\leq p}\left|[\mathbf{S}(Z_{i})^{\top}\mathbf{ S}(Z_{j})]_{k\ell}\right|^{2}\] \[\leq\frac{1}{p^{2}}\mathbb{E}\left[\left|\mathbf{E}_{11}\right|^ {2q}\right]^{2/q}\max_{u,v\in\mathcal{Z}}\left\|\mathbf{S}(u)^{\top}\mathbf{S }(v)\right\|_{\mathrm{F}}^{2},\]

where the final inequality holds by Jensen's inequality. Substituting back into (25) and using \(\|\mathbf{S}(u)^{\top}\mathbf{S}(v)\|_{\mathrm{F}}\leq p^{1/2}\|\mathbf{S}(u) ^{\top}\mathbf{S}(v)\|_{\mathrm{op}}\leq p^{1/2}\|\mathbf{S}(u)\|_{\mathrm{op }}\|\mathbf{S}(v)\|_{\mathrm{op}}\), we obtain:

\[\mathbb{E}\left[\left|\Delta_{4}\right|^{q}\left|Z_{1},\ldots,Z_{n}\right]\leq C _{2}(q)\frac{1}{p^{q/2}}\mathbb{E}\left[\left|\mathbf{E}_{11}\right|^{2q} \right]\max_{u\in\mathcal{Z}}\left\|\mathbf{S}(u)\right\|_{\mathrm{op}}^{2q}.\] (26)

Combining (22), (24) and (26) using the fact that for \(a,b\geq 0\), \((a+b)^{q}\leq 2^{q-1}(a^{q}+b^{q})\), we find that there exists a constant \(C(q,\varphi)\) depending only on \(q\) and \(\varphi\) such that

\[\mathbb{E}\left[\left|p^{-1}\left\langle\mathbf{Y}_{i},\mathbf{Y }_{j}\right\rangle-\alpha(Z_{i},Z_{j})\right|^{q}\right|Z_{1},\ldots,Z_{n}\right]\] \[\leq C(q,\varphi)\frac{1}{p^{q/2}}M(q,\mathbf{X},\mathbf{E}, \mathbf{S}),\]

where \(M(q,\mathbf{X},\mathbf{E},\mathbf{S})\) is defined in the statement of the proposition and is finite by assumptions **A4** and **A5**. By Markov's inequality, for any \(\delta\geq 0\),

\[\mathbb{P}\left(\left|p^{-1}\left\langle\mathbf{Y}_{i},\mathbf{Y }_{j}\right\rangle-\alpha(Z_{i},Z_{j})\right|\geq\delta\big{|}\,Z_{1},\ldots,Z _{n}\right)\leq\frac{1}{\delta^{q}}C(q,\varphi)\frac{1}{p^{q/2}}M(q,\mathbf{X}, \mathbf{E},\mathbf{S})\] (27)

and the proof is completed by a union bound:

\[\mathbb{P}\left(\max_{1\leq i<j\leq n}\left|p^{-1}\left\langle \mathbf{Y}_{i},\mathbf{Y}_{j}\right\rangle-\alpha(Z_{i},Z_{j})\right|<\delta \bigg{|}\,Z_{1},\ldots,Z_{n}\right)\] \[=1-\mathbb{P}\left(\bigcup_{1\leq i<j\leq n}\left|p^{-1}\left\langle \mathbf{Y}_{i},\mathbf{Y}_{j}\right\rangle-\alpha(Z_{i},Z_{j})\right|\geq\delta \Bigg{|}\,Z_{1},\ldots,Z_{n}\right)\] \[\geq 1-\frac{n(n-1)}{2}\frac{1}{\delta^{q}}C(q,\varphi)\frac{1}{p^ {q/2}}M(q,\mathbf{X},\mathbf{E},\mathbf{S}).\]

### Interpretation of merge heights and exact tree recovery

Here we expand on the discussion in section 3.3 and provide further interpretation of merge heights and algorithm 1. In particular our aim is to clarify in what circumstances algorithm 1 will asymptotically correctly recover underlying tree structure. For ease of exposition throughout section C.3 we assume that \(\mathcal{Z}\) are the leaf vertices of \(\mathcal{T}\).

As a preliminary we note the following corollary to theorem 1: assuming \(b>0\), if one takes as input to algorithm 1 the true merge heights, i.e. (up to bijective relabelling of leaf vertices)\(\hat{\alpha}(\cdot,\cdot)\coloneqq m(\cdot,\cdot)=\alpha(\cdot,\cdot)\), where \(n=|\mathcal{Z}|\), then theorem 1 implies that algorithm 1 outputs a dendrogram \(\mathcal{D}\) whose merge heights \(\hat{m}(\cdot,\cdot)\) are equal to \(m(\cdot,\cdot)\) (up to bijective relabeling over vertices). This clarifies that with knowledge of \(m(\cdot,\cdot)\), algorithm 1) constructs a dendrogram which has \(m(\cdot,\cdot)\) as its merge heights.

We now ask for more: if once again \(m(\cdot,\cdot)\) is taken as input to algorithm 1, under what conditions is the output tree \(\hat{\mathcal{T}}\) equal to \(\mathcal{T}\) (upto bijective relabelling of vertices)? We claim this holds when \(\mathcal{T}\) is a binary tree and that all its non-leaf nodes have different heights. We provide a sketch proof of this claim, since a complete proof involves many tedious and notationally cumbersome details.

To remove the need for repeated considerations of relabelling, suppose \(\mathcal{T}=(\mathcal{V},\mathcal{E})\) is given, then w.l.o.g. relabel the leaf vertices of \(\mathcal{T}\) as \(\{1\},\ldots,\{|\mathcal{Z}|\}\) and relabel each non-leaf vertex to be the union of its children. Thus each vertex is some subset of \([|\mathcal{Z}|]\).

Now assume that \(\mathcal{T}\) is a binary tree and that all its non-leaf nodes have different heights. Note that \(|\mathcal{V}|=2|\mathcal{Z}|-1\), i.e., there are \(|\mathcal{Z}|-1\) non-leaf vertices. The tree \(\mathcal{T}\) is uniquely characterized by a sequence of partitions \(\tilde{P}_{0},\ldots,\tilde{P}_{|\mathcal{Z}|-1}\) where \(\tilde{P}_{0}\coloneqq\{\{1\},\ldots,\{|\mathcal{Z}|\}\}\), and for \(m=1,\ldots,|\mathcal{Z}|-1\), \(\tilde{P}_{m}\) is constructed from \(\tilde{P}_{m-1}\) by merging the two elements of \(\tilde{P}_{m-1}\) whose most recent common ancestor is the \(m\)th highest non-leaf vertex (which is uniquely defined since we are assuming no two non-leaf vertices have equal heights).

To see that in this situation algorithm 1, with \(\hat{\alpha}(\cdot,\cdot)\coloneqq m(\cdot,\cdot)\) and \(n=|\mathcal{Z}|\) as input, performs exact recovery of the tree, i.e., \(\hat{\mathcal{T}}=\mathcal{T}\), it suffices to notice that the sequence of partitions \(P_{0},\ldots,P_{|\mathcal{Z}|-1}\) constructed by algorithm 1 uniquely characterizes \(\hat{\mathcal{T}}\), and moreover \((\tilde{P}_{0},\ldots,\tilde{P}_{|\mathcal{Z}|-1})=(P_{0},\ldots,P_{|\mathcal{ Z}|-1})\). The details of this last equality involve simple but tedious substitutions of \(m(\cdot,\cdot)\) in place of \(\hat{\alpha}(\cdot,\cdot)\) in algorithm 1, so are omitted.

## Appendix D Further details of numerical experiments and data preparation

All real datasets used are publicly available under the CC0: Public domain license. Further, all experiments were run locally on a laptop with an integrated GPU (Intel UHD Graphics 620).

### Simulated data

For each \(v\in\mathcal{V}\), \(X_{1}(v),\ldots,X_{p}(v)\) are independent and identically distributed Gaussian random variables with:

\[X_{j}(1) \sim N(X_{j}(6),5),\] \[X_{j}(2) \sim N(X_{j}(6),2),\] \[X_{j}(3) \sim N(X_{j}(6),2),\] \[X_{j}(4) \sim N(X_{j}(7),0.5),\] \[X_{j}(5) \sim N(X_{j}(7),7),\] \[X_{j}(6) \sim N(X_{j}(8),2),\] \[X_{j}(7) \sim N(X_{j}(8),1),\] \[X_{j}(8) \sim N(0,1),\]

for \(j=1,\ldots,p\).

### 20 Newsgroups

The dataset originates from [29], however, the version used is the one available in the Python package'scikit-learn' [38]. Each document is pre-processed in the following way: generic stopwords and e-mail addresses are removed, and words are lemmatised. The processed documents are then converted into a matrix of TF-IDF features. Labels can be found on the 20 Newsgroups website http://qwone.com/~jason/20Newsgroups/, but are mainly intuitive from the title of labels, with full stops separating levels of hierarchy. When using PCA a dimension of \(r=34\) was selected by the method described in appendix A.

The following numerical results complement those in the main part of the paper.

### Zebrafish gene counts

These data were collected over a 24-hour period (timestamps available in the data), however, the temporal aspect of the data was ignored when selecting a sub-sample. To process the data, we followed the steps in [47] which are the standard steps in the popular SCANPY [50] - a Python package for analysing single-cell gene-expression data to process the data. This involves filtering out genes that are present in less than 3 cells or are highly variable, taking the logarithm, scaling and regressing out the effects of the total counts per cell. When using PCA a dimension of \(r=29\) was selected by the method described in appendix A.

### Amazon reviews

The pre-processing of this dataset is similar to that of the Newsgroup data except e-mail addresses are no longer removed. Some data points had a label of 'unknown' in the third level of the hierarchy, these were removed from the dataset. In addition, reviews that are two words or less are not included. When using PCA a dimension of \(r=22\) was selected by the method described in appendix A.

### S&P 500 stock data

This dataset was found through the paper [8] with the authors code used to process the data. The labels follow the Global Industry Classification Standard and can be found here: [3]. The return on day \(i\) of each stock is calculated by

\[\text{return}_{i}=\frac{p_{i}^{cl}-p_{i}^{op}}{p_{i}^{op}},\]

where \(p_{i}^{op}\) and \(p_{i}^{cl}\) is the respective opening and closing price on day \(i\). When using PCA a dimension of \(r=10\) was used as selected by the method described in appendix A.

\begin{table}
\begin{tabular}{c c|c c c c c} \hline \hline Data & Input & \(p=500\) & \(p=1000\) & \(p=5000\) & \(p=7500\) & \(p=12818\) \\ \hline \multirow{2}{*}{Newsgroups} & \(\mathbf{Y}_{1:n}\) & 0.026 (0.55) & 0.016 (1.0) & 0.13 (2.2) & 0.17 (2.5) & 0.26 (2.9) \\  & \(\zeta_{1:n}\) & 0.017 (0.72) & 0.047 (1.2) & 0.12 (1.9) & 0.15 (2.5) & 0.24 (2.6) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Kendall \(\tau_{b}\) ranking performance measure, for Algorithm 1 and the 20 Newsgroups data set. The mean Kendall \(\tau_{b}\) correlation coefficient is reported alongside the standard error (numerical value shown is the standard error\(\times 10^{3}\)). This numerical results are plotted in figure 4 below.

Figure 4: Performance of Algorithm 1 for the 20 Newsgroups data set as a function of number of TF-IDF features, \(p\), with \(n\) fixed. See table 1 for numerical values and standard errors.

### Additional method comparison

Table 3 reports additional method comparison results, complementing those in table 1 which concerned only the average linkage function (noting UPGMA is equivalent to using the average linkage function with Euclidean distances). In table 3 we also compare to Euclidean and cosine distances paired with complete and single linkage functions. To aid comparison, the first column (average linkage with the dot product) is the same as in table 1. In general, using complete or single linkage performs worse for both Euclidean and cosine distances. The only notable exception being a slight improvement on the simulated dataset.

Appendix E Understanding agglomerative clustering with Euclidean or cosine distances in our framework

### Quantifying dissimilarity using Euclidean distance

The first step of many standard variants of agglomerative clustering such as UPGMA and Ward's method is to find and merge the pair of data vectors which are closest to each other in Euclidean distance. From the elementary identity:

\[\|\mathbf{Y}_{i}-\mathbf{Y}_{j}\|^{2}=\|\mathbf{Y}_{i}\|^{2}+\|\mathbf{Y}_{j} \|^{2}-2\langle\mathbf{Y}_{i},\mathbf{Y}_{j}\rangle,\]

we see that, in general, this is not equivalent to finding the pair with largest dot product, because of the presence of the terms \(\|\mathbf{Y}_{i}\|^{2}\) and \(\|\mathbf{Y}_{j}\|^{2}\). For some further insight in to how this relates to our

\begin{table}
\begin{tabular}{c c|c c} \hline \hline Data & Input & UPGMA with dot product *dissimilarity* & UPGMA with Manhattan distance \\ \hline \multirow{2}{*}{Newsgroups} & \(\mathbf{Y}_{1:n}\) & -0.0053 (0.24) & -0.0099 (1.3) \\  & \(\zeta_{1:n}\) & 0.0029 (0.33) & 0.052 (1.6) \\ \hline \multirow{2}{*}{Zebrafish} & \(\mathbf{Y}_{1:n}\) & 0.0012 (0.13) & 0.16 (2.4) \\  & \(\zeta_{1:n}\) & 0.00046 (0.12) & 0.050 (2.8) \\ \hline \multirow{2}{*}{Reviews} & \(\mathbf{Y}_{1:n}\) & -0.0005 (0.29) & 0.0018 (0.44) \\  & \(\zeta_{1:n}\) & -0.0015 (0.41) & 0.061 (1.3) \\ \hline \multirow{2}{*}{S\&P 500} & \(\mathbf{Y}_{1:n}\) & 0.0026 (7.7) & 0.37 (9.4) \\  & \(\zeta_{1:n}\) & 0.0028 (7.5) & 0.39 (11) \\ \hline \multirow{2}{*}{Simulated} & \(\mathbf{Y}_{1:n}\) & -0.0026 (1.6) & 0.55 (8.7) \\  & \(\zeta_{1:n}\) & -0.0023 (1.8) & 0.84 (2) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Kendall \(\tau_{b}\) ranking performance measure. The mean Kendall \(\tau_{b}\) correlation coefficient is reported alongside the standard error (numerical value shown is the standard error\(\times 10^{3}\)).

\begin{table}
\begin{tabular}{c c|c c c c c} \hline \hline \multirow{2}{*}{Data} & \multirow{2}{*}{Input} & \multicolumn{2}{c}{Average linkage} & \multicolumn{2}{c}{Complete linkage} & \multicolumn{2}{c}{Single linkage} \\  & & Dot product & Euclidean & Cosine & Euclidean & Cosine \\ \hline \multirow{2}{*}{Newsgroups} & \(\mathbf{Y}_{1:n}\) & 0.26 (2.9) & 0.022 (0.87) & -0.010 (0.88) & -0.0025 (0.62) & -0.0025 (0.62) \\  & \(\zeta_{1:n}\) & 0.24 (2.6) & 0.0041 (1.2) & 0.036 (1.2) & -0.016 (2.0) & 0.067 (1.5) \\ \hline \multirow{2}{*}{Zebrafish} & \(\mathbf{Y}_{1:n}\) & 0.34 (3.4) & 0.15 (2.2) & 0.24 (3.2) & 0.023 (3.0) & 0.032 (2.9) \\  & \(\zeta_{1:n}\) & 0.34 (3.4) & 0.17 (2.0) & 0.30 (3.4) & 0.12 (2.8) & 0.15 (3.2) \\ \hline \multirow{2}{*}{Reviews} & \(\mathbf{Y}_{1:n}\) & 0.15 (2.5) & 0.019 (0.90) & 0.023 (1.0) & 0.0013 (0.81) & 0.0013 (0.81) \\  & \(\zeta_{1:n}\) & 0.14 (2.4) & 0.058 (1.5) & 0.063 (1.8) & 0.015 (1.2) & 0.038 (1.0) \\ \hline \multirow{2}{*}{S\&P 500} & \(\mathbf{Y}_{1:n}\) & 0.34 (10) & 0.33 (10) & 0.33 (10) & 0.17 (10) & 0.17 (10) \\  & \(\zeta_{1:n}\) & 0.36 (9.4) & 0.32 (10) & 0.31 (10) & 0.36 (13) & 0.39 (12) \\ \hline \multirow{2}{*}{Simulated} & \(\mathbf{Y}_{1:n}\) & 0.86 (1) & 0.55 (8.7) & 0.84 (2.0) & 0.55 (8.7) & 0.84 (2.0) \\  & \(\zeta_{1:n}\) & 0.86 (1) & 0.55 (8.7) & 0.84 (2.0) & 0.55 (8.7) & 0.84 (2.0) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Kendall \(\tau_{b}\) ranking performance measure. For the dot product method, i.e., algorithm 1, \(\mathbf{Y}_{1:n}\) as input corresponds to using \(\hat{\alpha}_{\text{data}}\), and \(\zeta_{1:n}\) corresponds to \(\hat{\alpha}_{\text{pea}}\). The mean Kendall \(\tau_{b}\) correlation coefficient is reported alongside the standard error (numerical value shown is the standard error \(\times 10^{3}\)).

modelling and theoretical analysis framework, it is revealing to the consider the idealised case of choosing to merge by maximising merge height \(m(\cdot,\cdot)\) versus minimising \(d(\cdot,\cdot)\) (recall the identities for \(m\) and \(d\) established in lemma 1). Figure 5 shows two simple scenarios in which geometry of the dendrogram has an impact on whether or not maximising merge height \(m(\cdot,\cdot)\) is equivalent to minimising \(d(\cdot,\cdot)\). From this example we see that, in situations where some branch lengths are disproportionately large, minimising \(d(\cdot,\cdot)\) will have different results to maximising \(m(\cdot,\cdot)\).

UPGMA and Ward's method differ in their linkage functions, and so differ in the clusters they create in practice after their respective first algorithmic steps. UPGMA uses average linkage to combine Euclidean distances, and there does not seem to be a mathematically simple connection between this and algorithm 1, except to say that in general it will return different results. Ward's method merges the pair of clusters which results in the minimum increase in within-cluster variance. When clusters contain equal numbers of samples, this increase is equal to the squared Euclidean distance between the clusters' respective centroids.

### Agglomerative clustering with cosine distance is equivalent to an instance of algorithm 1

The cosine 'distance' between \(\mathbf{Y}_{i}\) and \(\mathbf{Y}_{j}\) is:

\[d_{\mathrm{cos}}(i,j)\coloneqq 1-\frac{\langle\mathbf{Y}_{i},\mathbf{Y}_{j} \rangle}{\|\mathbf{Y}_{i}\|\|\mathbf{Y}_{j}\|}.\]

In table 1 we show results for standard agglomerative clustering using \(d_{\mathrm{cos}}\) as a measure of dissimilarity, combined with average linkage. At each iteration, this algorithm works by merging the pair of data-vectors/clusters which are closest with respect to \(d_{\mathrm{cos}}(\cdot,\cdot)\), say \(u\) and \(v\) merged to form \(w\), with dissimilarities between \(w\) and the existing data-vectors/clusters computed according to the average linkage function:

\[d_{\mathrm{cos}}(w,\cdot)\coloneqq\frac{|u|}{|w|}d_{\mathrm{cos}}(u,\cdot)+ \frac{|v|}{|w|}d_{\mathrm{cos}}(v,\cdot).\] (28)

This procedure can be seen to be equivalent to algorithm 1 with input \(\hat{\alpha}(\cdot,\cdot)=1-d_{\mathrm{cos}}(\cdot,\cdot)\). Indeed maximising \(1-d_{\mathrm{cos}}(\cdot,\cdot)\) is clearly equivalent to minimizing \(d_{\mathrm{cos}}(\cdot,\cdot)\), and with \(\hat{\alpha}(\cdot,\cdot)=1-d_{\mathrm{cos}}(\cdot,\cdot)\) the affinity computation at line 6 of algorithm 1 is:

\[\hat{\alpha}(w,\cdot) \coloneqq\frac{|u|}{|w|}\hat{\alpha}(u,\cdot)+\frac{|v|}{|w|} \hat{\alpha}(v,\cdot)\] \[=\frac{|u|}{|w|}\left[1-d_{\mathrm{cos}}(u,\cdot)\right]+\frac{|v |}{|w|}\left[1-d_{\mathrm{cos}}(v,\cdot)\right]\] \[=\frac{|u|+|v|}{|w|}-\frac{|u|}{|w|}d_{\mathrm{cos}}(u,\cdot)- \frac{|u|}{|w|}d_{\mathrm{cos}}(v,\cdot)\] \[=1-\left[\frac{|u|}{|w|}d_{\mathrm{cos}}(u,\cdot)+\frac{|u|}{|w|} d_{\mathrm{cos}}(v,\cdot)\right],\]

where on the r.h.s. of the final equality we recognise (28).

Figure 5: Illustration of how maximising merge height \(m(\cdot,\cdot)\) may or may not be equivalent to minimising distance \(d(\cdot,\cdot)\), depending on the geometry of the dendrogram. (a) Equivalence holds (b) Equivalence does not hold.

### Using cosine similarity as an affinity measure removes multiplicative noise

Building from the algorithmic equivalence identified in section E.2, we now address the theoretical performance of agglomerative clustering with cosine distance in our modelling framework.

Intuitively, cosine distance is used in situations where the magnitudes of data vectors are thought not to convey useful information about dissimilarity. To formalise this idea, we consider a variation of our statistical model from section 2.1 in which:

* \(\mathcal{Z}\) are the leaf vertices of \(\mathcal{T}\), \(|\mathcal{Z}|=n\), and we take these vertices to be labelled \(\mathcal{Z}=\{1,\ldots,n\}\)
* \(\mathbf{X}\) is as in section 2.1, properties **A1** and **A2** hold, and it is assumed that for all \(v\in\mathcal{Z}\), \(p^{-1}\mathbb{E}[\|\mathbf{X}(v)\|^{2}]=1\).
* the additive model (1) is replaced by a multiplicative noise model: \[\mathbf{Y}_{i}=\gamma_{i}\mathbf{X}(i),\] (29) where \(\gamma_{i}>0\) are all strictly positive random scalars, independent of other variables, but otherwise arbitrary.

The interpretation of this model is that the expected square magnitude of the data vector \(\mathbf{Y}_{i}\) is entirely determined by \(\gamma_{i}\), indeed we have

\[\frac{1}{p}\mathbb{E}[\|\mathbf{Y}_{i}\|^{2}|\gamma_{i}]=\gamma_{i}^{2}\frac {1}{p}\mathbb{E}[\|\mathbf{X}(i)\|^{2}]=\gamma_{i}^{2}h(i)=\gamma_{i}^{2},\]

where \(h\) is as in section 2.1. We note that in this multiplicative model, the random vectors \(\mathbf{E}_{i}\) and matrices \(\mathbf{S}(v)\) from section 2.1 play no role, and one can view the random variables \(Z_{1},\ldots,Z_{n}\) as being replaced by constants \(Z_{i}=i\), rather than being i.i.d.

Now define:

\[\hat{\alpha}_{\cos}(i,j)\coloneqq\frac{\langle\mathbf{Y}_{i},\mathbf{Y}_{j} \rangle}{\|\mathbf{Y}_{i}\|\|\mathbf{Y}_{j}\|}=1-d_{\cos}(i,j).\]

**Theorem 3**.: _Assume that the model specified in section E.3 satisfies **A3** and for some \(q\geq 2\), \(\sup_{j\geq 1}\max_{v\in\mathcal{Z}}\mathbb{E}[|X_{j}(v)|^{2q}]<\infty\). Then_

\[\max_{i,j\in[n],i\neq j}|\alpha(i,j)-\hat{\alpha}_{\cos}(i,j)|\in O_{\mathbb{ P}}\left(\frac{n^{2/q}}{\sqrt{p}}\right).\]

Proof.: The main ideas of the proof are that the multiplicative factors \(\gamma_{i}\),\(\gamma_{j}\) in the numerator \(\hat{\alpha}_{\cos}(i,j)\) cancel out with those in the denominator, and combined with the condition \(p^{-1}\mathbb{E}[\|\mathbf{X}(v)\|^{2}]=1\) we may then establish that \(\alpha(i,j)\) and \(\hat{\alpha}_{\cos}(i,j)\) are probabilistically close using similar arguments to those in the proof of proposition 2.

Consider the decomposition:

\[\alpha(i,j)-\hat{\alpha}_{\cos}(i,j) =\alpha(i,j)-\frac{p^{-1}\langle\mathbf{X}(i),\mathbf{X}(j)\rangle }{p^{-1/2}\|\mathbf{X}(i)\|p^{-1/2}\|\mathbf{X}(j)\|}\] \[=\alpha(i,j)-\frac{1}{p}\langle\mathbf{X}(i),\mathbf{X}(j)\rangle\] (30) \[+\frac{\frac{1}{p}\langle\mathbf{X}(i),\mathbf{X}(j)\rangle}{p^{ -1/2}\|\mathbf{X}(i)\|p^{-1/2}\|\mathbf{X}(j)\|}\left[p^{-1/2}\|\mathbf{X}(i) \|p^{-1/2}\|\mathbf{X}(j)\|-1\right].\] (31)

Applying the Cauchy-Schwartz inequality, and adding and subtracting \(p^{-1/2}\|\mathbf{X}(j)\|\) and \(1\) in the final term of this decomposition leads to:

\[\max_{i,j\in[n],i\neq j}|\alpha(i,j)-\hat{\alpha}_{\cos}(i,j)|\] \[\qquad\leq\max_{i,j\in[n],i\neq j}\left|\alpha(i,j)-\frac{1}{p} \langle\mathbf{X}(i),\mathbf{X}(j)\rangle\right|\] (32) \[\qquad+\max_{i\in[n]}\left|p^{-1/2}\|\mathbf{X}(i)\|-1\right|\] (33) \[\qquad+\left(1+\max_{i\in[n]}\left|p^{-1/2}\|\mathbf{X}(i)\|-1 \right|\right)\max_{j\in[n]}\left|p^{-1/2}\|\mathbf{X}(j)\|-1\right|.\] (34)

[MISSING_PAGE_EMPTY:28]