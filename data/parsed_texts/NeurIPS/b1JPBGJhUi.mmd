# Stable Nonconvex-Nonconcave Training

via Linear Interpolation

 Thomas Pethick

EPFL (LIONS)

thomas.pethick@epfl.ch &Wanyun Xie

EPFL (LIONS)

wanyun.xie@epfl.ch &Volkan Cevher

EPFL (LIONS)

volkan.cevher@epfl.ch

###### Abstract

This paper presents a theoretical analysis of linear interpolation as a principled method for stabilizing (large-scale) neural network training. We argue that instabilities in the optimization process are often caused by the nonmonotonicity of the loss landscape and show how linear interpolation can help by leveraging the theory of nonexpansive operators. We construct a new optimization scheme called relaxed approximate proximal point (RAPP), which is the first 1-SCLI method to achieve last iterate convergence rates for \(\rho\)-comonotone problems while only requiring \(\rho>-\frac{1}{2L}\). The construction extends to constrained and regularized settings. By replacing the inner optimizer in RAPP we rediscover the family of Lookahead algorithms for which we establish convergence in cohypomonotone problems even when the base optimizer is taken to be gradient descent ascent. The range of cohypomonotone problems in which Lookahead converges is further expanded by exploiting that Lookahead inherits the properties of the base optimizer. We corroborate the results with experiments on generative adversarial networks which demonstrates the benefits of the linear interpolation present in both RAPP and Lookahead.

## 1 Introduction

Stability is a major concern when training large scale models. In particular, generative adversarial networks (GANs) are known to be notoriously difficult to train. To stabilize training, the Lookahead algorithm of Zhang et al. (2019) was recently proposed for GANs Chavdarova et al. (2020) which linearly interpolates with a slow moving iterate. The mechanism has enjoyed superior empirical performance in both minimization and minimax problems, but it largely remains a heuristic with little theoretical motivation.

One major obstacle for providing a theoretical treatment, is in capturing the (fuzzy) notion of stability. Loosely speaking, a training dynamics is referred to as _unstable_ in practice when the iterates either cycle indefinitely or (eventually) diverge--as has been observed for the Adam optimizer (see e.g. Gidel et al. (2018, Fig. 12) and Chavdarova et al. (2020, Fig. 6) respectively). Conversely, a _stable_ dynamics has some bias towards stationary points. The notion of stability so far (e.g. in Chavdarova et al. (2020, Thm. 2-3)) is based on the spectral radius and thus inherently _local_.

In this work, we are interested in establishing _global_ convergence properties, in which case some structural assumptions are needed. One (nonmonotone) structure that lends itself well to the study of stability is that of cohypomonotonicity studied in Combettes & Pennanen (2004); Diakonikolas et al. (2021), since even the extragradient method has been shown to cycle and diverge in this problem class (see Pethick et al. (2022, Fig. 1) and Pethick et al. (2023, Fig. 2) respectively). We provide a geometric intuition behind these difficulties in Figure 1. Biasing the optimization schemes towards stationary points becomes a central concern and we demonstrate in Figure 2 that Lookahead can indeed converge for such nonmonotone problems.

A principled approach to cohypomonotone problems is the extragradient+ algorithm (EG+) proposed by Diakonikolas et al. (2021). However, the only known rates are on the best iterate, which can be problematic to pick in practice. It is unclear whether _last_ iterate rates for EG+ are possible even in the monotone case (see discussion prior to Thm. 3.3 in Gorbunov et al. (2022)). For this reason, the community has instead resorted to showing last iterate of extragradient (EG) method of Korpelevich (1977), despite originally being developed for the monotone case. Maybe not surprisingly, EG only enjoys a last iterate guarantee under mild form of cohypomonotonicity and have so far only been studied in the unconstrained case (Luo and Tran-Dinh; Gorbunov et al., 2022). Recently, last iterate rate were established for the same (tight) range of cohypomonotone problems for which EG+ has best iterate guarantees. However, the analyzed scheme is _implicit_ and the complexity blows up with increasing cubypomonotonicity (Gorbunov et al., 2022). This leaves the questions: _Can an explicit scheme enjoy last iterate rates for the same range of cohypomonotone problems? Can the rate be agnostic to the degree of cohypomonotonicity?_ We answer both in the affirmative.

This work focuses on 1-SCLI schemes (Arjevani et al., 2015; Golowich et al., 2020), whose update rule only depends on the previous iterate in a time-invariant fashion. Another approach to establishing last iterate is Halpern-type methods with an explicit scheme developed in Lee and Kim (2021) for cohypomonotone problems and later extended to the constrained case in Cai et al. (2022) (c.f. Appendix A).

As will become clear, a principled mechanism behind convergence in this nonmonotone class is the linear interpolation also used in Lookahead. This iterative interpolation is more broadly referred to as the Krasnosel'skii-Mann (KM) iteration in the theory of nonexpansive operators. We show that the extragradient+ algorithm (EG+) of Diakonikolas et al. (2021), our proposed relaxed approximate proximal point method (RAPP), and Lookahead based algorithms are all instances of the (inexact) KM iteration and provide simple proofs of these schemes in the cohypomonotone case.

More concretely we make the following contributions:

1. We prove global convergence rates for the last iterate of our proposed algorithm RAPP which additionally handles constrained and regularized settings. This makes RAPP the first 1-SCLI scheme to have non-asymptotic guarantees for \(\rho\)-comonotone problems while only requiring \(\rho>-\nicefrac{{1}}{{2L}}\). As a byproduct we obtain a last iterate convergence rate for an implicit scheme that is _independent_ of the degree of cohypomonotonicity. The last iterate rates are established by showing monotonic decrease of the operator norm-something which is not possible for EG+. This contrast is maybe surprising, since RAPP can be viewed as an extension of EG+, which simply takes multiple extrapolation steps.
2. By replacing the inner optimization routine in RAPP with gradient descent ascent (GDA) and extragradient (EG) we rediscover the Lookahead algorithms considered in Chavdarova et al. (2020). We obtain guarantees for the Lookahead variants by deriving nonexpansive properties of the base optimizers. By casting Lookahead as a KM iteration we find that the optimal interpolation constant is \(\lambda=0.5\). This choice corresponds to the default value used in practice for both minimization and minimax--thus providing theoretical motivation for the parameter value.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline  & Method & Setting & \(\rho\) & Handles constraints & \(\rho\)-independent rates & Reference \\ \hline \multirow{4}{*}{\(\rho\)-comonotone} & PP & Comonotone & \((-\nicefrac{{1}}{{2L}},\infty)\) & ✓ & ✗ & (Gorbunov et al., 2022, Thm. 3.1) \\  & Relaxed PP & Comonotone & \((-\nicefrac{{1}}{{2L}},\infty)\) & ✓ & ✓ & Theorem 6.2 \\ \cline{2-7}  & EG & Comonotone \& Lips. & \((-\nicefrac{{1}}{{8L}},\infty)\) & ✗ & ✗ & (Gorbunov et al., 2022, Thm. 4.1) \\  & EG+ & Comonotone \& Lips. & & & Unknown rates & \\ \cline{2-7}  & RAPP & Comonotone \& Lips. & \((-\nicefrac{{1}}{{2L}},\infty)\) & ✓ & ✓ & Corollary 6.4 \\ \hline \multirow{4}{*}{\(\rho\)-comonotone} & \multirow{2}{*}{LA-GDA} & Local & - & ✗ & - & (Chavdarova et al., 2020, Thm. 2) \\  & & Bilinear & - & ✗ & - & (Ha and Kim, 2022, Cor. 7) \\ \cline{2-7}  & & Comonotone \& Lips. & \((-\nicefrac{{1}}{{3}}\nicefrac{{3L}}{{3L}},\infty)\) & ✗ & - & Theorem 7.1 \\ \cline{2-7}  & \multirow{2}{*}{LA-EG} & Bilinear & - & ✗ & - & (Ha and Kim, 2022, Cor. 8) \\  & & Monotone \& Lips. & - & ✓ & - & Theorem E.1 \\ \cline{2-7}  & LA-CEG+ & Comonotone \& Lips. & \((-\nicefrac{{1}}{{2L}},\infty)\) & ✓ & - & Corollary 7.7 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Overview of last iterate results with our contribution highlighted in blue. Prior to this work there existed no rates for 1-SCLI schemes handling \(\rho\)-comonotone problems with \(\rho\in(-\nicefrac{{1}}{{2L}},\infty)\) and no global convergence guarantees for Lookahead beyond bilinear games.

3. For \(\tau=2\) inner iterations we observe that \(\mathrm{LA-GDA}\) reduces to a linear interpolation between GDA and \(\mathrm{EG}\)+ which allows us to obtain global convergence in \(\rho\)-comonotone problems when \(\rho>-\nicefrac{{1}}{{3}}\sqrt{3}L\). However, for \(\tau\) large, we provide a counterexample showing that \(\mathrm{LA-GDA}\) cannot be guaranteed to converge. This leads us to instead propose \(\mathrm{LA-CEG}\)+ which corrects the inner optimization to guarantee global convergence for \(\rho\)-comonotone problems when \(\rho>-\nicefrac{{1}}{{2L}}\).
4. We test the methods on a suite of synthetic examples and GAN training where we confirm the stabilizing effect. Interestingly, RAPP seems to provide a similar benefit as Lookahead, which suggest that linear interpolation could play a key role also experimentally.

An overview of the theoretical results is provided in Table 1 and Figure 58B.

## 2 Related work

LookaheadThe Lookahead algorithm was first introduced for minimization in Zhang et al. (2019). In the context of Federated Averaging in federated learning (McMahan et al., 2017) and the Reptile algorithm in meta-learning (Nichol et al., 2018), the method can be seen as a single worker and single task instance respectively. Analysis for Lookahead was carried out for nonconvex minimization (Wang et al., 2020; Zhou et al., 2021) and a nested variant proposed in (Pushkin and Barba, 2021). Chavdarova et al. (2020) popularized the Lookahead algorithm for minimax training by showing state-of-the-art performance on image generation tasks. Apart from the original local convergence analysis in Chavdarova et al. (2020) and the bilinear case treated in Ha and Kim (2022) we are not aware of any convergence analysis for Lookahead for minimax problems and beyond.

CohypomonotoneCohypomonotone problems were first studied in Iusem et al. (2003); Combettes and Pennanen (2004) for proximal point methods and later expanded on in greater detail in Bauschke et al. (2021). The condition was relaxed to the star-variant referred to as the weak Minty variational inequality (MVI) in Diakonikolas et al. (2021) and the extragradient+ algorithm (\(\mathrm{EG}\)+) was analyzed. The analysis of \(\mathrm{EG}\)+ was later tightened and extended to the constrained case in Pethick et al. (2022).

Proximal pointThe proximal point method (PP) has a long history. For maximally monotone operators (and thus convex-concave minimax problems) convergence of PP follows from Opial (1967). The first convergence analysis of _inexact_ PP dates back to Rockafellar (1976); Brezis and Lions (1978). It was later shown that convergence also holds for the _relaxed_ inexact PP as defined in (8) (Eckstein and Bertsekas, 1992). In recent times, PP has gained renewed interest due to its success for certain nonmonotone structures. Inexact PP was studied for cohypomonotone problems in Iusem et al. (2003). Asymptotic convergence was established of the relaxed inexact PP for a sum of cohypomonotone operators in Combettes and Pennanen (2004), and later considered in Grimmer et al. (2022) without inexactness. Last iterate rates were established for PP in \(\rho\)-comonotone problems (with a dependency on \(\rho\)) (Gorbunov et al., 2022b). Explicit approximations of PP through a contractive map was used for convex-concave minimax problems in Cevher et al. (2023) and was the original motivation for MirrorProx of Nemirovski (2004). See Appendix A for additional references in the stochastic setting.

## 3 Setup

We are interested in finding a zero of an operator \(S:\mathbb{R}^{d}\rightrightarrows\mathbb{R}^{d}\) which decomposes into a Lipschitz continuous (but possibly nonmonotone) operator \(F\) and a maximally monotone operator \(A\), i.e. find \(z\in\mathbb{R}^{d}\) such that,

\[0\in Sz:=Az+Fz.\] (1)

Most relevant in the context of GAN training is that (1) includes constrained minimax problems.

**Example 3.1**.: _Consider the following minimax problem_

\[\min_{x\in\mathcal{X}}\max_{y\in\mathcal{Y}}\phi(x,y).\] (2)

_The problem can be recast as the inclusion problem (1) by defining the joint iterates \(z=(x,y)\), the stacked

Figure 1: Consider \(\min_{x\in\mathcal{X}}\max_{y\in\mathcal{Y}}\phi(z)\) with \(z=(x,y)\). As opposed to convex-concave minimax problems, the cohypomonotone condition allows the gradients \(Fz=(\nabla_{x}\phi(z),-\nabla_{y}\phi(z))\) to point away from the solutions (see Appendix B.1 for the relationship between cohypomonotonicity and the weak MVI). This can lead to instability issues for standard algorithms such as the Adam optimizer.

gradients \(Fz=(\nabla_{x}\phi(x,y),-\nabla_{y}\phi(x,y))\), and \(A=(\mathcal{N}_{\mathcal{X}},\mathcal{N}_{\mathcal{Y}})\) where \(\mathcal{N}\) denotes the normal cone. As will become clear (cf. Algorithm 1), \(A\) will only be accessed through the resolvent \(J_{\gamma A}:=(\mathrm{id}+\gamma A)^{-1}\) which reduces to the proximal operator. More specifically \(J_{\gamma A}(z)=(\mathrm{proj}_{\mathcal{X}}(x),\mathrm{proj}_{\mathcal{Y}}(y))\)._

We will rely on the following assumptions (see Appendix B for any missing definitions).

**Assumption 3.2**.: _In problem (1),_

1. _The operator_ \(A:\mathbb{R}^{d}\to\mathbb{R}^{d}\) _is maximally monotone._
2. _The operator_ \(F:\mathbb{R}^{d}\to\mathbb{R}^{d}\) _is_ \(L\)_-Lipschitz, i.e. for some_ \(L\in[0,\infty)\)_,_ \[\|Fz-Fz^{\prime}\|\leq L\|z-z^{\prime}\|\quad\forall z,z^{\prime}\in\mathbb{R} ^{d}.\]
3. _The operator_ \(S:=F+A\) _is_ \(\rho\)_-comonotone for some_ \(\rho\in(-\nicefrac{{1}}{{2L}},\infty)\)_, i.e._ \[\langle v-v^{\prime},z-z^{\prime}\rangle\geq\rho\|v-v^{\prime}\|^{2}\quad \forall(v,z),(v^{\prime},z^{\prime})\in\mathrm{grph}\,S.\]

_Remark 3.3_.: Assumption 3.2(iii) is also known as \(|\rho|\)-cohypomonotonicity when \(\rho<0\), which allows for increasing nonmonotonicity as \(|\rho|\) grows. See Appendix B.1 for the relationship with weak MVI.

When only stochastic feedback \(\hat{F}_{\sigma}(\cdot,\xi)\) is available we make the following classical assumptions.

**Assumption 3.4**.: _For the operator \(\hat{F}_{\sigma}(\cdot,\xi):\mathbb{R}^{d}\to\mathbb{R}^{d}\) the following holds._

1. _Unbiased:_ \(\mathbb{E}_{\xi}\big{[}\hat{F}_{\sigma}(z,\xi)\big{]}=Fz\quad\forall z\in \mathbb{R}^{d}\)_._
2. _Bounded variance:_ \(\mathbb{E}_{\xi}\big{[}\|\hat{F}_{\sigma}(z,\xi)-Fz\|^{2}\big{]}\leq\sigma^{2 }\quad\forall z,z^{\prime}\in\mathbb{R}^{d}\)_._

## 4 Inexact Krasnosel'skii-Mann iterations

The main work horse we will rely on is the inexact Krasnosel'skii-Mann (IKM) iteration from monotone operators (also known as the _averaged_ iteration), which acts on an operator \(T:\mathbb{R}^{d}\to\mathbb{R}^{d}\) with inexact feedback,

\[z^{k+1}=(1-\lambda)z^{k}+\lambda(Tz^{k}+e^{k}),\] (IKM)

where \(\lambda\in(0,1)\) and \(e^{k}\) is a random variable with dependency on all variables up until (and including) \(k\). The operator \(\widehat{T}_{k}:z\mapsto Tz+e^{k}\) can crucially be an iterative optimization scheme in itself. This is important, since we can obtain \(\textsc{RAPP}\), \(\textsc{LA-GDA}\) and \(\textsc{LA-CEG+}\) by plugging in different optimization routines. In fact, \(\textsc{RAPP}\) is derived by taking \(\widehat{T}_{k}\) to be a (contractive) fixed point iteration in itself, which approximates the resolvent.

We note that also the extragradient+ (EG+) method of Diakonikolas et al. (2021), which converges for cohypomonotone and Lipschitz problems, can be seen as a Krasnosel'skii-Mann iteration on an extragradient step

\[\begin{split}\mathrm{EG}(z)&=z-\gamma F(z-\gamma Fz )\\ z^{k+1}&=(1-\lambda)z^{k}+\lambda\,\mathrm{EG}(z^{ k})\end{split}\] (EG+)

where \(\lambda\in(0,1)\). We provide a proof of EG+ in Theorem G.1 which extends to the constrained case using the construction from Pethick et al. (2022) but through a simpler argument under fixed stepsize.

Essentially, the IKM iteration leads to a conservative update that stabilizes the update using the previous iterate. This is the key mechanism behind showing convergence in the nonmonotone setting known as cohypomonotonicity. Very generally, it is possible to provide convergence guarantees for IKM when the following holds (Theorem C.1 is deferred to the appendix due to space limitations).

**Definition 4.1**.: _An operator \(T:\mathbb{R}^{n}\to\mathbb{R}^{d}\) is said to be quasi-nonexpansive if_

\[\|Tz-z^{\prime}\|\leq\|z-z^{\prime}\|\quad\forall z\in\mathbb{R}^{d},\forall z ^{\prime}\in\mathrm{fix}\,T.\] (3)

_Remark 4.2_.: This notion is crucial to us since the resolvent \(J_{B}:=(\mathrm{id}+B)^{-1}\) is (quasi)-nonexpansive if \(B\) is \(\nicefrac{{1}}{{2}}\)-cohypomonotone (Bauschke et al., 2021, Prop. 3.9(iii)).

## 5 Approximating the resolvent

As apparent from Remark 4.2, the IKM iteration would provide convergence to a zero of the cohypomonotone operator \(S\) from Assumption 3.2 by using its resolvent \(\hat{T}=J_{\gamma S}\). However, the update is implicit, so we will instead approximate \(J_{\gamma S}\). Given \(z\in\mathbb{R}^{d}\) we seek \(z^{\prime}\in\mathbb{R}^{d}\) such that

\[z^{\prime}=J_{\gamma S}(z)=(\operatorname{id}+\gamma S)^{-1}z=(\operatorname{ id}+\gamma A)^{-1}(z-\gamma Fz^{\prime})\]

This can be approximated with a fixed point iteration of

\[Q_{z}:w\mapsto(\operatorname{id}+\gamma A)^{-1}(z-\gamma Fw)\] (4)

which is a contraction for small enough \(\gamma\) since \(F\) is Lipschitz continuous. It follows from Banach's fixed-point theorem Banach (1922) that the sequence converges linearly. We formalize this in the following theorem, which additionally applies when only stochastic feedback is available.

\[w^{t+1}=(\operatorname{id}+\gamma A)^{-1}(z-\gamma\hat{F}_{\sigma}(w^{t},\xi_ {t}))\quad\xi_{t}\sim\mathcal{P}\] (5)

**Lemma 5.1**.: _Suppose Assumptions 3.2(i), 3.2(ii) and 3.4. Given \(z\in\mathbb{R}^{d}\), the iterates generated by (5) with \(\gamma\in(0,\nicefrac{{1}}{{L}})\) converges to a neighborhood linearly, i.e.,_

\[\mathbb{E}\big{[}\|w^{\tau}-J_{\gamma S}(z)\|^{2}\big{]}\leq(\gamma L)^{2\tau }\|w^{0}-w^{\star}\|^{2}+\tfrac{\gamma^{2}}{(1-\gamma L)^{2}}\sigma^{2}.\] (6)

The resulting update in (5) is identical to GDA but crucially always steps from \(z\). We use this as a subroutine in RAPP to get convergence under a cohypomonotone operator while only suffering a logarithmic factor in the rate.

InterpretationIn the special case of the constrained minimax problem in (2), the application of the resolvent \(J_{\gamma S}(z)\) is equivalent to solving the following optimization problem

\[\min_{x^{\prime}\in\mathcal{X}}\max_{y^{\prime}\in\mathcal{Y}}\Bigl{\{}\phi_{ \mu}(x^{\prime},y^{\prime}):=\phi(x^{\prime},y^{\prime})+\frac{1}{2\mu}\left\| x^{\prime}-x\right\|^{2}-\frac{1}{2\mu}\left\|y^{\prime}-y\right\|^{2}\Big{\}}.\] (7)

for appropriately chosen \(\mu\in(0,\infty)\). (5) can thus be interpreted as solving a particular regularized subproblem. Later we will drop this regularization to arrive at the Lookahead algorithm.

## 6 Last iterate under cohypomonotonicity

As stated in Section 5, we can obtain convergence using the approximate resolvent through Theorem C.1. The convergence is provided in terms of the average, so additional work is needed for a last iterate result. IKM iteration on the approximate resolvent (i.e. \(\widetilde{T}_{k}(z)=J_{\gamma S}(z)+e^{k}\)) becomes,

\[\bar{z}^{k} =z^{k}-v^{k}\quad\text{with}\quad v^{k}\in\gamma S(\bar{z}^{k})\] (8a) \[z^{k+1} =(1-\lambda)z^{k}+\lambda(\bar{z}^{k}+e^{k})\] (8b)

with \(\lambda\in(0,1)\) and \(\gamma>0\) and error \(e^{k}\in\mathbb{R}^{d}\). Without error, (8) reduces to relaxed proximal point

\[z^{k+1}=(1-\lambda)z^{k}+\lambda J_{\gamma S}(z^{k})\] (Relaxed PP )

For a last iterate result it remains to argue that the residual \(\|J_{\gamma S}(z^{k})-z^{k}\|\) is monotonically decreasing (up to an error we can control). Showing monotonic decrease is fairly straightforward if \(\lambda=1\) (see Lemma E.1 and the associated proof). However, we face additional complication due to the averaging, which is apparent both from the proof and the slightly more complicated error term in the following lemma.

**Lemma 6.1**.: _If \(S\) is \(\rho\)-comonotone with \(\rho>-\tfrac{\gamma}{2}\) then (8) satisfies for all \(z^{\star}\in\operatorname{zer}S\),_

\[\|J_{\gamma S}(z^{k})-z^{k}\|^{2}\leq\|J_{\gamma S}(z^{k-1})-z^{k-1}\|^{2}+ \delta_{k}(z^{\star})\]

_where \(\delta_{k}(z):=4\|e^{k}\|(\|z^{k+1}-z\|+\|z^{k}-z\|)\)._

The above lemma allows us to obtain last iterate convergence for IKM on the inexact resolvent by combing the lemma with Theorem C.1.

**Theorem 6.2** (Last iterate of inexact resolvent).: _Suppose Assumptions 3.2 and 3.4 with \(\sigma_{k}\). Consider the sequence \((z^{k})_{k\in\mathbb{N}}\) generated by (8) with \(\lambda\in(0,1)\) and \(\rho>-\frac{\gamma}{2}\). Then, for all \(z^{\star}\in\mathrm{zer}\,S\),_

\[\mathbb{E}[\|J_{\gamma S}(z^{K})-z^{K}\|^{2}]\leq\frac{\|z^{0}-z^{\star}\|^{2}+ \sum_{k=0}^{K-1}\varepsilon_{k}(z^{\star})}{\lambda(1-\lambda)K}\quad+\frac{1} {K}\sum_{k=0}^{K-1}\sum_{j=k}^{K-1}\delta_{j}(z^{\star}),\]

_where \(\varepsilon_{k}(z):=2\lambda\mathbb{E}[\|e^{k}\|\|z^{k}-z\|]+\lambda^{2} \mathbb{E}[\|e^{k}\|^{2}]\) and \(\delta_{k}(z):=4\mathbb{E}[\|e^{k}\|(\|z^{k+1}-z\|+\|z^{k}-z\|)]\)._

_Remark 6.3_.: Notice that the rate in Theorem 6.2 has _no_ dependency on \(\rho\). Specifically, it gets rid of the factor \(\gamma/(\gamma+2\rho)\) which Gorbunov et al. (2022b, Thm. 3.2) shows is unimprovable for PP. Theorem 6.2 requires that the iterates stays bounded. In Corollary 6.4 we will assume bounded diameter for simplicity, but it is relatively straightforward to show that the iterates can be guaranteed to be bounded by controlling the inexactness (see Lemma E.2).

All that remains to get convergence of the explicit scheme in RAPP, is to expand and simplify the errors \(\varepsilon_{k}(z)\) and \(\delta_{k}(z)\) using the approximation of the resolvent analyzed in Lemma 5.1.

**Corollary 6.4** (Explicit inexact resolvent).: _Suppose Assumption 3.2 holds. Consider the sequence \((z^{k})_{k\in\mathbb{N}}\) generated by RAPP with deterministic feedback and \(\rho>-\frac{\gamma}{2}\). Then, for all \(z^{\star}\in\mathrm{zer}\,S\) with \(D:=\sup_{j\in\mathbb{N}}\|z^{j}-z^{\star}\|<\infty\),_

1. _with_ \(\tau=\frac{\log K}{\log(\nicefrac{{1}}{{\gamma}}\varepsilon)}\)_:_ \(\frac{1}{K}\sum_{i=0}^{K-1}\|J_{\gamma S}(z^{k})-z^{k}\|^{2}=\mathcal{O}\Big{(} \frac{\|z^{0}-z^{\star}\|^{2}}{\lambda(1-\lambda)K}+\frac{D^{2}}{(1-\lambda)K} \Big{)}\)_._
2. _with_ \(\tau=\frac{\log K^{2}}{\log(\nicefrac{{1}}{{\gamma}}\varepsilon)}\)_:_ \(\|J_{\gamma S}(z^{K})-z^{K}\|^{2}=\mathcal{O}\Big{(}\frac{\|z^{0}-z^{\star}\|^{ 2}}{\lambda(1-\lambda)K}+\frac{D^{2}}{K}+\frac{D^{2}}{(1-\lambda)K^{2}},\Big{)}\)_._

_Remark 6.5_.: Corollary 6.4(ii) implies an oracle complexity of \(\mathcal{O}\big{(}\log(\varepsilon^{-2})\varepsilon^{-1}\big{)}\) for ensuring that the last iterate satisfies \(\|J_{\gamma S}(z^{K})-z^{K}\|^{2}\leq\varepsilon\). A stochastic extension is provided in Corollary E.3 by taking the batch size increasing. Notice that RAPP, for \(\tau=2\) inner steps, reduces to EG+ in the unconstrained case where \(A\equiv 0\).

## 7 Analysis of Lookahead

The update in RAPP leads to a fairly conservative update in the inner loop, since it corresponds to optimizing a highly regularized subproblem as noted in Section 5. Could we instead replace the optimization procedure with gradient descent ascent (GDA)? If we replace the inner optimization routine we recover what is known as the Lookahead (LA) algorithm

\[w_{k}^{0} =z^{k}\] (LA-GDA) \[w_{k}^{t+1} =w_{k}^{t}-\gamma Fw_{k}^{t}\quad\forall t=0,...,\tau-1\] (LA-GDA) \[z^{k+1} =(1-\lambda)z^{k}+\lambda w_{k}^{\tau}\]

We empirically demonstrate that this scheme can converge for nonmonotone problems for certain choices of parameters (see Figure 2). However, what global guarantees can we provide theoretically?

It turns out that for LA-GDA with two inner steps (\(\tau=2\)) we have an affirmative answer. After some algebraic manipulation it is not difficult to see that the update can be simplified as follows

\[z^{k+1}=\tfrac{1}{2}(z^{k}-2\lambda\gamma Fz^{k})+\tfrac{1}{2}(z^{k}-2\lambda \gamma F(z^{k}-\gamma Fz^{k})).\] (9)This is the average of GDA and EG+ (when \(\lambda\in(0,\nicefrac{{1}}{{2}})\)). This observation allows us to show convergence under cohypomonotonicity. This positive result for nonmonotone problems partially explains the stabilizing effect of LA-GDA.

**Theorem 7.1**.: _Suppose Assumption 3.2 holds. Consider the sequence \((z^{k})_{k\in\mathbb{N}}\) generated by LA-GDA with \(\tau=2\), \(\gamma\leq\nicefrac{{1}}{{L}}\) and \(\lambda\in(0,\nicefrac{{1}}{{2}})\). Furthermore, suppose that_

\[2\rho>-(1-2\lambda)\gamma\quad\text{and}\quad 2\rho\geq 2\lambda\gamma-(1-\gamma^{2 }L^{2})\gamma.\] (10)

_Then, for all \(z^{\star}\in\mathrm{zer}\,F\),_

\[\frac{1}{K}\sum_{k=0}^{K-1}\|F\bar{z}^{k}\|^{2}\leq\frac{\|z^{0}-z^{\star}\|^{ 2}}{\lambda\gamma\big{(}(1-2\lambda)\gamma+2\rho\big{)}K}.\] (11)

_Remark 7.2_.: For \(\lambda\to 0\) and \(\gamma=\nicefrac{{c}}{{L}}\) where \(c\in(0,\infty)\), sufficient condition reduces to \(\rho\geq-\gamma(1-\gamma^{2}L^{2})/2=-\nicefrac{{c(1-c^{2})}}{{2L}}\), of which the minimum is attained with \(c=\nicefrac{{1}}{{\sqrt{3}}}\), leading to the requirement \(\rho\geq-\nicefrac{{1}}{{3\sqrt{3}L}}\). A similar statement is possible for \(z^{k}\). Thus, (LA-GDA) improves on the range of \(\rho\) compared with EG (see Table 1).

For larger \(\tau\), LA-GDA does not necessarily converge (see Figure 3 for a counterexample). We next ask what we would require of the base optimizer to guarantee convergence for any \(\tau\). To this end, we replace the inner iteration with some abstract algorithm \(\mathrm{Alg}:\mathbb{R}^{d}\to\mathbb{R}^{d}\), i.e.

\[w_{k}^{0} =z^{k}\] (LA) \[w_{k}^{t+1} =\mathrm{Alg}(w_{k}^{t})\quad\forall t=0,...,\tau-1\] \[z^{k+1} =(1-\lambda)z^{k}+\lambda w_{k}^{\tau}\]

Convergence follows from quasi-nonexpansiveness.

**Theorem 7.3**.: _Suppose \(\mathrm{Alg}:\mathbb{R}^{d}\to\mathbb{R}^{d}\) is quasi-nonexpansive. Then \((z^{k})_{k\in\mathbb{N}}\) generated by (LA) converges to some \(z^{\star}\in\mathrm{fix}\,\mathrm{Alg}\)._

_Remark 7.4_.: Even though the base optimizer \(\mathrm{Alg}\) might not converge (since nonexpansiveness is not sufficient), Theorem 7.3 shows that the outer loop converges. Interestingly, this aligns with the benefit observed in practice of using the outer iteration of Lookahead (see Figure 4).

CocoerciveFrom Theorem 7.3 we almost immediately get converge of LA-GDA for coercive problems since \(V=\mathrm{id}-\gamma F\) is nonexpansive iff \(\gamma F\) is \(\nicefrac{{1}}{{2}}\)-cocoercive.

**Corollary 7.5**.: _Suppose \(F\) is \(\nicefrac{{1}}{{L}}\)-cocoercive. Then \((z^{k})_{k\in\mathbb{N}}\) generated by LA-GDA with \(\gamma\leq\nicefrac{{2}}{{L}}\) converges to some \(z^{\star}\in\mathrm{zer}\,F\)._

_Remark 7.6_.: Corollary 7.5 can trivially be extended to the constrained case by observing that also \(V=(\mathrm{id}+\gamma A)^{-1}(\mathrm{id}-\gamma F)\) is nonexpansive when \(A\) is maximally monotone. As a special case this captures constrained convex and gradient Lipschitz minimization problems.

Figure 2: LA-GDA and RAPP can converge for Hsieh et al. (2021, Ex. 5.2). Interestingly, we can set the stepsize \(\gamma\) larger than \(\nicefrac{{1}}{{L}}\) while RAPP remains stable. Approximate proximal point (APP) with the same stepsize diverges (the iterates of APP are deferred to Figure 6). In this example, it is apparent from the rates, that there is a benefit in replacing the conservative inner update in RAPP with GDA in LA-GDA as explored in Section 7.

MonotoneWhen only monotonicity and Lipschitz holds we may instead consider the following extragradient based version of Lookahead (first empirically investigated in Chavdarova et al. (2020))

\[w_{k}^{0} =z^{k}\] (LA-EG) \[w_{k}^{t+1} =\mathrm{EG}(w_{t}^{k})\quad\forall t=0,...,\tau-1\] \[z^{k+1} =(1-\lambda)z^{k}+\lambda w_{k}^{\tau}\]

where \(\mathrm{EG}(z)=z-\gamma F(z-\gamma Fz)\). We show in Theorem F.1 that the \(\mathrm{EG}\)-operator of the inner loop is quasi-nonexpansive, which implies convergence of LA-EG through Theorem 7.3. Theorem F.1 extends even to cases where \(A\not\equiv 0\) by utilizing the forward-backward-forward construction of Tseng (1991). This providing the first global convergence guarantee for Lookahead beyond bilinear games.

CohypomonotoneFor cohypomonotone problems large \(\tau\) may prevent LA-GDA from converging (see Figure 3 for a counterexample). Therefore we propose replacing the inner optimization loop in LA-GDA with the method proposed in (Pethick et al., 2022, Alg. 1). Let \(H=\mathrm{id}-\gamma F\). We can write one step of the inner update with \(\alpha\in(0,1)\) as

\[\mathrm{CEG}^{+}(w)=w+2\alpha(H\bar{w}-Hw)\quad\text{with}\quad\bar{w}=\left( \mathrm{id}+\gamma A\right)^{-1}Hw.\] (12)

The usefulness of the operator \(\mathrm{CEG}^{+}:\mathbb{R}^{d}\to\mathbb{R}^{d}\) comes from the fact that it is quasi-nonexpansive under Assumption 3.2 (see Theorem G.1). Thus, Theorem 7.3 applies even when \(F\) is only cohypomonotone if we make the following modification to LA-GDA

\[w_{k}^{0} =z^{k}\] (LA-CEG+) \[w_{k}^{t+1} =\mathrm{CEG}^{+}(w_{k}^{t})\quad\forall t=0,...,\tau-1\] \[z^{k+1} =(1-\lambda)z^{k}+\lambda w_{k}^{\tau}\]

In the unconstrained case (\(A\equiv 0\)) this reduces to using the EG+ algorithm of Diakonikolas et al. (2021) for the inner loop. We have the following convergence guarantee.

**Corollary 7.7**.: _Suppose Assumption 3.2 holds. Then \((z^{k})_{k\in\mathbb{N}}\) generated by LA-CEG+ with \(\lambda\in(0,1)\), \(\gamma\in(\lfloor-2\rho\rfloor_{+},\nicefrac{{1}}{{L}})\) and \(\alpha\in(0,1+\frac{2\rho}{\gamma})\) converges to some \(z^{\star}\in\mathrm{zer}\,S\)._

## 8 Experiments

This section demonstrates that linear interpolation can lead to an improvement over common baselines.

Synthetic examplesFigures 2 and 3 demonstrate RAPP, LA-GDA and LA-CEG+ on a host of nonmonotone problems (Hsieh et al. (2021, Ex. 5.2), Pethick et al. (2022, Ex. 3(iii)), Pethick et al. (2022, Ex. 5)). See Appendix H.2 for definitions and further details.

Figure 3: We test the Lookahead variants on Pethick et al. (2022, Ex. 3(iii)) where \(\rho\in(\nicefrac{{-1}}{{8L}},\nicefrac{{-1}}{{10L}})\) (left) and Pethick et al. (2022, Ex. 5) with \(\rho=\nicefrac{{-1}}{{3}}\) (right). For the left example LA-GDA (provably) converges for \(\tau=2\), but may be nonconvergent for larger \(\tau\) as illustrate. Both variants of LA-GDA diverges in the more difficult example on the right, while LA-CEG+ in contrast provably converges. It seems that LA-CEG+ trades off a constant slowdown in the rate for convergence in a larger class.

[MISSING_PAGE_FAIL:9]

Acknowledgements

Thanks to Leello Dadi and Stratis Skoulakis for helpful discussion. This work was supported by Google. This work was supported by Hasler Foundation Program: Hasler Responsible AI (project number 21043). This work was supported by the Swiss National Science Foundation (SNSF) under grant number 200021_205011. This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement n\({}^{\circ}\) 725594 - time-data).

## References

* Arjevani et al. (2015) Arjevani, Y., Shalev-Shwartz, S., and Shamir, O. On lower and upper bounds for smooth and strongly convex optimization problems. _arXiv preprint arXiv:1503.06833_, 2015.
* Banach (1922) Banach, S. Sur les operations dans les ensembles abstraits et leur application aux equations integrales. _Fund. math_, 3(1):133-181, 1922.
* Bauschke & Combettes (2017) Bauschke, H. H. and Combettes, P. L. _Convex analysis and monotone operator theory in Hilbert spaces_. CMS Books in Mathematics. Springer, 2017. ISBN 978-3-319-48310-8.
* Bauschke et al. (2021) Bauschke, H. H., Moursi, W. M., and Wang, X. Generalized monotone operators and their averaged resolvents. _Mathematical Programming_, 189(1):55-74, 2021.
* Bertsekas (2011) Bertsekas, D. P. Incremental proximal methods for large scale convex optimization. _Mathematical programming_, 129(2):163-195, 2011.
* Bianchi (2015) Bianchi, P. On the convergence of a stochastic proximal point algorithm. In _CAMSAP_, 2015.
* Bravo & Cominetti (2022) Bravo, M. and Cominetti, R. Stochastic fixed-point iterations for nonexpansive maps: Convergence and error bounds. _arXiv preprint arXiv:2208.04193_, 2022.
* Brezis & Lions (1978) Brezis, H. and Lions, P. L. Produits infinis de resolvantes. _Israel Journal of Mathematics_, 29(4):329-345, 1978.
* Cai et al. (2022) Cai, Y., Oikonomou, A., and Zheng, W. Accelerated algorithms for monotone inclusions and constrained nonconvex-nonconcave min-max optimization. _arXiv preprint arXiv:2206.05248_, 2022.
* Cevher et al. (2023) Cevher, V., Piliouras, G., Sim, R., and Skoulakis, S. Min-max optimization made simple: Approximating the proximal point method via contraction maps, 2023. URL https://arxiv.org/abs/2301.03931.
* Chavdarova et al. (2020) Chavdarova, T., Pagliardini, M., Stich, S. U., Fleuret, F., and Jaggi, M. Taming GANs with Lookahead-Minmax. _arXiv preprint arXiv:2006.14567_, 2020.
* Combettes (2001) Combettes, P. L. Quasi-Fejerian analysis of some optimization algorithms. In _Studies in Computational Mathematics_, volume 8, pp. 115-152. Elsevier, 2001.
* Combettes & Pennanen (2004) Combettes, P. L. and Pennanen, T. Proximal methods for cohypomonotone operators. _SIAM journal on control and optimization_, 43(2):731-742, 2004.
* Diakonikolas (2020) Diakonikolas, J. Halpern iteration for near-optimal and parameter-free monotone inclusion and strong solutions to variational inequalities. In _Conference on Learning Theory_, pp. 1428-1451. PMLR, 2020.
* Diakonikolas et al. (2021) Diakonikolas, J., Daskalakis, C., and Jordan, M. I. Efficient methods for structured nonconvex-nonconcave min-max optimization. In _International Conference on Artificial Intelligence and Statistics_, pp. 2746-2754. PMLR, 2021.
* Eckstein & Bertsekas (1992) Eckstein, J. and Bertsekas, D. P. On the Douglas--Rachford splitting method and the proximal point algorithm for maximal monotone operators. _Mathematical Programming_, 55(1):293-318, 1992.
* Gidel et al. (2018) Gidel, G., Berard, H., Vignoud, G., Vincent, P., and Lacoste-Julien, S. A variational inequality perspective on generative adversarial networks. _arXiv preprint arXiv:1802.10551_, 2018.
* Gidel et al. (2019)Golovich, N., Pattathil, S., Daskalakis, C., and Ozdaglar, A. Last iterate is slower than averaged iterate in smooth convex-concave saddle point problems. In _Conference on Learning Theory_, pp. 1758-1784. PMLR, 2020.
* Gorbunov et al. (2022a) Gorbunov, E., Loizou, N., and Gidel, G. Extragradient method: O (1/k) last-iterate convergence for monotone variational inequalities and connections with cocoercivity. In _International Conference on Artificial Intelligence and Statistics_, pp. 366-402. PMLR, 2022a.
* Gorbunov et al. (2022b) Gorbunov, E., Taylor, A., Horvath, S., and Gidel, G. Convergence of proximal point and extragradient-based methods beyond monotonicity: the case of negative comonotonicity. _arXiv preprint arXiv:2210.13831_, 2022b.
* Grimmer et al. (2022) Grimmer, B., Lu, H., Worah, P., and Mirrokni, V. The landscape of the proximal point method for nonconvex-nonconcave minimax optimization. _Mathematical Programming_, pp. 1-35, 2022.
* Ha & Kim (2022) Ha, J. and Kim, G. On convergence of Lookahead in smooth games. In _International Conference on Artificial Intelligence and Statistics_, pp. 4659-4684. PMLR, 2022.
* Halpern (1967) Halpern, B. Fixed points of nonexpanding maps. 1967.
* Heusel et al. (2017) Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. _Advances in neural information processing systems_, 30, 2017.
* Hsieh et al. (2021) Hsieh, Y.-P., Mertikopoulos, P., and Cevher, V. The limits of min-max optimization algorithms: Convergence to spurious non-critical sets. In _International Conference on Machine Learning_, pp. 4337-4348. PMLR, 2021.
* Iusem et al. (2003) Iusem, A. N., Pennanen, T., and Svaiter, B. F. Inexact variants of the proximal point algorithm without monotonicity. _SIAM Journal on Optimization_, 13(4):1080-1097, 2003.
* Korpelevich (1977) Korpelevich, G. Extragradient method for finding saddle points and other problems. _Matekon_, 13(4):35-49, 1977.
* Krizhevsky et al. (2009) Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009.
* Lee & Kim (2021) Lee, S. and Kim, D. Fast extra gradient methods for smooth structured nonconvex-nonconcave minimax problems. _Advances in Neural Information Processing Systems_, 34:22588-22600, 2021.
* Lieder (2021) Lieder, F. On the convergence rate of the halpern-iteration. _Optimization letters_, 15(2):405-418, 2021.
* Luo & Tran-Dinh (2017) Luo, Y. and Tran-Dinh, Q. Extragradient-type methods for co-monotone root-finding problems.
* McMahan et al. (2017) McMahan, B., Moore, E., Ramage, D., Hampson, S., and y Arcas, B. A. Communication-efficient learning of deep networks from decentralized data. In _Artificial intelligence and statistics_, pp. 1273-1282. PMLR, 2017.
* Miyato et al. (2018) Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y. Spectral normalization for generative adversarial networks. _arXiv preprint arXiv:1802.05957_, 2018.
* Nemirovski (2004) Nemirovski, A. Prox-method with rate of convergence o (1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems. _SIAM Journal on Optimization_, 15(1):229-251, 2004.
* Nichol et al. (2018) Nichol, A., Achiam, J., and Schulman, J. On first-order meta-learning algorithms. _arXiv preprint arXiv:1803.02999_, 2018.
* Obukhov et al. (2020) Obukhov, A., Seitzer, M., Wu, P.-W., Zhydenko, S., Kyl, J., and Lin, E. Y.-J. High-fidelity performance metrics for generative models in PyTorch, 2020. URL https://github.com/toshas/torch-fidelity. Version: 0.3.0, DOI: 10.5281/zenodo.4957738.
* Opial (1967) Opial, Z. Weak convergence of the sequence of successive approximations for nonexpansive mappings. _Bulletin of the American Mathematical Society_, 73(4):591-597, 1967.
* O'Hagan (1967)* Patrascu and Irofti (2021) Patrascu, A. and Irofti, P. Stochastic proximal splitting algorithm for composite minimization. _Optimization Letters_, 15(6):2255-2273, 2021.
* Patrascu and Necoara (2017) Patrascu, A. and Necoara, I. Nonasymptotic convergence of stochastic proximal point methods for constrained convex optimization. _The Journal of Machine Learning Research_, 18(1):7204-7245, 2017.
* Pethick et al. (2022) Pethick, T., Latafat, P., Patrinos, P., Fercoq, O., and Cevher, V. Escaping limit cycles: Global convergence for constrained nonconvex-nonconcave minimax problems. In _International Conference on Learning Representations_, 2022.
* Pethick et al. (2023) Pethick, T., Fercoq, O., Latafat, P., Patrinos, P., and Cevher, V. Solving stochastic weak Minty variational inequalities without increasing batch size. In _The Eleventh International Conference on Learning Representations_, 2023.
* Pushkin and Barba (2021) Pushkin, D. and Barba, L. Multilayer Lookahead: a nested version of Lookahead. _arXiv preprint arXiv:2110.14254_, 2021.
* Rockafellar (1976) Rockafellar, R. T. Monotone operators and the proximal point algorithm. _SIAM journal on control and optimization_, 14(5):877-898, 1976.
* Salimans et al. (2016) Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen, X. Improved techniques for training GANs. _Advances in neural information processing systems_, 29, 2016.
* Toulis et al. (2015) Toulis, P., Horel, T., and Airoldi, E. M. The proximal Robbins-Monro method. _arXiv preprint arXiv:1510.00967_, 2015.
* Toulis et al. (2016) Toulis, P., Tran, D., and Airoldi, E. Towards stability and optimality in stochastic gradient descent. In _Artificial Intelligence and Statistics_, pp. 1290-1298. PMLR, 2016.
* Tseng (1991) Tseng, P. Applications of a splitting algorithm to decomposition in convex programming and variational inequalities. _SIAM Journal on Control and Optimization_, 29(1):119-138, 1991.
* Wang et al. (2020) Wang, J., Tantia, V., Ballas, N., and Rabbat, M. Lookahead converges to stationary points of smooth non-convex functions. In _ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pp. 8604-8608. IEEE, 2020.
* Yoon and Ryu (2021) Yoon, T. and Ryu, E. K. Accelerated algorithms for smooth convex-concave minimax problems with o (1/k^ 2) rate on squared gradient norm. In _International Conference on Machine Learning_, pp. 12098-12109. PMLR, 2021.
* Zhang et al. (2019) Zhang, M., Lucas, J., Ba, J., and Hinton, G. E. Lookahead optimizer: k steps forward, 1 step back. _Advances in neural information processing systems_, 32, 2019.
* Zhou et al. (2021) Zhou, P., Yan, H., Yuan, X., Feng, J., and Yan, S. Towards understanding why lookahead generalizes better than SGD and beyond. _Advances in Neural Information Processing Systems_, 34:27290-27304, 2021.