# Lovasz Principle for Unsupervised Graph Representation Learning

Ziheng Sun\({}^{1,2}\)  Chris Ding\({}^{1}\)  Jicong Fan \({}^{1,2}\)

\({}^{1}\)School of Data Science, The Chinese University of Hong Kong, Shenzhen, China

\({}^{2}\)Shenzhen Research Institute of Big Data, Shenzhen, China

zihengsun@link.cuhk.edu.cn  {chrisding,fanjicong}@cuhk.edu.cn

Corresponding author

###### Abstract

This paper focuses on graph-level representation learning that aims to represent graphs as vectors that can be directly utilized in downstream tasks such as graph classification. We propose a novel graph-level representation learning principle called Lovasz principle, which is motivated by the Lovasz number in graph theory. The Lovasz number of a graph is a real number that is an upper bound for graph Shannon capacity and is strongly connected with various global characteristics of the graph. Specifically, we show that the handle vector for computing the Lovasz number is potentially a suitable choice for graph representation, as it captures a graph's global properties, though a direct application of the handle vector is difficult and problematic. We propose to use neural networks to address the problems and hence provide the Lovasz principle. Moreover, we propose an enhanced Lovasz principle that is able to exploit the subgraph Lovasz numbers directly and efficiently. The experiments demonstrate that our Lovasz principles achieve competitive performance compared to the baselines in unsupervised and semi-supervised graph-level representation learning tasks. The code of our Lovasz principles is publicly available on GitHub1.

Footnote 1: https://github.com/SunZiheng0/Lovasz-Principle

## 1 Introduction

Graphs, such as chemical compounds, protein structures, and social networks, are non-Euclidean data that represent the relationships between entities. There have been a large number of previous works studying many aspects of graphs, including mutagenicity prediction of chemical compounds [1, 16], protein structure prediction [2], and community analysis of social networks [21].

Graph-based learning problems can be organized into two categories: node-level learning and graph-level learning. In this paper, we will only focus on graph-level learning. It is known that in graph-level learning, one fundamental task or step is to measure the distance or similarity between graphs. An important class of methods comparing graphs is graph kernel and many graph kernels have been proposed in the past decades [20]. For instance, random walk kernels [1] are the most widely-used and well-studied graph kernel family, which measure the graph similarity by counting the common random walks between graphs. The Weisfeiler-Lehman [17] family kernels are based on node label reassignment. Most graph kernels extract the similarity information between graphs by sampling sub-structures of graphs such as walks or reassigning the attributes of nodes with their neighborhoods. Note that graph kernels are implicit graph representation methods and hence their flexibilities are not high. In addition, the time and space complexities are quadratic with the number of graphs.

Graph representation learning aims to convert data with graph structure into vector representations that can be applied to various downstream tasks, such as graph clustering and classification. Many studies have been conducted on graph-level representation learning, and some of them use neural message-passing algorithms (Kipf et al., 2018; Xie and Grossman, 2018; Gilmer et al., 2017). For instance, the InfoGraph proposed by (Sun et al., 2019) achieves graph-level representations by maximizing the mutual information between the graph-level representation and the node-level representations. Graph contrastive learning (GraphCL) (You et al., 2020) and adversarial graph contrastive learning (AD-GCL) (Suresh et al., 2021) obtain graph-level representations by training graph neural networks (GNNs) to maximize the correspondence between the same graph's representations in its various augmented forms. JOint Augmentation Optimization (JOAO) (You et al., 2021) is a framework that automatically and adaptively selects data augmentations for GraphCL on specific graph data, using a unified bi-level min-max optimization approach. Automated Graph Contrastive Learning (AutoGCL) (Yin et al., 2022) uses learnable graph view generators and auto-augmentation strategy to generate contrastive samples while preserving the most representative structures of the original graph. These graph-level representation learning methods are all based on the InfoMax principle (Linsker, 1988). Note that there are many other graph representation learning methods such as VGAE (Kipf and Welling, 2016; Hamilton et al., 2017; Cui et al., 2020), graph embedding (Wu et al., 2020; Yu et al., 2021; Bai et al., 2019; Verma and Zhang, 2019), self-supervised learning (Liu et al., 2022; Hou et al., 2022; Lee et al., 2022; Xie et al., 2022; Wu et al., 2021; Rong et al., 2020; Zhang et al., 2021, 2021; Xiao et al., 2022), and contrastive learning (Le-Khac et al., 2020; Qiu et al., 2020; Ding et al., 2022; Xia et al., 2022; Fang et al., 2022; Trivedi et al., 2022; Han et al., 2022; Mo et al., 2022; Yin et al., 2022; Xu et al., 2021; Zhao et al., 2021; Zeng and Xie, 2021; Li et al., 2022, 2022, 2022), which will not be detailed in this paper due to the page length limit.

The InfoMax principle (Linsker, 1988), which is very popular in graph-level representation learning, advocates maximizing the mutual information between the representations of entire graphs and the representations of substructures of varying sizes (Peng et al., 2020; Velickovic et al., 2019; Hassani and Khasahmadi, 2020; Xie et al., 2022; Qiu et al., 2020). These InfoMax-based methods usually evaluate the mutual information (MI) between different representations using Jensen-Shannon MI estimator (Sun et al., 2019), following the formulations of \(f\)-GAN (Nowozin et al., 2016) and Mutual Information Neural Estimation (MINE) (Belghazi et al., 2018). However, the Jensen-Shannon MI estimator necessitates the training of a neural network parameterized discriminator, which is overly complex. In addition, the estimator is based on sampling, which may not be accurate enough in exploiting the mutual information. As opposed to InfoMax, researchers proposed the graph information bottleneck (GIB) (Wu et al., 2020) and the subgraph information bottleneck (SIB) (Yu et al., 2021) that aim to learn the minimal sufficient representation for downstream tasks. But GIB (Wu et al., 2020) and SIB (Yu et al., 2021) may fail if the downstream tasks are not available in the representation learning stage.

In this work, we introduce a novel graph learning principle called Lovasz principle, which is inspired by the Lovasz number (Lovasz, 1979) in graph theory. The Lovasz number is an upper bound for a graph's Shannon capacity. It is closely associated with various global characteristics of a graph, such as the clique number and chromatic number of the complement graph. The handle vector for calculating the Lovasz number is potentially a suitable choice for the graph-level representation, as it captures a graph's global features, though it suffers from a few difficulties. The contributions of this work are summarized as follows.

* We propose the Lovasz principle, a novel framework for unsupervised graph representation learning. We show how to effectively and efficiently utilize the handle vectors to represent graphs. The Lovasz principle exploits the topological structures of graphs globally via neural networks.
* We propose an enhanced Lovasz principle via effectively incorporating subgraph Lovasz numbers, while direct computation of subgraph Lovasz numbers is extremely costly. The enhanced Lovasz principle ensures similar graphs have similar representations.
* We extend the Lovasz principles to semi-supervised representation learning. Note that it is possible to adapt the Lovasz principles to more graph-based learning problems.

The experimental results of unsupervised learning, semi-supervised learning, and transfer learning on many benchmark graph datasets show that the proposed Lovasz principles outperform graph kernels, classical graph embedding methods, and InfoMax principle based representation learning methods.

## 2 Notations and Preliminaries

In this work, we use \(x\), \(\bm{x}\), \(\bm{X}\), \(\mathcal{X}\) (or \(X\)) to denote scalar, vector, matrix, and set respectively. \(\mathbf{1}_{\alpha\times b}\) is a matrix of size \(a\times b\) consisting only ones. \(\bm{I}_{n}\) denotes an identity matrix of size \(n\times n\). Let \(G=(V,E)\) be a graph with \(n\) nodes and \(a\)-dimensional node features \(\{\bm{x}_{v}\in\mathbb{R}^{a}|v\in V\}\). We denote \(\bm{A}\in\mathbb{R}^{n\times n}\) as the adjacency matrix and \(\bm{X}=[\bm{x}_{1},...,\bm{x}_{n}]^{\top}\in\mathbb{R}^{n\times a}\) as the node features matrix. Let \(\bm{z}\in\mathbb{R}^{d}\) be the \(d\)-dimensional graph-level representation of \(G\), \(\bm{h}_{v}\in\mathbb{R}^{d}\) be the \(d\)-dimensional node-level representation of node \(v\), and \(\bm{H}=[\bm{h}_{1},...,\bm{h}_{n}]^{\top}\in\mathbb{R}^{n\times d}\) be the node-level representations matrix of \(G\). We denote \((p,q)\) as an edge between nodes \(p,q\) and \((p,q)\in E\) if they are connected.

Let \(\mathcal{G}=\{G_{1},\dots G_{N}\}\) be a dataset of \(N\) graphs with \(K\) classes, where \(G_{i}=(V_{i},E_{i})\). For \(G_{i}\), we denote its number of nodes as \(n_{i}\), graph-level representation as \(\bm{z}_{i}\), the adjacency matrix as \(\bm{A}_{i}\), the node feature matrix as \(\bm{X}_{i}\), and node-level representation matrix as \(\bm{H}_{i}\). The graph-level representation matrix of dataset \(\mathcal{G}\) is denoted as \(\bm{Z}=[\bm{z}_{1},...,\bm{z}_{N}]^{\top}\in\mathbb{R}^{N\times d}\). The set of all node-level representations is denoted as \(\mathcal{H}=\{\bm{H}_{1},\dots,\bm{H}_{N}\}\).

### Lovasz number

The definition of Lovasz number [10] is based on orthonormal representations of a graph. Therefore we first introduce the definition of orthonormal representations.

**Definition 2.1** (Orthonormal representations).: Given a graph \(G=(V,E)\) with \(|V|=n\). Let

\[\mathcal{U}:=\{\bm{U}\in\mathbb{R}^{d\times n}:\|\bm{u}_{p}\|_{2}=1,p=1,2, \dots,n;\;\bm{u}_{p}^{\top}\bm{u}_{q}=0,\;\forall(p,q)\notin E\},\] (1)

where \(\bm{u}_{p}\) is the \(p\)-th column of \(\bm{U}\). Then every \(\bm{U}\in\mathcal{U}\) is an orthonormal representation of \(G\) in \(\mathbb{R}^{d}\).

Clearly, every graph has at least one orthonormal representation. For example, a trivial representation is that each node \(p\) is represented by the standard basis vector \(\bm{e}_{p}\). Based on Definition 2.1, we introduce the Lovasz number [10] of a graph as follows.

**Definition 2.2** (Lovasz number).: The Lovasz number of a graph \(G=(V,E)\) is defined as

\[\vartheta(G):=\min_{\bm{e},\bm{U}\in\mathcal{U}}\max_{p\in V}\frac{1}{(\bm{c} ^{\top}\bm{u}_{p})^{2}},\] (2)

where \(\bm{c}\in\mathbb{R}^{d}\) ranges over all unit vectors. The vector \(\bm{c}\) yielding the minimum for (2), denoted by \(\bm{c}^{*}\), is called the _handle_ of the representation, where the corresponding \(\bm{U}\) is denoted as \(\bm{U}^{*}\) for convenience. \(\bm{U}^{*}\) is called the optimal representation of \(G\) in \(\mathbb{R}^{d}\).

Laszlo Lovasz provided a pentagon example, shown in Figure 1, to explain Lovasz number defined by (2). The visualization of \(\bm{U}^{*}\) and \(\bm{c}^{*}\) of a pentagon is like an umbrella whose handle is \(\bm{c}^{*}\) and the ribs are the five columns of \(\bm{U}^{*}\). These five disjoint node pairs, i.e., \((\bm{u}_{1}^{*},\bm{u}_{3}^{*}),(\bm{u}_{1}^{*},\bm{u}_{4}^{*}),(\bm{u}_{2}^{ *},\bm{u}_{4}^{*}),(\bm{u}_{2}^{*},\bm{u}_{5}^{*}),(\bm{u}_{3}^{*},\bm{u}_{5}^ {*})\), are orthogonal to each other in visualization.

### Lovasz theta kernel

Johansson _et al._[2014] defined the Lovasz theta kernel to evaluate the similarity between graphs. Suppose \(S\subseteq V\) is a subset of the vertices of graph \(G\), then the Lovasz number of the subgraph

Figure 1: Pentagon example for Lovasz numberinduced by \(S\) is defined as

\[\vartheta_{S}(G):=\min_{\bm{c}}\max_{p\in S}\frac{1}{(\bm{c}^{\top}\bm{u}_{p})^{2}},\] (3)

where \(\bm{U}\) was pre-computed by Eq. (2) and \(\bm{c}\) ranges over all unit vectors.

**Definition 2.3** (Lovasz-\(\vartheta\) kernel (Johansson _et al._, 2014)).: Let \(k\) be a base kernel. The Lovasz theta kernel between graphs \(G=(V,E)\) and \(G^{\prime}=(V^{\prime},E^{\prime})\) is defined as

\[k_{\mathsf{Lo}}(G,G^{\prime})=\sum_{S\subseteq V}\sum_{S^{\prime}\subseteq V ^{\prime}}\frac{\delta(|S|,|S^{\prime}|)}{C_{S,S^{\prime}}}k(\vartheta_{S}(G),\vartheta_{S^{\prime}}(G^{\prime})),\] (4)

where \(C_{S,S^{\prime}}=\binom{|V|}{|S^{\prime}|}\), \(\delta(|S|,|S^{\prime}|)=1\) if \(|S|=|S^{\prime}|\), and \(\delta(|S|,|S^{\prime}|)=0\) otherwise.

\(k_{\mathsf{Lo}}\) is a positive semi-definite kernel (Johansson _et al._, 2014). It is able to capture global properties of graphs and has been shown useful in SVM-based graph classification (Johansson _et al._, 2014).

## 3 Lovasz Principle for Graph Representation Learning

The Lovasz number \(\vartheta(G)\) of a graph \(G\) provides an insight into the global property of the graph. It is a unique and deterministic value associated with an orthonormal representation \(\bm{U}^{*}\) and a unit _handle_ vector \(\bm{c}^{*}\). The umbrella example in Figure 1 explains how to compute the Lovasz number: compacting the ribs (i.e. \(\bm{U}^{*}\)) as much as possible and using \(\bm{c}^{*}\) as the handle. This example provides intuition that the handle vector \(\bm{c}^{*}\) is a natural and suitable representation of the graph \(G\).

Given \(\mathcal{G}=\{G_{1},G_{2},\ldots,G_{N}\}\) drawn from an unknown distribution \(\mathcal{D}_{G}\), we want to represent each graph as a vector such that these vectors preserve some important information of \(\mathcal{D}_{G}\). Suppose we have an algorithm \(\mathcal{A}\) such that

\[(\bm{U}^{*}_{i},\bm{c}^{*}_{i})=\mathcal{A}(G_{i}),\quad i=1,2,\ldots,N,\] (5)

where \(\mathcal{A}\) is some solver for (2). It is natural to use \(\bm{c}^{*}_{1},\bm{c}^{*}_{2},\ldots,\bm{c}^{*}_{N}\) as representations of \(G_{1},G_{2},\ldots,G_{N}\) respectively. However, this method has the following limitations4.

Footnote 4: In our experiments (Table 1), this naive method, termed as LovaszNum, is tested. In addition, the Lovasz-\(\vartheta\) kernel introduced in Section 2.2 is also tested.

1. **Non-uniqueness** For any \(G_{i}\), both \(\bm{U}^{*}_{i}\) and \(\bm{c}^{*}_{i}\) are not unique. For example, let \(\bm{Q}\in\mathbb{R}^{d\times d}\) be an orthonormal matrix, i.e., \(\bm{Q}^{\top}\bm{Q}=\bm{Q}\bm{Q}^{\top}=\bm{I}_{d}\), and let \(\bm{c}^{\prime}_{i}=\bm{Q}\bm{c}^{*}_{i}\) and \(\bm{U}^{\prime}_{i}=\bm{Q}\bm{U}^{*}_{i}\). We have \(\|\bm{c}^{\prime}\|_{2}=1\), \((\bm{U}^{\prime})^{\top}\bm{U}=(\bm{U}^{*}_{i})^{\top}\bm{U}^{*}_{i}\), and \((\bm{c}^{\prime})^{\top}\bm{u}^{\prime}_{p}=\vartheta(G_{i})\). This means \((\bm{c}^{*}_{i},\bm{U}^{*}_{i})\) and \((\bm{c}^{\prime}_{i},\bm{U}^{\prime})\) yield the same Lovasz number for \(G_{i}\), though they could be very different. Thus, for two graphs \(G_{i}\) and \(G_{j}\) in \(\mathcal{G}\), even when they are isomorphic, \(\bm{c}^{*}_{i}\) and \(\bm{c}^{*}_{j}\) could be very different. However, for graph representation, we hope that similar graphs have similar representations. For two graphs \(G_{i}\) and \(G_{j}\), one may align their orthonormal representations using \(\hat{\bm{Q}}=\text{argmin}_{\bm{Q}^{\top}\bm{Q}=\bm{I}_{d}}\|\bm{U}^{*}_{i}- \bm{Q}\bm{U}^{*}_{j}\|_{F}^{2}\) and compare them according to \(\|\bm{c}^{*}_{i}-\bm{c}^{*}_{j}\hat{\bm{Q}}\|_{2}\). This however only works when the \(n_{i}=n_{j}\) and \(G_{i}\) and \(G_{j}\) are matched.
2. **High computational cost** For each \(G_{i}\) in \(\mathcal{G}\), we need to solve the optimization problem (2), for which the time complexity of SDP is at least \(\mathcal{O}(|E_{i}|n_{i}^{2.5})\)(Jiang _et al._, 2020). Thus the total time complexity for \(\mathcal{G}\) is \(\mathcal{O}(\sum_{i=1}^{N}|E_{i}|n_{i}^{2.5})\). Therefore, this representation method is not scalable to large datasets.
3. **Ignorance of node features** The computation of (5) solely relies on the graph structure and does not take advantage of the node feature matrix \(\bm{X}_{i}\) that is often available and informative.
4. **Non-generalization** Suppose we have some new graphs and want to obtain their representations. We cannot utilize the representations of \(\mathcal{G}\) and we have to solve (2) again for each new graph.
5. **Non-global sensing** The computation of (5) treats each graph separately and cannot effectively take advantage of the global information or structure of \(\mathcal{G}\). Individual graphs may have noise or outliers, which cannot be handled by a local method.

To solve the aforementioned five issues, we present a machine learning method. We use a neural network \(\mathcal{F}_{W}\) (parameterized by \(W\)) to approximate \(\mathcal{A}\). \(\mathcal{F}_{W}\) can be learned from \(\mathcal{G}\) as well as some additional information such as the node feature matrices \(\{\bm{X}_{1},\dots,\bm{X}_{N}\}\). Specifically, we hope that

\[(\bm{U}_{i}^{*},\bm{c}_{i}^{*})\approx\mathcal{F}_{W}(\bm{A}_{i},\bm{X}_{i}), \quad i=1,2,\dots,N.\] (6)

Thus, \(\mathcal{F}_{W}\) plays a role representing a graph (drawn from \(\mathcal{D}_{G}\)) to a matrix of nodes representation and a vector of graph representation. For new graphs sampled from \(\mathcal{D}_{G}\), \(\mathcal{F}_{W}\) should generalize well when the approximation errors in (6) are small enough and \(\mathcal{F}_{W}\) is not too complex. For convenience, we split \(\mathcal{F}_{W}\) into two parts, i.e., \(\mathcal{F}_{W}(\cdot,\cdot)=(F(\cdot,\cdot;\theta),f(\cdot,\cdot;\phi))\), though \(F\) and \(f\) can share some parameters. We let \(\bm{U}_{i}^{*}\approx F(\bm{A}_{i},\bm{X}_{i};\theta)\) and \(\bm{c}_{i}^{*}\approx f(\bm{A}_{i},\bm{X}_{i};\phi)\). \(F(\cdot,\cdot;\theta)\) is the model of node-level representation learning while \(f(\cdot,\cdot;\phi)\) is the model of graph-level representation learning. We let

\[\bm{H}_{i}^{\theta}:=F(\bm{A}_{i},\bm{X}_{i};\theta),\ \ \text{and}\ \ \bm{z}_{i}^{\phi}:=f(\bm{A}_{i},\bm{X}_{i};\phi),\ \ \forall i=1,2,...,N.\] (7)

We denote the graph-level representations matrix as \(\bm{Z}_{\phi}=[\bm{z}_{1}^{\phi},...,\bm{z}_{N}^{\phi}]^{\top}\) and the node-level representations set as \(\mathcal{H}_{\theta}=\{\bm{H}_{1}^{\theta},...,\bm{H}_{N}^{\theta}\}\). To achieve (6), we propose to solve

\[\underset{\phi,\theta}{\text{minimize}}\sum_{i=1}^{N}\Bigg{\{}\underbrace{ \max_{p\in V_{i}}\frac{1}{((\bm{z}_{i}^{\phi})^{\top}\bm{h}_{p}^{\theta})^{2} }}_{\ell_{1}}+\mu\bigg{(}\underbrace{\big{\|}\bm{M}_{i}\odot\big{(}\bm{H}_{i}^ {\theta}(\bm{H}_{i}^{\theta})^{\top}-\bm{I}_{n_{i}}\big{)}\big{\|}_{F}^{2}}_{ \ell_{2}}+\underbrace{\big{(}(\bm{z}_{i}^{\phi})^{\top}\bm{z}_{i}^{\phi}-1 \big{)}^{2}}_{\ell_{3}}\bigg{)}\Bigg{\}},\] (8)

where \(\bm{M}_{i}=\bm{1}_{n_{i}\times n_{i}}-\bm{A}_{i}\) is a mask matrix and \(\mu>0\) is a regularization parameter. The roles of \(\ell_{1}\), \(\ell_{2}\), and \(\ell_{3}\) in (8) are explained as follows.

* \(\ell_{1}\) corresponds to the objective in the definition of Lovasz number of \(G_{i}\).
* \(\ell_{2}\) is to approximate the orthonormal representation for \(G_{i}\), i.e., \((\bm{h}_{p}^{\theta})^{\top}\bm{h}_{q}^{\theta}\approx 0\) if \((p,q)\notin E_{i}\) and \(\|\bm{h}_{p}\|_{2}\approx 1\ \forall p\in V_{i}\).
* \(\ell_{3}\) corresponds to the unit-length requirement for the handle vector of \(G_{i}\), i.e., \(\|\bm{z}_{i}^{\phi}\|_{2}\approx 1\).

We call (8) **Lovasz principle**SS, since it aims to learn an \(\mathcal{F}_{W}\) to solve the optimization of Lovasz number for the graphs drawn from \(\mathcal{D}_{G}\). It is known that the Lovasz number \(\vartheta(G)\) is an upper bound on the Shannon capacity of \(G=(V,E)\). The Shannon capacity [Shannon, 1956] models the amount of information that can be transmitted across a noisy communication channel, where certain signal values can be confused with each other. Here, one signal value corresponds to one node of \(G\) and \((p,q)\in E\) means that the corresponding two signals can be confused with each other. Therefore, the graph-level and node-level representations given by our Lovasz principle correspond to the upper bound of the amount of information transmitted over the graph that is distinguishable between nodes.

Footnote §: We also provide an equivalent formulation based on the complement graph of \(G\) in Appendix A.

Note that instead of the regularized unconstrained optimization (8), we can also use constrained optimization (\(\ell_{2}=\ell_{3}=0\)), which we call strict Lovasz principle. We may use the Lagrange multipliers method, projected gradient descent, or exact (or inexact) penalty method to solve the constrained optimization. Take the inexact penalty method as an example, we just need to increase the \(\mu\) in (8) gradually in the optimization. The graph representation performance comparison between unconstrained and constrained optimizations will be shown in Section 6.5 and Appendix E.

For convenience, we let

\[\mathcal{L}_{\text{Lo}}:=\sum_{i=1}^{|\mathcal{G}|}\max_{p\in V_{i}}\frac{1}{(( \bm{z}_{i}^{\phi})^{\top}\bm{h}_{p}^{\theta})^{2}}+\mu\left(\big{\|}\bm{M}_{i} \odot\big{(}\bm{H}_{i}^{\theta}(\bm{H}_{i}^{\theta})^{\top}-\bm{I}_{n_{i}} \big{)}\big{\|}_{F}^{2}+\Big{(}(\bm{z}_{i}^{\phi})^{\top}\bm{z}_{i}^{\phi}-1 \Big{)}^{2}\right),\] (9)

and call it Lovasz loss. The Lovasz loss is mainly designed for unsupervised graph-level representation learning [Wu _et al._, 2022; Maron _et al._, 2019; Oono and Suzuki, 2019; Stahlberg _et al._, 2022], which can be used as an alternative to the popular InfoMax loss [Linsker, 1988] (see (16)).

Lovasz principle for semi-supervised learningInspired by InfoGraph [Sun _et al._, 2019] (see (17)), we propose a Lovasz loss function for semi-supervised learning tasks. Suppose the dataset \(\mathcal{G}\) hastwo subsets: a labeled dataset \(\mathcal{G}^{L}\) and an unlabeled dataset \(\mathcal{G}^{U}\). Then we deploy another supervised encoder with parameter \(\psi\) and generate the supervised node-level representations \(\bm{H}_{i}^{\psi}\), graph-level representations \(\bm{z}_{i}^{\psi}\), and then prediction \(\bm{\hat{y}}_{i}^{\psi}\). The overall loss function is

\[\mathcal{L}_{\text{Lo-semi}}:=\sum_{l=1}^{|\mathcal{G}^{L}|}\ell_{\text{ supervised}}(\bm{\hat{y}}_{l}^{\psi},\bm{y}_{l})+\mathcal{L}_{\text{unsupervised}}(\mathcal{G})+\lambda\sum_{i=1}^{| \mathcal{G}|}\left\|\bm{z}_{i}^{\phi}-\bm{z}_{i}^{\psi}\right\|_{2}^{2},\] (10)

where \(\lambda\) is a positive hyperparameter, the supervised loss \(\ell_{\text{supervised}}\) is the cross-entropy loss, and the unsupervised loss \(\mathcal{L}_{\text{unsupervised}}\) is the Lovasz loss \(\mathcal{L}_{\text{Lo}}\) (Eq. (9)) or the enhanced Lovasz loss \(\mathcal{L}_{\text{ELo}}\) (Eq. (14)). The last term encourages the representations learned by the two encoders to be similar.

## 4 Enhancing Lovasz Principle with Subgraph Lovasz Number

Lovasz principle does not explicitly utilize the Lovasz number in graph embedding, though the Lovasz numbers of subgraphs can be useful in comparing graphs (Johansson _et al._, 2014). Therefore, we propose to use subgraph Lovasz number to enhance Lovasz principle based graph representation learning. We may consider taking advantage of the Lovasz-\(\vartheta\) kernel proposed by (Johansson _et al._, 2014). However, we encounter the following two difficulties.

1. Computing the Lovasz numbers (3) of subgraphs is time-consuming because we need to solve (2) for every graph and the number of subgraphs of each graph is often very large (up to \(2^{|V|}\)). Hence, for large graph dataset, we cannot use (4) directly.
2. The Lovasz-\(\vartheta\) kernel (4) is a pair-wise method and cannot effectively exploit the global structure of \(\mathcal{G}\).

To solve the aforementioned problems, we present an iterative-refinement strategy that computes the subgraph Lovasz numbers using the embeddings given by the Lovasz principle. Specifically, at iteration \(t\), we have the graph-level representations \(\bm{Z}_{\phi}^{(t-1)}\) and the node-level representations \(\mathcal{H}_{\theta}^{(t-1)}\) given by iteration \(t-1\). Inspired by the Lovasz-\(\vartheta\) kernel (4), we compute the similarity between graph \(G_{i}\) and \(G_{j}\) as

\[K_{ij}^{(t-1)}=\sum_{S_{i}\subseteq V_{i}}\sum_{S_{j}\subseteq V_{j}}\frac{ \delta(|S_{i}|,|S_{j}|)}{C_{S_{i},S_{j}}}k(\vartheta_{S_{i}}^{(t-1)}(G_{i}), \vartheta_{S_{j}}^{(t-1)}(G_{j})),\] (11)

where \(C_{S_{i},S_{j}}=\binom{|V_{i}|}{|S_{i}|}\binom{|V_{j}|}{|S_{j}|}\) and \(\vartheta_{S_{i}}^{(t-1)}(G_{i})\) (similar for \(G_{j}\)) is obtained by

\[\vartheta_{S_{i}}^{(t-1)}(G_{i})=\max_{p\in S_{i}}\frac{1}{(\bm{z}_{i}^{(t-1) \top}\bm{h}_{p}^{(t-1)})^{2}}.\] (12)

The computation of \(1/(\bm{z}_{i}^{(t-1)\top}\bm{h}_{p}^{(t-1)})^{2}\) for every \(p\in V_{i}\) was already done when computing \(\mathcal{L}_{\text{Lo}}\) via (9) at iteration \(t-1\) and there is no need to solve (3). For (11), we do not need to consider all possible subgraphs and we can just randomly sample subgraphs with some fixed sizes (numbers of nodes), which is similar to the truncated Lovasz-\(\vartheta\) kernel of (Johansson _et al._, 2014). Thus we can obtain the similarity \(K_{ij}^{(t-1)}\) efficiently. Adapting the idea of spectral embedding (Belkin and Niyogi, 2001), we propose the following subgraph Lovasz number (SLN) loss (at iteration \(t\))

\[\mathcal{L}_{\text{SLN}}^{(t)}:=\sum_{i=1}^{|\mathcal{G}|}\sum_{j=1}^{| \mathcal{G}|}K_{ij}^{(t-1)}\left\|\bm{z}_{i}^{\phi}-\bm{z}_{j}^{\phi}\right\|_ {2}^{2}+\gamma\left(\left\|\bm{Z}_{\phi}^{\top}\bm{Z}_{\phi}-\bm{I}_{d}\right\| _{F}^{2}+\left\|\bm{Z}_{\phi}^{\top}\bm{1}_{N\times 1}\right\|_{2}^{2}\right),\] (13)

where \(\gamma>0\). The two regularization terms in \(\mathcal{L}_{\text{SLN}}^{(t)}\) aim to make the graph-level representations orthonormal and centered, which is consistent with the constraints in spectral embedding. Minimizing \(\mathcal{L}_{\text{SLN}}^{(t)}\) encourages that the graph-level representations of similar graphs (in the sense of subgraph Lovasz numbers) are closer to each other at iteration \(t\). Integrating (13) with (9), we obtain the following enhanced Lovasz loss at iteration \(t\)

\[\mathcal{L}_{\text{ELo}}^{(t)}:=\mathcal{L}_{\text{Lo}}^{(t)}+\eta\mathcal{L}_ {\text{SLN}}^{(t)},\] (14)

where \(\eta>0\) is a hyperparameter. It is worth noting that \(\mathcal{L}_{\text{ELo}}\) as well as \(\mathcal{L}_{\text{Lo}}\) can be implemented batch-wisely, via replacing \(\mathcal{G}\) with its subsets. Similar to \(\mathcal{L}_{\text{Lo}}\), \(\mathcal{L}_{\text{ELo}}\) can also be applied to semi-supervised graph classification, i.e., (10).

## 5 Related Work

Besides the Lovasz-\(\vartheta\) introduced in Section 2.2, the closest work to our Lovasz principle is the InfoMax principle. Following [16, 17, 18], suppose the node-level representation \(\bm{h}_{p}(x)\) and the graph-level representation \(\bm{z}(x)\) are depending on the input \(x\), \(T_{\varphi}\) is a discriminator parameterized by a neural network with parameters \(\varphi\), the Jensen-Shannon mutual information (MI) estimator [19, 16, 15]\(I_{\varphi}\) between \(\bm{h}_{p}\) and \(\bm{z}\) is defined as

\[I_{\varphi}(\bm{h}_{p},\bm{z})=\mathbb{E}_{\mathbb{P}}[-\text{sp}(-T_{\varphi} (\bm{h}_{p}(x),\bm{z}(x)))]-\mathbb{E}_{\mathbb{P}\times\tilde{\mathbb{P}}}[ \text{sp}(T_{\varphi}(\bm{h}_{p}(x^{\prime}),\bm{z}(x)))],\] (15)

where \(x\) is the input sample from distribution \(\mathbb{P}\), \(x^{\prime}\) is the negative sample from distribution \(\tilde{\mathbb{P}}\), and \(\text{sp}(a)=\log(1+e^{a})\) denotes the softplus function. Many recent graph-level representation learning methods [17, 16, 18] are based on the InfoMax principle, i.e., maximizing (15). For instance, the InfoGraph proposed by [17] obtains graph-level representations by maximizing the mutual information between the graph-level representation and the node-level representations as follows

\[\phi^{*},\theta^{*},\varphi^{*}=\operatorname*{arg\,max}_{\phi,\theta,\varphi} \sum_{i=1}^{|\mathcal{G}|}\frac{1}{|V_{i}|}\sum_{p\in V_{i}}I_{\varphi}(\bm{h }_{p}^{\theta},\bm{z}_{i}^{\phi})\triangleq-\mathcal{L}_{\text{unsupervised} }^{I_{\varphi}}(\mathcal{G}).\] (16)

For semi-supervised learning, the dataset \(\mathcal{G}\) is split into labeled dataset \(\mathcal{G}^{L}\) and unlabeled dataset \(\mathcal{G}^{U}\). They deploy another supervised encoder with parameter \(\psi\) and then generate the supervised node-level representations \(\bm{H}_{i}^{\psi}\), graph-level representations \(\bm{z}_{i}^{\psi}\) and prediction \(\bm{\hat{g}}_{i}^{\psi}\). The loss function of InfoGraph for semi-supervised learning is defined as follows

\[\mathcal{L}_{\text{info-semi}}=\sum_{l=1}^{|\mathcal{G}^{L}|}\ell_{\text{ supervised}}(\bm{\hat{g}}_{l}^{\psi},\bm{y}_{l})+\mathcal{L}_{\text{ unsupervised}}^{I_{\varphi}}(\mathcal{G})-\lambda\sum_{i=1}^{|\mathcal{G}|}\frac{1}{|V_{i} |}I_{\varphi}(\bm{z}_{i}^{\phi}.\bm{z}_{i}^{\psi}).\] (17)

The comparison between the InfoMax principle and our Lovasz principle is as follows.

* The InfoMax principle focuses on the mutual information between graph-level representation and node-level representation, while our Lovasz principle is derived from the Lovasz number, a fundamental topological property of graph.
* Our Lovasz principle only needs to optimize \(\phi\) and \(\theta\). Differently, besides \(\phi\) and \(\theta\), the InfoMax principle has to optimize an additional discriminator parameter \(\varphi\) for the Jensen-Shannon MI estimator. Thus, our Lovasz principle is simpler than the InfoMax principle.
* Approximating mutual information using neural network is challenging [16] and the Jensen-Shannon MI estimator \(I_{\varphi}\) only provides an approximation by sampling rather than an exact computation. In contrast, our Lovasz principle does not rely on mutual information and sampling.

It is worth noting that the Lovasz convolutional networks (LCN) proposed by [16] was motivated by the observation that removing certain vertices from a graph doesn't affect the graph's global properties such as the Lovasz number. LCN does not involve any optimization related to the Lovasz number and was designed as an alternative to GCN. Our Lovasz principle is an optimization principle that can be used in any graph neural network (e.g. LCN). It is also useful in many applications such as graph prompt learning [19, 17] and graph anomaly detection [19, 17, 18].

## 6 Experiments

In this section, we evaluate the effectiveness of the Lovasz principle compared to the InfoMax principle in graph representation learning methods and a few other baselines such as graph kernels. The graph representation learning methods we considered in this paper include InfoGraph [17], GraphCL [16], AD-GCL [19], JAO [16], and AutoGCL [18], which are the most current and influential methods spanning from

[MISSING_PAGE_FAIL:8]

### Semi-supervised Learning

Following [Hu _et al._, 2019; You _et al._, 2021; Yin _et al._, 2022], we compare Lovasz principle with InfoMax principle in semi-supervised learning tasks. The semi-supervised losses of our Lovasz principle based methods and InfoMax based methods \(\mathcal{L}_{\text{info-semi}}\) were shown in (10) and (17) respectively. Following the settings of AutoGCL [Yin _et al._, 2022], we employ a 10-fold cross-validation on each dataset. For each fold, we use 80% of the total data as the unlabeled data, 10% as labeled training data, and 10% as labeled testing data. The classifier for labeled data is a ResGCN [Chen _et al._, 2019] with 5 layers and a hidden size of 128. We repeat each experiment 10 times and report the average accuracy in Table 2. We see that our Lovasz loss \(\mathcal{L}_{\text{Lo}}\) and the enhanced Lovasz loss \(\mathcal{L}_{\text{ELO}}\) outperformed InfoMax loss in all cases. Furthermore, \(\mathcal{L}_{\text{ELO}}\) outperformed \(\mathcal{L}_{\text{Lo}}\) in most cases. These results are consistent with those in Secon 6.1.

### Transfer Learning

Following [Hu _et al._, 2019; You _et al._, 2021; Yin _et al._, 2022], we compare the performance of our Lovasz principles with the InfoMax principle in the task of transfer learning. We use the Pretrain-GNN method [Hu _et al._, 2019] as a baseline and employ the Infomax, EdgePred, AttrMasking, and ContextPred pre-training strategies. The experimental settings follow those of AutoGCL [Yin _et al._, 2022]. More details are in the appendix. As shown in Table 3, the improved Lovasz loss \(\mathcal{L}_{\text{ELO}}\) performs the best on transfer learning tasks. In addition, the Lovasz principle based methods generally outperform those based on the InfoMax principle in most cases.

### Overall Performance and Significance Analysis

For convenience, we show the average performance of all methods over all datasets in Figure 2. We see our Lovasz principles outperformed other methods in the three tasks.

To measure the significance of the improvement over the baselines, we implement paired t-tests on the mean scores obtained from the datasets. A p-value below 0.05 indicates a significant difference. The results presented in Table 4 demonstrate the statistical significance of the improvements achieved by our methods across all the datasets.

\begin{table}
\begin{tabular}{c c|c c c c c c c c} \hline  & methods & NCHI & PROTEINS & DD & COLLAB & REDDIT-B & REDDIT-MSK & GITHUB \\ \hline  & _no Pretrain_ & 73.72\(\pm\)0.24 & 70.40\(\pm\)1.54 & 73.56\(\pm\)0.41 & 73.71\(\pm\)0.27 & 86.63\(\pm\)0.27 & 51.33\(\pm\)0.44 & 60.87\(\pm\)0.17 \\ \hline Pretrain- & Infomax & 74.86\(\pm\)0.26 & 72.27\(\pm\)0.40 & 75.78\(\pm\)0.34 & 73.76\(\pm\)0.29 & 88.66\(\pm\)0.95 & 53.61\(\pm\)0.31 & 65.21\(\pm\)0.88 \\ GNN & ContextPred & 73.00\(\pm\)0.30 & 70.23\(\pm\)0.63 & 74.66\(\pm\)0.51 & 3.69\(\pm\)0.37 & 84.76\(\pm\)0.52 & 51.23\(\pm\)0.84 & 62.35\(\pm\)0.73 \\ \hline \multirow{2}{*}{\begin{tabular}{c} InfoMax \\ principle \\ \end{tabular} } & GraphCL & 74.63\(\pm\)0.23 & 74.17\(\pm\)0.34 & 76.17\(\pm\)0.37 & 74.23\(\pm\)0.21 & 89.71\(\pm\)0.19 & 52.55\(\pm\)0.46 & 65.81\(\pm\)0.79 \\  & AD-GCL & 75.18\(\pm\)0.31 & 73.96\(\pm\)0.47 & 77.91\(\pm\)0.73 & 75.82\(\pm\)0.26 & 90.10\(\pm\)0.15 & 53.49\(\pm\)0.28 & 64.17\(\pm\)1.38 \\  & JOAOv2 & 74.86\(\pm\)0.39 & 73.31\(\pm\)0.48 & 75.81\(\pm\)0.73 & 75.53\(\pm\)0.18 & 88.79\(\pm\)0.65 & 52.71\(\pm\)0.28 & 66.66\(\pm\)0.60 \\  & AutoGCL & 73.75\(\pm\)2.25 & 75.65\(\pm\)2.40 & 77.50\(\pm\)4.41 & 77.16\(\pm\)1.48 & 79.80\(\pm\)3.47 & 49.91\(\pm\)2.70 & 62.46\(\pm\)1.51 \\ \hline \hline Lovasz & GraphCL & 75.46\(\pm\)1.53 & 75.12\(\pm\)1.87 & 77.74\(\pm\)1.52 & 76.12\(\pm\)1.15 & 89.87\(\pm\)1.68 & 53.99\(\pm\)1.68 & 66.72\(\pm\)1.53 \\ \cline{2-11}  & AD-GCL & 76.62\(\pm\)1.87 & 74.21\(\pm\)1.78 & 78.21\(\pm\)1.39 & 76.27\(\pm\)1.74 & 90.36\(\pm\)1.56 & 54.06\(\pm\)1.32 & 65.32\(\pm\)1.04 \\ \cline{2-11}  & JOAOv2 & 76.13\(\pm\)1.76 & 73.73\(\pm\)1.86 & 76.27\(\pm\)1.48 & 77.35\(\pm\)1.27 & 89.31\(\pm\)1.85 & 53.17\(\pm\)1.76 & 66.35\(\pm\)1.96 \\ \cline{2-11}  & AutoGCL & 75.77\(\pm\)1.48 & 76.30\(\pm\)1.57 & 78.16\(\pm\)1.61 & 76.63\(\pm\)1.78 & 84.64\(\pm\)2.53 & 51.31\(\pm\)1.81 & 64.87\(\pm\)1.62 \\ \hline \multirow{2}{*}{
\begin{tabular}{c} Lovasz \\ principle \\ (use \(\mathcal{L}_{\text{ELO}}\)) \\ \end{tabular} } & GraphCL & 75.81\(\pm\)1.68 & 75.38\(\pm\)1.67 & 78.43\(\pm\)1.48 & 75.75\(\pm\)1.58 & 50.67\(\pm\)1.27 & 54.81\(\pm\)1.73 & 67.04\(\pm\)1.45 \\ \cline{2-11}  & AD-GCL & **77.28\(\pm\)1.04** & 75.43\(\pm\)1.58 & 78.67\(\pm\)1.64 & 76.98\(\pm\)1.87 & **91.54\(\pm\)1.39** & **55.46\(\pm\)1.59** & 66.87\(\pm\)1.25 \\ \cline{2-11}  & JOAOv2 & 76.25\(\pm\)1.59 & 74.67\(\pm\)1.37 & 77.96\(\pm\)1.86 & **78.84\(\pm\)1.75** & 90.25\(\pm\)1.22 & 54.32\(\pm\)1.89 & **67.52\(\pm\)1.73** \\ \cline{2-11}  & AutoGCL & 76.53\(\pm\)1.92 & **76.89\(\pm\)1.55** & **78.82\(\pm\)1.90** & 78.46\(\pm\)1.39 & 87.31\(\pm\)1.57 & 53.17\(\pm\)1.50 & 66.47\(\pm\)1.26 \\ \hline \end{tabular}
\end{table}
Table 2: Performance (ACC) of semi-supervised learning.

\begin{table}
\begin{tabular}{c c|c c c c c c c c} \hline  & methods & BBBP & Tox21 & ToxCast & SIDER & ClinTox & MUV & HIV & BACE \\ \hline  & _no Pretrain_ & 65.8\(\pm\)4.3 & 74.0\(\pm\)0.8 & 63.4\(\pm\)0.6 & 57.3\(\pm\)1.6 & 58.0\(\pm\)4.4 & 71.8\(\pm\)2.5 & 75.3\(\pm\)1.9 & 70.1\(\pm\)5.4 \\ \hline Pretrain- & Infomax & 68.8\(\pm\)0.8 & 75.3\(\pm\)0.5 & 62.7\(\pm\)0.4 & 58.0\(\pm\)0.8 & 69.9\(\pm\)3.0 & 75.3\(\pm\)2.5 & 76.0\(\pm\)0.7 & 75.9\(\pm\)1.6 \\ GNN-s & EdgePred & 67.3\(\pm\)2.4 & 76.0\(\pm\)0.6 & 6.41\(\pm\)0.6 & 60.0\(\pm\)0.7 & 61.1\(\pm\)3.7 & 74.1\(\pm\)2.1 & 76.3\(\pm\)1.0 & 79.9\(\pm\)0.9 \\ strategies & AutoGCL & 64.3\(\pm\)2.8 & 76.7\(\pm\)0.4 & 64.2\(\pm\)0.5 & 61.0\(\pm\)0.7 & 71.8\(\pm\)1.4

### Measuring the Quality of Solver Approximation

Given a GNN model \(\mathcal{F}_{W}\) trained via the Lovasz principle, the predicted Lovasz number of a graph \(G\) is denoted as \(\hat{\vartheta}(G)\), while the ground-truth Lovasz number \(\vartheta(G)\) can be computed by SDP (Wolkowicz _et al._, 2012). Then we define the relative prediction error for the Lovasz number as

\[e_{\vartheta}=|\hat{\vartheta}(G)-\vartheta(G)|/\vartheta(G).\] (18)

Besides the regularized optimization of the Lovasz principle in (9), we also propose a constrained optimization method in Appendix E. We select 50 graphs from each of the four datasets and report \(e_{\vartheta}\) given by both the regularized (\(\mu=10\)) optimization and the constrained optimization for Lovasz principle in Table 5. We can see that in almost all cases, the relative prediction errors are less than \(10\%\). This indicates that the \(\mathcal{F}_{W}\) trained by the Lovasz principle is a good and reliable approximator for the solver \(\mathcal{A}\) of the Lovasz number. This is similar to the idea of learning to optimize.

### More Numerical Results

The results of **parameter sensitivity analysis, ablation study**, and **time cost comparison** are in Appendix C, Appendix D, and Appendix F respectively.

## 7 Conclusions

This paper proposed a novel method called Lovasz principle for unsupervised graph-level representation learning. An extension using the subgraph Lovasz number was also presented. The numerical results of unsupervised learning, semi-supervised learning, and transfer learning showed that the proposed methods are more effective than graph kernels and InfoMax principle based representation learning methods. Besides unsupervised representation learning, it is possible to apply our methods to other tasks such as graph-level clustering and graph generation. For instance, we can add a clustering module (e.g. (Xie _et al._, 2016)) to \(\mathcal{L}_{\text{Lo}}\) to construct an end-to-end clustering algorithm. We can combine \(\mathcal{L}_{\text{Lo}}\) with variational autoencoder (Kingma and Welling, 2013) to train a model to generate new graphs. Nevertheless, the implementation of these methods is out of the scope of this paper.

\begin{table}
\begin{tabular}{c|c|c c c c c} \hline tasks & principles comparison & InfoGraph & GraphCL & AD-GCL & JOAOV2 & AutoGCL \\ \hline \multirow{2}{*}{unsupervised} & InfoMax vs Lovasz (\(\mathcal{L}_{\text{Lo}}\)) & 0.00067 & 0.00286 & 0.02238 & 0.07347 & 0.00059 \\  & InfoMax vs Lovasz (\(\mathcal{L}_{\text{Lo}}\)) & 0.00005 & 0.01626 & 0.01541 & 0.01319 & 0.00035 \\  & Lovasz (\(\mathcal{L}_{\text{Lo}}\)) vs Lovasz (\(\mathcal{L}_{\text{Lo}}\)) & 0.00429 & 0.10925 & 0.01522 & 0.00079 & 0.00466 \\ \hline \multirow{2}{*}{semi-supervised} & InfoMax vs Lovasz (\(\mathcal{L}_{\text{Lo}}\)) & - & 0.00028 & 0.01115 & 0.04290 & 0.02147 \\  & InfoMax vs Lovasz (\(\mathcal{L}_{\text{Le}}\)) & - & 0.00051 & 0.00051 & 0.00116 & 0.01129 \\  & Lovasz (\(\mathcal{L}_{\text{Lo}}\)) vs Lovasz (\(\mathcal{L}_{\text{Le}}\)) & - & 0.00169 & 0.00076 & 0.00133 & 0.00545 \\ \hline \end{tabular}
\end{table}
Table 4: Significance analysis (\(p\)-values) of improvement via the paired t-test. A \(p\)-value less than 0.05 indicates a significant improvement.

\begin{table}
\begin{tabular}{c|c c c c c} \hline \(e_{\vartheta}\) (\%) & MUTAG & PROTEINS & DD & NCII \\ \hline regularized optimization & 9.7\(\pm\) 3.4 & 8.2\(\pm\)2.1 & 6.3\(\pm\)1.1 & 10.2\(\pm\) 3.6 \\ constrained optimization & 6.5\(\pm\) 2.4 & 7.3\(\pm\)1.6 & 6.1\(\pm\)1.2 & 8.5\(\pm\) 2.3 \\ \hline \end{tabular}
\end{table}
Table 5: Relative prediction errors \(e_{\vartheta}\) given by regularized optimization and constrained optimization

Figure 2: The average performance of different types of methods

## Acknowledgments

This work was partially supported by the Youth program 62106211 of the National Natural Science Foundation of China, the General Program JCYJ20210324130208022 of Shenzhen Fundamental Research, the research funding T00120210002 of Shenzhen Research Institute of Big Data, the Guangdong Key Lab of Mathematical Foundations for Artificial Intelligence, and the funding UDP01001770 of The Chinese University of Hong Kong, Shenzhen.

The authors appreciate the reviewers' and AC's comments and time.

## References

* Adhikari et al. (2018) Bijaya Adhikari, Yao Zhang, Naren Ramakrishnan, and B Aditya Prakash. Sub2vec: Feature learning for subgraphs. In _Advances in Knowledge Discovery and Data Mining: 22nd Pacific-Asia Conference, PAKDD 2018, Melbourne, VIC, Australia, June 3-6, 2018, Proceedings, Part II 22_, pages 170-182. Springer, 2018.
* Bai et al. (2019) Yunsheng Bai, Hao Ding, Yang Qiao, Agustin Marinovic, Ken Gu, Ting Chen, Yizhou Sun, and Wei Wang. Unsupervised inductive graph-level representation learning via graph-graph proximity. _arXiv preprint arXiv:1904.01098_, 2019.
* Belghazi et al. (2018) Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. Mutual information neural estimation. In _International conference on machine learning_, pages 531-540. PMLR, 2018.
* Belkin and Niyogi (2001) Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. _Advances in neural information processing systems_, 14, 2001.
* Borgwardt et al. (2005) Karsten M Borgwardt, Cheng Soon Ong, Stefan Schonauer, SVN Vishwanathan, Alex J Smola, and Hans-Peter Kriegel. Protein function prediction via graph kernels. _Bioinformatics_, 21(suppl_1):i47-i56, 2005.
* Cai et al. (2023) Jinyu Cai, Yunhe Zhang, and Jicong Fan. Self-discriminative modeling for anomalous graph detection. _arXiv preprint arXiv:2310.06261_, 2023.
* Chen et al. (2019) Ting Chen, Song Bian, and Yizhou Sun. Are powerful graph neural nets necessary? a dissection on graph classification. _arXiv preprint arXiv:1905.04579_, 2019.
* Cui et al. (2020) Ganqu Cui, Jie Zhou, Cheng Yang, and Zhiyuan Liu. Adaptive graph encoder for attributed graph embedding. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 976-985, 2020.
* Debnath et al. (1991) Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. _Journal of medicinal chemistry_, 34(2):786-797, 1991.
* Ding et al. (2022) Kaize Ding, Zhe Xu, Hanghang Tong, and Huan Liu. Data augmentation for deep graph learning: A survey. _ACM SIGKDD Explorations Newsletter_, 24(2):61-77, 2022.
* Fang et al. (2022) Yin Fang, Qiang Zhang, Haihong Yang, Xiang Zhuang, Shumin Deng, Wen Zhang, Ming Qin, Zhuo Chen, Xiaohui Fan, and Huajun Chen. Molecular contrastive learning with chemical element knowledge graph. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 3968-3976, 2022.
* Fuglede and Topsoe (2004) Bent Fuglede and Flemming Topsoe. Jensen-shannon divergence and hilbert space embedding. In _International symposium onInformation theory, 2004. ISIT 2004. Proceedings._, page 31. IEEE, 2004.
* Galli and Letchford (2017) Laura Galli and Adam N. Letchford. On the lovasz theta function and some variants. _Discrete Optimization_, 25:159-174, 2017.
* Gartner et al. (2003) Thomas Gartner, Peter Flach, and Stefan Wrobel. On graph kernels: Hardness results and efficient alternatives. In _Learning theory and kernel machines_, pages 129-143. Springer, 2003.
* Gartner et al. (2018)Anna Gaulton, Louisa J Bellis, A Patricia Bento, Jon Chambers, Mark Davies, Anne Hersey, Yvonne Light, Shaun McGlinchey, David Michalovich, Bissan Al-Lazikani, et al. Chembl: a large-scale bioactivity database for drug discovery. _Nucleic acids research_, 40(D1):D1100-D1107, 2012.
* Gilmer et al. (2017) Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In _International conference on machine learning_, pages 1263-1272. PMLR, 2017.
* Grotschel et al. (1981) Martin Grotschel, Laszlo Lovasz, and Alexander Schrijver. The ellipsoid method and its consequences in combinatorial optimization. _Combinatorica_, 1:169-197, 1981.
* Grover and Leskovec (2016) Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In _Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 855-864, 2016.
* Hamilton et al. (2017) William L Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods and applications. _arXiv preprint arXiv:1709.05584_, 2017.
* Han et al. (2022) Xiaotian Han, Zhimeng Jiang, Ninghao Liu, and Xia Hu. G-mixup: Graph data augmentation for graph classification. In _International Conference on Machine Learning_, pages 8230-8248. PMLR, 2022.
* Hassani and Khasahmadi (2020) Kaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on graphs. In _International conference on machine learning_, pages 4116-4126. PMLR, 2020.
* Hjelm et al. (2019) R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In _International Conference on Learning Representations_, 2019.
* Hou et al. (2022) Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, and Jie Tang. Graphmae: Self-supervised masked graph autoencoders. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 594-604, 2022.
* Hu et al. (2019) Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. _arXiv preprint arXiv:1905.12265_, 2019.
* Jiang et al. (2020) Haotian Jiang, Tarun Kathuria, Yin Tat Lee, Swati Padmanabhan, and Zhao Song. A faster interior point method for semidefinite programming. In _2020 IEEE 61st annual symposium on foundations of computer science (FOCS)_, pages 910-918. IEEE, 2020.
* Johansson et al. (2014) Fredrik Johansson, Vinay Jethava, Devdatt Dubhashi, and Chiranjib Bhattacharyya. Global graph kernels using geometric embeddings. In _International Conference on Machine Learning_, pages 694-702. PMLR, 2014.
* Kingma and Welling (2013) Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* Kipf and Welling (2016) Thomas N Kipf and Max Welling. Variational graph auto-encoders. _arXiv preprint arXiv:1611.07308_, 2016.
* Kipf et al. (2018) Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational inference for interacting systems. In _International Conference on Machine Learning_, pages 2688-2697. PMLR, 2018.
* Knuth (1993) Donald E Knuth. The sandwich theorem. _arXiv preprint math/9312214_, 1993.
* Krieg and Mutzel (2012) Nils Krieg and Petra Mutzel. Subgraph matching kernels for attributed graphs. _arXiv preprint arXiv:1206.6483_, 2012.
* Le-Khac et al. (2020) Phuc H Le-Khac, Graham Healy, and Alan F Smeaton. Contrastive representation learning: A framework and review. _Ieee Access_, 8:193907-193934, 2020.
* Lee et al. (2022) Namkeong Lee, Junseok Lee, and Chanyoung Park. Augmentation-free self-supervised learning on graphs. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 7372-7380, 2022.
* Le et al. (2019)Shuangli Li, Jingbo Zhou, Tong Xu, Dejing Dou, and Hui Xiong. Geomgcl: Geometric graph contrastive learning for molecular property prediction. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 4541-4549, 2022.
* Li et al. (2022) Yinqi Li, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. Optimal positive generation via latent transformation for contrastive learning. _Advances in Neural Information Processing Systems_, 35:18327-18342, 2022.
* Linsker (1988) Ralph Linsker. Self-organization in a perceptual network. _Computer_, 21(3):105-117, 1988.
* Liu et al. (2022) Yixin Liu, Ming Jin, Shirui Pan, Chuan Zhou, Yu Zheng, Feng Xia, and S Yu Philip. Graph self-supervised learning: A survey. _IEEE Transactions on Knowledge and Data Engineering_, 35(6):5879-5900, 2022.
* Liu et al. (2023) Zemin Liu, Xingtong Yu, Yuan Fang, and Xinming Zhang. Graphprompt: Unifying pre-training and downstream tasks for graph neural networks. In _Proceedings of the ACM Web Conference 2023_, pages 417-428, 2023.
* Lovasz (1979) Laszlo Lovasz. On the shannon capacity of a graph. _IEEE Transactions on Information theory_, 25(1):1-7, 1979.
* Ma et al. (2021) Xiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Quan Z Sheng, Hui Xiong, and Leman Akoglu. A comprehensive survey on graph anomaly detection with deep learning. _IEEE Transactions on Knowledge and Data Engineering_, 2021.
* Maron et al. (2019) Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph networks. _Advances in neural information processing systems_, 32, 2019.
* Mayr et al. (2018) Andreas Mayr, Gunter Klambauer, Thomas Unterthiner, Marvin Steijaert, Jorg K Wegner, Hugo Ceulemans, Djork-Arne Clevert, and Sepp Hochreiter. Large-scale comparison of machine learning methods for drug target prediction on chembl. _Chemical science_, 9(24):5441-5451, 2018.
* Mo et al. (2022) Yujie Mo, Liang Peng, Jie Xu, Xiaoshuang Shi, and Xiaofeng Zhu. Simple unsupervised graph representation learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 7797-7805, 2022.
* Morris et al. (2020) Christopher Morris, Nils M Krige, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. _arXiv preprint arXiv:2007.08663_, 2020.
* Narayanan et al. (2017) Annamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan, Lihui Chen, Yang Liu, and Shantanu Jaiswal. graph2vec: Learning distributed representations of graphs. _arXiv preprint arXiv:1707.05005_, 2017.
* Nowozin et al. (2016) Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. _Advances in neural information processing systems_, 29, 2016.
* Oono and Suzuki (2019) Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classification. _arXiv preprint arXiv:1905.10947_, 2019.
* Peng et al. (2020) Zhen Peng, Wenbing Huang, Minnan Luo, Qinghua Zheng, Yu Rong, Tingyang Xu, and Junzhou Huang. Graph representation learning via graphical mutual information maximization. In _Proceedings of The Web Conference 2020_, pages 259-270, 2020.
* Qiu et al. (2020) Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, and Jie Tang. Gcc: Graph contrastive coding for graph neural network pre-training. In _Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 1150-1160, 2020.
* Rong et al. (2020) Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-supervised graph transformer on large-scale molecular data. _Advances in Neural Information Processing Systems_, 33:12559-12571, 2020.
* Rong et al. (2020)C. Shannon. The zero error capacity of a noisy channel. _IRE Transactions on Information Theory_, 2(3):8-19, 1956.
* Shervashidze et al. (2009) Nino Shervashidze, SVN Vishwanathan, Tobias Petri, Kurt Mehlhorn, and Karsten Borgwardt. Efficient graphlet kernels for large graph comparison. In _Artificial intelligence and statistics_, pages 488-495. PMLR, 2009.
* Shervashidze et al. (2011) Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, and Karsten M Borgwardt. Weisfeiler-lehman graph kernels. _Journal of Machine Learning Research_, 12(9), 2011.
* Siglidis et al. (2020) Giannis Siglidis, Giannis Nikolentzos, Stratis Limnios, Christos Giatsidis, Konstantinos Skianis, and Michalis Vazirgiannis. Grakel: A graph kernel library in python. _The Journal of Machine Learning Research_, 21(1):1993-1997, 2020.
* Stahlberg et al. (2022) Simon Stahlberg, Blai Bonet, and Hector Geffner. Learning general optimal policies with graph neural networks: Expressive power, transparency, and limits. In _Proceedings of the International Conference on Automated Planning and Scheduling_, volume 32, pages 629-637, 2022.
* Sun et al. (2019) Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization. _arXiv preprint arXiv:1908.01000_, 2019.
* Sun et al. (2022) Mingchen Sun, Kaixiong Zhou, Xin He, Ying Wang, and Xin Wang. Gptt: Graph pre-training and prompt tuning to generalize graph neural networks. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 1717-1727, 2022.
* Suresh et al. (2021) Susheel Suresh, Pan Li, Cong Hao, and Jennifer Neville. Adversarial graph augmentation to improve graph contrastive learning. _Advances in Neural Information Processing Systems_, 34:15920-15933, 2021.
* Trivedi et al. (2022) Puja Trivedi, Ekdeep Singh Lubana, Yujun Yan, Yaoqing Yang, and Danai Koutra. Augmentations in graph contrastive learning: Current methodological flaws & towards better practices. In _Proceedings of the ACM Web Conference 2022_, pages 1538-1549, 2022.
* Velickovic et al. (2019) Petar Velickovic, William Fedus, William L Hamilton, Pietro Lio, Yoshua Bengio, and R Devon Hjelm. Deep graph infomax. _ICLR (Poster)_, 2(3):4, 2019.
* Verma & Zhang (2019) Saurabh Verma and Zhi-Li Zhang. Learning universal graph neural network embeddings with aid of transfer learning. _arXiv preprint arXiv:1909.10086_, 2019.
* Wei et al. (2022) Chunyu Wei, Jian Liang, Di Liu, and Fei Wang. Contrastive graph structure learning via information bottleneck for recommendation. _Advances in Neural Information Processing Systems_, 35:20407-20420, 2022.
* Weisfeiler & Leman (1968) Boris Weisfeiler and Andrei Leman. The reduction of a graph to canonical form and the algebra which appears therein. _NTI, Series_, 2(9):12-16, 1968.
* Wolkowicz et al. (2012) Henry Wolkowicz, Romesh Saigal, and Lieven Vandenberghe. _Handbook of semidefinite programming: theory, algorithms, and applications_, volume 27. Springer Science & Business Media, 2012.
* Wu et al. (2020) Tailin Wu, Hongyu Ren, Pan Li, and Jure Leskovec. Graph information bottleneck. _Advances in Neural Information Processing Systems_, 33:20437-20448, 2020.
* Wu et al. (2021) Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, and Xing Xie. Self-supervised graph learning for recommendation. In _Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval_, pages 726-735, 2021.
* Wu et al. (2022) Lingfei Wu, Peng Cui, Jian Pei, Liang Zhao, and Le Song. _Graph neural networks_. Springer, 2022.
* Xia et al. (2022) Jun Xia, Lirong Wu, Jintao Chen, Bozhen Hu, and Stan Z Li. Simgrace: A simple framework for graph contrastive learning without data augmentation. In _Proceedings of the ACM Web Conference 2022_, pages 1070-1079, 2022.
* Xu et al. (2022)* Xiao et al. (2022) Teng Xiao, Zhengyu Chen, Zhimeng Guo, Zeyang Zhuang, and Suhang Wang. Decoupled self-supervised learning for graphs. _Advances in Neural Information Processing Systems_, 35:620-634, 2022.
* Xie & Grossman (2018) Tian Xie and Jeffrey C Grossman. Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties. _Physical review letters_, 120(14):145301, 2018.
* Xie et al. (2016) Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis. In _International conference on machine learning_, pages 478-487. PMLR, 2016.
* Xie et al. (2022) Yaochen Xie, Zhao Xu, Jingtun Zhang, Zhengyang Wang, and Shuiwang Ji. Self-supervised learning of graph neural networks: A unified review. _IEEE transactions on pattern analysis and machine intelligence_, 2022.
* Xu et al. (2018) Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? _arXiv preprint arXiv:1810.00826_, 2018.
* Xu et al. (2021) Dongkuan Xu, Wei Cheng, Dongsheng Luo, Haifeng Chen, and Xiang Zhang. Infogcl: Information-aware graph contrastive learning. _Advances in Neural Information Processing Systems_, 34:30414-30425, 2021.
* Yadav et al. (2019) Prateek Yadav, Madhav Nimishakavi, Naganand Yadati, Shikhar Vashishth, Arun Rajkumar, and Partha Talukdar. Lovasz convolutional networks. In _The 22nd international conference on artificial intelligence and statistics_, pages 1978-1987. PMLR, 2019.
* Yanardag & Vishwanathan (2015) Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In _Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining_, pages 1365-1374, 2015.
* Yin et al. (2022) Yihang Yin, Qingzhong Wang, Siyu Huang, Haoyi Xiong, and Xiang Zhang. Autogcl: Automated graph contrastive learning via learnable view generators. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 8892-8900, 2022.
* You et al. (2020) Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. _Advances in Neural Information Processing Systems_, 33:5812-5823, 2020.
* You et al. (2021) Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning automated. In _International Conference on Machine Learning_, pages 12121-12132. PMLR, 2021.
* Yu et al. (2021) Junchi Yu, Tingyang Xu, Yu Rong, Yatao Bian, Junzhou Huang, and Ran He. Recognizing predictive substructures with subgraph information bottleneck. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2021.
* Zeng & Xie (2021) Jiaqi Zeng and Pengtao Xie. Contrastive self-supervised learning for graph classification. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 10824-10832, 2021.
* Zhang et al. (2021) Hengrui Zhang, Qitian Wu, Junchi Yan, David Wipf, and Philip S Yu. From canonical correlation analysis to self-supervised graph neural networks. _Advances in Neural Information Processing Systems_, 34:76-89, 2021.
* Zhang et al. (2021) Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and Chee-Kong Lee. Motif-based graph self-supervised learning for molecular property prediction. _Advances in Neural Information Processing Systems_, 34:15870-15882, 2021.
* Zhang et al. (2023) Yunhe Zhang, Yan Sun, Jinyu Cai, and Jicong Fan. Deep graph-level orthogonal hypersphere compression for anomaly detection. _arXiv preprint arXiv:2302.06430_, 2023.
* Zhao et al. (2021) Han Zhao, Xu Yang, Zhenru Wang, Erkun Yang, and Cheng Deng. Graph debiased contrastive learning with joint representation clustering. In _IJCAI_, pages 3434-3440, 2021.
* Zhuang et al. (2021)

[MISSING_PAGE_FAIL:16]

[MISSING_PAGE_FAIL:17]

The results show that a small \(d\) adversely affects the performance because \(d\) may be less than \(\alpha(G)\) on some graphs. When \(d\) is too large, the average accuracy decreases slightly because the representations with large \(d\) may capture some noisy information of a graph.

### \(\mu\) for orthonormal representation regularization

In Lovasz principle, \(\mu\) is the hyperparameter for orthonormal representation regularization in Lovasz loss \(\mathcal{L}_{\text{Lo}}\) (9) as follows

\[\mathcal{L}_{\text{Lo}}:=\sum_{i=1}^{|\mathcal{G}|}\max_{p\in V_{i}}\frac{1}{( (\bm{z}_{i}^{\phi})^{\top}\bm{h}_{p}^{\theta})^{2}}+\mu\left(\left\|\bm{M}_{i} \odot\left(\bm{H}_{i}^{\theta}(\bm{H}_{i}^{\theta})^{\top}-\bm{I}_{n}\right) \right\|_{F}^{2}+\left((\bm{z}_{i}^{\phi})^{\top}\bm{z}_{i}^{\phi}-1\right)^{2 }\right).\] (22)

In Figure 5, we fix other hyperparameters and tune \(\mu\) from \(\{10^{-3},10^{-2},...,10^{5},10^{6}\}\). The results show that \(\mu\) is not sensitive when \(0.1\leq\mu\leq 1e3\). If \(\mu\) is too small, the average accuracy decreases slightly because the node-level representations \(\bm{H}\) may not be orthonormal representations. A very large \(\mu\) adversely affects the performance because the orthonormal representation regularization dominates the representation learning such that the Lovasz principle fails.

Figure 4: The average ACC of different \(d\) on different dataset

Figure 5: The average ACC of different \(\mu\) on different data

### \(\gamma\) for orthogonal regularization in subgraph Lovasz number (SLN) loss

In Lovasz principle, \(\gamma\) is the hyperparameter for orthogonal regularization in subgraph Lovasz number (SLN) loss \(\mathcal{L}^{(t)}_{\text{SLN}}\) as follows

\[\mathcal{L}^{(t)}_{\text{SLN}}:=\sum_{i=1}^{|\mathcal{G}|}\sum_{j=1}^{| \mathcal{G}|}K^{(t-1)}_{ij}\|\bm{z}^{\phi}_{i}-\bm{z}^{\phi}_{j}\|_{2}^{2}+ \gamma(\|\bm{Z}^{\top}_{\phi}\bm{Z}_{\phi}-\bm{I}_{d}\|_{F}^{2}+\|\bm{Z}^{\top} _{\phi}\bm{1}_{N\times 1}\|_{2}^{2}),\] (23)

In Figure 6, we fix other hyperparameters and tune \(\gamma\) from \(\{10^{-3},10^{-2},...,10^{5},10^{6}\}\). The results show that \(\gamma\) is not sensitive when \(0.1\leq\gamma\leq 1e3\). If \(\gamma\) is too small, the average accuracy decreases slightly because the orthogonal constraints of spectral embedding may not hold. A very large \(\gamma\) adversely affects the performance because the orthogonal regularization of spectral embedding dominates the representation learning such that the Lovasz principle fails.

### \(\eta\) for subgraph Lovasz number (SLN) loss in enhanced Lovasz loss

In Lovasz principle, \(\eta\) is the hyperparameter for subgraph Lovasz number (SLN) loss \(\mathcal{L}^{(t)}_{\text{SLN}}\) in enhanced Lovasz loss \(\mathcal{L}^{(t)}_{\text{ELo}}\) as follows

\[\mathcal{L}^{(t)}_{\text{ELo}}:=\mathcal{L}^{(t)}_{\text{Lo}}+\eta\mathcal{L} ^{(t)}_{\text{SLN}}.\]

Figure 6: The average ACC of different \(\gamma\) on different data

Figure 7: The average ACC of different \(\eta\) on different dataIn Figure 7, we fix other hyperparameters and tune \(\eta\) from \(\{10^{-3},10^{-2},...,10^{5},10^{6}\}\). The results show that \(\eta\) is not sensitive when \(10^{-2}\leq\eta\leq 1e4\). If \(\eta\) is a very small number, the enhanced Lovasz loss \(\mathcal{L}_{\text{ELO}}^{(t)}\) degenerates into the Lovasz loss \(\mathcal{L}_{\text{Lo}}\), which also performs well in representation learning. A very large \(\eta\) adversely affects the performance because the subgraph Lovasz number (SLN) loss \(\mathcal{L}_{\text{SLN}}^{(t)}\) dominates the representation learning such that the Lovasz principle may fail.

### \(\lambda\) for the \(\ell_{2}\)-norm regularization in semi-supervised Lovasz loss

In semi-supervised learning, \(\lambda\) the hyperparameter for the \(\ell_{2}\)-norm regularization in semi-supervised Lovasz loss as follows

\[\mathcal{L}_{\text{Lo-semi}}:=\sum_{l=1}^{|\mathcal{G}^{L}|} \mathcal{L}_{\text{supervised}}(\boldsymbol{\hat{y}}_{l}^{\psi},\boldsymbol {y}_{l})+\mathcal{L}_{\text{unsupervised}}(\boldsymbol{H}_{i}^{\theta}, \boldsymbol{z}_{i}^{\phi})+\lambda\sum_{i=1}^{|\mathcal{G}|}\|\boldsymbol{z} _{i}^{\phi}-\boldsymbol{z}_{i}^{\psi}\|_{2}^{2},\] (24)

In Figure 8, we fix other hyperparameters and tune \(\lambda\) from \(\{10^{-3},10^{-2},...,10^{5},10^{6}\}\). The results show that \(\eta\) is not sensitive when \(0.1\leq\gamma\leq 1e3\). If \(\lambda\) is a very small number, the supervised encoder and unsupervised encoder may learn different information of a graph \(G\) such that the average accuracy slightly decreases. A very large \(\eta\) will cause the training to be trapped in early iterations such that the representation learning fails.

## Appendix D Ablation study

In this section, we analyze the importance of the orthonormal representation regularization (Eq. (9)) and the subgraph Lovasz number (SLN) loss (Eq. (14)) by ablation study on unsupervised learning.

### Ablation study of orthonormal representation regularization

In the ablation study, we remove the orthonormal representation regularization in Eq. (9) by setting \(\mu=0\). The results in Table 9 show that the orthonormal representation regularization can improve the performance of graph representation learning for two reasons:

* The orthonormal representation constraint is part of the definition of Lovasz number such that the Lovasz principle may fail without the orthonormal representation regularization.
* The orthonormal representation regularization guides the method the learn the structure property of a graph.

Figure 8: The average ACC of different \(\lambda\) on different data

[MISSING_PAGE_FAIL:21]

The comparisons between the regularized (\(\mu=1\)) optimization and the constrained optimization for two methods on four datasets are as follows.

## Appendix F Time cost comparison

We compare the time cost between the InfoMax principle and Lovasz principle using the InfoGraph Sun _et al._[2019] model on different datasets. We run the programming on a machine with Intel 7 CPU and RTX 3090 GPU. We repeat the experiment five times and report the results in Table 12. The Lovasz principle is the fastest method among the three.

The Lovasz principle is the fastest method among the three, and the reasons are as follows:

* The Lovasz principle is faster than the InfoMax principle because the former does not use the \(f\)-GAN [Nowozin _et al._, 2016] and the Jensen-Shannon MI estimator \(I_{\varphi}\) to evaluate the mutual information.
* Enhanced Lovasz principle is slightly lower than Lovasz principle because the computation of \(1/(\bm{z}_{i}^{(t-1)\top}\bm{u}_{p}^{(t-1)})^{2}\) for every \(p\in V_{i}\) was already done when computing \(\mathcal{L}_{\text{Lo}}\).

\begin{table}
\begin{tabular}{c|c|c c c c c} \hline tasks & principle & MUTAG & PROTEINS & DD & NCI \\ \hline \multirow{3}{*}{unsupervised learning} & InfoMax & 2.3 m & 12.6 m & 1 h 39 m & 36.5 m & 1 h 50 m & 5.8 m & 3 h 14 m & 7 h 31 m \\ \cline{2-8}  & Lovasz & 1.8 m & 11.7 m & 1 h 22 m & 33.2 m & 1 h 40 m & 5.1 m & 3 h 9 m & 7 h 26 m \\ \cline{2-8}  & Enhanced & 1.8 m & 12.2 m & 1 h 30 m & 34.3 m & 1 h 47 m & 5.3 m & 3 h 10 m & 7 h 27 m \\ \hline \hline \multirow{3}{*}{semi-supervised learning} & InfoMax & 2.3 m & 13.1 m & 1 h 47 m & 46.3 m & 2 h 21 m & 9.6 m & 3 h 47 m & 8 h 52 m \\ \cline{2-8}  & Lovasz & 2.0 m & 12.7 m & 1 h 39 m & 45.1 m & 2 h 13 m & 8.1 m & 3 h 27 m & 8 h 10 m \\ \cline{2-8}  & Enhanced & 2.0 m & 12.8 m & 1 h 40 m & 42.6 m & 2 h 17 m & 8.3 m & 3 h 29 m & 8 h 20 m \\ \hline \end{tabular}
\end{table}
Table 12: Time cost of InfoGraph. h stands for hour and m stands for minute. The brown value indicates the lowest time cost.

\begin{table}
\begin{tabular}{c|c|c c c c c} \hline  & method & MUTAG & PROTEINS & DD & NCI \\ \hline regularized opt. & InfoGraph & 89.67\(\pm\) 1.54 & 75.26 \(\pm\) 1.43 & 74.13\(\pm\) 1.49 & 78.21\(\pm\) 1.35 \\ regularized opt. & GraphCL & 87.24\(\pm\)1.96 & 75.87\(\pm\) 2.17 & 79.14 \(\pm\) 1.67 & 79.13\(\pm\) 1.27 \\ constrained opt. & InfoGraph & 86.12\(\pm\) 2.32 & 75.49\(\pm\) 1.52 & 76.42\(\pm\) 1.56 & 77.80\(\pm\) 1.24 \\ constrained opt. & GraphCL & 87.52 \(\pm\) 2.75 & 76.11\(\pm\) 1.36 & 78.54 \(\pm\) 2.21 & 77.63\(\pm\) 1.58 \\ \hline \end{tabular}
\end{table}
Table 11: Comparison between regularized optimization and constrained optimization