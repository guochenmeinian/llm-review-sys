# Saddle-to-Saddle Dynamics

in Diagonal Linear Networks

 Scott Pesme

EPFL

scott.pesme@epfl.ch &Nicolas Flammarion

EPFL

nicolas.flammarion@epfl.ch

###### Abstract

In this paper we fully describe the trajectory of gradient flow over \(2\)-layer diagonal linear networks for the regression setting in the limit of vanishing initialisation. We show that the limiting flow successively jumps from a saddle of the training loss to another until reaching the minimum \(\ell_{1}\)-norm solution. We explicitly characterise the visited saddles as well as the jump times through a recursive algorithm reminiscent of the LARS algorithm used for computing the Lasso path. Starting from the zero vector, coordinates are successively activated until the minimum \(\ell_{1}\)-norm solution is recovered, revealing an incremental learning. Our proof leverages a convenient arc-length time-reparametrisation which enables to keep track of the transitions between the jumps. Our analysis requires negligible assumptions on the data, applies to both under and overparametrised settings and covers complex cases where there is no monotonicity of the number of active coordinates. We provide numerical experiments to support our findings.

## 1 Introduction

Strikingly simple algorithms such as gradient descent are driving forces for deep learning and have led to remarkable empirical results. Nonetheless, understanding the performances of such methods remains a challenging and exciting mystery: (i) their global convergence on highly non-convex losses is far from being trivial and (ii) the fact that they lead to solutions which generalise well [53] is still not fully understood.

To explain this second point, a major line of work has focused on the concept of implicit regularisation: amongst the infinite space of zero-loss solutions, the optimisation process must be implicitly biased towards solutions which have good generalisation properties for the considered real-world prediction tasks. Many papers have therefore shown that gradient methods have the fortunate property of asymptotically leading to solutions which have a well-behaving structure [38, 24, 16].

Aside from these results which mostly focus on characterising the asymptotic solution, a slightly different point of view has been to try to describe the full trajectory. Indeed it has been experimentally observed that gradient methods with small initialisations have the property of learning models of increasing complexity across the training of neural networks [29]. This behaviour is usually referred to as _incremental learning_ or as a _saddle-to-saddle process_ and describes learning curves which are piecewise constant: the training process makes very little progress for some time, followed by a sharp transition where a new "feature" is suddenly learned. In terms of optimisation trajectory, this corresponds to the iterates "jumping" from a saddle of the training loss to another.

Several settings exhibiting such dynamics for small initialisation have been considered: matrix and tensor factorisation [44, 27], simplified versions of diagonal linear networks [23, 7], linear networks [22, 45, 26], \(2\)-layer neural networks with orthogonal inputs [10], learning leap functions with \(2\)-layer neural networks [1] and matrix sensing [2, 33, 28]. However, all these results requirerestrictive assumptions on the data or only characterise the first jump. Obtaining a complete picture of the saddle-to-saddle process by describing all the visited saddles and jump times is mathematically challenging and still missing. We intend to fill this gap by considering diagonal linear networks which are simplified neural networks that have received significant attention lately [50; 48; 25; 43; 20] as they are ideal proxy models for gaining a deeper understanding of complex phenomenons such as saddle-to-saddle dynamics.

### Informal statement of the main result

In this paper, we provide a full description of the trajectory of gradient flow over \(2\)-layer diagonal linear networks in the limit of vanishing initialisation. The main result is informally presented here.

**Theorem 1** (Main result, informal).: _In the regression setting and in the limit of vanishing initialisation, the trajectory of gradient flow over a \(2\)-layer diagonal linear network converges towards a limiting process which is piecewise constant: the iterates successively jump from a saddle of the training loss to another, each visited saddle and jump time can recursively be computed through an algorithm (Algorithm 1) reminiscent of the LARS algorithm for the Lasso._

The incremental learning stems from the particular structure of the saddles as they correspond to minimisers of the training loss with a constraint on the set of non-zero coordinates. The saddles therefore correspond to sparse vectors which partially fit the dataset. For simple datasets, a consequence of our main result is that **the limiting trajectory successively starts from the zero vector and successively learns the support of the sparse ground truth vector until reaching it. However, we make minimal assumptions on the data and our analysis also holds for complex datasets**. In that case, the successive active sets are not necessarily increasing in size and coordinates can deactivate as well as activate until reaching the minimum \(\ell_{1}\)-norm solution (see Figure 1 (middle) for an example of a deactivating coordinate). The regression setting and the diagonal network architecture are introduced in Section 2. Section 3 provides an intuitive construction of the limiting saddle-to-saddle dynamics and presents the algorithm that characterises it. Our main result regarding the convergence of the iterates towards this process is presented in Section 4 and further discussion is provided in Section 5.

## 2 Problem setup and leveraging the mirror structure

### Setup

**Linear regression.** We study a linear regression problem with inputs \((x_{1},\dots,x_{n})\in(\mathbb{R}^{d})^{n}\) and outputs \((y_{1},\dots,y_{n})\in\mathbb{R}^{n}\). We consider the typical quadratic loss:

\[L(\beta)=\frac{1}{2n}\sum_{i=1}^{n}(\langle\beta,x_{i}\rangle-y_{i})^{2}\,.\] (1)

Figure 1: Gradient flow \((\beta_{t}^{\alpha})_{t}\) with small initialisation scale \(\alpha\) over a \(2\)-layer diagonal linear network (for the precise experimental setting, see Appendix A). _Left:_ Training loss across time, the learning is piecewise constant. _Middle:_ The magnitudes of the coordinates are plotted across time: the process is piecewise constant. _Right:_ In the \(\mathbb{R}^{3}\) space in which the iterates evolve (the remaining coordinates stay at \(0\)), the iterates jump from a saddle of the training loss to another. The jumping times \(t_{i}\) as well as the visited saddles \(\beta_{i}\) are entirely predicted by our theory.

We make no assumption on the number of samples \(n\) nor the dimension \(d\). The only assumption we make on the data throughout the paper is that the inputs \((x_{1},\ldots,x_{n})\) are in _general position_. In order to state this assumption, let \(X\in\mathbb{R}^{n\times d}\) be the feature matrix whose \(i^{th}\) row is \(x_{i}\) and let \(\tilde{x}_{j}\in\mathbb{R}^{n}\) be its \(j^{th}\) column for \(j\in[d]\).

**Assumption 1** (General position).: _For any \(k\leq\min(n,d)\) and arbitrary signs \(\sigma_{1},\ldots,\sigma_{k}\in\{-1,1\}\), the affine span of any \(k\) points \(\sigma_{1}\tilde{x}_{j_{1}},\ldots,\sigma_{k}\tilde{x}_{j_{k}}\) does not contain any element of the set \(\{\pm\tilde{x}_{j},j\neq j_{1},\ldots,j_{k}\}\)._

This assumption is slightly technical but is standard in the Lasso literature [47]. Note that it is not restrictive as it is almost surely satisfied when the data is drawn from a continuous probability distribution [47, Lemma 4]. Letting \(\mathcal{S}=\operatorname*{arg\,min}_{\beta}L(\beta)\) denote the affine space of solutions, Assumption 1 ensures that the minimisation problem \(\min_{\beta^{*}\in\mathcal{S}}\lVert\beta^{*}\rVert_{1}\) has a unique minimiser which we denote \(\beta^{*}_{\ell_{1}}\) and which corresponds to the minimum \(\ell_{1}\)-norm solution.

**2-layer diagonal linear network.** In an effort to understand the training dynamics of neural networks, we consider a \(2\)-layer diagonal linear network which corresponds to writing the regression vector \(\beta\) as

\[\beta_{w}=u\odot v\;\;\text{where}\;\;w=(u,v)\in\mathbb{R}^{2d}\,.\] (2)

This parametrisation can be interpreted as a simple neural network \(x\mapsto\langle u,\sigma(\operatorname*{diag}(v)x)\rangle\) where \(u\) are the output weights, the diagonal matrix \(\operatorname*{diag}(v)\) represents the inner weights, and the activation \(\sigma\) is the identity function. We refer to \(w=(u,v)\in\mathbb{R}^{2d}\) as the _weights_ and to \(\beta\coloneqq u\odot v\in\mathbb{R}^{d}\) as the _prediction parameter_. With the parametrisation (2), the loss function \(F\) over the parameters \(w=(u,v)\in\mathbb{R}^{2d}\) is defined as:

\[F(w)\coloneqq L(u\odot v)=\frac{1}{2n}\sum_{i=1}^{n}(\langle u\odot v,x_{i} \rangle-y_{i})^{2}\,.\] (3)

Though this parametrisation is simple, the associated optimisation problem is non-convex and highly non-trivial training dynamics already occur. The critical points of the function \(F\) exhibit a very particular structure, as highlighted in the following proposition proven in Appendix B.

**Proposition 1**.: _All the critical points \(w_{c}\) of \(F\) which are not global minima, i.e., \(\nabla F(w_{c})=\mathbf{0}\) and \(F(w_{c})>\min_{w}F(w)\), are necessarily saddle points (i.e., not local extrema). They map to parameters \(\beta_{c}=u_{c}\odot v_{c}\) which satisfy \(|\beta_{c}|\odot\nabla L(\beta_{c})=\mathbf{0}\) and:_

\[\beta_{c}\in\operatorname*{arg\,min}_{\beta[i]=0}\operatorname*{arg\,min}_{ \begin{subarray}{c}\beta[i]=0\end{subarray}}L(\beta)\] (4)

_where \(\operatorname*{supp}(\beta_{c})=\{i\in[d],\beta_{c}[i]\neq 0\}\) corresponds to the support of \(\beta_{c}\)._

The optimisation problem in Eq. (4) states that the saddle points of the train loss \(F\) correspond to **sparse vectors that minimise the loss function \(L\) over its non-zero coordinates**. This property already shows that the saddle points possess interesting properties from a learning perspective. In the following we loosely use the term of'saddle' to refer to points \(\beta_{c}\in\mathbb{R}^{d}\) solution of Eq. (4) **that are not saddles of the convex loss function \(L\)**. We adopt this terminology because they correspond to points \(w_{c}\in\mathbb{R}^{2d}\) that are indeed saddles of the non-convex loss \(F\).

**Gradient Flow and necessity of "accelerating" time.** We minimise the loss \(F\) using gradient flow:

\[\mathrm{d}w_{t}=-\nabla F(w_{t})\mathrm{d}t\,,\] (5)

initialised at \(u_{0}=\sqrt{2}\alpha\mathbf{1}\in\mathbb{R}^{d}_{>0}\) with \(\alpha>0\), and \(v_{0}=\mathbf{0}\in\mathbb{R}^{d}\). This initialisation results in \(\beta_{0}=\mathbf{0}\in\mathbb{R}^{d}\) independently of the chosen weight initialisation scale \(\alpha\). We denote \(\beta^{\alpha}_{t}\coloneqq u^{\alpha}_{t}\odot v^{\alpha}_{t}\) the prediction iterates generated from the gradient flow to highlight its dependency on the initialisation scale \(\alpha\)1. The origin \(\mathbf{0}\in\mathbb{R}^{2d}\) is a critical point of the function \(F\) and taking the initialisation \(\alpha\to 0\) therefore arbitrarily slows down the dynamics. In fact, it can be easily shown for any fixed time \(t\), that \((u^{\alpha}_{t},v^{\alpha}_{t})\to\mathbf{0}\) as \(\alpha\to 0\), indicating that the iterates are stuck at the origin. Therefore if we restrict ourselves to a finite time analysis, there is no hope of exhibiting the observed saddle-to-saddle behaviour. To do so, we must find an appropriate bijection \(\tilde{t}_{\alpha}\) in \(\mathbb{R}_{\geq 0}\) which "accelerates" time (_i.e._\(\tilde{t}_{\alpha}(t)\underset{\alpha\to 0}{\longrightarrow}+\infty\) for all \(t\)) and consider the accelerated iterates \(\beta^{\alpha}_{\tilde{t}_{\alpha}(t)}\) which can escape the saddles. Finding this bijection becomes very natural once the mirror structure is unveiled.

### Leveraging the mirror flow structure

While the iterates \((w_{t}^{\alpha})_{t}\) follow a gradient flow on the non-convex loss \(F\), it is shown in [5] that the iterates \(\beta_{t}^{\alpha}\) follow a mirror flow on the convex loss \(L\) with potential \(\phi_{\alpha}\) and initialisation \(\beta_{t=0}^{\alpha}=\mathbf{0}\):

\[\mathrm{d}\nabla\phi_{\alpha}(\beta_{t}^{\alpha})=-\nabla L(\beta_{t}^{\alpha} )\mathrm{d}t,\] (6)

where \(\phi_{\alpha}\) is the hyperbolic entropy function [21] defined as:

\[\phi_{\alpha}(\beta)=\frac{1}{2}\sum_{i=1}^{d}\Big{(}\beta_{i} \mathrm{arcsinh}(\frac{\beta_{i}}{\alpha^{2}})-\sqrt{\beta_{i}^{2}+\alpha^{4} }+\alpha^{2}\Big{)}.\] (7)

Unveiling the mirror flow structure enables to leverage convex optimisation tools to prove convergence of the iterates to a global minimiser \(\beta_{\alpha}^{\star}\) as well as a simple proof of the implicit regularisation problem it solves. As shown by Woodworth et al. [50], in the overparametrised setting where \(d>n\) and where there exists an infinite number of global minima, the limit \(\beta_{\alpha}^{\star}\) is the solution of the problem:

\[\beta_{\alpha}^{\star}=\operatorname*{arg\,min}_{y_{i}=\langle x _{i},\beta\rangle,\forall i}\ \phi_{\alpha}(\beta).\] (8)

Furthermore, a simple function analysis shows that \(\phi_{\alpha}\) behaves as a rescaled \(\ell_{1}\)-norm as \(\alpha\) goes to \(0\), meaning that the recovered solution \(\beta_{\alpha}^{\star}\) converges to the minimum \(\ell_{1}\)-norm solution \(\beta_{\ell_{1}}^{\star}=\operatorname*{arg\,min}_{y_{i}=\langle x_{i},\beta \rangle}\|\beta\|_{1}\) as \(\alpha\) goes to \(0\) (see [49] for a precise rate). To bring to light the saddle-to-saddle dynamics which occurs as we take the initialisation to \(0\), we make substantial use of the nice mirror structure from Eq. (6).

**Appropriate time rescaling.** To understand the limiting dynamics of \(\beta_{t}^{\alpha}\), it is natural to consider the limit \(\alpha\to 0\) in Eq. (6). However, the potential \(\phi_{\alpha}\) is such that \(\phi_{\alpha}(\beta)\sim\ln(1/\alpha)\|\beta\|_{1}\) for small \(\alpha\) and therefore degenerates as \(\alpha\to 0\). Similarly, for \(\beta\neq\mathbf{0}\), \(\|\nabla\phi_{\alpha}(\beta)\|\to\infty\) as \(\alpha\to 0\). The formulation from Eq. (6) is thus not appropriate to take the limit \(\alpha\to 0\). We can nonetheless obtain a meaningful limit by considering the opportune time acceleration \(\tilde{t}_{\alpha}(t)=\ln(1/\alpha)\cdot t\) and looking at the accelerated iterates

\[\tilde{\beta}_{t}^{\alpha}\coloneqq\beta_{t_{\alpha}(t)}^{\alpha }=\beta_{\ln(1/\alpha)t}^{\alpha}.\] (9)

Indeed, a simple chain rule leads to the "accelerated mirror flow": \(\mathrm{d}\nabla\phi_{\alpha}(\tilde{\beta}_{t}^{\alpha})=-\ln{(\frac{1}{ \alpha})}\nabla L(\tilde{\beta}_{t}^{\alpha})\mathrm{d}t\). The accelerated iterates \((\tilde{\beta}_{t}^{\alpha})_{t}\) follow a mirror descent with a rescaled potential:

\[\mathrm{d}\nabla\tilde{\phi}_{\alpha}(\tilde{\beta}_{t}^{\alpha} )=-\nabla L(\tilde{\beta}_{t}^{\alpha})\mathrm{d}t,\qquad\mathrm{where} \qquad\tilde{\phi}_{\alpha}\coloneqq\frac{1}{\ln(1/\alpha)}\cdot\phi_{\alpha},\] (10)

with \(\tilde{\beta}_{t=0}=\mathbf{0}\) and where \(\phi_{\alpha}\) is defined Eq. (7). Our choice of time acceleration ensures that the rescaled potential \(\tilde{\phi}_{\alpha}\) is non-degenerate as the initialisation goes to \(0\) since \(\tilde{\phi}_{\alpha}(\beta)\underset{\alpha\to 0}{\sim}\|\beta\|_{1}\).

## 3 Intuitive construction of the limiting flow and saddle-to-saddle algorithm

In this section, we aim to give a comprehensible construction of the limiting flow. We therefore choose to provide intuition over pure rigor, and defer the full and rigorous proof to the Appendix E. The technical crux of our analysis is to demonstrate the existence of a piecewise constant limiting process towards which the iterates \(\tilde{\beta}^{\alpha}\) converge to. The convergence result is deferred to the following Section 4. **In this section we assume this convergence and refer to this piecewise constant limiting process as \((\tilde{\beta}_{t}^{\alpha})_{t}\)**. Our goal is then to determine the jump times \((t_{1},\dots,t_{p})\) as well as the saddles \((\beta_{0},\dots,\beta_{p})\) which fully define this process.

To do so, it is natural to examine the limiting equation obtained when taking the limit \(\alpha\to 0\) in Eq. (10). We first turn to its integral form which writes:

\[-\int_{0}^{t}\nabla L(\tilde{\beta}_{s}^{\alpha})\mathrm{d}s= \nabla\tilde{\phi}_{\alpha}(\tilde{\beta}_{t}^{\alpha}).\] (11)Provided the convergence of the flow \(\tilde{\beta}^{\alpha}\) towards \(\tilde{\beta}^{\circ}\), the left hand side of the previous equation converges to \(-\int_{0}^{t}\nabla L(\tilde{\beta}^{\circ}_{s})\mathrm{d}s\). For the right hand side, recall that \(\tilde{\phi}_{\alpha}(\beta)\stackrel{{\alpha\to 0}}{{\sim}}\|\beta \|_{1}\), it is therefore natural to expect the right hand side of Eq. (11) to converge towards an element of \(\partial\|\tilde{\beta}^{\circ}_{t}\|_{1}\), where we recall the definition of the subderivative of the \(\ell_{1}\)-norm as:

\[\partial\|\tilde{\beta}\|_{1}=\ \{1\}\ \ \text{if}\ \ \ \tilde{\beta}>0,\ \ \ \ \{-1\}\ \ \text{if}\ \ \tilde{\beta}<0,\ \ \ \ [-1,1]\ \ \text{if}\ \ \tilde{\beta}=0.\]

The arising key equation which must satisfy the limiting process \(\tilde{\beta}^{\circ}\) is then, for all \(t\geq 0\):

\[-\int_{0}^{t}\nabla L(\tilde{\beta}^{\circ}_{s})\mathrm{d}s\in \partial\|\tilde{\beta}^{\circ}_{t}\|_{1}.\] (12)

We show that **this equation uniquely determines the piecewise constant process \(\tilde{\beta}^{\circ}\)** by imposing the number of jumps \(p\), the jump times as well as the saddles which are visited between the jumps. Indeed the relation described in Eq. (12) provides \(4\) restrictive properties that enable to construct \(\tilde{\beta}^{\circ}\). To state them, let \(s_{t}=-\int_{0}^{t}\nabla L(\tilde{\beta}^{\circ}_{s})\mathrm{d}s\) and notice that it is continuous and piecewise linear since \(\tilde{\beta}^{\circ}\) is piecewise constant. For each coordinate \(i\in[d]\), it holds that:

* \(s_{t}[i]=1\Rightarrow\tilde{\beta}^{\circ}_{t}[i]\geq 0\) and \(s_{t}[i]=-1\Rightarrow\tilde{\beta}^{\circ}_{t}[i]\leq 0\)

* \(\tilde{\beta}^{\circ}_{t}[i]>0\Rightarrow s_{t}[i]=1\) and \(\tilde{\beta}^{\circ}_{t}[i]<0\Rightarrow s_{t}[i]=-1\)

To understand how these conditions lead to the algorithm which determines the jump times and the visited saddles, we present a \(2\)-dimensional example for which we can walk through each step. The general case then naturally follows from this simple example.

### Construction of the saddle-to-saddle algorithm with an illustrative \(2d\) example.

Let us consider \(n=d=2\) and data matrix \(X\in\mathbb{R}^{2\times 2}\) such that \(X^{\top}X=((1,0.2),(0.2,-0.2))\). We consider \(\beta^{\star}=(-0.2,2)\in\mathbb{R}^{2}\) and outputs \(y=X\beta^{\star}\). This setting is such that the loss \(L\) has \(\beta^{\star}\) as its unique minimum and \(L(\beta^{\star})=0\). Furthermore the non-convex loss \(F\) has \(3\) saddles which map to: \(\beta_{c,0}\coloneqq(0,0)=\operatorname*{arg\,min}_{\beta_{i}=0,\forall i}L(\beta)\), \(\beta_{c,1}\coloneqq(0.2,0)=\operatorname*{arg\,min}_{\beta[2]=0}L(\beta)\) and \(\beta_{c,2}\coloneqq(0,1.6)=\operatorname*{arg\,min}_{\beta[1]=0}L(\beta)\). The loss function \(L\) is sketched in Figure 2 (_Left_). Notice that by the definition of \(\beta_{c,1}\) and \(\beta_{c,2}\), the gradients of the loss at these points are orthogonal to the axis they belong to. When running gradient flow with a small initialisation over our diagonal linear network, we obtain the plots illustrated Figure 2 (_Middle and Right_). We observe three jumps: the iterates jump from the saddle at the origin to \(\beta_{c,1}\) at time \(t_{1}\), then to \(\beta_{c,2}\) at time \(t_{2}\) and finally to the global minimum \(\beta^{\star}\)at time \(t_{3}\).

Let us show how Eq. (12) enables us to theoretically recover this trajectory. A simple observation which we will use several times below is that for any \(t^{\prime}>t\) such that \(\tilde{\beta}^{\circ}\) is constant equal to \(\beta\) over the time interval \((t,t^{\prime})\), the definition of \(s\) enables to write that \(s_{t^{\prime}}=s_{t}-(t^{\prime}-t)\cdot\nabla L(\beta)\).

Figure 2: _Left_: Sketch of the \(2d\) loss. _Middle and right_: Outputs of gradient flow with small initialisation scale: the iterates are piecewise constant and \(s_{t}\) is piecewise linear across time. We refer to the main text for further details.

Zeroth saddle:The iterates are at the saddle at the origin: \(\hat{\beta}_{t}^{c}=\beta_{0}\coloneqq\beta_{c,0}\) and therefore \(s_{t}=-t\cdot\nabla L(\beta_{0})\). Our key equation Eq. (12) is verified since \(s_{t}=-t\cdot\nabla L(\beta_{0})\in\partial\|\beta_{0}\|_{1}=[-1,1]^{d}\). However the iterates cannot stay at the origin after time \(t_{1}\coloneqq 1/\|\nabla L(\beta_{0})\|_{\infty}\) which corresponds to the time at which the first coordinate of \(s_{t}\) hits \(+1\): \(s_{t_{1}}[1]=1\). If the iterates stayed at the origin after \(t_{1}\), 1 for \(i=1\) would be violated. The iterates must hence jump.

First saddle:The iterates can only jump to a point different from the origin which maintains Eq. (12) valid. We denote this point as \(\beta_{1}\). Notice that:

* \(s_{t_{1}}[2]=-t_{1}\cdot\nabla L(\beta_{0})[2]\in(-1,1)\) and since \(s_{t}\) is continuous, we must have \(\beta_{1}[2]=0\) (K3)
* \(s_{t_{1}}[1]=1\) and hence for \(t\geq t_{1}\), \(s_{t}[1]=1-(t-t_{1})\nabla L(\beta_{1})[1]\). We cannot have \(\nabla L(\beta_{1})[1]<0\) (K1), and neither \(\nabla L(\beta_{1})[1]>0\) since otherwise \(s_{t}[1]\in(-1,1)\) and \(\beta_{1}=\mathbf{0}\) (K3)

The two conditions \(\beta_{1}[2]=0\) and \(\nabla L(\beta_{1})[1]=0\)**uniquely defines \(\beta_{1}\) as equal to \(\beta_{c,1}\)**. We now want to know if and when the iterates jump again. We saw that \(s_{t}[1]\) remains at the value \(+1\). However since \(\beta_{1}\) is not a global minimum, \(\nabla L(\beta_{1})[2]\neq 0\) and \(s_{t}[2]\) hits \(+1\) at time \(t_{2}\) defined such that \(-(t_{1}\nabla L(\beta_{0})+(t_{2}-t_{1})\nabla L(\beta_{1}))[2]=1\). The iterates must jump otherwise 1 would break.

The iterates cannot jump to \(\beta^{\star}\) yet:As the second coordinate of the iterates can activate, one could expect the iterates to be able to jump to the global minimum. However note that \(s_{t}\) is a continuous function and that \(s_{t_{2}}\) is equal to the vector \((1,1)\). If the iterates jumped to the global minimum, then the first coordinate of the iterates would change sign from \(+0.2\) to \(-0.2\). Due to 4 this would lead \(s_{t}\) jumping from \(+1\) to \(-1\), violating its continuity.

Second saddle:We denote as \(\beta_{2}\) the point to which the iterates jump. \(s_{t_{2}}\) is now equal to the vector \((1,1)\) and therefore _(i)_\(\beta_{2}\geq 0\) (coordinate-wise) from 2 and 3 and the continuity of \(s\). Since \(s_{t}=s_{t_{2}}-(t-t_{2})\nabla L(\beta_{2})\), we must also have: _(ii)_\(\nabla L(\beta_{2})\geq 0\) from 1 for \(i\in\{1,2\}\), if \(\beta_{2}[i]\neq 0\) then \(\nabla L(\beta_{2})[i]=0\) from 4. The three conditions _(i)_, _(ii)_ and _(iii)_ precisely correspond to the optimality conditions of the following problem:

\[\operatorname*{arg\,min}_{\beta[1]\geq 0,\beta[2]\geq 0}L(\beta).\]

The unique minimiser of this problem is \(\beta_{c,2}\), hence \(\beta_{2}=\beta_{c,2}\), which means that the first coordinate deactivates. Similar to before, 1 is valid until the time \(t_{3}\) at which the first coordinate of \(s_{t}=s_{t_{2}}-(t-t_{2})\nabla L(\beta_{2})\) reaches \(-1\) due to the fact that \(\nabla L(\beta_{2})[1]>0\).

Global minimum:We follow the exact same reasoning as for the second saddle. We now have \(s_{t_{3}}\) equal to the vector \((-1,1)\) and the iterates must jump to a point \(\beta_{3}\) such that _(i)_\(\beta_{3}[1]\leq 0\), \(\beta_{3}[2]\geq 0\) (K2 and K3), _(ii)_\(\nabla L(\beta_{3})[1]\leq 0\), \(\nabla L(\beta_{3})[2]\geq 0\) (K1), _(iii)_ for \(i\in\{1,2\}\), if \(\beta_{3}[i]\neq 0\) then \(\nabla L(\beta_{3})[i]=0\) (K4). Again, these are the optimality conditions of the following problem:

\[\operatorname*{arg\,min}_{\beta[1]\leq 0,\beta[2]\geq 0}L(\beta).\]

\(\beta^{\star}\) is the unique minimiser of this problem and \(\beta_{3}=\beta^{\star}\). For \(t\geq t_{3}\) we have \(s_{t}=s_{t_{3}}\) and Eq. (12) is satisfied for all following times: the iterates do not have to move anymore.

### Presentation of the full saddle-to-saddle algorithm

We can now provide the full algorithm (Algorithm 1) which computes the jump times \((t_{1},\dots,t_{p})\) and saddles \((\beta_{0}=\mathbf{0},\beta_{1},\dots,\beta_{p})\) as the values and vectors such that the associated piecewise constant process satisfies Eq. (12) for all \(t\). This algorithm therefore defines our limiting process \(\tilde{\beta}^{\circ}\).

Algorithm 1 in words.The algorithm is a concise representation of the steps we followed in the previous section to construct \(\tilde{\beta}^{\circ}\). We explain each step in words below. Starting from \(k=0\), assume we enter the loop number \(k\) at the saddle \(\beta_{k}\) computed in the previous loop:

* The set \(\mathcal{A}_{k}\) contains the set of coordinates "which are unstable": by having a non-zero derivative, the loss could be decreased by moving along each one of these coordinates and one of these coordinates will have to activate.

* The time gap \(\Delta_{k}\) corresponds to the time spent at the saddle \(\beta_{k}\). It is computed as being the elapsed time just before (1) breaks if the coordinates do not jump.
* We update \(t_{k+1}=t_{k}+\Delta_{k}\) and \(s_{k+1}=s_{k}-\Delta_{k}\nabla L(\beta_{k})\): \(t_{k+1}\) corresponds to the time at which the iterates leave the saddle \(\beta_{k}\) and \(s_{k+1}\) constrains the signs of the next saddle \(\beta_{k+1}\)
* The solution \(\beta_{k+1}\) of the constrained minimisation problem is the saddle to which the flow jumps to at time \(t_{k+1}\). The optimality conditions of this problem are such that Eq. (12) is maintained for \(t\geq t_{k+1}\).

Various comments on Algorithm 1.First we point out that any solution \(\beta_{c}\) of the constrained minimisation problem which appears in Algorithm 1 also satisfies \(\beta_{c}=\arg\min_{\beta|\in\mathbb{R}^{d}\ \text{for}\ i\notin\text{supp}( \beta_{c})}\ L(\beta)\) as in Eq. (4): the algorithm hence indeed outputs saddles as expected. Up until now we have never checked whether the algorithm's constrained minimisation problem has a unique minimum. This is crucial otherwise the assignment step would be ill-defined. Showing the uniqueness is non-trivial and is guaranteed thanks to the general position Assumption 1 on the data (see Proposition 7 in Appendix D.1). In this same proposition, we also show that the algorithm terminates in at most \(\min(2^{d},\sum_{k=0}^{n}\binom{d}{k})\) steps, that the loss strictly decreases at each step and that the final output \(\beta_{p}\) is the minimum \(\ell_{1}\)-norm solution. These last two properties are expected given the fact that the algorithm arises as being the limit process of \(\tilde{\beta}^{\alpha}\) which follows the mirror flow Eq. (10).

``` Initialise:\((t,\beta,s)\leftarrow(0,\mathbf{0},\mathbf{0})\); while\(\nabla L(\beta)\neq\mathbf{0}\)do \(\mathcal{A}\leftarrow\{j\in[d],\nabla L(\beta)(j)\neq 0\}\) \(\Delta\leftarrow\inf\left\{\delta>0\ \text{s.t.}\ \exists i\in\mathcal{A},\ s(i)-\delta\nabla L(\beta)(i)=\pm 1\right\}\) \((t,\ s)\leftarrow(t+\Delta,\ s-\Delta\cdot\nabla L(\beta))\) \(\beta\leftarrow\arg\min\ L(\beta)\ \ \text{where}\ \ \beta\in\left\{\beta\in\mathbb{R}^{d}\ \text{ s.t.}\ \begin{array}{l}\beta_{i}\geq 0\ \text{if}\ s(i)=+1\\ \beta_{i}\leq 0\ \text{if}\ s(i)=-1\\ \beta_{i}=0\ \text{if}\ s(i)\in(-1,1)\end{array}\right\}\)  end while Output: Successive values of \(\beta\) and \(t\) ```

**Algorithm 1**Successive saddles and jump times of \(\lim_{\alpha\to 0}\tilde{\beta}^{\alpha}\)

**Links with the LARS algorithm for the Lasso.** Recall that the Lasso problem [46; 15] is formulated as:

\[\beta_{\lambda}^{\star}=\operatorname*{arg\,min}_{\beta\in\mathbb{R}^{d}}\ L(\beta)+\lambda\|\beta\|_{1}.\] (13)

The optimality condition of Eq. (13) writes \(-\nabla L(\beta_{\lambda}^{\star})\in\lambda\partial\|\beta_{\lambda}^{\star}\|_ {1}\). Now notice the similarity with Eq. (12): the two would be equivalent with \(\lambda=1/t\) if the integration on the left hand side of Eq. (12) did not average over the whole trajectory but only on the final iterate, in which case \(-\int_{0}^{t}\nabla L(\tilde{\beta}_{t}^{\circ})\mathrm{d}s=-t\cdot\nabla L( \tilde{\beta}_{t}^{\circ})\). Though the difference is small, the trajectories of our limiting trajectory \(\tilde{\beta}^{\circ}\) and the lasso path \((\beta_{\lambda}^{\star})_{\lambda}\) are quite different: one has jumps, whereas the other is continuous. Nonetheless, the construction of Algorithm 1 shares many similarities with that of the Least Angle Regression (LARS) algorithm [19] (originally named the Homotopy algorithm [39]) which is used to compute the Lasso path. A notable difference however is the fact that each step of our algorithm depends on the whole trajectory through the vector \(s\), whereas the LARS algorithm can be started from any point on the path.

### Outputs of the algorithm under a RIP and gap assumption on the data.

Unlike previous results on incremental learning, complex behaviours can occur when the feature matrix is ill designed: several coordinates can activate and deactivate at the same time (see Appendix A for various cases). However, if the feature matrix satisfies the \(2r\)-restricted isometry property (RIP) [14] and there exists an \(r\)-sparse solution \(\beta^{\star}\), the visited saddles can be easily approximated using Algorithm 1. We provide the precise characterisation below.

**Sparse regression with RIP and gap assumption.** _(RIP) Assume that there exists an \(r\)-sparse vector \(\beta^{\star}\) such that \(y_{i}=\langle x_{i},\beta^{\star}\rangle\). Furthermore we assume that the feature matrix \(X\in\mathbb{R}^{n,d}\) satisfies the \(2r\)-restricted isometry property with constant \(\tilde{\varepsilon}<\sqrt{2}-1<1/2\): i.e. for all submatrix \(X_{s}\) where we extract any \(s\leq 2r\) columns of \(X\), the matrix \(X_{s}^{\star}X_{s}/n\) of size \(s\times s\) has all its eigenvalues in the interval \([1-\tilde{\varepsilon},1+\tilde{\varepsilon}]\)._ (_Gap assumption) Furthermore we assume that the \(r\)-sparse vector \(\beta^{\star}\) has coordinates which have a "sufficient gap'_. _W.l.o.g we write \(\beta^{\star}=(\beta_{1}^{\star},\ldots,\beta_{r}^{\star},0,\ldots,0)\) with \(|\beta_{1}^{\star}|\geq\ldots\geq|\beta_{r}^{\star}|>0\) and we define \(\lambda\coloneqq\min_{i\in[r]}(|\beta_{i}^{\star}|-|\beta_{i+1}^{\star}|)\geq 0\) which corresponds to the smallest gap between the entries of \(|\beta^{\star}|\). We assume that \(5\tilde{\varepsilon}\|\beta^{\star}\|_{2}<\lambda/2\) and we let \(\varepsilon\coloneqq 5\tilde{\varepsilon}\)._

A classic result from compressed sensing (see Candes (13, Theorem 1.2)) is that the \(2r\)-restricted isometry property with constant \(\sqrt{2}-1\) ensures that the minimum \(\ell_{0}\)-minimisation problem has a unique \(r\)-sparse solution which is \(\beta^{\star}\). This means that Algorithm 1 will have \(\beta^{\star}\) as final output and the following proposition shows that we can precisely characterise each of its outputs when the data satisfies the previous assumptions.

**Proposition 2**.: _Under the restricted isometry property and the gap assumption stated right above, Algorithm 1 terminates in \(r\)-loops and outputs:_

\[\beta_{1} =(\beta_{1}[1],0,\ldots,0) \text{with}\quad\beta_{1}[1]\in[\beta_{1}^{\star}-\varepsilon\| \beta^{\star}\|,\beta_{2}^{\star}+\varepsilon\|\beta^{\star}\|]\] \[\beta_{2} =(\beta_{2}[1],\beta_{2}[2],0,\ldots,0) \text{with}\quad\left\{\begin{array}{l}\beta_{2}[1]\in[\beta_{1 }^{\star}-\varepsilon\|\beta^{\star}\|,\beta_{1}^{\star}+\varepsilon\|\beta^{ \star}\|]\\ \beta_{2}[2]\in[\beta_{2}^{\star}-\varepsilon\|\beta^{\star}\|,\beta_{2}^{ \star}+\varepsilon\|\beta^{\star}\|]\end{array}\right.\] \[\vdots\] \[\beta_{r-1} =(\beta_{r-1}[1],\ldots,\beta_{r-1}[r-1],0,\ldots,0) \text{with}\ \ \beta_{r-1}[i]\in[\beta_{i}^{\star}-\varepsilon\|\beta^{\star}\|,\beta_{i}^{ \star}+\varepsilon\|\beta^{\star}\|\;]\] \[\beta_{r} =\beta^{\star}=(\beta_{1}^{\star},\ldots,\beta_{r}^{\star},0, \ldots,0),\]

_at times \(t_{1},\ldots,t_{r}\) such that \(t_{i}\in\left[\frac{1}{|\beta_{i}^{\star}|+\varepsilon\|\beta^{\star}\|},\, \frac{1}{|\beta_{i}^{\star}|-\varepsilon\|\beta^{\star}\|}\right]\) and where \(\|\cdot\|\) denotes the \(\ell_{2}\) norm._

Informally, this means that the algorithm terminates in exactly \(r\) loops and outputs jump times and saddles roughly equal to \(t_{i}=1/|\beta_{i}^{\star}|\) and \(\beta_{i}=(\beta_{1}^{\star},\cdots,\beta_{i}^{\star},0,\ldots,0)\). Therefore, in simple settings, the support of the sparse vector is learnt a coordinate at a time, without any deactivations. We refer to Appendix D.2 for the proof.

## 4 Convergence of the iterates towards the process defined by Algorithm 1

We are now fully equipped to state our main result which formalises the convergence of the accelerated iterates towards the limiting process \(\tilde{\beta}^{\circ}\) which we built in the previous section.

**Theorem 2**.: _Let the saddles \((\beta_{0}=\mathbf{0},\beta_{1},\ldots,\beta_{p-1},\beta_{p}=\beta_{t_{1}}^{ \star})\) and jump times \((t_{0}=0,t_{1},\ldots,t_{p})\) be the outputs of Algorithm 1 and let \((\tilde{\beta}_{t}^{\circ})_{t}\) be the piecewise constant process defined as follows:_

\[\text{(Saddles)}\qquad\tilde{\beta}_{t}^{\circ}=\beta_{k}\qquad\qquad\text{ for }t\in(t_{k},t_{k+1})\text{ and }0\leq k\leq p,\ \ t_{p+1}=+\infty.\]

_The accelerated flow \((\tilde{\beta}_{t}^{\circ})_{t}\) defined in Eq. (9) uniformly converges towards the limiting process \((\tilde{\beta}_{t}^{\circ})_{t}\) on any compact subset of \(\mathbb{R}_{\geq 0}\backslash\{t_{1},\ldots,t_{p}\}\)._

**Convergence result.** We recall that from a technical point of view, showing the existence of a limiting process \(\lim_{\alpha\to 0}\tilde{\beta}^{\alpha}\) is the toughest part. Theorem 2 provides this existence as well as the uniform convergence of the accelerated iterates towards \(\tilde{\beta}^{\circ}\) over all closed intervals of \(\mathbb{R}\) which do not contain the jump times. We highlight that this is the strongest type of convergence we could expect and a uniform convergence over all intervals of the form \([0,T]\) is impossible given that the limiting process \(\tilde{\beta}^{\circ}\) is discontinuous. In Proposition 3, we give an even stronger result by showing a graph convergence of the iterates which takes into account the path followed between the jumps. We also point out that we can easily show the same type of convergence for the accelerated weights \(\tilde{w}_{t}^{\alpha}\coloneqq w_{\tilde{t}^{\alpha}(t)}^{\alpha}\). Indeed, using the bijective mapping which links the weights \(w_{t}\) and the predictors \(\beta_{t}\) (see Lemma 1 in Appendix C), we immediately get that the accelerated weights \((\tilde{u}^{\alpha},\tilde{v}^{\alpha})\) uniformly converge towards the limiting process \((\sqrt{|\tilde{\beta}^{\circ}|},\operatorname{sign}(\tilde{\beta}^{\circ}) \sqrt{|\tilde{\beta}^{\circ}|})\) on any compact subset of \(\mathbb{R}_{\geq 0}\backslash\{t_{1},\ldots,t_{p}\}\).

**Estimates for the non-accelerated iterates \(\beta^{\alpha}_{t}\).** We point out that our result provides no speed of convergence of \(\tilde{\beta}^{\alpha}\) towards \(\tilde{\beta}^{\circ}\). We believe that a non-asymptotic result is challenging and leave it as future work. Note that we experimentally notice that the convergence rate quickly degrades after each saddle. Nonetheless, we can still write for the non-accelerated iterates that \(\beta^{\alpha}_{t}=\tilde{\beta}^{\alpha}_{t/\ln(1/\alpha)}\sim\tilde{\beta}^{ \circ}_{t/\ln(1/\alpha)}\) as \(\alpha\to 0\). Hence, for \(\alpha\) small enough the iterates \(\beta^{\alpha}_{t}\) are roughly equal to \(0\) until time \(t_{1}\cdot\ln(1/\alpha)\) and the minimum \(\ell_{1}\)-norm interpolator is reached at time \(t_{p}\cdot\ln(1/\alpha)\). **Such a precise estimate of the global convergence time is rather remarkable** and goes beyond classical Lyapunov analyses which only leads to \(L(\beta^{\alpha}_{t})\lesssim\ln(1/\alpha)/t\) (see Proposition 4 in Appendix C).

**Natural extensions of our setting.** More general initialisations can easily be dealt with. For instance, initialisations of the form \(u_{t=0}=\alpha\mathbf{u_{0}}\in\mathbb{R}^{d}\) lead to the exact same result as it is shown in [50] (Discussion after Theorem 1) that the associated mirror still converges to the \(\ell_{1}\)-norm. Initialisations of the form \([u_{t=0}]_{i}=\alpha^{k_{i}}\), where \(k_{i}>0\), lead to the associated potential converging towards a weighted \(\ell_{1}\)-norm and one should modify Algorithm 1 by accordingly weighting \(\nabla L(\beta)\) in the algorithm. Also, deeper linear architectures of the form \(\beta_{w}=w^{D}_{+}-w^{D}_{-}\) as in [50] do not change our result as the associated mirror still converges towards the \(\ell_{1}\)-norm. Though we only consider the square loss in the paper, we believe that all our results should hold for any loss of the type \(L(\beta)=\sum_{i=1}^{n}\ell(y_{i},\langle x_{i},\beta\rangle)\) where for all \(y\in\mathbb{R}\), \(\ell(y,\cdot)\) is strictly convex with a unique minimiser at \(y\). In fact, the only property which cannot directly be adapted from our results is showing the uniform boundedness of the iterates (see discussion before Proposition 5 in Appendix C).

High level sketch of proof of \(\tilde{\beta}^{\alpha}\to\tilde{\beta}^{\circ}\) which leverages an arc-length parametrisation

In this section, we give the high level ideas concerning the proof of the convergence \(\tilde{\beta}^{\alpha}\to\tilde{\beta}^{\circ}\) given in Theorem 2. A full and detailed proof can be found in Appendix E. The main difficulty stems from the non-continuity of the limit process \(\tilde{\beta}^{\circ}\). To circumvent this difficulty, a clever trick which we borrow to [18; 36] is to "slow-down" time when the jumps occur by considering **an arc-length parametrisation of the path**. We consider the \(\mathbb{R}_{\geq 0}\) arclength bijection \(\tau^{\alpha}\) and leverage it to define the 'appropriately slowed down' iterates \(\hat{\beta}^{\alpha}_{\tau}\) as:

\[\hat{\beta}^{\alpha}_{\tau}=\tilde{\beta}^{\alpha}_{\tilde{t}^{\alpha}(\tau) }\qquad\text{ where }\qquad\hat{t}^{\alpha}_{\tau}=(\tau^{\alpha})^{-1}(\tau)\ \ \text{ and }\ \ \tau^{\alpha}(t)=t+\int_{0}^{t}\lVert\hat{\tilde{\beta}}^{\alpha}_{s}\rVert \mathrm{d}s.\]

This time reparametrisation has the fortunate but crucial property of leading to \(\dot{\hat{t}}^{\alpha}(\tau)+\lVert\hat{\tilde{\beta}}^{\alpha}_{\tau}\rVert =1\) by a simple chain rule, which means that the speed of \((\hat{\beta}^{\alpha}_{\tau})_{\tau}\)**is uniformly upperbounded by \(1\) independently of \(\alpha\)**. This behaviour is in stark contrast with the process \((\hat{\beta}^{\alpha}_{t})_{t}\) which has a speed which explodes at the jumps. This change of time now allows us to use Arzela-Ascoli's theorem to extract a subsequence which uniformly converges to a limiting process which we denote \(\hat{\beta}\). Importantly, \(\hat{\beta}\) enables to keep track of the path followed between the jumps as we show that its trajectory has two regimes:

**Saddles:**\(\hat{\beta}_{\tau}=\beta_{k}\qquad\)**Connections:**\(\dot{\hat{\beta}}_{\tau}=-\dfrac{\lvert\hat{\beta}_{\tau}\rvert\odot\nabla L(\hat{ \beta}_{\tau})}{\lVert\hat{\beta}_{\tau}\rvert\odot\nabla L(\hat{\beta}_{ \tau})\rVert}\).

The process \(\hat{\beta}\) is illustrated on the right: the red curves correspond to the paths which the iterates follow during the jumps. These paths are called _heteroclinic orbits_ in the dynamical systems literature [31; 3]. To prove Theorem 2, we can map back the convergence of \(\hat{\beta}^{\alpha}\) to show that of \(\tilde{\beta}^{\alpha}\). Moreover from the convergence \(\hat{\beta}^{\alpha}\to\hat{\beta}\) we get a more complete picture of the limiting dynamics of \(\hat{\beta}^{\alpha}\) as it naturally implies the convergence of the graph of the iterates \((\hat{\beta}^{\alpha}_{t})_{t}\) converges towards that of \((\hat{\beta}_{\tau})_{\tau}\). The graph convergence result is formalised in this last proposition.

**Proposition 3**.: _For all \(T>t_{p}\), the graph of the iterates \((\tilde{\beta}^{\alpha}_{t})_{t\leq T}\) converges to that of \((\hat{\beta}_{\tau})_{\tau}:\)_

\[\mathrm{dist}(\{\hat{\beta}^{\alpha}_{t}\}_{t\leq T},\{\hat{\beta}_{\tau}\}_{ \tau\geq 0})\ \underset{\alpha\to 0}{\longrightarrow}\ 0\qquad\text{(Hausdorff distance)}\]Further discussion and conclusion

**Link between incremental learning and saddle-to-saddle dynamics.** The incremental learning phenomenon and the saddle-to-saddle process are often complementary facets of the same idea and refer to the same phenomenon. Indeed for gradient flows \(\mathrm{d}w_{t}=-\nabla F(w_{t})\mathrm{d}t\), fixed points of the dynamics correspond to critical points of the loss. Stages with little progress in learning and minimal movement of the iterates necessarily correspond to the iterates being in the vicinity of a critical point of the loss. It turns out that in many settings (linear networks [30], matrix sensing [8; 41]), critical points are necessarily saddle points of the loss (if not global minima) and that they have a very particular structure (high sparsity, low rank, etc.). We finally note that an alternative approach to realising saddle-to-saddle dynamics is through the perturbation of the gradient flow by a vanishing noise as studied in [6].

**Characterisation of the visited saddles.** A common belief is that the saddle-to-saddle trajectory can be found by successively computing the direction of most negative curvature of the loss (i.e. the eigenvector corresponding to the most negative eigenvalue) and following this direction until reaching the next saddle [26]. However this statement cannot be accurate as it is inconsistent with our algorithm in our setting. In fact, it can be shown that this algorithm would match the orthogonal matching pursuit (OMP) algorithm [42; 17] which does not necessarily lead to the minimum \(\ell_{1}\)-norm interpolator. In [7], which is the closest to our work and the first to prove convergence of the iterates towards a piece-wise constant process, the successive saddles are entirely characterised and connected to the Lasso regularisation path in the underparameterised setting. Recently, [9] extended the diagonal linear network setting to diagonal parametrisations of the form \(f_{u\odot v}\), but at the cost of stronger assumptions on the trajectory.

**Adaptive Inverse Scale Space Method.** Following the submission of our paper, we were informed that Algorithm 1 had already been proposed and analysed in the compressed sensing literature. Indeed it exactly corresponds to the Adaptive Inverse Scale Space Method (aISS) proposed in [11]. The motivations behind its study are extremely different from ours and originate from the study of Bregman iteration [12; 40; 52] which is an efficient method for solving \(\ell_{1}\) related minimisation problems. The so-called inverse scale space flow which corresponds to Eq. (12) in our paper can be seen as the continuous version of Bregman iteration. As in our paper, [11] show that this equation can be solved through an iterative algorithm. We refer to [51; Section 2] for further details. However we did not find any results in this literature concerning the uniqueness of the constrained minimisation problem due to Assumption 1, nor on the maximum number of iterations, the behaviour under RIP assumptions and the maximum number of active coordinates.

**Subdifferential equations and rate-independent systems.** As in Eq. (12), subdifferential inclusions of the form \(\nabla L(\beta_{t})\in\frac{\mathrm{d}}{\mathrm{d}t}\partial h(\beta_{t})\) for non-differential functions \(h\) have been studied by Attouch et al. [4] but for strongly convex functions \(h\). In this case, the solutions are continuous and do not exhibit jumps. On another hand, [18; 36; 37] consider so-called _rate-independent systems_ of the form \(\partial_{g}E(t,q_{t})\in\partial h(\dot{q}_{t})\) for \(1\)-homogeneous _dissipation_ potentials \(h\). Examples of such systems are ubiquitous in mechanics and appear in problems related to friction, crack propagation, elastoplasticity and ferromagnetism to name a few [35, Ch. 6 for a survey]. As in our case, the main difficulty with such processes is the possible appearance of jumps when the energy \(E\) is non-convex.

Conclusion.Our study examines the behaviour of gradient flow with vanishing initialisation over diagonal linear networks. We prove that it leads to the flow jumping from a saddle point of the loss to another. Our analysis characterises each visited saddle point as well as the jumping times through an algorithm which is reminiscent of the LARS method used in the Lasso framework. There are several avenues for further exploration. The most compelling one is the extension of these techniques to broader contexts for which the implicit bias of gradient flow has not yet fully been understood.

Acknowledgments.S.P. would like to thank Loucas Pillaud-Vivien for introducing him to this beautiful topic and for the many insightful discussions. S.P. also thanks Quentin Rebjock for the many helpful discussions and Johan S. Wind for reaching out and providing the reference of [11]. The authors also thank Jerome Bolte for the discussions concerning subdifferential equations, Aris Daniilidis for the reference of [32], as well as Aditya Varre and Mathieu Even for proofreading the paper.

## References

* [1] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 2552-2623. PMLR, 2023.
* [2] Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. _Advances in Neural Information Processing Systems_, 32, 2019.
* [3] Peter Ashwin and Michael Field. Heteroclinic networks in coupled cell systems. _Arch. Ration. Mech. Anal._, 148(2):107-143, 1999.
* [4] H. Attouch, J. Bolte, P. Redont, and M. Teboulle. Singular Riemannian barrier methods and gradient-projection dynamical systems for constrained optimization. _Optimization_, 53(5-6):435-454, 2004.
* [5] Shahar Azulay, Edward Moroshko, Mor Shpigel Nacson, Blake E Woodworth, Nathan Srebro, Amir Globerson, and Daniel Soudry. On the implicit bias of initialization shape: Beyond infinitesimal mirror descent. In _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 468-477. PMLR, 18-24 Jul 2021.
* [6] Yuri Bakhtin. Noisy heteroclinic networks. _Probab. Theory Related Fields_, 150(1-2):1-42, 2011.
* [7] Raphael Berthier. Incremental learning in diagonal linear networks. _arXiv preprint arXiv:2208.14673_, 2022.
* [8] Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank matrix recovery. In _Advances in Neural Information Processing Systems_, volume 29, 2016.
* [9] Enric Boix-Adsera, Etai Littwin, Emmanuel Abbe, Samy Bengio, and Joshua Susskind. Transformers learn through gradual rank increase. _arXiv preprint arXiv:2306.07042_, 2023.
* [10] Etienne Boursier, Loucas Pillaud-Vivien, and Nicolas Flammarion. Gradient flow dynamics of shallow reLU networks for square loss and orthogonal inputs. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [11] Martin Burger, Michael Moller, Martin Benning, and Stanley Osher. An adaptive inverse scale space method for compressed sensing. _Mathematics of Computation_, 82(281):269-299, 2013.
* [12] Jian-Feng Cai, Stanley Osher, and Zuowei Shen. Split bregman methods and frame based image restoration. _Multiscale modeling & simulation_, 8(2):337-369, 2010.
* [13] Emmanuel J Candes. The restricted isometry property and its implications for compressed sensing. _Comptes rendus mathematique_, 346(9-10):589-592, 2008.
* [14] E. Candes, J. Romberg, and T. Tao. Stable signal recovery from incomplete and inaccurate measurements. _Communications on Pure and Applied Mathematics_, 59(8):1207-1223, 2006.
* [15] Scott Shaobing Chen, David L Donoho, and Michael A Saunders. Atomic decomposition by basis pursuit. _SIAM review_, 43(1):129-159, 2001.
* [16] Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss. In _Proceedings of Thirty Third Conference on Learning Theory_, volume 125 of _Proceedings of Machine Learning Research_, pages 1305-1338. PMLR, 09-12 Jul 2020.
* [17] Geoff Davis, Stephane Mallat, and Marco Avellaneda. Adaptive greedy approximations. _Constructive approximation_, 13:57-98, 1997.
* [18] Messoud A. Efendiev and Alexander Mielke. On the rate-independent limit of systems with dry friction and small viscosity. _J. Convex Anal._, 13(1):151-167, 2006.

* Efron et al. [2004] Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. Least angle regression. 2004.
* Even et al. [2023] Mathieu Even, Scott Pesme, Suriya Gunasekar, and Nicolas Flammarion. (s)gd over diagonal linear networks: Implicit regularisation, large stepsizes and edge of stability. _arXiv preprint arXiv:2302.08982_, 2023.
* Ghai et al. [2020] Udaya Ghai, Elad Hazan, and Yoram Singer. Exponentiated gradient meets gradient descent. In Aryeh Kontorovich and Gergely Neu, editors, _Proceedings of the 31st International Conference on Algorithmic Learning Theory_, volume 117 of _Proceedings of Machine Learning Research_, pages 386-407. PMLR, 08 Feb-11 Feb 2020.
* Gidel et al. [2019] Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient dynamics in linear neural networks. _Advances in Neural Information Processing Systems_, 32, 2019.
* Gissin et al. [2020] Daniel Gissin, Shai Shalev-Shwartz, and Amit Daniely. The implicit bias of depth: How incremental learning drives generalization. In _International Conference on Learning Representations_, 2020.
* Gunasekar et al. [2017] Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. In _Advances in Neural Information Processing Systems_, volume 30, 2017.
* HaoChen et al. [2021] Jeff Z HaoChen, Colin Wei, Jason Lee, and Tengyu Ma. Shape matters: Understanding the implicit bias of the noise covariance. In _Conference on Learning Theory_, pages 2315-2357. PMLR, 2021.
* Jacot et al. [2021] Arthur Jacot, Francois Ged, Berlin Simsek, Clement Hongler, and Franck Gabriel. Saddle-to-saddle dynamics in deep linear networks: Small initialization training, symmetry, and sparsity. _arXiv preprint arXiv:2106.15933_, 2021.
* Jiang et al. [2022] Liwei Jiang, Yudong Chen, and Lijun Ding. Algorithmic regularization in model-free over-parametrized asymmetric matrix factorization. _arXiv preprint arXiv:2203.02839_, 2022.
* Jin et al. [2023] Jikai Jin, Zhiyuan Li, Kaifeng Lyu, Simon S Du, and Jason D Lee. Understanding incremental learning of gradient descent: A fine-grained analysis of matrix sensing. _arXiv preprint arXiv:2301.11500_, 2023.
* Kalimeris et al. [2019] Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz Barak, and Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity. _Advances in neural information processing systems_, 32, 2019.
* Kawaguchi [2016] Kenji Kawaguchi. Deep learning without poor local minima. In _Advances in Neural Information Processing Systems_, volume 29, 2016.
* Krupa [1997] M. Krupa. Robust heteroclinic cycles. _J. Nonlinear Sci._, 7(2):129-176, 1997.
* Kurdyka [1998] Krzysztof Kurdyka. On gradients of functions definable in o-minimal structures. _Ann. Inst. Fourier (Grenoble)_, 48(3):769-783, 1998.
* Li et al. [2021] Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning. In _International Conference on Learning Representations_, 2021.
* Mairal and Yu [2012] Julien Mairal and Bin Yu. Complexity analysis of the lasso regularization path. _arXiv preprint arXiv:1205.0079_, 2012.
* Mielke [2005] Alexander Mielke. Evolution of rate-independent systems. _Evolutionary equations_, 2:461-559, 2005.
* Mielke et al. [2009] Alexander Mielke, Riccarda Rossi, and Giuseppe Savare. Modeling solutions with jumps for rate-independent systems on metric spaces. _Discrete Contin. Dyn. Syst._, 25(2):585-615, 2009.

* [37] Alexander Mielke, Riccarda Rossi, and Giuseppe Savare. Variational convergence of gradient flows and rate-independent evolutions in metric spaces. _Milan Journal of Mathematics_, 80:381-410, 2012.
* [38] Behnam Neyshabur. Implicit regularization in deep learning. _arXiv preprint arXiv:1709.01953_, 2017.
* [39] Michael R Osborne, Brett Presnell, and Berwin A Turlach. A new approach to variable selection in least squares problems. _IMA journal of numerical analysis_, 20(3):389-403, 2000.
* [40] Stanley Osher, Martin Burger, Donald Goldfarb, Jinjun Xu, and Wotao Yin. An iterative regularization method for total variation-based image restoration. _Multiscale Modeling & Simulation_, 4(2):460-489, 2005.
* [41] Dohyung Park, Anastasios Kyrillidis, Constantine Carmanis, and Sujay Sanghavi. Non-square matrix sensing without spurious local minima via the Burer-Monteiro approach. In _Proceedings of the 20th International Conference on Artificial Intelligence and Statistics_, volume 54 of _Proceedings of Machine Learning Research_, pages 65-74. PMLR, 20-22 Apr 2017.
* [42] Yagyensh Chandra Pati, Ramin Rezaiifar, and Perinkulam Sambamurthy Krishnaprasad. Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition. In _Proceedings of 27th Asilomar conference on signals, systems and computers_, pages 40-44. IEEE, 1993.
* [43] Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. Implicit bias of sgd for diagonal linear networks: a provable benefit of stochasticity. In _Advances in Neural Information Processing Systems_, 2021.
* [44] Noam Razin, Asaf Maman, and Nadav Cohen. Implicit regularization in tensor factorization. In _International Conference on Machine Learning_, pages 8913-8924. PMLR, 2021.
* [45] Andrew M Saxe, James L McClelland, and Surya Ganguli. A mathematical theory of semantic development in deep neural networks. _Proceedings of the National Academy of Sciences_, 116(23):11537-11546, 2019.
* [46] Robert Tibshirani. Regression shrinkage and selection via the lasso. _Journal of the Royal Statistical Society: Series B (Methodological)_, 58(1):267-288, 1996.
* [47] Ryan J. Tibshirani. The lasso problem and uniqueness. _Electron. J. Stat._, 7:1456-1490, 2013.
* [48] Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini. Implicit regularization for optimal sparse recovery. _Advances in Neural Information Processing Systems_, 32, 2019.
* [49] Johan S Wind, Vegard Antun, and Anders C Hansen. Implicit regularization in ai meets generalized hardness of approximation in optimization-sharp results for diagonal linear networks. _arXiv preprint arXiv:2307.07410_, 2023.
* [50] Blake Woodworth, Suriya Gunasekar, Jason D. Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In _Proceedings of Thirty Third Conference on Learning Theory_, volume 125 of _Proceedings of Machine Learning Research_, pages 3635-3673. PMLR, 09-12 Jul 2020.
* [51] Yi Yang, Michael Moller, and Stanley Osher. A dual split bregman method for fast l1 minimization. _Mathematics of computation_, 82(284):2061-2085, 2013.
* [52] W Yin, S Osher, D Goldfarb, and J Darbon. Bregman iterative algorithms for l1-minimization with applications to compressed sensing: Siam journal on imaging sciences, 1, 143-168. _LIST OF FIGURES_, 2008.
* [53] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In _International Conference on Learning Representations_, 2017.

Organisation of the Appendix.
1. In Appendix A, we give the experimental setup and provide additional experiments.
2. In Appendix B, we prove Proposition 1 and provide additional comments concerning the unicity of the minimisation problem which appears in the proposition.
3. In Appendix C, we provide some general results on the flow.
4. In Appendix D, we prove Proposition 2 and give standalone properties of Algorithm 1.
5. In Appendix E, we explain in more detail the arc-length parametrisation explained in the main text as well as prove Theorem 2 and Proposition 3.
6. In Appendix F, we provide technical lemmas which are useful to prove the main results.

[MISSING_PAGE_EMPTY:15]

Proof of Proposition 1

**Proposition 1**.: _All the critical points \(w_{c}\) of \(F\) which are not global minima, i.e., \(\nabla F(w_{c})=\mathbf{0}\) and \(F(w_{c})>\min_{w}F(w)\), are necessarily saddle points (i.e., not local extrema). They map to parameters \(\beta_{c}=u_{c}\odot v_{c}\) which satisfy \(|\beta_{c}|\odot\nabla L(\beta_{c})=\mathbf{0}\) and:_

\[\beta_{c}\in\operatorname*{arg\,min}_{\beta[i]=0\text{ for }i\notin\operatorname*{ supp}(\beta_{c})}\,L(\beta)\] (4)

_where \(\operatorname*{supp}(\beta_{c})=\{i\in[d],\beta_{c}[i]\neq 0\}\) corresponds to the support of \(\beta_{c}\)._

Proof.: **Non-existence of maxima / non-global minima.** This is a simpler version of results which appear in [30], for the sake of completeness we provide here a simple proof adapted to our setting. The intuition follows the fact that if there existed a local maximum / non-global minimum for \(F\) then this would translate to the existence of a local maximum / non-global minimum for the convex loss \(L\), which is absurd.

Assume that there exists a local maximum \(w^{\star}=(u^{\star},v^{\star})\), i.e. assume that there exists \(\varepsilon>0\) such that for all \(w=(u,v)\) such that \(\|w-w^{\star}\|_{2}^{2}\leq\varepsilon\), \(F(w)\leq F(w^{\star})\). We show that this would imply that \(\beta^{\star}=u^{\star}\odot v^{\star}\) is a local maximum of \(L\), which is absurd.

The mapping \(g:(u,v)\mapsto(u\odot v,\sqrt{(u^{2}-v^{2})/2})\) from \(\mathbb{R}_{\geq 0}^{d}\times\mathbb{R}^{d}\to\mathbb{R}^{d}\times\mathbb{R}_{ \geq 0}^{d}\) is a bijection with inverse

\[g^{-1}:(\beta,\alpha)\mapsto(\sqrt{\alpha^{2}+\sqrt{\beta^{2}+\alpha^{4}}}, \operatorname*{sign}(\beta)\odot\sqrt{-\alpha^{2}+\sqrt{\beta^{2}+\alpha^{4}} }).\] (14)

Also notice that \(F(g^{-1}(\beta,\alpha))=L(\beta)\) for all \(\beta\) and \(\alpha\). Now let \(\tilde{\varepsilon}>0\) and let \(\beta\in\mathbb{R}^{d}\) such that \(\|\beta-\beta^{\star}\|_{2}^{2}\leq\tilde{\varepsilon}\), then for \((u,v)=g^{-1}(\beta,\alpha_{\star})\) where \(\alpha_{\star}=\sqrt{((u^{\star})^{2}-(v^{\star})^{2})/2}\) we have that:

\[\|(u,v)-(u^{\star},v^{\star})\|_{2}^{2} =2\Big{\|}\Big{(}\sqrt{\alpha_{\star}^{4}+\beta^{2}}-\sqrt{ \alpha_{\star}^{4}+{\beta^{\star}}^{2}}\,\,\Big{)}^{2}\Big{\|}_{1}\] \[\leq 2\|\beta^{2}-{\beta^{\star}}^{2}\|_{1}\] \[=2\|(\beta-\beta^{\star})^{2}+2(\beta-\beta^{\star})\beta^{\star} \|_{1}\] \[\leq 2\|(\beta-\beta^{\star})^{2}\|_{1}{+}2\|\beta^{\star}\|_{ \infty}\|\beta-\beta^{\star}\|_{1}\] \[\leq 2(1+\sqrt{d}\|\beta^{\star}\|_{\infty})\tilde{\varepsilon}\] \[\leq\varepsilon\]

where the last inequality is for \(\tilde{\varepsilon}\) small enough. This means that \(L(\beta)=F(w)\leq F(w^{\star})=L(\beta^{\star})\) and \(\beta^{\star}\) is a local maximum of \(L\), which is absurd.

The exact same proof holds to show that there are no local minima of \(F\) which are not global minima.

**Critical points.** The gradient of the loss function \(F\) writes:

\[\nabla_{w}F(w)=\begin{pmatrix}\nabla_{u}F(w)\\ \nabla_{v}F(w)\end{pmatrix}=\begin{pmatrix}\nabla L(\beta)\odot v\\ \nabla L(\beta)\odot u\end{pmatrix}\in\mathbb{R}^{2d}.\]

Therefore \(\nabla F(w_{c})=\mathbf{0}\in\mathbb{R}^{2d}\) implies that \(\nabla L(\beta_{c})\odot\beta_{c}=\mathbf{0}\in\mathbb{R}^{d}\). Now consider such a \(\beta_{c}\) and let \(\operatorname*{supp}(\beta_{c})=\{i\in[d]\text{ such that }\beta_{c}(i)\neq 0\}\) denote the support of \(\beta_{c}\). Since \([\nabla L(\beta_{c})]_{i}=0\) for \(i\notin\operatorname*{supp}(\beta_{c})\), we can therefore write that

\[\beta_{c}\in\operatorname*{arg\,min}_{\beta_{i}=0\text{ for }i\notin\operatorname*{ supp}(\beta_{c})}L(\beta).\]

Furthermore we point out that since \(\operatorname*{supp}(\beta_{c})\subset[d]\), there are at most \(2^{d}\) distinct sets \(\operatorname*{supp}(\beta_{c})\), and therefore at most \(2^{d}\) values \(F(w_{c})=L(\beta_{c})\), where \(w_{c}\) is a critical point of \(F\). 

**Additional comment concerning the uniqueness of \(\operatorname*{arg\,min}_{\beta_{i}=0,i\notin\operatorname*{supp}(\beta_{c})}L( \beta)\).**We point out that the constrained minimisation problem (4) does not necessarily have a unique solution, even when \(\beta_{c}\) is not a global solution. Though not required for any of our results, for the sake of completeness, we show here that under an additional mild assumption on the data, we can ensure that the minimisation problem (4) which appears in Proposition 1 has a unique minimum when \(L(\beta_{c})>0\). Under this additional assumption, there is therefore a finite number of saddles \(\beta_{c}\). Recall that we let \(X\in\mathbb{R}^{n\times d}\) be the feature matrix and \((\tilde{x}_{1},\ldots,\tilde{x}_{d})\) be its columns. Now assume _temporarily_ that the following assumption holds.

**Assumption 2** (Assumption used just in this short section).: _Any subset of \((\tilde{x}_{1},\ldots,\tilde{x}_{d})\) of size smaller than \(\min(n,d)\) is linearly independent._

One can easily check that this assumption holds with probability \(1\) as soon as the data is drawn from a continuous probability distribution, similarly to [47, Lemma 4]). In the following, for a subset \(\xi=\{i_{1},\ldots,i_{k}\}\subset[d]\), we write \(X_{\xi}=(\tilde{x}_{i_{1}},\ldots,\tilde{x}_{i_{k}})\in\mathbb{R}^{n\times k}\) (we extract the columns from \(X\)). For a vector \(\beta\in\mathbb{R}^{d}\) we write \(\beta[\xi]=(\beta_{i_{1}},\ldots,\beta_{i_{k}})\) and \(\beta[\xi^{C}]=(\beta_{i})_{i\notin\xi}\). We distinguish two different settings:

* Underparametrised setting (\(n\geq d\)) : in this case, for any \(\xi=\{i_{1},\ldots,i_{k}\}\subset[d]\), then \(\beta^{\star}\coloneqq\underset{\beta_{i}=0,i\notin\xi}{\text{argmin}}\ L(\beta)\) is unique. Indeed we simply set the gradient to 0 and notice that due to Assumption 2, there exists a unique solution, indeed it is \(\beta^{\star}\) such that \(\beta^{\star}[\xi]=(X_{\xi}^{\top}X_{\xi})^{-1}X_{\xi}^{\top}y\) and \(\beta^{\star}[\xi^{C}]=0\).
* Overparametrised setting (\(d>n\)) : **Global solutions:**\(\arg\min_{\beta\in\mathbb{R}^{d}}L(\beta)\) is an affine space spanned by the orthogonal of \((x_{1},\ldots,x_{n})\) in \(\mathbb{R}^{d}\). Since \(\operatorname{span}(\tilde{x}_{1},\ldots,\tilde{x}_{d})=\mathbb{R}^{n}\) from Assumption 2, any \(\beta^{\star}\in\arg\min_{\beta\in\mathbb{R}^{d}}L(\beta)\) satisfies \(X\beta^{\star}=y\) and \(L(\beta^{\star})=0\). **"Saddle points":** now let \(\beta_{c}\in\mathbb{R}^{d}\) be such that we can write \(\beta_{c}\in\arg\min_{\beta_{i}=0,i\notin\operatorname{supp}(\beta_{c})}L(\beta)\) and assume that \(L(\beta_{c})>0\) (i.e., not a global solution), then: (1) \(\beta_{c}\) has at most \(n\) non-zero entries, indeed if it were not the case, then \(y\) would necessarily belong to \(\operatorname{span}(\tilde{x}_{i})_{i\in\operatorname{supp}(\beta_{c})}\) due to the assumption on the data, and this would lead to \(L(\beta_{c})=0\), (2) therefore, similar to the underparametrised case, \(\arg\min_{\beta_{i}=0,i\notin\operatorname{supp}(\beta_{c})}L(\beta)\) is unique, equal to \(\beta_{c}\), and we have that \(\beta_{c}[\xi]=(X_{\xi}^{\top}X_{\xi})^{-1}X_{\xi}^{\top}y\) and \(\beta_{c}[\xi^{C}]=0\) where \(\xi=\operatorname{supp}(\beta_{c})\).

Thus, in both the underparametrised and overparametrised settings, the minimisation problem (4) appearing in Proposition 1 has a unique minimum when \(L(\beta_{c})>0\) and Assumption 2 holds.

General results on the iterates

In the following lemma we recall a few results concerning the gradient flow Eq. (5):

\[\mathrm{d}w_{t}=-\nabla F(w_{t})\mathrm{d}t\,,\] (15)

where \(F\) is defined in Eq. (3) as:

\[F(w)\coloneqq L(u\odot v)=\frac{1}{2n}\sum_{i=1}^{n}(\langle u\odot v,x_{i} \rangle-y_{i})^{2}\,.\]

**Lemma 1**.: _For an initialisation \(u_{0}=\sqrt{2}\alpha\), \(v_{0}=\mathbf{0}\), the flow \(w_{t}^{\alpha}=(u_{t}^{\alpha},v_{t}^{\alpha})\) from Eq. (15) is such that the quantity \((u_{t}^{\alpha})^{2}-(v_{t}^{\alpha})^{2}\) is constant and equal to \(2\alpha^{2}\mathbf{1}\). Furthermore \(u_{t}^{\alpha}>|v_{t}^{\alpha}|\geq 0\) and therefore from the bijection Eq. (14) we have that:_

\[u_{t}^{\alpha}=\sqrt{\alpha^{2}+\sqrt{(\beta_{t}^{\alpha})^{2}+\alpha^{4}}}, \quad v_{t}^{\alpha}=\mathrm{sign}(\beta_{t}^{\alpha})\odot\sqrt{-\alpha^{2}+ \sqrt{(\beta_{t}^{\alpha})^{2}+\alpha^{4}}}.\]

Proof.: From the expression of \(\nabla F(w)\), notice that the derivative of \((u_{t}^{\alpha})^{2}-(v_{t}^{\alpha})^{2}\) is equal to \(\mathbf{0}\) and therefore equal to its initial value.

Since \((u_{t}^{\alpha})^{2}-(v_{t}^{\alpha})^{2}=(u_{t}^{\alpha}+v_{t}^{\alpha})(u_{ t}^{\alpha}-v_{t}^{\alpha})>0\), by continuity we get that \(u_{t}^{\alpha}+v_{t}^{\alpha}>0\) and \(u_{t}^{\alpha}-v_{t}^{\alpha}>0\) and therefore \(u_{t}^{\alpha}>|v_{t}^{\alpha}|\). 

In this section we consider the accelerated iterates Eq. (9) which follow:

\[\mathrm{d}\nabla\tilde{\phi}_{\alpha}(\tilde{\beta}_{t}^{\alpha})=-\nabla L( \tilde{\beta}_{t}^{\alpha})\mathrm{d}t,\qquad\mathrm{where}\qquad\tilde{\phi} _{\alpha}\coloneqq\frac{1}{\ln(1/\alpha)}\cdot\tilde{\phi}_{\alpha}\] (16)

with \(\tilde{\beta}_{t=0}=\mathbf{0}\) and where \(\phi_{\alpha}\) is defined Eq. (7).

**Proposition 4**.: _For all \(\alpha>0\) and minimum \(\beta^{\star}\in\arg\min_{\beta}L(\beta)\), the loss values \(L(\tilde{\beta}_{t}^{\alpha})\) and the Bregman divergence \(\mathrm{D}_{\tilde{\phi}_{\alpha}}(\beta^{\star},\tilde{\beta}_{t}^{\alpha})\) are decreasing. Moreover_

\[L(\tilde{\beta}_{t}^{\alpha})-L(\beta^{\star})\leq\frac{\tilde {\phi}_{\alpha}(\beta^{\star})}{2t},\] (17) \[L\Big{(}\frac{1}{t}\int_{0}^{t}\tilde{\beta}_{s}^{\alpha}\mathrm{ d}s\Big{)}-L(\beta^{\star})\leq\frac{\tilde{\phi}_{\alpha}(\beta^{\star})}{2t}.\] (18)

Proof.: The loss is decreasing since: \(\frac{\mathrm{d}}{\mathrm{d}t}L(\tilde{\beta}_{t}^{\alpha})=\nabla L(\tilde{ \beta}_{t}^{\alpha})^{\top}\dot{\beta}_{t}^{\alpha}=-\dot{\tilde{\beta}}_{t}^ {\alpha^{\top}}\nabla^{2}\tilde{\phi}_{\alpha}(\tilde{\beta}_{t}^{\alpha}) \dot{\tilde{\beta}}_{t}^{\alpha}\leq 0\).

\(\frac{\mathrm{d}}{\mathrm{d}t}\mathrm{D}_{\tilde{\phi}_{\alpha}}(\beta^{ \star},\tilde{\beta}_{t}^{\alpha})=-\nabla L(\tilde{\beta}_{t}^{\alpha})^{ \top}(\tilde{\beta}_{t}^{\alpha}-\beta^{\star})=-2(L(\tilde{\beta}_{t}^{ \alpha})-L(\beta^{\star}))\) (since \(L\) is the quadratic loss), therefore the Bregman distance is decreasing. We can also integrate this last equality from \(0\) to \(t\), and divide by \(-2t\):

\[\frac{1}{t}\int_{0}^{t}L(\tilde{\beta}_{s}^{\alpha})\mathrm{d}s -L(\beta^{\star}) =\frac{\mathrm{D}_{\tilde{\phi}_{\alpha}}(\beta^{\star},\beta_{0}^ {\alpha}=\mathbf{0})-\mathrm{D}_{\tilde{\phi}_{\alpha}}(\beta^{\star},\beta_{t }^{\alpha})}{2t}\] \[\leq\frac{\tilde{\phi}_{\alpha}(\beta^{\star})}{2t}.\]

Since the loss is decreasing we get that \(L(\tilde{\beta}_{t}^{\alpha})-L(\beta^{\star})\leq\frac{\tilde{\phi}_{\alpha}( \beta^{\star})}{2t}\) and from the convexity of \(L\) we get that \(L\Big{(}\frac{1}{t}\int_{0}^{t}\tilde{\beta}_{s}^{\alpha}\mathrm{d}s\Big{)}-L( \beta^{\star})\leq\frac{\tilde{\phi}_{\alpha}(\beta^{\star})}{2t}\). 

In the following proposition, we show that for \(\alpha\) small enough, the iterates are bounded independently of \(\alpha\). Note that this result unfortunately only holds for the quadratic loss, we expect it to hold for other convex losses of the type \(L(\beta)=\frac{1}{n}\sum_{i}\ell(y_{i},\langle x_{i},\beta\rangle)\) where \(\ell(y,\cdot)\) is strictly convex has a unique root at \(y\) but we don't know how to show it. Also note that bounding the accelerated iterates \(\tilde{\beta}^{\alpha}\) is equivalent to bounding the iterates \(\beta^{\alpha}\) since \(\tilde{\beta}_{t}^{\alpha}=\beta_{\ln(1/\alpha)t}^{\alpha}\).

**Proposition 5**.: _For \(\alpha<\alpha_{0}\), where \(\alpha_{0}\) depends on \(\beta^{\star}_{\ell_{1}}\), the iterates \(\tilde{\beta}^{\alpha}_{t}\) are bounded independently of \(\alpha\):_

\[\|\tilde{\beta}^{\alpha}_{t}\|_{\infty}\leq 3\|\beta^{\star}_{\ell_{1}}\|_{1}+1\]

Proof.: From Eq. (16), integrating and using that \(L\) is the quadratic loss, we get:

\[\nabla\tilde{\phi}_{\alpha}(\tilde{\beta}^{\alpha}_{t})=\frac{t}{n}X^{\top}(y -X\tilde{\beta}^{\alpha}_{t})=-\frac{t}{n}X^{\top}X(\tilde{\beta}^{\alpha}_{t} -\beta^{\star}),\]

where we recall that \(X\in\mathbb{R}^{n\times d}\) is the input data represented as a matrix and where we denote the averaged iterate by \(\tilde{\beta}^{\alpha}_{t}=\frac{1}{t}\int_{0}^{t}\tilde{\beta}^{\alpha}_{s} \mathrm{d}s\). Thus we get

\[\nabla\tilde{\phi}_{\alpha}(\tilde{\beta}^{\alpha}_{t})^{\top}(\tilde{\beta}^ {\alpha}_{t}-\beta^{\star})=-\frac{t}{n}(\tilde{\beta}^{\alpha}_{t}-\beta^{ \star})^{\top}X^{\top}X(\tilde{\beta}^{\alpha}_{t}-\beta^{\star}).\] (19)

By convexity of \(\tilde{\phi}_{\alpha}\) we have \(\tilde{\phi}_{\alpha}(\beta^{\alpha}_{t})-\tilde{\phi}_{\alpha}(\beta^{ \star})\leq\nabla\tilde{\phi}_{\alpha}(\beta^{\alpha}_{t})^{\top}(\beta^{ \alpha}_{t}-\beta^{\star})\). By the Cauchy-Schwarz inequality, we also have \((\tilde{\beta}^{\alpha}_{t}-\beta^{\star})^{\top}X^{\top}X(\beta^{\alpha}_{t} -\beta^{\star})\leq\|X(\beta^{\alpha}_{t}-\beta^{\star})\|\|X(\tilde{\beta}^{ \alpha}_{t}-\beta^{\star})\|\). Using Proposition 4: \(\|X(\beta^{\alpha}_{t}-\beta^{\star})\|^{2}\leq n\tilde{\phi}_{\alpha}(\beta^{ \star})/t\) and \(\|X(\tilde{\beta}^{\alpha}_{t}-\beta^{\star})\|^{2}\leq n\tilde{\phi}_{\alpha} (\beta^{\star})/t\) we can further bound the right hand side of Eq. (19) as

\[-\frac{t}{n}(\tilde{\beta}^{\alpha}_{t}-\beta^{\star})^{\top}X^{\top}X(\beta^ {\alpha}_{t}-\beta^{\star})\leq\tilde{\phi}_{\alpha}(\beta^{\star}).\]

Thus it yields

\[\tilde{\phi}_{\alpha}(\beta^{\alpha}_{t})-\tilde{\phi}_{\alpha}(\beta^{\star })\leq\tilde{\phi}_{\alpha}(\beta^{\star}).\]

From [50] (proof of Lemma 1 in the appendix) we get that for

\[\alpha<\min\left\{1,\sqrt{\|\beta\|_{1}},(2\|\beta\|_{1})^{-1}\right\}\]

then:

\[\tilde{\phi}_{\alpha}(\beta)\leq\frac{3}{2}\|\beta\|_{1},\]

and for all \(\alpha<\exp(-d/2)\):

\[\tilde{\phi}_{\alpha}(\beta) \geq\|\beta\|_{1}-\frac{d}{\ln(1/\alpha^{2})}\] \[\geq\|\beta\|_{1}-1,\]

which finally leads for

\[\alpha<\alpha_{0}\coloneqq\min\left\{1,\sqrt{\|\beta^{\star}_{\ell_{1}}\|_{1} },\left(2\|\beta^{\star}_{\ell_{1}}\|_{1}\right)^{-1},\exp(-d/2)\right\}\]

to the result. 

The following proposition shows that we can bound the path length of the flow \(\tilde{\beta}^{\alpha}\) independently of \(\alpha\). Keep in mind that the path length of \(\tilde{\beta}^{\alpha}\) is equivalent to that of \(\beta^{\alpha}\) as the first is just an acceleration of the second: \(\tilde{\beta}^{\alpha}_{t}=\beta^{\alpha}_{\ln(1/\alpha)t}\).

**Proposition 6**.: _For \(\alpha<\alpha_{0}\) where \(\alpha_{0}\) is the same as in Proposition 5, the path length of the iterates \((\beta^{\alpha}_{t})_{t\geq 0}\) is bounded independently of \(\alpha>0\):_

\[\int_{0}^{+\infty}\|\hat{\beta}^{\alpha}_{t}\|\mathrm{d}t<C,\]

_where \(C\) does not depend on \(\alpha\). Hence the path length of the accelerated flow \(\tilde{\beta}^{\alpha}\) is also bounded independently of \(\alpha\)._Proof.: Having shown that the iterates \(\beta_{t}^{\alpha}\) are bounded independently of \(\alpha\), it also implies that the iterates \(w_{t}=(u_{t},v_{t})\) are bounded following Lemma 1. Since the loss \(w\mapsto F(w)\) is a multivariate polynomial function, it is a semialgebraic function and we can consequently apply the result of Kurdyka [32, Theorem 2] which grants that

\[\int_{0}^{+\infty}\lVert\dot{w}_{t}\rVert\mathrm{d}t<C,\]

where the constant \(C\) only depends on the loss and on the bound on the iterates. We further use that \(\dot{\beta}=\dot{u}\odot v+u\odot\dot{v}\) and \(\lVert\dot{u}\odot v+u\odot\dot{v}\rVert\leq C_{1}(\lVert\dot{u}\rVert+\lVert \dot{v}\rVert)\) using that \(u\) and \(v\) are bounded and \(\lVert\dot{u}\rVert+\lVert\dot{v}\rVert\leq C_{2}\lVert\dot{w}\rVert\) using the equivalence of norms. Therefore \(\int_{0}^{+\infty}\lVert\dot{\beta}_{t}^{\alpha}\rVert\mathrm{d}t<C\) for some \(C\) which is independent of the initialisation scale \(\alpha\).

Standalone properties of Algorithm 1

### "Well-definedness" of Algorithm 1 and upperbound on its number of loops

Notice that this proposition highlights the fact that Algorithm 1 is on its own an algorithm of interest for finding the minimum \(\ell_{1}\)-norm solution in an overparametrised regression setting. We point out that the provided upperbound on the number of iterations is very crude and could certainly be improved.

**Proposition 7**.: _Algorithm 1 is well defined: at each iteration (i) the attribution of \(\Delta\) is well defined as \(\Delta<+\infty\), (ii) the constrained minimisation problem has a unique solution and the attribution of the value of \(\beta\) is therefore well-founded. Furthermore, along the loops: the iterates \(\beta\) have at most \(n\) non-zero coordinates, the loss is strictly decreasing and the algorithm terminates in at most \(\min{(2^{d},\sum_{k=0}^{n}{d\choose k})}\) steps by outputting the minimum \(\ell_{1}\)-norm solution \(\beta_{\ell_{1}}^{*}\coloneqq\underset{\beta\in\,arg\,min\,L}{\text{arg min}}\|\beta\|_{1}\)._

Proof.: In the following, for the matrix \(X\) and for a subset \(I=\{i_{1},\ldots,i_{k}\}\subset[d]\), we write \(X_{I}=(\tilde{x}_{i_{1}},\ldots,\tilde{x}_{i_{k}})\in\mathbb{R}^{n\times k}\) (we extract the columns from \(X\)). For a vector \(\beta\in\mathbb{R}^{d}\) we write \(\beta_{I}=(\beta_{i_{1}},\ldots,\beta_{i_{k}})\).

**(1) The constrained minimisation problem has a unique solution:** we follow the proof of [47, Lemma 2]. Following the notations in Algorithm 1, we define \(I=\{i\in[d],\;|s_{i}|=1\}\) and we point out that after \(k\) loops of the algorithm, the value of \(s\) is equal to \(s=-(\Delta_{1}\nabla L(\beta_{0})+\cdots+\Delta_{k}\nabla L(\beta_{k-1}))\in \operatorname{span}(x_{1},\ldots,x_{n})\). We can therefore write \(s=X^{\top}r\) for some \(r\in\mathbb{R}^{n}\).

Now assume that \(\ker(X_{I})\neq\{0\}\). Then, for some \(i\in I\), we have \(\tilde{x}_{i}=\sum_{j\in I\setminus\{i\}}c_{j}\tilde{x}_{j}\) where \(c_{j}\in\mathbb{R}\). Without loss of generality, we can assume that \(I\setminus\{i\}\) has at most \(n\) elements. Indeed, we can otherwise always find \(n\) elements \(\tilde{I}\subset I\setminus\{i\}\) such that \(\tilde{x}_{i}=\sum_{j\in I}c_{j}\tilde{x}_{j}\). Rewriting the previous equality, we get

\[s_{i}\tilde{x}_{i}=\sum_{j\in I\setminus\{i\}}(s_{i}s_{j}c_{j})(s_{j}\tilde{ x}_{j}).\] (20)

Now by definitions of the set \(I\) and of \(r\), we have that \(\langle\tilde{x}_{j},r\rangle=s_{j}\in\{+1,-1\}\) for any \(j\in I\). Taking the inner product of Eq. (20) with \(r\), we obtain that \(1=\sum_{j\in I\setminus\{i\}}(s_{i}s_{j}c_{j})\). Consequently, we have shown that if \(\ker(X_{I})\neq\{0\}\), then we necessarily have for some \(i\in I\),

\[s_{i}\tilde{x}_{i}=\sum_{j\in I\setminus\{i\}}a_{j}(s_{j}\tilde{x}_{j}),\]

with \(\sum_{j\in I\setminus\{i\}}a_{j}=1\), which means that \(s_{i}\tilde{x}_{i}\) lies in the affine space generated by \((s_{j}\tilde{x}_{j})_{j\in I\setminus\{i\}}\). This fact is however impossible due to Assumption 1 (recall that without loss of generality we have that \(I\setminus\{i\}\) has at most \(n\) elements, and trivially less that \(d\) elements). **Therefore \(X_{I}\) is full rank**, and \(\operatorname{Card}(I)\leq n\). Now notice that the constrained minimisation problem corresponds to \(\arg\min_{\begin{subarray}{c}\beta_{i}\geq 0,i\in I_{+}\\ \beta_{i}\leq 0,i\in I_{-}\end{subarray}}\|y-X_{I}\beta_{I}\|_{2}^{2}\). Since \(X_{I}\) is full rank, this restricted loss is strictly convex and the constrained minimisation problem **has a unique minimum.**

**(2) \(\Delta<+\infty\):** Notice that the optimality conditions of

\[\beta=\underset{\begin{subarray}{c}\beta_{i}\geq 0,i\in I_{+}\\ \beta_{i}\leq 0,i\in I_{-}\\ \beta_{i}=0,i\notin I\end{subarray}}{\text{arg min}}\|y-X_{I}\beta_{I}\|_{2}^{2},\]

are (i) \(\beta\) satisfies the constraints, (ii) if \(i\in I_{+}\) (resp \(i\in I_{-}\)) then \([-\nabla L(\beta)]_{i}\leq 0\) (resp \([-\nabla L(\beta)]_{i}\geq 0\)) and (iii) if \(\beta_{i}\neq 0\) then \([\nabla L(\beta)]_{i}=0\). One can notice that condition (ii) ensures that at each iteration, for \(\delta\leq\Delta_{k}\), \(s_{k-1}-\delta\nabla L(\beta_{k-1})\in[-1,1]\) coordinate wise. Also, if \(L(\beta_{k-1})\neq\mathbf{0}\), then a coordinate of the vector \(|s_{k-1}-\delta\nabla L(\beta_{k-1})|\) must necessarily hit \(1\), this value of \(\delta\) corresponds to \(\Delta_{k}\).

**(3) The loss is strictly decreasing:** Let \(I_{k-1,\pm}\) and \(I_{k,\pm}\) be the equicorrelation sets defined in the algorithm at step \(k-1\) and \(k\), and \(\beta_{k-1}\) and \(\beta_{k}\) the solutions of the constrained minimisation problems. Also, let \(i_{k}\) be the newly added coordinate which breaks the constraint at step \(k\) (which we assume to be unique for simplicity). Without loss of generality, assume that \(s_{k}(i_{k})=+1\). Since the sets\(I_{k-1,+}\setminus(I_{k,+}\setminus\{i_{k}\})\) and \(I_{k-1,-}\setminus I_{k,-}\) are (if not empty) only composed of indexes of coordinates of \(\beta_{k-1}\) which are equal to \(0\), one can notice that \(\beta_{k-1}\) also satisfies the new constraints at step \(k\). Therefore \(L(\beta_{k})\leq L(\beta_{k-1})\). Now since \([-\nabla L(\beta_{k-1})|_{i_{k}}>0\), from the strict convexity of the restricted loss on \(I_{k}\), this means that \(\beta_{k}(i_{k})>0\) (which also means that newly activated coordinate \(i_{k}\)**must activate**), and therefore \(\beta_{k-1}\neq\beta_{k}\) and \(L(\beta_{k})<L(\beta_{k-1})\).

**(4) The algorithm terminates in at most \(\min\left(2^{d},\sum_{k=0}^{n}{d\choose k}\right)\)steps:** Recall that we showed in part (1) of the proof that at each iteration \(k\) of the algorithm, \(I_{k}\) as at most \(\min(n,d)\) elements. Since \(\operatorname{supp}(\beta_{k})\subset I_{k}\), we have that \(\beta_{k}\) has at most \(\min(n,d)\) non-zero elements, also recall that we always have \(\beta_{k}=\arg\min_{\beta_{i}=0,i\notin\operatorname{supp}(\beta_{k})}L(\beta)\) (we have have paucity of this minimisation problem following part (1) of the proof). There are hence at most

\[\sum_{k=0}^{\min(n,d)}{d\choose k}=\min\left(2^{d},\sum_{k=0}^{n}{d\choose k}\right)\]

such minimisation problems. The loss being strictly decreasing, the algorithm cannot output the same solution \(\beta\) at two different loops, and the algorithm must terminate in at most \(\min\left(2^{d},\sum_{k=0}^{n}{d\choose k}\right)\) iterations by outputting a vector \(\beta^{\star}\) such that \(\nabla L(\beta^{\star})=0\), _i.e._\(\beta^{\star}\in\arg\min L(\beta)\).

**(5) The algorithm outputs the minimum \(\ell_{1}\)-norm solution.** Let \(\beta^{\star}\) be the output of the algorithm after \(p\) iterations. Notice that by the definition of the successive sets \(I_{k,\pm}\) and of the constraints on the minimisation problem, we have that at each iteration \(s_{k}\in\partial\|\beta_{k}\|_{1}\). Therefore \(s_{p}\in\partial\|\beta^{\star}\|_{1}\). Also, recall from part (1) of the proof that \(s_{p}\in\operatorname{span}(x_{1},\ldots,x_{n})\) which means that there exists \(r\in\mathbb{R}^{n}\) such that \(s_{p}=X^{\top}r\). Putting the two together we get that \(X^{\top}r\in\partial\|\beta^{\star}\|_{1}\), this condition along with the fact that \(L(\beta^{\star})=\min L(\beta)\) are exactly the KKT conditions of \(\underset{\beta\in\arg\min L}{\text{min}}\ \|\beta\|_{1}\). 

To put our upperbound on the number of iterations into perspective, the worst-case number of iterations for the LARS algorithm is \((3^{d}+1)/2\)[34]. Hence Algorithm 1 has fewer iterations in the worst-case setting. Whether an exponential dependency in the dimension is inevitable for Algorithm 1 is unknown and we leave this as future work.

However, when the number of samples is much smaller than the dimension we lose the exponential dependency. Indeed, for \(\varepsilon\coloneqq n/d\leq 1/2\), we have the upperbound \(\sum_{k=0}^{n}{d\choose k}\leq 2^{H(\varepsilon)d}\) where \(H(\varepsilon)=-\varepsilon\log_{2}(\varepsilon)-(1-\varepsilon)\log_{2}(1-\varepsilon)\) is the binary entropy. Since for \(\varepsilon\leq 1/2\), \(H(\varepsilon)\leq-2\varepsilon\log_{2}(\varepsilon)\), we get the upperbound \(\sum_{k=0}^{n}{d\choose k}\leq 2^{H(\varepsilon)d}\leq(\frac{d}{n})^{2n}\), which is much better than \(2^{d}\).

### Proof of Proposition 2

As mentioned several times, for general feature matrices \(X\) complex behaviours can occur with coordinates deactivating and changing sign several times. Here we show that for simple datasets which have a feature matrix \(X\) that satisfy the restricted isometry property (RIP) [14], we can simply determine the jump times and the saddles as a function of the sparse predictor which we seek to recover.

The non-realistic but enlightening extreme case of the RIP assumption is to consider that the feature matrix is such that \(X^{\top}X/n=I_{d}\). In this case, by letting \(\beta^{\star}\) be the unique vector such that \(y=\langle x,\beta^{\star}\rangle\) and assuming that \(\beta^{\star}=(\beta^{\star}_{1},\ldots,\beta^{\star}_{r},0,\ldots,0)\) with \(|\beta^{\star}_{1}|>\cdots>|\beta^{\star}_{r}|>0\), then the loss writes \(L(\beta)=\|\beta-\beta^{\star}\|_{2}^{2}/2\) and one can easily check that Algorithm 1 would terminate in \(r\) loops and output exactly \(t_{i}=\frac{1}{|\beta^{\star}_{i}|}\) and \(\beta_{i}=(\beta^{\star}_{1},\ldots,\beta^{\star}_{i},0,\ldots,0)\) for \(i\leq r\) (the case where several coordinates of \(\beta^{\star}\) are stricely equal can also be treated: for example if \(\beta^{\star}_{1}=\beta^{\star}_{2}\) then the first output of the algorithm is directly \(\beta_{1}=(\beta^{\star}_{1},\beta^{\star}_{2},0,\ldots,0)\)).

We now recall the more realistic RIP setting which is an adaptation of the previous observation.

**Sparse regression with RIP and gap assumption.**_(RIP) Assume that there exists an \(r\)-sparse vector \(\beta^{\star}\) such that \(y_{i}=\langle x_{i},\beta^{\star}\rangle\). Furthermore we assume that the feature matrix \(X\in\mathbb{R}^{n,d}\) satisfies the \(2r\)-restricted isometry property with constant \(\tilde{\varepsilon}<\sqrt{2}-1<1/2\): i.e. for all submatrix \(X_{s}\) where we extract any \(s\leq 2r\) columns of \(X\), the matrix \(X_{s}^{\top}X_{s}/n\) of size \(s\times s\) has all its eigenvalues in the interval \([1-\tilde{\varepsilon},1+\tilde{\varepsilon}]\). (**Gap assumption**) Furthermore we assume that the \(r\)-sparse vector _has coordinates which have a "sufficient gap'. W.l.o.g we write \(\beta^{\star}=(\beta_{1}^{\star},\ldots,\beta_{r}^{\star},0,\ldots,0)\) with \(|\beta_{1}^{\star}|\geq\ldots\geq|\beta_{r}^{\star}|>0\) and we define \(\lambda\coloneqq\min_{i\in[r]}(|\beta_{i}^{\star}|-|\beta_{i+1}^{\star}|)\geq 0\) which corresponds to the smallest gap between the entries of \(|\beta^{\star}|\). We assume that \(5\tilde{e}\|\beta^{\star}\|_{2}<\lambda/2\) and we let \(\varepsilon\coloneqq 5\tilde{e}\)._

A classic result from compressed sensing (see Candes [13, Theorem 1.2]) is that the \(2r\)-restricted isometry property with constant \(\sqrt{2}-1\) ensures that the minimum \(\ell_{0}\)-minimisation problem has a unique \(r\)-sparse solution which is \(\beta^{\star}\). Furthermore it ensures that the minimum \(\ell_{1}\)-norm solution is unique and is equal to \(\beta^{\star}\). This means that Algorithm 1 will have \(\beta^{\star}\) as a final output.

We now recall the result which characterises the outputs of Algorithm 1 when the data satisfies the previous assumptions.

**Proposition 2**.: _Under the restricted isometry property and the gap assumption stated right above, Algorithm 1 terminates in \(r\)-loops and outputs:_

\[\begin{array}{ll}\beta_{1}=(\beta_{1}[1],0,\ldots,0)&\text{with}\quad\ \ \beta_{1}[1]\in[\beta_{1}^{\star}-\varepsilon\|\beta^{\star}\|,\beta_{2}^{\star }+\varepsilon\|\beta^{\star}\|]\\ \beta_{2}=(\beta_{2}[1],\beta_{2}[2],0,\ldots,0)&\text{with}\ \ \left\{\begin{array}{ll}\beta_{2}[1]\in[\beta_{1}^{\star}- \varepsilon\|\beta^{\star}\|,\beta_{1}^{\star}+\varepsilon\|\beta^{\star}\|] \\ \beta_{2}[2]\in[\beta_{2}^{\star}-\varepsilon\|\beta^{\star}\|,\beta_{2}^{ \star}+\varepsilon\|\beta^{\star}\|]\end{array}\right.\\ \vdots&\\ \beta_{r-1}=(\beta_{r-1}[1],\ldots,\beta_{r-1}[r-1],0,\ldots,0)&\text{with}\ \ \beta_{r-1}[i]\in\ [\beta_{i}^{\star}- \varepsilon\|\beta^{\star}\|,\beta_{i}^{\star}+\varepsilon\|\beta^{\star}\| \ ]\\ \beta_{r}=\beta^{\star}=(\beta_{1}^{\star},\ldots,\beta_{r}^{\star},0,\ldots, 0),&\end{array}\]

_at times \(t_{1},\ldots,t_{r}\) such that \(t_{i}\in\left[\frac{1}{|\beta_{i}^{\star}|+\varepsilon\|\beta^{\star}\|}, \,\frac{1}{|\beta_{i}^{\star}|-\varepsilon\|\beta^{\star}\|}\right]\) and where \(\|\cdot\|\) denotes the \(\ell_{2}\) norm._

Proof.: In all the proof \(\|\cdot\|\) denotes the \(\ell_{2}\) norm \(\|\cdot\|_{2}\). For simplicity we assume that \(\beta_{i}^{\star}>0\) for all \(i\in[r]\), the proof can easily be adapted to the general case. We first define \(\xi\coloneqq X^{\top}X/n-I_{d}\). By the restricted isometry property, for any \(k\leq 2r\), we have that any \(k\times k\) square matrix extracted from \(\xi\) which we denote \(\xi_{kk}\) has its eigenvalues in \([-\tilde{\varepsilon},\tilde{\varepsilon}]\). It also means that the eigenvalues of \((I_{k}+\xi_{kk})^{-1}-I_{k}\) are in \([\frac{1}{1+\tilde{\varepsilon}}-1,\frac{1}{1-\tilde{\varepsilon}}-1]\subset[-2 \tilde{\varepsilon},2\tilde{\varepsilon}]\).

We now proceed by induction with the following induction hypothesis:

* \(\beta_{k-1}\) has its support on its \((k-1)\) first coordinates with \(|\beta_{k-1}[i]-\beta_{i}^{\star}|\leq 5\tilde{\varepsilon}\|\beta^{\star}\|\) for \(i<k\)
* \(t_{k}\in\left[\frac{1}{\beta_{k}^{\star}+5\tilde{e}\|\beta^{\star}\|},\frac{1}{ \beta_{k}^{\star}-5\tilde{e}\|\beta^{\star}\|}\right]\) and \(s_{t_{k}}[k]=1\)
* \(s_{t_{k}}[i]\in[t_{k}(\beta_{i}^{\star}-5\tilde{e}\|\beta^{\star}\|),t_{k}( \beta_{i}^{\star}+5\tilde{e}\|\beta^{\star}\|)]\subset(-1,1)\) for \(i>k\)

From the recurrence hypothesis, the output of the algorithm at step \(k\) is hence \(\beta_{k}=\arg\min L(\beta)\) under the constraint \(\beta[i]\geq 0\) for \(i\leq k\) and \(\beta[i]=0\) otherwise. We first search for the solution of the minimisation problem without the sign constraint and still (abusively) denote it \(\beta_{k}\): we will show that it turns out to satisfy the sign constraint and that it is therefore indeed \(\beta_{k}\).

In the following, for a vector \(v\), we denote by \(v[\cdot\ k]\) its \(k\) first coordinates. Setting the \(k\) first coordinates of the gradient to \(0\), we get that \([X^{\top}X(\beta_{k}-\beta^{\star})][:k]=\mathbf{0}\), which leads to \((I_{k}+\xi_{kk})\beta_{k}[:k]=\beta^{\star}[:k]+[\xi\beta^{\star}][:k]\), which gives:

\[\begin{array}{ll}\beta_{k}[:k]&=(I_{k}+\xi_{kk})^{-1}(\beta^{\star}[:k]+[\xi \beta^{\star}][:k])\\ &=\beta^{\star}[:k]+[\xi\beta^{\star}][:k]+v_{1}\end{array}\]

where from the bound on the eigenvalues of \((I_{k}+\xi_{kk})^{-1}-I_{k}\) and \(\|\xi\beta^{\star}\|\leq\tilde{\varepsilon}\|\beta^{\star}\|\):

\[\begin{array}{ll}\|v_{1}\|&\leq 2\tilde{\varepsilon}\|\beta^{\star}[:k]+[\xi \beta^{\star}][:k])\|\\ &\leq 2\tilde{\varepsilon}(\|\beta^{\star}\|+\|\xi\beta^{\star}\|)\\ &\leq 2\tilde{\varepsilon}(\|\beta^{\star}\|+\tilde{\varepsilon}\|\beta^{\star}\|) \\ &\leq 4\tilde{\varepsilon}\|\beta^{\star}\|.\end{array}\]

Therefore

\[\beta_{k}[:k]=\beta^{\star}[:k]+v_{2}\]where \(v_{2}=[\delta\beta^{\star}][:k+v_{1})\) hence \(|v_{2}|_{\infty}\leq\|v_{2}\|\leq 5\tilde{\varepsilon}\|\beta^{\star}\|\). Notice that from the definition of \(\lambda\) and the fact that \(\tilde{\varepsilon}\|\beta^{\star}\|<\lambda/2\) we have that \(\beta_{k}[:k]\geq 0\) coordinate-wise, hence verifying the sign constraint. Also note that \(\|\beta_{k}\|\leq\|\beta^{\star}\|+5\tilde{\varepsilon}\|\beta^{\star}\|\leq 4 \|\beta^{\star}\|\).

For \(t\geq t_{k}\), \(s_{t}=s_{t_{k}}-(t-t_{k})\nabla L(\beta_{k})\), and \([\nabla L(\beta_{k})][:k]=0\) therefore \(s_{t}[:k]=s_{t_{k}}[:k]\). Now for \(i>k\), \([-\nabla L(\beta_{k})]_{i}=n^{-1}[X^{\top}X(\beta^{\star}-\beta_{k})]_{i}= \beta_{i}^{\star}+[\xi(\beta_{k}-\beta^{\star})]_{i}\). Now since \((\beta_{k}-\beta^{\star})\) is \(r\)-sparse we have that:

\[\|\xi(\beta_{k}-\beta^{\star})\|_{\infty} \leq\|\xi(\beta_{k}-\beta^{\star})\|\] \[\leq\tilde{\varepsilon}\|\beta_{k}-\beta^{\star}\|\] \[\leq\tilde{\varepsilon}(\|\beta_{k}\|+\|\beta^{\star}\|)\] \[\leq 5\tilde{\varepsilon}\|\beta^{\star}\|<\lambda/2,\] (21)

Now from the fact that \(s_{t}[i]=s_{t_{k}}[i]+(t-t_{k})\beta_{i}^{\star}+(t-t_{k})[\xi(\beta_{k}- \beta^{\star})]_{i}\) and using the recurrence hypothesis: \(s_{t_{k}}[i]\in[t_{k}(\beta_{i}^{\star}-5\tilde{\varepsilon}\|\beta^{\star}) ],t_{k}(\beta_{i}^{\star}+5\tilde{\varepsilon}\|\beta^{\star}\|)]\), we get (using the bound Eq. (21)) that \(s_{t}[i]\in[t(\beta_{i}^{\star}-5\tilde{\varepsilon}\|\beta^{\star}\|),t( \beta_{i}^{\star}+5\tilde{\varepsilon}\|\beta^{\star}\|)]\). From the "separation assumption" we have that \(5\tilde{\varepsilon}\|\beta^{\star}\|<\lambda/2\) and therefore the next coordinate to activate is necessarily the \((k+1)^{th}\) at time \(t_{k+1}\) with \(s_{t_{k+1}}[k+1]=1\) and:

\[t_{k+1}\in\Big{[}\frac{1}{\beta_{k+1}^{\star}+5\tilde{\varepsilon}\|\beta^{ \star}\|},\frac{1}{\beta_{k+1}^{\star}-5\tilde{\varepsilon}\|\beta^{\star}\|} \Big{]}.\]

This proves the recursion. The algorithm cannot stop before iteration \(r\) as \(\beta^{\star}\) is the unique minimiser of \(L\) that has at most \(r\) non-zero coordinates. But it stops at iteration \(r\) as \(\beta^{\star}\) is the unique minimiser of \(L(\beta)\) under the constraints \(\beta_{i}\geq 0\) for \(i\leq r\) and \(\beta_{i}=0\) otherwise.

Proof of Theorem 2 and Proposition 3 through the arc-length parametrisation

In this section, we explain in more details the arc-length reparametrisation which circumvents the apparition of discontinuous jumps and leads to the proof of Theorem 2. The main difficulty to show the convergence stems from the non-continuity of the limit process \(\hat{\beta}^{\circ}\). Therefore we cannot expect uniform convergence of \(\hat{\beta}^{\alpha}\) towards \(\hat{\beta}\) as \(\alpha\to 0\). In addition, \(\hat{\beta}^{\circ}\) does not provide any insights into the path followed between the jumps.

**Arc-length parametrisation.** The high-level idea is to "slow-down" time when the jumps occur. To do so we follow the approach from [18, 36] and we consider an arc-length parametrisation of the path, i.e., we consider \(\tau^{\alpha}\) equal to:

\[\tau^{\alpha}(t)=t+\int_{0}^{t}\|\hat{\beta}^{\alpha}_{s}\|\mathrm{d}s.\]

In Proposition 6, we showed that the full path length \(\int_{0}^{+\infty}\|\hat{\beta}^{\alpha}_{s}\|\mathrm{d}s\) is finite and bounded independently of \(\alpha\). Therefore \(\tau^{\alpha}\) is a bijection in \(\mathbb{R}_{\geq 0}\). We can then define the following quantities:

\[\hat{t}^{\alpha}_{\tau}=(\tau^{\alpha})^{-1}(\tau)\quad\text{ and }\quad\hat{ \beta}^{\alpha}_{\tau}=\hat{\beta}^{\alpha}_{\hat{t}^{\alpha}(\tau)}.\]

By construction, a simple chain rule leads to \(\dot{\hat{t}}^{\alpha}(\tau)+\|\dot{\hat{\beta}}^{\alpha}_{\tau}\|=1\), which means that the speed of \((\dot{\beta}^{\alpha}_{\tau})_{\tau}\) is always upperbounded by \(1\), independently of \(\alpha\). This behaviour is in stark contrast with the process \((\hat{\beta}^{\alpha}_{t})_{t}\) which has a speed which explodes at the jumps. It presents a major advantage as we can now use Arzela-Ascoli's theorem to extract a converging subsequent. A simple change of variable shows that the new process satisfies the following equations:

\[-\int_{0}^{\tau}\dot{\hat{t}}^{\alpha}_{s}\nabla L(\hat{\beta}^{\alpha}_{s}) \mathrm{d}s=\nabla\tilde{\phi}_{\alpha}(\hat{\beta}^{\alpha}_{\tau})\quad \text{and}\quad\dot{\hat{t}}^{\alpha}_{\tau}+\|\dot{\hat{\beta}}^{\alpha}_{ \tau}\|=1\] (22)

started from \(\hat{\beta}^{\alpha}_{\tau}=0\) and \(\hat{t}_{0}=0\). The next proposition states the convergence of the rescaled process, up to a subsequence.

**Proposition 8**.: _Let \(T\geq 0\). For every \(\alpha>0\), let \((\hat{t}^{\alpha},\hat{\beta}^{\alpha})\) be the solution of Eq. (22). Then, there exists a subsequence \((\hat{t}^{\alpha_{k}},\hat{\beta}^{\alpha_{k}})_{k\in\mathbb{N}}\) and \((\hat{t},\hat{\beta})\) such that as \(\alpha_{k}\to 0\) :_

\[(\hat{t}^{\alpha_{k}},\hat{\beta}^{\alpha_{k}})\rightarrow(\hat{t},\hat{\beta}) \text{in }(C^{0}([0,T],\mathbb{R}\times\mathbb{R}^{d}),\|\cdot\|_{ \infty})\] (23) \[(\hat{t}^{\alpha_{k}},\hat{\beta}^{\alpha_{k}})\rightarrow(\hat{t },\hat{\beta}) \text{in }L_{1}[0,T]\] (24)

_Limiting dynamics._ The limits \((\hat{t},\hat{\beta})\) satisfy:

\[-\int_{0}^{\tau}\dot{\hat{t}}_{s}\nabla L(\hat{\beta}_{s})\mathrm{d}s\in \partial\|\hat{\beta}_{\tau}\|_{1}\quad\text{and}\quad\dot{\hat{t}}_{\tau}+\| \dot{\hat{\beta}}_{\tau}\|\leq 1\] (25)

_Heteroclinic orbit._ In addition, when \(\hat{\beta}_{\tau}\) is such that \(|\hat{\beta}_{\tau}|\odot\nabla L(\hat{\beta}_{\tau})\neq 0\), we have

\[\dot{\hat{\beta}}_{\tau}=-\frac{|\hat{\beta}_{\tau}|\odot\nabla L(\hat{\beta}_ {\tau})}{\|\dot{\beta}_{\tau}|\odot\nabla L(\hat{\beta}_{\tau})\|}\quad\text{ and }\quad\dot{\hat{t}}_{\tau}=0.\] (26)

_Furthermore, the loss strictly decreases along the heteroclinic orbits and the path length \(\int_{0}^{T}\|\dot{\hat{\beta}}_{\tau}\|\mathrm{d}\tau\) is upperbounded independently of \(T\)._

Proof.: Differentiating Eq. (22) and from the Hessian of \(\tilde{\phi}_{\alpha}\) we get:

\[\dot{\hat{\beta}}^{\alpha}_{\tau} =-\dot{\hat{t}}^{\alpha}_{\tau}(\nabla^{2}\tilde{\phi}_{\alpha}( \hat{\beta}^{\alpha}_{\tau}))^{-1}\nabla L(\hat{\beta}^{\alpha}_{\tau})\] \[=-(1-\|\dot{\hat{\beta}}^{\alpha}_{\tau}\|)(\nabla^{2}\tilde{\phi }_{\alpha}(\hat{\beta}^{\alpha}_{\tau}))^{-1}\nabla L(\hat{\beta}^{\alpha}_{ \tau}).\]Therefore taking the norm on the right hand side we obtain that

\[\|\mathring{\mathring{\beta}}^{\alpha}_{\tau}\|=\frac{\|(\nabla^{2}\tilde{\phi}_{ \alpha}(\mathring{\beta}^{\alpha}_{\tau}))^{-1}\nabla L(\mathring{\beta}^{ \alpha}_{\tau})\|}{1+\|(\nabla^{2}\tilde{\phi}_{\alpha}(\mathring{\beta}^{ \alpha}_{\tau}))^{-1}\nabla L(\mathring{\beta}^{\alpha}_{\tau})\|},\]

and therefore

\[\mathring{\mathring{\beta}}^{\alpha}_{\tau}=-\frac{(\nabla^{2}\tilde{\phi}_{ \alpha}(\mathring{\beta}^{\alpha}_{\tau}))^{-1}\nabla L(\mathring{\beta}^{ \alpha}_{\tau})}{1+\|(\nabla^{2}\tilde{\phi}_{\alpha}(\mathring{\beta}^{ \alpha}_{\tau}))^{-1}\nabla L(\mathring{\beta}^{\alpha}_{\tau})\|}.\] (27)

**Subsequence extraction.** By construction Eq. (22) we have \(\dot{\dot{t}}^{\alpha}_{\tau}+\|\mathring{\mathring{\beta}}^{\alpha}_{\tau}\|=1\), therefore the sequences \((\dot{\dot{t}}^{\alpha})_{\alpha}\), \((\mathring{\beta}^{\alpha})_{\alpha}\) as well as \((\dot{\hat{t}}^{\alpha})_{\alpha}\), \((\mathring{\beta}^{\alpha})_{\alpha}\) are uniformly bounded on \([0,T]\). The Arzela-Ascoli theorem yields that, up to a subsequence, there exists \((\dot{t},\mathring{\beta})\) such that \((\dot{t}^{\alpha_{k}},\mathring{\beta}^{\alpha_{k}})\rightarrow(\dot{t}, \mathring{\beta})\) in \((C^{0}([0,T],\mathbb{R}\times\mathbb{R}^{d})),\|\cdot\|_{\infty})\). Since \(\|\mathring{\mathring{\beta}}^{\alpha}_{\tau}\|,\|\dot{\dot{t}}^{\alpha}_{\tau}\|\leq 1\) we have, applying the Banach-Alaoglu theorem, that up to a new subsequence

\[(\dot{\dot{t}}^{\alpha_{k}},\mathring{\beta}^{\alpha_{k}})\overset{*}{ \rightharpoonup}(\dot{t},\mathring{\mathring{\beta}})\text{ in }L_{\infty}(0,T)\] (28)

and \(\|\mathring{\mathring{\beta}}_{\tau}\|\leq\text{liminf}\inf_{\alpha_{k}}\| \mathring{\mathring{\beta}}^{\alpha_{k}}_{\tau}\|\leq 1\) and thus \(\dot{\dot{t}}_{\tau}+\|\mathring{\mathring{\beta}}_{\tau}\|\leq 1\):

\[\int_{0}^{T}\|\mathring{\mathring{\beta}}_{\tau}\|\mathrm{d}\tau\leq\int_{0}^{T }\text{liminf}\|\mathring{\mathring{\beta}}^{\alpha_{k}}_{\tau}\|\mathrm{d} \tau\leq\int_{0}^{+\infty}\text{liminf}\|\mathring{\mathring{\beta}}^{\alpha_ {k}}_{\tau}\|\mathrm{d}\tau\leq\text{liminf}\inf_{\alpha_{k}}\int_{0}^{+ \infty}\|\mathring{\mathring{\beta}}^{\alpha_{k}}_{\tau}\|\mathrm{d}\tau<C,\]

where the third inequality is by Fatou's lemma. Note that since \([0,T]\) is bounded then it also implies the weak convergence in any \(L_{p}(0,T)\), \(1\leq p<\infty\). Since \((\mathring{\beta}^{\alpha})\) converges uniformly on \([0,T]\), and \(\nabla L\) is continuous, we have that \(\nabla L(\mathring{\beta}^{\alpha})\) converges uniformly to \(\nabla L(\mathring{\beta})\). Since \(\dot{\dot{t}}^{\alpha_{k}}\rightharpoonup\dot{t}\) in \(L_{1}(0,T)\), passing to the limit in the equation \(\nabla\mathring{\phi}_{\alpha}(\mathring{\beta}^{\alpha}_{\tau})=-\int_{0}^{\tau }\dot{t}^{\alpha}_{s}\nabla L(\mathring{\beta}^{\alpha}_{s})\mathrm{d}s\) leads to

\[-\int_{0}^{\tau}\dot{\dot{t}}_{s}\nabla L(\mathring{\beta}_{s})\mathrm{d}s\in \partial\|\mathring{\beta}_{\tau}\|_{1},\]

due to Lemma 2.

Recall from Eq. (27) and the definition of \(\tilde{\phi}_{\alpha}\) that:

\[\dot{\mathring{\beta}}^{\alpha}_{\tau}=-\frac{\sqrt{\mathring{\beta}^{\alpha}_ {\tau}+\alpha^{4}}\odot\nabla L(\mathring{\beta}^{\alpha}_{\tau})}{1/\mathrm{ ln}(1/\alpha)+\|\sqrt{\mathring{\beta}^{\alpha}_{\tau}+\alpha^{4}}\odot\nabla L( \mathring{\beta}^{\alpha}_{\tau})\|}.\] (29)

Hence assuming that \(\mathring{\beta}_{\tau}\) is such that \(|||\mathring{\beta}_{\tau}|\odot\nabla L(\mathring{\beta}_{\tau})\|\neq 0\), we can ensure that \(|||\mathring{\beta}_{\tau^{\prime}}|\odot\nabla L(\mathring{\beta}_{\tau^{ \prime}})\|\neq 0\) for \(\tau^{\prime}\in[\tau,\tau+\varepsilon]\) and \(\varepsilon\) small enough. We have then \(\frac{\sqrt{\mathring{\beta}^{\alpha}_{\tau^{\prime}}+\alpha^{4}}\odot\nabla L (\mathring{\beta}^{\alpha}_{\tau^{\prime}})}{1/\mathrm{ln}(1/\alpha)+\|\sqrt{ \mathring{\beta}^{\alpha}_{\tau^{\prime}}+\alpha^{4}}\odot\nabla L(\mathring{ \beta}^{\alpha}_{\tau^{\prime}})\|}\) converges uniformly toward \(-\frac{|\mathring{\beta}_{\tau^{\prime}}|\odot\nabla L(\mathring{\beta}_{\tau^{ \prime}})}{\|\mathring{\beta}_{\tau^{\prime}}|\odot\nabla L(\mathring{\beta}_{ \tau^{\prime}})\|}\) on \([\tau,\tau+\varepsilon]\). Using the dominated convergence theorem, we have \(\int_{\tau}^{\tau+\varepsilon}\frac{\sqrt{\mathring{\beta}^{\alpha}_{\tau^{ \prime}}+\alpha^{4}}\odot\nabla L(\mathring{\beta}^{\alpha}_{\tau^{\prime}})}{1 /\mathrm{log}(1/\alpha)+\|\sqrt{\mathring{\beta}^{\alpha}_{\tau^{\prime}}+ \alpha^{4}}\odot\nabla L(\mathring{\beta}^{\alpha}_{\tau^{\prime}})\|}\mathrm{d} \tau^{\prime}\rightarrow\int_{\tau}^{\tau+\varepsilon}\frac{|\mathring{\beta}_{ \tau^{\prime}}|\odot\nabla L(\mathring{\beta}_{\tau^{\prime}})}{\|\mathring{\beta}_ {\tau^{\prime}}|\odot\nabla L(\mathring{\beta}_{\tau^{\prime}})\|}\mathrm{d}\tau^{\prime}\). We therefore obtain \(\dot{\mathring{\beta}}_{\tau}=-\frac{|\mathring{\beta}_{\tau}|\odot\nabla L( \mathring{\beta}_{\tau})}{\|\mathring{\beta}_{\tau}|\odot\nabla L(\mathring{ \beta}_{\tau})\|}\) in \(L_{1}[0,T]\). Consequently \(\|\dot{\mathring{\beta}}_{\tau}\|=1\) and \(\dot{\dot{t}}_{\tau}=0\).

**Proof that the loss stricly decreases along the heteroclinic orbits.**

Assume \(\mathring{\beta}_{\tau}\) is such that \(|\mathring{\beta}_{\tau}|\odot\nabla L(\mathring{\beta}_{\tau})\neq 0\), then the flow follows

\[\dot{\mathring{\beta}}_{\tau}=-\frac{|\mathring{\beta}_{\tau}|\odot\nabla L( \mathring{\beta}_{\tau})}{\|\mathring{\beta}_{\tau}|\odot\nabla L(\mathring{ \beta}_{\tau})\|}\]

Letting \(\gamma(\tau)=\frac{1}{\|\mathring{\beta}_{\tau}|\odot\nabla L(\mathring{\beta}_{ \tau})\|}\) we get:

\[\mathrm{d}L(\mathring{\beta}_{\tau})=-\gamma(\tau)\sum_{i}|\mathring{\beta}_{ \tau}(i)|\odot[\nabla L(\mathring{\beta}_{\tau})]_{i}^{2}\mathrm{d}\tau<0,\]

because \(|\mathring{\beta}_{\tau}|\odot\nabla L(\mathring{\beta}_{\tau})^{2}\neq 0\).

Borrowing terminologies from [18], we can distinguish two regimes: when \(\dot{\hat{\beta}}_{\tau}=0\), the system is _sticked_ to the saddle point. When \(\dot{\hat{t}}_{\tau}=0\) and \(\|\dot{\hat{\beta}}_{\tau}\|=1\) the system switches to a _viscous slip_ which follows the normalised flow Eq. (26). We use the term of _heteroclinic orbit_ as in the dynamical systems literature since in the weight space \((u,v)\) it corresponds to a path with links two distinct critical points of the loss \(F\). Since \(\dot{\hat{t}}_{\tau}=0\), this regime happens instantly for the original \(t\) time scale (_i.e._ a jump occurs).

From Proposition 8, following the same reasoning as in Section 3, we can show that the rescaled process converges uniformly to a continuous saddle-to-saddle process where the saddles are linked by normalized flows.

**Theorem 3**.: _Let \(T>0\). For all subsequences defined in Proposition 8, there exist times \(0=\tau_{0}^{\prime}<\tau_{1}<\tau_{1}^{\prime}<\cdots<\tau_{p}<\tau_{p}^{\prime}< \tau_{p+1}=+\infty\) such that the the iterates \((\dot{\hat{\beta}}_{\tau}^{\alpha_{k}})_{\tau}\) converge uniformly on \([0,T]\) to the following limit trajectory :_

_("Saddle")_ \[\dot{\hat{\beta}}_{\tau} =\beta_{k} \text{for }\tau\in[\tau_{k}^{\prime},\tau_{k+1}]\text{ where }0\leq k\leq p\] _(Orbit)_ \[\dot{\hat{\beta}}_{\tau} =-\frac{|\hat{\beta}_{\tau}|\odot\nabla L(\hat{\beta}_{\tau})}{ \|\dot{\hat{\beta}}_{\tau}|\odot\nabla L(\hat{\beta}_{\tau})\|} \text{for }\tau\in[\tau_{k+1},\tau_{k+1}^{\prime}]\text{ where }0\leq k\leq p-1\]

_where the saddles \((\beta_{0}=0,\beta_{1},\ldots,\beta_{p}=\beta_{t_{1}}^{\star})\) are constructed in Algorithm 1. Also, the loss \((L(\hat{\beta}_{\tau}))_{\tau}\) is constant on the saddles and strictly decreasing on the orbits. Finally, independently of the chosen subsequence, for \(k\in[p]\) we have \(\hat{t}_{\tau_{k}}=\hat{t}_{\tau_{k}^{\prime}}=t_{k}\) where the times \((t_{k})_{k\in[p]}\) are defined through Algorithm 1._

Proof.: Some parts of the proof are slightly technical. To simplify the understanding, we make use of auxiliary lemmas which are stated in Appendix F. The overall spirit follows the intuitive ideas given in Section 3 and relies on showing that Eq. (25) can only be satisfied if the iterates visit the saddles from Algorithm 1.

We let \(\hat{s}_{\tau}\coloneqq-\int_{0}^{\tau}\dot{\hat{t}}_{\star}\nabla L(\hat{ \beta}_{s})\mathrm{d}s\), which is continuous and satisfies \(\hat{s}_{\tau}\in\partial\|\hat{\beta}_{\tau}\|_{1}\) from Eq. (25). Let \(S=\{\beta\in\mathbb{R}^{d},|\beta|\odot\nabla L(\beta)=\mathbf{0}\}\) denote the set of critical points and let \((\beta_{k},t_{k},s_{k})\) be the successive values of \((\beta,t,s)\) which appear in the loops of Algorithm 1.

**We do a proof by induction:** we start by assuming that the iterates are stuck at the saddle \(\beta_{k-1}\) at time \(\tau\geq\tau_{k-1}^{\prime}\) where \(\hat{t}_{\tau_{k-1}^{\prime}}=t_{k-1}\) and \(\hat{s}_{\tau_{k-1}^{\prime}}=s_{k-1}\) (recurrence hypothesis), we then show that they can only move at a time \(\tau_{k}\) and follow the normalised flow Eq. (26). We finally show that they must end up "stuck" at the new critical point \(\beta_{k}\), validating the recurrence hypothesis.

_Proof of the jump time \(\tau_{k}\) such that \(\hat{t}_{\tau_{k}}=t_{k}:\)_ we set ourselves at time \(\tau\geq\tau_{k-1}^{\prime}\), stuck at the saddle \(\beta_{k-1}\). Let \(\tau_{k}\coloneqq\sup\{\tau,\hat{t}_{\tau}\leq t_{k}\}\), we have that \(\tau_{k}<\infty\) from Lemma 3. Note that by continuity of \(\hat{t}_{\tau}\) it holds that \(\hat{t}_{\tau_{k}}=t_{k}\). Now notice that \(\hat{s}_{\tau}=\hat{s}_{\tau_{k-1}^{\prime}}-(\hat{t}_{\tau}-\hat{t}_{\tau_{k -1}^{\prime}})\nabla L(\beta_{k-1})=s_{k-1}-(\hat{t}_{\tau}-t_{k-1})\nabla L( \beta_{k-1})\). We argue that for any \(\varepsilon>0\), we cannot have \(\hat{\beta}_{\tau}=\beta_{k-1}\) on \((\tau_{k},\tau_{k}+\varepsilon)\). Indeed by the definition of \(\tau_{k}\) and from the algorithmic construction of time \(t_{k}\), it would lead to \(|\hat{s}_{\tau}(i)|>1\) for some coordinate \(i\in[d]\), which contradicts Eq. (25). Therefore the iterates must move at the time \(\tau_{k}\).

_Heterocline leaving \(\beta_{k-1}\) for \(\tau\in[\tau_{k},\tau_{k}^{\prime}]:\)_ contrary to before, our time rescaling enables to capture what happens during the "jump". We have shown that for any \(\varepsilon\), there exists \(\tau_{\varepsilon}\in(\tau_{k},\tau_{k}+\varepsilon)\), such that \(\hat{\beta}_{\tau_{\varepsilon}}\neq\beta_{k-1}\). From Lemma 4, since the saddles are distinct along the flow, we must have that \(\hat{\beta}_{\tau_{\varepsilon}}\notin S\) for \(\varepsilon\) small enough. The iterates therefore follow a heterocline flow leaving \(\beta_{k-1}\) with a speed of \(1\) given by Eq. (26). We now define \(\tau_{k}^{\prime}:=\inf\{\tau>\tau_{k},\exists\varepsilon_{0}>0,\forall\varepsilon \in[0,\varepsilon_{0}],\ \hat{\beta}_{\tau+\varepsilon}\in S\}\) which corresponds to the time at which the iterates reach a new critical point and stay there for at least a small time \(\varepsilon_{0}\). We have just shown that \(\tau_{k}^{\prime}>\tau_{k}\). Now from Proposition 8, the path length of \(\hat{\beta}\) is finite, and from Lemma 4 the flow visits a finite number of distinct saddles at a speed of \(1\). These two arguments put together, we get that \(\tau_{k}^{\prime}<+\infty\) and also \(\hat{\beta}_{\tau_{k}^{\prime}+\varepsilon}=\hat{\beta}_{\tau_{k}^{\prime}}\), \(\forall\varepsilon\in[0,\varepsilon_{0}]\). On another note, since \(\dot{\hat{t}}_{\tau}=0\) for \(\tau\in[\tau_{k},\tau_{k}^{\prime}]\) we have \(\hat{t}_{\tau_{k}^{\prime}}=\hat{t}_{\tau_{k}}(=t_{k})\) as well as \(\hat{s}_{\tau_{k}}=\hat{s}_{\tau_{k}^{\prime}}(=s_{k})\).

[MISSING_PAGE_FAIL:28]

### Proof of Proposition 3

We restate and prove Proposition 3 below.

**Proposition 3**.: _For all \(T>t_{p}\), the graph of the iterates \((\tilde{\beta}^{\alpha}_{t})_{t\leq T}\) converges to that of \((\hat{\beta}_{\tau})_{\tau}:\)_

\[\operatorname{dist}(\{\hat{\beta}^{\alpha}_{t}\}_{t\leq T},\{\hat{\beta}_{\tau }\}_{\tau\geq 0})\underset{\alpha\to 0}{\longrightarrow}0\qquad\text{(Hausdorff distance)}\]

Proof.: For \(\alpha\) small enough, we have that \(\hat{t}^{\alpha}_{\tau_{p}^{\prime}}\leq t_{p}+\varepsilon\leq T\)

\[\sup_{\tau\geq 0}d(\hat{\beta}_{\tau},\{\tilde{\beta}^{\alpha}_{t} \}_{t\leq T}) =\sup_{\tau\leq\tau_{p}^{\prime}}d(\hat{\beta}_{\tau},\{\tilde{ \beta}^{\alpha}_{t}\}_{t\leq T})\] \[\leq\sup_{\tau\leq\tau_{p}^{\prime}}\lVert\hat{\beta}_{\tau}- \tilde{\beta}^{\alpha}_{t_{\tau}}\rVert\] \[=\sup_{\tau\leq\tau_{p}^{\prime}}\lVert\hat{\beta}_{\tau}-\hat{ \beta}^{\alpha}_{\tau}\rVert\underset{\alpha\to 0}{\longrightarrow}0,\]

according to Theorem 3.

Similarly:

\[\sup_{t\leq T}d(\tilde{\beta}^{\alpha}_{t},\{\hat{\beta}_{\tau^{ \prime}}\}_{\tau^{\prime}}) =\sup_{\tau\leq\tau_{p}^{\alpha}}d(\hat{\beta}^{\alpha}_{\tau}, \{\hat{\beta}_{\tau^{\prime}}\}_{\tau^{\prime}})\] \[\leq\sup_{\tau\leq\tau_{p}^{\prime}}\lVert\hat{\beta}^{\alpha}_{ \tau}-\hat{\beta}_{\tau}\rVert\underset{\alpha\to 0}{\longrightarrow}0,\]

according to Theorem 3, which concludes the proof.

Technical lemmas

The following lemma describes the behaviour of \(\nabla\tilde{\phi}_{\alpha}(\beta^{\alpha})\) as \(\alpha\to 0\) in function of the subdifferential \(\partial\|\cdot\|_{1}\).

**Lemma 2**.: _Let \((\beta^{\alpha})_{\alpha>0}\) such that \(\beta^{\alpha}\underset{\alpha\to 0}{\longrightarrow}\beta\in\mathbb{R}^{d}\)._

* _if_ \(\beta_{i}>0\) _then_ \([\nabla\tilde{\phi}_{\alpha}(\beta^{\alpha})]_{i}\) _converges to_ \(1\)__
* _if_ \(\beta_{i}<0\) _then_ \([\nabla\tilde{\phi}_{\alpha}(\beta^{\alpha})]_{i}\) _converges to_ \(-1\)_._

_Moreover if we assume that \(\nabla\tilde{\phi}_{\alpha}(\beta^{\alpha})\) converges to \(\eta\in\mathbb{R}^{d}\), we have that:_

* \(\eta_{i}\in(-1,1)\Rightarrow\beta_{i}=0\)__
* \(\beta_{i}=0\Rightarrow\eta_{i}\in[-1,1]\)_._

_Overall, assuming that \((\beta^{\alpha},\nabla\tilde{\phi}_{\alpha}(\beta^{\alpha}))\underset{ \alpha\to 0}{\longrightarrow}(\beta,\eta)\), we can write:_

\[\eta\in\partial\|\beta\|_{1}.\]

Proof.: We have that

\[[\nabla\tilde{\phi}_{\alpha}(\beta^{\alpha})]_{i} =\frac{1}{2\ln(1/\alpha)}\mathrm{arcsinh}(\frac{\beta^{\alpha}_ {i}}{\alpha^{2}})\] \[=\frac{1}{2\ln(1/\alpha)}\ln\Big{(}\frac{\beta^{\alpha}_{i}}{ \alpha^{2}}+\sqrt{\frac{(\beta^{\alpha}_{i})^{2}}{\alpha^{4}}+1}\Big{)}.\]

Now assume that \(\beta^{\alpha}_{i}\rightarrow\beta_{i}>0\), then \([\nabla\tilde{\phi}_{\alpha}(\beta^{\alpha})]_{i}\to 1\), if \(\beta_{i}<0\) we conclude using that \(\mathrm{arcsinh}\) is an odd function. All the claims are simple consequences of this. 

The following lemma shows that the extracted limits \(\hat{t}\) as defined in Proposition 8 diverge to \(\infty\). This divergence is crucial as it implies that the rescaled iterates \((\hat{\beta}_{\tau})_{\tau}\) explore the whole trajectory..

**Lemma 3**.: _For any extracted limit \(\hat{t}\) as defined in Proposition 8, we have that \(\tau-C\leq\hat{t}_{\tau}\) where \(C\) is the upperbound on the length of the curves defined in proposition 6._

Proof.: Recall that

\[\tau^{\alpha}(t)=t+\int_{0}^{t}\|\dot{\beta}^{\alpha}_{s}\|\mathrm{d}s.\]

From Proposition 6, the full path length \(\int_{0}^{+\infty}\|\dot{\beta}^{\alpha}_{s}\|\mathrm{d}s\) is finite and bounded by some constant \(C\) independently of \(\alpha\). Therefore \(\tau^{\alpha}\) is a bijection in \(\mathbb{R}_{\geq 0}\) and we defined \(\hat{t}^{\alpha}_{\tau}=(\tau^{\alpha})^{-1}(\tau)\). Furthermore \(\tau^{\alpha}(t)\leq t+C\) leads to \(t\leq\hat{t}^{\alpha}(t+C)\) and therefore \(\tau-C\leq\hat{t}^{\alpha}(\tau)\) for all \(\tau\geq 0\). This inequality still holds for any converging subsequence, which proves the result. 

Under a mild additional assumption on the data (see Assumption 2), we showed after the proof of Proposition 1 in Appendix B that the number of saddles of \(F\) is finite. Without this assumption, the number of saddles is _a priori_ not finite. However the following lemma shows that along the flow of \(\hat{\beta}\) the number of saddles which can potentially be visited is indeed finite.

**Lemma 4**.: _The limiting flow \(\hat{\beta}\) as defined in Proposition 8 can only visit a finite number of critical points \(\beta\in S\coloneqq\{\beta\in\mathbb{R}^{d},\beta\odot\nabla L(\beta)=\mathbf{0}\}\) and can visit each one of them at most once._

Proof.: Let \(\tau\geq 0\), and assume that \(\hat{\beta}_{\tau}\in S\), i.e., we are at a critical point at time \(\tau\). From Proposition 1, we have that

\[\hat{\beta}_{\tau}\in\underset{\beta_{i}=0\text{ for }i\notin\mathrm{supp}( \hat{\beta}_{\tau})}{\arg\min}\;L(\beta),\] (32)Let us define the sets

\[I_{\pm}\coloneqq\{i\in\{1,\ldots,d\},\text{ s.t. }\hat{s}_{\tau}(i)=\pm 1\}\quad \text{ and }\quad I=I_{+}\cup I_{-}.\]

The set \(I\) corresponds to the coordinate of \(\hat{\beta}_{\tau}\) which "are allowed" (but not obliged) to be non-zero since from Eq.25, \(\operatorname{supp}(\hat{\beta}_{\tau})\subset I\). Now given the fact that the sub-matrix \(X_{I}=(\tilde{x}_{i})_{i\in I}\in\mathbb{R}^{n\times\operatorname{card}(I)}\) is full rank (see part (1) of the proof of Proposition7 for the explanation), the solution of the minimisation problem (32) is unique and equal to \(\hat{\beta}[\xi]=(X_{\xi}^{\top}X_{\xi})^{-1}X_{\xi}^{\top}y\) and \(\beta[\xi^{C}]=0\) where \(\xi=\operatorname{supp}(\hat{\beta}_{\tau})\). There are \(2^{d}=\operatorname{Card}(P([d]))\) (where \(P([d])\) contains all the subsets of \([d]\)) number of constraints of the form \(\{\beta_{i}=0,i\notin\mathcal{A}\}\), where \(\mathcal{A}\subset[d]\), and \(\hat{\beta}_{\tau}\) is the unique solution of one of them. \(\hat{\beta}_{\tau}\) can therefore take at most \(2^{d}\) values (very crude upperbound). There is therefore a finite number of critical points which can be reached by the flow \(\hat{\beta}\). Furthermore, from Proposition8, the loss is strictly decreasing along the heteroclinic orbits, each of these critical points can therefore be visited at most once.