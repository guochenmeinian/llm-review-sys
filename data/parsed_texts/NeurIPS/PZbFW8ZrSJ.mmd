# TabularBench: Benchmarking Adversarial Robustness for Tabular Deep Learning in Real-world Use-cases

 Thibault Simonetto

University of Luxembourg

 Luxembourg

thibault.simonetto@uni.lu

&Salah Ghamizi

LIST / RIKEN AIP

 Luxembourg

 salah.ghamizi@gmail.com

&Maxime Cordy

University of Luxembourg

 Luxembourg

 maxime.cordy@uni.lu

###### Abstract

While adversarial robustness in computer vision is a mature research field, fewer researchers have tackled the evasion attacks against tabular deep learning, and even fewer investigated robustification mechanisms and reliable defenses. We hypothesize that this lag in the research on tabular adversarial attacks is in part due to the lack of standardized benchmarks. To fill this gap, we propose TabularBench, the first comprehensive benchmark of robustness of tabular deep learning classification models. We evaluated adversarial robustness with CAA, an ensemble of gradient and search attacks which was recently demonstrated as the most effective attack against a tabular model. In addition to our open benchmark https://github.com/serval-uni-lu/tabularbench where we welcome submissions of new models and defenses, we implement 7 robustification mechanisms inspired by state-of-the-art defenses in computer vision and propose the largest benchmark of robust tabular deep-learning over 200 models across five critical scenarios in finance, healthcare, and security. We curated real datasets for each use case, augmented with hundreds of thousands of realistic synthetic inputs, and trained and assessed our models with and without data augmentations. We open-source our library that provides API access to all our pre-trained robust tabular models, and the largest datasets of real and synthetic tabular inputs. Finally, we analyze the impact of various defenses on the robustness and provide actionable insights to design new defenses and robustification mechanisms.

## 1 Introduction

Modern machine learning (ML) models have reached or surpassed human-level performance in numerous tasks, leading to their adoption in critical settings such as finance, security, and healthcare. However, concomitantly to their increasing deployment, researchers have uncovered significant vulnerabilities in generating valid adversarial examples (i.e., constraint-satisfying) where test or deployment data are manipulated to deceive the model. Most analyses of these performance drops have focused on the fields of Computer Vision and Large Language Models where extensive benchmarks for adversarial robustness are available (e.g., Croce et al. (2020) and Wang et al. (2023)).

Despite the widespread use of tabular data and the maturity of Deep Learning (DL) models for this field, the impact of evasion attacks on tabular data has not been thoroughly investigated. Although there are existing benchmarks for _in-distribution_ (ID) tabular classification (Borisov et al., 2021), and distribution shifts (Gardner et al., 2023), there is no available benchmark of adversarial robustness for deep tabular models, in particular in critical real-world settings. We summarize in Table 1 these related benchmarks.

The need for dedicated benchmarks for tabular model robustness is enhanced by the unique challenges that tabular machine learning raises compared to computer vision and NLP tasks.

One significant challenge is that tabular data exhibit _feature constraints_, which are complex relationships and interactions between features. Satisfying these feature constraints can be a non-convex or even nondifferentiable problem, making established evasion attack algorithms relying on gradient descent ineffective in generating valid adversarial examples (i.e., constraint-satisfying) (Ghamizi et al., 2020). Furthermore, attacks designed specifically for tabular data often disregard feature-type constraints (Ballet et al., 2019) or, at best, consider categorical features without accounting for feature relationships (Wang et al., 2020; Xu et al., 2023; Bao et al., 2023), and are evaluated on datasets that contain only such features. This limitation restricts their applicability to domains with heterogeneous feature types.

Moreover, tabular ML models often involve specific feature engineering, that is, "secret" and inaccessible to an attacker. For example, in credit scoring applications, the end user can alter a subset of model features, but the other features result from internal processing that adds domain knowledge before reaching the model (Ghamizi et al., 2020). This raises the need for new threat models that take into account these specificities. We summarize the unique specificities of tabular machine learning and the challenges they pose to an adversarial user in Figure 1.

Thus, the machine learning research community currently lacks not only (1) an empirical understanding of the impact of architecture and robustification mechanisms on tabular data model architectures, but also (2) a reliable and high-quality benchmark to enable such investigations. Such a benchmark for tabular adversarial attacks should feature deployable attacks and defenses that reflect as accurately as possible the robustness of models within a reasonable computational budget. A reliable benchmark should also consider recent advances in tabular deep learning architectures and data augmentation techniques, and tackle realistic attack scenarios and real-world use cases considering their domain constraints and realistic capabilities of an attacker.

To address both gaps, we propose TabularBench, the first comprehensive benchmark of robustness of tabular deep learning classification models. We evaluated adversarial robustness using _Constrained Adaptive Attack (CAA)_(Simonetto et al., 2024), a combination of gradient-based and search-based attacks that have recently been shown to be the most effective against tabular models. We take advantage of our new benchmark and uncover unique findings on deep tabular learning architectures and defenses. We focus our study on defenses based on adversarial training (AT), and draw the following insights:

**Test performance is misleading:** Given the same tasks, different architectures have similar ID performance but lead to very disparate robust performances. Even more, data augmentations that improve ID performance can hurt robust performance.

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline Benchmark & Domain & Metric & Realistic evaluation \\ \hline Tabsurvey (Borisov et al., 2021) & Tabular & ID performance & No \\ Tableshift (Gardner et al., 2023) & Tabular & OOD performance & No \\ ARES (Dong et al., 2020) & CV & Adversarial performance & No \\ Robustbench (Croce et al., 2020) & CV & Adversarial performance & Yes \\ DecodingTrust (Wang et al., 2023) & LLM & Trust (incl adversarial) & Yes \\ \hline
**OURS** & Tabular & Adversarial performance & Yes \\ \hline \hline \end{tabular}
\end{table}
Table 1: Existing related benchmarks and their differences with ours

Figure 1: The main challenges for adversarial attacks in Tabular Machine Learning: When an adversary perturbs some features (red), it may not be aware of the new features that are computed internally and added (blue), or the relationships between features (green). If the monitoring system detects a constraint violation, the input is quarantined and a rejection (1) is returned.

**Importance of domain constraints:** Disregarding domain constraints overestimates robustness and leads to selection of sub-optimal architectures and defenses when considering the domain constraints.

**Data augmentation effectiveness is task-specific.** There is no data augmentation that is optimal for both ID and robust performance across all tasks. Some simpler augmentations (like Cutmix) can outperform complex generative approaches.

Contributions.To summarize, our work makes the following key contributions:

* **Leaderboard** (https://serval-uni-lu.github.io/tabularbench): a website with a leaderboard based on _more than 200_ evaluations to track the progress and the current state of the art in adversarial robustness of tabular deep learning models for each critical setting. The goal is to clearly identify the most successful ideas in tabular architectures and robust training mechanisms to accelerate progress in the field.
* **Dataset Zoo** : a collection of real and synthetic datasets generated with and without domain-constraint satisfaction, over five critical tabular machine learning use cases.
* **Model Zoo** : a collection of the most robust models that are easy to use for any downstream application. We pre-trained these models in particular on our five downstream tasks and we expect that this collection will promote the creation of more effective adversarial attacks by simplifying the evaluation process across a broad set of _over 200_ models.
* **Analysis**: based on our trained models, we analyze how architectures, AT, and data augmentation mechanisms affect the robust performance of tabular deep learning models and provide insights on the best strategies per use case.

Figure 2: Summary of our main experiments; Y-axis: Robust Accuracy, X-axis ID accuracyBackground

Tabular data are one of the most common forms of data (Shwartz-Ziv and Armon, 2021), especially in critical applications such as medical diagnosis (Ulmer et al., 2020; Somani et al., 2021) and financial applications (Ghamizi et al., 2020; Cartella et al., 2021).

Traditional ML such as random forests and XGBoost often outperform DL on tabular data, primarily due to their robustness in handling feature heterogeneity and interdependence (Borisov et al., 2022).

To bridge the gap, researchers have proposed various improvements, from regularization mechanisms (e.g., RLN (Shavitt and Segal, 2018)) to attention layers (TabNet (Arik and Pfister, 2021)). These innovations are catching up and even outperforming shallow models in some settings, demonstrating the competitiveness of DL for Tabular Data.

The maturity of DL for ID tasks opens new perspectives for studying its performance in advanced settings, such as out-of-distribution (OOD) performance and adversarial robustness. One major work on OOD research is the Tabeshift benchmark (Gardner et al., 2023), an exhaustive evaluation of the OOD performance of a variety of DNN classifiers. There is, however, to the best of our knowledge, no similar work on adversarial robustness, while the use cases when DL models are deployed for tabular data are among the most critical settings, and many are prone to malicious users.

Our work is the first exhaustive benchmark for the critical property of adversarial robustness of DL models. Our work is timely and leverages CAA (Simonetto et al., 2024), a novel attack previously demonstrated as the most effective and efficient tabular attack in the literature in multiple classification tasks under realistic constraints. CAA combines two attacks, CAPGD and MOEVA. CAPGD is an iterative gradient attack that maximizes the error and minimizes the features' constraint violations with regularization losses and projection mechanisms. MOEVA is a genetic algorithm attack that considers the three adversarial objectives: (1) classifier's error maximization, (2) perturbation minimization, and (3) constraint violations minimization, in its fitness function.

Although CAA was only evaluated against vanilla and simple madry AT, we have implemented advanced robustification mechanisms, inspired by proven techniques from top-performing research in the Robustbench computer vision benchmark Robustbench (Croce et al., 2020). Our work is the first implementation and evaluation of state-of-the-art defense mechanisms for tabular DL models.

## 3 TabularBench: Adversarial Robustness Benchmark for Tabular Data

In Appendix A.3 we report the detailed evaluation settings such as metrics, attack parameters, and hardware. We focus below on the datasets, classifiers, and synthetic data generators.

### Tasks

We curated datasets meeting the following criteria: (1) **open source:** the datasets must be publicly available with a clear definition of the features and preprocessing, (2) **from real-world applications:** datasets that do not contain simulated data, (3) **binary classification:** datasets that support a meaningful binary classification task, and (4) **with feature relationships**: datasets that contain feature relationships and constraints, or they can be inferred directly from the definitions of features.

After an extensive review of tabular datasets, only the following five datasets match our requirements.

The **CTU**(Chernikova and Oprea, 2022) includes legitimate and botnet traffic from CTU University. Its challenge lies in the extensive number of linear domain constraints, totaling 360. **LCLD**(George, 2018) is a credit-scoring containing accepted and rejected credit requests. It has \(28\) features and \(9\)_non-linear_ constraints. The most challenging dataset of our benchmark is the **Malware** dataset prepared by Dymrishi et al. (2023). The very large number of features (\(24222\)), most of which are involved in each constraint, make this dataset challenging to attack. **URL**(Hannousse and Yahiouche, 2021) is a dataset comprising both legitimate and phishing URLs. Featuring only 14 linear domain constraints and 63 features, it represents the simplest case in our benchmark. The **WiDS**(Lee et al., 2020) includes medical data on the survival of patients admitted to the ICU, with only 31 linear domain constraints.

Our datasets include varying complexity in terms of number of features and constraints and diverse class imbalance intensity. We summarize the datasets and their relevant properties in Table 2 and provide more details in Appendix A.1.

### Architectures

We consider five state-of-the-art deep tabular architectures from the survey by Borisov et al. (2021): **TabTransformer**Huang et al. (2020) and **TabNet**Arik and Pfister (2021), are based on transformer architectures. **RLN**Shavitt and Segal (2018) uses a regularization coefficient to minimize a counterfactual loss, **STG**Yamada et al. (2020) improves feature selection using stochastic gates, while **VIME**Yoon et al. (2020) depends on self-supervised learning. We provide in Appendix A.2 the details of the architectures and the training hyperparameters. These architectures are on par with XGBoost, the top shallow machine-learning model for our applications.

### Data Augmentation

Our benchmark considers synthetic data augmentation using five state-of-the-art tabular data generators. These generators were pre-trained to learn the distribution of the training data. Then, we augmented each of our datasets 100-fold (for example, for URL dataset, we generated \(1.143.000\) synthetic examples). Appendix A.4 details the generator architectures and the training hyperparameters.

**WGAN**Arjovsky et al. (2017) is a typical generator-discriminator GAN model using Wasserstein loss. We follow the implementation of Stoian et al. (2024) and apply a MinMax transformation for continuous features and one-hot encoding for categorical to adapt this architecture for tabular data.

**TableGAN**Park et al. (2018) is an improvement over standard GAN generators for tabular data. It adds a classifier (trained to learn the labels and feature relationships) to the generator-discriminator setup to improve semantic accuracy. TableGAN uses MinMax transformation for features.

**CTGAN**Xu et al. (2019) uses a conditional generator and training-by-sampling strategy in a generator-discriminator GAN architecture to model tabular data.

**TVAE**Xu et al. (2019) is an adaptation of the Variational AutoEncoder architecture for tabular data. It uses the same data transformations as CTGAN and training with ELBO loss.

**GOGBLE**Liu et al. (2023) is a graph-based model that learns relational and functional dependencies in data using graphs and a message passing DNN, generating variables based on their neighborhood.

**Cutmix**Yun et al. (2019) In computer vision, patches are cut and pasted among training images where the labels are also mixed proportionally. We adapted the approach to tabular ML and for each pair of rows of the same class, we randomly mix half of the features to generate a new sample.

For training, each batch of real examples is augmented with a same-size random synthetic batch (without replacement). However, the evaluation only runs on real examples. In AT, we generate adversarials from half of the real examples randomly selected and half of the synthetic examples.

### Attack

To build our robustness benchmark, we leverage the Constrained Adaptive Attack (CAA) Simonetto et al. (2024) as the attack algorithm. To the best of our knowledge, CAA is the most effective and efficient tabular attack in the literature in multiple classification tasks under realistic constraints that appear in real-world applications. These constraints can be of four types: (1) mutability (whether a feature can be modified), (2) range (the minimum and maximum values a feature can take), (3) types

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline \hline Dataset & Domain & Output to flip & Total size & \# Features & \# Ctrs & Inbalance \\ \hline CTU & Botnet detection & Malicious connections & 198 128 & 756 & 360 & 99.3/0.7 \\ LCLD & Credit scoring & Reject loan request & 1 220 092 & 28 & 9 & 80/20 \\ Malware & Malware detection & Malicious software & 17 584 & 24 222 & 7 & 45.5/54.5 \\ URL & Phishing & Malicious URL & 11 430 & 63 & 14 & 50/50 \\ WIDS & ICU survival & Expected survival & 91 713 & 186 & 31 & 91.4/8.6 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Properties of the use cases of our benchmark.

(the type of the feature, e.g., categorical or numerical), and (4) relations (the dependencies between features, e.g., the sum of two features must be equal to a third feature).

CAA is a novel attack that combines two attacks, Constrained Adaptive Projected Gradient Descent (CAPGD) and Multi-Objective Evolutionary Adversarial (MOEVA) attack.

We denote by \(x\in\mathbb{R}^{d}\) an input example and by \(y\in\{1,\dots,C\}\) its correct label. Let \(h:\mathbb{R}^{d}\rightarrow\mathbb{R}^{C}\) be a classifier and \(h_{c_{k}}(x)\) the classification score that \(h\) outputs for input \(x\) to be in class \(c_{k}\).

CAPGD (Simonetto et al., 2024) is an iterative attack that generates adversarial examples by computing the following perturbed example at each iteration:

\[z^{(k+1)} =P_{\mathcal{S}}\big{(}x^{(k)}+\eta^{(k)}\big{(}\nabla\mathcal{L }^{\prime}(x^{(k)})\big{)}\] (1) \[x^{(k+1)} =R_{\Omega}\Big{(}P_{\mathcal{S}}\big{(}x^{(k)}+\alpha\cdot(z^{( k+1)}-x^{(k)})+(1-\alpha)\cdot(x^{(k)}-x^{(k-1)})\big{)}\Big{)}\]

where \(P_{\mathcal{S}}\) is the projection operator onto the set of maximum perturbation \(\delta\) denoted \(\mathcal{S}\), \(R_{\Omega}\) is a repair operator for a subset of constraints \(\Omega\), \(\eta^{(k)}\) is the step size, \(\alpha\) is the momentum parameter, and \(\mathcal{L}^{\prime}\) abbreviates the objective function to be maximized defined as:

\[\mathcal{L}^{\prime}(x)=\mathcal{L}(x,y,h,\Omega)=l(h(x),y)-\sum_{\omega_{i} \in\Omega}penalty(x,\omega_{i}).\] (2)

where \(l\) is the loss function of the model, and \(penalty\) is the penalty function for each relation constraint \(\omega_{i}\in\Omega\).

MOEVA (Simonetto et al., 2022) multi-objective evolutionary algorithm based on NSGA-III that generates adversarial examples by minimizing the following objectives:

\[minimise\;g_{1}(x) \equiv h(x)\] (3) \[minimise\;g_{2}(x) \equiv L_{p}(x-x^{0})\] (4) \[minimise\;g_{3}(x) \equiv\sum_{\omega_{i}\in\Omega}penalty(x,\omega_{i})\] (5)

where \(x^{0}\) is the original input, \(L_{p}\) is the \(L_{p}\) norm, in our case \(L_{2}\).

CAA only applies MOEVA when CAPGD fails to find an adversarial example. The attack is successful if it finds an adversarial example that is misclassified by the classifier and satisfies all the constraints.

Although to the extent of our knowledge, CAA is the best attack, we acknowledge that better attacks may be developed in the future. We provide the code for CAA in our repository, and we encourage the community to develop new attacks and evaluate them on our benchmark. Additionally, CAA is extendable in its design. Inspired by Auto-Attack, CAA is the sequential application of multiple strong attacks and complementary attacks, from fastest to slowest to find the best adversarial example. The attacks are complementary in the sense that they generate adversarial examples from different examples in the input space. This design allows for the easy integration of new attacks into the CAA framework. We encourage the development of new effective attacks and the evaluation of their complementarity with CAA.

### TabularBench API

To encourage the wide adoption of TabularBench as the go-to place for Tabular Machine Learning evaluation, we designed its API to be modular, extensible, and standardized. We split its architecture into three independent components. More details of each component are provided in Appendix C.

**A dataset Zoo** For each dataset in this study, we have collected, cleaned, and pre-processed the existing raw dataset. We implemented a novel _Constraint Parser_ where the user can write the relations in a natural human-readable format to describe the relationships between features. The processed datasets are loaded with a _Dataset factory_, then the user gets their associated meta-data and pre-defined constraints. The datasets are automatically downloaded when not found.

ds = dataset_factory.get_dataset("lcld_v2_iid") metadata = ds.get_metadata(only_x=True) constraints = ds.get_constraints() ```

**A model Zoo** Our API supports five architectures, and for each, six data augmentation techniques (as well as no data augmentation) and two training schemes (standard training and adversarial training). Hence, 70 pre-trained models for each of our five datasets are accessible. Below, we fine-tune with CAA AT and CTGAN augmentation a pre-trained Tabtramsofter with Cutmix augmentation:

``` scaler=TabScaler(num_scaler="min_max",one_hot_encode=True) scaler.fit(x,metadata["type"]) model=TabTransformer("regression",metadata,scaler-scaler,pretrained="lCLD_TabTr_Cutmix") train_dataloader=CTGANDataLoader(dataset=ds,split="train",scaler=scaler,attack="caa") model.fit(train_dataloader) ```

**A standardized benchmark** To generate our leaderboard, we offer a one-line command that loads a pre-trained model from the zoo, and reports the clean and robust accuracy of the model following our benchmark's setting (taking into consideration constraint satisfaction and L2 minimization):

``` clean_acc,robust_acc=benchmark(dataset="LCLD',model="TabTr_Cutmix",distance="L2',constraints=True) ```

## 4 Empirical Findings

In the main paper, we provide multiple figures to visualize the main insights. We only report scenarios where data augmentation and adversarial training do not lead to performance collapse. We report in Appendix B all the results and investigate the collapsed scenarios.

### Without Data Augmentations

We report the ID and robust accuracies of our architectures prior to data increase in Table 3.

**All models on malware dataset are robust without data augmentation.** AT improves adversarial accuracy for all the cases, but AT alone is not sufficient to completely robustify the models on URL and WIDS datasets. All malware classification models are completely robust with and without adversarial training; hence, we will restrict the study of improved defenses with augmentation in the following sections to the remaining datasets.

### Impact of Data Augmentations

**With data augmentation alone, ID and robust performances are not aligned.** In Figure 2 we study the impact of data augmentation on ID and robust performance, both in standard and adversarial

\begin{table}
\begin{tabular}{l l|c c c c c} \hline \hline Dataset & Accuracy & TabTr. & RLN & VIME & STG & TabNet \\ \hline \multirow{2}{*}{CTU} & ID & \(95.3/95.3\) & \(97.8/97.3\) & \(95.1/95.1\) & \(95.3/95.1\) & \(96.0/0.2\) \\  & Robust & \(95.3/95.3\) & \(94.1/97.1\) & \(40.8/94.0\) & \(95.3/95.1\) & \(0.0/0.2\) \\ \hline \multirow{2}{*}{LCLD} & ID & \(69.5/73.9\) & \(68.3/69.5\) & \(67.0/65.5\) & \(66.4/15.6\) & \(67.4/0.0\) \\  & Robust & \(7.9/70.3\) & \(0.0/63.0\) & \(2.4/10.4\) & \(53.6/12.1\) & \(0.4/0.0\) \\ \hline \multirow{2}{*}{MALWARE} & ID & \(95.0/95.0\) & \(95.0/96.0\) & \(95.0/92.0\) & \(93.0/93.0\) & \(99.0/99.0\) \\  & Robust & \(94.0/95.0\) & \(94.0/96.0\) & \(95.0/92.0\) & \(93.0/93.0\) & \(97.0/99.0\) \\ \hline \multirow{2}{*}{URL} & ID & \(93.6/93.9\) & \(94.4/95.2\) & \(92.5/93.4\) & \(93.3/94.3\) & \(93.4/99.5\) \\  & Robust & \(8.9/56.7\) & \(10.8/56.2\) & \(49.5/69.8\) & \(58.0/90.0\) & \(11.0/91.8\) \\ \hline \multirow{2}{*}{WIDS} & ID & \(75.5/77.3\) & \(77.5/78.0\) & \(72.3/72.1\) & \(77.7/62.6\) & \(79.8/98.4\) \\  & Robust & \(45.9/65.1\) & \(60.9/66.6\) & \(50.3/52.1\) & \(50.3/45.2\) & \(5.3/58.4\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Clean and robust performances across all architectures in the form XX/YY. XX is the accuracy with standard training, and YY is the accuracy with adversarial training.

training. With standard training, ID performance is misleading in CTU and URL datasets. Although all models exhibit similar ID performance, some of the augmentations lead to robust models, while others decrease it. CTGAN data augmentation is the best data augmentation for ID performance in all use cases, both with standard and adversarial training.

### Impact of Adversarial Training

**With data augmentation and AT, ID and robust performances are correlated.** Although there is no trend of relationship between ID performance and robust performance in standard training, our study shows that robustness and ID performance are correlated after adversarial training. For example, the Pearson correlation between ID and robust performance increases from \(0.15\) to \(0.76\) for LCLD. All correlation values are in Appendix B.4.

Overall, all architectures can benefit from at least one data augmentation technique with adversarial training; however, standard training with data augmentation can outperform adversarial training without data augmentation (for e.g., on URL dataset using GOGGLE or CTGAN augmentations).

### Impact of Architecture

In Figure 3 we study the robustness of each architecture with different defense mechanisms. We report both the robustness against unconstrained attacks (attacks unaware of domain knowledge) and attacks optimized to preserve the feature relationships and constraints.

Figure 3: Robust performance while considering domain constraints (ADV+CTR: Y-axis) and without (ADV: X-axis) on all our use cases confirms the relevance of studying constrained-aware attacks.

**Evaluation with unconstrained attacks is misleading.** Under standard training (orange scatters in Fig. 3), there is no relation between robustness to unconstrained attacks and the robustness when domain constraints are enforced. There is, however, a linear relationship under adversarial training with data augmentation only for STG, Tabistransformer, and VIME architectures. These results show that nonconstrained attacks are not sufficient to reliably assess the robustness of deep tabular models. Detailed correlation values are in the Appendix B.4.

**No data augmentation consistently outperforms the baselines with AT.** Among the 20 scenarios in Fig. 3, the original models achieve better constrained robustness than augmented models with adversarial training only for 4 scenarios: TabNet architecture on URL, LCLD and WIDS, and STG architecture on URL datasets. No data-augmentation technique consistently outperforms the others across all architectures. Cutmix, the simplest data augmentation, is often the best (in 7/20 scenarios).

### Impact of Attack Budgets

We evaluated each robustified model against variants of the CAA attack, varying the \(L_{2}\) distance of the perturbation \(\epsilon\) from \(0.5\) to \(\{0.25,1,5\}\), the gradient iterations from \(10\) to \(\{5,20,100\}\), and the search iterations from \(100\) to \(\{50,200,1000\}\). We report per architecture for each dataset the most robust model with AT and augmentation, and the robust model with AT only. We present in Fig. 4 the results for the URL dataset and refer to Appendix B.5 for the other use cases.

**AT+Augmentations models remain robust even under stronger attacks.** Our results show that the best defenses with AT+Augmentations (continuous lines) remain robust against increased gradient and search iteration budgets and remain more robust than AT alone (dashed lines) for VIME, RLN, and Tabtransformer architectures. Against an increase in perturbation size \(\epsilon\), AT+Augmentations is more robust than AT alone for TabNet, TabTransformer, VIME, and RLN architectures. In particular, for \(\epsilon=5\), the robust accuracy of TabNet architectures remains above 40% with AT+Augmentations while the robust accuracy with AT alone drops to 0%.

## 5 Limitations

While our benchmark is the first to tackle adversarial robustness in tabular deep learning models, it does not cover all the directions of the field and focuses on domain constraints and defense mechanisms. Some of the orthogonal work is not addressed:

**Generalization to other distances:** We restricted our study to the \(L_{2}\) distance to measure imperceptibility. Imperceptibility varies by domain, and several methods have been proposed to measure it (Ballet et al., 2019; Kireev et al., 2022; Dymishi et al., 2023). These methods have not been evaluated against human judgment or compared with one another, so there is no clear motivation to use one or another. In our research, we chose to use the well-established \(L_{2}\) norm (following Dymishi et al. (2023)). Our algorithms and benchmarks support other distances and definitions of imperceptibility. We provide in Appendix B.6 an introduction to how our benchmark generalizes to other distances.

**Generalization to non-binary classification:** We restricted our study to binary tabular classification as it is the only case where we identified public datasets with domain constraints. The attacks used in our benchmark natively support multi-class classification. Our live leaderboard welcomes new datasets and will be updated if relevant datasets are designed by the community.

Figure 4: Impact of attack budget on the robust accuracy for URL dataset.

**Generalization to other types of defenses:** We only considered defenses based on data augmentation with adversarial training. Adversarial training-based defenses are recognized as the only reliable defenses against evasion attack (Tramer et al., 2020; Carlini, 2023). All other defenses are proven ineffective when the attacker is aware of them and performs adaptive attacks.

**Generalization to other (adaptive) attacks:** We only considered the Constrained Adaptive Attack (CAA) as it is the most effective and efficient attack in the literature. We encourage the community to develop new attacks and evaluate them on our benchmark. We provide the code for CAA in our repository and encourage the community to develop new attacks and evaluate them on our benchmark. In the face of new attacks, we will update our benchmark to include them. A limitation of our current evaluation regarding attacks is the lack of adaptive attacks, that adapt their strategy based on the defense mechanism. We welcome the development of new adaptive attacks and their evaluation on our benchmark at https://github.com/serval-uni-lu/tabularbench/issues/new/choose.

## 6 Broader Impact

Our work proposes the first benchmark of robustness of constrained tabular deep learning against evasion attacks. We focus on designing new defense mechanisms, inspired by effective approaches in computer vision (by combining data augmentation and adversarial training). Hence, we expect that our research will significantly contribute to the enhancement of defenses and will lead to even more resilient models, which may balance the potential harms research on adversarial attacks can have.

## Conclusion

In this work, we introduce TabularBench, the first benchmark of adversarial robustness of tabular deep learning models against constrained evasion attacks. We leverage Constrained Adaptive Attack (CAA), the best constrained tabular attack, to benchmark state-of-the-art architectures and defenses.

We provide a Python API to access the datasets, along with implementations of multiple tabular deep learning architectures, and provide all our pre-trained robust models directly through the API.

We conducted an empirical study that constitutes the first large-scale study of tabular data model robustness against evasion attacks. Our study covers five real-world use cases, five architectures, and six data augmentation mechanisms totaling more than 200 models. Our study identifies the best augmentation mechanisms for IID performance (CTGAN) and robust performance (Cutmix), and provides actionable insights on the selection of architectures and robustification mechanisms.

We are confident that our benchmark will accelerate the research of adversarial defenses for tabular ML and welcome all contributions to improve and extend our benchmark with new realistic use cases (multiclass), models, and defenses.

## Acknowledgments and Disclosure of Funding

This research was funded in whole, or in part, by the Luxembourg National Research Fund (FNR), grant reference NCER22/IS/16570468/NCER-FT and grant BRIDGES/2022/IS/17437536. This research was supported by BGL BNP Paribas Luxembourg. In particular, we would like to thank Anne Goujon and Andrey Boytsov for their support with the financial use case. The experiments presented in this paper were carried out using the HPC facilities of the University of Luxembourg Varrette et al. (2022) (see hpc.uni.lu). We would like to thank Salijona Dymrishi for her help in generating synthetic examples using the data augmentation methods.

## References

* Arik and Pfister (2021) Sercan O Arik and Tomas Pfister. 2021. Tabnet: Attentive interpretable tabular learning. In _Proceedings of the AAAI conference on artificial intelligence_, Vol. 35. 6679-6687.
* Arjovsky et al. (2017) Martin Arjovsky, Soumith Chintala, and Leon Bottou. 2017. Wasserstein generative adversarial networks. In _International conference on machine learning_. PMLR, 214-223.
* Arjovsky et al. (2018)Vincent Ballet, Jonathan Aigrain, Thibault Laugel, Pascal Frossard, Marcin Detyniecki, et al. 2019. Imperceptible Adversarial Attacks on Tabular Data. In _NeurIPS 2019 Workshop on Robust AI in Financial Services: Data, Fairness, Explainability, Trustworthiness and Privacy (Robust AI in FS 2019)_.
* Bao et al. (2023) Hongyan Bao, Yufei Han, Yujun Zhou, Xin Gao, and Xiangliang Zhang. 2023. Towards efficient and domain-agnostic evasion attack with high-dimensional categorical inputs. In _Proceedings of the AAAI Conference on Artificial Intelligence_, Vol. 37. 6753-6761.
* Borisov et al. (2021) Vadim Borisov, Tobias Leemann, Kathrin Sessler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci. 2021. Deep neural networks and tabular data: A survey. _arXiv preprint arXiv:2110.01889_ (2021).
* Borisov et al. (2022) Vadim Borisov, Tobias Leemann, Kathrin Sessler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci. 2022. Deep Neural Networks and Tabular Data: A Survey. _IEEE Transactions on Neural Networks and Learning Systems_ (2022), 1-21. https://doi.org/10.1109/tnnls.2022.3229161
* Carlini (2023) Nicholas Carlini. 2023. A LLM assisted exploitation of AI-Guardian. _arXiv preprint arXiv:2307.15008_ (2023).
* Cartella et al. (2021) Francesco Cartella, Orlando Anunciacao, Yuki Funabiki, Daisuke Yamaguchi, Toru Akishita, and Olivier Elshocht. 2021. Adversarial Attacks for Tabular Data: Application to Fraud Detection and Imbalanced Data. (2021).
* Chernikova and Oprea (2019) Alesia Chernikova and Alina Oprea. 2019. Fence: Feasible evasion attacks on neural networks in constrained environments. _arXiv preprint arXiv:1909.10480_ (2019).
* Chernikova and Oprea (2022) Alesia Chernikova and Alina Oprea. 2022. Fence: Feasible evasion attacks on neural networks in constrained environments. _ACM Transactions on Privacy and Security_ 25, 4 (2022), 1-34.
* Croce et al. (2020) Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. 2020. Robustbench: a standardized adversarial robustness benchmark. _arXiv preprint arXiv:2010.09670_ (2020).
* Dong et al. (2020) Yinpeng Dong, Qi-An Fu, Xiao Yang, Tianyu Pang, Hang Su, Zihao Xiao, and Jun Zhu. 2020. Benchmarking Adversarial Robustness on Image Classification. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_. 321-331.
* Dymrishi et al. (2023) Salijona Dymrishi, Salah Ghamizi, Thibault Simonetto, Yves Le Traon, and Maxime Cordy. 2023. On the empirical effectiveness of unrealistic adversarial hardening against realistic adversarial attacks. In _2023 IEEE symposium on security and privacy (SP)_. IEEE, 1384-1400.
* Gardner et al. (2023) Joshua P Gardner, Zoran Popovi, and Ludwig Schmidt. 2023. Benchmarking Distribution Shift in Tabular Data with TableShift. In _Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_. https://openreview.net/forum?id=XYxNk10MMX
* George (2018) Nathan George. 2018. Lending Club loan data. https://www.kaggle.com/datasets/wordsforthewise/lending-club.
* Ghamizi et al. (2020) Salah Ghamizi, Maxime Cordy, Martin Gubri, Mike Papadakis, Andrey Boystov, Yves Le Traon, and Anne Goujon. 2020. Search-based adversarial testing and improvement of constrained credit scoring systems. In _Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering_. 1089-1100.
* Hannousse and Yahiouche (2021) Abdelhakim Hannousse and Salima Yahiouche. 2021. Towards benchmark datasets for machine learning based website phishing detection: An experimental study. _Engineering Applications of Artificial Intelligence_ 104 (2021), 104347.
* Huang et al. (2020) Xin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar Karnin. 2020. Tabtransformer: Tabular data modeling using contextual embeddings. _arXiv preprint arXiv:2012.06678_ (2020).
* Kireev et al. (2022) Klim Kireev, Bogdan Kulynych, and Carmela Troncoso. 2022. Adversarial Robustness for Tabular Data through Cost and Utility Awareness. _arXiv preprint arXiv:2208.13058_ (2022).
* Krizhevsky et al. (2014)Meredith Lee, Jesse Raffa, Marzyeh Ghassemi, Tom Pollard, Sharada Kalanidhi, Omar Badawi, Karen Matthys, and Leo Anthony Celi. 2020. WiDS (Women in Data Science) Datathon 2020: ICU Mortality Prediction. PhysioNet.
* Lin et al. (2018) Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh. 2018. Pacgan: The power of two samples in generative adversarial networks. _Advances in neural information processing systems_ 31 (2018).
* Liu et al. (2023) Tennison Liu, Zhaozhi Qian, Jeroen Berrevoets, and Mihaela van der Schaar. 2023. GOGGLE: Generative Modelling for Tabular Data by Learning Relational Structure. In _The Eleventh International Conference on Learning Representations_. https://openreview.net/forum?id=fPVRcJqspu
* Park et al. (2018) Noseong Park, Mahmoud Mohammadi, Kshitij Gorde, Sushil Jajodia, Hongkyu Park, and Youngmin Kim. 2018. Data synthesis based on generative adversarial networks. _Proceedings of the VLDB Endowment_ 11, 10 (June 2018), 1071-1083. https://doi.org/10.14778/3231751.3231757
* Shavitt and Segal (2018) Ira Shavitt and Eran Segal. 2018. Regularization learning networks: deep learning for tabular datasets. _Advances in Neural Information Processing Systems_ 31 (2018).
* Shwartz-Ziv and Armon (2021) Ravid Shwartz-Ziv and Amitai Armon. 2021. Tabular Data: Deep Learning is Not All You Need. _arXiv preprint arXiv:2106.03253_ (2021).
* Simonetto et al. (2022) Thibault Simonetto, Salijona Dymrishi, Salah Ghamizi, Maxime Cordy, and Yves Le Traon. 2022. A Unified Framework for Adversarial Attack and Defense in Constrained Feature Space. In _Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22_, Lud De Raedt (Ed.). International Joint Conferences on Artificial Intelligence Organization, 1313-1319. https://doi.org/10.24963/ijcai.2022/183 Main Track.
* Simonetto et al. (2024) Thibault Simonetto, Salah Ghamizi, and Maxime Cordy. 2024. Constrained Adaptive Attack: Effective Adversarial Attack Against Deep Neural Networks for Tabular Data. In _Advances in Neural Information Processing Systems_.
* Somani et al. (2021) Sulaiman Somani, Adam J Russak, Felix Richter, Shan Zhao, Akhil Vaid, Fayzan Chaudhry, Jessica K De Freitas, Nidhi Naik, Riccardo Miotto, Girish N Nadkarni, et al. 2021. Deep learning and the electrocardiogram: review of the current state-of-the-art. _EP Europace_ (2021).
* Stoian et al. (2024) Mihaela C Stoian, Salijona Dymrishi, Maxime Cordy, Thomas Lukasiewicz, and Eleonora Giunchiglia. 2024. How Realistic Is Your Synthetic Data? Constraining Deep Generative Models for Tabular Data. In _The Twelfth International Conference on Learning Representations_. https://openreview.net/forum?id=tBROVsE29G
* Tramer et al. (2020) Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. 2020. On adaptive attacks to adversarial example defenses. _Advances in Neural Information Processing Systems_ 33 (2020), 1633-1645.
* Ulmer et al. (2020) Dennis Ulmer, Lotta Meijerink, and Giovanni Cina. 2020. Trust Issues: Uncertainty Estimation Does Not Enable Reliable OOD Detection On Medical Tabular Data. In _Machine Learning for Health_. PMLR, 341-354.
* Varrette et al. (2022) S. Varrette, H. Cartiaux, S. Peter, E. Kieffer, T. Valette, and A. Olloh. 2022. Management of an Academic HPC & Research Computing Facility: The ULHPC Experience 2.0. In _Proc. of the 6th ACM High Performance Computing and Cluster Technologies Conf. (HPCCT 2022)_. Association for Computing Machinery (ACM), Fuzhou, China.
* Wang et al. (2023) Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, and Bo Li. 2023. DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. In _Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_. https://openreview.net/forum?id=kAdpo80Zw2
* Wang et al. (2020) Yutong Wang, Yufei Han, Hongyan Bao, Yun Shen, Fenglong Ma, Jin Li, and Xiangliang Zhang. 2020. Attackability characterization of adversarial evasion attack on discrete data. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_. 1415-1425.

Han Xu, Pengfei He, Jie Ren, Yuxuan Wan, Zitao Liu, Hui Liu, and Jiliang Tang. 2023. Probabilistic categorical adversarial attack and adversarial training. In _International Conference on Machine Learning_. PMLR, 38428-38442.
* Xu et al. (2019a) Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. 2019a. Modeling Tabular data using Conditional GAN. arXiv:1907.00503 [cs.LG]
* Xu et al. (2019b) Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. 2019b. Modeling tabular data using conditional GAN. In _Advances in Neural Information Processing Systems_, Vol. 33.
* Yamada et al. (2020) Yutaro Yamada, Ofir Lindenbaum, Sahand Negahban, and Yuval Kluger. 2020. Feature Selection using Stochastic Gates. In _Proceedings of Machine Learning and Systems 2020_. 8952-8963.
* Yoon et al. (2020) Jinsung Yoon, Yao Zhang, James Jordon, and Mihaela van der Schaar. 2020. Vime: Extending the success of self-and semi-supervised learning to tabular domain. _Advances in Neural Information Processing Systems_ 33 (2020), 11033-11043.
* Yun et al. (2019) Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. 2019. Cutmix: Regularization strategy to train strong classifiers with localizable features. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_. 6023-6032.

## Appendix A Experimental protocol

### Datasets

Our dataset design followed the same protocol as Simonetto et al.Simonetto et al. (2022). We present in Table 4 the attributes of our datasets and the test performance achieved by each of the architectures.

Credit Scoring - LCLD(license: CC0: Public Domain) We develop a dataset derived from the publicly accessible Lending Club Loan Data

footnotehttps://www.kaggle.com/wordsforthewise/lending-club. This dataset includes 151 features, with each entry representing a loan approved by the Lending Club. However, some of these approved loans are not repaid and are instead charged off. Our objective is to predict, at the time of the request, whether the borrower will repay the loan or if it will be charged off. This dataset has been analyzed by various practitioners on Kaggle. Nevertheless, the original dataset only contains raw data, and to the best of our knowledge, there is no commonly used feature-engineered version. Specifically, caution is needed when reusing feature-engineered versions, as many proposed versions exhibit data leakage in the training set, making the prediction trivial. Therefore, we propose our own feature engineering. The original dataset contains 151 features. We exclude examples where the feature "loan status" is neither "Fully paid" nor "Charged Off," as these are the only definitive statuses of a loan; other values indicate an uncertain outcome. For our binary classifier, a "Fully paid" loan is represented as 0, and a "Charged Off" loan is represented as 1. We begin by removing all features that are missing in more than 30% of the examples in the training set. Additionally, we remove all features that are not available at the time of the loan request to avoid bias. We impute features that are redundant (e.g., grade and sub-grade) or too detailed (e.g., address) to be useful for classification. Finally, we apply one-hot encoding to categorical features. We end up with 47 input features and one target feature. We split the dataset using random sampling stratified by the target class, resulting in a training set of 915K examples and a testing set of 305K examples. Both sets are unbalanced, with only 20% of loans being charged off (class 1). We trained a neural network to classify accepted and rejected loans, consisting of 3 fully connected hidden layers with 64, 32, and 16 neurons, respectively. For each feature in this dataset, we define boundary constraints based on the extreme values observed in the training set. We consider the 19 features under the control of the Lending Club as immutable. We identify 10 relationship constraints (3 linear and 7 non-linear).

URL Phishing - ISCX-URL2016(license CC BY 4.0) Phishing attacks are commonly employed to perpetrate cyber fraud or identity theft. These attacks typically involve a URL that mimics a legitimate one (e.g., a user's preferred e-commerce site) but directs the user to a fraudulent website that solicits personal or banking information. Hannousse and Yahiouche (2021) extracted features from both legitimate and fraudulent URLs, as well as external service-based features, to develop a classifier capable of distinguishing between fraudulent and legitimate URLs. The features extracted from the URL include the number of special substrings such as "www", "&", ", "S", "and", the length of the URL, the port, the presence of a brand in the domain, subdomain, or path, and the inclusion of "http" or "https". External service-based features include the Google index, page rank, and the domain's presence in DNS records. The full list of features is available in the reproduction package. Hannousse and Yahiouche (2021) provide a dataset containing 5715 legitimate and 5715 malicious URLs. We use 75% of the dataset for training and validation, and the remaining 25% for testing and adversarial generation. We extract a set of 14 relational constraints between the URL features. Among these, 7 are linear constraints (e.g., the length of the hostname is less than or equal

\begin{table}
\begin{tabular}{l|l l l l} \hline \hline Dataset & \multicolumn{4}{c}{Properties} \\  & Task & Size & \# Features & Balance \\ \hline LCLD (George, 2018) & Credit Scoring & 1 220 092 & 28 & 80/20 \\ CTU-13 (Chemikova and Oprea, 2022) & Botnet detection & 198 128 & 756 & 99.3/0.7 \\ URL (Hannousse and Yahiouche, 2021) & Phishing URL detection & 11 430 & 63 & 50/50 \\ WIDS (Lee et al., 2020) & ICU patient survival & 91 713 & 186 & 91.4/8.6 \\ \hline \hline \end{tabular}
\end{table}
Table 4: The datasets evaluated in the empirical study, with the class imbalance of each dataset (Balance in %).

to the length of the URL) and 7 are Boolean constraints of the form \(if\ a>0\ then\ b>0\) (e.g., if the number of "http" \(>0\), then the number of slashes "\(\prime\)" \(>0\)).

Botnet attacks - CTU-13(license CC BY NC SA 4.0) This is a feature-engineered version of CTU-13 proposed by Chernikova and Oprea (2019). It includes a combination of legitimate and botnet traffic flows from the CTU University campus. Chernikova et al. aggregated raw network data related to packets, duration, and bytes for each port from a list of commonly used ports. The dataset consists of 143K training examples and 55K testing examples, with 0.74% of examples labeled as botnet traffic (traffic generated by a botnet). The data contains 756 features, including 432 mutable features. We identified two types of constraints that define what constitutes feasible traffic data. The first type pertains to the number of connections and ensures that an attacker cannot reduce it. The second type involves inherent constraints in network communications (e.g., the maximum packet size for TCP/UDP ports is 1500 bytes). In total, we identified 360 constraints.

WiDs(license: PhysioNet Restricted Health Data License 1.5.0 1) Lee et al. (2020) dataset contains medical data on the survival of patients admitted to the ICU. The objective is to predict whether a patient will survive or die based on biological features (e.g., for triage). This highly unbalanced dataset has 30 linear relational constraints.

Footnote 1: https://physionet.org/content/widsdatathon2020/view-license/1.0.0/

Malware(license MIT) contains 24222 features extracted from a collection of benign and malware Portable Executable (PE) files Dymrishi et al. (2023). The features include the DLL imports, the API imports, PE sections, and statistic features such as the proportion of each possible byte value. The dataset contains 17,584 samples. The number of total features and the number of features involved in each constraint make this dataset challenging to attack. The objective of the classifier is to distinguish between malware and benign software.

### Model architectures

Table 5 provides an overview of the family, model architecture, and hyperparameters adjusted during the training of our models.

TabTransformeris a transformer-based model Huang et al. (2020). It employs self-attention to convert categorical features into an interpretable contextual embedding, which the paper asserts enhances the model's robustness to noisy inputs.

TabNetis another transformer-based model Arik and Pfister (2021). It utilizes multiple sub-networks in sequence. At each decision step, it applies sequential attention to select which features to consider. TabNet combines the outputs of each step to make the final decision.

RLNor Regularization Learning Networks Shavitt and Segal (2018) employs an efficient hyperparameter tuning method to minimize counterfactual loss. The authors train a regularization coefficient for the neural network weights to reduce sensitivity and create very sparse networks.

\begin{table}
\begin{tabular}{l l l} \hline \hline Family & Model & Hyperparameters \\ \hline Transformer & TabTransformer & \(hidden\_dim,n\_layers,\) \\  & \(learning\_rate,norm,\theta\) \\  & & \(n\_d,n\_steps,\) \\ Transformer & TabNet & \(\gamma,cat\_emb\_dim,n\_independent,\) \\  & & \(n\_shared,momentum,mask\_type\) \\  & & \(hidden\_dim,depth,\) \\ Regularization & RLN & \(heads,weight\_decay,\) \\  & & \(learning\_rate,dropout\) \\ Regularization & STG & \(hidden\_dims,learning\_rate,lam\) \\ Encoding & VIME & \(p_{m},\alpha,K,\beta\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: The three model architectures of our study.

STGor Stochastic Gates Yamada et al. (2020) uses stochastic gates for feature selection in neural network estimation tasks. The technique is based on a probabilistic relaxation of the \(l_{0}\) norm of features or the count of selected features.

Vimeor Value Imputation for Mask Estimation Yoon et al. (2020) employs self-supervised and semi-supervised learning through deep encoders and predictors.

### Evaluation settings

MetricsThe models are fine-tuned to maximize cross-validation AUC. This metric is threshold-independent and is not affected by the class unbalance of our dataset.

We only attack clean examples that are not already misclassified by the model and from the critical class, that is respectively for each aforementioned dataset the class of phishing URLs, rejected loans, malwares, botnets, and not surviving patients. Because we consider a single class, the only relevant metric is robust accuracy on constrained examples. Unsuccessful adversarial examples count as correctly classified when measuring robust accuracy.

We only consider examples that respect domain constraints to compute robust accuracy. If an attack generates invalid examples, they are defacto considered unsuccessful and are reverted to their original example (correctly classified).

We report in the Appendix 8 all the remaining performance metrics, including the recall, the precision, and the Mattheu Correlation Coefficient (MCC).

Attacks parametersCAA applies CAPGD and MOEVA with the following parameters.

CAPGD uses \(N_{iter}=10\) iterations. The step reduction schedule for CPGD uses \(M=7\). In CAPGD, checkpoints are set as \(w_{j}=\lceil p_{j}\times N_{iter}\rceil\leq N_{iter}\), with \(p_{j}\in[0,1]\) defined as \(p_{0}=0\), \(p_{1}=0.22\), and

\[p_{j+1}=p_{j}+\max p_{j}-p_{j-1}-0.03,0.06.\]

The influence of the previous update on the current update is set to \(\alpha=0.75\), and \(\rho=0.75\) for step halving. MOEVA runs for \(n_{gen}=100\) iterations, generating \(n_{off}=100\) offspring per iteration. Among the offspring, \(n_{pop}=200\) survive and are used for mating in the subsequent iteration.

HardwareOur experiments are conducted on an HPC cluster node equipped with 32 cores and 64GB of RAM allocated for our use. Each node is composed of 2 AMD Epyc ROME 7H12 processors running at 2.6 GHz, providing a total of 128 cores and 256 GB of RAM.

### Generator architectures

In our experimental study, we use the same five generative models as Stoian et al. (2024):

* **WGAN** (Arjovsky et al., 2017) is a GAN model trained with Wasserstein loss within a standard generator-discriminator GAN framework. In our implementation, WGAN utilizes a MinMax transformer for continuous features and one-hot encoding for categorical features. It is not specifically designed for tabular data.
* **TableGAN** (Park et al., 2018) is one of the pioneering GAN-based methods for generating tabular data. Besides the conventional generator and discriminator setup in GANs, the authors introduced a classifier trained to understand the relationship between labels and other features. This classifier ensures a higher number of semantically correct generated records. TableGAN applies a MinMax transformer to the features.
* **CTGAN** (Xu et al., 2019) employs a conditional generator and a training-by-sampling strategy within a generator-discriminator GAN framework to model tabular data. The conditional generator produces synthetic rows conditioned on one of the discrete columns. The training-by-sampling method ensures that data are sampled according to the log frequency of each category, aiding in better modeling of imbalanced categorical columns. CTGAN uses one-hot encoding for discrete features and a mode-based normalization for continuous features. A variational Gaussian mixture model (**?**) is used to estimate the number of modes and fit a Gaussian mixture. For each continuous value, a mode is sampled based on probability densities, and its mean and standard deviation are used for normalization.
* **TVAE**(Xu et al., 2019) was introduced as a variant of the standard Variational AutoEncoder to handle tabular data. It employs the same data transformations as CTGAN and trains the encoder-decoder architecture using evidence lower-bound (ELBO) loss.
* **GOGGE**(Liu et al., 2023) is a graph-based method for learning the relational structure of data as well as functional relationships (dependencies between features). The relational structure is learned by constructing a graph where nodes represent variables and edges indicate dependencies between them. Functional dependencies are learned through a message-passing neural network (MPNN). The generative model generates each variable considering its surrounding neighborhood.

The hyperparameters for training these models are based on Stoian et al. (2024) as well:

For GOGGE,we employed the same optimizer and learning rate configuration as described in Liu et al. (2023). Specifically, ADAM was used with five different learning rates: \(\{1\times 10^{-3},5\times 10^{-3},1\times 10^{-2}\}\).

For TVAE,ADAM was utilized with five different learning rates: \(\{5\times 10^{-6},1\times 10^{-5},1\times 10^{-4},2\times 10^{-4},1\times 10^{-3}\}\).

For the other DGM models, three different optimizers were tested: ADAM, RMSPROP, and SGD, each with distinct sets of learning rates.

For WGAN,the learning rates were \(\{1\times 10^{-4},1\times 10^{-3}\}\), \(\{5\times 10^{-5},1\times 10^{-4},1\times 10^{-3}\}\), and \(\{1\times 10^{-4},1\times 10^{-3}\}\), respectively.

For TableGAN,the learning rates were \(\{5\times 10^{-5},1\times 10^{-4},2\times 10^{-4},1\times 10^{-3}\}\), \(\{1\times 10^{-4},2\times 10^{-4},1\times 10^{-3}\}\), and \(\{1\times 10^{-4},1\times 10^{-3}\}\), respectively.

For CTGAN,the learning rates were \(\{5\times 10^{-5},1\times 10^{-4},2\times 10^{-4}\}\), \(\{1\times 10^{-4},2\times 10^{-4},1\times 10^{-3}\}\), and \(\{1\times 10^{-4},1\times 10^{-3}\}\), respectively.

For each optimizer-learning rate combination, three different batch sizes were tested, depending on the DGM model: \(\{64,128,256\}\) for WGAN, \(\{128,256,512\}\) for TableGAN, \(\{70,280,500\}\) for CTGAN and TVAE, and \(\{64,128\}\) for GOGGE. The batch sizes for CTGAN are multiples of 10 to accommodate the recommended PAC value of 10 as suggested in Lin et al. (2018), among other values.

### Reproduction package and availability

The source code, datasets, and pre-trained models required to replicate the experiments in this paper are publicly accessible under the MIT license on the repository https://github.com/serval-uni-lu/tabularbench.

## Appendix B Detailed results

### Baseline models performances

We compare in 6 the ID performance of XGBoost and our deep learning models under standard training. We confirm that DL models are on par with the performances achieved by shallow models.

### Execution time

We provide in Table 7 the execution time in seconds for each dataset and architecture. We run the benchmark on 1.000 examples each with the standard benchmark parameters (100 iterations, no time limit). Given the search component MOEVA within CAA, the execution time linearly increases with the complexity of the dataset. The malware dataset that we curated for this benchmark is very robust 

[MISSING_PAGE_EMPTY:18]

[MISSING_PAGE_EMPTY:22]

For LCLD dataset only Goggle and WGAN data augmentations lead to \(MCC=0\). To uncover what happens with some generated data, we study the distribution of artificial examples on the LCLD dataset for 3 cases: Two cases where performance did not collapse: TableGAN and CTGAN and one problematic case WGAN.

Kernel Density Estimation.We first compare the artificial examples distributions in Figure 5. The results show that the labels and the main features of TableGAN, a "healthy" generator are closer to the distribution of the "problematic" generator WGAN than to the distribution of CTGAN, another "healthy" generator. Feature and label distributions are not problematic.

Statistical analysis.We perform the following statistical tests to compare the distributions quantitatively between the examples generated by the three generators. Kolmogorov-Smirnov test, t-test, or MWU test. We report the results in Table 9. Across all statistical tests, there is no specific pattern to the faulty generator "WGAN" compared to CTGAN and TableGAN.

Figure 5: Impact of attack budget on the robust accuracy for LCLD dataset.

[MISSING_PAGE_FAIL:24]

Robust performance after data augmentationWe report below the robustness of our 270 models trained with various combinations of architecture, data augmentation, and adversarial training.

[MISSING_PAGE_EMPTY:29]

[MISSING_PAGE_EMPTY:30]

[MISSING_PAGE_EMPTY:31]

## 6 Conclusion

Figure 10: Impact of attack budget on the robust accuracy for Malware dataset.

### Generalization to other distances

We define for all attacks a distance function. This method is used for MOEVA (the evolution attack) to measure the fitness value related to the distance objective, and in the evaluation method to validate the correctness of the adversarial examples.

By default, it supports \(L_{\infty}\) and \(L_{2}\) distances 3:

Footnote 3: https://github.com/serval-uni-lu/tabularbench/tabularbench/attacks/utils.py

``` fromtabularbench.utils.typingimportNDBool,NDInt,NDNumber defcompute_distance(x_1:NDNumber,x_2:NDNumber,norm:Any)-> INDNumber: ifnormin["inf",np.inf,"Linf"]: distance=np.linalg.norm(x_1-x_2,ord=np.inf,axis=-1) elifnormin["2",2,"L2","12"]: distance=np.linalg.norm(x_1-x_2,ord=2,axis=-1) else: raiseNotImplementedError returndistance ```

One can define any new distance metric, like structural similarity index measure (SSIM), or some semantic measure after embedding the features \(x_{1}\) and \(x_{2}\). The distance used here does not need to be differentiable and is not backpropagated in the gradient attacks.

Hence, for CAPGD component of the benchmark attack, we need to define a custom project mechanism for each distance. We implemented a projection over sphere of \(L_{\infty}\) and \(L_{2}\) distances https://github.com/serval-uni-lu/tabularbench/blob/main/tabularbench/attacks/capgd/capgd.py#L196.

To extend the projected gradient attacks to other distances, custom projection mechanisms are then needed.

## Appendix C Api

The library https://github.com/serval-uni-lu/tabularbench/tree/main/tabularbench is split in 4 main components. The _test_ folder provides meaningful examples for each component.

### Datasets

Our dataset factory support 5 datasets: CTU, LCLD, MALWARE, URL, and WIDS. each dataset can be invoked with the following aliases:

``` fromtabularbench.datasetsimportdataset_factory dataset_aliases=[ "ctu_13_neris", "1cld_time", "malware", "url", "wids", ] fordataset_nameintdataset_aliases: dataset=dataset_factory.get_dataset(dataset_name) x,_=dataset.get_x_y() metadata=dataset.get_metadata(only_x=True) assertx.shape[1]==metadata.shape[0] ```Each dataset can be defined in a single.py file (example: https://github.com/serval-uni-lu/tabularbench/blob/main/tabularbench/datasets/samples/url.py).

A dataset needs at least a source (local or remote csv) for the raw features, and a definition of feature constraints. The said definition can be empty for non-constrained datasets.

### Constraints

One of the features of our benchmark is the support of feature constraints, in the dataset definition and in the attacks.

Constraints can be expressed in natural language. For example, we express the constraint \(F_{0}=F_{1}+F_{2}\) such as:

``` fromtabularbench.constraints.relation_constraintimportFeature constraint1=Feature(0)==Feature(1)+Feature(2) ```

Given a dataset, one can check the constraint satisfaction over all constraints, given a tolerance.

``` fromtabularbench.constraints.constraints_checkerimport ConstraintChecker fromtabularbench.datasetsimportdataset_factory dataset=dataset_factory.get_dataset("url") x,_=dataset.get_x_y() constraints_checker=ConstraintChecker(dataset.get_constraints(),tolerance ) out=constraints_checker.check_constraints(x.to_numpy()) ```

In the provided datasets, all constraints are satisfied. During the attack, Constraints can be fixed as follows:

``` importnumpyasnp fromtabularbench.constraints.constraints_fixerimport ConstraintsFixer x=np.arange(9).reshape(3,3) constraints_fixer=ConstraintsFixer( guard_constraints=[constraint1], fix_constraints=[constraint1], ) x_fixed=constraints_fixer.fix(x) x_expected=np.array([[3,1,2],[9,4,5],[15,7,8]]) assertnp.equal(x_fixed,x_expected).all() ```

Constraint violations can be translated into losses and one can compute the gradient to repair the faulty constraints as follows:

``` importtorch fromtabularbench.constraints.constraints_backend_executorimport( ConstraintsExecutor, ) fromtabularbench.constraints.pytorch_backendimportPytorchBackend fromtabularbench.datasets.dataset_factoryimportget_dataset ds=get_dataset("url") ```constraints=ds.get_constraints() constraint1=constraints.relation_constraints[0] x,y=ds.get_x_y() x_metadata=ds.get_metadata(only_x=True) x=torch.tensor(x.values,dtype=torch.float32) constraints_executor=ConstraintsExecutor( constraint1, PytorchBackend(), feature_names=x_metadata["feature"].to_list(), ) x.requires_grad=True loss=constraints_executor.execute(x) grad=torch.autograd.grad( loss.sum(), x, )[0] ```

### Models

All models need to extend the class **BaseModelTorch4**. This class implements the definitions, the fit and evaluation methods, and the save and loading methods. Depending on the architecture, scaler and feature encoders can be required by the constructors.

Footnote 4: https://github.com/serval-uni-lu/tabularbench/blob/main/tabularbench/models/torch_models.py

So far, our API natively supports: multi-layer perceptrons (MLP), RLN, STG, TabNet, TabTransformer, and VIME. Our implementation is based on Tabsurvey Borisov et al. (2021). All models from this framework can be easily adapted to our API.

### Benchmark

The leaderboard is available on https://serval-uni-lu.github.io/tabularbench/.

This leaderboard will be updated regularly, and all the models listed in leaderboard are downloadable using our API

Figure 11: Screenshot of the TabularBench leaderboard on 12/06/2024

The benchmark leverages Constrained Adaptive Attack (CAA) by default and can be extended for other attacks.

``` clean_acc,robust_acc=benchmark(dataset='LCLD',model="TabTr_Cutmix",distance='L2',constraints=True) ```

The model attribute refers to a pre-trained model in the relevant model folder. The API infers the architecture from the first term of the model name, but it can be defined manually. In the above example, a **TabTransformer** architecture will be initialized.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] The paper's method and empirical study are about the first benchmark or adversarial robustness for deep tabular models, which is the claim of the abstract and introduction. 2. Did you describe the limitations of your work? [Yes] In section 5. 3. Did you discuss any potential negative societal impacts of your work? In section 6. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? Yes, our work conforms to them.
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? In [N/A] 2. Did you include complete proofs of all theoretical results? In [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? Yes, all replication elements are provided in the public repository: https://github.com/serval-uni-lu/tabularbench 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? In appendix A 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? We report the mean values in the plots of the main paper and report the mean, standard deviation, and the 95% confidence intervals in the appendix B. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? In appendix A.3
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? In section 3.1. 2. Did you mention the license of the assets? In Appendix A.1. 3. Did you include any new assets either in the supplemental material or as a URL? The public repository contains all the transformed assets (datasets) and trained models. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? The datasets used are all with open-source license that allow their usage in this work. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? No identifiable information or offensive content is present in our assets.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? No crowdsourcing or conducted research with human subjects. 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? No crowdsourcing or conducted research with human subjects. 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? No crowdsourcing or conducted research with human subjects.