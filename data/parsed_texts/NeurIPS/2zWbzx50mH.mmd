# Compact Proofs of Model Performance via Mechanistic Interpretability

 Jason Gross\({}^{*}\) Rajashree Agrawal Thomas Kwa Euan Ong Chun Hei Yip Alex Gibson Soufliane Noubir

Corresponding author. Please direct correspondence to jgross@mit.edu. These authors contributed equally to this work. These authors contributed equally to this work.

###### Abstract

We propose using mechanistic interpretability - techniques for reverse engineering model weights into human-interpretable algorithms - to derive and compactly prove formal guarantees on model performance. We prototype this approach by formally proving accuracy lower bounds for a small transformer trained on Max-of-\(K\), validating proof transferability across 151 random seeds and four values of \(K\). We create 102 different computer-assisted proof strategies and assess their length and tightness of bound on each of our models. Using quantitative metrics, we find that shorter proofs seem to require and provide more mechanistic understanding. Moreover, we find that more faithful mechanistic understanding leads to tighter performance bounds. We confirm these connections by qualitatively examining a subset of our proofs. Finally, we identify compounding structureless errors as a key challenge for using mechanistic interpretability to generate compact proofs on model performance.

## 1 Introduction

One approach to ensuring the safety and reliability of powerful AI systems is via formally verified proofs of model performance [48, 11]. If we hope to deploy formal verification on increasingly large models [24, 27] with powerful emergent capabilities [56] across more diverse and broader domains [5, 46], we will need _compact_ proofs of generalization bounds on _specific_ models that certify _global_ robustness. However, existing approaches tend to use proof strategies that suffer from bad asymptotic complexity, while verifying either generalization properties of training procedures or local robustness properties of specific models.

One key challenge to verification is that neural network architectures are highly expressive [51, 58], and models with similar training procedure and performance may still have learned significantly different weights [38, 9]. This expressivity makes it difficult to adequately _compress_ explanations of global model behavior in ways that _correspond_ closely enough to the model's actual mechanisms to be useful for efficient verification without being too _lossy_, especially when using only knowledge of the architecture or training procedure. We propose verifying model performance using understanding derived from _mechanistic interpretability_ (Section 2) - that is, reverse engineering the specific implementation of the algorithm from the learned weights of particular models. Knowledge of the specific implementation allows us to construct less lossy simplifications of the model, and more efficiently reason about model performance over possible inputs.

In this work, we provide a case study of translating mechanistic interpretations into compact proofs. We train an attention-only transformer on a Max-of-\(K\) task with 151 random seeds (Section 3), andthen reverse engineer the models using standard mechanistic interpretability techniques. We use our understanding to define a set of 102 different computer-assisted proof strategies with varying tightness of bound and with different asymptotic complexity and number of required FLOPs (Section 4).4 We validate our technique against an additional 604 models for varying values of \(K\) (Appendix A.2.1).

Footnote 4: Our 102 proof strategies are can be broken up as \(1+1+10\times 5\times 2\): two standalone strategies, and a class of strategies parameterized on three axes of cardinality 10, 5, and 2 (Appendix H).

We define a quantitative metric to assess the mechanistic understanding used in a proof strategy by the dimensionality of the function space that the proof strategy must consider, which we deem the _unexplained dimensionality_ of the proof strategy (Sections 5.1, and A.5). Using this metric, we find a negative relationship between proof length and degree of understanding. We qualitatively examine proof strategies to confirm and explain this relationship, finding that more compact proofs both require and provide more mechanistic understanding. We also find suggestive evidence that the trade-off between proof length and tightness of bound is modulated by the faithfulness of the mechanistic understanding used to derive the proof (Section 5.2).5

Footnote 5: Code: https://github.com/JasonGross/guarantees-based-mechanistic-interpretability/

However, we also identify compounding structureless error terms as a key challenge for generating compact proofs on model behavior (Sections 5.3, and G.2.5). The implementation of algorithms inside of neural networks may contain components that defy mechanistic understanding and appear to us as "noise". When we don't know how noise composes across model components, establishing a bound requires pessimizing over the ways the composition could occur. Worst-case noise can quickly grow even when the empirical noise is small, leading to vacuous performance bounds.

## 2 Mechanistic interpretability for proofs

**Generalization bounds on global performance** In the style of prior mechanistic interpretability evaluation work [6], we target theorem templates that establish bounds on the expected global performance of the model. Let \(\mathcal{M}:X\to Y\) be a model (here assumed to be a neural network), \(\mathcal{D}\) be a probability distribution over input-label pairs \((l,\mathbf{t})\in L\times X\), notated as \(\mathcal{D}|_{X}\) when marginalized over labels, and \(f:L\times Y\to\mathbb{R}\) be a scoring function for evaluating the performance of the model. Then, we seek to establish lower bounds \(b\) on the expected \(\bar{s}\) as the form:

\[\bar{s}:=\mathbb{E}_{(l,\mathbf{t})\sim\mathcal{D}}\left[f(l,\mathcal{M}( \mathbf{t}))\right]\geq b.\] (1)

Figure 1: We construct proofs using different degrees of mechanistic interpretation. (Left) The models we consider in this paper are one-layer attention-only transformers, and so contain three "paths”: the OV circuit, the QK circuit, and the direct path. (Right) For the brute-force proof (Section 4.3.1), we treat the model as a black box and thus need to check all possible combinations of inputs. For the cubic proof (Section 4.3.1), we decompose the model into its three corresponding paths, but still check the correctness of each path via brute force. Finally, in some subcubic proofs (Section 4.3), we use all parts of the mechanistic interpretation presented in Section 3. (Bottom) For each of the three categories of proof, we report the number of FLOPs used in computing the certificate (lower=better, Appendix A.6), lower bound on model accuracy (higher=better), effective dimension of the unexplained parts of the model (lower=better, Appendix A.5), and asymptotic complexity of the proof strategy as we scale the inputs and model (lower=better). Significantly more compact proofs have vacuous accuracy bounds by default. Using more mechanistic understanding allows us to recover some, but not all, of the accuracy bounds on these more compact proofs, as our understanding is not fully faithful to the model internals.

As \(f\) can be any metric, this is a fully general template for theorems that can capture any aspect of model performance for which we have a formal specification. However, in this work we restrict \(f\) to be the accuracy and \(\mathcal{D}|_{X}\) to be uniform, so our theorems lower bound the accuracy of the model. Our proof methodology generalizes straightforwardly to other input distributions (Appendix A.8), and only a little work is required to generalize from accuracy to log-loss (Appendix A.11).

**Proof template** The proofs of model performance in this work have two components: a computational component \(C:\) model weights \(\to\mathbb{R}\) and a non-computational component \(Q\) arguing that for any model \(\mathcal{M}^{\prime}\), \(C(\mathcal{M}^{\prime})\leq\mathbb{E}_{(l,\mathbf{t})\sim\mathcal{D}}f(l, \mathcal{M}^{\prime}(\mathbf{t}))\), thus implying that \(C\) generates a valid lower bound for the performance of \(\mathcal{M}\). The whole proof is \(Q\) paired with a trace of running \(C\) that certifies its output on \(\mathcal{M}\).6 Here, \(b=C(\mathcal{M})\). As even the size of the model parameters is much larger than any reasonable \(Q\), we approximate the length of a proof pair \(C,Q\) by the length of a trace of \(C(\mathcal{M})\).

Footnote 6: Other components of the proof to account for the difference between floating point numbers and reals are described in Appendix A.7. Note that all proofs explicitly given in this paper are of \(Q\) only; we do not include any traces of running \(C\).

**Proof compactness vs. tightness of bound** Different proof strategies make different tradeoffs between compactness and tightness of bound. For example, consider two extreme proof strategies: We can "prove" a vacuous bound using a null proof. On the other hand, in the brute-force proof, we simply run the model on the entirety of \(\mathcal{D}\) to achieve \(b=\bar{s}\), albeit with a very long proof.

We quantify the length of \(C(\mathcal{M})\) using two metrics: the _asymptotic time complexity_ of \(C\) as we scale the size of the model and the input \(\mathbf{t}\), as well as the empirical average _number of floating point operations_ required to evaluate \(C(\mathcal{M}^{\prime})\) over a given set of models \(\{\mathcal{M}_{i}\}\). We measure _tightness of bound_ of \(C(\mathcal{M})\) using the ratio of the bound to the true accuracy: \(b/\bar{s}\).

**Proof as pessimal ablation** A standard way of assessing the faithfulness of mechanistic interpretability is by ablating the parts of the model that your interpretation does not explain [54; 6; 23]. In this framework, proofs can be thought of as performing a _pessimal ablation_ over the unexplained parts of the model - we set the remaining components of the model (the "noise" or error terms) to values over \(X\) that minimize the performance of the model. However, the number of ablations required for a complete argument might be quite high. Thus, we construct _relaxations_ (Appendix A.4) over input sequences, such that performing pessimal ablations on a smaller number of relaxed input sequences is sufficient to lower bound the performance on \(\mathcal{D}\).

## 3 Experimental setting

We study our approach to generating compact proofs in a simple toy setting: Max-of-\(K\).

**Model Architecture** We study one-layer, one-head, attention-only transformers with no biases but with learned positional embeddings, with vocabulary size \(d_{\mathrm{vocab}}\), model and head dimension \(d=d_{\mathrm{model}}=d_{\mathrm{head}}\), and context length \(n_{\mathrm{ctx}}:=k\). The model parameters consist of the \(n_{\mathrm{ctx}}\times d_{\mathrm{model}}\) positional embedding \(P\); the \(d_{\mathrm{vocab}}\times d_{\mathrm{model}}\) token embed \(E\); the \(d_{\mathrm{model}}\times d_{\mathrm{model}}\) query, key, value, and output matrices of the attention head \(Q\), \(K\), \(V\), and \(O\); as well as the \(d_{\mathrm{model}}\times d_{\mathrm{vocab}}\) unembed matrix \(U\). We assume (as is standard in language modeling) that \(d_{\mathrm{model}}<d_{\mathrm{vocab}}\).

For an \(n_{\mathrm{ctx}}\times d_{\mathrm{vocab}}\) one-hot encoding \(\mathbf{x}=[x_{0},x_{1},\ldots,x_{n_{\mathrm{ctx}}-1}]\) of an input sequence \(\mathbf{t}=[t_{0},t_{1},\ldots,t_{n_{\mathrm{ctx}}-1}]\), we compute the logits of the model as follows:

\[h^{(0)} =\mathbf{x}E+P \text{Initial residual stream }(n_{\mathrm{ctx}}\times d_{\mathrm{model}})\] \[\alpha =h^{(0)}QK^{T}h^{(0)}{}^{T}/\sqrt{d} \text{Attention matrix }(n_{\mathrm{ctx}}\times n_{\mathrm{ctx}})\] \[h^{(1)} =\sigma^{*}(\alpha)\cdot h^{(0)}VO+h^{(0)} \text{Final residual stream }(n_{\mathrm{ctx}}\times d_{\mathrm{model}})\] \[\mathcal{M}(\mathbf{t}) =\ell=h^{(1)}_{n_{\mathrm{ctx}}-1}U \text{Final seq. position logits }(d_{\mathrm{vocab}})\]

where \(\sigma^{*}\) is the masked softmax function used in causal attention. Because we only look at outputs of the model above the final sequence position \(i=n_{\mathrm{ctx}}-1\), we also denote this position as the "query position" and the value of the token in this position as \(t_{\mathrm{query}}\), one-hot encoded as \(x_{\mathrm{query}}\). The model's prediction is the token corresponding to the max-valued logit \(\ell_{\mathrm{max}}\).

**Task** Specifically, we study the setting with \(n_{\mathrm{ctx}}=k=4\) because it is the largest sequence length for which we can feasibly evaluate the brute-force proof. We set hidden dimension \(d_{\mathrm{model}}=32\)and a vocabulary of size \(d_{\rm vocab}=64\) comprising integers between 0 and 63 inclusive. For an input sequence \(\mathbf{t}\), we denote the _true_ maximum of the sequence by \(t_{\rm max}\). Outputting the correct behavior is equivalent to outputting logits \(\ell\) such that \(\Delta\ell_{t^{*}}:=\ell_{t^{*}}-\ell_{\rm max}<0\) for all \(t^{*}\neq t_{\rm max}\). We trained 151 models on this task. Models achieved average accuracy \(0.9992\pm 0.0015\) over the entire distribution.

**Path decomposition** Following prior work [13], we expand the logits of the model and split the paths through the model into three components - the QK circuit, the OV circuit, and the direct path:

\[\mathcal{M}(\mathbf{t})=\sigma^{*}\Big{(}\underbrace{\left(x_{ \rm query}E+P_{\rm query}\right)QK^{T}\left(\mathbf{x}E+P\right)^{T}}_{\text {QK circuit}}/\sqrt{d}\Big{)}\cdot\underbrace{\left(\mathbf{x}E+P\right) VOU}_{\text{OV circuit}}+\underbrace{\left(x_{\rm query}E+P_{\rm query}\right)U}_{\text{direct path}}\] (2)

Intuitively, the QK circuit determines _which_ tokens the model attends to from a particular query token and sequence position, while the OV circuit _processes_ the tokens and sequence positions the model attends to. The direct path is simply the skip connection around the attention head.

We further divide the QK and OV circuits into token (position-independent) and position-dependent components. Let \(P_{\rm avg}=\sum_{i}P_{i}/n_{\rmctx}\) be the average position embeds across positions (of size \(d_{\rm model}\)), and let \(\mathbf{\bar{P}}\) denote either \(\mathbf{1}_{n_{\rmctx}}\otimes P_{\rm avg}\) or \(\mathbf{1}_{d_{\rm vocab}}\otimes P_{\rm avg}\) depending on context, the result of broadcasting \(P_{\rm avg}\) back into the shape of \(P\) or \(E\) (that is, \(n_{\rmctx}\times d_{\rm model}\) or \(d_{\rm vocab}\times d_{\rm model}\)). Similarly, let \(\mathbf{P}_{q}=\mathbf{1}_{d_{\rm vocab}}\otimes P_{\rm query}\) be the result of broadcasting \(P_{\rm query}\). Then for one-hot encoded \(\mathbf{x}\), we can rewrite the QK and OV circuits, as well as the direct path, as follows:

\[\text{QK circuit} =x_{\rm query}\Big{(}\underbrace{\mathbf{E}_{q}QK^{T}\mathbf{ \bar{E}}^{T}}_{\text{EQK}}\mathbf{x}^{T}+\underbrace{\mathbf{E}_{q}QK^{T} \mathbf{\hat{P}}^{T}}_{\text{EQKP}}\Big{)}\] \[\text{OV circuit} =\mathbf{x}\underbrace{\mathbf{\bar{E}}VOU}_{\text{EVOU}}+ \underbrace{\mathbf{\hat{P}}VOU}_{\text{PVOU}}\qquad\text{Direct Path}=x_{\rm query }\underbrace{\mathbf{E}_{q}U}_{\text{EU}}\]

where \(\mathbf{\hat{P}}=P-\mathbf{\bar{P}}\) and \(\mathbf{\bar{E}}=E+\mathbf{\bar{P}}\) and \(\mathbf{E}_{q}=E+\mathbf{P}_{q}\) (since \(h^{(0)}=\mathbf{x}\mathbf{\bar{E}}+\mathbf{\hat{P}}\)).

### Mechanistic interpretation of learned models

Using standard empirical mechanistic interpretability techniques, we interpret one of our learned models (our "mainline" model) by independently examining the QK and OV circuits and the direct path.7 We find that the model outputs the largest logit on the true max token \(t_{\rm max}\) by attending more to larger tokens via the QK circuit and copying the tokens it attends to via the OV circuit. We then quantitatively confirm that these interpretations hold for all 151 models by reporting the mean plus minus standard deviation for various summary statistics. Plots for this section are available in Appendix B.2.

Footnote 7: All of our trained models behave similarly; see Appendix B.3.

**QK circuit** By qualitatively examining the position-independent QK component \(\rm EQKE\), we find the amount of pre-softmax attention paid to a key token is approximately independent of the value of the query token \(t_{\rm query}\), and increases monotonically based on the size of the key token. We confirm this hypothesis by performing a singular-value decomposition (SVD) of the \(\rm EQKE\) matrices (Appendix G.2.3), and find that it contains a single large rank-one component with singular value around \(7800\pm 380\), around \(620\pm 130\) times larger than the second largest component with singular value \(13\pm 3\). The left (query-side) singular vector is approximately constant in all dimensions, with value \(0.1243\pm 0.0003\approx\nicefrac{{1}}{{\sqrt{d_{\rm vocab}}}}\). The right (key-side) singular vector of this component is monotonically increasing as we increase the size of the key token, with (\(1/\sqrt{d}\)-scaled) pre-softmax attention increasing by an average of \(1.236\pm 0.056\) when the key token increases by 1.8

Footnote 8: This implies that the ratio of attention paid to token \(t\) and \(t-1\) is approximately \(\exp(1.236)\approx 3.442\).

In comparison, each \(1/\sqrt{d}\)-scaled entry of the position-dependent QK component \(\rm EQKP\) has negligible size (average \(0.31\pm 0.18\)), suggesting that \(\rm EQKP\) is unimportant to the functioning of the model.

Figure 2: The models in our setting implement Max-of-\(K\) by attending exponentially more to larger tokens and copying the attended-to tokens (Section 3.1).

We confirm this by zero ablating \(\mathrm{EQKP}\), which changes the models' accuracies from \(0.9992\pm 0.0015\) to \(0.9993\pm 0.0011\). Combined with our interpretation of \(\mathrm{EQKE}\), this implies that the attention pattern of the model depends only on the token values and not the ordering of the sequence.

**OV circuit** Then, by qualitatively examining the position-independent OV component \(\mathrm{EVOU}\), we see that it has large positive entries along the diagonal. In fact, the entry along the diagonal is the largest in the row for all rows corresponding to \(t>6.6\pm 1.2\). Since each entry in the sequence is uniformly sampled and \(d_{\mathrm{vocab}}=64\), this means that \(\mathrm{EVOU}\) is a good approximation for the identity matrix for all but \(\approx(7/64)^{4}\approx 1.2\times 10^{-2}\,\%\) of the sequences.

As with the position-dependent QK component, the position-dependent OV component \(\mathrm{PVOU}\) also has negligible size and is unimportant to model performance. Taken together with the above results on \(\mathrm{EVOU}\), this suggests that the attention head copies the tokens it attends to.

**Direct path** As with the two position-dependent components, the entries in \(\mathrm{EU}\) have small absolute magnitude \(2.54\pm 0.20\),9 and contribute negligibly to model performance.

Footnote 9: For comparison, the average off-diagonal element of \(\mathrm{EVOU}\) is \(21.68\pm 0.83\) below the corresponding diagonal element.

## 4 Proofs of model performance

In this section we describe intuitions for three categories of proof that are developed around different mechanistic interpretations and methods for using the interpretations. The strategies result in proofs of different complexities with varying bound tightness (Table 1). We provide detailed theorem statements, proofs, algorithms, and explanations of proof search in Appendices C, D, E, F, and G.

Our theorem statements for \(Q\) will all be of the form

\[\forall\mathcal{M}^{\prime},C_{\text{\emph{specific strategy}}}(\mathcal{M}^{ \prime})\leq\mathbb{E}_{\mathbf{t}\sim\mathcal{D}|_{X}}f(t_{\max},\mathcal{M} ^{\prime}(\mathbf{t})).\]

We leave implicit the traces of running \(C_{\text{\emph{specific strategy}}}\) on our specific models to give the overall theorem. We report the computational complexity or estimated FLOPs of running \(C_{\text{\emph{specific strategy}}}\) as approximations for our proof lengths.

### The brute-force baseline

We start by considering the brute-force proof (Appendix D), which treats the model as a black box and evaluates it on all possible sequences.10 However, this proof strategy has bad asymptotic complexity and is untenable for larger models and larger input distributions. So in subsequent sections, we use knowledge of the model drawn from the interpretation in Section 3.1 to derive more compact proofs.

Footnote 10: Appendix A.10 discusses how to compute the “brute-force” accuracy of a model on an infinite distribution.

### A cubic proof

Next, we use the fact that the model is composed of the direct path and the QK and OV circuits (Section 3) to decrease the number of sequences that we need to consider, and the fact that only the position-independent components \(\mathrm{EQKE}\) and \(\mathrm{EVOU}\) contribute meaningfully to performance (Section 3.1) to pessimize over sequence ordering.

First, let a pure sequence \(\xi\) be a sequence with at most three distinct tokens: the max token \(t_{\max}\), the final token \(t_{\mathrm{query}}\leq t_{\max}\), and optionally a third token \(t^{\prime}<t_{\max}\), and let \(\Xi^{\mathrm{pure}}\) be the set of all pure sequences in \(X\).11 For a given input sequence \(\mathbf{t}\), define the adjacent pure sequences \(\mathrm{Adj}(\mathbf{t})\) as the set of sequences that share the same max and query token, and only take on values in \(\mathbf{t}\):

Footnote 11: In Section 4.3, we will consider a smaller set of “pure sequences”.

\[\mathrm{Adj}(\mathbf{t})=\left\{\xi\in\Xi^{\mathrm{pure}}\,\Big{|}\max_{i}\xi _{i}=t_{\max},\;\xi_{\mathrm{query}}=t_{\mathrm{query}},\forall i<n_{ \mathrm{ctx}},\;\xi_{i}\in\mathbf{t}\right\}\]

Using the convexity of softmax and the fact that the model contains three paths, we can show that one-layer attention-only transformers satisfies a variant of the following convexity property: for a given \(\mathbf{t}\), if \(\mathcal{M}(\xi)\) is correct for all \(\xi\in\mathrm{Adj}(\mathbf{t})\), then \(\mathcal{M}(\mathbf{t})\) is correct. That is, for these transformers, we can bound the accuracy on all sequences by evaluating \(\mathcal{M}\) on only the \(O(d_{\mathrm{vocab}}{}^{3}(n_{\mathrm{ctx}}-1)!)\)pure sequences. This allows us to bound the accuracy of our actual \(\mathcal{M}\) on all \({d_{\mathrm{vocab}}}^{n_{\mathrm{ctx}}}\) sequences, while evaluating it on \(O({d_{\mathrm{vocab}}}^{3}(n_{\mathrm{ctx}}-1)!)\) sequences.

We can reduce the number of sequences that we need to evaluate by pessimizing over the order of a sequence. For a given tuple of \((t_{\mathrm{max}},t_{\mathrm{query}},t^{\prime})\), there are \((n_{\mathrm{ctx}}-1)!\) pure sequences, corresponding to the permutations of the tuple. Pessimizing over the order of sequences reduces the number of sequences to consider for each \((t_{\mathrm{max}},t_{\mathrm{query}},t^{\prime})\) tuple to the number of \(t^{\prime}\) in the pure sequence, and the total number of sequences to \(O({d_{\mathrm{vocab}}}^{3}n_{\mathrm{ctx}})\). By precomputing the five component matrices \(\mathrm{EU}\), \(\mathrm{EQKE}\), \(\mathrm{EQKP}\), \(\mathrm{EVOU}\), \(\mathrm{PVOU}\) and cleverly caching intermediate outputs, we can reduce the additional work of each sequence to the \(O(n_{\mathrm{ctx}})\) required to compute the softmax over \(n_{\mathrm{ctx}}\) elements, resulting in asymptotic complexity \(O({d_{\mathrm{vocab}}}^{3}{n_{\mathrm{ctx}}}^{2})\) (Theorem 12, additional details in Appendix E).

### Sub-cubic proofs

We now consider proofs that are more compact than \(O({d_{\mathrm{vocab}}}^{3})\). These require avoiding iteration over any set of size \(O({d_{\mathrm{vocab}}}^{3})\) (e.g. the set of pure sequences) and performing operations that take \(O({d_{\mathrm{vocab}}})\) time on each of \(O({d_{\mathrm{vocab}}}^{2})\) combinations. Unfortunately, some methods of avoiding these operations can lead to vacuous bounds (i.e. accuracy lower bounds near \(0\%\)). In order to recover non-vacuous bounds, we introduce two tricks: the "mean+diff trick" to better approximate the sum of two components with unequal variance, and the "max row diff trick" to improve upon the low-rank approximations for \(\mathrm{EU}\) and \(\mathrm{EQKE}\). We consider applying variants of these tricks at different locations in the naive subcubic proof, leading to 100 distinct subcubic proof strategies. See Appendix G.2 for a formal description of these strategies.

#### 4.3.1 Removing cubic-time computations

**Reducing the number of cases by pessimizing over sufficiently small tokens** Previously, we considered \(\Theta({d_{\mathrm{vocab}}}^{3}n_{\mathrm{ctx}})\) pure sequences \(\xi\), with \(\xi\) parameterized by \((t_{\mathrm{max}},t_{\mathrm{query}},t^{\prime},c)\). Recall from our mechanistic interpretation in Section 3.1 that the pre-softmax attention paid from \(t_{\mathrm{query}}\) to a key token \(t^{\prime}\) is broadly invariant in \(t_{\mathrm{query}}\) and increases roughly linearly with the size of \(t^{\prime}\). This allows us to pessimize over the OV circuit over all "sufficiently small" tokens.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**Description** & **Complexity Cost** & **Bound** & **Est.** & **Unexplained** \\
**of Proof** & & & **FLOPs** & **Dimensions** \\ \hline Brute force & \(\mathcal{O}(v^{k+1}kd)\) & \(0.9992\pm 0.0015\) & \(2^{47}\) & \(2^{30}\) \\ \hline Cubic & \(\mathcal{O}(v^{3}k^{2})\) & \(0.9531\pm 0.0087\) & \(2^{25}\) & \(2^{14}\) \\ \hline Sub-cubic & \(\mathcal{O}(v^{2}\cdot k^{2}+v^{2}\cdot d)\) & \(0.702\pm 0.033\) & \(2^{21}\) & \(2^{13}\) \\   w/o mean+diff & & \(0.349\pm 0.080\) & \(2^{21}\) & \(2^{13}\) \\ \hline Low-rank QK & \(\mathcal{O}(v^{2}k^{2}+\underbrace{vd^{2}}_{\mathrm{OK}}+\underbrace{v^{2}d}_{ \mathrm{EUKOV}})\) & \(0.675\pm 0.035\) & \(2^{22}\) & \(2^{12}\) \\   SVD only & & \(0.284\pm 0.072\) & \(2^{22}\) & \(2^{12}\) \\ \hline Low-rank EU & \(\mathcal{O}(v^{2}k^{2}+\underbrace{vd}_{\mathrm{EU}}+\underbrace{v^{2}d}_{ \mathrm{OKKOV}})\) & \(0.633\pm 0.062\) & \(2^{21}\) & \(2^{13}\) \\   SVD only & & \(0.610\pm 0.060\) & \(2^{21}\) & \(2^{13}\) \\   SVD only & & \(0.38\pm 0.06\) & \(2^{22}\) & \(2^{13}\) \\ \hline Quadratic QK & \(\mathcal{O}(v^{2}k^{2}+\underbrace{vd}_{\mathrm{OK}}+\underbrace{v^{2}d}_{ \mathrm{EUKOV}})\) & \(0.316\pm 0.037\) & \(2^{21}\) & \(2^{12}\) \\ \hline Quadratic QK\&EU & \(\mathcal{O}(v^{2}k^{2}+\underbrace{vd}_{\mathrm{OK}\mathrm{REU}}+\underbrace{v^{2}d}_{ \mathrm{OV}})\) & \(0.283\pm 0.036\) & \(2^{21}\) & \(2^{13}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: We report the proof complexity, accuracy bound, and estimated flops required (Equation 2), as well as unexplained dimensionality (Section 5). We round the FLOP and unexplained dimension counts to the closest power of 2, and report the mean/standard deviation of the bound averaged across all 151 models. As we include more aspects of the mechanistic interpretation (reflected by a lower number of unexplained dimensions), we get more compact proofs (in terms of both asymptotic complexity and FLOPs), albeit with worse bounds. For space reasons, we use \(k:=n_{\mathrm{ctx}}\), \(d:=d_{\mathrm{model}}\), and \(v:=d_{\mathrm{vocab}}\).

More formally, suppose we are given some gap \(g\in\mathbb{N}\). For each pure sequence \(\xi\) with max token \(t_{\rm max}\), query token \(t_{\rm query}\), such that \(t_{\rm query}\leq t_{\rm max}-g\), and \(c\) copies of the third token type \(t^{\prime}\leq t_{\rm max}-g\), we pessimally ablate the OV circuit over the set \(\Xi^{\rm pure}(t_{\rm max},t_{\rm query},c;g)\) of pure sequences \(\xi^{\prime}\) with the same max and query tokens and \(c\) copies of the third token type \(t^{\prime}\). If the model gets all sequences in \(\Xi^{\rm pure}(t_{\rm max},t_{\rm query},c;g)\) correct, then we can conclude that it gets \(\xi\) correct, otherwise, we treat the model as having gotten \(\xi\) wrong. This means that it suffices to only consider the \(O({d_{\rm vocab}}^{2}n_{\rm ctx})\) pessimal pure sequences of each of the \(O({d_{\rm vocab}}^{2}n_{\rm ctx})\) sets of the form \(\Xi^{\rm pure}(t_{\rm max},t_{\rm query},c;g)\).

Decoupling and pessimizing computations that require \(O({d_{\rm vocab}}^{3})\) computationsMany parts of our cubic certificate require iterating through \(O({d_{\rm vocab}}^{2})\) cases parameterized by \((t_{\rm max},t_{\rm query})\) or \((t_{\rm max},t^{\prime})\). For example, as part of the pessimization procedure over pure sequences, for each of the \({d_{\rm vocab}}\) possible values of \(t_{\rm max}\), we need to consider the relative effects on the \({d_{\rm vocab}}\)-sized logits of attending to each of the \(O({d_{\rm vocab}})\) other tokens \(t^{\prime}<t_{\rm max}\), and for each \(t_{\rm max}\) and \(t_{\rm query}\), we need to check that the contribution of the direct path on logits \(x_{\rm query}{\rm EU}\) is not sufficiently large as to overwhelm the contribution from \(x_{\rm max}{\rm EVOU}\). We independently pessimize over each of these components over one of the \({d_{\rm vocab}}\)-sized axes: for example, instead of computing \(x_{\rm max}{\rm EVOU}+x_{\rm query}{\rm EU}\) for each \(t_{\rm max}\), \(t_{\rm query}\) pair, we first pessimally ablate the direct path along the query token (which takes \(O({d_{\rm vocab}}^{2})\) time as it does not depend on the \(t_{\rm max}\), and then consider the sum \(x_{\rm max}{\rm EVOU}+\max_{x^{\prime}}{x^{\prime}}{\rm EU}\). Since this sum no longer depends on \(t_{\rm query}\), we only need to perform it \(O({d_{\rm vocab}})\) times, for a total cost of \(O({d_{\rm vocab}}^{2})\).

Low rank approximations to \({\rm EQKE}\) and \({\rm EU}\) Recall from Section 3.1 that \({\rm EQKE}\) is approximately rank 1, where the sole direction of variation is the size of the key token. By computing only the low rank approximation to \({\rm EQKE}\), we can more cheaply compute the most significant component of the behavior in the QK circuit. To bound the remaining error, we can use the fact that after pulling off the first principal component from each of the four matrices we multiply, very little structure remains.

We can find the rank 1/2 approximations by performing SVD on \({\rm EQKE}\). We can efficiently compute the SVD in \(\mathcal{O}({d_{\rm vocab}}{d_{\rm model}}^{2})\) time by using the fact that \({\rm EQKE}\) can be written as the product of a \({d_{\rm vocab}}\times{d_{\rm model}}\) matrix and a \({d_{\rm model}}\times{d_{\rm vocab}}\) matrix. This allows us to avoid performing the \(\mathcal{O}({d_{\rm vocab}}^{2}{d_{\rm model}})\)-cost matrix multiplications to explicitly compute \({\rm EQKE}\).

Similarly, we can more efficiently check that the direct path \({\rm EU}\) contributes negligibly to the model outputs, by using SVD to decompose \({\rm EU}\) into a sum of rank 1 products (which we can evaluate exactly) and a high-rank error term that we can cheaply bound.

#### 4.3.2 Additional subcubic proof strategies

Tighter bounds for sums of variables with unequal variance via the "mean+diff trick"Suppose we want to lower bound the minimum of the sum of two functions over three variables \(h(x,y,z)=f(x,y)+g(y,z)\), while only iterating over two variables at a time. The naive way is to minimize \(f(x,y)\) and \(g(x,y)\) independently:

\[\min_{x,y,z}h(x,y,z)\geq\min_{x,y}f(x,y)+\min_{y,z}g(y,z)\]

Here, the error comes from setting the \(y\)s in \(f\) and \(g\) to different values. But in cases where \(g(y,z)\) varies significantly with \(y\) and only slightly with \(z\), rewriting \(g\) as a sum of a component that is independent of \(z\) (only varying along \(y\)), and one that depends on \(z\), yields a better lower bound:

\[\min_{x,y,z}h(x,y,z)\geq\min_{x,y}\left(f(x,y)+\mathbb{E}_{z}^{\prime}g(y,z^{ \prime})\right)+\min_{y,z}(g(y,z)-\mathbb{E}_{z}^{\prime}g(y,z^{\prime}))\]

This estimate will have error at most \(\varepsilon\), while the naive estimator can have arbitrarily large error. We refer to this rewrite as the "mean+diff trick".12 From the mechanistic interpretation in Section 3.1, we know that some of the components barely vary among one or more axes. So we can apply the mean+diff trick to get tighter lower bounds.

Footnote 12: In fact, this is the motivation behind the standard rewrites of QK and OV into position-independent and position-dependent components (Section 3).

Avoiding matrix multiplications using the "max row-diff trick"Using properties of linear algebra, we derive a cheap approximation to the max row-diff for the product of matrices \(AB\) in terms of the product of the max row-diff of \(B\) and the absolute value of \(A\), which we deem the "max row-diff"trick. We apply this trick to get a better cheap bound on the error terms of low-rank approximations, without having to multiply out the full matrices. See Appendices G.2.2, and F for more details.

## 5 Results

We run each of \(151\) transformers on the various proof strategies of different asymptotic complexity, and analyze these proofs to empirically examine the relationship between proof length, bound tightness, and degree of understanding. For each proof on each transformer, we approximate the length of the proof by estimating the number of FLOPs used, and plot this against the ratio of certified bound the true accuracy \(b/\bar{s}\) (Equation 2) in Figure 3. There exists a clear trade-off between bound tightness and compactness of the proof - more compact proofs yield looser bounds, and tighter bounds are associated with more expensive proofs.

### Compact proofs both require and provide mechanistic understanding

**Quantifying mechanistic understanding using unexplained dimensionality** We first quantify the amount of mechanistic understanding used in a proof by measuring its **unexplained dimensionality** - the number of free parameters required to fully describe model behavior, assuming the structural assumptions of the proof are correct. More detailed interpretations leave fewer free parameters needing to be filled in via empirical observation (Appendix A.5). In Figure 5, we plot these axes and find a suggestive correlation: proofs based on less mechanistic understanding are longer.

**More mechanistic understanding allows for more compact proofs** In addition to the constructions in Section 4, the parts of proofs we were unable to compact seem to correspond to components that we do not mechanistically understand. For example, we could not cheaply bound the behavior of \(\mathrm{EVOU}\) without multiplying out the matrices, and this seems in part because we do have a mechanistic understanding of how \(\mathrm{EVOU}\) implements low-rank copying.

**Compact proofs seem to provide understanding** By examining compact proofs, we can extract understanding about the model. For example, the fact that replacing each row of \(\mathrm{EU}\) with its average across rows has little effect on the bound implies that \(\mathrm{EU}\) does not vary much based on \(t_{\mathrm{query}}\).

### Proof length vs. bound tightness trade-off is modulated by faithfulness of interpretation

**Compact proofs are less faithful to model internals** To derive more compact proofs, we use our mechanistic understanding to simplify the model computation in ways that diverge from the original model internals. For example, in some subcubic proofs (Section 4.3), we approximate \(\mathrm{EQKE}\) with a rank-1 approximation corresponding to the "size direction". However, while other components are small, they're nonzero; this approximation harms model internals.

**Less faithful interpretations lead to worse bounds on performance** To confirm that faithfulness of understanding affects the tightness of bound independent of proof length, we plot the normalized accuracy bound of subcubic proofs that perform a rank-1 approximation to \(\mathrm{EQKE}\), versus the ratio of the first two singular components. A larger ratio between the components implies that the rank-1 approximation is more faithful. In Figure 4, we see a positive correlation between the two axes: when the interpretation is more faithful, the bounds are tighter, even at a fixed proof length.

Figure 3: For each of the proofs in Section 4, we plot the number of FLOPs used to compute the certificate, as well as the normalized accuracy lower-bound (\(b/\bar{s}\)). The brute-force proof (Section 4.1) computes the exact performance uses orders of magnitude more compute than other approaches. The cubic proof (Section 4.3) uses a small amount of mechanistic understanding and less compute, while still retaining good accuracy lower bounds. Finally, subcubic proofs (Section 4.3) require the entirety of the mechanistic interpretation of the model to attain non-vacuous bounds; this understanding allows us to further reduces compute costs, but we still achieve worse bounds. See Appendix H.2.1 for a detailed description of the various proof strategies.

### Compounding structureless noise is a big challenge for compacting global-behavior proofs

**Pessimal error terms compound in the absence of known structure** The rank-1 approximation of \(\mathrm{EQKE}\) has small error. However, when making rank-1 approximations of each of the constituent matrices \(E,Q,K\), pessimizing over the worst way to composing the individual small error terms leads to a bound on the error term of \(\mathrm{EQKE}\) that is orders of magnitude larger than the actual error term. Because we don't understand how the matrices compose in a way that doesn't cause errors to compound (without just multiplying out the matrices), this approximation leads to a trivial bound on performance (Appendix G.2.5). We speculate that in many cases, there is no short human-interpretable description for why random noise or approximation errors do not compound across layers of neural networks (e.g., see the error correction results on _randomly initialized_ neural networks from Hanni et al. (2018)), and thus that compounding structureless errors may be an issue in practice.

## 6 Related Work

**Generalization Bounds** Prior work in the PAC-Bayes framework (Pedregosa et al., 2017; Goyal et al., 2018; Goyal et al., 2018) proves generalization bounds over learning procedures, which are similar to the global performance bounds we consider in this work. These proofs tend to provide statistical guarantees (Pedregosa et al., 2017; Goyal et al., 2018) about the outputs of a known stochastic training procedure, while we seek to bound the performance of particular trained models.

**Formally verifying neural networks** Most prior work formally verifies neural networks either via model checking (Pedregosa et al., 2017; Goyal et al., 2018) or by relaxing the problem setting and taking an automated theorem proving approach (Goyal et al., 2018; Goyal et al., 2018; Goyal et al., 2018; Goyal et al., 2018) to verify _local_ robustness properties. These proof strategies tend to be derived by examining only the network architecture. We take an approach more akin to interactive theorem proving (Pedregosa et al., 2018) and verify _global_ performance properties by reverse-engineering the weights.

**Mechanistic Interpretability** Finally, mechanistic interpretability is the subfield of the broader field of understanding model internals (Pedregosa et al., 2017), which is too large to faithfully summarize. Our work takes most direct inspiration from efforts to deeply understand how either toy models (Pedregosa et al., 2017; Goyal et al., 2018; Goyal et al., 2018) or small pretrained text transformers (Pedregosa et al., 2017; Goyal et al., 2018) implement algorithmic tasks, generally by performing ablations and SVD. In contrast, we formally prove that a transformer implements an algorithm.

Figure 4: We plot the normalized accuracy bound versus the ratio of first and second singular values of \(\mathrm{EQKE}\), for various types of subcubic proofs that depend on a rank-1 approximation \(\mathrm{EQKE}\). For each class of proof, the closer \(\mathrm{EQKE}\) is to rank-1, the tighter the accuracy bound. This suggests that more faithful interpretations lead to tighter bounds even holding proof length fixed. See Appendix H.2.2 for a detailed description of proof strategies.

Figure 5: We plot, for each proof, the approximate number of flops required to evaluate the proof, versus the unexplained dimensionality (Section 5.1). More mechanistic understanding leaves fewer dimensions unexplained. We observe that more compact proofs seem to leave fewer unexplained dimensions, which is indicative of the relationship of mechanistic understanding and compact proofs. See Appendix H.2.1 for more detail.

Nichani et al. [39] proves that, in a significantly simplified 2-layer, 1-head attention-only transformer model and for the task of in-context bigram statistics, gradient descent will create induction heads [40]. Our results concern transformers with fixed weights. In concurrent work, Michaud et al. [34] use techniques inspired by mechanistic interpretability to perform automated program synthesis on 2-dimensional RNNs, while our work works with significantly larger transformer models.

## 7 Conclusion and Future Work

**Summary** In this work, we used a Max-of-\(K\) setting to prototype the use of mechanistic interpretability to derive compact proofs of model behavior. Using varying amounts of understanding, we derived more efficient proof computations lower bounding model accuracy. We found preliminary evidence that mechanistic understanding can compactify proofs. Moreover, we observed that the tightness of the lower bound offered by various proof strategies can be used to grade the faithfulness our mechanistic interpretation. Finally, we identified compounding structureless errors as a key obstacle to deriving compact proofs of model behavior.

**Limitations and future work** We study one-layer attention-only transformers on a toy algorithmic task. Future work should explore the viability of deriving proofs via interpretability using larger models featuring MLPs or layernorm on more complex domains. In addition, we were unable to significantly compact the part of the proof involving the OV circuit, which future work can explore. The proofs we explored in this work also did not lead to qualitatively novel insights; future work may be able to derive such insights with improved techniques. Finally, future work can address the problem of compounding structureless errors, perhaps by relaxing from worst-case pessimal ablations to typical-case heuristic guarantees [8].

## Acknowledgments and Disclosure of Funding

We are immensely grateful to Paul Christiano for providing the initial support for this project and for his invaluable research advice, encouragement, and feedback throughout its duration.

Additionally, we are thankful for clarifying discussions and feedback from Jacob Hilton, Matthew Coudron, Adria Garriga-Alonso, Aryan Bhatt, Leo Gao, Jenny Nitishinskaya, Somsubhro Bagchi, Gabriel Wu, Erik Jenner, Ryan Greenblatt, Ronak Mehta, Louis Jaburi and many others. Louis Jaburi in particular contributed the text of the final proof of Theorem 11 in Appendix E.

We are indebted to various organizations for their support: **Alignment Research Center** for funding this project and making it possible at all; **Mentorship for Alignment Research Students** (MARS) program of the **Cambridge AI Safety Hub** (CAISH) for setting up the collaboration between a subset of authors, and providing funding for compute and in-person research sprints; **Constellation** and **FAR Labs** for hosting a subset of the authors and providing an excellent research environment, including as part of the Visiting Fellows Program and Astra Fellowship.

## Author Contributions

**Jason Gross** led the project, including managing the team and conceptualizing the proofs approach. He ran the Max-of-4 experiments, devised the proof strategies, and wrote up the formal proofs. He worked on various case studies and developed general methodology for computing complexity and length bounds for proofs. He also developed the particular convex relaxations presented in the paper.

**Rajashree Agrawal** was invaluable in steering the direction of the project, including contributing to the preliminary experiment on Max-of-2 and developing the pessimal ablation approach. She worked on framing the results, and contributed text to the paper.

**Thomas Kwa** and **Euan Ong** extended the preliminary experiments to larger values of \(k\) and contributed substantially to the cubic proof. **Chun Hei Yip**, **Alex Gibson**, and **Soufiane Noubir** worked on case studies other than the Max-of-\(K\) task and informed discussion on proof complexity.

**Lawrence Chan** spearheaded the writing of the paper, including turning informal claims into formal theorem statements, creating figures, and writing the core text. He also developed the unexplained dimensionality metric for clarifying the takeaway of the paper.

## References

* Akbarpour et al. [2010] Behzad Akbarpour, Amr Abdel-Hamid, Sofiene Tahar, and John Harrison. Verifying a synthesized implementation of IEEE-754 floating-point exponential function using HOL. _The Computer Journal_, 53:465-488, May 2010. doi: 10.1093/comjnl/bxp023.
* Akyurek et al. [2022] Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models, 2022.
* Appel and Kellison [2024] Andrew Appel and Ariel Kellison. VCFloat2: Floating-point error analysis in Coq. In _Proceedings of the 13th ACM SIGPLAN International Conference on Certified Programs and Proofs_, CPP 2024, pages 14-29, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400704888. doi: 10.1145/3636501.3636953.
* Boldo and Melquiond [2011] Sylvie Boldo and Guillaume Melquiond. Flocq: A unified library for proving floating-point algorithms in Coq. In _2011 IEEE 20th Symposium on Computer Arithmetic_, pages 243-252, July 2011. doi: 10.1109/ARITH.2011.40.
* Brown et al. [2020] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165.
* Chan et al. [2022] Lawrence Chan, Adria Garriga-Alonso, Nicholas Goldwosky-Dill, Ryan Greenblatt, Jenny Nitishinskaya, Ansh Radhakrishnan, Buck Shlegeris, and Nate Thomas. Causal scrubbing, a method for rigorously testing interpretability hypotheses. _AI Alignment Forum_, 2022. URL https://www.alignmentforum.org/posts/JvZhhzyChu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing.
* Cheng et al. [2017] Chih-Hong Cheng, Georg Nuhrenberg, and Harald Ruess. Maximum resilience of artificial neural networks. In _Automated Technology for Verification and Analysis: 15th International Symposium, ATVA 2017, Pune, India, October 3-6, 2017, Proceedings 15_, pages 251-268. Springer, 2017.
* Christiano et al. [2022] Paul Christiano, Eric Neyman, and Mark Xu. Formalizing the presumption of independence. _arXiv preprint arXiv:2211.06738_, 2022. doi: 10.48550/arxiv.2211.06738.
* Chughtai et al. [2023] Bilal Chughtai, Lawrence Chan, and Neel Nanda. A toy model of universality: Reverse engineering how networks learn group operations, 2023.
* Clarke et al. [2012] Edmund M. Clarke, William Klieber, Milos Novacek, and Paolo Zuliani. _Model Checking and the State Explosion Problem_, pages 1-30. Springer Berlin Heidelberg, Berlin, Heidelberg, 2012. ISBN 978-3-642-35746-6. doi: 10.1007/978-3-642-35746-6_1. URL https://doi.org/10.1007/978-3-642-35746-6_1.
* Dalrymple et al. [2024] David Dalrymple, Joar Skalse, Yoshua Bengio, Stuart Russell, Max Tegmark, Sanjit Seshia, Steve Omohundro, Christian Szegedy, Ben Goldhaber, Nora Ammann, et al. Towards guaranteed safe AI: A framework for ensuring robust and reliable AI systems. _arXiv preprint arXiv:2405.06624_, 2024.
* Dziugaite and Roy [2017] Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI 2016, August 11-15, 2017, Sydney, NSW, Australia, 2017.
* Elhage et al. [2021] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. _Transformer Circuits Thread_, 2021. URL https://transformer-circuits.pub/2021/framework/index.html.

* [14] Martin H. Escardo. Synthetic topology of data types and classical spaces. _Electronic Notes in Theoretical Computer Science_, 87:21-156, November 2004.
* [15] Martin H. Escardo. Infinite sets that admit fast exhaustive search. In _Proceedings of the 22nd Annual IEEE Symposium on Logic in Computer Science (LICS 2007)_, Wroclaw, Poland, July 2007.
* [16] Martin H. Escardo. Seemingly impossible functional programs, 2007. URL https://math.andrej.com/2007/09/28/seemingly-impossible-functional-programs/. Accessed: 2024-05-15.
* [17] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and M. Vechev. AI2: Safety and robustness certification of neural networks with abstract interpretation. In _2018 IEEE Symposium on Security and Privacy (SP)_, pages 3-18, Los Alamitos, CA, USA, May 2018. IEEE Computer Society. doi: 10.1109/SP.2018.00058.
* [18] Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval bound propagation for training verifiably robust models. _arXiv preprint arXiv:1810.12715_, 2018.
* [19] Jason S. Gross. _Performance Engineering of Proof-Based Software Systems at Scale_. PhD thesis, Massachusetts Institute of Technology, February 2021. URL https://dspace.mit.edu/handle/1721.1/130763.
* [20] Michael Hanna, Ollie Liu, and Alexandre Variengien. How does GPT-2 compute greater-than. _Interpreting mathematical abilities in a pre-trained language model_, 2:11, 2023.
* [21] Kaarel Hanni, Jake Mendel, Dmitry Vaintrob, and Lawrence Chan. Mathematical models of computation in superposition. In _ICML 2024 Workshop on Mechanistic Interpretability_, 2024. URL https://openreview.net/forum?id=OcVJP8kClR.
* [22] John Harrison, Josef Urban, and Freek Wiedijk. History of interactive theorem proving. In _Handbook of the History of Logic_, volume 9, pages 135-214. Elsevier, 2014.
* [23] Stefan Heimersheim and Neel Nanda. How to use and interpret activation patching. _arXiv preprint arXiv:2404.15255_, 2024.
* [24] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_, 2022.
* [25] Xiaowei Huang, Daniel Kroening, Wenjie Ruan, James Sharp, Youcheng Sun, Emese Thamo, Min Wu, and Xinping Yi. A survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarial attack and defence, and interpretability. _arXiv preprint arXiv:1812.08342_, 2018.
* [26] Xiaowei Huang, Wenjie Ruan, Wei Huang, Gaojie Jin, Yi Dong, Changshun Wu, Saddek Bensalem, Ronghui Mu, Yi Qi, Xingyu Zhao, Kaiwen Cai, Yanghao Zhang, Sihao Wu, Peipei Xu, Dengyu Wu, Andre Freitas, and Mustafa A. Mustafa. A survey of safety and trustworthiness of large language models through the lens of verification and validation, 2023.
* [27] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.
* [28] Guy Katz, Clark Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer. Reluplex: An efficient SMT solver for verifying deep neural networks. In _Computer Aided Verification: 29th International Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part I 30_, pages 97-117. Springer, 2017.
* [29] A. E. Kellison, A. W. Appel, M. Tekriwal, and D. Bindel. LAProof: A library of formal proofs of accuracy and correctness for linear algebra programs. In _2023 IEEE 30th Symposium on Computer Arithmetic (ARITH)_, pages 36-43, Los Alamitos, CA, USA, September 2023. IEEE Computer Society. doi: 10.1109/ARITH58626.2023.00021.

* Klein et al. [2009] Gerwin Klein, Kevin Elphinstone, Gernot Heiser, June Andronick, David Cock, Philip Derrin, Dhammika Elkaduwe, Kai Engelhardt, Rafal Kolanski, Michael Norrish, Thomas Sewell, Harvey Tuch, and Simon Winwood. seL4: Formal verification of an OS kernel. In _Proceedings of the ACM SIGOPS 22nd Symposium on Operating Systems Principles_, SOSP '09, pages 207-220, New York, NY, USA, 2009. Association for Computing Machinery. ISBN 9781605587523. doi: 10.1145/1629575.1629596. URL https://doi.org/10.1145/1629575.1629596.
* Leroy [2009] Xavier Leroy. A formally verified compiler back-end. _Journal of Automated Reasoning_, 43:363-446, 2009.
* Li [2020] Chuan Li. OpenAI's GPT-3 language model: A technical overview, June 2020. URL https://lambdalabs.com/blog/demystifying-gpt-3. Lambda Labs Blog, accessed October 30, 2024.
* McKinney [2010] Wes McKinney. Data Structures for Statistical Computing in Python. In Stefan van der Walt and Jarrod Millman, editors, _Proceedings of the 9th Python in Science Conference_, pages 56-61, 2010. doi: 10.25080/Majora-92bf1922-00a.
* Michaud et al. [2024] Eric J. Michaud, Isaac Liao, Vedang Lad, Ziming Liu, Anish Mudide, Chloe Loughridge, Zifan Carl Guo, Tara Rezaei Kheirkhah, Mateja Vukelic, and Max Tegmark. Opening the AI black box: program synthesis via mechanistic interpretability, 2024.
* Mirman et al. [2018] Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for provably robust neural networks. In _International Conference on Machine Learning_, pages 3578-3586. PMLR, 2018.
* Nagarajan and Kolter [2019] Vaishnavh Nagarajan and J. Zico Kolter. Uniform convergence may be unable to explain generalization in deep learning. _Advances in Neural Information Processing Systems_, 32, 2019.
* Nanda and Bloom [2022] Neel Nanda and Joseph Bloom. TransformerLens. https://github.com/TransformerLensOrg/TransformerLens, 2022.
* Nanda et al. [2023] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. _arXiv preprint_, 2023. doi: 10.48550/arXiv.2301.05217.
* Nichani et al. [2024] Eshaan Nichani, Alex Damian, and Jason D. Lee. How transformers learn causal structure with gradient descent. _arXiv preprint_, 2024. doi: 10.48550/arXiv.2402.14735.
* Olsson et al. [2022] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. _Transformer Circuits Thread_, 2022. URL https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.
* Paszke et al. [2019] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.
* Inc [2015] Plotly Technologies Inc. Collaborative data science, 2015. URL https://plot.ly.
* Raghunathan et al. [2018] Aditi Raghunathan, Jacob Steinhardt, and Percy S. Liang. Semidefinite relaxations for certifying robustness to adversarial examples. _Advances in neural information processing systems_, 31, 2018.
* Ramananandro et al. [2016] Tahina Ramananandro, Paul Mountcastle, Beno't Meister, and Richard Lethin. A unified Coq framework for verifying C programs with floating-point computations. In _Proceedings of the 5th ACM SIGPLAN Conference on Certified Programs and Proofs_, CPP 2016, pages 15-26, New York, NY, USA, 2016. Association for Computing Machinery. ISBN 9781450341271. doi: 10.1145/2854065.2854066.

* Rauker et al. [2022] Tilman Rauker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell. Toward transparent AI: A survey on interpreting the inner structures of deep neural networks. In _First IEEE Conference on Secure and Trustworthy Machine Learning_, 2022. doi: 10.48550/arxiv.2207.13243.
* Reed et al. [2022] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent, 2022. URL https://arxiv.org/abs/2205.06175.
* Rogozhnikov [2022] Alex Rogozhnikov. Einops: Clear and reliable tensor manipulations with Einstein-like notation. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=oapKSVM2bcj.
* Seshia et al. [2019] Sanjit A. Seshia, Dorsa Sadigh, and S. Shankar Sastry. Toward verified artificial intelligence making AI more trustworthy with a formal methods-based approach to AI system verification and validation.
* Simpson [1998] Alex K. Simpson. Lazy functional algorithms for exact real functionals. In Lubos Brim, Jozef Gruska, and Jiri Zlatuska, editors, _Mathematical Foundations of Computer Science 1998_, pages 456-464, Berlin, Heidelberg, 1998. Springer Berlin Heidelberg. ISBN 978-3-540-68532-6.
* Singh et al. [2019] Gagandeep Singh, Timon Gehr, Markus Puschel, and Martin Vechev. An abstract domain for certifying neural networks. _Proc. ACM Program. Lang._, 3(POPL), January 2019. doi: 10.1145/3290354.
* Szegedy et al. [2013] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. _arXiv preprint arXiv:1312.6199_, 2013.
* Virtanen et al. [2020] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stefan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C. J. Carey, Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental algorithms for scientific computing in Python. _Nature Methods_, 17:261-272, 2020. doi: 10.1038/s41592-019-0686-2.
* Oswald et al. [2023] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In _International Conference on Machine Learning_, pages 35151-35174. PMLR, 2023.
* Wang et al. [2022] Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. _arXiv preprint_, 2022. doi: 10.48550/arXiv.2211.00593.
* Waskom [2021] Michael L. Waskom. seaborn: statistical data visualization. _Journal of Open Source Software_, 6(60):3021, 2021. doi: 10.21105/joss.03021.
* Wei et al. [2022] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. _arXiv preprint arXiv:2206.07682_, 2022.
* Wong and Kolter [2018] Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In _International conference on machine learning_, pages 5286-5295. PMLR, 2018.
* Zhang et al. [2021] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. _Communications of the ACM_, 64(3):107-115, 2021.

Subtleties of our approach

In this section, we address some subtleties and frequently asked questions about our approach.

### Why study this simple task?

Formal reasoning is computationally expensive; very few large software projects have ever been verified [31, 30], none of them comparable to large transformer models [10, 19]. Separately, there is a high fixed cost to taking on any verification project, regardless of computational efficiency of the verification itself. Thus, we picked the simplest setting to study the question of interest: Is it even possible to formally reason more efficiently than by brute force about model behavior?

### Scalability

In this section, we address concerns about the scalability of our approach.

#### a.2.1 Larger input spaces

We demonstrate that our proof strategies can be reused on larger input spaces while scaling better than the brute force approach does.

We applied our proof strategies to models trained for Max-of-5, Max-of-10, and Max-of-20. While running the brute force proof on Max-of-20 would require approximately \(2^{148}\) FLOPs, which is about \(2^{70}\times\) the cost of training GPT-3 [32], our cubic proof achieves bounds of \((94.1\pm 1.1)\,\%\) (Max-of-5), \((91.4\pm 2.1)\,\%\) (Max-of-10), and \((88.4\pm 4.0)\,\%\) (Max-of-20) in under two minutes. See Tables 2, 3, 4, and 5 for more detailed numbers, and Figures 6, and 7 for visualizations. These results demonstrate that proof strategies can be reused on larger input spaces while scaling better than the brute force approach does.

#### a.2.2 Different tasks

In this paper, we worked on highly optimizing our relaxation to make our bounds as tight as possible when incorporating as little understanding as possible. This is not necessary for deriving proofs. Our general formalization of mechanistic interpretability is replicable: (1) theorem statements are exact expressions for the difference between the actual behavior of the model and the purported behavior, and (2) proofs are computations that bound the expression. Furthermore, our convexity theorems and proofs are applicable much more generally generally to element retrieval tasks.

#### a.2.3 More complicated architectures

We worked on a simple model studied in _A Mathematical Framework for Transformer Circuits_[13]. In follow-up work, we will extend this approach to proving bounds on 1L transformers with ReLU MLP trained on modular addition.

#### a.2.4 Larger models

It is an open question whether or not the mechanistic interpretability approach to proofs can scale to larger models. However, a large part of this question lies in the feasibility of deriving a high degree of faithful mechanistic understanding from large models -- that is, whether mechanistic interpretability itself will scale. This is widely recognized in the field, and scaling interpretability approaches while getting both a high degree of mechanistic understanding and assurances that said understanding is faithful to the model is an active area of research. Broadly, we see the compact proofs approach as a metric on the quality of mechanistic understanding -- we are not purporting to have a general solution to the problem of scaling interpretability, but instead claim that the challenges in proofs are in fact challenges in understanding networks.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**Description** & **Complexity Cost** & **Bound** & **Est.** & **Unexplained** \\
**of Proof** & & & **FLOPs** & **Dimensions** \\ \hline Brute force & \(\mathcal{O}(v^{k+1}kd)\) & \(0.9990\pm 0.0018\) & \(2^{54}\) & \(2^{36}\) \\ \hline Cubic & \(\mathcal{O}(v^{3}k^{2})\) & \(0.941\pm 0.011\) & \(2^{26}\) & \(2^{14}\) \\ \hline Sub-cubic & \(\mathcal{O}(v^{2}\cdot k^{2}+v^{2}\cdot d)\) & \(0.705\pm 0.031\) & \(2^{22}\) & \(2^{13}\) \\ w/o mean+diff & & \(0.405\pm 0.073\) & \(2^{22}\) & \(2^{13}\) \\ \hline Low-rank QK & \(\mathcal{O}(v^{2}k^{2}+\underbrace{vd}_{\text{QK}}^{2}+\underbrace{v^{2}d}_{ \text{EUAVOV}})\) & \(0.682\pm 0.033\) & \(2^{22}\) & \(2^{12}\) \\ SVD only & & \(0.335\pm 0.066\) & \(2^{22}\) & \(2^{12}\) \\ \hline Low-rank EU & \(\mathcal{O}(v^{2}k^{2}+\underbrace{vd}_{\text{QK}}^{2}+\underbrace{v^{2}d}_{ \text{QK}})\) & \(0.649\pm 0.055\) & \(2^{21}\) & \(2^{13}\) \\ SVD only & & \((4.8\pm 0.1)\times 10^{-8}\) & \(2^{21}\) & \(2^{13}\) \\ \hline Low-rank QK\&EU & \(\mathcal{O}(v^{2}k^{2}+\underbrace{vd}_{\text{QK}}^{2}+\underbrace{vd}_{ \text{EU}}+\underbrace{v^{2}d}_{\text{OV}})\) & \(0.628\pm 0.053\) & \(2^{22}\) & \(2^{13}\) \\ SVD only & & \((4.8\pm 0.1)\times 10^{-8}\) & \(2^{22}\) & \(2^{13}\) \\ \hline Quadratic QK & \(\mathcal{O}(v^{2}k^{2}+\underbrace{vd}_{\text{QK}}+\underbrace{v^{2}d}_{ \text{EUAVOV}})\) & \(0.354\pm 0.034\) & \(2^{21}\) & \(2^{12}\) \\ \hline Quadratic QK\&EU & \(\mathcal{O}(v^{2}k^{2}+\underbrace{vd}_{\text{QK}}+\underbrace{v^{2}d}_{ \text{OV}})\) & \(0.335\pm 0.033\) & \(2^{21}\) & \(2^{13}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Version of Table 1 from Section 4.1 with \(n_{\text{ctx}}=5\), \(d_{\text{vocab}}=64\). We report the proof complexity, accuracy bound, and estimated flops required (Equation 2), as well as unexplained dimensionality (Section 5). Unlike Table 1, which computes the brute force bound exactly, we instead use importance sampling to estimate the bound; estimated FLOPs are reported for what the full brute force proof would take. We round the FLOP and unexplained dimension counts to the closest power of 2, and report the mean/standard deviation of the bound averaged across all 151 models. For space reasons, we use \(k:=n_{\text{ctx}}\), \(d:=d_{\text{model}}\), and \(v:=d_{\text{vocab}}\).

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**Description** & **Complexity Cost** & **Bound** & **Est.** & **Unexplained** \\
**of Proof** & & & **FLOPs** & **Dimensions** \\ \hline Brute force & \(\mathcal{O}(v^{k+1}kd)\) & \(0.9988\pm 0.0013\) & \(2^{86}\) & \(2^{66}\) \\ \hline Cubic & \(\mathcal{O}(v^{3}k^{2})\) & \(0.914\pm 0.021\) & \(2^{28}\) & \(2^{14}\) \\ \hline Sub-cubic & \(\mathcal{O}(v^{2}\cdot k^{2}+v^{2}\cdot d)\) & \(0.674\pm 0.028\) & \(2^{23}\) & \(2^{13}\) \\ w/o mean+diff & & \(0.539\pm 0.061\) & \(2^{23}\) & \(2^{13}\) \\ \hline Low-rank QK & \(\mathcal{O}(v^{2}k^{2}+\underbrace{vd}_{\text{QK}}^{2}+\underbrace{v^{2}d}_{ \text{EUAVOV}})\) & \(0.657\pm 0.028\) & \(2^{23}\) & \(2^{12}\) \\ SVD only & & \(0.469\pm 0.059\) & \(2^{23}\) & \(2^{12}\) \\ \hline Low-rank EU & \(\mathcal{O}(v^{2}k^{2}+\underbrace{vd}_{\text{QK}}+\underbrace{v^{2}d}_{ \text{QK}})\) & \(0.639\pm 0.032\) & \(2^{23}\) & \(2^{13}\) \\ SVD only & & \((0\pm 100)\times 10^{-12}\) & \(2^{22}\) & \(2^{13}\) \\ \hline Low-rank QK\&EU & \(\mathcal{O}(v^{2}k^{2}+\underbrace{vd}_{\text{QK}}+\underbrace{v^{2}d}_{ \text{OV}})\) & \(0.625\pm 0.031\) & \(2^{23}\) & \(2^{13}\) \\ SVD only & & \((2.9\pm 0.1)\times 10^{-17}\) & \(2^{23}\) & \(2^{13}\) \\ \hline Quadratic QK & \(\mathcal{O}(v^{2}k^{2}+\underbrace{vd}_{\text{QK}}+\underbrace{v^{2}d}_{ \text{EUAVOV}})\) & \(0.392\pm 0.030\) & \(2^{22}\) & \(2^{12}\) \\ \hline Quadratic QK\&EU & \(\mathcal{O}(v^{2}k^{2}+\underbrace{vd}_{\text{QK}}+\underbrace{v^{2}d}_{ \text{OV}})\) & \(0.390\pm 0.028\) & \(2^{22}\) & \(2^{13}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Version of Table 1 from Section 4.1 with \(n_{\text{ctx}}=10\), \(d_{\text{vocab}}=64\). We report the proof complexity, accuracy bound, and estimated flops required (Equation 2), as well as unexplained dimensionality (Section 5). Unlike Table 1, which computes the brute force bound exactly, we instead use importance sampling to estimate the bound; estimated FLOPs are reported for what the full brute force proof would take. We round the FLOP and unexplained dimension counts to the closest power of 2, and report the mean/standard deviation of the bound averaged across all 151 models. For space reasons, we use \(k:=n_{\text{ctx}}\), \(d:=d_{\text{model}}\), and \(v:=d_{\text{vocab}}\).

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**Description** & **Complexity Cost** & **Bound** & **Est.** & **Unexplained** \\
**of Proof** & & & **FLOPs** & **Dimensions** \\ \hline Brute force & \(\mathcal{O}(v^{k+1}kd)\) & \(0.9972\pm 0.0031\) & \(2^{96}\) & \(2^{77}\) \\ \hline Cubic & \(\mathcal{O}(v^{3}k^{2})\) & \(0.882\pm 0.012\) & \(2^{31}\) & \(2^{16}\) \\ \hline Sub-cubic & \(\mathcal{O}(v^{2}\cdot k^{2}+v^{2}\cdot d)\) & \(0.622\pm 0.031\) & \(2^{24}\) & \(2^{15}\) \\ w/o mean+diff & & \(0.390\pm 0.070\) & \(2^{24}\) & \(2^{15}\) \\ \hline Low-rank QK & \(\mathcal{O}(v^{2}k^{2}+\underbrace{vd}_{\text{QK}}^{2}+\underbrace{v^{2}d}_{ \text{EUAVOV}})\) & \(0.594\pm 0.035\) & \(2^{24}\) & \(2^{14}\) \\ SVD only & & \(0.320\pm 0.053\) & \(2^{25}\) & \(2^{14}\) \\ \hline Low-rank EU & \(\mathcal{O}(v^{2}k^{2}+\underbrace{vd}_{\text{QK}}^{2}+\underbrace{v^{2}d}_{ \text{QK}})\) & \(0.607\pm 0.031\) & \(2^{24}\) & \(2^{15}\) \\ SVD only & & \((5.4\pm 0.2)\times 10^{-20}\) & \(2^{24}\) & \(2^{15}\) \\ \hline Low-rank QK\&EU & \(\mathcal{O}(v^{2}k^{2}+\underbrace{vd}_{\text{QK}}^{2}+\underbrace{vd}_{ \text{EUAVOV}}+\underbrace{v^{2}d}_{\text{OV}})\) & \(0.595\pm 0.030\) & \(2^{24}\) & \(2^{14}\) \\ SVD only & & \((5.4\pm 0.2)\times 10^{-20}\) & \(2^{25}\) & \(2^{14}\) \\ \hline Quadratic QK & \(\mathcal{O}(v^{2}k^{2}+\underbrace{vd}_{\text{QK}}+\underbrace{v^{2}d}_{ \text{EUAVOV}})\) & \(0.350\pm 0.029\) & \(2^{24}\) & \(2^{14}\) \\ \hline Quadratic QK\&EU & \(\mathcal{O}(v^{2}k^{2}+\underbrace{vd}_{\text{QK}}+\underbrace{v^{2}d}_{ \text{OV}})\) & \(0.384\pm 0.025\) & \(2^{24}\) & \(2^{14}\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Version of Table 1 from Section 4.1 with \(n_{\text{ctx}}=10\) and \(d_{\text{vocab}}=128\). We report the proof complexity, accuracy bound, and estimated flops required (Equation 2), as well as unexplained dimensionality (Section 5). Unlike Table 1, which computes the brute force bound exactly, we instead use importance sampling to estimate the bound; estimated FLOPs are reported for what the full brute force proof would take. We round the FLOP and unexplained dimension counts to the closest power of 2, and report the mean/standard deviation of the bound averaged across all 151 models. For space reasons, we use \(k:=n_{\text{ctx}}\), \(d:=d_{\text{model}}\), and \(v:=d_{\text{vocab}}\).

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**Description** & **Complexity Cost** & **Bound** & **Est.** & **Unexplained** \\
**of Proof** & & & **FLOPs** & **Dimensions** \\ \hline Brute force & \(\mathcal{O}(v^{k+1}kd)\) & \(0.995\pm 0.015\) & \(2^{148}\) & \(2^{126}\) \\ \hline Cubic & \(\mathcal{O}(v^{3}k^{2})\) & \(0.884\pm 0.040\) & \(2^{29}\) & \(2^{14}\) \\ \hline Sub-cubic & \(\mathcal{O}(v^{2}\cdot k^{2}+v^{2}\cdot d)\) & \(0.561\pm 0.043\) & \(2^{24}\) & \(2^{13}\) \\ w/o mean+diff & & \(0.486\pm 0.060\) & \(2^{24}\) & \(2^{13}\) \\ \hline Low-rank QK & \(\mathcal{O}(v^{2}k^{2}+\underbrace{vd}_{\text{QK}}^{2}+\underbrace{v^{2}d}_{ \text{EUAVOV}})\) & \(0.547\pm 0.043\) & \(2^{24}\) & \(2^{12}\) \\ SVD only & & \(0.431\pm 0.060\) & \(2^{24}\) & \(2^{12}\) \\ \hline Low-rank EU & \(\mathcal{O}(v^{2}k^{2}+\underbrace{vd}_{\text{QK}}+\underbrace{v^{2}d}_{ \text{QK}})\) & \(0.538\pm 0.043\) & \(2^{24}\) & \(2^{13}\) \\ SVD only & & \((1.0\pm 6.0)\times 10^{-4}\) & \(2^{24}\) & \(2^{13}\) \\ \hline Low-rank QK\&EU & \(\mathcal{O}(v^{2}k^{2}+\underbrace{vd}_{\text{QK}}+\underbrace{v^{2}d}_{ \text{OV}})\) & \(0.526\pm 0.041\) & \(2^{24}\) & \(2^{13}\) \\ SVD only & & \((1.0\pm 5.0)\times 10^{-4}\) & \(2^{24}\) & \(2^{13}\) \\ \hline Quadratic QK & \(\mathcal{O}(v^{2}k^{2}+\underbrace{vd}_{\text{QK}}+\underbrace{v^{2}d}_{ \text{EUAVOV}})\) & \(0.322\pm 0.035\) & \(2^{24}\) & \(2^{12}\) \\ \hline Quadratic QK\&EU & \(\mathcal{O}(v^{2}k^{2}+\underbrace{vd}_{\text{QK}}+\underbrace{v^{2}d}_{ \text{OV}})\) & \(0.321\pm 0.035\) & \(2^{24}\) & \(2^{13}\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Version of Table 1 from Section 4.1 with \(n_{\text{ctx}}=20\), \(d_{\text{vocab}}=64\). We report the proof complexity, accuracy bound, and estimated flops required (Equation 2), as well as unexplained dimensionality (Section 5). Unlike Table 1, which computes the brute force bound exactly, we instead use importance sampling to estimate the bound; estimated FLOPs are reported for what the full brute force proof would take. We round the FLOP and unexplained dimension counts to the closest power of 2, and report the mean/standard deviation of the bound averaged across all 151 models. For space reasons, we use \(k:=n_{\text{ctx}}\), \(d:=d_{\text{model}}\), and \(v:=d_{\text{vocab}}\).

Figure 6: Version of Figure 3 from page 8 with varying \(n_{\rm ctx}\) and \(d_{\rmvocab}\). The brute-force proof (Section 4.1) computes the exact performance uses orders of magnitude more compute than other approaches; unlike in Figure 3, here we use importance sampling to estimate the bound.

Figure 7: Version of Figure 4 from page 9 with varying \(n_{\rm ctx}\) and \(d_{\rm vocab}\). Note that the “svd” proof strategy has a clear upward trend, especially on early points.

### Why is more mechanistic understanding correlated with worse bounds?

Figure 3 exhibits Simpson's Paradox: although more faithful mechanistic understanding is correlated with better bounds within each class of proof (and moreover the most extensive mechanistic understanding results in the greatest improvement in bound tightness over baseline), when we aggregate across all proof strategies, we find that more mechanistic understanding is correlated with worse bounds.

This relationship is summarized in Figure 8.

From the compression perspective, more mechanistic understanding is about having more compression. Unless the model is losselessly compressible, we should expect that more compression will inherently be more lossy, no matter how good our compression scheme is. Correspondingly, using more understanding to get more compression will often result in a weaker bound, no matter how good our understanding is.

Conversely, we can think of the quality of proofs (the combination of tightness of bound, and length of proof) as a metric for how good our mechanistic understanding is. From this perspective, the fact that mechanistic-interpretability-derived bounds are bad suggests gaps in our mechanistic understanding. As the field matures and we develop tools that enable more faithful and complete understanding of model behavior, we expect that the quality of bounds we derive from mechanistic understanding will improve.

### Convex relaxation

In this work, we construct convex relaxations to perform the pessimal ablations for our proofs.

In what sense are we using "convexity"? The intuition is that we are attempting to optimize a function \(f\) over its domain \(X\) by incrementally making local changes to the input sequence, such as replacing one token by another, or by changing the order of tokens. The reason that convex optimization problems are easy to solve is that all local extrema are global extrema. This is not the case for our optimization problem, so we find a relaxation of \(f\) and its domain such that all local extrema are in fact global extrema.

Furthermore, most convex optimizers perform optimization at runtime by repeatedly stepping towards extrema. In this work, we "optimize by hand", performing the optimization in the proof of our general theorems. The computation of the bound then only needs to instantiate the precomputed possible extrema with the actual values of the model's parameters to determine the the extrema actually are.

We now give a formal description of what we mean by "convex relaxation".

For a set of inputs \(X_{i}\), we define a set of "relaxed inputs" \(X_{i}^{\mathrm{relaxed}}\) with an injection \(T_{i}:X_{i}\hookrightarrow\mathcal{P}(X_{i}^{\mathrm{relaxed}})\) mapping input to the model to the set of corresponding relaxed inputs. On the relaxed input, we define a function \(h_{i}:X_{i}^{\mathrm{relaxed}}\to\mathbb{R}\) such that for all \(\mathbf{t}\in X_{i}\) and all labels \(l\) for which \((l,\mathbf{t})\) is supported by (has non-zero probability in) \(\mathcal{D}\), we can find \(\mathbf{t}^{\mathrm{relaxed}}\in T_{i}(\mathbf{t})\) with \(f(l,\mathcal{M}(\mathbf{t}))\geq h_{i}(\mathbf{t}^{\mathrm{relaxed}})\). We proceed by finding a small subset of "boundary" examples \(B_{i}\subset X_{i}^{\mathrm{relaxed}}\), proving that if \(h_{i}(\mathbf{t}^{\mathrm{relaxed}})\geq b_{i}\) for all \(\mathbf{t}^{\mathrm{relaxed}}\in B_{i}\) then \(h_{i}(\mathbf{t}^{\mathrm{relaxed}})\geq b_{i}\) for all \(\mathbf{t}^{\mathrm{relaxed}}\in X_{i}^{\mathrm{relaxed}}\).

Then, the computational component \(C\) of the proof validates that that \(h_{i}(\mathbf{t}^{\mathrm{relaxed}})\geq b_{i}\) for some \(b_{i}\) for all \(\mathbf{t}^{\mathrm{relaxed}}\in X_{i}^{\mathrm{relaxed}}\). This allows us to conclude that \(f(l,\mathcal{M}(\mathbf{t}))\geq b_{i}\) for all \(\mathbf{t}\in X_{i}\).

### Computing unexplained dimensionality

We claim in Figure 5 that we can use unexplained dimensionality as a metric for understanding. Here we describe how we compute the unexplained dimensionality of a proof strategy.

As in Figure 1, for any given proof, we can separate our treatment of transformer components into "black-box" (e.g., matrix multiplication) and "white-box" components (e.g., specifying that the QK circuit is approximately rank one; pessimizing over non-max tokens). Considering the performance score as a large white-box component which may reference black-boxes internally, we define the unexplained dimensionality of a single black-box computation as the log-cardinality of it function space (so, e.g, \(2\cdot 64\) for a function \(\underline{64}\to\mathbb{R}^{2}\), whose cardinality is \((\mathbb{R}^{2})^{\underline{64}}\), where \(\underline{64}\) denotes the finiteset on 64 elements). The unexplained dimensionality of the entire proof is the sum of the unexplained dimensions of all black-box components.

Intuitively speaking, unexplained dimensionality tries to capture the degrees of freedom that we have to check via brute enumeration over black-box computations. Proofs with less unexplained dimensionality contain more mechanistic understanding, and vice versa.

### Computing approximate FLOPs

In Figure 3 and Table 1 on page 3 and on page 3, we display approximate floating point operations. We instrument our code to execute on phantom tensors that track their shape and accumulate an approximate count of floating point operations. We compute matrix additions and multiplications in the obvious way. We take the instruction count of SVD to be the cost of verifying that the output of SVD is a valid decomposition: that we have a pair of orthonormal bases which when multiplied out give the original basis.

### IEEE 754 vs. \(\mathbb{R}\)

In Section 2 we defined \(C\) and \(Q\) and glossed over whether we were reasoning over reals or floats. Here we clarify this point that we've so far been sweeping under the rug.

Let \(\mathbb{F}\) denote the set of the relevant flavor of IEEE 754 Floating Point numbers (generally 32-bit for our concrete models, but everything would hold just as well for 64-bit). Let \(\mathbb{F}^{*}\) denote \(\mathbb{F}\) restricted to finite numbers (that is, without NaNs and without \(\pm\infty\)).

We parameterize \(C\), \(\mathcal{M}\), and \(\mathcal{D}\) over the real field13 they operate on, so that, e.g., \(C_{F}:\text{model weights}\to F\). Then we have \(Q\) establishing that for any model \(\mathcal{M}^{\prime}\), \(C_{\mathbb{R}}(\mathcal{M}^{\prime}_{\mathbb{R}})\leq\mathbb{E}_{(l, \mathbf{t})\sim\mathcal{D}_{\mathbb{R}}}f_{\mathbb{R}}(l,\mathcal{M}^{\prime} _{\mathbb{R}}(\mathbf{t}))\), and we have a trace demonstrating that \(C_{\mathbb{F}}(\mathcal{M}_{\mathbb{F}})=b\).

Figure 8: The theoretical relationship between proof length and bound tightness.

Let \(i:\mathbb{F}^{*}\to\mathbb{R}\) be any injection that maps each floating point number to some real number that it is "closest to". Supposing that \(b\in\mathbb{F}^{*}\) and thus \(b\in\mathbb{R}\), we need two additional components of the proof. We need to find \(\varepsilon,\varepsilon^{\prime}\in\mathbb{R}^{+}\) prove that

\[\left|C_{\mathbb{R}}(\mathcal{M}_{\mathbb{R}})-i\left(C_{\mathbb{F}}(\mathcal{ M}_{\mathbb{F}})\right)\right|<\varepsilon\qquad\text{and}\qquad\left|\left( \mathbb{E}_{(l,\mathbf{t})\sim\mathcal{D}_{\mathbb{R}}}f_{\mathbb{R}}(l, \mathcal{M}_{\mathbb{R}}(\mathbf{t}))\right)-i\left(\mathbb{E}_{(l,\mathbf{t} )\sim\mathcal{D}_{\mathbb{F}}}f_{\mathbb{F}}(l,\mathcal{M}_{\mathbb{F}}( \mathbf{t}))\right)\right|<\varepsilon^{\prime}\]

Then we can chain these proofs to prove that

\[i\left(\mathbb{E}_{(l,\mathbf{t})\sim\mathcal{D}_{\mathbb{F}}}f_{\mathbb{F}}(l,\mathcal{M}_{\mathbb{F}}(\mathbf{t}))\right)\geq b-\varepsilon-\varepsilon^{\prime}\]

Such \(\varepsilon\)-ball robustness proofs should be well within the scope of existing approaches to formal methods on neural nets, see, e.g., [44, 3, 4, 29, 1, 57]. We leave actually dealing with the gap between floating point numbers and real numbers to future work.

### Non-uniform distributions

In Equation 1 in Section 2 we defined the expected model performance as the expectation of the distribution \(\mathcal{D}\):

\[\bar{s}:=\mathbb{E}_{(l,\mathbf{t})\sim\mathcal{D}}\left[f(l,\mathcal{M}( \mathbf{t}))\right]\geq b.\]

We then immediately specialized to the case where the marginalization \(\mathcal{D}|_{X}\) of \(\mathcal{D}\) over labels is uniform. As we'll see in Theorem 1 in Appendix D and Algorithm 3 in Appendix E, the bound computation is modularized between a function that bounds the performance \(f(l,\mathcal{M}(\mathbf{t}))\) over a restricted collection of inputs, and a much simpler function that combines the bounds on individual cases into a bound on the expectation over the entire distribution. The per-input bound computation is correctness in Algorithm 1 and relaxed-correctness-pessimizing-over-position in Algorithm 3; the expectation computation is brute-force in Algorithm 1 and cubic in Algorithm 3.

Since the expectation computation is modularized, it is straightforward to extend our approach to non-uniform distributions simply by adjusting the weighting of each region of inputs. However, if the distribution is too far off from the uniform training distribution, the bound we get may not be very good, as we may not be allocating adequate computation to the high-probability regions of the input space.

### Adversarial robustness via flexibility in \(\mathcal{D}\)

There is flexibility inherent in Equation 1. Normally, by out-of-distribution (OOD) or adversarial inputs, we suppose that there is a distribution \(\mathcal{D}_{\text{in}}\) that's used for training and (in-distribution) validation, and another distribution \(\mathcal{D}^{\prime}\) that is the deployment distribution or generated by an adversary. If we had knowledge of \(\mathcal{D}^{\prime}\), we could compute the expected performance from inputs sampled from \(\mathcal{D}^{\prime}\). Even if we don't have exact knowledge of \(\mathcal{D}^{\prime}\), we can still define a very broad distribution \(\mathcal{D}\) that covers possible \(\mathcal{D}^{\prime}\)s.

In this work, \(\mathcal{D}\) is the distribution of all \(64^{4}\) possible valid input sequences. In addition, as our proofs partition \(\mathcal{D}\) into subdistributions, and bound the performance on each subdistribution, we can bound the model's performance on any possible distribution over valid input sequences.

### Infinite distributions

In the brute force proof in Section 4.1, we run the model on the entirety of \(\mathcal{D}\). This operation is straightforward when \(X\) is finite. Perhaps surprisingly, we can do this even if \(X\) is infinite as long as the PDF \(L\times X\to\mathbb{R}\) of \(\mathcal{D}\) is computable and the natural computational topology of \(X\) is compact [16, 15, 14], because integration of computable functions on computable reals is computable [49].

### Using alternate loss functions

Building on the point from Appendix A.8, it is also relatively straightforward to extend our approach from bounding expected accuracy to bounding log-loss. We will see in Figure 12 that the accuracy and log-loss share a subterm \(\Delta\ell_{i}\). Since we compute this subterm in all of our algorithms, we can easily extend our approach to log-loss by combining \(\Delta\ell_{i}\) directly rather than merely checking that the value is negative as we currently do in relaxed-correctness-pessimizing-over-position in Algorithm 3. Although this is sufficient for the brute-force and cubic proofs, for the subcubic proof using Algorithm 6 in Appendix F, we would additionally have to compute a log-loss bound for the sequences where the largest non-max token is "too close" to the max token, which we currently neglect by considering the model to get them wrong in the worst case.

### Proving upper bounds

In this work, we focus on proving lower bounds on model performance. Most of our theorems, for example in Appendices E, and F, prove two-sided bounds. Most of the other theorems can be straightforwardly adapted to proving upper bounds by swapping uses of \(\min\) and \(\max\). Therefore, we expect that proving upper bounds on model performance should be straightforward.

### What proof system?

Length of proof depends on what proof system we use. We permit any proof system where proof-checking time is linear in the length of the proof. This excludes dependently typed proof systems such as Martin-Lof type theory, but such proof systems can easily be accommodated by considering a proof-checking-trace rather than the proof object itself. Alternatively, a more conventional proof system like ZF, ZFC, or the proof system underlying Isabelle/HOL should suffice.

## Appendix B Experimental details

### Training details

To train each model, we generate 384,000 random sequences of 4 integers picked uniformly at random, corresponding to less than 2.5% of the input distribution. We use AdamW with \(\text{batch\_size}=128\), \(\text{lr}=0.001\), \(\text{betas}=(0.9,0.999)\), weight_decay left at the default \(0.01\). We train for 1 epoch (3000 steps). Over our 151 seeds, models trained with this procedure achieve \((99.92\pm 0.15)\,\%\) train accuracy and a loss of \((4\pm 8)\times 10^{-3}\).14 When qualitatively examining a single model (for example in Section 3.1 or Appendix H.1), we use the model with config seed 123, model seed15 613947648.

Footnote 14: Numbers reported as mean across training runs \(\pm\) std dev across training runs of mean accuracy and loss.

Footnote 15: The model seed is deterministically pseudorandomly derived from the seed 123.

As our models as sufficiently small, we did not have to use any GPUs to accelerate training our inference. Each training run takes less than a single CPU-hour to complete. In total, the experiments in this paper took less than 1000 CPU-hours.

We use the following software packages in our work: Paszke et al. [41], Plotly Technologies Inc. [42], Nanda and Bloom [37], Rogozhnikov [47], Virtanen et al. [52], McKinney [33], Waskom [55]

### Additional details supporting our mechanistic interpretation of the model

We provide heatmaps of the matrices corresponding to the five components described/defined in Section 3, for the mainline model.

### Distribution of model mechanisms

We provide some analysis of the distribution of the mechanisms of the models trained on the same configuration. At a glance, there is not that much variation across models.

The statistics of interest are: (1) \(\sigma_{1}/\sigma_{2}\), the ratio of the first two singular values of \(\mathrm{EQKE}\), a measure of the extent to which the attention score computation is low-rank; (2) \(\bar{s}\), the average score (accuracy) of the model across the entire input distribution; (3) \(b_{\mathrm{cubic}}/\bar{s}\), the percent-score-recovered accuracy bound achieved by the cubic proof from Section 4.2; (4) \(b_{\mathrm{subcubic}}/\bar{s}\), the percent-score-recovered accuracy bound achieved by the (per-model best)16 subcubic proof from Section 4.3.

Footnote 16: “Per-model best” here means that for each model seed, we select the variant of the subcubic proof with the highest bound.

For each statistic of interest, Table 6 presents an eleven-number summary of the statistic. Plots, seeds, and statistic values are shown for models whose values are closest to each of the corresponding summary statistics.17 Additionally, each group contains a boxplot of the summary:

Footnote 17: If a single model is the closest to two statistics, for example when the mean and median are very similar, the model is shown only once.

* the minimum, maximum; the first and third quartiles; the median and mean; percentiles \(2.15\,\%\), \(97.85\,\%\), \(8.87\,\%\), and \(91.13\,\%\); these are displayed as:
* top and bottom of the vertical whisker lines; top and bottom of the box; horizontal line inside the box, and the square; horizontal whisker lines and whisker crosshatches.

Figure 10: The OV circuit is a sum of \(\mathrm{EVOU}\) and \(\mathrm{PVOU}\). In Figure (a) we see that \(\mathrm{EVOU}\) “copies” — with the exception of input tokens \(\leq 5\) (\(6.6\pm 1.2\) across all models) — by virtue of the fact that above 5, the diagonal is larger than all the other elements in the same row. We see that the range on Figure (b) is much smaller than Figure (a), indicating that positional contribution to the copying is minimal. In Figure (c) we see that direct path values matter a bit more than \(\mathrm{PVOU}\), being only \(\approx 20\times\) smaller than the typical \(\mathrm{EVOU}\) difference. They don’t matter that much, though, being so small. Additionally, the vertical banding indicates that the primary effect of this is a largely-query-independent bias towards larger numbers, reflecting the fact that the input distribution is biased towards larger numbers being the maximum. The weak diagonal pattern indicates a slight bias towards upweighting the query token itself as a (possible) maximum token.

Figure 9: The QK circuit can be decomposed into the position-independent and position-dependent components \(\mathrm{EQKE}\) and \(\mathrm{EQKP}\). It computes the pre-softmax attention score for the model. The positional contribution to the attention score, as shown in Figure (b), is minimal. In Figure (a), the gradient from left to right along the key axis indicates that the single attention head pays more attention to larger tokens. The uniformity along the query axis suggests that this behavior is largely independent of the query token. Further, the light and dark bands imply that some queries are better than others at focusing more on larger tokens.

[MISSING_PAGE_FAIL:25]

## Appendix C Mathematical definitions

We provide a detailed breakdown of the mathematical notation used in the appendix.

Let

\[\begin{array}{ll}\begin{array}{ll}n&\text{be the finite set on $n$ elements; we write}\\ \mathbb{N}_{<n}&:=\{0,1,\ldots,n-1\}\end{array}\end{array}\]

\[\begin{array}{ll}\sigma(\mathbf{v})&\text{be the softmax function $e^{\mathbf{v}}/\sum_{i}e^{v_{i}}$}\\ \sigma^{\star}(\mathbf{v})&\text{be the casually-masked softmax function,}\\ &\sigma^{\star}(\mathbf{v})_{i}:=e^{v_{i}}/\sum_{j<i}e^{v_{j}}\end{array}\]

\[\begin{array}{ll}d_{\mathrm{vocab}}&\text{be the size of the vocabulary}\\ d&\text{be the dimension of the attention head, in our case, equal to the hidden dimension of the model}\\ &d_{\mathrm{model}}\text{ (assumption: $d<d_{\mathrm{vocab}}$)}\\ n_{\mathrm{ctx}}&\text{be the context length, the number of tokens in the input sequence, equal to $K$ in Max-of-$K$}\\ P&\text{be the $n_{\mathrm{ctx}}\times d_{\mathrm{model}}$ positional embedding}\\ E&\text{be the $d_{\mathrm{vocab}}\times d_{\mathrm{model}}$ token embed}\\ Q,K,&\text{be the $d_{\mathrm{model}}\times d_{\mathrm{model}}$ query, key, value, and output}\\ V,O&\text{matrices of the attention head}\\ U&\text{be the $d_{\mathrm{model}}\times d_{\mathrm{vocab}}$ unembed matrix}\\ \mathbf{t}&\text{be the input token sequence $[t_{0},t_{1},\ldots,t_{n_{\mathrm{ctx}}-1}]$}\\ \mathbf{x}&\text{be the $n_{\mathrm{ctx}}\times d_{\mathrm{vocab}}$ one-hot-encoded input token}\\ &\text{sequence $[x_{0},x_{1},\ldots,x_{n_{\mathrm{ctx}}-1}]$}\\ x_{\mathrm{query}}&:=x_{-1}:=x_{n_{\mathrm{ctx}}-1}\text{be the query token}\\ t_{\mathrm{query}}&:=t_{-1}:=t_{n_{\mathrm{ctx}}-1}\text{be the one-hot encoded query token}\\ t_{\mathrm{max}}&\text{be the true maximum token in the input sequence,}\\ &\max_{i}t_{i}\end{array}\]

We define the two typical performance functions corresponding to accuracy and log-loss. Note the shared subterm \(\Delta\ell_{i}\).

\[f^{\text{accuracy}}(t_{\mathrm{max}},\ell) :=\mathds{1}[\operatorname*{argmax}_{i}\ell_{i}=t_{\mathrm{max}} ]=\mathds{1}[0>\max_{i\neq\ell_{\mathrm{max}}}\underbrace{\ell_{i}-\ell_{t_{ \mathrm{max}}}}_{\Delta\ell_{i}}]\] (4) \[f^{\text{log-loss}}(t_{\mathrm{max}},\ell) :=(\sigma(\ell))_{t_{\mathrm{max}}}=\,\log(\sum_{i}\,\exp( \underbrace{\ell_{i}-\ell_{t_{\mathrm{max}}}}_{\Delta\ell_{i}}))\] (5)

We present the model definition in four different regroupings to define via underbrace labels various useful quantities:

\[\mathcal{M}(\mathbf{t})=\ell(\mathbf{t}) =\sigma^{\star}\Big{(}\underbrace{\left(x_{\mathrm{query}}E+P_{ \mathrm{query}}\right)QK^{T}\left(\mathbf{x}E+P\right)^{T}}_{\text{QK circuit}}/\sqrt{d}\Big{)}\cdot \underbrace{\left(\mathbf{x}E+P\right)VOU}_{\text{OV circuit}}+\underbrace{ \left(x_{\mathrm{query}}E+P_{\mathrm{query}}\right)U}_{\text{direct path}}\] (6) \[=\underbrace{\sigma^{\star}\Big{(}x_{\mathrm{query}}\Big{(} \underbrace{\mathbf{E}_{q}QK^{T}\mathbf{\tilde{E}}^{T}}_{\text{EQKE}}\mathbf{x }^{T}+\underbrace{\mathbf{E}_{q}QK^{T}\mathbf{\tilde{P}}^{T}}_{\text{EQK}} \Big{)}/\sqrt{d}\Big{)}}_{\alpha^{\star}(\mathbf{t})}\cdot\underbrace{\left( \underbrace{\mathbf{x}\mathbf{E}VOU}_{\text{EVOU}}+\underbrace{\mathbf{\hat{P} }VOU}_{\text{PVOU}}\right)}_{\text{EPVOU}(\mathbf{t})}+x_{\mathrm{query}} \underbrace{\mathbf{E}_{q}U}_{\text{EU}}\] (7) \[=\underbrace{\alpha^{\star}(\mathbf{t})\cdot\mathbf{x}\mathbf{E}VOU }_{\text{$\ell^{\mathrm{VOU}}(\mathbf{t})$}}+\underbrace{\alpha^{\star}( \mathbf{t})\cdot\mathbf{\hat{P}}VOU}_{\text{$\ell^{\mathrm{VOU}}(\mathbf{t})$} }+\underbrace{x_{\mathrm{query}}\mathbf{E}_{q}U}_{\text{$\ell^{\mathrm{EU}}(x_ {\mathrm{query}})$}}\] (8) \[=\Big{(}\sum_{i=0}^{n_{\mathrm{ctx}}-1}\underbrace{\left(\alpha^ {\star}(\mathbf{t})\right)_{i}x_{i}\mathbf{\hat{E}}VOU}_{\text{$\ell^{ \mathrm{VOU}}:(\mathbf{t})$}}+\underbrace{\left(\alpha^{\star}(\mathbf{t}) \right)_{i}\mathbf{\hat{P}}_{i}VOU}_{\text{$\ell^{\mathrm{VOU}}:(\mathbf{t})$} }\Big{)}+\underbrace{x_{\mathrm{query}}\mathbf{E}_{q}U}_{\text{$\ell^{ \mathrm{EU}}(x_{\mathrm{query}})$}}\] (9)

Figure 11: Preliminary model definitions

Figure 12: Definitions of the model behavior

## Appendix D Brute-force proof

**Theorem 1**.: _For \(\textsc{brute-force}(d_{\mathrm{vocab}},n_{\mathrm{ctx}},\mathcal{M})\) as defined in Algorithm 1,_

\[\mathbb{E}_{\mathbf{t}\sim U(0,1,\dots,d_{\mathrm{vocab}}-1)^{n_{\mathrm{ctx}}}} \left[\operatorname*{argmax}_{i}(\mathcal{M}(\mathbf{t}))_{i}=\max_{i}t_{i} \right]\geq\textsc{brute-force}(d_{\mathrm{vocab}},n_{\mathrm{ctx}}, \mathcal{M})\]

Proof.: In fact the two sides of the inequality are equal by definition. Hence the inequality follows by reflexivity of \(\geq\). 

```
1:functioncorrectness(\(\mathcal{M}\), input-sequence)
2:return model-behavior(\(\mathcal{M}\), input-sequence) \(==\) max(input-sequence)
3:endfunction
4:functionbrute-force(\(d_{\mathrm{vocab}},n_{\mathrm{ctx}},\mathcal{M}\))
5:return\(\frac{1}{d_{\mathrm{vocab}}^{n_{\mathrm{ctx}}}}\)sum(correctness(\(\mathcal{M}\), tokens)\)for tokens\(\in(\textsc{range}(d_{\mathrm{vocab}}))^{n_{\mathrm{ctx}}}\)
6:endfunction ```

**Algorithm 1** Counting Correct Sequences By Brute Force

## Appendix E Details of cubic proof

In this section, we prove formally the result used in Section 4.2, A cubic proof.

At its heart, the convexity of softmax19 is an extension to a simple idea: a weighted average of scalar values is extremized by putting 100% of the weight on an extremal value.

Footnote 19: See Appendix A.4 for the reason that we call this “convexity”. Note that our use of “convexity” is purely descriptive in this section; all theorems are written out explicitly.

Using this simple version of the theorem, however, gives a useless bound of 0% accuracy: if we pay no attention to the maximum of the sequence, of course we're going to get the wrong answer. Since in fact the space of possible weightings we may see in practice is much smaller (finite, in fact, with at most \(d_{\mathrm{vocab}}^{n_{\mathrm{ctx}}}\) values), we may look for a more general version of this idea that gives us tighter bounds that still cover the space of possible weightings.

The weights are _not_ linearly independently choosable (softmax is non-linear), so extremal values do not necessarily result from putting maximal attention on the worst token. It may be, when trying to find the worst case, that some positions are so dis-preferred that it makes more sense to choose a token that is "less bad" for those positions, if it draws enough attention away from the correct token. See Lemma 3 for details.

We thus spend this section characterizing a relaxation of the constraints on weights:

1. that contains all actually possible weightings,
2. that is extremized at weights that still correspond to some notion of "put the most weight on the extremal tokens", and
3. for which computing the extremal weightings is computationally efficient.

Before diving in, let's recall the proof that a weighted average of scalar values is extremized by putting 100% of the weight on extremal values:

**Theorem 2** (Warmup: Extremizing weighted averages).: _Fix a set of values \(v_{i}\in\mathbb{R}\). The weighted average is bounded by the extremal values: for any \(w_{i}\) such that \(\sum_{i}w_{i}=1\) and \(0\leq w_{i}\leq 1\),_

\[\min_{i}v_{i}\leq\sum_{i}w_{i}v_{i}\leq\max_{i}v_{i}\]Proof.: The proof is simple. We have

\[\sum_{i}w_{i}v_{i}-\min_{i}v_{i} =\sum_{i}w_{i}(v_{i}-\min_{j}v_{j})\geq 0\] and \[\max_{i}v_{i}-\sum_{i}w_{i}v_{i} =\sum_{i}w_{i}(\max_{j}v_{j}-v_{i})\geq 0\] so the result follows. 

### Proof strategy

The model computes the true maximum \(t_{\max}\) when its outputs logits \(\ell\) are such that \(\Delta\ell_{t^{*}}:=\ell_{t^{*}}-\ell_{t_{\max}}<0\) for all \(t^{*}\neq t_{\max}\).20 As a result, it suffices to lower-bound the proportion of sequences where (an upper bound on) \(\Delta\ell_{t^{*}}\) is negative for all \(t^{*}\neq t_{\max}\). In particular, we will upper-bound the contribution from incorrect tokens \(t\) in positions \(i\) to the difference \(\Delta\ell^{i}\) between incorrect (\(t^{*}\)) and correct (\(t_{\max}\)) output tokens \(\Delta\ell^{i}_{t^{*}}=\ell^{i}_{t^{*}}-\ell^{i}_{t_{\max}}\).

Footnote 20: We use the logit difference \(\Delta\ell_{t^{*}}\) because: (a) it is shared in the computation of \(f^{0,1}\) and \(f^{\text{log-loss}}\); (b) it is a linear function of the various paths through the model, which can therefore be analyzed separately; (c) it leaves open both the options of pessimizing over output logit before or after combining contributions of various paths through the model.

We do this by arguing that the logit difference \(\Delta\ell_{t^{*}}\) satisfies a certain notion of convexity over the space of a relaxation of sequences (Theorem 6), and constructing a set of \(\Theta({d_{\text{vocab}}}^{3}n_{\text{ctx}})\) "extremal" relaxed sequences where the position and token embedding components of attention are pessimized independently.

We start by first rewriting the contribution of each token through the attention head to the logit difference into the contributions involving \(\operatorname{PVOU}\) and \(\operatorname{EVOU}\):

\[\Delta\ell^{t}_{t^{*}}(\mathbf{t})=\Delta\ell^{\operatorname{PVOU},i}{t^{*}}( \mathbf{t})+\Delta\ell^{\operatorname{EVOU},i}{t^{*}}(\mathbf{t})\]

We then upper bound \(\Delta\ell^{\operatorname{PVOU},i}{t^{*}}(\mathbf{t})\) by noting that because the softmax attention is a weighted average of \(\operatorname{PVOU}\),

\[\Delta\ell^{\operatorname{PVOU},i}{t^{*}}(\mathbf{t}) =\ell^{\operatorname{PVOU},i}(\mathbf{t})_{t^{*}}-\ell^{ \operatorname{PVOU},i}(\mathbf{t})_{\max_{j}t_{j}}\] \[=\alpha^{*}_{i}(\mathbf{t})\operatorname{PVOU}_{i,t^{*}}-\alpha^ {*}_{i}(\mathbf{t})\operatorname{PVOU}_{i,\max_{j}t_{j}}\] \[=\alpha^{*}_{i}(\mathbf{t})\left(\operatorname{PVOU}_{i,t^{*}}- \operatorname{PVOU}_{i,\max_{j}t_{j}}\right)\] \[\leq\alpha^{*}_{i}(\mathbf{t})\max_{i}\left(\operatorname{PVOU}_ {i,t^{*}}-\operatorname{PVOU}_{i,\max_{j}t_{j}}\right)\]

Since \(\sum_{i}\alpha^{*}_{i}(\mathbf{t})=1\), we have

\[\sum_{i=0}^{n_{\text{ctx}}-1}\Delta\ell^{\operatorname{PVOU},i}{t^{*}}( \mathbf{t})\leq\max_{i}\left(\operatorname{PVOU}_{i,t^{*}}-\operatorname{ PVOU}_{i,\max_{j}t_{j}}\right)\]

We then construct a set \(\Xi^{\operatorname{pure}}\) of "pure sequences" consisting of only three types of tokens in one of two orders, and show that for each input sequence \(\mathbf{t}\) and readoff logit \(t^{*}\), we bound the logit difference from the token embeddings \(\Delta\ell^{\operatorname{EVOU},i}{t^{*}}(\mathbf{t})\) using a small subset \(\mathcal{X}\) of \(\Xi^{\operatorname{pure}}\):

\[\sum_{i=0}^{n_{\text{ctx}}-1}\Delta\ell^{\operatorname{EVOU},i}{t^{*}}( \mathbf{t})\leq\max_{\xi\in\mathcal{X}}\sum_{i=0}^{n_{\text{ctx}}-1}\Delta \ell^{\operatorname{EVOU},i}{t^{*}}(\xi)\]

We construct a set \(X^{\operatorname{relaxed}}\) of relaxed sequences, where each relaxed sequence \(\mathbf{t}^{\operatorname{relaxed}}\) consists of a sequence and a position \((\mathbf{t},i)\), where \(\Delta\ell_{t^{*}}(\mathbf{t},i)\) is evaluated by separately considering the positional 

[MISSING_PAGE_FAIL:29]

except swapping \(i\) and \(j\):_

\[\sigma_{i\leftrightarrow j}(k)=\begin{cases}\sigma(i)&\text{if }k=j\\ \sigma(j)&\text{if }k=i\\ \sigma(k)&\text{otherwise}\end{cases}\]

_Define \(\Delta_{\sigma,i\leftrightarrow j}\) to be the difference in sequence scores when you swap \(i\) and \(j\):_

\[\Delta_{\sigma,i\leftrightarrow j}:=s_{\sigma_{i\leftrightarrow j}}-s_{\sigma}\]

**Lemma 3** (Characterization of swapping tokens).: _Fix a non-decreasing sequence of tokens \(t_{0}\leq\cdots\leq t_{n_{\text{\tiny ctx}}-1}\in\mathbb{N}\). Fix \(\sigma:\mathbb{N}\rightarrow\mathbb{N}\) be a permutation of the \(n_{\text{\tinyctx}}\) positions. Fix indices \(0\leq i,j<n_{\text{\tinyctx}}\)._

_Then there are two cases for \(\operatorname{sign}\left(\Delta_{\sigma,i\leftrightarrow j}\right)\):_

1. _If_ \(a_{t_{i}}=a_{t_{j}}\) _then_ \(\operatorname{sign}\left(\Delta_{\sigma,i\leftrightarrow j}\right)=- \operatorname{sign}\left(b_{\sigma(i)}-b_{\sigma(j)}\right)\operatorname{ sign}\left(v_{t_{i}}-v_{t_{j}}\right)\)_._
2. _Otherwise,_ \(\operatorname{sign}\left(\Delta_{\sigma,i\leftrightarrow j}\right)= \operatorname{sign}\left(a_{t_{i}}-a_{t_{j}}\right)\operatorname{sign}\left( b_{\sigma(i)}-b_{\sigma(j)}\right)\operatorname{sign}\left(s_{\sigma}-\frac{v_{t_{i}} e^{a_{t_{i}}}-v_{t_{j}}e^{a_{t_{j}}}}{e^{a_{t_{i}}}-e^{a_{t_{j}}}}\right)\)_._

Intuitively, Lemma 3 says that, if the token contribution to attention is equal between tokens \(t_{i}\) and \(t_{j}\), then the impact of swapping their positions \(\sigma(i)\) and \(\sigma(j)\) is entirely determined by how much attention is paid to the positions of \(i\) and \(j\) and the relative difference in their value. (Notably, by swapping these tokens, we don't affect the attention paid on other tokens, and so the effect of the change does not depend on the values of the other tokens.) Alternatively, if the attentions are not equal, then swapping the positions changes the allocation of attention to other tokens in the sequence, and so it may the case that this change in allocation in attention dominates the attention-weighted values of these two tokens.

Proof.: First note that the theorem is trivial for \(i=j\).

For the rest of the proof, we take \(i\neq j\).

The proof proceeds just by algebraic manipulation with no deep insight. We first list the facts we use, the proceed to computing \(\operatorname{sign}\left(\Delta_{\sigma,i\leftrightarrow j}\right)\). We abbreviate \(\sigma_{i\leftrightarrow j}\) as \(\sigma^{\prime}\) for brevity.

\[\operatorname{sign}\left(e^{b_{\sigma(i)}}-e^{b_{\sigma(j)}}\right)= \operatorname{sign}\left(b_{\sigma(i)}-b_{\sigma(j)}\right)\]

\[\operatorname{sign}\left(\Delta_{\sigma,i\leftrightarrow j}\right) =\operatorname{sign}\left(s_{\sigma^{\prime}}-s_{\sigma}\right)\] \[=\operatorname{sign}\left(\frac{\sum_{0\leq p<n_{\text{\tiny ctx }}}v_{t_{p}}e^{a_{t_{p}}+b_{\sigma^{\prime}(p)}}}{\sum_{0\leq p<n_{\text{\tiny ctx }}}e^{a_{t_{p}}+b_{\sigma^{\prime}(p)}}}-s_{\sigma}\right)\]

Now multiply through by the denominator, which is positive

\[=\operatorname{sign}\left(\sum_{0\leq p<n_{\text{\tiny ctx}}}v_{ t_{p}}e^{a_{t_{p}}+b_{\sigma^{\prime}(p)}}-s_{\sigma}\underset{0\leq p<n_{ \text{\tinyctx}}}{\sum}e^{a_{t_{p}}+b_{\sigma^{\prime}(p)}}\right)\] \[=\operatorname{sign}\left(\sum_{0\leq p<n_{\text{\tiny ctx}}}v_{ t_{p}}e^{a_{t_{p}}+b_{\sigma(p)}}-v_{t_{i}}e^{a_{t_{i}}}\left(e^{b_{\sigma(i)}}-e^{b_{ \sigma^{\prime}(i)}}\right)-v_{t_{j}}e^{a_{t_{j}}}\left(e^{b_{\sigma(j)}}-e^{b _{\sigma^{\prime}(j)}}\right)\right.\] \[\qquad\qquad-s_{\sigma}\sum_{0\leq p<n_{\text{\tiny ctx}}}e^{a_{ t_{p}}+b_{\sigma(p)}}+s_{\sigma}e^{a_{t_{i}}}\left(e^{b_{\sigma(i)}}-e^{b_{\sigma^{ \prime}(i)}}\right)+s_{\sigma}e^{a_{t_{j}}}\left(e^{b_{\sigma(j)}}-e^{b_{ \sigma^{\prime}(j)}}\right)\right)\] \[=\operatorname{sign}\left(\sum_{0\leq p<\widehat{n_{\text{\tiny ctx }}}}\hskip-14.226378ptv_{t_{p}}e^{a_{t_{p}}\pm b_{\sigma(p)}}-v_{t_{i}}e^{a_{ t_{i}}}\left(e^{b_{\sigma(i)}}-e^{b_{\sigma(j)}}\right)-v_{t_{j}}e^{a_{t_{j}}}\left(e^{b_{ \sigma(j)}}-e^{b_{\sigma(i)}}\right)\right.\] \[\qquad\qquad\qquad-\sum_{0\leq p<\widehat{n_{\text{\tiny ctx }}}}\hskip-14.226378ptv_{t_{p}}e^{a_{t_{p}}\pm b_{\sigma(p)}}+s_{\sigma}e^{a_{ t_{i}}}\left(e^{b_{\sigma(i)}}-e^{b_{\sigma(j)}}\right)+s_{\sigma}e^{a_{t_{j}}} \left(e^{b_{\sigma(j)}}-e^{b_{\sigma(i)}}\right)\right)\]\[=\operatorname{sign}\left(\left(v_{t_{j}}e^{a_{t_{j}}}-v_{t_{i}}e^{a_{t_ {i}}}\right)\left(e^{b_{\sigma(i)}}-e^{b_{\sigma(j)}}\right)+s_{\sigma}\left(e^{ a_{t_{i}}}-e^{a_{t_{j}}}\right)\left(e^{b_{\sigma(i)}}-e^{b_{\sigma(j)}}\right)\right)\] \[=\operatorname{sign}\left(e^{b_{\sigma(i)}}-e^{b_{\sigma(j)}} \right)\operatorname{sign}\left(\left(v_{t_{j}}e^{a_{t_{j}}}-v_{t_{i}}e^{a_{t _{i}}}\right)+s_{\sigma}\left(e^{a_{t_{i}}}-e^{a_{t_{j}}}\right)\right)\] \[=\operatorname{sign}\left(b_{\sigma(i)}-b_{\sigma(j)}\right) \operatorname{sign}\left(s_{\sigma}\left(e^{a_{t_{i}}}-e^{a_{t_{j}}}\right)- \left(v_{t_{i}}e^{a_{t_{i}}}-v_{t_{j}}e^{a_{t_{j}}}\right)\right)\]

Divide through by non-zero values when possible

\[=\operatorname{sign}\left(b_{\sigma(i)}-b_{\sigma(j)}\right)\] \[\quad\cdot\begin{cases}\operatorname{sign}\left(v_{t_{i}}-v_{t_ {j}}\right)&\text{if }a_{t_{i}}=a_{t_{j}}\\ \operatorname{sign}\left(e^{a_{t_{i}}}-e^{a_{t_{j}}}\right)\operatorname{ sign}\left(s_{\sigma}-\frac{v_{t_{i}}e^{a_{t_{i}}}-v_{t_{i}}e^{a_{t_{j}}}}{e^{a_{t _{i}}}-e^{a_{t_{j}}}}\right)&\text{otherwise}\end{cases}\] \[=\begin{cases}-\operatorname{sign}\left(b_{\sigma(i)}-b_{\sigma(j )}\right)\operatorname{sign}\left(v_{t_{i}}-v_{t_{j}}\right)&\text{if }a_{t_{i}}=a_{t_{j}}\\ \operatorname{sign}\left(a_{t_{i}}-a_{t_{j}}\right)\operatorname{sign}\left(b _{\sigma(i)}-b_{\sigma(j)}\right)\operatorname{sign}\left(s_{\sigma}-\frac{v_ {t_{i}}e^{a_{t_{i}}}-v_{t_{j}}e^{a_{t_{j}}}}{e^{a_{t_{i}}}-e^{a_{t_{j}}}}\right) &\text{otherwise}\end{cases}\]

**Definition 5** (\(\sigma\) fixes \(F\)).: _Fix a set of fixed indices \(F\subseteq\mathbb{N}_{<n_{\mathrm{ctx}}}\) and an assignment of token values to each of the fixed positions \(t_{F}:F\rightarrow\mathbb{N}_{<d_{\mathrm{vocab}}}\). (\(F\) is the set of positions for which we are not pessimizing over the value of the token in that position.) Fix a non-decreasing sequence of tokens \(t_{0}\leq\cdots\leq t_{n_{\mathrm{ctx}}-1}\in\mathbb{N}\)._

_Given a permutation \(\sigma:\mathbb{N}_{<n_{\mathrm{ctx}}}\rightarrow\mathbb{N}^{n_{\mathrm{ctx}}}\), say that \(\sigma\) fixes \(F\) (relative to \(t_{0},\ldots,t_{n_{\mathrm{ctx}}-1}\)) if \(t_{i}=t_{F}(\sigma(i))\) whenever \(\sigma(i)\in F\)._

Note that in this section, for the cubic proofs, we will in fact generally take \(F=\{n_{\mathrm{ctx}}-1\}\), so that we are fixing the final query token, though in Theorems 7, 8, and 9\(F\) will also contain all positions with the maximum token \(t_{\max}\). In Appendix F, we will take \(F=\emptyset\) or \(F\) to be the set of positions of the maximum token. However, none of these theorems are specific to \(F\) being subsingleton, and we prove them in generality.

**Definition 6** (position-sorting permutation).: _Fix a set of fixed indices \(F\subseteq\mathbb{N}_{<n_{\mathrm{ctx}}}\) and an assignment of token values to each of the fixed positions \(t_{F}:F\rightarrow\mathbb{N}_{<d_{\mathrm{vocab}}}\)._

_Define the position-sorting permutation fixing indices in \(F\)\(\sigma_{s}:\mathbb{N}_{<n_{\mathrm{ctx}}}\rightarrow\mathbb{N}_{<n_{ \mathrm{ctx}}}\) to be the permutation that sorts the indices not in \(F\) according to \(b\): for \(0\leq i,j<n_{\mathrm{ctx}}\) with \(i,j\not\in F\), \(b_{i}\leq b_{j}\) whenever \(\sigma_{s}(i)<\sigma_{s}(j)\); and \(\sigma_{s}(i)=i\) for \(i\in F\)._

**Definition 7** (contiguous on equal tokens).: _Fix a set of fixed indices \(F\subseteq\mathbb{N}_{<n_{\mathrm{ctx}}}\) and an assignment of token values to each of the fixed positions \(t_{F}:F\rightarrow\mathbb{N}_{<d_{\mathrm{vocab}}}\). Fix a non-decreasing sequence of tokens \(t_{0}\leq\cdots\leq t_{n_{\mathrm{ctx}}-1}\in\mathbb{N}\)._

_Say that the sequence represented by a permutation \(\sigma:\mathbb{N}_{<n_{\mathrm{ctx}}}\rightarrow\mathbb{N}_{<n_{\mathrm{ctx}}}\) is contiguous on equal tokens if, for all \(0\leq i,j,k<n_{\mathrm{ctx}}\) with \(t_{i}=t_{j}\neq t_{k}\) and \(i,j,k\not\in\sigma^{-1}(F)\), it is never the case that \(\sigma_{s}(\sigma(i))<\sigma_{s}(\sigma(k))<\sigma_{s}(\sigma(j))\)._

**Theorem 4** (Pessimization over sequence ordering is possible and results in contiguous sequences).: _Fix a set of fixed indices \(F\subseteq\mathbb{N}_{<n_{\mathrm{ctx}}}\) and an assignment of token values to each of the fixed positions \(t_{F}:F\rightarrow\mathbb{N}_{<d_{\mathrm{vocab}}}\). Fix a non-decreasing sequence of tokens \(t_{0}\leq\cdots\leq t_{n_{\mathrm{ctx}}-1}\in\mathbb{N}\)._

_Let \(\sigma_{\min},\sigma_{\max}:\mathbb{N}\rightarrow\mathbb{N}\) be permutations of the \(n_{\mathrm{ctx}}\) positions, fixing positions in \(F\), satisfying the following property: For all \(\sigma:\mathbb{N}\rightarrow\mathbb{N}\) a permutation fixing \(F\), we have_

\[s_{\sigma_{\min}}\leq s_{\sigma}\leq s_{\sigma_{\max}}\] (10)

_(Such permutations are guaranteed to exist because the permutation group on \(n_{\mathrm{ctx}}\) elements is finite.)_

_Then \(\sigma_{\max}\) and \(\sigma_{\min}\) may be taken to be contiguous on equal tokens. That is, there exist \(\sigma_{\max}\) and \(\sigma_{\min}\) satisfying the property of Equation 10 which additionally satisfy the definition of Definition 7._

The basic idea is that we will assume that one of \(\sigma_{\max}\) and \(\sigma_{\min}\) cannot be contiguous on equal tokens and derive a contradiction. We will pick the extremal permutation that is closest to being contiguous, take a contiguity violation, and then show that either we can correct the contiguity violation without changing the score--thus violating the presumption that the permutation is _closest_to being contiguous--or we will find one swap of indices that decreases the score and another swap of indices that increases the score, thus violating the presumption of extremality.

In slightly more detail, but still informally, we will consider the sign of the difference between scores of our purported extremal permutation and a permutation that has swapped some indices. The theorem follows from showing that there exists a triple of indices \(i\), \(j\), \(k\) such that the sign of the score difference from swapping \(i\) and \(j\) is different from the sign of the score difference from swapping \(j\) and \(k\).

First, a definition and some helpful facts about it.

**Definition 8** (contiguous on equally-attended positions).: _Fix a set of fixed indices \(F\subseteq\mathbb{N}_{<n_{\mathrm{ctx}}}\) and an assignment of token values to each of the fixed positions \(t_{F}:F\rightarrow\mathbb{N}_{<d_{\mathrm{vocab}}}\). Fix a non-decreasing sequence of tokens \(t_{0}\leq\dots\leq t_{n_{\mathrm{ctx}}-1}\in\mathbb{N}\)._

_Say that a permutation \(\sigma\) is contiguous on equally-attended positions if, for all \(0\leq i<n_{\mathrm{ctx}}\) with \(i\not\in\sigma^{-1}(F)\), the sorting order according \(\sigma_{s}\) on the contiguous block of positions with contribution to the attention score equal to that of \(\sigma(i)\), \(\left\{\sigma(j)\,\middle|\,b_{\sigma(j)}=b_{\sigma(i)}\text{ and }\sigma(j)\not\in F\right\}\), is the same as the sorting order according to the fraction of tokens equal to \(t_{j}\) with \(b\)-values greater than \(b_{\sigma(i)}\), with ties broken by the value of \(t_{j}\). Equationally, this second sorting order is defined by the score_

\[\left(\left|\left\{k\,\middle|\,t_{k}=t_{j}\text{ and }b_{\sigma(k)}>b_{\sigma(i)} \text{ and }\sigma(k)\not\in F\right\}\right|+\frac{t_{j}}{d_{\mathrm{vocab}}} \right)\middle/\left|\left\{k\,\middle|\,t_{k}=t_{j}\text{ and }\sigma(k)\not\in F\right\}\right|\,.\]

Most importantly, any permutation that is contiguous on equally-attended positions has the property that for any indices \(0\leq i,j,k<n_{\mathrm{ctx}}\) with \(i,j,k\not\in\sigma^{-1}(F)\) and \(t_{i}=t_{j}\neq t_{k}\) and \(\sigma_{s}(\sigma(i))<\sigma_{s}(\sigma(k))<\sigma_{s}(\sigma(j))\), we will have the _strict_ inequality \(b_{\sigma(i)}<b_{\sigma(k)}<b_{\sigma(j)}\). Additionally, we may always sort equally-attended positions to make any permutation contiguous on equally-attended positions.

We will define an additional notion of contiguity-violations which we avoid up-front by arbitrarily swapping involved indices without changing the score \(s_{\sigma}\).

**Definition 9** (needlessly non-contiguous).: _Fix a set of fixed indices \(F\subseteq\mathbb{N}_{<n_{\mathrm{ctx}}}\) and an assignment of token values to each of the fixed positions \(t_{F}:F\rightarrow\mathbb{N}_{<d_{\mathrm{vocab}}}\). Fix a non-decreasing sequence of tokens \(t_{0}\leq\dots\leq t_{n_{\mathrm{ctx}}-1}\in\mathbb{N}\)._

_Say that a permutation \(\sigma\) is needlessly non-contiguous at \(i\), \(j\), \(k\) (for \(i,j,k\not\in\sigma^{-1}(F)\)) if \(\Delta_{\sigma,i\leftrightarrow k}=0\) or \(\Delta_{\sigma,j\leftrightarrow k}=0\), for \(0\leq i,j,k<n_{\mathrm{ctx}}\) with \(i,j,k\not\in\sigma^{-1}(F)\) with \(t_{i}=t_{j}\neq t_{k}\) and \(\sigma_{s}(\sigma(i))<\sigma_{s}(\sigma(k))<\sigma_{s}(\sigma(j))\)._

_Say that a permutation \(\sigma\) is needlessly non-contiguous if it is needlessly non-contiguous at any \(i,j,k\not\in\sigma^{-1}(F)\)._

**Lemma 5**.: _Fix a set of fixed indices \(F\subseteq\mathbb{N}_{<n_{\mathrm{ctx}}}\) and an assignment of token values to each of the fixed positions \(t_{F}:F\rightarrow\mathbb{N}_{<d_{\mathrm{vocab}}}\). Fix a non-decreasing sequence of tokens \(t_{0}\leq\dots\leq t_{n_{\mathrm{ctx}}-1}\in\mathbb{N}\)._

_Any needlessly non-contiguous sequence \(\sigma\) which fixes \(F\) can be made into a sequence \(\sigma^{\prime}\) which still fixes \(F\) and is both simultaneously contiguous on equally-attended positions and not needlessly non-contiguous, and for which \(s_{\sigma}=s_{\sigma^{\prime}}\)._

Proof.: First, sort regions of equally-attended positions to make \(\sigma\) contiguous on equally-attended positions. If the resulting permutation is not needlessly non-contiguous, then we are done.

Otherwise, we have \(\Delta_{\sigma,i\leftrightarrow k}=0\) or \(\Delta_{\sigma,j\leftrightarrow k}=0\) for some \(i\), \(j\), \(k\), for \(0\leq i,j,k<n_{\mathrm{ctx}}\) with \(i,j,n\not\in\sigma^{-1}(F)\) and \(t_{i}=t_{j}\neq t_{k}\) and \(\sigma_{s}(\sigma(i))<\sigma_{s}(\sigma(k))<\sigma_{s}(\sigma(j))\). Since the sequence is contiguous on equally-attended positions, we have the strict inequality \(b_{\sigma(i)}<b_{\sigma(k)}<b_{\sigma(j)}\).

By Lemma 3, we have two cases. Noting that \(t_{i}=t_{j}\), we can write them as

1. \(v_{t_{k}}=v_{t_{i}}\) and \(a_{t_{i}}=a_{t_{k}}\)
2. \(a_{t_{i}}\neq a_{t_{k}}\) and \(s_{\sigma}=\frac{v_{t_{i}}e^{a_{t_{i}}}-v_{t_{k}}e^{a_{t_{k}}}}{e^{a_{t_{i}}}-e ^{a_{t_{k}}}}\)In the first case, we may fully freely interchange tokens equal to \(t_{i}\) with tokens equal to \(t_{k}\) without changing the score; in this case we may use the token value as a sorting tie-breaker and swap tokens until there are no more needlessly non-contiguous triples falling into case (1).

In the second case, since swapping tokens does not change \(s_{\sigma}\), the property will continue to hold for these tokens after the swap. We may then swap tokens, again using token value as a tie-breaker, until there are no more needlessly non-contiguous triples falling into case (2). 

We can now finally make our argument for Theorem 4 more precise.

Proof of Theorem 4.: Choose \(\sigma_{\max}\) and \(\sigma_{\min}\) to be contiguous on equally-attended positions and not needlessly non-contiguous, and suppose that we have \(\sigma\in\{\sigma_{\max},\sigma_{\min}\}\) such that for some \(0\leq i,j,k<n_{\mathrm{ctx}}\) with \(i,j,k\not\in\sigma^{-1}(F)\) and \(t_{i}=t_{j}\neq t_{k}\), we have \(b_{\sigma(i)}<b_{\sigma(k)}<b_{\sigma(j)}\). We will derive a contradiction with the presumption that \(\sigma\) is extremal by showing that we can swap \(i\) and \(k\) to change the score in one direction and that we can swap \(j\) and \(k\) to change the score in the other direction.

Take \(\sigma^{\prime}_{0}\) to be \(\sigma\) but swapping \(i\) and \(k\), and take \(\sigma^{\prime}_{1}\) to be \(\sigma\) but swapping \(j\) and \(k\).

Now we will consider the cases for the sign of the score difference \(\Delta_{0}:=s_{\sigma^{\prime}_{0}}-s_{\sigma}\) and \(\Delta_{1}:=s_{\sigma^{\prime}_{1}}-s_{\sigma}\). By the presumption of not being needlessly non-contiguous, \(\Delta_{z}\neq 0\) for \(z\in\{0,1\}\). If we can show that the sign of \(\Delta_{0}\) is distinct from the sign of \(\Delta_{1}\), then we will have a contradiction with extremality because we will have either \(s_{\sigma^{\prime}_{0}}<s_{\sigma}<s_{\sigma^{\prime}_{1}}\) or \(s_{\sigma^{\prime}_{1}}<s_{\sigma}<s_{\sigma^{\prime}_{0}}\). That is, we would be able to swap \(i\leftrightarrow k\) and \(j\leftrightarrow k\) to get a lower and higher score, making \(\sigma\) not extremal.

Noting that \(t_{i}=t_{j}\),

\[\mathrm{sign}\left(\Delta_{0}\right) =\mathrm{sign}\left(b_{\sigma(i)}-b_{\sigma(k)}\right)\begin{cases} \mathrm{sign}\left(v_{t_{k}}-v_{t_{i}}\right)&\text{if }a_{t_{i}}=a_{t_{k}}\\ \mathrm{sign}\left(a_{t_{i}}-a_{t_{k}}\right)\mathrm{sign}\left(s_{\sigma}- \frac{v_{t_{i}}e^{a_{t_{i}}}-v_{t_{k}}e^{a_{t_{k}}}}{e^{a_{t_{i}}}-e^{a_{t_{k} }}}\right)&\text{otherwise}\end{cases}\] \[\mathrm{sign}\left(\Delta_{1}\right) =\mathrm{sign}\left(b_{\sigma(j)}-b_{\sigma(k)}\right)\begin{cases} \mathrm{sign}\left(v_{t_{k}}-v_{t_{i}}\right)&\text{if }a_{t_{i}}=a_{t_{k}}\\ \mathrm{sign}\left(a_{t_{i}}-a_{t_{k}}\right)\mathrm{sign}\left(s_{\sigma}- \frac{v_{t_{i}}e^{a_{t_{i}}}-v_{t_{k}}e^{a_{t_{k}}}}{e^{a_{t_{i}}}-e^{a_{t_{k} }}}\right)&\text{otherwise}\end{cases}\]

Noting that the product is non-zero by presumption, that right multiplicand is equal for \(\Delta_{0}\) and \(\Delta_{1}\), and \(\mathrm{sign}\left(b_{\sigma(i)}-b_{\sigma(k)}\right)=-1\) and \(\mathrm{sign}\left(b_{\sigma(j)}-b_{\sigma(k)}\right)=1\), we have our desired contradiction. 

Note that the proof of Theorem 4 does not go through if we include the position value function \(w\) in the score, because we may trade off the position value function against the token value function. We now show that we can _independently_ pessimize over positional attention.

**Definition 10** (full sequence score).: _Given a non-decreasing sequence of tokens \(t_{0}\leq\dots\leq t_{n_{\mathrm{ctx}}-1}\in\mathbb{N}_{<d_{\mathrm{vocab}}}\) and a permutation \(\sigma:\mathbb{N}_{<n_{\mathrm{ctx}}}\rightarrow\mathbb{N}_{<n_{\mathrm{ctx}}}\) define the full sequence score \(s^{\prime}_{t_{0},\dots,t_{n_{\mathrm{ctx}}-1},\sigma}\) as:_

\[s^{\prime}_{t_{0},\dots,t_{n_{\mathrm{ctx}}-1},\sigma}:=\hskip-10.0pt\sum_{0 \leq i<n_{\mathrm{ctx}}}\hskip-10.0pt(v_{t_{i}}+w_{\sigma(i)})e^{a_{t_{i}}+b_{ \sigma(i)}}\hskip-10.0pt\left/\hskip-10.0pt\sum_{0\leq i<n_{\mathrm{ctx}}}\hskip-10.0pte ^{a_{t_{i}}+b_{\sigma(i)}}\hskip-10.0pt\right.\]

_We will drop the token subscript, writing only \(s^{\prime}_{\sigma}\), when the token values are unambiguous by context._

The sequence score here will be computing \(\Delta\ell^{\mathrm{EPVOU}}_{t^{*}}:=\Delta\ell^{\mathrm{EVOU}}_{t^{*}}+\Delta \ell^{\mathrm{PVOU}}_{t^{*}}\) for some fixed \(t^{*}\) and \(t_{\max}\). As with Definition 3, with the way we've set up our definitions, high scores predict \(t^{*}\) (and are thus bad), negative scores predict \(t_{\max}\) (and are thus good), and more negative the scores, the stronger the prediction of \(t_{\max}\).

**Definition 11** (relaxed sequence score).: _Given a non-decreasing sequence of tokens \(t_{0}\leq\dots\leq t_{n_{\mathrm{ctx}}-1}\in\mathbb{N}_{<d_{\mathrm{vocab}}}\) and a permutation \(\sigma:\mathbb{N}_{<n_{\mathrm{ctx}}}\rightarrow\mathbb{N}_{<n_{\mathrm{ctx}}}\) define the relaxed sequence scores \(r_{t_{0},\dots,t_{n_{\mathrm{ctx}}-1},\sigma,\min}\) and \(r_{t_{0},\dots,t_{n_{\mathrm{ctx}}-1},\sigma,\max}\) as:_

\[r_{t_{0},\dots,t_{n_{\mathrm{ctx}}-1},\sigma,\min} :=s_{t_{0},\dots,t_{n_{\mathrm{ctx}}-1},\sigma}+\min_{0\leq i<n _{\mathrm{ctx}}}\hskip-10.0ptw_{i}\] \[r_{t_{0},\dots,t_{n_{\mathrm{ctx}}-1},\sigma,\max} :=s_{t_{0},\dots,t_{n_{\mathrm{ctx}}-1},\sigma}+\max_{0\leq i<n _{\mathrm{ctx}}}\hskip-10.0ptw_{i}\]

_We will drop the token subscript, writing only \(r_{\sigma,\min}\) or \(r_{\sigma,\max}\), when the token values are unambiguous by context._

[MISSING_PAGE_EMPTY:34]

[MISSING_PAGE_FAIL:35]

Proof.: The proof goes by straightforward computation.

\[\operatorname{sign}((s_{\sigma_{x},t_{x,0},\ldots,t_{x,n_{\text{ctx} -1}}})-(s_{\sigma_{y},t_{y,0},\ldots,t_{y,n_{\text{ctx}-1}}}))\] \[=\operatorname{sign}\left(\frac{v_{x}e^{a_{x}}f+c}{e^{a_{x}}f+d}- \frac{v_{y}e^{a_{y}}f+c}{e^{a_{y}}f+d}\right)\]

Multiply through by non-negative denominators

\[=\operatorname{sign}\left((v_{x}e^{a_{x}}f+c)\left(e^{a_{y}}f+d \right)-(v_{y}e^{a_{y}}f+c)\left(e^{a_{x}}f+d\right)\right)\] \[=\operatorname{sign}\left(-cfe^{a_{x}}+cfe^{a_{y}}+dfv_{x}e^{a_{x }}-dfv_{y}e^{a_{y}}+f^{2}v_{x}e^{a_{x}+a_{y}}-f^{2}v_{y}e^{a_{x}+a_{y}}\right)\]

Use \(f>0\)

\[=\operatorname{sign}\left(-ce^{a_{x}}+ce^{a_{y}}+dv_{x}e^{a_{x}}- dv_{y}e^{a_{y}}+fv_{x}e^{a_{x}+a_{y}}-fv_{y}e^{a_{x}+a_{y}}\right)\] \[=\operatorname{sign}\left(c\left(e^{a_{y}}-e^{a_{x}}\right)+d \left(v_{x}e^{a_{x}}-v_{y}e^{a_{y}}\right)+f\left(v_{x}e^{a_{x}+a_{y}}-v_{y}e^ {a_{x}+a_{y}}\right)\right)\] \[=\operatorname{cmp}(x,y)\]

**Corollary 10**.: _Define the relation \(\leq_{\operatorname{cmp}}\) by \(x\leq_{\operatorname{cmp}}y\) if and only if \(\operatorname{cmp}(x,y)\in\{-1,0\}\). The relation \(\leq_{\operatorname{cmp}}\) is always transitive._

Proof.: Note that by Lemma 9, \(\operatorname{cmp}\) is comparing two sequence scores. Since \(\leq\) is transitive over the reals, the relation \(\leq_{\operatorname{cmp}}\) is also transitive. 

Finally, we combine the previous lemmas to complete our proof of Theorem 7:

Proof of Theorem 7.: Extremal sequences with scores \(s_{\sigma,\min}\) and \(s_{\sigma,\max}\) are guaranteed to exist because there are only finitely many elements of \(S\) and therefore only finitely many sequences. By Lemma 8, the extremal sequences must be pure (have \(t_{i}=t_{j}\) whenever \(\sigma(i),\sigma(j)\not\in F\)). By Lemma 9, the extremal sequences must have tokens that are extremal according to \(\operatorname{cmp}\). 

We now have all the tools necessary to prove the following theorem. We refer to Algorithm 3 and Algorithm 4 or the proof of Theorem 11 for a definition of the cubic algorithm.

**Theorem 11**.: \[\mathbb{E}_{\mathbf{t}\sim U(0,1,\ldots,d_{\operatorname{vocab} -1})^{n_{\text{ctx}}}}\left[\operatorname*{argmax}_{i}(\mathcal{M}(\mathbf{ t}))_{i}=\max_{i}t_{i}\right]\geq\textsc{cubic}(d_{\operatorname{vocab}},n_{\text{ctx}},\mathcal{M})\]

Before we give the proof of this theorem, we introduce some helpful notation.

**Definition 12**.: _Fix an element \((r_{m},r_{q},c)\in\{0,...,d_{\operatorname{vocab}}\}^{2}\times\{0,...3\}\) such that \(r_{m}\geq r_{q}\). We define \(X_{(r_{m},r_{q},c)}\) to be the set of tokens \(\mathbf{t}\) such that_

1. _The max token_ \(t_{\max}\) _is equal to_ \(r_{m}\)

Figure 13: Recapulation of some relevant definitions from Figure 12, parameterized by the arguments they actually depend on.

2. _The query token_ \(t_{\mathrm{query}}\) _is equal to_ \(r_{q}\)_,_
3. _The cardinality of tokens that are not at the query position and not equal to_ \(t_{\max}\) _is equal to_ \(c\)_._

For clarity, we list all the possible cases. We always take \(t_{\mathrm{query}}\leq t_{\max}\) and let \(S_{3}\) act on sequences by permuting the first three factors (i.e. keeping the query position fixed).

1. If \(c=0\), then \(X_{(t_{\max},t_{\mathrm{query}},0)}=\{[t_{\max},t_{\max},t_{\max},t_{\mathrm{ query}}]\}\),
2. If \(c=1\), then \(X_{(t_{\max},t_{\mathrm{query}},1)}=S_{3}.\{[t_{1},t_{\max},t_{\max},t_{ \mathrm{query}}]\mid t_{1}<t_{\max}\}\),
3. If \(c=2\), then \(X_{(t_{\max},t_{\mathrm{query}},2)}=S_{3}.\{[t_{1},t_{2},t_{\max},t_{\mathrm{ query}}]\mid t_{i}<t_{\max}\}\),
4. If \(c=3\), then \(X_{(t_{\max},t_{\max},3)}=S_{3}.\{[t_{1},t_{2},t_{3},t_{\max}]\mid t_{i}<t_{ \max}\}\).

**Definition 13**.: _Let \(\mathbf{t}\in X\) be a sequence. We say \(t\) is pure, if it has at most three distinct tokens: the max token \(t_{\max}\), the query token \(t_{\mathrm{query}}\), and optionally a third token \(t^{*}<t_{\max}\)._

_We denote by \(X^{pure}\) the subset of pure tokens. For any subset \(Y\subset X\), we set \(Y^{pure}:=Y\cap X^{pure}\)._

We now come to the proof of Theorem 11. We will show how to use the previous theorems to get explicit bounds and explain how \(\textsc{cubic}(d_{\mathrm{vocab}},n_{\mathrm{ctx}},\mathcal{M})\) computes these bounds.

Proof of Theorem 11.: First of all, we note that the algorithm \(\textsc{cubic}=\textsc{cubic}(d_{\mathrm{vocab}},n_{\mathrm{ctx}}, \mathcal{M})\) yields a lower bound for the accuracy on the set \(X_{(t_{\max},t_{\mathrm{query}},c)}\). We can therefore compute the bound on \(X=\coprod_{(t_{\max},t_{\mathrm{query}},c)}X_{(t_{\max},t_{\mathrm{query}},c)}\) by computing it for each such choice \((t_{\max},t_{\mathrm{query}},c)\) and summing over them

\[\mathbb{E}_{\mathbf{t}\sim U(0,1,...,d_{\mathrm{vocab}}-1)^{n_{\mathrm{ctx }}}}\left[\operatorname*{argmax}_{i}(\mathcal{M}(\mathbf{t}))_{i}=\max_{i}t_ {i}\right]\geq\sum_{(t_{\max},t_{\mathrm{query}},c)}\textsc{cubic}(X_{(t_{ \max},t_{\mathrm{query}},c)}).\]

So from now on we will fix one such subset \(X_{(t_{\max},t_{\mathrm{query}},c)}\).

We begin by defining a map

\[f:X_{(t_{\max},t_{\mathrm{query}},c)}\rightarrow\{0,...,d_{\mathrm{vocab}}\}^{c}\]

which sends a sequence to the subsequence of elements which are not at the query position and not equal to \(t_{\max}\). Then Theorem 7 can be restated as follows22:

Footnote 22: In fact, the theorem yields a stronger result, but we will only need the following formulation.

```
1:functionmodel-behavior-relaxed(\(\mathcal{M}\), query-tok, max-tok, non-max-tok, n-copies-nonmax)
2:\(t_{\mathrm{query}}\leftarrow\) query-tok, \(t_{\mathrm{max}}\leftarrow\) max-tok, \(t^{\prime}\leftarrow\) non-max-tok, \(c\leftarrow\) n-copies-nonmax
3:\(0\leq t_{\mathrm{query}}\leq t_{\mathrm{max}}<d_{\mathrm{vocab}}\), \(0\leq t^{\prime}\leq t_{\mathrm{max}}<d_{\mathrm{vocab}}\), \(0\leq c<n_{\mathrm{ctx}}\)
4:if n-copies-nonmax\(=0\)then non-max-tok\(=\) max-tok
5:if query-tok\(\neq\) max-tok then n-copies-nonmax\(<n_{\mathrm{ctx}}-1\)
6:return\(\geq\) model-behavior\((\mathcal{M},\mathbf{t})\)for all\(\mathbf{t}\) with specified \(t_{\mathrm{query}}\), \(c\) copies of \(t^{\prime}\) in non-query positions, and the remainder of the tokens equal to \(t_{\mathrm{max}}\)
7: skip-score\({}_{t^{*}}\leftarrow\Delta\ell^{\mathrm{EU}_{t^{*}}}(t_{\mathrm{query}},t_{ \mathrm{max}})\)\(\triangleright\) Cache by \(t_{\mathrm{max}}\), \(t_{\mathrm{query}}\), \(t^{*}\)
8:\(w_{i}\leftarrow\mathrm{PVOU}(i)\)for\(0\leq i<n_{\mathrm{ctx}}\)\(\triangleright\) Cache by \(i\)
9:\(\Delta w_{\mathrm{max},t^{*}}\leftarrow\max_{0\leq i<n_{\mathrm{ctx}}}(w_{i,t^ {*}}-w_{i,t_{\mathrm{max}}})\)\(\triangleright\) Cache by \(t_{\mathrm{max}}\), \(t^{*}\)
10:\(v_{t}\leftarrow\mathrm{EVOU}(t)\), \(\Delta v_{t,t^{*}}\gets v_{t,t^{*}}-v_{t,t_{\mathrm{max}}}\)for\(t\in\{t_{\mathrm{query}},t_{\mathrm{max}},t^{\prime}\}\)\(\triangleright\) Cache by \(t_{\mathrm{max}}\), \(t\), \(t^{*}\)
11:\(a_{t}\leftarrow\mathrm{EQKE}(t_{\mathrm{query}},t)/\sqrt{d}\)for\(t\in\{t_{\mathrm{query}},t_{\mathrm{max}},t^{\prime}\}\)\(\triangleright\) Cache by \(t_{\mathrm{query}}\), \(t\)
12:\(b_{n_{\mathrm{ctx}}-1}\leftarrow\mathrm{EQKP}(t_{\mathrm{query}},n_{\mathrm{ctx}}-1)/\sqrt{d}\)\(\triangleright\) Cache by \(t_{\mathrm{query}}\)
13:\(b_{0,:-1}\leftarrow\textsc{sort}(\mathrm{EQKP}(t_{\mathrm{query}},:-1))/\sqrt{d}\)\(\triangleright\) Cache by \(t_{\mathrm{query}}\), \(i\)
14:\(b_{1,:-1}\leftarrow\textsc{reverse}(b_{0,:-1})\)
15:attn-weights-unscaled\({}_{,n_{\mathrm{ctx}}-1}\gets a_{t_{\mathrm{query}}}+b_{n_{ \mathrm{ctx}}-1}\)\(\triangleright\) Cache by \(t_{\mathrm{query}}\)
16:attn-weights-unscaled\({}_{0,i}\gets a_{t_{\mathrm{max}}}+b_{0,i}\)for\(0\leq i<n_{\mathrm{ctx}}-c-1\)\(\triangleright\) Cache by \(t_{\mathrm{max}}\), \(c\), \(i\), \(t_{\mathrm{query}}\)
17:attn-weights\({}_{1}\leftarrow\textsc{softmax}(\)attn-weights-unscaled\({}_{1})\)\(\triangleright\) Cache by \(t_{\mathrm{max}}\), \(t^{\prime}\), \(c\), \(i\), \(t_{\mathrm{query}}\)
18:if\(c=0\)then\(\triangleright\) In this case, attn-weights\({}_{0,i}=\) attn-weights\({}_{1,i}\), so we drop the first subscript
19:return\(\max_{t^{*}\neq t_{\mathrm{max}}}(\)skip-score\({}_{t^{*}}\)\(+\)\(\Delta w_{\mathrm{max},t^{*}}\)\(+\)\(\Delta v_{t_{-1,t^{*}}}\)attn-weights\({}_{-1}\)\(+\)\(\Delta v_{t_{\mathrm{max}},t^{*}}\sum_{i=0}^{n_{\mathrm{ctx}}-2}\)attn-weights\({}_{i}\))
20:else
21:\(\Delta v_{i,t^{*}}\leftarrow\Delta v_{t_{\mathrm{max}},t^{*}}\)for\(0\leq i<n_{\mathrm{ctx}}-c-1\)
22:\(\Delta v_{i,t^{*}}\leftarrow\Delta v_{t^{\prime},t^{*}}\)for\(n_{\mathrm{ctx}}-c-1\leq i<n_{\mathrm{ctx}}-1\)
23:\(\Delta v_{n_{\mathrm{ctx}}-1,t^{*}}\leftarrow\Delta v_{t_{\mathrm{query}},n_{ \mathrm{ctx}}-1}\)
24:return\(\max_{t^{*}\neq t_{\mathrm{max}}}\) skip-score\({}_{t^{*}}\)\(+\)max\(\left\{\sum_{i=0}^{n_{\mathrm{ctx}}-1}\max_{t^{*}\neq t_{\mathrm{max}}}(\Delta w_{ \mathrm{max},t^{*}}+\Delta v_{i,t^{*}})\cdot\mathrm{attn-weights}_{1,i}\right.\)
25:endif
26:endfunction
27:functionrelaxed-correctness-pessimizing-over-position(\(\mathcal{M}\), \(t_{\mathrm{query}}\), \(t_{\mathrm{max}}\), \(t^{\prime}\), \(c\))
28:\(\triangleright\) runs the model on a relaxed variant of input sequences compatible with the arguments
29:return is False if correctness-pessimizing-over-position-slow(\(\mathcal{M}\), \(\mathbf{t}\)) is False for any\(\mathbf{t}\) with specified \(t_{\mathrm{query}}\), \(c\) copies of \(t^{\prime}\) in non-query positions, and the remainder of the tokens equal to \(t_{\mathrm{max}}\)
30:return model-behavior-relaxed\((\mathcal{M},t_{\mathrm{query}},t_{\mathrm{max}},t^{\prime},c)<0\)
31:endfunction ```

**Algorithm 3** Counting Correct Sequences in Cubic Time, Part I. Lines are annotated with comments indicating the parameters for a cache to avoid duplicate computations.

Let \(S\subset\{0,...,d_{\rmvocab}\}\). Then full accuracy \(f^{-1}(S^{c})^{pure}:=X_{(t_{\max},t_{\rm query},c)}^{pure}\cap f^{-1}(S^{c})\), implies full accuracy on \(f^{-1}(S^{c})\).

Now instead of computing the output of the model for every element \(f^{-1}(S^{c})^{pure}\), we use Theorem 4 (combined with Theorem 6) to run a relaxed version of this. In particular, we may assume that the pure sequence is contiguous on equal tokens. Here contiguous on equal tokens means that for the positional part of the attention (i.e. the EQKP part), we have either \(b_{t_{\max}}<\{b_{i},b_{j}\}\) or \(b_{t_{\max}}>\{b_{i},b_{j}\}\), where \(i,j\in\{0,...,n_{\rm ctx}-1\}\) are indices of tokens not equal to \(t_{\max}\).

For the algorithm \(\textsc{cubic}(d_{\rmvocab},n_{\rm ctx},\mathcal{M})\) we fix a \(t^{*}\in\{0,...,t_{\max}-1\}\) (unless \(c=0\), in which case there is no such choice). We then run the relaxed accuracy computation \(\textsc{RCPOP}(t_{\rm query},t_{\max},t^{\prime},c)\) as described in Theorem 6. If \(\textsc{RCPOP}(t_{\rm query},t_{\max},t^{\prime},c)<0\), we add \(t^{\prime}\) to \(S\). If we do, we add \(t^{*}\) to \(S\). Therefore by construction of \(S\) we know that we get full accuracy on \(f^{-1}(S^{c})^{pure}\) and therefore we get full accuracy on \(f^{-1}(S^{c})\).

Now we count the cardinality of \(f^{-1}(S^{c})\) and add it to the count of correct sequences.

**Theorem 12**.: _The running time of Algorithm 3, after using caching to avoid duplicate computations, is \(\mathcal{O}(d_{\rmvocab}{}^{3}n_{\rm ctx}{}^{2})\)._

Proof.: The nested loops in cubic execute the innermost body \(\mathcal{O}(d_{\rmvocab}{}^{2}n_{\rm ctx})\) times, and the summation on Line 13 costs \(\mathcal{O}(n_{\rm ctx})\) per iteration. What remains is to show that the call to relaxed-correctness-pessimizing-over-position\((\mathcal{M},t_{\rm query},t_{\max},t^{\prime},c)\) costs \(\mathcal{O}(n_{\rm ctx})\) when \(c\neq 0\) and at most \(\mathcal{O}(d_{\rmvocab}n_{\rm ctx})\) when \(c=0\) and \(t^{\prime}=t_{\max}\).

The matrix multiplications in EQKE, EQKP, \(\textsc{EVOU}\), \(\textsc{PVOU}\), and \(\ell^{\rm EU}\) can be cached upfront, costing \(\mathcal{O}(\max(d_{\rmvocab},d_{\rm model},n_{\rm ctx})^{2}d_{\rm model}) \leq\mathcal{O}(d_{\rmvocab}{}^{3})\) since we assume \(d_{\rmvocab}>d_{\rm model}\) and \(d_{\rmvocab}>n_{\rm ctx}\).

The sorting on Line 9 can also be cached upfront (per \(t_{\rm query}\)), costing \(\mathcal{O}(d_{\rmvocab}n_{\rm ctx}\log n_{\rm ctx})\).

Note that each variable assignment in relaxed-correctness-pessimizing-over-position can be cached into a table parameterized over at most three variables which range over \(d_{\rmvocab}\) and over at most two variables that range over \(n_{\rm ctx}\).

What remains is the **return** statements.

When \(c=0\), we have on Line 19: **return**\(\max_{t^{*}\neq t_{\max}}(\text{skip-score}_{t^{*}}+\Delta w_{\max,t^{*}}+ \Delta v_{t_{-1},t^{*}}\text{attn-weights}_{-1}+\Delta v_{t_{\max},t^{*}} \sum_{i=0}^{n_{\rm ctx}-2}\text{attn-weights}_{i})\). This is \(\mathcal{O}(d_{\rm vocab}n_{\rm ctx})\) as desired.

[MISSING_PAGE_FAIL:40]

[MISSING_PAGE_FAIL:41]

[MISSING_PAGE_FAIL:42]

[MISSING_PAGE_EMPTY:43]

```
1:functionmodel-behavior-relaxed-over-gap(\(\mathcal{M}\), \(t_{\max}\), \(t_{\rm query}\), \(c\), \(g\), \(g^{*}\))
2:correctness-pessimizing-over-gap-slow is False \(\Longrightarrow\) result is False
3:\(0\leq g^{*}\leq g\leq t_{\max}\)
4:if\(c=0\)then\(t_{\rm query}=t_{\max}\)
5: skip-score \(\leftarrow\max_{t^{\prime}}\ell^{\rm EU}(t_{\rm query})_{t^{*}}-\min_{t^{*}} \ell^{\rm EU}(t_{\rm query})_{t^{*}}\)\(\triangleright\) Cache by \(t_{\rm query}\)
6:\(v_{t}\leftarrow\text{EVOU}(t)\)
7:\(w_{i}\leftarrow\text{PVOU}(i)\)
8:\(\Delta w_{\max,t^{*}}\leftarrow\max_{i}w_{i,t^{*}}-w_{i,t_{\max}}\)\(\triangleright\) Cache by \(t_{\max}\), \(t^{*}\)
9:\(\Delta w_{\max,\max}\leftarrow\max_{t^{*}}\Delta w_{\max,t^{*}}\)\(\triangleright\) Cache by \(t_{\max}\)
10:\(\Delta v_{t}\leftarrow\max_{t^{*}}v_{t,t^{*}}-\min_{t^{*}}v_{t,t^{*}}\)\(\triangleright\) Cache by \(t\)
11:\(\Delta v_{\max}\leftarrow\max_{0\leq t\leq t_{\max}-g^{*}}\Delta v_{t}\)\(\triangleright\) Cache by \(t_{\max}-g^{*}\)
12:\(\Delta v_{t^{\prime\max}}^{t_{\max}}\gets v_{t_{\max},t^{*}}-v_{t_{\max},t _{\max}}\)\(\triangleright\) Cache by \(t_{\max}\)
13:\(\Delta v_{t^{\prime\max}}^{t_{\max}}\leftarrow\max_{t^{*}\neq t_{\max}}\Delta v _{t^{\prime\max}}\)\(\triangleright\) Cache by \(t_{\max}\)
14:if\(c=0\)then
15:\(\ell_{t^{*}}\leftarrow\ell^{\rm EU}(t_{\max})_{t^{*}}+v_{t_{\max},t^{*}}+\Delta w _{\max,t^{*}}\)
16:return\(\max_{t^{*}\neq t_{\max}}(\ell_{t^{*}}-\ell_{t_{\max}})\)
17:endif
18:\(b_{:,n_{\rm ctx}-1}\leftarrow\text{EQKP}(t_{\rm query},n_{\rm ctx}-1)/\sqrt{d}\)\(\triangleright\) Cache by \(t_{\rm query}\)
19:\(b_{0,:-1}\leftarrow\text{\scort}(\text{EQKP}(t_{\rm query},:-1))/\sqrt{d}\)\(\triangleright\) Cache by \(t_{\rm query}\), \(i\)
20:\(b_{1,:-1}\leftarrow\text{\sc reverse}(b_{0,:-1})\)
21:\(a_{t}\leftarrow\text{EQKE}(t_{\rm query},t)/\sqrt{d}\)\(\triangleright\) Cache by \(t_{\rm query}\), \(t\)
22:\(a_{\min,t}\leftarrow\min_{0\leq t^{\prime\prime}\leq t}a_{t^{\prime\prime}}\)\(\triangleright\) Cache by \(t_{\rm query}\), \(t\), compute in amortized \(\mathcal{O}({d_{\rm vocab}}^{2})\)
23:\(a_{\max,t}\leftarrow\max_{0\leq t^{\prime\prime}\leq t}a_{t^{\prime\prime}}\)\(\triangleright\) Cache by \(t_{\rm query}\), \(t\), compute in amortized \(\mathcal{O}({d_{\rm vocab}}^{2})\)
24:\(\Delta a_{\max}\gets a_{t_{\max}}-a_{\min,t_{\max}-g}\)\(\triangleright\) Cache by \(t_{\rm query}\), \(t_{\max}\), \(c\)
25:\(\Delta a_{\min}\gets a_{t_{\max}}-a_{\max,t_{\max}-g}\)\(\triangleright\) Cache by \(t_{\rm query}\), \(t_{\max}\), \(c\)
26:idx-set \(\leftarrow\{0,\ldots,n_{\rm ctx}-c-1\}\)if\(t_{\max}\neq t_{\rm query}\)else\(\{0,\ldots,n_{\rm ctx}-c-2,n_{\rm ctx}-1\}\)
27:attn-weights-unscaled\({}_{0,i}\gets b_{0,i}+(\Delta a_{\min}\)if\(i\in\text{idx-set}\)else\(0)\)
28:attn-weights-unscaled\({}_{1,i}\gets b_{1,i}+(\Delta a_{\max}\)if\(i\in\text{idx-set}\)else\(0)\)\(\triangleright\) Cache by \(t_{\rm query}\), \(t_{\max}\), \(i\), \(c\)
29:\(\text{attn-weights}_{0}\leftarrow\text{softmax}(\text{attn-weights-unscaled}_{0})\)\(\triangleright\) Cache by \(t_{\rm query}\), \(t_{\max}\), \(i\), \(c\)
30:attn-weights\({}_{1}\leftarrow\text{softmax}(\text{attn-weights-unscaled}_{1})\)\(\triangleright\) Cache by \(t_{\rm query}\), \(t_{\max}\), \(i\), \(c\)
31:\(\text{attn-max}_{0}\leftarrow\sum_{i\in\text{idx-set}}\text{attn-weights}_{0,i}\)
32:attn-max\({}_{1}\leftarrow\sum_{i\in\text{idx-set}}\text{attn-weights}_{1,i}\)
33:attn-max \(\leftarrow\)attn-max\({}_{0}\)if\(\Delta v_{\max}^{t_{\max}}<\Delta v_{\max}\)else\(\text{attn-max}_{1}\)
34:\(\triangleright\) Recall that \(\Delta v_{\max}^{t_{\max}}\) is negative when the model outputs the correct answer
35:return\(\text{skip-score}+\Delta w_{\max,\max}+\text{attn-max}\cdot\Delta v_{\max}^{t_{ \max}}+(1-\text{attn-max})\Delta v_{\max}\)
36:endfunction
37:functionrelaxed-correctness-pessimizing-over-gap(\(\mathcal{M}\), \(d_{\rm vocab}\), \(n_{\rm ctx}\), \(t_{\max}\), \(t_{\rm query}\), \(c\), \(g\), \(g^{*}\))
38:\(\triangleright\) runs the model on a relaxed variant of input sequences compatible with the arguments
39:correctness-pessimizing-over-gap-slow is False \(\Longrightarrow\) result is False
40:return is False if correctness-pessimizing-over-gap-slow(\(\mathcal{M}\), \(\mathbf{t}\)) is False for any \(\mathbf{t}\) with specified \(t_{\max}\), \(t_{\rm query}\), and \(c\) tokens not equal to \(t_{\max}\)
41:return model-behavior-relaxed-over-gap(\(\mathcal{M},t_{\max},t_{\rm query},c,g,g^{*})<0\)
42:endfunction ```

**Algorithm 6** Counting Correct Sequences in Subcubic Time

**Theorem 16**.: _For all \(G\),_

\[\mathbb{E}_{\mathbf{t}\sim U(0,1,\ldots,d_{\mathrm{vocab}}-1)^{n_{\mathrm{ctx}}}} \left[\operatorname*{argmax}_{i}(\mathcal{M}(\mathbf{t}))_{i}=\max_{i}t_{i} \right]\geq\textsc{subcubic}(d_{\mathrm{vocab}},n_{\mathrm{ctx}},\mathcal{M},G)\]

Proof.: (sketch) Apply preceding lemmas and theorems to Algorithm6 

**Theorem 17**.: _The running time of Algorithm6, after using caching to avoid duplicate computations, is \(\mathcal{O}(d_{\mathrm{vocab}}{}^{2}d_{\mathrm{model}}+d_{\mathrm{vocab}}{}^{2}n_{\mathrm{ctx}}{}^{2})\)._

Proof.: (sketch) Sum the complexities indicated along the right side of Algorithm3. The \(d_{\mathrm{vocab}}{}^{2}d_{\mathrm{model}}\) term comes from the precomputing \(\mathrm{EVOU}\), \(\mathrm{EU}\), and \(\mathrm{EQKP}\). The \(d_{\mathrm{vocab}}{}^{2}n_{\mathrm{ctx}}{}^{2}\) term comes from the softmax over \(n_{\mathrm{ctx}}\) tokens for \(O(d_{\mathrm{vocab}}{}^{2}n_{\mathrm{ctx}})\) pessimized pure sequences. Confirming that none of the complexities on the right side exceeds \(\mathcal{O}(d_{\mathrm{vocab}}{}^{2}d_{\mathrm{model}}+d_{\mathrm{vocab}}{}^{2}n_{ \mathrm{ctx}}{}^{2})\) completes the proof.

## Appendix G Subcubic proof strategies

In this section, we present a number of proof strategies that we use to reduce the computational cost of the proof, ultimately driving down the cost of \(\mathrm{EU}\) and \(\mathrm{EQKE}\) verification to \(\mathcal{O}(d_{\mathrm{vocab}}d_{\mathrm{model}})\), while unfortunately leaving the cost of \(\mathrm{EVOU}\) verification at \(\mathcal{O}(d_{\mathrm{vocab}}{}^{2}d_{\mathrm{model}})\).

The three main tricks we cover are the mean+diff trick (AppendixG.1), the max row-diff trick (AppendixG.2.2), and the rank one / rank two SVD decomposition of \(\mathrm{EQKE}\) (AppendixG.2.3). While the mean+diff trick is useful for getting slightly better bounds, the SVD decomposition of \(\mathrm{EQKE}\) is the place where we get to insert the most understanding (without which we'd have no hope of non-vacuous bounds below \(\mathcal{O}(d_{\mathrm{vocab}}{}^{2}d_{\mathrm{model}})\)), and the max row-diff trick is the workhorse that allows us to drive down the error term computations from cubic to quadratic without getting completely vacuous bounds.

### The mean+diff trick

Suppose we have quantities \(f_{x,y}\) and \(g_{y,z}\) and we want to pessimize (WLOG, suppose minimize) the quantity \(f_{x,y}+g_{y,z}\) over \(x\), \(y\), and \(z\) in time less than \(\mathcal{O}(n_{x}n_{y}n_{z})\), say we allow \(\mathcal{O}(n_{x}n_{y}+n_{y}n_{z}+n_{x}n_{z})\). Also suppose the variation of \(f\) over the \(y\) axis is much larger than the variation of f over the x-axis.

We can of course say

\[\min_{x,y}f_{x,y}+\min_{y,z}g_{y,z}\leq f_{x,y}+g_{y,z}\]

But we can do better!

Note that

\[f_{x,y}=\mathbb{E}_{x}f_{x,y}+(f_{x,y}-\mathbb{E}_{x}f_{x,y})\]

Suppose that \(f_{x,y}\) varies much less over \(x\) than it does over \(y\), and much less than \(g_{y,z}\) varies over either of \(y\) and \(z\). This will make the following bound a good approximation, though the bound is sound even without this assumption. We can write

\[f_{x,y}+g_{y,z} \geq\min_{x,y,z}[f_{x,y}+g_{y,z}]\] \[=\min_{x,y,z}[\mathbb{E}_{x}f_{x,y}+g_{y,z}+f_{x,y}-\mathbb{E}_{x }f_{x,y}]\] \[\geq\min_{x,y,z}[\mathbb{E}_{x}f_{x,y}+g_{y,z}]+\min_{x,y,z}[f_{ x,y}-\mathbb{E}_{x}f_{x,y}]\] \[=\min_{y,z}[\mathbb{E}_{x}f_{x,y}+g_{y,z}]+\min_{x,y}[f_{x,y}- \mathbb{E}_{x}f_{x,y}]\]

By averaging the variation over certain axes, we have

**Theorem 18** (Mean+Diff).: \[\min_{x,y,z}f_{x,y}+g_{y,z} \geq\min_{y,z}[\mathbb{E}_{x}f_{x,y}+g_{y,z}]+\min_{x,y}[f_{x,y}- \mathbb{E}_{x}f_{x,y}]\] \[\max_{x,y,z}f_{x,y}+g_{y,z} \leq\max_{y,z}[\mathbb{E}_{x}f_{x,y}+g_{y,z}]+\max_{x,y}[f_{x,y}- \mathbb{E}_{x}f_{x,y}]\]

_and the RHSs can be computed in time \(\mathcal{O}(n_{x}n_{y}+n_{y}n_{z}+n_{x}n_{z})\) for \(n_{x}\), \(n_{y}\), and \(n_{z}\) the number of possible values of \(x\), \(y\), and \(z\), respectively._

Example for how this helps with small variation:

Take any function \(k(y)\) and then take

\[f_{x,y} :=k(y)+\varepsilon_{1}(x,y)\] \[g_{y,z} :=-k(y)+\varepsilon_{2}(y,z)\]

Then we have

\[\min_{x,y,z}[f_{x,y}+g_{y,z}] =\min_{x,y,z}[\varepsilon_{1}(x,y)+\varepsilon_{2}(y,z)]\] \[\min_{x,y}f_{x,y}+\min_{y,z}g_{y,z} =\min_{y}k(y)+\min_{y}-k(y)+\min_{x,y}\varepsilon_{1}(x,y)+\min_{ y,z}\varepsilon_{2}(y,z)\] \[=\min_{y}k(y)-\max_{y}k(y)+\min_{x,y}\varepsilon_{1}(x,y)+\min_{ y,z}\varepsilon_{2}(y,z)\] \[\min_{x,y}[f_{x,y}-\mathbb{E}_{x}f_{x,y}]+\min_{y,z}[g_{y,z}+ \mathbb{E}_{x}f_{x,y}] =\min_{x,y}\varepsilon_{1}(x,y)+\min_{y,z}[\varepsilon_{2}(y,z)+ \mathbb{E}_{x}\varepsilon_{1}(x,y)]\]

If \(\varepsilon_{1}\) and \(\varepsilon_{2}\) are small compared to \(\min_{y}k(y)-\max_{y}k(y)\), then using \(\mathbb{E}_{x}f_{x,y}\) gives a much better bound.

Note, though, that this could be a worse bound if the assumption of small variation does not hold.

Note also that this trick is not restricted to adding and subtracting \(\mathbb{E}_{x}f_{x,y}\). If \(f\) is a matrix indexed by \(x\) and \(y\), we might also try taking SVD and using the first principal component instead. A basic application of the triangle inequality gives the following, more general, result:

**Theorem 19** (Summarize+Diff).: _For any \(h_{y}\) which can be computed in time \(\mathcal{O}(n_{h})\),_

\[\min_{x,y,z}f_{x,y}+g_{y,z} \geq\min_{y,z}[h_{y}+g_{y,z}]+\min_{x,y}[f_{x,y}-h_{y}]\] \[\max_{x,y,z}f_{x,y}+g_{y,z} \leq\max_{y,z}[h_{y}+g_{y,z}]+\max_{x,y}[f_{x,y}-h_{y}]\]

_and the RHSs can be computed in time \(\mathcal{O}(n_{x}n_{y}+n_{y}n_{z}+n_{h})\) for \(n_{x}\), \(n_{y}\), and \(n_{z}\) the number of possible values of \(x\), \(y\), and \(z\), respectively._

We see that if the variation of \(f\) in the \(x\)-axis is indeed much smaller than the variation in the \(y\)-axis, then letting

\[f_{x,y}=h_{y}+\varepsilon_{x,y}\]

we get

\[\left|\min_{x,y,z}f_{x,y}+g_{y,z}-\min_{y,z}[h_{y}+g_{y,z}]-\min_ {x,y}[f_{x,y}-h_{y}]\right|\] \[\leq\left|\min_{x,y,z}[f_{x,y}+g_{y,z}]-\min_{y,z}[h_{y}+g_{y,z}] \right|+\left|\min_{x,y}[\varepsilon_{x,y}]\right|\] \[\leq 2\max_{x,y}|\varepsilon_{x,y}|\]

so indeed this bound isn't too much worse and we are able to compute it in quadratic rather than cubic time.

### Details of SVD of QK proof

As discussed in Section 4.3.1, to further reduce the computation cost of proof, we need to avoid computing the residual stream, EVOU, and EPQKE matrices fully. Using mechanistic insight or otherwise, we observe that these matrices (apart from EVOU) can be well-approximated by rank one matrices. This will remove the dominant computation cost of \(\mathcal{O}(d_{\mathrm{vocab}}\,^{2}\cdot d_{\mathrm{model}})\).

#### g.2.1 Comments on relationship between mechanistic insight and proof size

Up to this point, we haven't really said much in our proofs about what the model is doing. All the mechanistic insight has been of the form "the model varies more along this axis than this other axis" or "the input data is distributed such that handling these inputs is more important than handling these other inputs" or, at best, "the model computes the answer by attending to the maximum token of the sequence; everything else is noise".

Here, finally, our proof-size constraints are tight enough that we will see something that we could plausibly call "how the model pays attention to the maximum token more than anything else", i.e., (if we squint a bit) "the model pays more attention to larger tokens in general.

#### g.2.2 The max row-diff trick

As stated above, we are breaking matrices into their rank one approximation and some error term. To bound the error, i.e. to bound expressions of the form \(\prod_{i}(A_{i}+E_{i})-\prod_{i}A_{i}\), where \(E_{i}\) denote the matrix errors, we can use the following trick:

**Lemma 20** (Max Row-Diff (vector-matrix version)).: _For a row vector \(\mathbf{a}\) and a matrix \(B\),_

\[\max_{i,j}\left((\mathbf{a}B)_{i}-(\mathbf{a}B)_{j}\right)\leq\sum_{k}|a_{k}| \max_{i,j}\left(B_{k,i}-B_{k,j}\right)\]

_Moreover, for a collection of \(n\) row vectors \(A_{r}\), if the shape of \(B\) is \(m\times p\), the right hand side can be computed for all \(r\) in time \(\mathcal{O}(nm+mp)\)._

Proof.: \[\max_{i,j}(\mathbf{a}B)_{i}-(\mathbf{a}B)_{j}\] \[=\max_{i,j}\sum_{k}a_{k}\left(B_{k,i}-B_{k,j}\right)\] \[\leq\sum_{k}\max_{i,j}a_{k}\left(B_{k,i}-B_{k,j}\right)\] \[=\sum_{k}a_{k}\begin{cases}\max_{i,j}\left(B_{k,i}-B_{k,j}\right)& \text{if }a_{k}\geq 0\\ \min_{i,j}\left(B_{k,i}-B_{k,j}\right)&\text{if }a_{k}<0\end{cases}\] \[=\sum_{k}a_{k}\begin{cases}\max_{i,j}\left(B_{k,i}-B_{k,j}\right)& \text{if }a_{k}\geq 0\\ -\max_{i,j}\left(B_{k,i}-B_{k,j}\right)&\text{if }a_{k}<0\end{cases}\] \[=\sum_{k}|a_{k}|\max_{i,j}\left(B_{k,i}-B_{k,j}\right)\]

The asymptotic complexity of computing the result follows from caching the computation of \(\max_{i,j}\left(B_{k,i}-B_{k,j}\right)\) for each \(k\) independently of \(r\), as the computation does not depend on \(A_{r}\). 

**Theorem 21** (Max Row-Diff).: _For matrices \(A\) and \(B\),_

\[\max_{r,i,j}\left((AB)_{r,i}-(AB)_{r,j}\right)\leq\max_{r}\sum_{k}|A_{r,k}| \max_{i,j}\left(B_{k,i}-B_{k,j}\right)\]

Proof.: By taking the max of Lemma 20 over rows \(r\) of \(A\). 

Lemma 20 can also be applied recursively for a product of more than two matrices.

**Lemma 22** (Max Row-Diff (vector-matrix recursive version)).: _For a row vector \(\mathbf{a}\) and a sequence of \(n\) matrices \(B_{p}\) of shapes \(r_{p}\times c_{p}\),_

\[\max_{i,j}\left(\left(\mathbf{a}\prod_{p}B_{p}\right)_{i}-\left(\mathbf{a} \prod_{p}B_{p}\right)_{j}\right)\leq\sum_{k_{0}}|a_{k_{0}}|\cdots\sum_{k_{n}} \left|(B_{n-1})_{k_{n-1},k_{n}}\right|\max_{i,j}\left((B_{n})_{k_{n},i}-(B_{n })_{k_{n},j}\right)\]

_Moreover, for a collection of \(q\) row vectors \(A_{\alpha}\), the right hand side can be computed for all \(\alpha\) in time \(\mathcal{O}(qr_{0}+\sum_{p}r_{p}c_{p})\)._Proof.: We proceed by induction on \(n\).

For \(n=1\), the statement is identical to Lemma 20.

Suppose the theorem holds for all positive \(n=s\); we show the theorem holds for \(n=s+1\). We reassociate the matrix multiplication as

\[\max_{i,j}\left(\left(\mathbf{a}\prod_{p=1}^{s+1}B_{p}\right)_{i} -\left(\mathbf{a}\prod_{p=1}^{s+1}B_{p}\right)_{j}\right)\] \[=\max_{i,j}\left(\left(\mathbf{a}B_{1}\right)\left(\left(\prod_{p =2}^{s+1}B_{p}\right)_{i}-\left(\prod_{p=2}^{s+1}B_{p}\right)_{j}\right)\right)\]

Using the induction hypothesis gives

\[\leq\sum_{k_{1}}\left|\sum_{k_{0}}a_{k_{0}}(B_{1})_{k_{0},k_{1}}\right|\sum_{k _{2}}\left|(B_{2})_{k_{1},k_{2}}\right|\cdots\sum_{k_{s+1}}\left|(B_{s})_{k_{s },k_{s+1}}\right|\max_{i,j}\left((B_{s+1})_{k_{s+1},i}-(B_{s+1})_{k_{s+1},j}\right)\]

The triangle inequality gives

\[\leq\sum_{k_{1}}\sum_{k_{0}}\left|a_{k_{0}}(B_{1})_{k_{0},k_{1}}\right|\sum_{ k_{2}}\left|(B_{2})_{k_{1},k_{2}}\right|\cdots\sum_{k_{s+1}}\left|(B_{s})_{k_{s },k_{s+1}}\right|\max_{i,j}\left((B_{s+1})_{k_{s+1},i}-(B_{s+1})_{k_{s+1},j}\right)\]

and algebra gives

\[=\sum_{k_{0}}\left|a_{k_{0}}\right|\sum_{k_{1}}\left|(B_{1})_{k_{0},k_{1}} \right|\sum_{k_{2}}\left|(B_{2})_{k_{1},k_{2}}\right|\cdots\sum_{k_{s+1}}\left| (B_{s})_{k_{s},k_{s+1}}\right|\max_{i,j}\left((B_{s+1})_{k_{s+1},i}-(B_{s+1})_ {k_{s+1},j}\right)\]

The asymptotic complexity of computing the right hand side also follows straightforwardly by induction. 

**Theorem 23** (Max Row-Diff (recursive)).: _For a sequence of \(n+1\) matrices \(A_{0}\),..., \(A_{n}\),_

\[\max_{r,i,j}\left(\left(\prod_{p}A_{p}\right)_{r,i}-\left(\prod_{p}A_{p}\right) _{r,j}\right)\leq\max_{r}\sum_{k_{0}}\left|(A_{0})_{r,k_{0}}\right|\cdots\sum _{k_{n}}\left|(A_{n-1})_{k_{n-1},k_{n}}\right|\max_{i,j}\left((A_{n})_{k_{n}, i}-(A_{n})_{k_{n},j}\right)\]

Proof.: By taking the max of Lemma 22 over rows \(r\) of \(A_{0}\). 

Note that Theorem 21 is compatible with the mean+diff trick of Appendix G.1.

**Theorem 24** (Combined Mean+Diff and Max Row-Diff).: _For matrices \(A\) and \(B\), and any column-wise summary vector \(H_{k}\) of \(A\) (for example we may take \(H_{k}:=\mathbb{E}_{r}A_{r,k}\))_

\[\max_{r,i,j}\left((AB)_{r,i}-(AB)_{r,j}\right)\leq\left(\max_{i,j}\sum_{k}H_{k }\left(B_{k,i}-B_{k,j}\right)\right)+\max_{r}\sum_{k}\left|A_{r,k}-H_{k} \right|\max_{i,j}\left(B_{k,i}-B_{k,j}\right)\]

Proof.: \[\max_{r,i,j}\left((AB)_{r,i}-(AB)_{r,j}\right)\] \[=\max_{r,i,j}\sum_{k}A_{r,k}\left(B_{k,i}-B_{k,j}\right)\] \[=\max_{r,i,j}\sum_{k}\left(H_{k}+(A_{r,k}-H_{k})\right)\left(B_{k,i}-B_{k,j}\right)\] \[=\max_{i,j}\left(\sum_{k}H_{k}\left(B_{k,i}-B_{k,j}\right)+\max_{ r}\sum_{k}\left(A_{r,k}-H_{k}\right)\left(B_{k,i}-B_{k,j}\right)\right)\]\[\leq\left(\max_{i,j}\sum_{k}H_{k}\left(B_{k,i}-B_{k,j}\right)\right)+ \max_{r}\sum_{k}\max_{i,j}\left(A_{r,k}-H_{k}\right)\left(B_{k,i}-B_{k,j}\right)\] \[\leq\left(\max_{i,j}\sum_{k}H_{k}\left(B_{k,i}-B_{k,j}\right) \right)+\max_{r}\sum_{k}\left|A_{r,k}-H_{k}\right|\max_{i,j}\left(B_{k,i}-B_{k,j}\right)\]

**Theorem 25** (Combined Mean+Diff and Vector-Matrix Recursive Max Row-Diff).: _For a row vector \(\mathbf{a}\), a vector of summaries \(\mathbf{h}\) corresponding to \(\mathbf{a}\) (for example, if \(\mathbf{a}\) is a row of a matrix, \(\mathbf{h}\) might be the average of the rows), a sequence of \(n\) matrices \(B_{p}\) of shapes \(r_{p}\times c_{p}\), and a corresponding sequence of column-wise summary vectors \(\mathbf{h_{p}}\) of \(B_{p}\) (for example we may take \((h_{p})_{k}:=\mathbb{E}_{r}(B_{p})_{r,k}\)),_

\[\max_{i,j}\left(\left(\mathbf{a}\prod_{p}B_{p}\right)_{i}- \left(\mathbf{a}\prod_{p}B_{p}\right)_{j}\right)\] \[\leq\max_{i,j}\left(\sum_{k_{0}}h_{k_{0}}\cdots\sum_{k_{n}}(B_{n- 1})_{k_{n-1},k_{n}}\left((B_{n})_{k_{n},i}-(B_{n})_{k_{n},j}\right)\right)\] \[\quad+\sum_{k_{0}}\left|a_{k_{0}}-h_{k_{0}}\right|\cdots\left( \left(\max_{i,j}\sum_{k_{n}}(h_{n-1})_{k_{n}}\left((B_{n})_{k_{n},i}-(B_{n})_ {k_{n},j}\right)\right)\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\left.+\sum_{k_{n}} \left|(B_{n-1})_{k_{n-1},k_{n}}-(h_{n-1})_{k_{n}}\right|\max_{i,j}\left((B_{n} )_{k_{n},i}-(B_{n})_{k_{n},j}\right)\right)\]

_Moreover, for a collection of \(q\) row vectors \(A_{\alpha}\), the right hand side can be computed for all \(\alpha\) in time \(\mathcal{O}(qr_{0}+\sum_{p}r_{p}c_{p})\)._

Proof sketch.: Apply the triangle inequality recursively, fusing the proofs of Lemmas 22, and 24. 

**Theorem 26** (Combined Mean+Diff and Recursive Max Row-Diff).: _For a sequence of \(n+1\) matrices \(A_{0}\),..., \(A_{n}\), and corresponding column-wise summary vectors \(\mathbf{h_{0}}\),..., \(\mathbf{h_{n-1}}\) of \(A_{0}\),..., \(A_{n-1}\),_

\[\max_{r,i,j}\left(\left(\prod_{p}A_{p}\right)_{r,i}-\left(\prod_{p }A_{p}\right)_{r,j}\right)\] \[\leq\max_{r,i,j}\left(\sum_{k_{0}}(h_{0})_{k_{0}}\cdots\sum_{k_{n }}(A_{n-1})_{k_{n-1},k_{n}}\left((A_{n})_{k_{n},i}-(A_{n})_{k_{n},j}\right)\right)\] \[\quad+\sum_{k_{0}}\left|(A_{0})_{k_{0}}-(h_{0})_{k_{0}}\right| \cdots\left(\left(\max_{i,j}\sum_{k_{n}}(h_{n-1})_{k_{n}}\left((A_{n})_{k_{n},i}-(A_{n})_{k_{n},j}\right)\right)\right.\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\left.+\sum_{k_{n}} \left|(A_{n-1})_{k_{n-1},k_{n}}-(h_{n-1})_{k_{n}}\right|\max_{i,j}\left((A_{n} )_{k_{n},i}-(A_{n})_{k_{n},j}\right)\right)\]

_Moreover, if the matrices \(A_{p}\) have shapes \(r_{p}\times c_{p}\), the right hand side can be computed in time \(\mathcal{O}(\sum_{p}r_{p}c_{p})\)._

Proof.: By taking the max of Theorem 25 over rows \(r\) of \(A_{0}\). 

#### g.2.3 Exploring rank one approximation via SVD

Let us first look at

\[\mathrm{EQKE}:=\mathbf{E}_{q}QK^{T}\mathbf{\bar{E}}^{T}.\]

From Figure (a)a, we see that there is not much variation along long query token direction. We can confirm this by performing a singular value decomposition (SVD) on \(\mathrm{EQKE}\), as seen in Figure 14.

The first singular value is just over \(7440\) (\(7800\pm 380\) across all seeds), while the second singular value is just under \(15\) (\(13.1\pm 2.8\) across all seeds). The ratio across all seeds is \(620\pm 130\). There's really not much going on here beyond the first singular component.23

Footnote 23: We might be tempted to keep analyzing the SVD, and notice that the query direction is mostly uniform, while the key direction is monotonic (nearly linear, even). But the proof complexity doesn’t demand this level of analysis, yet, and so we can’t expect that any automated compact proof discovery system will give it to us.

Call the first singular component of EQKE the "query direction" \(d_{q}\) and the "size direction" \(d_{k}\) on the query-side and key-side, respectively.

There are two ways that we can decompose \(\mathrm{EQKE}\) into a low-rank component that we can compute exactly, and a full-rank error term that we approximate bounds for.

#### g.2.4 The simple SVD decomposition of QK

In time \(\mathcal{O}(d_{\mathrm{vocab}}d_{\mathrm{model}}{}^{2})\) we can perform SVD on each of the four component matrices \(\mathbf{E}_{q}\), \(Q\), \(K\), \(\mathbf{\bar{E}}\) and perform low-rank SVD on the matrix product \(\mathbf{E}_{q}QK^{T}\mathbf{\bar{E}}^{T}\).

We can then bound the difference between two elements in the same row of \(\mathrm{EQKE}\) by computing exactly the difference between the two elements in the same row of the rank one approximation of \(\mathrm{EQKE}\), and adding to that a bound on the difference between the two elements in the same row of the error term.

That is, we can decompose \(E\) into a part parallel to \(d_{q}\) and a part orthogonal to \(d_{q}\), say \(\mathbf{E}_{q}=E_{q}+E_{q}^{\perp}\), and similarly \(\mathbf{\bar{E}}=E_{k}+E_{k}^{\perp}\). Note that \(E_{q}\) and \(E_{k}\) are both rank one, and hence can be multiplied with other matrices of shape \(d_{\mathrm{model}}\times a\) in time \(\mathcal{O}(d_{\mathrm{model}}a)\) rather than time \(\mathcal{O}(d_{\mathrm{vocab}}d_{\mathrm{model}}{}^{a})\).

Hence we can define \(\mathrm{EQKE\_err}_{1}\) (subscript one for "rank one") and decompose \(\mathrm{EQKE}\) as

\[\mathrm{EQKE}=E_{q}QK^{T}(E_{k})^{T}+\mathrm{EQKE\_err}_{1}.\]

Define for any vector \(v\)

\[\Delta_{i,j}v:=v_{i}-v_{j}\]

so that we get

\[\Delta_{i,j}(E_{q}QK^{T}(E_{k})^{T})_{t_{\mathrm{query}}}+\min_{i \neq j}\Delta_{i,j}(\mathrm{EQKE\_err}_{1})_{t_{\mathrm{query}}}\] \[\leq\Delta_{i,j}\mathrm{EQKE}_{t_{\mathrm{query}}}\leq\] \[\Delta_{i,j}(E_{q}QK^{T}(E_{k})^{T})_{t_{\mathrm{query}}}+\max_{i \neq j}\Delta_{i,j}(\mathrm{EQKE\_err}_{1})_{t_{\mathrm{query}}}\]

Then we may use any method we please to pessimize \(\Delta_{i,j}(\mathrm{EQKE\_err}_{1})_{t_{\mathrm{query}}}\) quickly. For example, since for any matrix \(M\) we have \(\sigma_{1}(M)=\sup_{x}\left\|Mx\right\|/\left\|x\right\|\), considering vectors with one \(1\), one

Figure 14: SVD of \(\mathrm{EQKE}\) for seed 123, with principal component vectors scaled by the square root of the corresponding singular value. This scaling allows us to see visually that there is not much going on beyond the first singular component. Numerically: the first singular value is just over \(7440\), while the second singular value is just under \(15\).

\(-1\), and zero elsewhere, the maximum difference between elements in a row upper bounded by \(\sqrt{2}\sigma_{1}(M)\):

\[\left|\Delta_{i,j}\mathrm{EQKE\_err}_{1_{t_{\mathrm{query}}}}\right|\leq\sqrt{2} \sigma_{1}(\mathrm{EQKE\_err}_{1})\] (11)

#### g.2.5 The complicated SVD decomposition of QK

While the "most mechanistic" interpretation would proceed with the analysis in terms of \(E_{q}\) and \(E_{k}\), perhaps decomposing them further, we can get more bang for our buck by extracting out all the low-rank structure available \(E\), \(Q\), and \(K\), so as to make our error bounds as tight as possible.

To this end, we perform SVD on \(E_{q}^{\perp}\), \(E_{k}^{\perp}\), \(Q\), and \(K\) and peel off the first singular components so as to get the decomposition

\[\mathbf{E}_{q} =E_{q}+E_{q,2}+E_{q,2}^{\perp}\] \[\mathbf{\bar{E}} =E_{k}+E_{k,2}+E_{k,2}^{\perp}\] \[Q =Q_{0}+Q^{\perp}\] \[K =K_{0}+K^{\perp}\]

Then \(\mathrm{EQKE}\), a product of these four matrices, can be expressed as a sum of \(2^{2}3^{2}-1=35\) rank one products and one high-rank error term. We can compute the sum of the rank one products in time \(\mathcal{O}(d_{\mathrm{vocab}}{}^{2})\) and express \(\mathrm{EQKE}\) as, say, \(\mathrm{EQKE}_{2}+E_{q,2}^{\perp}Q^{\perp}(E_{k,2}^{\perp}K^{\perp})^{T}\). Call the second term \(\mathrm{EQKE\_err}\) (Figure 15). We must now bound for each \(q\) and \(m\) the quantity \(\max_{i\leq m-G}\mathrm{EQKE\_err}[q,i]-\mathrm{EQKE\_err}[q,m]\).

How big is this?

Even if we relax to \(\max_{i,j}\mathrm{EQKE\_err}[q,i]-\mathrm{EQKE\_err}[q,j]\), the maximum such value across all rows is under \(1.85\) (\(1.99\pm 0.68\) across all seeds). And the rows don't have any particular structure to them; the maximum absolute element of the entire matrix is just barely over \(1\) (\(1.12\pm 0.40\) across all seeds), so doubling that doesn't give too bad an estimate.

But we somehow need to compute this value without multiplying out the four matrices.

One option is to try to use singular value decomposition again. Since \(\sigma_{1}(M)=\sup_{x}\left\|Mx\right\|/\left\|x\right\|\), considering vectors with one \(1\), one \(-1\), and zero elsewhere, the maximum difference between elements in a row upper bounded by \(\sqrt{2}\sigma_{1}(M)\). The largest singular value of \(\mathrm{EQKE\_err}\) (Figure 16) is just under \(7.6\) (\(8.4\pm 2.0\) across all seeds), giving a row-diff bound of about \(10.7\) (\(11.8\pm 2.8\) across all seeds), which is large but not unusably so.

If we perform SVD before multiplying out the matrices (Figure 17), however, their first singular values are about \(4\), \(1.4\), \(1.4\), and \(4\), giving a product of about \(30\), which when multiplied by \(\sqrt{2}\) is about \(43\). (Across all seeds, these numbers are \(3.79\pm 0.12\), \(1.525\pm 0.067\), \(1.513\pm 0.073\), and \(3.78\pm 0.12\), giving a product of about \(33.1\pm 2.9\), which when multiplied by \(\sqrt{2}\) is about \(46.8\pm 4.2\).) This works because \(\sigma_{1}(AB)\leq\sigma_{1}(A)\sigma_{1}(B)\), but note that we can do factored SVD without needing to use this technique. This bound is still usable, but pretty big.

**Can we use Frobenius?** Note that using anything close to this method to drop below \(d_{\mathrm{vocab}}{d_{\mathrm{model}}}^{2}\) might seem infeasible (it'll eventually turn out not to be). For example, the best bound we know on the largest singular value that can be verified even in the worst-case in strictly less time than

Figure 15: The error term \(\mathrm{EQKE\_err}\) for seed 123.

it takes to compute the full SVD is the Frobenius norm, which is defined as \(\text{tr}(MM^{T})\), can be computed in \(d_{\text{model}}d_{\text{vocab}}\) time, and is equal to the square root of the sum of the squares of the singular values. While the Frobenius norm of \(\mathrm{EQKE\_err}\) is only about \(12\) (giving a bound of about \(17\) on the row-diff), the Frobenius norms of the four multiplicand matrices are a bit over \(10\), \(4\), \(4\), and \(10\), giving a product of \(1932\) and a bound of \(2732\)(!). (Across all seeds, the Frobenius norm of \(\mathrm{EQKE\_err}\) is about \(13.1\pm 1.9\) (giving a bound of about \(18.6\pm 2.7\) on the row-diff), the Frobenius norms of the four multiplicand matrices are a bit over \(9.92\pm 0.19\), \(4.43\pm 0.01\), \(4.361\pm 0.095\), and \(9.85\pm 0.19\), giving a product of \(1888\pm 99\) and a bound of \(2670\pm 140\).) This is unusably large.

However, we can get a much better bound on the max row-diff of \(\mathrm{EQKE\_err}\) without having to multiply out all four matrices. We can use an approach vaguely similar to the mean+diff trick, as follows.

If we want to compute the max row-diff of a product of matrices \(AB\), we can compute by Theorem 21

\[\max_{r,i,j}\left((AB)_{r,i}-(AB)_{r,j}\right)\leq\max_{r}\sum_{k}\left|A_{r,k }\right|\max_{i,j}\left(B_{k,i}-B_{k,j}\right)\] (12)

or by combining this approximation with Theorem 18 via Theorem 24 we may compute

\[\max_{r,i,j}\left((AB)_{r,i}-(AB)_{r,j}\right)\] \[\leq\left(\max_{i,j}\sum_{k}\mathbb{E}_{r}A_{r,k}\left(B_{k,i}-B_ {k,j}\right)\right)+\max_{r}\sum_{k}\left|A_{r,k}-\mathbb{E}_{r}A_{r,k}\right| \max_{i,j}\left(B_{k,i}-B_{k,j}\right)\]

taking whichever bound is better.

The first gives us a bound of \(7.94\) on the maximum row-diff, which is better than we can get by doing SVD on the product of the matrices! We can get an even better bound by peeling off the first two singular values of all four matrices before multiplying them; this gives us a bound of \(5.67\). Combining it with the avg+diff trick wouldn't give us much (\(8.05\) and \(5.66\) respectively), as we've effectively already done this by peeling off the leading singular contributions; the mean of \(\mathrm{EQKE\_err}\) over dimension zero has norm \(0.025\) (\(0.030\pm 0.012\) across all seeds).

Although this error bound is no longer the leading asymptotic bottleneck, we can peek ahead to what we get if we want to be linear in parameter count. In this case, we can apply the recursive version of Equation 12 via Theorem 23, giving a bound of \(97.06\) on the maximum row-diff.

Figure 16: SVD of \(\mathrm{EQKE\_err}\) for seed 123.

Figure 17: SVD of the four component matrices of \(\mathrm{EQKE\_err}\) for seed 123. Matrices look like noise.

The mechanistic understanding we get here is roughly "for any given basis vector of the residual stream, the difference between the overlap of any two input tokens with this direction is small once we factor out the first two singular components", and this is sufficient to drive a low error term overall if we factor out the leading singular components in other places. We don't mechanistically understand how to combine the \(\mathbf{E}_{q}QK^{T}\) (without multiplying them out) in a way that allows getting a good bound, though, which corresponds to our inability to drop below \(d_{\mathrm{vocab}}{d_{\mathrm{model}}}^{2}\) here.

If we use this trick on QK only, and use the mean+diff trick on final attention handling (without which we lose about \(19\,\%\)), we can achieve a bound of \(0.7840\) (\(0.661\pm 0.035\) across all seeds).

If we use this trick on the skip connection (\(\mathrm{EU}\)) only, we can achieve a bound of \(0.6768\) (\(0.632\pm 0.061\) across all seed).

Using this trick on both EU and QK drops us down only to \(0.6354\) (\(0.601\pm 0.060\) across all seeds).

If we use this trick on EU and use the recursive version of this trick on QK, we get a bound of \(0.2927\) (\(0.281\pm 0.036\) across all seeds).

Unfortunately, it's not clear how this trick would apply to \(\mathrm{EVOU}\). A fancier convex hull checking algorithm seems required, and an analysis thereof is in progress.

### The algorithm

We now put all of these tricks together into the subcubic algorithm Algorithm 7, which is the full version of Algorithm 6. The format we give here is parameterized over the summarization strategy (from Theorem 19 in Appendix G.1), the decomposition of \(\mathrm{EQKE}\), and the handling of \(\mathrm{EQKE}\_{\mathrm{err}}\) and \(\mathrm{EU}\).

```
1:functionmodel-behavior-relaxed-over-gap(\(\mathcal{M}\), \(t_{\max}\), \(t_{\mathrm{query}}\), \(c\), \(g\), \(g^{*}\))
2:coretness-pessimizing-over-gap-slow is False \(\Longrightarrow\) result is False
3:\(0\leq g^{*}\leq g\leq t_{\max}\)
4:if\(c=0\)then\(t_{\mathrm{query}}=t_{\max}\)
5:\(\textsc{skip-score}_{t^{*}}\leftarrow\textsc{summarize}_{\textsc{EU},t_{ \mathrm{query}}}(\ell^{\textsc{EU}}(t_{\mathrm{query}})_{t^{*}})\)\(\triangleright\)Cache by \(t^{*}\)
6:\(\textsc{skip-score}\leftarrow\max_{t^{*}}\ell^{\textsc{EU}}(t_{\mathrm{query}})_{t^{*}}- \min_{t^{*}}\ell^{\textsc{EU}}(t_{\mathrm{query}})_{t^{*}}\)\(\triangleright\)Cache by \(t_{\mathrm{query}}\)
7:\(v_{t}\leftarrow\textsc{EVOU}(t)\)
8:\(w_{i}\leftarrow\textsc{PVOU}(i)\)
9:\(\Delta w_{\max,t^{*}}\leftarrow\max_{i}w_{i,t^{*}}-w_{i,t_{\max}}\)\(\triangleright\)Cache by \(t_{\max}\), \(t^{*}\)
10:\(\Delta w_{\max,\max}\leftarrow\max_{t^{*}}\Delta w_{\max,t^{*}}\)\(\triangleright\)Cache by \(t_{\max}\)
11:\(\Delta v_{t}\leftarrow\max_{t^{*}}v_{t,t^{*}}-\min_{t^{*}}v_{t,t^{*}}\)\(\triangleright\)Cache by \(t\)
12:\(\Delta v_{\max}\leftarrow\max_{0\leq t\leq t_{\max}-g^{*}}\Delta v_{t}\)\(\triangleright\)Cache by \(t_{\max}-g^{*}\)
13:\(\Delta v_{t^{\max}}^{t_{\max}}\leftarrow v_{t_{\max},t^{*}}-v_{t_{\max},t_{ \max}}\)\(\triangleright\)Cache by \(t_{\max}\)
14:\(\Delta v_{t^{\max}}^{t_{\max}}\leftarrow\max_{t^{*}\neq t_{\max}}\Delta v_{t^{* }}^{t_{\max}}\)\(\triangleright\)Cache by \(t_{\max}\)
15:if\(c=0\)then
16:\(\ell_{t^{*}}\leftarrow\ell^{\textsc{EU}}(t_{\mathrm{max}})_{t^{*}}+v_{t_{\max},t^{* }}+\Delta w_{\max,t^{*}}\)
17:return\(\max_{t^{*}\neq t_{\max}}(\ell_{t^{*}}-\ell_{t_{\max}})\)
18:endif
19:\(b_{:,n_{\mathrm{ctx}}-1}\leftarrow\textsc{EQKP}(t_{\mathrm{query}},n_{ \mathrm{ctx}}-1)\)\(\triangleright\)Cache by \(t_{\mathrm{query}}\)
20:\(b_{0,:-1}\leftarrow\textsc{sort}(\textsc{EQKP}(t_{\mathrm{query}},:-1))\)\(\triangleright\)Cache by \(t_{\mathrm{query}}\), \(i\)
21:\(b_{1,:-1}\leftarrow\textsc{reverse}(b_{0,:-1})\)
22:\(\textsc{EQKE}^{(1)},\textsc{EQKE}_{\textsc{err}}\leftarrow\textsc{decompose}( \textsc{EQKE})\)
23:\(\textsc{EQKE}^{(1)}(t_{\mathrm{query}},t)-\textsc{EQKE}^{(1)}(t_{\mathrm{query }},t_{\max})-\textsc{EQKE}_{\textsc{err}}_{t_{\mathrm{query}}}\leq\textsc{EQKE }(t_{\mathrm{query}},t)-\textsc{EQKE}(t_{\mathrm{query}},t_{\max})\leq \textsc{EQKE}(t_{\mathrm{query}},t_{\max})\)
24:\(a_{t}\leftarrow\textsc{EQKE}^{(1)}(t_{\mathrm{query}},t)\)\(\triangleright\)Cache by \(t_{\mathrm{query}}\), \(t\)
25:\(a_{\mathrm{min},t}\leftarrow\min_{0\leq t^{\prime\prime}\leq t}a_{t^{\prime \prime}}\)\(\triangleright\)Cache by \(t_{\mathrm{query}}\), \(t\), compute in amortized \(\mathcal{O}({d_{\mathrm{vocab}}}^{2})\)
26:\(a_{\mathrm{max},t}\leftarrow\max_{0\leq t^{\prime\prime}\leq t}a_{t^{\prime \prime}}\)\(\triangleright\)Cache by \(t_{\mathrm{query}}\), \(t\), compute in amortized \(\mathcal{O}({d_{\mathrm{vocab}}}^{2})\)
27:\(\Delta a_{\mathrm{max}}\gets a_{a_{\mathrm{max}}}-a_{\mathrm{min},a_{ \mathrm{max}}-g}+\textsc{EQKE}_{\textsc{err}}_{t_{\mathrm{query}}}\)\(\triangleright\)Cache by \(t_{\mathrm{query}}\), \(t_{\max}\), \(c\)
28:\(\Delta a_{\mathrm{min}}\gets a_{t_{\mathrm{max}}}-a_{\mathrm{max},t_{ \mathrm{max}}-g}-\textsc{EQKE}_{\textsc{err}}_{t_{\mathrm{query}}}\)\(\triangleright\)Cache by \(t_{\mathrm{query}}\), \(t_{\max}\), \(c\)
29:idx-set \(\leftarrow\{0,\ldots,n_{\mathrm{ctx}}-c-1\}\)if\(t_{\mathrm{max}}\neq t_{\mathrm{query}}\)else\(\{0,\ldots,n_{\mathrm{ctx}}-c-2,n_{\mathrm{ctx}}-1\}\)
30:\(\textsc{attn-weights-unscaled}_{0,i}\gets b_{0,i}+(\Delta a_{\mathrm{min}}\)if\(i\in\mathrm{idx-set}\)else\(0)\)
31:\(\textsc{attn-weights-unscaled}_{1,i}\gets b_{1,i}+(\Delta a_{\mathrm{max}}\)if\(i\in\mathrm{idx-set}\)else\(0)\)\(\triangleright\)Cache by \(t_{\mathrm{query}}\), \(t_{\max}\), \(i\), \(c\)
32:\(\textsc{attn-weights}_{0}\leftarrow\textsc{softmax}(\textsc{attn-weights-unscaled}_{0}/\sqrt{d})\)\(\triangleright\)Cache by \(t_{\mathrm{query}}\), \(t_{\max}\), \(i\), \(c\)
33:\(\textsc{attn-weights}_{1}\leftarrow\textsc{softmax}(\textsc{attn-weights-unscaled}_{1}/\sqrt{d})\)\(\triangleright\)Cache by \(t_{\mathrm{query}}\), \(t_{\max}\), \(i\), \(c\)
34:\(\textsc{attn-max}_{0}\leftarrow\sum_{i\in\mathrm{idx-set}}\textsc{attn-weights}_{0,i}\)
35:\(\textsc{attn-max}_{1}\leftarrow\sum_{i\in\mathrm{idx-set}}\textsc{attn-weights}_{1,i}\)
36:\(\textsc{attn-max}\leftarrow\textsc{attn-max}_{0}\)if\(\Delta v_{\max}^{t_{\max}}\geq\Delta v_{\max}\)else\(\textsc{attn-max}_{1}\)
37:\(\textsc{attn-max}\leftarrow\textsc{summarize}_{\textsc{attn},t_{ \mathrm{query}}}(\textsc{attn-max})\)\(\triangleright\)Cache by \(t_{\max}\), \(c\)
38:\(\textsc{attn-max}^{\prime}\leftarrow\textsc{attn-max}-\textsc{attn-max}\)\(\triangleright\)Cache by \(t_{\max}\), \(c\)
39:\(\textsc{summary}_{t^{*}}\leftarrow\Delta w_{\max,t^{*}}+\textsc{skip-score}_{t^{*}}+ \textsc{attn-max}\Delta v_{t^{*}}^{t_{\max}}+(1-\overline{\textsc{attn-max}}) \Delta v_{\max}\)\(\triangleright\)Cache by \(t_{\max}\), \(t^{*}\)
40:return\(\textsc{skip-score}+\textsc{attn-max}^{\prime}\cdot\Delta v_{\max}^{t_{ \max}}+(-\textsc{attn-max}^{\prime})\Delta v_{\max}+\max_{t^{*}\neq t_{\max}}\textsc{summary}_{t^{*}}\)
41:endfunction ```

**Algorithm 7** Counting Correct Sequences in Subcubic TimeComparison of proof strategies

In this section, we compare the various proof strategies that we have developed in Appendix G. We do some traditional mechanistic interpretability analysis to justify that the choices that we made could be expected to lead to reasonably good bounds in Appendix H.1. We then compare the complexities and performance of various proof strategies in Appendix H.2 to line up with the legends of Figures 3, and 4. We close with a figure relating the various categories of proof strategies.

### Justification of pessimization choices

In Sections 4.3, F, and G we make a number of choices about which axes of variation are more or less important to track at various points in the bound computation.

Here we do some more traditional mechanistic interpretability analysis to justify that the choices that we made could be expected to lead to reasonably good bounds.

#### h.1.1 Justifying the gap

We take advantage of the fact that attention is mostly monotonically increasing in input integers and that for most sequences, the attentional contribution of the particular query token matters much more than the particular non-max token in the sequence.

We justify this as follows.

We can look at the typical diff, when attending to the max token, between the largest non-max logit and the max logit. As shown in Figure 17(a), the largest difference between an off-diagonal entry of \(\mathrm{EVOU}\) and the diagonal of that row is typically at most \(-7\).24 The typical worst contribution to the wrong logit from a non-max token (this is typical over non-max tokens, worst over choice of output token-logit index) is around \(43\), as shown in Figure 17(b).

Footnote 24: “Typically” here means about \(96\,\%\) of the time.

The difference in attention between tokens is approximately linear in the gap between the tokens, as seen in Figure 19. The slope of the line, that is, the difference in pre-softmax attention scores divided by the gap between the key token and the max token, is approximately \(1.2\).

Exponentiating, the post-softmax attention paid to the max is typically about \(3\times\) larger than to the token one below the max; here the logit difference between the max and non-max token is significant, typically being around \(13\) (\(43/3\)) for the worst output logit. But by the time the gap is 3, this difference has dropped to about \(1.1\), and by the time the gap is 4 it is around \(0.3\).

Figure 18: Plots of the difference in logit for the attention computation, \(\mathrm{EVOU}:=\bar{\mathbf{E}}VOU\) for seed 123.

So for sequences where the largest non-max and the max are close together, the particular structure of the non-max \(\mathrm{EVOU}\) matters a lot; but when the max is separated from the largest non-max by a modest gap, the structure of the non-max \(\mathrm{EVOU}\) does not matter so much.

The upshot is that to handle most sequences, we need only ask an oracle for the minimum gap \(g>0\) between the max token \(t_{\mathrm{max}}\) and largest non-max tokens \(t^{\prime}\neq t_{\mathrm{max}}\), such that the model outputs the correct answer for all sequences where the non-max, non-query tokens have value at most \(t_{\mathrm{max}}-g\).

While computing this gap may be expensive (and indeed the naive computation of the oracle takes longer than the brute-force proof--though it should be very easy to optimize), we don't have to pay the cost of computing the gap in the size of the proof, only the cost of storing the gap table (\(\mathcal{O}(d_{\mathrm{vocab}}{}^{2}n_{\mathrm{ctx}})\)) and of verifying the gap. Empirically, gaps are typically 1-5, as seen in Figure 20.

If we rely on the gaps, this results in leaving behind about \(6.9\,\%\) of sequences.

**Picking up more sequences** In this paragraph / bulleted list, we sketch out how we might go about picking up more sequences to get a tighter bound. This is not coded up, and is left as future work. We propose computing the following quantities:

* First, we could build in time (\(\mathcal{O}(d_{\mathrm{vocab}}{}^{2})\)) a table indexed on pairs \((t,t_{\mathrm{max}})\) of the maximum token and a non-maximum token: the table would store pressimal logit contributions from \(t\) to maximum output tokens \(\leq\) the \(t_{\mathrm{max}}\) parameter. The table could be further split to pessimize separately for tokens within and outside of the gap window.
* Compute a table of pre-softmax attention differences between tokens \(t\) and \(t+1\) in time (\(\mathcal{O}(d_{\mathrm{vocab}}{}^{2})\)).
* Next sort the queries by overlap with the query direction.

Figure 19: Plots of attention difference vs. token gap, for \(\mathrm{EQKE}:=\mathbf{E}_{q}QK^{\mathcal{T}}\mathbf{\bar{E}}^{T}\) for seed 123. The difference in attention between tokens is approximately linear in the gap between the tokens.

Figure 20: Histogram of the minimum gap between the max token and the largest non-max token, for the seed 123.

* Compute for each number of queries handled (where we assume we handle all queries with greater overlap than the current one) and for each maximum input token \(t_{\max}\), how many of the query tokens \(t_{\mathrm{query}}\) fall strictly below the max \(t_{\max}\) (and whether or not the model succeeds when \(t_{\max}=t_{\mathrm{query}}\)). This will tell us how many query tokens we can count for a given maximum token.
* Compute a table indexed on pairs of # of queries handled and input tokens \(t\) which stores the smallest difference in more attention paid to \(t+1\) than to \(t\) (\(\mathcal{O}(d_{\mathrm{vocab}}{}^{2})\)).
* Compute a table indexed on pairs \(t_{\max}\), \(t\) storing an upper bound on amount more attention paid to non-maximum tokens than to \(t_{\max}\) by Oracle-permitted query tokens (the Oracle is indexed only on \(t_{\max}\) (\(\mathcal{O}(d_{\mathrm{vocab}}{}^{2})\)).
* For each # queries permitted: compute for each \(t_{\max}\), \(t\), \(c\), if the non-maximum token \(t\) contributes little enough to incorrect logits that even with the worst skip connection the model still gets the correct answer.

#### h.1.2 Stopping after 1-2 principal components of QK

Did we miss out on any structure in the error term of \(\mathrm{EQKE}\)? The distribution of entries of the four matrices looks pretty close to normal as seen in Figure 21.

Figure 21: The distribution of entries of the four residual matrices (after removing two principal components from \(\mathbf{E}_{\delta}\) and \(\bar{\mathbf{E}}\) and one principal component from \(Q\) and \(K\)). Distributions look pretty close to normal. Plots are for the seed 123.

If we replace the entries of \(E_{q,2}^{\perp}\), \(E_{k,2}^{\perp}\), \(Q^{\perp}\), and \(K^{\perp}\) with randomly sampled values, we get (sample size 100) that the maximum row-diff of the product of the matrices is approximately \(1.31\pm 0.13\) (sampling without replacement from the empirical distribution) or \(1.31\pm 0.14\) (sampling from the normal distribution). So in fact our max row-diff is unusually high (by about \(4\sigma\)).25

Footnote 25: This shows up in the bias towards having larger values (both positive and negative) in the lower-right corner of the plot, indicating that errors are larger for larger query and key values. We hypothesize that this is due to the distribution of data: larger values are more likely to have more space between the maximum and next-most-maximum token, so a bit of noise matters less for larger masses than for smaller ones.

### How various combinations of tricks perform

Recall Figures 3 and 4 on page 8 and on page 9 from Section 5, recapitulated here without captions for convenience as Figures 22, and 23.

We describe what each subcubic proof strategy in the legend means. Note that all subcubic proof strategies (that is, all proof strategies except for "brute force" and "cubic") use the quadratic counting algorithm of Appendix F.

#### h.2.1 Proof strategies grouped by complexity

In Figures 3, and 22, proof strategies are grouped by computational complexity.

The 102 proof strategies break down into \(1+1+2\times 5\times 10\times 2\) strategies.

The **brute force** and **cubic** proofs (\(1+1\)) were fully covered in Appendices D, and E.

There are 5 options for handling \(\mathrm{EU}\):

**direct-quadratic** refers to handling \(\mathrm{EU}\) in time \(\mathcal{O}(d_{\mathrm{vocab}}d_{\mathrm{model}})\) with either the max row-diff trick (Appendix G.2.2)26 or the max row-diff trick fused with mean+diff or some other summary statistic (Theorem 24)27.

Footnote 26: This strategy is labeled “max_diff” in the Python source code.

When **direct** is not mentioned, this indicates that we handle \(\mathrm{EU}\) in time \(\mathcal{O}(d_{\mathrm{vocab}}{}^{2}d_{\mathrm{model}})\) by first multiplying out \(\mathbf{E}_{q}U\) and then either taking the maximum row-diff in each row28 or by taking the maximum row-diff across all rows29. The latter is included purely for comparison's sake, and never gives a tighter bound than the former.

Footnote 28: “max_diff_exact”

Footnote 29: “global_max_diff_exact”

There are 10 options for handling the high-rank attention error term \(\mathrm{EQKE\_err}\):

Figure 22: Recreations of Figure 3 for ease of viewing of the legend. Top is a strict recreation; bottom includes points not on the Pareto frontier.

attention-quadratic** refers to handling the high-rank attention error term \(\mathrm{EQKE\_err}\) from Appendix G.2.5 in time \(\mathcal{O}(d_{\mathrm{vocab}}d_{\mathrm{model}})\) either with the recursive max row-diff trick (Theorem 23)30 or with the recursive max row-diff trick fused with the mean+diff trick either just on the query side31 or throughout32 (Theorem 26).

Footnote 30: “max_diff_subproduct_recursive”

Footnote 31: “mean=max_diff_subproduct_recursive”

Footnote 32: “mean_recursive+max_diff_subproduct_recursive”

Footnote 33: “std”

**attention-\(d_{\mathrm{vocab}}d_{\mathrm{model}}\)2** indicates that we use one of the various \(\mathcal{O}(d_{\mathrm{vocab}}d_{\mathrm{model}}\)2) strategies for handling \(\mathrm{EQKE\_err}_{1}\) from Appendix G.2.4 or \(\mathrm{EQKE\_err}\) from Appendix G.2.5. These include using \(\sqrt{2}\sigma_{1}\)--computed via low-rank SVD--as the bound (Equation 11)33, considering all ways of multiplying out a subset of the matrices and taking the maximum row-diff of the resulting pair of matrices34 (Theorem 21), or fusing the max row-diff trick with the mean+diff trick35 (Theorem 24).

Footnote 34: “max_diff” for \(\mathrm{EQKE\_err}_{1}\), “mean+max_diff_subproduct” for \(\mathrm{EQKE\_err}\)

Footnote 36: “max_diff_exact”

Footnote 37: “exact_EQKE+max_diff_exact”

Footnote 38: “mean_query+diff”

Footnote 39: “drop_average_query_per_output_logit_reasoning”

When **attention is not mentioned**, this indicates that we handle the attention error term in time \(\mathcal{O}(d_{\mathrm{vocab}}d_{\mathrm{model}})\), either by taking the per-row maximum row-diff36 or by using the full rank \(\mathrm{EQKE}\) matrix and taking the per-row maximum row diff37.

Footnote 37: “exact_EQKE+max_diff_exact”

Finally, note that in combining the rank one attention computation with \(\mathrm{EVOU}\), \(\mathrm{PVOU}\), and \(\mathrm{EU}\), we may either use the mean+diff trick38 (Appendix G.1) or not39; this makes up the final factor of \(2\).

Footnote 38: “mean_query+diff”

#### h.2.2 Proof strategies grouped by attention handling

This section slightly reorganizes the information just covered in Appendix H.2.1, for convenience of legend correspondence. Here we group by the strategy used to handle the attention error term. Strategies that involve using the full rank \(\mathrm{EQKE}\) matrix are elided. The dashed descriptors here correspond to underscore-joined descriptors in footnotes of Appendix H.2.1.

**max-diff-exact** (\(\mathcal{O}(d_{\mathrm{vocab}}d_{\mathrm{model}})\)) corresponds to taking the full rank \(\mathrm{EQKE\_err}_{1}\) term and taking the maximum row-diff in each row.

**mean+max-diff-subproduct** (\(\mathcal{O}(d_{\mathrm{vocab}}d_{\mathrm{model}})\)) corresponds to fusing the max row-diff trick with the mean+diff trick (Theorem 24) and considering all ways of associating the multiplication of \(\mathrm{EQKE\_err}\).

**max-diff-subproduct** (\(\mathcal{O}(d_{\mathrm{vocab}}d_{\mathrm{model}})\)) corresponds to using the max row-diff trick (Theorem 21) and considering all ways of associating the multiplication of \(\mathrm{EQKE\_err}\).

**max-diff** (\(\mathcal{O}(d_{\mathrm{vocab}}d_{\mathrm{model}}\)2)) corresponds to using the max row-diff trick (Theorem 21) on the factored SVD of \(\mathrm{EQKE\_err}_{1}\).

**mean+max-diff** (\(\mathcal{O}(d_{\mathrm{vocab}}d_{\mathrm{model}}\)2)) corresponds to fusing the max row-diff trick with the mean+diff trick (Theorem 24) and applying it on the factored SVD of \(\mathrm{EQKE\_err}_{1}\).

Figure 23: Recreation of Figure 4 for ease of viewing of the legend.

**svd** (\(\mathcal{O}(d_{\mathrm{vocab}}d_{\mathrm{model}}{}^{2})\)) corresponds to using \(\sqrt{2}\sigma_{1}\)--computed via low-rank SVD--as the bound (Equation 11).

**mean+max-diff-subproduct-recursive** (\(\mathcal{O}(d_{\mathrm{vocab}}d_{\mathrm{model}})\)) corresponds to handling the high-rank attention error term \(\mathrm{EQKE\_err}\) from Appendix G.2.5 with the recursive max row-diff trick fused with the mean+diff trick on the query-side only (Theorem 26, taking all but the first summary vector to be zero).

**max-diff-subproduct-recursive** (\(\mathcal{O}(d_{\mathrm{vocab}}d_{\mathrm{model}})\)) corresponds to handling the high-rank attention error term \(\mathrm{EQKE\_err}\) from Appendix G.2.5 with the recursive max row-diff trick (Theorem 23).

**mean-recursive+max-diff-subproduct-recursive** (\(\mathcal{O}(d_{\mathrm{vocab}}d_{\mathrm{model}})\)) corresponds to handling the high-rank attention error term \(\mathrm{EQKE\_err}\) from Appendix G.2.5 with the recursive max row-diff trick recursively fused with the mean+diff trick (Theorem 26).

#### h.2.3 What understanding do we get from each proof strategy?

Throughout most of this paper, we talk about doing mechanistic interpretability and using understanding to allow more compact proofs to have tighter bounds. We can also look at the reverse problem: we can take a collection of proof strategies, check by brute force which strategies give the tightest bounds for each model, and ask what this implies about how that model works. We do this here.

In general, which proof methods perform best is an indication of where structure exists in the model. For example, in quadratic \(\mathrm{EU}\) proofs, when max_diff performs worse than mean_query+max_diff and svd_query+max_diff, this indicates that \(E\) has a relatively strong behavioral component shared across query tokens that \(U\) is not that good at filtering out. Similarly, when, e.g., mean_recursive+max_diff_subproduct_recursive performs better than max_diff_subproduct_recursive, this indicates that even after removing the first one or two principle components from \(\mathbf{E}_{q}\), \(Q\), \(K\), and \(\mathbf{E}\), there is still enough common structure that it is worth factoring out the mean behavior.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We present the key challenges in the field of formal verification for neural networks, and describe our proposed solution. Then we summarize our experimental setup and results.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Section 7.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We provide intuitions for our proof constructions in the main body of the paper. In the supplemental material, we lay out in full detail theorem statements and proofs. Along with this, we provide algorithms and plots that are useful for understanding how the proofs were constructed.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes]
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We directly link to a codebase with the specific implementation of our case study.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: In supplementary materials, we provide the full details of our model training set up. In the main body of the paper, we describe our experimental setting and provide reasoning for why we chose this experimental setup as a very simple case study of our theoretical work.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We run our experiment on models trained with 151 different hyperparameters. For most reported computations, we provide statistical significance information. We do not need to perform explicit t-tests or such since it is not relevant to our setup.
8. **Experiments Compute Resources**Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Appendix B.1.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We confirm that we have read the Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This paper seeks to advance the fields of mechanistic interpretability and formal verification of machine learning systems. While there are many indirect societal consequences of our work through the impacts on these fields, we feel that none are sufficiently consequential as to be highlighted here.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper does not involve any such assets. Guidelines:
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We list all important Python packages used in the paper in Appendix B.1.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We introduce no new assets except for the codebase needed to reproduce our experiments, which does contain appropriate documentation.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects**

Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

Answer: [NA]

Justification: This paper does not involve crowdsourcing nor research with human subjects.