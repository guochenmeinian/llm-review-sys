RAMP: Boosting Adversarial Robustness Against Multiple \(l_{p}\) Perturbations for Universal Robustness

 Enyi Jiang

Department of Computer Science

University of Illinois Urbana-Champaign

Urbana, IL 61801

enyij20@illinois.edu

&Gagandeep Singh

Department of Computer Science

University of Illinois Urbana-Champaign

Urbana, IL 61801

ggnds@illinois.edu

###### Abstract

Most existing works focus on improving robustness against adversarial attacks bounded by a single \(l_{p}\) norm using adversarial training (AT). However, these AT models' multiple-norm robustness (union accuracy) is still low, which is crucial since in the real-world an adversary is not necessarily bounded by a single norm. The tradeoffs among robustness against multiple \(l_{p}\) perturbations and accuracy/robustness make obtaining good union and clean accuracy challenging. We design a logit pairing loss to improve the union accuracy by analyzing the tradeoffs from the lens of distribution shifts. We connect natural training (NT) with AT via gradient projection, to incorporate useful information from NT into AT, where we empirically and theoretically show it moderates the accuracy/robustness tradeoff. We propose a novel training framework **RAMP**, to boost the robustness against multiple \(l_{p}\) perturbations. **RAMP** can be easily adapted for robust fine-tuning and full AT. For robust fine-tuning, **RAMP** obtains a union accuracy up to \(53.3\%\) on CIFAR-10, and \(29.1\%\) on ImageNet. For training from scratch, **RAMP** achieves a union accuracy of \(44.6\%\) and good clean accuracy of \(81.2\%\) on ResNet-18 against AutoAttack on CIFAR-10. Beyond multi-norm robustness **RAMP**-trained models achieve superior _universal robustness_, effectively generalizing against a range of unseen adversaries and natural corruptions.

## 1 Introduction

Though deep neural networks (DNNs) demonstrate superior performance in various vision applications, they are vulnerable against adversarial examples (Goodfellow et al., 2014; Kurakin et al., 2018). Adversarial training (AT) (Tramer et al., 2017; Madry et al., 2017) which works by injecting adversarial examples into training for enhanced robustness, is currently the most popular defense. However, most AT methods address only a _single_ type of perturbation (Wang et al., 2020; Wu et al., 2020; Carmon et al., 2019; Gowal et al., 2020; Raghunathan et al., 2020; Zhang et al., 2021; Debenedetti and Troncoso--EPFL, 2022; Peng et al., 2023; Wang et al., 2023). An \(l_{\infty}\) robust model may not be robust against \(l_{p}(p\neq\infty)\) attacks. Also, enhancing robustness against one perturbation type can sometimes increase vulnerability to others (Engstrom et al., 2017; Schott et al., 2018). On the contrary, training a model to be robust against multiple \(l_{p}\) perturbations is crucial as it reflects real-world scenarios (Sharif et al., 2016; Eykholt et al., 2018; Song et al., 2018; Athalye et al., 2018) where adversaries can use multiple \(l_{p}\) perturbations. We show that multi-norm robustness is the key to improving generalization against other threat models (Croce and Hein, 2022). For instance, we show it enables robustness against perturbations not easily defined mathematically, such as image corruptions and unseen adversaries (Wong and Kolter, 2020).

Two main challenges exist for training models robust against multiple perturbations: (i) tradeoff among robustness against different perturbation models (Tramer and Boneh, 2019) and (ii) tradeoff between accuracy and robustness (Zhang et al., 2019; Raghunathan et al., 2020). Adversarial examples induce a shift from the original distribution, causing a drop in clean accuracy with AT (Xie et al., 2020; Benz et al., 2021). The distinct distributions created by \(l_{1},l_{2},l_{\infty}\) adversarial examples make the problem even more challenging. Through a finer analysis of the distribution shifts caused by these adversaries, we propose the **RAMP** framework to efficiently boost the **R**obustness **A**gainst **M**ultiple **P**erturbations. **RAMP** can be used for both fine-tuning and training from scratch. It utilizes a novel logit pairing loss on a certain pair and connects NT with AT via gradient projection (Jiang et al., 2023) to improve union accuracy while maintaining good clean accuracy and training efficiency.

**Logit pairing loss.** We visualize the changing of \(l_{1},l_{2},l_{\infty}\) robustness when fine-tuning a \(l_{\infty}\)-AT pre-trained model in Figure 1 using the CIFAR-10 training dataset. The DNN loses substantial robustness against \(l_{\infty}\) attack after only \(1\) epoch of fine-tuning: \(l_{1}\) fine-tuning and E-AT (Crocce and Hein, 2022) (red and yellow histograms under Linf category) both lose significant \(l_{\infty}\) robustness (compared with blue histogram under Linf category). Inspired by this observation, we devise a new logit pairing loss for a \(l_{q}-l_{r}\) tradeoff pair to attain better union accuracy, which enforces the logit distributions of \(l_{q}\) and \(l_{r}\) adversarial examples to be close, specifically on the correctly classified \(l_{q}\) subsets. In comparison, our method (green histogram under Linf and union categories) preserves more \(l_{\infty}\) and union robustness than others after \(1\) epoch. We show this technique works on larger models and datasets (Section 5.1).

**Connect natural training (NT) with AT.** We explore the connections between NT and AT to obtain a better accuracy/robustness trade-off. We find that NT can help with adversarial robustness: useful information in natural distribution can be extracted and leveraged to achieve better robustness. To this end, we compare the similarities of model updates of NT and AT _layer-wise_ for each epoch, where we find and incorporate useful NT components into AT via gradient projection (GP), as outlined in Algorithm 2. In Figure 2 and Section 5.1, we empirically and theoretically show this technique strikes a better balance between accuracy and robustness, for both single and multiple \(l_{p}\) perturbations. We provide a theoretical analysis of why GP works for adversarial robustness in Theorem A.2 & 4.5.

**Main contributions**:

* We design a new logit pairing loss to mitigate the \(l_{q}-l_{r}\) tradeoff for better union accuracy, by enforcing the logit distributions of \(l_{q}\) and \(l_{r}\) adversarial examples to be close.
* We empirically and theoretically show that connecting NT with AT via gradient projection better balances the accuracy/robustness tradeoff for \(l_{p}\) perturbations, compared with standard AT.
* **RAMP** achieves good union accuracy, accuracy-robustness tradeoff, and generalizes better to diverse perturbations and corruptions (Section 5.1) achieving superior _universal robustness_ (\(75.5\%\) for common corruption and \(26.1\%\) union accuracy against unseen adversaries). **RAMP** fine-tuned DNNs achieve union accuracy up to \(53.3\%\) on CIFAR-10, and \(29.1\%\) on ImageNet. **RAMP** achieves a \(44.6\%\) union accuracy and good clean accuracy on ResNet-18 against AutoAttack on CIFAR-10. Our code is available at https://github.com/uiuc-focal-lab/RAMP.

## 2 Related Work

**Adversarial training (AT).** Adversarial Training (AT) usually employs gradient descent to discover adversarial examples, incorporating them into training for enhanced adversarial robustness (Tramer et al., 2017; Madry et al., 2017). Numerous works focus on improving robustness by exploring the trade-off between robustness and accuracy (Zhang et al., 2019; Wang et al., 2020), instance reweighting (Zhang et al., 2021), loss landscapes (Wu et al., 2020), wider/larger architectures (Gowal

Figure 1: **Multiple-norm tradeoff with robust fine-tuning**: We observe that fine-tuning on \(l_{\infty}\)-AT model using \(l_{1}\) examples drastically reduces \(l_{\infty}\) robustness. **RAMP** preserves more \(l_{\infty}\) and union robustness.

et al., 2020; Debenedetti and Troncoso--EPFL, 2022), data augmentation (Carmon et al., 2019; Raghunathan et al., 2020), and using synthetic data (Peng et al., 2023; Wang et al., 2023). However, these methods often yield DNNs robust against a _single_ perturbation type while remaining vulnerable to other types.

**Robustness against multiple perturbations.**Tramer and Boneh (2019); Kang et al. (2019) observe that robustness against \(l_{p}\) attacks does not necessarily transfer to other \(l_{q}\) attacks (\(q\neq p\)). Previous studies (Tramer and Boneh, 2019; Maini et al., 2020; Madaan et al., 2021; Croce and Hein, 2022) modified Adversarial Training (AT) to enhance robustness against multiple \(l_{p}\) attacks, employing average-case (Tramer and Boneh, 2019), worst-case (Tramer and Boneh, 2019; Maini et al., 2020), and random-sampled (Madaan et al., 2021; Croce and Hein, 2022) defenses. There are also works (Nandy et al., 2020; Liu et al., 2020; Xu et al., 2021; Xiao et al., 2022; Maini et al., 2022) using preprocessing, ensemble methods, mixture of experts, and stability analysis to solve this problem. Ensemble models and preprocessing methods are weakened since their performance heavily relies on correctly classifying or detecting various types of adversarial examples. In certified training, Banerjee et al. (2024); Banerjee and Singh (2024) propose verification/cerifiable training methods under different threat models for \(l_{p}\) universal adversarial perturbation. However, prior works are hard to scale to larger models and datasets, e.g. ImageNet, due to the efficiency issue. Furthermore, Croce and Hein (2022) devise Extreme norm Adversarial Training (E-AT) and fine-tune a \(l_{p}\) robust model on another \(l_{q}\) perturbation to quickly make a DNN robust against multiple \(l_{p}\) attacks. However, E-AT does not adapt to varying epsilon values. Our work demonstrates that the suboptimal tradeoff observed in prior studies can be improved with our proposed framework.

**Logit pairing in adversarial training.** Adversarial logit pairing methods encourage logits for pairs of examples to be similar (Kannan et al., 2018; Engstrom et al., 2018). People apply this technique to both clean images and their adversarial counterparts, to devise a stronger form of adversarial training. In our work, we devise a novel logit pairing loss to train a DNN originally robust against \(l_{p}\) attack to become robust against another \(l_{q}(q\neq p)\) attack on the correctly predicted \(l_{p}\) subsets, which helps gain better union accuracy.

**Adversarial versus distributional robustness.**Sinha et al. (2018) theoretically studies the AT problem through distributional robust optimization. Mehrabi et al. (2021) establishes a pareto-optimal tradeoff between standard and adversarial risks by perturbing the test distribution. Other works explore the connection between natural and adversarial distribution shifts (Moayeri et al., 2022; Alhamoud et al., 2023), assessing transferability and generalizability of adversarial robustness across datasets. However, little research delves into distribution shifts induced by \(l_{1},l_{2},l_{\infty}\) adversarial examples and their interplay with the robustness-accuracy tradeoff (Zhang et al., 2019; Yang et al., 2020; Rade and Moosavi-Dezfooli, 2021). Our work, inspired by recent domain adaptation techniques (Jiang, 2023; Jiang et al., 2023), designs a logit pairing loss and utilizes model updates from NT via GP to enhance adversarial robustness. We show that GP adapts to both single and multi-norm scenarios.

## 3 AT against Multiple Perturbations

We consider a standard classification task with samples \(\{(x_{i},y_{i})\}_{i=0}^{N}\) from an empirical data distribution \(\widehat{\mathcal{D}}_{n}\); we have input images \(x\in\mathbb{R}^{d}\) and corresponding labels \(y\in\mathbb{R}^{k}\). Standard training aims to obtain a classifier \(f\) parameterized by \(\theta\) to minimize a loss function \(\mathcal{L}:\mathbb{R}^{k}\times\mathbb{R}^{k}\rightarrow\mathbb{R}\) on \(\widehat{\mathcal{D}}_{n}\). Adversarial training (AT) (Madry et al., 2017; Tramer et al., 2017) aims to find a DNN robust against adversarial examples. It is framed as a min-max problem where a DNN is optimized using the worst-case examples within an adversarial region around each \(x_{i}\). Different types of adversarial regions \(B_{p}(x,\epsilon_{p})=\{x^{\prime}\in\mathbb{R}^{d}:\|x^{\prime}-x\|_{p}\leq \epsilon_{p}\}\) can be defined around a given image \(x\) using various \(l_{p}\)-based perturbations. Formally, we can write the optimization problem of AT against a certain \(l_{p}\) attack as follows:

\[\min_{\theta}\mathbb{E}_{(x,y)\sim\widehat{\mathcal{D}}_{n}}\left[\max_{x^{ \prime}\in B_{p}(x,\epsilon_{p})}\mathcal{L}(f(x^{\prime}),y)\right]\]

The above optimization is only for certain \(p\) values and is usually vulnerable to other perturbation types. To this end, prior works have proposed several approaches to train the network robust against multiple perturbations (\(l_{1},l_{2},l_{\infty}\)) at the same time. We focus on the union threat model \(\Delta=B_{1}(x,\epsilon_{1})\cup B_{2}(x,\epsilon_{2})\cup B_{\infty}(x,\epsilon_{ \infty})\) which requires the DNN to be robust within the \(l_{1},l_{2},l_{\infty}\) adversarial regions simultaneously (Croce and Hein, 2022). Union accuracy is then defined as the robustness against \(\Delta_{(i)}\) for each \(x_{i}\) sampled from \(\mathcal{D}\). In this paper, similar to the prior works, we use union accuracy as the main metric to evaluate the multiple-norm robustness. Apart from that, we define _universal robustness_ as the generalization ability against a range of unseen adversaries and common corruptions. Specifically, we have average accuracy across five severity levels for common corruption and union accuracy against a range of unseen adversaries used in Laidlaw et al. (2020).

**Worst-case defense** follows the following min-max optimization problem to train DNNs using the worst-case example from the \(l_{1},l_{2},l_{\infty}\) adversarial regions:

\[\min_{\theta}\mathbb{E}_{(x,y)\sim\widehat{\mathcal{D}}_{n}}\left[\max_{p\in \{1,2,\infty\}}\max_{x^{\prime}\in B_{p}(x,\epsilon_{p})}\mathcal{L}(f(x^{ \prime}),y)\right]\]

MAX (Tramer and Boneh, 2019) and MSD (Maini et al., 2020) fall into this category. Finding worst-case examples yields a good union accuracy but results in a loss of clean accuracy as the distribution of generated examples is different from the clean data distribution.

**Average-case defense** train DNNs using the average of the \(l_{1},l_{2},l_{\infty}\) worst-case examples:

\[\min_{\theta}\mathbb{E}_{(x,y)\sim\widehat{\mathcal{D}}_{n}}\left[\mathbb{E}_ {p\in\{1,2,\infty\}}\max_{x^{\prime}\in B_{p}(x,\epsilon_{p})}\mathcal{L}(f(x^ {\prime}),y)\right]\]

AVG (Tramer and Boneh, 2019) is of this type. This method generally leads to good clean accuracy but suboptimal union accuracy as it does not penalize worst-case behavior within the \(l_{1},l_{2},l_{\infty}\) regions.

**Random-sampled defense.** The defenses mentioned above lead to a high training cost as they compute multiple attacks for each sample. SAT (Madaan et al., 2021) and E-AT (Croce and Hein, 2022) randomly sample one attack out of each type at a time, contributing to a similar computational cost as standard AT on a single perturbation model. They achieve a slightly better union accuracy compared with AVG and relatively good clean accuracy. However, they are not better than worst-case defenses for multiple-norm robustness, since they do not consider the strongest attack within the union region all the time.

## 4 Ramp

There are two main tradeoffs in achieving better union accuracy while maintaining good accuracy: 1. Among perturbations: there is a tradeoff among different attacks, e.g., a \(l_{\infty}\) pre-trained AT DNN is not robust against \(l_{1},l_{2}\) perturbations, which makes the union accuracy harder to attain. Also, we observe there exists a main tradeoff pair of two attacks among the union over \(l_{1}\), \(l_{2}\), \(l_{\infty}\) attacks. 2. Accuracy and robustness: all defenses lead to degraded clean accuracy. To address these tradeoffs, we study the problem from the lens of distribution shifts.

**Interpreting tradeoffs from the lens of distribution shifts.** The adversarial examples with respect to an empirical data distribution \(\widehat{\mathcal{D}}_{n}\), adversarial region \(B_{p}(x,\epsilon_{p})\), and DNN \(f_{\theta}\) generate a new adversarial distribution \(\widehat{\mathcal{D}}_{a}\) with samples \(\{(x^{\prime}_{i},y_{i})\}_{i=0}^{N}\), that are correlated by adding certain perturbations but different from the original \(\widehat{\mathcal{D}}_{n}\). Because of the shifts between \(\widehat{\mathcal{D}}_{n}\) and \(\widehat{\mathcal{D}}_{a}\), DNN decreases performance on \(\widehat{\mathcal{D}}_{n}\) when we move away from it and towards \(\widehat{\mathcal{D}}_{a}\). Also, the distinct distributions created by multiple perturbations, \(\widehat{\mathcal{D}}_{a}^{l}\), \(\widehat{\mathcal{D}}_{a}^{l_{2}}\), \(\widehat{\mathcal{D}}_{a}^{l_{\infty}}\), contribute to the tradeoff among \(l_{1},l_{2},l_{\infty}\) attacks. To address the tradeoff among perturbations while maintaining good efficiency, we focus on the distributional interconnections between \(\widehat{\mathcal{D}}_{n}\) and \(\widehat{\mathcal{D}}_{a}^{l_{1}}\), \(\widehat{\mathcal{D}}_{a}^{l_{2}}\), \(\widehat{\mathcal{D}}_{a}^{l_{\infty}}\). From the insights we get from above, we propose our framework **RAMP**, which includes (i) logit pairing to improve tradeoffs among multiple perturbations, and (ii) identifying and combining the useful DNN components using the model updates from NT and AT, to obtain a better robustness/accuracy tradeoff.

**Identify the Key Tradeoff Pair.** We study the common case with \(l_{p}\) norms \(\epsilon_{1}=12,\epsilon_{2}=0.5,\epsilon_{\infty}=\frac{8}{255}\) on CIFAR-10 (Tramer and Boneh, 2019). The distributions generated by the two strongest attacks show the largest shifts from \(\widehat{\mathcal{D}}_{n}\); also, they have the largest distribution shifts between each other 

[MISSING_PAGE_FAIL:5]

**NT can help adversarial robustness.** Let us consider two models \(f_{1}\) and \(f_{2}\), where \(f_{1}\) is randomly initialized and \(f_{2}\) undergoes NT on \(\widehat{\mathcal{D}}_{n}\) for \(k\) epochs: \(f_{2}\) results in a better _decision boundary_ and higher clean accuracy. Performing AT on \(f_{1}\) and \(f_{2}\) subsequently, intuitively, \(f_{2}\) becomes more robust than \(f_{1}\) due to its improved decision boundary, leading to fewer misclassifications of adversarial examples. This effect is empirically shown in Figure 2. For **AT** (blue), standard AT against \(l_{\infty}\) attack [10] is performed, while for **AT-pre** (red), \(50\) epochs of pre-training precede the standard AT procedure. **AT-pre** shows superior clean and robust accuracy on CIFAR-10 against \(l_{\infty}\) PGD-20 attack with \(\epsilon_{\infty}=0.031\). Despite \(\widehat{\mathcal{D}}_{n}\) and \(\widehat{\mathcal{D}}_{a}\) are different, Figure 2 suggests valuable information in \(\widehat{\mathcal{D}}_{n}\) that potentially enhances performance on \(\widehat{\mathcal{D}}_{a}\).

**AT with Gradient Projection.** To connect NT with AT more effectively, we analyze the training procedures on \(\widehat{\mathcal{D}}_{n}\) and \(\widehat{\mathcal{D}}_{a}\). We consider model updates over all samples from \(\widehat{\mathcal{D}}_{n}\) and \(\widehat{\mathcal{D}}_{a}\), with the initial model \(f^{(r)}\) at epoch \(r\), and models \(f^{(r)}_{n}\) and \(f^{(r)}_{a}\) after \(1\) epoch of natural and adversarial training from the same starting point \(f^{(r)}\), respectively. Here, we compare the natural updates \(\widehat{g}_{n}=f^{(r)}_{n}-f^{(r)}\) and adversarial updates \(\widehat{g}_{a}=f^{(r)}_{a}-f^{(r)}\). Due to distribution shift, an _angle_ exists between them. Our goal is to identify useful components from \(g_{n}\) and incorporate them into \(g_{a}\) for increased robustness in \(\widehat{\mathcal{D}}_{a}\) while maintaining accuracy in \(\widehat{\mathcal{D}}_{n}\). Inspired by Jiang et al. [2023], we _layer-wisely_ compute the cosine similarity between \(\widehat{g}_{n}\) and \(\widehat{g}_{a}\). For a specific layer \(l\) of \(\widehat{g}_{n}^{l}\) and \(\widehat{g}_{a}^{l}\), we preserve a portion of \(\widehat{g}_{n}^{l}\) based on their cosine similarity score (Eq.4). Negative scores indicate that \(\widehat{g}_{n}^{l}\) is not beneficial for robustness in \(\widehat{\mathcal{D}}_{a}\). Therefore, we filter components with similarity score \(\leq 0\). We define the **GP** (Gradient Projection) operation in Eq.5 by projecting \(\widehat{g}_{a}^{l}\) towards \(\widehat{g}_{n}^{l}\).

\[\cos(\widehat{g}_{n}^{l},\widehat{g}_{a}^{l})=\frac{\widehat{g}_{n}^{l}\cdot \widehat{g}_{a}^{l}}{\|\widehat{g}_{n}^{l}\|\|\widehat{g}_{a}^{l}\|}\quad\quad \quad\quad\quad\quad\mathbf{GP}(\widehat{g}_{n}^{l},\widehat{g}_{a}^{l})= \begin{cases}\cos(\widehat{g}_{n}^{l},\widehat{g}_{a}^{l})\cdot\widehat{g}_{n }^{l},&\cos(\widehat{g}_{n}^{l},\widehat{g}_{a}^{l})>0\\ 0,&\cos(\widehat{g}_{n}^{l},\widehat{g}_{a}^{l})\leq 0\end{cases}\] (5)

Therefore, the total projected (useful) model updates \(g_{p}\) coming from \(\widehat{g}_{n}\) could be computed as Eq. 6. We use \(\mathcal{M}\) to denote all layers of the current model update. Note that \(\bigcup_{l\in\mathcal{M}}\) concatenates all layers' useful natural model update components. A hyper-parameter \(\beta\) is used to balance the contributions of \(g_{GP}\) and \(\widehat{g}_{a}\), as shown in Eq. 7. By finding a proper \(\beta\) (0.5 as in Figure 3(c)), we can obtain better robustness on \(\widehat{D}_{a}\), as shown in Figure 2 and Figure 3. In Figure 2, with \(\beta=0.5\), **AT-GP** refers to AT with GP; for **AT-GP-pre**, we perform \(50\) epochs of NT before doing **AT-GP**. We see **AT-GP** obtains a better accuracy/robustness tradeoff than **AT**. We observe a similar trend for **AT-GP-pre** vs. **AT-pre**. Further, in Figure 3, **RN-18**\(l_{\infty}\)-**GP** achieves good clean accuracy and better robustness than **RN-18**\(l_{\infty}\) against AutoAttack [12].

\[g_{p}=\bigcup_{l\in\mathcal{M}}\mathbf{GP}(\widehat{g}_{n}^{l},\widehat{g}_ {a}^{l})\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad f^{ (r+1)}=f^{(r)}+\beta\cdot g_{p}+(1-\beta)\cdot\widehat{g}_{a}\] (7)

```
1:Input: model \(f\), input samples \((x,y)\) from distribution \(\widehat{\mathcal{D}}_{n}\), fine-tuning rounds \(R\), hyper-parameter \(\lambda\), adversarial regions \(B_{q},B_{r}\) with size \(\epsilon_{q}\) and \(\epsilon_{r}\), **APGD** attack.
2:for\(r=1,2,...,R\)do
3:for\((x,y)\sim\) training set \(\mathcal{D}\)do
4:\(x^{\prime}_{q},p_{q}\leftarrow\mathbf{APGD}(B_{q}(x,\epsilon_{q}),y)\)
5:\(x^{\prime}_{r},p_{r}\leftarrow\mathbf{APGD}(B_{r}(x,\epsilon_{r}),y)\)
6:\(\gamma\gets where(argmax\;p_{q}=y)\)
7:\(n_{c}\leftarrow\gamma.size()\)
8: calculate \(\mathcal{L}\) using Eq. 3 and update \(f\)
9:endfor
10:endfor
11:Output: model \(f\). ```

**Algorithm 1** Fine-tuning via Logit Pairing

```
1:Input: model \(f\), input images with distribution \(\widehat{\mathcal{D}}_{n}\), training rounds \(R\), adversarial region \(B_{p}\) and its size \(\epsilon_{p}\), \(\beta\), natural training **NT** and adversarial training **AT**.
2:for\(r=1,2,...,R\)do
3:\(f_{n}\leftarrow\mathbf{NT}(f^{(r)},\mathcal{D})\)
4:\(f_{a}\leftarrow\mathbf{AT}(f^{(r)},\mathcal{B}_{p},\epsilon_{p},\mathcal{D})\)
5: compute \(\widehat{g}_{n}\gets f_{n}-f^{(r)}\), \(\widehat{g}_{a}\gets f_{a}-f^{(r)}\)
6: compute \(g_{p}\) using Eq. 6
7: update \(f^{(r+1)}\) using Eq. 7 with \(\beta\) and \(\widehat{g}_{a}\)
8:endfor
9:Output: model \(f\). ```

**Algorithm 2** Connect AT with NT via GP

### Theoretical Analysis of GP for Adversarial Robustness

We define \(\mathcal{D}_{n}=\{(x_{i},y_{i})\}_{i=0}^{\infty}\) as the ideal data distribution with an infinite cardinality. Here, we consider a classifier \(f_{\theta}\) at epoch \(t\). We define \(\mathcal{D}_{a}\) as the distribution created by \(\{(x_{i}+\epsilon(f_{\theta},x_{i},y_{i}),y_{i})\}_{i=0}^{\infty}\) where \((x_{i},y_{i})\sim\mathcal{D}_{n}\). \(x_{i}+\epsilon(f_{\theta},x_{i},y_{i})\) denotes the perturbed image, which could be both single and multiple perturbations based on \(f_{\theta}\) itself.

**Assumption 4.1**.: _We assume \(\widehat{\mathcal{D}}_{n}\) consists of \(N\) i.i.d. samples from the ideal distribution \(\mathcal{D}_{n}\) and \(\widehat{\mathcal{D}}_{a}=\{(x_{i}+\epsilon(f^{\theta},x_{i},y_{i}),y_{i})\}_ {i=0}^{N}\) where \((x_{i},y_{i})\sim\widehat{\mathcal{D}}_{n}\) consists of \(N\) i.i.d. samples from \(\mathcal{D}_{a}\)._

We define the population loss as \(\mathcal{L}_{D}(\theta):=\mathbb{E}_{(x,y)\sim\mathcal{D}}\mathcal{L}(f(x),y)\), and let \(g_{\mathcal{D}}(\theta):=\nabla\mathcal{L}_{\mathcal{D}}(\theta)\). For simplification, we use \(g_{a}:=\nabla\mathcal{L}_{\mathcal{D}_{a}}(\theta)\), \(\widehat{g}_{a}:=\nabla\mathcal{L}_{\widehat{\mathcal{D}}_{a}}(\theta)\), and \(\widehat{g}_{n}:=\nabla\mathcal{L}_{\widehat{\mathcal{D}}_{n}}(\theta)\). \(g_{GP}=\beta\cdot g_{p}+(1-\beta)\cdot\widehat{g}_{a}\) (Definition A.3) is the aggregation using GP. We define the following optimization problem.

**Definition 4.2** (Aggregation for NT and AT).: \(f_{\theta}\) _is trained by iteratively updating the parameter_

\[\theta\leftarrow\theta-\mu\cdot\texttt{Aggr}(\widehat{g}_{a},\widehat{g}_{n}),\]

_where \(\mu\) is the step size. We seek an aggregation rule \(\texttt{Aggr}(\cdot)=\widehat{g}_{\texttt{Aggr}}\) such that after training, \(f_{\theta}\) minimizes the population loss function \(\mathcal{L}_{\mathcal{D}_{a}}(\theta)\)._

We need \(\widehat{g}_{\texttt{Aggr}}\) to be close to \(g_{a}\) for each iteration, since \(g_{a}\) is the optimal update on \(\mathcal{D}_{a}\). Thus, we define \(L^{\pi}\)-Norm and delta error to indicate the performance of different aggregation rules.

**Definition 4.3** (\(L^{\pi}\)-Norm [Enyi Jiang, 2024]).: _Given a distribution \(\pi\) on the parameter space \(\theta\), we define an inner product \(\langle g_{\mathcal{D}},g_{\mathcal{D}^{\prime}}\rangle_{\pi}=\mathbb{E}_{ \theta\sim\pi}[\langle g_{\mathcal{D}}(\theta),g_{\mathcal{D}^{\prime}}(\theta) \rangle]\). The inner product induces the \(L^{\pi}\)-norm on \(g_{\mathcal{D}}\) as \(\|g_{\mathcal{D}}\|_{\pi}:=\sqrt{\mathbb{E}_{\theta\sim\pi}\|g_{\mathcal{D}}( \theta)\|^{2}}\). We use \(L^{\pi}\)-norm to measure the gradient differences under certain \(\mathcal{D}\)._

**Definition 4.4** (Delta Error of an aggregation rule \(\texttt{Aggr}(\cdot)\)).: _We define the following squared error term to measure the closeness between \(\widehat{g}_{\texttt{Aggr}}\) and \(g_{a}\) under \(\widehat{\mathcal{D}}_{a}^{i}\) (distribution at time step \(t\)), i.e.,_

\[\Delta^{2}_{\texttt{Aggr}}:=\mathbb{E}_{\widehat{\mathcal{D}}_{a}^{i}}\|g_{a} -\widehat{g}_{\texttt{Aggr}}\|_{\pi}^{2}.\]

Delta errors \(\Delta^{2}_{AT}\) and \(\Delta^{2}_{GP}\) measure the closenessness of \(g_{GP},\widehat{g}_{a}\) from \(g_{a}\) in \(\widehat{\mathcal{D}}_{a}\) at each iteration.

**Theorem 4.5** (Error Analysis of GP).: _When the model dimension \(m\rightarrow\infty\), for an epoch \(t\), we have an approximation of the error difference \(\Delta^{2}_{AT}-\Delta^{2}_{GP}\) as follows_

\[\Delta^{2}_{AT}-\Delta^{2}_{GP}\approx\beta(2-\beta)\mathbb{E}_{\widehat{ \mathcal{D}}_{a}^{1}}\|g_{a}-\widehat{g}_{a}\|_{\pi}^{2}-\beta^{2}\bar{\tau}^ {2}\|g_{a}-\widehat{g}_{n}\|_{\pi}^{2}\]

\(\bar{\tau}^{2}=\mathbb{E}_{\pi}[\tau^{2}]\in[0,1]\)_, where \(\tau(\theta)\) is the \(\sin(\cdot)\) value of the angle between \(\widehat{g}_{n}\) and \(g_{a}-\widehat{g}_{n}\)._

Theorem 4.5 shows \(\Delta^{2}_{GP}\) is generally smaller than \(\Delta^{2}_{AT}\) for a large model dimension during each iteration, as is the case for the models in our evaluation, with \(\beta=0.5\), since \(\beta(1-\beta)>\beta^{2}(0.75>0.25)\) and the small value of \(\bar{\tau}\) in practice (see Interpretation of Theorem A.2 in Appendix A, where we show the order of difference is between \(1e^{-8}\) and \(1e^{-12}\)). Thus, GP achieves better robust accuracy than AT by achieving a smaller delta error; GP also obtains good clean accuracy by combining parts of the model updates from the clean distribution \(\widehat{\mathcal{D}}_{n}\). Further, we provide an error analysis of a single gradient step in Theorem A.1 and convergence analysis in Theorem A.2, showing that a smaller Delta error results in better convergence. The full proof of all theorems is in Appendix A.

We outline the **AT-GP** method in Algorithm 2 and it can be extended to the multiple-norm scenario. The overhead of this algorithm comes from natural training and GP operation. Their costs are small, and we discuss this more in Section 5.2. Combining logit pairing and gradient projection methods, we provide the **RAMP** framework which is similar to Algorithm 2, except that we replace line 4 of Algorithm 2 as Algorithm 1 line 3-9.

## 5 Experiment

**Datasets, baselines, and models.** CIFAR-10 [Krizhevsky et al., 2009] includes \(60\)K images with \(50\)K and \(10\)K images for training and testing respectively. ImageNet has \(\approx 14.2\)M images and \(1\)K classes, containing \(\approx 1.3\)M training, \(50\)K validation, and \(100\)K test images [Russakovsky et al.,2015). We compare **RAMP** with following baselines: 1. **SAT**(Madaan et al., 2021): randomly sample one of the \(l_{1}\), \(l_{2}\), \(l_{\infty}\) attacks. 2. **AVG**(Tramer and Boneh, 2019): take the average of \(l_{1},l_{2},l_{\infty}\) examples. 3. **MAX**(Tramer and Boneh, 2019): take the worst of \(l_{1},l_{2},l_{\infty}\) attacks. 4. **MSD**(Maini et al., 2020): find the worst-case examples over \(l_{1},l_{2},l_{\infty}\) steepest descent directions during each step of inner maximization. 5. **E-AT**(Croce and Hein, 2022): randomly sample between \(l_{1}\), \(l_{\infty}\) attacks. For models, we use PreAct-ResNet-18, ResNet-50, WideResNet-34-20, and WideResNet-70-16 for CIFAR-10, as well as ResNet-50 and XCiT-S transformer for ImageNet.

**Implementations and Evaluation.** For AT from scratch for CIFAR-10, we train PreAct ResNet-18 (He et al., 2016) with a \(lr=0.05\) for \(70\) epochs and \(0.005\) for \(10\) more epochs. We set \(\lambda=2\), \(\beta=0.5\) for training from scratch, and \(\lambda=0.5\) for robust fine-tuning. For all methods, we use \(10\) steps for the inner maximization in AT. For ImageNet, we perform \(1\) epoch of fine-tuning and use a learning rate \(lr=0.005\), \(\lambda=0.5\) for ResNet-50 and \(lr=1e^{-4}\), \(\lambda=0.5\) for XCiT-S models. We reduce the rate by a factor of \(10\) every \(\frac{1}{3}\) of the training epoch and set the weight decay to \(1e^{-4}\). We use APGD with \(5\) steps for \(l_{\infty}\) and \(l_{2}\), \(15\) steps for \(l_{1}\). Settings are similar to (Croce and Hein, 2022). We use the standard values of \(\epsilon_{1}=12,\epsilon_{2}=0.5,\epsilon_{\infty}=\frac{8}{255}\) for CIFAR-10 and \(\epsilon_{1}=255,\epsilon_{2}=2,\epsilon_{\infty}=\frac{4}{255}\) for ImageNet. We focus on \(l_{\infty}\)-AT models for fine-tuning, as Croce and Hein (2022) shows their higher union accuracy for the \(\epsilon\) values in our evaluation. We report the clean accuracy, robust accuracy against \(\{l_{1},l_{2},l_{\infty}\}\) attacks, union accuracy, universal robustness against common corruptions and unseen adversaries, as well as runtime for **RAMP**. The robust accuracy is evaluated using Autoattack (Croce and Hein, 2020). More implementation details are in Appendix B.

### Main Results

**Robust fine-tuning.** In Table 2, we apply **RAMP** to larger models and datasets (ImageNet). However, the implementation of other baselines is not publicly available and Croce and Hein (2022) do not report other baseline results except E-AT on larger models and datasets, so we only compare against E-AT in Table 2, which shows **RAMP** consistently obtains better union accuracy and accuracy-robustness tradeoff than E-AT. We observe that **RAMP** improves the performance more as the model becomes larger. We obtain the SOTA union accuracy of \(53.3\%\) on CIFAR-10 and \(29.1\%\) on ImageNet.

**RAMP with varying \(\epsilon_{1},\epsilon_{2},\epsilon_{\infty}\) values.** We provide results with 1. \((\epsilon_{1}=12,\epsilon_{2}=0.5,\epsilon_{\infty}=\frac{2}{255})\) where \(\epsilon_{\infty}\) size is small and 2. \((\epsilon_{1}=12,\epsilon_{2}=1.5,\epsilon_{\infty}=\frac{8}{255})\) where \(\epsilon_{2}\) size is large, using PreAct ResNet-18 model for CIFAR-10 dataset: these cases have different tradeoff pair compared to

\begin{table}
\begin{tabular}{c c c c c c c c c c c c}  & \multicolumn{4}{c}{\((12,0.5,\frac{25}{255})\)} \\  & & Clean & \(l_{\infty}\) & \(l_{2}\) & \(l_{1}\) & Union & Clean & \(l_{\infty}\) & \(l_{2}\) & \(l_{1}\) & Union \\ \hline \multirow{2}{*}{Training from Scratch} & E-AT & 87.2 & 73.3 & 64.1 & 55.4 & 55.4 & 83.5 & 41.0 & 25.5 & 52.9 & 25.5 \\  & MAX & 85.6 & 72.1 & 63.6 & 56.4 & 56.4 & 74.6 & 42.9 & 35.7 & 50.3 & 35.6 \\  & **RAMP** & 86.3 & 73.3 & 64.9 & 59.1 & **59.1** & 74.4 & 43.4 & 37.2 & 51.1 & **37.1** \\ \hline \multirow{2}{*}{Robust Fine-tuning} & E-AT & 86.5 & 74.8 & 66.7 & 57.9 & 57.9 & 80.2 & 42.8 & 31.5 & 52.4 & 31.5 \\  & MAX & 85.7 & 74.0 & 66.2 & 60.0 & 60.0 & 74.8 & 43.8 & 36.7 & 50.2 & 36.6 \\  & **RAMP** & 85.8 & 74.0 & 66.2 & 60.1 & **60.1** & 74.9 & 43.7 & 37.0 & 50.2 & **36.9** \\ \hline \end{tabular}
\end{table}
Table 1: **Different epsilon values**: **RAMP** consistently outperforms E-AT and MAX for both training from scratch and robust fine-tuning when the key tradeoff pair changes.

**Universal Robustness.** In Table 4, we report average accuracy against common corruptions and union accuracy against unseen adversaries from Laidlaw et al. (2020) (implementation details are in Appendix B.3). We compare against \(l_{p}\) pretrained models, E-AT, MAX, winningband (Diflenderfer et al., 2021) (a SOTA method for natural corruptions) using WideResNet-28-10 architecture on the CIFAR-10 dataset. Compared to E-AT and MAX, **RAMP** achieves \(4\%\) higher accuracy for common corruptions with five severity levels and \(2\)-\(4\%\) better union accuracy against multiple unseen adversaries. Winninghand has high corruption robustness but no adversarial robustness. The results show that **RAMP** obtains a better robustness and accuracy tradeoff with stronger universal robustness. In Appendix B.3, we evaluate on ResNet-18 to support this fact further.

### Ablation Study and Discussion

**Sensitivities of \(\lambda\).** We perform experiments with different \(\lambda\) values in \([0.1,0.5,1.0,1.5,2,3,4,5]\) for robust fine-tuning and \([1.5,2,3,4,5,6]\) for AT from scratch using PreAct-ResNet-18 model for CIFAR-10 dataset. In Figure 4, we observe a decreased clean accuracy when \(\lambda\) becomes larger. We

\begin{table}
\begin{tabular}{l c|c c c c c c c} Methods & \multicolumn{1}{c}{Common Corruptions} & \(l_{0}\) & fog & swift & fabric & fabric & jpegiff & Avg & Union \\ \hline \(l_{1}\)-AT & 78.2 & 79.0 & 41.4 & 22.9 & 40.5 & 48.9 & 48.4 & 46.9 & 12.8 \\ \(l_{2}\)-AT & 77.2 & 67.5 & 48.7 & 26.1 & 44.1 & 53.2 & 45.4 & 47.5 & 16.2 \\ \(l_{3\infty}\)-AT & 73.4 & 55.5 & 44.7 & 32.9 & 53.8 & 56.6 & 33.4 & 46.2 & 19.1 \\ Winninghand (Diflenderfer et al., 2021) & **91.1** & 74.1 & 74.5 & 18.3 & 76.5 & 12.6 & 0.0 & 42.7 & 0.0 \\ E-AT & 71.5 & 58.5 & 35.9 & 35.3 & 50.7 & 55.7 & 60.3 & 49.4 & 21.9 \\ MAX & 71.0 & 56.2 & 42.9 & 35.4 & 49.8 & 57.8 & 55.7 & 49.6 & 24.4 \\
**RAMP** & 75.5 & 55.5 & 40.5 & 40.2 & 52.9 & 60.3 & 56.1 & **50.9** & **26.1** \\ \hline \end{tabular}
\end{table}
Table 4: Individual, average, and union accuracy against common corruptions (averaged across five levels) and unseen adversaries using WideResNet-28-10 on CIFAR-10 dataset.

\begin{table}
\begin{tabular}{l l l l l l l} Methods & \multicolumn{1}{c}{Clean} & \(l_{\infty}\) & \(l_{2}\) & \(l_{1}\) & Union \\ \hline SAT & 83.9\(\pm\)0.8 & 40.7\(\pm\)0.7 & 68.0\(\pm\)0.4 & 54.0\(\pm\)1.2 & 40.4\(\pm\)0.7 \\ AVG & 84.6\(\pm\)0.3 & 40.8\(\pm\)0.7 & 68.4\(\pm\)0.7 & 52.1\(\pm\)0.4 & 40.1\(\pm\)0.8 \\ MAX & 80.4\(\pm\)0.3 & 45.7\(\pm\)0.9 & 66.0\(\pm\)0.4 & 48.6\(\pm\)0.8 & 44.0\(\pm\)0.7 \\ MSD & 81.1\(\pm\)1.1 & 44.9\(\pm\)0.6 & 65.9\(\pm\)0.6 & 49.5\(\pm\)1.2 & 43.9\(\pm\)0.8 \\ E-AT & 82.2\(\pm\)1.8 & 42.7\(\pm\)0.7 & 67.5\(\pm\)0.5 & 53.6\(\pm\)0.1 & 42.4\(\pm\)0.6 \\
**RAMP** (\(\lambda\)\(\approx\)5) & 81.2\(\pm\)0.3 & 46.0\(\pm\)0.5 & 65.8\(\pm\)0.2 & 48.3\(\pm\)0.6 & **44.6\(\pm\)0.6** \\
**RAMP** (\(\lambda\)\(\sim\)2) & 82.1\(\pm\)0.3 & 45.5\(\pm\)0.3 & 66.6\(\pm\)0.3 & 48.4\(\pm\)0.2 & 44.0\(\pm\)0.2 \\ \hline \end{tabular}
\end{table}
Table 3: **RN-18 model trained from random initialization on CIFAR-10 over 5 trials: **RAMP** achieves the best union robustness and good clean accuracy compared with other baselines. Baseline results are from Croce and Hein (2022).

Figure 1: The pair identified using our heuristic are \(l_{1}\) - \(l_{2}\) and \(l_{2}\) - \(l_{\infty}\). In Table 1, we observe that **RAMP** consistently outperforms E-AT and MAX with significant margins in union accuracy, when training from scratch and performing robust fine-tuning. In Table 1, when \(l_{2}\) is the bottleneck, E-AT obtains a lower union accuracy as it does not leverage \(l_{2}\) examples. Similar observations are made across various epsilon values, with **RAMP** consistently outperforming other baselines, as detailed in Appendix B.4. Appendix B includes more training details/results, and ablation studies. Results for applying the trades loss to **RAMP** outperforming E-AT are detailed in Appendix B.6. Appendix B presents robust fine-tuning using ResNet-18, where **RAMP** achieves the highest union accuracy.

\begin{table}
\begin{tabular}{l l l l l l l} Methods & \multicolumn{1}{c}{Clean} & \(l_{\infty}\) & \(l_{2}\) & \(l_{1}\) & Union \\ \hline SAT & 83.9\(\pm\)0.8 & 40.7\(\pm\)0.7 & 68.0\(\pm\)0.4 & 54.0\(\pm\)1.2 & 40.4\(\pm\)0.7 \\ AVG & 84.6\(\pm\)0.3 & 40.8\(\pm\)0.7 & 68.4\(\pm\)0.7 & 52.1\(\pm\)0.4 & 40.1\(\pm\)0.8 \\ MAX & 80.4\(\pm\)0.3 & 45.7\(\pm\)0.9 & 66.0\(\pm\)0.4 & 48.6\(\pm\)0.8 & 44.0\(\pm\)0.7 \\ MSD & 81.1\(\pm\)1.1 & 44.9\(\pm\)0.6 & 65.9\(\pm\)0.6 & 49.5\(\pm\)1.2 & 43.9\(\pm\)0.8 \\ E-AT & 82.2\(\pm\)1.8 & 42.7\(\pm\)0.7 & 67.5\(\pm\)0.5 & 53.6\(\pm\)0.1 & 42.4\(\pm\)0.6 \\
**RAMP** (\(\lambda\)\(\approx\)5) & 81.2\(\pm\)0.3 & 46.0\(\pm\)0.5 & 65.8\(\pm\)0.2 & 48.3\(\pm\)0.6 & **44.6\(\pm\)0.6** \\
**RAMP** (\(\lambda\)\(\sim\)2) & 82.1\(\pm\)0.3 & 45.5\(\pm\)0.3 & 66.6\(\pm\)0.3 & 48.4\(\pm\)0.2 & 44.0\(\pm\)0.2 \\ \hline \end{tabular}
\end{table}
Table 4: Individual, average, and union accuracy against common corruptions (averaged across five levels) and unseen adversaries using WideResNet-28-10 on CIFAR-10 dataset.

pick \(\lambda=2.0\) for training from scratch (Figure 3(a)) and \(\lambda=0.5\) for robust fine-tuning (Figure 3(b)) in our main experiments, as these values of \(\lambda\) yield both good clean and union accuracy.

**Choices of \(\beta\).** Figure 3(c) shows the performance of **RAMP** with varying \(\beta\) values on CIFAR-10 ResNet-18 experiments. We pick \(\beta=0.5\) for combining natural training and AT via GP, which achieves comparatively good robustness and clean accuracy. This choice is also based on Theorem 4.5 when \(\beta(2-\beta)\) has the largest difference from \(\beta^{2}\) (0.75 vs 0.25).

**Fine-tune \(l_{p}\) AT models with RAMP.** Table 5 shows the robust fine-tuning results using **RAMP** with \(l_{\infty}\)-AT (\(q=\infty,r=1\)), \(l_{1}\)-AT (\(q=1,r=\infty\)), \(l_{2}\)-AT (\(q=\infty,r=1\)) RN-18 models for CIFAR-10 dataset. For \(l_{\infty}-l_{1}\) tradeoffs, RAMP on \(l_{\infty}\)-AT pre-trained model achieves the best union accuracy.

**Computational analysis and Limitations.** The extra training costs of AT-GP are small, e.g. for each epoch on ResNet-18, the extra NT takes \(6\) seconds and the standard AT takes \(78\) seconds using a single NVIDIA A100 GPU, and the **GP** operation only takes \(0.04\) seconds on average. RAMP is more expensive than E-AT and less expensive than MAX. We have a complete runtime analysis in Appendix B.2. We notice occasional drops in clean accuracy during fine-tuning with **RAMP**. In some cases, union accuracy improves slightly but clean accuracy and single \(l_{p}\) robustness reduce. Further, we find no negative societal impact from this work.

## 6 Conclusion

We introduce **RAMP**, a framework enhancing multiple-norm robustness and achieving superior _universal robustness_ against corruptions and perturbations by addressing tradeoffs among \(l_{p}\) perturbations and accuracy/robustness. We apply a new logit pairing loss and use gradient projection to obtain SOTA union accuracy with favorable accuracy/robustness tradeoffs against common corruptions and other unseen adversaries. Results demonstrate that **RAMP** surpasses SOTA methods in union accuracy across model architectures on CIFAR-10 and ImageNet.

## Acknowledgments

This work was supported in part by NSF Grants No. CCF-2238079, CCF-2316233, CNS-2148583. We would like to thank Jacky Yibo Zhang for the helpful discussions and advice on the proof. Also, we thank anonymous reviewers for their valuable feedback on the paper.

## References

* Alhamoud et al. (2023) Kumail Alhamoud, Hasan Abed Al Kader Hammoud, Motasem Alfarra, and Bernard Ghanem. Generalizability of adversarial robustness under distribution shifts. _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=XNFo3dQiCJ. Featured Certification.
* Alhamoud et al. (2018)

Figure 4: Alabtion studies on \(\lambda\) and \(\beta\) hyper-parameters.

Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial examples. In _International conference on machine learning_, pages 284-293. PMLR, 2018.
* Banerjee and Singh (2024) Debangshu Banerjee and Gagandeep Singh. Relational dnn verification with cross executional bound refinement. _arXiv preprint arXiv:2405.10143_, 2024.
* Banerjee et al. (2024) Debangshu Banerjee, Changming Xu, and Gagandeep Singh. Input-relational verification of deep neural networks. _Proceedings of the ACM on Programming Languages_, 8(PLDI):1-27, 2024.
* Benz et al. (2021) Philipp Benz, Chaoning Zhang, and In So Kweon. Batch normalization increases adversarial vulnerability and decreases adversarial transferability: A non-robust feature perspective. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 7818-7827, 2021.
* Carmon et al. (2019) Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Unlabeled data improves adversarial robustness. _Advances in neural information processing systems_, 32, 2019.
* Croce and Hein (2020) Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In _International conference on machine learning_, pages 2206-2216. PMLR, 2020.
* Croce and Hein (2022) Francesco Croce and Matthias Hein. Adversarial robustness against multiple and single \(l\_p\)-threat models via quick fine-tuning of robust classifiers. In _International Conference on Machine Learning_, pages 4436-4454. PMLR, 2022.
* Croce et al. (2020) Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. _arXiv preprint arXiv:2010.09670_, 2020.
* Croce et al. (2022) Francesco Croce, Maksym Andriushchenko, Naman D Singh, Nicolas Flammarion, and Matthias Hein. Sparse-rs: a versatile framework for query-efficient sparse black-box adversarial attacks. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 6437-6445, 2022.
* Debenedetti and Troncoso--EPFL (2022) Edoardo Debenedetti and Carmela Troncoso--EPFL. Adversarially robust vision transformers, 2022.
* Diffenderfer et al. (2021) James Diffenderfer, Brian Bartoldson, Shreya Chaganti, Jize Zhang, and Bhavya Kailkhura. A winning hand: Compressing deep networks can improve out-of-distribution robustness. _Advances in neural information processing systems_, 34:664-676, 2021.
* Engstrom et al. (2017) Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A rotation and a translation suffice: Fooling cnns with simple transformations. 2017.
* Engstrom et al. (2018) Logan Engstrom, Andrew Ilyas, and Anish Athalye. Evaluating and understanding the robustness of adversarial logit pairing. _arXiv preprint arXiv:1807.10272_, 2018.
* Engstrom et al. (2019) Logan Engstrom, Andrew Ilyas, Hadi Salman, Shibani Santurkar, and Dimitris Tsipras. Robustness (python library), 2019. URL https://github.com/MadryLab/robustness.
* Jiang (2024) Sanmi Koyejo Enyi Jiang, Yibo Jacky Zhang. Principled federated domain adaptation: Gradient projection and auto-weighting. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=6J3ehSUrMU.
* Eykholt et al. (2018) Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning visual classification. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1625-1634, 2018.
* Goodfellow et al. (2014) Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. _arXiv preprint arXiv:1412.6572_, 2014.
* Gowal et al. (2020) Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli. Uncovering the limits of adversarial training against norm-bounded adversarial examples. _arXiv preprint arXiv:2010.03593_, 2020.
* Ghahramani et al. (2019)Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* Hendrycks and Dietterich (2019) Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. _arXiv preprint arXiv:1903.12261_, 2019.
* Jiang (2023) Enyi Jiang. Federated domain adaptation for healthcare, 2023.
* Jiang et al. (2023) Enyi Jiang, Yibo Jacky Zhang, and Oluwasanmi Koyejo. Federated domain adaptation via gradient projection. _arXiv preprint arXiv:2302.05049_, 2023.
* Kang et al. (2019) Daniel Kang, Yi Sun, Tom Brown, Dan Hendrycks, and Jacob Steinhardt. Transfer of adversarial robustness between perturbation types. _arXiv preprint arXiv:1905.01034_, 2019.
* Kannan et al. (2018) Harini Kannan, Alexey Kurakin, and Ian Goodfellow. Adversarial logit pairing. _arXiv preprint arXiv:1803.06373_, 2018.
* Krizhevsky et al. (2009) Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* Kurakin et al. (2018) Alexey Kurakin, Ian J Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In _Artificial intelligence safety and security_, pages 99-112. Chapman and Hall/CRC, 2018.
* Laidlaw et al. (2020) Cassidy Laidlaw, Sahil Singla, and Soheil Feizi. Perceptual adversarial robustness: Defense against unseen threat models. _arXiv preprint arXiv:2006.12655_, 2020.
* Liu et al. (2020) Aishan Liu, Shiyu Tang, Xianglong Liu, Xinyun Chen, Lei Huang, Zhuozhuo Tu, Dawn Song, and Dacheng Tao. Towards defending multiple adversarial perturbations via gated batch normalization. _arXiv preprint arXiv:2012.01654_, 2020.
* Madaan et al. (2021) Divyam Madaan, Jinwoo Shin, and Sung Ju Hwang. Learning to generate noise for multi-attack robustness. In _International Conference on Machine Learning_, pages 7279-7289. PMLR, 2021.
* Madry et al. (2017) Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. _arXiv preprint arXiv:1706.06083_, 2017.
* Maini et al. (2020) Pratyush Maini, Eric Wong, and Zico Kolter. Adversarial robustness against the union of multiple perturbation models. In _International Conference on Machine Learning_, pages 6640-6650. PMLR, 2020.
* Maini et al. (2022) Pratyush Maini, Xinyun Chen, Bo Li, and Dawn Song. Perturbation type categorization for multiple adversarial perturbation robustness. In _Uncertainty in Artificial Intelligence_, pages 1317-1327. PMLR, 2022.
* Mehrabi et al. (2021) Mohammad Mehrabi, Adel Javanmard, Ryan A Rossi, Anup Rao, and Tung Mai. Fundamental tradeoffs in distributionally adversarial training. In _International Conference on Machine Learning_, pages 7544-7554. PMLR, 2021.
* Moayeri et al. (2022) Mazda Moayeri, Kiarash Banihashem, and Soheil Feizi. Explicit tradeoffs between adversarial and natural distributional robustness. _Advances in Neural Information Processing Systems_, 35:38761-38774, 2022.
* Nandy et al. (2020) Jay Nandy, Wynne Hsu, and Mong Li Lee. Approximate manifold defense against multiple adversarial perturbations. In _2020 International Joint Conference on Neural Networks (IJCNN)_, pages 1-8. IEEE, 2020.
* Peng et al. (2023) ShengYun Peng, Weilin Xu, Cory Cornelius, Matthew Hull, Kevin Li, Rahul Duggal, Mansi Phute, Jason Martin, and Duen Horng Chau. Robust principles: Architectural design principles for adversarially robust cnns. _arXiv preprint arXiv:2308.16258_, 2023.
* Rade and Moosavi-Dezfooli (2021) Rahul Rade and Seyed-Mohsen Moosavi-Dezfooli. Reducing excessive margin to achieve a better accuracy vs. robustness trade-off. In _International Conference on Learning Representations_, 2021.
* Rade et al. (2020)Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John Duchi, and Percy Liang. Understanding and mitigating the tradeoff between robustness and accuracy. _arXiv preprint arXiv:2002.10716_, 2020.
* Russakovsky et al. (2015) Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. _International journal of computer vision_, 115:211-252, 2015.
* Schott et al. (2018) Lukas Schott, Jonas Rauber, Matthias Bethge, and Wieland Brendel. Towards the first adversarially robust neural network model on mnist. _arXiv preprint arXiv:1805.09190_, 2018.
* Sharif et al. (2016) Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In _Proceedings of the 2016 acm sigsac conference on computer and communications security_, pages 1528-1540, 2016.
* Sinha et al. (2018) Aman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with principled adversarial training. In _International Conference on Learning Representations_, 2018. URL https://openreview.net/forum?id=Hk6kPgZA-.
* Song et al. (2018) Dawn Song, Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Florian Tramer, Atul Prakash, and Tadayoshi Kohno. Physical adversarial examples for object detectors. In _12th USENIX workshop on offensive technologies (WOOT 18)_, 2018.
* Tramer & Boneh (2019) Florian Tramer and Dan Boneh. Adversarial training and robustness for multiple perturbations. _Advances in neural information processing systems_, 32, 2019.
* Tramer et al. (2017) Florian Tramer, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble adversarial training: Attacks and defenses. _arXiv preprint arXiv:1705.07204_, 2017.
* Wang et al. (2020) Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving adversarial robustness requires revisiting misclassified examples. In _ICLR_, 2020.
* Wang et al. (2023) Zekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu, and Shuicheng Yan. Better diffusion models further improve adversarial training. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 36246-36263. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/wang23ad.html.
* Wong & Kolter (2020) Eric Wong and J Zico Kolter. Learning perturbation sets for robust machine learning. _arXiv preprint arXiv:2007.08450_, 2020.
* Wu et al. (2020) Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization. _Advances in Neural Information Processing Systems_, 33:2958-2969, 2020.
* Xiao et al. (2022) Jiancong Xiao, Zeyu Qin, Yanbo Fan, Baoyuan Wu, Jue Wang, and Zhi-Quan Luo. Adaptive smoothness-weighted adversarial training for multiple perturbations with its stability analysis. _arXiv preprint arXiv:2210.00557_, 2022.
* Xie et al. (2020) Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L Yuille, and Quoc V Le. Adversarial examples improve image recognition. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 819-828, 2020.
* Xu et al. (2021) Kaidi Xu, Chenan Wang, Hao Cheng, Bhavya Kailkhura, Xue Lin, and Ryan Goldhahn. Mixture of robust experts (more): A robust denoising method towards multiple perturbations. _arXiv preprint arXiv:2104.10586_, 2021.
* Yang et al. (2020) Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Russ R Salakhutdinov, and Kamalika Chaudhuri. A closer look at accuracy vs. robustness. _Advances in neural information processing systems_, 33:8588-8601, 2020.
* Zagoruyko & Komodakis (2016) Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In _Proceedings of the British Machine Vision Conference 2016_. British Machine Vision Association, 2016.
* Zhang et al. (2017)Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In _International conference on machine learning_, pages 7472-7482. PMLR, 2019.
* Zhang et al. (2021) Jingfeng Zhang, Jianing Zhu, Gang Niu, Bo Han, Masashi Sugiyama, and Mohan Kankanhalli. Geometry-aware instance-reweighted adversarial training. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=iAX016Cz8ub.

Proof of Theorems

### Proof of Theorem a.2

We first show what happens during one step of optimization, where we highlight the importance of analyzing delta error.

**Theorem A.1**.: _Consider model parameter \(\theta\sim\pi\) and an aggregation rule \(\texttt{Aggr}(\cdot)\) with step size \(\mu>0\). Define the updated parameter as_

\[\theta^{+}:=\theta-\mu\widehat{g}_{\texttt{Aggr}}(\theta).\]

_Assuming the gradient \(\nabla\mathcal{L}(\theta)\) is \(\gamma\)-Lipschitz in \(\theta\) for any input, and let the step size \(\mu\leq\frac{1}{\gamma},\) we have_

\[\mathbb{E}_{\widehat{\mathcal{D}}_{a},\theta}[\mathcal{L}_{\mathcal{D}_{a}}( \theta^{+})-\mathcal{L}_{\mathcal{D}_{a}}(\theta)]\leq-\tfrac{\mu}{2}(\|g_{a }\|_{\pi}^{2}-\Delta_{\texttt{Aggr}}^{2}).\]

Proof.: The proof is the same as Theorem A.1 in [Enyi Jiang, 2024]. 

**Theorem A.2** (Convergence of \(\texttt{Aggr}(\cdot)\)).: _For any probability measure \(\pi\) over the parameter space, and an aggregation rule \(\texttt{Aggr}(\cdot)\) with step size \(\mu>0\). We update the parameter for \(T\) steps by \(\theta^{t+1}:=\theta^{t}-\mu\widehat{g}_{\texttt{Aggr}}(\theta^{t}).\) Assume the gradient \(\nabla\mathcal{L}(\theta)\) and \(\widehat{g}_{\texttt{Aggr}}(\theta)\) are \(\frac{\gamma}{2}\)-Lipschitz in \(\theta\) such that \(\theta^{t}\to\widehat{\theta}_{\texttt{Aggr}}\). \(\Delta_{\texttt{Aggr},\texttt{max}}\) is the Delta error at time \(t^{\prime}\) when \(\|\widehat{g}_{\texttt{Aggr}}(\widehat{\theta}_{\texttt{Aggr}})-\nabla \mathcal{L}_{\mathcal{D}_{a}^{\prime}}(\widehat{\theta}_{\texttt{Aggr}})\|^ {2}\) is maximized. Then, given step size \(\mu\leq\frac{1}{\gamma}\) and a small enough \(\epsilon>0\), with probability at least \(1-\delta\) we have_

\[\|\nabla\mathcal{L}_{\mathcal{D}_{a}^{\prime}}(\theta^{T})\|^{2}\leq\frac{1}{ \delta^{2}}\left(\sqrt{C_{\epsilon}\cdot\Delta_{\texttt{Aggr},\texttt{max}}^ {2}}+\mathcal{O}(\epsilon)\right)^{2}+\mathcal{O}\left(\frac{1}{T}\right)+ \mathcal{O}(\epsilon),\]

_where \(C_{\epsilon}=\mathbb{E}_{\widehat{\mathcal{D}}_{a}^{\prime}}[1/\pi(B_{ \epsilon}(\widehat{\theta}_{\texttt{Aggr}}))]^{2}\) and \(B_{\epsilon}(\widehat{\theta}_{\texttt{Aggr}})\subset\mathbb{R}^{m}\) is the ball with radius \(\epsilon\) centered at \(\widehat{\theta}_{\texttt{Aggr}}\). The \(C_{\epsilon}\) measures how well \(\pi\) covers where the optimization goes._

Proof.: Denote random function \(\widehat{f}:\mathbb{R}^{m}\to\mathbb{R}_{+}\) as

\[\widehat{f}(\theta)=\|\widehat{g}_{\texttt{Aggr}}(\theta)-\nabla\mathcal{L}_ {\mathcal{D}_{a}}(\theta)\|,\] (8)

where the randomness comes from \(\widehat{\mathcal{D}}_{a}\). Note that \(\widehat{f}\) is \(\gamma\)-Lipschitz by assumption. Now we consider \(B_{\epsilon}(\widehat{\theta}_{\texttt{Aggr}})\subset\mathbb{R}^{m}\), i.e., the ball with radius \(\epsilon\) centered at \(\widehat{\theta}_{\texttt{Aggr}}\). Then, by \(\gamma\)-Lipschitzness we have

\[\mathbb{E}_{\theta\sim\pi}\widehat{f}(\theta)=\int\widehat{f}( \theta)\,\mathrm{d}\pi(\theta)\] \[\geq \int_{B_{\epsilon}(\widehat{\theta}_{\texttt{Aggr}})}(\widehat{f} (\widehat{\theta}_{\texttt{Aggr}})-\gamma\epsilon)\,\mathrm{d}\pi(\theta)\] \[=(\widehat{f}(\widehat{\theta}_{\texttt{Aggr}})-\gamma\epsilon) \pi(B_{\epsilon}(\widehat{\theta}_{\texttt{Aggr}}))\]

Therefore,

\[\widehat{f}(\widehat{\theta}_{\texttt{Aggr}})\leq\frac{1}{\pi(B_{\epsilon}( \widehat{\theta}_{\texttt{Aggr}}))}\cdot\mathbb{E}_{\theta\sim\pi}\widehat{f }(\theta)+\mathcal{O}(\epsilon).\]

Taking expectation w.r.t. \(\widehat{\mathcal{D}}_{a}\) on both sides, we have

\[\mathbb{E}_{\widehat{\mathcal{D}}_{a}}\widehat{f}(\widehat{\theta }_{\texttt{Aggr}}) \leq\mathbb{E}_{\widehat{\mathcal{D}}_{a}}\left[\frac{1}{\pi(B_{ \epsilon}(\widehat{\theta}_{\texttt{Aggr}}))}\cdot\mathbb{E}_{\theta\sim\pi} \widehat{f}(\theta)\right]+\mathcal{O}(\epsilon)\] \[\leq\sqrt{\mathbb{E}_{\widehat{\mathcal{D}}_{a}}\left[\frac{1}{ \pi(B_{\epsilon}(\widehat{\theta}_{\texttt{Aggr}}))}\right]^{2}\cdot\mathbb{E}_ {\widehat{\mathcal{D}}_{a}}\left[\mathbb{E}_{\theta\sim\pi}\widehat{f}( \theta)\right]^{2}+\mathcal{O}(\epsilon)}\] (Cauchy-Schwarz) \[=\sqrt{C_{\epsilon}\cdot\mathbb{E}_{\widehat{\mathcal{D}}_{a}} \left[\mathbb{E}_{\theta\sim\pi}\widehat{f}(\theta)\right]^{2}}+\mathcal{O}(\epsilon)\] (by definition of \[C_{\epsilon}\] ) \[\leq\sqrt{C_{\epsilon}\cdot\mathbb{E}_{\widehat{\mathcal{D}}_{a}} \mathbb{E}_{\theta\sim\pi}\left[\widehat{f}(\theta)\right]^{2}}+\mathcal{O}(\epsilon)\] (Jensen's inequality) \[=\sqrt{C_{\epsilon}\cdot\Delta_{\texttt{Aggr}}^{2}}+\mathcal{O}(\epsilon)\]By Markov's inequality, with probability at least \(1-\delta\) we have a sampled dataset \(\widehat{\mathcal{D}}_{a}\) such that

\[\widehat{f}(\widehat{\theta}_{\texttt{Aggr}})\leq\frac{1}{\delta}\mathbb{E}_{ \widehat{\mathcal{D}}_{a}}\widehat{f}(\widehat{\theta}_{\texttt{Aggr}})\leq \frac{1}{\delta}\sqrt{C_{\epsilon}\cdot\Delta_{\texttt{Aggr}}^{2}}+\mathcal{O} (\epsilon/\delta)\] (9)

Conditioned on such event, we proceed on to the optimization part.

Note that Theorem A.1 characterizes how the optimization works for one gradient update. We denote \(\mathcal{D}_{a}^{t}\) as the data distribution \(\mathcal{D}_{a}\) at time step \(t\). Therefore, for any time step \(t=0,\ldots,T-1\), we can apply Theorem A.1 which only requires the Lipschitz assumption:

\[\mathcal{L}_{\mathcal{D}_{a}^{t}}(\theta^{t+1})-\mathcal{L}_{ \mathcal{D}_{a}^{t}}(\theta^{t})\leq-\frac{\mu}{2}\left(\|\nabla\mathcal{L}_{ \mathcal{D}_{a}^{t}}(\theta^{t})\|^{2}-\|\widehat{g}_{\texttt{Aggr}}(\theta^{ t})-\nabla\mathcal{L}_{\mathcal{D}_{a}^{t}}(\theta^{t})\|^{2}\right).\]

We notice that \(\mathcal{D}_{a}^{t}\) changes based on \(\theta\)s of different time steps. On both sides, to sum over \(t=0,\ldots,T-1\), we first consider two terms:

\[(\mathcal{L}_{\mathcal{D}_{a}^{t}}(\theta^{t+1})-\mathcal{L}_{ \mathcal{D}_{a}^{t}}(\theta^{t}))+(\mathcal{L}_{\mathcal{D}_{a}^{t-1}}(\theta^ {t})-\mathcal{L}_{\mathcal{D}_{a}^{t-1}}(\theta^{t-1}))\]

To compare \(\mathcal{L}_{\mathcal{D}_{a}^{t}}(\theta^{t})\) and \(\mathcal{L}_{\mathcal{D}_{a}^{t-1}}(\theta^{t})\), since \(\mathcal{D}_{a}^{t}\) optimizes one more step than \(\mathcal{D}_{a}^{t-1}\), we assume \(\mathcal{L}_{\mathcal{D}_{a}^{t}}(\theta^{t})\leq\mathcal{L}_{\mathcal{D}_{a} ^{t-1}}(\theta^{t})\) for \(\forall t\). Therefore, we have:

\[(\mathcal{L}_{\mathcal{D}_{a}^{t}}(\theta^{t+1})-\mathcal{L}_{ \mathcal{D}_{a}^{t}}(\theta^{t}))+(\mathcal{L}_{\mathcal{D}_{a}^{t-1}}(\theta^ {t})-\mathcal{L}_{\mathcal{D}_{a}^{t-1}}(\theta^{t-1}))\geq(\mathcal{L}_{ \mathcal{D}_{a}^{t}}(\theta^{t+1})-\mathcal{L}_{\mathcal{D}_{a}^{t-1}}(\theta ^{t}))+(\mathcal{L}_{\mathcal{D}_{a}^{t-1}}(\theta^{t})-\mathcal{L}_{\mathcal{ D}_{a}^{t-1}}(\theta^{t-1}))\]

Summing up all time steps,

\[\mathcal{L}_{\mathcal{D}_{a}^{t}}(\theta^{t+1})-\mathcal{L}_{ \mathcal{D}_{a}^{0}}(\theta^{0})\leq\mathcal{L}_{\mathcal{D}_{a}^{t}}(\theta^ {t+1})-\mathcal{L}_{\mathcal{D}_{a}^{t-1}}(\theta^{t})+\mathcal{L}_{\mathcal{ D}_{a}^{t-1}}(\theta^{t})-\mathcal{L}_{\mathcal{D}_{a}^{t-2}}(\theta^{t-1})+...- \mathcal{L}_{\mathcal{D}_{a}^{0}}(\theta^{0})\] \[\qquad\leq-\frac{\mu}{2}\left(\sum_{t=0}^{T-1}\|\nabla\mathcal{L }_{\mathcal{D}_{a}^{t-1}}(\theta^{t})\|^{2}-\sum_{t=0}^{T-1}\|\widehat{g}_{ \texttt{Aggr}}(\theta^{t})-\nabla\mathcal{L}_{\mathcal{D}_{a}^{t}}(\theta^{t} )\|^{2}\right).\]

Dividing both sides by \(T\), and with regular algebraic manipulation we derive

\[\frac{1}{T}\sum_{t=0}^{T-1}\|\nabla\mathcal{L}_{\mathcal{D}_{a}^{ t-1}}(\theta^{t})\|^{2}\leq\frac{2}{\mu T}(\mathcal{L}_{\mathcal{D}_{a}^{0}}( \theta^{0})-\mathcal{L}_{\mathcal{D}_{a}^{t-1}}(\theta^{T}))+\frac{1}{T}\sum_{t =0}^{T-1}\|\widehat{g}_{\texttt{Aggr}}(\theta^{t})-\nabla\mathcal{L}_{ \mathcal{D}_{a}^{t}}(\theta^{t})\|^{2}.\]

Note that we assume the loss function \(\mathcal{L}_{\mathcal{D}}(\theta):=\mathbb{E}_{(x,y)\sim\mathcal{D}}\mathcal{L} (f(x),y)\), is non-negative. Thus, we have

\[\frac{1}{T}\sum_{t=0}^{T-1}\|\nabla\mathcal{L}_{\mathcal{D}_{a}^{ t-1}}(\theta^{t})\|^{2}\leq\frac{2\mathcal{L}_{\mathcal{D}_{a}^{0}}(\theta^{0})}{ \mu T}+\frac{1}{T}\sum_{t=0}^{T-1}\|\widehat{g}_{\texttt{Aggr}}(\theta^{t})- \nabla\mathcal{L}_{\mathcal{D}_{a}^{t}}(\theta^{t})\|^{2}.\] (10)

Note that we assume given \(\widehat{\mathcal{D}}_{a}\) we have \(\theta^{t}\rightarrow\widehat{\theta}_{\texttt{Aggr}}\). Therefore, for any \(\epsilon>0\) there exist \(T_{\epsilon}\) such that

\[\forall t>T_{\epsilon}:\|\theta^{t}-\widehat{\theta}_{\texttt{Aggr}}\|<\epsilon.\] (11)

This implies that \(\forall t>T_{\epsilon}\):

\[\mu\|\widehat{g}_{\texttt{Aggr}}(\theta^{t})\|=\|\theta^{t+1}- \widehat{\theta}_{\texttt{Aggr}}+\widehat{\theta}_{\texttt{Aggr}}-\theta^{t}\| \leq\|\theta^{t+1}-\widehat{\theta}_{\texttt{Aggr}}\|+\|\widehat{\theta}_{ \texttt{Aggr}}-\theta^{t}\|<2\epsilon.\] (12)

Moreover, (11) also implies \(\forall t_{1},t_{2}>T_{\epsilon}\):

\[\|\nabla\mathcal{L}_{\mathcal{D}_{a}^{t_{1}}}(\theta^{t_{1}})- \nabla\mathcal{L}_{\mathcal{D}_{a}^{t_{2}}}(\theta^{t_{2}})\| \leq\gamma\|\theta^{t_{1}}-\theta^{t_{2}}\|\] ( \[\gamma\] -Lipschitzness) \[<2\epsilon.\] (13)

[MISSING_PAGE_EMPTY:17]

To complete the proof, let us investigate the left-hand side.

\[\frac{1}{T}\sum_{t=0}^{T-1}\|\nabla\mathcal{L}_{\mathcal{D}_{a}^{t-1} }(\theta^{t})\|^{2} =\frac{1}{T}\sum_{t=0}^{T_{e}-1}\|\nabla\mathcal{L}_{\mathcal{D}_{ a}^{t}}(\theta^{t})\|^{2}+\frac{1}{T}\sum_{t=T_{e}}^{T-1}\|\nabla\mathcal{L}_{ \mathcal{D}_{a}^{t}}(\theta^{t})\|^{2}\] \[=\mathcal{O}\left(\frac{1}{T}\right)+\frac{1}{T}\sum_{t=T_{e}}^{T -1}\|\nabla\mathcal{L}_{\mathcal{D}_{a}^{t}}(\theta^{t})\|^{2}\] \[\geq\mathcal{O}\left(\frac{1}{T}\right)+\frac{1}{T}\sum_{t=T_{e} }^{T-1}\left(\|\nabla\mathcal{L}_{\mathcal{D}_{a}^{t}}(\theta^{t})-\nabla \mathcal{L}_{\mathcal{D}_{a}^{T}}(\theta^{T})\|-\|\nabla\mathcal{L}_{\mathcal{ D}_{a}^{T}}(\theta^{T})\|\right)^{2}\] (triangle inequality) \[=\mathcal{O}\left(\frac{1}{T}\right)+\frac{1}{T}\sum_{t=T_{e}}^{ T-1}\left(\mathcal{O}(\epsilon)+\|\nabla\mathcal{L}_{\mathcal{D}_{a}^{T}}( \theta^{T})\|^{2}\right)\] (by (13)) \[=\mathcal{O}\left(\frac{1}{T}\right)+\mathcal{O}(\epsilon)+\| \nabla\mathcal{L}_{\mathcal{D}_{a}^{T}}(\theta^{T})\|^{2}.\] (16)

Combining (15) and (16), we finally have

\[\|\nabla\mathcal{L}_{\mathcal{D}_{a}^{T}}(\theta^{T})\|^{2}\leq\mathcal{O} \left(\frac{1}{T}\right)+\mathcal{O}(\epsilon)+\frac{1}{\delta^{2}}\left( \sqrt{C_{\epsilon}\cdot\Delta^{2}_{\texttt{kgr\_max}}}+\mathcal{O}(\epsilon) \right)^{2},\]

which completes the proof. 

### Proof of Theorem 4.5

To prove Theorem 4.5, we first use the following definitions and lemmas from (Enyi Jiang, 2024), to get the delta errors of Gradient Projection (GP) and standard adversarial training (AT):

**Definition A.3** (GP Aggregation).: _Let \(\beta\in[0,1]\) be the weight that balances between \(\widehat{g}_{a}\) and \(\widehat{g}_{n}\). The GP aggregation operation is_

\[GP(\widehat{g}_{a},\widehat{g}_{n})=\left((1-\beta)\widehat{g}_{a}+\beta \texttt{Proj}_{+}(\widehat{g}_{a}|\widehat{g}_{n})\right).\]

_where \(\texttt{Proj}_{+}(\widehat{g}_{a}|\widehat{g}_{n})=\max\{\langle\widehat{g}_{ a},\widehat{g}_{n}\rangle,0\}\widehat{g}_{n}/\|\widehat{g}_{n}\|^{2}\) is the operation that projects \(\widehat{g}_{a}\) to the positive direction of \(\widehat{g}_{n}\)._

**Definition A.4** (AT Aggregation).: _The AT aggregation operation is_

\[AT(\widehat{g}_{a})=\widehat{g}_{a}.\]

_standard AT only leverages the gradient update on \(\widehat{\mathcal{D}}_{a}\)._

**Lemma A.5** (Delta Error of GP).: _Given distributions \(\widehat{\mathcal{D}}_{a}\), \(\mathcal{D}_{a}\) and \(\widehat{\mathcal{D}}_{n}\), as well as the model updates \(\widehat{g}_{a},g_{a},\widehat{g}_{n}\) on these distributions per epoch, we have \(\Delta^{2}_{GP}\) as follows_

\[\Delta^{2}_{GP}\approx\left((1-\beta)^{2}+\frac{2\beta-\beta^{2}}{m}\right) \mathbb{E}_{\widehat{\mathcal{D}}_{a}}\|g_{a}-\widehat{g}_{a}\|_{\pi}^{2}+ \beta^{2}\bar{\tau}^{2}\|g_{a}-\widehat{g}_{n}\|_{\pi}^{2},\]

_In the above equation, \(m\) is the model dimension and \(\bar{\tau}^{2}=\mathbb{E}_{\pi}[\tau^{2}]\in[0,1]\) where \(\tau(\theta)\) is the \(\sin(\cdot)\) value of the angle between \(\widehat{g}_{n}\) and \(g_{a}-\widehat{g}_{n}\). \(\|\cdot\|_{\pi}\) is the \(\pi\)-norm over the model parameter space._

Proof.: The proof is the same as Theorem 4.4 in Enyi Jiang (2024). 

**Lemma A.6** (Delta Error of AT).: _Given distributions \(\widehat{\mathcal{D}}_{a}\), \(\mathcal{D}_{a}\) and \(\widehat{\mathcal{D}}_{n}\), as well as the model updates \(\widehat{g}_{a},g_{a},\widehat{g}_{n}\) on these distributions per epoch, we have \(\Delta^{2}_{AT}\) as follows_

\[\Delta^{2}_{AT}=\mathbb{E}_{\widehat{\mathcal{D}}_{a}}\|g_{a}-\widehat{g}_{a} \|_{\pi}^{2},\]

_where \(\|\cdot\|_{\pi}\) is the \(\pi\)-norm over the model parameter space._Then, we prove Theorem 4.5.

**Theorem A.7** (Error Analysis of GP).: _When the model dimension is large (\(m\rightarrow\infty\)) at time step \(t\), we have_

\[\Delta_{AT}^{2}-\Delta_{GP}^{2}\approx\beta(2-\beta)\mathbb{E}_{ \widehat{\mathcal{D}}_{a}^{t}}\|g_{a}-\widehat{g_{a}}\|_{\pi}^{2}-\beta^{2}\bar {\tau}^{2}\|g_{a}-\widehat{g}_{n}\|_{\pi}^{2}.\]

\(\bar{\tau}^{2}=\mathbb{E}_{\pi}[\tau^{2}]\in[0,1]\) _where \(\tau\) is the \(\sin(\cdot)\) value of the angle between \(\widehat{g}_{n}\) and \(g_{a}-\widehat{g}_{n}\), \(\|\cdot\|_{\pi}\) is the \(\pi\)-norm over the model parameter space._

Proof.: \(\Delta_{\texttt{AT}}^{2}-\Delta_{\texttt{GP}}^{2}\)

\(\approx\mathbb{E}_{\widehat{\mathcal{D}}_{a}^{t}}\|g_{a}-\widehat{g_{a}}\|_{ \pi}^{2}-\left((1-\beta)^{2}+\frac{2\beta-\beta^{2}}{m}\right)\mathbb{E}_{ \widehat{\mathcal{D}}_{a}^{t}}\|g_{a}-\widehat{g}_{a}\|_{\pi}^{2}-\beta^{2} \bar{\tau}^{2}\|g_{a}-\widehat{g}_{n}\|_{\pi}^{2}\)

\(=\left(1-((1-\beta)^{2}+\frac{2\beta-\beta^{2}}{m})\right)\mathbb{E}_{ \widehat{\mathcal{D}}_{a}^{t}}\|g_{a}-\widehat{g_{a}}\|_{\pi}^{2}-\beta^{2} \bar{\tau}^{2}\|g_{a}-\widehat{g}_{n}\|_{\pi}^{2}\)

\(=(1+\frac{1}{m})\beta(2-\beta)\mathbb{E}_{\widehat{\mathcal{D}}_{a}^{t}}\|g _{a}-\widehat{g_{a}}\|_{\pi}^{2}-\beta^{2}\bar{\tau}^{2}\|g_{a}-\widehat{g}_{ n}\|_{\pi}^{2}\)

When \(m\rightarrow\infty\), we have a simplified version of the error difference as follows

\[\Delta_{AT}^{2}-\Delta_{GP}^{2}\approx\beta(2-\beta)\mathbb{E}_{ \widehat{\mathcal{D}}_{a}^{t}}\|g_{a}-\widehat{g_{a}}\|_{\pi}^{2}-\beta^{2} \bar{\tau}^{2}\|g_{a}-\widehat{g}_{n}\|_{\pi}^{2}\]

**Interpretation.** When \(\beta=0.5\), we can usually show \(\Delta_{AT}^{2}>\Delta_{GP}^{2}\), because \(\beta(2-\beta)>\beta^{2}\bar{\tau}^{2}(0.75>0.25)\) for the coefficients of two terms. We estimate the actual values of terms \(E_{\widehat{D}_{a}^{t}}\|g_{a}-\widehat{g_{a}}\|_{\pi}^{2}\) (variance), \(\|g_{a}-\widehat{g}_{n}\|_{\pi}^{2}\) (bias), and \(\bar{\tau}\) using the estimation methods in Enyi Jiang (2024). Table 6 displays the values of those terms as well as the error differences on ResNet18 experiments at epoch \(5,10,15,20,60\). We plot the changing of these terms on the ResNet18 experiment in Figure 5. The order of difference is always positive and usually smaller than \(1e^{-08}\) and approaches the order of \(1e^{-12}\) in the end.

## Appendix B Additional Experiment Information

In this section, we provide more training details, additional experiment results on the universal robustness of **RAMP** to common corruptions and unseen adversaries, runtime analysis of RAMP, additional ablation studies on different logit pairing losses, and AT from random initialization results on CIFAR-10 using WideResNet-28-10.

### More Training Details

We set the batch size to \(128\) for the experiments on ResNet-18 and WideResNet-28-10 architectures. We use an SGD optimizer with \(0.9\) momentum and \(5e^{-4}\) weight decay. For other experiments on ImageNet, we use a batch size of \(64\) to fit into the GPU memory for larger models. For all training procedures, we select the last checkpoint for the comparison. When the pre-trained model was originally trained with extra data beyond the CIFAR-10 dataset, similar to Croce and Hein (2022), we use the extra \(500\)k images introduced by Carmon et al. (2019) for fine-tuning, and each batch contains the same amount of standard and extra images. An epoch is completed when the whole standard training set has been used.

### Runtime Analysis of RAMP

We present runtime analysis results demonstrating the fact that RAMP is more expensive than E-AT and less expensive than MAX in Table 7. These results, recorded in seconds per epoch, were obtained using a single A100 40GB GPU. RAMP consistently supports that fact in all experiments.

Additional Results on RAMP Generalizing to Common Corruptions and Unseen Adversaries for Universal Robustness

In this section, we show **RAMP** can generalize better to other corruptions and unseen adversaries on union accuracy for stronger universal robustness.

**Implementations.** For the \(l_{0}\) attack, we use Croce et al. (2022) with an epsilon of \(9\) pixels and \(5k\) query points. For common corruptions, we directly use the implementation of RobustBench (Croce et al., 2020) for evaluation across 5 severity levels on all corruption types used in Hendrycks and Dietterich (2019). For other unseen adversaries, we follow the implementation of Laidlaw et al. (2020), where we set \(eps=12\) for the fog attack, \(eps=0.5\) for the snow attack, \(eps=60\) for the gabor attack, \(eps=0.125\) for the elastic attack, and \(eps=0.125\) for the jpeglinf attack with \(100\) iterations. For ResNet-18 experiments, we do not compare with Winninghand (Diffenderfer et al.,

\begin{table}
\begin{tabular}{l r r r} \hline Models \& Methods & E-AT (Croce and Hein, 2022) & MAX & **RAMP** \\ \hline CIFAR-10 RN-18 scratch & 78 & 219 & 157 \\ CIFAR-10 WRN-28-10 scratch & 334 & 1048 & 660 \\ CIFAR-10 RN-50 & 188 & 510 & 388 \\ CIFAR-10 WRN-34-20 & 1094 & 2986 & 2264 \\ CIFAR-10 WRN-28-10 carmon & 546 & 1420 & 1110 \\ CIFAR-10 WRN-28-10 gowal & 698 & 1895 & 1456 \\ CIFAR-10 WRN-70-16 & 3486 & 10330 & 7258 \\ ImageNet ResNet50 & 15656 & 41689 & 35038 \\ ImageNet Transformer & 38003 & 101646 & 81279 \\ \hline \end{tabular}
\end{table}
Table 7: Analysis of time per epoch for RAMP and related baselines. RAMP is more expensive than E-AT and less expensive than MAX.

2021] since it uses a Wide-ResNet architecture. Also, we select the strongest baselines (E-AT and MAX) from the Wide-ResNet experiment results to compare for ResNet-18 experiments on universal robustness.

**Results.** For the ResNet-18 training from scratch experiment on CIFAR-10, in Table 8 and 9, we also show **RAMP** generally outperforms by \(0.5\%\) on common corruptions and \(7\%\) on union accuracy against unseen adversaries compared with E-AT.

### Additional Experiments with Different Epsilon Values

In this section, we provide additional results with different \(\epsilon_{1},\epsilon_{2},\epsilon_{\infty}\) values. We select \(\epsilon_{\infty}=[\frac{2}{255},\frac{4}{255},\frac{12}{255},\frac{16}{255}]\), \(\epsilon_{1}=[6,9,12,15]\), and \(\epsilon_{2}=[0.25,0.75,1.0,1.5]\). We provide additional **RAMP** results compared with related baselines with training from scratch and performing robust fine-tuning in Section B.4.1 and Section B.4.2, respectively. We observe that **RAMP** can surpass E-AT with significant margins as well as a better accuracy-robustness tradeoff for both training from scratch and robust fine-tuning with \(\lambda=2.0\) for training from scratch and \(\lambda=0.5\) for robust fine-tuning in most cases.

#### b.4.1 Additional Results with Training from Scratch

**Changing \(l_{\infty}\) perturbations with \(\epsilon_{\infty}=[\frac{2}{255},\frac{4}{255},\frac{12}{255},\frac{16}{255}]\).** Table 10 and Table 11 show that **RAMP** consistently outperforms E-AT [Croce and Hein, 2022] on union accuracy when training from scratch.

**Changing \(l_{1}\) perturbations with \(\epsilon_{1}=[6,9,12,15]\).** Table 12 and Table 13 show that **RAMP** consistently outperforms E-AT [Croce and Hein, 2022] on union accuracy when training from scratch.

\begin{table}
\begin{tabular}{l c} Models & common corruptions \\ \hline E-AT & 73.8 \\ MAX & 75.1 \\
**RAMP** & 74.3 \\ \hline \end{tabular}
\end{table}
Table 8: Accuracy against common corruptions using ResNet-18 on CIFAR-10 dataset.

\begin{table}
\begin{tabular}{l c c c c c c c c c c}  & Clean & \(l_{\infty}\) & \(l_{2}\) & \(l_{1}\) & Union & Clean & \(l_{\infty}\) & \(l_{2}\) & \(l_{1}\) & Union \\ \hline E-AT & 87.2 & 73.3 & 64.1 & 55.4 & 55.4 & E-AT & 86.8 & 58.9 & 66.4 & 54.6 & 53.7 \\
**RAMP** & 86.3 & 73.3 & 64.9 & 59.1 & **59.1** & **RAMP** & 86.1 & 60.0 & 67.4 & 58.5 & **57.4** \\ \hline \end{tabular}
\end{table}
Table 10: \((\epsilon_{\infty}=\frac{\mathbf{2}}{\mathbf{255}},\epsilon_{1}=12,\epsilon_{2}=0.5)\) and \((\epsilon_{\infty}=\frac{\mathbf{4}}{\mathbf{255}},\epsilon_{1}=12,\epsilon_{2}= 0.5)\) with random initializations.

#### b.4.2 Additional Results with Robust Fine-tuning

**Changing \(l_{\infty}\) perturbations with \(\epsilon_{\infty}=[\frac{2}{255},\frac{4}{255},\frac{12}{255},\frac{16}{255}]\).** Table 16 and Table 17 show that **RAMP** consistently outperforms E-AT [13] on union accuracy when performing robust fine-tuning.

**Changing \(l_{1}\) perturbations with \(\epsilon_{1}=[6,9,12,15]\).** Table 12 and Table 13 show that **RAMP** consistently outperforms E-AT [13] on union accuracy when performing robust fine-tuning.

### Different Logit Pairing Methods

In this section, we test **RAMP** with robust fine-tuning using two more different logit pairing losses: (1) Mean Squared Error Loss (\(\mathcal{L}_{mse}\)) (Eq. 17), (2) Cosine-Similarity Loss (\(\mathcal{L}_{cos}\)) (Eq. 18). We replace the KL loss we used in the paper using the following losses. We use the same lambda value \(\lambda=1.5\) for both cases.

\[\mathcal{L}_{mse}=\frac{1}{n_{c}}\cdot\sum_{i=0}^{n_{c}}\frac{1}{2}\left(p_{q} [\gamma[i]]-p_{r}[\gamma[i]]\right)^{2}\] (17)

\[\mathcal{L}_{cos}=\frac{1}{n_{c}}\cdot\sum_{i=0}^{n_{c}}\left(1-\cos(p_{q}[ \gamma[i]],p_{r}[\gamma[i]])\right)\] (18)

Table 22 displays **RAMP** robust fine-tuning results of different logit pairing losses using PreAct-ResNet-18 on CIFAR-10 with \(\lambda=1.5\). We see those losses generally improve union accuracy compared with baselines in Table 24. \(\mathcal{L}_{cos}\) has a better clean accuracy yet slightly worsened union accuracy. \(\mathcal{L}_{mse}\) has the best union accuracy and the worst clean accuracy. \(\mathcal{L}_{KL}\) is in the middle of the two others. However, we acknowledge the possibility that each logit pairing loss may have its own best-tuned \(\lambda\) value.

\begin{table}
\begin{tabular}{l c c c c c c c c c} Losses & Clean & \(l_{\infty}\) & \(l_{2}\) & \(l_{1}\) & Union & Clean & \(l_{\infty}\) & \(l_{2}\) & \(l_{1}\) & Union \\ \hline KL & 80.9 & **45.5** & 66.2 & 47.3 & 43.1 \\ MSE & 80.4 & **45.6** & 65.8 & **47.6** & **43.5** \\ Cosine & **81.6** & 45.4 & **66.7** & 47.0 & 42.9 \\ \hline \end{tabular}
\end{table}
Table 22: **RAMP** fine-tuning results of different logit pairing losses using PreAct-ResNet-18 on CIFAR-10.

### AT from Scratch Using WideResNet-28-10

**Implementations.** We use a cyclic learning rate with a maximum rate of \(0.1\) for \(30\) epochs and adopt the outer minimization trades loss from Zhang et al. (2019) with the default hyperparameters, same as Croce and Hein (2022); also, we set \(\lambda=2.0\) and \(\beta=0.5\) for training **RAMP**. Additionally, we use the WideResNet-28-10 architecture same as Zagoruyko and Komodakis (2016) for our reimplementations on CIFAR-10.

**Results.** Since the implementation of experiments on WideResNet-28-10 in Croce and Hein (2022) paper is not public at present, we report our implementation results on E-AT, where our results show that **RAMP** outperforms E-AT in union accuracy with a significant margin, as shown in Table 23. Also, we experiment with using the trade loss (**RAMP w trades**) for the outer minimization, we observe that **RAMP w trades** achieves a better union accuracy at the loss of some clean accuracy.

### Robust Fine-tuning Using PreAct-ResNet-18

**Implementations.** For robust fine-tuning with ResNet-18, we perform \(3\) epochs on CIFAR-10. We set the learning rate as \(0.05\) for PreAct-ResNet-18 and \(0.01\) for other models. We set \(\lambda=0.5\) in this case. Also, we reduce the learning rate by a factor of \(10\) after completing each epoch.

**Result.** Table 24 shows the robust fine-tuning results using PreAct ResNet-18 model on the CIFAR-10 dataset with different methods. The results for all baselines are directly from the E-AT paper (Croce and Hein, 2022) where the authors reimplemented other baselines (e.g., MSD, MAX) to achieve better union accuracy than presented in the original works. **RAMP** surpasses all other methods on union accuracy.

### Robust Fine-tuning with More Epochs

In Table 25, we apply robust fine-tuning on the PreAct ResNet-18 model for the CIFAR-10 dataset with \(5,7,10,15\) epochs, and compare it with E-AT. **RAMP** consistently outperforms the baseline on union accuracy, with a larger improvement when we increase the number of epochs.

## Appendix C Additional Visualization Results

In this section, we provide additional t-SNE visualizations of the multiple-norm tradeoff and robust fine-tuning procedures using different methods.

\begin{table}
\begin{tabular}{l c c c c c} Methods & Clean & \(l_{\infty}\) & \(l_{2}\) & \(l_{1}\) & Union \\ \hline E-AT w trades (reported in Croce and Hein (2022)) & 79.9 & 46.6 & 66.2 & 56.0 & 46.4 \\ E-AT w trades (ours) & 79.2 & 44.2 & 64.9 & 54.9 & 44.0 \\
**RAMP w/o trades** (ours) & 81.1 & 46.6 & 65.9 & 48.1 & 44.6 \\
**RAMP w trades** (ours) & 79.9 & 47.1 & 65.1 & 49.0 & **45.8** \\ \hline \end{tabular}
\end{table}
Table 23: **WideResNet-28-10 trained from random initialization** on CIFAR-10. **RAMP** outperforms E-AT on union accuracy with our implementation.

\begin{table}
\begin{tabular}{l c c c c c} Methods & Clean & \(l_{\infty}\) & \(l_{2}\) & \(l_{1}\) & Union \\ \hline RN-18-\(l_{\infty}\)-AT & \(83.7\) & \(48.1\) & \(59.8\) & \(7.7\) & \(38.5\) \\ + SAT & \(83.5\pm 0.2\) & \(43.5\pm 0.2\) & \(68.0\pm 0.4\) & \(47.4\pm 0.5\) & \(41.0\pm 0.3\) \\ + AVG & \(84.2\pm 0.4\) & \(43.3\pm 0.4\) & \(68.4\pm 0.6\) & \(46.9\pm 0.6\) & \(40.6\pm 0.4\) \\ + MAX & \(82.2\pm 0.3\) & \(45.2\pm 0.4\) & \(67.0\pm 0.7\) & \(46.1\pm 0.4\) & \(42.2\pm 0.6\) \\ + MSD & \(82.2\pm 0.4\) & \(44.9\pm 0.3\) & \(67.1\pm 0.6\) & \(47.2\pm 0.6\) & \(42.6\pm 0.2\) \\ + E-AT & \(82.7\pm 0.4\) & \(44.3\pm 0.6\) & \(68.1\pm 0.5\) & \(48.7\pm 0.5\) & \(42.2\pm 0.8\) \\ **+ RAMP** (\(\lambda\)=1.5) & \(81.1\pm 0.2\) & \(45.4\pm 0.3\) & \(66.1\pm 0.2\) & \(47.2\pm 0.1\) & \(\mathbf{43.1\pm 0.2}\) \\ **+ RAMP** (\(\lambda\) = 0.5) & \(81.5\pm 0.1\) & \(45.5\pm 0.2\) & \(66.4\pm 0.2\) & \(47.0\pm 0.1\) & \(\mathbf{42.9\pm 0.2}\) \\ \hline \end{tabular}
\end{table}
Table 24: **RN-18 \(l_{\infty}\)-AT model fine-tuned** for 3 epochs (repeated for 5 seeds). **RAMP** has the highest union accuracy. Baseline results are from Croce and Hein (2022).

[MISSING_PAGE_FAIL:25]

Figure 8: **Finetune RN18 \(l_{\infty}\)-AT model with E-AT for 3 epochs**. Each row represents the prediction results of epoch \(0,1,2,3\) respectively.

Figure 7: **Finetune RN18 \(l_{\infty}\)-AT model on \(l_{1}\) examples for 3 epochs**. Each row represents the prediction results of epoch \(0,1,2,3\) respectively.

Figure 9: **Finetune RN18 \(l_{\infty}\)-AT model with RAMP for 3 epochs**. Each row represents the prediction results of epoch \(0,1,2,3\) respectively.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction present the claims made in the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of our work in the discussion section of the paper.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We state the assumptions along with theoretical results in the theory part of the paper. The proofs are in the supplemental material.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide the implementations of our experiments in both main paper and supplementary materials.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide scripts to reproduce all experimental results for the new proposed method.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The full details are provided with the code and in appendix.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The results are accompanied by confidence intervals.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide the information on the computer resources in the discussion section of the paper.

9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We conduct the research with the NeurIPS Code of Ethics.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We mention the societal impacts of the work in the discussion section of the paper.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite the original paper that produced the code package or dataset.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide README and scripts to run for the new asset we introduce. They are well documented.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not include crowdsourcing or research involving human subjects.