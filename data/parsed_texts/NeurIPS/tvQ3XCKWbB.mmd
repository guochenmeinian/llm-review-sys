# Enriching Disentanglement:

From Logical Definitions to Quantitative Metrics

 Yivan Zhang

The University of Tokyo, RIKEN AIP

Tokyo, Japan

yivanzhang@ms.k.u-tokyo.ac.jp&Masashi Sugiyama

RIKEN AIP, The University of Tokyo

Tokyo, Japan

sugi@k.u-tokyo.ac.jp

###### Abstract

Disentangling the explanatory factors in complex data is a promising approach for generalizable and data-efficient representation learning. While a variety of quantitative metrics for learning and evaluating disentangled representations have been proposed, it remains unclear what properties these metrics truly quantify. In this work, we establish algebraic relationships between logical definitions and quantitative metrics to derive theoretically grounded disentanglement metrics. Concretely, we introduce a compositional approach for converting a higher-order predicate into a real-valued quantity by replacing (i) equality with a strict premetric, (ii) the Heyting algebra of binary truth values with a quantale of continuous values, and (iii) quantifiers with aggregators. The metrics induced by logical definitions have strong theoretical guarantees, and some of them are easily differentiable and can be used as learning objectives directly. Finally, we empirically demonstrate the effectiveness of the proposed metrics by isolating different aspects of disentangled representations.

## 1 Introduction

In _supervised learning_, we usually use a real-valued cost function \(\ell:Y\times Y\rightarrow\mathbb{R}_{\geq 0}\) to measure how close an output \(f(x)\) of a function \(f:X\to Y\) is to a target label \(y\), i.e., \(\ell(f(x),\tilde{y})\), to quantify the cost of inaccurate prediction. Then, we can use the _total cost_ over a collection of input-output pairs to measure the performance of this function. From a functional perspective, this construction induces a quantitative metric \(L:[X,Y]\times[X,Y]\rightarrow\mathbb{R}_{\geq 0}\) between functions:1

Footnote 1: \([X,Y]\) denotes the set of all functions from a set \(X\) to a set \(Y\).

\[L(f,g):=\sum_{x\in X}\ell(f(x),g(x)),\] (1)

where \(g:X\to Y\) is a "_ground-truth function_" that maps each input \(x\) to its target label \(y\). This metric can be used as both _learning objective_ and _evaluation metric_ for the learning model \(f:X\to Y\). What does \(L(f,g)\) quantify? It quantifies the extent to which two functions \(f\) and \(g\) are _equal_:

\[(f=_{[X,Y]}g):=\forall x\in X.\ f(x)=_{Y}g(x).\lx@note{footnote}{$\{\top,\bot\}$ denotes the set of binary truth values: _true_}\top\text{ and }\mathit{false}\bot.\] (2)

Considering the equality as a predicate, we can observe a parallel between

* (binary-valued) equality \(=_{Y}\): \(Y\times Y\rightarrow\{\top,\bot\}\) and (real-valued) cost \(\ell:Y\times Y\rightarrow\mathbb{R}_{\geq 0}\),3

Footnote 3: \(\{\top,\bot\}\) denotes the set of binary truth values: _true_\(\top\) and _false_\(\bot\).
* universal quantifier ("_for all_") \(\forall x\in X\) and summation \(\sum_{x\in X}\), and
* function equality \(=_{[X,Y]}\): \([X,Y]\times[X,Y]\rightarrow\{\top,\bot\}\) and total cost \(L:[X,Y]\times[X,Y]\rightarrow\mathbb{R}_{\geq 0}\).

We would like to ask: _Is it possible to measure and optimize other properties in the same way?_In _representation learning_(Bengio et al., 2013), measuring and optimizing the performance of a learning model becomes a non-trivial task. The quality of a model cannot always be measured by how close it is to a fixed ground truth. Instead, we often need to consider the properties of the model architecture or learned representation itself, such as _convexity_(Amos et al., 2017), _uniformity_(Wang and Isola, 2020), _invariance_(Kvinge et al., 2022), and _equivariance_(Lee et al., 2019; Brehmer et al., 2023). A proper comprehension of what constitutes good representations and how to assess their quality is important for designing suitable models, learning objectives, and evaluation metrics.

Disentangled representation learning: definitions, metrics, and methods_Disentanglement_ is an important property in representation learning, which intuitively means that different explanatory factors in data should be encoded separately (Bengio et al., 2013). However, disentanglement has no universally agreed-upon formal definition (Higgins et al., 2018; Suter et al., 2019; Shu et al., 2020; Fumero et al., 2021), and it is typically viewed not as a single property but rather as a combination of several requirements (Ridgeway and Mozer, 2018; Eastwood and Williams, 2018; Do and Tran, 2020; Tokui and Sato, 2022). While many metrics for measuring disentanglement have been proposed (Carbonneau et al., 2022), it remains unclear what properties these metrics truly quantify and how they can be optimized directly. Often, a new evaluation metric is introduced along with a new learning method, but it is usually unproven that the method can optimize the new metric (Higgins et al., 2017; Kim and Mnih, 2018; Chen et al., 2018; Li et al., 2020). This lack of theoretical understanding makes it difficult to design learning models that can effectively learn disentangled representations.

A logical and algebraic approach to defining and measuring disentangled representationsRecently, Zhang and Sugiyama (2023) proposed a general and abstract definition of disentanglement, shedding light on the common structures underlying the algebraic, statistical, and topological definitions of disentanglement. It was shown that the abstract concept of _product_(Mac Lane, 1978) underlies an essential property of disentanglement called _modularity_(Ridgeway and Mozer, 2018), and other properties of learning models, such as _informativeness_(Eastwood and Williams, 2018), can also be defined abstractly using only the composition and identity of morphisms. Following this algebraic approach, we aim to derive theoretically grounded quantitative metrics of disentanglement from the logical definitions of the desired properties, extending the parallel between Eqs. (1) and (2).

ContributionsIn this paper, we focus on logically defined properties of disentangled representation learning, such as modularity and informativeness (Section 2). We introduce a compositional approach to converting a _higher-order equational predicate_ into a _real-valued quantity_ (Table 1), which serves as a quantitative metric of the extent to which a function satisfies the predicate (Section 3). Our analysis on the relationship between the logical definitions and the induced quantitative metrics provides theoretical guarantee on the properties of the optimal functions (Theorem 1). Then, we demonstrate the usefulness of this conversion method by deriving quantitative metrics for measuring properties of disentangled representations, and we analyze these metrics in terms of computation, optimization, and differentiability (Section 4). Lastly, we compare the derived metrics with several existing ones in a fully controlled experiment and demonstrate that the proposed metrics are able to isolate different aspects of disentangled representations (Section 5).

## 2 Logical definitions of disentangled representations

In this section, let us first take a closer look at the logical definitions of two properties of disentangled representation learning -- informativeness (Eastwood and Williams, 2018) and modularity (Ridgeway and Mozer, 2018), which are arguably more important than other properties (Carbonneau et al., 2022). We limit our discussion to sets and functions, but the generalization to other morphisms, such as equivariant, stochastic, or continuous functions, is straightforward.

### Informativeness: injectivity or retractability of a learning model

Being informative, expressive, faithful, or useful is a basic requirement for learned representations (Bengio et al., 2013). We want a representation learning model to preserve explanatory factors in data that are informative to the downstream tasks. For functions, this criterion could be formulated as follows: If two factors \(y\) and \(y^{\prime}\) are different, then their representations \(m(y)\) and \(m(y^{\prime})\) extracted by a function \(m:Y\to Z\) should be different too. This means that the function \(m\) should be _injective_:

**Definition 1** (Injective function).: A function \(m:Y\to Z\) is _injective_ if

\[p_{\text{injective}}(m:Y\to Z):=\forall y\in Y.\;\forall y^{\prime}\in Y.\;(m(y)=_ {Z}m(y^{\prime}))\rightarrow(y=_{Y}y^{\prime}).\] (3)

Alternatively, because injective functions are precisely functions with _retractions_ (left inverses) [Lawvere and Rosebrough, 2003, Chapter 2], we can measure the _retractability_ instead:

**Definition 2** (Retractable function).: A function \(m:Y\to Z\) has a _retraction_\(h:Z\to Y\) if

\[p_{\text{retractable}}(m:Y\to Z):=\exists h:Z\to Y.\;h\circ m=_{[Y,Y]} \operatorname{id}_{Y}.\] (4)

Note that these properties are _predicates_\(p_{\text{injective}},p_{\text{retractable}}:[Y,Z]\rightarrow\{\top,\bot\}\) on the set \([Y,Z]\) of all functions from \(Y\) to \(Z\). Analogous to using the total cost in Eq. (1) to measure function equality in Eq. (2), if we want to measure the _injectivity_ in Eq. (3) or _retractability_ in Eq. (4), we need to find quantitative counterparts of the **implication**\(\rightarrow\), **universal quantifier**\(\forall\), and **existential quantifier**\(\exists\) used in their logical definitions. Generally, it is desirable to extend the parallel between Eqs. (1) and (2) to other predicates by finding quantitative operations corresponding to logical connectives and quantifiers. This correspondence allows us to construct and analyze quantitative metrics for machine learning models in a _compositional_ manner [Boole, 1854].

### Modularity: product structure preserved by a learning model

_Modularity_[Ridgeway and Mozer, 2018] is an essential property of disentangled representation learning, which means that the explanatory factors in data, such as the color and shape of an object, are separated into independent components in the learned representation [Bengio et al., 2013].

As shown in Fig. 1, modularity can be defined as follows. We assume that data with multiple explanatory factors (e.g., color and shape) is generated via a function \(g:Y\to X\) from a product \(Y:=Y_{1}\times Y_{2}\) of _factors_. An encoder \(f:X\to Z\) is a function to a product \(Z:=Z_{1}\times Z_{2}\) of _codes_. Then, an encoder is said to be _modular_ if it can **reconstruct the product structure**, such that the composition \(m:=f\circ g:Y\to Z\) of the generator \(g\) and the encoder \(f\) is a product function.4

Footnote 4: For two functions \(f:A\to C\) and \(g:B\to D\), their _product_\(f\times g:A\times B\to C\times D\) applies these two functions “_in parallel_” by mapping a pair \((a,b)\) in \(A\times B\) to a pair \((f(a),g(b))\) in \(C\times D\).

Formally, being a product function is also a property that can be represented as a predicate:

**Definition 3** (Product function).: Let \(Y:=Y_{1}\times Y_{2}\) and \(Z:=Z_{1}\times Z_{2}\) be products of sets. A function \(m:Y\to Z\) is a _product function_ if

\[p_{\text{product}}(m:Y\to Z):=\exists m_{1,1}:Y_{1}\to Z_{1}.\;\exists m_{2,2 }:Y_{2}\to Z_{2}.\;m=_{[Y,Z]}m_{1,1}\times m_{2,2}.\] (5)

**Example 1**.: Let us compare the following two functions from \(Y:=\{0,1\}^{2}\) to \(Z:=\mathbb{R}^{2}\):

\[m:=\begin{cases}(0,0)\mapsto(1,2)\\ (0,1)\mapsto(3,4)\\ (1,0)\mapsto(5,6)\\ (1,1)\mapsto(7,8)\end{cases}\quad\quad\quad m^{\prime}:=\begin{cases}(0,0) \mapsto(a,c)\\ (0,1)\mapsto(a,d)\\ (1,0)\mapsto(b,c)\\ (1,1)\mapsto(b,d)\end{cases}=\underbrace{\begin{cases}0\mapsto a\\ 1\mapsto b\\ m_{1,1}\end{cases}\times\begin{cases}0\mapsto c\\ 1\mapsto d\\ m_{2,2}\end{cases}}_{\begin{subarray}{c}\\ 1\mapsto d\\ \end{cases}}\] (7)

where \(a\), \(b\), \(c\), and \(d\) are arbitrary real numbers. According to Definition 3, only \(m^{\prime}=m_{1,1}\times m_{2,2}\) is a product function, whose first/second output depends only on the first/second input.

In Example 1, although \(m\) is not a product function, we want to address the following questions:

* (Metric) Can we quantify the extent to which it resembles a product function?
* (Approximation) Can we find a product function that is closest to it?
* (Differentiability) Can we make it slightly closer to a product function?

Answers to these questions will be given in the following sections.

Figure 1: Disentangled representation learning

## 3 Enrichment: from logic to metric

In Appendices A and B, we describe in detail the theory of converting a higher-order predicate into a real-valued quantity. In this section, we only introduce the conversion procedure using concrete examples and present the theoretical results. A summary of the conversion is given in Table 1.

First of all, let us clarify the terms predicate and quantity. In the realm of classical logic, a _predicate_\(p:A\rightarrow\{\top,\bot\}\) on a set \(A\) is a function from the set \(A\) to the set \(\{\top,\bot\}\) of binary truth values. For example, the predicates \(p_{\text{injective}}\), \(p_{\text{retractable}}\), and \(p_{\text{product}}\) in Definitions 1 to 3 are functions from the set \([Y,Z]\) of functions to the set \(\{\top,\bot\}\). They are _logical definitions_ of some properties of functions. On the other hand, in this work, a _quantity_\(q:A\rightarrow[0,\infty]\) on a set \(A\) is defined as a function to the set \([0,\infty]\) of extended non-negative real numbers. The quantities associated with a predicate will serve as _quantitative metrics_ for the property defined by the predicate.

### From equality predicate to strict premetric

In this work, a predicate of central importance is the _equality predicate_\(=_{A}\): \(A\times A\rightarrow\{\top,\bot\}\)[Mazur, 2008]. A quantity associated with the equality predicate should be a strict premetric:

**Definition 4** (Strict premetric).: A _strict premetric_ on a set \(A\) is a function \(d_{A}:A\times A\rightarrow[0,\infty]\) that

\[\forall a\in A.\;\forall a^{\prime}\in A.\;(d_{A}(a,a^{\prime})=0)\leftrightarrow (a=_{A}a^{\prime}).\] (8)

### From logical operation to quantitative operation

Next, let us have a look at the logical connectives and quantifiers used in the definitions of properties. The product of sets and functions plays a significant role in this work. Two functions \(f,g:C\to A\times B\) to a product are equal if and only if all their component functions are equal:

\[(f=_{[C,A\times B]}g):=(f_{1}=_{[C,A]}g_{1})\wedge(f_{2}=_{[C,B]}g_{2}).\lx@note{ footnote}{For a function $f:C\to A\times B$ to a product $A\times B$, its component functions $f_{1}:C\to A:=p_{1}\circ f$ and $f_{2}:C\to A:=p_{2}\circ f$ are denoted by numeric subscripts, where $p_{1}:A\times B\to A:=(a,b)\mapsto a$ and $p_{2}:A\times B\to B:=(a,b)\mapsto b$ are projection functions}.\] (9)

Note that the **conjunction**\(\wedge:\{\top,\bot\}\times\{\top,\bot\}\rightarrow\{\top,\bot\}\), a logical connective, is used in Eq. (9). To obtain a corresponding quantity, we replace it with the _addition_\(+:[0,\infty]\times[0,\infty]\rightarrow[0,\infty]\):

\[d_{[C,A\times B]}(f,g):=d_{[C,A]}(f_{1},g_{1})+d_{[C,B]}(f_{2},g_{2}).\] (10)

The **universal quantifier** on a set \(A\) is a specific (second-order) predicate \(\forall_{A}:\{\top,\bot\}^{A}\rightarrow\{\top,\bot\}\) on the set \(\{\top,\bot\}^{A}\) of predicates. We can replace it with the _supremum_\(\sup:[0,\infty]^{A}\rightarrow[0,\infty]\). We can also choose a function from the (i) _maximum_, (ii) _sum_, (iii) _mean_, and (iv) _mean square_ when the set \(A\) is finite. More generally, we can replace it with a quantity \(\triangledown_{A}:[0,\infty]^{A}\rightarrow[0,\infty]\) on the set \([0,\infty]^{A}\) of quantities that satisfies some conditions, which we refer to as a (universal) _aggregator_. Intuitively, a universal aggregator should output \(0\) if and only if all inputs are \(0\). Therefore, the median, mode, and range are non-examples. Different choices of aggregators yield metrics with different characteristics in computation and optimization. For example, the function equality predicate

\[(f=_{[A,B]}g):=\forall a\in A.\;f(a)=_{B}g(a)\] (11)

converts to a quantity whose aggregator \(\triangledown\) is not limited to the sum (cf. Eqs. (1) and (2)):

\[d_{[A,B]}(f,g):=\underset{a\in A}{\triangledown}d_{B}(f(a),g(a)).\] (12)

Dually, we also need to consider the **disjunction**\(\vee\) and the **existential quantifier**\(\exists\). We replace them with the _minimum_ and the _infimum_, respectively. Lastly, we replace the **implication**\(a\to b\) with the (truncated) _subtraction_\(b\mathrel{\raise 0.43pt\hbox{$\rightharpoonup$}}a:=\max\{b-a,0\}\). These operations are illustrated in Fig. 2.

\begin{table}
\begin{tabular}{l c l c} \hline \multicolumn{2}{c}{Logic} & \multicolumn{2}{c}{Metric} \\ \hline truth values & \(\{\top,\bot\}\) & real values & \([0,\infty]\) \\ equality & \(=\) & strict premetric & \(d\) \\ conjunction & \(\wedge\) & addition & \(+\) \\ disjunction & \(\vee\) & minimum & \(\min\) \\ implication & \(\rightarrow\) & subtraction\({}^{*}\) & \(\bot\) \\ universal quantifier & \(\forall\) & aggregator\({}^{**}\) & \(\triangledown\) \\ existential quantifier & \(\exists\) & infimum & \(\inf\) \\ \hline \end{tabular} \({}^{*}\) truncated subtraction: \(b\mathrel{\raise 0.43pt\hbox{$\rightharpoonup$}}a:=\max\{b-a,0\}\)

\({}^{**}\) e.g., maximum, sum, mean, and mean square

\end{table}
Table 1: From logic to metric

### From compound predicate to compound quantity

Following Table 1, we can convert any _compound predicate_ defined using equational predicates and logical operations into a corresponding _compound quantity_ defined using strict premetrics and quantitative operations. Our main result on their relationship is as follows:

**Theorem 1**.: _Let \(p:A\rightarrow\{\top,\bot\}\) be a predicate on a set \(A\), and let \(q:A\rightarrow[0,\infty]\) be a quantity converted from \(p\) according to Table 1. Then, for any \(a\in A\), \(q(a)=0\) implies \(p(a)=\top\). Conversely, for any \(a\in A\), \(p(a)=\top\) implies \(q(a)=0\) if and only if \(p\) does not contain the implication._

The implication is special because we must sacrifice logical equivalence for the sake of continuity, which is necessary for gradient-based optimization. We will explore this through a concrete example regarding injectivity in Section 4.3 and discuss it in detail in Appendices B and D.

### (Sub)homomorphism from metric to logic

Finally, for readers interested in the theoretical background, we briefly introduce the following algebraic concepts and a proof sketch underlying Table 1 and Theorem 1.

**Definition 5** (Zero predicate).: The _zero predicate_\(\zeta:[0,\infty]\rightarrow\{\top,\bot\}:=x\mapsto(x=0)\) is a function that maps \(0\) to true \(\top\) and any positive value to false \(\bot\).

**Definition 6** ((Sub)homomorphism from a quantity to a predicate).: Let \(A\) be a set. A quantity \(q:A\rightarrow[0,\infty]\) on the set \(A\) is _homomorphic_ to a predicate \(p:A\rightarrow\{\top,\bot\}\) via the zero predicate \(\zeta:[0,\infty]\rightarrow\{\top,\bot\}\) if \(\zeta\circ q=p\), and is _subhomomorphic_ to \(p\) if \(\zeta\circ q\to p\).6

Footnote 6: We use the infix notation, so \(\zeta\circ q=p\) means that \(\forall a\in A\). \((q(a)=0)\leftrightarrow p(a)\), and \(\zeta\circ q\to p\) means that \(\forall a\in A\). \((q(a)=0)\to p(a)\).

**Definition 7** ((Sub)homomorphism from a quantitative operation to a logical operation).: Let \(n\in\mathbb{N}\) be a natural number. An \(n\)-ary quantitative operation \(\alpha:[0,\infty]^{n}\rightarrow[0,\infty]\) is _homomorphic_ to a logical operation \(\beta:\{\top,\bot\}^{n}\rightarrow\{\top,\bot\}\) via the zero predicate \(\zeta:[0,\infty]\rightarrow\{\top,\bot\}\) if \(\zeta\circ\alpha=\beta\circ\zeta^{n}\), and is _subhomomorphic_ to \(\beta\) if \(\zeta\circ\alpha\rightarrow\beta\circ\zeta^{n}\).7

Footnote 7: \(\zeta^{n}:[0,\infty]^{n}\rightarrow\{\top,\bot\}^{n}:=(q_{1},\ldots,q_{n}) \mapsto(q_{1}=0,\ldots,q_{n}=0)\) is the \(n\)-fold _product_ of the zero predicate \(\zeta:[0,\infty]\rightarrow\{\top,\bot\}\).

**Definition 8** ((Sub)homomorphism from an aggregator to a quantifier).: Let \(A\) be a set. An aggregator \(\alpha_{A}:[0,\infty]^{A}\rightarrow[0,\infty]\) on the set \(A\) is _homomorphic_ to a quantifier \(\beta_{A}:\{\top,\bot\}^{A}\rightarrow\{\top,\bot\}\) via the zero predicate \(\zeta:[0,\infty]\rightarrow\{\top,\bot\}\) if \(\zeta\circ\alpha_{A}=\beta_{A}\circ\zeta^{A}\), and is _subhomomorphic_ to \(\beta_{A}\) if \(\zeta\circ\alpha_{A}\rightarrow\beta_{A}\circ\zeta^{A}\).8

Footnote 8: \(\zeta^{A}:[0,\infty]^{A}\rightarrow\{\top,\bot\}^{A}:=\zeta\circ(-)\) is the _postcomposition_ with the zero predicate \(\zeta:[0,\infty]\rightarrow\{\top,\bot\}\) that maps a quantity \(q:A\rightarrow[0,\infty]\) to the predicate \(\zeta\circ q:A\rightarrow\{\top,\bot\}\).

Homomorphic quantities, quantitative operations, and aggregators can be illustrated as follows:

(13)

Figure 2: From predicates and logical operations to quantities and quantitative operationsBased on these algebraic concepts, we can say that strict premetrics are homomorphic to equality predicates, addition is homomorphic to conjunction (since the sum is zero if and only if both addends are zero), minimum is homomorphic to disjunction, truncated subtraction is _subhomomorphic_ to implication, and universal aggregators are homomorphic to the universal quantifier.

Theorem 1 means that any compound quantity is (sub)homomorphic to the corresponding compound predicate if each component (quantities, quantitative operations, aggregators) is (sub)homomorphic to the corresponding component (predicates, logical operations, quantifiers). For implication, we use the truncated subtraction, which is only subhomomorphic, since there is no _continuous_ operation that is homomorphic to implication (see also Appendix D).

More abstractly and concisely, we can say that we replace the **Heyting algebra** of truth values \(\{\top,\bot\}\) with a _quantale_ of extended non-negative real numbers \([0,\infty]\), and we replace the quantifiers \(\forall\) and \(\exists\) with aggregators \(\triangledown\) and \(\inf\) (see also Appendix B). In this way, we can derive quantitative metrics for any logically defined properties of learning models _compositionally_.

## 4 Quantitative metrics of disentangled representations

In this section, we demonstrate how to apply the conversion method introduced above to derive quantitative metrics for measuring the modularity (Definition 3) and informativeness (Definitions 1 and 2) of disentangled representations.

In Sections 4.1 and 4.2, we introduce modularity metrics based on two approaches and discuss their differences in terms of computation and optimization. We point out that the main obstacle lies in the optimization step, resulting from the existential quantifiers in the definition. Then, we show that we can derive easily computable and differentiable metrics from a logically equivalent definition. In Section 4.3, we introduce informativeness metrics and present a result of Theorem 1.

### Modularity metrics via product approximation

We begin with _modularity_, which is an essential property of disentangled representation learning. Recall that modularity can be defined using the _product function_ (Definition 3). For easier reference, we provide the following diagram, which shows the domains and codomains of the functions involved in the upcoming discussion:

(14)

From Definition 3, we can derive the following metric:

**Definition 9** (Product approximation).: Let \(m:Y\to Z\) be a function from a product \(Y:=Y_{1}\times Y_{2}\) of sets to another product \(Z:=Z_{1}\times Z_{2}\) of sets. The extent to which \(m\) resembles a _product function_ can be measured by a distance between \(m\) and its best product function approximation:

\[q_{\text{product}}(m:Y\to Z):=\inf_{m_{1,1}\in[Y_{1},Z_{1}]}\inf_{m_{2,2}\in[Y _{2},Z_{2}]}d_{[Y,Z]}(m,m_{1,1}\times m_{2,2}).\] (15)

The derivation of \(q_{\text{product}}\) from \(p_{\text{product}}\) follows the conversion described in Table 1: replacing the equality \(=_{[Y,Z]}:[Y,Z]\times[Y,Z]\rightarrow\{\top,\bot\}\) with a strict premetric \(d_{[Y,Z]}:[Y,Z]\times[Y,Z]\rightarrow[0,\infty]\) and the existential quantifiers \(\exists\) with the infimum operators \(\inf\).

This modularity metric can be interpreted as a distance from a point \(m\in[Y,Z]\) to a subset \(\{m_{1,1}\times m_{2,2}\mid m_{1,1}\in[Y_{1},Z_{1}],m_{2,2}\in[Y_{2},Z_{2}]\} \subset[Y,Z]\) of product functions (cf. the Hausdorff distance (Lawvere, 1986; Tuzhilin, 2016)). Following from Theorem 1, \(q_{\text{product}}(m)=0\) if and only if \(p_{\text{product}}(m)=\top\). This means that the minimizers of this metric are precisely product functions.

However, we still face two obstacles: the product operation and the minimization problem. For the product operation, we can employ Eqs. (10) and (12) to rewrite \(q_{\text{product}}\) into a more computable form:

**Proposition 2**.: _The quantity \(q_{\text{product}}(m:Y\to Z)\) equals_

\[\mathop{\bigtriangledown}\limits_{y_{1}\in Y_{1}}\mathop{\bigtriangledown} \limits_{y_{2}\in Y_{2}}d_{Z_{1}}(m_{1}(y_{1},y_{2}),m_{1,1}^{*}(y_{1}))+ \mathop{\bigtriangledown}\limits_{y_{2}\in Y_{2}}\mathop{\bigtriangledown} \limits_{y_{1}\in Y_{1}}d_{Z_{2}}(m_{2}(y_{1},y_{2}),m_{2,2}^{*}(y_{2})),\] (16)

_where the functions \(m_{1,1}^{*}:Y_{1}\to Z_{1}\) and \(m_{2,2}^{*}:Y_{2}\to Z_{2}\) are given by_

\[m_{1,1}^{*}:Y_{1}\to Z_{1} :=y_{1}\mapsto\mathop{\arg\inf}\limits_{z_{1}\in Z_{1}}\mathop{ \bigtriangledown}\limits_{y_{2}\in Y_{2}}d_{Z_{1}}(m_{1}(y_{1},y_{2}),z_{1}),\] (17) \[m_{2,2}^{*}:Y_{2}\to Z_{2} :=y_{2}\mapsto\mathop{\arg\inf}\limits_{z_{2}\in Z_{2}}\mathop{ \bigtriangledown}\limits_{y_{1}\in Y_{1}}d_{Z_{2}}(m_{2}(y_{1},y_{2}),z_{2}).\] (18)

A detailed derivation can be found in Appendix C. Note that we can obtain the optimal product function approximation \(m_{1,1}^{*}\times m_{2,2}^{*}\) explicitly via Eqs. (17) and (18). Intuitively, we need to find an approximation of a (multi)set of codes with one factor fixed and other factors varying, and then we use the aggregation of all the approximation errors as a modularity metric.

The second obstacle -- the minimization problem -- still needs to be addressed. Since the code spaces \(Z_{1}\) and \(Z_{2}\) can be infinite sets, the minimization problem may not have a closed-form minimizer or even an exact solver. Even if an exact solver exists, the solution may not be differentiable with respect to the inputs. Let us examine some concrete examples of \(q_{\text{product}}\) by choosing different aggregators \(\triangledown\) in Eqs. (17) and (18). In the following three examples, we assume that the code spaces \(Z_{1}\) and \(Z_{2}\) are Euclidean spaces equipped with the usual Euclidean distances.

**Example 2**.: If the aggregator \(\triangledown\) is the _supremum_, the best approximation is the _center_ of the smallest bounding sphere (Megiddo, 1983), and the approximation error is the _radius_.

This metric has the advantage of being definable even when the factor spaces \(Y_{1}\) and \(Y_{2}\) are infinite sets, and it can be computed using either randomized (Welzl, 1991) or exact (Fischer et al., 2003) algorithms. However, it is not easy to calculate its gradient. Thus, we cannot use it as a learning objective and directly optimize it using gradient-based optimization.

**Example 3**.: If the aggregator \(\triangledown\) is the _mean_, the best approximation is the _(geometric) median_(Weiszfeld, 1937), and the approximation error is the _mean absolute deviation around the median_.

It is known that there is no exact algorithm for obtaining the geometric median (Cockayne and Melzak, 1969), but it can be effectively approximated using convex optimization (Cohen et al., 2016). The geometric median has found applications in robust estimation in the fields of statistics and machine learning (Meer et al., 1991; Minsker, 2015; Pillutla et al., 2022; Guerraoui et al., 2023).

**Example 4**.: If the aggregator \(\triangledown\) is the _mean square_, the best approximation is the _mean_, and the approximation error is the _variance_. In this case, \(q_{\text{product}}(m)\) can be simplified to

\[\mathop{\mathrm{mean}}\limits_{y_{1}\in\mathcal{T}_{1}}\mathop{\mathrm{var} \limits_{y_{2}\in\mathcal{T}_{2}}}m_{1}(y_{1},y_{2})+\mathop{\mathrm{mean}} \limits_{y_{2}\in Y_{2}}\mathop{\mathrm{var}\limits_{y_{1}\in Y_{1}}}m_{2}( y_{1},y_{2}).\] (19)

The variance is easier to compute and differentiate than the radius of the smallest bounding sphere and the mean absolute deviation around the median, but it is also more susceptible to outliers and noise. Further work could explore the theoretical implications of these metrics, especially in cases where only partial combinations of factors or noisy annotations are available.

Then, let us revisit our motivating example in Example 1:

**Example 5**.: Let us consider the function \(m:\left\{0,1\right\}^{2}\to\mathbb{R}^{2}\) in Eq. (6). Its best product function approximation is

\[m^{*}:\left\{0,1\right\}^{2}\to\mathbb{R}^{2}:=\begin{cases}(0,0)\mapsto(2,4) \\ (0,1)\mapsto(2,6)\\ (1,0)\mapsto(6,4)\\ (1,1)\mapsto(6,6)\end{cases}=\underbrace{\begin{cases}0\mapsto 2&\text{$ 1\mapsto 6$}\\ \text{$ 1\mapsto 6$}\\ \text{$ 2$}\end{cases}}_{m_{1,1}^{*}}\times\underbrace{\begin{cases}0\mapsto 4 \\ \text{$ 1\mapsto 6$}\\ \text{$ 2$}\end{cases}}_{m_{2,2}}\] (20)

because \(m_{1,1}^{*}(0)=2\) is the center/median/mean of the set \(\left\{m_{1}(0,0)=1,m_{1}(0,1)=3\right\}\), and so on. The modularity metric is a distance between \(m\) and \(m^{*}\).

### Modularity metrics via constancy

Upon analyzing the metrics above, it becomes evident that what we need is not the best approximation itself (e.g., the mean) but rather the approximation error (e.g., the variance) -- a measure of the _constancy_ of a set of codes. Following this insight, our next objective is to formulate a modularity metric that eliminates the need for an optimization step. Zhang and Sugiyama (2023) have proved that a function is a product function if and only if the _curried functions9_ of its component functions are constant, as shown in the following example:

Footnote 9: For a binary function \(f:A\times B\to C\), its _curried function_\(\widehat{f}:A\to[B,C]\) is a unary function such that for all \(a\in A\) and \(b\in B\), \(f(a,b)=\widehat{f}(a)(b)\)Curry (1980).

**Example 6**.: Consider the functions \(m,m^{\prime}:\left\{0,1\right\}^{2}\to\mathbb{R}^{2}\) in Eqs. (6) and (7) and the curried functions of their second component functions \(m_{2},m_{2}^{\prime}:\left\{0,1\right\}^{2}\to\mathbb{R}\):

\[m_{2}=\begin{cases}\left(0,0\right)\mapsto 2\\ \left(0,1\right)\mapsto 4\\ \left(1,0\right)\mapsto 6\\ \left(1,1\right)\mapsto 8\end{cases}\cong\begin{cases}0\mapsto\begin{cases}0\mapsto 2 \\ 1\mapsto 4\\ \left(0,1\right)\mapsto d\\ 1\mapsto 8\end{cases}\end{cases}\quad m_{2}^{\prime}=\begin{cases}\left(0,0 \right)\mapsto c\\ \left(0,1\right)\mapsto d\\ \left(1,0\right)\mapsto c\\ \left(1,1\right)\mapsto d\end{cases}\cong\begin{cases}0\mapsto\begin{cases}0 \mapsto c\\ 1\mapsto d\\ 1\mapsto d\end{cases}\end{cases}\] (21)

The curried function \(\widehat{m_{2}}:\left\{0,1\right\}\to\left\{\left\{0,1\right\},\mathbb{R}\right\}\) is not constant, while \(\widehat{m_{2}^{\prime}}\) is constant with value \(\left\{0\mapsto c,1\mapsto d\right\}\in\left\{\left\{0,1\right\},\mathbb{R}\right\}\) (and so is \(\widehat{m_{1}^{\prime}}\)), indicating that \(m^{\prime}\) is a product function.

Based on this fact, we propose an alternative approach for measuring modularity:

**Definition 10** (Constancy of curried function).: Let \(m_{1}\) and \(m_{2}\) be the component functions of a function \(m:Y\to Z\) from a product \(Y:=Y_{1}\times Y_{2}\) of sets to another product \(Z:=Z_{1}\times Z_{2}\) of sets. The extent to which \(m\) resembles a product function can be measured by the _constancy_ of the curried functions of \(m_{1}\) and \(m_{2}\):

\[q_{\text{const-curly}}(m:Y\to Z):=q_{\text{const}}(\widehat{m_{1}})+q_{\text {const}}(\widehat{m_{2}}),\] (22)

where \(q_{\text{const}}\) is a quantity for constant functions.

To complete this construction, we adopt the following definition and metric of the constant function:

**Definition 11** (Constant function).: A function \(f:A\to B\) is _constant_ if

\[p_{\text{const}}(f:A\to B):=\forall a\in A.\;\forall a^{\prime}\in A.\;f(a)=_{ B}f(a^{\prime}),\] (23)

which can be measured by

\[q_{\text{const}}(f:A\to B):=\underset{a\in A}{\triangledown}\underset{a^{ \prime}\in A}{\triangledown}d_{B}(f(a),f(a^{\prime})).\] (24)

This constancy metric \(q_{\text{const}}\) only needs to compute pairwise distances between the outputs, requiring \(\left|A\right|^{2}\) times distance computation but no optimization. Incorporating \(q_{\text{const}}\) into \(q_{\text{const-curly}}\), we can get the following metric:

**Proposition 3**.: _The quantity \(q_{\text{const-curly}}(m:Y\to Z)\) equals_

\[\begin{split}\underset{y_{1}\in Y_{1}}{\triangledown}\underset{ y_{2}\in Y_{2}}{\triangledown}\underset{y_{2}^{\prime}\in Y_{2}}{\triangledown}d_{Z_{1}}(m_{1}(y_{1},y_{2}),m_{1}(y_{1},y_{2}^{ \prime}))\\ +\underset{y_{2}\in Y_{2}}{\triangledown}\underset{y_{1}\in Y_{1}}{ \triangledown}\underset{y_{1}^{\prime}\in Y_{1}}{\triangledown}d_{Z_{2}}(m_{ 2}(y_{1},y_{2}),m_{2}(y_{1}^{\prime},y_{2})).\end{split}\] (25)

Here are two examples of \(q_{\text{const}}\) and \(q_{\text{const-curly}}\) using different aggregators \(\triangledown\) in Eqs. (24) and (25).

**Example 7**.: If the aggregator \(\triangledown\) is the _maximum_, \(q_{\text{const}}\) is the _diameter_ (the maximum pairwise distance) of the outputs. In this case, \(q_{\text{const-curly}}(m)\) can be simplified to

\[\max_{y_{1}\in Y_{1}}\operatorname*{diam}_{y_{2}\in Y_{2}}m_{1}(y_{1},y_{2})+ \max_{y_{2}\in Y_{2}}\operatorname*{diam}_{y_{1}\in Y_{1}}m_{2}(y_{1},y_{2}).\] (26)

**Example 8**.: If the aggregator \(\triangledown\) is the _mean square_, \(q_{\text{const}}\) is the mean pairwise squared distance, which equals the _variance_. In this case, \(q_{\text{const-curly}}(m)\) coincides with Eq. (19).

In summary, Eqs. (19) and (26) are easily computable and differentiable metrics, and their minimizers are precisely product functions. They do not contain any hyperparameters or stochastic components and thus can serve as both learning objectives and evaluation metrics.

### Informativeness metrics

If an encoder \(f:X\to Z\) is constant, mapping everything to the same value, according to Definition 3, it is perfectly modular. However, a constant encoder is also completely useless. In this subsection, we shift our focus to the property of _informativeness_ -- a measurement of usefulness.

Informativeness is not a unique requirement for disentangled representations. Other representation learning paradigms, such as contrastive learning (Jaiswal et al., 2020; Wang and Isola, 2020) and metric learning (Musgrave et al., 2020), also emphasize the importance of mapping dissimilar data to far-apart locations in the representation space. While one could integrate this requirement into a single disentanglement score (e.g., (Higgins et al., 2017; Kim and Mnih, 2018)), we argue that it is better to evaluate the usefulness of representations separately for a more fine-grained assessment (Carbonneau et al., 2022).

One straightforward way to measure informativeness is to measure how much we can invert the encoding process:

**Definition 12** (Retraction approximation).: Let \(m:Y\to Z\) be a function. The extent to which \(m\) is _retractable_ can be measured by a distance between the composition of \(m\) and its best retraction approximation and the identity function:

\[q_{\text{retractable}}(m:Y\to Z):=\inf_{h\in[Z,Y]}d_{[Y,Y]}(h\circ m,\text{id} _{Y})=\inf_{h\in[Z,Y]}\underset{y\in Y}{\triangledown}d_{Y}(h(m(y)),y).\] (28)

This metric \(q_{\text{retractable}}\) is derived from Definition 2 following the conversion procedure in Table 1. This informativeness metric also involves an optimization step similar to the modularity metric \(q_{\text{product}}\), potentially introducing randomness or higher computation costs. Note that we may use a parameterized subset of the set \([Z,Y]\) of all functions from codes \(Z\) to factors \(Y\), such as the set of linear functions. Then, the problem becomes a regression/classification problem, and the metric is the performance of the predictor. A number of existing works adopted this approach and used the accuracy, the area under the ROC curve (AUC-ROC), or the mean squared error (MSE) to measure the informativeness (Ridgeway and Mozer, 2018; Eastwood and Williams, 2018; Eastwood et al., 2023). However, such metrics necessitate additional hyperparameter tuning and are more likely to exhibit varying behavior across different implementations (Carbonneau et al., 2022).

It raises the question of whether we can measure the informativeness of an encoder without approximating its retraction. We propose to measure informativeness by directly measuring the injectivity of the encoding process:

**Definition 13** (Contraction).: Let \(m:Y\to Z\) be a function. The extent to which \(m\) is _injective_ can be measured by how much \(m\) contracts pairs of inputs:

\[q_{\text{injective}}(m:Y\to Z):=\underset{y\in Y}{\triangledown}\underset{y^{ \prime}\in Y}{\triangledown}d_{Y}(y,y^{\prime})\overset{\cdot}{\cdot}d_{Z}(m(y ),m(y^{\prime})).\] (29)

This metric \(q_{\text{injective}}\) is derived from Definition 1 following the conversion procedure in Table 1. According to Theorem 1, we know that \(q_{\text{retractable}}(m)=0\) if and only if \(m\) is retractable. However, \(q_{\text{injective}}(m)=0\) implies the injectivity of \(m\) but not the other way around:

\[\begin{split}&(p_{\text{retractable}}(m)=\top)\underset{ \begin{subarray}{c}\text{logically}\\ \text{\tiny{Theorem 1}}\end{subarray}}{\triangleleft}(p_{\text{ injective}}(m)=\top)\\ &\text{\tiny{Theorem 1}}\end{split}\] (30)

In other words, a minimizer of \(q_{\text{injective}}\) is required to be _non-contractive_, which is a stronger condition than being injective. For example, let us consider the function \(m:[0,1]\to\mathbb{R}:=y\mapsto 0.01\times y\). Although it is injective, its outputs are less distinguishable from each other in terms of the Euclidean distance. Therefore, \(q_{\text{injective}}\) still assign a non-zero value to this function.

Although not all injective functions necessarily minimize \(q_{\text{injective}}\), according to Theorem 1, we can guarantee that minimizing \(q_{\text{injective}}\) will not lead to non-injective functions. Moreover, \(q_{\text{injective}}\) does not require training regressors or classifiers to approximate the retraction. Consequently, it does not need any time-consuming hyperparameter tuning or cross-validation like existing informativeness metrics (Eastwood and Williams, 2018; Ridgeway and Mozer, 2018).

[MISSING_PAGE_FAIL:10]

## Acknowledgments

We thank Paolo Perrone for generously sharing insights on Markov categories enriched in divergence spaces. We thank Ken Sakayori for reviewing an earlier version of Appendices A and B and providing constructive suggestions. We thank Zhiyuan Zhan for insightful discussions on metrics, topology, measures, and optimization of function properties, and for reviewing and proofreading some parts of Appendix D. We thank Masahiro Negishi for valuable discussions on disentanglement metrics and weakly supervised disentanglement. We thank Jingwen Fu for checking the algebraic concepts in Section 3. We also thank Johannes Ackermann, Xin-Qiang Cai, and Tongtong Fang for their valuable feedback on the manuscript.

YZ was supported by JSPS KAKENHI Grant Number 22KJ0880. MS was supported by JST CREST Grant Number JPMJCR18A2 and a grant from Apple, Inc. Any views, opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and should not be interpreted as reflecting the views, policies or position, either expressed or implied, of Apple Inc.

## References

* Adamek et al. (1990) Jiri Adamek, Horst Herrlich, and George Strecker. _Abstract and Concrete Categories: The Joy of Cats_. John Wiley and Sons, 1990. URL http://www.tac.mta.ca/tac/reprints/articles/17/tr17abs.html.
* Amer (1984) K Amer. Equationally complete classes of commutative monoids with monus. _Algebra Universalis_, 18:129-131, 1984. URL https://doi.org/10.1007/BF01182254.
* Amos et al. (2017) Brandon Amos, Lei Xu, and J Zico Kolter. Input convex neural networks. In _International Conference on Machine Learning_, 2017. URL http://proceedings.mlr.press/v70/amos17b.html.
* Awodey (2010) Steve Awodey. _Category Theory_. Oxford University Press, 2010. URL https://doi.org/10.1093/acprof:oso/9780198568612.001.0001.
* Bacci et al. (2023) Giorgio Bacci, Radu Mardare, Prakash Panangaden, and Gordon Plotkin. Propositional logics for the Lawvere quantale. _Electronic Notes in Theoretical Informatics and Computer Science_, 3, 2023. URL https://doi.org/10.46298/entics.12292. https://arxiv.org/abs/2302.01224.
* Bacci et al. (2024) Giorgio Bacci, Radu Mardare, Prakash Panangaden, and Gordon Plotkin. Polynomial Lawvere logic. _arXiv preprint_, 2024. URL https://arxiv.org/abs/2402.03543.
* Badreddine et al. (2022) Samy Badreddine, Artur d'Avila Garcez, Luciano Serafini, and Michael Spranger. Logic tensor networks. _Artificial Intelligence_, 303:103649, 2022. URL https://doi.org/10.1016/j.artint.2021.103649. https://arxiv.org/abs/2012.13635.
* Balabin et al. (2024) Nikita Balabin, Daria Voronkova, Ilya Trofimov, Evgeny Burnaev, and Serguei Barannikov. Disentanglement learning via topology. In _International Conference on Machine Learning_, 2024. D.
* Bao and Sugiyama (2020) Han Bao and Masashi Sugiyama. Calibrated surrogate maximization of linear-fractional utility in binary classification. In _International Conference on Artificial Intelligence and Statistics_, pages 2337-2347, 2020. URL http://proceedings.mlr.press/v108/bao20a.html.
* Bao et al. (2020) Han Bao, Clay Scott, and Masashi Sugiyama. Calibrated surrogate losses for adversarially robust classification. In _Conference on Learning Theory_, pages 408-451, 2020. URL http://proceedings.mlr.press/v125/bao20a.html.
* Barr and Wells (1990) Michael Barr and Charles Wells. _Category Theory for Computing Science_, volume 1. Prentice Hall New York, 1990. URL https://www.math.mcgill.ca/triples/Barr-Wells-ctc_s.pdf.
* Basu et al. (2002) Sugato Basu, Arindam Banerjee, and Raymond J Mooney. Semi-supervised clustering by seeding. In _International Conference on Machine Learning_, 2002. URL https://dl.acm.org/doi/10.5555/645531.656012.
* Bacci et al. (2020)Jens Behrmann, Will Grathwohl, Ricky TQ Chen, David Duvenaud, and Jorn-Henrik Jacobsen. Invertible residual networks. In _International Conference on Machine Learning_, 2019. URL https://proceedings.mlr.press/v97/behrmann19a.html. D.1
* Yaacov (2022) Itai Ben Yaacov. On the expressive power of quantifiers in continuous logic. _arXiv preprint_, 2022. URL https://arxiv.org/abs/2207.01863. D.2
* Yaacov and Usvyatsov (2010) Itai Ben Yaacov and Alexander Usvyatsov. Continuous first order logic and local stability. _Transactions of the American Mathematical Society_, 362(10):5213-5259, 2010. URL https://doi.org/10.1090/S0002-9947-10-04837-3. https://arxiv.org/abs/0801.4303. D.2
* Snacov et al. (2008) Itai Ben Yaacov, Alexander Berenstein, C. Ward Henson, and Alexander Usvyatsov. _Model Theory for Metric Structures_, page 315-427. London Mathematical Society Lecture Note Series. Cambridge University Press, 2008. URL https://doi.org/10.1017/CBO9780511735219.011. D.2
* Bengio et al. (2013) Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 35(8):1798-1828, 2013. URL https://doi.org/10.1109/TPAMI.2013.50. https://arxiv.org/abs/1206.5538.
* Bergmann (2008) Merrie Bergmann. _An Introduction to Many-Valued and Fuzzy Logic: Semantics, Algebras, and Derivation Systems_. Cambridge University Press, 2008. URL https://doi.org/10.1017/CBO9780511801129. B.1, D.2
* Bilenko et al. (2004) Mikhail Bilenko, Sugato Basu, and Raymond J Mooney. Integrating constraints and metric learning in semi-supervised clustering. In _International Conference on Machine Learning_, 2004. URL https://dl.acm.org/doi/10.1145/1015330.1015360. E.2
* Boole (1854) George Boole. _An Investigation of the Laws of Thought: On Which Are Founded the Mathematical Theories of Logic and Probabilities_. Cambridge University Press, 1854. URL https://doi.org/10.1017/CBO9780511693090.2.1, D.2
* Bradley et al. (2022) Tai-Danae Bradley, John Terilla, and Yiannis Vlassopoulos. An enriched category theory of language: From syntax to semantics. _La Matematica_, 1(2):551-580, 2022. URL https://doi.org/10.1007/s44007-022-00021-2. https://arxiv.org/abs/2106.07890. A.3
* Brehmer et al. (2022) Johann Brehmer, Pim De Haan, Phillip Lippe, and Taco Cohen. Weakly supervised causal representation learning. In _Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=dZ79MhQXWvg. D.2
* Brehmer et al. (2023) Johann Brehmer, Pim De Haan, Sonke Behrends, and Taco Cohen. Geometric algebra transformer. In _Neural Information Processing Systems_, 2023. URL https://openreview.net/forum?id=M7r2CO4tDC.
* Breiman et al. (1984) Leo Breiman, Jerome Friedman, Richard A. Olshen, and Charles J. Stone. _Classification and Regression Trees_. Routledge, 1984. URL https://doi.org/10.1201/9781315139470. D.1
* Burgess and Kim (2018) Chris Burgess and Hyunjik Kim. 3D shapes dataset, 2018. https://github.com/deepmin d/3d-shapes (Apache License 2.0).
* Capucci (2024) Matteo Capucci. On quantifiers for quantitative reasoning. _arXiv preprint_, 2024. URL https://arxiv.org/abs/2406.04936. D.2
* Carbonneau et al. (2022) Marc-Andre Carbonneau, Julian Zaidi, Jonathan Boilard, and Ghyslain Gagnon. Measuring disentanglement: A review of metrics. _IEEE Transactions on Neural Networks and Learning Systems_, 2022. URL https://doi.org/10.1109/TNNLS.2022.3218982. https://arxiv.org/abs/2012.09276.
* Caselles-Dupre et al. (2019) Hugo Caselles-Dupre, Michael Garcia Ortiz, and David Filliat. Symmetry-based disentangled representation learning requires interaction with environments. In _Neural Information Processing Systems_, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/36e729ec173b9433d8fa552e4029f8b-Abstract.html. D.1Chen Chung Chang. Algebraic analysis of many valued logics. _Transactions of the American Mathematical society_, 88(2):467-490, 1958. URL https://doi.org/10.2307/1993227.D.2
* Chang and Keisler (1966) Chen Chung Chang and H Jerome Keisler. _Continuous Model Theory_, volume 58 of _Annals of Mathematics Studies_. Princeton University Press, 1966. URL https://doi.org/10.1515/9781400882052.D.2
* Chen et al. (2018) Ricky TQ Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disentanglement in variational autoencoders. In _Neural Information Processing Systems_, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/1ee3dfcd8a064_5a25a3597/7997223d22-Abstract.html.
* Chen et al. (2020) Shuxiao Chen, Edgar Dobriban, and Jane H Lee. A group-theoretic framework for data augmentation. _The Journal of Machine Learning Research_, 21(245):1-71, 2020. URL http://jmlr.org/papers/v21/20-163.html. D.1
* Chen et al. (2024) Yiting Chen, Zhanpeng Zhou, and Junchi Yan. Going beyond neural network feature similarity: The network feature complexity and its interpretation using category theory. In _International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=4bSQ3l5fEV. A.1
* Cho and Jacobs (2019) Kenta Cho and Bart Jacobs. Disintegration and Bayesian inversion via string diagrams. _Mathematical Structures in Computer Science_, 29(7):938-971, 2019. URL https://doi.org/10.1017/S0960129518000488. https://arxiv.org/abs/1709.00322.
* Cho (2020) Simon Cho. Categorical semantics of metric spaces and continuous logic. _The Journal of Symbolic Logic_, 85(3):1044-1078, 2020. URL https://doi.org/10.1017/j5l.2020.44.D.2
* Cockayne and Melzak (1969) Ernest J Cockayne and Zdzislaw A Melzak. Euclidean constructibility in graph-minimization problems. _Mathematics Magazine_, 42(4):206-208, 1969. URL https://doi.org/10.108/0025570X.1969.11975961.
* Cohen et al. (2016) Michael B Cohen, Yin Tat Lee, Gary Miller, Jakub Pachocki, and Aaron Sidford. Geometric median in nearly linear time. In _Proceedings of the forty-eighth annual ACM symposium on Theory of Computing_, pages 9-21, 2016. URL https://doi.org/10.1145/2897518.2897647.4.
* Cohen (2021) Taco Cohen. _Equivariant convolutional networks_. PhD thesis, University of Amsterdam, 2021. URL https://hdl.handle.net/11245.1/6f7014ae-ee94-430e-a5d8-37d03d8d10e6. D.1
* Cohen and Welling (2014) Taco Cohen and Max Welling. Learning the irreducible representations of commutative Lie groups. In _International Conference on Machine Learning_, 2014. URL https://proceedings.mlr.press/v32/cohen14.html. D.1
* Cohen and Welling (2015) Taco Cohen and Max Welling. Transformation properties of learned visual representations. In _International Conference on Learning Representations_, 2015. URL http://arxiv.org/abs/1412.7659. D.1
* Cohen and Welling (2016) Taco Cohen and Max Welling. Group equivariant convolutional networks. In _International Conference on Machine Learning_, 2016. URL http://proceedings.mlr.press/v48/cohenc16.html. D.1
* Cruttwell et al. (2022) Geoffrey SH Cruttwell, Bruno Gavranovic, Neil Ghani, Paul Wilson, and Fabio Zanasi. Categorical foundations of gradient-based learning. In _Programming Languages and Systems_, pages 1-28. Springer International Publishing, 2022. URL https://doi.org/10.1007/978-3-030-999336-8_1. https://arxiv.org/abs/2103.01931. A.1
* Curry (1980) Haskell B. Curry. Some philosophical aspects of combinatory logic. In _Studies in Logic and the Foundations of Mathematics_, volume 101, pages 85-101. Elsevier, 1980. URL https://doi.org/10.1016/S0049-237X(08)71254-0.9Francesco Dagnino and Fabio Pasquali. Logical foundations of quantitative equality. In _Logic in Computer Science_, 2022. URL https://doi.org/10.1145/3531130.3533337. D.2 Daniels and Velikova (2010) Hennie Daniels and Marina Velikova. Monotone and partially monotone neural networks. _IEEE Transactions on Neural Networks_, 21(6):906-917, 2010. URL https://doi.org/10.1109/TNN.2010.2044803. D.1 Artur S. d'Avila Garcez, Krysia B. Broda, and Dov M. Gabbay. _Neural-Symbolic Learning Systems_. Springer, 2002. URL https://doi.org/10.1007/978-1-4471-0211-3. D.2 Pim de Haan, Taco Cohen, and Max Welling. Natural graph networks. In _Neural Information Processing Systems_, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/2517756c5a9be6ac007fe9bb7fb92611-Abstract.html. A.1, D.1 Andrea Dittadi, Frederik Trauble, Francesco Locatello, Manuel Wuthrich, Vaibhav Agrawal, Ole Winther, Stefan Bauer, and Bernhard Scholkopf. On the transfer of disentangled representations in realistic settings. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=8VXvj1QNR11. D.1 Kien Do and Truyen Tran. Theory and evaluation metrics for learning disentangled representations. In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=HJgKeh4vwr. 1, D.1 Andrew Dudzik. _Quantales and Hyperstructures: Monads, Mo'Problems_. PhD thesis, University of California, Berkeley, 2017. URL https://arxiv.org/abs/1707.09227. B.9 Andrew Joseph Dudzik and Petar Velickovic. Graph neural networks are dynamic programmers. In _Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=wu1Za9dY1GY. A.1 Cian Eastwood and Christopher KI Williams. A framework for the quantitative evaluation of disentangled representations. In _International Conference on Learning Representations_, 2018. URL https://openreview.net/forum?id=By-7dz-AZ. 1, 1, 2, 4.3, 4.3, 2, 5, D.1, 5, 6, 7, E.4, E.5 Cian Eastwood, Andrei Liviu Nicolicioiu, Julius Von Kugelgen, Armin Kekic, Frederik Trauble, Andrea Dittadi, and Bernhard Scholkopf. DCI-ES: An extended disentanglement framework with connections to identifiability. In _International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=4622-glgSht. 4.3, D.1 Ronald Fagin, Ryan Riegel, and Alexander Gray. Foundations of reasoning with uncertainty via real-valued logics. _Proceedings of the National Academy of Sciences_, 121(21), 2024. URL https://doi.org/10.1073/pnas.2309905121. D.2 Daniel Figueroa and Benno van den Berg. A topos for continuous logic. _Theory and Applications of Categories_, 38(28), 2022. URL http://www.tac.mta.ca/tac/volumes/38/28/38-28abs.html. D.2 Kaspar Fischer, Bernd Gartner, and Martin Kutz. Fast smallest-enclosing-ball computation in high dimensions. In _European Symposium on Algorithms_, pages 630-641. Springer, 2003. URL https://doi.org/10.1007/978-3-540-39658-1_57.
* Fong and Spivak (2019) Brendan Fong and David I Spivak. _An Invitation to Applied Category Theory: Seven Sketches in Compositionality_. Cambridge University Press, 2019. URL https://doi.org/10.1017/9781108668804. https://arxiv.org/abs/1803.05316. A.1, B.1, B.6, B.8 Tobias Fritz. A synthetic approach to Markov kernels, conditional independence and theorems on sufficient statistics. _Advances in Mathematics_, 370:107239, 2020. URL https://doi.org/10.1016/j.aim.2020.107239. https://arxiv.org/abs/1908.07021. A.1 Soichiro Fujii. Ordered semirings and subadditive morphisms. _arXiv preprint_, 2023. URL https://arxiv.org/abs/2311.03862. B.9

[MISSING_PAGE_FAIL:15]

Max Kelly. Basic concepts of enriched category theory. In _London Mathematical Society Lecture Note Series_, volume 64. Cambridge University Press, 1982. URL http://www.tac.mta.ca /tac/reprints/articles/1e/tru6.html.
* Kendall (1938) Maurice G. Kendall. A new measure of rank correlation. _Biometrika_, 30(1/2):81-93, 1938. URL https://doi.org/10.2307/2332226.
* learning group structured representations from observed transitions. In _International Conference on Machine Learning_, 2023. URL https://pr oceedings.mlr.press/v2o2/keurti23a.html.
* Kim and Mnih (2018) Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In _International Conference on Machine Learning_, 2018. URL http://proceedings.mlr.press/v8o/kim8b.html.
* Kingma and Welling (2014) Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In _International Conference on Learning Representations_, 2014. URL https://openreview.net/forum?id=33X9 fd2-9FyZd. http://arxiv.org/abs/1312.6114.
* Kohler et al. (2020) Jonas Kohler, Leon Klein, and Frank Noe. Equivariant flows: exact likelihood generative learning for symmetric densities. In _International Conference on Machine Learning_, 2020. URL https://proceedings.mlr.press/v119/kohler2oa.html.
* Koller and Friedman (2009) Daphne Koller and Nir Friedman. _Probabilistic Graphical Models: Principles and Techniques_. MIT press, 2009. URL https://mitpress.mit.edu/9780262013192/.
* Kullback and Leibler (1951) Solomon Kullback and Richard A. Leibler. On information and sufficiency. _The Annals of Mathematical Statistics_, 22(1):79-86, 1951. URL https://doi.org/10.1214/aoms/1177729694.
* Kvinge et al. (2022) Henry Kvinge, Tegan Emerson, Grayson Jorgenson, Scott Vasquez, Timothy Doster, and Jesse Lew. In what ways are deep neural networks invariant and how should we measure this? In _Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=SCD0hn3kMHw.
* Lawvere (1969) F. William Lawvere. Adjointness in foundations. _Dialectica_, pages 281-296, 1969. URL https://doi.org/10.1111/j.1746-8361.1969.tb01194.x. http://www.tac.mta.ca/tac/reprints/articles/16/tr16abs.html.
* Lawvere (1973) F. William Lawvere. Metric spaces, generalized logic, and closed categories. _Rendiconti del seminario matematico e fisico di Milano_, 43:135-166, 1973. URL https://doi.org/10.1007/BF02924844. http://www.tac.mta.ca/tac/reprints/articles/1/trabs.html.
* Lawvere (1978) F. William Lawvere. Taking categories seriously. _Revista colombiana de matematicas_, 20(3-4):147-178, 1986. http://www.tac.mta.ca/tac/reprints/articles/8/tr8abs.html.
* Lawvere and Rosebugh (2003) F William Lawvere and Robert Rosebugh. _Sets for Mathematics_. Cambridge University Press, 2003. URL https://doi.org/10.1017/CBO9780511755460.
* Lee et al. (2019) Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. In _International Conference on Machine Learning_, 2019. URL http://proceedings.mlr.press/v97/lee9d.
* Leinster (2010) Tom Leinster. An informal introduction to topos theory. _arXiv preprint_, 2010. URL https://arxiv.org/abs/1012.5647.
* Leinster (2014) Tom Leinster. _Basic Category Theory_. Cambridge University Press, 2014. URL https://doi.org/10.1017/CBO9781107360068. https://arxiv.org/abs/1612.09375.
* Leinster (2014)Zhiyuan Li, Jaideep Vitthal Murkute, Prashnna Kumar Gyawali, and Linwei Wang. Progressive learning and disentanglement of hierarchical representations. In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=S3xPSxrYPS.
* Locatello et al. (2019a) Francesco Locatello, Gabriele Abbati, Thomas Rainforth, Stefan Bauer, Bernhard Scholkopf, and Olivier Bachem. On the fairness of disentangled representations. In _Neural Information Processing Systems_, 2019a. URL https://proceedings.neurips.cc/paper/2019/hash/1b486d7a5189ebe8d8c46afc64bd01b4-Abstract.html.
* Locatello et al. (2019b) Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Scholkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. In _International Conference on Machine Learning_, 2019b. URL https://proceedings.mlr.press/v97/locatello19a.html.
* Locatello et al. (2020) Francesco Locatello, Ben Poole, Gunnar Ratsch, Bernhard Scholkopf, Olivier Bachem, and Michael Tschannen. Weakly-supervised disentanglement without compromises. In _International Conference on Machine Learning_, 2020. URL http://proceedings.mlr.press/v119/locatello1020a.html.
* Lukasiewicz (1920) Jan Lukasiewicz. O logice trojwartosciowej (On three-valued logic). In _Selected Works_, volume 11 of _Studies in Logic_, pages 87-88. North-Holland Publishing Company, 1920.
* Lukasiewicz and Tarski (1930) Jan Lukasiewicz and Alfred Tarski. Untersuchungen uber den Aussagenkalkul (Investigations on the propositional calculus). _Comptes Rendus des Seances de la Societe des Sciences et des Lettres de Varsovie, Class III_, 23, 1930.
* Lane (1978) Saunders Mac Lane. _Categories for the Working Mathematician_. Springer, 1978. URL https://doi.org/10.1007/978-1-4757-4721-8.
* Lane and Moerdijk (1994) Saunders Mac Lane and leke Moerdijk. _Sheaves in Geometry and Logic: A First Introduction to Topos Theory_. Springer, 1994. URL https://doi.org/10.1007/978-1-4612-0927-0.
* Mahon et al. (2023) Louis Mahon, Lei Shah, and Thomas Lukasiewicz. Correcting flaws in common disentanglement metrics. _arXiv preprint_, 2023. URL https://arxiv.org/abs/2304.02335.
* Malinowski (2007) Gregorz Malinowski. Many-valued logic and its philosophy. In _Handbook of the History of Logic_, volume 8, pages 13-94. Elsevier, 2007. URL https://doi.org/10.1016/51874-5857(07)80004-5.
* Manhaeve et al. (2018) Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, and Luc De Raedt. DeepProbLog: Neural probabilistic logic programming. In _Neural Information Processing Systems_, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/dc5d637ed5e62c36ecb73b654b05baa-Abstract.html.
* Mardare et al. (2016) Radu Mardare, Prakash Panangaden, and Gordon Plotkin. Quantitative algebraic reasoning. In _Logic in Computer Science_, 2016. URL https://doi.org/10.1145/2933575.2934518.
* Mardare et al. (2021) Radu Mardare, Prakash Panangaden, and Gordon Plotkin. Fixed-points for quantitative equational logics. In _Logic in Computer Science_, 2021. URL https://doi.org/10.1109/LICS52264.2021.9470662.
* Maron et al. (2019) Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph networks. In _International Conference on Learning Representations_, 2019. URL https://openreview.net/forum?id=Syx72jC9tm.
* Matthey et al. (2017) Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dSprites: Disentanglement testing sprites dataset, 2017. https://github.com/deepmind/dSprites-dataset (Apache License 2.0).
* Mazur (2008) Barry Mazur. When is one thing equal to some other thing? In _Proof and Other Dilemmas: Mathematics and Philosophy_, page 221-242. Mathematical Association of America, 2008. URL https://doi.org/10.5948/UP09781614445050.015. https://people.math.harvard.edu/~mazur/preprints/when_is_one.pdf.
* Mazur (2017)

[MISSING_PAGE_FAIL:18]

Paolo Perrone. Markov categories and entropy. _IEEE Transactions on Information Theory_, 2023. URL https://doi.org/10.1109/TIT.2023.3328825. https://arxiv.org/abs/2212.11719. A.1, D.2
* Pfau et al. (2020) David Pfau, Irina Higgins, Alex Botev, and Sebastien Racaniere. Disentangling by subspace diffusion. In _Neural Information Processing Systems_, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/cgf029a6a1b20a84a08f372351b321dd8-Abstract.html.
* Pillutla et al. (2022) Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning. _IEEE Transactions on Signal Processing_, 70:1142-1154, 2022. URL https://doi.org/10.1109/TSP.2022.3153135.
* Pin (1998) Jean-Eric Pin. Tropical semirings. In _Idempotency_, pages 50-69. Cambridge University Press, 1998. URL https://doi.org/10.1017/CBO9780511662508.094.
* Quessard et al. (2020) Robin Quessard, Thomas Barrett, and William Clements. Learning disentangled representations and group structure of dynamical environments. In _Neural Information Processing Systems_, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/e449b9317dad92ecodd5adoa2a2d5ea9-Abstract.html.
* Reed et al. (2015) Scott E Reed, Yi Zhang, Yuting Zhang, and Honglak Lee. Deep visual analogy-making. In _Neural Information Processing Systems_, 2015. URL https://proceedings.neurips.cc/paper/2015/hash/e07413354875be01a996dc560274708e-Abstract.html.
* Reid and Williamson (2010) Mark D. Reid and Robert C. Williamson. Composite binary losses. _Journal of Machine Learning Research_, 11(83):2387-2422, 2010. URL http://jmlr.org/papers/v11/reid1oa.html.
* Rezende and Mohamed (2015) Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In _International Conference on Machine Learning_, 2015. URL https://proceedings.mlr.press/v37/rezende15.html.
* Ridgeway and Mozer (2018) Karl Ridgeway and Michael C Mozer. Learning deep disentangled embeddings with the f-statistic loss. In _Neural Information Processing Systems_, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/2b240495052a8ce66358eb576b8912c8-Abstract.html.
* Roth et al. (2023) Karsten Roth, Mark Ibrahim, Zeynep Akata, Pascal Vincent, and Diane Bouchacourt. Disentanglement of correlated factors via hausdorff factorized support. In _International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=0KcJhpQiGiX.D.7
* Scholkopf and von Kugelgen (2022) Bernhard Scholkopf and Julius von Kugelgen. From statistical to causal learning. In _International Congress of Mathematicians_. EMS Press, 2022. URL https://doi.org/10.4171/icm2022/173. https://arxiv.org/abs/2204.00607. D.1
* Sen et al. (2022) Prithviraj Sen, Breno WSR de Carvalho, Ryan Riegel, and Alexander Gray. Neuro-symbolic inductive logic programming with logical neural networks. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2022. URL https://doi.org/10.1609/aaai.v36i8.20795.D.2
* Shiebler et al. (2021) Dan Shiebler, Bruno Gavranovic, and Paul Wilson. Category theory in machine learning. _arXiv preprint_, 2021. URL https://arxiv.org/abs/2106.07032.
* Shiebler (2023) Daniel Shiebler. _Compositionality and Functorial Invariants in Machine Learning_. PhD thesis, University of Oxford, 2023. URL http://doi.org/10.5287/bodleian:DE1aDx4Zw.A.1
* Shu et al. (2020) Rui Shu, Yining Chen, Abhishek Kumar, Stefano Ermon, and Ben Poole. Weakly supervised disentanglement with guarantees. In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=HJgSwyBKvr.
* Shu et al. (2020)* Sill (1997) Joseph Sill. Monotonic networks. In _Neural Information Processing Systems_, 1997. URL https://proceedings.neurips.cc/paper/1997/hash/83adc9225e4deb67d7ce42d58fe5157c-Abstract.html.
* Steinwart (2007) Ingo Steinwart. How to compare different loss functions and their risks. _Constructive Approximation_, 26:225-287, 2007. URL https://doi.org/10.1007/s00365-006-0662-3.
* Suter et al. (2019) Raphael Suter, Djordje Miladinovic, Bernhard Scholkopf, and Stefan Bauer. Robustly disentangled causal mechanisms: Validating deep representations for interventional robustness. In _International Conference on Machine Learning_, 2019. URL http://proceedings.mlr.press/v97/suter19a.html.
* Tokui and Sato (2022) Seiya Tokui and Issei Sato. Disentanglement analysis with partial information decomposition. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=pETy-HVvGtt.
* Tonnaer et al. (2022) Loek Tonnaer, Luis A Perez Rey, Vlado Menkovski, Mike Holenderski, and Jacobus W Portegies. Quantifying and learning linear symmetry-based disentanglement. In _International Conference on Machine Learning_, 2022. URL https://proceedings.mlr.press/v162/tonnaer22a.html.
* Trauble et al. (2021) Frederik Trauble, Elli Creager, Niki Kilbertus, Francesco Locatello, Andrea Dittadi, Anirudh Goyal, Bernhard Scholkopf, and Stefan Bauer. On disentangled representations learned from correlated data. In _International Conference on Machine Learning_, 2021. URL https://proceedings.mlr.press/v139/trauble21a.html.
* Trimble (2019) Todd Trimble. An elementary approach to elementary topos theory, 2019. URL https://ncatlab.org/toddtrimble/published/An+elementary+approach+to+elementary+topos+theory.
* Tuzhilin (2016) Alexey A. Tuzhilin. Who invented the Gromov-Hausdorff distance? _arXiv preprint_, 2016. URL https://arxiv.org/abs/1612.00728.
* van der Pol et al. (2022) Elise van der Pol, Herke van Hoof, Frans A Oliehoek, and Max Welling. Multi-agent MDP homomorphic networks. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=H7HDG--DJF6.
* Virtanen et al. (2020) Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stefan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental algorithms for scientific computing in Python. _Nature methods_, 17(3):261-272, 2020. URL https://doi.org/10.1038/541592-019-0686-2.
* Wagstaff et al. (2001) Kiri Wagstaff, Claire Cardie, Seth Rogers, and Stefan Schrodl. Constrained k-means clustering with background knowledge. In _International Conference on Machine Learning_, 2001. URL https://dl.acm.org/doi/10.5555/645530.65569.
* Wang and Isola (2020) Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In _International conference on machine learning_, 2020. URL https://proceedings.mlr.press/v1i9/wang2ok.html.
* Weiszfeld (1937) Endre Weiszfeld. Sur le point pour lequel la somme des distances de n points donnes est minimum (on the point for which the sum of the distances to n given points is minimum). _Tohoku Mathematical Journal, First Series_, 43:355-386, 1937. URL https://doi.org/10.1007/s10479-008-0352-z.
* Welzl (1991) Emo Welzl. Smallest enclosing disks (balls and ellipsoids). In _New Results and New Trends in Computer Science_, pages 359-370. Springer, 1991. URL https://doi.org/10.1007/BFb0038202.
* Wojciech et al. (2019)Zhenlin Xu, Marc Niethammer, and Colin A Raffel. Compositional generalization in unsupervised compositional representation learning: A study on disentanglement and emergent language. In _Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=ZEQ5Gf8DiD. D.1
* Yang et al. (2022) Tao Yang, Xuanchi Ren, Yuwang Wang, Wenjun Zeng, and Nanning Zheng. Towards building a group-based unsupervised representation disentanglement framework. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=YgPqNctmyd. D.1
* Yuan (2023) Yang Yuan. On the power of foundation models. In _International Conference on Machine Learning_, 2023. URL https://proceedings.mlr.press/v202/yuan23b.html. A.1
* Zaheer et al. (2017) Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. In _Neural Information Processing Systems_, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/f22e4747daaa27e363d86d4off442fe-Abstract.html. D.1
* Zhang et al. (2021) Sharon Zhang, Amit Moscovich, and Amit Singer. Product manifold learning. In _International Conference on Artificial Intelligence and Statistics_, 2021. URL http://proceedings.mlr.press/v130/zhang2ij.html. D.1
* Zhang and Sugiyama (2023) Yivan Zhang and Masashi Sugiyama. A category-theoretical meta-analysis of definitions of disentanglement. In _International Conference on Machine Learning_, 2023. URL https://proceedings.mlr.press/v202/zhang23ak.html. 1, 4.2, A.1, D.1
* Zhou et al. (2020) Sharon Zhou, Eric Zelikman, Fred Lu, Andrew Y Ng, Gunnar Carlsson, and Stefano Ermon. Evaluating the disentanglement of deep generative models through manifold topology. In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=djwSom4Ft_A. D.1

###### Contents

* 1 Introduction
* 2 Logical definitions of disentangled representations
	* 2.1 Informativeness: injectivity or retractability of a learning model
	* 2.2 Modularity: product structure preserved by a learning model
* 3 Enrichment: from logic to metric
	* 3.1 From equality predicate to strict premetric
	* 3.2 From logical operation to quantitative operation
	* 3.3 From compound predicate to compound quantity
	* 3.4 (Sub)homomorphism from metric to logic
* 4 Quantitative metrics of disentangled representations
	* 4.1 Modularity metrics via product approximation
	* 4.2 Modularity metrics via constancy
	* 4.3 Informativeness metrics
* 5 Experiments
* 6 Conclusion
* A Preliminaries
* A.1 Basic category theory
* A.2 Elementary topos theory
* A.3 Enriched category theory
* B Theory
* B.1 Subobject quantifier and quantizer
* B.2 Equality and premetric
* B.3 Preorder
* B.4 Operation: algebra over the product endofunctor
* B.5 Negation
* B.6 Conjunction
* B.7 Disjunction
* B.8 Implication
* B.9 Heyting algebra, quantale, and ordered semiring
* B.10 Quantifier: algebra over the exponentiation endofunctor
* B.11 Enrichment
* B.12 Summary
* C Proofs
* C.1 Product function
* C.2 Constant curried function
* D Discussions
* D.1 Background
* D.2 Related work
* D.3 Implication and equivalence
* D.4 Constant function
* D.5 Rank of imperfect representations
* D.6 Implementation
* D.7 Limitations
* D.8 Broader impact
* E Experiments
* E.1 Synthetic data
* E.2 Weakly supervised modularity metrics
* E.3 Evaluation of existing models on image datasets
* E.4 Kendall tau distance between metrics
* E.5 Computation time of metrics
* E.6 Factor-wise modularity metrics

List of Figures
* 1 Disentangled representation learning
* 2 From predicates and logical operations to quantities and quantitative operations
* 3 Negation
* 4 Conjunction
* 5 Disjunction
* 6 Implication
* 7 Equivalence
* 8 Quantitative operations for implication
* 9 Quantitative operations for equivalence
* 10 Constancy metrics
* 11 Radius and variance
* 12 Rank of imperfect representations
* 13 Illustration of synthetic data
* 14 Entanglement of a distribution

List of Tables
* 1 From logic to metric
* 2 Supervised disentanglement metrics
* 3 Supervised modularity metrics
* 4 Weakly supervised modularity metrics
* 5 Supervised disentanglement metrics on image datasets
* 6 Average Kendall tau rank distances between disentanglement metrics
* 7 Computation time (seconds) of supervised disentanglement metrics on image datasets
* 8 Factor-wise modularity metrics on **3D Cars**(Reed et al., 2015)
* 9 Factor-wise modularity metrics on **dSprites**(Matthey et al., 2017)
* 10 Factor-wise modularity metrics on **3D Shapes**(Burgess and Kim, 2018)
* 11 Factor-wise modularity metrics on **MPI3D**(Gondal et al., 2019)Preliminaries

In this paper, we used abstract mathematical tools such as category theory and topos theory to develop a theory of the relationship between logical definitions and quantitative metrics. However, this level of abstraction may be unfamiliar or even intimidating to some readers, and sometimes unnecessary for machine learning practitioners. Therefore, we have used only the most basic algebraic concepts, such as homomorphism, in the main text. For readers interested in the mathematical background, we provide a brief introduction to the basic categorical concepts in this section.

### Basic category theory

_Category theory_ is a branch of mathematics that studies mathematical structures in an abstract way, which is suitable for identifying and organizing common patterns across various fields of mathematics [12, 13]. It has found applications in many fields, including computer science [14, 15], probability theory [16, 17, 18], and machine learning [19, 15, 14, 16, 17, 18, 19, 20].

The most fundamental concept is that of a _category_:

**Definition 14**.: A _category_\(\mathbf{C}=(\mathrm{Obj},\mathrm{Hom},\circ,\mathrm{id})\) consists of

* a collection \(\mathrm{Obj}\) of objects,
* a set \(\mathrm{Hom}(A,B)\) of morphisms between objects,10 Footnote 10: To be more precise, a category whose morphisms are sets is called a _locally small_ category.
* a composition function \(\circ:\mathrm{Hom}(B,C)\times\mathrm{Hom}(A,B)\to\mathrm{Hom}(A,C)\) for each triple of objects, and
* an identity morphism \(\mathrm{id}_{A}\in\mathrm{Hom}(A,A)\) for each object,

subject to

* associativity: \((h\circ g)\circ f=h\circ(g\circ f)\) and
* identity: \(\mathrm{id}_{B}\circ f=f=f\circ\mathrm{id}_{A}\).

A crucial example is the category \(\mathbf{Set}\) of sets and functions. However, what is of particular interest is not the category itself but its relationships with other categories. Building on the concepts of the _functor_ and _natural transformation_, whose definitions are omitted here, we can develop tools to better understand the properties of a category.

Moreover, we can _define_ objects in terms of their relations with other objects, employing what is known as _universal construction_. For example, a _terminal object_\(1\) in a category is an object such that for any object \(A\), there exists a unique morphism \(e_{A}:A\to 1\) to it, which we call a _terminal morphism_. In \(\mathbf{Set}\), any set \(\{*\}\) with only one element is a terminal object. Based on the concept of the terminal object, a _global element_ of an object \(B\) is defined to be a morphism \(b:1\to B\) from a terminal object.

We write \(b_{A}:A\to B\) as an abbreviation for the _constant morphism_\(b\circ e_{A}:A\xrightarrow{e_{A}}1\xrightarrow{b}B\) with value \(b:1\to B\). These concepts will be used to develop our theory in Appendix B. Other important universal constructions include the _product_, _pullback_, and _exponential_. The concept of the product is of great importance in disentangled representation learning [14].

Regarding the pullback, we need to mention the following useful lemma:

**Lemma 4** (Pullback lemma).: _Suppose that in the following commutative diagram, the right square is a pullback._

(31)

_Then, the left square is a pullback if and only if the outer rectangle is a pullback._

This lemma is usually left as an exercise in textbooks [e.g., 12, p. 72, Exercise 8, Leinster, 2014, Exercise 5.1.35]. A proof can be found in Fong and Spivak [2019, Proposition 7.3]. We need to use this lemma to (de)compose pullbacks. As a side note, we use the asterisk \(f^{*}g\) to denote the pullback of \(g\) along \(f\).

### Elementary topos theory

_Topos theory_ studies categories that, in some sense, exhibit behavior akin to the category of sets and functions [Lawvere and Rosebrough, 2003]. Topos theory has found applications in geometry, topology, and logic [Mac Lane and Moerdijk, 1994, Johnstone, 2002, Leinster, 2010, Trimble, 2019]. In this work, we only explore its relation to logic.

To formally define a _topos_, two essential concepts are those of the subobject and subobject classifier. A _subobject_ of an object \(C\) is simply a monomorphism \(b:B\to C\) to the object \(C\). The subobject classifier is defined as follows:

**Definition 15**.: In a finitely complete category, a _subobject classifier_ is a universal subobject \(\top:1\mapsto\Omega\) such that for every subobject \(b:B\to C\), there exists a unique morphism \(\chi_{b}:C\to\Omega\) such that \(b\) is a pullback of \(\top\) along \(\chi_{b}\). The morphism \(\chi_{b}\) is called the _classifying morphism_ of \(b\).

(32)

Alternatively, we can state that

**Proposition 5**.: _A subobject classifier is precisely a terminal object in the category of monomorphisms and pullbacks._

Then, we can study the morphisms to the object \(\Omega\):

**Definition 16**.: In a category with a subobject classifier \(\top:1\mapsto\Omega\), a _predicate_ on an object \(C\) is a morphism \(p:C\to\Omega\).

Based on Definition 15, we can state that subobjects of an object \(C\) are classified by predicates on \(C\).

For example, in \(\mathbf{Set}\), subobjects are subsets, a function from a singleton \(\{*\}\) to a two-element set is a subobject classifier, which is usually denoted by \(\top:\{*\}\to\{\top,\bot\}\), a predicate on a set \(C\) is a function \(p:C\to\{\top,\bot\}\), and a subset precisely corresponds to its indicator function.

Among various equivalent definitions of a topos, a concise one is as follows:

**Definition 17**.: An _elementary topos_ is a finitely complete and cartesian closed category with a subobject classifier.

Despite its concise definition, a great number of logical structures can be derived from it, which will be explored in Appendix B.

### Enriched category theory

_Enriched category theory_ generalizes the concept of the category by replacing the sets of morphisms with objects in a suitable category [Kelly, 1982]. It has been used to better understand a wide range of domains, from metric spaces [Lawvere, 1973] to language [Bradley et al., 2022].

Let us dive into the definition of an enriched category:

**Definition 18**.: A category \(\mathbf{C}=(\mathrm{Obj},\mathrm{Hom},\circ,\mathrm{id})\) enriched in a monoidal category \((\mathbf{V},\otimes,I)\) consists of

* a collection \(\mathrm{Obj}\) of objects,
* a \(\mathrm{hom}\)-object \(\mathrm{Hom}(A,B)\in\mathrm{Obj}_{\mathbf{V}}\) between objects,
* a composition \(\underline{\mathrm{morphism}}\circ:\mathrm{Hom}(B,C)\otimes\mathrm{Hom}(A,B) \to\mathrm{Hom}(A,C)\) for each triple of objects, and
* an identity element \(\mathrm{id}_{A}:I\to\mathrm{Hom}(A,A)\) for each object,

subject to associativity and identity.

Comparing Definitions 14 and 18, we can say that a (locally small) category is a category enriched in the category \(\mathbf{Set}\) of sets and functions. Enrichment is a way to describe the additional structures of morphisms and the properties that need to be respected by composition.

An example is a preorder, which can be seen as a category enriched in the category of boolean values. Another example is a _Lawvere metric space_[Lawvere, 1973], which is a set with a premetric that satisfies the triangle inequality. Note that the transitivity of a preorder and the triangle inequality of a Lawvere metric are described by their composition morphisms, respectively.

In this work, we use enrichment to describe the association of a set of morphisms with additional operations, such as a strict premetric and aggregators.

## Appendix B Theory

In this section, we detail the theory of converting logical definitions into their corresponding quantitative metrics based on elementary topos theory and enriched category theory.

### Subobject quantifier and quantizer

Since our main goal is to develop a multi-valued (possibly continuous and differentiable) quantification of properties defined by a certain type of logic, we begin with a category \(\mathbf{E}\) that has sufficient structures to allow the desired logical operations and build the quantification upon these structures.

Firstly, recall that a subobject classifier \(\Omega\), if exists, is the _representing object_ of the subobject functor \(\mathrm{Sub}_{\mathbf{E}}\), such that a subobject \(b:B\rightarrowtail C\) corresponds to a unique classifying morphism \(\chi_{b}:C\rightarrow\Omega\), and an _external operation_ on the set \(\mathrm{Sub}_{\mathbf{E}}(C)\) of subobjects that is natural in the object \(C\) corresponds to an _internal operation_.

For example, the intersection

\[\cap_{C}:\mathrm{Sub}_{\mathbf{E}}(C)\times\mathrm{Sub}_{\mathbf{E}}(C) \rightarrow\mathrm{Sub}_{\mathbf{E}}(C)\] (33)

corresponds a natural transformation

\[\mathrm{Hom}_{\mathbf{E}}(-,\Omega)\times\mathrm{Hom}_{\mathbf{E}}(-,\Omega) \Rightarrow\mathrm{Hom}_{\mathbf{E}}(-,\Omega),\] (34)

which, because the hom-functor \(\mathrm{Hom}_{\mathbf{E}}\) preserves limits, is isomorphic to a natural transformation between hom-functors

\[\mathrm{Hom}_{\mathbf{E}}(-,\Omega\times\Omega)\Rightarrow\mathrm{Hom}_{ \mathbf{E}}(-,\Omega),\] (35)

which, by the Yoneda lemma, is isomorphic to an internal operation on the subobject classifier \(\Omega\):

\[\wedge:\Omega\times\Omega\rightarrow\Omega.\] (36)

Note that the subobject classifier, the classifying morphisms, and those internal operations are determined uniquely up to isomorphism. However, in order to obtain a multi-valued quantification, the requirement for uniqueness might be too restrictive. Thus, we propose to study a weakened concept instead:

**Definition 19**.: In a finitely complete category, a _subobject quantifier_ is a subobject \(o:1\rightarrowtail\Psi\) such that for every subobject \(b:B\rightarrowtail C\), there exists at least one morphism \(\phi_{b}:C\rightarrowtail\Psi\) such that \(b\) is a pullback of \(o\) along \(\phi_{b}\). The morphism \(\phi_{b}\) is called a _quantifying morphism_ of \(b\). If the category has a subobject classifier \(\top:1\rightarrowtail\Omega\), the _quantizer_\(\kappa:\Psi\rightarrowtail\Omega\) of the subobject quantifier \(o\) is the classifying morphism of \(o\).

(37)

More succinctly, we can state that (cf. Proposition 5)

**Proposition 6**.: _A subobject quantifier is a weakly terminal object in the category of monomorphisms and pullbacks._Thus, there is a unique morphism \(\chi_{b}:C\to\Omega\) classifying a subobject \(b:B\to C\) of an object \(C\), but there could be multiple morphisms \(\phi_{b}:C\to\Psi\) quantifying this subobject.

It is provable that the domain of a terminal object \(\top\) in the category of monomorphisms and pullbacks (Proposition 5) must be a terminal object \(1\) in the category \(\mathbf{E}\), but not all weakly terminal objects in the category of monomorphisms and pullbacks (Proposition 6) are monomorphisms out of a terminal object. We choose Definition 19 because we want only one global element \(o:1\to\Psi\) to be designated to the truth value \(\top:1\to\Omega\).

Here, we give two examples to motivate this definition of subobject quantifier. One is related to _three-valued logic_[1, 13, Exercise 2.34]:

**Example 9**.: In \(\mathbf{Set}\), the function

\[\mathtt{yes}:\{*\}\to\{\mathtt{no},\mathtt{maybe},\mathtt{yes}\},\] (38)

which maps the element \(*\) in a singleton set \(\{*\}\) (a terminal object in \(\mathbf{Set}\)) to an element \(\mathtt{yes}\) in a three-element set \(\{\mathtt{no},\mathtt{maybe},\mathtt{yes}\}\) is a subobject quantifier. For any subset \(B\) of a set \(C\), a quantifying morphism \(\phi_{b}:C\to\Psi\) is a function that maps all elements in the subset \(B\) to \(\mathtt{yes}\) and all other elements to either \(\mathtt{maybe}\) or \(\mathtt{no}\).

The other is related to _metric spaces_[10] and will be our running example in the following subsections:

**Example 10**.: In \(\mathbf{Set}\), the function \(0:\{*\}\to[0,\infty]\) selecting the number \(0\) out of the set \([0,\infty]\) of extended non-negative real numbers is a subobject quantifier. The quantizer is a function

\[\kappa:[0,\infty]\to\{\top,\bot\}:=n\mapsto\begin{cases}\top&n=0,\\ \bot&n>0,\end{cases}\] (39)

which maps \(0\) to \(\top\) and any non-zero number to \(\bot\).

Intuitively, with a subobject quantifier, there is only one way to be true, but there may be many ways to be false. In \(\mathbf{Set}\), a quantizer \(\kappa\) maps multiple "_degrees of truth_" from a potentially large, even infinite set \(\Psi\) to a smaller set \(\Omega\) of truth values, hence the name.

Next, we define the counterpart of the concept of predicate (Definition 16):

**Definition 20**.: In a category with a subobject quantifier \(o:1\to\Psi\), a _quantity_ on an object \(C\) is a morphism \(q:C\to\Psi\).

Since we weakened the requirement for uniqueness, there is no one-to-one correspondence between subobjects and quantities. However, they are still related as follows:

**Lemma 7**.: _In a category with a subobject classifier \(\top:1\to\Omega\), a subobject quantifier \(o:1\to\Psi\), and a quantizer \(\kappa:\Psi\to\Omega\), a quantity \(q:C\to\Psi\) on an object \(C\) is a quantifying morphism of a subobject \(q^{*}o\) of the object \(C\), which is isomorphic to a subobject \((\kappa\circ q)^{*}\top\)._

Proof.: \(q^{*}o\) and \((\kappa\circ q)^{*}\top\) are both subobjects of \(C\) because pullbacks preserve subobjects. Their isomorphism follows from the pullback lemma. 

**Lemma 8**.: _In a category with a subobject classifier \(\top:1\to\Omega\), a subobject quantifier \(o:1\to\Psi\), and a quantizer \(\kappa:\Psi\to\Omega\), a quantity \(q:C\to\Psi\) on an object \(C\) is a quantifying morphism of a subobject \(b:B\to C\) if and only if \(\kappa\circ q=\chi_{b}\), where \(\chi_{b}\) is the classifying morphism of the subobject \(b\)._

Proof.: Necessity follows from the pullback lemma and the uniqueness of the classifying morphism; sufficiency follows from Lemma 7. 

The following relationship between a quantity and the quantizer is also useful:

**Lemma 9**.: _In a category with a subobject classifier \(\top:1\to\Omega\), a subobject quantifier \(o:1\to\Psi\), and a quantizer \(\kappa:\Psi\to\Omega\), for any quantity \(q:C\to\Psi\) on an object \(C\), \(q=o_{C}\) if and only if \(\kappa\circ q=\kappa\circ o_{C}=\top_{C}\), where \(o_{C}\) is the constant morphism \(o\circ e_{C}:C\xrightarrow{e_{C}}1\xrightarrow{o_{i}}\Psi\) with value \(o:1\to\Psi\)._

Proof.: This is due to the universal property of pullback \(o\) of \(\top\) along \(\kappa\).

We can see that the hom-functor on the quantizer \(\operatorname{Hom}_{\mathbf{E}}(-,\kappa):\operatorname{Hom}_{\mathbf{E}}(-,\Psi) \Rightarrow\operatorname{Hom}_{\mathbf{E}}(-,\Omega)\) is a natural transformation which maps the quantities \(\operatorname{Hom}_{\mathbf{E}}(C,\Psi)\) on an object \(C\) to the predicates \(\operatorname{Hom}_{\mathbf{E}}(C,\Omega)\) on \(C\), which are precisely subobjects of \(C\).

### Equality and premetric

Next, we take a closer look at a concrete and important predicate -- equality -- and its corresponding quantities.

**Definition 21**.: In a category with a subobject classifier \(\top:1\to\Omega\), the _equality predicate_\(=_{C}\): \(C\times C\to\Omega\) on an object \(C\) is the classifying morphism of the diagonal morphism \(\Delta_{C}:C\to C\times C:=\langle\mathrm{id}_{C},\mathrm{id}_{C}\rangle\).

By Lemma 8, a quantity \(d_{C}:C\times C\to\Psi\) is a quantifying morphism of \(\Delta_{C}\) if and only if \(\kappa\circ d_{C}==_{C}\), depicted in the following diagram:

(40)

In \(\mathbf{Set}\), we have the following definitions:

**Definition 22**.: A _premetric_ on a set \(C\) is a binary function \(d_{C}:C\times C\to[0,\infty]\) such that

\[\forall c\in C.\ d_{C}(c,c)=0.\] (41)

Or equivalently, \(d_{C}\circ\Delta_{C}=0_{C}\), depicted in the following diagram:

(42)

**Definition 23**.: A _strict premetric_ on a set \(C\) is a premetric \(d_{C}:C\times C\to[0,\infty]\) such that

\[\forall c_{1}\in C.\ \forall c_{2}\in C.\ (d_{C}(c_{1},c_{2})=0)\to(c_{1}=_{C}c_{2}).\] (43)

Or equivalently, \(\Delta_{C}\) is a pullback of \(0\) along \(d_{C}\):

(44)

In other words, strict premetrics are precisely quantifying morphisms of the diagonal morphism \(\Delta_{C}\) in the category \(\mathbf{Set}\) with \(0:\{*\}\to[0,\infty]\) as a subobject quantifier.

Note that the symmetry

\[\forall c_{1}\in C.\ \forall c_{2}\in C.\ d_{C}(c_{1},c_{2})=d_{C}(c_{2},c_{1})\] (45)

and the triangle inequality

\[\forall c_{1}\in C.\ \forall c_{2}\in C.\ \forall c_{3}\in C.\ d_{C}(c_{1},c_{2 })+d_{C}(c_{2},c_{3})\geq d_{C}(c_{1},c_{3}),\] (46)

which make \(d_{C}\) a _metric_, are not required. The addition \(+\) and the order \(\geq\) on the set \([0,\infty]\) are not needed to define a strict premetric. However, they are necessary for defining other operations and properties, which will be discussed in the next subsection.

### Preorder

There is a preorder \(\subseteq_{C}\) of inclusion defined on the set \(\mathrm{Sub}_{\mathbf{E}}(C)\) of subobjects of an object \(C\): for two subobjects \(a:A\to C\) and \(b:B\to C\), \(a\subseteq_{C}b\) if and only if there exists a morphism \(f:A\to B\) such that \(a=b\circ f\):

(47)

This preorder on the subobjects \(\mathrm{Sub}_{\mathbf{E}}(C)\) induces a preorder on the predicates \(\mathrm{Hom}_{\mathbf{E}}(C,\Omega)\) via the isomorphism. We can generalize this construction and define a preorder on other hom-sets:

**Definition 24**.: In a category \(\mathbf{E}\) with pullbacks, the inclusion preorder \(\subseteq_{C}\) on the set \(\mathrm{Sub}_{\mathbf{E}}(C)\) of subobjects of an object \(C\) and a subobject \(m:S\rightsquigarrow T\) induce a preorder \(\preceq_{C}^{m}\) on the hom-set \(\mathrm{Hom}_{\mathbf{E}}(C,T)\) via pullback of \(m\): for any two morphisms \(f_{1},f_{2}:C\to T\), \(f_{1}\preceq_{C}^{o}f_{2}\) if and only if \(f_{1}^{*}m\subseteq_{C}f_{2}^{*}m\).

\[\begin{CD}f^{*}S@>{}>{}>S\\ @V{f^{*}m}V{\rightsquigarrow}V@V{}V{m}V\\ C@>{}>{f}>T\end{CD}\] (48)

Based on this definition, we can explore the preorders on any hom-sets. From now, we assume that \(\mathbf{E}\) is a category with necessary structures that we need.

Then, the preorder of predicates is \(\preceq_{C}^{\top}\) on \(\mathrm{Hom}_{\mathbf{E}}(C,\Omega)\), and the preorder of quantities is \(\preceq_{C}^{o}\) on \(\mathrm{Hom}_{\mathbf{E}}(C,\Psi)\). By Lemma 7, we know that for two quantities \(q_{1},q_{2}:C\to\Psi\), \(q_{1}\preceq_{C}^{o}q_{2}\) if and only if \((\kappa\circ q_{1})\preceq_{C}^{\top}(\kappa\circ q_{2})\), which means that \(\mathrm{Hom}_{\mathbf{E}}(C,\kappa)\) is an order-preserving function from \((\mathrm{Hom}_{\mathbf{E}}(C,\Psi),\preceq_{C}^{o})\) to \((\mathrm{Hom}_{\mathbf{E}}(C,\Omega),\preceq_{C}^{\top})\).

Next, we will explore the structures of \(\mathrm{Hom}_{\mathbf{E}}(C,\Omega)\) and \(\mathrm{Hom}_{\mathbf{E}}(C,\Psi)\). To begin with, \(\top_{C}\) is a top in \(\mathrm{Hom}_{\mathbf{E}}(C,\Omega)\), and \(o_{C}\) is a top in \(\mathrm{Hom}_{\mathbf{E}}(C,\Psi)\), because \(\mathrm{id}_{C}\) is a top in \(\mathrm{Sub}_{\mathbf{E}}(C)\).

\[\begin{CD}C@>{}>{}>1@>{}>{}>1\\ @V{\mathrm{id}_{C}}V{\rightsquigarrow}V@V{}V{\rightsquigarrow}V\\ C@>{}>{o_{C}}>\Psi@>{}>{\kappa}>\Omega\end{CD}\] (49)

The inclusion preorder on \(\mathrm{Sub}_{\mathbf{E}}(C)\) also has a bottom -- the initial morphism \(i_{C}:0\rightsquigarrow C\) from an initial object \(0\) in the category \(\mathbf{E}\) to the object \(C\). Then, its classifying morphism \(\bot_{C}:C\to\Omega\) is a bottom in \(\mathrm{Hom}_{\mathbf{E}}(C,\Omega)\). It can be proven that \(\bot_{C}\) is a constant morphism with value \(\bot:1\to\Omega\), which is the classifying morphism of the initial/terminal morphism \(0\rightsquigarrow 1\). Any quantifying morphism \(\psi_{C}:C\to\Psi\) of \(i_{C}\) is a bottom in \(\mathrm{Hom}_{\mathbf{E}}(C,\Psi)\), but it is not necessarily a constant morphism.

\[\begin{CD}0@>{}>{}>1@>{}>{}>1\\ @V{}V{\rightsquigarrow}V@V{}V{}V\\ C@>{}>{\psi_{C}}V@>{}>{\kappa}>\Omega\end{CD}\] (50)

The preorder on the global elements plays a special role:

**Example 11**.: In \(\mathbf{Set}\), \(\preceq_{1}^{\top}\), also denoted by \(\vdash\), is a preorder on the set \(\{\top,\bot\}\) with only one non-identity relation \(\bot\vdash\top\).

**Example 12**.: In \(\mathbf{Set}\), \(\preceq_{1}^{0}\) is a preorder on the set \([0,\infty]\) where \(n\preceq_{1}^{0}0\) for any number \(n\), and \(m\preceq_{1}^{0}n\) and \(n\preceq_{1}^{0}m\) for any positive numbers \(m\) and \(n\).

By definition, \((\{\top,\bot\},\vdash)\) and \(([0,\infty],\preceq_{1}^{0})\) are equivalent. However, we can consider a _suborder_ of \(([0,\infty],\preceq_{1}^{0})\), e.g., the usual "_greater than or equal to_" \(\geq\) total order, to further differentiate positive numbers. Note that \(0\) remains the top in this suborder \(([0,\infty],\geq)\).

### Operation: algebra over the product endofunctor

In an elementary topos \(\mathbf{E}\), the subobjects \(\mathrm{Sub}_{\mathbf{E}}(C)\) of an object \(C\) not only forms a preorder by inclusion but also are equipped with certain _set operations_ (e.g., intersection and disjoint union), which are defined in terms of the universal properties of their corresponding _order operations_ (e.g., meet and join). Further, these operations are reflected in the structures of the subobject classifier (e.g., conjunction and disjunction).

Here, we establish a link between the structures of the subobject classifier and those of a subobject quantifier. Our primary result is as follows:

**Theorem 10**.: _Consider a category with a subobject classifier \(\top:1\rightarrowtail\Omega\), a subobject quantifier \(o:1\rightarrowtail\Psi\), and a quantizer \(\kappa:\Psi\rightarrow\Omega\)._

_Let \(n\) be a natural number. Let \(\beta:\Omega^{n}\rightarrow\Omega\) be an \(n\)-ary logical operation on \(\Omega\), and let \(\alpha:\Psi^{n}\rightarrow\Psi\) be an \(n\)-ary quantitative operation on \(\Psi\)._

_For \(i\in\{1,\ldots,n\}\), let \(p_{i}:C\rightarrow\Omega\) be a predicate on an object \(C\), and let \(q_{i}:C\rightarrow\Psi\) be a quantity on the object \(C\) such that \(p_{i}=\kappa\circ q_{i}\). Let \(p=\langle p_{1},\ldots,p_{n}\rangle\) be the upling of the predicates, and let \(q=\langle q_{1},\ldots,q_{n}\rangle\) be the upling of the quantities. Let \(b:B\rightarrowtail C:=(\beta\circ p)^{*}\top\) be the subobject classified by \(\beta\circ p\), and let \(a:A\rightarrowtail C:=(\alpha\circ q)^{*}o\) be the subobject quantified by \(\alpha\circ q\)._

(51)

_Then, we have_

1. \(\kappa^{n}\circ q=p\)__
2. \((\beta\circ\kappa^{n}\circ\alpha^{*}o=\top_{\alpha^{*}1})\rightarrow((\beta \circ p)\circ a=\top_{A})\)__
3. \((\beta\circ\kappa^{n}=\kappa\circ\alpha)\rightarrow(\beta\circ\kappa^{n} \circ\alpha^{o}=\top_{\alpha^{*}1})\)__
4. \((\beta\circ\kappa^{n}=\kappa\circ\alpha)\rightarrow(\kappa\circ(\alpha\circ q )=\beta\circ p)\)__
5. \((\beta\circ\kappa^{n}=\kappa\circ\alpha)\rightarrow((\alpha\circ q)\circ b= \alpha_{B})\)__

For convenience, we call an \(n\)-ary operation \(\alpha:\Psi^{n}\rightarrow\Psi\) (as an algebra over the product endofunctor \((-)^{n}\)) _homomorphic_ to \(\beta:\Omega^{n}\rightarrow\Omega\) via a morphism \(\kappa:\Psi\rightarrow\Omega\) if

\[\beta\circ\kappa^{n}=\kappa\circ\alpha\] (52)

and _subhomomorphic_ to \(\beta:\Omega^{n}\rightarrow\Omega\) if it satisfies the condition

\[\beta\circ\kappa^{n}\circ\alpha^{*}o=\top_{\alpha^{*}1}.\] (53)

Proof.: (i) follows from the property of tupling and product: \(\kappa^{n}\circ q=\langle\kappa\circ q_{1},\ldots,\kappa\circ q_{n}\rangle= \langle p_{1},\ldots,p_{n}\rangle=p\).

(ii) states that if \(\alpha\) is subhomomorphic to \(\beta\), then \(\beta\circ p\) is the classifying morphism of the subobject quantified by \(\alpha\circ q\).

\[\beta\circ p\circ a\] (54) \[=\beta\circ\kappa^{n}\circ q\circ a\] (i) (55) \[=\beta\circ\kappa^{n}\circ\alpha^{*}o\circ(\alpha^{*}o)^{*}q\] (pullback) (56) \[=\top_{\alpha^{*}1}\circ(\alpha^{*}o)^{*}q\] (subhomomorphism) (57) \[=\top_{A}\] (composition) (58)

(iii) means that \(\alpha\) being homomorphic to \(\beta\) is a stronger condition than being merely subhomomorphic to \(\beta\).

\[\beta\circ\kappa^{n}\circ\alpha^{*}o\] (59) \[=\kappa\circ\alpha\circ\alpha^{*}o\] (homomorphism) (60) \[=\kappa\circ o\circ\alpha^{*}\alpha\] (pullback) (61) \[=\top_{\alpha^{*}1}\] (composition) (62)

(iv) shows the relationship between the predicate \(\beta\circ p\) and the quantity \(\alpha\circ q\) when \(\alpha\) is homomorphic to \(\beta\).

\[\kappa\circ\alpha\circ q\] (63) \[=\beta\circ\kappa^{n}\circ q\] (homomorphism) (64) \[=\beta\circ p\] (i) (65)(v) means that if \(\alpha\) is homomorphic to \(\beta\), then \(\alpha\circ q\) is a quantifying morphism of \(b\).

\[\kappa\circ\alpha\circ q\circ b\] (66) \[=\beta\circ p\circ b\] (iv) (67) \[=\top_{B}\] (pullback) (68)

\(\alpha\circ q\circ b=o_{B}\) follows from Lemma 9. 

In summary, if \(\alpha\) is subhomomorphic to \(\beta\), then \(a\) is included in \(b\) ((ii)); if \(\alpha\) is homomorphic to \(\beta\), then \(a\) and \(b\) are isomorphic ((iii) and (v)). We consider this weaker condition because subhomomorphic but non-homomorphic operations may exhibit favorable properties in other aspects, such as continuity. We will discuss several concrete examples in the following subsections.

### Negation

First, let us take a closer look at a unary logical operation -- _negation_\(\neg:\Omega\to\Omega\), which is defined as the classifying morphism of \(\bot:1\to\Omega\). Recall that \(\bot\) is the classifying morphism of \(0\mapsto 1\).

Let us consider a unary quantitative operation \(\sim:\Psi\to\Psi\). If \(\sim\) is homomorphic to \(\neg\) via the quantizer \(\kappa\), it means that \(\neg\circ\kappa=\kappa\circ\sim\), or the following diagram commutes:

\[\begin{CD}\Psi@>{\sim}>{}>\Psi\\ @V{\kappa}V{}V@V{\kappa}V{\kappa}V\\ \Omega@>{}>{}>\Omega\end{CD}\] (69)

Let us consider the set \([0,\infty]\) in \(\mathbf{Set}\). A quantitative operation \(\sim:[0,\infty]\to[0,\infty]\)_homomorphic_ to the negation \(\neg:\{\top,\bot\}\to\{\top,\bot\}\) is a function

\[\sim(n):=[n=0]\times n_{0}=\begin{cases}n_{0}&n=0,\\ 0&n>0,\end{cases}\] (70)

which maps \(0\) to a non-zero number \(n_{0}\) and any non-zero number to \(0\).11 However, this function is discontinuous at \(0\). On the other hand, if we consider a quantitative operation \(\sim\)_subhomomorphic_ to the negation \(\neg\), then the only requirement is that for all \(n\), \(\sim(n)=0\) implies \(n>0\), or, by contraposition, \(\sim(0)>0\). Continuous choices include the _hinge function_\(n\mapsto 1\dot{\neg}n=\max\{1-n,0\}\), _reciprocal function_\(n\mapsto\frac{1}{n}\), and _exponential decay function_\(n\mapsto e^{-n}\) (see Fig. 3). Note that the latter

Figure 3: Negation

two are actually homomorphic to the constant false \(\bot\) because their outputs are always non-zero. The hinge function \(1\mathbin{\raisebox{0.86pt}{\scalebox{1.2}{$\perp$}}}q\), as discussed later, can be seen as derived from the implication \(p\to\bot\).

In this way, if we have a quantity \(q\) for a predicate \(p\), we can obtain a quantity \(\sim\!\!q\) for the negation \(\neg p\) of the predicate as well. If the quantitative operation \(\sim\) is subhomomorphic but not homomorphic to the logical operation \(\neg\), we can guarantee that for any \(x\), \(\sim\!\!q(x)=0\) implies \(\neg p(x)=\top\), but not vice versa.

### Conjunction

Next, let us move on to an important binary logical operation -- _conjunction_\(\wedge:\Omega\times\Omega\to\Omega\), which is defined as the classifying morphism of \((\top,\top):1\mapsto\Omega\times\Omega\). Similarly, we consider a binary quantitative operation \(\otimes:\Psi\times\Psi\to\Psi\) homomorphic to the conjunction \(\wedge\) via the quantizer \(\kappa\):

\[\begin{CD}\Psi\times\Psi@>{\otimes}>{}>\Psi\\ @V{\kappa\times\kappa}V{}V@V{}V{\kappa}V\\ \Omega\times\Omega@>{}>{\sim}>\Omega\end{CD}\] (71)

Figure 4: Conjunction Figure 5: Disjunction

Figure 6: Implication Figure 7: Equivalence

By abuse of notation, the conjunction \(\wedge\) also denotes a binary operation on the set \(\operatorname{Hom}_{\mathbf{E}}(C,\Omega)\) of predicates, such that for any two predicates \(p_{1},p_{2}\in\operatorname{Hom}_{\mathbf{E}}(C,\Omega)\),

\[p_{1}\wedge p_{2}:=\wedge\circ\langle p_{1},p_{2}\rangle.\] (72)

The same goes for the quantitative operation \(\otimes\).

For the conjunction, we can prove a stronger result:

**Theorem 11**.: _Consider a category with a subobject classifier \(\top:1\rightarrowtail\Omega\), a subobject quantifier \(o:1\rightarrowtail\Psi\), and a quantizer \(\kappa:\Psi\rightarrowtail\Omega\)._

_Let the conjunction \(\wedge:\Omega\times\Omega\rightarrowtail\Omega\) be the classifying morphism of \(\langle\top,\top\rangle:1\rightarrowtail\Omega\times\Omega\), and let \(\otimes:\Psi\times\Psi\rightarrowtail\Psi\) be a binary operation on \(\Psi\) homomorphic to the conjunction \(\wedge\) via the quantizer \(\kappa\)._

_Let \(p_{1},p_{2}:C\rightarrowtail\Omega\) be two predicates on an object \(C\), and let \(q_{1},q_{2}:C\rightarrowtail\Psi\) be two quantities on the object \(C\) such that \(p_{1}=\kappa\circ q_{1}\) and \(p_{2}=\kappa\circ q_{2}\). Let \(p=\langle p_{1},p_{2}\rangle\) be the pairing of the predicates, and let \(q=\langle q_{1},q_{2}\rangle\) be the pairing of the quantities. Let \(b:B\rightarrowtail C:=(p_{1}\wedge p_{2})^{*}\top\) be the subobject classified by \(p_{1}\wedge p_{2}\)._

(73)

_Then, \(\otimes\) is a quantifying morphism of \(\langle o,o\rangle\), and \(b\) is a pullback of \(\langle o,o\rangle\) along \(\langle q_{1},q_{2}\rangle\)._

Proof.: If \(q_{1}\otimes q_{2}=o_{C}\), then \((\kappa\circ q_{1})\wedge(\kappa\circ q_{2})=\top_{C}\), which leads to \(\langle\kappa\circ q_{1},\kappa\circ q_{2}\rangle=\langle\top,\top\rangle \circ e_{C}=\langle\top_{C},\top_{C}\rangle\) due to the universal property of pullback. Then, we have \(\kappa\circ q_{1}=\kappa\circ q_{2}=\top_{C}\) due to the universal property of pairing, and consequently \(q_{1}=q_{2}=o_{C}\) according to Lemma 9, i.e., \(\langle q_{1},q_{2}\rangle=\langle o,o\rangle\circ e_{C}\). Therefore, \(\langle o,o\rangle\) is a pullback of \(o\) along \(\otimes\). According to Theorem 10, \(b\) is a pullback of \(o\) along \(q_{1}\otimes q_{2}\). Then, following the pullback lemma, \(b\) is a pullback of \(\langle o,o\rangle\) along \(\langle q_{1},q_{2}\rangle\). 

In other words, the pullback square symbols in Eq. (73) are unambiguous -- the top row, the left column, the right column, the top-left square, and the top-right square are all pullbacks.

Note that for any quantities \(a,b,c:C\rightarrowtail\Psi\), we have

\[\kappa\circ((a\otimes b)\otimes c) =\kappa\circ(a\otimes(b\otimes c)),\] (74) \[\kappa\circ(a\otimes b) =\kappa\circ(b\otimes a),\] (75) \[\kappa\circ(o_{C}\otimes a) =\kappa\circ a,\] (76)

due to the associativity, commutativity, and unitality of the conjunction, but the quantitative operation \(\otimes\) is not required to satisfy these properties, i.e., it is possible that

\[(a\otimes b)\otimes c \neq a\otimes(b\otimes c),\] (77) \[a\otimes b \neq b\otimes a,\] (78) \[o_{C}\otimes a \neq a.\] (79)

However, in the following examples, we mainly consider quantitative operations \(\otimes\) such that these properties are satisfied. In such cases, \((\operatorname{Hom}_{\mathbf{E}}(C,\Psi),\otimes,o_{C})\) forms a commutative monoid, and consequently \(\operatorname{Hom}_{\mathbf{E}}(C,\kappa)\) is a monoid homomorphism from it to \((\operatorname{Hom}_{\mathbf{E}}(C,\Omega),\wedge,\top_{C})\).

In Appendix B.3, we introduced preorder structures on the predicates \(\operatorname{Hom}_{\mathbf{E}}(C,\Omega)\) and the quantities \(\operatorname{Hom}_{\mathbf{E}}(C,\Psi)\). The conjunction \(\wedge\) is a commutative monoidal structure compatible with the preorder \((\operatorname{Hom}_{\mathbf{E}}(C,\Omega),\preceq_{C}^{\top})\) because it is the meet operation. For the set \(\operatorname{Hom}_{\mathbf{E}}(C,\Psi)\) of quantities, we can choose the quantitative operation \(\otimes\) to be the meet operation as well. Alternatively, we can only require it to be compatible with the preorder, in the sense that the monoid product is order-preserving: for any quantities \(q_{1},q_{1}^{\prime},q_{2},q_{2}^{\prime}\in\operatorname{Hom}_{\mathbf{E}}(C,\Psi)\), if \(q_{1}\preceq_{C}^{o_{C}}q_{1}^{\prime}\) and \(q_{2}\preceq_{C}^{o_{C}}q_{2}^{\prime}\), then \(q_{1}\otimes q_{2}\preceq_{C}^{o_{C}}q_{1}^{\prime}\otimes q_{2}^{\prime}\). In other words, we require \((\operatorname{Hom}_{\mathbf{E}}(C,\Psi),\preceq_{C}^{o},\otimes,o_{C})\) to be a _symmetric monoidal preorder_[11, Definition 2.2].

**Example 13**.: Let us consider two commutative monoidal structures on the preorder \(([0,\infty],\geq)\). The max operation \(\max:[0,\infty]\times[0,\infty]\to[0,\infty]\) is the meet, i.e., cartesian product, while the addition \(+:[0,\infty]\times[0,\infty]\to[0,\infty]\) is a monoidal product. They are both semicartesian because the top \(0\) is the unit.

Note that \(([0,\infty],+,0)\), \(([0,1],\times,1)\), and \(([1,\infty],\times,1)\) are isomorphic to each other with the following isomorphisms:

(80)

We can also induce a monoidal structure on a set if it is isomorphic to a monoid:

**Lemma 12**.: _Let \(f:A\rightleftarrows B:g\) be a pair of bijections between sets \(A\) and \(B\). If \((B,\otimes_{B},I_{B})\) is a monoid, then \((A,\otimes_{A}:=g\circ\otimes_{B}\circ(f\times f),I_{A}:=g\circ I_{B})\) is also a monoid._

Proof.: Associativity:

\((a\otimes_{A}b)\otimes_{A}c\) (81) \(=g((f(a)\otimes_{B}f(b))\otimes_{B}f(c))\) (82) \(=g(f(a)\otimes_{B}(f(b)\otimes_{B}f(c)))\) (83) \(=a\otimes_{A}(b\otimes_{A}c)\) (84) Left untality:

\(I_{A}\otimes_{A}a\) (85) \(=g(I_{B})\otimes_{A}a\) (86) \(=g(f(g(I_{B}))\otimes_{B}f(a))\) (87) \(=g(I_{B}\otimes_{B}f(a))\) (88) \(=g(f(a))\) (89) \(=a\) Right untality can be proven similarly. 

In this way, we can obtain a richer choice of monoidal structures on the set \([0,\infty]\) beyond the addition (see Fig. 4):

**Example 14** (Semicartesian monoidal product).: \[e^{x}-1:([0,\infty],\otimes,0) \rightleftarrows([0,\infty],+,0):\log(1+x) a\otimes b:=\log(e^{a}+e^{b}-1)\] (91) \[x^{2}:([0,\infty],\otimes,0) \rightleftarrows([0,\infty],+,0):\sqrt{x} a\otimes b:=\sqrt{a^{2}+b^{2}}\] (92) \[\sqrt{x}:([0,\infty],\otimes,0) \rightleftarrows([0,\infty],+,0):x^{2} a\otimes b:=a+b+2\sqrt{ab}\] (93) \[x+1:([0,\infty],\otimes,0) \rightleftarrows([1,\infty],\times,1):x-1 a\otimes b:=a+b+ab\] (94)

In summary, the max operation on \([0,\infty]\) can be regarded as a _continuous conjunction_, whereas a semicartesian monoidal product, such as the addition, can be viewed as a _soft max_.

### Disjunction

Dually, the _disjunction_\(\vee:\Omega\times\Omega\to\Omega\) reflects the join of the inclusion preorder of subobjects. Similarly, we want to find a quantitative operation \(\oplus:\Psi\times\Psi\to\Psi\) homomorphic to the disjunction \(\vee\) via the quantizer \(\kappa\):

(95)

Using the same technique as in Lemma 12, we can obtain several monoidal structures on the set \([0,\infty]\) homomorphic to the disjunction (see Fig. 5):

**Example 15** (Semicocartesian monoidal product).: \[\frac{1}{x}:([0,\infty],\oplus,\infty) \rightleftarrows([0,\infty],+,0):\frac{1}{x} a\oplus b:=\frac{ab}{a+b}\] (96) \[\frac{1}{x^{2}}:([0,\infty],\oplus,\infty) \rightleftarrows([0,\infty],+,0):\frac{1}{\sqrt{x}} a\oplus b:=\frac{ab}{\sqrt{a^{2}+b^{2}}}\] (97) \[1-e^{-x}:([0,\infty],\oplus,\infty) \rightleftarrows([0,1],\times,1):-\log(1-x) a\oplus b:=-\log(e^{-a}+e^{-b}-e^{-(a+b)})\] (98) \[\tanh:([0,\infty],\oplus,\infty) \rightleftarrows([0,1],\times,1):\operatorname{arctanh} a\oplus b:=\operatorname{arctanh}(\tanh(a)\tanh(b))\] (99)

However, we usually choose the quantitative operation \(\oplus\) to be the join operation \(\min\) of the preorder \(([0,\infty],\geq)\). In this way, \(\otimes\) distributes over \(\oplus\), i.e., for any quantities \(a,b,c:C\to\Psi\), we have

\[a\otimes(b\oplus c)=(a\otimes b)\oplus(a\otimes c).\] (100)

A typical example is the min-plus semiring \(([0,\infty],\min,+)\)(Pin, 1998).

### Implication

Finally, let us construct a quantitative counterpart of the _implication_\(\to:\Omega\times\Omega\to\Omega\), which is right adjoint to the conjunction \(\wedge\). For global elements \(a,b,c:1\to\Omega\), this means that

\[c\wedge a\vdash b\text{ if and only if }c\vdash a\to b.\] (101)

There are two ways to construct a quantitative operation \(\multimap:\Psi\times\Psi\to\Psi\) corresponding to the implication \(\to\). One way is to find a quantitative operation \(\multimap\) homomorphic to the implication \(\to\) via the quantizer \(\kappa\):

\[\begin{CD}\Psi\times\Psi@>{\multimap}>{\multimap}>{\Psi}\\ @V{\kappa\times\kappa}V{}V@V{}V{\kappa}V\\ \Omega\times\Omega@>{}>{\longrightarrow}>\Omega\end{CD}\] (102)

**Example 16**.: For the set \([0,\infty]\), a quantitative operation \(\multimap:[0,\infty]\times[0,\infty]\to[0,\infty]\) homomorphic to the implication \(\to:\{\top,\bot\}\times\{\top,\bot\}\to\{\top,\bot\}\) is a function

\[(a,b)\mapsto[a=0]\times[b>0]\times f(b)=\begin{cases}f(b)&a=0\text{ and }b>0,\\ 0&\text{otherwise},\end{cases}\] (103)

where \(f:(0,\infty]\to(0,\infty]\) is an arbitrary function to non-zero numbers. Note that this function is discontinuous at the line \(a=0\) and \(b>0\) (cf. Appendix B.5).

The other way is to find a quantitative operation \(\multimap\) right adjoint to an operation \(\otimes\) homomorphic to the conjunction \(\wedge\), i.e., the _internal hom_ of the _monoidal closed preorder_(Fong and Spivak, 2019, Definition 2.79).

**Example 17**.: For the meet-semilattice \(([0,\infty],\geq,\max,0)\), the function

\[(a,b)\mapsto[a<b]\times b=\begin{cases}0&a\geq b,\\ b&a<b,\end{cases}\] (104)

is right adjoint to the max because

\[\max\{c,a\}\geq b\text{ if and only if }c\geq\begin{cases}0&a\geq b,\\ b&a<b.\end{cases}\] (105)

While this function is subhomomorphic to the implication, it is still discontinuous at the line \(a=b\).

**Example 18**.: For the monoidal preorder \(([0,\infty],\geq,+,0)\), the truncated subtraction \(\multimap\) (a.k.a. _monus_(Amer, 1984))

\[b\mathbin{\dot{\phantom{\dot{a}}}}a\mathrel{\mathop{:}}=\max\{b-a,0\}= \begin{cases}0&a\geq b,\\ b-a&a<b,\end{cases}\] (106)is right adjoint to the addition because

\[c+a\geq b\text{ if and only if }c\geq b\doteq a.\] (107)

The truncated subtraction is continuous and subhomomorphic to the implication. Note that the hinge function \(n\mapsto\max\{1-n,0\}=1\doteq n\) for the negation can be interpreted as the quantitative operation derived from \(\neg n=n\to\bot\), where \(1\) is homomorphic to the constant false \(\bot\).

Similarly to Lemma 12, if two symmetric monoidal preorders are isomorphic and one of them is closed, we can induce that the other is also closed:

**Lemma 13**.: _Let \(f:A\rightleftarrows B:g\) be a pair of isomorphisms between symmetric monoidal preorders \((A,\preceq_{A},\otimes_{A},I_{A})\) and \((B,\preceq_{B},\otimes_{B},I_{B})\). If \((B,\preceq_{B},\otimes_{B},I_{B},\multimap_{B})\) is closed, then \((A,\preceq_{A},\otimes_{A},I_{A},\multimap_{A}:=g\circ\multimap_{B}\circ(f \times f))\) is also closed._

Proof.: For all \(a,b,c\in A\), we have

\[c\preceq_{A}(a\multimap_{A}b)\] \[\equiv c\preceq_{A}g(f(a)\multimap_{B}f(b))\] (108) \[\equiv f(c)\preceq_{B}f(a)\multimap_{B}f(b)\] (109) \[\equiv f(c)\otimes_{B}f(a)\preceq_{B}f(b)\] (110) \[\equiv c\otimes_{A}a\preceq_{A}b\] (111)

This means that \(\multimap_{A}\) is right adjoint to \(\otimes_{A}\). 

In this way, we can find the internal homs corresponding to the monoidal products discussed in Appendix B.6 (see Fig. 6).

As a side note, we can use the quantitative operations discussed above to define quantitative operations for other logical connectives. For example, the logical equivalence \(a\leftrightarrow b\) can be represented as \((a\to b)\wedge(b\to a)\) (bi-implication), \((\neg a\lor b)\wedge(\neg b\lor a)\) (conjunctive normal form (CNF)), or \((a\wedge b)\vee(\neg a\wedge\neg b)\) (disjunctive normal form (DNF)), and its quantitative operations can be defined accordingly. Some examples are shown in Fig. 7.

### Heyting algebra, quantale, and ordered semiring

Now, having constructed the logical operations and their corresponding quantitative operations, we can compare the structures of the subobject classifier with those of a subobject quantifier.

It is known that the global elements of the subobject classifier with the logical operations form a Heyting algebra \((\Omega,\vdash,\top,\bot,\wedge,\vee,\rightarrow)\):

**Definition 25**.: A _ Heyting algebra_ is a cartesian closed bounded lattice.

In categorical terms, \(\top\) is the terminal object, \(\bot\) is the initial object, \(\wedge\) is the product, \(\vee\) is the coproduct, and \(\rightarrow\) is the exponential in the preorder \((\Omega,\vdash)\) (as a thin category).

We weakened the requirements to construct the algebraic structures of the subobject quantifier, which usually forms what is called a quantale \((\Psi,\preceq,1,0,\otimes,\oplus,\multimap)\)[10, 10]:

**Definition 26**.: A (unital) _quantale_ is a monoidal closed sublattice.

This means that we can consider a preorder \((\Psi,\preceq)\) on the subobject quantifier as a thin category, where \(1\) is the terminal object, \(0\) is the initial object, \(\otimes\) is a monoidal product and not necessarily the product, \(\oplus\) is still the coproduct, and \(\multimap\) is the internal hom right adjoint to the monoidal product \(\otimes\). An example is the Lawvere quantale \(([0,\infty],\geq,0,\infty,+,\min,\multimap)\)[10, 11]. Then, the quantizer \(\kappa:\Psi\rightarrow\Omega\) is a homomorphism preserving some or all the structures.

Note that due to the adjoint functor theorem and the fact that left adjoints preserve colimits, the product distributes over the coproduct in a Heyting algebra, and the monoidal product distributes over the coproduct in a quantale. We can further relax the requirement for \(\oplus\) to be the coproduct and instead consider a monoidal product (Appendix B.7). If we still require the distributivity for the two monoidal structures \(\otimes\) and \(\oplus\), the algebraic structure is an _ordered semiring_[10]. Further investigation is left for future work.

### Quantifier: algebra over the exponentiation endofunctor

Up to this point, our focus has been on \(n\)-ary operations in propositional logic (Appendix B.4). Next, we introduce the universal quantification \(\forall\) and existential quantification \(\exists\) used in predicate logic and their quantitative counterparts.

Externally, the universal quantification and the existential quantification are right and left adjoint to the pullback of projection, respectively [1969]. Internally, the universal quantifier \(\forall_{D}:\Omega^{D}\to\Omega\) and existential quantifier \(\exists_{D}:\Omega^{D}\to\Omega\) are given by morphisms from the power object \(\Omega^{D}\) of an object \(D\) to the subobject classifier \(\Omega\).

Recall that the power object \(\Omega^{D}\) is also an exponential object into the subobject classifier \(\Omega\), so the universal quantifier \(\forall_{D}\) and existential quantifier \(\exists_{D}\) can also be viewed as algebras over the exponentiation endofunctor \((-)^{D}\) of exponentiation on the subobject classifier \(\Omega\). Then, we can consider algebras on the subobject quantifier \(\Psi\) homomorphic to them, which serve as quantitative counterparts of these quantifiers.

Our main result is as follows (cf. Theorem 10):

**Theorem 14**.: _Consider an elementary topos with a subobject classifier \(\top:1\rightarrowtail\Omega\), a subobject quantifier \(o:1\rightarrowtail\Psi\), and a quantizer \(\kappa:\Psi\rightarrow\Omega\)._

_Let \(p:C\times D\rightarrow\Omega\) be a predicate on a product, and let \(q:C\times D\rightarrow\Psi\) be a quantity such that \(p=\kappa\circ q\). Let \(\widehat{p}:C\rightarrow\Omega^{D}\) and \(\widehat{q}:C\rightarrow\Psi^{D}\) be their exponential transposes._

_Let \(\beta_{D}:\Omega^{D}\rightarrow\Omega\) be a predicate, and let \(\alpha_{D}:\Psi^{D}\rightarrow\Psi\) be a quantity homomorphic to \(\beta_{D}\) via the quantizer \(\kappa\), i.e., \(\beta_{D}\circ\kappa^{D}=\kappa\circ\alpha_{D}\)._

_Let \(b:B\rightarrowtail C:=(\beta_{D}\circ\widehat{p})^{*}\top\) be the subobject classified by \(\beta_{D}\circ\widehat{p}\), and let \(a:A\rightarrowtail C:=(\alpha_{D}\circ\widehat{q})^{*}o\) be the subobject quantified by \(\alpha_{D}\circ\widehat{q}\)._

(113)

_Then, we have_

1. \(\kappa^{D}\circ\widehat{q}=\widehat{p}\)__
2. \(\kappa\circ(\alpha_{D}\circ\widehat{q})=\beta_{D}\circ\widehat{p}\)__
3. \((\beta_{D}\circ\widehat{p})\circ a=\top_{A}\)__
4. \((\alpha_{D}\circ\widehat{q})\circ b=o_{B}\)__

Proof.: (i) follows from the property of exponential.

(ii) shows the relationship between the predicate \(\beta_{D}\circ\widehat{p}\) and the quantity \(\alpha_{D}\circ\widehat{q}\) when \(\alpha_{D}\) is homomorphic to \(\beta_{D}\).

\[\kappa\circ\alpha_{D}\circ\widehat{q}\] (114) \[=\beta_{D}\circ\kappa^{D}\circ\widehat{q}\] (homomorphism) (115) \[=\beta_{D}\circ\widehat{p}\] (i)

(iii) means that \(\beta_{D}\circ\widehat{p}\) is a classifying morphism of \(a\).

\[\beta_{D}\circ\widehat{p}\circ a\] (117) \[=\kappa\circ\alpha_{D}\circ\widehat{q}\circ a\] (ii) (118) \[=\kappa\circ o_{A}\] (pullback) (119) \[=\top_{A}\] (composition) (120)(iv) means that \(\alpha_{D}\circ\widehat{q}\) is a quantifying morphism of \(b\).

\[\kappa\circ\alpha_{D}\circ\widehat{q}\circ b\] (121) \[=\beta_{D}\circ\widehat{p}\circ b\] (ii) (122) \[=\top_{B}\] (pullback) (123)

\(\alpha_{D}\circ\widehat{q}\circ b=o_{B}\) follows from Lemma 9. 

**Definition 27** (Universal aggregator).: **A universal aggregator**\(\triangledown_{D}:\Psi^{D}\to\Psi\) is a quantity that is homomorphic to the universal quantifier \(\forall_{D}:\Omega^{D}\to\Omega\):

(124)

**Definition 28** (Existential aggregator).: An _existential aggregator_\(\vartriangle_{D}:\Psi^{D}\to\Psi\) is a quantity that is homomorphic to the existential quantifier \(\exists_{D}:\Omega^{D}\to\Omega\):

(125)

**Example 19**.: For the set \([0,\infty]\), the canonical choices of universal aggregator \(\triangledown_{D}\) and existential aggregator \(\vartriangle_{D}\) are \(\sup\) and \(\inf\). If the set \(D\) is finite, we can also use \(\operatorname{sum}\) and \(\operatorname{mean}\) as the universal aggregator. Non-examples of universal aggregator include \(\operatorname{median}\) and \(\operatorname{mode}\), which are not homomorphic to the universal quantifier.

Note that the universal quantifier and existential quantifier are commutative up to isomorphism:

\[\forall_{A\times B} \cong\forall_{B}\circ\forall_{A}^{B}\cong\forall_{A}\circ\forall _{B}^{A}\cong\forall_{B\times A},\] (126) \[\exists_{A\times B} \cong\exists_{B}\circ\exists_{A}^{B}\cong\exists_{A}\circ\exists _{B}^{A}\cong\exists_{B\times A}.\] (127)

However, we are free to choose different aggregators for different objects that are not necessarily commutative. For example, the sum of max is usually not equal to the max of sum.

### Enrichment

Lastly, we describe the conversion based on enrichment.

First, let us define the enriching category:

**Definition 29**.: Let \((\Psi,\preceq,\otimes,\oplus,-\infty)\) be an internal quantale object in \(\mathbf{Set}\). We define \(\Psi\)-\(\mathbf{Set}\) to be a category whose objects are tuples consisting of a set \(C\), a \(\Psi\)-valued strict premetric \(d_{C}\) on \(C\), and universal and existential aggregators on \(C\):

\[(C,d_{C}:C\times C\to\Psi,\triangledown_{C},\vartriangle_{C}:\Psi^{C}\to\Psi),\] (128)

and morphisms from \((A,d_{A},\triangledown_{A},\vartriangle_{A})\) to \((B,d_{B},\triangledown_{B},\vartriangle_{B})\) are functions \(f:A\to B\).

**Definition 30**.: A bifunctor \(\boxtimes\) on \(\Psi\)-\(\mathbf{Set}\) is given by the Cartesian product of sets and functions, together with the following products of strict premetrics and aggregators:

\[d_{A}\boxtimes d_{B}: (A\times B)\times(A\times B)\cong(A\times A)\times(B\times B) \xrightarrow{d_{A}\times d_{B}}\Psi\times\Psi\xrightarrow{\otimes}\Psi.\] (129) \[\triangledown_{A}\boxtimes\triangledown_{B}: \Psi^{A\times B}\cong(\Psi^{A})^{B}\xrightarrow{\triangledown_{ B}^{B}}\Psi^{B}\xrightarrow{\triangledown_{B}}\Psi,\] (130) \[\vartriangle_{A}\boxtimes\vartriangledown_{B}: \Psi^{A\times B}\cong(\Psi^{A})^{B}\xrightarrow{\vartriangle_{ B}^{B}}\Psi^{B}\xrightarrow{\vartriangle_{B}}\Psi.\] (131)

**Proposition 15**.: _The product \(d_{A}\boxtimes d_{B}\) of strict premetrics given in Definition 30 is again a strict premetric._Proof.: This is a result of Theorem 11. Consider the following diagram:

(132)

\(\Delta_{A}\times\Delta_{B}\) is a pullback of \(o\) along \(\otimes\circ(d_{A}\times d_{B})\) because \(\langle o,o\rangle\) is a pullback of \(o\) along \(\otimes\) according to Theorem 11, \(\Delta_{A}\times\Delta_{B}\) is a pullback of \(\langle o,o\rangle\) along \(d_{A}\times d_{B}\), and we can apply the pullback lemma. \(\Delta_{A}\times\Delta_{B}\) is isomorphic to \(\Delta_{A\times B}\), which means that \(\otimes\circ(d_{A}\times d_{B})\) is a strict premetric. 

Based on this definition, we can show that

\[(d_{A}\boxtimes d_{B})\boxtimes d_{C}\cong d_{A}\boxtimes(d_{B}\boxtimes d_{C}),\] (133)

because \(\times\) and \(\otimes\) are associative up to isomorphism.

Further, we have

\[(\triangledown_{A}\boxtimes\triangledown_{B})\boxtimes\triangledown_{C} \cong\triangledown_{A}\boxtimes(\triangledown_{B}\boxtimes \triangledown_{C}),\] (134) \[(\vartriangle_{A}\boxtimes\vartriangle_{B})\boxtimes\vartriangle_{C} \cong\vartriangle_{A}\boxtimes(\vartriangle_{B}\boxtimes\vartriangle_{C}),\] (135)

because composition is associative.

The singleton \((\{\ast\},o_{\{\ast\}\times\{\ast\}},\operatorname{id}_{\{\ast\}}, \operatorname{id}_{\{\ast\}})\) is the unit of the bifunctor \(\boxtimes\). In this way, \((\Psi\text{-}\mathbf{Set},\boxtimes,\{\ast\})\) forms a monoidal category.

However, note that the bifunctor \(\boxtimes\) is not symmetric because \(\triangledown_{A}\boxtimes\triangledown_{B}\) and \(\vartriangle_{A}\boxtimes\vartriangle_{B}\) are not necessarily symmetric, and \(d_{A}\boxtimes d_{B}\cong d_{B}\boxtimes d_{A}\) if and only if the monoidal product \(\otimes\) of the quantale \(\Psi\) is commutative.

Since \(\Psi\text{-}\mathbf{Set}\) is a monoidal category, we can consider a strict monoidal functor \(F_{\Psi}:\mathbf{Set}\to\Psi\text{-}\mathbf{Set}\), which equips products of sets with product strict premetrics and product aggregators:

\[d_{A\times B} :=d_{A}\boxtimes d_{B},\] (136) \[\triangledown_{A\times B} :=\triangledown_{A}\boxtimes\triangledown_{B},\] (137) \[\vartriangle_{A\times B} :=\vartriangle_{A}\boxtimes\vartriangle_{B}.\] (138)

Such a monoidal functor induces a functor from a (\(\mathbf{Set}\)-enriched) category to a \(\Psi\text{-}\mathbf{Set}\)-enriched category, called the base change of enriching category. This means that we have a systematic way to equip a set \([A,B]\) of morphisms with a strict premetric

\[d_{[A,B]}:[A,B]\times[A,B]\to\Psi,\] (139)

a universal aggregator

\[\triangledown_{[A,B]}:\Psi^{[A,B]}\to\Psi,\] (140)

and an existential aggregator

\[\vartriangle_{[A,B]}:\Psi^{[A,B]}\to\Psi,\] (141)

which is compatible with the product.

Then, Theorem 1 is a special case of this enrichment. The relationship between predicates and quantities follows from Theorems 10 and 14.

### Summary

Finally, to accommodate readers without a background in category theory, we present the instantiated definitions and theoretical results free of categorical terminology. The non-categorical proofs are omitted. We will be using the following functions:

* the _zero predicate_\(\zeta:[0,\infty]\to\{\top,\bot\}:=x\mapsto(x=0)\),
* the _product_\(\zeta^{n}:[0,\infty]^{n}\to\{\top,\bot\}^{n}:(q_{1},\ldots,q_{n})\mapsto(q_{1 }=0,\ldots,q_{n}=0)\), and
* the _postcomposition_\(\zeta^{A}:[0,\infty]^{A}\to\{\top,\bot\}^{A}:=q\mapsto\zeta\circ q\) of the zero predicate.

**Definition 31** (Quantity).: A quantity \(q:A\rightarrow[0,\infty]\) is _homomorphic_ to a predicate \(p:A\rightarrow\{\top,\bot\}\) if \(p=\zeta\circ q\):

\[\forall a\in A.\ (q(a)=0)\leftrightarrow p(a).\] (142)

A quantity \(q\) is _subhomomorphic_ to a predicate \(p\) if \(\zeta\circ q\to p\):

\[\forall a\in A.\ (q(a)=0)\to p(a).\] (143)

**Example 20**.: A _strict premetric \(d_{A}:A\times A\rightarrow[0,\infty]\)_ is a quantity on the product set \(A\times A\) homomorphic to the equality predicate \(=_{A}:A\times A\rightarrow\{\top,\bot\}\).

**Definition 32** (Quantitative operation).: Let \(n\in\mathbb{N}\) be a natural number. A quantitative operation \(\alpha:\left[0,\infty\right]^{n}\rightarrow[0,\infty]\) is _homomorphic_ to a logical operation \(\beta:\left\{\top,\bot\right\}^{n}\rightarrow\{\top,\bot\}\) via the zero predicate \(\zeta\) if \(\zeta\circ\alpha=\beta\circ\zeta^{n}\):

\[\forall(q_{1},\ldots,q_{n})\in\left[0,\infty\right]^{n}.\ (\alpha(q_{1}, \ldots,q_{n})=0)\leftrightarrow\beta(q_{1}=0,\ldots,q_{n}=0).\] (144)

A quantitative operation \(\alpha\) is _subhomomorphic_ to a logical operation \(\beta\) via the zero predicate if \(\zeta\circ\alpha\rightarrow\beta\circ\zeta^{n}\):

\[\forall(q_{1},\ldots,q_{n})\in\left[0,\infty\right]^{n}.\ (\alpha(q_{1}, \ldots,q_{n})=0)\rightarrow\beta(q_{1}=0,\ldots,q_{n}=0).\] (145)

The relationship between quantitative operations and logical operations is as follows (Theorem 10):

**Proposition 16**.: _Let \(n\in\mathbb{N}\) be a natural number. For \(i\in\{1,\ldots,n\}\), let \(p_{i}:A\rightarrow\{\top,\bot\}\) be a predicate, and let \(q_{i}:A\rightarrow[0,\infty]\) be a quantity. Let \(p:A\rightarrow\{\top,\bot\}^{n}:=\left\langle p_{1},\ldots,p_{n}\right\rangle\) be the tupling of the predicates, and let \(q:A\rightarrow[0,\infty]^{n}:=\left\langle q_{1},\ldots,q_{n}\right\rangle\) be the tupling of the quantities. Let \(\alpha:\left[0,\infty\right]^{n}\rightarrow[0,\infty]\) be a quantitative operation, and let \(\beta:\left\{\top,\bot\right\}^{n}\rightarrow\{\top,\bot\}\) be a logical operation. Assume that for all \(i\in\{1,\ldots,n\}\), \(q_{i}\) is homomorphic to \(p_{i}\). Then,_

* _if_ \(\alpha\) _is homomorphic to_ \(\beta\)_,_ \(\alpha\circ q\) _homomorphic to_ \(\beta\circ p\)_; and_
* _if_ \(\alpha\) _is subhomomorphic to_ \(\beta\)_,_ \(\alpha\circ q\) _subhomomorphic to_ \(\beta\circ p\)_._

In fact, we can also show that if for all \(i\in\{1,\ldots,n\}\), \(q_{i}\) is subhomomorphic to \(p_{i}\), and \(\alpha\) is subhomomorphic to \(\beta\), then \(\alpha\circ q\) is subhomomorphic to \(\beta\circ p\).

**Definition 33** (Aggregator).: An aggregator \(\alpha_{A}:\left[0,\infty\right]^{A}\rightarrow[0,\infty]\) is _homomorphic_ to a quantifier \(\beta_{A}:\{\top,\bot\}^{A}\rightarrow\{\top,\bot\}\) via the zero predicate \(\zeta\) if \(\zeta\circ\alpha_{A}=\beta_{A}\circ\zeta^{A}\):

\[\forall q\in\left[0,\infty\right]^{A}.\ \bigg{(}\mathop{\left(\mathop{ \alpha}\limits_{a\in A}q(a)\right)}=0\bigg{)}\leftrightarrow\bigg{(}\mathop{ \beta}\limits_{a\in A}\left(q(a)=0\right)\bigg{)}.\] (146)

**Example 21**.: A _universal aggregator \(\triangledown_{A}:\left[0,\infty\right]^{A}\rightarrow[0,\infty]\)_ is a function such that

\[\forall q\in\left[0,\infty\right]^{A}.\ \bigg{(}\mathop{\left(\mathop{ \triangledown}\limits_{a\in A}q(a)\right)}=0\bigg{)}\leftrightarrow(\forall a \in A.\ q(a)=0).\] (147)

**Example 22**.: An _existential aggregator \(\triangle_{A}:\left[0,\infty\right]^{A}\rightarrow[0,\infty]\)_ is a function such that

\[\forall q\in\left[0,\infty\right]^{A}.\ \bigg{(}\mathop{\left(\mathop{ \alpha}\limits_{a\in A}q(a)\right)}=0\bigg{)}\leftrightarrow(\exists a\in A. \ q(a)=0).\] (148)

The relationship between aggregators and quantifiers is as follows (Theorem 14):

**Proposition 17**.: _Let \(p:A\times B\rightarrow\{\top,\bot\}\) be a predicate, and let \(q:A\times B\rightarrow[0,\infty]\) be a quantity. Let \(\widehat{p}:B\rightarrow\{\top,\bot\}^{A}\) and \(\widehat{q}:B\rightarrow\left[0,\infty\right]^{A}\) be the exponential transposes of \(p\) and \(q\). Let \(\alpha_{A}:\left[0,\infty\right]^{A}\rightarrow[0,\infty]\) be an aggregator, and let \(\beta_{A}:\{\top,\bot\}^{A}\rightarrow\{\top,\bot\}\) be a quantifier. Then, if \(q\) is homomorphic to \(p\), and \(\alpha\) is homomorphic to \(\beta\), then \(\alpha_{A}\circ\widehat{q}\) is homomorphic to \(\beta_{A}\circ\widehat{p}\)._

We have a compositional way to assign a strict premetric and an aggregator to a product of sets:

**Proposition 18**.: _Let \(d_{A}:A\times A\rightarrow[0,\infty]\) and \(d_{B}:B\times B\rightarrow[0,\infty]\) be strict premetrics. Then,_

\[d_{A\times B}:(A\times B)\times(A\times B)\rightarrow[0,\infty]:=((a,b),(a^{ \prime},b^{\prime}))\mapsto d(a,a^{\prime})+d(b,b^{\prime})\] (149)

_is a strict premetric on the product set \(A\times B\)._

**Proposition 19**.: _Let \(\triangledown_{A}:\left[0,\infty\right]^{A}\rightarrow[0,\infty]\) and \(\triangledown_{B}:\left[0,\infty\right]^{B}\rightarrow[0,\infty]\) be universal aggregators. Then,_

\[\mathop{\triangledown}\limits_{A\times B}:\left[0,\infty\right]^{A\times B} \rightarrow[0,\infty]:=q\mapsto\mathop{\triangledown}\limits_{b\in B} \mathop{\triangledown}\limits_{a\in A}q(a,b)\] (150)

_is a universal aggregator on the product set \(A\times B\)._Proofs

### Proposition 2

Proof.: \[q_{\text{product}}(m:Y\to Z)\] (151) \[=\inf_{m_{1,1}\in[Y_{1},Z_{1}]}\inf_{m_{2,2}\in[Y_{2},Z_{2}]}d_{[Y,Z] }(m,m_{1,1}\times m_{2,2})\] (152) \[=\inf_{m_{1,1}\in[Y_{1},Z_{1}]}\inf_{m_{2,2}\in[Y_{2},Z_{2}]}(d_{[Y,Z_{1}]}(m_{1},m_{1,1}\circ p_{1})+d_{[Y,Z_{2}]}(m_{2},m_{2,2}\circ p_{2}))\] (153) \[=\inf_{m_{1,1}\in[Y_{1},Z_{1}]}d_{[Y,Z_{1}]}(m,m_{1,1}\circ p_{1}) +\inf_{m_{2,2}\in[Y_{2},Z_{2}]}d_{[Y,Z_{2}]}(m_{2},m_{2,2}\circ p_{2})\] (154) \[=\inf_{m_{1,1}\in[Y_{1},Z_{1}]}\mathop{\mathbb{V}}_{y\in Y}d_{Z_{1 }}(m_{1}(y),m_{1,1}(y_{1}))+\inf_{m_{2,2}\in[Y_{2},Z_{2}]}\mathop{\mathbb{V}}_ {y\in Y}d_{Z_{2}}(m_{2}(y),m_{2,2}(y_{2}))\] (155) \[=\inf_{m_{1,1}\in[Y_{1},Z_{1}]}\mathop{\mathbb{V}}_{y_{1}\in Y_{1 }}\mathop{\mathbb{V}}_{y_{2}\in Y_{2}}d_{Z_{1}}(m_{1}(y_{1},y_{2}),m_{1,1}(y_{ 1}))\] \[+\inf_{m_{2,2}\in[Y_{2},Z_{2}]}\mathop{\mathbb{V}}_{y_{2}\in Y_{2 }}\mathop{\mathbb{V}}_{y_{1}\in Y_{1}}\mathop{\mathbb{V}}_{Z_{2}}(m_{2}(y_{1},y_{2}),m_{2,2}(y_{2}))\] (156) \[=\mathop{\mathbb{V}}_{y_{1}\in Y_{1}}\mathop{\mathbb{V}}_{y_{2} \in Y_{2}}d_{Z_{1}}(m_{1}(y_{1},y_{2}),m_{1,1}^{*}(y_{1}))\] \[+\mathop{\mathbb{V}}_{y_{2}\in Y_{2}}\mathop{\mathbb{V}}_{y_{1} \in Y_{1}}d_{Z_{2}}(m_{2}(y_{1},y_{2}),m_{2,2}^{*}(y_{2})),\] (157)

where

\[m_{1,1}^{*} :=\mathop{\arg\inf}_{m_{1,1}\in[Y_{1},Z_{1}]}\mathop{\mathbb{V}}_{y_{1 }\in Y_{1}}\mathop{\mathbb{V}}_{y_{2}\in Y_{2}}d_{Z_{1}}(m_{1}(y_{1},y_{2}),m _{1,1}(y_{1}))\] (158) \[=y_{1}\mapsto\mathop{\arg\inf}_{z_{1}\in Z_{1}}\mathop{\mathbb{V} }_{y_{2}\in Y_{2}}d_{Z_{1}}(m_{1}(y_{1},y_{2}),z_{1}),\] (159) \[m_{2,2}^{*} :=\mathop{\arg\inf}_{m_{2,2}\in[Y_{2},Z_{2}]}\mathop{\mathbb{V} }_{y_{2}\in Y_{2}}\mathop{\mathbb{V}}_{y_{1}\in Y_{1}}d_{Z_{2}}(m_{2}(y_{1},y_{ 2}),m_{2,2}(y_{2}))\] (160) \[=y_{2}\mapsto\mathop{\arg\inf}_{z_{2}\in Z_{2}}\mathop{\mathbb{V} }_{y_{1}\in Y_{1}}d_{Z_{2}}(m_{2}(y_{1},y_{2}),z_{2}).\] (161)

### Proposition 3

Proof.: \[q_{\text{const-curly}}(m:Y\to Z)\] (162) \[=q_{\text{const}}(\widehat{m_{1}})+q_{\text{const}}(\widehat{m_{2}})\] (163) \[=\mathop{\mathbb{V}}_{y_{2}\in Y_{2}}\mathop{\mathbb{V}}_{y_{2 }^{\prime}\in Y_{2}}d_{[Y_{1},Z_{1}]}(\widehat{m_{1}}(y_{2}),\widehat{m_{1}}( y_{2}^{\prime}))+\mathop{\mathbb{V}}_{y_{1}\in Y_{1}}\mathop{\mathbb{V}}_{y_{1}^{ \prime}\in Y_{1}}d_{[Y_{2},Z_{2}]}(\widehat{m_{2}}(y_{1}),\widehat{m_{2}}(y_{1} ^{\prime}))\] (164) \[=\mathop{\mathbb{V}}_{y_{2}\in Y_{2}}\mathop{\mathbb{V}}_{y_{2 }^{\prime}\in Y_{2}}\mathop{\mathbb{V}}_{y_{1}\in Y_{1}}d_{Z_{1}}(\widehat{m_{ 1}}(y_{2})(y_{1}),\widehat{m_{1}}(y_{2}^{\prime})(y_{1}))\] \[+\mathop{\mathbb{V}}_{y_{1}\in Y_{1}}\mathop{\mathbb{V}}_{y_{1}^ {\prime}\in Y_{1}}\mathop{\mathbb{V}}_{y_{2}\in Y_{2}}d_{Z_{2}}(\widehat{m_{2}}( y_{1})(y_{2}),\widehat{m_{2}}(y_{1}^{\prime})(y_{2}))\] (165) \[=\mathop{\mathbb{V}}_{y_{2}\in Y_{2}}\mathop{\mathbb{V}}_{y_{2 }^{\prime}\in Y_{2}}\mathop{\mathbb{V}}_{y_{1}\in Y_{1}}d_{Z_{1}}(m_{1}(y_{1},y_{ 2}),m_{1}(y_{1},y_{2}^{\prime}))\] \[+\mathop{\mathbb{V}}_{y_{1}\in Y_{1}}\mathop{\mathbb{V}}_{y_{1}^ {\prime}\in Y_{1}}\mathop{\mathbb{V}}_{y_{2}\in Y_{2}}d_{Z_{2}}(m_{2}(y_{1},y_{ 2}),m_{2}(y_{1}^{\prime},y_{2}))\] (166) \[=\mathop{\mathbb{V}}_{y_{1}\in Y_{1}}\mathop{\mathbb{V}}_{y_{2} \in Y_{2}}\mathop{\mathbb{V}}_{y_{2}^{\prime}\in Y_{2}}d_{Z_{1}}(m_{1}(y_{1},y_{ 2}),m_{1}(y_{1},y_{2}^{\prime}))\] \[+\mathop{\mathbb{V}}_{y_{2}\in Y_{2}}\mathop{\mathbb{V}}_{y_{1} \in Y_{1}}\mathop{\mathbb{V}}_{y_{1}^{\prime}\in Y_{1}}d_{Z_{2}}(m_{2}(y_{1},y_{ 2}),m_{2}(y_{1}^{\prime},y_{2})).\] (167)Discussions

### Background

Defining and measuring the properties of learning models is a core topic in machine learning, especially representation learning (Bengio et al., 2013). A proper comprehension of what constitutes good representations and how to assess their quality is important for developing suitable learning objectives and evaluation metrics. To define these properties, many important concepts are given by _equational predicates_, such as _independence_ of random variables, extensively used in statistical learning and causal learning (Hyvarinen and Oja, 2000; Koller and Friedman, 2009; Scholkopf and von Kugelgen, 2022), and _equivariance_ of learning models, reflecting the symmetries and structures of the data (Cohen and Welling, 2016; Zaheer et al., 2017; Higgins et al., 2018; Maron et al., 2019; de Haan et al., 2020; Cohen, 2021; van der Pol et al., 2022; Navon et al., 2023).

Considerable efforts have been put into designing model architectures that perfectly satisfy specific properties, such as _monotonicity_(Sill, 1997; Daniels and Velikova, 2010), _invertibility_(Rezende and Mohamed, 2015; Behrmann et al., 2019; Ishikawa et al., 2023), _convexity_(Amos et al., 2017), and _equivariance_(Lee et al., 2019; Brehmer et al., 2023). However, hard-coding multiple properties into a model by design could be challenging (Kohler et al., 2020). Hence, it is desirable to devise quantitative metrics to directly measure these properties, even if the models do not have the properties built-in (Goodfellow et al., 2009; Chen et al., 2020; Kvinge et al., 2022). Ideally, these metrics should be easily computable or even differentiable, allowing us to directly optimize the properties.

_Disentangled representation learning_(Bengio et al., 2013), our main focus of this paper, is such a field where defining and measuring the desired properties are not straightforward tasks (Carbonneau et al., 2022; Zhang and Sugiyama, 2023). It has been suggested that disentangling the underlying explanatory factors in complex data is a promising approach for reliable, interpretable, generalizable, and data-efficient representation learning (Locatello et al., 2019; Montero et al., 2021; Dittadi et al., 2021; Xu et al., 2022). However, in contrast to the wealth of results regarding invariant and equivariant layers, the exploration of designing a "_disentangled layer_" has been relatively limited. One reason is that disentanglement was not considered a singular property but rather a combination of several requirements. The absence of a clear definition and appropriate metrics for disentanglement has created a gap between the learning objectives and evaluation metrics. A new evaluation metric is often introduced along with a new representation learning method (Carbonneau et al., 2022), but it is usually unproven that the method can optimize the new metric, and the metric truly quantifies the alleged property (Higgins et al., 2017; Kim and Mnih, 2018; Chen et al., 2018; Li et al., 2020).

To formally define disentanglement, a line of research utilized group theory and representation theory (Cohen and Welling, 2014, 2015; Higgins et al., 2018), with a focus on the _direct product of groups_. Thanks to the rich algebraic structure, it becomes possible to derive various model architectures and learning objectives from the equational requirements of the product and equivariance (Caselles-Dupre et al., 2019; Pfau et al., 2020; Quessard et al., 2020; Painter et al., 2020; Miyato et al., 2022; Yang et al., 2022; Tonnaer et al., 2022; Keurti et al., 2023). Another approach adopted a topological perspective, using concepts such as the _product manifold_ to define disentanglement (Zhou et al., 2020; Fumero et al., 2021; Zhang et al., 2021; Balabin et al., 2024). However, theoretically comparing different approaches has been a challenging task.

To quantitatively measure disentanglement, Ridgeway and Mozer (2018) proposed three concepts called _modularity, compactness, and explicitness_, which were defined verbally but not mathematically. Eastwood and Williams (2018) proposed similar three criteria called _disentanglement, completeness, and informativeness_ and corresponding evaluation metrics. However, it was unclear what properties these metrics truly quantify. Additionally, due to the necessity for additional training of classifiers along with hyperparameter tuning and the involvement of non-differentiable regressors such as the random forest (Breiman et al., 1984), it is impossible to directly optimize these metrics using gradient-based optimization. Recently, Eastwood et al. (2023) extended this framework with two new metrics called _explicitness/ease-of-use and size_ based on the functional capacity. Do and Tran (2020) introduced metrics for _informativeness, separability, independence, and interpretability_ from an information-theoretic perspective, while Tokui and Sato (2022) introduced a new metric in terms of _uniqueness, redundancy, and synergy_ based on partial information decomposition. These metrics have been mainly used during the evaluation stage, after a model is trained with other learning objectives.

### Related work

EquivarianceThe work by Kvinge et al. (2022) might be the closest to our approach in spirit. They directly converted _equivariance_, an equational predicate, to a quantitative metric and analyzed their relationship (Proposition 3.2). In contrast, based on our proposed conversion method, we can use the following definition and metric:

**Definition 34** (Equivariant function).: Let \(A\), \(B\), and \(C\) be sets. A function \(f:A\to B\) is _equivariant_ to actions (any binary functions) \(\cdot_{A}:C\times A\to A\) and \(\cdot_{B}:C\times B\to B\) if

\[p_{\text{equivariant}}(f:A\to B) :=\forall c\in C.\ f\circ(c\cdot_{A}-)=_{[A,B]}(c\cdot_{B}-)\circ f\] (168) \[=\forall c\in C.\ \forall a\in A.\ f(c\cdot_{A}a)=_{B}c\cdot_{B}f(a),\] (169)

which can be measured by

\[q_{\text{equivariant}}(f:A\to B):=\mathop{\triangledown}\limits_{c\in C} \mathop{\triangledown}\limits_{a\in A}d_{B}(f(c\cdot_{A}a),c\cdot_{B}f(a)).\] (170)

CalibrationMore broadly, the study of the relationship between different metrics in statistical learning is called _calibration analysis_(Steinwart, 2007; Reid and Williamson, 2010; Ni et al., 2019; Bao and Sugiyama, 2020; Bao et al., 2020). Our work can be seen as an extension of the concept of the calibration to a wider range of properties defined by equational predicates.

Disentanglement metricIn disentangled representation learning, metrics similar to Eq. (26) have been proposed by Higgins et al. (2017); Kim and Mnih (2018). Their metrics also fix one factor and vary all others and calculate some constancy metrics (the mean pairwise distance in Higgins et al. (2017) and the variance in Kim and Mnih (2018)). However, both studies took an indirect approach, involving the training of a classifier to predict the fixed factor. Consequently, the resulting metrics are not differentiable anymore and entangle modularity and informativeness. In this work, we argue that it is better to measure these two properties separately.

Weakly supervised disentanglementRidgeway and Mozer (2018) proposed and investigated the similarity supervision and argued that such supervision is easy to obtain via crowdsourcing. Shu et al. (2020) further studied this type of supervision based on distribution matching and referred it as match pairing. Other weaker forms of supervision were also investigated, such as the number of changed factors (Locatello et al., 2020) or paired data with unknown intervention (Brehmer et al., 2022). Given that our theory can establish connections between logical definitions and quantitative metrics, it holds promise for deriving disentanglement metrics for various types of weak supervision based on logical inference.

Multi-valued logicAristotelian logic assumes that every proposition is either true or false, adhering to the _principle of bivalence_. The law of Aristotelian logic can be algebraically represented on the set \(\{0,1\}\) of binary truth values (Boole, 1854), known as the two-element _Boolean algebra_. The exploration of non-Aristotelian logic involves investigating logical systems that relax or modify this strict binary valuation, allowing for a broader range of truth values and accommodating various forms of uncertainty, vagueness, or context-dependence in reasoning (Hajek, 1998; Malinowski, 2007; Bergmann, 2008).

The mathematical study of multi-valued logic can date back to the seminal work by Lukasiewicz in 1920, who introduced a third truth value interpreted as "_possibility_" and symbolized by \(\frac{1}{2}\). Lukasiewicz (1920) examined several principles in this _three-valued logic_ such as the principles of identity, implication, syllogism, and contradiction, and discussed its theoretical and practical importance in indeterministic philosophy and deductive sciences. Later, Lukasiewicz and Tarski (1930) proposed _propositional calculus_, a theory of propositions with values from the real interval \([0,1]\), which is now also commonly known as _Lukasiewicz logic_. Lukasiewicz logic involves new continuous logical connectives such as strong/weak conjunction and disjunction.

Furthermore, Chang (1958) studied the algebraic systems for _many-valued logic_, called _MV-algebras_. Chang and Keisler (1966) then proposed _continuous model theory_, also referred to as _compact-valued logic_(Ben Yaacov, 2022), where the truth values can be in arbitrary compact Hausdorff spaces and a wide variety of quantifiers was studied. Later, Ben Yaacov et al. (2008) studied model theory for metric structures and proposed _(real-valued) continuous first-order logic_(Ben Yaacov and Usvyatsov,

[MISSING_PAGE_FAIL:44]

### Implication and equivalence

In Section 3, we only used the truncated subtraction \(\hat{\leavevmode\hbox to17.49pt{\vbox to17.49pt{\pgfpicture\makeatletter\hbox to 0.0pt{\pgfsys@beginscope{} \definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}{} \pgfsys@color@rgb@fill{0}{0}{0}{}\pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox t o 0.0pt{\pgfsys@beginscope{}\definecolor{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@rgb@stroke{0}{0}{0}{}\pgfsys@color@rgb@fill{0}{0}{0}{} \pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox to 0.0pt{\pgfsys@beginscope{} \definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}{} \pgfsys@color@rgb@fill{0}{0}{0}{}\pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox t o 0.0pt{\pgfsys@beginscope{}\definecolor{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@rgb@stroke{0}{0}{0}{}\pgfsys@color@rgb@fill{0}{0}{0}{} \pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox to 0.0pt{\pgfsys@beginscope{} \definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}{} \pgfsys@color@rgb@fill{0}{0}{0}{}\pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox t o 0.0pt{\pgfsys@beginscope{}\definecolor{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@rgb@stroke{0}{0}{0}{}\pgfsys@color@rgb@fill{0}{0}{0}{} \pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox to 0.0pt{\pgfsys@beginscope{} \definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}{} \pgfsys@color@rgb@fill{0}{0}{0}{}\pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox t o 0.0pt{\pgfsys@beginscope{}\definecolor{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@rgb@stroke{0}{0}{0}{}\pgfsys@color@rgb@fill{0}{0}{0}{} \pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox to 0.0pt{\pgfsys@beginscope{} \definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}{} \pgfsys@color@rgb@fill{0}{0}{0}{}\pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox t o 0.0pt{\pgfsys@beginscope{}\definecolor{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@rgb@stroke{0}{0}{0}{}\pgfsys@color@rgb@fill{0}{0}{}{} \pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox to 0.0pt{\pgfsys@beginscope{} \definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}{} \pgfsys@color@rgb@fill{0}{0}{0}{}\pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox t o 0.0pt{\pgfsys@beginscope{}\definecolor{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@rgb@stroke{0}{0}{0}{}\pgfsys@color@rgb@fill{0}{0}{}{}\pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox to 0.0pt{\pgfsys@beginscope{} \definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}{} \pgfsys@color@rgb@fill{0}{0}{0}{}\pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox t o 0.0pt{\pgfsys@beginscope{}\definecolor{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@rgb@stroke{0}{0}{0}{}\pgfsys@color@rgb@fill{0}{0}{}{}\pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox to 0.0pt{\pgfsys@beginscope{} \definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}{} \pgfsys@color@rgb@fill{0}{0}{0}{}\pgfsys@setlinewidth{0.4pt}{}{}\nullfont \hbox to 0.0pt{\pgfsys@beginscope{} \definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}{} \pgfsys@color@rgb@fill{0}{0}{0}{}\pgfsys@setlinewidth{0.4pt}{}{}\nullfont \hbox to 0.0pt{\pgfsys@beginscope{} \definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}{} \pgfsys@color@rgb@fill{0}{0}{0}{}\pgfsys@setlinewidth{0.4pt}{}{}\nullfont \hbox to 0.0pt{\pgfsys@beginscope{}\definecolor{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@rgb@stroke{0}{0}{0}{}\pgfsys@color@rgb@fill{0}{0}{}{}\pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox to 0.0pt{\pgfsys@beginscope{}\definecolor{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@rgb@stroke{0}{0}{0}{}\pgfsys@color@rgb@fill{0}{0}{0}{} \pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox to 0.0pt{\pgfsys@beginscope{} \definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}{}\pgfsys@color@rgb@fill{0}{0}{}{} \pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox to 0.0pt{\pgfsys@beginscope{} \definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}{} \pgfsys@color@rgb@fill{0}{0}{}{}\pgfsys@setlinewidth{0.4pt}{}{}\nullfont \hbox to 0.0pt{\pgfsys@beginscope{}\definecolor{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@rgb@stroke{0}{0}{0}{}\pgfsys@color@rgb@fill{0}{0}{}{} \pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox to 0.0pt{\pgfsys@beginscope{} \definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}{} \pgfsys@color@rgb@fill{0}{0}{}{}\pgfsys@setlinewidth{0.4pt}{}{}\nullfont \hbox to 0.0pt{\pgfsys@beginscope{}\definecolor{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@rgb@stroke{0}{0}{0}{}\pgfsys@color@rgb@fill{0}{0}{}{}\pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox to 0.0pt{\pgfsys@beginscope{}\definecolor{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@rgb@stroke{0}{0}{0}{}\pgfsys@color@rgb@fill{0}{0}{}{}\pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox to 0.0pt{\pgfsys@beginscope{}\definecolor{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@rgb@stroke{0}{0}{0}{}\pgfsys@color@rgb@fill{0}{0}{}{}\pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox to 0.0pt{\pgfsys@beginscope{}\definecolor{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@rgb@stroke{0}{0}{0}{}\pgfsys@color@rgb@fill{0}{0}{}{}\pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox to 0.0pt{\pgfsys@beginscope{}\definecolor{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@rgb@stroke{0}{0}{0}{}\pgfsys@color@rgb@fill{0}{0}{0}{}\pgfsys@setlinewidth{ 0.4pt}{}{}\nullfont\hbox to 0.0pt{\pgfsys@beginscope{}\definecolor{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@rgb@stroke{0}{0}{0}{}\pgfsys@color@rgb@fill{0}{0}{}{}\pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox to 0.0pt{\pgfsys@beginscope{}\definecolor{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@rgb@stroke{0}{0}{0}{}\pgfsys@color@rgb@fill{0}{0}{}{}\pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox to 0.0pt{\pgfsys@beginscope{}{ \definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}{} \pgfsys@color@rgb@fill{0}{0}{}{}\pgfsys@setlinewidth{0.4pt}{}{}\nullfont \hbox to 0.0pt{\pgfsys@beginscope{}\definecolor{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@rgb@stroke{0}{0}{0}{}\pgfsys@color@rgb@fill{0}{0}{}{}\pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox to 0.0pt{\pgfsys@beginscope{}{ \definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}{}\pgfsys@color@rgb@fill{0}{0}{}{} \pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox to 0.0pt{\pgfsys@beginscope{} \definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}{} \pgfsys@color@rgb@fill{0}{0}{}{}\pgfsys@setlinewidth{0.4pt}{}{}\nullfont \hbox to 0.0pt{\pgfsys@beginscope{}\definecolor{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@rgb@stroke{0}{0}{0}{}\pgfsys@color@rgb@fill{0}{0}{0}{} \pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox to 0.0pt{\pgfsys@beginscope{} \definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}{} \pgfsys@color@rgb@fill{0}{0}{0}{}\pgfsys@setlinewidth{0.4pt}{}{}\nullfont \hbox to 0.0pt{\pgfsys@beginscope{}{}\definecolor{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@rgb@stroke{0}{0}{0}{}\pgfsys@color@rgb@fill{0}{0}{0}{}\pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox to 0.0pt{\pgfsys@beginscope{}{}\definecolor{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@rgb@stroke{0}{0}{0}{}\pgfsys@color@rgb@fill{0}{0}{0}{} \pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox to 0.0pt{\pgfsys@beginscope{} \definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}{}\pgfsys@color@rgb@fill{0}{0}{0}{} \pgfsys@setlinewidth{0.4pt}{}{}\nullfont\hbox to 0.0pt{\pgfsys@beginscope{Note that a quantitative operation homomorphic to the implication cannot be continuous everywhere, which is undesirable for gradient-based optimization. For example, the following quantity also measures the injectivity of a function \(m:Y\to Z\):

\[\underset{y\in Y}{\triangledown}\underset{y^{\prime}\in Y}{ \triangledown}[d_{Z}(m(y),m(y^{\prime}))=0]\times[d_{Y}(y,y^{\prime})>0]\times d _{Y}(y,y^{\prime})\] (171) \[= \underset{y\in Y}{\triangledown}\underset{y^{\prime}\in Y}{ \triangledown}[m(y)=_{Z}m(y^{\prime})]\times[y\neq_{Y}y^{\prime}]\times d_{Y} (y,y^{\prime}).\] (172)

This quantity aggregates distances between pairs of different inputs mapped to the same outputs. However, unlike \(q_{\text{injective}}\) introduced in Section 4.3, it is not differentiable with respect to the function \(m:Y\to Z\). Thus, we cannot use it to improve the injectivity of a function by gradient descent.

### Constant function

Note that there are two logically equivalent definitions of a constant function (to a non-empty set). One is based on the equality between all pairs, as in Definition 11. The other is based on the constant output value (see Fig. 10):

**Definition 35** (Constant function with value).: A function \(f:A\to B\) is a _constant function_ with value \(b\in B\) if

\[p_{\text{const-v}}(f:A\to B):=\exists b\in B.\;\forall a\in A.\;(f(a)=_{B}b),\] (173)

which can be measured by

\[q_{\text{const-v}}(f:A\to B):=\inf_{b\in B}\underset{a\in A}{ \triangledown}d_{B}(f(a),b).\] (174)

This quantity \(q_{\text{const-v}}\) finds a central point in the codomain that best approximates all the outputs of a function, which is similar to the approach we discussed in Section 4.1. In fact, we can prove that if we use \(q_{\text{const-v}}\) in \(q_{\text{const-curry}}\), we will end up with the same quantity \(q_{\text{product}}\).

Figure 10: Two approaches for measuring the _constancy_ of a set in \(\mathbb{R}^{2}\): (a) finding a central point, such as the center of the smallest bounding sphere, the geometric median, or the mean, and then measuring the dispersion around this point; and (b) aggregating pairwise distances between points.

### Rank of imperfect representations

It is worth noting that Theorem 1 only guarantees that the minimizers of different quantitative metrics derived from the same logical definition are the same, but imperfect representations, whose evaluation results are non-zero, may be ranked differently by different metrics.

For example, Fig. 11 illustrates two constancy metrics, the radius of the smallest bounding sphere and the variance, on two sets of points in \(\mathbb{R}^{2}\), where one set has a small radius but a large variance, while the other has a large radius but a small variance. More examples are presented in Fig. 12, and such results can also be observed in Table 2. This difference can lead to differences in risk preferences, sensitivity to outliers, and learning dynamics when these metrics are used as learning objectives. Further investigation of the characteristics of these metrics for imperfect representations is left for future work.

Figure 11: Metrics may rank imperfect representations differently.

Figure 12: For a pair of constancy metrics (each column), we can find two sets of points in \(\mathbb{R}^{2}\) ranked differently by these metrics, except for the radius and diameter, because for a subset \(A_{0}\) in a set \(A\), we have \(\inf_{a_{0}\in A}\sup_{a\in A_{0}}d_{A}(a_{0},a)\leq\inf_{a_{0}\in A_{0}}\sup_{ a\in A_{0}}d_{A}(a_{0},a)\leq\sup_{a_{0}\in A_{0}}\sup_{a\in A_{0}}d_{A}(a_{0},a)\).

### Implementation

Thanks to advanced indexing (e.g., NumPy [Harris et al., 2020] and PyTorch [Paszke et al., 2019]) and analytical solutions to some optimization problems (e.g., Eq. (19)), some of the proposed metrics can be easily implemented, even as Python one-liners.

For example, the following function implements a family of modularity metrics:

``` defq_product(y:np.ndarray,z:np.ndarray,aggregate,deviation): returnnp.sum(aggregate([deviation(zi[yi:=yv])foryvinnp.unique(yyi)])foryi,ziinzip(y,z)]) ```

Here, \(\mathsf{y}\) and \(\mathsf{z}\) are NumPy arrays of shape (factor, index); aggregate can be max, mean, or sum; deviation can be a function calculating the radius of the smallest bounding sphere,12 mean absolute deviation around the geometric median,13 variance, diameter, or mean pairwise distance. Please note, however, that the deviation function can be computationally expensive, depending on the dimension of the codes.

Footnote 12: https://github.com/marmakoide/miniball (MIT License) [Welzl, 1991]

Footnote 13: https://github.com/krishnap25/geom_median (GNU General Public License, Version 3 (GPLv3)) [Pillutla et al., 2022]

### Limitations

Lastly, we discuss several aspects that are not covered in this work and potential directions for future research.

Function equalityA collection of input-out pairs \(\left\{\left(x_{i},y_{i}\right)\right\}_{i=1}^{n}\in\left(X\times Y\right)^{n}\) may not define a _function_\(g:X\to Y\) for two reasons: First, the set of all inputs \(X_{0}:=\left\{x_{i}\right\}_{i=0}^{n}\) is unlikely to enumerate all possible inputs (i.e., \(X_{0}\subsetneq X\)), especially when the cardinality of the domain \(X\) is infinite (e.g., \(\mathbb{R}\)), so the data may only define a _partial function_\(g:X\to Y\) or a function from a smaller domain \(g_{0}:X_{0}\to Y\). Second, the inputs may not be distinct, e.g., when an input is given multiple labels by different annotators, so the data may define a _multi-valued function_. The extension from functions to relations or stochastic maps is an important future direction of our work.

Partial combinationsA more general issue is learning and evaluating disentangled representations given only a subset of all combinations of factors, which is common when dealing with a large number of factors [Trauble et al., 2021, Montero et al., 2022, Roth et al., 2023]. It is crucial to evaluate and justify whether a metric computed on partial combinations of factors is a reliable proxy for the performance of the model on unseen combinations.

Unknown projectionsAnother common scenario is when the extracted representation is not properly aligned with the underlying factors. For example, a model may extract a three-dimensional representation \(z\in\mathbb{R}^{3}\) for two factors \(y\in\left[0,1\right]^{2}\), and it can project to \(\left((z_{1},z_{2}),z_{3}\right)\) or \(\left(z_{1},(z_{2},z_{3})\right)\). How can we determine which is better, without enumerating all possible projections? Finding the optimal assignment [Mahon et al., 2023] and correcting a pre-trained model post hoc [Trauble et al., 2021] based on the proposed metrics are interesting future directions.

### Broader impact

This paper focuses on the theoretical aspects of disentangled representation learning, and we do not foresee any immediate negative societal consequences. However, we would acknowledge that disentanglement is closely related to data-efficiency and fairness, potentially sparking discussions on ethical considerations. Besides, the application of category theory may facilitate the transfer and integration of knowledge across disciplines, fostering closer connections between various fields of study, even beyond the machine learning community.

## Appendix E Experiments

In this section, we provide the detailed data configuration used in Section 5 and further experimental results.

### Synthetic data

We used a simple synthetic setup to simulate entanglement of factors and common failures patterns. Concretely, we used a Cartesian product \(Y:=\{0,0.1,\dots,1\}^{3}\) of three sets as the underlying factors (Fig. 12(a)). We used a random rotation matrix \(R\) to entangle factors and componentwise exponential as a non-linear transformation. We composited this procedure twice and used an affine transformation to normalize the outputs (Fig. 12(b)). That is, we used the following function as the data generating process:

\[g:Y\to X:y\mapsto a\cdot\exp(R\cdot\exp(R\cdot y))+b.\] (175)

Note that this function is injective but not a product or linear.

We used the following functions as the function \(m:Y\to Z\):

\[\begin{array}{llll}\text{entanglement}&(y_{1},y_{2},y_{3})&\mapsto&g(y_{1},y_{2},y_{3})\\ \text{rotation}&(y_{1},y_{2},y_{3})&\mapsto&R\cdot(y_{1},y_{2},y_{3})\\ \text{duplicate}&(y_{1},y_{2},y_{3})&\mapsto&((y_{1},y_{2},y_{3}),(y_{1},y_{2},y_ {3}),y_{3})\\ \text{complement}&(y_{1},y_{2},y_{3})&\mapsto&((y_{2},y_{3}),(y_{1},y_{3}),(y_{ 1},y_{2}))\\ \text{misalignment}&(y_{1},y_{2},y_{3})&\mapsto&(y_{2},y_{3},y_{1})\\ \text{redundancy}&(y_{1},y_{2},y_{3})&\mapsto&((y_{1},y_{1}-y_{1}),y_{2},y_{3})\\ \text{contraction}&(y_{1},y_{2},y_{3})&\mapsto&0.01\times(y_{1},y_{2},y_{3})\\ \text{nonlinear}&(y_{1},y_{2},y_{3})&\mapsto&(y_{1}^{2},y_{2}^{2},y_{3}^{2})\\ \text{constant}&(y_{1},y_{2},y_{3})&\mapsto&(0,0,0)\end{array}\]

The rotation operation entangles factors but can be (linearly) inverted. The duplicate encoder has a modular decoder (projections), but itself is not modular. The redundancy encoder is both modular and informative, but not all codes can be decoded. The constant encoder is perfectly modular but not informative.

Figure 14: Entangling a multivariate distribution via probability integral transform, inverse transform, and orthogonal transformation of standard normal distribution (Locatello et al., 2019, Theorem 1).

Figure 13: (a) a set of _factors_\(Y\) represented by the RGB color model; (b) a set of entangled _codes_\(Z\) extracted by an encoder \(m:Y\to Z\); (c) a _product_ function approximation; and (d) a linear approximation of the _retraction_\(h:Z\to Y\) of the encoder.

### Weakly supervised modularity metrics

We briefly comment on the possibility of employing weak supervision for measuring disentangled representations.

For supervised disentanglement metrics we discussed in Section 4, the necessary data consists of observation-factor pairs \((x,y)\), representing a generator \(g:Y\to X\). In order to evaluate an encoder \(f:X\to Z\), we compose it with a generator and study the properties of the composition \(m:Y\to Z:=f\circ g\).

It is worth noting that \(p_{\text{product}}\) and \(p_{\text{injective}}\) are equational predicates, which means that they are _invariant to bijections_. Similarly, \(q_{\text{product}}\) and \(q_{\text{injective}}\) are _invariant to isometries_. This implies that the exact values of the factors are not important; we only need to know if two factors are equal or not. Hence, we only need weak supervision of the form \((x,x^{\prime},y_{i}=_{Y_{i}}y^{\prime}_{i})\) so that we can construct some equivalence classes of factors, and we can still calculate or approximate Eqs. (19) and (27). Note that this type of supervision has been partially investigated by Ridgeway and Mozer (2018); Shu et al. (2020).

For example, suppose we have an object A (e.g., red circle) and an object B (e.g., red triangle), and all we know is that objects A and B have the same color. Based on such weak information, we can still construct an equivalence class containing objects with the same color as object A. Then, we can regularize an encoder \(f:X\to Z\) by minimizing the variance of the color representations over this equivalence class (Eq. (19)). In this way, the modularity of the encoder can be improved. The challenge arises when there is noise or only partial combinations, which is an interesting future work direction. In such cases, we may need to use semi-supervised clustering to group the data (Wagstaff et al., 2001; Basu et al., 2002; Bilenko et al., 2004).

To validate this idea, we conducted experiments where we only used a random sample of pairs and their similarities. Table 3 is an excerpt of Table 2, showing only the proposed modularity metrics, and Table 4 shows these metrics calculated using only similarity supervision. We reported the mean values of \(10\) random samples of pairs, and the variances are negligible. Comparing Tables 3 and 4, we can observe that weakly supervised metrics may overestimate imperfect representations, but they can still maintain the ranks. This observation suggests the potential utility of employing weak supervision for both learning and evaluating disentangled representations using the proposed metrics.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & \multicolumn{3}{c}{Product approx.} & \multicolumn{3}{c}{Constancy} \\ \cline{2-6}  & Rad. & MAD & Var. & Diam. & MPD \\ \hline entanglement & \(0.44\) & \(0.75\) & \(0.96\) & \(0.19\) & \(0.82\) \\ rotation & \(0.22\) & \(0.51\) & \(0.80\) & \(0.05\) & \(0.64\) \\ duplicate & \(0.24\) & \(0.43\) & \(0.67\) & \(0.06\) & \(0.56\) \\ complement & \(0.12\) & \(0.28\) & \(0.55\) & \(0.01\) & \(0.42\) \\ misalignment & \(0.22\) & \(0.44\) & \(0.74\) & \(0.05\) & \(0.58\) \\ random & \(0.22\) & \(0.48\) & \(0.78\) & \(0.05\) & \(0.61\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Supervised modularity metrics

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & \multicolumn{3}{c}{Product approx.} & \multicolumn{3}{c}{Constancy} \\ \cline{2-6}  & Rad. & MAD & Var. & Diam. & MPD \\ \hline entanglement & \(0.50\) & \(0.77\) & \(0.96\) & \(0.26\) & \(0.84\) \\ rotation & \(0.24\) & \(0.54\) & \(0.83\) & \(0.06\) & \(0.68\) \\ duplicate & \(0.28\) & \(0.46\) & \(0.71\) & \(0.07\) & \(0.60\) \\ complement & \(0.14\) & \(0.32\) & \(0.59\) & \(0.02\) & \(0.47\) \\ misalignment & \(0.22\) & \(0.48\) & \(0.77\) & \(0.05\) & \(0.62\) \\ random & \(0.24\) & \(0.52\) & \(0.81\) & \(0.06\) & \(0.64\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Weakly supervised modularity metrics

### Evaluation of existing models on image datasets

We also report the results of several widely used unsupervised disentangled representation learning methods (VAE (Kingma and Welling, 2014), \(\beta\)-VAE (Higgins et al., 2017), FactorVAE (Kim and Mnih, 2018), and \(\beta\)-TCVAE (Chen et al., 2018)) evaluated on four image datasets (**3D Cars**(Reed et al., 2015), **dSprites**(Matthey et al., 2017), **3D Shapes**(Burgess and Kim, 2018), and **MPI3D**(Gondal et al., 2019)) in Table 5.

We used a public PyTorch implementation (Paszke et al., 2019) of these methods and used the same encoder/decoder architecture with the default hyperparameters described in Locatello et al. (2019) for all methods for a fair comparison. We used linear projection to find the most informative representations for each factor. The experiments were conducted on a NVIDIA Tesla V100 GPU.

Before analyzing these results, it is important to note that the evaluation of these learning models is _not_ meant to be a proof of the correctness of the proposed metrics, since we cannot tell whether a bad result is due to the insufficiency of a learning method, to the quality of the datasets, or to the problem of the evaluation, if we have no theoretical guarantee for the metrics. We can trust the results of the proposed metrics because the properties of their minimizers are guaranteed by Theorem 1.

From Table 5 we can observe that the considered learning methods do not exhibit significant difference in terms of modularity and informativeness. This result supports the theoretical finding of Locatello et al. (2019) that unsupervised learning of disentangled representations by matching the distributions of observations is fundamentally impossible (see also Fig. 14) as well as their empirical finding that there is no evidence that learning disentangled representations in an unsupervised manner is reliable.

### Kendall tau distance between metrics

To analyze the relationship between these metrics, we report the Kendall tau distance (Kendall, 1938; Virtanen et al., 2020) averaged over experimental settings in Table 6. The Kendall tau distance is a correlation measure for ordinal data valued in \([-1,1]\) which counts the number of pairwise disagreements between two ranking lists. Values close to \(1\) indicate strong agreement, and values close to \(-1\) indicate strong disagreement.

From Table 6 we can observe that even though different metrics derived from the same logical definition may rank imperfect representations differently (see also Fig. 12), they still have positive correlations with each other, indicating that they measure the same property. The metrics proposed by Higgins et al. (2017) and Kim and Mnih (2018) have the highest correlations with each other (except for themselves), and we hypothesize that this is because they are both based on the pairwise distance

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{Modularity} & \multicolumn{6}{c}{Informativeness} & \multicolumn{6}{c}{Existing metrics} \\ \cline{2-13}  & \multicolumn{3}{c}{Product approx.} & \multicolumn{3}{c}{Constancy} & \multicolumn{3}{c}{Retraction approx.} & \multicolumn{3}{c}{Contraction} & \multicolumn{3}{c}{Pair} & \multicolumn{3}{c}{Info.} & \multicolumn{3}{c}{Regressor} \\ \cline{2-13}  & Rad. & MAD & Var. & Diam. & MPD & ME & MAE & MSE & Max & Mean & Beta\({}^{a}\) & Factor\({}^{b}\) & MIG\({}^{c}\) & Dis.\({}^{d}\) & Com.\({}^{d}\) & Info.\({}^{d}\) \\ \hline
**3D Cars**(Reed et al., 2015) & & & & & & & & & & & & & & & & & \\ \hline VAE & 0.27 & 0.76 & 0.95 & 0.07 & 0.83 & 0.44 & 0.82 & 0.94 & 0.21 & 0.75 & 0.90 & 0.22 & 0.02 & 0.07 & 0.05 & 0.54 \\ \(\beta\)-VAE & 0.26 & 0.76 & 0.95 & 0.07 & 0.82 & 0.42 & 0.82 & 0.94 & 0.20 & 0.74 & 0.90 & 0.21 & 0.01 & 0.13 & 0.10 & 0.54 \\ FactorVAE & 0.24 & 0.75 & 0.95 & 0.06 & 0.82 & 0.35 & 0.82 & 0.94 & 0.21 & 0.74 & 0.89 & 0.20 & 0.03 & 0.11 & 0.08 & 0.54 \\ \(\beta\)-TCVAE & 0.28 & 0.77 & 0.95 & 0.08 & 0.83 & 0.43 & 0.82 & 0.94 & 0.21 & 0.74 & 0.90 & 0.21 & 0.02 & 0.14 & 0.11 & 0.59 \\ \hline
**dSprites**(Matthey et al., 2017) & & & & & & & & & & & & & & & & & \\ \hline VAE & 0.24 & 0.64 & 0.94 & 0.06 & 0.74 & 0.37 & 0.82 & 0.94 & 0.18 & 0.68 & 0.54 & 0.26 & 0.09 & 0.16 & 0.15 & 0.40 \\ \(\beta\)-VAE & 0.14 & 0.64 & 0.93 & 0.02 & 0.73 & 0.41 & 0.83 & 0.94 & 0.20 & 0.70 & 0.58 & 0.29 & 0.13 & 0.20 & 0.24 & 0.44 \\ FactorVAE & 0.18 & 0.63 & 0.93 & 0.03 & 0.73 & 0.38 & 0.82 & 0.94 & 0.17 & 0.67 & 0.48 & 0.26 & 0.13 & 0.20 & 0.23 & 0.36 \\ \(\beta\)-TCVAE & 0.22 & 0.64 & 0.93 & 0.05 & 0.73 & 0.42 & 0.83 & 0.94 & 0.21 & 0.70 & 0.56 & 0.27 & 0.18 & 0.29 & 0.31 & 0.59 \\ \hline
**3D Shapes**(Burgess and Kim, 2018) & & & & & & & & & & & & & & & & & \\ \hline VAE & 0.25 & 0.76 & 0.96 & 0.06 & 0.82 & 0.29 & 0.88 & 0.96 & 0.20 & 0.78 & 0.99 & 0.94 & 0.19 & 0.35 & 0.29 & 0.76 \\ \(\beta\)-VAE & 0.20 & 0.72 & 0.95 & 0.04 & 0.79 & 0.39 & 0.84 & 0.94 & 0.19 & 0.69 & 0.86 & 0.77 & 0.26 & 0.69 & 0.62 & 0.99 \\ FactorVAE & 0.24 & 0.49 & 0.96 & 0.06 & 0.78 & 0.34 & 0.82 & 0.93 & 0.16 & 0.64 & 0.83 & 0.57 & 0.15 & 0.40 & 0.40 & 0.84 \\ \(\beta\)-TCVAE & 0.25 & 0.69 & 0.94 & 0.06 & 0.77 & 0.32 & 0.82 & 0.93 & 0.18 & 0.63 & 0.76 & 0.50 & 0.11 & 0.68 & 0.57 & 0.98 \\ \hline
**MPI3D**(Gondal et al., 2019) & & & & & & & & & & & & & & & & & & \\ \hline VAE & 0.04 & 0.56 & 0.91 & 0.00 & 0.66 & 0.30 & 0.76 & 0.89 & 0.12 & 0.46 & 0.48 & 0.11 & 0.12 & 0.29 & 0.33 & 0.64 \\ \(\beta\)-VAE & 0.02 & 0.84 & 0.97 & 0.00 & 0.87 & 0.28 & 0.75 & 0.89 & 0.10 & 0.41 & 0.39 & 0.11 & 0.07 & 0.15 & 0.18 & 0.47 \\ FactorVAE & 0.09 & 0.60 & 0.93 & 0.01 & 0.70 & 0.27 & 0.75 & 0.89 & 0.11 & 0.43 & 0.47 & 0.09 & 0.12 & 0.26 & 0.30 & 0.60 \\ \(\beta\)-TCVAE & 0.07 & 0.65 & 0.95 & 0.01 & 0.74 & 0.28 & 0.76 & 0.89 & 0.12 & 0.46 & 0.45 & 0.07 & 0.12 & 0.24 & 0.31 & 0.57 \\ \hline \hline \end{tabular}

\({}^{a}\)Higgins et al. (2017) \({}^{b}\)(Kim and Mnih, 2018) \({}^{c}\)(Chen et al., 2018) \({}^{d}\)(Eastwood and Williams, 2018)

\end{table}
Table 5: Supervised disentanglement metrics on image datasetsapproach. The DCI disentanglement metric [Eastwood and Williams, 2018] weakly agrees with the modularity metrics. However, the DCI informativeness metric [Eastwood and Williams, 2018] weakly disagrees with the informativeness metrics. It is possible that this is because of the different regressors (sklearn.ensemble.GradientBoostingClassifier, sklearn.linear_model.LinearRegression, and sklearn.linear_model.QuantileRegressor [Pedregosa et al., 2011]) used in predicting factors from codes, showing that random seeds and hyperparameters of the metrics may matter more than the models when additional predictors need to be trained to evaluate the learning methods. This result indicates the advantage of \(q_{\text{injective}}\) over \(q_{\text{retractable}}\).

However, it is important to note the limitations of these experimental results. Since the representations were learned from data and not fully controlled, it is possible that such results are due to the choices of datasets, learning algorithms, hyperparameters, and optimization errors. A high rank correlation coefficient between two metrics in this specific setting cannot guarantee that these metrics always measure the same property, or that they rank imperfect representations similarly in other settings. To gain a deeper understanding of these metrics, it is preferable to analyze their minimizers theoretically (Theorem 1) or test them in a fully controlled environment (Section 5).

### Computation time of metrics

Finally, we report the computation time of the considered metrics in Table 7 to support our claim that the proposed metrics are much faster than those that require training additional predictors and hyperparameter tuning. We can see that, in an extreme case, the calculation of the DCI metrics [Eastwood and Williams, 2018] using GradientBoostingClassifier [Pedregosa et al., 2011] takes around 15 minutes, while other metrics can be calculated within seconds. This computation time may be acceptable if the metrics are only used in the evaluation phase, but it is not feasible to use them as learning objectives even in derivative-free optimization.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c c c c} \hline \hline \multicolumn{1}{c}{\multirow{3}{*}{}} & \multicolumn{3}{c}{Moduliarity} & \multicolumn{6}{c}{Informativeness} & \multicolumn{6}{c}{Existing metrics} \\ \cline{2-13}  & \multicolumn{2}{c}{Product approx.} & \multicolumn{2}{c}{Constancy} & \multicolumn{2}{c}{Retraction approx.} & \multicolumn{2}{c}{Contraction} & \multicolumn{2}{c}{Pair} & \multicolumn{2}{c}{Info.} & \multicolumn{2}{c}{Regressor} \\ \cline{2-13}  & Rad. & MAD & Var. & Daam. & MPD & ME & MAE & MSE & Max & Mean & Beta\({}^{a}\) & Factor\({}^{b}\) & MIO\({}^{c}\) & Dis.\({}^{d}\) & Com.\({}^{d}\) & Info.\({}^{d}\) \\ \hline Rad. & 1.00 & 0.08 & 0.25 & 1.00 & 0.33 & \(-\)0.08 & 0.08 & 0.17 & 0.08 & 0.17 & \(-\)0.25 & 0.17 & 0.00 & 0.00 & 0.17 & \(-\)0.08 \\ MAD & 0.08 & 1.00 & 0.50 & 0.08 & 0.75 & 0.33 & 0.67 & 0.58 & 0.00 & 0.42 & \(-\)0.50 & \(-\)0.58 & \(-\)0.25 & 0.25 & 0.08 & \(-\)0.00 \\ Var. & 0.25 & 0.50 & 1.00 & 0.25 & 0.75 & 0.33 & 0.42 & \(-\)0.17 & 0.25 & \(-\)0.00 & \(-\)0.25 & \(-\)0.08 & 0.58 & 0.42 & 0.33 \\ Dam. & 1.00 & 0.08 & 0.25 & 1.00 & 0.33 & \(-\)0.08 & 0.08 & 0.17 & 0.08 & 0.17 & \(-\)0.25 & 0.17 & 0.00 & 0.00 & 0.17 & \(-\)0.08 \\ MDP & 0.33 & 0.75 & 0.75 & 0.33 & 1.00 & 0.25 & 0.42 & 0.50 & \(-\)0.08 & 0.33 & \(-\)0.25 & \(-\)0.33 & \(-\)0.17 & 0.33 & 0.17 & 0.08 \\ \hline ME & \(-\)0.08 & 0.32 & 0.33 & \(-\)0.08 & 0.25 & 1.00 & 0.50 & 0.58 & 0.32 & 0.42 & \(-\)0.17 & \(-\)0.25 & \(-\)0.42 & 0.08 & \(-\)0.08 & 0.00 \\ MAE & 0.08 & 0.67 & 0.33 & 0.08 & 0.42 & 0.50 & 1.00 & 0.92 & 0.17 & 0.75 & \(-\)0.67 & \(-\)0.58 & \(-\)0.25 & 0.08 & \(-\)0.08 & \(-\)0.17 \\ MSE & 0.17 & 0.58 & 0.42 & 0.17 & 0.50 & 0.58 & 0.92 & 1.00 & 0.25 & 0.83 & \(-\)0.58 & \(-\)0.50 & \(-\)0.33 & 0.00 & \(-\)0.17 & \(-\)0.25 \\ Max & 0.08 & 0.00 & -0.17 & 0.08 & 0.03 & 0.33 & 0.17 & 0.25 & 1.00 & 0.42 & \(-\)0.33 & 0.08 & \(-\)0.42 & \(-\)0.25 & \(-\)0.42 & \(-\)0.33 \\ Mean & 0.17 & 0.42 & 0.25 & 0.17 & 0.33 & 0.42 & 0.75 & 0.33 & 0.42 & 1.00 & \(-\)0.75 & \(-\)0.50 & \(-\)0.33 & \(-\)0.17 & \(-\)0.33 & \(-\)0.42 \\ \hline Beta & \(-\)0.25 & \(-\)0.50 & \(-\)0.00 & \(-\)0.25 & \(-\)0.17 & \(-\)0.67 & \(-\)0.55 & \(-\)0.33 & \(-\)0.75 & 1.00 & 0.58 & 0.25 & 0.25 & 0.50 \\ Factor & 0.17 & \(-\)0.25 & 0.25 & 0.17 & \(-\)0.33 & \(-\)0.25 & \(-\)0.58 & \(-\)0.50 & \(-\)0.08 & \(-\)0.50 & 0.58 & 1.00 & \(-\)0.17 & \(-\)0.17 & 0.17 & 0.08 \\ \hline MIG & 0.00 & \(-\)0.25 & -0.08 & 0.00 & 0.17 & \(-\)0.42 & \(-\)0.25 & \(-\)0.33 & \(-\)0.42 & \(-\)0.33 & 0.25 & \(-\)0.17 & 1.00 & 0.17 & 0.33 & 0.08 \\ Disc.s & 0.00 & 0.25 & 0.58 & 0.00 & 0.33 & 0.08 & 0.00 & \(-\)0.25 & \(-\)0.17 & 0.25 & 0.17 & 0.17 & 1.00 & 0.83 & 0.75 \\ Com. & 0.17 & 0.08 & 0.42 & 0.17 & 0.17 & \(-\)0.08 & \(-\)0.08 & \(-\)0.17 & \(-\)0.42 & \(-\)0.33 & 0.25 & \(-\)0.17 & 0.33 & 0.83 & 1.00 & 0.75 \\ Info. & \(-\)0.08 & \(-\)0.00 & 0.33 & \(-\)0.08 & 0.08 & 0.00 & \(-\)0.17 & \(-\)0.25 & \(-\)0.33 & \(-\)0.42 & 0.50 & 0.08 & 0.08 & 0.75 & 0.75 & 1.00 \\ \hline \hline \end{tabular} \({}^{a}\)[Higgins et al., 2017] [Kim and Mnih, 2018] [Chen et al., 2018] [Eastwood and Williams, 2018]

\end{table}
Table 6: Average Kendall tau rank distances bewteen disentanglement metrics

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c c c c} \hline \hline \hline  & \multicolumn{3}{c}{Moduliarity} & \multicolumn{6}{c}{Informativeness} & \multicolumn{6}{c}{Existing metrics} \\ \cline{2-13}  & \multicolumn{2}{c}{Product approx.} & \multicolumn{2}{c}{Constancy} & \multicolumn{2}{c}{Retraction approx.} & \multicolumn{2}{c}{Contraction} & \multicolumn{2}{c}{Pair} & \multicolumn{2}{c}{Info.} & \multicolumn{2}{c}{Regressor} \\ \cline{2-13}  & Rad. & MAD & Var. & Daam. & MPD & ME & MAE & MSE & Max & Mean & Beta\({}^{a}\) & Factor\({}^{b}\) & MIO\({}^{c}\) & DCF\({}^{d}\) \\ \hline
**3D Cars**[Reed et al., 2015]

### Factor-wise modularity metrics

An advantage of the proposed modularity metrics is that we can even evaluate each factor separately, which is impossible for those metrics that entangle modularity and informativeness. The results were reported in Tables 8 to 11. We found that some learning methods may outperform others on one factor but underperform on others, and different modularity metrics may rank learning methods differently (see also Appendix D.5).

For example, on **MPI3D**[Gondal et al., 2019] (Table 11), \(\beta\)-VAE [Higgins et al., 2017] has the highest scores (radius, MAD, variance, diameter, and MPD) on the horizontal and vertical axis factors, but the lowest scores (radius and diameter) on the object size, camera height, and background color factors. However, as measured by the MAD and MPD, it still has the highest scores on these factors. This means that \(\beta\)-VAE may generally encode the object size, camera height, and background color well compared to other considered methods, but has a small number of outliers. We believe that such fine-grained evaluation can guide the design of learning objectives, data collection, and further refinement of trained representation learning models.

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline  & \multicolumn{3}{c}{Product approx.} & \multicolumn{3}{c}{Constancy} \\ \cline{2-6}  & Rad. & MAD & Var. & Diam. & MPD \\ \hline \multicolumn{6}{l}{Elevation (4)} \\ \hline VAE & \(0.49\) & \(0.87\) & \(0.97\) & \(0.24\) & \(0.90\) \\ \(\beta\)-VAE & \(0.52\) & \(0.86\) & \(0.97\) & \(0.27\) & \(0.90\) \\ FactorVAE & \(0.50\) & \(0.86\) & \(0.97\) & \(0.25\) & \(0.90\) \\ \(\beta\)-TCVAE & \(0.50\) & \(0.87\) & \(0.97\) & \(0.25\) & \(0.90\) \\ \hline \multicolumn{6}{l}{Azimuth (24)} \\ \hline VAE & \(0.62\) & \(0.91\) & \(0.99\) & \(0.39\) & \(0.94\) \\ \(\beta\)-VAE & \(0.60\) & \(0.91\) & \(0.98\) & \(0.37\) & \(0.93\) \\ FactorVAE & \(0.57\) & \(0.90\) & \(0.98\) & \(0.32\) & \(0.93\) \\ \(\beta\)-TCVAE & \(0.65\) & \(0.91\) & \(0.99\) & \(0.42\) & \(0.94\) \\ \hline \multicolumn{6}{l}{Object (183)} \\ \hline VAE & \(0.87\) & \(0.97\) & \(1.00\) & \(0.75\) & \(0.98\) \\ \(\beta\)-VAE & \(0.84\) & \(0.97\) & \(1.00\) & \(0.71\) & \(0.98\) \\ FactorVAE & \(0.85\) & \(0.97\) & \(1.00\) & \(0.72\) & \(0.98\) \\ \(\beta\)-TCVAE & \(0.86\) & \(0.97\) & \(1.00\) & \(0.75\) & \(0.98\) \\ \hline \hline \end{tabular}
\end{table}
Table 8: Factor-wise modularity metrics on **3D Cars**[Reed et al., 2015]

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & \multicolumn{3}{c}{Product approx.} & \multicolumn{3}{c}{Constancy} \\ \cline{2-6}  & Rad. & MAD & Var. & Diam. & MPD \\ \hline \multicolumn{6}{l}{Shape (3)} \\ \hline VAE & \(0.74\) & \(0.89\) & \(0.98\) & \(0.55\) & \(0.92\) \\ \(\beta\)-VAE & \(0.70\) & \(0.88\) & \(0.98\) & \(0.50\) & \(0.92\) \\ FactorVAE & \(0.73\) & \(0.89\) & \(0.98\) & \(0.53\) & \(0.92\) \\ \(\beta\)-TCVAE & \(0.72\) & \(0.88\) & \(0.98\) & \(0.53\) & \(0.92\) \\ \hline \multicolumn{6}{l}{Scale (6)} \\ \hline VAE & \(0.71\) & \(0.90\) & \(0.98\) & \(0.51\) & \(0.93\) \\ \(\beta\)-VAE & \(0.55\) & \(0.88\) & \(0.98\) & \(0.30\) & \(0.92\) \\ FactorVAE & \(0.67\) & \(0.90\) & \(0.98\) & \(0.45\) & \(0.93\) \\ \(\beta\)-TCVAE & \(0.77\) & \(0.90\) & \(0.98\) & \(0.59\) & \(0.93\) \\ \hline \multicolumn{6}{l}{Orientation (40)} \\ \hline VAE & \(0.92\) & \(0.97\) & \(1.00\) & \(0.84\) & \(0.98\) \\ \(\beta\)-VAE & \(0.86\) & \(0.98\) & \(1.00\) & \(0.74\) & \(0.98\) \\ FactorVAE & \(0.95\) & \(0.98\) & \(1.00\) & \(0.90\) & \(0.99\) \\ \(\beta\)-TCVAE & \(0.88\) & \(0.97\) & \(1.00\) & \(0.77\) & \(0.98\) \\ \hline \multicolumn{6}{l}{Position X (32)} \\ \hline VAE & \(0.71\) & \(0.90\) & \(0.98\) & \(0.51\) & \(0.93\) \\ \(\beta\)-VAE & \(0.66\) & \(0.92\) & \(0.99\) & \(0.43\) & \(0.95\) \\ FactorVAE & \(0.62\) & \(0.89\) & \(0.98\) & \(0.39\) & \(0.92\) \\ \(\beta\)-TCVAE & \(0.67\) & \(0.91\) & \(0.99\) & \(0.44\) & \(0.94\) \\ \hline \multicolumn{6}{l}{Position Y (32)} \\ \hline VAE & \(0.68\) & \(0.91\) & \(0.99\) & \(0.47\) & \(0.94\) \\ \(\beta\)-VAE & \(0.66\) & \(0.92\) & \(0.99\) & \(0.44\) & \(0.94\) \\ FactorVAE & \(0.64\) & \(0.90\) & \(0.98\) & \(0.41\) & \(0.93\) \\ \(\beta\)-TCVAE & \(0.67\) & \(0.90\) & \(0.98\) & \(0.45\) & \(0.93\) \\ \hline \hline \end{tabular}
\end{table}
Table 9: Factor-wise modularity metrics on **dSprites**[Matthey et al., 2017]

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & \multicolumn{3}{c}{Product approx.} & \multicolumn{3}{c}{Constancy} \\ \cline{2-6}  & Rad. & MAD & Var. & Diam. & MPD \\ \hline \multicolumn{6}{l}{Floor hue (10)} \\ \hline VAE & \(0.85\) & \(0.97\) & \(1.00\) & \(0.72\) & \(0.98\) \\ \(\beta\)-VAE & \(0.87\) & \(0.97\) & \(1.00\) & \(0.75\) & \(0.98\) \\ FactorVAE & \(0.75\) & \(0.93\) & \(0.99\) & \(0.57\) & \(0.95\) \\ \(\beta\)-TCVAE & \(0.99\) & \(1.00\) & \(1.00\) & \(0.97\) & \(1.00\) \\ \hline \multicolumn{6}{l}{Wall hue (10)} \\ \hline VAE & \(0.85\) & \(0.97\) & \(1.00\) & \(0.73\) & \(0.98\) \\ \(\beta\)-VAE & \(0.94\) & \(0.99\) & \(1.00\) & \(0.87\) & \(0.99\) \\ FactorVAE & \(0.78\) & \(0.94\) & \(0.99\) & \(0.60\) & \(0.96\) \\ \(\beta\)-TCVAE & \(0.82\) & \(0.94\) & \(0.99\) & \(0.68\) & \(0.96\) \\ \hline \multicolumn{6}{l}{Object hue (10)} \\ \hline VAE & \(0.77\) & \(0.95\) & \(0.99\) & \(0.59\) & \(0.96\) \\ \(\beta\)-VAE & \(0.75\) & \(0.92\) & \(0.99\) & \(0.56\) & \(0.95\) \\ FactorVAE & \(0.75\) & \(0.93\) & \(0.99\) & \(0.56\) & \(0.95\) \\ \(\beta\)-TCVAE & \(0.77\) & \(0.92\) & \(0.99\) & \(0.59\) & \(0.95\) \\ \hline \multicolumn{6}{l}{Scale (8)} \\ \hline VAE & \(0.80\) & \(0.95\) & \(0.99\) & \(0.65\) & \(0.96\) \\ \(\beta\)-VAE & \(0.56\) & \(0.91\) & \(0.98\) & \(0.32\) & \(0.93\) \\ FactorVAE & \(0.79\) & \(0.93\) & \(0.99\) & \(0.63\) & \(0.95\) \\ \(\beta\)-TCVAE & \(0.80\) & \(0.95\) & \(1.00\) & \(0.63\) & \(0.97\) \\ \hline \multicolumn{6}{l}{Shape (4)} \\ \hline VAE & \(0.59\) & \(0.91\) & \(0.98\) & \(0.35\) & \(0.93\) \\ \(\beta\)-VAE & \(0.62\) & \(0.91\) & \(0.98\) & \(0.38\) & \(0.93\) \\ FactorVAE & \(0.77\) & \(0.93\) & \(0.99\) & \(0.60\) & \(0.95\) \\ \(\beta\)-TCVAE & \(0.69\) & \(0.92\) & \(0.98\) & \(0.47\) & \(0.94\) \\ \hline \multicolumn{6}{l}{Orientation (15)} \\ \hline VAE & \(0.95\) & \(0.99\) & \(1.00\) & \(0.91\) & \(0.99\) \\ \(\beta\)-VAE & \(0.94\) & \(0.99\) & \(1.00\) & \(0.89\) & \(0.99\) \\ FactorVAE & \(0.89\) & \(0.98\) & \(1.00\) & \(0.80\) & \(0.99\) \\ \(\beta\)-TCVAE & \(0.72\) & \(0.91\) & \(0.98\) & \(0.52\) & \(0.94\) \\ \hline \hline \end{tabular}
\end{table}
Table 10: Factor-wise modularity metrics on **3D Shapes**[10]

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & \multicolumn{3}{c}{Product approx.} & \multicolumn{3}{c}{Constancy} \\ \cline{2-6}  & Rad. & MAD & Var. & Diam. & MPD \\ \hline \multicolumn{6}{l}{Object color (6)} \\ \hline VAE & \(0.52\) & \(0.92\) & \(0.99\) & \(0.27\) & \(0.94\) \\ \(\beta\)-VAE & \(0.51\) & \(0.97\) & \(0.99\) & \(0.26\) & \(0.97\) \\ FactorVAE & \(0.66\) & \(0.94\) & \(0.99\) & \(0.43\) & \(0.96\) \\ \(\beta\)-TCVAE & \(0.57\) & \(0.92\) & \(0.99\) & \(0.33\) & \(0.94\) \\ \hline \multicolumn{6}{l}{Object shape (6)} \\ \hline VAE & \(0.88\) & \(0.97\) & \(1.00\) & \(0.77\) & \(0.98\) \\ \(\beta\)-VAE & \(0.94\) & \(1.00\) & \(1.00\) & \(0.89\) & \(1.00\) \\ FactorVAE & \(0.93\) & \(0.99\) & \(1.00\) & \(0.87\) & \(0.99\) \\ \(\beta\)-TCVAE & \(0.95\) & \(1.00\) & \(1.00\) & \(0.91\) & \(1.00\) \\ \hline \multicolumn{6}{l}{Object size (2)} \\ \hline VAE & \(0.70\) & \(0.93\) & \(0.99\) & \(0.49\) & \(0.95\) \\ \(\beta\)-VAE & \(0.63\) & \(0.98\) & \(1.00\) & \(0.40\) & \(0.98\) \\ FactorVAE & \(0.68\) & \(0.94\) & \(0.99\) & \(0.46\) & \(0.95\) \\ \(\beta\)-TCVAE & \(0.75\) & \(0.96\) & \(1.00\) & \(0.56\) & \(0.97\) \\ \hline \multicolumn{6}{l}{Camera height (3)} \\ \hline VAE & \(0.48\) & \(0.87\) & \(0.97\) & \(0.23\) & \(0.90\) \\ \(\beta\)-VAE & \(0.34\) & \(0.95\) & \(0.99\) & \(0.11\) & \(0.96\) \\ FactorVAE & \(0.58\) & \(0.85\) & \(0.96\) & \(0.34\) & \(0.90\) \\ \(\beta\)-TCVAE & \(0.69\) & \(0.91\) & \(0.99\) & \(0.47\) & \(0.94\) \\ \hline \multicolumn{6}{l}{Background color (3)} \\ \hline VAE & \(0.60\) & \(0.92\) & \(0.99\) & \(0.36\) & \(0.94\) \\ \(\beta\)-VAE & \(0.34\) & \(0.96\) & \(0.99\) & \(0.11\) & \(0.97\) \\ FactorVAE & \(0.76\) & \(0.94\) & \(0.99\) & \(0.58\) & \(0.96\) \\ \(\beta\)-TCVAE & \(0.59\) & \(0.93\) & \(0.99\) & \(0.34\) & \(0.95\) \\ \hline \multicolumn{6}{l}{Horizontal axis (40)} \\ \hline VAE & \(0.72\) & \(0.93\) & \(0.99\) & \(0.52\) & \(0.95\) \\ \(\beta\)-VAE & \(0.74\) & \(0.99\) & \(1.00\) & \(0.55\) & \(0.99\) \\ FactorVAE & \(0.69\) & \(0.92\) & \(0.99\) & \(0.47\) & \(0.94\) \\ \(\beta\)-TCVAE & \(0.73\) & \(0.94\) & \(0.99\) & \(0.54\) & \(0.96\) \\ \hline \multicolumn{6}{l}{Vertical axis (40)} \\ \hline VAE & \(0.67\) & \(0.91\) & \(0.99\) & \(0.45\) & \(0.94\) \\ \(\beta\)-VAE & \(0.75\) & \(0.99\) & \(1.00\) & \(0.56\) & \(0.99\) \\ FactorVAE & \(0.73\) & \(0.93\) & \(0.99\) & \(0.53\) & \(0.95\) \\ \(\beta\)-TCVAE & \(0.60\) & \(0.93\) & \(0.99\) & \(0.36\) & \(0.95\) \\ \hline \hline \end{tabular}
\end{table}
Table 11: Factor-wise modularity metrics on **MPI3D**[Gondal et al., 2019]

## NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: "_A theoretical connection between logical definitions of disentanglement and quantitative metrics_" was detailed in Appendices A and B. "_A systematic approach for converting a first-order predicate into a real-valued quantity_" was introduced in Section 3. "_The metrics induced by logical definitions,_" which were introduced in Section 4, "_have strong theoretical guarantees,_" which was supported by Theorem 1. "_The effectiveness of the proposed metrics_" was empirically validated in Section 5 "_by isolating different aspects of disentangled representations_" and further investigated in Appendix E. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discussed the limitations in Appendix D.7. Another limitation is that we only studied the properties of the minimizers of the metrics (Theorem 1). Research on the behavior of these metrics on imperfect representations is limited (Appendix D.5). We claimed that some proposed metrics can serve as differentiable learning objectives, but empirical evaluation is out of the scope of this work. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: All definitions of the proposed terms, theoretical results, and detailed proofs were provided in Appendices A to C. The main theoretical result was stated in Theorem 1 and instantiated in Section 4. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The contribution is primarily the new theoretical framework. For experiments, we provided code snippets in Appendix D.6. We provided the detailed data configuration used in Section 5 in Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provided the code to reproduce the modularity metrics in Table 2. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/pu blic/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The results of the proposed metrics reported in Table 2 are deterministic and do not contain randomness. We reported the mean values of \(10\) random trials in Table 4, and the variances are negligible. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean.

* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We reviewed the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See Appendix D.8. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.

* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

* Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper mainly focused on the theory of disentangled representation learning. Guidelines:
* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licensees for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: See Appendix E. Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects**Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.