# Beta Diffusion

Mingyuan Zhou, Tianqi Chen, Zhendong Wang, and Huangjie Zheng

The University of Texas at Austin

Austin, TX 78712

Corresponding to:mingyuan.zhou@mccombs.utexas.edu

###### Abstract

We introduce beta diffusion, a novel generative modeling method that integrates demasking and denoising to generate data within bounded ranges. Using scaled and shifted beta distributions, beta diffusion utilizes multiplicative transitions over time to create both forward and reverse diffusion processes, maintaining beta distributions in both the forward marginals and the reverse conditionals, given the data at any point in time. Unlike traditional diffusion-based generative models relying on additive Gaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is multiplicative and optimized with KL-divergence upper bounds (KLUBs) derived from the convexity of the KL divergence. We demonstrate that the proposed KLUBs are more effective for optimizing beta diffusion compared to negative ELBOs, which can also be derived as the KLUBs of the same KL divergence with its two arguments swapped. The loss function of beta diffusion, expressed in terms of Bregman divergence, further supports the efficacy of KLUBs for optimization. Experimental results on both synthetic data and natural images demonstrate the unique capabilities of beta diffusion in generative modeling of range-bounded data and validate the effectiveness of KLUBs in optimizing diffusion models, thereby making them valuable additions to the family of diffusion-based generative models and the optimization techniques used to train them.

## 1 Introduction

Diffusion-based deep generative models have been gaining traction recently. One representative example is Gaussian diffusion [57, 59, 23, 60, 35] that uses a Gaussian Markov chain to gradually diffuse images into Gaussian noise for training. The learned reverse diffusion process, defined by a Gaussian Markov chain in reverse order, iteratively refines noisy inputs towards clean photo-realistic images. Gaussian diffusion can also be viewed from the lens of denoising score matching [28, 64, 59, 60] and stochastic differential equations [61]. They have shown remarkable success across a wide range of tasks, including but not limited to generating, restoring, and editing images [14, 24, 50, 53, 54, 70, 67], transforming 2D to 3D [49, 1], synthesizing audio [9, 37, 73], reinforcement learning [31, 66, 48], quantifying uncertainty [19], and designing drugs and proteins [56, 44, 32].

Constructing a diffusion-based generative model often follows a general recipe [57, 23, 35, 34]. The recipe involves three basic steps: First, defining a forward diffusion process that introduces noise into the data and corrupts it with decreasing signal-to-noise ratio (SNR) as time progresses from 0 to 1. Second, defining a reverse diffusion process that denoises the corrupted data as time reverses from 1 to 0. Third, discretizing the time interval from 0 to 1 into a finite number of intervals, and viewing the discretized forward and reverse processes as a fixed inference network and a learnable generator, respectively. Auto-encoding variational inference [36, 51] is then applied to optimize the parameters of the generator by minimizing a weighted negative ELBO that includes a Kullback-Leibler (KL) divergence-based loss term for each discretized reverse step.

Although the general diffusion-modeling recipe is simple in concept, it requires access to the corrupted data at any time during the forward diffusion process given a data observation, as well as the analytic form of the conditional posterior for any earlier time given both a data observation and its corrupted version at the present time. The latter requirement, according to Bayes' rule, implies access to the analytical form of the distribution of a corrupted data observation at the present time given its value at any previous time. Linear operations of the Gaussian distributions naturally satisfy these requirements since they are conjugate to themselves with respect to their mean parameters. This means that the marginal form and the conditional distribution of the mean remain Gaussian when two Gaussian distributions are mixed. Similarly, the requirements can be met under the categorical distribution [26, 2, 18, 27] and Poisson distribution [10]. However, few additional distributions are known to meet these requirements and it remains uncertain whether negative ELBO would be the preferred loss.

While previous works have primarily used Gaussian, categorical, or Poisson distribution-based diffusion processes, this paper introduces beta diffusion as a novel addition to the family of diffusion-based generative models. Beta diffusion is specifically designed to generate data within bounded ranges. Its forward diffusion process is defined by the application of beta distributions in a multiplicative manner, whereas its reverse diffusion process is characterized by the use of scaled and shifted beta distributions. Notably, the distribution at any point in time of the forward diffusion given a data observation remains a beta distribution. We illustrate the forward beta diffusion process in Figure 1, which simultaneously adds noise to and masks the data, and the reverse one in Figure 2, which iteratively performs demasking and denoising for data generation. We provide the details on how these images are obtained in Appendix D.

Since the KL divergence between two beta distributions is analytic, one can follow the general recipe to define a negative ELBO to optimize beta diffusion. However, our experiments show that minimizing the negative ELBO of beta diffusion can fail to optimally estimate the parameters of the reverse diffusion process. For each individual time-dependent KL loss term of the negative ELBO, examining it in terms of Bregman divergence [4] reveals that the model parameters and corrupted data are placed in its first and second arguments, respectively. However, to ensure that the optimal solution under the Bregman divergence agrees with the expectation of the clean data given the corrupted data, the order of the two arguments must be swapped.

By swapping the Bregman divergence's two arguments, we obtain an upper bound on the KL divergence from the joint distribution of corrupted observations in the reverse chain to that in the forward chain. This bound arises from the convexity of the KL divergence. In addition, there exists another Bregman divergence that upper bounds the KL divergence from the univariate marginal of a corrupted observation in the reverse chain to that in the forward chain. These two Bregman divergences, which can be derived either through KL divergence upper bounds (KLUBs) or the logarithmically convex beta function, share the same optimal solution but have distinct roles in targeting reverse accuracy at each step or counteracting accumulation errors over the course of reverse

Figure 1: Illustration of the beta forward diffusion process for two example images. The first column displays the original images, while the other 21 columns display the images noised and masked by beta diffusion at time \(t=0,0.05,\ldots,1\), using \(\eta=10000\) and the sigmoid diffusion schedule with \(c_{0}=10\) and \(c_{1}=-13\).

Figure 2: Illustration of reverse beta diffusion for two example generations. The time \(t\) decreases from 1 to 0 when moving from left to right. In each image generation, the top row shows the trajectory of \(z_{t}\) (rescaled for visualization), which has been demasked and denoised using reverse diffusion, whereas the bottom row shows \(\dot{x}_{0}=f_{\theta}(z_{t},t)\), whose theoretical optimal solution is equal to \(\mathbb{E}[x_{0}\,|\,z_{t}]\). See Appendix D for more details.

diffusion. We further demonstrate that combining these two KLUBs presents a computationally viable substitution for a KLUB derived at the chain level, which upper bounds the KL divergence from the joint distribution of all latent variables in a forward diffusion chain to that of its reverse. Either KLUB works on its own, which is not unexpected as they both share the same optimal solutions in theory, but combining them could lead to the best overall performance. In beta diffusion, the KL divergence is asymmetric, which enables us to derive an alternative set of KLUBs by swapping its two arguments. We will demonstrate that these augment-swapped KLUBs, which will be referred to as AS-KLUBs, essentially reduce to negative ELBOs. In Gaussian diffusion, the KL divergence is often made symmetric, resulting in KLUBs that are equivalent to (weighted) negative ELBOs.

Our main contributions are the introduction of beta diffusion as a novel diffusion-based multiplicative generative model for range-bounded data, as well as the proposal of KLUBs as effective loss objectives for optimizing diffusion models, in place of (weighted) negative ELBOs. Additionally, we introduce the log-beta divergence, a Bregman divergence corresponding to the differentiable and strictly convex log-beta function, as a useful tool for analyzing KLUBs. These contributions enhance the existing family of diffusion-based generative models and provide a new perspective on optimizing them.

## 2 Beta Diffusion and Optimization via KLUBs

We begin by specifying the general requirements for constructing a diffusion-based generative model and establish the notation accordingly [57; 23; 35]. Let \(x_{0}\) denote the observed data, and let \(z_{s}\) and \(z_{t}\) represent their corrupted versions at time \(s\) and time \(t\), respectively, where \(0<s<t<1\). In the forward diffusion process, we require access to random samples from the marginal distribution \(q(z_{t}\,|\,x_{0})\) at any time \(t\), as well as an analytical expression of the probability density function (PDF) of the conditional distribution \(q(z_{s}\,|\,z_{t},x_{0})\) for any \(s<t\).

The forward beta diffusion chain uses diffusion scheduling parameters \(\alpha_{t}\) to control the decay of its expected value over the course of forward diffusion, given by \(\mathbb{E}[z_{t}\,|\,x_{0}]=\alpha_{t}x_{0}\), and a positive concentration parameter \(\eta\) to control the tightness of the diffusion process around its expected value. We typically set \(\alpha_{t}\) to approach 1 and 0 as \(t\) approaches 0 and 1, respectively, and satisfy the condition

\[1\geq\alpha_{0}>\alpha_{s}>\alpha_{t}>\alpha_{1}\geq 0\text{ for all }s\in(0,t),\;t\in(0,1).\]

Let \(\Gamma(\cdot)\) denote the gamma function and \(B(\cdot,\cdot)\) denote the beta function. The beta distribution \(\text{Beta}(x;a,b)=B(a,b)^{-1}x^{a-1}(1-x)^{b-1}\) is a member of the exponential family [6; 65; 7]. Its log partition function is a log-beta function as \(\ln B(a,b)=\ln\Gamma(a)+\ln\Gamma(b)-\ln\Gamma(a+b),\) which is differentiable, and strictly convex on \((0,\infty)^{2}\) as a function of two variables [15]. As a result, the KL divergence between two beta distributions can be expressed as the Bregman divergence associated with the log-beta function. Specifically, as in Appendix A, one can show by their definitions that

\[\text{KL}(\text{Beta}(\alpha_{p},\beta_{p})||\text{Beta}(\alpha _{q},\beta_{q})) =\ln\frac{B(\alpha_{q},\beta_{q})}{B(\alpha_{p},\beta_{p})}-( \alpha_{q}-\alpha_{p},\beta_{q}-\beta_{p})\begin{pmatrix}\nabla_{\alpha}\ln B (\alpha_{p},\beta_{p})\\ \nabla_{\beta}\ln B(\alpha_{p},\beta_{p})\end{pmatrix}\] (1) \[=D_{\ln B(a,b)}((\alpha_{q},\beta_{q}),(\alpha_{p},\beta_{p})).\]

We refer to the above Bregman divergence as the log-beta divergence. Moreover, if \((\alpha_{q},\beta_{q})\) are random variables, applying Proposition 1 of Banerjee et al. [4], we can conclude that the optimal value of \((\alpha_{p},\beta_{p})\) that minimizes this log-beta divergence is \((\alpha_{p}^{*},\beta_{p}^{*})=\mathbb{E}[(\alpha_{q},\beta_{q})]\).

Next, we introduce a conditional bivariate beta distribution, which given a data observation has (scaled and shifted) beta distributions for not only its two marginals but also two conditionals. These properties are important for developing the proposed diffusion model with multiplicative transitions.

### Conditional Bivariate Beta Distribution

We first present the conditional bivariate beta distribution in the following Lemma, which generalizes previous results on the distribution of the product of independent beta random variables [30; 38; 33].

**Lemma 1** (Conditional Beta Bivariate Distribution).: _Denote \((z_{s},z_{t})\) as variables over a pair of time points \((s,t)\), with \(0<s<t<1\). Given a random sample \(x_{0}\in(0,1)\) from a probability-valued data distribution \(p_{data}(x_{0})\), we define a conditional bivariate beta distribution over \((z_{s},z_{t})\) with PDF:_

\[q(z_{s},z_{t}\,|\,x_{0})=\frac{\Gamma(\eta)}{\Gamma(\eta\alpha_{t}x_{0}) \Gamma(\eta(1-\alpha_{s}x_{0}))\Gamma(\eta(\alpha_{s}-\alpha_{t})x_{0})}\frac{ z_{t}^{\eta\alpha_{t}x_{0}-1}(1-z_{s})^{\eta(1-\alpha_{s}x_{0})-1}}{(z_{s}-z_{t})^{1 -\eta(\alpha_{s}-\alpha_{t})x_{0}}}.\] (2)_Marginals: Given \(x_{0}\), the two univariate marginals of this distribution are both beta distributed as_

\[q(z_{s}\,|\,x_{0}) =\text{Beta}(\eta\alpha_{s}x_{0},\eta(1-\alpha_{s}x_{0})),\] (3) \[q(z_{t}\,|\,x_{0}) =\text{Beta}(\eta\alpha_{t}x_{0},\eta(1-\alpha_{t}x_{0})).\] (4)

_Conditions: Given \(x_{0}\), a random sample \((z_{t},z_{s})\) from this distribution can be either generated in forward order, by multiplying a beta variable from (3) with beta variable \(\pi_{s\to t}\), as_

\[z_{t}=z_{s}\pi_{s\to t},\ \ \pi_{s\to t}\sim\text{Beta}(\eta\alpha_{t}x_{0}, \eta(\alpha_{s}-\alpha_{t})x_{0}),\ \ z_{s}\sim q(z_{s}\,|\,x_{0}),\] (5)

_or generated in reverse order, by combining a beta variable from (4) with beta variable \(p_{s\gets t}\), as_

\[z_{s}=z_{t}+(1-z_{t})p_{s\gets t},\ \ p_{s\gets t}\sim\text{ Beta}(\eta(\alpha_{s}-\alpha_{t})x_{0},\eta(1-\alpha_{s}x_{0})),\ \ z_{t}\sim q(z_{t}\,|\,x_{0}).\] (6)

The proof starts with applying change of variables to obtain two scaled and shifted beta distributions

\[q(z_{t}\,|\,z_{s},x_{0}) =\tfrac{1}{z_{s}}\text{Beta}\left(\tfrac{z_{t}}{z_{s}};\eta\alpha _{t}x_{0},\eta(\alpha_{s}-\alpha_{t})x_{0}\right),\] (7) \[q(z_{s}\,|\,z_{t},x_{0}) =\tfrac{1}{1-z_{t}}\text{Beta}\left(\tfrac{z_{s}-z_{t}}{1-z_{t}}; \eta(\alpha_{s}-\alpha_{t})x_{0},\eta(1-\alpha_{s}x_{0})\right),\] (8)

and then takes the products of them with their corresponding marginals, given by (3) and (4), respectively, to show that the PDF of the joint distribution defined in (5) and that defined in (6) are both equal to the PDF of \(q(z_{s},z_{t}\,|\,x_{0})\) defined in (2). The detailed proof is provided in Appendix B. To ensure numerical accuracy, we will calculate \(z_{s}=z_{t}+(1-z_{t})p_{s\gets t}\) in (6) in the logit space as

\[\text{logit}(z_{s})=\ln\big{(}e^{\text{logit}(z_{t})}+e^{\text{ logit}(p_{s\gets t})}+e^{\text{logit}(z_{t})+\text{logit}(p_{s\gets t})} \big{)}.\] (9)

### Continuous Beta Diffusion

Forward Beta Diffusion.We can use the conditional bivariate beta distribution to construct a forward beta diffusion chain, beginning with the beta distribution from (3) and proceeding with the scaled beta distribution from (7). The marginal at any time \(t\) for a given data observation \(x_{0}\), as shown in (4), stays as beta-distributed in the forward chain. For the beta distribution given by (4), we have

\[\mathbb{E}[z_{t}\,|\,x_{0}]=\alpha_{t}x_{0},\ \ \text{var}[z_{t}\,|\,x_{0}]= \tfrac{(\alpha_{t}x_{0})(1-\alpha_{t}x_{0})}{\eta+1},\ \ \text{SNR}_{t}=\Big{(}\tfrac{\mathbb{E}[z_{t}\,|\,x_{0}]}{ \text{std}[z_{t}\,|\,x_{0}]}\Big{)}^{2}=\tfrac{\alpha_{t}x_{0}(\eta+1)}{1- \alpha_{t}x_{0}}.\]

Thus when \(\alpha_{t}\) approaches 0 (\(i.e.\), \(t\to 1\)), both \(z_{t}\) and \(\text{SNR}_{t}\) are shrunk towards 0, and if \(\alpha_{1}=0\), we have \(z_{1}\sim\text{Beta}(0,\eta)\), a degenerate beta distribution that becomes a unit point mass at 0.

**Infinite Divisibility.** We consider beta diffusion as a form of continuous diffusion, as its forward chain is infinitely divisible given \(x_{0}\). This means that for any time \(k\in(s,t)\), we can perform forward diffusion from \(z_{s}\) to \(z_{t}\) by first setting \(z_{k}=z_{s}\pi_{s\to k}\), where \(\pi_{s\to k}\sim\text{Beta}(\eta\alpha_{k}x_{0},\eta(\alpha_{k}-\alpha_{s})x_ {0})\), and then setting \(z_{t}=z_{k}\pi_{k\to t}\), where \(\pi_{k\to t}\sim\text{Beta}(\eta\alpha_{t}x_{0},\eta(\alpha_{t}-\alpha_{k})x_ {0})\). The same approach can be used to show the infinite divisibility of reverse beta diffusion given \(x_{0}\).

**Reverse Beta Diffusion.** We follow Gaussian diffusion to use \(q(z_{s}\,|\,z_{t},x_{0})\) to help define \(p_{\theta}(z_{s}\,|\,z_{t})\)[58, 35, 72]. To construct a reverse beta diffusion chain, we will first need to learn how to reverse from \(z_{t}\) to \(z_{s}\), where \(s<t\). If we know the \(x_{0}\) used to sample \(z_{t}\) as in (3), then we can readily apply the conditional in (8) to sample \(z_{s}\). Since this information is unavailable during inference, we make a weaker assumption that we can exactly sample from \(z_{t}\sim q(z_{t})=\mathbb{E}_{x_{0}\sim p_{data}}[q(z_{t}\,|\,x_{0})]\), which is the "\(x_{0}\)-free" univariate marginal at time \(t\). It is straightforward to sample during training but must be approximated during inference. Under this weaker assumption on \(q(z_{t})\), utilizing (8) but replacing its true \(x_{0}\) with an approximation \(\hat{x}_{0}=f_{\theta}(z_{t},t)\), where \(f_{\theta}\) denotes the learned generator parameterized by \(\theta\), we introduce our "\(x_{0}\)-free" and hence "causal" time-reversal distribution as

\[p_{\theta}(z_{s}\,|\,z_{t})=q(z_{s}\,|\,z_{t},\hat{x}_{0}=f_{ \theta}(z_{t},t)).\] (10)

### Optimization via KLUBs and Log-Beta Divergence

KLUB for Time Reversal.The time-reversal distribution \(p_{\theta}(z_{s}\,|\,z_{t})\) reaches its optimal when its product with \(q(z_{t})\) becomes equivalent to \(q(z_{s},z_{t})=\mathbb{E}_{x_{0}\sim p_{data}}[q(z_{s},z_{t}\,|\,x_{0})]\), which is a marginal bivariate distribution that is "\(x_{0}\)-free." Thus we propose to optimize \(\theta\) by minimizing \(\text{KL}(p_{\theta}(z_{s}\,|\,z_{t})q(z_{t})||q(z_{s},z_{t}))\) in theory but introduce a surrogate loss in practice:

**Lemma 2** (Klub (conditional)).: _The KL divergence from \(q(z_{s},z_{t})\) to \(p_{\theta}(z_{s}\,|\,z_{t})q(z_{t})\), two "\(x_{0}\)-free" joint distributions defined by forward and reverse diffusions, respectively, can be upper bounded:_

\[\text{KL}(p_{\theta}(z_{s}\,|\,z_{t})q(z_{t})||q(z_{s},z_{t})) \leq\text{KLUB}_{s,t}=\mathbb{E}_{(z_{t},x_{0})\sim q(z_{t}\,|\,x_ {0})p_{data}(x_{0})}[\text{KLUB}(s,z_{t},x_{0})],\] (11) \[\text{KLUB}(s,z_{t},x_{0}) =\text{KL}(q(z_{s}\,|\,z_{t},\hat{x}_{0}=f_{\theta}(z_{t},t))||q(z _{s}\,|\,z_{t},x_{0}))].\] (12)

The proof in Appendix B utilizes the equation \(p_{\theta}(z_{s}\,|\,z_{t})q(z_{t})=\mathbb{E}_{x_{0}\sim p_{data}}[p_{\theta }(z_{s}\,|\,z_{t})q(z_{t}\,|\,x_{0})]\) and then applies the convexity of the KL divergence [11, 74] and the definition in (10).

Log-Beta DivergenceTo find out the optimal solution under KLUB, following (1), we can express \(\text{KLUB}(s,z_{t},x_{0})\) given by (12) as a log-beta divergence as

\[D_{\ln B(a,b)}\big{\{}\left[\eta(\alpha_{s}-\alpha_{t})x_{0}, \eta(1-\alpha_{s}x_{0})\right],\ \left[\eta(\alpha_{s}-\alpha_{t})f_{\theta}(z_{t},t), \eta(1-\alpha_{s}f_{\theta}(z_{t},t))\right]\big{\}}.\] (13)

We note \(\text{KLUB}_{s,t}\) defined in (11) can also be written as \(\mathbb{E}_{z_{t}\sim q(z_{t})}\mathbb{E}_{x_{0}\sim q(x_{0}\,|\,z_{t})}[ \text{KLUB}(s,z_{t},x_{0})]\), where the log-beta divergence for \(\text{KLUB}(s,z_{t},x_{0})\), defined as in (13), includes \(x_{0}\sim q(x_{0}\,|\,z_{t})\) in its first argument and the generator \(f_{\theta}(z_{t},t)\) in its second argument. Therefore, applying Proposition 1 of Banerjee et al. [4], we have the following Lemma.

**Lemma 3**.: _The objective \(\text{KLUB}_{s,t}\) defined in (11) for any \(s<t\) is minimized when_

\[f_{\theta^{*}}(z_{t},t)=\mathbb{E}[x_{0}\,|\,z_{t}]=\mathbb{E}_ {x_{0}\sim q(x_{0}\,|\,z_{t})}[x_{0}]\quad\text{ for all }z_{t}\sim q(z_{t}).\] (14)

Thus under the \(\text{KLUB}_{s,t}\)-optimized \(\theta^{*}\), we have \(p_{\theta^{*}}(z_{s}\,|\,z_{t})=q(z_{s}\,|\,z_{t},\mathbb{E}[x_{0}\,|\,z_{t}])\), which is different from the optimal solution of the original KL loss in (11), which is \(p_{\theta}^{*}(z_{s}\,|\,z_{t})=q(z_{s},z_{t})/q(z_{t})=q(z_{s}\,|\,z_{t})= \mathbb{E}_{x_{0}\sim q(x_{0}\,|\,z_{t})}[q(z_{s}\,|\,z_{t},x_{0})]\). It is interesting to note that they only differ on whether the expectation is carried out inside or outside the conditional posterior.

In practice, we need to control the gap between \(p_{\theta^{*}}(z_{s}\,|\,z_{t})\) and \(q(z_{s}\,|\,z_{t})\) and hence \(s\) needs to be close to \(t\). Furthermore, the assumption of having access to unbiased samples from the true marginal \(q(z_{t})\) is also rarely met. Thus we need to discretize the time from 1 to \(t\) into sufficiently fine intervals and perform time-reversal sampling over these intervals. Specifically, we can start with \(z_{1}=0\) and iterate (10) over these intervals to obtain an approximate sample from \(z_{t}\sim q(z_{t})\). However, the error could accumulate along the way from \(z_{1}\) to \(z_{t}\), to which we present a solution below.

KLUB for Error Accumulation ControlTo counteract error accumulation during time reversal, we propose to approximate the true marginal \(q(z^{\prime}_{t})\) using a "distribution-cycle-consistency" approach. This involves feeding a random sample \(z_{t}\) from \(q(z_{t})\) into the generator \(f_{\theta}\), followed by the forward marginal \(q(z^{\prime}_{t}\,|\,\hat{x}_{0}=f_{\theta}(z_{t},t))\), with the aim of recovering the distribution \(q(z^{\prime}_{t})\) itself. Specifically, we propose to approximate \(q(z^{\prime}_{t})\) with \(p_{\theta}(z^{\prime}_{t}):=\mathbb{E}_{z_{t}\sim q(z_{t})}[q(z^{\prime}_{t}\,| \,\hat{x}_{0}=f_{\theta}(z_{t},t))]\) by minimizing \(\text{KL}(p_{\theta}(z^{\prime}_{t})||q(z^{\prime}_{t}))\) in theory, but introducing a surrogate loss in practice:

**Lemma 4** (Klub (marginal)).: _The KL divergence \(\text{KL}(p_{\theta}(z^{\prime}_{t})||q(z^{\prime}_{t}))\) can be upper bounded:_

\[\text{KL}(p_{\theta}(z^{\prime}_{t})||q(z^{\prime}_{t}))\leq\text {KLUB}_{t}=\mathbb{E}_{(z_{t},x_{0})\sim q(z_{t}\,|\,x_{0})p_{data}(x_{0})}[ \text{KLUB}(z_{t},x_{0})],\] (15) \[\text{KLUB}(z_{t},x_{0})=\text{KL}(q(z^{\prime}_{t}\,|\,f_{ \theta}(z_{t},t))||q(z^{\prime}_{t}\,|\,x_{0})).\] (16)

The proof in Appendix B utilizes the fact that \(q(z^{\prime}_{t})=\mathbb{E}_{(z_{t},x_{0})\sim q(z_{t}\,|\,x_{0})p_{data}(x_{0} )}[q(z^{\prime}_{t}\,|\,x_{0})]\).

Note that the mathematical definition of KLUB is reused throughout the paper and can refer to any of the equations (11), (12), (15), or (16) depending on the context. Similar to previous analysis, we have

\[\text{KLUB}(z_{t},x_{0})=D_{\ln B(a,b)}\big{\{}\left[\eta\alpha_{ t}x_{0},\eta(1-\alpha_{t}x_{0})\right],\ \left[\eta\alpha_{t}f_{\theta}(z_{t},t),\eta(1-\alpha_{t}f_{\theta}(z_{t},t)) \right]\big{\}},\] (17)

and can conclude with the following Lemma.

**Lemma 5**.: \(\text{KLUB}_{t}\) _in (15) is optimized when the same optimal solution given by (14) is met._

Optimization via KLUBsWith the two KLUBs for both time reversal and error accumulation control, whose optimal solutions are the same as in (14), we are ready to optimize the generator \(f_{\theta}\) via stochastic gradient descent (SGD). Specifically, denoting \(\omega,\pi\in[0,1]\) as two weight coefficients, the loss term for the \(i\)th data observation \(x_{0}^{(i)}\) in a mini-batch can be computed as

\[\mathcal{L}_{i}=\omega\text{KLUB}(s_{i},z_{t_{i}},x_{0}^{(i)})+(1-\omega)\text {KLUB}(z_{t_{i}},x_{0}^{(i)}),\] (18) \[z_{t_{i}}\sim q(z_{t_{i}}\,|\,x_{0}^{(i)})=\text{Beta}(\eta \alpha_{t_{i}}x_{0}^{(i)},\eta(1-\alpha_{t_{i}}x_{0}^{(i)})),\ \ s_{i}=\pi t_{i},\quad t_{i}\sim\text{Unif}(0,1).\]

### Discretized Beta Diffusion for Generation of Range-Bounded Data

For generating range-bounded data, we discretize the beta diffusion chain. Denote \(z_{t_{0}}=1\) and let \(t_{j}\) increase with \(j\). Repeating (5) for \(T\) times, we define a discretized forward beta diffusion chain:

\[q(z_{t_{1:T}}\,|\,x_{0})=\prod_{j=1}^{T}q(z_{t_{j}}\,|\,z_{t_{j-1}},x_{0})=\prod_ {j=1}^{T}\tfrac{1}{z_{t_{j-1}}}\text{Beta}\left(\tfrac{z_{t_{j}}}{z_{t_{j-1}}}; \eta\alpha_{t_{j}}x_{0},\eta(\alpha_{t_{j-1}}-\alpha_{t_{j}})x_{0}\right).\] (19)

A notable feature of (19) is that the marginal at any discrete time step \(t_{j}\) follows a beta distribution, similarly defined as in (4). We also note while \(q(z_{t_{1:T}}\,|\,x_{0})\) defines a Markov chain, the marginal

\[q(z_{t_{1:T}})=\mathbb{E}_{x_{0}\sim p_{data}(x_{0})}[q(z_{t_{1:T}}\,|\,x_{0})]\] (20)

in general does not. Unlike in beta diffusion, where the transitions between \(z_{t}\) and \(z_{t-1}\) are applied multiplicatively, in Gaussian diffusion, the transitions between \(z_{t}\) and \(z_{t-1}\) are related to each other additively and \(z_{t_{1:T}}\) forms a Markov chain regardless of whether \(x_{0}\) is marginalized out.

The discretized forward beta diffusion chain given by (19) is reversible assuming knowing \(x_{0}\). This means given \(x_{0}\), it can be equivalently sampled in reverse order by first sampling \(z_{t_{T}}\sim q(z_{t_{T}}\,|\,x_{0})=\text{Beta}(\eta\alpha_{t_{T}}x_{0},\eta (1-\alpha_{t_{T}}x_{0}))\) and then repeating (8) for \(t_{T},\ldots,t_{2}\), with PDF \(q(z_{t_{1:T}}\,|\,x_{0})=q(z_{t_{T}}\,|\,x_{0})\prod_{j=2}^{T}\tfrac{1}{1-z_{t _{j}}}\text{Beta}\left(\tfrac{z_{t_{j-1}}-z_{t_{j}}}{1-z_{t_{j}}};\eta(\alpha_ {t_{j-1}}-\alpha_{t_{j}})x_{0},\eta(1-\alpha_{t_{j-1}}x_{0})\right).\) This non-causal chain, while not useful by itself, serves as a blueprint for approximate generation.

Specifically, we approximate the marginal given by (20) with a Markov chain in reverse order as

\[p_{\theta}(z_{t_{1:T}})=p_{prior}(z_{t_{T}})\prod_{j=2}^{T}p_{\theta}(z_{t_{j- 1}}\,|\,z_{t_{j}})=p_{prior}(z_{t_{T}})\prod_{j=2}^{T}q(z_{t_{j-1}}\,|\,z_{t_{j }},\,f_{\theta}(z_{t_{j}},\alpha_{t_{j}})).\] (21)

To start the reverse process, we choose to approximate \(q(z_{t_{T}})=\mathbb{E}_{x_{0}\sim p_{data}(x_{0})}[q(z_{t_{T}}\,|\,x_{0})]\) with \(p_{prior}(z_{t_{T}})=q(z_{t_{T}}\,|\,\mathbb{E}[x_{0}])\), which means we let \(z_{t_{T}}\sim\text{Beta}(\eta\alpha_{t_{T}}\mathbb{E}[x_{0}],\eta(1-\alpha_{t_ {T}}\mathbb{E}[x_{0}]))\). To sample \(z_{t_{1:T-1}}\), we use the remaining terms in (21), which are scaled and shifted beta distributions that are specified as in (8) and can be sampled as in (6) and (9).

KLUB for Discretized Beta Diffusion.An optimized generator is expected to make the "\(x_{0}\)-free" joint distribution over all \(T\) steps of the discretized reverse beta diffusion chain, expressed as \(p_{\theta}(z_{t_{1:T}})\), to approach that of the discretized forward chain, expressed as \(q(z_{t_{1:T}})\). Thus an optimized \(\theta\) is desired to minimize the KL divergence \(\text{KL}(p_{\theta}(z_{t_{1:T}})||q(z_{t_{1:T}}))\). While this KL loss is in general intractable to compute, it can also be bounded using the KLUB shown as follows.

**Lemma 6** (KLUB for discretized diffusion chain).: \(\text{KL}(p_{\theta}(z_{t_{1:T}})||q(z_{t_{1:T}}))\) _is upper bounded by_

\[\text{KLUB}=\mathbb{E}_{x_{0}\sim p_{data}(x_{0})}\left[\text{KL}(p _{prior}(z_{t_{T}})||q(z_{t_{T}}\,|\,x_{0}))\right]+\sum_{j=2}^{T}\widetilde{ \text{KLUB}}_{t_{j-1},t_{j}}\quad\text{where}\] (22) \[\widetilde{\text{KLUB}}_{s,t}=\mathbb{E}_{(z_{t},x_{0})\sim p_{ \theta}(z_{t}\,|\,x_{0})p_{data}(x_{0})}[\text{KLUB}(s,z_{t},x_{0})],\ \ \text{KLUB}(s,z_{t},x_{0})=\text{KL}(p_{\theta}(z_{s}\,|\,z_{t})||q(z_{s}\,|\,z_{ t},x_{0}))].\]

We provide the proof in Appendix B. We note Lemma 6 is a general statement applicable for any diffusion models with a discrete forward chain \(q(z_{t_{1:T}}\,|\,x_{0})=\prod_{j=1}^{T}q(z_{t_{j}}\,|\,z_{t_{j-1}},x_{0})\) and a discrete reverse chain \(p_{\theta}(z_{t_{1:T}})=p_{prior}(z_{t_{T}})\prod_{j=2}^{T}p_{\theta}(z_{t_{j-1 }}\,|\,z_{t_{j}})\). To estimate the KLUB in (22), however, during training, one would need to sample \(z_{t_{j}}\sim p_{\theta}(z_{t_{j}}\,|\,x_{0})\propto p(x_{0}\,|\,z_{t_{j}})p_{ \theta}(z_{t_{j}})\), which is often infeasible. If we replace \(z_{t_{j}}\sim p_{\theta}(z_{t_{j}}\,|\,x_{0})\) with \(z_{t_{j}}\sim q(z_{t_{j}}\,|\,x_{0})\), then \(\widetilde{\text{KLUB}}_{s,t}\) becomes the same as \(\text{KLUB}_{s,t}\) given by (11), and \(\text{KLUB}_{t}\) given by (15) can be considered to remedy the impact of approximating \(p_{\theta}(z_{t_{j}})\) with \(q(z_{t_{j}})\). Therefore, we can consider the combination of \(\text{KLUB}_{s,t}\) given by (11) and \(\text{KLUB}_{t}\) given by (15) as a computationally viable solution to compute \(\widetilde{\text{KLUB}}_{s,t}\), which hence justifies the use of the loss in (18) to optimize the discretized reverse beta diffusion chain. We summarize the training and sampling algorithms of beta diffusion in Algorithms 1 and 2, respectively.

### Argument-swapped KLUBs and Negative ELBOs

We note that in theory, instead of the KL divergences in (11) and (15), \(f_{\theta}\) can also be optimized using two argument-swapped KL divergences: \(\text{KL}(q(z_{t},z_{s})||p_{\theta}(z_{s}|z_{t})q(z_{t}))\) and \(\text{KL}(q(z_{t}^{\prime})||p_{\theta}(z_{t}^{\prime}))\). By the same analysis, these KL divergences can also be bounded by KLUBs and log-beta divergences that are equivalent to the previous ones, but with swapped arguments. The argument-swapped KLUBs and log-beta divergences will be shown to be closely related to optimizing a discretized beta reversediffusion chain via \(-\)ELBO, but they do not guarantee an optimal solution that satisfies (14) in beta diffusion and are found to provide clearly inferior empirical performance.

Negative ELBO for Discretized Beta Diffusion.As an alternative to KLUB, one can also consider following the convention in diffusion modeling to minimize the negative ELBO, expressed as

\[-\mathbb{E}_{x_{0}\sim p_{data}(x_{0})}\ln p_{\theta}(x_{0})\leq- \text{ELBO}=\mathbb{E}_{x_{0}\sim p_{data}(x_{0})}\mathbb{E}_{q(z_{t_{1:T}}\,| \,x_{0})}\left[-\ln\frac{p(x_{0}\,|\,z_{t_{1:T}})p_{\theta}(z_{t_{1:T}})}{q(z_ {t_{1:T}}\,|\,x_{0})}\right]\\ =-\mathbb{E}_{x_{0}}\mathbb{E}_{q(z_{t_{1}}\,|\,x_{0})}\ln p(x_{0} \,|\,z_{t_{1}})+\mathbb{E}_{x_{0}}\text{KL}[q(z_{t_{T}}\,|\,x_{0})||p_{prior}(z _{t_{T}})]+\sum_{j=2}^{T}\mathbb{E}_{x_{0}}\mathbb{E}_{q(z_{t_{j}}\,|\,x_{0})} [L(t_{j-1},z_{t_{j}},x_{0})],\] (23)

where the first two terms are often ignored and the focus is placed on the remaining \(T-2\) terms as

\[L(t_{j-1},z_{t_{j}},x_{0})=\text{KL}(q(z_{t_{j-1}}\,|\,z_{t_{j}}, x_{0})||q(z_{t_{j-1}}\,|\,z_{t_{j}},\hat{x}_{0}=f_{\theta}(z_{t_{j}},t_{j})))\\ =D_{\ln B(\alpha_{i})}\Big{\{}[\eta(\alpha_{t_{j-1}}-\alpha_{t_{j} })f_{\theta}(z_{t_{j}},t_{j}),\eta(1-\alpha_{t_{j-1}}f_{\theta}(z_{t_{j}},t_{j }))],\ \ [\eta(\alpha_{t_{j-1}}-\alpha_{t_{j}})x_{0},\eta(1-\alpha_{t_{j-1}} x_{0})]\Big{\}}.\] (24)

**Lemma 7** (\(-\)ELBO and argument-swapped KLUB).: _Optimizing the generator \(f_{\theta}\) with \(-\)ELBO is equivalent to using an upper-bound for the augment-swapped KL divergence \(\text{KL}(q(z_{t_{1:T}})||p_{\theta}(z_{t_{1:T}}))\)._

The proof in Appendix B relies on the convex nature of both the KL divergence and the negative logarithmic function. We find for beta diffusion, optimizing \(\text{KL}(p_{\theta}(z_{t_{1:T}}))\|q(z_{t_{1:T}}))\) via the proposed KLUBs is clearly preferred to optimizing \(\text{KL}(q(z_{t_{1:T}})||p_{\theta}(z_{t_{1:T}}))\) via \(-\)ELBOs (\(i.e.,\) augment-swapped KLUBs) and leads to stable and satisfactory performance.

KLUBs and (Weighted) Negative ELBOs for Gaussian Diffusion.We note KLUBs are directly applicable to Gaussian diffusion, but they may not result in new optimization algorithms for Gaussian diffusion that drastically differ from the weighted ELBO, which weighs the KL terms using the corresponding SNRs. Moreover, whether the default or argument-swapped KLUBs are used typically does not make any difference in Gaussian diffusion and would result in the same squared error-based Bregman divergence. We provide the derivation of the (weighted) ELBOs from the lens of KLUBs in Appendix C, providing theoretical support for Gaussian diffusion to use the SNR weighted ELBO [23; 35; 20], which was often considered as a heuristic but crucial modification of ELBO.

## 3 Related Work, Limitations, and Future Directions

Various diffusion processes, including Gaussian, categorical, Poisson, and beta diffusions, employ specific distributions in both forward and reverse sampling. Gaussian diffusion starts at \(\mathcal{N}(0,1)\) in its reverse process, while both Poisson and beta diffusion start at \(0\). Beta diffusion's reverse sampling is a monotonically non-decreasing process, similar to Poisson diffusion, but while Poisson diffusion takes count-valued discrete jumps, beta diffusion takes probability-valued continuous jumps. A future direction involves extending beta diffusion to encompass the exponential family [6; 65; 45; 7].

Several recent works have explored alternative diffusion processes closely related to Gaussian diffusion. Cold diffusion by Bansal et al. [5] builds models around arbitrary image transformations instead of Gaussian corruption, but it still relies on \(L_{1}\) loss, resembling Gaussian diffusion's squared Euclidean distance. Rissanen et al. [52] propose an inverse heat dispersion-based diffusion process that reverses the heat equation using inductive biases in Gaussian diffusion-like models. Soft diffusion by Daras et al. [12] uses linear corruption processes like Gaussian blur and masking. Blurring diffusion by Hoogeboom and Salimans [25] shows that blurring (or heat dissipation) can be equivalently defined using a Gaussian diffusion process with non-isotropic noise and proposes to incorporate blurring into Gaussian diffusion. These alternative diffusion processes share similarities with Gaussian diffusion in loss definition and the use of Gaussian-based reverse diffusion for generation. By contrast, beta diffusion is distinct from all of them in forward diffusion, training loss, and reverse diffusion.

A concurrent work by Avdeyev et al. [3] utilizes the Jacobi diffusion process for discrete data diffusion models. Unlike Gaussian diffusion's SDE definition, the Jacobi diffusion process in Avdeyev et al. [3] is defined by the SDE \(dx=\frac{s}{2}[a(1-x)-bx]dt+\sqrt{sx(1-x)}dw\), with \(x\in[0,1]\) and \(s,a,b>0\). The stationary distribution is a univariate beta distribution \(\text{Beta}(a,b)\). Beta diffusion and the Jacobi process are related to the beta distribution, but they differ in several aspects: Beta diffusion ends its forward process at \(\text{Beta}(0,\eta)\), a unit point mass at \(0\), not a \(\text{Beta}(a,b)\) random variable. The marginal distribution of beta diffusion at time \(t\) is expressed as \(q(z_{t}\,|\,x_{0})\sim\text{Beta}(\eta\alpha_{t}x_{0},\eta(1-\alpha_{t}x_{0}))\), while the Jacobi diffusion process involves an infinite sum. Potential connections between beta diffusion and the Jacobi process under specific parameterizations are worth further investigation.

Several recent studies have been actively exploring the adaptation of diffusion models to constrained scenarios [13; 40; 41; 17], where data is bounded within specific ranges or constrained to particular manifolds. These approaches are all rooted in the framework of Gaussian diffusion, which involves the incorporation of additive Gaussian noise. In sharp contrast, beta diffusion introduces a distinct perspective by incorporating multiplicative noise, leading to the emergence of an inherently hypercubic-constrained diffusion model that offers new development and exploration opportunities.

Classifier-free guidance (CFG), often used in conjunction with heuristic clipping, is a widely used technique to perform conditional generation with Gaussian diffusion [22; 41]. Beta diffusion offers the potential for a seamless integration of CFG by directly applying it to the logit space, the operating space of its \(f_{\theta}\) network, thereby eliminating the necessity for heuristic clipping.

To adapt Gaussian diffusion for high-resolution images, a common approach is to perform diffusion within the latent space of an auto-encoder [53; 62]. A promising avenue to explore is the incorporation of sigmoid or tanh activation functions in the encoder's final layer. This modification would establish a bounded latent space conducive to applying beta diffusion, ultimately leading to the development of latent beta diffusion tailored for high-resolution image generation.

One limitation of beta diffusion is that its training is computationally expensive and data-intensive, akin to Gaussian diffusion. Specifically, with four Nvidia RTX A5000 GPUs, beta diffusion and Gaussian diffusion (VP-EDM) both take approximately 1.46 seconds to process 1000 images of size \(32\times 32\times 3\). Processing 200 million CIFAR-10 images, the default number required to reproduce the results of VP-EDM, would thus take over 80 hours. Several recent works have explored different techniques to make Gaussian diffusion faster and/or more data efficient in training [68; 20; 76; 71]. It is worth exploring how to adapt these methods to enhance the training efficiency of beta diffusion.

Beta diffusion has comparable sampling costs to Gaussian diffusion with the same NFE. However, various methods have been developed to accelerate the generation of Gaussian diffusion, including combining it with VAEs, GANs, or conditional transport [77] for faster generation [72; 47; 78; 69], distilling the reverse diffusion chains [43; 55; 76], utilizing reinforcement learning [16], and transforming the SDE associated with Gaussian diffusion into an ODE, followed by fast ODE solvers [58; 39; 42; 75; 34]. Given these existing acceleration techniques for Gaussian diffusion, it is worth exploring their generalization to enhance the sampling efficiency of beta diffusion.

Beta diffusion raises concerns regarding potential negative societal impact when trained on image datasets curated with ill intentions. This issue is not exclusive to beta diffusion but applies to diffusion-based generative models as a whole. It is crucial to address how we can leverage these models for the betterment of society while mitigating any potential negative consequences.

## 4 Experiments

The training and sampling algorithms for beta diffusion are described in detail in Algorithms 1 and 2, respectively, in the Appendix. Our experiments, conducted on two synthetic data and the CIFAR10 images, primarily aim to showcase beta diffusion's effectiveness in generating range-bounded data. We also underscore the superiority of KLUBs over negative ELBOs as effective optimization objectives for optimizing beta diffusion. Additionally, we highlight the differences between beta and Gaussian diffusion, specifically in whether the data are generated through additive or multiplicative transforms and their ability to model the mixture of range-bounded distributions with disjoint supports.

We compare the performance of "Gauss ELBO," "Beta ELBO," and "Beta KLUB," which respectively correspond to a Gaussian diffusion model optimized with the SNR weighted negative ELBO [23; 35], a beta diffusion model optimized with the proposed KLUB loss defined in (18) but with the two arguments inside each KL term swapped, and a beta diffusion model optimized with the proposed KLUB loss defined in (18). On CIFAR-10, we also evaluate beta diffusion alongside a range of non-Gaussian or Gaussian-like diffusion models for comparison.

### Synthetic Data

We consider a discrete distribution that consists of an equal mixture of five unit point masses located at \(x_{0}\in\mathcal{D}=\{1/7,2/7,3/7,4/7,5/7\}\). We would like to highlight that a unit point mass can also be seen as an extreme case of range-bounded data, where the range is zero. Despite being simple,this data could be challenging to model by a continuous distribution, as it would require the generator to concentrate its continuous-valued generations on these five discrete points.

We follow previous works to choose the beta linear diffusion schedule as \(\alpha_{t}=e^{-\frac{1}{2}\beta_{d}t^{2}-\beta_{\min}t}\), where \(\beta_{d}=19.9\) and \(\beta_{\min}=0.1\). This schedule, widely used by Gaussian diffusion [23; 61; 34], is applied consistently across all experiments conducted on synthetic data. We set \(\eta=10000\), \(\pi=0.95\), and \(\omega=0.5\). As the data already falls within the range of 0 to 1, necessitating neither scaling nor shifting, we set \(S_{cale}=1\) and \(S_{lift}=0\). We use the same structured generator \(f_{\theta}\) for both Gaussian and beta diffusion. We choose 20-dimensional sinusoidal position embeddings [63], with the positions set as \(1000t\). The network is an MLP structured as (21-256)-ReLU-(256-256)-ReLU-(256-1). We utilize the Adam optimizer with a learning rate of \(5\)e-\(4\) and a mini-batch size of 1000.

For data generation, we set \(\text{NFE}=200\). We provide the generation results in Figure 3, which shows the true probability mass function (PMF) of the discrete distribution and the empirical PMFs over 100 equal-sized bins between 0 and 1. Each empirical PMF is computed based on 100k random data points generated by the model trained after 400k iterations. It is clear from Figure 3 that "Gauss ELBO" is the worst in terms of mis-aligning the data supports and placing its data into zero-density regions; "Beta ELBO" is the worst in terms of systematically overestimating the density at smaller-valued supports; whereas "Beta KLUB" reaches the best compromise between accurately identifying the data supports and maintaining correct density ratios between different supports.

In Appendix E, we further provide quantitative performance comparisons between different diffusion models and conduct an ablation study between KLUB and its two variants for beta diffusion: "KLUB Conditional" and "KLUB Marginal," corresponding to \(\omega=1\) and \(\omega=0\), respectively, in the loss given by (18). Additionally, we evaluate beta diffusion on another synthetic data, which comes from a mixture of range-bounded continuous distributions and point masses supported on disjoint regions.

### Experiments on Image Generation

We employ the CIFAR-10 dataset and build upon VP-EDM [34] as the foundation of our codebase. Our initial foray into applying beta diffusion to generative modeling of natural images closely mirrors the settings of Gaussian diffusion, including the choice of the generator's network architecture. We introduce a sigmoid diffusion schedule defined as \(\alpha_{t}=1/(1+e^{-c_{0}-(c_{1}-c_{0})t})\), which has been observed to offer greater flexibility than the beta linear schedule for image generation. This schedule bears resemblance to the sigmoid-based one introduced for Gaussian diffusion [35; 29]. We configure the parameters for beta diffusion as follows: \(c_{0}=10\), \(c_{1}=-13\), \(S_{lift}=0.6\), \(S_{cale}=0.39\), \(\eta=10000\), \(\omega=0.99\), and \(\pi=0.95\). We utilize the Adam optimizer with a learning rate of \(2\)e-\(4\). We use EDM's data augmentation approach, but restrict augmented images to a 0-1 range before scaling and shifting. We use the beta diffusion model trained on 200M images to calculate the FID [21]. Explanations regarding the intuition behind these parameter selections can be located in Appendix G.

Below we provide numerical comparison of beta diffusion with not only Gaussian diffusion models but also alternative non-Gaussian or Gaussian-like ones. We also conduct an ablation study to compare KLUB and negative ELBO across different NFE, \(\eta\), and \(B\). A broad spectrum of diffusion models is encompassed in Table 1, which shows that beta diffusion outperforms all non-Gaussian diffusion models on CIFAR10, including Cold Diffusion [5] and Inverse Heat Dispersion [52], as well as categorical and count-based diffusion models [2; 8; 10]. In comparison to Gaussian diffusion and Gaussian+blurring diffusion, beta diffusion surpasses VDM [35], Soft Diffusion [12], and Blurring Diffusion [25]. While it may fall slightly short of DDPM [23], improved DDPM [46], TPDM+ [78], VP-EDM [34], it remains a competitive alternative that uses non-Gaussian based diffusion.

Figure 3: Comparison of the true PMF, marked in red square, and the empirical PMFs of three different methods—Gauss ELBO, Beta ELBO, and Beta KLUB— calculated over 100 equal-sized bins between 0 and 1. Each empirical PMF is marked in solid dot.

Table 2 presents a comparison between KLUB and negative ELBO-optimized beta diffusion across different NFE under two different mini-batch sizes \(B\). Table 3 in the Appendix includes the results under several different combinations of \(\eta\) and \(B\). We also include Figure 4 to visually compare generated images under KLUB and negative ELBO. The findings presented in Tables 2 and 3 and Figure 4 provide further validation of KLUB's efficacy in optimizing beta diffusion.

As each training run takes a long time and FID evaluation is also time-consuming, we have not yet optimized the combination of these hyperparameters given the limit of our current computation resources. Thus the results reported in this paper, while demonstrating that beta diffusion can provide competitive image generation performance, do not yet reflect the full potential of beta diffusion. These results are likely to be further improved given an optimized hyperparameter setting or a network architecture that is tailored to beta diffusion. We leave these further investigations to our future work.

## 5 Conclusion

We introduce beta diffusion characterized by the following properties: 1) **Analytic Marginal:** Given a probability-valued data observation \(x_{0}\in(0,1)\), the distribution at any time point \(t\in[0,1]\) of the forward beta diffusion chain, expressed as \(q(z_{t}\,|\,x_{0})\), is a beta distribution. 2) **Analytical Conditional:** Conditioning on a data \(x_{0}\) and a forward-sampled latent variable \(z_{t}\sim q(z_{t}\,|\,x_{0})\), the forward beta diffusion chain can be reversed from time \(t\) to the latent variable at any previous time \(s\in[0,t)\) by sampling from an analytic conditional posterior \(z_{s}\sim q(z_{s}\,|\,z_{t},x_{0})\) that follows a scaled and shifted beta distribution. 3) **KLUBs:** We introduce the combination of two different Kullback-Leibler Upper Bounds (KLUBs) for optimization and represent them under the log-beta Bregman divergence, showing that their optimal solutions of the generator are both achieved at \(f_{\theta^{*}}(z_{t},t)=\mathbb{E}[x_{0}\,|\,z_{t}]\). We also establish the connection between augment-swapped KLUBs and (weighted) negative ELBOs for diffusion models. Our experimental results confirm the distinctive qualities of beta diffusion when applied to generative modeling of range-bounded data spanning disjoint regions or residing in high-dimensional spaces, as well as the effectiveness of KLUBs for optimizing beta diffusion.

\begin{table}
\begin{tabular}{c l c} \hline \hline Diffusion Space & Model & FID (\(\downarrow\)) \\ \hline \multirow{3}{*}{Gaussian} & DDPM [23] & 3.17 \\  & VDM [35] & 4.00 \\  & Improved DDPM [46] & 2.90 \\  & TDPM+ [78] & 2.83 \\  & VP-EDM [34] & **1.97** \\ \hline \multirow{2}{*}{Gaussian+Blurring} & Soft Diffusion [12] & 3.86 \\  & Blurring Diffusion [25] & 3.17 \\ \hline \multirow{3}{*}{Deterministic} & Cold Diffusion (image reconstruction) [5] & 80.08 (deblurring) \\  & Inverse Heat Dispersion [52] & 8.92 (inpainting) \\ \hline \multirow{2}{*}{Categorical} & D3PM Gauss+Logistic [2] & 7.34 \\  & \(\tau\)LDR-10 [8] & 3.74 \\ \hline Count & JUMP (Poisson Diffusion) [10] & 4.80 \\ \hline Range-bounded & Beta Diffusion & 3.06 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of the FID scores of various diffusion models trained on CIFAR-10.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Loss & \begin{tabular}{c} -ELBO \\ \(B\) \\ \end{tabular} & \begin{tabular}{c} -ELBO \\ \(512\) \\ \end{tabular} & \begin{tabular}{c} KLUB \\ \(288\) \\ \end{tabular} & \begin{tabular}{c} KLUB \\ \(512\) \\ \end{tabular} & 
\begin{tabular}{c} KLUB \\ \(288\) \\ \end{tabular} \\ \hline
20 & 16.04 & 16.10 & 17.06 & 16.09 \\
50 & 6.82 & 6.82 & 6.48 & 5.96 \\
200 & 4.55 & 4.84 & 3.69 & 3.31 \\
500 & 4.39 & 4.65 & 3.45 & 3.10 \\
1000 & 4.41 & 4.61 & 3.38 & 3.08 \\
2000 & 4.50 & 4.66 & 3.37 & **3.06** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparing FID scores for KLUB and negative ELBO-optimized Beta Diffusion on CIFAR-10 with varying NFE under \(\eta=10000\) and two different mini-batch sizes \(B\).

Figure 4: Uncurated randomly-generated images by beta diffusion optimized with \(-\)ELBO or KLUB with \(\eta=10000\) and \(B=288\). The generation with \(\text{NFE}=1000\) starts from the same random seed.

## Acknowledgements

The authors acknowledge the support of NSF-IIS 2212418, NIH-R37 CA271186, the Fall 2022 McCombs REG award, the Texas Advanced Computing Center (TACC), and the NSF AI Institute for Foundations of Machine Learning (IFML).

## References

* Armandpour et al. [2023] Mohammadreza Armandpour, Huangjie Zheng, Ali Sadeghian, Amir Sadeghian, and Mingyuan Zhou. Re-imagine the negative prompt algorithm: Transform 2D diffusion into 3D, alleviate Janus problem and beyond. _arXiv preprint arXiv:2304.04968_, 2023. URL https://perp-neg.github.io/. (the first three authors contributed equally).
* Austin et al. [2021] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. _Advances in Neural Information Processing Systems_, 34:17981-17993, 2021.
* Avdeyev et al. [2023] Pavel Avdeyev, Chenlai Shi, Yuhao Tan, Kseniia Dudnyk, and Jian Zhou. Dirichlet diffusion score model for biological sequence generation. _arXiv preprint arXiv:2305.10699_, 2023.
* Banerjee et al. [2005] Arindam Banerjee, Srujana Merugu, Inderjit S Dhillon, Joydeep Ghosh, and John Lafferty. Clustering with Bregman divergences. _Journal of machine learning research_, 6(10), 2005.
* Bansal et al. [2022] Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Cold diffusion: Inverting arbitrary image transforms without noise. _arXiv preprint arXiv:2208.09392_, 2022.
* Barndorff-Nielsen [2014] O. Barndorff-Nielsen. _Information and Exponential Families : In Statistical Theory_. Wiley Series in Probability and Statistics Ser. John Wiley & Sons, Incorporated, New York, 2nd ed. edition, 2014. ISBN 9781118857373.
* Blei et al. [2017] David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. _Journal of the American statistical Association_, 112(518):859-877, 2017.
* Campbell et al. [2022] Andrew Campbell, Joe Benton, Valentin De Bortoli, Tom Rainforth, George Deligiannidis, and Arnaud Doucet. A continuous time framework for discrete denoising models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=DmT862YAieY.
* Chen et al. [2021] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. WaveGrad: Estimating gradients for waveform generation. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=NsMLjcFa080.
* Chen and Zhou [2023] Tianqi Chen and Mingyuan Zhou. Learning to Jump: Thinning and thickening latent counts for generative modeling. In _ICML 2023: International Conference on Machine Learning_, July 2023. URL http://arxiv.org/abs/2305.18375.
* Cover and Thomas [1991] Thomas M Cover and Joy A Thomas. Elements of information theory. 1991.
* Daras et al. [2023] Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alex Dimakis, and Peyman Milanfar. Soft diffusion: Score matching with general corruptions. _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=W98rebBxlQ.
* De Bortoli et al. [2022] Valentin De Bortoli, Emile Mathieu, Michael Hutchinson, James Thornton, Yee Whye Teh, and Arnaud Doucet. Riemannian score-based generative modelling. _Advances in Neural Information Processing Systems_, 35:2406-2422, 2022.
* Dhariwal and Nichol [2021] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021. URL https://openreview.net/forum?id=AAWuCvzaVt.

* Dragomir et al. [2000] Sever Silvestru Dragomir, Ravi P. Agarwal, and N. S. Barnett. Inequalities for beta and gamma functions via some classical and new integral inequalities. _Journal of Inequalities and Applications_, 2000:504054, 2000.
* Fan and Lee [2023] Ying Fan and Kangwook Lee. Optimizing DDPM sampling with shortcut fine-tuning. _arXiv preprint arXiv:2301.13362_, 2023.
* Fishman et al. [2023] Nic Fishman, Leo Klarner, Valentin De Bortoli, Emile Mathieu, and Michael Hutchinson. Diffusion models for constrained domains. _arXiv preprint arXiv:2304.05364_, 2023.
* Gu et al. [2022] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10696-10706, 2022.
* Han et al. [2022] Xizewen Han, Huangjie Zheng, and Mingyuan Zhou. CARD: Classification and regression diffusion models. In _Advances in Neural Information Processing Systems_, 2022.
* Hang et al. [2023] Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong Chen, Han Hu, Xin Geng, and Baining Guo. Efficient diffusion training via Min-SNR weighting strategy. _arXiv preprint arXiv:2303.09556_, 2023.
* Heusel et al. [2017] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In _Advances in Neural Information Processing Systems_, pages 6626-6637, 2017.
* Ho and Salimans [2022] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.
* Ho et al. [2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In _Advances in Neural Information Processing Systems_, 2020.
* Ho et al. [2022] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. _J. Mach. Learn. Res._, 23:47-1, 2022.
* Hoogeboom and Salimans [2023] Emiel Hoogeboom and Tim Salimans. Blurring diffusion models. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=OjDkC57x5sz.
* Hoogeboom et al. [2021] Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forre, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. _Advances in Neural Information Processing Systems_, 34:12454-12465, 2021.
* Hu et al. [2022] Minghui Hu, Yujie Wang, Tat-Jen Cham, Jianfei Yang, and Ponnuthurai N Suganthan. Global context with discrete diffusion in vector quantised modelling for image generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11502-11511, 2022.
* Hyvarinen [2005] Aapo Hyvarinen. Estimation of non-normalized statistical models by score matching. _Journal of Machine Learning Research_, 6(24):695-709, 2005. URL http://jmlr.org/papers/v6/hyvarinen05a.html.
* Jabri et al. [2022] Allan Jabri, David Fleet, and Ting Chen. Scalable adaptive computation for iterative generation. _arXiv preprint arXiv:2212.11972_, 2022.
* Jambunathan [1954] M. V. Jambunathan. Some properties of beta and gamma distributions. _The Annals of mathematical statistics_, 25(2):401-405, 1954. ISSN 0003-4851.
* Janner et al. [2022] Michael Janner, Yilun Du, Joshua B Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. _arXiv preprint arXiv:2205.09991_, 2022.

* Jing et al. [2022] Bowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi S. Jaakkola. Torsional diffusion for molecular conformer generation. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=w6fj2r62r_H.
* Johnson et al. [2005] N. L. Johnson, A. W. Kemp, and S. Kotz. _Univariate Discrete Distributions_. John Wiley & Sons, 2005.
* Karras et al. [2022] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=k7FuT0WMDC7.
* Kingma et al. [2021] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. _Advances in neural information processing systems_, 34:21696-21707, 2021.
* Kingma and Welling [2013] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* Kong et al. [2021] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. DiffWave: A versatile diffusion model for audio synthesis. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=a-xFK8Ymz5J.
* Kottarski [1962] I. Kottarski. On groups of n independent random variables whose product follows the beta distribution. _Colloquium Mathematicum_, 9(2):325-332, 0 1962.
* Liu et al. [2022] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=PlKWVdJzByKY.
* Liu et al. [2023] Xingchao Liu, Lemeng Wu, Mao Ye, and Qiang Liu. Learning diffusion bridges on constrained domains. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=WHlyCaOTbB.
* Lou and Ermon [2023] Aaron Lou and Stefano Ermon. Reflected diffusion models. _arXiv preprint arXiv:2304.04740_, 2023.
* Lu et al. [2022] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPM-solver: A fast ODE solver for diffusion probabilistic model sampling in around 10 steps. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=2uAaGwlP_V.
* Luhman and Luhman [2021] Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. _arXiv preprint arXiv:2101.02388_, 2021.
* Luo et al. [2022] Shitong Luo, Yufeng Su, Xingang Peng, Sheng Wang, Jian Peng, and Jianzhu Ma. Antigenspecific antibody design and optimization with diffusion-based generative models for protein structures. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=jSorGn2Tjg.
* Murphy [2012] Kevin P Murphy. _Machine Learning: A Probabilistic Perspective_. MIT Press, 2012.
* Nichol and Dhariwal [2021] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In _International Conference on Machine Learning_, pages 8162-8171. PMLR, 2021.
* Pandey et al. [2022] Kushagra Pandey, Avideep Mukherjee, Piyush Rai, and Abhishek Kumar. Diffusevae: Efficient, controllable and high-fidelity generation from low-dimensional latents. _arXiv preprint arXiv:2201.00308_, 2022.
* Pearce et al. [2023] Tim Pearce, Tabish Rashid, Anssi Kanervisto, Dave Bignell, Mingfei Sun, Raluca Georgescu, Sergio Valcarcel Macua, Shan Zheng Tan, Ida Momennejad, Katja Hofmann, et al. Imitating human behaviour with diffusion models. _arXiv preprint arXiv:2301.10677_, 2023.

* Poole et al. [2023] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D diffusion. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=FjNys5c7VyY.
* Ramesh et al. [2022] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.
* Rezende et al. [2014] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In _International conference on machine learning_, pages 1278-1286. PMLR, 2014.
* Rissanen et al. [2023] Severi Rissanen, Markus Heinonen, and Arno Solin. Generative modelling with inverse heat dissipation. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=4PJUBT9f201.
* Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.
* Saharia et al. [2022] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.
* Salimans and Ho [2022] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=TIdAIIpzhoI.
* Shi et al. [2021] Chence Shi, Shitong Luo, Minkai Xu, and Jian Tang. Learning gradient fields for molecular conformation generation. In _International Conference on Machine Learning_, pages 9558-9568. PMLR, 2021.
* Sohl-Dickstein et al. [2015] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Machine Learning_, pages 2256-2265. PMLR, 2015.
* Song et al. [2020] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _International Conference on Learning Representations_, 2020.
* Song and Ermon [2019] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In _Advances in Neural Information Processing Systems_, pages 11918-11930, 2019.
* Sorn and Ermon [2020] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. _Advances in neural information processing systems_, 33:12438-12448, 2020.
* Song et al. [2021] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=PxTIG12RRHS.
* Vahdat et al. [2021] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. In _Advances in neural information processing systems_, 2021.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in neural information processing systems_, pages 5998-6008, 2017.
* Vincent [2011] Pascal Vincent. A connection between score matching and denoising autoencoders. _Neural computation_, 23(7):1661-1674, 2011.
* Wainwright and Jordan [2008] Martin J Wainwright and Michael Irwin Jordan. _Graphical models, exponential families, and variational inference_. Now Publishers Inc, 2008.

* Wang et al. [2023] Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class for offline reinforcement learning. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=AHvFDPi-FA.
* Wang et al. [2023] Zhendong Wang, Yifan Jiang, Yadong Lu, Yelong Shen, Pengcheng He, Weizhu Chen, Zhangyang Wang, and Mingyuan Zhou. In-context learning unlocked for diffusion models. _arXiv preprint arXiv:2305.01115_, 2023.
* Wang et al. [2023] Zhendong Wang, Yifan Jiang, Huangjie Zheng, Peihao Wang, Pengcheng He, Zhangyang Wang, Weizhu Chen, and Mingyuan Zhou. Patch diffusion: Faster and more data-efficient training of diffusion models. _arXiv preprint arXiv:2304.12526_, 2023. URL https://arxiv.org/abs/2304.12526.
* Wang et al. [2023] Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Diffusion-GAN: Training GANs with diffusion. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=HZf7UppWHuA.
* Wang et al. [2023] Zhixin Wang, Xiaoyun Zhang, Zijing Zhang, Huangjie Zheng, Mingyuan Zhou, Ya Zhang, and Yanfeng Wang. DR2: Diffusion-based robust degradation remover for blind face restoration. In _CVPR 2023: Conference on Computer Vision and Pattern Recognition_, 2023. URL https://openreview.net/forum?id=HmcDUPEGAO.
* Wu et al. [2023] Zike Wu, Pan Zhou, Kenji Kawaguchi, and Hanwang Zhang. Fast diffusion model. _arXiv preprint arXiv:2306.06991_, 2023.
* Xiao et al. [2022] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion GANs. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=JprMOp-q0Co.
* Yang et al. [2023] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu. Diffsound: Discrete diffusion model for text-to-sound generation. _IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 31:1720-1733, 2023. doi: 10.1109/TASLP.2023.3268730.
* Yin and Zhou [2018] Mingzhang Yin and Mingyuan Zhou. Semi-implicit variational inference. In _International Conference on Machine Learning_, pages 5660-5669, 2018.
* Zhang and Chen [2023] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=Loek7hfb46P.
* Zheng et al. [2023] Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of diffusion models via operator learning. In _International Conference on Machine Learning_, pages 42390-42402. PMLR, 2023.
* Zheng and Zhou [2021] Huangjie Zheng and Mingyuan Zhou. Exploiting chain rule and Bayes' theorem to compare probability distributions. _Advances in Neural Information Processing Systems_, 34:14993-15006, 2021.
* Zheng et al. [2023] Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Truncated diffusion probabilistic models and diffusion-based adversarial auto-encoders. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=HDxgAKb9561.

[MISSING_PAGE_FAIL:16]

```
0: Number of function evaluations (NFE) \(J=200\), generator \(f_{\theta}\), and timesteps \(\{t_{j}\}_{j=0}^{J}\): \(t_{j}=1-(1-1\text{e-}5)*(J-j)/(J-1)\) for \(j=1,\dots,J\) and \(t_{0}=0\)
1:if NFE \(>350\)then
2:\(\alpha_{t_{j}}=1/(1+e^{-c_{0}-(c_{1}-c_{0})t_{j}})\)
3:else
4:\(\alpha_{t_{j}}=(1/(1+e^{-c_{1}}))^{t_{j}}\)
5:endif
6: Initialize \(\hat{x}_{0}=\mathbb{E}[x_{0}]*S_{cale}+S_{hift}\)
7:\(z_{t_{j}}\sim\text{Beta}(\eta\alpha_{t_{j}}\hat{x}_{0},\eta(1-\alpha_{t_{j}} \hat{x}_{0}))\)
8:for\(j=J\) to \(1\)do
9:\(\hat{x}_{0}=f_{\theta}(z_{t_{j}},\alpha_{t_{j}})*S_{cale}+S_{hift}\)
10:\(p_{(t_{j-1}\gets t_{j})}\sim\text{Beta}\left(\eta(\alpha_{t_{j-1}}-\alpha _{t_{j}})\hat{x}_{0},\eta(1-\alpha_{t_{j-1}}\hat{x}_{0})\right),\) \(\triangleright\) which is implemented in the logit space as \[\text{logit}(p_{(t_{j-1}\gets t_{j})})=\ln u-\ln v,\] \[u\sim\text{Gamma}(\eta(\alpha_{t_{j-1}}-\alpha_{t_{j}})\hat{x}_ {0},1),\] \[v\sim\text{Gamma}(\eta(1-\alpha_{t_{j-1}}\hat{x}_{0}),1)\]
11:\(z_{t_{j-1}}=z_{t_{j}}+(1-z_{t_{j}})p_{(t_{j-1}\gets t_{j})}\), \(\triangleright\) which is implemented in the logit space as \[\text{logit}(z_{t_{j-1}})=\ln\left(e^{\text{logit}(z_{t_{j}})}+e^{\text{ logit}(p_{(t_{j-1}\gets t_{j})})}+e^{\text{logit}(z_{t_{j}})+\text{logit}(p_{(t_{j-1} \gets t_{j})})}\right)\]
12:endfor
13:return\((\hat{x}_{0}-S_{hift})/S_{cale}\) or \((z_{t_{0}}/\alpha_{t_{0}}-S_{hift})/S_{cale}\) ```

**Algorithm 2** Sampling of Beta Diffusion

## Appendix A Log-beta Divergence and KL Divergence between Beta Distributions

By the definition of Bregman divergence, the log-beta divergence corresponding to the log-beta function \(\ln B(a,b)=\ln\Gamma(a)+\ln\Gamma(b)-\ln\Gamma(a+b)\), which is differentiable, and strictly convex on \((0,\infty)^{2}\) as a function of \(a\) and \(b\), can be expressed as

\[D_{\ln B(a,b)}((\alpha_{q},\beta_{q}),(\alpha_{p},\beta_{p}))\] \[=\ln\frac{B(\alpha_{q},\beta_{q})}{B(\alpha_{p},\beta_{p})}-( \alpha_{q}-\alpha_{p},\beta_{q}-\beta_{p})\begin{pmatrix}\nabla_{\alpha_{p}} \ln B(\alpha_{p},\beta_{p})\\ \nabla_{\beta_{p}}\ln B(\alpha_{p},\beta_{p})\end{pmatrix}\] \[=\ln\frac{B(\alpha_{q},\beta_{q})}{B(\alpha_{p},\beta_{p})}-( \alpha_{q}-\alpha_{p},\beta_{q}-\beta_{p})\begin{pmatrix}\psi(\alpha_{p})-\psi (\alpha_{p}+\beta_{p})\\ \psi(\beta_{p})-\psi(\alpha_{p}+\beta_{p})\end{pmatrix}\] \[=\ln\frac{B(\alpha_{q},\beta_{q})}{B(\alpha_{p},\beta_{p})}-( \alpha_{q}-\alpha_{p})\psi(\alpha_{p})-(\beta_{q}-\beta_{p})\psi(\beta_{p})+( \alpha_{q}-\alpha_{p}+\beta_{q}-\beta_{p})\psi(\alpha_{p}+\beta_{p}),\]

which is equivalent to the analytic expression of

\[\text{KL}(\text{Beta}(\alpha_{p},\beta_{p})||\text{Beta}(\alpha_{q},\beta_{q} )).\]Proof of Lemma 1.: The joint distribution of \(z_{t}\) and \(z_{s}\) in (5) can be expressed as

\[q(z_{t},z_{s}\,|\,x_{0})=q(z_{t}\,|\,z_{s})q(z_{s}\,|\,x_{0})\] \[=\frac{1}{z_{s}}\text{Beta}\left(\frac{z_{t}}{z_{s}};\eta\alpha_{ t}x_{0},\eta(\alpha_{s}-\alpha_{t})x_{0}\right)\text{Beta}(z_{s};\eta\alpha_{s}x_{0}, \eta(1-\alpha_{s}x_{0}))\] \[=\frac{1}{z_{s}}\frac{\Gamma(\eta\alpha_{s}x_{0})}{\Gamma(\eta \alpha_{t}x_{0})\Gamma(\eta(\alpha_{s}-\alpha_{t})x_{0})}\left(\frac{z_{t}}{z_ {s}}\right)^{\eta\alpha_{t}x_{0}-1}\left(1-\frac{z_{t}}{z_{s}}\right)^{\eta( \alpha_{s}-\alpha_{t})x_{0}-1}\] \[\quad\times\frac{\Gamma(\eta)}{\Gamma(\eta\alpha_{s}x_{0})\Gamma (\eta(1-\alpha_{s}x_{0}))}z_{s}^{\eta\alpha_{s}x_{0}-1}(1-z_{s})^{\eta(1- \alpha_{s}x_{0})-1}\] \[=\frac{\Gamma(\eta)}{\Gamma(\eta\alpha_{t}x_{0})\Gamma(\eta( \alpha_{s}-\alpha_{t})x_{0})\Gamma(\eta(1-\alpha_{s}x_{0}))}\] \[\quad\times z_{t}^{\eta\alpha_{t}x_{0}-1}(z_{s}-z_{t})^{\eta( \alpha_{s}-\alpha_{t})x_{0}-1}(1-z_{s})^{\eta(1-\alpha_{s}x_{0})-1}.\] (25)

The joint distribution of \(z_{t}\) and \(z_{s}\) in (6) can be expressed as

\[q(z_{t},z_{s}\,|\,x_{0})=q(z_{s}\,|\,z_{t},x_{0})q(z_{t}\,|\,x_{0})\] \[=\frac{1}{1-z_{t}}\text{Beta}\left(\frac{z_{s}-z_{t}}{1-z_{t}}; \eta(\alpha_{s}-\alpha_{t})x_{0},\eta(1-\alpha_{s}x_{0})\right)\text{Beta}(z_ {t};\eta\alpha_{t}x_{0},\eta(1-\alpha_{t}x_{0}))\] \[=\frac{1}{1-z_{t}}\frac{\Gamma(\eta(1-\alpha_{t}x_{0}))}{\Gamma( \eta(1-\alpha_{s}x_{0}))\Gamma(\eta(\alpha_{s}-\alpha_{t})x_{0})}\] \[\quad\times\left(\frac{z_{s}-z_{t}}{1-z_{t}}\right)^{\eta(\alpha_ {s}-\alpha_{t})x_{0}-1}\left(1-\frac{z_{s}-z_{t}}{1-z_{t}}\right)^{\eta(1- \alpha_{s}x_{0})-1}\] \[\quad\times\frac{\Gamma(\eta)}{\Gamma(\eta\alpha_{t}x_{0})\Gamma (\eta(1-\alpha_{t})x_{0})}z_{t}^{\eta\alpha_{t}x_{0}-1}(1-z_{t})^{\eta(1- \alpha_{t}x_{0})-1}\] \[=\frac{\Gamma(\eta)}{\Gamma(\eta\alpha_{t}x_{0})\Gamma(\eta( \alpha_{s}-\alpha_{t})x_{0})\Gamma(\eta(1-\alpha_{s}x_{0}))}\] \[\quad\times(z_{s}-z_{t})^{\eta(\alpha_{s}-\alpha_{t})x_{0}-1}(1-z _{s})^{\eta(1-\alpha_{s}x_{0})-1}z_{t}^{\eta\alpha_{t}x_{0}-1}.\] (26)

The joint distribution shown in (25) is the same as that in (26). 

Proof of Lemma 2.: Since we can re-express \(q(z_{t})\) and \(q(z_{s},z_{t})\) as

\[q(z_{t})=\mathbb{E}_{x_{0}\sim p_{data}(x_{0})}[q(z_{t}\,|\,x_{0})],\]

\[q(z_{s},z_{t})=\mathbb{E}_{x_{0}\sim p_{data}(x_{0})}[q(z_{s},z_{t}\,|\,x_{0})],\]

and \(q(z_{s},z_{t}\,|\,x_{0})=q(z_{s}\,|\,z_{t},x_{0})q(z_{t}\,|\,x_{0})\), using the convexity of the KL divergence, we have

\[\text{KL}(p_{\theta}(z_{s}\,|\,z_{t})q(z)||q(z_{s},z_{t}))\] \[=\text{KL}(\mathbb{E}_{x_{0}\sim p_{data}(x_{0})}[p_{\theta}(z_{s }\,|\,z_{t})q(z_{t}\,|\,x_{0})]||\mathbb{E}_{x_{0}\sim p_{data}(x_{0})}[q(z_{s },z_{t}\,|\,x_{0})])\] \[\leq\mathbb{E}_{x_{0}\sim p_{data}(x_{0})}[\text{KL}(p_{\theta}(z _{s}\,|\,z_{t})q(z_{t}\,|\,x_{0})||q(z_{s},z_{t}\,|\,x_{0}))]\] \[=\mathbb{E}_{x_{0}\sim p_{data}(x_{0})}\mathbb{E}_{(z_{s},z_{t}) \sim p_{\theta}(z_{s}\,|\,z_{t})q(z_{t}\,|\,x_{0})}\ln\frac{p_{\theta}(z_{s}\,| \,z_{t})q(z_{t}\,|\,x_{0})}{q(z_{s}\,|\,z_{t},x_{0})q(z_{t}\,|\,x_{0})}\] \[=\mathbb{E}_{x_{0}\sim p_{data}(x_{0})}\mathbb{E}_{z_{t}\sim q(z_{ t}\,|\,x_{0})}\mathbb{E}_{z_{s}\sim p_{\theta}(z_{s}\,|\,z_{t})}\ln\frac{p_{\theta}(z_{s }\,|\,z_{t})}{q(z_{s}\,|\,z_{t},x_{0})}\] \[=\mathbb{E}_{(z_{t},x_{0})\sim q(z_{t}\,|\,x_{0})p_{data}(x_{0})}[ \text{KL}(p_{\theta}(z_{s}\,|\,z_{t})||q(z_{s}\,|\,z_{t},x_{0}))]\] \[=\mathbb{E}_{(z_{t},x_{0})\sim q(z_{t}\,|\,x_{0})p_{data}(x_{0})}[ \text{KL}(q(z_{s}\,|\,z_{t},\hat{x}_{0}=f_{\theta}(z_{t},t)||q(z_{s}\,|\,z_{t},x _{0}))].\]Proof of Lemma 4.: Since we can re-express \(p_{\theta}(z_{t}^{\prime})\) and \(q(z_{t}^{\prime})\) as

\[p_{\theta}(z_{t}^{\prime}):=\mathbb{E}_{z_{t}\sim q(z_{t})}[q(z_{t}^{\prime}|\, \hat{x}_{0}=f_{\theta}(z_{t},t))]=\mathbb{E}_{(z_{t},x_{0})\sim q(z_{t}|\,x_{0}) p_{data}(x_{0})}[q(z_{t}^{\prime}|\,\hat{x}_{0}=f_{\theta}(z_{t},t))],\]

\[q(z_{t}^{\prime})=\mathbb{E}_{(z_{t},x_{0})\sim q(z_{t}|\,x_{0})p_{data}(x_{0}) }[q(z_{t}^{\prime}|\,x_{0})],\]

using the convexity of the KL divergence, we have

\[\text{KL}(p_{\theta}(z_{t}^{\prime})||q(z_{t}^{\prime}))\] \[=\text{KL}(\mathbb{E}_{(z_{t},x_{0})\sim q(z_{t}|\,x_{0})p_{data} (x_{0})}[q(z_{t}^{\prime}|\,\hat{x}_{0}=f_{\theta}(z_{t},t))]||\mathbb{E}_{(z _{t},x_{0})\sim q(z_{t}|\,x_{0})p_{data}(x_{0})}[q(z_{t}^{\prime}|\,x_{0})])\] \[\leq\mathbb{E}_{(z_{t},x_{0})\sim q(z_{t}|\,x_{0})p_{data}(x_{0}) }[\text{KL}(q(z_{t}^{\prime}|\,f_{\theta}(z_{t},t))||q(z_{t}^{\prime}|\,x_{0} ))].\]

Proof of Lemma 4.: The proof utilizes the convexity of the KL divergence to show that

\[\text{KL}(p_{\theta}(z_{t_{1:T}})||q(z_{t_{1:T}}))=\text{KL}( \mathbb{E}_{x_{0}\sim p_{data}(x_{0})}[p_{\theta}(z_{1:T}|\,x_{0})]||\mathbb{E} _{x_{0}\sim p_{data}(x_{0})}[q(z_{1:T}|\,z_{0})])\] \[\leq\mathbb{E}_{x_{0}\sim p_{data}(x_{0})}\text{KL}(p_{\theta}(z _{t_{1:T}}|\,x_{0})||q(z_{t_{1:T}}|\,x_{0}))\] \[=\mathbb{E}_{x_{0}\sim p_{data}(x_{0})}\mathbb{E}_{z_{t_{1:T}} \sim p_{\theta}(z_{t_{1:T}}|\,x_{0})}\ln\frac{p_{\theta}(z_{t_{1:T}}|\,x_{0}) }{q(z_{t_{1:T}}|\,x_{0})}\] \[=\mathbb{E}_{x_{0}\sim p_{data}(x_{0})}\bigg{[}\text{KL}(p_{prior }(z_{t_{T}}|\,x_{0})||q(z_{t_{T}}|\,x_{0}))\] \[\qquad\qquad\qquad+\sum_{j=2}^{T}\mathbb{E}_{z_{t_{j}}\sim p_{ \theta}(z_{t_{j}}|\,x_{0})}\mathbb{E}_{z_{t_{j-1}}\sim p_{\theta}(z_{t_{j-1}}| \,z_{t_{j}})}\ln\frac{p_{\theta}(z_{t_{j-1}}|\,z_{t_{j}})}{q(z_{t_{j-1}}|\,z_ {t_{j}},x_{0})}\bigg{]}.\]

We note \(p_{\theta}(z_{t_{j}}|\,x_{0})\propto p(x_{0}|\,z_{t_{j}})p_{\theta}(z_{t_{j}})\), which in general is intractable to sample from, motivating us to replace \(p_{\theta}(z_{t_{j}}|\,x_{0})\) with \(q(z_{t_{j}}|\,x_{0})\) for tractable computation. 

Proof of Lemma 7.: Utilizing the convexity of the KL divergence, we present an upper-bound of the augmented-swapped KL divergence \(\text{KL}(q(z_{t_{1:T}})||p_{\theta}(z_{t_{1:T}}))\), referred to as AS-KLUB, as

\[\text{KL}(q(z_{t_{1:T}})||p_{\theta}(z_{t_{1:T}}))=\text{KL}( \mathbb{E}_{x_{0}\sim p_{data}(x_{0})}[q(z_{t_{1:T}}|\,x_{0})]||\mathbb{E}_{x_ {0}\sim p_{data}(x_{0})}[p_{\theta}(z_{t_{1:T}}|\,x_{0})])\] \[\leq\text{AS-KLUB}=\mathbb{E}_{x_{0}\sim p_{data}(x_{0})}\text{KL} (q(z_{t_{1:T}}|\,x_{0})||p_{\theta}(z_{t_{1:T}}|\,x_{0}))\] \[=\mathbb{E}_{x_{0}\sim p_{data}(x_{0})}\mathbb{E}_{z_{t_{1:T}} \sim q(z_{t_{1:T}}|\,x_{0})}\ln\frac{q(z_{t_{1:T}}|\,x_{0})}{p_{\theta}(z_{t_ {1:T}}|\,x_{0})}\] \[=\mathbb{E}_{x_{0}\sim p_{data}(x_{0})}\bigg{[}\text{KL}(q(z_{t_ {1}}|\,x_{0})||p_{prior}(z_{t_{T}}|\,x_{0}))\] \[\qquad\qquad\qquad+\sum_{j=2}^{T}\mathbb{E}_{z_{t_{j}}\sim q(z_{t _{j}}|\,x_{0})}\mathbb{E}_{z_{t_{j-1}}\sim q(z_{t_{j-1}}|\,z_{t_{j}},x_{0})} \ln\frac{q(z_{t_{j-1}}|\,z_{t_{j}},x_{0})}{p_{\theta}(z_{t_{j-1}}|\,z_{t_{j}}) }\bigg{]}.\]

Utilizing the convexity of the negative logarithmic function, we have

\[-\mathbb{E}_{x_{0}\sim p_{data}(x_{0})}\ln p_{\theta}(x_{0})=- \mathbb{E}_{x_{0}\sim p_{data}(x_{0})}\ln\mathbb{E}_{p_{\theta}(z_{t_{1:T}})}[p(x_ {0}|\,z_{t_{1:T}})]\] \[=-\mathbb{E}_{x_{0}\sim p_{data}(x_{0})}\ln\mathbb{E}_{q(z_{t_{1:T }}|\,x_{0})}\bigg{[}\frac{p(x_{0}|\,z_{t_{1:T}})p_{\theta}(z_{t_{1:T}})}{q(z_{t _{1:T}}|\,x_{0})}\bigg{]}\] \[\leq-\text{ELBO}=-\mathbb{E}_{x_{0}\sim p_{data}(x_{0})}\mathbb{E }_{q(z_{t_{1:T}}|\,x_{0})}\left[\ln\frac{p(x_{0}|\,z_{t_{1:T}})p_{\theta}(z_{t 1:T})}{q(z_{t_{1:T}}|\,x_{0})}\right]\] \[=\mathbb{E}_{x_{0}\sim p_{data}(x_{0})}\bigg{[}-\mathbb{E}_{q(z_{t _{1}}|\,x_{0})}\ln p(x_{0}|\,z_{t_{1}})+\text{KL}[q(z_{t_{T}}|\,x_{0})||p_{ prior}(z_{t_{T}})]\] \[\qquad\qquad\qquad+\sum_{j=2}^{T}\mathbb{E}_{z_{t_{j}}\sim q(z_{t _{j}}|\,x_{0})}\mathbb{E}_{z_{t_{j-1}}\sim q(z_{t_{j-1}}|\,z_{t_{j}},x_{0})}\ln \frac{q(z_{t_{j-1}}|\,z_{t_{j}},x_{0})}{p_{\theta}(z_{t_{j-1}}|\,z_{t_{j}}) }\bigg{]}.\]

Thus, when we disregard its first term, AS-KLUB is equivalent to \(-\text{ELBO}\) without its first two terms. Since these three terms typically do not affect optimization, optimizing the generator \(f_{\theta}\) with AS-KLUB is equivalent to using \(-\text{ELBO}\)Derivation of (Weighted) ELBOs of Gaussian Diffusion from KLUB

Let us denote \(\alpha_{0}=1\) and \(z_{0}=x_{0}\). Following the definition in Ho et al. [23] and Song et al. [58], we define a Gaussian diffusion-based generative model as

\[z_{1} \sim\mathcal{N}(\sqrt{\alpha_{1}}x_{0},1-\alpha_{1}),\] \[z_{2} \sim\mathcal{N}\left(\sqrt{\frac{\alpha_{2}}{\alpha_{1}}}z_{1},1 -\frac{\alpha_{2}}{\alpha_{1}}\right),\] \[\ldots\] \[z_{t} \sim\mathcal{N}\left(\sqrt{\frac{\alpha_{t}}{\alpha_{t-1}}}z_{t- 1},1-\frac{\alpha_{t}}{\alpha_{t-1}}\right),\] \[\ldots\] \[z_{T} \sim\mathcal{N}\left(\sqrt{\frac{\alpha_{T}}{\alpha_{T-1}}}z_{T- 1},1-\frac{\alpha_{T}}{\alpha_{T-1}}\right),\]

where the diffusion scheduling parameters \(\alpha_{t}\) in this paper is related to \(\beta_{t}\) in Ho et al. [23] as

\[\beta_{t}:=1-\frac{\alpha_{t}}{\alpha_{t-1}}.\]

The same as the derivations in Ho et al. [23], we can express the forward marginal distribution at time \(t\) as

\[z_{t}\sim\mathcal{N}(\sqrt{\alpha_{t}}x_{0},1-\alpha_{t}).\]

Assuming \(\mathbb{E}[x_{0}]=0\) and \(\text{var}[x_{0}]=1\), the signal-to-noise ratio at time \(t\) is defined as

\[\text{SNR}_{t}=\mathbb{E}_{x_{0}}\left[\left(\frac{\mathbb{E}[z_{t}\,|\,x_{0} ]}{\text{std}[z_{t}\,|\,x_{0}]}\right)^{2}\right]=\frac{\alpha_{t}}{1-\alpha_{ t}}\mathbb{E}[x_{0}^{2}]=\frac{\alpha_{t}}{1-\alpha_{t}}.\]

Since \(\sqrt{\frac{\alpha_{t-1}}{\alpha_{t}}}x_{t}\sim\mathcal{N}(z_{t-1},\frac{ \alpha_{t-1}}{\alpha_{t}}-1)\) and \(z_{t-1}\sim\mathcal{N}(\sqrt{\alpha_{t-1}}z_{0},1-\alpha_{t-1})\), using the conjugacy of the Gaussian distributions with respect to their mean, we can express the conditional posterior as

\[q(z_{t-1}\,|\,x_{0},z_{t})\] \[=\mathcal{N}\left(\frac{1}{1-\alpha_{t-1}}+\frac{\alpha_{t}}{ \alpha_{t-1}-\alpha_{t}}\right)^{-1}\left(\frac{\sqrt{\alpha_{t-1}}x_{0}}{1- \alpha_{t-1}}+\frac{\sqrt{\alpha_{t-1}\alpha_{t}}x_{t}}{\alpha_{t-1}-\alpha_{ t}}\right),\left(\frac{1}{1-\alpha_{t-1}}+\frac{\alpha_{t}}{\alpha_{t-1}- \alpha_{t}}\right)^{-1}\right)\] \[=\mathcal{N}\left(\frac{\sqrt{\alpha_{t-1}}}{1-\alpha_{t}}(1- \frac{\alpha_{t}}{\alpha_{t-1}})x_{0}+\frac{1-\alpha_{t-1}}{1-\alpha_{t}}\sqrt {\frac{\alpha_{t}}{\alpha_{t-1}}}z_{t},\ \ \frac{1-\alpha_{t-1}}{1-\alpha_{t}}(1-\frac{\alpha_{t}}{\alpha_{t-1}})\right).\]

The forward Gaussian diffusion chain

\[q(z_{1:T}\,|\,x_{0})=\prod_{t=1}^{T}p(z_{t}\,|\,z_{t-1}))\]

can be equivalently sampled in reverse order as

\[q(z_{1:T}\,|\,x_{0})=q(z_{T}\,|\,x_{0})\prod_{t=2}^{T}q(z_{t-1}\,|\,z_{t},x_{0 }).\]

As this reverse chain is non-causal and non-Markovian, we need to approximate it with a Markov chain during inference, which is expressed as

\[p(z_{1:T})=p_{prior}(z_{T})\prod_{t=2}^{T}p_{\theta}(z_{t-1}\,|\,z_{t}).\]

The usual strategy [58, 35, 72] is to utilize the conditional posterior to define

\[p_{\theta}(z_{t-1}\,|\,z_{t})=q(z_{t-1}\,|\,z_{t},\hat{x}_{0}=f_{\theta}(z_{t}, t)).\]

In what follows, we redefine the time \(t\) as a continuous variable between 0 and 1. We let \(t_{1:T}\) be a set of \(T\) discrete time points within that interval, with \(0\leq t_{1}<t_{2}<\ldots<t_{T}\leq 1\). The corrupted data observations over these \(T\) time points are defined as \(z_{t_{1:T}}\).

### Optimization of Gaussian Diffusion via Negative ELBO

Viewing \(p_{\theta}(x_{0}\,|\,z_{1:T})\) as the decoder, \(p_{\theta}(z_{1:T})\) as the prior, and \(q(z_{1:T}\,|\,x_{0})\) as the inference network, we can optimize the generator parameter via the negative ELBO as

\[-\mathbb{E}_{x_{0}}\ln p_{\theta}(x_{0})\leq-\text{ELBO}=\mathbb{E} _{x_{0}}\mathbb{E}_{q(z_{1_{1:T}}\,|\,x_{0})}\left[-\ln\frac{p(x_{0}\,|\,z_{t_{ 1:T}})p(z_{t_{1:T}})}{q(z_{t_{1:T}}\,|\,x_{0})}\right]\] \[=\mathbb{E}_{x_{0}}\mathbb{E}_{q(z_{t_{1}}\,|\,x_{0})}\ln p(x_{0} \,|\,z_{t_{1}})+\mathbb{E}_{x_{0}}\text{KL}[q(z_{t}\,|\,x_{0})||p_{prior}(x_{t _{T}})]+\sum_{j=2}^{T}\mathbb{E}_{x_{0}}\mathbb{E}_{q(z_{t_{j}}\,|\,x_{0})}[L( t_{j-1},z_{t_{j}},x_{0})],\]

where the first two terms are often ignored and the focus is placed on the remaining \(T-2\) terms, defined as

\[L(s,z_{t},x_{0})=\text{KL}(q(z_{s}\,|\,z_{t},x_{0})||q(z_{s}\,|\, z_{t},\hat{x}_{0}=f_{\theta}(z_{t},t)))\] \[=\frac{1}{2\frac{1-\alpha_{s}}{1-\alpha_{t}}(1-\frac{\alpha_{t}} {\alpha_{s}})}\left(\frac{\sqrt{\alpha_{s}}}{1-\alpha_{t}}\left(1-\frac{ \alpha_{t}}{\alpha_{s}}\right)\right)^{2}\|x_{0}-f_{\theta}(z_{t},t)\|_{2}^{2}\] \[=\frac{1}{2}\left(\frac{\alpha_{s}}{1-\alpha_{s}}-\frac{\alpha_{ t}}{1-\alpha_{t}}\right)\|x_{0}-f_{\theta}(z_{t},t)\|_{2}^{2},\] (27)

where \(0<s<t<1\). We choose \(s=\max(t-1/T,0)\) during training.

Since \(z_{t}=\sqrt{\alpha_{t}}x_{0}+\sqrt{1-\alpha_{t}}\epsilon_{t},\ \epsilon_{t}\sim\mathcal{N}(0,1)\), it is true that

\[x_{0}=\frac{z_{t}-\sqrt{1-\alpha_{t}}\epsilon_{t}}{\sqrt{\alpha_{t}}}.\]

Instead of directly predicting \(x_{0}\) given \(x_{t}\), we can equivalently predict \(\hat{\epsilon}_{t}=\epsilon_{\theta}(z_{t},t)\) given \(z_{t}\) and let

\[\hat{x}_{0}=f_{\theta}(z_{t},t)=\frac{z_{t}-\sqrt{1-\alpha_{t}}\epsilon_{ \theta}(z_{t},t)}{\sqrt{\alpha_{t}}}.\]

Thus (27) can also be written as

\[L(s,z_{t},x_{0}) =\frac{1}{2}\left(\frac{\alpha_{s}}{1-\alpha_{s}}-\frac{\alpha_{t }}{1-\alpha_{t}}\right)\|x_{0}-f_{\theta}(z_{t},t)\|_{2}^{2}\] \[=\frac{1}{2}(\text{SNR}_{s}-\text{SNR}_{t})\|x_{0}-f_{\theta}(z_ {t},t)\|_{2}^{2}\] \[=\frac{1}{2}\left(\frac{\alpha_{s}}{1-\alpha_{s}}-\frac{\alpha_{ t}}{1-\alpha_{t}}\right)\frac{1-\alpha_{t}}{\alpha_{t}}\|\epsilon_{t}- \epsilon_{\theta}(z_{t},t)\|_{2}^{2}\] \[=\frac{1}{2}\left(\frac{\text{SNR}_{s}}{\text{SNR}_{t}}-1\right) \|\epsilon_{t}-\epsilon_{\theta}(z_{t},t)\|_{2}^{2},\] (28)

which agrees with Equations 13 and 14 in Kingma et al. [35].

### Optimization of Gaussian Diffusion via KLUB Conditional

Following the definition of KLUB (Conditional), for Gaussian diffusion we have

\[\text{KLUB}(s,z_{t},x_{0}) =\text{KL}(q(z_{s}\,|\,\hat{x}_{0}=f_{\theta}(z_{t},t),z_{t})||q(z_ {s}\,|\,x_{0},z_{t}))\]

For two Gaussian distributions \(q_{1}\) and \(q_{2}\) that have the same variance, we have \(\text{KL}(q_{1}\|q_{2})=\text{KL}(q_{2}||q_{1})\) and hence

\[\text{KLUB}(s,z_{t},x_{0}) =\text{KL}(q(z_{s}\,|\,\hat{x}_{0}=f_{\theta}(z_{t},t),z_{t})||q( z_{s}\,|\,x_{0},z_{t}))\] \[=\text{KL}(q(z_{s}\,|\,x_{0},z_{t})||q(z_{s}\,|\,\hat{x}_{0}=f_{ \theta}(z_{t},t),z_{t}))\] \[=L(s,z_{t},x_{0}).\]

Therefore, over the same set of time points \(t_{1:T}\), optimizing via KLUB conditional is identical to optimizing via \(-\)ELBO. As analyzed before, optimizing via KLUB conditional (\(i.e.,\,-\)ELBO) may not be able to directly counteract the error accumulated over the course of diffusion, and hence could lead to slow convergence.

### Optimization of Gaussian Diffusion via KLUB Marginal and SNR-weighted Negative ELBO

Following the definition of KLUB (Marginal), for Gaussian diffusion we have

\[\text{KLUB}(z_{t},x_{0}) =\text{KL}(\mathcal{N}(\sqrt{\alpha_{t}}x_{0},1-\alpha_{t})|| \mathcal{N}(\sqrt{\alpha_{t}}f(z_{t},t),1-\alpha_{t})\] \[=\frac{\alpha_{t}}{1-\alpha_{t}}\|x_{0}-f(z_{t},t)\|_{2}^{2}\] \[=\text{SNR}_{t}\|x_{0}-f(z_{t},t)\|_{2}^{2}\] \[=\|\epsilon_{0}-\epsilon_{\theta}(z_{t},t)\|_{2}^{2}.\]

It is surprising to find out that the KLUB Marginal is identical to the SNR weighted \(-\)ELBO first introduced in Ho et al. [23] and further discussed in Hang et al. [20].

### Optimization of Gaussian Diffusion via KLUB

Following beta diffusion, we can also combine two KLUBs as

\[\omega\text{KLUB}(s,z_{t},x_{0})+(1-\omega)\text{KLUB}(z_{t},x_{0})\] \[=\left[\frac{\omega}{2}\left(\frac{\alpha_{s}}{1-\alpha_{s}} \frac{1-\alpha_{t}}{\alpha_{t}}-1\right)+(1-\omega)\right]\|\epsilon_{0}- \epsilon_{\theta}(z_{t},t)\|_{2}^{2}.\]

Since \(\frac{\alpha_{s}}{1-\alpha_{s}}\frac{1-\alpha_{t}}{\alpha_{t}}\) is in general close to 1 when \(s\) is not too far from \(t\), when \(\omega\) is not too close to 1, a combination of these two KLUBs does not result in an algorithm that clearly differs from the use of an SNR-weighted negative ELBO.

## Appendix D Illustration of Forward and Reverse Beta Diffusion

Illustration of Forward Beta Diffusion.We first visualize the beta forward diffusion process by displaying a true image \(x_{0}\) and its noise-corrupted versions over the course of the forward diffusion process. Specifically, we display the noise corrupted and masked image at time \(t=0,0.05,0.1\dots,1\) as

\[\tilde{z}_{t}=\max\left(\min\left(\frac{1}{S_{\text{\tiny{calc}}}}\left( \frac{z_{t}}{\alpha_{t}}-S_{\text{\tiny{lift}}}\right),1\right),0\right),\; \;z_{t}\sim\text{Beta}(\eta\alpha_{t}x_{0},\eta(1-\alpha_{t}x_{0})).\]

It is clear from Figure 1 that with multiplicative beta-distributed noises, the image becomes both noisier and sparser as time increases, and eventually becomes almost completely dark. Thus the forward process of beta diffusion can be considered as simultaneously noising and masking the pixels. This clearly differs beta diffusion from Gaussian diffusion, whose forward diffusion gradually applies additive random noise and eventually ends at a Gaussian random noise. In addition, the reverse diffusion process of Gaussian diffusion can be considered a denoising process, whereas that of beta diffusion is simultaneously demasking and denoising the data, as illustrated below.

Illustration of Reverse Beta Diffusion.We further illustrate the reverse process of beta diffusion by displaying

\[\tilde{z}_{t_{j-1}}=\max\left(\min\left(\frac{1}{S_{\text{\tiny{ calc}}}}\left(\frac{z_{t_{j-1}}}{\alpha_{t_{j-1}}}-S_{\text{\tiny{lift}}} \right),1\right),0\right),\]

where \(z_{t_{j-1}}=z_{t_{j}}+(1-z_{t_{j}})p_{(t_{j-1}\gets t_{j})}\) is iteratively computed as in Algorithm 2. We also display

\[\hat{x}_{0}=f_{\theta}(z_{t_{j}},t_{j})\approx\mathbb{E}[x_{0}\,|\,z_{t_{j}}],\]

where the approximation would become more and more accurate when \(\theta\) approaches its theoretical optimal \(\theta^{*}\), under which we have \(f_{\theta^{*}}(z_{t_{j}},t_{j})=\mathbb{E}[x_{0}\,|\,z_{t_{j}}]\).

As shown in Figure 2, starting from a random image drawn from

\[z_{t_{j}}\sim\text{Beta}(\eta\alpha_{t_{j}}\hat{x}_{0},\eta(1-\alpha_{t_{j}} \hat{x}_{0})),\;\hat{x}_{0}=\mathbb{E}[x_{0}],\]

most of whose pixels would be completely dark, beta diffusion gradually demasks and denoises the image towards a clean image through multiplicative transforms, as shown in Algorithm 2.

## Appendix E Additional Results for Synthetic Data

Quantitative Evaluation Metrics.We consider Wasserstein-1, Jensen-Shannon divergence (JSD), and Hellinger distance as three complimentary evaluation metrics. Wasserstein-1 has a high tolerance of misalignment between the supports of the true data density and these of the generated data. In other words, it is not sensitive to both misaligning the modes of the true density and these of the generated density and placing density in the regions where there are no data. By contrast, both JSD and Hellinger can well reflect the misalignments between the supports of high data density regions.

_Wasserstein-1_: We monitor the performance of different algorithms during training by generating 100k data points using the trained model and drawing 100k data points from the data distribution, and computing the _Wasserstein-1_ distance between their empirical distributions, which can be done by sorting them and taking the mean of their element-wise absolute differences. If the true distribution is discrete in that it takes values uniformly at random from a discrete set \(\mathcal{D}\), then we compute the Wasserstein-1 distance by sorting a set of 10k generated samples from small to large and calculating the mean of their absolute element-wise differences with a non-decreasing vector of the same size, which consists of an equal number of copies for each unique value in \(\mathcal{D}\).

_JSD and Hellinger distance:_ We discretize the 100k generated data into 100 equal-sized bins between 0 and 1 and compute the frequency of the data in each bin, which provides an empirical probability mass function (PMF). We then compute both the JSD and Hellinger distance between the empirical PMF of the generated data and the empirical (true) PMF of the true (discrete) data.

Additional Results.The observations in Figure 3 are further confirmed by Figure 5, which shows that while Gaussian diffusion has good performance measured by the Wasserstein-1 distance, it is considerably worse than beta diffusion in aligning the supports between the true and generated data distributions. Within beta diffusion, either KLUB or its argument-swapped version leads to good performance measured by both the JSD and Hellinger distance, suggesting their excellent ability to concentrate their generations around the true data supports. However, the proposed KLUB has a much better recovery of the true underlying density in comparison to its argument-swapped version, as confirmed by examining the first subplot in Figure 5 and comparing the second and third subplots in Figure 3.

Figure 5: Comparison of the statistical distances between the true and generated data distributions over the course of training. The blue, green, and orange curves are for “Gauss ELBO,” “Beta ELBO,” and “Beta KLUB,” respectively. From the left to right are the plots for Wasserstein-1 distance, Jensen–Shannon divergence, and Hellinger distance, respectively.

Figure 6: Analogy to Figure 5 for comparing “KLUB Conditional,” “KLUB Marginal,” and “KLUB.”We further conduct an ablation study between KLUB and its two variants: "KLUB Conditional" and "KLUB Marginal," corresponding to \(\omega=1\) and \(\omega=0\), respectively, in the loss given by (18). The results suggest "KLUB Conditional" and "KLUB Marginal" have comparable performance, which is not that surprising considering that in theory, they both share the same optimal solution given by (14). More closely examining the plots suggests that "KLUB Marginal" converges faster than "KLUB Conditional" in aligning the generation with the data supports, but eventually delivers comparable performance, and combining them leads to the "KLUB" that has a good overall performance.

We note we have conducted additional experiments by varying model parameters and adopting different diffusion schedules. Our observation is that while each algorithm could do well by carefully tuning these parameters, "Beta KLUB" is the least sensitive to parameter variations and is consistently better than or comparable to the other methods across various combinations of model parameters. While the toy data is valuable for showcasing the unique properties of beta diffusion, the performance of "Beta KLUB" is not that sensitive to model parameters, and the observed patterns may be disrupted by image-specific settings. Hence, their utility in tuning beta diffusion for image generation, where data dimension and model size/architecture are much larger and more complex, is limited.

### Mixture of Continuous Range-bounded Distributions and Point Masses

We consider an equal mixture of three range-bounded distributions and two unit point masses, including a Uniform distribution between 0.1 and 0.2, a Beta(1,5) distribution first scaled by 0.1 and then shifted by 0.3, a unit point mass at \(x_{0}=0.5\), a Beta(0.5,0.5) distribution first scaled by 0.1 and

Figure 8: Analogy to Figure 5 for the mixture of continuous range-bounded data and point masses.

Figure 7: Analogy to Figure 3 for the mixture of continuous range-bounded data and point masses.

Figure 9: Analogy to Figure 8 for comparing “KLUB Conditional,” “KLUB Marginal,” and “KLUB.”

then shifted by 0.6, and a unit point mass at \(x_{0}=0.8\). A random sample is generated as

\[x_{0}=y_{k},\ k\sim\text{Discrete}(\{1,2,3,4,5\}),\ y_{1}\sim \text{Unif}(0.1,0.2),\ y_{2}\sim\frac{1}{0.1}\text{Beta}(\frac{y_{2}-0.3}{0.1}; 1,5),\] \[y_{3}=0.5,\ y_{4}\sim\frac{1}{0.1}\text{Beta}(\frac{y_{4}-0.6}{0. 1};0.5,0.5),\ y_{5}=0.9.\] (29)

We follow the same experimental protocol in Section 4.1. The results are shown in Figures 7 and 8, from which we again observe that "Gauss ELBO" does a poor job in aligning its generation with the true data supports and places a proportion of its generation to form smooth transitions between the boundaries of high-density regions; "Beta ELBO" well captures the data supports and density shapes, but has a tendency to overestimate the density of smaller-valued data; whereas "Beta KLUB" very well aligns its generated data with the true data supports and captures the overall density shapes and the sharp transitions between these five range-bounded density regions, largely avoiding placing generated data into zero-density regions.

## Appendix F Preconditioning of Beta Diffusion for Image Generation

It is often a good practice to precondition the input of a neural network. For beta diffusion, most of our computation is operated in the logit space and hence we will consider how to precondition \(\text{logit}(z_{t})\) before feeding it as the input to the generator \(f_{\theta}\). Specifically, since we have

\[z_{t}\sim\text{Beta}(\eta\alpha_{t}x_{0},\eta(1-\alpha_{t}x_{0})),\]

we can draw the logit of \(z_{t}\) as

\[\text{logit}(z_{t})=\ln u_{t}-\ln v_{t},\ \ u_{t}\sim\text{ Gamma}(\eta\alpha_{t}x_{0},1),\ \ v_{t}\sim\text{Gamma}(\eta(1-\alpha_{t}x_{0}),1).\]

Assume \(x_{0}\sim\text{Unif}[x_{\min},x_{\max}]\), where \(x_{\min}=S_{lift}\) and \(x_{\max}=S_{cale}+S_{lift}\), we have

\[\mathbb{E}[\text{logit}(z_{t})]=\mathbb{E}[\ln u_{t}]-\mathbb{E} [\ln v_{t}]\] \[=\mathbb{E}_{x_{0}}[\psi(\eta\alpha_{t}x_{0})]-\mathbb{E}_{x_{0}}[ \psi(\eta(1-\alpha_{t}x_{0}))],\]

where

\[\mathbb{E}_{x_{0}}[\psi(\eta\alpha_{t}x_{0})] =\frac{1}{\eta\alpha_{t}(x_{\max}-x_{\min})}\int_{\eta\alpha_{t}x _{\min}}^{\eta\alpha_{t}x_{\max}}d\ln\Gamma(z)\] \[=\frac{1}{\eta\alpha_{t}(x_{\max}-x_{\min})}[\ln\Gamma(\eta\alpha _{t}x_{\max})-\ln\Gamma(\eta\alpha_{t}x_{\min})],\]

\[\mathbb{E}_{x_{0}}[\psi(\eta(1-\alpha_{t}x_{0}))] =\frac{1}{\eta\alpha_{t}(x_{\max}-x_{\min})}\int_{\eta(1-\alpha_ {t}x_{\max})}^{\eta(1-\alpha_{t}x_{\min})}d\ln\Gamma(z)\] \[=\frac{1}{\eta\alpha_{t}(x_{\max}-x_{\min})}[\ln\Gamma(\eta(1- \alpha_{t}x_{\min}))-\ln\Gamma(\eta(1-\alpha_{t}x_{\max}))].\]

We further estimate the variance of \(\text{logit}(z_{t})\) as

\[\text{var}[\text{logit}(z_{t})]=\text{var}[\ln u_{t}]+\text{var}[ \ln v_{t}]\] \[=\mathbb{E}_{x_{0}}[\psi^{(1)}(\eta\alpha_{t}x_{0})]+\mathbb{E}_ {x_{0}}[\psi^{(1)}(\eta(1-\alpha_{t}x_{0}))]+\text{var}_{x_{0}}[\psi(\eta \alpha_{t}x_{0})]+\text{var}_{x_{0}}[\psi(\eta(1-\alpha_{t}x_{0}))]\]

where

\[\mathbb{E}_{x_{0}}[\psi^{(1)}(\eta\alpha_{t}x_{0})]=\frac{1}{\eta\alpha_{t}(x _{\max}-x_{\min})}[\psi(\eta\alpha_{t}x_{\max})-\psi(\eta\alpha_{t}x_{\min})],\]

\[\mathbb{E}_{x_{0}}[\psi^{(1)}(\eta(1-\alpha_{t}x_{0}))]=\frac{1}{\eta\alpha_{t} (x_{\max}-x_{\min})}[\psi(\eta(1-\alpha_{t}x_{\min}))-\psi(\eta(1-\alpha_{t}x _{\max}))],\]

\[\text{var}_{x_{0}}[\psi(\eta\alpha_{t}x_{0})]=\mathbb{E}_{x_{0}}[\psi^{2}(\eta \alpha_{t}x_{0})]-(\mathbb{E}_{x_{0}}[\psi(\eta\alpha_{t}x_{0})])^{2}\] \[\approx\max\left(\frac{1}{100}\sum_{i=0}^{100}\frac{\psi^{2}\left( \eta\alpha_{t}\left(x_{\min}+\frac{i}{100}(x_{\max}-x_{\min})\right)\right)}{2 ^{\mathbf{1}(i=0)+\mathbf{1}(i=100)}}-(\mathbb{E}_{x_{0}}[\psi(\eta\alpha_{t}x _{0})])^{2},\ 0\right),\]\[\text{var}_{x_{0}}[\psi(\eta(1-\alpha_{t}x_{0}))]=\mathbb{E}_{x_{0}}[ \psi^{2}(\eta(1-\alpha_{t}x_{0}))]-(\mathbb{E}_{x_{0}}[\psi(\eta(1-\alpha_{t}x_{ 0}))])^{2}\] \[\approx\max\left(\frac{1}{100}\sum_{i=0}^{100}\frac{\psi^{2}\left( \eta\left(1-\alpha_{t}\left(x_{\min}+\frac{i}{100}(x_{\max}-x_{\min})\right) \right)\right)}{2^{1(i=0)+1(i=100)}}-(\mathbb{E}_{x_{0}}[\psi(\eta(1-\alpha_{t }x_{0}))])^{2},\ \ 0\right).\]

Now we are ready to precondition \(\text{logit}(z_{t})\) as

\[g(z_{t})=\frac{\text{logit}(z_{t})-\mathbb{E}[\text{logit}(z_{t})]}{\sqrt{\text {var}[\text{logit}(z_{t})]}}.\]

We use \(g(z_{t})\) as the input to the generator \(f_{\theta}\) and add a skip connection layer to add \(g(z_{t})\) into the output of the generator. Specifically, following the notation of EDM, we define the network as

\[f_{\theta}(z_{t},t)=c_{\text{skip}}(t)g(z_{t})+c_{\text{out}}(t)F_{\theta}(c _{\text{in}}(t)g(z_{t}),c_{\text{noise}}(t)),\]

where for simplicity, we set \(c_{\text{skip}}(t)=c_{\text{out}}(t)=c_{\text{in}}(t)=1\) and \(c_{\text{noise}}(t)=-\text{logit}(\alpha_{t})/8\). A more sophisticated selection of these parameters has the potential to enhance the performance of beta diffusion. However, due to our current limitations in computing resources, we defer this investigation to future studies.

## Appendix G Parameter Settings of Beta Diffusion on CIFAR10

We present the intuition on how we set the model parameters, including \(S_{shift},S_{cale},\eta,\omega,\pi,c_{0}\), and \(c_{1}\). Given the limitation of our computation resources, we leave the careful tuning of these model parameters to future work.

We set \(\omega=0.99\) and \(\pi=0.95\) across all experiments conducted on CIFAR10 with beta diffusion.

We pre-process the range-bounded data by linearly scaling and shifting them to lie between \([S_{shift},S_{shift}+S_{cale}]\), where \(0<S_{shift}<S_{shift}+S_{cale}<1\). This linear transformation serves two purposes: firstly, it enables us to use beta diffusion to model any range-bounded data, and secondly, it helps avoid numerical challenges in computing the log-beta divergence when its arguments are small. When evaluating the performance, we linearly transform the generated data back to their original scale by reversing the scaling and shifting operations applied during the pre-processing step. For CIFAR10 images, as beta diffusion is diffusing pixels towards \(\text{Beta}(0,\eta)\), our intuition is to set both \(S_{cale}\) and \(S_{shift}\) large enough to differentiate the diffusion trajectories of different pixel values. However, \(S_{cale}+S_{shift}\) needs to be smaller than 1, motivating us to choose \(S_{cale}=0.39\) and \(S_{lift}=0.60\).

For the diffusion concentration parameter \(\eta\), our intuition is that a larger \(\eta\) provides a higher ability to differentiate different pixel values and allows a finer discretization of the reverse diffusion process, but leads to slower training and demands more discretized steps during sampling. We set \(\eta=10000\) and the mini-batch size as \(B=288\) by default, but also perform an ablation study on CIFAR10 with several different values, as shown in Table 3.

We use a sigmoid-based diffusion schedule as

\[\alpha_{t}=1/(1+e^{-c_{0}-(c_{1}-c_{0})t}),\]

where we set \(c_{0}=10\) and \(c_{1}=-13\) by default. We note a similar schedule had been introduced in Kingma et al. [35] and Jabri et al. [29] for Gaussian diffusion.

For the CIFAR-10 dataset2, we utilize the parameterization of EDM3[34] as the code base. We replace the variance preserving (VP) loss and the corresponding network parameterization implemented in EDM, which is the weighted negative ELBO of Gaussian diffusion, with the KLUB loss of beta diffusion that is given by (24). We keep the generator network the same as that of VP-EDM, except that we set \(c_{noise}=-\text{logit}(\alpha_{t})/8\) and simplify the other parameters as \(c_{\text{skip}}=1\), \(c_{\text{out}}=1\), and \(c_{\text{in}}=1\). The image pixel values between 0 and 255 are divided by 255 and then scaled by \(S_{cale}\) and shifted by \(S_{shift}\), and hence we have \(x_{0}\in[S_{shift},S_{hift}+S_{cale}]\). The two inputs to the generator network \(f_{\theta}(\cdot,\cdot)\), originally designed for VP-EDM and adopted directly for beta diffusion, are \(g(\text{logit}(z_{t}))\) and \(\text{logit}(\alpha_{t})\), where \(g(\cdot)\) is a preconditioning function described in detail in Appendix F. The output of the generator network is first transformed by a sigmoid function and then scaled by \(S_{cale}\) and shifted by \(S_{shift}\), and hence \(\hat{x}_{0}=f_{\theta}(z_{t},t)\in[S_{hift},S_{hift}+S_{cale}]\).

Footnote 2: https://www.cs.toronto.edu/~kriz/cifar.html

Footnote 3: https://github.com/NVlabs/edm