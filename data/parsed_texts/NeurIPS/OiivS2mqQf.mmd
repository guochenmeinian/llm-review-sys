# Augmentation-Aware Self-Supervision

for Data-Efficient GAN Training

 Liang Hou\({}^{1,3,4}\), Qi Cao\({}^{1}\), Yige Yuan\({}^{1,3}\), Songtao Zhao\({}^{4}\), Chongyang Ma\({}^{4}\),

**Siyuan Pan\({}^{4}\), Pengfei Wan\({}^{4}\), Zhongyuan Wang\({}^{4}\), Huawei Shen\({}^{1,3}\), Xueqi Cheng\({}^{2,3}\)**

\({}^{1}\)CAS Key Laboratory of AI Safety and Security,

Institute of Computing Technology, Chinese Academy of Sciences

\({}^{2}\)CAS Key Laboratory of Network Data Science and Technology,

Institute of Computing Technology, Chinese Academy of Sciences

\({}^{3}\)University of Chinese Academy of Sciences

\({}^{4}\)Kuaishou Technology

lianghou96@gmail.com

###### Abstract

Training generative adversarial networks (GANs) with limited data is challenging because the discriminator is prone to overfitting. Previously proposed differentiable augmentation demonstrates improved data efficiency of training GANs. However, the augmentation implicitly introduces undesired invariance to augmentation for the discriminator since it ignores the change of semantics in the label space caused by data transformation, which may limit the representation learning ability of the discriminator and ultimately affect the generative modeling performance of the generator. To mitigate the negative impact of invariance while inheriting the benefits of data augmentation, we propose a novel augmentation-aware self-supervised discriminator that predicts the augmentation parameter of the augmented data. Particularly, the prediction targets of real data and generated data are required to be distinguished since they are different during training. We further encourage the generator to adversarially learn from the self-supervised discriminator by generating augmentation-predictable real and not fake data. This formulation connects the learning objective of the generator and the arithmetic - harmonic mean divergence under certain assumptions. We compare our method with state-of-the-art (SOTA) methods using the class-conditional BigGAN and unconditional StyleGAN2 architectures on data-limited CIFAR-10, CIFAR-100, FFHQ, LSUN-Cat, and five low-shot datasets. Experimental results demonstrate significant improvements of our method over SOTA methods in training data-efficient GANs.1

Footnote 1: Our code is available at https://github.com/liang-hou/augself-gan.

## 1 Introduction

Generative adversarial networks (GANs) [10] have achieved great progress in synthesizing diverse and high-quality images in recent years [2, 17, 19, 20, 13]. However, the generation quality of GANs depends heavily on the amount of training data [47, 18]. In general, the decrease of training samples usually yields a sharp decline in both fidelity and diversity of the generated images [39, 48]. This issue hinders the wide application of GANs due to the fact of insufficient data in real-world applications. For instance, it is valuable to imitate the style of an artist whose paintings are limited. GANs typically consist of a generator that is designed to generate new data and a discriminator that guides the generator to recover the real data distribution. The major challenge of training GANs under limited data is that the discriminator is prone to overfitting [47, 18], and therefore lacks generalization to teach the generator to learn the underlying real data distribution.

In order to alleviate the overfitting issue, recent researches have suggested a variety of approaches, mainly from the perspectives of training data [18], loss functions [40], and network architectures [28]. Among them, data augmentation-based methods have gained widespread attention due to its simplicity and extensibility. Specifically, DiffAugment [47] introduced differentiable augmentation techniques for GANs, in which both real and generated data are augmented to supplement the training set of the discriminator. However, this straightforward augmentation method overlooks augmentation-related semantic information, as it solely augments the domain of the discriminator while neglecting the range. Such a practice might introduces an inductive bias that potentially forces the discriminator to remain invariant to different augmentations [24], which could limit the representation learning of the discriminator and subsequently affect the generation performance of the generator [12].

In this paper, we propose a novel augmentation-aware self-supervised discriminator that predicts the augmentation parameter of augmented data with the original data as reference to address the above problem. Meanwhile, the self-supervised discriminator is required to be distinguished between the real data and the generated data since their distributions are different during training, especially in the early stage. The proposed discriminator can benefit the generator in two ways, implicitly and explicitly. On one hand, the self-supervised discriminator can transfer the learned augmentation-aware knowledge to the original discriminator through parameter sharing. On the other hand, we allow the generator to learn adversarially from the self-supervised discriminator by generating augmentation-predictable real and not fake data (Equation (6)). We also theoretically analyzed the connection between this objective function and the minimization of a robust \(f\)-divergence divergence (the arithmetic \(-\) harmonic mean divergence [37]). In experiments, we show that the proposed method compares favorably to the data augmentation counterparts and other state-of-the-art (SOTA) methods on common data-limited benchmarks (CIFAR-10 [21], CIFAR-100 [21], FFHO [17], LSUN-Cat [45], and five low-shot image generation datasets [36]) based on the class-conditional BigGAN [2] and unconditional StyleGAN2 [19] architectures. In addition, we carried out extensive experiments to demonstrate the effectiveness of the objective function design, the adaptability to stronger data augmentations, and the robustness of hyper-parameter selection in our method.

## 2 Related Work

In this section, we provide an overview of existing work related to training GANs in data-limited scenarios. We also discuss methodologies incorporating self-supervised learning techniques.

### GANs under Limited Training Data

Recently, researchers have become interested in freeing training GANs from the need to collect large amounts of data for adaptability in real-world scenarios. Previous studies typically fall into two main categories. The first one involves adopting a pre-trained GAN model to the target domain by fine-tuning partial parameters [41; 31; 42; 30]. However, it requires external training data, and the adoption performance depends heavily on the correlation between the source and target domains.

The other one focuses on training GANs from scratch with elaborated data-efficient training strategies. DiffAugment [47] utilized differentiable augmentation to supplement the training set to prevent discriminator from overfitting in limited data regimes. Concurrently, ADA [18] introduced adaptive data augmentation with a richer set of augmentation categories. APA [16] adaptively augmented the real data with the most plausible generated data. LeCam-GAN [40] proposed adaptive regularization for the discriminator and showed a connection to the Le Cam divergence [23]. [3] discovered that sparse sub-network (lottery tickets) [5] and feature-level adversarial augmentation could offer orthogonal gains to data augmentation methods. InsGen [43] improved the data efficiency of training GANs by incorporating instance discrimination tasks to the discriminator. MaskedGAN [14] employed masking in the spatial and spectral domains to alleviate the discriminator overfitting issue. GenCo [7] discriminated samples from multiple views with weight-discrepancy and data-discrepancy mechanisms. FreGAN [44] focused on discriminating between real and fake samples in the high-frequency domain. DigGAN [8] constrains the discriminator gradient gap between real and generated data. FastGAN [28] designed a lightweight generator architecture and observed that a self-supervised discriminator could enhance low-shot generation performance. Our method falls into the second category, supplementing data augmentation-based GANs and can be also applied to other methods.

### GANs with Self-Supervised Learning

Self-supervised learning techniques excel at learning meaningful representations without human-annotated labels by solving pretext tasks. Transformation-based self-supervised learning methods such as rotation recognition [9] have been incorporated into GANs to address catastrophic forgetting in discriminators [4; 38; 12]. Various other self-supervised tasks have also been explored, including jigsaw puzzle solving [1], latent transformation detection [33], and mutual information maximization [25]. Moreover, ContraD [15] decouples the representation learning and discrimination of the discriminator, utilizing contrastive learning for representation learning and a discriminator head for distinguishing real from fake upon the contrastive representations. In contrast to ours, CR-GAN [46] and ICR-GAN proposed consistency regularization for the discriminator, which corresponds to an explicit augmentation-invariant of the discriminator. both our proposed method and SSGAN-LA [12] belong to adversarial self-supervised learning, they differ in the type of self-supervised signals and model inputs. SSGAN-LA is limited to categorical self-supervision [9], which is incompatible with popular augmentation-based GANs like DiffAugment [47]. Our method is applicable for continuous self-supervision and integrates seamlessly with DiffAugment. Furthermore, continuous self-supervision have a magnitude relationship and thus can provide more refined gradient feedback for the model to overcome overfitting in data-limited scenarios. Additionally, unlike SSGAN-LA, our method does not constrain the invertibility of data transformations (Theorem 1) because it additionally take the original sample as input for the self-supervised discriminator (Equation (5)).

## 3 Preliminaries

In this section, we introduce the necessary concepts and preliminaries for completeness of the paper.

### Generative Adversarial Networks

Generative adversarial networks (GANs) [10] typically contain a generator \(G:\mathcal{Z}\rightarrow\mathcal{X}\) that maps a low-dimensional latent code \(\mathbf{z}\in\mathcal{Z}\) endowed with a tractable prior \(p(\mathbf{z})\), e.g., multivariate normal distribution \(\mathcal{N}(\mathbf{0},\mathbf{I})\), to a high-dimensional data point \(\mathbf{x}\in\mathcal{X}\), which induces a generated data distribution (density) \(p_{G}(\mathbf{x})=\int_{\mathcal{Z}}p(\mathbf{z})\delta(\mathbf{x}-G(\mathbf{ z}))\mathbf{dz}\) with the Dirac delta distribution \(\delta(\cdot)\), and also contain a discriminator \(D:\mathcal{X}\rightarrow\mathbb{R}\) that is required to distinguish between the real data sampled from the underlying data distribution (density) \(p_{\mathrm{data}}(\mathbf{x})\) and the generated ones. The generator attempts to fool the discriminator to eventually recover the real data distribution, i.e., \(p_{G}(\mathbf{x})=p_{\mathrm{data}}(\mathbf{x})\). Formally, the loss functions for the discriminator and the generator can be formulated as follows:

\[\mathcal{L}_{D} =\mathbb{E}_{\mathbf{x}\sim p_{\mathrm{data}}(\mathbf{x})}[f(D( \mathbf{x}))]+\mathbb{E}_{\mathbf{z}\sim p(\mathbf{z})}[h(D(G(\mathbf{z})))],\] (1) \[\mathcal{L}_{G} =\mathbb{E}_{\mathbf{z}\sim p(\mathbf{z})}[g(D(G(\mathbf{z})))].\] (2)

Different real-valued functions \(f\), \(h\), and \(g\) correspond to different variants of GANs [32]. For example, the minimax GAN [10] can be constructed by setting \(f(x)=-\log(\sigma(x))\) and \(h(x)=-g(x)=-\log(1-\sigma(x))\) with the sigmoid function \(\sigma(x)=1/(1+\exp(-x))\). In this study, we follow the practices of DiffAugment [47] to adopt the hinge loss [26], i.e., \(f(x)=h(-x)=\max(0,1-x)\) and \(g(x)=-x\), for experiments based on BigGAN [2] and the log loss [10], i.e., \(f(x)=g(x)=-\log(\sigma(x))\) and \(h(x)=-\log(1-\sigma(x))\), for experiments based on StyleGAN2 [19].

Figure 1: Examples of images with different kinds of differentiable augmentation (including the original unaugmented one) and their re-scaled corresponding augmentation parameters \(\bm{\omega}\in[0,1]^{d}\).

### Differentiable Augmentation for GANs

DiffAugment [47] introduces differentiable augmentation \(T:\mathcal{X}\times\Omega\rightarrow\hat{\mathcal{X}}\) parameterized by a randomly-sampled parameter \(\bm{\omega}\in\Omega\) with a prior \(p(\bm{\omega})\) for data-efficient GAN training. The parameter \(\bm{\omega}\) determines exactly how to transfer a sample \(\mathbf{x}\) to an augmented one \(\hat{\mathbf{x}}\in\hat{\mathcal{X}}\) for the discriminator. After manually re-scaling (for \(\bm{\omega}\in[0,1]^{d}\)), the parameters of all three kinds of differentiable augmentation used in DiffAugment for 2D images can be expressed as follows:

* color: \(\bm{\omega}_{\mathrm{color}}=(\lambda_{\mathrm{brightness}},\lambda_{\mathrm{ saturation}},\lambda_{\mathrm{contrast}})\in[0,1]^{3}\);
* translation: \(\bm{\omega}_{\mathrm{translation}}=(x_{\mathrm{translation}},y_{\mathrm{ translation}})\in[0,1]^{2}\);
* cutout: \(\bm{\omega}_{\mathrm{cutout}}=(x_{\mathrm{offset}},y_{\mathrm{offset}})\in[0,1]^{2}\).

Figure 1 illustrates the augmentation operations and their parameters. Formally, the loss functions for the discriminator and the generator of GANs with DiffAugment are defined as follows:

\[\mathcal{L}_{D}^{\mathrm{da}}=\mathbb{E}_{\bm{\kappa}\sim p_{ \mathrm{data}}(\bm{\kappa}),\bm{\omega}\sim p(\bm{\omega})}[f(D(T(\mathbf{x} ;\bm{\omega})))]+\mathbb{E}_{\bm{\mathbf{z}}\sim p(\bm{\mathbf{z}}),\bm{ \omega}\sim p(\bm{\omega})}[h(D(T(G(\mathbf{z});\bm{\omega})))],\] (3) \[\mathcal{L}_{G}^{\mathrm{da}}=\mathbb{E}_{\bm{\mathbf{z}}\sim p( \bm{\mathbf{z}}),\bm{\omega}\sim p(\bm{\omega})}[g(D(T(G(\mathbf{z});\bm{ \omega})))],\] (4)

where \(\bm{\omega}\) can represent any combination of these parameters. We choose all augmentations by default, which means augmentation color, translation, and cutout are adopted for each image sequentially.

## 4 Method

Data augmentation for GANs allows the discriminator to distinguish a single sample from multiple perspectives by transforming it into various augmented samples according to different augmentation parameters. However, it overlooks the differences in augmentation intensity, such as color contrast and translation magnitude, leading the discriminator to implicitly maintain invariance to these varying intensities. The invariance may limit the representation learning ability of the discriminator because it loses augmentation-related information (e.g., color and position) [24]. Figure 2 confirms the impact of this point on the discriminator representation learning task [4]. We argue that a discriminator that captures comprehensive representations contributes to better convergence of the generator [35; 22]. Moreover, data augmentation may lead to augmentation leaking in generated data, when using specific data augmentations such as random 90-degree rotations [18; 12]. Therefore, our goal is to eliminate the unnecessary potential inductive bias (invariance to augmentations) for the discriminator while preserving the benefits of data augmentation for training data-efficient GANs.

To achieve this goal, we propose a novel augmentation-aware self-supervised discriminator \(\hat{D}:\hat{\mathcal{X}}\times\mathcal{X}\rightarrow\Omega^{+}\cup\Omega^{-}\) that predicts the augmentation parameter and authenticity of the augmented data given the original data as reference. Distinguishing between the real data and the generated data with different self-supervision is because they are different during training, especially in the early stage. Specifically, the predictive targets of real data and generated data are represented as \(\bm{\omega}^{+}\in\Omega^{+}\) and \(\bm{\omega}^{-}\in\Omega^{-}\), respectively. They are constructed from the augmentation parameter \(\bm{\omega}\) with different transformations, i.e., \(\bm{\omega}^{+}=-\bm{\omega}^{-}=\bm{\omega}\). Since the augmentation parameter is a continuous

Figure 2: Comparison of representation learning ability of discriminator between BigGAN + DiffAugment and our AugSelf-BigGAN on CIFAR-10 and CIFAR-100 using linear logistic regression.

vector, we use mean squared error loss to regress it. The proposed method combines continuous self-supervised signals with real-vs-fake discrimination signals, thus can be considered as soft-label augmentation [12]. Comparison with self-supervision that does not distinguish between real and fake is referred to Table 6 in Appendix C. Notice that the predictive targets (augmentations) can be a subset of performed augmentations (see Table 7 in Appendix C for comparison). Mathematically, the loss function for the augmentation-aware self-supervised discriminator is formulated as the following:

\[\mathcal{L}^{\text{ss}}_{\hat{D}}=\mathbb{E}_{\mathbf{x},\boldsymbol{\omega} }\left[\|\hat{D}(T(\mathbf{x};\boldsymbol{\omega}),\mathbf{x})-\boldsymbol{ \omega}^{+}\|_{2}^{2}\right]+\mathbb{E}_{\mathbf{z},\boldsymbol{\omega}}\left[ \|\hat{D}(T(G(\mathbf{z});\boldsymbol{\omega}),G(\mathbf{z}))-\boldsymbol{ \omega}^{-}\|_{2}^{2}\right].\] (5)

In our implementations, the proposed self-supervised discriminator \(\hat{D}=\varphi\circ\phi\) shares the backbone \(\phi:\mathcal{X}\rightarrow\mathbb{R}^{d}\) with the original discriminator \(D=\psi\circ\phi\) except the output linear layer \(\varphi:\mathbb{R}^{d}\rightarrow\Omega^{+}\cup\Omega^{-}\). This parameter-sharing design not only improves the representation learning ability of the original discriminator but also saves the number of parameters in our model compared to the base model, e.g., \(0.04\%\) more parameters in BigGAN and \(0.01\%\) in StyleGAN2. More specifically, the self-supervised discriminator predicts the target based on the difference between learned representations of the augmented data and the original data, i.e., \(\hat{D}(T(\mathbf{x};\boldsymbol{\omega}),\mathbf{x})=\varphi(\phi(T(\mathbf{ x};\boldsymbol{\omega}))-\phi(\mathbf{x}))\) (see Table 8 in Appendix C for comparison with other architectures). The philosophy behind our design is that the backbone \(\phi\) should capture rich (which necessitates the design of a simple head \(\varphi\)) and linear (inspiring us to perform subtraction on the features) representations.

In order for the generator to directly benefit from the self-supervision of data augmentation, we establish a novel adversarial game between the augmentation-aware self-supervised discriminator and the generator with the objective function for the generator defined as follows:

\[\mathcal{L}^{\text{ss}}_{G}=\mathbb{E}_{\mathbf{z},\boldsymbol{\omega}}\left[ \|\hat{D}(T(G(\mathbf{z});\boldsymbol{\omega}),G(\mathbf{z}))-\boldsymbol{ \omega}^{+}\|_{2}^{2}\right]-\mathbb{E}_{\mathbf{z},\boldsymbol{\omega}}\left[ \|\hat{D}(T(G(\mathbf{z});\boldsymbol{\omega}),G(\mathbf{z}))-\boldsymbol{ \omega}^{-}\|_{2}^{2}\right].\] (6)

The objective function is actually the combination of the non-saturating loss (regarding the generated data as real, \(\min_{G}\mathbb{E}_{\mathbf{z},\boldsymbol{\omega}}[\|\hat{D}(T(G(\mathbf{z}) ;\boldsymbol{\omega}),G(\mathbf{z}))-\boldsymbol{\omega}^{+}\|_{2}^{2})\)) and the saturating loss (reversely optimizing the objective function of the discriminator, \(\max_{G}\mathbb{E}_{\mathbf{z},\boldsymbol{\omega}}[\|\hat{D}(T(G(\mathbf{z}) ;\boldsymbol{\omega}),G(\mathbf{z}))-\boldsymbol{\omega}^{-}\|_{2}^{2})\) (see Table 9 in Appendix C for ablation). Intuitively, the non-saturating loss encourages the generator to produce augmentation-predictable data, facilitating fidelity but reducing diversity. Conversely, the saturating loss strives for the generator to avoid generating augmentation-predictable data, promoting diversity at the cost of fidelity. We will elucidate in Section 5 how this formalization assists the generator in matching the fidelity and diversity of real data, ultimately leading to an accurate approximation of the target data distribution.

The total objective functions for the original discriminator, the augmentation-aware self-supervised discriminator, and the generator of our proposed method, named AugSelf-GAN, are given by:

\[\min_{D,\hat{D}}\mathcal{L}^{\text{da}}_{D}+\lambda_{d}\cdot \mathcal{L}^{\text{ss}}_{\hat{D}},\] (7) \[\min_{G}\mathcal{L}^{\text{da}}_{G}+\lambda_{g}\cdot\mathcal{L}^{ \text{ss}}_{G},\] (8)

Figure 3: Diagram of AugSelf-GAN. The original augmentation-based discriminator is \(D(T(\cdot))=\psi(\phi(T(\cdot)))\). The augmentation-aware self-supervised discriminator is \(\hat{D}(T(\cdot),\cdot)=\varphi(\phi(T(\cdot))-\phi(\cdot))\), where \(\varphi\) is our newly introduced linear layer with negligible additional parameters.

where the hyper-parameters are set as \(\lambda_{d}=\lambda_{g}=1\) in experiments by default unless otherwise specified (see Figure 6 for empirical studies). Details of objective functions are referred to Appendix B.

## 5 Theoretical Analysis

In this section, we analyze the connection between the theoretical learning objective of AugSelf-GAN and the arithmetic \(-\) harmonic mean (AHM) divergence [37] under certain assumptions.

**Proposition 1**.: _For any generator \(G\) and given unlimited capacity in the function space, the optimal augmentation-aware self-supervised discriminator \(\hat{D}^{*}\) has the form of:_

\[\hat{D}^{*}(\hat{\mathbf{x}},\mathbf{x})=\frac{\int p_{\mathrm{data}}( \mathbf{x},\boldsymbol{\omega},\hat{\mathbf{x}})\boldsymbol{\omega}^{+} \mathrm{d}\boldsymbol{\omega}+\int p_{G}(\mathbf{x},\boldsymbol{\omega},\hat{ \mathbf{x}})\boldsymbol{\omega}^{-}\mathrm{d}\boldsymbol{\omega}}{p_{\mathrm{ data}}(\mathbf{x},\hat{\mathbf{x}})+p_{G}(\mathbf{x},\hat{\mathbf{x}})}\] (9)

The proofs of all theoretical results (including the following ones) are deferred in Appendix A.

**Theorem 1**.: _Assume that \(\boldsymbol{\omega}^{+}=-\boldsymbol{\omega}^{-}=\boldsymbol{c}\) is constant, under the optimal self-supervised discriminator \(\hat{D}^{*}\), optimizing the self-supervised task for the generator \(G\) is equivalent to:_

\[\min_{G}4c\cdot M_{\mathrm{AH}}(p_{\mathrm{data}}(\mathbf{x}, \hat{\mathbf{x}})\|p_{G}(\mathbf{x},\hat{\mathbf{x}})),\] (10)

_where \(c=\|\mathbf{c}\|_{2}^{2}\) is constant and \(M_{\mathrm{AH}}\) is the arithmetic \(-\) harmonic mean divergence [37], of which the minimum is achieved if and only if \(p_{G}(\mathbf{x},\hat{\mathbf{x}})=p_{\mathrm{data}}(\mathbf{x},\hat{ \mathbf{x}})\Rightarrow p_{G}(\mathbf{x})=p_{\mathrm{data}}(\mathbf{x})\)._

Theorem 1 reveals that the generator of AugSelf-GAN theoretically still satisfies generative modeling, i.e., accurately learning the target data distribution, under certain assumptions. Although AugSelf-GAN does not obey the strict assumption, we note that this is not rare in the literature.2 Under this assumption, AugSelf-GAN can be regarded as a multi-dimensional extension of LS-GAN [29] in terms of the loss function, while excluding that of the generator. Additionally, our analysis offers an alternative theoretically grounded generator loss function for the LS-GAN family.3

Footnote 2: Goodfellow et al. [10] analyzed that saturated GAN optimizes the Jensen Shannon (JS) divergence, but in fact it uses non-saturated loss. LeCam-GAN [40] showed a connection between the Le Cam (LC) divergence [23] and its objective function based on fixed regularization, but in practice it uses exponential moving average.

Footnote 3: The theoretical analysis of LS-GAN is actually inconsistent with its generator loss function.

**Corollary 1**.: _The following equality and inequality hold for the AHM divergence:_

* \(M_{\mathrm{AH}}(p_{\mathrm{data}}(\mathbf{x},\hat{\mathbf{x}})\|p_{G}( \mathbf{x},\hat{\mathbf{x}}))+M_{\mathrm{AH}}(p_{G}(\mathbf{x},\hat{\mathbf{x }})\|p_{\mathrm{data}}(\mathbf{x},\hat{\mathbf{x}}))=\Delta(p_{\mathrm{data}}( \mathbf{x},\hat{\mathbf{x}})\|p_{G}(\mathbf{x},\hat{\mathbf{x}}))\)__
* \(M_{\mathrm{AH}}(p_{\mathrm{data}}(\mathbf{x},\hat{\mathbf{x}})\|p_{G}( \mathbf{x},\hat{\mathbf{x}}))=1-W(p_{\mathrm{data}}(\mathbf{x},\hat{\mathbf{x }})\|p_{G}(\mathbf{x},\hat{\mathbf{x}}))\leq 1\)__

_where \(\Delta\) is the Le Cam (LC) divergence [23], and \(W\) is the harmonic mean divergence [37]._

Corollary 1 reveals an inequality \(M_{\mathrm{AH}}(p_{\mathrm{data}}(\mathbf{x},\hat{\mathbf{x}})\|p_{G}( \mathbf{x},\hat{\mathbf{x}}))\leq\Delta(p_{\mathrm{data}}(\mathbf{x},\hat{ \mathbf{x}})\|p_{G}(\mathbf{x},\hat{\mathbf{x}}))\) because of non-negativity of AHM divergence \(M_{\mathrm{AH}}(p_{G}(\mathbf{x},\hat{\mathbf{x}})\|p_{\mathrm{data}}( \mathbf{x},\hat{\mathbf{x}}))\geq 0\). Figure 4 plots the function \(f\) in AHM divergence and other common \(f\)-divergences in the GAN literature. The AHM divergence shows better robustness of the function \(f\) than others for extremely large inputs \(p(\mathbf{x})/q(\mathbf{x})=D^{*}(\mathbf{x})/(1-D^{*}(\mathbf{x}))\), which is likely for the optimal discriminator \(D^{*}\) in data-limited scenarios.

Figure 4: Comparison of the function \(f\) in different \(f\)-divergences. The \(f\)-divergence between two probability distributions \(p(\mathbf{x})\) and \(q(\mathbf{x})\) is defined as \(D_{f}(p(\mathbf{x})\|q(\mathbf{x}))=\int_{\mathcal{X}}q(\mathbf{x})f(p( \mathbf{x})/q(\mathbf{x}))\mathrm{d}\mathbf{x}\) with a convex function \(f:\mathbb{R}_{\geq 0}\rightarrow\mathbb{R}\) satisfying \(f(1)=0\). The x- and y-axis denote the input and the value of the function \(f\) in the \(f\)-divergence. The function \(f\) of the AHM divergence yields the most robust value for large inputs.

## 6 Experiments

We implement AugSelf-GAN based on DiffAugment [47], keeping the backbones and settings unchanged for fair comparisons with prior work under two evaluation metrics, IS [34] and FID [11]. The mean and standard deviation (if reported) are obtained with five evaluation runs at the best FID checkpoint. Each of experiments in this work was conducted on an 32GB NVIDIA V100 GPU.

### Comparison with State-of-the-Art Methods

Cifar-10 and CIFAR-100.Table 1 reports the results on CIFAR-10 and CIFAR-100 [21]. These experiments are based on the BigGAN architecture [2]. Our method significantly outperforms the direct baseline DiffAugment [47] and yields the best generation performance in terms of FID and IS compared with SOTA methods. Notably, our method achieves further improvement when using stronger augmentation (see Table 5), i.e., AugSelf-BigGAN+ (translation\(\uparrow\) and cutout\(\uparrow\)).

FFHQ and LSUN-Cat.Table 2 reports the FID results on FFHQ [17] and LSUN-Cat [45]. The hyper-parameter is \(\lambda_{g}=0.2\). AugSelf-GAN performs substantially better than baselines with the same network backbone. Also, stronger augmentation, i.e., AugSelf-StyleGAN2+ (translation\(\uparrow\) and cutout\(\uparrow\)), further improves the performance when training data is very limited.

\begin{table}
\begin{tabular}{l l c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{4}{c}{100\% training data} & \multicolumn{2}{c}{20\% training data} & \multicolumn{2}{c}{10\% training data} \\ \cline{3-10}  & & IS (\(\uparrow\)) & FID (\(\downarrow\)) & IS (\(\uparrow\)) & FID (\(\downarrow\)) & IS (\(\uparrow\)) & FID (\(\downarrow\)) \\ \hline \multirow{8}{*}{
\begin{tabular}{l} **CIFAR-10** \\ **CIFAR-100** \\ **C

[MISSING_PAGE_FAIL:8]

Stronger augmentation.Translation and cutout actually erase parts of image information, which help prevent the discriminator from overfitting, but could suffer from underfitting if excessive. Our self-supervised task enables the discriminator to be aware of different levels of translation and cutout, which helps alleviate underfitting and allows us to explore stronger translation and cutout. Table 5 compares AugSelf-GAN with DiffAugment in this setting. Overall, when data is limited, AugSelf-GAN can further benefit from stronger translation and cutout and achieve new SOTA FID results, while DiffAugment cannot. This implicitly indicates that our method enables the model o learn meaningful features to overcome underfitting, even under strong data augmentation.

Hyper-parameters.Figure 6 plots the FID results of AugSelf-GAN with different hyper-parameters \(\lambda=\lambda_{d}=\lambda_{g}\) ranging from \([0,10]\) on CIFAR-10 and CIFAR-100. Notice that \(\lambda=0\) corresponds to the baseline BigGAN + DiffAugment. AugSelf-BigGAN performs the best when \(\lambda\) is near \(1\). It is worth noting that AugSelf-BigGAN outperforms the baseline even for \(\lambda=10\) with \(10\%\) and \(20\%\) training data, demonstrating superior robustness with respect to the hyper-parameter \(\lambda\).

## 7 Conclusion

This paper proposes a data-efficient GAN training method by utilizing augmentation parameters as self-supervision. Specifically, a novel self-supervised discriminator is proposed for predicting the augmentation parameters and data authenticity of augmented (real and generated) data simultaneously, given the original data. Meanwhile, the generator is encouraged to generate real rather than fake data of which augmentation parameters can be recognized by the self-supervised discriminator after augmentation. Theoretical analysis reveals a connection between the optimization objective of the generator and the arithmetic \(-\) harmonic mean divergence under certain assumptions. Experiments on data-limited benchmarks demonstrate superior qualitative and quantitative performance of the proposed method compared to previous methods.

\begin{table}
\begin{tabular}{l l c c c c c c} \hline \hline  & \multirow{2}{*}{Method} & \multicolumn{2}{c}{100\% training data} & \multicolumn{2}{c}{20\% training data} & \multicolumn{2}{c}{10\% training data} \\ \cline{3-8}  & & IS (\(\uparrow\)) & FID (\(\downarrow\)) & IS (\(\uparrow\)) & FID (\(\downarrow\)) & IS (\(\uparrow\)) & FID (\(\downarrow\)) \\ \hline \multirow{4}{*}{\begin{tabular}{l} CSV \\ \end{tabular} } & DiffAugment & 9.29\({}_{\pm.02}\) & 8.48\({}_{\pm.13}\) & 8.84\({}_{\pm.12}\) & 15.14\({}_{\pm.47}\) & **8.80\({}_{\pm.01}\)** & 20.60\({}_{\pm.13}\) \\  & + trans.\(\uparrow\) cut.\(\uparrow\) & 9.28\({}_{\pm.06}\) & 8.42\({}_{\pm.18}\) & 8.78\({}_{\pm.06}\) & 14.28\({}_{\pm.27}\) & 8.69\({}_{\pm.07}\) & 20.93\({}_{\pm.21}\) \\  & AugSelf-GAN & **9.43\({}_{\pm.14}\)** & 7.68\({}_{\pm.06}\) & 8.98\({}_{\pm.09}\) & 10.97\({}_{\pm.09}\) & 8.76\({}_{\pm.05}\) & 15.68\({}_{\pm.26}\) \\  & + trans.\(\uparrow\) cut.\(\uparrow\) & 9.27\({}_{\pm.05}\) & **7.54\({}_{\pm.04}\)** & **9.08\({}_{\pm.04}\)** & **9.95\({}_{\pm.17}\)** & 8.79\({}_{\pm.04}\) & **12.76\({}_{\pm.14}\)** \\ \hline \multirow{4}{*}{
\begin{tabular}{l} CSV \\ \end{tabular} } & DiffAugment & 11.02\({}_{\pm.07}\) & 11.49\({}_{\pm.21}\) & 9.45\({}_{\pm.05}\) & 24.98\({}_{\pm.48}\) & 8.50\({}_{\pm.09}\) & 34.92\({}_{\pm.63}\) \\  & + trans.\(\uparrow\) cut.\(\uparrow\) & 11.10\({}_{\pm.08}\) & 11.28\({}_{\pm.20}\) & 9.58\({}_{\pm.05}\) & 24.10\({}_{\pm.66}\) & 8.59\({}_{\pm.04}\) & 35.32\({}_{\pm.46}\) \\ \cline{1-1}  & AugSelf-GAN & **11.19\({}_{\pm.09}\)** & **9.88\({}_{\pm.07}\)** & **10.25\({}_{\pm.06}\)** & 16.11\({}_{\pm.25}\) & 9.78\({}_{\pm.08}\) & 21.30\({}_{\pm.15}\) \\ \cline{1-1}  & + trans.\(\uparrow\) cut.\(\uparrow\) & 11.12\({}_{\pm.10}\) & 10.09\({}_{\pm.05}\) & 10.14\({}_{\pm.11}\) & **15.33\({}_{\pm.20}\)** & **9.93\({}_{\pm.06}\)** & **18.64\({}_{\pm.09}\)** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Study on stronger augmentation. The best is in **bold** and the second best is underlined.

Figure 6: FID curves with varying hyper-parameters \(\lambda=\lambda_{d}=\lambda_{g}\in[0,10]\) on CIFAR-10 and CIFAR-100. The hyper-parameter \(\lambda=0\) corresponds to the baseline BigGAN + DiffAugment.

Limitations.In our experiments, we observed less significant improvement of AugSelf-GAN under sufficient training data. Furthermore, its effectiveness depends on the specific data augmentation used. In some cases, inappropriate data augmentation may limit the performance gain.

Broader impacts.This work aims at improving GANs under limited training data. While this may result in negative societal impacts, such as lowering the threshold of generating fake content or exacerbating bias and discrimination due to data issues, we believe that these risks can be mitigated. By establishing ethical guidelines for users and exploring fake content detection techniques, one can prevent these undesirable outcomes. Furthermore, this work contributes to the overall development of GANs and even generative models, ultimately promoting their potential benefits for society.

## Acknowledgements

We thank the anonymous reviewers for their valuable and constructive feedback. This work is funded by the National Natural Science Foundation of China under Grant Nos. 62272125, 62102402, U21B2046. Huawei Shen is also supported by Beijing Academy of Artificial Intelligence (BAAI).

## References

* Baykal et al. [2022] Gulcin Baykal, Furkan Ozcelik, and Gozde Unal. Exploring deshufflegans in self-supervised generative adversarial networks. _Pattern Recognition_, 2022. ISSN 0031-3203.
* Brock et al. [2019] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In _International Conference on Learning Representations_, 2019.
* Chen et al. [2021] Tianlong Chen, Yu Cheng, Zhe Gan, Jingjing Liu, and Zhangyang Wang. Data-efficient gan training beyond (just) augmentations: A lottery ticket perspective. In _Advances in Neural Information Processing Systems_, 2021.
* Chen et al. [2019] Ting Chen, Xiaohua Zhai, Marvin Ritter, Mario Lucic, and Neil Houlsby. Self-supervised gans via auxiliary rotation loss. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2019.
* Chen et al. [2021] Xuxi Chen, Zhenyu Zhang, Yongduo Sui, and Tianlong Chen. {GAN}s can play lottery tickets too. In _International Conference on Learning Representations_, 2021.
* Choi et al. [2020] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2020.
* Cui et al. [2022] Kaiwen Cui, Jiaxing Huang, Zhipeng Luo, Gongjie Zhang, Fangneng Zhan, and Shijian Lu. Genco: Generative co-training for generative adversarial networks with limited data. _Proceedings of the AAAI Conference on Artificial Intelligence_, 2022. doi: 10.1609/aaai.v36i1.19928.
* Fang et al. [2022] Tiantian Fang, Ruoyu Sun, and Alex Schwing. Diggan: Discriminator gradient gap regularization for gan training with limited data. In _Advances in Neural Information Processing Systems_, 2022.
* Gidaris et al. [2018] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In _International Conference on Learning Representations_, 2018.
* Goodfellow et al. [2014] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In _Advances in Neural Information Processing Systems_, 2014.
* Heusel et al. [2017] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In _Advances in Neural Information Processing Systems_, 2017.
* Hou et al. [2021] Liang Hou, Huawei Shen, Qi Cao, and Xueqi Cheng. Self-supervised gans with label augmentation. In _Advances in Neural Information Processing Systems_, 2021.

* [13] Liang Hou, Qi Cao, Huawei Shen, Siyuan Pan, Xiaoshuang Li, and Xueqi Cheng. Conditional GANs with auxiliary discriminative classifier. In _Proceedings of the 39th International Conference on Machine Learning_, Proceedings of Machine Learning Research, 2022.
* [14] Jiaxing Huang, Kaiwen Cui, Dayan Guan, Aoran Xiao, Fangneng Zhan, Shijian Lu, Shengcai Liao, and Eric Xing. Masked generative adversarial networks are data-efficient generation learners. In _Advances in Neural Information Processing Systems_, 2022.
* [15] Jongheon Jeong and Jinwoo Shin. Training {gan}s with stronger augmentations via contrastive discriminator. In _International Conference on Learning Representations_, 2021.
* [16] Liming Jiang, Bo Dai, Wayne Wu, and Chen Change Loy. Deceive d: Adaptive pseudo augmentation for gan training with limited data. In _Advances in Neural Information Processing Systems_, 2021.
* [17] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2019.
* [18] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. In _Advances in Neural Information Processing Systems_, 2020.
* [19] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2020.
* [20] Tero Karras, Miika Aittala, Samuli Laine, Erik Harkonen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. In _Advances in Neural Information Processing Systems_, 2021.
* [21] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. _Master's thesis, Department of Computer Science, University of Toronto_, 2009.
* [22] Nupur Kumari, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Ensembling off-the-shelf models for gan training. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.
* [23] Lucien Le Cam. _Asymptotic methods in statistical decision theory_. 2012.
* [24] Hankok Lee, Kibok Lee, Kimin Lee, Honglak Lee, and Jinwoo Shin. Improving transferability of representations via augmentation-aware self-supervision. In _Advances in Neural Information Processing Systems_, 2021.
* [25] Kwot Sin Lee, Ngoc-Trung Tran, and Ngai-Man Cheung. Infomax-gan: Improved adversarial image generation via information maximization and contrastive learning. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)_, 2021.
* [26] Jae Hyun Lim and Jong Chul Ye. Geometric gan. _arXiv preprint arXiv:1705.02894_, 2017.
* [27] Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh. Pacgan: The power of two samples in generative adversarial networks. In _Advances in Neural Information Processing Systems_, 2018.
* [28] Bingchen Liu, Yizhe Zhu, Kunpeng Song, and Ahmed Elgammal. Towards faster and stabilized {gan} training for high-fidelity few-shot image synthesis. In _International Conference on Learning Representations_, 2021.
* [29] Xudong Mao, Qing Li, Haoran Xie, Raymond Y.K. Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In _Proceedings of the IEEE International Conference on Computer Vision (ICCV)_, 2017.
* [30] Sangwoo Mo, Minsu Cho, and Jinwoo Shin. Freeze the discriminator: a simple baseline for fine-tuning gans. _arXiv preprint arXiv:2002.10964_, 2020.

* Noguchi and Harada [2019] Atsuhiro Noguchi and Tatsuya Harada. Image generation from small datasets via batch statistics adaptation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2019.
* Nowozin et al. [2016] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In _Advances in Neural Information Processing Systems_, 2016.
* Patel et al. [2021] Parth Patel, Nupur Kumari, Mayank Singh, and Balaji Krishnamurthy. Lt-gan: Self-supervised gan with latent transformation detection. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)_, 2021.
* Salimans et al. [2016] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and Xi Chen. Improved techniques for training gans. In _Advances in Neural Information Processing Systems_, 2016.
* Sauer et al. [2021] Axel Sauer, Kashyap Chitta, Jens Muller, and Andreas Geiger. Projected gans converge faster. In _Advances in Neural Information Processing Systems_, 2021.
* Si and Zhu [2011] Zhangzhang Si and Song-Chun Zhu. Learning hybrid image templates (hit) by information projection. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2011.
* Taneja [2008] Inder Jeet Taneja. On mean divergence measures. _Advances in Inequalities from probability theory and statistics. Nova, USA_, 2008.
* Tran et al. [2019] Ngoc-Trung Tran, Viet-Hung Tran, Bao-Ngoc Nguyen, Linxiao Yang, and Ngai-Man (Man) Cheung. Self-supervised gan: Analysis and improvement with multi-class minimax game. In _Advances in Neural Information Processing Systems_, 2019.
* Tran et al. [2021] Ngoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen, Trung-Kien Nguyen, and Ngai-Man Cheung. On data augmentation for gan training. _IEEE Transactions on Image Processing_, 2021. doi: 10.1109/TIP.2021.3049346.
* Tseng et al. [2021] Hung-Yu Tseng, Lu Jiang, Ce Liu, Ming-Hsuan Yang, and Weilong Yang. Regularizing generative adversarial networks under limited data. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2021.
* Wang et al. [2018] Yaxing Wang, Chenshen Wu, Luis Herranz, Joost van de Weijer, Abel Gonzalez-Garcia, and Bogdan Raducanu. Transferring gans: generating images from limited data. In _Proceedings of the European Conference on Computer Vision (ECCV)_, 2018.
* Wang et al. [2020] Yaxing Wang, Abel Gonzalez-Garcia, David Berga, Luis Herranz, Fahad Shahbaz Khan, and Joost van de Weijer. Minegan: Effective knowledge transfer from gans to target domains with few images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2020.
* Yang et al. [2021] Ceyuan Yang, Yujun Shen, Yinghao Xu, and Bolei Zhou. Data-efficient instance generation from instance discrimination. In _Advances in Neural Information Processing Systems_, 2021.
* yang et al. [2022] mengping yang, Zhe Wang, Ziqiu Chi, and Yanbing Zhang. Fregan: Exploiting frequency components for training gans under limited data. In _Advances in Neural Information Processing Systems_, 2022.
* Yu et al. [2015] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. _arXiv preprint arXiv:1506.03365_, 2015.
* Zhang et al. [2020] Han Zhang, Zizhao Zhang, Augustus Odena, and Honglak Lee. Consistency regularization for generative adversarial networks. In _International Conference on Learning Representations_, 2020.
* Zhao et al. [2020] Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han. Differentiable augmentation for data-efficient gan training. In _Advances in Neural Information Processing Systems_, 2020.
* Zhao et al. [2020] Zhengli Zhao, Zizhao Zhang, Ting Chen, Sameer Singh, and Han Zhang. Image augmentations for gan training. _arXiv preprint arXiv:2006.02595_, 2020.

Proofs

**Proposition 1**.: _For any generator \(G\) and given unlimited capacity in the function space, the optimal augmentation-aware self-supervised discriminator \(\hat{D}^{*}\) has the form of:_

\[\hat{D}^{*}(\hat{\mathbf{x}},\mathbf{x})=\frac{\int p_{\mathrm{data}}(\mathbf{x },\bm{\omega},\hat{\mathbf{x}})\bm{\omega}^{+}\mathrm{d}\bm{\omega}+\int p_{G} (\mathbf{x},\bm{\omega},\hat{\mathbf{x}})\bm{\omega}^{-}\mathrm{d}\bm{\omega}} {p_{\mathrm{data}}(\mathbf{x},\hat{\mathbf{x}})+p_{G}(\mathbf{x},\hat{ \mathbf{x}})}\] (9)

Proof.: The objective function of the self-supervised discriminator can be written as follows:

\[\mathcal{L}_{D}^{\mathrm{ss}} =\mathbb{E}_{\mathbf{x},\bm{\omega}}\left[\|\hat{D}(T(\mathbf{x };\bm{\omega}),\mathbf{x})-\bm{\omega}^{+}\|_{2}^{2}\right]+\mathbb{E}_{ \mathbf{z},\bm{\omega}}\left[\|\hat{D}(T(G(\mathbf{z});\bm{\omega}),G(\mathbf{ z}))-\bm{\omega}^{-}\|_{2}^{2}\right]\] \[=\iiint\left[p_{\mathrm{data}}(\mathbf{x},\bm{\omega},\hat{ \mathbf{x}})\|\hat{D}(\hat{\mathbf{x}},\mathbf{x})-\bm{\omega}^{+}\|_{2}^{2} +p_{G}(\mathbf{x},\bm{\omega},\hat{\mathbf{x}})\|\hat{D}(\hat{\mathbf{x}}, \mathbf{x})-\bm{\omega}^{-}\|_{2}^{2}\right]\mathrm{d}\mathbf{x}\mathrm{d}\bm{ \omega}\mathrm{d}\hat{\mathbf{x}}.\]

Minimizing the integral objective is equivalent to minimizing the objective on each data point:

\[\mathcal{L}_{D}^{\mathrm{ss}}(\mathbf{x},\hat{\mathbf{x}})=\int\left[p_{ \mathrm{data}}(\mathbf{x},\bm{\omega},\hat{\mathbf{x}})\|\hat{D}(\hat{ \mathbf{x}},\mathbf{x})-\bm{\omega}^{+}\|_{2}^{2}+p_{G}(\mathbf{x},\bm{\omega},\hat{\mathbf{x}})\|\hat{D}(\hat{\mathbf{x}},\mathbf{x})-\bm{\omega}^{-}\|_{2 }^{2}\right]\mathrm{d}\bm{\omega}.\]

Getting its derivative with respect to the self-supervised discriminator and let it equals to 0, we have the optimal self-supervised discriminator as:

\[\frac{\partial\mathcal{L}_{D}^{\mathrm{ss}}}{\partial\hat{D}( \hat{\mathbf{x}},\mathbf{x})} =\int\left[p_{\mathrm{data}}(\mathbf{x},\bm{\omega},\hat{\mathbf{ x}})2(\hat{D}(\hat{\mathbf{x}},\mathbf{x})-\bm{\omega}^{+})+p_{G}(\mathbf{x},\bm{ \omega},\hat{\mathbf{x}})2(\hat{D}(\hat{\mathbf{x}},\mathbf{x})-\bm{\omega}^{ -})\right]\mathrm{d}\bm{\omega}=0\] \[\Rightarrow\hat{D}^{*}(\hat{\mathbf{x}},\mathbf{x}) =\frac{\int p_{\mathrm{data}}(\mathbf{x},\bm{\omega},\hat{ \mathbf{x}})\bm{\omega}^{+}\mathrm{d}\bm{\omega}+\int p_{G}(\mathbf{x},\bm{ \omega},\hat{\mathbf{x}})\bm{\omega}^{-}\mathrm{d}\bm{\omega}}{p_{\mathrm{data }}(\mathbf{x},\hat{\mathbf{x}})+p_{G}(\mathbf{x},\hat{\mathbf{x}})}.\]

**Theorem 1**.: _Assume that \(\bm{\omega}^{+}=-\bm{\omega}^{-}=\bm{c}\) is constant, under the optimal self-supervised discriminator \(\hat{D}^{*}\), optimizing the self-supervised task for the generator \(G\) is equivalent to:_

\[\min_{G}4c\cdot M_{\mathrm{AH}}(p_{\mathrm{data}}(\mathbf{x},\hat{\mathbf{x}} )\|p_{G}(\mathbf{x},\hat{\mathbf{x}})),\] (10)

_where \(c=\|\mathbf{c}\|_{2}^{2}\) is constant and \(M_{\mathrm{AH}}\) is the arithmetic \(-\) harmonic mean divergence [37], of which the minimum is achieved if and only if \(p_{G}(\mathbf{x},\hat{\mathbf{x}})=p_{\mathrm{data}}(\mathbf{x},\hat{\mathbf{ x}})\Rightarrow p_{G}(\mathbf{x})=p_{\mathrm{data}}(\mathbf{x})\)._

Proof.: The objective function of the self-supervised task for the generator can be written as follows:

\[\mathbb{E}_{\mathbf{z},\bm{\omega}}\left[\|\hat{D}(T(G(\mathbf{z} );\bm{\omega}),G(\mathbf{z}))-\bm{\omega}^{+}\|_{2}^{2}\right]-\mathbb{E}_{ \mathbf{z},\bm{\omega}}\left[\|\hat{D}(T(G(\mathbf{z});\bm{\omega}),G( \mathbf{z}))-\bm{\omega}^{-}\|_{2}^{2}\right]\] \[=\iiint p_{G}(\mathbf{x},\hat{\mathbf{x}})\left[\|\frac{\bm{c}(p_ {\mathrm{data}}(\mathbf{x},\hat{\mathbf{x}})-p_{G}(\mathbf{x},\hat{\mathbf{x}} ))}{p_{\mathrm{data}}(\mathbf{x},\hat{\mathbf{x}})+p_{G}(\mathbf{x},\hat{ \mathbf{x}})}-\bm{c}\|_{2}^{2}-\|\frac{\bm{c}(p_{\mathrm{data}}(\mathbf{x}, \hat{\mathbf{x}})-p_{G}(\mathbf{x},\hat{\mathbf{x}}))}{p_{\mathrm{data}}( \mathbf{x},\hat{\mathbf{x}})+p_{G}(\mathbf{x},\hat{\mathbf{x}})}+\bm{c}\|_{2 }^{2}\right]\mathrm{d}\mathbf{x}\mathrm{d}\hat{\mathbf{x}}\] \[=c\cdot\iint p_{G}(\mathbf{x},\hat{\mathbf{x}})\left[\|\frac{p_ {\mathrm{data}}(\mathbf{x},\hat{\mathbf{x}})-p_{G}(\mathbf{x},\hat{\mathbf{x}} )}{p_{\mathrm{data}}(\mathbf{x},\hat{\mathbf{x}})+p_{G}(\mathbf{x},\hat{ \mathbf{x}})}-1\|_{2}^{2}-\|\frac{p_{\mathrm{data}}(\mathbf{x},\hat{\mathbf{x}} )-p_{G}(\mathbf{x},\hat{\mathbf{x}})}{p_{\mathrm{data}}(\mathbf{x},\hat{ \mathbf{x}})+p_{G}(\mathbf{x},\hat{\mathbf{x}})}+1\|_{2}^{2}\right]\mathrm{d} \mathbf{x}\mathrm{d}\hat{\mathbf{x}}\] \[=c\cdot\iint p_{G}(\mathbf{x},\hat{\mathbf{x}})\left[\|\frac{2p_{G} (\mathbf{x},\hat{\mathbf{x}})}{p_{\mathrm{data}}(\mathbf{x},\hat{\mathbf{x}})+p_{G }(\mathbf{x},\hat{\mathbf{x}})}\|_{2}^{2}-\|\frac{2p_{\mathrm{data}}(\mathbf{x}, \hat{\mathbf{x}})}{p_{\mathrm{data}}(\mathbf{x},\hat{\mathbf{x}})+p_{G}( \mathbf{x},\hat{\mathbf{x}})}\|_{2}^{2}\right]\mathrm{d}\mathbf{x}\mathrm{d} \hat{\mathbf{x}}\] \[=4c\cdot\iint p_{G}(\mathbf{x},\hat{\mathbf{x}})\left[\frac{p_{G} (\mathbf{x},\hat{\mathbf{x}})^{2}-p_{\mathrm{data}}(\mathbf{x},\hat{\mathbf{x}} )^{2}}{(p_{\mathrm{data}}(\mathbf{x},\hat{\mathbf{x}})+p_{G}(\mathbf{x}, \hat{\mathbf{x}}))^{2}}\right]\mathrm{d}\mathbf{x}\mathrm{d}\hat{\mathbf{x}}\] \[=4c\cdot\iint p_{G}(\mathbf{x},\hat{\mathbf{x}})\left[\frac{p_{G} (\mathbf{x},\hat{\mathbf{x}})-p_{\mathrm{data}}(\mathbf{x},\hat{\mathbf{x}} )}{p_{\mathrm{data}}(\mathbf{x},\hat{\mathbf{x}})+p_{G}(\mathbf{x},\hat{ \mathbf{x}})}\right]\mathrm{d}\mathbf{x}\mathrm{d}\hat{\mathbf{x}}\] \[=4c\cdot M_{\mathrm{AH}}(p_{\mathrm{data}}(\mathbf{x},\hat{\mathbf{ x}})\|p_{G}(\mathbf{x},\hat{\mathbf{x}})),\]

where \(c=\|\mathbf{

**Corollary 1**.: _The following equality and inequality hold for the AHM divergence:_

* \(M_{\rm AH}(p_{\rm data}(\mathbf{x},\hat{\mathbf{x}})\|p_{G}(\mathbf{x},\hat{ \mathbf{x}}))+M_{\rm AH}(p_{G}(\mathbf{x},\hat{\mathbf{x}})\|p_{\rm data}( \mathbf{x},\hat{\mathbf{x}}))=\Delta(p_{\rm data}(\mathbf{x},\hat{\mathbf{x}}) \|p_{G}(\mathbf{x},\hat{\mathbf{x}}))\)__
* \(M_{\rm AH}(p_{\rm data}(\mathbf{x},\hat{\mathbf{x}})\|p_{G}(\mathbf{x},\hat{ \mathbf{x}}))=1-W(p_{\rm data}(\mathbf{x},\hat{\mathbf{x}})\|p_{G}(\mathbf{x}, \hat{\mathbf{x}}))\leq 1\)__

_where \(\Delta\) is the Le Cam (LC) divergence [23], and \(W\) is the harmonic mean divergence [37]._

Proof.: We first prove the first corollary:

\[M_{\rm AH}(p_{\rm data}(\mathbf{x},\hat{\mathbf{x}})\|p_{G}( \mathbf{x},\hat{\mathbf{x}}))\] \[\leq M_{\rm AH}(p_{\rm data}(\mathbf{x},\hat{\mathbf{x}})\|p_{G}( \mathbf{x},\hat{\mathbf{x}}))+M_{\rm AH}(p_{G}(\mathbf{x},\hat{\mathbf{x}}) \|p_{\rm data}(\mathbf{x},\hat{\mathbf{x}}))\] \[=\iint p_{G}(\mathbf{x},\hat{\mathbf{x}})\frac{p_{G}(\mathbf{x}, \hat{\mathbf{x}})-p_{\rm data}(\mathbf{x},\hat{\mathbf{x}})}{p_{\rm data}( \mathbf{x},\hat{\mathbf{x}})+p_{G}(\mathbf{x},\hat{\mathbf{x}})}\mathrm{d} \mathbf{x}\mathrm{d}\hat{\mathbf{x}}+\iint p_{\rm data}(\mathbf{x},\hat{ \mathbf{x}})\frac{p_{\rm data}(\mathbf{x},\hat{\mathbf{x}})-p_{G}(\mathbf{x}, \hat{\mathbf{x}})}{p_{G}(\mathbf{x},\hat{\mathbf{x}})+p_{\rm data}(\mathbf{x}, \hat{\mathbf{x}})}\mathrm{d}\mathbf{x}\mathrm{d}\hat{\mathbf{x}}\] \[=\iint p_{G}(\mathbf{x},\hat{\mathbf{x}})\frac{p_{G}(\mathbf{x}, \hat{\mathbf{x}})-p_{\rm data}(\mathbf{x},\hat{\mathbf{x}})}{p_{\rm data}( \mathbf{x},\hat{\mathbf{x}})+p_{G}(\mathbf{x},\hat{\mathbf{x}})}\mathrm{d} \mathbf{x}\mathrm{d}\hat{\mathbf{x}}-\iint p_{\rm data}(\mathbf{x},\hat{ \mathbf{x}})\frac{p_{G}(\mathbf{x},\hat{\mathbf{x}})-p_{\rm data}(\mathbf{x}, \hat{\mathbf{x}})}{p_{G}(\mathbf{x},\hat{\mathbf{x}})+p_{\rm data}(\mathbf{x}, \hat{\mathbf{x}})}\mathrm{d}\mathbf{x}\mathrm{d}\hat{\mathbf{x}}\] \[=\iint\frac{(p_{G}(\mathbf{x},\hat{\mathbf{x}})-p_{\rm data}( \mathbf{x},\hat{\mathbf{x}}))^{2}}{p_{\rm data}(\mathbf{x},\hat{\mathbf{x}})+ p_{G}(\mathbf{x},\hat{\mathbf{x}})}\mathrm{d}\mathbf{x}\mathrm{d}\hat{\mathbf{x}}\] \[=\Delta(p_{\rm data}(\mathbf{x},\hat{\mathbf{x}})\|p_{G}(\mathbf{x},\hat{\mathbf{x}})),\]

where \(0\leq\Delta(p_{\rm data}(\mathbf{x},\hat{\mathbf{x}})\|p_{G}(\mathbf{x},\hat{ \mathbf{x}}))\leq 2\) is the Le Cam (LC) divergence [23].

The following proves the second corollary:

\[0 \leq M_{\rm AH}(p_{\rm data}(\mathbf{x},\hat{\mathbf{x}})\|p_{G} (\mathbf{x},\hat{\mathbf{x}}))\] \[=\iint p_{G}(\mathbf{x},\hat{\mathbf{x}})\frac{p_{G}(\mathbf{x}, \hat{\mathbf{x}})-p_{\rm data}(\mathbf{x},\hat{\mathbf{x}})}{p_{\rm data}( \mathbf{x},\hat{\mathbf{x}})+p_{G}(\mathbf{x},\hat{\mathbf{x}})}\mathrm{d} \mathbf{x}\mathrm{d}\hat{\mathbf{x}}\] \[=\iint p_{G}(\mathbf{x},\hat{\mathbf{x}})\left[1-\frac{2p_{\rm data }(\mathbf{x},\hat{\mathbf{x}})}{p_{\rm data}(\mathbf{x},\hat{\mathbf{x}})+p_{G }(\mathbf{x},\hat{\mathbf{x}})}\right]\mathrm{d}\mathbf{x}\mathrm{d}\hat{ \mathbf{x}}\] \[=1-\iint\frac{2p_{\rm data}(\mathbf{x},\hat{\mathbf{x}})p_{G}( \mathbf{x},\hat{\mathbf{x}})}{p_{\rm data}(\mathbf{x},\hat{\mathbf{x}})+p_{G}( \mathbf{x},\hat{\mathbf{x}})}\mathrm{d}\mathbf{x}\mathrm{d}\hat{\mathbf{x}}\] \[=1-W(p_{\rm data}(\mathbf{x},\hat{\mathbf{x}})\|p_{G}(\mathbf{x},\hat{\mathbf{x}}))\leq 1,\]

where \(0\leq W(p_{\rm data}(\mathbf{x},\hat{\mathbf{x}})\|p_{G}(\mathbf{x},\hat{ \mathbf{x}}))\leq 1\) is the well known harmonic mean divergence [37].

\begin{table}
\begin{tabular}{l l l l l l l l l} \hline \hline \multirow{4}{*}{} & \multirow{2}{*}{Method} & \multirow{2}{*}{Method} & \multicolumn{2}{c}{100\% training data} & \multicolumn{2}{c}{20\% training data} & \multicolumn{2}{c}{10\% training data} \\ \cline{3-8}  & & IS (\(\uparrow\)) & FID (\(\downarrow\)) & IS (\(\uparrow\)) & FID (\(\downarrow\)) & IS (\(\uparrow\)) & FID (\(\downarrow\)) \\ \hline \multirow{4}{*}{
\begin{tabular}{} \end{tabular} } & Baseline & 9.29\({}_{\pm 0.02}\) & 8.48\({}_{\pm 0.13}\) & 8.84\({}_{\pm 0.12}\) & 15.14\({}_{\pm 0.47}\) & **8.80\({}_{\pm 0.01}\)** & 20.60\({}_{\pm 0.13}\) \\  & SS (\(\lambda_{g}=0\)) & 9.28\({}_{\pm 0.06}\) & 8.30\({}_{\pm 0.12}\) & 8.86\({}_{\pm 0.09}\) & 13.96\({}_{\pm 0.19}\) & 8.62\({}_{\pm 0.09}\) & 20.94\({}_{\pm 0.42}\) \\  & SS & 9.29\({}_{\pm 0.07}\) & 8.24\({}_{\pm 0.10}\) & **8.98\({}_{\pm 0.04}\)** & 13.42\({}_{\pm 0.36}\) & 8.69\({}_{\pm 0.05}\) & 19.40\({}_{\pm 0.28}\) \\  & SS+ (\(\lambda_{g}=0\)) & 9.25\({}_{\pm 0.06}\) & 8.11\({}_{\pm 0.18}\) & 8.81\({}_{\pm 0.05}\) & 13.86\({}_{\pm 0.31}\) & 8.76\({}_{\pm 0.06}\) & 19.52\({}_{\pm 0.24}\) \\  & SS+ & 9.26\({}_{\pm 0.04}\) & 8.26\({}_{\pm 0.28}\) & 8.85\({}_{\pm 0.05}\) & 13.81\({}_{\pm 0.27}\) & 8.63\({}_{\pm 0.05}\) & 19.85\({}_{\pm 0.18}\) \\  & ASS (\(\lambda_{g}=0\)) & 9.12\({}_{\pm 0.05}\) & 8.90\({}_{\pm 0.07}\) & 8.82\({}_{\Loss Functions

AugSelf-GAN adopts all kinds of augmentations of DiffAugment as the self-supervised signals by default, thus the self-supervised loss functions of the discriminator and the generator (Equations (5) and (6)) actually each comprise three sub self-supervised loss functions. Specifically, for AugSelf-GAN that uses color, translation, and cutout as self-supervision, the objective functions are:

\[\min_{D,\hat{D}_{\mathrm{color}},\hat{D}_{\mathrm{translation}}, \hat{D}_{\mathrm{outout}}} \mathcal{L}_{D}^{\mathrm{da}}+\lambda_{d}\cdot\left(\mathcal{L}_{D_{ \mathrm{color}}}^{\mathrm{color}}+\mathcal{L}_{\hat{D}_{\mathrm{translation}}}^{ \mathrm{translation}}+\mathcal{L}_{\hat{D}_{\mathrm{cutout}}}^{\mathrm{cutout}} \right),\] (11) \[\min_{G}\mathcal{L}_{G}^{\mathrm{da}}+\lambda_{g}\cdot\left( \mathcal{L}_{G}^{\mathrm{color}}+\mathcal{L}_{G}^{\mathrm{translation}}+ \mathcal{L}_{G}^{\mathrm{cutout}}\right),\] (12)

where \(\mathcal{L}_{D_{\mathrm{color}}}^{\mathrm{color}}\), \(\mathcal{L}_{D_{\mathrm{translation}}}^{\mathrm{translation}}\), \(\mathcal{L}_{D_{\mathrm{cutout}}}^{\mathrm{cutout}}\), \(\mathcal{L}_{G}^{\mathrm{color}}\), \(\mathcal{L}_{G}^{\mathrm{translation}}\), and \(\mathcal{L}_{G}^{\mathrm{cutout}}\) are defined as:

\[\mathcal{L}_{D_{\mathrm{color}}}^{\mathrm{color}} =\mathbb{E}_{\mathbf{x},\boldsymbol{\omega}}\left[\|\hat{D}_{ \mathrm{color}}(T(\mathbf{x};\boldsymbol{\omega}),\mathbf{x})-\boldsymbol{ \omega}_{\mathrm{color}}^{+}\|_{2}^{2}\right]\] \[+\mathbb{E}_{\mathbf{z},\boldsymbol{\omega}}\left[\|\hat{D}_{ \mathrm{color}}(T(G(\mathbf{z});\boldsymbol{\omega}),G(\mathbf{z}))- \boldsymbol{\omega}_{\mathrm{color}}^{-}\|_{2}^{2}\right],\] (13) \[\mathcal{L}_{\hat{D}_{\mathrm{translation}}}^{\mathrm{translation}} =\mathbb{E}_{\mathbf{x},\boldsymbol{\omega}}\left[\|\hat{D}_{ \mathrm{translation}}(T(\mathbf{x};\boldsymbol{\omega}),\mathbf{x})- \boldsymbol{\omega}_{\mathrm{translation}}^{+}\|_{2}^{2}\right]\] \[+\mathbb{E}_{\mathbf{z},\boldsymbol{\omega}}\left[\|\hat{D}_{ \mathrm{translation}}(T(G(\mathbf{z});\boldsymbol{\omega}),G(\mathbf{z}))- \boldsymbol{\omega}_{\mathrm{translation}}^{-}\|_{2}^{2}\right],\] (14) \[\mathcal{L}_{\hat{D}_{\mathrm{cutout}}}^{\mathrm{cutout}} =\mathbb{E}_{\mathbf{x},\boldsymbol{\omega}}\left[\|\hat{D}_{ \mathrm{cutout}}(T(\mathbf{x};\boldsymbol{\omega}),\mathbf{x})-\boldsymbol{ \omega}_{\mathrm{cutout}}^{+}\|_{2}^{2}\right]\] \[+\mathbb{E}_{\mathbf{z},\boldsymbol{\omega}}\left[\|\hat{D}_{ \mathrm{cutout}}(T(G(\mathbf{z});\boldsymbol{\omega}),G(\mathbf{z}))- \boldsymbol{\omega}_{\mathrm{cutout}}^{-}\|_{2}^{2}\right],\] (15) \[\mathcal{L}_{G}^{\mathrm{color}} =\mathbb{E}_{\mathbf{z},\boldsymbol{\omega}}\left[\|\hat{D}_{ \mathrm{color}}(T(G(\mathbf{z});\boldsymbol{\omega}),G(\mathbf{z}))- \boldsymbol{\omega}_{\mathrm{color}}^{+}\|_{2}^{2}\right]\] \[-\mathbb{E}_{\mathbf{z},\boldsymbol{\omega}}\left[\|\hat{D}_{ \mathrm{color}}(T(G(\mathbf{z});\boldsymbol{\omega}),G(\mathbf{z}))- \boldsymbol{\omega}_{\mathrm{color}}^{-}\|_{2}^{2}\right],\] (16) \[\mathcal{L}_{G}^{\mathrm{translation}} =\mathbb{E}_{\mathbf{z},\boldsymbol{\omega}}\left[\|\hat{D}_{ \mathrm{translation}}(T(G(\mathbf{z});\boldsymbol{\omega}),G(\mathbf{z}))- \boldsymbol{\omega}_{\mathrm{translation}}^{+}\|_{2}^{2}\right]\] \[-\mathbb{E}_{\mathbf{z},\boldsymbol{\omega}}\left[\|\hat{D}_{ \mathrm{translation}}(T(G(\mathbf{z});\boldsymbol{\omega}),G(\mathbf{z}))- \boldsymbol{\omega}_{\mathrm{translation}}^{-}\|_{2}^{2}\right],\] (17) \[\mathcal{L}_{G}^{\mathrm{cutout}} =\mathbb{E}_{\mathbf{z},\boldsymbol{\omega}}\left[\|\hat{D}_{ \mathrm{cutout}}(T(G(\mathbf{z});\boldsymbol{\omega}),G(\mathbf{z}))- \boldsymbol{\omega}_{\mathrm{cutout}}^{+}\|_{2}^{2}\right]\] \[-\mathbb{E}_{\mathbf{z},\boldsymbol{\omega}}\left[\|\hat{D}_{ \mathrm{cutout}}(T(G(\mathbf{z});\boldsymbol{\omega}),G(\mathbf{z}))- \boldsymbol{\omega}_{\mathrm{cutout}}^{-}\|_{2}^{2}\right],\] (18)

where \(\hat{D}_{\mathrm{color}}=\varphi_{\mathrm{color}}\circ\phi\), \(\hat{D}_{\mathrm{translation}}=\varphi_{\mathrm{translation}}\circ\phi\), and \(\hat{D}_{\mathrm{cutout}}=\varphi_{\mathrm{cutout}}\circ\phi\) share the backbone \(\phi\) but differ in the heads \(\varphi_{\mathrm{color}}\), \(\varphi_{\mathrm{translation}}\), or \(\varphi_{\mathrm{cutout}}\), respectively. For the simplicity of notations, we write Equations (5) and (6) in the main text as the objective function.

## Appendix C Ablation Studies

Self-supervised tasks.We introduce two non-adversarial self-supervised tasks for comparison. The first version is that the discriminator only learns self-supervision on real data, defined as:

\[\mathcal{L}_{\hat{D}}^{\mathrm{ss}}=\mathbb{E}_{\mathbf{x}\sim p_{\mathrm{data} }(\mathbf{x}),\boldsymbol{\omega}\sim p(\boldsymbol{\omega})}\left[\|\hat{D}( T(\mathbf{x};\boldsymbol{\omega}),\mathbf{x})-\boldsymbol{\omega}\|_{2}^{2} \right].\] (19)

The second version is that the discriminator learns self-supervised tasks on both real and generated data simultaneously, given by:

\[\mathcal{L}_{\hat{D}}^{\mathrm{ss}}=\mathbb{E}_{\mathbf{x},\boldsymbol{\omega}} \left[\|\hat{D}(T(\mathbf{x};\boldsymbol{\omega}),\mathbf{x})-\boldsymbol{ \omega}\|_{2}^{2}\right]+\mathbb{E}_{\mathbf{z},\boldsymbol{\omega}}\left[\| \hat{D}(T(G(\mathbf{z});\boldsymbol{\omega}),G(\mathbf{z}))-\boldsymbol{ \omega}\|_{2}^{2}\right].\] (20)

For both versions, the generator is encouraged to produce augmentation-recognizable data, as follows:

\[\mathcal{L}_{G}^{\mathrm{ss}}=\mathbb{E}_{\mathbf{z}\sim p(\boldsymbol{\omega}), \boldsymbol{\omega}\sim p(\boldsymbol{\omega})}\left[\|\hat{D}(T(G(\mathbf{z}); \boldsymbol{\omega}),G(\mathbf{z}))-\boldsymbol{\omega}\|_{2}^{2}\right].\] (21)

According to the self-supervised task, we denote different approaches as SS (Equations (19) and (21)), SS+ (Equations (20) and (21)), and ASS (Equations (5) and (6), short for adversarial self-supervised learning, i.e., the proposed AugSelf-GAN).

[MISSING_PAGE_FAIL:16]

[MISSING_PAGE_FAIL:17]

[MISSING_PAGE_EMPTY:18]

Figure 9: Comparison of random generated images between StyleGAN2 + DiffAugment and AugSelf-StyleGAN2+ on FFHQ with different amounts of training data.

Figure 10: Comparison of randomly generated images between StyleGAN2 + DiffAugment and AugSelf-StyleGAN2+ on LSUN-Cat with different amounts of training data.