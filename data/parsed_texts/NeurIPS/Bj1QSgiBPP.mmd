# Participatory Personalization in Classification

 Hailey Joren

UC San Diego

Chirag Nagpal

Google Research

Katherine Heller

Google Research

Berk Ustun

UC San Diego

###### Abstract

Machine learning models are often personalized with information that is protected, sensitive, self-reported, or costly to acquire. These models use information about people but do not facilitate nor inform their _consent_. Individuals cannot opt out of reporting personal information to a model, nor tell if they benefit from personalization in the first place. We introduce a family of classification models, called _participatory systems_, that let individuals opt into personalization at prediction time. We present a model-agnostic algorithm to learn participatory systems for personalization with categorical group attributes. We conduct a comprehensive empirical study of participatory systems in clinical prediction tasks, benchmarking them with common approaches for personalization and imputation. Our results demonstrate that participatory systems can facilitate and inform consent while improving performance and data use across all groups who report personal data.

## 1 Introduction

Machine learning models routinely assign predictions to _people_ - be it to screen a patient for a mental illness [35], their risk of mortality in an ICU [44], or their likelihood of responding to treatment [1]. Many models in such applications are designed to target heterogeneous subpopulations using features that explicitly encode personal information. Typically, models are _personalized_ with categorical attributes that define groups [i.e., "categorization" as per 27]. In medicine, for example, clinical prediction models use _group attributes_ that are _protected_ (e.g., sex in the ASCVD Score for cardiovascular disease), _sensitive_ (e.g., HIV_status in the VA COVID-19 Mortality Score), _self-reported_ (e.g., alcohol_use in the HAS-BLED Score for Major Bleeding Risk), or _costly_ to acquire (e.g., leukocytosis in the Alvarado Appendicitis Score).

Individuals expect the right to opt out of providing personal data and the ability to understand how it will be used [see, e.g., personal data guidelines in GDPR, OECD privacy guidelines 26, 40]. In many contexts, personalized models do not provide such functionality: individuals cannot opt out of reporting data used to personalize their predictions nor tell if it would improve their predictions. At the same time, practitioners assume that data available for training will be available at inference time. In practice, this assumption has led to a proliferation of models that use information that individuals may be unwilling or unable to report at prediction time [see e.g., the Denver HIV Risk Score 29, which asks patients to report age, gender, race, and sexual_practices]. In tasks where individuals self-report, they may not voluntarily report information that could improve their predictions or may report incorrect information.

The broader need to facilitate and inform consent in personalized prediction tasks stems from the fact that personalization may not improve performance for each group that reports personal data [51]. In practice, a personalized model can perform _worse_ or the same as a _generic model_ fit without personal information for a group with specific characteristics. Such models violate the implicit promise of personalization as individuals report personal information without receiving a tailored performance gain in return. These instances of "worsenalization" are prevalent, hard to detect, and hard to resolve [see 42, 51]. However, they would be resolved if individuals could opt out of personalization and understand its expected gains (see Fig. 1).

This work introduces a family of classification models that operationalize informed consent called _participatory systems_. Participatory systems _facilitate consent_ by allowing individuals to report personal information at prediction time. Moreover, they _inform consent_ by showing how reporting personal information will change their predictions. Models that facilitate consent operate as markets in which individuals trade personal information for performance gains. This work seeks to develop systems that perform as well as possible both when individuals opt-in - to incentivize voluntary reporting - and when they opt out - to safeguard against abstention. Our main contributions include:

1. We present a variety of participatory systems that provide opportunities for individuals to make informed decisions about data provision. Each system ensures that individuals who opt into personalization will receive the most accurate possible predictions possible.
2. We develop a model-agnostic algorithm to learn participatory systems. Our approach can produce a variety of systems that promote participation and minimize data use in deployment.
3. We conduct a comprehensive study of participatory systems in real-world clinical prediction tasks. The results show how our approach can facilitate and inform consent in a way that improves performance and minimizes data use.
4. We provide a Python library to build and evaluate participatory systems.

Related WorkParticipatory systems support modern principles of responsible data use articulated in OECD privacy guidelines [40], the GDPR [26], and the California Consumer Privacy Act [16]. These include: _informed consent_, i.e., that data should be collected with the data subject's consent; and _collection limitation_, i.e., that data collected should be restricted to only what is necessary. These principles stem from extensive work on the right to data privacy [33]. They are motivated, in part, by research showing that individuals care deeply about their ability to control personal data [4, 8, 10] but differ considerably in their desire or capacity to share it [see e.g. 5, 7, 9, 17, 18, 39, 41]. Our proposed systems let decision subjects report personal data in exchange for performance, which is aligned with principles articulated in recent work on data privacy [13, 46] and related to work in designing incentive-compatible prediction functions [24].

We consider models that are personalized with categorical attributes that encode personal characteristics [i.e., "categorization" rather than "individualization" as per 27]. Modern techniques for learning with categorical attributes [see e.g., 2, 48] use them to improve performance at a population level - e.g., by accounting for higher-order interaction effects [14, 38, 58] or recursive partitioning [11, 12, 15, 25]. Our methods can be used to achieve these goals in tasks where models use features that are optional or costly to acquire [see e.g., 6, 7, 52, 61].

Our work is related to algorithmic fairness in that we seek to improve model performance at a group level. Recent work shows that personalization with group attributes does not uniformly improve performance and can reduce accuracy at a group level [see 42, 51, 57]. Our systems can safeguard against such instances of "worsenalization" by informing users of the gains in reporting and allowing

Figure 1: Classification task where participation improves accuracy and minimizes data use. We consider a dataset that has no features, two group attributes \(\mathcal{G}=\texttt{sex}\times\texttt{age},n^{-}=51\) negative examples and \(n^{+}=50\) positive examples. Here, the best personalized linear model \(h:\mathcal{X}\times\mathcal{G}\rightarrow\mathcal{Y}\) with a one-hot encoding of \(\mathcal{G}\) makes 24 mistakes, and the best generic model \(h_{0}:\mathcal{X}\times\mathcal{Y}\) makes 50 mistakes as it predicts the majority class (\(-\)). Under traditional personalization, individuals report group membership to receive personalized predictions from \(h\). As shown, personalization benefits the population as a whole by reducing overall error from 50 to 24 (\(\Delta R_{\boldsymbol{g}}(h,h_{0})=26\)). However, personalization has a detrimental effect on [female,old], who receive less accurate predictions from the personalized model (\(\Delta R_{\boldsymbol{g}}(h,h_{0})=\ -24\)), and no effect on [male,young] who receive the same predictions from the personalized and generic models(\(\Delta R_{\boldsymbol{g}}(h,h_{0})=\ \texttt{!}\)). In a minimal participatory system, individuals _opt in_ to personalization, choosing to receive predictions from \(h\) or \(h_{0}\). Here, individuals in groups [female,old] and [male,young] opt out of personalization, leading to an overall error of 0 (\(\Delta R_{\boldsymbol{g}}(h,h_{0})=\ 50\)) and a reduction in unnecessary data collection (\(\varnothing\)).

them to opt out of reporting. This line of broad work complements research on preference-based fairness [22; 36; 57; 59; 62], on ensuring fairness across complex group structures [28; 30; 34], and promoting privacy across subpopulations [13; 53].

## 2 Participatory Systems

We consider a classification task where we personalize a model with categorical attributes. We start with a dataset \(\{(\bm{x}_{i},y_{i},\bm{g}_{i})\}_{i=1}^{n}\) where each example consists of a feature vector \(\bm{x}_{i}\in\mathbb{R}^{d}\), a label \(y_{i}\in\mathcal{Y}\), and a vector of \(m\) categorical attributes \(\bm{g}_{i}=[g_{i,1},\ldots,g_{i,m}]\in\mathcal{G}_{1}\times\ldots\times \mathcal{G}_{m}=\mathcal{G}\). We refer to \(\mathcal{G}\) as _group attributes_, and to \(\bm{g}_{i}\) as the _group membership_ of person \(i\). We use \(n_{\bm{g}}:=|\{i\,|\,\bm{g}_{i}=\bm{g}\}|\) denote the size of group \(\bm{g}\), and use \(|\mathcal{G}_{k}|\) to denote the number of categories for group attribute \(k\).

We use the dataset to train a personalized model \(h:\mathcal{X}\times\mathcal{G}\to\mathcal{Y}\) via empirical risk minimization with a loss function \(\ell:\mathcal{Y}\times\mathcal{Y}\to\mathbb{R}_{+}\). Given a model \(h\), we denote its empirical risk and true risk as \(\hat{R}(h)\) and \(R(h)\), respectively, and evaluate model performance at the group level. We denote the empirical risk and true risk of a model \(h\) on group \(\bm{g}\in\mathcal{G}\) as

\[R_{\bm{g}}(h(\cdot,\bm{g})):=\mathbb{E}\left[\ell\left(h(\cdot,\bm{g}),y) \right],\qquad\hat{R}_{\bm{g}}(h(\cdot,\bm{g})):=\frac{1}{n_{\bm{g}}}\sum_{i: \bm{g}_{i}=\bm{g}}\ell\left(h(\cdot,\bm{g}),y_{i}\right).\]

We consider tasks where every individual prefers more accurate predictions.

**Assumption 1**.: Given models \(h\) and \(h^{\prime}\), individuals in group \(\bm{g}\) prefer \(h\) to \(h^{\prime}\) when \(R_{\bm{g}}(h)<R_{\bm{g}}(h^{\prime})\).

Assumption 1 holds in settings where every individual prefers more accurate predictions - e.g., clinical prediction tasks such as screening or diagnosing illnesses [49; 56]. It does not hold in applications where some individuals prefer predictions that may be inaccurate - e.g., such as predicting the risk of organ failure for a transplant (see e.g., 43, for other "polar" clinical applications).

Operationalizing ConsentWe consider models where individuals consent to personalization by deciding whether or not to report their group attributes at prediction time. We let \(\varnothing\) denote an attribute that was not reported, and let \(\bm{r}_{i}=[r_{i,1},\ldots,r_{i,k}]\in\mathcal{R}\subseteq\mathcal{G}\times \bm{\varnothing}\). For example, a person with \(\bm{g}_{i}=[\texttt{female},\texttt{HIV}=+]\) would report \(\bm{r}_{i}=[\texttt{female},\varnothing]\) if they only disclose sex, and would report \(\bm{r}_{i}=\bm{\varnothing}:=[\varnothing,\varnothing]\) if they opt out of reporting entirely.

We associate each model with a set of _reporting options_\(\mathcal{R}\). A traditional model, which requires each person to report group attributes, has \(\mathcal{R}=\mathcal{G}\). A model where each person could report any subset of group attributes has \(\mathcal{R}=\mathcal{G}\times\bm{\varnothing}\). We represent individual decisions to opt into personalization at prediction time through a _reporting interface_ defined below.

**Definition 1**.: Given a personalized classification task with group attributes \(\mathcal{G}\), a _reporting interface_ is a tree \(T\) whose nodes represent attributes reported at prediction time. The tree is rooted at \(\operatorname{root}(T)=[\varnothing,\ldots,\varnothing]\) and branches as a person reports personal attributes. Given a node \(\bm{r}\), we denote its parent as \(\mathsf{pa}(\bm{r})\). Each parent-child pair represents a _reporting decision_, and the height of the tree represents the maximum number of reporting decisions.

**Definition 2**.: Given a personalized classification task with group attributes \(\mathcal{G}\), a _participatory system_ with reporting interface \(T\) is a prediction model \(f_{T}:\mathcal{X}\times\mathcal{R}\to\mathcal{Y}\) that obeys the following properties:

1. _Baseline Performance_: Opting out of personalization entirely guarantees the expected performance from a _generic model_ trained without group attributes \(h_{0}\in\operatorname{argmin}_{h\in\mathcal{H}}R(h)\). \[R_{\bm{r}}(f_{T}(\cdot,\bm{\varnothing}))=R_{\bm{r}}(h_{0})\text{ for all reporting groups }\bm{r}\in\mathcal{R}.\]
2. _Incentive Compatibility_: Opting into personalization improves expected performance \[R_{\bm{r}}(f_{T}(\cdot,\bm{r}))<R_{\bm{r}}(f_{T}(\cdot,\bm{r}^{\prime}))\text{ for all nested reporting groups }\bm{r},\bm{r}^{\prime}\in\mathcal{G}\times\bm{\varnothing}\text{ such that }\bm{r}^{\prime}=\mathsf{pa}(\bm{r}).\]

Here, the _Baseline Performance_ property ensures that individuals who choose not to share personal information receive the performance of a generic model - i.e., the most accurate model that could be trained without this information. This property also ensures individuals retain the ability to opt out of personalization - i.e., \(\boldsymbol{\varnothing}\in\mathcal{R}\). The _Incentive Compatibility_ property ensures that personalization will improve expected performance - i.e., when individuals report personal data, the system can effectively leverage that data to deliver more accurate predictions in expectation. Together, these properties lead to data minimization, as systems that obey these properties will not request data from a reporting group when it will not lead to an improvement in expected performance.

On Data Minimization via ImputationAn alternative approach to allow individuals to opt out of reporting personal information at prediction time is to impute their group membership. Imputation allows individuals to opt out of personalization but does not guarantee the accuracy of their predictions. As a result, individuals who opt out of personalization by reporting \(\boldsymbol{r}=\boldsymbol{\varnothing}\) may receive a less accurate prediction than they would receive from a generic model. In the best-case scenario where we could perfectly impute group membership, a group might be assigned better predictions from a generic model (see Fig. 1). In the worst case, imputation may be incorrect, leading to even more inaccurate predictions than those of the generic or personalized model. We highlight these effects on real-world datasets in our experiments in Section 4.

Characterizing System PerformanceOne of the key differences between traditional models and participatory systems is that their performance depends on individual reporting decisions. In what follows, we characterize the performance under a general model of individual disclosure. Given a participatory system \(f_{T}\), we assume that each individual reports personal information to maximize an individual utility function of the form:

\[u_{i}(\boldsymbol{r};f_{T})=b_{i}(\boldsymbol{r};f_{T})-c_{i}(\boldsymbol{r})\] (1)

Here, \(c_{i}(\cdot)\) and \(b_{i}(\cdot)\) denote the cost and benefit that individual \(i\) receives from reporting \(\boldsymbol{r}\) to \(f_{T}\) respectively. We assume that individuals incur no cost when they do not report any attributes such that \(c_{i}(\boldsymbol{\varnothing})=0\), and incur costs that increase monotonically with information disclosed such that \(c_{i}(\boldsymbol{r})\leq c_{i}(\boldsymbol{r}^{\prime})\) for \(\boldsymbol{r}\subseteq\boldsymbol{r}^{\prime}\). We assume that benefits increase monotonically with expected gains in true risk so that \(R_{\boldsymbol{r}}(f_{T}(\cdot,\boldsymbol{r}))<R_{\boldsymbol{r}}(f_{T}( \cdot,\boldsymbol{r}^{\prime}))\implies b_{i}(\boldsymbol{r},f_{T})>b_{i}( \boldsymbol{r}^{\prime},f_{T})\).

Figure 2: Participatory systems for a personalized classification task with group attributes \(\texttt{sex}\times\texttt{age}=[\texttt{male},\texttt{female}]\times[ \texttt{old},\texttt{young}]\). Each system allows a person to opt out of personalization by reporting \(\varnothing\) and informs their choice by showing the expected gains of personalization (e.g., +0.2% gain in accuracy). Systems minimize data use by removing reporting options that do not improve accuracy (see grey-striped boxes). Here, \([\texttt{young},\texttt{female}]\) is pruned in all systems as it leads to a gain \(\leq\) 0.0%.

In Fig. 3, we show how the system performance for each reporting group can change with respect to participation when we simulate individual disclosure decisions from a model that satisfies the assumptions listed above. When a personalized model \(h\) requires individuals to report information that reduces performance as in Fig. 1, individuals incur a cost of disclosure without receiving a benefit in return. In such cases, individuals who interact with a minimal system would opt out of worsenalization and receive more accurate predictions from a generic model, thereby improving the overall performance of the system.

We observe that the maximum utility that each individual can receive from a participatory system can only increase as we add more reporting options. Thus, flat and sequential systems should exhibit better performance than a minimal system.

Given a participatory system \(f_{T}\) with reporting options \(\mathcal{R}\), a participatory system \(f_{T^{\prime}}\) with more reporting options \(\mathcal{R}^{\prime}\supseteq\mathcal{R}\) can only improve performance, - i.e., \(R(f_{T^{\prime}})\leq R(f_{T})\). Similarly, the system with more reporting options can only improve utility, - i.e., \(u_{i}(\bm{r};f_{T^{\prime}})\geq u_{i}(\bm{r};f_{T})\) for all individuals \(i\).

## 3 Learning Participatory Systems

This section describes a model-agnostic algorithm to learn participatory systems that ensures incentive compatibility and baseline performance in deployment. We outline our procedure in Algorithm 1 to learn the three kinds of participatory systems in Fig. 2. The procedure takes as input a pool of candidate models \(\mathcal{M}\), a dataset for model assignment \(\mathcal{D}^{\text{assign}}\), and a dataset for pruning \(\mathcal{D}^{\text{prune}}\). It outputs a collection of participatory systems that obey the properties described in Definition 2 on test data. The procedure combines three routines to (1) generate viable reporting interfaces (Line 1); (2) assign models over the interface (Line 3); (3) prune the system to limit unnecessary data collection (Line 4). We present complete procedures for each routine in Appendix A and discuss them below.

Figure 3: Performance profile of participatory systems for the saps dataset for each intersectional group in the saps dataset. We plot out-of-sample performance for different levels of participation in the target population. We control participation by varying the reporting cost in a simulated model of individual disclosure. As shown, minimal and sequential systems outperform a generic model at a group level regardless of participation. In regimes where the cost of disclosure is low, participation is high. Consequently, a minimal system will achieve the same performance as a personalized model, and a sequential system will achieve the performance of the component model for each subgroup. We provide details and results in Appendix D.

Model PoolOur procedure takes as input a _pool of candidate models_\(\mathcal{M}\) to assign over a reporting interface. At a minimum, every pool should contain two models: a personalized model \(h\) for individuals who opt into personalization, and a generic model \(h_{0}\) for individuals who opt out of personalization. A single personalized model can perform unreliably across reporting groups due to differences in the data distribution or trade-offs between groups. Using a pool of models safeguards against these effects by drawing on models from different model classes that have been personalized using different techniques for each reporting group. By default, we include models trained specifically on the data for each reporting group, as such models can perform well on heterogeneous subgroups [51, 57].

Enumerating InterfacesWe call the ViableTrees routine in Line 1 to enumerate _viable_ reporting interfaces. We only call this routine for sequential systems since minimal and flat systems use a single reporting interface that is known a priori. ViableTrees takes as input a group attributes \(\mathcal{G}\) and a dataset \(\mathcal{D}^{\text{assign}}\). It returns all \(m\)-ary trees that obey constraints on sample size and reporting (e.g., users who report male should report age before HIV). By default, we only generate trees so that we have sufficient data to estimate gains at each node of the reporting interface1. In general, ViableTrees scales to tasks with \(\leq 8\) group attributes. Beyond this limit, one can reduce the enumeration size by specifying ordering constraints or a threshold number of trees to enumerate before stopping. For a task with three binary group attributes, \(\mathbb{T}\) contains \(24\) 3-ary trees of depth 3. Given a complete ordering of all \(3\) group attributes, however, \(\mathbb{T}\) would have \(1\) tree. We can also consider a greedy algorithm (see Appendix A.4), which may be practical for large-scale problems.

Footnote 1: For example, trees whose leaves contain at least one positive sample, one negative sample, and \(n_{\bm{r}}\geq d+1\) samples to avoid overfitting

Model AssignmentWe assign each reporting group a model using the AssignModels routine in Line 3. Given a reporting group \(\bm{r}\), we consider all models that could use any subset of group attributes in \(\bm{r}\). Thus, a group that reports age and sex could be assigned predictions from a model that requires age, sex, both, or neither. This implies that we can always assign the generic model to any reporting group, ensuring that the model at each node performs as well as the generic model on out-of-sample data (i.e., _baseline performance_ in Definition 2).

Pruning Reporting OptionsAssignModels may output trees that violate incentive compatibility by requesting personal information that fails to improve performance. This can happen when the routine assigns a model that performs equally well to nested reporting groups - see, e.g., Fig. 2 where the Flat system assigns \(h_{0}\) to \([\texttt{female},\varnothing]\) and \([\texttt{female},\texttt{young}]\).

We can avoid requesting data from reporting groups in such cases by calling the Prune routine in Line 4. This routine takes as input a participatory system \(f_{T}\) and a pruning dataset \(\mathcal{D}^{\text{prune}}\) and outputs a system \(f_{T^{\prime}}\) with a pruned interface \(T^{\prime}\subseteq T\). The routine uses a bottom-up pruning procedure that calls a one-sided hypothesis test at each node:

\[H_{0}:\Delta_{\bm{r}}(\bm{r},\text{pa}(\bm{r}))\leq 0\qquad H_{A}:\Delta_{\bm{ r}}(\bm{r},\text{pa}(\bm{r}))>0\]

The test checks if each reporting group \(\bm{r}\) receives more accurate predictions from the personalized model assigned to its current node or \(\bm{r}\) its parent \(\text{pa}(\bm{r})\). Here, \(H_{0}\) assumes a reporting group prefers the parent model. Thus, we reject \(H_{0}\) when we can reliably tell that \(f_{T}(\cdot,\bm{r})\) performs better for \(\bm{r}\) on the pruning dataset. The exact test should chosen based on the performance metric for the underlying prediction task. In general, we can use a bootstrap hypothesis test [20] and draw on more powerful tests for salient performance metrics [e.g., 19, 21, 50, for accuracy and AUC].

On ComputationOur approach provides several options to moderate the computation cost of training a pool of models. For example, we can train only two models and build a minimal system. Alternatively, we can also build a flat or sequential system using a limited number of models in the pool. In practice, the primary bottleneck when building participatory systems is _data_ rather than _compute_. Given a finite sample dataset, we are limited in the number of categorical attributes used for personalization. This is because we require a minimum number of samples for each intersectional group to train a personalized model and evaluate its performance. Given that the number of intersectional groups increases exponentially with each attribute, we quickly enter a regime where we cannot reliably evaluate model performance for assignment and pruning [see 42].

On CustomizationOur procedure allows practitioners to learn systems for prediction tasks by specifying the performance metric used in assignment and pruning. A suitable performance metric should represent the gains we would show users (e.g., error for a diagnosis, AUC for triage, ECE for risk assessment). Using a pool of models allows practitioners to optimize performance across groups, which translates to gains at the population level. For sequential systems, the procedure outputs all configurations, allowing practitioners to choose between systems based on criteria not known at training time. For example, one can swap the trees to use a system that always requests HIV status last. By default, we select the configuration that minimizes data collection across groups, such that the ordering of attributes results leads to the most significant number of data requests pruned.

## 4 Experiments

We benchmark participatory systems on real-world clinical prediction tasks. Our goal is to evaluate these approaches in terms of performance, data usage, and consent in applications where individuals have a low reporting cost. We include code to reproduce these results in an Python library.

### Setup

We consider six classification tasks for clinical decision support where we personalize a model with group attributes that are protected or sensitive (see Table 2 and Appendix B). Each task pertains to an application where we expect individuals to have a low cost of reporting and to report personal information when there is any expected gain. This is because the information used for personalization is readily available, relevant to the prediction task, and likely to be disclosed given legal protections related to the confidentiality of health data [4, 10, 54]. One exception is cardio_eicu and cardio_mimic, which are personalized based on race and ethnicity. 2 We split each dataset into a test sample (20% for evaluating out-of-sample performance) and a training sample (80% for training, pruning, assignment, and estimating gains to show users). We train three kinds of personalized models for each dataset:

Footnote 2: The use of race in clinical risk scores should be approached with caution [60]; participatory systems offer one way to safeguard against inappropriate use.

* _Static_: These models are personalized using a one-hot encoding of group attributes (1Hot), and a one-hot encoding of intersectional groups (mHot)
* _Imputed_: These are variants of static models where we impute the group membership for each person (KNN-1Hot, KNN-mHot). In practice, personalized systems with imputation will range between the performance for these systems and the performance of 1Hot and mHot.
* _Participatory_: These are participatory systems built using our approach. These include Minimal, a minimal system built from 1Hot and its generic counterpart; and Flat and Seq, flat and sequential systems built from 1Hot, mHot and their generic counterparts.

We train all models - personalized models and the components of participatory systems - from the same model class and evaluate them using the metrics in Table 1. We repeat the experiments four times, varying the model class (logistic regression, random forests) and the target performance metric (error rate for decision-making tasks, AUC for ranking tasks) to evaluate the sensitivity of our findings with respect to model classes and use cases.

### Discussion

We show results for logistic regression models and error rate in Table 2 and results for other model classes and classification tasks in Appendix C. In what follows, we discuss these results.

On PerformanceOur results in Table 2 show that participatory systems can improve performance across reporting groups. Here, Flat and Seq achieve the best overall performance on 6/6 datasets and improve the gains from personalization for every reporting group on 5/6 datasets. In contrast, traditional models improve overall performance while reducing performance at a group level (see rationality violations on five datasets for 1Hot, mHot). The performance benefits from participatory systems stem from (i) allowing users to opt out of these instances of "worsenalization" and (ii) assigning personalized predictions with multiple models. Using Table 2, we can measure the impact of (i) by comparing the performance of Minimal vs. 1Hot, and the impact of (ii) by comparing the performance of Minimal to Flat (or Seq). For example, on apnea, 1Hot exhibits a significant rationality violation for group [30_to_60,male], meaning they would have been better off with a generic model. By comparing the performance of 1Hot to Minimal, we see that allowing users to opt out of worsenalization reduces test error from 29.1% to 28.9%. By comparing the performance on Minimal to Flat and Seq, we see that using multiple models can further reduce test error from 28.9% to 24.1%.

On Informed ConsentOur results show how Flat and Seq systems can inform consent by allowing users to report a subset of group attributes (e.g., by including reporting options such as [30+, 2] or [2, HIV+]). Although both Flat and Seq systems allow for partial personalization, their capacity to inform consent differs. In a flat system, users may inaccurately gauge the marginal benefit of reporting an attribute by comparing the gains between reporting options. For example, in Fig. 4, users who are HIV positive would see a gain of \(3.7\%\) for reporting [2, HIV+], and \(16.7\%\) for reporting [30+, HIV+] and may mistakenly conclude that the gain of reporting age is \(16.7\%-3.7\%=13.0\%\). This estimate incorrectly presumes that the gains of \(3.7\%\) were distributed equally across age groups. Sequential systems directly inform users of the gains for partial reporting. In the sequential system, group [30+, HIV+] is informed that they would see a marginal gain of 21.5% for reporting age, while group [<30, HIV+] is informed they would see a marginal gain of reporting age of \(0.0\%\).

On Data MinimizationOur results show that participatory systems perform better across all groups while requesting less personal data on 6/6 datasets. For example, on cardio_eicu, Seq reduces error by \(11.3\%\) compared to 1Hot while requesting, on average, \(83.3\%\) of the data needed by 1Hot. In general, participatory systems can limit data use where personalization does not improve

\begin{table}
\begin{tabular}{l l l}
**Metric** & **Definition** & **Description** \\ \hline Overall & \(\sum\limits_{g\in\mathcal{G}}\frac{n_{g}}{n}R_{g}(h_{g})\) & Population-level performance of a personalized system/model, computed as a weighted average over all groups \\ \hline Overall Gain & \(\sum\limits_{g\in\mathcal{G}}\frac{n_{g}}{n}\hat{\Delta}_{g}(\bm{g},\bm{ \varnothing})\) & Population-level gain in performance of a personalized system/model over its generic counterpart \\ \hline Group Gains & \(\min\limits_{g\in\mathcal{G}}/\max\limits_{g\in\mathcal{G}}\hat{\Delta}_{g}( \bm{g},\bm{\varnothing})\) & Range of group-level gains of a personalized system/model over its generic counterpart across all groups \\ \hline Rationality & \(\sum\limits_{g\in\mathcal{G}}\left|\text{\sc{[reject}}\ H_{0}\right|\) & Number of rationality violations detected using a bootstrap test with 100 resamples at a significance of 10\% where \(H_{0}:\Delta_{g}(\bm{g},\bm{\varnothing})\geq 0\). \\ \hline Imputation & \(\min\limits_{g\in\mathcal{G}}\hat{\Delta}_{g}(\bm{g},\bm{g}^{{}^{\prime}})\) & Worst-case loss in performance due to incorrect imputation. This metric can only be computed for static models \\ \hline Options & \(\frac{\left|\mathcal{R}\left|-\left|\mathcal{R}\left(h\right)\right|\right.}{ \left|\mathcal{R}\right|}\) & Proportion of reporting options pruned from a system/model. Here, \(\mathcal{R}\) denotes all reporting options and \(\mathcal{R}(h)\) denotes those after \(h\) is pruned \\ \hline Data Use & \(\sum\limits_{g\in\mathcal{G}}\frac{n_{g}}{n}\frac{\text{\sc{mean}}(\bm{g},\bm{ g})}{\text{\sc{mean}}(\bm{g})}\) & Proportion of group attributes requested by \(h\) from each group, averaged over all groups in \(\mathcal{G}\) \\ \end{tabular}
\end{table}
Table 1: Metrics used to evaluate performance, data use, and consent of personalized models and systems. We report performance on a held-out test sample. We assume that individuals report group membership to static models, do not report group membership to imputed models, and only report to participatory systems when informed that it would lead to a strictly positive gain, as computed on the validation set in the training sample.

[MISSING_PAGE_FAIL:9]

model that exhibits "worsenalization" in Fig. 1. Even if one could correctly impute the group membership for every person, individuals may receive more accurate predictions from a generic model \(h_{0}\). In practice, imputation is imperfect - as individuals who opt out of reporting their group membership to a personalized model may be assigned "worse" predictions because they are imputed the group membership of a different group. In such cases, opting out may be beneficial, making it difficult for model developers to promote participation while informing consent. Our results highlight the prevalence of these effects in practice. For example, on cardio_eicu the estimated "risk of imputation" is \(-4.6\%\), indicating that every intersectional group can experience an increase of \(4.6\%\) in the error rate as a result of incorrect imputation. The results for KNN-1Hot show that this potential harm can be realized in practice using KNN-imputation, as we find that the imputed system leads to rationality violations on 5/6 datasets.

## 5 Concluding Remarks

We introduced a new family of classification models that allow individuals to report personal data at prediction time. Our work focuses on personalization with group attributes; our approach could be used to facilitate and inform consent in a broader class of prediction tasks. In such cases, the key requirement for building a participatory system is that we can reliably estimate the gains of personalization for each person who reports personal data.

Our results show that participatory systems can inform consent while improving performance and reducing data use across groups. Reaping these benefits in practice will hinge on the ability to effectively inform decision subjects on the impact of their reporting decisions. [4]. Even as there may be good "default practices" for what kind of information we should show decision subjects, practitioners should tailor this information to the application and target audience [23].

One common concern in using a participatory system arises when practitioners wish to collect data from a model in deployment to improve its performance in the future. In practice, a participatory system can thwart data collection in such settings by allowing individuals to opt out. In such cases, we would note that this issue should be resolved in a way that is aligned with the principle of _purpose specification_[40]. If the goal of data collection is to improve a model, then individuals could always be asked to report information voluntarily for this purpose. If the goal of data collection is to personalize predictions, then individuals should be able to opt out, especially when it may lead to worse performance.

Figure 4: Participatory systems for the saps dataset. These models predict ICU mortality for groups defined by \(\mathcal{G}=\texttt{HIV}\times\texttt{age}=\texttt{[+,-]}\times\texttt{[<30, 30+]}\) using logistic regression component models. Here, \(h_{0}\) is a generic model, \(h_{1}\) is a 1Hot model fit with a one-hot encoding of \(\mathcal{G}\), and \(h_{2}\cdots h_{m}\) are 1Hot and mHot models fit for each reporting group. We show the gains of each reporting option above each box and highlight pruned options in grey. For example, in Seq, the group (HIV+, 30+) sees an estimated \(21.5\%\) error reduction after reporting HIV if they report age. In contrast, the group (HIV+, <30) sees no gain from reporting age in addition to HIV status, so this option is pruned.

## Acknowledgements

We thank the following individuals for helpful discussions: Taylor Joren, Sanmi Koyejo, Charlie Marx, Julian McAuley, and Nisarg Shah. This work was supported by funding from the National Science Foundation IIS 2040880, the NIH Bridge2AI Center Grant U54HG012510, and an Amazon Research Award.

## References

* [1] Abajian, Aaron, Nikitha Murali, Lynn Jeanette Savic, Fabian Max Laage-Gaupp, Nariman Nezami, James S Duncan, Todd Schlachter, MingDe Lin, Jean-Francois Geschwind, and Julius Chapiro. Predicting treatment response to intra-arterial therapies for hepatocellular carcinoma with the use of supervised machine learning--an artificial intelligence concept. _Journal of Vascular and Interventional Radiology_, 29(6):850-857, 2018.
* [2] Agresti, Alan. _An introduction to categorical data analysis_. John Wiley & Sons, 2018.
* [3] Allyn, Jerome, Cyril Ferdynus, Michel Bohrer, Cecile Dalban, Dorothee Valance, and Nicolas Allou. Simplified acute physiology score ii as predictor of mortality in intensive care units: a decision curve analysis. _PloS one_, 11(10):e0164828, 2016.
* [4] Anderson, Catherine L and Ritu Agarwal. The digitization of healthcare: boundary risks, emotion, and consumer willingness to disclose personal health information. _Information Systems Research_, 22(3):469-490, 2011.
* [5] Arellano, April Moreno, Wenrui Dai, Shuang Wang, Xiaoqian Jiang, and Lucila Ohno-Machado. Privacy policy and technology in biomedical data science. _Annual review of biomedical data science_, 1:115, 2018.
* [6] Atan, Onur, William Whoiles, and Mihaela Schaar. Data-driven online decision making with costly information acquisition. _Arxiv_, 02 2016.
* [7] Auer, Peter, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement learning. _Advances in neural information processing systems_, 21, 2008.
* [8] Auxier, Brooke, Lee Rainie, Monica Anderson, Andrew Perrin, Madhu Kumar, and Erica Turner. Americans and privacy: Concerned, confused and feeling lack of control over their personal information. _Pew Research Center: Internet, Science and Tech_, 2019.
* [9] Awad, Naveen Farag and Mayuram S Krishnan. The personalization privacy paradox: an empirical evaluation of information transparency and the willingness to be profiled online for personalization. _MIS quarterly_, pages 13-28, 2006.
* [10] Bansal, Gaurav, David Gefen, et al. The impact of personal dispositions on information sensitivity, privacy concern and trust in disclosing health information online. _Decision support systems_, 49(2):138-150, 2010.
* [11] Bertsimas, Dimitris and Nathan Kallus. From predictive to prescriptive analytics. _Management Science_, 66(3):1025-1044, 2020.
* [12] Bertsimas, Dimitris, Jack Dunn, and Nishanth Mundru. Optimal prescriptive trees. _INFORMS Journal on Optimization_, 1(2):164-183, 2019.
* [13] Biega, Asia J, Peter Potash, Hal Daume, Fernando Diaz, and Michele Finck. Operationalizing the legal principle of data minimization for personalization. In _Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval_, pages 399-408, 2020.
* [14] Bien, Jacob, Jonathan Taylor, and Robert Tibshirani. A lasso for hierarchical interactions. _Annals of statistics_, 41(3):1111, 2013.
* [15] Biggs, Max, Wei Sun, and Markus Ettl. Model distillation for revenue optimization: Interpretable personalized pricing. _arXiv preprint arXiv:2007.01903_, 2020.
* [16] Bukaty, P. _The California Consumer Privacy Act (CCPA): An implementation guide_. IT Governance Publishing, 2019. ISBN 9781787781337. URL https://books.google.com/books?id=vGWfDWA4QBAJ.
* [17] Campbell, Tim S and William A Kracaw. Information production, market signalling, and the theory of financial intermediation. _the Journal of Finance_, 35(4):863-882, 1980.
* [18] Chemmanur, Thomas J. The pricing of initial public offerings: A dynamic model with information production. _The Journal of Finance_, 48(1):285-304, 1993.
* [19] DeLong, Elizabeth R, David M DeLong, and Daniel L Clarke-Pearson. Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach. _Biometrics_, pages 837-845, 1988.

* DiCiccio et al. [1996] DiCiccio, Thomas J and Bradley Efron. Bootstrap confidence intervals. _Statistical science_, pages 189-212, 1996.
* Dietterich [1998] Dietterich, Thomas G. Approximate statistical tests for comparing supervised classification learning algorithms. _Neural computation_, 10(7):1895-1923, 1998.
* Do et al. [2021] Do, Virginie, Sam Corbett-Davies, Jamal Atif, and Nicolas Usunier. Online certification of preference-based fairness for personalized recommender systems. _arXiv preprint arXiv:2104.14527_, 2021.
* Edwards et al. [2013] Edwards, Adrian GK, Gurudut Naik, Harry Ahmed, Glyn J Elwyn, Timothy Pickles, Kerry Hood, and Rebecca Playle. Personalised risk communication for informed decision making about taking screening tests. _Cochrane database of systematic reviews_, Cochrane database of systematic reviews(2), 2013.
* Eliaz and Spiegler [2022] Eliaz, Kfir and Ran Spiegler. On incentive-compatible estimators. _Games and Economic Behavior_, 132:204-220, 2022.
* Elmachtoub et al. [2018] Elmachtoub, Adam N, Vishal Gupta, and Michael Hamilton. The value of personalized pricing. _Available at SSRN 3127719_, 2018.
* European Parliament and of the Council [2016] European Parliament and of the Council. Regulation 2016/679 of the european parliament and of the council of 27 april 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing directive 95/46/ec (general data protection regulation), 2016. URL https://eur-lex.europa.eu/eli/reg/2016/679/oj. Official Journal of the European Union.
* Fan and Poole [2006] Fan, Haiyan and Marshall Scott Poole. What is personalization? perspectives on the design and implementation of personalization in information systems. _Journal of Organizational Computing and Electronic Commerce_, 16(3-4):179-202, 2006.
* Globus-Harris et al. [2022] Globus-Harris, Ira, Michael Kearns, and Aaron Roth. An algorithmic framework for bias bounties. _2022 ACM Conference on Fairness, Accountability, and Transparency_, Jun 2022. doi: 10.1145/3531146.3533172. URL http://dx.doi.org/10.1145/3531146.3533172.
* Haukoos et al. [2012] Haukoos, Jason S, Michael S Lyons, Christopher J Lindsell, Emily Hopkins, Brooke Bender, Richard E Rothman, Yu-Hsiang Hsieh, Lynsay A MacLaren, Mark W Thrun, Comilla Sasson, et al. Derivation and validation of the denver human immunodeficiency virus (hiv) risk score for targeted hiv screening. _American journal of epidemiology_, 175(8):838-846, 2012.
* Hebert-Johnson et al. [2018] Hebert-Johnson, Ursula, Michael Kim, Omer Reingold, and Guy Rothblum. Multicalibration: Calibration for the (computationally-identifiable) masses. In _Proceedings of the International Conference on Machine Learning_, pages 1944-1953, 2018.
* Hollenberg [2003] Hollenberg, SM. Cardiogenic shock. In _Intensive Care Medicine_, pages 447-458. Springer, 2003.
* Johnson et al. [2016] Johnson, Alistair EW, Tom J Pollard, Lu Shen, H Lehman Li-Wei, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible critical care database. _Scientific data_, 3(1):1-9, 2016.
* Kaminski [2019] Kaminski, Margot E. The right to explanation, explained. _Berkeley Tech. LJ_, 34:189, 2019.
* Kearns et al. [2018] Kearns, Michael, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymandering: Auditing and learning for subgroup fairness. In _International Conference on Machine Learning_, pages 2564-2572, 2018.
* Kessler et al. [2005] Kessler, Ronald C, Lenard Adler, Minnie Ames, Olga Demler, Steve Faraone, EVA Hirpi, Mary J Howes, Robert Jin, Kristina Secnik, Thomas Spencer, et al. The world health organization adult adhd self-report scale (asrs): a short screening scale for use in the general population. _Psychological medicine_, 35(2):245-256, 2005.
* Kim et al. [2019] Kim, Michael P, Aleksandra Korolova, Guy N Rothblum, and Gal Yona. Preference-informed fairness. _arXiv preprint arXiv:1904.01793_, 2019.
* Gall et al. [1993] Le Gall, Jean-Roger, Stanley Lemeshow, and Fabienne Saulnier. A new simplified acute physiology score (saps ii) based on a european/north american multicenter study. _Jama_, 270(24):2957-2963, 1993.
* Lim and Hastie [2015] Lim, Michael and Trevor Hastie. Learning interactions via hierarchical group-lasso regularization. _Journal of Computational and Graphical Statistics_, 24(3):627-654, 2015.
* Lundberg et al. [2019] Lundberg, Ian, Arvind Narayanan, Karen Levy, and Matthew J Salganik. Privacy, ethics, and data access: A case study of the fragile families challenge. _Socius_, 5:2378023118813023, 2019.
* OECD [2013] OECD. Recommendation of the council concerning guidelines governing the protection of privacy and transborder flows of personal data, 2013. URL https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0188.
* Ortlieb and Garner [2016] Ortlieb, Martin and Ryan Garner. Sensitivity of personal data items in different online contexts. _it-Information Technology_, 58(5):217-228, 2016.

* [42] Paes, Lucas Monteiro, Carol Xuan Long, Berk Ustun, and Flavio Calmon. On the epistemic limits of personalized prediction. In Oh, Alice H., Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=Snp3iEj7NJ.
* [43] Paulus, Jessica K and David M Kent. Predictably unequal: understanding and addressing concerns that algorithmic clinical prediction may increase health disparities. _NPJ digital medicine_, 3(1):1-8, 2020.
* [44] Pollard, Tom J, Alistair EW Johnson, Jesse D Raffa, Leo A Celi, Roger G Mark, and Omar Badawi. The eicu collaborative research database, a freely available multi-center database for critical care research. _Scientific data_, 5(1):1-13, 2018.
* [45] Scosyrev, Emil, James Messing, Katia Noyes, Peter Veazie, and Edward Messing. Surveillance epidemiology and end results (seer) program and population-based research in urologic oncology: an overview. In _Urologic Oncology: Seminars and Original Investigations_, volume 30, pages 126-132. Elsevier, 2012.
* [46] Shanmugam, Divya, Fernando Diaz, Samira Shabanian, Michele Finck, and Asia Biega. Learning to limit data collection via scaling laws: A computational interpretation for the legal principle of data minimization. In _Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency_, pages 839-849, 2022.
* [47] Sharp, Rachel. The hamilton rating scale for depression. _Occupational Medicine_, 65(4):340-340, 2015.
* [48] Steyerberg, Ewout W et al. _Clinical prediction models_. Springer, 2019.
* [49] Struck, Aaron F, Berk Ustun, Andres Rodriguez Ruiz, Jong Woo Lee, Suzette M LaRoche, Lawrence J Hirsch, Emily J Gilmore, Jan Vlachy, Hiba Arif Haider, and Cynthia Rudin. Association of an electroencephalography-based risk score with seizure probability in hospitalized patients. _JAMA neurology_, 74(12):1419-1424, 2017.
* [50] Sun, Xu and Weichao Xu. Fast implementation of delong's algorithm for comparing the areas under correlated receiver operating characteristic curves. _IEEE Signal Processing Letters_, 21(11):1389-1393, 2014. doi: 10.1109/LSP.2014.2337313.
* [51] Suriyakumar, Vinith M, Marzyeh Ghassemi, and Berk Ustun. When personalization harms: Reconsidering the use of group attributes in prediction. In _International Conference on Machine Learning_, 2023.
* [52] Tran, Cuong and Ferdinando Fioretto. Personalized privacy auditing and optimization at test time. _arXiv preprint arXiv:2302.00077_, 2023.
* [53] Tran, Cuong, My Dinh, and Ferdinando Fioretto. Differentially private empirical risk minimization under the fairness lens. _Advances in Neural Information Processing Systems_, 34:27555-27565, 2021.
* [54] U.S. Congress. Health insurance portability and accountability act of 1996, 1996. URL https://www.hhs.gov/hipaa/for-professionals/privacy/index.html. Public Law 104-191.
* [55] Ustun, Berk, M Brandon Westover, Cynthia Rudin, and Matt T Bianchi. Clinical prediction models for sleep apnea: the importance of medical history over symptoms. _Journal of Clinical Sleep Medicine_, 12(02):161-168, 2016.
* [56] Ustun, Berk, Lenard A Adler, Cynthia Rudin, Stephen V Faraone, Thomas J Spencer, Patricia Berglund, Michael J Gruber, and Ronald C Kessler. The world health organization adult attention-deficit/hyperactivity disorder self-report screening scale for dsm-5. _Jama psychiatry_, 74(5):520-526, 2017.
* [57] Ustun, Berk, Yang Liu, and David Parkes. Fairness without harm: Decoupled classifiers with preference guarantees. In _International Conference on Machine Learning_, pages 6373-6382, 2019.
* [58] Vaughan, Gregory, Robert Aseltine, Kun Chen, and Jun Yan. Efficient interaction selection for clustered data via stagewise generalized estimating equations. _Statistics in Medicine_, 39(22):2855-2868, 2020.
* [59] Viviano, Davide and Jelena Bradic. Fair policy targeting. _arXiv preprint arXiv:2005.12395_, 2020.
* [60] Vyas, Darshali A, Leo G Eisenstein, and David S Jones. Hidden in plain sight--reconsidering the use of race correction in clinical algorithms, 2020.
* [61] Yu, Shipeng, Balaji Krishnapuram, Romer Rosales, and R. Bharat Rao. Active sensing. In _AISTATS_, 2009.
* [62] Zafar, Muhammad Bilal, Isabel Valera, Manuel Rodriguez, Krishna Gummadi, and Adrian Weller. From parity to preference-based notions of fairness in classification. In _Advances in Neural Information Processing Systems_, pages 228-238, 2017.

**Supplementary Material**

* Experiments C Results for Different Model Classes and Prediction Tasks C.1 Logistic Regression for Ranking (AUC) C.2 Random Forests for Decision-Making (Error) C.3 Random Forests for Ranking (AUC) D Supporting Material for Performance Profiles
Supporting Material for Section 3

### Enumeration Routine for Algorithm 1

We summarize the Enumeration routine in Algorithm 2. Algorithm 2 takes as input a set of group attributes \(\mathcal{G}\) and a dataset \(\mathcal{D}\) and outputs a collection of reporting interfaces \(\mathbb{T}\) that obey ordering and plausibility constraints.

```
1:procedureViableTrees(\(\mathcal{G}\), \(\mathcal{D}\))
2:if\(\text{dim}(\mathcal{G})\) = 1 return\([T_{\mathcal{G}}]\) base case: we are left with only a single attribute on which to branch
3:\(\mathbb{T}\leftarrow[\big{]}\)
4:for each group attribute \(\mathcal{A}\in[\mathcal{G}_{1},\dots,\mathcal{G}_{k}]\)do
5:\(T_{\mathcal{A}}\leftarrow\) reporting tree of depth 1 with \(|\mathcal{A}|\) leaves
6:\(\mathcal{S}\leftarrow\)ViableTrees(\(\mathcal{G}\setminus\mathcal{A},\mathcal{D}\))
7:for\(\Pi\)inVaidAssignments(\(\mathcal{S},\mathcal{A},\mathcal{D}\))do: each assignment is a permutation of \(|\mathcal{A}|\) to leaves of \(T_{\mathcal{A}}\)
8:\(\mathbb{T}\leftarrow\mathbb{T}\cup T_{\mathcal{A}}\).assign(\(\Pi\)) extends the tree by assigning subtrees to each leaf
9:endfor
10:endfor
11:return\(\mathbb{T}\), reporting interfaces for group attributes \(\mathcal{G}\) that obey plausibility and ordering constraints
12:endprocedure ```

**Algorithm 2** Enumerate All Possible Reporting Trees for Reporting Options \(\mathcal{G}\)

The routine enumerates all possible reporting interfaces for a given set of group attributes \(\mathcal{G}\) through a recursive branching process. Given a set of group attributes, the routine is called for each attribute that has yet to be considered in the tree Line 4, ensuring a complete enumeration. We note that the routine is only called for building Sequential systems since there is only one possible reporting interface for Minimal and Flat systems.

Enumerating all possible trees ensures we can recover the best tree given the selection criteria and allows practitioners to choose between models based on other criteria. We generate trees that meet plausibility constraints based on the dataset, such as having at least one negative and one positive sample and at least \(s\) total samples at each leaf. In settings constrained by computational resources, we can impose additional stopping criteria and modify the ordering to enumerate more plausible trees first or exclusively (e.g., by changing the ordering of \(\mathcal{G}\) or imposing constraints in ValidAssignments).

### Assignment Routine for Algorithm 1

We summarize the routine for AssignModels procedure in Algorithm 3.

```
1:procedureAssignModels(\(T,\mathcal{M},\mathcal{D}\))
2:\(Q\leftarrow[T.\)**root**] initialize with the root of the tree, reporting group \(\boldsymbol{\varnothing}\)
3:while\(Q\) is not empty do
4:\(\boldsymbol{r}\leftarrow\)\(Q\).pop\(()\)
5:\(\mathcal{M}_{\boldsymbol{r}}\leftarrow\)ViableModels(\(\mathcal{M},\boldsymbol{r}\)) filter \(\mathcal{M}\) to models that can be assigned to \(\boldsymbol{r}\)
6:\(h^{*}\leftarrow\underset{h\in\mathcal{M}_{\boldsymbol{r}}}{\text{argmin}}\, \hat{R}_{r}(h,\mathcal{D})\) assign the model with the best training performance
7:\(T.\)set_model\((\boldsymbol{r},h^{*})\)
8:for\(\boldsymbol{r}^{\prime}\in T.\)get_subgroups\((\boldsymbol{r})\)do iterate through the children reporting groups of \(\boldsymbol{r}\)
9:\(Q.\)enqueue\((\boldsymbol{r}^{\prime})\)
10:endfor
11:endwhile
12:return\(T\) that maximizes gain for each reporting group
13:endprocedure ```

**Algorithm 3** Assigning Models

Algorithm 3 takes as inputs a reporting tree \(T\), a pool candidate models \(\mathcal{M}\), and an assignment (training) dataset \(\mathcal{D}\) and outputs a tree \(T\) that maximizes the gains of reporting group information. The pool of candidate models is filtered to viable models for each reporting group. Since the pool of candidate models includes the generic model \(h_{0}\), each reporting group will have at least one viable model. We assign each reporting group the best-performing model on the training set and default to the generic model \(h_{0}\) when a better-performing personalized model is not found. We assign performance on the training set and then prune using performance on the validation set to avoid biased gain estimations.

[MISSING_PAGE_EMPTY:16]

## Appendix B Description of Datasets used in Section 4 - Experiments

We include additional information about the datasets used in Section 4.

apneaWe use the obstructive sleep apnea (OSA) dataset outlined in Ustun et al. [55]. This dataset includes a cohort of 1,152 patients where 23% have OSA. We use all available features (e.g. BMI, comorbidities, age, and sex) and binarize them, resulting in 26 binary features.

cardio_eicu & cardio_mimicCardiogenic shock is an acute condition in which the heart cannot provide sufficient blood to the vital organs [31]. These datasets are designed to predict cardiogenic shock for patients in intensive care. Each dataset contains the same features, group attributes, and outcome variables for patients in different cohorts. The cardio_eicu dataset contains records for a cohort of patients in the Collaborative Research Database V2.0 [44]. The cardio_eicu dataset contains records for a cohort of patients in the MIMIC-III [32] database. Here, the outcome variable indicates whether a patient in the ICU with cardiogenic shock will die while in the ICU. The features encode the results of vital signs and routine lab tests (e.g. systolic BP, heart rate, hemoglobin count) that were collected up to 24 hours before the onset of cardiogenic shock.

lungcancerWe consider a cohort of 120,641 patients who were diagnosed with lung cancer between 2004-2016 and monitored as part of the National Cancer Institute SEER study [45]. Here, the outcome variable indicates if a patient dies within five years from any cause, and 16.9% of patients died within the first five years from diagnosis. The cohort includes patients from Greater California, Georgia, Kentucky, New Jersey, and Louisiana, and does not cover patients who were lost to follow-up (censored). Age and Sex were considered as group attributes. The features reflect the morphology and histology of the tumor (e.g., size, metastasis, stage, node count and location, number and location of notes) as well as interventions that were administered at the time of diagnosis (e.g., surgery, chemo, radiology).

coloncancerWe consider a cohort of 120,641 patients who were diagnosed with colorectal cancer between 2004-2016 and monitored as part of the National Cancer Institute SEER study [45]. Here, the outcome variable indicates if a patient dies within five years from any cause, and 42.1% of patients die within the first five years from diagnosis. The cohort includes patients from Greater California. Age and Sex were considered as group attributes. The features reflect the morphology and histology of the tumor (e.g., size, metastasis, stage, node count and location, number and location of notes) as well as interventions that were administered at the time of diagnosis (e.g., surgery, chemo, radiology).

sapsThe Simplified Acute Physiology Score II (SAPS II) score predicts the risk of mortality of critically-ill patients in intensive care [37]. The data contains records of 7,797 patients from 137 medical centers in 12 countries. Here, the outcome variable indicates whether a patient dies in the ICU, with 12.8% patient of patients dying. The features reflect comorbidities, vital signs, and lab measurements.

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline
**Dataset** & **Reference** & **Outcome Variable** & \(n\) & \(d\) & \(m\) & \(\mathcal{G}\) \\ \hline apnea & Ustun et al. [55] & patient has obstructive sleep apnea & 1,152 & 28 & 6 & \{age,sex\} \\ \hline cardio\_eicu & Pollard et al. [44] & patient with cardiogenic shock dies & 1,341 & 49 & 8 & \{age,sex,race\} \\ \hline cardio\_mimic & Johnson et al. [32] & patient with cardiogenic shock dies & 5,289 & 49 & 8 & \{age,sex,race\} \\ \hline coloncancer & Scosyrev et al. [45] & patient dies within 5 years & 29,211 & 72 & 6 & \{age,sex\} \\ \hline lungcancer & Scosyrev et al. [45] & patient dies within 5 years & 120,641 & 84 & 6 & \{age,sex\} \\ \hline saps & Allyn et al. [3] & ICU mortality & 7,797 & 36 & 4 & \{age,HIV\} \\ \hline \hline \end{tabular}
\end{table}
Table 3: Datasets used to fit clinical prediction models in Section 4. Here: \(n\) denotes the number of examples in each dataset; \(d\) denotes the number of features; \(\mathcal{G}\) denotes the group attributes that are used for personalization; and \(m=|\mathcal{G}|\) denotes the number of intersectional groups. Each dataset is de-identified and available to the public. The cardio_eicu, cardio_mimic, lungcancer datasets require access to public repositories listed under the references. The saps and apnea datasets must be requested from the authors. The support dataset can be downloaded directly from the URL below.

[MISSING_PAGE_EMPTY:18]

[MISSING_PAGE_EMPTY:19]

[MISSING_PAGE_EMPTY:20]

Supporting Material for Performance Profiles

In the performance profiles, we measure the benefit of disclosure in terms of their expected performance gain and simulate the cost of reporting for each individual by sampling their reporting cost from a uniform distribution - i.e., for each individual \(i\), we sample \(c_{i}\) as \(c_{i}\sim\text{Uniform}(0,\gamma),\text{ where }\gamma\in[0,0.2]\). For each value of \(\gamma\), we sample reporting costs ten times and average over the per group performance error for each sampled cost.