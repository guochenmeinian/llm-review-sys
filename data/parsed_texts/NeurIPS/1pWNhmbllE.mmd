# Uncertainty-Aware Instance Reweighting for

Off-Policy Learning

 Xiaoying Zhang\({}^{1}\)  Junpu Chen\({}^{2}\)  Hongning Wang\({}^{3}\)  Hong Xie\({}^{4}\)  Yang Liu\({}^{1}\)

John C.S. Lui\({}^{5}\)  Hang Li\({}^{1}\)

\({}^{1}\)ByteDance Research \({}^{2}\)ChongQing University \({}^{3}\)Tsinghua University

\({}^{4}\) Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Science

\({}^{5}\) The Chinese University of Hong Kong

{zhangxiaoying.xy,yang.liu01,lihang.lh}@bytedance.com

{jumpchan98,hongx87,wang.hongn}@gmail.com

cslui@cse.cuhk.edu.hk

###### Abstract

Off-policy learning, referring to the procedure of policy optimization with access only to logged feedback data, has shown importance in various real-world applications, such as search engines and recommender systems. While the ground-truth logging policy is usually unknown, previous work simply employs its estimated value for the off-policy learning, ignoring the negative impact from both high bias and high variance resulted from such an estimator. And such impact is often magnified on samples with small and inaccurately estimated logging probabilities. The contribution of this work is to explicitly model the uncertainty in the estimated logging policy, and propose an Uncertainty-aware Inverse Propensity Score estimator (UIPS) for improved off-policy learning, with a theoretical convergence guarantee. Experiment results on the synthetic and real-world recommendation datasets demonstrate that UIPS significantly improves the quality of the discovered policy, when compared against an extensive list of state-of-the-art baselines.

## 1 Introduction

In many real-world applications, including search engines [2], online advertisements [35], recommender systems [8; 22], only logged data is available for subsequent policy learning. For example, in recommender systems, various complex recommendation policies are optimized over logged user interactions (e.g., clicks or stay time) with items recommended by previous recommendation policies (referred to as the _logging policy_) [51; 14]. However, such logged data is often known to be biased, since the feedback on items where the logging policy did not take is unknown. This inevitably distorts the evaluation and optimization of a new policy when it differs from the logging policy.

Off-policy learning [41; 27] thus emerges as a preferred way to learn an improved policy only from the logged data, by addressing the mismatch between the learning and logging policies. One of the most commonly used off-policy learning methods is the Inverse Propensity Scoring (IPS) [8; 25], which assigns per-sample importance weight (i.e., propensity score) to the training objective on the logged data, so as to get an unbiased optimization objective in expectation. The importance weight in IPS is the probability ratio of taking an action between the learning and logging policies.

Unfortunately, the ground-truth logging policy is oftentimes unavailable to the learner in practice, due to reasons like legacy issues, i.e., it was not recorded in the data. Additionally, in specific situations like the healthcare domain [28] or two-stage recommender systems [8], access to the ground-truth logging policy is not feasible. One common treatment by many previous studies [35; 22; 8; 24] is to first estimate the logging policy using a supervised learning method (e.g., logistic regression,neural networks, etc.), and then employ the estimated logging policy for off-policy learning. In this work, we first show that such an approximation results in a biased estimator which is sensitive to data with small estimated logging probabilities. Worse still, small estimated logging probabilities usually suggest there are limited related samples in the logged data, whose estimations can have high uncertainties, i.e., being wrong with a high probability. Figure 1 shows a piece of empirical evidence from a large-scale recommendation benchmark KuaiRec dataset [12], where items with lower frequencies in the logged dataset have lower estimated logging probabilities (via a neural network estimator) and higher uncertainties at the same time. The high bias and variance caused by these samples can greatly hinder the performance of subsequent off-policy learning. We defer detailed discussions of this result in Section 2.

In this work, we explicitly take the uncertainty of the estimated logging policy into consideration and design an Uncertainty-aware Inverse Propensity Score estimator (UIPS) for off-policy learning. UIPS reweighs the propensity score of each logged sample to control its impact on policy optimization, and learns an improved policy by alternating between: (1) Find the optimal weight that makes the estimator as accurate as possible, based on the uncertainty of the estimated logging policy; (2) Improve the policy by optimizing the resulting objective function. The optimal weight for each sample is obtained by minimizing the upper bound of the mean squared error (MSE) to the ground-truth policy evaluation, with a closed-form solution. Furthermore, UIPS ensures that off-policy learning converges to a stationary point where the true policy gradient is zero; while convergence may not be guaranteed when directly using the estimated logging policy. Extensive experiments on a synthetic and three real-world recommendation datasets against a rich set of state-of-the-art baselines demonstrate the power of UIPS. All data and code can be found in [https://github.com/Xiaoyinggit/UIPS.git](https://github.com/Xiaoyinggit/UIPS.git).

## 2 Preliminary: off-policy learning

We focus on the standard contextual bandit setup to explain the key concepts in UIPS. Following the convention [16; 29; 36], let \(\mathbf{x}\in\mathcal{X}\subseteq R^{d}\) be a \(d\)-dimensional context vector drawn from an unknown distribution \(p(\mathbf{x})\). Each context is associated with a finite set of actions denoted by \(\mathcal{A}\), where \(|\mathcal{A}|<\infty\). Let \(\pi:\mathcal{A}\times\mathcal{X}^{\prime}\to[0,1]\) denote a stochastic policy, such that \(\pi(a|\mathbf{x})\) is the probability of selecting action \(a\) under context \(\mathbf{x}\) and \(\sum_{a\in\mathcal{A}}\pi(a|\mathbf{x})=1\). Under a given context \(\mathbf{x}\), the reward \(r_{\mathbf{x},a}\) is only observed when action \(a\) is chosen, i.e., bandit feedback. Without loss of generality, we assume \(r_{\mathbf{x},a}\in[0,1]\). Let \(V(\pi)\) denote the expected reward of the policy \(\pi\):

\[V(\pi)=\mathbb{E}_{\mathbf{x}\sim p(\mathbf{x}),a\sim\pi(a|\mathbf{x})}[r_{\mathbf{x},a}]. \tag{1}\]

We look for a policy \(\pi(a|\mathbf{x})\) to maximize \(V(\pi)\). In the rest, we denote \(\mathbb{E}_{\mathbf{x}\sim p(\mathbf{x}),a\sim\pi(a|\mathbf{x})}[\cdot]\) as \(\mathbb{E}_{\pi}[\cdot]\).

In off-policy learning, one can only access a set of logged feedback data \(D:=\{(\mathbf{x}_{n},a_{n},r_{\mathbf{x}_{n},a_{n}})|n\in[N]\}\). Given \(\mathbf{x}_{n}\), the action \(a_{n}\) was generated by a stochastic logging policy \(\beta^{*}\), i.e., \(a_{n}\sim\beta^{*}(a|\mathbf{x}_{n})\), which is usually different from the learning policy \(\pi(a|\mathbf{x})\)[24; 40; 8]. The actions \(\{a_{1},\ldots,a_{N}\}\) and their corresponding rewards \(\{r_{\mathbf{x}_{1},a_{1}},\ldots,r_{\mathbf{x}_{N},a_{N}}\}\) are generated independently given \(\beta^{*}\). The main challenge is then to address the distributional discrepancy between \(\beta^{*}(a|\mathbf{x})\) and \(\pi(a|\mathbf{x})\), when optimizing \(\pi(a|\mathbf{x})\) to maximize \(V(\pi)\) with access only to the logged dataset \(D\).

Figure 1: Estimated logging policy and its uncertainty under different item frequency on KuaiRec.

One of the most widely used methods to address the distribution shift between \(\pi(a|\mathbf{x})\) and \(\beta^{*}(a|\mathbf{x})\) is the Inverse Propensity Scoring (IPS) [8; 25]. One can easily get that:

\[V(\pi)=\mathbb{E}_{\beta^{*}}\left[\frac{\pi(a|\mathbf{x})}{\beta^{*}(a|\mathbf{x})}r_{ \mathbf{x},a}\right],\]

yielding the following empirical estimator of \(V(\pi)\):

\[\hat{V}_{\mathrm{IPS}}(\pi)=\frac{1}{N}\sum_{n=1}^{N}\frac{\pi(a_{n}|\mathbf{x}_{n })}{\beta^{*}(a_{n}|\mathbf{x}_{n})}r_{\mathbf{x}_{n},a_{n}}, \tag{2}\]

where \(\pi(a_{n}|\mathbf{x}_{n})/\beta^{*}(a_{n}|\mathbf{x}_{n})\) is referred to as the propensity score. Various algorithms can be readily used for policy optimization under \(\hat{V}_{\mathrm{IPS}}(\pi)\), including value-based methods [33] and policy-based methods [19; 31; 42]. In this work, we adopt a well-known policy gradient algorithm, REINFORCE [42]. Assume the policy \(\pi(a|\mathbf{x})\) is parameterized by \(\mathbf{\vartheta}\), via the "log-trick", the gradient of \(\hat{V}_{\mathrm{IPS}}(\pi_{\mathbf{\vartheta}})\) with respect to \(\mathbf{\vartheta}\) can be readily derived as,

\[\nabla_{\mathbf{\vartheta}}\hat{V}_{\mathrm{IPS}}(\pi_{\mathbf{\vartheta}})=\frac{1}{N }\sum_{n=1}^{N}\frac{\pi_{\mathbf{\vartheta}}(a_{n}|\mathbf{x}_{n})}{\beta^{*}(a_{n}| \mathbf{x}_{n})}r_{\mathbf{x}_{n},a_{n}}\nabla_{\mathbf{\vartheta}}\log(\pi_{\mathbf{\vartheta }}(a_{n}|\mathbf{x}_{n})).\]

**Approximation with an unknown logging policy**. In many real-world applications, the ground-truth logging probabilities, i.e., \(\beta^{*}(a|\mathbf{x})\) of each observation \((\mathbf{x},a)\) in \(D\), are unknown. As a typical walk-around, previous work employs supervised learning methods such as logistic regression [30] and neural networks [8] to estimate the logging policy, and replaces \(\beta^{*}(a|\mathbf{x})\) with its estimated value \(\hat{\beta}(a|\mathbf{x})\) to get the following BIPS estimator for policy learning:

\[\hat{V}_{\mathrm{BIPS}}(\pi_{\mathbf{\vartheta}})=\frac{1}{N}\sum_{n=1}^{N}\frac{ \pi_{\mathbf{\vartheta}}(a_{n}|\mathbf{x}_{n})}{\hat{\beta}(a_{n}|\mathbf{x}_{n})}r_{\mathbf{x }_{n},a_{n}}. \tag{3}\]

However, as shown in the following proposition, inaccurate \(\hat{\beta}(a|\mathbf{x})\) leads to high bias and variance in BIPS. Worse still, smaller and inaccurate \(\hat{\beta}(a|\mathbf{x})\) further enlarges this bias and variance.

**Proposition 2.1**.: _The bias and variance of \(\hat{V}_{\mathrm{BIPS}}(\pi_{\mathbf{\vartheta}})\) can be derived as follows:_

\[\mathrm{Bias}\left(\hat{V}_{\mathrm{BIPS}}(\pi_{\mathbf{\vartheta}}) \right)=\mathbb{E}_{D}\left[\hat{V}_{\mathrm{BIPS}}(\pi_{\mathbf{\vartheta}})-V(\pi _{\mathbf{\vartheta}})\right]=\mathbb{E}_{\pi_{\mathbf{\vartheta}}}\left[r_{\mathbf{x},a} \left(\frac{\beta^{*}(a|\mathbf{x})}{\hat{\beta}(a|\mathbf{x})}-1\right)\right]\] \[N\cdot\mathrm{Var}_{D}\left(\hat{V}_{\mathrm{BIPS}}(\pi_{\mathbf{ \vartheta}})\right)=\mathrm{Var}_{\pi_{\mathbf{\vartheta}}}\left(\frac{\beta^{*}( a|\mathbf{x})}{\hat{\beta}(a|\mathbf{x})}r_{\mathbf{x},a}\right)+\mathbb{E}_{\pi_{\mathbf{ \vartheta}}}\left[\left(\frac{\pi_{\mathbf{\vartheta}}(a|\mathbf{x})}{\beta^{*}(a|\mathbf{x })}-1\right)\frac{\beta^{*}(a|\mathbf{x})^{2}}{\hat{\beta}(a|\mathbf{x})^{2}}r_{\mathbf{x},a}^{2}\right]\]

However, smaller \(\hat{\beta}(a|\mathbf{x})\) usually implies less number of related training samples in the logged data, and thus \(\hat{\beta}(a|\mathbf{x})\) can be inaccurate with a higher probability. To make it more explicit, let us revisit the empirical results shown in Figure 1. We followed the method introduced in [8] to estimate the logging policy on KuaiRec dataset [12] and plotted the estimated \(\hat{\beta}(a|\mathbf{x})\) and its corresponding uncertainties on items of different observation frequencies in the logged dataset. We adopted the method in [45] to measure the confidence interval of \(\hat{\beta}(a|\mathbf{x})\) on each instance. A wider confidence interval, i.e., higher uncertainty in estimation, implies that with a high probability the true value may be further away from the empirical mean estimate. We can observe in Figure 1 that as item frequency decreases, the estimated logging probability also decreases, but the estimation uncertainty increases. This implies that a smaller \(\hat{\beta}(a|\mathbf{x})\) is usually 1) more inaccurate and 2) associated with a higher uncertainty.

As a result, with high bias and variance caused by inaccurate \(\hat{\beta}(a|\mathbf{x})\), it is erroneous to learn \(\pi_{\mathbf{\vartheta}}(a|\mathbf{x})\) by simply optimizing \(\hat{V}_{\mathrm{BIPS}}(\pi_{\mathbf{\vartheta}})\). Furthermore, this approach may also hinder the convergence of off-policy learning, as discussed later in Section 3.2.

## 3 Uncertainty-aware off-policy learning

Our idea is to consider the uncertainty of the estimated logging policy by incorporating per-sample weight \(\phi_{\mathbf{x},a}\), and perform policy learning by optimizing the following empirical estimator:

\[\hat{V}_{\mathrm{UIPS}}(\pi_{\mathbf{\vartheta}})=\frac{1}{N}\sum_{n=1}^{N}\frac{ \pi_{\mathbf{\vartheta}}(a_{n}|\mathbf{x}_{n})}{\hat{\beta}(a_{n}|\mathbf{x}_{n})}\cdot \phi_{\mathbf{x}_{n},a_{n}}\cdot r_{\mathbf{x}_{n},a_{n}}. \tag{4}\]Intuitively, one should assign lower weights to samples whose \(\hat{\beta}(a|\mathbf{x})\) is small and far away from the ground-truth \(\beta^{*}(a|\mathbf{x})\). We then divide off-policy optimization into two iterative steps:

* **Deriving the optimal instance weight:** Find the optimal \(\phi_{\mathbf{x},a}\) to make \(\hat{V}_{\mathrm{UIPS}}(\pi_{\mathbf{\theta}})\) approach its ground-truth \(V(\pi)\) as closely as possible, so as to facilitate policy learning. The derived optimal weight is denoted as \(\phi^{*}_{\mathbf{x},a}\) (see Theorem 3.2).
* **Policy improvement:** Update the policy \(\pi_{\mathbf{\theta}}(a|\mathbf{x})\) using the following gradient: \[\nabla_{\mathbf{\theta}}\hat{V}_{\mathrm{UIPS}}(\pi_{\mathbf{\theta}})=\frac{1}{N} \sum_{n=1}^{N}\frac{\pi_{\mathbf{\theta}}(a_{n}|\mathbf{x}_{n})}{\hat{\beta}(a_{n}|\bm {x}_{n})}\cdot\phi^{*}_{\mathbf{x}_{n},a_{n}}\cdot r_{\mathbf{x}_{n},a_{n}}\nabla_{\bm {\theta}}\log(\pi_{\mathbf{\theta}}(a_{n}|\mathbf{x}_{n}))\] (5) The whole algorithm framework and its computational cost, as well as important notations are summarized in Appendix 7.1.

### Derive the optimal uncertainty-aware instance weight

We expect to find the optimal weight \(\phi_{x,a}\) to make the empirical estimator \(\hat{V}_{\mathrm{UIPS}}(\pi_{\mathbf{\theta}})\) as accurate as possible, taking into account the uncertainty in estimated logging probabilities. Intuitively, a high accuracy of the estimator is crucial for determining the correct direction of policy learning. We follow previous work [36; 29] and measure the mean squared error (MSE) of \(\hat{V}_{\mathrm{UIPS}}(\pi_{\mathbf{\theta}})\) to the ground-truth policy value \(V(\pi_{\mathbf{\theta}})\), which captures both the bias and variance of an estimator. A lower MSE indicates a more accurate estimator.

In UIPS, instead of directly minimizing the MSE, which is intractable, we find \(\phi_{\mathbf{x},a}\) to minimize the upper bound of MSE. As we show later, the optimal \(\phi_{\mathbf{x},a}\) has a closed-form solution which relates to both the value of \(\pi_{\mathbf{\theta}}(a|\mathbf{x})/\hat{\beta}(a|\mathbf{x})\) and the estimation uncertainty of \(\hat{\beta}(a|\mathbf{x})\).

**Theorem 3.1**.: _The mean squared error (\(\mathrm{MSE}\)) between \(\hat{V}_{\mathrm{UIPS}}(\pi_{\mathbf{\theta}})\) and ground-truth estimator \(V(\pi_{\mathbf{\theta}})\) is upper bounded as follows:_

\[\mathrm{MSE}\left(\hat{V}_{\mathrm{UIPS}}(\pi_{\mathbf{\theta}})\right)=\mathbb{E }_{D}\left[\left(\hat{V}_{\mathrm{UIPS}}(\pi_{\mathbf{\theta}})-V(\pi_{\mathbf{\theta} })\right)^{2}\right]=\mathrm{Bias}\left(\hat{V}_{\mathrm{UIPS}}(\pi_{\mathbf{ \theta}})\right)^{2}+\mathrm{Var}\left(\hat{V}_{\mathrm{UIPS}}(\pi_{\mathbf{\theta }})\right)\]

As the first expectation term \(\mathbb{E}_{\pi_{\mathbf{\theta}}}\left[r_{\mathbf{x},a}^{2}\frac{\pi_{\mathbf{\theta}}(a| \mathbf{x})}{\beta^{*}(a|\mathbf{x})}\right]\) is a non-negative constant, we denote it as \(\lambda\in[0,\infty)\) when searching for \(\phi_{\mathbf{x},a}\). To minimize this upper bound of MSE, the optimal \(\phi_{\mathbf{x},a}\) for each sample \((\mathbf{x},a)\) should minimize the following,

\[\lambda\left(\frac{\beta^{*}(a|\mathbf{x})}{\hat{\beta}(a|\mathbf{x})}\phi_{\mathbf{x},a} -1\right)^{2}+\frac{\pi_{\mathbf{\theta}}(a|\mathbf{x})^{2}}{\hat{\beta}(a|\mathbf{x})^{2 }}\phi^{2}_{\mathbf{x},a}. \tag{6}\]

An interesting observation is that setting \(\phi_{\mathbf{x},a}=\frac{\hat{\beta}(a|\mathbf{x})}{\beta^{*}(a|\mathbf{x})}\), i.e., turning \(\frac{\pi(a|\mathbf{x})}{\hat{\beta}(a|\mathbf{x})}\phi_{\mathbf{x},a}\) into \(\frac{\pi(a|\mathbf{x})}{\beta^{*}(a|\mathbf{x})}\) does not result in the optimal solution of Eq.(6). This is because such a setting only reduces bias (i.e., the first term of Eq.(6)), but fails to control the second term, which is related to the variance. Moreover, we cannot directly minimize Eq.(6) due to the unknown \(\beta^{*}(a|\mathbf{x})\). But it is possible to obtain a confidence interval which contains \(\beta^{*}(a|\mathbf{x})\) with a high probability, when \(\hat{\beta}(a|\mathbf{x})\) is obtained via a specific estimator, e.g., (generalized) linear model or kernel methods.

Following previous work [23; 16; 22], we adopt the realizable assumption that \(\beta^{*}(a|\mathbf{x})\) can be represented by a softmax modified upper a parametric function \(f_{\mathbf{\theta}^{*}}(\mathbf{x},a)\). Moreover, the universal approximation theorem [18] states that a parametric function with sufficient capacity, when combined with a softmax function, can approximate any distribution. Then we have:

\[\beta^{*}(a|\mathbf{x})\propto\exp(f_{\mathbf{\theta}^{*}}(\mathbf{x},a)),\hat{\beta}(a|\bm {x})\propto\exp(f_{\mathbf{\theta}}(\mathbf{x},a)), \tag{7}\]

where \(f_{\mathbf{\theta}}(\mathbf{x},a)\) is an estimate of \(f_{\mathbf{\theta}^{*}}(\mathbf{x},a)\). Following the conventional definition of confidence interval [20], we define \(\gamma\) and \(U_{\mathbf{x},a}\) such that \(|f_{\mathbf{\theta}^{*}}(\mathbf{x},a)-f_{\mathbf{\theta}}(\mathbf{x},a)|\leq\gamma U_{\mathbf{x},a}\) holds with probability at least 1-\(\delta\), where \(\gamma\) is a function of \(\delta\) (typically the smaller \(\delta\) is, the larger \(\gamma\) is). Then \(\gamma U_{\mathbf{x},a}\) measures the width of confidence interval of \(f_{\mathbf{\theta}}(\mathbf{x},a)\) against its ground-truth \(f_{\mathbf{\theta}^{*}}(\mathbf{x},a)\). As derived in Appendix 7.2, with probability at least 1-\(\delta\), we have \(\beta^{*}(a|\mathbf{x})\in\mathbf{B}_{\mathbf{x},a}\) and

\[\mathbf{B}_{\mathbf{x},a}=\left[\frac{\hat{Z}\exp{(-\gamma U_{\mathbf{x},a})}}{Z^{*}}\hat{ \beta}(a|\mathbf{x}),\frac{\hat{Z}\exp{(\gamma U_{\mathbf{x},a})}}{Z^{*}}\hat{\beta}(a| \mathbf{x})\right],\]

where \(Z^{*}=\sum_{a^{\prime}}\exp(f_{\theta^{*}}(a^{\prime}|\mathbf{x}))\) and \(\hat{Z}=\sum_{a^{\prime}}\exp(f_{\theta}(a^{\prime}|\mathbf{x}))\).

As \(\beta^{*}(a|\mathbf{x})\) can be any value in \(\mathbf{B}_{\mathbf{x},a}\) with high probability, we aim to find the optimal \(\phi_{\mathbf{x},a}\) that minimizes the worst case of Eq.(6), thereby ensuring that \(\hat{V}_{\mathrm{UIPS}}(\pi_{\mathbf{\phi}})\) approaches its ground-truth \(V(\pi_{\mathbf{\phi}})\) under the sense of MSE, even in the worst possible scenarios. This ensures the subsequent policy improvement direction will not be much worse with high probability. Thus, we formulate the following optimization problem:

\[\min_{\phi_{\mathbf{x},a}}\max_{\beta_{\mathbf{x},a}\in\mathbf{B}_{\mathbf{x},a}}\lambda\left( \frac{\beta_{\mathbf{x},a}}{\hat{\beta}(a|\mathbf{x})}\phi_{\mathbf{x},a}-1\right)^{2}+ \frac{\pi_{\mathbf{\phi}}(a|\mathbf{x})^{2}}{\hat{\beta}(a|\mathbf{x})^{2}}\phi_{\mathbf{x},a} ^{2}. \tag{8}\]

The following theorem derives a closed-form formula for the optimal solution of Eq.(8).

**Theorem 3.2**.: _Let \(\eta\in[\exp(-\gamma U_{\mathbf{x}}^{\mathrm{max}}),\exp(\gamma U_{\mathbf{x}}^{ \mathrm{max}})]\), where \(U_{\mathbf{x}}^{\mathrm{max}}=\max_{a}U_{\mathbf{x},a}\). The optimization problem in Eq.(8) has a closed-form solution:_

\[\phi_{\mathbf{x},a}^{*}=\min\left(\lambda/\bigg{[}\frac{\lambda}{\eta}\exp\bigl{(}- \gamma U_{\mathbf{x},a}\bigr{)}+\frac{\eta\pi_{\mathbf{\phi}}(a|\mathbf{x})^{2}}{\hat{ \beta}(a|\mathbf{x})^{2}\exp{(-\gamma U_{\mathbf{x},a})}}\bigg{]},2\eta/\bigg{[}\exp \bigl{(}\gamma U_{\mathbf{x},a}\bigr{)}+\exp\bigl{(}-\gamma U_{\mathbf{x},a}\bigr{)} \bigg{]}\right).\]

The following corollary demonstrates the advantage of UIPS. The detailed proof of Theorem 3.2 and Corollary 3.3 can be found in Appendix 7.8.

**Corollary 3.3**.: _With \(\phi_{\mathbf{x},a}^{*}\) derived in Theorem 3.2, \(\hat{V}_{\mathrm{UIPS}}(\pi_{\mathbf{\phi}})\) in Eq.(4) achieves a smaller upper bound of MSE than \(\hat{V}_{\mathrm{BIPS}}(\pi_{\mathbf{\phi}})\) in Eq. (3)._

**Insights about \(\phi_{\mathbf{x},a}^{*}\).** The detailed analysis of the effect of \(\phi_{\mathbf{x},a}^{*}\) can be found in Lemma 7.1 in Appendix 7.8. In summary, we have the following key findings,

* For samples whose largest possible propensity score is under control: i.e., \(\frac{\pi_{\mathbf{\phi}}(a|\mathbf{x})}{\min B_{\mathbf{x},a}}<\sqrt{\lambda}\), higher uncertainty implies smaller values of \(\pi/\hat{\beta}\). This suggests samples of this type with positive rewards are underestimated, and the extend of underestimation increases with the estimation uncertainty. UIPS thus chooses to increase \(\phi_{\mathbf{x},a}^{*}\) with uncertainty, to emphasize these long-tail positive samples.
* Conversely, for samples with large propensity scores, UIPS decreases \(\phi_{\mathbf{x},a}^{*}\) as the uncertainty increases, so as to prevent their distortion in policy learning.

**Uncertainty estimation.** Now we describe how to calculate \(U_{\mathbf{x},a}\), i.e., the uncertainty of the estimated \(\hat{\beta}(a|\mathbf{x})\). In this work, we choose to estimate \(\beta^{*}(a|\mathbf{x})\) using a neural network, because 1) its representation learning capacity has been proved in numerous studies, and 2) various ways [11; 45] can be leveraged to perform the uncertainty estimation in a neural network. We adopt [45] due to its computational efficiency and theoretical soundness. Following the proof of Theorem 4.4 in [45], given the logged dataset \(D\), we can get with a high probability that there exists \(\gamma\) such that:

\[|f_{\mathbf{\theta}}(\mathbf{x}_{n},a_{n})-f_{\mathbf{\theta}^{*}}(\mathbf{x}_{n},a_{n}))|\leq \gamma\sqrt{g(\mathbf{x}_{n},a_{n})^{\top}M_{D}^{-1}\mathbf{g}(\mathbf{x}_{n},a_{n})}\]

where \(\mathbf{g}(\mathbf{x}_{n},a_{n})\) is the gradient of \(f_{\mathbf{\theta}}(\mathbf{x}_{n},a_{n})\) with respect to the neural network's last layer's parameter \(\mathbf{\theta}_{w}\subset\mathbf{\theta}\), i.e., \(\mathbf{g}(\mathbf{x}_{n},a_{n})=\nabla_{\mathbf{\theta}_{w}}f_{\mathbf{\theta}}(\mathbf{x}_{n},a _{n})\). And \(M_{D}=\sum_{n=1}^{N}\mathbf{g}(\mathbf{x}_{n},a_{n})\mathbf{g}(\mathbf{x}_{n},a_{n})^{\top}\), implying \(U_{\mathbf{x}_{n},a_{n}}=\sqrt{g(\mathbf{x}_{n},a_{n})^{\top}M_{D}^{-1}\mathbf{g}(\mathbf{x}_{n },a_{n})}\).

### Convergence of policy learning under UIPS

The following theorem provides the convergence result for UIPS, which converges to a stationary point of the expected reward function. The proof is provided in Appendix 7.9.

**Theorem 3.4**.: _Denote \(G_{\max}\) and \(\Phi\) as the maximum value of \(\|\frac{\partial\pi_{\phi}(a|\mathbf{x})}{\partial\vartheta}\|\) and \(\mathbb{E}_{\beta^{*}}\left[\frac{\pi_{\phi}^{*}(a|\mathbf{x})}{\beta^{2}(a|\mathbf{x} )}(\phi_{\mathbf{x},a}^{*})^{2}\right]\) respectively, i.e., \(\|\frac{\partial\pi_{\phi}(a|\mathbf{x})}{\partial\vartheta}\|\leq G_{\max}\) and \(\mathbb{E}_{\beta^{*}}\left[\frac{\pi_{\phi}^{*}(a|\mathbf{x})}{\beta^{2}(a|\mathbf{x} )}(\phi_{\mathbf{x},a}^{*})^{2}\right]\leq\Phi\). And denote \(V_{\max}\) as the finite maximum expected reward that can be achieved, and \(\varphi_{\max}=\max_{\mathbf{x},a}\left\{\left|\frac{\beta^{*}(a|\mathbf{x})}{\beta(a| \mathbf{x})}\phi_{\mathbf{x},a}^{*}-1\right|\right\}\). Assume that the expected reward of \(\pi_{\vartheta}\), i.e., \(V(\pi_{\vartheta})\), is a differentiable and L-smooth function w.r.t \(\vartheta\). Denote the policy parameters obtained by Eq.(5) at iteration \(k\in[K]\) as \(\vartheta_{k}\), then \(\varphi_{\max}\in(0,1)\) and_

\[\frac{1}{K}\sum_{k=1}^{K}\mathbb{E}[\|\nabla V(\pi_{\vartheta_{k}})\|]^{2}\leq \frac{2LV_{\max}}{K(1-\varphi_{\max})}+\left(L+\frac{2V_{\max}}{(1-\varphi_{ \max})}\right)\frac{G_{\max}\sqrt{\Phi}}{\sqrt{K}},\]

_where \(\nabla V(\pi_{\vartheta})\) is the true policy gradient under ground-truth logging probability, i.e., \(\nabla V(\pi_{\vartheta})=E_{\beta^{*}}[\frac{\pi_{\vartheta}(a|\mathbf{x})}{\beta^{* }(a|\mathbf{x})}r_{\mathbf{x},a}\nabla_{\vartheta}\log(\pi_{\vartheta}(a|\mathbf{x}))]\)._

Theorem 3.4 shows that, as \(K\rightarrow\infty\) and with \(1/(1-\varphi_{\max})\) and \(\Phi\) being controlled, UIPS leads policy update to converge to a stationary point where the true policy gradient \(\nabla V(\pi_{\vartheta_{k}})\) is zero. And fortunately, UIPS is effective in controlling both \(1/(1-\varphi_{\max})\) and \(\Phi\). Specifically, we denote \(\varphi_{\mathbf{x},a}=\left|\frac{\beta^{*}(a|\mathbf{x})}{\beta(a|\mathbf{x})}\phi_{\mathbf{ x},a}^{*}-1\right|\) and \(\Phi_{x,a}=\frac{\pi_{\phi}^{*}(a|\mathbf{x})}{\beta^{2}(a|\mathbf{x})}(\phi_{\mathbf{x},a}^ {*})^{2}\). It is clear to note that \(\lambda\varphi_{x,a}^{2}+\Phi_{x,a}\) corresponds to the objective in Eq.(6) for deriving \(\phi_{\mathbf{x},a}^{*}\) for each sample \((\mathbf{x},a)\). In other words, UIPS selects \(\{\phi_{\mathbf{x},a}^{*}\}\) to minimize \(\varphi_{\max}=\max\{\varphi_{x,a}\}\) and \(\Phi=\mathbb{E}_{\beta^{*}}[\Phi_{\mathbf{x},a}]\), which directly accelerate the policy converge to a stationary point with the true policy gradient being zero.

In the case of BIPS in Eq.(3), we have \(\phi_{\mathbf{x},a}\equiv 1\). Although \(\Phi\) may be large due to small logging probabilities, the more concerning issue is that the requirement \(\varphi_{\max}\in(0,1)\) is no longer satisfied when \(\beta^{*}(a|\mathbf{x})\geq 2\hat{\beta}(a|\mathbf{x})\), which may happen with a non-negligible probability. Hence, the convergence of policy learning under \(\hat{V}_{\mathrm{BIPS}}\) is no better than that under UIPS.

## 4 Empirical evaluations

We evaluate UIPS on both synthetic data and three real-world datasets with unbiased collection. We compare UIPS with the following baselines, which can be grouped into five categories:

* **Cross-Entropy (CE)**: A supervised learning method with the cross-entropy loss over its softmax output. No off-policy correction is performed in this method.
* **BIPS-Cap**[8]: The off-policy learning solution under the BIPS estimator in Eq.(3). The estimated propensity scores are further suppressed to control variance, i.e., taking \(\min\left(c,\frac{\pi_{\mathbf{\theta}}(a|\mathbf{x})}{\beta(a|\mathbf{x})}\right)\) as the propensity score. Setting \(c\) to a small value can reduce variance, but introduces bias.
* **MinVar & stableVar**[46], **Shrinkage**[36]: This line of work improves off-policy evaluation by reweighing each sample. For example, MinVar and stableVar reweigh each sample by \(\frac{h_{\mathbf{x},a}}{\sum_{\sigma^{\prime}}h_{\mathbf{x},a^{\prime}}}\) with \(h_{\mathbf{x},a}=\frac{\hat{\beta}(a|\mathbf{x})}{\pi_{\mathbf{\theta}}(a|\mathbf{x})}\) and \(h_{\mathbf{x},a}=\frac{\sqrt{\hat{\beta}(a|\mathbf{x})}}{\pi_{\mathbf{\theta}}(a|\mathbf{x})}\) respectively, since they find that \(\pi_{\mathbf{\theta}}(a|\mathbf{x})^{2}/\hat{\beta}(a|\mathbf{x})\) is directly related to policy evaluation variance. Su et al. [36] propose to shrink the propensity score by \(\lambda/(\lambda+\frac{\pi_{\mathbf{\theta}}(a|\mathbf{x})^{2}}{\beta(a|\mathbf{x})^{2}})\), which is a special case of our UIPS with \(U_{\mathbf{x},a}=0\) and \(\eta=1\). All these methods simply treat \(\hat{\beta}(a|\mathbf{x})\) as \(\beta^{*}(a|\mathbf{x})\), and none of them consider the uncertainty of \(\hat{\beta}(a|\mathbf{x})\).
* **SNIPS**[39], **BanditNet**[16], **POEM**[38], **POXM**[23], **Adaptive**[22]: This line of work aims for more stable and accurate policy learning. For example, SNIPS normalizes the estimator by the sum of propensity scores in each batch. BanditNet extends SNIPS and leverages an additional Lagrangian term to normalize the estimator by an approximated sum of propensity scores of all samples. POEM jointly optimizes the estimator and its variance. POXM controls estimation variance by pruning samples with small logging probabilities. Adaptive proposes a new formulation to utilize negative samples.
* **ApproxKNN**[5] and **IPS-C-TS**: The line of work improves off-policy learning by applying calibration to estimated logging probabilities. ApproxKNN utilizes the K-Nearest Neighbor algorithm for calibration, which exhibits the lowest calibration error in [5]. IPS-C-TS, on the other hand, employs temperature scaling, a widely recognized and effective calibration method for probability distribution [13].

[MISSING_PAGE_FAIL:7]

logging probabilities, IPS-GT still suffered from high variance caused by samples with small logging probabilities, which is the main cause of its worse performance when \(\tau=0.5\) and \(\tau=1\). In contrast, UIPS effectively controlled the negative impact of these high-variance samples, resulting in a better bias-variance trade-off.

With an increasing \(\tau\), suggesting a decrease in the probability of selecting positive actions, most algorithms experienced a drop in performance. However, UIPS consistently outperformed all other algorithms across all three datasets and metrics. Interestingly, as \(\tau\) decreases, the performance improvement of UIPS became even more pronounced, despite SNIPS, BanditNet, and POXM being designed to handle small logging probabilities of positive actions.

ApproxKNN and IPS-C-TS generally achieved better performance than BIPS-Cap, implying the effectiveness of calibration of estimated logging probabilities. However, UIPS still consistently outperformed both ApproxKNN and IPS-C-TS. The main reason is that calibration primarily focuses on adjusting the estimated probabilities to ensure on _average_ the model's predictions are reliable and accurate. In contrast, UIPS specifically handles the impact from each _individual_ sample in policy learning.

UIPS also consistently outperformed Shrinkage (a special case of UIPS with uncertainties always being zero) on all three datasets, demonstrating the benefits of considering the estimation uncertainty. Finally, blindly reweighing through uncertainties, regardless of their impact on the accuracy of the resulting estimator and the learned policy, ultimately resulted in poor performance, as demonstrated by UIPS-P and UIPS-O.

**Performance under different uncertainty levels.** As shown in Figure 1, low-frequency samples in the logged dataset suffer higher uncertainties in their propensity estimation. Thus, we divided the test set into two subsets according to the average frequency of associated actions, where the uncertainty in the subset associated with low-frequency actions is on average 8% higher than that in high-frequency actions. Table 2 shows the results on these two subsets when \(\tau=0.5\). In addition, we include the results of the top three baselines that directly utilize the estimated logging policy. Table 2 clearly demonstrates that only UIPS performed better than CE on the test set with low-frequency actions, implying the distortion of inaccurately estimated logging probabilities and the effectiveness of UIPS in efficiently handling them.

**Off-policy Evaluation.** We further inspected whether \(\hat{V}_{\mathrm{UIPS}}\) in Eq.(4) leads to more accurate off-policy evaluation. Following previous work [29; 46; 36], we evaluated the following \(\epsilon\)-greedy policy: \(\pi(a|\mathbf{x})=\frac{1-\epsilon}{|M_{x}|}\cdot\mathbb{I}\{a\in M_{x}\}+\epsilon /|\mathcal{A}|\), where \(M_{x}\) contains all positive actions associated with instance \(\mathbf{x}\). For each \(\mathbf{x}\) in the test set, we randomly sample 100 actions following the logging policy in Eq.(9) to generate the logged dataset. Table 3 shows the MSE of the estimators to the ground-truth policy value under 20 different random seeds. From Table 3, one can observe that: 1) IPS-GT with a skewer logging policy (i.e., smaller \(\tau\)) leads to higher MSE, consistent with previous findings [29; 46; 36]; 2) inaccurate logging probabilities result in high bias and variance, leading to much larger MSE of BIPS compared to IPS-GT. Furthermore, this distortion is particularly pronounced when the ground-truth logging policy is skewed (\(\tau=0.5\)); and 3) although all using the estimated logging policy, \(\hat{V}_{\mathrm{UIPS}}\) yields the smallest MSE, comparing to other baselines that are designed to improve over BIPS.

**Hyper-parameter Tuning.** Discussions about hyper-parameter tuning and performance of UIPS under different hyper-parameters can also be found in Appendix 7.3.1.

### Real-world data

To demonstrate the effectiveness of UIPS in real-world scenarios, we evaluate it on three recommendation datasets: (1) Yahoo! R31; (2) Coat2; (3) KuaiRec [12], for music, fashion and short-video recommendations respectively. All these datasets contain an unbiased test set collected from a randomized controlled trial where items are randomly selected. The statistics of the three datasets and implementation details, e.g., model architectures and dataset splits, can be found in Appendix 7.3.2.

Footnote 1: [https://webscope.sandbox.yahoo.com/](https://webscope.sandbox.yahoo.com/)

Footnote 2: [https://www.cs.cornell.edu/~schnabts/mnar/](https://www.cs.cornell.edu/~schnabts/mnar/)

Following [10], we take \(K=5\) on Yahoo! R3 and Coat datasets, and \(K=50\) on KuaiRec dataset. The \(p\)-value under the t-test between UIPS and the best baseline on each dataset is also reported to investigate the significance of improvement.

[MISSING_PAGE_FAIL:9]

However, all these solutions directly use the estimated logging policy for off-policy correction, leading to sub-optimal performance as shown in our experiments. A recent study on causal recommendation [10] also argues that propensity scores may not be correct due to unobserved confounders. They assume the effect of unobserved confounder for any sample can be bounded by a pre-defined hyperparameter, and adversarially search for the worst-case propensity for learning. Mapping to off-policy learning, their solution is a special case of our UIPS-O variant with uncertainty as a pre-defined constant.

There were existing studies [34; 21; 26; 47; 9] also explore direct estimation of the propensity ratio to bypass estimating the logging policy. However, as discussed in Appendix 7.6 and Appendix 7.5, they demonstrate inferior performance compared to UIPS. This is primarily due to either the lack of consideration for the accuracy of the estimated propensity ratio, similar to the limitations of existing IPS-type algorithms in handling inaccurately estimated logging probabilities, or the degeneration to a specific IPS estimator that suffers high variance.

Recent work on distributionally robust off-policy evaluation and learning [32; 17; 44] also addresses uncertainty in off-policy learning. However, their approach to handling uncertainty and the underlying motivation differ significantly from ours, resulting in distinct techniques employed. Further details can be found in Appendix 7.7. Additionally, experiments conducted in Appendix 7.7 demonstrate that directly adapting methods from distributionally robust off-policy learning to handle inaccurately estimated logging probabilities leads to poor performance.

Off-policy learning can also be directly built on off-policy evaluation. Several work [36; 46] also propose to control the variance of the estimator caused by small logging probabilities through instance reweighing. Again, they directly use the estimated logging policy for correction, and thus performed worse than UIPS as observed in our experiments. A recent study [29] assumed additional structure in the action space and proposed the marginalized IPS. Instead, our work considers the uncertainty in the estimated logging policy and thus does not add any new assumptions about the problem space.

**Uncertainty-aware learning.** Estimation uncertainty has been extensively studied [45; 50; 1]. In the context of on-policy reinforcement leanring and bandits [1; 48; 49], the use of uncertainty aims to strike a balance between exploration and exploitation by adopting an optimistic approach (i.e., UCB in bandits). One the other hand, most research on offline reinforcement learning/bandits [43; 4; 6] tends to be more conservative, employing techniques such as Lower Confidence Bounds (LCB) or penalizing out-of-distribution states and actions based on uncertainty to address extrapolation errors. However, these principles differ fundamentally from UIPS, which directly minimizes the mean square error of off-policy evaluation. The closed-form solution of the resulting per-instance weight in UIPS reflects how uncertainty contributes to the policy evaluation error. Moreover, Our UIPS-O and UIPS-P baselines leverage uncertainties using the two aforementioned general principles respectively. However, empirical findings indicate that blindly penalizing or boosting samples based on uncertainty is problematic. Proper correction depends on both uncertainty in logging policy estimation and the actual value of estimated logging probabilities.

## 6 Conclusion

In this paper, we propose a Uncertainty-aware Inverse Propensity Score estimator (UIPS) to explicitly model the uncertainty of the estimated logging policy for improved off-policy learning. UIPS weighs each logged instance to reduce its policy evaluation error, where the optimal weights have a closed-form solution derived by minimizing the upper bound of the resulting estimator's mean squared error (MSE) to its ground-truth value. An improved policy is then obtained by optimizing the resulting estimation. Extensive experiments on synthetic and three real-world datasets as well as the theoretical convergence guarantee demonstrate the efficiency of UIPS.

As demonstrated in this work, explicitly modeling the uncertainty of the estimated logging policy is crucial for effective off-policy learning; but the best use of this uncertainty is not to simply down-weigh or drop instances with uncertain estimations, but to balance it with the actually estimated logging probabilities in a per-instance basis. As our future work, it is promising to investigate how UIPS can be extended to value-based learning methods, e.g., actor-critics. And on the other hand, it is also important to analyze how tight our upper bound analysis of policy evaluation error is; and if possible, find new ways to tighten it for improvements.

## References

* [1] Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. In _Advances in Neural Information Processing Systems_, pages 2312-2320, 2011.
* [2] Aman Agarwal, Ivan Zaitsev, Xuanhui Wang, Cheng Li, Marc Najork, and Thorsten Joachims. Estimating position bias without intrusive interventions. In _Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining_, pages 474-482, 2019.
* [3] Ahmad Ajalloeian and Sebastian U Stich. On the convergence of sgd with biased gradients. _arXiv preprint arXiv:2008.00051_, 2020.
* [4] Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcement learning with diversified q-ensemble. _Advances in neural information processing systems_, 34:7436-7447, 2021.
* [5] Raghu Aniruddh, Gottesman Omer, Liu Yao, Komorowski Matthieu, Faisal Aldo, Doshi-Velez Finale, and Brunskill Emma. Behaviour policy estimation in off-policy policy evaluation: Calibration matters. _arXiv preprint arXiv:1807.01066_, 2018.
* [6] Chenjia Bai, Lingxiao Wang, Zhuoran Yang, Zhihong Deng, Animesh Garg, Peng Liu, and Zhaoran Wang. Pessimistic bootstrapping for uncertainty-driven offline reinforcement learning. _arXiv preprint arXiv:2202.11566_, 2022.
* [7] K. Bhatia, K. Dahiya, H. Jain, P. Kar, A. Mittal, Y. Prabhu, and M. Varma. The extreme classification repository: Multi-label datasets and code, 2016.
* [8] Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and Ed H Chi. Top-k off-policy correction for a reinforce recommender system. In _Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining_, pages 456-464, 2019.
* [9] Bo Dai, Ofir Nachum, Yinlam Chow, Lihong Li, Csaba Szepesvari, and Dale Schuurmans. Coindice: Off-policy confidence interval estimation. _Advances in neural information processing systems_, 33:9398-9411, 2020.
* [10] Sihao Ding, Peng Wu, Fuli Feng, Yitong Wang, Xiangnan He, Yong Liao, and Yongdong Zhang. Addressing unmeasured confounder for recommendation with sensitivity analysis. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 305-315, 2022.
* [11] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In _international conference on machine learning_, pages 1050-1059. PMLR, 2016.
* [12] Chongming Gao, Shijun Li, Wenqiang Lei, Jiawei Chen, Biao Li, Peng Jiang, Xiangnan He, Jiaxin Mao, and Tat-Seng Chua. Kuairec: A fully-observed dataset and insights for evaluating recommender systems. In _Proceedings of the 31st ACM International Conference on Information and Knowledge Management_, CIKM '22, 2022.
* [13] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In _International conference on machine learning_, pages 1321-1330. PMLR, 2017.
* [14] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. Deepfm: a factorization-machine based neural network for ctr prediction. _arXiv preprint arXiv:1703.04247_, 2017.
* [15] Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. In _International Conference on Machine Learning_, pages 652-661. PMLR, 2016.
* [16] Thorsten Joachims, Adith Swaminathan, and Maarten De Rijke. Deep learning with logged bandit feedback. In _International Conference on Learning Representations_, 2018.
* [17] Nathan Kallus, Xiaojie Mao, Kaiwen Wang, and Zhengyuan Zhou. Doubly robust distributionally robust off-policy evaluation and learning. In _International Conference on Machine Learning_, pages 10598-10632. PMLR, 2022.

* Kidger and Lyons [2020] Patrick Kidger and Terry Lyons. Universal approximation with deep narrow networks. In _Conference on learning theory_, pages 2306-2327. PMLR, 2020.
* Levine and Koltun [2013] Sergey Levine and Vladlen Koltun. Guided policy search. In _International conference on machine learning_, pages 1-9. PMLR, 2013.
* Li et al. [2017] Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contextual bandits. In _International Conference on Machine Learning_, pages 2071-2080. PMLR, 2017.
* Liu et al. [2021] Xu-Hui Liu, Zhenghai Xue, Jingcheng Pang, Shengyi Jiang, Feng Xu, and Yang Yu. Regret minimization experience replay in off-policy reinforcement learning. _Advances in Neural Information Processing Systems_, 34:17604-17615, 2021.
* Liu et al. [2022] Yaxu Liu, Jui-Nan Yen, Bowen Yuan, Rundong Shi, Peng Yan, and Chih-Jen Lin. Practical counterfactual policy learning for top-k recommendations. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 1141-1151, 2022.
* Lopez et al. [2021] Romain Lopez, Inderjit S Dhillon, and Michael I Jordan. Learning from extreme bandit feedback. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 8732-8740, 2021.
* Ma et al. [2020] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Ji Yang, Minmin Chen, Jiaxi Tang, Lichan Hong, and Ed H Chi. Off-policy learning in two-stage recommender systems. In _Proceedings of The Web Conference 2020_, pages 463-473, 2020.
* Munos et al. [2016] Remi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy reinforcement learning. _Advances in neural information processing systems_, 29, 2016.
* Nachum et al. [2019] Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections. _Advances in Neural Information Processing Systems_, 32, 2019.
* Precup [2000] Doina Precup. Eligibility traces for off-policy policy evaluation. _Computer Science Department Faculty Publication Series_, page 80, 2000.
* Raghu et al. [2018] Aniruddh Raghu, Omer Gottesman, Yao Liu, Matthieu Komorowski, Aldo Faisal, Finale Doshi-Velez, and Emma Brunskill. Behaviour policy estimation in off-policy policy evaluation: Calibration matters. _arXiv preprint arXiv:1807.01066_, 2018.
* Saito and Joachims [2022] Yuta Saito and Thorsten Joachims. Off-policy evaluation for large action spaces via embeddings. _arXiv preprint arXiv:2202.06317_, 2022.
* Schnabel et al. [2016] Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak, and Thorsten Joachims. Recommendations as treatments: Debiasing learning and evaluation. In _international conference on machine learning_, pages 1670-1679. PMLR, 2016.
* Schulman et al. [2015] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In _International conference on machine learning_, pages 1889-1897. PMLR, 2015.
* Si et al. [2020] Nian Si, Fan Zhang, Zhengyuan Zhou, and Jose Blanchet. Distributionally robust policy evaluation and learning in offline contextual bandits. In _International Conference on Machine Learning_, pages 8884-8894. PMLR, 2020.
* Silver et al. [2016] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. _nature_, 529(7587):484-489, 2016.
* Sinha et al. [2022] Samarth Sinha, Jiaming Song, Animesh Garg, and Stefano Ermon. Experience replay with likelihood-free importance weights. In _Learning for Dynamics and Control Conference_, pages 110-123. PMLR, 2022.

* [35] Alex Strehl, John Langford, Lihong Li, and Sham M Kakade. Learning from logged implicit exploration data. _Advances in neural information processing systems_, 23, 2010.
* [36] Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudik. Doubly robust off-policy evaluation with shrinkage. In _International Conference on Machine Learning_, pages 9167-9176. PMLR, 2020.
* [37] Adith Swaminathan and Thorsten Joachims. Batch learning from logged bandit feedback through counterfactual risk minimization. _The Journal of Machine Learning Research_, 16(1):1731-1755, 2015.
* [38] Adith Swaminathan and Thorsten Joachims. Counterfactual risk minimization: Learning from logged bandit feedback. In _International Conference on Machine Learning_, pages 814-823. PMLR, 2015.
* [39] Adith Swaminathan and Thorsten Joachims. The self-normalized estimator for counterfactual learning. _advances in neural information processing systems_, 28, 2015.
* [40] Adith Swaminathan, Akshay Krishnamurthy, Alekh Agarwal, Miro Dudik, John Langford, Damien Jose, and Imed Zitouni. Off-policy evaluation for slate recommendation. _Advances in Neural Information Processing Systems_, 30, 2017.
* [41] Sebastian Thrun and Michael L Littman. Reinforcement learning: an introduction. _AI Magazine_, 21(1):103-103, 2000.
* [42] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. _Machine learning_, 8(3):229-256, 1992.
* [43] Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua Susskind, Jian Zhang, Ruslan Salakhutdinov, and Hanlin Goh. Uncertainty weighted actor-critic for offline reinforcement learning. _arXiv preprint arXiv:2105.08140_, 2021.
* [44] Da Xu, Yuting Ye, Chuanwei Ruan, and Bo Yang. Towards robust off-policy learning for runtime uncertainty. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 10101-10109, 2022.
* [45] Pan Xu, Zheng Wen, Handong Zhao, and Quanquan Gu. Neural contextual bandits with deep representation and shallow exploration. In _International Conference on Learning Representations_, 2021.
* [46] Ruohan Zhan, Vitor Hadad, David A Hirshberg, and Susan Athey. Off-policy evaluation via adaptive weighting with data from contextual bandits. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 2125-2135, 2021.
* [47] Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans. Gendice: Generalized offline estimation of stationary values. _arXiv preprint arXiv:2002.09072_, 2020.
* [48] Xiaoying Zhang, Hong Xie, Hang Li, and John CS Lui. Conversational contextual bandit: Algorithm and application. In _Proceedings of the web conference 2020_, pages 662-672, 2020.
* [49] Xiaoying Zhang, Hong Xie, and John CS Lui. Heterogeneous information assisted bandit learning: Theory and application. In _2021 IEEE 37th International Conference on Data Engineering (ICDE)_, pages 2135-2140. IEEE, 2021.
* [50] Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural contextual bandits with ucb-based exploration. In _International Conference on Machine Learning_, pages 11492-11502. PMLR, 2020.
* [51] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. Deep interest network for click-through rate prediction. In _Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 1059-1068, 2018.

## 7 Appendix

### Notations and Algorithm Framework.

For ease of reading, we list important notations in Table 5 and summarize the main framework of the proposed UIPS in Algorithm 1.

**Computation Cost.** The additional computation cost of UIPS over IPS comes from two parts:

* Pre-calculate uncertainties (line 1-5 in Algorithm 1) : This part calculates uncertainty of the logging probability for each \((s,a)\) pair, and it _only needs to be executed once_. The computational cost of this step is \(O(Nd^{2}+d^{3})\), where \(O(Nd^{2})\) is for calculating uncertainties in each \((s,a)\) pair and \(O(d^{3})\) is for matrix inverse.
* Calculate \(\phi^{*}_{\mathbf{x},a}\) during training (line 8 in Algorithm 1): It only takes O(1) time, the same computational cost as calculating IPS score.

\begin{table}
\begin{tabular}{c|l} \hline \hline Notation & Description \\ \hline \(\mathcal{X}\) & context space \\ \hline \(\mathcal{A}\) & action set \\ \hline \(\mathbf{x}\in R^{d}\) & context vector \\ \hline \(a\) & action \\ \hline \(r_{\mathbf{x},a}\) & reward \\ \hline \(\pi(a|\mathbf{x})\) & targeted policy to evaluate \\ \hline \(\beta^{*}(a|\mathbf{x})\) & the unknown ground-truth logging policy \\ \hline \(\hat{\beta}(a|\mathbf{x})\) & the estimated logging policy \\ \hline \(V(\pi)\) & value function \\ \hline \(D:=\{(\mathbf{x}_{n},a_{n},r_{\mathbf{x}_{n,a}})|n\in[N]\}\) & logged dataset containing \(N\) samples \\ \hline \(\phi^{*}_{\mathbf{x},a}\) & the optimal uncertainty-aware weight \\ \hline \(f_{\mathbf{\theta}^{*}}(\mathbf{x},a)\) & the unknown ground-truth function \\ \hline \(f_{\mathbf{\theta}}(\mathbf{x},a)\) & the estimate of \(f_{\mathbf{\theta}^{*}}(\mathbf{x},a)\) that generates \(\hat{\beta}(a|\mathbf{x})\) \\ \hline \(B_{\mathbf{x},a}\) & confidence interval of \(\hat{\beta}(a|\mathbf{x})\) \\ \hline \(U_{\mathbf{x},a}\) & uncertainty defined as \(\left\lceil\int_{\mathbf{\theta}^{*}}(\mathbf{x},a)-f_{\mathbf{\theta}}(\mathbf{x},a)\right\rceil \leq\gamma U_{\mathbf{x},a}\) \\ \hline \(\mathbf{g}(\mathbf{x}_{n},a_{n})\) & gradient of \(f_{\mathbf{\theta}}(\mathbf{x},a)\) regarding to the last layer. \\ \hline \hline \end{tabular}
\end{table}
Table 5: NotationsNote that calculating the logging probability for each sample, which is essential for both UIPS and IPS, takes \(O(Nd|\mathcal{A}|)\) time. Since the dimension \(d\) is usually much less than action size \(|\mathcal{A}|\) and sample size \(N\), UIPS does not introduce significant computational overhead compared to the original IPS solution.

### Derivation of confidence interval of logging probability.

Given that \(|f_{\mathbf{\theta}^{*}}(\mathbf{x},a)-f_{\mathbf{\theta}}(\mathbf{x},a)|\leq\gamma U_{\mathbf{x},a}\) holds with probability at least \(1-\sigma\) and

\[\beta^{*}(a|\mathbf{x})=\frac{\exp(f_{\mathbf{\theta}^{*}}(\mathbf{x},a))}{Z^{*}},\hat{ \beta}(a|\mathbf{x})=\frac{\exp(f_{\mathbf{\theta}}(\mathbf{x},a))}{\hat{Z}},\]

where \(Z^{*}=\sum_{a^{\prime}}\exp(f_{\mathbf{\theta}^{*}}(a^{\prime}|\mathbf{x}))\) and \(\hat{Z}=\sum_{a^{\prime}}\exp(f_{\theta}(a^{\prime}|\mathbf{x}))\), we can get that with probability at least \(1-\sigma\):

\[\begin{split}&\quad|f_{\mathbf{\theta}^{*}}(\mathbf{x},a)-f_{\mathbf{\theta}}( \mathbf{x},a)|\leq\gamma U_{\mathbf{x},a}\\ \Leftrightarrow& f_{\mathbf{\theta}}(\mathbf{x},a)-\gamma U_{ \mathbf{x},a}\leq f_{\mathbf{\theta}^{*}}(\mathbf{x},a)\leq f_{\mathbf{\theta}}(\mathbf{x},a)+ \gamma U_{\mathbf{x},a}\\ \Leftrightarrow&\exp(f_{\mathbf{\theta}}(\mathbf{x},a))\exp( -\gamma U_{\mathbf{x},a})\leq\exp(f_{\mathbf{\theta}^{*}}(\mathbf{x},a))\leq\exp(f_{\mathbf{ \theta}}(\mathbf{x},a))\exp(\gamma U_{\mathbf{x},a})\\ \stackrel{{(1)}}{{\Leftrightarrow}}&\hat{Z} \hat{\beta}(a|\mathbf{x})\exp(-\gamma U_{\mathbf{x},a})\leq\exp(f_{\mathbf{\theta}^{*}}(\bm {x},a))\leq\hat{Z}\hat{\beta}(a|\mathbf{x})\exp(\gamma U_{\mathbf{x},a})\\ \stackrel{{(2)}}{{\Leftrightarrow}}&\frac{\hat {Z}\exp(-\gamma U_{\mathbf{x},a})}{Z^{*}}\hat{\beta}(a|\mathbf{x})\leq\beta^{*}(a|\bm {x})\leq\frac{\hat{Z}\exp(\gamma U_{\mathbf{x},a})}{Z^{*}}\hat{\beta}(a|\mathbf{x}) \end{split}\]

The step labeled as \((1)\) is due to the modelling of \(\hat{\beta}(a|\mathbf{x})\). And the step labeled as \((2)\) is because \(Z^{*}\) is a positive constant independent of \(\hat{\beta}(a|\mathbf{x})\). Thus with probability at least 1-\(\delta\), we have \(\beta^{*}(a|\mathbf{x})\in\mathbf{B}_{\mathbf{x},a}\) and

\[\mathbf{B}_{\mathbf{x},a}=\left[\frac{\hat{Z}\exp{(-\gamma U_{\mathbf{x},a})}}{Z^{*}}\hat{ \beta}(a|\mathbf{x}),\frac{\hat{Z}\exp{(\gamma U_{\mathbf{x},a})}}{Z^{*}}\hat{\beta}(a |\mathbf{x})\right].\]

### Experiment details

#### 7.3.1 Synthetic Data

**Data generation.** Given the ground-truth logging policy \(\beta^{*}(a|\mathbf{x})\), we generate the logged dataset as follows. For each sample in train set, we first get the embedded context vector \(\mathbf{x}\) from its original feature vector \(\tilde{\mathbf{x}}\). We then sample an action \(a\) according to \(\beta^{*}(a|\mathbf{x})\), and obtain the reward \(r_{\mathbf{x},a}=\mathbf{y}_{\tilde{\mathbf{x}},a}\), resulting bandit feedback \((\mathbf{x},a,r_{\mathbf{x},a})\), where \(\mathbf{y}_{\tilde{\mathbf{x}},a}\) is the label of class \(a\) under the original feature vector \(\tilde{\mathbf{x}}\). We repeat above process \(N\) times to collect the logged dataset. In our experiments, we take \(d=64,N=100\).

We model the logging policy as in Eq.(9), where \(\{\mathbf{\theta}_{a}\}\) are the parameters to be estimated. To train the logging policy, we take all samples in the logged dataset \(D\) as positive instances, and randomly sample non-selected actions as negative instances as in [8].

#### 7.3.2 Real-world Data

**Statistics of datasets.** The statistics of three real-world recommendation datasets with unbiased data can be found in Table 6.

All these datasets contain a set of biased data collected from users' interactions on the platform, and a set of unbiased data collected from a randomized controlled trial where items are randomly selected. As in [10], on each dataset, the biased data is used for training, and the unbiased data is for testing, with a small part of unbiased data split for validation purpose (5%

\begin{table}
\begin{tabular}{|c|r r r r|} \hline Dataset & \#User & \#Item & \#Biased Data & \#Unbiased Data \\ \hline Yahoo R3 & 15,400 & 1,000 & 311,704 & 54,000 \\ Coat & 290 & 300 & 6,960 & 4,640 \\ KuaiRec & 7,176 & 10,729 & 12,530,806 & 4,676,570 \\ \hline \end{tabular}
\end{table}
Table 6: The statistics of three real-world datasets.

and 15% on KuaiRec). We take the reward as 1 if : (1) the rating is larger than 3 in Yahoo! R3 and Coat datasets; (2) the user watched more than 70% of the video in KuaiRec. Otherwise, the reward is labeled as 0.

We adopted a two-tower neural network architecture to implement both the logging and learning policy, as shown in Figure 2. For the learning policy, the user representation and item representation are first modelled through two separate neural networks (i.e., the user tower and the item tower), and then their element-by-element product vector is projected to predict the user's preference for the item. We then re-use the user state generated from the user tower of the learning policy, and model the logging policy with another separate item tower, following [8]. We also block gradients to prevent the logging policy learning interfering the user state of the learning policy. In each learning epoch, we will first estimate the logging policy, and then take the estimated logging probabilities as well as their uncertainties to optimize the learning policy.

#### 7.3.3 Implementation details.

To facilitate hyper-parameter tuning, we disentangled two \(\eta\)s in two the terms of \(\phi^{*}_{\mathbf{x},a}\) in Theorem 3.2, and introduce \(\eta_{1}\) and \(\eta_{2}\) to represent \(\eta\) in the first and second term respectively in our implementation. This is due to the scale of \(\eta\) in the first term is closely related to the scale of \(\lambda\), with \(\lambda/\sqrt{\eta}\) as the truly effective hyper-parameter. But the scale of \(\eta\) in the second term is independent from \(\lambda\).

Moreover, while \(\lambda=\mathbb{E}_{\pi_{\mathbf{\theta}}}\left[r^{2}_{\mathbf{x},a}\frac{\pi_{\mathbf{ \theta}}(a|\mathbf{x})}{\beta^{*}(a|\mathbf{x})}\right]\) depends on \(\pi_{\theta}\) as discussed in Eq.(6), we cannot adaptively set the value of \(\lambda\) since the ground-truth logging policy \(\beta^{*}(a|\mathbf{x})\) is unknown. However, we have:

\[\lambda:=E_{\pi}\left[r^{2}_{\mathbf{x},a}\frac{\pi_{\theta}(a|\mathbf{x})}{\beta^{*} (a|\mathbf{x})}\right]\leq\left(\sum_{a}\pi_{\theta}(a|\mathbf{x})^{4}\right)\left( \sum_{a}\frac{r^{4}_{\mathbf{x},a}}{\beta^{*}(a|\mathbf{x})^{2}}\right)\leq\left(\sum _{a}\frac{r^{4}_{\mathbf{x},a}}{\beta^{*}(a|\mathbf{x})^{2}}\right),\]

where the first inequality is due to the Cauchy-Schwarz inequality and the second inequality is because \(\sum_{a}\pi_{\theta}(a|\mathbf{x})^{4}\leq\sum_{a}\pi_{\theta}(a|\mathbf{x})=1\) with \(\pi_{\theta}(a|\mathbf{x})\in[0,1]\). We denote \(\tilde{\lambda}=\left(\sum_{a}\frac{r^{4}_{\mathbf{x},a}}{\beta^{*}(a|\mathbf{x})^{2}}\right)\), which is dataset-specific constant and independent from \(\pi_{\theta}\). By replacing \(\lambda\) in Eq.(6) with \(\tilde{\lambda}\), we can still minimize an upper bound of MSE\((\hat{V}_{\mathrm{UIPS}}(\pi_{\mathbf{\theta}}))\), which ensures that the result of our analysis still holds. Thus, considering ease of computation and efficiency, we take a fixed \(\lambda\) during our policy learning.

We then use grid search to select hyperparameters based on the model's performance on the validation dataset: the learning rate was searched in {1\(e^{-5}\), \(1e^{-4}\), \(1e^{-3}\), \(1e^{-2}\)}; \(\lambda,\gamma,\eta_{1}\) were searched in {0.5, 0.1, 1, 2,5, 10, 15, 20, 25, 30, 40, 50}. And \(\eta_{2}\) was searched in {1, 10, 100, 1000}. For baseline algorithms, we also performed a similar grid search as mentioned above, and the search range follows the original papers.

**Ablation study: hyper-parameter tuning.** Although UIPS has four hyperparameters (\(\lambda\), \(\gamma\), \(\eta_{1}\), and \(\eta_{2}\)), one only needs to carefully finetune two of them, i.e., \(\gamma\) and \(\eta_{1}^{2}/\lambda\), to obtain good performance of UIPS. This is because:

Figure 2: Model architecture of the logging and the learning policy in real-world datasets

* \(\eta_{2}\) acts as a capping threshold to ensure \(\phi_{\mathbf{x},a}^{*}\leq 2\eta_{2}\) holds even when the corresponding propensity scores are very low. Hence, it should be set to a reasonably large value (e.g., 100).
* The key component (i.e., the first term) of \(\phi_{\mathbf{x},a}^{*}\) can be rewritten in the following way. While all \((\mathbf{x},a)\) pairs will be multiplied by \(\phi_{\mathbf{x},a}^{*}\), \(\eta_{1}\) in the numerator will not affect final performance too much, and the key is to find a good value of \(\eta_{1}^{2}/\lambda\) to balance the two terms in the denominator: \[\eta_{1}/\left[\exp\left(-\gamma U_{\mathbf{x},a}\right)+\frac{\eta_{1}^{2}/ \lambda\cdot\pi_{\mathbf{\phi}}(a|\mathbf{x})^{2}}{\tilde{\beta}(a|\mathbf{x})^{2}\exp \left(-\gamma U_{\mathbf{x},a}\right)}\right].\]

Thus with \(\eta_{1}\) and \(\eta_{2}\) fixed, the effect of hyper-parameter \(\gamma\) and \(\lambda\) on precision, recall as well as NDCG can be found in Figure 3. We can observe that to make UIPS excel, \(\mathbf{B}_{\mathbf{x},a}\) needs to be of high confidence, e.g., \(\gamma=25\) performed the best on the dataset with \(\tau=0.5\). Moreover, the threshold \(\sqrt{\lambda}/\eta_{1}\) cannot be too small or too large.

### Experiments on the doubly robust estimators.

The doubly robust (DR) estimator [15], which is a hybrid of _direct method_ (DM) estimator and _inverse propensity score_ (IPS) estimator, is also widely used for off-policy evaluation. More specifically, let \(\hat{\eta}:\mathcal{X}\times\mathcal{A}\to R\) be the imputation model in DM that estimates the reward of action \(a\) under context vector \(\mathbf{x}\), and \(\hat{\beta}(a|\mathbf{x})\) be the estimated logging policy in the IPS estimator. The DR estimator evaluates policy \(\pi\) based on the logged dataset \(D:=\{(\mathbf{x}_{n},a_{n},r_{\mathbf{x}_{n},a_{n}})|n\in[N]\}\), by:

\[\hat{V}_{\mathrm{DR}}(\pi)=\hat{V}_{\mathrm{DM}}(\pi)+\frac{1}{N}\sum_{n=1}^{N }\frac{\pi(a_{n}|\mathbf{x}_{n})}{\tilde{\beta}(a_{n}|\mathbf{x}_{n})}\big{(}r_{\mathbf{x} _{n},a_{n}}-\hat{\eta}(\mathbf{x}_{n},a_{n})\big{)} \tag{10}\]

where \(\hat{V}_{\mathrm{DM}}(\pi)\) is the DM estimator:

\[\hat{V}_{\mathrm{DM}}(\pi)=\frac{1}{N}\sum_{n=1}^{N}\sum_{a\in\mathcal{A}}\pi( a|\mathbf{x}_{n})\hat{\eta}(\mathbf{x}_{n},a). \tag{11}\]

Again assume the policy \(\pi(a|\mathbf{x})\) is parameterized by \(\mathbf{\vartheta}\), the REINFORCE gradient of \(\hat{V}_{\mathrm{DR}}(\pi_{\mathbf{\vartheta}})\) with respect to \(\mathbf{\vartheta}\) can be readily derived as follows:

\[\nabla_{\mathbf{\vartheta}}\hat{V}_{\mathrm{DR}}(\pi_{\mathbf{\vartheta}})= \frac{1}{N}\sum_{n=1}^{N}\left(\sum_{a\in\mathcal{A}}\pi_{\mathbf{ \vartheta}}(a|\mathbf{x}_{n})\hat{\eta}(\mathbf{x}_{n},a)\nabla_{\mathbf{\vartheta}}\log( \pi_{\mathbf{\vartheta}}(a|\mathbf{x}_{n}))\right)\] \[+\frac{1}{N}\sum_{n=1}^{N}\left(\frac{\pi(a_{n}|\mathbf{x}_{n})}{ \tilde{\beta}(a_{n}|\mathbf{x}_{n})}\big{(}r_{\mathbf{x}_{n},a_{n}}-\hat{\eta}(\mathbf{x} _{n},a_{n})\big{)}\nabla_{\mathbf{\vartheta}}\log(\pi_{\mathbf{\vartheta}}(a_{n}|\mathbf{x }_{n})\right). \tag{12}\]

The imputation model \(\hat{\eta}(\mathbf{x},a)\) is pre-trained following previous work [22] with the same neural network architecture as the logging policy model. Besides the standard DR estimator, we also adapt

Figure 3: Effect of \(\lambda\) and \(\gamma\) on synthetic dataset with \(\tau=0.5\)

[MISSING_PAGE_FAIL:18]

[MISSING_PAGE_FAIL:19]

investigate the significance of improvements. Notably, UIPS consistently outperformed IPS-LFiW with statistically significant improvements. One major reason for the worse performance of IPS-LFiW is that it does not consider the accuracy of the estimated propensity ratio, in a direct analogy to failing to handle uncertainty in the estimated logging probabilities in existing IPS-type algorithms.

### Comparison against distributionally robust off-policy evaluation and learning.

Our work is fundamentally different from the line of work on distributionally robust off-policy evaluation and learning [32; 17; 44]. This results in different objectives that guide the use of min-max optimization.

Specifically, the work in [32; 17; 44] assumes unknown changes exist between their training and deployment environments, such as user preference drift or unforeseen events during policy execution. Thus they choose to maximize the policy value (e.g., \(\hat{V}_{\mathrm{IPS}}(\pi)\)) in the worst environment within an uncertainty set around the training environment,

\[\max_{\pi}\min_{\mathcal{U}}\quad\hat{V}_{\mathrm{IPS}}(\pi).\]

As a result, their uncertainty set is created by introducing a small perturbation to the training environment. For example, the work in [32; 17] searches the worst environment \(\mathcal{P}_{\infty}\) in the \(\sigma\)-close perturbed environments around the training environment \(\mathcal{P}_{0}\) (Eq.(1) in [17]):

\[\mathcal{U}(\sigma)=\{\mathcal{P}_{1}:\mathcal{P}_{1}<<\mathcal{P}_{0}\quad \text{and}\quad D_{KL}(\mathcal{P}_{1}||\mathcal{P}_{0})\leq\sigma\}.\]

And the work in [44] adversarially perturbs the known ground-truth logging policy \(\pi_{0}(\cdot|\cdot)\) in searching for the worst case (Eq.(4) in [44]):

\[\mathcal{U}(\alpha)=\{\pi_{u}:\max_{a,x}\quad\max\{\frac{\pi_{u}(a|x)}{\pi_{0} (a|x)},\frac{\pi_{0}(a|x)}{\pi_{u}(a|x)}\}\leq e^{\alpha}\}.\]

In contrast, UIPS assumes the training and deployment environments stay the same, but the ground-truth logging policy is unknown. To control the high bias and high variance caused by inaccurately and small estimated logging probabilities, UIPS explicitly models the uncertainty in the estimated logging policy by incorporating a per-sample weight \(\phi_{\pi,a}\) as discussed in Eq.(4). In order to make \(\hat{V}_{\mathrm{UIPS}}(\pi_{\mathbf{\sigma}})\) as accurate as possible despite the unknown ground-truth logging policy \(\beta^{*}(\cdot|\cdot)\), UIPS solves a min-max optimization problem in Eq.(8). This optimization problem seeks to find the optimal \(\phi_{\mathbf{x},a}\) that minimizes the upper bound of the mean squared error (MSE) of \(\hat{V}_{\mathrm{UIPS}}\) to its ground-truth value, within an uncertainty set of the unknown ground-truth logging policy \(\beta^{*}(\cdot|\cdot)\). The closed-form solution for the min-max optimization is also derived as in Theorem 3.2.

Furthermore, observing that work in [44] also performs optimization over an uncertainty set of the logging policy, we further adapted their method to handle the inaccuracy of the estimated logging policy, by taking \(\pi_{0}(\cdot|\cdot)\) as the estimated logging policy, i.e., \(\pi_{0}(\cdot|\cdot)=\hat{\beta}(\cdot|\cdot)\). We name the adapted methods as IPS-UN. Table 12 demonstrates the performance of the learned policy under IPS-UN on three synthetic datasets. The results suggest that directly applying IPS-UN to handle inaccurately estimated logging probabilities is not be a feasible solution to our problem. One important reason for the worse performance of IPS-UN is that it strives to optimize for the worst potential environment, which might not be the case in our experiment datasets. On the other hand, UIPS assumes the training and deployment environments stay same and strives to identify the optimal policy with an unknown ground-truth logging policy.

### Theoretical Proofs.

**Proof of Proposition 2.1:**

\begin{table}
\begin{tabular}{|l|c c|c c|c c|c c|} \hline  & \multicolumn{3}{c|}{\(\tau=0.5\)} & \multicolumn{3}{c|}{\(\tau=1\)} & \multicolumn{3}{c|}{\(\tau=2\)} \\ \hline Algorithm & P\&S & R\(\sharp\) & NDCG\(\sharp\) & P\&S & R\(\sharp\) & NDCG\(\sharp\) & P\&S & NDCCG\(\sharp\) \\ \hline IPS-LFiW & 0.542\(\pm\)1\(\times\) & 0.1568\(\pm\) & 0.6031\(\pm\) & 0.4721\(\pm\) & 0.5491\(\pm\) & 0.5795\(\pm\) & 0.5253\(\pm\) & 0.1453\(\pm\) & 0.57

[MISSING_PAGE_FAIL:21]

We first bound the bias term:

\[\mathrm{Bias}(\hat{V}_{\mathrm{UIPS}}(\pi_{\mathbf{\sigma}})) =\mathbb{E}_{D}\left[\hat{V}_{\mathrm{UIPS}}(\pi_{\mathbf{\theta}})-V( \pi_{\mathbf{\sigma}})\right]\] \[\stackrel{{(1)}}{{=}}\mathbb{E}_{\beta^{*}}\left[ \frac{\pi_{\mathbf{\sigma}}(a|\mathbf{x})}{\hat{\beta}(a|\mathbf{x})}\phi_{\mathbf{x},a}r_{\bm {x},a}\right]-V(\pi_{\mathbf{\theta}})\] \[=\mathbb{E}_{\beta^{*}}\left[\frac{\pi_{\mathbf{\sigma}}(a|\mathbf{x})}{ \hat{\beta}(a|\mathbf{x})}\phi_{\mathbf{x},a}r_{\mathbf{x},a}-\frac{\pi_{\mathbf{\sigma}}(a| \mathbf{x})}{\hat{\beta}^{*}(a|\mathbf{x})}r_{\mathbf{x},a}\right]\] \[=\mathbb{E}_{\beta^{*}}\left[r_{\mathbf{x},a}\frac{\pi_{\mathbf{\theta}}( a|\mathbf{x})}{\beta^{*}(a|\mathbf{x})}\cdot\left(\frac{\beta^{*}(a|\mathbf{x})}{\hat{ \beta}(a|\mathbf{x})}\phi_{\mathbf{x},a}-1\right)\right]\] \[\stackrel{{(2)}}{{\leq}}\sqrt{\mathbb{E}_{\pi_{\mathbf{ \theta}}}\left[r_{\mathbf{x},a}^{2}\frac{\pi_{\mathbf{\theta}}(a|\mathbf{x})}{\beta^{*}(a| \mathbf{x})}\right]}\cdot\sqrt{\mathbb{E}_{\beta^{*}}\left[\left(\frac{\beta^{*}(a |\mathbf{x})}{\hat{\beta}(a|\mathbf{x})}\phi_{\mathbf{x},a}-1\right)^{2}\right]}\]

Step (1) follows the linearity of expectation and step (2) is due to the Cauchy-Schwarz inequality. We then bound the variance term:

\[\mathrm{Var}(\hat{V}_{\mathrm{UIPS}}(\pi_{\mathbf{\theta}})) =\frac{1}{N}\mathrm{Var}_{\beta^{*}}\left(\frac{\pi_{\mathbf{\theta}} (a|\mathbf{x})}{\hat{\beta}(a|\mathbf{x})}\phi_{\mathbf{x},a}r_{\mathbf{x},a}\right)\] \[=\frac{1}{N}\left(\mathbb{E}_{\beta^{*}}\left[\frac{\pi_{\mathbf{ \sigma}}(a|\mathbf{x})^{2}}{\hat{\beta}(a|\mathbf{x})^{2}}\phi_{\mathbf{x},a}^{2}r_{\mathbf{x},a}^{2}\right]-\left(\mathbb{E}_{\beta^{*}}\left[\frac{\pi_{\mathbf{\theta}}(a|\bm {x})}{\hat{\beta}(a|\mathbf{x})}\phi_{\mathbf{x},a}r_{\mathbf{x},a}\right]\right)^{2}\right)\] \[\leq\frac{1}{N}\mathbb{E}_{\beta^{*}}\left[\frac{\pi_{\mathbf{\theta }}(a|\mathbf{x})^{2}}{\hat{\beta}(a|\mathbf{x})^{2}}\phi_{\mathbf{x},a}^{2}r_{\mathbf{x},a}^{2} \right]\leq\mathbb{E}_{\beta^{*}}\left[\frac{\pi_{\mathbf{\theta}}(a|\mathbf{x})^{2}}{ \hat{\beta}(a|\mathbf{x})^{2}}\phi_{\mathbf{x},a}^{2}\right]\]

Combining the bound of bias and variance completes the proof. 

**Proof of Theorem 3.2:**

Proof.: We first define several notations:

* \(T(\phi_{\mathbf{x},a},\beta_{\mathbf{x},a})=\lambda\mathbb{E}_{\beta^{*}}\left[\left( \frac{\beta_{\mathbf{x},a}}{\hat{\beta}(a|\mathbf{x})}\phi_{\mathbf{x},a}-1\right)^{2} \right]+\mathbb{E}_{\beta^{*}}\left[\frac{\pi_{\mathbf{\theta}}(a|\mathbf{x})^{2}}{ \hat{\beta}(a|\mathbf{x})^{2}}\phi_{\mathbf{x},a}^{2}\right]\).
* \(\tilde{T}(\phi_{\mathbf{x},a})=\max_{\beta_{\mathbf{x},a}\in\mathbf{B}_{\mathbf{x},a}}T(\phi_ {\mathbf{x},a},\beta_{\mathbf{x},a})\) denotes the maximum value of inner optimization problem.
* \(T^{*}=\min_{\phi_{\mathbf{x},a}}\tilde{T}(\phi_{\mathbf{x},a})=\min_{\phi_{\mathbf{x},a}} \max_{\beta_{\mathbf{x},a}\in\mathbf{B}_{\mathbf{x},a}}T(\phi_{\mathbf{x},a},\beta_{\mathbf{x},a})\) denote the optimal min-max value. And \(\phi_{\mathbf{x},a}^{*}=\arg\min_{\phi_{\mathbf{x},a}}\tilde{T}(\phi_{\mathbf{x},a})\).
* \(\mathbf{B}_{\mathbf{x},a}^{-}:=\frac{\tilde{Z}\exp(-\gamma U_{\mathbf{x},a})}{Z^{*}}\hat{ \beta}(a|\mathbf{x})\), and \(\mathbf{B}_{\mathbf{x},a}^{+}:=\frac{\tilde{Z}\exp(\gamma U_{\mathbf{x},a})}{Z^{*}}\hat{ \beta}(a|\mathbf{x})\).

We first find the maximum value of the inner optimization problem, i.e., \(\tilde{T}(\phi_{\mathbf{x},a})\) for any fixed \(\phi_{\mathbf{x},a}\). And there are three cases shown in Figure 4:

**Case I:** When \(\frac{\hat{\beta}(a|\mathbf{x})}{\phi_{\mathbf{x},a}}\geq\mathbf{B}_{\mathbf{x},a}^{+}\),, \(\tilde{T}(\phi_{\mathbf{x},a})\) achieves the maximum value at \(\beta_{\mathbf{x},a}=\mathbf{B}_{\mathbf{x},a}^{-}\). In other words, \(\tilde{T}(\phi_{\mathbf{x},a})=T(\phi_{\mathbf{x},a},\mathbf{B}_{\mathbf{x},a}^{-})\) when \(\phi_{\mathbf{x},a}\leq\frac{\hat{\beta}(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}^{+}}\).

**Case II:** When \(\mathbf{B}_{\mathbf{x},a}^{-}\leq\frac{\hat{\beta}(a|\mathbf{x})}{\phi_{\mathbf{x},a}}\leq\mathbf{ B}_{\mathbf{x},a}^{+}\), i.e., \(\frac{Z^{*}\exp(-\gamma U_{\mathbf{x},a})}{\tilde{Z}}\leq\phi_{\mathbf{x},a}\leq\frac{Z ^{*}\exp(\gamma U_{\mathbf{x},a})}{\tilde{Z}}\), then \(\tilde{T}(\phi_{\mathbf{x},a})\) will be the maximum between \(T(\phi_{\mathbf{x},a},\mathbf{B}_{\mathbf{x},a}^{-})\) and \(T(\phi_{\mathbf{x},a},\mathbf{B}_{\mathbf{x},a}^{+})\).

More specifically, when \(\frac{\hat{\beta}(a|\mathbf{x})}{\phi_{\mathbf{x},a}}\leq\frac{\mathbf{B}_{\mathbf{x},a}^{+}+\bm {B}_{\mathbf{x},a}^{-}}{\mathbf{B}_{\mathbf{x},a}^{+}+\mathbf{B}_{\mathbf{x},a}^{-}}\), \(\tilde{T}(\phi_{\mathbf{x},a})=T(\phi_{\mathbf{x},a},\mathbf{B}_{\mathbf{x},a}^{+})\). Otherwise when \(\phi_{\mathbf{x},a}<\frac{2\hat{\beta}(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}^{+}+\mathbf{B}_{\bm {x},a}^{-}}\), \(\tilde{T}(\phi_{\mathbf{x},a})=T(\phi_{\mathbf{x},a},\mathbf{B}_{\mathbf{x},a}^{-})\).

**Case III:** When \(\phi_{\mathbf{x},a}\geq\frac{\beta(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}}\). implying \(\frac{\beta(a|\mathbf{x})}{\phi_{\mathbf{x},a}}\leq\mathbf{B}_{\mathbf{x},a}^{-}\), \(\tilde{T}(\phi_{\mathbf{x},a})=T(\phi_{\mathbf{x},a},\mathbf{B}_{\mathbf{x},a}^{+})\).

Overall, we get that:

\[\tilde{T}(\phi_{\mathbf{x},a})=\left\{\begin{array}{ll}T(\phi_{\mathbf{x},a},\mathbf{B}_ {\mathbf{x},a}^{-}),&\phi_{\mathbf{x},a}\in(-\infty,\frac{2\hat{\beta}(a|\mathbf{x})}{\bm {B}_{\mathbf{x},a}^{+}+\mathbf{B}_{\mathbf{x},a}^{-}}]\\ T(\phi_{\mathbf{x},a},\mathbf{B}_{\mathbf{x},a}^{+})&\phi_{\mathbf{x},a}\in[\frac{2\hat{\beta}( a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}^{+}+\mathbf{B}_{\mathbf{x},a}^{-}},\infty)\end{array}\right. \tag{15}\]

Next we try to find the minimum value of \(\tilde{T}(\phi_{\mathbf{x},a})\). We first observe that without considering constraint on \(\phi_{\mathbf{x},a}\), when

\[\phi_{\mathbf{x},a}^{+}=\frac{\lambda}{\lambda\frac{\mathbf{B}_{\mathbf{x},a}^{+}}{\hat{ \beta}(a|\mathbf{x})}+\frac{\pi_{\mathbf{\theta}}(a|\mathbf{x})^{2}}{\hat{\beta}(a|\mathbf{x}) \mathbf{B}_{\mathbf{x},a}^{+}}}\]

\(T(\phi_{\mathbf{x},a},\mathbf{B}_{\mathbf{x},a}^{+})\) achieves the global minimum value. However, \(\phi_{\mathbf{x},a}^{+}\leq\frac{\hat{\beta}(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}^{+}}\leq \frac{2\hat{\beta}(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}^{-}+\mathbf{B}_{\mathbf{x},a}}\), which implies when \(\phi_{\mathbf{x},a}\in\left[\frac{2\hat{\beta}(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}^{2}+ \mathbf{B}_{\mathbf{x},a}^{-}},\infty\right)\), the minimum value of \(T(\phi_{\mathbf{x},a},\mathbf{B}_{\mathbf{x},a}^{+})\) is achieved at \(\frac{2\hat{\beta}(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}^{+}+\mathbf{B}_{\mathbf{x},a}^{-}}\).

On the other hand, without considering any constraint on \(\phi_{\mathbf{x},a}\), the global minimum value of \(T(\phi_{\mathbf{x},a},\mathbf{B}_{\mathbf{x},a}^{-})\) is achieved at:

\[\phi_{\mathbf{x},a}^{-}=\frac{\lambda}{\lambda\frac{\mathbf{B}_{\mathbf{x},a}^{-}}{\hat{ \beta}(a|\mathbf{x})}+\frac{\pi_{\mathbf{\theta}}(a|\mathbf{x})^{2}}{\hat{\beta}(a|\mathbf{x}) \mathbf{B}_{\mathbf{x},a}^{-}}}. \tag{16}\]

Thus if \(\phi_{\mathbf{x},a}^{-}\leq\frac{2\hat{\beta}(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}^{2}+\mathbf{ B}_{\mathbf{x},a}^{-}}\), \(\phi_{\mathbf{x},a}^{*}=\phi_{\mathbf{x},a}^{-}\), since \(T(\phi_{\mathbf{x},a}^{-},\mathbf{B}_{\mathbf{x},a}^{-})\leq T(\frac{2\hat{\beta}(a|\mathbf{x}) }{\mathbf{B}_{\mathbf{x},a}^{2}+\mathbf{B}_{\mathbf{x},a}^{-}},\mathbf{B}_{\mathbf{x},a}^{-})=T(\frac{ 2\hat{\beta}(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}^{2}+\mathbf{B}_{\mathbf{x},a}^{-}},\mathbf{B}_{ \mathbf{x},a}^{+})\). Otherwise, when \(\phi_{\mathbf{x},a}^{-}>\frac{2\hat{\beta}(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}^{2}+\mathbf{B}_ {\mathbf{x},a}^{-}}\), the minimum value of \(T(\phi_{\mathbf{x},a},\mathbf{B}_{\mathbf{x},a}^{-})\) is also achieved at \(\frac{2\hat{\beta}(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}^{*}+\mathbf{B}_{\mathbf{x},a}^{-}}\), implying \(\phi_{\mathbf{x},a}^{*}=\frac{2\hat{\beta}(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}^{*}+\mathbf{B}_ {\mathbf{x},a}^{-}}\).

Overall,

\[\phi_{\mathbf{x},a}^{*}=\min\left(\frac{\lambda}{\lambda\frac{\mathbf{B}_{\mathbf{x},a}^{ -}}{\hat{\beta}(a|\mathbf{x})}+\frac{\pi_{\mathbf{\theta}}(a|\mathbf{x})^{2}}{\hat{\beta}( a|\mathbf{x})^{2}\exp(-\gamma U_{\mathbf{x},a})}},\frac{2\hat{\beta}(a|\mathbf{x})}{\mathbf{B}_{ \mathbf{x},a}^{*}+\mathbf{B}_{\mathbf{x},a}^{-}}\right) \tag{17}\]

Let \(\eta=\frac{Z^{*}}{Z}\), we can get

\[\phi_{\mathbf{x},a}^{*}=\min\left(\frac{\lambda}{\frac{\lambda}{\eta}\exp\left(- \gamma U_{\mathbf{x},a}\right)+\frac{\eta\pi_{\mathbf{\theta}}(a|\mathbf{x})^{2}}{\hat{ \beta}(a|\mathbf{x})^{2}\exp(-\gamma U_{\mathbf{x},a})}},\frac{2\eta}{\exp\left(\gamma U _{\mathbf{x},a}\right)+\exp\left(-\gamma U_{\mathbf{x},a}\right)}\right). \tag{18}\]

Figure 4: Three cases for maximizing the inner optimization problem.

Note that \(\eta\in[\exp(-\gamma U_{s}^{\max}),\exp(\gamma U_{s}^{\max})]\), since \(\hat{Z}\exp(-\gamma U_{s}^{\max})\leq Z^{*}=\sum_{a^{\prime}}\exp(f_{\theta^{*}} (a^{\prime}|\mathbf{x}))\leq\hat{Z}\exp(U_{s}^{\max})\).

This completes the proof. 

**Proof of Corollary 3.3:**

Proof.: Following the similar procedure as in Theorem 3.1, we can derive the mean squared error (MSE) between \(\hat{V}_{\rm BIPS}(\pi_{\mathbf{\theta}})\) and ground-truth estimator \(V(\pi_{\mathbf{\theta}})\) is upper bounded as follows:

\[\mathrm{MSE}\left(\hat{V}_{\rm BIPS}(\pi_{\mathbf{\theta}})\right) =\mathbb{E}_{D}\left[\left(\hat{V}_{\rm BIPS}(\pi_{\mathbf{\theta}})- V(\pi_{\mathbf{\theta}})\right)^{2}\right]\] \[\leq\mathbb{E}_{\pi_{\mathbf{\theta}}}\left[r_{\mathbf{x},a}^{2}\frac{\pi _{\mathbf{\theta}}(a|\mathbf{x})}{\beta^{*}(a|\mathbf{x})}\right]\cdot\mathbb{E}_{\beta^{ *}}\left[\left(\frac{\beta^{*}(a|\mathbf{x})}{\hat{\beta}(a|\mathbf{x})}-1\right)^{2} \right]+\mathbb{E}_{\beta^{*}}\left[\frac{\pi_{\mathbf{\theta}}(a|\mathbf{x})^{2}}{ \hat{\beta}(a|\mathbf{x})^{2}}\right].\]

When \(\beta^{*}(a|\mathbf{x})\in[\mathbf{B}_{\mathbf{x},a}^{-},\mathbf{B}_{\mathbf{x},a}^{+}]\), with \(\mathbf{B}_{\mathbf{x},a}^{-}:=\frac{\hat{Z}\exp(-\gamma U_{\mathbf{x},a})}{Z^{*}}\hat{ \beta}(a|\mathbf{x})\) and \(\mathbf{B}_{\mathbf{x},a}^{+}:=\frac{\hat{Z}\exp(\gamma U_{\mathbf{x},a})}{Z^{*}}\hat{\beta }(a|\mathbf{x})\), \(\mathrm{MSE}\left(\hat{V}_{\rm BIPS}(\pi_{\mathbf{\theta}})\right)\) can be further upper bounded as follows.

For ease of illustration, we set \(\lambda=\mathbb{E}_{\pi_{\mathbf{\theta}}}\left[r_{\mathbf{x},a}^{2}\frac{\pi_{\mathbf{ \theta}}(a|\mathbf{x})}{\beta^{*}(a|\mathbf{x})}\right]\) and

\[T(\phi_{\mathbf{x},a},\beta_{\mathbf{x},a})=\lambda\mathbb{E}_{\beta^{*}}\left[\left( \frac{\beta_{\mathbf{x},a}}{\hat{\beta}(a|\mathbf{x})}\phi_{\mathbf{x},a}-1\right)^{2} \right]+\mathbb{E}_{\beta^{*}}\left[\frac{\pi_{\mathbf{\theta}}(a|\mathbf{x})^{2}}{ \hat{\beta}(a|\mathbf{x})^{2}}\phi_{\mathbf{x},a}^{2}\right].\]

Let \(T_{\rm BIPS}^{*}\) denote the upper bound of \(\mathrm{MSE}\left(\hat{V}_{\rm BIPS}(\pi_{\mathbf{\theta}})\right)\), then we can derive that

\[T_{\rm BIPS}^{*}=\left\{\begin{array}{ll}T(1,\mathbf{B}_{\mathbf{x},a}^{+}),&\hat{ \beta}(a|\mathbf{x})\leq\frac{\mathbf{B}_{\mathbf{x},a}^{+}+\mathbf{B}_{\mathbf{x},a}^{-}}{2}\\ T(1,\mathbf{B}_{\mathbf{x},a}^{-}),&\hat{\beta}(a|\mathbf{x})>\frac{\mathbf{B}_{\mathbf{x},a}^{+}+ \mathbf{B}_{\mathbf{x},a}^{-}}{2}.\end{array}\right. \tag{19}\]

Recall that in Theorem 3.2, we show that the upper bound of \(\mathrm{MSE}\left(\hat{V}_{\rm UIPS}(\pi_{\mathbf{\theta}})\right)\) is as follows:

\[T_{\rm UIPS}^{*}=\left\{\begin{array}{ll}T(\phi_{\mathbf{x},a}^{-},\mathbf{B}_{\mathbf{x },a}^{-}),&\phi_{\mathbf{x},a}^{-}<\frac{2\hat{\beta}(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}^ {2}+\mathbf{B}_{\mathbf{x},a}^{-}}\\ T(\frac{2\hat{\beta}(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}^{-}+\mathbf{B}_{\mathbf{x},a}^{-}}, \mathbf{B}_{\mathbf{x},a}^{+}),&\phi_{\mathbf{x},a}^{-}\geq\frac{2\hat{\beta}(a|\mathbf{x})}{ \mathbf{B}_{\mathbf{x},a}^{2}+\mathbf{B}_{\mathbf{x},a}^{-}}.\end{array}\right. \tag{20}\]

where \(\phi_{\mathbf{x},a}^{-}\) is defined in Eq.(16). Next we show that \(T_{\rm UIPS}^{*}\geq T_{\rm BIPS}^{*}\).

* When \(\hat{\beta}(a|\mathbf{x})\leq\frac{\mathbf{B}_{\mathbf{x},a}^{+}+\mathbf{B}_{\mathbf{x},a}^{-}}{2}\), we can get \(T_{\rm BIPS}^{*}=T(1,\mathbf{B}_{\mathbf{x},a}^{+})\) and \(\frac{2\hat{\beta}(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}^{2}+\mathbf{B}_{\mathbf{x},a}^{-}}\leq 1\). If \(\phi_{\mathbf{x},a}^{-}<\frac{2\hat{\beta}(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}^{+}+\mathbf{B} _{\mathbf{x},a}^{-}}\), then \(T_{\rm UIPS}^{*}=T(\phi_{\mathbf{x},a}^{-},\mathbf{B}_{\mathbf{x},a}^{-})<T(\frac{2\hat{ \beta}(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}^{2}+\mathbf{B}_{\mathbf{x},a}^{-}},\mathbf{B}_{\mathbf{x}, a}^{-})=T(\frac{2\hat{\beta}(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}^{2}+\mathbf{B}_{\mathbf{x},a}^{-}}, \mathbf{B}_{\mathbf{x},a}^{+})\leq T(1,\mathbf{B}_{\mathbf{x},a}^{+})\). And if \(\phi_{\mathbf{x},a}^{-}\geq\frac{2\hat{\beta}(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}^{2}+\mathbf{ B}_{\mathbf{x},a}^{-}}\), \(T_{\rm UIPS}^{*}=T(\frac{2\hat{\beta}(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}^{2}+\mathbf{B}_{\mathbf{ x},a}^{-}},\mathbf{B}_{\mathbf{x},a}^{+})\leq T(1,\mathbf{B}_{\mathbf{x},a}^{+})\);
* When \(\hat{\beta}(a|\mathbf{x})>\frac{\mathbf{B}_{\mathbf{x},a}^{+}+\mathbf{B}_{\mathbf{x},a}^{-}}{2}\), we can get \(T_{\rm BIPS}^{*}=T(1,\mathbf{B}_{\mathbf{x},a}^{-})\) and \(\frac{2\hat{\beta}(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}^{2}+\mathbf{B}_{\mathbf{x},a}^{-}}>1\). If \(\phi_{\mathbf{x},a}^{-}<\frac{2\hat{\beta}(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}^{-}+\mathbf{B} _{\mathbf{x},a}^{-}}\), then \(T_{\rm UIPS}^{*}=T(\phi_{\mathbf{x},a}^{-},\mathbf{B}_{\mathbf{x},a}^{-})\leq T(1,\mathbf{B}_{ \mathbf{x},a}^{-})\), since \(\phi_{\mathbf{x},a}^{-}\) is a global minimum. Otherwise when \(\phi_{\mathbf{x},a}^{-}\geq\frac{2\hat{\beta}(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}^{2}+\mathbf{B} _{\mathbf{x},a}^{-}}>1\), \(T_{\rm UIPS}^{*}=T(\frac{2\hat{\beta}(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}^{2}+\mathbf{B} _{\mathbf{x},a}^{-}},\mathbf{B}_{\mathbf{x},a}^{+})=T(\frac{2\hat{\beta}(a|\mathbf{x})}{\mathbf{B}_{ \mathbf{x},a}^{2}+\mathbf{B}_{\mathbf{x},a}^{-}},\mathbf{B}_{\mathbf{x},a}^{-})<T(1,\mathbf{B}_{\mathbf{x},a}^{-})\).

In both cases, we have \(T_{\rm UIPS}^{*}\leq T_{\rm BIPS}^{*}\), thus completing the proof.

**Lemma 7.1**.: _Under fixed \(\pi_{\mathbf{\phi}}(a|\mathbf{x})\) and \(\hat{\beta}(a|\mathbf{x})\), and \(\alpha_{\mathbf{x},a}=\sqrt{\frac{\lambda}{2\eta^{2}}-\frac{\lambda(1-\eta)}{\eta^{2} }\exp(-2\gamma U_{\mathbf{x},a})}\), we have the following observations:_

* _If_ \(\frac{\pi_{\mathbf{\phi}}(a|\mathbf{x})}{\beta(a|\mathbf{x})}\leq\alpha_{\mathbf{x},a},\ \phi_{ \mathbf{x},a}^{*}=2\eta/\left[\exp(\gamma U_{\mathbf{x},a})+\exp(-\gamma U_{\mathbf{x},a})\right]\)_. Otherwise_ \(\phi_{\mathbf{x},a}^{*}=\lambda/\left[\frac{\eta}{\eta}\exp\left(-\gamma U_{\mathbf{x},a}\right)+\frac{\eta\pi_{\mathbf{\phi}}(a|\mathbf{x})^{2}}{\beta^{2}(a|\mathbf{x})\exp(- \gamma U_{\mathbf{x},a})}\right]\right]\)_. In other words,_ \(\phi_{\mathbf{x},a}^{*}\leq 2\eta\) _always holds._
* _If_ \(\alpha_{\mathbf{x},a}\leq\frac{\pi_{\mathbf{\phi}}(a|\mathbf{x})}{\beta(a|\mathbf{x})}\) _and_ \(\frac{\pi_{\mathbf{\phi}}(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}}<\sqrt{\lambda}\)_, larger_ \(U_{\mathbf{x},a}\) _brings larger_ \(\phi_{\mathbf{x},a}^{*}\)_._
* _Otherwise_ \(\phi_{\mathbf{x},a}^{*}\) _decreases as_ \(U_{\mathbf{x},a}\) _increases._

Proof.: The following inequality validates the first observation:

\[\frac{\lambda}{\frac{\lambda}{\eta}\exp\left(-\gamma U_{\mathbf{x},a} \right)+\frac{\eta\pi_{\mathbf{\phi}}(a|\mathbf{x})^{2}}{\beta^{2}(a|\mathbf{x})\exp(- \gamma U_{\mathbf{x},a})}}\leq\frac{2\eta}{\exp\left(\gamma U_{\mathbf{x},a}\right)+ \exp\left(-\gamma U_{\mathbf{x},a}\right)}\]

For the second and third observations, \(\alpha_{\mathbf{x},a}\leq\frac{\sqrt{\lambda}}{\sqrt{2}\eta}\leq\frac{\sqrt{\lambda }}{\eta}\). Let \(\mathcal{L}(u)=\frac{\lambda}{\eta}\exp(-\gamma u)+\frac{\eta\pi_{\mathbf{\phi}}(a| \mathbf{x})^{2}}{\beta(a|\mathbf{x})^{2}\exp(-\gamma u)}\), we can have:

\[\nabla_{u}\mathcal{L}(u)=-\gamma\frac{\lambda}{\eta}\exp(-\gamma u )+\gamma\frac{\eta\pi_{\mathbf{\phi}}(a|\mathbf{x})^{2}}{\hat{\beta}(a|\mathbf{x})^{2}} \exp(\gamma u)\]

To make \(\nabla_{u}\mathcal{L}(u)\geq 0\), we need \(u\geq\frac{1}{\gamma}\log\left(\frac{\sqrt{\lambda}\hat{\beta}(a|\mathbf{x})}{\eta \pi_{\mathbf{\phi}}(a|\mathbf{x})}\right)\). This implies when \(U_{\mathbf{x},a}\geq\frac{1}{\gamma}\log\left(\frac{\sqrt{\lambda}\hat{\beta}(a| \mathbf{x})}{\eta\pi_{\mathbf{\phi}}(a|\mathbf{x})}\right)\), \(\phi_{\mathbf{x},a}^{*}\) will decrease as \(U_{\mathbf{x},a}\) increases; otherwise as \(U_{\mathbf{x},a}\) increases, \(\phi_{\mathbf{x},a}^{*}\) also increases.

More specifically, when \(\alpha_{\mathbf{x},a}\leq\frac{\pi_{\mathbf{\phi}}(a|\mathbf{x})}{\beta(a|\mathbf{x})}\), we have:

\[U_{\mathbf{x},a}\leq\frac{1}{\gamma}\log\left(\frac{\sqrt{\lambda} \hat{\beta}(a|\mathbf{x})}{\eta\pi_{\mathbf{\phi}}(a|\mathbf{x})}\right)\Leftrightarrow \frac{\pi_{\mathbf{\phi}}(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}^{*}}<\sqrt{\lambda}\quad \Leftrightarrow\quad\frac{\pi_{\mathbf{\phi}}(a|\mathbf{x})}{\hat{\beta}(a|\mathbf{x})}< \frac{\sqrt{\lambda}}{\eta}\exp(-\gamma U_{\mathbf{x},a}).\]

In other words, for these samples, higher uncertainty implies smaller value of \(\pi/\hat{\beta}\), and \(\mathrm{UIPS}\) tends to boost such safe sample with higher \(\phi_{\mathbf{x},a}^{*}\).

In other cases, \(\phi_{\mathbf{x},a}^{*}\) decreases as \(U_{\mathbf{x},a}\) increases. This completes the proof. 

### Convergence Analysis

Next we provide the convergence analysis of policy improvement under UIPS.

**Definition 7.2**.: A function \(f:\mathbb{R}^{d}\to\mathbb{R}\) is \(L\)-smooth when \(\|\nabla f(x)-\nabla f(y)\|_{2}\leq L\|x-y\|_{2}\), for all \(x,y\in\mathbb{R}^{d}\).

We first state and prove a general result, which serves as the basis to complete our convergence analysis of policy improvement under UIPS. The proof is a special case of convergence proof of stochastic gradient descent with biased gradients in [3]. Suppose we have a differentiable function \(f:\mathbb{R}^{d}\to\mathbb{R}\), which is \(L\)-smooth, and attains a finite minimum value \(f^{*}:=\min_{x\in\mathbb{R}^{d}}f(x)\).

Suppose we cannot directly assess the gradient \(\nabla f(x)\). Instead we can only assess a noisy but unbiased gradient \(\zeta(x)\in\mathbb{R}^{d}\) at the given \(x\) of the function \(\tilde{f}(x)\).

Let \(b(x)=\nabla\tilde{f}(x)-\nabla f(x)\) denote the difference between \(\nabla\tilde{f}(x)\) and \(\nabla f(x)\), and \(\delta(x)=\zeta(x)-\nabla\tilde{f}(x)\) denote the noise in gradients. We assume that:

\[\|b(x)\|\leq\varphi\|\nabla f(x)\|\quad\text{and}\quad\mathbb{E}[ \delta(x)]=0\quad\text{and}\quad\mathbb{E}[\|\delta(x)\|^{2}|x]\leq M\mathbb{E} [\|\nabla\tilde{f}(x)\|^{2}]+\sigma^{2} \tag{21}\]

where the constants \(\varphi\) and \(M\) satisfy \(0<\varphi<1\) and \(M\geq 0\) respectively. When running stochastic gradient descent algorithms, i.e. \(x_{k+1}=x_{k}-\eta_{k}\zeta(x_{k})\), we have the following guarantee on the convergence of \(x_{k}\) to an approximate stationary point of \(f\).

**Theorem 7.3**.: _Suppose \(f(\cdot)\) is differentiable and \(L\)-smooth, and the assessed approximate gradient meets the conditions in Eq. (21) with parameters \((\sigma_{k},\varphi_{k})\) at iteration \(k\). Denote \(\sigma_{\max}=\max_{k}\sigma_{k}\) and \(\varphi_{\max}=\max_{k}\varphi_{k}\). Set the stepsizes \(\{\eta_{k}\}\) to \(\eta_{k}=\min\{\frac{1}{(M+1)L},1/(\sigma_{\max}\sqrt{K})\}\), after \(K\) iterations, the stochastic gradient descent satisfies :_

\[\frac{1}{K}\sum_{k=1}^{K}\mathbb{E}\left[\|\nabla f(x_{k})\|^{2}\right]\leq \frac{2L(f(x_{1})-f^{*})}{K(1-\varphi_{\max})}+\left(L+\frac{2(f(x_{1})-f^{*}) }{(1-\varphi_{\max})}\right)\frac{\sigma_{\max}}{\sqrt{K}}\]

Proof.: With \(\eta_{k}\leq\frac{1}{(M+1)L}\), we have:

\[f(x_{k+1}) \leq f(x_{k})+\langle\nabla f(x_{k}),x_{k+1}-x_{k}\rangle+\frac{L }{2}\|x_{k+1}-x_{k}\|^{2} \tag{22}\] \[=f(x_{k})-\eta_{k}\langle\nabla f(x_{k}),\zeta(x_{k})\rangle+ \frac{L\eta_{k}^{2}}{2}\|\zeta(x_{k})\|^{2}\] \[=f(x_{k})-\eta_{k}\langle\nabla f(x_{k}),\delta(x_{k})+b(x_{k})+ \nabla f(x_{k})\rangle+\frac{L\eta_{k}^{2}}{2}\|\delta(x_{k})+b(x_{k})+\nabla f (x_{k})\|^{2}\]

By taking expectations on both side, we have:

\[\mathbb{E}[f(x_{k+1})] \leq\mathbb{E}[f(x_{k})]-\eta_{k}\mathbb{E}[\langle\nabla f(x_{k} ),\delta(x_{k})\rangle]-\eta_{k}\mathbb{E}[\langle\nabla f(x_{k}),b(x)+\nabla f (x_{k})\rangle]+\frac{L\eta_{k}^{2}}{2}\mathbb{E}[\|\delta(x_{k})+b(x_{k})+ \nabla f(x_{k})\|^{2}]\] \[\stackrel{{(1)}}{{\leq}}\mathbb{E}[f(x_{k})]-\eta_{k }\mathbb{E}[\langle\nabla f(x_{k}),b(x)+\nabla f(x_{k})\rangle]+\frac{L\eta_{k} ^{2}}{2}\left(\mathbb{E}[\|\delta(x_{k})\|^{2}]+\mathbb{E}[\|b(x)+\nabla f(x_{ k})\|^{2}]\right)\] \[\leq\mathbb{E}[f(x_{k})]-\eta_{k}\mathbb{E}[\langle\nabla f(x_{k }),b(x)+\nabla f(x_{k})\rangle]+\frac{L\eta_{k}^{2}}{2}\left((M+1)\mathbb{E}[ \|b(x)+\nabla f(x_{k})\|^{2}]+\sigma_{k}^{2}\right)\] \[\stackrel{{(2)}}{{\leq}}\mathbb{E}[f(x_{k})]+\frac{ \eta_{k}}{2}\mathbb{E}\left[\left(-2\langle\nabla f(x_{k}),b(x)+\nabla f(x_{k} )\rangle+\|b(x)+\nabla f(x_{k})\|^{2}\right)\right]+\frac{L\eta_{k}^{2}}{2} \sigma_{k}^{2}\] \[\leq\mathbb{E}[f(x_{k})]+\frac{\eta_{k}}{2}\mathbb{E}[\left(-\| \nabla f(x_{k})\|^{2}+\|b(x)\|^{2}\right)]+\frac{L\eta_{k}^{2}}{2}\sigma_{k} ^{2}\] \[\stackrel{{(3)}}{{\leq}}\mathbb{E}[f(x_{k})]+\frac{ \eta_{k}}{2}(\varphi_{k}-1)\mathbb{E}[\|\nabla f(x_{k})\|^{2}]+\frac{L\eta_{k} ^{2}}{2}\sigma_{k}^{2}\]

where the inequality labeled as (1) is due to \(\mathbb{E}[\delta(x)]=0\), inequality labeled as (2) is due to \(\eta_{k}\leq\frac{1}{(M+1)L}\), and inequality labeled as (3) is due to \(\|b(x_{k})\|\leq\varphi_{k}\|\nabla f(x_{k})\|\).

By summing over iterations \(k=1,2,\ldots,K\) and re-arranging the terms, we obtain:

\[\frac{1}{2}\sum_{k=1}^{K}(1-\varphi_{k})\eta_{k}\mathbb{E}[\| \nabla f(x_{k})\|^{2}] \leq f(x_{1})-\mathbb{E}[f(x_{K+1})]+\frac{L}{2}\sum_{k=1}^{K} \eta_{k}^{2}\sigma_{k}^{2}\] \[\leq f(x_{1})-f^{*}+\frac{L}{2}\sum_{k=1}^{K}\eta_{k}^{2}\sigma_{ k}^{2}\]

where the last inequality follows from \(f(x_{K+1})\geq f^{*}\). Since \(\eta_{k}=\min\{\frac{1}{(M+1)L},\frac{1}{\sigma_{\max}\sqrt{K}}\},\forall k=1, \ldots,K\), we can obtain:

\[(1-\varphi_{\max})\eta_{1}\sum_{k=1}^{K}\mathbb{E}[\|\nabla f(x_{k})\|^{2}] \leq 2(f(x_{1})-f^{*})+LK\eta_{1}^{2}\sigma_{\max}^{2}\]

Dividing both sides of the above inequality by \(K\eta_{1}(1-\varphi_{\max})\), we obtain the following,

\[\frac{1}{K}\sum_{k=1}^{K}\mathbb{E}[\|\nabla f(x_{k})\|^{2}] \leq\frac{2(f(x_{1})-f^{*})+LK\eta_{1}^{2}\sigma_{\max}^{2}}{K \eta_{1}(1-\varphi_{\max})}\] \[\leq\frac{2(f(x_{1})-f^{*})}{K(1-\varphi_{\max})}\max\{L,\sigma_{ \max}\sqrt{K}\}+L\sigma_{\max}^{2}\frac{1}{\sigma_{\max}\sqrt{K}}\] \[\leq\frac{2L(f(x_{1})-f^{*})}{K(1-\varphi_{\max})}+\left(L+\frac{2 (f(x_{1})-f^{*})}{(1-\varphi_{\max})}\right)\frac{\sigma_{\max}}{\sqrt{K}}\]Given this general result, we can now prove Theorem 3.4 by showing that the gradient in UIPS meets the requirements in Theorem 7.3.

**Proof of Theorem 3.4:**

Proof.: UIPS aims to maximize the expected return \(V(\pi_{\vartheta})\). Therefore, we can utilize Theorem 7.3 by setting \(f=-V(\pi_{\vartheta})\). Since \(f^{*}=-V_{\max}\) and the expected reward is always non-negative, it follows that \(\tilde{f}(x_{1})-f^{*}\leq V_{\max}\).

We first introduce some additional notations. Let \(\rho_{\vartheta}^{*}(\mathbf{x},a)=\frac{\pi_{\vartheta}(a|\mathbf{x})}{\beta^{*}(a|\bm {x})}\) denote the propensity score under ground-truth logging policy, and \(\hat{\rho}_{\vartheta}(\mathbf{x},a)=\frac{\pi_{\vartheta}(a|\mathbf{x})}{\beta(a|\bm {x})}\phi_{\mathbf{x},a}^{*}\) represent the propensity score of UIPS. Recall that \(\phi_{\mathbf{x},a}^{*}\) is derived through solving the optimization problem in Eq (8). Let \(g_{\vartheta}(\mathbf{x},a)=\frac{\partial\pi_{\vartheta}(a|\mathbf{x})}{\partial\vartheta}\). The true off-policy policy gradient is computed as follows:

\[\nabla V(\pi_{\vartheta})=\mathbb{E}_{\beta^{*}}[\rho^{*}rg_{ \vartheta}],\]

For UIPS, the approximate policy gradient in each batch with batch size as \(B\) is:

\[\nabla\hat{V}_{\mathrm{UIPS}}(\pi_{\vartheta})=\frac{1}{B}\sum_{i=1}^{B}\hat{ \rho}_{i}r_{i}g_{\vartheta}^{i},\]

which is an unbiased estimate of :

\[\nabla V_{\mathrm{UIPS}}(\pi_{\vartheta})=\mathbb{E}_{\beta^{*}}[ \hat{\rho}rg_{\vartheta}].\]

To utilize Theorem 7.3, we set \(\nabla f=-\nabla V(\pi_{\vartheta})\), \(\nabla\tilde{f}=-\nabla V_{\mathrm{UIPS}}(\pi_{\vartheta})\), and \(\zeta=\nabla\hat{V}_{\mathrm{UIPS}}(\pi_{\vartheta})\). We will now demonstrate that the assumptions in Eq. (21) can be satisfied.

Let \(\varphi_{\vartheta}=\max\left\{\left|\frac{\hat{\rho}_{\vartheta}(\mathbf{x},a)}{ \hat{\rho}_{\vartheta}(\mathbf{x},a)}-1\right|\right\}\), We first have:

\[\|b(\vartheta)\| =\|\nabla V_{\mathrm{UIPS}}(\pi_{\vartheta})-\nabla V(\pi_{ \vartheta})\|\] \[=\|\mathbb{E}_{\beta^{*}}[(\hat{\rho}-\rho^{*})rg_{\vartheta}]\|= \|\mathbb{E}_{\beta^{*}}[\rho^{*}(\frac{\hat{\rho}}{\rho^{*}}-1)rg_{\vartheta}]\|\] \[\leq\varphi_{\vartheta}\|\mathbb{E}_{\beta^{*}}[\rho^{*}rg_{ \vartheta}]\| \tag{23}\]

And next we show that \(0<\varphi_{\vartheta}<1\).

\[\varphi_{\vartheta}=\max\left\{\left|\frac{\hat{\rho}_{\vartheta}(\mathbf{x},a)}{ \rho_{\vartheta}^{*}(\mathbf{x},a)}-1\right|\right\}=\max\left\{\left|\frac{\beta^ {*}(a|\mathbf{x})}{\hat{\beta}(a|\mathbf{x})}\cdot\phi_{\mathbf{x},a}^{*}-1\right|\right\} \tag{24}\]

Recall from Eq.(17) in proof of Theorem 3.2, we have :

\[\phi_{\mathbf{x},a}^{*}=\min\left(\frac{\lambda}{\lambda\frac{\mathbf{B}_{\mathbf{x},a}^{ *}}{\beta(a|\mathbf{x})}+\frac{\pi_{\vartheta}(a|\mathbf{x})^{2}}{\beta(a|\mathbf{x})\bm {B}_{\mathbf{x},a}^{*}}},\frac{2\hat{\beta}(a|\mathbf{x})}{\mathbf{B}_{\mathbf{x},a}^{*}+\mathbf{ B}_{\mathbf{x},a}^{*}}\right)\]

where \(\mathbf{B}_{\mathbf{x},a}^{-}:=\frac{\mathcal{Z}\exp(-\gamma U_{\mathbf{x},a})}{Z^{*}} \hat{\beta}(a|\mathbf{x})\), and \(\mathbf{B}_{\mathbf{x},a}^{+}:=\frac{\mathcal{Z}\exp(\gamma U_{\mathbf{x},a})}{Z^{*}}\hat{ \beta}(a|\mathbf{x})\). Thus we have:

\[\frac{\beta^{*}(a|\mathbf{x})}{\hat{\beta}(a|\mathbf{x})}\cdot\phi_{\mathbf{x},a}^{*}=\min \left(\frac{\lambda}{\lambda\frac{\mathbf{B}_{\mathbf{x},a}^{*}}{\beta^{*}(a|\mathbf{x})}+ \frac{\pi_{\vartheta}(a|\mathbf{x})^{2}}{\beta^{*}(a|\mathbf{x})\mathbf{B}_{\mathbf{x},a}^{*} }},\frac{2}{\frac{\mathbf{B}_{\mathbf{x},a}^{*}}{\beta^{*}(a|\mathbf{x})}+\frac{\mathbf{B}_{ \mathbf{x},a}^{*}}{\beta^{*}(a|\mathbf{x})}}\right) \tag{25}\]

Since \(\mathbf{B}_{\mathbf{x},a}^{+}\geq\beta^{*}(a|\mathbf{x})\), thus \(0\leq\frac{\beta^{*}(a|\mathbf{x})}{\beta(a|\mathbf{x})}\cdot\phi_{\mathbf{x},a}^{*}\leq 2\), implying \(0<\varphi_{\vartheta}<1\).

Also, since \(\nabla\hat{V}_{\mathrm{UIPS}}(\pi_{\vartheta})\) is an unbiased estimate of \(\nabla V_{\mathrm{UIPS}}(\pi_{\vartheta})\), we have:

\[\mathbb{E}[\delta(\vartheta)]=\mathbb{E}[\nabla\hat{V}_{\mathrm{UIPS}}(\pi_{ \vartheta})-\nabla V_{\mathrm{UIPS}}(\pi_{\vartheta})]=\mathbf{0} \tag{26}\]Finally, we have the following,

\[\mathbb{E}[\|\delta(\vartheta)\|^{2}] =\mathbb{E}\left[\|\nabla\hat{V}_{\rm UIPS}(\pi_{\vartheta})-\nabla V _{\rm UIPS}(\pi_{\vartheta})\|^{2}\right]=\mathbb{E}\left[\left\|\frac{1}{B} \sum_{i=1}^{B}\left(\hat{\rho}_{i}r_{i}g_{\vartheta}^{i}-\mathbb{E}_{\beta^{*}} [\hat{\rho}rg_{\vartheta}]\right)\right\|^{2}\right]\] \[\leq\frac{1}{B^{2}}\mathbb{E}\left[\left(\sum_{i=1}^{B}\|\hat{\rho }_{i}r_{i}g_{\vartheta}^{i}-E_{\beta^{*}}[\hat{\rho}rg_{\vartheta}]\|\right)^{ 2}\right]\leq\frac{1}{B}\mathbb{E}\left[\sum_{i=1}^{B}\|\hat{\rho}_{i}r_{i}g_{ \vartheta}^{i}-\mathbb{E}_{\beta^{*}}[\hat{\rho}rg_{\vartheta}]\|^{2}\right]\] \[\leq\mathbb{E}_{\beta^{*}}[\|\hat{\rho}rg_{\vartheta}-\mathbb{E}_ {\beta^{*}}[\hat{\rho}rg_{\vartheta}]\|^{2}]\stackrel{{(1)}}{{=}} \mathbb{E}_{\beta^{*}}[\|\hat{\rho}rg_{\vartheta}\|^{2}]-\|\mathbb{E}_{\beta^ {*}}[\hat{\rho}rg_{\vartheta}]\|^{2}\] \[\leq\mathbb{E}_{\beta^{*}}[\|\hat{\rho}rg_{\vartheta}\|^{2}]\leq G _{\max}^{2}\Phi \tag{27}\]

where the equality labeled as \((1)\) is due to \(E[\|Y-E[Y]\|^{2}]=E[\|Y\|^{2}]-\|E[Y]\|^{2}\).

Hence, by applying Theorem 7.3, we have:

\[\frac{1}{K}\sum_{k=1}^{K}\mathbb{E}[\|\nabla V(\pi_{\vartheta_{k}})\|^{2}\leq \frac{2LV_{\max}}{K(1-\varphi_{\max})}+\left(L+\frac{2V_{\max}}{(1-\varphi_{ \max})}\right)\frac{G_{\max}\sqrt{\Phi}}{\sqrt{K}} \tag{28}\]