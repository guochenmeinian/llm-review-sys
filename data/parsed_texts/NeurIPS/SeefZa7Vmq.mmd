# Unlearnable 3D Point Clouds: Class-wise Transformation Is All You Need

Xianlong Wang\({}^{1,2,4,5}\), Minghui Li\({}^{\ddagger}\), Wei Liu\({}^{1,2,4,5}\), Hangtao Zhang\({}^{4,5}\),

**Shengshan Hu\({}^{1,2,4,5}\), Yechao Zhang\({}^{1,2,4,5}\), Ziqi Zhou\({}^{1,2,3}\), Hai Jin\({}^{1,2,3}\)**

\({}^{1}\) National Engineering Research Center for Big Data Technology and System

\({}^{2}\) Services Computing Technology and System Lab \({}^{3}\) Cluster and Grid Computing Lab

\({}^{4}\) Hubei Engineering Research Center on Big Data Security

\({}^{5}\) Hubei Key Laboratory of Distributed System Security

\(\dagger\) School of Cyber Science and Engineering, Huazhong University of Science and Technology

\(\ddagger\) School of Software Engineering, Huazhong University of Science and Technology

\(\lx@sectionsign\) School of Computer Science and Technology, Huazhong University of Science and Technology

(wx199,minghuili,weiliu73,hangt_zhang,hushengshan,ycz,zhouziqi,hjin)@hust.edu.cn

Minghui Li is the corresponding author.

###### Abstract

Traditional unlearnable strategies have been proposed to prevent unauthorized users from training on the 2D image data. With more 3D point cloud data containing sensitivity information, unauthorized usage of this new type data has also become a serious concern. To address this, we propose the first integral unlearnable framework for 3D point clouds including two processes: (i) we propose an unlearnable data protection scheme, involving a class-wise setting established by a category-adaptive allocation strategy and multi-transformations assigned to samples; (ii) we propose a data restoration scheme that utilizes class-wise inverse matrix transformation, thus enabling authorized-only training for unlearnable data. This restoration process is a practical issue overlooked in most existing unlearnable literature, _i.e._, even authorized users struggle to gain knowledge from 3D unlearnable data. Both theoretical and empirical results (including 6 datasets, 16 models, and 2 tasks) demonstrate the effectiveness of our proposed unlearnable framework. Our code is available at https://github.com/CGCL-codes/UnlearnablePC.

## 1 Introduction

Recently, 3D point cloud deep learning has been making remarkable strides in various domains, _e.g._, self-driving [6] and virtual reality [1; 46]. Specifically, numerous 3D sensors scan the surrounding environment and synthesize massive 3D point cloud data containing sensitive information such as pedestrian and vehicles [32] to the cloud server for deep learning analysis [12; 23]. However, the raw point cloud data can be exploited for point cloud unauthorized deep learning if a data breach occurs, posing a significant privacy threat. Fortunately, the privacy protection approaches for preventing unauthorized training have been extensively studied in the 2D image domain [9; 19; 28; 29; 39; 42]. They apply elaborate perturbations on images such that trained networks over them exhibit extremely low generalization, thus failing to learn knowledge from the protected data, known as "making data unlearnable". Nonetheless, the stark disparity between 2D images and 3D point clouds poses significant challenges for drawing lessons from existing 2D solutions.

Specifically, migrating 2D unlearnable schemes to 3D suffers from following challenges: **(i) Incompatibility with 3D data.** Numerous model-agnostic 2D image unlearnable schemes operatein the pixel space, such as convolutional operations [39; 42], making them fail to be directly transferred to the 3D point space. **(ii) Poor visual quality.** Migrating model-dependent 2D unlearnable methods [5; 9; 19; 28] to 3D point clouds requires perturbing substantial points, leading to irregular three-dimensional shifts which may significantly degrade visual quality. Hence, these challenges spur us to start directly from the characteristics of point clouds for proposing 3D unlearnable solutions.

Recent works observe that 3D transformations can alter test-time results of models [7; 17; 44]. To explore this, we conduct an in-depth investigation into the properties of seven 3D transformations as shown in Fig. 1 and reveal the mechanisms by which transformations employed in a certain pattern serve as unlearnable schemes (Sec. 3.2). In light of this, we propose the first unlearnable approach in 3D point clouds via multi class-wise transformation (UMT), transforming samples to various forms for privacy protection. Concretely, we newly propose a category-adaptive allocation strategy by leveraging uniform distribution sampling and category constraints to establish a class-wise setting, thereby multiplying multi-transformations to samples based on categories. To theoretically analyze UMT, we define a binary classification setup similar to that used in [20; 33; 39]. Meanwhile, we employ a _Gaussian Mixture Model_ (GMM) [38] to model the clean training set and use the Bayesian optimal decision boundary to model the point cloud classifier. Theoretically, we prove that there exists a UMT training set follows a GMM distribution and the classification accuracy of UMT dataset is lower than that of the clean dataset in a Bayesian classifier.

Moreover, an incompatible issue in existing unlearnable works [9; 19; 28; 29; 39; 43] is identified [54], _i.e._, these approaches prevent unauthorized learning to protected data, but they also impede authorized users from effectively learning from unlearnable data. To address this, we propose a data restoration scheme that applies class-wise inverse transformations, determined by a lightweight message received from the protector. Our proposed unlearnable framework including UMT approach and data restoration scheme is depicted in Fig. 3.

Extensive experiments on 6 benchmark datasets (including synthetic and real-world datasets) using 16 point cloud models across CNN, MLP, Graph-based Network, and Transformer on two tasks (classification and semantic segmentation), verified the effectiveness of our proposed unlearnable scheme. We summarize our main contributions as follows:

* **The First Integral 3D Unlearnable Framework.** To the best of our knowledge, we propose the first integral unlearnable 3D point cloud framework, utilizing class-wise multi-transformation as its unlearnable mechanism (effectively safeguarding point cloud data against unauthorized exploitation) and proposing a novel data restoration approach that leverages class-wise reversible 3D transformation matrices (addressing an incompatible issue in most existing unlearnable works, where even authorized users cannot effectively learn knowledge from unlearnable data).
* **Theoretical Analysis.** We theoretically indicate the existence of an unlearnable situation that the classification accuracy of the UMT dataset is lower than that of the clean dataset under the decision boundary of the Bayes classifier in Gaussian Mixture Model.
* **Experimental Evaluation.** Extensive experiments on 3 synthetic datasets and 3 real-world datasets using 16 widely used point cloud model architectures on classification and semantic segmentation tasks verify the superiority of our proposed schemes.

Figure 1: An overview of existing seven types of 3D transformations. “*” denotes _rigid transformations_ that do not alter the shape of the point cloud samples, while the remaining transformations are non-rigid transformations.

## 2 Preliminaries

**Notation.** Considering the raw point cloud data \((\mathbf{X},\mathbf{Y})\in\mathcal{X}\times\mathcal{Y}\) sampled from a clean distribution \(\mathcal{D}\) for training a point cloud network, the user's goal is to obtain a model \(\mathcal{F}:\mathcal{X}\rightarrow\mathcal{Y}\) by minimizing the loss function (_e.g._, cross-entropy loss) \(\mathcal{L}(\mathcal{F}(\mathbf{X}),\mathbf{Y})\). Let \(\mathbf{T}\in\mathcal{T}\) be a 3D transformation matrix that does not seriously damage the visual quality of point clouds. Note that \(\mathcal{X}\in\mathbb{R}^{3\times p}\), \(\mathcal{T}\in\mathbb{R}^{3\times 3}\), and \(p\) represents the number of points. In theoretical analysis, following [20; 33], we simplify a training dataset \(\mathcal{D}_{k}\) to a _Gaussian Mixture Model_ (GMM) [38]\(\mathcal{N}(y\mu,\bm{I})\), where \(y\in\{\pm 1\}\) denotes the class labels, \(\mu\in\mathbb{R}^{d}\) denotes the mean value, and \(\bm{I}\in\mathbb{R}^{d\times d}\) denotes the identity matrix. Thus the Bayes optimal decision boundary for classifying \(\mathcal{D}_{k}\) is defined by \(P(x)\equiv\mu^{T}x=0\). The accuracy of the decision boundary \(P\) on \(\mathcal{D}_{k}\) is equal to \(\phi(||\mu||_{2})\), where \(\phi\) denotes the _Cumulative Distribution Function_ (CDF) of the standard normal distribution.

**Data protector \(\bm{\mathcal{G}_{p}}\).**\(\mathcal{G}_{p}\) aims to protect the knowledge from the clean training set (with size of \(n\)) \(\mathcal{D}_{c}=\{\mathbf{X}_{i},\mathbf{Y}_{i}\}_{i=1}^{n}\sim\mathcal{D}\) by compromising the unauthorized models who train on the unlearnable point cloud data \(\{\mathbf{T}_{i}(\mathbf{X}_{i}),\mathbf{Y}_{i}\}_{i=1}^{n}\), resulting in extremely poor generalization on the clean test distribution \(\mathcal{D}_{t}\subseteq\mathcal{D}\). This objective can be formalized as:

\[\max_{(\mathbf{X},\mathbf{Y})\sim\mathcal{D}_{t}}\mathcal{L}\left(\mathcal{F} \left(\mathbf{X};\theta_{u}\right),\mathbf{Y}\right),\text{ s.t. }\theta_{u}=\operatorname*{arg\,min}_{\theta}\sum_{(\mathbf{X}_{i},\mathbf{Y} _{i})\in\mathcal{D}_{c}}\mathcal{L}\left(\mathcal{F}\left(\mathbf{T}_{i}( \mathbf{X}_{i});\theta\right),\mathbf{Y}_{i}\right)\] (1)

where \(\mathcal{G}_{p}\) assumes that training samples are all transformed into unlearnable ones while maintaining normal visual effects, in line with previous unlearnable works [19; 28; 39; 42]. By the way, solving Eq. (1) directly is infeasible for neural networks because it necessitates unrolling the entire training procedure within the inner objective and performing backpropagation through it to execute a single step of gradient descent on the outer objective [8].

**Authorized user \(\bm{\mathcal{G}_{a}}\).**\(\mathcal{G}_{a}\) aims to apply another transformation \(\mathbf{T}^{\prime}\in\mathcal{T}\) on the unlearnable sample, making the protected data learnable. This is formally defined as:

\[\min_{(\mathbf{X},\mathbf{Y})\sim\mathcal{D}_{t}}\mathcal{L}\left(\mathcal{F} \left(\mathbf{X};\theta_{r}\right),\mathbf{Y}\right),\text{ s.t. }\theta_{r}=\operatorname*{arg\,min}_{\theta}\sum_{(\mathbf{X}_{i},\mathbf{Y} _{i})\in\mathcal{D}_{c}}\mathcal{L}\left(\mathcal{F}\left(\mathbf{T}^{\prime}_ {i}(\mathbf{T}_{i}(\mathbf{X}_{i}));\theta\right),\mathbf{Y}_{i}\right)\] (2)

where \(\mathcal{G}_{a}\) assumes that, without access to any clean training samples, \(\mathbf{T}^{\prime}\) can be constructed by utilizing a lightweight message \(M\) received from data protectors.

## 3 Our Proposed Unlearnable Schemes

### Key Intuition

Several recent works [7; 13; 44] reveal employing 3D transformations can mislead the model's classification results. Such a phenomenon implies that there might be some defects in point cloud classifiers when processing transformed samples, leading us to infer that 3D transformations are probable candidates for data protection against unauthorized training. If the transformed point cloud data are used to train unauthorized DNNs, only simple linear features inherent in 3D transformations (at which transformations may act as shortcuts [14]) are captured by the DNNs, successfully protecting point cloud data privacy.

Figure 2: (a) Training on the transformed ModelNet10 dataset (employing sample-wise, dataset-wise, and class-wise patterns) using PointNet classifier; (b) The high-level overview of the class-wise setting

### Exploring the Mechanism

We summarize existing 3D transformations in Fig. 1 and formally define them in Appendix A. To seek clarity on the application and selection of transformations, we explore three aspects: **(i) execution mechanism**, **(ii) exclusion mechanism**, and **(iii) working mechanism** as follows.

**(i) Which execution mechanism successfully satisfy Eq. (1)?** The extensively employed execution patterns in 2D unlearnable approaches are sample-wise [9; 19] and class-wise [19; 39] settings. We further complement the dataset-wise setting (using universal transformation) and implement the above execution mechanisms for training a PointNet classifier [35] on the transformed ModelNet10 [50], obtaining test accuracy results in Fig. 2 (a). We discover that model achieves considerably low test accuracy under the class-wise setting, satisfying Eq. (1). Sample-wise and dataset-wise settings do not obviously compromise model performance, which cannot serve as promising unlearnable routes. Moreover, we note that sample-wise transformation is often considered as a data augmentation scheme to improve generalization, which contradicts our aim of using class-wise transformation to lower model generalization.

**(ii) Which transformations need to be excluded?** Not all transformations are suitable candidates. We exclude three transformations, tapering, reflection, and translation. (1) The tapering matrix may cause point cloud samples to become a planar projection when \(\eta z\) defined in Eq. (18) equals to -1, rendering the tapered samples meaningless; (2) The reflection matrix has only three distinct transformation matrices, rendering it incapable of assigning class-wise transformations when the number of categories exceeds three; (3) The translation transformation is a straightforward and simple additive transformation that is too easily defeated by point cloud data pre-processing approaches.

**(iii) Why does class-wise transformation work?** We conduct experiments using class-wise transformations (see Tab. 6), indicating that the model training on the class-wise transformed training set achieves a relatively high accuracy on the class-wise transformed test set (using the same transformation process as the training set). Besides, if we permute the class-wise transformation for the test set, we obtain a significant low accuracy on the test set. Therefore, we conclude that the reason why class-wise transformation works is that the model learns the mapping between class-wise transformations and corresponding category labels as shown in Fig. 2 (b), which results in the model being unable to predict the corresponding labels on a clean test set lacking transformations. This analytical process yields conclusions that are in agreement with prior research [39; 44].

### Our Design for UMT

#### 3.3.1 Category-Adaptive Allocation Strategy

We assign transformation parameters based on categories to realize class-wise setting. For rotation transformation \(\mathcal{R}\in\mathbb{R}^{3\times 3}\), we refer to \(\alpha\) and \(\beta\) as slight angles imposed on the \(x\) and \(y\) axes, \(\gamma\) as the primary angle for \(z\) axis. We generate random angles for \(\mathcal{A}_{N}\) times in three directions:

\[\alpha,\beta\sim\mathcal{U}(0,r_{s}),\gamma\sim\mathcal{U}(0,r_{p}),\mathcal{ A}_{N}=\left\lceil\sqrt[3]{N}\right\rceil\] (3)

where \(\mathcal{U}\) denotes uniform distribution, \(N\) denotes the number of categories, \(r_{s}\) is a small range that controls \(\alpha\) and \(\beta\), while \(r_{p}\) is a large range that controls \(\gamma\). \(\mathcal{A}_{N}\) is computed in such a way to ensure that the number of combinations of three angles is greater than or equal to \(N\). Concretely, in the rotation operation, each of the three directions has \(\mathcal{A}_{N}\) distinct angles, which means that the final rotation matrix has \(\mathcal{A}_{N}^{3}\) possible combinations. To satisfy the class-wise setup, \(\mathcal{A}_{N}^{3}\) must be at least \(N\), requiring \(\mathcal{A}_{N}\) to be no less than \(\left\lceil\sqrt[3]{N}\right\rceil\). Finally, we randomly select \(N\) combinations of angles for the allocation. The scaling transformation \(\mathcal{S}\in\mathbb{R}^{3\times 3}\) resizes the position of each point in the 3D point cloud sample by a certain scaling factor \(\lambda\), which is sampled \(N\) times from a uniform distribution \(\mathcal{U}\):

\[\lambda\sim\mathcal{U}(b_{l},b_{u})\] (4)

where \(b_{l}\) and \(b_{u}\) represent the lower bound and upper bound of the scaling factor, respectively. For shear \(\mathcal{H}\in\mathbb{R}^{3\times 3}\) defined in Eq. (13), twisting \(\mathcal{W}\in\mathbb{R}^{3\times 3}\) defined in Eq. (16), the process of generating parameters within ranges \((\omega_{l},\omega_{u})\) and \((h_{l},h_{u})\) is consistent to scaling. The range of these parameters ensures the visual effect of the sample.

**Property 1**.: _Since rotation matrices \(\mathcal{R}_{\alpha}\), \(\mathcal{R}_{\beta}\), and \(\mathcal{R}_{\gamma}\) around three directions are all orthogonal matrices, \(\mathcal{R}\) is also an orthogonal matrix, which can be defined as:_

\[\forall\mathcal{M}\in\{\mathcal{R}_{\alpha},\mathcal{R}_{\beta}, \mathcal{R}_{\gamma},\mathcal{R}\},\quad\text{s.t.}\quad\mathcal{MM}^{T}= \boldsymbol{I}\] (5)

where we can determine the orthogonality by matrix multiplication through the definitions of Eq. (11). Since \(\mathcal{R}=\mathcal{R}_{\alpha}\mathcal{R}_{\beta}\mathcal{R}_{\gamma}\) and \(\mathcal{RR}^{T}=\mathcal{R}_{\alpha}\mathcal{R}_{\beta}\mathcal{R}_{\gamma} \mathcal{R}_{\gamma}^{T}\mathcal{R}_{\gamma}^{T}\mathcal{R}_{\alpha}^{T}= \boldsymbol{I}\), so \(\mathcal{R}\) is also an orthogonal matrix.

**Property 2**.: _All four transformation matrices we employ, \(\mathcal{R}\), \(\mathcal{S}\), \(\mathcal{W}\), and \(\mathcal{H}\), and the multiplicative combinations of any these matrices are all invertible matrices, which can be formally defined as:_

\[\forall\mathcal{J}\in\{f(\mathcal{R})f(\mathcal{S})f(\mathcal{W})f( \mathcal{H})\mid f(x)\in\{x,\boldsymbol{I}\}\},\quad\exists\mathcal{K}\quad \text{s.t.}\quad\mathcal{JK}=\mathcal{KJ}=\boldsymbol{I}\] (6)

where the inverse matrices of \(\mathcal{R}\), \(\mathcal{S}\), \(\mathcal{W}\), and \(\mathcal{H}\) are given in Appendix A. This property allows the authorized users to normally train on the protected data due to that multiplying a matrix by its inverse results in the identity matrix, leading us to propose a data restoration scheme in Sec. 3.4.

#### 3.3.2 Employing Class-wise Transformations

Assuming the point cloud training set \(\mathcal{D}_{c}\) is defined as \(\{(\mathbf{X}_{c1},\mathbf{Y}_{i}),(\mathbf{X}_{c2},\mathbf{Y}_{i}),...,( \mathbf{X}_{cn_{i}},\mathbf{Y}_{i})\}_{i=1}^{N}\), where \(n_{1},n_{2},...,n_{N}\) represent the number of samples in the 1st, 2nd,..., \(N\)-th category, respectively. We formally define the spectrum of transformations as \(k\) to indicate the number of transformations involved. Thus the ultimate unlearnable transformation matrix \(\mathbf{T}_{k}\) is defined as:

\[\mathbf{T}_{k}=\prod_{i=1}^{k}\mathcal{V}_{i},\quad\forall i\neq j,\quad \text{s.t.}\quad\mathcal{V}_{i},\mathcal{V}_{j}\in\{\mathcal{R},\mathcal{S}, \mathcal{W},\mathcal{H}\},\mathcal{V}_{i}\neq\mathcal{V}_{j}\] (7)

Once we employ the proposed category-adaptive allocation strategy to \(\mathbf{T}_{k}\), the unlearnable point cloud dataset \(\mathcal{D}_{u}\) is constructed as:

\[\mathcal{D}_{u}=\{(\mathbf{T}_{k_{i}}(\mathbf{X}_{c1}),\mathbf{Y}_{i}),( \mathbf{T}_{k_{i}}(\mathbf{X}_{c2}),\mathbf{Y}_{i}),...,(\mathbf{T}_{k_{i}}( \mathbf{X}_{cn_{i}}),\mathbf{Y}_{i})\}_{i=1}^{N}\] (8)

Our proposed UMT scheme is described in Algorithm 1. We enumerate possible transformations in Eq. (7) to obtain the unlearnability in Tab. 7 and select one type of class-wise transformation for each \(k\) for more comprehensive experiments in Tab. 1. To facilitate the theoretical study of UMT2, we opt for \(\mathcal{RS}\) as the transformation matrix \(\mathbf{T}\), which achieves the best unlearnable effect as suggested in Tabs. 1 and 7. Thus, in the GMM scenario, the class-wise transformation matrix is defined as \(\mathbf{T}_{y}=\mathcal{R}_{y}\mathcal{S}_{y}=\lambda_{y}\mathcal{R}_{y}\in \mathbb{R}^{d\times d}\), where \(\lambda_{y}\in\mathbb{R}\) is the scaling factor.

Footnote 2: In the subsequent theoretical descriptions, UMT refers to UMT with \(k\)=2 using transformation matrix \(\mathcal{RS}\).

**Lemma 3**.: _The unlearnable dataset \(\mathcal{D}_{u}\) generated using UMT on \(\mathcal{D}_{c}\) can also be represented using a GMM, i.e., \(\mathcal{D}_{u}\sim\mathcal{N}(y\mathbf{T}_{y}\mu,\lambda_{y}^{2}\boldsymbol{I})\)._

Proof.: See Appendix D.1. Lemma 3 demonstrates that the unlearnable dataset \(\mathcal{D}_{u}\) can also be represented as a GMM, which is derived from Property 1.

**Lemma 4**.: _The Bayes optimal decision boundary for classifying \(\mathcal{D}_{u}\) is given by \(P_{u}(x)\equiv\mathcal{A}x^{\top}x+\mathcal{B}^{\top}x+\mathcal{C}=0\), where \(\mathcal{A}=\lambda_{-1}^{-2}-\lambda_{1}^{-2}\), \(\mathcal{B}=2(\lambda_{-1}^{-2}\mathbf{T}_{-1}+\lambda_{1}^{-2}\mathbf{T}_{1})\mu\), and \(\mathcal{C}=\ln\frac{|\lambda_{-1}^{2}\boldsymbol{I}|}{|\lambda_{1}^{2} \boldsymbol{I}|}\)._

Figure 3: An overview of our proposed integral unlearnable pipeline_Proof:_ See Appendix D.2. Lemma 4 reveals that the Bayesian decision boundary for classifying \(\mathcal{D}_{u}\) is a quadratic surface based on the GMM expression of \(\mathcal{D}_{u}\).

**Lemma 5**.: _Let \(z\sim\mathcal{N}(0,\bm{I})\), \(Z=z^{\top}z+b^{\top}z+c\), where \(b=\frac{\mathcal{B}}{A},c=\frac{\mathcal{C}}{A}\), and \(\left\lVert\cdot\right\rVert_{2}\) denote 2-norm of vectors. For any \(t\geq 0\) and \(\gamma\in\mathbb{R}\), we employ Chernoff bound to have:_

\[\mathbb{P}\{Z\geq\mathbb{E}[Z]+\gamma\}\leq\frac{\exp\left\{\frac{t^{2}}{2(1-2 t)}||b||_{2}^{2}-t(\gamma+d)\right\}}{|(1-2t)\bm{I}|^{\frac{1}{2}}}\]

_Proof:_ See Appendix D.3. Lemma 5 enables us to establish an upper bound on the accuracy of the unlearnable decision boundary \(P_{u}\) applied to the clean dataset \(\mathcal{D}_{c}\), denoted as \(\tau_{\mathcal{D}_{c}}(P_{u})\), as presented in Theorem 6 below.

**Theorem 6**.: _For any constant \(t_{1}\) and \(t_{2}\) satisfying \(0\leq t_{1}<\frac{1}{2}\) and \(0\leq t_{2}<\frac{1}{2}\), the accuracy of the unlearnable decision boundary \(P_{u}\) on \(\mathcal{D}_{c}\) can be upper-bounded as:_

\[\tau_{\mathcal{D}_{c}}(P_{u}) \leq\frac{\exp\left\{\frac{t_{1}^{2}}{2(1-2t_{1})}||b+2\mu||_{2}^ {2}+t_{1}(\mu^{\top}\mu+b^{\top}\mu+c)\right\}}{2|(1-2t_{1})\bm{I}|^{\frac{1}{2}}}\] \[+\frac{\exp\left\{\frac{t_{2}^{2}}{2(1-2t_{2})}||b-2\mu||_{2}^{2}- t_{2}(\mu^{\top}\mu-b^{\top}\mu+c+2d)\right\}}{2|(1-2t_{2})\bm{I}|^{\frac{1}{2}}}\] \[:=p_{1}+p_{2}\]

_Furthermore, if \(\mu^{\top}\mu+b^{\top}\mu+c+d<0\) and \(-\mu^{\top}\mu+b^{\top}\mu-c-d<0\), we have \(\tau_{\mathcal{D}_{c}}(P_{u})<1\). Moreover, for any \(\mu\neq 0\), \(\exists\) matrix \(\mathbf{T}_{i}\) such that \(\tau_{\mathcal{D}_{c}}(P_{u})<\tau_{\mathcal{D}_{c}}(P)\), where \(P\) is the Bayes optimal decision boundary for classifying \(\mathcal{D}_{c}\)._

_Proof:_ See Appendix D.4. The unlearnable effect takes place when \(\tau_{\mathcal{D}_{c}}(P_{u})<\tau_{\mathcal{D}_{c}}(P)\). To achieve this, we elaborately choose \(\mathbf{T}_{y}\), which is formalized as \(\mu^{\top}\frac{\lambda_{-1}^{-2}\mathbf{T}_{1}^{-1}+\lambda_{1}^{-2}\mathbf{T }_{1}^{\top}}{\lambda_{-1}^{-2}-\lambda_{1}^{-2}}\mu\ll 0\). Therefore, Theorem 6 theoretically explains why UMT is effective in generating unlearnable point cloud data.

### Data Restoration Scheme

To ensure that authorized users can achieve better generalization after training on unlearnable data, _i.e._, satisfying Eq. (2), we exploit the inverse properties of 3D transformations, presented in Property 2, to calculate the inverse matrix of \(\mathbf{T}_{k}\) as:

\[\mathbf{T}_{k}{}^{-1}=\prod_{i=k}^{1}\mathcal{V}_{i}{}^{-1},\quad\forall i\neq j,\quad\text{s.t.}\quad\mathcal{V}_{i}{}^{-1},\mathcal{V}_{j}{}^{-1}\in\{ \mathcal{R}^{-1},\mathcal{S}^{-1},\mathcal{W}^{-1},\mathcal{H}^{-1}\},\mathcal{ V}_{i}{}^{-1}\neq\mathcal{V}_{j}{}^{-1}\] (9)

In particular, we note that \(\mathcal{R}^{-1}=\mathcal{R}^{T}\), \(\mathcal{S}^{-1}=\frac{1}{\lambda}\bm{I}\). Afterwards, the authorized user receives a lightweight message \(M\) containing class-wise parameters from the data protector through a secure channel, thereby assigning \(M\) to the inverse transformation matrix in Eq. (9) for multiplying the unlearnable samples. Our proposed integral unlearnable process is illustrated in Fig. 3.

## 4 Experiments

### Experimental Details

**Datasets and Models.** Three synthetic 3D point cloud datasets, ModelNet40 [50], ModelNet10 [50], ShapeNetPart [4], and three real-world datasets including autonomous driving dataset KITTI [32] and indoor datasets ScanObjectNN [41], S3DIS [2] are used. We choose 16 widely used 3D point cloud models PointNet [35], PointNet++ [36], DGCNN [45], PointCNN [25], PCT [16], PointConv [48], CurveNet [51], SimpleView [15], 3DGCN [26], LGR-Net [59], RIConv [57], RIConv++ [58], PointMLP [31], PointNN [56], PointTransformerV3 [49], and SegNN [62] for evaluation of classification and semantic segmentation tasks.

**Experimental Setup.** The training process involves Adam optimizer [22], CosineAnnealingLR scheduler [30], initial learning rate of 0.001, weight decay of 0.0001. We empirically set \(r_{s}\), \(r_{p}\), \(b_{l}\), \(b_{u}\), \(\omega_{l}\), \(\omega_{u}\), \(h_{l}\), and \(h_{u}\) to 15\({}^{\circ}\), 120\({}^{\circ}\), 0.6, 0.8, 0\({}^{\circ}\), 20\({}^{\circ}\), 0, and 0.4 respectively. The main results of different 

[MISSING_PAGE_FAIL:7]

knowledge about the \(\mathcal{G}_{p}\)'s use of \(\mathcal{RS}\). Thus we propose random rotation & scaling as an adaptive scheme. In Tab. 2, the adaptive scheme exhibits a higher accuracy than other schemes, confirming its effectiveness. Nonetheless, it remains 28.67% lower than clean baseline, revealing the robustness of UMT against adaptive attack. More results of adaptive attacks are provided in Appendix C.3.

**Visual Effect.** We visualize UMT samples in Figs. 7 to 10, indicating that the unlearnable point cloud samples still retain their normal feature structure with visual rationality.

**Evaluation of Semantic Segmentation.** We evaluate UMT using common metrics for point cloud semantic segmentation tasks in Tab. 3. As can be seen, the performance of semantic segmentation of data protected by UMT significantly decreases. The underlying reason is that the DNNs learn the features of class-wise transformations and establish a new mapping, which leads to the inability of test samples without transformations to be correctly segmented by the segmentation model.

**Evaluation of Data Restoration.** We multiply UMT samples by the transformation matrix in Eq. (9). The data becomes learnable after the restoration process, with test accuracy reaching a level comparable to the clean baseline as shown in Fig. 4. This strongly validates the effectiveness of the proposed data restoration scheme.

### Ablation Study and Hyper-Parameter Sensitivity Analysis

**Ablation on Rotation Module.** As shown in Tab. 4, the average accuracy increases by 15.18% and 21.65%, respectively, when only using \(\mathcal{S}\). This suggests the importance of class-wise rotation module. The high test accuracy demonstrated by 3DGCN [26] can be attributed to its scaling invariance, which endows it with robustness against scaling transformation.

**Ablation on Scaling Module.** As also shown in Tab. 4, the average accuracy increases by 18.80% and 16.88% when only using the rotation module, respectively. The high test accuracy achieved on RIConv [57] and LGR-Net [59] is due to the fact that both networks are rotation-invariant, thus providing resistance against rotation transformations. These ablation results furthermore emphasize the importance of incorporating more non-rigid transformations.

**Hyper-Parameter Analysis.** We analyze four hyperparameters \(r_{s}\), \(r_{p}\), \(b_{l}\), and \(b_{u}\) in Fig. 5. The influence of \(r_{s}\) and \(r_{p}\) on the accuracy remains relatively small, exhibiting their best unlearnable effect when set to 15\({}^{\circ}\) and 120\({}^{\circ}\), respectively. We attribute this to the crucial role played by the class-wise setting, while it is not highly sensitive to the size of specific values. The unlearnable effect is the best when \(b_{l}\) and \(b_{u}\) are set to 0.6 and 0.8, respectively. Similarly, the variations in \(b_{l}\) and \(b_{u}\) do not significantly alter the effect due to the class-wise setting.

### Insightful Analysis Into UMT

We formalize \(\mathcal{L}(\mathcal{D})=\mathbb{E}_{(\mathbf{X},\mathbf{Y})\sim\mathcal{D}}[ \mathcal{L}_{c}(\mathcal{F}\left(\mathbf{X};\theta\right),\mathbf{Y})]\), where \(\mathcal{L}_{c}\) is the cross-entropy loss, \(\mathbf{X},\mathbf{Y}\) is the point cloud data sampled from dataset \(\mathcal{D}\). We define \(\mathcal{D}_{tr}\), \(\mathcal{D}_{u}\), and \(\mathcal{D}_{c}\) as the training set, unlearnable test set (_i.e._, test set transformed by UMT), and clean test set, respectively. Thus we have the _training loss_\(\mathcal{L}_{train}=\mathcal{L}(\mathcal{D}_{tr})\), _unlearnable test loss_\(\mathcal{L}_{u}=\mathcal{L}(\mathcal{D}_{u})\), and _clean test loss_\(\mathcal{L}_{c}=\mathcal{L}(\mathcal{D}_{c})\).

The models trained on both clean and UMT training sets exhibit low \(\mathcal{L}_{train}\) as shown in Fig. 6 (_yellow ellipses_), indicating the models converge well during training. Furthermore, when tested on \(\mathcal{D}_{c}\) as shown in Fig. 6 (a), the clean model (trained with clean training set) achieves a low \(\mathcal{L}_{c}\) (_blue ellipse_), while the UMT model (trained with UMT training set) exhibits a high \(\mathcal{L}_{c}\) (_red ellipse_), also supporting the unlearnable effectiveness of UMT. Fig. 6 (b) reveals that the clean model and UMT model both exhibit low \(\mathcal{L}_{u}\) (_green ellipse_), suggesting that they can classify UMT samples. But the mechanisms underlying the two cases of low \(\mathcal{L}_{u}\) differ. The clean one is due to that the semantics of samples can be remained by UMT and thus normally classified. The UMT one is that the UMT model learns the mapping between transformations and labels, thereby correctly predicting the samples containing the same transformations. We conclude that the clean model effectively classifies both clean and UMT samples, while the UMT model successfully classifies UMT samples (the UMT process is the same for both training and test samples) but fails to classify clean samples.

## 5 Related Work

### 2D Unlearnable Schemes

The development of 2D unlearnable schemes [19; 29; 37; 39; 42; 55] has been booming. Specifically, model-dependent methods are initially proposed in abundance [9; 19; 29]. Afterwards, numerous model-agnostic methods that significantly improve the generation efficiency have surfaced [39; 42; 47]. However, due to the structural disparities between 3D point cloud data and 2D images, applying unlearnable methods directly from 2D to 3D reveals significant challenges.

### Protecting 3D Point Cloud Data Privacy

Some works proposed an encryption scheme based on chaotic mapping [21] or optical chaotic encryption [27], and a 3D object reconstruction technique was introduced [34], both achieving privacy protection for individual 3D point cloud data. Nevertheless, no privacy-preserving solution has been proposed specifically for the scenario of unauthorized DNN learning on abundant raw 3D point cloud data. It is worth mentioning that both parallel works [44; 63] study availability poisoning attacks against 3D point cloud networks, which largely reduce model accuracy, and both have the potential to be applied as unlearnable schemes. However, the feature collision error-minimization poisoning scheme proposed by Zhu _et al._[63] overlooks the problem of effective training for authorized users, which limits its practical use in real-world applications. The rotation-based poisoning approach proposed by Wang _et al._[44] is easily defeated by rotation-invariant networks [57; 58; 59], as revealed in Tabs. 1 and 4.

Figure 6: UMT in weight space. The blue arrow represents the clean training trajectory of the weights \(\theta_{i}\) at step \(i\), while the red arrows denote the UMT training trajectory. The values for plotting this figure are provided in Appendix C.6. (a) Testing on clean test set (_blue and red ellipses_); (b) Testing on UMT test set (_green ellipse_)Conclusion, Limitation, and Broader Impacts

In this research, we propose the first integral unlearnable framework in 3D point clouds, which utilizes class-wise multi transformations, preventing unauthorized deep learning while allowing authorized training. Extensive experiments on synthetic and real-world datasets and theoretical evidence verify the superiority of our framework. The transformations include rotation, scaling, twisting, and shear, which are all common 3D transformation operations. If unauthorized users design a network that is invariant to all these transformations, they could potentially defeat our proposed UMT. So far, only networks invariant to rigid transformations like rotation and scaling have been proposed, while networks invariant to non-rigid transformations like twisting and shear, have not yet been introduced. Therefore, our research also contributes to the design of more transformation-invariant networks.

Our research calls for the design of more robust point cloud networks, which helps improve the robustness and security of 3D point cloud processing systems. On the other hand, if our proposed UMT scheme is maliciously exploited, it may have negative impacts on society, such as causing a sharp decline in the performance of models trained on it, affecting the security and reliability of technologies based on 3D point cloud networks. More transformation-invariant 3D point cloud networks need to be proposed in the future to avoid potential negative impacts.

## Acknowledgements

Shengshan's work is supported by the National Natural Science Foundation of China (Grant Nos. U20A20177, 62372196). Minghui Li is the corresponding author.

## References

* [1] Evangelos Alexiou, Nanyang Yang, and Touradj Ebrahimi. Pointxr: A toolbox for visualization and subjective evaluation of point clouds in virtual reality. In _Proceedings of the 12th International Conference on Quality of Multimedia Experience (QoMEX'20)_, pages 1-6, 2020.
* [2] Iro Armeni, Ozan Sener, Amir R Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3D semantic parsing of large-scale indoor spaces. In _Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'16)_, pages 1534-1543, 2016.
* [3] Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. SemanticKITTI: A dataset for semantic scene understanding of lidar sequences. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV'19)_, pages 9297-9307, 2019.
* [4] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. Shapenet: An information-rich 3D model repository. _arXiv preprint arXiv:1512.03012_, 2015.
* [5] Sizhe Chen, Geng Yuan, Xinwen Cheng, Yifan Gong, Minghai Qin, Yanzhi Wang, and Xiaolin Huang. Self-ensemble protection: Training checkpoints are good data protectors. In _Proceedings of the 11th International Conference on Learning Representations (ICLR'23)_, 2023.
* [6] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia. Multi-view 3D object detection network for autonomous driving. In _Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'17)_, pages 1907-1915, 2017.
* [7] Wenda Chu, Linyi Li, and Bo Li. Tpc: Transformation-specific smoothing for point cloud models. In _Proceedings of the 39th International Conference on Machine Learning (ICML'22)_, pages 4035-4056, 2022.
* [8] Liam Fowl, Micah Goldblum, Ping-yeh Chiang, Jonas Geiping, Wojciech Czaja, and Tom Goldstein. Adversarial examples make strong poisons. In _Proceedings of the 35th Neural Information Processing Systems (NeurIPS'21)_, pages 30339-30351, 2021.
* [9] Shaopeng Fu, Fengxiang He, Yang Liu, Li Shen, and Dacheng Tao. Robust unlearnable examples: Protecting data against adversarial learning. In _Proceedings of the 10th International Conference on Learning Representations (ICLR'22)_, 2022.
* [10] Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. SE (3)-transformers: 3D roto-translation equivariant attention networks. In _Proceedings of the 34th Neural Information Processing Systems (NeurIPS'20)_, volume 33, pages 1970-1981, 2020.

* [11] Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. SE(3)-transformers: 3D roto-translation equivariant attention networks. In _Proceedings of the 34th Neural Information Processing Systems (NeurIPS'20)_, volume 33, pages 1970-1981, 2020.
* [12] Cong Gao, Geng Wang, Weisong Shi, Zhongmin Wang, and Yanping Chen. Autonomous driving security: State of the art and challenges. _IEEE Internet of Things Journal_, 9(10):7572-7595, 2021.
* [13] Kuofeng Gao, Jiawang Bai, Baoyuan Wu, Mengxi Ya, and Shu-Tao Xia. Imperceptible and robust backdoor attack in 3D point cloud. _IEEE Transactions on Information Forensics and Security (TIFS'23)_, 19:1267-1282, 2023.
* [14] Robert Geirhos, Jorn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. _Nature Machine Intelligence_, pages 665-673, 2020.
* [15] Ankit Goyal, Hei Law, Bwei Liu, Alejandro Newell, and Jia Deng. Revisiting point cloud shape classification with a simple and effective baseline. In _Proceedings of the 38th International Conference on Machine Learning (ICML'21)_, pages 3809-3820. PMLR, 2021.
* [16] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R. Martin, and Shi-Min Hu. Pct: Point cloud transformer. _Computational Visual Media_, 7:187-199, 2021.
* [17] Shengshan Hu, Wei Liu, Minghui Li, Yechao Zhang, Xiaogeng Liu, Xianlong Wang, Leo Yu Zhang, and Junhui Hou. Pointcrt: Detecting backdoor in 3D point cloud via corruption robustness. In _Proceedings of the 31st ACM International Conference on Multimedia (MM'23)_, pages 666-675, 2023.
* [18] Shengshan Hu, Junwei Zhang, Wei Liu, Junhui Hou, Minghui Li, Leo Yu Zhang, Hai Jin, and Lichao Sun. Pointca: Evaluating the robustness of 3D point cloud completion models against adversarial examples. In _Proceedings of the 37th AAAI Conference on Artificial Intelligence (AAAI'23)_, pages 872-880, 2023.
* [19] Hanxun Huang, Xingjun Ma, Sarah Monazam Erfani, James Bailey, and Yisen Wang. Unlearnable examples: Making personal data unexploitable. In _Proceedings of the 9th International Conference on Learning Representations (ICLR'21)_, 2021.
* [20] Adel Javanmard and Mahdi Soltanolkotabi. Precise statistical analysis of classification accuracies for adversarial training. _The Annals of Statistics_, 50(4):2127-2156, 2022.
* [21] Xin Jin, Zhaoxing Wu, Chenggen Song, Chunwei Zhang, and Xiaodong Li. 3D point cloud encryption through chaotic mapping. In _Proceedings of the 17th Pacific Rim Conference on Multimedia (PCM'16)_, pages 119-129, 2016.
* [22] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [23] Peiliang Li, Siqi Liu, and Chaojie Shen. Multi-sensor 3D object box refinement for autonomous driving. _arXiv preprint arXiv:1909.04942_, 2019.
* [24] Xinle Li, Zhirui Chen, Yue Zhao, Zekun Tong, Yabang Zhao, Andrew Lim, and Joey Tianyi Zhou. Pointba: Towards backdoor attacks in 3D point cloud. In _Proceedings of the 18th IEEE/CVF International Conference on Computer Vision (ICCV'21)_, pages 16492-16501, 2021.
* [25] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. Pointcnn: Convolution on x-transformed points. In _Proceedings of the 32nd Neural Information Processing Systems (NeurIPS'18)_, pages 828-838, 2018.
* [26] Zhi-Hao Lin, Sheng-Yu Huang, and Yu-Chiang Frank Wang. Convolution in the cloud: Learning deformable kernels in 3D graph convolution networks for point cloud analysis. In _Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR'20)_, pages 1800-1809, 2020.
* [27] Bocheng Liu, Yongxiang Liu, Yiyuan Xie, Xiao Jiang, Yichen Ye, Tingting Song, Junxiong Chai, Meng Liu, Manying Feng, and Haodong Yuan. Privacy protection for 3D point cloud classification based on an optical chaotic encryption scheme. _Optics Express_, 31(5):8820-8843, 2023.
* [28] Shuang Liu, Yihan Wang, and Xiao-Shan Gao. Game-theoretic unlearnable example generator. In _Proceedings of the AAAI Conference on Artificial Intelligence (AAAI'24)_, 2024.
* [29] Yixin Liu, Kaidi Xu, Xun Chen, and Lichao Sun. Stable unlearnable example: Enhancing the robustness of unlearnable examples via stable error-minimizing noise. In _Proceedings of the AAAI Conference on Artificial Intelligence (AAAI'24)_, pages 3783-3791, 2024.
* [30] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. _arXiv preprint arXiv:1608.03983_, 2016.
* [31] Xu Ma, Can Qin, Haoxuan You, Haoxi Ran, and Yun Fu. Rethinking network design and local geometry in point cloud: A simple residual mlp framework. _arXiv preprint arXiv:2202.07123_,2022.
* [32] Moritz Menze and Andreas Geiger. Object scene flow for autonomous vehicles. In _Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'15)_, pages 3061-3070, 2015.
* [33] Yifei Min, Lin Chen, and Amin Karbasi. The curious case of adversarially robust models: More data can help, double descend, or hurt generalization. In _Uncertainty in Artificial Intelligence_, pages 129-139. PMLR, 2021.
* [34] Arpit Nama, Amaya Dharmasiri, Kanchana Thilakarathna, Albert Zomaya, and Jaybie Agullo de Guzman. User configurable 3D object regeneration for spatial privacy. _arXiv preprint arXiv:2108.08273_, 2021.
* [35] Charles Ruizhongtai Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. Pointnet: Deep learning on point sets for 3D classification and segmentation. In _Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'17)_, pages 652-660, 2017.
* [36] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In _Proceedings of the 31st Neural Information Processing Systems (NeurIPS'17)_, pages 5099-5108, 2017.
* [37] Jie Ren, Han Xu, Yuxuan Wan, Xingjun Ma, Lichao Sun, and Jiliang Tang. Transferable unlearnable examples. In _Proceedings of the 11th International Conference on Learning Representations (ICLR'23)_, 2023.
* [38] Douglas A. Reynolds. Gaussian mixture models. _Encyclopedia of Biometrics_, 741(659-663), 2009.
* [39] Vinu Sankar Sadasivan, Mahdi Soltanolkotabi, and Soheil Feizi. CUDA: Convolution-based unlearnable datasets. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR'23)_, pages 3862-3871, 2023.
* [40] StubbornAtom. Distributions of quadratic form of a normal random variable. Cross Validated, 2020. https://stats.stackexchange.com/q/478682.
* [41] Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Duc Thanh Nguyen, and Sai-Kit Yeung. Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data. In _Proceedings of the 17th IEEE/CVF International Conference on Computer Vision (ICCV'19)_, 2019.
* [42] Xianlong Wang, Shengshan Hu, Minghui Li, Zhifei Yu, Ziqi Zhou, and Leo Yu Zhang. Corrupting convolution-based unlearnable datasets with pixel-based image transformations. _arXiv preprint arXiv:2311.18403_, 2023.
* [43] Xianlong Wang, Shengshan Hu, Yechao Zhang, Ziqi Zhou, Leo Yu Zhang, Peng Xu, Wei Wan, and Hai Jin. ECLIPSE: Exparping clean-label indiscriminate poisons via sparse diffusion purification. In _Proceedings of the 29th European Symposium on Research in Computer Security (ESORICS'24)_, pages 146-166, 2024.
* [44] Xianlong Wang, Minghui Li, Peng Xu, Wei Liu, Leo Yu Zhang, Shengshan Hu, and Yanjun Zhang. PointAPA: Towards availability poisoning attacks in 3D point clouds. In _Proceedings of the 29th European Symposium on Research in Computer Security (ESORICS'24)_, pages 125-145, 2024.
* [45] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon. Dynamic graph cnn for learning on point clouds. _ACM Transactions On Graphics (TOG'19)_, pages 1-12, 2019.
* [46] Florian Wirth, Jannik Quehl, Jeffrey Ota, and Christoph Stiller. Pointatme: efficient 3D point cloud labeling in virtual reality. In _Proceedings of the 2019 IEEE Intelligent Vehicles Symposium (IV'19)_, pages 1693-1698, 2019.
* [47] Shutong Wu, Sizhe Chen, Cihang Xie, and Xiaolin Huang. One-pixel shortcut: on the learning preference of deep neural networks. In _Proceedings of the 11th International Conference on Learning Representations (ICLR'23)_, 2023.
* [48] Wenxuan Wu, Zhongang Qi, and Fuxin Li. Pointconv: Deep convolutional networks on 3D point clouds. In _Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR'19)_, pages 9621-9630, 2019.
* [49] Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang Zhao. Point transformer v3: Simpler, faster, stronger. _arXiv preprint arXiv:2312.10035_, 2023.
* [50] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3D shapenets: A deep representation for volumetric shapes. In _Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'15)_, pages 1912-1920, 2015.

* [51] Tiange Xiang, Chaoyi Zhang, Yang Song, Jianhui Yu, and Weidong Cai. Walk in the cloud: Learning curves for point clouds shape analysis. In _Proceedings of the 18th IEEE/CVF International Conference on Computer Vision (ICCV'21)_, pages 915-924, 2021.
* [52] Zhen Xiang, David J. Miller, Siheng Chen, Xi Li, and George Kesidis. A backdoor attack against 3D point cloud classifiers. In _Proceedings of the 18th IEEE/CVF International Conference on Computer Vision (ICCV'21)_, pages 7597-7607, 2021.
* [53] Jiancheng Yang, Qiang Zhang, Rongyao Fang, Bingbing Ni, Jinxian Liu, and Qi Tian. Adversarial attack and defense on point sets. _arXiv preprint arXiv:1902.10899_, 2019.
* [54] Jingwen Ye and Xinchao Wang. Ungeneralizable examples. In _Proceedings of the 2024 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'24)_, 2024.
* [55] Jiaming Zhang, Xingjun Ma, Qi Yi, Jitao Sang, Yugang Jiang, Yaowei Wang, and Changsheng Xu. Unlearnable clusters: Towards label-agnostic unlearnable examples. In _Proceedings of the 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR'23)_, 2023.
* [56] Renrui Zhang, Liuhui Wang, Yali Wang, Peng Gao, Hongsheng Li, and Jianbo Shi. Parameter is not all you need: Starting from non-parametric networks for 3D point cloud analysis. _arXiv preprint arXiv:2303.08134_, 2023.
* [57] Zhiyuan Zhang, Binh-Son Hua, David W. Rosen, and Sai-Kit Yeung. Rotation invariant convolutions for 3D point clouds deep learning. In _Proceedings of the 2019 International Conference on 3D Vision (3DV'19)_, pages 204-213. IEEE, 2019.
* [58] Zhiyuan Zhang, Binh-Son Hua, and Sai-Kit Yeung. Riconv++: Effective rotation invariant convolutions for 3D point clouds deep learning. _International Journal of Computer Vision (IJCV'22)_, 130(5):1228-1243, 2022.
* [59] Chen Zhao, Jiaqi Yang, Xin Xiong, Angafian Zhu, Zhiguo Cao, and Xin Li. Rotation invariant point cloud classification: Where local geometry meets global topology. _arXiv preprint arXiv:1911.00195_, 2019.
* [60] Hang Zhou, Kejiang Chen, Weiming Zhang, Han Fang, Wenbo Zhou, and Nenghai Yu. Dup-net: Denoiser and upsampler network for 3D adversarial point clouds defense. In _Proceedings of the 17th IEEE/CVF International Conference on Computer Vision (ICCV'19)_, pages 1961-1970, 2019.
* [61] Ziqi Zhou, Yufei Song, Minghui Li, Shengshan Hu, Xianlong Wang, Leo Yu Zhang, Dezhong Yao, and Hai Jin. Darksam: Fooling segment anything model to segment nothing. _arXiv preprint arXiv:2409.17874_, 2024.
* [62] Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Jiaming Liu, Han Xiao, Chaoyou Fu, Hao Dong, and Peng Gao. No time to train: Empowering non-parametric networks for few-shot 3D scene segmentation. In _Proceedings of the 2024 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'24)_, 2024.
* [63] Yifan Zhu, Yibo Miao, Yinpeng Dong, and Xiao-Shan Gao. Toward availability attacks in 3D point clouds. _arXiv preprint arXiv:2407.11011_, 2024.

## Appendix A Definitions of 3D Transformations

Existing 3D transformations are summarized in Fig. 1 and formally defined in this section. The 3D transformations are mathematical operations applied to three-dimensional objects to change their position, orientation, and scale in space, which are often represented using transformation matrices. We formally define the transformed point cloud sample with transformation matrix \(\mathbf{T}\in\mathbb{R}^{3\times 3}\) as:

\[\mathbf{X}_{t}=\mathbf{T}\cdot\mathbf{X}\] (10)

where \(\mathbf{X}\in\mathbb{R}^{3\times p}\) is the clean point cloud sample, \(\mathbf{X}_{t}\in\mathbb{R}^{3\times p}\) is the transformed point cloud sample.

### Rotation Transformation

The rotation transformation that alters the orientation and angle of 3D point clouds is controlled by three angles \(\alpha\), \(\beta\), and \(\gamma\). The rotation matrices in three directions can be formally defined as:

\[\mathcal{R}_{\alpha}=\left[\begin{array}{ccc}1&0&0\\ 0&\cos\alpha&-\sin\alpha\\ 0&\sin\alpha&\cos\alpha\end{array}\right],\mathcal{R}_{\beta}=\left[\begin{array} []{ccc}\cos\beta&0&\sin\beta\\ 0&1&0\\ -\sin\beta&0&\cos\beta\end{array}\right],\mathcal{R}_{\gamma}=\left[\begin{array} []{ccc}\cos\gamma&-\sin\gamma&0\\ \sin\gamma&\cos\gamma&0\\ 0&0&1\end{array}\right]\] (11)

Thus we have \(\mathbf{T}=\mathcal{R}_{\alpha}\mathcal{R}_{\beta}\mathcal{R}_{\gamma}\) while employing rotation transformation. Besides, we have \(\mathcal{R}_{\alpha}^{-1}=\mathcal{R}_{\alpha}^{T}\), \(\mathcal{R}_{\beta}^{-1}=\mathcal{R}_{\beta}^{T}\), and \(\mathcal{R}_{\gamma}^{-1}=\mathcal{R}_{\gamma}^{T}\).

### Scaling Transformation

The scaling matrix \(\mathcal{S}\) can be represented as:

\[\mathcal{S}=\left[\begin{array}{ccc}\lambda&0&0\\ 0&\lambda&0\\ 0&0&\lambda\end{array}\right]=\lambda\left[\begin{array}{ccc}1&0&0\\ 0&1&0\\ 0&0&1\end{array}\right]\] (12)

where the scaling factor \(\lambda\) is used to perform a proportional scaling of the coordinates of each point in the point cloud.

### Shear Transformation

In the three-dimensional space, shearing 3 is represented by different matrices, and the specific form depends on the type of shear being performed. Specifically, the shear transformation matrix \(\mathcal{H}_{xy}\) (employed in UMT) of shifting \(x\) and \(y\) by the other coordinate \(z\), and its corresponding inverse matrix can be expressed as:

Footnote 3: https://www.mauriciopppe.com/notes/computer-graphics/transformation-matrices/shearing/

\[\mathcal{H}_{xy}=\begin{bmatrix}1&0&s\\ 0&1&t\\ 0&0&1\end{bmatrix},\mathcal{H}_{xy}^{-1}=\begin{bmatrix}1&0&-s\\ 0&1&-t\\ 0&0&1\end{bmatrix}\] (13)

The shear transformation matrix \(\mathcal{H}_{xz}\) of shifting \(x\) and \(z\) by the other coordinate \(y\), and its corresponding inverse matrix can be expressed as:

\[\mathcal{H}_{xz}=\begin{bmatrix}1&s&0\\ 0&1&0\\ 0&t&1\end{bmatrix},\mathcal{H}_{xz}^{-1}=\begin{bmatrix}1&-s&0\\ 0&1&0\\ 0&-t&1\end{bmatrix}\] (14)

The shear transformation matrix \(\mathcal{H}_{yz}\) of shifting \(y\) and \(z\) by the other coordinate \(x\), and its corresponding inverse matrix can be expressed as:

\[\mathcal{H}_{yz}=\begin{bmatrix}1&0&0\\ s&1&0\\ t&0&1\end{bmatrix},\mathcal{H}_{yz}^{-1}=\begin{bmatrix}1&0&0\\ -s&1&0\\ -t&0&1\end{bmatrix}\] (15)``` Input: Clean 3D point cloud dataset \(\mathcal{D}_{c}=\{(\mathbf{X}_{i},\mathbf{Y}_{i})\}_{i=1}^{n}\); number of categories \(N\); slight range \(r_{s}\); primary range \(r_{p}\); scaling lower bound \(b_{l}\); scaling upper bound \(b_{u}\); twisting lower bound \(w_{l}\); twisting upper bound \(w_{u}\); shear lower bound \(h_{l}\); shear upper bound \(h_{u}\); spectrum of transformations \(k\); matrix set \(\mathcal{T}_{s}=\{\mathcal{R},\mathcal{S},\mathcal{H},\mathcal{W}\}\). Output: Unlearnable 3D point cloud dataset \(\mathcal{D}_{u}=\{(\mathbf{X}_{\mathbf{u}i},\mathbf{Y}_{i})\}_{i=1}^{n}\). Initialize the slight angle lists \(\mathcal{L}_{\alpha}\)= [ ], \(\mathcal{L}_{\beta}\)=[ ], the primary angle list \(\mathcal{L}_{\gamma}\)=[ ], the rotation list \(\mathcal{L}_{\mathcal{R}}\)=[ ], the scaling list \(\mathcal{L}_{\mathcal{S}}\)=[ ], the twisting list \(\mathcal{L}_{\mathcal{W}}\)=[ ], the shear list \(\mathcal{L}_{\mathcal{H}}\)=[ ], and \(\mathcal{A}_{N}=\left\lceil\sqrt[3]{N}\right\rceil\); for\(c=1\) to \(k\)do  Randomly sample a transformation matrix \(\mathcal{V}_{c}\in\mathcal{T}_{s}\);  Remove transformation matrix \(\mathcal{V}_{c}\) from \(\mathcal{T}_{s}\); if\(\mathcal{V}_{c}==\mathcal{R}\)then for\(i=1\) to \(\mathcal{A}_{N}\)do for\(j=1\) to \(\mathcal{A}_{N}\)do for\(k=1\) to \(\mathcal{A}_{N}\)do \(\mathcal{L}_{\mathcal{R}}\leftarrow\mathcal{L}_{\mathcal{R}}\cup\{[\mathcal{L}_ {\alpha}[i],\mathcal{L}_{\beta}[j],\mathcal{L}_{\gamma}[k]]\}\);  end for  end for  end for  Get the \(\mathcal{L}_{\mathcal{R}}\gets random.sample(\mathcal{L}_{\mathcal{R}},N)\);  end for else if\(\mathcal{V}_{c}==\mathcal{S}\)then for\(i=1\) to \(N\)do  Randomly sample \(\lambda_{i}\sim\mathcal{U}(b_{l},b_{u})\);  Add to the list \(\mathcal{L}_{\mathcal{S}}\leftarrow\mathcal{L}_{\mathcal{S}}\cup\{\lambda_{i}\}\);  end for  end for else if\(\mathcal{V}_{c}==\mathcal{W}\)then for\(i=1\) to \(N\)do  Randomly sample \(\omega_{i}\sim\mathcal{U}(w_{l},w_{u})\);  Add to the list \(\mathcal{L}_{\mathcal{W}}\leftarrow\mathcal{L}_{\mathcal{W}}\cup\{\omega_{i}\}\);  end for  end for else if\(\mathcal{V}_{c}==\mathcal{H}\)then for\(i=1\) to \(N\)do  Randomly sample \(h_{i}\sim\mathcal{U}(h_{l},h_{u})\);  Add to the list \(\mathcal{L}_{\mathcal{H}}\leftarrow\mathcal{L}_{\mathcal{H}}\cup\{h_{i}\}\);  end for  end for  end for for\(i=1\) to \(n\)do  Get the transformation matrix \(\mathbf{T}_{k}=\prod_{i=1}^{k}\mathcal{V}_{i}\) by the parameter lists above;  Get the transformed data \(\mathbf{X}_{\mathbf{u}_{i}}=\mathbf{T}_{k}{}_{i}\cdot\overline{\mathbf{X}}_{i}\);  end for Return: Unlearnable 3D point cloud dataset \(\mathcal{D}_{u}\). ```

**Algorithm 1** Our proposed UMT scheme

### Twisting Transformation

The 3D twisting transformation [7] involves a rotational deformation applied to an object in three-dimensional space, creating a twisted or spiraled effect. Unlike simple rotations around fixed axes, a twisting transformation introduces a variable rotation that may change based on the spatial coordinates of the object. For instance, considering a twisting transformation along the \(z\)-axis, where the rotation angle is a function related to the \(z\)-coordinate, it can be expressed as:

\[\mathcal{W}_{z}(\theta,z)=\begin{bmatrix}\cos(\theta z)&-\sin(\theta z)&0\\ \sin(\theta z)&\cos(\theta z)&0\\ 0&0&1\end{bmatrix}\] (16)where \(\theta\) is the parameter of the twisting transformation, and \(z\) is the \(z\)-coordinate of the object. The inverse matrix of \(\mathcal{W}_{z}(\theta,z)\) is:

\[\mathcal{W}_{z}^{-1}(\theta,z)=\begin{bmatrix}\cos(\theta z)&\sin(\theta z)&0\\ -\sin(\theta z)&\cos(\theta z)&0\\ 0&0&1\end{bmatrix}\] (17)

### Tapering Transformation

The tapering transformation [7] is a linear transformation used to alter the shape of an object, causing it to gradually become pointed or shortened. In three-dimensional space, tapering transformation can adjust the dimensions of an object along one or more axes, creating a tapering effect. The specific matrix representation of tapering transformation depends on the chosen axis and the design of the transformation. Generally, tapering transformation can be represented by a matrix that is multiplied by the coordinates of the object to achieve the shape adjustment, which is defined as:

\[\mathcal{A}_{z}(\eta,z)=\begin{bmatrix}1+\eta z&0&0\\ 0&1+\eta z&0\\ 0&0&1\end{bmatrix}\] (18)

where \(z\) is the \(z\)-coordinate of the object. Considering that \(\eta z\) could indeed equal to -1, in such a case, the 3D point cloud samples would be projected onto the \(z\)-plane, losing their practical significance and the tapering matrix is also irreversible.

### Reflection Transformation

Reflection transformation is a linear transformation that inverts an object along a certain plane. This plane is commonly referred to as a reflection plane or mirror. For reflection transformations in three-dimensional space, we can represent them through a matrix. Regarding the reflection transformation matrices of the \(xy\) plane, \(yz\) plane, and \(xz\) plane, we have:

\[\mathcal{R}_{xy}=\begin{bmatrix}1&0&0\\ 0&1&0\\ 0&0&-1\end{bmatrix},\mathcal{R}_{yz}=\begin{bmatrix}-1&0&0\\ 0&1&0\\ 0&0&1\end{bmatrix},\mathcal{R}_{xz}=\begin{bmatrix}1&0&0\\ 0&-1&0\\ 0&0&1\end{bmatrix}\] (19)

### Translation Transformation

The 3D translation transformation4 refers to the process of moving an object in three-dimensional space. This transformation involves moving the object along the \(x\), \(y\), and \(z\) axes, respectively, smoothly transitioning it from one position to another, which is defined as:

Footnote 4: https://www.javatpoint.com/computer-graphics-3d-transformations

\[\mathcal{L}=\begin{bmatrix}1&0&0&t_{x}\\ 0&1&0&t_{y}\\ 0&0&1&t_{z}\\ 0&0&0&1\end{bmatrix}\] (20)

where \(t_{x}\), \(t_{y}\), and \(t_{z}\) represents the translation along the \(x\), \(y\), and \(z\) axes, respectively. This 4\(\times\)4 matrix is a homogeneous coordinate matrix that describes the translation in three-dimensional space. Additionally, the translation matrix also can be represented as a additive matrix to original point \(x_{i}\in\mathbb{R}^{3\times 3}\), which can be defined as:

\[\mathcal{L}=\begin{bmatrix}t_{x}&t_{x}&t_{x}\\ t_{y}&t_{y}&t_{y}\\ t_{z}&t_{z}&t_{z}\end{bmatrix}\] (21)

## Appendix B Supplementary Experimental Settings

### Experimental Platform

Our experiments are conducted on a server running a 64-bit Ubuntu 20.04.1 system with an Intel Xeon Silver 4210R CPU @ 2.40GHz processor, 125GB memory, and four Nvidia GeForce RTX 3090 GPUs, each with 24GB memory. The experiments are performed using the Python language, version 3.8.19, and PyTorch library version 1.12.1.

### Hyper-Parameter Settings

The model training process on the unlearnable dataset and the clean dataset remains consistent, using the Adam optimizer [22], CosineAnnealingLR scheduler [30], initial learning rate of 0.001, weight decay of 0.0001, batch size of 16 (due to insufficient GPU memory, the batch size is set to 8 when training 3DGCN on ModelNet40 dataset), and training for 80 epochs. Due to the longer training process required by PCT [16], the training epochs for PCT in Tab. 1 on ModelNet10, ModelNet40, and ScanObjectNN datasets are all set to 240.

### Settings of Exploring Experiments

We initially investigate which approach yields the best unlearnable effect among _sample-wise_, _dataset-wise_, and _class-wise_ settings in Fig. 2 (a). Specific experimental settings are as follows:

In the _dataset-wise_ setting, the same parameter values are applied to the entire dataset. Specifically, we have \(\alpha=\beta=\gamma=10^{\circ}\) in the rotation transformation, the scaling factor \(\lambda\) is set to 0.8, both shearing factors \(s\) and \(t\) are set to 0.2. The angle \(\theta\) in twisting is \(25^{\circ}\). The tapering angle \(\eta\) is set to \(25^{\circ}\), and the parameters in translation transformation \(t_{x}\), \(t_{y}\), and \(t_{z}\) are set to 0.15.

In the _sample-wise_ setting, each sample has its independent set of parameters, meaning the parameter values for each sample are randomly generated within a certain range. In the rotation transformation, \(\alpha\), \(\beta\), \(\gamma\) are uniformly sampled from the range of \(0^{\circ}\) to \(20^{\circ}\). The scaling factor \(\lambda\) is uniformly sampled from 0.6 to 0.8, shearing factors \(s\) and \(t\) are uniformly sampled from the range of 0 to 0.4. Both the twist angle \(\theta\) and tapering angle \(\eta\) are sampled from \(0^{\circ}\) to \(50^{\circ}\), and the parameters in translation transformation \(t_{x}\), \(t_{y}\), and \(t_{z}\) are sampled from 0 to 0.3.

In the _class-wise_ setting, the parameters for transformations are associated with the point cloud's class. The selection of parameters for each class is also obtained by random sampling within a fixed range. The chosen ranges are generally consistent with those described for the _sample-wise_ setting above. However, a difference lies in the consideration of slight angle range and primary angle range in the case of rotation transformations, where these ranges are \(20^{\circ}\) and \(120^{\circ}\), respectively.

The specific results of test accuracy under different transformation modes, including sample-wise (random), class-wise, and dataset-wise (universal), are provided in Tab. 5. It can be seen that under the class-wise setting, the final unlearnable effect is the best.

### Benchmark Datasets

**Dataset Introduction.** The ModelNet40 dataset is a point cloud dataset containing 40 categories, comprising 9843 training and 2468 test point cloud data. ModelNet10 is a subset of ModelNet40 dataset with 10 categories. ShapeNetPart that includes 16 categories is a subset of ShapeNet, comprising 12137 training and 2874 test point cloud samples. ScanObjectNN is a real-world point cloud dataset with 15 categories, comprising 2309 training samples and 581 test samples. Similar to [17; 52], we split KITTI object clouds into class "vehicle" and "human" containing 1000 training

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline Test sets \(\downarrow\) Transformations \(\longrightarrow\) & Rotation & Scaling & Shear & Twisting & Tapering & Translation \\ \hline Class-wise test set & 99.67 & 93.80 & 97.91 & 94.05 & 96.04 & 98.24 \\ Permuted class-wise test set & 10.68 & 38.22 & 58.37 & 60.68 & 42.51 & 31.94 \\ Clean test set & 29.85 & 20.81 & 67.84 & 63.22 & 52.42 & 36.67 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Accuracy results obtained with different test sets when training the PointNet classifier using class-wise transformed training sets

\begin{table}
\begin{tabular}{c|c c c c c|c} \hline \hline Transformations & Rotation & Scaling & Shear & Twisting & Tapering & Translation & **AVG** \\ \hline w/o & 89.32 & 89.32 & 89.32 & 89.32 & 89.32 & 89.32 \\ Sample-wise & 91.52 & 87.78 & 89.10 & 92.62 & 90.31 & 90.31 & 89.80 \\ Dataset-wise & 87.89 & 79.41 & 85.79 & 92.18 & 88.22 & 88.22 & 83.45 \\ Class-wise & 29.85 & 20.81 & 67.84 & 63.22 & 52.42 & 36.67 & 45.14 \\ \hline \hline \end{tabular}
\end{table}
Table 5: The test accuracy (%) results on diverse types of transformations on ModelNet10 training set using PointNet classifier

[MISSING_PAGE_FAIL:18]

two types of rigid transformations, achieves the most effective unlearnable effect. The transformation parameters used in this section are consistent with Appendix B.3.

To ensure the reliability of the experimental results, we combine two different transformations and obtain the results from three runs using random seeds with 23, 1023, and 2023, then average them. The results are shown in Tab. 8. From the results on four popular point cloud models, it can be seen that when all transformations are rigid, the test accuracy is still the lowest.

### More Results of UMT Against Adaptive Attacks

To further explore the performance of UMT against adaptive attacks, we supplement the experimental results using four types of random augmentations \(\mathcal{RSHW}\) in Tab. 10, and find that the conclusion is consistent with Tab. 2, _i.e._, UMT exhibits a certain degree of robustness against the adaptive attack random transformations.

### More Results of UMT Against Semantic Segmentation

To further evaluate the UMT performance against semantic segmentation, we employ the semantic scene understanding dataset SemanticKITTI [3] in Tab. 11. It can be observed that UMT is still effective against this real-world segmentation dataset.

### Results of UMT Against SE(3) Equivariant Models

In the main text, we have discussed the robustness of UMT against rotation/scaling invariant models. The results for RIConv++ [58] (rotation invariant) and 3DGCN [26] (scaling invariant) networks in Tab. 1, as well as RIConv [57], LGR-Net [59] (rotation invariant), and 3DGCN in Tab. 4, demonstrating that these invariant networks can indeed defend against class-wise rotation and scaling. It is worth noting that these networks, which are invariant to a single transformation, cannot defend against UMT formed by a combination of multiple transformations. Thus, it appears that networks like SE(3) equivariant model, which are invariant to multiple transformations in space, can poten

\begin{table}
\begin{tabular}{c|c c c c c|c} \hline \hline Transformations & PointNet & PointNet++ & DGCNN & PointCNN & PCT & **AVG** \\ \hline \(\mathcal{R}\) & 46.70 & 38.99 & 49.67 & 64.87 & 27.53 & 45.55 \\ \(\mathcal{S}\) & 34.80 & 57.05 & 42.62 & 33.81 & 29.63 & 39.58 \\ \(\mathcal{H}\) & 64.10 & 66.41 & 71.15 & 60.68 & 60.13 & 64.49 \\ \(\mathcal{W}\) & 76.98 & 54.19 & 78.63 & 86.69 & 38.55 & 67.05 \\ \(\mathcal{RS}\) & 22.25 & **28.08** & **14.10** & **21.48** & **11.89** & **19.56** \\ \(\mathcal{RH}\) & 30.51 & 39.98 & 45.93 & 36.23 & 33.81 & 37.29 \\ \(\mathcal{RW}\) & 34.91 & 44.27 & 29.52 & 46.92 & 32.16 & 37.56 \\ \(\mathcal{SH}\) & 20.26 & 44.93 & 38.66 & 43.50 & 34.80 & 36.43 \\ \(\mathcal{SW}\) & 18.28 & 44.27 & 31.39 & 34.25 & 19.93 & 29.62 \\ \(\mathcal{HW}\) & 63.33 & 62.56 & 61.89 & 64.76 & 51.43 & 60.79 \\ \(\mathcal{RSH}\) & **15.97** & 29.19 & 36.56 & 21.81 & 31.28 & 26.96 \\ \(\mathcal{RSW}\) & 25.22 & 31.50 & 23.57 & 31.83 & 38.55 & 30.13 \\ \(\mathcal{SHW}\) & 21.70 & 46.37 & 55.29 & 37.11 & 38.77 & 39.85 \\ \(\mathcal{RSHW}\) & 16.52 & 33.37 & 30.51 & 30.73 & 32.49 & 28.72 \\ \hline \hline \end{tabular}
\end{table}
Table 7: The test accuracy (%) results obtained from training the point cloud classifiers PointNet, PointNet++, DGCNN, PointCNN, PCT using a ModelNet10 dataset generated with diverse combinations of transformations under a class-wise setting, where \(\mathcal{R}\), \(\mathcal{S}\), \(\mathcal{H}\), and \(\mathcal{W}\) correspond to rotation, scaling, shear, and twisting respectively

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline ModelNet10 dataset & PointNet & PointNet++ & DGCNN & PointCNN & **AVG** \\ \hline \(\mathcal{RS}\) & 15.12\({}_{46.20}\) & 26.62\({}_{45.11}\) & 25.22\({}_{10.20}\) & 17.26\({}_{33.68}\) & 21.05\({}_{0.73}\) \\ \(\mathcal{RH}\) & 33.70\({}_{11.09}\) & 30.65\({}_{17.12}\) & 36.67\({}_{15.64}\) & 34.99\({}_{13.88}\) & 34.00\({}_{013.66}\) \\ \(\mathcal{RW}\) & 39.83\({}_{46.71}\) & 30.18\({}_{12.22}\) & 36.71\({}_{18.10}\) & 43.47\({}_{92.00}\) & 37.55\({}_{0.07}\) \\ \(\mathcal{SH}\) & 21.22\({}_{28.04}\) & 46.15\({}_{8.85}\) & 31.87\({}_{76.82}\) & 30.87\({}_{10.94}\) & 32.53\({}_{33.81}\) \\ \(\mathcal{SW}\) & 23.50\({}_{0.68}\) & 51.28\({}_{46.69}\) & 31.87\({}_{28.72}\) & 35.34\({}_{28.85}\) & 35.34\({}_{28.88}\) \\ \(\mathcal{HW}\) & 54.41\({}_{47.99}\) & 54.22\({}_{24.14}\) & 55.14\({}_{49.82}\) & 57.75\({}_{0.09}\) & 55.38\({}_{0.98}\) \\ \hline \hline \end{tabular}
\end{table}
Table 8: The test accuracy (%) results with standard deviations from three runs (random seeds are set to 23, 1023, and 2023) obtained from training the point cloud classifiers PointNet, PointNet++, DGCNN, and PointCNN using a ModelNet10 dataset generated with diverse two class-wise transformations

[MISSING_PAGE_FAIL:20]

decreases, indicating that the unlearnable effect becomes more pronounced. This strongly suggests that the class-wise setting is more effective than the sample-wise setting.

### Additional Visual Presentations for 3D Point Cloud Samples

We visualize clean point cloud samples and UMT (\(k=2\) using \(\mathcal{RS}\)) point cloud samples on four benchmark datasets ModelNet10 [50], ModelNet40 [50], ScanObjectNN [41], and ShapeNetPart [4],

\begin{table}
\begin{tabular}{c|c c|c c} \hline Evaluation metrics & \multicolumn{2}{c|}{Eval accuracy (\%)} & \multicolumn{2}{c}{mIoU(\%)} \\ Segmentation models & PointNet++ + & Point Transformer V2 & PointNet++ + & Point Transformer V2 \\ \hline Clean baseline & 29.89 & 72.92 & 14.16 & 54.78 \\ UMT (\(k=2,\mathcal{RS}\)) & **4.69** & **19.40** & **0.80** & **13.39** \\ \hline \hline \end{tabular}
\end{table}
Table 11: **Semantic segmentation:** Evaluation of UMT on semantic segmentation task using SemanticKITTI dataset

Figure 8: Clean and UMT samples from ModelNet40 dataset

Figure 7: Clean and UMT samples from ModelNet10 dataset

Figure 9: Clean and UMT samples from ScanObjectNN dataset

as depicted in Figs. 7 to 10, respectively. The UMT parameters consist of rotation and scaling parameters [\(\alpha\), \(\beta\), \(\gamma\), \(\lambda\)]. It can be observed that these unlearnable samples exhibit similar feature information to normal samples, presenting good visual effects and making it difficult to be detected as abnormalities.

## Appendix D Proofs for Theories

### Proof for Lemma 3

**Lemma 3**.: _The unlearnable dataset \(\mathcal{D}_{u}\) generated using UMT on \(\mathcal{D}_{c}\) can also be represented using a GMM, i.e., \(\mathcal{D}_{u}\sim\mathcal{N}(y\mathbf{T}_{y}\mu,\lambda_{y}^{2}\bm{I})\)._

**Proof:** Assuming \(y=1\), then \(\mathcal{D}_{c1}\sim\mathcal{N}(\mu,\bm{I})\), and we have

\[\mathbb{E}_{(x,y)\sim\mathcal{D}_{c1}}\mathbf{T}_{1}x=\mathbf{T}_ {1}\mathbb{E}_{(x,y)\sim\mathcal{D}_{c1}}x=\mathbf{T}_{1}\mu,\] \[\mathbb{E}_{(x,y)\sim\mathcal{D}_{c1}}(\mathbf{T}_{1}x-\mathbf{T} _{1}\mu)(\mathbf{T}_{1}x-\mathbf{T}_{1}\mu)^{\top}\] \[=\mathbf{T}_{1}\mathbb{E}_{(x,y)\sim\mathcal{D}_{c1}}(x-\mu)(x- \mu)^{\top}\mathbf{T}_{1}{}^{\top}\] \[=\mathbf{T}_{1}I\bm{\Gamma}\mathbf{T}_{1}{}^{\top}=\lambda_{1}{}^ {2}\mathcal{R}_{1}\mathcal{R}_{1}{}^{\top}\bm{I}=\lambda_{1}{}^{2}\bm{I}\]

Similarly, assuming \(y=-1\), we can obtain

\[\mathbb{E}_{(x,y)\sim\mathcal{D}_{c-1}}\mathbf{T}_{-1}x=-\mathbf{T }_{-1}\mu,\] \[\mathbb{E}_{(x,y)\sim\mathcal{D}_{c-1}}(\mathbf{T}_{-1}x-\mathbf{ T}_{-1}\mu)(\mathbf{T}_{-1}x-\mathbf{T}_{-1}\mu)^{\top}=\lambda_{-1}{}^{2}\bm{I}.\]

Thus we have: \(\mathcal{D}_{u}\sim\mathcal{N}(y\mathbf{T}_{y}\mu,\lambda_{y}^{2}\bm{I})\).

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c} \hline Benchmark datasets & Modules\(\backslash\) Models\(\backslash\) & PointNet & PointNet++ & DGCNN & PointCNN & **PCT** & **AVG** \\ \hline \multirow{2}{*}{ShapeNetPart [4]} & Rotation module & 53.51 & 44.05 & 57.69 & 49.48 & **43.84** & 49.71 \\  & Scaling module & 28.74 & 77.21 & 37.93 & 44.02 & 51.84 & 47.95 \\  & **UMT***2 & **15.14** & **41.16** & **26.17** & **32.36** & 44.05 & **31.78** \\ \hline \end{tabular}
\end{table}
Table 14: **Ablation modules:** The test accuracy (%) results achieved by training on unlearnable data created by different modules on the ShapeNetPart dataset

Figure 10: Clean and UMT samples from ShapeNetPart dataset

### Proof for Lemma 4

**Lemma 4**.: _The Bayes optimal decision boundary for classifying \(\mathcal{D}_{u}\) is given by \(P_{u}(x)\equiv\mathcal{A}x^{\top}x+\mathcal{B}^{\top}x+\mathcal{C}=0\), where \(\mathcal{A}=\lambda_{-1}^{-2}-\lambda_{1}^{-2}\), \(\mathcal{B}=2(\lambda_{-1}^{-2}\mathbf{T}_{-1}+\lambda_{1}^{-2}\mathbf{T}_{1})\mu\), and \(\mathcal{C}=\ln\frac{|\lambda_{-1}^{2}\mathbf{I}|}{|\lambda_{1}^{2}\mathbf{I}|}\)._

**Proof:** At the optimal decision boundary the probabilities of any point \(x\in\mathbb{R}^{d}\) belonging to class \(y=1\) and \(y=-1\) modeled by \(\mathcal{D}_{u}\) are the same. Similar to the optimal decision boundary of the clean dataset \(\mathcal{D}_{c}\), we have:

\[\frac{exp[-\frac{1}{2}(x-\mathbf{T}_{-1}\mu_{-1})^{\top}(\lambda_ {-1}^{2}\mathbf{I})^{-1}(x-\mathbf{T}_{-1}\mu_{-1})]}{\sqrt{(2\pi)^{d}|\lambda _{-1}^{2}\mathbf{I}|}}\] \[=\frac{exp[-\frac{1}{2}(x-\mathbf{T}_{1}\mu_{1})^{\top}(\lambda_ {1}^{2}\mathbf{I})^{-1}(x-\mathbf{T}_{1}\mu_{1})]}{\sqrt{(2\pi)^{d}|\lambda_{1 }^{2}\mathbf{I}|}}\] \[\Rightarrow(x-\mathbf{T}_{-1}\mu_{-1})^{\top}\lambda_{-1}^{-2}(x -\mathbf{T}_{-1}\mu_{-1})+\ln|\lambda_{-1}^{2}\mathbf{I}|\] \[=(x-\mathbf{T}_{1}\mu_{1})^{\top}\lambda_{1}^{-2}(x-\mathbf{T}_{ 1}\mu_{1})+\ln|\lambda_{1}^{2}\mathbf{I}|\] \[\Rightarrow\lambda_{-1}^{-2}(x^{\top}x-x^{\top}\mathbf{T}_{-1}\mu _{-1}-\mu_{-1}^{\top}\mathbf{T}_{-1}^{\top}x+\mu_{-1}^{\top}\mathbf{T}_{-1}^{ \top}\mathbf{T}_{-1}\mu_{-1})\] \[-\lambda_{1}^{-2}(x^{\top}x-x^{\top}\mathbf{T}_{1}\mu_{1}-\mu_{1} ^{\top}\mathbf{T}_{1}^{\top}x+\mu_{1}^{\top}\mathbf{T}_{1}^{\top}\mathbf{T}_{ 1}\mu_{1})+\ln\frac{|\lambda_{-1}^{2}\mathbf{I}|}{|\lambda_{1}^{2}\mathbf{I}| }=0\] \[\Rightarrow(\lambda_{-1}^{-2}-\lambda_{1}^{-2})x^{\top}x-2( \lambda_{-1}^{-2}\mu_{-1}^{\top}\mathbf{T}_{-1}^{\top}-\lambda_{1}^{-2}\mu_{1} ^{\top}\mathbf{T}_{1}^{\top})x\] \[+\mu_{-1}^{\top}\mu_{-1}-\mu_{1}^{\top}\mu_{1}+\ln\frac{|\lambda_ {-1}^{2}\mathbf{I}|}{|\lambda_{1}^{2}\mathbf{I}|}=0\] \[\Rightarrow P_{u}(x)\equiv\mathcal{A}x^{\top}x+2[(\lambda_{-1}^{-2} \mathbf{T}_{-1}+\lambda_{1}^{-2}\mathbf{T}_{1})\mu]^{\top}x+\ln\frac{|\lambda_ {-1}^{2}\mathbf{I}|}{|\lambda_{1}^{2}\mathbf{I}|}=0\] \[\Rightarrow P_{u}(x)\equiv\mathcal{A}x^{\top}x+\mathcal{B}^{\top}x+ \mathcal{C}=0\]

where \(\mathcal{A}=\lambda_{-1}^{-2}-\lambda_{1}^{-2}\), \(\mathcal{B}=2(\lambda_{-1}^{-2}\mathbf{T}_{-1}+\lambda_{1}^{-2}\mathbf{T}_{1})\mu\), and \(\mathcal{C}=\ln\frac{|\lambda_{-1}^{2}\mathbf{I}|}{|\lambda_{1}^{2}\mathbf{I}|}\). Besides, note that if \(P_{u}(x)\) is less than 0, the category of the Bayesian optimal classification is -1; otherwise, it is 1.

### Proof for Lemma 5

**Lemma 5**.: _Let \(z\sim\mathcal{N}(0,\mathbf{I})\), \(Z=z^{\top}z+b^{\top}z+c\), and \(\left\lVert\cdot\right\rVert_{2}\) denote 2-norm of vectors. For any \(t\geq 0\) and \(\gamma\in\mathbb{R}\), we use Chernoff bound to have:_

\[\mathbb{P}\{Z\geq\mathbb{E}[Z]+\gamma\}\leq\frac{\exp\left\{\frac{t^{2}}{2(1-2 t)}||b||_{2}^{2}-t(\gamma+d)\right\}}{|(1-2t)\mathbf{I}|^{\frac{1}{2}}}\]

**Proof:** Since \(\mathcal{A}\) is a constant, we have:

\[P_{u}(x)\equiv x^{\top}x+(\frac{\mathcal{B}}{\mathcal{A}})^{\top}x+\frac{ \mathcal{C}}{\mathcal{A}}=0\] \[\Rightarrow P_{u}(x)\equiv x^{\top}x+b^{\top}x+c=0\]

\begin{table}
\begin{tabular}{c|c c c c|c} \hline \hline ModelNet10 [50] & PointNet & PointNet++ & DGCNN & PointCNN & **AVG** \\ \hline
100\% sample-wise data & 72.69 & 79.52 & 85.79 & 75.66 & 78.42 \\
20\% class-wise data, 80\% sample-wise data & 65.64 & 69.05 & 78.19 & 58.04 & 67.73 \\
40\% class-wise data, 60\% sample-wise data & 58.04 & 59.91 & 60.57 & 58.59 & 59.28 \\
60\% class-wise data, 80\% sample-wise data & 50.11 & 48.13 & 60.35 & 46.48 & 51.27 \\
80\% class-wise data, 20\% sample-wise data & 31.83 & 29.19 & 44.09 & 50.55 & 39.02 \\
100\% class-wise data & **22.25** & **28.08** & **14.10** & **21.48** & **21.48** \\ \hline \hline \end{tabular}
\end{table}
Table 15: **Mixture results:** The test accuracy (%) results achieved by training on the mixture data consisting of class-wise UMT samples and sample-wise UMT sampleswhere \(b=\frac{B}{\mathcal{A}},c=\frac{C}{\mathcal{A}}\). Let \(Z=z^{\top}z+b^{\top}z+c\) and \(z\sim\mathcal{N}(0,\bm{I})\subset\mathbb{R}^{d}\). Thus we have:

\[Z=z^{\top}z+b^{\top}z+c=(z^{\top}+\frac{1}{2}b^{\top})(z+\frac{1} {2}b)+c-\frac{1}{4}b^{\top}b\] \[=(z+\frac{1}{2}b)^{\top}(z+\frac{1}{2}b)+c-\frac{1}{4}b^{\top}b\]

For any \(t\geq 0\) and \(x\sim\mathcal{N}(0,\bm{I})\), we write the moment generating function for a quadratic random variable \(Y=x^{\top}x\) as5:

Footnote 5: [40]

\[\mathbb{E}[\exp(tY)]=\frac{1}{(2\pi)^{d/2}}\int_{\mathbb{R}^{d}} \exp\left\{tx^{\top}x\right\}\exp\left\{-\frac{1}{2}(x-\mu)^{\top}(x-\mu) \right\}dx\] \[=\frac{\exp\left\{-\mu^{\top}\mu/2\right\}}{(2\pi)^{d/2}}\int_{ \mathbb{R}^{d}}\exp\left\{\frac{2t-1}{2}x^{\top}x+\mu^{\top}x\right\}dx\] \[=\frac{\exp\left\{-\mu^{\top}\mu/2\right\}}{(2\pi)^{d/2}}\frac{( 2\pi)^{d/2}\exp\left\{\frac{1}{2(1-2t)}\mu^{\top}\mu\right\}}{|\bm{I}-2t\bm{I}| ^{\frac{1}{2}}}\] \[=\frac{\exp\left\{\frac{t}{1-2t}\mu^{\top}\mu\right\}}{|\bm{I}-2 t\bm{I}|^{\frac{1}{2}}}\]

\[\Longrightarrow\mathbb{E}[\exp(tZ)]=\frac{\exp\left\{\frac{t}{4(1-2t)}b^{\top }b+t(c-\frac{1}{4}b^{\top}b)\right\}}{|(1-2t)\bm{I}|^{\frac{1}{2}}}=\frac{\exp \{\frac{t^{2}}{2(1-2t)}b^{\top}b+tc\}}{|(1-2t)\bm{I}|^{\frac{1}{2}}}\]

After that, we employ Chernoff bound, for some \(\gamma\), we have:

\[\mathbb{P}\{Z\geq\mathbb{E}[Z]+\gamma\}\leq\frac{\mathbb{E}[\exp (tZ)]}{\exp\{t[\gamma+\mathbb{E}(Z)]\}}\] \[=\frac{\exp\left\{\frac{t^{2}}{2(1-2t)}b^{\top}b+tc\right\}}{\exp \{t(\gamma+\mathbb{E}[z^{\top}z]+c)\}|(1-2t)\bm{I}|^{\frac{1}{2}}}\] \[=\frac{\exp\left\{\frac{t^{2}}{2(1-2t)}b^{\top}b+tc\right\}}{\exp \{t(\gamma+Tr(\bm{I})+\mathbb{E}(b^{\top}z)+c)\}|(1-2t)\bm{I}|^{\frac{1}{2}}}\] \[=\frac{\exp\left\{\frac{t^{2}}{2(1-2t)}b^{\top}b+tc\right\}}{\exp \{t(\gamma+d+c)\}|(1-2t)\bm{I}|^{\frac{1}{2}}}\] \[=\frac{\exp\left\{\frac{t^{2}}{2(1-2t)}||b||_{2}^{2}-t(\gamma+d) \right\}}{|(1-2t)\bm{I}|^{\frac{1}{2}}}\]

### Proof for Theorem 6

**Theorem 6**.: _For any constant \(t_{1}\) and \(t_{2}\) satisfying \(0\leq t_{1}<\frac{1}{2}\) and \(0\leq t_{2}<\frac{1}{2}\), the accuracy of the unlearnable decision boundary \(P_{u}\) on the dataset \(D_{c}\) can be upper-bounded as:_

\[\tau_{\mathcal{D}_{c}}(P_{u}) \leq\frac{\exp\left\{\frac{t_{1}^{2}}{2(1-2t_{1})}||b+2\mu||_{2}^{ 2}+t_{1}(\mu^{\top}\mu+b^{\top}\mu+c)\right\}}{2|(1-2t_{1})\bm{I}|^{\frac{1}{2}}}\] \[+\frac{\exp\left\{\frac{t_{2}^{2}}{2(1-2t_{2})}||b-2\mu||_{2}^{2}- t_{2}(\mu^{\top}\mu-b^{\top}\mu+c+2d)\right\}}{2|(1-2t_{2})\bm{I}|^{\frac{1}{2}}}\] \[:=p_{1}+p_{2}\]_Furthermore, if \(\mu^{\top}\mu+b^{\top}\mu+c+d<0\) and \(-\mu^{\top}\mu+b^{\top}\mu-c-d<0\), we have \(\tau_{\mathcal{D}_{c}}(P_{u})<1\). Moreover, for any \(\mu\neq 0\)\(\exists\) transformation matrix \(\mathbf{T}_{i}\) such that \(\tau_{\mathcal{D}_{c}}(P_{u})<\tau_{\mathcal{D}_{c}}(P)\)._

**Proof:** We note that if \(P_{u}(x)\) is less than 0, the category of the Bayesian optimal classification is -1; otherwise, it is 1. Here, \(x=y\mu+z\) where \(z\sim\mathcal{N}(0,\bm{I})\) and \(y\in\{\pm 1\}\) since \((x,y)\sim\mathcal{D}_{c}\).

\[\tau_{\mathcal{D}_{c}}(P_{u}) =\mathbb{E}\{\mathbb{I}(y(x^{\top}x+b^{\top}x+c)>0)\}\] \[=\mathbb{P}\{y(\mu^{\top}\mu+z^{\top}z+2y\mu^{\top}z+yb^{\top}\mu +b^{\top}z+c)>0\}\] \[=\mathbb{P}(y=1)\mathbb{P}\{y(\mu^{\top}\mu+z^{\top}z+2y\mu^{\top }z+yb^{\top}\mu+b^{\top}z\] \[+c)>0|y=1\}+\mathbb{P}(y=-1)\mathbb{P}\{y(\mu^{\top}\mu+z^{\top}z\] \[+2y\mu^{\top}z+yb^{\top}\mu+b^{\top}z+c)>0|y=-1\}\] \[=\frac{1}{2}\mathbb{P}\{z^{\top}z+(b+2\mu)^{\top}z+\mu^{\top}\mu+ b^{\top}\mu+c>0\}\] \[+\frac{1}{2}\mathbb{P}\{-z^{\top}z-(b-2\mu)^{\top}z-\mu^{\top}\mu +b^{\top}\mu-c>0\}\] \[:=p_{1}+p_{2}\]

We can see that:

\[-\gamma_{1} :=\mathbb{E}\{z^{\top}z+(b+2\mu)^{\top}z+\mu^{\top}\mu+b^{\top}\mu +c\}\] \[=Tr(\bm{I})+\mu^{\top}\mu+b^{\top}\mu+c\] \[-\gamma_{2} :=\mathbb{E}\{-z^{\top}z-(b-2\mu)^{\top}z-\mu^{\top}\mu+b^{\top} \mu-c\}\] \[=-Tr(\bm{I})-\mu^{\top}\mu+b^{\top}\mu-c\]

Applying Lemma 5, with \(\gamma=\gamma_{1}\), \(t=t_{1}\) for the computation of \(p_{1}\), as well as \(\gamma=\gamma_{2}\) and \(t=t_{2}\) for the computation of \(p_{2}\), where \(t_{1}\) and \(t_{2}\) are specific non-negative constants, we obtain:

\[p_{1}=\frac{\exp\left\{\frac{t_{1}^{2}}{2(1-2t_{1})}||b+2\mu||_{2}^{2}+t_{1}( \mu^{\top}\mu+b^{\top}\mu+c)\right\}}{2|(1-2t_{1})\bm{I}|^{\frac{1}{2}}}\]

\[p_{2}=\frac{\exp\left\{\frac{t_{2}^{2}}{2(1-2t_{2})}||b-2\mu||_{2}^{2}-t_{2}( \mu^{\top}\mu-b^{\top}\mu+c+2d)\right\}}{2|(1-2t_{2})\bm{I}|^{\frac{1}{2}}}\]

This provides us with the upper bound for \(\tau_{\mathcal{D}_{c}}(P_{u})\). Nonetheless, to ensure that this upper bound is less than 1, additional conditions need to be affirmed. As \(\gamma_{1}\) and \(\gamma_{2}\) increase, the values of \(p_{1}\) and \(p_{2}\) diminish (\(p_{1}>0\), \(p_{2}>0\)), and as \(\gamma_{1}\) increases, \(\gamma_{2}\) decreases (since \(\gamma_{1}+\gamma_{2}=-2b^{\top}\mu\)). We let \(\frac{||b+2\mu||_{2}^{2}}{2}\) equal to \(\alpha_{1}\geq 0\), \(\mu^{\top}\mu+b^{\top}\mu+c\) (also equals to \(||\mu||_{2}^{2}+c-\frac{\gamma_{1}+\gamma_{2}}{2}\)) equal to \(\beta_{1}\), resulting in:

\[\frac{\mathrm{d}p_{1}}{\mathrm{d}t_{1}}=\frac{1}{2}\frac{\mathrm{ d}[\exp\{\frac{\alpha_{1}t_{1}^{2}}{1-2t_{1}}+\beta_{1}t_{1}\}/(1-2t_{1})^{\frac{d}{2}} ]}{\mathrm{d}t_{1}}\] \[=\frac{\exp\{\frac{\alpha_{1}t_{1}^{2}}{1-2t_{1}}+\beta_{1}t_{1} \}}{2}[(1-2t_{1})^{-\frac{d}{2}-1}d\] \[+(1-2t_{1})^{-\frac{d}{2}}(\frac{2\alpha_{1}t_{1}(1-t_{1})}{(1-2t_ {1})^{2}}+\beta_{1})]\] \[=\frac{\exp\{\frac{\alpha_{1}t_{1}^{2}}{1-2t_{1}}+\beta_{1}t_{1} \}}{2(1-2t_{1})^{\frac{d}{2}}}[\frac{d}{1-2t_{1}}+\frac{2\alpha_{1}t_{1}(1-t_{ 1})}{(1-2t_{1})^{2}}+\beta_{1}]\] \[=p_{1}(\frac{d}{1-2t_{1}}+\frac{2\alpha_{1}t_{1}(1-t_{1})}{(1-2t_ {1})^{2}}+\beta_{1})\] \[=\frac{2(2\beta_{1}-\alpha_{1})t_{1}^{2}+2(\alpha_{1}-2\beta_{1}-d )t_{1}+\beta_{1}+d}{(1-2t_{1})^{2}}p_{1}\]Making \(|(1-2t_{1})\bm{I}|^{\frac{1}{2}}\) meaningful requires satisfying the following condition:

\[1-2t_{1}>0\Longrightarrow 0\leq t_{1}<\frac{1}{2}\]

We note that \(p_{1}(t_{1}=0)=\frac{1}{2}\), \(p_{1}(t_{1}\to\frac{1}{2})=+\infty\). When we set \(\frac{\mathrm{d}p_{1}}{\mathrm{d}t_{1}}=0\), we obtain that:

\[2(2\beta_{1}-\alpha_{1})t_{1}^{2}+2(\alpha_{1}-2\beta_{1}-d)t_{1 }+d+\beta_{1}=0\] \[\Longrightarrow t_{1}=\frac{2\beta_{1}-\alpha_{1}+d\pm\sqrt{ \alpha_{1}^{2}-2\alpha_{1}\beta_{1}+d^{2}}}{2(2\beta_{1}-\alpha_{1})}\] \[\Longrightarrow\alpha_{1}^{2}-2\beta_{1}\alpha_{1}+d^{2}\geq 0\] \[\Longrightarrow\beta_{1}\leq\frac{d^{2}+\alpha_{1}^{2}}{2\alpha_{ 1}}=\frac{d^{2}}{2\alpha_{1}}+\frac{\alpha_{1}}{2}\] \[\Longrightarrow\beta_{1}\leq(\frac{d^{2}}{2\alpha_{1}}+\frac{ \alpha_{1}}{2})_{min}=d\]

**Explanation of the Last Inequality.** Assuming we define \(s(\alpha_{1})=\frac{d^{2}}{2\alpha_{1}}+\frac{\alpha_{1}}{2}\). The minimum value of this function can be obtained using the _Arithmetic Mean-Geometric Mean Inequality_ (AM-GM Inequality), which states that \(a+b\geq 2\sqrt{ab}\) for \(a,b\geq 0\). Thus, \(s(\alpha_{1})\geq 2\sqrt{\frac{d^{2}}{2\alpha_{1}}\cdot\frac{\alpha_{1}}{2}}=d\). Since \(\beta_{1}\leq s(\alpha_{1})\), we can infer that \(\beta_{1}\leq s(\alpha_{1})_{\text{min}}\), which means \(\beta_{1}\leq d\).

The product of the roots of the equation is: \(t_{11}t_{12}=\frac{d+\beta_{1}}{2(2\beta_{1}-\alpha_{1})}\) (we assume that \(t_{11}<t_{12}\)). We let \(f(t_{1})=2(2\beta_{1}-\alpha_{1})t_{1}^{2}+2(\alpha_{1}-2\beta_{1}-d)t_{1}+d+\beta _{1}\). Thus we have \(f(0)=d+\beta_{1}\), \(f(\frac{1}{2})=\frac{\alpha_{1}}{2}>0\).

_Situation (i):_\(\beta_{1}<-d\). At this time, \(f(0)<0\), \(t_{11}t_{12}>0\), based on the trend of quadratic functions and the distribution of roots, we can infer that \(0<t_{11}<\frac{1}{2}\), \(t_{12}>\frac{1}{2}\), and \(2\beta_{1}<\alpha_{1}\). Thus we conclude that there exists \(t_{11}\) such that \(p_{1}(t_{1}=t_{11})<\frac{1}{2}\).

_Situation (ii):_\(-d<\beta_{1}<0\). At this time, \(f(0)>0\), \(t_{11}t_{12}<0\), based on the trend of quadratic functions and the distribution of roots, we also can infer that \(t_{11}<0\), \(t_{12}>\frac{1}{2}\), and \(2\beta_{1}<\alpha_{1}\). Thus we have that \(p_{1}\geq\frac{1}{2}\).

_Situation (iii):_\(0<\beta_{1}<d\). At this time, \(f(0)>0\), the sign of \(t_{11}t_{12}\) simultaneously determines the direction of the opening of the quadratic function \(f(t_{1})\). When \(t_{11}t_{12}<0\), \(t_{11}<0\), \(t_{12}>\frac{1}{2}\), thus we have \(p_{1}\geq\frac{1}{2}\); when \(t_{11}t_{12}>0\), \(0<t_{11}<t_{12}<\frac{1}{2}\), the minimum point of \(p_{1}\) is at \(p_{1}(t_{12})\). However, it is challenging to compare \(p_{1}(t_{12})\) and \(\frac{1}{2}\) to determine which is greater or smaller.

Similarly for \(p_{2}\), we let \(\frac{||b-2\mu||_{2}^{2}}{2}\) equal to \(\alpha_{2}>0\), \(-\mu^{\top}\mu+b^{\top}\mu-c-2d\) equal to \(\beta_{2}\), resulting in:

\[\frac{\mathrm{d}p_{2}}{\mathrm{d}t_{2}}=\frac{2(2\beta_{2}-\alpha_{2})t_{2}^{2} +2(\alpha_{2}-2\beta_{2}-d)t_{2}+\beta_{2}+d}{(1-2t_{2})^{2}}p_{2}\]

Thus we have the similar situation, _i.e._, \(\beta_{2}<-d\). At this time, \(f(0)<0\), \(t_{21}t_{22}>0\), based on the trend of quadratic functions and the distribution of roots, we can infer that \(0<t_{21}<\frac{1}{2}\), \(t_{22}>\frac{1}{2}\), and \(2\beta_{2}<\alpha_{2}\). Thus we conclude that there exists \(t_{21}\) such that \(p_{2}(t_{2}=t_{21})<\frac{1}{2}\).

Taking into account the above situations, we have that: _when \(\beta_{1}<-d\), \(\beta_{2}<-d\) (i.e., \(\mu^{\top}\mu+b^{\top}\mu+c+d<0\) and \(-\mu^{\top}\mu+b^{\top}\mu-c-d<0\)), there exists \(t_{11}\) and \(t_{21}\) respectively, making \(p_{1}<\frac{1}{2}\), \(p_{2}<\frac{1}{2}\), i.e., \(p_{1}+p_{2}<1\), where \(t_{11}=\frac{1}{2}+\frac{d-\sqrt{\alpha_{1}^{2}-2\alpha_{1}\beta_{1}+d^{2}}}{2(2 \beta_{1}-\alpha_{1})}\), \(t_{21}=\frac{1}{2}+\frac{d-\sqrt{\alpha_{2}^{2}-2\alpha_{2}\beta_{2}+d^{2}}}{2(2 \beta_{2}-\alpha_{2})}\)._

We know that for \(\mu\neq 0\), \(\tau_{\mathcal{D}_{c}}(P)=\phi(\mu)>\frac{1}{2}\). Therefore, we need to introduce additional conditions to further ensure that \(p_{1}<\frac{1}{4}\), and \(p_{2}<\frac{1}{4}\), and consequently \(\tau_{\mathcal{D}_{c}}(P_{u})=p_{1}+p_{2}<\frac{1}{2}<\tau_{\mathcal{D}_{c}}(P)\).

To let \(p_{1}\) satisfy \(p_{1}<\frac{1}{4}\) (\(\alpha_{1}>0\), \(\beta_{1}<-d\), and \(0\leq t_{1}<\frac{1}{2}\)), that is,

\[\exp\{\frac{\alpha_{1}t_{1}^{2}}{1-2t_{1}}+\beta_{1}t_{1}\}<\frac{( 1-2t_{1})^{\frac{d}{2}}}{2}\] \[\Rightarrow\frac{\alpha_{1}t_{1}^{2}}{1-2t_{1}}+\beta_{1}t_{1}< \frac{d}{2}ln(1-2t_{1})-ln2\] \[\Rightarrow\frac{\alpha_{1}t_{1}^{2}}{1-2t_{1}}+\beta_{1}t_{1}- \frac{d}{2}ln(1-2t_{1})<-ln2=-0.693\]

We assume that \(g(t)=\frac{\alpha_{1}t^{2}}{1-2t}+\beta_{1}t-\frac{d}{2}ln(1-2t)\). Let us assume \(\alpha_{1}=\frac{1}{2}\) and \(d=3\) for 3D point cloud data, then \(\beta_{1}<-3\). Upon analyzing the function \(g(t)\), we observe that as \(\beta_{1}\) decreases, the minimum value of \(g(t)\) also decreases. We utilize various \(\beta_{1}\) values and their corresponding function value to provide a more intuitive understanding as shown in Tab. 16.

As for \(p_{2}\), since \(\alpha_{2}\neq\alpha_{1}\), we need to reselect appropriate values to demonstrate the existence of \(p_{2}<\frac{1}{4}\). Similarly, to let \(p_{2}\) satisfy \(p_{2}<\frac{1}{4}\) (\(\alpha_{2}>0\), \(\beta_{2}<-d\), and \(0\leq t_{2}<\frac{1}{2}\)), that is,

\[\exp\{\frac{\alpha_{2}t_{2}^{2}}{1-2t_{2}}+\beta_{2}t_{2}\}<\frac {(1-2t_{2})^{\frac{d}{2}}}{2}\] \[\Rightarrow\frac{\alpha_{2}t_{2}^{2}}{1-2t_{2}}+\beta_{2}t_{2}- \frac{d}{2}ln(1-2t_{2})<-ln2=-0.693\]

We assume that \(h(t)=\frac{\alpha_{2}t^{2}}{1-2t}+\beta_{2}t-\frac{d}{2}ln(1-2t)\). Let us assume \(\alpha_{2}=\frac{1}{3}\) and \(d=3\), similarly, we utilize various \(\beta_{2}\) values and their corresponding function value to provide a more intuitive understanding as shown in Tab. 17.

Therefore, we conclude that there exist \(\alpha_{1},\beta_{1},t_{1}\) such that \(p_{1}<\frac{1}{4}\), \(\alpha_{2},\beta_{2},t_{2}\) such that \(p_{2}<\frac{1}{4}\), _i.e._, \(p_{1}+p_{2}<\frac{1}{2}\). At the same time, we observe that a smaller value of \(\beta\) makes it easier to satisfy the above conditions, _i.e._, the more negative \(\beta_{1}\) and \(\beta_{2}\) are, the more likely it is to satisfy the above conditions. We formally combine and assert these conditions as, \(b^{\top}\mu\ll\) 0, _i.e._, \(\mu^{\top}\frac{\lambda_{-1}^{-1}\Upsilon_{-1}^{-1}+\lambda_{-1}^{-2}\Upsilon_ {-1}^{\top}}{\lambda_{-1}^{-2}-\lambda_{-1}^{-2}}\mu\ll\) 0 (we sufficiently support this condition in the empirical results from Tabs. 16 and 17). Thus we conclude that _for any \(\mu\neq 0\)\(\exists\) transformation parameters \(\mathbf{T}_{i}\) such that \(\tau_{\mathcal{D}_{c}}(P_{u})<\tau_{\mathcal{D}_{c}}(P)\)._

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \(\beta_{1}\) & -4 & -6 & -8 & -10 & -12 & -14 \\ \hline \(g(0.3)\) & 0.287 & -0.313 & **-0.913** & **-1.513** & **-2.113** & **-2.713** \\ \(g(0.4)\) & 1.214 & 0.414 & -0.386 & **-1.186** & **-1.986** & **-2.786** \\ \hline \end{tabular}
\end{table}
Table 16: Different \(\beta_{1}\) values and corresponding \(g(t)\) with \(t=0.3\) and \(t=0.4\). The **bold values** represent cases where \(p_{1}<\frac{1}{4}\) is satisfied.

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \(\beta_{2}\) & -4 & -6 & -8 & -10 & -12 & -14 \\ \hline \(h(0.3)\) & 0.249 & -0.351 & **-0.951** & **-1.551** & **-2.151** & **-2.751** \\ \(h(0.4)\) & 1.081 & 0.281 & -0.519 & **-1.319** & **-2.119** & **-2.919** \\ \hline \end{tabular}
\end{table}
Table 17: Different \(\beta_{2}\) values and corresponding \(h(t)\) with \(t=0.3\) and \(t=0.4\). The **bold values** represent cases where \(p_{2}<\frac{1}{4}\) is satisfied.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction accurately reflect the scope of the contributions made in the paper regarding the first proposed unlearnable scheme UMT and the data restoration scheme, as well as the theoretical and experimental evaluations.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The paper discussed the limitations of the work in Sec. 6.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The paper clearly and accurately provides the assumptions and proofs for each theory.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The formulas in the methods section, the schematic diagrams of the schemes, Algorithm 1, as well as the details of the training process and descriptions of the datasets and models together support the reproducibility of the experimental results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The paper has open-sourced code, which are sufficient to reproduce the experimental results.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The paper provides sufficient training and testing details, which are adequate for understanding the experimental results.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The paper conducted experiments with three different random seeds, studying the results in terms of statistical significance.
8. **Experiments Compute Resources**Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The paper clearly specifies the parameters of the server used for the experiment and the code execution environment.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: After a thorough review of the paper, no violations of the NeurIPS Code of Ethics are found.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The paper discuss both potential positive societal impacts and negative societal impacts of the work in Sec. 6.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: The paper described safeguards that have been put in place for responsible release of data in Sec. 6.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The data used in the paper are properly credited, and the license and terms of use are explicitly mentioned and properly respected.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: Justification: The paper does not release new assets.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: Justification: The paper does not involve crowdsourcing nor research with human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: Justification: The paper does not involve crowdsourcing nor research with human subjects.