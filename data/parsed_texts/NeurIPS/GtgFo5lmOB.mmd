# Joint Data-Task Generation for Auxiliary Learning

 Hong Chen\({}^{1}\), Xin Wang\({}^{1,2,}\) Yuwei Zhou\({}^{1}\), Yijian Qin\({}^{1}\), Chaoyu Guan\({}^{1}\), Wenwu Zhu\({}^{1,2}\)

\({}^{1}\)Department of Computer Science and Technology, Tsinghua University

\({}^{2}\)Beijing National Research Center for Information Science and Technology, Tsinghua

{h-chen20,zhou-yw21,qinyj19,guancy19}@mails.tsinghua.edu.cn

{xin_wang,wuzhu}@tsinghua.edu.cn

Corresponding Authors.

###### Abstract

Current auxiliary learning methods mainly adopt the methodology of reweighing losses for the manually collected auxiliary data and tasks. However, these methods heavily rely on domain knowledge during data collection, which may be hardly available in reality. Therefore, current methods will become less effective and even do harm to the primary task when unhelpful auxiliary data and tasks are employed. To tackle the problem, we propose a joint data-task generation framework for auxiliary learning (DTG-AuxL), which can bring benefits to the primary task by generating the new auxiliary data and task in a joint manner. The proposed DTG-AuxL framework contains a joint generator and a bi-level optimization strategy. Specifically, the joint generator contains a feature generator and a label generator, which are designed to be applicable and expressive for various auxiliary learning scenarios. The bi-level optimization strategy optimizes the joint generator and the task learning model, where the joint generator is effectively optimized in the upper level via the implicit gradient from the primary loss and the explicit gradient of our proposed instance regularization, while the task learning model is optimized in the lower level by the generated data and task. Extensive experiments show that our proposed DTG-AuxL framework consistently outperforms existing methods in various auxiliary learning scenarios, particularly when the manually collected auxiliary data and tasks are unhelpful.

## 1 Introduction

Auxiliary learning aims to improve the model generalization ability on the primary task with the help of related auxiliary tasks [1, 2]. This learning paradigm has been widely adopted and has shown its effectiveness in various areas, like image classification [3], recommendation [4] and reinforcement learning [5, 6]. Different auxiliary tasks are often chosen manually according to the primary task, e.g., [7] utilize the task of visual attribute classification to help the fine-grained bird classification and [4] improve the click conversion rate prediction with the help of click-through rate prediction task.

Most existing works utilize the auxiliary information by first reweighing the losses of the auxiliary data and tasks, and then use the sum of the weighted losses together with the primary loss to optimize the task learning model. The weights are employed to balance the primary loss and the auxiliary losses to avoid negative auxiliary transfer, which are tuned with HPO tools [4, 3, 8]. More recent works [9, 6, 10, 1, 11] propose to dynamically weigh different auxiliary losses during training.

However, the existing methods require that there exists beneficial information in the auxiliary data and tasks, so that the beneficial loss terms can be selected through the loss reweighing process. This condition cannot always be satisfied because whether the auxiliary data or task is beneficial depends on many factors including _the chosen auxiliary task, the scale of primary task dataset_ and _the selectedlearning model for the tasks_[1; 11], making it difficult to manually collect the beneficial auxiliary data and task via prior knowledge. Therefore, existing approaches may adopt useless auxiliary information and finally do harm to the primary task when the involved auxiliary data and tasks are improperly collected, as observed in [1; 12]. Although [2] propose to generate fine-grained auxiliary classification tasks, this method can be only applied to the classification primary task. Additionally, they only generate the new task without considering new data, while the data-level information is pointed out to be an important factor in auxiliary learning [11].

To address the problem, in this paper, we propose to simultaneously generate the auxiliary data and task for auxiliary learning in a joint manner. However, there are three challenges for the joint generation. First, it is challenging to design a generic framework that can accommodate various tasks with different inputs. This is because the types of data and tasks are quite diversified in different auxiliary learning scenarios, e.g., the primary task of image classification takes visual images as input and outputs categorical labels for classification [7], while the primary task of rating prediction for recommendation takes tabular data as input and outputs numeric labels for regression [4]. Second, it is challenging to develop a generation framework that is expressive enough to produce beneficial data and tasks for the primary task. Finally, to guarantee the jointly generated auxiliary data and task are beneficial to primary task, how to effectively optimize the parameters in the generation framework is a challenging problem as well.

To tackle these challenges, we propose a joint Data-Task Generation framework for Auxiliary Learning (DTG-AuxL), which involves a joint generator and a bi-level optimization strategy. Specifically, the joint generator consists of a feature generator and a label generator, which generates the new auxiliary data and new task based on the existing manually collected data and tasks. The data generation process is conducted in the feature space so that it can accommodate various input types, while the label generator architecture is inspired by the model in recommendation which can accommodate both categorical labels for classification and numeric labels for regression. Moreover, we introduce nonlinear interaction terms in the joint generator, making it more expressive to produce beneficial auxiliary data and tasks. To effectively optimize the joint generator and the task learning model, we propose the bi-level optimization strategy with instance regularization. In the lower optimization, the task learning model is optimized by the generated auxiliary data and task. In the upper optimization for the joint generator, we not only utilize the implicit gradient from the primary loss but also the explicit gradient from the proposed instance regularization, to avoid label generation mode collapse. Extensive experiments show that DTG-AuxL outperforms existing methods in various auxiliary learning scenarios, especially when the manually collected auxiliary data and task are unhelpful to the primary task. We summarize our contributions as follows,

* We propose to simultaneously generate auxiliary data and tasks in a joint manner in auxiliary learning for the first time, to the best of our knowledge.
* We propose DTG-AuxL, a joint data-task generation framework applicable in various auxiliary learning scenarios, containing a joint generator and a bi-level optimization strategy.
* Extensive experiments in various auxiliary learning scenarios demonstrate the superiority of our proposed DTG-AuxL framework. We further analyze when and how DTG-AuxL works to bring performance boost.

## 2 Related Work

**Auxiliary Learning** The most widely adopted way in auxiliary learning is to combine the loss of the primary task and the losses of the auxiliary tasks in a linear way [3; 7; 4; 5], where the linear weights are tuned manually or with HPO methods. More recent works [9; 6; 10] propose to automatically assign dynamic weights to the auxiliary losses. [9] propose to assign weights to each loss based on the cosine gradient similarity between the primary loss and each auxiliary loss. Later work [10] aims to make the weighted auxiliary gradient sum closest to the gradient of the primary loss. [13; 1] utilize the bi-level optimization strategy to learn the auxiliary weights, where [1] even propose to combine the losses not only limited to a linear form, but also a nonlinear form. [11; 14] point out that only considering the task-level information is not sufficient, so they jointly consider the weights of different tasks and different data samples within the same task through a joint selector. As aforementioned, these reweighing methods will easily fail to bring improvement when the chosen auxiliary tasks and data are unhelpful. There are also works [2; 1] that generate fine-grained classification auxiliary tasks for the primary classification task. However, they can only be applied to a classification problem. Additionally, they only generate auxiliary tasks without new data, limiting their performance especially when the data of the primary task is inadequate, which is an often encountered scenario in auxiliary learning [9; 1].

**Multi-task learning**  Multi-task learning aims to share information among tasks to improve model performance. However, the goal of multi-task learning is to obtain good performance for all the tasks, while auxiliary learning only focuses on the primary task. Multi-task learning methods can be categorized into three parts [15]: multi-task architecture design [16; 17], multi-task optimization [18; 19] and multi-task relationship learning [20], where the multi-task optimization methods involve techniques for optimizing several losses, like loss reweighing [18] and gradient modulation [21].

## 3 The Proposed Method

The overall DTG-AuxL framework is shown in Figure 1. Next, we give the problem formulation, describe the designs of the joint generator, and present the proposed bi-level optimization strategy.

### Preliminaries and Problem Formulation

In auxiliary learning, we have one primary task \(T_{p}\), and totally \(K\) auxiliary tasks \(\{T_{ai}\}_{i=1}^{K}\). Each of these tasks has its corresponding training dataset, including the primary dataset \(D_{p}=\{(x_{p,j},y_{p,j})\}_{j=1}^{|D_{p}|}\), and the dataset for each auxiliary task \(D_{ai}=\{(x_{ai,j},y_{ai,j})\}_{j=1}^{|D_{ai}|}\), where \(|\cdot|\) denotes the data sample number of the dataset. If the auxiliary tasks share the same input with the primary task, which is a widely encountered and studied scenario in previous works [11; 1; 9; 10], we have \(x_{ai,j}=x_{p,j}\) and \(|D_{ai}|=|D_{p}|\). Besides the datasets, we have a task learning model parameterized by \(\theta\) which is used to learn all the tasks together. There is also a validation dataset \(D_{v}\) which is used to evaluate the model performance on the primary task. With these notations, the widely adopted training objective of auxiliary learning is:

\[\mathcal{L}_{t}(\theta)=\mathcal{L}_{p}(D_{p};\theta)+\sum_{i=1}^{K}w_{i}\cdot \mathcal{L}_{ai}(D_{ai};\theta), \tag{1}\]

where \(\mathcal{L}_{p}\) and \(\mathcal{L}_{ai}\) indicate the loss functions of the primary and each auxiliary task. Current reweighing methods focus on how to decide \(w_{i}\) so that the task learning model \(\theta\) can achieve the best

Figure 1: The overall DTG-AuxL framework. In the model design part, the joint generator contains a feature generator and a label generator. The feature generator utilizes the primary and auxiliary features to generate the new auxiliary feature \(f_{g,j}\), whose label \(\hat{y}_{g,j}\) is generated by the label generator by combining the information of all the auxiliary and primary labels. The optimization part shows that we optimize the task learning model and the joint generator in an alternating bi-level manner.

performance on \(D_{v}\). However, as aforementioned, the terms \(\mathcal{L}_{ai}(D_{ai},\theta)\) are defined by the manually collected auxiliary data and tasks, with no guarantee to bring benefits for the primary task. Therefore, we propose to simultaneously generate the new beneficial auxiliary data and task in a joint manner, with the following training objective:

\[\mathcal{L}_{t}(\theta,\phi)=\mathcal{L}_{p}(D_{p};\theta)+\sum_{i=1}^{K}w_{i} \mathcal{L}_{ai}(D_{ai};\theta)+w_{g}\mathcal{L}_{g}(D_{g}(\phi);\theta), \tag{2}\]

where the last term is the loss on the generated auxiliary data and task. \(D_{g}(\phi)=\{x_{g,j},\hat{y}_{g,j}\}_{j=1}^{|D_{g}|}\) is the generated dataset which contains the new data together with the new label defining the new task, and \(\phi\) denotes the parameters of the joint generator that are used to generate \(D_{g}\). Since we only care about the performance of the primary task in auxiliary learning, we keep \(\mathcal{L}_{g}\) the same loss function as \(\mathcal{L}_{p}\), which can be cross entropy or MSE loss, etc. Note that we still keep the original auxiliary loss terms in our training objective, so that it can accommodate the scenario where the existing manually collected auxiliary data and tasks are beneficial. The task weights \(w_{i}\) and \(w_{g}\) are also learnable, which will be optimized together with the generator parameters, and we uniformly denote them all as \(\phi\) for convenience. Next, we discuss the details of the generator and how we optimize \(\phi\) and \(\theta\).

### Joint Generator Design

The joint generator involves a feature generator to generate new features and a label generator to generate a new task for the new feature, as shown in the Model Design of Figure 1.

#### 3.2.1 Feature Generator

Since we expect that our data generator can tackle different types of data, a neat and elegant solution is to conduct the generation process in the feature space. In the feature space, different types of input data are transformed to vectors, so we can tackle these vectors in a unified way. Specifically, for the input from different tasks \([x_{p,j},x_{a1,j},\cdots,x_{aK,j}]\), the task learning model will first map them to the feature space with an encoder which is generally noted as "backbone", and then use different task-specific heads to tackle each of the tasks. We generate the new features based on the features extracted via the backbone. In another word, the input of the feature generator is a feature list \([f_{p,j},f_{a1,j},\cdots,f_{aK,j}]\), where \(f_{p,j}=f_{backbone}(x_{p,j};\theta)\) is a \(d\)-dimension feature. We next describe how the new features are generated.

**Feature linear term.** A natural way to generate new features is to combine different features with linear masks. We assign an individual feature mask to each of the \(K+1\) features, i.e., we have a mask list \([m_{p},m_{a1},\cdots,m_{aK}]\), where each mask is a \(d\)-dimension vector. Then, we denote the subscript set for the task IDs \(\{p,a1,\cdots,aK\}\) as \(S\), and the linear combination is conducted as follows:

\[f_{gl,j}=\sum_{i\in S}\hat{m}_{i}*f_{i,j},\;\hat{m}_{i}[k]=\frac{e^{m_{i}[k]}}{ \sum_{j\in S}e^{m_{j}[k]}},k=1\cdots d, \tag{3}\]

where \(gl\) indicates "generated linearly", \(*\) means the element-wise multiplication, and \(\hat{m}_{i}\) is the normalized mask. \(m_{i}[k]\) is the \(k^{th}\) element of the mask \(m_{i}\), and is normalized by softmax with the elements in the same dimension in other masks. This linear combination with normalized mask works as feature selection from all the input features, and then combines them into a new one.

**Feature nonlinear term.** To make the generated features more expressive, we introduce an MLP (Multi-Layer-Perceptron) to capture the nonlinear feature interaction:

\[f_{gn,j}=MLP_{F}([f_{p,j};f_{a1,j};\cdots;f_{aK,j}]), \tag{4}\]

where \(gn\) indicates "generated nonlinearly". All the features are concatenated together and the nonlinear feature interactions are modeled by an MLP, whose output dimension is \(d\). Finally, the generated feature is the sum of the linear and nonlinear term, i.e., \(f_{g,j}=f_{gl,j}+f_{gn,j}\).

#### 3.2.2 Label Generator

The label generator aims to generate a proper label for the feature produced by the feature generator. We use the labels of all the input data as the input of the label generator to make the generated label expressive enough, i.e., the input of the label generator is a list of labels \([y_{p,j},y_{a1,j},\cdots,y_{aK,j}]\)However, this label list may contain both numeric labels (if the corresponding task is regression) and categorical labels (if the corresponding task is classification). How to utilize different types of label information to generate a new label is the key problem. Inspired by the CTR (click-through rate) model in recommendation [22, 23], which predicts the user's preference towards an item also based on both their numeric and categorical features, we design the label generator with both linear terms and deep nonlinear terms similar to those in the CTR model.

**Label linear term.** The linear term models the direct and independent relationship between each input label and the generated label. We simply keep the dimension of the generated label the same as the primary label \(y_{p,j}\) and more adaptive ways to choose the dimension can be an interesting future work. Here, we assume that the dimension of \(y_{p,j}\) is \(m\), where \(m=1\) if the primary task \(T_{p}\) is regression, or \(m\) equals to the number of the total categories if \(T_{p}\) is classification. If the primary task is regression, the generated label is also a scalar value for regression, while if the primary task is classification, the generated label is an \(m\)-dimension probability distribution vector. Specifically, for each task ID \(i\in S\), if \(T_{i}\) is a classification task and it has totally \(d_{i}\) categories, there is an embedding table \(E_{i}\) with dimension \(d_{i}\times m\) for this task. We directly map the label \(y_{i,j}\) in task \(T_{i}\) to its \(m\)-dimension embedding space through the embedding table:

\[y_{gl,i,j}=E_{i}(y_{i,j}). \tag{5}\]

If \(T_{i}\) is a regression task, we follow [23] to map \(y_{i,j}\) to its \(m\)-dimension embedding. Specifically, we also have an embedding table \(E_{i}\) for this task \(T_{i}\), whose dimension is \(d_{i}\times m\), where \(d_{i}=H\) is a hyper-parameter which is fixed to 10 in our experiments. Then \(y_{i,j}\) is mapped into the \(m\)-dimension embedding space as follows:

\[c_{i,j}=Softmax(Linear(y_{i,j})),c_{i,j}\in R^{H},y_{gl,i,j}=\sum_{k=1}^{H}c_{i,j}[k]E_{i}[k], \tag{6}\]

where the numeric label \(y_{i,j}\) is linearly transformed to a \(H\)-dimension vector \(c_{i,j}\), which is used to weigh all the embeddings in the embedding table to obtain the final embedding. Then the final label linear term is obtained by the sum of all the input label embeddings \(y_{gl,j}=\sum_{i\in S}y_{gl,i,j}\).

**Label nonlinear term.** In addition to the label linear term capturing how each input label independently influences the final generated label, we also propose a nonlinear term to model the influence of more complex label interactions on the generated label. Specifically, we introduce another group of embedding tables for all the input tasks \(\{EN_{i}\}_{i\in S}\), where the ways to obtain the embeddings of the categorical and numeric labels are the same as eq. (5) and eq. (6), respectively. The dimension of \(EN_{i}\) is \(d_{i}\times m_{n}\), where \(d_{i}\) equals to the number of total categories if \(T_{i}\) is classification. \(d_{i}\) equals to \(H\) (i.e.,10) for regression task, and \(m_{n}\) is a hyper-parameter. With these embedding tables, we can map all the input label list \([y_{p,j},y_{a1,j},\cdots,y_{aK,j}]\) to an embedding list \([e_{p,j},e_{a1,j},\cdots,e_{aK,j}]\). Then the nonlinear label interactions are captured through an MLP:

\[y_{gn,j}=MLP_{L}([e_{p,j};e_{a1,j};\cdots;e_{aK,j}]). \tag{7}\]

The final generated label for the generated feature \(f_{g,j}\) is the sum of the linear and nonlinear terms,

\[y_{g,j}=y_{gl,j}+y_{gn,j}. \tag{8}\]

**Label bias term.** Besides the linear and nonlinear terms, we also introduce a label bias term that guides the generated label to possess similar semantic meaning to the label from the target primary task, \(y_{p,j}\). As such, we add \(y_{p,j}\) as the label bias term as follows,

\[\hat{y}_{g,j}=\alpha y_{p,j}+(1-\alpha)*norm(y_{g,j}), \tag{9}\]

where \(\alpha\in(0,1)\) is a learnable parameter initialized as 0.5 and \(norm(\cdot)\) is the normalization function. If the primary task is classification, \(norm(\cdot)\) will be \(softmax(\cdot)\) and \(y_{p,j}\) will be converted to the one-hot form. If the primary task is regression with range \((a,b)\), then \(norm(\cdot)\) is set to be \((b-a)sigmoid(\cdot)+a\). This bias term makes the generated label lie in the same space as the original label, enabling us to more conveniently explore its semantic meaning, which is also verified to improve model performance in our experiments.

#### 3.2.3 Discussion about the Share Input Scenario

The proposed generator can generate the new feature by combining the features from the primary and auxiliary tasks. However, in the most widely studied auxiliary learning scenario [9, 1, 11], all the tasks share the same input data. We only have the training dataset \(\{(x_{p,j},y_{p,j},y_{a1,j},\cdots,y_{aK,j})\}_{j=1}^{|D_{p}|}\). Therefore, we cannot obtain the auxiliary data \([x_{a1,j},\cdots,x_{aK,j}]\) from the auxiliary tasks to conduct the feature generation. To tackle this problem, we obtain the new auxiliary data by randomly sampling another data sample in the dataset, which is easy to implement with randomly shuffling the training batch to match another sample for the original sample. Specifically, for a sample \((x_{p,j},y_{p,j},y_{a1,j},\cdots,y_{aK,j})\), we consider another sample \((x_{p,j2},y_{p,j2},y_{a1,j2},\cdots,y_{aK,j2})\) in the dataset to provide new auxiliary data information. The input of the feature generator is \([f_{p,j},f_{p,j2}]\), which are the features extracted by the backbone with \([x_{p,j},x_{p,j2}]\) as input. When generating new labels, we need to combine all the existing labels of the two samples, i.e., the input of the label generator is \([y_{p,j},y_{a1,j},\cdots,y_{aK,j},y_{p,j2},y_{a1,j2},\cdots,y_{aK,j2}]\), and then the label generation process is the same as before. Note that there are two small details: 1) During the label embedding process, \(y_{ai,j}\) and \(y_{ai,j2}\) share the same embedding table, because they both belong to the same task. ii) For the bias term in eq. (9), we now have two labels from the primary task, \(y_{p,j}\) and \(y_{p,j2}\), and then eq. (9) can be adjusted to:

\[\hat{y}_{g,j}=\alpha(\beta y_{p,j}+(1-\beta)y_{p,j2})+(1-\alpha)*norm(y_{g,j}), \tag{10}\]

where \(\beta\in(0,1)\) is a learnable parameter of the generator. Till now, we have described the designs of the joint generator, which is applicable in various auxiliary learning scenarios. However, the generator has several parameters to be optimized, like the masks and MLPs. We denote all the learnable parameters in the generator together with the task weights \(w_{i}\) in eq. (2) as \(\phi\). Next, we will elaborate how we optimize the task learning model parameters \(\theta\) and the generator parameters \(\phi\).

### Optimization Strategy

**Bi-level optimization.** In our whole framework, the task learning model parameters \(\theta\) are expected to minimize the loss of all the selected and generated tasks, while the generator parameters \(\phi\) aim to make \(\theta\) achieve the best performance on the primary task. These two different goals give rise to the following bi-level optimization problem:

\[\begin{split}&\phi^{*}=\arg\min_{\phi}\mathcal{L}_{p}(\theta^{*}( \phi);D_{v}),\\ & s.t.\ \theta^{*}(\phi)=\arg\min_{\theta}\mathcal{L}_{t}(\theta,\phi), \end{split} \tag{11}\]

where \(\mathcal{L}_{t}(\theta,\phi)\) is the objective in eq. (2), and \(\mathcal{L}_{p}(\theta^{*}(\phi);D_{v})\) is the primary task loss of the task learning model on the validation dataset. The lower optimization is easy, we can directly obtain the gradient of \(\theta\) as \(\nabla_{\theta}\mathcal{L}_{t}(\theta,\phi)\). However, in the upper optimization, \(\mathcal{L}_{p}(\theta^{*}(\phi);D_{v})\) directly relies on \(\theta\) instead of \(\phi\). Assuming that the Hessian \(\nabla_{\theta}^{2}\mathcal{L}_{t}(\theta^{*}(\phi),\phi)\) is positive-definite, we can use the implicit theorem to obtain its implicit gradient \(\nabla_{\phi}\mathcal{L}_{p}(\theta^{*}(\phi);D_{v})\),

\[\nabla_{\phi}\mathcal{L}_{p}(\theta^{*}(\phi);D_{v})=-\nabla_{\theta} \mathcal{L}_{p}\cdot(\nabla_{\theta}^{2}\mathcal{L}_{t})^{-1}\cdot\nabla_{ \phi}\nabla_{\theta}\mathcal{L}_{t}(_{\phi,\theta^{*}(\phi)}). \tag{12}\]

Detailed derivation can be found in Appendix 1. Since the inverse of the Hessian is often intractable, we follow [24] to use truncated Neumann series to approximate it. \((\nabla_{\theta}^{2}\mathcal{L}_{t})^{-1}\approx\sum_{i=0}^{n}(I-\nabla_{ \theta}^{2}\mathcal{L}_{t})^{i}\), and \(n\) is fixed to 3 in all our experiments. Thus, the complete implicit gradient is approximated as:

\[\nabla_{\phi}\mathcal{L}_{p}(\theta^{*}(\phi);D_{v})\approx-\nabla_{\theta} \mathcal{L}_{p}\cdot\sum_{i=0}^{n}(I-\nabla_{\theta}^{2}\mathcal{L}_{t})^{i} \cdot\nabla_{\phi}\nabla_{\theta}\mathcal{L}_{t}. \tag{13}\]

**Instance regularization.** In the upper optimization, we additionally introduce an instance regularization for the parameterized label \(y_{g,j}\) in eq. (8) as follows, to prevent generation mode collapse.

\[\mathcal{L}_{reg}(\phi;D_{g})=\left\{\begin{array}{ll}\sum_{j=1}^{|D_{g}|} \sum_{j^{\prime}\neq j}cos(y_{g,j},y_{g,j^{\prime}})&categorical\\ -\sum_{j=1}^{|D_{g}|}\bar{y}_{g,j}log(\bar{y}_{g,j})&numerical\end{array}\right. \tag{14}\]

where if the generated label is categorical, we expect that the cosine similarity of different generated labels is small. This regularization means we expect that the generated label can keep its instance-level uniqueness, preventing the generated labels of all the generated features from being the same. Similarly, if the generated label is numerical, since it is 1-dimension, we use the entropy regularization to achieve this goal, where \(\bar{y}_{g,j}=e^{y_{g,j}}/\sum_{j^{\prime}=1}^{|D_{g}|}e^{y_{g,j^{\prime}}}\). Then in the upper optimization, the gradient of \(\phi\) is the sum of the implicit gradient and explicit gradient from the regularization:

\[\nabla_{\phi}=\nabla_{\phi}\mathcal{L}_{p}(\theta^{*}(\phi);D_{v})+\nabla_{ \phi}\mathcal{L}_{reg}(\phi;D_{g}). \tag{15}\]Now the gradients of \(\theta\) and \(\phi\) are obtained, and we follow previous works [1; 11] to alternatingly update \(\theta\) and \(\phi\). The update loop will continue until convergence, where in each loop we first update \(\theta\) for \(N\) times, and then update \(\phi\) using the upper gradient eq. (15) as shown in Figure 1. \(N\) is the interval between two upper updates. We summarize the complete algorithm in Appendix 2, where we do not require an additional validation dataset \(D_{v}\) to calculate the upper objective but reuse the training primary set \(D_{p}\) as done in [13; 14].

## 4 Experiments

### Experimental Setup

**Task and Dataset.** We conduct our experiments on two scenarios to validate the generalization ability of our method. One is the most widely studied scenario in previous auxiliary learning works, where the auxiliary tasks share the same input as the primary task (share input). The other one is that the inputs of the primary and auxiliary tasks are different (different input). In the **Share Input** setting, **(i) CUB**[25]: we follow previous works [1; 11] to use the bird visual attribute classification (e.g., whether the bird has white belly) to help the bird species classification primary task, where both the auxiliary and primary labels are categorical. **(ii) CIFAR100**[26]: it is a widely adopted image classification dataset, where there are totally 100 categories. Additionally, each image has a coarse class, e.g., a car belongs to the "vehicles 1" coarse class. We use the coarse classification as the auxiliary task to help the 100-classification primary task. **(iii)** Besides the classification problem, we also focus on the regression problem, where we follow previous works [11; 12] to regard the rating prediction in recommender system as the primary task, and the CTR prediction as the auxiliary task. The primary task is regression and the auxiliary task is binary classification. We choose the widely used Amazon **Toys** and **Movies**[27] datasets, where we use the user ID, item ID and item category as the input data. In the **Different Input** setting, **(i) CIFAR10-100** is a setting where our primary task is the CIFAR10 classification problem, and the auxiliary task is the CIFAR100 classification problem. **(ii) Pet-CUB** is a similar setting where our primary task is fine-grained pet classification on the Pet [28] dataset, while our auxiliary task is the bird species classification in the previously mentioned CUB dataset. More detailed data information is presented in Appendix 4.

**Baselines.** We compare with SOTA auxiliary learning methods, including the reweighing methods and the auxiliary task generation method. Single task learning(STL) is a natural baseline where we only train on the primary task. Equal is a baseline where we assign equal weights 1.0 to all the tasks. Uncert [18] is a dynamic weighting method for multi-task learning based on task uncertainty. GCS [9] and AuxL [1] dynamically reweight the auxiliary losses, and JTDS [11] not only reweighs the tasks but also each data sample within each task. MAXL [1] is a method that automatically generates a fine-grained auxiliary task for the primary classification task. We provide detailed differences between our work and the baselines in Appendix 3.1.

**Implementation details.** In CUB dataset, we respectively adopt ResNet18 [29] and ResNet50 as our backbone. In CIFAR100, we respectively adopt ResNet18 and a 4-layer ConvNet composed of Convolution, Batch Normalization and Relu layers as our backbone. In Amazon Toys and Movies, we adopt AutoINT [30] as the backbone. In CIFAR10-100, the backbone is the 4-layer ConvNet and in Pet-CUB the backbone is ResNet18. For the head of each task, we adopt Multi-Layer-Perceptron(MLP) whose layer is searched from \(\{1,2\}\). In the generator, the embedding dimension \(m_{n}\) is searched from \(\{32,64\}\), and the layer number of the MLP is searched from \(\{2,3,4\}\). More details are presented in Appendix 4.

### Experimental Results

#### 4.2.1 Method Performance

**Main results.** Table 1 presents the overall experimental results, showing that our proposed method consistently outperforms existing methods on the diversified auxiliary learning scenarios. During training in the CUB dataset and Pet-CUB dataset, different from [11] that does not use the learning rate scheduler, we found that applying a learning rate scheduler to all the baselines can obtain better or at least on-par performance. Therefore, we report the results with the scheduler. From the results, we have the following observations. **(i)** As expected, our proposed method brings benefits for the primary task when the original manually collected auxiliary data and task are unhelpful. For example,

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

can not only combine information from the label of the same task but also label from other tasks. **(ii)** Our method can not only model linear relations in generation but also non-linear relations. **(iii)** Our method can handle the Different Input scenario, while MixUp cannot.

To further show the advantage of our proposed generation compared to MixUp, we apply Auto-F-Mix on top of auxiliary learning methods (AuxL, JTDS, MAXL) and the experimental results are presented in Table 6. The results show that both directly combining auxiliary weighting methods(AuxL, JTDS) with Auto-F-Mix and combining the current auxiliary generation method (MAXL) with Auto-F-mix are not as effective as our method, indicating the advantage of our proposed joint generation.

## 5 Conclusion

In this paper, we propose to jointly generate beneficial auxiliary data and tasks for auxiliary learning, so that the primary task can still obtain benefits when the manually collected auxiliary data and tasks are unhelpful. We propose the DTG-AuxL framework with a joint generator and a bi-level optimization strategy, which can be applied in various auxiliary learning scenarios. Future works like designing more adaptive generators and more efficient bi-level optimization algorithms can further improve the generation.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Dataset** & **CUB** & **CUB-5shot** & **Toys** \\ \hline
**Metric** & **Acc(\%)\(\uparrow\)** & **Acc(\%)\(\uparrow\)** & **RMSE\(\downarrow\)** \\ \hline
**Backbone** & ResNet50 & ResNet50 & AutoINT \\ \hline AFM & 80.30\({}_{0.57}\) & 46.97\({}_{0.95}\) & 0.9187\({}_{0.0003}\) \\ AuxL+AFM & 79.70\({}_{1.07}\) & 47.84\({}_{1.25}\) & 0.9195\({}_{0.0011}\) \\ MAXL+AFM & 80.76\({}_{0.38}\) & 47.96\({}_{0.89}\) & - \\ JTDS+AFM & 80.52\({}_{0.90}\) & 48.14\({}_{0.87}\) & 0.9189\({}_{0.0013}\) \\ \hline ours & **81.73\({}_{0.20}\)** & **52.33\({}_{1.36}\)** & **0.9153\({}_{0.0004}\)** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Comparison to the combination of Auto-F-Mix and current auxiliary learning methods. We denote Auto-F-Mix as AFM.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Dataset** & **CUB** & **Pet-CUB** & **Toys** \\ \hline
**Metric** & **Acc(\%)\(\uparrow\)** & **Acc(\%)\(\uparrow\)** & **RMSE\(\downarrow\)** \\ \hline
**Backbone** & ResNet50 & ResNet18 & AutoINT \\ \hline w/o F-nonlinear & 81.22\({}_{0.27}\) & 67.76\({}_{0.17}\) & 0.9195\({}_{0.0021}\) \\ w/o L-nonlinear & 80.96\({}_{0.37}\) & 66.58\({}_{1.22}\) & 0.9213\({}_{0.0026}\) \\ w/o L-bias & 79.28\({}_{0.34}\) & 69.36\({}_{0.37}\) & 0.9204\({}_{0.0011}\) \\ \hline w/o bi-level & 80.32\({}_{0.42}\) & 68.53\({}_{0.49}\) & 0.9163\({}_{0.0008}\) \\ w/o instance reg & 80.44\({}_{0.34}\) & 67.53\({}_{0.95}\) & 0.9174\({}_{0.0012}\) \\ \hline complete & **81.73\({}_{0.20}\)** & **70.48\({}_{0.28}\)** & **0.9153\({}_{0.0004}\)** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Effectiveness of framework designs. The first three rows explore the effectiveness of the generator components, while the rest show the effectiveness of the training strategies.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Dataset** & **CUB** & **Movies** \\ \hline
**Metric** & **Acc(\%)\(\uparrow\)** & **RMSE\(\downarrow\)** \\ \hline
**Backbone** & ResNet18 & AutoINT \\ \hline MixUp & 75.88\({}_{0.50}\) & - \\ Auto-Mix & 76.53\({}_{0.36}\) & - \\ F-MixUp & 75.33\({}_{0.67}\) & 1.0460\({}_{0.0023}\) \\ Auto-F-Mix & 76.88\({}_{0.52}\) & 1.0464\({}_{0.0003}\) \\ \hline ours & **77.75\({}_{0.27}\)** & **1.0426\({}_{0.0009}\)** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparison with MixUp Methods. Note that MixUp cannot be applied to the recommendation scenario where the input are categorical features.

## Acknowledgement

This work was supported by the National Key Research and Development Program of China No. 2020AAA0106300, National Natural Science Foundation of China (No. 62222209, 62250008, 62102222), Beijing National Research Center for Information Science and Technology under Grant No. BNR2023RC01003, BNR2023TD03006, and Beijing Key Lab of Networked Multimedia.

## References

* [1]A. Navon, I. Achituve, H. Maron, G. Chechik, and E. Fetaya (2021) Auxiliary learning by implicit differentiation. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, Cited by: SS1, SS2.
* [2]S. Liu, A. J. Davison, and E. Johns (2019) Self-supervised generalisation with meta auxiliary learning. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 1677-1687. Cited by: SS1, SS2.
* November 2, 2019, pp. 1476-1485. Cited by: SS1, SS2.
* [4]H. Wen, J. Zhang, Y. Wang, F. Lv, W. Bao, Q. Lin, and K. Yang (2020) Entire space multi-task modeling via post-click behavior decomposition for conversion rate prediction. In SIGIR 2020, Virtual Event, China, July 25-30, 2020, pp. 2377-2386. Cited by: SS1, SS2.
* [5]E. Shelhamer, P. Mahmoudieh, M. Argus, and T. Darrell (2017) Loss is its own reward: self-supervision for reinforcement learning. In ICLR 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings, Cited by: SS1, SS2.
* [6]X. Lin, H. Singh Baweja, G. Kantor, and D. Held (2019) Adaptive auxiliary task weighting for reinforcement learning. In NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 4773-4784. Cited by: SS1, SS2.
* ECCV 2018
- 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part VIII, pp. 241-256. Cited by: SS1, SS2.
* [8]Z. Zhang, P. Luo, C. Loy, and X. Tang (2014) Facial landmark detection by deep multi-task learning. In European conference on computer vision, pp. 94-108. Cited by: SS1, SS2.
* [9]Y. Du, W. M. Czarnecki, S. M. Jayakumar, R. Pascanu, and B. Lakshminarayanan (2018) Adapting auxiliary losses using gradient similarity. CoRR. Cited by: SS1, SS2.
* [10]B. Shi, J. Hoffman, K. Saenko, T. Darrell, and H. Xu (2020) Auxiliary task reweighting for minimum-data learning. In NeurIPS 2020, December 6-12, 2020, virtual, Cited by: SS1, SS2.
* [11]H. Chen, X. Wang, C. Guan, Y. Liu, and W. Zhu (2022) Auxiliary learning with joint task and data scheduling. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, pp. 3634-3647. Cited by: SS1, SS2.
* [12]H. Chen, X. Wang, Y. Liu, Y. Zhou, C. Guan, and W. Zhu (2022) Module-aware optimization for auxiliary learning. In Advances in Neural Information Processing Systems, Cited by: SS1, SS2.
* [13]S. Liu, S. James, A. J. Davison, and E. Johns (2022) Auto-lambda: disentangling dynamic task relationships. arXiv preprint arXiv:2202.03091. Cited by: SS1, SS2.
* [14]H. Chen, X. Wang, R. Xie, Y. Zhou, and W. Zhu (2023) Cross-domain recommendation with behavioral importance perception. In Proceedings of the ACM Web Conference 2023, pp. 1294-1304. Cited by: SS1, SS2.
* [15]M. Crawshaw (2020) Multi-task learning with deep neural networks: a survey. arXiv preprint arXiv:2009.09796. Cited by: SS1, SS2.
* [16]S. Vandenhende, S. Georgoulis, B. De Brabandere, and L. Van Gool (2019) Branched multi-task networks: deciding what layers to share. arXiv preprint arXiv:1904.02920. Cited by: SS1, SS2.

* [17] Pengsheng Guo, Chen-Yu Lee, and Daniel Ulbricht. Learning to branch for multi-task learning. In _International Conference on Machine Learning_, pages 3854-3863. PMLR, 2020.
* [18] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. In _CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018_, pages 7482-7491, 2018.
* [19] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. In _ICML 2018_, pages 793-802. PMLR, 2018.
* [20] Jie Song, Yixin Chen, Xinchao Wang, Chengchao Shen, and Mingli Song. Deep model transferability from attribution maps. _Advances in Neural Information Processing Systems_, 32, 2019.
* [21] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. In _Advances in Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* [22] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. Wide & deep learning for recommender systems. In _Proceedings of the 1st workshop on deep learning for recommender systems_, pages 7-10, 2016.
* [23] Huifeng Guo, Bo Chen, Ruiming Tang, Weinan Zhang, Zhenguo Li, and Xiuqiang He. An embedding learning framework for numerical features in ctr prediction. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 2910-2918, 2021.
* [24] Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. In _The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy]_, Proceedings of Machine Learning Research, pages 1540-1552. PMLR, 2020.
* [25] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 Dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.
* [26] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. _Handbook of Systemic Autoimmune Diseases_, 1(4), 2009.
* 15, 2016_, pages 507-517. ACM, 2016.
* [28] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In _2012 IEEE Conference on Computer Vision and Pattern Recognition_, pages 3498-3505, 2012.
* [29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016_, pages 770-778. IEEE Computer Society, 2016.
* [30] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. Autoint: Automatic feature interaction learning via self-attentive neural networks. In _Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019, Beijing, China, November 3-7, 2019_, pages 1161-1170. ACM, 2019.
* [31] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. _arXiv preprint arXiv:1710.09412_, 2017.
* [32] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In _International Conference on Machine Learning_, pages 6438-6447. PMLR, 2019.