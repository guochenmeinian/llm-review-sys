Interpret Your Decision: Logical Reasoning Regularization for Generalization in Visual Classification

Zhaorui Tan\({}^{1,2}\), Xi Yang\({}^{1}\), Qiufeng Wang\({}^{1}\), Anh Nguyen\({}^{2}\), Kaizhu Huang\({}^{3}\)

\({}^{1}\) Xi'an-Jiaotong Liverpool University

\({}^{2}\) University of Liverpool

\({}^{3}\)Duke Kunshan University

Corresponding authors. <Xi.Yang01@xjtlu.edu.cn; kaizhu.huang@dukekunshan.edu.cn>

###### Abstract

Vision models excel in image classification but struggle to generalize to unseen data, such as classifying images from unseen domains or discovering novel categories. In this paper, we explore the relationship between logical reasoning and deep learning generalization in visual classification. A logical regularization termed L-Reg is derived which bridges a logical analysis framework to image classification. Our work reveals that L-Reg reduces the complexity of the model in terms of the feature distribution and classifier weights. Specifically, we unveil the interpretability brought by L-Reg, as it enables the model to extract the salient features, such as faces to persons, for classification. Theoretical analysis and experiments demonstrate that L-Reg enhances generalization across various scenarios, including multi-domain generalization and generalized category discovery. In complex real-world scenarios where images span unknown classes and unseen domains, L-Reg consistently improves generalization, highlighting its practical efficacy.

## 1 Introduction

One critical challenge in visual classification models is their ability to generalize effectively to unseen samples or unknown classes. For instance, a model trained on real images of various animals should ideally classify animal sketches accurately (referred to as multi-domain generalization classification [20, 35, 34, 23, 25, 37, 50]) or discover novel categories not present in the training set (referred to as generalized category discovery [54, 16]). These problems are prevalent in real-world scenarios, where training data-target pairs are usually insufficient, and labeling is time-consuming so that not every data is paired with a label. Meanwhile, test data is likely to contain shifts in both data and targets, making it essential to propose methods that generalize to border scenarios.

Regularization terms, such as \(L_{2}\) regularization leading to weight decay, are commonly employed during training to improve a model's generalization capabilities. However, the \(L_{2}\) regularization

Figure 1: GradCAM [45] visualizations for the unknown class ‘person’ across seen and unseen domains of the GMDG baseline with \(L_{2}\) regularization that is trained without and with L-Reg, respectively. Both experiments share the same hyper-parameters, except the latter uses the L-Reg.

is _parametric-based_ rather than _sample-based_, which may lead to ambiguous interpretability [58]. As illustrated in Fig. 1, the model trained solely with \(L_{2}\) regularization exhibits low interpretability. Other regularization terms [57; 58; 59] attempt to improve the interpretability of deep learning models for sequential signals rather than vision, whereas [39] proposes a regularization term to enhance interpretability for robustness in visual classification models rather than generalization. Drawing inspiration from logical reasoning has shown promise for better generalization and interpretability in various tasks. Current work unveils the effectiveness of logical reasoning in generalization tasks, such as boosting performance in length generalization [1; 3; 2; 60] and abstract symbol relational reasoning [10; 36] (e.g., mathematical solving and psychological tests). Several efforts, such as [6], explore the explicit entropy-based logical explanations of neural networks for image classification, confirming the presence and interpretability of logical reasoning within visual tasks. Yet, there are limited studies tackling the generalization of visual classification tasks through the lens of logical reasoning.

This paper studies two pivotal questions corresponding to the above: _1) How does logical reasoning relate to visual tasks such as image classification? 2) How can we derive a logical reasoning-based regularization term to benefit generalization?_ To achieve these, we correlate the image classification procedure in computer vision with the framework of logic studies [4], positing that training an image classifier involves learning a _good general_ logical relationship between images and labels via an encoder. This good general logic is attained when the semantics generated by the encoder and classifier can be combined to form atomic formulas. Our exploration leads to the introduction of a sample-based Logical regularization term named L-Reg. We reveal that L-Reg efficiently reduces the _complexity_ of the model from two aspects: 1) L-Reg leads to a balanced feature distribution in the semantic space; 2) L-Reg reduces the number of weights with extreme values in the classifier.

Intuitively, the complexity reduction achieved by L-Reg stems from its ability to filter out redundant features or semantics, focusing instead on the minimal yet sufficient semantics for classification - defined as semantic support in Definition 3.2, where the interpretability also emerges. This filtering feature benefits the generalization when there is a domain shift in data where the domain-dependent features are ignored for classification. Moreover, it further promotes generalization when unlabeled data from the unknown classes is present. If such data lacks the semantic support associated with known classes, it is then classified as belonging to an unknown class, and its corresponding semantic supports are extracted. These capabilities equip L-Reg with explicit interpretability. As Fig. 1 shows, with L-Reg, the model can identify the unknown class 'person', and pinpoint faces which are the crucial features for classifying this category. In contrast, the model trained solely with \(L_{2}\) (without L-Reg) focuses on the ambiguous features for classification.

Rigorous theoretical analysis and experimental results validate that L-Reg yields better generalization across diverse scenarios. Specifically, L-Reg facilitates better performance under the aforementioned multi-domain generalization and generalized category discovery tasks, whose settings are presented in Fig. 2 (a)(b). Furthermore, to evaluate L-Reg's robustness, we introduce a more complex real-world scenario, as shown in Fig. 2 (c), where unlabeled images may not only belong to unknown classes, but also originate from unseen domains. Even in this challenging context, L-Reg is still able to consistently demonstrate notable improvements in generalization, underscoring its practical utility and effectiveness. Our code is available at https://github.com/zhaorui-tan/L-Reg_NeurIPS24.

## 2 Preliminaries and generalization settings for visual classification

Consider paired \((X,Y)\sim(\mathcal{X},\mathcal{Y})\), \((X_{s},Y_{s})\sim(\mathcal{X}_{s},\mathcal{Y}_{s})\), and \((X_{u},Y_{u})\sim(\mathcal{X}_{u},\mathcal{Y}_{u})\) denote all sets of inputs and labels, seen paired subsets of \((X,Y)\), and unseen paired subsets of \((X,Y)\), respectively. Note that \(X_{u},Y_{u}\) may be accessible for the model separately, but their pairing relationships are not accessible. Let \(D\) denote the possible domains, with \(D_{s},D_{u}\subset D\) representing the seen and unseen domains. In classification tasks, an encoding function \(g(x)\to Z\in\mathbb{R}^{M}\) is commonly introduced to map \(X\) into the latent feature set \(Z\), where each latent feature has \(M\) dimensions. A predictor \(h(Z)\rightarrow\hat{Y}\in\mathbb{R}^{K}\) maps \(Z\) to predictions \(\hat{Y}\), where \(K\) denotes

Figure 2: Diagrams of different generalization settings in visual classification tasks.

the number of classes and the dimensions of predictions. \(P(\cdot)\) and \(H(\cdot)\) symbolize probability and entropy, respectively. This paper discusses two typical cases for generalization in image classification tasks: (1) _Data-shift generalization:_\(X_{s}\) and \(X_{u}\) have distribution shifts, such as multi-domain generalization (mDG); and (2) _Target-shift generalization:_\(Y_{s}\) and \(Y_{u}\) have distribution shifts, which stands for tasks like generalized category discovery (GCD). We additionally explore a challenging scenario called _All-shift generalization:_ both \(X_{s}\) and \(X_{u}\), \(Y_{s}\) and \(Y_{u}\) have distribution shifts, which is a combination of mDG and GCD tasks (mDG + GCD). The following lists the detailed settings for generalization. Please refer to Fig. 2 for brief diagrams.

**Data-shift generalization: Problem setting for mDG.** Illustrated in Fig. 2 (a), mDG [9] intends to generalize well to unseen domains having the objective of \(\min H(X_{s},Y_{s}\mid D_{s})\) and expecting the model to be generalized to \(X_{u}\) when predicting \(Y_{u}\) from the unseen domain \(D_{u}\). In such cases, \(Y_{u}\) is fully accessible to the model since \(Y_{s}\) and \(Y_{u}\) share the same domain: \(\mathcal{Y}_{s}=\mathcal{Y}_{u}\) but there are shifts in \(X\) where \(\mathcal{X}_{s}\neq\mathcal{X}_{u}\).

**Target-shift generalization: Problem setting for GCD.** GCD [54] (Fig. 2 (b)) aims to discover possible unseen labels among unlabeled datasets \(X_{u}\). The challenge is that the samples in \(X_{u}\) may belong to known classes or unknown classes: \(\mathcal{Y}_{s}\neq\mathcal{Y}_{u}\) and probably \(\mathcal{Y}_{s}\cap\mathcal{Y}_{u}\neq\emptyset\). The model should be able to distinguish the samples from the known classes and cluster the samples for unknown classes simultaneously. Note that \(X_{u}\) is used for model training, but the relationship between \(X_{u}\) and \(Y_{u}\) is unseen for the model. In summary, shifts exist between \(Y_{s}\) and \(Y_{u}\) but not between \(X_{s}\) and \(X_{u}\).

**All-shift generalization: Problem setting for mDG + GCD.** To explore the generalization problem further, we introduce a setting that is the combination of mDG and GCD as shown in Fig. 2 (c). Specifically, the model is trained on the labeled pairs \((X_{s},Y_{s})\) and unlabeled set \(X_{u}\) from the seen domains \(D_{s}\); \(X_{u}\) may belong to known and unknown classes. Furthermore, the model is tested on \(X_{u}\) from the unseen domain \(D_{u}\), where \(X_{u}\) may also come from the known and unknown classes. In this setting, the model is expected to 1) classify samples to the seen classes and discover the unseen classes among unlabeled samples from seen domains and 2) generalize this ability to the samples from the unseen domain. In this scenario, \(X_{s}\) and \(X_{u}\) have shifts, and so do \(Y_{s}\) and \(Y_{u}\).

For all aforementioned generalization settings, the objective can be summarized as minimizing the _generalization loss_:

**Definition 2.1** (Generalization loss).: Let the target model \(f^{*}:f^{*}(X,Y):X\to Y\), can generalize across both seen and unseen sets \(X,Y\). Denote its trainable form \(f\), which is only trained on the seen sets. The generalization loss for the unseen sets is defined as:

\[GL(f,f^{*},(X_{u},Y_{u}))=\mathbb{E}_{(x,y)\in(X_{u},Y_{u})}||f(x,y)-f^{*}(x, y)||_{2}.\] (1)

## 3 Logical regularization for generalization in image classification

Under the problem settings defined in Section 2, we introduce Logic regularization (L-Reg) targeting the objective:

\[\min_{h,g}\mathbb{E}_{z_{i}\in z,z\in Z}[H(\hat{Y}|z_{i},D)]-\mathbb{E}_{z\in Z }[H(\hat{Y}|Z,D)],\] (2)

Figure 3: Visualizations of classifiers’ weights form models trained using GMDG on PACS dataset without and with L-Reg under mDG+GCD setting, respectively. Both experiments share the same hyper-parameters using Regnety-16g backbone, except the latter uses additional L-Reg.

where \(\hat{Y}\in\mathbb{R}^{K}=h\circ g(X)\) is the prediction set. The corresponding Logic regularization loss (L-Reg) is defined as:

\[\begin{split} L_{L-Reg}=&-\frac{1}{M}\sum_{i=1}^{M} \left[\sum_{j=1}^{K}\sigma_{j,i}(\hat{Y}^{T}Z)\log\sigma_{j,i}(\hat{Y}^{T}Z) \right]\\ &+\sum_{j=1}^{K}\left[\frac{1}{M}\sum_{i=1}^{M}\sigma_{j,i}(\hat{ Y}^{T}Z)\log(\frac{1}{M}\sum_{i=1}^{M}\sigma_{j,i}(\hat{Y}^{T}Z))\right],\end{split}\] (3)

where \(\sigma_{j,i}(\hat{Y}^{T}Z)\) denotes the value at the \(i,j\) position of \(softmax(\hat{Y}^{T}Z)\) and the soft-max function is applied at the last dimension. By incorporating other existing methods' losses denoted by \(L_{main}\), the overall loss is formulated as:

\[L_{all}=L_{main}+\alpha L_{L-Reg},\] (4)

with a weight \(\alpha\) applied to balance two losses. As depicted in Fig. 1, L-Reg plays a pivotal role in extracting crucial features for image classification, thus enhancing generalization capabilities. This beneficial outcome can be attributed to two primary factors:

**Reducing classifier complexity:** L-Reg streamlines the complexity of the classifier itself, as depicted in Fig. 3 (a). Notably, the heat map of the model with L-Reg displays fewer extremely valued weights, evidenced by the diminished presence of intense blue and red colors. This reduction implies that the classifier focuses on leveraging semantically rich and relevant features for decision-making (classification), sidelining the less relevant ones. Additionally, Fig. 3 (b) reveals a reduction in the number of semantic features used to classify each class.

**Balancing feature complexity:** L-Reg results in a more balanced distribution of features compared to the baseline, as illustrated in Fig. 4. This balanced distribution suggests the elimination of certain extracted semantics characterized by dominant frequencies across all samples. Semantics that occur frequently across samples often lack decisiveness for classification. Hence, reducing their prominence contributes to more expressive feature space and less complex feature distributions. Coupled with the reduced classifier complexity, a simplified classifier achieved through L-Reg facilitates improved generalization across various settings. Specifically, the top row also indicates the distance between the feature distributions of the known and unknown classes, which is enlarged; thus, they are more dividable, leading to classification improvements.

We present a logical-based theoretical analysis in Section 3.1 and provide the derivation details of L-Reg in Section 3.2. In addition, we discuss the efficacy of L-Reg under various generalization settings in Section 4. Furthermore, L-Reg serves as a plug-and-play loss function that is compatible with most existing frameworks. We conduct experiments applying L-Reg to various established approaches across different generalization settings, as outlined in Section 5.

### Logical framework for visual classification

This part provides the connections between logical reasoning and visual classification tasks. We would like to remind readers of the framework for studying logics and link it with our practical scenarios.

**Definition 3.1**.: Following [4], a logic \(\mathcal{L}\) is defined as a five-tuple in the form:

\[\mathcal{L}=\left\langle F_{\mathcal{L}},M_{\mathcal{L}},\models_{\mathcal{L}},mng_{\mathcal{L}},\vdash_{\mathcal{L}}\right\rangle,\] (5)

where 1) \(F_{\mathcal{L}}\) denotes the set of formulas formed by images and labels (\(X,Y\)); 2) \(M_{\mathcal{L}}\) represents different domains \(D\) of \(X\); 3) \(\models_{\mathcal{L}}\) is a binary relation relating the truth of whether the formulas are true or false, which has \(\models_{\mathcal{L}}\subseteq M_{\mathcal{L}}\times F_{\mathcal{L}}\); 4) \(mng_{\mathcal{L}}:F_{\mathcal{L}}\times M_{\mathcal{L}}\longrightarrow\) Sets defines the meaning of \(X\) as determined by classifiers, where Sets indicate the class of all sets. (5) \(\vdash_{\mathcal{L}}\) symbolizes the provability relation of \(\mathcal{L}\), evaluating formulas formed by \(mng_{\mathcal{L}}\) is true or false in one possible world, such as the estimation criteria. More details of \(\mathcal{L}\) can be seen in Appendix B.

Figure 4: Visualizations of latent features form models trained using GMDG on PACS dataset without and with L-Reg under mDD+GCD setting using RegNetY-16G backbone, respectively.

For clarity, we specify \(\mathcal{L}_{(X_{s},Y_{s})}=\left\langle F_{(X_{s},Y_{s})},D,\models_{(X_{s},Y_{s}) },h,\vdash_{(h(X),Y)}\right\rangle\) as the logic formed on the given \(X,Y\) sets. With the goal for logic to generalize across a broader scenario and provide extrapolation across all possible formulas in \(\mathcal{L}\), a good general logic \(\mathcal{L}^{*}\) should be derived from \(\mathcal{L}\) through the feature extractor \(g\):

\[\mathcal{L}^{*}=\left\langle F_{(g(X_{s}),Y_{s})},D,\models_{(g(X_{s}),Y_{s}) },h,\vdash_{(hog(X),Y)}\right\rangle,s.t.,\vdash_{(hog(X),Y)}=\models_{(g(X_{s} ),Y_{s})}.\] (6)

Importantly, as a good general logic, \(F_{(g(X_{s}),Y_{s})}\) and \(h\) in \(\mathcal{L}^{*}\) should form the _atomic formulas_, i.e., the tuple of terms with a predicate: \(h\circ g(x)\) belongs/not belongs to class \(y\) in domain \(d\ \rightarrow\)\(Ture/False,\ \ \text{where}\ x,y,d\in X,Y,D\), which makes that \(\vdash_{(hog(X_{u}),Y_{u})}=\models_{(g(X_{s}),Y_{s})}\) still holds. We simply denote one atomic formula in the form of \(h(g(x),y,d)\) mapping to binary values. Additionally, \(\vdash_{(hog(X),Y)}=\models_{(g(X_{s}),Y_{s})}\) in Eq. (6) can be safely omitted in the rest of the paper. Please see more details about the conditions of the good general logic in Appendix B.

An additional tool is necessary to convert the logic problem into a continuous form, enabling the application of machine learning algorithms. The conditional entropy-based method enables a logically sound derivation of knowledge from the provided dataset with constraints [43]. Specifically, the probabilistic inference process adheres to a probabilistic version of Modus Ponens: \(A\to B,A\vdash B\) (if \(A\) then \(B\); not \(A\) therefore not \(B\)). It is important to note that the logical propositions in probabilistic Modus Ponens are uncertain, with the conditional probability replacing the material implication \(A\to B\). This framework allows us to interpret logical deduction through the lens of entropy. Therefore, for Eq. (6) which implies

\[\exists h\circ g,\ \forall(x,y)\in(X,Y),\ \forall d\in D,\ h\circ g(x)\to y,\] (7)

finding \(h\circ g\) through optimization is equivalent to

\[\max_{h,g}\mathbb{E}_{(x,y)\in(X,Y),d\in D}P(y|g(x),d)-\mathcal{R}\Longleftrightarrow \min_{h,g}\mathbb{E}_{(x,y)\in(X,Y),d\in D}H(y|g(x),d)+\mathcal{R},\] (8)

where \(\mathcal{R}\) denotes any other possible regularization.

As the logical framework for image classification takes shape, it becomes evident that the unresolved question of identifying an appropriate function \(g\) to generate suitable atomic formulas emerges as a critical factor in ensuring the effectiveness of the overarching logic \(\mathcal{L}^{*}\). This paper proposes L-Reg as the regularization to ensure \(F_{(g(X_{s}),Y_{s})}\) are formed by atomic formulas in Section 3.2.

### Constructing atomic formulas using L-Reg

In this part, we show the derivation details of L-Reg the aims to ensure the formation of suitable atomic formulas, as depicted in Eq. (6). As highlighted in [1], current algorithms may induce implicit biases towards unseen data, resulting in varied solutions for such data. However, expecting an algorithm to generalize effectively to unseen data domains without appropriate incentivization, such as specifically designed regularization, is unreasonable. Therefore, we aim to enhance the generalization capability of models by employing a logic-based regularization approach. To this end, we introduce the concept of _semantic support_ for image classification.

**Definition 3.2** (Semantic support).: We denote \(z=g(x)\), where \(z\in Z\), as a set of compositions of these semantics: \(z:=\{z^{i}\}_{i=1}^{M}\), where \(M\) is the number of dimensions or semantics. Notably, not all semantics in \(z\) may be useful for deduction or inference. We define the subset \(\gamma\) of \(z\), extracted from the sample \(x\sim\mathcal{X}\), as the semantic support of \(x\) if \(\gamma\) is sufficient for deducing the relationship between \(x\) and a \(y\sim\mathcal{Y}\).

For instance, if the subset \(\{z^{1},z^{2}\}\subseteq z\) is sufficient for accurate inference, the values of other semantics \(\{z^{i}\}_{i=3}^{M}\) will not impact the inference process. When \(\{z^{1},z^{2}\}\) constitutes the minimal combination of semantics required for inference, it is termed the semantic support. We denote \(\Gamma\) as the set of semantic supports of \(X\) for deducing each individual class.

**Derivation of L-Reg.** Regarding Eq. (6), if the semantic supports and their relationship with \(Y\) form atomic formulas, Eq. (6) holds as a good general logic, and the generalization would be improved. Thus, we aim to learn the latent features \(Z\), which contain sufficient semantic supports for the deduction of \(Y\):

\[\exists\gamma\in\Gamma,\gamma\subseteq z,\ \forall(z,y)\in(Z,Y),\forall d\in D, \ h(\gamma|d)\to y.\] (9)

Specifically, \(g(\cdot)\) should meet the following:

\[\forall(\Gamma_{i},y_{i}),(\Gamma_{j},y_{j})\in(Z,Y),\forall d\in D,\ y_{i} \neq y_{j}\Longleftrightarrow\Gamma_{i}\neq\Gamma_{j},\] (10)i.e., the semantic support set for each class should be distinct. The multiple-class classification task has that \(\forall\Gamma,|\Gamma|\leq M\). Under the constraints demonstrated in Eq. (9) and Eq. (10), we need to achieve the following through optimization:

\[\min_{h,g}H(Y|g(\Gamma),D),\max_{h,g}H(Y|g(\bar{\Gamma}),D)\Longleftrightarrow \min_{h,g}H(Y|g(\Gamma),D)-H(Y|g(\bar{\Gamma}),D),\] (11)

where \(\bar{\Gamma}\) denotes the negation of \(\Gamma\), i.e., the set of semantics which does not include semantic support.

Intuitively, Eq. (11) regularizes that the model should be able to judge whether a sample belongs to a class by using a minimal set of semantic supports; simultaneously, the semantic support sets are also implicitly disentangled for each class, not only for maintaining rich and useful semantics but also for enhancing the independence of deduction of each class. The actual collection of \(\Gamma\) appears to be intractable during optimization. Hence, we resort to deriving its bounds. Regarding Eq. (11), its former term can be elaborated as follows:

\[H(Y|g(\Gamma),D)\leq H(Y|h(z_{i}),D)\leq\mathbb{E}_{z_{i}\sim z}[H(Y|g(z_{i}), D)],\] (12)

where \(z_{i}\) is minimal semantics form \(z\), and \(\mathbb{E}_{i=1}^{M}H(Y|g(z_{i}),D)\) is the upper-bound for \(\min_{h,g}H(Y|g(\Gamma),D)\). Therefore, minimizing \(\mathbb{E}_{i=1}^{M}H(Y|g(z_{i}),D)\) is equivalent to minimizing \(H(Y|g(\Gamma),D)\). Meanwhile, for the latter in Eq. (11), we have:

\[H(Y|g(\bar{\Gamma}),D)\geq H(Y|g(z),D),\] (13)

where \(H(Y|h(z)),D)\) is the lower-bound for \(\max_{h,g}H(Y|g(\bar{\Gamma}),D)\). Combining the aforementioned bounds, we have the L-Reg objective as Eq. (2).

**Interpretability of semantic supports roots in forming atomic formulas.** The atomic formula \(\mathcal{A}^{y}\) is of the form \(h(g(x),y,d)\). Our aim is to find the good (most) general \(\mathcal{A}^{y*}\in\mathcal{A}^{y}\) for \(y\) class from which the interpretability of L-Reg is derived. Consider \(\mathcal{A}_{1}^{y},\mathcal{A}_{2}^{y}\in\mathcal{A}^{y}\), if \(\mathcal{A}_{1}^{y}\) is more general than \(\mathcal{A}_{2}^{y}\), there will be a substitution \(\psi\) such that \(\mathcal{A}_{1}^{y}\psi=\mathcal{A}_{2}^{y}\)[52]. \(\mathcal{A}^{y*}\) should meet \(\mathcal{A}^{y*}\psi=\mathcal{A}_{1}^{y}\in\mathcal{A}^{y}\), which infers that \(\gamma^{y}\psi=z^{y}\) (cf. Eq. (9)) for predication of \(y\) where \(\gamma^{y}\) is the semantic support. Note here that the form of \(\mathcal{A}^{y}\) is constructed for \(y\in Y\), i.e., predicate whether the sample belongs to the \(y\) class. Considering multiple classes \(y_{i},\,y_{j}\in Y,i\neq j\), it has \(\mathcal{A}^{y_{i}*}\neq\mathcal{A}^{y_{j}*}\) thus \(\gamma^{y_{i}}\neq\gamma^{y_{j}}\) (cf. Eq. (10)), which constrains that different minimal semantic supports should be used for predicting different classes. The interpretability of L-Reg is based on \(\mathcal{A}^{y*}\), compelling the model to use distinct minimal semantic supports for each class. These minimal semantic supports can be interpreted as the most critical features for efficient prediction. For example, as shown in Fig. 1, the model with L-Reg has learned the facial features of the person class (see more examples in Appendix Figs. 7 to 12), forming the (informal) atomic formula \(h(\text{has a human face},\text{is person},d\in D)\rightarrow\text{True}\). Similarly, it also leads to \(h(\text{not has a human face},\text{is person},d\in D)\rightarrow\text{False}\).

## 4 L-Reg under different generalization settings

**L-Reg under data-shift generalization.** The task mDG endeavors to facilitate a model's ability to generalize to unseen domains by fostering invariance across seen domains [50]. In the context of mDG, the term \(|D|\geq 2\) in Eq. (8) typically denotes multiple domains. Traditionally, existing methods focus on minimizing domain gaps, leading to remarkable results [25; 50]. However, it is noteworthy that even when the domain gap is effectively minimized, and \(|D|=1\) for the latent features can be considered, L-Reg still demonstrates its efficacy in promoting the generalization of \(X_{u}\) from \(D_{u}\).

**Proposition 4.1** (Effectiveness of L-Reg in enhancing data-shift generalization.).: _Assume the gap across all domains is well minimized. Let \(f^{*}\) denote the target model that generalizes to the data \(X_{u}\) from the unseen domain with the lowest complexity. For a model \(f^{R}_{(X_{s},Y_{s})},f_{(X_{s},Y_{s})}\) trained under the data-shift generalization setting (i.e., \((X_{s},Y_{s})\) is accessible and \(\mathcal{Y}_{s}=\mathcal{Y}_{u}\)). We have:_

\[GL(f^{R}_{(X_{s},Y_{s})},f^{*},X_{u})\leq GL(f_{(X_{s},Y_{s})},f^{*},X_{u}).\] (14)

Please see proof details in Proposition C.1. To illustrate Proposition 4.1, consider the following intuitive example: In the seen domains, all cats are either black or white, while all dogs are brown. Now, imagine encountering a sample labeled 'a brown cat' from an unseen domain. Without the application of L-Reg, the model might erroneously classify it as a dog. However, with L-Reg in 

[MISSING_PAGE_FAIL:7]

DomainNet [42]. Following MIRO [25] and GMDG [50], the RegNetY-16GF backbone with SWAG pre-training [47]) is used. Specifically, we train the backbone using GMDG with L-Reg. Accuracy is adopted as the evaluation metric, and the results of the averages from three trials of each experiment, with standard deviations, are presented. See Supplementary H for more experimental details.

**Results.** The experimental results presented in Table 1 demonstrate the efficacy of L-Reg in improving the performance of GMDG across all datasets in mDG classification tasks. Notably, more substantial improvements are observed when the GMDG baseline achieves relatively low accuracy. These observed enhancements provide empirical support for Proposition 4.1. Please see using L-Reg with basic ERM in Appendix E. For detailed insights into each domain within each dataset, please refer to Appendix H.1.

### Experiments on GCD

**Experimental settings.** We validate our approach through training PIM additionally with L-Reg. Six image datasets are adopted to validate the feasibility of our proposed L-Reg with PIM compared to other competitors, including three generic object recognition datasets, CIFAR10 [29], CIFAR100 [29] and ImageNet-100 [17]; two fine-grained datasets CUB [56] and Stanford Cars [28]; and the long-tail dataset Herbarium19 [49]. Following prior works [54; 16], we use the proposed accuracy metric from [54] of all classes, known classes, and unknown classes for evaluation. Please see a detailed description of the experimental setup in Appendix H.2.

**Results.** The average results across all datasets for utilizing L-Reg with PIM are presented in Table 2, while detailed dataset-specific information is available in Appendix H Table 17. The results highlight that L-Reg consistently increases the accuracy of all unknown classes across all datasets, thus confirming the validity of Proposition 4.2. However, it is notable that L-Reg may marginally compromise the performance of known classes, as it reduces the size of semantic support for deducing \(Y\), thereby reducing the information available for known classification. Nevertheless, this compromise is deemed acceptable given the significant improvements observed for the unknown classes.

### Experiments on mDG + GCD

**Experimental settings.** We utilize datasets designed for mDG tasks to conduct mDG + GCD experiments. During the training stage, only samples from seen domains are available, with half of the classes masked as unknown, and only their unlabeled data are utilized. Notably, even though all the unlabeled data originates from unknown classes during training, this prior knowledge is not assumed or constrained, aligning the setting with GCD. Similar to mDG, we adopt the leave-one-out cross-validation method. This entails testing each domain in each dataset as the unseen domain. The performance is tested on unseen domains by employing GCD metrics. To validate L-Reg's efficacy comprehensively, we re-implement four methods under the mDG + GCD setting, testing them both with and without L-Reg. The four methods include ERM, PIM, MIRO, and GMDG. ERM serves as the baseline approach without additional regularization, while PIM maximizes information without minimizing domain gaps. MIRO and GMDG focus on minimizing domain gaps, with GMDG offering a comprehensive approach in this regard. It is worth noting that PIM has been re-implemented. For further experimental details, please refer to Appendix H.3.

**Results.** The averaged results across all unseen domains of all datasets are summarized in Table 3. For a detailed breakdown of results for each domain in each dataset, please refer to Appendix H.3. As discussed in Proposition 4.1 and Proposition 4.2, a noticeable trend is observed wherein, as the

\begin{table}
\begin{tabular}{l|c c c} \hline \hline Average & All & Known & Unknown \\ \hline K-means [38] & 44.7 & 46.0 & 43.9 \\ RankRankRankRankRank (2) (TPAMI-2) & 88.6 & 54.6 & 25.6 \\ U(ND+ 19) (ConvC-2) & 51.2 & 74.5 & 36.7 \\ ORCA 13(1) (Ck-2) & 46.3 & 51.3 & 41.2 \\ ORCA - VINTG & 56.7 & 65.6 & 49.9 \\ GCD 13 (CKP-22) & 60.4 & 71.8 & 52.9 \\ RGB [27] (ConvC-10) & 62.0 & 72.5 & 53.4 \\ TIM [11] (NemiP-20) & 62.7 & 72.6 & 56.4 \\ \hline PIM (16) (ICCV-2) & 67.4 & **79.3** & 59.9 \\ \hline PIM (**R+Reg**) & **68.8** & **79.0** & **56.7** \\ \hline \hline \end{tabular}
\end{table}
Table 2: GCD results: Average results across all datasets of PIM with L-Reg. Improvements and degradation are highlighted in red and blue, respectively.

\begin{table}
\begin{tabular}{l l c c} \hline \hline Method & Domain gap & All & Known & Unknown \\ \hline ERM & Not & 46.9 & 53.3 & 23.54 \\
**+LR-Reg** & minimized & 45.0 & 61.3 & 21.63 \\ \hline Img. & **0.81** & **2.09** & -1.91 \\ PIM & Not & 46.95 & 0.35 & 25.90 \\
**+LR-Reg** & minimized & 47.27 & 60.83 & 26.34 \\ \hline Img. & 0.32 & 0.48 & 0.57 \\ \hline MIRO & Not sufficiently & 49.67 & 68.86 & 25.79 \\
**+LR-Reg** & minimized & 52.11 & 71.26 & 26.49 \\ Img. & 2.44 & 23.90 & 0.71 \\ \hline GMDG & 47.94 & 68.75 & 30.68 \\
**+LR-Reg** & minimized & 51.94 & 69.87 & 27.68 \\ Img. & 4.00 & 1.12 & 7.01 \\ \hline \hline \end{tabular}
\end{table}
Table 3: MDG+GCD results: Averaged accuracy scores for all, known and unknown classes across all five datasets. Improvements and degradation are highlighted in red and blue, respectively.

domain gap is gradually minimized, the improvements for unknown classes increase, with the best results achieved using GMDG with L-Reg.

**L-Reg forms atomic formulas and improves interpretability.** Furthermore, Fig. 5 provides visual insights into the behavior of models trained with L-Reg. Evidently, these models tend to focus on minimal semantics sufficient for class distinctions. For the known classes, the efficacy of L-Reg can be intuitively understood as extracting the minimal semantic supports for a given class label. For instance, the presence of a guitar's fingerboard, even in unseen domains, helps classify a sample as belonging to the guitar category, whose informal forms can be denoted as \(h(\text{has fingerboard},\text{ is guitar},d\in D)\rightarrow\text{True}\) and \(h(\text{not has fingerboard},\text{ is guitar},d\in D)\rightarrow\text{False}\). For all known classes, samples with these minimal semantic supports are recognized accordingly. In contrast, if a sample lacks these minimal supports for any known class, it is very likely categorized as an unknown class. This behavior stems from Paper Eq.10 which ensures \(\mathcal{A}^{y_{i}*}\neq\mathcal{A}^{y_{i}*}\) through constraining \(\gamma^{y_{i}}\neq\gamma^{y_{j}}\). L-Reg further enhances the model's ability to identify minimal supports for unknown classes by filtering out co-covariant features associated with other classes and thus generalizing to unseen domains. Therefore, the very interpretable features for unknown classes from unseen domains can be extracted using L-Reg. Fig. 5 (right side) demonstrates that the model with L-Reg can even extract facial features for the unknown person class and can generalize this to the unseen domain. Similarly, here we obtain (informal) atomic formulas as \(h(\text{has a face},\text{is person},d\in D)\rightarrow\text{True}\), \(h(\text{not has a face},\text{is person},d\in D)\rightarrow\text{False}\).

However, as shown in Row 3, significant domain shifts, such as those between the sketch domain and other domains, pose challenges. Specifically, the differences between the stick-figure style of sketches of persons and figures from other domains can hinder the model's ability to cluster sketches with other domains' figures when the class label is unknown. Thus, under this circumstance, the model may fail to extract meaningful features from those sketches. We acknowledge this limitation and will explore solutions in future work.

**L-Reg should be applied to features from deep layers.** One crucial precondition highlighted in the theoretical analysis is that L-Reg operates effectively with a representation \(Z\), where each dimension represents independent semantics. The semantic features usually come from the deeper layers of the model architecture [51]. However, Table 4 shows that applying L-Reg to features from earlier layers, which may not necessarily represent semantics, leads to a degradation in performance for known classes, albeit improving performance for unknown classes. This phenomenon arises due to the potential interdependence among features from earlier layers, resulting in penalization that may hinder the capture of semantic supports essential for known classes. To ensure generalization improvements without significant compromise to the performance of known classes, we advocate for applying L-Reg specifically to features extracted from deeper layers, such as the bottleneck layer. These suggest that the compromised results observed in Table 2 could be attributed to the less depth of the model structure, which fails to provide the expected semantic features.

### Apply L-Reg to congestion prediction for circuit design.

**Experimental settings.** We also test L-Reg in Congestion prediction on the CircuitNet [15] dataset by using CircuitFormer [63] backbone. The congestion prediction is for circuit design and benefits from logical reasoning-based approaches. All parameters, except for L-Reg, remain consistent with CircuitFormer, and we follow its metrics.

\begin{table}
\begin{tabular}{l|c c c} \hline  & All & Known & Unknown \\ \hline GMDG & 58.33 & 91.46 & 10.18 \\ \hline L-Reg: Deep layer & **67.82** & **91.86** & 31.33 \\ L-Reg: Earlier and the deep layers & 58.97 & 80.73 & **35.05** \\ \hline \end{tabular}
\end{table}
Table 4: Averaged results of applying L-Reg to different layers across domains in PACS.

Figure 5: GradCAM visualizations of GMDG trained without and with L-Reg. The seen, unseen domains and known, unknown classes are denoted.

**Results.** Table 5 shows the results of prediction results on the CircuitNet dataset. We also include the results of Gpdl with UNet++ and CircuitFormer for better comparison. Notably, the improvements brought by CircuitFormer with L-Reg across all metrics, especially for the person metric can be observed. The consistent improvement with L-Reg across all metrics indicates L-Reg's feasibility.

## 6 Related work

**Logical reasoning for deep learning.** Current studies focus on length generalization or symbolic reasoning in the logic-based scope. For length generalization, [1] proposes the generalization to the unseen setting, theoretically verifying that commonly used models can generalize to the unseen and degree curriculum promotes the generalization ability of the transformer, followed by [3; 2; 60]. Another branch is to improve the logical reasoning ability for abstract symbols, such as learning the logical-based temples and expecting the model to generalize to unseen samples [10; 36]. These studies are closely related to languages, such as generating longer answering sequences or solving mathematical problems in large language models, lacking explicit connections to visual tasks. [6] delves into the logical explanations in image classification by explicitly extracting logical relationships. While this logical-based approach sheds light on the interpretability of image classification models, its specific benefits for visual generalization remain relatively unexplored.

**Multi-domain generalization.** Current approaches for mDG in image classification focus on learning invariant representation across domains. Previous approaches like DANN [20] minimize feature divergences between source domains. CDANN [35], CIDG [34], and MDA [23] consider conditions for learning conditionally invariant features. MIRO [25] and GMDG [50] take advantage of pre-trained models to improve generalization. Specifically, in comparison to MIRO, GMDG proposes a general entropy-based learning objective for mDG and sufficiently minimizes the domain gaps, yielding better generalization results.

**Generalized category discovery.** Generalized category discovery, pioneered by [54], addresses unlabeled samples with both known and unknown classes. Furthermore, PIM [16] integrates InfoMax into generalized category discovery, effectively handling imbalanced datasets and surpassing GCD on both short- and long-tailed datasets.

## 7 Conclusion

This paper presents L-Reg, a logical regularization approach tailored for image classification tasks using logic analysis frameworks. L-Reg yields better generalization across different settings by fostering balanced feature distributions and streamlining the classification model's complexity. Rigorous theoretical analyses and empirical validations underscore its efficacy, as L-reg consistently improves generalization performance with different frameworks under various scenarios.

**Limitation.** L-Reg narrows the extent of semantic supports, potentially diminishing the amount of information available for classification and leading to certain trade-offs in the performance of seen datasets. This effect is evidenced by the slight decline in the accuracy of known classes when L-Reg is applied, as shown in Table 2. A similar phenomenon is observed in Fig. 5, where the model fails to recognize a person in the sketch domain lacking facial features. Analysis from Table 4 suggests that these compromises may result from improper \(Z\). Future work should focus on mitigating potential compromises on seen datasets by exploring strategies for better capturing \(Z\) through improved model architecture design. We offer more experimental results of possible solutions to this limitation in Appendix G, such as further constraining the independence of each dimension in \(Z\). Those results may suggest a direction for future work.

## Acknowledgments

The work was partially supported by the following: National Natural Science Foundation of China under No. 92370119, No. 62376113, No. 62206225, and No. 62276258.

\begin{table}
\begin{tabular}{c|c c c} \hline  & pearson & spearman & kendall \\ \hline Gpdl with UNet++ & 0.6085 & 0.5202 & 0.3855 \\ CircuitFormer (SOTA) & 0.6374 & 0.5282 & 0.3935 \\ \hline
**CircuitFormer+ L-Reg (Ours)** & **0.6553** & **0.5289** & **0.3944** \\ \hline \end{tabular}
\end{table}
Table 5: **Results of Congestion prediction:** Congestion prediction is proposed for circuit design.

## References

* [1] Emmanuel Abbe, Samy Bengio, Aryo Lotfi, and Kevin Rizk. Generalization on the unseen, logic reasoning and degree curriculum. In _International Conference on Machine Learning_, pages 31-60. PMLR, 2023.
* [2] Emmanuel Abbe, Elisabetta Cornacchia, and Aryo Lotfi. Provable advantage of curriculum learning on parity targets with mixed inputs. _Advances in Neural Information Processing Systems_, 36, 2024.
* [3] Kartik Ahuja and Amin Mansouri. On provable length and compositional generalization. _arXiv preprint arXiv:2402.04875_, 2024.
* [4] Hajnal Andreka, Istvan Nemeti, and Ildiko Sain. Universal algebraic logic. _Studies in Logic, Springer, due to_, 2017.
* [5] Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. _arXiv preprint arXiv:1907.02893_, 2019.
* [6] Pietro Barbiero, Gabriele Ciarvegna, Francesco Giannini, Pietro Lio, Marco Gori, and Stefano Melacci. Entropy-based logic explanations of neural networks. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 6046-6054, 2022.
* [7] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In _Proceedings of the European conference on computer vision (ECCV)_, pages 456-473, 2018.
* [8] Gilles Blanchard, Aniket Anand Deshmukh, Urun Dogan, Gyemin Lee, and Clayton Scott. Domain generalization by marginal transfer learning. _The Journal of Machine Learning Research_, 22(1):46-100, 2021.
* [9] Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generalizing from several related classification tasks to a new unlabeled sample. _Advances in neural information processing systems_, 24, 2011.
* [10] Enric Boix-Adsera, Omid Saremi, Emmanuel Abbe, Samy Bengio, Etai Littwin, and Joshua Susskind. When can transformers reason with abstract symbols? _arXiv preprint arXiv:2310.09753_, 2023.
* [11] Malik Boudiaf, Imtiaz Ziko, Jerome Rony, Jose Dolz, Pablo Piantanida, and Ismail Ben Ayed. Information maximization for few-shot learning. _Advances in Neural Information Processing Systems_, 33:2445-2457, 2020.
* [12] Manh-Ha Bui, Toan Tran, Anh Tran, and Dinh Phung. Exploiting domain-specific features to enhance domain generalization. _Advances in Neural Information Processing Systems_, 34:21189-21201, 2021.
* [13] Kaidi Cao, Maria Brbic, and Jure Leskovec. Open-world semi-supervised learning. In _International Conference on Learning Representations_, 2022.
* [14] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 9650-9660, 2021.
* [15] Zhuomin Chai, Yuxiang Zhao, Wei Liu, Yibo Lin, Runsheng Wang, and Ru Huang. Circuitnet: An open-source dataset for machine learning in vlsi cad applications with improved domain-specific evaluation metric and learning strategies. _IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems_, 42(12):5034-5047, 2023.
* [16] Florent Chiaroni, Jose Dolz, Ziko Imtiaz Masud, Amar Mitiche, and Ismail Ben Ayed. Parametric information maximization for generalized category discovery. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1729-1739, 2023.
* [17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* [18] Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In _Proceedings of the IEEE International Conference on Computer Vision_, pages 1657-1664, 2013.
* [19] Enrico Fini, Enver Sangineto, Stephane Lathuiliere, Zhun Zhong, Moin Nabi, and Elisa Ricci. A unified objective for novel class discovery. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9284-9292, 2021.

* [20] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. _The journal of machine learning research_, 17(1):2096-2030, 2016.
* [21] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. _arXiv preprint arXiv:2007.01434_, 2020.
* [22] Kai Han, Sylvestre-Alvise Rebuffi, Sebastien Ehrhardt, Andrea Vedaldi, and Andrew Zisserman. Autonovel: Automatically discovering and learning novel visual categories. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2021.
* [23] Shoubo Hu, Kun Zhang, Zhitang Chen, and Laiwan Chan. Domain generalization via multidomain discriminant analysis. In _Uncertainty in Artificial Intelligence_, pages 292-302. PMLR, 2020.
* [24] Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II 16_, pages 124-140. Springer, 2020.
* [25] Cha Junbum, Lee Kyungjae, Park Sungrae, and Chun Sanghyuk. Domain generalization by mutual-information regularization with pre-trained models. _European Conference on Computer Vision (ECCV)_, 2022.
* [26] Daehee Kim, Youngjun Yoo, Seunghyun Park, Jinkyu Kim, and Jaeko Lee. Selfreg: Self-supervised contrastive regularization for domain generalization. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9619-9628, 2021.
* [27] Andreas Krause, Pietro Perona, and Ryan Gomes. Discriminative clustering by regularized information maximization. _Advances in neural information processing systems_, 23, 2010.
* [28] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In _Proceedings of the IEEE international conference on computer vision workshops_, pages 554-561, 2013.
* [29] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [30] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In _International Conference on Machine Learning_, pages 5815-5826. PMLR, 2021.
* [31] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Learning to generalize: Meta-learning for domain generalization. In _Proceedings of the AAAI conference on artificial intelligence_, volume 32, 2018.
* [32] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In _Proceedings of the IEEE international conference on computer vision_, pages 5542-5550, 2017.
* [33] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adversarial feature learning. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5400-5409, 2018.
* [34] Ya Li, Mingming Gong, Xinmei Tian, Tongliang Liu, and Dacheng Tao. Domain generalization via conditional invariant representations. In _Proceedings of the AAAI conference on artificial intelligence_, volume 32, 2018.
* [35] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generalization via conditional invariant adversarial networks. In _Proceedings of the European conference on computer vision (ECCV)_, pages 624-639, 2018.
* [36] Zenan Li, Yunpeng Huang, Zhaoyu Li, Yuan Yao, Jingwei Xu, Taolue Chen, Xiaoxing Ma, and Jian Lu. Neuro-symbolic learning yielding logical constraints. _Advances in Neural Information Processing Systems_, 36, 2024.
* [37] Ziyue Li, Kan Ren, Xinyang Jiang, Yifei Shen, Haipeng Zhang, and Dongsheng Li. Simple: Specialized model-sample matching for domain generalization. In _The Eleventh International Conference on Learning Representations_, 2022.
* [38] J MacQueen. Classification and analysis of multivariate observations. In _Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability_, pages 281-297, 1967.

* [39] Ofir Moshe, Gil Fidel, Ron Bitton, and Asaf Shabtai. Improving interpretability via regularization of neural activation sensitivity. _arXiv preprint arXiv:2211.08686_, 2022.
* [40] Hyeonsebo Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, and Donggeun Yoo. Reducing domain gap by reducing style bias. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8690-8699, 2021.
* [41] Konstantinos Panagiotis Panousis, Dino Ienco, and Diego Marcos. Sparse linear concept discovery models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 2767-2771, 2023.
* [42] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 1406-1415, 2019.
* [43] Wilhelm Rodder. Conditional logic and the principle of entropy. _Artificial Intelligence_, 117(1):83-106, 2000.
* [44] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. _arXiv preprint arXiv:1911.08731_, 2019.
* [45] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In _Proceedings of the IEEE international conference on computer vision_, pages 618-626, 2017.
* [46] Yuge Shi, Jeffrey Seely, Philip HS Torr, N Siddharth, Awni Hannun, Nicolas Usunier, and Gabriel Synnaeve. Gradient matching for domain generalization. _arXiv preprint arXiv:2104.09937_, 2021.
* [47] Mannat Singh, Laura Gustafson, Aaron Adcock, Vinicius de Freitas Reis, Bugra Gedik, Raj Prateek Kosaraju, Dhruv Mahajan, Ross Girshick, Piotr Dollar, and Laurens Van Der Maaten. Revisiting weakly supervised pre-training of visual perception models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 804-814, 2022.
* [48] Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In _Computer Vision-ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part III 14_, pages 443-450. Springer, 2016.
* [49] Kiat Chuan Tan, Yulong Liu, Barbara Ambrose, Melissa Tulig, and Serge Belongie. The herbarium challenge 2019 dataset. _arXiv preprint arXiv:1906.05372_, 2019.
* [50] Zhaorui Tan, Xi Yang, and Kaizhu Huang. Rethinking multi-domain generalization with a general learning objective. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 23512-23522, June 2024.
* [51] Zhaorui Tan, Xi Yang, and Kaizhu Huang. Semantic-aware data augmentation for text-to-image synthesis. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 5098-5107, 2024.
* [52] Irene Tsapara and Gyorgy Turan. Learning atomic formulas with prescribed properties. In _Proceedings of the eleventh annual conference on Computational learning theory_, pages 166-174, 1998.
* [53] Vladimir N. Vapnik. _Statistical Learning Theory_. Wiley-Interscience, 1998.
* [54] Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Generalized category discovery. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7492-7501, 2022.
* [55] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5018-5027, 2017.
* [56] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.
* [57] Chunyang Wu, Mark JF Gales, Anton Ragni, Penny Karanasou, and Khe Chai Sim. Improving interpretability and regularization in deep learning. _IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 26(2):256-265, 2017.
* [58] Mike Wu, Michael Hughes, Sonali Parbhoo, Maurizio Zazzi, Volker Roth, and Finale Doshi-Velez. Beyond sparsity: Tree regularization of deep models for interpretability. In _Proceedings of the AAAI conference on artificial intelligence_, volume 32, 2018.

* [59] Mike Wu, Sonali Parbhoo, Michael Hughes, Ryan Kindle, Leo Celi, Maurizio Zazzi, Volker Roth, and Finale Doshi-Velez. Regional tree regularization for interpretability in deep neural networks. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 6413-6421, 2020.
* [60] Changnan Xiao and Bing Liu. A theory for length generalization in learning to reason. _arXiv preprint arXiv:2404.00560_, 2024.
* [61] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk minimization: Learning to adapt to domain shift. _Advances in Neural Information Processing Systems_, 34:23664-23678, 2021.
* [62] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Domain generalization with mixstyle. _arXiv preprint arXiv:2104.02008_, 2021.
* [63] Jialv Zou, Xinggang Wang, Jiahao Guo, Wenyu Liu, Qian Zhang, and Chang Huang. Circuit as set of points. _Advances in Neural Information Processing Systems_, 36, 2024.

[MISSING_PAGE_FAIL:15]

congruence by the union of the tautological congruence relations of the logics belonging to \(P_{i},i\in I\).
7. \(Sig\) contains at least one non-empty set.

Our L-Reg aims to regularize the semantics extracted by \(g\) and the classifier to satisfy condition \(1\).

\(\vdash_{(\mathit{hog}(X),Y)}=\models_{(g(X_{s}),Y_{s})}\) in Eq. (6) can be safely omitted in the rest of the paper.Consider the logic formed on \(X,Y\): \(\mathcal{L}_{(X_{s},Y_{s})}=\left\langle F_{(X_{s},Y_{s})},D,\models_{(X_{s}, Y_{s})},h,\vdash_{(h(X),Y)}\right\rangle\). Assume we want to study the logic of \(\vdash\) which can be defined in the form of \(\mathcal{L}_{\vdash}\stackrel{{\text{def}}}{{\leftarrow}} \left\langle F_{X_{s},Y_{s}},D_{\vdash},h_{\vdash},\models_{\vdash}\right\rangle\), where \(D_{\vdash},h_{\vdash},\models_{\vdash}\) are pseudo-components associated with \(\vdash\). Particularly, \(D_{\vdash}\) is a subset of all possible world/domains from \(F_{(X_{s},Y_{s})}\): \(D_{\vdash}\stackrel{{\text{def}}}{{\leftarrow}}\left\{T\subseteq F _{(X_{s},Y_{s})}:T\text{ is closed under }\vdash_{(h(X),Y)}\right\}\). For any \(T\in D_{\vdash}\) and \(a\in F_{(X_{s},Y_{s})}\), it has \(h_{\vdash}(a,T)\stackrel{{\text{def}}}{{=}}\left\{b\in F:T\vdash(a \leftrightarrow b)\right\}\). Further, \(\models_{\vdash}\) in \(T\in D_{\vdash}\) is defined as \(T\models_{\vdash}a\stackrel{{\text{def}}}{{\Leftrightarrow}}a \in T\). [4] points out that the following condition is almost always satisfied: (Cond) \(\forall a,b\in F_{\vdash},d\in D_{\vdash}\), we have \((h_{\vdash}(a,d)=h_{\vdash}(b,d))\) and \(d\models_{\vdash}a\Rightarrow d\models_{\vdash}b\). Therefore, the semantical consequence relation induced by \(\models_{\vdash}\) coincides with the original syntactical \(\vdash_{(\mathit{hog}(X),Y)}\) while Cond holds. Due to that \(D_{\vdash}\subseteq D\), it infers that \(\models_{(g(X_{s}),Y_{s})}\) coincides with \(\models_{\vdash}\). Therefore, \(\vdash_{(\mathit{hog}(X),Y)}=\models_{(g(X_{s}),Y_{s})}\) can be safely omitted in the rest of the paper.

## Appendix C Details of proofs

**Proposition C.1** (L-Reg reduces the complexity of the model, promoting data-shift generalization performance.).: _Assume the domain gap is well minimized. Consider a \(f^{*}\) is the target model that generalizes to the unseen with the lowest complexity. There are \(f^{R}_{(X_{s},Y_{s})},f_{(X_{s},Y_{s})}\) trained under the setting of data-shift generalization (i.e., \((X_{s},Y_{s})\) is accessible and \(\mathcal{Y}_{s}=\mathcal{Y}_{u}\)), it has that:_

\[GL(f^{R}_{(X_{s},Y_{s})},f^{*},X_{u})\leq GL(f_{(X_{s},Y_{s})},f^{*},X_{u}),\] (18)

Proof.: We assume the loss is achieved for the tractable form by minimizing the mean squared error. In that case, we have \(f^{*}_{(X_{s},Y_{s})}\) for the given training set as:

\[f^{*}_{(X,Y)}=(g(X)^{T}g(X))^{-1}h\circ g(X)^{T}Y_{s}=(Z^{T}Z)^{-1}h(Z^{T})Y,\] (19)

In comparison to \(f^{*}\), \(f_{(X_{s},Y_{s})}\) for the given seen sets is as:

\[f_{(X_{s},Y_{s})}=(Z^{T}_{s}Z_{s})^{-1}h(Z^{T}_{s})Y_{s},\] (20)

and \(f^{R}_{(X_{s},Y_{s})}\) is derived from \(f_{(X_{s},Y_{s})}\), where \(Z_{s}\) is constrained additionally by L-Reg and the constrained \(Z_{s}\) is denoted as \(Z^{R}_{s}\):

\[f^{R}_{(X_{s},Y_{s})}=(Z^{R\;T}_{s}Z_{s})^{-1}h(Z^{R\;T}_{s})Y_{s}.\] (21)

For simplification, we denote \((Z^{T}Z)^{-1}Z^{T}\), \((Z^{T}_{s}Z_{s})^{-1}Z^{T}_{s}\), and \((Z^{R\;T}_{s}Z_{s})^{-1}Z^{R\;T}_{s}\) as \(\mathcal{N}^{*}\), \(\mathcal{N}\), and \(\mathcal{N}^{R}\), respectively.

**The form of \(\mathcal{N}\).** For multi-domain generalization, the model is tested on the unseen domain, referring that \(X_{u}\) contains some unseen semantics besides the seen: \(Z_{s}\sim\mathcal{Z}_{s},Z_{u}\sim\mathcal{Z}_{u},\mathcal{Z}_{s}\neq\mathcal{ Z}_{u},\mathcal{Z}_{s}\cap\mathcal{Z}_{u}\neq\emptyset\). Considering each dimension of \(Z\) represents a specific semantics, we denote \(\Gamma\) as the dimensions of \(Z\) that contain the seen semantics support in \(X_{s}\) and \(\bar{\Gamma}\) for the unseen, we can decompose \(\mathcal{N}\) as:

\[\mathcal{N}=\begin{bmatrix}\Gamma^{T}\bar{\Gamma}&\Gamma^{T}\bar{\Gamma}\\ \bar{\Gamma}^{T}\Gamma&\bar{\Gamma}^{T}\bar{\Gamma}\end{bmatrix}^{-1}[h(\Gamma )\;h(\bar{\Gamma})]^{T}.\] (22)

**The form of \(\mathcal{N}^{*}\).** Assume \(\Gamma\) already contains semantic support for deducting \(Y\); thus, \(\bar{\Gamma}\) would not affect the deduction of \(Y\). In such case, it has that \(\Gamma^{T}\bar{\Gamma}=\textbf{0}\) and \(\bar{\Gamma}^{T}\Gamma=\textbf{0}\) and \(\bar{\Gamma}^{T}\bar{\Gamma}=\textbf{1}\) where \(\textbf{0},\textbf{1}\)denote zero matrix and identity matrix:

\[\begin{split}&\mathcal{N}^{*}=\begin{bmatrix}\Gamma^{T}\Gamma& \textbf{0}\\ \textbf{0}&\textbf{1}\end{bmatrix}^{-1}[h(\Gamma)\ h(\bar{\Gamma})]^{T}\\ &=\begin{bmatrix}(\Gamma^{T}\Gamma)&\textbf{0}\\ \textbf{0}&\textbf{1}\end{bmatrix}^{-1}[h(\Gamma)\ h(\bar{\Gamma})]^{T}\\ &=\begin{bmatrix}(\Gamma^{T}\Gamma)^{-1}h(\Gamma)\\ h(\bar{\Gamma})\end{bmatrix},\end{split}\] (23)

where we also expect \(h(\bar{\Gamma})=\textbf{0}\) so that \(z_{u}\) does not influence the deduction. We now have \(\mathcal{N}^{*}\):

\[\mathcal{N}^{*}=\begin{bmatrix}\Gamma^{T}\Gamma&\textbf{0}\\ \textbf{0}&\textbf{1}\end{bmatrix}^{-1}[h(\Gamma)\ h(\bar{\Gamma})]^{T},\ \text{s.t.},h(\bar{\Gamma})=\textbf{0}.\] (24)

Note that for \(\mathcal{N}\) in \(f_{(X_{s},Y_{s})}\), \(\Gamma^{T}\bar{\Gamma}\) and \(\bar{\Gamma}^{T}\Gamma\) are not constrained. Please refer to Lemma C.2. Furthermore, \(h(\bar{\Gamma})\) is also not constrained.

**The form of \(\mathcal{N}^{R}\).** Now we discuss the trainable \(\mathcal{N}^{R}\) obtained with the application of L-Reg. The form of \(\mathcal{N}^{R}\) is similar to \(\mathcal{N}^{*}\). However, Eq. (11) indicates that L-Reg minimizes \(||\Gamma^{T}\bar{\Gamma}||_{2}\) and \(||\bar{\Gamma}^{T}\Gamma||_{2}\) through \(-H(Y|g(\bar{\Gamma})),D)\) and also minimizing \(||h(\bar{\Gamma})||_{2}\):

\[\mathcal{N}^{R}=\begin{bmatrix}\Gamma^{T}\bar{\Gamma}&\Gamma^{T}\bar{\Gamma} \bar{\Gamma}\\ \bar{\Gamma}^{T}\Gamma&\bar{\Gamma}^{T}\bar{\Gamma}\end{bmatrix}^{-1}[h( \Gamma)\ h(\bar{\Gamma})]^{T},\ \text{s.t.},\min||\Gamma^{T}\bar{\Gamma}||_{2}+||\bar{\Gamma}^{T}\Gamma||_{2} +|h(\bar{\Gamma})||_{2}.\] (25)

**Compare \(GL(f^{R}_{(X_{s},Y_{s})},f^{*},X_{u})\) with \(GL(f_{(X_{s},Y_{s})},f^{*},X_{u})\).** By comparing the forms of \(\mathcal{N}^{R},\mathcal{N}\) and \(\mathcal{N}^{*}\), it is obvious that \(||\mathcal{N}^{R}-\mathcal{N}^{*}||_{2}\leq||\mathcal{N}-\mathcal{N}^{*}||_{2}\). Therefore, we have that: \(GL(f^{R}_{(X_{s},Y_{s})},f^{*},X_{u})\leq GL(f_{(X_{s},Y_{s})},f^{*},X_{u})\). 

**Lemma C.2** (Minimizing \(H(Y|g(X),D)+\mathcal{R}\) solely may cause generalization degradation).: _Minimizing \(H(Y|g(X),D)+\mathcal{R}\) solely without L-Reg may conflict with \(\max_{h,g}H(Y|g(\bar{\Gamma}),D)\), causing invalid semantics for decision process and degrading the generalization._

Proof.: We have the following relationship for \(H(Y|g(z),D)\):

\[\begin{split} H(Y|g(z),D)=H(Y|g(\bar{\Gamma}),g(\Gamma),D)\\ H(Y,g(\Gamma)|g(\bar{\Gamma}),D)-H(g(\Gamma)|g(\bar{\Gamma}),D)=H(Y|g( \bar{\Gamma}),g(\Gamma),D)+H(g(\bar{\Gamma})|g(\Gamma),D).\end{split}\] (26)

Since the independence between \(\{z_{i}\}_{i=1}^{M}\) is unconstrained, \(H(Y,g(\Gamma)|g(\bar{\Gamma}),D)\) may cause that \(Y\) can be deducted from \(\bar{\Gamma}\). Therefore, \(\Gamma^{T}\bar{\Gamma}\) and \(\bar{\Gamma}^{T}\Gamma\) are not constrained even when the domain gap is minimized where \(|D|=1\), causing the sub-optimal generalization. 

## Appendix D One toy example

We present a simplified informal illustrative example to compare the efficacy of our proposed L-Reg against conventional L1 and L2 regularization methods. As depicted in Fig. 6, the ground truth (GT) image represents the underlying data, generated according to \(f^{*}(x_{1},x_{2})=\sin(2\pi x_{1})\cdot\sin(2\pi x_{2})\), where \(x_{1}\) and \(x_{2}\) denote the horizontal and vertical coordinates respectively, and the pixel color corresponds to the value of \(f^{*}(x_{1},x_{2})\). The training domain is delineated by the black box, while the testing domain encompasses the area outside of this boundary.

For our experiments, we use a 6-linear-layer size-110 ReLU model network. Mean squared error serves as the loss function.

Our experimental results reveal that L-Reg enhances the model's ability to extrapolate beyond the training domain. Notably, our proposed L-Reg demonstrates superior extrapolative capabilities compared to traditional \(L_{1}\) and \(L_{2}\) regularization methods. This observation highlights the efficacy of L-Reg in fostering improved generalization.

## Appendix E Apply L-Reg to ERM Baseline for mDG

To further validate L-Reg's efficacy for mDG, we use ERM as the baseline on the TerraIncognita dataset. For a fair comparison, all experiments share the same hyperparameter settings and use the Regnety-16gf backbone. Original ERM results are also included alongside our reproduced results. The results in Table 8 reveal that ERM with L-Reg significantly improves mDG performance (from 49.9% to 52.9%).

## Appendix F Compare L-Reg with more regularization terms

We also compare L-Reg with other regularization terms: The Ortho-Reg - the orthogonality regularization that constrains the independence of each dimension of the semantic feature \(z\); and Sparsity - implemented as Bernoulli Sample of the latent features from the sparse linear concept discovery models [41] on our used PIM backbone. To investigate this fairly, we re-implemented the Bernoulli Sample of the latent features from the Sparse Linear Concept Discovery Models [41] on the same PIM backbone that we used, to achieve the sparsity. Table 6&Table 7 demonstrate that L-Reg outperforms Ortho-Reg and Sparsity.

Especially, while a common sparse concept model may be able to achieve \(\gamma^{y}\psi=z^{y}\) by filtering irrelevant features through the sparsity, it may not ensure \(\gamma^{y_{i}}\neq\gamma^{y_{j}}\), which is crucial for disentangling features used for predicting different classes. This limitation can potentially lead to degradation in generalization performance for common sparse concept models. 6&7 indicate that while L-Reg consistently achieves overall improvement, the sparse concept-based approach does not consistently improve generalization, validating the aforementioned difference.

## Appendix G Limitation of L-Reg and possible solutions

As analyzed and discussed in the paper, L-Reg is based on the precondition that each dimension of the latent features represents an independent semantic.

We hypothesize this is due to the fact that our L-Reg is derived based on the precondition that \(z^{i},z^{j}\in z,I\neq j\) is independent of each other. This condition holds for most deep-layer features but may not apply to shallow layers. Thus, applying L-Reg to the semantic features from the deep layers may improve the performance for unknown classes without negatively impacting known classes.

Derived from this hypothesis, another possible solution is further regularizing the independence, which may lead to further improvements. To validate this hypothesis, we test L-Reg by reinforcing independence with Ortho-Reg. MDG results in Table 8 and GCD results in Table 6&Table 7 show that combining L-Reg with Ortho-Reg leads to further improvements, whereas Ortho-Reg alone may not guarantee improvements. These findings support our hypothesis and suggest that L-Reg, particularly

Figure 6: Prediction visualizations of MLP with different regularization terms.

when applied to deep layers or in conjunction with Ortho-Reg, is beneficial. This suggests a direction for future work.

## Appendix H More experimental details and results

All experiments can be conducted on one NVIDIA GeForce RTX 3090 GPU.

### Multi-domain generalization

**Competitors.** We listed results from previous important work in the mDG field for better validation. They are: MMD [33], Mixstyle [62], GroupDRO [44], IRM [5], ARM [61], VREx [30], CDANN [35], DANN [20], RSC [24], MTL [8], MLDG [31], Fish [46], ERM [53], SagNet [40], SelfReg [26], CORAL [48], mDSDI [12], MIRO [25], and GMDG [50]. Among them, GMDG is treated as our baseline since it sufficiently minimizes the domain gaps.

**Datasets.** We use PACS (4 domains, 9,991 samples, 7 classes) [32], VLCS (\(4\) domains, \(10,729\) samples, \(5\) classes) [18], OfficeHome (4 domains, 15,588 samples, 65 classes) [55], TerraIncognita

\begin{table}
\begin{tabular}{l|c c c} \hline \hline  & \multicolumn{2}{c}{Avg} \\  & All & Known & Unknown \\ \hline PIM & 67.4 & 79.3 & 59.9 \\ \hline +Sparsity & 66.6 & 77.3 & 60.0 \\ Improvements & -0.7 & -2.0 & 0.1 \\ \hline +Ortho-Reg & 68.4 & 79.2 & 61.9 \\ Improvements & 1.0 & -0.1 & 2.0 \\ \hline **+L-Reg** & 68.8 & 79.0 & 62.7 \\ Improvements & 1.4 & -0.3 & 2.8 \\ \hline **+L-Reg+Ortho-Reg** & **69.3** & **79.6** & **63.4** \\ Improvements & **2.0** & **0.3** & **3.5** \\ \hline \hline \end{tabular}
\end{table}
Table 6: **Results of GCD:** Averaged results across all datasets of PIM with different regularization applied to the latent features: Sparsity: achieved through Bernoulli Sample; Ortho-Reg: orthogonality regularization. +L-Reg outperforms other regularization terms when they are applied solely; +L-Reg+Ortho-Reg achieves the best performance and alleviates the performance degradation of unknown classes, validating our hypothesis in the paper that the improper \(Z\) may result in compromises and constraining the independence of each \(z^{t}\in z,z\in Z\) may be helpful.

\begin{table}
\begin{tabular}{l|c c c|c c c|c c c} \hline \hline  & \multicolumn{4}{c|}{**CUB**} & \multicolumn{4}{c|}{**Stanford Cars**} & \multicolumn{4}{c}{**Herbarium19**} \\  & All & Known & Unknown & All & Known & Unknown & All & Known & Unknown \\ \hline PIM & 62.7 & 75.7 & 56.2 & 43.1 & 66.9 & 31.6 & 42.3 & 56.1 & 34.8 \\ \hline PIM + Sparsity & 60.1 & 72.7 & 53.8 & 40.4 & 61.7 & 30.1 & 42.0 & 53.7 & 35.8 \\ Improvements & -2.6 & -3.0 & -2.4 & -2.7 & -5.2 & -1.5 & -0.3 & -2.4 & 1.0 \\ \hline PIM + Ortho-Reg & 64.9 & 76.7 & 58.9 & 44.3 & 65.6 & 34.1 & 42.9 & 57.2 & 35.1 \\ Improvements & 2.2 & 1.0 & 2.7 & 1.2 & -1.3 & 2.5 & 0.6 & 1.1 & 0.3 \\ \hline
**PIM + L-Reg** & 65.3 & 76.0 & 60.0 & 44.8 & 66.0 & 34.0 & **43.7** & 55.8 & **37.2** \\ Improvements & 2.6 & 0.3 & 3.8 & 1.7 & -0.9 & 3.0 & 1.4 & -0.3 & 2.4 \\ \hline
**PIM + L-Reg + Ortho-Reg** & **66.8** & **77.3** & **61.6** & **45.8** & **67.3** & **35.5** & 43.3 & 57.5 & 35.6 \\ Improvements & 4.1 & 1.6 & 5.4 & 2.7 & 0.4 & 3.9 & 1.0 & 1.4 & 0.8 \\ \hline \hline \end{tabular} 
\begin{tabular}{l|c c c|c c|c c|c c} \hline \hline  & \multicolumn{4}{c|}{**CIFAR10**} & \multicolumn{4}{c}{**CIFAR100**} & \multicolumn{4}{c}{**ImageNet-100**} \\  & All & Known & Unknown & All & Known & Unknown & All & Known & Unknown \\ \hline PIM & 94.7 & 97.4 & 93.3 & 78.3 & 84.2 & 66.5 & 83.1 & **95.3** & 77.0 \\ \hline PIM + Sparsity & 94.2 & 97.4 & 92.6 & 79.7 & **84.6** & 69.7 & 83.4 & 93.7 & 78.2 \\ Improvements & -0.5 & 0.0 & -0.7 & 1.4 & 0.4 & 3.2 & 0.3 & -1.6 & 1.2 \\ \hline PIM + Ortho-Reg & 95.1 & 97.4 & 93.9 & 80.2 & **84.6** & 71.4 & 83.0 & 93.4 & 77.7 \\ Improvements & 0.4 & 0.0 & 0.6 & 1.9 & 0.4 & 4.9 & -0.1 & -1.9 & 0.7 \\ \hline
**PIM + L-Reg** & 94.8 & **97.6** & 93.4 & 80.8 & **84.6** & 73.2 & 83.4 & 94.0 & 78.0 \\ Improvements & 0.1 & 0.2 & 0.1 & 2.5 & 0.4 & 6.7 & 0.3 & -1.3 & 1.0 \\ \hline
**PIM + L-Reg + Ortho-Reg** & **95.1** & **97.6** & **93.9** & **81.2** & 84.2 & **75.0** & **83.7** & 93.6 & **78.7** \\ Improvements & 0.4 & 0.2 & 0.6 & 2.9 & 0.0 & 8.5 & 0.6 & -1.7 & 1.7 \\ \hline \hline \end{tabular}
\end{table}
Table 7: **Results of GCD:** Detailed results across all datasets of PIM with different regularization applied to the latent features: Sparsity: achieved through Bernoulli Sample; Ortho-Reg: orthogonality regularization.

(TerraIncognita, \(4\) domains, \(24,778\) samples, \(10\) classes) [7], and DomainNet (6 domains, 586,575 samples, 345 classes) [42].

**Training details.** We use GMDG [50] as our baseline. Especially, we use all loss terms proposed in GMDG as \(L_{main}\). The training procedure is the same as MIRO [25] and GMDG. We use seeds \(0,1,2\) for all three trails training.

**Parameters.** We adhere to the parameters proposed by GMDG, particularly focusing on its recommended loss terms. Furthermore, we provide a detailed listing of the hyper-parameters pertaining to L-Reg, along with the tuned 'lr mult', as outlined in Table 9, to facilitate the reproducibility of our results.

**Evaluation metric.** The models undergo training on known domains and subsequent testing on unseen domains. For each trial, a distinct domain within the datasets is designated as the unseen domain. The evaluation metric reports the prediction accuracy achieved on these unseen domains. The aggregated results across all unseen domains within the datasets provide a comprehensive assessment of the algorithm's performance in domain generalization for the given datasets.

**More results.** Results of each domain for each dataset are presented in Tables 10 to 14.

### Generalized category discovery

**Competitors.** We compare our proposed method with existing generalized category discovery methods: GCD [54], and PIM [16]. In particular, PIM based on information maximization is the current state-of-the-art (SOTA) generalized category discovery method. Additionally, the traditional machine learning method, k-means [38]; three novel category discovery methods: RankStats+ [22], UNO+ [19], ORCA [13]; and several information maximization methods: RIM [27], and TIM [11] are adapted for generalized category discovery as competitors. The results of the modified novel category discovery methods are reported in [54], and the modified information maximization methods are reported in [16].

**Usage details of datasets for GCD.** Following the protocols of GCD and PIM [54, 16], the initial training set of each dataset is divided into labeled and unlabeled subsets; samples from half of the classes are assigned as unlabeled, and their labels are not used for training. Specifically, half of the image samples from known classes are allocated to the labeled subset, while the remaining half are assigned to the unlabeled subset. Additionally, the unlabeled subset includes all image samples from

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline
**TerraIncognita** & **Location 100** & **Location 38** & **Location 43** & **Location 46** & **Avg \(\pm\) Std.** \\ \hline ERM & 54.3 & 42.5 & 55.6 & 38.8 & 47.8 \\ ERM Reproduced & 50.6 & 49.7 & 58 & 41.2 & 49.9\(\pm\)3.6 \\ +Ortho-Reg & 50.7 & 52.6 & 60.5 & 42.7 & 51.6\(\pm\)2.5 \\ \hline \hline
**+L-Reg** & 52.7 & 51.7 & 61.3 & 45.8 & 52.9\(\pm\)4.2 \\
**+L-Reg+Ortho-Reg** & 61.5 & 48.6 & 60.3 & 44 & 53.6\(\pm\)0.5 \\ \hline \hline \end{tabular}
\end{table}
Table 8: **Results of mDG: Results of using ERM as the baseline. We use the ERM method as the baseline to test L-Reg’s efficacy. Ortho-Reg: orthogonality regularization. This table includes results: (1) The improved performance of L-Reg on ERM baseline. (2) Comparison between L-Reg and the Ortho-Reg on ERM baseline. (3) Using L-Reg and Ortho-Reg together yields further promotion, validating our ‘improper \(z\)’ hypothesis in the Paper limitation part. The used dataset is TerraIncognita. All experiments share the same hyperparameters except the added regularization term. Each group of experiments is run with seeds [0,1,2], and the averaged results for each domain and additionally with the standard deviation (Std) are reported.**

\begin{table}
\begin{tabular}{l c c} \hline \hline Use RegNetY-16GF & lr mult & \(\alpha\) \\ \hline TerraIncognita & 2.5 & 1e-3 \\ OfficeHome & 0.1 & 1e-3 \\ VLCS & 0.1 & 1e-4 \\ PACS & 0.1 & 5e-4 \\ DomainNet & 5.0 & 1e-3 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Parameters for mDG task the novel classes in the original dataset. As a result, the unlabeled subset consists of instances from \(K\) different classes. The detailed statistics of datasets are listed in Table 15.

**Training details.** Consistent with PIM, we utilize latent features extracted by the feature encoder DINO (VIT-B/16) [14] that is pre-trained on ImageNet [17] through self-supervised learning. The losses proposed in PIM are treated as \(L_{main}\). The original PIM freezes the feature extractor during the training, directly using the pre-saved extracted features as the model input. For a fair comparison, we only added one linear layer as \(g\) on the extracted features, which is the minimal modification.

\(L_{2}\) (**weight decay) value searching.** For a more fair comparison, we conduct weight decay value searching to ensure that the weight of \(L_{2}\) is the best. To address this, we devised a methodology for weight decay searching involving the construction of smaller labeled and unlabeled subsets derived solely from the labeled data. To conduct parameter searching, we split the labeled samples to construct a'smaller' sub-labeled and sub-unlabeled set. Specifically, we take \(50\%\) of the samples from known classes as sub-unlabeled samples from unknown classes. Additionally, we take \(25\%\) of the samples from the remaining \(50\%\) of known classes as sub-unlabeled samples from known classes. The remaining samples are treated as sub-labeled samples. Hyper-parameters are then searched on these sub-labeled and sub-unlabeled sets.

**Parameters of L-Reg.** The hyper-parameters of L-Reg values are shown in Table 16.

**Evaluation metric.** Following prior works [54, 16], we use the proposed accuracy metric from [54] of all classes, known classes, and unknown classes for evaluation.

**More results.** The results for each dataset are presented in Table 17. It is evident that L-Reg yields enhanced performance across half of the datasets for both known and unknown classes. On the remaining datasets, while L-Reg may slightly compromise the performance of known classes, it demonstrates significant improvements in the unknown classes, resulting in an overall enhancement in the performance across all classes.

**More ablation results.** Due to the introduction of tuned weight decay and the additional \(g\) component, we have conducted ablation studies to assess their impact. The results are summarized in Table 18. It is observed that the baseline model utilizing the tuned weight decays performs slightly better than the original weight decay settings. Notably, the tuned weight decays contribute to improvements in unknown classes while often leading to slight decreases in known classes across most datasets. Inclusion of the proposed extra component \(g\) results in marginal improvements in both known and unknown classes compared to the tuned baseline. Our proposed L-Reg demonstrates significant improvements specifically in the unknown classes, thereby corroborating Proposition 4.2. However, as discussed in the main paper, it is acknowledged that L-Reg may entail compromises in the performance of known classes.

### Combination of multi-domain generalization and generalized category discovery

**Datasets.** We leverage the datasets utilized in mDG tasks to construct the mDG+GCD datasets. Specifically, during the seen domains of training, labels from approximately half of the classes are masked. For instance, in the PCAS dataset comprising \(7\) classes, classes labeled within the range \([0,1,2,3]\) are retained, while classes in \([4,5,6]\) are masked. It is noteworthy that data categorized as unknown classes in our setup are from unknown classes. However, we acknowledge that this prior is not explicitly known. **To align with the GCD setting, we operate under the assumption that the unlabeled set may potentially include samples from known classes.** Consequently, we refrain from constraining the model by mandating that unlabeled data be classified solely as unknown classes. This adjustment introduces a more challenging generalization scenario.

**Training details.** For all experiments, the implementation directly adds L-Reg to their previously proposed loss sets. The models are trained with the aforementioned labeled and unlabeled sets from the seen domains and tested on the samples from the unseen domain.

**Parameters.** We include all the parameters for reproducing our experiments in the code. Please refer to the code for details.

**Evaluation metric.** We use the same metric from the GCD task for the mDG+GCD task. Similarly, the metrics include the accuracy for all, known and unknown classes.

**More results.** The averaged results of each dataset are exhibited in Table 19, while the detailed results of each dataset are presented in Tables 20 to 24.

### More GradCAM visualizations

We provide more visualized examples of L-Reg. Examples of known classes can be seen in Figs. 7 to 10 and unknown classes in Figs. 11 and 12. Compromises in known sets, as discussed in the limitations, can be seen in Figs. 8 and 12.

\begin{table}
\begin{tabular}{l|c c c c|c} \hline \hline
**PACS** & **art\_painting** & **cartoon** & **photo** & **sketch** & **Avg.** \\ \hline ERM [21] & 84.7 & 80.8 & 97.2 & 79.3 & 84.2 \\ MIRO [25] (use ResNet-50) & - & - & - & - & 85.4\(\pm\)0.4 \\ GMDG [50] (use ResNet-50) & 84.7\(\pm\)1.0 & 81.7\(\pm\)2.4 & 97.5\(\pm\)0.4 & 80.5\(\pm\)1.8 & 85.6\(\pm\)0.3 \\ \hline MIRO [25] (use ResNetY-16GF) & - & - & - & - & 97.4\(\pm\)0.2 \\ GMDG [50] (use ResNetY-16GF) & 97.5\(\pm\)1.0 & 97.0\(\pm\)0.2 & **99.4\(\pm\)**0.2 & 95.2\(\pm\)0.4 & 97.3\(\pm\)0.1 \\ \hline GMDG + **L-Reg** (use ResNetY-16GF) & **97.6\(\pm\)**0.8 & **97.1\(\pm\)**0.3 & 99.3\(\pm\)0.2 & **95.3\(\pm\)**0.9 & **97.4\(\pm\)**0.2 \\ \hline \hline \end{tabular}
\end{table}
Table 14: MDG experiments on DomainNet: More results of full GMDG+L-Reg for each category.

\begin{table}
\begin{tabular}{l|c c c c|c} \hline \hline
**PACS** & **art\_painting** & **cartoon** & **photo** & **sketch** & **Avg.** \\ \hline ERM [21] & 84.7 & 80.8 & 97.2 & 79.3 & 84.2 \\ MIRO [25] (use ResNet-50) & - & - & - & - & 85.4\(\pm\)0.4 \\ GMDG [50] (use ResNet-50) & 84.7\(\pm\)1.0 & 81.7\(\pm\)2.4 & 97.5\(\pm\)0.4 & 80.5\(\pm\)1.8 & 85.6\(\pm\)0.3 \\ \hline MIRO [25] (use ResNetY-16GF) & - & - & - & - & 97.4\(\pm\)0.2 \\ GMDG [50] (use ResNetY-16GF) & 97.5\(\pm\)1.0 & 97.0\(\pm\)0.2 & **99.4\(\pm\)**0.2 & 95.2\(\pm\)0.4 & 97.3\(\pm\)0.1 \\ \hline GMDG + **L-Reg** (use ResNetY-16GF) & **97.6\(\pm\)**0.8 & **97.1\(\pm\)**0.3 & 99.3\(\pm\)0.2 & **95.3\(\pm\)**0.9 & **97.4\(\pm\)**0.2 \\ \hline \hline \end{tabular}
\end{table}
Table 13: MDG experiments on PACS: More results of full GMDG+L-Reg for each category.

\begin{table}
\begin{tabular}{l|c c c c|c} \hline \hline
**PACS** & **art\_painting** & **cartoon** & **photo** & **sketch** & **Avg.** \\ \hline ERM [21] & 84.7 & 80.8 & 97.2 & 79.3 & 84.2 \\ MIRO [25] (use ResNet-50) & - & - & - & - & 85.4\(\pm\)0.4 \\ GMDG [50] (use ResNet-50) & 84.7\(\pm\)1.0 & 81.7\(\pm\)2.4 & 97.5\(\pm\)0.4 & 80.5\(\pm\)1.8 & 85.6\(\pm\)0.3 \\ \hline MIRO [25] (use ResNetY-16GF) & - & - & - & - & 97.4\(\pm\)0.2 \\ GMDG [50] (use ResNetY-16GF) & 97.5\(\pm\)1.0 & 97.0\(\pm\)0.2 & **99.4\(\pm\)**0.2 & 95.2\(\pm\)0.4 & 97.3\(\pm\)0.1 \\ \hline GMDG + **L-Reg** (use ResNetY-16GF) & **97.6\(\pm\)**0.8 & **97.1\(\pm\)**0.3 & 99.3\(\pm\)0.2 & **95.3\(\pm\)**0.9 & **97.4\(\pm\)**0.2 \\ \hline \hline \end{tabular}
\end{table}
Table 13: MDG experiments on PACS: More results of full GMDG+L-Reg for each category.

\begin{table}
\begin{tabular}{l|c c c c|c} \hline \hline
**PercaIncapita** & **Location 100** & **Location 38** & **Location 43** & **Location 46** & **Avg.** \\ \hline ERM [21] & 54.3 & 42.5 & 55.6 & 38.8 & 47.8 \\ MIRO [25] (use ResNet-50) & - & - & - & - & 50.4 \\ GMDG [50] (use ResNet-50) & 59.8\(\pm\)1.0 & 45.3\(\pm\)1.7 & 57.1\(\pm\)1.8 & 38.2\(\pm\)5 & 50.1\(\pm\)1.2 \\ \hline MIRO [25] (use ResNetY-16GF) & - & - & - & - & 58.9\(\pm\)1.3 \\ GMDG [50] (use ResNet-50) & 73.3\(\pm\)3.3 & 54.7\(\pm\)1.4 & 67.1\(\pm\)0.3 & 48.6\(\pm\)6.5 & 60.7\(\pm\)1.8 \\ \hline GMDG + **L-Reg** (use ResNetY-16GF) & **73.9\(\pm\)**0.8 & **57.1\(\pm\)**2.3 & **67.9\(\pm\)**1.1 & **52.7\(\pm\)**4.0 & **62.9\(\pm\)**0.9 \\ \hline \hline \end{tabular}
\end{table}
Table 10: MDG experiments on TerraIncognita: More results of full GMDG+L-Reg for each category.

\begin{table}
\begin{tabular}{l|c c c c|c} \hline \hline
**PACS** & **caltech101** & **labelme** & **sun09** & **voc2007** & **Avg.** \\ \hline ERM [21] & 97.7 & 64.3 & 73.4 & 74.6 & 77.3 \\ MIRO [25] (use ResNet-50) & - & - & - & - & 79.0\(\pm\)0.0 \\ GMDG [50] (use ResNet-50) & 98.3\(\pm\)0.4 & 65.9\(\pm\)1 & 73.4\(\pm\)0.8 & 79.3\(\pm\)1.3 & 79.2\(\pm\)0.3 \\ \hline MIRO [25] (use ResNetY-16GF) & - & - & - & - & 79.9\(\pm\)0.6 \\ GMDG [50] (use ResNetY-16GF) & 97.9\(\pm\)1.3 & 66.8\(\pm\)2.1 & **80.8\(\pm\)**1 & **83.9\(\pm\)**1.8 & 82.4\(\pm\)0.6 \\ \hline GMDG + **L-Reg** (use ResNetY-16GF) & **98.6\(\pm\)**0.1 & **67.1\(\pm\)**0.1 & 80.7\(\pm\)0.7 & 83.0\(\pm\)0.8 & **82.4\(\pm\)**0.0 \\ \hline \hline \end{tabular}
\end{table}
Table 12: MDG experiments on VLCS: More results of full GMDG+L-Reg for each category.

\begin{table}
\begin{tabular}{l|c c c c c c c c} \hline \hline  & & **CUB** & & \multicolumn{3}{c|}{**Stanford Cars**} & \multicolumn{3}{c}{**Herbarium19**} \\ \hline Approach & All & Known & Unknown & All & Known & Unknown & All & Known & Unknown \\ \hline K-means & 34.3 & 38.9 & 32.1 & 12.8 & 10.6 & 13.8 & 12.9 & 12.9 & 12.8 \\ RankStats+ [22] (TPAMI-21) & 33.3 & 51.6 & 24.2 & 28.3 & 61.8 & 12.1 & 27.9 & 55.8 & 12.8 \\ UNO+ [19] (ICCV-21) & 35.1 & 49.0 & 28.1 & 35.5 & **70.5** & 18.6 & 28.3 & **53.7** & 14.7 \\ ORCA [13] (ICLR-22) & 27.5 & 20.1 & 31.1 & 15.9 & 17.1 & 15.3 & 22.9 & 25.9 & 21.3 \\ ORCA [13] (VPIB & 38.0 & 45.6 & 31.8 & 33.8 & 52.5 & 25.1 & 25.0 & 30.6 & 19.8 \\ GCD [54] (CVPR-22) & **51.3** & **56.6** & **48.7** & **39.0** & 57.6 & **29.9** & **35.4** & 51.0 & **27.0** \\ \hline \multicolumn{8}{c|}{InfInfMax based methods} \\ \hline RIM [27] (NeurIPS-10) & 52.3 & 51.8 & 52.5 & 38.9 & 57.3 & 30.1 & 40.1 & **57.6** & 30.7 \\ TIM [11] (NeurIPS-20) & 53.4 & 51.8 & 54.2 & 39.3 & 56.8 & 30.8 & 40.1 & 57.4 & 30.7 \\ PIM [16] (ICCV-23) & 62.7 & **75.7** & 56.2 & 43.1 & **66.9** & 31.6 & 42.3 & 56.1 & 34.8 \\
**PIM + L-Reg (Ours)** & **65.3**\({}^{\text{rd}}\) & **76.0**\({}^{\text{rd}}\) & **60.0**\({}^{\text{rd}}\) & **48.1**\({}^{\text{rd}}\) & **66.0**\({}^{\text{rd}}\) & **34.6**\({}^{\text{rd}}\) & **43.7**\({}^{\text{rd}}\) & **55.8**\({}^{\text{rd}}\) & **37.2**\({}^{\text{rd}}\) \\ \hline \hline \multicolumn{8}{c|}{InfInfMax based methods} \\ \hline Approach & All & Known & Unknown & All & Known & Unknown & All & Known & Unknown \\ \hline K-means & 83.6 & 85.7 & 82.5 & 52.0 & 52.2 & 50.8 & 72.7 & 75.5 & 71.3 \\ RankStats [22] (TPAMI-21) & 46.8 & 19.2 & 60.5 & 58.2 & **77.6** & 19.3 & 37.1 & 61.6 & 24.8 \\ UNO+ [19] (ICCV-21) & 68.6 & **98.3** & 53.8 & 69.5 & 80.6 & 47.2 & 70.3 & **95.0** & 57.9 \\ ORCA [13] (ICLR-22) & 88.9 & 88.2 & 89.2 & 55.1 & 65.5 & 34.4 & 67.6 & 90.9 & 56.0 \\ ORCA [13] - VITB16 & **97.1** & 96.2 & **97.6** & 69.6 & 76.4 & 56.1 & **76.5** & 92.2 & **68.9** \\ GCD [54] (CVPR-22) & 91.5 & 97.9 & 88.2 & **70.8** & **77.6** & **57.0** & 74.1 & 89.8 & 66.3 \\ \hline \multicolumn{8}{c|}{InfInfMax based methods} \\ \hline RIM [27] (NeurIPS-10) & 92.4 & **98.1** & 89.5 & 73.8 & 78.9 & 63.4 & 74.4 & 91.2 & 66.0 \\ TIM [11] (NeurIPS-20) & 93.1 & 98.0 & 90.6 & 73.4 & 78.3 & 63.4 & 76.7 & 93.1 & 68.4 \\ \hline PIM [16] (ICCV-23) & 94.7 & 97.4 & 93.3 & 78.3 & 84.2 & 66.5 & 83.1 & **95.3** & 77.0 \\
**PIM + L-Reg (Ours)** & **94.8**\({}^{\text{rd}}\) & **97.6**\({}^{\text{rd}}\) & **93.4**\({}^{\text{rd}}\) & **80.8**\({}^{\text{rd}}\) & **84.6**\({}^{\text{rd}}\) & **73.2**\({}^{\text{rd}}\) & **83.4**\({}^{\text{rd}}\) & 94.0**\({}^{\text{rd}}\) & **78.0**\({}^{\text{rd}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 18: GCD results: Accuracy scores across fine-grained and generic datasets of each setting. The best results are highlighted in **bold**. To eliminate the impact of hyper-parameters on performance, we also present the results of PIM with tuned hyper-parameters (termed baseline tuned). \(L_{main}\) denotes the losses used in PIM. \(g\) denotes the transformation applied to the input features.

\begin{table}
\begin{tabular}{l|c c c c c c} \hline \hline  & **CUB** & **Stanford Cars** & **Herbarium19** \\ \hline Known classes & 100 & 98 & 341 & 5 & 80 & 50 \\ Seen data & 1.5K & 2.0K & 8.9K & 12.5K & 20K & 31.9K \\ \hline All classes & 200 & 196 & 683 & 10 & 100 & 100 \\ Unseen data & 4.5K & 6.1K & 25.4K & 37.5K & 30K & 95.3K \\ \hline \hline \end{tabular}
\end{table}
Table 15: Statistics of datasets.

\begin{table}
\begin{tabular}{l l|c c c c c c c} \hline \hline  & & **CUB** & & \multicolumn{3}{c|}{**Stanford Cars**} & \multicolumn{3}{c}{**Herbarium19**} \\ \hline ID & Settings & All & Known & Unknown & All & Known & Unknown & All & Known & Unknown \\ \hline
1 & Baseline (\(L_{main}\)) & 62.7 & 75.7 & 56.2 & 43.1 & 66.9 & 31.6 & 42.3 & 56.1 & 34.8 \\ \hline
2 & Baseline tuned (\(L_{main}\)) & 64.8 & 75.1 & 59.6 & 42.6 & 59.3 & 34.6 & 43.1 & 57.6 & 35.4 \\
6 & \(L_{main}\) + 9 & 64.9 & **76.7** & 58.9 & 44.7 & 65.8 & 34.6 & 43.0 & **57.4** & 35.2 \\
9 & **Ours (\(L_{main}\)+\(\pi\)+\(L_{-Reg}\))** & **65.3** & 76.0 & **60.0** & **44.8** & **66.0** & **34.6** & **34.7** & 55.8 & **37.2** \\ \hline \hline \multicolumn{8}{c|}{**CIFAR10**} & \multicolumn{3}{c|}{**CIFAR100**} & \multicolumn{3}{c}{**ImageNet-100**} \\ \hline ID & Settings & All & Known & Unknown & All & Known & Unknown & All & Known & Unknown \\ \hline
1 & Baseline (\(L_{main}\)) & 94.7 & 97.4 & 93.3 & 78.3 & 84.2 & 66.5 & 83.1 & **95.3** & 77.0 \\ \hline
2 & Baseline tuned (\(L_{main}\)) & 95.0 & 96.1 & 94.4 & 80.3 & 84.6 & 71.8 & 83.5 & 95.0 & 77.7 \\
6 & \(L_{main}\)+ 9 & 94.7 & 97.5 & 93.3 & 80.8 & 84.6 & 73.1 & 83.1 & 95.0 & 77.1 \\
9 & **Ours (\(L_{main}\)+\(\pi\)+\(L_{-Reg}\))** & **94.8** & **97.6** & **93.4** & **80.8** & **84.6** & **73.2** & **83.4** & 94.0 & **78.0** \\ \hline \hline \end{tabular}
\end{table}
Table 16: GCD task: Tuned weight decay values for each dataset.

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{
\begin{tabular}{c} PromOffice \\ Method \\ All \\ \end{tabular} } & \multicolumn{2}{c}{**Avg**} & \multicolumn{2}{c}{**Art**} & \multicolumn{2}{c}{**Cibert**} & \multicolumn{2}{c}{**Predicted**} & \multicolumn{2}{c}{**Real World**} \\  & All & Known & Unknown & All & Known & Unknown & All & Known & Unknown & All & Known & Unknown & All & Known & Unknown \\ \hline EBM & 43.80 & 76.77 & 8.50 & 43.50 & 44.36 & 6.72 & 47.88 & 35.79 & 64.48 & 10.93 & 46.71 & 39.77 & 46.28 & 32.80 & 10.68 \\ with our reg & 43.56 & 71.78 & 9.68 & 41.30 & 62.72 & 8.47 & 35.91 & 60.78 & 9.90 & 48.34 & 78.95 & 13.14 & 48.68 & 34.67 & 7.22 \\ Improvements & 1.24 & 2.99 & 1.88 & 3.56 & 9.96 & 5.22 & 1.03 & -3.09 & -3.02 & 1.63 & 2.11 & 3.21 & 4.00 & 1.88 & 3.46 \\ \hline PJM & 43.42 & 72.44 & 8.11 & 12.53 & 45.38 & 60.99 & 3.39 & 35.77 & 56.55 & 13.83 & 47.27 & 77.35 & 12.41 & 44.81 & 87.35 & 2.90 \\ with our reg & 44.20 & 71.75 & 10.85 & 44.64 & 68.58 & 7.56 & 3.54 & 48.09 & 89.00 & 4.99 & 74.29 & 13.45 & 30.17 & 89.02 & 12.59 \\ Improvements & 9.78 & 4.70 & 2.70 & 2.21 & 0.77 & 4.17 & 4.37 & 4.29 & 4.52 & 0.23 & 1.26 & 19.40 & 1.66 & 4.63 & 9.69 \\ \hline MDG & 43.82 & 80.61 & 9.70 & 50.57 & 9.57 & 63.13 & 95.57 & 67.23 & 10.60 & 51.35 & 86.16 & 11.32 & 51.56 & 89.50 & 8.09 & 8.09 \\ with our reg & 33.59 & 79.90 & 22.21 & 34.02 & 77.77 & 14.47 & 33.70 & 98.52 & 99.94 & 38.32 & 35.24 & 35.64 & 85.21 & 32.52 \\ Improvements & 5.31 & -1.11 & 11.88 & 3.45 & -1.70 & 11.34 & 4.22 & 3.75 & 4.92 & 8.99 & -2.21 & 21.00 & 4.88 & -4.29 & 15.43 \\ \hline MDG & 43.85 & 81.41 & 9.22 & 80.61 & 81.96 & 5.50 & 40.89 & 69.93 & 11.19 & 51.58 & 87.33 & 9.21 & 57.56 & 86.71 & 11.30 \\ with our reg & 51.96 & 79.74 & 18.15 & 32.83 & 79.15 & 12.52 & 43.59 & 60.92 & 16.99 & 56.31 & 83.11 & 25.48 & 55.11 & 87.67 & 17.59 \\ Improvements & 3.11 & -1.68 & 89.2 & 12.4 & -2.81 & 7.43 & 2.69 & -4.28 & 5.30 & 5.15 & -4.42 & 16.16 & 3.36 & 0.80 & 6.30 \\ \hline \hline \end{tabular}
\end{table}
Table 22: MDG+GCD results: accuracy scores of each domain in UCLS dataset.

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{
\begin{tabular}{c} PromOffice \\ Method \\ All \\ \end{tabular} } & \multicolumn{2}{c}{**Avg**} & \multicolumn{2}{c}{**Att**} & \multicolumn{2}{c}{**Att**} & \multicolumn{2}{c}{**Cibert**} & \multicolumn{2}{c}{**Predicted**} & \multicolumn{2}{c}{**Real World**} \\  & All & Known & Unknown & All & Known & Unknown & All & Known & Unknown & All & Known & Unknown & All & Known & Unknown \\ \hline EBM & 43.80 & 76.77 & 8.50 & 43.50 & 44.36 & 6.70 & 5.38 & 46.48 & 10.93 & 46.71 & 46.88 & 10.93 & 46.71 & 39.77 & 46.28 & 32.80 & 10.68 \\ with our reg & 43.56 & 71.78 & 9.68 & 41.30 & 62.72 & 8.47 & 35.91 & 60.78 & 9.90 & 48.34 & 78.95 & 13.14 & 48.68 & 34.67 & 7.22 \\ Improvements & 1.24 & 2.99 & 1.88 & 3.56 & 9.96 & 5.22 & 1.03 & -3.09 & -3.02 & 1.63 & 2.11 & 3.21 & 4.57 & -6.00 & 1.88 & -3.46 \\ \hline PM & 43.42 & 72.44 & 8.11 & 12.53 & 45.38 & 60.99 & 3.39 & 35.77 & 56.55 & 13.83 & 47.27 & 77.35 & 12.41 & 44.81 & 87.35 & 2.90 \\ with our reg & 44.20 & 71.75 & 10.85 & 44.64 & 68.58 & 7.56 & 3.48 & 60.90 & 5.49 & 7.49 & 76.32 & 14.35 & 40.17 & 89.02 & 12.59 \\ Improvements & 9.78 & 4.70 & 2.27 & 2.11 & 0.77 & 4.17 & 4.37 & 4.29 & 4.52 & 0.23 & 1.26 & 19.40 & 1.76 & -6.43 & 9.69 \\ \hline MDG & 43.82 & 80.61 & 9.73 & 50.57 & 79.57 & 6.13 & 95.57 & 67.23 & 10.60 & 51.35 & 86.16 & 11.32 & 51.56 & 89.90 & 5.09 & 8.09 \\ with our reg & 43.39 & 79.50 & 2.21 & 34.02 & 77.97 & 11.47 & 4.37 & 30.98 & 15.52 & 9.94 & 89.35 & 13.22 & 35.64 & 85.81 & 23.52 \\ Improvements & 5.31 & -1.11 & 11.88 & 3.45 & -1.70 & 11.34 & 4.22 & 3.75 & 4.92 & 8.99 & -2.21 & 21.00 & 4.88 & -4.29 & 15.43 \\ \hline MDG & 43.85 & 81.41 & 9.22 & 80.61 & 81.96 & 5.50 & 40.89 & 69.93 & 11.19 & 51.58 & 87.33 & 9.21 & 57.57 & 86.77 & 11.39 \\ with our reg & 51.96 & 79.74 & 18.15 & 32.83 & 79.15 & 12.52 & 43.59 & 60.92 & 16.99 & 56.31 & 83.11 & 25.48 & 55.11 & 87.67 & 17.59 \\ Improvements & 3.11 & -1.68 & 89.2 & 12.4 & -2.81 & 7.43 & 2.69 & -4.28 & 5.30 & 5.15 & -4.42 & 16.16 & 3.36 & 0.80 & 6.30 \\ \hline \hline \end{tabular}
\end{table}
Table 19: MDG+GCD results: accuracy scores of each dataset. Improvements are highlighted in red.

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{
\begin{tabular}{c} PromResolution \\ Method \\ All \\ \end{tabular} } & \multicolumn{2}{c}{**Avg**} &

\begin{table}
\begin{tabular}{l|c c c|c c c|c c c|c c c} \hline DomainNet & \multicolumn{3}{c|}{**Avg**} & \multicolumn{3}{c|}{**cipart**} & \multicolumn{3}{c|}{**info**} & \multicolumn{3}{c}{**painting**} \\ Method & All & Known & Unknown & All & Known & Unknown & All & Known & Unknown & All & Known \\ \hline ERM & 22.56 & 40.89 & 6.85 & 31.04 & 58.32 & 7.15 & 17.94 & 34.71 & 6.85 & 30.59 & 51.82 & 9.34 \\ with our reg & 25.86 & 47.07 & 7.19 & 32.03 & 58.43 & 8.91 & 18.17 & 34.31 & 7.50 & 31.93 & 52.58 & 11.24 \\ Improvements & 3.31 & 6.18 & 0.34 & 0.99 & 0.11 & 1.76 & 0.23 & -0.41 & 0.65 & 1.33 & 0.76 & 1.90 \\ \hline PIM & 24.03 & 42.59 & 7.86 & 32.01 & 57.38 & 9.80 & 18.80 & 33.56 & 9.03 & 22.22 & 36.62 & 7.80 \\ with our reg & 24.66 & 43.86 & 7.78 & 31.91 & 57.76 & 9.26 & 16.99 & 30.77 & 7.89 & 28.17 & 45.94 & 10.37 \\ Improvements & 0.63 & 1.27 & -0.08 & -0.11 & 0.38 & -0.54 & -1.80 & -2.79 & -1.15 & 5.95 & 9.32 & 2.57 \\ \hline MIRO & 31.49 & 55.44 & 10.57 & 40.13 & 67.55 & 16.11 & 25.84 & 48.53 & 10.84 & 37.89 & 62.45 & 13.29 \\ with our reg & 31.58 & 54.97 & 10.98 & 40.61 & 66.72 & 17.75 & 25.58 & 45.83 & 12.19 & 36.74 & 62.29 & 11.15 \\ Improvements & 0.10 & -0.47 & 0.41 & 0.49 & -0.83 & 1.64 & -0.26 & -2.70 & 1.35 & -1.15 & -0.16 & -2.14 \\ \hline GMDG & 31.15 & 55.17 & 10.18 & 40.38 & 70.69 & 13.84 & 24.96 & 46.50 & 10.72 & 36.29 & 59.80 & 12.75 \\ with our reg & 31.75 & 55.18 & 11.30 & 40.91 & 68.17 & 17.05 & 26.60 & 49.11 & 11.71 & 36.82 & 60.76 & 12.85 \\ Improvements & 0.60 & 0.01 & 1.13 & 0.53 & -2.52 & 3.21 & 1.63 & 2.61 & 0.99 & 0.53 & 0.96 & 0.10 \\ \hline DomainNet & \multicolumn{3}{c|}{**Avg**} & \multicolumn{3}{c|}{**quickdraw**} & \multicolumn{3}{c|}{**real**} & \multicolumn{3}{c}{**sketch**} \\ Method & All & Known & Unknown & All & Known & Unknown & All & Known & Unknown & All & Known & Unknown \\ \hline ERM & - & - & - & 8.88 & 12.83 & 4.91 & 17.88 & 31.20 & 4.10 & 29.01 & 56.45 & 8.76 \\ with our reg & - & - & - & 9.04 & 14.73 & 3.31 & 34.34 & 63.94 & 3.69 & 29.68 & 58.41 & 8.49 \\ Improvements & - & - & - & 0.16 & 1.91 & -1.59 & 16.45 & 32.74 & -0.41 & 0.67 & 1.96 & -0.27 \\ \hline PIM & - & - & - & 9.92 & 14.73 & 5.09 & 29.09 & 53.88 & 3.42 & 32.12 & 59.35 & 12.03 \\ with our reg & - & - & - & 9.94 & 15.11 & 4.74 & 30.26 & 56.13 & 3.47 & 30.68 & 57.43 & 10.95 \\ Improvements & - & - & - & 0.02 & 0.38 & -0.35 & 1.17 & 2.25 & 0.05 & -1.44 & -1.93 & -1.08 \\ \hline MIRO & - & - & - & 8.06 & 12.12 & 3.98 & 42.19 & 75.49 & 7.72 & 34.83 & 66.51 & 11.46 \\ with our reg & - & - & - & 9.36 & 15.73 & 2.95 & 42.00 & 74.36 & 8.50 & 35.23 & 64.89 & 13.34 \\ Improvements & - & - & - & 1.30 & 3.61 & -1.03 & -0.20 & -1.14 & 0.78 & 0.40 & -1.62 & 1.88 \\ \hline GMDG & - & - & - & 7.43 & 11.83 & 3.01 & 42.84 & 75.27 & 9.27 & 35.01 & 66.95 & 11.46 \\ with our reg & - & - & - & 9.11 & 13.51 & 4.70 & 42.63 & 74.42 & 9.72 & 34.44 & 65.13 & 11.80 \\ Improvements & - & - & - & 1.68 & 1.67 & 1.69 & -0.21 & -0.84 & 0.45 & -0.58 & -1.81 & 0.34 \\ \hline \end{tabular}
\end{table}
Table 24: MDG+GCD results: accuracy scores of each domain in DomainNet dataset.

Figure 7: GradCAM visualizations: Baseline is GMDG. The used dataset is PACS. The model is trained under uDG+GCD setting with and without L-Reg, respectively. It can be seen that for the **known** class ‘dog,’ the model trained with L-Reg extracts the area around the nose area for classification across all seen and unseen domains.

Figure 8: GradCAM visualizations: Baseline is GMDG. The used dataset is PACS. The model is trained under uDG+GCD setting with and without L-Reg, respectively. It can be seen that for the **known** class ‘elephant,’ the model trained with L-Reg extracts the shape of long noses, teeth, and big ears for classification across all seen and unseen domains. The compromise of the known sets can be seen in the sketch domain, where those features are not significant.

Figure 9: GradCAM visualizations: Baseline is GMDG. The used dataset is PACS. The model is trained under uDG+GCD setting with and without L-Reg, respectively. It can be seen that for the **known** class ‘giraffe,’ the model trained with L-Reg extracts the feature of the long necks for classifying across all seen and unseen domains.

Figure 10: GradCAM visualizations: Baseline is GMDG. The used dataset is PACS. The model is trained under uDG+GCD setting with and without L-Reg, respectively. It can be seen that for the **known** class ‘guitar,’ the model trained with L-Reg extracts the features of the necks and the strings of the guitar for classification across all seen and unseen domains.

Figure 11: GradCAM visualizations: Baseline is GMDG. The used dataset is PACS. The model is trained under uDG+GCD setting with and without L-Reg, respectively. It can be seen that for the **unknown** class ‘horse,’ the model trained with L-Reg extracts the features of the overall outline shapes of horses for classification across all seen and unseen domains.

Figure 12: GradCAM visualizations: Baseline is GMDG. The used dataset is PACS. The model is trained under uDG+GCD setting with and without L-Reg, respectively. It can be seen that for the **unknown** class ‘person,’ the model trained with L-Reg extracts the features of human faces for classification across all seen and unseen domains. The compromise of the known sets can be seen in the sketch domain, where those faces are not drawn.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Yes, the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Yes, the paper discusses the limitations of the work in the Conclusion Section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Yes, the paper provides the full set of assumptions and a complete proof in the main paper and appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: This paper provides all the information needed to reproduce the main experimental results of the paper as much as possible in the appendix and supplementary materials. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The paper provides uses open-source data and open access to the code. The code is especially attached as supplementary material. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Yes, this paper specifies all the training and test details in the Experiments Section and the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Following previous work, this paper does not report error bars. However, this paper reports some results in the form of mean with standard deviation, as shown in Table 1. However, Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The paper provides information on the computer resources in the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Yes, the research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: We think that there is no societal impact of the work performed. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper seems to pose no such risks Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All assets (e.g., code, data, models) used in the paper are properly credited, and the license and terms of use are explicitly mentioned and properly respected. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: New assets introduced in the paper are documented, and the documentation is provided alongside the assets. The model training code is provided in the supplementary materials. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.