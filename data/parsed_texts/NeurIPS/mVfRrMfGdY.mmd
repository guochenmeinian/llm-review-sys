# Unified Mechanism-Specific Amplification by Subsampling and Group Privacy Amplification

Jan Schuchardt1, Mihail Stoian2, Arthur Kosmala1, Stephan Gunnemann1

{j.schuchardt, a.kosmala, s.guennemann}@tum.de, mihail.stoian@utn.de

1Dept. of Computer Science & Munich Data Science Institute, Technical University of Munich

2Dept. of Engineering, University of Technology Nuremberg

###### Abstract

Amplification by subsampling is one of the main primitives in machine learning with differential privacy (DP): Training a model on random batches instead of complete datasets results in stronger privacy. This is traditionally formalized via _mechanism-agnostic_ subsampling guarantees that express the privacy parameters of a subsampled mechanism as a function of the original mechanism's privacy parameters. We propose the first general framework for deriving _mechanism-specific_ guarantees, which leverage additional information beyond these parameters to more tightly characterize the subsampled mechanism's privacy. Such guarantees are of particular importance for _privacy accounting_, i.e., tracking privacy over multiple iterations. Overall, our framework based on conditional optimal transport lets us derive existing and novel guarantees for approximate DP, accounting with Renyi DP, and accounting with dominating pairs in a unified, principled manner. As an application, we analyze how subsampling affects the privacy of groups of multiple users. Our tight mechanism-specific bounds outperform tight mechanism-agnostic bounds and classic group privacy results.

## 1 Introduction

Composability and amplification by subsampling are two key properties of Differential Privacy (DP) [1, 2, 3] that make it possible to provide provable privacy guarantees for machine learning algorithms that iteratively learn from batches of data.

Composability means that privacy gracefully deteriorates when iteratively applying a differentially private mechanism to a dataset [2]. Kairouz et al. [4], Murtagh and Vadhan [5] ultimately derived tight composition theorems that optimally characterize the privacy parameters \((\varepsilon^{\prime},\delta^{\prime})\) of a composed mechanism as a function of the component mechanisms' parameters \((\varepsilon,\delta)\). However, later work demonstrated that these guarantees are only tight in a _mechanism-agnostic_ sense [6]. Stronger _mechanism-specific_ composition guarantees can often be obtained by using additional information beyond the fact that the mechanisms satisfies some notion of DP. Methods for tracking privacy over multiple iterations (e.g. [6, 7, 8, 9, 10, 11, 12]) are referererd to as _privacy accountants_.

Amplification by subsampling means that privacy can be strenghtened by applying a differentially private mechanism to randomly sampled batches of a dataset [13, 14]. Similar to Kairouz et al. [4], Balle et al. [15] ultimately proposed a framework for deriving tight subsampling theorems that optimally characterize the privacy parameters \((\varepsilon^{\prime},\delta^{\prime})\) of a subsampled mechanism as a function of the underlying _base mechanism_'s parameters \((\varepsilon,\delta)\).

However, their framework for deriving subsampling theorems has two key limitations. Firstly, as we shall demonstrate, the resultant guarantees are generally only tight in a mechanism-agnosticsense: Given \((\varepsilon,\delta)\), one can construct _some_ worst-case \((\varepsilon,\delta)\)-DP mechanism that is \((\varepsilon^{\prime},\delta^{\prime})\)-DP under subsampling. The specific mechanism at hand may however be significantly more private. Secondly, their framework does not explain how to derive subsampling guarantees for privacy accountants. In fact, there is so far no unified framework for deriving subsampling guarantees for privacy accountants.

To address these two limitations, we propose a novel framework for _unified mechanism-specific amplification by subsampling_ analysis, using conditional optimal transport. Our proposed framework (1) lets us derive mechanism-specific subsampling guarantees, which can be stronger than mechanism-agnostic guarantees, (2) lets us recover mechanism-agnostic guarantees via a pessimistic upper bound - essentially subsuming the approach from [15] - and (3) lets us derive guarantees for approximate differential privacy [2], moments accounting [6] (i.e., Renyi differential privacy [7]), and accounting with dominating pairs [10] (e.g., numerical accounting [16; 8; 17; 18; 19; 20; 21]) in a unified, principled manner.

As a practical application, we consider the problem of tightly analyzing group privacy under subsampling (see Fig. 1). Assume a scenario where \(K\) individuals may independently decide to contribute to a dataset. Further assume that this dataset is processed by randomly sampling a batch and applying an \((\varepsilon,\delta)\)-DP _base mechanism_. A special case of this setting is discussed in a recent technical note [22], which assumes that the \(K\) individuals collaboratively agree to either contribute or not contribute all their data and that the base mechanism is Gaussian. In the general setting we consider, the best known method for providing privacy guarantees for the entire group is to (1) use existing bounds to show that the subsampled mechanism guarantees \((\varepsilon^{\prime},\delta^{\prime})\)-DP for individuals and (2) use the group privacy property[23] to show that this implies \((K\cdot\varepsilon^{\prime},K\cdot e^{K\cdot\varepsilon^{\prime}}\cdot \delta^{\prime})\)-DP for the group.

Our proposed framework lets us derive stronger, tight _group privacy amplification_ guarantees. By analyzing group privacy and subsampling jointly, these guarantees accurately capture that it is unlikely for a large fraction of the group's data to simultaneously appear in a batch. Our framework further lets us derive dominating pairs [10], which enables tight tracking of group privacy under composition. Overall, our main contributions are that we

* demonstrate for the first time that there is a qualitative difference between mechanism-specific and mechanism-agnostic tightness in privacy amplification,
* propose a general framework for deriving mechanism-specific amplification by subsampling guarantees, subsuming prior work on mechanism-agnostic amplification,
* develop the first unified method for deriving subsampling guarantees for privacy accounting,
* and derive the first tight subsampling guarantees for general group privacy.

As part of our group privacy analysis, we also significantly generalize recent results derived for subsampled matrix mechanisms [24], which is of independent interest. Experimental evaluation demonstrates that our tight mechanism-specific guarantees outperform both tight mechanism-agnostic bounds and post-hoc use of the group privacy property.

## 2 Background and preliminaries

**Problem setting.** We consider the same setting as Balle et al. [15], but with an additional privacy accounting and group privacy perspective. We assume a space of datasets \(\mathbb{X}\), a space of batches \(\mathbb{Y}\) that can be constructed from these datasets, and an output space \(\mathbb{Z}\). For the sake of exposition, we assume that \(\mathbb{X}\) is the powerset \(\mathcal{P}(\mathbb{A})\) of some finite discrete set \(\mathbb{A}\), that \(\mathbb{Y}=\{y\subseteq x\mid x\in\mathbb{X}\}\), and that \(\mathbb{Z}=\mathbb{R}^{D}\). We further consider a random _subsampling scheme_\(S:\mathbb{X}\rightarrow\mathbb{Y}\) that generates batches from datasets, and a random _base mechanism_\(B:\mathbb{Y}\rightarrow\mathbb{R}^{D}\) that maps batches to outputs. We write \(s_{x}(y)\) for the pmf of \(S(X)\) and \(b_{y}(z)\) for the pdf of \(B(y)\). In Appendix D, we introduce a more general setting that admits arbitrary spaces, for which we derive all our results. Our goal is to

Figure 1: Group members \(x_{1}^{\prime},x_{2}^{\prime}\) contribute to a dataset, while group member \(x_{3}^{\prime}\) does not. For small subsampling rates \(r\), it is unlikely to access a single (\(\Pr=2r(1-r)\)) or even both (\(\Pr=r^{2}\)) inserted elements when applying a base mechanism \(B\) to a subsampled batch (e.g., the yellow one). This further obfuscates which data was contributed by members of group \(\{x_{1}^{\prime},x_{2}^{\prime},x_{3}^{\prime}\}\).

provide formal privacy guarantees for the _subsampled_ mechanism \(M=B\circ S\), which takes a dataset, subsamples a batch from it, and then applies the random base mechanism to this batch.

**Dataset and batch relations.** Specifically, we want to prove privacy under _neighboring relations_\(\simeq_{\mathbb{X}}\subseteq\mathbb{X}^{2}\) between datasets. For example, the _insertion/removal_ relation \(x\simeq_{\pm,\mathbb{X}}x^{\prime}\) implies that \(x^{\prime}=x\cup\{a\}\) or \(x^{\prime}=x\setminus\{a\}\) for some \(a\). The _substitution_ relation \(x\simeq_{\Delta,\mathbb{X}}x^{\prime}\) implies that \(x^{\prime}=x\setminus\{a\}\cup\{a^{\prime}\}\) for some \(a,a^{\prime}\). In addition, we assume that \(\mathbb{Y}\) is equipped with a batch neighboring relation \(\simeq_{\mathbb{Y}}\subseteq\mathbb{Y}^{2}\). The batch relation \(\simeq_{\mathbb{Y}}\) can be distinct from the dataset relation \(\simeq_{\mathbb{X}}\).

**Subsampling schemes.** While we consider arbitrary subsampling schemes \(S:\mathbb{X}\to\mathbb{Y}\), we shall later apply our framework to three particularly common ones: Subsampling _without replacement_ and _with replacement_ sample sets and multisets of fixed size \(|y|=q\) uniformly at random. _Poisson_ subsampling includes each element of \(x\) with independent probability ("rate") \(r\in[0,1]\).

**Approximate differential privacy.** A mechanism \(M:\mathbb{X}\to\mathbb{R}^{D}\) is privacy-preserving under symmetric neighboring relation \(\simeq_{\mathbb{X}}\) when the distributions of \(M(x),M(x^{\prime})\) with densities \(m_{x},m^{\prime}_{x}\) are almost indistinguishable for all \(x\simeq_{\mathbb{X}}x^{\prime}\). Approximate differential privacy (ADP) [2] quantifies this via hockey stick divergences [25]:

**Definition 2.1**.: For \(\varepsilon\geq 0\), a mechanism \(M:\mathbb{X}\to\mathbb{R}^{D}\) is \((\varepsilon,\delta)\)-DP under symmetric relation \(\simeq_{\mathbb{X}}\) iff \(\forall x\simeq_{\mathbb{X}}x^{\prime}:H_{\varepsilon^{c}}(m_{x}||m_{x^{ \prime}})\leq\delta\) with \(H_{\alpha}(m_{x}||m_{x^{\prime}})=\int_{\mathbb{R}^{D}}\max\{m_{x}(z)/m_{x^{ \prime}}(z)-\alpha,0\}\cdot m_{x^{\prime}}(z)\,\mathrm{d}z\).

Note that this divergence-based definition of approximate differential privacy is equivalent to requiring for all datasets \(x\simeq_{\mathbb{X}}x^{\prime}\) and all events \(S\subseteq\mathbb{R}^{D}\) that \(\Pr[M(x)\in S]\leq e^{\varepsilon}\cdot\Pr[M(x^{\prime})\in S]+\delta\).

**Renyi differential privacy.** Privacy accounting was first popularized by methods that used moment bounds instead of \((\varepsilon,\delta)\) pairs to better track privacy under composition [6, 26, 27]. These methods were later developed into a moments-based notion of privacy - Renyi differential privacy (RDP) [7]:

**Definition 2.2**.: For \(\alpha\geq 1\), a mechanism \(M:\mathbb{X}\to\mathbb{R}^{D}\) is \((\alpha,\rho)\)-RDP under symmetric relation \(\simeq_{\mathbb{X}}\) iff \(\forall x\simeq_{\mathbb{X}}x^{\prime}:\log(\Lambda_{\alpha}(m_{x}||m_{x^{ \prime}}))/\left(\alpha-1\right)\leq\rho\) with \(\Lambda_{\alpha}(m_{x}||m_{x^{\prime}})=\int_{\mathbb{R}^{D}}m_{x}(z)^{\alpha} \cdot m_{x^{\prime}}(z)^{1-\alpha}\mathrm{d}z\).

Note that \(\Lambda_{\alpha}\)_is not the Renyi divergence, but its \(\alpha\)th moment, i.e., a scaled and exponentiated Renyi divergence._ We use this notation to eliminate exponential terms that arise in amplification for RDP (see [28, 29, 30]). If a mechanism is \((\alpha,\rho)\)-RDP, its \(T\)-fold self-composition is \((\alpha,T\cdot\rho)\)-RDP. These RDP parameters can then be converted to ADP parameters (\(\varepsilon,\delta\)), albeit in a lossy manner [7, 31, 32].

**Dominating pairs.** Later work proposed other numerical [16, 17, 18, 19, 20, 8, 21] or analytical accounting techniques [9, 33, 28, 34, 35]. Zhu et al. [10] developed a unified view on these works by introducing the notion of _dominating pairs_:

**Definition 2.3**.: A pair of distributions \((P,Q)\) with densities \((p,q)\) is a dominating pair for mechanism \(M\) under neighboring relation \(\simeq_{\mathbb{X}}\), if \(H_{\alpha}(m_{x}||m_{x^{\prime}})\leq H_{\alpha}(p||q)\) for all \(x\simeq_{\mathbb{X}}x^{\prime}\) and all \(\alpha\geq 0\).

Given a dominating pair \((P,Q)\), one can use various numerical or analytic techniques, such as convolution of privacy loss distributions [16] or central limit theorems of tradeoff functions [9], to track the privacy of mechanism \(M\) under composition (see Fig. 2 in [10]).

**Group privacy.** The _group privacy property_ is the graceful decay of privacy when considering multiple user's data. This is normally formalized via the notion of induced distance (see [7, 15, 23]):

**Definition 2.4**.: The distance \(d_{\mathbb{X}}(x,x^{\prime})\) induced by relation \(\simeq_{\mathbb{X}}\) is the length of the shortest sequence \((x_{1},\ldots,x_{K-1})\in\mathbb{X}^{K-1}\) such that \(x\simeq_{\mathbb{X}}x_{1}\), \(\forall k:x_{k}\simeq_{\mathbb{X}}x_{k+1}\), and \(x_{K-1}\simeq_{\mathbb{X}}x^{\prime}\).

_Example 2.5_.: Let \(\simeq_{\mathbb{X}}\) be the insertion/removal relation \(\simeq_{\pm}\). Then the induced distance \(d_{\mathbb{X}}\) between \(x=\{1,2\}\) and \(x^{\prime}=\{2,3\}\) is \(2\), because \(\{1,2\}\simeq_{\pm}\{1,2,3\}\simeq_{\pm}\{2,3\}\).

**Proposition 2.6** (Vadhan [23]).: _If mechanism \(M:\mathbb{X}\to\mathbb{R}^{D}\) is \((\varepsilon,\delta)\)-DP under relation \(\simeq_{\mathbb{X}}\), then it is \((K\cdot\varepsilon,K\cdot e^{K\cdot\varepsilon}\cdot\delta)\)-DP under group relation \(\{(x,x^{\prime})\in\mathbb{X}^{2}\mid d_{\mathbb{X}}(x,x^{\prime})=K\}\)._

That is, for sufficiently small \(\varepsilon\) the ADP parameters deteriorate approximately linearly with induced distance. As baselines for our experiments, we use even tighter bounds (see Appendix C.1.4).

## 3 Unified mechanism-specific amplification by subsampling

Our goal is to develop a general procedure for (tightly) deriving ADP parameters \((\varepsilon^{\prime},\delta^{\prime})\), RDP parameters \((\alpha^{\prime},\rho^{\prime})\), or dominating pairs \((P^{\prime},Q^{\prime})\) for the subsampled mechanism \(M=B\circ S\) with subsampling scheme \(S\) and base mechanism \(B\). Based on Definitions 2.1 to 2.3, this requires that we evaluate or bound the hockey stick divergence \(H_{\alpha}\) or the scaled and exponentiated Renyi divergence \(\Lambda_{\alpha}\) between the distributions of \(M(x)\) and \(M(x^{\prime})\) with \(x\simeq_{\mathbb{X}}x^{\prime}\). To discuss both simultaneously, let us write \(\Psi_{\alpha}\) for either \(H_{\alpha}\) or \(\Lambda_{\alpha}\) and assume that \(\alpha\geq 0\) or \(\alpha\geq 1\), respectively.

There are two challenges: Firstly, the distribution of \(M(x)\) is high-dimensional mixture distribution with one component per possible batch \(y\in\mathbb{Y}\) and weights given by batch probabilitites \(s_{x}(y)\), i.e., \(m_{x}(z)=\sum_{y\in\mathbb{Y}}b_{y}(z)\cdot s_{x}(y)\). Secondly, there may be no simple analytic formula for the component densities. For instance, evaluating the density of perturbed model gradients in noisy SGD [36] requires backpropagation. Both challenges make it hard to evaluate or bound \(\Psi_{\alpha}(m_{x}||m_{x^{\prime}})\).

A useful property that will help us in addressing the first problem of having a large number of mixture components is the joint convexity of \(\Psi_{\alpha}\) in the space of probability density functions [37, 38]:

**Lemma 3.1**.: _Consider arbitrary densities \(f_{1}^{(1)},f_{2}^{(1)},f_{1}^{(2)},f_{2}^{(2)}:\mathbb{R}^{D}\to\mathbb{R}_{+}\) and weight \(w\in[0,1]\). Then, \(\Psi_{\alpha}(wf_{1}^{(1)}+(1-w)f_{2}^{(1)}||wf_{1}^{(2)}+(1-w)f_{2}^{(2)})\leq w \Psi_{\alpha}(f_{1}^{(1)}||f_{1}^{(2)})+(1-w)\Psi_{\alpha}(f_{2}^{(1)}||f_{2} ^{(2)})\)._

In our case, the \(f\)s can be base mechanism densities \(b_{y}\) with different \(y\in\mathbb{Y}\), and the weights can be given by the subsampling distribution. We could thus upper-bound the mixture divergence \(\Psi_{\alpha}(m_{x}||m_{x^{\prime}})\) in terms of component divergences - if the mixtures had identical weights. This is generally not the case, since subsampling mass functions \(s_{x}(\cdot)\) and \(s_{x^{\prime}}(\cdot)\) depend on datasets \(x\neq x^{\prime}\).

To still leverage the joint convexity of \(\Psi_{\alpha}(m_{x}||m_{x^{\prime}})\), we want to rewrite \(m_{x}\) and \(m_{x^{\prime}}\) as mixtures with identical weights. This is exactly what is offered by _couplings_ between probability mass functions. Intuitively, a coupling between two pmfs \(p_{1},p_{2}:\mathbb{Y}\to[0,1]\) specifies for every \(y_{1},y_{2}\in\mathbb{Y}\) how much probability should be transported from \(y_{1}\) to \(y_{2}\) to transform \(p_{1}\) into \(p_{2}\). Couplings can be formalized and generalized to multiple pmfs as follows (for a more thorough introduction, see [39]):

**Definition 3.2**.: A coupling between \(N\) pmfs \(p_{1},\ldots,p_{N}:\mathbb{Y}\to[0,1]\) on batch space \(\mathbb{Y}\) is a joint pmf \(\gamma:\mathbb{Y}^{N}\to[0,1]\) where the \(n\)th marginal is \(p_{n}\), i.e., \(p_{n}(y_{n}^{*})=\sum_{\bm{y}\in\mathbb{Y}^{N}}\mathbbm{1}[y_{n}=y_{n}^{*}] \cdot\gamma(\bm{y})\).

Given a valid coupling \(\gamma\) between subsampling mass functions \(s_{x}\) and \(s_{x^{\prime}}\), we can use marginalization to rewrite \(m_{x}(z)\) as \(\sum_{\bm{y}\in\mathbb{Y}^{2}}b_{y_{1}}(z)\gamma(\bm{y})\). Similarly, we can rewrite \(m_{x^{\prime}}(z)\) as \(\sum_{\bm{y}\in\mathbb{Y}^{2}}b_{y_{2}}(z)\gamma(\bm{y})\). Now, both \(m_{x}\) and \(m_{x^{\prime}}\) are mixtures with identical weights and one component per pair of batches in \(\mathbb{Y}^{2}\). We can thus recursively apply Lemma 3.1 to show (full proof in Appendix E):

**Theorem 3.3**.: _Consider a subsampled mechanism \(M=B\circ S\), and an arbitrary coupling \(\gamma\) between subsampling mass functions \(s_{x}(\cdot)\) and \(s_{x^{\prime}}(\cdot)\). Then,_

\[\Psi_{\alpha}(m_{x}||m_{x^{\prime}})\leq\sum_{\bm{y}\in\mathbb{Y}^{2}}c_{ \alpha}(y^{(1)},y^{(2)})\cdot\gamma(y^{(1)},y^{(2)})\] (1)

_with cost function \(c_{\alpha}:\mathbb{Y}\times\mathbb{Y}\to\mathbb{R}_{+}\) defined by \(c_{\alpha}(y^{(1)},y^{(2)})=\Psi_{\alpha}(b_{y^{(1)}}||b_{y^{(2)}})\)._

We write \(y^{(1)}\) instead of \(y_{1}\) to simplify later notations. While every coupling \(\gamma\) yields a valid upper bound, the guarantees can be tightened by finding a coupling \(\gamma^{*}\) that minimizes the r.h.s. of Eq. (1). We thus have an _optimal transport problem_, where the cost \(c_{\alpha}\) of transporting probability from batch \(y^{(1)}\) to batch \(y^{(2)}\) depends on the divergence \(\Psi_{\alpha}\) of base mechanism densities \(b_{y^{(1)}}\) and \(b_{y^{(2)}}\).

Unfortunately, experimental evaluation in Appendix B.1.4 shows that the resultant guarantees can be much weaker than those from prior work - even with an optimal coupling \(\gamma^{*}\). This is because Theorem 3.3 results from recursively applying the joint convexity property (Lemma 3.1) to \(\Psi_{\alpha}(m_{x}||m_{x^{\prime}})\). Each recursive step splits each mixture into two smaller mixtures and further upper-bounds the divergence that is achieved by _our specific subsampled mechanism \(M\) on our specific pair of datasets \(x,x^{\prime}\)_. Upon fully decomposing the overall divergence into divergences between single-mixture components, this sequence of bounds is in fact larger than even the divergence \(\Psi_{\alpha}(\tilde{m}_{\tilde{x}}||\tilde{m}_{\tilde{x}^{\prime}})\) achieved by _a worst-case subsampled mechanism \(\tilde{M}\) on a worst-case pair of datasets \(\tilde{x},\tilde{x}^{\prime}\)_. To overcome this limitation, we propose to limit the recursion depth in order to obtain a tighter upper bound that matches _our specific subsampled mechanism \(M\) on a worst-case pair of datasets \(\tilde{x},\tilde{x}^{\prime}\)_.

Limiting the recursion depth to which Lemma 3.1 is applied means upper-bounding \(\Psi_{\alpha}(m_{x}||m_{x^{\prime}})\) in terms of mixture divergences that have not been fully decomposed into their individual components. Specifically, we propose to do so by defining an optimal transport problem between _multiple_ subsampling mass functions conditioned on different events (proof in Appendix E):

**Theorem 3.4**.: _Consider a subsampled mechanism \(M=B\circ S\). Further consider two disjoint partitionings \(\bigcup_{i=1}^{I}A_{i}=\mathbb{Y}\) and \(\bigcup_{j=1}^{J}E_{j}=\mathbb{Y}\) of batch space \(\mathbb{Y}\) such that all \(A_{i}\) and \(E_{j}\) have non-zero probability under the distribution of \(S_{x}\) and \(S_{x^{\prime}}\), respectively. Let \(\gamma\) be an arbitrary coupling between conditional mass functions \(s_{x}(\cdot\mid A_{1}),\ldots,s_{x}(\cdot\mid A_{I}),s_{x^{\prime}}(\cdot\mid E _{1}),\ldots,s_{x^{\prime}}(\cdot\mid E_{J})\). Then,_

\[\Psi_{\alpha}(m_{x}||m_{x^{\prime}})\leq\sum_{\bm{y}\in\mathbb{Y}^{I+J}}c_{ \alpha}(\bm{y}^{(1)},\bm{y}^{(2)})\cdot\gamma((\bm{y}^{(1)},\bm{y}^{(2)})),\]

_with cost function \(c_{\alpha}:\mathbb{Y}^{I}\times\mathbb{Y}^{J}\to\mathbb{R}_{+}\) defined by_

\[c_{\alpha}(\bm{y}^{(1)},\bm{y}^{(2)})=\Psi_{\alpha}\left(\sum_{i=1}^{I}b_{y_{ i}^{(1)}}\cdot\Pr[S(x)\in A_{i}]\|\sum_{j=1}^{J}b_{y_{j}^{(2)}}\cdot\Pr[S(x^{ \prime})\in E_{j}]\right).\] (2)

In other words: We now have an optimal transport problem between \(I+J\) probability mass functions coupled by \(\gamma\). The transport cost \(c_{\alpha}\) is a divergence between two mixtures. The components of the first mixture are base mechanisms densities given batches \(y_{i}^{(1)}\in\mathbb{Y}\) from batch tuple \(\bm{y}^{(1)}\in\mathbb{Y}^{I}\). The weights of the first mixture are probabilities of events \(A_{i}\). The second mixture is defined analogously.

Note that we can recover Theorem 3.3 by conditioning on a single event, i.e., \(A_{1}=\mathbb{Y},E_{1}=\mathbb{Y}\). As we shall demonstrate, a more fine-grained partitioning lets us obtain tighter bounds that match the divergence \(\Psi_{\alpha}(m_{\tilde{x}}||m_{\tilde{x}^{\prime}})\) of our specific subsampled mechanism \(M\) on a worst-case pair of datasets \(\hat{x},\hat{x}^{\prime}\). In the extreme case of defining event per possible batch, Theorem 3.4 holds with equality.

Overall, Theorem 3.4 reduces the broad problem of bounding mixture divergences to the canonical problem of optimal transport between conditional distributions. But before we can apply it, we need to address two open problems: Evaluating the cost function and designing optimal couplings.

**Cost function bound.** Not having an analytic expression for every base mechanism density \(b_{y}(z)\) may make it intractable to evaluate cost function \(c_{\alpha}\). We thus propose to bound it via an approach that is inherent to differential privacy: Considering worst-case inputs (proof in Appendix E).

**Proposition 3.5**.: _Consider \(\bm{y}^{(1)}\in\mathbb{Y}^{I},\bm{y}^{(2)}\in\mathbb{Y}^{J}\), and cost function \(c\) defined in Eq.2. Let \(d_{\mathbb{Y}}\) be the distance induced by \(\simeq_{\mathbb{Y}}\) (see Definition 2.4). Then, \(c_{\alpha}(\bm{y}^{(1)},\bm{y}^{(2)})\leq\hat{c}_{\alpha}(\bm{y}^{(1)},\bm{y}^ {(2)})\), with_

\[\hat{c}_{\alpha}(\bm{y}^{(1)},\bm{y}^{(2)})=\max_{\hat{\bm{y}}^{(1)},\hat{\bm{ y}}^{(2)}}c_{\alpha}(\hat{\bm{y}}^{(1)},\hat{\bm{y}}^{(2)})\] (3)

_subject to \(\forall k,l\in\{1,2\},\forall t,u:d_{\mathbb{Y}}(\hat{y}_{t}^{(k)},\hat{y}_{ u}^{(l)})\leq d_{\mathbb{Y}}(y_{t}^{(k)},y_{u}^{(l)})\) and \(\hat{\bm{y}}^{(1)}\in\mathbb{Y}^{I},\hat{\bm{y}}^{(2)}\in\mathbb{Y}^{J}\)._

Put differently: We construct two new mixtures with components that are adversarially chosen to maximize divergence while retaining the pairwise distances between batches in \(\bm{y}^{(1)}\) and \(\bm{y}^{(2)}\). Note that, for the special case of \(\Psi_{\alpha}=H_{\alpha}\) and \(I=J=1\), this bound corresponds to the "group privacy profile" in [15]. As we shall demonstrate in the next sections, this bound \(\hat{c}\) can often be evaluated using high-level information about the base mechanism \(B:\mathbb{Y}\to\mathbb{R}^{D}\), such as global sensitivity.

**Sufficient optimality condition.** While every coupling \(\gamma\) yields a valid upper bound in Theorem 3.4, this bound can be tightened by designing an optimal coupling \(\gamma^{*}\). To inform this design, we generalize the notion of distance-compatible couplings from [15] to an arbitrary number of distributions:

**Definition 3.6**.: A coupling \(\gamma\) between mass functions \(p_{1},\ldots,p_{N}\) on batch space \(\mathbb{Y}\) is \(d_{\mathbb{Y}}\)-compatible when \(\gamma(\bm{y})>0\) only if \(\forall u>1:d_{\mathbb{Y}}(y_{1},y_{u})=d_{\mathbb{Y}}(\{y_{1}\},\mathrm{ supp}(p_{u}))\) and \(\forall u>t>1:d_{\mathbb{Y}}(y_{u},y_{t})=d_{\mathbb{Y}}(\mathrm{supp}(p_{t}), \mathrm{supp}(p_{u}))\), where \(\mathrm{supp}(p_{u})\) is the support of \(s_{u}\) and the distance between two sets is the minimum distance of their elements.

Essentially, a \(d_{\mathbb{Y}}\)-compatible coupling only assigns probability to a tuple of batches \(\bm{y}\) when all pairs \(y_{i},y_{j}\) have the smallest possible distance to \(y_{1}\) and each other while still being in the support of their distributions. In Appendix F, we prove that \(d_{\mathbb{Y}}\)-compatibility is sufficient for optimality. We further show that the optimal value has a canonical form whenever a \(d_{\mathbb{Y}}\)-compatible couplings exists.

**Summary.** To summarize, we propose the following three-step procedure for deriving subsampling guarantees: (1) Define two partitions of the batch space into \(I\) and \(J\) events. (2) Define a simultaneous coupling between the corresponding \(I+J\) conditional distributions to obtain a bound in terms of divergences between small mixtures with \(I\) and \(J\) components. (3) Bound these mixture divergences by considering \(I+J\) worst-case mixture components under pairwise batch distance constraints.

### Tight mechanism-specific group privacy amplification

As a concrete example, and to illustrate the difference between mechanism-specific and -agnostic (see Section 3.2) bounds, let us consider the group privacy setting from Fig. 1. We have a group of \(K\) users that can independently decide to contribute or not contribute their data, and the resulting dataset is Poisson subsampled. To provide formal privacy guarantees to the group, we need to prove that the distributions of \(M(x),M(x^{\prime})\) are almost indistinguishable for \(x,x^{\prime}\) with induced distance \(d_{\pm,\mathbb{X}}(x,x^{\prime})\leq K\). Let \(x\simeq_{K_{+},K_{-},\mathbb{X}}x^{\prime}\) be the relation that implies that \(K_{+}\) records are inserted and \(K_{-}\) records are removed to construct \(x^{\prime}\) from \(x\). We can then show (full proof in Appendix M):

**Theorem 3.7**.: _Let \(M=B\circ S\), where \(S\) is Poisson subsampling with rate \(r\). Let \(\simeq_{\mathbb{Y}}\) be the insertion/removal batch relation \(\simeq_{\pm,\mathbb{Y}}\). Then, for all \(x\simeq_{K_{+},K_{-},\mathbb{X}}x^{\prime}\), \(\Psi_{\alpha}(m_{x}||m_{x^{\prime}})\) is l.e.q._

\[\max_{\boldsymbol{y}}\Psi_{\alpha}\left(\sum_{i=1}^{K_{-}+1}b_{y_{i}^{(1)}} \cdot\mathrm{Binom}(i-1\mid K_{-},r)||\sum_{j=1}^{K_{+}+1}b_{y_{j}^{(2)}} \cdot\mathrm{Binom}(j-1\mid K_{+},r)\right),\] (4)

_subject to constraints \(\boldsymbol{y}\in\mathbb{Y}^{K_{-}+K_{+}+2}\), as well as \(\forall l\in\{1,2\},\forall t,u:d_{\mathbb{Y}}(y_{t}^{(l)},y_{u}^{(l)})\leq|t-u|\), and \(\forall t,u:d_{\mathbb{Y}}(y_{t}^{(1)},y_{u}^{(2)})\leq(t-1)+(u-1)\)._

Proof sketch.: We let \(A_{i}\) and \(E_{j}\) be the events that \(S(x)\) contains \(i-1\) deleted elements and \(S(x^{\prime})\) contains \(j-1\) inserted elements, respectively. Our coupling defines the following generative process: We sample \(y_{1}^{(1)}\) from \(s_{x}(\cdot\mid A_{1})\) and let \(y_{1}^{(2)}\gets y_{1}^{(1)}\). We then iteratively generate the other \(y_{i}^{(1)}\) by sampling a permutation in which we add the \(K_{-}\) deleted elements to \(y_{1}^{(1)}\). We then generate the other \(y_{i}^{(2)}\) by sampling a permutation in which we add the \(K_{+}\) inserted elements to \(y_{1}^{(2)}\). The result then follows from the cost function bound in Proposition 3.5 and \(d_{\mathbb{Y}}\)-compatibility of the coupling. Batches \(y_{u}^{(1)}\) and \(y_{t}^{(1)}\) and have a distance bounded by \(|u-t|\), because one can be obtained from the other by removing/inserting \(|u-t|\) elements. The constraints for \(y_{u}^{(2)}\) and \(y_{t}^{(2)}\) are analogous. Batches \(y_{u}^{(1)}\) and \(y_{t}^{(2)}\) have a distance bounded by \((t-i)+(u-1)\) because we need to remove \(t-1\) elements and insert \(u-1\) elements to construct one from the other. 

Next, we can solve the constrained optimization problem in Theorem 3.7 - i.e., determine worst-case components - to obtain mechanism-specific guarantees. For instance (proof in Appendix M.2.1):

**Theorem 3.8**.: _Let \(M=B\circ S\), where \(S\) is Poisson subsampling with rate \(r\), and \(B\) is the Gaussian mechanism \(h+V\) with \(h:\mathbb{Y}\to\mathbb{R}^{D}\) and \(V\sim\mathcal{N}(\boldsymbol{0},\sigma^{2}\boldsymbol{I}_{D})\). Define the \(\ell_{2}\)-sensitivity \(L_{2}=\max_{y\simeq_{\pm,\mathbb{Y}}y^{\prime}}||f(y)-f(y^{\prime})||_{2}\). Then for all \(x\simeq_{K_{+},K_{-},\mathbb{X}}x^{\prime}\), \(\Psi_{\alpha}(m_{x}||m_{x^{\prime}})\) is l.e.q._

\[\Psi_{\alpha}\left(\sum_{i=1}^{K_{-}+1}f_{i}^{(1)}\cdot\mathrm{Binom}(i-1\mid K _{-},r)||\sum_{j=1}^{K_{+}+1}f_{j}^{(2)}\cdot\mathrm{Binom}(j-1\mid K_{+},r) \right),\]

_with univariate normal densities \(f_{i}^{(1)}=\mathcal{N}(\cdot\mid(i-1),\sigma\mid/\,L_{2})\), \(f_{j}^{(2)}=\mathcal{N}(\cdot\mid-(j-1),\sigma\mid/\,L_{2})\)._

Note that this bound is mechanism-specific in that it explicitly depends on \(B\) being a Gaussian mechanism with standard deviation \(\sigma\) and sensitivity \(L_{2}\). The bound can be numerically evaluated to arbitrary precision using standard techniques from privacy accounting literature (see Appendix M.4). In Appendix M.2 we derive similar guarantees for Laplace and randomized response mechanisms.

**Tightness.** These bounds are tight in a mechanism-specific sense: One cannot derive stronger ADP or RDP guarantees without additional information about the datasets \(x,x^{\prime}\in\mathbb{X}\) or the underlying function \(h\) (proofs in Appendix M.3).

**Asymptotic bounds.** Our focus is on tight bounds that can be explicitly computed. However, some early works on RDP accounting (e.g.[6, 28]) also provided asymptotic versions of their bounds, e.g., as a function of divergence parameter \(\alpha\). For completeness, we use Theorem 3.8 to generalize the asymptotic bounds of Abadi et al. [6] to the group privacy setting in Appendix N.5.

**Other contributions.** Our solutions to the optimization problem in Theorem 3.7 are of independent interest: The special case of \(\Psi_{\alpha}=H_{\alpha}\), \(K_{-}=0\) or \(K_{+}=0\), and Gaussian mechanisms wasderived to analyze matrix mechanisms in [24]. We significantly generalize it via an entirely different proof strategy that admits \(\Psi_{\alpha}\in\{H_{\alpha},\Lambda_{\alpha}\}\), arbitrary \(K_{+},K_{-}\in\mathbb{N}_{0}\), and non-Gaussian mechanisms (see Appendix O).

### Tight mechanism-agnostic group privacy amplification

To further illustrate the difference between mechanism-specific and -agnostic bounds, let us apply the framework of Balle et al. [15] to the same setting: For \(K_{-}=0\) or \(K_{+}=0\) and \(\varepsilon\geq 0\), their ansatz shows that the subsampled mechanism \(M=B\circ S\) is \((\varepsilon^{\prime},\delta^{\prime})\)-DP with \(\varepsilon^{\prime}=\log(1+(1-\operatorname{Binom}(0\mid K,r))\cdot(e^{ \varepsilon}-1))\) and \(\delta^{\prime}=\sum_{k=1}^{K}\operatorname{Binom}(k\mid K,r)\cdot\delta_{k}\), with group privacy parameters \(\delta_{k}=\max_{y,y^{\prime}}H_{\exp(\varepsilon)}(b_{y}\|b_{y^{\prime}})\) s.t. \(d_{\mathbb{Y}}(y,y^{\prime})\leq k\) (see Appendix N).

**Tightness.** This bound is tight in a mechanism-agnostic sense, i.e., for every \(\varepsilon\geq 0\), one can construct _some_ worst-case mechanism that exactly attains the bound (see Appendix N.2).

**Mechanism-specific vs mechanism-agnostic tightness.** An apparent advantage of this result is that it expresses \((\varepsilon^{\prime},\delta^{\prime})\) as a simple analytic formula of base mechanism DP parameters \(\varepsilon,\delta_{1},\ldots,\delta_{K}\). However, this simplicity comes at a cost: As we show in Appendix N.3, this guarantee implicitly upper-bounds the tight mixture divergence bound from Theorem 3.7 in terms of its component divergences using joint convexity.2 As we experimentally demonstrate in Section 4, this relaxation leads to weaker privacy guarantees for group size \(K\geq 2\). This gap has gone unnoticed in earlier work, because Theorem 3.8 happens to be identical to the bound of Balle et al. [15] for the special case of \(K=1\) (see proof of Proposition 30 in [10]). By studying the more complex group privacy setting, we demonstrate for the first time that _there is a qualitative difference between mechanism-agnostic and mechanism-specific tightness in privacy amplification_.

Footnote 2: In combination with “advanced joint convexity” [15].

### From mechanism-specific to mechanism-agnostic guarantees

The observed relation between tight mechanism-specific and mechanism-agnostic bounds does in fact extend beyond group privacy (see Fig. 2): As we demonstrate in Appendices G and H, mechanism-agnostic subsampling guarantees from prior work can equivalently be derived by (1) conditioning on at most \(4\) events indicating the presence of inserted / deleted / substituted elements, (2) defining a simultaneous coupling, and (3) using joint convexity to upper-bound the resultant mechanism-specific guarantee by component divergences. This includes the ADP guarantees of Balle et al. [15] (and thus prior work [14; 43; 44]) and the RDP guarantees from [28; 29; 30; 40].

**Subsumption of [15].** Going even further, we show in Appendix G.2 that our approach subsumes the entire framework of Balle et al. [15]: Any mechanism-agnostic guarantee derived via their ansatz can equivalently be derived by defining a (potentially suboptimal) simultaneous coupling between four subsampling distributions and upper-bounding the resultant guarantee via (advanced) joint convexity.

**Contribution.** Importantly, our contribution is not in the bounds themselves, but in the identification of this implicitly underlying pattern that unifies prior work. We further generalize this pattern through our proposed framework to enable tight analysis for more challenging scenarios like group privacy.

**Novel RDP accounting bounds.** In Appendix I, we use this observation to derive a variety of novel mechanism-agnostic RDP bounds for different combinations of \(\simeq_{\mathbb{Y}}\), \(\simeq_{\mathbb{Y}}\), and subsampling schemes, such as insertion/removal and subsampling without replacement. We further derive a simple but tight mechanism-specific guarantee for subsampled randomized response under substitution (see Theorem I.3) that outperforms the best known mechanism-agnostic bound (see Section 4).

Figure 2: Mechanism-agnostic guarantees for (a) graph modification [40; 41; 42] (b) insertion/removal [14; 15; 29; 30] (c) substitution [43; 44; 15; 28] can be derived from (d) our proposed framework. In (b-c), events \(A_{i}\) and \(E_{j}\) indicate the presence of inserted or substituted elements.

### From mechanism-specific guarantees to dominating pairs.

Because the mechanism-specific guarantees derived via our framework do not obfuscate the underlying distributions, they can be used to identify dominating pairs. These dominating pairs can then be combined with arbitrary (numerical) accountants to track privacy under composition. For example, we can immediately read off from Theorem 3.8 that the two Gaussian mixtures are a dominating pair for the "insert-\(K_{+}\)-remove-\(K_{-}\)" relation. In Appendix J, we describe a procedure for constructing dominating pairs when the bound is a weighted sum of multiple mixture divergences. We can thus determine dominating pairs for any bound derived via optimal transport. As we demonstrate in Appendix K, the dominating pairs for subsampled mechanisms derived by Zhu et al. [10] (special cases of which appear in [17, 18, 8, 19, 11]) can equivalently be proven via our framework.

**Subsampling with replacement.** As a novel contribution, we derive for the first time dominating pairs for subsampling _with_ replacement (see Theorem L.5), which were posited but not proven in [8] (see discussion in Appendix L). This is enabled by our solution to the problem in Theorem 3.7. As we experimentally show in Appendix B.1.5, these bounds can be much stronger than those derived via the framework of Balle et al. [15]. These results thus demonstrate that _there is a qualitative difference between mechanism-specific and mechanism-agnostic tightness even for group size \(K=1\)._

### Limitations and future work

**Limitations.** While our proposed framework can be applied to arbitrary subsampling distributions, conditioning on a finite set of events may be too restrictive in certain settings (e.g., continuous batch spaces). Also, while we found conditioning on the number of modified elements to be sufficient for all considered scenarios, an automated procedure for selecting events would be desirable. Maximal couplings fulfill this purpose in [15], but only yield pairs of distributions.

**Future work.** A natural direction is applying our ansatz to novel settings other than group privacy. In particular, it could potentially be used to provide epoch-level guarantees for correlated subsampling (e.g., batching via shuffling), similar to [38]. We present a preliminary result for \(2\)-fold non-adaptive composition in Appendix P. Future work could also use our solutions to the problem in Theorem 3.7 to generalize the matrix mechanism analysis from [24] to substitutions and non-Gaussian distributions.

## 4 Experimental evaluation

The purpose of the following experiments is to verify that there can be a benefit to using mechanism-specific over mechanism-agnostic subsampling bounds, and that a joint analysis of subsampling and group privacy can offer stronger guarantees than post-hoc application of the generic group privacy property. In all figures, "specific" refers to our proposed mechanism-specific bounds, "agnostic" refers to (tight) mechanism-agnostic bounds, and "post-hoc" refers to applying the generic group privacy property to tight mechanism-specific bounds derived for group size \(1\). For all experiments, we assume \(\ell_{p}\) sensitivities of \(1\). Further details on the experimental setup are provided in Appendix C. An implementation will be made available at https://cs.cit.tum.de/daml/group-amplification.

Figure 3: Randomized response with WOR subsampling (\(q\)\(/\)\(N=0.001\)), group size \(1\), and varying true response probability \(\theta\).

### Mechanism-agnostic and mechanism-specific guarantees

**Randomized response and RDP.** One potential source of looseness in mechanism-agnostic bounds is that they bound mixture divergences in terms of component divergences that can be summarized by a single privacy parameter. As an example, let us consider the best known guarantee for substitution, subsampling without replacement, and RDP from [28]. As discussed by the authors, it is only tight up to factors that are constant in \(\alpha\). In Fig. 4 we compare it to our tight mechanism-specific bound for randomized response (see Theorem I.3) with batch-to-dataset ratio \(q\ /\ N=0.001\) and true response probability \(\theta\in\{0.6,0.75,0.9\}\). In Appendix B.1.1 we consider other ratios. The tight guarantee eliminates the constant factors and thus achieves much smaller \(\rho\) for a wide range of \(\alpha\in(1,10^{4}]\).

**Group privacy and ADP.** Another potential source of looseness in mechanism-agnostic bounds is that they stem from a binary partitioning of the batch space (recall Fig. 2), which may not be sufficient when there are multiple possible levels of privacy leakage (e.g. number of sampled group elements). We demonstrate this in Fig. 4, by comparing the tight mechanism-agnostic group privacy bound derived via the framework from [15] to our tight mechanism-specific bound for Laplace mechanisms (see Theorem M.2), ADP, scale \(\lambda=1\), subsampling rate \(r=0.2\), and varying group size. In Appendix B.1.2 we repeat the experiment with other mechanisms and parameter values, and also consider RDP. As discussed in Section 3.2, the mechanism-agnostic bound is identical to the mechanisms-specific bound for group size \(K=1\). For group sizes \(K\geq 2\), however, the fine-grained partitioning underlying the mechanism-specific bound yields stronger privacy guarantees. These results confirm that we need to distinguish between mechanism-agnostic and mechanism-specific tightness when analyzing complex subsampling settings.

### Post-hoc and mechanism-specific group privacy analysis

**Single-iteration group privacy.** Our next goal is to demonstrate the benefit of analyzing group privacy and subsampling jointly. In Fig. 6, we evaluate our tight guarantee for Gaussian mechanisms (see Theorem 3.8) with \(\sigma=2\), rate \(r=0.2\), and varying group size. As a baseline, we evaluate the same tight guarantee with \(K_{+}+K_{-}=1\) and apply the generic group privacy property (see Appendix C.1.4) in a post-hoc manner. As can be seen, our tight analysis can lead to much stronger privacy guarantees. However, we interestingly observe in Appendix B.2.1 that the generic group privacy properties of ADP and RDP can serve as increasingly tight upper bounds when considering more private base mechanisms (e.g., \(\sigma=5\)) and much smaller subsampling rates (e.g., \(r=10^{-3}\)).

**Composed group privacy.** Nevertheless, even the gaps in tightness for very private mechanisms may quickly accumulate when repeatedly applying these mechanisms to a dataset. In Fig. 6, we use the dominating pairs derived via our framework to conduct tight PLD accounting [16, 8] with "connect the dots" [20] quantization using Google's dp_accounting library [45]. For \(\sigma=5\), \(r=10^{-3}\), and fixed \(\varepsilon=2\) (for other paramaters and mechanisms, see Appendix B.2.3), the post-hoc analysis quickly diverges with increasing group size and number of iterations. For example, with group size \(16\) and privacy budget \(\delta=10^{-6}\), the post-hoc analysis allows less than \(100\) iterations of DP-SGD training [36, 6], whereas our tight analysis enables training for over \(1000\) iterations. In Appendix B.2.5 we train an image classification model on MNIST with PLD accounting to demonstrate that this increased number of training iterations can translate to much higher model utility.

## 5 Related Work

**Amplification by subsampling for ADP.** Using subsampling to strenghten \((\varepsilon,\delta)\)-DP guarantees has a long history [13; 14] particularly in privacy-preserving machine learning [46; 47; 6]. For a thorough introduction to subsampling and its interaction with composition, we refer the reader to [48]. Balle et al. [15] ultimately proposed a framework for deriving tight mechanism-agnostic subsampling guarantees. Our work subsumes theirs and enables the derivation of mechanism-specific guarantees.

**Amplification for privacy accountants.** Abadi et al. [6]'s seminal work on moments accounting already considered subsampling. Similiar results were derived for the more general notion of RDP [7] in [28; 29; 30]. More recent works on accounting generally include a discussion of either Poisson subsampling or subsampling without replacement [17; 18; 19; 21; 11]. Subsampling with replacement is discussed in [8], albeit without complete proofs, which we provide in Appendix L. We demonstrate that amplification guarantees for the central notions of RDP [7] and dominating pairs [10] can - just like guarantees for ADP - be derived via optimal transport. Our novel approach not only unifies prior, but also enables a principled analysis of challenging scenarios like group privacy.

**Group privacy amplification.** A special case of group privacy amplification (which we use as an example to demonstrate the utility of our general framework) with hockey stick divergences, Gaussian mechanisms, and groups that collaboratively agree to contribute all their data was posited in [24] and proven in a recent follow-up note [22]. However, their result (and proof strategy) does not cover Renyi divergences, other mechanisms (Laplace, randomized response), and the standard notion of group privacy under which group members are not forced to collaborate (see, e.g., [3; 7; 9; 15; 23]).

**Hierarchical randomized smoothing.** Randomized smoothing [49; 50; 51] is a technique for constructing provably robust models via DP achieved through input perturbations (e.g. [52; 53; 54; 55; 56; 57; 58]). Similar to subsampling, Scholten et al. [69] proposed to only apply perturbations to randomly sampled parts of an input. We repurpose their technique of treating subsampling indicators as observable random variables in order to construct dominating pairs from weighted sums of divergences (see Appendix J.1).

**Unified amplification for \(f\)-DP.** Wang et al. [70] proved a form of joint concavity for the tradeoff functions underlying \(f\)-DP accounting [9; 34]. This enables them to analyze mixtures induced by random initialization and shuffling in a unified manner. However, they do _not_ consider amplification by subsampling, and explicitly state that they need to address this problem in future work. Note that dominating pairs, which can be derived via our proposed framework, can be used in f-DP accounting due to duality of privacy profiles and tradeoff functions [9; 10].

## 6 Conclusion

The main purpose of this work is to provide a unified, principled framework for mechanism-specific subsampling analysis and subsampling analysis for privacy accountants. To this end, we proposed a three-step procedure based on optimal transport between conditional subsampling distributions. Beyond recovering known guarantees for Renyi DP and dominating pairs, this procedure lets us derive novel results that were previously only available for approximate DP, such as non-standard combinations of subsampling schemes and neighboring relations, or subsampling with replacement. We then applied this procedure to the problem of analyzing group privacy under subsampling. Our experimental evaluation demonstrates that our mechanism-specific group privacy amplification bounds are not only tight, but can also significantly outperform tight mechanism-agnostic bounds and traditional group privacy results - even under composition. On a higher level, these bounds represent a novel contribution to a larger body of work (e.g., [71; 38; 72]) that demonstrates the benefit of analyzing multiple properties of differential privacy jointly instead of independently.

## 7 Acknowledgements

We are grateful to Georgios Kaissis for valuable discussions and feedback on generalizing our work beyond Renyi differential privacy, Yan Scholten for pointing out connections between subsampled mechanisms and hierarchical randomized smoothing, as well as Leo Schwinn and Nicholas Gao for proofreading our manuscript. This research was funded by the German Research Foundation, grant GU 1409/4-1, and the Munich Data Science Institute (MDSI) at Technical University of Munich (TUM) via the Linde/MDSI Doctoral Fellowship program and the MDSI Seed Fund.

## References

* [1] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In _Theory of Cryptography_, pages 265-284. Springer, 2006.
* EUROCRYPT_, pages 486-503, 2006.
* [3] Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. _Foundations and Trends in Theoretical Computer Science_, 9(3-4):211-407, 2014.
* [4] Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for differential privacy. In _International conference on machine learning_, pages 1376-1385, 2015.
* [5] Jack Murtagh and Salil Vadhan. The complexity of computing the optimal composition of differential privacy. In _Theory of Cryptography Conference_, pages 157-175, 2015.
* [6] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In _Proceedings of the SIGSAC conference on computer and communications security_, pages 308-318, 2016.
* [7] Ilya Mironov. Renyi differential privacy. In _IEEE 30th computer security foundations symposium (CSF)_, pages 263-275, 2017.
* [8] Antti Koskela, Joonas Jalko, and Antti Honkela. Computing tight differential privacy guarantees using fft. In _International Conference on Artificial Intelligence and Statistics_, pages 2560-2569, 2020.
* [9] Jinshuo Dong, Aaron Roth, and Weijie J Su. Gaussian differential privacy. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 84(1):3-37, 2022.
* [10] Yuqing Zhu, Jinshuo Dong, and Yu-Xiang Wang. Optimal accounting of differential privacy via characteristic function. In _International Conference on Artificial Intelligence and Statistics_, pages 4782-4817, 2022.
* [11] Wael Alghamdi, Juan Felipe Gomez, Shahab Asoodeh, Flavio P. Calmon, Oliver Kosut, and Lalitha Sankar. The saddle-point method in differential privacy. In _International Conference on Machine Learning_, 2023.
* [12] Jiachen Tianhao Wang, Saeed Mahloujifar, Tong Wu, Ruoxi Jia, and Prateek Mittal. A randomized approach to tight privacy accounting. _Advances in Neural Information Processing Systems_, 36, 2023.
* [13] Shiva Prasad Kasiviswanathan, Homin K Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. What can we learn privately? _SIAM Journal on Computing_, 40(3):793-826, 2011.
* [14] Ninghui Li, Wahbeh Qardaji, and Dong Su. On sampling, anonymization, and differential privacy or, k-anonymization meets differential privacy. In _Proceedings of the 7th ACM Symposium on Information, Computer and Communications Security_, pages 32-33, 2012.
* [15] Borja Balle, Gilles Barthe, and Marco Gaboardi. Privacy amplification by subsampling: Tight analyses via couplings and divergences. _Advances in neural information processing systems_, 31, 2018.
* [16] Sebastian Meiser and Esfandiar Mohammadi. Tight on budget? tight bounds for r-fold approximate differential privacy. In _Proceedings of the ACM SIGSAC Conference on Computer and Communications Security_, pages 247-264, 2018.
* [17] Antti Koskela and Antti Honkela. Computing differential privacy guarantees for heterogeneous compositions using fft. In _International Conference on Learning Representations_, 2021.
* [18] Antti Koskela, Joonas Jalko, Lukas Prediger, and Antti Honkela. Tight differential privacy for discrete-valued mechanisms and for the subsampled gaussian mechanism using fft. In _International Conference on Artificial Intelligence and Statistics_, pages 3358-3366, 2021.

* [19] Sivakanth Gopi, Yin Tat Lee, and Lukas Wutschitz. Numerical composition of differential privacy. _Advances in Neural Information Processing Systems_, 34:11631-11642, 2021.
* [20] Vadym Doroshenko, Badih Ghazi, Pritish Kamath, Ravi Kumar, and Pasin Manurangsi. Connect the dots: Tighter discrete approximations of privacy loss distributions. _Proceedings on Privacy Enhancing Technologies_, 2022.
* [21] Badih Ghazi, Pritish Kamath, Ravi Kumar, and Pasin Manurangsi. Faster privacy accounting via evolving discretization. In _International Conference on Machine Learning_, pages 7470-7483, 2022.
* [22] Arun Ganesh. Tight group-level dp guarantees for dp-sgd with sampling via mixture of gaussians mechanisms. _arXiv preprint arXiv:2401.10294_, 2024.
* [23] Salil Vadhan. The complexity of differential privacy. _Tutorials on the Foundations of Cryptography_, pages 347-450, 2017.
* [24] Christopher A. Choquette-Choo, Arun Ganesh, Thomas Steinke, and Abhradeep Guha Thakurta. Privacy amplification for matrix mechanisms. In _International Conference on Learning Representations_, 2024.
* [25] Gilles Barthe and Federico Olmedo. Beyond differential privacy: Composition theorems and relational logic for f-divergences between probabilistic programs. In Fedor V. Fomin, Rusins Freivalds, Marta Kwiatkowska, and David Peleg, editors, _Automata, Languages, and Programming_, pages 49-60, 2013.
* [26] Cynthia Dwork and Guy N Rothblum. Concentrated differential privacy. _arXiv preprint arXiv:1603.01887_, 2016.
* [27] Mark Bun and Thomas Steinke. Concentrated differential privacy: Simplifications, extensions, and lower bounds. In _Theory of Cryptography Conference_, pages 635-658, 2016.
* [28] Yu-Xiang Wang, Borja Balle, and Shiva Prasad Kasiviswanathan. Subsampled renyi differential privacy and analytical moments accountant. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 1226-1235, 2019.
* [29] Ilya Mironov, Kunal Talwar, and Li Zhang. Renyi differential privacy of the sampled gaussian mechanism. _arXiv preprint arXiv:1908.10530_, 2019.
* [30] Yuqing Zhu and Yu-Xiang Wang. Poisson subsampled renyi differential privacy. In _International Conference on Machine Learning_, pages 7634-7642, 2019.
* [31] Borja Balle, Gilles Barthe, Marco Gaboardi, Justin Hsu, and Tetsuya Sato. Hypothesis testing interpretations and renyi differential privacy. In _Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics_, volume 108, pages 2496-2506, 2020.
* [32] Shahab Asoodeh, Jiachun Liao, Flavio P Calmon, Oliver Kosut, and Lalitha Sankar. A better bound gives a hundred rounds: Enhanced privacy guarantees via f-divergences. In _IEEE International Symposium on Information Theory (ISIT)_, 2020.
* [33] David M. Sommer, Sebastian Meiser, and Esfandiar Mohammadi. Privacy loss classes: The central limit theorem in differential privacy. _Proceedings on Privacy Enhancing Technologies_, 2019(2):245-269, 2019.
* [34] Zhiqi Bu, Jinshuo Dong, Qi Long, and Weijie J Su. Deep learning with gaussian differential privacy. _Harvard data science review_, 2020(23):10-1162, 2020.
* [35] Hua Wang, Sheng Gao, Huanyu Zhang, Milan Shen, and Weijie J Su. Analytical composition of differential privacy via the edgeworth accountant. _arXiv preprint arXiv:2206.04236_, 2022.
* [36] Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with differentially private updates. In _IEEE global conference on signal and information processing_, pages 245-248, 2013.

* [37] Borja Balle, Gilles Barthe, and Marco Gaboardi. Privacy profiles and amplification by subsampling. _Journal of Privacy and Confidentiality_, 10(1), 2020.
* [38] Jiayuan Ye and Reza Shokri. Differentially private learning needs hidden state (or much faster convergence). _Advances in Neural Information Processing Systems_, 35:703-715, 2022.
* [39] Cedric Villani. _Couplings and changes of variables_, pages 5-20. 2009. ISBN 978-3-540-71050-9.
* [40] Ameya Daigavane, Gagan Madan, Aditya Sinha, Abhradeep Guha Thakurta, Gaurav Aggarwal, and Prateek Jain. Node-level differentially private graph neural networks. In _ICLR 2022 Workshop on PAIR\({}^{\text{\textregistered}}\)2Struct: Privacy, Accountability, Interpretability, Robustness, Reasoning on Structured Data_, 2022.
* [41] Morgane Ayle, Jan Schuchardt, Lukas Gosch, Daniel Zugner, and Stephan Gunnemann. Training differentially private graph neural networks with random walk sampling. In _Workshop on Trustworthy and Socially Responsible Machine Learning, NeurIPS_, 2022.
* [42] Zihang Xiang, Tianhao Wang, and Di Wang. Preserving Node-level Privacy in Graph Neural Networks. In _IEEE Symposium on Security and Privacy (SP)_, pages 4714-4732, 2024.
* [43] Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil Vadhan. Differentially private release and learning of threshold functions. In _2015 IEEE 56th Annual Symposium on Foundations of Computer Science_, pages 634-649, 2015.
* [44] Jonathan Ullman. Cs7880: Rigorous approaches to data privacy. https://www.khoury.northeastern.edu/home/jullman/cs7880s17/HW1sol.pdf, 2017. Accessed May 21, 2024.
* [45] Google Differential Privacy Team. Privacy loss distributions. https://raw.githubusercontent.com/google/differential-privacy/main/common_docs/Privacy_Loss_Distributions.pdf, 2024. Accessed May 22, 2024.
* [46] Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithms and tight error bounds. In _IEEE 55th annual symposium on foundations of computer science_, pages 464-473, 2014.
* [47] Yu-Xiang Wang, Stephen Fienberg, and Alex Smola. Privacy for free: Posterior sampling and stochastic gradient monte carlo. In _International Conference on Machine Learning_, pages 2493-2502, 2015.
* [48] Thomas Steinke. Composition of differential privacy & privacy amplification by subsampling. _arXiv preprint arXiv:2210.00597_, 2022.
* [49] Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified robustness to adversarial examples with differential privacy. In _IEEE Symposium on Security and Privacy_, pages 656-672, 2019.
* [50] Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Certified adversarial robustness with additive noise. _Advances in neural information processing systems_, 2019.
* [51] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In _International Conference on Machine Learning_, 2019.
* [52] Alexander Levine and Soheil Feizi. Robustness certificates for sparse adversarial attacks by randomized ablation. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 4585-4593, 2020.
* [53] Aleksandar Bojchevski, Johannes Klicpera, and Stephan Gunnemann. Efficient robustness certificates for discrete data: Sparsity-aware randomized smoothing for graphs, images and more. In _International Conference on Machine Learning_, 2020.
* [54] Marc Fischer, Maximilian Baader, and Martin Vechev. Certified defense to image transformations via randomized smoothing. _Advances in Neural information processing systems_, 2020.

* Wu et al. [2021] Yihan Wu, Aleksandar Bojchevski, Aleksei Kuvshinov, and Stephan Gunnemann. Completing the picture: Randomized smoothing suffers from curse of dimensionality for a large family of distributions. In _International Conference on Artificial Intelligence and Statistics_, 2021.
* Kumar and Goldstein [2021] Aounon Kumar and Tom Goldstein. Center smoothing: Certified robustness for networks with structured outputs. _Advances in Neural Information Processing Systems_, 2021.
* Schuchardt and Gunnemann [2022] Jan Schuchardt and Stephan Gunnemann. Invariance-aware randomized smoothing certificates. In _Advances in Neural Information Processing Systems_, 2022.
* Alfarra et al. [2022] Motasem Alfarra, Adel Bibi, Naeemullah Khan, Philip HS Torr, and Bernard Ghanem. DeformRS: Certifying input deformations with randomized smoothing. In _Proceedings of the AAAI Conference on Artificial Intelligence_, number 6, pages 6001-6009, 2022.
* Scholten et al. [2022] Yan Scholten, Jan Schuchardt, Simon Geisler, Aleksandar Bojchevski, and Stephan Gunnemann. Randomized message-interception smoothing: Gray-box certificates for graph neural networks. In _Advances in Neural Information Processing Systems_, 2022.
* Murarev and Petiushko [2022] Nikita Murarev and Aleksandr Petiushko. Certified robustness via randomized smoothing over multiplicative parameters of input transformations. In _International Joint Conference on Artificial Intelligence_, 2022.
* Sukenik et al. [2022] Peter Sukenik, Aleksei Kuvshinov, and Stephan Gunnemann. Intriguing properties of input-dependent randomized smoothing. In _International conference on machine learning_, 2022.
* Pautov et al. [2022] Mikhail Pautov, Olesya Kuznetsova, Nurislam Turysnbek, Aleksandr Petiushko, and Ivan Oseledets. Smoothed embeddings for certified few-shot learning. _Advances in Neural Information Processing Systems_, 2022.
* Schuchardt et al. [2023] Jan Schuchardt, Tom Wollschlager, Aleksandar Bojchevski, and Stephan Gunnemann. Localized randomized smoothing for collective robustness certification. In _International Conference on Learning Representations_, 2023.
* Rumezhak et al. [2023] Taras Rumezhak, Francisco Girbal Eiras, Philip HS Torr, and Adel Bibi. Rancer: Non-axis aligned anisotropic certification with randomized smoothing. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 4672-4680, 2023.
* Saxena et al. [2023] Aman Saxena, Tom Wollschlager, Nicola Franco, Jeanette Miriam Lorenz, and Stephan Gunnemann. Randomized smoothing-inspired quantum encoding schemes with formal robustness guarantees. In _Quantum Techniques in Machine Learning_, 2023.
* Pfrommer et al. [2023] Samuel Pfrommer, Brendon G. Anderson, and Somayeh Sojoudi. Projected randomized smoothing for certified adversarial robustness. _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856.
* Schuchardt et al. [2023] Jan Schuchardt, Yan Scholten, and Stephan Gunnemann. Provable adversarial robustness for group equivariant tasks: Graphs, point clouds, molecules, and more. In _Advances in Neural Information Processing Systems_, 2023.
* Huang et al. [2023] Zhuoqun Huang, Neil Marchant, Keane Lucas, Lujo Bauer, Olya Ohrimenko, and Benjamin I. P. Rubinstein. RS-Del: Edit distance robustness certificates for sequence classifiers via randomized deletion. In _Advances in Neural Information Processing Systems_, NeurIPS, 2023.
* Scholten et al. [2023] Yan Scholten, Jan Schuchardt, Aleksandar Bojchevski, and Stephan Gunnemann. Hierarchical randomized smoothing. _Advances in Neural Information Processing Systems_, 36, 2023.
* Wang et al. [2023] Chendi Wang, Buxin Su, Jiayuan Ye, Reza Shokri, and Weijie J Su. Unified enhancement of privacy bounds for mixture mechanisms via $f$-differential privacy. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* Altschuler and Talwar [2022] Jason Altschuler and Kunal Talwar. Privacy of noisy stochastic gradient descent: More iterations without more privacy loss. _Advances in Neural Information Processing Systems_, 35, 2022.

* [72] Antonious Girgis, Deepesh Data, and Suhas Diggavi. Renyi differential privacy of the subsampled shuffle model in distributed learning. In _Advances in Neural Information Processing Systems_, volume 34, pages 29181-29192, 2021.
* [73] Ashkan Yousefpour, Igor Shilov, Alexandre Sablayrolles, Davide Testuggine, Karthik Prasad, Mani Malek, John Nguyen, Sayan Ghosh, Akash Bharadwaj, Jessica Zhao, Graham Cormode, and Ilya Mironov. Opacus: User-friendly differential privacy library in PyTorch. _arXiv preprint arXiv:2109.12298_, 2021.
* [74] Borja Balle and Yu-Xiang Wang. Improving the gaussian mechanism for differential privacy: Analytical calibration and optimal denoising. In _International Conference on Machine Learning_, pages 394-403, 2018.
* [75] Ulfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and Abhradeep Thakurta. Amplification by shuffling: From local to central differential privacy via anonymity. In _Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms_, pages 2468-2479, 2019.
* [76] Albert Cheu, Adam Smith, Jonathan Ullman, David Zeber, and Maxim Zhilyaev. Distributed differential privacy via shuffling. In _Advances in Cryptology-EUROCRYPT_, pages 375-403, 2019.

## Appendix A Table of contents

* B Additional experiments
* B.1 Mechanism-agnostic and mechanism-specific bounds
* B.2 Post-hoc and tight group privacy
* C Experimental setup
* C.1 Evaluation of privacy guarantees
* C.2 Computational resources
* C.3 Assets and licenses
* D General setting and definitions
* D.1 Spaces
* D.2 Neighboring relations
* D.3 Mechanisms and subsampling schemes
* D.4 Differential privacy notions
* D.5 Joint convexity
* D.6 Couplings
* E Proof of optimal transport bounds
* E.1 Proof of Theorem 3.3
* E.2 Proof of Theorem 3.4
* E.3 Proof of Proposition 3.5
* F Distance-compatible couplings of multiple distributions
* G Recovering known mechanism-agnostic ADP guarantees
* G.1 Overview of Balle et al. framework
* G.2 Subsumption of Balle et al. framework
* G.3 Deficiencies of mechanism-agnostic ADP bounds.
* H Recovering known mechanism-agnostic RDP guarantees
* H.1 Subsampling without replacement and substitution
* H.2 Poisson subsampling and insertion/removal
* H.3 Graph subsampling without replacement and node modification
* I Novel RDP guarantees
	* I.1 Hybrid neighboring relations
	* I.2 Subsampling without replacement under insertion/removal
	* I.3 Tight mechanism-specific subsampling without replacement for randomized response
* J From mechanism-specific guarantees to dominating pairs
* J.1 Step 1: Eliminating weighted sums
* 2 Step 2: Resolving non-constancy in dataset pairs
	* 2.3 Step 3: Resolving non-constancy in divergence order
* K Recovering known dominating pairs
* K.1 Poisson subsampling and insertion/removal
* K.2 Subsampling without replacement and insertion/removal
* K.3 Subsampling without replacement and substitution
* L Novel results for dominating pairs
* L.1 Subsampling without replacement and substitution
* L.2 Subsampling with replacement and substitution
* M Tight mechanism-specific group privacy amplification
* M.1 Proof of Theorem 3.7
* M.2 Instantiations
* M.3 Tightness
* M.4 Numerical evaluation
* N Tight mechanism-agnostic group privacy amplification
* N.1 ADP guarantee
* N.2 Tightness of ADP guarantee
* N.3 Relation to tight mechanism-specific bound
* N.4 RDP guarantee
* N.5 Asymptotic RDP guarantees
* O Worst-case mixture components
* O.1 Gaussian and Laplacian mixtures
* O.2 Reduction of the divergence to a univariate integral
* O.3 Randomized Response Proofs
* P Towards epoch-level subsampling analysis
* P.1 Problem setting
* P.2 Optimal transport without conditioning
* P.3 Optimal transport with conditioning
* P.4 Experimental Evaluation
* Q Broader impact
Additional experiments

### Mechanism-agnostic and mechanism-specific bounds

#### b.1.1 Randomized response and RDP

In Fig. 7, we repeat the experiment from Fig. 3 with other ratios of batch and dataset sizes. That is, we compare our tight mechanism-specific RDP guarantee from Theorem I.3 to the best known mechanism-agnostic bound from [28] to illustrate that mechanism-agnostic bounds may loose privacy by bounding mixture divergences in terms of component divergences - even outside the group privacy setting. The tight guarantee eliminates constant factors and thus achieves much smaller \(\rho\) for a wide range of \(\alpha\in(1,10^{4}]\), especially for small ratios.

#### b.1.2 Group privacy and ADP

In Figs. 8 and 9 we repeat the experiment from Fig. 4 for other subsampling rates \(r\), standard deviations \(\sigma\), and also with Laplace mechanisms. That is, we compare our tight mechanism-specific group privacy guarantees to the tight mechanism-agnostic bounds derived via the framework of [15]. The results demonstrate that mechanism-specific tightness is a stronger property that can result in better privacy guarantees - although the difference in significantly more pronounced for Laplace mechanisms and/or larger group sizes.

In Fig. 10 we repeat the experiment with randomized response mechanisms. Here, both approaches yield identical guarantees. This verifies that randomized response is indeed the worst-case mechanisms for which the mechanism-agnostic bound is optimal (see [15] and Appendix N.2).

Figure 7: Randomized response under subsampling without replacement, with varying true response probability \(\theta\) and batch-to-dataset ratio \(q\ /\ N\). Theorem I.3 significantly improves upon the baseline for a wide range of \(\alpha\).

## Gaussian mechanism

Figure 8: Gaussian mechanisms under Poisson subsampling, with varying standard deviation \(\sigma\), subsampling rate \(r\), and group size. The tight mechanism-specific guarantees are stronger than the tight mechanism-agnostic bounds, especially for larger group sizes and smaller subsampling rates.

## Appendix A

Figure 9: Laplace mechanisms under Poisson subsampling, with varying scale \(\lambda\), subsampling rate \(r\), and group size. The tight mechanism-specific guarantees are stronger than the tight mechanism-agnostic bounds, especially for larger group sizes.

### Randomized response mechanism

Figure 10: Randomized response under Poisson subsampling, with varying true response probability \(\theta\), subsampling rate \(r\), and group size. Randomized response is the worst-case mechanism for which the mechanism-agnostic guarantee is optimal, so both ansätze yield identical results.

#### b.1.3 Group privacy and RDP

In Fig. 11 we repeat our comparison of mechanism-agnostic (Theorem N.5) and tight mechanism-specific group privacy amplification (Theorem 3.8) with RDP instead of ADP. We observe that the tight guarantee delays the phase transition from a high- to a low-privacy regime that was already observed in [30]. In other words, we have small \(\rho\) for larger \(\alpha\), which is beneficial for conversion to ADP after composition (see conversion formulae in [7, 31]).

Figure 11: Gaussian mechanisms under Poisson subsampling, with varying standard deviation \(\sigma\), subsampling rate \(r\), and group size. The mechanism-specific guarantee delays the phase transition from high to low privacy.

#### b.1.4 Benefit of conditioning

Next, we demonstrate the benefit of coupling multiple conditional distributions over coupling just two subsampling distributions in deriving amplification guarantees. Specifically, we evaluate Proposition H.7, which can be derived via Theorem 3.3, i.e., optimal transport without conditioning. Note that, unlike with the guarantees of Wang et al. [28], this bound depends on both the dataset size and the batch size, not just their ratio. We make the following observations: For large \(\alpha\) Proposition H.7 converges to the baseline. Furthermore, this approach never outperforms the baseline for group size \(1\). Neither does it outperform the baseline for ratios \(q\ /\ L\in\{0.01,0.001\}\). But, for \(q\ /\ L=0.1\), there is a sweet spot of alphas between \(10^{1}\) and \(10^{2}\) in which it offers stronger guarantees. This further reinforces our claim that there is a benefit to treating group privacy and amplification jointly. Furthermore, we observe that in the case of \(q=1\), Proposition H.7 outperforms the baseline for large alpha, since it can capture that one cannot possibly include more than one substituted element in a singleton batch.

Overall, we can conclude that there is a benefit to jointly analyzing group privacy and subsampling in this manner, but that optimal transport without conditioning is not sufficient for tightly analyzing such complicated scenarios.

Figure 12: Proposition H.7 derived from Theorem 3.3 applied to Gaussian mechanism (\(\sigma=5.0\)) under sampling without replacement for varying dataset size \(N\), batch size \(q\), and group size. Optimal transport without conditioning does not always improve upon the baseline.

#### b.1.5 Subsampling with replacement

The previous experiments demonstrated that mechanism-specific analysis can improve upon non-tight mechanism-agnostic and - in the group privacy setting - tight mechanism-agnostic analysis. In the following, we demonstrate that mechanism-specific bounds can improve upon tight mechanism-agnostic bounds even for single-element relations, i.e., group size 1. To this end, we consider subsampling with replacement for Gaussian mechanisms under the substitution relation, comparing Theorem L.5 to Theorem 10 of Balle et al. [15].

Fig. 13 shows the resultant privacy profiles for standard deviation \(\sigma=1\), dataset size \(N=100\), and batch size \(q=8\). For \(\varepsilon\geq 2\), the mechanism-specific bound is more than an order of magnitude smaller. Intuitively, this gap can be explained similarly to the gap in the group privacy setting: The single substituted element can be sampled \(0\), \(1\), \(2\), or up to \(q\) times, with each case causing different levels of privacy leakage. Mechanism-agnostic bounds rely on a binary partitioning of the event space, which is not sufficient for capturing this granular behavior (recall Fig. 2.

Note that due to the relaxations of distance constraints we performed in deriving Theorem L.5, the mechanism-specific bound is not tight. A tight bound might lead to an even larger gap. Thus, this experiments further reinforces that there is a qualitative difference between mechanism-specific and mechanism-agnostic tightness.

Figure 13: Gaussian mechanism (\(\sigma=1\)) under sampling with replacement for single-element substitutions, dataset size \(N=100\), and batch size \(q=8\). Using the mechanism-specific bound results in stronger privacy guarantees.

### Post-hoc and tight group privacy

#### b.2.1 Single-iteration ADP

In Figs. 14 to 15 we repeat the experiment from Fig. 5 for other subsampling rates \(r\), standard deviations \(\sigma\), and mechanisms. That is, we compare our tight mechanism-specific guarantees to post-hoc use of the group privacy property. For all mechanisms, the tight mechanism-specific analysis yields stronger privacy guarantees than the baseline - particularly for large group sizes. However, we interestingly see that with increasing base mechanisms noise level and decreasing subsampling rate, the post-hoc bound converges towards the tight bound.

## Gaussian mechanism

Figure 14: Gaussian mechanisms under Poisson subsampling, with varying standard deviation \(\sigma\), subsampling rate \(r\), and group size. Analyzing group privacy and subsampling jointly instead of in a post-hoc manner offers stronger guarantees.

## Appendix A

Figure 15: Laplace mechanisms under Poisson subsampling, with varying scale \(\lambda\), subsampling rate \(r\), and group size. Analyzing group privacy and subsampling jointly instead of in a post-hoc manner offers stronger guarantees.

## Randomized response mechanism

Figure 16: Randomized response mechanisms under Poisson subsampling, with varying true response probability \(\theta\), subsampling rate \(r\), and group size. Analyzing group privacy and subsampling jointly instead of in a post-hoc manner offers stronger guarantees.

#### b.2.2 Single-iteration RDP

In Fig. 17 we repeat our comparison of tight mechanism-specific group privacy amplification and post-hoc group privacy for RDP instead of ADP. The tight analysis yields stronger guarantees and delays the phase transition from a high- to a low-privacy regime. However, as with ADP, the post-hoc bound can be a good upper bound for small subsampling rates and very private base mechanisms.

Figure 17: Gaussian mechanisms under Poisson subsampling, with varying standard deviation \(\sigma\), subsampling rate \(r\), and group size. Analyzing group privacy and subsampling jointly instead of in a post-hoc manner delays the phase transition from high to low privacy. For very private base mechanisms and small subsampling rates, the baseline is nevertheless a good upper bound.

#### b.2.3 PLD Accounting

In Figs. 18 to 21 we repeat our comparison of tight mechanism-specific and post-hoc group privacy amplification guarantees under composition from. We consider different combinations of subsampling rate \(r\), standard deviation \(\sigma\), privacy parameter \(\varepsilon\), as well as Laplace mechanisms. Even in high privacy scenarios, where the mechanism-specific and post-hoc analysis yield similar results on a single-iteration level, the mechanism-specific guarantees are significantly stronger under composition. Specifically, the post-hoc analysis diverges to much larger \(\delta\) after some number of iterations. In settings with moderate privacy (e.g. \(\sigma=1\)) or large group sizes (e.g. \(16\)), the baseline diverges after less than \(100\) iterations.

**Gaussian mechanism (\(r=0.001\))**

Figure 18: Self-composed Gaussian mechanisms under Poisson subsampling with subsampling rate \(r=0.001\) and varying standard deviation \(\sigma\), privacy parameter \(\varepsilon\) and group size. For sufficiently large group sizes, the post-hoc analysis divergence from the tight mechanism-specific guarantee within less than \(1000\) iterations.

## Gaussian mechanism (\(r=0.01\))

Figure 19: Self-composed Gaussian mechanisms under Poisson subsampling with subsampling rate \(r=0.01\) and varying standard deviation \(\sigma\), privacy parameter \(\varepsilon\) and group size. For sufficiently large group sizes, the post-hoc analysis divergence from the tight mechanism-specific guarantee within less than \(1000\) iterations.

### Laplace mechanism (\(r=0.001\))

Figure 20: Self-composed Laplace mechanisms under Poisson subsampling with subsampling rate \(r=0.001\) and varying scale \(\lambda\), privacy parameter \(\varepsilon\) and group size. For sufficiently large group sizes, the post-hoc analysis divergence from the tight mechanism-specific guarantee within less than \(1000\) iterations.

### Laplace mechanism (\(r=0.01\))

Figure 21: Self-composed Laplace mechanisms under Poisson subsampling with subsampling rate \(r=0.001\) and varying scale \(\lambda\), privacy parameter \(\varepsilon\) and group size. For sufficiently large group sizes, the post-hoc analysis divergence from the tight mechanism-specific guarantee within less than \(1000\) iterations.

#### b.2.4 RDP accounting

Finally, we compare tight group privacy amplification to the post-hoc approach for RDP accounting. We begin with the Gaussian mechanism and subsampling rate \(r=0.001\) in Fig. 22. Our bound offers stronger group privacy guarantees for \(\sigma=1.0\), but both results are almost identical for very private base mechanisms with \(\sigma=5.0\). We suspect that this is because the phase transition from high to low privacy (recall Fig. 17), which the tight guarantee delays, gets shifted to very high \(\alpha\) regions that are not useful for conversion to \((\varepsilon,\delta)\)-DP.

Our observations for randomized response mechanisms (see Fig. 23 are also consistent with earlier results: For moderate subsampling rates \(r=0.1\), our guarantees are stronger, particularly for group size \(8\) and large numbers of iterations. But, when decreasing the subsampling rate to \(0.001\), both methods are almost identical. Nevertheless, group privacy amplification can demonstrably improve upon a direct combination of independently derived group privacy and amplification guarantees.

Note that these observations are mostly of theoretic interest. In practice, one would use PLD accounting, which tightly characterizes the composed mechanism's privacy leakage, instead of the looser RDP accounting. As shown in Figs. 18 to 21, the tight mechanisms-specific analysis drastically outperforms the post-hoc analysis for PLD accounting.

**Gaussian mechanism**

### Randomized response mechanism

Figure 23: Self-composed randomized response mechanisms under Poisson subsampling with privacy parameter \(\delta=10^{-8}\), true response probability \(\theta=0.6\), and varying subsampling rate \(r\) and group size. The tight mechanism-specific analysis yields better privacy guarantees than the post-hoc baseline, except for small \(r\) where the baseline is a good upper bound.

Figure 22: Self-composed Gaussian mechanisms under Poisson subsampling with privacy parameter \(\delta=10^{-8}\), subsampling rate \(r=0.001\), and varying standard deviation \(\sigma\) and group size. The tight mechanism-specific analysis yields better privacy guarantees than the post-hoc baseline, except for large \(\sigma\) where the baseline is a good upper bound.

#### b.2.5 Model utility

The previous results demonstrated that handling group privacy via a tight mechanism-specific analysis allows for a larger number of compositions, i.e., iterations at a given privacy budget than post-hoc use of the group privacy property. In the following, we demonstrate that this increased number of iterations can in fact result in increased utility for group-private machine learning models.

We train a convolutional neural network (2 convolution layers with kernel sizes \(3\) and \(32\) / \(64\) channels, followed by two linear layers with hidden dimension \(128\)) for image classification on MNIST (\(55000\) training, \(5000\) validation, \(10000\) test samples). We set the gradient clipping norm of DP-SGD [6] to \(C=10^{-4}\), the Gaussian noise standard deviation to \(0.6\cdot C\), and the subsampling rate to \(r=64\) / \(55000\). The optimizer is ADAM with learning rate \(1e-3\). If the privacy budget is not used up earlier, training is terminated after \(8\) epochs, with \(\left\lceil 55000\right./64\right]\) iterations per epoch.

Even with a large privacy budget of \(\varepsilon=8\) and \(\delta=1e-5\), training with the post-hoc privacy analysis needs to terminate after less than \(200\) iterations. With the mechanism-specific analysis, \(\delta\) does not even exceed \(1e-7\) after \(8\) epochs, i.e., the model could potentially be trained for even more iterations. The resultant validation accuracy's are \(79.6\%\) and \(91.2\%\), respectively. This showcases the superior privacy-utility trade-off that can be achieved by analyzing subsampling and group privacy jointly.

Figure 24: Differentially private training of a \(2\)-layer convolutional network on MNIST with PLD accounting for group size \(2\). Our tight mechanism-specific analysis allows us to train for significantly more epochs or to terminate training after \(8\) epochs with less privacy leakage and higher accuracy.

Experimental setup

### Evaluation of privacy guarantees

In all experiments, we assume that all mechanisms have underlying sensitivity \(1\) w.r.t. \(\ell_{\infty}\) (randomized response), \(\ell_{1}\) (Laplace), or \(\ell_{2}\) (Gaussian) output norms. We thus only specify noise parameters \(\sigma\) or \(\lambda\), rather than the ratio of \(\sigma\) or \(\lambda\) and the sensitivities.

#### c.1.1 Single-iteration approximate differential privacy

**Privacy parameters.** We evaluate all guarantees for \(121\) equidistant values of \(\varepsilon\) in \([0,4]\) and \(121\) logspace-equidistant values in \(10^{-3},10^{1}\), i.e., \(\{10^{x}\mid x\in\{-3,-3+\frac{4}{120},\ldots,1\}\}\). We clip values of \(\delta(\varepsilon)\) that are larger than \(1\) to \([0,1]\). For our baselines, we enforce that \(\delta(\varepsilon)\) is monotonically decreasing by taking a running minimum from larger to smaller \(\varepsilon\) (this may improve but never worsens the baselines).

**Mechanism-agnostic group privacy baseline.** As our mechanism-agnostic group privacy baselines, we use Proposition N.1. We take the maximum over the two cases \((K_{+}=K,K_{-}=0)\) and \((K_{+}=0,K_{-}=K)\), where \(K\) is the group size. For Gaussian and Laplace mechanisms, we evaluate group privacy profiles \(\delta_{k}(\varepsilon)\) by determining \(\delta(\varepsilon)\) for the same mechanism with sensitivity \(k\). For randomized response, we let \(\delta_{k}=\delta_{1}\). This is referred to to as "white-box" group privacy in [15]). We evaluate this baseline analytically.

**Post-hoc group privacy baseline.** For our post-hoc baseline, we combine the tight Poisson subsampling guarantee for insertion/removal from [15] with the group privacy property of approximate differential privacy (see Appendix C.1.4 below). We evaluate this baseline analytically.

**Tight mechanism-specific group privacy.** In all ADP figures "specific" refers to our tight group privacy guarantees for Gaussian (Theorem 3.8), Laplace (Theorem M.2), or randomized response mechanisms (Theorem M.3). We take the maximum over all \(K_{+},K_{-}\in\mathbb{N}_{0}\) with \(K_{+}+K_{-}=K\), where \(K\) is the group size. To evaluate the bounds for Gaussian and Laplace mechanisms, we pessimistically invert the privacy loss of dominating pairs \(P,Q\) via binary search (see details in Appendix M.4), as implemented in the dp_accounting library [45]. We set the precision to \(10^{-6}\), i.e., find the global optimum over multiples of \(10^{-6}\). The bound for the randomized response mechanism is evaluated analytically in \(\mathcal{O}(1)\).

#### c.1.2 Single-iteration Renyi differential privacy

**Privacy parameters**. We evaluate all guarantees for \(\alpha\in\{2,3,\ldots,1000\}\), as well as \(121\) logspace-equidistant values in \([0,10^{4}]\) rounded to the next smallest integer (without \(0\) or \(1\)), and \(121\) logspace-equidistance values in \((1,10]\). For our baselines, we enforce that \(\rho(\alpha)\) is monotonically increasing by taking a running minimum from smaller to larger \(\alpha\) (this may improve but never worsens the baselines).

**Mechanism-agnostic group privacy baseline.** As our mechanism-agnostic group privacy baselines, we use Proposition N.1. We take the maximum over the two cases \((K_{+}=K,K_{-}=0)\) and \((K_{+}=0,K_{-}=K)\), where \(K\) is the group size. For integer \(\alpha\), we use the binomial expansion in Eq. (33) (without factor \(2\)). For Gaussian and Laplace mechanisms, we evaluate group privacy profiles \(\zeta_{k}(\alpha)\) by determining \(\zeta(\alpha)\) for the same mechanism with sensitivity \(k\). For randomized response, we let \(\zeta_{k}=\zeta_{1}\). For continuous \(\alpha\), we apply \(\tanh\)-\(\sinh\)-quadrature with \(50\) digits of decimal precision to Eq. (33)

**Post-hoc group privacy baseline.** For our post-hoc baselines, we combine either tight Poisson subsampling guarantee for insertion/removal from [30] or the subsampling without replacement guarantee for insertion/removal from [28] with the group privacy property of approximate differential privacy (see Appendix C.1.4 below). For Poisson subsampling and integer \(\alpha\), we use the binomial expansion from [30] (without factor \(2\)). For continuous \(\alpha\), we apply \(\tanh\)-\(\sinh\)-quadrature with \(50\) digits of decimal precision. For subsampling without replacement, we use the improved self-consistency bound (Theorem 27 from [28]), which we evaluate via \(\tanh\)-\(\sinh\)-quadrature with \(50\) digits of decimal precision due to the intractability of the nested binomial expansions for larger \(\alpha\).

**Tight mechanism-specific group privacy.** In all RDP figures "specific" refers to our tight group privacy guarantees for Gaussian (Theorem 3.8), Laplace (Theorem M.2), or randomized response mechanisms (Theorem M.3). We take the maximum over all \(K_{+},K_{-}\in\mathbb{N}_{0}\) with \(K_{+}+K_{-}=K\), where \(K\) is the group size. To evaluate the bounds for Gaussian and Laplace mechanisms, we use \(\tanh\)-\(\sinh\)-quadrature with \(50\) digits of decimal precision (see discussion in Appendix M.4). The bound for the randomized response mechanism is evaluated analytically in \(\mathcal{O}(1)\).

#### c.1.3 Privacy accounting

**RDP accounting.** For RDP accounting, we simply evaluate the single-iteration guarantees as described in Appendix C.1.2. We then multiply the \(\rho(\alpha)\) with the number of iterations, apply the improved RDP-to-ADP formula from [31], and take the minimum over all obtained values. For the post-hoc baseline, we apply the group privacy property before composition (which is equivalent to applying it after composition, due to additivity of composition).

**PLD accounting.** For PLD accounting with dominating pairs, we use the implementation of the dp_accounting library [45]. To quantize the privacy loss distribution, we use "connect the dots" [20] with pessimistic estimates, a discretization interval size of \(10^{-3}\), and truncation of \(e^{-50}\) of the probability mass. During composition, we truncate \(10^{-15}\) of the tail mass. For the post-hoc baseline, we apply the group privacy property after composition.

#### c.1.4 Post-hoc group privacy

**ADP.** For RDP, we use the following result from the proof of Lemma 2.2 in [23], which provides tighter guarantees than Lemma 2.2 itself: Let \(M\) be \((\varepsilon,\delta)\)-DP under neighboring relation \(\simeq_{\mathbb{X}}\). Then, \(M\) is \((\varepsilon^{\prime},\delta^{\prime})\)-DP with \(\varepsilon^{\prime}=\varepsilon\cdot K\) and \(\delta^{\prime}=\sum_{k=0}^{K-1}e^{k\cdot\varepsilon}\cdot\delta\).

**RDP.** For RDP, we use the following result from Corollary 4 of [7], which provides tighter guarantees than the upper bound in their Proposition 2. Let \(D_{\alpha}\) be \(\log(\Lambda_{\alpha})\,/\,(\alpha-1)\). This \(D_{\alpha}\) fulfills the following triangle inequality:

\[D_{\alpha}(p||q)\leq\frac{\alpha-\frac{1}{2}}{\alpha-1}D_{2\alpha}(p||r)+ \frac{\alpha}{\alpha-1}D_{2\alpha-1}(r||q).\]

We recursively apply this bound \(\log_{2}(K)\) times when evaluating the group privacy of our baselines for groups of size \(K\).

### Computational resources

We conduct all experiments on a set of Xeon E5-2630 v4 CPUs @ \(2.2\,\mathrm{GHz}\).

We use one worker and job per subsampling theorem, base mechanism noise level, subsampling rate, and combination of group privacy parameters \(K_{-},K_{+}\). Per job, we allocate \(2\) CPU cores, \(4\,\mathrm{GB}\) and \(10\) minutes of runtime (in practice, most jobs were completed in a few seconds). In total, we ran \(13060\) such jobs for ADP, \(9731\) for RDP, and \(984\) for RDP accounting. We estimate that over the course of the full research project twice as many jobs were executed.

### Assets and licenses

To perform high-precision quadrature for RDP guarantees, we use the \(\tanh\)-\(\sinh\) quadrature implementation from the mpmath library (version \(1.3.0.\)), which is available under the BSD-\(3\)-Clause license at https://github.com/mpmath/mpmath.

For PLD accounting and evaluation of ADP guarantees via bisection, we use and extend the dp_accounting library [45] (commit 0b109e959470c43e9f177d5411603b70a56cdc7a), which is available under Apache-\(2.0\) license at https://github.com/google/differential-privacy

For conversion from RDP to ADP guarantees, we use the get_privacy_spent method implemented in the Opacus library [73] (version \(1.4.1\)), which is available under Apache-\(2.0\) license at https://github.com/pytorch/opacus.

An implementation will be made available at https://cs.cit.tum.de/daml/group-amplification.

General setting and definitions

In the following, we generalize some of the definitions introduced in Sections 2 and 3 to their measure-theoretic equivalents. The purpose of this generalization is to handle both continuous and discrete spaces, base mechanisms, and subsampling schemes, without having to make constant case distinctions.

We use these more general definitions throughout the remaining appendix sections. The theoretic results presented in Section 3 follow as special cases.

### Spaces

Unlike before, we assume that dataset space \(\mathbb{X}\) is some arbitrary space, whose elements do not have to be sets. Instead, datasets \(x\in\mathbb{X}\) can also be graphs, sequences or any other data collection. We further assume that the batch space is a measurable space \((\mathbb{Y},\mathcal{Y})\) with \(\sigma\)-algebra \(\mathcal{Y}\). For example, \(\mathbb{Y}\) can be composed of subsets, subgraphs, or subsequences. We also assume the output space to be a measurable space \((\mathbb{Z},\mathcal{Z})\). Finally, we assume the existence of some measure \(\lambda\) on the output space, such as the Lebesgue measure for continuous \(\mathbb{Z}\) or the counting measure \(\#\) for discrete \(\mathbb{Z}\).

Whenever we consider Poisson subsampling, we assume that \(\mathbb{X}\subseteq\mathcal{P}(\mathbb{A})\), \(\mathbb{Y}=\{y\subseteq x\mid x\in\mathbb{X}\}\), \(\mathcal{Y}=\mathcal{P}(\mathbb{Y})\), where \(\mathbb{A}\) is some discrete, finite set and \(\mathcal{P}(\cdot)\) is the powerset.

Whenever we consider subsampling without replacement with batch size \(q\), we assume that \(\mathbb{X}\subseteq\{x\in\mathcal{P}(\mathbb{A})\mid|x|>q\}\), \(\mathbb{Y}=\{y\subseteq x\mid x\in\mathbb{X},|y|=q\}\), \(\mathcal{Y}=\mathcal{P}(\mathbb{Y})\), where \(\mathbb{A}\) is some discrete, finite set and \(\mathcal{P}(\cdot)\) is the powerset.

### Neighboring relations

For set-valued datasets and batches, the insertion/removal relation and the substitution relation are formally defined as follows:

**Definition D.1**.: Sets \(x,x^{\prime}\in\mathbb{X}\) are related by the insertion/removal relation (\(x\simeq_{\pm}x^{\prime}\)) if \(x\neq x^{\prime}\) and there is some \(a\) such that \(x^{\prime}=x\cup\{a\}\) or \(x^{\prime}=x\setminus\{a\}\).

**Definition D.2**.: Sets \(x,x^{\prime}\in\mathbb{X}\) are related by the substitution relation (\(x\simeq_{\Delta}x^{\prime}\)) if there is some \(a\in x\) and \(a^{\prime}\notin x\) such that \(x^{\prime}=x\setminus\{a\}\cup\{a^{\prime}\}\).

In our general setting, we also allow non-symmetric neighboring relations.

### Mechanisms and subsampling schemes

As before, the term "mechanism" refers to random functions that map to the output space \((\bm{Z},\mathcal{Z})\). Formally, a random function \(M:\mathbb{X}\to\mathbb{Z}\) is a family of random variables indexed by elements of \(\mathbb{X}\).

**Definition D.3**.: A random function \(M:\mathbb{X}\to\mathbb{Z}\) is a function \(M:\mathbb{X}\times\Omega\to\mathbb{Z}\), where \((\Omega,\mathcal{F},P)\) is some probability space and all \(M(x,\cdot):\omega\mapsto M(x,\omega)\) are measurable.

We write \(P_{M_{x}}:\mathcal{Z}\to[0,1]\) for the distribution of random variable \(M(x)\). We further assume that each \(P_{M_{x}}\) is absolutely continuous w.r.t. the aforementioned output measure \(\lambda\), i.e., \(\forall x\in\mathbb{X}:P_{M_{x}}\ll\lambda\), and write \(m_{x}:\mathbb{Z}\to\mathbb{R}_{+}\) for the corresponding Radon-Nikodym derivative \(\mathrm{d}P_{M_{x}}\ /\ \mathrm{d}\lambda\). For example, when the output space \((\mathbb{Z},\mathcal{Z})\) is continuous and \(\lambda\) is the Lebesgue measure, then \(m_{x}\) is the density.

Similarly, we define our base mechanism to be a random function \(B:\mathbb{Y}\to\mathbb{Z}\) and write \(P_{B_{y}}:\mathcal{Z}\to[0,1]\) for the distribution of base mechanism outputs given a batch \(y\in\mathbb{Y}\). We assume that each \(P_{B_{y}}\) is absolutely continuous w.r.t. output measure \(\lambda\) and write \(b_{y}:\mathbb{Z}\to\mathbb{R}_{+}\) for \(\mathrm{d}P_{B_{y}}\ /\ \mathrm{d}\lambda\).

Finally, we define our subsampling scheme to be a random function \(S:\mathbb{X}\to\mathbb{Z}\) and write \(P_{S_{x}}:\mathcal{Y}\to[0,1]\) for the distribution of batches given a dataset \(x\in\mathbb{X}\). We generally do not require \(P_{S_{x}}\) to be absolutely continuous w.r.t. some other measure. When \(P_{S_{x}}\) is absolutely continuous w.r.t. counting measure \(\#\), we write \(s_{x}:\mathbb{Y}\to[0,1]\) for the corresponding probability mass function \(\mathrm{d}P_{S_{x}}\ /\ \mathrm{d}\#\).

In particular, Poisson subsampling and subsampling without replacement are defined as follows:

**Definition D.4**.: Poisson subsampling with rate \(r\in[0,1]\) has probability mass function \(s_{x}(y)=r^{|y|}(1-r)^{|x|-|y|}\) for batches \(y\subseteq x\).

Definition D.5: Subsampling without replacement with batch size \(q\) has probability mass function \(s_{x}(y)=\binom{|x|}{q}^{-1}\) for batches \(y\subseteq x\) with \(|y|=q\).

As before, our goal is to provide privacy guarantees for subsampled mechanisms \(M=B\circ S\). Similar to our discussion in Section 2, its distribution \(P_{M_{x}}\) given a dataset \(x\in\mathbb{X}\) is a mixture3 with

Footnote 3: assuming that \((y,Z)\mapsto P_{B_{y}}(Z)\) is a valid Markov kernel

\[m_{x}(z)=\int_{\mathbb{Y}}b_{y}(z)\;\mathrm{d}P_{S_{x}}(y).\] (5)

There is one component per batch \(y\) from batch space \(\mathbb{Y}\), and the weights depend on subsampling distribution \(P_{S_{x}}\).

### Differential privacy notions

Since we no longer require the output space to be continuous, we also need to generalize the definitions of approximate differential privacy, Renyi differential privacy, dominating pairs, and the divergences underlying their definition. For this, recall that \(\lambda\) is our assumed measure on output space \((\mathbb{Z},\mathcal{Z})\).

Definition D.6: For \(\varepsilon\geq 0\), a mechanism \(M:\mathbb{X}\to\mathbb{Z}\) is \((\varepsilon,\delta)\)-DP under relation \(\simeq_{\mathbb{X}}\) if \(\forall x\simeq_{\mathbb{X}}x^{\prime}:H_{\exp(\varepsilon)}(m_{x}||m_{x^{ \prime}})\leq\delta\) and \(H_{\exp(\varepsilon)}(m_{x^{\prime}}||m_{x})\leq\delta\) with hockey stick divergence

\[H_{\alpha}(m_{x}||m_{x^{\prime}})=\int_{\mathbb{Z}}\max\{m_{x}(z)\;/\;m_{x^{ \prime}}(z)-\alpha,0\}\cdot m_{x^{\prime}}(z)\;\mathrm{d}\lambda(z).\] (6)

Definition D.7: A pair of distributions \((P,Q)\) with \(p=\mathrm{d}P\;/\;\mathrm{d}\lambda\) and \(q=\mathrm{d}Q\;/\;\mathrm{d}\lambda\) is a dominating pair for mechanism \(M\) under neighboring relation \(\simeq_{\mathbb{X}}\), if \(H_{\alpha}(m_{x}||m_{x^{\prime}})\leq H_{\alpha}(p||q)\) for all \(x\simeq_{\mathbb{X}}x^{\prime}\) and all \(\alpha\geq 0\).

Definition D.8: For \(\alpha\geq 1\), a mechanism \(M:\mathbb{X}\to\mathbb{R}^{D}\) is \((\alpha,\rho)\)-RDP under neighboring relation \(\simeq_{\mathbb{X}}\) if \(\forall x\simeq_{\mathbb{X}}x^{\prime}:\log(\Lambda_{\alpha}(m_{x}||m_{x^{ \prime}}))\;/\;(\alpha-1)\leq\rho\) and \(\log(\Lambda_{\alpha}(m_{x^{\prime}}||m_{x}))\;/\;(\alpha-1)\leq\rho\) with

\[\Lambda_{\alpha}(m_{x}||m_{x^{\prime}})=\int_{\mathbb{R}^{D}}m_{x}(z)^{\alpha }\cdot m_{x^{\prime}}(z)^{1-\alpha}\;\mathrm{d}z.\] (7)

Importantly, note that \(\Lambda_{\alpha}\)_is not the Renyi divergence_ as used in [7]. It is its \(\alpha\)th moment, i.e., a scaled and exponentiated Renyi divergence. This is why a logarithm and quotient appears in Definition D.8. We use this definition, so that we can simultaneously discuss ADP and RDP without notational clutter.

### Joint convexity

The joint convexity of \(\Psi_{\alpha}\in\{H_{\alpha},\Lambda_{\alpha}\}\) is not limited to densitites, but also applies to other non-negative Radon-Nikodym derivatives of probability distributions [37, 38]:

Lemma D.9: _Consider arbitrary Radon-Nikodym derivatives \(f_{1}^{(1)},f_{2}^{(1)},f_{1}^{(2)},f_{2}^{(2)}:\mathbb{Z}\to\mathbb{R}_{+}\) and weight \(w\in[0,1]\). Then,_

\[\Psi_{\alpha}(wf_{1}^{(1)}+(1-w)f_{2}^{(1)}||wf_{1}^{(2)}+(1-w)f_{2}^{(2)}) \leq w\Psi_{\alpha}(f_{1}^{(1)}||f_{1}^{(2)})+(1-w)\Psi_{\alpha}(f_{2}^{(1)}|| f_{2}^{(2)}).\]

### Couplings

As with the discrete, finite-support subsampling distributions we considered in Section 3, the key tool we use for analyzing amplification are couplings. However, we no longer assume the subsampling scheme to always have a mass function. We thus use a more general notion of couplings between distributions instead of couplings between mass functions:

Definition D.10: A coupling between probability measures \(P_{1},\ldots,P_{N}\) on space \((\mathbb{Y},\mathcal{Y})\) is a probability measure \(\Gamma\) on product space \((\mathbb{Y},\mathcal{Y})^{N}\), where the \(n\)th marginal is \(P_{n}\), i.e., \(\Gamma\circ\pi_{n}^{-1}=P_{n}\) with projection \(\pi_{n}(\bm{y})=y_{n}\).

Here, \(\circ\) is the composition operator and \(\pi_{n}^{-1}\) is the preimage (not necessarily the inverse) of the projection function. As before, when considering a coupling between two distributions \(P_{1}\), \(P_{2}\), the value \(\Gamma(T,R)\) specifies for all events \(T,R\in\mathcal{Y}\) how much probability should be transported from \(P_{1}(T)\) to \(P_{2}(R)\) to transform \(P_{1}\) into \(P_{2}\).

Proof of optimal transport bounds

### Proof of Theorem 3.3

In the following, we show a more general statement for the general setting introduced in Appendix D. Theorem 3.3 immediately follows in the special case where batch space \(\mathbb{Y}\) is finite and discrete, and the subsampling distribution has a probability mass function \(s_{x}\).

**Theorem E.1**.: _Consider a subsampled mechanism \(M=B\circ S\), and an arbitrary coupling \(\Gamma\) between subsampling distributions \(P_{S_{x}}\) and \(P_{S_{x^{\prime}}}\). Then_

\[\Psi_{\alpha}(m_{x}||m_{x^{\prime}})\leq\int_{\mathbb{Y}^{2}}c_{ \alpha}(y^{(1)},y^{(2)})\;\mathrm{d}\Gamma((y^{(1)},y^{(2)}))\] (8)

_with cost function \(c_{\alpha}(y^{(1)},y^{(2)})=\Psi_{\alpha}(b_{y^{(1)}}||b_{y^{(2)}})\)._

Proof.: Recall that \(m_{x}\) and \(m_{x^{\prime}}\) are mixtures with \(m_{x}(z)=\int b_{y}(z)\;dP_{S_{x}}(y)\) and \(m_{x^{\prime}}(z)=\int b_{y}(z)\;dP_{S_{x^{\prime}}}(y)\). Since \(\Gamma\) is a coupling between \(P_{S_{x}}\) and \(P_{S_{x^{\prime}}}\), we can use the projection \(\pi_{n}(\bm{y})=y_{n}\) and change of variables to rewrite these mixtures as

\[m_{x}(z)=\int_{\mathbb{Y}}b_{y}(z)\;d\left(\Gamma\circ\pi_{1}^{- 1}\right)(y)=\int_{\mathbb{Y}^{2}}b_{\pi_{1}(\bm{y})}(z)\;d\Gamma(\bm{y})=\int _{\mathbb{Y}^{2}}b_{y_{1}}(z)\;d\Gamma(\bm{y}),\] \[m_{x^{\prime}}(z)=\int_{\mathbb{Y}}b_{y}(z)\;d\left(\Gamma\circ \pi_{2}^{-1}\right)(y)=\int_{\mathbb{Y}^{2}}b_{\pi_{2}(\bm{y})}(z)\;d\Gamma( \bm{y})=\int_{\mathbb{Y}^{2}}b_{y_{2}}(z)\;d\Gamma(\bm{y}).\]

Since \(m_{x}(z)\) and \(m_{x^{\prime}}(z)\) are now expectations w.r.t. the same measure, we can use the joint convexity of \(\Psi_{\alpha}\) (Lemma D.9) to show \(\Psi_{\alpha}(m_{x},m_{x^{\prime}})\leq\int_{\mathbb{Y}^{2}}\Psi_{\alpha}(b_{ y_{1}}||b_{y_{2}})\;\mathrm{d}\Gamma(\bm{y})\). 

### Proof of Theorem 3.4

As before, we prove a more general statement from which Theorem 3.4 immediately follows. For this, recall that \(\mathcal{Y}\) is the \(\sigma\)-algebra of batch space \((\mathbb{Y},\mathcal{Y})\), and that \(P(T\mid R)=P(T\cap R)\,/\,P(R)\).

**Theorem E.2**.: _Consider a subsampled mechanism \(M=B\circ S\). Further consider two disjoint partitionings \(\bigcup_{i=1}^{I}A_{i}=\mathbb{Y}\) and \(\bigcup_{j=1}^{J}E_{j}=\mathbb{Y}\) such that all \(A_{i},E_{j}\) are in \(\mathcal{Y}\) and have non-zero measure under \(S_{x}\) and \(S_{x^{\prime}}\), respectively. Let \(\Gamma\) be an arbitrary coupling between \(P_{S_{x}}(\cdot\mid A_{1}),\ldots,P_{S_{x}}(\cdot\mid A_{I}),P_{S_{x^{\prime} }}(\cdot\mid E_{1}),\ldots,P_{S_{x^{\prime}}}(\cdot\mid E_{J})\). Then,_

\[\Psi_{\alpha}(m_{x}||m_{x^{\prime}})\leq\int_{\mathbb{Y}^{I+J}}c_{\alpha}(\bm{ y}^{(1)},\bm{y}^{(2)})\;\mathrm{d}\Gamma((\bm{y}^{(1)},\bm{y}^{(2)})),\]

_with cost function \(c:\mathbb{Y}^{I}\times\mathbb{Y}^{J}\to\mathbb{R}_{+}\) defined by_

\[c_{\alpha}(\bm{y}^{(1)},\bm{y}^{(2)})=\Psi_{\alpha}\left(\sum_{i=1}^{I}b_{y^{ (1)}_{i}}\cdot P_{S_{x}}(A_{i})||\sum_{j=1}^{J}b_{y^{(2)}_{j}}\cdot P_{S_{x^{ \prime}}}(E_{j})\right).\] (9)

Proof.: Using the law of total expectation, linearity of integration, and change of variables with projection \(\pi_{n}(\bm{y})=y_{n}\) shows that

\[m_{x}(z) =\sum_{i=1}^{I}\left(\int_{\mathbb{Y}}b_{y}(z)\;dP_{S_{x}}(y\mid A _{i})\right)P_{S_{x}}(A_{i})\] \[=\int_{\mathbb{Y}}\sum_{i=1}^{I}b_{y}(z)\cdot P_{S_{x}}(A_{i})\;dP _{S_{x}}(y\mid A_{i})\] \[=\int_{\mathbb{Y}}\sum_{i=1}^{I}b_{y}(z)\cdot P_{S_{x}}(A_{i})\;d( \Gamma\circ\pi_{i}^{-1})(y)\] \[=\int_{\mathbb{Y}^{I+J}}\left(\sum_{i=1}^{I}b_{y_{i}}(z)\cdot P_{ S_{x}}(A_{i})\right)\;d\Gamma(\bm{y})\]\[m_{x^{\prime}}(z)=\int_{\mathbb{Y}^{I+J}}\left(\sum_{j=1}^{J}b_{y_{(j+l)}}(z)\cdot P _{S_{x}}(E_{j})\right)\ d\Gamma(\bm{y}).\]

Since \(m_{x}(z)\) and \(m_{x^{\prime}}(z)\) are now expectations w.r.t. the same measure, we can use the joint convexity of \(\Psi_{\alpha}\) (Lemma D.9) to show

\[\Psi_{\alpha}(m_{x}||m_{x^{\prime}})\leq\int_{\mathbb{Y}^{I+J}} \Psi_{\alpha}\left(\sum_{i=1}^{I}b_{y_{i}}(z)\cdot P_{S_{x}}(A_{i})||\sum_{j=1 }^{J}b_{y_{(j+l)}}(z)\cdot P_{S_{x}}(E_{j})\right)\ d\Gamma(\bm{y}).\]

A change of indexing via \(\bm{y}_{i}^{(1)}=\bm{y}_{i}\) and \(\bm{y}_{j}^{(2)}=\bm{y}_{(j+I)}\) concludes our proof. 

### Proof of Proposition 3.5

**Proposition 3.5**.: _Consider \(\bm{y}^{(1)}\in\mathbb{Y}^{I},\bm{y}^{(2)}\in\mathbb{Y}^{J}\), and cost function \(c\) defined in Eq. (2). Let \(d_{\mathbb{Y}}\) be the distance induced by \(\simeq_{\mathbb{Y}}\) (see Definition 2.4). Then, \(c_{\alpha}(\bm{y}^{(1)},\bm{y}^{(2)})\leq\hat{c}_{\alpha}(\bm{y}^{(1)},\bm{y} ^{(2)})\), with_

\[\hat{c}_{\alpha}(\bm{y}^{(1)},\bm{y}^{(2)})=\max_{\hat{\bm{y}}^{(1)},\hat{\bm {y}}^{(2)}}c_{\alpha}(\hat{\bm{y}}^{(1)},\hat{\bm{y}}^{(2)})\] (3)

_subject to \(\forall k,l\in\{1,2\},\forall t,u:d_{\mathbb{Y}}(\hat{y}_{t}^{(k)},\hat{y}_{ u}^{(l)})\leq d_{\mathbb{Y}}(y_{t}^{(k)},y_{u}^{(l)})\) and \(\hat{\bm{y}}^{(1)}\in\mathbb{Y}^{I},\hat{\bm{y}}^{(2)}\in\mathbb{Y}^{J}\)._

Proof.: The original tuples of batches \(\bm{y}^{(1)}\in\mathbb{Y}^{I}\) and \(\bm{y}^{(2)}\in\mathbb{Y}^{J}\) constitute a feasible solution to the maximization problem, since they fulfill the constraints with equality, i.e., \(\forall k,l,t,u:d_{\mathbb{Y}}(y_{t}^{(k)},y_{u}^{(l)})=d_{\mathbb{Y}}(y_{t}^ {(k)},y_{u}^{(l)})\). The value of any feasible solution to a maximization problem is l.e.q. its optimal value.

## Appendix F Distance-compatible couplings of multiple distributions

In the following, we generalize the notion of distance-compatible couplings proposed in [15] from two distributions to an arbitrary number of distributions. This provides a sufficient optimality condition for Theorem E.2.

Note that we use a more general, measure-theoretic definition of distance-compatibility (see Definition F.4). Definition 3.6 is a special case for subsampling schemes with finite, discrete support.

As discussed in Section 3, a \(d_{\mathbb{Y}}\)-compatible coupling between (conditional) subsampling distributions only assigns probability to tuples of batches \(\bm{y}\) when all pairs \(y_{i},y_{j}\) have the smallest possible distance to \(y_{1}\) and to each other, while still being in the support of their respective distributions.

To avoid having to make additional assumptions about the topology of batch space \((\mathbb{Y},\mathcal{Y})\), we will assume that all subsampling schemes have densities and reason about the support of these densities.

**Definition F.1**.: The support of a function \(p:\mathbb{Y}\to\mathbb{R}_{+}\) is \(\mathrm{supp}(p)=\{y\in\mathbb{Y}\mid p(y)>0\}\).

Based on this notion of support, we can define a notion of distance between a batch \(y\in\mathbb{Y}\) and the support of a density:

**Definition F.2**.: Consider a distance \(d_{\mathbb{Y}}\) induced by a neighboring relation \(\simeq_{\mathbb{Y}}\). The distance between an element \(y\in\mathbb{Y}\) and the support of a function \(p:\mathbb{Y}\to\mathbb{R}_{+}\) is defined as \(d_{\mathbb{Y}}(y,\mathrm{supp}(p))=\min_{y^{\prime}\in\mathrm{supp}(p)}d_{ \mathbb{Y}}(y,y^{\prime})\).

Furthermore, we can define a notion of distance between the support of two different densities:

**Definition F.3**.: Consider a distance \(d_{\mathbb{Y}}\) induced by a neighboring relation \(\simeq_{\mathbb{Y}}\). The distance between the support of functions \(p_{1},p_{2}:\mathbb{Y}\to\mathbb{R}_{+}\) is defined as

\[d_{\mathbb{Y}}(\mathrm{supp}(p_{1}),\mathrm{supp}(p_{2}))=\min_{y_{1},y_{2}}d _{\mathbb{Y}}(y_{1},y_{2})\quad\text{s.t.}\quad\forall i\in\{1,2\}:y_{i}\in \mathrm{supp}(p_{i}).\]

Based on these definitions, we can now formally define distance-compatible couplings between multiple distributions.

**Definition F.4**.: Consider a coupling \(\Gamma\) between probability measures \(P_{1},\ldots,P_{N}\) on measurable space \((\mathbb{Y},\mathcal{Y})\) with symmetric neighboring relation \(\simeq_{\mathbb{Y}}\) and induced distance \(d_{\mathbb{Y}}\). Assume that \(\forall n:P_{n}\ll\nu_{n}\) for some measures \(\nu_{1},\ldots,\nu_{N}\) and define \(p_{n}=\mathrm{d}P_{n}\,/\,\mathrm{d}\nu_{n}\). Further assume that \(\Gamma\ll\prod_{n=1}^{N}\nu_{n}\) with product measure \(\prod_{n=1}^{N}\nu_{n}\), and define \(\gamma=d\Gamma\,/\,\mathrm{d}\prod\nu_{n}\). Then, \(\Gamma\) is a \(d_{\mathbb{Y}}\)-compatible coupling if

\[(\bm{y}\in\mathrm{supp}(\Gamma) \implies\forall u>1:d_{\mathbb{Y}}(y_{1},y_{u})=d_{\mathbb{Y}}( y_{1},\mathrm{supp}(s_{u})))\] \[\wedge(\bm{y}\in\mathrm{supp}(\Gamma) \implies\forall u>t>1:d_{\mathbb{Y}}(y_{u},y_{t})=d_{\mathbb{Y}}( \mathrm{supp}(s_{t}),\mathrm{supp}(s_{u})))\,.\]

That is, \(\Gamma\) only assigns probability to a tuple of batches \(\bm{y}\) if all \(y_{t}\) and \(y_{u}\) are as close as possible to \(y_{1}\) and as close as possible to each other, while still being in the support of their corresponding densities. Note that our choice of focusing on \(y_{1}\) is arbitrary, and \(d_{\mathbb{Y}}\)-compatibility could also be defined for any other reference index \(n\in\{1,\ldots,N\}\).

We shall now prove that \(d_{\mathbb{Y}}\)-compatibility is a sufficient optimality condition for our optimal transport problem. For this proof, we will use the following lemma, which immediately follows from Definitions F.2 and F.3:

Figure 25: Example of a distance-compatible and a distance-incompatible coupling

**Lemma F.5**.: _Consider a distance \(d_{\mathbb{Y}}\) induced by a relation \(\simeq_{\mathbb{Y}}\) and two functions \(p_{1},p_{2}:\mathbb{Y}\to\mathbb{R}_{+}\). Then, for all \(y_{1}\in\operatorname{supp}(p_{1}),y_{2}\in\operatorname{supp}(p_{2})\),_

\[d_{\mathbb{Y}}(y_{1},y_{2})\geq d_{\mathbb{Y}}(y_{1},\operatorname{supp}(p_{2}) )\geq d_{\mathbb{Y}}(\operatorname{supp}(p_{1}),\operatorname{supp}(p_{2})).\]

**Theorem F.6**.: _Consider a subsampled mechanism \(M=B\circ S\). Further consider two finite partitions \(A_{1},\ldots,A_{I}\in\mathcal{Y}\) and \(E_{1},\ldots,E_{J}\in\mathcal{Y}\) of \(\mathbb{Y}\) such that all \(A_{i}\) and \(E_{j}\) have non-zero measure under \(S_{x}\) and \(S_{x^{\prime}}\), respectively. Let \(d_{\mathbb{Y}}\) be the distance induced by a symmetric neighboring relation \(\simeq_{\mathbb{Y}}\). Let \(\Gamma^{*}\) be a \(d_{\mathbb{Y}}\)-compatible coupling between \(P_{S_{x}}(\cdot\mid A_{1}),\ldots,P_{S_{x}}(\cdot\mid A_{I}),P_{S_{x^{\prime} }}(\cdot\mid E_{1}),\ldots,P_{S_{x^{\prime}}}(\cdot\mid E_{J})\), which have Radon-Nikodym derivatives \(s_{1},\ldots,s_{I+J}\). Then, for all \(\alpha>1\),_

\[\Gamma^{*}\in\operatorname*{arg\,min}_{\Gamma\in\mathbb{G}}\leq\int_{\mathbb{ Y}^{I+J}}\hat{c}_{\alpha}(\bm{y}^{(1)},\bm{y}^{(2)})\;\mathrm{d}\Gamma((\bm{y}^{ (1)},\bm{y}^{(2)})).\] (10)

_where \(\mathbb{G}\) is the set of valid couplings between the \(I+J\) measures, and \(\hat{c}_{\alpha}:\mathbb{Y}^{I}\times\mathbb{Y}^{J}\to\mathbb{R}_{+}\) is the cost function upper bound defined in Proposition 3.5._

Proof.: Consider an arbitrary, not necessarily \(d_{\mathbb{Y}}\)-compatible coupling \(\Gamma\). By definition of \(\hat{c}\) and symmetry of \(\simeq\), we have

\[\int_{\mathbb{Y}^{I+J}}\hat{c}_{\alpha}(\bm{y}^{(1)},\bm{y}^{(2) })\;\mathrm{d}\Gamma((\bm{y}^{(1)},\bm{y}^{(2)}))\] \[= \int_{\mathbb{Y}^{I+J}}\left(\max_{\hat{y}\in\mathbb{Y}^{I+J}}c_ {\alpha}(\hat{\bm{y}}_{:I},\hat{\bm{y}}_{I:})\text{ s.t. }\forall t<u:d_{\mathbb{Y}}(\hat{y}_{t},\hat{y}_{u})\leq d_{\mathbb{Y}}(y_{t}, y_{u})\right)\;\mathrm{d}\Gamma(\bm{y}),\]

with original cost function \(c_{\alpha}:\mathbb{Y}^{I}\times\mathbb{Y}^{J}\to\mathbb{R}_{+}\) defined in Theorem E.2.

We can now use Lemma F.5 to tighten the constraints of the optimization problem inside the integrand and thus lower-bound its optimal value for all \(\bm{y}\in\operatorname{supp}(\Gamma)\):

\[\max_{\hat{y}\in\mathbb{Y}^{I+J}}c_{\alpha}(\hat{\bm{y}}_{:I}, \hat{\bm{y}}_{I:})\text{ s.t. }\forall t<u:d_{\mathbb{Y}}(\hat{y}_{t},\hat{y}_{u})\leq d_{ \mathbb{Y}}(y_{t},y_{u})\] \[\geq \max_{\hat{y}\in\mathbb{Y}^{I+J}}c_{\alpha}(\hat{\bm{y}}_{:I}, \hat{\bm{y}}_{I:})\] \[\text{ s.t. }\forall u>1:d_{\mathbb{Y}}(\hat{y}_{1},\hat{y}_{u}) \leq d_{\mathbb{Y}}(y_{1},\operatorname{supp}(s_{u})),\] \[\forall u>t>1:d_{\mathbb{Y}}(\hat{y}_{t},\hat{y}_{u})\leq d_{ \mathbb{Y}}(\operatorname{supp}(s_{t}),\operatorname{supp}(s_{u})).\]

We notice that the lower bound only depends on \(y_{1}\) and shall thus refer to it as \(\kappa_{\alpha}(y_{1})\). Further note that any \(\bm{y}\notin\operatorname{supp}(\Gamma)\) does not contribute to the integral. Since \(\Gamma\) is a valid coupling, we can marginalize out all variables except \(y_{1}\) via projection \(\pi_{1}(\bm{y})=y_{1}\) to show

\[\int_{\mathbb{Y}^{I+J}}\hat{c}_{\alpha}(\bm{y}^{(1)},\bm{y}^{(2) })\;\mathrm{d}\Gamma((\bm{y}^{(1)},\bm{y}^{(2)}))\] \[\geq \int_{\mathbb{Y}^{I+J}}\kappa_{\alpha}(\pi_{1}(\bm{y}))\;\mathrm{ d}\Gamma(\bm{y})=\int_{\mathbb{Y}}\kappa_{\alpha}(y_{1})\;\mathrm{d}(\Gamma \circ\pi_{1}^{-1})(y_{1})=\int_{\mathbb{Y}}\kappa_{\alpha}(y_{1})\;\mathrm{d}P _{S_{x}}(y_{1}\mid A_{1}).\]

By construction of \(\kappa_{\alpha}\), this holds with equality whenever \(\Gamma^{*}\) is a \(d_{\mathbb{Y}}\)-compatible coupling. 

Finally, the exact derivations we used for Theorem F.6 can also be used to show that the optimal value of our transport problem has a simple, canonical form whenever a \(d_{\mathbb{Y}}\)-compatible coupling exists:

**Corollary F.7**.: _Consider a subsampled mechanism \(M=B\circ S\). Further consider two finite partitions \(A_{1},\ldots,A_{I}\in\mathcal{Y}\) and \(E_{1},\ldots,E_{J}\in\mathcal{Y}\) of \(\mathbb{Y}\) such that all \(A_{i}\) and \(E_{j}\) have non-zero measure under \(S_{x}\) and \(S_{x^{\prime}}\), respectively. Let \(d_{\mathbb{Y}}\) be the distance induced by a symmetric neighboring relation \(\simeq_{\mathbb{Y}}\). Assume that a \(d_{\mathbb{Y}}\)-compatible coupling between \(P_{S_{x}}(\cdot\mid A_{1}),\ldots,P_{S_{x}}(\cdot\mid A_{I}),P_{S_{x^{\prime} }}(\cdot\mid E_{1}),\ldots,P_{S_{x^{\prime}}}(\cdot\mid E_{J})\) with Radon-Nikodym derivatives \(s_{1},\ldots,s_{I+J}\) exists. Then, for all \(\alpha>1\),_

\[\min_{\Gamma\in\mathbb{G}}\int_{\mathbb{Y}^{I+J}}\hat{c}_{\alpha}(\bm{y}^{(1)}, \bm{y}^{(2)})\;\mathrm{d}\Gamma((\bm{y}^{(1)},\bm{y}^{(2)}))=\int_{\mathbb{Y}} \kappa_{\alpha}(y_{1})\;\mathrm{d}P_{S_{x}}(y_{1}\mid A_{1}),\] (11)

_where \(\mathbb{G}\) is the space of valid couplings between the \(I+J\) measures,_

\[\kappa_{\alpha}(y_{1})=\max_{\hat{y}\in\mathbb{Y}^{I+J}}c_{\alpha}(\hat{\bm{y}}_{: I},\hat{\bm{y}}_{I:})\quad\text{ s.t. }\quad\forall u>1:d_{\mathbb{Y}}(\hat{y}_{1},\hat{y}_{u})\leq d_{\mathbb{Y}}(y_{1}, \operatorname{supp}(s_{u})),\] \[\forall u>t>1:d_{\mathbb{Y}}(\hat{y}_{t},\hat{y}_{u})\leq d_{ \mathbb{Y}}(\operatorname{supp}(s_{t}),\operatorname{supp}(s_{u})),\]

_and \(c_{\alpha}:\mathbb{Y}^{I}\times\mathbb{Y}^{J}\to\mathbb{R}_{+}\) is the original cost function defined in Theorem E.2._Thus, we can focus on constructing distance-compatible couplings when trying to derive existing or novel amplification by subsampling guarantees. Note that these result also generalize to asymmetric neighboring relations \(\simeq_{\forall}\). We just wanted to avoid further complicating the indexing. Future work may want to generalize these results to a more general, topological notion of support that does not rely on the existence of subsampling densities.

Recovering known mechanism-agnostic ADP guarantees

In the following, we first provide an overview of the framework for deriving mechanism-agnostic ADP guarantees proposed by Balle et al. [15]. We then show that the same guarantees can be derived via optimal transport between multiple subsampling distributions. Finally, we use observations made during the proof to explain why the mechanism-agnostic guarantees derived via this approach can be suboptimal.

### Overview of Balle et al. framework

The framework from [15] uses four steps to derive tight mechanism-agnostic guarantees for subsampled mechanisms: (1) Partitioning the subsampling distributions via maximal couplings, (2) applying advanced joint convexity, (3) applying joint convexity, and (4) defining two couplings involving two distributions.

Maximal couplings are a construction that makes it possible to partition the subsampling probability mass functions as

\[s_{x}(y)=(1-w)p_{x}(y)+wq_{x}(y)\] \[s_{x^{\prime}}(y)=(1-w)p_{x}(y)+wq_{x^{\prime}}(y),\]

with probability mass functions \(p_{x},q_{x},q_{x^{\prime}}:\mathbb{Y}\to[0,1]\) chosen such that \(q_{x}\) and \(q_{x^{\prime}}\) have disjoint support and \(w\in[0,1]\) is as small as possible.

Note that, when considering typical subsampling schemes (Poisson, without replacement, with replacement) and single-element neighboring relations (insertion/removal, substitution), this partition is simply equivalent to

\[s_{x}(y)=\Pr[S_{x}\in A_{1}]\cdot s_{x}(y\mid A_{1})+\Pr[S_{x} \in A_{2}]\cdot s_{x}(y\mid A_{2})\] \[s_{x^{\prime}}(y)=\Pr[S_{x^{\prime}}\in E_{1}]\cdot s_{x^{\prime }}(y\mid E_{1})+\Pr[S_{x}\in E_{2}]\cdot s_{x^{\prime}}(y\mid E_{2}),\]

where \(A_{1}\) and \(E_{1}\) are the event that the inserted/removed or substituted element is not sampled, and \(A_{2}\), \(E_{2}\) are their complements (see Appendix B in [15]).

Using this construction, the densitities \(m_{x}\) and \(m_{x^{\prime}}\) of subsampled mechanisms \(M=B\circ S\) can be rewritten as

\[m_{x}(z)=(1-w)\sum_{y\in\mathbb{Y}}b_{y}(z)p_{x}(y)+w\sum_{y\in \mathbb{Y}}b_{y}(z)q_{x}(y)\] (12) \[m_{x^{\prime}}(z)=(1-w)\sum_{y\in\mathbb{Y}}b_{y}(z)p_{x}(y)+w \sum_{y\in\mathbb{Y}}b_{y}(z)q_{x^{\prime}}(y)\] (13)

Next, one can rewrite the divergence \(H_{\alpha}(m_{x}||m_{x^{\prime}})\) via the following property:

**Proposition G.1** (Advanced joint convexity [15]).: _Let \(m_{x},m^{\prime}_{x}:\mathbb{Z}\to[0,1]\) be probability mass functions satisfying \(m_{x}(z)=(1-w)f(z)+wg(z)\) and \(m_{x^{\prime}}(z)=(1-w)f(z)+wg^{\prime}(z)\) for some \(w\in[0,1]\), \(f,g,g^{\prime}:\mathbb{Z}\to[0,1]\). Given \(\alpha\geq 1\), let \(\alpha^{\prime}=1+w(\alpha-1)\) and \(\beta=\alpha^{\prime}\,/\,\alpha\). Then the following holds:_

\[D_{\alpha^{\prime}}(m_{x}||m_{x^{\prime}})=wD_{\alpha}(g||(1-\beta)f+\beta g^{ \prime}).\]

Applying advanced joint convexity, followed by joint convexity to Eqs. (12) and (13) shows that

\[H_{\alpha^{\prime}}(m_{x}||m_{x^{\prime}})\leq (1-\beta)H_{\alpha}\left(\sum_{y\in\mathbb{Y}}b_{y}q_{x}(y)||\sum _{y\in\mathbb{Y}}b_{y}p_{x}(y)\right)\] \[+\beta H_{\alpha}\left(\sum_{y\in\mathbb{Y}}b_{y}q_{x}(y)||\sum_ {y\in\mathbb{Y}}b_{y}q_{x^{\prime}}(y)\right)\]

Finally, one can construct a coupling \(\gamma:\mathbb{Y}^{2}\to[0,1]\) between \(q_{x}\) and \(p_{x}\), as well as a coupling \(\gamma^{\prime}:\mathbb{Y}^{2}\to[0,1]\) between \(q_{x}\) and \(q_{x^{\prime}}\). One can then invoke a special case of Theorem 3.3 with \(\Psi_{\alpha}=H_{\alpha}\) and the cost function upper bound from Proposition 3.5 to show

\[H_{\alpha^{\prime}}(m_{x}||m_{x^{\prime}})\leq(1-\beta)\sum_{\bm{y}\in\mathbb{Y }^{2}}\hat{c}_{\alpha}(y^{(1)},y^{(2)})\gamma(y^{(1)},y^{(2)})+\beta\sum_{\bm{ y}\in\mathbb{Y}^{2}}\hat{c}_{\alpha}(y^{(1)},y^{(2)})\gamma^{\prime}(y^{(1)},y^{(2)}).\]Since we are only considering pairs of batches, we have

\[\hat{c}_{\alpha}(y^{(1)},y^{(2)})=\max_{\hat{\bm{y}}}H_{\alpha}(b_{\hat{y}^{(1)}} ||b_{\hat{y}^{(1)}})\text{ s.t. }d_{\mathbb{Y}}(y^{(1)},y^{(2)})\leq d_{\mathbb{Y}}(\hat{y}^{(1)},\hat{y}^{(2)})\]

with induced distance \(d_{\mathbb{Y}}\) from Definition 2.4. This specific cost function bound is referred to as "group privacy profile" in [15].

### Subsumption of Balle et al. framework

Next, we show that we can always obtain the same guarantee by defining a (potentially suboptimal) coupling between four subsampling distributions, and then pessimistically upper-bounding the guarantee we would obtain through Theorem 3.4:

**Theorem G.2**.: _Consider a subsampled mechanism \(M=B\circ S\) and some \(x,x^{\prime}\in\mathbb{X}\). Assume that subsampling pmfs \(s_{x},s_{x^{\prime}}:\mathbb{Y}\to[0,1]\) satisfy \(s_{x}(z)=(1-w)p_{x}(z)+wq_{x}(z)\) and \(s_{x^{\prime}}(z)=(1-w)p_{x}(z)+wq_{x^{\prime}}(z)\) for some \(w\in[0,1]\) and \(p_{x},q_{x},q_{x^{\prime}}:\mathbb{Y}\to[0,1]\). Let \(\gamma:\mathbb{Y}^{2}\to[0,1]\) be an arbitrary coupling of \(q_{x},p_{x}\), and \(\gamma^{\prime}:\mathbb{Y}^{2}\to[0,1]\) be an arbitrary coupling of \(q_{x},p_{x^{\prime}}\). Then, there is a coupling \(\tilde{\gamma}:\mathbb{Y}^{4}\to[0,1]\) of \(p_{x},q_{x},p_{x},q_{x^{\prime}}\) such that for all \(\alpha\geq 1\)_

\[H_{\alpha^{\prime}}(m_{x}||m_{x^{\prime}}) \leq\sum_{\bm{y}\in\mathbb{Y}^{2+2}}\hat{c}_{\alpha^{\prime}}( \bm{y}^{(1)},\bm{y}^{(2)})\tilde{\gamma}(\bm{y}^{(1)},\bm{y}^{(1)})\] \[\leq(1-\beta)\sum_{\bm{y}\in\mathbb{Y}^{1+1}}\hat{c}_{\alpha}(y^{ (1)},y^{(2)})\gamma(y^{(1)},y^{(2)})+\beta\sum_{\bm{y}\in\mathbb{Y}^{1+1}}\hat {c}_{\alpha}(y^{(1)},y^{(2)})\gamma^{\prime}(y^{(1)},y^{(2)}),\]

_with cost function upper bound \(\hat{c}_{\alpha}\) defined in Proposition 3.5, \(\alpha^{\prime}=1+w(\alpha-1)\), and \(\beta=\alpha^{\prime}\,/\,\alpha\)._

Proof.: The main idea is to invoke Theorem 3.4 with a specifically crafted coupling, and then apply advanced joint convexity and joint convexity.

Specifically, we define a coupling \(\tilde{\gamma}:\mathbb{Y}^{\to}[0,1]\) that corresponds to the following generative process: We first sample \(y_{2}^{(1)}\) and \(y_{2}^{(2)}\) from the coupling \(\gamma^{\prime}\) (recall that a coupling is a joint mass function). This gives us two elements from the support of \(q_{x}\) and \(q_{x^{\prime}}\), respectively. Then, we sample \(y_{1}^{(1)}\) from \(\gamma\) conditioned on \(y_{2}^{(1)}\). This gives us an element from the support of \(p_{x}\). Finally, we let \(y_{1}^{(2)}\gets y_{1}^{(1)}\). Formally, this coupling can be defined as

\[\tilde{\gamma}(\bm{y}^{(1)},\bm{y}^{(2)})=\gamma^{\prime}(y_{2}^{(1)},y_{2}^{ (2)})\cdot\frac{\gamma(y_{2}^{(1)},y_{1}^{(1)})}{q_{x}(y_{2}^{(1)})}\cdot \mathbbm{1}\left[y_{1}^{(2)}=y_{1}^{(1)}\right].\]

**Validity of coupling.** Before proceeding, we need to verify that this is a valid coupling, i.e., its four marginals are \(p_{x},q_{x},p_{x},q_{x^{\prime}}\). For any \(y_{1}^{(1)}\in\mathbb{Y}\), we have

\[\sum_{y_{2}^{(1)},y_{1}^{(2)},y_{2}^{(2)}\in\mathbb{Y}^{3}}\tilde{ \gamma}(y_{1}^{(1)},y_{2}^{(1)},y_{1}^{(2)},y_{2}^{(2)})\] \[=\sum_{y_{2}^{(1)},y_{2}^{(2)}\in\mathbb{Y}^{2}}\gamma^{\prime}(y _{2}^{(1)},y_{2}^{(2)})\cdot\frac{\gamma(y_{2}^{(1)},y_{1}^{(1)})}{q_{x}(y_{2} ^{(1)})}\] \[=\sum_{y_{2}^{(1)}\in\mathbb{Y}}q_{x}(y_{2}^{(1)})\cdot\frac{ \gamma(y_{2}^{(1)},y_{1}^{(1)})}{q_{x}(y_{2}^{(1)})}\] \[=\sum_{y_{2}^{(1)}\in\mathbb{Y}}\gamma(y_{2}^{(1)},y_{1}^{(1)})\] \[= p_{x}(y_{1}^{(1)}).\]

where the first inequality is due to the indicator function, the second equality follows from marginalizing \(\gamma^{\prime}\), and the last equality follows from marginalizing \(\gamma\).

The proof for any \(y_{1}^{(2)}\in\mathbb{Y}\) is analogous.

For any \(y_{2}^{(1)}\in\mathbb{Y}\), we similarly have

\[\sum_{y_{1}^{(1)},y_{2}^{(2)},y_{2}^{(2)}\in\mathbb{Y}^{3}}\tilde{ \gamma}(y_{1}^{(1)},y_{2}^{(1)},y_{1}^{(2)},y_{2}^{(2)})\] (14) \[=\sum_{y_{1}^{(1)},y_{2}^{(2)}\in\mathbb{Y}^{2}}\gamma^{\prime}(y_ {2}^{(1)},y_{2}^{(2)})\cdot\frac{\gamma(y_{2}^{(1)},y_{1}^{(1)})}{q_{x}(y_{2}^ {(1)})}\] (15) \[=\sum_{y_{1}^{(1)}\in\mathbb{Y}^{1}}\gamma(y_{2}^{(1)},y_{1}^{(1)})\] (16) \[= q_{x}(y_{2}^{(1)}),\] (17)

and for any \(y_{2}^{(2)}\) we have

\[\sum_{y_{1}^{(1)},y_{2}^{(1)},y_{1}^{(2)}\in\mathbb{Y}^{3}}\tilde {\gamma}(y_{1}^{(1)},y_{2}^{(1)},y_{2}^{(2)})\] (18) \[=\sum_{y_{1}^{(1)},y_{2}^{(1)}\in\mathbb{Y}^{2}}\gamma^{\prime}(y _{2}^{(1)},y_{2}^{(2)})\cdot\frac{\gamma(y_{2}^{(1)},y_{1}^{(1)})}{q_{x}(y_{2 }^{(1)})}\] (19) \[=\sum_{y_{2}^{(1)}\in\mathbb{Y}}\gamma^{\prime}(y_{2}^{(1)},y_{2} ^{(2)})\cdot\frac{q_{x}(y_{2}^{(1)})}{q_{x}(y_{2}^{(1)})}\] (20) \[= q_{x^{\prime}}(y_{2}^{(2)}).\] (21)

**First inequality.** Now that we have a valid coupling, we can use the same joint-convexity argument as in our proof of Theorem 3.4, combined with our cost function bound \(\hat{c}_{\alpha}\) to show

\[H_{\alpha^{\prime}}(m_{x}||m_{x^{\prime}})\leq\sum_{\mathbb{Y}^{ 2+2}}\max_{\hat{\bm{y}}}H_{\alpha^{\prime}}\left((1-w)b_{\hat{y}_{1}^{(1)}}+ wb_{\hat{y}_{2}^{(1)}}|||-w)b_{\hat{y}_{1}^{(2)}}+wb_{\hat{y}_{2}^{(2)}} \right)\cdot\gamma(\bm{y}^{(1)},\bm{y}^{(2)}),\]

with each of the \(|\mathbb{Y}^{2+2}|\) optimization problems being constrained by \(\forall k,l,t,u:d_{\mathbb{Y}}(\hat{y}_{t}^{(k)},\hat{y}_{u}^{(l)})\leq d_{ \mathbb{Y}}(y_{t}^{(k)},y_{u}^{(l)})\) and \(\hat{\bm{y}}^{(1)}\in\mathbb{Y}^{2},\hat{\bm{y}}^{(2)}\in\mathbb{Y}^{2}\). This corresponds to the first equality in our Theorem.

**Second inequality.** Due to construction of our coupling, we always have \(d_{\mathbb{Y}}(y_{1}^{(1)},y_{1}^{(2)})=0\), i.e., \(y_{1}^{(1)}=y_{1}^{(2)}\). We can thus use advanced joint convexity, joint convexity, and linearity of summation to obtain a looser bound via

\[H_{\alpha^{\prime}}(m_{x}||m_{x^{\prime}})\leq (1-\beta)\sum_{\mathbb{Y}^{2+2}}\max_{\hat{\bm{y}}}H_{\alpha}\left( b_{\hat{y}_{2}^{(1)}}||b_{\hat{y}_{1}^{(1)}}\right)\cdot\gamma(\bm{y}^{(1)}, \bm{y}^{(2)})\] \[+\beta\sum_{\mathbb{Y}^{2+2}}\max_{\hat{\bm{y}}}H_{\alpha}\left( b_{\hat{y}_{2}^{(1)}}||b_{\hat{y}_{2}^{(2)}}\right)\cdot\gamma(\bm{y}^{(1)}, \bm{y}^{(2)}),\]

with each of the \(2\cdot|\mathbb{Y}^{2+2}|\) optimization problems being constrained by \(\forall k,l,t,u:d_{\mathbb{Y}}(\hat{y}_{t}^{(k)},\hat{y}_{u}^{(l)})\leq d_{ \mathbb{Y}}(y_{t}^{(k)},y_{u}^{(l)})\) and \(\hat{\bm{y}}^{(1)}\in\mathbb{Y}^{2},\hat{\bm{y}}^{(2)}\in\mathbb{Y}^{2}\).

Next, we can further loosen this bound by dropping all constraints involving \(y_{1}^{(2)}\) and \(y_{2}^{(2)}\) in the first optimization problem. We can also drop all constraints involving \(y_{1}^{(1)}\) and \(y_{1}^{(2)}\) in the second optimization problem. Thus, by definition of the group privacy profile, we have

\[H_{\alpha^{\prime}}(m_{x}||m_{x^{\prime}})\leq (1-\beta)\sum_{\mathbb{Y}^{2+2}}\hat{c}_{\alpha}\left(\bm{y}_{2}^{ (1)},\bm{y}_{1}^{(1)}\right)\cdot\gamma(\bm{y}^{(1)},\bm{y}^{(2)})\] \[+\beta\sum_{\mathbb{Y}^{2+2}}\hat{c}_{\alpha}\left(\bm{y}_{2}^{(1) },\bm{y}_{2}^{(2)}\right)\cdot\gamma(\bm{y}^{(1)},\bm{y}^{(2)}).\]

Note that the first cost function term does not depend on \(y_{1}^{(2)}\) nor \(y_{2}^{(2)}\), and the second cost function term does not depend on \(y_{1}^{(1)}\) nor \(y_{1}^{(2)}\). We can thus marginalize out these variables (recall Eq. (16) and Eq. (20)) to conclude our proof.

The same argument also applies to the general problem setting defined in Appendix D: Given two couplings \(\Gamma,\Gamma^{\prime}\), we can define a product coupling \(\tilde{\Gamma}\propto\Gamma\cdot\Gamma^{\prime}\), invoke Theorem 3.4, and then apply (advanced) joint convexity to pessimistically upper-bound the guarantee that would be obtained through our proposed framework.

### Deficiencies of mechanism-agnostic ADP bounds.

Following our discussion, we can identify three potential sources for looseness in this approach for deriving mechanism-agnostic bounds. Firstly, it only uses a binary partitioning of subsampling pmfs \(s_{x}\) and \(s_{x^{\prime}}\). The resultant mechanism-specific guarantee only depends on divergences between two-component mixtures, which may not be sufficient to tightly bound the overall divergence in complicated scenarios like group privacy. Secondly, it neglects the pairwise distances of \(y_{1}^{(1)}\) and \(y_{2}^{(2)}\), resulting in potentially very large values of the cost function. Thirdly, the bound might be further loosened by applying joint convexity once more.

Recovering known mechanism-agnostic RDP guarantees

In the following, we demonstrate that existing amplification by subsampling guarantees for Renyi-DP can be derived by instantiating our proposed framework (see Fig. 2). Specifically, we demonstrate that these guarantees can be derived via the procedure discussed in Section 3.3 and shown in Figs. 2b and 2c: (1) Conditioning on at most \(4\) events indicating the presence of inserted / deleted / substituted elements, (2) defining a simultaneous coupling, and (3) using joint convexity to upper-bound the resultant mechanism-specific guarantee by component divergences.

These guarantees are mechanism-agnostic in the sense that they express the subsampled mechanism's privacy parameters \((\alpha,\rho)\) as a function of the base mechanism's privacy parameters. We derive guarantees for the general measure-theoretic problem setting introduced in Appendix D, where the base mechanism can be either discrete or continuous.

The reader may notice that parts of the proofs in Appendices H.1 and H.2 are very similar to those in [28, 30], safe for the discussion of couplings and \(d_{\mathbb{Y}}\)-compatibility. That is precisely the point: There is an optimal transport problem that implicitly underlies results from prior work, which we have identified and can now generalize to more challenging scenarios like group privacy amplification.

For this section, recall that \(\Delta_{\alpha}\) is not the Renyi divergence, but its \(\alpha\)th moment, i.e., a scaled and exponentiated Renyi divergence (see Definition D.8).

### Subsampling without replacement and substitution

For subsampling without replacement and substitution relation \(\simeq_{\Delta}\) we first show a more general result Theorem H.1. We then demonstrate that it can be upper-bounded via joint convexity of exponentiated Renyi divergence \(\Delta_{\alpha}\) to recover the guarantee from [28].

**Theorem H.1**.: _Let \(M=B\circ S\) be a subsampled mechanism, where \(S\) is subsampling without replacement with batch size \(q\). Let \(\simeq_{\mathbb{Y}}\) be the substitution relation \(\simeq_{\Delta,\mathbb{Y}}\). Then, for \(\alpha>1\) and all \(x\simeq_{\Delta,\mathbb{X}}x^{\prime}\) of size \(N\),_

\[\Delta_{\alpha}(m_{x}||m_{x^{\prime}})\leq\max_{\tilde{y}}\Delta_{\alpha}((1- w)\cdot b_{y_{1}^{(1)}}+w\cdot b_{y_{2}^{(1)}}||(1-w)\cdot b_{y_{1}^{(2)}}+w \cdot b_{y_{2}^{(2)}})\]

_subject to \(d_{\Delta,\mathbb{Y}}(y_{1}^{(1)},y_{2}^{(1)})\leq 1\), \(d_{\Delta,\mathbb{Y}}(y_{1}^{(1)},y_{2}^{(2)})\leq 1\), \(d_{\Delta,\mathbb{Y}}(y_{2}^{(1)},y_{2}^{(2)})\leq 1\), \(y_{1}^{(1)}=y_{1}^{(2)}\), and with \(w=q\,/\,N\)._

Proof.: Consider arbitrary \(x\simeq_{\Delta,\mathbb{X}}x^{\prime}\). By definition of \(\simeq_{\Delta}\), there must be some \(a\in x\), \(a^{\prime}\in x^{\prime}\) such that \(x^{\prime}=x\setminus\{a\}\cup\{a^{\prime}\}\). We thus define both \(A_{1}\) and \(E_{1}\) from Theorem E.2 to be the event that neither \(a\) nor \(a^{\prime}\) is sampled, i.e., \(A_{1}=E_{1}=\{y\in\mathbb{Y}\mid y\cap\{a,a^{\prime}\}=\varnothing\}\). We further define \(A_{2}\) and \(E_{2}\) to be the event that \(a\) or \(a^{\prime}\) is sampled, i.e., \(A_{2}=\overline{A_{1}}\) and \(E_{2}=\overline{E_{1}}\).

By definition of subsampling without replacement, we have

\[P_{S_{x}}(A_{1})=P_{S_{x^{\prime}}}(E_{1})=\mathrm{HyperGeom}(0 \mid N,1,q)=1-\frac{q}{N},\] \[P_{S_{x}}(A_{2})=P_{S_{x^{\prime}}}(E_{2})=\mathrm{HyperGeom}(1 \mid N,1,q)=\frac{q}{N},\]

which corresponds to the weights \((1-w)\) and \(w\), respectively. We further have

\[s_{x}(y\mid A_{1})=\begin{cases}\binom{|x|-1}{q}^{-1}&\text{if }y\subseteq x\wedge a \notin y\\ 0&\text{otherwise}\end{cases},\quad s_{x}(y\mid A_{2})=\begin{cases}\binom{|x|- 1}{q-1}^{-1}&\text{if }y\subseteq x\wedge a\in y\\ 0&\text{otherwise}\end{cases},\]

and

\[s_{x^{\prime}}(y\mid E_{1})=\begin{cases}\binom{|x|-1}{q}^{-1}&\text{if }y\subseteq x^{\prime}\wedge a^{\prime}\notin y\\ 0&\text{otherwise}\end{cases},\quad s_{x}(y\mid E_{2})=\begin{cases}\binom{|x|- 1}{q-1}^{-1}&\text{if }y\subseteq x^{\prime}\wedge a^{\prime}\in y\\ 0&\text{otherwise}\end{cases}.\]

**Coupling.** We now define a coupling \(\gamma:\mathbb{Y}^{2+2}\rightarrow\mathbb{R}_{+}\) that corresponds to the following generative process: We first generate \(y_{1}^{(1)}\) by sampling a batch that does not contain \(a\) uniformly at random from x. We then let \(y_{1}^{(2)}\gets y_{1}^{(1)}\) Finally, we pick a random element \(\tilde{a}\) of \(y_{1}^{(1)}\) and replace it with \(a\) and \(a^{\prime}\) to generate \(y_{2}^{(1)}\) and \(y_{2}^{(2)}\), respectively. More formally:

\[\gamma(\bm{y}^{(1)},\bm{y}^{(2)})=\begin{cases}s_{x}(y_{1}^{(1)}\mid A_{1}) \cdot\frac{1}{q}&\text{if }\bm{y}^{(1)},\bm{y}^{(2)}\text{ fulfills H.2}\\ 0&\text{otherwise.}\end{cases}\]

**Condition H.2**.: A tuple \(\bm{y}^{(1)}\in\mathbb{Y}^{2}\), \(\bm{y}^{(2)}\in\mathbb{Y}^{2}\) fulfills this condition when \(y_{1}^{(2)}=y_{1}^{(1)}\) and \(\exists\tilde{a}\in y_{1}^{(1)}:\Big{(}y_{2}^{(1)}=y_{1}^{(1)}\setminus\{ \tilde{a}\}\cup\{a\}\wedge y_{2}^{(2)}=y_{1}^{(1)}\setminus\{\tilde{a}\}\cup \{a^{\prime}\}\Big{)}\).

**Validity.** We now show that this constitutes a valid coupling. Consider \(y_{1}^{(1)}\). If and only if \(s_{x}(y_{1}^{(1)}\mid A_{1})>0\), there are exactly \(q\) combinations of \(y_{2}^{(1)},y_{1}^{(2)},y_{2}^{(2)}\) for which \(\gamma(\bm{y})\) is non-zero. Thus,

\[\sum_{y_{2}^{(1)},y_{1}^{(2)},y_{2}^{(2)}\in\mathbb{Y}^{3}}\gamma(\bm{y})=q \cdot s_{x}(y_{1}^{(1)}\mid A_{1}).\]

The proof for \(y_{1}^{(2)}\) is analogous.

Next, consider \(y_{2}^{(1)}\). If and only if \(s_{x}(y_{2}^{(1)}\mid A_{1})>0\), there are exactly \(|x|-q\) elements that could have been replaced by \(a\) to generate \(y_{2}^{(1)}\) from \(y_{2}^{(1)}\). Specifically, these elements are all elements that do not appear in \(y_{2}^{(1)}\). We thus have

\[\sum_{y_{1}^{(1)},y_{2}^{(2)},y_{2}^{(2)}\in\mathbb{Y}^{3}}\gamma(\bm{y})=(|x |-q)\cdot\frac{(|x|-1-q)!\cdot q!}{|x|-1}\cdot\frac{1}{q}=\binom{|x|-1}{q-1}^ {-1}.\]

The proof for \(y_{2}^{(2)}\) is analogous.

**Compatibility.** Finally, we show that \(\gamma\) is a \(d_{\mathbb{Y}}\)-compatible coupling (see Appendix F). Whenever \(\gamma(\bm{y})>0\), then

\[d_{\mathbb{Y}}(y_{1}^{(1)},y_{2}^{(1)}) =d_{\mathbb{Y}}\left(y_{1}^{(1)},\operatorname{supp}(s_{x}(\cdot \mid A_{2}))\right)=1,\] \[d_{\mathbb{Y}}(y_{1}^{(1)},y_{1}^{(2)}) =d_{\mathbb{Y}}\left(y_{1}^{(1)},\operatorname{supp}(s_{x^{\prime }}(\cdot\mid E_{1}))\right)=0,\] \[d_{\mathbb{Y}}(y_{1}^{(1)},y_{2}^{(2)}) =d_{\mathbb{Y}}\left(y_{1}^{(1)},\operatorname{supp}(s_{x^{\prime }}(\cdot\mid E_{2}))\right)=1.\]

Similarly, the pairwise distances between all \(y_{t},y_{u}\) with \(u>t>1\) are identical to the distance of their respective supports: The batches have a distance of \(1\) because one can transform one into another using a single substitution. The supports have a distance of \(1\) because one can transition from one to another using a single substitution.

The result then immediately follows from Corollary F.7. 

Next, we can derive the upper bound from [28]. For this derivation, we will use the following Lemma, which is proven in Appendix B of [6]:

**Lemma H.3**.: _Consider two probability measures \(P,Q\) on output measure space \((\mathbb{Z},\mathcal{Z},\lambda)\). Define \(p=\operatorname{d}P\,/\operatorname{d}\lambda\) and \(q=\operatorname{d}Q\,/\operatorname{d}\lambda\). Then_

\[\Delta_{\alpha}(p||q)=1+\sum_{l=2}^{\alpha}\binom{\alpha}{l}\int(p(z)-q(z))^{l }q(z)^{1-l}d\lambda(z).\]

The following proof essentially follows that of [28], but skips their Appendix B.2, since we have already successfully decomposed mixtures \(m_{x}\) and \(m_{x^{\prime}}\) into small terms that only involve base mechanism densities.

**Proposition H.4** (Wang et al. [28]).: _Let \(M=B\circ S\) be a subsampled mechanism, where \(S\) is subsampling without replacement with batch size \(q\). Let \(\simeq_{\mathbb{Y}}\) be the substitution relation \(\simeq_{\Delta,\mathbb{Y}}\). Then, for \(\alpha>1\) and all \(x\simeq_{\Delta,\mathbb{X}}x^{\prime}\) of size \(N\), \(\Delta_{\alpha}(m_{x}||m_{x^{\prime}})\) is l.e.q._

\[1+2\sum_{l=2}^{\alpha}{\alpha\choose l}w^{l}\max_{y\simeq_{\Delta,\mathbb{Y}} y^{\prime}}\Delta_{l}(b_{y}||b_{y^{\prime}}),\]

_with \(w=q\:/\:N\)._

Proof.: Using the constraint \(y_{1}^{(1)}=y_{1}^{(2)}\) in Theorem H.1, we can rewrite its objective as

\[\max_{y_{1}^{(1)},y_{2}^{(1)},y_{2}^{(2)}}\Delta_{\alpha}((1-w)\cdot b_{y_{1}^ {(1)}}+w\cdot b_{y_{2}^{(1)}}||(1-w)\cdot b_{y_{1}^{(1)}}+w\cdot b_{y_{2}^{(2)}}).\]

Using Lemma H.3, we can upper-bound its optimal value via

\[\max_{y_{1}^{(1)},y_{2}^{(2)},y_{2}^{(2)}}\Delta_{\alpha}((1-w) \cdot b_{y_{1}^{(1)}}+w\cdot b_{y_{2}^{(1)}}||(1-w)\cdot b_{y_{1}^{(1)}}+w \cdot b_{y_{2}^{(2)}})\] \[= \max_{y_{1}^{(1)},y_{2}^{(2)},y_{2}^{(2)}}1+\sum_{l=2}^{\alpha}{ \alpha\choose l}\int\frac{(w\cdot b_{y_{2}^{(1)}}-w\cdot b_{y_{2}^{(2)}})^{l} }{\left((1-w)\cdot b_{y_{1}^{(1)}}+w\cdot b_{y_{2}^{(2)}}\right)^{l-1}}d\lambda (z)\] \[\leq \max_{y_{1}^{(1)},y_{2}^{(1)},y_{2}^{(2)}}1+\sum_{l=2}^{\alpha}{ \alpha\choose l}w^{l}\int\frac{|b_{y_{2}^{(1)}}-b_{y_{2}^{(2)}}|^{l}}{\left((1 -w)\cdot b_{y_{1}^{(1)}}+w\cdot b_{y_{2}^{(2)}}\right)^{l-1}}d\lambda(z)\] \[\leq \sum_{l=0}^{\alpha}{\alpha\choose l}w^{l}\max_{y_{1}^{(1)},y_{2}^ {(2)}}\int\frac{|b_{y_{2}^{(1)}}-b_{y_{2}^{(2)}}|^{l}}{\left((1-w)\cdot b_{y_ {1}^{(1)}}+w\cdot b_{y_{2}^{(2)}}\right)^{l-1}}d\lambda(z),\]

where each of the \(\alpha+1\) optimization problems is independently constrained by \(d_{\mathbb{Y}}(y_{1}^{(1)},y_{2}^{(1)})\leq 1\), \(d_{\mathbb{Y}}(y_{1}^{(1)},y_{2}^{(2)})\leq 1\), and \(d_{\mathbb{Y}}(y_{2}^{(1)},y_{2}^{(2)})\leq 1\).

Next, we bound the optimal value of each of the \(\alpha+1\) optimization problems. Using the joint convexity of \(x,y\mapsto x^{l}\cdot y^{1-l}\), which implies convexity in the second component, shows that

\[\max_{y_{1}^{(1)},y_{2}^{(1)},y_{2}^{(2)}}\int\frac{|b_{y_{2}^{(1 )}}-b_{y_{2}^{(2)}}|^{l}}{\left((1-w)\cdot b_{y_{1}^{(1)}}+w\cdot b_{y_{2}^{(2 )}}\right)^{l-1}}d\lambda(z)\] \[\leq \max_{y_{1}^{(1)},y_{2}^{(1)},y_{2}^{(2)}}(1-w)\cdot\int\frac{|b_ {y_{2}^{(1)}}-b_{y_{2}^{(2)}}|^{l}}{b_{y_{1}^{(1)}}^{l-1}}d\lambda(z)+w\cdot \int\frac{|b_{y_{2}^{(1)}}-b_{y_{2}^{(2)}}|^{l}}{b_{y_{2}^{(2)}}^{l-1}}d \lambda(z)\] \[\leq (1-w)\cdot\left(\max_{y_{1}^{(1)},y_{2}^{(1)},y_{2}^{(2)}}\int \frac{|b_{y_{2}^{(1)}}-b_{y_{2}^{(2)}}|^{l}}{b_{y_{1}^{(1)}}^{l-1}}d\lambda(z) \right)+w\cdot\left(\max_{y_{1}^{(1)},y_{2}^{(1)},y_{2}^{(2)}}\int\frac{|b_{y_ {2}^{(1)}}-b_{y_{2}^{(2)}}|^{l}}{b_{y_{1}^{(1)}}^{l-1}}d\lambda(z)\right)\] \[\leq (1-w)\cdot\left(\max_{y_{1}^{(1)},y_{2}^{(1)},y_{2}^{(2)}}\int \frac{|b_{y_{2}^{(1)}}-b_{y_{2}^{(2)}}|^{l}}{b_{y_{1}^{(1)}}^{l-1}}d\lambda(z) \right)+w\cdot\left(\max_{y_{1}^{(1)},y_{2}^{(1)},y_{2}^{(2)}}\int\frac{|b_{y_ {2}^{(1)}}-b_{y_{2}^{(2)}}|^{l}}{b_{y_{1}^{(1)}}^{l-1}}d\lambda(z)\right)\] \[= \max_{y_{1}^{(1)},y_{2}^{(1)},y_{2}^{(2)}}\int\frac{|b_{y_{2}^{(1 )}}-b_{y_{2}^{(2)}}|^{l}}{b_{y_{1}^{(1)}}^{l-1}}d\lambda(z),\]

where all optimization problems are independent, with each one being independently constrained by \(d_{\mathbb{Y}}(y_{1}^{(1)},y_{2}^{(1)})\leq 1\), \(d_{\mathbb{Y}}(y_{1}^{(1)},y_{2}^{(2)})\leq 1\), and \(d_{\mathbb{Y}}(y_{2}^{(1)},y_{2}^{(2)})\leq 1\). Note that, for the last inequality, we replaced a \(y_{2}^{(2)}\) in the second maximization with a \(y_{1}^{(1)}\), which essentially adds a degree of freedom and thus leads to an upper bound.

In [28], the optimal value of the final problem is referred to as the ternary-\(|\chi|^{l}\)-divergence of \(b_{z}\), \(b_{y_{2}^{(2)}}\), and \(b_{y_{1}^{(1)}}\). As shown in their Lemma 19, it can be upper bounded via \(2\cdot\max_{y\simeq_{\Delta,\mathbb{Y}}y^{\prime}}\Delta_{l}(b_{y}||b_{y^{ \prime}})\), which concludes our proof.

**Additional terms.** Note that [28] derive three additional bounds (see their Lemma 17, Lemma 19, and Theorem 27) on the the ternary-\(|\chi|^{l}\)-divergence. This introduces additional terms, but does not change the fact that their result is lower-bounded by Theorem H.1. We use their full theorem as a baseline in our experiments.

### Poisson subsampling and insertion/removal

Next, we show that the Poisson subsampling guarantees in [30, 7] follow from another optimal transport problem. For our proof, we will use the following Lemma, which corresponds to the "novel alternative decomposition" in Appendix A.1 of [30].

**Lemma H.5**.: _Consider \(K+1\in\mathbb{N}\) distributions \(P,Q_{1},\ldots,Q_{K}\) on output measure space \((\mathbb{Z},\mathcal{Z},\lambda)\), and define \(p=\mathrm{d}P\ /\mathrm{d}\lambda\), \(q_{k}=\mathrm{d}Q_{k}\ /\ \mathrm{d}\lambda\). Further consider some \(w_{1},\ldots,w_{K}\in[0,1]\) with \(\sum_{k=1}^{K}w_{k}=1\). Then,_

\[\Delta_{\alpha}\left(p||\sum_{k=1}^{K}w_{k}\cdot q_{k}\right)\leq\sum_{k=1}^{ K}w_{k}\cdot\Delta_{\alpha}\left(q_{k}+p-\sum_{l=1}^{K}w_{l}\cdot q_{l}||q_{k} \right).\]

Proof.: Based on the definition of \(\Delta_{\alpha}\), we have

\[\Delta_{\alpha}\left(p||\sum_{k=1}^{K}w_{k}\cdot q_{k}\right)\] \[=\int\frac{p(z)^{\alpha}}{\left(\sum_{k=1}^{K}w_{k}\cdot q_{k}(z) \right)^{\alpha-1}}\ \mathrm{d}\lambda(z)\] \[=\int\frac{\left(\left(\sum_{k=1}^{K}w_{k}\cdot q_{k}(z)\right)+p (z)-\left(\sum_{l=1}^{K}w_{l}\cdot q_{l}(z)\right)\right)^{\alpha}}{\left( \sum_{k=1}^{K}w_{k}\cdot q_{k}(z)\right)^{\alpha-1}}\ \mathrm{d}\lambda(z)\] \[=\int\frac{\left(\sum_{k=1}^{K}w_{k}\cdot\left(q_{k}(z)+p(z)- \sum_{l=1}^{K}w_{l}\cdot q_{l}(z)\right)\right)^{\alpha}}{\left(\sum_{k=1}^{K }w_{k}\cdot q_{k}(z)\right)^{\alpha-1}}\ \mathrm{d}\lambda(z)\]

The result then follows from joint convexity of \(\Delta_{\alpha}\). 

Note that the proof of this lemma is very similar to the proof strategy we used in deriving Proposition H.4 using Lemma H.3: We add \(0=c-c\) with some \(c\) to the numerator (which leads to the binomial expansion in Lemma H.3) and then apply joint convexity to obtain an upper bound.

**Proposition H.6** (Zhu and Wang [30]).: _Let \(M=B\circ S\) be a subsampled mechanism, where \(S\) is Poisson subsampling with rate \(r\). Let \(\simeq_{\mathbb{Y}}\) be the insertion/removal relation \(\simeq_{\pm,\mathbb{Y}}\). Then, for \(\alpha>1\) and all \(x\simeq_{\pm,\mathbb{X}}x^{\prime}\), \(\Delta_{\alpha}(m_{x}||m_{x^{\prime}})\) is l.e.q._

\[2\cdot\sum_{l=0}^{\alpha}\binom{\alpha}{l}r^{l}(1-r)^{\alpha-l}\max_{y\simeq_ {\pm,\mathbb{Y}}y^{\prime}}\Delta_{l}(b_{y}||b_{y^{\prime}}).\]

Proof.: Since we are concerned with insertion/removal, we need to consider two cases:

**Case 1: Removal.** In this case, there is some \(a\in x\) such that \(x^{\prime}=x\setminus\{a\}\). We let \(A_{1}\) be the event that \(a\) is not sampled, i.e. \(A_{1}=\{y\in\mathbb{Y}\ |\ a\notin y\}\), and let \(A_{2}=\overline{A_{1}}\). We let \(E_{1}=\mathbb{Y}\), i.e., do not condition on any particular event.

By definition of Poisson subsampling, we have \(P_{S_{x}}(A_{1})=1-r\), \(P_{S_{x}}(A_{2})=r\), and \(P_{S_{x^{\prime}}}(E_{1})=1\). We further have

\[s_{x}(y\ |\ A_{1}) =\begin{cases}r^{|y|}(1-r)^{|x|-|y|-1}&\text{if }y\subseteq x \wedge a\notin y\\ 0&\text{otherwise}\end{cases}\] \[s_{x}(y\ |\ A_{2}) =\begin{cases}r^{|y|-1}(1-r)^{|x|-|y|}&\text{if }y\subseteq x \wedge a\in y\\ 0&\text{otherwise}\end{cases},\]\[s_{x^{\prime}}(y\mid E_{1})=\begin{cases}r^{|y|}(1-r)^{|x^{\prime}|-|y|}&\text{if }y \subseteq x^{\prime}\\ 0&\text{otherwise}\end{cases}.\]

Note that \(s_{x}(y\mid A_{1})=s_{x^{\prime}}(y\mid E_{1})\).

**Coupling.** We now define a coupling \(\gamma:\mathbb{Y}^{2+1}\to\mathbb{R}_{+}\) that corresponds to the following generative process: We first generate \(y_{1}^{(1)}\) by sampling a batch that does not contain \(a\) via Poisson subsampling from \(x\setminus\{a\}\). We then let \(y_{1}^{(2)}\gets y_{1}^{(1)}\). Finally, we deterministically insert \(a\) to generate \(y_{2}^{(1)}\). This can be formally defined via

\[\gamma(\boldsymbol{y}^{(1)},\boldsymbol{y}^{(2)})=\begin{cases}s_{x}(y_{1}^{( 1)}\mid A_{1})&\text{if }y_{1}^{(1)}=y_{1}^{(2)}\wedge y_{2}^{(1)}=y_{1}^{(1)}\cup\{a\}\\ 0&\text{otherwise}.\end{cases}\]

**Validity.** We can verify the validity of this coupling as follows: For every \(y_{1}^{(1)}\) with \(s_{x}(y_{1}^{(1)}\mid A_{1})>0\), there is exactly one combination \(y_{2}^{(1)},y_{1}^{(2)}\) for which \(\gamma(\boldsymbol{y})>0\), namely \(y_{1}^{(2)}=y_{1}^{(1)}\) and \(y_{2}^{(1)}=y_{1}^{(1)}\cup\{a\}\). We thus have \(\sum_{y_{2}^{(1)},y_{1}^{(2)}\in\mathbb{Y}^{2}}\gamma(y_{1}^{(1)},y_{2}^{(1) },y_{1}^{(2)})=s_{x}(y_{1}^{(1)}\mid A_{1}).\) The proof for \(y_{1}^{(2)}\) is analogous. For every \(y_{2}^{(1)}\), we have exactly one combination of \(y_{1}^{(1)},y_{1}^{(2)}\) for which \(\gamma(\boldsymbol{y})>0\), namely \(y_{1}^{(1)}=y_{1}^{(2)}=y_{2}^{(1)}\setminus\{a\}\). We thus have

\[\sum_{y_{1}^{(1)},y_{2}^{(1)}\in\mathbb{Y}^{2}}\gamma(y_{1}^{(1)},y_{2}^{(1) },y_{1}^{(2)})\]

\[= s_{x}(y_{2}^{(1)}\setminus\{a\}\mid A_{1})=r^{|y_{2}^{(1)}|-1}(1-r)^{|x|-(|y |-1)-1}=r^{|y_{2}^{(1)}|-1}(1-r)^{|x|-|y|}=s_{x}(y_{2}^{(1)}\mid A_{2}).\]

**Compatibility.** Finally, it can be easily shown that \(\gamma\) is a \(d_{\mathbb{Y}}\)-compatible coupling (see Appendix F). Whenever \(\gamma(\boldsymbol{y})>0\), then

\[d_{\mathbb{Y}}(y_{1}^{(1)},y_{2}^{(1)}) =d_{\mathbb{Y}}\left(y_{1}^{(1)},\operatorname{supp}(s_{x}(\cdot \mid A_{2}))\right)=1,\] \[d_{\mathbb{Y}}(y_{1}^{(1)},y_{1}^{(2)}) =d_{\mathbb{Y}}\left(y_{1}^{(1)},\operatorname{supp}(s_{x^{\prime }}(\cdot\mid E_{1}))\right)=0.\]

Similarly, the pairwise distance between \(y_{2}^{(1)}\) and \(y_{1}^{(2)}\) is always \(1\). This is identical to the distance of their supports, because one can always transition between them via a single insertion/removal.

It thus follows from Corollary F.7 that

\[\Delta_{\alpha}(m_{x}||m_{x^{\prime}}) \leq\max_{y_{1}^{(1)},y_{2}^{(1)},y_{1}^{(1)}}\Delta_{\alpha}((1- r)\cdot b_{y_{1}^{(1)}}+r\cdot b_{y_{2}^{(1)}}||b_{y_{1}^{(2)}})\] s.t. \[d_{\pm,\mathbb{Y}}(y_{1}^{(1)},y_{2}^{(1)})\leq 1,y_{1}^{(1)}=y_{1}^{(2 )},d_{\pm,\mathbb{Y}}(y_{2}^{(1)},y_{1}^{(2)})\leq 1\]

Because \(y_{1}^{(1)}=y_{1}^{(2)}\) and because \(d_{\pm,\mathbb{Y}}(y_{1}^{(1)},y_{2}^{(1)})\leq 1\) is equivalent to \(y_{1}^{(1)}\simeq_{\pm,\mathbb{Y}}y_{2}^{(1)}\), this bound can be restated as

\[\Delta_{\alpha}(m_{x}||m_{x^{\prime}})\leq\max_{y\simeq_{\pm,\mathbb{Y}}y^{ \prime}}\Delta_{\alpha}((1-r)\cdot b_{y}+r\cdot b_{y^{\prime}}||b_{y}).\]

Finally, one can use the definition of \(\Delta_{\alpha}\) and binomial expansion to show

\[\Delta_{\alpha}(m_{x}||m_{x^{\prime}}) \leq\max_{y\simeq_{\pm,\mathbb{Y}}y^{\prime}}\int_{\mathbb{Z}} \frac{\left((1-r)b_{y}(z)+rb_{y^{\prime}}(z)\right)^{\alpha}}{b_{y}(z)^{\alpha -1}}\,\mathrm{d}\lambda(z)\] \[=\max_{y\simeq_{\pm,\mathbb{Y}}y^{\prime}}\int_{\mathbb{Z}}\left( \frac{(1-r)b_{y}(z)+rb_{y^{\prime}}(z)}{b_{y}(z)}\right)^{\alpha}b_{y}(z)\, \mathrm{d}\lambda(z)\] \[=\max_{y\simeq_{\pm,\mathbb{Y}}y^{\prime}}\int_{\mathbb{Z}}\left( (1-r)+r\frac{b_{y^{\prime}}(z)}{b_{y}(z)}\right)^{\alpha}b_{y}(z)\,\mathrm{d} \lambda(z)\] \[=\max_{y\simeq_{\pm,\mathbb{Y}}y^{\prime}}\sum_{l=0}^{\alpha} \binom{\alpha}{l}r^{l}(1-r)^{\alpha-l}\Delta_{l}(b_{y}||b_{y^{\prime}})\] \[\leq\sum_{l=0}^{\alpha}\binom{\alpha}{l}r^{l}(1-r)^{\alpha-l}\max_ {y^{\alpha}\doteq_{\pm,\mathbb{Y}}y^{\prime}}\Delta_{l}(b_{y}||b_{y^{\prime}}).\]Note that this is smaller than the term in Proposition H.6 by a factor of \(2\).

**Case 2: Insertion** In this case, there is some \(a\in x^{\prime}\) such that \(x=x^{\prime}\setminus\{a\}\). Very similar to before, we let \(A_{1}=\mathbb{Y}\), i.e., we do not condition on any particular event. We further let \(E_{1}\) be the event that \(a\) is not sampled, i.e. \(E_{1}=\{y\in\mathbb{Y}\mid a\notin y\}\), and let \(E_{2}=\overline{E_{1}}\). We notice that we have exactly the same conditional distributions as in the first case. We can thus define exactly the same coupling (up to changes in indexing) to show that

\[\Delta_{\alpha}(m_{x}||m_{x^{\prime}})\leq\max_{y\geq\pm,\cdot y^{\prime}} \Delta_{\alpha}(b_{y}||(1-r)b_{y}+rb_{y^{\prime}}).\]

Next, applying Lemma H.5 and using the definition of \(\Delta_{\alpha}\) shows that

\[\Delta_{\alpha}(m_{x}||m_{x^{\prime}})\] \[\leq \max_{y\leq\pm,\cdot y^{\prime}}\Delta_{\alpha}(b_{y}||(1-r)b_{y} +rb_{y^{\prime}})\] \[\leq \max_{y\leq\pm,\cdot y^{\prime}}(1-r)\Delta_{\alpha}(b_{y}+b_{y} -((1-r)b_{y}+rb_{y^{\prime}})||b_{y})+r\Delta_{\alpha}(b_{y^{\prime}}+b_{y}-( (1-r)b_{y}+rb_{y^{\prime}})||b_{y^{\prime}})\] \[= \max_{y\geq\pm,\cdot y^{\prime}}(1-r)\Delta_{\alpha}((1+r)b_{y}-rb _{y^{\prime}}||b_{y})+r\Delta_{\alpha}((1-r)b_{y^{\prime}}+rb_{y}||b_{y^{ \prime}})\]

This corresponds exactly to Eq. 6 of the "novel alternative decomposition" in Appendix A.1 of [30]. One can then through the remaining steps in their Appendix A to show that this term is at most two times larger than the one we derived in the deletion case. 

As already mentioned at the beginning of this section, the proof is very similar to that in [30], except for the discussion of couplings and \(d_{\mathbb{Y}}\)-compatibility. We emphasize that the crucial aspect, which is not discussed in any prior work on Renyi-DP amplification, is that there is an implicit, underlying optimal transport problem between conditional subsampling distributions. Now that we have identified this connection, we can apply this more general optimal transport principle to a much broader range of amplification by subsampling scenarios for Renyi-DP.

**Further tightening.** The factor \(2\) in Proposition H.6 can be eliminated for distributions with particular symmetries (see Theorem 5 in [29]) or bounded Pearson-Vajda \(\chi^{l}\)-pseudo-divergence (see Theorem 8 in [30]). We thus do not include the factor \(2\) when using this amplification guarantee as a baseline in our experiments.

### Graph subsampling without replacement and node modification

Daigavane et al. [40] consider a setting that differs from the usual insertional/removal into datasets: Node-level privacy for graphs. There, the dataset space is the set of all directed, attributed graphs \(\mathbb{X}=\bigcup_{N,D\in\mathbb{N}}\mathbb{R}^{N\times D}\times\{0,1\}^{N \times N}\), which are composed of a continuous feature matrix and a discrete adjacency matrix. Two graphs \(x,x^{\prime}\) are related by dataset relation \(\simeq_{\mathbb{X}}\) if \(x^{\prime}\) can be constructed by inserting a node (including new edges) into \(x\), or removing a node (including its edges) from \(x\).

To analyze this problem setting, they perform a preprocessing step (see their Algorithm 2) to represent each graph by a set of subgraphs, with each subgraph corresponding to a node in the graph. Via this construction, they return to the traditional setting where \(\mathbb{X}\subseteq\mathcal{P}(\mathbb{A})\) for some set \(\mathbb{A}\), and the modification of a node's features and edges corresponds to the substitution of \(K\) elements,4 i.e.

Footnote 4: Note that they do not actually consider insertion/removal, because their analysis assumes the number of subgraphs to remain constant.

\[\simeq_{K\Delta,\mathbb{X}}=\{(x,x^{\prime})\in\mathbb{X}\mid\exists g\in x \setminus x^{\prime},g^{\prime}\in x^{\prime}\setminus x:x^{\prime}=x \setminus g\cup g^{\prime}\wedge|g|=|g^{\prime}|=K\},\] (22)

where \(K\) depends on the maximum considered graph degree.

Irrespective of the graph neural network specific details, this discussion suggests that group privacy and differentially private learning for graphs are related.

We shall now demonstrate that their result for subsampling without replacement can be derived from Theorem E.1, i.e. optimal transport without conditioning.

**Proposition H.7** (Daigavane et al. [40]).: _Let \(M=B\circ S\) be a subsampled mechanism, where \(S\) is subsampling without replacement with batch size \(q\). Let \(\simeq_{\mathbb{Y}}\) be the substitution relation \(\simeq_{\Delta,\mathbb{Y}}\). Let\(\simeq_{K\Delta,\mathbb{X}}\) be the K-fold substitution relation defined in Eq. (22). Then, for \(\alpha>1\) and all \(x\simeq_{K\Delta,\mathbb{X}}x^{\prime}\) of size \(N\),_

\[\Delta_{\alpha}(m_{x}||m_{x^{\prime}})\leq\sum_{k=0}^{K}w_{k}\cdot\max_{y\simeq_ {k\Delta,\mathbb{Y}}y^{\prime}}\Delta_{k}(b_{y}||b_{y^{\prime}}).\]

_with \(w_{k}=\mathrm{HyperGeom}(k\mid N,K,q)\)._

Proof.: Consider arbitrary \(x\simeq_{\Delta,\mathbb{X}}x^{\prime}\). By definition of \(\simeq_{\Delta}\), there must be some \(g\in x\setminus x^{\prime}\), \(g^{\prime}\in x^{\prime}\setminus x\) such that \(x^{\prime}=x\setminus g\cup g^{\prime}\) and \(|g|=|g^{\prime}|=K\).

We condition on the events \(A_{1}=\mathbb{Y}\) and \(E_{1}=\mathbb{Y}\), meaning

\[s_{x}(y\mid A_{1})=\begin{cases}\binom{N}{q}^{-1}&\text{if }y\subseteq x\\ 0&\text{otherwise}\end{cases}\quad\text{and}\quad s_{x^{\prime}}(y\mid E_{1})= \begin{cases}\binom{N}{q}^{-1}&\text{if }y\subseteq x^{\prime}\\ 0&\text{otherwise}\end{cases}.\]

**Coupling.** We now define a coupling via \(\gamma:\mathbb{Y}^{1+1}\to\mathbb{R}_{+}\) that define the following generative process: We first define \(y^{(1)}\) by sampling a batch of size \(q\) from \(x\) without replacement. We then remove all elements that are in \(g\) and insert an equal number of elements, which are chosen uniformly at random from \(g^{\prime}\). This coupling can be formally defined as

\[\gamma(y^{(1)},y^{(2)})=\begin{cases}s_{x}(y^{(1)}\mid A_{1})\cdot\binom{K}{ \lfloor y^{(1)}\cap g\rfloor}^{-1}&\text{if }y^{(1)},y^{(2)}\text{ fulfills Condition H.8,}\\ 0&\text{otherwise}.\end{cases}.\]

**Condition H.8**.: A tuple \(y^{(1)}\in\mathbb{Y}\), \(y^{(2)}\in\mathbb{Y}\) fulfills this condition when there exists a \(\tilde{g}\subseteq g\) such that \(y^{(2)}=y^{(1)}\setminus g\cup\tilde{g}\) and \(|y^{(2)}\cap\tilde{g}|=|y^{(1)}\cap g|\).

**Validity.** We now show that this constitutes a valid coupling. Consider any \(y^{(1)}\) with \(s_{x}(y^{(1)}\mid A_{1})>0\) and \(|y^{(1)}\cap g|=k\). There are exactly \(\binom{K}{k}\) different \(y^{(2)}\) for which \(\gamma(\bm{y})\) is non-zero, and each one has the same probability. Thus,

\[\sum_{y^{(2)}\in\mathbb{Y}}\gamma(\bm{y})=\binom{K}{k}\cdot s_{x}(y^{(1)}\mid A _{1})\cdot\binom{K}{k}^{-1}=s_{x}(y^{(1)}\mid A_{1}).\binom{|x-1|}{q}.\]

The other case is analogous.

**Compatibility.** Evidently, this is a \(d_{\mathbb{Y}}\)-compatible coupling, because (a) whenever \(y^{(1)}\) contains \(k\) elements from \(g\), it has a distance of \(k\) from \(\mathrm{s_{\mathbb{Y}}}(y\mid E_{1})\) and (b) whenever \(y^{(1)}\) contains \(k\) elements from \(g\), we generate \(y^{(2)}\) by substituting exactly \(k\) elements.

It thus follows from Corollary F.7 that

\[\Delta_{\alpha}(m_{x}||m_{x^{\prime}})\leq\sum_{y}s_{x}(y^{(1)}\mid A_{1}) \kappa_{\alpha}(y^{(1)})\]

with

\[\kappa_{\alpha}(y^{(1)})=\max_{y,y^{\prime}}\Delta_{\alpha}(b_{y}||b_{y^{ \prime}})\quad\text{s.t.}\quad d_{\mathbb{Y}}(y,y^{\prime})\leq|y^{(1)}\cap g|.\]

The result then follows from the definition of induced distance, the definition of the K-fold substitution relation \(\simeq_{K\Delta,\mathbb{X}}\) and the fact that \(|y^{(1)}\cap g|\) is a random variable with distribution \(\mathrm{HyperGeom}(N,K,q)\). 

Note that [40] instantiate this result with Gaussian mechanisms with sensitivity \(C\), for which \(\max_{y\simeq_{k\Delta,\mathbb{Y}}y^{\prime}}\Delta_{\alpha}(b_{y}||b_{y^{ \prime}})=\exp(\alpha\cdot(\alpha-1)\cdot\frac{k^{2}C^{2}}{\sigma^{2}})\).

In Appendix B.1.4, we experimentally demonstrate that this result can improve upon the baseline of combining [28] with the traditional group privacy property, but is not sufficiently tight to consistently outperform it across a wide range of parameters. This emphasizes the benefit of considering optimal transport between proper conditional distributions when deriving amplification guarantees.

Novel RDP guarantees

Now that we have a framework that enables generic subsampling analysis for Renyi differential privacy, we can derive a variety of novel guarantees that were previously only available for approximate differential privacy.

In the following, we first demonstrate that RDP guarantees for a dataset relation \(\simeq_{\mathbb{X}}\) can be derived from a base mechanism that is only known to be RDP under a different batch relation \(\simeq_{\mathbb{Y}}\), i.e., "hybrid neighboring relations" [15]. We then show that we can derive RDP guarantees for non-standard combinations of subsampling schemes and neighboring relations, such as subsampling without replacement and insertion/removal. Finally, we derive a simple, tight mechanism-specific subsampling guarantee for randomized response. We use this guarantee in Section 4 to demonstrate the limitations of using mechanism-agnostic RDP bounds instead of mechanism-specific RDP bounds.

For this section, recall again that \(\Lambda_{\alpha}\) is not the Renyi divergence, but its \(\alpha\)th moment, i.e., a scaled and exponentiated Renyi divergence (see Definition D.8).

### Hybrid neighboring relations

Hybrid neighboring relations have thus far not been discussed in the context of Renyi-DP. Analyzing such scenarios may be particularly useful when the dataset space \(\mathbb{X}\) and the batch space \((\mathbb{Y},\mathcal{Y})\) are different from each other, e.g., when mapping from large text corpora to short sequences of token embeddings. Note that hybrid relations may also be useful for noisy stochastic gradient descent with fixed batch sizes (see end of Appendix I.2).

As an example, we consider the following scenario: Subsampling without replacement, with substitution relation \(\simeq_{\Delta,\mathbb{X}}\) for datasets and insertion/removal relation \(\simeq_{\pm,\mathbb{Y}}\) for batches. The proof is largely identical to that of Theorem H.1, but uses the fact that a substitution can be represented by an insertion, followed by a removal.

**Theorem I.1**.: _Let \(M=B\circ S\) be a subsampled mechanism, where \(S\) is subsampling without replacement with batch size \(q\). Let \(\simeq_{\mathbb{Y}}\) be the insertion/removal relation \(\simeq_{\pm,\mathbb{Y}}\) and \(\simeq_{\mathbb{X}}\) be the substitution relation \(\simeq_{\Delta,\mathbb{X}}\). Then, for \(\alpha>1\) and all \(x\simeq_{\Delta,\mathbb{X}}x^{\prime}\) of size \(N\),_

\[\Lambda_{\alpha}(m_{x}||m_{x^{\prime}})\leq\max_{\bm{y}}\Lambda_{\alpha}((1-w) \cdot b_{y_{1}^{(1)}}+w\cdot b_{y_{2}^{(1)}}\|(1-w)\cdot b_{y_{1}^{(2)}}+w \cdot b_{y_{2}^{(2)}})\]

_with \(\bm{y}\in\mathbb{Y}^{2+2}\) subject to \(y_{1}^{(1)}=y_{1}^{(2)}\), \(d_{\pm,\mathbb{Y}}(y_{1}^{(1)},y_{2}^{(1)})\leq 2\), \(d_{\pm,\mathbb{Y}}(y_{1}^{(1)},y_{2}^{(2)})\leq 2\), \(d_{\pm,\mathbb{Y}}(y_{1}^{(1)},y_{2}^{(2)})\leq 2\), and with \(w=q\ /\ N\)._

Proof.: Consider arbitrary \(x\simeq_{\Delta,\mathbb{X}}x^{\prime}\). By definition of \(\simeq_{\Delta}\), there must be some \(a\in x\), \(a^{\prime}\in x^{\prime}\) such that \(x^{\prime}=x\setminus\{a\}\cup\{a^{\prime}\}\). We thus define both \(A_{1}\) and \(E_{1}\) from Theorem E.2 to be the event that neither \(a\) nor \(a^{\prime}\) is sampled, i.e., \(A_{1}=E_{1}=\{y\in\mathbb{Y}\mid y\cap\{a,a^{\prime}\}=\varnothing\}\). We further define \(A_{2}\) and \(E_{2}\) to be the event that \(a\) or \(a^{\prime}\) is sampled, i.e., \(A_{2}=\overline{A_{1}}\) and \(E_{2}=\overline{E_{1}}\).

By definition of subsampling without replacement, we have

\[P_{S_{x}}(A_{1}) =P_{S_{x^{\prime}}}(E_{1})=\mathrm{HyperGeom}(0\mid N,1,q)=1- \frac{q}{N},\] \[P_{S_{x}}(A_{2}) =P_{S_{x^{\prime}}}(E_{2})=\mathrm{HyperGeom}(1\mid N,1,q)=\frac {q}{N},\]

which corresponds to the weights \((1-w)\) and \(w\), respectively.

**Coupling.** We now define a coupling via \(\gamma:\mathbb{Y}^{4}\to\mathbb{R}_{+}\):

\[\gamma(\bm{y}^{(1)},\bm{y}^{(2)})=\begin{cases}s_{x}(y_{1}^{(1)}\mid A_{1}) \cdot\frac{1}{q}&\text{if }\bm{y}^{(1)},\bm{y}^{(2)}\text{ fulfill Condition H.2,}\\ 0&\text{otherwise.}\end{cases}\]

**Condition H.2**.: A tuple \(\bm{y}^{(1)}\in\mathbb{Y}^{2}\), \(\bm{y}^{(2)}\in\mathbb{Y}^{2}\) fulfills this condition when \(y_{1}^{(2)}=y_{1}^{(1)}\) and \(\exists\tilde{a}\in y_{1}^{(1)}:\Big{(}y_{2}^{(1)}=y_{1}^{(1)}\setminus\{ \tilde{a}\}\cup\{a\}\wedge y_{2}^{(2)}=y_{1}^{(1)}\setminus\{\tilde{a}\}\cup\{a^{ \prime}\}\Big{)}\).

As explained before, \(\gamma\) defines the following generative process: We first generate \(y_{1}^{(1)}\) by sampling a batch that does not contain \(a\) uniformly at random from x. We then let \(y_{1}^{(2)}\gets y_{1}^{(1)}\) Finally, we pick a random element \(\tilde{a}\) of \(y_{1}^{(1)}\) and replace it with \(a\) and \(a^{\prime}\) to generate \(y_{2}^{(1)}\) and \(y_{2}^{(2)}\), respectively. Note that each of these substitution corresponds to exactly one insertion and one removal.

**Validity.** The validity of this coupling has already been demonstrated in our proof of Theorem H.1, since validity of a coupling does not depend on batch neighboring relation \(\simeq_{\mathbb{Y}}\).

**Compatibility.** We can thus focus on showing that \(\gamma\) is a \(d_{\mathbb{Y}}\)-compatible coupling (see Appendix F). Whenever \(\gamma(\bm{y})>0\), then

\[d_{\mathbb{Y}}(y_{1}^{(1)},y_{2}^{(1)}) =d_{\mathbb{Y}}\left(y_{1}^{(1)},\mathrm{supp}(s_{x}(\cdot\mid A_ {2}))\right)=2,\] \[d_{\mathbb{Y}}(y_{1}^{(1)},y_{1}^{(2)}) =d_{\mathbb{Y}}\left(y_{1}^{(1)},\mathrm{supp}(s_{x^{\prime}}( \cdot\mid E_{1}))\right)=0,\] \[d_{\mathbb{Y}}(y_{1}^{(1)},y_{2}^{(2)}) =d_{\mathbb{Y}}\left(y_{1}^{(1)},\mathrm{supp}(s_{x^{\prime}}( \cdot\mid E_{2}))\right)=2.\]

We see that the pairwise distances between all \(y_{t},y_{u}\) with \(u>t>1\) are identical to the distance of their respective supports: The batches have a distance of \(2\) because one can transform one into another using a single substitution, i.e. an insertion and a removal. The supports also have a distance of \(2\) because one can transition from one to another using a single substitution.

The result then immediately follows from Corollary F.7. 

If we wanted to express this result in terms of the Renyi-DP parameters of the base mechanism, we could go through the same derivations as in our proof of Proposition H.4 to show that

\[\Lambda_{\alpha}(m_{x}||m_{x^{\prime}})\leq 1+2\sum_{l=2}^{\alpha}\binom{ \alpha}{l}w^{l}\left(\max_{y,y^{\prime}}\Lambda_{l}(b_{y}||b_{y^{\prime}}) \quad\text{s.t.}\quad d_{\pm,\mathbb{Y}}(y,y^{\prime})\leq 2\right),\]

with \(w=q\,/\,N\). The inner term can, for instance, be evaluated using the traditional group privacy property from [31].

### Subsampling without replacement under insertion/removal

To demonstrate that our framework is not limited to analyzing the usually considered combination of subsampling without replacement and substitution or Poisson subsampling and insertion/removal, we consider the following non-standard combination: Subsampling without replacement and insertion/removal.

We show that we can derive a guarantee that is qualitatively similar to that of [30], while preserving a fixed batch size. Such results could be useful when implementing noisy stochastic gradient descent in deep learning frameworks with static computation graphs, where the variable batch sizes resulting from Poisson subsampling may be problematic. However, note that this guarantee only guarantees privacy for spaces of datasets whose elements are larger than some \(Q\in\mathbb{N}\) with \(Q>q\), where q is the batch size.

For the following proof, we condition on the same events as in our proof of Proposition H.6, but need to construct a different \(d_{\mathbb{Y}}\)-compatible coupling, since we have a different subsampling distribution.

**Theorem I.2**.: _Assume a dataset space and batch space defined by \(\mathbb{X}\subseteq\{x\in\mathcal{P}(\mathbb{A})\mid|x|>Q\}\), \(\mathbb{Y}=\{y\subseteq x\mid x\in\mathbb{X},|y|=q\}\), \(\mathcal{Y}=\mathcal{P}(\mathbb{Y})\) and finite set \(\mathbb{A}\), with \(Q,q\in\mathbb{N}\) and \(Q>q\). Let \(M=B\circ S\) be a subsampled mechanism, where \(S\) is subsampling without replacement with batch size \(q\ r\). Let \(\simeq_{\mathbb{Y}}\) be the insertion/removal relation \(\simeq_{\pm,\mathbb{Y}}\). Then, for \(\alpha>1\) and all \(x\simeq_{\pm,\mathbb{X}}x^{\prime}\),_

\[\Lambda_{\alpha}(m_{x}||m_{x^{\prime}})\leq\max_{N\in\mathbb{N}}\left(2\cdot \sum_{l=0}^{\alpha}\binom{\alpha}{l}w^{l}(1-w)^{\alpha-l}\left(\max_{y,y^{ \prime}}\Lambda_{l}(b_{y}||b_{y^{\prime}})\ \text{s.t.}\ \ d_{\mathbb{Y}}(y,y^{\prime})\leq 2 \right)\right)\]

_subject to \(N>Q\) and with \(w=q\,/\,N\)._

Proof.: Like in our proof of Proposition H.6, we need to distinguish between insertion and removal.

**Case 1: Removal.** In this case, there is some \(a\in x\) such that \(x^{\prime}=x\setminus\{a\}\). We define \(N=|x^{\prime}|\), which fulfills \(N>Q\) by definition of dataset space \(\mathbb{X}\).

We let \(A_{1}\) be the event that \(a\) is not sampled, i.e. \(A_{1}=\{y\in\mathbb{Y}\mid a\notin y\}\), and let \(A_{2}=\overline{A_{1}}\). We let \(E_{1}=\mathbb{Y}\), i.e., do not condition on any particular event.

By definition of subsampling without replacement, we have \(P_{S_{x}}(A_{1})=1-qN\), \(P_{S_{x}}(A_{2})=q\,/\,N\), and \(P_{S_{x^{\prime}}}(E_{1})=1\). We further have

\[s_{x}(y\mid A_{1})=\begin{cases}\binom{N}{q}^{-1}&\text{if }y\subseteq x\wedge a \notin y\\ 0&\text{otherwise}\end{cases},\qquad s_{x}(y\mid A_{2})=\begin{cases}\binom{N} {q-1}^{-1}&\text{if }y\subseteq x\wedge a\in y\\ 0&\text{otherwise}\end{cases},\]

and

\[s_{x^{\prime}}(y\mid E_{1})=\begin{cases}\binom{N}{q}^{-1}&\text{if }y\subseteq x^{ \prime}\\ 0&\text{otherwise}\end{cases}.\]

Note that \(s_{x}(y\mid A_{1})=s_{x^{\prime}}(y\mid E_{1})\) and that all three conditional distribution are subsampling without replacement from sets of size \(N\), not of size \(N+1\) or \(N-1\). Further note that, for event \(A_{1}\), we only need to sample \(q-1\) elements, since one element is always fixed to be \(a\).

**Coupling.** We now define a coupling via \(\gamma:\mathbb{Y}^{2+1}\to\mathbb{R}_{+}\):

\[\gamma(\boldsymbol{y}^{(1)},\boldsymbol{y}^{(2)})=\begin{cases}s_{x}(y_{1}^{ (1)}\mid A_{1})\cdot\frac{1}{q}&\text{if }y_{1}^{(1)}=y_{1}^{(2)}\wedge y_{2}^{(1)}=y_{1}^{(1)} \cup\{a\}\\ 0&\text{otherwise}.\end{cases}\]

Simply put, \(\gamma\) defines the following generative process: We first generate \(y_{1}^{(1)}\) by sampling a batch that does not contain \(a\) via subsampling without replacement from \(x\setminus\{a\}\). We then let \(y_{1}^{(2)}\gets y_{1}^{(1)}\). We then randomly replace one of the \(q\) batch elements with \(a\) to generate \(y_{2}^{(1)}\).

**Validity.** We can verify the validity of this coupling as follows: For every \(y_{1}^{(1)}\) with \(s_{x}(y_{1}^{(1)}\mid A_{1})>0\), there are exactly \(q\) combination \(y_{2}^{(1)},y_{1}^{(2)}\) for which \(\gamma(\boldsymbol{y})>0\). We thus have \(\sum_{y_{2}^{(1)},y_{1}^{(2)}\in\mathbb{Y}^{2}}\gamma(y_{1}^{(1)},y_{1}^{(1)},y_{1}^{(2)})=s_{x}(y_{1}^{(1)}\mid A_{1})\). The proof for \(y_{1}^{(2)}\) is analogous.

For every \(y_{2}^{(1)}\), there are exactly \(N-(q-1)\) combinations of \(y_{1}^{(1)},y_{1}^{(2)}\) for which \(\gamma(\boldsymbol{y})>0\). This is because \(a\) must have replaced one of the \(N-(q-1)\) elements of \(x^{\prime}\) that are not in \(y_{1}^{(2)}\).

We thus have

\[\sum_{y_{1}^{(1)},y_{1}^{(2)}\in\mathbb{Y}^{2}}\gamma(y_{1}^{(1) },y_{2}^{(1)},y_{1}^{(2)})\] \[= (N-(q-1))\cdot\binom{N}{q}^{-1}\cdot\frac{1}{q}=\left(\frac{q}{N -q+1}\cdot\binom{N}{q}\right)^{-1}=\binom{N}{q-1}^{-1}.\]

**Compatibility.** Finally, it can be easily shown that \(\gamma\) is a \(d_{\mathbb{Y}}\)-compatible coupling. Whenever \(\gamma(\boldsymbol{y})>0\), then

\[d_{\mathbb{Y}}(y_{1}^{(1)},y_{2}^{(1)}) =d_{\mathbb{Y}}\left(y_{1}^{(1)},\operatorname{supp}(s_{x}(\cdot \mid A_{2}))\right)=2,\] \[d_{\mathbb{Y}}(y_{1}^{(1)},y_{1}^{(2)}) =d_{\mathbb{Y}}\left(y_{1}^{(1)},\operatorname{supp}(s_{x^{\prime }}(\cdot\mid E_{1}))\right)=0.\]

Similarly, the pairwise distance between \(y_{2}^{(1)}\) and \(y_{1}^{(2)}\) is always \(2\). This is identical to the distance of their supports, because one can always transition between them via a substitution, i.e., an insertion and a removal.

It thus follows from Corollary F.7 that

\[\Lambda_{\alpha}(m_{x}||m_{x^{\prime}})\leq \max_{y_{1}^{(1)},y_{2}^{(1)},y_{1}^{(2)}}\Lambda_{\alpha}((1-q\,/ \,N)\cdot b_{y_{1}^{(1)}}+q\,/\,N\cdot b_{y_{2}^{(1)}}||b_{y_{1}^{(2)}})\] \[\text{s.t.}d_{\pm,\mathbb{Y}}(y_{1}^{(1)},y_{2}^{(1)})\leq 1As in our proof of Proposition H.6 one can use the definition of \(\Lambda_{\alpha}\) and binomial expansion to finally show that

\[\Lambda_{\alpha}(m_{x}||m_{x^{\prime}})\leq\sum_{l=0}^{\alpha}\binom{\alpha}{l}w^ {l}(1-w)^{\alpha-l}\left(\max_{y,y^{\prime}}\Lambda_{l}(b_{y}||b_{y^{\prime}}) \quad\text{s.t.}\quad d_{\bar{\nu}}y,y^{\prime}\leq 2.\right)\]

with \(w=q\:/\:N\). Note that this is smaller than the term in Proposition H.6 by a factor of \(2\).

**Case 2: Insertion** In this case, there is some \(a\in x^{\prime}\) such that \(x=x^{\prime}\setminus\{a\}\). We can thus define the same events and the same coupling in a symmetric manner to show

\[\Lambda_{\alpha}(m_{x}||m_{x^{\prime}})\leq\max_{y,y^{\prime}} (1-w)\int_{\mathbb{Z}}\left(\frac{(1+w)b_{y}(z)-wb_{y^{\prime}}(z)}{b_{y}(z)} \right)^{\alpha}b_{y}(z)\:\mathrm{d}\lambda(z)\] \[+w\int_{\mathbb{Z}}\left(\frac{(1-w)b_{y^{\prime}}(z)+wb_{y}(z)}{ b_{y^{\prime}}(z)}\right)^{\alpha}b_{y^{\prime}}(z)\:\mathrm{d}\lambda(z),\]

subject to \(d_{\bar{\nu}}(y,y^{\prime})\leq 2\). with \(w=q\:/\:N\). Again, this corresponds exactly to Eq. 6 of the "novel alternative decomposition" in Appendix A.1 of [30]. One can then through the remaining steps in their Appendix A to show that this term is at most two times larger than the one we derived in the deletion case.

Finally, since this bound depends on \(N\), and we want the guarantee to hold for all \(x\simeq_{\pm,\mathbb{X}}x^{\prime}\), we need to determine the \(N\) that maximizes the bound. 

**Hybrid Relations.** Note that we could have also gone through the above derivations with batch substitution relation \(\simeq_{\Delta,\mathbb{Y}}\). Then, each substitution would correspond to an actual substitution, instead of a pair of insertion and deletion, and we would obtain

\[\Lambda_{\alpha}(m_{x}||m_{x^{\prime}})\leq\max_{N\in\mathbb{N}}\left(2\cdot \sum_{l=0}^{\alpha}\binom{\alpha}{l}w^{l}(1-w)^{\alpha-l}\max_{y\simeq_{\Delta,\mathbb{Y}}y^{\prime}}\Lambda_{l}(b_{y}||b_{y^{\prime}})\right)\quad\text{s.t.}\quad N>Q,\]

with \(w=q\:/\:N\).

### Tight mechanism-specific subsampling without replacement for randomized response

As mentioned earlier, the following result is meant as a simple example to demonstrate the benefit of using mechanism-specific over mechanism-agnostic RDP bounds in Section 4. For the following discussion, recall Theorem H.1, which we proved in the previous section.

**Theorem H.1**.: _Let \(M=B\circ S\) be a subsampled mechanism, where \(S\) is subsampling without replacement with batch size \(q\). Let \(\simeq_{\mathbb{Y}}\) be the substitution relation \(\simeq_{\Delta,\mathbb{Y}}\). Then, for \(\alpha>1\) and all \(x\simeq_{\Delta,\mathbb{X}}x^{\prime}\) of size \(N\),_

\[\Delta_{\alpha}(m_{x}||m_{x^{\prime}})\leq\max_{\tilde{\bm{y}}}\Delta_{\alpha} ((1-w)\cdot b_{y^{(1)}_{1}}+w\cdot b_{y^{(1)}_{2}}||(1-w)\cdot b_{y^{(2)}_{1} }+w\cdot b_{y^{(2)}_{2}})\]

_subject to \(d_{\Delta,\mathbb{Y}}(y^{(1)}_{1},y^{(1)}_{2})\leq 1\), \(d_{\Delta,\mathbb{Y}}(y^{(1)}_{1},y^{(2)}_{2})\leq 1\), \(d_{\Delta,\mathbb{Y}}(y^{(1)}_{2},y^{(2)}_{2})\leq 1\), \(y^{(1)}_{1}=y^{(2)}_{1}\), and with \(w=q\:/\:N\)._

We shall now solve Theorem H.1 for randomized response and prove the tightness of the resultant guarantee.

**Theorem I.3**.: _Let \(B\) be the randomized response mechanism \(|h-(1-V)|\) with \(h:\mathbb{Y}\rightarrow\{0,1\}\), \(V\sim\mathrm{Bern}(\theta)\), and true response probability \(\theta\in[0,1]\). Let \(S\) be subsampling without replacement with batch size \(q\). Then, for all \(x\simeq_{\Delta,\mathbb{X}}x^{\prime}\) of size \(N\), \(\Lambda_{\alpha}(m_{x}||m_{x^{\prime}})\) is i.e.q._

\[\max_{\tau}\Lambda_{\alpha}((1-w)\mathrm{Bern}(\cdot\mid\theta)+w\mathrm{ Bern}(\cdot\mid\tau)||(1-w)\mathrm{Bern}(\cdot\mid\theta)+w\mathrm{Bern}( \cdot\mid 1-\tau))\]

_subject to \(\tau\in\{\theta,1-\theta\}\) and with \(w=q\:/\:N\)._

Proof.: Because we have no further information, we must consider the worst possible underlying function \(h:\mathbb{Y}\rightarrow\{0,1\}\).

Due to the last constraint in Theorem H.1, we must have \(h(y_{1}^{(1)})=h(y_{1}^{(2)})\). Due to symmetry in \(\Lambda_{\alpha}\) (we are summing over \(z\in\{0,1\}\)), we can assume w.l.o.g. that \(h(y_{1}^{(1)})=h(y_{1}^{(2)})=1\) and thus \(b_{y_{1}^{(1)}}(z)=b_{y_{1}^{(1)}}(z)=\operatorname{Bern}(z\mid\theta)\).

Because the batches \(y_{2}^{(1)}\) and \(y_{2}^{(1)}\) can differ from \(y_{1}^{(1)}\), we can choose the corresponding function values \(h(y_{2}^{(1)})\) and \(h(y_{2}^{(2)})\) arbitrarily. To make \(\Lambda_{\alpha}\) greater than \(1\), the two mixtures must be different from each other. Thus, we must choose either \(h(y_{2}^{(1)})=1\) and \(h(y_{2}^{(2)})=0\), or \(h(y_{2}^{(1)})=0\) and \(h(y_{2}^{(2)})=1\). This corresponds to \(b_{y_{2}^{(1)}}(z)=\operatorname{Bern}(z\mid\tau)\) and \(b_{y_{2}^{(2)}}(z)=\operatorname{Bern}(z\mid 1-\tau)\) with \(\tau\in\{\theta,1-\theta\}\). Maximizing over both options yields our result. 

Note that this guarantee can be evaluated in \(\mathcal{O}(1)\): We need to iterate over two possible values of \(\tau\), and evaluating the corresponding \(\Lambda_{\alpha}\) requires summing over two values \(z\in\{0,1\}\).

Next, we prove the mechanism-specific tightness of this result, meaning it is not possible to derive stronger guarantees without additional information about dataset space \(\mathbb{X}\) and the function \(h\) underlying the randomized response mechanism.

**Theorem I.4**.: _Let \(S\) be subsampling without replacement with arbitrary batch size \(q\in\mathbb{N}\) and \(\theta\in[0,1]\) be some true response probability. There exists a dataset space \(\mathbb{X}\) and a batch space \((\mathbb{Y},\mathcal{Y})\) fulfilling the constraints in Definition D.5, as well a pair of datasets \(x\simeq_{\Delta,\mathbb{X}}\) of size \(N\), and a function \(h:\mathbb{Y}\to\{0,1\}\), such that the corresponding subsampled randomized response mechanism \(M=B\circ S\) fulfills_

\[\Lambda_{\alpha}(m_{x}||m_{x^{\prime}})\] \[= \max_{\tau}\Lambda_{\alpha}((1-w)\operatorname{Bern}(\cdot\mid \theta)+w\operatorname{Bern}(\cdot\mid\tau)||(1-w)\operatorname{Bern}(\cdot \mid\theta)+w\operatorname{Bern}(\cdot\mid 1-\tau))\]

_subject to \(\tau\in\{\theta,1-\theta\}\) and with \(w=q\ /\ N\)._

Proof.: Let \(\mathbb{X}=\{x\subseteq\mathbb{N}\mid|x|>q\}\). Consider an arbitrary \(x\in\mathbb{X}\) and select arbitrary elements \(a\in x\), \(a^{\prime}\in\mathbb{N}\setminus x\). Define \(x^{\prime}=x\setminus\{a\}\cup\{a^{\prime}\}\).

Assume w.l.o.g. that the divergence is maximized by \(\tau=\theta\). We now construct an indicator function \(h:\mathbb{Y}\to\{0,1\}\) for \(a\) and \(a^{\prime}\) that leads to the largest possible divergence.

\[h(y)=\begin{cases}1&\text{if }y\cap\{a,a^{\prime}\}=\emptyset\\ 1&\text{if }a\in y\\ 0&\text{if }a^{\prime}\in y\end{cases}\]

By construction of \(h\), the corresponding base mechanism pmf is

\[b_{y}(z)=\begin{cases}\operatorname{Bern}(z\mid\theta)&\text{if }y\cap\{a,a^{ \prime}\}=\emptyset\\ \operatorname{Bern}(z\mid\theta)&\text{if }a\in y\\ \operatorname{Bern}(z\mid 1-\theta)&\text{if }a^{\prime}\in y\end{cases}\]

Under the distribution of \(S(x)\), the first case occurs with probability \(1-w\) and the second case occurs with probability \(w\). Under the distribution of \(S(x)\), the first case occurs with probability \(1-w\) and the second case occurs with probability \(w\). We thus have

\[m_{x}(z)=(1-w)\operatorname{Bern}(z\mid\theta)+w\operatorname{Bern }(z\mid\theta),\] \[m_{x^{\prime}}(z)=(1-w)\operatorname{Bern}(z\mid\theta)+w \operatorname{Bern}(z\mid 1-\theta),\]

which exactly attains the desired divergence when \(\tau=\theta\) is the optimal value.

From mechanism-specific guarantees to dominating pairs

Our proposed optimal transport approach lets us derive mechanism-specific guarantees, which upper-bound the hockey stick divergence between the distribution of \(M(x)\) and \(M(x^{\prime})\) with \(x\simeq_{\mathbb{X}}x^{\prime}\) via a weighted sum 5 of mixture divergences, i.e.,

Footnote 5: assuming that the batch space is finite and discrete

\[H_{\alpha}(m_{x}||m_{x^{\prime}})\leq\sum_{k=1}^{K}w_{\alpha,k}^{(x,x^{\prime} )}\cdot H_{\alpha}(p_{\alpha,k}^{(x,x^{\prime})}||q_{\alpha,k}^{(x,x^{\prime}) })\] (23)

Here, the \(w_{\alpha,k}^{(x,x^{\prime})}\) are weights with \(\sum_{k=1}^{K}w_{\alpha,k}^{(x,x^{\prime})}=1\), which depend on the chosen coupling \(\gamma\) and are indexed by \(\alpha\geq 0\), \(1\leq k\leq K\), and \(x,x^{\prime}\in\mathbb{X}\). The \(p_{\alpha,k}^{(x,x^{\prime})}\) and \(p_{\alpha,K}^{(x,x^{\prime})}\) are densities of mixture distributions \(P_{\alpha,k}^{(x,x^{\prime})}\) and \(q_{\alpha,K}^{(x,x^{\prime})}\) on the output space \(\mathbb{R}^{D}\), 6 which are also indexed by \(\alpha\geq 0\), \(1\leq k\leq K\), and \(x,x^{\prime}\in\mathbb{X}\).

Footnote 6: the arguments in this section also apply to the general problem setting from Appendix D, where we allow arbitrary output spaces

Our goal is to construct dominating pairs from such bounds, so that we can then use them for privacy accounting. For this discussion, recall the definition of dominating pairs:

**Definition 2.3**.: A pair of distributions \((P,Q)\) with densities \((p,q)\) is a dominating pair for mechanism \(M\) under neighboring relation \(\simeq_{\mathbb{X}}\), if \(H_{\alpha}(m_{x}||m_{x^{\prime}})\leq H_{\alpha}(p||q)\) for all \(x\simeq_{\mathbb{X}}x^{\prime}\) and all \(\alpha\geq 0\).

If the densities on the r.h.s. of Eq. (23) are constant in \(\alpha\), \(k\), \(x\), and \(x^{\prime}\), i.e., \(p_{\alpha,k}^{(x,x^{\prime})}=\bar{p}\) and \(q_{\alpha,k}^{(x,x^{\prime})}=\tilde{q}\) with some densitities \(\tilde{p}\), \(\tilde{q}\), then we can immediately identify that the corresponding distributions \(\tilde{P}\) and \(\tilde{Q}\) are a dominating pair. For instance, in Section 3.4 we could immediately determine that the two Gaussian mixtures in Theorem 3.8 are a dominating pair for Poisson subsampling and the "insert-\(K_{+}\)-remove-\(K_{-}\)" relation.

However, this is generally not the case. In the following, we discuss a three-step procedure that let us (1) reduce the weighted sum of divergences in Eq. (23) to a single divergence (2) resolve non-constancy in the dataset pairs \((x,x^{\prime})\), and (3) resolve non-constancy in the divergence order \(\alpha\).

Note that, depending on the considered setting, it may be possible to skip one or multiple of these steps (like in our group privacy example).

### Step 1: Eliminating weighted sums

Consider some fixed order \(\alpha\) and fixed datasets \(x\simeq_{\mathbb{X}}x^{\prime}\). Let us thus omit the corresponding indexes. In this step, we construct a pair of distributions \(\tilde{P}\), \(\tilde{Q}\) with densitities \(\tilde{p}\) and \(\tilde{q}\) such that

\[\sum_{k=1}^{K}w_{k}\cdot H_{\alpha}(p_{k}||q_{k})=H_{\alpha}(\tilde{p}||\tilde {q}).\]

To achieve this, we notice that there is no requirement for dominating pairs to be distributions on the same space \(\mathbb{R}^{D}\). Taking inspiration from hierarchical randomized smoothing [69], we thus construct \(\tilde{p}\) and \(\tilde{q}\) that are mixtures of the \(p_{k}\) and \(q_{k}\) and additionally release their component indices:

**Lemma J.1**.: _Consider arbitrary densities \(p_{1},\ldots,p_{K},q_{1},\ldots,q_{K}:\mathbb{R}^{D}\rightarrow\mathbb{R}_{+}\) and arbitrary weights \(w_{1},\ldots,w_{K}\in[0,1]\) with \(\sum_{k=1}^{K}w_{k}=1\). Further let \(r:\{1,\ldots,K\}\rightarrow[0,1]\) be the probability mass function of \(\mathrm{Categorical}(w_{1},\ldots,w_{K})\). Define \(\tilde{p},\tilde{q}:\mathbb{R}^{D}\times\{1,\ldots,K\}\rightarrow\mathbb{R}_{+}\) as \(\tilde{p}(z,k)=p_{k}(z)\cdot r(k)\) and \(\tilde{q}(z,k)=q_{k}(z)\cdot r(k)\). Then, \(H_{\alpha}(\tilde{p}||\tilde{q})=\sum_{k=1}^{K}w_{k}\cdot H_{\alpha}(p_{k}||q_ {k})\)._Proof.: By the definition of hockey stick divergence and \(\tilde{p},\tilde{q}\), we have

\[H_{\alpha}(\tilde{p}||\tilde{q})= \sum_{k=1}^{K}\int_{\mathbb{R}^{D}}\max\left\{\frac{\tilde{p}(z,v)} {\tilde{q}(z,v)},0\right\}\cdot\tilde{q}(z,v)\;\mathrm{d}z\] \[= \sum_{k=1}^{K}\int_{\mathbb{R}^{D}}\max\left\{\frac{p_{k}(z)\cdot r (k)}{q_{k}(z)\cdot r(k)},0\right\}\cdot q_{k}(z)\cdot r(k)\;\mathrm{d}z\] \[= \sum_{k=1}^{K}r(k)\cdot\int_{\mathbb{R}^{D}}\max\left\{\frac{p_{k }(z)}{q_{k}(z)},0\right\}\cdot q_{k}(z)\;\mathrm{d}z\] \[= \sum_{k=1}^{K}w_{k}\cdot H_{\alpha}(p_{k}||q_{k}).\]

Note that one could equivalently construct continuous distributions \(\tilde{P}\) and \(\tilde{Q}\) by defining the categorical distribution as a transformation of a uniform distribution. Further note that the distribution of privacy loss random variable \(\log(\tilde{p}(Z)\,/\,\tilde{q})\) with \(Z\sim\tilde{P}\) is simply a weighted sum of the components' privacy loss distributions, which can be easily shown via law of total probability. This makes this construction amenable to numerical privacy accounting.

### Step 2: Resolving non-constancy in dataset pairs

After applying the construction from the previous step, we have bounds of the form \(H_{\alpha}(m_{x}||m_{x^{\prime}})\leq H_{\alpha}(\tilde{p}_{\alpha}^{(x,x^{ \prime})}||\tilde{q}_{\alpha}^{(x,x^{\prime})})\). If these bounds are not constant in the datasets \(x,x^{\prime}\in\mathbb{X}\), we cannot simply read off a dominating pair.

However, the ultimate goal behind identifying dominating pairs is to use them in a privacy accounting method to algorithmically derive guarantees for all possible pair \(x\simeq_{\mathbb{X}}x^{\prime}\) under composition. Providing guarantees for all possible pair does not require that we derive guarantees for all pairs simultaneously. To formalize this, recall that the neighboring relation \(\simeq_{\mathbb{X}}\) is a set of tuples from \(\mathbb{X}^{2}\).

**Lemma J.2**.: _Consider any \(\alpha\geq 0\), mechanism \(M\) and \(L\) different neighboring relations \(\simeq^{(1)},\ldots,\simeq^{(L)}\subseteq\mathbb{X}^{2}\). Assume that there are \(L\) constants \(c^{(1)},\ldots,c^{(L)}\) such that \(H_{\alpha}(m_{x}||m_{x^{\prime}})\leq c^{(l)}\) for all \(l\in\{1,\ldots,L\}\) and all \(x\simeq^{(l)}x^{\prime}\). If the neighboring relations partition \(\simeq_{\mathbb{X}}\), i.e., \(\simeq_{\mathbb{X}}=\bigcup_{l=1}^{L}\simeq^{(l)}\), then_

\[\forall x\simeq_{\mathbb{X}}x^{\prime}:H_{\alpha}(m_{x}||m_{x^{\prime}})\leq \max_{l}c^{(l)}.\]

Proof.: Consider any \(x\simeq_{\mathbb{X}}x^{\prime}\). Because the neighboring relations partition \(\simeq_{\mathbb{X}}\), there must be some \(l^{*}\) such that \(x\simeq^{(l^{*})}x^{\prime}\) and thus \(H_{\alpha}(m_{x}||m_{x^{\prime}})\leq c^{(l^{*})}\leq\max_{l}c^{(l)}\). 

For the purposes of privacy accounting, it is thus sufficient to determine some sufficiently fine-grained partition \(\simeq^{(1)},\ldots,\simeq^{(L)}\) such that for all \(l\in\{1,\ldots,L\}\) and all \(\alpha\geq 0\)

\[\forall x\simeq^{(l)}x^{\prime}:H_{\alpha}(\tilde{p}_{\alpha}^{(x,x^{\prime}) }||\tilde{q}_{\alpha}^{(x,x^{\prime})})=H_{\alpha}(\hat{p}_{\alpha}^{(l)}|| \hat{q}_{\alpha}^{(l)}),\]

where \(\hat{p}_{\alpha}^{l}\) and \(\hat{q}_{\alpha}^{(l)}\) are families of densities indexed by partition index \(l\) and divergence order \(\alpha\). One can then perform privacy accounting independently for each of the relations.

This goal can always be achieved by having one atomic relation per possible pair of neighboring elements. In practice, however, much more coarse-grained partitions may be sufficient. Zhu et al. [10] applied this ansatz to the insertion/removal relation, by deriving dominating pairs for insertion and removal separately. In our work, we partition the group insertion/removal relation for groups of size \(K\) into \(K+1\) different "insert-\(K_{+}\)-remove-\(K_{-}\)" relations with \(K_{+}+K_{-}=K\).

### Step 3: Resolving non-constancy in divergence order

Consider some fixed partition index \(l\in\{1,\dots,L\}\), which we omit in the following. We are now left with a bound of the form \(\forall x\simeq x^{\prime}:H_{\alpha}(m_{x}||m_{x^{\prime}})\leq H_{\alpha}(\hat {p}_{\alpha}||\hat{q}_{\alpha})\). If the families of densitities \(\hat{p}_{\alpha}\), \(\hat{q}_{\alpha}\) is constant in \(\alpha\), i.e.

\[\forall\alpha:H_{\alpha}(\hat{p}_{\alpha}||\hat{q}_{\alpha})=H_{\alpha}(\hat{p }||\hat{q}),\]

with some \(\hat{p},\hat{q}\), then the corresponding distributions \(\hat{P},\hat{Q}\) are dominating pairs. Importantly, the numeric value of the bound can vary with \(\alpha\). We just want the numeric value to be identical to the divergence \(H_{\alpha}\) of two specific distributions for all \(\alpha\).

If this is not the case, there are two options to construct a dominating pair. The first option uses a characterization of dominating pairs as a function of privacy profiles. It was applied by Zhu et al. [10] to determine dominating pairs for subsampling without replacement and the substitution relation. The second option is enabled by our novel optimal transport and constrained optimization perspective on subsampling.

#### j.3.1 Option 1: Convex conjugation of privacy profiles

The following result from [10] establishes a correspondence between privacy profiles and dominating pairs, as well as providing a formula for constructing dominating pairs from privacy profiles:

**Proposition J.3** (Zhu et al. [10]).: _For a given \(f:\mathbb{R}_{+}\rightarrow\mathbb{R}\), there exists \(\hat{P},\hat{Q}\) such that \(\forall\alpha\geq 0:H(\alpha)=H_{\alpha}(P,Q)\) if and only if \(f\in\mathcal{F}\) where_

\[\mathcal{F}=\left\{f:\mathbb{R}_{+}\rightarrow\mathbb{R}\ |\text{ $f$ is convex, decreasing, $f(0)=1$ and $f(x)\geq\max\{1-x,0\}$}\right\}.\] (24)

_Moreoever, one can explicitly construct such \(P\) and \(Q\): \(P\) has \(CDF\)\(1+f^{*}(x-1)\) in \([0,1)\) and \(Q=\operatorname{Uniform}([0,1])\), where \(f^{*}\) is the convex conjugate of \(f\)._

Next, we show how to construct a function \(f(\alpha)\in\mathcal{F}\) from our families of densities \(\hat{q}_{\alpha},\hat{q}_{\alpha}\) indexed by \(\alpha\) that allows us to determine dominating pairs.

**Lemma J.4**.: _Consider two families of densities \(\hat{p}_{\alpha},\hat{q}_{\alpha}\) indexed by \(\alpha\geq 0\) such that \(\forall x\simeq x^{\prime}:H_{\alpha}(m_{x}||m_{x^{\prime}})\leq H_{\alpha}( \hat{p}_{\alpha}||\hat{q}_{\alpha}).\) Define the function \(f:\mathbb{R}_{+}\rightarrow\mathbb{R}\) with_

\[f(\alpha)=\max_{\beta\geq 0}H_{\alpha}(\hat{p}_{\beta}||\hat{q}_{\beta}).\]

_Then \(f\) is a valid privacy profile, i.e., \(f\in\mathcal{F}\) with \(\mathcal{H}\) defined in Eq. (24)._

Proof.: For every \(\beta\geq 0\), let \(f_{\beta}(\alpha)=H_{\alpha}(\hat{p}_{\beta}||\hat{q}_{\beta})\) be the privacy profile associated with a specific pair from the considered families. All these functions are elements of \(\mathcal{F}\) as per the first part of Proposition J.3. We thus know that each \(f_{\beta}\) is convex, decreasing, and has value \(1\) at \(\alpha=0\). Naturally, their maximum is also convex, decreasing, and has value \(1\) at \(\alpha=0\). We further know that, for all \(\beta\geq 0\), \(f_{\beta}(x)\geq\max\{1-x,0\}.\) Thus, we have \(f(x)\geq f_{\beta}(x)\geq(1-x)\) for any \(\beta\geq 0\). 

We can then invoke the second part of Proposition J.3 to define our dominating pair.

As mentioned earlier, Zhu et al. [10] applied this principle to subsampling without replacement by taking a maximum over two different pairs of distributions. Lemma J.4 generalizes this principle to larger families.

#### j.3.2 Option 2: Relaxation of cost function constraints

A novel solution is enabled by our proposed framework for deriving mechanism-specific subsampling guarantees: Recall that the densitities \(\hat{p}_{\alpha}\) and \(\hat{q}_{\alpha}\) are the result of identifying worst-case mixture components under pairwise batch distance constraints (Proposition 3.5). In Appendix L, we demonstrate that it is possible to relax some of these constraints to obtain a single pair of densitities that bounds \(H_{\alpha}(m_{x}||m_{x^{\prime}}\) for all \(\alpha\) and pairs of datasets \(x\simeq x^{\prime}\). Specifically, we apply this principle to the substitution relation and subsampling without replacement, as well as subsampling with replacement. Due to the relaxation, these dominating pairs are not necessarily tight. They may however be easier to use in practice than convex conjugation, which requires solving an optimization problem.

Recovering known dominating pairs

In this section, we demonstrate that known dominating pairs for the standard combinations of Poisson subsampling under insertion/removal and subsampling without replacement under substitution, as well as subsampling without replacement under insertion/removal, can also be derived via our proposed optimal transport framework.

As before, our contribution are not the guarantees themselves. Our contribution is identifying that, just like mechanism-agnostic guarantees for ADP and RDP, these guarantees can be derived by defining an (optimal) coupling and then identifying worst-case mixture components under pairwise batch distance constraints.

### Poisson subsampling and insertion/removal

As discussed in Appendix J, it is beneficial to partition the insertion removal \(\simeq_{\pm,\mathbb{X}}\) into two relations: The insertion relation \(\simeq_{+,\mathbb{X}}\), where \(x\simeq_{+,\mathbb{X}}x^{\prime}\) implies that there is some \(a\notin x\) such that \(x^{\prime}=x\cup\{a\}\) and the removal relation \(\simeq_{-,\mathbb{X}}\), where \(x\simeq_{-,\mathbb{X}}\) implies that there is some \(a\in x\) such that \(x^{\prime}=x\setminus\{a\}\).

While we could derive dominating pairs for both relations from scratch, we can use the following result from [10] to reduce our workload:

**Lemma K.1**.: _If distributions \((P,Q)\) are a dominating pair for mechanism \(M\) under the insertion relation \(\simeq_{+}\), then \((Q,P)\) are a dominating pair under the removal relation \(\simeq_{-}\)._

Next, we use our proposed framework to derive the following guarantee in a very natural manner that does not require reasoning about tradeoff functions and optimal testing rules (c.f. proof of Lemma 29 in [10]).

**Proposition K.2** (Zhu et al. [10]).: _Consider a subsampled mechanism \(M=B\circ S\), where \(S\) is Poisson subsampling with subsampling rate \(r\in[0,1]\). assume that \((P,Q)\) are a dominating pair for base mechanism \(B\) under the batch insertion relation \(\simeq_{+,\mathbb{Y}}\). Then, \((P,(1-r)P+rQ)\) are a dominating pair for \(M\) under the dataset insertion relation \(\simeq_{+,\mathbb{X}}\) and \(((1-r)P+rQ,Q)\) are a dominating pair for \(M\) under the dataset removal relation \(\simeq_{-,\mathbb{X}}\)._

Proof.: We begin by deriving the dominating pair for the removal relation \(\simeq_{-,\mathbb{X}}\). Consider an arbitrary pair \(x\simeq_{-}x^{\prime}\) and an arbitrary \(\alpha\geq 0\). Recall from our proof of the mechanism-agnostic Poisson subsampling guarantee Proposition H.6 that we can construct a \(d_{\mathbb{Y}}\)-compatible coupling to show that

\[H_{\alpha}(m_{x}||m_{x^{\prime}})\leq\max_{\bm{y}}H_{\alpha}((1-r)b_{y_{1}^{( 1)}}+rb_{y_{2}^{(1)}}||b_{y_{1}^{(2)}})\]

subject to \(\bm{y}\in\mathbb{Y}^{2+1}\), \(y_{1}^{(1)}=y_{1}^{(2)}\), \(d_{\mathbb{Y}}(y_{2}^{(1)},y_{1}^{(1)})\leq 1\), \(d_{\mathbb{Y}}(y_{1}^{(1)},y_{2}^{(1)})\leq\infty\).

The last constraint is because there is no sequence of deletions that will result in an insertion. It can be ignored, meaning our optimization problem is equivalent to

\[\max_{y\simeq-y^{\prime}}H_{\alpha}((1-r)b_{y^{\prime}}+rb_{y}||b_{y^{\prime}}).\]

Using the definition of hockey stick divergence,7 this can be restated as

Footnote 7: For simplicity, we assume that we have continuous-valued mechanisms, but the same proof strategy can also be used for the more general definition of hockey stick divergence from Appendix D by calculating \(\operatorname{d}((1-r)P_{B_{y^{\prime}}}+rP_{B_{y}})\,/\,\operatorname{d}P_{B _{y^{\prime}}}\).

\[\max_{y\simeq-y^{\prime}}H_{\alpha}((1-r)b_{y^{\prime}}+rb_{y}||b_ {y^{\prime}})\] \[= \max_{y\simeq-y^{\prime}}\int_{\mathbb{R}^{D}}\max\left(\frac{(1 -r)b_{y^{\prime}}(z)+rb_{y}(z)}{b_{y^{\prime}}(z)}-\alpha,0\right)\cdot b_{y^{ \prime}}(z)\;\mathrm{d}z\] \[= \max_{y\simeq-y^{\prime}}r\cdot\int_{\mathbb{R}^{D}}\max\left\{ \frac{b_{y}(z)}{b_{y^{\prime}}(z)}-\left(\alpha-\frac{1-r}{r}\right),0\right\} \cdot b_{y^{\prime}}(z)\;\mathrm{d}z\]We can now distinguish two cases, based on the value of \(\alpha^{i}=\alpha-(1-r)\,/\,r\).

**Case 1:** Assume that \(\alpha^{\prime}<0\). Then, the integrand is always non-negative, and the objective function evaluates to a constant \(r(1-a^{\prime})\). We can thus choose \(y,y^{\prime}\) arbitrarily, because

\[\max_{y\simeq_{+}y^{\prime}}H_{\alpha}((1-r)b_{y^{\prime}}+rb_{y}||b_{y^{ \prime}})=r(1-a^{\prime})=H_{\alpha}((1-r)q+rp||q),\]

where \(p\) and \(q\) are the densities of dominating pair \((P,Q)\).

**Case 2:** Assume that \(\alpha^{\prime}\geq 0\). Because \((P,Q)\) is a dominating pair for the batch insertion relation \(\simeq_{+,\mathbb{Y}}\), we know from Lemma K.1 that \((Q,P)\) is a dominating pair for the batch removal relation \(\simeq_{-,\mathbb{Y}}\). Thus

\[\max_{y\simeq_{-}y^{\prime}}H_{\alpha}((1-r)b_{y^{\prime}}+rb_{y} ||b_{y^{\prime}})\] \[= \max_{y\simeq_{-}y^{\prime}}r\cdot H_{\alpha^{\prime}}(b_{y}||b_ {y^{\prime}})\] \[\leq r\cdot H_{\alpha^{\prime}}(q||p)\] \[= H_{\alpha}((1-r)p+rq||q)\]

This shows that \(((1-r)P+rQ,Q)\) is a dominating pair for the removal relation \(\simeq_{-,\mathbb{X}}\). It then follows from Lemma K.1 that \((Q,(1-r)P+rQ)\) is a dominating pair for the insertion relation \(\simeq_{+,\mathbb{X}}\). 

Note that case 1 (but not case 2) of Theorem 11 in [10] states that \(((1-r)Q+rP,P)\) were a dominating pair for the removal relation. This does however appear to be a typographical error, since the authors use the same symmetry argument (Lemma K.1) for their proof.

### Subsampling without replacement and insertion/removal

The proof for the following result is virtually identical to the previous one. They only differ in the coupling which we need to construct, which again highlights the generality and usefulness of our proposed framework.

**Proposition K.3** (Zhu et al. [10]).: _Assume a dataset space and batch space defined by \(\mathbb{X}\subseteq\{x\in\mathcal{P}(\mathbb{A})\mid|x|\in\{N,N-1\}\), \(\mathbb{Y}=\{y\subseteq x\mid x\in\mathbb{X},|y|=q\}\), and finite set \(\mathbb{A}\), with \(q<N\). Consider a subsampled mechanism \(M=B\circ S\), where \(S\) is subsampling without replacement with batch size \(q\), and define batch-to-dataset ratio \(w=q\,/\,N\). assume that \((P,Q)\) are a dominating pair for base mechanism \(B\) under the batch substitution relation \(\simeq_{\Delta,\mathbb{Y}}\). Then, \((P,(1-w)P+wQ)\) are a dominating pair for \(M\) under the dataset insertion relation \(\simeq_{+,\mathbb{X}}\) and \(((1-w)P+wQ,Q)\) are a dominating pair for \(M\) under the dataset removal relation \(\simeq_{-,\mathbb{X}}\)._

Proof.: As in the previous proof, we begin with removal relation \(\simeq_{-,\mathbb{X}}\). Consider an arbitrary pair \(x\simeq_{-}x^{\prime}\) and an arbitrary \(\alpha\geq 0\). Recall from our novel proof of the mechanism-agnostic RDP guarantee for subsampling without replacement and insertion/removal (see Theorem I.2) that we can construct a \(d_{\mathbb{Y}}\)-compatible coupling to show that

\[H_{\alpha}(m_{x}||m_{x^{\prime}})\leq\max_{\bm{y}}H_{\alpha}((1-w)b_{y_{1}^{( 1)}}+wb_{y_{2}^{(1)}}||b_{y_{1}^{(2)}})\]

subject to \(\bm{y}\in\mathbb{Y}^{2+1}\), \(y_{1}^{(1)}=y_{1}^{(2)}\), \(d_{\mathbb{Y}}(y_{2}^{(1)},y_{1}^{(1)})\leq 1\), \(d_{\mathbb{Y}}(y_{1}^{(1)},y_{2}^{(1)})\leq 1\).

By definition of the induced distance, this optimization problem is equivalent to

\[\max_{y\simeq_{\Delta}y^{\prime}}H_{\alpha}((1-r)b_{y^{\prime}}+rb_{y}||b_{y^{ \prime}}).\]

We can now go through exactly the same steps as in our derivation of Proposition K.2, replacing every occurrence of "\(\simeq_{-}\)" with "\(\simeq_{\Delta}^{\prime\prime}\), to conclude our proof. 

### Subsampling without replacement and substitution

In the case of subsampling without replacement and substitution (as well as Poisson subsampling and insertion/removal without partitioning of the neighboring relation) we do not need to provide a new proof. This is because Zhu et al. [10] prove this result by invoking a tight mechanism-agnostic subsampling guarantee derived by Balle et al. [15] and then applying advanced joint convexity in reverse order (see proof of Proposition 30 in [10]).

As we discussed in Appendix G.2 and formalized in Theorem G.2, any guarantee that is derived via the framework from [15] can be equivalently derived by defining a (potentially suboptimal) coupling between multiple conditional subsampling distributions. Thus, we know that the guarantee could have equivalently been derived via our proposed framework

**Proposition K.4** (Zhu et al. [10]).: _Consider a subsampled mechanism \(M=B\circ S\), where \(S\) is subsampling without replacement with batch size \(q\in\mathbb{N}\). Assume that \((P,Q)\) are a dominating pair for base mechanism \(B\) under the batch substitution relation \(\simeq_{\Delta,\mathbb{Y}}\). Consider arbitrary \(x\simeq_{\Delta,\mathbb{X}}x^{\prime}\) of size \(N\) and define batch-to-dataset ratio \(w=q\:/\:N\). Then_

\[H_{\alpha}(m_{x}||m_{x^{\prime}})\leq\begin{cases}H_{\alpha}((1-w)q+wp||q)& \text{for }\alpha\geq 1\\ H_{\alpha}(p,(1-w)p+wq||q)&\text{for }0<\alpha<1\end{cases}\]

Note that this result does not immediately specify a dominating pair. However, as shown in [10], one can construct a dominating pair via convex conjugation (recall Appendix J.3.1 and Proposition J.3).

Novel results for dominating pairs

In the following, we formally derive dominating pairs for subsampling with replacement under the substitution relation. We also derive an alternative dominating pair for subsampling without replacement under substitution which does not require convex conjugation (c.f. Appendix K.3). These results are novel in three respects.

**Novel contribution 1 - Formal guarantees.** Using the dominating pairs we shall shortly derive for privacy accounting was already proposed in [8]. However, the authors did not provide a formal proof for why they should lead to valid privacy guarantees. They only proved that if two pairs of multivariate Gaussians with colinear means were to be a dominating pair8, then one could use a change of variable and marginalization to reduce them to a univariate dominating pair (see Appendix B.1 in [8]). They did not prove why this assumption should hold. A formal proof of this assumption is now enabled by the analysis we conducted for solving the group privacy optimization problem in Theorem M.6 (see Appendix O).

Footnote 8: Technically, the notion of dominating pairs[10] had not been introduced at this point, which is why the authors discuss a vague notion of “worst-case distributions”, similar to other early work on numerical accounting.

**Novel contribution 2 - Other base mechanisms.** The generality of our solution to the group privacy optimization problem in Theorem M.6 lets us not only derive dominating pairs for Gaussian mechanisms, but also for Laplace mechanisms and randomized response.

**Novel contribution 3 - Resolving non-constancy via constraint relaxation.** Perhaps most importantly, our proposed framework offers a novel perspective on an issue encountered when analyzing subsampling without replacement: While one can derive a tight mechanism-specific guarantee, this guarantee depends on different mixture distributions for \(\alpha<1\) and \(\alpha\geq 1\) (see Proposition K.4 [10] and our more general discussion in Appendix J.3). Thus, one cannot simply read off a dominating pair from this mechanism-specific guarantee, as we did with our group privacy bounds. However, as we shall demonstrate, this issue no longer appears when we relax some of the batch distance constraints in our proposed cost function bound from Proposition 3.5. This suggests that this could be a more general approach for addressing this non-constancy issue. While the resultant dominating pairs are not necessarily tight, they may be easier to use in practice than the convex conjugation construction from [10].

### Subsampling without replacement and substitution

We begin with subsampling without replacement to demonstrate the potential benefit of relaxing batch distance constraints to determine tractable dominating pairs.

**Theorem L.1**.: _Let \(M=B\circ S\), where \(S\) is subsampling without replacement with batch size \(q\), and \(B\) is the Gaussian mechanism \(h+V\) with \(h:\mathbb{Y}\rightarrow\mathbb{R}^{D}\) and \(V\sim\mathcal{N}(\mathbf{0},\sigma^{2}\bm{I}_{D})\). Define the \(\ell_{2}\)-sensitivity \(L_{2}=\max_{y\simeq_{\Delta},y^{\prime}}||f(y)-f(y^{\prime})||_{2}\), where \(\simeq_{\Delta}\) is the substitution relation. Consider an arbitrary dataset size \(N\in\mathbb{N}\) and define batch-to-dataset ratio \(w=\frac{q}{N}\). Then, for any pair \(x\simeq_{\Delta,\mathbb{X}}x^{\prime}\) of size \(N\),_

\[H_{\alpha}(m_{x}||m_{x^{\prime}})\leq H_{\alpha}\left(f_{1}^{(1)}\cdot(1-w)+f_{ 2}^{(1)}\cdot w||f_{1}^{(2)}\cdot(1-w)+f_{2}^{(2)}\cdot w\right),\] (25)

_with univariate normal densities \(f_{i}^{(1)}=\mathcal{N}(\cdot\mid(i-1),\sigma\:/\:L_{2})\), \(f_{j}^{(2)}=\mathcal{N}(\cdot\mid-(j-1),\sigma\:/\:L_{2})\)._

Proof.: Recall from our proof of the mechanism-agnostic subsampling without replacement guarantee Proposition H.4 that we can construct a \(d_{\mathbb{Y}}\)-compatible coupling to show that

\[H_{\alpha}(m_{x}||m_{x^{\prime}})\leq\max_{\bm{y}}H_{\alpha}((1-w)b_{y_{1}^{( 1)}}+wb_{y_{2}^{(1)}}||(1-w)b_{y_{1}^{(2)}}+wb_{y_{2}^{(2)}})\]

subject to \(\bm{y}\in\mathbb{Y}^{2+1}\), \(y_{1}^{(1)}=y_{1}^{(2)}\), \(d_{\mathbb{Y}}(y_{1}^{(1)},y_{2}^{(1)})\leq 1\), \(d_{\mathbb{Y}}(y_{1}^{(1)},y_{2}^{(2)})\leq 1\), \(d_{\mathbb{Y}}(y_{1}^{(2)},y_{1}^{(2)})\leq 1\).

Next, we relax the constraint between the two components that contain a substituted element: \(d_{\mathbb{Y}}(y_{1}^{(2)},y_{1}^{(2)})\leq 2\).

We now observe that this optimization problem is identical to the optimization problem from our group privacy bound (Theorem 3.7) with one insertion (\(K_{+}=1\)), one deletion \((K_{-}=1)\), and subsampling rate \(r=w\). The result thus follows immediately from our mechanism-specific group privacy amplification bound for Gaussian mechanism (Theorem 3.8).

We can generalize this result to other mechanisms by using group privacy amplification bounds for Laplace mechanisms (Theorem M.2) and randomized response mechanisms (Theorem M.2).

We see that the upper bound in Eq. (25) depends on the same pair of mixture distributions for all \(\alpha\geq 0\). Thus, this pair of mixture distributions is a dominating pair for subsampling without replacement under substitution.

### Subsampling with replacement and substitution

Next, we provide the first formal derivation of dominating pairs for subsampling with replacement.

In subsampling with replacement, a single element may appear in a batch multiple times. To formalize this, we assume a dataset space \(\mathbb{X}\subseteq\mathcal{P}(\mathbb{A})\) that is composed of sets (not multisets) of elements from an underlying set \(\mathbb{A}\). We further assume a batch space \(\mathbb{Y}\subseteq\mathcal{P}_{\mathrm{multi}}(\mathbb{A})\) that is composed of multisets of elements from \(\mathbb{A}\). Given some \(y\in\mathbb{Y}\), we write \(\xi_{y}(a)\) for the number of times that \(a\) appears in \(y\). We further define the support of batch \(y\in\mathbb{Y}\) as \(\mathrm{supp}(y)=\{a\in\mathbb{Y}\mid\xi_{y}(a)\geq 0\}\).

**Definition L.2**.: Subsampling with replacement with batch size \(q\) has probability mass function \(s_{x}(y)=\binom{|x|+q-1}{q}^{-1}\) for batches \(y\in\mathbb{Y}\) with \(\mathrm{supp}(y)\subseteq x\) and \(|y|=q\).

Given this subsampling scheme, we can use optimal transport to derive the following constrained optimization problem, which we shall then relax and solve:

**Theorem L.3**.: _Let \(M=B\circ S\), where \(S\) is subsampling without replacement with batch size \(q\), and \(B\) is the Gaussian mechanism \(h+V\) with \(h:\mathbb{Y}\rightarrow\mathbb{R}^{D}\) and \(V\sim\mathcal{N}(\mathbf{0},\sigma^{2}\bm{I}_{D})\). Define the \(\ell_{2}\)-sensitivity \(L_{2}=\max_{y\simeq\Delta,y^{\prime}}||f(y)-f(y^{\prime})||_{2}\), where \(\simeq_{\Delta}\) is the substitution relation. Consider an arbitrary dataset size \(N\in\mathbb{N}\) and define batch-to-dataset ratio \(w=\frac{q}{N}\). Then, for any pair \(x\simeq_{\Delta,\mathbb{X}}x^{\prime}\) of size \(N\),_

\[H_{\alpha}(m_{x}||m_{x^{\prime}})\leq\max_{\bm{y}}H_{\alpha}\left(\sum_{i=1}^{ q+1}b_{y_{i}^{(1)}}\cdot w_{i}||\sum_{j=1}^{q+1}b_{y_{i}^{(2)}}\cdot w_{j} \right),\]

_subject to \(\bm{y}\in\mathbb{Y}^{(q+1)+(q+1)}\), \(\forall l,t,u:d_{\mathbb{Y}}(y_{t}^{(l)},y_{u}^{(l)})\leq|t-u|\), \(\forall t,u:d_{\mathbb{Y}}(y_{t}^{(1)},y_{u}^{(2)})\leq\max\{t,u\}\), and with weights \(w_{i}=\left(\frac{1}{N}\right)^{i}\cdot\left(1-\frac{1}{N}\right)^{q-i}\)._

Proof.: By definition of the substitution relation \(\simeq_{\Delta}\) there is some \(a\in x\) with \(a\notin x^{\prime}\) and some \(a^{\prime}\in x^{\prime}\) with \(a^{\prime}\notin x\) such that \(x^{\prime}=x\setminus\{a\}\cup\{a^{\prime}\}\).

To simplify indexing, we define zero-based indexed events \(A_{0},\ldots,A_{q}\) and \(E_{0},\ldots,E_{q}\). We define \(A_{i}\) to be the event that \(a\) is sampled \(i\) times, i.e., \(\xi_{y}(a)=i\). We define \(E_{j}\) to be the event that \(a^{\prime}\) is sampled \(j\) times, i.e., \(\xi_{y}(a^{\prime})=j\).

By definition subsampling with replacement, we have \(P_{S_{x}}(A_{i})=w_{i}\), and \(P_{S_{x}}(E_{j})=w_{j}\) with weights \(w_{i}=\left(\frac{1}{N}\right)^{i}\cdot\left(1-\frac{1}{N}\right)^{q-i}\).

For dataset size \(N\), we have

\[s_{x}(y\mid A_{i}) =\begin{cases}\binom{(N-1)+(q-i)-1}{q-i}^{-1}&\text{if }\mathrm{supp}(y)\subseteq x\wedge\xi_{y}(a)=i\\ 0&\text{otherwise}\end{cases}\] \[=\begin{cases}(\frac{1}{N-1})^{q-i}(q-i)!\prod_{\bar{a}\neq a^{ \prime}}\frac{1}{\xi_{y}(\bar{a})!}&\text{if }\mathrm{supp}(y)\subseteq x\wedge\xi_{y}(a)=i\\ 0&\text{otherwise}\end{cases}\]

and

\[s_{x^{\prime}}(y\mid E_{j}) =\begin{cases}\binom{(N-1)+(q-i)-1}{q-i}^{-1}&\text{if }\mathrm{supp}(y)\subseteq x^{\prime}\wedge\xi_{y}(a^{\prime})=j\\ 0&\text{otherwise}\end{cases}\] \[=\begin{cases}(\frac{1}{N-1})^{q-i}(q-i)!\prod_{\bar{a}\neq a^{ \prime}}\frac{1}{\xi_{y}(\bar{a})!}&\text{if }\mathrm{supp}(y)\subseteq x^{\prime}\wedge\xi_{y}(a^{\prime})=j\\ 0&\text{otherwise}\end{cases}\]

Note that \(s_{x}(y\mid A_{0})=s_{x^{\prime}}(y\mid E_{0})\). Further note that \(s_{x}(y\mid A_{q})=1[\mathrm{supp}(y)=\{a\}\wedge|y|=q]\) and \(s_{x^{\prime}}(y\mid E_{q})=1[\mathrm{supp}(y)=\{a^{\prime}\}\wedge|y|=q]\).

**Coupling.** We now define a coupling \(\gamma:\,\mathbb{Y}^{(q+1)+(q+1)}\to\mathbb{R}_{+}\) that corresponds to the following generative process: We first generate \(y_{q}^{(1)}\) by constructing a batch of size \(q\) whose elements are all \(a\). Beginning at \(i\gets q\), we then iteratively generate \(y_{i-1}^{(1)}\) from \(y_{i-1}^{(1)}\) by replacing one instance of \(a\) with an element \(\tilde{a}\in x\setminus a\) that is sampled uniformly at random. Finally, we construct the \(y_{j}^{(2)}\) by replacing every occurence of \(a\) in \(y_{j}^{(1)}\) with \(a^{\prime}\). More formally,

\[\gamma(\boldsymbol{y}^{(1)},\boldsymbol{y}^{(2)})= \begin{cases}\left(\frac{1}{N-1}\right)^{q}&\text{if }\boldsymbol{y}^{(1)}, \boldsymbol{y}^{(2)}\text{ fulfills Condition L.4}\\ 0&\text{otherwise.}\end{cases}\]

**Condition L.4**.: A tuple \(\boldsymbol{y}^{(1)}\in\mathbb{Y}^{q+1}\), \(\boldsymbol{y}^{(q+1)}\in\mathbb{Y}^{K_{+}+1}\) fulfills this condition when \(\xi_{y_{q}^{(1)}}(a)=\xi_{y_{q}^{(2)}}(a^{\prime})=q\) and \(\forall i,\exists\tilde{\alpha}\in x\setminus\{a\}:y_{i-1}^{(1)}=y_{i-1}^{(1 )}\setminus\{a\}\cup\{\tilde{a}\}\) and \(\forall i,\forall\tilde{a}\notin\{a,a^{\prime}\}):\xi_{y_{q}^{(1)}}(\tilde{a} )=\xi_{y_{q}^{(2)}}(\tilde{a})\) and \(\xi_{y_{q}^{(1)}}(a)=\xi_{y_{q}^{(2)}}(a^{\prime})\).

**Validity.** To verify the validity of this coupling, consider an arbitrary \(i\) and an arbitrary \(\boldsymbol{y}\) such that \(\gamma(\boldsymbol{y}^{(1)},\boldsymbol{y}^{(2)})>0\). We know that there are \((q-i)!\prod_{\tilde{a}\neq a}\frac{1}{\xi_{y}(\tilde{a})!}\) sequences of \(y_{q}^{(1)},y_{q-1}^{(1)},\ldots,y_{i}^{(1)}\) that could have generated \(y_{i}^{(1)}\). We further know that there are \(\left(\frac{1}{N-1}\right)^{i}\) possible sequences of \(y_{i-1}^{(1)},\ldots,y_{0}^{(1)}\) that can be generated from \(y_{i}^{(1)}\). All \(y_{j}^{(2)}\) are deterministically determined by \(y_{j}^{(1)}\). We thus have

\[\sum_{\boldsymbol{y}\in\mathbb{Y}^{(q+1)+(q+1)}}\mathds{1}[y_{i}^ {(1)}=y]\gamma(\boldsymbol{y}^{(1)},\boldsymbol{y}^{(2)})\] \[= (q-i)!\prod_{\tilde{a}\neq a}\frac{1}{\xi_{y}(\tilde{a})!}\left( \frac{1}{N-1}\right)^{i}\left(\frac{1}{N-1}\right)^{q}\] \[= \left(\frac{1}{N-1}\right)^{q-i}(q-i)!\prod_{\tilde{a}\neq a} \frac{1}{\xi_{y}(\tilde{a})!}\] \[= s_{x}(y\mid A_{i}).\]

The proof for the \(\boldsymbol{y}_{j}^{(2)}\) is analogous.

**Compatibility.** Finally, it can be easily shown that \(\gamma\) is a \(d_{\mathbb{Y}}\)-compatible coupling (recall Appendix F). Whenever \(\gamma(\boldsymbol{y})>0\), then

\[\forall 1\leq i\leq K_{-}:d_{\mathbb{Y}}(y_{0}^{(1)},y_{i}^{(1)})=d_ {\mathbb{Y}}\left(y_{0}^{(1)},\mathrm{supp}(s_{x}(\cdot\mid A_{i}))\right)=i,\] \[\forall 0\leq j\leq K_{+}:d_{\mathbb{Y}}(y_{0}^{(1)},y_{i}^{(2)})=d_ {\mathbb{Y}}\left(y_{0}^{(1)},\mathrm{supp}(s_{x^{\prime}}(\cdot\mid E_{j})) \right)=j\]

because we generate the \(y_{i}^{(1)}\) from \(y_{0}^{(1)}\) by substituting exactly \(i\) elements, and construct the \(y_{j}^{(2)}\) by substituting exactly \(j\) elements. Similarly, we have for the pairwise distances that do not involve \(y_{0}^{(1)}\)

\[\forall 0<t<u\leq K_{-}:d_{\mathbb{Y}}(y_{t}^{(1)},y_{u}^{(1)})=d_{ \mathbb{Y}}\left(\mathrm{supp}(s_{x}(\cdot\mid A_{t}),\mathrm{supp}(s_{x}( \cdot\mid A_{u}))\right)=|t-u|,\] \[\forall 0<t<u\leq K_{-}:d_{\mathbb{Y}}(y_{t}^{(2)},y_{u}^{(2)})=d_{ \mathbb{Y}}\left(\mathrm{supp}(s_{x^{\prime}}(\cdot\mid E_{t}),\mathrm{supp}(s_ {x^{\prime}}(\cdot\mid E_{u}))\right)=|t-u|.\] \[\forall 0\leq t<u\leq K_{-}:d_{\mathbb{Y}}(y_{t}^{(1)},y_{u}^{(2)})=d_ {\mathbb{Y}}\left(\mathrm{supp}(s_{x}(\cdot\mid A_{t}),\mathrm{supp}(s_{x^{ \prime}}(\cdot\mid E_{u}))\right)=\max\{t,u\}.\]

The last equality holds for \(t\leq u\) because to construct \(y_{u}^{(2)}\) from \(y_{t}^{(1)}\) we need to substitute all occurrences of \(a\) with \(a^{\prime}\) and then substitute \(u-t\) additional elements with \(a^{\prime}\) It also holds for \(t>u\), because then we need to replace \(u\) occurencence of \(a\) with \(a^{\prime}\) and then replace another \(t-u\) occurencence with values other than \(a^{\prime}\).

Applying Corollary F.7, with \(P_{S_{x}}(A_{i})=w_{i}\), \(P_{S_{x^{\prime}}}(E_{j})=w_{j}\), shows that 

Finally, we can relax and solve Theorem L.3 for specific mechanisms, such as the Gaussian mechanism:

**Theorem L.5**.: _Let \(M=B\circ S\), where \(S\) is subsampling with replacement with batch size \(q\), and \(B\) is the Gaussian mechanism \(h+V\) with \(h:\mathbb{Y}\to\mathbb{R}^{D}\) and \(V\sim\mathcal{N}(\mathbf{0},\sigma^{2}I_{D})\). Define the \(\ell_{2}\)-sensitivity \(L_{2}=\max_{y\succeq\Delta,\mathbb{Y}^{\prime}}\|f(y)-f(y^{\prime})\|_{2}\), where \(\simeq_{\Delta}\) is the substitution relation. Consider an arbitrary dataset size \(N\in\mathbb{N}\). Then, for any pair \(x\simeq_{\Delta,\mathbb{X}}x^{\prime}\) of size \(N\),_

\[H_{\alpha}(m_{x}\|m_{x^{\prime}})\leq H_{\alpha}\left(\sum_{i=1}^{q}f_{i}^{(1) }\cdot w_{i}\|\sum_{j=1}^{q}f_{j}^{(2)}\cdot w_{j}\right),\] (26)

_with univariate normal densities \(f_{i}^{(1)}=\mathcal{N}(\cdot\mid(i-1),\sigma\mathbin{/}L_{2})\), \(f_{j}^{(2)}=\mathcal{N}(\cdot\mid-(j-1),\sigma\mathbin{/}L_{2})\), and weights \(w_{i}=\left(\frac{1}{N}\right)^{q}\cdot\left(1-\frac{1}{N}\right)^{q-i}\)._

Proof.: We first relax the optimization problem in Theorem L.3 by replacing the last constraint \(\forall t,u:d_{\mathbb{Y}}(y_{t}^{(1)},y_{u}^{(2)})\leq\max\{t,u\}\) with a new constraint \(\forall t,u:d_{\mathbb{Y}}(y_{t}^{(1)},y_{u}^{(2)})\leq t+u\).

This leaves us with

\[\max_{\boldsymbol{y}}H_{\alpha}\left(\sum_{i=1}^{q+1}b_{y_{i}^{(1)}}\cdot w_{i }\|\sum_{j=1}^{q+1}b_{y_{i}^{(2)}}\cdot w_{j}\right),\]

subject to \(\boldsymbol{y}\in\mathbb{Y}^{(q+1)+(q+1)}\), \(\forall l,t,u:d_{\mathbb{Y}}(y_{t}^{(l)},y_{u}^{(l)})\leq|t-u|\), \(\forall t,u:d_{\mathbb{Y}}(y_{t}^{(1)},y_{u}^{(2)})\leq t+u\), and with weights \(w_{i}=\left(\frac{1}{N}\right)^{i}\cdot\left(1-\frac{1}{N}\right)^{q-i}\).

We now observe that this optimization problem is identical to the optimization problem from our group privacy bound (Theorem 3.7) with \(q\) insertions (\(K_{+}=q\)), \(q\) deletions \((K_{-}=q)\), and a different set of weights. The result thus follows immediately from our solution to the group privacy amplification problem from Theorem 3.7, which did not make any specific assumptions about the weights. 

We see that the upper bound in Eq. (25) depends on the same pair of mixture distributions for all \(\alpha\geq 0\). Thus, this pair of mixture distributions is a dominating pair for subsampling without replacement under substitution.

Tight mechanism-specific group privacy amplification

In the following, we first prove our generic group privacy amplification guarantee for Poisson subsampling and insertion/removal, which gives us a constrained optimization problem whose optimal value bounds \(\Psi_{\alpha}(m_{x}||m_{x^{\prime}}\). We then solve this constrained optimization problem for Gaussian, Laplace, and randomized response mechanisms. After that, we prove the mechanism-specific tightness for the resultant guarantees. Finally, we discuss how to evaluate these guarantees numerically.

### Proof of Theorem 3.7

**Theorem 3.7**.: _Let \(M=B\circ S\), where \(S\) is Poisson subsampling with rate \(r\). Let \(\simeq_{\gamma}\) be the insertion/removal batch relation \(\simeq_{\pm,\mathbb{Y}}\). Then, for all \(x\simeq_{K_{+},K_{-},\mathbb{X}}x^{\prime}\), \(\Psi_{\alpha}(m_{x}||m_{x^{\prime}})\) is l.e.q._

\[\max_{\bm{y}}\Psi_{\alpha}\left(\sum_{i=1}^{K_{-}+1}b_{y_{i}^{(1)}}\cdot \mathrm{Binom}(i-1\mid K_{-},r)||\sum_{j=1}^{K_{+}+1}b_{y_{j}^{(2)}}\cdot \mathrm{Binom}(j-1\mid K_{+},r)\right),\] (4)

_subject to constraints \(\bm{y}\in\mathbb{Y}^{K_{-}+K_{+}+2}\), as well as \(\forall l\in\{1,2\},\forall t,u:d_{\mathbb{Y}}(y_{t}^{(l)},y_{u}^{(l)})\leq|t-u|\), and \(\forall t,u:d_{\mathbb{Y}}(y_{t}^{(1)},y_{u}^{(2)})\leq(t-1)+(u-1)\)._

Proof.: By definition of the "insert \(K_{+}\), remove \(K_{-}\)" relation \(\simeq_{K_{+},K_{-},\mathbb{X}}\) there is some inserted set \(g_{+}\subseteq x^{\prime}\) of size \(K_{+}\) with \(g_{+}\cap x=\emptyset\) and some removed set \(g_{-}\subseteq x\) of size \(K_{-}\) with \(g_{-}\cap x^{\prime}=\emptyset\) such that \(x^{\prime}=x\setminus g_{-}\cup g_{+}\).

To simplify indexing, we define zero-based indexed events \(A_{0},\ldots,A_{K_{-}}\) and \(E_{0},\ldots,E_{K_{+}}\). We define \(A_{i}\) to be the event that \(i\) removed elements are sampled, i.e. \(|y\cap g_{-}|=i\). We define \(E_{j}\) to be the event that \(j\) inserted elements are sampled, i.e. \(|y\cap g_{+}|=j\).

By definition of Poisson subsampling, we have \(P_{S_{x}}(A_{i})=\mathrm{Binom}(i\mid K_{-},r)\), and \(P_{S_{x}}(E_{j})=\mathrm{Binom}(j\mid K_{+},r)\). We further have

\[s_{x}(y\mid A_{i}) =\begin{cases}r^{|y|}(1-r)^{|x|-|y|}\cdot\mathrm{Binom}(i\mid K_{ -},r)^{-1}&\text{if }y\subseteq x\wedge|g_{-}\cap y|=i\\ 0&\text{otherwise}\end{cases}\] \[=\begin{cases}r^{|y|-i}(1-r)^{|x|-|y|-(K_{-}-i)}\cdot\binom{K_{-}}{i }^{-1}&\text{if }y\subseteq x\wedge|g_{-}\cap y|=i\\ 0&\text{otherwise}\end{cases}\]

and

\[s_{x^{\prime}}(y\mid E_{j}) =\begin{cases}r^{|y|}(1-r)^{|x|-|y|}\cdot\mathrm{Binom}(i\mid K_ {+},r)^{-1}&\text{if }y\subseteq x^{\prime}\wedge|g_{+}\cap y|=i\\ 0&\text{otherwise}\end{cases}\] \[=\begin{cases}r^{|y|-j}(1-r)^{|x|-|y|-(K_{+}-j)}\cdot\binom{K_{+}}{j }^{-1}&\text{if }y\subseteq x^{\prime}\wedge|g_{+}\cap y|=i\\ 0&\text{otherwise}\end{cases}\]

Note that \(s_{x^{\prime}}(y\mid E_{0})=s_{x}(y\mid A_{0})\).

**Coupling.** We now define a coupling \(\gamma:\mathbb{Y}^{(K_{+}+1)+(K_{-}+1)}\to\mathbb{R}_{+}\) that corresponds to the following generative process: We first generate \(y_{0}^{(1)}\) by sampling a batch that does not contain any inserted or removed elements via Poisson subsampling from \(x\setminus g_{-}\). We then let \(y_{0}^{(2)}\gets y_{0}^{(0)}\). Finally, we sample uniformly at random an order in which we include removed elements from \(g_{-}\) and inserted elements from \(g_{+}\) to iteratively construct batches from \((A_{i})_{i>1}\) and \((E_{j})_{j>1}\), respectively. More formally,

\[\gamma(\bm{y}^{(1)},\bm{y}^{(2)})=\begin{cases}s_{x}(y_{0}^{(1)}\mid A_{0}) \cdot(K_{+}!\cdot K_{-}!)^{-1}&\text{if }\bm{y}^{(1)},\bm{y}^{(2)}\text{ fulfills Condition M.1}\\ 0&\text{otherwise}.\end{cases}\]

**Condition M.1**.: A tuple \(\bm{y}^{(1)}\in\mathbb{Y}^{K_{-}+1}\), \(\bm{y}^{(2)}\in\mathbb{Y}^{K_{+}+1}\) fulfills this condition when \(y_{0}^{(1)}=y_{0}^{(2)}\), and \(\forall i:\exists a_{-}\in g_{-}\setminus y_{i}^{(1)}:y_{i+1}^{(1)}=y_{i}^{(1)} \cup\{a_{-}\}\), and \(\forall j:\exists a_{+}\in g_{+}\setminus y_{j}^{(2)}:y_{j+1}^{(2)}=y_{j}^{(2)} \cup\{a_{-}\}\).

**Validity.** We can verify the validity of this coupling as follows:

For \(A_{0}\) and every \(y\) with \(s_{x}(y\mid A_{0})>0\), there are exactly \(K_{+}!\cdot K_{-}!\) combinations with \(y_{0}^{(1)}=y\) for which \(\gamma(\bm{y})>0\). We thus have \(\sum_{\bm{y}\in\mathbb{Y}^{K_{+}+K_{-}+2}}\mathbbm{1}[y_{0}^{(1)}=y]\gamma(\bm {v})=s_{x}(y\mid A_{0}).\) The proof for \(E_{0}\) is analogous.

Next, consider an arbitrary \(A_{i}\) with \(0<i\leq K_{-}\). For every \(y\) with \(s_{x}(y\mid A_{0})>0\), there are exactly \(i!\cdot(K_{-}-i)!\cdot K_{+}!\) combinations with \(y_{i}^{(1)}=y\) for which \(\gamma(\bm{y})>0\), because there are (a) \(i!\) orders in which the removed elements leading up to \(i\) could have been sampled (b) \((K_{-}-i!)\) permutations for the remaining removed elements (c) \(K_{+}!\) permutations for the inserted elements that are only relevant for \(\bm{y}^{(2)}\). We thus have

\[\sum_{\bm{y}\in\mathbb{Y}^{K_{+}+K_{-}+2}}\mathbbm{1}[y_{i}^{(1)}= y]\gamma(\bm{y})\] \[= (i!\cdot(K_{-}-i)!\cdot K_{+}!)\cdot s_{x}(y_{0}^{(1)}\mid A_{0} )\cdot(K_{+}!\cdot K_{-}!)^{-1}\] \[= (i!\cdot(K_{-}-i)!\cdot K_{+}!)\cdot r^{|y_{0}^{(1)}|}(1-r)^{|x| -|y_{0}^{(1)}|-(K_{-})}\cdot(K_{+}!\cdot K_{-}!)^{-1}\] \[= i!(K_{-}-i)!\cdot r^{|y_{0}^{(1)}|}(1-r)^{|x|-|y_{0}^{(1)}|-(K_{ -})}\cdot(K_{-}!)^{-1}\] \[= r^{|y_{0}^{(1)}|}(1-r)^{|x|-|y_{0}^{(1)}|-(K_{-})}\cdot\binom{K_ {-}}{i}^{-1}\] \[= r^{|y|-i}(1-r)^{|x|-|y|-(K_{-}-i)}\cdot\binom{K_{-}}{i}^{-1}\]

For the last step, we have used that \(y_{0}^{(1)}\) contains \(0\) of the removed elements from \(g_{-}\), whereas \(y\) contains \(i\) of the removed elements.

The proof for \(E_{j}\) with \(0<j\leq K_{+}\) is analogous.

**Compatibility.** Finally, it can be easily shown that \(\gamma\) is a \(d_{\mathbb{Y}}\)-compatible coupling (recall Appendix F). Whenever \(\gamma(\bm{y})>0\), then

\[\forall 1\leq i\leq K_{-}:d_{\mathbb{Y}}(y_{0}^{(1)},y_{i}^{(1)})=d_ {\mathbb{Y}}\left(y_{0}^{(1)},\mathrm{supp}(s_{x}(\cdot\mid A_{i}))\right)=i,\] \[\forall 0\leq j\leq K_{+}:d_{\mathbb{Y}}(y_{0}^{(1)},y_{i}^{(2)})=d_ {\mathbb{Y}}\left(y_{0}^{(1)},\mathrm{supp}(s_{x^{\prime}}(\cdot\mid E_{j})) \right)=j\]

because we generate the \(y_{i}^{(1)}\) from \(y_{0}^{(1)}\) by inserting exactly \(i\) elements, and construct the \(y_{j}^{(2)}\) by inserting exactly \(j\) elements. Similarly, we have for the pairwise distances that do not involve \(y_{0}^{(1)}\)

\[\forall 0<t<u\leq K_{-}:d_{\mathbb{Y}}(y_{t}^{(1)},y_{u}^{(1)})=d_ {\mathbb{Y}}\left(\mathrm{supp}(s_{x}(\cdot\mid A_{t}),\mathrm{supp}(s_{x}( \cdot\mid A_{u}))\right)=|t-u|,\] \[\forall 0<t<u\leq K_{-}:d_{\mathbb{Y}}(y_{t}^{(1)},y_{u}^{(2)})=d_ {\mathbb{Y}}\left(\mathrm{supp}(s_{x^{\prime}}(\cdot\mid E_{t}),\mathrm{supp}( s_{x^{\prime}}(\cdot\mid E_{u}))\right)=|t-u|.\] \[\forall 0\leq t<u\leq K_{-}:d_{\mathbb{Y}}(y_{t}^{(1)},y_{u}^{(2)})=d_ {\mathbb{Y}}\left(\mathrm{supp}(s_{x}(\cdot\mid A_{t}),\mathrm{supp}(s_{x^{ \prime}}(\cdot\mid E_{u}))\right)=t+u.\]

The last equality holds because to construct \(y_{u}^{(2)}\) from \(y_{t}^{(1)}\) we need to remove \(t\) elements and insert \(u\) elements.

The result then follows from Corollary F.7, \(P_{S_{x}}(A_{i})=\mathrm{Binom}(i\mid K_{-},r)\), \(P_{S_{x^{\prime}}}(E_{j})=\mathrm{Binom}(j\mid K_{+},r)\), and reverting back to one-based indexing. 

### Instantiations

Next, we can solve the derived optimization problem. In this section, we only provide proof sketches in which we reduce the optimization problem over batches to an optimization problem over the means of multiple Gaussian, Laplace, or Bernoulli random variables. Because solving these problems requires some further exposition, we then refer the reader to Appendix O.

#### d.2.1 Gaussian mechanism guarantee

**Theorem 3.8**.: _Let \(M=B\circ S\), where \(S\) is Poisson subsampling with rate \(r\), and \(B\) is the Gaussian mechanism \(h+V\) with \(h:\mathbb{Y}\to\mathbb{R}^{D}\) and \(V\sim\mathcal{N}(\bm{0},\sigma^{2}\bm{I}_{D})\). Define the \(\ell_{2}\)-sensitivity\(\max_{y\simeq_{\pm,\gamma^{\prime}}}||f(y)-f(y^{\prime})||_{2}\). Then for all \(x\simeq_{K_{+},K_{-},\overline{\chi}}x^{\prime}\), \(\Psi_{\alpha}(m_{x}||m_{x^{\prime}})\) is l.e.q._

\[\Psi_{\alpha}\left(\sum_{i=1}^{K_{-}+1}f_{i}^{(1)}\cdot\mathrm{Binom}(i-1\mid K _{-},r)||\sum_{j=1}^{K_{+}+1}f_{j}^{(2)}\cdot\mathrm{Binom}(j-1\mid K_{+},r) \right),\]

_with univariate normal densities \(f_{i}^{(1)}=\mathcal{N}(\cdot\mid(i-1),\sigma\,/\,L_{2})\), \(f_{j}^{(2)}=\mathcal{N}(\cdot\mid-(j-1),\sigma\,/\,L_{2})\)._

Proof sketch.: Recall from Theorem 3.7 that we need to solve the optimization problem

\[\max_{\bm{y}}\Psi_{\alpha}\left(\sum_{i=1}^{K_{-}+1}w_{i}^{(1)}\cdot b_{y_{i} ^{(1)}}||\sum_{j=1}^{K_{+}+1}w_{j}^{(2)}\cdot b_{y_{j}^{(2)}}\right),\]

subject to \(\bm{y}\in\mathbb{Y}^{(K_{+}+1)+(K_{-}+1)},\forall l,t,u:d_{\mathbb{Y}}(y_{t}^ {(l)},y_{u}^{(l)})\leq|t-u|\), \(\forall t,u:d_{\mathbb{Y}}(y_{t}^{(1)},y_{u}^{(2)})\leq(t-1)+(u-1)\), and with \(w_{i}^{(1)}=\mathrm{Binom}(i-1\mid K_{-},r)\), \(w_{j}^{(2)}=\mathrm{Binom}(j-1\mid K_{+},r)\).

Let \(\bm{\mu}_{i}^{(1)}=h(y_{i}^{(1)})\) and \(\bm{\mu}_{j}^{(2)}=h(y_{j}^{(2)})\). Since we do not have any additional information about \(h\) beyond its \(\ell_{2}\)-sensitivity, we have to make the worst-case assumption that the \(\bm{\mu}_{i}^{(1)},\bm{\mu}_{j}^{(2)}\) are arbitrary vectors constrained by \(\forall l,t,u:||\bm{\mu}_{t}^{(l)}-\bm{\mu}_{u}^{(l)}||_{2}\leq L_{2}|t-u|\), \(\forall t,u:||\bm{\mu}_{t}^{(1)}-\bm{\mu}_{u}^{(2)}||_{2}\leq L_{2}\left((t-1 )+(u-1)\right)\),

Thus, the optimization problem is equivalent to

\[\max_{\bm{\mu}^{(1)},\bm{\mu}^{(2)}}\left(\Psi_{\alpha}(\sum_{i=1}^{K_{-}+1}w _{i}^{(1)}\mathcal{N}(\cdot\mid\bm{\mu}_{i}^{(1)},\sigma^{2}\bm{I}_{D}\,/\,L_ {2}^{2})\|\sum_{j=1}^{K_{+}+1}w_{j}^{(2)}\mathcal{N}(\cdot\mid\bm{\mu}_{j}^{(2 )},\sigma^{2}\bm{I}_{D}\,/\,L_{2}^{2})\right)\]

subject to \(\forall l,t,u:||\bm{\mu}_{t}^{(l)}-\bm{\mu}_{u}^{(l)}||_{2}\leq|t-u|\), \(\forall t,u:||\bm{\mu}_{t}^{(1)}-\bm{\mu}_{u}^{(2)}||_{2}\leq(t-1)+(u-1)\).

In Appendix O, we rigorously prove that the maximum is attained by co-linear, equidistant means that fulfill these distance constraints exactly. Thus, we can perform a coordinate transformation such that \(\bm{\mu}_{1}=\bm{0}\) and \(\bm{\mu}_{i}^{(1)}=(i-1)e_{1}\) and \(\bm{\mu}_{j}^{(2)}=-(j-1)e_{1}\) with first-component indicator vector \(\bm{e}_{1}\in\mathbb{R}^{D}\). Since the likelihood ratio in \(\Psi_{\alpha}\) is Gaussian with zero mean in all but the first dimension, we can marginalize all other dimensions out to obtain our one-dimensional result (cf. Appendix O). 

#### a.2.2 Laplace mechanism guarantee

**Theorem M.2**.: _Let \(M=B\circ S\), where \(S\) is Poisson subsampling with rate \(r\), and \(B\) is the Laplacian mechanism \(h+V\) with \(h:\mathbb{Y}\to\mathbb{R}^{D}\) and \(V\sim\mathrm{Lap}(\bm{0},\sigma^{2}\bm{I}_{D})\), with location \(\bm{0}\in\mathbb{R}^{D}\) and diagonal scale matrix \(\lambda\bm{I}_{D}\in\mathbb{R}_{+}^{D\times D}\), which adds independent Laplacian noise to each dimension. Define the \(\ell_{1}\)-sensitivity \(L_{1}=\max_{y\simeq_{\pm,\gamma^{\prime}}y^{\prime}}||f(y)-f(y^{\prime})||_{1}\). Then, for all \(x\simeq_{K_{+},K_{-},\overline{\chi}}x^{\prime}\), \(\Psi_{\alpha}(m_{x}||m_{x^{\prime}})\) is l.e.q._

\[\Psi_{\alpha}\left(\sum_{i=1}^{K_{-}+1}f_{i}^{(1)}\cdot\mathrm{Binom}(i-1\mid K _{-},r)||\sum_{j=1}^{K_{+}+1}f_{j}^{(2)}\cdot\mathrm{Binom}(j-1\mid K_{+},r) \right),\]

_with univariate Laplace densities \(f_{i}^{(1)}=\mathrm{Lap}(\cdot\mid(i-1),\lambda\,/\,L_{1})\), \(f_{j}^{(2)}=\mathrm{Lap}(\cdot\mid-(j-1),\lambda\,/\,L_{1})\)._

Proof sketch.: Recall from Theorem 3.7 that we need to solve the optimization problem

\[\Psi_{\alpha}\left(\sum_{i=1}^{K_{-}+1}b_{y_{i}^{(1)}}\cdot w_{i}^{(1)}||\sum_ {j=1}^{K_{+}+1}b_{y_{j}^{(2)}}\cdot w_{j}^{(2)}\right),\]

subject to \(\bm{y}\in\mathbb{Y}^{K_{+}+K_{-}}\), \(\forall l,t,u:d_{\mathbb{Y}}(y_{t}^{(l)},y_{u}^{(l)})\leq|t-u|\), \(\forall t,u:d_{\mathbb{Y}}(y_{t}^{(1)},y_{u}^{(2)})\leq(t-1)+(u-1)\), and with \(w_{i}^{(1)}=\mathrm{Binom}(i-1\mid K_{-},r)\), \(w_{j}^{(2)}=\mathrm{Binom}(j-1\mid K_{+},r)\).

Let \(\bm{\mu}_{i}^{(1)}=h(y_{i}^{(1)})\) and \(\bm{\mu}_{j}^{(2)}=h(y_{j}^{(2)})\). Since we do not have any additional information about \(h\) beyond its \(\ell_{1}\)-sensitivity, we have to make the worst-case assumption that the \(\bm{\mu}_{i}^{(1)},\bm{\mu}_{j}^{(2)}\) are arbitrary vectors constrained by \(\forall l,t,u:||\bm{\mu}_{t}^{(l)}-\bm{\mu}_{u}^{(l)}||_{1}\leq L_{1}|t-u|\), \(\forall t,u:||\bm{\mu}_{t}^{(1)}-\bm{\mu}_{u}^{(2)}||_{1}\leq L_{1}\left((t-1 )+(u-1)\right)\),

Thus, the optimization problem is equivalent to.

\[\max_{\bm{\mu}^{(1)},\bm{\mu}^{(2)}}\left(\Psi_{\alpha}(\sum_{i=1}^{K_{-}+1} \mathrm{Lap}(\cdot\mid\bm{\mu}_{i}^{(1)},\lambda\bm{I}_{D}\mathbin{/}L_{1}) \cdot w_{i}^{(1)}\|\sum_{j=1}^{K_{+}+1}\mathrm{Lap}(\cdot\mid\bm{\mu}_{j}^{(2) },\lambda\bm{I}_{D}\mathbin{/}L_{1})\cdot w_{j}^{(2)}\right)\]

subject to \(\forall l,t,u:||\bm{\mu}_{t}^{(l)}-\bm{\mu}_{u}^{(l)}||_{1}\leq|t-u|\), \(\forall t,u:||\bm{\mu}_{t}^{(1)}-\bm{\mu}_{u}^{(2)}||_{1}\leq(t-1)+(u-1)\).

In Appendix O, we rigorously show that the maximum is attained by collinear, equidistant vectors along a single coordinate axis that leave no slack on the pairwise distance constraints. This then allows us to marginalize out all remaining dimensions to obtain our guarantee in terms of univariate Laplace densitities (see Appendix O). 

#### a.2.3 Randomized response mechanism guarantee

**Theorem M.3**.: _Let \(B\) be the randomized response mechanism \(|h-(1-V)|\) with \(h:\mathbb{Y}\to\{0,1\}\), \(V\sim\mathrm{Bernoulli}(\theta)\), and true response probability \(\theta\in[0,1]\). Let \(M=B\circ S\) be the corresponding subsampled mechanism, where \(S\) is Poisson subsampling with rate \(r\). Finally, let \(\simeq_{\mathbb{Y}}\) be the insertion/removal relation \(\simeq_{\pm,\mathbb{Y}}\). Then, for all \(x\simeq_{K_{+},K_{-},\mathbb{X}}x^{\prime}\), \(\Psi_{\alpha}(m_{x}||m_{x^{\prime}})\) is l.e.q._

\[\max_{\tau}\Psi_{\alpha}((1-w^{(1)})\mathrm{Bern}(\cdot\mid\theta)+w^{(1)} \mathrm{Bern}(\cdot\mid\tau)||(1-w^{(2)})\mathrm{Bern}(\cdot\mid\theta)+w^{( 2)}\mathrm{Bern}(\cdot\mid 1-\tau))\]

_subject to \(\tau\in\{\theta,1-\theta\}\), and with \(w^{(1)}=1-(1-r)^{K_{-}}\) and \(w^{(2)}=1-(1-r)^{K_{+}}\)._

Proof sketch.: Recall from Theorem 3.7 that we need to solve the optimization problem

\[\Psi_{\alpha}\left(\sum_{i=1}^{K_{-}+1}b_{y_{i}^{(1)}}\cdot w_{i}^{(1)}||\sum _{j=1}^{K_{+}+1}b_{y_{j}^{(2)}}\cdot w_{j}^{(2)}\right),\]

subject to \(\bm{y}\in\mathbb{Y}^{K_{+}+K_{-}}\), \(\forall l,t,u:d_{\mathbb{Y}}(y_{t}^{(l)},y_{u}^{(l)})\leq|t-u|\), \(\forall t,u:d_{\mathbb{Y}}(y_{t}^{(1)},y_{u}^{(2)})\leq(t-1)+(u-1)\), and with \(w_{i}^{(1)}=\mathrm{Binom}(i-1\mid K_{-},r)\), \(w_{j}^{(2)}=\mathrm{Binom}(j-1\mid K_{+},r)\).

Let \(\mu_{i}^{(1)}=h(y_{i}^{(1)})\) and \(\mu_{j}^{(2)}=h(y_{j}^{(2)})\). Since we do not have any additional information about \(h\) beyond it mapping to \(\{0,1\}\), we have to make the worst-case assumption that the \(\mu_{i}^{(1)},\mu_{j}^{(2)}\) are only constrained by \(\mu_{2}^{(1)}=\mu_{1}^{(2)}\).

Thus, the optimization problem is equivalent to.

\[\max_{\bm{\mu}^{(1)},\bm{\mu}^{(2)}}\left(\Psi_{\alpha}(\sum_{i=1}^{K_{-}+1} \mathrm{Bern}(\cdot\mid\mu_{i}^{(1)})\cdot w_{i}^{(1)}\|\sum_{j=1}^{K_{+}+1} \mathrm{Bern}(\cdot\mid\mu_{j}^{(2)})\cdot w_{j}^{(2)}\right)\]

subject to \(\mu_{1}^{(1)}=\mu_{1}^{(2)}\).

In Appendix O, we rigorously show that the maximum is attained whenever all \(\mu_{i}^{(1)}\) with \(i>1\) are simultaneously set to either \(0\) or \(1\), and all \(\mu_{j}^{(2)}\) with \(j>1\) are set to the opposite value. 

Note that this guarantee can be evaluated in \(\mathcal{O}(1)\): We need to iterate over two possible values of \(\tau\), and evaluating the corresponding \(\Psi_{\alpha}\) requires summing over two values \(z\in\{0,1\}\).

### Tightness

Next, we prove tightness by constructing datasets and underlying functions \(h\) such that the corresponding base mechanism \(B\) exactly attains the different bounds under subsampling scheme \(S\).

#### d.3.1 Gaussian mechanism tightness

**Theorem M.4**.: _Let \(S\) be Poisson subsampling with rate \(r\in[0,1]\) and \(\theta\in[0,1]\) be some true response probability. There exists a dataset space \(\mathbb{X}\) and a batch space \((\mathbb{Y},\mathcal{Y})\) fulfilling the constraints in Definition D.4, as well a pair of datasets \(x\simeq_{K_{+},K_{-},\mathbb{X}}x^{\prime}\), and a function \(h:\mathbb{Y}\to\mathbb{R}^{D}\) with \(\ell_{2}\)-sensitivity \(L_{2}\), such that the corresponding subsampled Gaussian mechanism \(M=B\circ S\) fulfills_

\[\Psi_{\alpha}(m_{x}||m_{x^{\prime}})=\Psi_{\alpha}\left(\sum_{i=1}^{K_{-}+1}f_ {i}^{(1)}\cdot\mathrm{Binom}(i-1\mid K_{-},r)||\sum_{j=1}^{K_{+}+1}f_{j}^{(2)} \cdot\mathrm{Binom}(j-1\mid K_{+},r)\right),\]

_with univariate normal densities \(f_{i}^{(1)}=\mathcal{N}(\cdot\mid(i-1),\sigma\;/\;L_{2})\), \(f_{j}^{(2)}=\mathcal{N}(\cdot\mid-(j-1),\sigma\;/\;L_{2})\)._

Proof.: Let \(\mathbb{X}=\mathcal{P}(\mathbb{N})\). Consider an arbitrary \(x\in\mathbb{X}\) and select arbitrary deleted elements \(g\subseteq x\), with \(|g|=K_{-}\) and arbitrary inserted elements \(g\subseteq\mathbb{X}\setminus x\). Define \(x^{\prime}=x\setminus g_{-}\cup g_{+}\).

We now construct a counting function for \(h\) that leads to the largest possible divergence under the sensitivity constraint. We define function \(h\) as follows:

\[h(y)=\begin{cases}(i-1)\cdot\bm{e}_{1}L_{2}&\text{if }|g_{-}\cap y|=i-1\\ -(j-1)\cdot\bm{e}_{1}L_{2}&\text{if }|g_{+}\cap y|=j-1\end{cases}\]

with first-component indicator vector \(\bm{e}_{1}\in\mathbb{R}^{D}\).

By construction, \(P_{M_{x}}\) is a mixture distributions with \(K+1\) components, each corresponding to a size of a subset of \(g_{-}\) that is included in batch \(y\). These cases each have probability \(\mathrm{Binom}(i-1\mid K_{-},r)\) under subsampling distributions \(P_{S_{x}}\). Each component has distribution \(\mathcal{N}(\cdot\mid(i-1),\bm{e}_{1}L_{2},\sigma^{2}\bm{I}_{D})\). Analogously, \(P_{m_{x^{\prime}}}\) is the other desired mixture of Gaussians.

As in our previous proof, we can now notice that the likelihood ratio in \(\Psi_{\alpha}\) is constant in all but the first dimension. We can thus marginalize out the remaining dimensions to obtain our result. 

#### d.3.2 Laplace mechanism tightness

**Theorem M.5**.: _Let \(S\) be Poisson subsampling with rate \(r\in[0,1]\) and \(\theta\in[0,1]\) be some true response probability. There exists a dataset space \(\mathbb{X}\) and a batch space \((\mathbb{Y},\mathcal{Y})\) fulfilling the constraints in Definition D.4, as well a pair of datasets \(x\simeq_{K_{+},K_{-},\mathbb{X}}x^{\prime}\), and a function \(h:\mathbb{Y}\to\mathbb{R}^{D}\) with \(\ell_{1}\)-sensitivity \(L_{1}\), such that the corresponding subsampled Laplace mechanism \(M=B\circ S\) fulfills_

\[\Psi_{\alpha}(m_{x}||m_{x^{\prime}})=\Psi_{\alpha}\left(\sum_{i=1}^{K_{-}+1}f_ {i}^{(1)}\cdot\mathrm{Binom}(i-1\mid K_{-},r)||\sum_{j=1}^{K_{+}+1}f_{j}^{(2)} \cdot\mathrm{Binom}(j-1\mid K_{+},r)\right),\]

_with univariate Laplace densities \(f_{i}^{(1)}=\mathrm{Lap}(\cdot\mid(i-1),\lambda\;/\;L_{1})\), \(f_{j}^{(2)}=\mathrm{Lap}(\cdot\mid-(j-1),\lambda\;/\;L_{1})\)._

Proof.: The proof is identical to that of the tightness guarantee from Theorem M.4. We can construct exactly the same counting function that indicates the number of inserted or removed element that appear in a batch sampled from \(S_{x}\) and \(S_{x^{\prime}}\), respectively.

As in our previous proof, we can now notice that the likelihood ratio in \(\Psi_{\alpha}\) is constant in all but the first dimension. We can thus marginalize out the remaining dimensions to obtain our result. 

#### d.3.3 Randomized response mechanism tightness

**Theorem M.6**.: _Let \(S\) be Poisson subsampling with rate \(r\in[0,1]\) and \(\theta\in[0,1]\) be some true response probability. There exists a dataset space \(\mathbb{X}\) and a batch space \((\mathbb{Y},\mathcal{Y})\) fulfilling the constraints in Definition D.4, as well a pair of datasets \(x\simeq_{K_{+},K_{-},\mathbb{X}}x^{\prime}\), and a function \(h:\mathbb{Y}\to 0,1\), such that the corresponding subsampled randomized response mechanism \(M=B\circ S\) fulfills_

\[\Psi_{\alpha}(m_{x}||m_{x}^{\prime})\] \[= \max_{\tau}\Psi_{\alpha}((1-w^{(1)})\mathrm{Bern}(\cdot\mid\theta) +w^{(1)}\mathrm{Bern}(\cdot\mid\tau)||(1-w^{(2)})\mathrm{Bern}(\cdot\mid \theta)+w^{(2)}\mathrm{Bern}(\cdot\mid 1-\tau))\]

_subject to \(\tau\in\{\theta,1-\theta\}\), and with \(w^{(1)}=1-(1-r)^{K_{-}}\) and \(w^{(2)}=1-(1-r)^{K_{+}}\)._Proof.: The following proof is largely identical to our randomized response tightness proof for subsampling without replacement from Appendix I.3.

Let \(\mathbb{X}=\mathcal{P}(\mathbb{N})\). Consider an arbitrary \(x\in\mathbb{X}\) and select arbitrary deleted elements \(g\subseteq x\), with \(|g|=K_{-}\) and arbitrary inserted elements \(g\subseteq\mathbb{X}\setminus x\). Define \(x^{\prime}=x\setminus g_{-}\cup g_{+}\).

Assume w.l.o.g. that the divergence is maximized by \(\tau=\theta\).

We now construct an indicator function \(h:\mathbb{Y}\to\{0,1\}\) for \(a\) and \(a^{\prime}\) that leads to the largest possible divergence.

\[h(y)=\begin{cases}1&\text{if }y\cap(g_{-}\cup g_{+})=\emptyset\\ 1&\text{if }y\cap g_{-}\neq\emptyset\\ 0&\text{otherwise.}\end{cases}\]

By construction of \(h\), the corresponding base mechanism pmf is

\[b_{y}(z)=\begin{cases}\mathrm{Bern}(z\mid\theta)&\text{if }y\cap(g_{-}\cup g_{+})= \emptyset\\ \mathrm{Bern}(z\mid\theta)&\text{if }y\cap g_{-}\neq\emptyset\\ \mathrm{Bern}(z\mid 1-\theta)&\text{otherwise.}\end{cases}\]

Under the distribution of \(S(x)\), the first case occurs with probability \((1-r)^{K_{-}}\) and the second case occurs with probability \(1-(1-r)^{K_{-}}\). Under the distribution of \(S(x^{\prime})\), the first case occurs with probability \((1-r)^{K_{+}}\) and the second case occurs with probability \(1-(1-r)^{K_{+}}\). We thus have

\[m_{x}(z)=(1-w^{(1)})\mathrm{Bern}(z\mid\theta)+w^{(1)}\mathrm{ Bern}(z\mid\theta),\] \[m_{x^{\prime}}(z)=(1-w^{(2)})\mathrm{Bern}(z\mid\theta)+w^{(2)} \mathrm{Bern}(z\mid 1-\theta),\]

which exactly attains the desired divergence when \(\tau=\theta\) is the optimal value. 

### Numerical evaluation

Evidently, we do not have a closed-form analytical expression for our tight mechanism-specific group privacy bounds. However, we can evaluate them to arbitrary precision using standard techniques from privacy accounting literature.

#### d.4.1 Gaussian mechanism

**ADP.** To evaluate the guarantee from Theorem 3.8, we can use Lemma \(5\) from [10], which generalizes an alternative characterization of privacy profiles from [74] to dominating pairs. Let \(P,Q\) be the dominating pair of two univariate Gaussian mixtures. Define privacy loss random variables \(L_{P,Q}=\frac{p(Z)}{q(Z)}\) with \(Z\sim P\) and \(L_{Q,P}=\frac{q(Z)}{p(Z)}\) with \(Z\sim Q\). Then

\[H_{\alpha}(p||q)\leq\Pr[L_{P,Q}>\log(\alpha)]-\alpha\Pr[L_{Q,P}<-\log(\alpha)].\]

Because the privacy loss between the two Gaussian mixtures is monotonically increasing in \(z\)[8], one can perform a change of variables via a binary search for a \(z^{*}\) such that \(\log(\frac{p(z^{*})}{q(z^{*})})\approx\log(\alpha)\). By picking one of the two search boundaries, one can either over- or under-approximate the hockey stick divergence (see, e.g., [8, 24]).

**RDP via quadrature.** To evaluate the guarantee for RDP, we can simply use numerical quadrature. This can be done efficiently because we only need to integrate over univariate Gaussians. This approach was already proposed and used in Abadi et al. [6]'s work on moments accounting.

**RDP via expansion.** For the special case of \(K_{-}=K\) and \(K_{+}=0\), one can also use multinomial expansion (similar to prior work on RDP subsampling from [28, 29, 30]): We have

\[\left(\sum_{k=0}^{K}w_{k}\cdot\mathcal{N}(z\mid\mu_{k},\sigma^ {2}\bm{I})\right)^{\alpha}\] \[= \sum_{l_{0}+\cdots+l_{K}=\alpha}\binom{\alpha}{l_{0},\ldots,l_{K} }\left(\prod_{k=0}^{K}w_{k}^{l_{k}}\right)\left(\prod_{k=0}^{K}\mathcal{N}(z \mid\mu_{k},\sigma^{2}\bm{I})^{l_{k}}\right)\] \[= \sum_{l_{0}+\cdots+l_{K}=\alpha}\binom{\alpha}{l_{0},\ldots,l_{K} }\left(\prod_{k=0}^{K}w_{k}^{l_{k}}\right)\left(\prod_{k=0}^{K}\mathcal{N}(z \mid\mu_{k},\sigma^{2}\bm{I})^{l_{k}/\alpha}\right)^{\alpha}\]Using quadratic expansion, we have

\[\prod_{k=0}^{K}\mathcal{N}(z\mid\mu_{k},\sigma^{2}\bm{I})^{l_{k}/\alpha}\] \[= \mathcal{N}\left(z\mid\sum_{k}\frac{l_{k}}{\alpha}\mu_{k},\sigma^{ 2}\bm{I}\right)\cdot\exp\left(-\frac{1}{2\sigma^{2}}\sum_{k}\frac{l_{k}}{ \alpha}||\mu_{k}||_{2}^{2}\right)\cdot\exp\left(\frac{1}{2\sigma^{2}}\sum_{k}|| (\frac{l_{k}}{\alpha}\mu_{k})||_{2}^{2}\right)\]

Since only the first factor depends on \(z\), our problem reduces to computing the divergence

\[\Psi_{\alpha}\left(\mathcal{N}\left(z\mid\sum_{k}\frac{l_{k}}{\alpha}\mu_{k}, \sigma^{2}\bm{I}\right)||\mathcal{N}\left(z\mid\bm{0},\sigma^{2}\bm{I}\right)\right)\]

for different \(l_{k}\). This can be done in closed form, as shown in [7].

#### d.4.2 Laplace mechanism

**ADP.** Because the privacy loss is constant on \((-\infty,-K_{+})\), monotonically increasing on \([-K_{+},K_{-}]\) and constant on \((K_{-},\infty)\), we can again use the same bisection method.

**RDP.** As with the Gaussian mechanism, we can evaluate the bound via univariate numerical quadrature. Because the privacy loss is non-smooth at the component means \(\{-K_{+},-K_{+}+1,\ldots,0,1,\ldots,K_{-}\}\), we partition \(\mathbb{R}\) at these means and integrate over each interval separately.

#### d.4.3 Randomized response mechanism

The guarantee for randomized smoothing can be evaluated exactly in \(\mathcal{O}(1)\). We just need to iterate over the two options \(\tau\in\{0,1\}\), and for each one evaluate the divergence on space \(\{0,1\}\), which only requires evaluating two fractions and two sums.

Tight mechanism-agnostic group privacy amplification

In this section, we apply the framework from [15], which we summarized in Appendix G.1, to the group privacy setting. We then show the tightness of the resultant guarantees, using the same proof strategy as in Section 5 of [15]. We then demonstrate that it is directly related to our tight mechanism-specific guarantee through joint convexity. Finally, we derive a qualitatively similar guarantee for RDP.

For this discussion, we focus on the special case where all of the \(K\) group members collaboratively agree to insert their data \((K_{+}=K,K_{-}=0)\) or delete their data \((K_{+}=0,K_{-}=K)\), so that the partition induced by the maximal coupling remains interpretable. We define the corresponding neighboring relation as

\[\simeq_{K\pm,\mathbb{X}}=\{(x,x^{\prime})\in\mathbb{X}^{2}\mid x\subset x^{ \prime}\wedge|x^{\prime}|=|x|+K\}\cup\{(x,x^{\prime})\in\mathbb{X}^{2}\mid x \supset x^{\prime}\wedge|x^{\prime}|=|x|-K\}.\] (27)

In our experiments, we only evaluate the baseline for \((K_{+}=K,K_{-}=0)\) and \((K_{+}=0,K_{-}=K)\), while we evaluate our method for all \(K_{+}+K_{-}=K\) and take the maximum. This evaluation favors the baseline.

### ADP guarantee

**Proposition N.1**.: _Let \(M=B\circ S\), where \(S\) is Poisson subsampling with rate \(r\). Let \(\simeq_{\mathbb{Y}}\) be the insertion/removal batch relation \(\simeq_{\pm,\mathbb{Y}}\). Then, for all \(x\simeq_{K\pm,\mathbb{X}}x^{\prime}\) and all \(\varepsilon\geq 0\)_

\[H_{\exp(\varepsilon^{\prime})}(m_{x}||m_{x^{\prime}})\leq\sum_{k=1}^{K} \operatorname{Binom}(k\mid K,r)\cdot\delta_{k}\]

_with \(\varepsilon^{\prime}=\log(1+(1-(1-r)^{K})\cdot(e^{\varepsilon}-1))\) and group privacy parameters_

\[\delta_{k}=\max_{y,y^{\prime}}H_{\exp(\varepsilon)}(b_{y}||b_{y^{\prime}}) \quad\text{s.t.}\quad d_{\mathbb{Y}}(y,y^{\prime})\leq k.\]

Proof.: **Case 1: Deletion.** We first consider the case where \(K\) elements are deleted, i.e., there is some deleted set \(g\subseteq x\) of size \(K\) with \(g_{-}\cap x^{\prime}=\emptyset\) such that \(x^{\prime}=x\setminus g\).

The partition induced by the maximal coupling 9 is

Footnote 9: as can be seen by the fact that \(w\) is the total variation distance of \(s_{x}\) and \(s_{x^{\prime}}\), and that \(s_{x}(y\mid A_{2})\) and \(s_{x}(y\mid A_{1})\) have disjoint support

\[s_{x}(y)=(1-w)s_{x}(y\mid A_{2})+ws_{x}(y\mid A_{1})\] \[s_{x^{\prime}}(y)=(1-w)s_{x}(y\mid A_{2})+ws_{x}(y\mid A_{2}),\]

where \(A_{2}=\{y\in\mathbb{Y}\mid y\cap g=\emptyset\) is the event that no element of \(g\) is sampled, \(A_{1}\) is its complement, and \(w=(1-(1-r)^{K})=P_{S_{x}}(A_{1})\). We use these indices for \(A_{2}\) and \(A_{1}\) because advanced joint convexity will later reverse their order.

Applying advanced joint convexity (see Proposition G.1) and joint convexity shows that

\[H_{\exp(\varepsilon^{\prime})}(m_{x}||m_{x^{\prime}})\leq w\cdot H_{\exp( \varepsilon)}\left(\sum_{y\in\mathbb{Y}}b_{y}\cdot s_{x}(y\mid A_{1})||\sum_{ y\in\mathbb{Y}}b_{y}\cdot s_{x}(y\mid A_{2})\right).\] (28)

Next, we can bound this mixture divergence by constructing a coupling invoking Theorem 3.3 with the special case of \(\Psi_{\alpha}=H_{\exp(\varepsilon)}\). For this, notice that

\[s_{x}(y\mid A_{1})=\begin{cases}\frac{1}{w}\cdot r^{|y|}\cdot(1-r)^{|x|-|y|}& \text{if }y\in A_{1}\wedge y\subseteq x\\ 0&\text{otherwise}.\end{cases}\]

and

\[s_{x}(y\mid A_{2})=s_{x^{\prime}}(y)=\begin{cases}r^{|y|}\cdot(1-r)^{|x^{ \prime}|-|y|}&\text{if }y\in A_{2}\wedge y\subseteq x^{\prime}\\ 0&\text{otherwise}.\end{cases}\]

**Coupling.** We define a coupling \(\gamma:\mathbb{Y}^{2}\rightarrow[0,1]\) that corresponds to the following generative process: We first sample \(y^{(2)}\) from \(s_{x}(y\mid A_{2})\). We then sample from a truncated binomial distribution how many elements \(k\) from group \(g\) should be included. Given this \(k\), we sample uniformly at random a \(\tilde{g}\subseteq g\) from all size-\(k\) subsets of \(g\) and let \(y^{(1)}\gets y^{(2)}\cup\tilde{g}\). Formally this is defined by

\[\gamma(y^{(1)},y^{(2)})=\begin{cases}s_{x}(y^{(2)}\mid A_{2})\cdot\frac{1}{w} \cdot\mathrm{Binom}(|y^{(1)}\cap g|\mid K,r)\cdot\binom{K}{|y^{(1)}\cap g|}^{- 1}&\text{under Condition N.2}\\ 0&\text{otherwise.}\end{cases}\]

**Condition N.2**.: A tuple \(y^{(1)}\in\mathbb{Y}\), \(y^{(2)}\in\mathbb{Y}\) fulfills this condition when there exists a \(\tilde{g}\subseteq g\) with \(|\tilde{g}|\geq 1\) such that \(y^{(1)}=y^{(2)}\cup\tilde{g}\).

**Validity.** Next, we verify the validity of this coupling. For every \(y^{(1)}\), there is exactly one \(y^{(2)}\) such that \(\gamma(y^{(1)},y^{(2)})>0\), namely \(y^{(2)}=y^{(1)}\setminus g^{\prime}\). Thus,

\[\sum_{y^{(2)}}\gamma(y,y^{(2)})= s_{x}(y\setminus g\mid A_{2})\cdot\frac{1}{w}\cdot\mathrm{Binom }(|y\cap g|\mid K,r)\cdot\binom{K}{y\cap g|}^{-1}\] \[= r^{|y\setminus g|}\cdot(1-r)^{|x^{\prime}|-|y\setminus g|}\cdot \frac{1}{w}\cdot r^{|y\cap g|}\cdot(1-r)^{K-|y\cap g|}\] \[= s_{x}(y\mid A_{1}).\]

For every \(y^{(2)}\), there are exactly \(\binom{K}{k}\) batches \(y^{(1)}\) such that \(\gamma(y^{(1)},y^{(2)})>0\) and \(|y^{(1)}\cap g=k|\). Thus,

\[\sum_{y^{(1)}}\gamma(y^{(1)},y)= \sum_{k=1}^{K}\binom{K}{k}s_{x}(y\mid A_{2})\cdot\frac{1}{w}\cdot \mathrm{Binom}(k\mid K,r)\cdot\binom{K}{k}^{-1}\] \[= s_{x}(y\mid A_{2})\cdot\frac{1}{w}\cdot\sum_{k=1}^{K}\mathrm{ Binom}(k\mid K,r)\] \[= s_{x}(y\mid A_{2}).\]

Now that we have proven the validity of the coupling, we can apply the optimal transport bound (Theorem 3.3) to Eq. (28) in order to prove

\[H_{\mathrm{exp}(\varepsilon^{\prime})}(m_{x}||m_{x^{\prime}})\leq w\cdot\sum_{y^{(1)},y^{(2)}}\gamma(y^{(1)},y^{(2)})\delta_{d_{ \varepsilon^{\prime}}(y^{(1)},y^{(2)})}\] \[= w\cdot\sum_{k=1}^{K}\binom{K}{k}\frac{1}{w}\mathrm{Binom}(k\mid K,r)\binom{K}{k}^{-1}\cdot\delta_{k}\] \[= \sum_{k=1}^{K}\mathrm{Binom}(k\mid K,r)\cdot\delta_{k}.\]

**Case 2: Insertion.** In this case, the partition induced by the maximal coupling is

\[s_{x}(y)=(1-w)s_{x}(y\mid A_{1})+ws_{x}(y\mid A_{2})\] \[s_{x^{\prime}}(y)=(1-w)s_{x}(y\mid A_{1})+ws_{x}(y\mid A_{1}),\]

which is identical to the previous partition, up to symmetry. The proof is identical, except for changes in indexing.

Taking the maximum over both guarantees yields the result. 

### Tightness of ADP guarantee

**Proposition N.3**.: _Let \(S\) be Poisson subsampling with rate \(r\). Let \(\simeq_{\mathbb{Y}}\) be the insertion/removal batch relation \(\simeq_{\mathbb{Z},\mathbb{Y}}\). Then, for all \(x\simeq_{K\pm,\mathbb{X}}x^{\prime}\) and all \(\varepsilon\geq 0\), there exists a worst-case base mechanism_such that the corresponding subsampled mechanism \(M=B\circ S\) fulfills_

\[H_{\exp(\varepsilon^{\prime})}(m_{x}||m_{x^{\prime}})=\sum_{k=1}^{K}\mathrm{ Binom}(k\mid K,r)\cdot\delta_{k}\] (29)

_with \(\varepsilon^{\prime}=\log(1+(1-(1-r)^{K})\cdot(e^{\varepsilon}-1))\) and group privacy parameters_

\[\delta_{k}=\max_{y,y^{\prime}}H_{\exp(\varepsilon)}(b_{y}||b_{y^{\prime}}) \quad\text{s.t.}\quad d_{\mathbb{Y}}(y,y^{\prime})\leq k.\]

Proof.: To show tightness of the bound, we notice that the bound \(\sum_{k=1}^{K}\mathrm{Binom}(k\mid K,r)\cdot\delta_{k}\) is identical to the bound for subsampling with replacement from [15], except for the numeric value of the weights in the weighted sum. We can thus use exactly the same proof strategy.

Assume w.l.o.g. that we are in the insertion case, i.e., there is some \(g\) of size \(K\) with \(g\cap x=\emptyset\) and \(x^{\prime}=x\cup g\). Consider an arbitrary \(\varepsilon\) and define an arbitrary true response probability \(\theta\in[0,1]\). Further define the randomized membership base mechanism

\[B(y)=|1[|y\cap g|>0]-(1-V)|\]

with \(V\sim\mathrm{Bern}(\theta)\). It is easy to verify [15] that \(\delta_{k}=\psi_{\theta}(\varepsilon)=\max\{\theta-e^{\varepsilon}(1-p),0\}\). We thus have for the r.h.s. of Eq. (29)

\[\sum_{k=1}^{K}\mathrm{Binom}(k\mid K,r)\cdot\delta_{k}=\sum_{k=1}^{K}\mathrm{ Binom}(k\mid K,r)\cdot\psi_{\theta}(\varepsilon):=w\cdot\psi_{\theta}( \varepsilon).\]

Because Poisson subsampling is a "natural subsampling"[15] scheme that only yields elements from the dataset it is applied to, it follows from Lemma 12 of [15] that also

\[H_{\exp(\varepsilon^{\prime})}(m_{x}||m_{x^{\prime}})=w\cdot\psi_{\theta}( \varepsilon).\]

### Relation to tight mechanism-specific bound

In the following, we show that this mechanism-agnostic guarantee implicitly upper bounds our tight mechanism-specific guarantee via joint convexity. Note that this is qualitatively different from our discussion in Appendix G. There we showed, that the mechanism-agnostic guarantees can be derived from mechanism-specific bounds that only use a binary partitioning of the batch space (unlike the mechanism-specific guarantee considered here). We show this result w.l.o.g. for \(K_{-}=0\) and \(K_{+}=K\).

**Theorem N.4**.: _Let \(B:\mathbb{Y}\rightarrow\mathbb{Z}\) be an arbitrary base mechanism. Let \(\bm{y}^{(1)}\in\mathbb{Y}^{1}\) and \(,\bm{y}^{(2)}\in\mathbb{Y}^{K+1}\) be arbitrary tuples of batches that fulfill \(\forall l,t,u:d_{\mathbb{Y}}(y^{(l)}_{t},y^{(l)}_{u})\leq|t-u|\), and \(\forall t,u:d_{\mathbb{Y}}(y^{(1)}_{t},y^{(2)}_{u})\leq(t-1)+(u-1)\). Then, for \(\alpha\geq 1\)_

\[H_{\alpha^{\prime}}\left(b_{y^{(1)}_{1}}||\sum_{j=1}^{K+1}b_{y^{(2)}_{j}}\cdot \mathrm{Binom}(j-1\mid K,r)\right)\leq\sum_{k=1}^{K}\mathrm{Binom}(k\mid K,r) \cdot\delta_{k}(\alpha)\]

_with \(w=(1-(1-r)^{K})\), \(\alpha^{\prime}=1+w(\alpha-1)\) and group privacy parameters_

\[\delta_{k}(\alpha)=\max_{y,y^{\prime}}H_{\alpha}(b_{y}||b_{y^{\prime}})\quad \text{s.t.}\quad d_{\mathbb{Y}}(y,y^{\prime})\leq k.\]Proof.: We can apply joint convexity to show

\[H_{\alpha^{\prime}}\left(b_{y_{1}^{{}^{(1)}}}||\sum_{j=1}^{K+1}b_{y_{ j}^{{}^{(2)}}}\cdot\mathrm{Binom}(j-1\mid K,r)\right)\] (30) \[= H_{\alpha^{\prime}}\left(b_{y_{1}^{{}^{(1)}}}||(1-w)\cdot b_{y_{ 1}^{{}^{(2)}}}+\sum_{j=2}^{K+1}w\cdot b_{y_{j}^{{}^{(2)}}}\cdot\left(\frac{1}{w} \cdot\mathrm{Binom}(j-1\mid K,r)\right)\right)\] (31) \[= H_{\alpha^{\prime}}\left(b_{y_{1}^{{}^{(1)}}}||\sum_{j=2}^{K+1} \left((1-w)\cdot b_{y_{1}^{{}^{(2)}}}+w\cdot b_{y_{j}^{{}^{(2)}}}\right)\cdot \left(\frac{1}{w}\cdot\mathrm{Binom}(j-1\mid K,r)\right)\right)\] (32) \[\leq \sum_{j=2}^{K+1}\frac{1}{w}\cdot\mathrm{Binom}(j-1\mid K,r)\cdot H _{\alpha^{\prime}}\left(b_{y_{1}^{{}^{(1)}}}||\left((1-w)\cdot b_{y_{1}^{{}^{ (2)}}}+w\cdot b_{y_{j}^{{}^{(2)}}}\right)\right)\] (33)

The result then immediately follows from advanced joint convexity (see Proposition G.1) and joint convexity. 

### RDP guarantee

For RDP, we can obtain a qualitatively similar guarantee by simply applying the known mechanism-agnostic RDP subsampling guarantee for Poisson subsampling from [29, 30] to each of the \(K\) divergences (although we could also derive this RDP guarantee from scratch via optimal transport) in Eq. (33) of the previous derivation.

**Theorem N.5**.: _Let \(M=B\circ S\), where \(S\) is Poisson subsampling with rate \(r\). Let \(\simeq_{\forall}\) be the insertion/removal batch relation \(\simeq_{\pm,\forall}\). Then, for all \(x\simeq_{K\pm,\mathbb{X}}x^{\prime}\) and all \(\alpha>0\)_

\[\Lambda_{\alpha}(m_{x}||m_{x^{\prime}})\leq\sum_{k=1}^{K}\frac{1}{w}\cdot \mathrm{Binom}(k\mid K,r)\cdot 2\cdot\sum_{l=0}^{\alpha}\binom{\alpha}{l}w^{l}(1-w)^{ \alpha-l}\zeta_{k}(l)\]

_with \(w=(1-(1-r)^{K})\) and group privacy parameters_

\[\zeta_{k}(l)=\max_{y,y^{\prime}}\Lambda_{l}(b_{y}||b_{y^{\prime}})\quad\text{ s.t.}\quad d_{\mathbb{Y}}(y,y^{\prime})\leq k.\]

The factor \(2\) in Theorem N.5 can be eliminated for distributions with particular symmetries (see Theorem 5 in [29]) or bounded Pearson-Vajda \(\chi^{l}\)-pseudo-divergence (see Theorem 8 in [30]). We thus do not include the factor \(2\) when using this amplification guarantee as a baseline in our experiments.

### Asymptotic RDP guarantees

As mentioned in Section 3.1, our focus is on tight bounds that can be explicitly computed. However, analyzing the asymptotic behavior of these bounds can provide a potentially useful high-level picture of their behavior. As discussed in the previous section, and as can be seen from Eq. (33), Renyi divergence in the group privacy setting is bounded by a weighted sum of Renyi divergences between a single distribution and a mixture of two distributions. For the special case of additive Gaussian mechanisms with global sensitivity \(L\), this bound is equivalent (see Appendix A of [30]) to

\[\sum_{k=1}^{K}\frac{1}{w}\cdot\mathrm{Binom}(k\mid K,r)\cdot 2\cdot\Lambda_{ \alpha}\left(\mathcal{N}(0,\sigma)||(1-w)N(0,\sigma)+w\cdot N(1,\sigma\,/ \,(k\cdot L))\right).\] (34)

Asymptotic bounds on Renyi divergences with one or two components have been derived in prior work on privacy accounting [6, 28, 48]. We can apply these asymptotic bounds to each summand. For instance (see Lemma 5 in [6]):

**Proposition N.6**.: _Abadi et al. [6] Let \(\sigma\geq 1\) and \(q\leq\frac{1}{16\sigma}\). Then, for any positive integer \(\alpha\leq\sigma^{2}\ln\left(\frac{1}{q\sigma}\right)-1\),_

\[\Lambda_{\alpha}\left(\mathcal{N}(0,\sigma)||(1-q)N(0,\sigma)+q\cdot N(1, \sigma)\right)\leq\frac{q^{2}\alpha(\alpha-1)}{(1-q)\sigma^{2}}+\mathcal{O}(q ^{3}\alpha^{3}\,/\,\sigma^{3}).\]Applying Proposition N.6 to Eq. (34) yields the following asymptotic bound for RDP group privacy:

**Theorem N.7**.: _Let \(M=B\circ S\), where \(S\) is Poisson subsampling with rate \(r\) and \(B\) is an additive gaussian mechanism with global sensitivity \(L\) under the insertion/removal relation \(\simeq_{\pm}\). Consider arbitrary datasets \(x\simeq_{K\pm,\mathbb{X}}x^{\prime}\). Define weight \(w=(1-(1-r)^{K})\). If \(\sigma\geq k\cdot L\) and \(w\leq\frac{1}{16\sigma}\), then it holds for any positive integer \(\alpha\leq\frac{\sigma^{2}}{k^{2}\cdot L^{2}}\ln\left(\frac{1}{w\sigma}\right)-1\) that_

\[\Lambda_{\alpha}(m_{x}||m_{x^{\prime}})\leq\sum_{k=1}^{K}\frac{1}{w}\cdot \mathrm{Binom}(k\mid K,r)\cdot 2\cdot\frac{k^{2}L^{2}q^{2}\alpha(\alpha-1)}{(1-w) \sigma^{2}}+\mathcal{O}(k^{3}L^{3}w^{3}\alpha^{3}\,/\,\,\sigma^{3}).\]

Alternatively, one could apply the asymptotic bound from Theorem 38 from [48] to each summand. If we were to instead consider group privacy under dataset-level substitutions, where each summand would include a divergence between two mixtures with two components, we could instead use the asymptotic bounds from Appendix C of [28].

Worst-case mixture components

### Gaussian and Laplacian mixtures

We outline a self-contained and extendable proving strategy which we use to find dominating pairs of Gaussian and Laplacian mixtures given divergences of the form \(\Psi_{\alpha}(P||Q)=\int_{\mathbb{R}^{D}}f\left(P(\bm{x}),-Q(\bm{x})\right)d\bm{x}\), where \(f\) is (not necessarily strictly) convex and increasing in both arguments. Our two main examples of the hockey stick divergence \(\Psi_{\alpha}=H_{\alpha}\) and (scaled and exponentiated) Renyi divergence \(\Psi_{\alpha}=\Lambda_{\alpha}\) are special cases with \(f(x,y)=\max\{x+\alpha y,0\}\) and \(f(x,y)=x^{\alpha}(-y)^{1-\alpha}\), respectively. Throughout the subsection, we use \(M\), \(N\) to denote the sets of means of the two mixtures \(P\), \(Q\). We start with two general lemmata before treating the Gaussian and Laplacian mixture cases in particular. Together, they provide a constructive toolset to connect any mixture pair with means \((M,N)\) to a dominating mixture pair with means \((M^{*},N^{*})\) via a path of geometric transformations \((M,N)\longmapsto(M^{(1)},N^{(1)})\longmapsto\cdots\longmapsto(M^{*},N^{*})\): concretely, these are

* mirroring the means of the two mixtures onto opposite sides of a hyperplane, and
* pushing such hyperplane-separated means further away along the hyperplane normal.

In the first part of this section, we prove that the divergence can only stay equal or grow under any such transformation. Afterwards, we construct an explicit path of transformations that maps any Gaussian, as well as any Laplacian mixture onto a dominating pair which is feasible under the pairwise distance constraints discussed in Appendix M.2.

**Lemma O.1**.: _Given \(p\in\{1,2\}\), consider two mixtures of the form \(P_{M}(\bm{x})=\sum_{k=0}^{K}w_{k}\rho(\|\bm{x}-\mu_{k}\|_{p})\), \(Q_{N}(\bm{x})=\sum_{\kappa=0}^{\mathcal{K}}\omega_{\kappa}\rho(\|\bm{x}-\nu_{ \kappa}\|_{p})\) with means in \(M:=\{\mu_{0},\,\ldots,\,\mu_{K}\}\subset\mathbb{R}^{D}\), \(N:=\{\nu_{0},\,\ldots,\,\nu_{\mathcal{K}}\}\subset\mathbb{R}^{D}\) and a decreasing \(\rho\colon\mathbb{R}^{+}_{0}\to[0,1]\). Consider all hyperplanes which contain zero and are normal to \(L^{2}\)-unit vectors \(\hat{\bm{n}}\in\mathcal{H}_{p}\), where_

_(1) \(\mathcal{H}_{1}=\{\sigma\hat{e}_{i}\mid 0\leq i\leq K,\sigma\in\{+,-\}\}\cup\{ \frac{1}{\sqrt{2}}(\sigma_{1}\hat{e}_{i}+\sigma_{2}\hat{e}_{j})\mid 0\leq i\neq j \leq K,\sigma_{1},\sigma_{2}\in\{+,-\}\}\), (2) \(\mathcal{H}_{2}=S^{D}\), the unit \(D\)-sphere,_

_and define the lower half-space \(\mathbb{R}^{-}_{\hat{\bm{n}}}=\{\bm{x}\in\mathbb{R}^{D}\mid\big{(}\bm{x}^{T} \hat{\bm{n}}\big{)}<0\}\) and upper half-space \(\mathbb{R}^{+}_{\hat{\bm{n}}}=\{\bm{x}\in\mathbb{R}^{D}\mid\big{(}\bm{x}^{T} \hat{\bm{n}}\big{)}>0\}\)._

_Consider the map_

\[(\,\cdot\,)^{\prime}_{\hat{\bm{n}}}\colon\mathbb{R}^{D}\longrightarrow\mathbb{ R}^{D}\setminus\mathbb{R}^{-}_{\hat{\bm{n}}},\quad\bm{x}\longmapsto\bm{x}- \mathds{1}_{\mathbb{R}^{-}_{\hat{\bm{n}}}}(\bm{x})(2\hat{\bm{n}}^{T}\bm{x}) \hat{\bm{n}}\]

_which mirror-reflects the lower into the upper half-space, and its image sets \(M^{\prime}_{\hat{\bm{n}}}=\mathbb{R}^{D}\setminus\mathbb{R}^{-}_{\hat{\bm{n} }}\) and \(N^{\prime}_{-\hat{\bm{n}}}\subset\mathbb{R}^{D}\setminus\mathbb{R}^{+}_{\hat{ \bm{n}}}\)._

_The reflected pair of mixtures has equal or greater divergence:_

\[\Psi_{\alpha}\left(P_{M^{\prime}_{\hat{\bm{n}}}}||Q_{N^{\prime}_{-\hat{\bm{n} }}}\right)\geq\Psi_{\alpha}\left(P_{M}||Q_{N}\right).\]

_Remark O.2_.: Lemma O.1 applies to Laplacian and Gaussian mixtures with \(p=1\) and \(p=2\), respectively.

Proof.: We will need the following lemma.

**Lemma O.3**.: _Given \(p\in\{1,2\}\), a hyperplane normal to \(\hat{\bm{n}}\in\mathcal{H}_{p}\), and points \(\bm{x}_{1},\bm{x}_{2}\in\mathbb{R}^{D}\), we have_

\[\begin{split}&\|(\bm{x}_{1})^{\prime}_{\hat{\bm{n}}}-(\bm{x}_{2})^{ \prime}_{\hat{\bm{n}}}\|_{p}\leq\|\bm{x}_{1}-\bm{x}_{2}\|_{p}& \text{if}\quad(\bm{x}_{1}\in\mathbb{R}^{+}_{\hat{\bm{n}}}\wedge\bm{x}_{2} \in\mathbb{R}^{-}_{\hat{\bm{n}}})\quad\vee\quad(\bm{x}_{1}\in\mathbb{R}^{-}_{ \hat{\bm{n}}}\wedge\bm{x}_{2}\in\mathbb{R}^{+}_{\hat{\bm{n}}})\\ &\|(\bm{x}_{1})^{\prime}_{\hat{\bm{n}}}-(\bm{x}_{2})^{\prime}_{ \hat{\bm{n}}}\|_{p}=\|\bm{x}_{1}-\bm{x}_{2}\|_{p}&\text{else}. \end{split}\]

**Corollary O.4**.: _If the sets \(M\), \(N\) are feasible under pairwise \(p\)-norm distance constraints, then so are \(M^{\prime}_{\hat{\bm{n}}}\), \(N^{\prime}_{-\hat{\bm{n}}}\)._

Proof of Lemma O.3.: If \(\text{sign}(\bm{n}^{T}\bm{x}_{1})=\text{sign}(\bm{n}^{T}\bm{x}_{2})\), then \((\,\cdot\,)^{\prime}_{\hat{\bm{n}}}\) acts uniformly on both vectors and the condition clearly holds. Else, assume w.l.o.g. that \(\bm{x}_{2}\in\mathbb{R}^{-}_{\hat{\bm{n}}}\). We start with the case \(p=1\).

First, assume \(\hat{\bm{n}}\in\{\sigma\hat{\bm{e}}_{i}\mid 0\leq i\leq K,\sigma_{1}\in\{+,-\}\}\). Since \((\,\cdot\,)^{\prime}_{\hat{\bm{n}}}\) now acts only on vector component \(i\), we can write \(\|(\bm{x}_{1})^{\prime}_{\hat{\bm{n}}}-(\bm{x}_{2})^{\prime}_{\hat{\bm{n}}}\|_ {1}-\|\bm{x}_{1}-\bm{x}_{2}\|_{1}\) = \(\|\hat{e}_{i}^{T}\bm{x}_{1}-|\hat{e}_{i}^{T}\bm{x}_{2}||-|\hat{e}_{i}^{T}\bm{x }_{1}-\hat{e}_{i}^{T}\bm{x}_{2}|\leq 0\) by the inverse triangle inequality.

Now assume instead that \(\hat{\bm{n}}\in\{\frac{1}{\sqrt{2}}(\sigma_{1}\hat{\bm{e}}_{i}+\sigma_{2}\hat {\bm{e}}_{j})\mid 1\leq i\neq j\leq D,\sigma_{1},\sigma_{2}\in\{+1,-1\}\}\). Now, \((\,\cdot\,)^{\prime}_{\hat{\bm{n}}}\) acts only on vector components \(i\) and \(j\). Thus, \(\|(\bm{x}_{1})^{\prime}_{\hat{\bm{n}}}-(\bm{x}_{2})^{\prime}_{\hat{\bm{n}}}\|_ {1}-\|\bm{x}_{1}-\bm{x}_{2}\|_{1}\) = \(\|\pi_{i,j}\left[(\bm{x}_{1})^{\prime}_{\hat{\bm{n}}}-(\bm{x}_{2})^{\prime}_{ \hat{\bm{n}}}\right]\|_{1}-\|\pi^{(i,j)}\left[\bm{x}_{1}-\bm{x}_{2}\right]\|_ {1}\) where \(\pi^{(i,j)}\) denotes projection onto the subspace spanned by the basis vectors \(\hat{\bm{e}}_{i},\hat{\bm{e}}_{j}\). It is useful to write

\[\|\pi_{i,j}\left[(\bm{x}_{1})^{\prime}_{\hat{\bm{n}}}-(\bm{x}_{2})^{\prime}_{ \hat{\bm{n}}}\right]\|_{1} =\max_{\bm{\sigma}\in\{-1,+1\}^{D}}\lvert\bm{\sigma}^{T}\pi_{i,j} \left[(\bm{x}_{1})^{\prime}_{\hat{\bm{n}}}-(\bm{x}_{2})^{\prime}_{\hat{\bm{n} }}\right]\rvert\]

The argument of the \(\max\) function admits two cases. Either, \(\bm{\sigma}^{T}(\pi_{i,j}\hat{\bm{n}})=0\), in which case \(\lvert\bm{\sigma}^{T}\pi_{i,j}\left[(\bm{x}_{1})^{\prime}_{\hat{\bm{n}}}-(\bm {x}_{2})^{\prime}_{\hat{\bm{n}}}\right]\rvert=\lvert\bm{\sigma}^{T}\pi_{i,j} \left[\bm{x}_{1}-\bm{x}_{2}\right]\rvert\). In the other case, \(\pi_{i,j}\bm{\sigma}=\pm\frac{2}{\sqrt{2}}\pi_{i,j}\hat{\bm{n}}\). Then, by the inverse triangle inequality,

\[\lvert\bm{\sigma}^{T}\pi_{i,j}\left[(\bm{x}_{1})^{\prime}_{\hat{\bm{n}}}-(\bm {x}_{2})^{\prime}_{\hat{\bm{n}}}\right]\rvert=\frac{2}{\sqrt{2}}\left|\pm\hat {\bm{n}}^{T}\bm{x}_{1}\mp\hat{\bm{n}}^{T}\bm{x}_{2}\pm 2\hat{\bm{n}}^{T}\bm{x}_{2} \right|=\frac{2}{\sqrt{2}}\left|\pm\hat{\bm{n}}^{T}\bm{x}_{1}\pm\hat{\bm{n}}^{ T}\bm{x}_{2}\right|\]

For completeness, we also prove \(p=2\). By invariance of the scalar product under mirror reflection, \(\|\bm{x}^{\prime}_{1}-\bm{x}^{\prime}_{2}\|_{2}=\bm{x}^{T}_{1}\bm{x}_{1}+\bm{ x}^{\prime T}_{2}\bm{x}^{\prime}_{2}-2\bm{x}^{T}_{1}\bm{x}^{\prime}_{2}=\bm{x}^{T}_{1} \bm{x}_{1}+\bm{x}^{T}_{2}\bm{x}_{2}-2\bm{x}^{T}_{1}\bm{x}_{2}+2(\bm{n}^{T}\bm{ x}_{2})(\bm{n}^{T}\bm{x}_{1})=\|\bm{x}_{1}-\bm{x}_{2}\|_{2}+2(\bm{n}^{T}\bm{x}_{2})( \bm{n}^{T}\bm{x}_{1})\leq\|\bm{x}_{1}-\bm{x}_{2}\|_{2}\). 

We recall the notation introduced at the start of the section, \(\Psi_{\alpha}(P||Q)=\int_{\mathbb{R}^{D}}f\left(P(\bm{x}),-Q(\bm{x})\right)d\bm{x}\), where we assume an integrand \(f\) that is convex in both arguments. The statement of Lemma O.1 will follow from the next lemma, which informally states that at any point \(\bm{x}\) in the upper half-space, the mirror-reflection of means contained in \(M^{-}_{\hat{\bm{n}}}\) causes an increase of the integrand \(f\left(P_{M}(\bm{x}),-Q_{N}(\bm{x})\right)\) at \(\bm{x}\) which dominates the corresponding decrease at the mirror image point \(\bm{x}^{\prime}\) in the lower half-space.

**Lemma O.5**.: _Given a hyperplane normal to \(\hat{\bm{n}}\in\mathcal{H}_{p}\) as defined in Lemma O.1 as well as any point \(\bm{x}\in\mathbb{R}^{+}_{\hat{\bm{n}}}\) along with its mirror image \(\bm{x}^{\prime}=\bm{x}-2\left(\bm{x}^{T}\hat{\bm{n}}\right)\hat{\bm{n}}\in \mathbb{R}^{-}_{\hat{\bm{n}}}\), the change of \(f\) satisfies_

\[\left[f\left(P_{M^{\prime}_{\hat{\bm{n}}}}(\bm{x}),-Q_{N^{\prime}_ {-\hat{\bm{n}}}}(\bm{x})\right)-f\left(P_{M}(\bm{x}),-Q_{N}(\bm{x})\right)\right]\] \[+ \left[f\left(P_{M^{\prime}_{\hat{\bm{n}}}}(\bm{x}^{\prime}),-Q_{N^ {\prime}_{-\hat{\bm{n}}}}(\bm{x}^{\prime})\right)-f\left(P_{M}(\bm{x}^{ \prime}),-Q_{N}(\bm{x}^{\prime})\right)\right]\geq 0.\]

Proof of Lemma O.5.: We introduce the following notation,

\[P_{0}(\bm{x}) :=\sum_{\mu_{k}\in M\setminus\mathbb{R}^{-}_{\hat{\bm{n}}}}w_{k} \rho(\|\bm{x}-\mu_{k}\|_{p}), Q_{0}(\bm{x}) :=\sum_{\nu_{k}\in N\setminus\mathbb{R}^{-}_{-\hat{\bm{n}}}}\omega_{k}\rho(\| \bm{x}-\nu_{\kappa}\|_{p}),\] \[\delta P(\bm{x}) :=\sum_{\mu_{k}\in M\cap\mathbb{R}^{-}_{\hat{\bm{n}}}}w_{k}\rho(\| \bm{x}-\mu_{k}\|_{p}), \delta Q(\bm{x}) :=\sum_{\nu_{k}\in N\cap\mathbb{R}^{-}_{-\hat{\bm{n}}}}\omega_{k}\rho(\| \bm{x}-\nu_{\kappa}\|_{p}),\]

thus decomposing the densities into a part that stays fixed and a part that gets mirror-reflected. Using Lemma O.3 and the monotonicity requirement on \(\rho\), we can directly verify that

\[P_{0}(\bm{x})\geq P_{0}(\bm{x}^{\prime}), \delta P(\bm{x})\leq\delta P(\bm{x}^{\prime}),\] (35) \[-Q_{0}(\bm{x})\geq-Q_{0}(\bm{x}^{\prime}), -\delta Q(\bm{x})\leq-\delta Q(\bm{x}^{\prime}).\] (36)

Using invariance of distances under simultaneous mirroring of both vectors, we rewrite the expression of the lemma as

\[\left[f\left(P_{0}(\bm{x})+\delta P(\bm{x}^{\prime}),-Q_{0}(\bm{x})-\delta Q( \bm{x}^{\prime})\right)-f\left(P_{0}(\bm{x})+\delta P(\bm{x}),-Q_{0}(\bm{x})- \delta Q(\bm{x})\right)\right]\] \[-\left[f\left(P_{0}(\bm{x}^{\prime})+\delta P(\bm{x}^{\prime}),-Q_ {0}(\bm{x}^{\prime})-\delta Q(\bm{x}^{\prime})\right)-f\left(P_{0}(\bm{x}^{ \prime})+\delta P(\bm{x}),-Q_{0}(\bm{x}^{\prime})-\delta Q(\bm{x})\right)\right]\]

The statement of the lemma follows from convexity of \(f\) in both arguments by Jensen's inequality.

We now proceed to prove Lemma O.1. The difference of divergences can be rewritten as

\[\Psi_{\alpha}\left(P_{M_{\bm{\hat{n}}}^{\prime}}||Q_{N_{-\bm{\hat{n} }}^{\prime}}\right)-\Psi_{\alpha}\left(P_{M}||Q_{N}\right)\] \[= \int_{\mathbb{R}^{D}}\left[f\left(P_{M_{\bm{\hat{n}}}^{\prime}}( \bm{x}),-Q_{N_{-\bm{\hat{n}}}^{\prime}}(\bm{x})\right)-f\left(P_{M}(\bm{x}),-Q _{N}(\bm{x})\right)\right]d\bm{x}\] \[= \int_{\mathbb{R}^{-}_{\bm{\hat{n}}}}\left[f\left(P_{M_{\bm{\hat{n} }}^{\prime}}(\bm{x}),-Q_{N_{-\bm{\hat{n}}}^{\prime}}(\bm{x})\right)-f\left(P_{M }(\bm{x}),-Q_{N}(\bm{x})\right)\right]d\bm{x}\] \[+ \int_{\mathbb{R}^{+}_{\bm{\hat{n}}}}\left[f\left(P_{M_{\bm{\hat{n} }}^{\prime}}(\bm{x}),-Q_{N_{-\bm{\hat{n}}}^{\prime}}(\bm{x})\right)-f\left(P_ {M}(\bm{x}),-Q_{N}(\bm{x})\right)\right]d\bm{x}\] \[= \int_{\mathbb{R}^{+}_{\bm{\hat{n}}}}\left[f\left(P_{M_{\bm{\hat{n }}}^{\prime}}(\bm{x}^{\prime}),-Q_{N_{-\bm{\hat{n}}}^{\prime}}(\bm{x}^{\prime })\right)-f\left(P_{M}(\bm{x}^{\prime}),-Q_{N}(\bm{x}^{\prime})\right)\right]\] \[+ \left[f\left(P_{M_{\bm{\hat{n}}}^{\prime}}(\bm{x}),-Q_{N_{-\bm{ \hat{n}}}^{\prime}}(\bm{x})\right)-f\left(P_{M}(\bm{x}),-Q_{N}(\bm{x})\right) \right]d\bm{x}\] \[\geq 0\]

Apart from Lemma O.5, we used the fact that mirror reflection has a unit absolute Jacobian determinant, and that the hyperplane, a null set, does not contribute to the integral. 

Lemma O.1 shows that we can construct a dominating mixture by mirroring all means onto opposite sides of a suitable hyperplane. This is exactly the condition under which the follow-up transform discussed in the next lemma yields equal or greater divergence. We slightly change perspective and consider the means of both mixtures as a joint vector. Given two sets of means \(M=\{\mu_{0},\,\dots,\,\mu_{K}\}\), \(N=\{\nu_{0},\,\dots,\,\nu_{\mathcal{K}}\}\) we define (in arbitrary ordering), \(\bm{\mu}=(\mu_{0},\,\dots,\,\mu_{K},\,\nu_{0},\,\dots,\,\nu_{\mathcal{K}}) \in\mathcal{F}\subset\mathbb{R}^{D\times(K+\mathcal{K})}\) where \(\mathcal{F}\) denotes the region feasible under the distance constraints. With this implicit mapping \((M,N)\mapsto\bm{\mu}\) between sets and vectors of means, we can treat the divergence as a function \(\Psi_{\alpha}\colon\mu\mapsto\int_{\mathbb{R}^{D}}\tilde{f}(\bm{x},\bm{\mu})d \bm{x}\) with integrand \(\tilde{f}(\bm{x},\bm{\mu})\coloneqq f(P_{M}(\bm{x}),-Q(\bm{x}))\). Now we can state:

**Lemma O.6**.: _For \(p\in\{1,2\}\), let \(\hat{\bm{n}}\in\mathcal{H}_{p}\) be a normal vector, \(M\), \(N\) with corresponding vector \(\bm{\mu}\) as above be two sets of means for which \(M\cap\mathbb{R}^{-}_{\hat{\bm{n}}}\) and \(N\cap\mathbb{R}^{-}_{-\hat{\bm{n}}}\) are empty (i.e., the hyperplane separates the two sets), and let \(\Psi_{\alpha}\) denote either \(\Lambda_{\alpha}\) or \(H_{\alpha}\)._

_Then, the normal directional derivatives of the divergence with respect to the means \(\mu_{l}\) and \(\nu_{\iota}\) are nonnegative for all \(l\in\{1,\dots,K\}\), \(\iota\in\{1,\dots,\mathcal{K}\}\),_

\[d_{\hat{\bm{n}}}^{(\mu_{l})}\Psi_{\alpha}\left(\bm{\mu}\right) \coloneqq \lim_{\varepsilon\to 0}\frac{1}{\varepsilon}\left[\Psi_{ \alpha}\left(\left(\mu_{0},\dots,\mu_{l}+\varepsilon\hat{\bm{n}},\dots,\nu_{ \mathcal{K}}\right)\right)-\Psi_{\alpha}\left(\bm{\mu}\right)\right]\geq 0\] \[d_{-\hat{\bm{n}}}^{(\nu_{\iota})}\Psi_{\alpha}\left(\bm{\mu}\right) \coloneqq \lim_{\varepsilon\to 0}\frac{1}{\varepsilon}\left[\Psi_{ \alpha}\left(\left(\mu_{0},\dots,\nu_{\iota}-\varepsilon\hat{\bm{n}},\dots,\nu_{ \mathcal{K}}\right)\right)-\Psi_{\alpha}\left(\bm{\mu}\right)\right]\geq 0.\]

_In the case \(\Psi_{\alpha}=H_{\alpha}\), the above expressions are instead defined in the sense of weak derivatives._

The lemma is quite intuitive: once both sets of means are already separated by a hyperplane, pushing them apart in the normal direction, thereby increasing the margin to the hyperplane, never decreases the divergence. As we will only ever use the directional derivatives within line integrals, the weak sense in which the lemma holds for \(\Psi_{\alpha}=H_{\alpha}\) is sufficient for our purposes.

Proof of Lemma o.6.: We first check that for all \(\bm{x}\in\mathbb{R}^{D}\) and \(\bm{\mu}\in\mathcal{F}\), the derivative \(\partial_{\bm{\mu}}\tilde{f}\) (possibly in the weak sense) exists and is dominated by an integrable function. In the case of Gaussian or Laplacian mixtures and \(\Psi_{\alpha}=\Lambda_{\alpha}\), existence of the derivative is clear. For Gaussian mixtures, the integrability condition holds since \(\partial_{\bm{\mu}}\tilde{f}=\mathcal{O}(\exp(-\|\bm{x}\|_{2}^{2}))\), for Laplacian mixtures \(\partial_{\bm{\mu}}\tilde{f}=\mathcal{O}(\exp(-\|\bm{x}\|_{1}))\) since the privacy loss factor in the integrand eventually becomes constant. In the case \(\Psi_{\alpha}=H_{\alpha}\), \(\tilde{f}\) has a weak derivative \(\partial_{\bm{\mu}}\tilde{f}=\mathds{1}_{f\geq 0}\partial_{\bm{\mu}}(P-\alpha Q)\), which agrees with its derivative except where the latter is not defined (i.e., on the null set of means and \(\bm{x}\) at the boundary of exact DP). Integrabilityfollows again from \(\partial_{\mu}\tilde{f}=\mathcal{O}(\exp(-\|\bm{x}\|_{1}))\). In the case of \(\Psi_{\alpha}=\Lambda_{\alpha}\),

\[d_{\hat{\bm{n}}}^{(\mu_{t})}\Psi_{\alpha}\left(\bm{\mu}\right)\] \[= \int_{\mathbb{R}^{D}}d_{\hat{\bm{n}}}^{(\mu_{t})}\left[\left( \sum_{k=0}^{K}w_{k}\rho(\|\bm{x}-\mu_{k}\|_{p})\right)^{\alpha}\right]\left( \sum_{\kappa=0}^{\mathcal{K}}\omega_{\kappa}\rho(\|\bm{x}-\nu_{\kappa}\|_{p}) \right)^{1-\alpha}d\bm{x}\] \[= \alpha w_{l}\int_{\mathbb{R}^{D}}\left(\frac{P_{M}(\bm{x})}{Q_{N }(\bm{x})}\right)^{\alpha-1}d_{\hat{\bm{n}}}^{(\mu_{t})}\rho(\|\bm{x}-\mu_{t} \|_{p})d\bm{x}\] \[= \alpha w_{l}\int_{\mathbb{R}^{D}}\left(\frac{P_{M}(\bm{x}+\mu_{l })}{Q_{N}(\bm{x}+\mu_{l})}\right)^{\alpha-1}d_{\hat{\bm{n}}}^{(\mu_{t})}\rho( \|\bm{x}\|_{p})\,d\bm{x}\]

Consider \(\bm{x}\in\mathbb{R}^{+}\) and its mirror image \(\bm{x}^{\prime}\in\mathbb{R}^{-}\). By Lemma O.3 and the assumption that the hyperplane separates \(M\) and \(N\), \(\frac{P_{M}(\bm{x}+\mu_{l})}{Q_{N}(\bm{x}+\mu_{l})}=\frac{\rho(\|\bm{x}+\mu_{l }\|_{p})P_{M}(\bm{x})}{\rho(\|\bm{x}+\mu_{l}\|_{p})P_{M}(\bm{x})}=\frac{P_{M} (\bm{x})}{Q_{N}(\bm{x})}\geq\frac{P_{M}(\bm{x}^{\prime})}{Q_{N}(\bm{x}^{\prime })}=\frac{P_{M}(\bm{x}^{\prime}+\mu_{l})}{Q_{N}(\bm{x}^{\prime}+\mu_{l})}\). Moreover, also by Lemma O.3, \(\rho(\|\bm{x}\|_{p})=\rho(\|\bm{x}^{\prime}\|_{p})\), and therefore \(d_{\hat{\bm{n}}}^{(\mu_{t})}\rho(\|\bm{x}\|_{p})=d_{-\hat{\bm{n}}}^{(\mu_{t})} \rho(\|\bm{x}^{\prime}\|_{p})=-d_{\hat{\bm{n}}}^{(\mu_{t})}\rho(\|\bm{x}^{ \prime}\|_{p})\).

Similarly, we observe how in

\[d_{-\hat{\bm{n}}}^{(\nu_{t})}\Psi_{\alpha}\left(\bm{\mu}\right)\] \[= \int_{\mathbb{R}^{D}}\left(\sum_{k=0}^{K}w_{k}\rho(\|\bm{x}-\mu_ {k}\|_{p})\right)^{\alpha}d_{-\hat{\bm{n}}}^{(\nu_{t})}\left[\left(\sum_{ \kappa=0}^{\mathcal{K}}\omega_{\kappa}\rho(\|\bm{x}-\nu_{\kappa}\|_{p})\right) ^{1-\alpha}\right]d\bm{x}\] \[= (-1)(1-\alpha)\omega_{\iota}\int_{\mathbb{R}^{D}}\left(\frac{P_{M }(\bm{x})}{Q_{N}(\bm{x})}\right)^{\alpha}d_{\hat{\bm{n}}}^{(\nu_{t})}\rho(\| \bm{x}-\nu_{\iota}\|_{p})d\bm{x}\] \[= (\alpha-1)\omega_{\iota}\int_{\mathbb{R}^{D}}\left(\frac{P_{M}( \bm{x}+\nu_{\iota})}{Q_{N}(\bm{x}+\nu_{\iota})}\right)^{\alpha}d_{\hat{\bm{n} }}^{(\nu_{t})}\rho(\|\bm{x}\|_{p})\,d\bm{x}\]

the negative signs of \(-\hat{\bm{n}}\) and \((1-\alpha)\) cancel and apply analogous reasoning thereafter.

The statement for \(\Psi_{\alpha}=\Lambda_{\alpha}\) now follows in analogy to the proof of Lemma O.1 via Lemma O.5.

For \(\Psi_{\alpha}=H_{\alpha}\), we find

\[d_{\hat{\bm{n}}}^{(\mu_{t})}\Psi_{\alpha}\left(\bm{\mu}\right)=w _{l}\int_{\mathbb{R}^{D}}\mathds{1}_{\{P(\bm{x})-\alpha Q(\bm{x})\geq 0\}}d_{ \hat{\bm{n}}}^{(\mu_{t})}\rho(\|\bm{x}-\mu_{l}\|_{p})d\bm{x}\] \[= w_{l}\int_{\mathbb{R}^{D}}\mathds{1}_{\left\{\frac{P_{M}(\bm{x }+\mu_{l})}{Q_{N}(\bm{x}+\mu_{l})}\geq\alpha\right\}}d_{\hat{\bm{n}}}^{(\mu_{t })}\rho(\|\bm{x}\|_{p})\,d\bm{x}\] \[d_{\hat{\bm{n}}}^{(\nu_{t})}\Psi_{\alpha}\left(\bm{\mu}\right)=-w _{\iota}\int_{\mathbb{R}^{D}}\mathds{1}_{\{P_{M}(\bm{x})-\alpha Q_{N}(\bm{x}) \geq 0\}}\left(-d_{\hat{\bm{n}}}^{(\mu_{t})}\rho(\|\bm{x}-\nu_{\iota}\|_{p} )\right)d\bm{x}\] \[= w_{\iota}\int_{\mathbb{R}^{D}}\mathds{1}_{\left\{\frac{P_{M}( \bm{x}+\mu_{l})}{Q_{N}(\bm{x}+\mu_{l})}\geq\alpha\right\}}d_{\hat{\bm{n}}}^{( \mu_{t})}\rho(\|\bm{x}\|_{p})\,d\bm{x}\]

and can proceed by analogous reasoning from here on.

We are now equipped to construct the sequence of transformations taking two arbitrary sets \(M,N\) of Gaussian or Laplacian mixture means to the means \(M^{*},N^{*}\) of a dominating pair. We start with the Gaussian case.

#### 0.1.1 Dominating pair of Gaussian mixtures

**Theorem 0.7**.: _Let \(\Psi_{\alpha}=H_{\alpha}\) or \(\Psi_{\alpha}=\Lambda_{\alpha}\). Any pair of Gaussian mixtures with means satisfying distance constraints of the form_

\[\mu_{0}=\nu_{0}=0\qquad\left\|\mu_{i}-\mu_{j}\right\|_{2}\leq|i-j|\quad\forall\, 0\leq i,\,j\leq K,\qquad\left\|\nu_{\iota}-\nu_{\tau}\right\|_{2}\leq|i-j|\quad \forall\,0\leq\iota,\,\tau\leq\mathcal{K},\]

_is dominated by a mixture with means_

\[\mu_{k}=k\cdot\hat{\bm{e}}\quad\forall k\in\{0,\,\ldots,\,K\},\qquad\nu_{\kappa}=- \kappa\cdot\hat{\bm{e}}\quad\forall\kappa\in\{0,\,\ldots,\,\mathcal{K}\},\qquad \text{with }\hat{\bm{e}}\in\mathbb{R}^{D},\,\left\|\hat{\bm{e}}\right\|_{2}=1.\]Proof.: We start by proving the following lemma.

**Lemma 0.8**.: _Any pair of Gaussian mixtures with means at a fixed set of radii,_

\[\left\|\mu_{k}\right\|_{2}=r_{k},\,\left\|\nu_{\kappa}\right\|_{2}=\rho_{\kappa}, \quad r_{0}=\rho_{0}=0,\quad r_{k},\rho_{\kappa}\in\mathbb{R}_{0}^{+}\quad \forall k\in\{1,\,\dots\,K\}\,\forall\kappa\in\{1,\,\dots\,\mathcal{K}\},\]

_and satisfying the constraints of Theorem 0.7 is dominated by a feasible pair with means at equal radii that are collinear on diametral half-lines through zero, i.e.,_

\[\mu_{k}=r_{k}\cdot\hat{\bm{e}},\quad\nu_{\kappa}=-\rho_{\kappa}\cdot\hat{\bm{e} }\quad\forall k\in\{0,\,\dots,\,K\}\,\forall\kappa\in\{0,\,\dots,\,\mathcal{K} \}\quad\text{with}\ \hat{\bm{e}}\in\mathbb{R}^{D},\,\left\|\hat{\bm{e}}\right\|_{2}=1.\]

Proof of Lemma 0.8.: Without loss of generality, we can pick as the collinearity direction from the lemma \(\hat{\bm{e}}=\bm{e}_{1}\), the first canonical basis vector, since the divergence is rotation invariant. Pick any orthogonal direction, say, the second basis vector \(\bm{e}_{2}\). By Lemma 0.1, we can mirror the two mixtures onto opposite sides of the hyperplanes with normal vectors \(\hat{\bm{e}}_{1}\) and \(\hat{\bm{e}}_{2}\) such that \(\hat{\bm{e}}_{1}^{T}\mu_{k}\geq 0\geq\hat{\bm{e}}_{2}^{T}\mu_{k}\) for all \(k\) and \(\hat{\bm{e}}_{1}^{T}\nu_{\kappa}\geq 0\geq\hat{\bm{e}}_{2}^{T}\nu_{\kappa}\) for all \(\kappa\).

Now, consider a hyperplane with normal vector \(\hat{\bm{n}}(\theta)=\hat{\bm{e}}_{1}\cos(\theta)+\hat{\bm{e}}_{2}\sin(\theta)\), \(\theta\in[0,\pi/2]\). Intuitively, we let this hyperplane undergo a full rotation and "scoop up" all the means which are not in the hyperplane normal to \(\hat{\bm{e}}_{2}\), giving a new set of means which are. Formally, we find that by the above sign condition, there is an angle \(\theta_{k}\in[0,\pi/2]\) for every \(k\) such that \(\text{sign}(\hat{\bm{n}}(\theta)^{T}\hat{\mu}_{k})=\text{sign}(\theta_{k}-\theta)\). Namely, using \(\pi_{12}\) to denote the projection onto the subspace spanned by vectors \(\hat{\bm{e}}_{1}\) and \(\hat{\bm{e}}_{2}\), \(\pi_{12}\mu_{k}=\|\pi_{12}\mu_{k}\|_{2}\left(\sin(\theta_{k})\hat{\bm{e}}_{1}- \cos(\theta_{k})\hat{\bm{e}}_{2}\right))\). Consider the paths \(\gamma_{k}\colon[0,\pi/2]\to\mathbb{R}^{D}\), where

\[\gamma_{k}(\theta)=\mathds{1}_{\{\theta_{k}-\theta\geq 0\}}\mu_{k}+\mathds{1}_{ \{\theta_{k}-\theta<0\}}\left((\mathds{1}-\pi_{12})\mu_{k}+\|\pi_{12}\mu_{k}\|_ {2}\left(\sin(\theta)\hat{\bm{e}}_{1}-\cos(\theta)\hat{\bm{e}}_{2}\right)\right)\]

and a sign-flipped construction with angles \(\theta_{\kappa}\) and curves \(\zeta_{\kappa}\) for the means \(\nu_{\kappa}\). For the derivatives along the curves, we find \(\gamma_{k}^{\prime}(\theta)=\|\pi_{12}\mu_{k}\|_{2}\hat{\bm{n}}(\theta)\) and \(\zeta_{\kappa}^{\prime}(\theta)=-\|\pi_{12}\zeta_{\kappa}\|_{2}\bm{n}(\theta)\). Also, by construction, \(\hat{\bm{n}}(\theta)^{T}\hat{\gamma}_{k}(\theta)\geq 0\geq\hat{\bm{n}}(\theta)^{T} \hat{\zeta}_{\kappa}(\theta)\). Hence, the prerequisites for Lemma 0.6 are fulfilled at every \(\theta\in[0,\pi/2]\). By evaluating the path integral along these curves and invoking Lemma 0.6, we find that the divergence along the path \(M,N\mapsto\tilde{M},\tilde{N}\) cannot decrease:

\[\Phi_{\alpha}(P_{\tilde{M}}||Q_{\tilde{N}})-\Phi_{\alpha}(P_{M}|| Q_{N})\] \[= \int_{0}^{\pi/2}\sum_{l=1}^{K}d_{\hat{\bm{n}}}^{(\mu_{l})}\Psi_{ \alpha}\left((\gamma_{1},\dots,\gamma_{K},\zeta_{1},\dots,\zeta_{\mathcal{K}} )(\theta)\right)\] \[\quad+\quad\sum_{\iota=1}^{K}d_{-\hat{\bm{n}}}^{(\nu_{\iota})} \Psi_{\alpha}\left((\gamma_{1},\dots,\gamma_{K},\zeta_{1},\dots,\zeta_{ \mathcal{K}})(\theta)\right)d\theta\geq 0\]

Clearly, the paths also preserve the radius of each mean. The final set of means \(\tilde{M}=\{\gamma_{1}(\pi/2),\dots,\gamma_{K}(\pi_{2})\},\tilde{N}=\{\zeta_{1 }(\pi/2),\dots,\zeta_{K}(\pi_{2})\}\) is contained in the hyperplane normal to \(\hat{\bm{e}}_{2}\). We can now simply repeat this procedure with all basis vectors orthogonal to \(\hat{\bm{e}}_{1}\) to find the set from the lemma. Since it is collinear with the same radii as \(M,N\), and the means \(M,N\) are feasible, the new set is also feasible by the Cauchy-Schwarz inequality. 

Since we can map any set of Gaussian mixture means onto one with the same radii that is collinear without decreasing the divergence, we can from now on study the problem restricted to a single radial dimension, using

\[\Psi_{\alpha}\bigg{|}_{\mathbb{R}\hat{\bm{e}}}:\ \prod_{k=0}^{K} \mathbb{R}_{0}^{+}\times\prod_{\kappa=0}^{\mathcal{K}}\mathbb{R}_{0}^{+} \longrightarrow\mathbb{R},\] \[(r_{0},\dots,r_{K},\rho_{0},\dots,\rho_{\mathcal{K}}) \longmapsto\Psi_{\alpha}\left(\sum_{\kappa=0}^{\mathcal{K}}\omega_{ \kappa}\mathcal{N}(-\rho_{\kappa}\hat{\bm{e}},\sigma^{2}\bm{I})||\sum_{k=0}^{ K}w_{k}\mathcal{N}(r_{k}\hat{\bm{e}},\sigma^{2}\bm{I})\right)\]

We can now state the next lemma which implies Theorem 0.7.

**Theorem 0.9**.: \((0,1,\dots,K,0,\dots,\mathcal{K})\) _is a global maximizer of \(\Psi_{\alpha}\bigg{|}_{\mathbb{R}\hat{\bm{e}}}\) under the constraints \(r_{0}=\rho_{0}=0\), \(|r_{i}-r_{j}|\leq|i-j|\ \forall i,j\in\{0,\dots,K\}\), \(|\rho_{\iota}-\rho_{\tau}|\leq|\iota-\tau|\ \forall\!t,\tau\in\{0,\dots,\mathcal{K}\}\)._Proof of Theorem o.9.: To avoid clutter, we constrain ourselves to the case with only means \(\mu_{k}\) and a single \(\nu_{0}=0\), since the proof involving several \(\nu_{\kappa}\) is completely analogous. Consider the line integral of the gradient

\[\bm{\nabla}\Psi_{\alpha}\bigg{|}_{\mathbb{R}_{0}^{+}\hat{\bm{e}}}:\ \prod_{k=0}^{K} \mathbb{R}_{0}^{+}\longrightarrow\mathbb{R}^{K},\quad(r_{0},\dots,r_{K}) \longmapsto\left(\frac{\partial}{\partial r_{0}}\Psi_{\alpha}\bigg{|}_{ \mathbb{R}_{0}^{+}\hat{\bm{e}}},\dots,\frac{\partial}{\partial r_{K}}\Psi_{ \alpha}\bigg{|}_{\mathbb{R}_{0}^{+}\hat{\bm{e}}}\right)\]

between any feasible \((r_{0},\dots,r_{K})\) and the point \((0,\dots,K)\) along the connecting path

\[\gamma_{r_{0},\dots,r_{K}}\colon\ [0,1]\longrightarrow\prod_{k=0}^{K} \mathbb{R}_{0}^{+},\] \[t\longmapsto\left(r_{0},\dots,r_{K}\right)+t\bm{v}:=\left(r_{0},\dots,r_{K}\right)+t\left[(0,\dots,K)-(r_{0},\dots,r_{K})\right].\]

First, observe that \(r_{k}\leq k\,\forall k\in\{0,\dots,K\}\) anywhere in the feasible region, since \(r_{k}=\sum_{j=0}^{k-1}\left(r_{j+1}-r_{j}\right)\leq\sum_{j=0}^{k-1}|r_{j+1}- r_{j}|\leq\sum_{j=0}^{k-1}1=k\). This means that \(\bm{v}\) is componentwise-nonnegative and vanishes only if \((r_{0},\dots,r_{K})=(0,\dots,K)\). By Lemma O.6, the gradient \(\bm{\nabla}\Psi_{\alpha}\bigg{|}_{\mathbb{R}_{0}^{+}\hat{\bm{e}}}\) is componentwise-nonnegative anywhere along \(\gamma_{r_{0},\dots,r_{K}}\). We may thus conclude that

\[\Psi_{\alpha}\bigg{|}_{\mathbb{R}_{0}^{+}\hat{\bm{e}}}(0,\dots,K) -\Psi_{\alpha}\bigg{|}_{\mathbb{R}_{0}^{+}\hat{\bm{e}}}(r_{0},\dots,r_{K})\] \[=\int_{\gamma_{(r_{0},\dots,r_{K})}}\bm{\nabla}\Psi_{\alpha}(\bm {r})\,d\bm{r}=\int_{0}^{1}\bm{\nabla}\Psi_{\alpha}(\gamma(t))^{T}\bm{v}(t)dt \geq 0.\]

This concludes the proof of Theorem O.7. 

#### o.1.2 Dominating pair of Laplacian mixtures

We also find an analogous result for Laplacian mixtures.

**Theorem O.10**.: _Let \(\Psi_{\alpha}=H_{\alpha}\) or \(\Psi_{\alpha}=\Lambda_{\alpha}\). Any pair of Laplacian mixtures with means in \(M=\{\mu_{0},\,\dots,\,\mu_{K}\},N=\{\nu_{0},\dots,\nu_{K}\}\subset\mathbb{R}^ {D}\) satisfying pairwise \(L^{1}\) distance constraints_

\[r_{0}=\rho_{0}=0,\left\|\mu_{i}-\mu_{j}\right\|_{1}\leq|i-j|\ \forall i,j\in\{0,\dots,K\},\left\|\nu_{\iota}-\nu_{\tau}\right\|_{1}\leq| \iota-\tau|\ \forall\iota,\tau\in\{0,\dots,\mathcal{K}\}.\]

_is dominated by a pair for \(M^{*},N^{*}\) of the form_

\[\mu_{k}^{*}=k\cdot\hat{\bm{e}},\quad\nu_{\kappa}^{*}=-\kappa\cdot\hat{\bm{e}} \quad\forall k\in\{0,\,\dots,\,K\}\,\forall\kappa\in\{0,\,\dots,\,\mathcal{K} \}\quad\text{with}\ \hat{\bm{e}}=\pm\hat{\bm{e}}_{i},\]

_where \(1\leq i\leq D\) such that \(\hat{\bm{e}}_{i}\) is the \(i\)-th canonical basis vector of \(\mathbb{R}^{D}\)._

Proof.: 

**Lemma O.11**.: _There is a dominating pair \(M^{*}\), \(N^{*}\) located in diametral quadrants of \(\mathbb{R}^{D}\): given the component-wise sign function,_

\[\exists\bm{\sigma}\in\{+,-\}^{D}\colon\text{sign}(\mu_{k})=\bm{\sigma}=-\text{ sign}(\nu_{\kappa})\quad\forall\mu_{k}\in M^{*},\,\nu_{\kappa}\in N^{*}.\]

Proof.: This is an immediate consequence of considering the canonical basis vectors as normal vectors in Lemma O.1. 

**Lemma O.12**.: _For \(\mu_{k}\in M\), \(\nu_{\kappa}\in N\), define the offset vectors \(\delta\mu_{k}\coloneqq\mu_{k}-\mu_{k-1}\), \(\delta\nu_{\kappa}\coloneqq\nu_{\kappa}-\nu_{\kappa-1}\) for \(1\leq k\leq K\), \(1\leq\kappa\leq\mathcal{K}\). A dominating pair \(M^{*}\), \(N^{*}\) has offset vectors located on diametral simplices of the 1-sphere: \(\exists\bm{\sigma}\in\{+1,-1\}^{D}\colon\,\delta\mu_{k}^{*}\in S_{\bm{\sigma}}\), \(\delta\nu_{\kappa}^{*}\in S_{-\bm{\sigma}}\), with \(S_{\bm{\sigma}}\coloneqq\{\bm{x}\in\mathbb{R}^{D}\mid\bm{\sigma}^{T}\bm{x}=1\}\), \(\forall\,k\in\{1,\dots,K\}\,\forall\,\kappa\in\{1,\dots,\mathcal{K}\}\)._Proof.: For any pair \(M,N\) of means not satisfying this condition, we will construct a new pair that does and has at least equal divergence. By Lemma O.11, we can assume without generality loss that \(\exists\,\boldsymbol{\sigma}^{(D)}\in\{+1,-1\}^{D}\colon\,\text{sign}(\mu_{k})= \boldsymbol{\sigma}^{(D)}=-\text{sign}(\nu_{\kappa})\quad\forall\mu_{k}\in M, \,\nu_{\kappa}\in N\), where we put a superscript \((D)\) above \(\boldsymbol{\sigma}\) for later reasons. At a later stage of the proof, we will invoke that shifting the means along the basis vector directions with signs prescribed by \(\boldsymbol{\sigma}^{(D)}\) are positive almost anywhere as a consequence of Lemma O.6.

It is useful to recast the optimization constraints in terms of the offset vectors: \(\forall\,k\in\{1,\ldots,K\}\,\forall\,\kappa\in\{1,\ldots,K\}\),

\[\mu_{0}=\nu_{0}=0,\quad\|\delta\mu_{k}\|_{1}=\max_{\boldsymbol{\sigma}\in\{-1, +1\}^{D}}\left(\boldsymbol{\sigma}^{T}\delta\mu_{k}\right)\leq 1,\quad\|\nu_{ \kappa}\|_{1}=\max_{\boldsymbol{\sigma}\in\{-1,+1\}^{D}}\left(\boldsymbol{ \sigma}^{T}\delta\nu_{\kappa}\right)\leq 1,\]

where the constraints between non-ascending and non-neighboring index pairs are implied by the symmetry and triangle inequality of the norm. For \(0\leq k\leq K\), and \(0\leq\kappa\leq\mathcal{K}\), set \(\mu_{k}^{(0)}:=\mu_{k}\), \(\nu_{\kappa}^{(0)}:=\nu_{\kappa}\), and define (recursively for \(1\leq i\leq D\)) the paths

\[\gamma_{k}^{(i)}\colon[0,1]\longrightarrow\mathbb{R}^{D},\quad \gamma_{k}^{(i)}(t)=\mu_{k}^{(i-1)}+t\sum_{l=1}^{k}\left(1-(\boldsymbol{\sigma} _{l}^{(i)})^{T}\delta\mu_{l}^{(i-1)}\right)\left((\boldsymbol{\sigma}^{(D)})^ {T}\hat{\boldsymbol{e}}_{i}\right)\hat{\boldsymbol{e}}_{i},\] \[\delta\mu_{l}^{(i-1)}=\mu_{l}^{(i-1)}-\mu_{l-1}^{(i-1)},\quad \boldsymbol{\sigma}_{l}^{(i)}=\operatorname*{arg\,max}_{\boldsymbol{\sigma} \in\{-1,+1\}^{D},\,\boldsymbol{\sigma}^{T}\hat{\boldsymbol{e}}_{i}=\left( \boldsymbol{\sigma}^{(D)}\right)^{T}\hat{\boldsymbol{e}}_{i}}\left( \boldsymbol{\sigma}^{T}\delta\mu_{l}^{(i-1)}\right),\quad\mu_{k}^{(i)}=\gamma _{k}^{(i)}(1),\] \[\zeta_{\kappa}^{(i)}\colon[0,1]\longrightarrow\mathbb{R}^{D}, \quad\zeta_{\kappa}^{(i)}(t)=\nu_{\kappa}^{(i-1)}+t\sum_{i=1}^{\kappa}\left(1- (\boldsymbol{\sigma}_{\kappa}^{(i)})^{T}\delta\nu_{\kappa}^{(i-1)}\right) \left(-(\boldsymbol{\sigma}^{(D)})^{T}\hat{\boldsymbol{e}}_{i}\right)\hat{ \boldsymbol{e}}_{i},\] \[\delta\nu_{\iota}^{(i-1)}=\nu_{\iota}^{(i-1)}-\nu_{\iota-1}^{(i- 1)},\quad\boldsymbol{\sigma}_{\iota}^{(i)}=\operatorname*{arg\,max}_{ \boldsymbol{\sigma}\in\{-1,+1\}^{D},\,\boldsymbol{\sigma}^{T}\hat{ \boldsymbol{e}}_{i}=-\left(\boldsymbol{\sigma}^{(D)}\right)^{T}\hat{ \boldsymbol{e}}_{i}}\left(\boldsymbol{\sigma}^{T}\delta\nu_{\iota}^{(i-1)} \right),\quad\nu_{\kappa}^{(i)}=\zeta_{\kappa}^{(i)}(1).\]

By construction,

\[\delta\mu_{k}^{(i)} =\delta\mu_{k}^{(i-1)}+\left(1-(\boldsymbol{\sigma}_{k}^{(i)})^{ T}\delta\mu_{k}^{(i-1)}\right)\left((\boldsymbol{\sigma}^{(D)})^{T}\hat{ \boldsymbol{e}}_{i}\right)\hat{\boldsymbol{e}}_{i},\] \[\delta\nu_{\kappa}^{(i)} =\delta\nu_{\kappa}^{(i-1)}+\left(1-(\boldsymbol{\sigma}_{\kappa }^{(i)})^{T}\delta\nu_{\kappa}^{(i-1)}\right)\left(-(\boldsymbol{\sigma}^{(D) })^{T}\hat{\boldsymbol{e}}_{i}\right)\hat{\boldsymbol{e}}_{i}.\]

We can easily check that \(\delta\mu_{k}^{(i)}\in S_{\boldsymbol{\sigma}_{k}^{(i)}}\), \(\delta\nu_{\kappa}^{(i)}\in S_{\boldsymbol{\sigma}_{\kappa}^{(i)}}\), for \(1\leq k\leq K,\,1\leq\kappa\leq\mathcal{K}\).

We can moreover verify that, based on the definitions of \(\sigma_{k}^{(i)}\) and \(\sigma_{\kappa}^{(i)}\), the feasibility conditions \(\|\delta\mu_{k}^{(i-1)}\|_{1}\leq 1\) and \(\|\delta\nu_{\kappa}^{(i-1)}\|_{1}\leq 1\) imply \(\|\delta\mu_{k}^{(i)}\|_{1}=\max_{\boldsymbol{\sigma}\in\{-1,+1\}^{D}}\left( \boldsymbol{\sigma}^{T}\delta\mu_{k}^{(i)}\right)\leq 1\) and \(\|\delta\nu_{\kappa}^{(i)}\|_{1}=\max_{\boldsymbol{\sigma}\in\{-1,+1\}^{D}} \left(\boldsymbol{\sigma}^{T}\delta\nu_{\kappa}^{(i)}\right)\leq 1\). Since, furthermore, \(\mu_{0}^{(i)}=\mu_{0}=0\), \(\nu_{0}^{(i)}=\nu_{0}=0\) for \(1\leq i\leq D\), and \(M,N\) are feasible by assumption, all pairs of \(M^{(i)}:=\{\mu_{0}^{(i)},\ldots,\mu_{K}^{(i)}\},N^{(i)}:=\{\nu_{0}^{(i)},\ldots, \nu_{\mathcal{K}}^{(i)}\}\) are also feasible by induction.

Finally, the above simplex and feasibility properties together imply that \(\forall\,k\in\{1,\ldots,K\},\,\forall\,\kappa\in\{1,\ldots\mathcal{K}\}\), \(\text{sign}\left(\hat{\boldsymbol{e}}_{i}^{T}\delta\mu_{k}^{(j)}\right)=\left( \boldsymbol{\sigma}^{(D)}\right)^{T}\hat{\boldsymbol{e}}_{i}\) and \(\text{sign}\left(\hat{\boldsymbol{e}}_{i}^{T}\delta\nu_{\kappa}^{(j)}\right)= -\left(\boldsymbol{\sigma}^{(D)}\right)^{T}\hat{\boldsymbol{e}}_{i}\) if \(j\geq i\). But this means in particular that we can identify \(\sigma_{k}^{(D)}=-\sigma_{\kappa}^{(D)}=\sigma^{(D)}\) for \(1\leq k\leq K\) and \(1\leq\kappa\leq\mathcal{K}\).

Putting everything together, we conclude that the pair \(M^{(D)},N^{(D)}\) obtained from \(M,N\) by concatenation of all paths is feasible and has offset vectors on diametral simplices, \(\delta\mu_{k}^{(D)}\in S_{\boldsymbol{\sigma}^{(D)}}\), \(\delta\nu_{\kappa}^{(D)}\in S_{-\boldsymbol{\sigma}^{(D)}}\,\forall\,k\in\{1, \ldots,K\},\,\forall\,\kappa\in\{1,\ldots\mathcal{K}\}\). What is left to show is that \(M^{(D)},N^{(D)}\) has at least the same divergence as \(M,N\). To this end, define the product curves

\[(\tilde{M}^{(i)}\times\tilde{N}^{(i)})\colon[0,1]\longrightarrow\mathbb{R}^{(K+ \mathcal{K})\times D},\quad t\longmapsto(\gamma_{0}^{(i)}(t),\ldots,\gamma_{K}^{( i)}(t),\zeta_{0}^{(i)}(t),\ldots,\zeta_{\mathcal{K}}^{(i)}(t)),\quad 1\leq i\leq D\]connecting the endpoints \((M^{(i-1)},N^{(i-1)})\) and \((M^{(i)},N^{(i)})\), and the image sets \(\mathcal{C}^{(i)}:=(\tilde{M}^{(i)}\times\tilde{N}^{(i)})([0,1])\subset\mathbb{R} ^{(K+K)\times D}\). The divergence difference is a sum of line integrals:

\[\Psi_{\alpha}(M^{(D)}||N^{(D)})-\Psi_{\alpha}(M||N)=\sum_{i=1}^{D}\Psi_{\alpha} (M^{(i)}||N^{(i)})-\Psi_{\alpha}(M^{(i-1)}||N^{(i-1)})\]

\[=\sum_{i=1}^{D}\int_{\mathcal{C}^{(i)}}\left(\sum_{k=0}^{K}d^{(\mu_{k})}_{ \boldsymbol{\hat{n}}_{i}}\Psi_{\alpha}(\tilde{M}^{(i)}||\tilde{N}^{(i)})+\sum _{\kappa=0}^{\mathcal{K}}d^{(\nu_{k})}_{-\boldsymbol{\hat{n}}_{i}}\Psi_{\alpha }(\tilde{M}^{(i)}||\tilde{N}^{(i)})\right)ds\]

\[=\sum_{i=1}^{D}\int_{t=0}^{t=1}\sum_{k=0}^{K}d^{(\mu_{k})}_{ \boldsymbol{\hat{n}}_{i}}\Psi_{\alpha}(\tilde{M}^{(i)}(t)||\tilde{N}^{(i)}(t) )\sum_{l=1}^{k}\left(1-(\boldsymbol{\sigma}_{l}^{(i)})^{T}\delta\mu_{l}^{(i-1) }\right)\]

At the start of the proof, we made the assumption without generality loss that \(\exists\,\boldsymbol{\sigma}^{(D)}\in\{+1,-1\}^{D}\colon\text{sign}(\mu_{k})= \boldsymbol{\sigma}^{(D)}=-\text{sign}(\nu_{\kappa})\ \ \forall\mu_{k}\in M,\,\nu_{\kappa}\in N\). Therefore, the directional derivatives \(d^{(\mu_{k})}_{\boldsymbol{\hat{n}}_{i}}\), \(d^{(\nu_{k})}_{-\boldsymbol{\hat{n}}_{i}}\) along the curve directions \(\pm\hat{\boldsymbol{n}}_{i}=\pm\left((\boldsymbol{\sigma}^{(D)})^{T}\hat{ \boldsymbol{e}}_{i}\right)\hat{\boldsymbol{e}}_{i}\) are nonnegative. The determinant factors \(\sum_{l=1}^{k}\left(1-(\boldsymbol{\sigma}_{l}^{(i)})^{T}\delta\mu_{l}^{(i-1) }\right)\) and \(\left(1-(\boldsymbol{\sigma}_{\iota}^{(i)})^{T}\delta\nu_{\iota}^{(i-1)}\right)\) are nonnegative due to the feasibility of \(M^{(i)},N^{(i)}\)\(\forall i\in\{0,\ldots,D\}\).

**Lemma O.13**.: _There is a dominating pair \(M^{*}\), \(N^{*}\) with \(i\in\{1,\ldots,D\}\) and \(\sigma\in\{-,+\}\) such that \(\delta\mu_{k}=\sigma\hat{\boldsymbol{e}}_{i}\), \(\delta\nu_{\kappa}=-\sigma\hat{\boldsymbol{e}}_{i}\)\(\forall\,k\in\{1,\ldots,K\}\,\forall\,\kappa\in\{1,\ldots,K\}\)._

Proof.: By Lemma O.12, we may assume without generality loss that \(\exists\boldsymbol{\sigma}\in\{+1,-1\}^{D}\colon\,\delta\mu_{k}\in S_{ \boldsymbol{\sigma}}\), \(\delta\nu_{\kappa}\in S_{-\boldsymbol{\sigma}}\), with \(S_{\boldsymbol{\sigma}}:=\{\boldsymbol{x}\in\mathbb{R}^{D}\mid\boldsymbol{ \sigma}^{T}\boldsymbol{x}=1\}\), \(\forall\,k\in\{1,\ldots,K\}\,\forall\,\kappa\in\{1,\ldots,\mathcal{K}\}\).

The following curves will be useful. For \(1\leq i\neq j\leq D\), \(l\in\{1,\ldots,D\}\) define

\[\gamma_{k}^{(ijl)}\colon[0,1]\longrightarrow\mathbb{R}^{D},\quad\gamma_{k}^{ (ijl)}(t)=\begin{cases}\mu_{k}\text{ for }k<l\\ \mu_{k}+t(\boldsymbol{\sigma}^{T}\hat{\boldsymbol{e}}_{i})(\hat{\boldsymbol{ e}}_{i}^{T}\delta\mu_{l})((\boldsymbol{\sigma}^{T}\hat{\boldsymbol{e}}_{j})\hat{ \boldsymbol{e}}_{j}-(\boldsymbol{\sigma}^{T}\hat{\boldsymbol{e}}_{i})\hat{ \boldsymbol{e}}_{i})\text{ for }k\geq l\end{cases}\]

\[\zeta_{\kappa}^{(ij\iota)}\colon[0,1]\longrightarrow\mathbb{R}^{D},\quad \zeta_{\kappa}^{(ij\iota)}(t)=\begin{cases}\nu_{\kappa}\text{ for }\kappa<\iota\\ \nu_{\kappa}+t(\boldsymbol{\sigma}^{T}\hat{\boldsymbol{e}}_{i})(\hat{ \boldsymbol{e}}_{i}^{T}\delta\nu_{\iota})((\boldsymbol{\sigma}^{T}\hat{ \boldsymbol{e}}_{j})\hat{\boldsymbol{e}}_{j}-(\boldsymbol{\sigma}^{T}\hat{ \boldsymbol{e}}_{i})\hat{\boldsymbol{e}}_{i})\text{ for }\kappa\geq\iota.\end{cases}\]

By construction, \(\gamma_{k}^{(ijl)}(0)=\mu_{k},\zeta_{\kappa}^{(ij)}(0)=\nu_{\kappa}\), \(\gamma_{0}^{(ijl)}(1)=0\), \(\zeta_{0}^{(ijl)}(1)=0\), \(\gamma_{k}^{(ijl)}(1)-\gamma_{k-1}^{(ijl)}(1)=\delta\mu_{k}\) for \(1\leq k\neq l\leq K\), \(\zeta_{\kappa}^{(iji)}(1)-\zeta_{\kappa-1}^{(ij\iota)}(1)=\delta\nu_{\kappa}\) for \(1\leq\kappa\neq\iota\leq\mathcal{K}\),

\[\left(\gamma_{l}^{(ijl)}(1)-\gamma_{l-1}^{(ijl)}(1)\right)^{T}\hat{\boldsymbol {e}}_{m}=\begin{cases}0\text{ for }m=i\\ (\boldsymbol{\hat{e}}_{i}^{T}\boldsymbol{\sigma})\delta\mu_{l}^{T}\hat{ \boldsymbol{e}}_{i}+(\hat{\boldsymbol{e}}_{j}^{T}\boldsymbol{\sigma})\delta\mu_{l }^{T}\hat{\boldsymbol{e}}_{j}\text{ for }m=j\\ \delta\mu_{l}^{T}\hat{\boldsymbol{e}}_{m}\text{ else},\end{cases}\]

and

\[\left(\zeta_{\iota}^{(ij\iota)}(1)-\zeta_{\iota-1}^{(ij\iota)}(1)\right)^{T} \hat{\boldsymbol{e}}_{m}=\begin{cases}0\text{ for }m=i\\ (-\hat{\boldsymbol{e}}_{i}^{T}\boldsymbol{\sigma})\delta\nu_{\iota}^{T}\hat{ \boldsymbol{e}}_{i}+(-\hat{\boldsymbol{e}}_{j}^{T}\boldsymbol{\sigma})\delta\nu_{ \iota}^{T}\hat{\boldsymbol{e}}_{j}\text{ for }m=j\end{cases}\]

In particular, this implies that the new pairs of sets \((M^{(l)}:=\{\gamma_{0}^{(ijl)}(1),\ldots,\gamma_{K}^{(ijl)}(1)\},N)\),

\((M,N^{(\iota)}:=\{\zeta_{0}^{(ijn)}(1),\ldots,\zeta_{K}^{(ij\iota)}(1)\})\) are feasible.

Assume that \(\exists l\in\{1,\ldots,K\},\,m\in\{1,\ldots,K\},\,i\in\{1,\ldots,D\},\,j\in\{1, \ldots,D\},\,i\neq j\colon\,(\delta\mu_{l}^{T}\hat{\boldsymbol{e}}_{i})\neq 0 \wedge(\delta\mu_{m}^{T}\hat{\boldsymbol{e}}_{j})\}\neq 0\). Note that we include the possibility of \(l=m\). We can then assume without generality loss (by Lemma O.1) that the conditions of Lemma O.6 hold for either \(\hat{\boldsymbol{n}}_{+}=\frac{1}{\sqrt{2}}((\boldsymbol{\sigma}^{T}\hat{ \boldsymbol{e}}_{j})\boldsymbol{e}_{j}-(\boldsymbol{\sigma}^{T}\hat{ \boldsymbol{e}}_{i})\hat{\boldsymbol{e}}_{i})\) or \(\hat{\boldsymbol{n}}_{-}=\frac{1}{\sqrt{2}}((\boldsymbol{\sigma}^{T}\hat{ \boldsymbol{e}}_{i})\hat{\boldsymbol{e}}_{i}-(\boldsymbol{\sigma}^{T}\hat{ \boldsymbol{e}}_{j})\hat{\boldsymbol{e}}_{j})\). In the case of \(\hat{\boldsymbol{n}}_{+}\), consider the product curve

\[(\tilde{M}^{(l)}\times N)\colon[0,1]\longrightarrow\mathbb{R}^{K+\mathcal{K} \times D},\quad t\longmapsto(\gamma_{0}^{(ijl)}(t),\ldots,\gamma_{K}^{(ijl)}(t), \nu_{0},\ldots,\nu_{\mathcal{K}})\]and the image set \(\mathcal{C}^{(l)}:=(\bar{M}^{(l)}\times N)([0,1])\). The divergence difference has the form

\[\Psi_{\alpha}(M^{(l)}||N)-\Psi_{\alpha}(M||N)=\int_{\mathcal{C}^{(l )}}\left(\sum_{k=0}^{K}d^{(\mu_{k})}_{\hat{\bm{n}}_{+}}\Psi_{\alpha}(\tilde{M}^ {(l)}||N)\right)ds\] \[=\int_{t=0}^{t=1}\left(\sum_{k=l}^{K}d^{(\mu_{k})}_{\hat{\bm{n}}_ {+}}\Psi_{\alpha}(\tilde{M}^{(l)}(t)||N)\right)\sqrt{2}(\bm{\sigma}^{T}\hat{ \bm{e}}_{i})(\hat{\bm{e}}_{i}^{T}\delta\mu_{l})dt\geq 0:\]

By construction, the directional derivatives \(d^{(\mu_{k})}_{\hat{\bm{n}}_{+}}\Psi_{\alpha}(\tilde{M}^{(k)}(t)||N)\) are nonnegative. Likewise, the determinant term \(\sqrt{2}(\bm{\sigma}^{T}\hat{\bm{e}}_{i})(\hat{\bm{e}}_{i}^{T}\delta\mu_{l})\) is nonnegative by the simplex condition together with feasibility of \((M,N)\).

If the separability condition is fulfilled for \(\hat{\bm{n}}_{-}\) instead, we can repeat the argument with indices \(i\leftrightarrow j\) swapped and index \(m\) instead of \(l\).

An analogous argument holds if \(\exists_{t}\in\{1,\ldots,\mathcal{K}\},\,\pi\in\{1,\ldots,\mathcal{K}\},\,i \in\{1,\ldots D\},\,j\in\{1,\ldots,D\},\,i\neq j\colon\,(\delta\nu_{\iota}^{T} \hat{\bm{e}}_{i})\neq 0\wedge(\delta\nu_{\pi}^{T}\hat{\bm{e}}_{j})\}\neq 0\). 

Theorem O.10 immediately follows from Lemma O.13 by induction. 

### Reduction of the divergence to a univariate integral

In the previous subsections, we showed that \(\Psi_{\alpha}(N||M)\) is maximized by collinear and equidistant sets of means. This allows to evaluate \(\Psi_{\alpha}(N||M)\) through a one-dimensional integral via marginalization. Recall (Theorem O.9). Note furthermore that in both cases, constraining the first element of either \(N\) or \(M\) into the origin incurs no generality loss due to the translation invariance of \(\Psi_{\alpha}\).

Being able to constrain the problem to a single dimension, we now show

**Lemma O.14**.: _In the collinear case, we may evaluate \(\Psi_{\alpha}\) as the one-dimensional divergence \(\Psi_{\alpha}^{\text{ID}}\) between two mixtures \(\sum_{k=0}^{K}w_{k}\mathcal{N}(r_{k},\sigma^{2}),\,\sum_{\kappa=0}^{\mathcal{ K}}\omega_{\kappa}\mathcal{N}(-\rho_{\kappa},\sigma^{2})\) of univariate Gaussians centered around the means \(r_{k}\) and \(-\rho_{\kappa}\),_

\[\Psi_{\alpha}\left(\sum_{\kappa=0}^{\mathcal{K}}\omega_{\kappa}\mathcal{N}(- \rho_{\kappa}\hat{\bm{e}},\sigma^{2}\bm{I})||\sum_{k=0}^{K}w_{k}\mathcal{N}(r _{k}\hat{\bm{e}},\sigma^{2}\bm{I})\right)=\Psi_{\alpha}^{\text{ID}}\left(\sum _{\kappa=0}^{\mathcal{K}}\omega_{\kappa}\mathcal{N}(-\rho_{\kappa},\sigma^{2} )||\sum_{k=0}^{K}w_{k}\mathcal{N}(r_{k},\sigma^{2})\right).\]

Proof.: Without loss of generality, we may assume \(\hat{\bm{e}}\) to be the indicator vector of the first component due to rotation invariance of \(\Psi_{\alpha}\). Using bracketed superscripts \((0)\) to indicate the first vector component, and \((1:)\) to indicate the rest, we can use Fubini's theorem to write

\[\Psi_{\alpha}\left(\sum_{\kappa=0}^{\mathcal{K}}\omega_{\kappa} \mathcal{N}(-\rho_{\kappa}\hat{\bm{e}},\sigma^{2}\bm{I})||\sum_{k=0}^{K}w_{k} \mathcal{N}(r_{k}\hat{\bm{e}},\sigma^{2}\bm{I})\right)\] \[=\ \int_{\mathbb{R}^{D}}\left[\sum_{\kappa=0}^{\mathcal{K}}\omega_{ \kappa}\mathcal{N}(\bm{x}\mid-\rho_{\kappa}\hat{\bm{e}},\sigma^{2}\bm{I}) \right]^{\alpha}\left[\sum_{k=0}^{K}w_{k}\mathcal{N}(\bm{x}\mid r_{k}\hat{\bm{e }},\sigma^{2}\bm{I})\right]^{(1-\alpha)}dx^{(0)}d\bm{x}^{(1:)}\] \[=\ \int_{\mathbb{R}^{D-1}}\mathcal{N}(\bm{x}^{(1:)}\mid 0,\sigma^{2} \bm{I})\,d\bm{x}^{(1:)}\int_{\mathbb{R}}\left[\sum_{\kappa=0}^{\mathcal{K}} \omega_{\kappa}\mathcal{N}(x^{(0)}\mid-\rho_{\kappa},\sigma^{2})\right]^{\alpha }\left[\sum_{k=0}^{K}w_{k}\mathcal{N}(x^{(0)}\mid r_{k},\sigma^{2})\right]^{( 1-\alpha)}dx^{(0)}\] \[=\ 1\cdot\Psi_{\alpha}^{\text{ID}}\left(\sum_{\kappa=0}^{\mathcal{ K}}\omega_{\kappa}\mathcal{N}(-\rho_{\kappa},\sigma^{2})||\sum_{k=0}^{K}w_{k} \mathcal{N}(r_{k},\sigma^{2})\right).\]

The proof is analogous for the Laplacian mechanism.

### Randomized Response Proofs

The following two results (first for the Renyi divergence, second for the hockey stick divergence) show that our (group) privacy guarantees for randomized response are given by the maximum of two terms that can be evaluated in constant time.

**Theorem O.15**.: \[\operatorname*{arg\,max}_{\boldsymbol{\tau}\in[0,1]^{(K_{+}+1)+(K_{-}+1)}}\sum_ {z=0}^{1}\left[\left(\sum_{i=1}^{K_{+}+1}w_{i}^{(1)}(1-|z-\tau_{i}^{(1)}|) \right)^{\alpha}\cdot\left(\sum_{j=1}^{K_{-}+1}w_{j}^{(2)}(1-|z-\tau_{j}^{(2) }|)\right)^{1-\alpha}\right]\]

_subject to_

\[\tau_{i}^{(1)}\in\{\theta,1-\theta\},\forall i,\] \[\tau_{j}^{(1)}\in\{\theta,1-\theta\},\forall j,\] \[\tau_{1}^{(1)}=\tau_{1}^{(2)},\]

_with_

\[w_{i}^{(1)}=\operatorname*{Binomial}(i-1\mid K_{-},r),\] \[w_{j}^{(2)}=\operatorname*{Binomial}(j-1\mid K_{+},r),\]

_is given by_

\[\operatorname*{arg\,max}\left\{\Psi\left(m_{\boldsymbol{\hat{\tau}}^{(1)}} \|m_{\boldsymbol{\hat{\tau}}^{(2)}}\right),\Psi\left(m_{\boldsymbol{\hat{\tau} }^{(1)}}\|m_{\boldsymbol{\hat{\tau}}^{(2)}}\right)\right\},\]

_where_

\[\tilde{\tau}_{1}^{(1)}=\tilde{\tau}_{1}^{(2)}=:\tau\in\{\theta,1- \theta\}\left(symmetric\right),\] \[\tilde{\tau}_{2,\ldots,(K_{+}+1)}^{(1)}=1-\tau,\] \[\tilde{\tau}_{2,\ldots,(K_{-}+1)}^{(2)}=\tau\]

_and_

\[\hat{\tau}_{1}^{(1)}=\hat{\tau}_{1}^{(2)}=:\tau\in\{\theta,1- \theta\}\left(symmetric\right),\] \[\hat{\tau}_{2,\ldots,(K_{+}+1)}^{(1)}=\tau,\] \[\hat{\tau}_{2,\ldots,(K_{-}+1)}^{(2)}=1-\tau\]

Proof.: The strategy is to show that

1. Any mixture pair \((m_{\boldsymbol{\tau}^{(1)}},m_{\boldsymbol{\tau}^{(2)}})\) where \(m_{\boldsymbol{\tau}^{(1)}}\) has greater mass in \((1-\tau)\), i.e., \(\sum_{i:\tau_{i}^{(1)}=1-\tau}w_{i}^{(1)}\geq\sum_{j:\tau_{j}^{(2)}=1-\tau}w_{ j}^{(2)}\), is dominated by \((m_{\boldsymbol{\hat{\tau}}^{(1)}},m_{\boldsymbol{\hat{\tau}}^{(2)}})\)
2. Any mixture pair \((m_{\boldsymbol{\tau}^{(1)}},m_{\boldsymbol{\tau}^{(2)}})\) where \(m_{\boldsymbol{\tau}^{(2)}}\) has greater mass in \((1-\tau)\), i.e., \(\sum_{i:\tau_{i}^{(1)}=1-\tau}w_{i}^{(1)}\leq\sum_{j:\tau_{j}^{(2)}=1-\tau}w_{ j}^{(2)}\), is dominated by \((m_{\boldsymbol{\hat{\tau}}^{(1)}},m_{\boldsymbol{\hat{\tau}}^{(2)}})\)

Notation:

\[\sum_{z=0}^{1}\left[\left(\sum_{i=1}^{K_{+}+1}w_{i}^{(1)}(1-|z-\tau_{i}^{(1)}| )\right)^{\alpha}\cdot\left(\sum_{j=1}^{K_{-}+1}w_{j}^{(2)}(1-|z-\tau_{j}^{(2) }|)\right)^{1-\alpha}\right]=\sum_{z=0}^{1}f(x(z))g(y(z)),\]

where \(f\) strictly convex, increasing, and \(g\) strictly convex, decreasing.

Trivial case: \(\theta=\frac{1}{2}\), thus assume \(\theta\neq\frac{1}{2}\) for the rest of the proof.

Proof of (a):

Given: Pair of mixtures \((m_{\boldsymbol{\tau}^{(1)}},m_{\boldsymbol{\tau}^{(2)}})\) such that \(w_{1-\tau}^{(1)}\,:=\,\sum_{i:\tau_{i}^{(1)}=1-\tau}w_{i}^{(1)}\,\geq\,\sum_{j: \tau_{j}^{(2)}=1-\tau}w_{j}^{(2)}\,=:\)

\(w_{1-\tau}^{(2)}\).

Further define \(w_{\tau}^{(1)}=1-w_{1-\tau}^{(1)},w_{\tau}^{(2)}=1-w_{1-\tau}^{(2)}\).

Define a _new_ mixture via

\[\tau_{1}^{\prime(1)}=\tau_{1}^{\prime(2)} =\tau,\] \[\tau_{2\ldots,K_{\tau}+1}^{\prime(1)} =1-\tau,\] \[\tau_{i}^{\prime(2)} =\tau_{i}^{(2)},\forall i.\]

Assume w.l.o.g. that \(\tau_{0}=\max\{\theta,1-\theta\}\), else swap roles of \(0\leftrightarrow 1\) in the argument.

Decompose \(x\): \(x(z)=x_{0}(z)+\delta x(z)\), where

\[x_{0}(z) =w_{1-\tau}^{(1)}(1-|z-(1-\tau)|)+w_{1}^{(1)}(1-|z-\tau|),\]

i.e., "part that stays fixed"; see Fig. 26,

\(\delta x(z)=(w_{\tau}^{(1)}-w_{1}^{(1)})(1-|z-\tau|)\), i.e., "part that relocates"; see Fig. 26.

Similarly,

\[x^{\prime}(z)=x_{0}^{\prime}(z)+\delta x^{\prime}(z),\]

and by construction,

\[x_{0}^{\prime}(z) =x_{0}(z),\] \[\delta x^{\prime}(z) =\delta x(1-z)\;(\star)\]

We can assume w.l.o.g. \(\tau_{0}=\max\{\theta,1-\theta\}\implies\delta x(1)>\delta x(0)\;(\star\star)\) (else, swap \(1\leftrightarrow 0\))

Then, \(\theta\neq\frac{1}{2}\), hence we can assume \(w_{\tau}^{(1)}>w_{1}^{(1)}\) (else, \(m^{\prime}\equiv m\) in the first place).

\(\Psi_{\alpha}(m_{\boldsymbol{\tau}^{(1)}},m_{\boldsymbol{\tau}^{\prime(2)} })-\Psi_{\alpha}(m_{\boldsymbol{\tau}^{(1)}},m_{\boldsymbol{\tau}^{(2)}})\)

\(\stackrel{{(\star)}}{{=}}\left[f\left(x_{0}(0)+\delta x(1) \right)-f\left(x_{0}(0)+\delta x(0)\right)\right]g(y(0))-\left[f\left(x_{0}(1) +\delta x(1)\right)-f\left(x_{0}(1)+\delta x(0)\right)\right]g(y(1))\)

Then, using that \(f\) is strictly convex, we have

\(\stackrel{{(\star\star)}}{{>}}f^{\prime}(x_{0}(0)+\delta x(0))g( y(0))(\delta x(1)-\delta x(0))-f^{\prime}(x_{0}(1)+\delta x(0))g(y(1))(\delta x(1)- \delta x(0))\)

Where \(f^{\prime}(x)=x^{\alpha-1}\), and thus \(f^{\prime}(x)g(y)=(\alpha-1)\left(\frac{x}{y}\right)^{\alpha-1}\)

\[=(\alpha-1)\left[\left(\frac{x_{0}(0)+\delta x(0)}{y(0)}\right)^{ \alpha-1}-\left(\frac{x_{0}(1)+\delta x(0)}{y(1)}\right)^{\alpha-1}\right]( \delta x(1)-\delta x(0))\]

\[=(\alpha-1)\left[\left(\frac{(1-w_{1-\tau}^{(1)}(1-\tau)+w_{1-\tau}^{(1)}\tau }{(1-w_{1-\tau}^{(2)})(1-\tau)+w_{1-\tau}^{(2)}\tau}\right)^{\alpha-1}-\left( \frac{w_{1}^{(1)}\tau+(1-w_{1}^{(1)})(1-\tau)}{(1-w_{1-\tau}^{(2)})\tau+w_{1- \tau}^{(2)}(1-\tau)}\right)^{\alpha-1}\right](\delta x(1)-\delta x(0))\]

Figure 26: Visualization of \(\delta x\) and \(\delta x^{\prime}\). Color legend: The color “blue” refers to terms with superscript \((1)\), while “red” refers to those with \((2)\).

The last inequality follows from the following facts. Since we assume \(w_{\tau}^{(1)}>w_{1}^{(1)}\) (else, \(m\equiv m^{\prime}\) in the first place), we can bound the second term by:

\[\left(\frac{w_{1}^{(1)}\tau+(1-w_{1}^{(1)})(1-\tau)}{(1-w_{1-\tau}^{(2)})\tau+w_ {1-\tau}^{(2)}(1-\tau)}\right)^{\alpha-1}<\left(\frac{(1-w_{1-\tau}^{(1)})\tau+ w_{1-\tau}(1-\tau)}{(1-w_{1-\tau}^{(2)})\tau+w_{1-\tau}^{(1)}(1-\tau)}\leq 1 \right)^{\alpha-1}(i)\]

Moreover, we have that the first term is bounded by

\[\left(\frac{(1-w_{1-\tau}^{(1)})(1-\tau)+w_{1-\tau}^{(1)}\tau}{(1-w_{1-\tau}^{ (2)})(1-\tau)+w_{1-\tau}^{(2)}\tau}\right)^{\alpha-1}\geq 1(ii)\]

(strict \(>\) if \(w_{1-\tau}^{(1)}>w_{1-\tau}^{(2)}\)). The conclusion follows from \((i)\), \((ii)\), and since WLOG \(\tau>(1-\tau)\).

We have shown so far:

\[\Psi_{\alpha}(m_{\boldsymbol{\tau}^{\prime(1)}},m_{\boldsymbol{\tau}^{\prime (2)}})>\Psi_{\alpha}(m_{\boldsymbol{\tau}^{(1)}},m_{\boldsymbol{\tau}^{(2)}}).\]

To complete the proof of \((a)\), we show now that \(\Psi_{\alpha}(m_{\tilde{\tau}^{(1)}},m_{\tilde{\tau}^{(2)}})>\Psi_{\alpha}(m_{ \tau^{\prime(1)}},m_{\tau^{\prime(2)}})\).

_Idea_: Apply symmetric argument to means \(\tau_{(2)}\).

Since 1 = \(w_{\boldsymbol{\tau}}^{(1)}+w_{1-\tau}^{(1)}=w_{\tau}^{(2)}+w_{1-\boldsymbol{ \tau}}^{(2)}\), we have

\[w_{\tau^{\prime}}^{(2)}=w_{\tau}^{(2)}\geq w_{\tau}^{(1)}>w_{\tau^{\prime}}^{ (1)},\]

where \(w_{\tau^{\prime}}^{(1)},w_{\tau^{\prime}}^{(2)}\) are defined like \(w_{\tau}^{(1)},w_{\tau}^{(2)}\) but for new means \(\tau^{\prime}\).

Decompose \(y^{\prime}\): \(y^{\prime}(z)=y^{\prime}_{0}(z)+\delta y^{\prime}(z)\), where

\[y^{\prime}_{0}(z)=w_{\tau}^{(2)}(1-|z-\tau|),\text{i.e., ``part that stays fixed''; see Fig.~{}\ref{fig:y0}.},\]

\[\delta y^{\prime}(z)=w_{1-\tau}^{(2)}(1-|z-(1-\tau)|),\text{i.e., ``part that relocates''; see Fig.~{}\ref{fig:y0}.}\]

We can assume w.l.o.g. that \(\tau=\max\{\theta,1-\theta\}\), so \(\delta y^{\prime}(0)>\delta y^{\prime}(1)\).

\[\Psi_{\alpha}(m_{\boldsymbol{\tau}^{(1)}},m_{\boldsymbol{\tau}^{(2)}})-\Psi_ {\alpha}(m_{\boldsymbol{\tau}^{\prime(1)}},m_{\boldsymbol{\tau}^{\prime(2)}})\]

And using the strict convexity of \(f\) we have

\[>\left[f(x^{\prime}(1))g^{\prime}(y^{\prime}_{0}(1)+\delta y^{ \prime}(1))-f(x^{\prime}(0))g^{\prime}(y^{\prime}_{0}(0)+\delta y^{\prime}(1) )\right](\delta y^{\prime}(0)-\delta y^{\prime}(1))\] \[=(\alpha-1)\left[\left(\frac{x^{\prime}(0)}{y^{\prime}_{0}(0)+ \delta y^{\prime}(1)}\right)^{\alpha}-\left(\frac{x^{\prime}(1)}{y^{\prime}_{ 0}(1)+\delta y^{\prime}(1)}\right)^{\alpha}\right](\delta y^{\prime}(0)-\delta y ^{\prime}(1))\] \[=(\alpha-1)\left[\left(\frac{w_{\tau}^{(1)}(1-\tau)+(1-w_{\tau}^ {(1)})\tau}{1-\tau}\right)^{\alpha}-\left(\frac{w_{\tau}^{(1)}\tau+(1-w_{\tau} ^{(1)})(1-\tau)}{w_{\tau}^{(2)}\tau+(1-w_{\tau}^{(2)})(1-\tau)}\right)^{ \alpha}\right]\cdot(\delta y^{\prime}(0)-\delta y^{\prime}(1))\] \[>0.\]

Figure 27: Visualization of \(\delta y\) and \(\delta y^{\prime}\). The color “blue” refers to terms with superscript \((1)\), while “red” refers to those with \((2)\).

Proof of \((b)\) is fully analogous.

This proves the theorem statement, since condition (a) or (b) is fulfilled for any mixture pair, and hence any mixture pair is dominated by the argmax of the two mixture pairs stated in the theorem.

\(\square\)

**Theorem O.16**.: \[\operatorname*{arg\,max}_{\boldsymbol{\tau}\in[0,1]^{(K_{+}+1)+(K_{-}+1)}}\sum_ {z=0}^{1}\left[\left(\sum_{i=1}^{K_{+}+1}w_{i}^{(1)}(1-|z-\tau_{i}^{(1)}|) \right)-e^{\varepsilon}\left(\sum_{j=1}^{K_{-}+1}w_{j}^{(2)}(1-|z-\tau_{j}^{(2 )}|)\right)\right]\]

_subject to_

\[\tau_{i}^{(1)} \in\{\theta,1-\theta\},\forall i,\] \[\tau_{j}^{(1)} \in\{\theta,1-\theta\},\forall j,\] \[\tau_{1}^{(1)} =\tau_{1}^{(2)},\]

_with_

\[w_{i}^{(1)} =\operatorname{Binomial}(i-1\mid K_{-},r),\] \[w_{j}^{(2)} =\operatorname{Binomial}(j-1\mid K_{+},r),\]

_is given by_

\[\operatorname*{arg\,max}\left\{\Psi\left(m_{\hat{\boldsymbol{\tau}}^{(1)}} \|m_{\hat{\boldsymbol{\tau}}^{(2)}}\right),\Psi\left(m_{\hat{\boldsymbol{\tau} }^{(1)}}\|m_{\hat{\boldsymbol{\tau}}^{(2)}}\right)\right\},\]

_where_

\[\hat{\tau}_{1}^{(1)}=\hat{\tau}_{1}^{(2)} =\cdot\tau\in\{\theta,1-\theta\}\,(symmetric),\] \[\hat{\tau}_{2,\ldots,(K_{+}+1)}^{(1)} =1-\tau,\] \[\hat{\tau}_{2,\ldots,(K_{+}+1)}^{(2)} =\tau\]

_and_

\[\hat{\tau}_{1}^{(1)}=\hat{\tau}_{1}^{(2)} =\cdot\tau\in\{\theta,1-\theta\}\,(symmetric),\] \[\hat{\tau}_{2,\ldots,(K_{+}+1)}^{(1)} =\tau,\] \[\hat{\tau}_{2,\ldots,(K_{-}+1)}^{(2)} =1-\tau\]

_Proof sketch._

The strategy is to show that

* Any mixture pair \((m_{\boldsymbol{\tau}^{(1)}},m_{\boldsymbol{\tau}^{(2)}})\) where \(m_{\boldsymbol{\tau}^{(1)}}\) has greater mass in \((1-\tau)\), i.e., \(\sum_{i:\tau_{i}^{(1)}=1-\tau}w_{i}^{(1)}\geq\sum_{j:\tau_{j}^{(2)}=1-\tau}w_{ j}^{(2)}\), is dominated by \((m_{\hat{\boldsymbol{\tau}}^{(1)}},m_{\hat{\boldsymbol{\tau}}^{(2)}})\)
* Any mixture pair \((m_{\boldsymbol{\tau}^{(1)}},m_{\boldsymbol{\tau}^{(2)}})\) where \(m_{\boldsymbol{\tau}^{(2)}}\) has greater mass in \((1-\tau)\), i.e., \(\sum_{i:\tau_{i}^{(1)}=1-\tau}w_{i}^{(1)}\leq\sum_{j:\tau_{j}^{(2)}=1-\tau}w_{ j}^{(2)}\), is dominated by \((m_{\hat{\boldsymbol{\tau}}^{(1)}},m_{\hat{\boldsymbol{\tau}}^{(2)}})\)

Notation:

\[\sum_{z=0}^{1}\left[\left(\sum_{i=1}^{K_{+}+1}w_{i}^{(1)}(1-|z-\tau_{i}^{(1)}|) \right)-e^{\varepsilon}\left(\sum_{j=1}^{K_{-}+1}w_{j}^{(2)}(1-|z-\tau_{j}^{( 2)}|)\right)\right]=\sum_{z=0}^{1}f(x(z)-e^{\varepsilon}y(z)),\]

where \(f=[\,\cdot\,]_{+}\) is convex and differentiable anywhere except at 0.

Trivial case: \(\theta=\frac{1}{2}\), thus assume \(\theta\neq\frac{1}{2}\) for the rest of the proof.

Proof of (a):

Given: Pair of mixtures \((m_{\boldsymbol{\tau}^{(1)}},m_{\boldsymbol{\tau}^{(2)}})\) such that \(w_{1-\tau}^{(1)}:=\sum_{i:\tau_{i}^{(1)}=1-\tau}w_{i}^{(1)}\geq\sum_{j:\tau_{j}^{ (2)}=1-\tau}w_{j}^{(2)}=:\)

\(w_{1-\tau}^{(2)}\).

Further define \(w_{\tau}^{(1)}=1-w_{1-\tau}^{(1)},w_{\tau}^{(2)}=1-w_{1-\tau}^{(2)}\).

Define a _new_ mixture via

\[\tau_{1}^{\prime(1)}=\tau_{1}^{\prime(2)} =\tau,\] \[\tau_{2\dots,K_{\tau}+1}^{\prime(1)} =1-\tau,\] \[\tau_{i}^{\prime(2)} =\tau_{i}^{(2)},\forall i.\]

Assume w.l.o.g. that \(\tau_{0}=\max\{\theta,1-\theta\}\), else swap roles of \(0\leftrightarrow 1\) in the argument.

Decompose \(x\): \(x(z)=x_{0}(z)+\delta x(z)\), where

\[x_{0}(z)=w_{1-\tau}^{(1)}(1-|z-(1-\tau)|)+w_{1}^{(1)}(1-|z-\tau|),\text{i.e., ``part that stays fixed''; see Fig. 26,}\]

\[\delta x(z)=(w_{\tau}^{(1)}-w_{1}^{(1)})(1-|z-\tau|),\text{i.e., ``part that relocates''; see Fig. 26.}\]

Similarly,

\[x^{\prime}(z)=x_{0}^{\prime}(z)+\delta x^{\prime}(z),\]

and by construction,

\[x_{0}^{\prime}(z)=x_{0}(z),\]

\[\delta x^{\prime}(z)=\delta x(1-z)\;(\star)\]

We can assume w.l.o.g. \(\tau_{0}=\max\{\theta,1-\theta\}\implies\delta x(1)>\delta x(0)\,(\star\star)\) (else, swap \(1\leftrightarrow 0\))

Then, \(\theta\neq\frac{1}{2}\), hence we can assume \(w_{\tau}^{(1)}>w_{1}^{(1)}\) (else, \(m^{\prime}\equiv m\) in the first place).

\[\Psi_{\alpha}(m_{\boldsymbol{\tau}^{(1)}},m_{\boldsymbol{\tau}^{(2)}})-\Psi _{\alpha}(m_{\boldsymbol{\tau}^{(1)}},m_{\boldsymbol{\tau}^{(2)}})\]

\[\begin{array}{l}\langle\stackrel{{\star}}{{\to}}[f\,(x_{0}(0) +\delta x(1)-e^{\varepsilon}y(0))-f\,(x_{0}(0)+\delta x(0)-e^{\varepsilon}y(0) )]\\ -\left[f\,(x_{0}(1)+\delta x(1)-e^{\varepsilon}y(1))-f\,(x_{0}(1)+\delta x(0)- e^{\varepsilon}y(1))\right]g(y(1))\end{array}\]

Assume for now that \(x_{0}(0)+\delta x(0)-e^{\varepsilon}y(0)\neq 0\neq x_{0}(1)+\delta x(0)-e^{ \varepsilon}y(1)\).

\[\begin{array}{l}\langle\stackrel{{\star}}{{\to}}\rangle\left( f^{\prime}(x_{0}(0)+\delta x(0)-e^{\varepsilon}y(0))-f^{\prime}(x_{0}(1)+\delta x(0)-e^{ \varepsilon}y(1))\right)(\delta x(1)-\delta x(0))\\ =f^{\prime}\left[\left((1-w_{1-\tau}^{(1)})(1-\tau)+w_{1-\tau}^{(1)}\tau\right) -e^{\varepsilon}\left((1-w_{1-\tau}^{(2)})(1-\tau)+w_{1-\tau}^{(2)}\tau \right)\right](\delta x(1)-\delta x(0))\\ -f^{\prime}\left[\left(w_{1}^{(1)}\tau+(1-w_{1}^{(1)})(1-\tau)\right)-e^{ \varepsilon}\left((1-w_{1-\tau}^{(2)})\tau+w_{1-\tau}^{(2)}(1-\tau)\right) \right](\delta x(1)-\delta x(0))\\ \geq f^{\prime}\left[\left((1-w_{1-\tau}^{(1)})(1-\tau)+w_{1-\tau}^{(1)}\tau \right)-e^{\varepsilon}\left((1-w_{1-\tau}^{(2)})(1-\tau)+w_{1-\tau}^{(2)} \tau\right)\right](\delta x(1)-\delta x(0))\\ -f^{\prime}\left[\left((1-w_{1-\tau}^{(1)})\tau+w_{1-\tau}(1-\tau)\right)-e^{ \varepsilon}\left((1-w_{1-\tau}^{(2)})\tau+w_{1-\tau}^{(2)}(1-\tau)\right) \right](\delta x(1)-\delta x(0))\\ \geq 0.\end{array}\]

The second-to-last inequality follows from \(w_{\tau}^{(1)}>w_{1}^{(1)}\) (else, \(m\equiv m^{\prime}\) in the first place). The last inequality follows from \(w_{1-\tau}^{(1)}>w_{1-\tau}^{(2)}\) and the prior assumption that, WLOG, \(\tau>1-\tau\).

If \(x_{0}(0)+\delta x(0)-e^{\varepsilon}y(0)>x_{0}(1)+\delta x(0)-e^{\varepsilon}y(1)\) and \(x_{0}(1)+\delta x(0)-e^{\varepsilon}y(1)=0\), then we can arrive at the same conclusion by using any subderivative of \(f\) in \([0,1]\) at \(x_{0}(1)+\delta x(0)-e^{\varepsilon}y(1)\) instead.

We have shown so far:

\[\Psi_{\alpha}(m_{\boldsymbol{\tau}^{(1)}},m_{\boldsymbol{\tau}^{(2)}})\geq\Psi _{\alpha}(m_{\boldsymbol{\tau}^{(1)}},m_{\boldsymbol{\tau}^{(2)}}).\]

Continue in analogy to Theorem O.15.

Towards epoch-level subsampling analysis

Standard composition theorems assume that the outputs of composed mechanisms are conditionally independent, meaning each output only depends on the previous outputs and the dataset (see, e.g., the proof of Proposition 1 in [7] and the proof of Theorem 27 in [10]). For this reason, one typically considers subsampling schemes like Poisson subsampling or subsampling without replacement, which yield independent batches in each iteration.

However, there may be scenarios where batches are not independent, such as when creating batches by permuting a dataset and splitting it into equal sized chunks. Such correlated subsampling schemes are appealing because one can, for instance, limit privacy leakage by ensuring that any element appears in only one iteration per epoch when training a model. A downside of these corelated subsampling schemes is that they may brake the conditional independence assumption of standard compositions theorems.

There already exist approaches to analyzing compositions of correlated mechanisms, such as probabilistically upper-bounding a mechanism's privacy leakage with uncorellated mechanisms [75, 24]. Furthermore, permutation-based batching has already been discussed in the context of convergence guarantees for noisy SGD [38]. Nevertheless, this problem setting provides a great opportunity to demonstrate that the utility of our conditional optimal transport framework extends beyond the subsampling schemes that are typically discussed in amplification-by-subsampling literature.

### Problem setting

For this example, we consider a _non-adaptive_ composition of two mechanisms, where two batches are created by permuting a dataset and splitting it in half. We assume that our dataset space is composed of size \(2N\) subsets of some finite, discrete set \(\mathbb{A}\), i.e., \(\mathbb{X}\subseteq\{x\in\mathcal{P}(\mathbb{A})\mid|x|=2N\}\), and equipped with substitution relation \(\simeq_{\Delta}\). We further define a per-iteration batch space \(\mathbb{Y}\) that is composed of size \(N\) subsets, i.e., \(\mathbb{Y}=\{y\subseteq x\mid x\in\mathbb{X}\land|y|=N\}\), which is also equipped with substitution relation \(\simeq_{\Delta}\). Finally, we assume a base mechanism \(B:\mathbb{Y}\rightarrow\mathbb{Z}\) with conditional density \(b_{y}:\mathbb{Z}\rightarrow\mathbb{R}_{+}\) that maps from this batch space to some output space.

To apply our framework to epoch-level subsampling distributions, we define the composed batch space \(\mathbb{Y}\), which consists of equal-sized partitions of datasets in \(\mathbb{X}\), i.e.,

\[\hat{\mathbb{Y}}=\{(y_{1},y_{2})\in\mathbb{Y}_{\mathrm{orig}}^{2}\mid\exists x \in\mathbb{X}:y_{1}\mathbin{\dot{\cup}}y_{2}=x\}.\] (37)

On this composed batch space, we can now define our epoch-level subsampling scheme:

**Definition P.1**.: The permute-and-partition subsampling scheme is the subsampling scheme \(S:\mathbb{X}\rightarrow\hat{\mathbb{Y}}\) with

\[s_{x}((y_{1},y_{2}))=\begin{cases}\binom{2N}{N}^{-1}&\text{if }y_{1}\cup y_{2}=x \\ 0&\text{otherwise }.\end{cases}\]

This definition follows from the fact that permuting-and-partitioning is equivalent to sampling a batch of size \(N\) without replacement for the first batch and using the remaining \(N\) elements for the second batch. We further consider the _non-adaptively_ composed base mechanism \(\hat{B}:\hat{\mathbb{Y}}\rightarrow\mathbb{Z}^{2}\) with \(\hat{B}_{(y_{1},y_{2})}=(B_{y_{1}},B_{y_{2}})\) and resultant joint density \(\hat{b}_{(y_{1},y_{2})}=b_{y_{1}}\cdot b_{y_{2}}\).

### Optimal transport without conditioning

As a baseline, we use Theorem 3.3, i.e., optimal transport without conditioning, to provide a Renyi-DP bound for the composed, subsampled mechanism. This guarantee captures the intuition that by permuting-and-partitioning, we only leak an elements' private information once. Again, note we prove this result for _non-adaptive_ composition.

**Theorem P.2**.: _Assume a dataset space \(\mathbb{X}\subseteq\{x\in\mathcal{P}(\mathbb{A})\mid|x|=2N\}\), batch space \(\mathbb{Y}=\{y\subseteq x\mid x\in\mathbb{X}\land|y|=N\}\), and a base mechanism \(B:\mathbb{Y}\rightarrow\mathbb{Z}\) that is \((\alpha,\varepsilon)\)-Renyi-DP w.r.t. single-element substitution relation \(\simeq_{\Delta,\mathbb{Y}}\). Let \(S:\mathbb{X}\rightarrow\hat{\mathbb{Y}}\) be the permute-and-partition subsampling scheme defined in Definition P.1. Let \(\hat{B}\) be the composed base mechanism with \(\hat{B}_{(y_{1},y_{2})}=(B_{y_{1}},B_{y_{2}})\). Then the subsampled, composed mechanism \(\hat{M}=\hat{B}\circ S\) is also \((\alpha,\varepsilon)\)-DP w.r.t. single-element substitution relation \(\simeq_{\Delta,\mathbb{X}}\)._Proof.: Consider an arbitrary pair of datasets \(x\simeq_{\Delta,\mathbb{X}}\). By definition of the substitution relation, there must be some \(a\in x\) and some \(a^{\prime}\in x^{\prime}\) such that \(x^{\prime}=x\setminus\{a\}\cup\{a^{\prime}\}\).

We can now define the following simple coupling between \(S_{x}\) and \(S_{x^{\prime}}\):

\[\gamma((y_{1}^{(1)},y_{2}^{(1)}),(y_{1}^{(2)},y_{2}^{(2)}))=\begin{cases} \binom{2N}{N}^{-1}&\text{if }a\in y_{1}^{(1)}\wedge y_{1}^{(2)}=y_{1}^{(1)} \setminus\{a\}\cup\{a^{\prime}\}\wedge y_{2}^{(1)}=y_{2}^{(2)}\\ \binom{2N}{N}^{-1}&\text{if }a\in y_{2}^{(1)}\wedge y_{2}^{(2)}=y_{2}^{(1)} \setminus\{a\}\cup\{a^{\prime}\}\wedge y_{1}^{(1)}=y_{1}^{(2)}\\ 0&\text{otherwise}.\end{cases}\]

This coupling generates a pair of batch sequences by first applying permute-and-partition to original dataset \(x\) and then replacing \(a\) with \(a^{\prime}\) in the one batch it appears in. Because we only couple pairs that are identical in one batch and differ by a substitution in the other batch, we have per Theorem 3.3

\[\Psi(\hat{m}_{x}||\hat{m}_{x^{\prime}})\leq\max_{(y_{1}^{(1)},y_{2}^{(1)}),(y _{1}^{(2)},y_{2}^{(2)})\in\mathbb{Y}^{2}}\Psi(b_{y_{1}^{(1)}}\cdot b_{y_{2}^{( 1)}}||b_{y_{1}^{(2)}}\cdot b_{y_{2}^{(2)}})\]

subject to \((y_{1}^{(1)}\simeq_{\Delta,\mathbb{Y}}y_{1}^{(2)})\wedge(y_{2}^{(1)}=y_{2}^{( 2)})\vee(y_{2}^{(1)}\simeq_{\Delta,\mathbb{Y}}y_{2}^{(2)})\wedge(y_{1}^{(1)}=y _{1}^{(2)})\). Either way, one of the factors is cancelled out when computing the likelihood ratio in the definition of Renyi-divergence (see Eq. (7)). We thus have

\[\Psi(\hat{m}_{x}||\hat{m}_{x^{\prime}})\leq\max_{y_{1}^{(1)},y_{1}^{(2)}\in \mathbb{Y}^{2}}\Psi(b_{y_{1}^{(1)}}||b_{y_{1}^{(2)}})\]

subject to \(y_{1}^{(1)}\simeq_{\Delta,\mathbb{Y}}y_{1}^{(2)}\). The result then follows from the definition of Renyi-DP (see Definition 2.2). 

### Optimal transport with conditioning

Next, we use Theorem 3.4, i.e., optimal transport between conditional subsampling distributions, to derive a stronger guarantee. Again, note we prove this result for _non-adaptive_ composition. Note that this result is similar in spirit to amplification by shuffling [76, 75], but considers central differential privacy of mechanisms operating on batches, instead of locally differentially private mechanisms that operate on individual elements. Again, note that we do not claim to be the first to consider the permute-and-partition scheme for composed mechanisms (see, e.g., [38]). It is just a nice showcase for the versatility of our framework in analyzing different subsampling schemes.

For the following result and proof we will use the following indexing convention: \(y_{i,k}^{(1)}\) is a batch associated with the first mixture \(\hat{m}_{x}\), event \(A_{i}\), and the \(k\)th iteration. Similarly, \(y_{j,k}^{(2)}\) is a batch associated with the second mixture \(\hat{m}_{x^{\prime}}\), event \(E_{j}\), and the \(k\)th iteration.

**Theorem P.3**.: _Assume a dataset space \(\mathbb{X}\subseteq\{x\in\mathcal{P}(\mathbb{A})\mid|x|=2N\}\), and corresponding batch space \(\mathbb{Y}=\{y\subseteq x\mid x\in\mathbb{X}\wedge|y|=N\}\), equipped with single-element substitution relation \(\simeq_{\Delta,\mathbb{Y}}\). Let \(S:\mathbb{X}\rightarrow\hat{\mathbb{Y}}\) be the permute-and-partition subsampling scheme defined in Definition P.1, and let \(B:\mathbb{Y}\rightarrow\mathbb{Z}\) be some base mechanism. Let \(\hat{B}\) be the composed base mechanism with \(\hat{B}_{(y_{1},y_{2})}=(B_{y_{1}},B_{y_{2}})\), and \(\hat{M}=\hat{B}\circ S\) be the subsampled, coposed mechanism. Then, for all \(x\simeq_{\Delta,\mathbb{X}}x^{\prime}\),_

\[\Psi_{\alpha}(\hat{m}_{x}||\hat{m}_{x^{\prime}})\] \[\leq\max_{y_{1,1}^{(1)},y_{1,2}^{(2)},y_{1,1}^{(2)}\atop y_{1,1 }^{(2)}}\Psi_{\alpha}\left(b_{y_{1,1}^{(1)}}\cdot b_{y_{1,2}^{(1)}}\cdot\frac{ 1}{2}+b_{y_{1,2}^{(1)}}\cdot b_{y_{1,1}^{(1)}}\cdot\frac{1}{2}||b_{y_{1,1}^{(2 )}}\cdot b_{y_{1,2}^{(1)}}\cdot\frac{1}{2}+b_{y_{1,2}^{(1)}}\cdot b_{y_{1,1}^{ (2)}}\cdot\frac{1}{2}\right)\]

_subject to \(y_{1,1}^{(1)}\simeq_{\Delta,\mathbb{Y}}y_{1,1}^{(2)}\)._

Proof.: By definition of the substitution relation, there must be some \(a\in x\) and some \(a^{\prime}\in x^{\prime}\) such that \(x^{\prime}=x\setminus\{a\}\cup\{a^{\prime}\}\).

For the original subsampling scheme \(S_{x}\) we let \(A_{1}\) be the event that \(a\) appears in the first batch and \(A_{2}\) be the event that \(a\) appears in the second batch. For the modified subsampling scheme \(S_{x^{\prime}}\) we let \(E_{1}\) be the event that \(a^{\prime}\) appears in the first batch and \(E_{2}\) be the event that \(a^{\prime}\) appears in the second batch. We have \(P_{S_{x}}(A_{1})=P_{S_{x}}(A_{2})=P_{S_{x}^{\prime}}(E_{1})=P_{S_{x}^{\prime}} (E_{2})=\frac{1}{2}\).

One can sample from the distributions conditioned on \(A_{1}\) or \(E_{1}\) by first sampling uniformly at random \(N\) elements from the \(2\cdot N-1\) elements that are _not_\(a\) or \(a^{\prime}\), and then using the remaining elements as the first batch. Similarly, one can sample from the distributions conditioned on \(A_{2}\) or \(E_{2}\) by first sampling uniformly at random \(N\) elements from the \(2\cdot N-1\) elements that are _not_\(a\) or \(a^{\prime}\), and then using the remaining elements as the second batch.

We thus have

\[s_{x}(y_{1}^{(1)}\mid A_{1})=\begin{cases}\binom{2N-1}{N}^{-1}&\text{if }y_{1}^{(1)}\in A _{1}\\ 0&\text{otherwise}\end{cases}\qquad s_{x}(y_{2}^{(1)}\mid A_{2})=\begin{cases} \binom{2N-1}{N}^{-1}&\text{if }y_{1}^{(1)}\in A_{2}\\ 0&\text{otherwise}\end{cases}\]

and

\[s_{x^{\prime}}(y_{1}^{(2)}\mid E_{1})=\begin{cases}\binom{2N-1}{N}^{-1}&\text{ if }y_{1}^{(1)}\in E_{1}\\ 0&\text{otherwise}\end{cases}\qquad s_{x^{\prime}}(y_{2}^{(2)}\mid E_{2})= \begin{cases}\binom{2N-1}{N}^{-1}&\text{if }y_{1}^{(1)}\in E_{2}\\ 0&\text{otherwise}\end{cases}\]

Next, we can define a coupling via

\[\gamma(y_{1}^{(1)},y_{2}^{(1)},y_{1}^{(2)},y_{2}^{(2)})=\begin{cases}s_{x}(y_{ 1}^{(1)}\mid A_{1})&\text{if Condition P.4 is fulfilled}\\ 0&\text{otherwise}\end{cases}\]

**Condition P.4**.: Batch tuples \(y_{1}^{(1)}\in A_{1},y_{2}^{(1)}\in A_{2},y_{1}^{(2)}\in E_{1},y_{2}^{(2)}\in E _{2}\) fulfill Condition P.4 when \(y_{1,1}^{(1)}=y_{2,2}^{(1)}\), and \(y_{1,2}^{(1)}=y_{2,1}^{(1)}\), and \(y_{1,2}^{(1)}=y_{1,2}^{(2)}\), and \(y_{2,1}^{(1)}=y_{2,1}^{(2)}\), and \(y_{1,1}^{(2)}=y_{1,1}^{(2)}\setminus\{a\}\cup\{a^{\prime}\}\), and \(y_{2,2}^{(2)}=y_{2,2}^{(2)}\setminus\{a\}\cup\{a^{\prime}\}\).

In short: We sample uniformly at random a batch tuple \((y_{1,1}^{(1)},y_{1,2}^{(1)})\) from \(A_{1}\). We then generate a batch tuple from \(A_{2}\) by permuting the \(A_{1}\) tuple. We then generate a batch tuple from \(E_{2}\) by replacing \(a\) with \(a^{\prime}\) in the \(A_{2}\) tuple. We finally generate a batch tuple from \(E_{1}\) by permuting the \(E_{2}\) tuple.

Because for each possible value of a batch tuple there is only one combination of the other three batch tuples such that \(\gamma(\bm{y})>0\), and in this case \(\gamma(\bm{y})=\binom{2N-1}{N}^{-1}\), this is a valid coupling.

The result then follows from Theorem 3.4 and the constraints imposed by Condition P.4. 

Note that this result could be generalized to \(K\) iterations: We can upper-bound the subsampled, composed mechanism's divergence by adversarially choosing a \(K\)-fold partition of \(x\), constructing a mixture with \(K\) (not \(K!\)) components, with each component corresponding to \(a\) appearing in a specific batch, and finally constructing a modified mixture by replacing every occurence of \(a\) with \(a^{\prime}\).

### Experimental Evaluation

Finally, we can compare our epoch-level amplification guarantees obtained via optimal transport with and without conditioning. As an additional baseline, we use \(2\)-fold composition of subsampling without replacement with batch size \(2\). Note that, with the baseline, a modified element may appear in \(0\), \(1\) or \(2\) of the iterations. For the sake of this simple example, we consider output space \(\mathbb{Z}=\mathbb{R}\), and let base mechanism \(b\) be the Gaussian mechanism \(f+\mathcal{N}(0,\sigma)\) with \(f:\mathbb{Y}\rightarrow\{0,1\}\) and univariate standard deviation \(\sigma\in\{0.5,1.0,2.0,5.0\}\). By this construction we do not need to reason about \(f\)'s sensitivity in determining the worst-case mixture components.

We make the following observation: For small \(\sigma\), both epoch-level guarantees are almost identical and outperform the without replacement guarantee. With increasing \(\sigma\), subsampling without replacement outperforms Theorem P.2 for some ranges of \(\alpha\). Theorem P.2, which is derived via conditional optimal transport, however yields smaller \(\varepsilon\).

The main takeaway of this example should be that (a) there is a benefit to using conditional optimal transport to bound privacy in terms of mixtures and (b) conditional optimal transport can be used to reason about schemes other than the typically considered Poisson subsampling, subsampling without replacement, and subsampling with replacement.

## Appendix Q Broader impact

Our work contributes towards provably protecting users, and specifically groups of users, from the negative societal impact of privacy leakage. However, differential privacy may have negatively impact other aspects of trustworthy machine learning, such as fairness or robustness. Also, training for differentially private machine learning is often performed with \(1<\varepsilon\leq 10\). This is useful for relative comparisons between models, but does not impose any meaningful constraints on absolute privacy leakage (the probability of any event may increase by a factor of up to \(\approx 22000\)) and may thus be used to falsely advertise privacy.

Figure 28: Comparison of our epoch-level permute-and-partition guarantees with (Theorem P.3) and without (Theorem P.2) conditioning, as well as subsampling without replacement, for \(2\)-fold non-adaptive composition. The base mechanism is a Gaussian mechanism with \(f:\mathbb{Y}\rightarrow\{0,1\}\) and varying standard deviations \(\sigma\). With increasing \(\sigma\), Theorem P.3 and subsampling without replacement become more similar, while Theorem P.3 consistently yields stronger guarantees.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We verify the generality of our proposed framework by formally deriving various known and novel subsampling guarantees Appendices G to I and K to M. We verify the benefit of using mechanism-specific guarantees and analyzing group privacy and subsampling jointly through our experimental evaluation in Section 4. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss limitations in Section 3.5. Our main contribution is a framework for proving theoretic results, as such many of the points listed below do not apply. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Yes, we provide formal proofs for all theoretical results in Appendix E through Appendix P. All theorems and lemmata state the underlying assumptions. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We fully describe our experimental setup in Appendix C. We further provide an implementation with the supplementary material, which includes the needed environment, code, configuration files, and plotting scripts. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide an implementation that includes the needed environment, code, configuration files, and plotting scripts for reproducing our experimental results at https://cs.cit.tum.de/daml/group-amplification. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We fully describe our experimental setup in Appendix C. Note that these experiments only require numerical evaluation of different divergences, meaning there are no involved data splits or optimizers. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Our experiments only require numerical evaluation of different divergences. There are no typical sources of randomness like data splits or weight initializations that could be visualized with error bars. Guidelines: ** The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Yes, we report the type and number of CPUs, as well as the number of workers and allocated runtime Appendix C.2. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research in this paper conforms, in every respect, with the code of ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes]Justification: Yes, we discuss both positive and negative broader impact in Appendix Q, which we reference in our "Limitations, broader impact, and future work" section Section 3.5.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not propose any new data or models that could be released. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We list the libraries that we use and/or extend to conduct our numerical experiments in Appendix C.3, including version numbers and licenses. We cite the original papers where applicable. Guidelines:

* The answer NA means that the paper does not use existing assets.

* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.