# _Goa_: Explaining Graph Neural Networks via

Graph Output Attribution

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Understanding the decision-making process of Graph Neural Networks (GNNs) is crucial to their interpretability. Present methods for explaining GNNs typically rely on training auxiliary models, and may struggle with issues such as overfitting to noise, insufficient discriminability, and inconsistent explanations across data samples of the same class. This paper introduces Graph Output Attribution (GOAt), a novel method to attribute graph outputs to input graph features, creating GNN explanations that are faithful, discriminative, as well as stable across similar samples. By expanding the GNN as a sum of scalar products involving node features, edge features and activation patterns, we propose an efficient analytical method to compute contribution of each node or edge feature to each scalar product and aggregate the contributions from all scalar products in the expansion form to derive the importance of each node and edge. Through extensive experiments on synthetic and real data, we show that our method has consistently outperformed various state-of-the-art GNN explainers in terms of fidelity, discriminability, and stability.

## 1 Introduction

Graph Neural Networks (GNNs) have demonstrated notable success in learning representations from graph-structured data in various fields [14; 9; 30]. However, their black-box nature has driven the need for explainability, especially in sectors where transparency and accountability are essential, such as finance [28], healthcare [1], and security [18]. The ability to interpret GNNs can provide insights into the mechanisms underlying deep models and help establish trustworthy predictions.

Existing attempts to explain GNNs usually focus on local-level or global-level explainability. Local-level explainers [31; 17; 21; 27; 10; 15; 23; 3] typically train a secondary model to identify the critical graph structures that best explain the behavior of a pretrained GNN for specific input instances. These methods are always optimized for ground-truth explanations or fidelity metrics, yet may not be able to generate consistent explanations for similar graph samples or produce accurate and human-intelligible explanations for class discrimination. Global-level explainers [2; 11] perform prototype learning or random walk on the explanation instances to extract the global explanations over a multitude of graph samples. However, their effectiveness rely heavily on the quality of local-level explanations.

In this paper, we introduce a computationally efficient local-level GNN explanation technique called **G**raph **O**utput **A**ttribution (_GOAt_) to overcome the limitations of existing methods. Unlike methods that rely on back-propagation with gradients [20; 4; 22; 8] and those relying on hyper-parameters or training complex black-box models [17; 15; 23; 3], our approach enables attribution of GNN output to input features, leveraging the repetitive sum-product structure in the forward pass of a GNN.

Given that the matrix multiplication in each GNN layer adheres to linearity properties and the activation functions operate element-wise, a GNN can be represented in an expansion form as asum of scalar product terms, involving input graph features, model parameters, as well as _activation patterns_ that indicate the activation levels of the scalar products. Based on the notion that all scalar variables \(X_{i}\) in a scalar product term \(g=cX_{1}X_{2}\ldots X_{N}\) contribute equally to \(g\), where \(c\) is a constant, we can attribute each product term to its corresponding factors and thus to input features, obtaining the importance of each node or edge feature in the input graph to GNN outputs. We present case studies that demonstrate the effectiveness of our analytical explanation method _GOAt_ on typical GNN variants, including GCN, GraphSAGE, and GIN.

Besides the fidelity metric, which is commonly used to assess the faithfulness of GNN explanations, we introduce two new metrics to evaluate the _discriminability_ and _stability_ of the explanation, which are under-investigated by prior literature. Discriminability refers to the ability of explanations to distinguish between classes, which is assessed by the difference between the mean explanation embeddings of different classes, while stability refers to the ability to generate consistent explanations across similar data instances, which is measured by the percentage of data samples covered by top-\(k\) explanations. Through comprehensive experiments based on on both synthetic and real-world datasets along with qualitative analysis, we show the outstanding performance of our proposed method, _GOAt_, in providing highly faithful, discriminative, and stable explanations for GNNs, as compared to a range of state-of-the-art methods.

## 2 Problem Formulation

**Graph Neural Networks** Let \(G=(\mathcal{V},\mathcal{E})\) be a graph, where \(\mathcal{V}=\{v_{1},v_{2},\ldots,v_{N}\}\) denotes the set of nodes and \(\mathcal{E}\subseteq\mathcal{V}\times\mathcal{V}\) denotes the set of edges. The node feature matrix of the graph is represented by \(X\in\mathbb{R}^{N\times d}\), and the adjacency matrix is represented by \(A\in\{0,1\}^{N\times N}\) such that \(A_{ij}=1\) if there exists an edge between nodes \(v_{i}\) and \(v_{j}\). The task of a GNN is to learn a function \(f(G)\), which maps the input graph \(G\) to a target output, such as node labels, graph labels, or edge labels. Formally speaking, for a given GNN, the hidden state \(h_{i}^{(l)}\) of node \(v_{i}\) at its layer \(l\) can be represented as:

\[h_{i}^{(l)}=\mathrm{COMBINE}^{(l)}\left\{h_{i}^{(l-1)},\mathrm{AGGREGATE}^{(l )}\left(\left\{h_{j}^{(l-1)},\forall v_{j}\in\mathcal{N}_{i}\right\}\right) \right\},\] (1)

where \(\mathcal{N}_{i}\) represents the set of neighbors of node \(v_{i}\) in the graph. \(\mathrm{COMBINE}^{(l)}(\cdot)\) is a COMBINE function such as concatenation [9], while \(\mathrm{AGGREGATE}^{(l)}(\cdot)\) are AGGREGATE functions with aggregators such as \(\mathrm{ADD}\). We focus on GNNs that adopt the non-linear activation function ReLU in COMBINE or AGGREGATE functions.

**Local-level GNN Explainability** Our goal is to generate a faithful explanation for a graph instance \(G=(\mathcal{V},\mathcal{E})\) by identifying a subset of edges \(S\subseteq\mathcal{E}\), given a GNN \(f(\cdot)\) pretrained on a set of graphs \(\mathcal{G}\). The term _faithful_ refers to the explanation's ability to perform well in not only _fidelity_[33] and _robustness_[3] metrics, but also _stability_ in identifying consistent patterns. We highlight edges instead of nodes as suggested by [7] that edges have more fine-grained information than nodes while giving human-understandable explanations like subgraphs.

## 3 Method

This section begins by presenting our fundamental definition of equal contribution in a product term and its application in an example of a toy graph neural network. Then, we mathematically present _GOAt_ method for explaining typical GNNs, followed by a case study on GCN [14]. Additional case studies of applying _GOAt_ to GraphSAGE [9] and GIN [30] are included in the Appendix.

### Definitions

Consider a function \(g(X_{1},\ldots,X_{M})\) of \(M\) variables \(\mathbf{X}=\{X_{1},\ldots,X_{M}\}\). If we let a pair of variables \((X_{i},X_{j})\) be set to \((X_{i},X_{j})=(x_{i},x_{j})\), we will obtain a manifold \(g_{X_{i}=x_{i},X_{j}=x_{j}}(\mathbf{X}\backslash\{X_{i},X_{j}\})\), which represents \(g(\cdot)\) when all variables excluding \(X_{i}\) and \(X_{j}\) can vary. Consider a base manifold \(g_{X_{i}=x^{\prime}_{i},X_{j}=x^{\prime}_{j}}\langle\mathbf{X}\backslash\{X_{i },X_{j}\}\rangle\). If we can obtain two identical manifolds by setting \((X_{i},X_{j})=(x_{i},x^{\prime}_{j})\) and \((X_{i},X_{j})=(x^{\prime}_{i},x_{j})\), it will indicate that changing \(X_{i}=x^{\prime}_{i}\) to \(X_{i}=x_{i}\) is equivalent to changing \(X_{j}=x^{\prime}_{j}\) to \(X_{j}=x_{j}\) with respect to the base manifold at \((X_{i},X_{j})=(x^{\prime}_{i},x^{\prime}_{j})\). For example, a function \(g(x,y,z)=2xy+x^{2}z\) has three variables \(x,y,z\), we consider taking \(g_{x=-1,y=-1}(z)=z+2\) as the base manifold. Since \(g_{x=1,y=-1}(z)=g_{x=-1,y=1}(z)=z-2\), we say that changing \(x=-1\) to \(x=1\) is equivalent to changing \(y=-1\) to \(y=1\) with respect to the base manifold at \((x,y)=(-1,-1)\).

**Definition 1** (**Equal Contribution**).: _Given a function \(g(\mathbf{X})\) where \(\mathbf{X}=\{X_{1},\ldots,X_{M}\}\) represents \(M\) variables, we say that variables \(X_{i}\) and \(X_{j}\) have equal contribution to function \(g\) at \((x_{i},x_{j})\) with respect to the base manifold at \((x^{\prime}_{i},x^{\prime}_{j})\) if and only if setting \(X_{i}=x_{i},X_{j}=x^{\prime}_{j}\) and setting \(X_{i}=x^{\prime}_{i},X_{j}=x_{j}\) yield the same manifold, i.e.,_

\[g_{X_{i}=x_{i},X_{j}=x^{\prime}_{j}}(\mathbf{X}\backslash\{X_{i},X_{j}\})=g_{ X_{i}=x^{\prime}_{i},X_{j}=x_{j}}(\mathbf{X}\backslash\{X_{i},X_{j}\})\]

_for any values of \(\mathbf{X}\) excluding \(X_{i}\) and \(X_{j}\)._

**Lemma 2** (**Equal Contribution in a product**).: _Given a function \(g(\mathbf{X})\) defined as \(g(\mathbf{X})=b\prod_{k=1}^{M}X_{k}\), where \(b\) is a constant, and \(\mathbf{X}=\{X_{1},\ldots,X_{M}\}\) represents \(M\) uncorrelated variables. Each variable \(X_{k}\) is either \(0\) or \(x_{k}\), depending on the absence or presence of a certain feature. Then, all the variables in \(\mathbf{X}\) contribute equally to \(g(\mathbf{X})\) at \([x_{1},\ldots,x_{M}]\) with respect to \([0,\ldots,0]\)._

Proofs of all Lemmas and Theorems can be found in the Appendix. Since all the binary variables have equal contribution, we define the contribution of each variable \(X_{k}\) to \(g(\mathbf{X})=b\prod_{k=1}^{M}X_{k}\) for all \(k=1,\ldots,M\), as

\[I_{X_{k}}=\frac{b\prod_{i=1}^{M}x_{i}}{M}.\] (2)

For example, let \(f(A,X)=AXW\) be a simple 2-node GNN for node classification, where \(A,X,W\) are \(2\times 2\) matrices that denote adjacency matrix, node feature matrix, and weight matrix, respectively. Then, we can represent each entry in the resulting \(2\times 2\) matrix \(f(A,X)\) as an expansion form:

\[f_{i,j}(A,X)=\sum_{k=0}^{1}\sum_{l=0}^{1}A_{i,k}X_{k,l}W_{l,j},\] (3)

where \(f_{i,j}(A,X)\) represents the prediction of the \(i\)-th node for the \(j\)-th class. In a pretrained GNN, parameter \(W\) is fixed. Thus, only \(A_{i,k}\) and \(X_{k,l}\) contribute to the value of each _scalar product_\(A_{i,k}X_{k,l}W_{l,j}\). As \(A_{i,k}\) is usually independent of \(X_{k,l}\) under proper data cleaning, we can calculate the contributions of \(A_{i,k}\) and \(X_{k,l}\) to the scalar product \(A_{i,k}X_{k,l}W_{l,j}\) by \(I_{A_{i,k}}=I_{X_{k,l}}=\frac{1}{2}A_{i,k}X_{k,l}W_{l,j}\) based on Lemma 2 and Equation (2). By similar computations for all the scalar products in the expansion form of \(f(\cdot)\), we can obtain the contribution of all the input features to each entry of the output matrix.

### Explaining Graph Neural Networks via Attribution

A typical GNN [14; 9; 30] for node or graph classification tasks usually comprises 2-6 message-passing layers for learning node or graph representations, followed by several fully connected layers that serve as the classifier. With the hidden state \(h_{i}^{(l)}\) of node \(v_{i}\) at the \(l\)-th message-passing layer defined as Equation (1), we generally have the hidden state \(H^{(l)}\) of a data sample as:

\[H^{(l)}=\sigma\left(\Phi^{(l)}\left(\left(A+\epsilon^{(l)}I\right)H^{(l-1)} \right)+\lambda\Psi^{(l)}\left(H^{(l-1)}\right)\right),\] (4)

where \(A\) is the adjacency matrix, \(\epsilon^{(l)}\) refers to the self-loop added to the graph if fixed to \(1\), otherwise it is a learnable parameter, \(\sigma(\cdot)\) is the element wise activation function, \(\Phi^{(l)}\) and \(\Psi^{(l)}\) can be Multilayer Perceptrons (MLP) or linear mappings, \(\lambda\in\{0,1\}\) determines whether a concatenation is required.

If the COMBINE step of a GNN requires a concatenation, we have \(\lambda=1\) and \(\epsilon^{(l)}=1\); if the COMBINE step requires a weighted sum, we have \(\epsilon^{(l)}\) set trainable and \(\lambda=0\). Alternatively, Equation (4) can be expanded to:

\[H^{(l)}=\sigma\left(AH^{(l-1)}\prod_{k=1}^{K}W^{\Phi^{(l)}_{k}}+\epsilon^{(l)} H^{(l-1)}\prod_{k=1}^{K}W^{\Phi^{(l)}_{k}}+\lambda H^{(l-1)}\prod_{q=1}^{Q}W^{ \Psi^{(l)}_{q}}\right),\] (5)

where \(K,Q\) refer to the number of MLP layers in \(\Phi^{(l)}(\cdot)\) and \(\Psi^{(l)}(\cdot)\), and \(W^{\Phi^{(l)}_{k}}\) and \(W^{\Psi^{(l)}_{q}}\) are the trainable parameters in \(\Phi^{(l)}_{k}\) and \(\Psi^{(l)}_{q}\).

Given a certain data sample and a pretrained GNN, for an element-wise activation function we can define the activation pattern as the ratio between the output and input of the activation function:

**Definition 3** (Activation Pattern).: _Denote \(H^{(l)\prime}\) and \(H^{(l)}\) as the hidden representations before and after passing through the element-wise activation function at the \(l\)-th layer, we define activation pattern \(P^{(l)}\) for a given data sample as_

\[P^{(l)}_{i,j}=\begin{cases}\frac{H^{(l)}_{i,j}}{H^{(l)\prime}_{i,j}},&\text{if }H^{(l)\prime}_{i,j}\neq 0\\ 0,&\text{otherwise}\end{cases}\]

_where \(P^{(l)}_{i,j}\) is the element-wise activation pattern for the \(j\)-th feature of \(i\)-th node at layer \(l\)._

Hence, the hidden state \(H^{(l)}\) at the \(l\)-th layer for a given sample can be written as

\[H^{(l)}=P^{(l)}\odot\left(AH^{(l-1)}\prod_{k=1}^{K}W^{\Phi^{(l)}_{k}}+\epsilon ^{(l)}H^{(l-1)}\prod_{k=1}^{K}W^{\Phi^{(l)}_{k}}+\lambda H^{(l-1)}\prod_{q=1}^ {Q}W^{\Psi^{(l)}_{q}}\right),\] (6)

where \(\odot\) represents element-wise multiplication. Thus, similar to Equation (3), we can expand the expression of each output entry in a GNN \(f(A,X)\) into a sum of scalar products, where each scalar product is the multiplication of corresponding entries in \(A\), \(X\), \(W\), and \(P\) in all layers. Then each scalar product can be written as

\[\begin{split} z=\mathbb{C}\cdot&\left(P^{(1)}_{ \alpha_{10},\beta_{11}}\dots P^{(L)}_{\alpha_{L0},\beta_{L1}}\right)\left(P^{ (c_{1})}_{\alpha_{L0},\gamma_{11}}\dots P^{(c_{(M-1)})}_{\alpha_{L0},\gamma_{( M-1)1}}\right)\cdot\\ &\left(A^{(L)}_{\alpha_{L0},\alpha_{L1}}\dots A^{(1)}_{\alpha_{1 0},\alpha_{11}}\right)X_{i,j}\left(W^{(1)}_{\beta_{10},\beta_{11}}\dots W^{(L) }_{\beta_{L0},\beta_{L1}}\right)\left(W^{(c_{1})}_{\gamma_{10},\gamma_{11}} \dots W^{(c_{M})}_{\gamma_{M0},\gamma_{M1}}\right),\end{split}\] (7)

where \(\mathbb{C}\) is a constant, \(c_{k}\) refers to the \(k\)-th layer of the classifier, \((\alpha_{l0},\alpha_{l1}),(\beta_{l0},\beta_{l1}),(\gamma_{l0},\gamma_{l1})\) are (_row, column_) indices of the corresponding matrices at layer \(l\). In a classifier with \(M\) MLP layers, only \((M-1)\) layers adopt activation functions. Therefore, we do not have \(P^{(c_{M})}_{\alpha_{L0},\gamma_{M1}}\) in Equation (7). For scalar products without factors of \(A\), all \(A\)'s are considered as constants equal to \(1\) in Equation (7). Since the GNN model parameters are pretrained and fixed, we only consider \(A\), \(X\), and all the \(P\) terms as the variables in each product term.

**Lemma 4** (**Equal Contribution variables in the GNN expansion form's scalar product)**.: _For a scalar product term \(z\) in the expansion form of a pretrained GNN \(f(\cdot)\), when the number of nodes \(N\) is large, all variables in \(z\) have equal contributions to the scalar product \(z\)._

Hence, by Equation (2), we can calculate the contribution \(I_{\nu}(z)\) of a variable \(\nu\) (i.e., an entry in \(A\), \(X\) and \(P\) matrices) to each scalar product \(z\) (given by Equation (7)) by:

\[I_{\nu}(z)=\frac{z}{|V(z)|},\] (8)

where function \(V(\cdot)\) represents the set of variables in its input, and \(|V(z)|\) denotes the number of unique **variables** in \(z\), e.g., \(V(x^{2}y)=\{x,y\}\), and \(|V(x^{2}y)|=2\).

Similar to Section 3.1, an entry \(f_{m,n}(A,X)\) of the output matrix \(f(A,X)\) can be expressed by the sum of all the related scalar products as

\[\begin{split} f_{m,n}(A,X)=\sum\mathbb{C}\cdot&\left( P^{(1)}_{\alpha_{10},\beta_{11}}\dots P^{(L)}_{m,\beta_{L1}}\right)\cdot \left(P^{(c_{1})}_{m,\gamma_{11}}\dots P^{(c_{(M-1)})}_{m,\gamma_{(M-1)1}} \right)\cdot\left(A^{(L)}_{m,\alpha_{L1}}\dots A^{(1)}_{\alpha_{10},\alpha_{1 1}}\right)\\ &\cdot X_{i,j}\cdot\left(W^{(1)}_{\beta_{10},\beta_{11}}\dots W^{ (L)}_{\beta_{L0},\beta_{L1}}\right)\cdot\left(W^{(c_{1})}_{\gamma_{10},\gamma_ {11}}\dots W^{(c_{M})}_{\gamma_{M0},n}\right),\end{split}\] (9)

where summation is over all possible \((\alpha_{l0},\alpha_{l1}),(\beta_{l0},\beta_{l1}),(\gamma_{l0},\gamma_{l1})\), for message passing layer \(l=1,\dots,L\) or classifier layer \(l=1,\dots,M\), as well as all \(i,j\) indices for \(X\). By summing up the contribution of each variable \(\nu\) among the entries in the \(A\), \(X\) and \(P\)'s in all the scalar products in the expansion form of \(f_{m,n}(\cdot)\), we can obtain the contribution of \(\nu\) to \(f_{m,n}(\cdot)\) as:

\[I_{\nu}(f_{m,n}(\cdot))=\sum_{z\,\mathrm{in}\,f_{m,n}(\cdot)\,\mathrm{hat\, contain}\,\nu}\frac{O(\nu,z)}{\sum_{\rho\,\mathrm{in}\,z}O(\rho,z)}\cdot z,\] (10)

**Theorem 5** (**Contribution of variables in the expansion form of a pretrained GNN)**.: _Given Equations (8) and (10), for each variable \(\nu\) (i.e., an entry in \(A\), \(X\) and \(P\) matrices), when the number of nodes \(N\) is large, we can approximate \(I_{\nu}(f_{m,n}(\cdot))\) by:_

\[I_{\nu}(f_{m,n}(\cdot))=\sum_{z\,\mathrm{in}\,f_{m,n}(\cdot)\,\mathrm{hat\, contain}\,\nu}\frac{O(\nu,z)}{\sum_{\rho\,\mathrm{in}\,z}O(\rho,z)}\cdot z,\] (11)

_where \(O(\nu,z)\) denotes the number of occurrences of \(\Recall that \(|V(z)|\) stand for the number of **unique variables** in \(z\). Hence the total number of occurrences of all the variables \(\sum_{\rho\,\mathrm{in}\,z}O(\rho,z)\) is not necessarily equal to \(|V(z)|\). For example, if all of \(\{A_{\alpha_{10},\alpha_{11}}^{(1)},\ldots,A_{\alpha_{k0},\alpha_{11}}^{(L)}\}\) in \(z\) are unique entries in \(A\), then they can be considered as \(L\) independent variables in the function representing \(z\). If two of these occurrences of variables refer to the same entry in \(A\), then there are only \((L-1)\) unique variables related to \(A\).

Although Theorem 5 gives the contribution of each entry in \(A\), \(X\) and \(P\)'s, we need to further attribute \(P\)'s to \(A\) and \(X\) and allocate the contribution of each activation pattern \(P_{a,b}^{(r)}\) to node features \(X\) and edges \(A\) by considering all non-zero features in \(X_{a}\) of node \(v_{a}\) and the edges within \(m\) hops of node \(v_{a}\), as these inputs may contribute to the activation pattern \(P_{a,b}^{(r)}\). However, determining the exact contribution of each feature that contributes to \(P_{a,b}^{(r)}\) is not straightforward due to non-linear activation. We approximately attribute all relevant features equally to \(P_{a,b}^{(r)}\). That is, each input feature \(\nu\) that has nonzero contribution to \(P_{a,b}^{(r)}\) will share an equal contribution of \(I_{P_{a,b}^{(r)}}\left(f_{m,n}(\cdot)\right)/|V(P_{a,b}^{(r)})|\), where \(|V(P_{a,b}^{(r)})|\) denotes the number of distinct node and edge features in \(X\) and \(A\) contributing to \(P_{a,b}^{(r)}\), which is exactly all non-zero features in \(X_{a}\) of node \(v_{a}\) and the adjacency matrix entries within \(r\) hops of node \(v_{a}\). Finally, based on Equation (11), we can obtain the contribution of an input feature \(\nu\) in \(X\), \(A\) of a graph instance to the \((m,n)\)-th entry of the GNN output \(f(\cdot)\) as:

\[\widehat{I}_{\nu}(f_{m,n}(\cdot))=I_{\nu}(f_{m,n}(\cdot))+\sum_{P_{a,b}^{(r)} \,\mathrm{in}\,f_{m,n}(\cdot),\,\mathrm{with}\,\nu\,\mathrm{in}\,P_{a,b}^{(r) }}\frac{I_{P_{a,b}^{(r)}}\left(f_{m,n}(\cdot)\right)}{|V(P_{a,b}^{(r)})|},\] (12)

where \(\nu\) is an entry in the adjacency matrix \(A\) or the input feature matrix \(X\), \(P_{a,b}^{(r)}\) denotes an entry in all the activation patterns. Thus, we have attributed \(f(\cdot)\) to each input feature of a given data instance.

Our approach meets the _completeness_ axiom, which is a critical requirement in attribution methods [25; 24; 6]. This axiom guarantees that the attribution scores for input features add up to the difference in the GNN's output with and without those features. Passing this sanity check implies that our approach provides a more comprehensive account of feature importance than existing methods that only rank the top features [3; 17; 20; 31; 27; 23].

### Case Study: Explaining Graph Convolutional Network (GCN)

GCNs use a simple sum in the combination step, and the adjacency matrix is normalized with the diagonal node degree matrix \(D\). Hence, the hidden state of a GCN's \(l\)-th message-passing layer is:

\[H^{(l)}=\mathrm{ReLU}\left(VH^{(l-1)}W^{(l)}+B^{(l)}\right),\] (13)

where \(V=D^{-\frac{1}{2}}(A+I)D^{-\frac{1}{2}}\) represents the normalized adjacency matrix with self-loops added. Suppose a GCN has three convolution layers and a 2-layer MLP as the classifier, then its expansion form without the activation functions \(\mathrm{ReLU}(\cdot)\) will be:

\[\begin{split} f(V,X)_{\mathbf{P}}&=V^{(3)}V^{(2)} V^{(1)}XW^{(1)}W^{(2)}W^{(3)}W^{(c_{1})}W^{(c_{2})}+V^{(3)}V^{(2)}B^{(1)}W^{(2)}W^{( 3)}W^{(c_{1})}W^{(c_{2})}\\ &+V^{(3)}B^{(2)}W^{(3)}W^{(c_{1})}W^{(c_{2})}+B^{(3)}W^{(c_{1})}W^ {(c_{2})}+B^{(c_{1})}W^{(c_{2})}+B^{(c_{2})},\end{split}\] (14)

where \(V^{(l)}=V\) is the normalized adjacency matrix in the \(l\)-th layer's calculation. In the actual expansion form with the activation patterns, the corresponding \(P^{(m)}\)'s are multiplied whenever there is a \(W^{(m)}\) or \(B^{(m)}\) in a scalar product, excluding the last layer \(W^{(c_{2})}\) and \(B^{(c_{2})}\). For example, in the scalar products corresponding to \(V^{(3)}V^{(2)}V^{(1)}XW^{(1)}W^{(2)}W^{(3)}W^{(c_{1})}W^{(c_{2})}\), there are eight variables consisting of four \(P\)'s, one \(X\), and three \(V\)'s. By Equation (11), an adjacency entry \(V_{i,j}\) itself will contribute \(\frac{1}{8}\) of \(p(V^{(3)}V^{(2)}_{:i}V^{(1)}_{i,j}X_{j}.W^{(1)}W^{(2)}W^{(3)}W^{(c_{1})}W^{(c_ {2})})+p(V^{(3)}_{:i}V^{(2)}_{:j}V^{(1)}_{j:}XW^{(1)}W^{(2)}W^{(3)}W^{(c_{1})}W ^{(c_{2})})+p(V^{(3)}_{:j}V^{(2)}_{:j}V^{(1)}XW^{(1)}W^{(2)}W^{(3)}W^{(c_{1})}W ^{(c_{2})})\), where \(p(\cdot)\) denotes the results with the element-wise multiplication of the corresponding activation patterns applied at the appropriate layers. After we obtain the contribution of \(V_{i,j}\) itself on all the scalar products, we can follow Equation (12) to allocate the contribution of activation patterns to \(V_{i,j}\)With Equation (14), we find that when both \(V\) and \(X\) are set to zeros, \(f(\cdot)\) remains non-zero and is:

\[f(\mathbf{0},\mathbf{0})=p(B^{(3)}W^{(c_{1})}W^{(c_{2})})+p(B^{(c_{1})}W^{(c_{2}) })+B^{(c_{2})},\] (15)

where \(B^{(c_{2})}\) is the global bias, and the other terms have non-zero entries at the activated neurons. In other words, certain GNN neurons in the \(3\)-rd and \(c_{1}\)-th layers may already be activated prior to any input feature being passed to the network. When we do feed input features, some of these neurons may remain activated or be toggled off. With Equation (12), we consider taking all \(0\)'s of the \(X\) entries, \(V\) entries and \(P\) entries as the base manifold. Now, given that some of the \(P\) entries in GCN are non-zero when all \(X\) and \(V\) set to zeros, as present in Equation (15), we will need to subtract the contribution of each features on these \(P\) from the contribution values calculated by Equation (12). We let \(\mathbf{P}^{\prime}\) represent the activation patterns of \(f(\mathbf{0},\mathbf{0})\), then the calibrated contribution \(\widehat{I}_{V_{i,j}}^{\mathrm{cali}}(f(\cdot))\) of \(V_{i,j}\) is given by:

\[\widehat{I}_{V_{i,j}}^{\mathrm{cali}}(f(\cdot))=\widehat{I}_{V_{i,j}}(f(V,X)) -\sum_{P^{\prime(r)}_{a,b}\,\mathrm{in}\,f(\mathbf{0},\mathbf{0}),\,\mathrm{ with}\,V_{i,j}\,\mathrm{in}\,P^{\prime(r)}_{a,b}}\frac{I_{P^{\prime(r)}_{a,b}} \left(f(\mathbf{0},\mathbf{0})\right)}{|V(P^{(r)}_{a,b})|}.\] (16)

In graph classification tasks, a pooling layer such as mean-pooling is added after the convolution layers to obtain the graph representation. To determine the contribution of each input feature, we can simply apply the same pooling operation as used in the pre-trained GCN.

As we mentioned in Section 2, we aim to obtain the explanations by the critical edges in this paper, since edges have more fine-grained information than nodes. Therefore, we treat the edges as variables, while considering the node features \(X\) as constants similar to parameters \(W\) or \(B\). This setup naturally aggregates the contribution of node features onto edges. By leveraging edge attributions, we are able to effectively highlight motifs within the graph structure.

## 4 Experiments

We conduct a series of experiments on the fidelity, discriminability and stability metrics to compare our method with the state-of-the-art methods including GNNExplainer [31], PGExplainer [17], PGMLExplainer [27], SubgraphX [33], CF-GNNExplainer [16], RCExplainer [3], RG-Explainer [23] and DEGREE [8]. As outlined in Section 2, we highlight edges as explanations as suggested by [7]. For baselines that identify nodes or subgraphs as explanations, we adopt the evaluation setup from [3].

We evaluate the performance of explanations on three variants of GNNs, which are GCN [14], GraphSAGE [9] and GIN [30]. The experiments are conducted on both the graph classification task and the node classification task. For graph classification task, we evaluate on a synthetic dataset, BA-2motifs [17], and two real-world datasets, Mutagenicity [13] and NCI1 [19]. For node classification task, we evaluate on three synthetic datasets [17], which are BA-shapes, BA-Community and Tree-grid. As space is limited, we will only present the key results here. Fidelity results on GIN and GraphSAGE, as well as the results of node classification tasks can be found in the Appendix. Discussions on the controversial metrics such as accuracy are also moved to the Appendix.

### Fidelity

Fidelity [20, 32, 29, 3] is the decrease of predicted probability between original and new predictions after removing important edges, which are used to evaluate the faithfulness of explanations. It is

Figure 1: Fidelity performance averaged across 10 runs on the pretrained GCNs for the datasets at different levels of average sparsity.

defined as \(fidelity(S,G)=f_{y}(G)-f_{y}(G\backslash S)\). As pointed out by [32], the fidelity may be sensitive to sparsity of explanations. The sparsity of an explanation \(S\subseteq\mathcal{E}\) for a graph \(G=\{\mathcal{V},\mathcal{E}\}\) is given by \(sparsity(S,G)=1-\frac{|S|}{|\mathcal{E}|}\). It indicates the percentage of edges that remain in \(G\) after the removal of edges in \(S\). Higher sparsity means fewer edges are identified as critical, which may have a smaller impact on the prediction probability. Therefore, we compare fidelity performance under similar levels of average sparsity, as in [33, 29, 3]. Figure 1 displays the fidelity results, with the baseline results sourced from [3]. Our proposed approach, _GOAt_, consistently outperforms the baselines in terms of fidelity across all sparsity levels, validating its superior performance in generating accurate and reliable faithful explanations. Among the other methods, RCExplainer exhibits the highest fidelity, as it is specifically designed for fidelity optimization. Notably, unlike the other methods that require training and hyperparameter tuning, _GOAt_ offers the advantage of being a training-free approach, thereby avoiding any errors across different runs.

### Discriminability

Discriminability, also known as discrimination ability [5, 12], refers to the ability of the explanations to distinguish between the classes. We define the discriminability between two classes \(c_{1}\) and \(c_{2}\) as the L2 norm of the difference between the mean values of explanation embeddings of the two classes. The embeddings used for explanations are taken prior to the last-layer classifier, with node embeddings employed for node classification tasks and graph embeddings utilized for graph classification tasks. In this procedure, only the explanation subgraph \(S\) is fed into the GNN instead of \(G\).

We show the discriminability across various sparsity levels on GCN, as illustrated in Figure 2. Due to the significant performance gap between the baselines and _GOAt_, a logarithmic scale is employed. Our approach consistently outperforms the baselines in terms of discriminability across all sparsity levels, demonstrating its superior ability to generate accurate and reliable class-specific explanations. Notably, at \(sparsity=0.7\), _GOAt_ achieves higher discriminability than the original graphs on the BA-2Motifs and NCI1 datasets. This indicates that _GOAt_ effectively reduces noise unrelated to the investigated class while producing informative class explanations. Additionally, we observe a

Figure 3: Visualization of explanation embeddings on the BA-2Motifs dataset. Subfigure (i) refers to the visualization of the original embeddings by directly feeding the original data into the GNN without any modifications or explanations applied.

Figure 2: Discriminability performance averaged across 10 runs of the explanations produced by various GNN explainers at different levels of sparsity. “Original” refer to the performance of feeding the original data into the GNN without any modifications or explanations applied.

substantial decrease in discriminability between sparsity levels of 0.75 and 0.8 on BA-2Motifs. This implies that a minimum of approximately 25% of the edges is necessary to distinguish between the classes, which is in line with our expectation, given that a "house" motif, consisting of 6 edges, usually represents 24% of the total edges (on average, the total number of edges in BA-2Motifs is 25).

Furthermore, we present scatter plots to visualize the explanation embeddings generated by various GNN explainers. Figure 3 showcases the explanation embeddings obtained from different GNN explaining methods on the BA-2Motifs dataset, with \(sparsity=0.7\). More scatter plots on Mutagenicity and NCI1 and can be found in the Appendix. The explanations generated by GNNExplainer fail to exhibit class discrimination, as all the data points are clustered together without any distinct separation. While some of the Class 1 explanations produced by PGExplainer, PGM-Explainer, RG-Explainer, RCExplainer, and DEGREE are noticeably separate from the Class 0 explanations, the majority of the data points remain closely clustered together. As for SubgraphX, most of its Class 1 explanations are isolated from the Class 0 explanations, but there is a discernible overlap between the Class 1 and Class 0 data points. In contrast, our method, _GOAt_, generates explanations that clearly and effectively distinguish between Class 0 and Class 1, with no overlapping points and a substantial separation distance, highlighting the strong discriminability of our approach.

### Stability of extracting motifs

As we will later show in Section 4.4, it is often observed that datasets contain specific class motifs. For instance, in the BA-2Motifs dataset, the Class 1 motif exhibits a "house" structure. To ensure the stability of GNN explainers in capturing the class motifs across diverse data samples, we aim for the explanation motifs to exhibit relative consistency for data samples with similar properties, rather than exhibiting significant variations. To quantify this characteristic, we introduce the _stability_ metric, which measures the coverage of the top-\(k\) explanations across the dataset. An ideal explainer should generate explanations that cover a larger number of data samples using fewer motifs. This characteristic is also highly desirable in global-level explainers, such as [2; 11]. We illustrate the stability of the unbiased class as the percentage converge of the top-\(k\) explanations produced on GCN with \(sparsity=0.7\) in Figure 4. Our approach surpasses the baselines by a considerable margin in terms of the stability of producing explanations. Specifically, _GOAt_ is capable of providing explanations for all the Class 1 data samples using only three explanations. This explains why there are only three Class 1 scatters visible in Figure 3.

### Qualitative analysis

We present the qualitative results of our approach in Table 1, where we compare it with state-of-the-art baselines such as PGExplainer, SubgraphX, and RCExplainer. The pretrained GNN achieves a 100% accuracy on the BA-2Motifs dataset. As long as it successfully identifies one class, the remaining data samples naturally belong to the other class, leading to a perfect accuracy rate. Based on the explanations from _GOAt_, we have observed that the GNN effectively recognizes the "house" motif that is associated with Class 1. In contrast, other approaches face difficulties in consistently capturing this motif. The Class 0 motifs in the Mutagenicity dataset generated by _GOAt_ represent multiple connected carbon rings. This indicates that the presence of more carbon rings in a molecule increases its likelihood of being mutagenic (Class 0), while the presence of more "C-H" or "O-H" bonds in a molecule increases its likelihood of being non-mutagenic (Class 1). Similarly, in the NCI1 dataset, _GOAt_ discovers that the GNN considers a higher number of carbon rings as evidence of chemical

Figure 4: Coverage of the top-\(k\) explanations across the datasets.

[MISSING_PAGE_FAIL:9]

[MISSING_PAGE_FAIL:10]

* [19] Douglas EV Pires, Tom L Blundell, and David B Ascher. pkcsm: predicting small-molecule pharmacokinetic and toxicity properties using graph-based signatures. _Journal of medicinal chemistry_, 58(9):4066-4072, 2015.
* [20] Phillip E Pope, Soheil Kolouri, Mohammad Rostami, Charles E Martin, and Heiko Hoffmann. Explainability methods for graph convolutional neural networks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10772-10781, 2019.
* [21] Michael Sejr Schlichtkrull, Nicola De Cao, and Ivan Titov. Interpreting graph neural networks for nlp with differentiable edge masking. In _International Conference on Learning Representations_, 2021.
* [22] Thomas Schnake, Oliver Eberle, Jonas Lederer, Shinichi Nakajima, Kristor T Schutt, Klaus-Robert Muller, and Gregoire Montavon. Higher-order explanations of graph neural networks via relevant walks. _IEEE transactions on pattern analysis and machine intelligence_, 44(11):7581-7596, 2021.
* [23] Caihua Shan, Yifei Shen, Yao Zhang, Xiang Li, and Dongsheng Li. Reinforcement learning enhanced explainer for graph neural networks. _Advances in Neural Information Processing Systems_, 34:22523-22533, 2021.
* [24] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating activation differences. In _International conference on machine learning_, pages 3145-3153. PMLR, 2017.
* [25] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In _International conference on machine learning_, pages 3319-3328. PMLR, 2017.
* [26] Juntao Tan, Shijie Geng, Zuohui Fu, Yingqiang Ge, Shuyuan Xu, Yunqi Li, and Yongfeng Zhang. Learning and evaluating graph neural network explanations based on counterfactual and factual reasoning. In _Proceedings of the ACM Web Conference 2022_, pages 1018-1027, 2022.
* [27] Minh Vu and My T Thai. Pgm-explainer: Probabilistic graphical model explanations for graph neural networks. _Advances in neural information processing systems_, 33:12225-12235, 2020.
* [28] Daixin Wang, Zhiqiang Zhang, Jun Zhou, Peng Cui, Jingli Fang, Quanhui Jia, Yanming Fang, and Yuan Qi. Temporal-aware graph neural network for credit risk prediction. In _Proceedings of the 2021 SIAM International Conference on Data Mining (SDM)_, pages 702-710. SIAM, 2021.
* [29] Lingfei Wu, Peng Cui, Jian Pei, and Liang Zhao. _Graph Neural Networks: Foundations, Frontiers, and Applications_. Springer Nature, 2022.
* [30] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _International Conference on Learning Representations_, 2019.
* [31] Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnonexplainer: Generating explanations for graph neural networks. _Advances in neural information processing systems_, 32, 2019.
* [32] Hao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji. Explainability in graph neural networks: A taxonomic survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.
* [33] Hao Yuan, Haiyang Yu, Jie Wang, Kang Li, and Shuiwang Ji. On explainability of graph neural networks via subgraph explorations. In _International Conference on Machine Learning_, pages 12241-12252. PMLR, 2021.
* [34] Yue Zhang, David Defazio, and Arti Ramesh. Relex: A model-agnostic relational model explainer. In _Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society_, pages 1042-1049, 2021.